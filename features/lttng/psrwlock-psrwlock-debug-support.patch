From 6d2370b256efa3c2b6f8a9b34071406a418e05d7 Mon Sep 17 00:00:00 2001
From: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
Date: Thu, 13 May 2010 19:26:47 -0400
Subject: [PATCH 168/391] psrwlock/psrwlock-debug-support

Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
CC: Linus Torvalds <torvalds@linux-foundation.org>
Cc: "H. Peter Anvin" <hpa@zytor.com>
CC: Jeremy Fitzhardinge <jeremy@goop.org>
CC: Andrew Morton <akpm@linux-foundation.org>
CC: Ingo Molnar <mingo@elte.hu>
CC: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
---
 include/linux/lockdep.h            |   15 ++
 include/linux/poison.h             |    4 +
 include/linux/psrwlock-api.h       |  243 ++++++++++++++++++++++++++++++++++
 include/linux/psrwlock-debug-api.h |  189 +++++++++++++++++++++++++++
 include/linux/psrwlock-debug.h     |   22 +++
 include/linux/psrwlock-types.h     |   82 +++++++++++-
 include/linux/psrwlock.h           |  254 ++----------------------------------
 include/linux/sched.h              |    3 +
 lib/Kconfig.debug                  |   10 ++
 lib/Makefile                       |    1 +
 lib/psrwlock-debug.c               |  131 ++++++++++++++++++
 lib/psrwlock-debug.h               |   31 +++++
 lib/psrwlock.c                     |  208 +++++++++++++++++++++---------
 lib/psrwlock.h                     |   22 +++
 14 files changed, 905 insertions(+), 310 deletions(-)
 create mode 100644 include/linux/psrwlock-api.h
 create mode 100644 include/linux/psrwlock-debug-api.h
 create mode 100644 include/linux/psrwlock-debug.h
 create mode 100644 lib/psrwlock-debug.c
 create mode 100644 lib/psrwlock-debug.h
 create mode 100644 lib/psrwlock.h

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index a03977a..dede84a 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -542,4 +542,19 @@ do {									\
 extern void lockdep_rcu_dereference(const char *file, const int line);
 #endif
 
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+# ifdef CONFIG_PROVE_LOCKING
+#  define psrwlock_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 2, NULL, i)
+#  define psrwlock_acquire_read(l, s, t, i)	lock_acquire(l, s, t, 2, 2, NULL, i)
+# else
+#  define psrwlock_acquire(l, s, t, i)		lock_acquire(l, s, t, 0, 1, NULL, i)
+#  define psrwlock_acquire_read(l, s, t, i)	lock_acquire(l, s, t, 2, 1, NULL, i)
+# endif
+# define psrwlock_release(l, n, i)		lock_release(l, n, i)
+#else
+# define psrwlock_acquire(l, s, t, i)		do { } while (0)
+# define psrwlock_acquire_read(l, s, t, i)	do { } while (0)
+# define psrwlock_release(l, n, i)		do { } while (0)
+#endif
+
 #endif /* __LINUX_LOCKDEP_H */
diff --git a/include/linux/poison.h b/include/linux/poison.h
index 34066ff..b6561c6 100644
--- a/include/linux/poison.h
+++ b/include/linux/poison.h
@@ -89,6 +89,10 @@
 /********** lib/flex_array.c **********/
 #define FLEX_ARRAY_FREE	0x6c	/* for use-after-free poisoning */
 
+/********** Priority-Sifting Reader-Writer Locks **********/
+#define PSRWLOCK_DEBUG_INIT	0x33
+#define PSRWLOCK_DEBUG_FREE	0x44
+
 /********** security/ **********/
 #define KEY_DESTROY		0xbd
 
diff --git a/include/linux/psrwlock-api.h b/include/linux/psrwlock-api.h
new file mode 100644
index 0000000..890993b
--- /dev/null
+++ b/include/linux/psrwlock-api.h
@@ -0,0 +1,243 @@
+#ifndef _LINUX_PSRWLOCK_API_H
+#define _LINUX_PSRWLOCK_API_H
+
+/* Reader lock */
+
+/*
+ * many readers, from irq/softirq/non preemptable and preemptable thread
+ * context. Protects against writers.
+ *
+ * Read lock fastpath :
+ *
+ * A cmpxchg is used here and _not_ a simple add because a lower-priority reader
+ * could block the writer while it is waiting for readers to clear the
+ * uncontended path. This would happen if, for instance, the reader gets
+ * interrupted between the add and the moment it gets to the slow path.
+ */
+
+/*
+ * Called from any context.
+ * Statically check for preemptable writer to compile-out the check if all the
+ * contexts accessing the lock are non-preemptable.
+ */
+static inline void psread_unlock(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc = atomic_sub_return(UC_READER_OFFSET, &rwlock->uc);
+	if (wctx == PSRW_PRIO_P || (rctx & PSR_PTHREAD))
+		psrwlock_preempt_check(uc, rwlock);
+}
+
+/*
+ * Called from interrupt disabled or interrupt context.
+ */
+static inline void psread_lock_irq(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	BUILD_BUG_ON(!(rctx & PSR_IRQ));
+	uc = atomic_cmpxchg(&rwlock->uc, 0, UC_READER_OFFSET);
+	if (likely(!uc))
+		return;
+	psread_lock_slow_irq(uc, rwlock);
+}
+
+static inline int psread_trylock_irq(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	BUILD_BUG_ON(!(rctx & PSR_IRQ));
+	uc = atomic_cmpxchg(&rwlock->uc, 0, UC_READER_OFFSET);
+	if (likely(!uc))
+		return 1;
+	return psread_trylock_slow_irq(uc, rwlock);
+}
+
+/*
+ * Called from softirq context.
+ */
+
+static inline void psread_lock_bh(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	BUILD_BUG_ON(!(rctx & PSR_BH));
+	uc = atomic_cmpxchg(&rwlock->uc, 0, UC_READER_OFFSET);
+	if (likely(!uc))
+		return;
+	psread_lock_slow_bh(uc, rwlock);
+}
+
+static inline int psread_trylock_bh(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	BUILD_BUG_ON(!(rctx & PSR_BH));
+	uc = atomic_cmpxchg(&rwlock->uc, 0, UC_READER_OFFSET);
+	if (likely(!uc))
+		return 1;
+	return psread_trylock_slow_bh(uc, rwlock);
+}
+
+
+/*
+ * Called from non-preemptable thread context.
+ */
+
+static inline void psread_lock_inatomic(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	BUILD_BUG_ON(!(rctx & PSR_NPTHREAD));
+	uc = atomic_cmpxchg(&rwlock->uc, 0, UC_READER_OFFSET);
+	if (likely(!uc))
+		return;
+	psread_lock_slow_inatomic(uc, rwlock);
+}
+
+static inline int psread_trylock_inatomic(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	BUILD_BUG_ON(!(rctx & PSR_NPTHREAD));
+	uc = atomic_cmpxchg(&rwlock->uc, 0, UC_READER_OFFSET);
+	if (likely(!uc))
+		return 1;
+	return psread_trylock_slow_inatomic(uc, rwlock);
+}
+
+
+/*
+ * Called from preemptable thread context.
+ */
+
+static inline void psread_lock(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	BUILD_BUG_ON(!(rctx & PSR_PTHREAD));
+	uc = atomic_cmpxchg(&rwlock->uc, 0, UC_READER_OFFSET);
+	if (likely(!uc))
+		return;
+	psread_lock_slow(uc, rwlock);
+}
+
+static inline int psread_lock_interruptible(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	BUILD_BUG_ON(!(rctx & PSR_PTHREAD));
+	uc = atomic_cmpxchg(&rwlock->uc, 0, UC_READER_OFFSET);
+	if (likely(!uc))
+		return 0;
+	return psread_lock_interruptible_slow(uc, rwlock);
+}
+
+static inline int psread_trylock(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	BUILD_BUG_ON(!(rctx & PSR_PTHREAD));
+	uc = atomic_cmpxchg(&rwlock->uc, 0, UC_READER_OFFSET);
+	if (likely(!uc))
+		return 1;
+	return psread_trylock_slow(uc, rwlock);
+}
+
+
+/* Writer Lock */
+
+/*
+ * ctx is the context map showing which contexts can take the read lock and
+ * which context is using the write lock.
+ *
+ * Write lock use example, where the lock is used by readers in interrupt,
+ * preemptable context and non-preemptable context. The writer lock is taken in
+ * preemptable context.
+ *
+ * static DEFINE_PSRWLOCK(lock, PSRW_PRIO_P, PSR_IRQ | PSR_PTHREAD);
+ * CHECK_PSRWLOCK_MAP(lock, PSRW_PRIO_P, PSR_IRQ | PSR_PTHREAD);
+ *
+ *  pswrite_lock(&lock, PSRW_PRIO_P, PSR_IRQ | PSR_PTHREAD);
+ *  ...
+ *  pswrite_unlock(&lock, PSRW_PRIO_P, PSR_IRQ | PSR_PTHREAD);
+ */
+static inline
+void pswrite_lock(psrwlock_t *rwlock, enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	write_context_disable(wctx, rctx);
+	/* no other reader nor writer present, try to take the lock */
+	uc = atomic_cmpxchg(&rwlock->uc, 0, UC_WRITER);
+	if (likely(!uc))
+		return;
+	else
+		pswrite_lock_slow(uc, rwlock);
+}
+
+static inline
+int pswrite_lock_interruptible(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	write_context_disable(wctx, rctx);
+	/* no other reader nor writer present, try to take the lock */
+	uc = atomic_cmpxchg(&rwlock->uc, 0, UC_WRITER);
+	if (likely(!uc))
+		return 0;
+	else
+		return pswrite_lock_interruptible_slow(uc, rwlock);
+}
+
+static inline
+int pswrite_trylock(psrwlock_t *rwlock, enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	write_context_disable(wctx, rctx);
+	/* no other reader nor writer present, try to take the lock */
+	uc = atomic_cmpxchg(&rwlock->uc, 0, UC_WRITER);
+	if (likely(!uc))
+		return 1;
+	else
+		return pswrite_trylock_slow(uc, rwlock);
+}
+
+static inline
+void pswrite_unlock(psrwlock_t *rwlock, enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	/*
+	 * atomic_cmpxchg makes sure we commit the data before reenabling
+	 * the lock. Will take the slow path if there are active readers, if
+	 * UC_SLOW_WRITER is set or if there are threads in the wait queue.
+	 */
+	uc = atomic_cmpxchg(&rwlock->uc, UC_WRITER, 0);
+	if (likely(uc == UC_WRITER)) {
+		write_context_enable(wctx, rctx);
+		/*
+		 * no need to check preempt because all wait queue masks
+		 * were 0. An active wait queue would trigger the slow path.
+		 */
+		return;
+	}
+	/*
+	 * Go through the slow unlock path to check if we must clear the
+	 * UC_SLOW_WRITER bit.
+	 */
+	pswrite_unlock_slow(uc, rwlock);
+}
+
+#endif /* _LINUX_PSRWLOCK_API_H */
diff --git a/include/linux/psrwlock-debug-api.h b/include/linux/psrwlock-debug-api.h
new file mode 100644
index 0000000..93e1e82
--- /dev/null
+++ b/include/linux/psrwlock-debug-api.h
@@ -0,0 +1,189 @@
+#ifndef _LINUX_PSRWLOCK_DEBUG_API_H
+#define _LINUX_PSRWLOCK_DEBUG_API_H
+
+#include <linux/lockdep.h>
+
+/*
+ * Priority-Sifting reader-writer lock debugging API. Using the slow path.
+ */
+
+/* Reader lock */
+
+/*
+ * many readers, from irq/softirq/non preemptable and preemptable thread
+ * context. Protects against writers.
+ */
+
+/*
+ * Called from any context.
+ * Statically check for preemptable writer to compile-out the check if all the
+ * contexts accessing the lock are non-preemptable.
+ */
+extern void psread_unlock(psrwlock_t *rwlock, enum psrw_prio wctx, u32 rctx);
+
+/*
+ * Called from interrupt disabled or interrupt context.
+ */
+static inline void psread_lock_irq(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	BUILD_BUG_ON(!(rctx & PSR_IRQ));
+	uc = atomic_read(&rwlock->uc);
+	psread_lock_slow_irq(uc, rwlock);
+}
+
+static inline int psread_trylock_irq(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	BUILD_BUG_ON(!(rctx & PSR_IRQ));
+	uc = atomic_read(&rwlock->uc);
+	return psread_trylock_slow_irq(uc, rwlock);
+}
+
+/*
+ * Called from softirq context.
+ */
+
+static inline void psread_lock_bh(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	BUILD_BUG_ON(!(rctx & PSR_BH));
+	uc = atomic_read(&rwlock->uc);
+	psread_lock_slow_bh(uc, rwlock);
+}
+
+static inline int psread_trylock_bh(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	BUILD_BUG_ON(!(rctx & PSR_BH));
+	uc = atomic_read(&rwlock->uc);
+	return psread_trylock_slow_bh(uc, rwlock);
+}
+
+
+/*
+ * Called from non-preemptable thread context.
+ */
+
+static inline void psread_lock_inatomic(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	BUILD_BUG_ON(!(rctx & PSR_NPTHREAD));
+	uc = atomic_read(&rwlock->uc);
+	psread_lock_slow_inatomic(uc, rwlock);
+}
+
+static inline int psread_trylock_inatomic(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	BUILD_BUG_ON(!(rctx & PSR_NPTHREAD));
+	uc = atomic_read(&rwlock->uc);
+	return psread_trylock_slow_inatomic(uc, rwlock);
+}
+
+
+/*
+ * Called from preemptable thread context.
+ */
+
+static inline void psread_lock(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	BUILD_BUG_ON(!(rctx & PSR_PTHREAD));
+	uc = atomic_read(&rwlock->uc);
+	psread_lock_slow(uc, rwlock);
+}
+
+static inline int psread_lock_interruptible(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	BUILD_BUG_ON(!(rctx & PSR_PTHREAD));
+	uc = atomic_read(&rwlock->uc);
+	return psread_lock_interruptible_slow(uc, rwlock);
+}
+
+static inline int psread_trylock(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	BUILD_BUG_ON(!(rctx & PSR_PTHREAD));
+	uc = atomic_read(&rwlock->uc);
+	return psread_trylock_slow(uc, rwlock);
+}
+
+
+/* Writer Lock */
+
+/*
+ * ctx is the context map showing which contexts can take the read lock and
+ * which context is using the write lock.
+ *
+ * Write lock use example, where the lock is used by readers in interrupt,
+ * preemptable context and non-preemptable context. The writer lock is taken in
+ * preemptable context.
+ *
+ * static DEFINE_PSRWLOCK(lock, PSRW_PRIO_P, PSR_IRQ | PSR_PTHREAD);
+ * CHECK_PSRWLOCK_MAP(lock, PSRW_PRIO_P, PSR_IRQ | PSR_PTHREAD);
+ *
+ *  pswrite_lock(&lock, PSRW_PRIO_P, PSR_IRQ | PSR_PTHREAD);
+ *  ...
+ *  pswrite_unlock(&lock, PSRW_PRIO_P, PSR_IRQ | PSR_PTHREAD);
+ */
+static inline
+void pswrite_lock(psrwlock_t *rwlock, enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	write_context_disable(wctx, rctx);
+	uc = atomic_read(&rwlock->uc);
+	pswrite_lock_slow(uc, rwlock);
+}
+
+static inline
+int pswrite_lock_interruptible(psrwlock_t *rwlock,
+		enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	write_context_disable(wctx, rctx);
+	uc = atomic_read(&rwlock->uc);
+	return pswrite_lock_interruptible_slow(uc, rwlock);
+}
+
+static inline
+int pswrite_trylock(psrwlock_t *rwlock, enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	write_context_disable(wctx, rctx);
+	uc = atomic_read(&rwlock->uc);
+	return pswrite_trylock_slow(uc, rwlock);
+}
+
+static inline
+void pswrite_unlock(psrwlock_t *rwlock, enum psrw_prio wctx, u32 rctx)
+{
+	unsigned int uc;
+
+	uc = atomic_read(&rwlock->uc);
+	pswrite_unlock_slow(uc, rwlock);
+}
+
+#endif /* _LINUX_PSRWLOCK_DEBUG_API_H */
diff --git a/include/linux/psrwlock-debug.h b/include/linux/psrwlock-debug.h
new file mode 100644
index 0000000..e71e61f
--- /dev/null
+++ b/include/linux/psrwlock-debug.h
@@ -0,0 +1,22 @@
+#ifndef _LINUX_PSRWLOCK_DEBUG_H
+#define _LINUX_PSRWLOCK_DEBUG_H
+
+#include <linux/lockdep.h>
+
+/*
+ * Priority-Sifting Reader-Writer Locks : debugging helpers:
+ */
+
+#define __DEBUG_PSRWLOCK_INITIALIZER(lockname)				\
+	.magic = &lockname,
+
+#define psrwlock_init(psrwlock, _rctx, _wctx)				\
+do {									\
+	static struct lock_class_key __key;				\
+									\
+	__psrwlock_init((psrwlock), #psrwlock, &__key, _rctx, _wctx);	\
+} while (0)
+
+extern void psrwlock_destroy(struct psrwlock *lock);
+
+#endif /* _LINUX_PSRWLOCK_DEBUG_H */
diff --git a/include/linux/psrwlock-types.h b/include/linux/psrwlock-types.h
index 53f1572..5606857 100644
--- a/include/linux/psrwlock-types.h
+++ b/include/linux/psrwlock-types.h
@@ -8,7 +8,10 @@
  * August 2008
  */
 
-#include <linux/wait.h>
+#include <linux/list.h>
+#include <linux/linkage.h>
+#include <linux/lockdep.h>
+
 #include <asm/atomic.h>
 
 /*
@@ -61,10 +64,51 @@ typedef struct psrwlock {
 	atomic_long_t prio[PSRW_NR_PRIO]; /* Per priority slow path counts */
 	u32 rctx_bitmap;		/* Allowed read execution ctx */
 	enum psrw_prio wctx;		/* Allowed write execution ctx */
-	wait_queue_head_t wq_read;	/* Preemptable readers wait queue */
-	wait_queue_head_t wq_write;	/* Preemptable writers wait queue */
+	struct list_head wait_list_r;	/* Preemptable readers wait queue */
+	struct list_head wait_list_w;	/* Preemptable writers wait queue */
+#ifdef CONFIG_DEBUG_PSRWLOCK
+	struct thread_info	*owner;
+	const char 		*name;
+	void			*magic;
+#endif
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	struct lockdep_map      dep_map;
+#endif
 } psrwlock_t;
 
+/*
+ * This is the control structure for tasks blocked on psrwlock,
+ * which resides on the blocked task's kernel stack:
+ */
+struct psrwlock_waiter {
+	struct list_head	list;
+	struct task_struct	*task;
+#ifdef CONFIG_DEBUG_PSRWLOCK
+	struct psrwlock		*lock;
+	void			*magic;
+#endif
+};
+
+#ifdef CONFIG_DEBUG_PSRWLOCK
+# include <linux/psrwlock-debug.h>
+#else
+# define __DEBUG_PSRWLOCK_INITIALIZER(lockname)
+# define psrwlock_init(psrwlock, _rctx, _wctx)				\
+do {									\
+	static struct lock_class_key __key;				\
+									\
+	__psrwlock_init((psrwlock), #psrwlock, &__key, _rctx, _wctx);	\
+} while (0)
+# define psrwlock_destroy(psrwlock)		do { } while (0)
+#endif
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+# define __DEP_MAP_PSRWLOCK_INITIALIZER(lockname)			\
+		.dep_map = { .name = #lockname },
+#else
+# define __DEP_MAP_PSRWLOCK_INITIALIZER(lockname)
+#endif
+
 #define __PSRWLOCK_UNLOCKED(x, _wctx, _rctx)				\
 	{								\
 		.uc = { 0 },						\
@@ -72,8 +116,10 @@ typedef struct psrwlock {
 		.prio[0 ... (PSRW_NR_PRIO - 1)] = { 0 },		\
 		.rctx_bitmap = (_rctx),					\
 		.wctx = (_wctx),					\
-		.wq_read = __WAIT_QUEUE_HEAD_INITIALIZER((x).wq_read),	\
-		.wq_write = __WAIT_QUEUE_HEAD_INITIALIZER((x).wq_write),\
+		.wait_list_r = LIST_HEAD_INIT((x).wait_list_r),		\
+		.wait_list_w = LIST_HEAD_INIT((x).wait_list_w),		\
+		__DEBUG_PSRWLOCK_INITIALIZER(x)				\
+		__DEP_MAP_PSRWLOCK_INITIALIZER(x)			\
 	}
 
 #define DEFINE_PSRWLOCK(x, wctx, rctx)					\
@@ -89,4 +135,30 @@ typedef struct psrwlock {
 		BUILD_BUG_ON((~(~0UL << (wctx))) & (rctx));		\
 	}
 
+extern void __psrwlock_init(struct psrwlock *lock, const char *name,
+			    struct lock_class_key *key,
+			    u32 rctx, enum psrw_prio wctx);
+
+/**
+ * psrwlock_is_locked - is the psrwlock locked
+ * @lock: the psrwlock to be queried
+ *
+ * Returns 1 if the psrwlock is locked or if any accessor is waiting for it,
+ * else returns 0.
+ * Also check the per-priority counts to make sure no reader nor writer is
+ * within the per-priority slow path waiting period, where they do not appear
+ * in the fastpath "uc".
+ */
+static inline int psrwlock_is_locked(struct psrwlock *lock)
+{
+	unsigned int i;
+
+	if (atomic_read(&lock->uc))
+		return 1;
+	for (i = 0; i < PSRW_NR_PRIO; i++)
+		if (atomic_long_read(&lock->prio[i]))
+			return 1;
+	return 0;
+}
+
 #endif /* _LINUX_PSRWLOCK_TYPES_H */
diff --git a/include/linux/psrwlock.h b/include/linux/psrwlock.h
index 415ff39..f65ef33 100644
--- a/include/linux/psrwlock.h
+++ b/include/linux/psrwlock.h
@@ -19,7 +19,8 @@
 
 #include <linux/hardirq.h>
 #include <linux/bottom_half.h>
-#include <linux/wait.h>
+#include <linux/list.h>
+#include <linux/linkage.h>
 #include <linux/psrwlock-types.h>
 
 #include <asm/atomic.h>
@@ -204,254 +205,17 @@ static inline void write_context_enable(enum psrw_prio wctx, u32 rctx)
  * barrier making sure the slow path variable writes and the UC_WQ_ACTIVE flag
  * read are done in this order (either a smp_mb() or a atomic_sub_return()).
  */
-static inline void psrwlock_preempt_check(unsigned int uc, psrwlock_t *rwlock)
+static __always_inline void psrwlock_preempt_check(unsigned int uc,
+						   psrwlock_t *rwlock)
 {
 	if (unlikely(uc & UC_WQ_ACTIVE))
 		psrwlock_wakeup(uc, rwlock);
 }
 
-
-/*
- * API
- */
-
-/* Reader lock */
-
-/*
- * many readers, from irq/softirq/non preemptable and preemptable thread
- * context. Protects against writers.
- *
- * Read lock fastpath :
- *
- * A cmpxchg is used here and _not_ a simple add because a lower-priority reader
- * could block the writer while it is waiting for readers to clear the
- * uncontended path. This would happen if, for instance, the reader gets
- * interrupted between the add and the moment it gets to the slow path.
- */
-
-/*
- * Called from any context.
- * Statically check for preemptable writer to compile-out the check if all the
- * contexts accessing the lock are non-preemptable.
- */
-static inline void psread_unlock(psrwlock_t *rwlock,
-		enum psrw_prio wctx, u32 rctx)
-{
-	unsigned int uc = atomic_sub_return(UC_READER_OFFSET, &rwlock->uc);
-	if (wctx == PSRW_PRIO_P || (rctx & PSR_PTHREAD))
-		psrwlock_preempt_check(uc, rwlock);
-}
-
-/*
- * Called from interrupt disabled or interrupt context.
- */
-static inline void psread_lock_irq(psrwlock_t *rwlock,
-		enum psrw_prio wctx, u32 rctx)
-{
-	unsigned int uc;
-
-	BUILD_BUG_ON(!(rctx & PSR_IRQ));
-	uc = atomic_cmpxchg(&rwlock->uc, 0, UC_READER_OFFSET);
-	if (likely(!uc))
-		return;
-	psread_lock_slow_irq(uc, rwlock);
-}
-
-static inline int psread_trylock_irq(psrwlock_t *rwlock,
-		enum psrw_prio wctx, u32 rctx)
-{
-	unsigned int uc;
-
-	BUILD_BUG_ON(!(rctx & PSR_IRQ));
-	uc = atomic_cmpxchg(&rwlock->uc, 0, UC_READER_OFFSET);
-	if (likely(!uc))
-		return 1;
-	return psread_trylock_slow_irq(uc, rwlock);
-}
-
-/*
- * Called from softirq context.
- */
-
-static inline void psread_lock_bh(psrwlock_t *rwlock,
-		enum psrw_prio wctx, u32 rctx)
-{
-	unsigned int uc;
-
-	BUILD_BUG_ON(!(rctx & PSR_BH));
-	uc = atomic_cmpxchg(&rwlock->uc, 0, UC_READER_OFFSET);
-	if (likely(!uc))
-		return;
-	psread_lock_slow_bh(uc, rwlock);
-}
-
-static inline int psread_trylock_bh(psrwlock_t *rwlock,
-		enum psrw_prio wctx, u32 rctx)
-{
-	unsigned int uc;
-
-	BUILD_BUG_ON(!(rctx & PSR_BH));
-	uc = atomic_cmpxchg(&rwlock->uc, 0, UC_READER_OFFSET);
-	if (likely(!uc))
-		return 1;
-	return psread_trylock_slow_bh(uc, rwlock);
-}
-
-
-/*
- * Called from non-preemptable thread context.
- */
-
-static inline void psread_lock_inatomic(psrwlock_t *rwlock,
-		enum psrw_prio wctx, u32 rctx)
-{
-	unsigned int uc;
-
-	BUILD_BUG_ON(!(rctx & PSR_NPTHREAD));
-	uc = atomic_cmpxchg(&rwlock->uc, 0, UC_READER_OFFSET);
-	if (likely(!uc))
-		return;
-	psread_lock_slow_inatomic(uc, rwlock);
-}
-
-static inline int psread_trylock_inatomic(psrwlock_t *rwlock,
-		enum psrw_prio wctx, u32 rctx)
-{
-	unsigned int uc;
-
-	BUILD_BUG_ON(!(rctx & PSR_NPTHREAD));
-	uc = atomic_cmpxchg(&rwlock->uc, 0, UC_READER_OFFSET);
-	if (likely(!uc))
-		return 1;
-	return psread_trylock_slow_inatomic(uc, rwlock);
-}
-
-
-/*
- * Called from preemptable thread context.
- */
-
-static inline void psread_lock(psrwlock_t *rwlock,
-		enum psrw_prio wctx, u32 rctx)
-{
-	unsigned int uc;
-
-	BUILD_BUG_ON(!(rctx & PSR_PTHREAD));
-	uc = atomic_cmpxchg(&rwlock->uc, 0, UC_READER_OFFSET);
-	if (likely(!uc))
-		return;
-	psread_lock_slow(uc, rwlock);
-}
-
-static inline int psread_lock_interruptible(psrwlock_t *rwlock,
-		enum psrw_prio wctx, u32 rctx)
-{
-	unsigned int uc;
-
-	BUILD_BUG_ON(!(rctx & PSR_PTHREAD));
-	uc = atomic_cmpxchg(&rwlock->uc, 0, UC_READER_OFFSET);
-	if (likely(!uc))
-		return 0;
-	return psread_lock_interruptible_slow(uc, rwlock);
-}
-
-static inline int psread_trylock(psrwlock_t *rwlock,
-		enum psrw_prio wctx, u32 rctx)
-{
-	unsigned int uc;
-
-	BUILD_BUG_ON(!(rctx & PSR_PTHREAD));
-	uc = atomic_cmpxchg(&rwlock->uc, 0, UC_READER_OFFSET);
-	if (likely(!uc))
-		return 1;
-	return psread_trylock_slow(uc, rwlock);
-}
-
-
-/* Writer Lock */
-
-/*
- * ctx is the context map showing which contexts can take the read lock and
- * which context is using the write lock.
- *
- * Write lock use example, where the lock is used by readers in interrupt,
- * preemptable context and non-preemptable context. The writer lock is taken in
- * preemptable context.
- *
- * static DEFINE_PSRWLOCK(lock, PSRW_PRIO_P, PSR_IRQ | PSR_PTHREAD);
- * CHECK_PSRWLOCK_MAP(lock, PSRW_PRIO_P, PSR_IRQ | PSR_PTHREAD);
- *
- *  pswrite_lock(&lock, PSRW_PRIO_P, PSR_IRQ | PSR_PTHREAD);
- *  ...
- *  pswrite_unlock(&lock, PSRW_PRIO_P, PSR_IRQ | PSR_PTHREAD);
- */
-static inline
-void pswrite_lock(psrwlock_t *rwlock, enum psrw_prio wctx, u32 rctx)
-{
-	unsigned int uc;
-
-	write_context_disable(wctx, rctx);
-	/* no other reader nor writer present, try to take the lock */
-	uc = atomic_cmpxchg(&rwlock->uc, 0, UC_WRITER);
-	if (likely(!uc))
-		return;
-	else
-		pswrite_lock_slow(uc, rwlock);
-}
-
-static inline
-int pswrite_lock_interruptible(psrwlock_t *rwlock,
-		enum psrw_prio wctx, u32 rctx)
-{
-	unsigned int uc;
-
-	write_context_disable(wctx, rctx);
-	/* no other reader nor writer present, try to take the lock */
-	uc = atomic_cmpxchg(&rwlock->uc, 0, UC_WRITER);
-	if (likely(!uc))
-		return 0;
-	else
-		return pswrite_lock_interruptible_slow(uc, rwlock);
-}
-
-static inline
-int pswrite_trylock(psrwlock_t *rwlock, enum psrw_prio wctx, u32 rctx)
-{
-	unsigned int uc;
-
-	write_context_disable(wctx, rctx);
-	/* no other reader nor writer present, try to take the lock */
-	uc = atomic_cmpxchg(&rwlock->uc, 0, UC_WRITER);
-	if (likely(!uc))
-		return 1;
-	else
-		return pswrite_trylock_slow(uc, rwlock);
-}
-
-static inline
-void pswrite_unlock(psrwlock_t *rwlock, enum psrw_prio wctx, u32 rctx)
-{
-	unsigned int uc;
-
-	/*
-	 * atomic_cmpxchg makes sure we commit the data before reenabling
-	 * the lock. Will take the slow path if there are active readers, if
-	 * UC_SLOW_WRITER is set or if there are threads in the wait queue.
-	 */
-	uc = atomic_cmpxchg(&rwlock->uc, UC_WRITER, 0);
-	if (likely(uc == UC_WRITER)) {
-		write_context_enable(wctx, rctx);
-		/*
-		 * no need to check preempt because all wait queue masks
-		 * were 0. An active wait queue would trigger the slow path.
-		 */
-		return;
-	}
-	/*
-	 * Go through the slow unlock path to check if we must clear the
-	 * UC_SLOW_WRITER bit.
-	 */
-	pswrite_unlock_slow(uc, rwlock);
-}
+#ifdef CONFIG_DEBUG_PSRWLOCK
+# include <linux/psrwlock-debug-api.h>
+#else
+# include <linux/psrwlock-api.h>
+#endif
 
 #endif /* _LINUX_PSRWLOCK_H */
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 30c8419..8453c63 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1378,6 +1378,9 @@ struct task_struct {
 	/* mutex deadlock detection */
 	struct mutex_waiter *blocked_on;
 #endif
+#ifdef CONFIG_DEBUG_PSRWLOCK
+	struct psrwlock_waiter *psrwlock_blocked_on;
+#endif
 #ifdef CONFIG_TRACE_IRQFLAGS
 	unsigned int irq_events;
 	unsigned long hardirq_enable_ip;
diff --git a/lib/Kconfig.debug b/lib/Kconfig.debug
index 0d03313..e29edc5 100644
--- a/lib/Kconfig.debug
+++ b/lib/Kconfig.debug
@@ -444,11 +444,19 @@ config DEBUG_MUTEXES
 	 This feature allows mutex semantics violations to be detected and
 	 reported.
 
+config DEBUG_PSRWLOCK
+	bool "Priority-Sifting Reader-Writer Locks: basic checks"
+	depends on DEBUG_KERNEL
+	help
+	 This feature allows psrwlock semantics violations to be detected and
+	 reported.
+
 config DEBUG_LOCK_ALLOC
 	bool "Lock debugging: detect incorrect freeing of live locks"
 	depends on DEBUG_KERNEL && TRACE_IRQFLAGS_SUPPORT && STACKTRACE_SUPPORT && LOCKDEP_SUPPORT
 	select DEBUG_SPINLOCK
 	select DEBUG_MUTEXES
+	select DEBUG_PSRWLOCK
 	select LOCKDEP
 	help
 	 This feature will check whether any held lock (spinlock, rwlock,
@@ -464,6 +472,7 @@ config PROVE_LOCKING
 	select LOCKDEP
 	select DEBUG_SPINLOCK
 	select DEBUG_MUTEXES
+	select DEBUG_PSRWLOCK
 	select DEBUG_LOCK_ALLOC
 	default n
 	help
@@ -526,6 +535,7 @@ config LOCK_STAT
 	select LOCKDEP
 	select DEBUG_SPINLOCK
 	select DEBUG_MUTEXES
+	select DEBUG_PSRWLOCK
 	select DEBUG_LOCK_ALLOC
 	default n
 	help
diff --git a/lib/Makefile b/lib/Makefile
index b78777b..a2d8ec3 100644
--- a/lib/Makefile
+++ b/lib/Makefile
@@ -48,6 +48,7 @@ obj-$(CONFIG_DEBUG_OBJECTS) += debugobjects.o
 
 obj-y += psrwlock.o
 obj-$(CONFIG_PSRWLOCK_LATENCY_TEST) += psrwlock-latency-trace.o
+obj-$(CONFIG_DEBUG_PSRWLOCK) += psrwlock-debug.o
 
 ifneq ($(CONFIG_HAVE_DEC_LOCK),y)
   lib-y += dec_and_lock.o
diff --git a/lib/psrwlock-debug.c b/lib/psrwlock-debug.c
new file mode 100644
index 0000000..064e62d
--- /dev/null
+++ b/lib/psrwlock-debug.c
@@ -0,0 +1,131 @@
+/*
+ * Priority Sifting Reader-Writer Lock Debug
+ *
+ * Inspired from kernel/mutex-debug.c.
+ *
+ * Copyright 2008 Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
+ */
+
+#include <linux/psrwlock.h>
+#include <linux/lockdep.h>
+#include <linux/sched.h>
+#include <linux/poison.h>
+#include <linux/module.h>
+
+#include "psrwlock-debug.h"
+
+/*
+ * Must be called with lock->wait_lock held.
+ */
+void debug_psrwlock_set_owner(struct psrwlock *lock,
+			      struct thread_info *new_owner)
+{
+	lock->owner = new_owner;
+}
+
+void debug_psrwlock_lock_common(struct psrwlock *lock,
+				struct psrwlock_waiter *waiter)
+{
+	memset(waiter, PSRWLOCK_DEBUG_INIT, sizeof(*waiter));
+	waiter->magic = waiter;
+	INIT_LIST_HEAD(&waiter->list);
+}
+
+void debug_psrwlock_wake_waiter(struct psrwlock *lock,
+				struct psrwlock_waiter *waiter)
+{
+	SMP_DEBUG_LOCKS_WARN_ON(!(atomic_read(&lock->ws) & WS_WQ_MUTEX));
+	DEBUG_LOCKS_WARN_ON(list_empty(&lock->wait_list_r) &&
+			    list_empty(&lock->wait_list_w));
+	DEBUG_LOCKS_WARN_ON(waiter->magic != waiter);
+	DEBUG_LOCKS_WARN_ON(list_empty(&waiter->list));
+}
+
+void debug_psrwlock_free_waiter(struct psrwlock_waiter *waiter)
+{
+	DEBUG_LOCKS_WARN_ON(!list_empty(&waiter->list));
+	memset(waiter, PSRWLOCK_DEBUG_FREE, sizeof(*waiter));
+}
+
+void debug_psrwlock_add_waiter(struct psrwlock *lock,
+			       struct psrwlock_waiter *waiter,
+                               struct thread_info *ti)
+{
+	SMP_DEBUG_LOCKS_WARN_ON(!(atomic_read(&lock->ws) & WS_WQ_MUTEX));
+
+	/* Mark the current thread as blocked on the lock: */
+	ti->task->psrwlock_blocked_on = waiter;
+	waiter->lock = lock;
+}
+
+void psrwlock_remove_waiter(struct psrwlock *lock,
+			    struct psrwlock_waiter *waiter,
+			    struct thread_info *ti)
+{
+	DEBUG_LOCKS_WARN_ON(list_empty(&waiter->list));
+	DEBUG_LOCKS_WARN_ON(waiter->task != ti->task);
+	DEBUG_LOCKS_WARN_ON(ti->task->psrwlock_blocked_on != waiter);
+	ti->task->psrwlock_blocked_on = NULL;
+
+	list_del_init(&waiter->list);
+	waiter->task = NULL;
+}
+
+void debug_psrwlock_unlock(struct psrwlock *lock, int rw)
+{
+	if (unlikely(!debug_locks))
+		return;
+
+	DEBUG_LOCKS_WARN_ON(lock->magic != lock);
+	if (rw)	/* read */
+		DEBUG_LOCKS_WARN_ON(lock->owner != (void *)-1UL);
+	else
+		DEBUG_LOCKS_WARN_ON(lock->owner != current_thread_info());
+	DEBUG_LOCKS_WARN_ON(!lock->wait_list_r.prev && !lock->wait_list_r.next);
+	DEBUG_LOCKS_WARN_ON(!lock->wait_list_w.prev && !lock->wait_list_w.next);
+}
+
+void debug_psrwlock_init(struct psrwlock *lock, const char *name,
+		      struct lock_class_key *key)
+{
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	/*
+	 * Make sure we are not reinitializing a held lock:
+	 */
+	debug_check_no_locks_freed((void *)lock, sizeof(*lock));
+	lockdep_init_map(&lock->dep_map, name, key, 0);
+#endif
+	lock->owner = NULL;
+	lock->magic = lock;
+}
+
+/***
+ * psrwlock_destroy - mark a psrwlock unusable
+ * @lock: the psrwlock to be destroyed
+ *
+ * This function marks the psrwlock uninitialized, and any subsequent
+ * use of the lock is forbidden. The lock must not be locked when
+ * this function is called.
+ */
+void psrwlock_destroy(struct psrwlock *lock)
+{
+	DEBUG_LOCKS_WARN_ON(psrwlock_is_locked(lock));
+	lock->magic = NULL;
+}
+
+EXPORT_SYMBOL(psrwlock_destroy);
+
+void psread_unlock(psrwlock_t *rwlock, enum psrw_prio wctx, u32 rctx)
+{
+	int nested = 1;	/* TODO support nested = 0 */
+	unsigned int uc;
+
+	psrwlock_release(&rwlock->dep_map, nested, _RET_IP_);
+	debug_psrwlock_unlock(rwlock, 1);
+	debug_psrwlock_clear_owner(rwlock);
+	uc = atomic_sub_return(UC_READER_OFFSET, &rwlock->uc);
+	if (wctx == PSRW_PRIO_P || (rctx & PSR_PTHREAD))
+		psrwlock_preempt_check(uc, rwlock);
+}
+
+EXPORT_SYMBOL(psread_unlock);
diff --git a/lib/psrwlock-debug.h b/lib/psrwlock-debug.h
new file mode 100644
index 0000000..e5b0173
--- /dev/null
+++ b/lib/psrwlock-debug.h
@@ -0,0 +1,31 @@
+#ifndef _LIB_PSRWLOCK_H
+#define _LIB_PSRWLOCK_H
+
+/*
+ * This must be called with lock->wait_lock held.
+ */
+extern void
+debug_psrwlock_set_owner(struct psrwlock *lock,
+			 struct thread_info *new_owner);
+
+static inline void debug_psrwlock_clear_owner(struct psrwlock *lock)
+{
+	lock->owner = NULL;
+}
+
+extern void debug_psrwlock_lock_common(struct psrwlock *lock,
+				       struct psrwlock_waiter *waiter);
+extern void debug_psrwlock_wake_waiter(struct psrwlock *lock,
+				       struct psrwlock_waiter *waiter);
+extern void debug_psrwlock_free_waiter(struct psrwlock_waiter *waiter);
+extern void debug_psrwlock_add_waiter(struct psrwlock *lock,
+				      struct psrwlock_waiter *waiter,
+				      struct thread_info *ti);
+extern void psrwlock_remove_waiter(struct psrwlock *lock,
+				   struct psrwlock_waiter *waiter,
+				   struct thread_info *ti);
+extern void debug_psrwlock_unlock(struct psrwlock *lock, int rw);
+extern void debug_psrwlock_init(struct psrwlock *lock, const char *name,
+				struct lock_class_key *key);
+
+#endif /* _LIB_PSRWLOCK_H */
diff --git a/lib/psrwlock.c b/lib/psrwlock.c
index f3f1062..acf4bf6 100644
--- a/lib/psrwlock.c
+++ b/lib/psrwlock.c
@@ -11,12 +11,20 @@
  */
 
 #include <linux/psrwlock.h>
-#include <linux/wait.h>
+#include <linux/list.h>
+#include <linux/linkage.h>
 #include <linux/freezer.h>
 #include <linux/module.h>
+#include <linux/debug_locks.h>
 
 #include <asm/processor.h>
 
+#ifdef CONFIG_DEBUG_PSRWLOCK
+# include "psrwlock-debug.h"
+#else
+# include "psrwlock.h"
+#endif
+
 #ifdef WBIAS_RWLOCK_DEBUG
 #define printk_dbg printk
 #else
@@ -41,7 +49,37 @@ enum v_type {
 static int rwlock_wait(void *vptr, psrwlock_t *rwlock,
 		unsigned long mask, unsigned long test_mask,
 		unsigned long full_mask, int check_full_mask,
-		enum v_type vtype, enum lock_type ltype, long state);
+		enum v_type vtype, enum lock_type ltype, long state,
+		unsigned long ip);
+
+/***
+ * psrwlock_init - initialize the psrwlock
+ * @lock: the psrwlock to be initialized
+ * @key: the lock_class_key for the class; used by mutex lock debugging
+ *
+ * Initialize the psrwlock to unlocked state.
+ *
+ * It is not allowed to initialize an already locked psrwlock.
+ */
+void
+__psrwlock_init(struct psrwlock *lock, const char *name,
+		struct lock_class_key *key, u32 rctx, enum psrw_prio wctx)
+{
+	unsigned int i;
+
+	atomic_set(&lock->uc, 0);
+	atomic_set(&lock->ws, 0);
+	for (i = 0; i < PSRW_NR_PRIO; i++)
+		atomic_set(&lock->prio[i], 0);
+	lock->rctx_bitmap = rctx;
+	lock->wctx = wctx;
+	INIT_LIST_HEAD(&lock->wait_list_r);
+	INIT_LIST_HEAD(&lock->wait_list_w);
+
+	debug_psrwlock_init(lock, name, key);
+}
+
+EXPORT_SYMBOL(__psrwlock_init);
 
 /*
  * Lock out a specific uncontended execution context from the read lock. Wait
@@ -59,7 +97,8 @@ static int _pswrite_lock_ctx_wait_sub(void *v_inout,
 		unsigned long wait_mask, unsigned long test_mask,
 		unsigned long full_mask, long offset,
 		enum v_type vtype, enum lock_type ltype,
-		enum preempt_type ptype, int trylock, long state)
+		enum preempt_type ptype, int trylock, long state,
+		unsigned long ip)
 {
 	long try = NR_PREEMPT_BUSY_LOOPS;
 	unsigned long newv;
@@ -77,12 +116,13 @@ static int _pswrite_lock_ctx_wait_sub(void *v_inout,
 
 	for (;;) {
 		if (v & wait_mask || (v & test_mask) >= full_mask) {
+			lock_contended(&rwlock->dep_map, ip);
 			if (trylock)
 				return 0;
 			if (ptype == PSRW_PREEMPT && unlikely(!(--try))) {
 				ret = rwlock_wait(vptr, rwlock, wait_mask,
 					test_mask, full_mask, 1,
-					vtype, ltype, state);
+					vtype, ltype, state, ip);
 				if (ret < 0)
 					return ret;
 				try = NR_PREEMPT_BUSY_LOOPS;
@@ -129,7 +169,8 @@ static int _pswrite_lock_ctx_wait_sub(void *v_inout,
 static int _pswrite_lock_ctx_wait(unsigned long v_in, void *vptr,
 		psrwlock_t *rwlock, unsigned long wait_mask,
 		enum v_type vtype, enum lock_type ltype,
-		enum preempt_type ptype, int trylock, long state)
+		enum preempt_type ptype, int trylock, long state,
+		unsigned long ip)
 {
 	int try = NR_PREEMPT_BUSY_LOOPS;
 	unsigned long v = v_in;
@@ -140,10 +181,11 @@ static int _pswrite_lock_ctx_wait(unsigned long v_in, void *vptr,
 	smp_mb();
 	while (v & wait_mask) {
 		if (ptype == PSRW_PREEMPT && unlikely(!(--try))) {
+			lock_contended(&rwlock->dep_map, ip);
 			if (trylock)
 				return 0;
 			ret = rwlock_wait(vptr, rwlock, wait_mask, 0, 0, 0,
-				vtype, ltype, state);
+				vtype, ltype, state, ip);
 			if (ret < 0)
 				return ret;
 			try = NR_PREEMPT_BUSY_LOOPS;
@@ -171,9 +213,11 @@ static int _pswrite_lock_ctx_wait(unsigned long v_in, void *vptr,
 static int rwlock_wait(void *vptr, psrwlock_t *rwlock,
 		unsigned long mask, unsigned long test_mask,
 		unsigned long full_mask, int check_full_mask,
-		enum v_type vtype, enum lock_type ltype, long state)
+		enum v_type vtype, enum lock_type ltype, long state,
+		unsigned long ip)
 {
-	DECLARE_WAITQUEUE(psrwlock_wq, current);
+	struct task_struct *task = current;
+	struct psrwlock_waiter waiter;
 	unsigned long v;
 	int wq_active, ws, ret = 1;
 
@@ -184,12 +228,15 @@ static int rwlock_wait(void *vptr, psrwlock_t *rwlock,
 	ws = atomic_read(&rwlock->ws);
 	_pswrite_lock_ctx_wait_sub(&ws, &rwlock->ws, rwlock,
 		0, WS_WQ_MUTEX, WS_WQ_MUTEX, WS_WQ_MUTEX,
-		V_INT, ltype, PSRW_NON_PREEMPT, 0, TASK_UNINTERRUPTIBLE);
+		V_INT, ltype, PSRW_NON_PREEMPT, 0, TASK_UNINTERRUPTIBLE, ip);
+
+	debug_psrwlock_lock_common(rwlock, &waiter);
+
 	/*
 	 * Got the waitqueue mutex, get into the wait queue.
 	 */
-	wq_active = waitqueue_active(&rwlock->wq_read)
-			|| waitqueue_active(&rwlock->wq_write);
+	wq_active = !list_empty(&rwlock->wait_list_r)
+			|| !list_empty(&rwlock->wait_list_w);
 	if (!wq_active)
 		atomic_add(UC_WQ_ACTIVE, &rwlock->uc);
 	/* Set the UC_WQ_ACTIVE flag before testing the condition. */
@@ -209,21 +256,26 @@ static int rwlock_wait(void *vptr, psrwlock_t *rwlock,
 	/*
 	 * got a signal ? (not done in TASK_UNINTERRUPTIBLE)
 	 */
-	if (unlikely(signal_pending_state(state, current))) {
+	if (unlikely(signal_pending_state(state, task))) {
 		ret = -EINTR;
 		goto skip_sleep;
 	}
 
+	debug_psrwlock_add_waiter(rwlock, &waiter, task_thread_info(task));
+
 	/*
+	 * Add waiting tasks to the end of the waitqueue (FIFO):
 	 * Only one thread will be woken up at a time.
 	 */
 	if (ltype == PSRW_WRITE)
-		add_wait_queue_exclusive_locked(&rwlock->wq_write,
-			&psrwlock_wq);
+		list_add_tail(&waiter.list, &rwlock->wait_list_w);
 	else
-		__add_wait_queue(&rwlock->wq_read, &psrwlock_wq);
-	__set_current_state(state);
+		list_add_tail(&waiter.list, &rwlock->wait_list_r);
+	waiter.task = task;
+	__set_task_state(task, state);
 	smp_mb();	/* Insure memory ordering when clearing the mutex. */
+
+
 	atomic_sub(WS_WQ_MUTEX, &rwlock->ws);
 	psrwlock_irq_enable();
 
@@ -237,20 +289,18 @@ static int rwlock_wait(void *vptr, psrwlock_t *rwlock,
 	ws = atomic_read(&rwlock->ws);
 	_pswrite_lock_ctx_wait_sub(&ws, &rwlock->ws, rwlock,
 		0, WS_WQ_MUTEX, WS_WQ_MUTEX, WS_WQ_MUTEX,
-		V_INT, ltype, PSRW_NON_PREEMPT, 0, TASK_UNINTERRUPTIBLE);
-	__set_current_state(TASK_RUNNING);
-	if (ltype == PSRW_WRITE)
-		remove_wait_queue_locked(&rwlock->wq_write, &psrwlock_wq);
-	else
-		remove_wait_queue_locked(&rwlock->wq_read, &psrwlock_wq);
+		V_INT, ltype, PSRW_NON_PREEMPT, 0, TASK_UNINTERRUPTIBLE, ip);
+	__set_task_state(task, TASK_RUNNING);
+	psrwlock_remove_waiter(rwlock, &waiter, task_thread_info(task));
 skip_sleep:
-	wq_active = waitqueue_active(&rwlock->wq_read)
-			|| waitqueue_active(&rwlock->wq_write);
+	wq_active = !list_empty(&rwlock->wait_list_r)
+			|| !list_empty(&rwlock->wait_list_w);
 	if (!wq_active)
 		atomic_sub(UC_WQ_ACTIVE, &rwlock->uc);
 	smp_mb();	/* Insure memory ordering when clearing the mutex. */
 	atomic_sub(WS_WQ_MUTEX, &rwlock->ws);
 	psrwlock_irq_enable();
+	debug_psrwlock_free_waiter(&waiter);
 	return ret;
 }
 
@@ -258,6 +308,13 @@ skip_sleep:
  * Reader lock
  */
 
+#ifdef CONFIG_DEBUG_PSRWLOCK
+static int _psread_lock_fast_check(unsigned int uc, psrwlock_t *rwlock,
+	unsigned int uc_rmask)
+{
+	return 0;
+}
+#else
 /*
  * _psread_lock_fast_check
  *
@@ -297,15 +354,18 @@ static int _psread_lock_fast_check(unsigned int uc, psrwlock_t *rwlock,
 	}
 	return 0;
 }
+#endif
 
 int __psread_lock_slow(psrwlock_t *rwlock,
 		unsigned int uc_rmask, atomic_long_t *vptr,
-		int trylock, enum preempt_type ptype, long state)
+		int trylock, enum preempt_type ptype, long state,
+		unsigned long ip)
 {
 	u32 rctx = rwlock->rctx_bitmap;
 	unsigned long v;
 	unsigned int uc;
 	int ret;
+	int subclass = SINGLE_DEPTH_NESTING;	/* TODO : parameter */
 
 	if (unlikely(in_irq() || irqs_disabled()))
 		WARN_ON_ONCE(!(rctx & PSR_IRQ) || ptype != PSRW_NON_PREEMPT);
@@ -325,13 +385,15 @@ int __psread_lock_slow(psrwlock_t *rwlock,
 				|| ptype != PSRW_PREEMPT));
 #endif
 
+	psrwlock_acquire_read(&rwlock->dep_map, subclass, trylock, ip);
+
 	/*
 	 * A cmpxchg read uc, which implies strict ordering.
 	 */
 	v = atomic_long_read(vptr);
 	ret = _pswrite_lock_ctx_wait_sub(&v, vptr, rwlock,
 		CTX_WMASK, CTX_RMASK, CTX_RMASK, CTX_ROFFSET,
-		V_LONG, PSRW_READ, ptype, trylock, state);
+		V_LONG, PSRW_READ, ptype, trylock, state, ip);
 	if (unlikely(ret < 1))
 		goto fail;
 
@@ -356,7 +418,7 @@ int __psread_lock_slow(psrwlock_t *rwlock,
 	uc = atomic_read(&rwlock->uc);
 	ret = _pswrite_lock_ctx_wait_sub(&uc, &rwlock->uc, rwlock,
 		UC_WRITER, UC_READER_MASK, uc_rmask, UC_READER_OFFSET,
-		V_INT, PSRW_READ, ptype, trylock, state);
+		V_INT, PSRW_READ, ptype, trylock, state, ip);
 	/*
 	 * _pswrite_lock_ctx_wait_sub has a memory barrier
 	 */
@@ -368,6 +430,9 @@ int __psread_lock_slow(psrwlock_t *rwlock,
 	if (unlikely(ret < 1))
 		goto fail_preempt;
 
+	lock_acquired(&rwlock->dep_map, ip);
+	debug_psrwlock_set_owner(rwlock, (void *)-1UL);	/* -1 : all readers */
+
 	/* Success */
 	return 1;
 
@@ -379,6 +444,7 @@ fail_preempt:
 	psrwlock_preempt_check(uc, rwlock);
 fail:
 	cpu_relax();
+	psrwlock_release(&rwlock->dep_map, 1, ip);
 	return ret;
 
 }
@@ -407,7 +473,7 @@ void _psread_lock_slow_irq(unsigned int uc, psrwlock_t *rwlock)
 		return;
 	__psread_lock_slow(rwlock, UC_HARDIRQ_READER_MASK,
 			&rwlock->prio[PSRW_PRIO_IRQ],
-			0, PSRW_NON_PREEMPT, TASK_UNINTERRUPTIBLE);
+			0, PSRW_NON_PREEMPT, TASK_UNINTERRUPTIBLE, _RET_IP_);
 }
 EXPORT_SYMBOL(_psread_lock_slow_irq);
 
@@ -421,7 +487,7 @@ void _psread_lock_slow_bh(unsigned int uc, psrwlock_t *rwlock)
 		return;
 	__psread_lock_slow(rwlock, UC_SOFTIRQ_READER_MASK,
 			&rwlock->prio[PSRW_PRIO_BH],
-			0, PSRW_NON_PREEMPT, TASK_UNINTERRUPTIBLE);
+			0, PSRW_NON_PREEMPT, TASK_UNINTERRUPTIBLE, _RET_IP_);
 }
 EXPORT_SYMBOL(_psread_lock_slow_bh);
 
@@ -435,7 +501,7 @@ void _psread_lock_slow_inatomic(unsigned int uc, psrwlock_t *rwlock)
 		return;
 	__psread_lock_slow(rwlock, UC_NPTHREAD_READER_MASK,
 			&rwlock->prio[PSRW_PRIO_NP],
-			0, PSRW_NON_PREEMPT, TASK_UNINTERRUPTIBLE);
+			0, PSRW_NON_PREEMPT, TASK_UNINTERRUPTIBLE, _RET_IP_);
 }
 EXPORT_SYMBOL(_psread_lock_slow_inatomic);
 
@@ -449,7 +515,7 @@ void _psread_lock_slow(unsigned int uc, psrwlock_t *rwlock)
 		return;
 	__psread_lock_slow(rwlock, UC_PTHREAD_READER_MASK,
 			&rwlock->prio[PSRW_PRIO_P],
-			0, PSRW_PREEMPT, TASK_UNINTERRUPTIBLE);
+			0, PSRW_PREEMPT, TASK_UNINTERRUPTIBLE, _RET_IP_);
 }
 EXPORT_SYMBOL(_psread_lock_slow);
 
@@ -463,7 +529,7 @@ int _psread_lock_interruptible_slow(unsigned int uc, psrwlock_t *rwlock)
 		return 0;
 	ret = __psread_lock_slow(rwlock, UC_PTHREAD_READER_MASK,
 			&rwlock->prio[PSRW_PRIO_P],
-			0, PSRW_PREEMPT, TASK_INTERRUPTIBLE);
+			0, PSRW_PREEMPT, TASK_INTERRUPTIBLE, _RET_IP_);
 	if (ret < 1)
 		return ret;
 	return 0;
@@ -480,7 +546,7 @@ int _psread_trylock_slow_irq(unsigned int uc, psrwlock_t *rwlock)
 		return 1;
 	return __psread_lock_slow(rwlock, UC_HARDIRQ_READER_MASK,
 			&rwlock->prio[PSRW_PRIO_IRQ],
-			1, PSRW_NON_PREEMPT, TASK_UNINTERRUPTIBLE);
+			1, PSRW_NON_PREEMPT, TASK_UNINTERRUPTIBLE, _RET_IP_);
 }
 EXPORT_SYMBOL(_psread_trylock_slow_irq);
 
@@ -494,7 +560,7 @@ int _psread_trylock_slow_bh(unsigned int uc, psrwlock_t *rwlock)
 		return 1;
 	return __psread_lock_slow(rwlock, UC_SOFTIRQ_READER_MASK,
 			&rwlock->prio[PSRW_PRIO_BH],
-			1, PSRW_NON_PREEMPT, TASK_UNINTERRUPTIBLE);
+			1, PSRW_NON_PREEMPT, TASK_UNINTERRUPTIBLE, _RET_IP_);
 }
 EXPORT_SYMBOL(_psread_trylock_slow_bh);
 
@@ -508,7 +574,7 @@ int _psread_trylock_slow_inatomic(unsigned int uc, psrwlock_t *rwlock)
 		return 1;
 	return __psread_lock_slow(rwlock, UC_NPTHREAD_READER_MASK,
 			&rwlock->prio[PSRW_PRIO_NP],
-			1, PSRW_NON_PREEMPT, TASK_UNINTERRUPTIBLE);
+			1, PSRW_NON_PREEMPT, TASK_UNINTERRUPTIBLE, _RET_IP_);
 }
 EXPORT_SYMBOL(_psread_trylock_slow_inatomic);
 
@@ -522,7 +588,7 @@ int _psread_trylock_slow(unsigned int uc, psrwlock_t *rwlock)
 		return 1;
 	return __psread_lock_slow(rwlock, UC_PTHREAD_READER_MASK,
 			&rwlock->prio[PSRW_PRIO_P],
-			1, PSRW_PREEMPT, TASK_UNINTERRUPTIBLE);
+			1, PSRW_PREEMPT, TASK_UNINTERRUPTIBLE, _RET_IP_);
 }
 EXPORT_SYMBOL(_psread_trylock_slow);
 
@@ -531,7 +597,7 @@ EXPORT_SYMBOL(_psread_trylock_slow);
 
 static int _pswrite_lock_out_context(unsigned int *uc_inout,
 	atomic_long_t *vptr, psrwlock_t *rwlock,
-	enum preempt_type ptype, int trylock, long state)
+	enum preempt_type ptype, int trylock, long state, unsigned long ip)
 {
 	int ret;
 	unsigned long v;
@@ -540,7 +606,7 @@ static int _pswrite_lock_out_context(unsigned int *uc_inout,
 	v = atomic_long_read(vptr);
 	ret = _pswrite_lock_ctx_wait_sub(&v, vptr, rwlock,
 		0, CTX_WMASK, CTX_WMASK, CTX_WOFFSET,
-		V_LONG, PSRW_WRITE, ptype, trylock, state);
+		V_LONG, PSRW_WRITE, ptype, trylock, state, ip);
 	if (unlikely(ret < 1))
 		return ret;
 	/*
@@ -548,14 +614,14 @@ static int _pswrite_lock_out_context(unsigned int *uc_inout,
 	 * removed by next subscription.
 	 */
 	ret = _pswrite_lock_ctx_wait(v, vptr, rwlock,
-		CTX_RMASK, V_LONG, PSRW_WRITE, ptype, trylock, state);
+		CTX_RMASK, V_LONG, PSRW_WRITE, ptype, trylock, state, ip);
 	if (unlikely(ret < 1))
 		goto fail_clean_slow;
 	/* Wait for uncontended readers and writers to unlock */
 	*uc_inout = atomic_read(&rwlock->uc);
 	ret = _pswrite_lock_ctx_wait(*uc_inout, &rwlock->uc, rwlock,
 		UC_WRITER | UC_READER_MASK,
-		V_INT, PSRW_WRITE, ptype, trylock, state);
+		V_INT, PSRW_WRITE, ptype, trylock, state, ip);
 	if (ret < 1)
 		goto fail_clean_slow;
 	return 1;
@@ -566,7 +632,7 @@ fail_clean_slow:
 }
 
 static void writer_count_inc(unsigned int *uc, psrwlock_t *rwlock,
-		enum preempt_type ptype)
+		enum preempt_type ptype, unsigned long ip)
 {
 	unsigned int ws;
 
@@ -578,7 +644,7 @@ static void writer_count_inc(unsigned int *uc, psrwlock_t *rwlock,
 	_pswrite_lock_ctx_wait_sub(&ws, &rwlock->ws, rwlock,
 		WS_COUNT_MUTEX, WS_MASK, WS_MASK,
 		WS_COUNT_MUTEX + WS_OFFSET,
-		V_INT, PSRW_WRITE, ptype, 0, TASK_UNINTERRUPTIBLE);
+		V_INT, PSRW_WRITE, ptype, 0, TASK_UNINTERRUPTIBLE, ip);
 	/* First writer in slow path ? */
 	if ((ws & WS_MASK) == WS_OFFSET) {
 		atomic_add(UC_SLOW_WRITER, &rwlock->uc);
@@ -589,7 +655,7 @@ static void writer_count_inc(unsigned int *uc, psrwlock_t *rwlock,
 }
 
 static void writer_count_dec(unsigned int *uc, psrwlock_t *rwlock,
-		enum preempt_type ptype)
+		enum preempt_type ptype, unsigned long ip)
 {
 	unsigned int ws;
 
@@ -601,7 +667,7 @@ static void writer_count_dec(unsigned int *uc, psrwlock_t *rwlock,
 	_pswrite_lock_ctx_wait_sub(&ws, &rwlock->ws, rwlock,
 		WS_COUNT_MUTEX, WS_COUNT_MUTEX, WS_COUNT_MUTEX,
 		WS_COUNT_MUTEX - WS_OFFSET,
-		V_INT, PSRW_WRITE, ptype, 0, TASK_UNINTERRUPTIBLE);
+		V_INT, PSRW_WRITE, ptype, 0, TASK_UNINTERRUPTIBLE, ip);
 	/* Last writer in slow path ? */
 	if (!(ws & WS_MASK)) {
 		atomic_sub(UC_SLOW_WRITER, &rwlock->uc);
@@ -612,13 +678,15 @@ static void writer_count_dec(unsigned int *uc, psrwlock_t *rwlock,
 }
 
 static int __pswrite_lock_slow_common(unsigned int uc, psrwlock_t *rwlock,
-		int trylock, long state)
+		int trylock, long state, unsigned long ip)
 {
+	struct task_struct *task = current;
 	enum psrw_prio wctx = rwlock->wctx;
 	u32 rctx = rwlock->rctx_bitmap;
 	enum preempt_type ptype;
 	unsigned int ws;
 	int ret;
+	int subclass = SINGLE_DEPTH_NESTING;	/* TODO : parameter */
 
 	write_context_enable(wctx, rctx);
 
@@ -641,14 +709,16 @@ static int __pswrite_lock_slow_common(unsigned int uc, psrwlock_t *rwlock,
 	else
 		ptype = PSRW_NON_PREEMPT;
 
+	psrwlock_acquire(&rwlock->dep_map, subclass, trylock, ip);
+
 	/* Increment the slow path writer count */
-	writer_count_inc(&uc, rwlock, ptype);
+	writer_count_inc(&uc, rwlock, ptype, ip);
 
 	if (rctx & PSR_PTHREAD) {
 		ptype = PSRW_PREEMPT;
 		ret = _pswrite_lock_out_context(&uc,
 			&rwlock->prio[PSRW_PRIO_P], rwlock,
-			ptype, trylock, state);
+			ptype, trylock, state, ip);
 		if (unlikely(ret < 1))
 			goto fail_dec_count;
 	}
@@ -662,7 +732,7 @@ static int __pswrite_lock_slow_common(unsigned int uc, psrwlock_t *rwlock,
 		ptype = PSRW_NON_PREEMPT;
 		ret = _pswrite_lock_out_context(&uc,
 			&rwlock->prio[PSRW_PRIO_NP], rwlock,
-			ptype, trylock, state);
+			ptype, trylock, state, ip);
 		if (unlikely(ret < 1))
 			goto fail_unsub_pthread;
 	}
@@ -674,7 +744,7 @@ static int __pswrite_lock_slow_common(unsigned int uc, psrwlock_t *rwlock,
 		ptype = PSRW_NON_PREEMPT;
 		ret = _pswrite_lock_out_context(&uc,
 			&rwlock->prio[PSRW_PRIO_BH], rwlock,
-			ptype, trylock, state);
+			ptype, trylock, state, ip);
 		if (unlikely(ret < 1))
 			goto fail_unsub_npthread;
 	}
@@ -686,7 +756,7 @@ static int __pswrite_lock_slow_common(unsigned int uc, psrwlock_t *rwlock,
 		ptype = PSRW_NON_PREEMPT;
 		ret = _pswrite_lock_out_context(&uc,
 			&rwlock->prio[PSRW_PRIO_IRQ], rwlock,
-			ptype, trylock, state);
+			ptype, trylock, state, ip);
 		if (unlikely(ret < 1))
 			goto fail_unsub_bh;
 	}
@@ -701,11 +771,14 @@ static int __pswrite_lock_slow_common(unsigned int uc, psrwlock_t *rwlock,
 	ws = atomic_read(&rwlock->ws);
 	ret = _pswrite_lock_ctx_wait_sub(&ws, &rwlock->ws, rwlock,
 		0, WS_LOCK_MUTEX, WS_LOCK_MUTEX, WS_LOCK_MUTEX,
-		V_INT, PSRW_WRITE, ptype, trylock, state);
+		V_INT, PSRW_WRITE, ptype, trylock, state, ip);
 	if (unlikely(ret < 1))
 		goto fail_unsub_irq;
 	/* atomic_cmpxchg orders writes */
 
+	lock_acquired(&rwlock->dep_map, ip);
+	debug_psrwlock_set_owner(rwlock, task_thread_info(task));
+
 	return 1;	/* success */
 
 	/* Failure paths */
@@ -732,9 +805,10 @@ fail_dec_count:
 		ptype = PSRW_PREEMPT;
 	else
 		ptype = PSRW_NON_PREEMPT;
-	writer_count_dec(&uc, rwlock, ptype);
+	writer_count_dec(&uc, rwlock, ptype, ip);
 	psrwlock_preempt_check(uc, rwlock);
 	cpu_relax();
+	psrwlock_release(&rwlock->dep_map, 1, ip);
 	return ret;
 }
 
@@ -745,7 +819,8 @@ fail_dec_count:
  */
 asmregparm void _pswrite_lock_slow(unsigned int uc, psrwlock_t *rwlock)
 {
-	__pswrite_lock_slow_common(uc, rwlock, 0, TASK_UNINTERRUPTIBLE);
+	__pswrite_lock_slow_common(uc, rwlock, 0, TASK_UNINTERRUPTIBLE,
+				   _RET_IP_);
 }
 EXPORT_SYMBOL_GPL(_pswrite_lock_slow);
 
@@ -757,7 +832,8 @@ asmregparm int _pswrite_lock_interruptible_slow(unsigned int uc,
 {
 	int ret;
 
-	ret = __pswrite_lock_slow_common(uc, rwlock, 0, TASK_INTERRUPTIBLE);
+	ret = __pswrite_lock_slow_common(uc, rwlock, 0, TASK_INTERRUPTIBLE,
+				   _RET_IP_);
 	if (ret < 1)
 		return ret;
 	return 0;
@@ -770,7 +846,8 @@ EXPORT_SYMBOL_GPL(_pswrite_lock_interruptible_slow);
 asmregparm
 int _pswrite_trylock_slow(unsigned int uc, psrwlock_t *rwlock)
 {
-	return __pswrite_lock_slow_common(uc, rwlock, 1, TASK_INTERRUPTIBLE);
+	return __pswrite_lock_slow_common(uc, rwlock, 1, TASK_INTERRUPTIBLE,
+				   _RET_IP_);
 }
 EXPORT_SYMBOL_GPL(_pswrite_trylock_slow);
 
@@ -780,6 +857,11 @@ void _pswrite_unlock_slow(unsigned int uc, psrwlock_t *rwlock)
 	enum psrw_prio wctx = rwlock->wctx;
 	u32 rctx = rwlock->rctx_bitmap;
 	enum preempt_type ptype;
+	int nested = 1;	/* FIXME : allow nested = 0 ? */
+
+	mutex_release(&rwlock->dep_map, nested, _RET_IP_);
+	debug_psrwlock_unlock(rwlock, 0);
+	debug_psrwlock_clear_owner(rwlock);
 
 	/*
 	 * We get here either :
@@ -829,7 +911,7 @@ void _pswrite_unlock_slow(unsigned int uc, psrwlock_t *rwlock)
 			ptype = PSRW_PREEMPT;
 		else
 			ptype = PSRW_NON_PREEMPT;
-		writer_count_dec(&uc, rwlock, ptype);
+		writer_count_dec(&uc, rwlock, ptype, _RET_IP_);
 		psrwlock_preempt_check(uc, rwlock);
 	}
 }
@@ -845,6 +927,7 @@ asmregparm void _psrwlock_wakeup(unsigned int uc, psrwlock_t *rwlock)
 {
 	unsigned long flags;
 	unsigned int ws;
+	struct psrwlock_waiter *waiter;
 
 	/*
 	 * Busy-loop waiting for the waitqueue mutex.
@@ -856,7 +939,8 @@ asmregparm void _psrwlock_wakeup(unsigned int uc, psrwlock_t *rwlock)
 	ws = atomic_read(&rwlock->ws);
 	_pswrite_lock_ctx_wait_sub(&ws, &rwlock->ws, rwlock,
 		0, WS_WQ_MUTEX, WS_WQ_MUTEX, WS_WQ_MUTEX,
-		V_INT, PSRW_READ, PSRW_NON_PREEMPT, 0, TASK_UNINTERRUPTIBLE);
+		V_INT, PSRW_READ, PSRW_NON_PREEMPT, 0, TASK_UNINTERRUPTIBLE,
+		_RET_IP_);
 	/*
 	 * If there is at least one non-preemptable writer subscribed or holding
 	 * higher priority write masks, let it handle the wakeup when it exits
@@ -888,10 +972,14 @@ asmregparm void _psrwlock_wakeup(unsigned int uc, psrwlock_t *rwlock)
 	 * First do an exclusive wake-up of the first writer if there is one
 	 * waiting, else wake-up the readers.
 	 */
-	if (waitqueue_active(&rwlock->wq_write))
-		wake_up_locked(&rwlock->wq_write);
+	if (!list_empty(&rwlock->wait_list_w))
+		waiter = list_entry(rwlock->wait_list_w.next,
+				    struct psrwlock_waiter, list);
 	else
-		wake_up_locked(&rwlock->wq_read);
+		waiter = list_entry(rwlock->wait_list_r.next,
+				    struct psrwlock_waiter, list);
+	debug_psrwlock_wake_waiter(rwlock, waiter);
+	wake_up_process(waiter->task);
 	smp_mb();	/*
 			 * Insure global memory order when clearing the mutex.
 			 */
diff --git a/lib/psrwlock.h b/lib/psrwlock.h
new file mode 100644
index 0000000..ce0f73d
--- /dev/null
+++ b/lib/psrwlock.h
@@ -0,0 +1,22 @@
+/*
+ * Psrwlock
+ *
+ * Internal psrwlock prototypes for !CONFIG_DEBUG_PSRWLOCK config.
+ */
+
+#define psrwlock_remove_waiter(lock, waiter, ti) \
+		__list_del((waiter)->list.prev, (waiter)->list.next)
+
+#define debug_psrwlock_set_owner(lock, new_owner)		do { } while (0)
+#define debug_psrwlock_clear_owner(lock)			do { } while (0)
+#define debug_psrwlock_wake_waiter(lock, waiter)		do { } while (0)
+#define debug_psrwlock_free_waiter(waiter)			do { } while (0)
+#define debug_psrwlock_add_waiter(lock, waiter, ti)	do { } while (0)
+#define debug_psrwlock_unlock(lock, rw)			do { } while (0)
+#define debug_psrwlock_init(lock, name, key)		do { } while (0)
+
+static inline void
+debug_psrwlock_lock_common(struct psrwlock *lock,
+			   struct psrwlock_waiter *waiter)
+{
+}
-- 
1.6.5.2

