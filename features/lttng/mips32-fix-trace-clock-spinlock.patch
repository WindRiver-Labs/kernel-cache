From 6653216fb75d5994c7b4ea2938ac00e1f84b467a Mon Sep 17 00:00:00 2001
From: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
Date: Thu, 13 May 2010 19:28:11 -0400
Subject: [PATCH 331/391] mips32-fix-trace-clock-spinlock

mips32 fix trace clock spinlock v2

Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
---
 arch/mips/kernel/trace-clock.c |   71 ++++++++++++++++++++-------------------
 1 files changed, 36 insertions(+), 35 deletions(-)

diff --git a/arch/mips/kernel/trace-clock.c b/arch/mips/kernel/trace-clock.c
index ed509e8..42e27b8 100644
--- a/arch/mips/kernel/trace-clock.c
+++ b/arch/mips/kernel/trace-clock.c
@@ -18,37 +18,11 @@ static DEFINE_SPINLOCK(async_tsc_lock);
 static int async_tsc_refcount;	/* Number of readers */
 static int async_tsc_enabled;	/* Async TSC enabled on all online CPUs */
 
-/*
- * Support for architectures with non-sync TSCs.
- * When the local TSC is discovered to lag behind the highest TSC counter, we
- * increment the TSC count of an amount that should be, ideally, lower than the
- * execution time of this routine, in cycles : this is the granularity we look
- * for : we must be able to order the events.
- */
-
-#if BITS_PER_LONG == 64
-notrace u64 trace_clock_async_tsc_read(void)
+#if (BITS_PER_LONG == 64)
+static inline u64 trace_clock_cmpxchg64(u64 *ptr, u64 old, u64 new)
 {
-	u64 new_tsc, last_tsc;
-
-	WARN_ON(!async_tsc_refcount || !async_tsc_enabled);
-	new_tsc = trace_clock_read_synthetic_tsc();
-	do {
-		last_tsc = trace_clock_last_tsc;
-		if (new_tsc < last_tsc)
-			new_tsc = last_tsc + TRACE_CLOCK_MIN_PROBE_DURATION;
-		/*
-		 * If cmpxchg fails with a value higher than the new_tsc, don't
-		 * retry : the value has been incremented and the events
-		 * happened almost at the same time.
-		 * We must retry if cmpxchg fails with a lower value :
-		 * it means that we are the CPU with highest frequency and
-		 * therefore MUST update the value.
-		 */
-	} while (cmpxchg64(&trace_clock_last_tsc, last_tsc, new_tsc) < new_tsc);
-	return new_tsc;
+	return cmpxchg64(ptr, old, new);
 }
-EXPORT_SYMBOL_GPL(trace_clock_async_tsc_read);
 #else
 /*
  * Emulate an atomic 64-bits update with a spinlock.
@@ -59,6 +33,9 @@ EXPORT_SYMBOL_GPL(trace_clock_async_tsc_read);
 static raw_spinlock_t trace_clock_lock =
 	(raw_spinlock_t)__RAW_SPIN_LOCK_UNLOCKED;
 
+/*
+ * Must be called under irqoff+spinlock on MIPS32.
+ */
 static inline u64 trace_clock_cmpxchg64(u64 *ptr, u64 old, u64 new)
 {
 	u64 val;
@@ -68,18 +45,41 @@ static inline u64 trace_clock_cmpxchg64(u64 *ptr, u64 old, u64 new)
 		*ptr = new;
 	return val;
 }
+#endif
+
+/*
+ * Must be called under irqoff+spinlock on MIPS32.
+ */
+static cycles_t read_last_tsc(void)
+{
+	return trace_clock_last_tsc;
+}
+
+/*
+ * Support for architectures with non-sync TSCs.
+ * When the local TSC is discovered to lag behind the highest TSC counter, we
+ * increment the TSC count of an amount that should be, ideally, lower than the
+ * execution time of this routine, in cycles : this is the granularity we look
+ * for : we must be able to order the events.
+ *
+ * MIPS32 does not have atomic 64-bit updates. Emulate it with irqoff+spinlock.
+ */
 
 notrace u64 trace_clock_async_tsc_read(void)
 {
 	u64 new_tsc, last_tsc;
+#if (BITS_PER_LONG == 32)
 	unsigned long flags;
 
-	WARN_ON(!async_tsc_refcount || !async_tsc_enabled);
 	local_irq_save(flags);
 	__raw_spin_lock(&trace_clock_lock);
+#endif
+
+	WARN_ON(!async_tsc_refcount || !async_tsc_enabled);
 	new_tsc = trace_clock_read_synthetic_tsc();
+	barrier();
+	last_tsc = read_last_tsc();
 	do {
-		last_tsc = trace_clock_last_tsc;
 		if (new_tsc < last_tsc)
 			new_tsc = last_tsc + TRACE_CLOCK_MIN_PROBE_DURATION;
 		/*
@@ -90,15 +90,16 @@ notrace u64 trace_clock_async_tsc_read(void)
 		 * it means that we are the CPU with highest frequency and
 		 * therefore MUST update the value.
 		 */
-	} while (trace_clock_cmpxchg64(&trace_clock_last_tsc, last_tsc,
-				       new_tsc) < new_tsc);
+		last_tsc = trace_clock_cmpxchg64(&trace_clock_last_tsc,
+						 last_tsc, new_tsc);
+	} while (unlikely(last_tsc < new_tsc));
+#if (BITS_PER_LONG == 32)
 	__raw_spin_unlock(&trace_clock_lock);
 	local_irq_restore(flags);
+#endif
 	return new_tsc;
 }
 EXPORT_SYMBOL_GPL(trace_clock_async_tsc_read);
-#endif
-
 
 static void update_timer_ipi(void *info)
 {
-- 
1.6.5.2

