From 3c5ba2823f9450cbb979df0dc44baa212b714338 Mon Sep 17 00:00:00 2001
From: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
Date: Thu, 13 May 2010 19:27:10 -0400
Subject: [PATCH 212/390] ltt-relay-locked-optimization-mark-likely-fastpath

ltt relay locked: mark likely fastpath

Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
---
 ltt/ltt-relay-locked.h |   71 +++++++++++++++++++++++++-----------------------
 1 files changed, 37 insertions(+), 34 deletions(-)

diff --git a/ltt/ltt-relay-locked.h b/ltt/ltt-relay-locked.h
index 5013c7d..1cdc3a0 100644
--- a/ltt/ltt-relay-locked.h
+++ b/ltt/ltt-relay-locked.h
@@ -211,7 +211,7 @@ static __inline__ int ltt_relay_try_reserve(
 	if (last_tsc_overflow(ltt_buf, *tsc))
 		*rflags = LTT_RFLAG_ID_SIZE_TSC;
 
-	if (SUBBUF_OFFSET(offsets->begin, buf->chan) == 0) {
+	if (unlikely(SUBBUF_OFFSET(offsets->begin, buf->chan) == 0)) {
 		offsets->begin_switch = 1;		/* For offsets->begin */
 	} else {
 		offsets->size = ltt_get_header_size(ltt_channel,
@@ -220,16 +220,19 @@ static __inline__ int ltt_relay_try_reserve(
 		offsets->size += ltt_align(offsets->begin + offsets->size,
 					   largest_align)
 				 + data_size;
-		if ((SUBBUF_OFFSET(offsets->begin, buf->chan) + offsets->size)
-				> buf->chan->subbuf_size) {
+		if (unlikely((SUBBUF_OFFSET(offsets->begin, buf->chan) +
+			      offsets->size) > buf->chan->subbuf_size)) {
 			offsets->end_switch_old = 1;	/* For offsets->old */
 			offsets->begin_switch = 1;	/* For offsets->begin */
 		}
 	}
-	if (offsets->begin_switch) {
+	if (unlikely(offsets->begin_switch)) {
 		long subbuf_index;
 
-		if (offsets->end_switch_old)
+		/*
+		 * We are typically not filling the previous buffer completely.
+		 */
+		if (likely(offsets->end_switch_old))
 			offsets->begin = SUBBUF_ALIGN(offsets->begin,
 						      buf->chan);
 		offsets->begin = offsets->begin + ltt_subbuffer_header_size();
@@ -240,12 +243,12 @@ static __inline__ int ltt_relay_try_reserve(
 			 >> ltt_channel->n_subbufs_order)
 			- (ltt_buf->commit_count[subbuf_index]
 			   & ltt_channel->commit_count_mask);
-		if (offsets->reserve_commit_diff == 0) {
+		if (likely(offsets->reserve_commit_diff == 0)) {
 			/* Next buffer not corrupted. */
-			if (!ltt_channel->overwrite &&
+			if (unlikely(!ltt_channel->overwrite &&
 				(SUBBUF_TRUNC(offsets->begin, buf->chan)
 				- SUBBUF_TRUNC(ltt_buf->consumed, buf->chan))
-				>= rchan->alloc_size) {
+				>= rchan->alloc_size)) {
 				/*
 				 * We do not overwrite non consumed buffers
 				 * and we are full : event is lost.
@@ -272,8 +275,8 @@ static __inline__ int ltt_relay_try_reserve(
 		offsets->size += ltt_align(offsets->begin + offsets->size,
 					   largest_align)
 				 + data_size;
-		if ((SUBBUF_OFFSET(offsets->begin, buf->chan) + offsets->size)
-				> buf->chan->subbuf_size) {
+		if (unlikely((SUBBUF_OFFSET(offsets->begin, buf->chan)
+			      + offsets->size) > buf->chan->subbuf_size)) {
 			/*
 			 * Event too big for subbuffers, report error, don't
 			 * complete the sub-buffer switch.
@@ -294,7 +297,7 @@ static __inline__ int ltt_relay_try_reserve(
 	}
 	offsets->end = offsets->begin + offsets->size;
 
-	if ((SUBBUF_OFFSET(offsets->end, buf->chan)) == 0) {
+	if (unlikely((SUBBUF_OFFSET(offsets->end, buf->chan)) == 0)) {
 		/*
 		 * The offset_end will fall at the very beginning of the next
 		 * subbuffer.
@@ -388,15 +391,15 @@ static __inline__ void ltt_reserve_push_reader(
 	 * If the buffer is not in overwrite mode, pushing the reader
 	 * only happens if a sub-buffer is corrupted.
 	 */
-	if ((SUBBUF_TRUNC(offsets->end-1, buf->chan)
+	if (unlikely((SUBBUF_TRUNC(offsets->end-1, buf->chan)
 	   - SUBBUF_TRUNC(consumed_old, buf->chan))
-	   >= rchan->alloc_size) {
+	   >= rchan->alloc_size)) {
 		consumed_new = SUBBUF_ALIGN(consumed_old, buf->chan);
 		ltt_buf->consumed = consumed_new;
 	} else
-		consumed_new = consumed_old;
+		return;
 
-	if (consumed_old != consumed_new) {
+	if (unlikely(consumed_old != consumed_new)) {
 		/*
 		 * Reader pushed : we are the winner of the push, we can
 		 * therefore reequilibrate reserve and commit. Atomic increment
@@ -408,7 +411,7 @@ static __inline__ void ltt_reserve_push_reader(
 		 * compared to the unordered data input, or there is a writer
 		 * that died between the reserve and the commit.
 		 */
-		if (offsets->reserve_commit_diff) {
+		if (likely(offsets->reserve_commit_diff)) {
 			/*
 			 * We have to alter the sub-buffer commit count.
 			 * We do not deliver the previous subbuffer, given it
@@ -418,9 +421,9 @@ static __inline__ void ltt_reserve_push_reader(
 			ltt_buf->commit_count[SUBBUF_INDEX(offsets->begin,
 							   buf->chan)] +=
 						offsets->reserve_commit_diff;
-			if (!ltt_channel->overwrite
+			if (unlikely(!ltt_channel->overwrite
 			    || offsets->reserve_commit_diff
-			       != rchan->subbuf_size) {
+			       != rchan->subbuf_size)) {
 				/*
 				 * The reserve commit diff was not subbuf_size :
 				 * it means the subbuffer was partly written to
@@ -470,10 +473,10 @@ static __inline__ void ltt_reserve_switch_old_subbuf(
 		- (SUBBUF_OFFSET(offsets->old - 1, rchan)
 		+ 1);
 	offsets->commit_count = ltt_buf->commit_count[oldidx];
-	if ((BUFFER_TRUNC(offsets->old - 1, rchan)
+	if (likely((BUFFER_TRUNC(offsets->old - 1, rchan)
 			>> ltt_channel->n_subbufs_order)
 			- ((offsets->commit_count - rchan->subbuf_size)
-			   & ltt_channel->commit_count_mask) == 0)
+			   & ltt_channel->commit_count_mask) == 0))
 		ltt_deliver(buf, oldidx, NULL);
 }
 
@@ -496,10 +499,10 @@ static __inline__ void ltt_reserve_switch_new_subbuf(
 	ltt_buf->commit_count[beginidx] += ltt_subbuffer_header_size();
 	offsets->commit_count = ltt_buf->commit_count[beginidx];
 	/* Check if the written buffer has to be delivered */
-	if ((BUFFER_TRUNC(offsets->end - 1, rchan)
+	if (unlikely((BUFFER_TRUNC(offsets->end - 1, rchan)
 			>> ltt_channel->n_subbufs_order)
 			- ((offsets->commit_count - rchan->subbuf_size)
-			   & ltt_channel->commit_count_mask) == 0)
+			   & ltt_channel->commit_count_mask) == 0))
 		ltt_deliver(buf, beginidx, NULL);
 }
 
@@ -536,10 +539,10 @@ static __inline__ void ltt_reserve_end_switch_current(
 		- (SUBBUF_OFFSET(offsets->end - 1, rchan)
 		+ 1);
 	offsets->commit_count = ltt_buf->commit_count[endidx];
-	if ((BUFFER_TRUNC(offsets->end - 1, rchan)
+	if (likely((BUFFER_TRUNC(offsets->end - 1, rchan)
 			>> ltt_channel->n_subbufs_order)
 			- ((offsets->commit_count - rchan->subbuf_size)
-			   & ltt_channel->commit_count_mask) == 0)
+			   & ltt_channel->commit_count_mask) == 0))
 		ltt_deliver(buf, endidx, NULL);
 }
 
@@ -578,16 +581,16 @@ static __inline__ int ltt_relay_reserve_slot(struct ltt_trace_struct *trace,
 	/*
 	 * Perform retryable operations.
 	 */
-	if (__get_cpu_var(ltt_nesting) > 4) {
+	if (unlikely(__get_cpu_var(ltt_nesting) > 4)) {
 		ltt_buf->events_lost++;
 		__raw_spin_unlock(&ltt_buf->lock);
 		raw_local_irq_restore(flags);
 		return -EPERM;
 	}
 
-	if (ltt_relay_try_reserve(ltt_channel, ltt_buf,
+	if (unlikely(ltt_relay_try_reserve(ltt_channel, ltt_buf,
 			rchan, buf, &offsets, data_size, tsc, rflags,
-			largest_align)) {
+			largest_align))) {
 		__raw_spin_unlock(&ltt_buf->lock);
 		raw_local_irq_restore(flags);
 		return -ENOSPC;
@@ -604,18 +607,18 @@ static __inline__ int ltt_relay_reserve_slot(struct ltt_trace_struct *trace,
 	/*
 	 * Switch old subbuffer if needed.
 	 */
-	if (offsets.end_switch_old)
+	if (unlikely(offsets.end_switch_old))
 		ltt_reserve_switch_old_subbuf(ltt_channel, ltt_buf, rchan, buf,
 			&offsets, tsc);
 
 	/*
 	 * Populate new subbuffer.
 	 */
-	if (offsets.begin_switch)
+	if (unlikely(offsets.begin_switch))
 		ltt_reserve_switch_new_subbuf(ltt_channel, ltt_buf, rchan,
 			buf, &offsets, tsc);
 
-	if (offsets.end_switch_current)
+	if (unlikely(offsets.end_switch_current))
 		ltt_reserve_end_switch_current(ltt_channel, ltt_buf, rchan,
 			buf, &offsets, tsc);
 
@@ -712,12 +715,12 @@ static __inline__ void ltt_write_commit_counter(struct rchan_buf *buf,
 		lost_old = header->lost_size;
 		commit_count = ltt_buf->commit_count[subbuf_idx];
 		/* SUBBUF_OFFSET includes commit_count_mask */
-		if (!SUBBUF_OFFSET(offset - commit_count, buf->chan)) {
+		if (likely(!SUBBUF_OFFSET(offset - commit_count, buf->chan))) {
 			lost_new = (uint32_t)buf->chan->subbuf_size
 				   - SUBBUF_OFFSET(commit_count, buf->chan);
 			lost_old = cmpxchg_local(&header->lost_size, lost_old,
 							lost_new);
-			if (lost_old <= lost_new)
+			if (likely(lost_old <= lost_new))
 				break;
 		} else {
 			break;
@@ -756,10 +759,10 @@ static __inline__ void ltt_relay_commit_slot(
 	ltt_buf->commit_count[endidx] += slot_size;
 	commit_count = ltt_buf->commit_count[endidx];
 	/* Check if all commits have been done */
-	if ((BUFFER_TRUNC(offset_end - 1, rchan)
+	if (unlikely((BUFFER_TRUNC(offset_end - 1, rchan)
 			>> ltt_channel->n_subbufs_order)
 			- ((commit_count - rchan->subbuf_size)
-			   & ltt_channel->commit_count_mask) == 0)
+			   & ltt_channel->commit_count_mask) == 0))
 		ltt_deliver(buf, endidx, NULL);
 	/*
 	 * Update lost_size for each commit. It's needed only for extracting
-- 
1.6.5.2

