From 37c26712a8c53d51401a4e52091cb9f55cfe46db Mon Sep 17 00:00:00 2001
From: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
Date: Thu, 13 May 2010 19:27:38 -0400
Subject: [PATCH 264/391] lttng-relay-lockless-fix-push-reader

LTTng relay lockless fix push reader

lockless relay code needs to push the reader even in the fast path, given this
part is executed our of order.

It is not needed in the locked and irqoff relay because sub-buffer switch
(slow path) is done in order (protected by disabling interrupts).

Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
---
 ltt/ltt-relay-lockless.c |   38 ++------------------------------------
 ltt/ltt-relay-lockless.h |   36 ++++++++++++++++++++++++++++++++++++
 2 files changed, 38 insertions(+), 36 deletions(-)

diff --git a/ltt/ltt-relay-lockless.c b/ltt/ltt-relay-lockless.c
index 50a3488..0e77cff 100644
--- a/ltt/ltt-relay-lockless.c
+++ b/ltt/ltt-relay-lockless.c
@@ -1138,39 +1138,6 @@ static void ltt_relay_print_user_errors(struct ltt_trace_struct *trace,
 			dbg->write, dbg->read);
 }
 
-static void ltt_reserve_push_reader(
-		struct ltt_channel_struct *ltt_channel,
-		struct ltt_channel_buf_struct *ltt_buf,
-		struct rchan *rchan,
-		struct rchan_buf *buf,
-		struct ltt_reserve_switch_offsets *offsets)
-{
-	long consumed_old, consumed_new;
-
-	do {
-		consumed_old = atomic_long_read(&ltt_buf->consumed);
-		/*
-		 * If buffer is in overwrite mode, push the reader consumed
-		 * count if the write position has reached it and we are not
-		 * at the first iteration (don't push the reader farther than
-		 * the writer). This operation can be done concurrently by many
-		 * writers in the same buffer, the writer being at the farthest
-		 * write position sub-buffer index in the buffer being the one
-		 * which will win this loop.
-		 * If the buffer is not in overwrite mode, pushing the reader
-		 * only happens if a sub-buffer is corrupted.
-		 */
-		if (unlikely((SUBBUF_TRUNC(offsets->end-1, buf->chan)
-		   - SUBBUF_TRUNC(consumed_old, buf->chan))
-		   >= rchan->alloc_size))
-			consumed_new = SUBBUF_ALIGN(consumed_old, buf->chan);
-		else
-			return;
-	} while (unlikely(atomic_long_cmpxchg(&ltt_buf->consumed, consumed_old,
-			consumed_new) != consumed_old));
-}
-
-
 /*
  * ltt_reserve_switch_old_subbuf: switch old subbuffer
  *
@@ -1408,8 +1375,7 @@ void ltt_force_switch_lockless_slow(struct rchan_buf *buf,
 	 * Push the reader if necessary
 	 */
 	if (mode == FORCE_ACTIVE)
-		ltt_reserve_push_reader(ltt_channel, ltt_buf, rchan,
-					buf, &offsets);
+		ltt_reserve_push_reader(ltt_buf, rchan, buf, offsets.end - 1);
 
 	/*
 	 * Switch old subbuffer if needed.
@@ -1595,7 +1561,7 @@ int ltt_reserve_slot_lockless_slow(struct ltt_trace_struct *trace,
 	/*
 	 * Push the reader if necessary
 	 */
-	ltt_reserve_push_reader(ltt_channel, ltt_buf, rchan, buf, &offsets);
+	ltt_reserve_push_reader(ltt_buf, rchan, buf, offsets.end - 1);
 
 	/*
 	 * Switch old subbuffer if needed.
diff --git a/ltt/ltt-relay-lockless.h b/ltt/ltt-relay-lockless.h
index 8b33032..30ae7e0 100644
--- a/ltt/ltt-relay-lockless.h
+++ b/ltt/ltt-relay-lockless.h
@@ -158,6 +158,37 @@ static __inline__ int last_tsc_overflow(struct ltt_channel_buf_struct *ltt_buf,
 }
 #endif
 
+static __inline__ void ltt_reserve_push_reader(
+		struct ltt_channel_buf_struct *ltt_buf,
+		struct rchan *rchan,
+		struct rchan_buf *buf,
+		long offset)
+{
+	long consumed_old, consumed_new;
+
+	do {
+		consumed_old = atomic_long_read(&ltt_buf->consumed);
+		/*
+		 * If buffer is in overwrite mode, push the reader consumed
+		 * count if the write position has reached it and we are not
+		 * at the first iteration (don't push the reader farther than
+		 * the writer). This operation can be done concurrently by many
+		 * writers in the same buffer, the writer being at the farthest
+		 * write position sub-buffer index in the buffer being the one
+		 * which will win this loop.
+		 * If the buffer is not in overwrite mode, pushing the reader
+		 * only happens if a sub-buffer is corrupted.
+		 */
+		if (unlikely((SUBBUF_TRUNC(offset, buf->chan)
+		   - SUBBUF_TRUNC(consumed_old, buf->chan))
+		   >= rchan->alloc_size))
+			consumed_new = SUBBUF_ALIGN(consumed_old, buf->chan);
+		else
+			return;
+	} while (unlikely(atomic_long_cmpxchg(&ltt_buf->consumed, consumed_old,
+			consumed_new) != consumed_old));
+}
+
 #ifdef CONFIG_LTT_VMCORE
 static __inline__ void ltt_check_deliver(struct ltt_channel_struct *ltt_channel,
 		struct ltt_channel_buf_struct *ltt_buf,
@@ -315,6 +346,11 @@ static __inline__ int ltt_reserve_slot(struct ltt_trace_struct *trace,
 	 */
 	save_last_tsc(ltt_buf, *tsc);
 
+	/*
+	 * Push the reader if necessary
+	 */
+	ltt_reserve_push_reader(ltt_buf, rchan, buf, o_end - 1);
+
 	*buf_offset = o_begin + before_hdr_pad;
 	return 0;
 slow_path:
-- 
1.6.5.2

