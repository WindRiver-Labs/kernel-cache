From efc96b6f6fea481f2f08693a6b259d4e5b59c25e Mon Sep 17 00:00:00 2001
From: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
Date: Thu, 13 May 2010 19:27:56 -0400
Subject: [PATCH 298/391] lttng-relay-refactoring

LTTng relay refactoring

- Change design to inheritance for channels and buffers.
  - Eliminate structure duplication for locking/allocation level of buffer and
    channel.
  - Remove now unnecessary krefs.
- Fix cpu hotplug for timers.
- Cleanups

Impact:
- Cleaner code
- Smaller d-cache footprint
- Complete license change for LTTng parts re-usable for userspace tracing to
  dual-license LGPL v2.1 / GPL v2.

Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
---
 include/linux/ltt-channels.h |   57 +--
 include/linux/ltt-relay.h    |  284 +++------
 include/linux/ltt-tracer.h   |  229 ++++----
 include/linux/marker.h       |    1 -
 kernel/marker.c              |    2 +-
 ltt/Kconfig                  |   16 +-
 ltt/Makefile                 |   11 +-
 ltt/ltt-ascii.c              |  107 ++--
 ltt/ltt-channels.c           |   78 +--
 ltt/ltt-filter.c             |    4 +-
 ltt/ltt-ftrace.c             |   36 +-
 ltt/ltt-kprobes.c            |   43 +-
 ltt/ltt-relay-alloc.c        |  661 +++++++-------------
 ltt/ltt-relay-lockless.c     | 1452 +++++++++++++++--------------------------
 ltt/ltt-relay-lockless.h     |  291 +++++----
 ltt/ltt-relay-splice.c       |  154 +++++
 ltt/ltt-relay-vfs.c          |  231 +++++++
 ltt/ltt-serialize.c          |  178 +++---
 ltt/ltt-statedump.c          |   57 +-
 ltt/ltt-trace-control.c      |  325 ++++++----
 ltt/ltt-tracer.c             |  226 ++++---
 ltt/ltt-type-serializer.c    |   37 +-
 ltt/ltt-userspace-event.c    |    9 +-
 23 files changed, 2129 insertions(+), 2360 deletions(-)
 create mode 100644 ltt/ltt-relay-splice.c
 create mode 100644 ltt/ltt-relay-vfs.c

diff --git a/include/linux/ltt-channels.h b/include/linux/ltt-channels.h
index 77e2841..51fe931 100644
--- a/include/linux/ltt-channels.h
+++ b/include/linux/ltt-channels.h
@@ -9,6 +9,7 @@
  * Dual LGPL v2.1/GPL v2 license.
  */
 
+#include <linux/ltt-relay.h>
 #include <linux/limits.h>
 #include <linux/kref.h>
 #include <linux/list.h>
@@ -16,54 +17,25 @@
 
 #define EVENTS_PER_CHANNEL	65536
 
-struct ltt_trace_struct;
-struct rchan_buf;
+struct ltt_trace;
 
-struct ltt_channel_struct {
-	/* First 32 bytes cache-hot cacheline */
-	struct ltt_trace_struct	*trace;
-	void *trans_channel_data;
+struct ltt_chan {
+	struct ltt_chan_alloc a;		/* Parent. First field. */
 	int overwrite:1;
 	int active:1;
-	unsigned int n_subbufs_order;
 	unsigned long commit_count_mask;	/*
 						 * Commit count mask, removing
 						 * the MSBs corresponding to
 						 * bits used to represent the
 						 * subbuffer index.
 						 */
-	/* End of first 32 bytes cacheline */
-
-	struct kref kref;	/* Channel transport reference count */
-	struct ltt_channel_buf_access_ops *buf_access_ops;
-	unsigned int subbuf_size;
-	unsigned int subbuf_cnt;
+	struct ltt_trace *trace;
 	unsigned long switch_timer_interval;
-	const char *channel_name;
-} ____cacheline_aligned;
-
-/*
- * ops for accessing struct channel data.
- * Only meant to be used in slow-path code (ascii formatter code,
- * buffer read-side access through a system call).
- */
-struct ltt_channel_buf_access_ops {
-	unsigned long (*get_offset)(struct rchan_buf *buf);
-	unsigned long (*get_consumed)(struct rchan_buf *buf);
-	int (*open)(struct rchan_buf *buf);
-	int (*release)(struct rchan_buf *buf);
-	int (*get_subbuf)(struct rchan_buf *buf, unsigned long *consumed);
-	int (*put_subbuf)(struct rchan_buf *buf, unsigned long consumed);
-	int (*is_finalized)(struct rchan_buf *buf);
-	unsigned long (*get_n_subbufs)(struct rchan_buf *buf);
-	unsigned long (*get_subbuf_size)(struct rchan_buf *buf);
-	void (*start_switch_timer)(struct ltt_channel_struct *ltt_channel);
-	void (*stop_switch_timer)(struct ltt_channel_struct *ltt_channel);
 };
 
 struct ltt_channel_setting {
-	unsigned int subbuf_size;
-	unsigned int subbuf_cnt;
+	unsigned int sb_size;
+	unsigned int n_sb;
 	struct kref kref;	/* Number of references to structure content */
 	struct list_head list;
 	unsigned int index;	/* index of channel in trace channel array */
@@ -79,17 +51,12 @@ int ltt_channels_set_default(const char *name,
 const char *ltt_channels_get_name_from_index(unsigned int index);
 int ltt_channels_get_index_from_name(const char *name);
 int ltt_channels_trace_ref(void);
-struct ltt_channel_struct *ltt_channels_trace_alloc(unsigned int *nr_channels,
-						    int overwrite,
-						    int active);
-void ltt_channels_trace_free(struct ltt_channel_struct *channels,
-	unsigned int nr_channels);
-void ltt_channels_trace_set_timer(struct ltt_channel_struct *channel,
+struct ltt_chan *ltt_channels_trace_alloc(unsigned int *nr_channels,
+					  int overwrite, int active);
+void ltt_channels_trace_free(struct ltt_chan *channels,
+			     unsigned int nr_channels);
+void ltt_channels_trace_set_timer(struct ltt_chan *chan,
 				  unsigned long interval);
-void ltt_channels_trace_start_timer(struct ltt_channel_struct *channels,
-	unsigned int nr_channels);
-void ltt_channels_trace_stop_timer(struct ltt_channel_struct *channels,
-	unsigned int nr_channels);
 
 int _ltt_channels_get_event_id(const char *channel, const char *name);
 int ltt_channels_get_event_id(const char *channel, const char *name);
diff --git a/include/linux/ltt-relay.h b/include/linux/ltt-relay.h
index 43f628d..30e4485 100644
--- a/include/linux/ltt-relay.h
+++ b/include/linux/ltt-relay.h
@@ -1,11 +1,9 @@
-/*
- * linux/include/linux/ltt-relay.h
+ /*
+ * include/linux/ltt-relay.h
  *
- * Copyright (C) 2002, 2003 - Tom Zanussi (zanussi@us.ibm.com), IBM Corp
- * Copyright (C) 1999, 2000, 2001, 2002 - Karim Yaghmour (karim@opersys.com)
- * Copyright (C) 2008 - Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ * Copyright (C) 2008,2009 - Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
  *
- * CONFIG_RELAY definitions and declarations.
+ * Dual LGPL v2.1/GPL v2 license.
  *
  * Credits to Steven Rostedt for proposing to use an extra-subbuffer owned by
  * the reader in flight recorder mode.
@@ -30,138 +28,80 @@
 
 #define RCHAN_SB_IS_NOREF(x)	((unsigned long)(x) & RCHAN_NOREF_FLAG)
 #define RCHAN_SB_SET_NOREF(x)	\
-	(x = (struct rchan_page *)((unsigned long)(x) | RCHAN_NOREF_FLAG))
+	(x = (struct chanbuf_page *)((unsigned long)(x) | RCHAN_NOREF_FLAG))
 #define RCHAN_SB_CLEAR_NOREF(x)	\
-	(x = (struct rchan_page *)((unsigned long)(x) & ~RCHAN_NOREF_FLAG))
-
-/*
- * Tracks changes to rchan/rchan_buf structs
- */
-#define LTT_RELAY_CHANNEL_VERSION		8
+	(x = (struct chanbuf_page *)((unsigned long)(x) & ~RCHAN_NOREF_FLAG))
 
-struct rchan_page {
+struct chanbuf_page {
 	void *virt;			/* page virtual address (cached) */
 	struct page *page;		/* pointer to page structure */
 };
 
-struct rchan_sb {
-	struct rchan_page *pages;	/* Pointer to rchan pages for subbuf */
+struct chanbuf_sb {
+	struct chanbuf_page *pages;	/* Pointer to rchan pages for subbuf */
 };
 
-/*
- * Per-cpu relay channel buffer
- */
-struct rchan_buf {
-	void *chan_private;		/* private data for this buf */
-	struct rchan_sb *rchan_wsb;	/* Array of rchan_sb for writer */
-	struct rchan_sb rchan_rsb;	/* rchan_sb for reader */
-	struct rchan *chan;		/* associated channel */
-	struct dentry *dentry;		/* channel file dentry */
-	struct kref kref;		/* channel buffer refcount */
+struct ltt_chanbuf_alloc {
+	struct chanbuf_sb *buf_wsb;	/* Array of rchan_sb for writer */
+	struct chanbuf_sb buf_rsb;	/* chanbuf_sb for reader */
 	void **_virt;			/* Array of pointers to page addr */
 	struct page **_pages;		/* Array of pointers to pages */
-	unsigned int page_count;	/* number of current buffer pages */
-	unsigned int cpu;		/* this buf's cpu */
-	unsigned int random_access;	/* buffer performs random page access */
-	struct dentry *ascii_dentry;	/* Text output dentry */
-} ____cacheline_aligned;
+	struct dentry *dentry;		/* Associated file dentry */
+	unsigned int nr_pages;		/* Number pages in buffer */
 
-/*
- * Relay channel data structure
- */
-struct rchan {
-	u32 version;			/* the version of this struct */
-	size_t subbuf_size;		/* sub-buffer size */
-	size_t n_subbufs;		/* number of sub-buffers per buffer */
-	size_t alloc_size;		/* total buffer size allocated */
-	struct rchan_callbacks *cb;	/* client callbacks */
-	struct kref kref;		/* channel refcount */
-	void *private_data;		/* for user-defined data */
-	struct rchan_buf *buf[NR_CPUS]; /* per-cpu channel buffers */
-	struct list_head list;		/* for channel list */
-	struct dentry *parent;		/* parent dentry passed to open */
-	int subbuf_size_order;		/* order of sub-buffer size */
-	int extra_reader_sb;		/* bool: has extra reader subbuffer */
-	char base_filename[NAME_MAX];	/* saved base filename */
+	struct ltt_chan_alloc *chan;	/* Associated channel */
+	struct kref kref;		/* Reference count */
+	unsigned int cpu;		/* This buffer's cpu */
+	unsigned int allocated:1;	/* Bool: is buffer allocated ? */
 };
 
 /*
- * Relay channel client callbacks
+ * Forward declaration of locking-specific per-cpu buffer structure.
  */
-struct rchan_callbacks {
-	/*
-	 * subbuf_start - called on buffer-switch to a new sub-buffer
-	 * @buf: the channel buffer containing the new sub-buffer
-	 * @subbuf: the start of the new sub-buffer
-	 * @prev_subbuf: the start of the previous sub-buffer
-	 * @prev_padding: unused space at the end of previous sub-buffer
-	 *
-	 * The client should return 1 to continue logging, 0 to stop
-	 * logging.
-	 *
-	 * NOTE: subbuf_start will also be invoked when the buffer is
-	 *       created, so that the first sub-buffer can be initialized
-	 *       if necessary.  In this case, prev_subbuf will be NULL.
-	 *
-	 * NOTE: the client can reserve bytes at the beginning of the new
-	 *       sub-buffer by calling subbuf_start_reserve() in this callback.
-	 */
-	int (*subbuf_start) (struct rchan_buf *buf,
-			     void *subbuf,
-			     void *prev_subbuf,
-			     size_t prev_padding);
+struct ltt_chanbuf;
+
+struct ltt_chan_alloc {
+	unsigned long buf_size;		/* Size of the buffer */
+	unsigned long sb_size;		/* Sub-buffer size */
+	unsigned int sb_size_order;	/* Order of sub-buffer size */
+	unsigned int n_sb_order;	/* Number of sub-buffers per buffer */
+	int extra_reader_sb:1;		/* Bool: has extra reader subbuffer */
+	struct ltt_chanbuf *buf;	/* Channel per-cpu buffers */
+
+	struct list_head list;		/* Channel list */
+	struct kref kref;		/* Reference count */
+	unsigned long n_sb;		/* Number of sub-buffers */
+	struct dentry *parent;		/* Associated parent dentry */
+	struct dentry *ascii_dentry;	/* Text output dentry */
+	char filename[NAME_MAX];	/* Filename for channel files */
+};
 
-	/*
-	 * create_buf_file - create file to represent a relay channel buffer
-	 * @filename: the name of the file to create
-	 * @parent: the parent of the file to create
-	 * @mode: the mode of the file to create
-	 * @buf: the channel buffer
-	 *
-	 * Called during relay_open(), once for each per-cpu buffer,
-	 * to allow the client to create a file to be used to
-	 * represent the corresponding channel buffer.  If the file is
-	 * created outside of relay, the parent must also exist in
-	 * that filesystem.
-	 *
-	 * The callback should return the dentry of the file created
-	 * to represent the relay buffer.
-	 *
-	 * Setting the is_global outparam to a non-zero value will
-	 * cause relay_open() to create a single global buffer rather
-	 * than the default set of per-cpu buffers.
-	 *
-	 * See Documentation/filesystems/relayfs.txt for more info.
-	 */
-	struct dentry *(*create_buf_file)(const char *filename,
-					  struct dentry *parent,
-					  int mode,
-					  struct rchan_buf *buf);
+int ltt_chanbuf_alloc_create(struct ltt_chanbuf_alloc *buf,
+			     struct ltt_chan_alloc *chan, int cpu);
+void ltt_chanbuf_alloc_free(struct ltt_chanbuf_alloc *buf);
+int ltt_chan_alloc_init(struct ltt_chan_alloc *chan,
+			const char *base_filename,
+			struct dentry *parent, size_t sb_size,
+			size_t n_sb, int extra_reader_sb, int overwrite);
+void ltt_chan_alloc_free(struct ltt_chan_alloc *chan);
+int ltt_chanbuf_create_file(const char *filename, struct dentry *parent,
+			    int mode, struct ltt_chanbuf *buf);
+int ltt_chanbuf_remove_file(struct ltt_chanbuf *buf);
 
-	/*
-	 * remove_buf_file - remove file representing a relay channel buffer
-	 * @dentry: the dentry of the file to remove
-	 *
-	 * Called during relay_close(), once for each per-cpu buffer,
-	 * to allow the client to remove a file used to represent a
-	 * channel buffer.
-	 *
-	 * The callback should return 0 if successful, negative if not.
-	 */
-	int (*remove_buf_file)(struct dentry *dentry);
-};
+void ltt_chan_for_each_channel(void (*cb) (struct ltt_chanbuf *buf), int cpu);
 
-extern void _ltt_relay_write(struct rchan_buf *buf, size_t offset,
-	const void *src, size_t len, ssize_t pagecpy);
+extern void _ltt_relay_write(struct ltt_chanbuf_alloc *bufa,
+			     size_t offset, const void *src, size_t len,
+			     ssize_t pagecpy);
 
-extern int ltt_relay_read(struct rchan_buf *buf, size_t offset,
-	void *dest, size_t len);
+extern int ltt_relay_read(struct ltt_chanbuf_alloc *bufa,
+			  size_t offset, void *dest, size_t len);
 
-extern int ltt_relay_read_cstr(struct rchan_buf *buf, size_t offset,
-	void *dest, size_t len);
+extern int ltt_relay_read_cstr(struct ltt_chanbuf_alloc *bufa,
+			       size_t offset, void *dest, size_t len);
 
-extern struct page *ltt_relay_read_get_page(struct rchan_buf *buf,
-	size_t offset);
+extern struct page *ltt_relay_read_get_page(struct ltt_chanbuf_alloc *bufa,
+					    size_t offset);
 
 /*
  * Return the address where a given offset is located.
@@ -169,13 +109,14 @@ extern struct page *ltt_relay_read_get_page(struct rchan_buf *buf,
  * it's never on a page boundary, it's safe to write directly to this address,
  * as long as the write is never bigger than a page size.
  */
-extern void *ltt_relay_offset_address(struct rchan_buf *buf,
-	size_t offset);
-extern void *ltt_relay_read_offset_address(struct rchan_buf *buf,
-	size_t offset);
+extern void *ltt_relay_offset_address(struct ltt_chanbuf_alloc *bufa,
+				      size_t offset);
+extern void *ltt_relay_read_offset_address(struct ltt_chanbuf_alloc *bufa,
+					   size_t offset);
 
 #ifdef CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS
-static __inline__ void ltt_relay_do_copy(void *dest, const void *src, size_t len)
+static __inline__
+void ltt_relay_do_copy(void *dest, const void *src, size_t len)
 {
 	switch (len) {
 	case 0:
@@ -206,7 +147,8 @@ static __inline__ void ltt_relay_do_copy(void *dest, const void *src, size_t len
  * Returns whether the dest and src addresses are aligned on
  * min(sizeof(void *), len). Call this with statically known len for efficiency.
  */
-static __inline__ int addr_aligned(const void *dest, const void *src, size_t len)
+static __inline__
+int addr_aligned(const void *dest, const void *src, size_t len)
 {
 	if (ltt_align((size_t)dest, len))
 		return 0;
@@ -215,7 +157,8 @@ static __inline__ int addr_aligned(const void *dest, const void *src, size_t len
 	return 1;
 }
 
-static __inline__ void ltt_relay_do_copy(void *dest, const void *src, size_t len)
+static __inline__
+void ltt_relay_do_copy(void *dest, const void *src, size_t len)
 {
 	switch (len) {
 	case 0:
@@ -253,43 +196,44 @@ memcpy_fallback:
 }
 #endif
 
-static __inline__ int ltt_relay_write(struct rchan_buf *buf, size_t offset,
-	const void *src, size_t len)
+static __inline__
+int ltt_relay_write(struct ltt_chanbuf_alloc *bufa,
+		    struct ltt_chan_alloc *chana, size_t offset,
+		    const void *src, size_t len)
 {
 	size_t sbidx, index;
 	ssize_t pagecpy;
-	struct rchan_page *rpages;
+	struct chanbuf_page *rpages;
 
-	offset &= buf->chan->alloc_size - 1;
-	sbidx = offset >> buf->chan->subbuf_size_order;
-	index = (offset & (buf->chan->subbuf_size - 1)) >> PAGE_SHIFT;
+	offset &= chana->buf_size - 1;
+	sbidx = offset >> chana->sb_size_order;
+	index = (offset & (chana->sb_size - 1)) >> PAGE_SHIFT;
 	pagecpy = min_t(size_t, len, (- offset) & ~PAGE_MASK);
-	rpages = buf->rchan_wsb[sbidx].pages;
+	rpages = bufa->buf_wsb[sbidx].pages;
 	WARN_ON_ONCE(RCHAN_SB_IS_NOREF(rpages));
 	ltt_relay_do_copy(rpages[index].virt + (offset & ~PAGE_MASK),
 			  src, pagecpy);
 
 	if (unlikely(len != pagecpy))
-		_ltt_relay_write(buf, offset, src, len, pagecpy);
+		_ltt_relay_write(bufa, offset, src, len, pagecpy);
 	return len;
 }
 
 /**
  * ltt_clear_noref_flag - Clear the noref subbuffer flag, for writer.
  */
-static __inline__ void ltt_clear_noref_flag(struct rchan *rchan,
-					    struct rchan_buf *buf,
-					    long idx)
+static __inline__
+void ltt_clear_noref_flag(struct ltt_chanbuf_alloc *bufa, long idx)
 {
-	struct rchan_page *sb_pages, *new_sb_pages;
+	struct chanbuf_page *sb_pages, *new_sb_pages;
 
-	sb_pages = buf->rchan_wsb[idx].pages;
+	sb_pages = bufa->buf_wsb[idx].pages;
 	for (;;) {
 		if (!RCHAN_SB_IS_NOREF(sb_pages))
 			return;	/* Already writing to this buffer */
 		new_sb_pages = sb_pages;
 		RCHAN_SB_CLEAR_NOREF(new_sb_pages);
-		new_sb_pages = cmpxchg(&buf->rchan_wsb[idx].pages,
+		new_sb_pages = cmpxchg(&bufa->buf_wsb[idx].pages,
 			sb_pages, new_sb_pages);
 		if (likely(new_sb_pages == sb_pages))
 			break;
@@ -300,19 +244,18 @@ static __inline__ void ltt_clear_noref_flag(struct rchan *rchan,
 /**
  * ltt_set_noref_flag - Set the noref subbuffer flag, for writer.
  */
-static __inline__ void ltt_set_noref_flag(struct rchan *rchan,
-					  struct rchan_buf *buf,
-					  long idx)
+static __inline__
+void ltt_set_noref_flag(struct ltt_chanbuf_alloc *bufa, long idx)
 {
-	struct rchan_page *sb_pages, *new_sb_pages;
+	struct chanbuf_page *sb_pages, *new_sb_pages;
 
-	sb_pages = buf->rchan_wsb[idx].pages;
+	sb_pages = bufa->buf_wsb[idx].pages;
 	for (;;) {
 		if (RCHAN_SB_IS_NOREF(sb_pages))
 			return;	/* Already set */
 		new_sb_pages = sb_pages;
 		RCHAN_SB_SET_NOREF(new_sb_pages);
-		new_sb_pages = cmpxchg(&buf->rchan_wsb[idx].pages,
+		new_sb_pages = cmpxchg(&bufa->buf_wsb[idx].pages,
 			sb_pages, new_sb_pages);
 		if (likely(new_sb_pages == sb_pages))
 			break;
@@ -323,58 +266,39 @@ static __inline__ void ltt_set_noref_flag(struct rchan *rchan,
 /**
  * update_read_sb_index - Read-side subbuffer index update.
  */
-static __inline__ int update_read_sb_index(struct rchan_buf *buf,
-					   long consumed_idx)
+static __inline__
+int update_read_sb_index(struct ltt_chanbuf_alloc *bufa,
+			 struct ltt_chan_alloc *chana,
+			 long consumed_idx)
 {
-	struct rchan_page *old_wpage, *new_wpage;
+	struct chanbuf_page *old_wpage, *new_wpage;
 
-	if (unlikely(buf->chan->extra_reader_sb)) {
+	if (unlikely(chana->extra_reader_sb)) {
 		/*
 		 * Exchange the target writer subbuffer with our own unused
 		 * subbuffer.
 		 */
-		old_wpage = buf->rchan_wsb[consumed_idx].pages;
+		old_wpage = bufa->buf_wsb[consumed_idx].pages;
 		if (unlikely(!RCHAN_SB_IS_NOREF(old_wpage)))
 			return -EAGAIN;
-		WARN_ON_ONCE(!RCHAN_SB_IS_NOREF(buf->rchan_rsb.pages));
-		new_wpage = cmpxchg(&buf->rchan_wsb[consumed_idx].pages,
+		WARN_ON_ONCE(!RCHAN_SB_IS_NOREF(bufa->buf_rsb.pages));
+		new_wpage = cmpxchg(&bufa->buf_wsb[consumed_idx].pages,
 				old_wpage,
-				buf->rchan_rsb.pages);
+				bufa->buf_rsb.pages);
 		if (unlikely(old_wpage != new_wpage))
 			return -EAGAIN;
-		buf->rchan_rsb.pages = new_wpage;
-		RCHAN_SB_CLEAR_NOREF(buf->rchan_rsb.pages);
+		bufa->buf_rsb.pages = new_wpage;
+		RCHAN_SB_CLEAR_NOREF(bufa->buf_rsb.pages);
 	} else {
 		/* No page exchange, use the writer page directly */
-		buf->rchan_rsb.pages = buf->rchan_wsb[consumed_idx].pages;
-		RCHAN_SB_CLEAR_NOREF(buf->rchan_rsb.pages);
+		bufa->buf_rsb.pages = bufa->buf_wsb[consumed_idx].pages;
+		RCHAN_SB_CLEAR_NOREF(bufa->buf_rsb.pages);
 	}
 	return 0;
 }
 
-/*
- * CONFIG_LTT_RELAY kernel API, ltt/ltt-relay-alloc.c
- */
-
-struct rchan *ltt_relay_open(const char *base_filename,
-			 struct dentry *parent,
-			 size_t subbuf_size,
-			 size_t n_subbufs,
-			 struct rchan_callbacks *cb,
-			 void *private_data,
-			 int extra_reader_sb);
-extern void ltt_relay_close(struct rchan *chan);
-
-void ltt_relay_get_chan(struct rchan *chan);
-void ltt_relay_put_chan(struct rchan *chan);
-
-void ltt_relay_get_chan_buf(struct rchan_buf *buf);
-void ltt_relay_put_chan_buf(struct rchan_buf *buf);
-
-/*
- * exported ltt_relay file operations, ltt/ltt-relay-alloc.c
- */
-extern const struct file_operations ltt_relay_file_operations;
+ssize_t ltt_relay_file_splice_read(struct file *in, loff_t *ppos,
+				   struct pipe_inode_info *pipe, size_t len,
+				   unsigned int flags);
 
 #endif /* _LINUX_LTT_RELAY_H */
-
diff --git a/include/linux/ltt-tracer.h b/include/linux/ltt-tracer.h
index 9b7108f..ea25ad3 100644
--- a/include/linux/ltt-tracer.h
+++ b/include/linux/ltt-tracer.h
@@ -48,10 +48,10 @@ struct ltt_serialize_closure;
 struct ltt_probe_private_data;
 
 /* Serialization callback '%k' */
-typedef size_t (*ltt_serialize_cb)(struct rchan_buf *buf, size_t buf_offset,
-			struct ltt_serialize_closure *closure,
-			void *serialize_private, int *largest_align,
-			const char *fmt, va_list *args);
+typedef size_t (*ltt_serialize_cb)(struct ltt_chanbuf *buf, size_t buf_offset,
+				   struct ltt_serialize_closure *closure,
+				   void *serialize_private, int *largest_align,
+				   const char *fmt, va_list *args);
 
 struct ltt_serialize_closure {
 	ltt_serialize_cb *callbacks;
@@ -59,10 +59,10 @@ struct ltt_serialize_closure {
 	unsigned int cb_idx;
 };
 
-size_t ltt_serialize_data(struct rchan_buf *buf, size_t buf_offset,
-			struct ltt_serialize_closure *closure,
-			void *serialize_private,
-			int *largest_align, const char *fmt, va_list *args);
+size_t ltt_serialize_data(struct ltt_chanbuf *buf, size_t buf_offset,
+			  struct ltt_serialize_closure *closure,
+			  void *serialize_private, int *largest_align,
+			  const char *fmt, va_list *args);
 
 struct ltt_available_probe {
 	const char *name;		/* probe name */
@@ -73,7 +73,7 @@ struct ltt_available_probe {
 };
 
 struct ltt_probe_private_data {
-	struct ltt_trace_struct *trace;	/*
+	struct ltt_trace *trace;	/*
 					 * Target trace, for metadata
 					 * or statedump.
 					 */
@@ -116,13 +116,13 @@ struct ltt_active_marker {
 };
 
 extern void ltt_vtrace(const struct marker *mdata, void *probe_data,
-	void *call_data, const char *fmt, va_list *args);
+		       void *call_data, const char *fmt, va_list *args);
 extern void ltt_trace(const struct marker *mdata, void *probe_data,
-	void *call_data, const char *fmt, ...);
+		      void *call_data, const char *fmt, ...);
 
-size_t ltt_serialize_printf(struct rchan_buf *buf,
-		unsigned long buf_offset, size_t *msg_size,
-		char *output, size_t outlen, const char *fmt);
+size_t ltt_serialize_printf(struct ltt_chanbuf *buf, unsigned long buf_offset,
+			    size_t *msg_size, char *output, size_t outlen,
+			    const char *fmt);
 
 /*
  * Unique ID assigned to each registered probe.
@@ -154,30 +154,28 @@ struct user_dbg_data {
 
 struct ltt_trace_ops {
 	/* First 32 bytes cache-hot cacheline */
-	void (*wakeup_channel) (struct ltt_channel_struct *ltt_channel);
-	int (*user_blocking) (struct ltt_trace_struct *trace,
-				unsigned int index, size_t data_size,
-				struct user_dbg_data *dbg);
+	void (*wakeup_channel) (struct ltt_chan *chan);
+	int (*user_blocking) (struct ltt_trace *trace, unsigned int index,
+			      size_t data_size, struct user_dbg_data *dbg);
 	/* End of first 32 bytes cacheline */
-	int (*create_dirs) (struct ltt_trace_struct *new_trace);
-	void (*remove_dirs) (struct ltt_trace_struct *new_trace);
-	int (*create_channel) (const char *trace_name,
-				struct ltt_trace_struct *trace,
-				struct dentry *dir, const char *channel_name,
-				struct ltt_channel_struct *ltt_chan,
-				unsigned int subbuf_size,
-				unsigned int n_subbufs, int overwrite);
-	void (*finish_channel) (struct ltt_channel_struct *channel);
-	void (*remove_channel) (struct ltt_channel_struct *channel);
-	void (*user_errors) (struct ltt_trace_struct *trace,
-				unsigned int index, size_t data_size,
-				struct user_dbg_data *dbg, int cpu);
+	int (*create_dirs) (struct ltt_trace *new_trace);
+	void (*remove_dirs) (struct ltt_trace *new_trace);
+	int (*create_channel) (const char *channel_name, struct ltt_chan *chan,
+			       struct dentry *parent, size_t sb_size,
+			       size_t n_sb, int overwrite,
+			       struct ltt_trace *trace);
+	void (*finish_channel) (struct ltt_chan *chan);
+	void (*remove_channel) (struct kref *kref);
+	void (*user_errors) (struct ltt_trace *trace, unsigned int index,
+			     size_t data_size, struct user_dbg_data *dbg,
+			     int cpu);
+	void (*start_switch_timer) (struct ltt_chan *chan);
+	void (*stop_switch_timer) (struct ltt_chan *chan);
 #ifdef CONFIG_HOTPLUG_CPU
-	int (*handle_cpuhp) (struct notifier_block *nb,
-				unsigned long action, void *hcpu,
-				struct ltt_trace_struct *trace);
+	int (*handle_cpuhp) (struct notifier_block *nb, unsigned long action,
+			     void *hcpu, struct ltt_trace *trace);
 #endif
-} ____cacheline_aligned;
+};
 
 struct ltt_transport {
 	char *name;
@@ -192,14 +190,14 @@ enum trace_mode { LTT_TRACE_NORMAL, LTT_TRACE_FLIGHT, LTT_TRACE_HYBRID };
 #define CHANNEL_FLAG_OVERWRITE	(1U<<1)
 
 /* Per-trace information - each trace/flight recorder represented by one */
-struct ltt_trace_struct {
+struct ltt_trace {
 	/* First 32 bytes cache-hot cacheline */
 	struct list_head list;
-	struct ltt_trace_ops *ops;
+	struct ltt_chan *channels;
+	unsigned int nr_channels;
 	int active;
 	/* Second 32 bytes cache-hot cacheline */
-	struct ltt_channel_struct *channels;
-	unsigned int nr_channels;
+	struct ltt_trace_ops *ops;
 	u32 freq_scale;
 	u64 start_freq;
 	u64 start_tsc;
@@ -210,7 +208,6 @@ struct ltt_trace_struct {
 		struct dentry			*trace_root;
 		struct dentry			*ascii_root;
 	} dentry;
-	struct rchan_callbacks callbacks;
 	struct kref kref; /* Each channel has a kref of the trace struct */
 	struct ltt_transport *transport;
 	struct kref ltt_transport_kref;
@@ -300,13 +297,13 @@ struct ltt_subbuffer_header {
 };
 
 /**
- * ltt_subbuffer_header_size - called on buffer-switch to a new sub-buffer
+ * ltt_sb_header_size - called on buffer-switch to a new sub-buffer
  *
  * Return header size without padding after the structure. Don't use packed
  * structure because gcc generates inefficient code on some architectures
  * (powerpc, mips..)
  */
-static __inline__ size_t ltt_subbuffer_header_size(void)
+static __inline__ size_t ltt_sb_header_size(void)
 {
 	return offsetof(struct ltt_subbuffer_header, header_end);
 }
@@ -331,12 +328,10 @@ static __inline__ size_t ltt_subbuffer_header_size(void)
  * The payload must itself determine its own alignment from the biggest type it
  * contains.
  * */
-static __inline__ unsigned char ltt_get_header_size(
-		struct ltt_channel_struct *channel,
-		size_t offset,
-		size_t data_size,
-		size_t *before_hdr_pad,
-		unsigned int rflags)
+static __inline__
+unsigned char ltt_get_header_size(struct ltt_chan *chan, size_t offset,
+				  size_t data_size, size_t *before_hdr_pad,
+				  unsigned int rflags)
 {
 	size_t orig_offset = offset;
 	size_t padding;
@@ -371,20 +366,19 @@ static __inline__ unsigned char ltt_get_header_size(
 	return offset - orig_offset;
 }
 
-extern size_t ltt_write_event_header_slow(struct ltt_trace_struct *trace,
-		struct ltt_channel_struct *channel,
-		struct rchan_buf *buf, long buf_offset,
-		u16 eID, u32 event_size,
-		u64 tsc, unsigned int rflags);
+extern
+size_t ltt_write_event_header_slow(struct ltt_chanbuf_alloc *bufa,
+				   struct ltt_chan_alloc *chana,
+				   long buf_offset, u16 eID, u32 event_size,
+				   u64 tsc, unsigned int rflags);
 
 /*
  * ltt_write_event_header
  *
  * Writes the event header to the offset (already aligned on 32-bits).
  *
- * @trace : trace to write to.
- * @channel : pointer to the channel structure..
  * @buf : buffer to write to.
+ * @chan : pointer to the channel structure..
  * @buf_offset : buffer offset to write to (aligned on 32 bits).
  * @eID : event ID
  * @event_size : size of the event, excluding the event header.
@@ -393,11 +387,11 @@ extern size_t ltt_write_event_header_slow(struct ltt_trace_struct *trace,
  *
  * returns : offset where the event data must be written.
  */
-static __inline__ size_t ltt_write_event_header(struct ltt_trace_struct *trace,
-		struct ltt_channel_struct *channel,
-		struct rchan_buf *buf, long buf_offset,
-		u16 eID, u32 event_size,
-		u64 tsc, unsigned int rflags)
+static __inline__
+size_t ltt_write_event_header(struct ltt_chanbuf_alloc *bufa,
+			      struct ltt_chan_alloc *chana,
+			      long buf_offset, u16 eID, u32 event_size, u64 tsc,
+			      unsigned int rflags)
 {
 	struct ltt_event_header header;
 
@@ -406,28 +400,29 @@ static __inline__ size_t ltt_write_event_header(struct ltt_trace_struct *trace,
 
 	header.id_time = eID << LTT_TSC_BITS;
 	header.id_time |= (u32)tsc & LTT_TSC_MASK;
-	ltt_relay_write(buf, buf_offset, &header, sizeof(header));
+	ltt_relay_write(bufa, chana, buf_offset, &header, sizeof(header));
 	buf_offset += sizeof(header);
 
 	return buf_offset;
 
 slow_path:
-	return ltt_write_event_header_slow(trace, channel, buf, buf_offset,
-			eID, event_size, tsc, rflags);
+	return ltt_write_event_header_slow(bufa, chana, buf_offset,
+					   eID, event_size, tsc, rflags);
 }
 
 /*
  * ltt_read_event_header
  * buf_offset must aligned on 32 bits
  */
-static __inline__ size_t ltt_read_event_header(struct rchan_buf *buf,
-		long buf_offset, u64 *tsc, u32 *event_size, u16 *eID,
-		unsigned int *rflags)
+static __inline__
+size_t ltt_read_event_header(struct ltt_chanbuf_alloc *bufa, long buf_offset,
+			     u64 *tsc, u32 *event_size, u16 *eID,
+			     unsigned int *rflags)
 {
 	struct ltt_event_header header;
 	u16 small_size;
 
-	ltt_relay_read(buf, buf_offset, &header, sizeof(header));
+	ltt_relay_read(bufa, buf_offset, &header, sizeof(header));
 	buf_offset += sizeof(header);
 
 	*event_size = INT_MAX;
@@ -437,28 +432,28 @@ static __inline__ size_t ltt_read_event_header(struct rchan_buf *buf,
 	switch (*eID) {
 	case 29:
 		*rflags = LTT_RFLAG_ID_SIZE_TSC;
-		ltt_relay_read(buf, buf_offset, eID, sizeof(u16));
+		ltt_relay_read(bufa, buf_offset, eID, sizeof(u16));
 		buf_offset += sizeof(u16);
-		ltt_relay_read(buf, buf_offset, &small_size, sizeof(u16));
+		ltt_relay_read(bufa, buf_offset, &small_size, sizeof(u16));
 		buf_offset += sizeof(u16);
 		if (small_size == LTT_MAX_SMALL_SIZE) {
-			ltt_relay_read(buf, buf_offset, event_size,
+			ltt_relay_read(bufa, buf_offset, event_size,
 					sizeof(u32));
 			buf_offset += sizeof(u32);
 		} else
 			*event_size = small_size;
 		buf_offset += ltt_align(buf_offset, sizeof(u64));
-		ltt_relay_read(buf, buf_offset, tsc, sizeof(u64));
+		ltt_relay_read(bufa, buf_offset, tsc, sizeof(u64));
 		buf_offset += sizeof(u64);
 		break;
 	case 30:
 		*rflags = LTT_RFLAG_ID_SIZE;
-		ltt_relay_read(buf, buf_offset, eID, sizeof(u16));
+		ltt_relay_read(bufa, buf_offset, eID, sizeof(u16));
 		buf_offset += sizeof(u16);
-		ltt_relay_read(buf, buf_offset, &small_size, sizeof(u16));
+		ltt_relay_read(bufa, buf_offset, &small_size, sizeof(u16));
 		buf_offset += sizeof(u16);
 		if (small_size == LTT_MAX_SMALL_SIZE) {
-			ltt_relay_read(buf, buf_offset, event_size,
+			ltt_relay_read(bufa, buf_offset, event_size,
 					sizeof(u32));
 			buf_offset += sizeof(u32);
 		} else
@@ -466,7 +461,7 @@ static __inline__ size_t ltt_read_event_header(struct rchan_buf *buf,
 		break;
 	case 31:
 		*rflags = LTT_RFLAG_ID;
-		ltt_relay_read(buf, buf_offset, eID, sizeof(u16));
+		ltt_relay_read(bufa, buf_offset, eID, sizeof(u16));
 		buf_offset += sizeof(u16);
 		break;
 	default:
@@ -486,15 +481,15 @@ static __inline__ size_t ltt_read_event_header(struct rchan_buf *buf,
  * the offset, which leaves only the buffer number.
  */
 #define BUFFER_TRUNC(offset, chan) \
-	((offset) & (~((chan)->alloc_size-1)))
-#define BUFFER_OFFSET(offset, chan) ((offset) & ((chan)->alloc_size - 1))
-#define SUBBUF_OFFSET(offset, chan) ((offset) & ((chan)->subbuf_size - 1))
+	((offset) & (~((chan)->a.buf_size - 1)))
+#define BUFFER_OFFSET(offset, chan) ((offset) & ((chan)->a.buf_size - 1))
+#define SUBBUF_OFFSET(offset, chan) ((offset) & ((chan)->a.sb_size - 1))
 #define SUBBUF_ALIGN(offset, chan) \
-	(((offset) + (chan)->subbuf_size) & (~((chan)->subbuf_size - 1)))
+	(((offset) + (chan)->a.sb_size) & (~((chan)->a.sb_size - 1)))
 #define SUBBUF_TRUNC(offset, chan) \
-	((offset) & (~((chan)->subbuf_size - 1)))
+	((offset) & (~((chan)->a.sb_size - 1)))
 #define SUBBUF_INDEX(offset, chan) \
-	(BUFFER_OFFSET((offset), chan) >> (chan)->subbuf_size_order)
+	(BUFFER_OFFSET((offset), chan) >> (chan)->a.sb_size_order)
 
 /*
  * Control channels :
@@ -505,8 +500,8 @@ static __inline__ size_t ltt_read_event_header(struct rchan_buf *buf,
  * cpu channel :
  * cpu
  */
-#define LTT_RELAY_ROOT		"ltt"
-#define LTT_RELAY_LOCKED_ROOT	"ltt-locked"
+#define LTT_RELAY_ROOT			"ltt"
+#define LTT_RELAY_LOCKED_ROOT		"ltt-locked"
 
 #define LTT_METADATA_CHANNEL		"metadata_state"
 #define LTT_FD_STATE_CHANNEL		"fd_state"
@@ -526,7 +521,7 @@ static __inline__ size_t ltt_read_event_header(struct rchan_buf *buf,
 #define LTT_MM_CHANNEL			"mm"
 #define LTT_RCU_CHANNEL			"rcu"
 
-#define LTT_FLIGHT_PREFIX	"flight-"
+#define LTT_FLIGHT_PREFIX		"flight-"
 
 #define LTT_ASCII			"ascii"
 
@@ -546,8 +541,9 @@ static __inline__ size_t ltt_read_event_header(struct rchan_buf *buf,
  * @trace: Trace information
  * @header: Memory address where the information must be written to
  */
-static __inline__ void ltt_write_trace_header(struct ltt_trace_struct *trace,
-		struct ltt_subbuffer_header *header)
+static __inline__
+void ltt_write_trace_header(struct ltt_trace *trace,
+			    struct ltt_subbuffer_header *header)
 {
 	header->magic_number = LTT_TRACER_MAGIC_NUMBER;
 	header->major_version = LTT_TRACER_VERSION_MAJOR;
@@ -577,7 +573,7 @@ enum ltt_module_function {
 };
 
 extern int ltt_module_register(enum ltt_module_function name, void *function,
-		struct module *owner);
+			       struct module *owner);
 extern void ltt_module_unregister(enum ltt_module_function name);
 
 void ltt_transport_register(struct ltt_transport *transport);
@@ -606,25 +602,30 @@ union ltt_control_args {
 
 int _ltt_trace_setup(const char *trace_name);
 int ltt_trace_setup(const char *trace_name);
-struct ltt_trace_struct *_ltt_trace_find_setup(const char *trace_name);
+struct ltt_trace *_ltt_trace_find_setup(const char *trace_name);
 int ltt_trace_set_type(const char *trace_name, const char *trace_type);
 int ltt_trace_set_channel_subbufsize(const char *trace_name,
-		const char *channel_name, unsigned int size);
+				     const char *channel_name,
+				     unsigned int size);
 int ltt_trace_set_channel_subbufcount(const char *trace_name,
-		const char *channel_name, unsigned int cnt);
+				      const char *channel_name,
+				      unsigned int cnt);
 int ltt_trace_set_channel_switch_timer(const char *trace_name,
-		const char *channel_name, unsigned long interval);
+				       const char *channel_name,
+				       unsigned long interval);
 int ltt_trace_set_channel_enable(const char *trace_name,
-		const char *channel_name, unsigned int enable);
+				 const char *channel_name,
+				 unsigned int enable);
 int ltt_trace_set_channel_overwrite(const char *trace_name,
-		const char *channel_name, unsigned int overwrite);
+				    const char *channel_name,
+				    unsigned int overwrite);
 int ltt_trace_alloc(const char *trace_name);
 int ltt_trace_destroy(const char *trace_name);
 int ltt_trace_start(const char *trace_name);
 int ltt_trace_stop(const char *trace_name);
 
 extern int ltt_control(enum ltt_control_msg msg, const char *trace_name,
-		const char *trace_type, union ltt_control_args args);
+		       const char *trace_type, union ltt_control_args args);
 
 enum ltt_filter_control_msg {
 	LTT_FILTER_DEFAULT_ACCEPT,
@@ -632,12 +633,10 @@ enum ltt_filter_control_msg {
 };
 
 extern int ltt_filter_control(enum ltt_filter_control_msg msg,
-		const char *trace_name);
+			      const char *trace_name);
 
 extern struct dentry *get_filter_root(void);
 
-extern void ltt_buffer_destroy(struct ltt_channel_struct *ltt_chan);
-
 void ltt_core_register(int (*function)(u8, void *));
 
 void ltt_core_unregister(void);
@@ -648,47 +647,43 @@ void ltt_release_transport(struct kref *kref);
 extern int ltt_probe_register(struct ltt_available_probe *pdata);
 extern int ltt_probe_unregister(struct ltt_available_probe *pdata);
 extern int ltt_marker_connect(const char *channel, const char *mname,
-		const char *pname);
+			      const char *pname);
 extern int ltt_marker_disconnect(const char *channel, const char *mname,
-		const char *pname);
-extern void ltt_dump_marker_state(struct ltt_trace_struct *trace);
+				 const char *pname);
+extern void ltt_dump_marker_state(struct ltt_trace *trace);
 
 void ltt_lock_traces(void);
 void ltt_unlock_traces(void);
 
 #ifdef CONFIG_LTT_ASCII
-extern int ltt_ascii_create_dir(struct ltt_trace_struct *new_trace);
-extern void ltt_ascii_remove_dir(struct ltt_trace_struct *trace);
-extern struct dentry *ltt_ascii_create(struct ltt_trace_struct *trace,
-		struct ltt_channel_struct *ltt_channel);
-extern void ltt_ascii_remove(struct ltt_channel_struct *ltt_channel,
-		struct dentry *ascii);
+extern int ltt_ascii_create_dir(struct ltt_trace *new_trace);
+extern void ltt_ascii_remove_dir(struct ltt_trace *trace);
+extern int ltt_ascii_create(struct ltt_chan *chan);
+extern void ltt_ascii_remove(struct ltt_chan *chan);
 #else
-static inline int ltt_ascii_create_dir(struct ltt_trace_struct *new_trace)
+static inline int ltt_ascii_create_dir(struct ltt_trace *new_trace)
 {
 	return 0;
 }
 
-static inline void ltt_ascii_remove_dir(struct ltt_trace_struct *trace)
+static inline void ltt_ascii_remove_dir(struct ltt_trace *trace)
 {
 }
 
-static inline struct dentry *ltt_ascii_create(struct ltt_trace_struct *trace,
-		struct ltt_channel_struct *ltt_channel)
+static inline int ltt_ascii_create(struct ltt_chan *chan)
 {
-	return NULL;
+	return 0;
 }
 
-static inline void ltt_ascii_remove(struct ltt_channel_struct *ltt_channel,
-		struct dentry *ascii)
+static inline void ltt_ascii_remove(struct ltt_chan *chan)
 {
 }
 #endif
 
-extern void ltt_statedump_register_kprobes_dump(
-	void (*callback)(void *call_data));
-extern void ltt_statedump_unregister_kprobes_dump(
-	void (*callback)(void *call_data));
+extern
+void ltt_statedump_register_kprobes_dump(void (*callback)(void *call_data));
+extern
+void ltt_statedump_unregister_kprobes_dump(void (*callback)(void *call_data));
 
 extern void ltt_dump_softirq_vec(void *call_data);
 
diff --git a/include/linux/marker.h b/include/linux/marker.h
index debfe9a..c92a369 100644
--- a/include/linux/marker.h
+++ b/include/linux/marker.h
@@ -15,7 +15,6 @@
 #include <stdarg.h>
 #include <linux/types.h>
 #include <linux/immediate.h>
-#include <linux/ltt-channels.h>
 
 struct module;
 struct marker;
diff --git a/kernel/marker.c b/kernel/marker.c
index 6e9e288..b304c0a 100644
--- a/kernel/marker.c
+++ b/kernel/marker.c
@@ -1267,7 +1267,7 @@ __initcall(init_markers);
 
 #endif /* CONFIG_MODULES */
 
-void ltt_dump_marker_state(struct ltt_trace_struct *trace)
+void ltt_dump_marker_state(struct ltt_trace *trace)
 {
 	struct marker_entry *entry;
 	struct ltt_probe_private_data call_data;
diff --git a/ltt/Kconfig b/ltt/Kconfig
index 8a8ab54..959a1b5 100644
--- a/ltt/Kconfig
+++ b/ltt/Kconfig
@@ -34,7 +34,7 @@ config LTT_FILTER
 config HAVE_LTT_DUMP_TABLES
 	def_bool n
 
-config LTT_RELAY_ALLOC
+config LTT_RELAY
 	def_bool n
 
 choice
@@ -47,7 +47,7 @@ choice
 
 	config LTT_RELAY_LOCKLESS
 		bool "Linux Trace Toolkit High-speed Lockless Data Relay"
-	select LTT_RELAY_ALLOC
+	select LTT_RELAY
 	select DEBUG_FS
 	help
 	  Support using the fast lockless algorithm to log the data obtained
@@ -58,16 +58,18 @@ choice
 
 	config LTT_RELAY_IRQOFF
 		bool "Linux Trace Toolkit Irq-off Data Relay"
-	select LTT_RELAY_ALLOC
+	select LTT_RELAY
 	select DEBUG_FS
+	depends on BROKEN
 	help
 	  Support using interrupt disable algorithm to log the data obtained
 	  through LTT.
 
 	config LTT_RELAY_LOCKED
 		bool "Linux Trace Toolkit Lock-Protected Data Relay"
-	select LTT_RELAY_ALLOC
+	select LTT_RELAY
 	select DEBUG_FS
+	depends on BROKEN
 	help
 	  Support using the slow spinlock and interrupt disable algorithm to log
 	  the data obtained through LTT.
@@ -76,7 +78,7 @@ endchoice
 
 config LTT_SERIALIZE
 	tristate "Linux Trace Toolkit Serializer"
-	depends on LTT_RELAY_ALLOC
+	depends on LTT_RELAY
 	depends on (LTT_RELAY_LOCKLESS || LTT_RELAY_IRQOFF || LTT_RELAY_LOCKED)
 	default y
 	help
@@ -85,7 +87,7 @@ config LTT_SERIALIZE
 
 config LTT_FAST_SERIALIZE
 	tristate "Linux Trace Toolkit Custom Serializer"
-	depends on LTT_RELAY_ALLOC
+	depends on LTT_RELAY
 	depends on (LTT_RELAY_LOCKLESS || LTT_RELAY_IRQOFF || LTT_RELAY_LOCKED)
 	default y
 	help
@@ -211,7 +213,7 @@ config LTT_ASCII
 	bool "Linux Trace Toolkit Ascii Output (EXPERIMENTAL)"
 	depends on EXPERIMENTAL
 	depends on LTT_TRACER
-	depends on LTT_RELAY_ALLOC
+	depends on LTT_RELAY
 	depends on LTT_SERIALIZE
 	default n
 	help
diff --git a/ltt/Makefile b/ltt/Makefile
index cc6fa92..88d3d46 100644
--- a/ltt/Makefile
+++ b/ltt/Makefile
@@ -8,18 +8,21 @@ obj-$(CONFIG_LTT_TRACER)		+= ltt-tracer.o
 obj-$(CONFIG_LTT_TRACE_CONTROL)		+= ltt-marker-control.o
 
 ifdef CONFIG_LTT_RELAY_LOCKLESS
-obj-$(CONFIG_LTT_TRACER)		+= ltt-relay-lockless.o
+RELAY_LOCKING := ltt-relay-lockless.o
 endif
 
 ifdef CONFIG_LTT_RELAY_IRQOFF
-obj-$(CONFIG_LTT_TRACER)		+= ltt-relay-irqoff.o
+RELAY_LOCKING := ltt-relay-irqoff.o
 endif
 
 ifdef CONFIG_LTT_RELAY_LOCKED
-obj-$(CONFIG_LTT_TRACER)		+= ltt-relay-locked.o
+RELAY_LOCKING := ltt-relay-locked.o
 endif
 
-obj-$(CONFIG_LTT_RELAY_ALLOC)		+= ltt-relay-alloc.o
+obj-$(CONFIG_LTT_RELAY) += ltt-relay.o
+ltt-relay-objs := $(RELAY_LOCKING) ltt-relay-alloc.o ltt-relay-splice.o \
+		  ltt-relay-vfs.o
+
 obj-$(CONFIG_LTT_SERIALIZE)		+= ltt-serialize.o
 obj-$(CONFIG_LTT_STATEDUMP)		+= ltt-statedump.o
 obj-$(CONFIG_LTT_FAST_SERIALIZE)	+= ltt-type-serializer.o
diff --git a/ltt/ltt-ascii.c b/ltt/ltt-ascii.c
index ea6dcd3..61f6f29 100644
--- a/ltt/ltt-ascii.c
+++ b/ltt/ltt-ascii.c
@@ -43,6 +43,8 @@
 #include <linux/cpu.h>
 #include <linux/fs.h>
 
+#include "ltt-relay-select.h"
+
 #if 0
 #define DEBUGP printk
 #else
@@ -56,7 +58,7 @@ struct ltt_relay_iter;
 
 struct ltt_relay_cpu_iter {
 	/* cpu buffer information */
-	struct rchan_buf *buf;
+	struct ltt_chanbuf *buf;
 	struct ltt_relay_iter *iter;
 	int sb_ref;		/* holding a reference to a subbuffer */
 	long read_sb_offset;	/* offset of the subbuffer read */
@@ -73,7 +75,7 @@ struct ltt_relay_cpu_iter {
 
 struct ltt_relay_iter {
 	struct ltt_relay_cpu_iter iter_cpu[NR_CPUS];
-	struct ltt_channel_struct *ltt_channel;
+	struct ltt_chan *chan;
 	loff_t pos;
 	int cpu;
 	int nr_refs;
@@ -85,7 +87,7 @@ static int is_subbuffer_offset_end(struct ltt_relay_cpu_iter *citer,
 	long sub_offset = SUBBUF_OFFSET(offset, citer->buf->chan);
 
 	return (sub_offset + citer->header->lost_size
-			>= citer->buf->chan->subbuf_size);
+			>= citer->buf->chan->sb_size);
 }
 
 static u64 calculate_tsc(u64 pre_tsc, u64 read_tsc, unsigned int rflags)
@@ -126,10 +128,12 @@ static void update_new_event(struct ltt_relay_cpu_iter *citer, long hdr_offset)
 
 	WARN_ON_ONCE(hdr_offset != citer->hdr_offset);
 
-	tmp_offset = ltt_read_event_header(citer->buf, hdr_offset,
-			&read_tsc, &citer->data_size, &citer->eID, &rflags);
+	tmp_offset = ltt_read_event_header(&citer->buf.a, hdr_offset,
+					   &read_tsc, &citer->data_size,
+					   &citer->eID, &rflags);
 	citer->payload_offset = calculate_payload_offset(tmp_offset,
-			citer->chID, citer->eID);
+							 citer->chID,
+							 citer->eID);
 
 	citer->tsc = calculate_tsc(citer->tsc, read_tsc, rflags);
 }
@@ -169,13 +173,10 @@ static int subbuffer_start(struct ltt_relay_cpu_iter *citer, long *offset)
 {
 	int ret;
 	struct ltt_relay_iter *iter = citer->iter;
-	struct ltt_channel_buf_access_ops *buf_access_ops;
-
-	buf_access_ops = iter->ltt_channel->buf_access_ops;
 
-	ret = buf_access_ops->get_subbuf(citer->buf, offset);
+	ret = ltt_chanbuf_get_subbuf(citer->buf, offset);
 	if (!ret) {
-		citer->header = ltt_relay_read_offset_address(citer->buf,
+		citer->header = ltt_relay_read_offset_address(&citer->buf.a,
 							      *offset);
 		citer->hdr_offset = (*offset) + ltt_subbuffer_header_size();
 		citer->tsc = citer->header->cycle_count_begin;
@@ -183,12 +184,12 @@ static int subbuffer_start(struct ltt_relay_cpu_iter *citer, long *offset)
 		citer->sb_ref = 1;
 		return 0;
 	} else {
-		if (buf_access_ops->is_finalized(citer->buf)) {
+		if (is_finalized(citer->buf)) {
 			/*
 			 * Currently, kill the iterator for
 			 * this cpu buffer (TODO resume support)
 			 */
-			buf_access_ops->release(citer->buf);
+			ltt_chanbuf_release_read(citer->buf);
 			citer->buf = NULL;
 			return -ENODATA;
 		}
@@ -203,8 +204,7 @@ static void subbuffer_stop(struct ltt_relay_cpu_iter *citer,
 	struct ltt_relay_iter *iter = citer->iter;
 
 	WARN_ON_ONCE(!citer->sb_ref);
-	ret = iter->ltt_channel->buf_access_ops->put_subbuf(
-			citer->buf, offset);
+	ret = ltt_chanbuf_put_subbuf(citer->buf, offset);
 	WARN_ON_ONCE(ret);
 	citer->sb_ref = 0;
 	iter->nr_refs--;
@@ -220,13 +220,13 @@ static void ltt_relay_advance_cpu_iter(struct ltt_relay_cpu_iter *citer)
 	if (unlikely(is_subbuffer_offset_end(citer,
 					     old_offset + citer->data_size))) {
 		DEBUGP(KERN_DEBUG "LTT ASCII stop cpu %d offset %lX\n",
-			citer->buf->cpu, citer->read_sb_offset);
+		       citer->buf->cpu, citer->read_sb_offset);
 		subbuffer_stop(citer, citer->read_sb_offset);
 		for (;;) {
 			ret = subbuffer_start(citer, &citer->read_sb_offset);
 			DEBUGP(KERN_DEBUG
-				"LTT ASCII start cpu %d ret %d offset %lX\n",
-				citer->buf->cpu, ret, citer->read_sb_offset);
+			       "LTT ASCII start cpu %d ret %d offset %lX\n",
+			       citer->buf->cpu, ret, citer->read_sb_offset);
 			if (!ret || ret == -ENODATA) {
 				break;	/* got data, or finalized */
 			} else {	/* -EAGAIN */
@@ -237,11 +237,10 @@ static void ltt_relay_advance_cpu_iter(struct ltt_relay_cpu_iter *citer)
 		}
 	} else {
 		new_offset += citer->data_size;
-		citer->hdr_offset = new_offset + ltt_align(new_offset,
-				sizeof(struct ltt_event_header));
+		citer->hdr_offset = new_offset + ltt_align(new_offset, sizeof(struct ltt_event_header));
 		DEBUGP(KERN_DEBUG
-			"LTT ASCII old_offset %lX new_offset %lX cpu %d\n",
-			old_offset, new_offset, citer->buf->cpu);
+		       "LTT ASCII old_offset %lX new_offset %lX cpu %d\n",
+		       old_offset, new_offset, citer->buf->cpu);
 	}
 
 	update_cpu_iter(citer, citer->hdr_offset);
@@ -312,14 +311,16 @@ static void ascii_stop(struct seq_file *m, void *v)
 {
 }
 
-static int seq_serialize(struct seq_file *m, struct rchan_buf *buf,
-		size_t buf_offset, const char *fmt, size_t *data_size)
+static
+int seq_serialize(struct seq_file *m, struct ltt_chanbuf *buf,
+		  size_t buf_offset, const char *fmt, size_t *data_size)
 {
 	int len;
 
 	if (m->count < m->size) {
 		len = ltt_serialize_printf(buf, buf_offset, data_size,
-				m->buf + m->count, m->size - m->count, fmt);
+					   m->buf + m->count,
+					   m->size - m->count, fmt);
 		if (m->count + len < m->size) {
 			m->count += len;
 			return 0;
@@ -377,27 +378,23 @@ static struct seq_operations ascii_seq_ops = {
 
 /* FIXME : cpu hotplug support */
 static int ltt_relay_iter_open_channel(struct ltt_relay_iter *iter,
-		struct ltt_channel_struct *ltt_channel)
+				       struct ltt_chan *chan)
 {
 	int i, ret;
-	struct rchan *chan = ltt_channel->trans_channel_data;
 	u16 chID = ltt_channels_get_index_from_name(ltt_channel->channel_name);
-	struct ltt_channel_buf_access_ops *buf_access_ops;
-
-	buf_access_ops = ltt_channel->buf_access_ops;
 
 	/* we don't need lock relay_channels_mutex */
 	for_each_possible_cpu(i) {
 		struct ltt_relay_cpu_iter *citer = &iter->iter_cpu[i];
 
-		citer->buf = chan->buf[i];
+		citer->buf = per_cpu_ptr(chan.a->buf, i);
 		if (!citer->buf)
 			continue;
 
 		citer->iter = iter;	/* easy lazy parent info */
 		citer->chID = chID;
 
-		ret = buf_access_ops->open(citer->buf);
+		ret = ltt_chanbuf_open_read(citer->buf);
 		if (ret) {
 			/* Failed to open a percpu buffer, close everything. */
 			citer->buf = NULL;
@@ -430,7 +427,7 @@ error:
 		struct ltt_relay_cpu_iter *citer = &iter->iter_cpu[i];
 
 		if (citer->buf)
-			buf_access_ops->release(citer->buf);
+			ltt_chanbuf_release_read(citer->buf);
 	}
 	return ret;
 }
@@ -438,9 +435,6 @@ error:
 static int ltt_relay_iter_release_channel(struct ltt_relay_iter *iter)
 {
 	int i;
-	struct ltt_channel_buf_access_ops *buf_access_ops;
-
-	buf_access_ops = iter->ltt_channel->buf_access_ops;
 
 	for_each_possible_cpu(i) {
 		struct ltt_relay_cpu_iter *citer = &iter->iter_cpu[i];
@@ -451,10 +445,10 @@ static int ltt_relay_iter_release_channel(struct ltt_relay_iter *iter)
 				"LTT ASCII release stop cpu %d offset %lX\n",
 				citer->buf->cpu, citer->read_sb_offset);
 			subbuffer_stop(&iter->iter_cpu[i],
-			       citer->read_sb_offset);
+				       citer->read_sb_offset);
 		}
 		if (citer->buf)
-			buf_access_ops->release(citer->buf);
+			ltt_chanbuf_release_read(citer->buf);
 	}
 	WARN_ON_ONCE(iter->nr_refs);
 	return 0;
@@ -463,13 +457,13 @@ static int ltt_relay_iter_release_channel(struct ltt_relay_iter *iter)
 static int ltt_relay_ascii_open(struct inode *inode, struct file *file)
 {
 	int ret;
-	struct ltt_channel_struct *ltt_channel = inode->i_private;
+	struct ltt_chan *chan = inode->i_private;
 	struct ltt_relay_iter *iter = kzalloc(sizeof(*iter), GFP_KERNEL);
 	if (!iter)
 		return -ENOMEM;
 
-	iter->ltt_channel = ltt_channel;
-	ret = ltt_relay_iter_open_channel(iter, ltt_channel);
+	iter->chan = chan;
+	ret = ltt_relay_iter_open_channel(iter, chan);
 	if (ret)
 		goto error_free_alloc;
 
@@ -505,38 +499,37 @@ static struct file_operations ltt_ascii_fops =
 	.owner = THIS_MODULE,
 };
 
-struct dentry *ltt_ascii_create(struct ltt_trace_struct *trace,
-		struct ltt_channel_struct *ltt_channel)
+int ltt_ascii_create(struct ltt_chan *chan)
 {
-	struct dentry *entry = debugfs_create_file(ltt_channel->channel_name,
-			S_IRUSR | S_IRGRP, trace->dentry.ascii_root,
-			ltt_channel,
-			&ltt_ascii_fops);
-	if (entry)
-		entry->d_inode->i_private = ltt_channel;
-	return entry;
+	struct dentry *dentry;
+
+	dentry = debugfs_create_file(chan->a.filename,
+				     S_IRUSR | S_IRGRP,
+				     trace->dentry.ascii_root,
+				     chan, &ltt_ascii_fops);
+	if (dentry)
+		dentry->d_inode->i_private = chan;
+	return PTR_ERR(dentry);
 }
 EXPORT_SYMBOL_GPL(ltt_ascii_create);
 
-void ltt_ascii_remove(struct ltt_channel_struct *ltt_channel,
-		struct dentry *ascii)
+void ltt_ascii_remove(struct ltt_chan *chan)
 {
-	if (ascii)
-		debugfs_remove(ascii);
+	debugfs_remove(chan->ascii_dentry);
 }
 EXPORT_SYMBOL_GPL(ltt_ascii_remove);
 
-int ltt_ascii_create_dir(struct ltt_trace_struct *new_trace)
+int ltt_ascii_create_dir(struct ltt_trace *new_trace)
 {
 	new_trace->dentry.ascii_root = debugfs_create_dir(new_trace->trace_name,
-		ltt_ascii_dir_dentry);
+							  ltt_ascii_dir_dentry);
 	if (!new_trace->dentry.ascii_root)
 		return -EEXIST;
 	return 0;
 }
 EXPORT_SYMBOL_GPL(ltt_ascii_create_dir);
 
-void ltt_ascii_remove_dir(struct ltt_trace_struct *trace)
+void ltt_ascii_remove_dir(struct ltt_trace *trace)
 {
 	debugfs_remove(trace->dentry.ascii_root);
 }
diff --git a/ltt/ltt-channels.c b/ltt/ltt-channels.c
index 4227c0f..e690cc0 100644
--- a/ltt/ltt-channels.c
+++ b/ltt/ltt-channels.c
@@ -158,12 +158,12 @@ EXPORT_SYMBOL_GPL(ltt_channels_unregister);
 /**
  * ltt_channels_set_default - Set channel default behavior.
  * @name: default channel name
- * @subbuf_size: size of the subbuffers
- * @subbuf_cnt: number of subbuffers
+ * @sb_size: size of the subbuffers
+ * @n_sb: number of subbuffers
  */
 int ltt_channels_set_default(const char *name,
-			     unsigned int subbuf_size,
-			     unsigned int subbuf_cnt)
+			     unsigned int sb_size,
+			     unsigned int n_sb)
 {
 	struct ltt_channel_setting *setting;
 	int ret = 0;
@@ -174,8 +174,8 @@ int ltt_channels_set_default(const char *name,
 		ret = -ENOENT;
 		goto end;
 	}
-	setting->subbuf_size = subbuf_size;
-	setting->subbuf_cnt = subbuf_cnt;
+	setting->sb_size = sb_size;
+	setting->n_sb = n_sb;
 end:
 	mutex_unlock(&ltt_channel_mutex);
 	return ret;
@@ -234,19 +234,18 @@ EXPORT_SYMBOL_GPL(ltt_channels_get_index_from_name);
 
 /**
  * ltt_channels_trace_alloc - Allocate channel structures for a trace
- * @subbuf_size: subbuffer size. 0 uses default.
- * @subbuf_cnt: number of subbuffers per per-cpu buffers. 0 uses default.
+ * @sb_size: subbuffer size. 0 uses default.
+ * @n_sb: number of subbuffers per per-cpu buffers. 0 uses default.
  * @flags: Default channel flags
  *
  * Use the current channel list to allocate the channels for a trace.
  * Called with trace lock held. Does not perform the trace buffer allocation,
  * because we must let the user overwrite specific channel sizes.
  */
-struct ltt_channel_struct *ltt_channels_trace_alloc(unsigned int *nr_channels,
-						    int overwrite,
-						    int active)
+struct ltt_chan *ltt_channels_trace_alloc(unsigned int *nr_channels,
+					  int overwrite, int active)
 {
-	struct ltt_channel_struct *channel = NULL;
+	struct ltt_chan *chan = NULL;
 	struct ltt_channel_setting *iter;
 
 	lock_markers();
@@ -258,24 +257,23 @@ struct ltt_channel_struct *ltt_channels_trace_alloc(unsigned int *nr_channels,
 	else
 		kref_get(&index_kref);
 	*nr_channels = free_index;
-	channel = kzalloc(sizeof(struct ltt_channel_struct) * free_index,
-			  GFP_KERNEL);
-	if (!channel)
+	chan = kzalloc(sizeof(struct ltt_chan) * free_index, GFP_KERNEL);
+	if (!chan)
 		goto end;
 	list_for_each_entry(iter, &ltt_channels, list) {
 		if (!atomic_read(&iter->kref.refcount))
 			continue;
-		channel[iter->index].subbuf_size = iter->subbuf_size;
-		channel[iter->index].subbuf_cnt = iter->subbuf_cnt;
-		channel[iter->index].overwrite = overwrite;
-		channel[iter->index].active = active;
-		channel[iter->index].channel_name = iter->name;
-		channel[iter->index].switch_timer_interval = 0;
+		chan[iter->index].a.sb_size = iter->sb_size;
+		chan[iter->index].a.n_sb = iter->n_sb;
+		chan[iter->index].overwrite = overwrite;
+		chan[iter->index].active = active;
+		strncpy(chan[iter->index].a.filename, iter->name, NAME_MAX - 1);
+		chan[iter->index].switch_timer_interval = 0;
 	}
 end:
 	mutex_unlock(&ltt_channel_mutex);
 	unlock_markers();
-	return channel;
+	return chan;
 }
 EXPORT_SYMBOL_GPL(ltt_channels_trace_alloc);
 
@@ -286,8 +284,8 @@ EXPORT_SYMBOL_GPL(ltt_channels_trace_alloc);
  * Called with trace lock held. The actual channel buffers must be freed before
  * this function is called.
  */
-void ltt_channels_trace_free(struct ltt_channel_struct *channels,
-		unsigned int nr_channels)
+void ltt_channels_trace_free(struct ltt_chan *channels,
+			     unsigned int nr_channels)
 {
 	lock_markers();
 	mutex_lock(&ltt_channel_mutex);
@@ -304,39 +302,13 @@ EXPORT_SYMBOL_GPL(ltt_channels_trace_free);
  * @interval: interval of timer interrupt, in jiffies. 0 inhibits timer.
  */
 
-void ltt_channels_trace_set_timer(struct ltt_channel_struct *channel,
-	unsigned long interval)
+void ltt_channels_trace_set_timer(struct ltt_chan *chan,
+				  unsigned long interval)
 {
-	channel->switch_timer_interval = interval;
+	chan->switch_timer_interval = interval;
 }
 EXPORT_SYMBOL_GPL(ltt_channels_trace_set_timer);
 
-/*
- * called with trace lock held.
- */
-void ltt_channels_trace_start_timer(struct ltt_channel_struct *channels,
-	unsigned int nr_channels)
-{
-	int i;
-
-	for (i = 0; i < nr_channels; i++)
-		channels[i].buf_access_ops->start_switch_timer(&channels[i]);
-}
-EXPORT_SYMBOL_GPL(ltt_channels_trace_start_timer);
-
-/*
- * called with trace lock held.
- */
-void ltt_channels_trace_stop_timer(struct ltt_channel_struct *channels,
-	unsigned int nr_channels)
-{
-	int i;
-
-	for (i = 0; i < nr_channels; i++)
-		channels[i].buf_access_ops->stop_switch_timer(&channels[i]);
-}
-EXPORT_SYMBOL_GPL(ltt_channels_trace_stop_timer);
-
 /**
  * _ltt_channels_get_event_id - get next event ID for a marker
  * @channel: channel name
diff --git a/ltt/ltt-filter.c b/ltt/ltt-filter.c
index 95c43dd..255ef08 100644
--- a/ltt/ltt-filter.c
+++ b/ltt/ltt-filter.c
@@ -33,8 +33,8 @@ struct dentry *get_filter_root(void)
 						    ltt_root_dentry);
 		if (!ltt_filter_dir)
 			printk(KERN_ERR
-				"ltt_filter_init: failed to create dir %s\n",
-				LTT_FILTER_DIR);
+			       "ltt_filter_init: failed to create dir %s\n",
+			       LTT_FILTER_DIR);
 	}
 err_no_root:
 	mutex_unlock(&ltt_filter_mutex);
diff --git a/ltt/ltt-ftrace.c b/ltt/ltt-ftrace.c
index 6da7492..47c1b72 100644
--- a/ltt/ltt-ftrace.c
+++ b/ltt/ltt-ftrace.c
@@ -30,8 +30,8 @@ static DEFINE_PER_CPU(int, system_tracing_cpu);
 static atomic_t system_trace_refcount __read_mostly;
 
 
-static notrace void
-ltt_tracer_call(unsigned long ip, unsigned long parent_ip)
+static notrace
+void ltt_tracer_call(unsigned long ip, unsigned long parent_ip)
 {
 	int cpu = raw_smp_processor_id();
 	if (likely(!per_cpu(tracing_cpu, cpu)
@@ -41,13 +41,13 @@ ltt_tracer_call(unsigned long ip, unsigned long parent_ip)
 		   ip, parent_ip);
 }
 
-static notrace void ltt_tap_marker(const struct marker *mdata,
-	void *probe_data, void *call_data,
-	const char *fmt, va_list *args)
+static notrace
+void ltt_tap_marker(const struct marker *mdata, void *probe_data,
+		    void *call_data, const char *fmt, va_list *args)
 {
 	int cpu = raw_smp_processor_id();
 	if (likely(!per_cpu(tracing_cpu, cpu)
-			&& !atomic_read(&system_trace_refcount)))
+	    && !atomic_read(&system_trace_refcount)))
 		return;
 	ltt_vtrace(mdata, probe_data, call_data, fmt, args);
 }
@@ -63,9 +63,9 @@ static struct ftrace_ops trace_ops __read_mostly =
 	.func = ltt_tracer_call,
 };
 
-static notrace void ftrace_cpu_start(const struct marker *mdata,
-	void *probe_data, void *call_data,
-	const char *fmt, va_list *args)
+static notrace
+void ftrace_cpu_start(const struct marker *mdata, void *probe_data,
+		      void *call_data, const char *fmt, va_list *args)
 {
 	int cpu = raw_smp_processor_id();
 	per_cpu(tracing_cpu, cpu) = 1;
@@ -77,9 +77,9 @@ struct ltt_available_probe ftrace_cpu_start_probe = {
 	.probe_func = ftrace_cpu_start,
 };
 
-static notrace void ftrace_cpu_stop(const struct marker *mdata,
-	void *probe_data, void *call_data,
-	const char *fmt, va_list *args)
+static notrace
+void ftrace_cpu_stop(const struct marker *mdata, void *probe_data,
+		     void *call_data, const char *fmt, va_list *args)
 {
 	int cpu = raw_smp_processor_id();
 	per_cpu(tracing_cpu, cpu) = 0;
@@ -91,9 +91,9 @@ struct ltt_available_probe ftrace_cpu_stop_probe = {
 	.probe_func = ftrace_cpu_stop,
 };
 
-static notrace void ftrace_system_start(const struct marker *mdata,
-	void *probe_data, void *call_data,
-	const char *fmt, va_list *args)
+static notrace
+void ftrace_system_start(const struct marker *mdata, void *probe_data,
+			 void *call_data, const char *fmt, va_list *args)
 {
 	int cpu = raw_smp_processor_id();
 	int value = xchg(&per_cpu(system_tracing_cpu, cpu), 1);
@@ -107,9 +107,9 @@ struct ltt_available_probe ftrace_system_start_probe = {
 	.probe_func = ftrace_system_start,
 };
 
-static notrace void ftrace_system_stop(const struct marker *mdata,
-	void *probe_data, void *call_data,
-	const char *fmt, va_list *args)
+static notrace
+void ftrace_system_stop(const struct marker *mdata, void *probe_data,
+			void *call_data, const char *fmt, va_list *args)
 {
 	int cpu = raw_smp_processor_id();
 	int value = xchg(&per_cpu(system_tracing_cpu, cpu), 0);
diff --git a/ltt/ltt-kprobes.c b/ltt/ltt-kprobes.c
index d29289d..4ff4716 100644
--- a/ltt/ltt-kprobes.c
+++ b/ltt/ltt-kprobes.c
@@ -47,22 +47,21 @@ static int module_exit;
 
 static void trace_kprobe_table_entry(void *call_data, struct kprobe_entry *e)
 {
-	char namebuf[KSYM_NAME_LEN];
 	unsigned long addr;
+	char *namebuf = (char *)__get_free_page(GFP_KERNEL);
 
 	if (e->kp.addr) {
-		sprint_symbol(namebuf,
-			      (unsigned long)e->kp.addr);
+		sprint_symbol(namebuf, (unsigned long)e->kp.addr);
 		addr = (unsigned long)e->kp.addr;
 	} else {
-		strcpy(namebuf, e->kp.symbol_name);
+		strncpy(namebuf, e->kp.symbol_name, PAGE_SIZE - 1);
 		/* TODO : add offset */
 		addr = kallsyms_lookup_name(namebuf);
 	}
 	if (addr)
-		__trace_mark(0, kprobe_state, kprobe_table,
-			call_data,
-			"ip 0x%lX symbol %s", addr, namebuf);
+		__trace_mark(0, kprobe_state, kprobe_table, call_data,
+			     "ip 0x%lX symbol %s", addr, namebuf);
+	free_page((unsigned long)namebuf);
 }
 
 DEFINE_MARKER(kernel, kprobe, "ip %lX");
@@ -75,7 +74,7 @@ static int ltt_kprobe_handler_pre(struct kprobe *p, struct pt_regs *regs)
 	data = (unsigned long)p->addr;
 	marker = &GET_MARKER(kernel, kprobe);
 	ltt_specialized_trace(marker, marker->single.probe_private,
-		&data, sizeof(data), sizeof(data));
+			      &data, sizeof(data), sizeof(data));
 	return 0;
 }
 
@@ -212,8 +211,8 @@ static ssize_t enable_op_write(struct file *file,
 	const char __user *user_buf, size_t count, loff_t *ppos)
 {
 	int err, buf_size;
-	char buf[NAME_MAX];
 	char *end;
+	char *buf = (char *)__get_free_page(GFP_KERNEL);
 
 	mutex_lock(&ltt_kprobes_mutex);
 	if (module_exit) {
@@ -221,7 +220,7 @@ static ssize_t enable_op_write(struct file *file,
 		goto error;
 	}
 
-	buf_size = min(count, sizeof(buf) - 1);
+	buf_size = min(count, PAGE_SIZE - 1);
 	err = copy_from_user(buf, user_buf, buf_size);
 	if (err)
 		goto error;
@@ -232,11 +231,13 @@ static ssize_t enable_op_write(struct file *file,
 	err = ltt_register_kprobe(buf);
 	if (err)
 		goto error;
-	mutex_unlock(&ltt_kprobes_mutex);
 
+	mutex_unlock(&ltt_kprobes_mutex);
+	free_page((unsigned long)buf);
 	return count;
 error:
 	mutex_unlock(&ltt_kprobes_mutex);
+	free_page((unsigned long)buf);
 	return err;
 }
 
@@ -248,14 +249,14 @@ static ssize_t disable_op_write(struct file *file,
 	const char __user *user_buf, size_t count, loff_t *ppos)
 {
 	int err, buf_size;
-	char buf[NAME_MAX];
 	char *end;
+	char *buf = (char *)__get_free_page(GFP_KERNEL);
 
 	mutex_lock(&ltt_kprobes_mutex);
 	if (module_exit)
 		goto end;
 
-	buf_size = min(count, sizeof(buf) - 1);
+	buf_size = min(count, PAGE_SIZE - 1);
 	err = copy_from_user(buf, user_buf, buf_size);
 	if (err)
 		goto error;
@@ -268,9 +269,11 @@ static ssize_t disable_op_write(struct file *file,
 		goto error;
 end:
 	mutex_unlock(&ltt_kprobes_mutex);
+	free_page((unsigned long)buf);
 	return count;
 error:
 	mutex_unlock(&ltt_kprobes_mutex);
+	free_page((unsigned long)buf);
 	return err;
 }
 
@@ -418,8 +421,9 @@ static int __init ltt_kprobes_init(void)
 	}
 
 	ltt_kprobes_enable_dentry = debugfs_create_file(LTT_KPROBES_ENABLE,
-		S_IWUSR, ltt_kprobes_dir, NULL,
-		&ltt_kprobes_enable);
+							S_IWUSR,
+							ltt_kprobes_dir, NULL,
+							&ltt_kprobes_enable);
 	if (IS_ERR(ltt_kprobes_enable_dentry) || !ltt_kprobes_enable_dentry) {
 		printk(KERN_ERR
 		       "ltt_kprobes_init: failed to create file %s\n",
@@ -429,8 +433,9 @@ static int __init ltt_kprobes_init(void)
 	}
 
 	ltt_kprobes_disable_dentry = debugfs_create_file(LTT_KPROBES_DISABLE,
-		S_IWUSR, ltt_kprobes_dir, NULL,
-		&ltt_kprobes_disable);
+							 S_IWUSR,
+							 ltt_kprobes_dir, NULL,
+							 &ltt_kprobes_disable);
 	if (IS_ERR(ltt_kprobes_disable_dentry) || !ltt_kprobes_disable_dentry) {
 		printk(KERN_ERR
 		       "ltt_kprobes_init: failed to create file %s\n",
@@ -440,8 +445,8 @@ static int __init ltt_kprobes_init(void)
 	}
 
 	ltt_kprobes_list_dentry = debugfs_create_file(LTT_KPROBES_LIST,
-		S_IWUSR, ltt_kprobes_dir, NULL,
-		&ltt_kprobes_list);
+						      S_IWUSR, ltt_kprobes_dir,
+						      NULL, &ltt_kprobes_list);
 	if (IS_ERR(ltt_kprobes_list_dentry) || !ltt_kprobes_list_dentry) {
 		printk(KERN_ERR
 		       "ltt_kprobes_init: failed to create file %s\n",
diff --git a/ltt/ltt-relay-alloc.c b/ltt/ltt-relay-alloc.c
index 81651a3..d50cc72 100644
--- a/ltt/ltt-relay-alloc.c
+++ b/ltt/ltt-relay-alloc.c
@@ -1,16 +1,11 @@
 /*
- * Public API and common code for kernel->userspace relay file support.
+ * ltt-relay-alloc.c
  *
- * Copyright (C) 2002-2005 - Tom Zanussi (zanussi@us.ibm.com), IBM Corp
- * Copyright (C) 1999-2005 - Karim Yaghmour (karim@opersys.com)
- * Copyright (C) 2008 - Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ * Copyright (C) 2008,2009 - Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
  *
- * Moved to kernel/relay.c by Paul Mundt, 2006.
- * November 2006 - CPU hotplug support by Mathieu Desnoyers
- * 	(mathieu.desnoyers@polymtl.ca)
- *
- * This file is released under the GPL.
+ * Dual LGPL v2.1/GPL v2 license.
  */
+
 #include <linux/errno.h>
 #include <linux/stddef.h>
 #include <linux/slab.h>
@@ -20,27 +15,32 @@
 #include <linux/vmalloc.h>
 #include <linux/mm.h>
 #include <linux/cpu.h>
-#include <linux/splice.h>
 #include <linux/bitops.h>
+#include <linux/ltt-tracer.h>
 
-/* list of open channels, for cpu hotplug */
-static DEFINE_MUTEX(relay_channels_mutex);
-static LIST_HEAD(relay_channels);
+#include "ltt-relay-select.h"	/* for cpu hotplug */
+
+static DEFINE_MUTEX(ltt_relay_alloc_mutex);
+/* list of open channels, for cpu hotplug. */
+static LIST_HEAD(ltt_relay_channels);
 
 /**
- *	relay_alloc_buf - allocate a channel buffer
- *	@buf: the buffer struct
- *	@size: total size of the buffer
+ * ltt_chanbuf_allocate - allocate a channel buffer
+ * @buf: the buffer struct
+ * @size: total size of the buffer
+ * @n_sb: number of subbuffers
+ * @extra_reader_sb: need extra subbuffer for reader
  */
-static int relay_alloc_buf(struct rchan_buf *buf, size_t *size,
-			   size_t n_subbufs, int extra_reader_sb)
+static
+int ltt_chanbuf_allocate(struct ltt_chanbuf_alloc *buf, size_t size,
+			 size_t n_sb, int extra_reader_sb)
 {
 	long i, j, n_pages, n_pages_per_sb, page_idx = 0;
 	struct page **pages;
 	void **virt;
 
-	n_pages = *size >> PAGE_SHIFT;
-	n_pages_per_sb = n_pages >> get_count_order(n_subbufs);
+	n_pages = size >> PAGE_SHIFT;
+	n_pages_per_sb = n_pages >> get_count_order(n_sb);
 	if (extra_reader_sb)
 		n_pages += n_pages_per_sb;	/* Add pages for reader */
 
@@ -63,60 +63,60 @@ static int relay_alloc_buf(struct rchan_buf *buf, size_t *size,
 			goto depopulate;
 		virt[i] = page_address(pages[i]);
 	}
-	buf->page_count = n_pages;
+	buf->nr_pages = n_pages;
 	buf->_pages = pages;
 	buf->_virt = virt;
 
 	/* Allocate write-side page index */
-	buf->rchan_wsb = kzalloc_node(max_t(size_t,
-				sizeof(struct rchan_sb) * n_subbufs,
+	buf->buf_wsb = kzalloc_node(max_t(size_t,
+				sizeof(struct chanbuf_sb) * n_sb,
 				1 << INTERNODE_CACHE_SHIFT),
 				GFP_KERNEL, cpu_to_node(buf->cpu));
-	if (unlikely(!buf->rchan_wsb))
+	if (unlikely(!buf->buf_wsb))
 		goto depopulate;
 
-	for (i = 0; i < n_subbufs; i++) {
-		buf->rchan_wsb[i].pages =
+	for (i = 0; i < n_sb; i++) {
+		buf->buf_wsb[i].pages =
 			kzalloc_node(max_t(size_t,
-				sizeof(struct rchan_page) * n_pages_per_sb,
+				sizeof(struct chanbuf_page) * n_pages_per_sb,
 				1 << INTERNODE_CACHE_SHIFT),
 				GFP_KERNEL, cpu_to_node(buf->cpu));
-		if (!buf->rchan_wsb[i].pages)
-			goto free_rchan_wsb;
+		if (!buf->buf_wsb[i].pages)
+			goto free_buf_wsb;
 	}
 
 	if (extra_reader_sb) {
 		/* Allocate read-side page index */
-		buf->rchan_rsb.pages =
+		buf->buf_rsb.pages =
 			kzalloc_node(max_t(size_t,
-				sizeof(struct rchan_page) * n_pages_per_sb,
+				sizeof(struct chanbuf_page) * n_pages_per_sb,
 				1 << INTERNODE_CACHE_SHIFT),
 				GFP_KERNEL, cpu_to_node(buf->cpu));
-		if (unlikely(!buf->rchan_rsb.pages))
-			goto free_rchan_wsb;
+		if (unlikely(!buf->buf_rsb.pages))
+			goto free_buf_wsb;
 	} else {
-		buf->rchan_rsb.pages = NULL;
+		buf->buf_rsb.pages = NULL;
 	}
 
 	/* Assign pages to write-side page index */
-	for (i = 0; i < n_subbufs; i++) {
+	for (i = 0; i < n_sb; i++) {
 		for (j = 0; j < n_pages_per_sb; j++) {
 			WARN_ON(page_idx > n_pages);
-			buf->rchan_wsb[i].pages[j].virt = virt[page_idx];
-			buf->rchan_wsb[i].pages[j].page = pages[page_idx];
+			buf->buf_wsb[i].pages[j].virt = virt[page_idx];
+			buf->buf_wsb[i].pages[j].page = pages[page_idx];
 			page_idx++;
 		}
-		RCHAN_SB_SET_NOREF(buf->rchan_wsb[i].pages);
+		RCHAN_SB_SET_NOREF(buf->buf_wsb[i].pages);
 	}
 
 	if (extra_reader_sb) {
 		for (j = 0; j < n_pages_per_sb; j++) {
 			WARN_ON(page_idx > n_pages);
-			buf->rchan_rsb.pages[j].virt = virt[page_idx];
-			buf->rchan_rsb.pages[j].page = pages[page_idx];
+			buf->buf_rsb.pages[j].virt = virt[page_idx];
+			buf->buf_rsb.pages[j].page = pages[page_idx];
 			page_idx++;
 		}
-		RCHAN_SB_SET_NOREF(buf->rchan_rsb.pages);
+		RCHAN_SB_SET_NOREF(buf->buf_rsb.pages);
 	}
 
 	/*
@@ -126,12 +126,12 @@ static int relay_alloc_buf(struct rchan_buf *buf, size_t *size,
 	vmalloc_sync_all();
 	return 0;
 
-free_rchan_wsb:
-	for (i = 0; i < n_subbufs; i++) {
-		RCHAN_SB_CLEAR_NOREF(buf->rchan_wsb[i].pages);
-		kfree(buf->rchan_wsb[i].pages);
+free_buf_wsb:
+	for (i = 0; i < n_sb; i++) {
+		RCHAN_SB_CLEAR_NOREF(buf->buf_wsb[i].pages);
+		kfree(buf->buf_wsb[i].pages);
 	}
-	kfree(buf->rchan_wsb);
+	kfree(buf->buf_wsb);
 depopulate:
 	/*
 	 * Free all pages from [ i - 1 down to 0 ].
@@ -146,412 +146,248 @@ pages_error:
 	return -ENOMEM;
 }
 
-/**
- *	relay_create_buf - allocate and initialize a channel buffer
- *	@chan: the relay channel
- *	@cpu: cpu the buffer belongs to
- *
- *	Returns channel buffer if successful, %NULL otherwise.
- */
-static struct rchan_buf *relay_create_buf(struct rchan *chan, int cpu)
+int ltt_chanbuf_alloc_create(struct ltt_chanbuf_alloc *buf,
+			     struct ltt_chan_alloc *chan, int cpu)
 {
-	int ret;
-	struct rchan_buf *buf = kzalloc(sizeof(struct rchan_buf), GFP_KERNEL);
-	if (!buf)
-		return NULL;
+	int ret = 0;
 
-	buf->cpu = cpu;
-	ret = relay_alloc_buf(buf, &chan->alloc_size, chan->n_subbufs,
-			      chan->extra_reader_sb);
+	ret = ltt_chanbuf_allocate(buf, chan->buf_size, chan->n_sb,
+				   chan->extra_reader_sb);
 	if (ret)
-		goto free_buf;
+		goto end;
 
+	kref_init(&buf->kref);
 	buf->chan = chan;
-	kref_get(&buf->chan->kref);
-	return buf;
-
-free_buf:
-	kfree(buf);
-	return NULL;
-}
-
-/**
- *	relay_destroy_channel - free the channel struct
- *	@kref: target kernel reference that contains the relay channel
- *
- *	Should only be called from kref_put().
- */
-static void relay_destroy_channel(struct kref *kref)
-{
-	struct rchan *chan = container_of(kref, struct rchan, kref);
-	kfree(chan);
-}
-
-void ltt_relay_get_chan(struct rchan *chan)
-{
-	kref_get(&chan->kref);
-}
-EXPORT_SYMBOL_GPL(ltt_relay_get_chan);
-
-void ltt_relay_put_chan(struct rchan *chan)
-{
-	kref_put(&chan->kref, relay_destroy_channel);
+	buf->allocated = 1;
+	buf->cpu = cpu;
+end:
+	return ret;
 }
-EXPORT_SYMBOL_GPL(ltt_relay_put_chan);
 
-/**
- *	relay_destroy_buf - destroy an rchan_buf struct and associated buffer
- *	@buf: the buffer struct
- */
-static void relay_destroy_buf(struct rchan_buf *buf)
+void ltt_chanbuf_alloc_free(struct ltt_chanbuf_alloc *buf)
 {
-	struct rchan *chan = buf->chan;
+	struct ltt_chan_alloc *chan = buf->chan;
 	struct page **pages;
 	long i;
 
 	/* Destroy index */
 	if (chan->extra_reader_sb) {
-		RCHAN_SB_CLEAR_NOREF(buf->rchan_rsb.pages);
-		kfree(buf->rchan_rsb.pages);
+		RCHAN_SB_CLEAR_NOREF(buf->buf_rsb.pages);
+		kfree(buf->buf_rsb.pages);
 	}
-	for (i = 0; i < chan->n_subbufs; i++) {
-		RCHAN_SB_CLEAR_NOREF(buf->rchan_wsb[i].pages);
-		kfree(buf->rchan_wsb[i].pages);
+	for (i = 0; i < chan->n_sb; i++) {
+		RCHAN_SB_CLEAR_NOREF(buf->buf_wsb[i].pages);
+		kfree(buf->buf_wsb[i].pages);
 	}
-	kfree(buf->rchan_wsb);
+	kfree(buf->buf_wsb);
 
 	/* Destroy pages */
 	pages = buf->_pages;
-	for (i = 0; i < buf->page_count; i++)
+	for (i = 0; i < buf->nr_pages; i++)
 		__free_page(pages[i]);
 	kfree(buf->_pages);
 	kfree(buf->_virt);
-	chan->buf[buf->cpu] = NULL;
-	kfree(buf);
-	kref_put(&chan->kref, relay_destroy_channel);
-}
-
-/**
- *	relay_remove_buf - remove a channel buffer
- *	@kref: target kernel reference that contains the relay buffer
- *
- *	Removes the file from the fileystem, which also frees the
- *	rchan_buf_struct and the channel buffer.  Should only be called from
- *	kref_put().
- */
-static void relay_remove_buf(struct kref *kref)
-{
-	struct rchan_buf *buf = container_of(kref, struct rchan_buf, kref);
-	buf->chan->cb->remove_buf_file(buf->dentry);
-	relay_destroy_buf(buf);
+	buf->allocated = 0;
 }
 
-void ltt_relay_get_chan_buf(struct rchan_buf *buf)
-{
-	kref_get(&buf->kref);
-}
-EXPORT_SYMBOL_GPL(ltt_relay_get_chan_buf);
-
-void ltt_relay_put_chan_buf(struct rchan_buf *buf)
-{
-	kref_put(&buf->kref, relay_remove_buf);
-}
-EXPORT_SYMBOL_GPL(ltt_relay_put_chan_buf);
-
-/*
- * High-level relay kernel API and associated functions.
- */
-
-/*
- * rchan_callback implementations defining default channel behavior.  Used
- * in place of corresponding NULL values in client callback struct.
- */
-
-/*
- * create_buf_file_create() default callback.  Does nothing.
- */
-static struct dentry *create_buf_file_default_callback(const char *filename,
-						       struct dentry *parent,
-						       int mode,
-						       struct rchan_buf *buf)
-{
-	return NULL;
-}
-
-/*
- * remove_buf_file() default callback.  Does nothing.
- */
-static int remove_buf_file_default_callback(struct dentry *dentry)
-{
-	return -EINVAL;
-}
-
-/* relay channel default callbacks */
-static struct rchan_callbacks default_channel_callbacks = {
-	.create_buf_file = create_buf_file_default_callback,
-	.remove_buf_file = remove_buf_file_default_callback,
-};
-
 /**
- *	__relay_reset - reset a channel buffer
- *	@buf: the channel buffer
- *	@init: 1 if this is a first-time initialization
- *
- *	See relay_reset() for description of effect.
- */
-static void __relay_reset(struct rchan_buf *buf, unsigned int init)
-{
-	if (init)
-		kref_init(&buf->kref);
-}
-
-/*
- *	relay_open_buf - create a new relay channel buffer
+ *	ltt_relay_hotcpu_callback - CPU hotplug callback
+ *	@nb: notifier block
+ *	@action: hotplug action to take
+ *	@hcpu: CPU number
  *
- *	used by relay_open() and CPU hotplug.
+ *	Returns the success/failure of the operation. (%NOTIFY_OK, %NOTIFY_BAD)
  */
-static struct rchan_buf *relay_open_buf(struct rchan *chan, unsigned int cpu)
+static
+int __cpuinit ltt_relay_hotcpu_callback(struct notifier_block *nb,
+					unsigned long action,
+					void *hcpu)
 {
-	struct rchan_buf *buf = NULL;
-	struct dentry *dentry;
-	char *tmpname;
-
-	tmpname = kzalloc(NAME_MAX + 1, GFP_KERNEL);
-	if (!tmpname)
-		goto end;
-	snprintf(tmpname, NAME_MAX, "%s%d", chan->base_filename, cpu);
-
-	buf = relay_create_buf(chan, cpu);
-	if (!buf)
-		goto free_name;
-
-	__relay_reset(buf, 1);
-
-	/* Create file in fs */
-	dentry = chan->cb->create_buf_file(tmpname, chan->parent, S_IRUSR,
-					   buf);
-	if (!dentry)
-		goto free_buf;
-
-	buf->dentry = dentry;
-
-	goto free_name;
-
-free_buf:
-	relay_destroy_buf(buf);
-	buf = NULL;
-free_name:
-	kfree(tmpname);
-end:
-	return buf;
-}
-
-/**
- *	relay_close_buf - close a channel buffer
- *	@buf: channel buffer
- *
- *	Restores the default callbacks.
- *	The channel buffer and channel buffer data structure are then freed
- *	automatically when the last reference is given up.
- */
-static void relay_close_buf(struct rchan_buf *buf)
-{
-	kref_put(&buf->kref, relay_remove_buf);
-}
-
-static void setup_callbacks(struct rchan *chan,
-				   struct rchan_callbacks *cb)
-{
-	if (!cb) {
-		chan->cb = &default_channel_callbacks;
-		return;
-	}
-
-	if (!cb->create_buf_file)
-		cb->create_buf_file = create_buf_file_default_callback;
-	if (!cb->remove_buf_file)
-		cb->remove_buf_file = remove_buf_file_default_callback;
-	chan->cb = cb;
-}
-
-/**
- * 	relay_hotcpu_callback - CPU hotplug callback
- * 	@nb: notifier block
- * 	@action: hotplug action to take
- * 	@hcpu: CPU number
- *
- * 	Returns the success/failure of the operation. (%NOTIFY_OK, %NOTIFY_BAD)
- */
-static int __cpuinit relay_hotcpu_callback(struct notifier_block *nb,
-				unsigned long action,
-				void *hcpu)
-{
-	unsigned int hotcpu = (unsigned long)hcpu;
-	struct rchan *chan;
+	unsigned int cpu = (unsigned long)hcpu;
+	struct ltt_chan *chan;
+	int ret;
 
 	switch (action) {
 	case CPU_UP_PREPARE:
 	case CPU_UP_PREPARE_FROZEN:
-		mutex_lock(&relay_channels_mutex);
-		list_for_each_entry(chan, &relay_channels, list) {
-			if (chan->buf[hotcpu])
-				continue;
-			chan->buf[hotcpu] = relay_open_buf(chan, hotcpu);
-			if (!chan->buf[hotcpu]) {
+		mutex_lock(&ltt_relay_alloc_mutex);
+		list_for_each_entry(chan, &ltt_relay_channels, a.list) {
+			struct ltt_chanbuf *buf = per_cpu_ptr(chan->a.buf, cpu);
+
+			ret = ltt_chanbuf_create(buf, &chan->a, cpu);
+			if (ret) {
 				printk(KERN_ERR
-					"relay_hotcpu_callback: cpu %d buffer "
-					"creation failed\n", hotcpu);
-				mutex_unlock(&relay_channels_mutex);
+					"ltt_relay_hotcpu_callback: cpu %d "
+					"buffer creation failed\n", cpu);
+				mutex_unlock(&ltt_relay_alloc_mutex);
 				return NOTIFY_BAD;
 			}
 		}
-		mutex_unlock(&relay_channels_mutex);
+		mutex_unlock(&ltt_relay_alloc_mutex);
 		break;
 	case CPU_DEAD:
 	case CPU_DEAD_FROZEN:
-		/* No need to flush the cpu : will be flushed upon
-		 * final relay_flush() call. */
+		/* No need to do a buffer switch here, because it will happen
+		 * when tracing is stopped, or will be done by switch timer CPU
+		 * DEAD callback. */
 		break;
 	}
 	return NOTIFY_OK;
 }
 
+void ltt_chan_for_each_channel(void (*cb) (struct ltt_chanbuf *buf), int cpu)
+{
+	struct ltt_chan *chan;
+
+	mutex_lock(&ltt_relay_alloc_mutex);
+	list_for_each_entry(chan, &ltt_relay_channels, a.list) {
+		struct ltt_chanbuf *buf;
+
+		if (!chan->active)
+			continue;
+		buf = per_cpu_ptr(chan->a.buf, cpu);
+		cb(buf);
+	}
+	mutex_unlock(&ltt_relay_alloc_mutex);
+}
+
 /**
- *	ltt_relay_open - create a new relay channel
- *	@base_filename: base name of files to create
- *	@parent: dentry of parent directory, %NULL for root directory
- *	@subbuf_size: size of sub-buffers (> PAGE_SIZE, power of 2)
- *	@n_subbufs: number of sub-buffers (power of 2)
- *	@cb: client callback functions
- *	@private_data: user-defined data
- *	@extra_reader_sb: allocate an extra subbuffer for the reader
+ * ltt_chan_create - create a new relay channel
+ * @base_filename: base name of files to create
+ * @parent: dentry of parent directory, %NULL for root directory
+ * @sb_size: size of sub-buffers (> PAGE_SIZE, power of 2)
+ * @n_sb: number of sub-buffers (power of 2)
+ * @extra_reader_sb: allocate an extra subbuffer for the reader
+ * @overwrite: channel is in overwrite mode
  *
- *	Returns channel pointer if successful, %NULL otherwise.
+ * Returns channel pointer if successful, %NULL otherwise.
  *
- *	Creates a channel buffer for each cpu using the sizes and
- *	attributes specified.  The created channel buffer files
- *	will be named base_filename0...base_filenameN-1.  File
- *	permissions will be %S_IRUSR.
+ * Creates per-cpu channel buffers using the sizes and attributes
+ * specified.  The created channel buffer files will be named
+ * base_filename_0...base_filename_N-1.  File permissions will
+ * be %S_IRUSR.
  */
-struct rchan *ltt_relay_open(const char *base_filename,
-			 struct dentry *parent,
-			 size_t subbuf_size,
-			 size_t n_subbufs,
-			 struct rchan_callbacks *cb,
-			 void *private_data,
-			 int extra_reader_sb)
+int ltt_chan_alloc_init(struct ltt_chan_alloc *chan,
+			const char *base_filename,
+			struct dentry *parent, size_t sb_size,
+			size_t n_sb, int extra_reader_sb, int overwrite)
 {
 	unsigned int i;
-	struct rchan *chan;
-	if (!base_filename)
-		return NULL;
+	int ret;
 
-	if (!(subbuf_size && n_subbufs))
-		return NULL;
+	if (!base_filename)
+		return -EPERM;
 
-	chan = kzalloc(sizeof(struct rchan), GFP_KERNEL);
-	if (!chan)
-		return NULL;
+	if (!(sb_size && n_sb))
+		return -EPERM;
 
 	/* Check that the subbuffer size is larger than a page. */
-	WARN_ON_ONCE(subbuf_size < PAGE_SIZE);
+	WARN_ON_ONCE(sb_size < PAGE_SIZE);
 
 	/*
 	 * Make sure the number of subbuffers and subbuffer size are power of 2.
 	 */
-	WARN_ON_ONCE(hweight32(subbuf_size) != 1);
-	WARN_ON(hweight32(n_subbufs) != 1);
-
-	chan->version = LTT_RELAY_CHANNEL_VERSION;
-	chan->n_subbufs = n_subbufs;
-	chan->subbuf_size = subbuf_size;
-	chan->subbuf_size_order = get_count_order(subbuf_size);
-	chan->alloc_size = subbuf_size * n_subbufs;
-	chan->parent = parent;
-	chan->private_data = private_data;
+	WARN_ON_ONCE(hweight32(sb_size) != 1);
+	WARN_ON(hweight32(n_sb) != 1);
+
+	chan->buf_size = n_sb * sb_size;
+	chan->sb_size = sb_size;
+	chan->sb_size_order = get_count_order(sb_size);
+	chan->n_sb_order = get_count_order(n_sb);
 	chan->extra_reader_sb = extra_reader_sb;
-	strlcpy(chan->base_filename, base_filename, NAME_MAX);
-	setup_callbacks(chan, cb);
+
+	chan->n_sb = n_sb;
+	chan->parent = parent;
+	strlcpy(chan->filename, base_filename, NAME_MAX);
 	kref_init(&chan->kref);
 
-	mutex_lock(&relay_channels_mutex);
+	/* Allocating the child structure */
+	chan->buf = alloc_percpu(struct ltt_chanbuf);
+	if (!chan->buf)
+		goto free_chan;
+
+	mutex_lock(&ltt_relay_alloc_mutex);
 	for_each_online_cpu(i) {
-		chan->buf[i] = relay_open_buf(chan, i);
-		if (!chan->buf[i])
+		ret = ltt_chanbuf_create(per_cpu_ptr(chan->buf, i), chan, i);
+		if (ret)
 			goto free_bufs;
 	}
-	list_add(&chan->list, &relay_channels);
-	mutex_unlock(&relay_channels_mutex);
+	list_add(&chan->list, &ltt_relay_channels);
+	mutex_unlock(&ltt_relay_alloc_mutex);
 
-	return chan;
+	return 0;
 
 free_bufs:
 	for_each_possible_cpu(i) {
-		if (!chan->buf[i])
-			break;
-		relay_close_buf(chan->buf[i]);
-	}
+		struct ltt_chanbuf *buf = per_cpu_ptr(chan->buf, i);
 
-	kref_put(&chan->kref, relay_destroy_channel);
-	mutex_unlock(&relay_channels_mutex);
-	return NULL;
+		if (!buf->a.allocated)
+			continue;
+		ltt_chanbuf_remove_file(buf);
+		kref_put(&buf->a.kref, ltt_chanbuf_free);
+	}
+	mutex_unlock(&ltt_relay_alloc_mutex);
+	free_percpu(chan->buf);
+free_chan:
+	kref_put(&chan->kref, ltt_chan_free);
+	return -ENOMEM;
 }
-EXPORT_SYMBOL_GPL(ltt_relay_open);
 
 /**
- *	ltt_relay_close - close the channel
- *	@chan: the channel
+ * ltt_chan_alloc_free - destroy the channel
+ * @chan: the channel
  *
- *	Closes all channel buffers and frees the channel.
+ * Destroy all channel buffers and frees the channel.
  */
-void ltt_relay_close(struct rchan *chan)
+void ltt_chan_alloc_free(struct ltt_chan_alloc *chan)
 {
 	unsigned int i;
 
-	if (!chan)
-		return;
-
-	mutex_lock(&relay_channels_mutex);
-	for_each_possible_cpu(i)
-		if (chan->buf[i])
-			relay_close_buf(chan->buf[i]);
+	mutex_lock(&ltt_relay_alloc_mutex);
+	for_each_possible_cpu(i) {
+		struct ltt_chanbuf *buf = per_cpu_ptr(chan->buf, i);
 
+		if (!buf->a.allocated)
+			continue;
+		ltt_chanbuf_remove_file(buf);
+		kref_put(&buf->a.kref, ltt_chanbuf_free);
+	}
 	list_del(&chan->list);
-	kref_put(&chan->kref, relay_destroy_channel);
-	mutex_unlock(&relay_channels_mutex);
+	mutex_unlock(&ltt_relay_alloc_mutex);
+	free_percpu(chan->buf);
 }
-EXPORT_SYMBOL_GPL(ltt_relay_close);
 
 /**
  * ltt_relay_write - write data to a ltt_relay buffer.
- * @buf : buffer
+ * @bufa : buffer
  * @offset : offset within the buffer
  * @src : source address
  * @len : length to write
  * @page : cached buffer page
  * @pagecpy : page size copied so far
  */
-void _ltt_relay_write(struct rchan_buf *buf, size_t offset,
-	const void *src, size_t len, ssize_t pagecpy)
+void _ltt_relay_write(struct ltt_chanbuf_alloc *bufa, size_t offset,
+		      const void *src, size_t len, ssize_t pagecpy)
 {
+	struct ltt_chan_alloc *chana = bufa->chan;
 	size_t sbidx, index;
-	struct rchan_page *rpages;
+	struct chanbuf_page *rpages;
 
 	do {
 		len -= pagecpy;
 		src += pagecpy;
 		offset += pagecpy;
-		sbidx = offset >> buf->chan->subbuf_size_order;
-		index = (offset & (buf->chan->subbuf_size - 1)) >> PAGE_SHIFT;
+		sbidx = offset >> chana->sb_size_order;
+		index = (offset & (chana->sb_size - 1)) >> PAGE_SHIFT;
 
 		/*
 		 * Underlying layer should never ask for writes across
 		 * subbuffers.
 		 */
-		WARN_ON(offset >= buf->chan->alloc_size);
+		WARN_ON(offset >= chana->buf_size);
 
 		pagecpy = min_t(size_t, len, PAGE_SIZE - (offset & ~PAGE_MASK));
-		rpages = buf->rchan_wsb[sbidx].pages;
+		rpages = bufa->buf_wsb[sbidx].pages;
 		WARN_ON_ONCE(RCHAN_SB_IS_NOREF(rpages));
 		ltt_relay_do_copy(rpages[index].virt + (offset & ~PAGE_MASK),
 				  src, pagecpy);
@@ -561,28 +397,29 @@ EXPORT_SYMBOL_GPL(_ltt_relay_write);
 
 /**
  * ltt_relay_read - read data from ltt_relay_buffer.
- * @buf : buffer
+ * @bufa : buffer
  * @offset : offset within the buffer
  * @dest : destination address
  * @len : length to write
  *
  * Should be protected by get_subbuf/put_subbuf.
  */
-int ltt_relay_read(struct rchan_buf *buf, size_t offset,
-	void *dest, size_t len)
+int ltt_relay_read(struct ltt_chanbuf_alloc *bufa, size_t offset, void *dest,
+		   size_t len)
 {
+	struct ltt_chan_alloc *chana = bufa->chan;
 	size_t index;
 	ssize_t pagecpy, orig_len;
-	struct rchan_page *rpages;
+	struct chanbuf_page *rpages;
 
 	orig_len = len;
-	offset &= buf->chan->alloc_size - 1;
-	index = (offset & (buf->chan->subbuf_size - 1)) >> PAGE_SHIFT;
+	offset &= chana->buf_size - 1;
+	index = (offset & (chana->sb_size - 1)) >> PAGE_SHIFT;
 	if (unlikely(!len))
 		return 0;
 	for (;;) {
 		pagecpy = min_t(size_t, len, PAGE_SIZE - (offset & ~PAGE_MASK));
-		rpages = buf->rchan_rsb.pages;
+		rpages = bufa->buf_rsb.pages;
 		WARN_ON_ONCE(RCHAN_SB_IS_NOREF(rpages));
 		memcpy(dest, rpages[index].virt + (offset & ~PAGE_MASK),
 		       pagecpy);
@@ -591,12 +428,12 @@ int ltt_relay_read(struct rchan_buf *buf, size_t offset,
 			break;
 		dest += pagecpy;
 		offset += pagecpy;
-		index = (offset & (buf->chan->subbuf_size - 1)) >> PAGE_SHIFT;
+		index = (offset & (chana->sb_size - 1)) >> PAGE_SHIFT;
 		/*
 		 * Underlying layer should never ask for reads across
 		 * subbuffers.
 		 */
-		WARN_ON(offset >= buf->chan->alloc_size);
+		WARN_ON(offset >= chana->buf_size);
 	}
 	return orig_len;
 }
@@ -604,7 +441,7 @@ EXPORT_SYMBOL_GPL(ltt_relay_read);
 
 /**
  * ltt_relay_read_cstr - read a C-style string from ltt_relay_buffer.
- * @buf : buffer
+ * @bufa : buffer
  * @offset : offset within the buffer
  * @dest : destination address
  * @len : destination's length
@@ -612,19 +449,20 @@ EXPORT_SYMBOL_GPL(ltt_relay_read);
  * return string's length
  * Should be protected by get_subbuf/put_subbuf.
  */
-int ltt_relay_read_cstr(struct rchan_buf *buf, size_t offset,
-		void *dest, size_t len)
+int ltt_relay_read_cstr(struct ltt_chanbuf_alloc *bufa, size_t offset,
+			void *dest, size_t len)
 {
+	struct ltt_chan_alloc *chana = bufa->chan;
 	size_t index;
 	ssize_t pagecpy, pagelen, strpagelen, orig_offset;
 	char *str;
-	struct rchan_page *rpages;
+	struct chanbuf_page *rpages;
 
-	offset &= buf->chan->alloc_size - 1;
-	index = (offset & (buf->chan->subbuf_size - 1)) >> PAGE_SHIFT;
+	offset &= chana->buf_size - 1;
+	index = (offset & (chana->sb_size - 1)) >> PAGE_SHIFT;
 	orig_offset = offset;
 	for (;;) {
-		rpages = buf->rchan_rsb.pages;
+		rpages = bufa->buf_rsb.pages;
 		WARN_ON_ONCE(RCHAN_SB_IS_NOREF(rpages));
 		str = (char *)rpages[index].virt + (offset & ~PAGE_MASK);
 		pagelen = PAGE_SIZE - (offset & ~PAGE_MASK);
@@ -638,14 +476,14 @@ int ltt_relay_read_cstr(struct rchan_buf *buf, size_t offset,
 			len -= pagecpy;
 		}
 		offset += strpagelen;
-		index = (offset & (buf->chan->subbuf_size - 1)) >> PAGE_SHIFT;
+		index = (offset & (chana->sb_size - 1)) >> PAGE_SHIFT;
 		if (strpagelen < pagelen)
 			break;
 		/*
 		 * Underlying layer should never ask for reads across
 		 * subbuffers.
 		 */
-		WARN_ON(offset >= buf->chan->alloc_size);
+		WARN_ON(offset >= chana->buf_size);
 	}
 	if (dest && len)
 		((char *)dest)[0] = 0;
@@ -655,19 +493,21 @@ EXPORT_SYMBOL_GPL(ltt_relay_read_cstr);
 
 /**
  * ltt_relay_read_get_page - Get a whole page to read from
- * @buf : buffer
+ * @bufa : buffer
  * @offset : offset within the buffer
  *
  * Should be protected by get_subbuf/put_subbuf.
  */
-struct page *ltt_relay_read_get_page(struct rchan_buf *buf, size_t offset)
+struct page *ltt_relay_read_get_page(struct ltt_chanbuf_alloc *bufa,
+				     size_t offset)
 {
 	size_t index;
-	struct rchan_page *rpages;
+	struct chanbuf_page *rpages;
+	struct ltt_chan_alloc *chana = bufa->chan;
 
-	offset &= buf->chan->alloc_size - 1;
-	index = (offset & (buf->chan->subbuf_size - 1)) >> PAGE_SHIFT;
-	rpages = buf->rchan_rsb.pages;
+	offset &= chana->buf_size - 1;
+	index = (offset & (chana->sb_size - 1)) >> PAGE_SHIFT;
+	rpages = bufa->buf_rsb.pages;
 	WARN_ON_ONCE(RCHAN_SB_IS_NOREF(rpages));
 	return rpages[index].page;
 }
@@ -675,7 +515,7 @@ EXPORT_SYMBOL_GPL(ltt_relay_read_get_page);
 
 /**
  * ltt_relay_read_offset_address - get address of a location within the buffer
- * @buf : buffer
+ * @bufa : buffer
  * @offset : offset within the buffer.
  *
  * Return the address where a given offset is located (for read).
@@ -683,14 +523,16 @@ EXPORT_SYMBOL_GPL(ltt_relay_read_get_page);
  * it's never on a page boundary, it's safe to write directly to this address,
  * as long as the write is never bigger than a page size.
  */
-void *ltt_relay_read_offset_address(struct rchan_buf *buf, size_t offset)
+void *ltt_relay_read_offset_address(struct ltt_chanbuf_alloc *bufa,
+				    size_t offset)
 {
 	size_t index;
-	struct rchan_page *rpages;
+	struct chanbuf_page *rpages;
+	struct ltt_chan_alloc *chana = bufa->chan;
 
-	offset &= buf->chan->alloc_size - 1;
-	index = (offset & (buf->chan->subbuf_size - 1)) >> PAGE_SHIFT;
-	rpages = buf->rchan_rsb.pages;
+	offset &= chana->buf_size - 1;
+	index = (offset & (chana->sb_size - 1)) >> PAGE_SHIFT;
+	rpages = bufa->buf_rsb.pages;
 	WARN_ON_ONCE(RCHAN_SB_IS_NOREF(rpages));
 	return rpages[index].virt + (offset & ~PAGE_MASK);
 }
@@ -698,7 +540,7 @@ EXPORT_SYMBOL_GPL(ltt_relay_read_offset_address);
 
 /**
  * ltt_relay_offset_address - get address of a location within the buffer
- * @buf : buffer
+ * @bufa : buffer
  * @offset : offset within the buffer.
  *
  * Return the address where a given offset is located.
@@ -706,62 +548,25 @@ EXPORT_SYMBOL_GPL(ltt_relay_read_offset_address);
  * it's never on a page boundary, it's safe to write directly to this address,
  * as long as the write is never bigger than a page size.
  */
-void *ltt_relay_offset_address(struct rchan_buf *buf, size_t offset)
+void *ltt_relay_offset_address(struct ltt_chanbuf_alloc *bufa, size_t offset)
 {
 	size_t sbidx, index;
-	struct rchan_page *rpages;
+	struct chanbuf_page *rpages;
+	struct ltt_chan_alloc *chana = bufa->chan;
 
-	offset &= buf->chan->alloc_size - 1;
-	sbidx = offset >> buf->chan->subbuf_size_order;
-	index = (offset & (buf->chan->subbuf_size - 1)) >> PAGE_SHIFT;
-	rpages = buf->rchan_wsb[sbidx].pages;
+	offset &= chana->buf_size - 1;
+	sbidx = offset >> chana->sb_size_order;
+	index = (offset & (chana->sb_size - 1)) >> PAGE_SHIFT;
+	rpages = bufa->buf_wsb[sbidx].pages;
 	WARN_ON_ONCE(RCHAN_SB_IS_NOREF(rpages));
 	return rpages[index].virt + (offset & ~PAGE_MASK);
 }
 EXPORT_SYMBOL_GPL(ltt_relay_offset_address);
 
-/**
- *	relay_file_open - open file op for relay files
- *	@inode: the inode
- *	@filp: the file
- *
- *	Increments the channel buffer refcount.
- */
-static int relay_file_open(struct inode *inode, struct file *filp)
-{
-	struct rchan_buf *buf = inode->i_private;
-	kref_get(&buf->kref);
-	filp->private_data = buf;
-
-	return nonseekable_open(inode, filp);
-}
-
-/**
- *	relay_file_release - release file op for relay files
- *	@inode: the inode
- *	@filp: the file
- *
- *	Decrements the channel refcount, as the filesystem is
- *	no longer using it.
- */
-static int relay_file_release(struct inode *inode, struct file *filp)
-{
-	struct rchan_buf *buf = filp->private_data;
-	kref_put(&buf->kref, relay_remove_buf);
-
-	return 0;
-}
-
-const struct file_operations ltt_relay_file_operations = {
-	.open		= relay_file_open,
-	.release	= relay_file_release,
-};
-EXPORT_SYMBOL_GPL(ltt_relay_file_operations);
-
-static __init int relay_init(void)
+static __init int ltt_relay_init(void)
 {
-	hotcpu_notifier(relay_hotcpu_callback, 5);
+	hotcpu_notifier(ltt_relay_hotcpu_callback, 5);
 	return 0;
 }
 
-module_init(relay_init);
+module_init(ltt_relay_init);
diff --git a/ltt/ltt-relay-lockless.c b/ltt/ltt-relay-lockless.c
index 5a2a4e4..65a63de 100644
--- a/ltt/ltt-relay-lockless.c
+++ b/ltt/ltt-relay-lockless.c
@@ -46,13 +46,9 @@
 #include <linux/timer.h>
 #include <linux/sched.h>
 #include <linux/bitops.h>
-#include <linux/fs.h>
 #include <linux/smp_lock.h>
-#include <linux/debugfs.h>
 #include <linux/stat.h>
 #include <linux/cpu.h>
-#include <linux/pipe_fs_i.h>
-#include <linux/splice.h>
 #include <asm/atomic.h>
 #include <asm/local.h>
 
@@ -70,160 +66,238 @@ struct ltt_reserve_switch_offsets {
 	size_t before_hdr_pad, size;
 };
 
-static int ltt_relay_create_buffer(struct ltt_trace_struct *trace,
-		struct ltt_channel_struct *ltt_chan,
-		struct rchan_buf *buf,
-		unsigned int cpu,
-		unsigned int n_subbufs);
+static
+void ltt_force_switch(struct ltt_chanbuf *buf, enum force_switch_mode mode);
 
-static void ltt_relay_destroy_buffer(struct ltt_channel_struct *ltt_chan,
-		unsigned int cpu);
-
-static void ltt_force_switch(struct rchan_buf *buf,
-		enum force_switch_mode mode);
+static
+void ltt_relay_print_buffer_errors(struct ltt_chan *chan, unsigned int cpu);
 
 static const struct file_operations ltt_file_operations;
 
-static void ltt_buffer_begin(struct rchan_buf *buf,
-			u64 tsc, unsigned int subbuf_idx)
+static
+void ltt_buffer_begin(struct ltt_chanbuf *buf, u64 tsc, unsigned int subbuf_idx)
 {
-	struct ltt_channel_struct *channel =
-		(struct ltt_channel_struct *)buf->chan->private_data;
+	struct ltt_chan *chan = container_of(buf->a.chan, struct ltt_chan, a);
 	struct ltt_subbuffer_header *header =
 		(struct ltt_subbuffer_header *)
-			ltt_relay_offset_address(buf,
-				subbuf_idx * buf->chan->subbuf_size);
+			ltt_relay_offset_address(&buf->a,
+				subbuf_idx * chan->a.sb_size);
 
 	header->cycle_count_begin = tsc;
 	header->lost_size = 0xFFFFFFFF; /* for debugging */
-	header->buf_size = buf->chan->subbuf_size;
-	ltt_write_trace_header(channel->trace, header);
+	header->buf_size = chan->a.sb_size;
+	ltt_write_trace_header(chan->trace, header);
 }
 
 /*
  * offset is assumed to never be 0 here : never deliver a completely empty
  * subbuffer. The lost size is between 0 and subbuf_size-1.
  */
-static void ltt_buffer_end(struct rchan_buf *buf,
-		u64 tsc, unsigned int offset, unsigned int subbuf_idx)
+static
+void ltt_buffer_end(struct ltt_chanbuf *buf, u64 tsc, unsigned int offset,
+		    unsigned int subbuf_idx)
 {
-	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+	struct ltt_chan *chan = container_of(buf->a.chan, struct ltt_chan, a);
 	struct ltt_subbuffer_header *header =
 		(struct ltt_subbuffer_header *)
-			ltt_relay_offset_address(buf,
-				subbuf_idx * buf->chan->subbuf_size);
+			ltt_relay_offset_address(&buf->a,
+				subbuf_idx * chan->a.sb_size);
 
-	header->lost_size = SUBBUF_OFFSET((buf->chan->subbuf_size - offset),
-				buf->chan);
+	header->lost_size = SUBBUF_OFFSET((chan->a.sb_size - offset), chan);
 	header->cycle_count_end = tsc;
-	header->events_lost = local_read(&ltt_buf->events_lost);
-	header->subbuf_corrupt = local_read(&ltt_buf->corrupted_subbuffers);
+	header->events_lost = local_read(&buf->events_lost);
+	header->subbuf_corrupt = local_read(&buf->corrupted_subbuffers);
 }
 
-static struct dentry *ltt_create_buf_file_callback(const char *filename,
-		struct dentry *parent, int mode,
-		struct rchan_buf *buf)
+void ltt_chanbuf_free(struct kref *kref)
 {
-	struct ltt_channel_struct *ltt_chan;
-	int err;
-	struct dentry *dentry;
-
-	ltt_chan = buf->chan->private_data;
-	err = ltt_relay_create_buffer(ltt_chan->trace, ltt_chan,
-					buf, buf->cpu,
-					buf->chan->n_subbufs);
-	if (err)
-		return ERR_PTR(err);
-
-	dentry = debugfs_create_file(filename, mode, parent, buf,
-			&ltt_file_operations);
-	if (!dentry)
-		goto error;
-	if (buf->cpu == 0)
-		buf->ascii_dentry = ltt_ascii_create(ltt_chan->trace, ltt_chan);
-	return dentry;
-error:
-	ltt_relay_destroy_buffer(ltt_chan, buf->cpu);
-	return NULL;
-}
+	struct ltt_chanbuf *buf = container_of(kref, struct ltt_chanbuf,
+					       a.kref);
+	struct ltt_chan *chan = container_of(buf->a.chan, struct ltt_chan, a);
 
-static int ltt_remove_buf_file_callback(struct dentry *dentry)
-{
-	struct rchan_buf *buf = dentry->d_inode->i_private;
-	struct ltt_channel_struct *ltt_chan = buf->chan->private_data;
+	ltt_relay_print_buffer_errors(chan, buf->a.cpu);
+#ifdef CONFIG_LTT_VMCORE
+	kfree(buf->commit_seq);
+#endif
+	kfree(buf->commit_count);
 
-	ltt_ascii_remove(ltt_chan, buf->ascii_dentry);
-	debugfs_remove(dentry);
-	ltt_relay_destroy_buffer(ltt_chan, buf->cpu);
+	kref_put(&chan->trace->kref, ltt_release_trace);
+	wake_up_interruptible(&chan->trace->kref_wq);
 
-	return 0;
+	ltt_chanbuf_alloc_free(&buf->a);
 }
 
 /*
- * Wake writers :
- *
- * This must be done after the trace is removed from the RCU list so that there
- * are no stalled writers.
+ * Must be called under ltt_relay_alloc_mutex protection to ensure serialization
+ * of CPU hotplug vs channel creation.
+ * ltt_chanbuf_free does not have this requirement, because it is never used for
+ * cpu hotplug.
  */
-static void ltt_relay_wake_writers(struct ltt_channel_buf_struct *ltt_buf)
+int ltt_chanbuf_create(struct ltt_chanbuf *buf, struct ltt_chan_alloc *chana,
+		       int cpu)
 {
+	struct ltt_chan *chan = container_of(chana, struct ltt_chan, a);
+	struct ltt_trace *trace = chan->trace;
+	unsigned int j, n_sb;
+	int ret;
+
+	/* Test for cpu hotplug */
+	if (buf->a.allocated)
+		return 0;
+
+	ret = ltt_chanbuf_alloc_create(&buf->a, &chan->a, cpu);
+	if (ret)
+		return ret;
+
+	buf->commit_count =
+		kzalloc_node(ALIGN(sizeof(*buf->commit_count) * chan->a.n_sb,
+				   1 << INTERNODE_CACHE_SHIFT),
+			GFP_KERNEL, cpu_to_node(cpu));
+	if (!buf->commit_count) {
+		ret = -ENOMEM;
+		goto free_chanbuf;
+	}
+
+#ifdef CONFIG_LTT_VMCORE
+	buf->commit_seq =
+		kzalloc_node(ALIGN(sizeof(*buf->commit_seq) * chan->a.n_sb,
+				   1 << INTERNODE_CACHE_SHIFT),
+			GFP_KERNEL, cpu_to_node(cpu));
+	if (!buf->commit_seq) {
+		kfree(buf->commit_count);
+		ret = -ENOMEM;
+		goto free_commit;
+	}
+#endif
+
+	kref_get(&chan->trace->kref);
+	local_set(&buf->offset, ltt_sb_header_size());
+	atomic_long_set(&buf->consumed, 0);
+	atomic_long_set(&buf->active_readers, 0);
+	n_sb = chan->a.n_sb;
+	for (j = 0; j < n_sb; j++) {
+		local_set(&buf->commit_count[j].cc, 0);
+		local_set(&buf->commit_count[j].cc_sb, 0);
+		local_set(&buf->commit_count[j].events, 0);
+	}
+	init_waitqueue_head(&buf->write_wait);
+	init_waitqueue_head(&buf->read_wait);
+	spin_lock_init(&buf->full_lock);
+
+	RCHAN_SB_CLEAR_NOREF(buf->a.buf_wsb[0].pages);
+	ltt_buffer_begin(buf, trace->start_tsc, 0);
+	/* atomic_add made on local variable on data that belongs to
+	 * various CPUs : ok because tracing not started (for this cpu). */
+	local_add(ltt_sb_header_size(), &buf->commit_count[0].cc);
 
-	if (waitqueue_active(&ltt_buf->write_wait))
-		wake_up_interruptible(&ltt_buf->write_wait);
+	local_set(&buf->events_lost, 0);
+	local_set(&buf->corrupted_subbuffers, 0);
+	buf->finalized = 0;
+
+	ret = ltt_chanbuf_create_file(chan->a.filename, chan->a.parent,
+				      S_IRUSR, buf);
+	if (ret)
+		goto free_init;
+
+	return 0;
+
+	/* Error handling */
+free_init:
+	kref_put(&chan->trace->kref, ltt_release_trace);
+	wake_up_interruptible(&chan->trace->kref_wq);
+#ifdef CONFIG_LTT_VMCORE
+	kfree(buf->commit_seq);
+#endif
+free_commit:
+	kfree(buf->commit_count);
+free_chanbuf:
+	ltt_chanbuf_alloc_free(&buf->a);
+	return ret;
 }
 
-/*
- * This function should not be called from NMI interrupt context
- */
-static void ltt_buf_unfull(struct rchan_buf *buf,
-		unsigned int subbuf_idx,
-		long offset)
+void ltt_chan_free(struct kref *kref)
 {
-	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+	struct ltt_chan *chan = container_of(kref, struct ltt_chan, a.kref);
 
-	ltt_relay_wake_writers(ltt_buf);
+	ltt_ascii_remove(chan);
+	ltt_chan_alloc_free(&chan->a);
 }
+EXPORT_SYMBOL_GPL(ltt_chan_free);
 
-/*
- * Reader API.
+/**
+ * ltt_chan_create - Create channel.
  */
-static unsigned long get_offset(struct rchan_buf *buf)
+int ltt_chan_create(const char *base_filename,
+		    struct ltt_chan *chan, struct dentry *parent,
+		    size_t sb_size, size_t n_sb,
+		    int overwrite, struct ltt_trace *trace)
 {
-	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
-	return local_read(&ltt_buf->offset);
-}
+	int ret;
 
-static unsigned long get_consumed(struct rchan_buf *buf)
-{
-	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
-	return atomic_long_read(&ltt_buf->consumed);
+	chan->overwrite = overwrite;
+	chan->trace = trace;
+
+	ret = ltt_chan_alloc_init(&chan->a, base_filename, parent, sb_size,
+				  n_sb, overwrite, overwrite);
+	if (ret)
+		goto error;
+
+	chan->commit_count_mask = (~0UL >> chan->a.n_sb_order);
+
+	ret = ltt_ascii_create(chan);
+	if (ret)
+		goto error_chan_alloc_free;
+
+	return ret;
+
+error_chan_alloc_free:
+	ltt_chan_alloc_free(&chan->a);
+error:
+	return ret;
 }
+EXPORT_SYMBOL_GPL(ltt_chan_create);
 
-static int _ltt_open(struct rchan_buf *buf)
+int ltt_chanbuf_open_read(struct ltt_chanbuf *buf)
 {
-	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
-
-	if (!atomic_long_add_unless(&ltt_buf->active_readers, 1, 1))
+	kref_get(&buf->a.chan->kref);
+	kref_get(&buf->a.kref);
+	if (!atomic_long_add_unless(&buf->active_readers, 1, 1)) {
+		kref_put(&buf->a.kref, ltt_chanbuf_free);
+		kref_put(&buf->a.chan->kref, ltt_chan_free);
 		return -EBUSY;
-	ltt_relay_get_chan(buf->chan);
+	}
+
 	return 0;
 }
 
-static int _ltt_release(struct rchan_buf *buf)
+void ltt_chanbuf_release_read(struct ltt_chanbuf *buf)
 {
-	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
-
-	ltt_relay_put_chan(buf->chan);
-	WARN_ON(atomic_long_read(&ltt_buf->active_readers) != 1);
-	atomic_long_dec(&ltt_buf->active_readers);
-	return 0;
+	//ltt_relay_destroy_buffer(&buf->a.chan->a, buf->a.cpu);
+	WARN_ON(atomic_long_read(&buf->active_readers) != 1);
+	atomic_long_dec(&buf->active_readers);
+	kref_put(&buf->a.kref, ltt_chanbuf_free);
+	kref_put(&buf->a.chan->kref, ltt_chan_free);
 }
 
-static int is_finalized(struct rchan_buf *buf)
+/*
+ * Wake writers :
+ *
+ * This must be done after the trace is removed from the RCU list so that there
+ * are no stalled writers.
+ */
+static void ltt_relay_wake_writers(struct ltt_chanbuf *buf)
 {
-	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
 
-	return ltt_buf->finalized;
+	if (waitqueue_active(&buf->write_wait))
+		wake_up_interruptible(&buf->write_wait);
+}
+
+/*
+ * This function should not be called from NMI interrupt context
+ */
+static void ltt_buf_unfull(struct ltt_chanbuf *buf)
+{
+	ltt_relay_wake_writers(buf);
 }
 
 /*
@@ -237,17 +311,15 @@ static void remote_mb(void *info)
 	smp_mb();
 }
 
-static int get_subbuf(struct rchan_buf *buf, unsigned long *consumed)
+int ltt_chanbuf_get_subbuf(struct ltt_chanbuf *buf, unsigned long *consumed)
 {
-	struct ltt_channel_struct *ltt_channel =
-		(struct ltt_channel_struct *)buf->chan->private_data;
-	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+	struct ltt_chan *chan = container_of(buf->a.chan, struct ltt_chan, a);
 	long consumed_old, consumed_idx, commit_count, write_offset;
 	int ret;
 
-	consumed_old = atomic_long_read(&ltt_buf->consumed);
-	consumed_idx = SUBBUF_INDEX(consumed_old, buf->chan);
-	commit_count = local_read(&ltt_buf->commit_count[consumed_idx].cc_sb);
+	consumed_old = atomic_long_read(&buf->consumed);
+	consumed_idx = SUBBUF_INDEX(consumed_old, chan);
+	commit_count = local_read(&buf->commit_count[consumed_idx].cc_sb);
 	/*
 	 * Make sure we read the commit count before reading the buffer
 	 * data and the write offset. Correct consumed offset ordering
@@ -292,21 +364,21 @@ static int get_subbuf(struct rchan_buf *buf, unsigned long *consumed)
 	 */
 	smp_rmb();
 #else
-	if (raw_smp_processor_id() != buf->cpu) {
+	if (raw_smp_processor_id() != buf->a.cpu) {
 		smp_mb();	/* Total order with IPI handler smp_mb() */
-		smp_call_function_single(buf->cpu, remote_mb, NULL, 1);
+		smp_call_function_single(buf->a.cpu, remote_mb, NULL, 1);
 		smp_mb();	/* Total order with IPI handler smp_mb() */
 	}
 #endif
-	write_offset = local_read(&ltt_buf->offset);
+	write_offset = local_read(&buf->offset);
 	/*
 	 * Check that the subbuffer we are trying to consume has been
 	 * already fully committed.
 	 */
-	if (((commit_count - buf->chan->subbuf_size)
-	     & ltt_channel->commit_count_mask)
-	    - (BUFFER_TRUNC(consumed_old, buf->chan)
-	       >> ltt_channel->n_subbufs_order)
+	if (((commit_count - chan->a.sb_size)
+	     & chan->commit_count_mask)
+	    - (BUFFER_TRUNC(consumed_old, chan)
+	       >> chan->a.n_sb_order)
 	    != 0) {
 		return -EAGAIN;
 	}
@@ -314,13 +386,13 @@ static int get_subbuf(struct rchan_buf *buf, unsigned long *consumed)
 	 * Check that we are not about to read the same subbuffer in
 	 * which the writer head is.
 	 */
-	if ((SUBBUF_TRUNC(write_offset, buf->chan)
-	   - SUBBUF_TRUNC(consumed_old, buf->chan))
+	if ((SUBBUF_TRUNC(write_offset, chan)
+	   - SUBBUF_TRUNC(consumed_old, chan))
 	   == 0) {
 		return -EAGAIN;
 	}
 
-	ret = update_read_sb_index(buf, consumed_idx);
+	ret = update_read_sb_index(&buf->a, &chan->a, consumed_idx);
 	if (ret)
 		return ret;
 
@@ -328,24 +400,23 @@ static int get_subbuf(struct rchan_buf *buf, unsigned long *consumed)
 	return 0;
 }
 
-static int put_subbuf(struct rchan_buf *buf, unsigned long consumed)
+int ltt_chanbuf_put_subbuf(struct ltt_chanbuf *buf, unsigned long consumed)
 {
-	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+	struct ltt_chan *chan = container_of(buf->a.chan, struct ltt_chan, a);
 	long consumed_new, consumed_old;
 
-	WARN_ON(atomic_long_read(&ltt_buf->active_readers) != 1);
+	WARN_ON(atomic_long_read(&buf->active_readers) != 1);
 
 	consumed_old = consumed;
-	consumed_new = SUBBUF_ALIGN(consumed_old, buf->chan);
-	WARN_ON_ONCE(RCHAN_SB_IS_NOREF(buf->rchan_rsb.pages));
-	RCHAN_SB_SET_NOREF(buf->rchan_rsb.pages);
+	consumed_new = SUBBUF_ALIGN(consumed_old, chan);
+	WARN_ON_ONCE(RCHAN_SB_IS_NOREF(buf->a.buf_rsb.pages));
+	RCHAN_SB_SET_NOREF(buf->a.buf_rsb.pages);
 
-	spin_lock(&ltt_buf->full_lock);
-	if (atomic_long_cmpxchg(&ltt_buf->consumed, consumed_old,
-				consumed_new)
+	spin_lock(&buf->full_lock);
+	if (atomic_long_cmpxchg(&buf->consumed, consumed_old, consumed_new)
 	    != consumed_old) {
 		/* We have been pushed by the writer. */
-		spin_unlock(&ltt_buf->full_lock);
+		spin_unlock(&buf->full_lock);
 		/*
 		 * We exchanged the subbuffer pages. No corruption possible
 		 * even if the writer did push us. No more -EIO possible.
@@ -355,664 +426,247 @@ static int put_subbuf(struct rchan_buf *buf, unsigned long consumed)
 		/* tell the client that buffer is now unfull */
 		int index;
 		long data;
-		index = SUBBUF_INDEX(consumed_old, buf->chan);
-		data = BUFFER_OFFSET(consumed_old, buf->chan);
-		ltt_buf_unfull(buf, index, data);
-		spin_unlock(&ltt_buf->full_lock);
+		index = SUBBUF_INDEX(consumed_old, chan);
+		data = BUFFER_OFFSET(consumed_old, chan);
+		ltt_buf_unfull(buf);
+		spin_unlock(&buf->full_lock);
 	}
 	return 0;
 }
 
-static unsigned long get_n_subbufs(struct rchan_buf *buf)
+static void switch_buffer(unsigned long data)
 {
-	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+	struct ltt_chanbuf *buf = (struct ltt_chanbuf *)data;
+	struct ltt_chan *chan = container_of(buf->a.chan, struct ltt_chan, a);
 
-	WARN_ON(atomic_long_read(&ltt_buf->active_readers) != 1);
-	return buf->chan->n_subbufs;
-}
+	ltt_force_switch(buf, FORCE_ACTIVE);
 
-static unsigned long get_subbuf_size(struct rchan_buf *buf)
-{
-	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
-
-	WARN_ON(atomic_long_read(&ltt_buf->active_readers) != 1);
-	return buf->chan->subbuf_size;
+	buf->switch_timer.expires += chan->switch_timer_interval;
+	add_timer_on(&buf->switch_timer, smp_processor_id());
 }
 
-static void switch_buffer(unsigned long data)
+static void ltt_chanbuf_start_switch_timer(struct ltt_chanbuf *buf)
 {
-	struct ltt_channel_buf_struct *ltt_buf =
-		(struct ltt_channel_buf_struct *)data;
-	struct rchan_buf *buf = ltt_buf->rbuf;
+	struct ltt_chan *chan = container_of(buf->a.chan, struct ltt_chan, a);
 
-	if (buf)
-		ltt_force_switch(buf, FORCE_ACTIVE);
+	if (!chan->switch_timer_interval)
+		return;
 
-	ltt_buf->switch_timer.expires += ltt_buf->switch_timer_interval;
-	add_timer_on(&ltt_buf->switch_timer, smp_processor_id());
+	init_timer(&buf->switch_timer);
+	buf->switch_timer.function = switch_buffer;
+	buf->switch_timer.expires = jiffies + chan->switch_timer_interval;
+	buf->switch_timer.data = (unsigned long)buf;
+	add_timer_on(&buf->switch_timer, buf->a.cpu);
 }
 
-static void start_switch_timer(struct ltt_channel_struct *ltt_channel)
+void ltt_chan_start_switch_timer(struct ltt_chan *chan)
 {
-	struct rchan *rchan = ltt_channel->trans_channel_data;
 	int cpu;
 
-	if (!ltt_channel->switch_timer_interval)
+	if (!chan->switch_timer_interval)
 		return;
 
-	// TODO : hotplug
+	get_online_cpus();
 	for_each_online_cpu(cpu) {
-		struct ltt_channel_buf_struct *ltt_buf;
-		struct rchan_buf *buf;
-
-		buf = rchan->buf[cpu];
-		ltt_buf = buf->chan_private;
-		buf->random_access = 1;
-		ltt_buf->switch_timer_interval =
-			ltt_channel->switch_timer_interval;
-		init_timer(&ltt_buf->switch_timer);
-		ltt_buf->switch_timer.function = switch_buffer;
-		ltt_buf->switch_timer.expires = jiffies +
-					ltt_buf->switch_timer_interval;
-		ltt_buf->switch_timer.data = (unsigned long)ltt_buf;
-		add_timer_on(&ltt_buf->switch_timer, cpu);
+		struct ltt_chanbuf *buf;
+
+		buf = per_cpu_ptr(chan->a.buf, cpu);
+		ltt_chanbuf_start_switch_timer(buf);
 	}
+	put_online_cpus();
 }
-
 /*
  * Cannot use del_timer_sync with add_timer_on, so use an IPI to locally
  * delete the timer.
  */
 static void stop_switch_timer_ipi(void *info)
 {
-	struct ltt_channel_buf_struct *ltt_buf =
-		(struct ltt_channel_buf_struct *)info;
+	struct ltt_chanbuf *buf = (struct ltt_chanbuf *)info;
 
-	del_timer(&ltt_buf->switch_timer);
+	del_timer(&buf->switch_timer);
 }
 
-static void stop_switch_timer(struct ltt_channel_struct *ltt_channel)
+static void ltt_chanbuf_stop_switch_timer(struct ltt_chanbuf *buf)
 {
-	struct rchan *rchan = ltt_channel->trans_channel_data;
-	int cpu;
+	struct ltt_chan *chan = container_of(buf->a.chan, struct ltt_chan, a);
 
-	if (!ltt_channel->switch_timer_interval)
+	if (!chan->switch_timer_interval)
 		return;
 
-	// TODO : hotplug
-	for_each_online_cpu(cpu) {
-		struct ltt_channel_buf_struct *ltt_buf;
-		struct rchan_buf *buf;
-
-		buf = rchan->buf[cpu];
-		ltt_buf = buf->chan_private;
-		smp_call_function(stop_switch_timer_ipi, ltt_buf, 1);
-		buf->random_access = 0;
-	}
+	smp_call_function(stop_switch_timer_ipi, buf, 1);
 }
 
-static struct ltt_channel_buf_access_ops ltt_channel_buf_accessor = {
-	.get_offset   = get_offset,
-	.get_consumed = get_consumed,
-	.get_subbuf = get_subbuf,
-	.put_subbuf = put_subbuf,
-	.is_finalized = is_finalized,
-	.get_n_subbufs = get_n_subbufs,
-	.get_subbuf_size = get_subbuf_size,
-	.open = _ltt_open,
-	.release = _ltt_release,
-	.start_switch_timer = start_switch_timer,
-	.stop_switch_timer = stop_switch_timer,
-};
-
-/**
- *	ltt_open - open file op for ltt files
- *	@inode: opened inode
- *	@file: opened file
- *
- *	Open implementation. Makes sure only one open instance of a buffer is
- *	done at a given moment.
- */
-static int ltt_open(struct inode *inode, struct file *file)
+void ltt_chan_stop_switch_timer(struct ltt_chan *chan)
 {
-	int ret;
-	struct rchan_buf *buf = inode->i_private;
+	int cpu;
 
-	ret = _ltt_open(buf);
-	if (!ret)
-		ret = ltt_relay_file_operations.open(inode, file);
-	return ret;
-}
+	if (!chan->switch_timer_interval)
+		return;
 
-/**
- *	ltt_release - release file op for ltt files
- *	@inode: opened inode
- *	@file: opened file
- *
- *	Release implementation.
- */
-static int ltt_release(struct inode *inode, struct file *file)
-{
-	struct rchan_buf *buf = inode->i_private;
-	int ret;
+	get_online_cpus();
+	for_each_online_cpu(cpu) {
+		struct ltt_chanbuf *buf;
 
-	_ltt_release(buf);
-	ret = ltt_relay_file_operations.release(inode, file);
-	WARN_ON(ret);
-	return ret;
+		buf = per_cpu_ptr(chan->a.buf, cpu);
+		ltt_chanbuf_stop_switch_timer(buf);
+	}
+	put_online_cpus();
 }
 
-/**
- *	ltt_poll - file op for ltt files
- *	@filp: the file
- *	@wait: poll table
- *
- *	Poll implementation.
- */
-static unsigned int ltt_poll(struct file *filp, poll_table *wait)
+static void ltt_chanbuf_switch(struct ltt_chanbuf *buf)
 {
-	unsigned int mask = 0;
-	struct inode *inode = filp->f_dentry->d_inode;
-	struct rchan_buf *buf = inode->i_private;
-	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
-
-	if (filp->f_mode & FMODE_READ) {
-		poll_wait_set_exclusive(wait);
-		poll_wait(filp, &ltt_buf->read_wait, wait);
-
-		WARN_ON(atomic_long_read(&ltt_buf->active_readers) != 1);
-		if (SUBBUF_TRUNC(local_read(&ltt_buf->offset),
-							buf->chan)
-		  - SUBBUF_TRUNC(atomic_long_read(&ltt_buf->consumed),
-							buf->chan)
-		  == 0) {
-			if (ltt_buf->finalized)
-				return POLLHUP;
-			else
-				return 0;
-		} else {
-			struct rchan *rchan = buf->chan;
-			if (SUBBUF_TRUNC(local_read(&ltt_buf->offset),
-					buf->chan)
-			  - SUBBUF_TRUNC(atomic_long_read(
-						&ltt_buf->consumed),
-					buf->chan)
-			  >= rchan->alloc_size)
-				return POLLPRI | POLLRDBAND;
-			else
-				return POLLIN | POLLRDNORM;
-		}
-	}
-	return mask;
+	ltt_force_switch(buf, FORCE_ACTIVE);
 }
 
 /**
- *	ltt_ioctl - control on the debugfs file
+ *	ltt_chanbuf_hotcpu_callback - CPU hotplug callback
+ *	@nb: notifier block
+ *	@action: hotplug action to take
+ *	@hcpu: CPU number
  *
- *	@inode: the inode
- *	@filp: the file
- *	@cmd: the command
- *	@arg: command arg
- *
- *	This ioctl implements three commands necessary for a minimal
- *	producer/consumer implementation :
- *	RELAY_GET_SUBBUF
- *		Get the next sub buffer that can be read. It never blocks.
- *	RELAY_PUT_SUBBUF
- *		Release the currently read sub-buffer. Parameter is the last
- *		put subbuffer (returned by GET_SUBBUF).
- *	RELAY_GET_N_BUBBUFS
- *		returns the number of sub buffers in the per cpu channel.
- *	RELAY_GET_SUBBUF_SIZE
- *		returns the size of the sub buffers.
- */
-static int ltt_ioctl(struct inode *inode, struct file *filp,
-		unsigned int cmd, unsigned long arg)
-{
-	struct rchan_buf *buf = inode->i_private;
-	u32 __user *argp = (u32 __user *)arg;
-
-	switch (cmd) {
-	case RELAY_GET_SUBBUF:
-	{
-		unsigned long consumed;
-		int ret;
-
-		ret = get_subbuf(buf, &consumed);
-		if (ret)
-			return ret;
-		else
-			return put_user((u32)consumed, argp);
-		break;
-	}
-	case RELAY_PUT_SUBBUF:
-	{
-		struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
-		u32 uconsumed_old;
-		int ret;
-		long consumed_old;
-
-		ret = get_user(uconsumed_old, argp);
-		if (ret)
-			return ret; /* will return -EFAULT */
-
-		consumed_old = atomic_long_read(&ltt_buf->consumed);
-		consumed_old = consumed_old & (~0xFFFFFFFFL);
-		consumed_old = consumed_old | uconsumed_old;
-		ret = put_subbuf(buf, consumed_old);
-		if (ret)
-			return ret;
-		break;
-	}
-	case RELAY_GET_N_SUBBUFS:
-		return put_user((u32)get_n_subbufs(buf), argp);
-		break;
-	case RELAY_GET_SUBBUF_SIZE:
-		return put_user((u32)get_subbuf_size(buf), argp);
-		break;
-	default:
-		return -ENOIOCTLCMD;
-	}
-	return 0;
-}
-
-#ifdef CONFIG_COMPAT
-static long ltt_compat_ioctl(struct file *file, unsigned int cmd,
-		unsigned long arg)
-{
-	long ret = -ENOIOCTLCMD;
-
-	lock_kernel();
-	ret = ltt_ioctl(file->f_dentry->d_inode, file, cmd, arg);
-	unlock_kernel();
-
-	return ret;
-}
-#endif
-
-static void ltt_relay_pipe_buf_release(struct pipe_inode_info *pipe,
-				   struct pipe_buffer *pbuf)
-{
-}
-
-static struct pipe_buf_operations ltt_relay_pipe_buf_ops = {
-	.can_merge = 0,
-	.map = generic_pipe_buf_map,
-	.unmap = generic_pipe_buf_unmap,
-	.confirm = generic_pipe_buf_confirm,
-	.release = ltt_relay_pipe_buf_release,
-	.steal = generic_pipe_buf_steal,
-	.get = generic_pipe_buf_get,
-};
-
-static void ltt_relay_page_release(struct splice_pipe_desc *spd, unsigned int i)
-{
-}
-
-/*
- *	subbuf_splice_actor - splice up to one subbuf's worth of data
+ *	Returns the success/failure of the operation. (%NOTIFY_OK, %NOTIFY_BAD)
  */
-static int subbuf_splice_actor(struct file *in,
-			       loff_t *ppos,
-			       struct pipe_inode_info *pipe,
-			       size_t len,
-			       unsigned int flags)
+static
+int ltt_chanbuf_hotcpu_callback(struct notifier_block *nb,
+					  unsigned long action,
+					  void *hcpu)
 {
-	struct rchan_buf *buf = in->private_data;
-	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
-	unsigned int poff, subbuf_pages, nr_pages;
-	struct page *pages[PIPE_BUFFERS];
-	struct partial_page partial[PIPE_BUFFERS];
-	struct splice_pipe_desc spd = {
-		.pages = pages,
-		.nr_pages = 0,
-		.partial = partial,
-		.flags = flags,
-		.ops = &ltt_relay_pipe_buf_ops,
-		.spd_release = ltt_relay_page_release,
-	};
-	long consumed_old, consumed_idx, roffset;
-	unsigned long bytes_avail;
+	unsigned int cpu = (unsigned long)hcpu;
+
+	switch (action) {
+	case CPU_DOWN_FAILED:
+	case CPU_DOWN_FAILED_FROZEN:
+	case CPU_ONLINE:
+	case CPU_ONLINE_FROZEN:
+		ltt_chan_for_each_channel(ltt_chanbuf_start_switch_timer, cpu);
+		return NOTIFY_OK;
+
+	case CPU_DOWN_PREPARE:
+	case CPU_DOWN_PREPARE_FROZEN:
+		/*
+		 * Performs an IPI to delete the timer locally on the target
+		 * CPU.
+		 */
+		ltt_chan_for_each_channel(ltt_chanbuf_stop_switch_timer, cpu);
+		return NOTIFY_OK;
 
-	/*
-	 * Check that a GET_SUBBUF ioctl has been done before.
-	 */
-	WARN_ON(atomic_long_read(&ltt_buf->active_readers) != 1);
-	consumed_old = atomic_long_read(&ltt_buf->consumed);
-	consumed_old += *ppos;
-	consumed_idx = SUBBUF_INDEX(consumed_old, buf->chan);
+	case CPU_DEAD:
+	case CPU_DEAD_FROZEN:
+		/*
+		 * Performing a buffer switch on a remote CPU. Performed by
+		 * the CPU responsible for doing the hotunplug after the target
+		 * CPU stopped running completely. Ensures that all data
+		 * from that remote CPU is flushed.
+		 */
+		ltt_chan_for_each_channel(ltt_chanbuf_switch, cpu);
+		return NOTIFY_OK;
 
-	/*
-	 * Adjust read len, if longer than what is available.
-	 * Max read size is 1 subbuffer due to get_subbuf/put_subbuf for
-	 * protection.
-	 */
-	bytes_avail = buf->chan->subbuf_size;
-	WARN_ON(bytes_avail > buf->chan->alloc_size);
-	len = min_t(size_t, len, bytes_avail);
-	subbuf_pages = bytes_avail >> PAGE_SHIFT;
-	nr_pages = min_t(unsigned int, subbuf_pages, PIPE_BUFFERS);
-	roffset = consumed_old & PAGE_MASK;
-	poff = consumed_old & ~PAGE_MASK;
-	printk_dbg(KERN_DEBUG "SPLICE actor len %zu pos %zd write_pos %ld\n",
-		len, (ssize_t)*ppos, local_read(&ltt_buf->offset));
-
-	for (; spd.nr_pages < nr_pages; spd.nr_pages++) {
-		unsigned int this_len;
-		struct page *page;
-
-		if (!len)
-			break;
-		printk_dbg(KERN_DEBUG "SPLICE actor loop len %zu roffset %ld\n",
-			len, roffset);
-
-		this_len = PAGE_SIZE - poff;
-		page = ltt_relay_read_get_page(buf, roffset);
-		spd.pages[spd.nr_pages] = page;
-		spd.partial[spd.nr_pages].offset = poff;
-		spd.partial[spd.nr_pages].len = this_len;
-
-		poff = 0;
-		roffset += PAGE_SIZE;
-		len -= this_len;
+	default:
+		return NOTIFY_DONE;
 	}
-
-	if (!spd.nr_pages)
-		return 0;
-
-	return splice_to_pipe(pipe, &spd);
 }
 
-static ssize_t ltt_relay_file_splice_read(struct file *in,
-				      loff_t *ppos,
-				      struct pipe_inode_info *pipe,
-				      size_t len,
-				      unsigned int flags)
+static
+void ltt_relay_print_written(struct ltt_chan *chan, long cons_off,
+			     unsigned int cpu)
 {
-	ssize_t spliced;
-	int ret;
-
-	ret = 0;
-	spliced = 0;
-
-	printk_dbg(KERN_DEBUG "SPLICE read len %zu pos %zd\n",
-		len, (ssize_t)*ppos);
-	while (len && !spliced) {
-		ret = subbuf_splice_actor(in, ppos, pipe, len, flags);
-		printk_dbg(KERN_DEBUG "SPLICE read loop ret %d\n", ret);
-		if (ret < 0)
-			break;
-		else if (!ret) {
-			if (flags & SPLICE_F_NONBLOCK)
-				ret = -EAGAIN;
-			break;
-		}
-
-		*ppos += ret;
-		if (ret > len)
-			len = 0;
-		else
-			len -= ret;
-		spliced += ret;
-	}
-
-	if (spliced)
-		return spliced;
-
-	return ret;
-}
-
-static void ltt_relay_print_written(
-		struct ltt_channel_struct *ltt_chan,
-		long cons_off, unsigned int cpu)
-{
-	struct rchan *rchan = ltt_chan->trans_channel_data;
-	struct ltt_channel_buf_struct *ltt_buf = rchan->buf[cpu]->chan_private;
+	struct ltt_chanbuf *buf = per_cpu_ptr(chan->a.buf, cpu);
 	long cons_idx, events_count;
 
-	cons_idx = SUBBUF_INDEX(cons_off, rchan);
-	events_count = local_read(&ltt_buf->commit_count[cons_idx].events);
+	cons_idx = SUBBUF_INDEX(cons_off, chan);
+	events_count = local_read(&buf->commit_count[cons_idx].events);
 
 	if (events_count)
 		printk(KERN_INFO
 			"LTT: %lu events written in channel %s "
 			"(cpu %u, index %lu)\n",
-			events_count, ltt_chan->channel_name, cpu, cons_idx);
+			events_count, chan->a.filename, cpu, cons_idx);
 }
 
-static void ltt_relay_print_subbuffer_errors(
-		struct ltt_channel_struct *ltt_chan,
-		long cons_off, unsigned int cpu)
+static
+void ltt_relay_print_subbuffer_errors(struct ltt_chanbuf *buf,
+				      struct ltt_chan *chan, long cons_off,
+				      unsigned int cpu)
 {
-	struct rchan *rchan = ltt_chan->trans_channel_data;
-	struct ltt_channel_buf_struct *ltt_buf = rchan->buf[cpu]->chan_private;
 	long cons_idx, commit_count, commit_count_sb, write_offset;
 
-	cons_idx = SUBBUF_INDEX(cons_off, rchan);
-	commit_count = local_read(&ltt_buf->commit_count[cons_idx].cc);
-	commit_count_sb = local_read(&ltt_buf->commit_count[cons_idx].cc_sb);
+	cons_idx = SUBBUF_INDEX(cons_off, chan);
+	commit_count = local_read(&buf->commit_count[cons_idx].cc);
+	commit_count_sb = local_read(&buf->commit_count[cons_idx].cc_sb);
 	/*
 	 * No need to order commit_count and write_offset reads because we
 	 * execute after trace is stopped when there are no readers left.
 	 */
-	write_offset = local_read(&ltt_buf->offset);
+	write_offset = local_read(&buf->offset);
 	printk(KERN_WARNING
-		"LTT : unread channel %s offset is %ld "
-		"and cons_off : %ld (cpu %u)\n",
-		ltt_chan->channel_name, write_offset, cons_off, cpu);
+	       "LTT : unread channel %s offset is %ld "
+	       "and cons_off : %ld (cpu %u)\n",
+	       chan->a.filename, write_offset, cons_off, cpu);
 	/* Check each sub-buffer for non filled commit count */
-	if (((commit_count - rchan->subbuf_size) & ltt_chan->commit_count_mask)
-	    - (BUFFER_TRUNC(cons_off, rchan) >> ltt_chan->n_subbufs_order)
+	if (((commit_count - chan->a.sb_size) & chan->commit_count_mask)
+	    - (BUFFER_TRUNC(cons_off, chan) >> chan->a.n_sb_order)
 	    != 0)
 		printk(KERN_ALERT
-			"LTT : %s : subbuffer %lu has non filled "
-			"commit count [cc, cc_sb] [%lu,%lu].\n",
-			ltt_chan->channel_name, cons_idx, commit_count,
-			commit_count_sb);
+		       "LTT : %s : subbuffer %lu has non filled "
+		       "commit count [cc, cc_sb] [%lu,%lu].\n",
+		       chan->a.filename, cons_idx, commit_count,
+		       commit_count_sb);
 	printk(KERN_ALERT "LTT : %s : commit count : %lu, subbuf size %zd\n",
-			ltt_chan->channel_name, commit_count,
-			rchan->subbuf_size);
+	       chan->a.filename, commit_count, chan->a.sb_size);
 }
 
-static void ltt_relay_print_errors(struct ltt_trace_struct *trace,
-		struct ltt_channel_struct *ltt_chan, int cpu)
+static
+void ltt_relay_print_errors(struct ltt_chanbuf *buf, struct ltt_chan *chan,
+			    struct ltt_trace *trace, int cpu)
 {
-	struct rchan *rchan = ltt_chan->trans_channel_data;
-	struct ltt_channel_buf_struct *ltt_buf = rchan->buf[cpu]->chan_private;
 	long cons_off;
 
 	/*
 	 * Can be called in the error path of allocation when
 	 * trans_channel_data is not yet set.
 	 */
-	if (!rchan)
+	if (!chan)
 		return;
-	for (cons_off = 0; cons_off < rchan->alloc_size;
-	     cons_off = SUBBUF_ALIGN(cons_off, rchan))
-		ltt_relay_print_written(ltt_chan, cons_off, cpu);
-	for (cons_off = atomic_long_read(&ltt_buf->consumed);
-			(SUBBUF_TRUNC(local_read(&ltt_buf->offset),
-				      rchan)
+	for (cons_off = 0; cons_off < chan->a.buf_size;
+	     cons_off = SUBBUF_ALIGN(cons_off, chan))
+		ltt_relay_print_written(chan, cons_off, cpu);
+	for (cons_off = atomic_long_read(&buf->consumed);
+			(SUBBUF_TRUNC(local_read(&buf->offset), chan)
 			 - cons_off) > 0;
-			cons_off = SUBBUF_ALIGN(cons_off, rchan))
-		ltt_relay_print_subbuffer_errors(ltt_chan, cons_off, cpu);
+			cons_off = SUBBUF_ALIGN(cons_off, chan))
+		ltt_relay_print_subbuffer_errors(buf, chan, cons_off, cpu);
 }
 
-static void ltt_relay_print_buffer_errors(struct ltt_channel_struct *ltt_chan,
-		unsigned int cpu)
+static
+void ltt_relay_print_buffer_errors(struct ltt_chan *chan, unsigned int cpu)
 {
-	struct ltt_trace_struct *trace = ltt_chan->trace;
-	struct rchan *rchan = ltt_chan->trans_channel_data;
-	struct ltt_channel_buf_struct *ltt_buf = rchan->buf[cpu]->chan_private;
+	struct ltt_trace *trace = chan->trace;
+	struct ltt_chanbuf *buf = per_cpu_ptr(chan->a.buf, cpu);
 
-	if (local_read(&ltt_buf->events_lost))
+	if (local_read(&buf->events_lost))
 		printk(KERN_ALERT
-			"LTT : %s : %ld events lost "
-			"in %s channel (cpu %u).\n",
-			ltt_chan->channel_name,
-			local_read(&ltt_buf->events_lost),
-			ltt_chan->channel_name, cpu);
-	if (local_read(&ltt_buf->corrupted_subbuffers))
+		       "LTT : %s : %ld events lost "
+		       "in %s channel (cpu %u).\n",
+		       chan->a.filename, local_read(&buf->events_lost),
+		       chan->a.filename, cpu);
+	if (local_read(&buf->corrupted_subbuffers))
 		printk(KERN_ALERT
-			"LTT : %s : %ld corrupted subbuffers "
-			"in %s channel (cpu %u).\n",
-			ltt_chan->channel_name,
-			local_read(&ltt_buf->corrupted_subbuffers),
-			ltt_chan->channel_name, cpu);
+		       "LTT : %s : %ld corrupted subbuffers "
+		       "in %s channel (cpu %u).\n",
+		       chan->a.filename,
+		       local_read(&buf->corrupted_subbuffers),
+		       chan->a.filename, cpu);
 
-	ltt_relay_print_errors(trace, ltt_chan, cpu);
+	ltt_relay_print_errors(buf, chan, trace, cpu);
 }
 
-static void ltt_relay_remove_dirs(struct ltt_trace_struct *trace)
+static void ltt_relay_remove_dirs(struct ltt_trace *trace)
 {
 	ltt_ascii_remove_dir(trace);
 	debugfs_remove(trace->dentry.trace_root);
 }
 
-/*
- * Create ltt buffer.
- */
-static int ltt_relay_create_buffer(struct ltt_trace_struct *trace,
-		struct ltt_channel_struct *ltt_chan, struct rchan_buf *buf,
-		unsigned int cpu, unsigned int n_subbufs)
-{
-	struct ltt_channel_buf_struct *ltt_buf;
-	unsigned int j;
-
-	ltt_buf = kzalloc_node(sizeof(*ltt_buf), GFP_KERNEL, cpu_to_node(cpu));
-	if (!ltt_buf)
-		return -ENOMEM;
-
-	ltt_buf->commit_count =
-		kzalloc_node(ALIGN(sizeof(*ltt_buf->commit_count) * n_subbufs,
-				   1 << INTERNODE_CACHE_SHIFT),
-			GFP_KERNEL, cpu_to_node(cpu));
-	if (!ltt_buf->commit_count) {
-		kfree(ltt_buf);
-		return -ENOMEM;
-	}
-
-#ifdef CONFIG_LTT_VMCORE
-	ltt_buf->commit_seq =
-		kzalloc_node(ALIGN(sizeof(*ltt_buf->commit_seq) * n_subbufs,
-				   1 << INTERNODE_CACHE_SHIFT),
-			GFP_KERNEL, cpu_to_node(cpu));
-	if (!ltt_buf->commit_seq) {
-		kfree(ltt_buf->commit_count);
-		kfree(ltt_buf);
-		return -ENOMEM;
-	}
-#endif
-
-	buf->chan_private = ltt_buf;
-
-	kref_get(&trace->kref);
-	kref_get(&trace->ltt_transport_kref);
-	local_set(&ltt_buf->offset, ltt_subbuffer_header_size());
-	atomic_long_set(&ltt_buf->consumed, 0);
-	atomic_long_set(&ltt_buf->active_readers, 0);
-	for (j = 0; j < n_subbufs; j++) {
-		local_set(&ltt_buf->commit_count[j].cc, 0);
-		local_set(&ltt_buf->commit_count[j].cc_sb, 0);
-		local_set(&ltt_buf->commit_count[j].events, 0);
-	}
-	init_waitqueue_head(&ltt_buf->write_wait);
-	init_waitqueue_head(&ltt_buf->read_wait);
-	spin_lock_init(&ltt_buf->full_lock);
-
-	RCHAN_SB_CLEAR_NOREF(buf->rchan_wsb[0].pages);
-	ltt_buffer_begin(buf, trace->start_tsc, 0);
-	/* atomic_add made on local variable on data that belongs to
-	 * various CPUs : ok because tracing not started (for this cpu). */
-	local_add(ltt_subbuffer_header_size(), &ltt_buf->commit_count[0].cc);
-
-	local_set(&ltt_buf->events_lost, 0);
-	local_set(&ltt_buf->corrupted_subbuffers, 0);
-	ltt_buf->finalized = 0;
-	ltt_buf->rbuf = buf;
-
-	return 0;
-}
-
-static void ltt_relay_destroy_buffer(struct ltt_channel_struct *ltt_chan,
-		unsigned int cpu)
-{
-	struct ltt_trace_struct *trace = ltt_chan->trace;
-	struct rchan *rchan = ltt_chan->trans_channel_data;
-	struct ltt_channel_buf_struct *ltt_buf = rchan->buf[cpu]->chan_private;
-
-	kref_put(&ltt_chan->trace->ltt_transport_kref,
-		ltt_release_transport);
-	ltt_relay_print_buffer_errors(ltt_chan, cpu);
-#ifdef CONFIG_LTT_VMCORE
-	kfree(ltt_buf->commit_seq);
-#endif
-	kfree(ltt_buf->commit_count);
-	kfree(ltt_buf);
-	kref_put(&trace->kref, ltt_release_trace);
-	wake_up_interruptible(&trace->kref_wq);
-}
-
-/*
- * Create channel.
- */
-static int ltt_relay_create_channel(const char *trace_name,
-		struct ltt_trace_struct *trace, struct dentry *dir,
-		const char *channel_name, struct ltt_channel_struct *ltt_chan,
-		unsigned int subbuf_size, unsigned int n_subbufs,
-		int overwrite)
-{
-	char *tmpname;
-	unsigned int tmpname_len;
-	int err = 0;
-
-	tmpname = kmalloc(PATH_MAX, GFP_KERNEL);
-	if (!tmpname)
-		return EPERM;
-	if (overwrite) {
-		strncpy(tmpname, LTT_FLIGHT_PREFIX, PATH_MAX-1);
-		strncat(tmpname, channel_name,
-			PATH_MAX-1-sizeof(LTT_FLIGHT_PREFIX));
-	} else {
-		strncpy(tmpname, channel_name, PATH_MAX-1);
-	}
-	strncat(tmpname, "_", PATH_MAX-1-strlen(tmpname));
-
-	ltt_chan->trace = trace;
-	ltt_chan->overwrite = overwrite;
-	ltt_chan->n_subbufs_order = get_count_order(n_subbufs);
-	ltt_chan->commit_count_mask = (~0UL >> ltt_chan->n_subbufs_order);
-	ltt_chan->trans_channel_data = ltt_relay_open(tmpname,
-			dir,
-			subbuf_size,
-			n_subbufs,
-			&trace->callbacks,
-			ltt_chan,
-			overwrite);
-	tmpname_len = strlen(tmpname);
-	if (tmpname_len > 0) {
-		/* Remove final _ for pretty printing */
-		tmpname[tmpname_len-1] = '\0';
-	}
-	if (ltt_chan->trans_channel_data == NULL) {
-		printk(KERN_ERR "LTT : Can't open %s channel for trace %s\n",
-				tmpname, trace_name);
-		goto relay_open_error;
-	}
-
-	ltt_chan->buf_access_ops = &ltt_channel_buf_accessor;
-
-	err = 0;
-	goto end;
-
-relay_open_error:
-	err = EPERM;
-end:
-	kfree(tmpname);
-	return err;
-}
-
-static int ltt_relay_create_dirs(struct ltt_trace_struct *new_trace)
+static int ltt_relay_create_dirs(struct ltt_trace *new_trace)
 {
 	struct dentry *ltt_root_dentry;
 	int ret;
@@ -1022,11 +676,11 @@ static int ltt_relay_create_dirs(struct ltt_trace_struct *new_trace)
 		return ENOENT;
 
 	new_trace->dentry.trace_root = debugfs_create_dir(new_trace->trace_name,
-			ltt_root_dentry);
+							  ltt_root_dentry);
 	put_ltt_root();
 	if (new_trace->dentry.trace_root == NULL) {
 		printk(KERN_ERR "LTT : Trace directory name %s already taken\n",
-				new_trace->trace_name);
+		       new_trace->trace_name);
 		return EEXIST;
 	}
 	ret = ltt_ascii_create_dir(new_trace);
@@ -1034,9 +688,6 @@ static int ltt_relay_create_dirs(struct ltt_trace_struct *new_trace)
 		printk(KERN_WARNING "LTT : Unable to create ascii output file "
 				    "for trace %s\n", new_trace->trace_name);
 
-	new_trace->callbacks.create_buf_file = ltt_create_buf_file_callback;
-	new_trace->callbacks.remove_buf_file = ltt_remove_buf_file_callback;
-
 	return 0;
 }
 
@@ -1046,59 +697,45 @@ static int ltt_relay_create_dirs(struct ltt_trace_struct *new_trace)
  * Must be called when no tracing is active in the channel, because of
  * accesses across CPUs.
  */
-static notrace void ltt_relay_buffer_flush(struct rchan_buf *buf)
+static notrace void ltt_relay_buffer_flush(struct ltt_chanbuf *buf)
 {
-	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
-
-	ltt_buf->finalized = 1;
+	buf->finalized = 1;
 	ltt_force_switch(buf, FORCE_FLUSH);
 }
 
-static void ltt_relay_async_wakeup_chan(struct ltt_channel_struct *ltt_channel)
+static void ltt_relay_async_wakeup_chan(struct ltt_chan *chan)
 {
-	struct rchan *rchan = ltt_channel->trans_channel_data;
 	unsigned int i;
 
 	for_each_possible_cpu(i) {
-		struct ltt_channel_buf_struct *ltt_buf;
+		struct ltt_chanbuf *buf;
 
-		if (!rchan->buf[i])
+		buf = per_cpu_ptr(chan->a.buf, i);
+		if (!buf->a.allocated)
 			continue;
 
-		ltt_buf = rchan->buf[i]->chan_private;
-		if (ltt_poll_deliver(ltt_channel, ltt_buf,
-				     rchan, rchan->buf[i]))
-			wake_up_interruptible(&ltt_buf->read_wait);
+		if (ltt_poll_deliver(buf, chan))
+			wake_up_interruptible(&buf->read_wait);
 	}
 }
 
-static void ltt_relay_finish_buffer(struct ltt_channel_struct *ltt_channel,
-		unsigned int cpu)
+static void ltt_relay_finish_buffer(struct ltt_chan *chan, unsigned int cpu)
 {
-	struct rchan *rchan = ltt_channel->trans_channel_data;
+	struct ltt_chanbuf *buf = per_cpu_ptr(chan->a.buf, cpu);
 
-	if (rchan->buf[cpu]) {
-		struct ltt_channel_buf_struct *ltt_buf =
-				rchan->buf[cpu]->chan_private;
-		ltt_relay_buffer_flush(rchan->buf[cpu]);
-		ltt_relay_wake_writers(ltt_buf);
+	if (buf->a.allocated) {
+		ltt_relay_buffer_flush(buf);
+		ltt_relay_wake_writers(buf);
 	}
 }
 
 
-static void ltt_relay_finish_channel(struct ltt_channel_struct *ltt_channel)
+static void ltt_relay_finish_channel(struct ltt_chan *chan)
 {
 	unsigned int i;
 
 	for_each_possible_cpu(i)
-		ltt_relay_finish_buffer(ltt_channel, i);
-}
-
-static void ltt_relay_remove_channel(struct ltt_channel_struct *channel)
-{
-	struct rchan *rchan = channel->trans_channel_data;
-
-	ltt_relay_close(rchan);
+		ltt_relay_finish_buffer(chan, i);
 }
 
 /*
@@ -1106,28 +743,24 @@ static void ltt_relay_remove_channel(struct ltt_channel_struct *channel)
  * blocking mode.  If one of the active traces has free space below a
  * specific threshold value, we reenable preemption and block.
  */
-static int ltt_relay_user_blocking(struct ltt_trace_struct *trace,
-		unsigned int chan_index, size_t data_size,
-		struct user_dbg_data *dbg)
+static
+int ltt_relay_user_blocking(struct ltt_trace *trace, unsigned int chan_index,
+			    size_t data_size, struct user_dbg_data *dbg)
 {
-	struct rchan *rchan;
-	struct ltt_channel_buf_struct *ltt_buf;
-	struct ltt_channel_struct *channel;
-	struct rchan_buf *relay_buf;
+	struct ltt_chanbuf *buf;
+	struct ltt_chan *chan;
 	int cpu;
 	DECLARE_WAITQUEUE(wait, current);
 
-	channel = &trace->channels[chan_index];
-	rchan = channel->trans_channel_data;
+	chan = &trace->channels[chan_index];
 	cpu = smp_processor_id();
-	relay_buf = rchan->buf[cpu];
-	ltt_buf = relay_buf->chan_private;
+	buf = per_cpu_ptr(chan->a.buf, cpu);
 
 	/*
 	 * Check if data is too big for the channel : do not
 	 * block for it.
 	 */
-	if (LTT_RESERVE_CRITICAL + data_size > relay_buf->chan->subbuf_size)
+	if (LTT_RESERVE_CRITICAL + data_size > chan->a.sb_size)
 		return 0;
 
 	/*
@@ -1135,62 +768,58 @@ static int ltt_relay_user_blocking(struct ltt_trace_struct *trace,
 	 * beginning after we resume (cpu id may have changed
 	 * while preemption is active).
 	 */
-	spin_lock(&ltt_buf->full_lock);
-	if (!channel->overwrite) {
-		dbg->write = local_read(&ltt_buf->offset);
-		dbg->read = atomic_long_read(&ltt_buf->consumed);
+	spin_lock(&buf->full_lock);
+	if (!chan->overwrite) {
+		dbg->write = local_read(&buf->offset);
+		dbg->read = atomic_long_read(&buf->consumed);
 		dbg->avail_size = dbg->write + LTT_RESERVE_CRITICAL + data_size
-				  - SUBBUF_TRUNC(dbg->read,
-						 relay_buf->chan);
-		if (dbg->avail_size > rchan->alloc_size) {
+				  - SUBBUF_TRUNC(dbg->read, chan);
+		if (dbg->avail_size > chan->a.buf_size) {
 			__set_current_state(TASK_INTERRUPTIBLE);
-			add_wait_queue(&ltt_buf->write_wait, &wait);
-			spin_unlock(&ltt_buf->full_lock);
+			add_wait_queue(&buf->write_wait, &wait);
+			spin_unlock(&buf->full_lock);
 			preempt_enable();
 			schedule();
 			__set_current_state(TASK_RUNNING);
-			remove_wait_queue(&ltt_buf->write_wait, &wait);
+			remove_wait_queue(&buf->write_wait, &wait);
 			if (signal_pending(current))
 				return -ERESTARTSYS;
 			preempt_disable();
 			return 1;
 		}
 	}
-	spin_unlock(&ltt_buf->full_lock);
+	spin_unlock(&buf->full_lock);
 	return 0;
 }
 
-static void ltt_relay_print_user_errors(struct ltt_trace_struct *trace,
-		unsigned int chan_index, size_t data_size,
-		struct user_dbg_data *dbg, int cpu)
+static
+void ltt_relay_print_user_errors(struct ltt_trace *trace,
+				 unsigned int chan_index, size_t data_size,
+				 struct user_dbg_data *dbg, int cpu)
 {
-	struct rchan *rchan;
-	struct ltt_channel_buf_struct *ltt_buf;
-	struct ltt_channel_struct *channel;
-	struct rchan_buf *relay_buf;
+	struct ltt_chanbuf *buf;
+	struct ltt_chan *chan;
 
-	channel = &trace->channels[chan_index];
-	rchan = channel->trans_channel_data;
-	relay_buf = rchan->buf[cpu];
-	ltt_buf = relay_buf->chan_private;
+	chan = &trace->channels[chan_index];
+	buf = per_cpu_ptr(chan->a.buf, cpu);
 
 	printk(KERN_ERR "Error in LTT usertrace : "
-	"buffer full : event lost in blocking "
-	"mode. Increase LTT_RESERVE_CRITICAL.\n");
+	       "buffer full : event lost in blocking "
+	       "mode. Increase LTT_RESERVE_CRITICAL.\n");
 	printk(KERN_ERR "LTT nesting level is %u.\n",
-		per_cpu(ltt_nesting, cpu));
-	printk(KERN_ERR "LTT avail size %lu.\n",
-		dbg->avail_size);
-	printk(KERN_ERR "avai write : %lu, read : %lu\n",
-			dbg->write, dbg->read);
+	       per_cpu(ltt_nesting, cpu));
+	printk(KERN_ERR "LTT available size %lu.\n",
+	       dbg->avail_size);
+	printk(KERN_ERR "available write : %lu, read : %lu\n",
+	       dbg->write, dbg->read);
 
-	dbg->write = local_read(&ltt_buf->offset);
-	dbg->read = atomic_long_read(&ltt_buf->consumed);
+	dbg->write = local_read(&buf->offset);
+	dbg->read = atomic_long_read(&buf->consumed);
 
-	printk(KERN_ERR "LTT cur size %lu.\n",
+	printk(KERN_ERR "LTT current size %lu.\n",
 		dbg->write + LTT_RESERVE_CRITICAL + data_size
-		- SUBBUF_TRUNC(dbg->read, relay_buf->chan));
-	printk(KERN_ERR "cur write : %lu, read : %lu\n",
+		- SUBBUF_TRUNC(dbg->read, chan));
+	printk(KERN_ERR "current write : %lu, read : %lu\n",
 			dbg->write, dbg->read);
 }
 
@@ -1214,17 +843,17 @@ static void ltt_relay_print_user_errors(struct ltt_trace_struct *trace,
  *
  * Note : offset_old should never be 0 here.
  */
-static void ltt_reserve_switch_old_subbuf(
-		struct ltt_channel_struct *ltt_channel,
-		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
-		struct rchan_buf *buf,
-		struct ltt_reserve_switch_offsets *offsets, u64 *tsc)
+static
+void ltt_reserve_switch_old_subbuf(struct ltt_chanbuf *buf,
+				   struct ltt_chan *chan,
+				   struct ltt_reserve_switch_offsets *offsets,
+				   u64 *tsc)
 {
-	long oldidx = SUBBUF_INDEX(offsets->old - 1, rchan);
+	long oldidx = SUBBUF_INDEX(offsets->old - 1, chan);
 	long commit_count, padding_size;
 
-	padding_size = rchan->subbuf_size
-			- (SUBBUF_OFFSET(offsets->old - 1, rchan) + 1);
+	padding_size = chan->a.sb_size
+			- (SUBBUF_OFFSET(offsets->old - 1, chan) + 1);
 	ltt_buffer_end(buf, *tsc, offsets->old, oldidx);
 
 	/*
@@ -1233,13 +862,11 @@ static void ltt_reserve_switch_old_subbuf(
 	 * sent by get_subbuf() when it does its smp_rmb().
 	 */
 	barrier();
-	local_add(padding_size,
-		  &ltt_buf->commit_count[oldidx].cc);
-	commit_count = local_read(&ltt_buf->commit_count[oldidx].cc);
-	ltt_check_deliver(ltt_channel, ltt_buf, rchan, buf,
-		offsets->old - 1, commit_count, oldidx);
-	ltt_write_commit_counter(buf, ltt_buf, oldidx,
-		offsets->old, commit_count, padding_size);
+	local_add(padding_size, &buf->commit_count[oldidx].cc);
+	commit_count = local_read(&buf->commit_count[oldidx].cc);
+	ltt_check_deliver(buf, chan, offsets->old - 1, commit_count, oldidx);
+	ltt_write_commit_counter(buf, chan, oldidx, offsets->old, commit_count,
+				 padding_size);
 }
 
 /*
@@ -1249,13 +876,13 @@ static void ltt_reserve_switch_old_subbuf(
  * sub-buffer before this code gets executed, caution.  The commit makes sure
  * that this code is executed before the deliver of this sub-buffer.
  */
-static void ltt_reserve_switch_new_subbuf(
-		struct ltt_channel_struct *ltt_channel,
-		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
-		struct rchan_buf *buf,
-		struct ltt_reserve_switch_offsets *offsets, u64 *tsc)
+static
+void ltt_reserve_switch_new_subbuf(struct ltt_chanbuf *buf,
+				   struct ltt_chan *chan,
+				   struct ltt_reserve_switch_offsets *offsets,
+				   u64 *tsc)
 {
-	long beginidx = SUBBUF_INDEX(offsets->begin, rchan);
+	long beginidx = SUBBUF_INDEX(offsets->begin, chan);
 	long commit_count;
 
 	ltt_buffer_begin(buf, *tsc, beginidx);
@@ -1266,14 +893,12 @@ static void ltt_reserve_switch_new_subbuf(
 	 * sent by get_subbuf() when it does its smp_rmb().
 	 */
 	barrier();
-	local_add(ltt_subbuffer_header_size(),
-		  &ltt_buf->commit_count[beginidx].cc);
-	commit_count = local_read(&ltt_buf->commit_count[beginidx].cc);
+	local_add(ltt_sb_header_size(), &buf->commit_count[beginidx].cc);
+	commit_count = local_read(&buf->commit_count[beginidx].cc);
 	/* Check if the written buffer has to be delivered */
-	ltt_check_deliver(ltt_channel, ltt_buf, rchan, buf,
-		offsets->begin, commit_count, beginidx);
-	ltt_write_commit_counter(buf, ltt_buf, beginidx,
-		offsets->begin, commit_count, ltt_subbuffer_header_size());
+	ltt_check_deliver(buf, chan, offsets->begin, commit_count, beginidx);
+	ltt_write_commit_counter(buf, chan, beginidx, offsets->begin,
+				 commit_count, ltt_sb_header_size());
 }
 
 
@@ -1295,17 +920,17 @@ static void ltt_reserve_switch_new_subbuf(
  * (uncommited) subbuffer will be declared corrupted, and that the new subbuffer
  * will be declared corrupted too because of the commit count adjustment.
  */
-static void ltt_reserve_end_switch_current(
-		struct ltt_channel_struct *ltt_channel,
-		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
-		struct rchan_buf *buf,
-		struct ltt_reserve_switch_offsets *offsets, u64 *tsc)
+static
+void ltt_reserve_end_switch_current(struct ltt_chanbuf *buf,
+				    struct ltt_chan *chan,
+				    struct ltt_reserve_switch_offsets *offsets,
+				    u64 *tsc)
 {
-	long endidx = SUBBUF_INDEX(offsets->end - 1, rchan);
+	long endidx = SUBBUF_INDEX(offsets->end - 1, chan);
 	long commit_count, padding_size;
 
-	padding_size = rchan->subbuf_size
-			- (SUBBUF_OFFSET(offsets->end - 1, rchan) + 1);
+	padding_size = chan->a.sb_size
+			- (SUBBUF_OFFSET(offsets->end - 1, chan) + 1);
 
 	ltt_buffer_end(buf, *tsc, offsets->end, endidx);
 
@@ -1315,13 +940,11 @@ static void ltt_reserve_end_switch_current(
 	 * sent by get_subbuf() when it does its smp_rmb().
 	 */
 	barrier();
-	local_add(padding_size,
-		  &ltt_buf->commit_count[endidx].cc);
-	commit_count = local_read(&ltt_buf->commit_count[endidx].cc);
-	ltt_check_deliver(ltt_channel, ltt_buf, rchan, buf,
-		offsets->end - 1, commit_count, endidx);
-	ltt_write_commit_counter(buf, ltt_buf, endidx,
-		offsets->end, commit_count, padding_size);
+	local_add(padding_size, &buf->commit_count[endidx].cc);
+	commit_count = local_read(&buf->commit_count[endidx].cc);
+	ltt_check_deliver(buf, chan, offsets->end - 1, commit_count, endidx);
+	ltt_write_commit_counter(buf, chan, endidx, offsets->end, commit_count,
+				 padding_size);
 }
 
 /*
@@ -1329,49 +952,47 @@ static void ltt_reserve_end_switch_current(
  * 0 if ok
  * !0 if execution must be aborted.
  */
-static int ltt_relay_try_switch_slow(
-		enum force_switch_mode mode,
-		struct ltt_channel_struct *ltt_channel,
-		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
-		struct rchan_buf *buf,
-		struct ltt_reserve_switch_offsets *offsets,
-		u64 *tsc)
+static
+int ltt_relay_try_switch_slow(enum force_switch_mode mode,
+			      struct ltt_chanbuf *buf, struct ltt_chan *chan,
+			      struct ltt_reserve_switch_offsets *offsets,
+			      u64 *tsc)
 {
-	long subbuf_index;
+	long sb_index;
 	long reserve_commit_diff;
 
-	offsets->begin = local_read(&ltt_buf->offset);
+	offsets->begin = local_read(&buf->offset);
 	offsets->old = offsets->begin;
 	offsets->begin_switch = 0;
 	offsets->end_switch_old = 0;
 
 	*tsc = trace_clock_read64();
 
-	if (SUBBUF_OFFSET(offsets->begin, buf->chan) != 0) {
-		offsets->begin = SUBBUF_ALIGN(offsets->begin, buf->chan);
+	if (SUBBUF_OFFSET(offsets->begin, chan) != 0) {
+		offsets->begin = SUBBUF_ALIGN(offsets->begin, chan);
 		offsets->end_switch_old = 1;
 	} else {
 		/* we do not have to switch : buffer is empty */
 		return -1;
 	}
 	if (mode == FORCE_ACTIVE)
-		offsets->begin += ltt_subbuffer_header_size();
+		offsets->begin += ltt_sb_header_size();
 	/*
 	 * Always begin_switch in FORCE_ACTIVE mode.
 	 * Test new buffer integrity
 	 */
-	subbuf_index = SUBBUF_INDEX(offsets->begin, buf->chan);
+	sb_index = SUBBUF_INDEX(offsets->begin, chan);
 	reserve_commit_diff =
-		(BUFFER_TRUNC(offsets->begin, buf->chan)
-		 >> ltt_channel->n_subbufs_order)
-		- (local_read(&ltt_buf->commit_count[subbuf_index].cc_sb)
-			& ltt_channel->commit_count_mask);
+		(BUFFER_TRUNC(offsets->begin, chan)
+		 >> chan->a.n_sb_order)
+		- (local_read(&buf->commit_count[sb_index].cc_sb)
+			& chan->commit_count_mask);
 	if (reserve_commit_diff == 0) {
 		/* Next buffer not corrupted. */
 		if (mode == FORCE_ACTIVE
-		    && !ltt_channel->overwrite
-		    && offsets->begin - atomic_long_read(&ltt_buf->consumed)
-		       >= rchan->alloc_size) {
+		    && !chan->overwrite
+		    && offsets->begin - atomic_long_read(&buf->consumed)
+		       >= chan->a.buf_size) {
 			/*
 			 * We do not overwrite non consumed buffers and we are
 			 * full : ignore switch while tracing is active.
@@ -1397,13 +1018,10 @@ static int ltt_relay_try_switch_slow(
  * operations, this function must be called from the CPU which owns the buffer
  * for a ACTIVE flush.
  */
-void ltt_force_switch_lockless_slow(struct rchan_buf *buf,
-		enum force_switch_mode mode)
+void ltt_force_switch_lockless_slow(struct ltt_chanbuf *buf,
+				    enum force_switch_mode mode)
 {
-	struct ltt_channel_struct *ltt_channel =
-			(struct ltt_channel_struct *)buf->chan->private_data;
-	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
-	struct rchan *rchan = ltt_channel->trans_channel_data;
+	struct ltt_chan *chan = container_of(buf->a.chan, struct ltt_chan, a);
 	struct ltt_reserve_switch_offsets offsets;
 	u64 tsc;
 
@@ -1413,11 +1031,10 @@ void ltt_force_switch_lockless_slow(struct rchan_buf *buf,
 	 * Perform retryable operations.
 	 */
 	do {
-		if (ltt_relay_try_switch_slow(mode, ltt_channel, ltt_buf,
-				rchan, buf, &offsets, &tsc))
+		if (ltt_relay_try_switch_slow(mode, buf, chan, &offsets, &tsc))
 			return;
-	} while (local_cmpxchg(&ltt_buf->offset, offsets.old,
-			offsets.end) != offsets.old);
+	} while (local_cmpxchg(&buf->offset, offsets.old, offsets.end)
+		 != offsets.old);
 
 	/*
 	 * Atomically update last_tsc. This update races against concurrent
@@ -1425,33 +1042,31 @@ void ltt_force_switch_lockless_slow(struct rchan_buf *buf,
 	 * events, never the opposite (missing a full TSC event when it would be
 	 * needed).
 	 */
-	save_last_tsc(ltt_buf, tsc);
+	save_last_tsc(buf, tsc);
 
 	/*
 	 * Push the reader if necessary
 	 */
 	if (mode == FORCE_ACTIVE) {
-		ltt_reserve_push_reader(ltt_buf, rchan, buf, offsets.end - 1);
-		ltt_clear_noref_flag(rchan, buf, SUBBUF_INDEX(offsets.end - 1,
-							      rchan));
+		ltt_reserve_push_reader(buf, chan, offsets.end - 1);
+		ltt_clear_noref_flag(&buf->a, SUBBUF_INDEX(offsets.end - 1,
+							   chan));
 	}
 
 	/*
 	 * Switch old subbuffer if needed.
 	 */
 	if (offsets.end_switch_old) {
-		ltt_clear_noref_flag(rchan, buf, SUBBUF_INDEX(offsets.old - 1,
-							      rchan));
-		ltt_reserve_switch_old_subbuf(ltt_channel, ltt_buf, rchan, buf,
-			&offsets, &tsc);
+		ltt_clear_noref_flag(&buf->a, SUBBUF_INDEX(offsets.old - 1,
+							   chan));
+		ltt_reserve_switch_old_subbuf(buf, chan, &offsets, &tsc);
 	}
 
 	/*
 	 * Populate new subbuffer.
 	 */
 	if (mode == FORCE_ACTIVE)
-		ltt_reserve_switch_new_subbuf(ltt_channel,
-			ltt_buf, rchan, buf, &offsets, &tsc);
+		ltt_reserve_switch_new_subbuf(buf, chan, &offsets, &tsc);
 }
 EXPORT_SYMBOL_GPL(ltt_force_switch_lockless_slow);
 
@@ -1460,69 +1075,68 @@ EXPORT_SYMBOL_GPL(ltt_force_switch_lockless_slow);
  * 0 if ok
  * !0 if execution must be aborted.
  */
-static int ltt_relay_try_reserve_slow(struct ltt_channel_struct *ltt_channel,
-		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
-		struct rchan_buf *buf,
-		struct ltt_reserve_switch_offsets *offsets, size_t data_size,
-		u64 *tsc, unsigned int *rflags, int largest_align)
+static
+int ltt_relay_try_reserve_slow(struct ltt_chanbuf *buf, struct ltt_chan *chan,
+			       struct ltt_reserve_switch_offsets *offsets,
+			       size_t data_size, u64 *tsc, unsigned int *rflags,
+			       int largest_align)
 {
 	long reserve_commit_diff;
 
-	offsets->begin = local_read(&ltt_buf->offset);
+	offsets->begin = local_read(&buf->offset);
 	offsets->old = offsets->begin;
 	offsets->begin_switch = 0;
 	offsets->end_switch_current = 0;
 	offsets->end_switch_old = 0;
 
 	*tsc = trace_clock_read64();
-	if (last_tsc_overflow(ltt_buf, *tsc))
+	if (last_tsc_overflow(buf, *tsc))
 		*rflags = LTT_RFLAG_ID_SIZE_TSC;
 
-	if (unlikely(SUBBUF_OFFSET(offsets->begin, buf->chan) == 0)) {
+	if (unlikely(SUBBUF_OFFSET(offsets->begin, chan) == 0)) {
 		offsets->begin_switch = 1;		/* For offsets->begin */
 	} else {
-		offsets->size = ltt_get_header_size(ltt_channel,
-					offsets->begin, data_size,
-					&offsets->before_hdr_pad, *rflags);
+		offsets->size = ltt_get_header_size(chan, offsets->begin,
+						    data_size,
+						    &offsets->before_hdr_pad,
+						    *rflags);
 		offsets->size += ltt_align(offsets->begin + offsets->size,
 					   largest_align)
 				 + data_size;
-		if (unlikely((SUBBUF_OFFSET(offsets->begin, buf->chan) +
-			     offsets->size) > buf->chan->subbuf_size)) {
+		if (unlikely((SUBBUF_OFFSET(offsets->begin, chan) +
+			     offsets->size) > chan->a.sb_size)) {
 			offsets->end_switch_old = 1;	/* For offsets->old */
 			offsets->begin_switch = 1;	/* For offsets->begin */
 		}
 	}
 	if (unlikely(offsets->begin_switch)) {
-		long subbuf_index;
+		long sb_index;
 
 		/*
 		 * We are typically not filling the previous buffer completely.
 		 */
 		if (likely(offsets->end_switch_old))
-			offsets->begin = SUBBUF_ALIGN(offsets->begin,
-						      buf->chan);
-		offsets->begin = offsets->begin + ltt_subbuffer_header_size();
+			offsets->begin = SUBBUF_ALIGN(offsets->begin, chan);
+		offsets->begin = offsets->begin + ltt_sb_header_size();
 		/* Test new buffer integrity */
-		subbuf_index = SUBBUF_INDEX(offsets->begin, buf->chan);
+		sb_index = SUBBUF_INDEX(offsets->begin, chan);
 		reserve_commit_diff =
-		  (BUFFER_TRUNC(offsets->begin, buf->chan)
-		   >> ltt_channel->n_subbufs_order)
-		  - (local_read(&ltt_buf->commit_count[subbuf_index].cc_sb)
-				& ltt_channel->commit_count_mask);
+		  (BUFFER_TRUNC(offsets->begin, chan)
+		   >> chan->a.n_sb_order)
+		  - (local_read(&buf->commit_count[sb_index].cc_sb)
+				& chan->commit_count_mask);
 		if (likely(reserve_commit_diff == 0)) {
 			/* Next buffer not corrupted. */
-			if (unlikely(!ltt_channel->overwrite &&
-				(SUBBUF_TRUNC(offsets->begin, buf->chan)
-				 - SUBBUF_TRUNC(atomic_long_read(
-							&ltt_buf->consumed),
-						buf->chan))
-				>= rchan->alloc_size)) {
+			if (unlikely(!chan->overwrite &&
+				(SUBBUF_TRUNC(offsets->begin, chan)
+				 - SUBBUF_TRUNC(atomic_long_read(&buf->consumed),
+						chan))
+				>= chan->a.buf_size)) {
 				/*
 				 * We do not overwrite non consumed buffers
 				 * and we are full : event is lost.
 				 */
-				local_inc(&ltt_buf->events_lost);
+				local_inc(&buf->events_lost);
 				return -1;
 			} else {
 				/*
@@ -1537,22 +1151,23 @@ static int ltt_relay_try_reserve_slow(struct ltt_channel_struct *ltt_channel,
 			 * overwrite mode. Caused by either a writer OOPS or
 			 * too many nested writes over a reserve/commit pair.
 			 */
-			local_inc(&ltt_buf->events_lost);
+			local_inc(&buf->events_lost);
 			return -1;
 		}
-		offsets->size = ltt_get_header_size(ltt_channel,
-					offsets->begin, data_size,
-					&offsets->before_hdr_pad, *rflags);
+		offsets->size = ltt_get_header_size(chan, offsets->begin,
+						    data_size,
+						    &offsets->before_hdr_pad,
+						    *rflags);
 		offsets->size += ltt_align(offsets->begin + offsets->size,
 					   largest_align)
 				 + data_size;
-		if (unlikely((SUBBUF_OFFSET(offsets->begin, buf->chan)
-			     + offsets->size) > buf->chan->subbuf_size)) {
+		if (unlikely((SUBBUF_OFFSET(offsets->begin, chan)
+			     + offsets->size) > chan->a.sb_size)) {
 			/*
 			 * Event too big for subbuffers, report error, don't
 			 * complete the sub-buffer switch.
 			 */
-			local_inc(&ltt_buf->events_lost);
+			local_inc(&buf->events_lost);
 			return -1;
 		} else {
 			/*
@@ -1568,7 +1183,7 @@ static int ltt_relay_try_reserve_slow(struct ltt_channel_struct *ltt_channel,
 	}
 	offsets->end = offsets->begin + offsets->size;
 
-	if (unlikely((SUBBUF_OFFSET(offsets->end, buf->chan)) == 0)) {
+	if (unlikely((SUBBUF_OFFSET(offsets->end, chan)) == 0)) {
 		/*
 		 * The offset_end will fall at the very beginning of the next
 		 * subbuffer.
@@ -1592,25 +1207,25 @@ static int ltt_relay_try_reserve_slow(struct ltt_channel_struct *ltt_channel,
  * Return : -ENOSPC if not enough space, else returns 0.
  * It will take care of sub-buffer switching.
  */
-int ltt_reserve_slot_lockless_slow(struct ltt_trace_struct *trace,
-		struct ltt_channel_struct *ltt_channel, void **transport_data,
-		size_t data_size, size_t *slot_size, long *buf_offset, u64 *tsc,
-		unsigned int *rflags, int largest_align, int cpu)
+int ltt_reserve_slot_lockless_slow(struct ltt_chan *chan,
+				   struct ltt_trace *trace, size_t data_size,
+				   int largest_align, int cpu,
+				   struct ltt_chanbuf **ret_buf,
+				   size_t *slot_size, long *buf_offset,
+				   u64 *tsc, unsigned int *rflags)
 {
-	struct rchan *rchan = ltt_channel->trans_channel_data;
-	struct rchan_buf *buf = *transport_data = rchan->buf[cpu];
-	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+	struct ltt_chanbuf *buf = *ret_buf = per_cpu_ptr(chan->a.buf, cpu);
 	struct ltt_reserve_switch_offsets offsets;
 
 	offsets.size = 0;
 
 	do {
-		if (unlikely(ltt_relay_try_reserve_slow(ltt_channel, ltt_buf,
-				rchan, buf, &offsets, data_size, tsc, rflags,
-				largest_align)))
+		if (unlikely(ltt_relay_try_reserve_slow(buf, chan, &offsets,
+							data_size, tsc, rflags,
+							largest_align)))
 			return -ENOSPC;
-	} while (unlikely(local_cmpxchg(&ltt_buf->offset, offsets.old,
-			offsets.end) != offsets.old));
+	} while (unlikely(local_cmpxchg(&buf->offset, offsets.old, offsets.end)
+			  != offsets.old));
 
 	/*
 	 * Atomically update last_tsc. This update races against concurrent
@@ -1618,38 +1233,35 @@ int ltt_reserve_slot_lockless_slow(struct ltt_trace_struct *trace,
 	 * events, never the opposite (missing a full TSC event when it would be
 	 * needed).
 	 */
-	save_last_tsc(ltt_buf, *tsc);
+	save_last_tsc(buf, *tsc);
 
 	/*
 	 * Push the reader if necessary
 	 */
-	ltt_reserve_push_reader(ltt_buf, rchan, buf, offsets.end - 1);
+	ltt_reserve_push_reader(buf, chan, offsets.end - 1);
 
 	/*
 	 * Clear noref flag for this subbuffer.
 	 */
-	ltt_clear_noref_flag(rchan, buf, SUBBUF_INDEX(offsets.end - 1, rchan));
+	ltt_clear_noref_flag(&buf->a, SUBBUF_INDEX(offsets.end - 1, chan));
 
 	/*
 	 * Switch old subbuffer if needed.
 	 */
 	if (unlikely(offsets.end_switch_old)) {
-		ltt_clear_noref_flag(rchan, buf, SUBBUF_INDEX(offsets.old - 1,
-							      rchan));
-		ltt_reserve_switch_old_subbuf(ltt_channel, ltt_buf, rchan, buf,
-			&offsets, tsc);
+		ltt_clear_noref_flag(&buf->a, SUBBUF_INDEX(offsets.old - 1,
+							  chan));
+		ltt_reserve_switch_old_subbuf(buf, chan, &offsets, tsc);
 	}
 
 	/*
 	 * Populate new subbuffer.
 	 */
 	if (unlikely(offsets.begin_switch))
-		ltt_reserve_switch_new_subbuf(ltt_channel, ltt_buf, rchan,
-			buf, &offsets, tsc);
+		ltt_reserve_switch_new_subbuf(buf, chan, &offsets, tsc);
 
 	if (unlikely(offsets.end_switch_current))
-		ltt_reserve_end_switch_current(ltt_channel, ltt_buf, rchan,
-			buf, &offsets, tsc);
+		ltt_reserve_end_switch_current(buf, chan, &offsets, tsc);
 
 	*slot_size = offsets.size;
 	*buf_offset = offsets.begin + offsets.before_hdr_pad;
@@ -1663,24 +1275,20 @@ static struct ltt_transport ltt_relay_transport = {
 	.ops = {
 		.create_dirs = ltt_relay_create_dirs,
 		.remove_dirs = ltt_relay_remove_dirs,
-		.create_channel = ltt_relay_create_channel,
+		.create_channel = ltt_chan_create,
 		.finish_channel = ltt_relay_finish_channel,
-		.remove_channel = ltt_relay_remove_channel,
+		.remove_channel = ltt_chan_free,
 		.wakeup_channel = ltt_relay_async_wakeup_chan,
 		.user_blocking = ltt_relay_user_blocking,
 		.user_errors = ltt_relay_print_user_errors,
+		.start_switch_timer = ltt_chan_start_switch_timer,
+		.stop_switch_timer = ltt_chan_stop_switch_timer,
 	},
 };
 
-static const struct file_operations ltt_file_operations = {
-	.open = ltt_open,
-	.release = ltt_release,
-	.poll = ltt_poll,
-	.splice_read = ltt_relay_file_splice_read,
-	.ioctl = ltt_ioctl,
-#ifdef CONFIG_COMPAT
-	.compat_ioctl = ltt_compat_ioctl,
-#endif
+static struct notifier_block fn_ltt_chanbuf_hotcpu_callback = {
+	.notifier_call = ltt_chanbuf_hotcpu_callback,
+	.priority = 6,
 };
 
 static int __init ltt_relay_init(void)
@@ -1688,6 +1296,7 @@ static int __init ltt_relay_init(void)
 	printk(KERN_INFO "LTT : ltt-relay init\n");
 
 	ltt_transport_register(&ltt_relay_transport);
+	register_cpu_notifier(&fn_ltt_chanbuf_hotcpu_callback);
 
 	return 0;
 }
@@ -1696,6 +1305,7 @@ static void __exit ltt_relay_exit(void)
 {
 	printk(KERN_INFO "LTT : ltt-relay exit\n");
 
+	unregister_cpu_notifier(&fn_ltt_chanbuf_hotcpu_callback);
 	ltt_transport_unregister(&ltt_relay_transport);
 }
 
diff --git a/ltt/ltt-relay-lockless.h b/ltt/ltt-relay-lockless.h
index 45d1aac..9412d26 100644
--- a/ltt/ltt-relay-lockless.h
+++ b/ltt/ltt-relay-lockless.h
@@ -73,7 +73,8 @@ struct commit_counters {
 };
 
 /* LTTng lockless logging buffer info */
-struct ltt_channel_buf_struct {
+struct ltt_chanbuf {
+	struct ltt_chanbuf_alloc a;	/* Parent. First field. */
 	/* First 32 bytes cache-hot cacheline */
 	local_t offset;			/* Current offset in the buffer */
 	struct commit_counters *commit_count;
@@ -107,9 +108,7 @@ struct ltt_channel_buf_struct {
 	wait_queue_head_t read_wait;	/* reader wait queue */
 	unsigned int finalized;		/* buffer has been finalized */
 	struct timer_list switch_timer;	/* timer for periodical switch */
-	unsigned long switch_timer_interval;	/* in jiffies. 0 unset */
-	struct rchan_buf *rbuf;		/* Pointer to rchan_buf */
-} ____cacheline_internodealigned_in_smp;
+};
 
 /*
  * A switch is done during tracing or as a final flush after tracing (so it
@@ -117,13 +116,16 @@ struct ltt_channel_buf_struct {
  */
 enum force_switch_mode { FORCE_ACTIVE, FORCE_FLUSH };
 
-extern int ltt_reserve_slot_lockless_slow(struct ltt_trace_struct *trace,
-		struct ltt_channel_struct *ltt_channel, void **transport_data,
-		size_t data_size, size_t *slot_size, long *buf_offset, u64 *tsc,
-		unsigned int *rflags, int largest_align, int cpu);
+extern
+int ltt_reserve_slot_lockless_slow(struct ltt_chan *chan,
+				   struct ltt_trace *trace, size_t data_size,
+				   int largest_align, int cpu,
+				   struct ltt_chanbuf **ret_buf,
+				   size_t *slot_size, long *buf_offset,
+				   u64 *tsc, unsigned int *rflags);
 
-extern void ltt_force_switch_lockless_slow(struct rchan_buf *buf,
-		enum force_switch_mode mode);
+extern void ltt_force_switch_lockless_slow(struct ltt_chanbuf *buf,
+					   enum force_switch_mode mode);
 
 /*
  * Last TSC comparison functions. Check if the current TSC overflows
@@ -132,49 +134,81 @@ extern void ltt_force_switch_lockless_slow(struct rchan_buf *buf,
  */
 
 #if (BITS_PER_LONG == 32)
-static __inline__ void save_last_tsc(struct ltt_channel_buf_struct *ltt_buf,
-					u64 tsc)
+static __inline__ void save_last_tsc(struct ltt_chanbuf *buf, u64 tsc)
 {
-	ltt_buf->last_tsc = (unsigned long)(tsc >> LTT_TSC_BITS);
+	buf->last_tsc = (unsigned long)(tsc >> LTT_TSC_BITS);
 }
 
-static __inline__ int last_tsc_overflow(struct ltt_channel_buf_struct *ltt_buf,
-					u64 tsc)
+static __inline__ int last_tsc_overflow(struct ltt_chanbuf *buf, u64 tsc)
 {
 	unsigned long tsc_shifted = (unsigned long)(tsc >> LTT_TSC_BITS);
 
-	if (unlikely((tsc_shifted - ltt_buf->last_tsc)))
+	if (unlikely((tsc_shifted - buf->last_tsc)))
 		return 1;
 	else
 		return 0;
 }
 #else
-static __inline__ void save_last_tsc(struct ltt_channel_buf_struct *ltt_buf,
-					u64 tsc)
+static __inline__ void save_last_tsc(struct ltt_chanbuf *buf, u64 tsc)
 {
-	ltt_buf->last_tsc = (unsigned long)tsc;
+	buf->last_tsc = (unsigned long)tsc;
 }
 
-static __inline__ int last_tsc_overflow(struct ltt_channel_buf_struct *ltt_buf,
-					u64 tsc)
+static __inline__ int last_tsc_overflow(struct ltt_chanbuf *buf, u64 tsc)
 {
-	if (unlikely((tsc - ltt_buf->last_tsc) >> LTT_TSC_BITS))
+	if (unlikely((tsc - buf->last_tsc) >> LTT_TSC_BITS))
 		return 1;
 	else
 		return 0;
 }
 #endif
 
-static __inline__ void ltt_reserve_push_reader(
-		struct ltt_channel_buf_struct *ltt_buf,
-		struct rchan *rchan,
-		struct rchan_buf *buf,
-		long offset)
+extern
+int ltt_chanbuf_create(struct ltt_chanbuf *buf, struct ltt_chan_alloc *chana,
+		       int cpu);
+extern void ltt_chanbuf_free(struct kref *kref);
+extern int ltt_chan_create(const char *base_filename, struct ltt_chan *chan,
+			   struct dentry *parent, size_t sb_size, size_t n_sb,
+			   int overwrite, struct ltt_trace *trace);
+extern void ltt_chan_free(struct kref *kref);
+
+/* Buffer access operations */
+
+extern int ltt_chanbuf_open_read(struct ltt_chanbuf *buf);
+extern void ltt_chanbuf_release_read(struct ltt_chanbuf *buf);
+extern int ltt_chanbuf_get_subbuf(struct ltt_chanbuf *buf,
+				  unsigned long *consumed);
+extern int ltt_chanbuf_put_subbuf(struct ltt_chanbuf *buf,
+				  unsigned long consumed);
+extern void ltt_chan_start_switch_timer(struct ltt_chan *chan);
+extern void ltt_chan_stop_switch_timer(struct ltt_chan *chan);
+
+static __inline__
+unsigned long ltt_chanbuf_get_offset(struct ltt_chanbuf *buf)
+{
+	return local_read(&buf->offset);
+}
+
+static __inline__
+unsigned long ltt_chanbuf_get_consumed(struct ltt_chanbuf *buf)
+{
+	return atomic_long_read(&buf->consumed);
+}
+
+static __inline__
+int ltt_chanbuf_is_finalized(struct ltt_chanbuf *buf)
+{
+	return buf->finalized;
+}
+
+static __inline__
+void ltt_reserve_push_reader(struct ltt_chanbuf *buf, struct ltt_chan *chan,
+			     long offset)
 {
 	long consumed_old, consumed_new;
 
 	do {
-		consumed_old = atomic_long_read(&ltt_buf->consumed);
+		consumed_old = atomic_long_read(&buf->consumed);
 		/*
 		 * If buffer is in overwrite mode, push the reader consumed
 		 * count if the write position has reached it and we are not
@@ -186,90 +220,84 @@ static __inline__ void ltt_reserve_push_reader(
 		 * If the buffer is not in overwrite mode, pushing the reader
 		 * only happens if a sub-buffer is corrupted.
 		 */
-		if (unlikely((SUBBUF_TRUNC(offset, buf->chan)
-		   - SUBBUF_TRUNC(consumed_old, buf->chan))
-		   >= rchan->alloc_size))
-			consumed_new = SUBBUF_ALIGN(consumed_old, buf->chan);
+		if (unlikely((SUBBUF_TRUNC(offset, chan)
+			      - SUBBUF_TRUNC(consumed_old, chan))
+			     >= chan->a.buf_size))
+			consumed_new = SUBBUF_ALIGN(consumed_old, chan);
 		else
 			return;
-	} while (unlikely(atomic_long_cmpxchg(&ltt_buf->consumed, consumed_old,
-			consumed_new) != consumed_old));
+	} while (unlikely(atomic_long_cmpxchg(&buf->consumed, consumed_old,
+					      consumed_new) != consumed_old));
 }
 
 #ifdef CONFIG_LTT_VMCORE
-static __inline__ void ltt_vmcore_check_deliver(
-		struct ltt_channel_buf_struct *ltt_buf,
-		long commit_count, long idx)
+static __inline__
+void ltt_vmcore_check_deliver(struct ltt_chanbuf *buf, long commit_count,
+			      long idx)
 {
-	local_set(&ltt_buf->commit_seq[idx], commit_count);
+	local_set(&buf->commit_seq[idx], commit_count);
 }
 #else
-static __inline__ void ltt_vmcore_check_deliver(
-		struct ltt_channel_buf_struct *ltt_buf,
-		long commit_count, long idx)
+static __inline__
+void ltt_vmcore_check_deliver(struct ltt_chanbuf *buf, long commit_count,
+			      long idx)
 {
 }
 #endif
 
-static __inline__ void ltt_check_deliver(struct ltt_channel_struct *ltt_channel,
-		struct ltt_channel_buf_struct *ltt_buf,
-		struct rchan *rchan,
-		struct rchan_buf *buf,
-		long offset, long commit_count, long idx)
+static __inline__
+void ltt_check_deliver(struct ltt_chanbuf *buf, struct ltt_chan *chan,
+		       long offset, long commit_count, long idx)
 {
-	long old_commit_count = commit_count - rchan->subbuf_size;
+	long old_commit_count = commit_count - chan->a.sb_size;
 
 	/* Check if all commits have been done */
-	if (unlikely((BUFFER_TRUNC(offset, rchan)
-			>> ltt_channel->n_subbufs_order)
-			- (old_commit_count
-			   & ltt_channel->commit_count_mask) == 0)) {
+	if (unlikely((BUFFER_TRUNC(offset, chan) >> chan->a.n_sb_order)
+		     - (old_commit_count & chan->commit_count_mask) == 0)) {
 		/*
 		 * If we succeeded in updating the cc_sb, we are delivering
 		 * the subbuffer. Deals with concurrent updates of the "cc"
 		 * value without adding a add_return atomic operation to the
 		 * fast path.
 		 */
-		if (likely(local_cmpxchg(&ltt_buf->commit_count[idx].cc_sb,
+		if (likely(local_cmpxchg(&buf->commit_count[idx].cc_sb,
 					 old_commit_count, commit_count)
 			   == old_commit_count)) {
 			/*
 			 * Set noref flag for this subbuffer.
 			 */
-			ltt_set_noref_flag(rchan, buf, idx);
-			ltt_vmcore_check_deliver(ltt_buf, commit_count, idx);
+			ltt_set_noref_flag(&buf->a, idx);
+			ltt_vmcore_check_deliver(buf, commit_count, idx);
 		}
 	}
 }
 
 
-static __inline__ int ltt_poll_deliver(struct ltt_channel_struct *ltt_channel,
-		struct ltt_channel_buf_struct *ltt_buf,
-		struct rchan *rchan,
-		struct rchan_buf *buf)
+static __inline__
+int ltt_poll_deliver(struct ltt_chanbuf *buf, struct ltt_chan *chan)
 {
 	long consumed_old, consumed_idx, commit_count, write_offset;
 
-	consumed_old = atomic_long_read(&ltt_buf->consumed);
-	consumed_idx = SUBBUF_INDEX(consumed_old, buf->chan);
-	commit_count = local_read(&ltt_buf->commit_count[consumed_idx].cc_sb);
+	consumed_old = atomic_long_read(&buf->consumed);
+	consumed_idx = SUBBUF_INDEX(consumed_old, chan);
+	commit_count = local_read(&buf->commit_count[consumed_idx].cc_sb);
 	/*
 	 * No memory barrier here, since we are only interested
 	 * in a statistically correct polling result. The next poll will
 	 * get the data is we are racing. The mb() that ensures correct
 	 * memory order is in get_subbuf.
 	 */
-	write_offset = local_read(&ltt_buf->offset);
+	write_offset = local_read(&buf->offset);
 
 	/*
 	 * Check that the subbuffer we are trying to consume has been
 	 * already fully committed.
 	 */
 
-	if (((commit_count - rchan->subbuf_size)
-	     & ltt_channel->commit_count_mask)
-	    - (BUFFER_TRUNC(consumed_old, buf->chan)
-	       >> ltt_channel->n_subbufs_order)
+	if (((commit_count - chan->a.sb_size)
+	     & chan->commit_count_mask)
+	    - (BUFFER_TRUNC(consumed_old, chan)
+	       >> chan->a.n_sb_order)
 	    != 0)
 		return 0;
 
@@ -277,8 +305,8 @@ static __inline__ int ltt_poll_deliver(struct ltt_channel_struct *ltt_channel,
 	 * Check that we are not about to read the same subbuffer in
 	 * which the writer head is.
 	 */
-	if ((SUBBUF_TRUNC(write_offset, buf->chan)
-	   - SUBBUF_TRUNC(consumed_old, buf->chan))
+	if ((SUBBUF_TRUNC(write_offset, chan)
+	   - SUBBUF_TRUNC(consumed_old, chan))
 	   == 0)
 		return 0;
 
@@ -289,38 +317,33 @@ static __inline__ int ltt_poll_deliver(struct ltt_channel_struct *ltt_channel,
 /*
  * returns 0 if reserve ok, or 1 if the slow path must be taken.
  */
-static __inline__ int ltt_relay_try_reserve(
-		struct ltt_channel_struct *ltt_channel,
-		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
-		struct rchan_buf *buf,
-		size_t data_size,
-		u64 *tsc, unsigned int *rflags, int largest_align,
-		long *o_begin, long *o_end, long *o_old,
-		size_t *before_hdr_pad, size_t *size)
+static __inline__
+int ltt_relay_try_reserve(struct ltt_chanbuf *buf, struct ltt_chan *chan,
+			  size_t data_size, u64 *tsc, unsigned int *rflags,
+			  int largest_align, long *o_begin, long *o_end,
+			  long *o_old, size_t *before_hdr_pad, size_t *size)
 {
-	*o_begin = local_read(&ltt_buf->offset);
+	*o_begin = local_read(&buf->offset);
 	*o_old = *o_begin;
 
 	*tsc = trace_clock_read64();
 
 #ifdef CONFIG_LTT_VMCORE
-	prefetch(&ltt_buf->commit_count[SUBBUF_INDEX(*o_begin, rchan)]);
-	prefetch(&ltt_buf->commit_seq[SUBBUF_INDEX(*o_begin, rchan)]);
+	prefetch(&buf->commit_count[SUBBUF_INDEX(*o_begin, chan)]);
+	prefetch(&buf->commit_seq[SUBBUF_INDEX(*o_begin, chan)]);
 #else
-	prefetchw(&ltt_buf->commit_count[SUBBUF_INDEX(*o_begin, rchan)]);
+	prefetchw(&buf->commit_count[SUBBUF_INDEX(*o_begin, chan)]);
 #endif
-	if (last_tsc_overflow(ltt_buf, *tsc))
+	if (last_tsc_overflow(buf, *tsc))
 		*rflags = LTT_RFLAG_ID_SIZE_TSC;
 
-	if (unlikely(SUBBUF_OFFSET(*o_begin, buf->chan) == 0))
+	if (unlikely(SUBBUF_OFFSET(*o_begin, chan) == 0))
 		return 1;
 
-	*size = ltt_get_header_size(ltt_channel,
-				*o_begin, data_size,
-				before_hdr_pad, *rflags);
+	*size = ltt_get_header_size(chan, *o_begin, data_size, before_hdr_pad,
+				    *rflags);
 	*size += ltt_align(*o_begin + *size, largest_align) + data_size;
-	if (unlikely((SUBBUF_OFFSET(*o_begin, buf->chan) + *size)
-		     > buf->chan->subbuf_size))
+	if (unlikely((SUBBUF_OFFSET(*o_begin, chan) + *size) > chan->a.sb_size))
 		return 1;
 
 	/*
@@ -329,7 +352,7 @@ static __inline__ int ltt_relay_try_reserve(
 	 */
 	*o_end = *o_begin + *size;
 
-	if (unlikely((SUBBUF_OFFSET(*o_end, buf->chan)) == 0))
+	if (unlikely((SUBBUF_OFFSET(*o_end, chan)) == 0))
 		/*
 		 * The offset_end will fall at the very beginning of the next
 		 * subbuffer.
@@ -339,14 +362,15 @@ static __inline__ int ltt_relay_try_reserve(
 	return 0;
 }
 
-static __inline__ int ltt_reserve_slot(struct ltt_trace_struct *trace,
-		struct ltt_channel_struct *ltt_channel, void **transport_data,
-		size_t data_size, size_t *slot_size, long *buf_offset, u64 *tsc,
-		unsigned int *rflags, int largest_align, int cpu)
+static __inline__
+int ltt_reserve_slot(struct ltt_chan *chan,
+		     struct ltt_trace *trace, size_t data_size,
+		     int largest_align, int cpu,
+		     struct ltt_chanbuf **ret_buf,
+		     size_t *slot_size, long *buf_offset, u64 *tsc,
+		     unsigned int *rflags)
 {
-	struct rchan *rchan = ltt_channel->trans_channel_data;
-	struct rchan_buf *buf = *transport_data = rchan->buf[cpu];
-	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+	struct ltt_chanbuf *buf = *ret_buf = per_cpu_ptr(chan->a.buf, cpu);
 	long o_begin, o_end, o_old;
 	size_t before_hdr_pad;
 
@@ -354,17 +378,16 @@ static __inline__ int ltt_reserve_slot(struct ltt_trace_struct *trace,
 	 * Perform retryable operations.
 	 */
 	if (unlikely(__get_cpu_var(ltt_nesting) > 4)) {
-		local_inc(&ltt_buf->events_lost);
+		local_inc(&buf->events_lost);
 		return -EPERM;
 	}
 
-	if (unlikely(ltt_relay_try_reserve(ltt_channel, ltt_buf,
-			rchan, buf, data_size, tsc, rflags,
-			largest_align, &o_begin, &o_end, &o_old,
-			&before_hdr_pad, slot_size)))
+	if (unlikely(ltt_relay_try_reserve(buf, chan, data_size, tsc, rflags,
+					   largest_align, &o_begin, &o_end,
+					   &o_old, &before_hdr_pad, slot_size)))
 		goto slow_path;
 
-	if (unlikely(local_cmpxchg(&ltt_buf->offset, o_old, o_end) != o_old))
+	if (unlikely(local_cmpxchg(&buf->offset, o_old, o_end) != o_old))
 		goto slow_path;
 
 	/*
@@ -373,24 +396,25 @@ static __inline__ int ltt_reserve_slot(struct ltt_trace_struct *trace,
 	 * events, never the opposite (missing a full TSC event when it would be
 	 * needed).
 	 */
-	save_last_tsc(ltt_buf, *tsc);
+	save_last_tsc(buf, *tsc);
 
 	/*
 	 * Push the reader if necessary
 	 */
-	ltt_reserve_push_reader(ltt_buf, rchan, buf, o_end - 1);
+	ltt_reserve_push_reader(buf, chan, o_end - 1);
 
 	/*
 	 * Clear noref flag for this subbuffer.
 	 */
-	ltt_clear_noref_flag(rchan, buf, SUBBUF_INDEX(o_end - 1, rchan));
+	ltt_clear_noref_flag(&buf->a, SUBBUF_INDEX(o_end - 1, chan));
 
 	*buf_offset = o_begin + before_hdr_pad;
 	return 0;
 slow_path:
-	return ltt_reserve_slot_lockless_slow(trace, ltt_channel,
-		transport_data, data_size, slot_size, buf_offset, tsc,
-		rflags, largest_align, cpu);
+	return ltt_reserve_slot_lockless_slow(chan, trace, data_size,
+					      largest_align, cpu, ret_buf,
+					      slot_size, buf_offset, tsc,
+					      rflags);
 }
 
 /*
@@ -402,8 +426,8 @@ slow_path:
  * operations, this function must be called from the CPU which owns the buffer
  * for a ACTIVE flush.
  */
-static __inline__ void ltt_force_switch(struct rchan_buf *buf,
-		enum force_switch_mode mode)
+static __inline__
+void ltt_force_switch(struct ltt_chanbuf *buf, enum force_switch_mode mode)
 {
 	return ltt_force_switch_lockless_slow(buf, mode);
 }
@@ -415,9 +439,10 @@ static __inline__ void ltt_force_switch(struct rchan_buf *buf,
  * crash dump.
  */
 #ifdef CONFIG_LTT_VMCORE
-static __inline__ void ltt_write_commit_counter(struct rchan_buf *buf,
-		struct ltt_channel_buf_struct *ltt_buf,
-		long idx, long buf_offset, long commit_count, size_t data_size)
+static __inline__
+void ltt_write_commit_counter(struct ltt_chanbuf *buf, struct ltt_chan *chan,
+			      long idx, long buf_offset, long commit_count,
+			      size_t data_size)
 {
 	long offset;
 	long commit_seq_old;
@@ -430,18 +455,19 @@ static __inline__ void ltt_write_commit_counter(struct rchan_buf *buf,
 	 * buffer full/empty mismatch because offset is never zero here
 	 * (subbuffer header and event headers have non-zero length).
 	 */
-	if (unlikely(SUBBUF_OFFSET(offset - commit_count, buf->chan)))
+	if (unlikely(SUBBUF_OFFSET(offset - commit_count, chan)))
 		return;
 
-	commit_seq_old = local_read(&ltt_buf->commit_seq[idx]);
+	commit_seq_old = local_read(&buf->commit_seq[idx]);
 	while (commit_seq_old < commit_count)
-		commit_seq_old = local_cmpxchg(&ltt_buf->commit_seq[idx],
+		commit_seq_old = local_cmpxchg(&buf->commit_seq[idx],
 					 commit_seq_old, commit_count);
 }
 #else
-static __inline__ void ltt_write_commit_counter(struct rchan_buf *buf,
-		struct ltt_channel_buf_struct *ltt_buf,
-		long idx, long buf_offset, long commit_count, size_t data_size)
+static __inline__
+void ltt_write_commit_counter(struct ltt_chanbuf *buf, struct ltt_chan *chan,
+			      long idx, long buf_offset, long commit_count,
+			      size_t data_size)
 {
 }
 #endif
@@ -452,22 +478,18 @@ static __inline__ void ltt_write_commit_counter(struct rchan_buf *buf,
  *
  * Parameters:
  *
- * @ltt_channel : channel structure
- * @transport_data: transport-specific data
+ * @buf: buffer.
+ * @chan: channel.
  * @buf_offset : offset following the event header.
  * @data_size : size of the event data.
  * @slot_size : size of the reserved slot.
  */
-static __inline__ void ltt_commit_slot(
-		struct ltt_channel_struct *ltt_channel,
-		void **transport_data, long buf_offset,
-		size_t data_size, size_t slot_size)
+static __inline__
+void ltt_commit_slot(struct ltt_chanbuf *buf, struct ltt_chan *chan,
+		     long buf_offset, size_t data_size, size_t slot_size)
 {
-	struct rchan_buf *buf = *transport_data;
-	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
-	struct rchan *rchan = buf->chan;
 	long offset_end = buf_offset;
-	long endidx = SUBBUF_INDEX(offset_end - 1, rchan);
+	long endidx = SUBBUF_INDEX(offset_end - 1, chan);
 	long commit_count;
 
 #ifdef LTT_NO_IPI_BARRIER
@@ -480,8 +502,8 @@ static __inline__ void ltt_commit_slot(
 	 */
 	barrier();
 #endif
-	local_add(slot_size, &ltt_buf->commit_count[endidx].cc);
-	local_inc(&ltt_buf->commit_count[endidx].events);
+	local_add(slot_size, &buf->commit_count[endidx].cc);
+	local_inc(&buf->commit_count[endidx].events);
 	/*
 	 * commit count read can race with concurrent OOO commit count updates.
 	 * This is only needed for ltt_check_deliver (for non-polling delivery
@@ -499,16 +521,15 @@ static __inline__ void ltt_commit_slot(
 	 *   reserve offset for a specific sub-buffer, which is completely
 	 *   independent of the order.
 	 */
-	commit_count = local_read(&ltt_buf->commit_count[endidx].cc);
+	commit_count = local_read(&buf->commit_count[endidx].cc);
 
-	ltt_check_deliver(ltt_channel, ltt_buf, rchan, buf,
-		offset_end - 1, commit_count, endidx);
+	ltt_check_deliver(buf, chan, offset_end - 1, commit_count, endidx);
 	/*
 	 * Update lost_size for each commit. It's needed only for extracting
 	 * ltt buffers from vmcore, after crash.
 	 */
-	ltt_write_commit_counter(buf, ltt_buf, endidx,
-				 buf_offset, commit_count, data_size);
+	ltt_write_commit_counter(buf, chan, endidx, buf_offset,
+				 commit_count, data_size);
 }
 
 #endif //_LTT_LTT_RELAY_LOCKLESS_H
diff --git a/ltt/ltt-relay-splice.c b/ltt/ltt-relay-splice.c
new file mode 100644
index 0000000..115da7e
--- /dev/null
+++ b/ltt/ltt-relay-splice.c
@@ -0,0 +1,154 @@
+/*
+ * Copyright (C) 2002-2005 - Tom Zanussi (zanussi@us.ibm.com), IBM Corp
+ * Copyright (C) 1999-2005 - Karim Yaghmour (karim@opersys.com)
+ * Copyright (C) 2008-2009 - Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * Re-using content from kernel/relay.c
+ *
+ * This file is released under the GPL.
+ */
+
+#include <linux/errno.h>
+#include <linux/stddef.h>
+#include <linux/slab.h>
+#include <linux/module.h>
+#include <linux/string.h>
+#include <linux/ltt-relay.h>
+#include <linux/vmalloc.h>
+#include <linux/mm.h>
+#include <linux/cpu.h>
+#include <linux/splice.h>
+#include <linux/pipe_fs_i.h>
+#include <linux/bitops.h>
+
+#include "ltt-relay-select.h"
+
+static void ltt_relay_pipe_buf_release(struct pipe_inode_info *pipe,
+				       struct pipe_buffer *pbuf)
+{
+}
+
+static struct pipe_buf_operations ltt_relay_pipe_buf_ops = {
+	.can_merge = 0,
+	.map = generic_pipe_buf_map,
+	.unmap = generic_pipe_buf_unmap,
+	.confirm = generic_pipe_buf_confirm,
+	.release = ltt_relay_pipe_buf_release,
+	.steal = generic_pipe_buf_steal,
+	.get = generic_pipe_buf_get,
+};
+
+static void ltt_relay_page_release(struct splice_pipe_desc *spd, unsigned int i)
+{
+}
+
+/*
+ *	subbuf_splice_actor - splice up to one subbuf's worth of data
+ */
+static int subbuf_splice_actor(struct file *in,
+			       loff_t *ppos,
+			       struct pipe_inode_info *pipe,
+			       size_t len,
+			       unsigned int flags)
+{
+	struct ltt_chanbuf *buf = in->private_data;
+	struct ltt_chan *chan = container_of(buf->a.chan, struct ltt_chan, a);
+	unsigned int poff, subbuf_pages, nr_pages;
+	struct page *pages[PIPE_BUFFERS];
+	struct partial_page partial[PIPE_BUFFERS];
+	struct splice_pipe_desc spd = {
+		.pages = pages,
+		.nr_pages = 0,
+		.partial = partial,
+		.flags = flags,
+		.ops = &ltt_relay_pipe_buf_ops,
+		.spd_release = ltt_relay_page_release,
+	};
+	long consumed_old, consumed_idx, roffset;
+	unsigned long bytes_avail;
+
+	/*
+	 * Check that a GET_SUBBUF ioctl has been done before.
+	 */
+	WARN_ON(atomic_long_read(&buf->active_readers) != 1);
+	consumed_old = atomic_long_read(&buf->consumed);
+	consumed_old += *ppos;
+	consumed_idx = SUBBUF_INDEX(consumed_old, chan);
+
+	/*
+	 * Adjust read len, if longer than what is available.
+	 * Max read size is 1 subbuffer due to get_subbuf/put_subbuf for
+	 * protection.
+	 */
+	bytes_avail = chan->a.sb_size;
+	WARN_ON(bytes_avail > chan->a.buf_size);
+	len = min_t(size_t, len, bytes_avail);
+	subbuf_pages = bytes_avail >> PAGE_SHIFT;
+	nr_pages = min_t(unsigned int, subbuf_pages, PIPE_BUFFERS);
+	roffset = consumed_old & PAGE_MASK;
+	poff = consumed_old & ~PAGE_MASK;
+	printk_dbg(KERN_DEBUG "SPLICE actor len %zu pos %zd write_pos %ld\n",
+		   len, (ssize_t)*ppos, local_read(&buf->offset));
+
+	for (; spd.nr_pages < nr_pages; spd.nr_pages++) {
+		unsigned int this_len;
+		struct page *page;
+
+		if (!len)
+			break;
+		printk_dbg(KERN_DEBUG "SPLICE actor loop len %zu roffset %ld\n",
+			   len, roffset);
+
+		this_len = PAGE_SIZE - poff;
+		page = ltt_relay_read_get_page(&buf->a, roffset);
+		spd.pages[spd.nr_pages] = page;
+		spd.partial[spd.nr_pages].offset = poff;
+		spd.partial[spd.nr_pages].len = this_len;
+
+		poff = 0;
+		roffset += PAGE_SIZE;
+		len -= this_len;
+	}
+
+	if (!spd.nr_pages)
+		return 0;
+
+	return splice_to_pipe(pipe, &spd);
+}
+
+ssize_t ltt_relay_file_splice_read(struct file *in, loff_t *ppos,
+				   struct pipe_inode_info *pipe, size_t len,
+				   unsigned int flags)
+{
+	ssize_t spliced;
+	int ret;
+
+	ret = 0;
+	spliced = 0;
+
+	printk_dbg(KERN_DEBUG "SPLICE read len %zu pos %zd\n", len,
+		   (ssize_t)*ppos);
+	while (len && !spliced) {
+		ret = subbuf_splice_actor(in, ppos, pipe, len, flags);
+		printk_dbg(KERN_DEBUG "SPLICE read loop ret %d\n", ret);
+		if (ret < 0)
+			break;
+		else if (!ret) {
+			if (flags & SPLICE_F_NONBLOCK)
+				ret = -EAGAIN;
+			break;
+		}
+
+		*ppos += ret;
+		if (ret > len)
+			len = 0;
+		else
+			len -= ret;
+		spliced += ret;
+	}
+
+	if (spliced)
+		return spliced;
+
+	return ret;
+}
diff --git a/ltt/ltt-relay-vfs.c b/ltt/ltt-relay-vfs.c
new file mode 100644
index 0000000..802d191
--- /dev/null
+++ b/ltt/ltt-relay-vfs.c
@@ -0,0 +1,231 @@
+/*
+ * ltt/ltt-relay-vfs.c
+ *
+ * (C) Copyright 2009 - Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * LTTng VFS interface.
+ *
+ * Author:
+ *	Mathieu Desnoyers (mathieu.desnoyers@polymtl.ca)
+ *
+ * Dual LGPL v2.1/GPL v2 license.
+ */
+
+#include <linux/module.h>
+#include <linux/fs.h>
+#include <linux/debugfs.h>
+#include <linux/ltt-tracer.h>
+#include <linux/ltt-relay.h>
+#include <linux/ltt-channels.h>
+
+#include <asm/atomic.h>
+
+#include "ltt-relay-select.h"
+
+/**
+ *	ltt_open - open file op for ltt files
+ *	@inode: opened inode
+ *	@file: opened file
+ *
+ *	Open implementation. Makes sure only one open instance of a buffer is
+ *	done at a given moment.
+ */
+static int ltt_open(struct inode *inode, struct file *file)
+{
+	struct ltt_chanbuf *buf = inode->i_private;
+	int ret;
+
+	ret = ltt_chanbuf_open_read(buf);
+	if (ret)
+		goto end;
+
+	file->private_data = buf;
+	ret = nonseekable_open(inode, file);
+end:
+	return ret;
+}
+
+/**
+ *	ltt_release - release file op for ltt files
+ *	@inode: opened inode
+ *	@file: opened file
+ *
+ *	Release implementation.
+ */
+static int ltt_release(struct inode *inode, struct file *file)
+{
+	struct ltt_chanbuf *buf = inode->i_private;
+	struct ltt_chan *chan = container_of(buf->a.chan, struct ltt_chan, a);
+
+	ltt_ascii_remove(chan);
+	debugfs_remove(buf->a.dentry);
+	ltt_chanbuf_release_read(buf);
+
+	return 0;
+}
+
+/**
+ *	ltt_poll - file op for ltt files
+ *	@filp: the file
+ *	@wait: poll table
+ *
+ *	Poll implementation.
+ */
+static unsigned int ltt_poll(struct file *filp, poll_table *wait)
+{
+	unsigned int mask = 0;
+	struct inode *inode = filp->f_dentry->d_inode;
+	struct ltt_chanbuf *buf = inode->i_private;
+	struct ltt_chan *chan = container_of(buf->a.chan, struct ltt_chan, a);
+
+	if (filp->f_mode & FMODE_READ) {
+		poll_wait_set_exclusive(wait);
+		poll_wait(filp, &buf->read_wait, wait);
+
+		WARN_ON(atomic_long_read(&buf->active_readers) != 1);
+		if (SUBBUF_TRUNC(ltt_chanbuf_get_offset(buf), chan)
+		  - SUBBUF_TRUNC(ltt_chanbuf_get_consumed(buf), chan)
+		  == 0) {
+			if (buf->finalized)
+				return POLLHUP;
+			else
+				return 0;
+		} else {
+			if (SUBBUF_TRUNC(ltt_chanbuf_get_offset(buf), chan)
+			  - SUBBUF_TRUNC(ltt_chanbuf_get_consumed(buf), chan)
+			  >= chan->a.buf_size)
+				return POLLPRI | POLLRDBAND;
+			else
+				return POLLIN | POLLRDNORM;
+		}
+	}
+	return mask;
+}
+
+/**
+ *	ltt_ioctl - control on the debugfs file
+ *
+ *	@inode: the inode
+ *	@filp: the file
+ *	@cmd: the command
+ *	@arg: command arg
+ *
+ *	This ioctl implements three commands necessary for a minimal
+ *	producer/consumer implementation :
+ *	RELAY_GET_SUBBUF
+ *		Get the next sub buffer that can be read. It never blocks.
+ *	RELAY_PUT_SUBBUF
+ *		Release the currently read sub-buffer. Parameter is the last
+ *		put subbuffer (returned by GET_SUBBUF).
+ *	RELAY_GET_N_BUBBUFS
+ *		returns the number of sub buffers in the per cpu channel.
+ *	RELAY_GET_SUBBUF_SIZE
+ *		returns the size of the sub buffers.
+ */
+static
+int ltt_ioctl(struct inode *inode, struct file *filp, unsigned int cmd,
+	      unsigned long arg)
+{
+	struct ltt_chanbuf *buf = inode->i_private;
+	u32 __user *argp = (u32 __user *)arg;
+
+	switch (cmd) {
+	case RELAY_GET_SUBBUF:
+	{
+		unsigned long consumed;
+		int ret;
+
+		ret = ltt_chanbuf_get_subbuf(buf, &consumed);
+		if (ret)
+			return ret;
+		else
+			return put_user((u32)consumed, argp);
+		break;
+	}
+	case RELAY_PUT_SUBBUF:
+	{
+		u32 uconsumed_old;
+		int ret;
+		long consumed_old;
+
+		ret = get_user(uconsumed_old, argp);
+		if (ret)
+			return ret; /* will return -EFAULT */
+
+		consumed_old = ltt_chanbuf_get_consumed(buf);
+		consumed_old = consumed_old & (~0xFFFFFFFFL);
+		consumed_old = consumed_old | uconsumed_old;
+		ret = ltt_chanbuf_put_subbuf(buf, consumed_old);
+		if (ret)
+			return ret;
+		break;
+	}
+	case RELAY_GET_N_SUBBUFS:
+		return put_user((u32)buf->a.chan->n_sb, argp);
+		break;
+	case RELAY_GET_SUBBUF_SIZE:
+		return put_user((u32)buf->a.chan->sb_size, argp);
+		break;
+	default:
+		return -ENOIOCTLCMD;
+	}
+	return 0;
+}
+
+#ifdef CONFIG_COMPAT
+static
+long ltt_compat_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
+{
+	long ret = -ENOIOCTLCMD;
+
+	lock_kernel();
+	ret = ltt_ioctl(file->f_dentry->d_inode, file, cmd, arg);
+	unlock_kernel();
+
+	return ret;
+}
+#endif
+
+static const struct file_operations ltt_file_operations = {
+	.open = ltt_open,
+	.release = ltt_release,
+	.poll = ltt_poll,
+	.splice_read = ltt_relay_file_splice_read,
+	.ioctl = ltt_ioctl,
+#ifdef CONFIG_COMPAT
+	.compat_ioctl = ltt_compat_ioctl,
+#endif
+};
+
+int ltt_chanbuf_create_file(const char *filename, struct dentry *parent,
+			    int mode, struct ltt_chanbuf *buf)
+{
+	struct ltt_chan *chan = container_of(buf->a.chan, struct ltt_chan, a);
+	char *tmpname;
+	int ret = 0;
+
+	tmpname = kzalloc(NAME_MAX + 1, GFP_KERNEL);
+	if (!tmpname) {
+		ret = -ENOMEM;
+		goto end;
+	}
+	snprintf(tmpname, NAME_MAX, "%s_%d", chan->a.filename, buf->a.cpu);
+
+	buf->a.dentry = debugfs_create_file(tmpname, mode, parent, buf,
+					    &ltt_file_operations);
+	if (!buf->a.dentry) {
+		ret = -ENOMEM;
+		goto free_name;
+	}
+free_name:
+	kfree(tmpname);
+end:
+	return ret;
+}
+
+int ltt_chanbuf_remove_file(struct ltt_chanbuf *buf)
+{
+	debugfs_remove(buf->a.dentry);
+
+	return 0;
+}
diff --git a/ltt/ltt-serialize.c b/ltt/ltt-serialize.c
index 925b769..6907735 100644
--- a/ltt/ltt-serialize.c
+++ b/ltt/ltt-serialize.c
@@ -127,9 +127,10 @@ enum ltt_type {
  * %*.*:*v expects sizeof(*ptr), __alignof__(*ptr), elem_num, ptr
  *         where elem_num is the number of elements in the sequence
  */
-static inline const char *parse_trace_type(const char *fmt,
-		char *trace_size, enum ltt_type *trace_type,
-		unsigned long *attributes)
+static inline
+const char *parse_trace_type(const char *fmt, char *trace_size,
+			     enum ltt_type *trace_type,
+			     unsigned long *attributes)
 {
 	int qualifier;		/* 'h', 'l', or 'L' for integer fields */
 				/* 'z' support added 23/7/1999 S.H.    */
@@ -228,8 +229,9 @@ parse_end:
  * Field width and precision are *not* supported.
  * %n not supported.
  */
-static inline const char *parse_c_type(const char *fmt,
-		char *c_size, enum ltt_type *c_type, char *outfmt)
+static inline
+const char *parse_c_type(const char *fmt, char *c_size, enum ltt_type *c_type,
+			 char *outfmt)
 {
 	int qualifier;		/* 'h', 'l', or 'L' for integer fields */
 				/* 'z' support added 23/7/1999 S.H.    */
@@ -320,11 +322,11 @@ parse_end:
 	return fmt;
 }
 
-static inline size_t serialize_trace_data(struct rchan_buf *buf,
-		size_t buf_offset,
-		char trace_size, enum ltt_type trace_type,
-		char c_size, enum ltt_type c_type,
-		int *largest_align, va_list *args)
+static inline
+size_t serialize_trace_data(struct ltt_chanbuf *buf, size_t buf_offset,
+			    char trace_size, enum ltt_type trace_type,
+			    char c_size, enum ltt_type c_type,
+			    int *largest_align, va_list *args)
 {
 	union {
 		unsigned long v_ulong;
@@ -361,16 +363,13 @@ static inline size_t serialize_trace_data(struct rchan_buf *buf,
 	case LTT_TYPE_UNSIGNED_INT:
 		switch (c_size) {
 		case 1:
-			tmp.v_ulong = (unsigned long)(uint8_t)
-					va_arg(*args, unsigned int);
+			tmp.v_ulong = (unsigned long)(uint8_t)va_arg(*args, unsigned int);
 			break;
 		case 2:
-			tmp.v_ulong = (unsigned long)(uint16_t)
-					va_arg(*args, unsigned int);
+			tmp.v_ulong = (unsigned long)(uint16_t)va_arg(*args, unsigned int);
 			break;
 		case 4:
-			tmp.v_ulong = (unsigned long)(uint32_t)
-					va_arg(*args, unsigned int);
+			tmp.v_ulong = (unsigned long)(uint32_t)va_arg(*args, unsigned int);
 			break;
 		case 8:
 			tmp.v_uint64 = va_arg(*args, uint64_t);
@@ -385,8 +384,8 @@ static inline size_t serialize_trace_data(struct rchan_buf *buf,
 			tmp.v_string.s = "<NULL>";
 		tmp.v_string.len = strlen(tmp.v_string.s)+1;
 		if (buf)
-			ltt_relay_write(buf, buf_offset, tmp.v_string.s,
-				tmp.v_string.len);
+			ltt_relay_write(&buf->a, buf->a.chan, buf_offset,
+					tmp.v_string.s, tmp.v_string.len);
 		buf_offset += tmp.v_string.len;
 		goto copydone;
 	default:
@@ -413,31 +412,37 @@ static inline size_t serialize_trace_data(struct rchan_buf *buf,
 			switch (trace_size) {
 			case 1:
 				if (c_size == 8)
-					ltt_relay_write(buf, buf_offset,
+					ltt_relay_write(&buf->a, buf->a.chan,
+					buf_offset,
 					(uint8_t[]){ (uint8_t)tmp.v_uint64 },
 					sizeof(uint8_t));
 				else
-					ltt_relay_write(buf, buf_offset,
+					ltt_relay_write(&buf->a, buf->a.chan,
+					buf_offset,
 					(uint8_t[]){ (uint8_t)tmp.v_ulong },
 					sizeof(uint8_t));
 				break;
 			case 2:
 				if (c_size == 8)
-					ltt_relay_write(buf, buf_offset,
+					ltt_relay_write(&buf->a, buf->a.chan,
+					buf_offset,
 					(uint16_t[]){ (uint16_t)tmp.v_uint64 },
 					sizeof(uint16_t));
 				else
-					ltt_relay_write(buf, buf_offset,
+					ltt_relay_write(&buf->a, buf->a.chan,
+					buf_offset,
 					(uint16_t[]){ (uint16_t)tmp.v_ulong },
 					sizeof(uint16_t));
 				break;
 			case 4:
 				if (c_size == 8)
-					ltt_relay_write(buf, buf_offset,
+					ltt_relay_write(&buf->a, buf->a.chan,
+					buf_offset,
 					(uint32_t[]){ (uint32_t)tmp.v_uint64 },
 					sizeof(uint32_t));
 				else
-					ltt_relay_write(buf, buf_offset,
+					ltt_relay_write(&buf->a, buf->a.chan,
+					buf_offset,
 					(uint32_t[]){ (uint32_t)tmp.v_ulong },
 					sizeof(uint32_t));
 				break;
@@ -446,7 +451,7 @@ static inline size_t serialize_trace_data(struct rchan_buf *buf,
 				 * c_size cannot be other than 8 here because
 				 * trace_size > 4.
 				 */
-				ltt_relay_write(buf, buf_offset,
+				ltt_relay_write(&buf->a, buf->a.chan, buf_offset,
 				(uint64_t[]){ (uint64_t)tmp.v_uint64 },
 				sizeof(uint64_t));
 				break;
@@ -463,12 +468,12 @@ static inline size_t serialize_trace_data(struct rchan_buf *buf,
 		if (buf) {
 			switch (trace_type) {
 			case LTT_TYPE_SIGNED_INT:
-				ltt_relay_write(buf, buf_offset,
+				ltt_relay_write(&buf->a, buf->a.chan, buf_offset,
 					(int64_t[]){ (int64_t)tmp.v_ulong },
 					sizeof(int64_t));
 				break;
 			case LTT_TYPE_UNSIGNED_INT:
-				ltt_relay_write(buf, buf_offset,
+				ltt_relay_write(&buf->a, buf->a.chan, buf_offset,
 					(uint64_t[]){ (uint64_t)tmp.v_ulong },
 					sizeof(uint64_t));
 				break;
@@ -484,10 +489,11 @@ copydone:
 	return buf_offset;
 }
 
-notrace size_t ltt_serialize_data(struct rchan_buf *buf, size_t buf_offset,
-			struct ltt_serialize_closure *closure,
-			void *serialize_private, int *largest_align,
-			const char *fmt, va_list *args)
+notrace size_t
+ltt_serialize_data(struct ltt_chanbuf *buf, size_t buf_offset,
+		   struct ltt_serialize_closure *closure,
+		   void *serialize_private, int *largest_align,
+		   const char *fmt, va_list *args)
 {
 	char trace_size = 0, c_size = 0;	/*
 						 * 0 (unset), 1, 2, 4, 8 bytes.
@@ -504,7 +510,7 @@ notrace size_t ltt_serialize_data(struct rchan_buf *buf, size_t buf_offset,
 				break;
 			attributes = 0;
 			fmt = parse_trace_type(fmt, &trace_size, &trace_type,
-				&attributes);
+					       &attributes);
 			break;
 		case '%':
 			/* c types (%) */
@@ -523,10 +529,11 @@ notrace size_t ltt_serialize_data(struct rchan_buf *buf, size_t buf_offset,
 			if (c_type == LTT_TYPE_STRING)
 				trace_type = LTT_TYPE_STRING;
 			/* perform trace write */
-			buf_offset = serialize_trace_data(buf,
-						buf_offset, trace_size,
-						trace_type, c_size, c_type,
-						largest_align, args);
+			buf_offset = serialize_trace_data(buf, buf_offset,
+							  trace_size,
+							  trace_type, c_size,
+							  c_type,
+							  largest_align, args);
 			trace_size = 0;
 			c_size = 0;
 			trace_type = LTT_TYPE_NONE;
@@ -540,13 +547,15 @@ notrace size_t ltt_serialize_data(struct rchan_buf *buf, size_t buf_offset,
 }
 EXPORT_SYMBOL_GPL(ltt_serialize_data);
 
-static inline uint64_t unserialize_base_type(struct rchan_buf *buf,
-		size_t *ppos, char trace_size, enum ltt_type trace_type)
+static inline
+uint64_t unserialize_base_type(struct ltt_chanbuf *buf,
+			       size_t *ppos, char trace_size,
+			       enum ltt_type trace_type)
 {
 	uint64_t tmp;
 
 	*ppos += ltt_align(*ppos, trace_size);
-	ltt_relay_read(buf, *ppos, &tmp, trace_size);
+	ltt_relay_read(&buf->a, *ppos, &tmp, trace_size);
 	*ppos += trace_size;
 
 	switch (trace_type) {
@@ -582,16 +591,18 @@ static inline uint64_t unserialize_base_type(struct rchan_buf *buf,
 	return 0;
 }
 
-static int serialize_printf_data(struct rchan_buf *buf, size_t *ppos,
-		char trace_size, enum ltt_type trace_type,
-		char c_size, enum ltt_type c_type,
-		char *output, ssize_t outlen, const char *outfmt)
+static
+int serialize_printf_data(struct ltt_chanbuf *buf, size_t *ppos,
+			  char trace_size, enum ltt_type trace_type,
+			  char c_size, enum ltt_type c_type, char *output,
+			  ssize_t outlen, const char *outfmt)
 {
 	u64 value;
 	outlen = outlen < 0 ? 0 : outlen;
 
 	if (trace_type == LTT_TYPE_STRING) {
-		size_t len = ltt_relay_read_cstr(buf, *ppos, output, outlen);
+		size_t len = ltt_relay_read_cstr(&buf->a, *ppos, output,
+						 outlen);
 		*ppos += len + 1;
 		return len;
 	}
@@ -618,8 +629,9 @@ static int serialize_printf_data(struct rchan_buf *buf, size_t *ppos,
  * '\0', as per ISO C99. If the return is greater than or equal to @outlen,
  * the resulting string is truncated.
  */
-size_t ltt_serialize_printf(struct rchan_buf *buf, unsigned long buf_offset,
-		size_t *msg_size, char *output, size_t outlen, const char *fmt)
+size_t ltt_serialize_printf(struct ltt_chanbuf *buf, unsigned long buf_offset,
+			    size_t *msg_size, char *output, size_t outlen,
+			    const char *fmt)
 {
 	char trace_size = 0, c_size = 0;	/*
 						 * 0 (unset), 1, 2, 4, 8 bytes.
@@ -644,7 +656,7 @@ size_t ltt_serialize_printf(struct rchan_buf *buf, unsigned long buf_offset,
 			}
 			attributes = 0;
 			fmt = parse_trace_type(fmt, &trace_size, &trace_type,
-				&attributes);
+					       &attributes);
 			break;
 		case '%':
 			/* c types (%) */
@@ -669,9 +681,9 @@ size_t ltt_serialize_printf(struct rchan_buf *buf, unsigned long buf_offset,
 
 			/* perform trace printf */
 			len = serialize_printf_data(buf, &msgpos, trace_size,
-					trace_type, c_size, c_type,
-					output + outpos, outlen - outpos,
-					outfmt);
+						    trace_type, c_size, c_type,
+						    output + outpos,
+						    outlen - outpos, outfmt);
 			outpos += len;
 			trace_size = 0;
 			c_size = 0;
@@ -715,7 +727,7 @@ unsigned int ltt_fmt_largest_align(size_t align_drift, const char *fmt)
 				break;
 			attributes = 0;
 			fmt = parse_trace_type(fmt, &trace_size, &trace_type,
-				&attributes);
+					       &attributes);
 
 			largest_align = max_t(int, largest_align, trace_size);
 			if (largest_align >= ltt_get_alignment())
@@ -762,21 +774,22 @@ EXPORT_SYMBOL_GPL(ltt_fmt_largest_align);
  * Calculate data size
  * Assume that the padding for alignment starts at a sizeof(void *) address.
  */
-static notrace size_t ltt_get_data_size(struct ltt_serialize_closure *closure,
-				void *serialize_private, int *largest_align,
-				const char *fmt, va_list *args)
+static notrace
+size_t ltt_get_data_size(struct ltt_serialize_closure *closure,
+			 void *serialize_private, int *largest_align,
+			 const char *fmt, va_list *args)
 {
 	ltt_serialize_cb cb = closure->callbacks[0];
 	closure->cb_idx = 0;
-	return (size_t)cb(NULL, 0, closure, serialize_private,
-				largest_align, fmt, args);
+	return (size_t)cb(NULL, 0, closure, serialize_private, largest_align,
+			  fmt, args);
 }
 
 static notrace
-void ltt_write_event_data(struct rchan_buf *buf, size_t buf_offset,
-				struct ltt_serialize_closure *closure,
-				void *serialize_private, int largest_align,
-				const char *fmt, va_list *args)
+void ltt_write_event_data(struct ltt_chanbuf *buf, size_t buf_offset,
+			  struct ltt_serialize_closure *closure,
+			  void *serialize_private, int largest_align,
+			  const char *fmt, va_list *args)
 {
 	ltt_serialize_cb cb = closure->callbacks[0];
 	closure->cb_idx = 0;
@@ -785,18 +798,18 @@ void ltt_write_event_data(struct rchan_buf *buf, size_t buf_offset,
 }
 
 
-notrace void ltt_vtrace(const struct marker *mdata, void *probe_data,
-			void *call_data, const char *fmt, va_list *args)
+notrace
+void ltt_vtrace(const struct marker *mdata, void *probe_data, void *call_data,
+		const char *fmt, va_list *args)
 {
 	int largest_align, ret;
 	struct ltt_active_marker *pdata;
 	uint16_t eID;
 	size_t data_size, slot_size;
 	unsigned int chan_index;
-	struct ltt_channel_struct *channel;
-	struct ltt_trace_struct *trace, *dest_trace = NULL;
-	struct rchan_buf *buf;
-	void *transport_data;
+	struct ltt_chanbuf *buf;
+	struct ltt_chan *chan;
+	struct ltt_trace *trace, *dest_trace = NULL;
 	uint64_t tsc;
 	long buf_offset;
 	va_list args_copy;
@@ -835,7 +848,7 @@ notrace void ltt_vtrace(const struct marker *mdata, void *probe_data,
 	 */
 	largest_align = 1;	/* must be non-zero for ltt_align */
 	data_size = ltt_get_data_size(&closure, serialize_private,
-					&largest_align, fmt, &args_copy);
+				      &largest_align, fmt, &args_copy);
 	largest_align = min_t(int, largest_align, sizeof(void *));
 	va_end(args_copy);
 
@@ -864,40 +877,37 @@ notrace void ltt_vtrace(const struct marker *mdata, void *probe_data,
 		 */
 		if (unlikely(chan_index >= trace->nr_channels))
 			continue;
-		channel = &trace->channels[chan_index];
-		if (!channel->active)
+		chan = &trace->channels[chan_index];
+		if (!chan->active)
 			continue;
 
 		/* reserve space : header and data */
-		ret = ltt_reserve_slot(trace, channel, &transport_data,
-					data_size, &slot_size, &buf_offset,
-					&tsc, &rflags,
-					largest_align, cpu);
+		ret = ltt_reserve_slot(chan, trace, data_size, largest_align,
+				       cpu, &buf, &slot_size, &buf_offset,
+				       &tsc, &rflags);
 		if (unlikely(ret < 0))
 			continue; /* buffer full */
 
 		va_copy(args_copy, *args);
-		/* FIXME : could probably encapsulate transport better. */
-		buf = ((struct rchan *)channel->trans_channel_data)->buf[cpu];
 		/* Out-of-order write : header and data */
-		buf_offset = ltt_write_event_header(trace,
-					channel, buf, buf_offset,
-					eID, data_size, tsc, rflags);
+		buf_offset = ltt_write_event_header(&buf->a, &chan->a,
+						    buf_offset, eID, data_size,
+						    tsc, rflags);
 		ltt_write_event_data(buf, buf_offset, &closure,
-					serialize_private,
-					largest_align, fmt, &args_copy);
+				     serialize_private, largest_align, fmt,
+				     &args_copy);
 		va_end(args_copy);
 		/* Out-of-order commit */
-		ltt_commit_slot(channel, &transport_data, buf_offset,
-				data_size, slot_size);
+		ltt_commit_slot(buf, chan, buf_offset, data_size, slot_size);
 	}
 	__get_cpu_var(ltt_nesting)--;
 	rcu_read_unlock_sched_notrace();
 }
 EXPORT_SYMBOL_GPL(ltt_vtrace);
 
-notrace void ltt_trace(const struct marker *mdata, void *probe_data,
-		       void *call_data, const char *fmt, ...)
+notrace
+void ltt_trace(const struct marker *mdata, void *probe_data, void *call_data,
+	       const char *fmt, ...)
 {
 	va_list args;
 
diff --git a/ltt/ltt-statedump.c b/ltt/ltt-statedump.c
index b9c115f..426d13d 100644
--- a/ltt/ltt-statedump.c
+++ b/ltt/ltt-statedump.c
@@ -82,7 +82,7 @@ enum lttng_process_status {
 
 #ifdef CONFIG_INET
 static void ltt_enumerate_device(struct ltt_probe_private_data *call_data,
-		struct net_device *dev)
+				 struct net_device *dev)
 {
 	struct in_device *in_dev;
 	struct in_ifaddr *ifa;
@@ -90,21 +90,21 @@ static void ltt_enumerate_device(struct ltt_probe_private_data *call_data,
 	if (dev->flags & IFF_UP) {
 		in_dev = in_dev_get(dev);
 		if (in_dev) {
-			for (ifa = in_dev->ifa_list;
-					ifa != NULL;
-					ifa = ifa->ifa_next)
+			for (ifa = in_dev->ifa_list; ifa != NULL;
+			     ifa = ifa->ifa_next)
 				__trace_mark(0, netif_state,
-					network_ipv4_interface,
-					call_data,
-					"name %s address #n4u%lu up %d",
-					dev->name,
-					(unsigned long)ifa->ifa_address, 0);
+					     network_ipv4_interface,
+					     call_data,
+					     "name %s address #n4u%lu up %d",
+					     dev->name,
+					     (unsigned long)ifa->ifa_address,
+					     0);
 			in_dev_put(in_dev);
 		}
 	} else
 		__trace_mark(0, netif_state, network_ip_interface,
-			call_data, "name %s address #n4u%lu up %d",
-			dev->name, 0UL, 0);
+			     call_data, "name %s address #n4u%lu up %d",
+			     dev->name, 0UL, 0);
 }
 
 static inline int
@@ -130,7 +130,7 @@ ltt_enumerate_network_ip_interface(struct ltt_probe_private_data *call_data)
 
 static inline void
 ltt_enumerate_task_fd(struct ltt_probe_private_data *call_data,
-		struct task_struct *t, char *tmp)
+		      struct task_struct *t, char *tmp)
 {
 	struct fdtable *fdt;
 	struct file *filp;
@@ -149,9 +149,9 @@ ltt_enumerate_task_fd(struct ltt_probe_private_data *call_data,
 		path = d_path(&filp->f_path, tmp, PAGE_SIZE);
 		/* Make sure we give at least some info */
 		__trace_mark(0, fd_state, file_descriptor, call_data,
-			"filename %s pid %d fd %u",
-			(IS_ERR(path))?(filp->f_dentry->d_name.name):(path),
-			t->pid, i);
+			     "filename %s pid %d fd %u",
+			     (IS_ERR(path))?(filp->f_dentry->d_name.name):(path),
+			     t->pid, i);
 	}
 	spin_unlock(&t->files->file_lock);
 }
@@ -200,14 +200,11 @@ ltt_enumerate_task_vm_maps(struct ltt_probe_private_data *call_data,
 			else
 				ino = 0;
 			__trace_mark(0, vm_state, vm_map, call_data,
-					"pid %d start %lu end %lu flags %lu "
-					"pgoff %lu inode %lu",
-					t->pid,
-					map->vm_start,
-					map->vm_end,
-					map->vm_flags,
-					map->vm_pgoff << PAGE_SHIFT,
-					ino);
+				     "pid %d start %lu end %lu flags %lu "
+				     "pgoff %lu inode %lu",
+				     t->pid, map->vm_start, map->vm_end,
+				     map->vm_flags, map->vm_pgoff << PAGE_SHIFT,
+				     ino);
 			map = map->vm_next;
 		}
 		up_read(&mm->mmap_sem);
@@ -249,8 +246,8 @@ static inline void list_interrupts(struct ltt_probe_private_data *call_data)
 		raw_spin_lock(&desc->lock);
 		for (action = desc->action; action; action = action->next)
 			__trace_mark(0, irq_state, interrupt, call_data,
-				"name %s action %s irq_id %u",
-				irq_chip_name, action->name, irq);
+				     "name %s action %s irq_id %u",
+				     irq_chip_name, action->name, irq);
 		raw_spin_unlock(&desc->lock);
 		local_irq_restore(flags);
 	}
@@ -323,10 +320,10 @@ ltt_enumerate_process_states(struct ltt_probe_private_data *call_data)
 			type = LTTNG_KERNEL_THREAD;
 
 		__trace_mark(0, task_state, process_state, call_data,
-				"pid %d parent_pid %d name %s type %d mode %d "
-				"submode %d status %d tgid %d",
-				t->pid, t->parent->pid, t->comm,
-				type, mode, submode, status, t->tgid);
+			     "pid %d parent_pid %d name %s type %d mode %d "
+			     "submode %d status %d tgid %d",
+			     t->pid, t->parent->pid, t->comm,
+			     type, mode, submode, status, t->tgid);
 		task_unlock(t);
 	} while (t != &init_task);
 
@@ -407,7 +404,7 @@ static int do_ltt_statedump(struct ltt_probe_private_data *call_data)
 	return 0;
 }
 
-int ltt_statedump_start(struct ltt_trace_struct *trace)
+int ltt_statedump_start(struct ltt_trace *trace)
 {
 	struct ltt_probe_private_data call_data;
 	printk(KERN_DEBUG "LTT state dump begin\n");
diff --git a/ltt/ltt-trace-control.c b/ltt/ltt-trace-control.c
index 6305059..de1f9ff 100644
--- a/ltt/ltt-trace-control.c
+++ b/ltt/ltt-trace-control.c
@@ -79,14 +79,14 @@ static struct dentry *dir_lookup(struct dentry *parent, const char *name)
 
 
 static ssize_t alloc_write(struct file *file, const char __user *user_buf,
-		size_t count, loff_t *ppos)
+			   size_t count, loff_t *ppos)
 {
 	int err = 0;
-	char buf[NAME_MAX];
 	int buf_size;
-	char cmd[NAME_MAX];
+	char *buf = (char *)__get_free_page(GFP_KERNEL);
+	char *cmd = (char *)__get_free_page(GFP_KERNEL);
 
-	buf_size = min(count, sizeof(buf) - 1);
+	buf_size = min(count, PAGE_SIZE - 1);
 	err = copy_from_user(buf, user_buf, buf_size);
 	if (err)
 		goto err_copy_from_user;
@@ -109,12 +109,16 @@ static ssize_t alloc_write(struct file *file, const char __user *user_buf,
 		goto err_alloc_trace;
 	}
 
+	free_page((unsigned long)buf);
+	free_page((unsigned long)cmd);
 	return count;
 
 err_alloc_trace:
 err_bad_cmd:
 err_get_cmd:
 err_copy_from_user:
+	free_page((unsigned long)buf);
+	free_page((unsigned long)cmd);
 	return err;
 }
 
@@ -124,14 +128,14 @@ static const struct file_operations ltt_alloc_operations = {
 
 
 static ssize_t enabled_write(struct file *file, const char __user *user_buf,
-		size_t count, loff_t *ppos)
+			     size_t count, loff_t *ppos)
 {
 	int err = 0;
-	char buf[NAME_MAX];
 	int buf_size;
-	char cmd[NAME_MAX];
+	char *buf = (char *)__get_free_page(GFP_KERNEL);
+	char *cmd = (char *)__get_free_page(GFP_KERNEL);
 
-	buf_size = min(count, sizeof(buf) - 1);
+	buf_size = min(count, PAGE_SIZE - 1);
 	err = copy_from_user(buf, user_buf, buf_size);
 	if (err)
 		goto err_copy_from_user;
@@ -154,8 +158,8 @@ static ssize_t enabled_write(struct file *file, const char __user *user_buf,
 		err = ltt_trace_start(file->f_dentry->d_parent->d_name.name);
 		if (IS_ERR_VALUE(err)) {
 			printk(KERN_ERR
-				"enabled_write: ltt_trace_start failed: %d\n",
-				err);
+			       "enabled_write: ltt_trace_start failed: %d\n",
+			       err);
 			err = -EPERM;
 			goto err_start_trace;
 		}
@@ -166,8 +170,8 @@ static ssize_t enabled_write(struct file *file, const char __user *user_buf,
 		err = ltt_trace_stop(file->f_dentry->d_parent->d_name.name);
 		if (IS_ERR_VALUE(err)) {
 			printk(KERN_ERR
-				"enabled_write: ltt_trace_stop failed: %d\n",
-				err);
+			       "enabled_write: ltt_trace_stop failed: %d\n",
+			       err);
 			err = -EPERM;
 			goto err_stop_trace;
 		}
@@ -177,6 +181,8 @@ static ssize_t enabled_write(struct file *file, const char __user *user_buf,
 		goto err_bad_cmd;
 	}
 
+	free_page((unsigned long)buf);
+	free_page((unsigned long)cmd);
 	return count;
 
 err_stop_trace:
@@ -184,6 +190,8 @@ err_start_trace:
 err_bad_cmd:
 err_get_cmd:
 err_copy_from_user:
+	free_page((unsigned long)buf);
+	free_page((unsigned long)cmd);
 	return err;
 }
 
@@ -193,14 +201,14 @@ static const struct file_operations ltt_enabled_operations = {
 
 
 static ssize_t trans_write(struct file *file, const char __user *user_buf,
-		size_t count, loff_t *ppos)
+			   size_t count, loff_t *ppos)
 {
+	char *buf = (char *)__get_free_page(GFP_KERNEL);
+	char *trans_name = (char *)__get_free_page(GFP_KERNEL);
 	int err = 0;
-	char buf[NAME_MAX];
 	int buf_size;
-	char trans_name[NAME_MAX];
 
-	buf_size = min(count, sizeof(buf) - 1);
+	buf_size = min(count, PAGE_SIZE - 1);
 	err = copy_from_user(buf, user_buf, buf_size);
 	if (err)
 		goto err_copy_from_user;
@@ -212,18 +220,22 @@ static ssize_t trans_write(struct file *file, const char __user *user_buf,
 	}
 
 	err = ltt_trace_set_type(file->f_dentry->d_parent->d_name.name,
-		trans_name);
+				 trans_name);
 	if (IS_ERR_VALUE(err)) {
 		printk(KERN_ERR "trans_write: ltt_trace_set_type failed: %d\n",
-			err);
+		       err);
 		goto err_set_trans;
 	}
 
+	free_page((unsigned long)buf);
+	free_page((unsigned long)trans_name);
 	return count;
 
 err_set_trans:
 err_get_transname:
 err_copy_from_user:
+	free_page((unsigned long)buf);
+	free_page((unsigned long)trans_name);
 	return err;
 }
 
@@ -236,13 +248,13 @@ static ssize_t channel_subbuf_num_write(struct file *file,
 		const char __user *user_buf, size_t count, loff_t *ppos)
 {
 	int err = 0;
-	char buf[NAME_MAX];
 	int buf_size;
 	unsigned int num;
 	const char *channel_name;
 	const char *trace_name;
+	char *buf = (char *)__get_free_page(GFP_KERNEL);
 
-	buf_size = min(count, sizeof(buf) - 1);
+	buf_size = min(count, PAGE_SIZE - 1);
 	err = copy_from_user(buf, user_buf, buf_size);
 	if (err)
 		goto err_copy_from_user;
@@ -259,15 +271,17 @@ static ssize_t channel_subbuf_num_write(struct file *file,
 	err = ltt_trace_set_channel_subbufcount(trace_name, channel_name, num);
 	if (IS_ERR_VALUE(err)) {
 		printk(KERN_ERR "channel_subbuf_num_write: "
-			"ltt_trace_set_channel_subbufcount failed: %d\n", err);
+		       "ltt_trace_set_channel_subbufcount failed: %d\n", err);
 		goto err_set_subbufcount;
 	}
 
+	free_page((unsigned long)buf);
 	return count;
 
 err_set_subbufcount:
 err_get_number:
 err_copy_from_user:
+	free_page((unsigned long)buf);
 	return err;
 }
 
@@ -276,17 +290,19 @@ static const struct file_operations ltt_channel_subbuf_num_operations = {
 };
 
 
-static ssize_t channel_subbuf_size_write(struct file *file,
-	const char __user *user_buf, size_t count, loff_t *ppos)
+static
+ssize_t channel_subbuf_size_write(struct file *file,
+				  const char __user *user_buf,
+				  size_t count, loff_t *ppos)
 {
 	int err = 0;
-	char buf[NAME_MAX];
 	int buf_size;
 	unsigned int num;
 	const char *channel_name;
 	const char *trace_name;
+	char *buf = (char *)__get_free_page(GFP_KERNEL);
 
-	buf_size = min(count, sizeof(buf) - 1);
+	buf_size = min(count, PAGE_SIZE - 1);
 	err = copy_from_user(buf, user_buf, buf_size);
 	if (err)
 		goto err_copy_from_user;
@@ -303,15 +319,17 @@ static ssize_t channel_subbuf_size_write(struct file *file,
 	err = ltt_trace_set_channel_subbufsize(trace_name, channel_name, num);
 	if (IS_ERR_VALUE(err)) {
 		printk(KERN_ERR "channel_subbuf_size_write: "
-		"ltt_trace_set_channel_subbufsize failed: %d\n", err);
+		       "ltt_trace_set_channel_subbufsize failed: %d\n", err);
 		goto err_set_subbufsize;
 	}
 
+	free_page((unsigned long)buf);
 	return count;
 
 err_set_subbufsize:
 err_get_number:
 err_copy_from_user:
+	free_page((unsigned long)buf);
 	return err;
 }
 
@@ -319,17 +337,19 @@ static const struct file_operations ltt_channel_subbuf_size_operations = {
 	.write = channel_subbuf_size_write,
 };
 
-static ssize_t channel_switch_timer_write(struct file *file,
-	const char __user *user_buf, size_t count, loff_t *ppos)
+static
+ssize_t channel_switch_timer_write(struct file *file,
+				   const char __user *user_buf,
+				   size_t count, loff_t *ppos)
 {
 	int err = 0;
-	char buf[NAME_MAX];
 	int buf_size;
 	unsigned long num;
 	const char *channel_name;
 	const char *trace_name;
+	char *buf = (char *)__get_free_page(GFP_KERNEL);
 
-	buf_size = min(count, sizeof(buf) - 1);
+	buf_size = min(count, PAGE_SIZE - 1);
 	err = copy_from_user(buf, user_buf, buf_size);
 	if (err)
 		goto err_copy_from_user;
@@ -349,15 +369,17 @@ static ssize_t channel_switch_timer_write(struct file *file,
 	err = ltt_trace_set_channel_switch_timer(trace_name, channel_name, num);
 	if (IS_ERR_VALUE(err)) {
 		printk(KERN_ERR "channel_switch_timer_write: "
-		"ltt_trace_set_channel_switch_timer failed: %d\n", err);
+		       "ltt_trace_set_channel_switch_timer failed: %d\n", err);
 		goto err_set_switch_timer;
 	}
 
+	free_page((unsigned long)buf);
 	return count;
 
 err_set_switch_timer:
 err_get_number:
 err_copy_from_user:
+	free_page((unsigned long)buf);
 	return err;
 }
 
@@ -365,17 +387,19 @@ static struct file_operations ltt_channel_switch_timer_operations = {
 	.write = channel_switch_timer_write,
 };
 
-static ssize_t channel_overwrite_write(struct file *file,
-	const char __user *user_buf, size_t count, loff_t *ppos)
+static
+ssize_t channel_overwrite_write(struct file *file,
+				const char __user *user_buf, size_t count,
+				loff_t *ppos)
 {
 	int err = 0;
-	char buf[NAME_MAX];
 	int buf_size;
-	char cmd[NAME_MAX];
 	const char *channel_name;
 	const char *trace_name;
+	char *buf = (char *)__get_free_page(GFP_KERNEL);
+	char *cmd = (char *)__get_free_page(GFP_KERNEL);
 
-	buf_size = min(count, sizeof(buf) - 1);
+	buf_size = min(count, PAGE_SIZE - 1);
 	err = copy_from_user(buf, user_buf, buf_size);
 	if (err)
 		goto err_copy_from_user;
@@ -399,10 +423,11 @@ static ssize_t channel_overwrite_write(struct file *file,
 	case 'y':
 	case '1':
 		err = ltt_trace_set_channel_overwrite(trace_name, channel_name,
-			1);
+						      1);
 		if (IS_ERR_VALUE(err)) {
 			printk(KERN_ERR "channel_overwrite_write: "
-			"ltt_trace_set_channel_overwrite failed: %d\n", err);
+			       "ltt_trace_set_channel_overwrite failed: %d\n",
+			       err);
 			goto err_set_subbufsize;
 		}
 		break;
@@ -410,10 +435,11 @@ static ssize_t channel_overwrite_write(struct file *file,
 	case 'n':
 	case '0':
 		err = ltt_trace_set_channel_overwrite(trace_name, channel_name,
-			0);
+						      0);
 		if (IS_ERR_VALUE(err)) {
 			printk(KERN_ERR "channel_overwrite_write: "
-			"ltt_trace_set_channel_overwrite failed: %d\n", err);
+			       "ltt_trace_set_channel_overwrite failed: %d\n",
+			       err);
 			goto err_set_subbufsize;
 		}
 		break;
@@ -422,12 +448,16 @@ static ssize_t channel_overwrite_write(struct file *file,
 		goto err_bad_cmd;
 	}
 
+	free_page((unsigned long)buf);
+	free_page((unsigned long)cmd);
 	return count;
 
 err_set_subbufsize:
 err_bad_cmd:
 err_get_cmd:
 err_copy_from_user:
+	free_page((unsigned long)buf);
+	free_page((unsigned long)cmd);
 	return err;
 }
 
@@ -436,17 +466,19 @@ static const struct file_operations ltt_channel_overwrite_operations = {
 };
 
 
-static ssize_t channel_enable_write(struct file *file,
-	const char __user *user_buf, size_t count, loff_t *ppos)
+static
+ssize_t channel_enable_write(struct file *file,
+			     const char __user *user_buf, size_t count,
+			     loff_t *ppos)
 {
 	int err = 0;
-	char buf[NAME_MAX];
 	int buf_size;
-	char cmd[NAME_MAX];
 	const char *channel_name;
 	const char *trace_name;
+	char *buf = (char *)__get_free_page(GFP_KERNEL);
+	char *cmd = (char *)__get_free_page(GFP_KERNEL);
 
-	buf_size = min(count, sizeof(buf) - 1);
+	buf_size = min(count, PAGE_SIZE - 1);
 	err = copy_from_user(buf, user_buf, buf_size);
 	if (err)
 		goto err_copy_from_user;
@@ -470,10 +502,11 @@ static ssize_t channel_enable_write(struct file *file,
 	case 'y':
 	case '1':
 		err = ltt_trace_set_channel_enable(trace_name, channel_name,
-			1);
+						   1);
 		if (IS_ERR_VALUE(err)) {
 			printk(KERN_ERR "channel_enable_write: "
-			"ltt_trace_set_channel_enable failed: %d\n", err);
+			       "ltt_trace_set_channel_enable failed: %d\n",
+			       err);
 			goto err_set_subbufsize;
 		}
 		break;
@@ -481,10 +514,11 @@ static ssize_t channel_enable_write(struct file *file,
 	case 'n':
 	case '0':
 		err = ltt_trace_set_channel_enable(trace_name, channel_name,
-			0);
+						   0);
 		if (IS_ERR_VALUE(err)) {
 			printk(KERN_ERR "channel_enable_write: "
-			"ltt_trace_set_channel_enable failed: %d\n", err);
+			       "ltt_trace_set_channel_enable failed: %d\n",
+			       err);
 			goto err_set_subbufsize;
 		}
 		break;
@@ -493,12 +527,16 @@ static ssize_t channel_enable_write(struct file *file,
 		goto err_bad_cmd;
 	}
 
+	free_page((unsigned long)buf);
+	free_page((unsigned long)cmd);
 	return count;
 
 err_set_subbufsize:
 err_bad_cmd:
 err_get_cmd:
 err_copy_from_user:
+	free_page((unsigned long)buf);
+	free_page((unsigned long)cmd);
 	return err;
 }
 
@@ -508,7 +546,7 @@ static const struct file_operations ltt_channel_enable_operations = {
 
 
 static int _create_trace_control_dir(const char *trace_name,
-				     struct ltt_trace_struct *trace)
+				     struct ltt_trace *trace)
 {
 	int err;
 	struct dentry *trace_root, *channel_root;
@@ -519,37 +557,37 @@ static int _create_trace_control_dir(const char *trace_name,
 	trace_root = debugfs_create_dir(trace_name, ltt_control_dir);
 	if (IS_ERR(trace_root) || !trace_root) {
 		printk(KERN_ERR "_create_trace_control_dir: "
-			"create control root dir of %s failed\n", trace_name);
+		       "create control root dir of %s failed\n", trace_name);
 		err = -ENOMEM;
 		goto err_create_trace_root;
 	}
 
 	/* debugfs/control/trace_name/alloc */
 	tmp_den = debugfs_create_file("alloc", S_IWUSR, trace_root, NULL,
-		&ltt_alloc_operations);
+				      &ltt_alloc_operations);
 	if (IS_ERR(tmp_den) || !tmp_den) {
 		printk(KERN_ERR "_create_trace_control_dir: "
-			"create file of alloc failed\n");
+		       "create file of alloc failed\n");
 		err = -ENOMEM;
 		goto err_create_subdir;
 	}
 
 	/* debugfs/control/trace_name/trans */
 	tmp_den = debugfs_create_file("trans", S_IWUSR, trace_root, NULL,
-		&ltt_trans_operations);
+				      &ltt_trans_operations);
 	if (IS_ERR(tmp_den) || !tmp_den) {
 		printk(KERN_ERR "_create_trace_control_dir: "
-			"create file of trans failed\n");
+		       "create file of trans failed\n");
 		err = -ENOMEM;
 		goto err_create_subdir;
 	}
 
 	/* debugfs/control/trace_name/enabled */
 	tmp_den = debugfs_create_file("enabled", S_IWUSR, trace_root, NULL,
-		&ltt_enabled_operations);
+				      &ltt_enabled_operations);
 	if (IS_ERR(tmp_den) || !tmp_den) {
 		printk(KERN_ERR "_create_trace_control_dir: "
-			"create file of enabled failed\n");
+		       "create file of enabled failed\n");
 		err = -ENOMEM;
 		goto err_create_subdir;
 	}
@@ -558,7 +596,7 @@ static int _create_trace_control_dir(const char *trace_name,
 	channel_root = debugfs_create_dir("channel", trace_root);
 	if (IS_ERR(channel_root) || !channel_root) {
 		printk(KERN_ERR "_create_trace_control_dir: "
-			"create dir of channel failed\n");
+		       "create dir of channel failed\n");
 		err = -ENOMEM;
 		goto err_create_subdir;
 	}
@@ -580,68 +618,72 @@ static int _create_trace_control_dir(const char *trace_name,
 
 	for (i = 0; i < trace->nr_channels; i++) {
 		struct dentry *channel_den;
-		struct ltt_channel_struct *channel;
+		struct ltt_chan *chan;
 
-		channel = &trace->channels[i];
-		if (!channel->active)
+		chan = &trace->channels[i];
+		if (!chan->active)
 			continue;
-		channel_den = debugfs_create_dir(channel->channel_name,
+		channel_den = debugfs_create_dir(chan->a.filename,
 						 channel_root);
 		if (IS_ERR(channel_den) || !channel_den) {
 			printk(KERN_ERR "_create_trace_control_dir: "
-				"create channel dir of %s failed\n",
-				channel->channel_name);
+			       "create channel dir of %s failed\n",
+			       chan->a.filename);
 			err = -ENOMEM;
 			goto err_create_subdir;
 		}
 
 		tmp_den = debugfs_create_file("subbuf_num", S_IWUSR,
-			channel_den, NULL, &ltt_channel_subbuf_num_operations);
+					      channel_den, NULL,
+					      &ltt_channel_subbuf_num_operations);
 		if (IS_ERR(tmp_den) || !tmp_den) {
 			printk(KERN_ERR "_create_trace_control_dir: "
-				"create subbuf_num in %s failed\n",
-				channel->channel_name);
+			       "create subbuf_num in %s failed\n",
+			       chan->a.filename);
 			err = -ENOMEM;
 			goto err_create_subdir;
 		}
 
 		tmp_den = debugfs_create_file("subbuf_size", S_IWUSR,
-			channel_den, NULL, &ltt_channel_subbuf_size_operations);
+					      channel_den, NULL,
+					      &ltt_channel_subbuf_size_operations);
 		if (IS_ERR(tmp_den) || !tmp_den) {
 			printk(KERN_ERR "_create_trace_control_dir: "
-				"create subbuf_size in %s failed\n",
-				channel->channel_name);
+			       "create subbuf_size in %s failed\n",
+			       chan->a.filename);
 			err = -ENOMEM;
 			goto err_create_subdir;
 		}
 
 		tmp_den = debugfs_create_file("enable", S_IWUSR, channel_den,
-			NULL, &ltt_channel_enable_operations);
+					      NULL,
+					      &ltt_channel_enable_operations);
 		if (IS_ERR(tmp_den) || !tmp_den) {
 			printk(KERN_ERR "_create_trace_control_dir: "
-				"create enable in %s failed\n",
-				channel->channel_name);
+			       "create enable in %s failed\n",
+			       chan->a.filename);
 			err = -ENOMEM;
 			goto err_create_subdir;
 		}
 
 		tmp_den = debugfs_create_file("overwrite", S_IWUSR, channel_den,
-			NULL, &ltt_channel_overwrite_operations);
+					      NULL,
+					      &ltt_channel_overwrite_operations);
 		if (IS_ERR(tmp_den) || !tmp_den) {
 			printk(KERN_ERR "_create_trace_control_dir: "
-				"create overwrite in %s failed\n",
-				channel->channel_name);
+			       "create overwrite in %s failed\n",
+			       chan->a.filename);
 			err = -ENOMEM;
 			goto err_create_subdir;
 		}
 
 		tmp_den = debugfs_create_file("switch_timer", S_IWUSR,
-			channel_den, NULL,
-			&ltt_channel_switch_timer_operations);
+					      channel_den, NULL,
+					      &ltt_channel_switch_timer_operations);
 		if (IS_ERR(tmp_den) || !tmp_den) {
 			printk(KERN_ERR "_create_trace_control_dir: "
-				"create switch_timer in %s failed\n",
-				channel->channel_name);
+			       "create switch_timer in %s failed\n",
+			       chan->a.filename);
 			err = -ENOMEM;
 			goto err_create_subdir;
 		}
@@ -655,16 +697,17 @@ err_create_trace_root:
 	return err;
 }
 
-static ssize_t setup_trace_write(struct file *file, const char __user *user_buf,
-		size_t count, loff_t *ppos)
+static
+ssize_t setup_trace_write(struct file *file, const char __user *user_buf,
+			  size_t count, loff_t *ppos)
 {
 	int err = 0;
-	char buf[NAME_MAX];
 	int buf_size;
-	char trace_name[NAME_MAX];
-	struct ltt_trace_struct *trace;
+	struct ltt_trace *trace;
+	char *buf = (char *)__get_free_page(GFP_KERNEL);
+	char *trace_name = (char *)__get_free_page(GFP_KERNEL);
 
-	buf_size = min(count, sizeof(buf) - 1);
+	buf_size = min(count, PAGE_SIZE - 1);
 	err = copy_from_user(buf, user_buf, buf_size);
 	if (err)
 		goto err_copy_from_user;
@@ -681,7 +724,7 @@ static ssize_t setup_trace_write(struct file *file, const char __user *user_buf,
 	err = _ltt_trace_setup(trace_name);
 	if (IS_ERR_VALUE(err)) {
 		printk(KERN_ERR
-			"setup_trace_write: ltt_trace_setup failed: %d\n", err);
+		       "setup_trace_write: ltt_trace_setup failed: %d\n", err);
 		goto err_setup_trace;
 	}
 	trace = _ltt_trace_find_setup(trace_name);
@@ -689,13 +732,15 @@ static ssize_t setup_trace_write(struct file *file, const char __user *user_buf,
 	err = _create_trace_control_dir(trace_name, trace);
 	if (IS_ERR_VALUE(err)) {
 		printk(KERN_ERR "setup_trace_write: "
-			"_create_trace_control_dir failed: %d\n", err);
+		       "_create_trace_control_dir failed: %d\n", err);
 		goto err_create_trace_control_dir;
 	}
 
 	ltt_unlock_traces();
 	mutex_unlock(&control_lock);
 
+	free_page((unsigned long)buf);
+	free_page((unsigned long)trace_name);
 	return count;
 
 err_create_trace_control_dir:
@@ -705,6 +750,8 @@ err_setup_trace:
 	mutex_unlock(&control_lock);
 err_get_tracename:
 err_copy_from_user:
+	free_page((unsigned long)buf);
+	free_page((unsigned long)trace_name);
 	return err;
 }
 
@@ -712,17 +759,17 @@ static const struct file_operations ltt_setup_trace_operations = {
 	.write = setup_trace_write,
 };
 
-static ssize_t destroy_trace_write(struct file *file,
-		const char __user *user_buf, size_t count, loff_t *ppos)
+static
+ssize_t destroy_trace_write(struct file *file, const char __user *user_buf,
+			    size_t count, loff_t *ppos)
 {
-	int err = 0;
-	char buf[NAME_MAX];
-	int buf_size;
-	char trace_name[NAME_MAX];
 	struct dentry *trace_den;
+	int buf_size;
+	int err = 0;
+	char *buf = (char *)__get_free_page(GFP_KERNEL);
+	char *trace_name = (char *)__get_free_page(GFP_KERNEL);
 
-
-	buf_size = min(count, sizeof(buf) - 1);
+	buf_size = min(count, PAGE_SIZE - 1);
 	err = copy_from_user(buf, user_buf, buf_size);
 	if (err)
 		goto err_copy_from_user;
@@ -738,8 +785,8 @@ static ssize_t destroy_trace_write(struct file *file,
 	err = ltt_trace_destroy(trace_name);
 	if (IS_ERR_VALUE(err)) {
 		printk(KERN_ERR
-			"destroy_trace_write: ltt_trace_destroy failed: %d\n",
-			err);
+		       "destroy_trace_write: ltt_trace_destroy failed: %d\n",
+		       err);
 		err = -EPERM;
 		goto err_destroy_trace;
 	}
@@ -747,8 +794,8 @@ static ssize_t destroy_trace_write(struct file *file,
 	trace_den = dir_lookup(ltt_control_dir, trace_name);
 	if (!trace_den) {
 		printk(KERN_ERR
-			"destroy_trace_write: lookup for %s's dentry failed\n",
-			trace_name);
+		       "destroy_trace_write: lookup for %s's dentry failed\n",
+		       trace_name);
 		err = -ENOENT;
 		goto err_get_dentry;
 	}
@@ -757,6 +804,8 @@ static ssize_t destroy_trace_write(struct file *file,
 
 	mutex_unlock(&control_lock);
 
+	free_page((unsigned long)buf);
+	free_page((unsigned long)trace_name);
 	return count;
 
 err_get_dentry:
@@ -764,6 +813,8 @@ err_destroy_trace:
 	mutex_unlock(&control_lock);
 err_get_tracename:
 err_copy_from_user:
+	free_page((unsigned long)buf);
+	free_page((unsigned long)trace_name);
 	return err;
 }
 
@@ -777,8 +828,9 @@ static void init_marker_dir(struct dentry *dentry,
 	dentry->d_inode->i_op = opt;
 }
 
-static ssize_t marker_enable_read(struct file *filp, char __user *ubuf,
-			    size_t cnt, loff_t *ppos)
+static
+ssize_t marker_enable_read(struct file *filp, char __user *ubuf,
+			   size_t cnt, loff_t *ppos)
 {
 	char *buf;
 	const char *channel, *marker;
@@ -815,21 +867,22 @@ static ssize_t marker_enable_read(struct file *filp, char __user *ubuf,
 	return len;
 }
 
-static ssize_t marker_enable_write(struct file *filp, const char __user *ubuf,
-				size_t cnt, loff_t *ppos)
+static
+ssize_t marker_enable_write(struct file *filp, const char __user *ubuf,
+			    size_t cnt, loff_t *ppos)
 {
-	char buf[NAME_MAX];
+	char *buf = (char *)__get_free_page(GFP_KERNEL);
 	int buf_size;
-	int err = 0;
+	ssize_t ret = 0;
 	const char *channel, *marker;
 
 	marker = filp->f_dentry->d_parent->d_name.name;
 	channel = filp->f_dentry->d_parent->d_parent->d_name.name;
 
-	buf_size = min(cnt, sizeof(buf) - 1);
-	err = copy_from_user(buf, ubuf, buf_size);
-	if (err)
-		return err;
+	buf_size = min(cnt, PAGE_SIZE - 1);
+	ret = copy_from_user(buf, ubuf, buf_size);
+	if (ret)
+		goto end;
 
 	buf[buf_size] = 0;
 
@@ -837,22 +890,25 @@ static ssize_t marker_enable_write(struct file *filp, const char __user *ubuf,
 	case 'Y':
 	case 'y':
 	case '1':
-		err = ltt_marker_connect(channel, marker, "default");
-		if (err)
-			return err;
+		ret = ltt_marker_connect(channel, marker, "default");
+		if (ret)
+			goto end;
 		break;
 	case 'N':
 	case 'n':
 	case '0':
-		err = ltt_marker_disconnect(channel, marker, "default");
-		if (err)
-			return err;
+		ret = ltt_marker_disconnect(channel, marker, "default");
+		if (ret)
+			goto end;
 		break;
 	default:
-		return -EPERM;
+		ret = -EPERM;
+		goto end;
 	}
-
-	return cnt;
+	ret = cnt;
+end:
+	free_page((unsigned long)buf);
+	return ret;
 }
 
 static const struct file_operations enable_fops = {
@@ -864,8 +920,9 @@ static const struct file_operations enable_fops = {
  * In practice, the output size should never be larger than 4096 kB. If it
  * ever happens, the output will simply be truncated.
  */
-static ssize_t marker_info_read(struct file *filp, char __user *ubuf,
-			    size_t cnt, loff_t *ppos)
+static
+ssize_t marker_info_read(struct file *filp, char __user *ubuf,
+			 size_t cnt, loff_t *ppos)
 {
 	char *buf;
 	const char *channel, *marker;
@@ -1285,28 +1342,32 @@ static int __init ltt_trace_control_init(void)
 	ltt_control_dir = debugfs_create_dir(LTT_CONTROL_DIR, ltt_root_dentry);
 	if (IS_ERR(ltt_control_dir) || !ltt_control_dir) {
 		printk(KERN_ERR
-			"ltt_channel_control_init: create dir of %s failed\n",
-			LTT_CONTROL_DIR);
+		       "ltt_channel_control_init: create dir of %s failed\n",
+		       LTT_CONTROL_DIR);
 		err = -ENOMEM;
 		goto err_create_control_dir;
 	}
 
 	ltt_setup_trace_file = debugfs_create_file(LTT_SETUP_TRACE_FILE,
-		S_IWUSR, ltt_root_dentry, NULL, &ltt_setup_trace_operations);
+						   S_IWUSR, ltt_root_dentry,
+						   NULL,
+						   &ltt_setup_trace_operations);
 	if (IS_ERR(ltt_setup_trace_file) || !ltt_setup_trace_file) {
 		printk(KERN_ERR
-			"ltt_channel_control_init: create file of %s failed\n",
-			LTT_SETUP_TRACE_FILE);
+		       "ltt_channel_control_init: create file of %s failed\n",
+		       LTT_SETUP_TRACE_FILE);
 		err = -ENOMEM;
 		goto err_create_setup_trace_file;
 	}
 
 	ltt_destroy_trace_file = debugfs_create_file(LTT_DESTROY_TRACE_FILE,
-		S_IWUSR, ltt_root_dentry, NULL, &ltt_destroy_trace_operations);
+						     S_IWUSR, ltt_root_dentry,
+						     NULL,
+						     &ltt_destroy_trace_operations);
 	if (IS_ERR(ltt_destroy_trace_file) || !ltt_destroy_trace_file) {
 		printk(KERN_ERR
-			"ltt_channel_control_init: create file of %s failed\n",
-			LTT_DESTROY_TRACE_FILE);
+		       "ltt_channel_control_init: create file of %s failed\n",
+		       LTT_DESTROY_TRACE_FILE);
 		err = -ENOMEM;
 		goto err_create_destroy_trace_file;
 	}
@@ -1315,8 +1376,8 @@ static int __init ltt_trace_control_init(void)
 						 ltt_root_dentry);
 	if (IS_ERR(markers_control_dir) || !markers_control_dir) {
 		printk(KERN_ERR
-			"ltt_channel_control_init: create dir of %s failed\n",
-			MARKERS_CONTROL_DIR);
+		       "ltt_channel_control_init: create dir of %s failed\n",
+		       MARKERS_CONTROL_DIR);
 		err = -ENOMEM;
 		goto err_create_marker_control_dir;
 	}
@@ -1349,7 +1410,7 @@ static void __exit ltt_trace_control_exit(void)
 
 	/* destory all traces */
 	list_for_each_entry(trace_dir, &ltt_control_dir->d_subdirs,
-		d_u.d_child) {
+			    d_u.d_child) {
 		ltt_trace_stop(trace_dir->d_name.name);
 		ltt_trace_destroy(trace_dir->d_name.name);
 	}
diff --git a/ltt/ltt-tracer.c b/ltt/ltt-tracer.c
index 81cc4dd..ad7b491 100644
--- a/ltt/ltt-tracer.c
+++ b/ltt/ltt-tracer.c
@@ -46,13 +46,14 @@ static void async_wakeup(unsigned long data);
 static DEFINE_TIMER(ltt_async_wakeup_timer, async_wakeup, 0, 0);
 
 /* Default callbacks for modules */
-notrace int ltt_filter_control_default(enum ltt_filter_control_msg msg,
-		struct ltt_trace_struct *trace)
+notrace
+int ltt_filter_control_default(enum ltt_filter_control_msg msg,
+			       struct ltt_trace *trace)
 {
 	return 0;
 }
 
-int ltt_statedump_default(struct ltt_trace_struct *trace)
+int ltt_statedump_default(struct ltt_trace *trace)
 {
 	return 0;
 }
@@ -60,20 +61,19 @@ int ltt_statedump_default(struct ltt_trace_struct *trace)
 /* Callbacks for registered modules */
 
 int (*ltt_filter_control_functor)
-	(enum ltt_filter_control_msg msg, struct ltt_trace_struct *trace) =
+	(enum ltt_filter_control_msg msg, struct ltt_trace *trace) =
 					ltt_filter_control_default;
 struct module *ltt_filter_control_owner;
 
 /* These function pointers are protected by a trace activation check */
 struct module *ltt_run_filter_owner;
-int (*ltt_statedump_functor)(struct ltt_trace_struct *trace) =
-					ltt_statedump_default;
+int (*ltt_statedump_functor)(struct ltt_trace *trace) = ltt_statedump_default;
 struct module *ltt_statedump_owner;
 
 struct chan_info_struct {
 	const char *name;
-	unsigned int def_subbufsize;
-	unsigned int def_subbufcount;
+	unsigned int def_sb_size;
+	unsigned int def_n_sb;
 } chan_infos[] = {
 	[LTT_CHANNEL_METADATA] = {
 		LTT_METADATA_CHANNEL,
@@ -181,11 +181,10 @@ static enum ltt_channels get_channel_type_from_name(const char *name)
 	return LTT_CHANNEL_DEFAULT;
 }
 
-size_t ltt_write_event_header_slow(struct ltt_trace_struct *trace,
-		struct ltt_channel_struct *channel,
-		struct rchan_buf *buf, long buf_offset,
-		u16 eID, u32 event_size,
-		u64 tsc, unsigned int rflags)
+size_t ltt_write_event_header_slow(struct ltt_chanbuf_alloc *bufa,
+				   struct ltt_chan_alloc *chana,
+				   long buf_offset, u16 eID, u32 event_size,
+				   u64 tsc, unsigned int rflags)
 {
 	struct ltt_event_header header;
 	u16 small_size;
@@ -203,44 +202,44 @@ size_t ltt_write_event_header_slow(struct ltt_trace_struct *trace,
 	}
 
 	header.id_time |= (u32)tsc & LTT_TSC_MASK;
-	ltt_relay_write(buf, buf_offset, &header, sizeof(header));
+	ltt_relay_write(bufa, chana, buf_offset, &header, sizeof(header));
 	buf_offset += sizeof(header);
 
 	switch (rflags) {
 	case LTT_RFLAG_ID_SIZE_TSC:
 		small_size = (u16)min_t(u32, event_size, LTT_MAX_SMALL_SIZE);
-		ltt_relay_write(buf, buf_offset,
+		ltt_relay_write(bufa, chana, buf_offset,
 			&eID, sizeof(u16));
 		buf_offset += sizeof(u16);
-		ltt_relay_write(buf, buf_offset,
+		ltt_relay_write(bufa, chana, buf_offset,
 			&small_size, sizeof(u16));
 		buf_offset += sizeof(u16);
 		if (small_size == LTT_MAX_SMALL_SIZE) {
-			ltt_relay_write(buf, buf_offset,
+			ltt_relay_write(bufa, chana, buf_offset,
 				&event_size, sizeof(u32));
 			buf_offset += sizeof(u32);
 		}
 		buf_offset += ltt_align(buf_offset, sizeof(u64));
-		ltt_relay_write(buf, buf_offset,
+		ltt_relay_write(bufa, chana, buf_offset,
 			&tsc, sizeof(u64));
 		buf_offset += sizeof(u64);
 		break;
 	case LTT_RFLAG_ID_SIZE:
 		small_size = (u16)min_t(u32, event_size, LTT_MAX_SMALL_SIZE);
-		ltt_relay_write(buf, buf_offset,
+		ltt_relay_write(bufa, chana, buf_offset,
 			&eID, sizeof(u16));
 		buf_offset += sizeof(u16);
-		ltt_relay_write(buf, buf_offset,
+		ltt_relay_write(bufa, chana, buf_offset,
 			&small_size, sizeof(u16));
 		buf_offset += sizeof(u16);
 		if (small_size == LTT_MAX_SMALL_SIZE) {
-			ltt_relay_write(buf, buf_offset,
+			ltt_relay_write(bufa, chana, buf_offset,
 				&event_size, sizeof(u32));
 			buf_offset += sizeof(u32);
 		}
 		break;
 	case LTT_RFLAG_ID:
-		ltt_relay_write(buf, buf_offset,
+		ltt_relay_write(bufa, chana, buf_offset,
 			&eID, sizeof(u16));
 		buf_offset += sizeof(u16);
 		break;
@@ -264,7 +263,7 @@ EXPORT_SYMBOL_GPL(ltt_write_event_header_slow);
  * synchronize the TLBs.
  */
 int ltt_module_register(enum ltt_module_function name, void *function,
-		struct module *owner)
+			struct module *owner)
 {
 	int ret = 0;
 
@@ -291,7 +290,7 @@ int ltt_module_register(enum ltt_module_function name, void *function,
 		}
 		ltt_filter_control_functor =
 			(int (*)(enum ltt_filter_control_msg,
-			struct ltt_trace_struct *))function;
+			struct ltt_trace *))function;
 		ltt_filter_control_owner = owner;
 		break;
 	case LTT_FUNCTION_STATEDUMP:
@@ -300,7 +299,7 @@ int ltt_module_register(enum ltt_module_function name, void *function,
 			goto end;
 		}
 		ltt_statedump_functor =
-			(int (*)(struct ltt_trace_struct *))function;
+			(int (*)(struct ltt_trace *))function;
 		ltt_statedump_owner = owner;
 		break;
 	}
@@ -378,8 +377,8 @@ void ltt_transport_unregister(struct ltt_transport *transport)
 }
 EXPORT_SYMBOL_GPL(ltt_transport_unregister);
 
-static inline int is_channel_overwrite(enum ltt_channels chan,
-	enum trace_mode mode)
+static inline
+int is_channel_overwrite(enum ltt_channels chan, enum trace_mode mode)
 {
 	switch (mode) {
 	case LTT_TRACE_NORMAL:
@@ -408,10 +407,10 @@ static inline int is_channel_overwrite(enum ltt_channels chan,
 	}
 }
 
-static void trace_async_wakeup(struct ltt_trace_struct *trace)
+static void trace_async_wakeup(struct ltt_trace *trace)
 {
 	int i;
-	struct ltt_channel_struct *chan;
+	struct ltt_chan *chan;
 
 	/* Must check each channel for pending read wakeup */
 	for (i = 0; i < trace->nr_channels; i++) {
@@ -424,7 +423,7 @@ static void trace_async_wakeup(struct ltt_trace_struct *trace)
 /* Timer to send async wakeups to the readers */
 static void async_wakeup(unsigned long data)
 {
-	struct ltt_trace_struct *trace;
+	struct ltt_trace *trace;
 
 	/*
 	 * PREEMPT_RT does not allow spinlocks to be taken within preempt
@@ -456,9 +455,9 @@ static void async_wakeup(unsigned long data)
  *
  * Returns a pointer to the trace structure, NULL if not found.
  */
-static struct ltt_trace_struct *_ltt_trace_find(const char *trace_name)
+static struct ltt_trace *_ltt_trace_find(const char *trace_name)
 {
-	struct ltt_trace_struct *trace;
+	struct ltt_trace *trace;
 
 	list_for_each_entry(trace, &ltt_traces.head, list)
 		if (!strncmp(trace->trace_name, trace_name, NAME_MAX))
@@ -472,9 +471,9 @@ static struct ltt_trace_struct *_ltt_trace_find(const char *trace_name)
  *
  * Returns a pointer to the trace structure, NULL if not found.
  */
-struct ltt_trace_struct *_ltt_trace_find_setup(const char *trace_name)
+struct ltt_trace *_ltt_trace_find_setup(const char *trace_name)
 {
-	struct ltt_trace_struct *trace;
+	struct ltt_trace *trace;
 
 	list_for_each_entry(trace, &ltt_traces.setup_head, list)
 		if (!strncmp(trace->trace_name, trace_name, NAME_MAX))
@@ -485,25 +484,13 @@ struct ltt_trace_struct *_ltt_trace_find_setup(const char *trace_name)
 EXPORT_SYMBOL_GPL(_ltt_trace_find_setup);
 
 /**
- * ltt_release_transport - Release an LTT transport
- * @kref : reference count on the transport
- */
-void ltt_release_transport(struct kref *kref)
-{
-	struct ltt_trace_struct *trace = container_of(kref,
-			struct ltt_trace_struct, ltt_transport_kref);
-	trace->ops->remove_dirs(trace);
-}
-EXPORT_SYMBOL_GPL(ltt_release_transport);
-
-/**
  * ltt_release_trace - Release a LTT trace
  * @kref : reference count on the trace
  */
 void ltt_release_trace(struct kref *kref)
 {
-	struct ltt_trace_struct *trace = container_of(kref,
-			struct ltt_trace_struct, kref);
+	struct ltt_trace *trace = container_of(kref, struct ltt_trace, kref);
+
 	ltt_channels_trace_free(trace->channels, trace->nr_channels);
 	kfree(trace);
 }
@@ -527,7 +514,7 @@ static inline void prepare_chan_size_num(unsigned int *subbuf_size,
 int _ltt_trace_setup(const char *trace_name)
 {
 	int err = 0;
-	struct ltt_trace_struct *new_trace = NULL;
+	struct ltt_trace *new_trace = NULL;
 	int metadata_index;
 	unsigned int chan;
 	enum ltt_channels chantype;
@@ -546,7 +533,7 @@ int _ltt_trace_setup(const char *trace_name)
 		goto traces_error;
 	}
 
-	new_trace = kzalloc(sizeof(struct ltt_trace_struct), GFP_KERNEL);
+	new_trace = kzalloc(sizeof(struct ltt_trace), GFP_KERNEL);
 	if (!new_trace) {
 		printk(KERN_ERR
 			"LTT : Unable to allocate memory for trace %s\n",
@@ -582,10 +569,10 @@ int _ltt_trace_setup(const char *trace_name)
 
 		chantype = get_channel_type_from_name(
 			ltt_channels_get_name_from_index(chan));
-		new_trace->channels[chan].subbuf_size =
-			chan_infos[chantype].def_subbufsize;
-		new_trace->channels[chan].subbuf_cnt =
-			chan_infos[chantype].def_subbufcount;
+		new_trace->channels[chan].a.sb_size =
+			chan_infos[chantype].def_sb_size;
+		new_trace->channels[chan].a.n_sb =
+			chan_infos[chantype].def_n_sb;
 	}
 
 	list_add(&new_trace->list, &ltt_traces.setup_head);
@@ -610,7 +597,7 @@ int ltt_trace_setup(const char *trace_name)
 EXPORT_SYMBOL_GPL(ltt_trace_setup);
 
 /* must be called from within a traces lock. */
-static void _ltt_trace_free(struct ltt_trace_struct *trace)
+static void _ltt_trace_free(struct ltt_trace *trace)
 {
 	list_del(&trace->list);
 	kfree(trace);
@@ -619,7 +606,7 @@ static void _ltt_trace_free(struct ltt_trace_struct *trace)
 int ltt_trace_set_type(const char *trace_name, const char *trace_type)
 {
 	int err = 0;
-	struct ltt_trace_struct *trace;
+	struct ltt_trace *trace;
 	struct ltt_transport *tran_iter, *transport = NULL;
 
 	ltt_lock_traces();
@@ -653,10 +640,11 @@ traces_error:
 EXPORT_SYMBOL_GPL(ltt_trace_set_type);
 
 int ltt_trace_set_channel_subbufsize(const char *trace_name,
-		const char *channel_name, unsigned int size)
+				     const char *channel_name,
+				     unsigned int size)
 {
 	int err = 0;
-	struct ltt_trace_struct *trace;
+	struct ltt_trace *trace;
 	int index;
 
 	ltt_lock_traces();
@@ -674,7 +662,7 @@ int ltt_trace_set_channel_subbufsize(const char *trace_name,
 		err = -ENOENT;
 		goto traces_error;
 	}
-	trace->channels[index].subbuf_size = size;
+	trace->channels[index].a.sb_size = size;
 
 traces_error:
 	ltt_unlock_traces();
@@ -683,10 +671,11 @@ traces_error:
 EXPORT_SYMBOL_GPL(ltt_trace_set_channel_subbufsize);
 
 int ltt_trace_set_channel_subbufcount(const char *trace_name,
-		const char *channel_name, unsigned int cnt)
+				      const char *channel_name,
+				      unsigned int cnt)
 {
 	int err = 0;
-	struct ltt_trace_struct *trace;
+	struct ltt_trace *trace;
 	int index;
 
 	ltt_lock_traces();
@@ -704,7 +693,7 @@ int ltt_trace_set_channel_subbufcount(const char *trace_name,
 		err = -ENOENT;
 		goto traces_error;
 	}
-	trace->channels[index].subbuf_cnt = cnt;
+	trace->channels[index].a.n_sb = cnt;
 
 traces_error:
 	ltt_unlock_traces();
@@ -713,10 +702,11 @@ traces_error:
 EXPORT_SYMBOL_GPL(ltt_trace_set_channel_subbufcount);
 
 int ltt_trace_set_channel_switch_timer(const char *trace_name,
-		const char *channel_name, unsigned long interval)
+				       const char *channel_name,
+				       unsigned long interval)
 {
 	int err = 0;
-	struct ltt_trace_struct *trace;
+	struct ltt_trace *trace;
 	int index;
 
 	ltt_lock_traces();
@@ -743,10 +733,10 @@ traces_error:
 EXPORT_SYMBOL_GPL(ltt_trace_set_channel_switch_timer);
 
 int ltt_trace_set_channel_enable(const char *trace_name,
-		const char *channel_name, unsigned int enable)
+				 const char *channel_name, unsigned int enable)
 {
 	int err = 0;
-	struct ltt_trace_struct *trace;
+	struct ltt_trace *trace;
 	int index;
 
 	ltt_lock_traces();
@@ -784,10 +774,11 @@ traces_error:
 EXPORT_SYMBOL_GPL(ltt_trace_set_channel_enable);
 
 int ltt_trace_set_channel_overwrite(const char *trace_name,
-		const char *channel_name, unsigned int overwrite)
+				    const char *channel_name,
+				    unsigned int overwrite)
 {
 	int err = 0;
-	struct ltt_trace_struct *trace;
+	struct ltt_trace *trace;
 	int index;
 
 	ltt_lock_traces();
@@ -830,8 +821,8 @@ EXPORT_SYMBOL_GPL(ltt_trace_set_channel_overwrite);
 int ltt_trace_alloc(const char *trace_name)
 {
 	int err = 0;
-	struct ltt_trace_struct *trace;
-	int subbuf_size, subbuf_cnt;
+	struct ltt_trace *trace;
+	int sb_size, n_sb;
 	unsigned long flags;
 	int chan;
 	const char *channel_name;
@@ -846,7 +837,6 @@ int ltt_trace_alloc(const char *trace_name)
 	}
 
 	kref_init(&trace->kref);
-	kref_init(&trace->ltt_transport_kref);
 	init_waitqueue_head(&trace->kref_wq);
 	trace->active = 0;
 	get_trace_clock();
@@ -883,16 +873,18 @@ int ltt_trace_alloc(const char *trace_name)
 
 		channel_name = ltt_channels_get_name_from_index(chan);
 		WARN_ON(!channel_name);
-		subbuf_size = trace->channels[chan].subbuf_size;
-		subbuf_cnt = trace->channels[chan].subbuf_cnt;
-		prepare_chan_size_num(&subbuf_size, &subbuf_cnt);
-		err = trace->ops->create_channel(trace_name, trace,
-				trace->dentry.trace_root,
-				channel_name,
-				&trace->channels[chan],
-				subbuf_size,
-				subbuf_cnt,
-				trace->channels[chan].overwrite);
+		/*
+		 * note: sb_size and n_sb will be overwritten with updated
+		 * values by channel creation.
+		 */
+		sb_size = trace->channels[chan].a.sb_size;
+		n_sb = trace->channels[chan].a.n_sb;
+		prepare_chan_size_num(&sb_size, &n_sb);
+		err = trace->ops->create_channel(channel_name,
+				      &trace->channels[chan],
+				      trace->dentry.trace_root,
+				      sb_size, n_sb,
+				      trace->channels[chan].overwrite, trace);
 		if (err != 0) {
 			printk(KERN_ERR	"LTT : Can't create channel %s.\n",
 				channel_name);
@@ -916,7 +908,8 @@ int ltt_trace_alloc(const char *trace_name)
 create_channel_error:
 	for (chan--; chan >= 0; chan--)
 		if (trace->channels[chan].active)
-			trace->ops->remove_channel(&trace->channels[chan]);
+			trace->ops->remove_channel(&trace->channels[chan].a.kref);
+	trace->ops->remove_dirs(trace);
 
 dirs_error:
 	module_put(trace->transport->owner);
@@ -933,11 +926,12 @@ EXPORT_SYMBOL_GPL(ltt_trace_alloc);
  * We will make a new ltt_control based on debugfs, and control each channel's
  * buffer.
  */
-static int ltt_trace_create(const char *trace_name, const char *trace_type,
-		enum trace_mode mode,
-		unsigned int subbuf_size_low, unsigned int n_subbufs_low,
-		unsigned int subbuf_size_med, unsigned int n_subbufs_med,
-		unsigned int subbuf_size_high, unsigned int n_subbufs_high)
+static
+int ltt_trace_create(const char *trace_name, const char *trace_type,
+		     enum trace_mode mode,
+		     unsigned int subbuf_size_low, unsigned int n_subbufs_low,
+		     unsigned int subbuf_size_med, unsigned int n_subbufs_med,
+		     unsigned int subbuf_size_high, unsigned int n_subbufs_high)
 {
 	int err = 0;
 
@@ -957,7 +951,7 @@ static int ltt_trace_create(const char *trace_name, const char *trace_type,
 }
 
 /* Must be called while sure that trace is in the list. */
-static int _ltt_trace_destroy(struct ltt_trace_struct	*trace)
+static int _ltt_trace_destroy(struct ltt_trace *trace)
 {
 	int err = -EPERM;
 
@@ -993,10 +987,10 @@ traces_error:
 }
 
 /* Sleepable part of the destroy */
-static void __ltt_trace_destroy(struct ltt_trace_struct	*trace)
+static void __ltt_trace_destroy(struct ltt_trace *trace)
 {
 	int i;
-	struct ltt_channel_struct *chan;
+	struct ltt_chan *chan;
 
 	for (i = 0; i < trace->nr_channels; i++) {
 		chan = &trace->channels[i];
@@ -1016,10 +1010,10 @@ static void __ltt_trace_destroy(struct ltt_trace_struct	*trace)
 	for (i = 0; i < trace->nr_channels; i++) {
 		chan = &trace->channels[i];
 		if (chan->active)
-			trace->ops->remove_channel(chan);
+			trace->ops->remove_channel(&chan->a.kref);
 	}
 
-	kref_put(&trace->ltt_transport_kref, ltt_release_transport);
+	trace->ops->remove_dirs(trace);
 
 	module_put(trace->transport->owner);
 
@@ -1038,7 +1032,7 @@ static void __ltt_trace_destroy(struct ltt_trace_struct	*trace)
 int ltt_trace_destroy(const char *trace_name)
 {
 	int err = 0;
-	struct ltt_trace_struct *trace;
+	struct ltt_trace *trace;
 
 	ltt_lock_traces();
 
@@ -1072,8 +1066,38 @@ error:
 }
 EXPORT_SYMBOL_GPL(ltt_trace_destroy);
 
+/*
+ * called with trace lock held.
+ */
+static
+void ltt_channels_trace_start_timer(struct ltt_chan *channels,
+				    unsigned int nr_channels)
+{
+	int i;
+
+	for (i = 0; i < nr_channels; i++) {
+		struct ltt_chan *chan = &channels[i];
+		chan->trace->ops->start_switch_timer(chan);
+	}
+}
+
+/*
+ * called with trace lock held.
+ */
+static
+void ltt_channels_trace_stop_timer(struct ltt_chan *channels,
+				   unsigned int nr_channels)
+{
+	int i;
+
+	for (i = 0; i < nr_channels; i++) {
+		struct ltt_chan *chan = &channels[i];
+		chan->trace->ops->stop_switch_timer(chan);
+	}
+}
+
 /* must be called from within a traces lock. */
-static int _ltt_trace_start(struct ltt_trace_struct *trace)
+static int _ltt_trace_start(struct ltt_trace *trace)
 {
 	int err = 0;
 
@@ -1104,7 +1128,7 @@ traces_error:
 int ltt_trace_start(const char *trace_name)
 {
 	int err = 0;
-	struct ltt_trace_struct *trace;
+	struct ltt_trace *trace;
 
 	ltt_lock_traces();
 
@@ -1144,7 +1168,7 @@ no_trace:
 EXPORT_SYMBOL_GPL(ltt_trace_start);
 
 /* must be called from within traces lock */
-static int _ltt_trace_stop(struct ltt_trace_struct *trace)
+static int _ltt_trace_stop(struct ltt_trace *trace)
 {
 	int err = -EPERM;
 
@@ -1174,7 +1198,7 @@ traces_error:
 int ltt_trace_stop(const char *trace_name)
 {
 	int err = 0;
-	struct ltt_trace_struct *trace;
+	struct ltt_trace *trace;
 
 	ltt_lock_traces();
 	trace = _ltt_trace_find(trace_name);
@@ -1234,7 +1258,7 @@ EXPORT_SYMBOL_GPL(ltt_control);
 int ltt_filter_control(enum ltt_filter_control_msg msg, const char *trace_name)
 {
 	int err;
-	struct ltt_trace_struct *trace;
+	struct ltt_trace *trace;
 
 	printk(KERN_DEBUG "ltt_filter_control : trace %s\n", trace_name);
 	ltt_lock_traces();
@@ -1284,7 +1308,7 @@ module_init(ltt_init)
 
 static void __exit ltt_exit(void)
 {
-	struct ltt_trace_struct *trace;
+	struct ltt_trace *trace;
 	struct list_head *pos, *n;
 
 	ltt_lock_traces();
@@ -1296,14 +1320,14 @@ static void __exit ltt_exit(void)
 	/* Safe iteration is now permitted. It does not have to be RCU-safe
 	 * because no readers are left. */
 	list_for_each_safe(pos, n, &ltt_traces.head) {
-		trace = container_of(pos, struct ltt_trace_struct, list);
+		trace = container_of(pos, struct ltt_trace, list);
 		/* _ltt_trace_destroy does a synchronize_sched() */
 		_ltt_trace_destroy(trace);
 		__ltt_trace_destroy(trace);
 	}
 	/* free traces in pre-alloc status */
 	list_for_each_safe(pos, n, &ltt_traces.setup_head) {
-		trace = container_of(pos, struct ltt_trace_struct, list);
+		trace = container_of(pos, struct ltt_trace, list);
 		_ltt_trace_free(trace);
 	}
 
diff --git a/ltt/ltt-type-serializer.c b/ltt/ltt-type-serializer.c
index 8dad295..c4864ea 100644
--- a/ltt/ltt-type-serializer.c
+++ b/ltt/ltt-type-serializer.c
@@ -12,8 +12,8 @@
 
 #include "ltt-relay-select.h"
 
-notrace void _ltt_specialized_trace(const struct marker *mdata,
-		void *probe_data,
+notrace
+void _ltt_specialized_trace(const struct marker *mdata, void *probe_data,
 		void *serialize_private, unsigned int data_size,
 		unsigned int largest_align)
 {
@@ -21,10 +21,9 @@ notrace void _ltt_specialized_trace(const struct marker *mdata,
 	uint16_t eID;
 	size_t slot_size;
 	unsigned int chan_index;
-	struct ltt_channel_struct *channel;
-	struct ltt_trace_struct *trace;
-	struct rchan_buf *buf;
-	void *transport_data;
+	struct ltt_chanbuf *buf;
+	struct ltt_chan *chan;
+	struct ltt_trace *trace;
 	uint64_t tsc;
 	long buf_offset;
 	int cpu;
@@ -65,33 +64,29 @@ notrace void _ltt_specialized_trace(const struct marker *mdata,
 		 */
 		if (unlikely(chan_index >= trace->nr_channels))
 			continue;
-		channel = &trace->channels[chan_index];
-		if (!channel->active)
+		chan = &trace->channels[chan_index];
+		if (!chan->active)
 			continue;
 
 		/* reserve space : header and data */
-		ret = ltt_reserve_slot(trace, channel, &transport_data,
-					data_size, &slot_size, &buf_offset,
-					&tsc, &rflags,
-					largest_align, cpu);
+		ret = ltt_reserve_slot(chan, trace, data_size, largest_align,
+				       cpu, &buf, &slot_size, &buf_offset, &tsc,
+				       &rflags);
 		if (unlikely(ret < 0))
 			continue; /* buffer full */
 
-		/* FIXME : could probably encapsulate transport better. */
-		buf = ((struct rchan *)channel->trans_channel_data)->buf[cpu];
 		/* Out-of-order write : header and data */
-		buf_offset = ltt_write_event_header(trace,
-					channel, buf, buf_offset,
-					eID, data_size, tsc, rflags);
+		buf_offset = ltt_write_event_header(&buf->a, &chan->a,
+						    buf_offset, eID, data_size,
+						    tsc, rflags);
 		if (data_size) {
 			buf_offset += ltt_align(buf_offset, largest_align);
-			ltt_relay_write(buf, buf_offset, serialize_private,
-				data_size);
+			ltt_relay_write(&buf->a, &chan->a, buf_offset,
+					serialize_private, data_size);
 			buf_offset += data_size;
 		}
 		/* Out-of-order commit */
-		ltt_commit_slot(channel, &transport_data, buf_offset,
-				data_size, slot_size);
+		ltt_commit_slot(buf, chan, buf_offset, data_size, slot_size);
 	}
 	__get_cpu_var(ltt_nesting)--;
 	rcu_read_unlock_sched_notrace();
diff --git a/ltt/ltt-userspace-event.c b/ltt/ltt-userspace-event.c
index 8d1695f..46edbed 100644
--- a/ltt/ltt-userspace-event.c
+++ b/ltt/ltt-userspace-event.c
@@ -32,8 +32,9 @@ static struct dentry *ltt_event_file;
  * Inspired from tracing_mark_write implementation from Steven Rostedt and
  * Ingo Molnar.
  */
-static ssize_t write_event(struct file *file, const char __user *user_buf,
-		size_t count, loff_t *ppos)
+static
+ssize_t write_event(struct file *file, const char __user *user_buf,
+		    size_t count, loff_t *ppos)
 {
 	struct marker *marker;
 	char *buf, *end;
@@ -60,8 +61,8 @@ static ssize_t write_event(struct file *file, const char __user *user_buf,
 	/* Add final \0 to copycount */
 	copycount++;
 	marker = &GET_MARKER(userspace, event);
-	ltt_specialized_trace(marker, marker->single.probe_private,
-		buf, copycount, sizeof(char));
+	ltt_specialized_trace(marker, marker->single.probe_private, buf,
+			      copycount, sizeof(char));
 	/* If there is no \0 nor \n in count, do not return a larger value */
 	ret = min_t(size_t, copycount, count);
 string_err:
-- 
1.6.5.2

