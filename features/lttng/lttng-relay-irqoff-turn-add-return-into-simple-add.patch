From d0cdb9d29e456310a7eaf1814639724515e555f8 Mon Sep 17 00:00:00 2001
From: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
Date: Thu, 13 May 2010 19:27:24 -0400
Subject: [PATCH 239/391] lttng-relay-irqoff-turn-add-return-into-simple-add

lttng irqoff turn local_add_return into local_add

Remove unneeded heavyweight atomic instruction, which also means that we can
simply use a prefetchw rather than prefetch of the commit count.

Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
---
 ltt/ltt-relay-irqoff.c |   63 +++++++++++++++++------------------
 ltt/ltt-relay-irqoff.h |   86 +++++++++++++++++++++++++++++++++++++++---------
 2 files changed, 100 insertions(+), 49 deletions(-)

diff --git a/ltt/ltt-relay-irqoff.c b/ltt/ltt-relay-irqoff.c
index 5ab723b..79a2522 100644
--- a/ltt/ltt-relay-irqoff.c
+++ b/ltt/ltt-relay-irqoff.c
@@ -66,7 +66,7 @@
 struct ltt_reserve_switch_offsets {
 	long begin, end, old;
 	long begin_switch, end_switch_current, end_switch_old;
-	long commit_count, reserve_commit_diff;
+	long reserve_commit_diff;
 	size_t before_hdr_pad, size;
 };
 
@@ -815,7 +815,6 @@ static int ltt_relay_create_buffer(struct ltt_trace_struct *trace,
 		local_set(&ltt_buf->commit_count[j], 0);
 	init_waitqueue_head(&ltt_buf->write_wait);
 	init_waitqueue_head(&ltt_buf->read_wait);
-	atomic_set(&ltt_buf->wakeup_readers, 0);
 	spin_lock_init(&ltt_buf->full_lock);
 
 	ltt_buffer_begin(buf, trace->start_tsc, 0);
@@ -957,10 +956,9 @@ static void ltt_relay_async_wakeup_chan(struct ltt_channel_struct *ltt_channel)
 			continue;
 
 		ltt_buf = rchan->buf[i]->chan_private;
-		if (atomic_read(&ltt_buf->wakeup_readers) == 1) {
-			atomic_set(&ltt_buf->wakeup_readers, 0);
+		if (ltt_poll_deliver(ltt_channel, ltt_buf,
+				     rchan, rchan->buf[i]))
 			wake_up_interruptible(&ltt_buf->read_wait);
-		}
 	}
 }
 
@@ -1185,20 +1183,20 @@ static void ltt_reserve_switch_old_subbuf(
 		struct ltt_reserve_switch_offsets *offsets, u64 *tsc)
 {
 	long oldidx = SUBBUF_INDEX(offsets->old - 1, rchan);
+	long commit_count, padding_size;
 
+	padding_size = rchan->subbuf_size
+			- (SUBBUF_OFFSET(offsets->old - 1, rchan) + 1);
 	ltt_buffer_end(buf, *tsc, offsets->old, oldidx);
 	/* Must write buffer end before incrementing commit count */
 	smp_wmb();
-	offsets->commit_count = local_read(&ltt_buf->commit_count[oldidx])
-				  + rchan->subbuf_size
-				  - (SUBBUF_OFFSET(offsets->old - 1, rchan)
-				     + 1);
-	local_set(&ltt_buf->commit_count[oldidx], offsets->commit_count);
-	if (likely((BUFFER_TRUNC(offsets->old - 1, rchan)
-			>> ltt_channel->n_subbufs_order)
-			- ((offsets->commit_count - rchan->subbuf_size)
-				& ltt_channel->commit_count_mask) == 0))
-		ltt_deliver(buf, oldidx, offsets->commit_count);
+	commit_count = local_read(&ltt_buf->commit_count[oldidx])
+				  + padding_size;
+	local_set(&ltt_buf->commit_count[oldidx], commit_count);
+	ltt_check_deliver(ltt_channel, ltt_buf, rchan, buf,
+		offsets->old - 1, commit_count, oldidx);
+	ltt_write_commit_counter(buf, ltt_buf, oldidx,
+		offsets->old, commit_count, padding_size);
 }
 
 /*
@@ -1215,19 +1213,18 @@ static void ltt_reserve_switch_new_subbuf(
 		struct ltt_reserve_switch_offsets *offsets, u64 *tsc)
 {
 	long beginidx = SUBBUF_INDEX(offsets->begin, rchan);
+	long commit_count;
 
 	ltt_buffer_begin(buf, *tsc, beginidx);
 	/* Must write buffer end before incrementing commit count */
 	smp_wmb();
-	offsets->commit_count = local_read(&ltt_buf->commit_count[beginidx])
+	commit_count = local_read(&ltt_buf->commit_count[beginidx])
 				  + ltt_subbuffer_header_size();
-	local_set(&ltt_buf->commit_count[beginidx], offsets->commit_count);
-	/* Check if the written buffer has to be delivered */
-	if (unlikely((BUFFER_TRUNC(offsets->begin, rchan)
-			>> ltt_channel->n_subbufs_order)
-			- ((offsets->commit_count - rchan->subbuf_size)
-				& ltt_channel->commit_count_mask) == 0))
-		ltt_deliver(buf, beginidx, offsets->commit_count);
+	local_set(&ltt_buf->commit_count[beginidx], commit_count);
+	ltt_check_deliver(ltt_channel, ltt_buf, rchan, buf,
+		offsets->begin, commit_count, beginidx);
+	ltt_write_commit_counter(buf, ltt_buf, beginidx,
+		offsets->begin, commit_count, ltt_subbuffer_header_size());
 }
 
 
@@ -1256,20 +1253,20 @@ static void ltt_reserve_end_switch_current(
 		struct ltt_reserve_switch_offsets *offsets, u64 *tsc)
 {
 	long endidx = SUBBUF_INDEX(offsets->end - 1, rchan);
+	long commit_count, padding_size;
 
+	padding_size = rchan->subbuf_size
+			- (SUBBUF_OFFSET(offsets->end - 1, rchan) + 1);
 	ltt_buffer_end(buf, *tsc, offsets->end, endidx);
 	/* Must write buffer begin before incrementing commit count */
 	smp_wmb();
-	offsets->commit_count = local_read(&ltt_buf->commit_count[endidx])
-				  + rchan->subbuf_size
-				  - (SUBBUF_OFFSET(offsets->end - 1, rchan)
-				     + 1);
-	local_set(&ltt_buf->commit_count[endidx], offsets->commit_count);
-	if (likely((BUFFER_TRUNC(offsets->end - 1, rchan)
-			>> ltt_channel->n_subbufs_order)
-			- ((offsets->commit_count - rchan->subbuf_size)
-				& ltt_channel->commit_count_mask) == 0))
-		ltt_deliver(buf, endidx, offsets->commit_count);
+	commit_count = local_read(&ltt_buf->commit_count[endidx])
+				  + padding_size;
+	local_set(&ltt_buf->commit_count[endidx], commit_count);
+	ltt_check_deliver(ltt_channel, ltt_buf, rchan, buf,
+		offsets->end - 1, commit_count, endidx);
+	ltt_write_commit_counter(buf, ltt_buf, endidx,
+		offsets->end, commit_count, padding_size);
 }
 
 /*
diff --git a/ltt/ltt-relay-irqoff.h b/ltt/ltt-relay-irqoff.h
index 281c349..9056abe 100644
--- a/ltt/ltt-relay-irqoff.h
+++ b/ltt/ltt-relay-irqoff.h
@@ -99,7 +99,6 @@ struct ltt_channel_buf_struct {
 					 * Wait queue for blocking user space
 					 * writers
 					 */
-	atomic_t wakeup_readers;	/* Boolean : wakeup readers waiting ? */
 	wait_queue_head_t read_wait;	/* reader wait queue */
 	unsigned int finalized;		/* buffer has been finalized */
 	struct timer_list switch_timer;	/* timer for periodical switch */
@@ -161,16 +160,72 @@ static __inline__ int last_tsc_overflow(struct ltt_channel_buf_struct *ltt_buf,
 }
 #endif
 
-static __inline__ void ltt_deliver(struct rchan_buf *buf,
-		unsigned int subbuf_idx,
-		long commit_count)
-{
-	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
-
 #ifdef CONFIG_LTT_VMCORE
-	local_set(&ltt_buf->commit_seq[subbuf_idx], commit_count);
+static __inline__ void ltt_check_deliver(struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf,
+		struct rchan *rchan,
+		struct rchan_buf *buf,
+		long offset, long commit_count, long idx)
+{
+	/* Check if all commits have been done */
+	if (unlikely((BUFFER_TRUNC(offset, rchan)
+			>> ltt_channel->n_subbufs_order)
+			- ((commit_count - rchan->subbuf_size)
+			   & ltt_channel->commit_count_mask) == 0)) {
+		local_set(&ltt_buf->commit_seq[idx], commit_count);
+	}
+}
+#else
+static __inline__ void ltt_check_deliver(struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf,
+		struct rchan *rchan,
+		struct rchan_buf *buf,
+		long offset, long commit_count, long idx)
+{
+}
 #endif
-	atomic_set(&ltt_buf->wakeup_readers, 1);
+
+static __inline__ int ltt_poll_deliver(struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf,
+		struct rchan *rchan,
+		struct rchan_buf *buf)
+{
+	long consumed_old, consumed_idx, commit_count, write_offset;
+
+	consumed_old = atomic_long_read(&ltt_buf->consumed);
+	consumed_idx = SUBBUF_INDEX(consumed_old, buf->chan);
+	commit_count = local_read(&ltt_buf->commit_count[consumed_idx]);
+	/*
+	 * No memory barrier here, since we are only interested
+	 * in a statistically correct polling result. The next poll will
+	 * get the data is we are racing. The mb() that ensures correct
+	 * memory order is in get_subbuf.
+	 */
+	write_offset = local_read(&ltt_buf->offset);
+
+	/*
+	 * Check that the subbuffer we are trying to consume has been
+	 * already fully committed.
+	 */
+
+	if (((commit_count - rchan->subbuf_size)
+	     & ltt_channel->commit_count_mask)
+	    - (BUFFER_TRUNC(consumed_old, buf->chan)
+	       >> ltt_channel->n_subbufs_order)
+	    != 0)
+		return 0;
+
+	/*
+	 * Check that we are not about to read the same subbuffer in
+	 * which the writer head is.
+	 */
+	if ((SUBBUF_TRUNC(write_offset, buf->chan)
+	   - SUBBUF_TRUNC(consumed_old, buf->chan))
+	   == 0)
+		return 0;
+
+	return 1;
+
 }
 
 /*
@@ -190,9 +245,11 @@ static __inline__ int ltt_relay_try_reserve(
 
 	*tsc = trace_clock_read64();
 
-	prefetch(&ltt_buf->commit_count[SUBBUF_INDEX(*o_begin, rchan)]);
 #ifdef CONFIG_LTT_VMCORE
+	prefetch(&ltt_buf->commit_count[SUBBUF_INDEX(*o_begin, rchan)]);
 	prefetch(&ltt_buf->commit_seq[SUBBUF_INDEX(*o_begin, rchan)]);
+#else
+	prefetchw(&ltt_buf->commit_count[SUBBUF_INDEX(*o_begin, rchan)]);
 #endif
 	if (last_tsc_overflow(ltt_buf, *tsc))
 		*rflags = LTT_RFLAG_ID_SIZE_TSC;
@@ -349,12 +406,9 @@ static __inline__ void ltt_commit_slot(
 	smp_wmb();
 	commit_count = local_read(&ltt_buf->commit_count[endidx]) + slot_size;
 	local_set(&ltt_buf->commit_count[endidx], commit_count);
-	/* Check if all commits have been done */
-	if (unlikely((BUFFER_TRUNC(offset_end - 1, rchan)
-			>> ltt_channel->n_subbufs_order)
-			- ((commit_count - rchan->subbuf_size)
-			   & ltt_channel->commit_count_mask) == 0))
-		ltt_deliver(buf, endidx, commit_count);
+
+	ltt_check_deliver(ltt_channel, ltt_buf, rchan, buf,
+		offset_end - 1, commit_count, endidx);
 	/*
 	 * Update lost_size for each commit. It's needed only for extracting
 	 * ltt buffers from vmcore, after crash.
-- 
1.6.5.2

