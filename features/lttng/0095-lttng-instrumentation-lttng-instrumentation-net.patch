From b5154a10e1336f0f53c798b93c26f7fcbfabf89f Mon Sep 17 00:00:00 2001
From: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
Date: Thu, 13 May 2010 19:26:05 -0400
Subject: [PATCH 095/391] lttng-instrumentation/lttng-instrumentation-net

LTTng instrumentation - net

Network device activity instrumentation (xmit/receive). Allows to detect when a
packet had arrived on the network card or when it is going to be sent. This is
the instrumentation point outside of the drivers that is the closest to the
hardware. It allows to detect the amount of time taken by a packet to go through
the kernel between the system call and the actual delivery to the network card
(given that system calls are instrumented).

Those tracepoints are used by LTTng.

About the performance impact of tracepoints (which is comparable to markers),
even without immediate values optimizations, tests done by Hideo Aoki on ia64
show no regression. His test case was using hackbench on a kernel where
scheduler instrumentation (about 5 events in code scheduler code) was added.
See the "Tracepoints" patch header for performance result detail.

2.6.29-rc : now instrument __napi_complete rather than napi_complete wrapper.


Note about 2.6.31: napi_poll instrumentation is before the poll, while mainline
instrumentation is after poll. Leaving both for now.

Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
CC: Andrew Morton <akpm@linux-foundation.org>
CC: netdev@vger.kernel.org
CC: Jeff Garzik <jgarzik@pobox.com>
CC: Masami Hiramatsu <mhiramat@redhat.com>
CC: 'Peter Zijlstra' <peterz@infradead.org>
CC: "Frank Ch. Eigler" <fche@redhat.com>
CC: 'Ingo Molnar' <mingo@elte.hu>
CC: 'Hideo AOKI' <haoki@redhat.com>
CC: Takashi Nishiie <t-nishiie@np.css.fujitsu.com>
CC: 'Steven Rostedt' <rostedt@goodmis.org>
CC: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
---
 include/linux/netdevice.h |    1 +
 include/trace/net.h       |   34 ++++++++++++++++++++++++++++++++++
 net/core/dev.c            |   14 ++++++++++++++
 3 files changed, 49 insertions(+), 0 deletions(-)
 create mode 100644 include/trace/net.h

diff --git a/include/linux/netdevice.h b/include/linux/netdevice.h
index fa8b476..89b8309 100644
--- a/include/linux/netdevice.h
+++ b/include/linux/netdevice.h
@@ -43,6 +43,7 @@
 #include <linux/rculist.h>
 #include <linux/dmaengine.h>
 #include <linux/workqueue.h>
+#include <trace/net.h>
 
 #include <linux/ethtool.h>
 #include <net/net_namespace.h>
diff --git a/include/trace/net.h b/include/trace/net.h
new file mode 100644
index 0000000..c33b136
--- /dev/null
+++ b/include/trace/net.h
@@ -0,0 +1,34 @@
+#ifndef _TRACE_NET_H
+#define _TRACE_NET_H
+
+#include <linux/tracepoint.h>
+
+struct sk_buff;
+DECLARE_TRACE(net_dev_xmit,
+	TP_PROTO(struct sk_buff *skb),
+	TP_ARGS(skb));
+DECLARE_TRACE(net_dev_receive,
+	TP_PROTO(struct sk_buff *skb),
+	TP_ARGS(skb));
+
+/*
+ * Note these first 2 traces are actually in __napi_schedule and net_rx_action
+ * respectively.  The former is in __napi_schedule because it uses at-most-once
+ * logic and placing it in the calling routine (napi_schedule) would produce
+ * countless trace events that were effectively  no-ops.  napi_poll is
+ * implemented in net_rx_action, because thats where we do our polling on
+ * devices.  The last trace point is in napi_complete, right where you would
+ * think it would be.
+ */
+struct napi_struct;
+DECLARE_TRACE(net_napi_schedule,
+	TP_PROTO(struct napi_struct *n),
+	TP_ARGS(n));
+DECLARE_TRACE(net_napi_poll,
+	TP_PROTO(struct napi_struct *n),
+	TP_ARGS(n));
+DECLARE_TRACE(net_napi_complete,
+	TP_PROTO(struct napi_struct *n),
+	TP_ARGS(n));
+
+#endif
diff --git a/net/core/dev.c b/net/core/dev.c
index 264137f..81cf832 100644
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@ -130,6 +130,7 @@
 #include <linux/jhash.h>
 #include <linux/random.h>
 #include <trace/events/napi.h>
+#include <trace/net.h>
 
 #include "net-sysfs.h"
 
@@ -196,6 +197,13 @@ static struct list_head ptype_all __read_mostly;	/* Taps */
 DEFINE_RWLOCK(dev_base_lock);
 EXPORT_SYMBOL(dev_base_lock);
 
+DEFINE_TRACE(net_dev_xmit);
+DEFINE_TRACE(net_dev_receive);
+DEFINE_TRACE(net_napi_schedule);
+DEFINE_TRACE(net_napi_poll);
+DEFINE_TRACE(net_napi_complete);
+EXPORT_TRACEPOINT_SYMBOL_GPL(net_napi_complete);
+
 static inline struct hlist_head *dev_name_hash(struct net *net, const char *name)
 {
 	unsigned hash = full_name_hash(name, strnlen(name, IFNAMSIZ));
@@ -2101,6 +2109,7 @@ int dev_queue_xmit(struct sk_buff *skb)
 	}
 
 gso:
+	trace_net_dev_xmit(skb);
 	/* Disable soft irqs for various locks below. Also
 	 * stops preemption for RCU.
 	 */
@@ -2518,6 +2527,7 @@ int netif_receive_skb(struct sk_buff *skb)
 
 	__get_cpu_var(netdev_rx_stat).total++;
 
+	trace_net_dev_receive(skb);
 	skb_reset_network_header(skb);
 	skb_reset_transport_header(skb);
 	skb->mac_len = skb->network_header - skb->mac_header;
@@ -2945,6 +2955,8 @@ void __napi_schedule(struct napi_struct *n)
 {
 	unsigned long flags;
 
+	trace_net_napi_schedule(n);
+
 	local_irq_save(flags);
 	list_add_tail(&n->poll_list, &__get_cpu_var(softnet_data).poll_list);
 	__raise_softirq_irqoff(NET_RX_SOFTIRQ);
@@ -2960,6 +2972,7 @@ void __napi_complete(struct napi_struct *n)
 	list_del(&n->poll_list);
 	smp_mb__before_clear_bit();
 	clear_bit(NAPI_STATE_SCHED, &n->state);
+	trace_net_napi_complete(n);
 }
 EXPORT_SYMBOL(__napi_complete);
 
@@ -3060,6 +3073,7 @@ static void net_rx_action(struct softirq_action *h)
 		 */
 		work = 0;
 		if (test_bit(NAPI_STATE_SCHED, &n->state)) {
+			trace_net_napi_poll(n);
 			work = n->poll(n, weight);
 			trace_napi_poll(n);
 		}
-- 
1.6.5.2

