From 976ef693d0a8f014ef68d2ab1b56268a4dd600d0 Mon Sep 17 00:00:00 2001
From: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
Date: Thu, 13 May 2010 19:28:04 -0400
Subject: [PATCH 316/391] lttng-remove-channel-list

lttng remove channel list

Simplify locking by iterating on traces+channels rather than keeping a separate
channel list.

Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
---
 include/linux/ltt-relay.h |    5 +--
 ltt/ltt-core.c            |    6 +++
 ltt/ltt-relay-alloc.c     |   83 ++++++++++++++++++--------------------------
 ltt/ltt-relay-lockless.c  |   42 +++++++++++++---------
 4 files changed, 66 insertions(+), 70 deletions(-)

diff --git a/include/linux/ltt-relay.h b/include/linux/ltt-relay.h
index 0b3801f..cdef02b 100644
--- a/include/linux/ltt-relay.h
+++ b/include/linux/ltt-relay.h
@@ -16,7 +16,6 @@
 #include <linux/sched.h>
 #include <linux/timer.h>
 #include <linux/wait.h>
-#include <linux/list.h>
 #include <linux/fs.h>
 #include <linux/poll.h>
 #include <linux/kref.h>
@@ -69,7 +68,6 @@ struct ltt_chan_alloc {
 	int extra_reader_sb:1;		/* Bool: has extra reader subbuffer */
 	struct ltt_chanbuf *buf;	/* Channel per-cpu buffers */
 
-	struct list_head list;		/* Channel list */
 	struct kref kref;		/* Reference count */
 	unsigned long n_sb;		/* Number of sub-buffers */
 	struct dentry *parent;		/* Associated parent dentry */
@@ -90,8 +88,7 @@ int ltt_chanbuf_create_file(const char *filename, struct dentry *parent,
 			    int mode, struct ltt_chanbuf *buf);
 int ltt_chanbuf_remove_file(struct ltt_chanbuf *buf);
 
-void ltt_chan_for_each_channel(void (*cb) (struct ltt_chanbuf *buf), int cpu,
-			       int sleepable);
+void ltt_chan_for_each_channel(void (*cb) (struct ltt_chanbuf *buf), int cpu);
 
 extern void _ltt_relay_write(struct ltt_chanbuf_alloc *bufa,
 			     size_t offset, const void *src, size_t len,
diff --git a/ltt/ltt-core.c b/ltt/ltt-core.c
index 37436b6..fcac8f7 100644
--- a/ltt/ltt-core.c
+++ b/ltt/ltt-core.c
@@ -11,6 +11,7 @@
 #include <linux/module.h>
 #include <linux/debugfs.h>
 #include <linux/kref.h>
+#include <linux/cpu.h>
 
 /* Traces structures */
 struct ltt_traces ltt_traces = {
@@ -64,14 +65,19 @@ out:
 }
 EXPORT_SYMBOL_GPL(get_ltt_root);
 
+/*
+ * ltt_lock_traces/ltt_unlock_traces also disables cpu hotplug.
+ */
 void ltt_lock_traces(void)
 {
 	mutex_lock(&ltt_traces_mutex);
+	get_online_cpus();
 }
 EXPORT_SYMBOL_GPL(ltt_lock_traces);
 
 void ltt_unlock_traces(void)
 {
+	put_online_cpus();
 	mutex_unlock(&ltt_traces_mutex);
 }
 EXPORT_SYMBOL_GPL(ltt_unlock_traces);
diff --git a/ltt/ltt-relay-alloc.c b/ltt/ltt-relay-alloc.c
index c4b72ef..5078a3c 100644
--- a/ltt/ltt-relay-alloc.c
+++ b/ltt/ltt-relay-alloc.c
@@ -17,15 +17,9 @@
 #include <linux/cpu.h>
 #include <linux/bitops.h>
 #include <linux/ltt-tracer.h>
-#include <linux/rculist.h>
 
 #include "ltt-relay-select.h"	/* for cpu hotplug */
 
-/* Protect list and channel structures (alloc/free) for sleepable operations */
-static DEFINE_MUTEX(ltt_relay_alloc_mutex);
-/* list of open channels, for cpu hotplug. */
-static LIST_HEAD(ltt_relay_channels);
-
 /**
  * ltt_chanbuf_allocate - allocate a channel buffer
  * @buf: the buffer struct
@@ -205,26 +199,31 @@ int __cpuinit ltt_relay_hotcpu_callback(struct notifier_block *nb,
 					void *hcpu)
 {
 	unsigned int cpu = (unsigned long)hcpu;
+	struct ltt_trace *trace;
 	struct ltt_chan *chan;
-	int ret;
+	struct ltt_chanbuf *buf;
+	int ret, i;
 
 	switch (action) {
 	case CPU_UP_PREPARE:
 	case CPU_UP_PREPARE_FROZEN:
-		mutex_lock(&ltt_relay_alloc_mutex);
-		list_for_each_entry_rcu(chan, &ltt_relay_channels, a.list) {
-			struct ltt_chanbuf *buf = per_cpu_ptr(chan->a.buf, cpu);
-
-			ret = ltt_chanbuf_create(buf, &chan->a, cpu);
-			if (ret) {
-				printk(KERN_ERR
-					"ltt_relay_hotcpu_callback: cpu %d "
-					"buffer creation failed\n", cpu);
-				mutex_unlock(&ltt_relay_alloc_mutex);
-				return NOTIFY_BAD;
+		/*
+		 * CPU hotplug lock protects trace lock from this callback.
+		 */
+		__list_for_each_entry_rcu(trace, &ltt_traces.head, list) {
+			for (i = 0; i < trace->nr_channels; i++) {
+				chan = &trace->channels[i];
+				buf = per_cpu_ptr(chan->a.buf, cpu);
+				ret = ltt_chanbuf_create(buf, &chan->a, cpu);
+				if (ret) {
+					printk(KERN_ERR
+					  "ltt_relay_hotcpu_callback: cpu %d "
+					  "buffer creation failed\n", cpu);
+					return NOTIFY_BAD;
+				}
+
 			}
 		}
-		mutex_unlock(&ltt_relay_alloc_mutex);
 		break;
 	case CPU_DEAD:
 	case CPU_DEAD_FROZEN:
@@ -236,27 +235,25 @@ int __cpuinit ltt_relay_hotcpu_callback(struct notifier_block *nb,
 	return NOTIFY_OK;
 }
 
-void ltt_chan_for_each_channel(void (*cb) (struct ltt_chanbuf *buf), int cpu,
-			       int sleepable)
+/*
+ * Must be called with either trace lock or rcu read lock sched held.
+ */
+void ltt_chan_for_each_channel(void (*cb) (struct ltt_chanbuf *buf), int cpu)
 {
+	struct ltt_trace *trace;
 	struct ltt_chan *chan;
-
-	if (sleepable)
-		rcu_read_lock();
-	else
-		rcu_read_lock_sched_notrace();
-	list_for_each_entry_rcu(chan, &ltt_relay_channels, a.list) {
-		struct ltt_chanbuf *buf;
-
-		if (!chan->active)
-			continue;
-		buf = per_cpu_ptr(chan->a.buf, cpu);
-		cb(buf);
+	struct ltt_chanbuf *buf;
+	int i;
+
+	__list_for_each_entry_rcu(trace, &ltt_traces.head, list) {
+		for (i = 0; i < trace->nr_channels; i++) {
+			chan = &trace->channels[i];
+			if (!chan->active)
+				continue;
+			buf = per_cpu_ptr(chan->a.buf, cpu);
+			cb(buf);
+		}
 	}
-	if (sleepable)
-		rcu_read_unlock();
-	else
-		rcu_read_unlock_sched_notrace();
 }
 
 /**
@@ -317,14 +314,11 @@ int ltt_chan_alloc_init(struct ltt_chan_alloc *chan, struct ltt_trace *trace,
 	if (!chan->buf)
 		goto free_chan;
 
-	mutex_lock(&ltt_relay_alloc_mutex);
 	for_each_online_cpu(i) {
 		ret = ltt_chanbuf_create(per_cpu_ptr(chan->buf, i), chan, i);
 		if (ret)
 			goto free_bufs;
 	}
-	list_add_rcu(&chan->list, &ltt_relay_channels);
-	mutex_unlock(&ltt_relay_alloc_mutex);
 
 	return 0;
 
@@ -337,7 +331,6 @@ free_bufs:
 		ltt_chanbuf_remove_file(buf);
 		ltt_chanbuf_free(buf);
 	}
-	mutex_unlock(&ltt_relay_alloc_mutex);
 	free_percpu(chan->buf);
 free_chan:
 	kref_put(&chan->kref, ltt_chan_free);
@@ -354,13 +347,6 @@ void ltt_chan_alloc_free(struct ltt_chan_alloc *chan)
 {
 	unsigned int i;
 
-	mutex_lock(&ltt_relay_alloc_mutex);
-	list_del_rcu(&chan->list);
-	/* Delay channel free for RCU channel list, protected by
-	 * RCU sched (tracing code) and standard RCU (hotplug management). */
-	synchronize_sched();
-	synchronize_rcu();
-
 	for_each_possible_cpu(i) {
 		struct ltt_chanbuf *buf = per_cpu_ptr(chan->buf, i);
 
@@ -369,7 +355,6 @@ void ltt_chan_alloc_free(struct ltt_chan_alloc *chan)
 		ltt_chanbuf_remove_file(buf);
 		ltt_chanbuf_free(buf);
 	}
-	mutex_unlock(&ltt_relay_alloc_mutex);
 	free_percpu(chan->buf);
 	kref_put(&chan->trace->kref, ltt_release_trace);
 	wake_up_interruptible(&chan->trace->kref_wq);
diff --git a/ltt/ltt-relay-lockless.c b/ltt/ltt-relay-lockless.c
index 1e8865f..a180ebc 100644
--- a/ltt/ltt-relay-lockless.c
+++ b/ltt/ltt-relay-lockless.c
@@ -112,6 +112,9 @@ void ltt_buffer_end(struct ltt_chanbuf *buf, u64 tsc, unsigned int offset,
 	header->subbuf_corrupt = local_read(&buf->corrupted_subbuffers);
 }
 
+/*
+ * Must be called under trace lock or cpu hotplug protection.
+ */
 void ltt_chanbuf_free(struct ltt_chanbuf *buf)
 {
 	struct ltt_chan *chan = container_of(buf->a.chan, struct ltt_chan, a);
@@ -126,10 +129,7 @@ void ltt_chanbuf_free(struct ltt_chanbuf *buf)
 }
 
 /*
- * Must be called under ltt_relay_alloc_mutex protection to ensure serialization
- * of CPU hotplug vs channel creation.
- * ltt_chanbuf_free does not have this requirement, because it is never used for
- * cpu hotplug.
+ * Must be called under trace lock or cpu hotplug protection.
  */
 int ltt_chanbuf_create(struct ltt_chanbuf *buf, struct ltt_chan_alloc *chana,
 		       int cpu)
@@ -454,6 +454,9 @@ static void ltt_chanbuf_start_switch_timer(struct ltt_chanbuf *buf)
 	add_timer_on(&buf->switch_timer, buf->a.cpu);
 }
 
+/*
+ * called with ltt traces lock held.
+ */
 void ltt_chan_start_switch_timer(struct ltt_chan *chan)
 {
 	int cpu;
@@ -461,14 +464,12 @@ void ltt_chan_start_switch_timer(struct ltt_chan *chan)
 	if (!chan->switch_timer_interval)
 		return;
 
-	get_online_cpus();
 	for_each_online_cpu(cpu) {
 		struct ltt_chanbuf *buf;
 
 		buf = per_cpu_ptr(chan->a.buf, cpu);
 		ltt_chanbuf_start_switch_timer(buf);
 	}
-	put_online_cpus();
 }
 /*
  * Cannot use del_timer_sync with add_timer_on, so use an IPI to locally
@@ -491,6 +492,9 @@ static void ltt_chanbuf_stop_switch_timer(struct ltt_chanbuf *buf)
 	smp_call_function(stop_switch_timer_ipi, buf, 1);
 }
 
+/*
+ * called with ltt traces lock held.
+ */
 void ltt_chan_stop_switch_timer(struct ltt_chan *chan)
 {
 	int cpu;
@@ -498,14 +502,12 @@ void ltt_chan_stop_switch_timer(struct ltt_chan *chan)
 	if (!chan->switch_timer_interval)
 		return;
 
-	get_online_cpus();
 	for_each_online_cpu(cpu) {
 		struct ltt_chanbuf *buf;
 
 		buf = per_cpu_ptr(chan->a.buf, cpu);
 		ltt_chanbuf_stop_switch_timer(buf);
 	}
-	put_online_cpus();
 }
 
 static void ltt_chanbuf_idle_switch(struct ltt_chanbuf *buf)
@@ -548,18 +550,20 @@ int ltt_chanbuf_hotcpu_callback(struct notifier_block *nb,
 	case CPU_DOWN_FAILED_FROZEN:
 	case CPU_ONLINE:
 	case CPU_ONLINE_FROZEN:
-		ltt_chan_for_each_channel(ltt_chanbuf_start_switch_timer,
-					  cpu, 1);
+		/*
+		 * CPU hotplug lock protects trace lock from this callback.
+		 */
+		ltt_chan_for_each_channel(ltt_chanbuf_start_switch_timer, cpu);
 		return NOTIFY_OK;
 
 	case CPU_DOWN_PREPARE:
 	case CPU_DOWN_PREPARE_FROZEN:
 		/*
 		 * Performs an IPI to delete the timer locally on the target
-		 * CPU.
+		 * CPU.	CPU hotplug lock protects trace lock from this
+		 * callback.
 		 */
-		ltt_chan_for_each_channel(ltt_chanbuf_stop_switch_timer,
-					  cpu, 1);
+		ltt_chan_for_each_channel(ltt_chanbuf_stop_switch_timer, cpu);
 		return NOTIFY_OK;
 
 	case CPU_DEAD:
@@ -568,9 +572,10 @@ int ltt_chanbuf_hotcpu_callback(struct notifier_block *nb,
 		 * Performing a buffer switch on a remote CPU. Performed by
 		 * the CPU responsible for doing the hotunplug after the target
 		 * CPU stopped running completely. Ensures that all data
-		 * from that remote CPU is flushed.
+		 * from that remote CPU is flushed. CPU hotplug lock protects
+		 * trace lock from this callback.
 		 */
-		ltt_chan_for_each_channel(ltt_chanbuf_switch, cpu, 0);
+		ltt_chan_for_each_channel(ltt_chanbuf_switch, cpu);
 		return NOTIFY_OK;
 
 	default:
@@ -581,9 +586,12 @@ int ltt_chanbuf_hotcpu_callback(struct notifier_block *nb,
 static int pm_idle_entry_callback(struct notifier_block *self,
 				  unsigned long val, void *data)
 {
-	if (val == IDLE_START)
+	if (val == IDLE_START) {
+		rcu_read_lock_sched_notrace();
 		ltt_chan_for_each_channel(ltt_chanbuf_idle_switch,
-					  smp_processor_id(), 0);
+					  smp_processor_id());
+		rcu_read_unlock_sched_notrace();
+	}
 	return 0;
 }
 
-- 
1.6.5.2

