From c57529c7dcdd9ee788934ae01e02c3385c7c558c Mon Sep 17 00:00:00 2001
From: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
Date: Thu, 30 Oct 2008 23:27:26 -0400
Subject: [PATCH] lttng-instrumentation-x86

LTTng - x86 instrumentation

Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
CC: Thomas Gleixner <tglx@linutronix.de>
CC: Ingo Molnar <mingo@redhat.com>
CC: H. Peter Anvin <hpa@zytor.com>
---
 arch/x86/ia32/ipc32.c                     |    3 ++
 arch/x86/kernel/apic_32.c                 |   21 +++++++++++++++++++
 arch/x86/kernel/apic_64.c                 |   15 +++++++++++++
 arch/x86/kernel/cpu/common_64.c           |    2 +
 arch/x86/kernel/cpu/mcheck/mce_intel_64.c |    6 +++++
 arch/x86/kernel/cpu/mcheck/p4.c           |    7 ++++++
 arch/x86/kernel/entry_64.S                |    6 ++--
 arch/x86/kernel/process_32.c              |    7 +++++-
 arch/x86/kernel/process_64.c              |   12 ++++++++++
 arch/x86/kernel/ptrace.c                  |    5 ++++
 arch/x86/kernel/sys_i386_32.c             |    3 ++
 arch/x86/kernel/tlb_32.c                  |    5 ++++
 arch/x86/kernel/tlb_64.c                  |    5 ++++
 arch/x86/kernel/traps_32.c                |   32 ++++++++++++++++++++++------
 arch/x86/kernel/traps_64.c                |   24 ++++++++++++++++++---
 arch/x86/mm/fault.c                       |    6 +++++
 16 files changed, 144 insertions(+), 15 deletions(-)

diff --git a/arch/x86/ia32/ipc32.c b/arch/x86/ia32/ipc32.c
index d21991c..8091aae 100644
--- a/arch/x86/ia32/ipc32.c
+++ b/arch/x86/ia32/ipc32.c
@@ -8,6 +8,7 @@
 #include <linux/shm.h>
 #include <linux/ipc.h>
 #include <linux/compat.h>
+#include <trace/ipc.h>
 
 asmlinkage long sys32_ipc(u32 call, int first, int second, int third,
 			  compat_uptr_t ptr, u32 fifth)
@@ -17,6 +18,8 @@ asmlinkage long sys32_ipc(u32 call, int first, int second, int third,
 	version = call >> 16; /* hack for backward compatibility */
 	call &= 0xffff;
 
+	trace_ipc_call(call, first);
+
 	switch (call) {
 	case SEMOP:
 		/* struct sembuf is the same on 32 and 64bit :)) */
diff --git a/arch/x86/kernel/apic_32.c b/arch/x86/kernel/apic_32.c
index f88bd0d..1b8cc33 100644
--- a/arch/x86/kernel/apic_32.c
+++ b/arch/x86/kernel/apic_32.c
@@ -43,6 +43,9 @@
 #include <mach_apicdef.h>
 #include <mach_ipi.h>
 
+#include <trace/irq.h>
+
+
 /*
  * Sanity check
  */
@@ -630,7 +633,13 @@ void smp_apic_timer_interrupt(struct pt_regs *regs)
 	 * interrupt lock, which is the WrongThing (tm) to do.
 	 */
 	irq_enter();
+
+	trace_irq_entry(LOCAL_TIMER_VECTOR, regs);
+
 	local_apic_timer_interrupt();
+
+	trace_irq_exit(IRQ_HANDLED);
+
 	irq_exit();
 
 	set_irq_regs(old_regs);
@@ -1275,6 +1284,9 @@ void smp_spurious_interrupt(struct pt_regs *regs)
 	unsigned long v;
 
 	irq_enter();
+
+	trace_irq_entry(SPURIOUS_APIC_VECTOR, regs);
+
 	/*
 	 * Check if this really is a spurious interrupt and ACK it
 	 * if it is a vectored one.  Just in case...
@@ -1288,6 +1300,9 @@ void smp_spurious_interrupt(struct pt_regs *regs)
 	printk(KERN_INFO "spurious APIC interrupt on CPU#%d, "
 	       "should never happen.\n", smp_processor_id());
 	__get_cpu_var(irq_stat).irq_spurious_count++;
+
+	trace_irq_exit(IRQ_HANDLED);
+
 	irq_exit();
 }
 
@@ -1299,6 +1314,9 @@ void smp_error_interrupt(struct pt_regs *regs)
 	unsigned long v, v1;
 
 	irq_enter();
+
+	trace_irq_entry(ERROR_APIC_VECTOR, regs);
+
 	/* First tickle the hardware, only then report what went on. -- REW */
 	v = apic_read(APIC_ESR);
 	apic_write(APIC_ESR, 0);
@@ -1318,6 +1336,9 @@ void smp_error_interrupt(struct pt_regs *regs)
 	*/
 	printk(KERN_DEBUG "APIC error on CPU%d: %02lx(%02lx)\n",
 		smp_processor_id(), v , v1);
+
+	trace_irq_exit(IRQ_HANDLED);
+
 	irq_exit();
 }
 
diff --git a/arch/x86/kernel/apic_64.c b/arch/x86/kernel/apic_64.c
index 446c062..e496483 100644
--- a/arch/x86/kernel/apic_64.c
+++ b/arch/x86/kernel/apic_64.c
@@ -27,6 +27,7 @@
 #include <linux/clockchips.h>
 #include <linux/acpi_pmtmr.h>
 #include <linux/module.h>
+#include <trace/irq.h>
 
 #include <asm/atomic.h>
 #include <asm/smp.h>
@@ -489,7 +490,9 @@ void smp_apic_timer_interrupt(struct pt_regs *regs)
 	 */
 	exit_idle();
 	irq_enter();
+	trace_irq_entry(LOCAL_TIMER_VECTOR, regs);
 	local_apic_timer_interrupt();
+	trace_irq_exit(IRQ_HANDLED);
 	irq_exit();
 	set_irq_regs(old_regs);
 }
@@ -959,6 +962,9 @@ asmlinkage void smp_spurious_interrupt(void)
 	unsigned int v;
 	exit_idle();
 	irq_enter();
+
+	trace_irq_entry(SPURIOUS_APIC_VECTOR, NULL);
+
 	/*
 	 * Check if this really is a spurious interrupt and ACK it
 	 * if it is a vectored one.  Just in case...
@@ -969,6 +975,9 @@ asmlinkage void smp_spurious_interrupt(void)
 		ack_APIC_irq();
 
 	add_pda(irq_spurious_count, 1);
+
+	trace_irq_exit(IRQ_HANDLED);
+
 	irq_exit();
 }
 
@@ -981,6 +990,9 @@ asmlinkage void smp_error_interrupt(void)
 
 	exit_idle();
 	irq_enter();
+
+	trace_irq_entry(ERROR_APIC_VECTOR, NULL);
+
 	/* First tickle the hardware, only then report what went on. -- REW */
 	v = apic_read(APIC_ESR);
 	apic_write(APIC_ESR, 0);
@@ -1000,6 +1012,9 @@ asmlinkage void smp_error_interrupt(void)
 	*/
 	printk(KERN_DEBUG "APIC error on CPU%d: %02x(%02x)\n",
 		smp_processor_id(), v , v1);
+
+	trace_irq_exit(IRQ_HANDLED);
+
 	irq_exit();
 }
 
diff --git a/arch/x86/kernel/cpu/common_64.c b/arch/x86/kernel/cpu/common_64.c
index a11f5d4..dd26a80 100644
--- a/arch/x86/kernel/cpu/common_64.c
+++ b/arch/x86/kernel/cpu/common_64.c
@@ -10,6 +10,7 @@
 #include <linux/delay.h>
 #include <linux/smp.h>
 #include <linux/percpu.h>
+
 #include <asm/i387.h>
 #include <asm/msr.h>
 #include <asm/io.h>
@@ -587,6 +588,7 @@ unsigned long kernel_eflags;
  * debugging, no special alignment required.
  */
 DEFINE_PER_CPU(struct orig_ist, orig_ist);
+EXPORT_PER_CPU_SYMBOL_GPL(orig_ist);
 
 /*
  * cpu_init() initializes state that is per-CPU. Some data is already
diff --git a/arch/x86/kernel/cpu/mcheck/mce_intel_64.c b/arch/x86/kernel/cpu/mcheck/mce_intel_64.c
index c17eaf5..efd98aa 100644
--- a/arch/x86/kernel/cpu/mcheck/mce_intel_64.c
+++ b/arch/x86/kernel/cpu/mcheck/mce_intel_64.c
@@ -6,6 +6,7 @@
 #include <linux/init.h>
 #include <linux/interrupt.h>
 #include <linux/percpu.h>
+#include <trace/irq.h>
 #include <asm/processor.h>
 #include <asm/msr.h>
 #include <asm/mce.h>
@@ -22,11 +23,16 @@ asmlinkage void smp_thermal_interrupt(void)
 	exit_idle();
 	irq_enter();
 
+	trace_irq_entry(THERMAL_APIC_VECTOR, NULL);
+
 	rdmsrl(MSR_IA32_THERM_STATUS, msr_val);
 	if (therm_throt_process(msr_val & 1))
 		mce_log_therm_throt_event(smp_processor_id(), msr_val);
 
 	add_pda(irq_thermal_count, 1);
+
+	trace_irq_exit(IRQ_HANDLED);
+
 	irq_exit();
 }
 
diff --git a/arch/x86/kernel/cpu/mcheck/p4.c b/arch/x86/kernel/cpu/mcheck/p4.c
index 9b60fce..80e4f08 100644
--- a/arch/x86/kernel/cpu/mcheck/p4.c
+++ b/arch/x86/kernel/cpu/mcheck/p4.c
@@ -7,6 +7,7 @@
 #include <linux/kernel.h>
 #include <linux/interrupt.h>
 #include <linux/smp.h>
+#include <trace/irq.h>
 
 #include <asm/processor.h>
 #include <asm/system.h>
@@ -60,8 +61,14 @@ static void (*vendor_thermal_interrupt)(struct pt_regs *regs) = unexpected_therm
 void smp_thermal_interrupt(struct pt_regs *regs)
 {
 	irq_enter();
+
+	trace_irq_entry(THERMAL_APIC_VECTOR, regs);
+
 	vendor_thermal_interrupt(regs);
 	__get_cpu_var(irq_stat).irq_thermal_count++;
+
+	trace_irq_exit(IRQ_HANDLED);
+
 	irq_exit();
 }
 
diff --git a/arch/x86/kernel/entry_64.S b/arch/x86/kernel/entry_64.S
index 7a02c65..70a069a 100644
--- a/arch/x86/kernel/entry_64.S
+++ b/arch/x86/kernel/entry_64.S
@@ -282,7 +282,7 @@ ENTRY(ret_from_fork)
 	CFI_ADJUST_CFA_OFFSET -4
 	call schedule_tail
 	GET_THREAD_INFO(%rcx)
-	testl $(_TIF_SYSCALL_TRACE|_TIF_SYSCALL_AUDIT),TI_flags(%rcx)
+	testl $(_TIF_SYSCALL_TRACE|_TIF_KERNEL_TRACE|_TIF_SYSCALL_AUDIT),TI_flags(%rcx)
 	jnz rff_trace
 rff_action:	
 	RESTORE_REST
@@ -396,7 +396,7 @@ sysret_check:
 	/* Handle reschedules */
 	/* edx:	work, edi: workmask */	
 sysret_careful:
-	testl $(_TIF_SYSCALL_TRACE|_TIF_SYSCALL_AUDIT|_TIF_SECCOMP),%edx
+	testl $(_TIF_SYSCALL_TRACE|_TIF_SYSCALL_AUDIT|_TIF_KERNEL_TRACE|_TIF_SECCOMP),%edx
 	jnz ret_from_sys_call_trace
 	bt $TIF_NEED_RESCHED,%edx
 	jnc sysret_signal
@@ -1182,7 +1182,7 @@ bad_gs:
  * asm input arguments:
  *	rdi: fn, rsi: arg, rdx: flags
  */
-ENTRY(kernel_thread)
+ENTRY(kernel_thread_asm)
 	CFI_STARTPROC
 	FAKE_STACK_FRAME $child_rip
 	SAVE_ALL
diff --git a/arch/x86/kernel/process_32.c b/arch/x86/kernel/process_32.c
index 31f40b2..29a92de 100644
--- a/arch/x86/kernel/process_32.c
+++ b/arch/x86/kernel/process_32.c
@@ -37,6 +37,7 @@
 #include <linux/tick.h>
 #include <linux/percpu.h>
 #include <linux/prctl.h>
+#include <trace/sched.h>
 
 #include <asm/uaccess.h>
 #include <asm/pgtable.h>
@@ -233,6 +234,7 @@ extern void kernel_thread_helper(void);
 int kernel_thread(int (*fn)(void *), void * arg, unsigned long flags)
 {
 	struct pt_regs regs;
+	long pid;
 
 	memset(&regs, 0, sizeof(regs));
 
@@ -248,7 +250,10 @@ int kernel_thread(int (*fn)(void *), void * arg, unsigned long flags)
 	regs.flags = X86_EFLAGS_IF | X86_EFLAGS_SF | X86_EFLAGS_PF | 0x2;
 
 	/* Ok, create the new process.. */
-	return do_fork(flags | CLONE_VM | CLONE_UNTRACED, 0, &regs, 0, NULL, NULL);
+	pid = do_fork(flags | CLONE_VM | CLONE_UNTRACED,
+			0, &regs, 0, NULL, NULL);
+	trace_sched_kthread_create(fn, pid);
+	return pid;
 }
 EXPORT_SYMBOL(kernel_thread);
 
diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index e12e0e4..14dde16 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -37,6 +37,7 @@
 #include <linux/kdebug.h>
 #include <linux/tick.h>
 #include <linux/prctl.h>
+#include <trace/sched.h>
 
 #include <asm/uaccess.h>
 #include <asm/pgtable.h>
@@ -54,6 +55,9 @@
 
 asmlinkage extern void ret_from_fork(void);
 
+asmlinkage long kernel_thread_asm(int (*fn)(void *), void * arg,
+	 unsigned long flags);
+
 unsigned long kernel_thread_flags = CLONE_VM | CLONE_UNTRACED;
 
 static ATOMIC_NOTIFIER_HEAD(idle_notifier);
@@ -857,3 +861,11 @@ unsigned long arch_randomize_brk(struct mm_struct *mm)
 	unsigned long range_end = mm->brk + 0x02000000;
 	return randomize_range(mm->brk, range_end, 0) ? : mm->brk;
 }
+
+asmlinkage int kernel_thread(int (*fn)(void *), void * arg,
+	 unsigned long flags)
+{
+	int pid = kernel_thread_asm(fn, arg, flags);
+	trace_sched_kthread_create(fn, pid);
+	return pid;
+}
diff --git a/arch/x86/kernel/ptrace.c b/arch/x86/kernel/ptrace.c
index 52e049e..a4f4fed 100644
--- a/arch/x86/kernel/ptrace.c
+++ b/arch/x86/kernel/ptrace.c
@@ -21,6 +21,7 @@
 #include <linux/seccomp.h>
 #include <linux/signal.h>
 #include <linux/memrlimitcgroup.h>
+#include <trace/syscall.h>
 
 #include <asm/uaccess.h>
 #include <asm/pgtable.h>
@@ -1434,6 +1435,8 @@ asmregparm long syscall_trace_enter(struct pt_regs *regs)
 	if (test_thread_flag(TIF_SINGLESTEP))
 		regs->flags |= X86_EFLAGS_TF;
 
+	trace_syscall_entry(regs, regs->orig_ax);
+
 	/* do the secure computing check first */
 	secure_computing(regs->orig_ax);
 
@@ -1463,6 +1466,8 @@ asmregparm long syscall_trace_enter(struct pt_regs *regs)
 
 asmregparm void syscall_trace_leave(struct pt_regs *regs)
 {
+	trace_syscall_exit(regs->ax);
+
 	if (unlikely(current->audit_context))
 		audit_syscall_exit(AUDITSC_RESULT(regs->ax), regs->ax);
 
diff --git a/arch/x86/kernel/sys_i386_32.c b/arch/x86/kernel/sys_i386_32.c
index 7066cb8..c6d9f41 100644
--- a/arch/x86/kernel/sys_i386_32.c
+++ b/arch/x86/kernel/sys_i386_32.c
@@ -18,6 +18,7 @@
 #include <linux/file.h>
 #include <linux/utsname.h>
 #include <linux/ipc.h>
+#include <trace/ipc.h>
 
 #include <linux/uaccess.h>
 #include <linux/unistd.h>
@@ -111,6 +112,8 @@ asmlinkage int sys_ipc(uint call, int first, int second,
 	version = call >> 16; /* hack for backward compatibility */
 	call &= 0xffff;
 
+	trace_ipc_call(call, first);
+
 	switch (call) {
 	case SEMOP:
 		return sys_semtimedop(first, (struct sembuf __user *)ptr, second, NULL);
diff --git a/arch/x86/kernel/tlb_32.c b/arch/x86/kernel/tlb_32.c
index fec1ece..592b9c0 100644
--- a/arch/x86/kernel/tlb_32.c
+++ b/arch/x86/kernel/tlb_32.c
@@ -1,9 +1,11 @@
 #include <linux/spinlock.h>
 #include <linux/cpu.h>
 #include <linux/interrupt.h>
+#include <trace/irq.h>
 
 #include <asm/tlbflush.h>
 
+
 DEFINE_PER_CPU(struct tlb_state, cpu_tlbstate)
 			____cacheline_aligned = { &init_mm, 0, };
 
@@ -104,6 +106,8 @@ void smp_invalidate_interrupt(struct pt_regs *regs)
 		 * BUG();
 		 */
 
+	trace_irq_entry(INVALIDATE_TLB_VECTOR, regs);
+
 	if (flush_mm == per_cpu(cpu_tlbstate, cpu).active_mm) {
 		if (per_cpu(cpu_tlbstate, cpu).state == TLBSTATE_OK) {
 			if (flush_va == TLB_FLUSH_ALL)
@@ -120,6 +124,7 @@ void smp_invalidate_interrupt(struct pt_regs *regs)
 out:
 	put_cpu_no_resched();
 	__get_cpu_var(irq_stat).irq_tlb_count++;
+	trace_irq_exit(IRQ_HANDLED);
 }
 
 void native_flush_tlb_others(const cpumask_t *cpumaskp, struct mm_struct *mm,
diff --git a/arch/x86/kernel/tlb_64.c b/arch/x86/kernel/tlb_64.c
index dcbf7a1..91ed739 100644
--- a/arch/x86/kernel/tlb_64.c
+++ b/arch/x86/kernel/tlb_64.c
@@ -7,6 +7,7 @@
 #include <linux/kernel_stat.h>
 #include <linux/mc146818rtc.h>
 #include <linux/interrupt.h>
+#include <trace/irq.h>
 
 #include <asm/mtrr.h>
 #include <asm/pgalloc.h>
@@ -19,6 +20,7 @@
 #include <asm/uv/uv_bau.h>
 
 #include <mach_ipi.h>
+
 /*
  *	Smarter SMP flushing macros.
  *		c/o Linus Torvalds.
@@ -142,6 +144,8 @@ asmlinkage void smp_invalidate_interrupt(struct pt_regs *regs)
 		 * BUG();
 		 */
 
+	trace_irq_entry(sender, regs);
+
 	if (f->flush_mm == read_pda(active_mm)) {
 		if (read_pda(mmu_state) == TLBSTATE_OK) {
 			if (f->flush_va == TLB_FLUSH_ALL)
@@ -155,6 +159,7 @@ out:
 	ack_APIC_irq();
 	cpu_clear(cpu, f->flush_cpumask);
 	add_pda(irq_tlb_count, 1);
+	trace_irq_exit(IRQ_HANDLED);
 }
 
 void native_flush_tlb_others(const cpumask_t *cpumaskp, struct mm_struct *mm,
diff --git a/arch/x86/kernel/traps_32.c b/arch/x86/kernel/traps_32.c
index 3b07302..c797a5a 100644
--- a/arch/x86/kernel/traps_32.c
+++ b/arch/x86/kernel/traps_32.c
@@ -33,6 +33,7 @@
 #include <linux/nmi.h>
 #include <linux/mm.h>
 #include <linux/ltt-core.h>
+#include <trace/trap.h>
 
 #ifdef CONFIG_EISA
 #include <linux/ioport.h>
@@ -487,6 +488,8 @@ do_trap(int trapnr, int signr, char *str, int vm86, struct pt_regs *regs,
 {
 	struct task_struct *tsk = current;
 
+	trace_trap_entry(regs, trapnr);
+
 	if (regs->flags & X86_VM_MASK) {
 		if (vm86)
 			goto vm86_trap;
@@ -513,7 +516,7 @@ trap_signal:
 		force_sig_info(signr, info, tsk);
 	else
 		force_sig(signr, tsk);
-	return;
+	goto end;
 
 kernel_trap:
 	if (!fixup_exception(regs)) {
@@ -521,13 +524,14 @@ kernel_trap:
 		tsk->thread.trap_no = trapnr;
 		die(str, regs, error_code);
 	}
-	return;
+	goto end;
 
 vm86_trap:
 	if (handle_vm86_trap((struct kernel_vm86_regs *) regs,
 						error_code, trapnr))
 		goto trap_signal;
-	return;
+end:
+	trace_trap_exit();
 }
 
 #define DO_ERROR(trapnr, signr, str, name)				\
@@ -655,7 +659,9 @@ do_general_protection(struct pt_regs *regs, long error_code)
 		printk("\n");
 	}
 
+	trace_trap_entry(regs, 13);
 	force_sig(SIGSEGV, tsk);
+	trace_trap_exit();
 	return;
 
 gp_in_vm86:
@@ -793,27 +799,29 @@ static notrace __kprobes void default_do_nmi(struct pt_regs *regs)
 	if (!cpu)
 		reason = get_nmi_reason();
 
+	trace_trap_entry(regs, 2);
+
 	if (!(reason & 0xc0)) {
 		if (notify_die(DIE_NMI_IPI, "nmi_ipi", regs, reason, 2, SIGINT)
 								== NOTIFY_STOP)
-			return;
+			goto end;
 #ifdef CONFIG_X86_LOCAL_APIC
 		/*
 		 * Ok, so this is none of the documented NMI sources,
 		 * so it must be the NMI watchdog.
 		 */
 		if (nmi_watchdog_tick(regs, reason))
-			return;
+			goto end;
 		if (!do_nmi_callback(regs, cpu))
 			unknown_nmi_error(reason, regs);
 #else
 		unknown_nmi_error(reason, regs);
 #endif
 
-		return;
+		goto end;
 	}
 	if (notify_die(DIE_NMI, "nmi", regs, reason, 2, SIGINT) == NOTIFY_STOP)
-		return;
+		goto end;
 
 	/* AK: following checks seem to be broken on modern chipsets. FIXME */
 	if (reason & 0x80)
@@ -825,6 +833,8 @@ static notrace __kprobes void default_do_nmi(struct pt_regs *regs)
 	 * as it's edge-triggered:
 	 */
 	reassert_nmi();
+end:
+	trace_trap_exit();
 }
 
 notrace __kprobes void do_nmi(struct pt_regs *regs, long error_code)
@@ -944,7 +954,9 @@ void __kprobes do_debug(struct pt_regs *regs, long error_code)
 	}
 
 	/* Ok, finally something we can handle */
+	trace_trap_entry(regs, 1);
 	send_sigtrap(tsk, regs, error_code);
+	trace_trap_exit();
 
 	/*
 	 * Disable additional traps. They'll be re-enabled when
@@ -955,7 +967,9 @@ clear_dr7:
 	return;
 
 debug_vm86:
+	trace_trap_entry(regs, 1);
 	handle_vm86_trap((struct kernel_vm86_regs *) regs, error_code, 1);
+	trace_trap_exit();
 	return;
 
 clear_TF_reenable:
@@ -1106,10 +1120,12 @@ void do_simd_coprocessor_error(struct pt_regs *regs, long error_code)
 
 void do_spurious_interrupt_bug(struct pt_regs *regs, long error_code)
 {
+	trace_trap_entry(regs, 16);
 #if 0
 	/* No need to warn about this any longer. */
 	printk(KERN_INFO "Ignoring P6 Local APIC Spurious Interrupt Bug...\n");
 #endif
+	trace_trap_exit();
 }
 
 unsigned long patch_espfix_desc(unsigned long uesp, unsigned long kesp)
@@ -1175,8 +1191,10 @@ asmlinkage void math_emulate(long arg)
 	printk(KERN_EMERG
 		"math-emulation not enabled and no coprocessor found.\n");
 	printk(KERN_EMERG "killing %s.\n", current->comm);
+	trace_trap_entry(NULL, 7);
 	force_sig(SIGFPE, current);
 	schedule();
+	trace_trap_exit();
 }
 
 #endif /* CONFIG_MATH_EMULATION */
diff --git a/arch/x86/kernel/traps_64.c b/arch/x86/kernel/traps_64.c
index 4e0e1c9..c343479 100644
--- a/arch/x86/kernel/traps_64.c
+++ b/arch/x86/kernel/traps_64.c
@@ -32,6 +32,7 @@
 #include <linux/bug.h>
 #include <linux/nmi.h>
 #include <linux/mm.h>
+#include <trace/trap.h>
 
 #if defined(CONFIG_EDAC)
 #include <linux/edac.h>
@@ -601,6 +602,8 @@ do_trap(int trapnr, int signr, char *str, struct pt_regs *regs,
 {
 	struct task_struct *tsk = current;
 
+	trace_trap_entry(regs, trapnr);
+
 	if (!user_mode(regs))
 		goto kernel_trap;
 
@@ -630,6 +633,7 @@ do_trap(int trapnr, int signr, char *str, struct pt_regs *regs,
 		force_sig_info(signr, info, tsk);
 	else
 		force_sig(signr, tsk);
+	trace_trap_exit();
 	return;
 
 kernel_trap:
@@ -638,6 +642,7 @@ kernel_trap:
 		tsk->thread.trap_no = trapnr;
 		die(str, regs, error_code);
 	}
+	trace_trap_exit();
 	return;
 }
 
@@ -728,7 +733,9 @@ do_general_protection(struct pt_regs *regs, long error_code)
 		printk("\n");
 	}
 
+	trace_trap_entry(regs, 13);
 	force_sig(SIGSEGV, tsk);
+	trace_trap_exit();
 	return;
 
 gp_in_kernel:
@@ -803,6 +810,8 @@ asmlinkage notrace __kprobes void default_do_nmi(struct pt_regs *regs)
 	unsigned char reason = 0;
 	int cpu;
 
+	trace_trap_entry(regs, 2);
+
 	cpu = smp_processor_id();
 
 	/* Only the BSP gets external NMIs from the system. */
@@ -812,26 +821,28 @@ asmlinkage notrace __kprobes void default_do_nmi(struct pt_regs *regs)
 	if (!(reason & 0xc0)) {
 		if (notify_die(DIE_NMI_IPI, "nmi_ipi", regs, reason, 2, SIGINT)
 								== NOTIFY_STOP)
-			return;
+			goto end;
 		/*
 		 * Ok, so this is none of the documented NMI sources,
 		 * so it must be the NMI watchdog.
 		 */
 		if (nmi_watchdog_tick(regs, reason))
-			return;
+			goto end;
 		if (!do_nmi_callback(regs, cpu))
 			unknown_nmi_error(reason, regs);
 
-		return;
+		goto end;
 	}
 	if (notify_die(DIE_NMI, "nmi", regs, reason, 2, SIGINT) == NOTIFY_STOP)
-		return;
+		goto end;
 
 	/* AK: following checks seem to be broken on modern chipsets. FIXME */
 	if (reason & 0x80)
 		mem_parity_error(reason, regs);
 	if (reason & 0x40)
 		io_check_error(reason, regs);
+end:
+	trace_trap_exit();
 }
 
 asmlinkage notrace __kprobes void
@@ -942,7 +953,9 @@ asmlinkage void __kprobes do_debug(struct pt_regs * regs,
 	info.si_errno = 0;
 	info.si_code = TRAP_BRKPT;
 	info.si_addr = user_mode(regs) ? (void __user *)regs->ip : NULL;
+	trace_trap_entry(regs, 1);
 	force_sig_info(SIGTRAP, &info, tsk);
+	trace_trap_exit();
 
 clear_dr7:
 	set_debugreg(0, 7);
@@ -1098,6 +1111,9 @@ asmlinkage void do_simd_coprocessor_error(struct pt_regs *regs)
 
 asmlinkage void do_spurious_interrupt_bug(struct pt_regs * regs)
 {
+	trace_trap_entry(regs, 16);
+
+	trace_trap_exit();
 }
 
 asmlinkage void __attribute__((weak)) smp_thermal_interrupt(void)
diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c
index ac4c918..914ae9b 100644
--- a/arch/x86/mm/fault.c
+++ b/arch/x86/mm/fault.c
@@ -26,6 +26,7 @@
 #include <linux/kprobes.h>
 #include <linux/uaccess.h>
 #include <linux/kdebug.h>
+#include <trace/trap.h>
 
 #include <asm/system.h>
 #include <asm/desc.h>
@@ -753,7 +754,9 @@ survive:
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
+	trace_trap_entry(regs, 14);
 	fault = handle_mm_fault(mm, vma, address, write);
+	trace_trap_exit();
 	if (unlikely(fault & VM_FAULT_ERROR)) {
 		if (fault & VM_FAULT_OOM)
 			goto out_of_memory;
@@ -794,6 +797,8 @@ bad_area_nosemaphore:
 		 */
 		local_irq_enable();
 
+		trace_trap_entry(regs, 14);
+
 		/*
 		 * Valid to do another page fault here because this one came
 		 * from user space.
@@ -820,6 +825,7 @@ bad_area_nosemaphore:
 		tsk->thread.error_code = error_code | (address >= TASK_SIZE);
 		tsk->thread.trap_no = 14;
 		force_sig_info_fault(SIGSEGV, si_code, address, tsk);
+		trace_trap_exit();
 		return;
 	}
 
-- 
1.5.5.1

