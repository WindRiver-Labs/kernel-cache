From f3519cf96076028e6a4f464f75816d023df4078e Mon Sep 17 00:00:00 2001
From: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
Date: Thu, 13 May 2010 19:27:07 -0400
Subject: [PATCH 206/390] lttng-call-for-slowpath

ltt relay lockless: call separate functions for slowpath

- Diminishes stack usage, register pressure and instruction cache pressure.

Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
---
 ltt/ltt-relay-lockless.c |  545 +++++++++++++++++++++++++++++++++++++++++++++
 ltt/ltt-relay-lockless.h |  548 +++++-----------------------------------------
 2 files changed, 600 insertions(+), 493 deletions(-)

diff --git a/ltt/ltt-relay-lockless.c b/ltt/ltt-relay-lockless.c
index 2ffb16a..29b9624 100644
--- a/ltt/ltt-relay-lockless.c
+++ b/ltt/ltt-relay-lockless.c
@@ -64,6 +64,13 @@
 #define printk_dbg(fmt, args...)
 #endif
 
+struct ltt_reserve_switch_offsets {
+	long begin, end, old;
+	long begin_switch, end_switch_current, end_switch_old;
+	long commit_count, reserve_commit_diff;
+	size_t before_hdr_pad, size;
+};
+
 static int ltt_relay_create_buffer(struct ltt_trace_struct *trace,
 		struct ltt_channel_struct *ltt_chan,
 		struct rchan_buf *buf,
@@ -78,6 +85,42 @@ static void ltt_force_switch(struct rchan_buf *buf,
 
 static const struct file_operations ltt_file_operations;
 
+static void ltt_buffer_begin(struct rchan_buf *buf,
+			u64 tsc, unsigned int subbuf_idx)
+{
+	struct ltt_channel_struct *channel =
+		(struct ltt_channel_struct *)buf->chan->private_data;
+	struct ltt_subbuffer_header *header =
+		(struct ltt_subbuffer_header *)
+			ltt_relay_offset_address(buf,
+				subbuf_idx * buf->chan->subbuf_size);
+
+	header->cycle_count_begin = tsc;
+	header->lost_size = 0xFFFFFFFF; /* for debugging */
+	header->buf_size = buf->chan->subbuf_size;
+	ltt_write_trace_header(channel->trace, header);
+}
+
+/*
+ * offset is assumed to never be 0 here : never deliver a completely empty
+ * subbuffer. The lost size is between 0 and subbuf_size-1.
+ */
+static void ltt_buffer_end(struct rchan_buf *buf,
+		u64 tsc, unsigned int offset, unsigned int subbuf_idx)
+{
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+	struct ltt_subbuffer_header *header =
+		(struct ltt_subbuffer_header *)
+			ltt_relay_offset_address(buf,
+				subbuf_idx * buf->chan->subbuf_size);
+
+	header->lost_size = SUBBUF_OFFSET((buf->chan->subbuf_size - offset),
+				buf->chan);
+	header->cycle_count_end = tsc;
+	header->events_lost = local_read(&ltt_buf->events_lost);
+	header->subbuf_corrupt = local_read(&ltt_buf->corrupted_subbuffers);
+}
+
 static struct dentry *ltt_create_buf_file_callback(const char *filename,
 		struct dentry *parent, int mode,
 		struct rchan_buf *buf)
@@ -1027,6 +1070,508 @@ static void ltt_relay_print_user_errors(struct ltt_trace_struct *trace,
 			dbg->write, dbg->read);
 }
 
+static void ltt_reserve_push_reader(
+		struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf,
+		struct rchan *rchan,
+		struct rchan_buf *buf,
+		struct ltt_reserve_switch_offsets *offsets)
+{
+	long consumed_old, consumed_new;
+
+	do {
+		consumed_old = atomic_long_read(&ltt_buf->consumed);
+		/*
+		 * If buffer is in overwrite mode, push the reader consumed
+		 * count if the write position has reached it and we are not
+		 * at the first iteration (don't push the reader farther than
+		 * the writer). This operation can be done concurrently by many
+		 * writers in the same buffer, the writer being at the farthest
+		 * write position sub-buffer index in the buffer being the one
+		 * which will win this loop.
+		 * If the buffer is not in overwrite mode, pushing the reader
+		 * only happens if a sub-buffer is corrupted.
+		 */
+		if (unlikely((SUBBUF_TRUNC(offsets->end-1, buf->chan)
+		   - SUBBUF_TRUNC(consumed_old, buf->chan))
+		   >= rchan->alloc_size))
+			consumed_new = SUBBUF_ALIGN(consumed_old, buf->chan);
+		else
+			return;
+	} while (unlikely(atomic_long_cmpxchg(&ltt_buf->consumed, consumed_old,
+			consumed_new) != consumed_old));
+
+	if (unlikely(consumed_old != consumed_new)) {
+		/*
+		 * Reader pushed : we are the winner of the push, we can
+		 * therefore reequilibrate reserve and commit. Atomic increment
+		 * of the commit count permits other writers to play around
+		 * with this variable before us. We keep track of
+		 * corrupted_subbuffers even in overwrite mode :
+		 * we never want to write over a non completely committed
+		 * sub-buffer : possible causes : the buffer size is too low
+		 * compared to the unordered data input, or there is a writer
+		 * that died between the reserve and the commit.
+		 */
+		if (likely(offsets->reserve_commit_diff)) {
+			/*
+			 * We have to alter the sub-buffer commit count.
+			 * We do not deliver the previous subbuffer, given it
+			 * was either corrupted or not consumed (overwrite
+			 * mode).
+			 */
+			local_add(offsets->reserve_commit_diff,
+				  &ltt_buf->commit_count[
+					SUBBUF_INDEX(offsets->begin,
+						     buf->chan)]);
+			if (unlikely(!ltt_channel->overwrite
+			    || offsets->reserve_commit_diff
+			       != rchan->subbuf_size)) {
+				/*
+				 * The reserve commit diff was not subbuf_size :
+				 * it means the subbuffer was partly written to
+				 * and is therefore corrupted. If it is multiple
+				 * of subbuffer size and we are in flight
+				 * recorder mode, we are skipping over a whole
+				 * subbuffer.
+				 */
+				local_inc(&ltt_buf->corrupted_subbuffers);
+			}
+		}
+	}
+}
+
+
+/*
+ * ltt_reserve_switch_old_subbuf: switch old subbuffer
+ *
+ * Concurrency safe because we are the last and only thread to alter this
+ * sub-buffer. As long as it is not delivered and read, no other thread can
+ * alter the offset, alter the reserve_count or call the
+ * client_buffer_end_callback on this sub-buffer.
+ *
+ * The only remaining threads could be the ones with pending commits. They will
+ * have to do the deliver themselves.  Not concurrency safe in overwrite mode.
+ * We detect corrupted subbuffers with commit and reserve counts. We keep a
+ * corrupted sub-buffers count and push the readers across these sub-buffers.
+ *
+ * Not concurrency safe if a writer is stalled in a subbuffer and another writer
+ * switches in, finding out it's corrupted.  The result will be than the old
+ * (uncommited) subbuffer will be declared corrupted, and that the new subbuffer
+ * will be declared corrupted too because of the commit count adjustment.
+ *
+ * Note : offset_old should never be 0 here.
+ */
+static void ltt_reserve_switch_old_subbuf(
+		struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
+		struct rchan_buf *buf,
+		struct ltt_reserve_switch_offsets *offsets, u64 *tsc)
+{
+	long oldidx = SUBBUF_INDEX(offsets->old - 1, rchan);
+
+	ltt_buffer_end(buf, *tsc, offsets->old, oldidx);
+	/* Must write buffer end before incrementing commit count */
+	smp_wmb();
+	offsets->commit_count =
+		local_add_return(rchan->subbuf_size
+				 - (SUBBUF_OFFSET(offsets->old - 1, rchan)
+				 + 1),
+				 &ltt_buf->commit_count[oldidx]);
+	if (likely((BUFFER_TRUNC(offsets->old - 1, rchan)
+			>> ltt_channel->n_subbufs_order)
+			- ((offsets->commit_count - rchan->subbuf_size)
+				& ltt_channel->commit_count_mask) == 0))
+		ltt_deliver(buf, oldidx, NULL);
+}
+
+/*
+ * ltt_reserve_switch_new_subbuf: Populate new subbuffer.
+ *
+ * This code can be executed unordered : writers may already have written to the
+ * sub-buffer before this code gets executed, caution.  The commit makes sure
+ * that this code is executed before the deliver of this sub-buffer.
+ */
+static void ltt_reserve_switch_new_subbuf(
+		struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
+		struct rchan_buf *buf,
+		struct ltt_reserve_switch_offsets *offsets, u64 *tsc)
+{
+	long beginidx = SUBBUF_INDEX(offsets->begin, rchan);
+
+	ltt_buffer_begin(buf, *tsc, beginidx);
+	/* Must write buffer end before incrementing commit count */
+	smp_wmb();
+	offsets->commit_count = local_add_return(ltt_subbuffer_header_size(),
+			&ltt_buf->commit_count[beginidx]);
+	/* Check if the written buffer has to be delivered */
+	if (unlikely((BUFFER_TRUNC(offsets->begin, rchan)
+			>> ltt_channel->n_subbufs_order)
+			- ((offsets->commit_count - rchan->subbuf_size)
+				& ltt_channel->commit_count_mask) == 0))
+		ltt_deliver(buf, beginidx, NULL);
+}
+
+
+/*
+ * ltt_reserve_end_switch_current: finish switching current subbuffer
+ *
+ * Concurrency safe because we are the last and only thread to alter this
+ * sub-buffer. As long as it is not delivered and read, no other thread can
+ * alter the offset, alter the reserve_count or call the
+ * client_buffer_end_callback on this sub-buffer.
+ *
+ * The only remaining threads could be the ones with pending commits. They will
+ * have to do the deliver themselves.  Not concurrency safe in overwrite mode.
+ * We detect corrupted subbuffers with commit and reserve counts. We keep a
+ * corrupted sub-buffers count and push the readers across these sub-buffers.
+ *
+ * Not concurrency safe if a writer is stalled in a subbuffer and another writer
+ * switches in, finding out it's corrupted.  The result will be than the old
+ * (uncommited) subbuffer will be declared corrupted, and that the new subbuffer
+ * will be declared corrupted too because of the commit count adjustment.
+ */
+static void ltt_reserve_end_switch_current(
+		struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
+		struct rchan_buf *buf,
+		struct ltt_reserve_switch_offsets *offsets, u64 *tsc)
+{
+	long endidx = SUBBUF_INDEX(offsets->end - 1, rchan);
+
+	ltt_buffer_end(buf, *tsc, offsets->end, endidx);
+	/* Must write buffer begin before incrementing commit count */
+	smp_wmb();
+	offsets->commit_count =
+		local_add_return(rchan->subbuf_size
+				 - (SUBBUF_OFFSET(offsets->end - 1, rchan)
+				 + 1),
+				 &ltt_buf->commit_count[endidx]);
+	if (likely((BUFFER_TRUNC(offsets->end - 1, rchan)
+			>> ltt_channel->n_subbufs_order)
+			- ((offsets->commit_count - rchan->subbuf_size)
+				& ltt_channel->commit_count_mask) == 0))
+		ltt_deliver(buf, endidx, NULL);
+}
+
+/*
+ * Returns :
+ * 0 if ok
+ * !0 if execution must be aborted.
+ */
+static int ltt_relay_try_switch_slow(
+		enum force_switch_mode mode,
+		struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
+		struct rchan_buf *buf,
+		struct ltt_reserve_switch_offsets *offsets,
+		u64 *tsc)
+{
+	long subbuf_index;
+
+	offsets->begin = local_read(&ltt_buf->offset);
+	offsets->old = offsets->begin;
+	offsets->begin_switch = 0;
+	offsets->end_switch_old = 0;
+
+	*tsc = trace_clock_read64();
+
+	if (SUBBUF_OFFSET(offsets->begin, buf->chan) != 0) {
+		offsets->begin = SUBBUF_ALIGN(offsets->begin, buf->chan);
+		offsets->end_switch_old = 1;
+	} else {
+		/* we do not have to switch : buffer is empty */
+		return -1;
+	}
+	if (mode == FORCE_ACTIVE)
+		offsets->begin += ltt_subbuffer_header_size();
+	/*
+	 * Always begin_switch in FORCE_ACTIVE mode.
+	 * Test new buffer integrity
+	 */
+	subbuf_index = SUBBUF_INDEX(offsets->begin, buf->chan);
+	offsets->reserve_commit_diff =
+		(BUFFER_TRUNC(offsets->begin, buf->chan)
+		 >> ltt_channel->n_subbufs_order)
+		- (local_read(&ltt_buf->commit_count[subbuf_index])
+			& ltt_channel->commit_count_mask);
+	if (offsets->reserve_commit_diff == 0) {
+		/* Next buffer not corrupted. */
+		if (mode == FORCE_ACTIVE
+		    && !ltt_channel->overwrite
+		    && offsets->begin - atomic_long_read(&ltt_buf->consumed)
+		       >= rchan->alloc_size) {
+			/*
+			 * We do not overwrite non consumed buffers and we are
+			 * full : ignore switch while tracing is active.
+			 */
+			return -1;
+		}
+	} else {
+		/*
+		 * Next subbuffer corrupted. Force pushing reader even in normal
+		 * mode
+		 */
+	}
+	offsets->end = offsets->begin;
+	return 0;
+}
+
+/*
+ * Force a sub-buffer switch for a per-cpu buffer. This operation is
+ * completely reentrant : can be called while tracing is active with
+ * absolutely no lock held.
+ *
+ * Note, however, that as a local_cmpxchg is used for some atomic
+ * operations, this function must be called from the CPU which owns the buffer
+ * for a ACTIVE flush.
+ */
+void ltt_force_switch_lockless_slow(struct rchan_buf *buf,
+		enum force_switch_mode mode)
+{
+	struct ltt_channel_struct *ltt_channel =
+			(struct ltt_channel_struct *)buf->chan->private_data;
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+	struct rchan *rchan = ltt_channel->trans_channel_data;
+	struct ltt_reserve_switch_offsets offsets;
+	u64 tsc;
+
+	offsets.reserve_commit_diff = 0;
+	offsets.size = 0;
+
+	/*
+	 * Perform retryable operations.
+	 */
+	do {
+		if (ltt_relay_try_switch_slow(mode, ltt_channel, ltt_buf,
+				rchan, buf, &offsets, &tsc))
+			return;
+	} while (local_cmpxchg(&ltt_buf->offset, offsets.old,
+			offsets.end) != offsets.old);
+
+	/*
+	 * Atomically update last_tsc. This update races against concurrent
+	 * atomic updates, but the race will always cause supplementary full TSC
+	 * events, never the opposite (missing a full TSC event when it would be
+	 * needed).
+	 */
+	save_last_tsc(ltt_buf, tsc);
+
+	/*
+	 * Push the reader if necessary
+	 */
+	if (mode == FORCE_ACTIVE)
+		ltt_reserve_push_reader(ltt_channel, ltt_buf, rchan,
+					buf, &offsets);
+
+	/*
+	 * Switch old subbuffer if needed.
+	 */
+	if (offsets.end_switch_old)
+		ltt_reserve_switch_old_subbuf(ltt_channel, ltt_buf, rchan, buf,
+			&offsets, &tsc);
+
+	/*
+	 * Populate new subbuffer.
+	 */
+	if (mode == FORCE_ACTIVE)
+		ltt_reserve_switch_new_subbuf(ltt_channel,
+			ltt_buf, rchan, buf, &offsets, &tsc);
+}
+EXPORT_SYMBOL_GPL(ltt_force_switch_lockless_slow);
+
+/*
+ * Returns :
+ * 0 if ok
+ * !0 if execution must be aborted.
+ */
+static int ltt_relay_try_reserve_slow(struct ltt_channel_struct *ltt_channel,
+		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
+		struct rchan_buf *buf,
+		struct ltt_reserve_switch_offsets *offsets, size_t data_size,
+		u64 *tsc, unsigned int *rflags, int largest_align)
+{
+	offsets->begin = local_read(&ltt_buf->offset);
+	offsets->old = offsets->begin;
+	offsets->begin_switch = 0;
+	offsets->end_switch_current = 0;
+	offsets->end_switch_old = 0;
+
+	*tsc = trace_clock_read64();
+	if (last_tsc_overflow(ltt_buf, *tsc))
+		*rflags = LTT_RFLAG_ID_SIZE_TSC;
+
+	if (unlikely(SUBBUF_OFFSET(offsets->begin, buf->chan) == 0)) {
+		offsets->begin_switch = 1;		/* For offsets->begin */
+	} else {
+		offsets->size = ltt_get_header_size(ltt_channel,
+					offsets->begin, data_size,
+					&offsets->before_hdr_pad, *rflags);
+		offsets->size += ltt_align(offsets->begin + offsets->size,
+					   largest_align)
+				 + data_size;
+		if (unlikely((SUBBUF_OFFSET(offsets->begin, buf->chan) +
+			     offsets->size) > buf->chan->subbuf_size)) {
+			offsets->end_switch_old = 1;	/* For offsets->old */
+			offsets->begin_switch = 1;	/* For offsets->begin */
+		}
+	}
+	if (unlikely(offsets->begin_switch)) {
+		long subbuf_index;
+
+		/*
+		 * We are typically not filling the previous buffer completely.
+		 */
+		if (likely(offsets->end_switch_old))
+			offsets->begin = SUBBUF_ALIGN(offsets->begin,
+						      buf->chan);
+		offsets->begin = offsets->begin + ltt_subbuffer_header_size();
+		/* Test new buffer integrity */
+		subbuf_index = SUBBUF_INDEX(offsets->begin, buf->chan);
+		offsets->reserve_commit_diff =
+			(BUFFER_TRUNC(offsets->begin, buf->chan)
+			 >> ltt_channel->n_subbufs_order)
+			- (local_read(&ltt_buf->commit_count[subbuf_index])
+				& ltt_channel->commit_count_mask);
+		if (likely(offsets->reserve_commit_diff == 0)) {
+			/* Next buffer not corrupted. */
+			if (unlikely(!ltt_channel->overwrite &&
+				(SUBBUF_TRUNC(offsets->begin, buf->chan)
+				 - SUBBUF_TRUNC(atomic_long_read(
+							&ltt_buf->consumed),
+						buf->chan))
+				>= rchan->alloc_size)) {
+				/*
+				 * We do not overwrite non consumed buffers
+				 * and we are full : event is lost.
+				 */
+				local_inc(&ltt_buf->events_lost);
+				return -1;
+			} else {
+				/*
+				 * next buffer not corrupted, we are either in
+				 * overwrite mode or the buffer is not full.
+				 * It's safe to write in this new subbuffer.
+				 */
+			}
+		} else {
+			/*
+			 * Next subbuffer corrupted. Force pushing reader even
+			 * in normal mode. It's safe to write in this new
+			 * subbuffer.
+			 */
+		}
+		offsets->size = ltt_get_header_size(ltt_channel,
+					offsets->begin, data_size,
+					&offsets->before_hdr_pad, *rflags);
+		offsets->size += ltt_align(offsets->begin + offsets->size,
+					   largest_align)
+				 + data_size;
+		if (unlikely((SUBBUF_OFFSET(offsets->begin, buf->chan)
+			     + offsets->size) > buf->chan->subbuf_size)) {
+			/*
+			 * Event too big for subbuffers, report error, don't
+			 * complete the sub-buffer switch.
+			 */
+			local_inc(&ltt_buf->events_lost);
+			return -1;
+		} else {
+			/*
+			 * We just made a successful buffer switch and the event
+			 * fits in the new subbuffer. Let's write.
+			 */
+		}
+	} else {
+		/*
+		 * Event fits in the current buffer and we are not on a switch
+		 * boundary. It's safe to write.
+		 */
+	}
+	offsets->end = offsets->begin + offsets->size;
+
+	if (unlikely((SUBBUF_OFFSET(offsets->end, buf->chan)) == 0)) {
+		/*
+		 * The offset_end will fall at the very beginning of the next
+		 * subbuffer.
+		 */
+		offsets->end_switch_current = 1;	/* For offsets->begin */
+	}
+	return 0;
+}
+
+/**
+ * ltt_relay_reserve_slot_lockless_slow - Atomic slot reservation in a buffer.
+ * @trace: the trace structure to log to.
+ * @ltt_channel: channel structure
+ * @transport_data: data structure specific to ltt relay
+ * @data_size: size of the variable length data to log.
+ * @slot_size: pointer to total size of the slot (out)
+ * @buf_offset : pointer to reserved buffer offset (out)
+ * @tsc: pointer to the tsc at the slot reservation (out)
+ * @cpu: cpuid
+ *
+ * Return : -ENOSPC if not enough space, else returns 0.
+ * It will take care of sub-buffer switching.
+ */
+int ltt_reserve_slot_lockless_slow(struct ltt_trace_struct *trace,
+		struct ltt_channel_struct *ltt_channel, void **transport_data,
+		size_t data_size, size_t *slot_size, long *buf_offset, u64 *tsc,
+		unsigned int *rflags, int largest_align, int cpu)
+{
+	struct rchan *rchan = ltt_channel->trans_channel_data;
+	struct rchan_buf *buf = *transport_data = rchan->buf[cpu];
+	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
+	struct ltt_reserve_switch_offsets offsets;
+
+	offsets.reserve_commit_diff = 0;
+	offsets.size = 0;
+
+	do {
+		if (unlikely(ltt_relay_try_reserve_slow(ltt_channel, ltt_buf,
+				rchan, buf, &offsets, data_size, tsc, rflags,
+				largest_align)))
+			return -ENOSPC;
+	} while (unlikely(local_cmpxchg(&ltt_buf->offset, offsets.old,
+			offsets.end) != offsets.old));
+
+	/*
+	 * Atomically update last_tsc. This update races against concurrent
+	 * atomic updates, but the race will always cause supplementary full TSC
+	 * events, never the opposite (missing a full TSC event when it would be
+	 * needed).
+	 */
+	save_last_tsc(ltt_buf, *tsc);
+
+	/*
+	 * Push the reader if necessary
+	 */
+	ltt_reserve_push_reader(ltt_channel, ltt_buf, rchan, buf, &offsets);
+
+	/*
+	 * Switch old subbuffer if needed.
+	 */
+	if (unlikely(offsets.end_switch_old))
+		ltt_reserve_switch_old_subbuf(ltt_channel, ltt_buf, rchan, buf,
+			&offsets, tsc);
+
+	/*
+	 * Populate new subbuffer.
+	 */
+	if (unlikely(offsets.begin_switch))
+		ltt_reserve_switch_new_subbuf(ltt_channel, ltt_buf, rchan,
+			buf, &offsets, tsc);
+
+	if (unlikely(offsets.end_switch_current))
+		ltt_reserve_end_switch_current(ltt_channel, ltt_buf, rchan,
+			buf, &offsets, tsc);
+
+	*slot_size = offsets.size;
+	*buf_offset = offsets.begin + offsets.before_hdr_pad;
+	return 0;
+}
+EXPORT_SYMBOL_GPL(ltt_reserve_slot_lockless_slow);
+
 static struct ltt_transport ltt_relay_transport = {
 	.name = "relay",
 	.owner = THIS_MODULE,
diff --git a/ltt/ltt-relay-lockless.h b/ltt/ltt-relay-lockless.h
index 639c53a..15875ac 100644
--- a/ltt/ltt-relay-lockless.h
+++ b/ltt/ltt-relay-lockless.h
@@ -102,6 +102,20 @@ struct ltt_channel_buf_struct {
 } ____cacheline_aligned;
 
 /*
+ * A switch is done during tracing or as a final flush after tracing (so it
+ * won't write in the new sub-buffer).
+ */
+enum force_switch_mode { FORCE_ACTIVE, FORCE_FLUSH };
+
+extern int ltt_reserve_slot_lockless_slow(struct ltt_trace_struct *trace,
+		struct ltt_channel_struct *ltt_channel, void **transport_data,
+		size_t data_size, size_t *slot_size, long *buf_offset, u64 *tsc,
+		unsigned int *rflags, int largest_align, int cpu);
+
+extern void ltt_force_switch_lockless_slow(struct rchan_buf *buf,
+		enum force_switch_mode mode);
+
+/*
  * Last TSC comparison functions. Check if the current TSC overflows
  * LTT_TSC_BITS bits from the last TSC read. Reads and writes last_tsc
  * atomically.
@@ -141,48 +155,6 @@ static __inline__ int last_tsc_overflow(struct ltt_channel_buf_struct *ltt_buf,
 }
 #endif
 
-/*
- * A switch is done during tracing or as a final flush after tracing (so it
- * won't write in the new sub-buffer).
- */
-enum force_switch_mode { FORCE_ACTIVE, FORCE_FLUSH };
-
-static __inline__ void ltt_buffer_begin(struct rchan_buf *buf,
-			u64 tsc, unsigned int subbuf_idx)
-{
-	struct ltt_channel_struct *channel =
-		(struct ltt_channel_struct *)buf->chan->private_data;
-	struct ltt_subbuffer_header *header =
-		(struct ltt_subbuffer_header *)
-			ltt_relay_offset_address(buf,
-				subbuf_idx * buf->chan->subbuf_size);
-
-	header->cycle_count_begin = tsc;
-	header->lost_size = 0xFFFFFFFF; /* for debugging */
-	header->buf_size = buf->chan->subbuf_size;
-	ltt_write_trace_header(channel->trace, header);
-}
-
-/*
- * offset is assumed to never be 0 here : never deliver a completely empty
- * subbuffer. The lost size is between 0 and subbuf_size-1.
- */
-static __inline__ void ltt_buffer_end(struct rchan_buf *buf,
-		u64 tsc, unsigned int offset, unsigned int subbuf_idx)
-{
-	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
-	struct ltt_subbuffer_header *header =
-		(struct ltt_subbuffer_header *)
-			ltt_relay_offset_address(buf,
-				subbuf_idx * buf->chan->subbuf_size);
-
-	header->lost_size = SUBBUF_OFFSET((buf->chan->subbuf_size - offset),
-				buf->chan);
-	header->cycle_count_end = tsc;
-	header->events_lost = local_read(&ltt_buf->events_lost);
-	header->subbuf_corrupt = local_read(&ltt_buf->corrupted_subbuffers);
-}
-
 static __inline__ void ltt_deliver(struct rchan_buf *buf, unsigned int subbuf_idx,
 		void *subbuf)
 {
@@ -191,395 +163,52 @@ static __inline__ void ltt_deliver(struct rchan_buf *buf, unsigned int subbuf_id
 	atomic_set(&ltt_buf->wakeup_readers, 1);
 }
 
-struct ltt_reserve_switch_offsets {
-	long begin, end, old;
-	long begin_switch, end_switch_current, end_switch_old;
-	long commit_count, reserve_commit_diff;
-	size_t before_hdr_pad, size;
-};
-
 /*
- * Returns :
- * 0 if ok
- * !0 if execution must be aborted.
+ * returns 0 if reserve ok, or 1 if the slow path must be taken.
  */
 static __inline__ int ltt_relay_try_reserve(
 		struct ltt_channel_struct *ltt_channel,
 		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
 		struct rchan_buf *buf,
-		struct ltt_reserve_switch_offsets *offsets, size_t data_size,
-		u64 *tsc, unsigned int *rflags, int largest_align)
+		size_t data_size,
+		u64 *tsc, unsigned int *rflags, int largest_align,
+		long *o_begin, long *o_end, long *o_old,
+		long *before_hdr_pad, long *size)
 {
-	offsets->begin = local_read(&ltt_buf->offset);
-	offsets->old = offsets->begin;
-	offsets->begin_switch = 0;
-	offsets->end_switch_current = 0;
-	offsets->end_switch_old = 0;
+	*o_begin = local_read(&ltt_buf->offset);
+	*o_old = *o_begin;
 
 	*tsc = trace_clock_read64();
 	if (last_tsc_overflow(ltt_buf, *tsc))
 		*rflags = LTT_RFLAG_ID_SIZE_TSC;
 
-	if (unlikely(SUBBUF_OFFSET(offsets->begin, buf->chan) == 0)) {
-		offsets->begin_switch = 1;		/* For offsets->begin */
-	} else {
-		offsets->size = ltt_get_header_size(ltt_channel,
-					offsets->begin, data_size,
-					&offsets->before_hdr_pad, *rflags);
-		offsets->size += ltt_align(offsets->begin + offsets->size,
-					   largest_align)
-				 + data_size;
-		if (unlikely((SUBBUF_OFFSET(offsets->begin, buf->chan) +
-			     offsets->size) > buf->chan->subbuf_size)) {
-			offsets->end_switch_old = 1;	/* For offsets->old */
-			offsets->begin_switch = 1;	/* For offsets->begin */
-		}
-	}
-	if (unlikely(offsets->begin_switch)) {
-		long subbuf_index;
-
-		/*
-		 * We are typically not filling the previous buffer completely.
-		 */
-		if (likely(offsets->end_switch_old))
-			offsets->begin = SUBBUF_ALIGN(offsets->begin,
-						      buf->chan);
-		offsets->begin = offsets->begin + ltt_subbuffer_header_size();
-		/* Test new buffer integrity */
-		subbuf_index = SUBBUF_INDEX(offsets->begin, buf->chan);
-		offsets->reserve_commit_diff =
-			(BUFFER_TRUNC(offsets->begin, buf->chan)
-			 >> ltt_channel->n_subbufs_order)
-			- (local_read(&ltt_buf->commit_count[subbuf_index])
-				& ltt_channel->commit_count_mask);
-		if (likely(offsets->reserve_commit_diff == 0)) {
-			/* Next buffer not corrupted. */
-			if (unlikely(!ltt_channel->overwrite &&
-				(SUBBUF_TRUNC(offsets->begin, buf->chan)
-				 - SUBBUF_TRUNC(atomic_long_read(
-							&ltt_buf->consumed),
-						buf->chan))
-				>= rchan->alloc_size)) {
-				/*
-				 * We do not overwrite non consumed buffers
-				 * and we are full : event is lost.
-				 */
-				local_inc(&ltt_buf->events_lost);
-				return -1;
-			} else {
-				/*
-				 * next buffer not corrupted, we are either in
-				 * overwrite mode or the buffer is not full.
-				 * It's safe to write in this new subbuffer.
-				 */
-			}
-		} else {
-			/*
-			 * Next subbuffer corrupted. Force pushing reader even
-			 * in normal mode. It's safe to write in this new
-			 * subbuffer.
-			 */
-		}
-		offsets->size = ltt_get_header_size(ltt_channel,
-					offsets->begin, data_size,
-					&offsets->before_hdr_pad, *rflags);
-		offsets->size += ltt_align(offsets->begin + offsets->size,
-					   largest_align)
-				 + data_size;
-		if (unlikely((SUBBUF_OFFSET(offsets->begin, buf->chan)
-			     + offsets->size) > buf->chan->subbuf_size)) {
-			/*
-			 * Event too big for subbuffers, report error, don't
-			 * complete the sub-buffer switch.
-			 */
-			local_inc(&ltt_buf->events_lost);
-			return -1;
-		} else {
-			/*
-			 * We just made a successful buffer switch and the event
-			 * fits in the new subbuffer. Let's write.
-			 */
-		}
-	} else {
-		/*
-		 * Event fits in the current buffer and we are not on a switch
-		 * boundary. It's safe to write.
-		 */
-	}
-	offsets->end = offsets->begin + offsets->size;
-
-	if (unlikely((SUBBUF_OFFSET(offsets->end, buf->chan)) == 0)) {
-		/*
-		 * The offset_end will fall at the very beginning of the next
-		 * subbuffer.
-		 */
-		offsets->end_switch_current = 1;	/* For offsets->begin */
-	}
-	return 0;
-}
-
-/*
- * Returns :
- * 0 if ok
- * !0 if execution must be aborted.
- */
-static __inline__ int ltt_relay_try_switch(
-		enum force_switch_mode mode,
-		struct ltt_channel_struct *ltt_channel,
-		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
-		struct rchan_buf *buf,
-		struct ltt_reserve_switch_offsets *offsets,
-		u64 *tsc)
-{
-	long subbuf_index;
-
-	offsets->begin = local_read(&ltt_buf->offset);
-	offsets->old = offsets->begin;
-	offsets->begin_switch = 0;
-	offsets->end_switch_old = 0;
+	if (unlikely(SUBBUF_OFFSET(*o_begin, buf->chan) == 0))
+		return 1;
 
-	*tsc = trace_clock_read64();
+	*size = ltt_get_header_size(ltt_channel,
+				*o_begin, data_size,
+				before_hdr_pad, *rflags);
+	*size += ltt_align(*o_begin + *size, largest_align) + data_size;
+	if (unlikely((SUBBUF_OFFSET(*o_begin, buf->chan) + *size)
+		     > buf->chan->subbuf_size))
+		return 1;
 
-	if (SUBBUF_OFFSET(offsets->begin, buf->chan) != 0) {
-		offsets->begin = SUBBUF_ALIGN(offsets->begin, buf->chan);
-		offsets->end_switch_old = 1;
-	} else {
-		/* we do not have to switch : buffer is empty */
-		return -1;
-	}
-	if (mode == FORCE_ACTIVE)
-		offsets->begin += ltt_subbuffer_header_size();
 	/*
-	 * Always begin_switch in FORCE_ACTIVE mode.
-	 * Test new buffer integrity
+	 * Event fits in the current buffer and we are not on a switch
+	 * boundary. It's safe to write.
 	 */
-	subbuf_index = SUBBUF_INDEX(offsets->begin, buf->chan);
-	offsets->reserve_commit_diff =
-		(BUFFER_TRUNC(offsets->begin, buf->chan)
-		 >> ltt_channel->n_subbufs_order)
-		- (local_read(&ltt_buf->commit_count[subbuf_index])
-			& ltt_channel->commit_count_mask);
-	if (offsets->reserve_commit_diff == 0) {
-		/* Next buffer not corrupted. */
-		if (mode == FORCE_ACTIVE
-		    && !ltt_channel->overwrite
-		    && offsets->begin - atomic_long_read(&ltt_buf->consumed)
-		       >= rchan->alloc_size) {
-			/*
-			 * We do not overwrite non consumed buffers and we are
-			 * full : ignore switch while tracing is active.
-			 */
-			return -1;
-		}
-	} else {
-		/*
-		 * Next subbuffer corrupted. Force pushing reader even in normal
-		 * mode
-		 */
-	}
-	offsets->end = offsets->begin;
-	return 0;
-}
-
-static __inline__ void ltt_reserve_push_reader(
-		struct ltt_channel_struct *ltt_channel,
-		struct ltt_channel_buf_struct *ltt_buf,
-		struct rchan *rchan,
-		struct rchan_buf *buf,
-		struct ltt_reserve_switch_offsets *offsets)
-{
-	long consumed_old, consumed_new;
+	*o_end = *o_begin + *size;
 
-	do {
-		consumed_old = atomic_long_read(&ltt_buf->consumed);
+	if (unlikely((SUBBUF_OFFSET(*o_end, buf->chan)) == 0))
 		/*
-		 * If buffer is in overwrite mode, push the reader consumed
-		 * count if the write position has reached it and we are not
-		 * at the first iteration (don't push the reader farther than
-		 * the writer). This operation can be done concurrently by many
-		 * writers in the same buffer, the writer being at the farthest
-		 * write position sub-buffer index in the buffer being the one
-		 * which will win this loop.
-		 * If the buffer is not in overwrite mode, pushing the reader
-		 * only happens if a sub-buffer is corrupted.
-		 */
-		if (unlikely((SUBBUF_TRUNC(offsets->end-1, buf->chan)
-		   - SUBBUF_TRUNC(consumed_old, buf->chan))
-		   >= rchan->alloc_size))
-			consumed_new = SUBBUF_ALIGN(consumed_old, buf->chan);
-		else
-			return;
-	} while (unlikely(atomic_long_cmpxchg(&ltt_buf->consumed, consumed_old,
-			consumed_new) != consumed_old));
-
-	if (unlikely(consumed_old != consumed_new)) {
-		/*
-		 * Reader pushed : we are the winner of the push, we can
-		 * therefore reequilibrate reserve and commit. Atomic increment
-		 * of the commit count permits other writers to play around
-		 * with this variable before us. We keep track of
-		 * corrupted_subbuffers even in overwrite mode :
-		 * we never want to write over a non completely committed
-		 * sub-buffer : possible causes : the buffer size is too low
-		 * compared to the unordered data input, or there is a writer
-		 * that died between the reserve and the commit.
+		 * The offset_end will fall at the very beginning of the next
+		 * subbuffer.
 		 */
-		if (likely(offsets->reserve_commit_diff)) {
-			/*
-			 * We have to alter the sub-buffer commit count.
-			 * We do not deliver the previous subbuffer, given it
-			 * was either corrupted or not consumed (overwrite
-			 * mode).
-			 */
-			local_add(offsets->reserve_commit_diff,
-				  &ltt_buf->commit_count[
-					SUBBUF_INDEX(offsets->begin,
-						     buf->chan)]);
-			if (unlikely(!ltt_channel->overwrite
-			    || offsets->reserve_commit_diff
-			       != rchan->subbuf_size)) {
-				/*
-				 * The reserve commit diff was not subbuf_size :
-				 * it means the subbuffer was partly written to
-				 * and is therefore corrupted. If it is multiple
-				 * of subbuffer size and we are in flight
-				 * recorder mode, we are skipping over a whole
-				 * subbuffer.
-				 */
-				local_inc(&ltt_buf->corrupted_subbuffers);
-			}
-		}
-	}
-}
-
-
-/*
- * ltt_reserve_switch_old_subbuf: switch old subbuffer
- *
- * Concurrency safe because we are the last and only thread to alter this
- * sub-buffer. As long as it is not delivered and read, no other thread can
- * alter the offset, alter the reserve_count or call the
- * client_buffer_end_callback on this sub-buffer.
- *
- * The only remaining threads could be the ones with pending commits. They will
- * have to do the deliver themselves.  Not concurrency safe in overwrite mode.
- * We detect corrupted subbuffers with commit and reserve counts. We keep a
- * corrupted sub-buffers count and push the readers across these sub-buffers.
- *
- * Not concurrency safe if a writer is stalled in a subbuffer and another writer
- * switches in, finding out it's corrupted.  The result will be than the old
- * (uncommited) subbuffer will be declared corrupted, and that the new subbuffer
- * will be declared corrupted too because of the commit count adjustment.
- *
- * Note : offset_old should never be 0 here.
- */
-static __inline__ void ltt_reserve_switch_old_subbuf(
-		struct ltt_channel_struct *ltt_channel,
-		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
-		struct rchan_buf *buf,
-		struct ltt_reserve_switch_offsets *offsets, u64 *tsc)
-{
-	long oldidx = SUBBUF_INDEX(offsets->old - 1, rchan);
-
-	ltt_buffer_end(buf, *tsc, offsets->old, oldidx);
-	/* Must write buffer end before incrementing commit count */
-	smp_wmb();
-	offsets->commit_count =
-		local_add_return(rchan->subbuf_size
-				 - (SUBBUF_OFFSET(offsets->old - 1, rchan)
-				 + 1),
-				 &ltt_buf->commit_count[oldidx]);
-	if (likely((BUFFER_TRUNC(offsets->old - 1, rchan)
-			>> ltt_channel->n_subbufs_order)
-			- ((offsets->commit_count - rchan->subbuf_size)
-				& ltt_channel->commit_count_mask) == 0))
-		ltt_deliver(buf, oldidx, NULL);
-}
-
-/*
- * ltt_reserve_switch_new_subbuf: Populate new subbuffer.
- *
- * This code can be executed unordered : writers may already have written to the
- * sub-buffer before this code gets executed, caution.  The commit makes sure
- * that this code is executed before the deliver of this sub-buffer.
- */
-static __inline__ void ltt_reserve_switch_new_subbuf(
-		struct ltt_channel_struct *ltt_channel,
-		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
-		struct rchan_buf *buf,
-		struct ltt_reserve_switch_offsets *offsets, u64 *tsc)
-{
-	long beginidx = SUBBUF_INDEX(offsets->begin, rchan);
-
-	ltt_buffer_begin(buf, *tsc, beginidx);
-	/* Must write buffer end before incrementing commit count */
-	smp_wmb();
-	offsets->commit_count = local_add_return(ltt_subbuffer_header_size(),
-			&ltt_buf->commit_count[beginidx]);
-	/* Check if the written buffer has to be delivered */
-	if (unlikely((BUFFER_TRUNC(offsets->begin, rchan)
-			>> ltt_channel->n_subbufs_order)
-			- ((offsets->commit_count - rchan->subbuf_size)
-				& ltt_channel->commit_count_mask) == 0))
-		ltt_deliver(buf, beginidx, NULL);
-}
-
-
-/*
- * ltt_reserve_end_switch_current: finish switching current subbuffer
- *
- * Concurrency safe because we are the last and only thread to alter this
- * sub-buffer. As long as it is not delivered and read, no other thread can
- * alter the offset, alter the reserve_count or call the
- * client_buffer_end_callback on this sub-buffer.
- *
- * The only remaining threads could be the ones with pending commits. They will
- * have to do the deliver themselves.  Not concurrency safe in overwrite mode.
- * We detect corrupted subbuffers with commit and reserve counts. We keep a
- * corrupted sub-buffers count and push the readers across these sub-buffers.
- *
- * Not concurrency safe if a writer is stalled in a subbuffer and another writer
- * switches in, finding out it's corrupted.  The result will be than the old
- * (uncommited) subbuffer will be declared corrupted, and that the new subbuffer
- * will be declared corrupted too because of the commit count adjustment.
- */
-static __inline__ void ltt_reserve_end_switch_current(
-		struct ltt_channel_struct *ltt_channel,
-		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
-		struct rchan_buf *buf,
-		struct ltt_reserve_switch_offsets *offsets, u64 *tsc)
-{
-	long endidx = SUBBUF_INDEX(offsets->end - 1, rchan);
+		return 1;
 
-	ltt_buffer_end(buf, *tsc, offsets->end, endidx);
-	/* Must write buffer begin before incrementing commit count */
-	smp_wmb();
-	offsets->commit_count =
-		local_add_return(rchan->subbuf_size
-				 - (SUBBUF_OFFSET(offsets->end - 1, rchan)
-				 + 1),
-				 &ltt_buf->commit_count[endidx]);
-	if (likely((BUFFER_TRUNC(offsets->end - 1, rchan)
-			>> ltt_channel->n_subbufs_order)
-			- ((offsets->commit_count - rchan->subbuf_size)
-				& ltt_channel->commit_count_mask) == 0))
-		ltt_deliver(buf, endidx, NULL);
+	return 0;
 }
 
-/**
- * ltt_relay_reserve_slot - Atomic slot reservation in a LTTng buffer.
- * @trace: the trace structure to log to.
- * @ltt_channel: channel structure
- * @transport_data: data structure specific to ltt relay
- * @data_size: size of the variable length data to log.
- * @slot_size: pointer to total size of the slot (out)
- * @buf_offset : pointer to reserved buffer offset (out)
- * @tsc: pointer to the tsc at the slot reservation (out)
- * @cpu: cpuid
- *
- * Return : -ENOSPC if not enough space, else returns 0.
- * It will take care of sub-buffer switching.
- */
 static __inline__ int ltt_reserve_slot(struct ltt_trace_struct *trace,
 		struct ltt_channel_struct *ltt_channel, void **transport_data,
 		size_t data_size, size_t *slot_size, long *buf_offset, u64 *tsc,
@@ -588,10 +217,8 @@ static __inline__ int ltt_reserve_slot(struct ltt_trace_struct *trace,
 	struct rchan *rchan = ltt_channel->trans_channel_data;
 	struct rchan_buf *buf = *transport_data = rchan->buf[cpu];
 	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
-	struct ltt_reserve_switch_offsets offsets;
-
-	offsets.reserve_commit_diff = 0;
-	offsets.size = 0;
+	long o_begin, o_end, o_old;
+	size_t before_hdr_pad;
 
 	/*
 	 * Perform retryable operations.
@@ -600,13 +227,15 @@ static __inline__ int ltt_reserve_slot(struct ltt_trace_struct *trace,
 		local_inc(&ltt_buf->events_lost);
 		return -EPERM;
 	}
-	do {
-		if (unlikely(ltt_relay_try_reserve(ltt_channel, ltt_buf,
-				rchan, buf, &offsets, data_size, tsc, rflags,
-				largest_align)))
-			return -ENOSPC;
-	} while (unlikely(local_cmpxchg(&ltt_buf->offset, offsets.old,
-			offsets.end) != offsets.old));
+
+	if (unlikely(ltt_relay_try_reserve(ltt_channel, ltt_buf,
+			rchan, buf, data_size, tsc, rflags,
+			largest_align, &o_begin, &o_end, &o_old,
+			&before_hdr_pad, slot_size)))
+		goto slow_path;
+
+	if (unlikely(local_cmpxchg(&ltt_buf->offset, o_old, o_end) != o_old))
+		goto slow_path;
 
 	/*
 	 * Atomically update last_tsc. This update races against concurrent
@@ -616,32 +245,12 @@ static __inline__ int ltt_reserve_slot(struct ltt_trace_struct *trace,
 	 */
 	save_last_tsc(ltt_buf, *tsc);
 
-	/*
-	 * Push the reader if necessary
-	 */
-	ltt_reserve_push_reader(ltt_channel, ltt_buf, rchan, buf, &offsets);
-
-	/*
-	 * Switch old subbuffer if needed.
-	 */
-	if (unlikely(offsets.end_switch_old))
-		ltt_reserve_switch_old_subbuf(ltt_channel, ltt_buf, rchan, buf,
-			&offsets, tsc);
-
-	/*
-	 * Populate new subbuffer.
-	 */
-	if (unlikely(offsets.begin_switch))
-		ltt_reserve_switch_new_subbuf(ltt_channel, ltt_buf, rchan,
-			buf, &offsets, tsc);
-
-	if (unlikely(offsets.end_switch_current))
-		ltt_reserve_end_switch_current(ltt_channel, ltt_buf, rchan,
-			buf, &offsets, tsc);
-
-	*slot_size = offsets.size;
-	*buf_offset = offsets.begin + offsets.before_hdr_pad;
+	*buf_offset = o_begin + before_hdr_pad;
 	return 0;
+slow_path:
+	return ltt_reserve_slot_lockless_slow(trace, ltt_channel,
+		transport_data, data_size, slot_size, buf_offset, tsc,
+		rflags, largest_align, cpu);
 }
 
 /*
@@ -656,54 +265,7 @@ static __inline__ int ltt_reserve_slot(struct ltt_trace_struct *trace,
 static __inline__ void ltt_force_switch(struct rchan_buf *buf,
 		enum force_switch_mode mode)
 {
-	struct ltt_channel_struct *ltt_channel =
-			(struct ltt_channel_struct *)buf->chan->private_data;
-	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
-	struct rchan *rchan = ltt_channel->trans_channel_data;
-	struct ltt_reserve_switch_offsets offsets;
-	u64 tsc;
-
-	offsets.reserve_commit_diff = 0;
-	offsets.size = 0;
-
-	/*
-	 * Perform retryable operations.
-	 */
-	do {
-		if (ltt_relay_try_switch(mode, ltt_channel, ltt_buf,
-				rchan, buf, &offsets, &tsc))
-			return;
-	} while (local_cmpxchg(&ltt_buf->offset, offsets.old,
-			offsets.end) != offsets.old);
-
-	/*
-	 * Atomically update last_tsc. This update races against concurrent
-	 * atomic updates, but the race will always cause supplementary full TSC
-	 * events, never the opposite (missing a full TSC event when it would be
-	 * needed).
-	 */
-	save_last_tsc(ltt_buf, tsc);
-
-	/*
-	 * Push the reader if necessary
-	 */
-	if (mode == FORCE_ACTIVE)
-		ltt_reserve_push_reader(ltt_channel, ltt_buf, rchan,
-					buf, &offsets);
-
-	/*
-	 * Switch old subbuffer if needed.
-	 */
-	if (offsets.end_switch_old)
-		ltt_reserve_switch_old_subbuf(ltt_channel, ltt_buf, rchan, buf,
-			&offsets, &tsc);
-
-	/*
-	 * Populate new subbuffer.
-	 */
-	if (mode == FORCE_ACTIVE)
-		ltt_reserve_switch_new_subbuf(ltt_channel,
-			ltt_buf, rchan, buf, &offsets, &tsc);
+	return ltt_force_switch_lockless_slow(buf, mode);
 }
 
 /*
-- 
1.6.5.2

