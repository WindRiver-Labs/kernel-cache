From f2fea764a34c493977bcae94398f0982d7b454f4 Mon Sep 17 00:00:00 2001
From: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
Date: Thu, 13 May 2010 19:28:03 -0400
Subject: [PATCH 314/391] lttng-fix-cpu-hotplug

LTTng fix cpu hotplug

Fix error in channel teardown, affecting CPU hotplug dependency on rcu read
side.

Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
---
 ltt/ltt-relay-alloc.c    |   20 ++++++++++++++------
 ltt/ltt-relay-lockless.c |    9 ++++++++-
 ltt/ltt-tracer.c         |   10 ++++++++--
 3 files changed, 30 insertions(+), 9 deletions(-)

diff --git a/ltt/ltt-relay-alloc.c b/ltt/ltt-relay-alloc.c
index d29069f..8c21e35 100644
--- a/ltt/ltt-relay-alloc.c
+++ b/ltt/ltt-relay-alloc.c
@@ -242,7 +242,10 @@ void ltt_chan_for_each_channel(void (*cb) (struct ltt_chanbuf *buf), int cpu,
 {
 	struct ltt_chan *chan;
 
-	rcu_read_lock();
+	if (sleepable)
+		rcu_read_lock();
+	else
+		rcu_read_lock_sched_notrace();
 	list_for_each_entry_rcu(chan, &ltt_relay_channels, a.list) {
 		struct ltt_chanbuf *buf;
 
@@ -251,7 +254,10 @@ void ltt_chan_for_each_channel(void (*cb) (struct ltt_chanbuf *buf), int cpu,
 		buf = per_cpu_ptr(chan->a.buf, cpu);
 		cb(buf);
 	}
-	rcu_read_unlock();
+	if (sleepable)
+		rcu_read_unlock();
+	else
+		rcu_read_unlock_sched_notrace();
 }
 
 /**
@@ -350,6 +356,12 @@ void ltt_chan_alloc_free(struct ltt_chan_alloc *chan)
 	unsigned int i;
 
 	mutex_lock(&ltt_relay_alloc_mutex);
+	list_del_rcu(&chan->list);
+	/* Delay channel free for RCU channel list, protected by
+	 * RCU sched (tracing code) and standard RCU (hotplug management). */
+	synchronize_sched();
+	synchronize_rcu();
+
 	for_each_possible_cpu(i) {
 		struct ltt_chanbuf *buf = per_cpu_ptr(chan->buf, i);
 
@@ -358,11 +370,7 @@ void ltt_chan_alloc_free(struct ltt_chan_alloc *chan)
 		ltt_chanbuf_remove_file(buf);
 		kref_put(&buf->a.kref, ltt_chanbuf_free);
 	}
-	list_del_rcu(&chan->list);
 	mutex_unlock(&ltt_relay_alloc_mutex);
-	/* Delay channel free for RCU channel list, protected by
-	 * RCU sched. */
-	synchronize_sched();
 	free_percpu(chan->buf);
 	kref_put(&chan->trace->kref, ltt_release_trace);
 	wake_up_interruptible(&chan->trace->kref_wq);
diff --git a/ltt/ltt-relay-lockless.c b/ltt/ltt-relay-lockless.c
index 2edeae1..5d2e3fd 100644
--- a/ltt/ltt-relay-lockless.c
+++ b/ltt/ltt-relay-lockless.c
@@ -521,9 +521,16 @@ static void ltt_chanbuf_idle_switch(struct ltt_chanbuf *buf)
 		ltt_force_switch(buf, FORCE_ACTIVE);
 }
 
+/*
+ * ltt_chanbuf_switch is called from a remote CPU to ensure that the buffers of
+ * a cpu which went down are flushed. Note that if we execute concurrently
+ * with trace allocation, a buffer might appear be unallocated (because it
+ * detects that the target CPU is offline).
+ */
 static void ltt_chanbuf_switch(struct ltt_chanbuf *buf)
 {
-	ltt_force_switch(buf, FORCE_ACTIVE);
+	if (buf->a.allocated)
+		ltt_force_switch(buf, FORCE_ACTIVE);
 }
 
 /**
diff --git a/ltt/ltt-tracer.c b/ltt/ltt-tracer.c
index 4e7b223..5a587fe 100644
--- a/ltt/ltt-tracer.c
+++ b/ltt/ltt-tracer.c
@@ -1024,9 +1024,16 @@ static void __ltt_trace_destroy(struct ltt_trace *trace)
 	 */
 	if (atomic_read(&trace->kref.refcount) > 1) {
 		int ret = 0;
+		/*
+		 * Unlock traces and CPU hotplug while we wait for lttd to
+		 * release the files.
+		 */
+		ltt_unlock_traces();
 		__wait_event_interruptible(trace->kref_wq,
 			(atomic_read(&trace->kref.refcount) == 1), ret);
+		ltt_lock_traces();
 	}
+
 	kref_put(&trace->kref, ltt_release_trace);
 }
 
@@ -1043,9 +1050,8 @@ int ltt_trace_destroy(const char *trace_name)
 		if (err)
 			goto error;
 
-		ltt_unlock_traces();
-
 		__ltt_trace_destroy(trace);
+		ltt_unlock_traces();
 		put_trace_clock();
 
 		return 0;
-- 
1.6.5.2

