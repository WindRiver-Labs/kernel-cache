From 1f6a381e9c379b42f947cc9dc671d196421e8c48 Mon Sep 17 00:00:00 2001
From: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
Date: Thu, 13 May 2010 19:27:04 -0400
Subject: [PATCH 204/390] lttng-force-inlines

ltt relay lockless: force inlines

Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
---
 include/linux/ltt-relay.h  |   12 ++++++------
 include/linux/ltt-tracer.h |   18 +++++++++---------
 ltt/ltt-relay-lockless.h   |   36 ++++++++++++++++++------------------
 3 files changed, 33 insertions(+), 33 deletions(-)

diff --git a/include/linux/ltt-relay.h b/include/linux/ltt-relay.h
index 3781cbf..c622a85 100644
--- a/include/linux/ltt-relay.h
+++ b/include/linux/ltt-relay.h
@@ -171,7 +171,7 @@ extern void *ltt_relay_offset_address(struct rchan_buf *buf,
  * Find the page containing "offset". Cache it if it is after the currently
  * cached page.
  */
-static inline struct buf_page *ltt_relay_cache_page(struct rchan_buf *buf,
+static __inline__ struct buf_page *ltt_relay_cache_page(struct rchan_buf *buf,
 		struct buf_page **page_cache,
 		struct buf_page *page, size_t offset)
 {
@@ -206,7 +206,7 @@ static inline struct buf_page *ltt_relay_cache_page(struct rchan_buf *buf,
 }
 
 #ifdef CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS
-static inline void ltt_relay_do_copy(void *dest, const void *src, size_t len)
+static __inline__ void ltt_relay_do_copy(void *dest, const void *src, size_t len)
 {
 	switch (len) {
 	case 0:
@@ -225,7 +225,7 @@ static inline void ltt_relay_do_copy(void *dest, const void *src, size_t len)
 		break;
 	default:
 		/*
-		 * What we really want here is an inline memcpy, but we don't
+		 * What we really want here is an __inline__ memcpy, but we don't
 		 * have constants, so gcc generally uses a function call.
 		 */
 		for (; len > 0; len--)
@@ -237,7 +237,7 @@ static inline void ltt_relay_do_copy(void *dest, const void *src, size_t len)
  * Returns whether the dest and src addresses are aligned on
  * min(sizeof(void *), len). Call this with statically known len for efficiency.
  */
-static inline int addr_aligned(const void *dest, const void *src, size_t len)
+static __inline__ int addr_aligned(const void *dest, const void *src, size_t len)
 {
 	if (ltt_align((size_t)dest, len))
 		return 0;
@@ -246,7 +246,7 @@ static inline int addr_aligned(const void *dest, const void *src, size_t len)
 	return 1;
 }
 
-static inline void ltt_relay_do_copy(void *dest, const void *src, size_t len)
+static __inline__ void ltt_relay_do_copy(void *dest, const void *src, size_t len)
 {
 	switch (len) {
 	case 0:
@@ -284,7 +284,7 @@ memcpy_fallback:
 }
 #endif
 
-static inline int ltt_relay_write(struct rchan_buf *buf, size_t offset,
+static __inline__ int ltt_relay_write(struct rchan_buf *buf, size_t offset,
 	const void *src, size_t len)
 {
 	struct buf_page *page;
diff --git a/include/linux/ltt-tracer.h b/include/linux/ltt-tracer.h
index 87656fd..19d43aa 100644
--- a/include/linux/ltt-tracer.h
+++ b/include/linux/ltt-tracer.h
@@ -136,7 +136,7 @@ enum marker_id {
 
 /* static ids 0-1 reserved for internal use. */
 #define MARKER_CORE_IDS		2
-static inline enum marker_id marker_id_type(uint16_t id)
+static __inline__ enum marker_id marker_id_type(uint16_t id)
 {
 	if (id < MARKER_CORE_IDS)
 		return (enum marker_id)id;
@@ -306,7 +306,7 @@ struct ltt_subbuffer_header {
  * structure because gcc generates inefficient code on some architectures
  * (powerpc, mips..)
  */
-static inline size_t ltt_subbuffer_header_size(void)
+static __inline__ size_t ltt_subbuffer_header_size(void)
 {
 	return offsetof(struct ltt_subbuffer_header, header_end);
 }
@@ -331,7 +331,7 @@ static inline size_t ltt_subbuffer_header_size(void)
  * The payload must itself determine its own alignment from the biggest type it
  * contains.
  * */
-static inline unsigned char ltt_get_header_size(
+static __inline__ unsigned char ltt_get_header_size(
 		struct ltt_channel_struct *channel,
 		size_t offset,
 		size_t data_size,
@@ -385,7 +385,7 @@ static inline unsigned char ltt_get_header_size(
  *
  * returns : offset where the event data must be written.
  */
-static inline size_t ltt_write_event_header(struct ltt_trace_struct *trace,
+static __inline__ size_t ltt_write_event_header(struct ltt_trace_struct *trace,
 		struct ltt_channel_struct *channel,
 		struct rchan_buf *buf, long buf_offset,
 		u16 eID, size_t event_size,
@@ -461,7 +461,7 @@ static inline size_t ltt_write_event_header(struct ltt_trace_struct *trace,
  * ltt_read_event_header
  * buf_offset must aligned on 32 bits
  */
-static inline size_t ltt_read_event_header(struct rchan_buf *buf,
+static __inline__ size_t ltt_read_event_header(struct rchan_buf *buf,
 		long buf_offset, u64 *tsc, u32 *event_size, u16 *eID,
 		unsigned int *rflags)
 {
@@ -587,7 +587,7 @@ static inline size_t ltt_read_event_header(struct rchan_buf *buf,
  * @trace: Trace information
  * @header: Memory address where the information must be written to
  */
-static inline void ltt_write_trace_header(struct ltt_trace_struct *trace,
+static __inline__ void ltt_write_trace_header(struct ltt_trace_struct *trace,
 		struct ltt_subbuffer_header *header)
 {
 	header->magic_number = LTT_TRACER_MAGIC_NUMBER;
@@ -700,7 +700,7 @@ void ltt_unlock_traces(void);
 #ifdef CONFIG_LTT_KPROBES
 extern void ltt_dump_kprobes_table(void *call_data);
 #else
-static inline void ltt_dump_kprobes_table(void *call_data)
+static __inline__ void ltt_dump_kprobes_table(void *call_data)
 {
 }
 #endif
@@ -711,11 +711,11 @@ extern void ltt_dump_softirq_vec(void *call_data);
 extern void ltt_dump_sys_call_table(void *call_data);
 extern void ltt_dump_idt_table(void *call_data);
 #else
-static inline void ltt_dump_sys_call_table(void *call_data)
+static __inline__ void ltt_dump_sys_call_table(void *call_data)
 {
 }
 
-static inline void ltt_dump_idt_table(void *call_data)
+static __inline__ void ltt_dump_idt_table(void *call_data)
 {
 }
 #endif
diff --git a/ltt/ltt-relay-lockless.h b/ltt/ltt-relay-lockless.h
index 7670607..ce988dc 100644
--- a/ltt/ltt-relay-lockless.h
+++ b/ltt/ltt-relay-lockless.h
@@ -108,13 +108,13 @@ struct ltt_channel_buf_struct {
  */
 
 #if (BITS_PER_LONG == 32)
-static inline void save_last_tsc(struct ltt_channel_buf_struct *ltt_buf,
+static __inline__ void save_last_tsc(struct ltt_channel_buf_struct *ltt_buf,
 					u64 tsc)
 {
 	ltt_buf->last_tsc = (unsigned long)(tsc >> LTT_TSC_BITS);
 }
 
-static inline int last_tsc_overflow(struct ltt_channel_buf_struct *ltt_buf,
+static __inline__ int last_tsc_overflow(struct ltt_channel_buf_struct *ltt_buf,
 					u64 tsc)
 {
 	unsigned long tsc_shifted = (unsigned long)(tsc >> LTT_TSC_BITS);
@@ -125,13 +125,13 @@ static inline int last_tsc_overflow(struct ltt_channel_buf_struct *ltt_buf,
 		return 0;
 }
 #else
-static inline void save_last_tsc(struct ltt_channel_buf_struct *ltt_buf,
+static __inline__ void save_last_tsc(struct ltt_channel_buf_struct *ltt_buf,
 					u64 tsc)
 {
 	ltt_buf->last_tsc = (unsigned long)tsc;
 }
 
-static inline int last_tsc_overflow(struct ltt_channel_buf_struct *ltt_buf,
+static __inline__ int last_tsc_overflow(struct ltt_channel_buf_struct *ltt_buf,
 					u64 tsc)
 {
 	if (unlikely((tsc - ltt_buf->last_tsc) >> LTT_TSC_BITS))
@@ -147,7 +147,7 @@ static inline int last_tsc_overflow(struct ltt_channel_buf_struct *ltt_buf,
  */
 enum force_switch_mode { FORCE_ACTIVE, FORCE_FLUSH };
 
-static inline void ltt_buffer_begin(struct rchan_buf *buf,
+static __inline__ void ltt_buffer_begin(struct rchan_buf *buf,
 			u64 tsc, unsigned int subbuf_idx)
 {
 	struct ltt_channel_struct *channel =
@@ -167,7 +167,7 @@ static inline void ltt_buffer_begin(struct rchan_buf *buf,
  * offset is assumed to never be 0 here : never deliver a completely empty
  * subbuffer. The lost size is between 0 and subbuf_size-1.
  */
-static inline void ltt_buffer_end(struct rchan_buf *buf,
+static __inline__ void ltt_buffer_end(struct rchan_buf *buf,
 		u64 tsc, unsigned int offset, unsigned int subbuf_idx)
 {
 	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
@@ -183,7 +183,7 @@ static inline void ltt_buffer_end(struct rchan_buf *buf,
 	header->subbuf_corrupt = local_read(&ltt_buf->corrupted_subbuffers);
 }
 
-static inline void ltt_deliver(struct rchan_buf *buf, unsigned int subbuf_idx,
+static __inline__ void ltt_deliver(struct rchan_buf *buf, unsigned int subbuf_idx,
 		void *subbuf)
 {
 	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
@@ -203,7 +203,7 @@ struct ltt_reserve_switch_offsets {
  * 0 if ok
  * !0 if execution must be aborted.
  */
-static inline int ltt_relay_try_reserve(
+static __inline__ int ltt_relay_try_reserve(
 		struct ltt_channel_struct *ltt_channel,
 		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
 		struct rchan_buf *buf,
@@ -320,7 +320,7 @@ static inline int ltt_relay_try_reserve(
  * 0 if ok
  * !0 if execution must be aborted.
  */
-static inline int ltt_relay_try_switch(
+static __inline__ int ltt_relay_try_switch(
 		enum force_switch_mode mode,
 		struct ltt_channel_struct *ltt_channel,
 		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
@@ -378,7 +378,7 @@ static inline int ltt_relay_try_switch(
 	return 0;
 }
 
-static inline void ltt_reserve_push_reader(
+static __inline__ void ltt_reserve_push_reader(
 		struct ltt_channel_struct *ltt_channel,
 		struct ltt_channel_buf_struct *ltt_buf,
 		struct rchan *rchan,
@@ -472,7 +472,7 @@ static inline void ltt_reserve_push_reader(
  *
  * Note : offset_old should never be 0 here.
  */
-static inline void ltt_reserve_switch_old_subbuf(
+static __inline__ void ltt_reserve_switch_old_subbuf(
 		struct ltt_channel_struct *ltt_channel,
 		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
 		struct rchan_buf *buf,
@@ -502,7 +502,7 @@ static inline void ltt_reserve_switch_old_subbuf(
  * sub-buffer before this code gets executed, caution.  The commit makes sure
  * that this code is executed before the deliver of this sub-buffer.
  */
-static inline void ltt_reserve_switch_new_subbuf(
+static __inline__ void ltt_reserve_switch_new_subbuf(
 		struct ltt_channel_struct *ltt_channel,
 		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
 		struct rchan_buf *buf,
@@ -542,7 +542,7 @@ static inline void ltt_reserve_switch_new_subbuf(
  * (uncommited) subbuffer will be declared corrupted, and that the new subbuffer
  * will be declared corrupted too because of the commit count adjustment.
  */
-static inline void ltt_reserve_end_switch_current(
+static __inline__ void ltt_reserve_end_switch_current(
 		struct ltt_channel_struct *ltt_channel,
 		struct ltt_channel_buf_struct *ltt_buf, struct rchan *rchan,
 		struct rchan_buf *buf,
@@ -579,7 +579,7 @@ static inline void ltt_reserve_end_switch_current(
  * Return : -ENOSPC if not enough space, else returns 0.
  * It will take care of sub-buffer switching.
  */
-static inline int ltt_reserve_slot(struct ltt_trace_struct *trace,
+static __inline__ int ltt_reserve_slot(struct ltt_trace_struct *trace,
 		struct ltt_channel_struct *ltt_channel, void **transport_data,
 		size_t data_size, size_t *slot_size, long *buf_offset, u64 *tsc,
 		unsigned int *rflags, int largest_align, int cpu)
@@ -652,7 +652,7 @@ static inline int ltt_reserve_slot(struct ltt_trace_struct *trace,
  * operations, this function must be called from the CPU which owns the buffer
  * for a ACTIVE flush.
  */
-static inline void ltt_force_switch(struct rchan_buf *buf,
+static __inline__ void ltt_force_switch(struct rchan_buf *buf,
 		enum force_switch_mode mode)
 {
 	struct ltt_channel_struct *ltt_channel =
@@ -715,7 +715,7 @@ static inline void ltt_force_switch(struct rchan_buf *buf,
  * subbuffer).
  */
 #ifdef CONFIG_LTT_VMCORE
-static inline void ltt_write_commit_counter(struct rchan_buf *buf,
+static __inline__ void ltt_write_commit_counter(struct rchan_buf *buf,
 		long buf_offset, size_t slot_size)
 {
 	struct ltt_channel_buf_struct *ltt_buf = buf->chan_private;
@@ -746,7 +746,7 @@ static inline void ltt_write_commit_counter(struct rchan_buf *buf,
 	}
 }
 #else
-static inline void ltt_write_commit_counter(struct rchan_buf *buf,
+static __inline__ void ltt_write_commit_counter(struct rchan_buf *buf,
 		long buf_offset, size_t slot_size)
 {
 }
@@ -763,7 +763,7 @@ static inline void ltt_write_commit_counter(struct rchan_buf *buf,
  * @buf_offset : offset following the event header.
  * @slot_size : size of the reserved slot.
  */
-static inline void ltt_commit_slot(
+static __inline__ void ltt_commit_slot(
 		struct ltt_channel_struct *ltt_channel,
 		void **transport_data, long buf_offset, size_t slot_size)
 {
-- 
1.6.5.2

