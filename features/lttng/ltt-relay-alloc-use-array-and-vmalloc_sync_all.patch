From f64ee0d6cf3ed39178ef6241c54baaef9f480270 Mon Sep 17 00:00:00 2001
From: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
Date: Thu, 13 May 2010 19:27:14 -0400
Subject: [PATCH 219/390] ltt-relay-alloc-use-array-and-vmalloc_sync_all

ltt relay alloc: use array and vmalloc_sync_all()

Use a pointer array instead of a list to keep the buffer pages. If the array
every gets too large to be allocated via kmalloc, we make sure it will never
cause a vmalloc fault by calling vmalloc_sync_all() after allocation.

Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
---
 include/linux/ltt-relay.h |   53 ++---------
 ltt/Kconfig               |   10 --
 ltt/ltt-relay-alloc.c     |  234 ++++++++++++---------------------------------
 3 files changed, 70 insertions(+), 227 deletions(-)

diff --git a/include/linux/ltt-relay.h b/include/linux/ltt-relay.h
index d0a6c7b..a8c8836 100644
--- a/include/linux/ltt-relay.h
+++ b/include/linux/ltt-relay.h
@@ -30,27 +30,16 @@
  */
 #define LTT_RELAY_CHANNEL_VERSION		8
 
-struct rchan_buf;
-
-struct buf_page {
-	struct page *page;
-	void *virt;		/* page address of the struct page */
-	size_t offset;		/* page offset in the buffer */
-	struct list_head list;	/* buffer linked list */
-};
-
 /*
  * Per-cpu relay channel buffer
  */
 struct rchan_buf {
 	void *chan_private;		/* private data for this buf */
+	void **virt;			/* Array of pointers to page addr */
+	struct page **pages;		/* Array of pointers to pages */
 	struct rchan *chan;		/* associated channel */
 	struct dentry *dentry;		/* channel file dentry */
 	struct kref kref;		/* channel buffer refcount */
-	struct list_head pages;		/* list of buffer pages */
-	struct buf_page *wpage;		/* current write page (cache) */
-	struct buf_page *hpage[2];	/* current subbuf header page (cache) */
-	struct buf_page *rpage;		/* current subbuf read page (cache) */
 	unsigned int page_count;	/* number of current buffer pages */
 	unsigned int cpu;		/* this buf's cpu */
 	unsigned int random_access;	/* buffer performs random page access */
@@ -140,14 +129,8 @@ struct rchan_callbacks {
 	int (*remove_buf_file)(struct dentry *dentry);
 };
 
-extern struct buf_page *ltt_relay_find_prev_page(struct rchan_buf *buf,
-	struct buf_page *page, size_t offset, ssize_t diff_offset);
-
-extern struct buf_page *ltt_relay_find_next_page(struct rchan_buf *buf,
-	struct buf_page *page, size_t offset, ssize_t diff_offset);
-
 extern void _ltt_relay_write(struct rchan_buf *buf, size_t offset,
-	const void *src, size_t len, struct buf_page *page, ssize_t pagecpy);
+	const void *src, size_t len, ssize_t pagecpy);
 
 extern int ltt_relay_read(struct rchan_buf *buf, size_t offset,
 	void *dest, size_t len);
@@ -155,7 +138,7 @@ extern int ltt_relay_read(struct rchan_buf *buf, size_t offset,
 extern int ltt_relay_read_cstr(struct rchan_buf *buf, size_t offset,
 	void *dest, size_t len);
 
-extern struct buf_page *ltt_relay_read_get_page(struct rchan_buf *buf,
+extern struct page *ltt_relay_read_get_page(struct rchan_buf *buf,
 	size_t offset);
 
 /*
@@ -167,23 +150,6 @@ extern struct buf_page *ltt_relay_read_get_page(struct rchan_buf *buf,
 extern void *ltt_relay_offset_address(struct rchan_buf *buf,
 	size_t offset);
 
-extern struct buf_page *ltt_relay_cache_page_slow(struct rchan_buf *buf,
-		struct buf_page **page_cache,
-		struct buf_page *page, size_t offset);
-
-/*
- * Find the page containing "offset". Cache it if it is after the currently
- * cached page.
- */
-static __inline__ struct buf_page *ltt_relay_cache_page(struct rchan_buf *buf,
-		struct buf_page **page_cache,
-		struct buf_page *page, size_t offset)
-{
-	if (unlikely((offset & PAGE_MASK) != page->offset))
-		return ltt_relay_cache_page_slow(buf, page_cache, page, offset);
-	return page;
-}
-
 #ifdef CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS
 static __inline__ void ltt_relay_do_copy(void *dest, const void *src, size_t len)
 {
@@ -266,18 +232,17 @@ memcpy_fallback:
 static __inline__ int ltt_relay_write(struct rchan_buf *buf, size_t offset,
 	const void *src, size_t len)
 {
-	struct buf_page *page;
+	size_t index;
 	ssize_t pagecpy;
 
 	offset &= buf->chan->alloc_size - 1;
-	page = buf->wpage;
-
-	page = ltt_relay_cache_page(buf, &buf->wpage, page, offset);
+	index = offset >> PAGE_SHIFT;
 	pagecpy = min_t(size_t, len, (- offset) & ~PAGE_MASK);
-	ltt_relay_do_copy(page->virt + (offset & ~PAGE_MASK), src, pagecpy);
+	ltt_relay_do_copy(buf->virt[index] + (offset & ~PAGE_MASK),
+			  src, pagecpy);
 
 	if (unlikely(len != pagecpy))
-		_ltt_relay_write(buf, offset, src, len, page, pagecpy);
+		_ltt_relay_write(buf, offset, src, len, pagecpy);
 	return len;
 }
 
diff --git a/ltt/Kconfig b/ltt/Kconfig
index 8ef8c16..6283c2b 100644
--- a/ltt/Kconfig
+++ b/ltt/Kconfig
@@ -84,16 +84,6 @@ config LTT_FAST_SERIALIZE
 	  Library for serializing information from custom, efficient, tracepoint
 	  probes.
 
-config LTT_RELAY_CHECK_RANDOM_ACCESS
-	bool "Debug check for random access in ltt relay buffers"
-	depends on LTT_RELAY_ALLOC
-	default n
-	help
-	  Add checks for random access to LTTng relay buffers. Given those
-	  buffers are a linked list, such access are rather slow. Rare accesses
-	  are OK; they can be caused by large writes (more than a page large) or
-	  by reentrancy (e.g. interrupt nesting over the tracing code).
-
 config LTT_TRACEPROBES
 	tristate "Compile lttng tracing probes"
 	depends on LTT_FAST_SERIALIZE
diff --git a/ltt/ltt-relay-alloc.c b/ltt/ltt-relay-alloc.c
index d4d8860..d073163 100644
--- a/ltt/ltt-relay-alloc.c
+++ b/ltt/ltt-relay-alloc.c
@@ -34,47 +34,53 @@ static LIST_HEAD(relay_channels);
  */
 static int relay_alloc_buf(struct rchan_buf *buf, size_t *size)
 {
-	unsigned int i, n_pages;
-	struct buf_page *buf_pages;
-	struct buf_page *buf_page, *n;
+	long i, n_pages;
+	struct page **pages;
+	void **virt;
 
 	*size = PAGE_ALIGN(*size);
 	n_pages = *size >> PAGE_SHIFT;
 
-	INIT_LIST_HEAD(&buf->pages);
+	pages = kmalloc_node(max_t(size_t, sizeof(*pages) * n_pages,
+				   1 << INTERNODE_CACHE_SHIFT),
+			GFP_KERNEL, cpu_to_node(buf->cpu));
+	if (unlikely(!pages))
+		goto pages_error;
 
-	buf_pages = kmalloc_node(ALIGN(sizeof(*buf_page) * n_pages,
-				       1 << INTERNODE_CACHE_SHIFT),
+	virt = kmalloc_node(ALIGN(sizeof(*virt) * n_pages,
+				  1 << INTERNODE_CACHE_SHIFT),
 			GFP_KERNEL, cpu_to_node(buf->cpu));
-	if (unlikely(!buf_pages))
-		return -ENOMEM;
+	if (unlikely(!virt))
+		goto virt_error;
 
 	for (i = 0; i < n_pages; i++) {
-		buf_page = &buf_pages[i];
-		buf_page->page = alloc_pages_node(cpu_to_node(buf->cpu),
+		pages[i] = alloc_pages_node(cpu_to_node(buf->cpu),
 			GFP_KERNEL | __GFP_ZERO, 0);
-		if (unlikely(!buf_page->page))
+		if (unlikely(!pages[i]))
 			goto depopulate;
-		list_add_tail(&buf_page->list, &buf->pages);
-		buf_page->virt = page_address(buf_page->page);
-		buf_page->offset = (size_t)i << PAGE_SHIFT;
-		set_page_private(buf_page->page, (unsigned long)buf_page);
-		if (i == 0) {
-			buf->wpage = buf_page;
-			buf->hpage[0] = buf_page;
-			buf->hpage[1] = buf_page;
-			buf->rpage = buf_page;
-		}
+		virt[i] = page_address(pages[i]);
 	}
 	buf->page_count = n_pages;
+	buf->pages = pages;
+	buf->virt = virt;
+	/*
+	 * If kmalloc ever uses vmalloc underneath, make sure the buffer pages
+	 * will not fault.
+	 */
+	vmalloc_sync_all();
 	return 0;
 
 depopulate:
-	list_for_each_entry_safe(buf_page, n, &buf->pages, list) {
-		list_del_init(&buf_page->list);
-		__free_page(buf_page->page);
-	}
-	kfree(buf_pages);
+	/*
+	 * Free all pages from [ i - 1 down to 0 ].
+	 * If i = 0, don't free anything.
+	 */
+	for (i--; i >= 0; i--)
+		__free_page(pages[i]);
+	kfree(virt);
+virt_error:
+	kfree(pages);
+pages_error:
 	return -ENOMEM;
 }
 
@@ -137,13 +143,14 @@ EXPORT_SYMBOL_GPL(ltt_relay_put_chan);
 static void relay_destroy_buf(struct rchan_buf *buf)
 {
 	struct rchan *chan = buf->chan;
-	struct buf_page *buf_page, *n;
-
-	list_for_each_entry_safe(buf_page, n, &buf->pages, list) {
-		list_del_init(&buf_page->list);
-		__free_page(buf_page->page);
-		kfree(buf_page);
-	}
+	struct page **pages;
+	long i;
+
+	pages = buf->pages;
+	for (i = 0; i < buf->page_count; i++)
+		__free_page(pages[i]);
+	kfree(buf->pages);
+	kfree(buf->virt);
 	chan->buf[buf->cpu] = NULL;
 	kfree(buf);
 	kref_put(&chan->kref, relay_destroy_channel);
@@ -428,86 +435,6 @@ void ltt_relay_close(struct rchan *chan)
 }
 EXPORT_SYMBOL_GPL(ltt_relay_close);
 
-/*
- * Start iteration at the previous element. Skip the real list head.
- */
-struct buf_page *ltt_relay_find_prev_page(struct rchan_buf *buf,
-	struct buf_page *page, size_t offset, ssize_t diff_offset)
-{
-	struct buf_page *iter;
-	size_t orig_iter_off;
-	unsigned int i = 0;
-
-	orig_iter_off = page->offset;
-	list_for_each_entry_reverse(iter, &page->list, list) {
-		/*
-		 * Skip the real list head.
-		 */
-		if (&iter->list == &buf->pages)
-			continue;
-		i++;
-		if (offset >= iter->offset
-			&& offset < iter->offset + PAGE_SIZE) {
-#ifdef CONFIG_LTT_RELAY_CHECK_RANDOM_ACCESS
-			if (!buf->random_access && i > 1) {
-				printk(KERN_WARNING
-					"Backward random access detected in "
-					"ltt_relay. Iterations %u, "
-					"offset %zu, orig iter->off %zu, "
-					"iter->off %zu diff_offset %zd.\n", i,
-					offset, orig_iter_off, iter->offset,
-					diff_offset);
-				WARN_ON(1);
-			}
-#endif
-			return iter;
-		}
-	}
-	WARN_ON(1);
-	return NULL;
-}
-EXPORT_SYMBOL_GPL(ltt_relay_find_prev_page);
-
-/*
- * Start iteration at the next element. Skip the real list head.
- */
-struct buf_page *ltt_relay_find_next_page(struct rchan_buf *buf,
-	struct buf_page *page, size_t offset, ssize_t diff_offset)
-{
-	struct buf_page *iter;
-	unsigned int i = 0;
-	size_t orig_iter_off;
-
-	orig_iter_off = page->offset;
-	list_for_each_entry(iter, &page->list, list) {
-		/*
-		 * Skip the real list head.
-		 */
-		if (&iter->list == &buf->pages)
-			continue;
-		i++;
-		if (offset >= iter->offset
-			&& offset < iter->offset + PAGE_SIZE) {
-#ifdef CONFIG_LTT_RELAY_CHECK_RANDOM_ACCESS
-			if (!buf->random_access && i > 1) {
-				printk(KERN_WARNING
-					"Forward random access detected in "
-					"ltt_relay. Iterations %u, "
-					"offset %zu, orig iter->off %zu, "
-					"iter->off %zu diff_offset %zd.\n", i,
-					offset, orig_iter_off, iter->offset,
-					diff_offset);
-				WARN_ON(1);
-			}
-#endif
-			return iter;
-		}
-	}
-	WARN_ON(1);
-	return NULL;
-}
-EXPORT_SYMBOL_GPL(ltt_relay_find_next_page);
-
 /**
  * ltt_relay_write - write data to a ltt_relay buffer.
  * @buf : buffer
@@ -518,21 +445,24 @@ EXPORT_SYMBOL_GPL(ltt_relay_find_next_page);
  * @pagecpy : page size copied so far
  */
 void _ltt_relay_write(struct rchan_buf *buf, size_t offset,
-	const void *src, size_t len, struct buf_page *page, ssize_t pagecpy)
+	const void *src, size_t len, ssize_t pagecpy)
 {
+	size_t index;
+
 	do {
 		len -= pagecpy;
 		src += pagecpy;
 		offset += pagecpy;
+		index = offset >> PAGE_SHIFT;
+
 		/*
 		 * Underlying layer should never ask for writes across
 		 * subbuffers.
 		 */
 		WARN_ON(offset >= buf->chan->alloc_size);
 
-		page = ltt_relay_cache_page(buf, &buf->wpage, page, offset);
 		pagecpy = min_t(size_t, len, PAGE_SIZE - (offset & ~PAGE_MASK));
-		ltt_relay_do_copy(page->virt
+		ltt_relay_do_copy(buf->virt[index]
 				+ (offset & ~PAGE_MASK), src, pagecpy);
 	} while (unlikely(len != pagecpy));
 }
@@ -548,23 +478,23 @@ EXPORT_SYMBOL_GPL(_ltt_relay_write);
 int ltt_relay_read(struct rchan_buf *buf, size_t offset,
 	void *dest, size_t len)
 {
-	struct buf_page *page;
+	size_t index;
 	ssize_t pagecpy, orig_len;
 
 	orig_len = len;
 	offset &= buf->chan->alloc_size - 1;
-	page = buf->rpage;
+	index = offset >> PAGE_SHIFT;
 	if (unlikely(!len))
 		return 0;
 	for (;;) {
-		page = ltt_relay_cache_page(buf, &buf->rpage, page, offset);
 		pagecpy = min_t(size_t, len, PAGE_SIZE - (offset & ~PAGE_MASK));
-		memcpy(dest, page->virt + (offset & ~PAGE_MASK), pagecpy);
+		memcpy(dest, buf->virt[index] + (offset & ~PAGE_MASK), pagecpy);
 		len -= pagecpy;
 		if (likely(!len))
 			break;
 		dest += pagecpy;
 		offset += pagecpy;
+		index = offset >> PAGE_SHIFT;
 		/*
 		 * Underlying layer should never ask for reads across
 		 * subbuffers.
@@ -587,16 +517,15 @@ EXPORT_SYMBOL_GPL(ltt_relay_read);
 int ltt_relay_read_cstr(struct rchan_buf *buf, size_t offset,
 		void *dest, size_t len)
 {
-	struct buf_page *page;
+	size_t index;
 	ssize_t pagecpy, pagelen, strpagelen, orig_offset;
 	char *str;
 
 	offset &= buf->chan->alloc_size - 1;
+	index = offset >> PAGE_SHIFT;
 	orig_offset = offset;
-	page = buf->rpage;
 	for (;;) {
-		page = ltt_relay_cache_page(buf, &buf->rpage, page, offset);
-		str = (char *)page->virt + (offset & ~PAGE_MASK);
+		str = (char *)buf->virt[index] + (offset & ~PAGE_MASK);
 		pagelen = PAGE_SIZE - (offset & ~PAGE_MASK);
 		strpagelen = strnlen(str, pagelen);
 		if (len) {
@@ -606,6 +535,7 @@ int ltt_relay_read_cstr(struct rchan_buf *buf, size_t offset,
 			dest += pagecpy;
 		}
 		offset += strpagelen;
+		index = offset >> PAGE_SHIFT;
 		if (strpagelen < pagelen)
 			break;
 		/*
@@ -625,14 +555,13 @@ EXPORT_SYMBOL_GPL(ltt_relay_read_cstr);
  * @buf : buffer
  * @offset : offset within the buffer
  */
-struct buf_page *ltt_relay_read_get_page(struct rchan_buf *buf, size_t offset)
+struct page *ltt_relay_read_get_page(struct rchan_buf *buf, size_t offset)
 {
-	struct buf_page *page;
+	size_t index;
 
 	offset &= buf->chan->alloc_size - 1;
-	page = buf->rpage;
-	page = ltt_relay_cache_page(buf, &buf->rpage, page, offset);
-	return page;
+	index = offset >> PAGE_SHIFT;
+	return buf->pages[index];
 }
 EXPORT_SYMBOL_GPL(ltt_relay_read_get_page);
 
@@ -648,55 +577,14 @@ EXPORT_SYMBOL_GPL(ltt_relay_read_get_page);
  */
 void *ltt_relay_offset_address(struct rchan_buf *buf, size_t offset)
 {
-	struct buf_page *page;
-	unsigned int odd;
+	size_t index;
 
 	offset &= buf->chan->alloc_size - 1;
-	odd = !!(offset & buf->chan->subbuf_size);
-	page = buf->hpage[odd];
-	if (offset < page->offset || offset >= page->offset + PAGE_SIZE)
-		buf->hpage[odd] = page = buf->wpage;
-	page = ltt_relay_cache_page(buf, &buf->hpage[odd], page, offset);
-	return page->virt + (offset & ~PAGE_MASK);
+	index = offset >> PAGE_SHIFT;
+	return buf->virt[index] + (offset & ~PAGE_MASK);
 }
 EXPORT_SYMBOL_GPL(ltt_relay_offset_address);
 
-
-extern struct buf_page *ltt_relay_cache_page_slow(struct rchan_buf *buf,
-		struct buf_page **page_cache,
-		struct buf_page *page, size_t offset)
-{
-	ssize_t diff_offset;
-	ssize_t half_buf_size = buf->chan->alloc_size >> 1;
-
-	/*
-	 * Make sure this is the page we want to write into. The current
-	 * page is changed concurrently by other writers. [wrh]page are
-	 * used as a cache remembering the last page written
-	 * to/read/looked up for header address. No synchronization;
-	 * could have to find the previous page is a nested write
-	 * occured. Finding the right page is done by comparing the
-	 * dest_offset with the buf_page offsets.
-	 * When at the exact opposite of the buffer, bias towards forward search
-	 * because it will be cached.
-	 */
-
-	diff_offset = (ssize_t)offset - (ssize_t)page->offset;
-	if (diff_offset <= -(ssize_t)half_buf_size)
-		diff_offset += buf->chan->alloc_size;
-	else if (diff_offset > half_buf_size)
-		diff_offset -= buf->chan->alloc_size;
-
-	if (unlikely(diff_offset >= (ssize_t)PAGE_SIZE)) {
-		page = ltt_relay_find_next_page(buf, page, offset, diff_offset);
-		*page_cache = page;
-	} else if (unlikely(diff_offset < 0)) {
-		page = ltt_relay_find_prev_page(buf, page, offset, diff_offset);
-	}
-	return page;
-}
-EXPORT_SYMBOL_GPL(ltt_relay_cache_page_slow);
-
 /**
  *	relay_file_open - open file op for relay files
  *	@inode: the inode
-- 
1.6.5.2

