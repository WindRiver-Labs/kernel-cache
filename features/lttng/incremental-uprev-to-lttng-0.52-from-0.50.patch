From b89030cb6f79cb1feeed35a45e2ca5bf69a114c0 Mon Sep 17 00:00:00 2001
From: Bruce Ashfield <bruce.ashfield@windriver.com>
Date: Fri, 7 Nov 2008 13:44:03 -0500
Subject: [PATCH] incremental uprev to lttng-0.52 from 0.50

This moves the effective version of lttng to 0.52 from
0.50 without refreshing the entire patch queue.

Signed-off-by: Bruce Ashfield <bruce.ashfield@windriver.com>
---
 arch/mips/Kconfig                      |    1 +
 arch/mips/kernel/Makefile              |    2 +
 arch/mips/kernel/smp.c                 |    1 +
 arch/mips/kernel/trace-clock.c         |  169 +++++++++++++++++++++++
 arch/powerpc/include/asm/trace-clock.h |    7 +
 arch/sh/include/asm/trace-clock.h      |    8 +
 arch/sparc/include/asm/trace-clock.h   |    7 +
 arch/x86/Kconfig                       |    5 +-
 arch/x86/kernel/Makefile               |    4 +-
 arch/x86/kernel/trace-clock.c          |  187 +++++++++++++++++++++++++-
 arch/x86/kernel/tsc_sync.c             |  189 --------------------------
 include/asm-generic/trace-clock.h      |    8 +
 include/asm-mips/barrier.h             |    6 +
 include/asm-mips/timex.h               |   32 ++++-
 include/asm-mips/trace-clock.h         |   22 +++-
 include/asm-x86/trace-clock.h          |   47 +------
 include/asm-x86/tsc.h                  |    8 +-
 include/linux/cnt32_to_63.h            |    7 +-
 init/Kconfig                           |    6 +-
 kernel/printk.c                        |    4 +-
 kernel/time/Makefile                   |    2 +-
 kernel/time/tsc-sync.c                 |  234 +++++++++++++++++++++++++-------
 kernel/trace/trace-clock-32-to-64.c    |   72 +++++++---
 ltt/ltt-tracer.c                       |    3 +
 ltt/probes/kernel-trace.c              |   28 ++--
 ltt/probes/ltt-type-serializer.h       |   13 ++
 ltt/probes/syscall-trace.c             |    6 +-
 ltt/probes/trap-trace.c                |    6 +-
 28 files changed, 745 insertions(+), 339 deletions(-)
 create mode 100644 arch/mips/kernel/trace-clock.c
 delete mode 100644 arch/x86/kernel/tsc_sync.c

diff --git a/arch/mips/Kconfig b/arch/mips/Kconfig
index 7c52ad6..c9f10fc 100644
--- a/arch/mips/Kconfig
+++ b/arch/mips/Kconfig
@@ -1593,6 +1593,7 @@ config HAVE_GET_CYCLES_32
 	depends on !CPU_R4400_WORKAROUNDS
 	select HAVE_TRACE_CLOCK
 	select HAVE_TRACE_CLOCK_32_TO_64
+	select HAVE_UNSYNCHRONIZED_TSC
 
 #
 # Use the generic interrupt handling code in kernel/irq/:
diff --git a/arch/mips/kernel/Makefile b/arch/mips/kernel/Makefile
index ebc9cf8..66bb20d 100644
--- a/arch/mips/kernel/Makefile
+++ b/arch/mips/kernel/Makefile
@@ -86,6 +86,8 @@ obj-$(CONFIG_GPIO_TXX9)		+= gpio_txx9.o
 obj-$(CONFIG_KEXEC)		+= machine_kexec.o relocate_kernel.o
 obj-$(CONFIG_EARLY_PRINTK)	+= early_printk.o
 
+obj-$(CONFIG_HAVE_GET_CYCLES_32)	+= trace-clock.o
+
 CFLAGS_cpu-bugs64.o	= $(shell if $(CC) $(KBUILD_CFLAGS) -Wa,-mdaddi -c -o /dev/null -xc /dev/null >/dev/null 2>&1; then echo "-DHAVE_AS_SET_DADDI"; fi)
 
 obj-$(CONFIG_HAVE_STD_PC_SERIAL_PORT)	+= 8250-platform.o
diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 6d3d197..e397d04 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -176,6 +176,7 @@ void __init smp_cpus_done(unsigned int max_cpus)
 {
 	mp_ops->cpus_done();
 	synchronise_count_master();
+	test_tsc_synchronization();
 }
 
 /* called from main before smp_init() */
diff --git a/arch/mips/kernel/trace-clock.c b/arch/mips/kernel/trace-clock.c
new file mode 100644
index 0000000..5d24809
--- /dev/null
+++ b/arch/mips/kernel/trace-clock.c
@@ -0,0 +1,169 @@
+/*
+ * arch/mips/kernel/trace-clock.c
+ *
+ * Trace clock for mips.
+ *
+ * Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>, October 2008
+ */
+
+#include <linux/module.h>
+#include <linux/trace-clock.h>
+#include <linux/jiffies.h>
+#include <linux/mutex.h>
+#include <linux/timer.h>
+#include <linux/spinlock.h>
+
+static u64 trace_clock_last_tsc;
+static DEFINE_PER_CPU(struct timer_list, update_timer);
+static DEFINE_MUTEX(async_tsc_mutex);
+static int async_tsc_refcount;	/* Number of readers */
+static int async_tsc_enabled;	/* Async TSC enabled on all online CPUs */
+
+/*
+ * Support for architectures with non-sync TSCs.
+ * When the local TSC is discovered to lag behind the highest TSC counter, we
+ * increment the TSC count of an amount that should be, ideally, lower than the
+ * execution time of this routine, in cycles : this is the granularity we look
+ * for : we must be able to order the events.
+ */
+
+#if BITS_PER_LONG == 64
+notrace u64 trace_clock_async_tsc_read(void)
+{
+	u64 new_tsc, last_tsc;
+
+	WARN_ON(!async_tsc_refcount || !async_tsc_enabled);
+	new_tsc = trace_clock_read_synthetic_tsc();
+	do {
+		last_tsc = trace_clock_last_tsc;
+		if (new_tsc < last_tsc)
+			new_tsc = last_tsc + TRACE_CLOCK_MIN_PROBE_DURATION;
+		/*
+		 * If cmpxchg fails with a value higher than the new_tsc, don't
+		 * retry : the value has been incremented and the events
+		 * happened almost at the same time.
+		 * We must retry if cmpxchg fails with a lower value :
+		 * it means that we are the CPU with highest frequency and
+		 * therefore MUST update the value.
+		 */
+	} while (cmpxchg64(&trace_clock_last_tsc, last_tsc, new_tsc) < new_tsc);
+	return new_tsc;
+}
+#else
+/*
+ * Emulate an atomic 64-bits update with a spinlock.
+ * Note : preempt_disable or irq save must be explicit with raw_spinlock_t.
+ * Given we use a spinlock for this time base, we should never be called from
+ * NMI context.
+ */
+static raw_spinlock_t trace_clock_lock =
+	(raw_spinlock_t)__RAW_SPIN_LOCK_UNLOCKED;
+
+static inline u64 trace_clock_cmpxchg64(u64 *ptr, u64 old, u64 new)
+{
+	u64 val;
+
+	val = *ptr;
+	if (likely(val == old))
+		*ptr = val = new;
+	return val;
+}
+
+notrace u64 trace_clock_async_tsc_read(void)
+{
+	u64 new_tsc, last_tsc;
+	unsigned long flags;
+
+	WARN_ON(!async_tsc_refcount || !async_tsc_enabled);
+	local_irq_save(flags);
+	__raw_spin_lock(&trace_clock_lock);
+	new_tsc = trace_clock_read_synthetic_tsc();
+	do {
+		last_tsc = trace_clock_last_tsc;
+		if (new_tsc < last_tsc)
+			new_tsc = last_tsc + TRACE_CLOCK_MIN_PROBE_DURATION;
+		/*
+		 * If cmpxchg fails with a value higher than the new_tsc, don't
+		 * retry : the value has been incremented and the events
+		 * happened almost at the same time.
+		 * We must retry if cmpxchg fails with a lower value :
+		 * it means that we are the CPU with highest frequency and
+		 * therefore MUST update the value.
+		 */
+	} while (trace_clock_cmpxchg64(&trace_clock_last_tsc, last_tsc,
+				       new_tsc) < new_tsc);
+	__raw_spin_unlock(&trace_clock_lock);
+	local_irq_restore(flags);
+	return new_tsc;
+}
+#endif
+
+EXPORT_SYMBOL_GPL(trace_clock_async_tsc_read);
+
+static void update_timer_ipi(void *info)
+{
+	(void)trace_clock_async_tsc_read();
+}
+
+/*
+ * update_timer_fct : - Timer function to resync the clocks
+ * @data: unused
+ *
+ * Fires every jiffy.
+ */
+static void update_timer_fct(unsigned long data)
+{
+	(void)trace_clock_async_tsc_read();
+
+	per_cpu(update_timer, smp_processor_id()).expires = jiffies + 1;
+	add_timer_on(&per_cpu(update_timer, smp_processor_id()),
+		     smp_processor_id());
+}
+
+static inline void enable_trace_clock(int cpu)
+{
+	init_timer(&per_cpu(update_timer, cpu));
+	per_cpu(update_timer, cpu).function = update_timer_fct;
+	per_cpu(update_timer, cpu).expires = jiffies + 1;
+	smp_call_function_single(cpu, update_timer_ipi, NULL, 1);
+	add_timer_on(&per_cpu(update_timer, cpu), cpu);
+}
+
+static inline void disable_trace_clock(int cpu)
+{
+	del_timer_sync(&per_cpu(update_timer, cpu));
+}
+
+void get_trace_clock(void)
+{
+	int cpu;
+
+	mutex_lock(&async_tsc_mutex);
+	if (async_tsc_refcount++ || tsc_is_sync())
+		goto end;
+
+	async_tsc_enabled = 1;
+	for_each_online_cpu(cpu)
+		enable_trace_clock(cpu);
+end:
+	mutex_unlock(&async_tsc_mutex);
+}
+EXPORT_SYMBOL_GPL(get_trace_clock);
+
+void put_trace_clock(void)
+{
+	int cpu;
+
+	mutex_lock(&async_tsc_mutex);
+	WARN_ON(async_tsc_refcount <= 0);
+	if (async_tsc_refcount != 1 || !async_tsc_enabled)
+		goto end;
+
+	for_each_online_cpu(cpu)
+		disable_trace_clock(cpu);
+	async_tsc_enabled = 0;
+end:
+	async_tsc_refcount--;
+	mutex_unlock(&async_tsc_mutex);
+}
+EXPORT_SYMBOL_GPL(put_trace_clock);
diff --git a/arch/powerpc/include/asm/trace-clock.h b/arch/powerpc/include/asm/trace-clock.h
index 97127cd..624f104 100644
--- a/arch/powerpc/include/asm/trace-clock.h
+++ b/arch/powerpc/include/asm/trace-clock.h
@@ -37,4 +37,11 @@ static inline u32 trace_clock_freq_scale(void)
 	return 1;
 }
 
+static inline void get_trace_clock(void)
+{
+}
+
+static inline void put_trace_clock(void)
+{
+}
 #endif /* _ASM_TRACE_CLOCK_H */
diff --git a/arch/sh/include/asm/trace-clock.h b/arch/sh/include/asm/trace-clock.h
index 994a35f..f46d49b 100644
--- a/arch/sh/include/asm/trace-clock.h
+++ b/arch/sh/include/asm/trace-clock.h
@@ -41,4 +41,12 @@ static inline u32 trace_clock_freq_scale(void)
 {
 	return 1;
 }
+
+static inline void get_trace_clock(void)
+{
+}
+
+static inline void put_trace_clock(void)
+{
+}
 #endif /* _ASM_SH_TRACE_CLOCK_H */
diff --git a/arch/sparc/include/asm/trace-clock.h b/arch/sparc/include/asm/trace-clock.h
index c4ab947..e0b36ee 100644
--- a/arch/sparc/include/asm/trace-clock.h
+++ b/arch/sparc/include/asm/trace-clock.h
@@ -32,4 +32,11 @@ static inline u32 trace_clock_freq_scale(void)
 	return 1;
 }
 
+static inline void get_trace_clock(void)
+{
+}
+
+static inline void put_trace_clock(void)
+{
+}
 #endif /* _ASM_SPARC_TRACE_CLOCK_H */
diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
index 9e51583..435dcaa 100644
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@ -29,9 +29,8 @@ config X86
 	select ARCH_WANT_OPTIONAL_GPIOLIB
 	select HAVE_KRETPROBES
 	select HAVE_TRACE_CLOCK
-	select HAVE_UNSTABLE_TSC
-	select HAVE_LTT_DUMP_TABLES
 	select HAVE_DYNAMIC_FTRACE
+	select HAVE_LTT_DUMP_TABLES
 	select HAVE_FTRACE
 	select HAVE_KVM if ((X86_32 && !X86_VOYAGER && !X86_VISWS && !X86_NUMAQ) || X86_64)
 	select HAVE_ARCH_KGDB if !X86_VOYAGER
@@ -183,6 +182,7 @@ config X86_SMP
 	bool
 	depends on SMP && ((X86_32 && !X86_VOYAGER) || X86_64)
 	select USE_GENERIC_SMP_HELPERS
+	select HAVE_UNSYNCHRONIZED_TSC
 	default y
 
 config X86_32_SMP
@@ -192,6 +192,7 @@ config X86_32_SMP
 config X86_64_SMP
 	def_bool y
 	depends on X86_64 && SMP
+	select HAVE_UNSYNCHRONIZED_TSC
 
 config X86_HT
 	bool
diff --git a/arch/x86/kernel/Makefile b/arch/x86/kernel/Makefile
index 81310f2..749d38d 100644
--- a/arch/x86/kernel/Makefile
+++ b/arch/x86/kernel/Makefile
@@ -57,9 +57,9 @@ obj-$(CONFIG_PCI)		+= early-quirks.o
 apm-y				:= apm_32.o
 obj-$(CONFIG_APM)		+= apm.o
 obj-$(CONFIG_X86_SMP)		+= smp.o
-obj-$(CONFIG_X86_SMP)		+= smpboot.o tsc_sync.o ipi.o tlb_$(BITS).o
+obj-$(CONFIG_X86_SMP)		+= smpboot.o ipi.o tlb_$(BITS).o
 obj-$(CONFIG_X86_32_SMP)	+= smpcommon.o
-obj-$(CONFIG_X86_64_SMP)	+= tsc_sync.o smpcommon.o
+obj-$(CONFIG_X86_64_SMP)	+= smpcommon.o
 obj-$(CONFIG_X86_TRAMPOLINE)	+= trampoline_$(BITS).o
 obj-$(CONFIG_X86_MPPARSE)	+= mpparse.o
 obj-$(CONFIG_X86_LOCAL_APIC)	+= apic_$(BITS).o nmi.o
diff --git a/arch/x86/kernel/trace-clock.c b/arch/x86/kernel/trace-clock.c
index 9ff9ff2..8be77b9 100644
--- a/arch/x86/kernel/trace-clock.c
+++ b/arch/x86/kernel/trace-clock.c
@@ -8,8 +8,44 @@
 
 #include <linux/module.h>
 #include <linux/trace-clock.h>
+#include <linux/jiffies.h>
+#include <linux/mutex.h>
+#include <linux/timer.h>
+#include <linux/cpu.h>
 
 static cycles_t trace_clock_last_tsc;
+static DEFINE_PER_CPU(struct timer_list, update_timer);
+static DEFINE_MUTEX(async_tsc_mutex);
+static int async_tsc_refcount;	/* Number of readers */
+static int async_tsc_enabled;	/* Async TSC enabled on all online CPUs */
+
+#if BITS_PER_LONG == 64
+static cycles_t read_last_tsc(void)
+{
+	return trace_clock_last_tsc;
+}
+#else
+/*
+ * A cmpxchg64 update can happen concurrently. Based on the assumption that
+ * two cmpxchg64 will never update it to the same value (the count always
+ * increases), reading it twice insures that we read a coherent value with the
+ * same "sequence number".
+ */
+static cycles_t read_last_tsc(void)
+{
+	cycles_t val1, val2;
+
+	val1 = trace_clock_last_tsc;
+	for (;;) {
+		val2 = val1;
+		barrier();
+		val1 = trace_clock_last_tsc;
+		if (likely(val1 == val2))
+			break;
+	}
+	return val1;
+}
+#endif
 
 /*
  * Support for architectures with non-sync TSCs.
@@ -20,14 +56,14 @@ static cycles_t trace_clock_last_tsc;
  */
 notrace cycles_t trace_clock_async_tsc_read(void)
 {
-	cycles_t new_tsc;
-	cycles_t last_tsc;
+	cycles_t new_tsc, last_tsc;
 
+	WARN_ON(!async_tsc_refcount || !async_tsc_enabled);
 	rdtsc_barrier();
 	new_tsc = get_cycles();
 	rdtsc_barrier();
+	last_tsc = read_last_tsc();
 	do {
-		last_tsc = trace_clock_last_tsc;
 		if (new_tsc < last_tsc)
 			new_tsc = last_tsc + TRACE_CLOCK_MIN_PROBE_DURATION;
 		/*
@@ -38,7 +74,150 @@ notrace cycles_t trace_clock_async_tsc_read(void)
 		 * it means that we are the CPU with highest frequency and
 		 * therefore MUST update the value.
 		 */
-	} while (cmpxchg64(&trace_clock_last_tsc, last_tsc, new_tsc) < new_tsc);
+		last_tsc = cmpxchg64(&trace_clock_last_tsc, last_tsc, new_tsc);
+	} while (unlikely(last_tsc < new_tsc));
 	return new_tsc;
 }
 EXPORT_SYMBOL_GPL(trace_clock_async_tsc_read);
+
+static void update_timer_ipi(void *info)
+{
+	(void)trace_clock_async_tsc_read();
+}
+
+/*
+ * update_timer_fct : - Timer function to resync the clocks
+ * @data: unused
+ *
+ * Fires every jiffy.
+ */
+static void update_timer_fct(unsigned long data)
+{
+	(void)trace_clock_async_tsc_read();
+
+	per_cpu(update_timer, smp_processor_id()).expires = jiffies + 1;
+	add_timer_on(&per_cpu(update_timer, smp_processor_id()),
+		     smp_processor_id());
+}
+
+static inline void enable_trace_clock(int cpu)
+{
+	init_timer(&per_cpu(update_timer, cpu));
+	per_cpu(update_timer, cpu).function = update_timer_fct;
+	per_cpu(update_timer, cpu).expires = jiffies + 1;
+	smp_call_function_single(cpu, update_timer_ipi, NULL, 1);
+	add_timer_on(&per_cpu(update_timer, cpu), cpu);
+}
+
+static inline void disable_trace_clock(int cpu)
+{
+	del_timer_sync(&per_cpu(update_timer, cpu));
+}
+
+/*
+ * 	hotcpu_callback - CPU hotplug callback
+ * 	@nb: notifier block
+ * 	@action: hotplug action to take
+ * 	@hcpu: CPU number
+ *
+ * 	Returns the success/failure of the operation. (NOTIFY_OK, NOTIFY_BAD)
+ */
+static int __cpuinit hotcpu_callback(struct notifier_block *nb,
+				unsigned long action,
+				void *hcpu)
+{
+	unsigned int hotcpu = (unsigned long)hcpu;
+	int cpu;
+
+	mutex_lock(&async_tsc_mutex);
+	if (!async_tsc_refcount)
+		goto end;
+	switch (action) {
+	case CPU_UP_PREPARE:
+	case CPU_UP_PREPARE_FROZEN:
+		break;
+	case CPU_ONLINE:
+	case CPU_ONLINE_FROZEN:
+		/*
+		 * tsc_is_sync() is updated by test-tsc code, protected by
+		 * cpu hotplug disable.
+		 * It is ok to let the hotplugged CPU read the timebase before
+		 * the CPU_ONLINE notification. It's just there to give a
+		 * maximum bound to the TSC error.
+		 */
+		if (!tsc_is_sync()) {
+			if (!async_tsc_enabled) {
+				async_tsc_enabled = 1;
+				for_each_online_cpu(cpu)
+					enable_trace_clock(cpu);
+			} else {
+				enable_trace_clock(hotcpu);
+			}
+		}
+		break;
+#ifdef CONFIG_HOTPLUG_CPU
+	case CPU_UP_CANCELED:
+	case CPU_UP_CANCELED_FROZEN:
+		break;
+	case CPU_DEAD:
+	case CPU_DEAD_FROZEN:
+		/*
+		 * We cannot stop the trace clock on other CPUs even if we
+		 * go back to a synchronized state (1 CPU) because this CPU
+		 * could be the one lagging behind.
+		 */
+		if (async_tsc_enabled)
+			disable_trace_clock(hotcpu);
+		break;
+#endif /* CONFIG_HOTPLUG_CPU */
+	}
+end:
+	mutex_unlock(&async_tsc_mutex);
+
+	return NOTIFY_OK;
+}
+
+void get_trace_clock(void)
+{
+	int cpu;
+
+	get_online_cpus();
+	mutex_lock(&async_tsc_mutex);
+	if (async_tsc_refcount++ || tsc_is_sync())
+		goto end;
+
+	async_tsc_enabled = 1;
+	for_each_online_cpu(cpu)
+		enable_trace_clock(cpu);
+end:
+	mutex_unlock(&async_tsc_mutex);
+	put_online_cpus();
+}
+EXPORT_SYMBOL_GPL(get_trace_clock);
+
+void put_trace_clock(void)
+{
+	int cpu;
+
+	get_online_cpus();
+	mutex_lock(&async_tsc_mutex);
+	WARN_ON(async_tsc_refcount <= 0);
+	if (async_tsc_refcount != 1 || !async_tsc_enabled)
+		goto end;
+
+	for_each_online_cpu(cpu)
+		disable_trace_clock(cpu);
+	async_tsc_enabled = 0;
+end:
+	async_tsc_refcount--;
+	mutex_unlock(&async_tsc_mutex);
+	put_online_cpus();
+}
+EXPORT_SYMBOL_GPL(put_trace_clock);
+
+static __init int init_unsync_trace_clock(void)
+{
+	hotcpu_notifier(hotcpu_callback, 4);
+	return 0;
+}
+early_initcall(init_unsync_trace_clock);
diff --git a/arch/x86/kernel/tsc_sync.c b/arch/x86/kernel/tsc_sync.c
deleted file mode 100644
index 9ffb01c..0000000
--- a/arch/x86/kernel/tsc_sync.c
+++ /dev/null
@@ -1,189 +0,0 @@
-/*
- * check TSC synchronization.
- *
- * Copyright (C) 2006, Red Hat, Inc., Ingo Molnar
- *
- * We check whether all boot CPUs have their TSC's synchronized,
- * print a warning if not and turn off the TSC clock-source.
- *
- * The warp-check is point-to-point between two CPUs, the CPU
- * initiating the bootup is the 'source CPU', the freshly booting
- * CPU is the 'target CPU'.
- *
- * Only two CPUs may participate - they can enter in any order.
- * ( The serial nature of the boot logic and the CPU hotplug lock
- *   protects against more than 2 CPUs entering this code. )
- */
-#include <linux/spinlock.h>
-#include <linux/kernel.h>
-#include <linux/init.h>
-#include <linux/smp.h>
-#include <linux/nmi.h>
-#include <asm/tsc.h>
-
-/*
- * Entry/exit counters that make sure that both CPUs
- * run the measurement code at once:
- */
-static __cpuinitdata atomic_t start_count;
-static __cpuinitdata atomic_t stop_count;
-
-/*
- * We use a raw spinlock in this exceptional case, because
- * we want to have the fastest, inlined, non-debug version
- * of a critical section, to be able to prove TSC time-warps:
- */
-static __cpuinitdata raw_spinlock_t sync_lock = __RAW_SPIN_LOCK_UNLOCKED;
-static __cpuinitdata cycles_t last_tsc;
-static __cpuinitdata cycles_t max_warp;
-static __cpuinitdata int nr_warps;
-
-/*
- * TSC-warp measurement loop running on both CPUs:
- */
-static __cpuinit void check_tsc_warp(void)
-{
-	cycles_t start, now, prev, end;
-	int i;
-
-	start = get_cycles();
-	/*
-	 * The measurement runs for 20 msecs:
-	 */
-	end = start + tsc_khz * 20ULL;
-	now = start;
-
-	for (i = 0; ; i++) {
-		/*
-		 * We take the global lock, measure TSC, save the
-		 * previous TSC that was measured (possibly on
-		 * another CPU) and update the previous TSC timestamp.
-		 */
-		__raw_spin_lock(&sync_lock);
-		prev = last_tsc;
-		now = get_cycles();
-		last_tsc = now;
-		__raw_spin_unlock(&sync_lock);
-
-		/*
-		 * Be nice every now and then (and also check whether
-		 * measurement is done [we also insert a 10 million
-		 * loops safety exit, so we dont lock up in case the
-		 * TSC readout is totally broken]):
-		 */
-		if (unlikely(!(i & 7))) {
-			if (now > end || i > 10000000)
-				break;
-			cpu_relax();
-			touch_nmi_watchdog();
-		}
-		/*
-		 * Outside the critical section we can now see whether
-		 * we saw a time-warp of the TSC going backwards:
-		 */
-		if (unlikely(prev > now)) {
-			__raw_spin_lock(&sync_lock);
-			max_warp = max(max_warp, prev - now);
-			nr_warps++;
-			__raw_spin_unlock(&sync_lock);
-		}
-	}
-	WARN(!(now-start),
-		"Warning: zero tsc calibration delta: %Ld [max: %Ld]\n",
-			now-start, end-start);
-}
-
-/*
- * Source CPU calls into this - it waits for the freshly booted
- * target CPU to arrive and then starts the measurement:
- */
-void __cpuinit check_tsc_sync_source(int cpu)
-{
-	int cpus = 2;
-
-	/*
-	 * No need to check if we already know that the TSC is not
-	 * synchronized:
-	 */
-	if (unsynchronized_tsc())
-		return;
-
-	printk(KERN_INFO "checking TSC synchronization [CPU#%d -> CPU#%d]:",
-			  smp_processor_id(), cpu);
-
-	/*
-	 * Reset it - in case this is a second bootup:
-	 */
-	atomic_set(&stop_count, 0);
-
-	/*
-	 * Wait for the target to arrive:
-	 */
-	while (atomic_read(&start_count) != cpus-1)
-		cpu_relax();
-	/*
-	 * Trigger the target to continue into the measurement too:
-	 */
-	atomic_inc(&start_count);
-
-	check_tsc_warp();
-
-	while (atomic_read(&stop_count) != cpus-1)
-		cpu_relax();
-
-	if (nr_warps) {
-		printk("\n");
-		printk(KERN_WARNING "Measured %Ld cycles TSC warp between CPUs,"
-				    " turning off TSC clock.\n", max_warp);
-		mark_tsc_unstable("check_tsc_sync_source failed");
-	} else {
-		printk(" passed.\n");
-	}
-
-	/*
-	 * Reset it - just in case we boot another CPU later:
-	 */
-	atomic_set(&start_count, 0);
-	nr_warps = 0;
-	max_warp = 0;
-	last_tsc = 0;
-
-	/*
-	 * Let the target continue with the bootup:
-	 */
-	atomic_inc(&stop_count);
-}
-
-/*
- * Freshly booted CPUs call into this:
- */
-void __cpuinit check_tsc_sync_target(void)
-{
-	int cpus = 2;
-
-	if (unsynchronized_tsc())
-		return;
-
-	/*
-	 * Register this CPU's participation and wait for the
-	 * source CPU to start the measurement:
-	 */
-	atomic_inc(&start_count);
-	while (atomic_read(&start_count) != cpus)
-		cpu_relax();
-
-	check_tsc_warp();
-
-	/*
-	 * Ok, we are done:
-	 */
-	atomic_inc(&stop_count);
-
-	/*
-	 * Wait for the source CPU to print stuff:
-	 */
-	while (atomic_read(&stop_count) != cpus)
-		cpu_relax();
-}
-#undef NR_LOOPS
-
diff --git a/include/asm-generic/trace-clock.h b/include/asm-generic/trace-clock.h
index 9629b2c..770edd2 100644
--- a/include/asm-generic/trace-clock.h
+++ b/include/asm-generic/trace-clock.h
@@ -49,4 +49,12 @@ static inline u32 trace_clock_freq_scale(void)
 {
 	return 1;
 }
+
+static inline void get_trace_clock(void)
+{
+}
+
+static inline void put_trace_clock(void)
+{
+}
 #endif /* _ASM_GENERIC_TRACE_CLOCK_H */
diff --git a/include/asm-mips/barrier.h b/include/asm-mips/barrier.h
index 8e9ac31..e787634 100644
--- a/include/asm-mips/barrier.h
+++ b/include/asm-mips/barrier.h
@@ -152,4 +152,10 @@
 #define smp_llsc_rmb()	__asm__ __volatile__(__WEAK_LLSC_MB : : :"memory")
 #define smp_llsc_wmb()	__asm__ __volatile__(__WEAK_LLSC_MB : : :"memory")
 
+/*
+ * MIPS does not have any instruction to serialize instruction execution on the
+ * core.
+ */
+#define sync_core()
+
 #endif /* __ASM_BARRIER_H */
diff --git a/include/asm-mips/timex.h b/include/asm-mips/timex.h
index b0ad5f1..e956698 100644
--- a/include/asm-mips/timex.h
+++ b/include/asm-mips/timex.h
@@ -42,7 +42,7 @@
 
 typedef unsigned int cycles_t;
 
-#ifdef HAVE_GET_CYCLES_32
+#ifdef CONFIG_HAVE_GET_CYCLES_32
 static inline cycles_t get_cycles(void)
 {
 	return read_c0_count();
@@ -56,15 +56,45 @@ static inline cycles_t get_cycles_rate(void)
 {
 	return CLOCK_TICK_RATE;
 }
+
+extern int test_tsc_synchronization(void);
+extern int _tsc_is_sync;
+static inline int tsc_is_sync(void)
+{
+	return _tsc_is_sync;
+}
 #else
 static inline cycles_t get_cycles(void)
 {
 	return 0;
 }
+static inline int test_tsc_synchronization(void)
+{
+	return 0;
+}
+static inline int tsc_is_sync(void)
+{
+	return 0;
+}
 #endif
 
 extern unsigned int mips_hpt_frequency;
 
+/*
+ * Currently unused, should update internal tsc-related timekeeping sources.
+ */
+static inline void mark_tsc_unstable(char *reason)
+{
+}
+
+/*
+ * Currently simply use the tsc_is_sync value.
+ */
+static inline int unsynchronized_tsc(void)
+{
+	return !tsc_is_sync();
+}
+
 #endif /* __KERNEL__ */
 
 #endif /*  _ASM_TIMEX_H */
diff --git a/include/asm-mips/trace-clock.h b/include/asm-mips/trace-clock.h
index 4c96cb5..0b09cf5 100644
--- a/include/asm-mips/trace-clock.h
+++ b/include/asm-mips/trace-clock.h
@@ -10,6 +10,8 @@
 #include <linux/timex.h>
 #include <asm/processor.h>
 
+#define TRACE_CLOCK_MIN_PROBE_DURATION 200
+
 extern u64 trace_clock_read_synthetic_tsc(void);
 
 /*
@@ -18,14 +20,28 @@ extern u64 trace_clock_read_synthetic_tsc(void);
  * tracing needs to detect delays longer than 8 seconds, we need a full 64-bits
  * TSC, whic is provided by trace-clock-32-to-64.
 */
+extern u64 trace_clock_async_tsc_read(void);
+
 static inline u32 trace_clock_read32(void)
 {
-	return get_cycles();
+	u32 cycles;
+
+	if (likely(tsc_is_sync())) {
+		cycles = (u32)get_cycles(); /* only need the 32 LSB */
+	} else
+		cycles = (u32)trace_clock_async_tsc_read();
+	return cycles;
 }
 
 static inline u64 trace_clock_read64(void)
 {
-	return trace_clock_read_synthetic_tsc();
+	u64 cycles;
+
+	if (likely(tsc_is_sync())) {
+		cycles = trace_clock_read_synthetic_tsc();
+	} else
+		cycles = trace_clock_async_tsc_read();
+	return cycles;
 }
 
 static inline void trace_clock_add_timestamp(unsigned long ticks)
@@ -41,4 +57,6 @@ static inline u32 trace_clock_freq_scale(void)
 	return 1;
 }
 
+extern void get_trace_clock(void);
+extern void put_trace_clock(void);
 #endif /* _ASM_MIPS_TRACE_CLOCK_H */
diff --git a/include/asm-x86/trace-clock.h b/include/asm-x86/trace-clock.h
index 867bb69..9cabab7 100644
--- a/include/asm-x86/trace-clock.h
+++ b/include/asm-x86/trace-clock.h
@@ -18,24 +18,16 @@
 /* Minimum duration of a probe, in cycles */
 #define TRACE_CLOCK_MIN_PROBE_DURATION 200
 
-#ifdef CONFIG_HAVE_TRACE_CLOCK_32_TO_64
-/* Only for testing. Never needed on x86. */
-u64 trace_clock_read_synthetic_tsc(void);
-#endif
-
-#ifdef CONFIG_HAVE_UNSTABLE_TSC
-extern int tsc_is_sync;
-
 extern cycles_t trace_clock_async_tsc_read(void);
 
 static inline u32 trace_clock_read32(void)
 {
 	u32 cycles;
 
-	if (likely(tsc_is_sync)) {
-		rdtsc_barrier();
+	if (likely(tsc_is_sync())) {
+		get_cycles_barrier();
 		cycles = (u32)get_cycles(); /* only need the 32 LSB */
-		rdtsc_barrier();
+		get_cycles_barrier();
 	} else
 		cycles = (u32)trace_clock_async_tsc_read();
 	return cycles;
@@ -45,40 +37,15 @@ static inline u64 trace_clock_read64(void)
 {
 	u64 cycles;
 
-	if (likely(tsc_is_sync)) {
-		rdtsc_barrier();
+	if (likely(tsc_is_sync())) {
+		get_cycles_barrier();
 		cycles = get_cycles();
-		rdtsc_barrier();
+		get_cycles_barrier();
 	} else
 		cycles = trace_clock_async_tsc_read();
 	return cycles;
 }
-#else /* CONFIG_HAVE_UNSTABLE_TSC */
-static inline u32 trace_clock_read32(void)
-{
-	u32 cycles;
 
-	rdtsc_barrier();
-	cycles = (u32)get_cycles(); /* only need the 32 LSB */
-	rdtsc_barrier();
-	return cycles;
-}
-
-static inline u64 trace_clock_read64(void)
-{
-	u64 cycles;
-
-	rdtsc_barrier();
-	cycles = get_cycles();
-	rdtsc_barrier();
-	return cycles;
-}
-#endif /* CONFIG_HAVE_UNSTABLE_TSC */
-
-/*
- * Periodic IPI to have an upper bound on TSC inaccuracy.
- * TODO: should implement this in arch/x86/kernel/trace-clock.c.
- */
 static inline void trace_clock_add_timestamp(unsigned long ticks)
 { }
 
@@ -92,4 +59,6 @@ static inline u32 trace_clock_freq_scale(void)
 	return 1000;
 }
 
+extern void get_trace_clock(void);
+extern void put_trace_clock(void);
 #endif /* _ASM_X86_TRACE_CLOCK_H */
diff --git a/include/asm-x86/tsc.h b/include/asm-x86/tsc.h
index 195e417..f7ab5f1 100644
--- a/include/asm-x86/tsc.h
+++ b/include/asm-x86/tsc.h
@@ -48,7 +48,7 @@ static __always_inline cycles_t vget_cycles(void)
 extern void tsc_init(void);
 extern void mark_tsc_unstable(char *reason);
 extern int unsynchronized_tsc(void);
-int check_tsc_unstable(void);
+extern int check_tsc_unstable(void);
 
 static inline cycles_t get_cycles_rate(void)
 {
@@ -71,4 +71,10 @@ extern void check_tsc_sync_target(void);
 
 extern int notsc_setup(char *);
 
+extern int test_tsc_synchronization(void);
+extern int _tsc_is_sync;
+static inline int tsc_is_sync(void)
+{
+	return _tsc_is_sync;
+}
 #endif
diff --git a/include/linux/cnt32_to_63.h b/include/linux/cnt32_to_63.h
index 8c0f950..31a59c2 100644
--- a/include/linux/cnt32_to_63.h
+++ b/include/linux/cnt32_to_63.h
@@ -65,12 +65,17 @@ union cnt32_to_63 {
  * implicitly by making the multiplier even, therefore saving on a runtime
  * clear-bit instruction. Otherwise caller must remember to clear the top
  * bit explicitly.
+ *
+ * Assume the time source is a global clock read from memory mapped I/O which
+ * insures that time will never *ever* go backward. Using a smp_rmb() to make
+ * sure the __m_cnt_hi value is read before the cnt_lo mmio read.
  */
 #define cnt32_to_63(cnt_lo) \
 ({ \
-	static volatile u32 __m_cnt_hi; \
+	static u32 __m_cnt_hi; \
 	union cnt32_to_63 __x; \
 	__x.hi = __m_cnt_hi; \
+	smp_rmb(); 	/* read __m_cnt_hi before mmio cnt_lo */ \
 	__x.lo = (cnt_lo); \
 	if (unlikely((s32)(__x.hi ^ __x.lo) < 0)) \
 		__m_cnt_hi = __x.hi = (__x.hi ^ 0x80000000) + (__x.hi >> 31); \
diff --git a/init/Kconfig b/init/Kconfig
index 182303f..e2f4fa7 100644
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -359,10 +359,10 @@ config HAVE_TRACE_CLOCK_32_TO_64
 	default n if HAVE_TRACE_CLOCK
 
 #
-# Architectures which need to dynamically detect if their TSC is unstable should
-# select this.
+# Architectures which need to dynamically detect if their TSC is unsynchronized
+# across cpus should select this.
 #
-config HAVE_UNSTABLE_TSC
+config HAVE_UNSYNCHRONIZED_TSC
 	def_bool n
 
 config GROUP_SCHED
diff --git a/kernel/printk.c b/kernel/printk.c
index aaeef5c..4b2bc18 100644
--- a/kernel/printk.c
+++ b/kernel/printk.c
@@ -736,8 +736,6 @@ asmlinkage int vprintk(const char *fmt, va_list args)
 	raw_local_irq_save(flags);
 	this_cpu = smp_processor_id();
 
-	_trace_kernel_vprintk(_RET_IP_, printk_buf, printed_len);
-
 	/*
 	 * Ouch, printk recursed into itself!
 	 */
@@ -773,6 +771,8 @@ asmlinkage int vprintk(const char *fmt, va_list args)
 	printascii(printk_buf);
 #endif
 
+	_trace_kernel_vprintk(_RET_IP_, printk_buf, printed_len);
+
 	/*
 	 * Copy the output into log_buf.  If the caller didn't provide
 	 * appropriate log level tags, we insert them here
diff --git a/kernel/time/Makefile b/kernel/time/Makefile
index f1f3e7a..d1dc6ee 100644
--- a/kernel/time/Makefile
+++ b/kernel/time/Makefile
@@ -6,4 +6,4 @@ obj-$(CONFIG_GENERIC_CLOCKEVENTS_BROADCAST)	+= tick-broadcast.o
 obj-$(CONFIG_TICK_ONESHOT)			+= tick-oneshot.o
 obj-$(CONFIG_TICK_ONESHOT)			+= tick-sched.o
 obj-$(CONFIG_TIMER_STATS)			+= timer_stats.o
-obj-$(CONFIG_HAVE_UNSTABLE_TSC)			+= tsc-sync.o
+obj-$(CONFIG_HAVE_UNSYNCHRONIZED_TSC)		+= tsc-sync.o
diff --git a/kernel/time/tsc-sync.c b/kernel/time/tsc-sync.c
index f0fb00b..598feb7 100644
--- a/kernel/time/tsc-sync.c
+++ b/kernel/time/tsc-sync.c
@@ -3,6 +3,24 @@
  *
  * Test TSC synchronization
  *
+ * marks the tsc as unstable _and_ keep a simple "_tsc_is_sync" variable, which
+ * is fast to read when a simple test must determine which clock source to use
+ * for kernel tracing.
+ *
+ * - CPU init :
+ *
+ * We check whether all boot CPUs have their TSC's synchronized,
+ * print a warning if not and turn off the TSC clock-source.
+ *
+ * Only two CPUs may participate - they can enter in any order.
+ * ( The serial nature of the boot logic and the CPU hotplug lock
+ *   protects against more than 2 CPUs entering this code.
+ *
+ * - When CPUs are up :
+ *
+ * TSC synchronicity of all CPUs can be checked later at run-time by calling
+ * test_tsc_synchronization().
+ *
  * Copyright 2007, 2008
  *    Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
  */
@@ -13,119 +31,231 @@
 #include <linux/cpu.h>
 #include <linux/kthread.h>
 #include <linux/mutex.h>
+#include <linux/cpu.h>
 
 #define MAX_CYCLES_DELTA 1000ULL
 
-static DEFINE_PER_CPU(cycles_t, tsc_count);
+/*
+ * Number of loops to take care of MCE, NMIs, SMIs.
+ */
+#define NR_LOOPS	10
+
 static DEFINE_MUTEX(tscsync_mutex);
 
-static DEFINE_PER_CPU(int, wait_sync);
-static DEFINE_PER_CPU(int, wait_end_sync);
+struct sync_data {
+	int nr_waits;
+	int wait_sync;
+	cycles_t tsc_count;
+} ____cacheline_aligned;
 
-int tsc_is_sync = 1;
-EXPORT_SYMBOL(tsc_is_sync);
+/* 0 is master, 1 is slave */
+static struct sync_data sync_data[2] = {
+	[0 ... 1] = {
+		.nr_waits = 3 * NR_LOOPS + 1,
+		.wait_sync = 3 * NR_LOOPS + 1,
+	},
+};
+
+int _tsc_is_sync = 1;
+EXPORT_SYMBOL(_tsc_is_sync);
 
 /*
  * Mark it noinline so we make sure it is not unrolled.
  * Wait until value is reached.
  */
-static noinline void tsc_barrier(long wait_cpu, int value)
+static noinline void tsc_barrier(long this_cpu)
 {
 	sync_core();
-	per_cpu(wait_sync, smp_processor_id())--;
+	sync_data[this_cpu].wait_sync--;
 	do {
 		smp_mb();
-	} while (unlikely(per_cpu(wait_sync, wait_cpu) > value));
-	rdtsc_barrier();
-	__get_cpu_var(tsc_count) = get_cycles();
-	rdtsc_barrier();
+	} while (unlikely(sync_data[1 - this_cpu].wait_sync >=
+		 sync_data[this_cpu].nr_waits));
+	sync_data[this_cpu].nr_waits--;
+	get_cycles_barrier();
+	sync_data[this_cpu].tsc_count = get_cycles();
+	get_cycles_barrier();
 }
 
 /*
  * Worker thread called on each CPU.
  * First wait with interrupts enabled, then wait with interrupt disabled,
  * for precision. We are already bound to one CPU.
+ * this_cpu 0 : master
+ * this_cpu 1 : slave
  */
 static void test_sync(void *arg)
 {
-	long wait_cpu = (long)arg;
+	long this_cpu = (long)arg;
 	unsigned long flags;
 
 	local_irq_save(flags);
 	/* Make sure the instructions are in I-CACHE */
-	tsc_barrier(wait_cpu, 1);
-	tsc_barrier(wait_cpu, 0);
-	per_cpu(wait_end_sync, smp_processor_id())--;
+	tsc_barrier(this_cpu);
+	tsc_barrier(this_cpu);
+	sync_data[this_cpu].wait_sync--;
 	do {
 		smp_mb();
-	} while (unlikely(per_cpu(wait_end_sync, wait_cpu) > 1));
-	per_cpu(wait_end_sync, smp_processor_id())--;
+	} while (unlikely(sync_data[1 - this_cpu].wait_sync >=
+			  sync_data[this_cpu].nr_waits));
+	sync_data[this_cpu].nr_waits--;
+	/*
+	 * Here, only the master will wait for the slave to reach this barrier.
+	 * This makes sure that the master, which holds the mutex and will reset
+	 * the barriers, waits for the slave to stop using the barrier values
+	 * before it continues. This is only done at the complete end of all the
+	 * loops. This is why there is a + 1 in original wait_sync value.
+	 */
+	if (sync_data[this_cpu].nr_waits == 1)
+		sync_data[this_cpu].wait_sync--;
 	local_irq_restore(flags);
 }
 
 /*
- * Do loops (making sure no unexpected event changes the timing), keep the
- * best one. The result of each loop is the highest tsc delta between the
- * master CPU and the slaves.
+ * Each CPU (master and target) must decrement the wait_sync value twice (one
+ * for priming in cache), and also once after the get_cycles. After all the
+ * loops, one last synchronization is required to make sure the master waits
+ * for the slave before resetting the barriers.
+ */
+static void reset_barriers(void)
+{
+	int i;
+
+	/*
+	 * Wait until slave is done so that we don't overwrite
+	 * wait_end_sync prematurely.
+	 */
+	while (unlikely(sync_data[1].wait_sync >= sync_data[0].nr_waits))
+		cpu_relax();
+
+	for (i = 0; i < 2; i++) {
+		WARN_ON(sync_data[i].wait_sync != 0);
+		WARN_ON(sync_data[i].nr_waits != 1);
+		sync_data[i].wait_sync = 3 * NR_LOOPS + 1;
+		sync_data[i].nr_waits = 3 * NR_LOOPS + 1;
+	}
+}
+
+/*
+ * Do loops (making sure no unexpected event changes the timing), keep the best
+ * one. The result of each loop is the highest tsc delta between the master CPU
+ * and the slaves. Stop CPU hotplug when this code is executed to make sure we
+ * are concurrency-safe wrt CPU hotplug also using this code.  Test TSC
+ * synchronization even if we already "know" CPUs were not synchronized. This
+ * can be used as a test to check if, for some reason, the CPUs eventually got
+ * in sync after a CPU has been unplugged. This code is kept separate from the
+ * CPU hotplug code because the slave CPU executes in an IPI, which we want to
+ * keep as short as possible (this is happening while the system is running).
+ * Therefore, we do not send a single IPI for all the test loops, but rather
+ * send one IPI per loop.
  */
-static int test_tsc_synchronization(void)
+int test_tsc_synchronization(void)
 {
 	long cpu, master;
 	cycles_t max_diff = 0, diff, best_loop, worse_loop = 0;
 	int i;
 
 	mutex_lock(&tscsync_mutex);
+	get_online_cpus();
+
+	printk(KERN_INFO
+	       "checking TSC synchronization across all online CPUs:");
+
 	preempt_disable();
 	master = smp_processor_id();
 	for_each_online_cpu(cpu) {
 		if (master == cpu)
 			continue;
-		best_loop = ULLONG_MAX;
-		for (i = 0; i < 10; i++) {
-			/*
-			 * Each CPU (master and slave) must decrement the
-			 * wait_sync value twice (one for priming in cache).
-			 */
-			per_cpu(wait_sync, master) = 2;
-			per_cpu(wait_sync, cpu) = 2;
-			per_cpu(wait_end_sync, master) = 2;
-			per_cpu(wait_end_sync, cpu) = 2;
+		best_loop = (cycles_t)ULLONG_MAX;
+		for (i = 0; i < NR_LOOPS; i++) {
 			smp_call_function_single(cpu, test_sync,
-						(void *)master, 0);
-			test_sync((void *)cpu);
-			/*
-			 * Wait until slave is done so that we don't overwrite
-			 * wait_end_sync prematurely.
-			 */
-			while (unlikely(per_cpu(wait_end_sync, cpu) > 0))
-				cpu_relax();
-
-			diff = abs(per_cpu(tsc_count, cpu)
-				- per_cpu(tsc_count, master));
+						(void *)1UL, 0);
+			test_sync((void *)0UL);
+			diff = abs(sync_data[1].tsc_count
+				- sync_data[0].tsc_count);
 			best_loop = min(best_loop, diff);
 			worse_loop = max(worse_loop, diff);
 		}
+		reset_barriers();
 		max_diff = max(best_loop, max_diff);
 	}
 	preempt_enable();
 	if (max_diff >= MAX_CYCLES_DELTA) {
 		printk(KERN_WARNING
-			"tsc_sync: your timestamp counter is not reliable.\n"
-			"Slower fallback will be used for tracing. See "
-			"the LTTng documentation to find an appropriate "
-			"workaround for your architecture.\n");
-		printk("TSC unsynchronized : %llu cycles delta is over "
-			"threshold %llu\n", max_diff, MAX_CYCLES_DELTA);
+			"Measured %llu cycles TSC offset between CPUs,"
+			" turning off TSC clock.\n", (u64)max_diff);
+		mark_tsc_unstable("check_tsc_sync_source failed");
+		_tsc_is_sync = 0;
+	} else {
+		printk(" passed.\n");
+		/*
+		 * This can reenable the _tsc_is_sync variable even if the TSC
+		 * clock source is left "unstable".
+		 */
+		_tsc_is_sync = 1;
 	}
+	put_online_cpus();
 	mutex_unlock(&tscsync_mutex);
 	return max_diff < MAX_CYCLES_DELTA;
 }
 EXPORT_SYMBOL_GPL(test_tsc_synchronization);
 
-static int __init tsc_test_sync_init(void)
+/*
+ * Test synchronicity of a single core when it is hotplugged.
+ * Source CPU calls into this - waits for the freshly booted target CPU to
+ * arrive and then start the measurement:
+ */
+void __cpuinit check_tsc_sync_source(int cpu)
 {
-	tsc_is_sync = test_tsc_synchronization();
-	return 0;
+	cycles_t diff, best_loop = (cycles_t)ULLONG_MAX, worse_loop = 0;
+	int i;
+
+	/*
+	 * No need to check if we already know that the TSC is not synchronized:
+	 */
+	if (unsynchronized_tsc()) {
+		/*
+		 * Make sure we mark _tsc_is_sync to 0 if the TSC is found
+		 * to be unsynchronized for other causes than non-synchronized
+		 * TSCs across CPUs.
+		 */
+		_tsc_is_sync = 0;
+		return;
+	}
+
+	printk(KERN_INFO "checking TSC synchronization [CPU#%d -> CPU#%d]:",
+			  smp_processor_id(), cpu);
+
+	for (i = 0; i < NR_LOOPS; i++) {
+		test_sync((void *)0UL);
+		diff = abs(sync_data[1].tsc_count
+			- sync_data[0].tsc_count);
+		best_loop = min(best_loop, diff);
+		worse_loop = max(worse_loop, diff);
+	}
+	reset_barriers();
+
+	if (best_loop >= MAX_CYCLES_DELTA) {
+		printk(KERN_WARNING
+			"Measured %llu cycles TSC offset between CPUs,"
+			" turning off TSC clock.\n", (u64)best_loop);
+		mark_tsc_unstable("check_tsc_sync_source failed");
+		_tsc_is_sync = 0;
+	} else {
+		printk(" passed.\n");
+	}
 }
 
-__initcall(tsc_test_sync_init);
+/*
+ * Freshly booted CPUs call into this:
+ */
+void __cpuinit check_tsc_sync_target(void)
+{
+	int i;
+
+	if (unsynchronized_tsc())
+		return;
+
+	for (i = 0; i < NR_LOOPS; i++)
+		test_sync((void *)1UL);
+}
diff --git a/kernel/trace/trace-clock-32-to-64.c b/kernel/trace/trace-clock-32-to-64.c
index ec0a9fb..2faaead 100644
--- a/kernel/trace/trace-clock-32-to-64.c
+++ b/kernel/trace/trace-clock-32-to-64.c
@@ -48,7 +48,7 @@
 atomic_t trace_clock;
 EXPORT_SYMBOL(trace_clock);
 
-static struct timer_list stsc_timer;
+static DEFINE_PER_CPU(struct timer_list, tsc_timer);
 static unsigned int precalc_expire;
 
 struct synthetic_tsc_struct {
@@ -131,25 +131,20 @@ static void synthetic_tsc_ipi(void *info)
 	update_synthetic_tsc();
 }
 
-/* We need to be in process context to do an IPI */
-static void synthetic_tsc_work(struct work_struct *work)
-{
-	on_each_cpu(synthetic_tsc_ipi, NULL, 1);
-}
-static DECLARE_WORK(stsc_work, synthetic_tsc_work);
-
 /*
- * stsc_timer : - Timer function synchronizing synthetic TSC.
+ * tsc_timer_fct : - Timer function synchronizing synthetic TSC.
  * @data: unused
  *
  * Guarantees at least 1 execution before low word of TSC wraps.
  */
-static void stsc_timer_fct(unsigned long data)
+static void tsc_timer_fct(unsigned long data)
 {
-	PREPARE_WORK(&stsc_work, synthetic_tsc_work);
-	schedule_work(&stsc_work);
+	update_synthetic_tsc();
 
-	mod_timer(&stsc_timer, jiffies + precalc_expire);
+	per_cpu(tsc_timer, smp_processor_id()).expires =
+		jiffies + precalc_expire;
+	add_timer_on(&per_cpu(tsc_timer, smp_processor_id()),
+		     smp_processor_id());
 }
 
 /*
@@ -189,30 +184,65 @@ static int __cpuinit hotcpu_callback(struct notifier_block *nb,
 
 	switch (action) {
 	case CPU_UP_PREPARE:
+	case CPU_UP_PREPARE_FROZEN:
+		printk("DEBUG : up prepare for cpu %u, current %u\n",
+			hotcpu, smp_processor_id());
 		cpu_synth = &per_cpu(synthetic_tsc, hotcpu);
 		local_count = trace_clock_read_synthetic_tsc();
 		cpu_synth->tsc[0].val = local_count;
 		cpu_synth->index = 0;
 		smp_wmb();	/* Writing in data of CPU about to come up */
+		init_timer(&per_cpu(tsc_timer, hotcpu));
+		per_cpu(tsc_timer, hotcpu).function = tsc_timer_fct;
+		per_cpu(tsc_timer, hotcpu).expires = jiffies + precalc_expire;
+		printk("DEBUG : up prepare for cpu %u, current %u done\n",
+			hotcpu, smp_processor_id());
 		break;
 	case CPU_ONLINE:
+	case CPU_ONLINE_FROZEN:
+		printk("DEBUG : online for cpu %u, current %u\n",
+			hotcpu, smp_processor_id());
 		/* As we are preemptible, make sure it runs on the right cpu */
-		smp_call_function_single(hotcpu, synthetic_tsc_ipi, NULL, 0);
+		/* This ipi should run before the time-base is read.
+		 * (it is worth being double-checked) */
+		smp_call_function_single(hotcpu, synthetic_tsc_ipi, NULL, 1);
+		add_timer_on(&per_cpu(tsc_timer, hotcpu), hotcpu);
+		printk("DEBUG : online for cpu %u, current %u done\n",
+			hotcpu, smp_processor_id());
+		/* As we are preemptible, make sure it runs on the right cpu */
+		break;
+#ifdef CONFIG_HOTPLUG_CPU
+	case CPU_UP_CANCELED:
+	case CPU_UP_CANCELED_FROZEN:
+	case CPU_DEAD:
+	case CPU_DEAD_FROZEN:
+		printk("DEBUG : offline for cpu %u, current %u\n",
+			hotcpu, smp_processor_id());
+		del_timer_sync(&per_cpu(tsc_timer, hotcpu));
+		printk("DEBUG : offline for cpu %u, current %u done\n",
+			hotcpu, smp_processor_id());
 		break;
+#endif /* CONFIG_HOTPLUG_CPU */
 	}
 	return NOTIFY_OK;
 }
 
-/* Called from one CPU, before any tracing starts, to init each structure */
+/* Called from CPU 0, before any tracing starts, to init each structure */
 static int __init init_synthetic_tsc(void)
 {
-	hotcpu_notifier(hotcpu_callback, 3);
+	printk("DEBUG : setting up timer cpu 0\n");
 	precalc_stsc_interval();
-	init_timer(&stsc_timer);
-	stsc_timer.function = stsc_timer_fct;
-	stsc_timer.expires = jiffies + precalc_expire;
-	add_timer(&stsc_timer);
+	WARN_ON(smp_processor_id() != 0);
+	/* This ipi should run before the time-base is read. */
+	smp_call_function_single(0, synthetic_tsc_ipi, NULL, 1);
+	init_timer(&per_cpu(tsc_timer, 0));
+	per_cpu(tsc_timer, 0).function = tsc_timer_fct;
+	per_cpu(tsc_timer, 0).expires = jiffies + precalc_expire;
+	add_timer_on(&per_cpu(tsc_timer, 0), 0);
+	hotcpu_notifier(hotcpu_callback, 3);
+	printk("DEBUG : setting up timer cpu 0 done\n");
 	return 0;
 }
 
-__initcall(init_synthetic_tsc);
+/* Before SMP is up */
+early_initcall(init_synthetic_tsc);
diff --git a/ltt/ltt-tracer.c b/ltt/ltt-tracer.c
index e4358cf..e86eb6d 100644
--- a/ltt/ltt-tracer.c
+++ b/ltt/ltt-tracer.c
@@ -369,6 +369,7 @@ static int ltt_trace_create(const char *trace_name, const char *trace_type,
 	new_trace->active = 0;
 	strncpy(new_trace->trace_name, trace_name, NAME_MAX);
 	new_trace->mode = mode;
+	get_trace_clock();
 	new_trace->freq_scale = trace_clock_freq_scale();
 
 	ltt_lock_traces();
@@ -497,6 +498,7 @@ trace_error:
 	kref_put(&new_trace->kref, ltt_release_trace);
 	wake_up_interruptible(&new_trace->kref_wq);
 	ltt_unlock_traces();
+	put_trace_clock();
 traces_error:
 	return err;
 }
@@ -591,6 +593,7 @@ static int ltt_trace_destroy(const char *trace_name)
 		goto error;
 	ltt_unlock_traces();
 	__ltt_trace_destroy(trace);
+	put_trace_clock();
 	return err;
 
 	/* Error handling */
diff --git a/ltt/probes/kernel-trace.c b/ltt/probes/kernel-trace.c
index 18b3004..21600c2 100644
--- a/ltt/probes/kernel-trace.c
+++ b/ltt/probes/kernel-trace.c
@@ -12,6 +12,7 @@
 #include <trace/timer.h>
 #include <trace/kernel.h>
 
+
 #include "ltt-type-serializer.h"
 
 /*
@@ -27,23 +28,23 @@
 void probe_irq_entry(unsigned int id, struct pt_regs *regs);
 
 DEFINE_MARKER_TP(kernel_irq_entry, irq_entry, probe_irq_entry,
-	"irq_id %u kernel_mode %u ip %lu");
+	"ip %lu irq_id #2u%u kernel_mode #1u%u");
 
 notrace void probe_irq_entry(unsigned int id, struct pt_regs *regs)
 {
 	struct marker *marker;
-	struct serialize_int_int_long data;
+	struct serialize_long_short_char data;
 
 	if (unlikely(!regs))
 		regs = get_irq_regs();
-	data.f1 = id;
 	if (likely(regs)) {
+		data.f1 = instruction_pointer(regs);
 		data.f2 = !user_mode(regs);
-		data.f3 = instruction_pointer(regs);
 	} else {
+		data.f1 = 0UL;
 		data.f2 = 1;
-		data.f3 = 0UL;
 	}
+	data.f3 = id;
 
 	marker = &GET_MARKER(kernel_irq_entry);
 	ltt_specialized_trace(marker->single.probe_private,
@@ -75,13 +76,13 @@ void probe_irq_softirq_entry(struct softirq_action *h,
 	struct softirq_action *softirq_vec);
 
 DEFINE_MARKER_TP(kernel_softirq_entry, irq_softirq_entry,
-	probe_irq_softirq_entry, "softirq_id %lu");
+	probe_irq_softirq_entry, "softirq_id #1u%lu");
 
 notrace void probe_irq_softirq_entry(struct softirq_action *h,
 	struct softirq_action *softirq_vec)
 {
 	struct marker *marker;
-	unsigned long data;
+	unsigned char data;
 
 	data = ((unsigned long)h - (unsigned long)softirq_vec) / sizeof(*h);
 
@@ -96,13 +97,13 @@ void probe_irq_softirq_exit(struct softirq_action *h,
 	struct softirq_action *softirq_vec);
 
 DEFINE_MARKER_TP(kernel_softirq_exit, irq_softirq_exit,
-	probe_irq_softirq_exit, "softirq_id %lu");
+	probe_irq_softirq_exit, "softirq_id #1u%lu");
 
 notrace void probe_irq_softirq_exit(struct softirq_action *h,
 	struct softirq_action *softirq_vec)
 {
 	struct marker *marker;
-	unsigned long data;
+	unsigned char data;
 
 	data = ((unsigned long)h - (unsigned long)softirq_vec) / sizeof(*h);
 
@@ -116,12 +117,12 @@ notrace void probe_irq_softirq_exit(struct softirq_action *h,
 void probe_irq_softirq_raise(unsigned int nr);
 
 DEFINE_MARKER_TP(kernel_softirq_raise, irq_softirq_raise,
-	probe_irq_softirq_raise, "softirq_id %u");
+	probe_irq_softirq_raise, "softirq_id #1u%u");
 
 notrace void probe_irq_softirq_raise(unsigned int nr)
 {
 	struct marker *marker;
-	unsigned int data;
+	unsigned char data;
 
 	data = nr;
 
@@ -338,7 +339,7 @@ void probe_timer_timeout(struct task_struct *p)
 void probe_kernel_printk(unsigned long retaddr)
 {
 	trace_mark_tp(kernel_printk, kernel_printk,
-		probe_kernel_printk, "ip %lu", retaddr);
+		probe_kernel_printk, "ip 0x%lX", retaddr);
 }
 
 void probe_kernel_vprintk(unsigned long retaddr, char *buf, int len)
@@ -364,7 +365,8 @@ void probe_kernel_vprintk(unsigned long retaddr, char *buf, int len)
 		saved_char = mark_buf[mark_len];
 		mark_buf[mark_len] = '\0';
 		trace_mark_tp(kernel_vprintk, kernel_vprintk,
-			probe_kernel_vprintk, "loglevel %c string %s ip %lu",
+			probe_kernel_vprintk,
+			"loglevel #1u%u string %s ip 0x%lX",
 			loglevel, mark_buf, retaddr);
 		mark_buf[mark_len] = saved_char;
 	}
diff --git a/ltt/probes/ltt-type-serializer.h b/ltt/probes/ltt-type-serializer.h
index 9d59726..295129b 100644
--- a/ltt/probes/ltt-type-serializer.h
+++ b/ltt/probes/ltt-type-serializer.h
@@ -62,4 +62,17 @@ struct serialize_long_long_int {
 	unsigned char end_field[0];
 } LTT_ALIGN;
 
+struct serialize_long_short_char {
+	unsigned long f1;
+	unsigned short f2;
+	unsigned char f3;
+	unsigned char end_field[0];
+} LTT_ALIGN;
+
+struct serialize_long_short {
+	unsigned long f1;
+	unsigned short f2;
+	unsigned char end_field[0];
+} LTT_ALIGN;
+
 #endif /* _LTT_TYPE_SERIALIZER_H */
diff --git a/ltt/probes/syscall-trace.c b/ltt/probes/syscall-trace.c
index a793ad4..0a6c3b8 100644
--- a/ltt/probes/syscall-trace.c
+++ b/ltt/probes/syscall-trace.c
@@ -15,15 +15,15 @@
 void probe_syscall_entry(struct pt_regs *regs, long id);
 
 DEFINE_MARKER_TP(kernel_syscall_entry, syscall_entry,
-	probe_syscall_entry, "ip #p%ld syscall_id %d");
+	probe_syscall_entry, "ip #p%ld syscall_id #2u%u");
 
 notrace void probe_syscall_entry(struct pt_regs *regs, long id)
 {
 	struct marker *marker;
-	struct serialize_long_int data;
+	struct serialize_long_short data;
 
 	data.f1 = instruction_pointer(regs);
-	data.f2 = (int)id;
+	data.f2 = (unsigned short)id;
 
 	marker = &GET_MARKER(kernel_syscall_entry);
 	ltt_specialized_trace(marker->single.probe_private,
diff --git a/ltt/probes/trap-trace.c b/ltt/probes/trap-trace.c
index 76489cd..360b7ea 100644
--- a/ltt/probes/trap-trace.c
+++ b/ltt/probes/trap-trace.c
@@ -14,18 +14,18 @@
 void probe_trap_entry(struct pt_regs *regs, long id);
 
 DEFINE_MARKER_TP(kernel_trap_entry, trap_entry,
-	probe_trap_entry, "ip #p%ld trap_id %d");
+	probe_trap_entry, "ip #p%ld trap_id #2u%u");
 
 notrace void probe_trap_entry(struct pt_regs *regs, long id)
 {
 	struct marker *marker;
-	struct serialize_long_int data;
+	struct serialize_long_short data;
 
 	if (likely(regs))
 		data.f1 = instruction_pointer(regs);
 	else
 		data.f1 = 0UL;
-	data.f2 = (unsigned int)id;
+	data.f2 = (unsigned short)id;
 
 	marker = &GET_MARKER(kernel_trap_entry);
 	ltt_specialized_trace(marker->single.probe_private,
-- 
1.5.5.1

