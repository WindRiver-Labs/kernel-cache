From 4b2d7bd6a46b983e227b3da0a77b3437e532273a Mon Sep 17 00:00:00 2001
From: Yang Shi <yang.shi@windriver.com>
Date: Thu, 10 May 2012 13:50:30 -0700
Subject: [PATCH] omap-debug-trace-clock

OMAP debug trace clock

Test if trace clock runs backward.

Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
---
 arch/arm/Kconfig.debug                        |  4 +++
 arch/arm/mach-omap2/trace-clock.c             | 39 +++++++++++++++++++++++++++
 arch/arm/plat-omap/include/plat/trace-clock.h | 22 +++++++++++++++
 3 files changed, 65 insertions(+)

diff --git a/arch/arm/Kconfig.debug b/arch/arm/Kconfig.debug
index 435ea5a..2377bb1 100644
--- a/arch/arm/Kconfig.debug
+++ b/arch/arm/Kconfig.debug
@@ -354,4 +354,8 @@ config MICROSTATE_ACCT_SCHED_CLOCK_CLOCKSOURCE
 
 endchoice
 
+config DEBUG_TRACE_CLOCK
+	bool "Debug trace clock"
+	depends on HAVE_TRACE_CLOCK
+
 endmenu
diff --git a/arch/arm/mach-omap2/trace-clock.c b/arch/arm/mach-omap2/trace-clock.c
index 99f6950..f43466b 100644
--- a/arch/arm/mach-omap2/trace-clock.c
+++ b/arch/arm/mach-omap2/trace-clock.c
@@ -600,6 +600,45 @@ static struct notifier_block cpufreq_trace_clock_nb = {
 	.notifier_call = cpufreq_trace_clock,
 };
 
+#ifdef CONFIG_DEBUG_TRACE_CLOCK
+/*
+ * Clock expected to never overflow and never go backward.
+ */
+static DEFINE_PER_CPU(u64, last_clock_value);
+static DEFINE_PER_CPU(u32, last_ccnt_value);
+DEFINE_PER_CPU(unsigned int, last_clock_nest);
+EXPORT_PER_CPU_SYMBOL_GPL(last_clock_nest);
+
+static int tc_print_done;
+
+/*
+ * Called with interrupts disabled.
+ */
+void trace_clock_debug(u64 value)
+{
+	int cpu;
+
+	cpu = smp_processor_id();
+	if (unlikely(per_cpu(last_clock_nest, cpu) != 1))
+		return;		/* fiq nesting, don't perform racy check */
+	if (unlikely(!tc_print_done
+		     && (per_cpu(last_clock_value, cpu) > value))) {
+		printk(KERN_WARNING "Trace clock going back last %llu new %llu "
+				    "diff %llu last_ccnt %u ccnt %u\n",
+		       (unsigned long long) per_cpu(last_clock_value, cpu),
+		       (unsigned long long) value,
+		       (unsigned long long) per_cpu(last_clock_value, cpu)
+					    - value,
+		       per_cpu(last_ccnt_value, cpu),
+		       trace_clock_read32());
+		tc_print_done = 1;
+	}
+	per_cpu(last_clock_value, cpu) = value;
+	per_cpu(last_ccnt_value, cpu) = trace_clock_read32();;
+}
+EXPORT_SYMBOL_GPL(trace_clock_debug);
+#endif
+
 static __init int init_trace_clock(void)
 {
 	int cpu;
diff --git a/arch/arm/plat-omap/include/plat/trace-clock.h b/arch/arm/plat-omap/include/plat/trace-clock.h
index 2820eea..54c0cd3 100644
--- a/arch/arm/plat-omap/include/plat/trace-clock.h
+++ b/arch/arm/plat-omap/include/plat/trace-clock.h
@@ -87,6 +87,15 @@ extern u64 trace_clock_async_tsc_read(void);
  */
 extern void _trace_clock_write_synthetic_tsc(u64 value);
 
+#ifdef CONFIG_DEBUG_TRACE_CLOCK
+DECLARE_PER_CPU(unsigned int, last_clock_nest);
+extern void trace_clock_debug(u64 value);
+#else
+static inline void trace_clock_debug(u64 value)
+{
+}
+#endif
+
 static inline u32 read_ccnt(void)
 {
 	u32 val;
@@ -109,6 +118,13 @@ static inline u64 trace_clock_read64(void)
 	struct pm_save_count *pm_count;
 	struct tc_cur_freq *cf;
 	u64 val;
+#ifdef CONFIG_DEBUG_TRACE_CLOCK
+	unsigned long flags;
+
+	local_irq_save(flags);
+	per_cpu(last_clock_nest, smp_processor_id())++;
+	barrier();
+#endif
 
 	preempt_disable();
 	pm_count = &per_cpu(pm_save_count, smp_processor_id());
@@ -118,8 +134,14 @@ static inline u64 trace_clock_read64(void)
 		      * cf->mul_fact) >> 10) + cf->virt_base, cf->floor);
 	} else
 		val = _trace_clock_read_slow();
+	trace_clock_debug(val);
 	preempt_enable();
 
+#ifdef CONFIG_DEBUG_TRACE_CLOCK
+	barrier();
+	per_cpu(last_clock_nest, smp_processor_id())--;
+	local_irq_restore(flags);
+#endif
 	return val;
 }
 
-- 
1.8.0.1.264.g226dcb5

