From 3949d86b78532dd3344b62153c6807384d423ff1 Mon Sep 17 00:00:00 2001
From: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
Date: Wed, 18 May 2011 18:42:29 -0400
Subject: [PATCH] mips-assume-synchronized-tsc

MIPS assume synchronized TSC

Code for non-synchronized TSC support was broken (at least for MIPS32). I
suspect the spinlock did not do well with NMIs or something like this. Revert to
assuming synchronized TSCs across cores until someone send me a MIPS board to
play with.

Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>

diff --git a/arch/mips/include/asm/trace-clock.h b/arch/mips/include/asm/trace-clock.h
index b600518..3d8cb0f 100644
--- a/arch/mips/include/asm/trace-clock.h
+++ b/arch/mips/include/asm/trace-clock.h
@@ -32,28 +32,15 @@ extern u64 trace_clock_read_synthetic_tsc(void);
  * tracing needs to detect delays longer than 8 seconds, we need a full 64-bits
  * TSC, whic is provided by trace-clock-32-to-64.
 */
-extern u64 trace_clock_async_tsc_read(void);
 
 static inline u32 trace_clock_read32(void)
 {
-	u32 cycles;
-
-	if (likely(tsc_is_sync()))
-		cycles = (u32)get_cycles(); /* only need the 32 LSB */
-	else
-		cycles = (u32)trace_clock_async_tsc_read();
-	return cycles;
+	return (u32)get_cycles(); /* only need the 32 LSB */
 }
 
 static inline u64 trace_clock_read64(void)
 {
-	u64 cycles;
-
-	if (likely(tsc_is_sync()))
-		cycles = trace_clock_read_synthetic_tsc();
-	else
-		cycles = trace_clock_async_tsc_read();
-	return cycles;
+	return trace_clock_read_synthetic_tsc();
 }
 
 static inline u64 trace_clock_frequency(void)
@@ -66,11 +53,19 @@ static inline u32 trace_clock_freq_scale(void)
 	return 1;
 }
 
-extern void get_trace_clock(void);
-extern void put_trace_clock(void);
 extern void get_synthetic_tsc(void);
 extern void put_synthetic_tsc(void);
 
+static inline void get_trace_clock(void)
+{
+	get_synthetic_tsc();
+}
+
+static inline void put_trace_clock(void)
+{
+	put_synthetic_tsc();
+}
+
 static inline void set_trace_clock_is_sync(int state)
 {
 }
diff --git a/arch/mips/kernel/Makefile b/arch/mips/kernel/Makefile
index b113205..d3d6fa9 100644
--- a/arch/mips/kernel/Makefile
+++ b/arch/mips/kernel/Makefile
@@ -102,7 +102,6 @@ obj-$(CONFIG_EARLY_PRINTK)	+= early_printk.o
 obj-$(CONFIG_SPINLOCK_TEST)	+= spinlock_test.o
 obj-$(CONFIG_MIPS_MACHINE)	+= mips_machine.o
 
-obj-$(CONFIG_HAVE_TRACE_CLOCK)	+= trace-clock.o
 obj-$(CONFIG_OF)		+= prom.o
 
 CFLAGS_cpu-bugs64.o	= $(shell if $(CC) $(KBUILD_CFLAGS) -Wa,-mdaddi -c -o /dev/null -x c /dev/null >/dev/null 2>&1; then echo "-DHAVE_AS_SET_DADDI"; fi)
diff --git a/arch/mips/kernel/trace-clock.c b/arch/mips/kernel/trace-clock.c
deleted file mode 100644
index 42e27b8..0000000
--- a/arch/mips/kernel/trace-clock.c
+++ /dev/null
@@ -1,171 +0,0 @@
-/*
- * arch/mips/kernel/trace-clock.c
- *
- * Trace clock for mips.
- *
- * Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>, October 2008
- */
-
-#include <linux/module.h>
-#include <linux/trace-clock.h>
-#include <linux/jiffies.h>
-#include <linux/timer.h>
-#include <linux/spinlock.h>
-
-static u64 trace_clock_last_tsc;
-static DEFINE_PER_CPU(struct timer_list, update_timer);
-static DEFINE_SPINLOCK(async_tsc_lock);
-static int async_tsc_refcount;	/* Number of readers */
-static int async_tsc_enabled;	/* Async TSC enabled on all online CPUs */
-
-#if (BITS_PER_LONG == 64)
-static inline u64 trace_clock_cmpxchg64(u64 *ptr, u64 old, u64 new)
-{
-	return cmpxchg64(ptr, old, new);
-}
-#else
-/*
- * Emulate an atomic 64-bits update with a spinlock.
- * Note : preempt_disable or irq save must be explicit with raw_spinlock_t.
- * Given we use a spinlock for this time base, we should never be called from
- * NMI context.
- */
-static raw_spinlock_t trace_clock_lock =
-	(raw_spinlock_t)__RAW_SPIN_LOCK_UNLOCKED;
-
-/*
- * Must be called under irqoff+spinlock on MIPS32.
- */
-static inline u64 trace_clock_cmpxchg64(u64 *ptr, u64 old, u64 new)
-{
-	u64 val;
-
-	val = *ptr;
-	if (likely(val == old))
-		*ptr = new;
-	return val;
-}
-#endif
-
-/*
- * Must be called under irqoff+spinlock on MIPS32.
- */
-static cycles_t read_last_tsc(void)
-{
-	return trace_clock_last_tsc;
-}
-
-/*
- * Support for architectures with non-sync TSCs.
- * When the local TSC is discovered to lag behind the highest TSC counter, we
- * increment the TSC count of an amount that should be, ideally, lower than the
- * execution time of this routine, in cycles : this is the granularity we look
- * for : we must be able to order the events.
- *
- * MIPS32 does not have atomic 64-bit updates. Emulate it with irqoff+spinlock.
- */
-
-notrace u64 trace_clock_async_tsc_read(void)
-{
-	u64 new_tsc, last_tsc;
-#if (BITS_PER_LONG == 32)
-	unsigned long flags;
-
-	local_irq_save(flags);
-	__raw_spin_lock(&trace_clock_lock);
-#endif
-
-	WARN_ON(!async_tsc_refcount || !async_tsc_enabled);
-	new_tsc = trace_clock_read_synthetic_tsc();
-	barrier();
-	last_tsc = read_last_tsc();
-	do {
-		if (new_tsc < last_tsc)
-			new_tsc = last_tsc + TRACE_CLOCK_MIN_PROBE_DURATION;
-		/*
-		 * If cmpxchg fails with a value higher than the new_tsc, don't
-		 * retry : the value has been incremented and the events
-		 * happened almost at the same time.
-		 * We must retry if cmpxchg fails with a lower value :
-		 * it means that we are the CPU with highest frequency and
-		 * therefore MUST update the value.
-		 */
-		last_tsc = trace_clock_cmpxchg64(&trace_clock_last_tsc,
-						 last_tsc, new_tsc);
-	} while (unlikely(last_tsc < new_tsc));
-#if (BITS_PER_LONG == 32)
-	__raw_spin_unlock(&trace_clock_lock);
-	local_irq_restore(flags);
-#endif
-	return new_tsc;
-}
-EXPORT_SYMBOL_GPL(trace_clock_async_tsc_read);
-
-static void update_timer_ipi(void *info)
-{
-	(void)trace_clock_async_tsc_read();
-}
-
-/*
- * update_timer_fct : - Timer function to resync the clocks
- * @data: unused
- *
- * Fires every jiffy.
- */
-static void update_timer_fct(unsigned long data)
-{
-	(void)trace_clock_async_tsc_read();
-
-	per_cpu(update_timer, smp_processor_id()).expires = jiffies + 1;
-	add_timer_on(&per_cpu(update_timer, smp_processor_id()),
-		     smp_processor_id());
-}
-
-static void enable_trace_clock(int cpu)
-{
-	init_timer(&per_cpu(update_timer, cpu));
-	per_cpu(update_timer, cpu).function = update_timer_fct;
-	per_cpu(update_timer, cpu).expires = jiffies + 1;
-	smp_call_function_single(cpu, update_timer_ipi, NULL, 1);
-	add_timer_on(&per_cpu(update_timer, cpu), cpu);
-}
-
-static void disable_trace_clock(int cpu)
-{
-	del_timer_sync(&per_cpu(update_timer, cpu));
-}
-
-void get_trace_clock(void)
-{
-	int cpu;
-
-	spin_lock(&async_tsc_lock);
-	if (async_tsc_refcount++ || tsc_is_sync()) {
-		get_synthetic_tsc();
-	} else {
-		async_tsc_enabled = 1;
-		get_synthetic_tsc();
-		for_each_online_cpu(cpu)
-			enable_trace_clock(cpu);
-	}
-	spin_unlock(&async_tsc_lock);
-}
-EXPORT_SYMBOL_GPL(get_trace_clock);
-
-void put_trace_clock(void)
-{
-	int cpu;
-
-	spin_lock(&async_tsc_lock);
-	WARN_ON(async_tsc_refcount <= 0);
-	if (async_tsc_refcount != 1 || !async_tsc_enabled) {
-		put_synthetic_tsc();
-	} else {
-		for_each_online_cpu(cpu)
-			disable_trace_clock(cpu);
-		async_tsc_enabled = 0;
-	}
-	async_tsc_refcount--;
-	spin_unlock(&async_tsc_lock);
-}
-EXPORT_SYMBOL_GPL(put_trace_clock);
-- 
1.8.3.1

