From 4973dabc7e00a2097bdba6b3e8f519923f9988cc Mon Sep 17 00:00:00 2001
From: Thomas Gleixner <tglx@linutronix.de>
Date: Thu, 16 Jul 2009 12:23:12 +0200
Subject: [PATCH 06/22] genirq: support forced threading of interrupts

commit 8baf330d664a262b4e6d42728b40e6161ef02183 from linux-2.6-tip

Based on the mainline infrastructure we force thread all interrupts
with per device threads.

Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
[minor porting changes to go from 2.6.33 to 2.6.34]
Integrated-by: Mark Asselstine <mark.asselstine@windriver.com>
---
 include/linux/interrupt.h |   13 +++-
 include/linux/irq.h       |    1 +
 include/linux/sched.h     |    1 +
 kernel/irq/chip.c         |    4 +-
 kernel/irq/handle.c       |    7 ++-
 kernel/irq/internals.h    |   10 +++
 kernel/irq/manage.c       |  149 +++++++++++++++++++++++++++++++++++++++++++--
 kernel/irq/migration.c    |    3 +-
 8 files changed, 177 insertions(+), 11 deletions(-)

diff --git a/include/linux/interrupt.h b/include/linux/interrupt.h
index e47b092..0078671 100644
--- a/include/linux/interrupt.h
+++ b/include/linux/interrupt.h
@@ -95,6 +95,7 @@ typedef irqreturn_t (*irq_handler_t)(int, void *);
  * @thread_fn:	interupt handler function for threaded interrupts
  * @thread:	thread pointer for threaded interrupts
  * @thread_flags:	flags related to @thread
+ * @thread_mask:	bit mask to account for forced threads
  */
 struct irqaction {
 	irq_handler_t handler;
@@ -107,6 +108,7 @@ struct irqaction {
 	irq_handler_t thread_fn;
 	struct task_struct *thread;
 	unsigned long thread_flags;
+	unsigned long thread_mask;
 };
 
 extern irqreturn_t no_action(int cpl, void *dev_id);
@@ -323,6 +325,7 @@ static inline int disable_irq_wake(unsigned int irq)
 
 #ifndef __ARCH_SET_SOFTIRQ_PENDING
 #define set_softirq_pending(x) (local_softirq_pending() = (x))
+// FIXME: PREEMPT_RT: set_bit()?
 #define or_softirq_pending(x)  (local_softirq_pending() |= (x))
 #endif
 
@@ -371,9 +374,15 @@ struct softirq_action
 	void	(*action)(struct softirq_action *);
 };
 
-#define __raise_softirq_irqoff(nr) \
+#ifdef CONFIG_PREEMPT_HARDIRQS
+# define __raise_softirq_irqoff(nr) raise_softirq_irqoff(nr)
+# define __do_raise_softirq_irqoff(nr) \
 	do { or_softirq_pending(1UL << (nr)); } while (0)
-#define __do_raise_softirq_irqoff(nr) __raise_softirq_irqoff(nr)
+#else
+# define __raise_softirq_irqoff(nr) \
+	do { or_softirq_pending(1UL << (nr)); } while (0)
+# define __do_raise_softirq_irqoff(nr) __raise_softirq_irqoff(nr)
+#endif
 
 asmlinkage void do_softirq(void);
 asmlinkage void __do_softirq(void);
diff --git a/include/linux/irq.h b/include/linux/irq.h
index 707ab12..7b0a0d4 100644
--- a/include/linux/irq.h
+++ b/include/linux/irq.h
@@ -201,6 +201,7 @@ struct irq_desc {
 #endif
 #endif
 	atomic_t		threads_active;
+	unsigned long		forced_threads_active;
 	wait_queue_head_t       wait_for_threads;
 #ifdef CONFIG_PROC_FS
 	struct proc_dir_entry	*dir;
diff --git a/include/linux/sched.h b/include/linux/sched.h
index fadef75..9ebd16c 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1703,6 +1703,7 @@ extern void thread_group_times(struct task_struct *p, cputime_t *ut, cputime_t *
 #define PF_EXITING	0x00000004	/* getting shut down */
 #define PF_EXITPIDONE	0x00000008	/* pi exit done on shut down */
 #define PF_VCPU		0x00000010	/* I'm a virtual CPU */
+#define PF_HARDIRQ	0x00000020	/* hardirq thread */
 #define PF_FORKNOEXEC	0x00000040	/* forked but didn't exec */
 #define PF_MCE_PROCESS  0x00000080      /* process policy on mce errors */
 #define PF_SUPERPRIV	0x00000100	/* used super-user privileges */
diff --git a/kernel/irq/chip.c b/kernel/irq/chip.c
index b7091d5..a791af0 100644
--- a/kernel/irq/chip.c
+++ b/kernel/irq/chip.c
@@ -309,6 +309,7 @@ static unsigned int default_startup(unsigned int irq)
 {
 	struct irq_desc *desc = irq_to_desc(irq);
 
+	desc->status &= ~IRQ_MASKED;
 	desc->chip->enable(irq);
 	return 0;
 }
@@ -501,7 +502,8 @@ handle_level_irq(unsigned int irq, struct irq_desc *desc)
 	raw_spin_lock(&desc->lock);
 	desc->status &= ~IRQ_INPROGRESS;
 
-	if (!(desc->status & (IRQ_DISABLED | IRQ_ONESHOT)))
+	if (!(desc->status & (IRQ_DISABLED | IRQ_ONESHOT)) &&
+	    !desc->forced_threads_active)
 		unmask_irq(desc, irq);
 out_unlock:
 	raw_spin_unlock(&desc->lock);
diff --git a/kernel/irq/handle.c b/kernel/irq/handle.c
index 76d5a67..9b3bb92 100644
--- a/kernel/irq/handle.c
+++ b/kernel/irq/handle.c
@@ -375,7 +375,7 @@ irqreturn_t handle_IRQ_event(unsigned int irq, struct irqaction *action)
 
 	do {
 		trace_irq_handler_entry(irq, action);
-		ret = action->handler(irq, action->dev_id);
+		ret = handle_irq_action(irq, action);
 		trace_irq_handler_exit(irq, action, ret);
 
 		switch (ret) {
@@ -452,6 +452,11 @@ unsigned int __do_IRQ(unsigned int irq)
 	struct irqaction *action;
 	unsigned int status;
 
+#ifdef CONFIG_PREEMPT_RT
+	printk(KERN_WARNING "__do_IRQ called for irq %d. "
+	       "PREEMPT_RT will crash your system soon\n", irq);
+	printk(KERN_WARNING "I hope you have a fire-extinguisher handy!\n");
+#endif
 	kstat_incr_irqs_this_cpu(irq, desc);
 
 	if (CHECK_IRQ_PER_CPU(desc->status)) {
diff --git a/kernel/irq/internals.h b/kernel/irq/internals.h
index c63f3bc..ce56cea 100644
--- a/kernel/irq/internals.h
+++ b/kernel/irq/internals.h
@@ -53,6 +53,16 @@ static inline void chip_bus_sync_unlock(unsigned int irq, struct irq_desc *desc)
 		desc->chip->bus_sync_unlock(irq);
 }
 
+#ifdef CONFIG_PREEMPT_HARDIRQS
+extern irqreturn_t handle_irq_action(unsigned int irq,struct irqaction *action);
+#else
+static inline irqreturn_t
+handle_irq_action(unsigned int irq, struct irqaction *action)
+{
+	return action->handler(irq, action->dev_id);
+}
+#endif
+
 /*
  * Debugging printout:
  */
diff --git a/kernel/irq/manage.c b/kernel/irq/manage.c
index 704e488..262dab5 100644
--- a/kernel/irq/manage.c
+++ b/kernel/irq/manage.c
@@ -280,7 +280,8 @@ void __enable_irq(struct irq_desc *desc, unsigned int irq, bool resume)
 			goto err_out;
 		/* Prevent probing on this irq: */
 		desc->status = status | IRQ_NOPROBE;
-		check_irq_resend(desc, irq);
+		if (!desc->forced_threads_active)
+			check_irq_resend(desc, irq);
 		/* fall-through */
 	}
 	default:
@@ -465,7 +466,117 @@ static irqreturn_t irq_nested_primary_handler(int irq, void *dev_id)
 	return IRQ_NONE;
 }
 
-static int irq_wait_for_interrupt(struct irqaction *action)
+#ifdef CONFIG_PREEMPT_HARDIRQS
+/*
+ * handler function for forced irq threading. Dummy code. See
+ * handle_irq_action() below.
+ */
+static irqreturn_t preempt_hardirq_handler(int irq, void *dev_id)
+{
+	return IRQ_WAKE_THREAD;
+}
+
+/*
+ * Momentary workaround until I have a brighter idea how to handle the
+ * accounting of forced thread handlers.
+ */
+irqreturn_t handle_irq_action(unsigned int irq, struct irqaction *action)
+{
+	if (action->handler == preempt_hardirq_handler) {
+		struct irq_desc *desc = irq_to_desc(irq);
+		unsigned long flags;
+
+		raw_spin_lock_irqsave(&desc->lock, flags);
+
+		/* FIXME: use some flag to do that */
+		if (desc->handle_irq == handle_fasteoi_irq) {
+			if (desc->chip->mask)
+				desc->chip->mask(irq);
+		}
+		desc->forced_threads_active |= action->thread_mask;
+		raw_spin_unlock_irqrestore(&desc->lock, flags);
+		return IRQ_WAKE_THREAD;
+	}
+	return action->handler(irq, action->dev_id);
+}
+
+/*
+ * forced threaded interrupts need to unmask the interrupt line
+ */
+static int preempt_hardirq_thread_done(struct irq_desc *desc,
+					struct irqaction *action)
+{
+	unsigned long masked;
+
+	if (action->handler != preempt_hardirq_handler)
+		return 0;
+again:
+	raw_spin_lock_irq(&desc->lock);
+	/*
+	 * Be careful. The hardirq handler might be running on the
+	 * other CPU.
+	 */
+	if (desc->status & IRQ_INPROGRESS) {
+		raw_spin_unlock_irq(&desc->lock);
+		cpu_relax();
+		goto again;
+	}
+
+	/*
+	 * Now check again, whether the thread should run. Otherwise
+	 * we would clear the forced_threads_active bit which was just
+	 * set.
+	 */
+	if (test_bit(IRQTF_RUNTHREAD, &action->thread_flags)) {
+		raw_spin_unlock_irq(&desc->lock);
+		return 1;
+	}
+
+	masked = desc->forced_threads_active;
+	desc->forced_threads_active &= ~action->thread_mask;
+
+	/*
+	 * Unmask the interrupt line when this is the last active
+	 * thread and the interrupt is not disabled.
+	 */
+	if (masked && !desc->forced_threads_active &&
+	    !(desc->status & IRQ_DISABLED)) {
+		if (desc->chip->unmask)
+			desc->chip->unmask(action->irq);
+		/*
+		 * Do we need to call check_irq_resend() here ?
+		 * No. check_irq_resend needs only to be checked when
+		 * we go from IRQ_DISABLED to IRQ_ENABLED state.
+		 */
+	}
+	raw_spin_unlock_irq(&desc->lock);
+	return 0;
+}
+
+/*
+ * If the caller does not request irq threading then the handler
+ * becomes the thread function and we use the above handler as the
+ * primary hardirq context handler.
+ */
+static void preempt_hardirq_setup(struct irqaction *new)
+{
+	if (new->thread_fn || (new->flags & IRQF_NODELAY))
+		return;
+
+	new->thread_fn = new->handler;
+	new->handler = preempt_hardirq_handler;
+}
+#else
+static inline void preempt_hardirq_setup(struct irqaction *new) { }
+static inline int
+preempt_hardirq_thread_done(struct irq_desc *d, struct irqaction *a)
+{
+	return 0;
+}
+#endif
+
+static int
+irq_wait_for_interrupt(struct irq_desc *desc, struct irqaction *action)
 {
 	while (!kthread_should_stop()) {
 		set_current_state(TASK_INTERRUPTIBLE);
@@ -475,7 +586,8 @@ static int irq_wait_for_interrupt(struct irqaction *action)
 			__set_current_state(TASK_RUNNING);
 			return 0;
 		}
-		schedule();
+		if (!preempt_hardirq_thread_done(desc, action))
+			schedule();
 	}
 	return -1;
 }
@@ -559,9 +671,10 @@ static int irq_thread(void *data)
 	int wake, oneshot = desc->status & IRQ_ONESHOT;
 
 	sched_setscheduler(current, SCHED_FIFO, &param);
+	current->flags |= PF_HARDIRQ;
 	current->irqaction = action;
 
-	while (!irq_wait_for_interrupt(action)) {
+	while (!irq_wait_for_interrupt(desc, action)) {
 
 		irq_thread_check_affinity(desc, action);
 
@@ -631,7 +744,7 @@ __setup_irq(unsigned int irq, struct irq_desc *desc, struct irqaction *new)
 {
 	struct irqaction *old, **old_ptr;
 	const char *old_name = NULL;
-	unsigned long flags;
+	unsigned long flags, thread_mask = 0;
 	int nested, shared = 0;
 	int ret;
 
@@ -661,6 +774,9 @@ __setup_irq(unsigned int irq, struct irq_desc *desc, struct irqaction *new)
 	if ((new->flags & IRQF_ONESHOT) && (new->flags & IRQF_SHARED))
 		return -EINVAL;
 
+	/* Preempt-RT setup for forced threading */
+	preempt_hardirq_setup(new);
+
 	/*
 	 * Check whether the interrupt nests into another interrupt
 	 * thread.
@@ -726,12 +842,20 @@ __setup_irq(unsigned int irq, struct irq_desc *desc, struct irqaction *new)
 
 		/* add new interrupt at end of irq queue */
 		do {
+			thread_mask |= old->thread_mask;
 			old_ptr = &old->next;
 			old = *old_ptr;
 		} while (old);
 		shared = 1;
 	}
 
+	/*
+	 * Setup the thread mask for this irqaction. No risk that ffz
+	 * will fail. If we have 32 resp. 64 devices sharing one irq
+	 * then .....
+	 */
+	new->thread_mask = 1 << ffz(thread_mask);
+
 	if (!shared) {
 		irq_chip_set_defaults(desc->chip);
 
@@ -820,6 +944,9 @@ __setup_irq(unsigned int irq, struct irq_desc *desc, struct irqaction *new)
 	new->dir = NULL;
 	register_handler_proc(irq, new);
 
+	if (new->thread)
+		wake_up_process(new->thread);
+
 	return 0;
 
 mismatch:
@@ -916,6 +1043,11 @@ static struct irqaction *__free_irq(unsigned int irq, void *dev_id)
 			desc->chip->disable(irq);
 	}
 
+	/*
+	 * Clear the forced_threaded bit of the removed action
+	 */
+	desc->forced_threads_active &= ~action->thread_mask;
+
 	raw_spin_unlock_irqrestore(&desc->lock, flags);
 
 	unregister_handler_proc(irq, action);
@@ -1107,13 +1239,18 @@ int request_threaded_irq(unsigned int irq, irq_handler_t handler,
 		 * run in parallel with our fake.
 		 */
 		unsigned long flags;
+		irqreturn_t ret;
 
 		disable_irq(irq);
 		local_irq_save(flags);
 
-		handler(irq, dev_id);
+		ret = action->handler(irq, dev_id);
 
 		local_irq_restore(flags);
+
+		if (ret == IRQ_WAKE_THREAD)
+			action->thread_fn(irq, dev_id);
+
 		enable_irq(irq);
 	}
 #endif
diff --git a/kernel/irq/migration.c b/kernel/irq/migration.c
index 4817563..70b7b5c 100644
--- a/kernel/irq/migration.c
+++ b/kernel/irq/migration.c
@@ -66,7 +66,8 @@ void move_native_irq(int irq)
 	 * If the irq is already in progress, it should be masked.
 	 * If we unmask it, we might cause an interrupt storm on RT.
 	 */
-	if (unlikely(desc->status & IRQ_INPROGRESS))
+	if (unlikely(desc->status & IRQ_INPROGRESS ||
+		     desc->forced_threads_active))
 		mask = 0;
 
 	if (mask)
-- 
1.7.0

