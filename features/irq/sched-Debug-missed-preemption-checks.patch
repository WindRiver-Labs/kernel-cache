From b6f90589b71273e331c2c98d750eaa6a94b58622 Mon Sep 17 00:00:00 2001
From: Thomas Gleixner <tglx@linutronix.de>
Date: Fri, 24 Jul 2009 10:22:02 +0200
Subject: [PATCH 14/22] sched: Debug missed preemption checks

Developers use preempt_enable_no_resched() in places where the code
calls schedule() immediately which is correct. But there are places
where preempt_enable_no_resched() is not followed by schedule().

Add debug infrastructre to find the offending code. The identified
correct users are converted to use __preempt_enable_no_resched().

For the ever repeating "preempt_enable_no_resched(); schedule();"
sequences a onvenience macro preempt_enable_and_schedule() is
introduced.

Based on a previous patch from Ingo Molnar <mingo@elte.hu>

Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
---
 include/linux/preempt.h         |   18 ++++++++++++++++--
 include/linux/spinlock_api_up.h |    2 +-
 init/main.c                     |    3 +--
 kernel/mutex.c                  |    3 +--
 kernel/sched.c                  |   18 +++++++++++++++---
 kernel/signal.c                 |    3 +--
 kernel/softirq.c                |    7 +++----
 lib/kernel_lock.c               |    2 +-
 8 files changed, 39 insertions(+), 17 deletions(-)

diff --git a/include/linux/preempt.h b/include/linux/preempt.h
index 2e681d9..5c7dba8 100644
--- a/include/linux/preempt.h
+++ b/include/linux/preempt.h
@@ -33,12 +33,24 @@ do { \
 	barrier(); \
 } while (0)
 
-#define preempt_enable_no_resched() \
+#define __preempt_enable_no_resched() \
 do { \
 	barrier(); \
 	dec_preempt_count(); \
 } while (0)
 
+#ifdef CONFIG_DEBUG_PREEMPT
+extern void notrace preempt_enable_no_resched(void);
+#else
+# define preempt_enable_no_resched() __preempt_enable_no_resched()
+#endif
+
+#define preempt_enable_and_schedule() \
+do { \
+	__preempt_enable_no_resched(); \
+	schedule(); \
+} while (0)
+
 #define preempt_check_resched() \
 do { \
 	if (unlikely(test_thread_flag(TIF_NEED_RESCHED))) \
@@ -47,7 +59,7 @@ do { \
 
 #define preempt_enable() \
 do { \
-	preempt_enable_no_resched(); \
+	__preempt_enable_no_resched(); \
 	barrier(); \
 	preempt_check_resched(); \
 } while (0)
@@ -84,6 +96,8 @@ do { \
 
 #define preempt_disable()		do { } while (0)
 #define preempt_enable_no_resched()	do { } while (0)
+#define __preempt_enable_no_resched()	do { } while (0)
+#define preempt_enable_and_schedule()	schedule()
 #define preempt_enable()		do { } while (0)
 #define preempt_check_resched()		do { } while (0)
 
diff --git a/include/linux/spinlock_api_up.h b/include/linux/spinlock_api_up.h
index af1f472..d05112d 100644
--- a/include/linux/spinlock_api_up.h
+++ b/include/linux/spinlock_api_up.h
@@ -40,7 +40,7 @@
   do { preempt_enable(); __release(lock); (void)(lock); } while (0)
 
 #define __UNLOCK_BH(lock) \
-  do { preempt_enable_no_resched(); local_bh_enable(); \
+  do { __preempt_enable_no_resched(); local_bh_enable(); \
 	  __release(lock); (void)(lock); } while (0)
 
 #define __UNLOCK_IRQ(lock) \
diff --git a/init/main.c b/init/main.c
index 3b4c08d..0a82221 100644
--- a/init/main.c
+++ b/init/main.c
@@ -441,8 +441,7 @@ static noinline void __init_refok rest_init(void)
 	 * at least once to get things moving:
 	 */
 	init_idle_bootup_task(current);
-	preempt_enable_no_resched();
-	schedule();
+	preempt_enable_and_schedule();
 	preempt_disable();
 
 	/* Call into cpu_idle with preempt disabled */
diff --git a/kernel/mutex.c b/kernel/mutex.c
index 03c7c5a..3838ee1 100644
--- a/kernel/mutex.c
+++ b/kernel/mutex.c
@@ -258,8 +258,7 @@ __mutex_lock_common(struct mutex *lock, long state, unsigned int subclass,
 
 		/* didnt get the lock, go to sleep: */
 		spin_unlock_mutex(&lock->wait_lock, flags);
-		preempt_enable_no_resched();
-		schedule();
+		preempt_enable_and_schedule();
 		preempt_disable();
 		spin_lock_mutex(&lock->wait_lock, flags);
 	}
diff --git a/kernel/sched.c b/kernel/sched.c
index 5d6d9bb..bbbbe52 100644
--- a/kernel/sched.c
+++ b/kernel/sched.c
@@ -3544,6 +3544,19 @@ notrace unsigned long get_parent_ip(unsigned long addr)
 	return addr;
 }
 
+#ifdef CONFIG_DEBUG_PREEMPT
+void notrace preempt_enable_no_resched(void)
+{
+	barrier();
+	dec_preempt_count();
+
+	WARN_ONCE(!preempt_count(),
+	     KERN_ERR "BUG: %s:%d task might have lost a preemption check!\n",
+	     current->comm, current->pid);
+}
+EXPORT_SYMBOL(preempt_enable_no_resched);
+#endif
+
 #if defined(CONFIG_PREEMPT) && (defined(CONFIG_DEBUG_PREEMPT) || \
 				defined(CONFIG_PREEMPT_TRACER))
 
@@ -3764,7 +3777,7 @@ need_resched_nonpreemptible:
 		goto need_resched_nonpreemptible;
 	}
 
-	preempt_enable_no_resched();
+	__preempt_enable_no_resched();
 	if (need_resched())
 		goto need_resched;
 }
@@ -4955,9 +4968,8 @@ SYSCALL_DEFINE0(sched_yield)
 	__release(rq->lock);
 	spin_release(&rq->lock.dep_map, 1, _THIS_IP_);
 	do_raw_spin_unlock(&rq->lock);
-	preempt_enable_no_resched();
 
-	schedule();
+	preempt_enable_and_schedule();
 
 	return 0;
 }
diff --git a/kernel/signal.c b/kernel/signal.c
index 825a3f2..3933e19 100644
--- a/kernel/signal.c
+++ b/kernel/signal.c
@@ -1655,8 +1655,7 @@ static void ptrace_stop(int exit_code, int clear_code, siginfo_t *info)
 		 */
 		preempt_disable();
 		read_unlock(&tasklist_lock);
-		preempt_enable_no_resched();
-		schedule();
+		preempt_enable_and_schedule();
 	} else {
 		/*
 		 * By the time we got the lock, our tracer went away.
diff --git a/kernel/softirq.c b/kernel/softirq.c
index 95516ec..3d8304f 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -357,7 +357,7 @@ void irq_exit(void)
 	if (idle_cpu(smp_processor_id()) && !in_interrupt() && !need_resched())
 		tick_nohz_stop_sched_tick(0);
 #endif
-	preempt_enable_no_resched();
+	__preempt_enable_no_resched();
 }
 
 /*
@@ -748,8 +748,7 @@ static int run_ksoftirqd(void * __data)
 	while (!kthread_should_stop()) {
 		preempt_disable();
 		if (!(local_softirq_pending() & mask)) {
-			preempt_enable_no_resched();
-			schedule();
+			preempt_enable_and_schedule();
 			preempt_disable();
 		}
 
@@ -764,7 +763,7 @@ static int run_ksoftirqd(void * __data)
 				goto wait_to_die;
 
 			local_irq_disable();
-			preempt_enable_no_resched();
+			__preempt_enable_no_resched();
 			set_softirq_pending(local_softirq_pending() & ~mask);
 			local_bh_disable();
 			local_irq_enable();
diff --git a/lib/kernel_lock.c b/lib/kernel_lock.c
index b135d04..5354922 100644
--- a/lib/kernel_lock.c
+++ b/lib/kernel_lock.c
@@ -53,7 +53,7 @@ int __lockfunc __reacquire_kernel_lock(void)
 void __lockfunc __release_kernel_lock(void)
 {
 	do_raw_spin_unlock(&kernel_flag);
-	preempt_enable_no_resched();
+	__preempt_enable_no_resched();
 }
 
 /*
-- 
1.7.0

