From dd3402f60931cec8b054b2104722244b09a4ec40 Mon Sep 17 00:00:00 2001
From: Tom Rix <trix@kernel-dev.(none)>
Date: Thu, 23 Oct 2008 17:07:22 -0500
Subject: [PATCH] Add 'early' tracer to report early boot timing analysis

CQID: WIND00122917

early
---
 Documentation/ftrace.txt   |   66 ++++++++++++++
 init/main.c                |   14 +++
 kernel/trace/Kconfig       |   16 ++++
 kernel/trace/Makefile      |    1 +
 kernel/trace/trace.c       |   85 +++++++++++++++++-
 kernel/trace/trace.h       |    5 +
 kernel/trace/trace_early.c |  216 ++++++++++++++++++++++++++++++++++++++++++++
 7 files changed, 402 insertions(+), 1 deletions(-)
 create mode 100644 kernel/trace/trace_early.c

diff --git a/Documentation/ftrace.txt b/Documentation/ftrace.txt
index d330fe3..117062d 100644
--- a/Documentation/ftrace.txt
+++ b/Documentation/ftrace.txt
@@ -166,6 +166,8 @@ Here is the list of current tracers that may be configured.
 		the highest priority task to get scheduled after
 		it has been woken up.
 
+  early - Traces boottime functions. 
+
   none - This is not a tracer. To remove all tracers from tracing
 		simply echo "none" into current_tracer.
 
@@ -1000,6 +1002,70 @@ is the stack for the hard interrupt. This hides the fact that NEED_RESCHED
 has been set. We do not see the 'N' until we switch back to the task's
 assigned stack.
 
+early
+-----
+
+early is a static tracer of early boottime functions.  It is used to profile 
+the early bootup of the kernel.  
+
+To configure, choose 'Early Tracing' in xconfig or manually set throudh the
+config variable CONFIG_TRACE_EARLY.  Be aware that this is a static tracer
+and will not function if CONFIG_DYNAMIC_FTRACE is set.  
+
+early produces its output when the system boots up.  Its output is accessed 
+through the normal ftrace method of reading the tracing/trace file in the 
+debugfs.  The tracer is started as early in the bootup as possible.  It 
+is started after kmem_cache_init in start_kernel.  This is an example of 
+output from x86. 
+
+# tracer: early
+#
+#           TASK-PID   CPU#    TIMESTAMP  FUNCTION
+#              | |      |          |         |
+          <idle>-0     [00]     0.001000: hpet_time_init <-start_kernel
+          <idle>-0     [00]     0.001000: hpet_enable <-hpet_time_init
+          <idle>-0     [00]     0.001000: setup_pit_timer <-hpet_time_init
+          <idle>-0     [00]     0.001000: time_init_hook <-hpet_time_init
+          <idle>-0     [00]     0.001368: pidmap_init <-start_kernel
+
+Note that the timestamp does not start at 0.  There will be variation because 
+in this case, the tracing started after the system clock.  In some cases the 
+tracing will start before the system clock.  This is an example of the omap3430
+start up. 
+
+# tracer: early
+#
+#           TASK-PID   CPU#    TIMESTAMP  FUNCTION
+#              | |      |          |         |
+          <idle>-0     [00]     0.000000: calibrate_delay <-start_kernel
+          <idle>-0     [00]     0.000000: pidmap_init <-start_kernel
+          <idle>-0     [00]     0.000000: anon_vma_init <-start_kernel
+          <idle>-0     [00]     0.000000: thread_info_cache_init <-start_kernel
+
+- removed output -
+
+           <...>-1     [00]     0.000000: omap_register_i2c_bus <-omap_i2c_init
+           <...>-1     [00]     0.000000: omap_init_clocksource_32k <-__exception_text_end
+           <...>-1     [00]  1433.023041: omap_init_devices <-__exception_text_end
+           <...>-1     [00]  1433.024933: omap_init_dma <-__exception_text_end
+
+Note that the timestamp for omap_init_devices looks valid of the call to 
+omap_init_clocksource_32k. 
+
+The functions listed are either in the kernel's init section, which is 
+unloaded when the kernel boots, or one of the many initcalls that are 
+responsible for initizing devices and kernel subsystems.  The tracing 
+continues until before the kernel hands off processing to the init process.  
+This is an example of the last part of a x86 trace log. 
+
+           <...>-1     [00]     3.858983: initrd_load <-prepare_namespace
+           <...>-1     [00]     3.859173: rd_load_image <-initrd_load
+           <...>-1     [00]     3.859456: mount_root <-prepare_namespace
+           <...>-1     [00]     3.859458: nfs_root_data <-mount_root
+           <...>-1     [00]     3.859463: root_nfs_parse <-nfs_root_data
+           <...>-1     [00]     3.859465: root_nfs_parse <-nfs_root_data
+           <...>-1     [00]     3.931151: do_mount_root <-mount_root
+
 ftrace
 ------
 
diff --git a/init/main.c b/init/main.c
index 71d79a5..043c7f1 100644
--- a/init/main.c
+++ b/init/main.c
@@ -104,6 +104,13 @@ static inline void mark_rodata_ro(void) { }
 extern void tc_init(void);
 #endif
 
+#ifdef CONFIG_FTRACE_EARLY
+extern int tracer_alloc_buffers(void);
+extern int init_function_trace(void);
+extern int tracing_early_ctrl_write(unsigned int val);
+extern int init_early_trace(void);
+#endif
+
 enum system_states system_state;
 EXPORT_SYMBOL(system_state);
 
@@ -652,6 +659,10 @@ asmlinkage void __init start_kernel(void)
 	cpu_hotplug_init();
 	kmem_cache_init();
 	kmemleak_init();
+#ifdef CONFIG_FTRACE_EARLY
+	tracer_alloc_buffers();
+	init_early_trace();
+#endif
 	debug_objects_mem_init();
 	idr_init_cache();
 	setup_per_cpu_pageset();
@@ -874,6 +885,9 @@ static int __init kernel_init(void * unused)
 
 	do_basic_setup();
 
+#ifdef CONFIG_FTRACE_EARLY
+	tracing_early_ctrl_write(0);
+#endif
 	/*
 	 * check if there is an early userspace init.  If yes, let it do all
 	 * the work
diff --git a/kernel/trace/Kconfig b/kernel/trace/Kconfig
index acdc6db..27ba55c 100644
--- a/kernel/trace/Kconfig
+++ b/kernel/trace/Kconfig
@@ -132,6 +132,22 @@ config DYNAMIC_FTRACE
 	 were made. If so, it runs stop_machine (stops all CPUS)
 	 and modifies the code to jump over the call to ftrace.
 
+config FTRACE_EARLY
+	bool "Early tracing (Experimental)"
+	default n
+	depends on HAVE_FTRACE
+	select TRACING
+	help
+	  This option enables static tracing of the early boot sequence.
+          This tracer is useful for finding, at a high level, how long early
+	  kernel initializations are taking to complete.  By removing or
+	  reducing the time for functions, the overall boottime can be reduced.
+
+	  The tracing starts in start_kernel after kmem_cache_init and finishes
+	  in kernel_init before init_post.
+
+	  This option depends on the early initialization of the system timer.
+
 config FTRACE_SELFTEST
 	bool
 
diff --git a/kernel/trace/Makefile b/kernel/trace/Makefile
index 58ec61c..7bfaced 100644
--- a/kernel/trace/Makefile
+++ b/kernel/trace/Makefile
@@ -22,4 +22,5 @@ obj-$(CONFIG_SCHED_TRACER) += trace_sched_wakeup.o
 obj-$(CONFIG_STACK_TRACER) += trace_stack.o
 obj-$(CONFIG_MMIOTRACE) += trace_mmiotrace.o
 
+obj-$(CONFIG_FTRACE_EARLY) += trace_early.o
 libftrace-y := ftrace.o
diff --git a/kernel/trace/trace.c b/kernel/trace/trace.c
index 942ce76..0219bb2 100644
--- a/kernel/trace/trace.c
+++ b/kernel/trace/trace.c
@@ -2351,6 +2351,32 @@ tracing_ctrl_write(struct file *filp, const char __user *ubuf,
 	return cnt;
 }
 
+#ifdef CONFIG_FTRACE_EARLY
+int __init tracing_early_ctrl_write(unsigned int val)
+{
+	struct trace_array *tr = &global_trace;
+	int ret = 0;
+
+	mutex_lock(&trace_types_lock);
+	if (tr->ctrl ^ val) {
+		if (val)
+			tracer_enabled = 1;
+		else
+			tracer_enabled = 0;
+
+		tr->ctrl = val;
+
+		if (current_trace && current_trace->ctrl_update)
+			current_trace->ctrl_update(tr);
+
+		ret = 1;
+	}
+	mutex_unlock(&trace_types_lock);
+
+	return ret;
+}
+#endif /* CONFIG_EARLY_FTRACE */
+
 static ssize_t
 tracing_set_trace_read(struct file *filp, char __user *ubuf,
 		       size_t cnt, loff_t *ppos)
@@ -2412,6 +2438,46 @@ tracing_set_trace_write(struct file *filp, const char __user *ubuf,
 	return cnt;
 }
 
+#ifdef CONFIG_FTRACE_EARLY
+ssize_t __init tracing_early_set_trace_write(const char *ubuf, size_t cnt)
+{
+	struct trace_array *tr = &global_trace;
+	struct tracer *t;
+	char buf[max_tracer_type_len+1];
+	int i;
+
+	if (cnt > max_tracer_type_len)
+		cnt = max_tracer_type_len;
+
+	memcpy(&buf, ubuf, cnt);
+	buf[cnt] = 0;
+
+	/* strip ending whitespace. */
+	for (i = cnt - 1; i > 0 && isspace(buf[i]); i--)
+		buf[i] = 0;
+
+	mutex_lock(&trace_types_lock);
+	for (t = trace_types; t; t = t->next) {
+		if (strcmp(t->name, buf) == 0)
+			break;
+	}
+	if (!t || t == current_trace)
+		goto out;
+
+	if (current_trace && current_trace->reset)
+		current_trace->reset(tr);
+
+	current_trace = t;
+	if (t->init)
+		t->init(tr);
+
+ out:
+	mutex_unlock(&trace_types_lock);
+
+	return cnt;
+}
+#endif /* CONFIG_FTRACE_EARLY */
+
 static ssize_t
 tracing_max_lat_read(struct file *filp, char __user *ubuf,
 		     size_t cnt, loff_t *ppos)
@@ -2870,7 +2936,11 @@ struct dentry *tracing_init_dentry(void)
 #include "trace_selftest.c"
 #endif
 
+#ifndef CONFIG_FTRACE_EARLY
 static __init void tracer_init_debugfs(void)
+#else
+static __init int tracer_init_debugfs(void)
+#endif
 {
 	struct dentry *d_tracer;
 	struct dentry *entry;
@@ -2952,6 +3022,9 @@ static __init void tracer_init_debugfs(void)
 #ifdef CONFIG_SYSPROF_TRACER
 	init_tracer_sysprof_debugfs(d_tracer);
 #endif
+#ifdef CONFIG_FTRACE_EARLY
+	return 0;
+#endif
 }
 
 static int trace_alloc_page(void)
@@ -3070,7 +3143,11 @@ static int trace_free_page(void)
 	return ret;
 }
 
+#ifndef CONFIG_FTRACE_EARLY
 __init static int tracer_alloc_buffers(void)
+#else
+__init int tracer_alloc_buffers(void)
+#endif
 {
 	struct trace_array_cpu *data;
 	void *array;
@@ -3139,8 +3216,9 @@ __init static int tracer_alloc_buffers(void)
 		pages, trace_nr_entries, (long)TRACE_ENTRY_SIZE);
 	pr_info("   actual entries %ld\n", global_trace.entries);
 
+#ifndef CONFIG_FTRACE_EARLY
 	tracer_init_debugfs();
-
+#endif
 	trace_init_cmdlines();
 
 	register_tracer(&no_tracer);
@@ -3178,4 +3256,9 @@ __init static int tracer_alloc_buffers(void)
 	}
 	return ret;
 }
+
+#ifndef CONFIG_FTRACE_EARLY
 fs_initcall(tracer_alloc_buffers);
+#else
+fs_initcall(tracer_init_debugfs);
+#endif
diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 9506845..c7adab1 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -337,4 +337,9 @@ enum trace_iterator_flags {
 	TRACE_ITER_SCHED_TREE		= 0x200,
 };
 
+#ifdef CONFIG_FTRACE_EARLY
+ssize_t __init tracing_early_set_trace_write(const char *ubuf, size_t cnt);
+int __init tracing_early_ctrl_write(unsigned int val);
+#endif /* CONFIG_EARLY_FTRACE */
+
 #endif /* _LINUX_KERNEL_TRACE_H */
diff --git a/kernel/trace/trace_early.c b/kernel/trace/trace_early.c
new file mode 100644
index 0000000..d280224
--- /dev/null
+++ b/kernel/trace/trace_early.c
@@ -0,0 +1,216 @@
+/*
+ * Statically trace early boot.
+ *
+ * Copyright (C) 2008 Tom Rix <Tom.Rix@windriver.com>
+ */
+#include <linux/kallsyms.h>
+#include <linux/debugfs.h>
+#include <linux/uaccess.h>
+#include <linux/module.h>
+#include <linux/ftrace.h>
+#include <linux/fs.h>
+#include <linux/sort.h>
+
+#include <linux/irq.h>
+#include <linux/stacktrace.h>
+#include <asm/sections.h>
+
+#include "trace.h"
+
+/* Uncomment for debug output */
+/* #define EARLY_TRACE_DB */
+
+extern ftrace_func_t ftrace_trace_function;
+extern int ftrace_function_enabled;
+
+static struct trace_array *early_trace;
+static unsigned long *itable;
+static unsigned long itable_size;
+
+extern initcall_t __initcall_start[], __initcall_end[], __early_initcall_end[];
+
+/* This routine determines which kernel functions are recorded and which
+   are ignored.  */
+static int skip(unsigned long ip)
+{
+	/* Functions in the kernel init section are OK */
+	if (ip >= (unsigned long)_sinittext &&
+	    ip <= (unsigned long)_einittext)
+		return 0;
+	/* Functions in the initcall tables are OK
+	   The itable holds these calls and is presorted */
+	if (itable &&
+	    ip >= itable[0] &&
+	    ip <= itable[itable_size - 1]) {
+		unsigned long s, e, m, v;
+
+		s = m = 0;
+		e = itable_size - 1;
+		for (;;) {
+			m = s + ((e - s) >> 1);
+			v = itable[m];
+			if (unlikely(ip == v))
+				return 0;
+			if (likely(e <= (s+1)))
+				break;
+			if (ip > v)
+				s = m;
+			else /* (ip < v) */
+				e = m;
+		}
+	}
+	/* Most functions we do not care to measure these are skipped */
+	return 1;
+}
+
+static void
+early_trace_func(unsigned long ip, unsigned long parent_ip)
+{
+	struct trace_array *tr = early_trace;
+	struct trace_array_cpu *data;
+	long disabled;
+	int cpu;
+
+	if (skip(ip))
+		return;
+	/*
+	 * Does not matter if we preempt. We test the flags
+	 * afterward, to see if irqs are disabled or not.
+	 * If we preempt and get a false positive, the flags
+	 * test will fail.
+	 */
+	cpu = raw_smp_processor_id();
+
+	data = tr->data[cpu];
+	disabled = atomic_inc_return(&data->disabled);
+
+	if (likely(disabled == 1))
+#ifndef CONFIG_PREEMPT_RT
+		trace_function(tr, data, ip, parent_ip, 0);
+#else
+		trace_function(tr, data, ip, parent_ip, 0, preempt_count());
+#endif
+
+	atomic_dec(&data->disabled);
+}
+
+static struct ftrace_ops early_trace_ops __read_mostly =
+{
+	.func = early_trace_func,
+};
+
+static int compare(const void *a, const void *b)
+{
+	unsigned long *la = (unsigned long *) a;
+	unsigned long *lb = (unsigned long *) b;
+
+	if (*la > *lb)
+		return 1;
+	else if (*lb > *la)
+		return -1;
+	else
+		return 0;
+}
+
+static void swap(void *a, void *b, int size)
+{
+	unsigned long *la = (unsigned long *)a;
+	unsigned long *lb = (unsigned long *)b;
+	unsigned long t = *la;
+	*la = *lb;
+	*lb = t;
+}
+
+static void early_trace_init(struct trace_array *tr)
+{
+#ifdef EARLY_TRACE_DB
+	pr_info("early_trace_init\n");
+#endif
+
+	ftrace_function_enabled = 0;
+
+	if (!ftrace_enabled)
+		ftrace_enabled = 1;
+
+	if (!itable) {
+		unsigned long s;
+
+		s = __initcall_end - __initcall_start;
+
+#ifdef EARLY_TRACE_DB
+		pr_info("early_trace_init init call table size %lu\n", s);
+#endif
+		if (s) {
+			itable = kzalloc(s, GFP_KERNEL);
+			if (itable) {
+				unsigned long l;
+				initcall_t *call = __initcall_start;
+				itable_size = s / sizeof(unsigned long);
+
+				BUG_ON(!itable_size);
+
+				for (l = 0; l < itable_size; l++)
+					itable[l] = (unsigned long) call[l];
+
+				sort(itable, itable_size, sizeof(unsigned long),
+				     compare, swap);
+			}
+		}
+	}
+	early_trace = tr;
+	if (tr->ctrl) {
+		int cpu;
+		for_each_online_cpu(cpu)
+			tracing_reset(tr->data[cpu]);
+
+		register_ftrace_function(&early_trace_ops);
+	}
+}
+
+static void early_trace_reset(struct trace_array *tr)
+{
+	int status = -1;
+
+	if (itable) {
+		kfree(itable);
+		itable = NULL;
+		itable_size = 0;
+	}
+	if (tr->ctrl)
+		status = unregister_ftrace_function(&early_trace_ops);
+}
+
+static void early_trace_ctrl_update(struct trace_array *tr)
+{
+	if (tr->ctrl)
+		register_ftrace_function(&early_trace_ops);
+	else
+		unregister_ftrace_function(&early_trace_ops);
+}
+
+#ifndef CONFIG_DYNAMIC_FTRACE
+static struct tracer early_tracer __read_mostly =
+{
+	.name	        = "early",
+	.init           = early_trace_init,
+	.reset          = early_trace_reset,
+	.ctrl_update	= early_trace_ctrl_update,
+};
+#endif
+
+__init int init_early_trace(void)
+{
+	int ret = -1;
+#ifndef CONFIG_DYNAMIC_FTRACE
+	ret = register_tracer(&early_tracer);
+
+	if (5 != tracing_early_set_trace_write("early", 5))
+		pr_warning("Could not enable early trace\n");
+	else
+		pr_info("Early trace enabled\n");
+
+	ftrace_function_enabled = 1;
+#endif
+	return ret;
+}
+
-- 
1.6.0.4

