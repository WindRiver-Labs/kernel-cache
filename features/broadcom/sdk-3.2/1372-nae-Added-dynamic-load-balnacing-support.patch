From 54c20dfd6a878022230f0a93d8403c64cd5d29a8 Mon Sep 17 00:00:00 2001
From: Mehul Vora <mehulv@broadcom.com>
Date: Wed, 10 Oct 2012 16:41:59 +0530
Subject: [PATCH 1372/1532] nae: Added dynamic load balnacing support.

Added support for dynamic flow migration between cpus. With this
feature all TCP flows are distributed uniformly to all available
cpus without modifying the application.
[Based on SDK 3.2]
Signed-off-by: Quanyang Wang <quanyang.wang@windriver.com>
---
 drivers/net/ethernet/broadcom/nae/init_nae.c |   6 +-
 drivers/net/ethernet/broadcom/nae/xlp_nae.c  | 497 ++++++++++++++++++++++++++-
 drivers/net/ethernet/broadcom/nae/xlp_nae.h  |  38 ++
 3 files changed, 534 insertions(+), 7 deletions(-)

diff --git a/drivers/net/ethernet/broadcom/nae/init_nae.c b/drivers/net/ethernet/broadcom/nae/init_nae.c
index a27c3ba..67bf7c2 100644
--- a/drivers/net/ethernet/broadcom/nae/init_nae.c
+++ b/drivers/net/ethernet/broadcom/nae/init_nae.c
@@ -240,7 +240,11 @@ int initialize_nae(unsigned int *phys_cpu_map, int mode, int *jumbo_enabled)
 				cpu_2_normal_frfifo[node][i] = (lnx_shinfo[0].cpu_2_freeinfifo_map[pos] >> bitoff) & 0x1f;
 			}
 		}
-
+#ifdef CONFIG_NLM_ENABLE_LOAD_BALANCING
+		if(mode == NLM_TCP_MODE)
+			lnx_shinfo[0].mode = NLM_TCP_LOAD_BALANCE_MODE;
+		else
+#endif
 		lnx_shinfo[0].mode = mode;
 		lnx_shinfo[0].jumbo_enabled = *jumbo_enabled;
 		lnx_shinfo[0].node = node;
diff --git a/drivers/net/ethernet/broadcom/nae/xlp_nae.c b/drivers/net/ethernet/broadcom/nae/xlp_nae.c
index 32d69ab..16b47c1 100755
--- a/drivers/net/ethernet/broadcom/nae/xlp_nae.c
+++ b/drivers/net/ethernet/broadcom/nae/xlp_nae.c
@@ -69,9 +69,6 @@
 #include "net_common.h"
 #include "xlp_nae.h"
 
-#define NLM_TCP_MODE	1
-#define NLM_RT_MODE	2
-
 #define TSO_ENABLED	1
 
 /*Enable sanity checks while receiving or transmitting buffer */
@@ -96,6 +93,12 @@ module_param(enable_jumbo, int, 0);
 
 static int exclusive_vc = 0;
 module_param(exclusive_vc, int, 0);
+
+#ifdef CONFIG_NLM_ENABLE_LOAD_BALANCING
+static int load_balance_timer_run = 1;
+module_param(load_balance_timer_run, int, S_IRUGO|S_IWUSR);
+#endif
+
 /***************************************************************
  *
  * Below parameters are set during FDT file parsing
@@ -106,7 +109,7 @@ extern uint32_t nae_fb_vc;
 /***************************************************************/
 
 static int enable_napi =  1;
-
+static int nlm_prepad_len = 0;
 unsigned char eth_hw_addr[NLM_MAX_NODES][MAX_GMAC_PORT][6];
 static unsigned int phys_cpu_map[NLM_MAX_NODES];
 extern uint32_t cpu_2_normal_frfifo[NLM_MAX_NODES][NLM_NCPUS_PER_NODE];
@@ -172,6 +175,15 @@ extern struct eeprom_data * get_nlm_eeprom(void);
 #ifdef  ENABLE_NAE_PIC_INT
 static irqreturn_t nlm_xlp_nae_int_handler(int irq, void * dev_id);
 #endif
+#ifdef CONFIG_NLM_ENABLE_LOAD_BALANCING
+static struct flow_meta_info *nlm_flow_meta_info;
+static struct active_flow_list *nlm_active_flow_list;
+static struct timer_list nlm_load_balance_timer[NR_CPUS];
+static int nlm_load_balance_search_cpu[NUM_LOAD_BALANCE_CPU][NUM_LOAD_BALANCE_CPU-1];
+static uint32_t nlm_pcpu_mask = 0;
+static uint32_t *ucore_shared_data = NULL;
+#endif
+
 
 #define Message(fmt, args...) { }
 //#define Message(fmt, args...) printk(fmt, ##args)
@@ -457,8 +469,15 @@ static __inline__ uint64_t nae_tso_desc1(
 #define NUM_VC_PER_THREAD 4
 #define NUM_CPU_VC	  128
 #define RX_PARSER_EN 	1
+
+#ifdef CONFIG_NLM_ENABLE_LOAD_BALANCING
+#define RX_PPAD_EN 	1
+#define RX_PPAD_SZ	0
+#else
 #define RX_PPAD_EN 	0
 #define RX_PPAD_SZ	3
+#endif
+
 static void nlm_enable_l3_l4_parser(int node)
 {
 	int l2proto = 1; //ethernet
@@ -555,6 +574,448 @@ static void dump_skbuff (struct sk_buff *skb)
 }
 #endif
 
+#ifdef CONFIG_NLM_ENABLE_LOAD_BALANCING
+
+static void dump_cpu_active_flow_info(int cpu, struct seq_file *m, int weight)
+{
+	struct active_flow_list *afl = nlm_active_flow_list + cpu;
+	unsigned long mflags;
+
+	if(!nlm_active_flow_list || !nlm_flow_meta_info)
+		return;
+
+	spin_lock_irqsave(&afl->lock, mflags);
+	seq_printf(m, "Cpu%d ==> WeightInUcore %d ActiveFlows %llu, FlowCreated %llu, FlowProcessed %llu\n", cpu, weight, (unsigned long long)afl->nr_active_flows, (unsigned long long)afl->nr_flow_created, (unsigned long long)afl->nr_flow_processed);
+	afl->nr_flow_created = 0;
+	afl->nr_flow_processed = 0;
+	spin_unlock_irqrestore(&afl->lock, mflags);
+	return;
+}
+
+static int nlm_dump_load_balance_stats(struct seq_file *m, void *v)
+{
+	int i = 0;
+	uint32_t cpu_weight[32] = {0};
+
+	if(!nlm_active_flow_list || !nlm_flow_meta_info){
+		return 0;
+	}
+
+	for(i=0; i<NLM_UCORE_SHARED_TABLE_SIZE; i++){
+		cpu_weight[*(ucore_shared_data + i)]++;
+	}
+
+	for(i=0; i<NUM_LOAD_BALANCE_CPU; i++){
+		if(!cpu_isset(i, phys_cpu_present_map))
+        	continue;
+		dump_cpu_active_flow_info(i, m, cpu_weight[__cpu_number_map[i]]);
+	}
+	return 0;
+}
+
+static int nlm_load_balance_proc_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, nlm_dump_load_balance_stats, NULL);
+}
+
+struct file_operations nlm_load_balance_proc_fops = 
+{
+		.owner = THIS_MODULE,
+		.open = nlm_load_balance_proc_open,
+		.read = seq_read,
+		.llseek = seq_lseek,
+		.release = seq_release,
+};
+
+static void nlm_remove_inactive_flow(int cpu)
+{
+	int i = 0;
+	struct active_flow_list *afl = nlm_active_flow_list + cpu;
+	uint64_t bytes_received;
+	struct flow_meta_info *fmi;
+	int index;
+	unsigned long mflags;
+
+	spin_lock_irqsave(&afl->lock, mflags);
+	for(i=0; i<afl->nr_active_flows; i++){
+		index = afl->index_to_flow_meta_info[i];
+		fmi = nlm_flow_meta_info + index;
+#ifdef LOAD_BALANCE_DEBUG_ENABLE
+		if(index > NLM_UCORE_SHARED_TABLE_SIZE){
+				printk("??? Index %d\n",index);
+		}
+#endif
+		bytes_received = fmi->total_bytes_rcvd - fmi->last_sample_bytes;
+		fmi->last_sample_bytes = fmi->total_bytes_rcvd;
+		mb();
+		if(bytes_received == 0){
+#ifdef LOAD_BALANCE_DEBUG_ENABLE
+			if(fmi->cpu_owner != cpu){
+					printk("Error!! fmi->cpu_owner = %lld, cpu %d, entry %d, total_active_flows %lld\n", fmi->cpu_owner, cpu, i, afl->nr_active_flows);
+					continue;
+			}
+#endif
+			/*No Packet Received on this flow!!! Delete the entry..*/
+			if(i+1 < afl->nr_active_flows){
+				/*Copy the last valid map to here..*/
+				afl->index_to_flow_meta_info[i] = afl->index_to_flow_meta_info[afl->nr_active_flows-1];
+				afl->index_to_flow_meta_info[afl->nr_active_flows - 1] = 0;
+			}else{
+				afl->index_to_flow_meta_info[i] = 0;
+			}
+			i--;
+			afl->nr_active_flows--;
+			afl->nr_flow_processed++;
+			fmi->cpu_owner = -1;
+			mb();
+			continue;
+		}
+		mb();
+	}
+	spin_unlock_irqrestore(&afl->lock, mflags);
+}
+
+static void setup_search_path(void)
+{
+	int i = 0;
+	int thrds[4] = {0, 1, 2, 3};
+	int core;
+	int previous, next;
+	int index = 0;
+	int num_phy_cpu=0;
+	unsigned int pcpu;
+	int previous_core, next_core;
+#ifdef LOAD_BALANCE_DEBUG_ENABLE
+	int j;
+	unsigned char buf[256];
+#endif
+
+	for(i=0; i<NUM_LOAD_BALANCE_CPU; i++){
+		if(!cpu_isset(i, phys_cpu_present_map))
+			continue;
+		nlm_pcpu_mask |= (1UL<<i);
+		num_phy_cpu++;
+	}
+#ifdef LOAD_BALANCE_DEBUG_ENABLE
+	printk("num_phy_cpu %d, nlm_pcpu_mask %#x\n",num_phy_cpu, nlm_pcpu_mask);
+#endif
+	for(pcpu=0; pcpu<NUM_LOAD_BALANCE_CPU; pcpu++){
+
+		if(!((1U<<pcpu) & nlm_pcpu_mask))
+			continue;
+
+		core = pcpu/4;
+		index = 0;
+		for(i=0; i<3; i++){
+			nlm_load_balance_search_cpu[pcpu][index] = thrds[(pcpu+i+1)%4] + core*4;
+			if(((1U<<nlm_load_balance_search_cpu[pcpu][index]) & nlm_pcpu_mask)){
+				index++;
+			}
+		}
+		if(core >= 1){
+			previous = (core-1)*4;
+		}else{
+			previous = 28;
+		}
+		previous_core = previous/4;
+
+		if(core < 7){
+			next = (core+1)*4;
+		}else{
+			next = 0;
+		}
+		next_core = next/4;
+
+		while(index<(num_phy_cpu-1)){
+			if((0xfU<<previous) & nlm_pcpu_mask){
+				for(i=0; i<4; i++){
+					if((1UL<<(i+previous) & nlm_pcpu_mask)){
+						nlm_load_balance_search_cpu[pcpu][index] = i+previous;
+						index++;
+					}
+				}
+			}
+			if((previous_core) >= 1){
+				previous = (previous_core-1)*4;
+			}else{
+				previous = 28;
+			}
+			previous_core = previous/4;
+			if((0xfU<<next) & nlm_pcpu_mask){
+				for(i=0; i<4; i++){
+					if((1UL<<(i+next)) & nlm_pcpu_mask){
+						nlm_load_balance_search_cpu[pcpu][index] = i+next;
+						index++;
+					}
+				}
+			}
+			if(next_core < 7){
+				next = (next_core+1)*4;
+			}else{
+				next = 0;
+			}
+			next_core = next/4;
+		}
+	}
+#ifdef LOAD_BALANCE_DEBUG_ENABLE
+	printk("\nSearchPath:\n");
+	for(i=0; i<NUM_LOAD_BALANCE_CPU; i++){
+		int len;
+		sprintf(buf, "Cpu%d ==> ", i);
+		len = strlen(buf);
+		for(j=0; j<NUM_LOAD_BALANCE_CPU-1; j++){
+			sprintf(buf+len, "%2d ", nlm_load_balance_search_cpu[i][j]);
+			len = strlen(buf);
+		}
+		sprintf(buf+len,"\n");
+		printk(buf);
+	}
+#endif
+	return;
+}
+
+static void nlm_load_balance_timer_func(unsigned long arg)
+{
+	int cpu = hard_smp_processor_id();
+	struct active_flow_list *myafl = nlm_active_flow_list + cpu;
+	struct active_flow_list *afl = NULL;
+	int i = 0, j;
+	int idx_to_fmi;
+	struct flow_meta_info *fmi;
+	unsigned long mflags;
+	int lcpu;
+
+	/*Remove inactive flows.*/
+	nlm_remove_inactive_flow(cpu);
+
+	/*Check if there is a `threshold %` of diff in load between us and any other cpus. If so
+	 borrow few flows from it.
+	 */
+	for(i=0; i<(NUM_LOAD_BALANCE_CPU-1); i++){
+		if(!cpu_isset(i, phys_cpu_present_map))
+			continue;
+		afl = nlm_active_flow_list + nlm_load_balance_search_cpu[cpu][i];
+		Message("Checking for CPU %d\n", nlm_load_balance_search_cpu[cpu][i]);
+
+		if(afl->nr_active_flows <= 1)
+			continue;
+
+		if(myafl->nr_active_flows < (afl->nr_active_flows - 1)){
+			/*Borrow a flow from this cpu*/
+			Message("NR_ACTIVE_FLOWS %ld",afl->nr_active_flows);
+
+restart:
+			spin_lock_irqsave(&afl->lock, mflags);
+			for(j=0; j<afl->nr_active_flows; j++){
+					fmi = nlm_flow_meta_info + afl->index_to_flow_meta_info[j];
+
+					if(myafl->nr_active_flows >= (afl->nr_active_flows - 1)){
+						break;
+					}
+					if(fmi->cpu_owner != nlm_load_balance_search_cpu[cpu][i]){
+							spin_unlock_irqrestore(&afl->lock, mflags);
+							Message("Flow is borrowed by cpu %lld\n",fmi->cpu_owner);
+							goto restart;
+					}
+					/*Borrow a flow...*/
+					fmi->cpu_owner = cpu;
+					idx_to_fmi = afl->index_to_flow_meta_info[j];
+					/*Update the active flow list of the cpu from which we just
+					  borrowed the flow.*/
+					if(j+1 < afl->nr_active_flows){
+							/*Copy the last valid map to here..*/
+							afl->index_to_flow_meta_info[j] = 
+									afl->index_to_flow_meta_info[afl->nr_active_flows - 1];
+							afl->index_to_flow_meta_info[afl->nr_active_flows-1] = 0;
+					}else{
+							afl->index_to_flow_meta_info[j] = 0;
+					}
+					j--;
+					afl->nr_active_flows--;
+					spin_unlock_irqrestore(&afl->lock, mflags);
+
+					spin_lock_irqsave(&myafl->lock, mflags);
+					/*Create a new entry for this FMI in ACTIVE_FLOW_LIST 
+					  Change the u-core shared memroy once all data structures
+					  are in place.
+					 */
+					myafl->index_to_flow_meta_info[myafl->nr_active_flows] = idx_to_fmi;
+					myafl->nr_active_flows++;
+					mb();
+					spin_unlock_irqrestore(&myafl->lock, mflags);
+
+					/*Update ucore shared memory*/
+					lcpu = __cpu_number_map[cpu];
+					nlm_hal_modify_nae_ucore_sram_mem(0, 0, &lcpu, NLM_UCORE_SHMEM_OFFSET+(idx_to_fmi*4),1);
+					*(ucore_shared_data + idx_to_fmi) = __cpu_number_map[cpu];
+
+					spin_lock_irqsave(&afl->lock, mflags);
+			}
+			spin_unlock_irqrestore(&afl->lock, mflags);
+		}
+	}
+	if(!myafl->nr_active_flows)
+		mod_timer(&nlm_load_balance_timer[cpu], jiffies + 50 );
+	else
+		mod_timer(&nlm_load_balance_timer[cpu], jiffies + load_balance_timer_run*HZ );
+}
+
+static void nlm_init_load_balance(void)
+{
+	int i = 0;
+	int j = 0;
+	uint32_t signature;
+	nlm_active_flow_list = vmalloc(sizeof(struct active_flow_list) * NUM_LOAD_BALANCE_CPU);
+
+	if(!nlm_active_flow_list){
+		printk("\nAllocation Failed!!! for size %lu bytes\n", sizeof(struct active_flow_list) * NUM_LOAD_BALANCE_CPU);
+		return;
+	}
+	memset(nlm_active_flow_list, 0, sizeof(struct active_flow_list)*NUM_LOAD_BALANCE_CPU);
+#ifdef LOAD_BALANCE_DEBUG_ENABLE
+	printk("Allocated active_flow_list @ %#lx, size %lu bytes\n", (unsigned long)nlm_active_flow_list, sizeof(struct active_flow_list) * NUM_LOAD_BALANCE_CPU);
+#endif	
+	nlm_flow_meta_info = vmalloc(sizeof(struct flow_meta_info)*(NLM_UCORE_SHARED_TABLE_SIZE+1));
+
+	if(!nlm_flow_meta_info){
+		printk("\nAllocation Failed!!! for size %lu bytes\n", sizeof(struct flow_meta_info) * NUM_LOAD_BALANCE_CPU);
+		return;
+	}
+	memset(nlm_flow_meta_info, 0, sizeof(struct flow_meta_info)*(NLM_UCORE_SHARED_TABLE_SIZE+1));
+#ifdef LOAD_BALANCE_DEBUG_ENABLE
+	printk("Allocated flow_meta_info @ %#lx, size %lu bytes\n",(unsigned long)nlm_flow_meta_info, sizeof(struct flow_meta_info)*(NLM_UCORE_SHARED_TABLE_SIZE+1));
+#endif	
+
+	/*Init per cpu flow lock*/
+	for(i=0; i<NUM_LOAD_BALANCE_CPU; i++){
+		spin_lock_init(&((nlm_active_flow_list + i)->lock));
+	}
+
+	/*Set owner field to -1*/
+	for(i=0; i<NLM_UCORE_SHARED_TABLE_SIZE; i++){
+		(nlm_flow_meta_info+i)->cpu_owner = -1;
+	}
+
+	for(i=0,j=0; i<NUM_LOAD_BALANCE_CPU; i++){
+		if(!cpu_isset(i, phys_cpu_present_map))
+			continue;
+		/*Register a timer for each cpu to calculate the flow rate*/
+		setup_timer(&nlm_load_balance_timer[i], nlm_load_balance_timer_func, i);
+		nlm_load_balance_timer[i].expires = jiffies + 5*HZ;
+		add_timer_on(&nlm_load_balance_timer[i], j++);
+	}
+
+	setup_search_path();
+	/*Update ucore shared memory with the table*/
+	ucore_shared_data = vmalloc(NLM_UCORE_SHARED_TABLE_SIZE*sizeof(uint32_t));
+	if(!ucore_shared_data){
+			printk("Ucore updation failed!!\n");
+			return;
+	}
+
+	for(i=0; i<NLM_UCORE_SHARED_TABLE_SIZE; ){
+			for(j=0; j<NUM_LOAD_BALANCE_CPU && i<NLM_UCORE_SHARED_TABLE_SIZE; j++){
+				if(nlm_pcpu_mask & (1U<<j)){
+					*(ucore_shared_data + i) = __cpu_number_map[j];
+					i++;
+				}
+			}
+	}
+	nlm_hal_modify_nae_ucore_sram_mem(0, 0, ucore_shared_data, NLM_UCORE_SHMEM_OFFSET, NLM_UCORE_SHARED_TABLE_SIZE);
+	mb();
+	signature = 0xdeadbeefU;
+	nlm_hal_modify_nae_ucore_sram_mem(0, 0, &signature, NLM_UCORE_SHMEM_OFFSET-4, 1);
+}
+
+#ifdef LOAD_BALANCE_DEBUG_ENABLE
+static void dump_packet(unsigned char *vaddr, int len)
+{
+	int i = 0;
+	for(i=0; i<len;){
+		printk("[%#x] [%#x] [%#x] [%#x] [%#x] [%#x] [%#x] [%#x]\n", *(vaddr + i + 0),
+			*(vaddr + i + 1), *(vaddr + i + 2), *(vaddr + i + 3), *(vaddr + i + 4), *(vaddr + i + 5), *(vaddr + i + 6), *(vaddr + i + 7));
+		i=i+8;
+	}
+}
+
+static void dump_prepad(unsigned char *vaddr)
+{
+	int i = 0;
+	unsigned int *tmp = (unsigned int *)vaddr;
+	if(*tmp == 0xdeadbeef){
+		for(i=0; i<4; i++){
+				printk("[%d] ==> [%#x]\n",i, *(tmp+i));
+		}
+	}else{
+#if 1
+	for(i=0; i<16;){
+		printk("[%#x] [%#x] [%#x] [%#x] [%#x] [%#x] [%#x] [%#x]\n", *(vaddr + i + 0),
+			*(vaddr + i + 1), *(vaddr + i + 2), *(vaddr + i + 3), *(vaddr + i + 4), *(vaddr + i + 5), *(vaddr + i + 6), *(vaddr + i + 7));
+		i=i+8;
+	}
+#endif
+	}
+}
+#endif
+
+static inline void nlm_update_flow_stats(unsigned int *prepad, uint32_t len, uint32_t context)
+{
+	int hash_index;
+	int cpu = hard_smp_processor_id();
+	unsigned long mflags;
+	struct active_flow_list *afl = nlm_active_flow_list + cpu;
+	struct flow_meta_info *fmi;
+	uint64_t index;
+
+	if(perf_mode != NLM_TCP_MODE)
+		return;
+
+	if(*prepad != NLM_LOAD_BALANCING_MAGIC){
+		/*No extractions have happend...*/
+#ifdef LOAD_BALANCE_DEBUG_ENABLE
+		printk("Invalid Packet!!\n");
+		dump_packet(prepad, 64+16);
+#endif
+		return;
+	}
+
+	hash_index = *(prepad + 1);
+	fmi = nlm_flow_meta_info + hash_index;
+
+	if(unlikely(fmi->cpu_owner == -1)){
+		/*New flow, Create an entry in active flow list*/
+		local_irq_save(mflags);
+		if(!spin_trylock(&afl->lock)){
+			local_irq_restore(mflags);
+			return;
+		}
+		if(fmi->cpu_owner != -1){
+			spin_unlock(&afl->lock);
+			local_irq_restore(mflags);
+			return; 
+		}
+		fmi->cpu_owner = cpu;
+		index = afl->nr_active_flows;
+#ifdef LOAD_BALANCE_DEBUG_ENABLE
+		if(index >= NLM_UCORE_SHARED_TABLE_SIZE){
+				printk("Index %lld Crossing Table size...\n", index);
+		}
+#endif
+		afl->index_to_flow_meta_info[index] = hash_index;
+		mb();
+		afl->nr_active_flows++;
+		afl->nr_flow_created++;
+		spin_unlock(&afl->lock);
+		local_irq_restore(mflags);
+	}
+	fmi->total_bytes_rcvd += len;
+	mb();
+	return;
+}
+
+#endif
+
+
 #ifdef CONFIG_NLM_NET_OPTS
 /* Get the hardware replenishment queue id */
 static int get_hw_frfifo_queue_id(int rxnode, nlm_nae_config_ptr nae_cfg, int cpu, unsigned int truesize)
@@ -819,7 +1280,7 @@ static inline void process_rx_packets(int cpu, unsigned int src_id,
 
 	vaddr = (uint64_t)(unsigned long)bus_to_virt(addr);
 
-	len = len  - ETH_FCS_LEN - PREPAD_LEN;
+	len = len  - ETH_FCS_LEN - nlm_prepad_len;
 
 	skb = mac_get_skb_back_ptr(vaddr);
 
@@ -844,7 +1305,12 @@ static inline void process_rx_packets(int cpu, unsigned int src_id,
 	memset(shhwtstamps, 0, sizeof(*shhwtstamps));
 	shhwtstamps->hwtstamp = ns_to_ktime(ns);
 	shhwtstamps->syststamp = timecompare_transform(&priv->compare, ns);
-	skb_reserve(skb, PREPAD_LEN);
+	skb_reserve(skb, nlm_prepad_len);
+#elif defined(CONFIG_NLM_ENABLE_LOAD_BALANCING)
+	if(!priv->mgmt_port){
+		nlm_update_flow_stats((unsigned int *)vaddr, len, context);
+	}
+	skb_reserve(skb, nlm_prepad_len);
 #endif
 
 	if(priv->index == XGMAC)
@@ -1256,6 +1722,7 @@ static void nlm_xlp_nae_init(void)
 			priv->node 	= node;
 			priv->block	= nae_cfg->ports[i].hw_port_id / 4;
 			priv->type	= nae_cfg->ports[i].iftype;
+			priv->mgmt_port = nae_cfg->ports[i].mgmt;
 
 			switch(nae_cfg->ports[i].iftype) {
 				case SGMII_IF:
@@ -1355,6 +1822,24 @@ static void nlm_xlp_nae_init(void)
 		nlm_spawn_kthread();
 	}
 
+#ifdef CONFIG_NLM_ENABLE_LOAD_BALANCING
+	if(perf_mode == NLM_TCP_MODE)
+		nlm_prepad_len = PREPAD_LEN;
+#elif defined(IEEE_1588_PTP_ENABLED)
+	nlm_prepad_len = PREPAD_LEN;
+#endif
+
+#ifdef CONFIG_NLM_ENABLE_LOAD_BALANCING
+	if(perf_mode == NLM_TCP_MODE){
+		entry = create_proc_entry("load_info", 0 /* def mode */ ,
+				   nlm_root_proc /* parent */ );
+		if(entry){
+				entry->proc_fops = &nlm_load_balance_proc_fops;
+		}
+		nlm_init_load_balance();
+	}
+#endif
+
 	if(replenish_freein_fifos() != 0) {
 		printk("Replenishmemt of freein fifos failed\n");
 	}
diff --git a/drivers/net/ethernet/broadcom/nae/xlp_nae.h b/drivers/net/ethernet/broadcom/nae/xlp_nae.h
index 5a83408..234cd48 100644
--- a/drivers/net/ethernet/broadcom/nae/xlp_nae.h
+++ b/drivers/net/ethernet/broadcom/nae/xlp_nae.h
@@ -15,9 +15,15 @@
 #define ETH_FCS_LEN     4               /* Octets in the FCS             */
 #endif
 
+#define NLM_TCP_MODE	1
+#define NLM_RT_MODE	2
+#define NLM_TCP_LOAD_BALANCE_MODE	3
 
 #ifdef IEEE_1588_PTP_ENABLED
 #define PREPAD_LEN		64
+#elif defined(CONFIG_NLM_ENABLE_LOAD_BALANCING)
+#define PREPAD_LEN		16
+#define NLM_LOAD_BALANCING_MAGIC	0xdeadbeefU
 #else
 #define PREPAD_LEN   		0
 #endif
@@ -27,6 +33,13 @@
 #define CACHELINE_SIZE		(1ULL << 6)
 #define CACHELINE_ALIGNED(addr)	( ((addr) + (CACHELINE_SIZE-1)) & ~(CACHELINE_SIZE-1) )
 
+#ifdef CONFIG_NLM_ENABLE_LOAD_BALANCING
+#define KB(x)	(x<<10)
+#define NLM_UCORE_SHARED_TABLE_SIZE ((KB(16))/(sizeof(uint32_t)))
+#define NLM_UCORE_SHMEM_OFFSET	(KB(16))
+#define NUM_LOAD_BALANCE_CPU	32
+#endif
+
 #define SKB_BACK_PTR_SIZE	CACHELINE_SIZE
 
 #define NLM_RX_ETH_BUF_SIZE	(ETH_DATA_LEN+ETH_HLEN+ETH_FCS_LEN+BYTE_OFFSET+PREPAD_LEN+SKB_BACK_PTR_SIZE+CACHELINE_SIZE)
@@ -71,6 +84,30 @@ struct phy_info {
         uint32_t *serdes_addr;
 };
 
+#ifdef CONFIG_NLM_ENABLE_LOAD_BALANCING
+/*Flow Meta Info*/
+struct flow_meta_info
+{
+	volatile uint64_t total_bytes_rcvd;
+	volatile uint64_t last_sample_bytes;
+	volatile uint64_t cpu_owner;
+	uint64_t pad[5];
+};
+
+/*Active flow list*/
+struct active_flow_list
+{
+	volatile int index_to_flow_meta_info[NLM_UCORE_SHARED_TABLE_SIZE];
+	volatile uint64_t cpu_data_rate;
+	spinlock_t lock;
+	volatile uint64_t nr_active_flows;
+	volatile uint64_t nr_flow_created;
+	volatile uint64_t nr_flow_processed;
+	uint64_t pad[3];
+};
+#endif
+
+
 struct dev_data
 {
         struct net_device *dev;
@@ -97,6 +134,7 @@ struct dev_data
         int nae_rx_qid;
         int nae_tx_qid;
 	int hw_port_id;
+	int mgmt_port;
 	struct net_lro_mgr lro_mgr[NR_CPUS];
 	struct net_lro_desc lro_arr[NR_CPUS][LRO_MAX_DESCS];
 	
-- 
1.9.1

