From a0d819ce3a679711b45282da6ea50509b8a51c80 Mon Sep 17 00:00:00 2001
From: Rahul Jain <rajain@netlogicmicro.com>
Date: Thu, 19 Jan 2012 10:13:01 +0530
Subject: [PATCH 1103/1532] Jumbo Frame support added in nae-perf. nae_perf:
 enable_jumbo parameter for enabling jumbo frames

[Based on SDK 3.2]
Signed-off-by: Quanyang Wang <quanyang.wang@windriver.com>
---
 drivers/misc/netlogic/nae-perf/init_nae.c |   3 +-
 drivers/misc/netlogic/nae-perf/xlp_nae.c  | 256 +++++++++++++++++++-----------
 drivers/misc/netlogic/nae-perf/xlp_nae.h  |  34 ++--
 3 files changed, 186 insertions(+), 107 deletions(-)

diff --git a/drivers/misc/netlogic/nae-perf/init_nae.c b/drivers/misc/netlogic/nae-perf/init_nae.c
index bca9840..6dad687 100644
--- a/drivers/misc/netlogic/nae-perf/init_nae.c
+++ b/drivers/misc/netlogic/nae-perf/init_nae.c
@@ -15,12 +15,11 @@
 
 extern int rely_on_firmware_config;
 
-
 int initialize_nae(uint32_t cm0, uint32_t cm1, uint32_t cm2, uint32_t cm3)
 {
 	int dom_id = 0;
-	unsigned long mflags;
 	int node = 0;
+	unsigned long __attribute__ ((unused)) mflags;
 
 	msgrng_access_enable(mflags);
 	nlm_hal_init_nae(fdt, dom_id);
diff --git a/drivers/misc/netlogic/nae-perf/xlp_nae.c b/drivers/misc/netlogic/nae-perf/xlp_nae.c
index e6e1039..62fa0fd 100755
--- a/drivers/misc/netlogic/nae-perf/xlp_nae.c
+++ b/drivers/misc/netlogic/nae-perf/xlp_nae.c
@@ -94,25 +94,22 @@
 #define MSG_INFLIGHT_MSG_EX             0x02
 #define MSG_TXQ_FULL                    0x04
 
-#define ETH_MTU_SIZE		 	1536
-#define MIN_ETH_FRAME_SIZE		64
-
 #define  DUMP_PKT(str, x, y)	if (debug == 2)  {	\
-	int i;      				\
-        printk(" %s \n", str);                  \
-        for(i = 0; i < y; i++)			\
-        {					\
+	int i;      					\
+        printk(" %s \n", str);                  	\
+        for(i = 0; i < y; i++)				\
+        {						\
                 printk("%02x ", (x)[i]);		\
-                if( i % 16 == 15)		\
-                        printk("\n");		\
-        }					\
+                if( i % 16 == 15)			\
+                        printk("\n");			\
+        }						\
 	printk("\n"); }
 
 #define NLM_TCP_MODE	1
 #define NLM_RT_MODE	2
 
 /* Applicable only in tcp mode */
-#define TSO_ENABLED 1
+#define TSO_ENABLED 	1
 
 /*Enable sanity checks while receiving or transmitting buffer */
 #undef ENABLE_SANITY_CHECKS
@@ -132,6 +129,9 @@ module_param(enable_napi, int, 0);
 static int num_descs_perq = 500;
 module_param(num_descs_perq, int, 0);
 
+static int enable_jumbo = 0;
+module_param(enable_jumbo, int, 0);
+
 /***************************************************************
  *
  * Below parameters are set during FDT file parsing
@@ -142,6 +142,8 @@ static uint32_t frin_total_queue = 18;
 static uint32_t nae_rx_vc = 0;
 static uint32_t nae_fb_vc = 0;
 static uint32_t napi_vc_mask;
+static uint32_t num_cpu_share_freein = 4;
+static uint32_t jumbo_freein_offset = 8;
 /***************************************************************/
 
 unsigned char eth_hw_addr[18][6] = {
@@ -171,6 +173,7 @@ unsigned char eth_hw_addr[18][6] = {
 uint64_t nlm_mode[NR_CPUS*8] ____cacheline_aligned;
 struct sk_buff *last_rcvd_skb[NR_CPUS * 8] ____cacheline_aligned;
 uint64_t last_rcvd_skb_phys[NR_CPUS * 8] ____cacheline_aligned;
+uint64_t receive_count[NR_CPUS * 8] __cacheline_aligned;
 uint64_t fast_replenish_count[NR_CPUS * 8] __cacheline_aligned;
 uint64_t slow_replenish_count[NR_CPUS * 8] __cacheline_aligned;
 uint64_t err_replenish_count[NR_CPUS * 8] __cacheline_aligned;
@@ -184,20 +187,19 @@ static struct pci_device_id soc_pci_table[] __devinitdata = {
         {}
 };
 
-
-#define MAX_TSO_SKB_PEND_REQS 	200
+#define MAX_TSO_SKB_PEND_REQS	200
 #define MAX_PACKET_SZ_PER_MSG	16384
-#define P2P_EXTRA_DESCS	      	((PAGE_SIZE / MAX_PACKET_SZ_PER_MSG) + 4)
-#define P2P_SKB_OFF	      	(MAX_SKB_FRAGS + P2P_EXTRA_DESCS - 1)
-#define tso_dbg(fmt, args...) //printk(fmt, ##args);
+#define P2P_EXTRA_DESCS		((PAGE_SIZE / MAX_PACKET_SZ_PER_MSG) + 4)
+#define P2P_SKB_OFF		(MAX_SKB_FRAGS + P2P_EXTRA_DESCS - 1)
+#define tso_dbg(fmt, args...)	//printk(fmt, ##args);
 #define RX_UNCLASSIFIED_PKT 	(1<<5)
 #define RX_IP_CSUM_VALID 	(1<<3)
 #define RX_TCP_CSUM_VALID 	(1<<2)
+
 static uint64_t dbg_tcp_rx_cons[NR_CPUS * 8] __cacheline_aligned;
 static uint64_t p2p_dynamic_alloc_cnt[NR_CPUS * 8] __cacheline_aligned;
 #define CPU_INDEX(x) (x * 8)
 
-
 struct p2p_desc_mem {
 	void *mem;
 	uint64_t dsize;
@@ -296,7 +298,7 @@ static __inline__ void print_fmn_send_error(const char *str, uint32_t send_resul
 
 static __inline__ struct sk_buff *mac_get_skb_back_ptr(uint64_t addr)
 {
-        uint64_t *back_ptr = (uint64_t *)(addr - CACHELINE_SIZE);
+        uint64_t *back_ptr = (uint64_t *)(addr - MAC_SKB_BACK_PTR_SIZE);
         /* this function should be used only for newly allocated packets. It assumes
          * the first cacheline is for the back pointer related book keeping info
          */
@@ -310,7 +312,7 @@ static __inline__ void mac_put_skb_back_ptr(struct sk_buff *skb)
         /* this function should be used only for newly allocated packets. It assumes
          * the first cacheline is for the back pointer related book keeping info
          */
-        skb_reserve(skb, CACHELINE_SIZE);
+        skb_reserve(skb, MAC_SKB_BACK_PTR_SIZE);
         *back_ptr = (uint64_t)skb;
 }
 
@@ -329,7 +331,7 @@ static __inline__ void *cacheline_aligned_kmalloc(int size, int gfp_mask)
 						    CACHELINE_SIZE));
         return buf;
 }
-
+#if 0
 /**********************************************************************
  * nlm_xlp_alloc_skb -  64 bits cache aligned skb buffer allocate
  * return - skb buffer address
@@ -352,16 +354,16 @@ static __inline__ struct sk_buff *nlm_xlp_alloc_skb(void)
 #endif
         return skb;
 }
-
+#endif
 /**********************************************************************
  * nlm_xlp_alloc_skb_atomic -  Atomically allocates 64 bits cache aligned skb buffer
  * return - skb buffer address
  *
  **********************************************************************/
-static __inline__ struct sk_buff *nlm_xlp_alloc_skb_atomic(void)
+static __inline__ struct sk_buff *nlm_xlp_alloc_skb_atomic(int size)
 {
         int offset = 0;
-        struct sk_buff *skb = __dev_alloc_skb(NLM_RX_BUF_SIZE, GFP_ATOMIC);
+        struct sk_buff *skb = __dev_alloc_skb(size, GFP_ATOMIC);
 
         if (!skb) {
                 return NULL;
@@ -514,7 +516,7 @@ static __inline__ uint64_t nae_tso_desc1(
 #define NUM_VC_PER_THREAD 4
 #define NUM_CPU_VC	  128
 #define RX_PARSER_EN 	1
-#define RX_PPAD_EN 	0 
+#define RX_PPAD_EN 	0
 #define RX_PPAD_SZ	3
 static void nlm_enable_l3_l4_parser(int node)
 {
@@ -528,7 +530,7 @@ static void nlm_enable_l3_l4_parser(int node)
 
 	//enabling hardware parser
 	naereg = nlm_hal_read_nae_reg(node, RX_CONFIG);
-	nlm_hal_write_nae_reg(node, RX_CONFIG, (naereg | RX_PARSER_EN << 12 | RX_PPAD_EN << 13 | RX_PPAD_SZ << 22)); 
+	nlm_hal_write_nae_reg(node, RX_CONFIG, (naereg | RX_PARSER_EN << 12 | RX_PPAD_EN << 13 | RX_PPAD_SZ << 22));
 	printk("Enabling parser, reg content = %x\n", nlm_hal_read_nae_reg(node, RX_CONFIG));
 
 	/* enabling extraction of data */
@@ -548,7 +550,7 @@ static void nlm_enable_l3_l4_parser(int node)
 
 	/* Configure flow table 1 to the num cpus as the modular */
 	for(i = 0; i < NR_CPUS; i++) {
-		if(cpu_isset(i, cpu_present_map)) 
+		if(cpu_isset(i, cpu_present_map))
 			num_cpus++;
 	}
 	for(i = 0; i < 20; i++) {
@@ -556,8 +558,8 @@ static void nlm_enable_l3_l4_parser(int node)
 		nlm_hal_write_nae_reg(node, 0x84, (num_cpus << 20)| i);
 	}
 
-	
-	/* Using hash based distribution, Configure the flow table to send packets to the nae_rx_vc of 
+
+	/* Using hash based distribution, Configure the flow table to send packets to the nae_rx_vc of
 	 each online cpu */
 	dstvc = nae_rx_vc;
 	for(i = 0; i < 512;) {
@@ -583,12 +585,12 @@ static int lro_get_skb_hdr(struct sk_buff *skb, void **iphdr, void **tcph,
 	skb_reset_network_header(skb);
 	skb_set_transport_header(skb, ip_hdrlen(skb));
 
-	if(ip_hdr(skb)->protocol != 0x6) 
+	if(ip_hdr(skb)->protocol != 0x6)
 		return -1;
 
 	*iphdr = ip_hdr(skb);
 	*tcph = tcp_hdr(skb);
-	
+
 	*hdr_flags = LRO_IPV4 | LRO_TCP;
 
 	return 0;
@@ -596,7 +598,7 @@ static int lro_get_skb_hdr(struct sk_buff *skb, void **iphdr, void **tcph,
 
 void dump_parser_config(int node)
 {
-	int i; 
+	int i;
 	uint32_t val;
 
 	val = nlm_hal_read_nae_reg(node, 0x2f);
@@ -699,13 +701,14 @@ static int mac_refill_frin_skb(uint64_t paddr, int qid)
 	return ret;
 }
 
-static int mac_refill_frin_one_buffer(struct net_device *dev, int cpu)
+static int mac_refill_frin_one_buffer(struct net_device *dev, int cpu, int size)
 {
 	struct dev_data* priv;
 	struct net_device *ndev;
 	struct sk_buff * skb;
+	int freein_fifo;
 
-	skb = nlm_xlp_alloc_skb_atomic();
+	skb = nlm_xlp_alloc_skb_atomic(size);
 	if(!skb)
 	  {
 	    printk("[%s] alloc skb failed\n",__FUNCTION__);
@@ -716,8 +719,16 @@ static int mac_refill_frin_one_buffer(struct net_device *dev, int cpu)
 	ndev = (struct net_device *)dev;
 	priv = netdev_priv(ndev);
 	skb->dev = ndev;
+
+	freein_fifo = frin_queue_base + (cpu/num_cpu_share_freein);
+	if (enable_jumbo && size > NLM_RX_BUF_SIZE)
+		freein_fifo += jumbo_freein_offset;
+
+	skb->queue_id = freein_fifo;
+
 	mac_put_skb_back_ptr(skb);
-	return mac_refill_frin_skb((unsigned long long)virt_to_bus(skb->data), frin_queue_base + (cpu/2));
+
+	return mac_refill_frin_skb((unsigned long long)virt_to_bus(skb->data), freein_fifo);
 }
 
 static int nae_proc_read(char *page, char **start, off_t off,
@@ -725,21 +736,35 @@ static int nae_proc_read(char *page, char **start, off_t off,
 {
 	int len = 0;
 	int i = 0;
-	uint64_t total_err = 0, total_fast = 0, total_slow = 0;
+	uint64_t total_err = 0, total_fast = 0, total_slow = 0, total_recv = 0;
 
 	for(i=0; i<32; i++){
-		printk("cpu%d, fast_repl %ld, slow_repl %ld, err_repl %ld tcprxcons %lld p2pdalloc %lld\n",i, (unsigned long)fast_replenish_count[LAST_RCVD_INDEX(i)], (unsigned long)slow_replenish_count[LAST_RCVD_INDEX(i)], (unsigned long)err_replenish_count[LAST_RCVD_INDEX(i)], dbg_tcp_rx_cons[CPU_INDEX(i)], p2p_dynamic_alloc_cnt[CPU_INDEX(i)]);
+		printk("cpu%d, recv %ld fast_repl %ld, slow_repl %ld, err_repl %ld tcprxcons %lld p2pdalloc %lld\n",i,
+			(unsigned long)receive_count[LAST_RCVD_INDEX(i)],
+			(unsigned long)fast_replenish_count[LAST_RCVD_INDEX(i)],
+			(unsigned long)slow_replenish_count[LAST_RCVD_INDEX(i)],
+			(unsigned long)err_replenish_count[LAST_RCVD_INDEX(i)],
+			dbg_tcp_rx_cons[CPU_INDEX(i)],
+			p2p_dynamic_alloc_cnt[CPU_INDEX(i)]);
+
 		total_err += err_replenish_count[LAST_RCVD_INDEX(i)];
 		total_fast += fast_replenish_count[LAST_RCVD_INDEX(i)];
 		total_slow += slow_replenish_count[LAST_RCVD_INDEX(i)];
+		total_recv += receive_count[LAST_RCVD_INDEX(i)];
+
 		dbg_tcp_rx_cons[CPU_INDEX(i)] = 0;
 		p2p_dynamic_alloc_cnt[CPU_INDEX(i)] = 0;
 		slow_replenish_count[LAST_RCVD_INDEX(i)] = 0;
 		fast_replenish_count[LAST_RCVD_INDEX(i)] = 0;
 		err_replenish_count[LAST_RCVD_INDEX(i)] = 0;
+		receive_count[LAST_RCVD_INDEX(i)] = 0;
 	}
 	/*check how many hash are empty...*/
-	printk("TOTAL_FAST_REPL %ld, TOTAL_SLOW_REPL %ld, TOTAL_ERR_REPL %ld\n",(unsigned long)total_fast, (unsigned long)total_slow, (unsigned long)total_err);
+	printk("TOTAL_FAST_REPL %ld, TOTAL_SLOW_REPL %ld, TOTAL_ERR_REPL %ld TOTAL_RECV %ld\n",
+			(unsigned long)total_fast,
+			(unsigned long)total_slow,
+			(unsigned long)total_err,
+			(unsigned long)total_recv);
 
 	*eof = 1;
 	return len;
@@ -755,6 +780,17 @@ static inline void nlm_enable_msgring_intr(void)
 
 }
 
+#if 0
+static void dump_skbuff (struct sk_buff* skb)
+{
+	printk ("len %d\n", skb->len);
+	printk ("data_len %d\n", skb->data_len);
+	printk ("mac_len %d\n", skb->mac_len);
+	printk ("hdr_len %d\n", skb->hdr_len);
+	printk ("truesize %d\n", skb->truesize);
+	printk ("protocol %#x\n", skb->protocol);
+}
+#endif
 
 /*
  * NAE poll function on upper four buckets
@@ -767,7 +803,9 @@ xlp_poll_upper(int cpu)
 	uint32_t src_id, size, code, context, port, node;
 	struct sk_buff* skb;
 	struct dev_data *priv;
+#ifdef TSO_ENABLED
 	uint64_t *p2pfbdesc;
+#endif
 
 	while (1) {
 			status = xlp_message_receive_1(nae_fb_vc, &src_id, &size, &code, &msg0);
@@ -796,7 +834,7 @@ xlp_poll_upper(int cpu)
 			} else
 #endif
 				skb = (struct sk_buff *)bus_to_virt(addr);
-			
+
 			if(skb)
 			{
 				priv = netdev_priv(skb->dev);
@@ -833,7 +871,7 @@ static int xlp_poll_lower(int budget, int cpu)
 	struct sk_buff* skb;
 	uint32_t src_id, size, code;
 
-#ifdef CONFIG_INET_LRO		  
+#ifdef CONFIG_INET_LRO
 	int lro_flush_priv_cnt = 0, i;
 	char lro_flush_needed[20] = { 0 };
 	struct dev_data *lro_flush_priv[20];
@@ -841,9 +879,9 @@ static int xlp_poll_lower(int budget, int cpu)
 	while (budget--) {
 
 		status = xlp_message_receive_2(nae_rx_vc, &src_id, &size, &code, &msg0, &msg1);
-		
+
 		if(status) {
-			if(enable_napi) 
+			if(enable_napi)
 				break;
 			continue;
 		}
@@ -864,7 +902,9 @@ static int xlp_poll_lower(int budget, int cpu)
 		context = (msg1 >> 54) & 0x3ff;
 
 		if (err) {
-			mac_refill_frin_skb(addr, frin_queue_base + (cpu/2));
+			vaddr = (uint64_t)bus_to_virt(addr);
+			skb = mac_get_skb_back_ptr(vaddr);
+			mac_refill_frin_skb(addr, skb->queue_id);
 			STATS_INC(priv->stats.rx_errors);
 			STATS_INC(priv->stats.rx_dropped);
 			err_replenish_count[LAST_RCVD_INDEX(cpu)]++;
@@ -900,6 +940,7 @@ static int xlp_poll_lower(int budget, int cpu)
 		len = len  - MAC_CRC_LEN;
 
 		skb = mac_get_skb_back_ptr(vaddr);
+		//printk ("cpu %d Freein_fifo %d len %d skb->len %d \n", cpu, skb->queue_id, len, skb->len);
 #ifdef ENABLE_SANITY_CHECKS
 		if (!skb) {
 			STATS_INC(priv->stats.rx_dropped);
@@ -916,14 +957,16 @@ static int xlp_poll_lower(int budget, int cpu)
 		skb->protocol = eth_type_trans(skb, pdev);
 		//skb->dev->last_rx = jiffies;
 
+		//dump_skbuff (skb);
+		//printk ("skb->len%d \n", skb->len);
 		/* Pass the packet to Network stack */
 		last_rcvd_skb[LAST_RCVD_INDEX(cpu)] = skb;
 		last_rcvd_skb_phys[LAST_RCVD_INDEX(cpu)] = addr;
 
 #ifdef CONFIG_INET_LRO
-		if((skb->dev->features & NETIF_F_LRO) && 
-				(msg1 & RX_IP_CSUM_VALID) && (msg1 & RX_TCP_CSUM_VALID)) {		
-			 
+		if((skb->dev->features & NETIF_F_LRO) &&
+				(msg1 & RX_IP_CSUM_VALID) && (msg1 & RX_TCP_CSUM_VALID)) {
+
 			skb->ip_summed = CHECKSUM_UNNECESSARY;
 			lro_receive_skb(&priv->lro_mgr[cpu], skb, (void *)msg1);
 			if(!lro_flush_needed[port]) {
@@ -941,11 +984,15 @@ static int xlp_poll_lower(int budget, int cpu)
 		STATS_ADD(priv->stats.rx_bytes, len);
 		STATS_INC(priv->stats.rx_packets);
 		//priv->cpu_stats[cpu].rx_packets++;
+		receive_count[LAST_RCVD_INDEX(cpu)]++;
 
 		if (last_rcvd_skb[LAST_RCVD_INDEX(cpu)]) {
 		  //printk("[%s@%d]: Unwanted buffer allocation in driver data path!\n", __FILE__, __LINE__);
 			slow_replenish_count[LAST_RCVD_INDEX(cpu)]++;
-			mac_refill_frin_one_buffer(pdev, cpu);
+			if (!enable_jumbo || ((len + MAC_CRC_LEN) <= NLM_RX_BUF_SIZE))
+				mac_refill_frin_one_buffer(pdev, cpu, NLM_RX_BUF_SIZE);
+			else
+				mac_refill_frin_one_buffer(pdev, cpu, NLM_RX_JUMBO_BUF_SIZE);
 			last_rcvd_skb[LAST_RCVD_INDEX(cpu)] = NULL;
 		}
 	}
@@ -954,8 +1001,6 @@ static int xlp_poll_lower(int budget, int cpu)
 	for(i = 0; i < lro_flush_priv_cnt; i++)
                 lro_flush_all(&lro_flush_priv[i]->lro_mgr[cpu]);
 #endif
-
-
 	return no_rx_pkt_rcvd;
 }
 
@@ -1034,7 +1079,7 @@ void nlm_spawn_kthread(void)
 
 static void nlm_update_ucore_shared_memory(int node)
 {
-	uint32_t data[33] = {0};
+	uint32_t data[34] = {0};
 	int i = 0;
 	int j = 0;
 
@@ -1050,32 +1095,41 @@ static void nlm_update_ucore_shared_memory(int node)
 		data[32] = NLM_TCP_MODE;
 	else
 		data[32] = NLM_RT_MODE;
+
+	data[33] = enable_jumbo;
 	nlm_hal_write_ucore_shared_mem(data, sizeof(data)/sizeof(uint32_t));
 }
 
-
 static void nlm_replenish_per_cpu_buffer(int qindex, int bufcnt)
 {
 	int i;
 	int vc_index = 0;
-	int mflags, code;
+	int __attribute__ ((unused)) mflags, code;
 	struct xlp_msg msg;
 	struct sk_buff * skb;
 	int ret = 0;
+	int size = 0;
+
+	if (!enable_jumbo || qindex < 8 || qindex > 15)
+		size = NLM_RX_BUF_SIZE;
+	else
+		size = NLM_RX_JUMBO_BUF_SIZE;
 
 	for(i = 0; i < bufcnt; i++)
 	{
-		vc_index =  qindex + frin_queue_base;
-		skb = nlm_xlp_alloc_skb_atomic();
+		vc_index = qindex + frin_queue_base;
+		skb = nlm_xlp_alloc_skb_atomic(size);
 		if(!skb)
 		{
 			printk("[%s] alloc skb failed\n",__FUNCTION__);
 			break;
 		}
-		/* Send the free Rx desc to the MAC */
+		skb->queue_id = vc_index;
+		/* Store skb in back_ptr */
 		mac_put_skb_back_ptr(skb);
 		code = 0;
 
+		/* Send the free Rx desc to the MAC */
 		msgrng_access_enable(mflags);
 		msg.entry[0] = (unsigned long long)virt_to_bus(skb->data) & 0xffffffffffULL;
 		msg.entry[1]= msg.entry[2] = msg.entry[3] = 0;
@@ -1095,7 +1149,7 @@ static void nlm_replenish_per_cpu_buffer(int qindex, int bufcnt)
 		}
 		msgrng_access_disable(mflags);
 	}
-	printk("Send %d descriptors for queue %d(vc %d)\n", bufcnt, qindex, vc_index);
+	printk("Send %d descriptors for queue %d(vc %d) of length %d\n", bufcnt, qindex, vc_index, size);
 }
 
 /*
@@ -1113,9 +1167,8 @@ DEFINE_PER_CPU(unsigned long long, xlp_napi_rx_count);
 static void napi_msgint_handler(int vc)
 {
 	struct napi_struct *napi;
-	int cpu = hard_smp_processor_id();
 
-	napi_dbg("%s in vc %d cpu %d\n", __FUNCTION__, vc, cpu);
+	napi_dbg("%s in vc %d cpu %d\n", __FUNCTION__, vc, hard_smp_processor_id());
 
 	napi = &__get_cpu_var(xlp_napi_poll_struct);
 	napi_schedule(napi);
@@ -1152,14 +1205,12 @@ static int nlm_xlp_napi_setup(void)
 	return 0;
 }
 
-
-
 static int nlm_initialize_vfbid(void)
 {
 	int cpu =0, tblidx, i = 0;
 	uint32_t vfbid_tbl[128];
 
-	/* freeback bucket vc based on hard cpu id */	
+	/* freeback bucket vc based on hard cpu id */
 	for (tblidx = 0; tblidx < 32 ; tblidx++, cpu++) {
 		vfbid_tbl[tblidx] = (cpu * 4) + nae_fb_vc;
 	}
@@ -1169,12 +1220,10 @@ static int nlm_initialize_vfbid(void)
 	}
 	/* NULL FBID Should map to cpu0 to detect NAE send message errors*/
 	vfbid_tbl[127] = 0;
-	nlm_config_vfbid_table(0, 0, 128, vfbid_tbl);    
+	nlm_config_vfbid_table(0, 0, 128, vfbid_tbl);
+	return 0;
 }
 
-
-
-
 /**********************************************************************
  * nlm_xlp_nae_init -  xlp_nae device driver init function
  * @dev  -  this is per device based function
@@ -1198,8 +1247,8 @@ static void nlm_xlp_nae_init(void)
 	}
 
 	printk("======= Module Parameters =========\n");
-	printk("num_descs_perq=%d perf_mode=%s enable_napi=%d enable_lro=%d\n",
-	       num_descs_perq, mode_str[perf_mode], enable_napi, enable_lro);
+	printk("num_descs_perq=%d perf_mode=%s enable_napi=%d enable_lro=%d enable_jumbo=%d\n",
+	       num_descs_perq, mode_str[perf_mode], enable_napi, enable_lro, enable_jumbo);
 
 	/* msgring intr may not work in [8421]xxAx parts, disabling the napi */
 	if(is_nlm_xlp8xx_ax() && enable_napi) {
@@ -1231,20 +1280,23 @@ static void nlm_xlp_nae_init(void)
 	if (nae_cfg == NULL) {
 		printk("Node %d NAE configuration is NULL \n",node);
 		return;
-	}	
+	}
 	nae_fb_vc = nae_cfg->fb_vc;
 	nae_rx_vc = nae_cfg->rx_vc;
 
-
 	if(nae_cfg->frin_queue_base != 0)
 		frin_queue_base = nae_cfg->frin_queue_base;
-
 	if(nae_cfg->frin_total_queue != 0)
 		frin_total_queue = nae_cfg->frin_total_queue;
 
+	/* Update RX_CONFIG for desc size */
+	if(enable_jumbo)
+		nlm_hal_init_ingress (node, DEFAULT_JUMBO_MTU);
+
 	printk("nae_cfg frin_queue_base %d, frin_total_queue %d\n",nae_cfg->frin_queue_base, nae_cfg->frin_total_queue);
 
 	nlm_initialize_vfbid();
+
 	for(i = 0; i < nae_cfg->num_ports; i++)
 	{
 		/* Register only valid ports which are management */
@@ -1271,16 +1323,17 @@ static void nlm_xlp_nae_init(void)
 		priv->hw_port_id = nae_cfg->ports[i].hw_port_id;
 
 		priv->inited	= 0;
-		priv->node = node;
-		priv->block 	= nae_cfg->ports[i].hw_port_id / 4;
-		priv->type = nae_cfg->ports[i].iftype;
+		priv->node 	= node;
+		priv->block	= nae_cfg->ports[i].hw_port_id / 4;
+		priv->type	= nae_cfg->ports[i].iftype;
+
 		switch(nae_cfg->ports[i].iftype) {
 			case SGMII_IF:
 				priv->index = nae_cfg->ports[i].hw_port_id & 0x3;
 				priv->phy.addr = nae_cfg->ports[i].hw_port_id;
 				break;
 			case XAUI_IF:
-				nlm_hal_write_mac_reg(priv->node, (nae_cfg->ports[i].hw_port_id / 4), 
+				nlm_hal_write_mac_reg(priv->node, (nae_cfg->ports[i].hw_port_id / 4),
 						XGMAC, XAUI_MAX_FRAME_LEN , 0x01800600);
 				priv->index = XGMAC;
 				break;
@@ -1293,9 +1346,9 @@ static void nlm_xlp_nae_init(void)
 		}
 		//nlm_print("port%d hw %d block %d index %d type %d \n",i, nae_cfg->ports[i].hw_port_id,
 		//							priv->block, priv->index, priv->type);
-		priv->nae_tx_qid 	= nae_cfg->ports[i].txq;
+		priv->nae_tx_qid	= nae_cfg->ports[i].txq;
 		priv->nae_rx_qid 	= nae_cfg->ports[i].rxq;
-		dev->features |= NETIF_F_LLTX;
+		dev->features 		|= NETIF_F_LLTX;
 
 		register_netdev(dev);
 
@@ -1334,21 +1387,21 @@ static void nlm_xlp_nae_init(void)
 		num_descs = num_descs_perq <= max_descs_pqueue ? num_descs_perq : max_descs_pqueue;
 
 		for(i = 0; i < frin_total_queue; i++) {
-			if(i < 16) 
+			if(i < 16)
 				nlm_replenish_per_cpu_buffer(i, num_descs);
 		}
 		/* configure the mgmt port, for mgmt ports take it from the port config */
 		for(i = 0; i < nae_cfg->num_ports; i++) {
 			if(!nae_cfg->ports[i].mgmt)
 				continue;
-			nlm_replenish_per_cpu_buffer(nae_cfg->ports[i].rxq - frin_queue_base, 
+			nlm_replenish_per_cpu_buffer(nae_cfg->ports[i].rxq - frin_queue_base,
 					nae_cfg->ports[i].num_free_desc);
 		}
 
 	} else {
 		for(i = 0; i < nae_cfg->num_ports; i++)
 		{
-			nlm_replenish_per_cpu_buffer(nae_cfg->ports[i].rxq - frin_queue_base, 
+			nlm_replenish_per_cpu_buffer(nae_cfg->ports[i].rxq - frin_queue_base,
 					nae_cfg->ports[i].num_free_desc);
 		}
 	}
@@ -1425,7 +1478,6 @@ static int  nlm_xlp_nae_open (struct net_device *dev)
 	priv->inited = 1;
 	nlm_xlp_mac_set_enable(priv, 1);
 
- out:
 	return ret;
 }
 
@@ -1525,7 +1577,7 @@ static inline int tso_xmit_skb(struct sk_buff *skb, struct net_device *dev)
 				pskb_expand_head(skb, 0, 0, GFP_ATOMIC)) {
 			goto out_unlock;
 		}
-		
+
 		iph = ip_hdr(skb);
 		iphdroff = (char *)iph - (char *)skb->data;
 		tcphdroff = iphdroff + ip_hdrlen(skb);
@@ -1645,23 +1697,24 @@ static int nlm_xlp_nae_start_xmit(struct sk_buff *skb, struct net_device *dev)
 #endif
 
 #ifdef CONFIG_NLM_NET_OPTS
-	if(skb->netl_skb && (last_rcvd_skb[LAST_RCVD_INDEX(cpu)] == skb->netl_skb) && !skb_shared(skb)) 
+	if(skb->netl_skb && (last_rcvd_skb[LAST_RCVD_INDEX(cpu)] == skb->netl_skb) && !skb_shared(skb))
 #else
-	if((last_rcvd_skb[LAST_RCVD_INDEX(cpu)] == skb) && !skb_shared(skb)) 
+	if((last_rcvd_skb[LAST_RCVD_INDEX(cpu)] == skb) && !skb_shared(skb))
 #endif
 	{
 		last_rcvd_skb[LAST_RCVD_INDEX(cpu)] = NULL;
-		/* Do h/w replenishment */
-		msg0 = nae_tx_desc(P2D_NEOP, 0, (cpu/2) + 32,
-					   0, last_rcvd_skb_phys[LAST_RCVD_INDEX(cpu)]);
+		/* Do h/w replenishment and 4 CPUs share a FIFO */
+		msg0 = nae_tx_desc(P2D_NEOP, 0, skb->queue_id - frin_queue_base + 32,
+				0, last_rcvd_skb_phys[LAST_RCVD_INDEX(cpu)]);
 		hw_repl = 1;
-		
-		//fast_replenish_count[LAST_RCVD_INDEX(cpu)]++;
+		//printk ("%s skb->len %d \n", __func__, skb->len);
+
+		fast_replenish_count[LAST_RCVD_INDEX(cpu)]++;
 	}
 	else {
 		msg0 = nae_tx_desc(P2D_NEOP, 0, cpu, 0, virt_to_bus(skb));
 	}
-       msg1 = nae_tx_desc(P2D_EOP, 0, NULL_VFBID, skb->len, 
+	msg1 = nae_tx_desc(P2D_EOP, 0, NULL_VFBID, skb->len,
 		       virt_to_bus(skb->data));
 	if(hw_repl) {
 		/* reset the skb for next rx */
@@ -1758,19 +1811,42 @@ static int  nlm_xlp_nae_ioctl (struct net_device *dev, struct ifreq *rq, int cmd
 
 /**********************************************************************
  * nlm_xlp_nae_change_mtu
- *
+ * @dev   -  this is per device based function
+ * @new_mtu -  this is new mtu to be set for the device
  **********************************************************************/
 static int nlm_xlp_nae_change_mtu(struct net_device *dev, int new_mtu)
 {
 	struct dev_data *priv = netdev_priv(dev);
 	unsigned long flags;
+	unsigned long local_jumbo_mtu;
+
+	if (enable_jumbo && ((new_mtu > (DEFAULT_JUMBO_MTU * MAX_SKB_FRAGS)) || (new_mtu < MIN_ETH_FRAME_SIZE))) {
+		return -EINVAL;
+	}
+
+	if (!enable_jumbo && (new_mtu > MAC_MAX_FRAME_SIZE || new_mtu < MIN_ETH_FRAME_SIZE)) {
+		return -EINVAL;
+	}
 
-	if ((new_mtu > ETH_MTU_SIZE) || (new_mtu < MIN_ETH_FRAME_SIZE)) {
+	if(priv->type==INTERLAKEN_IF){
 		return -EINVAL;
 	}
+	printk("Setting mtu %d bytes \n", new_mtu);
 
 	spin_lock_irqsave(&priv->lock, flags);
 
+	local_jumbo_mtu = new_mtu + ETH_HLEN + ETH_FCS_LEN;
+	local_jumbo_mtu = (local_jumbo_mtu + SMP_CACHE_BYTES) & ~(SMP_CACHE_BYTES - 1);
+
+	if(priv->type==SGMII_IF){
+		nlm_hal_set_sgmii_framesize(priv->node, priv->block, priv->index, local_jumbo_mtu);
+		printk ("New MTU for MAC (SGMII) is %ld bytes\n", local_jumbo_mtu);
+	}
+	if(priv->type==XAUI_IF){
+		nlm_hal_set_xaui_framesize(priv->node, priv->block, local_jumbo_mtu, local_jumbo_mtu);
+		printk ("New MTU for MAC (XAUI) is %ld bytes\n", local_jumbo_mtu);
+	}
+
 	dev->mtu = new_mtu;
 
 	if (netif_running(dev)) {
diff --git a/drivers/misc/netlogic/nae-perf/xlp_nae.h b/drivers/misc/netlogic/nae-perf/xlp_nae.h
index 75df364..01e3d8e 100644
--- a/drivers/misc/netlogic/nae-perf/xlp_nae.h
+++ b/drivers/misc/netlogic/nae-perf/xlp_nae.h
@@ -1,24 +1,29 @@
 #ifndef _XLP_NAE_H
 #define _XLP_NAE_H
 
-#define MAC_MAX_FRAME_SIZE      1600
-#define MAC_SKB_BACK_PTR_SIZE   SMP_CACHE_BYTES
-
+#define MIN_ETH_FRAME_SIZE	64
+#define MAC_MAX_FRAME_SIZE	1600
+#define DEFAULT_JUMBO_MTU	16384
+#define MAC_SKB_BACK_PTR_SIZE	SMP_CACHE_BYTES
 
 #define MAC_PREPAD		0
-#define BYTE_OFFSET             2
-#define NLM_RX_BUF_SIZE (MAC_MAX_FRAME_SIZE+BYTE_OFFSET+MAC_PREPAD+MAC_SKB_BACK_PTR_SIZE+SMP_CACHE_BYTES)
-#define MAC_CRC_LEN             4
-#define CACHELINE_SIZE          (1ULL << 6)
-#define CACHELINE_ALIGNED(addr) ( ((addr) + (CACHELINE_SIZE-1)) & ~(CACHELINE_SIZE-1) )
-#define PHYS_TO_VIRT(paddr) (uint64_t)((paddr) - (netlib_paddrb) + (netlib_vaddrb))
-#define VIRT_TO_PHYS(vaddr) (uint64_t)((vaddr) - (netlib_vaddrb) + (netlib_paddrb))
-extern  unsigned long long netlib_vaddrb;
+#define BYTE_OFFSET		2
+#define NLM_RX_BUF_SIZE		(MAC_MAX_FRAME_SIZE+BYTE_OFFSET+MAC_PREPAD+MAC_SKB_BACK_PTR_SIZE+SMP_CACHE_BYTES)
+#define NLM_RX_JUMBO_BUF_SIZE	(DEFAULT_JUMBO_MTU+BYTE_OFFSET+MAC_PREPAD+MAC_SKB_BACK_PTR_SIZE+SMP_CACHE_BYTES)
+#define MAC_HDR_LEN		14
+#define MAC_CRC_LEN		4
+#define CACHELINE_SIZE		(1ULL << 6)
+#define CACHELINE_ALIGNED(addr)	( ((addr) + (CACHELINE_SIZE-1)) & ~(CACHELINE_SIZE-1) )
+
+extern unsigned long long netlib_vaddrb;
 extern unsigned long long netlib_paddrb;
-#define PADDR_BASE 0x100000ULL
-#define PADDR_SIZE 0x200000
 #define INIT_VBASE( vbase, pbase) {netlib_vaddrb = vbase ; netlib_paddrb = pbase;}
-#define LRO_MAX_DESCS 8
+#define PHYS_TO_VIRT(paddr) 	(uint64_t)((paddr) - (netlib_paddrb) + (netlib_vaddrb))
+#define VIRT_TO_PHYS(vaddr) 	(uint64_t)((vaddr) - (netlib_vaddrb) + (netlib_paddrb))
+
+#define PADDR_BASE		0x100000ULL
+#define PADDR_SIZE		0x200000
+#define LRO_MAX_DESCS		8
 
 struct cpu_stat {
         unsigned long tx_packets;
@@ -27,7 +32,6 @@ struct cpu_stat {
         unsigned long interrupts;
 };
 
-
 typedef enum xlp_net_types { TYPE_XLP_GMAC = 0, TYPE_XLP_XGMAC, TYPE_XLP_XAUI, TYPE_XLP_INTERLAKEN, MAX_XLP_NET_TYPES }xlp_interface_t;
 
 typedef enum { xlp_mac_speed_10, xlp_mac_speed_100,
-- 
1.9.1

