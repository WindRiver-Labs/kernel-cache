From 406f56c85a288be99de31a265a6a0f73051553a4 Mon Sep 17 00:00:00 2001
From: Om Narasimhan <onarasimhan@netlogicmicro.com>
Date: Wed, 31 Aug 2011 14:56:48 -0700
Subject: [PATCH 1078/1532] This commit provides pcie-offload host and device
 (char and net) drivers.

Requires some kernel support (dma driver in device mode).

Fix for slow ping response
Fixed ping taking more than a second for response. Setup scripts (for both
device and host, char and net drivers) and locations are in the top directory.
Readme.txt is modified to reflect these changes. Fix for crash during rmmod
(especially when a transaction is under progress). Removed compilation time
prints. Increased number of descriptors.
[Based on SDK 3.2]
Signed-off-by: Quanyang Wang <quanyang.wang@windriver.com>
---
 drivers/misc/netlogic/pcie-offload/README.txt      |  89 ++
 .../misc/netlogic/pcie-offload/char/device/Kbuild  |   5 +
 .../netlogic/pcie-offload/char/device/Makefile     |  33 +
 .../pcie-offload/char/device/netl_pti_dev.c        | 594 +++++++++++++
 .../misc/netlogic/pcie-offload/char/host/Makefile  |  37 +
 .../pcie-offload/char/host/netl_pti_host.c         | 937 +++++++++++++++++++++
 .../netlogic/pcie-offload/include/chunking-api.h   |  45 +
 .../netlogic/pcie-offload/include/chunking-lib.c   | 508 +++++++++++
 drivers/misc/netlogic/pcie-offload/include/mgmt.h  | 234 +++++
 .../misc/netlogic/pcie-offload/include/netl_pti.h  | 331 ++++++++
 .../netlogic/pcie-offload/include/netl_pti_char.c  | 489 +++++++++++
 .../netlogic/pcie-offload/include/netl_pti_char.h  |  24 +
 .../pcie-offload/include/netl_pti_common.c         | 716 ++++++++++++++++
 .../netlogic/pcie-offload/include/netl_pti_dev.h   |  90 ++
 .../netlogic/pcie-offload/include/netl_pti_test.c  | 167 ++++
 .../misc/netlogic/pcie-offload/include/nlm_pcie.h  |  11 +
 .../netlogic/pcie-offload/include/nlm_pcieip.h     | 199 +++++
 drivers/misc/netlogic/pcie-offload/include/perf.h  |  36 +
 .../misc/netlogic/pcie-offload/include/redirect.c  |  55 ++
 .../misc/netlogic/pcie-offload/net/device/Kbuild   |   5 +
 .../misc/netlogic/pcie-offload/net/device/Makefile |  35 +
 .../pcie-offload/net/device/pcieip_common.c        | 277 ++++++
 .../netlogic/pcie-offload/net/device/pcieip_dev.c  | 273 ++++++
 .../misc/netlogic/pcie-offload/net/host/Makefile   |  35 +
 .../netlogic/pcie-offload/net/host/pcieip_host.c   | 531 ++++++++++++
 drivers/misc/netlogic/pcie-offload/setup_dev.sh    |  57 ++
 drivers/misc/netlogic/pcie-offload/setup_host.sh   |  59 ++
 27 files changed, 5872 insertions(+)
 create mode 100644 drivers/misc/netlogic/pcie-offload/README.txt
 create mode 100644 drivers/misc/netlogic/pcie-offload/char/device/Kbuild
 create mode 100644 drivers/misc/netlogic/pcie-offload/char/device/Makefile
 create mode 100644 drivers/misc/netlogic/pcie-offload/char/device/netl_pti_dev.c
 create mode 100644 drivers/misc/netlogic/pcie-offload/char/host/Makefile
 create mode 100644 drivers/misc/netlogic/pcie-offload/char/host/netl_pti_host.c
 create mode 100644 drivers/misc/netlogic/pcie-offload/include/chunking-api.h
 create mode 100644 drivers/misc/netlogic/pcie-offload/include/chunking-lib.c
 create mode 100644 drivers/misc/netlogic/pcie-offload/include/mgmt.h
 create mode 100644 drivers/misc/netlogic/pcie-offload/include/netl_pti.h
 create mode 100644 drivers/misc/netlogic/pcie-offload/include/netl_pti_char.c
 create mode 100644 drivers/misc/netlogic/pcie-offload/include/netl_pti_char.h
 create mode 100644 drivers/misc/netlogic/pcie-offload/include/netl_pti_common.c
 create mode 100644 drivers/misc/netlogic/pcie-offload/include/netl_pti_dev.h
 create mode 100644 drivers/misc/netlogic/pcie-offload/include/netl_pti_test.c
 create mode 100644 drivers/misc/netlogic/pcie-offload/include/nlm_pcie.h
 create mode 100644 drivers/misc/netlogic/pcie-offload/include/nlm_pcieip.h
 create mode 100644 drivers/misc/netlogic/pcie-offload/include/perf.h
 create mode 100644 drivers/misc/netlogic/pcie-offload/include/redirect.c
 create mode 100644 drivers/misc/netlogic/pcie-offload/net/device/Kbuild
 create mode 100644 drivers/misc/netlogic/pcie-offload/net/device/Makefile
 create mode 100644 drivers/misc/netlogic/pcie-offload/net/device/pcieip_common.c
 create mode 100644 drivers/misc/netlogic/pcie-offload/net/device/pcieip_dev.c
 create mode 100644 drivers/misc/netlogic/pcie-offload/net/host/Makefile
 create mode 100644 drivers/misc/netlogic/pcie-offload/net/host/pcieip_host.c
 create mode 100755 drivers/misc/netlogic/pcie-offload/setup_dev.sh
 create mode 100755 drivers/misc/netlogic/pcie-offload/setup_host.sh

diff --git a/drivers/misc/netlogic/pcie-offload/README.txt b/drivers/misc/netlogic/pcie-offload/README.txt
new file mode 100644
index 0000000..45d189b
--- /dev/null
+++ b/drivers/misc/netlogic/pcie-offload/README.txt
@@ -0,0 +1,89 @@
+Location of this directory should be
+<TOP-OF-SDK>/linux-userspace/kmod/pcie-offload
+
+For Device deployment:
+=====================
+1. Source @<TOP-OF-SDK>/sdk-base/build.sh to configure cross compilation
+environment.
+2. Compile linux kernel for the device:
+   a. Run 'make menuconfig'.
+   b. Enable Machine selection  --->
+      	   [*] Enable XLP device mode
+   c. Device Drivers  --->
+      	     Character devices  --->
+   	     	       <*> Enable netlogic XLP DMA driver
+   d. make vmlinux
+3. Compile Driver modules.
+$ cd pcie-offload
+$ ./setup_dev.sh
+
+   
+IMPORTANT NOTE :
+This script will compile the modules and installs the modules in the root file
+system. To use them, you might have to re-build linux kernel image again.
+
+For Host module compilation:
+===========================
+1. It is recommended that one should use another terminal console (one where
+build.sh is NOT sourced)
+$ cd pcie-offload
+$ ./setup_host.sh <path to host kernel sources>
+  e.g. on a typical Linux PC,
+   ./setup_host.sh /lib/modules/`uname -r`/build
+
+This command would compile character and network interface host drivers in
+respective char/host and net/host directories. One may optionally copy these
+kernel modules to another location.
+
+For the network over PCIe interface:
+===================================
+1. Use the vmlinux to boot the device
+2. In the booted pcie device, run
+   $ modprobe pcieipdev
+3. in the host (as root or using sudo)
+   $ insmod ./net/host/pcieip.ko
+4. On the pcie device
+   $ ifconfig eth0 <IP addr> <netmask>	...etc
+4. On the host
+   $ ifconfig ethX <IP addr> <netmas> ...etc
+   # You need to find the ethX using $ifconfig -a
+
+You can do ftp, tftp ..etc between the host and device interfaces now.
+
+
+For the character device interface :
+===================================
+Once card boots up
+#modprobe netl_pti.ko
+
+There are guidelines till an all automated version of this program is completed.
+
+1. The device has to be fully up before host driver portion is insmod-ed.
+   Once you execute the above 'modprobe' command, you would see some messages
+   appearing on the device console like,
+
+    0:<7>parse_hs_from_host()@/space/bringup-master/sdk-base/linux-userspace/pci-offload/char/device/netl_pti_dev.c:449 No host signature yet
+
+   You need to wait till these messages repeatedly printed out before insmod-ing
+   netl_pti.ko on the host side.
+
+2. Once the handshake is complete, you would see something like,
+   "Ctrl Subsys init done. DEV ready for TXRX" printed on the console.
+
+3. You need to figure out the minor and major numbers from /proc/devices before
+you create a node. Execute
+ $ cat /proc/devices | grep netl_pti
+Use the major number printed like,
+ #mknod /dev/netl_char1 c <MAJOR> 1 # 1 is minor number
+
+4. Userspace components need be initialized before dd can work. For that,
+ $ dd if=/dev/zero of=/dev/netl_char1 bs=4k count=0
+
+5. Repeat the above command on host side as well.
+
+*NOTE*
+=====
+It is important that you execute the above command in the host first.
+Otherwise, device would never see anything from host.
+
+For testing the device,  you may use dd command on the device and host.
diff --git a/drivers/misc/netlogic/pcie-offload/char/device/Kbuild b/drivers/misc/netlogic/pcie-offload/char/device/Kbuild
new file mode 100644
index 0000000..9a6a85b
--- /dev/null
+++ b/drivers/misc/netlogic/pcie-offload/char/device/Kbuild
@@ -0,0 +1,5 @@
+EXTRA_CFLAGS := -DNLM_HAL_LINUX_KERNEL	-I$(src)/../../include -DNETL_PTI_DEVICE -DNETL_ERR_DEBUG
+EXTRA_CFLAGS += -DNETL_PTI_DEBUG -g
+
+obj-m += netl_pti.o
+netl_pti-y := netl_pti_dev.o netl_pti_common.o netl_pti_char.o
diff --git a/drivers/misc/netlogic/pcie-offload/char/device/Makefile b/drivers/misc/netlogic/pcie-offload/char/device/Makefile
new file mode 100644
index 0000000..fe15940
--- /dev/null
+++ b/drivers/misc/netlogic/pcie-offload/char/device/Makefile
@@ -0,0 +1,33 @@
+# Just the make file.
+# Moved all module specific build instructions to Kbuild
+#
+PWD = $(shell pwd -L)
+KERNELDIR := $(shell readlink -fn $(PWD)/../../../../../linux/)
+ROOTFS_DIR = $(shell readlink -fn $(KERNELDIR)/usr/rootfs.xlp/)
+TAGS = /usr/bin/ctags
+CSCOPE = /usr/bin/cscope
+all: modules
+
+modules:
+	$(MAKE) -C $(KERNELDIR) M=$(PWD) $@
+
+modules_install: modules
+	make -C $(KERNELDIR) M=$(PWD) modules_install INSTALL_MOD_PATH=$(ROOTFS_DIR)
+
+help:
+	make -C $(KERNELDIR) M=$(PWD) help
+
+cscope:
+	$(CSCOPE) $(CSCOPEFLAGS) -R
+
+tags:
+	$(TAGS) netl_pti_dev.c ../../include/{nlm_pcie.h,netl_pti_common.c,netl_pti_char.c,netl_pti.h,netl_pti_dev.h}
+
+clean:
+	make -C $(KERNELDIR) M=$(PWD) clean
+	rm -rf *.o *.mod.* *.ko *.o.p *.i
+distclean: clean
+	rm -rf cscope*
+	rm -rf tags
+
+.PHONY: help cscope tags clean distclean image install
diff --git a/drivers/misc/netlogic/pcie-offload/char/device/netl_pti_dev.c b/drivers/misc/netlogic/pcie-offload/char/device/netl_pti_dev.c
new file mode 100644
index 0000000..a9d7018
--- /dev/null
+++ b/drivers/misc/netlogic/pcie-offload/char/device/netl_pti_dev.c
@@ -0,0 +1,594 @@
+#include <netl_pti.h>
+
+static struct handshake_struct *dev_hs;
+extern int init_subsystems(struct handshake_struct *hs);
+extern void dealloc_all_subsys(struct handshake_struct *hs);
+extern void dump_hs(struct seq_file *seq, struct handshake_struct *hs);
+extern void dump_desc_ring(struct seq_file *seq, struct desc_ring *ring, int dump_all);
+extern void dump_descriptor(struct seq_file *seq, struct descriptor *ptr);
+extern int add_to_rxlist(struct pti_subsys *ss, volatile struct kvec *iov);
+extern int netl_register_chrdev(struct handshake_struct *hs);
+extern int init_handshake_struct(struct handshake_struct *hs);
+extern struct pti_subsys *init_one_subsys(struct handshake_struct *, int );
+void netl_unregister_chrdev(struct handshake_struct *hs);
+
+static u64 pti_mem_start;
+struct workqueue_struct *ss_workq;
+
+/* Function delcarations */
+static void hs_dev_func(struct work_struct *work);
+static int hs_get_cmdval(struct handshake_struct *, volatile u32 *);
+void hs_set_cmdreg(struct handshake_struct *hs);
+void subsys_poll(struct pti_subsys *);
+static void subsys_poll_rx(struct pti_subsys *);
+int send_one_packet_to_host(struct pti_subsys *ss, unsigned long page, u32 len);
+void copy_desc_pci(volatile struct _desc_pci *new, volatile struct _desc_pci *orig);
+irqreturn_t dev_int_handler(int irq, void *d);
+static void temp_int_func(struct work_struct *work);
+volatile void *get_start_of_mem(void);
+int __dev_def_rx_cmpl(struct pti_subsys *ss, struct descriptor *d, struct iovec *iov);
+void check_and_set_int_flags(struct pti_subsys *ss);
+
+/*
+ * Host worker function. Will take out after interrupt implementation
+ *
+ */
+static void temp_int_func(struct work_struct *work)
+{
+	struct handshake_struct *hs = container_of(work, struct handshake_struct, temp_int_work.work);
+	unsigned long flags;
+
+	spin_lock_irqsave(&hs->hs_lock, flags);
+	dev_int_handler(PTI_DEV_IRQ, hs);
+	spin_unlock_irqrestore(&hs->hs_lock, flags);
+	schedule_delayed_work(&hs->temp_int_work, 0);
+}
+
+/*
+ * Alternate function calls for temp_int_func
+ */
+void hs_task_func(unsigned long d)
+{
+	struct handshake_struct *hs = (struct handshake_struct *)d;
+	// unsigned long flags;
+
+	dev_int_handler(PTI_DEV_IRQ, hs);
+	tasklet_schedule(&hs->hs_stask);
+}
+
+/* Interrupt handler
+ *
+ * @irq		: irq number
+ * @d		: data supplied during registration
+ */
+irqreturn_t dev_int_handler(int irq, void *d)
+{
+	struct handshake_struct *hs = (struct handshake_struct *)d;
+	struct pti_subsys *ss;
+	struct hlist_node *node;
+	int count;
+
+	/* go through registered subsystems and start threads */
+	for(count = 0; count < NETL_SUBSYS_HASH; count++){
+		if(hlist_empty(&hs->hhash[count])){
+			continue;
+		}
+		hlist_for_each_entry(ss, node, &hs->hhash[count], snode){
+		/* Need a better way findng out subsystem with job pending */
+			check_and_set_int_flags(ss);
+			subsys_poll(ss);
+		}
+	}
+	return IRQ_HANDLED;
+}
+
+/*
+ * Async callback function to process data.
+ *
+ * @ptr	: descriptor id
+ */
+void dev_def_rx_cmpl(void *ptr, uint64_t ignored)
+{
+	struct pti_subsys *ss = NULL;
+	struct descriptor *d = (struct descriptor *)ptr;
+	unsigned long flags;
+	volatile struct kvec rx;
+	u32 tmp;
+
+	if(!ptr){
+		PDEBUG("NULL func ptr\n");
+		return;
+	}
+	ss = d->pring->ss;
+	//PDEBUG("ss->id = %#x, d->idx = %#x\n", ss->id, d->idx);
+	spin_lock_irqsave(&d->pring->rng_lock, flags);
+	pti_pci_write32(0, (void *)&d->pci_desc->dma_stat);
+	d->pring->end = INCR_CEIL(d->idx);
+	/* Why increment idx before writing to tail?
+	 * Host side assumes,
+	 * a. head=tail => empty ring
+	 * b. buffers with DMA complete is from ring->start inclusive to tail-1
+	 * */
+	pti_pci_write32(INCR_CEIL(d->idx), (void *)d->pring->tail);
+	mb();
+#ifdef NETL_ERR_DEBUG
+	if((tmp = pti_pci_read32((void *)&d->pci_desc->dma_stat)) != 0){
+		PERROR("W 0, R %#x\n", tmp);
+	}
+	if((tmp = pti_pci_read32((void *)d->pring->tail)) != INCR_CEIL(d->idx)){
+		PERROR("W %#x, R %#x\n", INCR_CEIL(d->idx), tmp);
+	}
+#endif
+	//PDEBUG("Updated STAT %#x, TAIL = %#x\n", pti_pci_read32((void *)&d->pci_desc->dma_stat), pti_pci_read32((void *)d->pring->tail));
+	/* Send data upstream here  TODO */
+	rx.iov_len = d->len;
+	ss->rxbytes += d->len;
+	rx.iov_base = (void *)d->buf;
+	spin_unlock_irqrestore(&d->pring->rng_lock, flags);
+	raise_host_interrupt(0);
+	add_to_rxlist(ss, &rx);
+	return;
+}
+
+/*
+ * Function to call when a tx complete interrupt is received
+ * When you have Tx int, that descriptor buffer should be free-ed.
+ */
+void dev_def_tx_cmpl(void *ptr, uint64_t ignored)
+{
+	struct descriptor *d = (struct descriptor *)ptr;
+	struct pti_subsys *ss;
+	unsigned long flags;
+	u32 tmp;
+
+	if(unlikely(ptr == NULL)){
+		PERROR("NULL pointer BUG\n");
+		BUG_ON(ptr == NULL);
+	}
+	ss = d->pring->ss;
+	spin_lock_irqsave(&d->pring->rng_lock, flags);
+	pti_pci_write32(d->len, (void *)&d->pci_desc->dma_len);
+	pti_pci_write32(0, (void *)&d->pci_desc->dma_stat);
+	mb();
+	pti_pci_write32(INCR_CEIL(d->idx), (void *)d->pring->head);
+	mb();
+#ifdef NETL_ERR_DEBUG
+	if((tmp = pti_pci_read32((void *)&d->pci_desc->dma_stat)) != 0){
+		PERROR("W 0, R %#x\n", tmp);
+	}
+	if((tmp = pti_pci_read32((void *)d->pring->head)) != INCR_CEIL(d->idx)){
+		PERROR("W %#x, R %#x\n", INCR_CEIL(d->idx), tmp);
+	}
+	if((tmp = pti_pci_read32((void *)&d->pci_desc->dma_len)) != d->len){
+		PERROR("W %#x, R %#x\n", d->len, tmp);
+	}
+#endif
+	//PDEBUG("Cleared dma_stat:%p, head = %#x\n", &d->pci_desc->dma_stat, d->idx);
+	//dump_desc_ring(NULL, d->pring, 0);
+	spin_unlock_irqrestore(&d->pring->rng_lock, flags);
+	raise_host_interrupt(0);
+}
+
+static void subsys_poll_tx(struct pti_subsys *ss)
+{
+	struct desc_ring *txr;
+	unsigned long fl_rng;
+	volatile u32 tail;
+	int loops;
+
+	txr = ss->txring;
+	spin_lock_irqsave(&txr->rng_lock, fl_rng);
+	tail = pti_pci_read32((void *)txr->tail);
+	loops = (ss->hs->params.nr_desc + tail - txr->end) % ss->hs->params.nr_desc;
+	// PDEBUG("tail = %#x, END = %#x\n", tail, txr->end);
+	while(loops--){
+		//PDEBUG("loops = %d, end = %#x\n", loops+1, txr->end);
+		free_page(txr->pdesc[txr->end].buf);
+		txr->pdesc[txr->end].buf = 0;
+		txr->pdesc[txr->end].len = 0;
+		txr->nfree++;
+		txr->end = INCR_CEIL(txr->end);
+	}
+	spin_unlock_irqrestore(&txr->rng_lock, fl_rng);
+
+	spin_lock_irqsave(&ss->slock, fl_rng);
+	ss->flags &= ~SUBSYS_FLAGS_TX;
+	ss->flags &= ~SUBSYS_FLAGS_TX_PENDING;
+	spin_unlock_irqrestore(&ss->slock, fl_rng);
+	wake_up_interruptible(&ss->wrq);
+	return;
+}
+
+/*
+ * Work queue for subsystem work threads
+ */
+/*
+ * Polling function
+ *
+ * @work	: ss->swork
+ */
+static void subsys_poll_rx(struct pti_subsys *ss)
+{
+	struct desc_ring *rxr;
+	unsigned long fl_rng;
+	volatile struct _desc_pci d_cp;
+	volatile u32 head, stmp = 0;
+	int loops;
+	rxr = ss->rxring;
+	spin_lock_irqsave(&rxr->rng_lock, fl_rng);
+	head = pti_pci_read32((void *)rxr->head);
+	loops = (ss->hs->params.nr_desc + head - rxr->start) % ss->hs->params.nr_desc;
+	spin_unlock_irqrestore(&rxr->rng_lock, fl_rng);
+	while(loops--){
+//		PDEBUG("loops = %d, head = %#x, start= %#x\n", loops+1, head, rxr->start);
+		spin_lock_irqsave(&rxr->rng_lock, fl_rng);
+		copy_desc_pci(&d_cp, rxr->pdesc[rxr->start].pci_desc);
+		if(!is_dma_set(d_cp.dma_stat)){
+			/* Nothing to do now. Should not happen*/
+			PERROR("Queue empty.rxr->start = %#x\n", rxr->start);
+			dump_desc_ring(NULL, rxr, 1);
+			spin_unlock_irqrestore(&rxr->rng_lock, fl_rng);
+			return;
+		}
+		/* setup DMA with the tail we have now */
+		//PDEBUG("d_cp.dma_len = %#x\n", d_cp.dma_len);
+		rxr->pdesc[rxr->start].buf = get_zeroed_page(GFP_ATOMIC);
+		rxr->pdesc[rxr->start].len = d_cp.dma_len;
+		rxr->pdesc[rxr->start].idx = rxr->start;
+		//PDEBUG("head = %#x, rxr->start = %#x, idx = %#x\n", head, rxr->start, rxr->pdesc[rxr->start].idx);
+		stmp = rxr->start;
+		rxr->start= INCR_CEIL(rxr->start);
+		spin_unlock_irqrestore(&rxr->rng_lock, fl_rng);
+		xlp_async_request_dma(((u64)d_cp.dma_addr),
+			virt_to_phys((void *)rxr->pdesc[stmp].buf),
+			d_cp.dma_len, dev_def_rx_cmpl,
+			(void *)&rxr->pdesc[stmp], DMA_TO_DEVICE);
+	}
+
+	spin_lock_irqsave(&ss->slock, fl_rng);
+	ss->flags &= ~SUBSYS_FLAGS_RX;
+	ss->flags &= ~SUBSYS_FLAGS_RX_PENDING;
+	spin_unlock_irqrestore(&ss->slock, fl_rng);
+//	PDEBUG("Exiting with head = %#x, start= %#x, end = %#x\n", head, rxr->start, rxr->end);
+	return;
+}
+
+/*
+ * Adds one packet to Tx queue of device.
+ * Tx queue is setup from the host side with physical addresses of buffers and
+ * lengths.
+ *
+ * @ss	: subsystem through which we send packet.
+ */
+int send_one_packet_to_host(struct pti_subsys *ss, unsigned long page, u32 len)
+{
+	struct desc_ring *txr;
+	unsigned long fl_rng;
+	volatile struct _desc_pci d_cp;
+	u32 tmp;
+	txr = ss->txring;
+	spin_lock_irqsave(&txr->rng_lock, fl_rng);
+	// PDEBUG("end = %#x, start = %#x\n", txr->end, txr->start);
+	if(INCR_CEIL(txr->start) == txr->end){	/* Queue full */
+		//PDEBUG("Queue full\n");
+		spin_unlock_irqrestore(&txr->rng_lock, fl_rng);
+		return EAGAIN;
+	}
+	copy_desc_pci(&d_cp, txr->pdesc[txr->start].pci_desc);
+	if(len > d_cp.dma_len){
+		PERROR("Req. len = %#x > dma_len %#x\n", len, d_cp.dma_len);
+		spin_unlock_irqrestore(&txr->rng_lock, fl_rng);
+		/* Allocated space on host is less than packet len. Abort */
+		return EINVAL;
+	}
+	if(!is_dma_set(d_cp.dma_stat)){
+		/* head to start-1 should be DMA-able */
+		PERROR("No DMA bit set@txring[%#x]: %#llx, %#x, %#x\n", txr->start, d_cp.dma_addr, d_cp.dma_len, d_cp.dma_stat);
+		dump_descriptor(NULL, &txr->pdesc[txr->start]);
+		dump_desc_ring(NULL, txr, 1);
+		spin_unlock_irqrestore(&txr->rng_lock, fl_rng);
+		return EAGAIN;
+	}
+	/* setup DMA now */
+	txr->pdesc[txr->start].buf = page;
+	txr->pdesc[txr->start].len = len;	/*len < PAGE_SIZE possible*/
+	/* update txr->head before starting async DMA */
+	tmp = txr->start;
+	txr->start = INCR_CEIL(txr->start);
+	txr->nfree--;
+	ss->txbytes += len;
+	spin_unlock_irqrestore(&txr->rng_lock, fl_rng);
+	if(xlp_async_request_dma(virt_to_phys((void *)page),
+		((u64)d_cp.dma_addr), len, dev_def_tx_cmpl,
+		(void *)&txr->pdesc[tmp], DMA_FROM_DEVICE) != 0){
+		PERROR("dma failure\n");
+		return EFAULT;
+	}
+	// PDEBUG("\n");
+	return 0;
+}
+
+EXPORT_SYMBOL(send_one_packet_to_host);
+void check_and_set_int_flags(struct pti_subsys *ss)
+{
+	unsigned long flags, flag1;
+	u32 tail, head;
+
+	spin_lock_irqsave(&ss->slock, flags);
+
+	/* Tx complete for sent data from device */
+	spin_lock_irqsave(&ss->txring->rng_lock, flag1);
+	tail = pti_pci_read32((void *)ss->txring->tail);
+	if(tail != ss->txring->end){
+		ss->flags |= SUBSYS_FLAGS_TX_PENDING;
+	}
+	spin_unlock_irqrestore(&ss->txring->rng_lock, flag1);
+
+	/* rx for sent data from host*/
+	spin_lock_irqsave(&ss->rxring->rng_lock, flag1);
+	head = pti_pci_read32((void *)ss->rxring->head);
+	if(head != ss->rxring->start){
+		ss->flags |= SUBSYS_FLAGS_RX_PENDING;
+	}
+	spin_unlock_irqrestore(&ss->rxring->rng_lock, flag1);
+
+	spin_unlock_irqrestore(&ss->slock, flags);
+}
+
+/*
+ * Polling function
+ *
+ * @work	: ss->swork
+ */
+void subsys_poll(struct pti_subsys *ss)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&ss->slock, flags);
+	if(ss->flags & SUBSYS_FLAGS_TX_DISABLED){
+		spin_unlock_irqrestore(&ss->slock, flags);
+		goto tx_done;
+	}
+	if(ss->flags & SUBSYS_FLAGS_TX_PENDING){
+		if(ss->flags & SUBSYS_FLAGS_TX){
+			/* currently running and pending: clear flag and reschedule self */
+			ss->flags &= ~SUBSYS_FLAGS_TX_PENDING;
+			spin_unlock_irqrestore(&ss->slock, flags);
+		}else{
+			/* Tx not running. Run */
+			ss->flags |= SUBSYS_FLAGS_TX;
+			spin_unlock_irqrestore(&ss->slock, flags);
+			subsys_poll_tx(ss);
+		}
+	}else{
+		spin_unlock_irqrestore(&ss->slock, flags);
+	}
+tx_done:
+	spin_lock_irqsave(&ss->slock, flags);
+	if(ss->flags & SUBSYS_FLAGS_RX_DISABLED){
+		spin_unlock_irqrestore(&ss->slock, flags);
+		goto rx_done;
+	}
+	if(ss->flags & SUBSYS_FLAGS_RX_PENDING){
+		if(ss->flags & SUBSYS_FLAGS_RX){
+			/* running and pending; clear flag and reschedule self */
+			ss->flags &= ~SUBSYS_FLAGS_RX_PENDING;
+			spin_unlock_irqrestore(&ss->slock, flags);
+		}else{
+			/* Tx not running and pending. Run */
+			ss->flags |= SUBSYS_FLAGS_RX;
+			spin_unlock_irqrestore(&ss->slock, flags);
+			subsys_poll_rx(ss);
+		}
+	}else{
+		spin_unlock_irqrestore(&ss->slock, flags);
+	}
+rx_done:
+	return;
+}
+
+/*
+ * Sets the cmd and stat pointers to pci mem space
+ *
+ * @hs : the handshake structure
+ */
+void hs_set_cmdreg(struct handshake_struct *hs)
+{
+	PDEBUG("hs->start = %p\n", hs->start);
+	hs->wcmd = (volatile u32 *)(hs->start + PTI_ADDR_OFFSET + DCMD0_OFFSET);
+	hs->rstat = (volatile u32 *)(hs->start + PTI_ADDR_OFFSET + DSTAT0_OFFSET);
+	hs->rcmd = (volatile u32 *)(hs->start + PTI_ADDR_OFFSET + HCMD0_OFFSET);
+	hs->wstat = (volatile u32 *)(hs->start + PTI_ADDR_OFFSET + HSTAT0_OFFSET);
+	return;
+}
+
+/*
+ * Helper function to read ONE cmd value, cmd0/stat0 or cmd1/stat1
+ * Need to call repeatedly till returns HS_PARAM_{FINISH,ERROR}
+ * @hs : handshake structure
+ */
+static int hs_get_cmdval(struct handshake_struct *hs, volatile u32 *pos)
+{
+	volatile u32 cval = 0;
+
+	//PDEBUG("Reading commands from host\n");
+	cval = pti_pci_read32((void *)pos);
+	/* clear what we read */
+	pti_pci_write32(0, (void *)pos);
+	hs->params.max_subsys = cval;
+
+	pos++;
+	cval = pti_pci_read32((void *)pos);
+	/* clear what we read */
+	pti_pci_write32(0, (void *)pos);
+	hs->params.flags = cval;
+
+	pos++;
+	cval = pti_pci_read32((void *)pos);
+	/* clear what we read */
+	pti_pci_write32(0, (void *)pos);
+	hs->params.nr_desc = cval;
+
+	return 0;
+}
+
+/*
+ * This function gets cmd/val pairs from host and fills hs structure
+ *
+ * @hs : handshake struct to fill
+ *
+ * returns : 0 if all is well
+ */
+u32 parse_hs_from_host(struct handshake_struct *hs)
+{
+	u32 ret = 0;
+	volatile u32 *pos = hs->rcmd + 1;
+
+	ret = pti_pci_read32((void *)hs->rcmd);
+	if(ret != NETL_HOST_SIGNATURE){
+		PDEBUG("No host signature yet\n");
+		return HS_STATUS_NOT_DONE;
+	}
+	pti_pci_write32(0, (void *)hs->rcmd);
+	hs_get_cmdval(hs, pos);
+	/* update device signature */
+	pti_pci_write32(NETL_DEV_SIGNATURE, (void *)(hs->rcmd));
+	return HS_STATUS_COMPLETE;
+}
+
+/*
+ * This function is the looping handshake function for handshake
+ *
+ * @work : handshake_struct -> hs_work
+ */
+static void hs_dev_func(struct work_struct *work)
+{
+	struct handshake_struct *hs = container_of(work, struct handshake_struct,
+			hs_work.work);
+	u32 ret = 0;
+
+	if(hs->status == HS_DEV_EXITING){
+		return;
+	}
+	ret = parse_hs_from_host(hs);
+	if(ret == HS_STATUS_COMPLETE){
+		netl_register_chrdev(hs);
+		dump_hs(NULL, hs);
+		if (init_one_subsys(hs, NETL_CONTROL_SUBSYS) == NULL) {
+			PERROR("FATAL : Ctrl Subsys init FAILED\n");
+			return;
+		}
+		hs->status = HS_DEV_READY_TXRX;
+		PDEBUG("Ctrl Subsys init done. DEV ready for TXRX\n");
+		/* No more self schedule. Just start tasklet */
+		tasklet_schedule(&hs->hs_stask);
+	}else{
+		//PDEBUG("(%#x). Rescheduling...\n", ret);
+		schedule_delayed_work(&hs->hs_work, HZ);
+	}
+	return;
+}
+
+int dev_specific_hs_init(struct handshake_struct *hs)
+{
+	hs_set_cmdreg(hs);
+	hs->status = HS_STATUS_NOT_DONE;
+	hs->params.nr_desc = 0;
+	hs->params.max_subsys = 0;
+	hs->params.flags = 0;
+	INIT_DELAYED_WORK(&hs->hs_work, hs_dev_func);
+	INIT_DELAYED_WORK(&hs->temp_int_work, temp_int_func);
+	tasklet_init(&hs->hs_stask, hs_task_func, (long unsigned int)hs);
+	return 0;
+}
+
+/* Init function
+ * Need to get the shared mem base and initialize the global variable.
+ * Start kernel thread for commands on cmd0
+ */
+int __init netl_pti_dev_init(void)
+{
+	dev_hs = kzalloc(sizeof(struct handshake_struct), GFP_KERNEL);
+	if(!dev_hs){
+		PERROR("No memory to allocate\n");
+		return ENOMEM;
+	}
+	pti_mem_start =  setup_pcie_shared_memspace((u64 *)&dev_hs->pci_len);
+	if (pti_mem_start == 0) {
+		return -ENOMEM;
+	}
+	ss_workq = create_workqueue("ss_workq");
+	BUG_ON(!ss_workq);
+
+	dev_hs->start = ioremap_nocache(virt_to_phys((volatile const void *)pti_mem_start), dev_hs->pci_len);
+	if (dev_hs->start == NULL) {
+		return -EFAULT;
+	}
+	init_handshake_struct(dev_hs);
+	schedule_delayed_work(&dev_hs->hs_work, 100);	/*In keventd*/
+	PERROR("pti_subsys = %ld, desc_ring = %ld, descriptor = %ld, _desc_pci = %ld, rx=%ld\n", sizeof(struct pti_subsys), sizeof(struct desc_ring), sizeof(struct descriptor), sizeof(struct _desc_pci), sizeof(struct rx_struct));
+	return 0;
+}
+
+void __exit netl_pti_dev_uninit(void)
+{
+	if(dev_hs->status == HS_DEV_READY_TXRX){
+		netl_unregister_chrdev(dev_hs);
+	}
+	dev_hs->status = HS_DEV_EXITING;
+	tasklet_kill(&dev_hs->hs_stask);
+	cancel_delayed_work_sync(&dev_hs->temp_int_work);
+	flush_delayed_work(&dev_hs->temp_int_work);
+	dealloc_all_subsys(dev_hs);
+	debugfs_remove_recursive(dev_hs->dbg_root);
+	kfree(dev_hs);
+	return;
+}
+
+module_init(netl_pti_dev_init);
+module_exit(netl_pti_dev_uninit);
+MODULE_LICENSE("GPL");
+
+#ifndef CONFIG_NLM_XLP
+static int xls_get_pci_mode(void)
+{
+	uint32_t mode;
+	phoenix_reg_t *pcix_ctrl_mmio = phoenix_io_mmio(PHOENIX_IO_GPIO_OFFSET);
+	/* gpio Reset Configuration Register */
+	mode = pcix_ctrl_mmio[21];
+	mode = mode >> 20;
+	if ((mode & 0x1) == 0) {
+		return XLR_PCI_DEV_MODE;
+	}
+	return XLR_PCI_HOST_MODE;
+}
+
+volatile void *get_start_of_mem(void)
+{
+	phoenix_reg_t *pcix_ctrl_mmio_le = phoenix_io_mmio(PHOENIX_IO_PCIE_1_OFFSET);
+	phoenix_reg_t *bridge_mmio = (phoenix_reg_t *) phoenix_io_base;
+	u64 start;
+
+	if(xls_get_pci_mode() != XLR_PCI_DEV_MODE){
+		printk("ERROR: HOST MODE\n");
+		pti_mem_start= 0;
+		return NULL;
+	}
+
+	/* PRM : PCI Address Map Registers : 22.8.7 */
+	bridge_mmio[0x3a] |= 0x2;
+	bridge_mmio[0x42] = 0x8000ffff;
+	start = (u64)pcix_ctrl_mmio_le[28];
+	printk("start from mmio %llx\n", start);
+	start = ((u64)start >> 15) << 23;
+	printk("start address is %llx\n", start);
+	start = start | CKSEG1;
+	pti_mem_start = (volatile void *)start;
+	printk("start =  %llx\n", start);
+	printk("pti_mem_start =  %p\n", pti_mem_start);
+	printk("device mem is at %llx\n", (u64) bridge_mmio[0x42]);
+	return pti_mem_start;
+}
+
+#endif	// CONFIG_NLM_XLP
+
diff --git a/drivers/misc/netlogic/pcie-offload/char/host/Makefile b/drivers/misc/netlogic/pcie-offload/char/host/Makefile
new file mode 100644
index 0000000..c469cff
--- /dev/null
+++ b/drivers/misc/netlogic/pcie-offload/char/host/Makefile
@@ -0,0 +1,37 @@
+CSCOPE = $(shell which cscope)
+CSCOPEFLAGS = -bvkq -R
+CTAGS = $(shell which ctags)
+#if host needs to send data (for testing) to device uncomment next line
+ifeq ($(KDIR),)
+	KDIR := /lib/modules/$(shell uname -r)/build
+endif
+# EXTRA_CFLAGS += -DNETL_PTI_TEST
+ifneq ($(KERNELRELEASE),)
+EXTRA_CFLAGS := -I$(src)/../../include -DNETL_PTI_HOST -DNETL_ERR_DEBUG
+EXTRA_CFLAGS += -DNETL_TARGET_XLP -DNETL_PTI_DEBUG -g
+obj-m += netl_pti.o
+netl_pti-objs := netl_pti_host.o netl_pti_common.o netl_pti_char.o
+
+else
+modules:
+	make -C $(KDIR) M=$(PWD) modules
+
+all: module
+
+help:
+	make -C $(KDIR) M=$(PWD) help
+
+cscope:
+	$(CSCOPE) $(CSCOPEFLAGS)
+
+tags:
+	$(CTAGS) netl_pti_host.c ../../include/{nlm_pcie.h,netl_pti_common.c,netl_pti_char.c,netl_pti.h,netl_pti_char.h}
+
+clean:
+	make -C $(KDIR) M=$(PWD) clean
+	rm -rf *.o *.mod.* *.ko
+distclean: clean
+	rm -rf cscope*
+	rm -rf tags
+.PHONY: help cscope tags clean distclean
+endif
diff --git a/drivers/misc/netlogic/pcie-offload/char/host/netl_pti_host.c b/drivers/misc/netlogic/pcie-offload/char/host/netl_pti_host.c
new file mode 100644
index 0000000..63dc6e7
--- /dev/null
+++ b/drivers/misc/netlogic/pcie-offload/char/host/netl_pti_host.c
@@ -0,0 +1,937 @@
+#include <netl_pti.h>
+
+struct workqueue_struct *ss_workq;	/* workqueue for int handling */
+
+#ifdef NETL_TARGET_XLP
+#define NETL_VENDOR_ID 0x184e
+#define NETL_DEVICE_ID 0x1004
+#else
+#define NETL_VENDOR_ID 0x182e
+#define NETL_DEVICE_ID 0xabcd
+#endif
+
+static struct pci_device_id netl_id_table[] = {
+	{NETL_VENDOR_ID, PCI_ANY_ID, PCI_ANY_ID, PCI_ANY_ID, 0, 0, 0},
+	{0,}
+};
+
+extern void dealloc_all_subsys(struct handshake_struct *hs);
+extern void dump_hs(struct seq_file *seq, struct handshake_struct *hs);
+extern void dump_desc_ring(struct seq_file *seq, struct desc_ring *ring, int dump_all);
+extern void netl_unregister_chrdev(struct handshake_struct *hs);
+extern int netl_register_chrdev(struct handshake_struct *hs);
+extern int init_handshake_struct(struct handshake_struct *hs);
+extern struct pti_subsys *init_one_subsys(struct handshake_struct *hs, int id);
+
+#define NETL_DRIVER "netl_pti"
+
+/*
+ * device_info keeps tracks of initialized instances of devices
+ * one device is typically one card in the system.
+ * A static allocation could be discontiguous in physical mem,
+ * so, we go with a list of devices
+ */
+struct netl_device{
+	int count;
+	spinlock_t lock;
+	struct list_head head;
+};
+struct netl_device *dev_list = NULL;
+
+/* function declarations */
+void netl_add_dev(struct handshake_struct *p);
+static int netl_phnx_generic_probe(struct pci_dev *pdev, const struct pci_device_id *id);
+void subsys_poll(struct pti_subsys*);
+static void subsys_poll_rx(struct pti_subsys *);
+void subsys_poll_tx(struct pti_subsys *);
+irqreturn_t host_msi_handler(int irq, void *d);
+int send_one_packet_to_device(struct pti_subsys *ss, unsigned long data, u32 len);
+void netl_rm_dev(struct handshake_struct *p);
+int init_device(struct handshake_struct *hs, int blocking);
+static void hs_work_func(struct work_struct *work);
+static void netl_phnx_generic_remove(struct pci_dev *pdev);
+int netl_pti_default_rx(uint8_t msg_code, void *laddr, unsigned int llen, void *addr, unsigned int len);
+static void raise_device_interrupt(void);
+int __host_def_rx_cmpl(struct pti_subsys *ss, struct descriptor *d, struct iovec *vec);
+int add_to_rxlist(struct pti_subsys *ss, volatile struct kvec *iov);
+
+struct pti_subsys *get_ss_from_idx(struct handshake_struct *hs, int idx);
+static int start_host_control_subsys(struct handshake_struct *hs, struct pti_subsys **ctrl_ss);
+
+void temp_int_func(struct work_struct *work);
+int setup_host_irq(struct handshake_struct *);
+void hs_task_func(unsigned long d);
+
+static void raise_device_interrupt(void)
+{
+//	PDEBUG("Raising an interrupt to device\n");
+}
+
+#define incr_sent_seq(hs)\
+do{\
+	volatile u32 sn = phnx_pci_readb(&(hs->sent_msg_seq));\
+	host_to_pci32(sn+1, &(hs->sent_msg_seq));\
+}while(0)
+
+
+static struct pci_driver netl_pci_driver = {
+	.name = NETL_DRIVER,
+	.id_table = netl_id_table,
+	.probe = netl_phnx_generic_probe,
+	.remove = netl_phnx_generic_remove,
+#ifdef POWER_MANAGEMENT
+	.suspend = netl_suspend,
+	.resume = netl_resume
+#endif
+};
+
+static void netl_phnx_generic_remove(struct pci_dev *pdev)
+{
+	struct handshake_struct *hs = NULL;
+	unsigned long flags;
+
+	hs = pci_get_drvdata(pdev);
+
+	netl_unregister_chrdev(hs);
+	/* Disable Interrupt scheduling */
+	spin_lock_irqsave(&hs->hs_lock, flags);
+	hs->status = HS_DEV_EXITING;
+	spin_unlock_irqrestore(&hs->hs_lock, flags);
+#ifdef NETL_TARGET_XLP
+	if(hs->msi_flag == NETL_MSI_ENABLE){
+		free_irq(pdev->irq, hs);
+		pci_disable_msi(pdev);
+	}else if(hs->msi_flag == NETL_INTX_ENABLE){
+		free_irq(pdev->irq, pdev);
+	}
+#else
+	cancel_delayed_work(&hs->temp_int_work);
+	flush_delayed_work(&hs->temp_int_work);
+#endif
+	/* No more tasklet scheduling from now onwards */
+	netl_rm_dev(hs);
+	dealloc_all_subsys(hs);
+	debugfs_remove_recursive(hs->dbg_root);
+	kfree(hs->hhash);
+	pci_set_drvdata(pdev, NULL);
+	cancel_delayed_work(&hs->hs_work);
+	flush_delayed_work(&hs->hs_work);
+	iounmap(hs->start);
+	pci_release_region(pdev, 0);
+	pci_disable_device(pdev);
+	pci_set_drvdata(pdev, NULL);
+	pci_clear_master(pdev);
+	kfree(hs);
+
+	return;
+}
+
+
+/*
+ * Function to call when a rx complete interrupt is received
+ * What does this do?
+ *	TODO
+ * @ss : subsystem pointer
+ * @d  : descriptor pointer (has a back reference to ss->{t,r}xring
+ */
+int host_def_rx_cmpl(struct descriptor *d)
+{
+	struct pti_subsys *ss = d->pring->ss;
+	volatile struct kvec vec;
+	unsigned long buf = 0;
+	//unsigned long flags;
+	volatile dma_addr_t daddr, dm_old;
+	u32 tmp;
+
+	buf = get_zeroed_page(GFP_ATOMIC);
+	if(!buf){
+		PERROR("Mem Alloc fail in rx completion\n");
+		/* Discard current data and make it DMA-able in dma_stat */
+		pti_pci_write32(NETL_BUFSIZE, (void *)&d->pci_desc->dma_len);
+		pti_pci_write32(DESC_DMA_READY, (void *)&d->pci_desc->dma_stat);
+		d->pring->end = INCR_CEIL(d->idx);
+		pti_pci_write32(d->pring->end, (void *)d->pring->tail);
+		/* dma_addr and the corresponding d->buf remains the same */
+		return 0;
+	}
+	daddr = pci_map_single(ss->hs->p_dev, (void *)buf, NETL_BUFSIZE, DMA_FROM_DEVICE);
+	BUG_ON(!daddr);
+	// PDEBUG("d->buf = %#lx, daddr = %p\n", buf, (void *)daddr);
+
+	/* store what we need to send up (buf and len) */
+	vec.iov_len = pti_pci_read32((void *)&d->pci_desc->dma_len);
+	vec.iov_base = (void *)d->buf;
+	ss->rxbytes += vec.iov_len;
+
+	/* free up the mapping */
+	dm_old = pti_pci_read64((void *)&d->pci_desc->dma_addr);
+	pci_unmap_single(ss->hs->p_dev, dm_old, NETL_BUFSIZE, DMA_FROM_DEVICE);
+
+	/* Setup DMA with new address and length*/
+	pti_pci_write64(daddr, (void *)&d->pci_desc->dma_addr);
+	pti_pci_write32(NETL_BUFSIZE, (void *)&d->pci_desc->dma_len);
+	pti_pci_write32(DESC_DMA_READY, (void *)&d->pci_desc->dma_stat);
+	d->pring->end = INCR_CEIL(d->idx);
+	pti_pci_write32(d->pring->end, (void *)d->pring->tail);/* Incremented val */
+	mb();
+#ifdef NETL_ERR_DEBUG
+	/* This is for debugging */
+	dm_old = pti_pci_read64((void *)&d->pci_desc->dma_addr);
+	if(dm_old != daddr){
+		PERROR("W %llx, R %#llx\n", dm_old, daddr);
+	}
+	if((tmp = pti_pci_read32((void *)&d->pci_desc->dma_stat)) != DESC_DMA_READY){
+		PERROR("W %#x, R %#x\n", DESC_DMA_READY, tmp);
+	}
+	if((tmp = pti_pci_read32((void *)&d->pci_desc->dma_len)) != NETL_BUFSIZE){
+		PERROR("W %#x, R %#x\n", NETL_BUFSIZE, tmp);
+	}
+	if((tmp = pti_pci_read32((void *)d->pring->tail)) > NETL_NR_DESC){
+		PERROR("W %#x, R %#x\n", INCR_CEIL(d->idx), tmp);
+	}
+#endif
+	/* preserve buf info in the descriptor */
+	d->buf = buf;
+	d->len = NETL_BUFSIZE;
+#ifdef NETL_PTI_TEST
+	free_page((unsigned long)vec.iov_base);
+#else
+	/* Call user supplied function from here  TODO*/
+	if(add_to_rxlist(ss, &vec) != 0){
+		PERROR("Failed to add kvec to list\n");
+		free_page((unsigned long)vec.iov_base);
+		vec.iov_base = NULL;
+	}
+#endif
+	return 0;
+}
+/*
+ * Handles the handshake of device
+ */
+static void hs_work_func(struct work_struct *work)
+{
+	struct handshake_struct *hs = container_of(
+		work, struct handshake_struct, hs_work.work);
+	unsigned long flags;
+	u32 ret;
+
+	if(!hs){
+		PDEBUG("Null HS\n");
+		return;
+	}
+	ret = pti_pci_read32((void *)hs->wcmd);
+	if(ret == NETL_DEV_SIGNATURE){
+		/* Now clear host and dev signatures */
+		pti_pci_write32(0, (void *)hs->wcmd);
+		pti_pci_write32(0, (void *)(hs->wcmd + (NETL_HS_MAX_PARAMS - 1)));
+		spin_lock_irqsave(&hs->hs_lock, flags);
+		hs->status = HS_DEV_READY_TXRX;
+		spin_unlock_irqrestore(&hs->hs_lock, flags);
+		PDEBUG("HS complete. Device ready for Tx and Rx\n");
+		ret = setup_host_irq(hs);
+		if(ret < 0){
+			PERROR("Failed to setup Interrupt. Exiting\n");
+			return;
+		}
+	}else{
+		schedule_delayed_work(&hs->hs_work, HZ);
+	}
+	return;
+}
+
+/*
+ * Starts the control system after handshaking with device
+ *
+ * @hs	: handshake struct
+ */
+static int start_host_control_subsys(struct handshake_struct *hs, struct pti_subsys **ctrl_ss)
+{
+	u32 ret = 0;
+	struct pti_subsys *ss;
+
+	switch(hs->status){
+	case HS_DEV_READY_TXRX:
+		ss = get_ss_from_idx(hs, NETL_CONTROL_SUBSYS);
+		if(ss == NULL){
+			ret = -EFAULT;
+			PDEBUG("HS has no control SS - XXX\n");
+			goto do_nothing;
+		}
+		goto success;
+	case HS_STATUS_NOT_DONE:
+		hs->status = HS_STATUS_PROGRESSING;
+		break;
+	case HS_STATUS_PROGRESSING:
+		goto do_nothing;
+	default:
+		PDEBUG("Inconsistent state during handshake\n");
+		ret = -EBUSY;
+		goto fail;
+	}
+	ss = init_one_subsys(hs, NETL_CONTROL_SUBSYS);
+	if(ss == NULL){
+		ret = -EFAULT;
+		goto fail;
+	}
+success:
+	if(ctrl_ss != NULL){
+		*ctrl_ss = ss;
+	}
+	return 0;
+
+do_nothing:
+	return ret;
+fail:
+	hs->status = HS_STATUS_NOT_DONE;
+	return ret;
+}
+
+/*
+ * Allocate and initialize handshake structure
+ */
+int host_specific_hs_init(struct handshake_struct *hs)
+{
+	hs->params.max_subsys = NETL_MAX_SUBSYS;
+	hs->params.nr_desc = NETL_NR_DESC;
+	hs->params.flags = 0;
+
+	hs->status = HS_STATUS_NOT_DONE;
+	INIT_LIST_HEAD(&hs->dhead);	/* device list */
+	hs->dev_list = dev_list;
+
+	/* host cmd/stat [0/4]
+	 * dev cmd/stat [8/0xc] */
+	hs->wcmd = (volatile u32 *)
+		((u8 *)hs->start + PTI_ADDR_OFFSET + HCMD0_OFFSET);
+	hs->rstat = (volatile u32 *)
+		((u8 *)hs->start + PTI_ADDR_OFFSET + HSTAT0_OFFSET);
+	hs->rcmd = (volatile u32 *)
+		((u8 *)hs->start + PTI_ADDR_OFFSET + DCMD0_OFFSET);
+	hs->wstat = (volatile u32 *)
+		((u8 *)hs->start + PTI_ADDR_OFFSET + DSTAT0_OFFSET);
+	INIT_DELAYED_WORK(&hs->temp_int_work, temp_int_func);
+	INIT_DELAYED_WORK(&hs->hs_work, hs_work_func);
+	tasklet_init(&hs->hs_stask, hs_task_func, (long unsigned int)hs);
+	dump_hs(NULL, hs);
+	PDEBUG("HS initialization success\n");
+	return 0;
+}
+
+/*
+ * Device waiting for handshake inputs. Supply them
+ * Write u32 values one by one with a header and footer.
+ * When footer is modified from HOST to DEV signature, handshake is complete
+ */
+int init_device(struct handshake_struct *hs, int blocking)
+{
+	u32 idx= 0;
+	volatile u32 *pos = hs->wcmd + 1;
+	u32 readval;
+	u32 param_array[NETL_HS_MAX_PARAMS] = {
+		NETL_HOST_SIGNATURE,
+		hs->params.max_subsys,
+		hs->params.flags,
+		hs->params.nr_desc,
+		NETL_HOST_SIGNATURE,
+	};
+
+	//PDEBUG("max_subsys = %#x, flags = %#x, nr_desc= %#x\n", hs->params.max_subsys, hs->params.flags, hs->params.nr_desc);
+	/* check the end offset. If it is set, something is wrong */
+	readval = pti_pci_read32((void *)(hs->wcmd + (NETL_HS_MAX_PARAMS - 1)));
+	if(readval == NETL_DEV_SIGNATURE){
+		/* something wrong. Cannot proceed */
+		return -EFAULT;
+	}
+	/* write params to pci space */
+	for(idx = 1; idx < NETL_HS_MAX_PARAMS; idx++){
+		pti_pci_write32(param_array[idx], (void *)pos);
+		pos++;
+	}
+	/* Now write the signature */
+	pti_pci_write32(NETL_HOST_SIGNATURE, (void *)hs->wcmd);
+	/* We dont read the values back for handshake.
+	 * TODO
+	 * An interrupt be generated when device is fully initialized */
+	return 0;
+}
+
+void __iomem *iomem_start = NULL;
+
+/*
+ * Adds one packet to Tx queue of host
+ * Tx queue is setup from the host side with physical addresses of buffers and
+ * lengths. Even though host marks dma, actual DMA is under device's control
+ *
+ * @ss		: subsystem through which we send packet.
+ * @page	: virtual page address to send
+ * @len		: #of bytes in data
+ */
+int send_one_packet_to_device(struct pti_subsys *ss, unsigned long page, u32 len)
+{
+	struct desc_ring *txr;
+	unsigned long fl_rng;
+	volatile u32 stat;
+	dma_addr_t dmaddr;
+	dma_addr_t dm_old;
+	u32 tmp, stmp;
+
+	txr = ss->txring;
+	dmaddr = pci_map_single(ss->hs->p_dev, (void *)page, NETL_BUFSIZE, DMA_TO_DEVICE);
+	if(!dmaddr){
+		return EAGAIN;
+	}
+	spin_lock_irqsave(&txr->rng_lock, fl_rng);
+	if(INCR_CEIL(txr->start) == txr->end){	/* Queue full, can't add */
+		BUG_ON(txr->nfree != 0);
+		spin_unlock_irqrestore(&txr->rng_lock, fl_rng);
+		return EAGAIN;
+	}
+	stat = pti_pci_read32((void *)&txr->pdesc[txr->start].pci_desc->dma_stat);
+	if(is_dma_set(stat)){
+		/* DMA under progress. Abandon write */
+		PERROR("DMA set for tx buffer\n");
+		pci_unmap_single(ss->hs->p_dev, dmaddr, len, DMA_TO_DEVICE);
+		spin_unlock_irqrestore(&txr->rng_lock, fl_rng);
+		// dump_desc_ring(NULL, txr, 1);
+		return EAGAIN;
+	}
+	txr->pdesc[txr->start].buf = page;
+	pti_pci_write64(dmaddr, (void *)&txr->pdesc[txr->start].pci_desc->dma_addr);
+	txr->pdesc[txr->start].len = len;
+	pti_pci_write32(len, (void *)&txr->pdesc[txr->start].pci_desc->dma_len);
+	pti_pci_write32(DESC_DMA_READY, (void *)&txr->pdesc[txr->start].pci_desc->dma_stat);
+	/* Incrementing txr->start should be the last step */
+	stmp = txr->start;
+	pti_pci_write32(INCR_CEIL(txr->start), (void *)txr->head);
+	txr->start = INCR_CEIL(txr->start);
+	mb();
+	ss->txbytes += len;
+#ifdef NETL_ERR_DEBUG
+	/* This is for debugging */
+	dm_old = pti_pci_read64((void *)&txr->pdesc[stmp].pci_desc->dma_addr);
+	if(dm_old != dmaddr){
+		PERROR("W %#llx, R %#llx\n", dmaddr, dm_old);
+	}
+	if((tmp = pti_pci_read32((void *)&txr->pdesc[stmp].pci_desc->dma_stat)) != DESC_DMA_READY){
+		PERROR("W %#x, R %#x\n", DESC_DMA_READY, tmp);
+	}
+	if((tmp = pti_pci_read32((void *)&txr->pdesc[stmp].pci_desc->dma_len)) != len){
+		PERROR("W %#x, R %#x\n", 0, tmp);
+	}
+	if((tmp = pti_pci_read32((void *)txr->head)) != INCR_CEIL(stmp)){
+		PERROR("W %#x, R %#x\n", INCR_CEIL(txr->pdesc[stmp].idx), tmp);
+	}
+#endif
+	txr->nfree--;
+	//PDEBUG("start = %#x, tail = %#x, nfree = %#x\n", txr->start, pti_pci_read32((void *)txr->tail), txr->nfree);
+	spin_unlock_irqrestore(&txr->rng_lock, fl_rng);
+	raise_device_interrupt();
+	return 0;
+}
+EXPORT_SYMBOL(send_one_packet_to_device);
+/* Interrupt handler
+ *
+ * @irq		: irq number
+ * @d		: data supplied during registration
+ */
+irqreturn_t host_msi_handler(int irq, void *d)
+{
+	struct handshake_struct *hs = (struct handshake_struct *)d;
+	int count;
+#if 0
+	/* 32 MB PCI Mem space is devided into four equal parts.
+	 * Last 8MB is used for register mapping
+	 */
+	volatile void *devint_mem = hs->start + ((0x3 << 23) + 0x8000);
+
+	// PDEBUG("hs->start = %llx, Read %x\n", hs->start, __be32_to_cpu(__raw_readl(devint_mem)));
+	/* Acknowledge interrupt by clearing ACK */
+	printk("cmd = %#x\n",__be32_to_cpu(__raw_readl(devint_mem + (0x258 << 2))));
+	printk("ack = %#x\n",__be32_to_cpu(__raw_readl(devint_mem + (0x259 << 2))));
+	__raw_writel(__cpu_to_be32(0x1), (devint_mem + (0x259 << 2)));
+	printk("cmd = %#x\n",__be32_to_cpu(__readl(devint_mem + (0x258 << 2))));
+	printk("ack = %#x\n",__be32_to_cpu(__readl(devint_mem + (0x259 << 2))));
+	__raw_writel(__cpu_to_be32(0x0), (devint_mem + (0x258 << 2)));
+#endif
+	/* go through registered subsystems and start threads */
+	for(count = 0; count < NETL_SUBSYS_HASH; count++){
+		if(hlist_empty(&hs->hhash[count])){
+			continue;
+		}
+		/* If at least one is not empty, schedule tasklet */
+		tasklet_schedule(&hs->hs_stask);
+		/*
+		hlist_for_each_entry(ss, node, &hs->hhash[count], snode){
+			tasklet_schedule(&ss->stask);
+		}
+		*/
+	}
+	return IRQ_HANDLED;
+}
+
+
+void check_and_set_int_flags(struct pti_subsys *ss)
+{
+	unsigned long flags, flag1;
+	u32 tail, head;
+
+	spin_lock_irqsave(&ss->slock, flags);
+	/* rx for sent data from device */
+	spin_lock_irqsave(&ss->rxring->rng_lock, flag1);
+	head = pti_pci_read32((void *)ss->rxring->head);
+	//PDEBUG("idx = %d, head = %#x, start = %#x\n", ss->id, head, ss->rxring->start);
+	if(head != ss->rxring->start){
+		ss->flags |= SUBSYS_FLAGS_RX_PENDING;
+	}
+	spin_unlock_irqrestore(&ss->rxring->rng_lock, flag1);
+
+	/* Tx complete for sent data from host */
+	spin_lock_irqsave(&ss->txring->rng_lock, flag1);
+	tail= pti_pci_read32((void *)ss->txring->tail);
+	//PDEBUG("idx = %d, tail = %#x, end = %#x\n", ss->id, tail, ss->txring->end);
+	if(tail != ss->txring->end){
+		ss->flags |= SUBSYS_FLAGS_TX_PENDING;
+	}
+	spin_unlock_irqrestore(&ss->txring->rng_lock, flag1);
+
+	spin_unlock_irqrestore(&ss->slock, flags);
+}
+
+void hs_task_func(unsigned long d)
+{
+	struct handshake_struct *hs = (struct handshake_struct *)d;
+	int count;
+	struct pti_subsys *ss;
+	struct hlist_node *node;
+
+
+	for(count = 0; count < NETL_SUBSYS_HASH; count++){
+		if(hlist_empty(&hs->hhash[count])){
+			continue;
+		}
+		hlist_for_each_entry(ss, node, &hs->hhash[count], snode){
+		/* Need a better way findng out subsystem with job pending */
+			check_and_set_int_flags(ss);
+			subsys_poll(ss);
+		}
+	}
+	return;
+}
+/*
+ * Polling function
+ *
+ * @work	: ss->swork
+ */
+void subsys_poll(struct pti_subsys *d)
+{
+	unsigned long flags;
+	struct pti_subsys *ss = (struct pti_subsys *)d;
+
+	spin_lock_irqsave(&ss->slock, flags);
+	if(ss->flags & SUBSYS_FLAGS_RX_DISABLED){
+		spin_unlock_irqrestore(&ss->slock, flags);
+		goto rx_done;
+	}
+	if(ss->flags & SUBSYS_FLAGS_RX_PENDING){
+		if(ss->flags & SUBSYS_FLAGS_RX){
+			/* running and pending; clear flag */
+			ss->flags &= ~SUBSYS_FLAGS_RX_PENDING;
+			spin_unlock_irqrestore(&ss->slock, flags);
+		}else{
+			/* Rx not running and pending. Run */
+			ss->flags |= SUBSYS_FLAGS_RX;
+			spin_unlock_irqrestore(&ss->slock, flags);
+			subsys_poll_rx(ss);
+		}
+	}else{
+		spin_unlock_irqrestore(&ss->slock, flags);
+	}
+rx_done:
+	spin_lock_irqsave(&ss->slock, flags);
+	if(ss->flags & SUBSYS_FLAGS_TX_DISABLED){
+		spin_unlock_irqrestore(&ss->slock, flags);
+		goto tx_done;
+	}
+	if(ss->flags & SUBSYS_FLAGS_TX_PENDING){
+		if(ss->flags & SUBSYS_FLAGS_TX){
+			/* running and pending; clear flag and reschedule self */
+			ss->flags &= ~SUBSYS_FLAGS_TX_PENDING;
+			spin_unlock_irqrestore(&ss->slock, flags);
+		}else{
+			/* Tx not running and pending. Run */
+			ss->flags |= SUBSYS_FLAGS_TX;
+			spin_unlock_irqrestore(&ss->slock, flags);
+			subsys_poll_tx(ss);
+		}
+	}else{
+		spin_unlock_irqrestore(&ss->slock, flags);
+	}
+tx_done:
+	return;
+}
+
+/*
+ * Handles Tx complete interrupt
+ *
+ * @work	: work struct
+ */
+void subsys_poll_tx(struct pti_subsys *ss)
+{
+	struct desc_ring *txr;
+	unsigned long fl_rng;
+	volatile u32 tail;
+	volatile u32 dmastat;
+	int loops;
+	struct descriptor *d;
+	volatile dma_addr_t dmaddr;
+	volatile dma_addr_t dm_old;
+	u32 tmp;
+
+	txr = ss->txring;
+	spin_lock_irqsave(&txr->rng_lock, fl_rng);
+	tail = pti_pci_read32((void *)txr->tail);
+	loops = (ss->hs->params.nr_desc + tail - txr->end) % ss->hs->params.nr_desc;
+	/* We loop from txr->end (incl) to tail (excl). As a result, at the end
+	 * of the loop, we would have freed all buffers with DMA complete and
+	 * reach
+	 * 1. head == tail == txr->end : Empty ring
+	 * 2. head != tail == txr->end : partially full ring
+	 * (valid data (DMA incompl by the device).
+	 * In 1., if data gets added, we start again from txr->end(incl)
+	 * In 2., we must begin processing from 'txr->end` because DMA can be
+	 * under progress and can get completed anytime
+	 */
+	while(loops--){
+		//PDEBUG("loops = %d, end = %#x, tail = %#x\n", loops+1, txr->end, tail);
+		BUG_ON(txr->end == tail);
+		d = &txr->pdesc[txr->end];
+		dmastat = pti_pci_read32((void *)&d->pci_desc->dma_stat);
+		if(is_dma_set(dmastat)){
+			spin_unlock_irqrestore(&txr->rng_lock, fl_rng);
+			PERROR("Error. DMA set in Tx complete buf %#x\n", d->idx);
+			dump_desc_ring(NULL, txr, 1);
+			// BUG_ON(is_dma_set(dmastat));
+			return;		/* No need, BUG() panics */
+		}
+		dmaddr = pti_pci_read64((void *)&d->pci_desc->dma_addr);
+		pci_unmap_single(ss->hs->p_dev, dmaddr, d->len, DMA_TO_DEVICE);
+		pti_pci_write64(0, (void *)&d->pci_desc->dma_addr);
+		pti_pci_write32(0, (void *)&d->pci_desc->dma_len);
+		pti_pci_write32(0, (void *)&d->pci_desc->dma_stat);
+#ifdef NETL_ERR_DEBUG
+		/* This is for debugging */
+		dm_old = pti_pci_read64((void *)&d->pci_desc->dma_addr);
+		if(dm_old != 0){
+			PERROR("W 0, R %#llx\n", dm_old);
+		}
+		if((tmp = pti_pci_read32((void *)&d->pci_desc->dma_stat)) != 0){
+			PERROR("W %#x, R %#x\n", 0, tmp);
+		}
+		if((tmp = pti_pci_read32((void *)&d->pci_desc->dma_len)) != 0){
+			PERROR("W %#x, R %#x\n", 0, tmp);
+		}
+#endif
+		txr->end = INCR_CEIL(txr->end);
+		free_page(d->buf);
+		d->buf = 0;
+		d->len = 0;
+		txr->nfree++;
+		//PDEBUG("nfree = %#x\n", d->pring->nfree);
+	}
+	spin_unlock_irqrestore(&txr->rng_lock, fl_rng);
+
+	spin_lock_irqsave(&ss->slock, fl_rng);
+	ss->flags &= ~SUBSYS_FLAGS_TX;
+	ss->flags &= ~SUBSYS_FLAGS_TX_PENDING;
+	spin_unlock_irqrestore(&ss->slock, fl_rng);
+
+	/* Wake up any processes sleeping on ss->wrq */
+	//PDEBUG("Wake up process\n");
+	wake_up_interruptible(&ss->wrq);
+	tail = pti_pci_read32((void *)txr->tail);
+	loops = (ss->hs->params.nr_desc + tail - txr->end) % ss->hs->params.nr_desc;
+	if(loops > 0){
+		tasklet_schedule(&ss->hs->hs_stask);
+	}
+	return;
+}
+/*
+ * Polling function for rx complete
+ *
+ * @work	: ss->swork
+ */
+static void subsys_poll_rx(struct pti_subsys *ss)
+{
+	struct desc_ring *rxr;
+	unsigned long fl_rng;
+	volatile u32 head, tmp, dma_stat;
+	int loops;
+
+	rxr = ss->rxring;
+	spin_lock_irqsave(&rxr->rng_lock, fl_rng);
+	head = pti_pci_read32((void *)rxr->head);
+	loops = (ss->hs->params.nr_desc + head - rxr->start) % ss->hs->params.nr_desc;
+	//PDEBUG("LOOPS = %d, HEAD = %#x, START = %#x\n", loops, head, rxr->start);
+	while(loops--){
+		//PDEBUG("loops = %d, head = %#x, start = %#x\n", loops, head, rxr->start);
+		BUG_ON(head == rxr->start);
+		dma_stat = pti_pci_read32((void *)&rxr->pdesc[rxr->start].pci_desc->dma_stat);
+		if(dma_stat == 0xFFFFFFFF){
+			PDEBUG("PCI read fail %llx\n",
+				(void *)&rxr->pdesc[rxr->start].pci_desc->dma_stat);
+			spin_unlock_irqrestore(&rxr->rng_lock, fl_rng);
+			return;
+		}
+		/* If empty, head = start, loops == 0 and does not enter this
+		 * loop. If full, INCR_CEIL(head) == rxr->start */
+		if(is_dma_set(dma_stat)){
+			PERROR("DMA under progress, idx = %#x, dma_stat = %#x\n", rxr->start, dma_stat);
+			//dump_desc_ring(NULL, rxr, 1);
+			spin_unlock_irqrestore(&rxr->rng_lock, fl_rng);
+			return;
+		}
+		tmp = rxr->start;
+		rxr->start = INCR_CEIL(rxr->start);
+		/* call receive complete handler */
+		host_def_rx_cmpl(&rxr->pdesc[tmp]);
+	}
+	spin_unlock_irqrestore(&rxr->rng_lock, fl_rng);
+
+	/* recalculate head and if loops > 0, reschedule ourself */
+	spin_lock_irqsave(&ss->slock, fl_rng);
+	ss->flags &= ~SUBSYS_FLAGS_RX;
+	ss->flags &= ~SUBSYS_FLAGS_RX_PENDING;
+	spin_unlock_irqrestore(&ss->slock, fl_rng);
+	head = pti_pci_read32((void *)rxr->head);
+	loops = (ss->hs->params.nr_desc + head - rxr->start) % ss->hs->params.nr_desc;
+	if(loops > 0){
+		tasklet_schedule(&ss->hs->hs_stask);
+	}
+	//PDEBUG("Exiting\n");
+	return;
+}
+
+void temp_int_func(struct work_struct *work)
+{
+	struct handshake_struct *hs = container_of(
+		work, struct handshake_struct, temp_int_work.work);
+	unsigned long flags;
+
+	spin_lock_irqsave(&hs->hs_lock, flags);
+	host_msi_handler(0, hs);
+	spin_unlock_irqrestore(&hs->hs_lock, flags);
+	schedule_delayed_work(&hs->temp_int_work, HZ);
+}
+#ifdef NETL_TARGET_XLP
+/*
+ * Sets up host irqs.
+ * Currently handles only MSI.
+ */
+int setup_host_irq(struct handshake_struct *hs)
+{
+	int ret;
+
+	ret = pci_enable_msi(hs->p_dev);
+	if(ret < 0){
+		PDEBUG("No MSI. Falling back to INTX\n");
+		goto setup_linex;
+	}
+	ret = request_irq(hs->p_dev->irq, host_msi_handler, 0, NETL_DRIVER, hs);
+	if(ret){
+		PERROR("MSI request_irq failed for %#x\n", hs->p_dev->irq);
+		return ret;
+	}
+	hs->msi_flag = NETL_MSI_ENABLE;
+	return 0;
+setup_linex:
+	ret = request_irq(hs->p_dev->irq, host_msi_handler, 0, NETL_DRIVER, hs);
+	if(ret){
+		PERROR("LINEX request_irq failed for %#x\n", hs->p_dev->irq);
+		return ret;
+	}
+	hs->msi_flag = NETL_INTX_ENABLE;
+	return 0;
+}
+#else
+int setup_host_irq(struct handshake_struct *hs)
+{
+	schedule_delayed_work(&hs->temp_int_work, 5 * HZ);
+	return 0;
+}
+#endif
+
+/*
+ * PCI(e) device probe function
+ *
+ * @pdev	: pci dev structure of this device
+ * @id		: device id list
+ */
+static int netl_phnx_generic_probe(struct pci_dev *pdev, const struct pci_device_id *id)
+{
+	int err;
+	unsigned long base;
+	u32 req_space, da_size;
+	struct handshake_struct *hs = NULL;
+
+	hs = kzalloc(sizeof(struct handshake_struct), GFP_KERNEL);
+	if(!hs){
+		return -ENOMEM;
+	}
+	da_size = DESC_ARRAY_SIZE(NETL_NR_DESC);
+	req_space = PTI_ADDR_OFFSET + 4 * sizeof(u32) +  da_size * 2 * NETL_MAX_SUBSYS;
+	//PDEBUG("Probing device\n");
+	hs->p_dev = pdev;
+	/* list_dev has 0 or more entries. Allocate this device now */
+	if (!(pci_resource_flags(pdev, 0) & IORESOURCE_MEM)) {
+		PERROR("No memory resource in BAR\n");
+		err = ENODEV;
+		goto err_return;
+	}
+	err = pci_request_region(pdev, 0, NETL_DRIVER);
+	if (err) {
+		PERROR("Cannot obtain PCI resources, aborting.");
+		err = ENODEV;
+		goto err_return;
+	}
+	base = pci_resource_start(pdev, 0);
+	if(base == 0){
+		PERROR("Cannot allocate memory resource in BAR\n");
+		goto resource_err;
+	}
+	iomem_start = ioremap_nocache(base, pci_resource_len(pdev, 0));
+	*((u32 *)iomem_start) = 0x2badf00d;
+	hs->start = (volatile void *)iomem_start;
+	pci_set_drvdata(pdev, (void *)hs);
+	if(hs->start == NULL){
+		PERROR("IOREMAP failed on resource in BAR\n");
+		goto resource_err;	/* this is RIGHT */
+	}
+	hs->pci_len = pci_resource_len(pdev, 0);
+	if(hs->pci_len < req_space){
+		PERROR("pci_len = %lx, req_space = %x. Not enough memory allocated\n", hs->pci_len, req_space);
+		goto shmem_fail;
+	}
+	if(init_handshake_struct(hs)){
+		PERROR("Failed to handshake\n");
+		goto shmem_fail;
+	}
+	// PDEBUG("start = 0x%p, len = %#lx\n", hs->start, hs->pci_len);
+	if(netl_register_chrdev(hs) != 0){
+		PDEBUG("Registering chr device fail\n");
+		goto reg_dev_fail;
+	}
+	netl_add_dev(hs);
+	if ((err = pci_enable_device(pdev))) {
+		PERROR("Cannot enable PCI device, aborting.");
+		goto pci_en_fail;
+	}
+	pci_set_master(pdev);
+	/* Setup host side control subsys now */
+	start_host_control_subsys(hs, NULL);
+	err = init_device(hs, 0);
+	if(err != 0){
+		goto pci_en_fail;
+	}
+	schedule_delayed_work(&hs->hs_work, HZ/100);
+	return err;
+
+pci_en_fail:
+	// TODO Need to check all these labels
+// pci_reg_fail:
+	cdev_del(&hs->hs_cdev);
+// dev_init_fail:
+reg_dev_fail:
+	dealloc_all_subsys(hs);
+shmem_fail:
+	iounmap(hs->start);
+resource_err:
+	pci_release_region(pdev, 0);
+err_return:
+	pci_set_drvdata(pdev, NULL);
+	kfree(hs->hhash);
+	kfree(hs);
+	PDEBUG("Failed return");
+	return err;
+}
+
+/*
+ * Adds a new device to global list of netl_device
+ *
+ * @p : device to be added
+ */
+void netl_add_dev(struct handshake_struct *p)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&dev_list->lock, flags);
+	p->hsid = dev_list->count;
+	dev_list->count++;
+	list_add(&dev_list->head, &p->dhead);
+	spin_unlock_irqrestore(&dev_list->lock, flags);
+	return;
+}
+
+void netl_rm_dev(struct handshake_struct *p)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&dev_list->lock, flags);
+	list_del(&p->dhead);
+	dev_list->count--;
+	spin_unlock_irqrestore(&dev_list->lock, flags);
+	return;
+}
+
+#ifdef NETL_PTI_TEST
+int get_hs_from_idx(struct handshake_struct **res, int idx)
+{
+	struct handshake_struct *cur, *n;
+
+	list_for_each_entry_safe(cur, n, &dev_list->head, dhead){
+		if(cur->hsid == idx){
+			*res = cur;
+			return 0;
+		}
+	}
+	return -ENODEV;
+}
+EXPORT_SYMBOL(get_hs_from_idx);
+#endif
+
+void __exit netl_pti_host_uninit(void)
+{
+	pci_unregister_driver(&netl_pci_driver);
+	destroy_workqueue(ss_workq);
+	kfree(dev_list);
+	PDEBUG("Exit complete\n");
+}
+
+int __init netl_pti_host_init(void)
+{
+	int ret = 0;
+
+	PDEBUG("Compiled on %s %s\n", __DATE__, __TIME__);
+	dev_list = kzalloc(sizeof(struct netl_device), GFP_KERNEL);
+	if((dev_list == NULL)){
+		return ENOMEM;
+	}
+	spin_lock_init(&dev_list->lock);
+	INIT_LIST_HEAD(&dev_list->head);
+	ss_workq = create_workqueue("ss_workq");
+	BUG_ON(!ss_workq);
+	//PDEBUG("Work queue created\n");
+	dev_list->count = 0;	/* setting explicitly */
+	ret = pci_register_driver(&netl_pci_driver);
+	if(ret < 0){
+		PDEBUG("Failed to register driver\n");
+		netl_pti_host_uninit();
+		return -1;
+	}
+PERROR("pti_subsys = %ld, desc_ring = %ld, descriptor = %ld, _desc_pci = %ld, rx=%ld\n", sizeof(struct pti_subsys), sizeof(struct desc_ring), sizeof(struct descriptor), sizeof(struct _desc_pci), sizeof(struct rx_struct));
+PERROR("host init success\n");
+	return 0;
+}
+module_init(netl_pti_host_init);
+module_exit(netl_pti_host_uninit);
+
+MODULE_LICENSE("GPL");
diff --git a/drivers/misc/netlogic/pcie-offload/include/chunking-api.h b/drivers/misc/netlogic/pcie-offload/include/chunking-api.h
new file mode 100644
index 0000000..379f2bb
--- /dev/null
+++ b/drivers/misc/netlogic/pcie-offload/include/chunking-api.h
@@ -0,0 +1,45 @@
+#ifndef CHUNKING_API_H
+#define CHUNKING_API_H
+
+/*
+ * NetL Digest APIs
+ */
+#include <sys/types.h>
+#include <linux/types.h>
+#include <sys/socket.h>
+#include <arpa/inet.h>
+#include <stdio.h>
+#include <errno.h>
+#include <unistd.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+#include <stdlib.h>
+#include <string.h>
+#include <sys/uio.h>
+#include <sys/uio.h>
+#include <signal.h>
+#include "mgmt.h"
+#include "netl_pti.h"
+
+
+struct split_request {
+	uint32_t req_id; /* OUT: Returned by the API library and used
+			for polling this split request */
+	struct iovec* in_data; /* IN: array of input data to split
+			and hash.*/
+	int in_cnt; /* IN: number of in_data array elements */
+	int flags; /* IN: options to be passed to the remote DRE module */
+	int max_chunk_count; /* IN: max number of chunks that can be
+			returned in the result. */
+	struct chunking_result* result; /* OUT: result of this split
+			request */
+};
+int hwch_init(void* arg, size_t len);
+int hwch_open(void* open_arg, size_t len, uint64_t* ctx_handle);
+int hwch_split_request(uint64_t ctx_handle, struct split_request* split_req);
+int hwch_split_response(uint64_t ctx_handle, struct split_request **split_req);
+int hwch_get_state(uint64_t ctx_handle, void* state, size_t len);
+int hwch_set_state(uint64_t ctx_handle, void* state, size_t len);
+int hwch_select(uint64_t* ctx_handles, int count);
+int hwch_close(uint64_t ctx_handle);
+#endif
diff --git a/drivers/misc/netlogic/pcie-offload/include/chunking-lib.c b/drivers/misc/netlogic/pcie-offload/include/chunking-lib.c
new file mode 100644
index 0000000..ac56782
--- /dev/null
+++ b/drivers/misc/netlogic/pcie-offload/include/chunking-lib.c
@@ -0,0 +1,508 @@
+#include "../../include/chunking-api.h"
+#include "../../include/netl_pti_char.h"
+
+/* control socket, initialized once */
+struct chunking_result *split(struct iovec* input_iovec, uint32_t n_iovec, struct comm_context *ctx, struct init_params *ip, struct hwch_params *hp);
+
+/* These are serialization buffers
+ * In client/server swap them */
+struct chunking_result *process_rx_data(struct comm_context *, struct lib_chunk_header *);
+int send_chunk_info(struct comm_context *, struct lib_chunk_header *, struct chunking_result *);
+void print_sig(FILE *fptr, unsigned char *);
+int populate_lib_chunk_header(struct split_request *sr, struct mgmt_msg *m);
+
+struct sr_elem{
+	struct list_head head;
+	struct chunking_result *cr;
+	uint32_t req_id;
+};
+
+/*
+ * Sends header data first
+ *
+ * @sock: socket on which we send data
+ * @buf: buffer where data recide
+ * @len: length of data to send
+ */
+static int netl_msg_tx(int sock, void *buf, uint32_t len)
+{
+	uint32_t sent = 0;
+	char *start = (char *)buf;
+	int ret;
+	while(sent < len){
+		//DPRINT("sock = %x, Sent = %d len = %d\n", sock, sent, len);
+		if((ret = write(sock, start + sent, len-sent)) < 0){
+			perror("Socket write failed\n");
+			return ret;
+		}
+		sent += ret;
+	}
+	return sent;
+}
+
+
+int send_coalesced_ci(struct comm_context *ctx, struct chunking_result *cr)
+{
+	struct list_head *elem;
+	struct chunk_info_elem ce, *p;
+	char buf[SER_BUF_LEN];
+	int top = 0;
+
+	for_each_list(&cr->head, elem){
+		memset(&ce, 0, sizeof(struct chunk_info_elem));
+		p = container_of(elem, struct chunk_info_elem *, head);
+		memcpy(ce.cid.fp, p->cid.fp, FP_SIZE);
+		memcpy(ce.cid.md5, p->cid.md5, MD5_SIZE);
+		ce.cid.offset = htonl(p->cid.offset);
+		ce.cid.size = htonl(p->cid.size);
+		ce.cid.flags = htonl(p->cid.flags);
+
+		if (top + sizeof(chunk_info_data) < SER_BUF_LEN){
+			memcpy(buf + top, &ce.cid, CTRL_MSG_SIZE);
+			top += sizeof(chunk_info_data);
+		} else {
+			write(ctx->data_fd, buf, top);
+			memset(buf, 0, SER_BUF_LEN);
+			top = 0;
+		}
+	}
+	return 0;
+}
+
+static int read_split_to_cis(struct comm_context *ctx, struct chunking_result *cr)
+{
+	int len = sizeof(struct chunk_info_data);
+	int did = 0;
+	char buf[SER_BUF_LEN];
+	int once = (int)(SER_BUF_LEN/len);
+	int count;
+	struct chunk_info_data rdcd;
+	struct chunk_info_elem *h;
+	char *st;
+
+	while (did < cr->rsp.chunk_count) {
+		count = ((cr->rsp.chunk_count - did) > once) ?
+			once : (cr->rsp.chunk_count - did);
+		memset(buf, 0, SER_BUF_LEN);
+		read(ctx->data_fd, buf, (count * len));
+		did += count;
+		st = buf;
+		while (count--) {
+			memcpy(&rdcd, st, len);
+			st += len;
+			h = (struct chunk_info_elem *) malloc(sizeof(struct chunk_info_elem));
+			/* Error check TODO */
+			INIT_LIST_HEAD(&h->head);
+			memcpy(h->cid.fp, rdcd.fp, FP_SIZE);
+			memcpy(h->cid.md5, rdcd.md5, MD5_SIZE);
+			h->cid.offset = ntohl(rdcd.offset);
+			h->cid.size = ntohl(rdcd.size);
+			h->cid.flags = ntohl(rdcd.flags);
+			list_add_tail(&cr->head, &h->head);
+		}
+	}
+	return 0;
+}
+
+/*
+ * receives messages sent using netl_msg_tx
+ *
+ */
+static struct mgmt_msg *rx_process_message(struct comm_context *ctx)
+{
+	int ret;
+	struct mgmt_msg rx_msg, *ret_msg;
+
+	DPRINT("Starting rx process\n");
+	ret_msg = (struct mgmt_msg *) malloc(CTRL_MSG_SIZE);
+	if(ret_msg == NULL){
+		return NULL;
+	}
+	if((ret = read(ctx->data_fd, &rx_msg, CTRL_MSG_SIZE)) < 0){
+		free(ret_msg);
+		perror("Read failure");
+		return NULL;
+	}
+	if(ret == 0){
+		DPRINT("EOF reached ... \n");
+		free(ret_msg);
+		return NULL;
+	}
+	/* We have a block of data now. convert it into host byte order */
+	cp_mgmt_msg_BO(ret_msg, &rx_msg, ntohl);
+	return ret_msg;
+}
+
+/*
+ * this is the main processing loop for server.
+ * this one initializes some params and loops for ever
+ *
+ * no params
+ */
+void server_proc_loop(struct comm_context *ctx)
+{
+	struct mgmt_msg *msg_rx;
+	struct chunking_result *cr;
+	struct timeval start, med, end;
+	long double s, m, e, d;
+
+	/* intial global params are set. now loop for channel requests */
+	while(1){
+		DPRINT("Starts loop...\n");
+		msg_rx = rx_process_message(ctx);
+		if(msg_rx == NULL){
+			return;
+		}
+		/* process various type messages */
+		switch(msg_rx->type){
+		case MSG_CHUNK_HEADER:
+			/* here we do not send any response back
+			 * We wait for data to show up */
+			gettimeofday(&start, NULL);
+			cr = process_rx_data(ctx, &msg_rx->lch);
+			gettimeofday(&med, NULL);
+			send_chunk_info(ctx, &msg_rx->lch, cr);
+			gettimeofday(&end, NULL);
+
+			s = start.tv_sec * 1000000 + start.tv_usec;
+			m = med.tv_sec * 1000000 + med.tv_usec;
+			e = end.tv_sec * 1000000 + end.tv_usec;
+
+			d = (m - s)/1000000;
+			fprintf(stderr, "processing %.8Lf sec\n", d);
+			d = (e - m)/1000000;
+			fprintf(stderr, "Sending %.8Lf sec\n", d);
+			break;
+		default:
+			DPRINT("Invalid messages\n");
+			break;
+		}
+	}
+}
+
+int Read(int sock, void *buf, int total)
+{
+	int ret = 0, rd = 0, bal = total;
+	char *start = (char *)buf;
+
+	while(rd < total){
+		ret = read(sock, (void *)start, bal);
+		bal -= ret; rd += ret; start += ret;
+	}
+	return total;
+}
+
+struct chunking_result *process_rx_data(struct comm_context *ctx, struct lib_chunk_header *lch)
+{
+	uint32_t tot_read = 0;
+	int ret;
+	struct iovec *iov;
+	uint32_t iov_num = (lch->num_bytes + SER_BUF_LEN - 1) / SER_BUF_LEN;
+	uint32_t to_read, i;
+	struct chunking_result *result;
+	struct timeval start, end;
+	long double s, e, d;
+
+	iov = (struct iovec *)malloc(iov_num * sizeof(struct iovec));
+	if(iov == NULL){
+		/* TODO */
+		return NULL;
+	}
+	gettimeofday(&start, NULL);
+	for(i = 0; i < iov_num; i++){
+		iov[i].iov_base = malloc(SER_BUF_LEN);
+		iov[i].iov_len = SER_BUF_LEN;
+		to_read = (lch->num_bytes - tot_read) > SER_BUF_LEN ?
+			SER_BUF_LEN : (lch->num_bytes - tot_read);
+		ret = Read(ctx->data_fd, iov[i].iov_base, to_read);
+		if(ret < 0){
+			DPRINT("Error in reading\n");
+			return NULL;
+		} else if(ret == 0){
+			DPRINT("EOF in socket ...\n");
+			break;
+		}
+		tot_read += ret;
+		//DPRINT("read %d bytes in %d iovecs\n", tot_read, i+1);
+	}
+	gettimeofday(&end, NULL);
+	d = ((end.tv_sec * 1000000 + end.tv_usec) - (start.tv_sec * 1000000 + start.tv_usec));
+	d /= (long double)1000000;
+	printf("Total %ld bytes read in %.8Lf seconds. Rx Speed %.8Lf MBps\n",
+			tot_read, d, (tot_read/(d * 1024 * 1024)));
+	/* supply this parameter to split constructor */
+	gettimeofday(&start, NULL);
+	result = split(iov, iov_num, ctx, &ctx->ip.glbp, &ctx->hwchp.chnp);
+	gettimeofday(&end, NULL);
+
+	d = ((end.tv_sec * 1000000 + end.tv_usec) - (start.tv_sec * 1000000 + start.tv_usec));
+	d /= (long double)1000000;
+	printf("Split took %.8Lf seconds for %ld bytes. %.8Lf MBps\n",
+			d, tot_read, (tot_read/(d * 1024 * 1024)));
+	for (i = 0; i< iov_num; i++) {
+		free(iov[i].iov_base);
+	}
+	free(iov);
+	return result;
+}
+
+int send_chunk_info(struct comm_context * ctx, struct lib_chunk_header *lch,
+		struct chunking_result *cr)
+{
+	uint32_t len;
+	struct mgmt_msg h2send;
+	struct list_head *elem;
+	struct chunk_info_data cd;
+	struct chunk_info_elem *p;
+	char buf[SER_BUF_LEN];
+	int top = 0;
+
+	assert(SER_BUF_LEN > sizeof(struct chunk_info_data));
+	memset(&h2send, 0, CTRL_MSG_SIZE);
+	h2send.type = ntohl(MSG_CHUNK_RESPONSE);
+	h2send.msgid = 0;
+	h2send.crh.rem_chunk_count = htonl(cr->rsp.rem_chunk_count);
+	h2send.crh.chunk_count = htonl(cr->rsp.chunk_count);
+	memcpy(h2send.crh.md5sum, cr->rsp.md5sum, MD5_SIZE);
+	netl_msg_tx(ctx->data_fd, (char *)&h2send, CTRL_MSG_SIZE);
+	len = sizeof(struct chunk_info_data);
+
+	/* Now, coalesce the ci-s and send out */
+	len = sizeof(struct chunk_info_data);
+	for_each_list(&cr->head, elem){
+		memset(&cd, 0, len);
+		p = container_of(elem, struct chunk_info_elem *, head);
+		memcpy(cd.fp, p->cid.fp, FP_SIZE);
+		memcpy(cd.md5, p->cid.md5, MD5_SIZE);
+		cd.offset = htonl(p->cid.offset);
+		cd.size = htonl(p->cid.size);
+		cd.flags = htonl(p->cid.flags);
+
+		if (top + len < SER_BUF_LEN){
+			memcpy(buf + top, &cd, len);
+			top += len;
+		}
+		/* We have at least one cd written onto buffer
+		 * if we can write one more, go with next elem, else
+		 * write current buffer out */
+		if (top + len >= SER_BUF_LEN){
+			write(ctx->data_fd, buf, top);
+			memset(buf, 0, SER_BUF_LEN);
+			top = 0;
+		}
+	}
+	/* We hit the last elem, but buffer not written out because there is
+	 * free space. Check it and push buffer out if required */
+	if(top != 0){
+		write(ctx->data_fd, buf, top);
+	}
+	return 0;
+}
+
+/*
+ * Sends data in iovec
+ *
+ */
+int send_iovec_data(struct comm_context *ctx, struct split_request *sr)
+{
+	int n, sent = 0;
+	struct iovec *iov;
+
+	for(n = 0; n < sr->in_cnt; n++){
+		iov = &sr->in_data[n];
+		/* now send buffer */
+		if((sent = netl_msg_tx(ctx->data_fd, (char *)iov->iov_base,
+						iov->iov_len)) < 0){
+			DPRINT("Failed to send iov\n");
+			return -1;
+		}
+	}
+	return 0;
+}
+
+void print_sig(FILE *fptr, unsigned char *digest)
+{
+	int n, *p = (int *)digest;
+	for(n = 0; n < 4; n++){
+		fprintf(fptr, "%08x", p[n]);
+	}
+	fprintf(fptr, "\n");
+}
+
+struct hw_handle {
+	struct comm_context ctx;
+	struct mgmt_msg hwch_params;
+	struct mgmt_msg init_params;
+	struct chunking_result *cr;
+};
+
+int hwch_init(void *p, size_t len)
+{
+	__label__ init_fail;
+	int ret = 0;
+	int fd;
+	struct mgmt_msg local;
+
+	if (p == NULL) {
+		return 0;
+	}
+	if (((struct mgmt_msg *)p)->type != MSG_INIT_PARAM) {
+		return -EINVAL;
+	}
+	if (len != CTRL_MSG_SIZE) {
+		DPRINT("Requires %d bytes\n", CTRL_MSG_SIZE);
+		return -EINVAL;
+	}
+	cp_mgmt_msg_BO(&local, (struct mgmt_msg *)p, htonl);
+
+	/* Open control connection */
+	fd = open(NETL_FILE_PATH, O_RDWR);
+	if (fd < 0) {
+		perror("File open failed\n");
+		return EFAULT;
+	}
+
+	/* set/modify global control parameters to passed param
+	 * What we do is to pass these params to the other end */
+	ret = write(fd, &local, CTRL_MSG_SIZE);
+	if (ret < 0) {
+		goto init_fail;
+	}
+init_fail:
+	close(fd);
+	return ret;
+}
+
+int hwch_open(void *open_arg, size_t len, uint64_t *hwc)
+{
+	/* Allocate a hw_handle struct and return it */
+	struct comm_context *ctx;
+	int ret;
+	char tmp[NETL_DEVFILE_LEN];
+	struct hwch_params *chp = (struct hwch_params *)open_arg;
+
+	if (chp == NULL) {
+		return -EINVAL;
+	}
+	if (chp->ssid <= 0) {
+		return -EINVAL;
+	}
+	ctx = (struct comm_context *) malloc(sizeof(struct comm_context));
+	if (ctx == NULL) {
+		return -ENOMEM;
+	}
+	sprintf(tmp, "%s%d", NETL_FILE_PATH, chp->ssid);
+	ret = open(tmp, O_RDWR);
+	if (ret == -1) {
+		free(ctx);
+		return -errno;
+	}
+	ctx->data_fd = ret;
+	memcpy(&ctx->hwchp.chnp, chp, sizeof(struct hwch_params));
+	ctx->hwchp.type = MSG_CHANNEL_PARAM;
+	ctx->hwchp.msgid = 0;
+
+	ret = open(NETL_FILE_PATH, O_RDWR);
+	if (ret == -1) {
+		close(ctx->data_fd);
+		free(ctx);
+		return -errno;
+	}
+	write(ret, &ctx->hwchp, CTRL_MSG_SIZE);
+	close(ret);
+	*((uint64_t *)hwc) = (uint64_t)ctx;
+	return 0;
+}
+
+int hwch_close(uint64_t ctx_handle)
+{
+	struct comm_context *ctx = (struct comm_context *)ctx_handle;
+	close(ctx->data_fd);
+	free(ctx);
+	return 0;
+}
+
+int hwch_split_request(uint64_t ctx_handle, struct split_request* split_req)
+{
+	struct mgmt_msg *rsp;
+	struct mgmt_msg lch;
+	struct chunking_result *cr = (struct chunking_result *) malloc(sizeof(struct chunking_result));
+	struct comm_context *ctx = (struct comm_context *)ctx_handle;
+	struct timeval start, end;
+	long double d;
+
+	if (cr == NULL) {
+		return -ENOMEM;
+	}
+	INIT_LIST_HEAD(&cr->head);
+	populate_lib_chunk_header(split_req, &lch);
+	netl_msg_tx(ctx->data_fd, &lch, CTRL_MSG_SIZE);
+	gettimeofday(&start, NULL);
+	send_iovec_data(ctx, split_req);
+	gettimeofday(&end, NULL);
+	d = ((end.tv_sec * 1000000 + end.tv_usec) - (start.tv_sec * 1000000 + start.tv_usec));
+	d /= (long double)1000000;
+	printf("Total %ld bytes sent in %.8Lf seconds. Tx Speed %.8Lf MBps\n",
+			htonl(lch.lch.num_bytes), d, ((long double)htonl(lch.lch.num_bytes)/(d * 1024 * 1024)));
+
+	rsp = rx_process_message(ctx);
+	memcpy(&cr->rsp, &rsp->crh, sizeof(struct chunk_rsp_header));
+	/* response contains #bytes to read for chunks */
+	read_split_to_cis(ctx, cr);
+	((struct comm_context *)ctx_handle)->cr = cr;	/* store away */
+	return 0;
+}
+
+/* TBD */
+int hwch_split_response(uint64_t ctx_handle, struct split_request** split_req)
+{
+	struct comm_context *ctx = (struct comm_context *)ctx_handle;
+	struct list_head *elem;
+	/* Shoudl check for max_chunk_count TODO */
+	(*split_req)->result = ctx->cr;
+	ctx->cr = NULL;
+	return 0;
+}
+
+int hwch_free_chunking_result(struct chunking_result *cr)
+{
+	/* TODO */
+	return 0;
+}
+
+int new_request_id(void)
+{
+	return 0;
+}
+
+int populate_lib_chunk_header(struct split_request *sr, struct mgmt_msg *m)
+{
+	int i;
+	int tot = 0;
+
+	for(i = 0; i < sr->in_cnt; i++){
+		tot += sr->in_data[i].iov_len;
+	}
+	DPRINT("Total bytes in iov = %d\n", tot);
+	sr->req_id = new_request_id();
+	sr->result = NULL;
+
+	m->type = htonl(MSG_CHUNK_HEADER);
+	m->msgid = 0;
+	m->lch.request_id = htonl(sr->req_id);
+	m->lch.flags = htonl(sr->flags);
+	m->lch.max_chunk_count = htonl(sr->max_chunk_count);
+	m->lch.num_bytes = htonl(tot);
+	return 0;
+}
+
+#ifndef SERVER
+/* Dummy function
+ * Do not remove. Host will not compile */
+struct chunking_result *split(struct iovec* input_iovec, uint32_t n_iovec,
+		struct comm_context *ctx, struct init_params *ip,
+		struct hwch_params *hp)
+{
+       return NULL;
+}
+#endif
diff --git a/drivers/misc/netlogic/pcie-offload/include/mgmt.h b/drivers/misc/netlogic/pcie-offload/include/mgmt.h
new file mode 100644
index 0000000..6b2dffd
--- /dev/null
+++ b/drivers/misc/netlogic/pcie-offload/include/mgmt.h
@@ -0,0 +1,234 @@
+#ifndef MGMT_H
+#define MGMT_H
+
+#include <sys/types.h>
+#include <sys/socket.h>
+#include <arpa/inet.h>
+#include <stdio.h>
+#include <errno.h>
+#include <unistd.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+#include <stdlib.h>
+#include <string.h>
+#include <sys/uio.h>
+#include <pthread.h>
+#include <getopt.h>
+#include <sys/wait.h>
+#include <sys/time.h>
+#include <assert.h>
+
+#include "netl_pti_char.h"
+#include "chunking-api.h"
+
+#define NETL_DEVFILE_LEN	64
+#define NETL_FILE_PATH		"/dev/netl_char"
+#define NETL_MAX_CHANNELS	0x20
+#define NETL_CLIENT_PATH	"./client"
+#define NETL_SERVER_PATH	"./server"
+
+#ifdef DEBUG
+#define DPRINT(msg, args...)\
+do{\
+	fprintf(stderr, "%s()@%s:%d " msg, __func__,__FILE__, __LINE__, ##args);\
+} while(0)
+#else
+#define DPRINT(msg, args...)
+#endif
+
+#define container_of(ptr, type, member)\
+({\
+	type t1 = (type)(((char *)ptr) - (char *)&((type)0)->member);\
+	t1;\
+})
+
+#define SER_BUF_LEN	4096
+#define FP_SIZE		16
+#define MD5_SIZE	16
+
+struct list_head { struct list_head *next, *prev; };
+static inline void INIT_LIST_HEAD(struct list_head *list)
+{
+	list->next = list;
+	list->prev = list;
+}
+
+
+static inline void list_init(struct list_head *ptr)
+{
+	ptr->next = ptr->prev = ptr;
+}
+
+#define list_add_tail(head, n)\
+do {\
+	(n)->next = (head);\
+	(n)->prev = (head)->prev;\
+	(head)->prev = (n);\
+	(n)->prev->next = (n);\
+} while(0)
+
+#define list_remove_elem(head, r)\
+do{\
+	if((r) != head){\
+		(r)->next->prev = (r)->prev;\
+		(r)->prev->next = (r)->next;\
+		(r)->next = (r)->prev = NULL;\
+	}\
+}while(0)
+
+#define for_each_list(head, tmp)\
+	for((tmp) = (head)->next; (tmp) != (head) && (tmp) != NULL; (tmp) = (tmp)->next)
+
+enum mgmt_msg_type {
+	MSG_CHANNEL_PARAM	= 0X100,
+	MSG_INIT_PARAM,
+	MSG_STATUS_PARAM,
+	MSG_CHUNK_HEADER,
+	MSG_CHUNK_RESPONSE,
+};
+
+struct init_params {		/* parameters passed to init fn. */
+	uint32_t min_chunk_size;
+	uint32_t max_chunk_size;
+};
+
+struct hwch_params {
+	uint32_t ssid;
+	uint32_t rabin32_poly;
+	uint32_t rabin32_window_size;
+	uint32_t sig_size;
+	uint32_t md5_size;
+	uint32_t flags;
+} __attribute__((packed));
+
+struct response {
+	uint32_t status;
+} __attribute__((packed));
+
+struct lib_chunk_header {
+	uint32_t request_id;
+	uint32_t flags;
+	uint32_t max_chunk_count;
+	uint32_t num_bytes;
+}__attribute__((packed));
+
+struct chunk_rsp_header {	/* struct chunking result minus list_head */
+	int rem_chunk_count;
+	int chunk_count;
+	unsigned char md5sum[MD5_SIZE];
+}__attribute__((packed));
+
+struct chunk_info_data {
+	unsigned char fp[FP_SIZE];
+	unsigned char md5[MD5_SIZE];
+	int offset; /* offset from the first byte of the first iovec buffer */
+	int size;
+	int flags;
+}__attribute__((packed));
+
+struct chunk_info_elem {
+	struct chunk_info_data cid;
+	struct list_head head;
+}__attribute__((packed));
+
+struct chunking_result{
+	struct chunk_rsp_header rsp;
+	struct list_head head;
+};
+
+struct mgmt_msg {
+	/* This must have a predefined size
+	 * Eg. 64 bytes
+	 */
+	uint32_t type;
+	uint32_t msgid;
+	union {
+		/* This is the channel parameters */
+		struct hwch_params chnp;
+		/* Global parameters : for init only */
+		struct init_params glbp;
+		/* Response from a device/host */
+		struct response rsp;
+		/* lib chunk header */
+		struct lib_chunk_header lch;
+		/* chunk response */
+		struct chunk_rsp_header crh;
+		/* Padding so that size is always fixed : 64bytes */
+		uint32_t filler[14]; /* 16 - 2 (type, msgid) */
+	};
+}__attribute__((packed));
+
+struct comm_context {
+	int data_fd;
+	struct mgmt_msg hwchp;
+	struct mgmt_msg ip;
+	struct chunking_result *cr;
+}__attribute__((packed));
+
+typedef uint32_t (*nh_byteorder)(uint32_t);
+static inline void cp_mgmt_msg_BO(struct mgmt_msg *dst, struct mgmt_msg *src, nh_byteorder func)
+{
+	assert(func != NULL);
+	memset(dst, 0, CTRL_MSG_SIZE);
+	dst->msgid = func(src->msgid);
+	dst->type = func(src->type);
+	switch(dst->type){
+	case MSG_CHANNEL_PARAM:
+		dst->chnp.ssid = func(src->chnp.ssid);
+		dst->chnp.rabin32_poly = func(src->chnp.rabin32_poly);
+		dst->chnp.rabin32_window_size = func(src->chnp.rabin32_window_size);
+		dst->chnp.sig_size = func(src->chnp.sig_size);
+		dst->chnp.md5_size = func(src->chnp.md5_size);
+		dst->chnp.flags = func(src->chnp.md5_size);
+		break;
+	case MSG_INIT_PARAM:
+		dst->glbp.min_chunk_size = func(src->glbp.min_chunk_size);
+		dst->glbp.max_chunk_size = func(src->glbp.max_chunk_size);
+		break;
+	case MSG_STATUS_PARAM:
+		dst->rsp.status = func(src->rsp.status);
+		break;
+	case MSG_CHUNK_HEADER:
+		dst->lch.request_id = func(src->lch.request_id);
+		dst->lch.flags = func(src->lch.flags);
+		dst->lch.max_chunk_count = func(src->lch.max_chunk_count);
+		dst->lch.num_bytes = func(src->lch.num_bytes);
+		break;
+	case MSG_CHUNK_RESPONSE:
+		dst->crh.rem_chunk_count = func(src->crh.rem_chunk_count);
+		dst->crh.chunk_count = func(src->crh.chunk_count);
+		memcpy(dst->crh.md5sum, src->crh.md5sum, MD5_SIZE);
+		break;
+	default:
+		break;
+	}
+	return;
+}
+
+static inline void dump_mgmt_msg(struct mgmt_msg *src)
+{
+	DPRINT("src->msgid = %#x\n", src->msgid);
+	DPRINT("src->type = %#x\n", src->type);
+	switch(src->type){
+	case MSG_CHANNEL_PARAM:
+		DPRINT("src->chnp.ssid = %#x\n", src->chnp.ssid);
+		DPRINT("src->chnp.rabin32_poly = %#x\n", src->chnp.rabin32_poly);
+		DPRINT("src->chnp.rabin32_window_size = %#x\n", src->chnp.rabin32_window_size);
+		DPRINT("src->chnp.sig_size = %#x\n", src->chnp.sig_size);
+		DPRINT("src->chnp.md5_size = %#x\n", src->chnp.md5_size);
+		DPRINT("src->chnp.md5_size = %#x\n", src->chnp.md5_size);
+		break;
+	case MSG_INIT_PARAM:
+		DPRINT("src->glbp.min_chunk_size = %#x\n", src->glbp.min_chunk_size);
+		DPRINT("src->glbp.max_chunk_size = %#x\n", src->glbp.max_chunk_size);
+		break;
+	case MSG_STATUS_PARAM:
+		DPRINT("src->rsp.status = %#x\n", src->rsp.status);
+		break;
+	default:
+		break;
+	}
+	return;
+}
+
+#endif
diff --git a/drivers/misc/netlogic/pcie-offload/include/netl_pti.h b/drivers/misc/netlogic/pcie-offload/include/netl_pti.h
new file mode 100644
index 0000000..4e3d9f9
--- /dev/null
+++ b/drivers/misc/netlogic/pcie-offload/include/netl_pti.h
@@ -0,0 +1,331 @@
+#ifndef _NETL_PTI_H
+#define _NETL_PTI_H
+
+
+#define NETL_CONTROL_SUBSYS		0
+#define NETL_DEF_MAX_SUBSYS		8
+#define NETL_BUFSIZE			4096	/*__MUST__ BE PAGE_SIZE */
+#define	NETL_NR_DESC		0x400
+#define NETL_MAX_SUBSYS		0x4
+
+#ifdef __KERNEL__
+#include <linux/types.h>
+#include <linux/spinlock.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/pci.h>
+#include <linux/string.h>
+#include <linux/delay.h>
+#include <linux/sched.h>
+#include <linux/init.h>
+#include <linux/io.h>
+#include <linux/fs.h>
+#include <linux/cdev.h>
+#include <linux/interrupt.h>
+#include <asm/uaccess.h>
+#include <linux/delay.h>
+#include <linux/debugfs.h>
+#include <linux/seq_file.h>
+#include <linux/signal.h>
+#include <linux/poll.h>
+#include <asm/bitops.h>
+
+#ifdef NETL_PTI_DEVICE
+
+#ifndef CONFIG_NLM_XLP
+#define XLR_PCI_HOST_MODE 0x1
+#define XLR_PCI_DEV_MODE 0x2
+#else
+#include <asm/netlogic/iomap.h>
+#include <asm/netlogic/cpumask.h>
+#include <asm/netlogic/msgring.h>
+#include <asm/netlogic/xlp_hal_pic.h>
+#include <hal/nlm_hal.h>
+#include <hal/nlm_hal_macros.h>
+#include <hal/nlm_hal_fmn.h>
+#include <hal/nlm_hal_xlp_dev.h>
+#include <nlm_pcie.h>
+#endif	// CONFIG_NLM_XLP
+
+#endif	// NETL_PTI_DEVICE
+
+#define PTI_ADDR_OFFSET (unsigned long)0x00100000
+
+#ifndef CONFIG_NLM_XLP	// kludge, replace with fdebug
+#define DPRINTK(level,fmt,args...)\
+do{\
+	printk(level "%s()@%s:%d " fmt,__func__, __FILE__, __LINE__, ##args);\
+}while(0)
+#endif
+
+#ifdef NETL_PTI_DEBUG
+#define PDEBUG(fmt, args...) DPRINTK(KERN_DEBUG, fmt, ##args)
+#else
+#define PDEBUG(fmt, args...)
+#endif
+
+#define PWARN(fmt, args...)\
+	DPRINTK(KERN_WARNING, fmt, ##args)
+#define PERROR(fmt, args...)\
+	DPRINTK(KERN_ERR, fmt, ##args)
+
+#define PTI_DEV_IRQ		16
+#define DESC_DMA_READY		(0x80000000)
+#define is_dma_set(p)          ((p) & DESC_DMA_READY)
+#define NETL_HOST_SIGNATURE		0x4e45544c	/* NETL */
+#define NETL_DEV_SIGNATURE		0x4e444556	/* NDEV */
+
+struct descriptor;
+struct pti_subsys;
+
+#define _pti_pci
+struct _desc_pci{
+	/* descriptor will have two parts
+	 * 1. The portion on the PCI mapped space (This structure)
+	 * 2. The part on the virtual address space (struct descriptor)
+	 *
+	 * PCI access should take care of endianness
+	 * Virtual address access should not take care endianness
+	 */
+	_pti_pci volatile u64 dma_addr;
+	_pti_pci volatile u32 dma_len;
+	_pti_pci volatile u32 dma_stat;
+} __attribute__((packed));
+
+struct descriptor{
+	volatile struct _desc_pci *pci_desc;	/* must be casted to offset*/
+	unsigned long buf;			/* buffer page */
+	u32 len;
+	u64 buf_flag;
+	struct desc_ring *pring;
+	int idx;
+} __attribute__((packed));
+
+/*
+ * Descriptor ring structure
+ * Has start and end; but they are currently not requried as we have fixed
+ * length structure. Nevertheless, we will implement it for sake of abstraction
+ */
+
+#define DESC_RING_TYPE_TX_DEV	0x0
+#define DESC_RING_TYPE_RX_DEV	0x1
+#define DESC_RING_TYPE_RX_HOST	0x0
+#define DESC_RING_TYPE_TX_HOST	0x1
+struct desc_ring{
+	_pti_pci volatile u32 *head;	/* ring head, updated by Host Tx */
+	_pti_pci volatile u32 *tail;	/* ring tail, updated by dev Rx */
+	struct descriptor *pdesc;	/* Should be kzalloc-ed */
+	u32 start;			/* In tx int, start freeing from here*/
+	u32 end;			/* unused */
+	u32 nfree;			/* #of free descriptors */
+	struct pti_subsys *ss;
+	spinlock_t rng_lock;
+} __attribute__((packed));
+
+#define SUBSYS_FLAGS_TX			(0x1 << 0)
+#define SUBSYS_FLAGS_TX_DISABLED	(0x1 << 1)
+#define SUBSYS_FLAGS_TX_PENDING		(0x1 << 2)
+#define SUBSYS_FLAGS_RX			(0x1 << 8)
+#define SUBSYS_FLAGS_RX_DISABLED	(0x1 << 9)
+#define SUBSYS_FLAGS_RX_PENDING		(0x1 << 10)
+#define SUBSYS_FLAGS_ORPHANED		(0x1 << 16)
+struct pti_subsys{
+	u32 id;
+	void *priv_data;
+	struct hlist_node snode;
+	struct desc_ring *rxring;
+	struct desc_ring *txring;
+	unsigned long flags;
+	spinlock_t slock;
+	struct handshake_struct *hs;
+	//struct tasklet_struct stask;
+	wait_queue_head_t rdq, wrq;	/* read and write queue heads */
+	spinlock_t rxlock;		/* for rx buffer struct */
+	int rnum;			/* # received buffers in rx_head */
+	struct list_head rx_head;		/* receive buffers' list */
+	struct dentry *dbgf;
+	u32 msgid;
+	u64 rxbytes, txbytes;
+};
+
+struct rx_struct{
+	struct list_head rhead;
+	struct kvec iov;
+	unsigned long soffset;
+};
+
+/* calculate size of subsystem here */
+#define DESC_ARRAY_SIZE(_nr) ((sizeof(u32) * 2) + sizeof(struct _desc_pci) * (_nr));
+
+#define INCR_CEIL(x)\
+({typeof(x) _x = (x);\
+ _x = ((_x + 1) % NETL_NR_DESC);\
+ _x;\
+})
+
+// #define NETL_MAX_MSG_RETRIES		8
+enum hshake_cmd{
+	HS_DEV_READY_TXRX,
+	HS_STATUS_NOT_DONE,
+	HS_STATUS_PROGRESSING,
+	HS_STATUS_COMPLETE,
+	HS_DEV_EXITING,
+};
+#define NETL_HS_MAX_PARAMS	5
+struct hs_params{
+	u32 max_subsys;
+	u32 flags;
+	u32 nr_desc;
+};
+
+struct handshake_struct{
+	volatile u32 *rcmd;	/* read commands from here */
+	volatile u32 *wstat;	/* write status here */
+	volatile u32 *wcmd;	/* read commands from here */
+	volatile u32 *rstat;	/* write status here */
+	volatile void *start;			/* PCI space start */
+	struct hs_params params;	/* host<->device params */
+	int status;
+	wait_queue_head_t openq;	/* blocking open wait queue */
+	spinlock_t hs_lock;
+	struct cdev hs_cdev;
+	struct dentry *dbg_root;
+	struct semaphore hhsem;
+	struct hlist_head *hhash;/* Array of NETL_SUBSYS_HASH hlist_heads */
+	volatile u64 rxpending;		/* bits corresponding to rxpending subsys */
+	volatile u64 txpending;		/* bits corresponding to txpending subsys */
+	struct delayed_work hs_work;	/* work queue for polling handshake */
+	struct delayed_work temp_int_work;	/* remove later */
+	struct tasklet_struct hs_stask;
+	unsigned long pci_len;		/* PCI space length */
+#ifdef NETL_PTI_HOST
+	int hsid;
+	struct netl_device *dev_list;	/* Back ref->list of devs present */
+	struct list_head dhead;		/* linked to dev_list */
+	struct pci_dev *p_dev;
+	unsigned int msi_flag;		/* MSI, MSIX or INTX */
+#endif
+};
+
+/* Offsets from top of the memory + PTI_ADDR_OFFSET */
+#define HCMD0_OFFSET		0x00
+#define HSTAT0_OFFSET		0X04
+#define END_HCMDSTAT		0X08
+#define DCMD0_OFFSET		0X08
+#define DSTAT0_OFFSET		0X0C
+#define END_DCMDSTAT		0x10
+#define NETL_DBG_ROOT		"netl"
+#define NETL_DBG_FNAME_LEN	0x10
+#define NETL_SUBSYS_HASH	512	/* Number of hash buckets */
+#define NETL_MSG_MATCH		0x25
+#define NETL_MSG_ANY		0x26
+#define NETL_MSGID_START	0xff
+#define	NETL_MSI_ENABLE		0xF1
+#define NETL_MSIX_ENABLE	0xF2
+#define NETL_INTX_ENABLE	0xF0
+
+/* One command structure has, u32 + u32 + u32 + u32 elements
+ * Do not use sizeof() unless __packed.
+ */
+#define pti_pci_read8(ptr)\
+({\
+	ioread8(ptr);\
+})
+
+#define pti_pci_read16(ptr)\
+({\
+	ioread16(ptr);\
+})
+
+#define pti_pci_read32(ptr)\
+({\
+	ioread32(ptr);\
+})
+
+#define pti_pci_read64(ptr)\
+({\
+	readq(ptr);\
+})
+
+#define pti_pci_write8(v,p)\
+do{\
+	iowrite(v,p);\
+}while(0)
+
+#define pti_pci_write16(v,p)\
+do{\
+	iowrite16(v,p);\
+}while(0)
+
+#define pti_pci_write32(v,p)\
+do{\
+	iowrite32(v,p);\
+}while(0)
+
+#define pti_pci_write64(v,p)\
+do{\
+	writeq(v, p);\
+}while(0)
+
+/* this is used by both host and device */
+
+#define NLMXLP_BDF2OFFSET(bus, dev, func)  (((bus) << 20) | ((dev) << 15) | ((func) << 12))
+#ifdef CONFIG_NLM_XLP
+/* This should go to iomap.h later // TODO
+ */
+#define DEFAULT_NLMXLP_IO_BASE DEFAULT_NETLOGIC_IO_BASE
+#define NLMXLP_CONF_SIZE	0x40000ULL
+#define NLMXLP_PHYS_START_PCI_MEM		0x7e000000
+/* From U-Boot, include/asm-mips64/arch-xlp8xx/cpu.h */
+/* -------------------------------- */
+/* Interface   | Bus | Dev | Func   */
+/* -------------------------------- */
+#define	NLMXLP_BRIDGE_BASE(node)\
+	(DEFAULT_NLMXLP_IO_BASE + ((node) * NLMXLP_CONF_SIZE) +\
+	 NLMXLP_BDF2OFFSET(0, 0, 0))
+#define	NLMXLP_PIC_BASE(node)\
+	(DEFAULT_NLMXLP_IO_BASE + ((node) * NLMXLP_CONF_SIZE) +\
+	NLMXLP_BDF2OFFSET(0, 0, 4))
+#define	NLMXLP_UART0_BASE(node)\
+	(DEFAULT_NLMXLP_IO_BASE + ((node) * NLMXLP_CONF_SIZE) +\
+	NLMXLP_BDF2OFFSET(0, 6, 0))
+#define	NLMXLP_UART1_BASE(node)\
+	(DEFAULT_NLMXLP_IO_BASE + ((node) * NLMXLP_CONF_SIZE) +\
+	NLMXLP_BDF2OFFSET(0, 6, 1))
+#define	NLMXLP_I2C0_BASE(node)\
+	(DEFAULT_NLMXLP_IO_BASE + ((node) * NLMXLP_CONF_SIZE) +\
+	NLMXLP_BDF2OFFSET(0, 6, 2))
+#define	NLMXLP_I2C1_BASE(node)\
+	(DEFAULT_NLMXLP_IO_BASE + ((node) * NLMXLP_CONF_SIZE) +\
+	NLMXLP_BDF2OFFSET(0, 6, 3))
+#define	NLMXLP_GPIO_BASE(node)\
+	(DEFAULT_NLMXLP_IO_BASE + ((node) * NLMXLP_CONF_SIZE) +\
+	NLMXLP_BDF2OFFSET(0, 6, 4))
+#define	NLMXLP_SYS_BASE(node)\
+	(DEFAULT_NLMXLP_IO_BASE + ((node) * NLMXLP_CONF_SIZE) +\
+	NLMXLP_BDF2OFFSET(0, 6, 5))
+#define	NLMXLP_JTAG_BASE(node)\
+	(DEFAULT_NLMXLP_IO_BASE + ((node) * NLMXLP_CONF_SIZE) +\
+	NLMXLP_BDF2OFFSET(0, 6, 6))
+#define	NLMXLP_GBU_BASE(node)\
+	(DEFAULT_NLMXLP_IO_BASE + ((node) * NLMXLP_CONF_SIZE) +\
+	NLMXLP_BDF2OFFSET(0, 7, 0))
+#define	NLMXLP_NOR_BASE(node)\
+	(DEFAULT_NLMXLP_IO_BASE + ((node) * NLMXLP_CONF_SIZE) +\
+	NLMXLP_BDF2OFFSET(0, 7, 0))
+#define	NLMXLP_NAND_BASE(node)\
+	(DEFAULT_NLMXLP_IO_BASE + ((node) * NLMXLP_CONF_SIZE) +\
+	NLMXLP_BDF2OFFSET(0, 7, 1))
+#define	NLMXLP_SPI_BASE(node)\
+	(DEFAULT_NLMXLP_IO_BASE + ((node) * NLMXLP_CONF_SIZE) +\
+	NLMXLP_BDF2OFFSET(0, 7, 2))
+#define	NLMXLP_MMC_BASE(node)\
+	(DEFAULT_NLMXLP_IO_BASE + ((node) * NLMXLP_CONF_SIZE) +\
+	NLMXLP_BDF2OFFSET(0, 7, 3))
+/* -------------------------------- */
+#endif		/* CONFIG_NLM_XLP */
+
+#endif		/* __KERNEL__ */
+
+#endif		/* NETL_PTI_H */
+
diff --git a/drivers/misc/netlogic/pcie-offload/include/netl_pti_char.c b/drivers/misc/netlogic/pcie-offload/include/netl_pti_char.c
new file mode 100644
index 0000000..66db6af
--- /dev/null
+++ b/drivers/misc/netlogic/pcie-offload/include/netl_pti_char.c
@@ -0,0 +1,489 @@
+/*
+ * Implementation of a char device abstraction over netl_pti
+ */
+#include "../include/netl_pti.h"
+#include "../include/netl_pti_char.h"
+
+#define NETL_CHAR_MAJOR		0x0	/* Let kernel decide */
+#define NETL_NUM_DEVS		0x8	/* 8 subsystems */
+
+static int major = 0;		/* default to dynamic major */
+module_param(major, int, 0);
+MODULE_PARM_DESC(major, "Major device number");
+
+int netl_char_open(struct inode *inode, struct file *filp);
+int netl_char_release(struct inode *inode, struct file *filp);
+loff_t netl_char_llseek(struct file *filp, loff_t offset, int orig);
+ssize_t netl_char_read(struct file *filp, char __user *buf, size_t len, loff_t *off);
+ssize_t netl_char_write(struct file *filp, const char __user *buf, size_t len, loff_t *off);
+ssize_t netl_char_aio_read(struct kiocb *kiocb, const struct iovec *iovec, unsigned long len, loff_t off);
+ssize_t netl_char_aio_write(struct kiocb *kiocb, const struct iovec *iovec, unsigned long len, loff_t off);
+void subsys_poll_tx(struct pti_subsys *);
+unsigned int netl_char_poll(struct file * filp, struct poll_table_struct *poll_table);
+long netl_char_unlocked_ioctl(struct file * filp, unsigned int cmd, unsigned long arg);
+long netl_char_compat_ioctl(struct file * filp, unsigned int cmd, unsigned long arg);
+int netl_char_mmap(struct file * filp, struct vm_area_struct *vma);
+int netl_char_flush(struct file *filp, fl_owner_t id);
+int netl_char_close(struct inode *inode, struct file *filp);
+void netl_unregister_chrdev(struct handshake_struct *hs);
+struct rx_struct *wait_control_response(struct pti_subsys *ss, u32 msgid, int match);
+
+extern struct pti_subsys *get_ss_from_idx(struct handshake_struct *hs, int idx);
+extern int hs_minor_to_idx(struct handshake_struct *hs, int minor);
+extern int get_packet_from_rxlist(struct pti_subsys *ss, struct rx_struct **ret);
+extern u32 netl_pti_max_subsys;
+extern void dump_desc_ring(struct seq_file *seq, struct desc_ring *ring, int dump_all);
+extern struct pti_subsys *init_one_subsys(struct handshake_struct *, int);
+extern void stop_subsystem(struct pti_subsys *ss);
+extern void dealloc_this_subsystem(struct pti_subsys *ss);
+extern int init_device(struct handshake_struct *hs, int blocking);
+extern void dump_raw_control_message(void *p);
+extern int copy_nbytes_from_rxlist(struct pti_subsys *ss, void *buf, ssize_t n, struct list_head *free_list);
+extern int start_host_control_subsys(struct handshake_struct *hs, struct pti_subsys **ctrl_ss);
+
+struct rx_struct *__dequeue_msg(struct pti_subsys *ss, u32 msgid, int match);
+#ifdef NETL_PTI_HOST
+int send_one_packet_to_device(struct pti_subsys *ss, unsigned long page, u32 len);
+#else
+int send_one_packet_to_host(struct pti_subsys *ss, unsigned long page, u32 len);
+#endif
+
+typedef int (*send_func)(struct pti_subsys *, unsigned long, u32);
+
+const struct file_operations netl_char = {
+	.owner = THIS_MODULE,
+	.open = netl_char_open,
+	.read = netl_char_read,
+	.write = netl_char_write,
+	.llseek = no_llseek,
+	.aio_read = netl_char_aio_read,
+	.aio_write = netl_char_aio_write,
+	.poll = netl_char_poll,
+	.unlocked_ioctl = netl_char_unlocked_ioctl,
+	.compat_ioctl = netl_char_compat_ioctl,
+	.mmap = netl_char_mmap,
+	.flush = netl_char_flush,
+	.release = netl_char_close,
+};
+
+int netl_char_open(struct inode *inode, struct file *filp)
+{
+	__label__ open_lbl;
+	struct handshake_struct *hs = container_of(inode->i_cdev, struct handshake_struct, hs_cdev);
+	int num_minor = iminor(inode);
+	struct pti_subsys *ss = NULL;
+
+	if(hs->status == HS_DEV_READY_TXRX){
+		goto open_lbl;
+	}
+	if(filp->f_flags & O_NONBLOCK){
+		return -EAGAIN;
+	}else{
+		PDEBUG("Waiting for READY_TXRX\n");
+		wait_event_interruptible(hs->openq,
+				(hs->status == HS_DEV_READY_TXRX));
+	}
+open_lbl:
+	/* check if subsystem is already open */
+	ss = get_ss_from_idx(hs, num_minor);
+	if(ss == NULL){
+		ss = init_one_subsys(hs, num_minor);
+		if(ss == NULL){
+			return -EFAULT;
+		}
+	}
+	PDEBUG("num_minor = %d, ss = %p\n", num_minor, ss);
+	filp->private_data = ss;
+	return nonseekable_open(inode, filp);	/* Do not MOVE this one */
+}
+
+int netl_char_release(struct inode *inode, struct file *filp)
+{
+	return 0;
+}
+
+/*
+ * read implementation for netl char device
+ *
+ * Bug? Does not take care of failed reads (in between *). Would cause data loss?
+ *
+ */
+ssize_t netl_char_read(struct file *filp, char __user *ubuf, size_t ulen, loff_t *off)
+{
+	struct pti_subsys *ss = filp->private_data;
+	void *kbuf = (void *)get_zeroed_page(GFP_KERNEL);
+	void *ustart = ubuf;
+	unsigned int totread = 0, this = 0;
+	unsigned int tocopy = 0;
+	int ret = 0;
+
+	if(ss->hs->status != HS_DEV_READY_TXRX){
+		return -EIO;
+	}
+	if(kbuf == NULL){
+		return -ENOMEM;
+	}
+	//PDEBUG("Reading buffers of %#x\n", ss->id);
+
+	while((ulen - totread) > 0){
+		tocopy = ulen - totread;
+		tocopy = (PAGE_SIZE > tocopy) ? tocopy: PAGE_SIZE;
+		this = copy_nbytes_from_rxlist(ss, kbuf, tocopy, NULL);
+		//PDEBUG("read %#x bytes\n", this);
+		if(this > 0){
+			/* we read something */
+			if(copy_to_user(ustart, kbuf, this)){
+				ret = -EFAULT;
+				goto read_some;
+			}
+			totread += this;
+			ustart += this;
+		}else{
+		/* could not read anything this time, for list could be
+		 * empty or we requested 0 bytes */
+			if(filp->f_flags & O_NONBLOCK){
+				/* Send data, if any, we read so far*/
+				goto read_some;
+			}else{
+				/* Block till we have data */
+//				PDEBUG("Waiting ss = %p, (rnum %#x)\n", ss, ss->rnum);
+				if(wait_event_interruptible(ss->rdq, (ss->rnum != 0))){
+					ret = -ERESTARTSYS;
+					PDEBUG("Signal delivery\n");
+					goto read_some;
+				}
+//				PDEBUG("Wait OVER ss = %p (rnum %#x)\n", ss, ss->rnum);
+				/* Wait over, retry reading */
+				continue;
+			}
+		}
+	}
+read_some:
+	//PDEBUG("totread = %#x\n", totread);
+	*off += totread;
+	free_page((unsigned long)kbuf);
+	return((totread != 0) ? totread : ret);
+}
+
+int add_to_rxlist(struct pti_subsys *, volatile struct kvec *);
+int send_on_loopback(struct pti_subsys *ss, unsigned long page, u32 len)
+{
+	volatile struct kvec vec;
+
+	/* No need to manipulate nfree on tx or rx */
+	vec.iov_len = len;
+	vec.iov_base = (void *)page;
+	ss->rxbytes += len;
+	ss->txbytes += len;
+	add_to_rxlist(ss, &vec);
+	return 0;
+}
+/*
+ * Write function
+ * Writes to the device using send_one_packet_to_device
+ *
+ */
+ssize_t netl_char_write(struct file *filp, const char __user *buf, size_t len, loff_t *off)
+{
+	unsigned long kpage = 0;
+	struct pti_subsys *ss = filp->private_data;
+	int ret = 0;
+	int len2s = 0, sent = 0;
+	int written = 0;
+#ifdef NETL_PTI_HOST
+	send_func send_packet = send_one_packet_to_device;
+#else
+	send_func send_packet = send_one_packet_to_host;
+#endif
+#ifdef NETL_LOOPBACK
+	send_packet = send_on_loopback;
+#endif
+	if(ss->hs->status != HS_DEV_READY_TXRX){
+		return -EIO;
+	}
+	/* split up the incoming packet
+	 * write(2) should succeed even if only one byte is written */
+	while(sent != len){
+		len2s = ((len - sent) > NETL_BUFSIZE) ? NETL_BUFSIZE: (len - sent);
+		//PDEBUG("len2s = %#x\n", len2s);
+		kpage = get_zeroed_page(GFP_KERNEL);
+		if(!kpage){
+			ret = -ENOMEM;
+			break;
+		}
+		if(copy_from_user((void *)kpage, (buf + sent), len2s)){
+			PDEBUG("copy from user failed\n");
+			free_page(kpage);
+			ret = -EFAULT;
+			break;
+		}
+		// PDEBUG("Sending packet on ss->id %#x\n", ss->id);
+		ret = send_packet(ss, kpage, len2s);
+		if(ret == 0){
+		//	PDEBUG("Sent one packet\n");
+			*off += len2s;
+			written = 1;
+			sent += len2s;
+			continue;
+		}
+		if(ret == EAGAIN){
+			free_page(kpage);
+			/* if noblock is set, return immediately */
+			if(filp->f_flags & O_NONBLOCK){
+				PDEBUG("NO space left\n");
+				ret = -EAGAIN;
+				break;
+			}
+			/* Else add to ss->wrq and wait */
+		//	PDEBUG("Waiting on queue, txring->nfree = %#x\n", ss->txring->nfree);
+			if(wait_event_interruptible(ss->wrq, (ss->txring->nfree != 0))){
+				ret = -ERESTARTSYS;
+				break;
+			}
+		}else{
+			PDEBUG("Sent fail\n");
+			free_page(kpage);
+			break;
+		}
+	}
+	//PDEBUG("sent bytes %#x\n", sent);
+	if(written){
+		return sent;
+	}else{
+		 return ret;
+	}
+}
+
+ssize_t netl_char_aio_read(struct kiocb *kiocb, const struct iovec *iovec, unsigned long len, loff_t off)
+{
+	return 0;
+}
+
+ssize_t netl_char_aio_write(struct kiocb *kiocb, const struct iovec *iovec, unsigned long len, loff_t off)
+{
+	return 0;
+}
+
+unsigned int netl_char_poll(struct file * filp, struct poll_table_struct *wait)
+{
+	struct pti_subsys *ss = (struct pti_subsys *)filp->private_data;
+	unsigned int mask = 0;
+	unsigned long flags;
+	int empty = 0;
+
+	PDEBUG("ss = %p\n", ss);
+	if(ss == NULL){		/* remove this check later TODO */
+		PDEBUG("No SS\n");
+		return 0;
+	}
+	PDEBUG("ss->id = %#x\n", ss->id);
+	/* I guess this call is applicable to control and data subsystems
+	 * So, not explicitly checking for ss->id == 0 */
+	poll_wait(filp, &ss->rdq, wait);
+	poll_wait(filp, &ss->wrq, wait);
+	spin_lock_irqsave(&ss->rxlock, flags);
+	empty = list_empty(&ss->rx_head);
+	spin_unlock_irqrestore(&ss->rxlock, flags);
+	if(!empty){
+		mask |= (POLLIN | POLLRDNORM);	/* Some data to read */
+	}
+	if(ss->txring->nfree != 0){
+		mask |= (POLLOUT | POLLWRNORM);	/* #free slot to write >= 1*/
+	}
+	PDEBUG("End of poll func\n");
+	return mask;
+}
+
+#ifdef NETL_PTI_HOST
+void host_flush_dma_pages(struct pti_subsys *ss)
+{
+	/* At this time, all rxdma pages are marked dma_stat = 0
+	 * Free all rxdma pages and any Tx dma pages */
+	struct desc_ring *rxr;
+	unsigned long fl_rng;
+	int loops;
+	volatile dma_addr_t dm_old;
+	u32 dma_stat;
+
+	subsys_poll_tx(ss);	/* Tx complete takes care of all pending tx */
+
+	rxr = ss->rxring;
+	spin_lock_irqsave(&rxr->rng_lock, fl_rng);
+	loops = ss->hs->params.nr_desc;
+	while(loops--){
+		dma_stat = pti_pci_read32((void *)&rxr->pdesc[loops].pci_desc->dma_stat);
+		if(is_dma_set(dma_stat)){
+			PERROR("Feeing page (offset %#x) under DMA\n", loops);
+			// dump_desc_ring(NULL, rxr, 1);
+			spin_unlock_irqrestore(&rxr->rng_lock, fl_rng);
+			return;
+		}
+		/* free up the mapping */
+		dm_old = pti_pci_read64((void *)&rxr->pdesc[loops].pci_desc->dma_addr);
+		pci_unmap_single(ss->hs->p_dev, dm_old, NETL_BUFSIZE, DMA_FROM_DEVICE);
+		free_page((unsigned long)rxr->pdesc[loops].buf);
+	}
+	spin_unlock_irqrestore(&rxr->rng_lock, fl_rng);
+	return;
+}
+#endif
+
+long netl_char_unlocked_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
+{
+	struct pti_subsys *ss = NULL;
+	struct inode *inode = filp->f_path.dentry->d_inode;
+	struct handshake_struct *hs = container_of(inode->i_cdev, struct handshake_struct, hs_cdev);
+
+	//PDEBUG("inode = 0x%p, hs = 0x%p\n", inode, hs, in_atomic());
+	if(down_interruptible(&hs->hhsem) != 0){
+		return -ERESTARTSYS;
+	}
+	switch(cmd){
+	case NETL_SS_SELECT:
+		PDEBUG("arg = %#x\n", (unsigned int)arg);
+		if(arg == 0){
+			up(&hs->hhsem);
+			return -EPERM;
+		}
+		ss = get_ss_from_idx(hs, (int)arg);
+		if(ss == NULL){
+			up(&hs->hhsem);
+			return -ENOENT;
+		}
+		filp->private_data = ss;
+		up(&hs->hhsem);
+		return 0;
+	case NETL_SS_PDATA_STORE:{
+		struct pti_subsys *ss;
+		void *ptr;
+
+		if (hs->status != HS_DEV_READY_TXRX) {
+			up(&hs->hhsem);
+			return -EAGAIN;
+		}
+		ptr = kmalloc(GFP_KERNEL, CTRL_MSG_SIZE);
+		if (!ptr) {
+			up(&hs->hhsem);
+			return -ENOMEM;
+		}
+		if (copy_from_user(ptr, (void *)arg, CTRL_MSG_SIZE)) {
+			kfree(ptr);
+			up(&hs->hhsem);
+			return -EFAULT;
+		}
+		ss = filp->private_data;
+		ss->priv_data = ptr;
+		up(&hs->hhsem);
+		return 0;
+	}
+	case NETL_SS_PDATA_FETCH:{
+		struct pti_subsys *ss;
+
+		if (hs->status != HS_DEV_READY_TXRX) {
+			up(&hs->hhsem);
+			return -EAGAIN;
+		}
+		ss = filp->private_data;
+		if (!ss->priv_data) {
+			up(&hs->hhsem);
+			return -ENOENT;
+		}
+		if (copy_to_user((void *)arg, ss->priv_data, CTRL_MSG_SIZE)) {
+			up(&hs->hhsem);
+			return -EFAULT;
+		}
+		up(&hs->hhsem);
+		return 0;
+	}
+	default:
+		up(&hs->hhsem);
+		return -ENOTTY;
+	}/* switch */
+	return 0;
+}
+
+long netl_char_compat_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
+{
+	//PDEBUG("cmd = %x, arg = %lx\n", cmd, arg);
+	return netl_char_unlocked_ioctl(filp, cmd, arg);
+}
+int netl_char_mmap(struct file * filp, struct vm_area_struct *vma)
+{
+	return 0;
+}
+
+int netl_char_flush(struct file *filp, fl_owner_t id)
+{
+	return 0;
+}
+
+int netl_char_close(struct inode *inode, struct file *filp)
+{
+	return 0;
+}
+
+
+int netl_register_chrdev(struct handshake_struct *hs)
+{
+	dev_t devid;
+	int rc;
+	int max_subsys = hs->params.max_subsys;
+
+	if(major){
+		devid = MKDEV(major, 0);
+		rc = register_chrdev_region(devid, max_subsys, "netl_char");
+	}else{
+		rc = alloc_chrdev_region(&devid, 0, max_subsys, "netl_char");
+		major = MAJOR(devid);
+	}
+	PDEBUG("Major number = %#x\n", major);
+	if(rc < 0){
+		PDEBUG("NetL PCI chrdev_region err: %d\n", rc);
+		return -1;
+	}
+	hs->hs_cdev.owner = THIS_MODULE;
+	cdev_init(&hs->hs_cdev, &netl_char);
+	rc = cdev_add(&hs->hs_cdev, devid, max_subsys);
+	if(rc != 0){
+		PDEBUG("Failed to register character device\n");
+		return rc;
+	}
+	return 0;
+}
+
+void netl_unregister_chrdev(struct handshake_struct *hs)
+{
+	dev_t devid = MKDEV(major, 0);
+	int max_subsys = hs->params.max_subsys;
+
+	cdev_del(&hs->hs_cdev);
+	unregister_chrdev_region(devid, max_subsys);
+	return;
+}
+
+
+/*
+ * Creates a control message block
+ *
+ * @pg : page (buffer)
+ * @ss : subsystem ptr
+ * @len : length (return value)
+ *
+ * __ORDER OF COPYING IS IMPORTANT. CROSS CHECK WITH ``parse_control_message``
+ * returns message id of the control message
+ */
+#define COPY_CTRL_LE32(ptr, param, tmp)\
+do{\
+	tmp = cpu_to_le32(param);\
+	memcpy(ptr, &tmp, sizeof(u32));\
+	ptr += sizeof(u32);\
+}while(0)
+
+
+#define COPY_TO_CTRL_LE32(ptr)\
+({\
+	u32 tmp = __le32_to_cpup((u32 *)ptr);\
+	ptr += sizeof(u32);\
+	tmp;\
+})
+
diff --git a/drivers/misc/netlogic/pcie-offload/include/netl_pti_char.h b/drivers/misc/netlogic/pcie-offload/include/netl_pti_char.h
new file mode 100644
index 0000000..8c14eb7
--- /dev/null
+++ b/drivers/misc/netlogic/pcie-offload/include/netl_pti_char.h
@@ -0,0 +1,24 @@
+#ifndef NETL_PTI_CHAR_H
+#define NETL_PTI_CHAR_H
+#ifdef __KERNEL__
+#include <linux/ioctl.h>
+#include <linux/signal.h>
+#else
+#include <sys/ioctl.h>
+#include <sys/signal.h>
+#endif
+#define NETL_PTI_IOCTL_MAGIC	0xf3
+#define CTRL_MSG_SIZE		0x40
+/*
+ * select : gets an idx, select the corresponding ss into the filp->private
+ *	if no ss exists, filp->private is unset and returns EINVAL
+ * setup : creates a ss corresponding to the arg. If already exists, returns
+ *	error
+ * setservapp	: sets up the server applicaiton on the device.
+ */
+#define _NIO(m,n)	(((m) << 8)|(n))
+#define NETL_SS_SELECT		_NIO(NETL_PTI_IOCTL_MAGIC, 0x10)
+#define NETL_SS_SETUP		_NIO(NETL_PTI_IOCTL_MAGIC, 0x11)
+#define NETL_SS_PDATA_STORE	_NIO(NETL_PTI_IOCTL_MAGIC, 0x12)
+#define NETL_SS_PDATA_FETCH	_NIO(NETL_PTI_IOCTL_MAGIC, 0x13)
+#endif /* NETL_PTI_CHAR_H */
diff --git a/drivers/misc/netlogic/pcie-offload/include/netl_pti_common.c b/drivers/misc/netlogic/pcie-offload/include/netl_pti_common.c
new file mode 100644
index 0000000..f03ccfa
--- /dev/null
+++ b/drivers/misc/netlogic/pcie-offload/include/netl_pti_common.c
@@ -0,0 +1,716 @@
+#include "../include/netl_pti.h"
+
+/* Static functions */
+static volatile u8 *ss_get_desc_offset(struct pti_subsys *ss, int idx, int is_rx);
+static volatile u8 *ss_get_ringcmd_offset(struct pti_subsys *ss, int is_rx);
+struct pti_subsys *init_one_subsys(struct handshake_struct *hs, int id);
+static void dealloc_desc_ring(struct desc_ring *ring, int type);
+static int init_desc_ring(struct pti_subsys *ss, struct desc_ring *ring, int type);
+int add_to_rxlist(struct pti_subsys *ss, volatile struct kvec *iov);
+int get_packet_from_rxlist(struct pti_subsys *ss, struct rx_struct **ret);
+static int netl_dbg_open(struct inode *inode, struct file *filp);
+int netl_pti_debugfs_init(struct handshake_struct *hs);
+int init_handshake_struct(struct handshake_struct *hs);
+
+/* functions called from other files */
+// int init_subsystems(struct handshake_struct *hs);
+void dealloc_all_subsys(struct handshake_struct *hs);
+void dealloc_this_subsystem(struct pti_subsys *ss);
+void copy_desc_pci(volatile struct _desc_pci *new, volatile struct _desc_pci *orig);
+void hs_clear_cmd(struct handshake_struct *hs, u32 sval);
+
+void dump_hs(struct seq_file *seq, struct handshake_struct *hs);
+void dump_subsystem(struct seq_file *seq, struct pti_subsys *ss, int txr, int rxr);
+void dump_desc_ring(struct seq_file *seq, struct desc_ring *ring, int dump_all);
+void dump_descriptor(struct seq_file *seq, struct descriptor *ptr);
+int copy_nbytes_from_rxlist(struct pti_subsys *ss, void *buf, ssize_t n, struct list_head *free_list);
+
+/* extern functions declared in dev/host speicifc files */
+extern void subsys_poll(unsigned long);
+extern struct workqueue_struct *ss_workq;
+extern void set_hs_status(struct handshake_struct *hs, enum hshake_cmd cmd);
+int get_packet_from_rxlist(struct pti_subsys *ss, struct rx_struct **ret);
+
+/* Dbg file support */
+static struct file_operations netl_dbg_fops = {
+	.owner = THIS_MODULE,
+	.open = netl_dbg_open,
+	.llseek = seq_lseek,
+	.read = seq_read,
+	.release = single_release,
+};
+
+
+void copy_desc_pci(volatile struct _desc_pci *new, volatile struct _desc_pci *orig)
+{
+	memset((void *)new, 0, sizeof(struct _desc_pci));
+	/* Must copy dma_stat first */
+	new->dma_stat = pti_pci_read32((void *)&orig->dma_stat);
+	new->dma_len = pti_pci_read32((void *)&orig->dma_len);
+	new->dma_addr = pti_pci_read64((void *)&orig->dma_addr);
+}
+
+/*
+ * Deallocate and frees a descriptor ring
+ *
+ * @ring : descriptor ring to allocate
+ */
+static int init_desc_ring(struct pti_subsys *ss, struct desc_ring *ring, int type)
+{
+	int count;
+	struct descriptor *ptr;
+#ifdef NETL_PTI_HOST
+	dma_addr_t daddr;
+	u32 len;
+#endif
+
+	// PDEBUG("ss[%#x]->hs = %p, ring = %p\n", ss->id, ss->hs, ring);
+	ring->pdesc = kzalloc(sizeof(struct descriptor) * ss->hs->params.nr_desc, GFP_KERNEL);
+	if(!ring->pdesc){
+		PDEBUG("Failed to allocate pdesc\n");
+		return ENOMEM;
+	}
+	if (ss->hs->params.max_subsys == 0 || ss->hs->params.nr_desc == 0) {
+		PERROR("Fatal failure. Can't allocate 0 bytes\n");
+		return EFAULT;
+	}
+	ring->ss = ss;
+	spin_lock_init(&ring->rng_lock);
+	ring->start = ring->end = 0;
+	ring->head = (volatile u32 *)ss_get_ringcmd_offset(ss,type);
+	ring->tail = ring->head + 1;	/* pointer arithmetic */
+	ring->nfree = ss->hs->params.nr_desc - 1;
+	/* Keeping one slot open. If head == tail, empty.
+	 * if tail == head + 1, full */
+#ifdef NETL_PTI_HOST
+	pti_pci_write32(0, (void *)ring->head);
+	pti_pci_write32(0, (void *)ring->tail);
+#endif
+	for(count = 0; count < ss->hs->params.nr_desc; count++){
+		ptr = &ring->pdesc[count];
+		ptr->idx = count;
+		ptr->pring = ring;
+		ptr->pci_desc = (struct _desc_pci *)ss_get_desc_offset
+						(ss, count, type);
+		//PDEBUG("pci_desc = %p\n", ptr->pci_desc);
+#ifdef NETL_PTI_HOST
+		len = NETL_BUFSIZE;
+		if(type == DESC_RING_TYPE_RX_HOST){
+			ptr->buf = get_zeroed_page(GFP_ATOMIC|GFP_DMA32);
+			BUG_ON(!ptr->buf);
+			ptr->len = len;
+			daddr = pci_map_single(ss->hs->p_dev,
+				(void *)ptr->buf, len, DMA_FROM_DEVICE);
+			BUG_ON(!daddr);
+			//PDEBUG("ptr->buf = %p, daddr = %p, &dma_addr = %p\n", ptr->buf, (void *)daddr, &ptr->pci_desc->dma_addr);
+			pti_pci_write64(daddr, &ptr->pci_desc->dma_addr);
+			pti_pci_write32(len, (void *)&ptr->pci_desc->dma_len);
+			pti_pci_write32(DESC_DMA_READY, (void *)&ptr->pci_desc->dma_stat);
+		}else{
+			/* XXX Delete later */
+			pti_pci_write64(0xFF, &ptr->pci_desc->dma_addr);
+			pti_pci_write32(0xFF, (void *)&ptr->pci_desc->dma_len);
+			pti_pci_write32(0, (void *)&ptr->pci_desc->dma_stat);
+		}
+
+#else
+		ptr->buf = 0;	/* allocate later with dma_size */
+#endif
+	}
+	// PDEBUG("Success\n");
+	return 0;
+}
+
+/*
+ * Sends the device to stop all DMAs in progress
+ *	TODO
+ */
+void send_device_dma_abort(void)
+{
+	PDEBUG("Messaging device to stop all DMAs\n");
+	return;
+}
+
+/*
+ * Deallocate and frees a descriptor ring
+ *
+ * @ring : descriptor ring to deallocate
+ */
+static void dealloc_desc_ring(struct desc_ring *ring, int type)
+{
+#ifdef NETL_PTI_HOST
+	int i;
+
+	send_device_dma_abort();
+	for(i = 0; i < NETL_NR_DESC; i++){
+		//PDEBUG("Freeing pdesc[%#x]\n", ring->pdesc[i].idx);
+		pti_pci_write32(0, (void *)&ring->pdesc[i].pci_desc->dma_stat);
+		pti_pci_write32(0, (void *)&ring->pdesc[i].pci_desc->dma_len);
+		pti_pci_write64(0, (void *)&ring->pdesc[i].pci_desc->dma_addr);
+		if(type == DESC_RING_TYPE_RX_HOST){
+			free_page(ring->pdesc[i].buf);
+		}
+		ring->pdesc[i].buf = 0;
+		ring->pdesc[i].pring = NULL;
+	}
+	pti_pci_write32(0, (void *)ring->head);
+	pti_pci_write32(0, (void *)ring->tail);
+#endif
+	kfree(ring->pdesc);
+	return;
+}
+
+/*
+ * deallocate resources and frees a subsytem
+ *
+ * @hs	: handshake structure
+ */
+void dealloc_all_subsys(struct handshake_struct *hs)
+{
+	struct pti_subsys *ss;
+	int count;
+	struct hlist_node *node, *n;
+	unsigned long flags;
+
+	// down(&hs->hhsem);	// Need to check the effect TODO
+	for(count = 0; count < NETL_SUBSYS_HASH; count++){
+		if(hlist_empty(&hs->hhash[count])){
+			continue;
+		}
+		hlist_for_each_entry_safe(ss, node, n, &hs->hhash[count], snode){
+			spin_lock_irqsave(&hs->hs_lock, flags);
+			hlist_del(&ss->snode);
+			spin_unlock_irqrestore(&hs->hs_lock, flags);
+			dealloc_this_subsystem(ss);
+			/* ss is freed and not valid anymore */
+		}
+	}
+	// up(&hs->hhsem);
+	return;
+}
+
+void dealloc_this_subsystem(struct pti_subsys *ss)
+{
+	debugfs_remove_recursive(ss->dbgf);
+#ifdef NETL_PTI_HOST
+	dealloc_desc_ring(ss->txring, DESC_RING_TYPE_RX_HOST);
+	dealloc_desc_ring(ss->rxring, DESC_RING_TYPE_TX_HOST);
+#else
+	dealloc_desc_ring(ss->txring, DESC_RING_TYPE_RX_DEV);
+	dealloc_desc_ring(ss->rxring, DESC_RING_TYPE_TX_DEV);
+#endif
+	kfree(ss->priv_data);
+	kfree(ss);
+	return;
+}
+
+/*
+ * Initializes one subsystem denoted by idx
+ *
+ * @hs	:handshake struct
+ * @idx	: index of the subsystem
+ */
+struct pti_subsys *init_one_subsys(struct handshake_struct *hs, int id)
+{
+	char fname[NETL_DBG_FNAME_LEN];
+	unsigned long flags;
+	struct pti_subsys *ss = NULL;
+#ifdef NETL_PTI_DEVICE
+	int rx_flag = DESC_RING_TYPE_RX_DEV;
+	int tx_flag = DESC_RING_TYPE_TX_DEV;
+#else
+	int rx_flag = DESC_RING_TYPE_RX_HOST;
+	int tx_flag = DESC_RING_TYPE_TX_HOST;
+#endif
+
+	PDEBUG("Init subsystem %d\n", id);
+	ss = kzalloc(sizeof(struct pti_subsys), GFP_KERNEL);
+	if(!ss){
+		PDEBUG("Failed to allocate subsystem...\n");
+		return NULL;
+	}
+	ss->txring = kzalloc(sizeof(struct desc_ring), GFP_KERNEL);
+	ss->rxring = kzalloc(sizeof(struct desc_ring), GFP_KERNEL);
+	if(!ss->rxring || !ss->txring){
+		PDEBUG("ring alloc fail\n");
+		goto ring_fail;
+	}
+	ss->id = id;
+	ss->rxbytes = ss->txbytes = 0;
+	INIT_HLIST_NODE(&ss->snode);
+	ss->flags = 0;
+	spin_lock_init(&ss->slock);
+	ss->hs = hs;
+	ss->msgid = NETL_MSGID_START;
+	init_waitqueue_head(&ss->rdq);
+	init_waitqueue_head(&ss->wrq);
+	spin_lock_init(&ss->rxlock);
+	ss->rnum = 0;
+	INIT_LIST_HEAD(&ss->rx_head);
+
+	/* Now that all other fields are initialized, allocate and
+	* initialize rings. This MUST BE DONE BEFORE init_desc_ring
+	*/
+	if(init_desc_ring(ss, ss->rxring, rx_flag) != 0){
+		PDEBUG("Ring init RX fail\n");
+		goto rx_init_fail;
+	}
+	if(init_desc_ring(ss, ss->txring, tx_flag) != 0){
+		PDEBUG("Ring init TX fail\n");
+		goto tx_init_fail;
+	}
+	if(0 != ss->hs->dbg_root){
+		memset(fname, 0, NETL_DBG_FNAME_LEN);
+		sprintf(fname, "ss%x", ss->id);
+		ss->dbgf = debugfs_create_file(fname, S_IRUGO, ss->hs->dbg_root,
+				ss, &netl_dbg_fops);
+		if(!ss->dbgf || (IS_ERR(ss->dbgf) != 0)){
+			ss->dbgf = NULL;
+			PDEBUG("Failed to create debugfs entry for ss = %x\n", ss->id);
+		}
+		PDEBUG("created %s/%s\n", NETL_DBG_ROOT, fname);
+	}
+	spin_lock_irqsave(&hs->hs_lock, flags);
+	hlist_add_head(&ss->snode, &(hs->hhash[ss->id % NETL_SUBSYS_HASH]));
+	spin_unlock_irqrestore(&hs->hs_lock, flags);
+	PDEBUG("Subsystem Done %d\n", ss->id);
+//	dump_subsystem(NULL, ss, 0, 0);
+//	Queueing should happen only after full initialization
+//	queue_delayed_work(ss_workq, &ss->swork, 10);
+	return ss;
+tx_init_fail:
+	kfree(ss->rxring->pdesc);
+rx_init_fail:
+ring_fail:
+	kfree(ss->txring);
+	kfree(ss->rxring);
+	kfree(ss);
+	return NULL;
+}
+
+/*
+ * returns the offset of the pci memory space descriptor
+ * for subsystem cmd register
+ *
+ * @desc_nr :	number of descriptors per ring
+ * @ssid :	subsystem id
+ * @idx:	index of desc_ring
+ * @is_rx :	whether an rx ring or tx ring
+ */
+volatile u8 *ss_get_ringcmd_offset(struct pti_subsys *ss, int is_rx)
+{
+	/* calculate size of subsystem here */
+#ifdef NETL_PTI_DEVICE
+	volatile u8 *ret = (volatile u8*)(ss->hs->wcmd + 2);
+#else
+	volatile u8 *ret = (volatile u8*)(ss->hs->wcmd + 4);
+#endif
+	u32 offset;
+	u16 da_size = DESC_ARRAY_SIZE(ss->hs->params.nr_desc);
+
+	offset = ((ss->id * 2 + is_rx) * da_size);
+	ret += offset;
+	return ret;
+}
+
+/*
+ * returns the offset of the pci memory space descriptor
+ * for descriptor in desc_ring of a subsystem[ssid]
+ *
+ * @desc_nr :	number of descriptors per ring
+ * @ssid :	subsystem id
+ * @idx:	index of desc_ring
+ * @is_rx :	whether an rx ring or tx ring
+ */
+volatile u8 *ss_get_desc_offset(struct pti_subsys *ss, int idx, int is_rx)
+{
+	volatile u8 *offset = ss_get_ringcmd_offset(ss, is_rx);
+	offset += (2 * (sizeof(u32))); /* head and tail */
+	offset += idx * (sizeof(struct _desc_pci));
+	return offset;
+}
+
+#define netl_print(seq,fmt,args...)\
+do{\
+	if(seq != NULL)\
+		seq_printf(seq, fmt, ##args);\
+	else\
+		printk(fmt, ##args);\
+}while(0)
+
+void dump_hs(struct seq_file *seq, struct handshake_struct *hs)
+{
+	netl_print(seq, "rcmd = %p, val = %#x\n",
+			hs->rcmd, pti_pci_read32((void*)hs->rcmd));
+	netl_print(seq, "wcmd = %p, val = %#x\n",
+			hs->wcmd, pti_pci_read32((void*)hs->wcmd));
+	netl_print(seq, "wstat = %p, val = %#x\n",
+			hs->wstat, pti_pci_read32((void*)hs->wstat));
+	netl_print(seq, "rstat = %p, val = %#x\n",
+			hs->rstat, pti_pci_read32((void*)hs->rstat));
+	netl_print(seq, "params.max_subsys = %#x\n", hs->params.max_subsys);
+	netl_print(seq, "params.flags = %#x\n", hs->params.flags);
+	netl_print(seq, "params.nr_desc = %#x\n", hs->params.nr_desc);
+	netl_print(seq, "status = %#x\n", hs->status);
+#ifdef NETL_PTI_HOST
+	netl_print(seq, "dev_list = %p\n", hs->dev_list);
+#endif
+	return;
+}
+
+void dump_subsystem(struct seq_file *seq, struct pti_subsys *ss, int txr, int rxr)
+{
+	netl_print(seq, "id = %#x\n", ss->id);
+	netl_print(seq, "flags = %#lx\n", ss->flags);
+	netl_print(seq, "hs = %p\n", ss->hs);
+	netl_print(seq, "rxbytes = %llx\n", ss->rxbytes);
+	netl_print(seq, "txbytes = %llx\n", ss->txbytes);
+	netl_print(seq, "rxring = %p\n", ss->rxring);
+	dump_desc_ring(seq, ss->rxring, rxr);
+	netl_print(seq, "txring = %p\n", ss->txring);
+	dump_desc_ring(seq, ss->txring, txr);
+	return;
+}
+
+void dump_desc_ring(struct seq_file *seq, struct desc_ring *ring, int dump_all)
+{
+	int i = 0;
+
+	if(ring == NULL){
+		PDEBUG("Can't dump NULL ring\n");
+		return;
+	}
+	netl_print(seq, "\thead = %p, val = %#x\n",
+			ring->head, pti_pci_read32((void*)ring->head));
+	netl_print(seq, "\ttail = %p, val = %#x\n",
+			ring->tail, pti_pci_read32((void*)ring->tail));
+	netl_print(seq, "\tpdesc = %p\n", ring->pdesc);
+	netl_print(seq, "\tnfree = %#x\n", ring->nfree);
+	netl_print(seq, "\tstart = %#x\n", ring->start);
+	netl_print(seq, "\tend = %#x\n", ring->end);
+	netl_print(seq, "\tss = %p\n", ring->ss);
+	if(dump_all){
+		for(i = 0; i < ring->ss->hs->params.nr_desc; i++){
+			dump_descriptor(seq, &ring->pdesc[i]);
+			netl_print(seq, "\n");
+		}
+	}else{
+		dump_descriptor(seq, &ring->pdesc[0]);
+	}
+	return;
+}
+
+void dump_desc_pci(struct seq_file *seq, struct _desc_pci *pci)
+{
+	netl_print(seq, "%#llx %#x %#x\n", (u64)readq(&pci->dma_addr), readl(&pci->dma_len), readl(&pci->dma_stat));
+}
+
+void dump_descriptor(struct seq_file *seq, struct descriptor *ptr)
+{
+	struct descriptor *desc;
+	if(ptr == NULL){
+		PDEBUG("Null desc\n");
+		return;
+	}
+	desc = ptr;
+	netl_print(seq, "\t\tpci_desc = %p\n", desc->pci_desc);
+	netl_print(seq, "\t\t\tdma_addr = %p val = %#llx\n",
+	&desc->pci_desc->dma_addr, (u64)pti_pci_read64((void*)&desc->pci_desc->dma_addr));
+	netl_print(seq, "\t\t\tdma_len = %p val = %#x\n",
+	&desc->pci_desc->dma_len, pti_pci_read32((void*)&desc->pci_desc->dma_len));
+	netl_print(seq, "\t\t\tdma_stat = %p val = %#x\n",
+	&desc->pci_desc->dma_stat, pti_pci_read32((void*)&desc->pci_desc->dma_stat));
+	netl_print(seq, "\t\tbuf = %lx\n", desc->buf);
+	netl_print(seq, "\t\tlen = %#x\n", desc->len);
+	netl_print(seq, "\t\tbuf_flag = %#llx\n", desc->buf_flag);
+	netl_print(seq, "\t\tpring = %p\n", desc->pring);
+	netl_print(seq, "\t\tidx = %#x\n", desc->idx);
+	return;
+}
+
+static int netl_dbg_ss_show(struct seq_file *seq, void *v)
+{
+	struct pti_subsys *ss = (struct pti_subsys *)seq->private;
+	dump_subsystem(seq,ss,1,1);
+	return 0;
+}
+
+static int netl_dbg_open(struct inode *inode, struct file *filp)
+{
+	return single_open(filp, netl_dbg_ss_show, inode->i_private);
+}
+
+int netl_pti_debugfs_init(struct handshake_struct *hs)
+{
+	struct dentry *d = debugfs_create_dir(NETL_DBG_ROOT, NULL);
+	if(!d || (IS_ERR(d) != 0)){
+		PDEBUG("Failed to create debugfs, d = %p\n", d);
+		return PTR_ERR(d);
+	}
+	hs->dbg_root = d;
+	return 0;
+}
+
+/*
+ * Dequeue a packet from rx buffer list
+ * Must be called with ss->rxlock held
+ */
+struct rx_struct *__dequeue_msg(struct pti_subsys *ss, u32 msgid, int match)
+{
+	struct rx_struct *pos = NULL, *n;
+
+	if(match == NETL_MSG_ANY){
+		pos = list_first_entry(&ss->rx_head, struct rx_struct, rhead);
+		list_del(&(pos->rhead));
+		ss->rnum--;
+		return pos;
+	}else{
+		list_for_each_entry_safe(pos, n, &ss->rx_head, rhead){
+			if(__le32_to_cpup((const u32 *)pos->iov.iov_base) == msgid){
+				list_del(&(pos->rhead));
+				ss->rnum--;
+				return pos;
+			}
+		}
+	}
+	return NULL;
+}
+
+/*
+ * Back end of read call : data path only
+ *
+ * @ss:	subsystem ptr
+ * @buf	: kernel buffer to copy
+ * @n	: #bytes to copy
+ * @free_list :	list of rx_buf to re-insert if something gos wrong. This is currently unused.
+ */
+int copy_nbytes_from_rxlist(struct pti_subsys *ss, void *buf, ssize_t n, struct list_head *free_list)
+{
+	unsigned long flags;
+	void *kstart = buf;
+	int totread = 0;
+	struct rx_struct *pos = NULL, *nxt;
+	void *start = NULL;
+	int tord = 0;
+
+	spin_lock_irqsave(&ss->rxlock, flags);
+	if(list_empty(&ss->rx_head)){
+		goto fnend;
+	}
+	list_for_each_entry_safe(pos, nxt, &ss->rx_head, rhead){
+	//PDEBUG("pos = %p, nxt = %p, &ss->rx_head = %p, next = %p, prev = %p\n", pos, nxt, &ss->rx_head, pos->rhead.next, pos->rhead.prev);
+	//PDEBUG("pos = %p, base = %p, soffset = %#lx\n", pos, pos->iov.iov_base, pos->soffset);
+		start = pos->iov.iov_base + pos->soffset;
+		tord = pos->iov.iov_len - pos->soffset;
+		tord = (tord > (n - totread)) ? (n - totread) : tord;
+//		PDEBUG("tord = %#x, n = %#x, totread = %#x\n", tord, n, totread);
+		memcpy(kstart, start, tord);
+		pos->soffset += tord;
+		totread += tord;
+		kstart += tord;
+		if(pos->soffset == pos->iov.iov_len){
+			/* Consumed one rx_struct */
+			ss->rnum--;
+//			PDEBUG("&pos->rhead = %pss->rnum = %#x\n", (&(pos->rhead)), ss->rnum);
+			list_del(&(pos->rhead));
+			if(free_list != NULL){
+				INIT_LIST_HEAD(&(pos->rhead));
+				list_add_tail(&(pos->rhead), free_list);
+			}else{
+				free_page((unsigned long)pos->iov.iov_base);
+				kfree(pos);
+			}
+		}else if(pos->soffset > pos->iov.iov_len){
+			/* Should never happen */
+			goto fnfail;
+		}
+		if(totread == n){
+			break;
+		}else if(totread > n){
+			/* Should never happen */
+			goto fnfail;
+		}
+	}	/* list_for_each_entry_safe */
+fnend:
+	spin_unlock_irqrestore(&ss->rxlock, flags);
+	return totread;
+fnfail:
+	spin_unlock_irqrestore(&ss->rxlock, flags);
+	PERROR("FATAL : Failed logic\n");
+	//PERROR("soffset = %#lx, len = %#x, kstart = %p, totread = %#x, " "n = %#lx\n",pos->soffset, pos->iov.iov_len, kstart, totread, n);
+	BUG();
+	return 0;	/* silence the compiler */
+}
+
+/*
+ * Gets an rx_struct from list -- control path only
+ */
+int get_packet_from_rxlist(struct pti_subsys *ss, struct rx_struct **ret)
+{
+	unsigned long flags;
+
+//	PDEBUG("Getting sem\n");
+	spin_lock_irqsave(&ss->rxlock, flags);
+	if(list_empty(&ss->rx_head)){
+		BUG_ON(ss->rnum != 0);
+		spin_unlock_irqrestore(&ss->rxlock, flags);
+//		PDEBUG("List empty\n");
+		return -EAGAIN;
+	}
+	*ret = __dequeue_msg(ss, 0, NETL_MSG_ANY);
+//	PDEBUG("remaining %#x elems\n", ss->rnum);
+	spin_unlock_irqrestore(&ss->rxlock, flags);
+	return 0;
+}
+
+EXPORT_SYMBOL(get_packet_from_rxlist);
+/*
+ * Adds a buffer and length to the receive buffer list
+ *
+ * @ss	:subsystem
+ * @buf	: buffer to add to rxlist
+ * @len	: length of the buffer
+ */
+int add_to_rxlist(struct pti_subsys *ss, volatile struct kvec *iov)
+{
+	unsigned long flags;
+	struct rx_struct *rx = kzalloc(sizeof(struct rx_struct), GFP_ATOMIC);
+
+	if(!rx){
+		PDEBUG("memory alloc failure\n");
+		return -ENOMEM;
+	}
+	INIT_LIST_HEAD(&rx->rhead);
+	spin_lock_irqsave(&ss->rxlock, flags);
+	rx->iov.iov_base = iov->iov_base;
+	rx->iov.iov_len = iov->iov_len;
+	list_add_tail(&rx->rhead, &ss->rx_head);
+	ss->rnum++;
+	spin_unlock_irqrestore(&ss->rxlock, flags);
+	//PDEBUG("waking up sleeping processes\n");
+	wake_up_interruptible(&ss->rdq);
+	return 0;
+}
+
+EXPORT_SYMBOL(add_to_rxlist);
+/*
+ * If we support more than one device, handshake struct must have an entry
+ * to show the minor number of the first device. minor_to_idx will return the
+ * index# derived from minor number.
+ *
+ * @hs	: handshake structure
+ * @minor	: minor number
+ */
+int hs_minor_to_idx(struct handshake_struct *hs, int minor)
+{
+	return minor;
+}
+
+/*
+ * Converts an integer to ss, if it is valid
+ *
+ * @hs	: handshake struct for this device
+ * @idx	: index of the subsystem
+ */
+struct pti_subsys *get_ss_from_idx(struct handshake_struct *hs, int idx)
+{
+	struct pti_subsys *ss;
+	struct hlist_node *node;
+	struct hlist_head *head;
+	unsigned long flags;
+
+	if(idx > hs->params.max_subsys){
+		return NULL;
+	}
+	spin_lock_irqsave(&hs->hs_lock, flags);
+	head = &hs->hhash[idx % NETL_SUBSYS_HASH];
+	hlist_for_each_entry(ss, node, head, snode){
+		if(idx == ss->id){
+			spin_unlock_irqrestore(&hs->hs_lock, flags);
+			return ss;
+		}
+	}
+	spin_unlock_irqrestore(&hs->hs_lock, flags);
+	return NULL;
+}
+
+void stop_subsystem(struct pti_subsys *ss)
+{
+	unsigned long flags;
+	struct desc_ring *r;
+	int count = 0;
+	struct rx_struct *ret;
+
+	spin_lock_irqsave(&ss->slock, flags);
+	ss->flags |= (SUBSYS_FLAGS_TX_DISABLED | SUBSYS_FLAGS_RX_DISABLED);
+	/* logic is suspect : TODO
+	if(ss->pid == 0){
+		ss->flags |= SUBSYS_FLAGS_ORPHANED;
+	}
+	*/
+	spin_unlock_irqrestore(&ss->slock, flags);
+
+	/* subsys_poll_* nad dev_def_*_cmpl have higher priority
+	 * So, I assume that by now, it must have executed fully
+	 * TODO : if suspects a race, change to rwlocks...???
+	 */
+	/* Mark all Tx and any Rx bufs as dma complete */
+	r = ss->rxring;
+	spin_lock_irqsave(&r->rng_lock, flags);
+	r->end = r->start = pti_pci_read32((void *)r->head);
+	for(count = 0; count < ss->hs->params.nr_desc; count++){
+		pti_pci_write32(0, (void *)&r->pdesc[count].pci_desc->dma_stat);
+	}
+	pti_pci_write32(r->start, (void *)r->tail);
+	spin_unlock_irqrestore(&r->rng_lock, flags);
+
+	r = ss->txring;
+	spin_lock_irqsave(&r->rng_lock, flags);
+	r->end = r->start = pti_pci_read32((void *)r->head);
+	for(count = 0; count < ss->hs->params.nr_desc; count++){
+		pti_pci_write32(0, (void *)&r->pdesc[count].pci_desc->dma_stat);
+	}
+	pti_pci_write32(r->start, (void *)r->tail);
+	spin_unlock_irqrestore(&r->rng_lock, flags);
+	while(ss->rnum != 0){
+		if(get_packet_from_rxlist(ss, &ret) != 0){
+			break;
+		}
+		free_page((unsigned long) ret->iov.iov_base);
+		kfree(ret);
+	}
+	dealloc_this_subsystem(ss);
+	return;
+}
+
+int init_handshake_struct(struct handshake_struct *hs)
+{
+	int ret, i;
+	typedef int(*sp_init)(struct handshake_struct *);
+	sp_init specific_init;
+#ifdef NETL_PTI_HOST
+	extern int host_specific_hs_init(struct handshake_struct *hs);
+	specific_init = host_specific_hs_init;
+#else
+	extern int dev_specific_hs_init(struct handshake_struct *hs);
+	specific_init = dev_specific_hs_init;
+#endif
+	init_waitqueue_head(&hs->openq);
+	spin_lock_init(&hs->hs_lock);
+	netl_pti_debugfs_init(hs);
+	hs->hhash = kzalloc(sizeof(struct hlist_head) * NETL_SUBSYS_HASH, GFP_KERNEL);
+	if(!hs->hhash){
+		kfree(hs);
+		return -ENOMEM;
+	}
+	for(i = 0; i < NETL_SUBSYS_HASH; i++){
+		INIT_HLIST_HEAD(&(hs->hhash[i]));
+	}
+	init_MUTEX(&hs->hhsem);
+	/* Common initialization here */
+	if((ret = specific_init(hs)) != 0){
+		kfree(hs->hhash);
+		return ret;
+	}
+	return 0;
+}
diff --git a/drivers/misc/netlogic/pcie-offload/include/netl_pti_dev.h b/drivers/misc/netlogic/pcie-offload/include/netl_pti_dev.h
new file mode 100644
index 0000000..b0258f33
--- /dev/null
+++ b/drivers/misc/netlogic/pcie-offload/include/netl_pti_dev.h
@@ -0,0 +1,90 @@
+#ifndef _NETL_PTI_H
+#define _NETL_PTI_H
+
+
+struct netl_pti_subsys {
+	int module_id;
+	int registered;
+	netl_pti_recv_handler *recv_handler;
+	netl_pti_alloc *alloc;
+	int desc_nr;
+	struct netl_pti_desc *rx_ring_start;
+	struct netl_pti_desc *tx_ring_start;
+	struct netl_pti_ring_info *rx_ring_info;
+	struct netl_pti_ring_info *tx_ring_info;
+	uint8_t *tx_ring_start_inband;
+	uint8_t *rx_ring_start_inband;
+	unsigned int free_processed;
+	struct free_list *free_list;
+#ifdef NETL_PTI_DEV
+	struct recv_list *recv_list;
+#endif
+	spinlock_t send_lock;
+	spinlock_t free_lock;
+};
+
+//Have two instances in case we need dev to host msgs
+struct netl_pti_handshake {
+	uint32_t ver;
+	uint32_t host_endianness;
+	uint32_t dev_endianness;
+	uint32_t max_subsys;
+	uint32_t subsys_id;
+	uint32_t sent_msg_seq;
+	uint32_t recv_msg_seq;
+	uint32_t ring_ptr_off;
+	uint32_t tx_ring_start_off;
+	uint32_t rx_ring_start_off;
+	uint32_t tx_ring_start_inband_off;
+	uint32_t rx_ring_start_inband_off;
+	uint64_t app_data;
+	uint32_t hs_type;
+};
+
+#ifdef NETL_PTI_DEV
+#define dtoh_16 dtoh_16_dev
+#define dtoh_32 dtoh_32_dev
+#define dtoh_64 dtoh_64_dev
+#define netl_pti_unregister       netl_pti_unregister_dev
+#endif
+#ifdef NETL_PTI_HOST
+#define dtoh_16 dtoh_16_host
+#define dtoh_32 dtoh_32_host
+#define dtoh_64 dtoh_64_host
+#define netl_pti_send_with_inband netl_pti_send_with_inband_host
+#define netl_pti_unregister netl_pti_unregister_host
+#define netl_pti_send_inband netl_pti_send_inband_host
+#define netl_pti_send netl_pti_send_host
+#define netl_pti_send_sg netl_pti_send_sg_host
+#define netl_pti_register_subsys netl_pti_register_subsys_host
+#define handshake_status handshake_status_host
+#endif
+
+extern int netl_pti_register_subsys(uint8_t subsys_id, int desc_nr,
+				    netl_pti_recv_handler * recv_handler,
+				    netl_pti_alloc * alloc);
+int netl_pti_send_with_inband(uint8_t subsys_id, uint8_t msg_code, void *mem,
+			      int len, void *inband_mem, int inband_len,
+			      netl_pti_free free_func, void *free_info
+#ifdef NETL_PTI_DEV
+			      , uint64_t dma_addr
+#endif
+    );
+extern int netl_pti_send_inband(uint8_t subsys_id, uint8_t msg_code,
+				void *inband_mem, int inband_len);
+int netl_pti_send(uint8_t subsys_id, uint8_t msg_code, void *mem, int len,
+		  netl_pti_free * free_func, void *free_info
+#ifdef NETL_PTI_DEV
+		  , uint64_t dma_addr
+#endif
+    );
+int netl_pti_send_sg(uint8_t subsys_id, uint8_t msg_code,
+		     struct scatterlist *sg_list, int len, void *inband_mem,
+		     int inband_len, netl_pti_free * free_func, void *free_info
+#ifdef NETL_PTI_DEV
+		     , uint64_t dma_addr
+#endif
+    );
+extern int handshake_status(void);
+
+#endif
diff --git a/drivers/misc/netlogic/pcie-offload/include/netl_pti_test.c b/drivers/misc/netlogic/pcie-offload/include/netl_pti_test.c
new file mode 100644
index 0000000..d7f85f2
--- /dev/null
+++ b/drivers/misc/netlogic/pcie-offload/include/netl_pti_test.c
@@ -0,0 +1,167 @@
+#include "../include/netl_pti.h"
+
+int send_packets_back(struct pti_subsys *ss);
+extern int add_to_rxlist(struct pti_subsys *ss, struct kvec *iov);
+extern int get_packet_from_rxlist(struct pti_subsys *ss, struct rx_struct **ret);
+extern int send_one_packet_to_host(struct pti_subsys *ss, void *data, u32 len);
+extern int send_one_packet_to_device(struct pti_subsys *ss, void *data, u32 len);
+extern int get_hs_from_idx(struct handshake_struct **res, int idx);
+extern struct pti_subsys *get_ss_from_idx(struct handshake_struct *hs, int idx);
+int netl_test_write(struct pti_subsys *ss, u32 len, char fill);
+
+struct netl_test_struct{
+	u64 wait_tot;
+	u64 spent_tot;
+	u64 sent_bytes;
+	struct dentry *dbg_root;
+};
+static ulong bytes2send = (1 << 12);
+module_param(bytes2send, ulong, 0);
+MODULE_PARM_DESC(bytes2send, "Bytes to send in one try");
+
+struct netl_test_struct gstat;
+
+static ssize_t netl_dbg_test_write(struct file *filp, const char __user *buf,
+		size_t len, loff_t *off)
+{
+	struct handshake_struct *hs = NULL;
+	struct pti_subsys *ss;
+	ulong inp;
+
+	if((get_hs_from_idx(&hs, 0) < 0) || (hs == NULL)){
+		PDEBUG("Failed to get HS\n");
+		return -ENODEV;
+	}
+	ss = get_ss_from_idx(hs, 1);
+	if(ss == NULL){
+		PDEBUG("SS = NULL\n");
+		return -ENODEV;
+	}
+	inp = simple_strtoul(buf, NULL, 0);
+	if(inp > 0){
+		PDEBUG("Setting bytes2send to %#lx\n", inp);
+		bytes2send = inp;
+	}else if(inp == 0){
+		PDEBUG("Clearing stat\n");
+		gstat.wait_tot = 0;
+		gstat.spent_tot = 0;
+		gstat.sent_bytes = 0;
+		return len;
+	}
+	netl_test_write(ss, bytes2send, 'A');
+	return len;
+}
+
+static int netl_test_dbg_show(struct seq_file *seq, void *v)
+{
+	struct timespec ts;
+	struct netl_test_struct *stat = (struct netl_test_struct *)seq->private;
+
+	seq_printf(seq, "Sent : %llu\n", stat->sent_bytes);
+	jiffies_to_timespec(stat->spent_tot, &ts);
+	seq_printf(seq, "Total time took: %llu(%ld.%09ld)\n",
+			stat->spent_tot, ts.tv_sec, ts.tv_nsec);
+	jiffies_to_timespec(stat->wait_tot, &ts);
+	seq_printf(seq, "Wait time: %llu(%ld.%09ld)\n",
+			stat->wait_tot, ts.tv_sec, ts.tv_nsec);
+	return 0;
+}
+
+static int netl_test_dbg_open(struct inode *inode, struct file *filp)
+{
+	return single_open(filp, netl_test_dbg_show, inode->i_private);
+}
+
+struct file_operations netl_test_dbg_ops = {
+	.owner = THIS_MODULE,
+	.open = netl_test_dbg_open,
+	.llseek = seq_lseek,
+	.read = seq_read,
+	.release = single_release,
+	.write = netl_dbg_test_write,
+};
+int netl_dev_test_init(void)
+{
+	struct dentry *d = debugfs_create_dir("netltest", NULL);
+	PDEBUG("Compiled on %s %s\n", __DATE__, __TIME__);
+
+	if(!d || (IS_ERR(d) != 0)){
+		PDEBUG("Failed to create debugfs, d = %p\n", d);
+		return PTR_ERR(d);
+	}
+	gstat.dbg_root = d;
+	debugfs_create_file("input", S_IRUGO|S_IWUGO, gstat.dbg_root, &gstat, &netl_test_dbg_ops);
+	return 0;
+}
+
+void netl_dev_test_exit(void)
+{
+	debugfs_remove_recursive(gstat.dbg_root);
+}
+
+module_init(netl_dev_test_init);
+module_exit(netl_dev_test_exit);
+MODULE_LICENSE("GPL");
+typedef int (*send_func)(struct pti_subsys *, void *, u32);
+
+int netl_test_write(struct pti_subsys *ss, u32 len, char fill)
+{
+	void *kpage = 0;
+	int ret = 0;
+	int len2s = 0, sent = 0;
+	int written = 0;
+	unsigned long beg, end, wbeg = 0, wend = 0;
+
+#ifdef NETL_PTI_HOST
+	send_func send_packet = send_one_packet_to_device;
+#else
+	send_func send_packet = send_one_packet_to_host;
+#endif
+
+	if(ss->hs->status != HS_DEV_READY_TXRX){
+		return -EIO;
+	}
+	/* split up the incoming packet
+	 * write(2) should succeed even if only one byte is written */
+	beg = jiffies;
+	while(sent != len){
+		len2s = ((len - sent) > NETL_BUFSIZE) ? NETL_BUFSIZE: (len - sent);
+		//PDEBUG("len2s = %#x\n", len2s);
+		kpage = (void *)__get_free_page(GFP_KERNEL);
+		if(!kpage){
+			ret = -ENOMEM;
+			break;
+		}
+		memset(kpage, fill, NETL_BUFSIZE);
+		//PDEBUG("Sending packet on ss->id %#x\n", ss->id);
+		ret = send_packet(ss, kpage, len2s);
+		if(ret == 0){
+			//PDEBUG("Sent one packet\n");
+			written = 1;
+			sent += len2s;
+			continue;
+		}
+		if(ret == EAGAIN){
+			//PDEBUG("Waiting\n");
+			free_page((unsigned long)kpage);
+			/* Else add to ss->wrq and wait */
+			wbeg = jiffies;
+			if(wait_event_interruptible(ss->wrq, (ss->txring->nfree != 0))){
+				ret = -ERESTARTSYS;
+				break;
+			}
+			wend = jiffies;
+			gstat.wait_tot += ((long)wend - (long)wbeg);
+		}else{
+			PDEBUG("Sent fail\n");
+			free_page((unsigned long)kpage);
+			break;
+		}
+	}
+	end = jiffies;
+	gstat.spent_tot += ((long)end - (long)beg);
+	gstat.sent_bytes += sent;
+	PDEBUG("sent bytes %#x, jiffies spent = %#lx, waited %#lx\n",
+			sent, ((long)end - (long)beg), ((long)wend - (long)wbeg));
+	return((written == 0) ? ret : sent);
+}
diff --git a/drivers/misc/netlogic/pcie-offload/include/nlm_pcie.h b/drivers/misc/netlogic/pcie-offload/include/nlm_pcie.h
new file mode 100644
index 0000000..2a17a4a
--- /dev/null
+++ b/drivers/misc/netlogic/pcie-offload/include/nlm_pcie.h
@@ -0,0 +1,11 @@
+#ifndef NLM_PCIE_H
+#define NLM_PCIE_H
+#include <asm/netlogic/nlm_dma.h>
+
+int nlm_common_request_msi_handler(irq_handler_t host_msi_handler, void *data);
+void nlm_common_free_msi_handler(void);
+volatile void *nlm_common_get_shared_mem_base_host(void);
+void nlm_common_interrupt_device(void);
+extern struct pci_dev *nlm_pdev;
+void __exit nlm_pcie_exit(void);
+#endif
diff --git a/drivers/misc/netlogic/pcie-offload/include/nlm_pcieip.h b/drivers/misc/netlogic/pcie-offload/include/nlm_pcieip.h
new file mode 100644
index 0000000..ed607dc
--- /dev/null
+++ b/drivers/misc/netlogic/pcie-offload/include/nlm_pcieip.h
@@ -0,0 +1,199 @@
+#ifndef NLM_PCIEIP_H
+#define NLM_PCIEIP_H
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/interrupt.h>
+#include <linux/types.h>
+#include <linux/string.h>
+#include <linux/socket.h>
+#include <linux/errno.h>
+#include <linux/fcntl.h>
+#include <linux/in.h>
+#include <linux/init.h>
+#include <linux/pci.h>
+#include <linux/workqueue.h>
+#include <linux/inet.h>
+#include <linux/netdevice.h>
+#include <linux/ethtool.h>
+#include <linux/etherdevice.h>
+#include <linux/skbuff.h>
+#include <net/sock.h>
+#include <linux/if_ether.h>	/* For the statistics structure. */
+#include <linux/if_arp.h>	/* For ARPHRD_ETHER */
+#include <linux/proc_fs.h>
+#include <linux/version.h>
+#include <linux/spinlock.h>
+#include <linux/module.h>
+#include <linux/sched.h>
+#include <linux/io.h>
+#include <linux/fs.h>
+#include <linux/cdev.h>
+#include <asm/uaccess.h>
+#include <linux/debugfs.h>
+#include <linux/seq_file.h>
+#include <linux/signal.h>
+#include <linux/poll.h>
+#include <asm/bitops.h>
+#include <asm/system.h>
+#include <asm/cache.h>
+#include <linux/delay.h>
+#include <linux/timer.h>
+
+#ifndef CONFIG_NLM_XLP
+#define fdebug(fmt,arg...)\
+	printk(KERN_WARNING "%s:%d " fmt, __FILE__, __LINE__, ##arg)
+#endif
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,35)
+#include <generated/autoconf.h>
+#else
+#include <linux/autoconf.h>
+#endif
+
+#ifdef CONFIG_NLM_XLP
+#include <asm/netlogic/debug.h>
+#include <asm/netlogic/pci.h>
+#include <asm/netlogic/pic.h>
+#include <asm/netlogic/mips-exts.h>
+#include <asm/netlogic/msgring.h>
+#include <asm/netlogic/sim.h>
+#include <asm/netlogic/proc.h>
+#include <asm/netlogic/nlm_pcix_gen_dev.h>
+#include <asm/netlogic/nlm_dma.h>
+#include <nlm_dma.h>
+#endif
+
+#define DRV_NAME	"nlm_pcieip"
+#define DRV_VERSION	"0.9"
+#define DRV_RELDATE	"8Aug2011"
+
+#define DESC_MAX 128	/* Max descriptors for RX and Tx */
+#define TX_DESC_MAX			DESC_MAX
+#define RX_DESC_MAX			DESC_MAX
+#define NLM_SMP_CACHE_BYTES 32
+
+#define PCIEIP_MAX_RXLEN 1536
+#define PCIEIP_MAGIC	0xdeadbeef
+
+struct xlp_desc{
+	u64 addr;
+	u32 info;/*  Bit 0 to 15 LEN, Bit 16 to 30 Reserved, Bit 31 OWN */
+}__attribute__((packed));
+
+#define DMA_BIT		(1 << 31)
+#define GET_DMA_READY(x) ((x) & DMA_BIT)
+#define CLEAR_DMA_READY(x) ((x) = ((x) & ~DMA_BIT))
+#define SET_DMA_READY(x) ((x) = (x) | DMA_BIT)
+#define GET_LEN(x) ((x) & 0x3fff)
+#define SET_LEN(x,len) (x) = (((x)->info & ~(0x3fff)); (x) |= len)
+#define NLM_PCIE_IP_OFFSET	(1 * 512 * 1024)
+
+#define PCIEIP_FLAG_MSI_ENABLED		0x1
+#define PCIEIP_SKB_ALLOC_SIZE		(1536 + 32 + 32)
+#define PCIEIP_IF_UP			(0x1 << 0)
+#define PCIEIP_IF_DOWN			(0x1 << 1)
+#define PCIEIP_DESC_RX_CLEAR		(0x1 << 2)
+#define PCIEIP_DESC_TX_CLEAR		(0x1 << 3)
+
+struct debug_ctrs {
+	int rx_pkt;
+	int tx_pkt;
+	int tx_q;
+	int rx_q;
+	int rx_dma;
+	int tx_dma;
+};
+
+struct skb_free_info {
+	struct sk_buff *skb;
+        dma_addr_t paddr;
+        int len;
+	int idx;
+	struct driver_data *priv;
+};
+
+#define INCR_CEIL(x)\
+({typeof(x) _x = (x);\
+ _x = ((_x + 1) % DESC_MAX);\
+ _x;\
+})
+
+#define nlm_pci_readl(s)\
+({\
+	readl((void*)s);\
+})
+
+#define nlm_pci_writel(v,d)\
+({\
+	writel(((u32)v), (void *)d);\
+ 	mb();\
+})
+
+#define nlm_pci_readq(s)\
+({\
+	readq((void*)s);\
+})
+
+#define nlm_pci_writeq(v,d)\
+({\
+	writeq(((u64)v), (void *)d);\
+ 	mb();\
+})
+
+#define NETL_VENDOR_ID 0x184e
+#define NETL_DEVICE_ID 0x1004
+#define	PCIEIP_LINK_TIMER_INTERVAL	(1)
+#define PCIEIP_MAX_LOOPS	16	/* Max # of Rx or Tx loops with spin lock held */
+
+struct driver_data {
+	struct net_device *ndev;
+	struct pci_dev *pdev;
+	volatile void *base;	/* base of pci memory */
+	struct xlp_desc *tx_base;
+	int xhead, xtail;
+	struct skb_free_info tx_q[TX_DESC_MAX];
+	struct xlp_desc *rx_base;
+	struct skb_free_info rx_q[RX_DESC_MAX];
+	volatile u32 *th_stat, *my_stat, *magic;/* status : my and their */
+	int rhead, rtail;
+	spinlock_t lock;
+	struct net_device_stats stats;
+	struct timer_list link_timer;
+	struct delayed_work poll_work;
+	struct msix_entry *msix;
+	u32 flags;	/* MSI/MSIX, status ..etc */
+	struct dentry *dbgf;
+	u32 poll_delay;
+};	/* Not shared between host and device */
+
+#define nlm_printk(seq,fmt,args...)\
+do{\
+	if(seq != NULL)\
+		seq_printf(seq, fmt, ##args);\
+	else\
+		printk(fmt, ##args);\
+}while(0)
+
+inline static void dump_one_desc(struct seq_file *s, struct xlp_desc *d, int i, char *prefix)
+{
+	nlm_printk(s, "%s %d: addr = %#llx, info = %#x\n", prefix, i, (u64)readq(&(d->addr)), readl(&(d->info)));
+}
+inline static void dump_all_desc(struct seq_file *s, struct driver_data *priv)
+{
+	int i;
+	nlm_printk(s, "rx_base = 0x%p, tx_base = 0x%p\n", priv->rx_base, priv->tx_base);
+	nlm_printk(s, "xhead = %d, xtail = %d\n", priv->xhead, priv->xtail);
+	nlm_printk(s, "rhead = %d, rtail = %d\n", priv->rhead, priv->rtail);
+	for (i = 0; i < DESC_MAX; i++) {
+		dump_one_desc(s, priv->tx_base + i, i, "TX:");
+	}
+	for (i = 0; i < DESC_MAX; i++) {
+		dump_one_desc(s, priv->rx_base + i, i, "RX:");
+	}
+}
+#ifdef CONFIG_NLM_PCIEIP_DEV
+/* Device specific functions goes here */
+
+
+#endif
+
+#endif	// NLM_PCIEIP_H
diff --git a/drivers/misc/netlogic/pcie-offload/include/perf.h b/drivers/misc/netlogic/pcie-offload/include/perf.h
new file mode 100644
index 0000000..ff1a598
--- /dev/null
+++ b/drivers/misc/netlogic/pcie-offload/include/perf.h
@@ -0,0 +1,36 @@
+#ifndef _NETL_PERF_H
+#define _NETL_PERF_H
+#include "netl_pti.h"
+#include "netl_pti_char.h"
+#define NETL_FILE_MAXLEN	512
+static inline int Read(int fd, void *buf, int total)
+{
+	int ret = 0, rd = 0, bal = total;
+	char *start = (char *)buf;
+
+	while(rd < total){
+		ret = read(fd, (void *)start, bal);
+		if(ret < 0){
+			return ret;
+		}
+		bal -= ret; rd += ret; start += ret;
+	}
+	return rd;
+}
+
+static inline int Write(int fd, void *buf, int total)
+{
+	int ret = 0, rd = 0, bal = total;
+	char *start = (char *)buf;
+
+	while(rd < total){
+		ret = write(fd, (void *)start, bal);
+		if(ret < 0){
+			return ret;
+		}
+		bal -= ret; rd += ret; start += ret;
+	}
+	return rd;
+}
+
+#endif		/* _NETL_PERF_H */
diff --git a/drivers/misc/netlogic/pcie-offload/include/redirect.c b/drivers/misc/netlogic/pcie-offload/include/redirect.c
new file mode 100644
index 0000000..75e5453
--- /dev/null
+++ b/drivers/misc/netlogic/pcie-offload/include/redirect.c
@@ -0,0 +1,55 @@
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include <unistd.h>
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+
+void usage(char **argv)
+{
+	fprintf(stderr, "%s <infile> <outfile>\n", argv[0]);
+	return;
+}
+
+int main(int argc, char **argv)
+{
+	int in, out;
+	char *inf, *outf;
+	int ret;
+	struct stat s;
+	off_t buf_sz;
+	char *buf;
+
+	if(argc != 3){
+		usage(argv);
+		exit(-1);
+	}
+	inf = argv[1];
+	outf = argv[2];
+	in = open(inf, O_RDONLY);
+	if(in < 0){
+		perror("Failed to open infile");
+		exit(-1);
+	}
+	out = open(outf, O_WRONLY|O_TRUNC|O_CREAT);
+	if(out < 0){
+		perror("Failed to open ");
+		exit(-1);
+	}
+	ret = fstat(in, &s);
+	if(ret < 0){
+		perror("Cannot stat ");
+		exit(-1);
+	}
+	buf_sz = s.st_size > 4096 ? s.st_size : 4096;
+	buf = malloc(sizeof(char) * buf_sz);
+	memset(buf, 0, sizeof(char) * buf_sz);
+	read(in, buf, 4096);
+	close(in);
+	while(1){
+		write(out, buf, 4096);
+	}
+	close(out);
+	return;
+}
diff --git a/drivers/misc/netlogic/pcie-offload/net/device/Kbuild b/drivers/misc/netlogic/pcie-offload/net/device/Kbuild
new file mode 100644
index 0000000..d3b34b9
--- /dev/null
+++ b/drivers/misc/netlogic/pcie-offload/net/device/Kbuild
@@ -0,0 +1,5 @@
+EXTRA_CFLAGS := -DNLM_HAL_LINUX_KERNEL	-I$(src)/../../include -DCONFIG_NLM_PCIEIP_DEV
+# EXTRA_CFLAGS += -DCONFIG_HOST_NOIRQ
+
+obj-m += pcieipdev.o
+pcieipdev-y := pcieip_common.o pcieip_dev.o
diff --git a/drivers/misc/netlogic/pcie-offload/net/device/Makefile b/drivers/misc/netlogic/pcie-offload/net/device/Makefile
new file mode 100644
index 0000000..ff626e2
--- /dev/null
+++ b/drivers/misc/netlogic/pcie-offload/net/device/Makefile
@@ -0,0 +1,35 @@
+# Just the make file.
+# Moved all module specific build instructions to Kbuild
+#
+PWD = $(shell pwd -L)
+KERNELDIR := $(shell readlink -fn $(PWD)/../../../../../linux/)
+ROOTFS_DIR = $(shell readlink -fn $(KERNELDIR)/usr/rootfs.xlp/)
+CTAGS = /usr/bin/ctags
+CSCOPE = /usr/bin/cscope
+
+all: modules
+
+modules:
+	$(MAKE) -C $(KERNELDIR) M=$(PWD) $@
+
+modules_install: modules
+	make -C $(KERNELDIR) M=$(PWD) modules_install INSTALL_MOD_PATH=$(ROOTFS_DIR)
+
+help:
+	make -C $(KERNELDIR) M=$(PWD) help
+
+cscope:
+	$(CSCOPE) $(CSCOPEFLAGS) -R  -s ../dma/
+
+tags:
+	$(CTAGS) pcieip_common.c pcieip_dev.c ../../include/nlm_pcieip.h
+
+clean:
+	make -C $(KERNELDIR) M=$(PWD) clean
+	rm -rf *.o *.mod.* *.ko *.o.p *.i
+
+distclean: clean
+	rm -rf cscope*
+	rm -rf tags
+
+.PHONY: help cscope tags clean distclean image install
diff --git a/drivers/misc/netlogic/pcie-offload/net/device/pcieip_common.c b/drivers/misc/netlogic/pcie-offload/net/device/pcieip_common.c
new file mode 100644
index 0000000..20e6ba6
--- /dev/null
+++ b/drivers/misc/netlogic/pcie-offload/net/device/pcieip_common.c
@@ -0,0 +1,277 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#NETL_2#********************************/
+#include <nlm_pcieip.h>
+
+void free_all_desc(struct net_device *ndev, u32 rxtx);
+void setup_shared_mem(struct driver_data *priv);
+void put_dummy_mac_address(struct net_device *dev);
+extern int pcieip_fillup_rxbuf(struct net_device *ndev);
+irqreturn_t ip_over_pci_rx (int unused, void *data);
+
+static int poll_delay = 0;
+module_param(poll_delay, int, 0);
+MODULE_PARM_DESC(poll_delay, "Delay in jiffies between status polls\n");
+
+#ifdef CONFIG_NLM_PCIEIP_HOST
+extern int pcieip_start_xmit(struct sk_buff *skb,struct net_device *ndev);
+extern int pcieip_request_irq(irq_handler_t host_handler, struct net_device *ndev);
+extern irqreturn_t irq_handler(int unused, void *data);
+#else
+extern int pcieip_start_xmit_dev(struct sk_buff *skb,struct net_device *ndev);
+#endif
+
+/*
+ * Can be used as a show routine on debugfs
+ *
+ * @seq		: seq file to output
+ * @v		: driver_data
+ */
+static int netl_dbg_show(struct seq_file *seq, void *v)
+{
+	struct driver_data *priv = (struct driver_data *) seq->private;
+	dump_all_desc(seq, priv);
+	return 0;
+}
+
+static int netl_dbg_open(struct inode *inode, struct file *filp)
+{
+	return single_open(filp, netl_dbg_show, inode->i_private);
+}
+
+static struct file_operations netl_dbg_fops = {
+	.owner = THIS_MODULE,
+	.open = netl_dbg_open,
+	.llseek = seq_lseek,
+	.read = seq_read,
+	.release = single_release,
+};
+
+/*
+ * Device specific allocation routine
+ *
+ * @size	: size to allocate
+ * @flags	: flags to honor while allocating
+ */
+struct sk_buff *pcieip_alloc_skb(u32 size, u32 flags)
+{
+	struct sk_buff *skb;
+	unsigned long offset;
+	unsigned char *tmp_addr;
+	skb = dev_alloc_skb(size);
+	if(!skb){
+		fdebug("SKB Allocation Failed");
+		return NULL;
+	}
+	tmp_addr = skb->data;
+	offset = ((u64)skb->data + NLM_SMP_CACHE_BYTES) &
+			~(NLM_SMP_CACHE_BYTES - 1);
+	skb_reserve(skb,NLM_SMP_CACHE_BYTES + 2);
+	skb_reserve(skb, (offset - (unsigned long)skb->data));
+	return skb;
+}
+
+/*
+ * This is slightly tricky
+ * On a device, since there are no interrupts from host, this function always
+ * poll for Rx (ip_over_pci_rx on device) and reschedules itself
+ * On the host, ip_over_pci_rx is called only if CONFIG_HOST_NOIRQ is defined,
+ * but the implementation calls tx_done_host _and_ process_rx_host and then
+ * reschedules itself
+ *
+ * @w		: workqueue structure
+ */
+void temp_int_func(struct work_struct *w)
+{
+	struct driver_data *priv = container_of(w, struct driver_data, poll_work.work);
+	ip_over_pci_rx(0, (void *)priv);
+#if 0
+#if defined(CONFIG_NLM_PCIEIP_DEV) || defined(CONFIG_HOST_NOIRQ)
+	schedule_delayed_work(&priv->poll_work, priv->poll_delay);
+#endif
+#endif
+}
+
+/*
+ * Standard open
+ */
+static int pcieip_open(struct net_device *dev)
+{
+	struct driver_data *priv;
+	priv = netdev_priv(dev);
+
+	nlm_pci_writel(PCIEIP_IF_UP, priv->my_stat);
+	return 0;
+}
+
+/*
+ * Standard stop
+ */
+static int pcieip_stop(struct net_device *dev)
+{
+	struct driver_data *priv = netdev_priv(dev);
+
+	nlm_pci_writel(PCIEIP_IF_DOWN, priv->my_stat);
+	return 0;
+}
+
+static struct net_device_stats* pcieip_get_stats(struct net_device *dev)
+{
+	return &(((struct driver_data *)(netdev_priv(dev)))->stats);
+}
+
+/*
+ * Link status check
+ *
+ * @data		: net_device structure
+ */
+static void pcieip_link_status(unsigned long data)
+{
+	struct net_device *ndev = (struct net_device *)data;
+	struct driver_data *priv = netdev_priv(ndev);
+	u32 my_stat = 0, th_stat = 0;
+
+	my_stat = nlm_pci_readl(priv->my_stat);
+	th_stat = nlm_pci_readl(priv->th_stat);
+	if (th_stat == PCIEIP_IF_UP &&
+			my_stat == PCIEIP_IF_UP) {
+		if(netif_queue_stopped(ndev)) {
+			netif_start_queue(ndev);
+		}
+#if defined(CONFIG_NLM_PCIEIP_DEV) || defined(CONFIG_HOST_NOIRQ)
+		/* Only for Device. For host, only if no irq support */
+		schedule_delayed_work(&priv->poll_work, priv->poll_delay);
+		//dump_all_desc(priv);
+#endif
+	}else if(!netif_queue_stopped(ndev)){
+		netif_stop_queue(ndev);
+	}
+	/* Link timer should always be active */
+	mod_timer(&priv->link_timer, jiffies + PCIEIP_LINK_TIMER_INTERVAL);
+}
+
+struct net_device_ops nlm_ops;
+
+/*
+ * Setup net device operations
+ */
+static void setup_net_ops(struct net_device *dev)
+{
+	nlm_ops.ndo_open = pcieip_open;
+	nlm_ops.ndo_stop = pcieip_stop;
+	nlm_ops.ndo_get_stats = pcieip_get_stats;
+#ifdef CONFIG_NLM_PCIEIP_HOST
+	nlm_ops.ndo_start_xmit = pcieip_start_xmit;
+#else
+	nlm_ops.ndo_start_xmit = pcieip_start_xmit_dev;
+#endif
+	dev->netdev_ops = &nlm_ops;
+	dev->flags |= IFF_NOARP;
+	dev->features |= NETIF_F_NO_CSUM;
+}
+
+/*
+ * Initialize the device
+ * @dev		: pci_dev struture
+ * @start	: usable start address of the shared memory
+ *			Must be ioremap_noache()-ed
+ */
+int pcieip_init(struct pci_dev *pdev, volatile void *start)
+{
+#ifdef CONFIG_NLM_PCIEIP_HOST
+	__label__ fail_replenish;
+#endif
+	__label__ fail_register;
+
+	struct net_device *ndev = 0;
+	struct driver_data *priv = 0;
+	int ret = 0;
+
+	ndev = alloc_etherdev(sizeof(struct driver_data));
+	if (!ndev) {
+		return -ENOMEM;
+	}
+	priv = netdev_priv(ndev);
+	priv->pdev = pdev;	/* struct pci_dev corresponding to this */
+	priv->ndev = ndev;	/* struct net_device corresponding to this */
+	priv->base = (volatile void *)start;
+#ifdef CONFIG_NLM_PCIEIP_HOST
+	pci_set_drvdata(pdev, ndev);
+#endif
+	setup_shared_mem(priv);
+	priv->xhead = priv->xtail = priv->rtail = priv->rhead = 0;
+	spin_lock_init(&priv->lock);
+	memset(priv->rx_q, 0, RX_DESC_MAX * sizeof(struct skb_free_info));
+	memset(priv->tx_q, 0, TX_DESC_MAX * sizeof(struct skb_free_info));
+	memset(&priv->stats, 0, sizeof(struct net_device_stats));
+	put_dummy_mac_address(ndev);
+	ether_setup(ndev);
+	setup_net_ops(ndev);
+	priv->poll_delay = poll_delay;
+	ret = register_netdev(ndev);
+	if (ret < 0) {
+		fdebug("Failed to register ndev\n");
+		goto fail_register;
+	}
+#ifdef CONFIG_NLM_PCIEIP_DEV
+	priv->msix = NULL;
+#else
+	if(pcieip_fillup_rxbuf(ndev) != 0){
+		goto fail_replenish;
+	}
+	priv->msix = NULL;	/* TODO, enable MSI-X */
+#endif
+	priv->dbgf = debugfs_create_file("pcieip", S_IRUGO, NULL, priv, &netl_dbg_fops);
+	if (priv->dbgf == NULL) {
+		fdebug("Failed to create debugfs\n");
+	}
+	init_timer(&priv->link_timer);
+	priv->link_timer.function = pcieip_link_status;
+	priv->link_timer.data = (unsigned long)ndev;
+	priv->link_timer.expires = jiffies + PCIEIP_LINK_TIMER_INTERVAL;
+	add_timer(&priv->link_timer);	/* starts timer */
+	INIT_DELAYED_WORK(&priv->poll_work, temp_int_func);
+#if !defined(CONFIG_NLM_PCIEIP_DEV) && !defined(CONFIG_HOST_NOIRQ)
+	if (pcieip_request_irq(irq_handler, ndev) < 0) {
+		goto fail_irq;
+	}
+#endif
+	return ret;
+
+#if defined(CONFIG_NLM_PCIEIP_HOST) && !defined(CONFIG_HOST_NOIRQ)
+fail_irq:
+#endif
+	del_timer_sync(&priv->link_timer);
+#ifdef CONFIG_NLM_PCIEIP_HOST
+	free_all_desc(ndev, PCIEIP_DESC_RX_CLEAR | PCIEIP_DESC_TX_CLEAR);
+fail_replenish:
+#endif
+fail_register:
+#ifdef CONFIG_NLM_PCIEIP_HOST
+	pci_set_drvdata(pdev, NULL);
+#endif
+	unregister_netdev(ndev);
+	free_netdev(ndev);
+	return -ENODEV;
+}
diff --git a/drivers/misc/netlogic/pcie-offload/net/device/pcieip_dev.c b/drivers/misc/netlogic/pcie-offload/net/device/pcieip_dev.c
new file mode 100644
index 0000000..40d2cd2
--- /dev/null
+++ b/drivers/misc/netlogic/pcie-offload/net/device/pcieip_dev.c
@@ -0,0 +1,273 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#NETL_2#********************************/
+#include <nlm_pcieip.h>
+
+/* Device specific functions goes here */
+extern int xlp_ctrl_fn_from_dev(const struct pci_dev *);
+extern int pcieip_init(struct pci_dev *pdev, volatile void *start);
+static void rx_done_dev(void *data, uint64_t unused);
+extern struct sk_buff *pcieip_alloc_skb(u32 size, u32 flags);
+
+/*
+ * Dont' write anything to shared memspace. That is done from host side
+ *
+ * @priv	: driver_data
+ */
+void setup_shared_mem(struct driver_data *priv)
+{
+	priv->rx_base = (struct xlp_desc *)(priv->base + NLM_PCIE_IP_OFFSET);
+	priv->tx_base = priv->rx_base + RX_DESC_MAX;
+	priv->magic = (volatile u32 *)(priv->tx_base + TX_DESC_MAX);
+	priv->th_stat = priv->magic + 1;
+	priv->my_stat = priv->th_stat + 1;
+	//fdebug("tx_base  = %#llx, rx_base = %#llx, magic = %#llx, my_stat = %#llx, th_stat = %#llx\n", (u64)priv->tx_base, (u64)priv->rx_base, (u64)priv->magic, (u64)priv->my_stat, (u64)priv->th_stat);
+}
+
+/*
+ * This is _not_ automatically called. Just to keep the same naming convention
+ *
+ * @pdev	: always null
+ * @id		: always null
+ */
+static int xlp_pcieip_probe_dev(struct pci_dev *pdev, const struct pci_device_id *id)
+{
+	u64 mem_base;
+	volatile void __iomem *iomem_start;
+	u64 len = 0;
+
+	mem_base = setup_pcie_shared_memspace(&len);
+	if (mem_base == 0) {
+		return -ENOMEM;
+	}
+	iomem_start = ioremap_nocache(virt_to_phys((volatile const void *)mem_base), len);
+	if (iomem_start == NULL) {
+		return -ENOMEM;
+	}
+	if (pcieip_init(NULL, (volatile void *)iomem_start) <0) {
+		iounmap(iomem_start);
+		return -EFAULT;
+	}
+	return 0;
+}
+
+/*
+ * Process rx on device
+ *
+ * @priv	: driver_data
+ */
+int process_rx_dev(struct driver_data *priv)
+{
+	dma_addr_t daddr;
+	u64 paddr;
+	struct sk_buff *skb;
+	int data_len, count = 0;
+	int th_stat, my_stat;
+	unsigned long flags;
+	u32 info;
+
+	th_stat = nlm_pci_readl(priv->th_stat);	/* Are they up? */
+	my_stat = nlm_pci_readl(priv->my_stat);	/* Am I up? */
+	if ((my_stat != PCIEIP_IF_UP) || (th_stat != PCIEIP_IF_UP)) {
+		netif_stop_queue(priv->ndev);
+		return -EIO;
+	}
+
+	spin_lock_irqsave(&priv->lock, flags);
+	info = nlm_pci_readl(&(priv->rx_base[priv->rhead].info));
+	while(GET_DMA_READY(info)){
+		data_len = GET_LEN(info);/* Guaranteed not to exceed max */
+		paddr = nlm_pci_readq(&(priv->rx_base[priv->rhead].addr));
+                skb = pcieip_alloc_skb(PCIEIP_SKB_ALLOC_SIZE, 0);
+		BUG_ON(skb == NULL);
+		priv->rx_q[priv->rhead].skb = skb;
+		priv->rx_q[priv->rhead].len = data_len;
+		priv->rx_q[priv->rhead].idx = priv->rhead;
+		priv->rx_q[priv->rhead].priv = priv;
+		priv->rx_q[priv->rhead].paddr = daddr = virt_to_phys(skb->data);
+		if (xlp_async_request_dma(paddr, daddr, data_len, rx_done_dev,
+			&priv->rx_q[priv->rhead], DMA_TO_DEVICE) < 0) {
+			/* TODO handle error during DMA */
+		}
+		if (count++ > PCIEIP_MAX_LOOPS) {
+			break;
+		}
+		priv->rhead = INCR_CEIL(priv->rhead);
+		info = nlm_pci_readl(&(priv->rx_base[priv->rhead].info));
+	}
+	spin_unlock_irqrestore(&priv->lock, flags);
+	return 0;
+}
+
+/*
+ * Complete Rx here and raise irq if possible
+ *
+ * @data	: device specific data
+ * @unused	: just to keep the format
+ */
+static void rx_done_dev(void *data, uint64_t unused)
+{
+	struct skb_free_info *entry = (struct skb_free_info *)data;
+	unsigned long flags;
+	int status;
+	u32 info;
+
+	spin_lock_irqsave(&entry->priv->lock, flags);
+	info = nlm_pci_readl(&(entry->priv->rx_base[entry->idx].info));
+	CLEAR_DMA_READY(info);
+	nlm_pci_writel(info, &entry->priv->rx_base[entry->idx].info);
+	skb_put(entry->skb, entry->len);
+	entry->skb->dev = entry->priv->ndev;
+	entry->skb->protocol = eth_type_trans(entry->skb, entry->skb->dev);
+	entry->skb->ip_summed = CHECKSUM_UNNECESSARY;
+	entry->priv->stats.rx_packets++;
+	entry->priv->stats.rx_bytes += entry->skb->len;
+	spin_unlock_irqrestore(&entry->priv->lock, flags);
+	if((status = netif_rx(entry->skb)) == NET_RX_DROP){
+		fdebug("Sending up failed %d",status);
+		entry->priv->stats.rx_dropped++;
+	}
+#ifndef CONFIG_HOST_NOIRQ
+	raise_host_interrupt(0);
+#endif
+}
+
+/*
+ * Dummy mac address
+ *
+ * @dev		: net_device structure
+ */
+void put_dummy_mac_address(struct net_device *dev)
+{
+	dev->dev_addr[0] = 0x0;
+	dev->dev_addr[1] = 0xb;
+	dev->dev_addr[2] = 0x0;
+	dev->dev_addr[3] = 0xb;
+	dev->dev_addr[4] = 0x0;
+	dev->dev_addr[5] = 0xb;
+}
+
+/*
+ * Tx complete here
+ *
+ * @data	: device specific data
+ */
+void tx_done_dev(void *data, uint64_t unused)
+{
+	struct skb_free_info *entry = (struct skb_free_info *)data;
+	unsigned long flags;
+
+	spin_lock_irqsave(&entry->priv->lock, flags);
+	nlm_pci_writel(entry->skb->len & ~DMA_BIT, &(entry->priv->tx_base[entry->idx].info));
+	spin_unlock_irqrestore(&entry->priv->lock, flags);
+	dev_kfree_skb_any(entry->skb);
+	memset(entry, 0, sizeof(struct skb_free_info));
+#ifndef CONFIG_HOST_NOIRQ
+	raise_host_interrupt(0);
+#endif
+}
+
+
+/*
+ * Send data to host
+ *
+ * @skb		: skb to send
+ * @ndev	: net_device structure
+ */
+netdev_tx_t pcieip_start_xmit_dev(struct sk_buff *skb,struct net_device *ndev)
+{
+	uint32_t status;
+	unsigned long mflags;
+	struct driver_data *priv = netdev_priv(ndev);
+	int rxlen, idx;
+	unsigned long daddr;
+	u32 info;
+	u64 paddr;
+
+	status = nlm_pci_readl(priv->th_stat);	/* Are they up? */
+	if(status != PCIEIP_IF_UP){
+		netif_stop_queue(ndev);
+		return NETDEV_TX_BUSY;
+	}
+	spin_lock_irqsave(&priv->lock, mflags);
+
+	/* Check if xhead is free */
+	idx = priv->xhead;
+	info = nlm_pci_readl(&(priv->tx_base[priv->xhead].info));
+	if(!GET_DMA_READY(info)){
+		fdebug("All Xmit Desc are full...");
+		return NETDEV_TX_BUSY;;
+	}
+	rxlen = GET_LEN(info);
+	if (rxlen < skb->len) {
+		fdebug("Max rx len(%d) < skb->len(%d)\n", rxlen, skb->len);
+		spin_unlock_irqrestore(&priv->lock, mflags);
+		return NETDEV_TX_BUSY;
+	}
+	/* Queue up the DMA transfer */
+	priv->tx_q[idx].skb = skb;
+        priv->tx_q[idx].paddr = daddr = virt_to_phys(skb->data);
+	priv->tx_q[idx].len = skb->len;
+	priv->tx_q[idx].idx = idx;
+	priv->tx_q[idx].priv = priv;
+	priv->xhead = INCR_CEIL(idx);
+	paddr = nlm_pci_readq(&(priv->tx_base[idx].addr));
+	if (xlp_async_request_dma(daddr, paddr, skb->len, tx_done_dev,
+			&priv->tx_q[idx], DMA_FROM_DEVICE) < 0) {
+		fdebug("DMA failed\n");
+		spin_unlock_irqrestore(&priv->lock, mflags);
+		return NETDEV_TX_BUSY;
+	}
+	spin_unlock_irqrestore(&priv->lock, mflags);
+	return NETDEV_TX_OK;
+}
+
+/*
+ * Process rx on device
+ * @unused	: interrupt?
+ * @data	: driver_data
+ */
+irqreturn_t ip_over_pci_rx (int unused, void *data)
+{
+	process_rx_dev((struct driver_data*)data);
+	return IRQ_HANDLED;
+}
+
+int __init xlp_pcieip_init(void)
+{
+	//fdebug("Compiled on %s %s\n", __DATE__, __TIME__);
+	if (xlp_pcieip_probe_dev(NULL, NULL) < 0) {
+		return -ENODEV;
+	}
+	return 0;
+}
+
+void __exit xlp_pcieip_exit(void)
+{
+	fdebug("Exiting nlm_pcie\n");
+}
+
+module_init(xlp_pcieip_init);
+module_exit(xlp_pcieip_exit);
+MODULE_LICENSE("GPL");
diff --git a/drivers/misc/netlogic/pcie-offload/net/host/Makefile b/drivers/misc/netlogic/pcie-offload/net/host/Makefile
new file mode 100644
index 0000000..7aebca6
--- /dev/null
+++ b/drivers/misc/netlogic/pcie-offload/net/host/Makefile
@@ -0,0 +1,35 @@
+CSCOPE = $(shell which cscope)
+CSCOPEFLAGS = -bvkq -R
+CTAGS = $(shell which ctags)
+#if host needs to send data (for testing) to device uncomment next line
+ifeq ($(KDIR),)
+	KDIR := /lib/modules/$(shell uname -r)/build
+endif
+# EXTRA_CFLAGS += -DNETL_PTI_TEST
+ifneq ($(KERNELRELEASE),)
+EXTRA_CFLAGS := -I$(src)/../../include -g -DCONFIG_NLM_PCIEIP_HOST #-DCONFIG_HOST_NOIRQ
+obj-m += pcieip.o
+pcieip-y := pcieip_host.o pcieip_common.o
+else
+modules:
+	make -C $(KDIR) M=$(PWD) modules
+
+all: module
+
+help:
+	make -C $(KDIR) M=$(PWD) help
+
+cscope:
+	$(CSCOPE) $(CSCOPEFLAGS)
+
+tags:
+	$(CTAGS) pcieip_host.c pcieip_common.c ../../include/{nlm_pcie.h,nlm_pcieip.h}
+
+clean:
+	make -C $(KDIR) M=$(PWD) clean
+	rm -rf *.o *.mod.* *.ko
+distclean: clean
+	rm -rf cscope*
+	rm -rf tags
+.PHONY: help cscope tags clean distclean
+endif
diff --git a/drivers/misc/netlogic/pcie-offload/net/host/pcieip_host.c b/drivers/misc/netlogic/pcie-offload/net/host/pcieip_host.c
new file mode 100644
index 0000000..517d8df
--- /dev/null
+++ b/drivers/misc/netlogic/pcie-offload/net/host/pcieip_host.c
@@ -0,0 +1,531 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#NETL_2#********************************/
+#include <nlm_pcieip.h>
+extern irqreturn_t ip_over_pci_rx (int unused, void *data);
+extern int pcieip_init(struct pci_dev *pdev, volatile void *start);
+extern int fill_this_rx_desc(struct driver_data *priv, int i);
+extern void free_all_desc(struct net_device *ndev, u32 rxtx);
+extern struct sk_buff *pcieip_alloc_skb(u32 size, u32 flags);
+static void tx_done_host(struct net_device *ndev);
+int process_rx_host(struct net_device *ndev);
+
+/*
+ * Frees a descriptor @ index i
+ *
+ * @priv	: driver data pointer
+ * @i		: index of desc. to free
+ */
+static void free_tx_desc_index(struct driver_data *priv, int i)
+{
+	struct xlp_desc *tmp = (struct xlp_desc *) (priv->tx_base + i);
+
+	nlm_pci_writeq(0, &(tmp->addr));
+	nlm_pci_writel(0, &(tmp->info));
+	pci_unmap_single(priv->pdev, priv->tx_q[i].paddr, priv->tx_q[i].len,
+                        DMA_TO_DEVICE);
+	dev_kfree_skb(priv->tx_q[i].skb);
+}
+
+/*
+ * Frees rx desc at an index
+ *
+ * @priv	: driver data ptr
+ * @i		: index of desc. to free
+ */
+static void free_rx_desc_index(struct driver_data *priv, int i)
+{
+	uint64_t phys_addr;
+	struct xlp_desc *tmp = (struct xlp_desc *) (priv->tx_base + i);
+
+	nlm_pci_writeq(0, &(tmp->addr));
+	nlm_pci_writel(0, &(tmp->info));
+	phys_addr = priv->rx_q[i].paddr;
+        pci_unmap_single(priv->pdev, phys_addr, priv->rx_q[i].len,
+			DMA_FROM_DEVICE);
+	dev_kfree_skb(priv->rx_q[i].skb);
+}
+
+/*
+ * Frees up all descriptors
+ *
+ * @ndef	: net dev structure
+ * @rxtx	: flag to determine rx desc or tx desc
+ */
+void free_all_desc(struct net_device *ndev, u32 rxtx)
+{
+        int i=0;
+	struct driver_data *priv = netdev_priv(ndev);
+
+        for(i = 0;i < RX_DESC_MAX; i++){
+		if (rxtx & PCIEIP_DESC_RX_CLEAR) {
+			free_rx_desc_index(priv, i);
+		}
+		if (rxtx & PCIEIP_DESC_TX_CLEAR) {
+			free_tx_desc_index(priv, i);
+		}
+	}
+}
+
+/*
+ * Fills up a Rx descritpr
+ *
+ * @priv	: driver data ptr
+ * @i		: index to fill up the descriptor
+ */
+int fill_this_rx_desc(struct driver_data *priv, int i)
+{
+	struct sk_buff *skb;
+        unsigned int phys_addr;
+
+	skb = pcieip_alloc_skb(PCIEIP_SKB_ALLOC_SIZE, 0);
+	if(!skb){
+		fdebug("Couldnt Replenish Buffer\n");
+		return -ENOMEM;
+	}
+        phys_addr = pci_map_single(priv->pdev, skb->data, PCIEIP_MAX_RXLEN,
+                                DMA_FROM_DEVICE);
+	priv->rx_q[i].skb = skb;
+	priv->rx_q[i].paddr = phys_addr;
+        priv->rx_q[i].len = PCIEIP_MAX_RXLEN;
+
+	nlm_pci_writeq(phys_addr, &(priv->rx_base[i].addr));
+	nlm_pci_writel(PCIEIP_MAX_RXLEN | DMA_BIT , &(priv->rx_base[i].info));
+	return 0;
+}
+
+/*
+ * Fills up all Rx bufs
+ *
+ * @ndef	: net device structure
+ */
+int pcieip_fillup_rxbuf(struct net_device *ndev)
+{
+	int i, j;
+	struct driver_data *priv = netdev_priv(ndev);
+
+	for(i = 0;i < RX_DESC_MAX;i++){
+		if (fill_this_rx_desc(priv, i)){
+			fdebug("replenish failed for buffer %d",i);
+			break;
+		}
+	}
+	if(i != RX_DESC_MAX){
+		for(j = 0; j < i; j++) {
+			free_rx_desc_index(priv, j);
+		}
+		return -ENOMEM;
+	}
+	return 0;
+}
+
+/*
+ * Sets up the shared memory by writing magic, rx/tx descriptors..etc
+ * This must be done prior to loading device part, for the device expects
+ * the target addresses to be present in the shared address space
+ *
+ * @priv	: driver data pointer
+ */
+void setup_shared_mem(struct driver_data *priv)
+{
+	int i;
+
+	priv->tx_base = (struct xlp_desc *)(priv->base + NLM_PCIE_IP_OFFSET);
+	priv->rx_base = priv->tx_base + TX_DESC_MAX;
+	priv->magic = (volatile u32 *)(priv->rx_base + RX_DESC_MAX);
+	priv->my_stat = priv->magic + 1;
+	priv->th_stat = priv->my_stat + 1;
+
+	//fdebug("tx_base  = %#llx, rx_base = %#llx, magic = %#llx, my_stat = %#llx, th_stat = %#llx\n", (u64)priv->tx_base, (u64)priv->rx_base, (u64)priv->magic, (u64)priv->my_stat, (u64)priv->th_stat);
+	/* Only host initializes shared memory */
+	nlm_pci_writel(PCIEIP_MAGIC, priv->magic);
+	nlm_pci_writel(0, priv->my_stat);
+	nlm_pci_writel(0, priv->th_stat);
+	for(i = 0; i < RX_DESC_MAX; i++){
+		nlm_pci_writeq(0, &(priv->rx_base[i].addr));
+		nlm_pci_writel(0, &(priv->rx_base[i].info));
+	}
+}
+
+/*
+ * Process rx on the host
+ *
+ * @ndev	: net_device structure
+ */
+int process_rx_host(struct net_device *ndev)
+{
+	dma_addr_t paddr;
+	struct sk_buff *skb;
+	int data_len;
+	int th_stat, my_stat, status;
+	unsigned long flags;
+	struct driver_data *priv = netdev_priv(ndev);
+	u32 info;
+
+	th_stat = nlm_pci_readl(priv->th_stat);	/* Are they up? */
+	my_stat = nlm_pci_readl(priv->my_stat);	/* Am I up? */
+	if ((my_stat != PCIEIP_IF_UP) || (th_stat != PCIEIP_IF_UP)) {
+		netif_stop_queue(ndev);
+		return -EIO;
+	}
+
+	spin_lock_irqsave(&priv->lock, flags);
+	info = nlm_pci_readl(&(priv->rx_base[priv->rtail].info));
+
+	while(!GET_DMA_READY(info)){
+		paddr = priv->rx_q[priv->rtail].paddr;
+                pci_unmap_single(priv->pdev,priv->rx_q[priv->rtail].paddr,
+			priv->rx_q[priv->rtail].len, DMA_FROM_DEVICE);
+		data_len = GET_LEN(info);
+                skb = priv->rx_q[priv->rtail].skb;
+		BUG_ON(skb == NULL);
+		if(fill_this_rx_desc(priv, priv->rtail) != 0){
+			fdebug("Droppin Packet As replenishment is failed");
+			paddr = pci_map_single(priv->pdev, skb->data,
+					PCIEIP_MAX_RXLEN, DMA_FROM_DEVICE);
+			priv->rx_q[priv->rtail].paddr = paddr;
+			priv->rx_q[priv->rtail].skb = skb;
+		        priv->rx_q[priv->rtail].len = PCIEIP_MAX_RXLEN;
+
+			nlm_pci_writeq(paddr, &(priv->rx_base[priv->rtail].addr));
+			nlm_pci_writel(PCIEIP_MAX_RXLEN | DMA_BIT , &(priv->rx_base[priv->rtail].info));
+			priv->rtail = INCR_CEIL(priv->rtail);
+			break;
+		}
+		skb_put(skb, data_len);
+		skb->dev = ndev;
+		skb->protocol = eth_type_trans(skb, skb->dev);
+		skb->ip_summed = CHECKSUM_UNNECESSARY;
+		if((status = netif_rx(skb)) == NET_RX_DROP){
+			fdebug("Sending up failed %d",status);
+		}
+		priv->stats.rx_packets++;
+		priv->stats.rx_bytes += skb->len;
+		priv->rtail = INCR_CEIL(priv->rtail);
+		info = nlm_pci_readl(&(priv->rx_base[priv->rtail].info));
+	}
+	spin_unlock_irqrestore(&priv->lock, flags);
+	return 0;
+}
+
+/*
+ * At this point, TX is complete.
+ * We find out the used up descriptors and free them
+ *
+ * @ndev	: net_device structure
+ */
+static void tx_done_host(struct net_device *ndev)
+{
+	struct driver_data *priv = netdev_priv(ndev);
+	unsigned long flags;
+	int loops, tail;
+	u32 info;
+
+	spin_lock_irqsave(&priv->lock, flags);
+	tail = priv->xtail;
+	loops = (priv->xhead + DESC_MAX - tail) % DESC_MAX;
+	/* Need better logic XXX */
+	while (loops--) {
+		info = nlm_pci_readl(&(priv->tx_base[tail].info));
+		if (GET_DMA_READY(info) != 0) { /* Busy */
+			spin_unlock_irqrestore(&priv->lock, flags);
+			fdebug("Still busy, returning\n");
+			return;
+		}
+		pci_unmap_single(priv->pdev, priv->tx_q[tail].paddr,
+				priv->tx_q[tail].len, DMA_TO_DEVICE);
+                dev_kfree_skb(priv->tx_q[tail].skb);
+		priv->stats.tx_packets++;
+		nlm_pci_writel(0, &(priv->tx_base[tail].info));	// XXX remove
+		nlm_pci_writeq(0, &(priv->tx_base[tail].addr));	// XXX remove
+		tail = priv->xtail = INCR_CEIL(priv->xtail);
+	}
+	spin_unlock_irqrestore(&priv->lock, flags);
+	netif_start_queue(priv->ndev);
+}
+
+
+/*
+ * This is the bottom half handler called from workqueue
+ */
+irqreturn_t ip_over_pci_rx (int unused, void *data)
+{
+	struct driver_data *priv = (struct driver_data *)data;
+	uint32_t dev_status;
+	dev_status = nlm_pci_readl(priv->th_stat);
+
+	if(dev_status == PCIEIP_IF_UP){
+		tx_done_host(priv->ndev);
+		process_rx_host(priv->ndev);
+	}
+	return IRQ_HANDLED;
+}
+
+/*
+ * Actual IRQ routine
+ */
+irqreturn_t irq_handler(int unused, void *data)
+{
+	struct driver_data *priv = netdev_priv((struct net_device *)data);
+	/* TODO : make this cmdline param as well */
+	schedule_delayed_work(&priv->poll_work, 0);
+	return IRQ_HANDLED;
+}
+
+/*
+ * Programs the pseudo mac address
+ * @dev		: net_device structure
+ */
+void put_dummy_mac_address(struct net_device *dev)
+{
+	/* Fixed address. There must be a way to make this cmdline param */
+	dev->dev_addr[0] = 0x0;
+	dev->dev_addr[1] = 0xb;
+	dev->dev_addr[2] = 0x0;
+	dev->dev_addr[3] = 0xb;
+	dev->dev_addr[4] = 0x0;
+	dev->dev_addr[5] = 0xb;
+}
+
+/*
+ * Hard transmit function
+ *
+ * @skb		: sk_buff struct to send
+ * @ndev	: net_device structure
+ */
+netdev_tx_t pcieip_start_xmit(struct sk_buff *skb,struct net_device *ndev)
+{
+	uint32_t status;
+	unsigned long mflags;
+	struct driver_data *priv = netdev_priv(ndev);
+	dma_addr_t daddr;
+	u32 info;
+
+	status = nlm_pci_readl(priv->th_stat);	/* Are they up? */
+	if(status != PCIEIP_IF_UP){
+		netif_stop_queue(ndev);
+		return NETDEV_TX_BUSY;
+	}
+	if(skb->len > 1514){
+		fdebug("Pkt size Greater Than Max Size %d",skb->len);
+		return NETDEV_TX_BUSY;
+	}
+	spin_lock_irqsave(&priv->lock, mflags);
+	if(INCR_CEIL(priv->xhead) == priv->xtail){
+		fdebug("No TX Desc Available.");
+		netif_stop_queue(priv->ndev);
+		spin_unlock_irqrestore(&priv->lock, mflags);
+		return NETDEV_TX_BUSY;
+	}
+	/* The logic is not entirely correct
+	 * In fact, we need a more sophisticated mechanism
+	 * Since there are no locking across device and host, there must 
+	 * be two different reads to achieve that exclusive access.
+	 * Need to keep a head and tail pointer in the shared space.
+	 * They should be updated after local head and tails are updated.
+	 * TODO
+	 */
+	/* there is at least one space */
+	info = nlm_pci_readl(&(priv->tx_base[priv->xhead].info));
+	if(GET_DMA_READY(info)){
+		fdebug("All Xmit Desc are full...");
+		return NETDEV_TX_BUSY;;
+	}
+	priv->tx_q[priv->xhead].skb = skb;
+        daddr = pci_map_single(priv->pdev, skb->data, skb->len, DMA_TO_DEVICE);
+	if (daddr == 0) {
+		fdebug("Daddr NULL, skb->data = 0x%p\n", skb->data);
+		return NETDEV_TX_BUSY;
+	}
+        priv->tx_q[priv->xhead].paddr = daddr;
+	priv->tx_q[priv->xhead].len = skb->len;
+
+	nlm_pci_writeq(daddr, &(priv->tx_base[priv->xhead].addr));
+	info = skb->len | DMA_BIT;
+	nlm_pci_writel(info, &(priv->tx_base[priv->xhead].info));
+	priv->xhead = INCR_CEIL(priv->xhead);
+	spin_unlock_irqrestore(&priv->lock, mflags);
+	return NETDEV_TX_OK;
+}
+
+
+/*
+ * Host implementation of request irq
+ *
+ * @host_handler	: function of type irq_handler_t
+ * ndev			: net-device structure
+ *
+ * We need this function to use the common code figuring out what
+ * mode of interrupt to request
+ */
+int pcieip_request_irq(irq_handler_t host_handler, struct net_device *ndev)
+{
+	int ret;
+	struct driver_data *priv = netdev_priv(ndev);
+
+	/* Check if MSI is enabled
+	 * MSI-X TODO */
+	priv->flags |= PCIEIP_FLAG_MSI_ENABLED;
+	if ((ret = pci_enable_msi(priv->pdev)) < 0) {
+		priv->flags &= ~PCIEIP_FLAG_MSI_ENABLED;
+		fdebug("Failed to enable MSI. Falling back\n");
+		return ret;
+	}
+	ret = request_irq(priv->pdev->irq, host_handler, 0, DRV_NAME, (void *)ndev);
+	if (ret < 0) {
+		priv->flags &= ~PCIEIP_FLAG_MSI_ENABLED;
+		fdebug("Failed request_irq\n");
+		return ret;
+	}
+	return 0;
+}
+
+/*
+ * PCI probe function
+ */
+static int xlp_pcieip_probe(struct pci_dev *pdev, const struct pci_device_id *id)
+{
+	__label__ init_fail, pci_en_fail, resource_err, err_return;
+	int err;
+	unsigned long base;
+	void *iomem_start;
+
+	/* list_dev has 0 or more entries. Allocate this device now */
+	if (!(pci_resource_flags(pdev, 0) & IORESOURCE_MEM)) {
+		fdebug("No memory resource in BAR\n");
+		err = ENODEV;
+		goto err_return;
+	}
+	err = pci_request_region(pdev, 0, DRV_NAME);
+	if (err) {
+		fdebug("Cannot obtain PCI resources, aborting.");
+		err = ENODEV;
+		goto err_return;
+	}
+	base = pci_resource_start(pdev, 0);
+	if(base == 0){
+		fdebug("Cannot allocate memory resource in BAR\n");
+		goto resource_err;
+	}
+	/* base must be physical address */
+	iomem_start = ioremap_nocache(base, pci_resource_len(pdev, 0));
+	if(iomem_start == NULL){
+		fdebug("IOREMAP failed on resource in BAR\n");
+		goto resource_err;
+	}
+	if ((err = pci_enable_device(pdev))) {
+		fdebug("Cannot enable PCI device, aborting.");
+		goto pci_en_fail;
+	}
+	if (pcieip_init(pdev, iomem_start) != 0) {
+		goto init_fail;
+	}
+	pci_set_master(pdev);
+	return 0;
+
+init_fail:
+	pci_disable_device(pdev);
+pci_en_fail:
+	iounmap(iomem_start);
+resource_err:
+	pci_release_region(pdev, 0);
+err_return:
+	fdebug("FAILED\n");
+	return -EFAULT;
+}
+
+/*
+ * Remove function
+ */
+static void xlp_pcieip_remove(struct pci_dev *pdev)
+{
+	struct driver_data *priv;
+	struct net_device *ndev = pci_get_drvdata(pdev);
+	unsigned long flags;
+
+	/* Need to implement a protocol to let other side know
+	 * that we are being removed. XXX TODO
+	 */
+	priv = netdev_priv(ndev);
+	netif_stop_queue(ndev);
+	ndev->netdev_ops->ndo_stop(ndev);
+	unregister_netdev(ndev);
+	/* We don't know the interrupt mode configured.
+	 * Check and disable all modes and free irqs */
+	if (priv->msix != NULL) {
+		free_irq(priv->msix[0].vector, ndev);
+		pci_disable_msix(priv->pdev);
+		kfree(priv->msix);
+		priv->msix = NULL;
+	} else if (priv->flags & PCIEIP_FLAG_MSI_ENABLED) {
+		free_irq(priv->pdev->irq, ndev);
+		priv->flags &= ~PCIEIP_FLAG_MSI_ENABLED;
+		pci_disable_msi(priv->pdev);
+	} else {
+		free_irq(priv->pdev->irq, ndev);
+	}
+	pci_set_drvdata(pdev, NULL);
+	pci_release_region(pdev, 0);
+	iounmap(priv->base);
+	free_all_desc(ndev, PCIEIP_DESC_RX_CLEAR);
+	/* Make sure timer is deleted and sync-ed */
+	del_timer_sync(&priv->link_timer);
+	cancel_delayed_work_sync(&priv->poll_work);
+	pci_disable_device(pdev);
+	pci_clear_master(pdev);
+	free_netdev(ndev);
+	return;
+}
+
+static struct pci_device_id netl_id_table[] = {
+	{NETL_VENDOR_ID, PCI_ANY_ID, PCI_ANY_ID, PCI_ANY_ID, 0, 0, 0},
+	{0,}
+};
+
+static struct pci_driver nlm_pcie_driver = {
+	.name = DRV_NAME,
+	.id_table = netl_id_table,
+	.probe = xlp_pcieip_probe,
+	.remove = xlp_pcieip_remove,
+};
+
+int __init xlp_pcieip_init(void)
+{
+	//fdebug("Compiled on %s %s\n", __DATE__, __TIME__);
+	if (pci_register_driver(&nlm_pcie_driver) < 0){
+		fdebug("Register failed\n");
+		return -ENODEV;
+	}
+	return 0;
+}
+
+void __exit xlp_pcieip_exit(void)
+{
+	pci_unregister_driver(&nlm_pcie_driver);
+	fdebug("Exiting nlm_pcie\n");
+}
+
+module_init(xlp_pcieip_init);
+module_exit(xlp_pcieip_exit);
+MODULE_LICENSE("GPL");
diff --git a/drivers/misc/netlogic/pcie-offload/setup_dev.sh b/drivers/misc/netlogic/pcie-offload/setup_dev.sh
new file mode 100755
index 0000000..b987274
--- /dev/null
+++ b/drivers/misc/netlogic/pcie-offload/setup_dev.sh
@@ -0,0 +1,57 @@
+#! /bin/bash
+/usr/bin/which mips64-nlm-linux-gcc > /dev/null 2>&1
+if [ $? != 0 ]; then
+	echo "No cross compiler found. source build.sh before proceeding"
+	exit -1
+fi
+
+SCRIPT_PATH=$(readlink -f "${BASH_SOURCE[0]}")
+echo $SCRIPT_PATH
+if ([ -e "${SCRIPT_PATH}" ]); then
+	NETL_OFFLOAD_SRC=$(readlink -fn "$(dirname ${SCRIPT_PATH})")
+fi
+
+SRCDIR="${NETL_OFFLOAD_SRC}/char/device/"
+if [ "${TGDIR}" == "" ] ; then
+TGDIR=$(readlink -f "${NETL_OFFLOAD_SRC}/../../../linux/usr/rootfs.xlp")
+fi
+
+function make_and_check_charlinks {
+	pushd "${1}" > /dev/null
+	ln -sf  ${NETL_OFFLOAD_SRC}/include/netl_pti_char.c "${1}"
+	ln -sf  ${NETL_OFFLOAD_SRC}/include/netl_pti_char.h "${1}"
+	ln -sf  ${NETL_OFFLOAD_SRC}/include/netl_pti_common.c "${1}"
+	ln -sf  ${NETL_OFFLOAD_SRC}/include/netl_pti.h "${1}"
+	popd > /dev/null
+
+	if [ -h "${1}"/netl_pti_char.c ] ||
+		[ -h "${1}"/netl_pti_char.h ] ||
+		[ -h "${1}"/netl_pti_common.c ] ||
+		[ -h "${1}"/netl_pti.h ] ; then
+		return 0
+	fi
+	return -1
+}
+
+# check if links are created
+make_and_check_charlinks "${SRCDIR}"
+
+if [ "$?" != "0" ] ; then
+	echo "REQUIRED links are not created. Exiting"
+	exit -1
+fi
+
+if [ "$START_BUILD" != "0" ] ; then
+	echo "Starting build process for character device..."
+	pushd "${NETL_OFFLOAD_SRC}/char/device/" > /dev/null &&
+	make clean &&\
+	make modules &&\
+	make modules_install INSTALL_MOD_PATH="${TGDIR}" &&
+	popd > /dev/null
+	echo "Starting build process for net device ..."
+	pushd "${NETL_OFFLOAD_SRC}/net/device/" > /dev/null &&
+	make clean &&\
+	make modules &&\
+	make modules_install INSTALL_MOD_PATH="${TGDIR}" &&
+	popd > /dev/null
+fi
diff --git a/drivers/misc/netlogic/pcie-offload/setup_host.sh b/drivers/misc/netlogic/pcie-offload/setup_host.sh
new file mode 100755
index 0000000..d45d920
--- /dev/null
+++ b/drivers/misc/netlogic/pcie-offload/setup_host.sh
@@ -0,0 +1,59 @@
+#! /bin/bash
+SCRIPT_PATH=$(readlink -f "${BASH_SOURCE[0]}")
+if ([ -e "${SCRIPT_PATH}" ]); then
+	NETL_OFFLOAD_SRC=$(readlink -fn "$(dirname ${SCRIPT_PATH})")
+fi
+
+function make_and_check_links {
+	pushd "${1}/char/host/" > /dev/null
+	ln -sf  ../../include/netl_pti_char.c
+	ln -sf  ../../include/netl_pti_char.h
+	ln -sf  ../../include/netl_pti_common.c
+	ln -sf  ../../include/netl_pti.h
+
+	if [ -h ./netl_pti_char.c ] || [ -h ./netl_pti_char.h ] ||
+		[ -h ./netl_pti_common.c ] || [ -h ./netl_pti.h ] ; then
+			popd > /dev/null
+			return 0
+	fi
+	popd > /dev/null
+	return -1
+}
+
+function make_and_check_net_links {
+	pushd "${1}/net/host/" > /dev/null
+	ln -sf ../device/pcieip_common.c
+	if [ -h ./pcieip_common.c ] ; then
+		popd > /dev/null
+		return 0
+	fi
+	popd > /dev/null
+	return -1
+}
+
+KDIR="${1}"
+if [ "${KDIR}" == "" ] ; then
+	echo "Please specify kernel source directory"
+	echo "$0 <path/to/host/kernel/source>"
+	exit -1
+fi
+
+# check if links are created
+make_and_check_links "${NETL_OFFLOAD_SRC}"
+
+if [ "$?" != "0" ] ; then
+	echo "REQUIRED links are not created in char. Exiting"
+fi
+make_and_check_net_links  "${NETL_OFFLOAD_SRC}"
+if [ "$?" != "0" ] ; then
+	echo "REQUIRED links are not created in net. Exiting"
+fi
+
+echo "Starting build process..."
+pushd "${NETL_OFFLOAD_SRC}/char/host/" > /dev/null
+make modules KDIR="${KDIR}"
+popd > /dev/null
+
+pushd "${NETL_OFFLOAD_SRC}/net/host/" > /dev/null
+make modules KDIR="${KDIR}"
+popd > /dev/null
-- 
1.9.1

