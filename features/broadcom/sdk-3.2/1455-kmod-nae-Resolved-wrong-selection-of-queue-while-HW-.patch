From 5fdadf040da3cd59ce18204e8f0ecf94040dcd53 Mon Sep 17 00:00:00 2001
From: PUNYA BHEEMESH <bheemesh@broadcom.com>
Date: Fri, 1 Aug 2014 17:39:37 +0530
Subject: [PATCH 1455/1532] kmod/nae: Resolved wrong selection of queue while
 HW replenishment enabled in multinode scenario. H/w replenishment is enabled
 by default now in route mode. It can be disabled using insmod parameter

[Based on SDK 3.2]
Signed-off-by: Quanyang Wang <quanyang.wang@windriver.com>
---
 drivers/net/ethernet/broadcom/nae/xlpge.h      |  4 --
 drivers/net/ethernet/broadcom/nae/xlpge_init.h |  1 +
 drivers/net/ethernet/broadcom/nae/xlpge_nae.c  | 12 +++---
 drivers/net/ethernet/broadcom/nae/xlpge_rx.c   | 51 ++++++++++++--------------
 drivers/net/ethernet/broadcom/nae/xlpge_tx.c   | 26 +++++--------
 5 files changed, 40 insertions(+), 54 deletions(-)

diff --git a/drivers/net/ethernet/broadcom/nae/xlpge.h b/drivers/net/ethernet/broadcom/nae/xlpge.h
index 6e14b03..f48eb91 100644
--- a/drivers/net/ethernet/broadcom/nae/xlpge.h
+++ b/drivers/net/ethernet/broadcom/nae/xlpge.h
@@ -30,8 +30,6 @@
 #ifndef	__XLPGE_H__
 #define	__XLPGE_H__
 
-#define CONFIG_NLM_NET_OPTS
-
 #include <asm/atomic.h>
 //#include <nlm_hal_nae.h>
 
@@ -596,7 +594,6 @@ extern uint64_t nlm_mode[];
 extern int nlm_prepad_len;
 extern int load_balance_timer_run;
 
-#ifdef CONFIG_NLM_NET_OPTS
 extern struct sk_buff *last_rcvd_skb[NR_CPUS * 8] ____cacheline_aligned;
 extern uint32_t last_rcvd_len[NR_CPUS * 8] ____cacheline_aligned;
 extern uint32_t last_rcvd_true_size[NR_CPUS * 8] ____cacheline_aligned;
@@ -604,7 +601,6 @@ extern uint32_t last_rcvd_node[NR_CPUS * 8] ____cacheline_aligned;
 extern uint64_t last_rcvd_skb_phys[NR_CPUS * 8] ____cacheline_aligned;
 extern int get_hw_frfifo_queue_id(int rxnode, nlm_nae_config_ptr nae_cfg,
                                   int cpu, uint32_t truesize, int hw_port_id);
-#endif
 
 
 #endif /* __ASSEMBLY__ */
diff --git a/drivers/net/ethernet/broadcom/nae/xlpge_init.h b/drivers/net/ethernet/broadcom/nae/xlpge_init.h
index 852eea0..59892f9 100644
--- a/drivers/net/ethernet/broadcom/nae/xlpge_init.h
+++ b/drivers/net/ethernet/broadcom/nae/xlpge_init.h
@@ -36,6 +36,7 @@
 #define NLM_PORT_FIFO_EN		8 /*can co-exist with any of the above 3 modes */
 
 extern int perf_mode;
+extern int hw_repl_mode;
 extern int num_cpus_per_node;
 extern int nae_family_9xx; 
 
diff --git a/drivers/net/ethernet/broadcom/nae/xlpge_nae.c b/drivers/net/ethernet/broadcom/nae/xlpge_nae.c
index 7d07a1e..c713e08 100644
--- a/drivers/net/ethernet/broadcom/nae/xlpge_nae.c
+++ b/drivers/net/ethernet/broadcom/nae/xlpge_nae.c
@@ -71,6 +71,9 @@ int nlm_prepad_len = 0;
 int perf_mode= NLM_TCP_MODE;
 extern cpumask_t phys_cpu_present_map;
 module_param(perf_mode, int, 0);
+
+int hw_repl_mode=1;
+module_param(hw_repl_mode, int, 0);
 /* Descriptors for each normal fifo. For xaui ports, if port fifo mode
    is enabled, this will be multiplied by 4 (3 fifos are unused) */
 int num_descs_per_normalq = 64;
@@ -124,13 +127,11 @@ static const struct file_operations nae_proc_info_ops = {
 	.release = single_release,
 };
 
-#ifdef CONFIG_NLM_NET_OPTS
 struct sk_buff *last_rcvd_skb[NR_CPUS * 8] ____cacheline_aligned;
 uint32_t last_rcvd_len[NR_CPUS * 8] ____cacheline_aligned;
 uint32_t last_rcvd_true_size[NR_CPUS * 8] ____cacheline_aligned;
 uint32_t last_rcvd_node[NR_CPUS * 8] ____cacheline_aligned;
 uint64_t last_rcvd_skb_phys[NR_CPUS * 8] ____cacheline_aligned;
-#endif
 
 int nae_debug_level = 1;
 module_param(nae_debug_level, int, 0);
@@ -1007,7 +1008,6 @@ static void nlm_enable_l3_l4_parser(nae_t *nae)
 
 }
 
-#ifdef CONFIG_NLM_NET_OPTS
 /* Get the hardware replenishment queue id */
 int get_hw_frfifo_queue_id(int rxnode, nae_t* nae_cfg,
 				  int cpu, uint32_t truesize, int hw_port_id)
@@ -1032,10 +1032,8 @@ int get_hw_frfifo_queue_id(int rxnode, nae_t* nae_cfg,
 	 * all the nodes vfbtable should be filled with starting node of
 	 * 0 to ending node with 20 entries each
 	 */
-	return nae_cfg->vfbtbl_hw_offset +
-		(rxnode * NLM_NAE_MAX_FREEIN_FIFOS_PER_NODE) + qid;
+	return nae_cfg->vfbtbl_hw_offset + qid;
 }
-#endif
 
 /**********************************************************************
  * nlm_xlp_nae_open -  called when bring up a device interface
@@ -1579,6 +1577,8 @@ void nlm_xlp_nae_init(void)
 		perf_mode = NLM_TCP_MODE;
 	}
 
+	if(perf_mode == NLM_TCP_MODE)
+		hw_repl_mode = 0;
 	nae_print(NAE_DBG_DEFAULT, "======= Module Parameters =========\n");
 	nae_print(NAE_DBG_DEFAULT, "nae debug mask 0x%x\n", nae_debug_level);
 	nae_print(NAE_DBG_DEFAULT, "num_descs_per_normalq=%d num_descs_per_jumboq=%d ",
diff --git a/drivers/net/ethernet/broadcom/nae/xlpge_rx.c b/drivers/net/ethernet/broadcom/nae/xlpge_rx.c
index 1952213..2086aa4 100644
--- a/drivers/net/ethernet/broadcom/nae/xlpge_rx.c
+++ b/drivers/net/ethernet/broadcom/nae/xlpge_rx.c
@@ -56,14 +56,12 @@ static uint32_t cpu_weight[NR_CPUS];
 //#define LOAD_BALANCE_DEBUG_ENABLE
 #endif
 
-#ifdef CONFIG_NLM_NET_OPTS
 extern struct sk_buff *last_rcvd_skb[NR_CPUS * 8] ____cacheline_aligned;
 extern uint32_t last_rcvd_len[NR_CPUS * 8] ____cacheline_aligned;
 extern uint32_t last_rcvd_true_size[NR_CPUS * 8] ____cacheline_aligned;
 extern uint32_t last_rcvd_node[NR_CPUS * 8] ____cacheline_aligned;
 extern uint64_t last_rcvd_skb_phys[NR_CPUS * 8] ____cacheline_aligned;
 struct dev_data *last_rcvd_priv[NR_CPUS * 8] ____cacheline_aligned;
-#endif
 
 uint64_t receive_count[NR_CPUS * 8] __cacheline_aligned;
 //#define MACSEC_DEBUG	1
@@ -819,22 +817,21 @@ static inline void process_rx_packets(void *arg, int cpu, unsigned int src_id,
 
 	truesize = skb->truesize;
 
-#ifdef CONFIG_NLM_NET_OPTS
-	/* Pass the packet to Network stack */
-	last_rcvd_skb[CPU_INDEX(cpu)] = skb;
-	last_rcvd_skb_phys[CPU_INDEX(cpu)] = addr;
-	last_rcvd_len[CPU_INDEX(cpu)] = len;
-	last_rcvd_node[CPU_INDEX(cpu)] = node;
-	last_rcvd_priv[CPU_INDEX(cpu)] = priv;
-	last_rcvd_true_size[CPU_INDEX(cpu)] = truesize;
-#endif
-
-#ifndef CONFIG_NLM_NET_OPTS
-	/* Setting this with NET_OPTS enabled can cause replenishment 
-	*  wrong for jumbo packets, as the hw-replenishment logic is 
-	*  depended on the skb->truesize */
-	skb->truesize = skb->len + sizeof(struct sk_buff);
-#endif
+	if(hw_repl_mode){
+		/* Pass the packet to Network stack */
+		last_rcvd_skb[CPU_INDEX(cpu)] = skb;
+		last_rcvd_skb_phys[CPU_INDEX(cpu)] = addr;
+		last_rcvd_len[CPU_INDEX(cpu)] = len;
+		last_rcvd_node[CPU_INDEX(cpu)] = node;
+		last_rcvd_priv[CPU_INDEX(cpu)] = priv;
+		last_rcvd_true_size[CPU_INDEX(cpu)] = truesize;
+	} else{
+
+		/* Setting this with NET_OPTS enabled can cause replenishment 
+		*  wrong for jumbo packets, as the hw-replenishment logic is 
+		*  depended on the skb->truesize */
+		skb->truesize = skb->len + sizeof(struct sk_buff);
+	}
 
 #ifdef CONFIG_INET_LRO
 	if ((skb->dev->features & NETIF_F_LRO) &&
@@ -862,18 +859,18 @@ static inline void process_rx_packets(void *arg, int cpu, unsigned int src_id,
 	/* Update Stats */
 	receive_count[CPU_INDEX(cpu)]++;
 
-#ifdef CONFIG_NLM_NET_OPTS
-	if (last_rcvd_skb[CPU_INDEX(cpu)]) {
+	if(hw_repl_mode){
+		if (last_rcvd_skb[CPU_INDEX(cpu)]){
+			slow_replenish_count[CPU_INDEX(cpu)]++;
+			mac_refill_frin_one_buffer(pdev, cpu, truesize);
+			last_rcvd_skb[CPU_INDEX(cpu)] = NULL;
+			last_rcvd_len[CPU_INDEX(cpu)] = 0;
+			last_rcvd_true_size[CPU_INDEX(cpu)] = 0;
+		}
+	}else{
 		slow_replenish_count[CPU_INDEX(cpu)]++;
 		mac_refill_frin_one_buffer(pdev, cpu, truesize);
-		last_rcvd_skb[CPU_INDEX(cpu)] = NULL;
-		last_rcvd_len[CPU_INDEX(cpu)] = 0;
-		last_rcvd_true_size[CPU_INDEX(cpu)] = 0;
 	}
-#else
-	slow_replenish_count[CPU_INDEX(cpu)]++;
-	mac_refill_frin_one_buffer(pdev, cpu, truesize);
-#endif
 }
 
 /*
diff --git a/drivers/net/ethernet/broadcom/nae/xlpge_tx.c b/drivers/net/ethernet/broadcom/nae/xlpge_tx.c
index 5cc3e44..4cf413d 100644
--- a/drivers/net/ethernet/broadcom/nae/xlpge_tx.c
+++ b/drivers/net/ethernet/broadcom/nae/xlpge_tx.c
@@ -42,9 +42,7 @@
 #include "xlpge.h"
 #include "xlpge_tso.h"
 
-#ifdef CONFIG_NLM_NET_OPTS
 extern struct dev_data *last_rcvd_priv[NR_CPUS * 8] ____cacheline_aligned;
-#endif
 //#include <asm/netlogic/kvm_xlp.h>
 
 uint64_t fast_replenish_count[NR_CPUS * 8] __cacheline_aligned;
@@ -138,37 +136,31 @@ int nlm_xlp_nae_start_xmit(struct sk_buff *skb, struct net_device *dev)
 		return tso_xmit_skb(skb, dev);
 #endif
 
-//	printk("skb: %llx, last_rcvd_skb: %llx, len: %llx, last_rcvd_len:  %llx, truesize: %llx, last_rcvd_true_size: %llx\n",
-//			skb, last_rcvd_skb[CPU_INDEX(cpu)], skb->len, last_rcvd_len[CPU_INDEX(cpu)], skb->truesize, last_rcvd_true_size[CPU_INDEX(cpu)]);
-#ifdef CONFIG_NLM_NET_OPTS
-	if((last_rcvd_skb[CPU_INDEX(cpu)] == skb)
-		&& !skb_shared(skb)
-		&& (last_rcvd_len[CPU_INDEX(cpu)] == skb->len)
-		&& (last_rcvd_true_size[CPU_INDEX(cpu)] == skb->truesize)
-		&& !skb_cloned(skb) && nae_cfg->vfbtbl_hw_nentries) {
+	if((hw_repl_mode) && (last_rcvd_skb[CPU_INDEX(cpu)] == skb)
+			&& !skb_shared(skb)
+			&& (last_rcvd_len[CPU_INDEX(cpu)] == skb->len)
+			&& (last_rcvd_true_size[CPU_INDEX(cpu)] == skb->truesize)
+			&& !skb_cloned(skb) && nae_cfg->vfbtbl_hw_nentries) {
 		struct dev_data *rpriv = (struct dev_data *)last_rcvd_priv[CPU_INDEX(cpu)];
 		last_rcvd_skb[CPU_INDEX(cpu)] = NULL;
 		last_rcvd_len[CPU_INDEX(cpu)] = 0;
 		last_rcvd_true_size[CPU_INDEX(cpu)] = 0;
 
 		qid = get_hw_frfifo_queue_id(last_rcvd_node[CPU_INDEX(cpu)],
-			nae_cfg, cpu, skb->truesize, rpriv->hw_port_id);
+				nae_cfg, cpu, skb->truesize, rpriv->hw_port_id);
 		msg0 = nae_tx_desc(DESC_TYPE_P2DNEOP, qid, 0, last_rcvd_skb_phys[CPU_INDEX(cpu)]);
 		hw_repl = 1;
 
 		Message("Tx, tx complete to nae, cpu %d len %d qid %d\n",
-			cpu, skb->len, qid);
+				cpu, skb->len, qid);
 
 		fast_replenish_count[CPU_INDEX(cpu)]++;
-	}
-	else
-#endif
-	{
+	}else{
 		qid = nae_cfg->vfbtbl_sw_offset + (cpu % num_cpus_per_node);
 		msg0 = nae_tx_desc(DESC_TYPE_P2DNEOP, qid, 0, virt_to_bus(skb));
 
 		Message("Tx, tx complete to cpu, cpu %d len %d qid %d\n",
-			cpu, skb->len, qid);
+				cpu, skb->len, qid);
 	}
 
 	{
-- 
1.9.1

