From 36c38616e3a187a0ba84b1406762cd1c2125f0cb Mon Sep 17 00:00:00 2001
From: Hareesh R <hareeshr@broadcom.com>
Date: Mon, 23 Sep 2013 06:33:49 -0700
Subject: [PATCH 1426/1532] nae: Load balancing , TSO support for 9xx

[Based on SDK 3.2]
Signed-off-by: Quanyang Wang <quanyang.wang@windriver.com>
---
 drivers/net/ethernet/broadcom/nae/xlpge.h      |   7 +-
 drivers/net/ethernet/broadcom/nae/xlpge_lro.c  |   5 +-
 drivers/net/ethernet/broadcom/nae/xlpge_nae.c  |  34 ++++---
 drivers/net/ethernet/broadcom/nae/xlpge_proc.c |   4 +
 drivers/net/ethernet/broadcom/nae/xlpge_rx.c   | 119 ++++++++++++++-----------
 drivers/net/ethernet/broadcom/nae/xlpge_tso.c  |  11 +--
 6 files changed, 102 insertions(+), 78 deletions(-)

diff --git a/drivers/net/ethernet/broadcom/nae/xlpge.h b/drivers/net/ethernet/broadcom/nae/xlpge.h
index b6c4242..0f6533d 100644
--- a/drivers/net/ethernet/broadcom/nae/xlpge.h
+++ b/drivers/net/ethernet/broadcom/nae/xlpge.h
@@ -70,6 +70,9 @@
 #define	RX_PPAD_EN			0
 #define	RX_PPAD_SZ			3
 #endif
+
+extern int num_cpus_per_node;
+
 /* TODO XXX: default enable prepad */
 #ifdef CONFIG_NLM_ENABLE_LOAD_BALANCING
 #define	PREPAD_LEN			16
@@ -116,7 +119,7 @@
 #define	KB(x)				(x<<10)
 #define	NLM_UCORE_SHARED_TABLE_SIZE	((KB(16))/(sizeof(uint32_t)))
 #define	NLM_UCORE_SHMEM_OFFSET		(KB(16))
-#define	NUM_LOAD_BALANCE_CPU		32
+#define	NUM_LOAD_BALANCE_CPU		(num_cpus_per_node)
 #endif
 
 #define	NLM_RX_ETH_BUF_SIZE		(ETH_DATA_LEN + ETH_HLEN + 	\
@@ -272,6 +275,7 @@ struct dev_data
 	unsigned short block;
 	unsigned short index;
 	unsigned short type;
+	unsigned int   port_index;
 	struct sk_buff* skb;
 	int phy_oldlinkstat;
 	__u8 hwaddr[6];
@@ -463,7 +467,6 @@ int nlm_load_balance_proc_open(struct inode *, struct file *);
 void nlm_init_load_balance(void);
 
 extern int enable_lro; 	
-extern int num_cpus_per_node;
 extern unsigned char eth_hw_addr[NLM_MAX_NODES][MAX_GMAC_PORT][6];
 extern struct proc_dir_entry *nlm_root_proc;
 extern uint64_t receive_count[NR_CPUS * 8] __cacheline_aligned;
diff --git a/drivers/net/ethernet/broadcom/nae/xlpge_lro.c b/drivers/net/ethernet/broadcom/nae/xlpge_lro.c
index 128c648..19e9dd8 100644
--- a/drivers/net/ethernet/broadcom/nae/xlpge_lro.c
+++ b/drivers/net/ethernet/broadcom/nae/xlpge_lro.c
@@ -58,6 +58,7 @@ static int lro_get_skb_hdr(struct sk_buff *skb, void **iphdr, void **tcph,
 	*tcph = tcp_hdr(skb);
 
 	*hdr_flags = LRO_IPV4 | LRO_TCP;
+	skb->ip_summed = CHECKSUM_UNNECESSARY;
 
 	return 0;
 }
@@ -100,8 +101,8 @@ inline void napi_lro_flush(int cpu)
 	for (i = 0; i < lro_flush_priv_cnt[cpu]; i++) {
 		priv = lro_flush_priv[cpu][i];
 		lro_flush_all(&priv->lro_mgr[cpu]);
-		lro_flush_needed[cpu][priv->port] = 0;
-		Message("Lro flush cpu %d port %d\n", cpu, priv->port);
+		lro_flush_needed[cpu][priv->port_index] = 0;
+		Message("Lro flush cpu %d port %d\n", cpu, priv->port_index);
 	}
 
 	lro_flush_priv_cnt[cpu] = 0;
diff --git a/drivers/net/ethernet/broadcom/nae/xlpge_nae.c b/drivers/net/ethernet/broadcom/nae/xlpge_nae.c
index 16ae038..d2ccaa4 100644
--- a/drivers/net/ethernet/broadcom/nae/xlpge_nae.c
+++ b/drivers/net/ethernet/broadcom/nae/xlpge_nae.c
@@ -61,10 +61,12 @@ module_param(exclusive_vc, int, 0);
 #ifdef CONFIG_NLM_ENABLE_LOAD_BALANCING
 int load_balance_timer_run = 1;
 module_param(load_balance_timer_run, int, S_IRUGO|S_IWUSR);
+int load_balance_en = 0;
+module_param(load_balance_en, int, 0);
 #endif
 int enable_napi = 1;
 int nlm_prepad_len = 0;
-int perf_mode= NLM_RT_MODE;
+int perf_mode= NLM_TCP_MODE;
 extern cpumask_t phys_cpu_present_map;
 module_param(perf_mode, int, 0);
 /* Descriptors for each normal fifo. For xaui ports, if port fifo mode
@@ -469,7 +471,7 @@ static int initialize_nae_per_node(nae_t * nae_cfg, uint32_t *phys_cpu_map, int
 
 	lnx_shinfo[0]->mode = nae_cfg->port_fifo_en ? NLM_PORT_FIFO_EN : 0;
 #ifdef CONFIG_NLM_ENABLE_LOAD_BALANCING
-	if(mode == NLM_TCP_MODE)
+	if((mode == NLM_TCP_MODE) && load_balance_en)
 		lnx_shinfo[0]->mode |= NLM_TCP_LOAD_BALANCE_MODE;
 	else
 #endif
@@ -683,7 +685,6 @@ static int nlm_replenish_per_cpu_buffer(nae_t* nae_cfg,
 	return ret;
 }
 
-
 int replenish_freein_fifos(void)
 {
 	int node, i, rv = 0, nae_id;
@@ -947,7 +948,8 @@ uint16_t pseuodo_chksum(uint16_t *ipsrc, uint16_t proto)
 
 static void nlm_enable_l3_l4_parser(nae_t *nae)
 {
-	
+	printk("Enabling parser for nae node %d naeid %d\n",
+			nae->node, nae->nae_id);
 	l2_parser_config_t l2;
 	l3_parser_config_t l3;
 	l4_parser_config_t l4;
@@ -1098,7 +1100,8 @@ static int  nlm_xlp_nae_open (struct net_device *dev)
 	int i, flow_ctrl;
 	int ret = 0;
 	nae_t* nae_cfg = priv->nae;
-	static int done = 0;
+	static int done[NLM_MAX_NODES][MAX_NAE_PERNODE] = { 
+		{0,0}, {0,0}, {0,0}, {0,0} };
 	if (perf_mode == NLM_TCP_MODE) {
 #ifdef TSO_ENABLED
 		tso_enable(dev, 1);
@@ -1106,12 +1109,16 @@ static int  nlm_xlp_nae_open (struct net_device *dev)
 #ifdef CONFIG_INET_LRO
 		lro_init(dev);
 #endif
-		if (!done) {
-			done = 1;
+		if (!done[nae_cfg->node][nae_cfg->nae_id]) {
+			done[nae_cfg->node][nae_cfg->nae_id] = 1;
 			nlm_enable_l3_l4_parser(nae_cfg);
 		}
+
+		if(nlm_prepad_len)
+			netsoc_prepad_enable(nae_cfg, nlm_prepad_len);
 	}
 
+
 	if (priv->inited) {
 		spin_lock_irq(&priv->lock);
 		if(nae_cfg->owned)
@@ -1478,6 +1485,7 @@ static int nlm_per_port_nae_init(nae_t* nae_cfg, int port, int maxnae)
 	struct dev_data *priv;
 	int cpu;
 	int node = nae_cfg->node;
+	static int port_index = 0;
 
 	if (!nae_cfg->ports[port].valid)
 		return -1;
@@ -1510,6 +1518,7 @@ static int nlm_per_port_nae_init(nae_t* nae_cfg, int port, int maxnae)
 	priv->block	= nae_cfg->ports[port].hw_port_id / 4;
 	priv->type	= nae_cfg->ports[port].iftype;
 	priv->mgmt_port	= nae_cfg->ports[port].mgmt;
+	priv->port_index = port_index++;
 
 	switch(nae_cfg->ports[port].iftype) {
 	case SGMII_IF:
@@ -1650,12 +1659,9 @@ void nlm_xlp_nae_init(void)
 	}
 
 #ifdef CONFIG_NLM_ENABLE_LOAD_BALANCING
-	if (perf_mode == NLM_TCP_MODE)
+	if ((perf_mode == NLM_TCP_MODE) && load_balance_en) {
 		nlm_prepad_len = PREPAD_LEN;
-#endif
 
-#ifdef CONFIG_NLM_ENABLE_LOAD_BALANCING
-	if (perf_mode == NLM_TCP_MODE) {
 		entry = create_proc_entry(
 				"load_info",
 				0,		/* def mode */
@@ -1663,7 +1669,11 @@ void nlm_xlp_nae_init(void)
 				);
 		if (entry)
 			entry->proc_fops = &nlm_load_balance_proc_fops;
-		//nlm_init_load_balance();
+		printk("Enabling load balance option\n");
+		nlm_init_load_balance();
+	} else {
+		printk("Disabling load balance option\n");
+		load_balance_en = 0;
 	}
 #endif
 
diff --git a/drivers/net/ethernet/broadcom/nae/xlpge_proc.c b/drivers/net/ethernet/broadcom/nae/xlpge_proc.c
index 31233ca..157b5a3 100644
--- a/drivers/net/ethernet/broadcom/nae/xlpge_proc.c
+++ b/drivers/net/ethernet/broadcom/nae/xlpge_proc.c
@@ -52,6 +52,10 @@ int nae_proc_read(char *page, char **start, off_t off,
 	uint64_t total_slow = 0, total_recv = 0;
 
 	for (i = 0; i < NR_CPUS; i++) {
+		if((receive_count[CPU_INDEX(i)]==0) &&
+				(err_replenish_count[CPU_INDEX(i)] == 0))
+			continue;
+
 		printk("cpu%d, recv %ld fast_repl %ld, slow_repl %ld "
 			"err_repl %ld p2pdalloc %lld\n", i,
 			(ulong)receive_count[CPU_INDEX(i)],
diff --git a/drivers/net/ethernet/broadcom/nae/xlpge_rx.c b/drivers/net/ethernet/broadcom/nae/xlpge_rx.c
index 50a33f1..9563152 100644
--- a/drivers/net/ethernet/broadcom/nae/xlpge_rx.c
+++ b/drivers/net/ethernet/broadcom/nae/xlpge_rx.c
@@ -43,12 +43,16 @@
 #include "xlpge.h"
 
 #ifdef CONFIG_NLM_ENABLE_LOAD_BALANCING
+#define NBITS_32 32
 static struct flow_meta_info *nlm_flow_meta_info;
 static struct active_flow_list *nlm_active_flow_list;
 static struct timer_list nlm_load_balance_timer[NR_CPUS];
-static int nlm_load_balance_search_cpu[NUM_LOAD_BALANCE_CPU][NUM_LOAD_BALANCE_CPU-1];
-static uint32_t nlm_pcpu_mask = 0;
+static int nlm_load_balance_search_cpu[NR_CPUS][NR_CPUS-1];
+static uint32_t nlm_pcpu_mask[NR_CPUS/NBITS_32] = {0};
 static uint32_t *ucore_shared_data = NULL;
+extern int load_balance_en;
+static uint32_t cpu_weight[NR_CPUS];
+//#define LOAD_BALANCE_DEBUG_ENABLE
 #endif
 
 #ifdef CONFIG_NLM_NET_OPTS
@@ -131,9 +135,10 @@ static void dump_cpu_active_flow_info(int cpu, struct seq_file *m, int weight)
 
 static int nlm_dump_load_balance_stats(struct seq_file *m, void *v)
 {
-	uint32_t cpu_weight[32] = {0};
 	int i = 0;
 
+	memset(cpu_weight, 0, sizeof(cpu_weight));
+
 	if (!nlm_active_flow_list || !nlm_flow_meta_info)
 		return 0;
 
@@ -205,7 +210,7 @@ static void nlm_remove_inactive_flow(int cpu)
 	}
 	spin_unlock_irqrestore(&afl->lock, mflags);
 }
-#if 0
+
 static void setup_search_path(void)
 {
 	uint32_t pcpu;
@@ -218,21 +223,21 @@ static void setup_search_path(void)
 	int previous_core, next_core;
 #ifdef LOAD_BALANCE_DEBUG_ENABLE
 	int j;
-	uchar buf[256];
+	unsigned char buf[256];
 #endif
 
 	for (i = 0; i < NUM_LOAD_BALANCE_CPU; i++) {
 		if (!cpumask_test_cpu(i, &phys_cpu_present_map))
 			continue;
-		nlm_pcpu_mask |= (1UL << i);
+		nlm_pcpu_mask[i/NBITS_32] |= (1UL << (i % NBITS_32));
 		num_phy_cpu++;
 	}
 #ifdef LOAD_BALANCE_DEBUG_ENABLE
 	printk("num_phy_cpu %d, nlm_pcpu_mask %#x\n",
-		num_phy_cpu, nlm_pcpu_mask);
+		num_phy_cpu, nlm_pcpu_mask[0]);
 #endif
 	for (pcpu = 0; pcpu < NUM_LOAD_BALANCE_CPU; pcpu++) {
-		if(!((1U<<pcpu) & nlm_pcpu_mask))
+		if(!((1U<<(pcpu % NBITS_32)) & nlm_pcpu_mask[pcpu/NBITS_32]))
 			continue;
 
 		core = pcpu / 4;
@@ -240,27 +245,27 @@ static void setup_search_path(void)
 		for (i = 0; i < 3; i++) {
 			nlm_load_balance_search_cpu[pcpu][index] =
 				thrds[(pcpu + i + 1) % 4] + core * 4;
-			if(((1U << nlm_load_balance_search_cpu[pcpu][index])
-				& nlm_pcpu_mask))
+			if(((1U << (nlm_load_balance_search_cpu[pcpu][index] % NBITS_32))
+				& nlm_pcpu_mask[pcpu/NBITS_32]))
 				index++;
 		}
 		if (core >= 1)
 			previous = (core - 1) * 4;
 		else
-			previous = 28;
+			previous = (NUM_LOAD_BALANCE_CPU - 4);
 		previous_core = previous / 4;
 
-		if (core < 7)
+		if (core < ((NUM_LOAD_BALANCE_CPU/4) - 1))
 			next = (core+1)*4;
 		else
 			next = 0;
 		next_core = next / 4;
 
 		while (index < (num_phy_cpu - 1)) {
-			if ((0xfU << previous) & nlm_pcpu_mask) {
+			if ((0xfU << (previous % NBITS_32)) & nlm_pcpu_mask[previous/NBITS_32]) {
 				for (i = 0; i < 4; i++) {
-					if ((1UL<<(i+previous) &
-						nlm_pcpu_mask)) {
+					if ((1UL<<((i+previous)%NBITS_32)) &
+						nlm_pcpu_mask[(i+previous)/NBITS_32]) {
 						nlm_load_balance_search_cpu[pcpu][index] =
 							i + previous;
 						index++;
@@ -270,18 +275,18 @@ static void setup_search_path(void)
 			if ((previous_core) >= 1)
 				previous = (previous_core - 1) * 4;
 			else
-				previous = 28;
+				previous = (NUM_LOAD_BALANCE_CPU - 4);
 			previous_core = previous / 4;
-			if ((0xfU << next) & nlm_pcpu_mask) {
+			if ((0xfU << (next % NBITS_32)) & nlm_pcpu_mask[next/NBITS_32]) {
 				for (i = 0; i < 4; i++) {
-					if ((1UL << (i + next)) & nlm_pcpu_mask) {
+					if ((1UL << ((i + next)%NBITS_32)) & nlm_pcpu_mask[(i+next)/NBITS_32]) {
 						nlm_load_balance_search_cpu[pcpu][index] =
 							i + next;
 						index++;
 					}
 				}
 			}
-			if (next_core < 7)
+			if (next_core < ((NUM_LOAD_BALANCE_CPU/4) - 1))
 				next = (next_core + 1) * 4;
 			else
 				next = 0;
@@ -305,9 +310,7 @@ static void setup_search_path(void)
 #endif
 	return;
 }
-#endif
 
-#if 0
 static void nlm_load_balance_timer_func(unsigned long arg)
 {
 	int cpu = hard_smp_processor_id();
@@ -318,6 +321,9 @@ static void nlm_load_balance_timer_func(unsigned long arg)
 	int idx_to_fmi;
 	int lcpu;
 	int i = 0, j, k;
+	int max_nae = get_num_nae_pernode();
+	int node = 0, nae;
+	nae_t *nae_cfg;
 
 	/*Remove inactive flows.*/
 	nlm_remove_inactive_flow(cpu);
@@ -388,9 +394,13 @@ restart:
 
 				/*Update ucore shared memory*/
 				lcpu = __cpu_number_map[cpu];
-				nlm_hal_modify_nae_ucore_sram_mem(0, 0, &lcpu,
-					NLM_UCORE_SHMEM_OFFSET +
-					(idx_to_fmi * 4), 1);
+				for(nae =0; nae < max_nae; nae++) {
+					nae_cfg = get_nae(node, nae);
+					if(nae_cfg)
+						netsoc_modify_ucore_sram(nae_cfg, &lcpu,
+						NLM_UCORE_SHMEM_OFFSET +
+						(idx_to_fmi * 4), 1);
+				}
 				*(ucore_shared_data + idx_to_fmi) = 
 					__cpu_number_map[cpu];
 
@@ -400,23 +410,28 @@ restart:
 		}
 	}
 	if (!myafl->nr_active_flows)
-		mod_timer(&nlm_load_balance_timer[cpu], jiffies + 50);
+		mod_timer_pinned(&nlm_load_balance_timer[cpu], jiffies + 50);
 	else
-		mod_timer(&nlm_load_balance_timer[cpu],
+		mod_timer_pinned(&nlm_load_balance_timer[cpu],
 			jiffies + load_balance_timer_run * HZ);
 }
-#endif
+
 void nlm_setup_load_balance_timer(void *data)
 {
 	int pcpu = hard_smp_processor_id();
 	nlm_load_balance_timer[pcpu].expires = jiffies + 5*HZ;
-	add_timer(&nlm_load_balance_timer[pcpu]);
+	mod_timer_pinned(&nlm_load_balance_timer[pcpu],  jiffies + 5*HZ);
 }
-#if 0
+
 void nlm_init_load_balance(void)
 {
 	uint32_t signature;
 	int i = 0, j = 0;
+	int max_nae = get_num_nae_pernode();
+	int node = 0, nae;
+	nae_t *nae_cfg;
+
+
 
 	nlm_active_flow_list = vmalloc(sizeof(struct active_flow_list) *
 		NUM_LOAD_BALANCE_CPU);
@@ -464,13 +479,7 @@ void nlm_init_load_balance(void)
 		setup_timer(&nlm_load_balance_timer[i],
 			nlm_load_balance_timer_func, i);
 	}
-	smp_call_function(nlm_setup_load_balance_timer, NULL, 1);
-
-	/*Add timer on self*/
-	preempt_disable();
-	nlm_load_balance_timer[hard_smp_processor_id()].expires = jiffies + 5*HZ;
-	add_timer(&nlm_load_balance_timer[hard_smp_processor_id()]);
-	preempt_enable();
+	on_each_cpu(nlm_setup_load_balance_timer, NULL, 1);
 
 	setup_search_path();
 	/*Update ucore shared memory with the table*/
@@ -484,20 +493,24 @@ void nlm_init_load_balance(void)
 	for (i = 0; i < NLM_UCORE_SHARED_TABLE_SIZE; ) {
 		for(j = 0; j < NUM_LOAD_BALANCE_CPU && 
 			i < NLM_UCORE_SHARED_TABLE_SIZE; j++) {
-			if (nlm_pcpu_mask & (1U << j)) {
+			if (nlm_pcpu_mask[j/NBITS_32] & (1U << (j%NBITS_32))) {
 				*(ucore_shared_data + i) = __cpu_number_map[j];
 				i++;
 			}
 		}
 	}
-	nlm_hal_modify_nae_ucore_sram_mem(0, 0, ucore_shared_data,
-		NLM_UCORE_SHMEM_OFFSET, NLM_UCORE_SHARED_TABLE_SIZE);
-	mb();
-	signature = 0xdeadbeefU;
-	nlm_hal_modify_nae_ucore_sram_mem(0, 0, &signature,
-		NLM_UCORE_SHMEM_OFFSET - 4, 1);
+	for(nae=0; nae < max_nae; nae++) {
+		nae_cfg = get_nae(node, nae);
+		if(nae_cfg) {
+			netsoc_modify_ucore_sram(nae_cfg, ucore_shared_data,
+				NLM_UCORE_SHMEM_OFFSET, NLM_UCORE_SHARED_TABLE_SIZE);
+			mb();
+			signature = 0xdeadbeefU;
+			netsoc_modify_ucore_sram(nae_cfg, &signature,
+				NLM_UCORE_SHMEM_OFFSET - 4, 1);
+		}
+	}
 }
-#endif
 
 #ifdef LOAD_BALANCE_DEBUG_ENABLE
 static void dump_packet(unsigned char *vaddr, int len)
@@ -654,8 +667,6 @@ static inline void process_rx_packets(int cpu, unsigned int src_id,
 	vaddr = (uint64_t)(unsigned long)bus_to_virt(addr);
 	nae_cfg = (nae_t*)mac_get_nae_back_ptr(vaddr);
 	port = nae_cfg->cntx2port[context]; 
-	Message("%s in cpu %d src_id %d len %d context %d node %d err %d port=%d\n",
-		__func__, cpu, src_id, len, context, node, err, port);
 
 #ifdef ENABLE_SANITY_CHECKS
 	if (port >= MAX_GMAC_PORT) {
@@ -689,7 +700,6 @@ static inline void process_rx_packets(int cpu, unsigned int src_id,
 		err_replenish_count[CPU_INDEX(cpu)]++;
 		return;
 	}
-
 	
 	len = len  - ETH_FCS_LEN - nlm_prepad_len;
 
@@ -706,9 +716,11 @@ static inline void process_rx_packets(int cpu, unsigned int src_id,
 #endif
 
 #ifdef CONFIG_NLM_ENABLE_LOAD_BALANCING
-	if (!priv->mgmt_port)
-		nlm_update_flow_stats((unsigned int *)vaddr, len, context);
-	skb_reserve(skb, nlm_prepad_len);
+	if(load_balance_en) {
+		if (!priv->mgmt_port)
+			nlm_update_flow_stats((unsigned int *)vaddr, len, context);
+		skb_reserve(skb, nlm_prepad_len);
+	}
 #endif
 
 	if (priv->index == XGMAC0)
@@ -781,14 +793,13 @@ static inline void process_rx_packets(int cpu, unsigned int src_id,
 	if ((skb->dev->features & NETIF_F_LRO) &&
 			(msg1 & RX_IP_CSUM_VALID) && (msg1 & RX_TCP_CSUM_VALID)) {
 
-		skb->ip_summed = CHECKSUM_UNNECESSARY;
 		lro_receive_skb(&priv->lro_mgr[cpu], skb, NULL);
-		if(!lro_flush_needed[cpu][priv->port]) {
+		if(!lro_flush_needed[cpu][priv->port_index]) {
 			lro_flush_priv[cpu][lro_flush_priv_cnt[cpu]] = priv;
-			lro_flush_needed[cpu][priv->port] = 1;
+			lro_flush_needed[cpu][priv->port_index] = 1;
 			lro_flush_priv_cnt[cpu]++;
 			Message("Adding to lro flush queue cpu %d port %d\n",
-				cpu, priv->port);
+				cpu, priv->port_index);
 		}
 	} else
 #endif
diff --git a/drivers/net/ethernet/broadcom/nae/xlpge_tso.c b/drivers/net/ethernet/broadcom/nae/xlpge_tso.c
index a2e6ad9..9e8c741 100644
--- a/drivers/net/ethernet/broadcom/nae/xlpge_tso.c
+++ b/drivers/net/ethernet/broadcom/nae/xlpge_tso.c
@@ -290,13 +290,8 @@ int tso_enable(struct net_device *dev, u32 data)
 {
 	int rv = 0;
 
-#ifdef NOTYET
-	rv = ethtool_op_set_tso(dev, data);
-	if(rv == 0)
-		rv = ethtool_op_set_tx_csum(dev, data);
-	if(rv == 0)
-		rv = ethtool_op_set_sg(dev, data);
-#endif
-	dev->features |= NETIF_F_FRAGLIST | NETIF_F_HIGHDMA;
+	dev->hw_features = NETIF_F_SG | NETIF_F_IP_CSUM | NETIF_F_TSO;
+	dev->features = dev->hw_features;
+	dev->features |= NETIF_F_HIGHDMA;
 	return rv;
 }
-- 
1.9.1

