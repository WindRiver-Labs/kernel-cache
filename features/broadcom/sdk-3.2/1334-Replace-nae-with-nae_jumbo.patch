From 3d894daf3868f8dc9ef7379e54728b10c4425bcc Mon Sep 17 00:00:00 2001
From: "P. Sadik" <psadik@netlogicmicro.com>
Date: Fri, 4 Nov 2011 23:15:23 +0530
Subject: [PATCH 1334/1532] Replace nae with nae_jumbo.

[Based on SDK 3.2]
Signed-off-by: Quanyang Wang <quanyang.wang@windriver.com>
---
 drivers/net/ethernet/broadcom/nae/.gitignore |   8 -
 drivers/net/ethernet/broadcom/nae/init_nae.c |   2 +
 drivers/net/ethernet/broadcom/nae/xlp_nae.c  | 890 ++++++++++++++++++++++-----
 drivers/net/ethernet/broadcom/nae/xlp_nae.h  |   3 +-
 4 files changed, 738 insertions(+), 165 deletions(-)
 delete mode 100644 drivers/net/ethernet/broadcom/nae/.gitignore

diff --git a/drivers/net/ethernet/broadcom/nae/.gitignore b/drivers/net/ethernet/broadcom/nae/.gitignore
deleted file mode 100644
index d7384ab..0000000
--- a/drivers/net/ethernet/broadcom/nae/.gitignore
+++ /dev/null
@@ -1,8 +0,0 @@
-.*.*.cmd
-Module.symvers
-TAGS
-modules.order
-*.ko
-*.mod.c
-.tmp_versions
-
diff --git a/drivers/net/ethernet/broadcom/nae/init_nae.c b/drivers/net/ethernet/broadcom/nae/init_nae.c
index 167e0a6..302251f 100644
--- a/drivers/net/ethernet/broadcom/nae/init_nae.c
+++ b/drivers/net/ethernet/broadcom/nae/init_nae.c
@@ -13,6 +13,8 @@
 
 #include "net_common.h"
 
+extern int rely_on_firmware_config;
+
 static void config_fmn(void)
 {
 	unsigned long mflags = 0;
diff --git a/drivers/net/ethernet/broadcom/nae/xlp_nae.c b/drivers/net/ethernet/broadcom/nae/xlp_nae.c
index 1f9b2e1..ce13b29 100755
--- a/drivers/net/ethernet/broadcom/nae/xlp_nae.c
+++ b/drivers/net/ethernet/broadcom/nae/xlp_nae.c
@@ -106,6 +106,176 @@
         }					\
 	printk("\n"); }
 
+/* This includes extra space and so with 64K PAGE_SIZE, we can have upto 
+   7 buffers. We need 32 bytes for prepad and another cacheline for storing
+   s/w info
+ */
+#define DEFAULT_JUMBO_MTU	5568  // for mtu 16384
+#define JUMBO_RX_OFFSET		64
+#define PREPAD_LEN		0	
+/* THIS MUST be multiple of cache line size */
+#define DEFAULT_JUMBO_BUFFER_SIZE	\
+		(DEFAULT_JUMBO_MTU + PREPAD_LEN + JUMBO_RX_OFFSET) 
+
+#define NETL_JUMBO_SKB_HDR_LEN 64
+#define MAC_FRIN_WORK_NUM 32
+#define PHOENIX_MAX_MACS 18
+#define MAX_TSO_SKB_PEND_REQS 50
+
+/* THIS MUST be multiple of cache line size */
+static int jumbo_buffer_size = DEFAULT_JUMBO_BUFFER_SIZE; /*or set in set_mtu */
+static int jumbo_mtu = DEFAULT_JUMBO_MTU; /* or set in set_mtu */
+typedef struct jumbo_rx_info {
+	struct page *page;
+	unsigned int page_offset; 
+	unsigned int space;
+	atomic_t alloc_fails[PHOENIX_MAX_MACS];
+}jumbo_rx_info_t;
+
+/* This struct size MUST be at most 32 bytes */
+struct jumbo_rx_cookie {
+	struct page *page;
+	unsigned int page_offset;
+};
+
+jumbo_rx_info_t  jumbo_rx_buff[NR_CPUS];
+//static struct tasklet_struct mac_frin_replenish_task[MAC_FRIN_WORK_NUM];
+static struct work_struct mac_frin_replenish_work[MAC_FRIN_WORK_NUM];
+static void mac_frin_replenish(unsigned long arg /* ignored */);
+static void tx_free_buffer(unsigned long arg /* ignored */);
+
+#define MAX_PACKET_SZ_PER_MSG	16384
+#define P2P_EXTRA_DESCS	      	((PAGE_SIZE / MAX_PACKET_SZ_PER_MSG) + 4)
+#define P2P_SKB_OFF	      	(MAX_SKB_FRAGS + P2P_EXTRA_DESCS - 1)
+#define CPU_INDEX(x) (x * 8)
+
+
+struct p2p_desc_mem {
+	void *mem;
+	uint64_t dsize;
+	uint64_t pad[6];
+	spinlock_t lock;
+};
+struct p2p_desc_mem p2p_desc_mem[NR_CPUS] __cacheline_aligned;
+static uint64_t p2p_dynamic_alloc_cnt[NR_CPUS * 8] __cacheline_aligned;
+static int p2p_desc_mem_init(void);
+
+enum msc_opcodes { IP_CHKSUM = 1,
+	TCP_CHKSUM,
+	UDP_CHKSUM,
+	SCTP_CRC,
+	FCOE_CRC,
+	IP_TCP_CHKSUM,
+	TSO_IP_TCP_CHKSUM,
+	IP_UDP_CHKSUM,
+	IP_CHKSUM_SCTP_CRC
+};
+
+uint16_t tcp_pseuodo_chksum(uint16_t *ipsrc)
+{
+	uint32_t sum = 0;
+	//*ipsrc = cpu_to_be16p(ipsrc);
+	sum += cpu_to_be16(ipsrc[0]);
+	sum += cpu_to_be16(ipsrc[1]);
+	sum += cpu_to_be16(ipsrc[2]);
+	sum += cpu_to_be16(ipsrc[3]);
+	sum += 6;
+	while(sum >> 16)
+		sum = (sum & 0xffff)  + (sum >> 16);
+	//      sum = ~sum;
+	return (uint16_t)sum;
+}
+
+uint16_t udp_pseuodo_chksum(uint16_t *ipsrc)
+{
+	uint32_t sum = 0;
+	sum += cpu_to_be16(ipsrc[0]);
+	sum += cpu_to_be16(ipsrc[1]);
+	sum += cpu_to_be16(ipsrc[2]);
+	sum += cpu_to_be16(ipsrc[3]);
+	sum += 0x11;
+	while(sum >> 16)
+		sum = (sum & 0xffff)  + (sum >> 16);
+	//      sum = ~sum;
+	return (uint16_t)sum;
+}
+
+
+static __inline__ uint64_t nae_tso_desc0(
+		unsigned int type,
+		unsigned int subtype,
+		unsigned int opcode,
+		unsigned int l3hdroff,
+		unsigned int l4hdroff,
+		unsigned int l3chksumoff,
+		unsigned int pseudohdrchksum,
+		unsigned int l4chksumoff,
+		unsigned int pyldoff)
+{
+
+	return ((uint64_t)(type & 0x3) << 62) |
+		((uint64_t)(subtype & 3) << 60) |
+		((uint64_t)(opcode & 0xf) << 56) |
+		((uint64_t)(l3hdroff & 0x3f) << 43) |
+		((uint64_t)(l4hdroff & 0x7f) << 36) |
+		((uint64_t)(l3chksumoff & 0x1f) << 31) |
+		((uint64_t)(pseudohdrchksum & 0xffff) << 15) |
+		((uint64_t)(l4chksumoff & 0x7f) << 8) |
+		((uint64_t)(pyldoff & 0xff));
+}
+
+static __inline__ uint64_t nae_tso_desc1(
+		unsigned int type,
+		unsigned int subtype,
+		unsigned int poly,
+		unsigned int mss,
+		unsigned int crcstopoff,
+		unsigned int crcinsoff)
+{
+	return ((uint64_t)(type & 0x3) << 62) |
+		((uint64_t)(subtype & 3) << 60) |
+		((uint64_t)(poly & 0x3) << 48) |
+		((uint64_t)(mss & 0xffff) << 32) |
+		((uint64_t)(crcstopoff & 0xffff) << 16) |
+		((uint64_t)(crcinsoff & 0xffff));
+
+}
+
+static __inline__ int tso_enable(struct net_device *dev, u32 data)
+{
+	int rv;
+	rv = ethtool_op_set_tso(dev, data);
+	if(rv == 0)
+		rv = ethtool_op_set_tx_csum(dev, data);
+	if(rv == 0)
+		rv = ethtool_op_set_sg(dev, data);
+	dev->features |= NETIF_F_FRAGLIST | NETIF_F_HIGHDMA;
+	return rv;
+}
+
+
+
+#ifdef CONFIG_64BIT
+#define MY_XKPHYS 0xa800000000000000ULL
+static inline struct jumbo_rx_cookie *get_rx_cookie(uint64_t phys)
+{
+	/* rx cookie is  stored one cacheline before the start of rxbuf */
+	unsigned long *ptr = (unsigned long *)(unsigned long)
+		 		(MY_XKPHYS | (phys - SMP_CACHE_BYTES));
+
+	//printk("Received pkt at 0x%llx %p\n", phys, ptr);
+	return (struct jumbo_rx_cookie *)ptr;
+}
+#else
+#error "get_rx_cookie() needs to be implemented"
+#endif
+
+/* Here 
+   length -> is the exact length of the data (excluding BYTE_OFFSET, CRClen,
+   prepad)
+   hlen -> is the length of the data to be copied to the skb 
+   */
+
 /* Module Parameters */
 static int debug = 0;
 module_param(debug, int, 0);
@@ -188,11 +358,10 @@ static struct net_device_stats *nlm_xlp_mac_get_stats(struct net_device *dev);
 static struct net_device *dev_mac[MAX_GMAC_PORT];
 
 extern struct proc_dir_entry *nlm_root_proc;
-static struct tasklet_struct mac_refill_task[MAX_GMAC_PORT];
-static int mac_refill_frin_desc(unsigned long dev);
 
 extern void nlm_xlp_mac_set_enable(struct dev_data *priv, int flag);
 
+
 static const struct net_device_ops nlm_xlp_nae_ops = {
 	.ndo_open	= nlm_xlp_nae_open,
 	.ndo_stop	= nlm_xlp_nae_stop,
@@ -272,128 +441,271 @@ static __inline__ void *cacheline_aligned_kmalloc(int size, int gfp_mask)
         return buf;
 }
 
-/**********************************************************************
- * nlm_xlp_alloc_skb -  64 bits cache aligned skb buffer allocate
- * return - skb buffer address
- *
- **********************************************************************/
-static __inline__ struct sk_buff *nlm_xlp_alloc_skb(void)
+
+static __inline__ int mac_send_fr(struct dev_data *priv, unsigned long addr, int len)
 {
-        int offset = 0;
-        struct sk_buff *skb = __dev_alloc_skb(NLM_RX_BUF_SIZE, GFP_KERNEL);
-
-        if (!skb) {
-                return NULL;
-        }
-        /* align the data to the next cache line */
-        offset = ((unsigned long)skb->data + CACHELINE_SIZE) &
-                ~(CACHELINE_SIZE - 1);
-        skb_reserve(skb, (offset - (unsigned long)skb->data));
-
-        return skb;
+	struct xlp_msg msg;
+	int code = 0;
+	int ret;
+	msg.entry[0] = (unsigned long long)virt_to_bus(addr) & 0xffffffffe0ULL;
+	msg.entry[1]= msg.entry[2] = msg.entry[3] = 0;
+	/* Send the packet to nae rx  */
+	//printk("mac_send_fr phys set=0x%lx\n", msg.entry[0]);
+	__sync();
+        if ( (ret = nlm_hal_send_msg1(priv->nae_rx_qid, code, msg.entry[0])) ){
+		print_fmn_send_error(__func__, ret);
+		printk("Unable to send configured free desc, check freein carving (qid=%d)\n", priv->nae_rx_qid);			 
+		return ret;
+	}
+	return ret;		
+	
 }
 
-
-/**********************************************************************
- * nlm_xlp_free_skb -  change msg into skb buffer address, free it
- * @msg - freeback msg that sent to cpu vc
- *
- **********************************************************************/
-static inline void nlm_xlp_free_skb(struct xlp_msg *msg)
+void build_skb(struct sk_buff *skb, uint64_t *rxp2d, uint32_t*p2d_len, int num_desc, 
+			uint32_t hlen, uint32_t length)
 {
-	struct sk_buff *skb;
-	struct dev_data *priv;
-	int cpu = hard_smp_processor_id();
-	unsigned long tmp;
+	int idx;
+	struct skb_shared_info *sp = skb_shinfo(skb);
+	struct jumbo_rx_cookie *rx_cookie = get_rx_cookie(rxp2d[0]);	
+	struct page *pg = rx_cookie->page;
+	int rx_offset =  JUMBO_RX_OFFSET;
+	uint8_t *va = page_address(pg) + rx_cookie->page_offset + rx_offset; 
+	skb_frag_t *fp = &sp->frags[0];
+
+	/* actual data starts IP header align */
+	skb_reserve(skb, 2);
+
+	skb->len = skb->data_len = length;
+	skb->truesize = length + sizeof(struct sk_buff);
+
+	fp->page= pg; 
+	fp->page_offset = rx_cookie->page_offset + rx_offset + hlen;
+	fp->size = p2d_len[0]- hlen;
+	
+	skb_copy_to_linear_data(skb, va, hlen);
+	skb->data_len -= hlen;
+	skb->tail += hlen;
+
+	/*fill other frags*/
+	for(idx=1; idx<num_desc; idx++){
+		//printk("build_skb data_len=0x%x\n", skb->data_len);
+		fp = &sp->frags[idx];
+		rx_cookie = get_rx_cookie(rxp2d[idx]);	
+		pg = rx_cookie->page;
+		rx_offset =  JUMBO_RX_OFFSET;
+		va = page_address(pg) + rx_cookie->page_offset + rx_offset; 
+		fp->page= pg; 
+		fp->page_offset = rx_cookie->page_offset + rx_offset;
+		fp->size = p2d_len[idx];
+	}	
+	//printk("build_skb data_len=0x%x\n", skb->data_len);
+	skb_shinfo(skb)->nr_frags = num_desc;
+}
 
-	tmp = (unsigned long)(msg->entry[0] & 0xffffffffffULL);
-	skb = (struct sk_buff *)bus_to_virt(tmp);
+/* assumes that buffer is setup correctly */
+static void recycle_rx_desc(uint64_t phys, struct net_device *dev)
+{
+	struct dev_data* priv = netdev_priv(dev);
+	unsigned long msgrng_flags;
 
-	if(!skb)
-		return;
-	/* Tx Complete */
+	msgrng_access_enable(msgrng_flags);
+	mac_send_fr(priv, phys, jumbo_mtu);
+	msgrng_access_disable(msgrng_flags);
+}
 
-	/* release the skb and update statistics outside the spinlock */
-	priv = netdev_priv(skb->dev);
-	STATS_INC(priv->stats.tx_packets);
-	STATS_ADD(priv->stats.tx_bytes, skb->len);
-	priv->cpu_stats[cpu].txc_packets++;
+static int mac_frin_replenish_one_normal_msg(struct net_device *dev)
+{
+	jumbo_rx_info_t *rx;
+	struct dev_data* priv = netdev_priv(dev);
+	int ret = 0;
+	unsigned long msgrng_flags;
+	struct jumbo_rx_cookie *rx_cookie;
+	struct page *pg;
+	void *va;
+	uint64_t pa, phys;
+	int cpu = smp_processor_id();
+
+	rx = &jumbo_rx_buff[cpu];
+
+	if((rx->space >= jumbo_buffer_size)) {
+		pg = rx->page;
+		/* if this is not the last particle in this page */
+		if((rx->space - jumbo_buffer_size) > jumbo_buffer_size)
+			get_page(pg);
+	} else {
+		/* alloc a new page */
+		pg = alloc_pages(GFP_KERNEL, 0);
+		if(pg == NULL) {
+			panic("alloc_pages failure\n");
+		}
 
+		rx->page = pg;
+		rx->page_offset = 0;
+		rx->space = PAGE_SIZE;
+		get_page(pg);
+	}
+	va = page_address(pg) + rx->page_offset;
+	pa = page_to_phys(pg) + rx->page_offset;
+	rx_cookie = (struct jumbo_rx_cookie *)va;
+	rx_cookie->page = pg;
+	rx_cookie->page_offset = rx->page_offset;
+
+	msgrng_access_enable(msgrng_flags);
+	/* account for s/w space and prepad */
+	phys = pa + JUMBO_RX_OFFSET;
+	if (mac_send_fr(priv, phys, jumbo_mtu)) {
+		msgrng_access_disable(msgrng_flags);
+		put_page(pg);
+		printk
+			("message_send failed!, unable to send free desc to mac\n");
+		ret = -EIO;
+		return ret;
+	}
+	msgrng_access_disable(msgrng_flags);
+	rx->page_offset += jumbo_buffer_size;
+	rx->space -= jumbo_buffer_size;
 
-	netif_tx_wake_all_queues(skb->dev);
-	/* nlm_netif_queue_tx_complete(skb->dev);*/
+	return ret;
+}
 
-	dev_kfree_skb_any(skb);
+static int mac_frin_replenish_msgs(struct net_device *dev, int num)
+{
+	jumbo_rx_info_t *rx;
+	struct dev_data* priv = netdev_priv(dev);
+	int cpu = smp_processor_id();
+
+	rx = &jumbo_rx_buff[cpu];
+	atomic_add(num, &rx->alloc_fails[priv->port]);
+	schedule_work(&mac_frin_replenish_work[cpu]);
+	//tasklet_schedule(&mac_frin_replenish_task[cpu]);
+	return 0;
 }
 
-/**********************************************************************
- * mac_refill_frin_desc -  refill rx freein buffer for a device
- * @dev -  this is per device based function
- *
- **********************************************************************/
-static int mac_refill_frin_desc(unsigned long dev)
+static void mac_frin_replenish(unsigned long  arg/* ignored */)
 {
-	struct dev_data* priv;
-	struct net_device *ndev;
-        int ret, mflags, i, code,limit;
-        struct xlp_msg msg;
-	struct sk_buff * skb;
-	uint64_t *idx_ptr;
+	jumbo_rx_info_t *rx;
+	int cpu = smp_processor_id();
+	int done = 0, i, j;
 
-	ndev = (struct net_device *) dev;
-	priv = netdev_priv(ndev);
-	ret = 0;
+	rx = &jumbo_rx_buff[cpu];
 
-	atomic64_inc(&priv->num_replenishes);
+	for (;;) {
 
-	limit = atomic64_read(&priv->frin_to_be_sent);
+		done = 0;
 
-	for(i = 0; i < limit; i++)
-	{
-		skb = nlm_xlp_alloc_skb();
-		if(!skb)
-		{
-			printk("[%s] alloc skb failed\n",__FUNCTION__);
+		for (i = 0; i < PHOENIX_MAX_MACS; i++) {
+			struct net_device *dev;
+			struct dev_data* priv;
+			atomic_t *frin_to_be_sent;
+			int num_fr_in=0;
 
-			ret = -ENOMEM;
-			break;
-		}
+			dev = dev_mac[i];
+			if (dev == 0)
+				goto skip;
 
-		skb->dev = ndev;
+			priv = netdev_priv(dev);
+			frin_to_be_sent = &rx->alloc_fails[i];
+			num_fr_in = atomic_read(frin_to_be_sent);
 
-		/* Send the free Rx desc to the MAC */
-		mac_put_skb_back_ptr(skb);
-		code = 0;
-		idx_ptr = (uint64_t*)((unsigned long)skb->data-20);
-		*idx_ptr = i;
+			//if(!(MSGRNG_OWN(priv->cfg_flag)))
+			//	goto skip;
 
-		msgrng_access_enable(mflags);
-		msg.entry[0] = (unsigned long long)virt_to_bus(skb->data) & 0xffffffffffULL;
-		msg.entry[1]= msg.entry[2] = msg.entry[3] = 0;
-		/* Send the packet to nae rx  */
-		__sync();
+			if (atomic_read(frin_to_be_sent) < 0) {
+				panic
+					("BUG?: [%s]: gmac_%d illegal value for frin_to_be_sent=%d\n",
+					 __FUNCTION__, i,
+					 atomic_read(frin_to_be_sent));
+			}
 
-                if ( (ret = nlm_hal_send_msg1(priv->nae_rx_qid, code, msg.entry[0])) )
-		{
-			print_fmn_send_error(__func__, ret);
-			printk("Unable to send configured free desc, check freein carving (qid=%d)\n", priv->nae_rx_qid);
+			if (!atomic_read(frin_to_be_sent))
+				goto skip;
 
-			/* free the buffer and return! */
-			dev_kfree_skb_any(skb);
+			for(j=0; j<num_fr_in; j++)
+				mac_frin_replenish_one_normal_msg(dev);
 
-			msgrng_access_disable(mflags);
+			atomic_sub(num_fr_in, frin_to_be_sent);
+			atomic64_add(num_fr_in, &priv->total_frin_sent);
 
-			ret = -EBUSY;
+			continue;
+		skip:
+			done++;
+		}
+		if (done == PHOENIX_MAX_MACS)
 			break;
-                }
-		msgrng_access_disable(mflags);
-
-		atomic64_dec(&priv->frin_to_be_sent);
+	}
+}
 
-		atomic64_inc(&priv->total_frin_sent);
+static int mac_fill_rxfr(struct net_device *dev)
+{
+	struct dev_data *priv = netdev_priv(dev);
+	unsigned long msgrng_flags;
+	int i, j;
+	int ret = 0;
+	struct page *pg;
+	void *va;
+	phys_t pa, phys;
+	struct jumbo_rx_cookie *rx_cookie;
+	int nr_buffs,limit;
+
+#ifdef CONFIG_PHOENIX_HW_BUFFER_MGMT
+#error "Jumbo support cannot be enabled with CONFIG_PHOENIX_HW_BUFFER_MGMT"
+#endif /* CONFIG_PHOENIX_HW_BUFFER_MGMT */
+
+	printk(" mac_fill_rxfr--------\n");
+	//if (!priv->init_frin_desc) return ret;
+	//priv->init_frin_desc = 0;	
+
+	//if(!(MSGRNG_OWN(priv->cfg_flag)))
+	//	return ret; 
+	limit = atomic64_read(&priv->frin_to_be_sent);
+	
+	for(i = 0; i < limit; i++){
+		/*  get a page */
+		pg = alloc_pages(GFP_KERNEL, 0);
+		if(pg == NULL) {
+			ret = -ENOMEM;
+			break;
+		}
+		nr_buffs = (PAGE_SIZE/jumbo_buffer_size);
+		if((i + nr_buffs) >= limit)
+			nr_buffs = (limit - i);
+		i += nr_buffs;
+
+		pa = page_to_phys(pg);
+		va = page_address(pg);
+		printk("Virt addr alloc_page=0x%lx\n", (unsigned long)va);
+		j = 0;
+		while(nr_buffs) {
+			rx_cookie = (struct jumbo_rx_cookie *)va;
+			rx_cookie->page = pg;
+			rx_cookie->page_offset = (j * jumbo_buffer_size) ;
+			/* Send the free Rx desc to the MAC */
+			msgrng_access_enable(msgrng_flags);
+			phys = pa + JUMBO_RX_OFFSET;
+			if (mac_send_fr(priv, phys, jumbo_mtu)) {
+				msgrng_access_disable(msgrng_flags);
+				if(j)
+					put_page(pg);
+				printk
+				("message_send failed!, unable to send free desc to mac\n");
+				ret = -EIO;
+				break;
+			}
+			msgrng_access_disable(msgrng_flags);
+			va += jumbo_buffer_size;
+			pa += jumbo_buffer_size;
+			/* increment ref count from second particle */
+			if(j) 
+				get_page(pg);
+			j++;
+			nr_buffs--;
+			atomic64_dec(&priv->frin_to_be_sent);
+
+			atomic64_inc(&priv->total_frin_sent);
+		}
 	}
+	return 0;
 
-        return ret;
 }
 
 /**********************************************************************
@@ -416,6 +728,12 @@ static void nlm_xlp_nae_init(void)
 
 	if (initialize_nae(cpumask_to_uint32(&cpu_present_map), 0, 0, 0))
 		return;
+	p2p_desc_mem_init();
+		
+	for (i = 0; i < MAC_FRIN_WORK_NUM; i++) {
+		//tasklet_init(&mac_frin_replenish_task[i], mac_frin_replenish, 0UL);		  
+		INIT_WORK(&mac_frin_replenish_work[i], mac_frin_replenish);
+	}
 
 	nae_fb_vc = nae_cfg.fb_vc;
 	nae_rx_vc = nae_cfg.rx_vc;
@@ -463,7 +781,7 @@ static void nlm_xlp_nae_init(void)
 				break;	
 			case INTERLAKEN_IF:
 				priv->index = INTERLAKEN;
-                                if (nae_cfg.ports[i].hw_port_id == 0) {
+				if (nae_cfg.ports[i].hw_port_id == 0) {
                                        if (dev_alloc_name(dev, "ilk0-%d") < 0)
                                                 printk("alloc name failed \n");
                                 }
@@ -476,8 +794,8 @@ static void nlm_xlp_nae_init(void)
 				priv->index=0;
 				break;
 		}
-		//nlm_print("port%d hw %d block %d index %d type %d \n",i, nae_cfg.ports[i].hw_port_id, 
-		//							priv->block, priv->index, priv->type);
+		printk("port%d hw %d block %d index %d type %d \n",i, nae_cfg.ports[i].hw_port_id, 
+									priv->block, priv->index, priv->type);
 		priv->nae_tx_qid 	= nae_cfg.ports[i].txq_range[0];
 		priv->nae_rx_qid 	= nae_cfg.ports[i].rxq;
 
@@ -486,9 +804,6 @@ static void nlm_xlp_nae_init(void)
 		dev_mac[i] = dev;
 		xlp_mac_setup_hwaddr(priv);
 
-		tasklet_init(&mac_refill_task[priv->port],
-			     (void (*)(long unsigned int))mac_refill_frin_desc,
-			     (unsigned long)dev);
 	}
 
 	entry = create_proc_read_entry("mac_stats", 0 /* def mode */ ,
@@ -514,6 +829,12 @@ static int  nlm_xlp_nae_open (struct net_device *dev)
 	int ret = 0;
 
 	if (priv->inited) return 0;
+	tso_enable(dev, 1);
+	dev->features |= NETIF_F_FRAGLIST | NETIF_F_HIGHDMA | NETIF_F_SG;
+	//dev->features |= NETIF_F_IP_CSUM | NETIF_F_HW_CSUM;
+	
+	ret = mac_fill_rxfr(dev);
+	if (ret) goto out;
 
 	if(register_xlp_msgring_handler( XLP_MSG_HANDLE_NAE_0 , nlm_xlp_nae_msgring_handler, dev))
 	{
@@ -522,8 +843,6 @@ static int  nlm_xlp_nae_open (struct net_device *dev)
 		goto out;
 	}
 
-	ret = mac_refill_frin_desc((unsigned long)dev);
-	if (ret) goto out;
 
 #ifdef ENABLE_NAE_PIC_INT
 	{
@@ -596,71 +915,205 @@ static int  nlm_xlp_nae_stop (struct net_device *dev)
 	return 0;
 }
 
+static int p2p_desc_mem_init(void)
+{
+	int cpu, cnt;
+	int dsize, tsize;
+	void *buf;
+	/* MAX_SKB_FRAGS + 4.  Out of 4, 2 will be used for skb and freeback storage */
+	dsize = ((((MAX_SKB_FRAGS + P2P_EXTRA_DESCS) * sizeof(uint64_t)) + CACHELINE_SIZE - 1) & (~((CACHELINE_SIZE)-1)));
+	tsize = dsize * MAX_TSO_SKB_PEND_REQS;
+
+	printk("%s in, dsize %d tsize %d \n", __FUNCTION__, dsize, tsize);
+
+	for(cpu = 0; cpu < NR_CPUS; cpu++) {
+		buf = cacheline_aligned_kmalloc(tsize, GFP_KERNEL);		      
+		//spin_lock_init(&p2p_desc_mem[cpu].lock);
+		if (!buf)
+			return -ENOMEM;
+		p2p_desc_mem[cpu].mem = buf;
+		for(cnt = 1; cnt < MAX_TSO_SKB_PEND_REQS; cnt++) {
+			*(unsigned long *)buf = (unsigned long)(buf + dsize);
+			buf += dsize;
+			*(unsigned long *)buf = 0;
+		}
+
+		p2p_desc_mem[cpu].dsize = dsize;
+	}
+	return 0;
+}
+
+static inline void *alloc_p2p_desc_mem(int cpu)
+{
+	void *buf;
+    	//unsigned long flags;
+    	//spin_lock_irqsave(&p2p_desc_mem[cpu].lock, flags);
+    	/*Disabling irq as the critical section shared between
+ 	inteerupt context and xmit path. */	
+    	local_irq_disable();	
+	buf = p2p_desc_mem[cpu].mem;
+	if(buf) {
+		p2p_desc_mem[cpu].mem = (void *)*(unsigned long *)(buf);
+
+	} else {
+		buf = cacheline_aligned_kmalloc(p2p_desc_mem[cpu].dsize, GFP_KERNEL);
+		p2p_dynamic_alloc_cnt[CPU_INDEX(cpu)]++;
+		//printk("alloc_p2p_desc_mem p2p_dynamic_alloc_cnt cpu=0x%x\n", cpu);	
+	}
+    	local_irq_enable();	
+	//spin_unlock_irqrestore(&p2p_desc_mem[cpu].lock, flags);
+	return buf;
+}
+
+static inline void free_p2p_desc_mem(int cpu, void *buf)
+{
+	unsigned long flags;
+    	//spin_lock_irqsave(&p2p_desc_mem[cpu].lock, flags);
+	*(unsigned long *)buf = (unsigned long)p2p_desc_mem[cpu].mem;
+	p2p_desc_mem[cpu].mem = buf;
+	//spin_unlock_irqrestore(&p2p_desc_mem[cpu].lock, flags);
+}
+
+static inline int create_p2p_desc(uint64_t paddr, uint64_t len, uint64_t *p2pmsg, int idx)
+{
+	int plen;
+	do {
+		plen = len >= MAX_PACKET_SZ_PER_MSG ? (MAX_PACKET_SZ_PER_MSG - 64): len;
+		p2pmsg[idx] = cpu_to_be64(nae_tx_desc(P2D_NEOP, 0, NULL_VFBID, plen, paddr));
+		len -= plen;
+		paddr += plen;
+		idx++;
+
+	} while(len > 0);
+	return idx;
+}
+
+static inline void create_last_p2p_desc(uint64_t *p2pmsg, struct sk_buff *skb, int idx)
+{
+	p2pmsg[idx - 1] |= cpu_to_be64(((uint64_t)P2D_EOP << 62));
+	p2pmsg[P2P_SKB_OFF] = (uint64_t)skb;
+}
+
 
-/**********************************************************************
- * nlm_xlp_nae_start_xmit -  transmit a packet from buffer
- * @dev  -  this is per device based function
- * @skb  -  data buffer to send
- **********************************************************************/
 static int nlm_xlp_nae_start_xmit(struct sk_buff *skb, struct net_device *dev)
 {
+	int mss  = 0, idx = 0, len, i ;
+	struct skb_shared_info *sp = skb_shinfo(skb);
+	struct iphdr *iph;
 	struct dev_data *priv = netdev_priv(dev);
+	uint64_t msg, mscmsg0, mscmsg1;
+	//unsigned int mflags;
+	uint64_t *p2pdesc;
+	int cpu = hard_smp_processor_id();
+	int  ret, retry_cnt = 0;
 	unsigned long mflags = 0;
-	int cpu = hard_smp_processor_id(), ret = 0;
-	struct xlp_msg msg =  { {0, 0, 0, 0} };
+	
 
-	if(!skb)
-	{
-		printk("[%s] skb is NULL\n",__FUNCTION__);
-		return -1;
-	}
-	if(skb->len == 0)
-	{
-		printk("[%s] skb empty packet\n",__FUNCTION__);
-		return -1;
+	p2pdesc = alloc_p2p_desc_mem(cpu);
+	if(p2pdesc == NULL) {
+		printk("Failed to allocate p2p desc\n");
+		dev_kfree_skb_any(skb);
+		goto out_unlock;
 	}
+	//printk("%s ipchksum_part= %d in gso_size %d nrfrags %d len %d p2pdesc %llx skb %llx headlen %d cpu=%d\n", __FUNCTION__, skb->ip_summed,
+	//		sp->gso_size, sp->nr_frags, skb->len, (uint64_t)p2pdesc, (uint64_t)skb, skb_headlen(skb), cpu);
 
-	msg.entry[0] = nae_tx_desc(P2D_NEOP, 0, cpu, 0, virt_to_bus(skb));
-	msg.entry[1] = nae_tx_desc(P2D_EOP,
-				   0,
-				   NULL_VFBID,
-				   skb->len,
-				   virt_to_bus(skb->data));
+	if (((mss = sp->gso_size) != 0) || (skb->ip_summed == CHECKSUM_PARTIAL)) {
+		u32 iphdroff, pyldoff, tcppcsum, udppcsum, l4hoff;
 
-	msg.entry[2] = msg.entry[3] = 0;
+		if (skb_header_cloned(skb) &&
+				pskb_expand_head(skb, 0, 0, GFP_ATOMIC)) {
+			dev_kfree_skb_any(skb);
+			free_p2p_desc_mem(cpu, p2pdesc);
+			goto out_unlock;
+		}
 
-	DUMP_PKT(__func__, skb->data, skb->len);
+		iph = ip_hdr(skb);
+		iphdroff = (char *)iph - (char *)skb->data;
+		l4hoff = iphdroff + ip_hdrlen(skb);
+
+		if(ip_hdr(skb)->protocol == IPPROTO_UDP){
+			pyldoff = iphdroff + ip_hdrlen(skb) + sizeof(struct udphdr);	
+			//printk("iphdroff %d udphdroff %d pyldoff %d\n", iphdroff, l4hoff, pyldoff);
+			udppcsum = udp_pseuodo_chksum((uint16_t *)((char *)iph + 12));
+			udp_hdr(skb)->check = 0;
+		}else{
+			pyldoff = iphdroff + ip_hdrlen(skb) + sizeof(struct tcphdr) + tcp_optlen(skb);
+			//printk("iphdroff %d tcphdroff %d pyldoff %d\n", iphdroff, l4hoff, pyldoff);
+			tcppcsum = tcp_pseuodo_chksum(((uint16_t *)((char *)iph + 12)));
+			tcp_hdr(skb)->check = 0;
+		}
 
-	__sync();
+		if(mss) {
+			iph->check = 0;
+			iph->tot_len = 0;
+			mscmsg0 = nae_tso_desc0(MSC, 1, TSO_IP_TCP_CHKSUM,
+				iphdroff, l4hoff, (iphdroff + 10),
+				tcppcsum, l4hoff + 16, pyldoff);
+			mscmsg1 = nae_tso_desc1(MSC, 2, 0, mss, 0, 0);
+		} else {
+			if(ip_hdr(skb)->protocol == IPPROTO_UDP){
+				mscmsg0 = nae_tso_desc0(MSC, 0, UDP_CHKSUM,
+					iphdroff, l4hoff, (iphdroff + 10),
+					udppcsum, l4hoff + 6, pyldoff);
+			}else{
+				mscmsg0 = nae_tso_desc0(MSC, 0, TCP_CHKSUM,
+					iphdroff, l4hoff, (iphdroff + 10),
+					tcppcsum, l4hoff + 16, pyldoff);
+			}
+		}
+	}
 
-	if (debug) {
-		printk("[%s]: tx_qid=%d, entry0=%llx, entry1=%llx\n", __func__,
-		       priv->nae_tx_qid, msg.entry[0], msg.entry[1]);
+
+	if((len = skb_headlen(skb)) != 0) {
+		idx = create_p2p_desc(virt_to_bus((char *)skb->data), len, p2pdesc, idx);
 	}
 
-	msgrng_access_enable(mflags);
-// retry_send:
-        if ( (ret = nlm_hal_send_msg2(priv->nae_tx_qid,
-				      0,
-				      msg.entry[0],
-				      msg.entry[1])) )
-        {
-		print_fmn_send_error(__func__, ret);
-		//printk("[%s] HACK ALERT! dropping packet(skb=%p)!\n", __func__, skb);
-		dev_kfree_skb_any(skb);
-		//goto retry_send;
-        }
+	for (i = 0; i < sp->nr_frags; i++)  {
+		skb_frag_t *fp = &sp->frags[i];
+		//printk("frags %d pageaddr %lx off %x size %d\n", i, (long)page_address(fp->page),
+		//		fp->page_offset, fp->size);
+		idx = create_p2p_desc(virt_to_bus(((char *)page_address(fp->page)) + fp->page_offset),
+				fp->size, p2pdesc, idx);
+	}
 
+	create_last_p2p_desc(p2pdesc, skb, idx);
+	msg = nae_tx_desc(P2P, 0, cpu, idx, virt_to_bus(p2pdesc));
+	
+retry_send:
+	msgrng_access_enable(mflags);
+	if(mss)
+		ret = nlm_hal_send_msg3(priv->nae_tx_qid, 0, mscmsg0, mscmsg1, msg);
+	else if(skb->ip_summed == CHECKSUM_PARTIAL)
+		ret = nlm_hal_send_msg2(priv->nae_tx_qid, 0, mscmsg0, msg);
+	else
+		ret = nlm_hal_send_msg1(priv->nae_tx_qid, 0, msg);
 	msgrng_access_disable(mflags);
-	dev->trans_start = jiffies;
+	if(ret){
+		//xlp_poll_upper(cpu);
+		retry_cnt++;
+		if(retry_cnt >= 128) {
+			dev_kfree_skb_any(skb);
+			free_p2p_desc_mem(cpu, p2pdesc);
+			STATS_ADD(priv->stats.tx_errors, 1);
+			goto out_unlock;
+		}
+		goto retry_send;
+	}
 
+	dev->trans_start = jiffies;
 	STATS_ADD(priv->stats.tx_bytes, skb->len);
-	STATS_INC(priv->stats.tx_packets);
-	priv->cpu_stats[cpu].tx_packets++;
+	STATS_ADD(priv->stats.tx_packets, 1);
+	priv->cpu_stats[cpu].tx_packets += 1;
 
+out_unlock:
+	//if(ret)
+	//	return NETDEV_TX_BUSY;
+	//	printk("Failed to send the msg\n");
 	return NETDEV_TX_OK;
-}
 
+}
+//#endif
 /**********************************************************************
  * nlm_xlp_set_multicast_list
  *
@@ -720,13 +1173,27 @@ static int nlm_xlp_nae_change_mtu(struct net_device *dev, int new_mtu)
 {
 	struct dev_data *priv = netdev_priv(dev);
 	unsigned long flags;
+	unsigned long local_jumbo_mtu;
 
-	if ((new_mtu > ETH_MTU_SIZE) || (new_mtu < MIN_ETH_FRAME_SIZE)) {
+	if ((new_mtu > (DEFAULT_JUMBO_MTU * MAX_SKB_FRAGS)) || (new_mtu < MIN_ETH_FRAME_SIZE)) {
+		return -EINVAL;
+	}
+	if(priv->type==INTERLAKEN_IF){
 		return -EINVAL;
 	}
+	printk("Setting mtu %d bytes \n", new_mtu);
 
 	spin_lock_irqsave(&priv->lock, flags);
 
+	local_jumbo_mtu = new_mtu + ETH_HLEN + ETH_FCS_LEN;
+	local_jumbo_mtu = (local_jumbo_mtu + SMP_CACHE_BYTES) & ~(SMP_CACHE_BYTES - 1);
+	if(priv->type==SGMII_IF){
+		nlm_hal_set_sgmii_framesize(priv->block, priv->index, local_jumbo_mtu);
+	}
+	if(priv->type==XAUI_IF){
+		nlm_hal_set_xaui_framesize(priv->block, local_jumbo_mtu, local_jumbo_mtu);
+	}
+
 	dev->mtu = new_mtu;
 
 	if (netif_running(dev)) {
@@ -814,6 +1281,8 @@ static irqreturn_t nlm_xlp_nae_int_handler(int irq, void *dev_id)
 }
 #endif
 
+
+
 /**********************************************************************
  * nlm_xlp_nae_msgring_handler -  message ring interrupt handler
  * @vc-  virtual channel number
@@ -827,10 +1296,12 @@ static void nlm_xlp_nae_msgring_handler(uint32_t vc, uint32_t src_id,
 {
         struct net_device *pdev;
         struct dev_data *priv;
-	unsigned int len, port = 0, context;
-        uint64_t addr , vaddr;
+	unsigned int len, hlen, port = 0, context;
+        uint64_t addr , vaddr = 0;
 	struct sk_buff* skb;
+	struct jumbo_rx_cookie *rx_cookie = NULL;
 	int cpu = 0;
+	uint64_t *p2pfbdesc;
 
 	cpu = hard_smp_processor_id();
 	vc = vc & 0x03;
@@ -844,14 +1315,29 @@ static void nlm_xlp_nae_msgring_handler(uint32_t vc, uint32_t src_id,
 	{
 		/* Process Transmit Complete, addr is the skb pointer */
 		addr = msg0 & 0xffffffffffULL;
+		
 
-		if (drop_uboot_pkt) {
+		if (!addr || drop_uboot_pkt) {
 			if ( (addr >= (192<<20)) && (addr < (256 << 20)) ) {
 				printk("Dropping firmware TXC packet (addr=%llx)!\n", addr);
 				stats_uboot_pkts++;
 				return;
 			}
-		}
+		}	
+		context = (msg0 >> 40) & 0x3fff;
+		port = cntx2port[context];
+		//nlm_xlp_free_skb(addr, port);
+//#if 0		
+		p2pfbdesc = bus_to_virt(addr);
+		skb = (struct sk_buff *)(p2pfbdesc[P2P_SKB_OFF]);
+		priv = netdev_priv(skb->dev);
+		free_p2p_desc_mem(cpu, p2pfbdesc);
+		//schedule_work(&tx_free_buffer_work[cpu]);
+		if(skb)
+			dev_kfree_skb_any(skb);
+		priv->cpu_stats[cpu].txc_packets++;
+//#endif
+#if 0
 
 		/* context field is currently unused */
 		context = (msg0 >> 40) & 0x3fff;
@@ -876,18 +1362,29 @@ static void nlm_xlp_nae_msgring_handler(uint32_t vc, uint32_t src_id,
 			printk("[%s]: [txc] Null skb? paddr = %llx (halting cpu!)\n", __func__, addr);
 			cpu_halt();
 		}
+#endif
 	}
 	else if(vc == nae_rx_vc && size == 2)
 	{
 		int bad_pkt = 0;
+		int is_p2p, num_p2d=0, tot_desc=0, idx;
 		int err = (msg1 >> 4) & 0x1;
 		int ip_csum_valid = (msg1 >> 3) & 0x1;
 		int tcp_csum_valid = (msg1 >> 2) & 0x1;
+		uint64_t p2d_addr[MAX_SKB_FRAGS];
+		uint32_t p2d_len[MAX_SKB_FRAGS];
 
 		/* Rx packet */
-		addr	= msg1 & 0xffffffffc0ULL;
+		is_p2p  = msg1 & 0x1;	
+		addr	= msg1 & 0xffffffffe0ULL;
 		len	= (msg1 >> 40) & 0x3fff;
 		context = (msg1 >> 54) & 0x3ff;
+		if(is_p2p){
+			num_p2d = len;
+			//printk("P2p size=0x%x\n", len);
+		}	
+		//printk("[%s] src_id=%d vc = %d, size = %d, entry0=%llx entry1=%llx\n", __func__,
+                 //      src_id, vc, size, msg0, msg1);
 
 #ifdef DEBUG_RXPKT_ADDR_NULL
 		if (addr == 0) {
@@ -899,6 +1396,7 @@ static void nlm_xlp_nae_msgring_handler(uint32_t vc, uint32_t src_id,
 #endif
 		if (err) bad_pkt = 1;
 
+
 		if (drop_uboot_pkt) {
 			if ( (addr >= (192<<20)) && (addr < (256 << 20)) ) {
 				printk("Dropping firmware RX packet (addr=%llx)!\n", addr);
@@ -924,7 +1422,39 @@ static void nlm_xlp_nae_msgring_handler(uint32_t vc, uint32_t src_id,
 		}
 		priv = netdev_priv(pdev);
 
-		vaddr = (uint64_t)bus_to_virt(addr);
+		/*check what kind of desc we received*/
+		if(is_p2p){
+			uint64_t p2d;
+			int idx;
+			struct page *pg;
+			//printk("Total p2d in p2p desc are = 0x%x\n", len);
+			num_p2d = len;
+			len = 0;
+			rx_cookie = get_rx_cookie(addr);
+			pg = rx_cookie->page;
+			/*free page count for P2P desc as it is not going to network stack */
+			put_page(pg);
+			/*Get actual length*/
+			for(idx=0; idx<num_p2d; idx++){
+				vaddr = (uint64_t)bus_to_virt(addr + (8*idx)); //got p2d virt addr
+				p2d = be64_to_cpu(*(uint64_t*)vaddr);
+				p2d_addr[idx] = p2d & 0xffffffffe0ULL;
+				p2d_len[idx] = (p2d >> 40) & 0x3fff;
+				bad_pkt = (p2d >> 4) & 0x1;
+				len += (p2d >> 40) & 0x3fff;
+				//printk("P2D is at = 0x%lx physbuff= 0x%lx len= 0x%x\n", vaddr, p2d_addr[idx],  p2d_len[idx] );
+			}
+			tot_desc = num_p2d +1; //p2ds + p2p
+			//printk("Total packet length is = 0x%x and desc = 0x%x\n", len, tot_desc);
+		}else{ /*only one P2Dl_skb*/
+			vaddr = (uint64_t)bus_to_virt(addr);
+			p2d_addr[0] = addr;
+			p2d_len[0] = len - MAC_CRC_LEN; 
+			len = len - MAC_CRC_LEN;
+			num_p2d = 1;
+			tot_desc = 1;
+			//printk("P2D  len = %d\n", len);
+		}
 
 		if (debug) {
 			printk("[%s][RX] addr=%llx, len=%d, context=%d, port=%d, vaddr=%llx\n",
@@ -932,8 +1462,51 @@ static void nlm_xlp_nae_msgring_handler(uint32_t vc, uint32_t src_id,
 		}
 
 		DUMP_PKT("RX Packet: ", (unsigned char *)vaddr, len);
+		if (bad_pkt) {
+			struct page *pg;
+			int rx_offset;
+			STATS_INC(priv->stats.rx_errors);
+			STATS_INC(priv->stats.rx_dropped);
+			/*increase free count for used pages*/
+			for(idx=0; idx<num_p2d; idx++){
+				rx_cookie = get_rx_cookie(p2d_addr[idx]);	
+				rx_offset =  JUMBO_RX_OFFSET;
+				pg = rx_cookie->page;
+				put_page(pg);
+			}
+			mac_frin_replenish_msgs(dev_mac[port], tot_desc);
+			return;
+		}
+		mac_frin_replenish_msgs(dev_mac[port], tot_desc);
 
-		len = len  - MAC_CRC_LEN;
+		/* allocate an skb for header */
+		skb = dev_alloc_skb(NETL_JUMBO_SKB_HDR_LEN + 16);
+		if(skb == NULL) {
+			printk("FAILED TO ALLOCATE skb\n");
+			STATS_INC(priv->stats.rx_errors);
+			
+			recycle_rx_desc(addr, pdev);
+			return;
+		}
+		skb->dev = dev_mac[port];
+		hlen = (len > NETL_JUMBO_SKB_HDR_LEN) ? 
+				NETL_JUMBO_SKB_HDR_LEN: len;
+		/* after this call, skb->data is pointing to start of MAChdr */
+		build_skb(skb, &p2d_addr[0], &p2d_len[0], num_p2d, hlen, len);
+		if (hlen == len) {
+			put_page(skb_shinfo(skb)->frags[0].page);
+			skb_shinfo(skb)->nr_frags = 0;
+			skb->data_len = 0;
+		}
+		skb->protocol = eth_type_trans(skb, skb->dev);
+		skb->dev->last_rx = jiffies;
+		netif_rx(skb);
+		/* Update Stats */
+		STATS_ADD(priv->stats.rx_bytes, len);
+		STATS_INC(priv->stats.rx_packets);
+		priv->cpu_stats[cpu].rx_packets++;
+		return;
+#if 0
 
 		skb = mac_get_skb_back_ptr(vaddr);
 		if (!skb) {
@@ -986,6 +1559,7 @@ static void nlm_xlp_nae_msgring_handler(uint32_t vc, uint32_t src_id,
 			tasklet_schedule(&mac_refill_task[port]);
 			//mac_refill_frin_desc((unsigned long) skb->dev) ;
 		}
+#endif
 	} else {
 		printk("[%s]: wrong vc=%d or size=%d?\n", __func__, vc, size);
 	}
@@ -1026,11 +1600,12 @@ static int xlp_mac_proc_read(char *page, char **start, off_t off,
 			       atomic64_read(&priv->total_frin_sent));
 
 		len += sprintf(page + len,
-			       "per port@%d: %lu(rxp) %lu(rxb) %lu(txp) %lu(txb)\n",
+			       "per port@%d: %lu(rxp) %lu(rxb) %lu(txp) %lu(txerr) %lu(txb)\n",
 			       i,
 			       STATS_READ(priv->stats.rx_packets),
 			       STATS_READ(priv->stats.rx_bytes),
 			       STATS_READ(priv->stats.tx_packets),
+			       STATS_READ(priv->stats.tx_errors),
 			       STATS_READ(priv->stats.tx_bytes));
 
 		for (cpu = 0; cpu < NR_CPUS ; cpu++) {
@@ -1045,6 +1620,9 @@ static int xlp_mac_proc_read(char *page, char **start, off_t off,
 				       cpu, tx, txc, rx, ints);
 		}
 	}
+	for (cpu = 0; cpu < NR_CPUS; cpu++) {
+		len += sprintf(page + len, "per cpu@%d: %lu(p2p_dynamic_alloc_cnt)\n", cpu ,p2p_dynamic_alloc_cnt[CPU_INDEX(cpu)]);
+	}
 
 	*eof = 1;
 
diff --git a/drivers/net/ethernet/broadcom/nae/xlp_nae.h b/drivers/net/ethernet/broadcom/nae/xlp_nae.h
index 485f2e1..6e241c3 100644
--- a/drivers/net/ethernet/broadcom/nae/xlp_nae.h
+++ b/drivers/net/ethernet/broadcom/nae/xlp_nae.h
@@ -1,7 +1,7 @@
 #ifndef _XLP_NAE_H
 #define _XLP_NAE_H
 
-#define MAC_MAX_FRAME_SIZE      1600
+#define MAC_MAX_FRAME_SIZE      9214
 #define MAC_SKB_BACK_PTR_SIZE   SMP_CACHE_BYTES
 
 
@@ -49,6 +49,7 @@ struct phy_info {
         uint32_t *serdes_addr;
 };
 
+
 struct dev_data
 {
         struct net_device *dev;
-- 
1.9.1

