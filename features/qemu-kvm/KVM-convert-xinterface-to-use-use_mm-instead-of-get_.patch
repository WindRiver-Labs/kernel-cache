From b42a282e4c199603a351f36449e85678f2f236a1 Mon Sep 17 00:00:00 2001
From: Gregory Haskins <ghaskins@novell.com>
Date: Tue, 25 Aug 2009 20:28:13 -0400
Subject: [PATCH] KVM: convert xinterface to use use_mm instead of get_user_pages.

use_mm+copy_to/from_user is an optimized path that is much faster than
get_user_pages for cases where the page is already in memory.  Since
devices typically converse with memory recently touched, the probability
that a given page is already resident is fairly high.  Therefore, we
can get a performance boost if we happen to be calling from a context
that is conducive to using the copy_to/from_user variants (via current->mm
or use_mm).  As it so happens, most contexts will be in one or the
other category anyway, so this results in a large (~25%) throughput
improvement overall.

Numbers on 10GE 802.x ethernet jumped from 4560Gb/s to 5708Gb/s with this
patch.

Signed-off-by: Gregory Haskins <ghaskins@novell.com>
[kvm->slots_lock as a mutex]
Signed-off-by: Mark Asselstine <mark.asselstine@windriver.com>
---
 virt/kvm/xinterface.c |   91 +++++++++++++++++++++++++++++++++++++++----------
 1 files changed, 73 insertions(+), 18 deletions(-)

diff --git a/virt/kvm/xinterface.c b/virt/kvm/xinterface.c
index a7fb1c2..c2accfe 100644
--- a/virt/kvm/xinterface.c
+++ b/virt/kvm/xinterface.c
@@ -29,6 +29,7 @@
 #include <linux/vmalloc.h>
 #include <linux/highmem.h>
 #include <linux/module.h>
+#include <linux/mmu_context.h>
 #include <linux/kvm_host.h>
 #include <linux/kvm_xinterface.h>
 
@@ -144,20 +145,18 @@ const static struct kvm_xvmap_ops _xvmap_ops = {
 
 /*------------------------------------------------------------------------*/
 
+/*
+ * This function is invoked in the cases where a process context other
+ * than _intf->mm tries to copy data.  Otherwise, we use copy_to_user()
+ */
 static unsigned long
-xinterface_copy_to(struct kvm_xinterface *intf, unsigned long gpa,
-		   const void *src, unsigned long n)
+_slow_copy_to_user(struct _xinterface *_intf, unsigned long dst,
+		    const void *src, unsigned long n)
 {
-	struct _xinterface *_intf = to_intf(intf);
 	struct task_struct *p = _intf->task;
 	struct mm_struct *mm = _intf->mm;
-	unsigned long dst;
-
-	mutex_lock(&_intf->kvm->slots_lock);
 
-	dst = gpa_to_hva(_intf, gpa);
-
-	while (dst && n) {
+	while (n) {
 		unsigned long offset = offset_in_page(dst);
 		unsigned long len = PAGE_SIZE - offset;
 		int ret;
@@ -187,25 +186,52 @@ xinterface_copy_to(struct kvm_xinterface *intf, unsigned long gpa,
 		n -= len;
 	}
 
-	mutex_unlock(&_intf->kvm->slots_lock);
-
 	return n;
 }
 
 static unsigned long
-xinterface_copy_from(struct kvm_xinterface *intf, void *dst,
-		     unsigned long gpa, unsigned long n)
+xinterface_copy_to(struct kvm_xinterface *intf, unsigned long gpa,
+		   const void *src, unsigned long n)
 {
 	struct _xinterface *_intf = to_intf(intf);
-	struct task_struct *p = _intf->task;
-	struct mm_struct *mm = _intf->mm;
-	unsigned long src;
+	unsigned long dst;
+	bool kthread = !current->mm;
 
 	mutex_lock(&_intf->kvm->slots_lock);
 
-	src = gpa_to_hva(_intf, gpa);
+	dst = gpa_to_hva(_intf, gpa);
+	if (!dst)
+		goto out;
+
+	if (kthread)
+		use_mm(_intf->mm);
+
+	if (kthread || _intf->task == current)
+		n = copy_to_user((void *)dst, src, n);
+	else
+		n = _slow_copy_to_user(_intf, dst, src, n);
+
+	if (kthread)
+		unuse_mm(_intf->mm);
+
+out:
+	mutex_unlock(&_intf->kvm->slots_lock);
 
-	while (src && n) {
+	return n;
+}
+
+/*
+ * This function is invoked in the cases where a process context other
+ * than _intf->mm tries to copy data.  Otherwise, we use copy_from_user()
+ */
+static unsigned long
+_slow_copy_from_user(struct _xinterface *_intf, void *dst,
+		     unsigned long src, unsigned long n)
+{
+	struct task_struct *p = _intf->task;
+	struct mm_struct *mm = _intf->mm;
+
+	while (n) {
 		unsigned long offset = offset_in_page(src);
 		unsigned long len = PAGE_SIZE - offset;
 		int ret;
@@ -234,6 +260,35 @@ xinterface_copy_from(struct kvm_xinterface *intf, void *dst,
 		n -= len;
 	}
 
+	return n;
+}
+
+static unsigned long
+xinterface_copy_from(struct kvm_xinterface *intf, void *dst,
+		     unsigned long gpa, unsigned long n)
+{
+	struct _xinterface *_intf = to_intf(intf);
+	unsigned long src;
+	bool kthread = !current->mm;
+
+	mutex_lock(&_intf->kvm->slots_lock);
+
+	src = gpa_to_hva(_intf, gpa);
+	if (!src)
+		goto out;
+
+	if (kthread)
+		use_mm(_intf->mm);
+
+	if (kthread || _intf->task == current)
+		n = copy_from_user(dst, (void *)src, n);
+	else
+		n = _slow_copy_from_user(_intf, dst, src, n);
+
+	if (kthread)
+		unuse_mm(_intf->mm);
+
+out:
 	mutex_unlock(&_intf->kvm->slots_lock);
 
 	return n;
-- 
1.7.4.1

