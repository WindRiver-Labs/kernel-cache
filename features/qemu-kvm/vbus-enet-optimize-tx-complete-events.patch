From b33a4be0c1d8fb40f48762102b8556b2c472c348 Mon Sep 17 00:00:00 2001
From: Gregory Haskins <ghaskins@novell.com>
Date: Mon, 25 Jan 2010 07:41:35 -0500
Subject: [PATCH 108/119] vbus-enet: optimize tx-complete events

We don't need to use the VALID index when TXC is enabled, and can therefore
eliminate the janitorial functions of cleanup up the VALID index for
every completion event we receive.

Signed-off-by: Gregory Haskins <ghaskins@novell.com>
---
 drivers/net/vbus-enet.c |   61 ++++++++++++++++++++++++-----------------------
 1 files changed, 31 insertions(+), 30 deletions(-)

diff --git a/drivers/net/vbus-enet.c b/drivers/net/vbus-enet.c
index bfb3fc4..5b579f6 100644
--- a/drivers/net/vbus-enet.c
+++ b/drivers/net/vbus-enet.c
@@ -790,6 +790,7 @@ vbus_enet_tx_start(struct sk_buff *skb, struct net_device *dev)
 {
 	struct vbus_enet_priv *priv = netdev_priv(dev);
 	struct ioq_iterator    iter;
+	enum ioq_idx_type      type;
 	int ret;
 	unsigned long flags;
 
@@ -808,11 +809,20 @@ vbus_enet_tx_start(struct sk_buff *skb, struct net_device *dev)
 		return 1;
 	}
 
-	/*
-	 * We want to iterate on the tail of both the "inuse" and "valid" index
-	 * so we specify the "both" index
-	 */
-	ret = ioq_iter_init(priv->tx.veq.queue, &iter, ioq_idxtype_both,
+	if (priv->evq.txc)
+		/*
+		 * We only need the inuse index when we use TXC
+		 */
+		type = ioq_idxtype_inuse;
+	else
+		/*
+		 * We want to iterate on the tail of both the "inuse" and
+		 * "valid" index (since valid will track our consumed/completed
+		 * packets) so we specify the "both" index
+		 */
+		type = ioq_idxtype_both;
+
+	ret = ioq_iter_init(priv->tx.veq.queue, &iter, type,
 			    IOQ_ITER_AUTOUPDATE);
 	BUG_ON(ret < 0);
 
@@ -933,13 +943,10 @@ vbus_enet_skb_complete(struct vbus_enet_priv *priv, struct sk_buff *skb)
 
 /*
  * reclaim any outstanding completed tx packets
- *
- * assumes priv->lock held
  */
-static struct sk_buff *
-vbus_enet_tx_reap_one(struct vbus_enet_priv *priv)
+static void
+vbus_enet_tx_reap(struct vbus_enet_priv *priv)
 {
-	struct sk_buff *skb = NULL;
 	struct ioq_iterator iter;
 	unsigned long flags;
 	int ret;
@@ -958,7 +965,8 @@ vbus_enet_tx_reap_one(struct vbus_enet_priv *priv)
 	ret = ioq_iter_seek(&iter, ioq_seek_head, 0, 0);
 	BUG_ON(ret < 0);
 
-	if (iter.desc->valid && !iter.desc->sown) {
+	while (iter.desc->valid && !iter.desc->sown) {
+		struct sk_buff *skb = NULL;
 
 		if (priv->sg) {
 			struct venet_sg *vsg;
@@ -971,6 +979,8 @@ vbus_enet_tx_reap_one(struct vbus_enet_priv *priv)
 		/* Reset the descriptor */
 		iter.desc->valid  = 0;
 
+		vbus_enet_skb_complete(priv, skb);
+
 		/* Advance the valid-index head */
 		ret = ioq_iter_pop(&iter, 0);
 		BUG_ON(ret < 0);
@@ -987,23 +997,6 @@ vbus_enet_tx_reap_one(struct vbus_enet_priv *priv)
 	}
 
 	spin_unlock_irqrestore(&priv->lock, flags);
-
-	return skb;
-}
-
-static void
-vbus_enet_tx_reap(struct vbus_enet_priv *priv)
-{
-	struct sk_buff *skb;
-
-	while ((skb = vbus_enet_tx_reap_one(priv))) {
-		if (!priv->evq.txc)
-			/*
-			 * We are responsible for freeing the packet upon
-			 * reap if TXC is not enabled
-			 */
-			vbus_enet_skb_complete(priv, skb);
-	}
 }
 
 static void
@@ -1080,9 +1073,17 @@ evq_txc_event(struct vbus_enet_priv *priv,
 	struct venet_event_txc *event =
 		(struct venet_event_txc *)header;
 
-	vbus_enet_tx_reap(priv);
-
 	vbus_enet_skb_complete(priv, (struct sk_buff *)(unsigned long)event->cookie);
+
+	/*
+	 * If we were previously stopped due to flow control, restart the
+	 * processing
+	 */
+	if (netif_queue_stopped(priv->dev)
+	    && !ioq_full(priv->tx.veq.queue, ioq_idxtype_inuse)) {
+		PDEBUG(priv->dev, "re-enabling tx queue\n");
+		netif_wake_queue(priv->dev);
+	}
 }
 
 static void
-- 
1.6.5.2

