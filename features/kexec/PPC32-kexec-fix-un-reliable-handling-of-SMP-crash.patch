From d0c7181c151d32dc53801a5a702f2cfd871d5c88 Mon Sep 17 00:00:00 2001
From: Benjamin Walsh <benjamin.walsh@windriver.com>
Date: Tue, 6 Apr 2010 18:51:47 -0400
Subject: [PATCH 5/5] PPC32/kexec: fix un-reliable handling of SMP crash

Due to the fact that handling a crash takes a different code path
than fast-reboot, and that the previous SMP handling patches did
not take it into account, crash handling was up until now unreliable
on SMP machines.

The generic PPC crash handling uses its own IPI handler to put
secondary CPUs in a tight spin and locks their local interrupts,
before calling the generic kexec code that brings the machine
down. The generic path thus cannot use IPIs to force CPUs onto
the intermediary control code page.

To solve this problem, it figures out if the code path it is coming
from is the crash case or the normal case. If it is the crash case,
it forces CPUs off their first spin loop by writing to the variable
they are spinning on. This variable doubles as a pointer to the kexec
image control structure needed to bring the CPUs into the control page,
instead of introducing another variable. When the CPUs are taken out of
their spin loop, they continue down the generic path with different
reponsibilities depending if they are hardware CPU0 or not (CPU0 has
to be the boot CPU in the new kernel).

Signed-off-by: Benjamin Walsh <benjamin.walsh@windriver.com>
---
 arch/powerpc/include/asm/kexec.h       |    3 ++
 arch/powerpc/kernel/crash.c            |   11 ++++++---
 arch/powerpc/kernel/machine_kexec_32.c |   38 +++++++++++++++++++++++++++++--
 3 files changed, 45 insertions(+), 7 deletions(-)

diff --git a/arch/powerpc/include/asm/kexec.h b/arch/powerpc/include/asm/kexec.h
index ed00a49..5e6253d 100644
--- a/arch/powerpc/include/asm/kexec.h
+++ b/arch/powerpc/include/asm/kexec.h
@@ -82,6 +82,9 @@ extern void machine_kexec_simple(struct kimage *image);
 extern void crash_kexec_secondary(struct pt_regs *regs);
 extern int overlaps_crashkernel(unsigned long start, unsigned long size);
 extern void reserve_crashkernel(void);
+#ifdef CONFIG_PPC32
+extern int kexec_is_handling_crash(void);
+#endif
 
 #else /* !CONFIG_KEXEC */
 static inline int kexec_sr_activated(int cpu) { return 0; }
diff --git a/arch/powerpc/kernel/crash.c b/arch/powerpc/kernel/crash.c
index 076c9f5..3a846ab 100644
--- a/arch/powerpc/kernel/crash.c
+++ b/arch/powerpc/kernel/crash.c
@@ -93,11 +93,7 @@ void crash_ipi_callback(struct pt_regs *regs)
 	if (ppc_md.kexec_cpu_down)
 		ppc_md.kexec_cpu_down(1, 1);
 
-#ifdef CONFIG_PPC64
 	kexec_smp_wait();
-#else
-	for (;;);	/* FIXME */
-#endif
 
 	/* NOTREACHED */
 }
@@ -418,3 +414,10 @@ void default_machine_crash_shutdown(struct pt_regs *regs)
 	if (ppc_md.kexec_cpu_down)
 		ppc_md.kexec_cpu_down(1, 0);
 }
+
+#ifdef CONFIG_PPC32
+int kexec_is_handling_crash(void)
+{
+	return (cpus_weight(cpus_in_crash) != 0);
+}
+#endif
diff --git a/arch/powerpc/kernel/machine_kexec_32.c b/arch/powerpc/kernel/machine_kexec_32.c
index 58d014e..c1df33e 100644
--- a/arch/powerpc/kernel/machine_kexec_32.c
+++ b/arch/powerpc/kernel/machine_kexec_32.c
@@ -16,6 +16,7 @@
 #include <asm/cacheflush.h>
 #include <asm/hw_irq.h>
 #include <asm/io.h>
+#include <asm/kexec.h>
 
 typedef NORET_TYPE void (*relocate_new_kernel_t)(
 				unsigned long indirection_page,
@@ -164,7 +165,6 @@ static void _smp_kexec_wait_for_secondaries(void *arg)
 	volatile u32 *ready;	 /* addr of the relocated ready variable,
 				  * we spin on it, don't want it to be
 				  * optimized out. */
-	char buffer[32];
 
 	local_irq_disable();
 	ready = (void*)kexec_find_reloc((struct kimage *)arg,
@@ -182,6 +182,7 @@ static void _smp_kexec_leave_kernel(void *arg)
 	kexec_leave_kernel(arg);
 }
 
+static struct kimage * __crash_smp_flag = NULL;
 void default_kexec_stop_cpus(void *arg)
 {
 	int cpu;
@@ -191,23 +192,54 @@ void default_kexec_stop_cpus(void *arg)
 	 * the rest of the shutdown sequence, then put ourselves on
 	 * a spin; if we're CPU0, call CPU1 to put itself on a spin,
 	 * then do the rest of the shutdown sequence. */
+
+	/* if this is coming while handling a crash, the CPU that did not
+	 * crash is already spinning in a loop with its interrupts locked:
+	 * get it out of that loop and execute its normal kexec shutdown
+	 * sequence */
 	preempt_disable();
 	/* get hardware CPU# from special Processor Identity Register */
 	cpu = mfspr(SPRN_PIR);
 	if (0 == cpu) {
 		/* shutdown cpu 1 and wait for it */
-		smp_call_function(_smp_kexec_secondary_cpu_down, arg, 0);
+		if(kexec_is_handling_crash()) {
+			__crash_smp_flag = arg;
+			smp_mb();
+		} else {
+			smp_call_function(_smp_kexec_secondary_cpu_down, arg, 0);
+		}
 		_smp_kexec_wait_for_secondaries(arg);
 
 		/* was called from default_machine_kexec, continues there */
 	} else {
-		smp_call_function(_smp_kexec_leave_kernel, arg, 0);
+		if(kexec_is_handling_crash()) {
+			__crash_smp_flag = arg;
+			smp_mb();
+		} else {
+			smp_call_function(_smp_kexec_leave_kernel, arg, 0);
+		}
 		_smp_kexec_secondary_cpu_down(arg);
 
 		/* not reached, going to wait on
 		 * relocate_new_kernel_secondary_spin() */
 	}
 }
+
+void kexec_smp_wait(void)
+{
+	int cpu;
+	while(!__crash_smp_flag) {
+		cpu_relax();
+	}
+	/* get hardware CPU# from special Processor Identity Register */
+	cpu = mfspr(SPRN_PIR);
+	if(0 == cpu) {
+		_smp_kexec_leave_kernel(__crash_smp_flag);
+	} else {
+		_smp_kexec_secondary_cpu_down(__crash_smp_flag);
+	}
+}
+
 #endif /* CONFIG_SMP */
 
 int default_machine_kexec_prepare(struct kimage *image)
-- 
1.6.5.2

