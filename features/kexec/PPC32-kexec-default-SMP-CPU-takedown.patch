From 34207bfd4e0fd0e26006844ad5b5d456dd68c13f Mon Sep 17 00:00:00 2001
From: Benjamin Walsh <benjamin.walsh@windriver.com>
Date: Tue, 30 Mar 2010 18:24:07 -0400
Subject: [PATCH 3/5] PPC32/kexec: default SMP CPU takedown

Default SMP kexec CPU takedown implementation. Can be overridden by
platform-specific implementation if needed.

Signed-off-by: Benjamin Walsh <benjamin.walsh@windriver.com>
---
 arch/powerpc/kernel/machine_kexec_32.c |  101 ++++++++++++++++++++++++++++++++
 arch/powerpc/kernel/misc_32.S          |   68 +++++++++++++++++++++-
 2 files changed, 167 insertions(+), 2 deletions(-)

diff --git a/arch/powerpc/kernel/machine_kexec_32.c b/arch/powerpc/kernel/machine_kexec_32.c
index 7cdb080..58d014e 100644
--- a/arch/powerpc/kernel/machine_kexec_32.c
+++ b/arch/powerpc/kernel/machine_kexec_32.c
@@ -12,6 +12,7 @@
 #include <linux/kexec.h>
 #include <linux/mm.h>
 #include <linux/string.h>
+#include <linux/delay.h>
 #include <asm/cacheflush.h>
 #include <asm/hw_irq.h>
 #include <asm/io.h>
@@ -38,6 +39,9 @@ void smp_stop_cpus(struct kimage *image)
 
 	smp_ops->kexec_stop_cpus(image);
 }
+extern u32 relocate_new_kernel_secondary_spin;
+extern u32 relocate_new_kernel_spin_addr;
+extern u32 relocate_new_kernel_ready;
 #else /* !CONFIG_SMP */
 void smp_stop_cpus(struct kimage *image)
 {
@@ -57,6 +61,20 @@ void smp_stop_cpus(struct kimage *image)
 
 extern const unsigned char relocate_new_kernel[];
 extern const unsigned int relocate_new_kernel_size;
+typedef void(*rnk_t)(unsigned long *, unsigned long *, unsigned long);
+
+#ifdef CONFIG_SMP
+/* This is currently only used by SMP code but could be used by UP or
+ * shared code if needed: if that's ever the case, remove conditional
+ * compile flag.
+ */
+static u32 kexec_find_reloc(struct kimage *image, u32 symbol)
+{
+	u32 base = (u32)page_address(image->control_code_page);
+	u32 offset = symbol - (u32)relocate_new_kernel;
+	return base + offset;
+}
+#endif
 
 /* This is in its own routine since it can be called by a CPU that is not
  * the one on which 'kexec -e' was invoked, in some special cases, eg. a
@@ -109,6 +127,89 @@ void default_machine_kexec(struct kimage *image)
 	/* not reached */
 }
 
+#ifdef CONFIG_SMP
+/* CPU 1 will always be the one calling this function */
+static void _smp_kexec_secondary_cpu_down(void *arg)
+{
+	u32 rnkss; 	 /* relocate_new_kernel_secondary_spin */
+	u32 spin;	 /* addr of the relocated start address
+			  * variable on which CPU1 will spin */
+	u32 ready;	 /* addr of the relocated ready variable */
+	rnk_t rnk;	 /* relocate_new_kernel() */
+
+	struct kimage *image = (struct kimage *)arg;
+
+	rnkss = (u32)&relocate_new_kernel_secondary_spin;
+	rnkss = virt_to_phys((void *)kexec_find_reloc(image, rnkss));
+
+	spin  = (u32)((&relocate_new_kernel_spin_addr));
+	spin  = virt_to_phys((void *)kexec_find_reloc(image, spin));
+
+	ready = (u32)&relocate_new_kernel_ready;
+	ready = virt_to_phys((void *)kexec_find_reloc(image, ready));
+
+	rnk = (rnk_t)kexec_find_reloc((struct kimage *)arg,
+				(u32)relocate_new_kernel);
+
+	local_irq_disable();
+
+	flush_icache_range((u32)rnk, (u32)rnk + KEXEC_CONTROL_PAGE_SIZE);
+
+	rnk((unsigned long *)spin, (unsigned long *)ready, rnkss);
+	/* not reached */
+}
+
+static void _smp_kexec_wait_for_secondaries(void *arg)
+{
+	volatile u32 *ready;	 /* addr of the relocated ready variable,
+				  * we spin on it, don't want it to be
+				  * optimized out. */
+	char buffer[32];
+
+	local_irq_disable();
+	ready = (void*)kexec_find_reloc((struct kimage *)arg,
+				(u32)&relocate_new_kernel_ready);
+	while(!(*ready)) {
+		cpu_relax();
+	}
+	mdelay(1);	/* should be plenty for cpu1 to start spinning
+			 * on the start address variable */
+}
+
+static void _smp_kexec_leave_kernel(void *arg)
+{
+	_smp_kexec_wait_for_secondaries(arg);
+	kexec_leave_kernel(arg);
+}
+
+void default_kexec_stop_cpus(void *arg)
+{
+	int cpu;
+
+	/* Initialization from head_[32|fsl_booke].S expects HW CPU #0 as
+	 * the boot CPU: thus, if we're CPU1, call CPU0 and have it do
+	 * the rest of the shutdown sequence, then put ourselves on
+	 * a spin; if we're CPU0, call CPU1 to put itself on a spin,
+	 * then do the rest of the shutdown sequence. */
+	preempt_disable();
+	/* get hardware CPU# from special Processor Identity Register */
+	cpu = mfspr(SPRN_PIR);
+	if (0 == cpu) {
+		/* shutdown cpu 1 and wait for it */
+		smp_call_function(_smp_kexec_secondary_cpu_down, arg, 0);
+		_smp_kexec_wait_for_secondaries(arg);
+
+		/* was called from default_machine_kexec, continues there */
+	} else {
+		smp_call_function(_smp_kexec_leave_kernel, arg, 0);
+		_smp_kexec_secondary_cpu_down(arg);
+
+		/* not reached, going to wait on
+		 * relocate_new_kernel_secondary_spin() */
+	}
+}
+#endif /* CONFIG_SMP */
+
 int default_machine_kexec_prepare(struct kimage *image)
 {
 	return 0;
diff --git a/arch/powerpc/kernel/misc_32.S b/arch/powerpc/kernel/misc_32.S
index f2fb07f..89faf4f 100644
--- a/arch/powerpc/kernel/misc_32.S
+++ b/arch/powerpc/kernel/misc_32.S
@@ -732,9 +732,14 @@ _GLOBAL(__main)
 	 */
 	.globl relocate_new_kernel
 relocate_new_kernel:
+	/* CPU0: */
 	/* r3 = page_list   */
 	/* r4 = reboot_code_buffer */
 	/* r5 = start_address      */
+	/* CPU1: */
+	/* r3 = spin variable   */
+	/* r4 = ready variable */
+	/* r5 = address of relocate_new_kernel_secondary_spin */
 
 #ifdef CONFIG_FSL_BOOKE
 	/*
@@ -815,7 +820,16 @@ relocate_new_kernel:
 	mr	r8, r0
 	ori     r8, r8, MSR_RI|MSR_ME
 	mtspr	SPRN_SRR1, r8
+#ifdef CONFIG_SMP
+	mfspr	r6, SPRN_PIR
+	cmpwi	r6, 0
+	beq	cpu0
+	addi	r8, r4, 1f - relocate_new_kernel_ready
+	b	all_cpus
+#endif
+cpu0:
 	addi	r8, r4, 1f - relocate_new_kernel
+all_cpus:
 	mtspr	SPRN_SRR0, r8
 	sync
 	rfi
@@ -824,6 +838,14 @@ relocate_new_kernel:
 	/* from this point address translation is turned off */
 	/* and interrupts are disabled */
 
+#ifdef CONFIG_SMP
+	/* if not CPU0, jump to spin */
+	mfspr	r6, SPRN_PIR
+	mtlr	r5
+	cmpwi	r6, 0
+	bnelr
+#endif
+
 	/* set a new stack at the bottom of our page... */
 	/* (not really needed now) */
 	addi	r1, r4, KEXEC_CONTROL_PAGE_SIZE - 8 /* for LR Save+Back Chain */
@@ -870,7 +892,7 @@ relocate_new_kernel:
 	subi    r8, r8, 4
 9:
 	lwzu    r0, 4(r9)  /* do the copy */
-	xor	r6, r6, r0
+	xor	r6, r6, r0	/* calculate checksum (unused ?) */
 	stwu    r0, 4(r8)
 	dcbst	0, r8
 	sync
@@ -889,12 +911,54 @@ relocate_new_kernel:
 	isync
 	sync
 
-	/* jump to the entry point, usually the setup routine */
+#ifdef CONFIG_SMP
+	/* save the spin address in a C callee-saved register that is not
+	 * trampled by purgatory (purgatory invokes some C code): at this
+	 * point, r4 is holding the physical address of the control code
+	 * page we're currently executing in */
+	addi	r25, r4, relocate_new_kernel_spin_addr - relocate_new_kernel
+#endif
+
+	/* jump to the entry point, usually the purgatory */
 	mtlr	r5
 	blrl
 
 1:	b	1b
 
+#ifdef CONFIG_SMP
+	.globl relocate_new_kernel_spin_addr
+relocate_new_kernel_spin_addr:
+	.long 0
+
+	.globl relocate_new_kernel_ready
+relocate_new_kernel_ready:
+	.long 0
+
+	.globl relocate_new_kernel_secondary_spin
+relocate_new_kernel_secondary_spin:
+	/* r3 contains the spin address */
+	/* r4 contains the ready address */
+
+	/* signal CPU0 that we've left the kernel */
+	lis	r5, 1
+	stw	r5, 0(r4)
+	sync
+
+	/* spin waiting for a non-zero address to branch to */
+1:	sync	/* relax */
+	lwz	r5, 0(r3)
+	cmpwi	r5, 0
+	beq	1b
+	/* end of spin loop */
+
+	isync
+	sync
+	mtlr	r5
+	blrl
+	/* not reached, CPU is now on hold in the new kernel */
+
+#endif	/* CONFIG_SMP */
+
 relocate_new_kernel_end:
 
 	.globl relocate_new_kernel_size
-- 
1.6.5.2

