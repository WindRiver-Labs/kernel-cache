From 7718175ee9aa07b79d4c67cab46151721bb8bdb9 Mon Sep 17 00:00:00 2001
From: Raistlin <raistlin@linux.it>
Date: Thu, 27 May 2010 19:22:19 -0700
Subject: [PATCH 10/15] sched: add reclaiming logic to -deadline tasks.

taked from:
git://gitorious.org/sched_deadline/linux-deadline.git sched-dl
commit f0a8c0d03316137f62067fd6a51a96e7190eeeec

The bandwidth enforcing mechanism implemented inside the
SCHED_DEADLINE policy ensures that overrunning tasks are slowed
down without interfering with well behaving ones.
This, however, comes at the price of limiting the capability of
a task to exploit more bandwidth than it is asigned.

The current implementation always stops a task that is trying
to use more than its runtime (every deadline). Something else that
could be done is to let it continue running, but with a "decreased
priority". This way, we can exploit full CPU bandwidth and still
avoid interferences.

In order of "decreasing the priority" of a deadline task, we can:
 - let it stay SCHED_DEADLINE and postpone its deadline. This way it
   will always be scheduled before -rt and -other tasks but it
   won't affect other -deadline tasks;
 - put it in SCHED_FIFO with some priority. This way it will always
   be scheduled before -other tasks but it won't affect -deadline
   tasks, nor other -rt tasks with higher priority;
 - put it in SCHED_OTHER.

Notice also that this can be done on a per-task basis, e.g., each
task can specify what kind of reclaiming mechanism it wants to use
by means of the sched_flags field of sched_param_ex.

Therefore, this patch:
 - adds the flags for specyfing DEADLINE, RT or OTHER reclaiming
   behaviour;
 - adds the logic that changes the scheduling class of a task when
   it overruns, according to the requested policy.

Integrated-by: Liming Wang <liming.wang@windriver>
---
 include/linux/sched.h |   16 ++++++++++
 kernel/sched.c        |   79 ++++++++++++++++++++++++++++++++++++++++++++++--
 kernel/sched_dl.c     |   28 +++++++++++++++--
 3 files changed, 115 insertions(+), 8 deletions(-)

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 18c8a6c..eae9eb3 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -160,9 +160,25 @@ struct sched_param_ex {
  *                      a runtime overrun occurs;
  *  @SCHED_SIG_DMISS	tells us the task wants to be notified whenever
  *                      a scheduling deadline is missed.
+ *  @SCHED_BWRECL_DL	tells us that the task doesn't stop when exhausting
+ *                      its runtime, and it remains a -deadline task, even
+ *                      though its deadline is postponed. This means it
+ *                      won't affect the scheduling of the other -deadline
+ *                      tasks, but if it is a CPU-hog, lower scheduling
+ *                      classes will starve!
+ *  @SCHED_BWRECL_RT	tells us that the task doesn't stop when exhausting
+ *                      its runtime, and it becomes a -rt task, with the
+ *                      priority specified in the sched_priority field of
+ *                      struct shced_param_ex.
+ *  @SCHED_BWRECL_OTH	tells us that the task doesn't stop when exhausting
+ *                      its runtime, and it becomes a normal task, with
+ *                      default priority.
  */
 #define SCHED_SIG_RORUN		0x0001
 #define SCHED_SIG_DMISS		0x0002
+#define SCHED_BWRECL_DL		0x0004
+#define SCHED_BWRECL_RT		0x0008
+#define SCHED_BWRECL_OTH	0x0010
 
 struct exec_domain;
 struct futex_pi_state;
diff --git a/kernel/sched.c b/kernel/sched.c
index 770c380..e3bd864 100644
--- a/kernel/sched.c
+++ b/kernel/sched.c
@@ -132,6 +132,13 @@ static inline int task_has_rt_policy(struct task_struct *p)
 	return rt_policy(p->policy);
 }
 
+static const struct sched_class dl_sched_class;
+
+static inline int dl_class(const struct sched_class *class)
+{
+	return class == &dl_sched_class;
+}
+
 static inline int dl_policy(int policy)
 {
 	if (unlikely(policy == SCHED_DEADLINE))
@@ -1852,8 +1859,6 @@ static inline void __set_task_cpu(struct task_struct *p, unsigned int cpu)
 #endif
 }
 
-static const struct sched_class dl_sched_class;
-
 #define sched_class_highest (&dl_sched_class)
 #define for_each_class(class) \
    for (class = sched_class_highest; class; class = class->next)
@@ -1872,7 +1877,7 @@ static void dec_nr_running(struct rq *rq)
 
 static void set_load_weight(struct task_struct *p)
 {
-	if (task_has_dl_policy(p) || task_has_rt_policy(p)) {
+	if (dl_class(p->sched_class) || task_has_rt_policy(p)) {
 		p->se.load.weight = prio_to_weight[0] * 2;
 		p->se.load.inv_weight = prio_to_wmult[0] >> 1;
 		return;
@@ -2814,6 +2819,16 @@ prepare_task_switch(struct rq *rq, struct task_struct *prev,
 	prepare_arch_switch(next);
 }
 
+static inline const struct sched_class *__policy_class(int policy)
+{
+	if (dl_policy(policy))
+		return &dl_sched_class;
+	else if (rt_policy(policy))
+		return &rt_sched_class;
+	else
+		return &fair_sched_class;
+}
+
 /**
  * finish_task_switch - clean up after a task-switch
  * @rq: runqueue associated with task-switch
@@ -2863,6 +2878,10 @@ static void finish_task_switch(struct rq *rq, struct task_struct *prev)
 	if (mm)
 		mmdrop(mm);
 	if (unlikely(prev_state == TASK_DEAD)) {
+		const struct sched_class *orig_class = __policy_class(prev->policy);
+
+		if (prev->sched_class != orig_class && orig_class->task_dead)
+			orig_class->task_dead(prev);
 		if (prev->sched_class->task_dead)
 			prev->sched_class->task_dead(prev);
 		/*
@@ -4292,6 +4311,45 @@ long __sched sleep_on_timeout(wait_queue_head_t *q, long timeout)
 }
 EXPORT_SYMBOL(sleep_on_timeout);
 
+static void __change_class_task(struct rq *rq, struct task_struct *p,
+				const struct sched_class *new_class)
+{
+	int running = task_current(rq, p);
+	int oldprio = p->prio, on_rq = p->se.on_rq;
+	const struct sched_class *prev_class = p->sched_class;
+
+	update_rq_clock(rq);
+
+	if (on_rq)
+		dequeue_task(rq, p, 0);
+	if (running)
+		p->sched_class->put_prev_task(rq, p);
+
+	if (new_class == &dl_sched_class)
+		p->normal_prio = -(MAX_SCHED_DEADLINE - p->dl.dl_deadline);
+	else if (new_class == &rt_sched_class)
+		p->normal_prio = MAX_RT_PRIO-1 - p->rt_priority;
+	else
+		p->normal_prio = __normal_prio(p);
+
+	p->prio = rt_mutex_getprio(p);
+	if (dl_prio(p->prio))
+		p->sched_class = &dl_sched_class;
+	else if (rt_prio(p->prio))
+		p->sched_class = &rt_sched_class;
+	else
+		p->sched_class = &fair_sched_class;
+	set_load_weight(p);
+
+	if (running)
+		p->sched_class->set_curr_task(rq);
+	if (on_rq) {
+		enqueue_task(rq, p, 0);
+
+		check_class_changed(rq, p, prev_class, oldprio, running);
+	}
+}
+
 #ifdef CONFIG_RT_MUTEXES
 
 /*
@@ -4638,19 +4696,32 @@ recheck:
 	 */
 	if (user && !capable(CAP_SYS_NICE)) {
 		if (dl_policy(policy)) {
-			u64 rlim_dline, rlim_rtime;
+			u64 rlim_dline, rlim_rtime, rlim_rtprio;
 			u64 dline, rtime;
 
 			if (!lock_task_sighand(p, &flags))
 				return -ESRCH;
 			rlim_dline = p->signal->rlim[RLIMIT_DLDLINE].rlim_cur;
 			rlim_rtime = p->signal->rlim[RLIMIT_DLRTIME].rlim_cur;
+			rlim_rtprio = p->signal->rlim[RLIMIT_RTPRIO].rlim_cur;
 			unlock_task_sighand(p, &flags);
 
 			/* can't set/change -deadline policy */
 			if (policy != p->policy && !rlim_rtime)
 				return -EPERM;
 
+			/* can't set/change reclaiming policy to -deadline */
+			if ((param_ex->sched_flags & SCHED_BWRECL_DL) !=
+			    (p->dl.flags & SCHED_BWRECL_DL))
+				return -EPERM;
+
+			/* can't set/increase -rt reclaiming priority */
+			if (param_ex->sched_flags & SCHED_BWRECL_RT &&
+			    (param_ex->sched_priority <= 0 ||
+			     (param_ex->sched_priority > p->rt_priority &&
+			      param_ex->sched_priority > rlim_rtprio)))
+				return -EPERM;
+
 			/* can't decrease the deadline */
 			rlim_dline *= NSEC_PER_USEC;
 			dline = timespec_to_ns(&param_ex->sched_deadline);
diff --git a/kernel/sched_dl.c b/kernel/sched_dl.c
index 4c7516a..2e703ca 100644
--- a/kernel/sched_dl.c
+++ b/kernel/sched_dl.c
@@ -76,6 +76,9 @@ static void enqueue_dl_entity(struct sched_dl_entity *dl_se);
 static void dequeue_dl_entity(struct sched_dl_entity *dl_se);
 static void check_dl_preempt_curr(struct task_struct *p, struct rq *rq);
 
+static void __change_class_task(struct rq *rq, struct task_struct *p,
+				const struct sched_class *new_class);
+
 /*
  * We are being explicitly informed that a new task instance is starting,
  * and this means that:
@@ -230,8 +233,15 @@ static int start_dl_timer(struct sched_dl_entity *dl_se, u64 wakeup)
 	 * is done at this time, in terms of smaller runtime replenishment
 	 * (and perhaps farther deadline postponement), but subsequent
 	 * instances _won't_ be affected.
+	 *
+	 * We also make the task continue --right after replenishing its
+	 * runtime and postponing its deadline-- if flags says it wants to
+	 * reclaim the unused system bandwidth while staying SCHED_DEADLINE.
+	 * Thanks to the deadline postponement, this won't affect the other
+	 * deadline tasks, but if the task is a CPU-hog, lower scheduling
+	 * classes will starve!
 	 */
-	if (dl_se_boosted(dl_se))
+	if (dl_se_boosted(dl_se) || dl_se->flags & SCHED_BWRECL_DL)
 		return 0;
 
 	/*
@@ -290,9 +300,12 @@ static enum hrtimer_restart dl_timer(struct hrtimer *timer)
 	 * task might have changed its scheduling policy to something
 	 * different from SCHED_DEADLINE (through sched_setscheduler()).
 	 */
-	if (!dl_task(p))
+	if (!dl_policy(p->policy))
 		goto unlock;
 
+	if (p->sched_class != &dl_sched_class)
+		__change_class_task(rq, p, &dl_sched_class);
+
 	dl_se->flags &= ~DL_THROTTLED;
 	if (p->se.on_rq) {
 		replenish_dl_entity(dl_se);
@@ -366,8 +379,15 @@ int dl_runtime_exceeded(struct rq *rq, struct sched_dl_entity *dl_se)
 			dl_se->runtime = 0;
 		replenish_dl_entity(dl_se);
 		enqueue_dl_entity(dl_se);
-	} else
+	} else {
 		dl_se->flags |= DL_THROTTLED;
+		if (dl_se->flags & SCHED_BWRECL_OTH)
+			__change_class_task(rq, dl_task_of(dl_se),
+					    &fair_sched_class);
+		else if (dl_se->flags & SCHED_BWRECL_RT)
+			__change_class_task(rq, dl_task_of(dl_se),
+					    &rt_sched_class);
+	}
 
 	return 1;
 }
@@ -689,7 +709,7 @@ static void set_curr_task_dl(struct rq *rq)
 static void switched_from_dl(struct rq *rq, struct task_struct *p,
 			     int running)
 {
-	if (hrtimer_active(&p->dl.dl_timer))
+	if (hrtimer_active(&p->dl.dl_timer) && !dl_policy(p->policy))
 		hrtimer_try_to_cancel(&p->dl.dl_timer);
 }
 
-- 
1.6.5.2

