From 7e2d892df662db88f185264c8fe5558ed4190064 Mon Sep 17 00:00:00 2001
From: Zumeng Chen <zumeng.chen@windriver.com>
Date: Thu, 20 Oct 2016 09:00:04 +0800
Subject: [PATCH 0857/1566] apf: Add kernel modules to the kernel

This patch comes from:
  https://github.com/Xilinx/linux-xlnx.git

This patch is to integrate the commit 3eae3e94

This patch is created from origin Xidane kernel modules
with these additional steps:
- Fix checkpatch.pl warnings
- Fix coding style
- Move all modules to staging/apf folder
- dos2unix translation
- Fix compilation warnings
- Change files permission

Signed-off-by: S Mohan <s.mohan@xilinx.com>
Signed-off-by: Michal Simek <michal.simek@xilinx.com>
Signed-off-by: Zumeng Chen <zumeng.chen@windriver.com>
---
 drivers/staging/Kconfig                      |    2 +
 drivers/staging/Makefile                     |    1 +
 drivers/staging/apf/Kconfig                  |   17 +
 drivers/staging/apf/Makefile                 |    8 +
 drivers/staging/apf/xilinx-dma-apf.c         | 1276 +++++++++++++++++++++++++
 drivers/staging/apf/xilinx-dma-apf.h         |  242 +++++
 drivers/staging/apf/xlnk-eng.c               |  278 ++++++
 drivers/staging/apf/xlnk-eng.h               |   33 +
 drivers/staging/apf/xlnk-event-tracer-type.h |   38 +
 drivers/staging/apf/xlnk-ioctl.h             |   39 +
 drivers/staging/apf/xlnk.c                   | 1311 ++++++++++++++++++++++++++
 drivers/staging/apf/xlnk.h                   |  117 +++
 12 files changed, 3362 insertions(+), 0 deletions(-)
 create mode 100644 drivers/staging/apf/Kconfig
 create mode 100644 drivers/staging/apf/Makefile
 create mode 100644 drivers/staging/apf/xilinx-dma-apf.c
 create mode 100644 drivers/staging/apf/xilinx-dma-apf.h
 create mode 100644 drivers/staging/apf/xlnk-eng.c
 create mode 100644 drivers/staging/apf/xlnk-eng.h
 create mode 100644 drivers/staging/apf/xlnk-event-tracer-type.h
 create mode 100644 drivers/staging/apf/xlnk-ioctl.h
 create mode 100644 drivers/staging/apf/xlnk.c
 create mode 100644 drivers/staging/apf/xlnk.h

diff --git a/drivers/staging/Kconfig b/drivers/staging/Kconfig
index 8fbef47..bd141f9 100644
--- a/drivers/staging/Kconfig
+++ b/drivers/staging/Kconfig
@@ -108,4 +108,6 @@ source "drivers/staging/lttng/Kconfig"
 
 source "drivers/staging/netatop/Kconfig"
 
+source "drivers/staging/apf/Kconfig"
+
 endif # STAGING
diff --git a/drivers/staging/Makefile b/drivers/staging/Makefile
index 9885c1b..c7aa435 100644
--- a/drivers/staging/Makefile
+++ b/drivers/staging/Makefile
@@ -43,3 +43,4 @@ obj-$(CONFIG_ISDN_I4L)		+= i4l/
 obj-$(CONFIG_KS7010)		+= ks7010/
 obj-$(CONFIG_LTTNG)			+= lttng/
 obj-$(CONFIG_NETATOP)		+= netatop/
+obj-$(CONFIG_XILINX_APF)	+= apf/
diff --git a/drivers/staging/apf/Kconfig b/drivers/staging/apf/Kconfig
new file mode 100644
index 0000000..1881a0a
--- /dev/null
+++ b/drivers/staging/apf/Kconfig
@@ -0,0 +1,17 @@
+#
+# APF driver configuration
+#
+
+menuconfig XILINX_APF
+	tristate "Xilinx APF Accelerator driver"
+	default n
+	help
+	  Select if you want to include APF accelerator driver
+
+if XILINX_APF
+config XILINX_DMA_APF
+        bool "Xilinx APF DMA engines support"
+        select DMA_ENGINE
+        ---help---
+        Enable support for the Xilinx APF DMA controllers.
+endif # XILINX_APF
diff --git a/drivers/staging/apf/Makefile b/drivers/staging/apf/Makefile
new file mode 100644
index 0000000..917098e
--- /dev/null
+++ b/drivers/staging/apf/Makefile
@@ -0,0 +1,8 @@
+# gpio support: dedicated expander chips, etc
+
+ccflags-$(CONFIG_DEBUG_XILINX_APF) += -DDEBUG
+ccflags-$(CONFIG_XILINX_APF) += -Idrivers/dma
+
+obj-$(CONFIG_XILINX_APF) += xlnk.o
+obj-$(CONFIG_XILINX_APF) += xlnk-eng.o
+obj-$(CONFIG_XILINX_DMA_APF) += xilinx-dma-apf.o
diff --git a/drivers/staging/apf/xilinx-dma-apf.c b/drivers/staging/apf/xilinx-dma-apf.c
new file mode 100644
index 0000000..49f4e2c
--- /dev/null
+++ b/drivers/staging/apf/xilinx-dma-apf.c
@@ -0,0 +1,1276 @@
+/*
+ * Xilinx AXI DMA Engine support
+ *
+ * Copyright (C) 2010 Xilinx, Inc. All rights reserved.
+ *
+ * Description:
+ * This driver supports Xilinx AXI DMA engine:
+ *  . Axi DMA engine, it does transfers between memory and device. It can be
+ *    configured to have one channel or two channels. If configured as two
+ *    channels, one is for transmit to device and another is for receive from
+ *    device.
+ *
+ * This is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+/* #include <linux/sysdev.h> */
+#include <linux/interrupt.h>
+#include <linux/dmapool.h>
+#include <linux/slab.h>
+#include <linux/dma-mapping.h>
+#include <linux/dma-attrs.h>
+#include <linux/pagemap.h>
+#include <linux/device.h>
+#include <linux/types.h>
+#include <linux/pm.h>
+#include <linux/fs.h>
+#include <linux/gfp.h>
+#include <linux/string.h>
+#include <linux/uaccess.h>
+#include <asm/cacheflush.h>
+#include <linux/sched.h>
+
+#include "xilinx-dma-apf.h"
+
+#include "xlnk-event-tracer-type.h"
+#include "xlnk.h"
+
+static DEFINE_MUTEX(dma_list_mutex);
+static LIST_HEAD(dma_device_list);
+
+static int unpin_user_pages(struct scatterlist *sglist, unsigned int cnt);
+/* Driver functions */
+static void xdma_clean_bd(struct xdma_desc_hw *bd)
+{
+	bd->src_addr = 0x0;
+	bd->control = 0x0;
+	bd->status = 0x0;
+	bd->app[0] = 0x0;
+	bd->app[1] = 0x0;
+	bd->app[2] = 0x0;
+	bd->app[3] = 0x0;
+	bd->app[4] = 0x0;
+	bd->dmahead = 0x0;
+	bd->sw_flag = 0x0;
+}
+
+static int dma_is_running(struct xdma_chan *chan)
+{
+	return !(chan->regs->sr & XDMA_SR_HALTED_MASK) &&
+		(chan->regs->cr & XDMA_CR_RUNSTOP_MASK);
+}
+
+static int dma_is_idle(struct xdma_chan *chan)
+{
+	return chan->regs->sr & XDMA_SR_IDLE_MASK;
+}
+
+static void dma_halt(struct xdma_chan *chan)
+{
+	chan->regs->cr &= ~XDMA_CR_RUNSTOP_MASK;
+}
+
+static void dma_start(struct xdma_chan *chan)
+{
+	chan->regs->cr |= XDMA_CR_RUNSTOP_MASK;
+}
+
+static int dma_init(struct xdma_chan *chan)
+{
+	int loop = XDMA_RESET_LOOP;
+
+	chan->regs->cr |= XDMA_CR_RESET_MASK;
+
+	/* Wait for the hardware to finish reset
+	 */
+	while (loop) {
+		if (!(chan->regs->cr & XDMA_CR_RESET_MASK))
+			break;
+
+		loop -= 1;
+	}
+
+	if (!loop)
+		return 1;
+
+	return 0;
+}
+
+static int xdma_alloc_chan_descriptors(struct xdma_chan *chan)
+{
+	int i;
+	u8 *ptr;
+
+	/*
+	 * We need the descriptor to be aligned to 64bytes
+	 * for meeting Xilinx DMA specification requirement.
+	 */
+	ptr = (u8 *)dma_alloc_coherent(chan->dev,
+				(sizeof(struct xdma_desc_hw) * XDMA_MAX_BD_CNT),
+				&chan->bd_phys_addr,
+				GFP_KERNEL);
+
+	if (!ptr) {
+		dev_err(chan->dev,
+			"unable to allocate channel %d descriptor pool\n",
+			chan->id);
+		return -ENOMEM;
+	}
+
+	memset(ptr, 0, (sizeof(struct xdma_desc_hw) * XDMA_MAX_BD_CNT));
+	chan->bd_cur = 0;
+	chan->bd_tail = 0;
+	chan->bd_used = 0;
+	chan->bd_chain_size = sizeof(struct xdma_desc_hw) * XDMA_MAX_BD_CNT;
+
+	/*
+	 * Pre allocate all the channels.
+	 */
+	for (i = 0; i < XDMA_MAX_BD_CNT; i++) {
+		chan->bds[i] = (struct xdma_desc_hw *)
+				(ptr + (sizeof(struct xdma_desc_hw) * i));
+		chan->bds[i]->next_desc = chan->bd_phys_addr +
+					(sizeof(struct xdma_desc_hw) *
+						((i + 1) % XDMA_MAX_BD_CNT));
+	}
+
+	/* there is at least one descriptor free to be allocated */
+	return 0;
+}
+
+static void xdma_free_chan_resources(struct xdma_chan *chan)
+{
+	dev_dbg(chan->dev, "Free all channel resources.\n");
+	dma_free_coherent(chan->dev, (sizeof(struct xdma_desc_hw) *
+			XDMA_MAX_BD_CNT), chan->bds[0], chan->bd_phys_addr);
+}
+
+static void xilinx_chan_desc_reinit(struct xdma_chan *chan)
+{
+	struct xdma_desc_hw *desc;
+	unsigned int start, end;
+	unsigned long flags;
+
+	spin_lock_irqsave(&chan->lock, flags);
+	start = 0;
+	end = XDMA_MAX_BD_CNT;
+
+	while (start < end) {
+		desc = chan->bds[start];
+		xdma_clean_bd(desc);
+		start++;
+	}
+	/* Re-initialize bd_cur and bd_tail values */
+	chan->bd_cur = 0;
+	chan->bd_tail = 0;
+	chan->bd_used = 0;
+	spin_unlock_irqrestore(&chan->lock, flags);
+}
+
+static void xilinx_chan_desc_cleanup(struct xdma_chan *chan)
+{
+	struct xdma_head *dmahead;
+	struct xdma_desc_hw *desc;
+	struct completion *cmp;
+	unsigned long flags;
+
+	spin_lock_irqsave(&chan->lock, flags);
+#define XDMA_BD_STS_RXEOF_MASK 0x04000000
+	desc = chan->bds[chan->bd_cur];
+	pr_debug("cleanup desc: %x\n", (u32)desc);
+	while ((desc->status & XDMA_BD_STS_ALL_MASK)) {
+		pr_debug("desc->status %x desc->dmahead %x\n",
+			desc->status, desc->dmahead);
+		if ((desc->status & XDMA_BD_STS_RXEOF_MASK) &&
+		    !(desc->dmahead)) {
+			pr_info("ERROR: premature EOF on DMA\n");
+			dma_init(chan); /* reset the dma HW */
+			while (!(desc->dmahead)) {
+				xdma_clean_bd(desc);
+				chan->bd_used--;
+				chan->bd_cur++;
+				if (chan->bd_cur >= XDMA_MAX_BD_CNT)
+					chan->bd_cur = 0;
+				desc = chan->bds[chan->bd_cur];
+			}
+		}
+		if (desc->dmahead) {
+
+			if ((desc->sw_flag & XDMA_BD_SF_POLL_MODE_MASK))
+				if (!(desc->sw_flag & XDMA_BD_SF_SW_DONE_MASK))
+					break;
+
+			dmahead = (struct xdma_head *)desc->dmahead;
+			cmp = (struct completion *)&dmahead->cmp;
+			if (dmahead->nappwords_o)
+				memcpy(dmahead->appwords_o, desc->app,
+					dmahead->nappwords_o * sizeof(u32));
+
+			if (chan->poll_mode)
+				cmp->done = 1;
+			else
+				complete(cmp);
+		}
+		xdma_clean_bd(desc);
+		chan->bd_used--;
+		chan->bd_cur++;
+		if (chan->bd_cur >= XDMA_MAX_BD_CNT)
+			chan->bd_cur = 0;
+		desc = chan->bds[chan->bd_cur];
+	}
+	spin_unlock_irqrestore(&chan->lock, flags);
+}
+
+static void xdma_err_tasklet(unsigned long data)
+{
+	struct xdma_chan *chan = (struct xdma_chan *)data;
+
+	if (chan->err) {
+		/* If reset failed, need to hard reset
+		 * Channel is no longer functional
+		 */
+		if (!dma_init(chan))
+			chan->err = 0;
+		else
+			dev_err(chan->dev,
+			    "DMA channel reset failed, please reset system\n");
+	}
+
+	rmb();
+	xilinx_chan_desc_cleanup(chan);
+
+	xilinx_chan_desc_reinit(chan);
+}
+
+static void xdma_tasklet(unsigned long data)
+{
+	struct xdma_chan *chan = (struct xdma_chan *)data;
+
+	rmb();
+	xilinx_chan_desc_cleanup(chan);
+}
+
+#if 0
+static void xdma_tasklet_tx(unsigned long data)
+{
+	/* xlnk_record_event(XLNK_ET_KERNEL_DMA_MM2S_HALF_BOTTOM_START); */
+	xdma_tasklet(data);
+	/* xlnk_record_event(XLNK_ET_KERNEL_DMA_MM2S_HALF_BOTTOM_END); */
+}
+
+static void xdma_tasklet_rx(unsigned long data)
+{
+	/* xlnk_record_event(XLNK_ET_KERNEL_DMA_S2MM_HALF_BOTTOM_START); */
+	xdma_tasklet(data);
+	/* xlnk_record_event(XLNK_ET_KERNEL_DMA_S2MM_HALF_BOTTOM_END); */
+}
+#endif
+
+static void dump_cur_bd(struct xdma_chan *chan)
+{
+	u32 index;
+
+	index = (((u32)chan->regs->cdr) - chan->bd_phys_addr) /
+			sizeof(struct xdma_desc_hw);
+
+	dev_err(chan->dev, "cur bd @ %08x\n",   (u32)chan->regs->cdr);
+	dev_err(chan->dev, "  buf  = 0x%08x\n", chan->bds[index]->src_addr);
+	dev_err(chan->dev, "  ctrl = 0x%08x\n", chan->bds[index]->control);
+	dev_err(chan->dev, "  sts  = 0x%08x\n", chan->bds[index]->status);
+	dev_err(chan->dev, "  next = 0x%08x\n", chan->bds[index]->next_desc);
+}
+
+static irqreturn_t xdma_rx_intr_handler(int irq, void *data)
+{
+	struct xdma_chan *chan = data;
+	u32 stat;
+
+	/* xlnk_record_event(XLNK_ET_KERNEL_DMA_S2MM_INTR); */
+	stat = chan->regs->sr;
+
+	if (!(stat & XDMA_XR_IRQ_ALL_MASK)) {
+		/* dev_err(chan->dev, "no rx irq\n"); */
+		return IRQ_NONE;
+	}
+
+	/* Ack the interrupts */
+	chan->regs->sr = stat & XDMA_XR_IRQ_ALL_MASK;
+
+	if (stat & XDMA_XR_IRQ_ERROR_MASK) {
+		dev_err(chan->dev, "Channel %s has errors %x, cdr %x tdr %x\n",
+			chan->name, (unsigned int)stat,
+			(unsigned int)chan->regs->cdr,
+			(unsigned int)chan->regs->tdr);
+
+		dump_cur_bd(chan);
+
+		chan->err = 1;
+		tasklet_schedule(&chan->dma_err_tasklet);
+	}
+
+	if (!(chan->poll_mode) && ((stat & XDMA_XR_IRQ_DELAY_MASK) ||
+			(stat & XDMA_XR_IRQ_IOC_MASK)))
+		tasklet_schedule(&chan->tasklet);
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t xdma_tx_intr_handler(int irq, void *data)
+{
+	struct xdma_chan *chan = data;
+	u32 stat;
+
+	/* xlnk_record_event(XLNK_ET_KERNEL_DMA_MM2S_INTR); */
+	stat = chan->regs->sr;
+
+	if (!(stat & XDMA_XR_IRQ_ALL_MASK)) {
+		/* dev_err(chan->dev, "no tx irq\n"); */
+		return IRQ_NONE;
+	}
+
+	/* Ack the interrupts */
+	chan->regs->sr = stat & XDMA_XR_IRQ_ALL_MASK;
+
+	if (stat & XDMA_XR_IRQ_ERROR_MASK) {
+		dev_err(chan->dev, "Channel %s has errors %x, cdr %x tdr %x\n",
+			chan->name, (unsigned int)stat,
+			(unsigned int)chan->regs->cdr,
+			(unsigned int)chan->regs->tdr);
+
+		dump_cur_bd(chan);
+
+		chan->err = 1;
+		tasklet_schedule(&chan->dma_err_tasklet);
+	}
+
+	if (!(chan->poll_mode) && ((stat & XDMA_XR_IRQ_DELAY_MASK) ||
+			(stat & XDMA_XR_IRQ_IOC_MASK)))
+		tasklet_schedule(&chan->tasklet);
+
+	return IRQ_HANDLED;
+}
+
+static void xdma_chan_remove(struct xdma_chan *chan)
+{
+	dma_halt(chan);
+	xdma_free_chan_resources(chan);
+	free_irq(chan->irq, chan);
+	kfree(chan);
+}
+
+static void xdma_start_transfer(struct xdma_chan *chan,
+				int start_index,
+				int end_index)
+{
+	dma_addr_t cur_phys;
+	dma_addr_t tail_phys;
+
+	if (chan->err)
+		return;
+
+	cur_phys = chan->bd_phys_addr + (start_index *
+					sizeof(struct xdma_desc_hw));
+	tail_phys = chan->bd_phys_addr + (end_index *
+					sizeof(struct xdma_desc_hw));
+	/* If hardware is busy, move the tail & return */
+	if (dma_is_running(chan) || dma_is_idle(chan)) {
+		/* Update tail ptr register and start the transfer */
+		chan->regs->tdr = tail_phys;
+		xlnk_record_event(XLNK_ET_KERNEL_AFTER_DMA_KICKOFF);
+		return;
+	}
+
+	chan->regs->cdr = cur_phys;
+
+	dma_start(chan);
+
+	/* Enable interrupts */
+	chan->regs->cr |=
+		chan->poll_mode ? XDMA_XR_IRQ_ERROR_MASK : XDMA_XR_IRQ_ALL_MASK;
+
+	/* Update tail ptr register and start the transfer */
+	chan->regs->tdr = tail_phys;
+	xlnk_record_event(XLNK_ET_KERNEL_AFTER_DMA_KICKOFF);
+}
+
+static int xdma_setup_hw_desc(struct xdma_chan *chan,
+				struct xdma_head *dmahead,
+				struct scatterlist *sgl,
+				unsigned int sg_len,
+				enum dma_data_direction direction,
+				unsigned int nappwords_i,
+				u32 *appwords_i)
+{
+	struct xdma_desc_hw *bd = NULL;
+	size_t copy;
+	struct scatterlist *sg;
+	size_t sg_used;
+	dma_addr_t dma_src;
+	int i, start_index = -1, end_index1 = 0, end_index2 = -1;
+	int status;
+	unsigned long flags;
+	unsigned int bd_used_saved;
+
+	if (!chan)
+		return -ENODEV;
+
+	/* if we almost run out of bd, try to recycle some */
+	if ((chan->poll_mode) && (chan->bd_used >= XDMA_BD_CLEANUP_THRESHOLD))
+		xilinx_chan_desc_cleanup(chan);
+
+	spin_lock_irqsave(&chan->lock, flags);
+
+	bd_used_saved = chan->bd_used;
+	/*
+	 * Build transactions using information in the scatter gather list
+	 */
+	for_each_sg(sgl, sg, sg_len, i) {
+		sg_used = 0;
+
+		/* Loop until the entire scatterlist entry is used */
+		while (sg_used < sg_dma_len(sg)) {
+			/* Allocate the link descriptor from DMA pool */
+			bd = chan->bds[chan->bd_tail];
+			if ((bd->control) & (XDMA_BD_STS_ACTUAL_LEN_MASK)) {
+				end_index2 = chan->bd_tail;
+				status = -ENOMEM;
+				/* If first was not set, then we failed to
+				 * allocate the very first descriptor,
+				 * and we're done */
+				if (start_index == -1)
+					goto out_unlock;
+				else
+					goto out_clean;
+			}
+			/*
+			 * Calculate the maximum number of bytes to transfer,
+			 * making sure it is less than the DMA controller limit
+			 */
+			copy = min((size_t)(sg_dma_len(sg) - sg_used),
+				   (size_t)chan->max_len);
+			/*
+			 * Only the src address for DMA
+			 */
+			dma_src = sg_dma_address(sg) + sg_used;
+			bd->src_addr = dma_src;
+
+			/* Fill in the descriptor */
+			bd->control = copy;
+
+			/*
+			 * If this is not the first descriptor, chain the
+			 * current descriptor after the previous descriptor
+			 *
+			 * For the first DMA_TO_DEVICE transfer, set SOP
+			 */
+			if (start_index == -1) {
+				start_index = chan->bd_tail;
+
+				if (nappwords_i)
+					memcpy(bd->app, appwords_i,
+						nappwords_i * sizeof(u32));
+
+				if (direction == DMA_TO_DEVICE)
+					bd->control |= XDMA_BD_SOP;
+			}
+
+			sg_used += copy;
+			end_index2 = chan->bd_tail;
+			chan->bd_tail++;
+			chan->bd_used++;
+			if (chan->bd_tail >= XDMA_MAX_BD_CNT) {
+				end_index1 = XDMA_MAX_BD_CNT;
+				chan->bd_tail = 0;
+			}
+		}
+	}
+
+	if (start_index == -1) {
+		status = -EINVAL;
+		goto out_unlock;
+	}
+
+	bd->dmahead = (u32) dmahead;
+	bd->sw_flag = chan->poll_mode ? XDMA_BD_SF_POLL_MODE_MASK : 0;
+	dmahead->last_bd_index = end_index2;
+
+	if (direction == DMA_TO_DEVICE)
+		bd->control |= XDMA_BD_EOP;
+
+	wmb();
+
+	xdma_start_transfer(chan, start_index, end_index2);
+
+	spin_unlock_irqrestore(&chan->lock, flags);
+
+	return 0;
+
+out_clean:
+	if (!end_index1) {
+		for (i = start_index; i < end_index2; i++)
+			xdma_clean_bd(chan->bds[i]);
+	} else {
+		/* clean till the end of bd list first, and then 2nd end */
+		for (i = start_index; i < end_index1; i++)
+			xdma_clean_bd(chan->bds[i]);
+
+		end_index1 = 0;
+		for (i = end_index1; i < end_index2; i++)
+			xdma_clean_bd(chan->bds[i]);
+	}
+	/* Move the bd_tail back */
+	chan->bd_tail = start_index;
+	chan->bd_used = bd_used_saved;
+
+out_unlock:
+	spin_unlock_irqrestore(&chan->lock, flags);
+
+	return status;
+}
+
+#define XDMA_SGL_MAX_LEN	XDMA_MAX_BD_CNT
+static struct scatterlist sglist_array[XDMA_SGL_MAX_LEN];
+
+/*
+ *  create minimal length scatter gather list for physically contiguous buffer
+ *  that starts at phy_buf and has length phy_buf_len bytes
+ */
+static unsigned int phy_buf_to_sgl(void *phy_buf, unsigned int phy_buf_len,
+			struct scatterlist **sgl)
+{
+	unsigned int sgl_cnt = 0;
+	struct scatterlist *sgl_head;
+	unsigned int dma_len;
+
+	if (!phy_buf || !phy_buf_len) {
+		pr_err("phy_buf is NULL or phy_buf_len = 0\n");
+		return sgl_cnt;
+	}
+
+	*sgl = sglist_array;
+	sgl_head = *sgl;
+
+	while (phy_buf_len > 0) {
+
+		sgl_cnt++;
+		if (sgl_cnt > XDMA_SGL_MAX_LEN)
+			return 0;
+
+		dma_len = (phy_buf_len > XDMA_MAX_TRANS_LEN) ?
+				XDMA_MAX_TRANS_LEN : phy_buf_len;
+
+		sg_dma_address(sgl_head) = (dma_addr_t)phy_buf;
+		sg_dma_len(sgl_head) = dma_len;
+		sgl_head = sg_next(sgl_head);
+
+		phy_buf += dma_len;
+		phy_buf_len -= dma_len;
+
+	}
+	return sgl_cnt;
+}
+
+/*  merge sg list, sgl, with length sgl_len, to sgl_merged, to save dma bds */
+static unsigned int sgl_merge(struct scatterlist *sgl, unsigned int sgl_len,
+			struct scatterlist **sgl_merged)
+{
+	struct scatterlist *sghead, *sgend, *sgnext, *sg_merged_head;
+	unsigned int sg_visited_cnt = 0, sg_merged_num = 0;
+	unsigned int dma_len = 0, i = 0;
+
+	pr_debug("\nsgl_merge: sg list length before merging = %u\n", sgl_len);
+	pr_debug("------------------------------------------------\n");
+
+	*sgl_merged = sglist_array;
+	sg_merged_head = *sgl_merged;
+	sghead = sgl;
+
+	while (sghead && (sg_visited_cnt < sgl_len)) {
+
+		dma_len = sg_dma_len(sghead);
+		sgend = sghead;
+		sg_visited_cnt++;
+		sgnext = sg_next(sgend);
+
+		pr_debug("%05u: dma_addr: 0x%08x, dma_len: 0x%08x\n", ++i,
+			    sg_dma_address(sgend), sg_dma_len(sgend));
+
+		while (sgnext && (sg_visited_cnt < sgl_len)) {
+
+			if ((sg_dma_address(sgend) + sg_dma_len(sgend)) !=
+				sg_dma_address(sgnext))
+				break;
+
+			if (dma_len + sg_dma_len(sgnext) >= XDMA_MAX_TRANS_LEN)
+				break;
+
+			sgend = sgnext;
+			dma_len += sg_dma_len(sgend);
+			sg_visited_cnt++;
+			sgnext = sg_next(sgnext);
+
+			pr_debug("%05u: dma_addr: 0x%08x, dma_len: 0x%08x\n",
+				 ++i, sg_dma_address(sgend), sg_dma_len(sgend));
+		}
+
+		sg_merged_num++;
+		if (sg_merged_num > XDMA_SGL_MAX_LEN)
+			return 0;
+
+		memcpy(sg_merged_head, sghead, sizeof(struct scatterlist));
+
+		sg_dma_len(sg_merged_head) = dma_len;
+
+		pr_debug("after: dma_addr: 0x%08x, dma_len: 0x%08x\n",
+			sg_dma_address(sg_merged_head),
+			sg_dma_len(sg_merged_head));
+		pr_debug("------------------------------------------------\n");
+
+		sg_merged_head = sg_next(sg_merged_head);
+		sghead = sg_next(sgend);
+	}
+
+	pr_debug("sgl_merge: sg list length after merging = %u\n\n",
+			sg_merged_num);
+
+	return sg_merged_num;
+}
+
+static struct page *mapped_pages[XDMA_MAX_BD_CNT];
+static int pin_user_pages(unsigned long uaddr,
+			   unsigned int ulen,
+			   int write,
+			   struct scatterlist **scatterpp,
+			   unsigned int *cntp,
+			   unsigned int user_flags)
+{
+	int status;
+	struct mm_struct *mm = current->mm;
+	struct task_struct *curr_task = current;
+	unsigned int first_page;
+	unsigned int last_page;
+	unsigned int num_pages;
+	struct scatterlist *sglist;
+
+	unsigned int pgidx;
+	unsigned int pglen;
+	unsigned int pgoff;
+	unsigned int sublen;
+
+	first_page = uaddr / PAGE_SIZE;
+	last_page = (uaddr + ulen - 1) / PAGE_SIZE;
+	num_pages = last_page - first_page + 1;
+	pr_debug("%s: %x %x %x\n", __func__, first_page, last_page, num_pages);
+	xlnk_record_event(XLNK_ET_KERNEL_BEFORE_GET_USER_PAGES);
+	down_read(&mm->mmap_sem);
+	/* TODO: it would be safe to alloc mapped_pages	*/
+	status = get_user_pages(curr_task, mm, uaddr, num_pages, write, 1,
+				mapped_pages, NULL);
+#if 0
+	status = xilinx_get_user_pages(curr_task, mm, uaddr, num_pages,
+					  write, 1, mapped_pages, NULL,
+					  user_flags);
+#endif
+	pr_debug("%s: get_user_pages done\n", __func__);
+	up_read(&mm->mmap_sem);
+	xlnk_record_event(XLNK_ET_KERNEL_AFTER_GET_USER_PAGES);
+
+	if (status == num_pages) {
+		pr_debug("got %d user pages for %lx len %x\n",
+			 status, uaddr, ulen);
+
+		sglist = kcalloc(num_pages,
+				 sizeof(struct scatterlist),
+				 GFP_KERNEL);
+		if (sglist == NULL) {
+			pr_err("%s: kcalloc failed to create sg list\n",
+			       __func__);
+			return -ENOMEM;
+		}
+		/* TODO: need to store this sglist into a list
+		   TODO: need to use an object pool to efficiently manage alloc
+		*/
+		sg_init_table(sglist, num_pages);
+		pr_debug("%s: sg_init_table done\n", __func__);
+		sublen = 0;
+		for (pgidx = 0; pgidx < status; pgidx++) {
+			pr_debug("page_address %lx count %d\n",
+			(unsigned long)page_address(mapped_pages[pgidx]),
+			page_count(mapped_pages[pgidx]));
+
+			if (pgidx == 0 && num_pages != 1) {
+				pgoff = uaddr & (~PAGE_MASK);
+				pglen = PAGE_SIZE - pgoff;
+			} else if (pgidx == 0 && num_pages == 1) {
+				pgoff = uaddr & (~PAGE_MASK);
+				pglen = ulen;
+			} else if (pgidx == num_pages - 1) {
+				pgoff = 0;
+				pglen = ulen - sublen;
+			} else {
+				pgoff = 0;
+				pglen = PAGE_SIZE;
+			}
+
+			sublen += pglen;
+
+			pr_debug("%s: pg %d off %x len %d\n", __func__,
+				 pgidx, pgoff, pglen);
+			sg_set_page(&sglist[pgidx],
+				    mapped_pages[pgidx],
+				    pglen, pgoff);
+			pr_debug("%s: after sg_set_page\n", __func__);
+
+			sg_dma_len(&sglist[pgidx]) = pglen;
+			pr_debug("%s: end of loop\n", __func__);
+		}
+
+		*scatterpp = sglist;
+		*cntp = num_pages;
+
+		pr_debug("pin_user_pages returning\n");
+		return 0;
+	} else {
+		pr_debug("get_user_pages return %d less than %d\n",
+			 status, num_pages);
+
+		for (pgidx = 0; pgidx < status; pgidx++) {
+			pr_debug("page_address %lx count %d\n",
+			(unsigned long)page_address(mapped_pages[pgidx]),
+			page_count(mapped_pages[pgidx]));
+
+			page_cache_release(mapped_pages[pgidx]);
+		}
+		return -ENOMEM;
+	}
+}
+
+static int unpin_user_pages(struct scatterlist *sglist, unsigned int cnt)
+{
+	struct page *pg;
+	unsigned int i;
+
+	if (!sglist)
+		return 0;
+
+	for (i = 0; i < cnt; i++) {
+		pg = sg_page(sglist + i);
+		if (pg) {
+			pr_debug("page_cache_release %lx\n",
+				 (long unsigned)pg);
+			page_cache_release(pg);
+		}
+	}
+
+	kfree(sglist);
+	return 0;
+}
+
+struct xdma_chan *xdma_request_channel(char *name)
+{
+	int i;
+	struct xdma_device *device, *tmp;
+
+	mutex_lock(&dma_list_mutex);
+	list_for_each_entry_safe(device, tmp, &dma_device_list, node) {
+		for (i = 0; i < device->channel_count; i++) {
+			if (device->chan[i]->client_count)
+				continue;
+			if (!strcmp(device->chan[i]->name, name)) {
+				device->chan[i]->client_count++;
+				mutex_unlock(&dma_list_mutex);
+				return device->chan[i];
+			}
+		}
+	}
+	mutex_unlock(&dma_list_mutex);
+	return NULL;
+}
+EXPORT_SYMBOL(xdma_request_channel);
+
+void xdma_release_channel(struct xdma_chan *chan)
+{
+	mutex_lock(&dma_list_mutex);
+	if (!chan->client_count) {
+		mutex_unlock(&dma_list_mutex);
+		return;
+	}
+	chan->client_count--;
+	dma_halt(chan);
+	xilinx_chan_desc_reinit(chan);
+	mutex_unlock(&dma_list_mutex);
+}
+EXPORT_SYMBOL(xdma_release_channel);
+
+void xdma_release_all_channels(void)
+{
+	int i;
+	struct xdma_device *device, *tmp;
+
+	list_for_each_entry_safe(device, tmp, &dma_device_list, node) {
+		for (i = 0; i < device->channel_count; i++) {
+			if (device->chan[i]->client_count) {
+				dma_halt(device->chan[i]);
+				xilinx_chan_desc_reinit(device->chan[i]);
+				device->chan[i]->client_count = 0;
+				pr_info("%s: chan %s freed\n",
+						__func__,
+						device->chan[i]->name);
+			}
+		}
+	}
+}
+EXPORT_SYMBOL(xdma_release_all_channels);
+
+int xdma_submit(struct xdma_chan *chan,
+			void *userbuf,
+			unsigned int size,
+			unsigned int nappwords_i,
+			u32 *appwords_i,
+			unsigned int nappwords_o,
+			unsigned int user_flags,
+			struct xdma_head **dmaheadpp)
+{
+	struct xdma_head *dmahead;
+	struct scatterlist *sglist, *sglist_dma;
+	unsigned int sgcnt, sgcnt_dma;
+	enum dma_data_direction dmadir;
+	int status;
+	DEFINE_DMA_ATTRS(attrs);
+
+	pr_debug("%s: chan_name = %s\n", __func__, chan->name);
+
+	xlnk_record_event(XLNK_ET_KERNEL_ENTER_DMA_SUBMIT);
+	dmahead = kmalloc(sizeof(struct xdma_head), GFP_KERNEL);
+	pr_debug("dmahead %x\n", (u32)dmahead);
+	if (!dmahead)
+		return -ENOMEM;
+
+	memset(dmahead, 0, sizeof(struct xdma_head));
+
+	dmahead->chan = chan;
+	dmahead->userbuf = userbuf;
+	dmahead->size = size;
+	dmahead->dmadir = chan->direction;
+	dmahead->userflag = user_flags;
+	dmadir = chan->direction;
+	if (user_flags & CF_FLAG_PHYSICALLY_CONTIGUOUS) {
+		pr_debug("buf phy addr = 0x%08x (len = 0x%08x)\n",
+			 (u32)userbuf, size);
+
+		/*
+		 * convert physically contiguous buffer into
+		 * minimal length sg list
+		 */
+		sgcnt = phy_buf_to_sgl(userbuf, size, &sglist);
+		if (!sgcnt)
+			return -ENOMEM;
+
+		sglist_dma = sglist;
+		sgcnt_dma = sgcnt;
+	} else {
+		/* TODO: error checking */
+		pr_debug("xdma_submit: pinning user pages\n");
+		/* pin user pages is monitored separately */
+		xlnk_record_event(XLNK_ET_KERNEL_BEFORE_PIN_USER_PAGE);
+		status = pin_user_pages((unsigned long)userbuf, size,
+					dmadir != DMA_TO_DEVICE,
+					&sglist, &sgcnt, user_flags);
+		if (status < 0) {
+			pr_err("pin_user_pages failed\n");
+			return status;
+		}
+		xlnk_record_event(XLNK_ET_KERNEL_AFTER_PIN_USER_PAGE);
+		pr_debug("xdma_submit: pinning user pages done\n");
+
+		pr_debug("dma_map_sg %lx %d\n", (long unsigned)sglist, sgcnt);
+		xlnk_record_event(XLNK_ET_KERNEL_BEFORE_DMA_MAP_SG);
+/*
+ * TODO: Change xilinx_dma_map_sg to get_dma_ops(chan->dev)->map_sg(...
+ * replace user_flags with attrs set to ...SKIP_CPU_SYNC if connected to ACP
+ */
+		if (!(user_flags & CF_FLAG_CACHE_FLUSH_INVALIDATE))
+			dma_set_attr(DMA_ATTR_SKIP_CPU_SYNC, &attrs);
+
+		status = get_dma_ops(chan->dev)->map_sg(chan->dev, sglist,
+							sgcnt, dmadir, &attrs);
+#if 0
+		status = xilinx_dma_map_sg(chan->dev, sglist, sgcnt,
+					   dmadir, user_flags);
+#endif
+		if (!status) {
+			/* TODO: error handling */
+			pr_err("dma_map_sg failed\n");
+			unpin_user_pages(sglist, sgcnt);
+			return -ENOMEM;
+		}
+		xlnk_record_event(XLNK_ET_KERNEL_AFTER_DMA_MAP_SG);
+
+		/* merge sg list to save dma bds */
+		sgcnt_dma = sgl_merge(sglist, sgcnt, &sglist_dma);
+		if (!sgcnt_dma) {
+			get_dma_ops(chan->dev)->unmap_sg(chan->dev, sglist,
+							 sgcnt, dmadir, &attrs);
+			/* xilinx_dma_unmap_sg(chan->dev, sglist, sgcnt,
+					       dmadir, user_flags); */
+			unpin_user_pages(sglist, sgcnt);
+			return -ENOMEM;
+		}
+	}
+	dmahead->sglist = sglist;
+	dmahead->sgcnt = sgcnt;
+
+	/* skipping config */
+	pr_debug("init_completion ...\n");
+	init_completion(&dmahead->cmp);
+
+	if (nappwords_i > XDMA_MAX_APPWORDS)
+		nappwords_i = XDMA_MAX_APPWORDS;
+
+	if (nappwords_o > XDMA_MAX_APPWORDS)
+		nappwords_o = XDMA_MAX_APPWORDS;
+
+	dmahead->nappwords_o = nappwords_o;
+
+	pr_debug("setup hw desc...\n");
+	xlnk_record_event(XLNK_ET_KERNEL_BEFORE_DMA_SETUP_BD);
+	status = xdma_setup_hw_desc(chan, dmahead, sglist_dma, sgcnt_dma,
+				    dmadir, nappwords_i, appwords_i);
+	xlnk_record_event(XLNK_ET_KERNEL_AFTER_DMA_SETUP_BD);
+	if (status) {
+		/* TODO: error handling */
+		pr_err("setup hw desc failed\n");
+		if (!(user_flags & CF_FLAG_PHYSICALLY_CONTIGUOUS)) {
+			get_dma_ops(chan->dev)->unmap_sg(chan->dev, sglist,
+							 sgcnt, dmadir, &attrs);
+			/* xilinx_dma_unmap_sg(chan->dev, sglist, sgcnt,
+					       dmadir, user_flags); */
+			unpin_user_pages(sglist, sgcnt);
+		}
+
+		return -ENOMEM;
+	}
+
+	*dmaheadpp = dmahead;
+
+	pr_debug("xdma_submit returning\n");
+
+	xlnk_record_event(XLNK_ET_KERNEL_LEAVE_DMA_SUBMIT);
+	return 0;
+}
+EXPORT_SYMBOL(xdma_submit);
+
+int xdma_wait(struct xdma_head *dmahead, unsigned int user_flags)
+{
+	struct xdma_chan *chan = dmahead->chan;
+	DEFINE_DMA_ATTRS(attrs);
+#if 0
+	if (dmahead->dmadir == DMA_TO_DEVICE)
+		/* do not wait on the send
+		 * this is a temp hack for debugging dpd
+		 */
+		return 0;
+#endif
+	xlnk_record_event(XLNK_ET_KERNEL_ENTER_DMA_WAIT);
+
+	if (chan->poll_mode) {
+#if 0
+		while (!(dmahead->cmp.done))
+#endif
+			xilinx_chan_desc_cleanup(chan);
+	} else
+		wait_for_completion(&dmahead->cmp);
+
+	if (!(user_flags & CF_FLAG_PHYSICALLY_CONTIGUOUS)) {
+		xlnk_record_event(XLNK_ET_KERNEL_BEFORE_DMA_UNMAP_SG);
+		if (!(user_flags & CF_FLAG_CACHE_FLUSH_INVALIDATE))
+			dma_set_attr(DMA_ATTR_SKIP_CPU_SYNC, &attrs);
+
+		get_dma_ops(chan->dev)->unmap_sg(chan->dev, dmahead->sglist,
+						 dmahead->sgcnt,
+						 dmahead->dmadir, &attrs);
+		/* xilinx_dma_unmap_sg(chan->dev, dmahead->sglist,
+				       dmahead->sgcnt, dmahead->dmadir,
+				       user_flags); */
+		xlnk_record_event(XLNK_ET_KERNEL_AFTER_DMA_UNMAP_SG);
+
+		unpin_user_pages(dmahead->sglist, dmahead->sgcnt);
+	}
+	xlnk_record_event(XLNK_ET_KERNEL_LEAVE_DMA_WAIT);
+	return 0;
+}
+EXPORT_SYMBOL(xdma_wait);
+
+int xdma_getconfig(struct xdma_chan *chan,
+				unsigned char *irq_thresh,
+				unsigned char *irq_delay)
+{
+	*irq_thresh = (chan->regs->cr >> XDMA_COALESCE_SHIFT) & 0xff;
+	*irq_delay = (chan->regs->cr >> XDMA_DELAY_SHIFT) & 0xff;
+	return 0;
+}
+EXPORT_SYMBOL(xdma_getconfig);
+
+int xdma_setconfig(struct xdma_chan *chan,
+				unsigned char irq_thresh,
+				unsigned char irq_delay)
+{
+	unsigned long val;
+
+	if (dma_is_running(chan))
+		return -EBUSY;
+
+	val = chan->regs->cr;
+	val &= ~((0xff << XDMA_COALESCE_SHIFT) |
+				(0xff << XDMA_DELAY_SHIFT));
+	val |= ((irq_thresh << XDMA_COALESCE_SHIFT) |
+				(irq_delay << XDMA_DELAY_SHIFT));
+
+	chan->regs->cr = val;
+	return 0;
+}
+EXPORT_SYMBOL(xdma_setconfig);
+
+/* Brute-force probing for xilinx DMA
+ */
+static int xdma_probe(struct platform_device *pdev)
+{
+	struct xdma_device *xdev;
+	struct resource *res;
+	int err = 0;
+	struct xdma_chan *chan;
+	struct dma_device_config *dma_config;
+	int dma_chan_dir;
+	int dma_chan_reg_offset;
+	int i;
+
+	pr_info("%s: probe dma %x, nres %d, id %d\n", __func__,
+		 (unsigned int)&pdev->dev,
+		 pdev->num_resources, pdev->id);
+
+	xdev = kzalloc(sizeof(struct xdma_device), GFP_KERNEL);
+	if (!xdev) {
+		dev_err(&pdev->dev, "Not enough memory for device\n");
+		err = -ENOMEM;
+		goto out_return;
+	}
+	xdev->dev = &(pdev->dev);
+
+	dma_config = (struct dma_device_config *)xdev->dev->platform_data;
+	if (dma_config->channel_count < 1 || dma_config->channel_count > 2)
+		goto out_free_xdev;
+
+	/* Get the memory resource */
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!res) {
+		pr_err("get_resource for MEM resource for dev %d failed\n",
+		       pdev->id);
+		err = -ENODEV;
+		goto out_free_xdev;
+	} else {
+		dev_info(&pdev->dev,
+			"AXIDMA device %d physical base address=0x%08x\n",
+			 pdev->id, (unsigned int)res->start);
+	}
+
+	if (!request_mem_region(res->start, res->end - res->start + 1,
+						pdev->name)) {
+		pr_err("memory request failue for base %x\n",
+		       (unsigned int)res->start);
+		err = -EBUSY;
+		goto out_free_xdev;
+	}
+
+	/* ioremp */
+	xdev->regs = ioremap(res->start, res->end - res->start + 1);
+	if (!xdev->regs) {
+		dev_err(&pdev->dev, "unable to iomap registers\n");
+		err = -EFAULT;
+		goto out_release_mem;
+	}
+	dev_info(&pdev->dev, "AXIDMA device %d remapped to 0x%08x\n",
+		       pdev->id, (unsigned int)xdev->regs);
+
+	/* Allocate the channels */
+
+	dev_info(&pdev->dev, "has %d channel(s)\n", dma_config->channel_count);
+	for (i = 0; i < dma_config->channel_count; i++) {
+		chan = kzalloc(sizeof(*chan), GFP_KERNEL);
+		if (!chan) {
+			dev_err(&pdev->dev, "no free memory for DMA channel\n");
+			err = -ENOMEM;
+			goto out_iounmap_regs;
+		}
+
+		dma_chan_dir = strcmp(dma_config->channel_config[i].type,
+					"axi-dma-mm2s-channel") ?
+				DMA_FROM_DEVICE : DMA_TO_DEVICE;
+		dma_chan_reg_offset = dma_chan_dir == DMA_TO_DEVICE ? 0 : 0x30;
+
+		/* Initialize channel parameters */
+		chan->id = i;
+		chan->regs = (void __iomem *)
+				((u32)xdev->regs + dma_chan_reg_offset);
+		/* chan->regs = xdev->regs; */
+		chan->dev = xdev->dev;
+		chan->max_len = XDMA_MAX_TRANS_LEN;
+		chan->direction = dma_chan_dir;
+		sprintf(chan->name, "%schan%d", dev_name(&pdev->dev), chan->id);
+		pr_info("  chan%d name: %s\n", chan->id, chan->name);
+		pr_info("  chan%d direction: %s\n", chan->id,
+			dma_chan_dir == DMA_FROM_DEVICE ?
+				"FROM_DEVICE" : "TO_DEVICE");
+
+		spin_lock_init(&chan->lock);
+		tasklet_init(&chan->tasklet, xdma_tasklet, (unsigned long)chan);
+		tasklet_init(&chan->dma_err_tasklet, xdma_err_tasklet,
+						(unsigned long)chan);
+
+		xdev->chan[chan->id] = chan;
+
+		/* The IRQ resource */
+		chan->irq = dma_config->channel_config[i].irq;
+		if (chan->irq <= 0) {
+			pr_err("get_resource for IRQ for dev %d failed\n",
+				pdev->id);
+			err = -ENODEV;
+			goto out_free_chan1;
+		}
+
+		err = request_irq(
+			chan->irq,
+			dma_chan_dir == DMA_TO_DEVICE ?
+				xdma_tx_intr_handler : xdma_rx_intr_handler,
+			IRQF_SHARED,
+			pdev->name,
+			chan);
+		if (err) {
+			dev_err(&pdev->dev, "unable to request IRQ\n");
+			goto out_free_chan1;
+		}
+		pr_info("  chan%d irq: %d\n", chan->id, chan->irq);
+
+		chan->poll_mode = dma_config->channel_config[i].poll_mode;
+		pr_info("  chan%d poll mode: %s\n", chan->id,
+				chan->poll_mode ? "on" : "off");
+
+		/* Allocate channel BD's */
+		err = xdma_alloc_chan_descriptors(xdev->chan[chan->id]);
+		if (err) {
+			dev_err(&pdev->dev, "unable to allocate BD's\n");
+			err = -ENOMEM;
+			goto out_free_chan1_irq;
+		}
+		pr_info("  chan%d bd ring @ 0x%08x (size: 0x%08x bytes)\n",
+				chan->id, chan->bd_phys_addr,
+				chan->bd_chain_size);
+
+		err = dma_init(xdev->chan[chan->id]);
+		if (err) {
+			dev_err(&pdev->dev, "DMA init failed\n");
+			err = -EIO;
+			goto out_free_chan1_res;
+		}
+	}
+	xdev->channel_count = dma_config->channel_count;
+
+	/* Add the DMA device to the global list */
+	mutex_lock(&dma_list_mutex);
+	list_add_tail(&xdev->node, &dma_device_list);
+	mutex_unlock(&dma_list_mutex);
+
+	platform_set_drvdata(pdev, xdev);
+
+	return 0;
+
+out_free_chan1_res:
+	xdma_free_chan_resources(xdev->chan[chan->id]);
+out_free_chan1_irq:
+	free_irq(xdev->chan[chan->id]->irq, xdev->chan[chan->id]);
+out_free_chan1:
+	kfree(xdev->chan[chan->id]);
+out_iounmap_regs:
+	iounmap(xdev->regs);
+out_release_mem:
+	release_mem_region(res->start, resource_size(res));
+out_free_xdev:
+	kfree(xdev);
+out_return:
+	return err;
+}
+
+static int xdma_remove(struct platform_device *pdev)
+{
+	int i;
+	struct xdma_device *xdev = platform_get_drvdata(pdev);
+	struct resource *res;
+
+	/* Remove the DMA device from the global list */
+	mutex_lock(&dma_list_mutex);
+	list_del(&xdev->node);
+	mutex_unlock(&dma_list_mutex);
+
+	for (i = 0; i < XDMA_MAX_CHANS_PER_DEVICE; i++) {
+		if (xdev->chan[i])
+			xdma_chan_remove(xdev->chan[i]);
+	}
+
+	iounmap(xdev->regs);
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	release_mem_region(res->start, resource_size(res));
+
+	dev_set_drvdata(&pdev->dev, NULL);
+	kfree(xdev);
+
+	return 0;
+}
+
+static void xdma_shutdown(struct platform_device *pdev)
+{
+	int i;
+	struct xdma_device *xdev = platform_get_drvdata(pdev);
+
+	for (i = 0; i < XDMA_MAX_CHANS_PER_DEVICE; i++)
+		dma_halt(xdev->chan[i]);
+}
+
+static struct platform_driver xdma_driver = {
+	.probe = xdma_probe,
+	.remove = xdma_remove,
+	.shutdown = xdma_shutdown,
+	.driver = {
+		.owner = THIS_MODULE,
+		.name = "xilinx-axidma",
+	},
+};
+
+/*----------------------------------------------------------------------------*/
+/* Module Init / Exit                                                         */
+/*----------------------------------------------------------------------------*/
+
+static __init int xdma_init(void)
+{
+	int status;
+
+	mutex_init(&dma_list_mutex);
+	status = platform_driver_register(&xdma_driver);
+	return status;
+}
+module_init(xdma_init);
+
+static void __exit xdma_exit(void)
+{
+	platform_driver_unregister(&xdma_driver);
+}
+
+module_exit(xdma_exit);
+
+MODULE_DESCRIPTION("Xilinx DMA driver");
+MODULE_LICENSE("GPL");
diff --git a/drivers/staging/apf/xilinx-dma-apf.h b/drivers/staging/apf/xilinx-dma-apf.h
new file mode 100644
index 0000000..3d96e60
--- /dev/null
+++ b/drivers/staging/apf/xilinx-dma-apf.h
@@ -0,0 +1,242 @@
+/*
+ * Xilinx AXI DMA Engine support
+ *
+ * Copyright (C) 2010 Xilinx, Inc. All rights reserved.
+ *
+ * This is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ */
+
+#ifndef __XILINX_DMA_APF_H
+#define __XILINX_DMA_APF_H
+
+/* ioctls */
+#include <linux/ioctl.h>
+
+/* tasklet */
+#include <linux/interrupt.h>
+
+/* dma stuff */
+#include <linux/dma-mapping.h>
+
+#define XDMA_IOC_MAGIC 'X'
+#define XDMA_IOCRESET		_IO(XDMA_IOC_MAGIC, 0)
+#define XDMA_IOCREQUEST		_IOWR(XDMA_IOC_MAGIC, 1, unsigned long)
+#define XDMA_IOCRELEASE		_IOWR(XDMA_IOC_MAGIC, 2, unsigned long)
+#define XDMA_IOCSUBMIT		_IOWR(XDMA_IOC_MAGIC, 3, unsigned long)
+#define XDMA_IOCWAIT		_IOWR(XDMA_IOC_MAGIC, 4, unsigned long)
+#define XDMA_IOCGETCONFIG	_IOWR(XDMA_IOC_MAGIC, 5, unsigned long)
+#define XDMA_IOCSETCONFIG	_IOWR(XDMA_IOC_MAGIC, 6, unsigned long)
+#define XDMA_IOC_MAXNR		6
+
+/* Specific hardware configuration-related constants
+ */
+#define XDMA_RESET_LOOP            1000000
+#define XDMA_HALT_LOOP             1000000
+#define XDMA_NO_CHANGE             0xFFFF;
+
+/* General register bits definitions
+ */
+#define XDMA_CR_RESET_MASK    0x00000004  /* Reset DMA engine */
+#define XDMA_CR_RUNSTOP_MASK  0x00000001  /* Start/stop DMA engine */
+
+#define XDMA_SR_HALTED_MASK   0x00000001  /* DMA channel halted */
+#define XDMA_SR_IDLE_MASK     0x00000002  /* DMA channel idle */
+
+#define XDMA_SR_ERR_INTERNAL_MASK 0x00000010/* Datamover internal err */
+#define XDMA_SR_ERR_SLAVE_MASK    0x00000020 /* Datamover slave err */
+#define XDMA_SR_ERR_DECODE_MASK   0x00000040 /* Datamover decode err */
+#define XDMA_SR_ERR_SG_INT_MASK   0x00000100 /* SG internal err */
+#define XDMA_SR_ERR_SG_SLV_MASK   0x00000200 /* SG slave err */
+#define XDMA_SR_ERR_SG_DEC_MASK   0x00000400 /* SG decode err */
+#define XDMA_SR_ERR_ALL_MASK      0x00000770 /* All errors */
+
+#define XDMA_XR_IRQ_IOC_MASK	0x00001000 /* Completion interrupt */
+#define XDMA_XR_IRQ_DELAY_MASK	0x00002000 /* Delay interrupt */
+#define XDMA_XR_IRQ_ERROR_MASK	0x00004000 /* Error interrupt */
+#define XDMA_XR_IRQ_ALL_MASK	    0x00007000 /* All interrupts */
+
+#define XDMA_XR_DELAY_MASK    0xFF000000 /* Delay timeout counter */
+#define XDMA_XR_COALESCE_MASK 0x00FF0000 /* Coalesce counter */
+
+#define XDMA_DELAY_SHIFT    24
+#define XDMA_COALESCE_SHIFT 16
+
+#define XDMA_DELAY_MAX     0xFF /**< Maximum delay counter value */
+#define XDMA_COALESCE_MAX  0xFF /**< Maximum coalescing counter value */
+
+/* BD definitions for Axi DMA
+ */
+#define XDMA_BD_STS_ACTUAL_LEN_MASK	0x007FFFFF
+#define XDMA_BD_STS_COMPL_MASK 0x80000000
+#define XDMA_BD_STS_ERR_MASK   0x70000000
+#define XDMA_BD_STS_ALL_MASK   0xF0000000
+
+/* DMA BD special bits definitions
+ */
+#define XDMA_BD_SOP       0x08000000    /* Start of packet bit */
+#define XDMA_BD_EOP       0x04000000    /* End of packet bit */
+
+/* BD Software Flag definitions for Axi DMA
+ */
+#define XDMA_BD_SF_POLL_MODE_MASK	0x00000002
+#define XDMA_BD_SF_SW_DONE_MASK		0x00000001
+
+/* driver defines */
+#define XDMA_MAX_BD_CNT			2048
+#define XDMA_MAX_CHANS_PER_DEVICE	2
+#define XDMA_MAX_TRANS_LEN		0x7FF000
+#define XDMA_MAX_APPWORDS		5
+#define XDMA_BD_CLEANUP_THRESHOLD	((XDMA_MAX_BD_CNT * 8) / 10)
+
+/* Platform data definition until ARM supports device tree */
+struct dma_channel_config {
+	char *type;
+	unsigned int include_dre;
+	unsigned int datawidth;
+	unsigned int max_burst_len;
+	unsigned int irq;
+	unsigned int poll_mode;
+	unsigned int lite_mode;
+};
+struct dma_device_config {
+	char *type;
+	unsigned int include_sg;
+	unsigned int sg_include_stscntrl_strm;  /* dma only */
+	unsigned int channel_count;
+	struct dma_channel_config *channel_config;
+};
+
+struct xdma_desc_hw {
+	u32 next_desc;	/* 0x00 */
+	u32 pad1;       /* 0x04 */
+	u32 src_addr;   /* 0x08 */
+	u32 pad2;       /* 0x0c */
+	u32 addr_vsize; /* 0x10 */
+	u32 hsize;       /* 0x14 */
+	u32 control;    /* 0x18 */
+	u32 status;     /* 0x1c */
+	u32 app[5];      /* 0x20 */
+	u32 dmahead;
+	u32 sw_flag;	/* 0x38 */
+	u32 Reserved0;
+} __aligned(64);
+
+/* shared by all Xilinx DMA engines */
+/* FIXME use proper readl/writel functions instead of volatile */
+struct xdma_regs {
+	volatile u32 cr;        /* 0x00 Control Register */
+	volatile u32 sr;        /* 0x04 Status Register */
+	volatile u32 cdr;       /* 0x08 Current Descriptor Register */
+	volatile u32 pad1;
+	volatile u32 tdr;       /* 0x10 Tail Descriptor Register */
+	volatile u32 pad2;
+	volatile u32 src;       /* 0x18 Source Address Register (cdma) */
+	volatile u32 pad3;
+	volatile u32 dst;       /* 0x20 Destination Address Register (cdma) */
+	volatile u32 pad4;
+	volatile u32 btt_ref;   /* 0x28 Bytes To Transfer (cdma) or
+					park_ref (vdma) */
+	volatile u32 version;   /* 0x2c version (vdma) */
+};
+
+/* Per DMA specific operations should be embedded in the channel structure */
+struct xdma_chan {
+	char name[64];
+	struct xdma_regs __iomem *regs;
+	struct device *dev;			/* The dma device */
+	struct xdma_desc_hw *bds[XDMA_MAX_BD_CNT];
+	dma_addr_t bd_phys_addr;
+	u32 bd_chain_size;
+	int bd_cur;
+	int bd_tail;
+	unsigned int bd_used;			/* # of BDs passed to hw chan */
+	enum dma_data_direction direction;	/* Transfer direction */
+	int id;					/* Channel ID */
+	int irq;				/* Channel IRQ */
+	int poll_mode;				/* Poll mode turned on? */
+	spinlock_t lock;			/* Descriptor operation lock */
+	struct tasklet_struct tasklet;		/* Cleanup work after irq */
+	struct tasklet_struct dma_err_tasklet;	/* Cleanup work after irq */
+	int    max_len;				/* Maximum len per transfer */
+	int    err;				/* Channel has errors */
+	int    client_count;
+};
+
+struct xdma_device {
+	void __iomem *regs;
+	struct device *dev;
+	struct list_head node;
+	struct xdma_chan *chan[XDMA_MAX_CHANS_PER_DEVICE];
+	u8 channel_count;
+};
+
+struct xdma_head {
+	void *userbuf;
+	unsigned int size;
+	unsigned int dmaflag;
+	enum dma_data_direction dmadir;
+	void *sglist;
+	unsigned int sgcnt;
+	struct completion cmp;
+	struct xdma_chan *chan;
+	unsigned int nappwords_o;
+	u32 appwords_o[XDMA_MAX_APPWORDS];
+	unsigned int userflag;
+	u32 last_bd_index;
+};
+
+typedef union {
+	struct {
+		char name[64];
+		unsigned long dmachan;
+	} dmarequest;
+	struct {
+		unsigned long dmachan;
+	} dmarelease;
+	struct {
+		void *buf;
+		unsigned int len;
+		unsigned long dmachan;
+		unsigned long dmahandle; /* return value */
+		unsigned int nappwords_i; /* n appwords passed to BD */
+		unsigned int appwords_i[XDMA_MAX_APPWORDS];
+		unsigned int nappwords_o; /* n appwords passed from BD */
+		unsigned int user_flags;
+	} dmasubmit;
+	struct {
+		unsigned long dmahandle;
+		unsigned int nappwords_o; /* n appwords read from BD */
+		unsigned int appwords_o[XDMA_MAX_APPWORDS];
+		unsigned int user_flags;
+	} dmawait;
+	struct {
+		unsigned long dmachan;
+		unsigned char irq_thresh;
+		unsigned char irq_delay;
+	} dmaconfig;
+} xdma_args;
+
+struct xdma_chan *xdma_request_channel(char *name);
+void xdma_release_channel(struct xdma_chan *chan);
+void xdma_release_all_channels(void);
+int xdma_submit(struct xdma_chan *chan,
+		void *userbuf,
+		unsigned int size,
+		unsigned int nappwords_i,
+		u32 *appwords_i,
+		unsigned int nappwords_o,
+		unsigned int user_flags,
+		struct xdma_head **dmaheadpp);
+int xdma_wait(struct xdma_head *dmahead, unsigned int user_flags);
+int xdma_getconfig(struct xdma_chan *chan,
+		   unsigned char *irq_thresh,
+		   unsigned char *irq_delay);
+int xdma_setconfig(struct xdma_chan *chan,
+		   unsigned char irq_thresh,
+		   unsigned char irq_delay);
+
+#endif
diff --git a/drivers/staging/apf/xlnk-eng.c b/drivers/staging/apf/xlnk-eng.c
new file mode 100644
index 0000000..23b9530
--- /dev/null
+++ b/drivers/staging/apf/xlnk-eng.c
@@ -0,0 +1,278 @@
+/*
+ * Xilinx XLNK Engine Driver
+ *
+ * Copyright (C) 2010 Xilinx, Inc. All rights reserved.
+ *
+ *
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/io.h>
+#include <linux/spinlock_types.h>
+#include <linux/slab.h>
+#include <linux/platform_device.h>
+#include <linux/interrupt.h>
+#include <linux/mutex.h>
+#include <linux/string.h>
+#include <linux/uio_driver.h>
+
+
+#include "xlnk-eng.h"
+
+static DEFINE_MUTEX(xlnk_eng_list_mutex);
+static LIST_HEAD(xlnk_eng_list);
+
+int xlnk_eng_register_device(struct xlnk_eng_device *xlnk_dev)
+{
+	mutex_lock(&xlnk_eng_list_mutex);
+	/* todo: need to add more error checking */
+
+	list_add_tail(&xlnk_dev->global_node, &xlnk_eng_list);
+
+	mutex_unlock(&xlnk_eng_list_mutex);
+
+	return 0;
+}
+EXPORT_SYMBOL(xlnk_eng_register_device);
+
+
+void xlnk_eng_unregister_device(struct xlnk_eng_device *xlnk_dev)
+{
+	mutex_lock(&xlnk_eng_list_mutex);
+	/* todo: need to add more error checking */
+
+	list_del(&xlnk_dev->global_node);
+
+	mutex_unlock(&xlnk_eng_list_mutex);
+}
+EXPORT_SYMBOL(xlnk_eng_unregister_device);
+
+struct xlnk_eng_device *xlnk_eng_request_by_name(char *name)
+{
+	struct xlnk_eng_device *device, *_d;
+	int found = 0;
+
+	mutex_lock(&xlnk_eng_list_mutex);
+
+	list_for_each_entry_safe(device, _d, &xlnk_eng_list, global_node) {
+		if (!strcmp(dev_name(device->dev), name)) {
+			found = 1;
+			break;
+		}
+	}
+	if (found)
+		device = device->alloc(device);
+	else
+		device = NULL;
+
+	mutex_unlock(&xlnk_eng_list_mutex);
+
+	return device;
+}
+EXPORT_SYMBOL(xlnk_eng_request_by_name);
+
+void xlnk_eng_release(struct xlnk_eng_device *xlnk_dev)
+{
+	if (!xlnk_dev)
+		return;
+
+	xlnk_dev->free(xlnk_dev);
+}
+EXPORT_SYMBOL(xlnk_eng_release);
+
+#define DRIVER_NAME "xilinx-xlnk-eng"
+
+struct xilinx_xlnk_eng_device {
+	struct xlnk_eng_device common;
+	void __iomem *base;
+	spinlock_t lock;
+	int cnt;
+};
+
+
+#define to_xilinx_xlnk(dev)	container_of(dev, \
+					struct xilinx_xlnk_eng_device, common)
+
+static struct xlnk_eng_device *xilinx_xlnk_alloc(
+					struct xlnk_eng_device *xlnkdev)
+{
+	struct xilinx_xlnk_eng_device *xdev;
+	struct xlnk_eng_device *retdev;
+
+	xdev = to_xilinx_xlnk(xlnkdev);
+
+	if (xdev->cnt == 0) {
+		xdev->cnt++;
+		retdev = xlnkdev;
+	} else
+		retdev = NULL;
+
+	return retdev;
+}
+
+static void xilinx_xlnk_free(struct xlnk_eng_device *xlnkdev)
+{
+	struct xilinx_xlnk_eng_device *xdev;
+
+	xdev = to_xilinx_xlnk(xlnkdev);
+
+	xdev->cnt = 0;
+}
+
+static int xlnk_eng_probe(struct platform_device *pdev)
+{
+	struct resource *res;
+	u32 reg_range;
+	struct xilinx_xlnk_eng_device *xdev;
+	struct uio_info *info;
+	char *devname;
+	int err = 0;
+
+	pr_info("xlnk_eng_probe ...\n");
+	xdev = kzalloc(sizeof(struct xilinx_xlnk_eng_device), GFP_KERNEL);
+	if (!xdev) {
+		dev_err(&pdev->dev, "Not enough memory for device\n");
+		err = -ENOMEM;
+		goto out_return;
+	}
+
+	/* more error handling */
+	info = kzalloc(sizeof(struct uio_info), GFP_KERNEL);
+	if (!info) {
+		dev_err(&pdev->dev, "Not enough memory for device\n");
+		err = -ENOMEM;
+		goto out_free_xdev;
+	}
+
+	devname = (char *) kzalloc(64, GFP_KERNEL);
+	if (!devname) {
+		dev_err(&pdev->dev, "Not enough memory for device\n");
+		err = -ENOMEM;
+		goto out_free_xdev;
+	}
+	sprintf(devname, "%s.%d", DRIVER_NAME, pdev->id);
+	pr_info("uio name %s\n", devname);
+	/* iomap registers */
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!res) {
+		pr_err("get_resource for MEM resource for dev %d failed\n",
+		       pdev->id);
+		err = -ENOMEM;
+		goto out_free_xdev;
+	}
+	reg_range = res->end - res->start + 1;
+	if (!request_mem_region(res->start, reg_range, "xilin-xlnk-eng")) {
+		pr_err("memory request failue for base %x\n",
+		       (unsigned int)res->start);
+		err = -ENOMEM;
+		goto out_free_xdev;
+	}
+
+	xdev->base = ioremap(res->start, reg_range);
+
+	pr_info("%s physical base : 0x%lx\n", DRIVER_NAME,
+		(unsigned long)res->start);
+	pr_info("%s register range : 0x%lx\n", DRIVER_NAME,
+		(unsigned long)reg_range);
+	pr_info("%s base remapped to: 0x%lx\n", DRIVER_NAME,
+		(unsigned long)xdev->base);
+	if (!xdev->base) {
+		dev_err(&pdev->dev, "unable to iomap registers\n");
+		err = -ENOMEM;
+		goto out_free_xdev;
+	}
+
+	info->mem[0].addr = res->start;
+	info->mem[0].size = reg_range;
+	info->mem[0].memtype = UIO_MEM_PHYS;
+	info->mem[0].internal_addr = xdev->base;
+
+	/* info->name = DRIVER_NAME; */
+	info->name = devname;
+	info->version = "0.0.1";
+
+	info->irq = -1;
+
+	xdev->common.dev = &pdev->dev;
+
+	xdev->common.alloc = xilinx_xlnk_alloc;
+	xdev->common.free = xilinx_xlnk_free;
+
+	dev_set_drvdata(&pdev->dev, xdev);
+
+	spin_lock_init(&xdev->lock);
+
+	xdev->cnt = 0;
+
+	xlnk_eng_register_device(&xdev->common);
+
+	if (uio_register_device(&pdev->dev, info)) {
+		dev_err(&pdev->dev, "uio_register_device failed\n");
+		err = -ENODEV;
+		goto out_unmap;
+	}
+	pr_info("xilinx-xlnk-eng uio registered\n");
+
+	return 0;
+
+
+out_unmap:
+	iounmap(xdev->base);
+
+	kfree(info);
+
+out_free_xdev:
+	kfree(xdev);
+
+out_return:
+	return err;
+}
+
+static int xlnk_eng_remove(struct platform_device *pdev)
+{
+	struct xlnk_eng_device *xlnk_dev =
+		(struct xlnk_eng_device *)platform_get_drvdata(pdev);
+	struct xilinx_xlnk_eng_device *xdev = to_xilinx_xlnk(xlnk_dev);
+
+	/* xlnk_eng_device_unregister(&xdev); */
+
+	iounmap(xdev->base);
+
+	dev_set_drvdata(&pdev->dev, NULL);
+
+	kfree(xdev);
+
+	return 0;
+}
+
+static struct platform_driver xlnk_eng_driver = {
+	.probe = xlnk_eng_probe,
+	.remove = xlnk_eng_remove,
+	.driver = {
+		.owner = THIS_MODULE,
+		.name = DRIVER_NAME,
+	},
+};
+
+/*----------------------------------------------------------------------------*/
+/* Module Init / Exit                                                         */
+/*----------------------------------------------------------------------------*/
+
+static __init int xlnk_eng_init(void)
+{
+	int status;
+	status = platform_driver_register(&xlnk_eng_driver);
+	return status;
+}
+module_init(xlnk_eng_init);
+
+static void __exit xlnk_eng_exit(void)
+{
+	platform_driver_unregister(&xlnk_eng_driver);
+}
+
+module_exit(xlnk_eng_exit);
+
+MODULE_DESCRIPTION("Xilinx xlnk engine generic driver");
+MODULE_LICENSE("GPL");
diff --git a/drivers/staging/apf/xlnk-eng.h b/drivers/staging/apf/xlnk-eng.h
new file mode 100644
index 0000000..c00d85c
--- /dev/null
+++ b/drivers/staging/apf/xlnk-eng.h
@@ -0,0 +1,33 @@
+/*
+ * Xilinx XLNK Engine Driver
+ *
+ * Copyright (C) 2010 Xilinx, Inc. All rights reserved.
+ *
+ *
+ */
+
+#ifndef XLNK_ENG_H
+#define XLNK_ENG_H
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/io.h>
+#include <linux/spinlock_types.h>
+#include <linux/slab.h>
+#include <linux/platform_device.h>
+#include <linux/interrupt.h>
+#include <linux/mutex.h>
+#include <linux/string.h>
+
+struct xlnk_eng_device {
+	struct list_head global_node;
+	struct xlnk_eng_device * (*alloc)(struct xlnk_eng_device *xdev);
+	void (*free)(struct xlnk_eng_device *xdev);
+	struct device *dev;
+};
+extern int xlnk_eng_register_device(struct xlnk_eng_device *xlnk_dev);
+extern void xlnk_eng_unregister_device(struct xlnk_eng_device *xlnk_dev);
+extern struct xlnk_eng_device *xlnk_eng_request_by_name(char *name);
+extern void xlnk_eng_release(struct xlnk_eng_device *xlnk_dev);
+
+#endif
diff --git a/drivers/staging/apf/xlnk-event-tracer-type.h b/drivers/staging/apf/xlnk-event-tracer-type.h
new file mode 100644
index 0000000..21217f4
--- /dev/null
+++ b/drivers/staging/apf/xlnk-event-tracer-type.h
@@ -0,0 +1,38 @@
+#ifndef XLNK_EVENT_TRACER_TYPE_H
+#define XLNK_EVENT_TRACER_TYPE_H
+
+#define XLNK_ET_USERSPACE_BEFORE_ACC_TRANSFER_CALL	50
+#define XLNK_ET_USERSPACE_BEFORE_PORT_WAIT4COMPLETION	51
+
+#define XLNK_ET_USERSPACE_BEFORE_DMA_SUBMIT		100
+#define XLNK_ET_USERSPACE_AFTER_DMA_SUBMIT		101
+#define XLNK_ET_USERSPACE_BEFORE_DMA_WAIT4COMPLETION	102
+#define XLNK_ET_USERSPACE_AFTER_DMA_WAIT4COMPLETION	103
+
+#define XLNK_ET_KERNEL_ENTER_IOCTL			5000
+#define XLNK_ET_KERNEL_LEAVE_IOCTL			5001
+#define XLNK_ET_KERNEL_ENTER_DMA_SUBMIT			5002
+#define XLNK_ET_KERNEL_LEAVE_DMA_SUBMIT			5003
+#define XLNK_ET_KERNEL_BEFORE_PIN_USER_PAGE		5004
+#define XLNK_ET_KERNEL_BEFORE_GET_USER_PAGES		5005
+#define XLNK_ET_KERNEL_AFTER_GET_USER_PAGES		5006
+#define XLNK_ET_KERNEL_AFTER_PIN_USER_PAGE		5007
+#define XLNK_ET_KERNEL_BEFORE_DMA_MAP_SG		5008
+#define XLNK_ET_KERNEL_AFTER_DMA_MAP_SG			5009
+#define XLNK_ET_KERNEL_BEFORE_DMA_SETUP_BD		5010
+#define XLNK_ET_KERNEL_AFTER_DMA_KICKOFF		5011
+#define XLNK_ET_KERNEL_AFTER_DMA_SETUP_BD		5012
+#define XLNK_ET_KERNEL_ENTER_DMA_WAIT			5013
+#define XLNK_ET_KERNEL_BEFORE_DMA_UNMAP_SG		5014
+#define XLNK_ET_KERNEL_AFTER_DMA_UNMAP_SG		5015
+#define XLNK_ET_KERNEL_LEAVE_DMA_WAIT			5016
+
+
+#define XLNK_ET_KERNEL_DMA_MM2S_INTR			5100
+#define XLNK_ET_KERNEL_DMA_MM2S_HALF_BOTTOM_START	5101
+#define XLNK_ET_KERNEL_DMA_MM2S_HALF_BOTTOM_END		5102
+#define XLNK_ET_KERNEL_DMA_S2MM_INTR			5110
+#define XLNK_ET_KERNEL_DMA_S2MM_HALF_BOTTOM_START	5111
+#define XLNK_ET_KERNEL_DMA_S2MM_HALF_BOTTOM_END		5112
+
+#endif
diff --git a/drivers/staging/apf/xlnk-ioctl.h b/drivers/staging/apf/xlnk-ioctl.h
new file mode 100644
index 0000000..665fc27
--- /dev/null
+++ b/drivers/staging/apf/xlnk-ioctl.h
@@ -0,0 +1,39 @@
+#ifndef _XLNK_IOCTL_H
+#define _XLNK_IOCTL_H
+
+#include <linux/ioctl.h>
+
+#define XLNK_IOC_MAGIC 'X'
+
+#define XLNK_IOCRESET		_IO(XLNK_IOC_MAGIC, 0)
+
+#define XLNK_IOCALLOCBUF	_IOWR(XLNK_IOC_MAGIC, 2, unsigned long)
+#define XLNK_IOCFREEBUF		_IOWR(XLNK_IOC_MAGIC, 3, unsigned long)
+
+
+
+#define XLNK_IOCDMAREQUEST	_IOWR(XLNK_IOC_MAGIC, 7, unsigned long)
+#define XLNK_IOCDMASUBMIT	_IOWR(XLNK_IOC_MAGIC, 8, unsigned long)
+#define XLNK_IOCDMAWAIT		_IOWR(XLNK_IOC_MAGIC, 9, unsigned long)
+#define XLNK_IOCDMARELEASE	_IOWR(XLNK_IOC_MAGIC, 10, unsigned long)
+
+
+
+
+
+#define XLNK_IOCDEVREGISTER	_IOWR(XLNK_IOC_MAGIC, 16, unsigned long)
+#define XLNK_IOCDMAREGISTER	_IOWR(XLNK_IOC_MAGIC, 17, unsigned long)
+#define XLNK_IOCDEVUNREGISTER	_IOWR(XLNK_IOC_MAGIC, 18, unsigned long)
+#define XLNK_IOCCDMAREQUEST	_IOWR(XLNK_IOC_MAGIC, 19, unsigned long)
+#define XLNK_IOCCDMASUBMIT	_IOWR(XLNK_IOC_MAGIC, 20, unsigned long)
+#define XLNK_IOCGETEVENTSIZE	_IOWR(XLNK_IOC_MAGIC, 21, unsigned long)
+#define XLNK_IOCDUMPEVENTS	_IOWR(XLNK_IOC_MAGIC, 22, unsigned long)
+#define XLNK_IOCMCDMAREGISTER	_IOWR(XLNK_IOC_MAGIC, 23, unsigned long)
+#define XLNK_IOCCACHECTRL	_IOWR(XLNK_IOC_MAGIC, 24, unsigned long)
+
+#define XLNK_IOCSHUTDOWN	_IOWR(XLNK_IOC_MAGIC, 100, unsigned long)
+#define XLNK_IOCRECRES		_IOWR(XLNK_IOC_MAGIC, 101, unsigned long)
+
+#define XLNK_IOC_MAXNR		101
+
+#endif
diff --git a/drivers/staging/apf/xlnk.c b/drivers/staging/apf/xlnk.c
new file mode 100644
index 0000000..cd92f3d
--- /dev/null
+++ b/drivers/staging/apf/xlnk.c
@@ -0,0 +1,1311 @@
+/*
+ * xlnk.c
+ *
+ * Xilinx Accelerator driver support.
+ *
+ * Copyright (C) 2010 Xilinx Inc.
+ *
+ * This package is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * THIS PACKAGE IS PROVIDED ``AS IS'' AND WITHOUT ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, WITHOUT LIMITATION, THE IMPLIED
+ * WARRANTIES OF MERCHANTIBILITY AND FITNESS FOR A PARTICULAR PURPOSE.
+ */
+
+/*  ----------------------------------- Host OS */
+#include <linux/module.h>
+#include <linux/types.h>
+#include <linux/platform_device.h>
+#include <linux/pm.h>
+#include <linux/fs.h>
+#include <linux/slab.h>
+#include <linux/gfp.h>
+#include <linux/mm.h>
+#include <asm/cacheflush.h>
+#include <linux/io.h>
+
+#include <linux/string.h>
+
+#include <linux/uaccess.h>
+
+#include <linux/dmaengine.h>
+#include <linux/completion.h>
+#include <linux/wait.h>
+
+#include "xlnk-ioctl.h"
+#include "xlnk-event-tracer-type.h"
+#include "xlnk.h"
+
+#ifdef CONFIG_XILINX_DMA_APF
+#include "xilinx-dma-apf.h"
+#endif
+
+#ifdef CONFIG_XILINX_MCDMA
+#include "xdma-if.h"
+#include "xdma.h"
+
+static void xdma_if_device_release(struct device *op)
+{
+}
+
+#endif
+
+#ifdef MODULE
+#include <linux/module.h>
+#endif
+
+#include <linux/device.h>
+#include <linux/init.h>
+#include <linux/moduleparam.h>
+#include <linux/cdev.h>
+
+#include <linux/sched.h>
+#include <linux/mm.h>	   /* everything */
+#include <linux/pagemap.h>
+#include <linux/errno.h>	/* error codes */
+#include <linux/dma-mapping.h>  /* dma */
+
+#define DRIVER_NAME  "xlnk"
+#define DRIVER_VERSION  "0.2"
+
+static struct platform_device *xlnk_pdev;
+static struct device *xlnk_dev;
+
+static struct cdev xlnk_cdev;
+
+static struct class *xlnk_class;
+
+static s32 driver_major;
+
+static char *driver_name = DRIVER_NAME;
+
+static void *xlnk_dev_buf;
+static ssize_t xlnk_dev_size;
+static int xlnk_dev_vmas;
+
+#define XLNK_BUF_POOL_SIZE	32
+static void **xlnk_bufpool;
+static unsigned int xlnk_bufpool_size = XLNK_BUF_POOL_SIZE;
+static dma_addr_t xlnk_phyaddr[XLNK_BUF_POOL_SIZE];
+static size_t xlnk_buflen[XLNK_BUF_POOL_SIZE];
+static unsigned int  xlnk_bufcacheable[XLNK_BUF_POOL_SIZE];
+
+
+static int __init xlnk_init(void);  /* Initialize bridge */
+static void __exit xlnk_exit(void); /* Opposite of initialize */
+static int xlnk_open(struct inode *ip, struct file *filp);  /* Open */
+static int xlnk_release(struct inode *ip, struct file *filp);   /* Release */
+static long xlnk_ioctl(struct file *filp, unsigned int code,
+				unsigned long args);
+static ssize_t xlnk_read(struct file *filp, char __user *buf,
+			  size_t count, loff_t *offp);
+static ssize_t xlnk_write(struct file *filp, const char __user *buf,
+			  size_t count, loff_t *offp);
+static int xlnk_mmap(struct file *filp, struct vm_area_struct *vma);
+static void xlnk_vma_open(struct vm_area_struct *vma);
+static void xlnk_vma_close(struct vm_area_struct *vma);
+
+static int xlnk_init_bufpool(void);
+
+static void xlnk_start_benchmark_counter(void);
+static int xlnk_dump_events(unsigned long buf);
+static int xlnk_get_event_size(unsigned long buf);
+
+static int xlnk_shutdown(unsigned long buf);
+static int xlnk_recover_resource(unsigned long buf);
+
+static const struct file_operations xlnk_fops = {
+	.open = xlnk_open,
+	.release = xlnk_release,
+	.read = xlnk_read,
+	.write = xlnk_write,
+	.unlocked_ioctl = xlnk_ioctl,
+	.mmap = xlnk_mmap,
+};
+
+#define MAX_XLNK_DMAS 16
+
+struct xlnk_device_pack {
+	char name[64];
+	struct platform_device pdev;
+	struct resource res[8];
+
+#ifdef CONFIG_XILINX_DMA_APF
+	struct dma_channel_config dma_chan_cfg[4];  /* for xidane dma only */
+	struct dma_device_config dma_dev_cfg;	   /* for xidane dma only */
+#endif
+
+#ifdef CONFIG_XILINX_MCDMA
+	struct xdma_device_info mcdma_dev_cfg;	 /* for mcdma only */
+#endif
+
+};
+
+static struct xlnk_device_pack *xlnk_devpacks[16];
+static void xlnk_devpacks_init(void)
+{
+	unsigned int i;
+
+	for (i = 0; i < 16; i++)
+		xlnk_devpacks[0] = NULL;
+
+}
+
+static void xlnk_devpacks_delete(struct xlnk_device_pack *devpack)
+{
+	unsigned int i;
+
+	for (i = 0; i < 16; i++) {
+		if (xlnk_devpacks[i] == devpack)
+			xlnk_devpacks[i] = NULL;
+	}
+}
+
+static void xlnk_devpacks_add(struct xlnk_device_pack *devpack)
+{
+	unsigned int i;
+
+	for (i = 0; i < 16; i++) {
+		if (xlnk_devpacks[i] == NULL) {
+			xlnk_devpacks[i] = devpack;
+			break;
+		}
+	}
+}
+
+static struct xlnk_device_pack *xlnk_devpacks_find(unsigned long base)
+{
+	unsigned int i;
+
+	for (i = 0; i < 16; i++) {
+		if (xlnk_devpacks[i]
+			&& xlnk_devpacks[i]->res[0].start == base)
+			return xlnk_devpacks[i];
+	}
+	return NULL;
+}
+
+static void xlnk_devpacks_free(unsigned long base)
+{
+	struct xlnk_device_pack *devpack;
+
+	devpack = xlnk_devpacks_find(base);
+	if (devpack) {
+		platform_device_unregister(&devpack->pdev);
+		kfree(devpack);
+		xlnk_devpacks_delete(devpack);
+	}
+}
+
+static void xlnk_devpacks_free_all(void)
+{
+	struct xlnk_device_pack *devpack;
+	unsigned int i;
+
+	for (i = 0; i < 16; i++) {
+		devpack = xlnk_devpacks[i];
+		if (devpack) {
+			platform_device_unregister(&devpack->pdev);
+			kfree(devpack);
+			xlnk_devpacks_delete(devpack);
+		}
+	}
+}
+
+static int xlnk_probe(struct platform_device *pdev)
+{
+	int err;
+	dev_t dev = 0;
+
+	/* use 2.6 device model */
+	err = alloc_chrdev_region(&dev, 0, 1, driver_name);
+	if (err) {
+		pr_err("%s: Can't get major %d\n", __func__, driver_major);
+		goto err1;
+	}
+
+	cdev_init(&xlnk_cdev, &xlnk_fops);
+
+	xlnk_cdev.owner = THIS_MODULE;
+
+	err = cdev_add(&xlnk_cdev, dev, 1);
+
+	if (err) {
+		pr_err("%s: Failed to add XLNK device\n", __func__);
+		goto err3;
+	}
+
+	/* udev support */
+	xlnk_class = class_create(THIS_MODULE, "xlnk");
+	if (IS_ERR(xlnk_class)) {
+		pr_err("%s: Error creating xlnk class\n", __func__);
+		goto err3;
+	}
+
+	driver_major = MAJOR(dev);
+
+	pr_info("xlnk major %d\n", driver_major);
+
+	device_create(xlnk_class, NULL, MKDEV(driver_major, 0),
+			  NULL, "xlnk");
+
+	xlnk_init_bufpool();
+
+	pr_info("%s driver loaded\n", DRIVER_NAME);
+
+	xlnk_pdev = pdev;
+	xlnk_dev = &pdev->dev;
+
+	if (xlnk_pdev)
+		pr_info("xlnk_pdev is not null\n");
+	else
+		pr_info("xlnk_pdev is null\n");
+
+	xlnk_devpacks_init();
+
+#ifdef CONFIG_ARCH_ZYNQ
+	xlnk_start_benchmark_counter();
+#endif
+
+	return 0;
+
+err3:
+	cdev_del(&xlnk_cdev);
+	unregister_chrdev_region(dev, 1);
+err1:
+	return err;
+}
+
+static int xlnk_buf_findnull(void)
+{
+	int i;
+
+	for (i = 1; i < xlnk_bufpool_size; i++) {
+		if (!xlnk_bufpool[i])
+			return i;
+	}
+
+	return 0;
+}
+
+/**
+ * allocate and return an id
+ * id must be a positve number
+ */
+static int xlnk_allocbuf(unsigned int len, unsigned int cacheable)
+{
+	int id;
+
+	id = xlnk_buf_findnull();
+
+	if (id <= 0)
+		return -ENOMEM;
+
+	xlnk_bufpool[id] = dma_alloc_coherent(xlnk_dev, len,
+					      &xlnk_phyaddr[id],
+					      GFP_KERNEL | GFP_DMA);
+	xlnk_buflen[id] = len;
+	xlnk_bufcacheable[id] = cacheable;
+
+	if (!xlnk_bufpool[id]) {
+		pr_err("%s: dma_alloc_coherent of %d byte buffer failed\n",
+		       __func__, len);
+		return -ENOMEM;
+	}
+
+	pr_debug("%s: dma_alloc_coherent(%x) got %lx @ phy addr %lx\n",
+		 __func__, len, (unsigned long)xlnk_bufpool[id],
+		 (unsigned long)xlnk_phyaddr[id]);
+
+	return id;
+}
+
+static int xlnk_init_bufpool(void)
+{
+	unsigned int i;
+
+	xlnk_dev_buf = kmalloc(8192, GFP_KERNEL | __GFP_DMA);
+	*((char *)xlnk_dev_buf) = '\0';
+
+	if (!xlnk_dev_buf) {
+		pr_err("%s: malloc failed\n", __func__);
+		return -ENOMEM;
+	}
+
+	xlnk_bufpool = kmalloc(sizeof(void *) * xlnk_bufpool_size,
+				   GFP_KERNEL);
+
+	xlnk_bufpool[0] = xlnk_dev_buf;
+	for (i = 1; i < xlnk_bufpool_size; i++)
+		xlnk_bufpool[i] = NULL;
+
+	return 0;
+}
+
+#define XLNK_SUSPEND NULL
+#define XLNK_RESUME NULL
+
+static int xlnk_remove(struct platform_device *pdev)
+{
+	dev_t devno;
+
+	kfree(xlnk_dev_buf);
+	xlnk_dev_buf = NULL;
+
+	kfree(xlnk_bufpool);
+	xlnk_bufpool = NULL;
+
+	devno = MKDEV(driver_major, 0);
+	cdev_del(&xlnk_cdev);
+	unregister_chrdev_region(devno, 1);
+	if (xlnk_class) {
+		/* remove the device from sysfs */
+		device_destroy(xlnk_class, MKDEV(driver_major, 0));
+		class_destroy(xlnk_class);
+	}
+
+	xlnk_devpacks_free_all();
+
+	return 0;
+}
+
+
+static struct platform_driver xlnk_driver = {
+	.driver = {
+		   .name = DRIVER_NAME,
+		   },
+	.probe = xlnk_probe,
+	.remove = xlnk_remove,
+	.suspend = XLNK_SUSPEND,
+	.resume = XLNK_RESUME,
+};
+
+static u64 dma_mask = 0xFFFFFFFFUL;
+
+static struct platform_device xlnk_device = {
+	.name = "xlnk",
+	.id = 0,
+	.dev = {
+		.platform_data = NULL,
+		.dma_mask = &dma_mask,
+		.coherent_dma_mask = 0xFFFFFFFF,
+	},
+	.resource = NULL,
+	.num_resources = 0,
+};
+
+
+static int __init xlnk_init(void)
+{
+	pr_info("%s driver initializing\n", DRIVER_NAME);
+
+	xlnk_dev_buf = NULL;
+	xlnk_dev_size = 0;
+	xlnk_dev_vmas = 0;
+	xlnk_bufpool = NULL;
+
+	platform_device_register(&xlnk_device);
+
+	return platform_driver_register(&xlnk_driver);
+}
+
+static void __exit xlnk_exit(void)
+{
+	platform_driver_unregister(&xlnk_driver);
+}
+
+/*
+ * This function is called when an application opens handle to the
+ * bridge driver.
+ */
+static int xlnk_open(struct inode *ip, struct file *filp)
+{
+	int status = 0;
+
+	if ((filp->f_flags & O_ACCMODE) == O_WRONLY)
+		xlnk_dev_size = 0;
+
+	return status;
+}
+
+static ssize_t xlnk_read(struct file *filp, char __user *buf,
+			  size_t count, loff_t *offp)
+{
+	ssize_t retval = 0;
+
+	/* todo: need semi for critical section */
+
+	if (*offp >= xlnk_dev_size)
+		goto out;
+
+	if (*offp + count > xlnk_dev_size)
+		count = xlnk_dev_size - *offp;
+
+	if (copy_to_user(buf, xlnk_dev_buf + *offp, count)) {
+		retval = -EFAULT;
+		goto out;
+	}
+	*offp += count;
+	retval = count;
+
+ out:
+	return retval;
+}
+
+static ssize_t xlnk_write(struct file *filp, const char __user *buf,
+			  size_t count, loff_t *offp)
+{
+	ssize_t retval = 0;
+
+	/* todo: need to setup semi for critical section */
+
+	if (copy_from_user(xlnk_dev_buf + *offp, buf, count)) {
+		retval = -EFAULT;
+		goto out;
+	}
+	*offp += count;
+	retval = count;
+
+	if (xlnk_dev_size < *offp)
+		xlnk_dev_size = *offp;
+
+ out:
+	return retval;
+}
+
+/*
+ * This function is called when an application closes handle to the bridge
+ * driver.
+ */
+static int xlnk_release(struct inode *ip, struct file *filp)
+{
+	int status = 0;
+	return status;
+}
+
+
+static int xlnk_devregister(char *name, unsigned int id,
+				unsigned long base, unsigned int size,
+				unsigned int *irqs,
+				xlnk_handle_t *handle)
+{
+	unsigned int nres;
+	unsigned int nirq;
+	unsigned int *irqptr;
+	struct xlnk_device_pack *devpack;
+	unsigned int i;
+	int status;
+
+	devpack = xlnk_devpacks_find(base);
+	if (devpack) {
+		*handle = (xlnk_handle_t)devpack;
+		return 0;
+	}
+	nirq = 0;
+	irqptr = irqs;
+
+	while (*irqptr) {
+		nirq++;
+		irqptr++;
+	}
+
+	if (nirq > 7)
+		return -ENOMEM;
+
+	nres = nirq + 1;
+
+	devpack = kzalloc(sizeof(struct xlnk_device_pack),
+			  GFP_KERNEL);
+	strcpy(devpack->name, name);
+	devpack->pdev.name = devpack->name;
+
+	devpack->pdev.id = id;
+
+	devpack->pdev.dev.dma_mask = &dma_mask;
+	devpack->pdev.dev.coherent_dma_mask = 0xFFFFFFFF;
+
+	devpack->res[0].start = base;
+	devpack->res[0].end = base + size - 1;
+	devpack->res[0].flags = IORESOURCE_MEM;
+
+	for (i = 0; i < nirq; i++) {
+		devpack->res[i+1].start = irqs[i];
+		devpack->res[i+1].end = irqs[i];
+		devpack->res[i+1].flags = IORESOURCE_IRQ;
+	}
+
+	devpack->pdev.resource = devpack->res;
+	devpack->pdev.num_resources = nres;
+
+	status = platform_device_register(&devpack->pdev);
+	if (status) {
+		kfree(devpack);
+		*handle = 0;
+	} else {
+		xlnk_devpacks_add(devpack);
+		*handle = (xlnk_handle_t)devpack;
+	}
+	return status;
+}
+
+static int xlnk_dmaregister(char *name, unsigned int id,
+				unsigned long base, unsigned int size,
+				unsigned int chan_num,
+				unsigned int chan0_dir,
+				unsigned int chan0_irq,
+				unsigned int chan0_poll_mode,
+				unsigned int chan0_include_dre,
+				unsigned int chan0_data_width,
+				unsigned int chan1_dir,
+				unsigned int chan1_irq,
+				unsigned int chan1_poll_mode,
+				unsigned int chan1_include_dre,
+				unsigned int chan1_data_width,
+				xlnk_handle_t *handle)
+{
+	int status = -1;
+
+#ifdef CONFIG_XILINX_DMA_APF
+
+	struct xlnk_device_pack *devpack;
+
+	pr_debug("name = %s\n", name);
+	pr_debug("id = %d\n", id);
+	pr_debug("chan_num = %d\n", chan_num);
+	pr_debug("base = 0x%08x\n", (unsigned int)base);
+	pr_debug("size = 0x%08x\n", size);
+
+	if (strcmp(name, "xilinx-axidma"))
+		return -EINVAL;
+
+	if (chan_num < 1 || chan_num > 2)
+		return -EINVAL;
+
+	devpack = xlnk_devpacks_find(base);
+	if (devpack) {
+		*handle = (xlnk_handle_t)devpack;
+		return 0;
+	}
+
+	devpack = kzalloc(sizeof(struct xlnk_device_pack),
+			  GFP_KERNEL);
+	if (!devpack)
+		return -ENOMEM;
+
+	strcpy(devpack->name, name);
+	devpack->pdev.name = devpack->name;
+
+	devpack->pdev.id = id;
+
+	devpack->dma_chan_cfg[0].include_dre = chan0_include_dre;
+	devpack->dma_chan_cfg[0].datawidth   = chan0_data_width;
+	devpack->dma_chan_cfg[0].irq = chan0_irq;
+	devpack->dma_chan_cfg[0].poll_mode   = chan0_poll_mode;
+	devpack->dma_chan_cfg[0].type = chan0_dir ?
+					"axi-dma-s2mm-channel" :
+					"axi-dma-mm2s-channel";
+
+	if (chan_num > 1) {
+		devpack->dma_chan_cfg[1].include_dre = chan1_include_dre;
+		devpack->dma_chan_cfg[1].datawidth   = chan1_data_width;
+		devpack->dma_chan_cfg[1].irq = chan1_irq;
+		devpack->dma_chan_cfg[1].poll_mode   = chan1_poll_mode;
+		devpack->dma_chan_cfg[1].type = chan1_dir ?
+						"axi-dma-s2mm-channel" :
+						"axi-dma-mm2s-channel";
+	}
+
+	devpack->dma_dev_cfg.type = "axi-dma";
+	devpack->dma_dev_cfg.include_sg = 1;
+	devpack->dma_dev_cfg.sg_include_stscntrl_strm = 1;
+	devpack->dma_dev_cfg.channel_count = chan_num;
+	devpack->dma_dev_cfg.channel_config = &devpack->dma_chan_cfg[0];
+
+	devpack->pdev.dev.platform_data = &devpack->dma_dev_cfg;
+
+	devpack->pdev.dev.dma_mask = &dma_mask;
+	devpack->pdev.dev.coherent_dma_mask = 0xFFFFFFFF;
+
+	devpack->res[0].start = base;
+	devpack->res[0].end = base + size - 1;
+	devpack->res[0].flags = IORESOURCE_MEM;
+
+	devpack->pdev.resource = devpack->res;
+	devpack->pdev.num_resources = 1;
+
+	status = platform_device_register(&devpack->pdev);
+	if (status) {
+		kfree(devpack);
+		*handle = 0;
+	} else {
+		xlnk_devpacks_add(devpack);
+		*handle = (xlnk_handle_t)devpack;
+	}
+
+#endif
+	return status;
+}
+
+static int xlnk_mcdmaregister(char *name, unsigned int id,
+			      unsigned long base, unsigned int size,
+			      unsigned int mm2s_chan_num,
+			      unsigned int mm2s_chan_irq,
+			      unsigned int s2mm_chan_num,
+			      unsigned int s2mm_chan_irq,
+			      xlnk_handle_t *handle)
+{
+	int status = -1;
+
+#ifdef CONFIG_XILINX_MCDMA
+	struct xlnk_device_pack *devpack;
+
+	pr_debug("name = %s\n", name);
+	pr_debug("id = %d\n", id);
+	pr_debug("base = 0x%08x\n", (unsigned int)base);
+	pr_debug("size = 0x%08x\n", size);
+	pr_debug("mm2s_chan_num = %d\n", mm2s_chan_num);
+	pr_debug("mm2s_chan_irq = %d\n", mm2s_chan_irq);
+	pr_debug("s2mm_chan_num = %d\n", s2mm_chan_num);
+	pr_debug("s2mm_chan_irq = %d\n", s2mm_chan_irq);
+
+	if (strcmp(name, "xdma"))
+		return -EINVAL;
+
+
+	devpack = xlnk_devpacks_find(base);
+	if (devpack) {
+		*handle = (xlnk_handle_t)devpack;
+		return 0;
+	}
+
+	devpack = kzalloc(sizeof(struct xlnk_device_pack),
+			  GFP_KERNEL);
+	if (!devpack)
+		return -ENOMEM;
+
+	strcpy(devpack->name, name);
+	devpack->pdev.name = devpack->name;
+	devpack->pdev.id = id;
+
+	devpack->mcdma_dev_cfg.tx_chans	= mm2s_chan_num;
+	devpack->mcdma_dev_cfg.rx_chans	= s2mm_chan_num;
+	devpack->mcdma_dev_cfg.legacy_mode = XDMA_MCHAN_MODE;
+	devpack->mcdma_dev_cfg.device_id   = id;
+
+	devpack->pdev.dev.platform_data	 = &devpack->mcdma_dev_cfg;
+	devpack->pdev.dev.dma_mask = &dma_mask;
+	devpack->pdev.dev.coherent_dma_mask = 0xFFFFFFFF;
+	devpack->pdev.dev.release = xdma_if_device_release,
+
+	devpack->res[0].start = base;
+	devpack->res[0].end   = base + size - 1;
+	devpack->res[0].flags = IORESOURCE_MEM;
+
+	devpack->res[1].start = mm2s_chan_irq;
+	devpack->res[1].end   = s2mm_chan_irq;
+	devpack->res[1].flags = IORESOURCE_IRQ;
+
+	devpack->pdev.resource	  = devpack->res;
+	devpack->pdev.num_resources = 2;
+
+	status = platform_device_register(&devpack->pdev);
+	if (status) {
+		kfree(devpack);
+		*handle = 0;
+	} else {
+		xlnk_devpacks_add(devpack);
+		*handle = (xlnk_handle_t)devpack;
+	}
+
+#endif
+
+	return status;
+}
+
+static int xlnk_allocbuf_ioctl(struct file *filp, unsigned int code,
+			unsigned long args)
+{
+
+	xlnk_args temp_args;
+	int status;
+	int id;
+
+	status = copy_from_user(&temp_args, (xlnk_args *)args,
+				sizeof(xlnk_args));
+
+	if (status)
+		return -ENOMEM;
+
+	id = xlnk_allocbuf(temp_args.allocbuf.len,
+			   temp_args.allocbuf.cacheable);
+
+	if (id <= 0)
+		return -ENOMEM;
+
+	put_user(id, temp_args.allocbuf.idptr);
+	put_user((u32)(xlnk_phyaddr[id]), temp_args.allocbuf.phyaddrptr);
+
+	pr_debug("xlnk_allocbuf allocated buf #%d @ phy addr 0x%x\n",
+			id, (u32)(xlnk_phyaddr[id]));
+
+	return 0;
+}
+
+static int xlnk_freebuf(int id)
+{
+	pr_debug("xlnk_freebuf buf with id %x\n", id);
+
+	if (id <= 0 || id >= xlnk_bufpool_size)
+		return -ENOMEM;
+
+	if (!xlnk_bufpool[id])
+		return -ENOMEM;
+
+	pr_debug("xlnk_freebuf: kernel virt addr = %lx, phy = %lx\n",
+		 (unsigned long)xlnk_bufpool[id],
+		 (unsigned long)xlnk_phyaddr[id]);
+
+	dma_free_coherent(xlnk_dev, xlnk_buflen[id], xlnk_bufpool[id],
+			  xlnk_phyaddr[id]);
+
+	xlnk_bufpool[id] = NULL;
+	xlnk_phyaddr[id] = (dma_addr_t)NULL;
+	xlnk_buflen[id] = 0;
+
+	return 0;
+}
+
+static void xlnk_free_all_buf(void)
+{
+	int i;
+
+	for (i = 1; i < xlnk_bufpool_size; i++)
+		xlnk_freebuf(i);
+}
+
+static int xlnk_freebuf_ioctl(struct file *filp, unsigned int code,
+			unsigned long args)
+{
+
+	xlnk_args temp_args;
+	int status;
+	int id;
+
+	status = copy_from_user(&temp_args, (xlnk_args *)args,
+				sizeof(xlnk_args));
+
+	if (status)
+		return -ENOMEM;
+
+	id = temp_args.freebuf.id;
+	return xlnk_freebuf(id);
+}
+
+static int xlnk_dmarequest_ioctl(struct file *filp, unsigned int code,
+				 unsigned long args)
+{
+
+#ifdef CONFIG_XILINX_DMA_APF
+
+	xlnk_args temp_args;
+	int status;
+	struct xdma_chan *chan;
+
+	status = copy_from_user(&temp_args, (xlnk_args *)args,
+				sizeof(xlnk_args));
+
+	if (status)
+		return -ENOMEM;
+
+	if (!temp_args.dmarequest.name[0])
+		return 0;
+
+	pr_debug("dma channel name: %s\n", temp_args.dmarequest.name);
+
+	chan = xdma_request_channel(temp_args.dmarequest.name);
+
+	if (!chan) {
+		pr_debug("dma channel request failed\n");
+		return -ENOMEM;
+	}
+
+	temp_args.dmarequest.dmachan = (xlnk_handle_t)chan;
+	temp_args.dmarequest.bd_space_phys_addr = chan->bd_phys_addr;
+	temp_args.dmarequest.bd_space_size = chan->bd_chain_size;
+
+	copy_to_user((void *)args, &temp_args, sizeof(xlnk_args));
+
+	return 0;
+
+#else
+
+	return -1;
+
+#endif
+
+}
+
+static int xlnk_dmasubmit_ioctl(struct file *filp, unsigned int code,
+				unsigned long args)
+{
+
+#ifdef CONFIG_XILINX_DMA_APF
+	xlnk_args temp_args;
+	struct xdma_head *dmahead;
+	int status = -1;
+
+	status = copy_from_user(&temp_args, (xlnk_args *)args,
+				sizeof(xlnk_args));
+
+	pr_debug("dmasubmit: copy_from_user done\n");
+	if (status)
+		return -ENOMEM;
+
+	if (!temp_args.dmasubmit.dmachan)
+		return -ENODEV;
+
+	status = xdma_submit((struct xdma_chan *)temp_args.dmasubmit.dmachan,
+						temp_args.dmasubmit.buf,
+						temp_args.dmasubmit.len,
+						temp_args.dmasubmit.nappwords_i,
+						temp_args.dmasubmit.appwords_i,
+						temp_args.dmasubmit.nappwords_o,
+						temp_args.dmasubmit.flag,
+						&dmahead);
+	pr_debug("dmasubmit: xdma_submit done\n");
+
+	if (!status) {
+		temp_args.dmasubmit.dmahandle = (xlnk_handle_t)dmahead;
+		temp_args.dmasubmit.last_bd_index =
+					(xlnk_handle_t)dmahead->last_bd_index;
+		copy_to_user((void *)args, &temp_args, sizeof(xlnk_args));
+		pr_debug("dmasubmit: copy_to_user done\n");
+
+		return 0;
+	}
+#endif
+	return -ENOMEM;
+}
+
+
+static int xlnk_dmawait_ioctl(struct file *filp, unsigned int code,
+				  unsigned long args)
+{
+	int status = -1;
+
+#ifdef CONFIG_XILINX_DMA_APF
+	xlnk_args temp_args;
+	struct xdma_head *dmahead;
+
+	status = copy_from_user(&temp_args, (xlnk_args *)args,
+				sizeof(xlnk_args));
+
+	if (status)
+		return -ENOMEM;
+
+	dmahead = (struct xdma_head *)temp_args.dmawait.dmahandle;
+	status = xdma_wait(dmahead, dmahead->userflag);
+
+	if (temp_args.dmawait.nappwords) {
+		memcpy(temp_args.dmawait.appwords, dmahead->appwords_o,
+			   dmahead->nappwords_o * sizeof(u32));
+
+		copy_to_user((void *)args, &temp_args, sizeof(xlnk_args));
+	}
+	kfree(dmahead);
+
+#endif
+
+	return status;
+}
+
+static int xlnk_dmarelease_ioctl(struct file *filp, unsigned int code,
+				 unsigned long args)
+{
+	int status = -1;
+
+#ifdef CONFIG_XILINX_DMA_APF
+
+	xlnk_args temp_args;
+	status = copy_from_user(&temp_args, (xlnk_args *)args,
+				sizeof(xlnk_args));
+
+	if (status)
+		return -ENOMEM;
+
+	xdma_release_channel((struct xdma_chan *)temp_args.dmarelease.dmachan);
+#endif
+
+	return status;
+}
+
+
+static int xlnk_devregister_ioctl(struct file *filp, unsigned int code,
+				  unsigned long args)
+{
+	xlnk_args temp_args;
+	int status;
+	xlnk_handle_t handle;
+
+	status = copy_from_user(&temp_args, (xlnk_args *)args,
+				sizeof(xlnk_args));
+
+	if (status)
+		return -ENOMEM;
+
+	status = xlnk_devregister(temp_args.devregister.name,
+				  temp_args.devregister.id,
+				  temp_args.devregister.base,
+				  temp_args.devregister.size,
+				  temp_args.devregister.irqs,
+				  &handle);
+
+	return status;
+}
+
+static int xlnk_dmaregister_ioctl(struct file *filp, unsigned int code,
+				  unsigned long args)
+{
+	xlnk_args temp_args;
+	int status;
+	xlnk_handle_t handle;
+
+	status = copy_from_user(&temp_args, (xlnk_args *)args,
+				sizeof(xlnk_args));
+
+	if (status)
+		return -ENOMEM;
+
+	status = xlnk_dmaregister(temp_args.dmaregister.name,
+				  temp_args.dmaregister.id,
+				  temp_args.dmaregister.base,
+				  temp_args.dmaregister.size,
+				  temp_args.dmaregister.chan_num,
+				  temp_args.dmaregister.chan0_dir,
+				  temp_args.dmaregister.chan0_irq,
+				  temp_args.dmaregister.chan0_poll_mode,
+				  temp_args.dmaregister.chan0_include_dre,
+				  temp_args.dmaregister.chan0_data_width,
+				  temp_args.dmaregister.chan1_dir,
+				  temp_args.dmaregister.chan1_irq,
+				  temp_args.dmaregister.chan1_poll_mode,
+				  temp_args.dmaregister.chan1_include_dre,
+				  temp_args.dmaregister.chan1_data_width,
+				  &handle);
+
+	return status;
+}
+
+static int xlnk_mcdmaregister_ioctl(struct file *filp, unsigned int code,
+				  unsigned long args)
+{
+	xlnk_args temp_args;
+	int status;
+	xlnk_handle_t handle;
+
+	status = copy_from_user(&temp_args, (xlnk_args *)args,
+				sizeof(xlnk_args));
+
+	if (status)
+		return -ENOMEM;
+
+	status = xlnk_mcdmaregister(temp_args.mcdmaregister.name,
+				  temp_args.mcdmaregister.id,
+				  temp_args.mcdmaregister.base,
+				  temp_args.mcdmaregister.size,
+				  temp_args.mcdmaregister.mm2s_chan_num,
+				  temp_args.mcdmaregister.mm2s_chan_irq,
+				  temp_args.mcdmaregister.s2mm_chan_num,
+				  temp_args.mcdmaregister.s2mm_chan_irq,
+				  &handle);
+
+	return status;
+}
+
+static int xlnk_devunregister_ioctl(struct file *filp, unsigned int code,
+					unsigned long args)
+{
+	xlnk_args temp_args;
+	int status;
+
+	status = copy_from_user(&temp_args, (xlnk_args *)args,
+				sizeof(xlnk_args));
+
+	if (status)
+		return -ENOMEM;
+
+	xlnk_devpacks_free(temp_args.devunregister.base);
+
+	return 0;
+}
+
+static int xlnk_cachecontrol_ioctl(struct file *filp, unsigned int code,
+				   unsigned long args)
+{
+	xlnk_args temp_args;
+	int status, size;
+	void *paddr, *kaddr;
+
+	status = copy_from_user(&temp_args, (xlnk_args *)args,
+							sizeof(xlnk_args));
+
+	if (status) {
+		pr_err("Error in copy_from_user. status = %d\n", status);
+		return -ENOMEM;
+	}
+
+	if (!(temp_args.cachecontrol.action == 0 ||
+		  temp_args.cachecontrol.action == 1)) {
+		pr_err("Illegal action specified to cachecontrol_ioctl: %d\n",
+		       temp_args.cachecontrol.action);
+		return -EINVAL;
+	}
+
+	size = temp_args.cachecontrol.size;
+	paddr = temp_args.cachecontrol.phys_addr;
+	kaddr = phys_to_virt((unsigned int)paddr);
+
+	if (temp_args.cachecontrol.action == 0) {
+		/* flush cache */
+		dmac_map_area(kaddr, size, DMA_TO_DEVICE);
+		outer_clean_range((unsigned int)paddr,
+				  (unsigned int)(paddr + size));
+	} else {
+		/* invalidate cache */
+		outer_inv_range((unsigned int)paddr,
+				(unsigned int)(paddr + size));
+		dmac_unmap_area(kaddr, size, DMA_FROM_DEVICE);
+	}
+
+	return 0;
+}
+
+/* This function provides IO interface to the bridge driver. */
+static long xlnk_ioctl(struct file *filp, unsigned int code,
+			 unsigned long args)
+{
+	int status = 0;
+
+	xlnk_record_event(XLNK_ET_KERNEL_ENTER_IOCTL);
+
+	if (_IOC_TYPE(code) != XLNK_IOC_MAGIC)
+		return -ENOTTY;
+	if (_IOC_NR(code) > XLNK_IOC_MAXNR)
+		return -ENOTTY;
+
+	/* some sanity check */
+	switch (code) {
+	case XLNK_IOCALLOCBUF:
+		status = xlnk_allocbuf_ioctl(filp, code, args);
+		break;
+	case XLNK_IOCFREEBUF:
+		status = xlnk_freebuf_ioctl(filp, code, args);
+		break;
+	case XLNK_IOCDMAREQUEST:
+		status = xlnk_dmarequest_ioctl(filp, code, args);
+		break;
+	case XLNK_IOCDMASUBMIT:
+		status = xlnk_dmasubmit_ioctl(filp, code, args);
+		break;
+	case XLNK_IOCDMAWAIT:
+		status = xlnk_dmawait_ioctl(filp, code, args);
+		break;
+	case XLNK_IOCDMARELEASE:
+		status = xlnk_dmarelease_ioctl(filp, code, args);
+		break;
+	case XLNK_IOCDEVREGISTER:
+		status = xlnk_devregister_ioctl(filp, code, args);
+		break;
+	case XLNK_IOCDMAREGISTER:
+		status = xlnk_dmaregister_ioctl(filp, code, args);
+		break;
+	case XLNK_IOCMCDMAREGISTER:
+		status = xlnk_mcdmaregister_ioctl(filp, code, args);
+		break;
+	case XLNK_IOCDEVUNREGISTER:
+		status = xlnk_devunregister_ioctl(filp, code, args);
+		break;
+	case XLNK_IOCGETEVENTSIZE:
+		status = xlnk_get_event_size(args);
+		break;
+	case XLNK_IOCCACHECTRL:
+		status = xlnk_cachecontrol_ioctl(filp, code, args);
+		break;
+	case XLNK_IOCDUMPEVENTS:
+		status = xlnk_dump_events(args);
+		break;
+	case XLNK_IOCSHUTDOWN:
+		status = xlnk_shutdown(args);
+		break;
+	case XLNK_IOCRECRES: /* recover resource */
+		status = xlnk_recover_resource(args);
+		break;
+	}
+
+	xlnk_record_event(XLNK_ET_KERNEL_LEAVE_IOCTL);
+	return status;
+}
+
+static struct vm_operations_struct xlnk_vm_ops = {
+	.open = xlnk_vma_open,
+	.close = xlnk_vma_close,
+};
+
+/* This function maps kernel space memory to user space memory. */
+static int xlnk_mmap(struct file *filp, struct vm_area_struct *vma)
+{
+
+	int bufid;
+	int status;
+
+	pr_debug("vm_start %lx, len %lx, vm_pgoff %lx\n",
+		   vma->vm_start,
+		   vma->vm_end - vma->vm_start,
+		   vma->vm_pgoff);
+
+	bufid = vma->vm_pgoff >> (24 - PAGE_SHIFT);
+
+	if (bufid == 0)
+		status = remap_pfn_range(vma, vma->vm_start,
+				virt_to_phys(xlnk_dev_buf) >> PAGE_SHIFT,
+				vma->vm_end - vma->vm_start,
+				vma->vm_page_prot);
+	else {
+		if (xlnk_bufcacheable[bufid] == 0)
+			vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
+
+		status = remap_pfn_range(vma, vma->vm_start,
+					 xlnk_phyaddr[bufid] >> PAGE_SHIFT,
+					 vma->vm_end - vma->vm_start,
+					 vma->vm_page_prot);
+	}
+	if (status)
+		return -EAGAIN;
+
+	xlnk_vma_open(vma);
+	vma->vm_ops = &xlnk_vm_ops;
+	vma->vm_private_data = xlnk_bufpool[bufid];
+
+	return 0;
+}
+
+static void xlnk_vma_open(struct vm_area_struct *vma)
+{
+	xlnk_dev_vmas++;
+}
+
+static void xlnk_vma_close(struct vm_area_struct *vma)
+{
+	xlnk_dev_vmas--;
+}
+
+
+#ifdef CONFIG_ARCH_ZYNQ
+
+/*
+ * Xidane XLNK benchmark counter support
+ */
+/* FIXME use readl/writel functions */
+static volatile u32 __iomem *bc_virt;
+
+
+/* Zynq global counter */
+static const unsigned long bc_phyaddr = 0xF8F00200;
+static const unsigned long bc_to_cpu_shift = 1;
+static const unsigned long bc_csr_size = 16;
+static const unsigned long bc_ctr_offset = 2;
+static const unsigned long bc_ctr_start = 1;
+static const unsigned long bc_data_offset;
+
+
+static void xlnk_start_benchmark_counter(void)
+{
+	bc_virt = ioremap(bc_phyaddr, bc_csr_size);
+	pr_info("xlnk: benchmark counter mapped phy addr %x --> virt addr %x\n",
+			(u32)bc_phyaddr, (u32)bc_virt);
+	if (bc_virt) {
+		*(bc_virt + bc_ctr_offset) = bc_ctr_start;
+		pr_info("xlnk: benchmark counter started\n");
+		/* iounmap(bc_virt); */
+	}
+}
+
+#define XLNK_EVENT_TRACER_ENTRY_NUM 60000
+struct event_tracer {
+	u32 event_id;
+	u32 event_time;
+} xlnk_et[XLNK_EVENT_TRACER_ENTRY_NUM];
+
+static unsigned long xlnk_et_index;
+static unsigned long xlnk_et_numbers_to_dump;
+
+void xlnk_record_event(u32 event_id)
+{
+	if (xlnk_et_index >= XLNK_EVENT_TRACER_ENTRY_NUM)
+		return;
+
+	xlnk_et[xlnk_et_index].event_id = event_id;
+	xlnk_et[xlnk_et_index].event_time = (*(bc_virt + bc_data_offset)) <<
+								bc_to_cpu_shift;
+	/* rmb(); */
+	xlnk_et_index++;
+}
+EXPORT_SYMBOL(xlnk_record_event);
+
+static int xlnk_get_event_size(unsigned long args)
+{
+	unsigned long __user *datap = (unsigned long __user *)args;
+
+	/* take a snapshot of current index and only copy this
+	 * size to user space thru xlnk_dump_events(), as the snapshot
+	 * value determine the dynamically created user space event
+	 * trace buffer size  but the xlnk_et_index could keep going up
+	 * with any xlnk_record_event() calls after this function
+	 */
+	xlnk_et_numbers_to_dump = xlnk_et_index;
+	put_user(xlnk_et_numbers_to_dump, datap);
+	return 0;
+}
+
+static int xlnk_dump_events(unsigned long buf)
+{
+	pr_debug("xlnk: # of event captured = %lu", xlnk_et_index);
+
+	/* only dump the number of event traces reported thru
+	 * xlnk_get_event_size() and ignore the rest to avoid
+	 * buffer overflow issue
+	 */
+	copy_to_user((void *)buf, xlnk_et,
+		xlnk_et_numbers_to_dump * sizeof(struct event_tracer));
+
+	/* clear up event pool so it's ready to use again */
+	xlnk_et_index = 0;
+	xlnk_et_numbers_to_dump = 0;
+
+	return 0;
+}
+#endif
+
+
+static int xlnk_shutdown(unsigned long buf)
+{
+	return 0;
+}
+
+static int xlnk_recover_resource(unsigned long buf)
+{
+	xlnk_free_all_buf();
+#ifdef CONFIG_XILINX_DMA_APF
+	xdma_release_all_channels();
+#endif
+	return 0;
+}
+
+/* APF driver initialization and de-initialization functions */
+module_init(xlnk_init);
+module_exit(xlnk_exit);
+
+MODULE_DESCRIPTION("Xilinx APF driver");
+MODULE_LICENSE("GPL");
diff --git a/drivers/staging/apf/xlnk.h b/drivers/staging/apf/xlnk.h
new file mode 100644
index 0000000..2cb19e8
--- /dev/null
+++ b/drivers/staging/apf/xlnk.h
@@ -0,0 +1,117 @@
+#ifndef _XLNK_OS_H
+#define _XLNK_OS_H
+
+#include <stddef.h>
+
+#define XLNK_FLAG_COHERENT		0x00000001
+#define XLNK_FLAG_KERNEL_BUFFER		0x00000002
+#define XLNK_FLAG_DMAPOLLING		0x00000004
+#define XLNK_FLAG_PHYSICAL_ADDR		0x00000100
+#define XLNK_FLAG_VIRTUAL_ADDR		0x00000200
+
+#define CF_FLAG_CACHE_FLUSH_INVALIDATE	0x00000001
+#define CF_FLAG_PHYSICALLY_CONTIGUOUS	0x00000002
+#define CF_FLAG_DMAPOLLING		0x00000004
+
+extern void xlnk_record_event(u32 event_id);
+
+typedef unsigned long xlnk_handle_t;
+
+enum xlnk_dma_direction {
+	XLNK_DMA_BI = 0,
+	XLNK_DMA_TO_DEVICE = 1,
+	XLNK_DMA_FROM_DEVICE = 2,
+	XLNK_DMA_NONE = 3,
+};
+
+typedef union {
+	struct {
+		unsigned int len;
+		unsigned int *idptr;
+		unsigned int *phyaddrptr;
+		unsigned int cacheable;
+	} allocbuf;
+	struct {
+		unsigned int id;
+		void *buf;
+	} freebuf;
+	struct {
+		char name[64]; /* max length of 64 */
+		xlnk_handle_t dmachan; /* return value */
+		unsigned int bd_space_phys_addr;/*for bd chain used by dmachan*/
+		unsigned int bd_space_size; /* bd chain size in bytes */
+	} dmarequest;
+#define XLNK_MAX_APPWORDS 5
+	struct {
+		xlnk_handle_t dmachan;
+		void *buf;      /* buffer base address */
+		void *buf2;	/* used to point src_buf in cdma case */
+		unsigned int buf_offset; /* used on kernel allocated buffers */
+		unsigned int len;
+		unsigned int bufflag; /* zero all the time so far */
+		xlnk_handle_t sglist; /* ignored */
+		unsigned int sgcnt; /* ignored */
+		enum xlnk_dma_direction dmadir;
+		unsigned int nappwords_i; /* n appwords passed to BD */
+		unsigned int appwords_i[XLNK_MAX_APPWORDS];
+		unsigned int nappwords_o; /* n appwords passed from BD */
+		/* appwords array we only accept 5 max */
+		unsigned int flag;
+		xlnk_handle_t dmahandle; /* return value */
+		unsigned int last_bd_index; /*index of last bd used by request*/
+	} dmasubmit;
+	struct {
+		xlnk_handle_t dmahandle;
+		unsigned int nappwords; /* n appwords read from BD */
+		unsigned int appwords[XLNK_MAX_APPWORDS];
+		/* appwords array we only accept 5 max */
+	} dmawait;
+	struct {
+		xlnk_handle_t dmachan;
+	} dmarelease;
+	struct {
+		unsigned long base;
+		unsigned int size;
+		unsigned int irqs[8];
+		char name[32];
+		unsigned int id;
+	} devregister;
+	struct {
+		unsigned int base;
+	} devunregister;
+	struct {
+		char name[32];
+		unsigned int id;
+		unsigned long base;
+		unsigned int size;
+		unsigned int chan_num;
+		unsigned int chan0_dir;
+		unsigned int chan0_irq;
+		unsigned int chan0_poll_mode;
+		unsigned int chan0_include_dre;
+		unsigned int chan0_data_width;
+		unsigned int chan1_dir;
+		unsigned int chan1_irq;
+		unsigned int chan1_poll_mode;
+		unsigned int chan1_include_dre;
+		unsigned int chan1_data_width;
+	} dmaregister;
+	struct {
+		char name[32];
+		unsigned int id;
+		unsigned long base;
+		unsigned int size;
+		unsigned int mm2s_chan_num;
+		unsigned int mm2s_chan_irq;
+		unsigned int s2mm_chan_num;
+		unsigned int s2mm_chan_irq;
+	} mcdmaregister;
+	struct {
+		void *phys_addr;
+		int size;
+		int action;
+	} cachecontrol;
+} xlnk_args;
+
+
+#endif
-- 
1.7.5.4

