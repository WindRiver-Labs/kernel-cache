From 9f8b7b1d5fd459965b1e582cca8e3166de8b3510 Mon Sep 17 00:00:00 2001
From: Paul Gortmaker <paul.gortmaker@windriver.com>
Date: Mon, 24 Nov 2014 13:19:08 -0500
Subject: [PATCH] sched: Keep at least 1 tick per second for active dynticks
 tasks

Mainly commit 265f22a975c1e4cc3a4d1f94a3ec53ffbb6f5b9f upstream,
plus part of commit 38033c37faab850ed5d33bb675c4de6c66be84d8
("sched: Push down pre_schedule() and idle_balance()") upstream.

Original log of 265f22a975c1e4cc3a4d1f94a3ec53ffbb6f5b9f was:
 ---------------
   The scheduler doesn't yet fully support environments
   with a single task running without a periodic tick.

   In order to ensure we still maintain the duties of scheduler_tick(),
   keep at least 1 tick per second.

   This makes sure that we keep the progression of various scheduler
   accounting and background maintainance even with a very low granularity.
   Examples include cpu load, sched average, CFS entity vruntime,
   avenrun and events such as load balancing, amongst other details
   handled in sched_class::task_tick().

   This limitation will be removed in the future once we get
   these individual items to work in full dynticks CPUs.

   Suggested-by: Ingo Molnar <mingo@kernel.org>
   Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
   Cc: Christoph Lameter <cl@linux.com>
   Cc: Hakan Akkan <hakanakkan@gmail.com>
   Cc: Ingo Molnar <mingo@kernel.org>
   Cc: Kevin Hilman <khilman@linaro.org>
   Cc: Li Zhong <zhong@linux.vnet.ibm.com>
   Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
   Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
   Cc: Peter Zijlstra <peterz@infradead.org>
   Cc: Steven Rostedt <rostedt@goodmis.org>
   Cc: Thomas Gleixner <tglx@linutronix.de>
 ---------------

The part of 38033 is to move rq_last_tick_reset to put_prev_task_idle.

Originally,  the commit 265f22a975c1e4cc3a4d1f94a3ec53ffbb6f5b9f
("sched: Keep at least 1 tick per second for active dynticks tasks")
added the call of rq_last_tick_reset() to pre_schedule_idle().

But pre_schedule_idle() came from CFS scheduler commits that created the
pre_schedule_idle infrastructure used by the 1 tick NOHZ requirement.

However, since that time, we can see that mainline in 38033c37f
has relocated the call to put_prev_task_idle() and so we do the same
here.  This allows us to avoid the CFS changes which, given this
change are now no longer strictly necessary.

The conditional on CONFIG_SMP is added since the original location
in put_prev_task_idle was all wrapped in CONFIG_SMP.

Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 53afc2e14c29..1d9adea823b2 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -2103,6 +2103,7 @@ static inline void wake_up_nohz_cpu(int cpu) { }
 
 #ifdef CONFIG_NO_HZ_FULL
 extern bool sched_can_stop_tick(void);
+extern u64 scheduler_tick_max_deferment(void);
 #else
 static inline bool sched_can_stop_tick(void) { return false; }
 #endif
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 44fe7fad9dd5..40bec53b1167 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -2772,8 +2772,35 @@ void scheduler_tick(void)
 	rq->idle_balance = idle_cpu(cpu);
 	trigger_load_balance(rq, cpu);
 #endif
+	rq_last_tick_reset(rq);
 }
 
+#ifdef CONFIG_NO_HZ_FULL
+/**
+ * scheduler_tick_max_deferment
+ *
+ * Keep at least one tick per second when a single
+ * active task is running because the scheduler doesn't
+ * yet completely support full dynticks environment.
+ *
+ * This makes sure that uptime, CFS vruntime, load
+ * balancing, etc... continue to move forward, even
+ * with a very low granularity.
+ */
+u64 scheduler_tick_max_deferment(void)
+{
+	struct rq *rq = this_rq();
+	unsigned long next, now = ACCESS_ONCE(jiffies);
+
+	next = rq->last_sched_tick + HZ;
+
+	if (time_before_eq(next, now))
+		return 0;
+
+	return jiffies_to_usecs(next - now) * NSEC_PER_USEC;
+}
+#endif
+
 notrace unsigned long get_parent_ip(unsigned long addr)
 {
 	if (in_lock_functions(addr)) {
@@ -7111,6 +7138,9 @@ void __init sched_init(void)
 #ifdef CONFIG_NO_HZ_COMMON
 		rq->nohz_flags = 0;
 #endif
+#ifdef CONFIG_NO_HZ_FULL
+		rq->last_sched_tick = 0;
+#endif
 #endif
 		init_rq_hrtick(rq);
 		atomic_set(&rq->nr_iowait, 0);
diff --git a/kernel/sched/idle_task.c b/kernel/sched/idle_task.c
index fdf752275724..3b0632a06ab8 100644
--- a/kernel/sched/idle_task.c
+++ b/kernel/sched/idle_task.c
@@ -43,6 +43,9 @@ dequeue_task_idle(struct rq *rq, struct task_struct *p, int flags)
 
 static void put_prev_task_idle(struct rq *rq, struct task_struct *prev)
 {
+#ifdef CONFIG_SMP
+	rq_last_tick_reset(rq);
+#endif
 }
 
 static void task_tick_idle(struct rq *rq, struct task_struct *curr, int queued)
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 989ec75184f9..642c31fbf01a 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -363,6 +363,9 @@ struct rq {
 	u64 nohz_stamp;
 	unsigned long nohz_flags;
 #endif
+#ifdef CONFIG_NO_HZ_FULL
+	unsigned long last_sched_tick;
+#endif
 	int skip_clock_update;
 
 	/* capture load from *all* tasks on this cpu: */
@@ -955,6 +958,13 @@ static inline void dec_nr_running(struct rq *rq)
 	rq->nr_running--;
 }
 
+static inline void rq_last_tick_reset(struct rq *rq)
+{
+#ifdef CONFIG_NO_HZ_FULL
+	rq->last_sched_tick = jiffies;
+#endif
+}
+
 extern void update_rq_clock(struct rq *rq);
 
 extern void activate_task(struct rq *rq, struct task_struct *p, int flags);
diff --git a/kernel/time/tick-sched.c b/kernel/time/tick-sched.c
index c32bd210690f..05bcc37894d6 100644
--- a/kernel/time/tick-sched.c
+++ b/kernel/time/tick-sched.c
@@ -604,6 +604,13 @@ static ktime_t tick_nohz_stop_sched_tick(struct tick_sched *ts,
 			time_delta = KTIME_MAX;
 		}
 
+#ifdef CONFIG_NO_HZ_FULL
+		if (!ts->inidle) {
+			time_delta = min(time_delta,
+					 scheduler_tick_max_deferment());
+		}
+#endif
+
 		/*
 		 * calculate the expiry time for the next timer wheel
 		 * timer. delta_jiffies >= NEXT_TIMER_MAX_DELTA signals
-- 
1.8.2.3

