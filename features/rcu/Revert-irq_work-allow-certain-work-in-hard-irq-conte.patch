From 4b687e8ccfc36a5fe9b7ad14db5300fb4ef0d4a9 Mon Sep 17 00:00:00 2001
From: Paul Gortmaker <paul.gortmaker@windriver.com>
Date: Mon, 13 Jul 2015 16:46:02 -0400
Subject: [PATCH] Revert "irq_work: allow certain work in hard irq context"

This reverts the rt specific commit named above.  It originally
created an extra hirq_work_list for hardirqs.

The revert allows us to re-use a mainline commit that creates separate
lists w/o all the ifdeffery used here ; see also 3.18-rt patch
entitled "irq_work_Delegate_non-immediate_irq_work_to_ksoftirqd.patch"
which essentially reverts this change so that the mainline lists can
be used as-is.

We do keep the notion of IRQ_WORK_HARD_IRQ, since we'll want the nohz
kick code to retain that special status, so that part is not reverted.

Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/arch/arm/kernel/smp.c b/arch/arm/kernel/smp.c
index 686f1d1eb32e..8cd3724714fe 100644
--- a/arch/arm/kernel/smp.c
+++ b/arch/arm/kernel/smp.c
@@ -463,14 +463,12 @@ void arch_send_call_function_single_ipi(int cpu)
 }
 
 #ifdef CONFIG_IRQ_WORK
-#ifndef CONFIG_PREEMPT_RT_FULL
 void arch_irq_work_raise(void)
 {
 	if (is_smp())
 		smp_cross_call(cpumask_of(smp_processor_id()), IPI_IRQ_WORK);
 }
 #endif
-#endif
 
 static const char *ipi_types[NR_IPI] = {
 #define S(x,s)	[x] = s
diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
index 6d37d7603a8f..f8b994abb57f 100644
--- a/arch/powerpc/kernel/time.c
+++ b/arch/powerpc/kernel/time.c
@@ -423,7 +423,7 @@ unsigned long profile_pc(struct pt_regs *regs)
 EXPORT_SYMBOL(profile_pc);
 #endif
 
-#if defined(CONFIG_IRQ_WORK) && !defined(CONFIG_PREEMPT_RT_FULL)
+#ifdef CONFIG_IRQ_WORK
 
 /*
  * 64-bit uses a byte in the PACA, 32-bit uses a per-cpu variable...
diff --git a/arch/sparc/kernel/pcr.c b/arch/sparc/kernel/pcr.c
index 927d9c5e50f5..7e967c8018c8 100644
--- a/arch/sparc/kernel/pcr.c
+++ b/arch/sparc/kernel/pcr.c
@@ -43,12 +43,10 @@ void __irq_entry deferred_pcr_work_irq(int irq, struct pt_regs *regs)
 	set_irq_regs(old_regs);
 }
 
-#ifndef CONFIG_PREEMPT_RT_FULL
 void arch_irq_work_raise(void)
 {
 	set_softint(1 << PIL_DEFERRED_PCR_WORK);
 }
-#endif
 
 const struct pcr_ops *pcr_ops;
 EXPORT_SYMBOL_GPL(pcr_ops);
diff --git a/kernel/irq_work.c b/kernel/irq_work.c
index 51f345eb5bc1..a4fbf8b2f3c1 100644
--- a/kernel/irq_work.c
+++ b/kernel/irq_work.c
@@ -20,9 +20,6 @@
 
 
 static DEFINE_PER_CPU(struct llist_head, irq_work_list);
-#ifdef CONFIG_PREEMPT_RT_FULL
-static DEFINE_PER_CPU(struct llist_head, hirq_work_list);
-#endif
 static DEFINE_PER_CPU(int, irq_work_raised);
 
 /*
@@ -51,11 +48,7 @@ static bool irq_work_claim(struct irq_work *work)
 	return true;
 }
 
-#ifdef CONFIG_PREEMPT_RT_FULL
-void arch_irq_work_raise(void)
-#else
 void __weak arch_irq_work_raise(void)
-#endif
 {
 	/*
 	 * Lame architectures will get the timer tick callback
@@ -77,12 +70,8 @@ bool irq_work_queue(struct irq_work *work)
 	/* Queue the entry and raise the IPI if needed. */
 	preempt_disable();
 
-#ifdef CONFIG_PREEMPT_RT_FULL
-	if (work->flags & IRQ_WORK_HARD_IRQ)
-		llist_add(&work->llnode, &__get_cpu_var(hirq_work_list));
-	else
-#endif
-		llist_add(&work->llnode, &__get_cpu_var(irq_work_list));
+	llist_add(&work->llnode, &__get_cpu_var(irq_work_list));
+
 	/*
 	 * If the work is not "lazy" or the tick is stopped, raise the irq
 	 * work interrupt (if supported by the arch), otherwise, just wait
@@ -128,12 +117,7 @@ static void __irq_work_run(void)
 	__this_cpu_write(irq_work_raised, 0);
 	barrier();
 
-#ifdef CONFIG_PREEMPT_RT_FULL
-	if (in_irq())
-		this_list = &__get_cpu_var(hirq_work_list);
-	else
-#endif
-		this_list = &__get_cpu_var(irq_work_list);
+	this_list = &__get_cpu_var(irq_work_list);
 	if (llist_empty(this_list))
 		return;
 
diff --git a/kernel/timer.c b/kernel/timer.c
index 5c3174fe9687..5c3285292766 100644
--- a/kernel/timer.c
+++ b/kernel/timer.c
@@ -1452,7 +1452,7 @@ void update_process_times(int user_tick)
 	scheduler_tick();
 	run_local_timers();
 	rcu_check_callbacks(cpu, user_tick);
-#if defined(CONFIG_IRQ_WORK)
+#if defined(CONFIG_IRQ_WORK) && !defined(CONFIG_PREEMPT_RT_FULL)
 	if (in_irq())
 		irq_work_run();
 #endif
-- 
1.9.1

