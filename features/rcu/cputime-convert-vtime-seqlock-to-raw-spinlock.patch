From 84b48b957578178596d15d529ccd03644268799b Mon Sep 17 00:00:00 2001
From: Paul Gortmaker <paul.gortmaker@windriver.com>
Date: Thu, 16 May 2013 10:37:28 -0400
Subject: [PATCH] cputime: convert vtime seqlock to raw spinlock

Commit 6a61671bb2f3a1bd12cd17b8fca811a624782632
"cputime: Safely read cputime of full dynticks CPUs"
adds a seqlock to the task struct to lock the vtime
accounting data.

We know this:

A cherry pick of 6a61671bb won't compile w/o a partial revert
of the rt patch that deletes seqlock support:

   ---------------------------
   +#define read_seqbegin_irqsave(lock, flags)                             \
   +       ({ local_irq_save(flags);   read_seqbegin(lock); })
   +
   +#define read_seqretry_irqrestore(lock, iv, flags)                      \
   +       ({                                                              \
   +               int ret = read_seqretry(lock, iv);                      \
   +               local_irq_restore(flags);                               \
   +               ret;                                                    \
   +       })
   +
   -------------------------

We don't need a revert though; instead we cherry pick this, which
deletes the call sites of read irq variants of seq ops:

   -----------------------
   commit cdc4e86b58a95005ef500916b4a8e91a0037a822
   Author: Thomas Gleixner <tglx@linutronix.de>
   Date:   Fri Feb 15 23:47:07 2013 +0100

       cputime: Remove irqsave from seqlock readers
   -----------------------

However, both the partial revert, and the cherry pick above result
in a lockdep splat (and a hang upon trying to launch userspace).

Note the RT seqlock changes in the 3.4.x patch repository:

3.4-rt-patches/patches$ ls -1 *seq*
0010-seqlock-Remove-unused-functions.patch
0011-seqlock-Use-seqcount.patch
0230-seqlock-Prevent-rt-starvation.patch
0273-net-netfilter-Serialize-xt_write_recseq-sections-on-.patch

0010 and 0011 are in mainline now, and not rt specific.  If you
look at 0230, you see it has no way out if you've created a
deadlock condition (one lockdep is warning us about) and hence
the hang after "Freeing unused kernel memory".

The splat happens because in RT, the seqlock (implemented with
spinlocks) just becomes another non-raw preemptible spinlock.
But if you look at where we are calling the vtime accounting
functions, i.e. in atomic places like irq_exit() and similar,
we can't be using a normal spinlock there.

Lockdep shows us we are taking the seqlock in IN-HARDIRQ-W
state during irq_exit() processing:

  [<ffffffff810a0b97>] vtime_account_irq_exit+0x37/0x90
  [<ffffffff810760ba>] irq_exit+0x1a/0xc0
  [<ffffffff8103b9ce>] do_IRQ+0x5e/0xd0
  [<ffffffff816d0dac>] ret_from_intr+0x0/0x13

and then also taking the vtime lock in HARDIRQ-ON-W state:

 [<ffffffff810a0f87>] vtime_init_idle+0x27/0x70
 [<ffffffff816c1638>] init_idle+0x1e0/0x20c
 [<ffffffff816c1458>] ? migration_call+0x299/0x299
 [<ffffffff816c0259>] fork_idle+0xb8/0xc9
 [<ffffffff816bebf9>] do_fork_idle+0x14/0x25

Given the usage locations of the vtime accounting lock, we
convert it to a raw lock, on the assumption that the accounting
code is of short runs and shouldn't spin for long durations.
Ongoing testing will have to bear the responsibility of ensuring
that this does indeed seem valid.

 ------------ full lockdep splat --------------------

=================================
[ INFO: inconsistent lock state ]
3.4.34-ovp1-rt40-WR5.0.1.0_preempt-rt+ #1 Not tainted
---------------------------------
inconsistent {IN-HARDIRQ-W} -> {HARDIRQ-ON-W} usage.
kworker/0:0/4 [HC0[0]:SC0[0]:HE1:SE1] takes:
 (&(&(&(&p->vtime_seqlock)->lock)->lock)->wait_lock){?.....}, at: [<ffffffff816cf973>] rt_spin_lock_slowlock+0x43/0x3a0
{IN-HARDIRQ-W} state was registered at:
  [<ffffffff810bb0df>] __lock_acquire+0x72f/0x1d60
  [<ffffffff810bcc6f>] lock_acquire+0x9f/0xd0
  [<ffffffff816d0746>] _raw_spin_lock+0x36/0x50
  [<ffffffff816cf973>] rt_spin_lock_slowlock+0x43/0x3a0
  [<ffffffff816d0127>] rt_spin_lock+0x27/0x60
  [<ffffffff810a0b97>] vtime_account_irq_exit+0x37/0x90
  [<ffffffff810760ba>] irq_exit+0x1a/0xc0
  [<ffffffff8103b9ce>] do_IRQ+0x5e/0xd0
  [<ffffffff816d0dac>] ret_from_intr+0x0/0x13
  [<ffffffff81059d56>] ioapic_read_entry+0x46/0x60
  [<ffffffff81059dce>] save_ioapic_entries+0x5e/0xb0
  [<ffffffff819d2920>] enable_IR_x2apic+0x34/0x202
  [<ffffffff819d48b3>] default_setup_apic_routing+0x12/0x78
  [<ffffffff819d07a7>] native_smp_prepare_cpus+0x39f/0x3e6
  [<ffffffff819c39d5>] kernel_init+0x5f/0x1c7
  [<ffffffff816d2c14>] kernel_thread_helper+0x4/0x10
irq event stamp: 209
hardirqs last  enabled at (209): [<ffffffff816d0a65>] _raw_spin_unlock_irqrestore+0x65/0x80
hardirqs last disabled at (208): [<ffffffff816d0837>] _raw_spin_lock_irqsave+0x17/0x60
softirqs last  enabled at (0): [<ffffffff8106c5d5>] copy_process.part.41+0x585/0x14c0
softirqs last disabled at (0): [<          (null)>]           (null)

other info that might help us debug this:
 Possible unsafe locking scenario:

       CPU0
       ----
  lock(&(&(&(&p->vtime_seqlock)->lock)->lock)->wait_lock);
  <Interrupt>
    lock(&(&(&(&p->vtime_seqlock)->lock)->lock)->wait_lock);

 *** DEADLOCK ***

2 locks held by kworker/0:0/4:
 #0:  (events){.+.+.+}, at: [<ffffffff8108a2bb>] process_one_work+0x12b/0x4c0
 #1:  ((&c_idle.work)){+.+.+.}, at: [<ffffffff8108a2bb>] process_one_work+0x12b/0x4c0

stack backtrace:
Pid: 4, comm: kworker/0:0 Not tainted 3.4.34-ovp1-rt40-WR5.0.1.0_preempt-rt+ #1
Call Trace:
 [<ffffffff816c5dbe>] print_usage_bug.part.36+0x26b/0x27a
 [<ffffffff81047c1a>] ? save_stack_trace+0x2a/0x50
 [<ffffffff810b8160>] ? print_shortest_lock_dependencies+0x1c0/0x1c0
 [<ffffffff810b8f45>] mark_lock+0x285/0x610
 [<ffffffff810bb04f>] __lock_acquire+0x69f/0x1d60
 [<ffffffff81047c1a>] ? save_stack_trace+0x2a/0x50
 [<ffffffff810bad7d>] ? __lock_acquire+0x3cd/0x1d60
 [<ffffffff810bad7d>] ? __lock_acquire+0x3cd/0x1d60
 [<ffffffff810bcc6f>] lock_acquire+0x9f/0xd0
 [<ffffffff816cf973>] ? rt_spin_lock_slowlock+0x43/0x3a0
 [<ffffffff816d0746>] _raw_spin_lock+0x36/0x50
 [<ffffffff816cf973>] ? rt_spin_lock_slowlock+0x43/0x3a0
 [<ffffffff816cf973>] rt_spin_lock_slowlock+0x43/0x3a0
 [<ffffffff816cfd68>] ? rt_spin_lock_slowunlock+0x18/0x7a
 [<ffffffff816d0127>] rt_spin_lock+0x27/0x60
 [<ffffffff8109c061>] ? migrate_disable+0x71/0xa0
 [<ffffffff810a0f87>] vtime_init_idle+0x27/0x70
 [<ffffffff816c1638>] init_idle+0x1e0/0x20c
 [<ffffffff816c1458>] ? migration_call+0x299/0x299
 [<ffffffff816c0259>] fork_idle+0xb8/0xc9
 [<ffffffff816bebf9>] do_fork_idle+0x14/0x25
 [<ffffffff8108a323>] process_one_work+0x193/0x4c0
 [<ffffffff8108a2bb>] ? process_one_work+0x12b/0x4c0
 [<ffffffff8109e452>] ? wake_up_process+0x22/0x40
 [<ffffffff816bebe5>] ? do_boot_cpu+0x69b/0x69b
 [<ffffffff8108ad51>] worker_thread+0x141/0x2f0
 [<ffffffff8108ac10>] ? manage_workers.isra.25+0x220/0x220
 [<ffffffff8109065f>] kthread+0xdf/0xf0
 [<ffffffff816d0ab6>] ? _raw_spin_unlock_irq+0x36/0x60
 [<ffffffff8109a2f8>] ? finish_task_switch+0x88/0x130
 [<ffffffff8109a2bb>] ? finish_task_switch+0x4b/0x130
 [<ffffffff816d2c14>] kernel_thread_helper+0x4/0x10
 [<ffffffff8109a2f8>] ? finish_task_switch+0x88/0x130
 [<ffffffff816d0e5d>] ? retint_restore_args+0xe/0xe
 [<ffffffff81090580>] ? __init_kthread_worker+0xa0/0xa0
 [<ffffffff816d2c10>] ? gs_change+0xb/0xb

Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index c77a16d..8baa483 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -152,7 +152,7 @@ extern struct task_group root_task_group;
 
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
 # define INIT_VTIME(tsk)						\
-	.vtime_seqlock = __SEQLOCK_UNLOCKED(tsk.vtime_seqlock),	\
+	.vtime_lock = __RAW_SPIN_LOCK_UNLOCKED(tsk.vtime_lock),		\
 	.vtime_snap = 0,				\
 	.vtime_snap_whence = VTIME_SYS,
 #else
diff --git a/include/linux/sched.h b/include/linux/sched.h
index c3eb182..507b691 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1410,7 +1410,7 @@ struct task_struct {
 	struct cputime prev_cputime;
 #endif
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
-	seqlock_t vtime_seqlock;
+	raw_spinlock_t vtime_lock;
 	unsigned long long vtime_snap;
 	enum {
 		VTIME_SLEEPING = 0,
diff --git a/kernel/fork.c b/kernel/fork.c
index bfd4397..8e25e1f 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1241,7 +1241,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	p->prev_cputime.utime = p->prev_cputime.stime = 0;
 #endif
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
-	seqlock_init(&p->vtime_seqlock);
+	raw_spin_lock_init(&p->vtime_lock);
 	p->vtime_snap = 0;
 	p->vtime_snap_whence = VTIME_SLEEPING;
 #endif
diff --git a/kernel/sched/cputime.c b/kernel/sched/cputime.c
index cf5609a..a45e354 100644
--- a/kernel/sched/cputime.c
+++ b/kernel/sched/cputime.c
@@ -684,9 +684,9 @@ void vtime_account_system(struct task_struct *tsk)
 	if (!vtime_accounting_enabled())
 		return;
 
-	write_seqlock(&tsk->vtime_seqlock);
+	raw_spin_lock(&tsk->vtime_lock);
 	__vtime_account_system(tsk);
-	write_sequnlock(&tsk->vtime_seqlock);
+	raw_spin_unlock(&tsk->vtime_lock);
 }
 
 void vtime_account_irq_exit(struct task_struct *tsk)
@@ -694,11 +694,11 @@ void vtime_account_irq_exit(struct task_struct *tsk)
 	if (!vtime_accounting_enabled())
 		return;
 
-	write_seqlock(&tsk->vtime_seqlock);
+	raw_spin_lock(&tsk->vtime_lock);
 	if (context_tracking_in_user())
 		tsk->vtime_snap_whence = VTIME_USER;
 	__vtime_account_system(tsk);
-	write_sequnlock(&tsk->vtime_seqlock);
+	raw_spin_unlock(&tsk->vtime_lock);
 }
 
 void vtime_account_user(struct task_struct *tsk)
@@ -710,10 +710,10 @@ void vtime_account_user(struct task_struct *tsk)
 
 	delta_cpu = get_vtime_delta(tsk);
 
-	write_seqlock(&tsk->vtime_seqlock);
+	raw_spin_lock(&tsk->vtime_lock);
 	tsk->vtime_snap_whence = VTIME_SYS;
 	account_user_time(tsk, delta_cpu, cputime_to_scaled(delta_cpu));
-	write_sequnlock(&tsk->vtime_seqlock);
+	raw_spin_unlock(&tsk->vtime_lock);
 }
 
 void vtime_user_enter(struct task_struct *tsk)
@@ -721,26 +721,26 @@ void vtime_user_enter(struct task_struct *tsk)
 	if (!vtime_accounting_enabled())
 		return;
 
-	write_seqlock(&tsk->vtime_seqlock);
+	raw_spin_lock(&tsk->vtime_lock);
 	tsk->vtime_snap_whence = VTIME_USER;
 	__vtime_account_system(tsk);
-	write_sequnlock(&tsk->vtime_seqlock);
+	raw_spin_unlock(&tsk->vtime_lock);
 }
 
 void vtime_guest_enter(struct task_struct *tsk)
 {
-	write_seqlock(&tsk->vtime_seqlock);
+	raw_spin_lock(&tsk->vtime_lock);
 	__vtime_account_system(tsk);
 	current->flags |= PF_VCPU;
-	write_sequnlock(&tsk->vtime_seqlock);
+	raw_spin_unlock(&tsk->vtime_lock);
 }
 
 void vtime_guest_exit(struct task_struct *tsk)
 {
-	write_seqlock(&tsk->vtime_seqlock);
+	raw_spin_lock(&tsk->vtime_lock);
 	__vtime_account_system(tsk);
 	current->flags &= ~PF_VCPU;
-	write_sequnlock(&tsk->vtime_seqlock);
+	raw_spin_unlock(&tsk->vtime_lock);
 }
 
 void vtime_account_idle(struct task_struct *tsk)
@@ -757,39 +757,39 @@ bool vtime_accounting_enabled(void)
 
 void arch_vtime_task_switch(struct task_struct *prev)
 {
-	write_seqlock(&prev->vtime_seqlock);
+	raw_spin_lock(&prev->vtime_lock);
 	prev->vtime_snap_whence = VTIME_SLEEPING;
-	write_sequnlock(&prev->vtime_seqlock);
+	raw_spin_unlock(&prev->vtime_lock);
 
-	write_seqlock(&current->vtime_seqlock);
+	raw_spin_lock(&current->vtime_lock);
 	current->vtime_snap_whence = VTIME_SYS;
 	current->vtime_snap = sched_clock_cpu(smp_processor_id());
-	write_sequnlock(&current->vtime_seqlock);
+	raw_spin_unlock(&current->vtime_lock);
 }
 
 void vtime_init_idle(struct task_struct *t, int cpu)
 {
 	unsigned long flags;
 
-	write_seqlock_irqsave(&t->vtime_seqlock, flags);
+	raw_spin_lock_irqsave(&t->vtime_lock, flags);
 	t->vtime_snap_whence = VTIME_SYS;
 	t->vtime_snap = sched_clock_cpu(cpu);
-	write_sequnlock_irqrestore(&t->vtime_seqlock, flags);
+	raw_spin_unlock_irqrestore(&t->vtime_lock, flags);
 }
 
 cputime_t task_gtime(struct task_struct *t)
 {
-	unsigned int seq;
 	cputime_t gtime;
 
-	do {
-		seq = read_seqbegin(&t->vtime_seqlock);
+	{
+		raw_spin_lock(&t->vtime_lock);
 
 		gtime = t->gtime;
 		if (t->flags & PF_VCPU)
 			gtime += vtime_delta(t);
 
-	} while (read_seqretry(&t->vtime_seqlock, seq));
+		raw_spin_unlock(&t->vtime_lock);
+	}
 
 	return gtime;
 }
@@ -805,14 +805,13 @@ fetch_task_cputime(struct task_struct *t,
 		   cputime_t *u_src, cputime_t *s_src,
 		   cputime_t *udelta, cputime_t *sdelta)
 {
-	unsigned int seq;
 	unsigned long long delta;
 
-	do {
+	{
 		*udelta = 0;
 		*sdelta = 0;
 
-		seq = read_seqbegin(&t->vtime_seqlock);
+		raw_spin_lock(&t->vtime_lock);
 
 		if (u_dst)
 			*u_dst = *u_src;
@@ -822,7 +821,7 @@ fetch_task_cputime(struct task_struct *t,
 		/* Task is sleeping, nothing to add */
 		if (t->vtime_snap_whence == VTIME_SLEEPING ||
 		    is_idle_task(t))
-			continue;
+			goto done;
 
 		delta = vtime_delta(t);
 
@@ -836,7 +835,9 @@ fetch_task_cputime(struct task_struct *t,
 			if (t->vtime_snap_whence == VTIME_SYS)
 				*sdelta = delta;
 		}
-	} while (read_seqretry(&t->vtime_seqlock, seq));
+done:
+		raw_spin_unlock(&t->vtime_lock);
+	}
 }
 
 
-- 
1.8.3.1

