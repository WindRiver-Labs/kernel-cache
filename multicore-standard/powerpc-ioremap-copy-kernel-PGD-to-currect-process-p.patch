From be93d93ee7569d9a2045258dfe0fc74a2fa75f29 Mon Sep 17 00:00:00 2001
From: Yang Shi <yang.shi@windriver.com>
Date: Sun, 13 Dec 2009 22:02:19 -0800
Subject: [PATCH] powerpc: ioremap: copy kernel PGD to currect process page table

For the powerpc guest OS, the kernel page table is setup statically
by ioremap, vmalloc, etc. This means that the Hypervisor can get
guest OS kernel address mapping in current process VMMU table if a
kernel address TLB error occurs and is handled by the hypervisor
itself, and never passed to the guest OS.

Because MAS registers and TLB instructions are privileged, the
TLB error handler calls do_page_fault directly. This copies the
init process kernel page table to current process when context
switching to make sure process has the correct kernel
page table.

But, if there is no context switching AND the kernel changes kernel
page table via ioremap, the current process will get the incorrect
kernel page table. For example, a kernel module map a memory area
via ioremap, then access this area, the current process will have
the incorrect kernel page table if there hasn't been a context switch.
To solve this, we need copy kernel PGD (share the L2 PTEs) to
current process page table.

Signed-off-by: Yang Shi <yang.shi@windriver.com>
---
 arch/powerpc/include/asm/paravirt.h |    2 +
 arch/powerpc/kernel/paravirt.c      |    5 ++
 arch/powerpc/kernel/vbi/wrhv.c      |  114 +++++++++++++++++++++++++++++++++++
 arch/powerpc/mm/pgtable_32.c        |   12 ++++-
 4 files changed, 132 insertions(+), 1 deletions(-)

diff --git a/arch/powerpc/include/asm/paravirt.h b/arch/powerpc/include/asm/paravirt.h
index 91a8318..caca81c 100644
--- a/arch/powerpc/include/asm/paravirt.h
+++ b/arch/powerpc/include/asm/paravirt.h
@@ -37,6 +37,7 @@ extern int paravirt_kgdb_arch_handle_exception(int vector, int signo,
 extern int __init native_early_init_dt_scan_memory(unsigned long node,
 				const char *uname, int depth, void *data);
 extern void __init native_time_init_cont(void);
+extern void __iomem* native___ioremap(phys_addr_t addr, unsigned long size, unsigned long flags);
 
 /*
  * paravirtual operations structure
@@ -95,6 +96,7 @@ struct pv_mmu_ops {
 	int (*map_page)(unsigned long va, phys_addr_t pa, int flags);
 	int (*early_init_dt_scan_memory)(unsigned long node,
 			const char *uname, int depth, void *data);
+	void (*__ioremap)(phys_addr_t, unsigned long size, unsigned long flags);
 
 };
 
diff --git a/arch/powerpc/kernel/paravirt.c b/arch/powerpc/kernel/paravirt.c
index 47b3eff..888418e 100644
--- a/arch/powerpc/kernel/paravirt.c
+++ b/arch/powerpc/kernel/paravirt.c
@@ -142,6 +142,7 @@ struct pv_mmu_ops pv_mmu_ops = {
 	.map_page = native_map_page,
 	.early_init_dt_scan_memory =
 		native_early_init_dt_scan_memory,
+	.__ioremap = native___ioremap,
 };
 
 
@@ -254,6 +255,10 @@ int paravirt_early_init_dt_scan_memory(unsigned long node,
 					uname, depth, data);
 }
 
+void paravirt___ioremap(phys_addr_t addr, unsigned long size, unsigned long flags)
+{
+	pv_mmu_ops.__ioremap(addr, size, flags);
+}
 inline int paravirt_enabled(void)
 {
         return pv_info.paravirt_enabled;
diff --git a/arch/powerpc/kernel/vbi/wrhv.c b/arch/powerpc/kernel/vbi/wrhv.c
index 2f8739b..f489f65 100644
--- a/arch/powerpc/kernel/vbi/wrhv.c
+++ b/arch/powerpc/kernel/vbi/wrhv.c
@@ -18,6 +18,7 @@
 #include <linux/profile.h>
 #include <linux/wrhv.h>
 #include <linux/interrupt.h>
+#include <linux/vmalloc.h>
 #include <vbi/interface.h>
 #include <vbi/interrupt.h>
 #include <vbi/errors.h>
@@ -63,6 +64,7 @@
 #include <asm/btext.h>
 #include <asm/tlb.h>
 #include <asm/sections.h>
+#include <asm/pgtable.h>
 
 #include <linux/sched.h>
 #include <linux/kernel.h>
@@ -134,6 +136,9 @@ extern int vb_context_mmu_on(int pid,  /* context id */
 extern atomic_t hwtimer_cpu_trigger[NR_CPUS];
 extern struct hwtimer powerpc_timer;
 
+#define p_mapped_by_bats(x)     (0UL)
+#define p_mapped_by_tlbcam(x)   (0UL)
+
 unsigned long wrhv_cpu_freq = 0;
 
 int wrhv_pci_devfn = -1;
@@ -866,6 +871,114 @@ int wrhv_map_page(unsigned long va, phys_addr_t pa, int flags)
 	return err;
 }
 
+void __iomem *
+wrhv___ioremap(phys_addr_t addr, unsigned long size, unsigned long flags)
+{
+	unsigned long v, i;
+	phys_addr_t p;
+	int err;
+
+	/* Make sure we have the base flags */
+	if ((flags & _PAGE_PRESENT) == 0)
+		flags |= _PAGE_KERNEL;
+
+	/* Non-cacheable page cannot be coherent */
+	if (flags & _PAGE_NO_CACHE)
+		flags &= ~_PAGE_COHERENT;
+
+	/*
+	 * Choose an address to map it to.
+	 * Once the vmalloc system is running, we use it.
+	 * Before then, we use space going down from ioremap_base
+	 * (ioremap_bot records where we're up to).
+	 */
+	p = addr & PAGE_MASK;
+	size = PAGE_ALIGN(addr + size) - p;
+
+	/*
+	 * If the address lies within the first 16 MB, assume it's in ISA
+	 * memory space
+	 */
+	if (p < 16*1024*1024)
+		p += _ISA_MEM_BASE;
+
+#ifndef CONFIG_CRASH_DUMP
+	/*
+	 * Don't allow anybody to remap normal RAM that we're using.
+	 * mem_init() sets high_memory so only do the check after that.
+	 */
+	if (mem_init_done && (p < virt_to_phys(high_memory))) {
+		printk("__ioremap(): phys addr 0x%llx is RAM lr %p\n",
+		       (unsigned long long)p, __builtin_return_address(0));
+		return NULL;
+	}
+#endif
+
+	if (size == 0)
+		return NULL;
+
+	/*
+	 * Is it already mapped?  Perhaps overlapped by a previous
+	 * BAT mapping.  If the whole area is mapped then we're done,
+	 * otherwise remap it since we want to keep the virt addrs for
+	 * each request contiguous.
+	 *
+	 * We make the assumption here that if the bottom and top
+	 * of the range we want are mapped then it's mapped to the
+	 * same virt address (and this is contiguous).
+	 *  -- Cort
+	 */
+	if ((v = p_mapped_by_bats(p)) /*&& p_mapped_by_bats(p+size-1)*/ )
+		goto out;
+
+	if ((v = p_mapped_by_tlbcam(p)))
+		goto out;
+
+	if (mem_init_done) {
+		struct vm_struct *area;
+		area = get_vm_area(size, VM_IOREMAP);
+		if (area == 0)
+			return NULL;
+		v = (unsigned long) area->addr;
+	} else {
+		v = (ioremap_bot -= size);
+	}
+
+	/*
+	 * Should check if it is a candidate for a BAT mapping
+	 */
+
+	err = 0;
+	for (i = 0; i < size && err == 0; i += PAGE_SIZE)
+		err = map_page(v+i, p+i, flags);
+	if (err) {
+		if (mem_init_done)
+			vunmap((void *)v);
+		return NULL;
+	}
+
+/* Just E500 Guest OS need copy kernel PTEs in ioremap.
+ * And, don't support 36 bit physical address now.
+ */
+#if !defined(CONFIG_PPC85xx_VT_MODE) && !defined(CONFIG_PHYS_64BIT)
+        pgd_t *kpd_start, *kpd_end, *upd_start, *pgd;
+        if (mem_init_done && (current->mm != NULL) && (current->mm != &init_mm)) {
+                pgd = current->mm->pgd;
+
+        	/* we attach (copy) kernel page mapping to the user page table
+		 * Note, we only copy the L1 entrys to user L1 pageTable,
+		 * then letting L1 share the same L2 page table.
+		 */
+                kpd_start = pgd_offset_k(KERNELBASE);
+                kpd_end =   pgd_offset_k(0xffffffff);
+
+                upd_start = pgd + pgd_index(KERNELBASE);
+                memcpy(upd_start, kpd_start, (kpd_end - kpd_start + 1) * sizeof (pgd_t));
+        }
+#endif
+out:
+	return (void __iomem *) (v + ((unsigned long)addr & ~PAGE_MASK));
+}
 
 /* arch/powerpc/kernel/traps.c */
 void __kprobes wrhv_DebugException(struct pt_regs *regs, unsigned long debug_status)
@@ -999,6 +1112,7 @@ void wrhv_init(void)
 	pv_mmu_ops.map_page = wrhv_map_page;
 	pv_mmu_ops.early_init_dt_scan_memory =
 		wrhv_early_init_dt_scan_memory;
+	pv_mmu_ops.__ioremap = wrhv___ioremap;
 
 }
 
diff --git a/arch/powerpc/mm/pgtable_32.c b/arch/powerpc/mm/pgtable_32.c
index 4fd6753..c9f0f3e 100644
--- a/arch/powerpc/mm/pgtable_32.c
+++ b/arch/powerpc/mm/pgtable_32.c
@@ -255,8 +255,13 @@ ioremap_flags(phys_addr_t addr, unsigned long size, unsigned long flags)
 }
 EXPORT_SYMBOL(ioremap_flags);
 
+void __iomem * paravirt___ioremap(phys_addr_t addr, unsigned long size, unsigned long flags)
+               __attribute__((weak, alias("native___ioremap")));
+
+/* Native __ioremap implementation. For paravirt version please
+refer to arch/powerpc/kernel/vbi/wrhv.c */
 void __iomem *
-__ioremap(phys_addr_t addr, unsigned long size, unsigned long flags)
+native___ioremap(phys_addr_t addr, unsigned long size, unsigned long flags)
 {
 	unsigned long v, i;
 	phys_addr_t p;
@@ -344,6 +349,11 @@ __ioremap(phys_addr_t addr, unsigned long size, unsigned long flags)
 out:
 	return (void __iomem *) (v + ((unsigned long)addr & ~PAGE_MASK));
 }
+
+void __iomem *
+__ioremap(phys_addr_t addr, unsigned long size, unsigned long flags) {
+	return paravirt___ioremap(addr, size, flags);
+}
 EXPORT_SYMBOL(__ioremap);
 
 void iounmap(volatile void __iomem *addr)
-- 
1.6.5.2

