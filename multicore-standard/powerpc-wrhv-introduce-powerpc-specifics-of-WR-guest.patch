From e93d90025cecad31753cef67acdb0f0913ba9f07 Mon Sep 17 00:00:00 2001
From: WRS Support <support@windriver.com>
Date: Fri, 2 Oct 2009 16:11:25 -0400
Subject: [PATCH] powerpc wrhv: introduce powerpc specifics of WR guest

These powerpc specific additons are the Linux specific
additions that were not a part of the reference VBI
implemenation.

Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
Signed-off-by: Bruce Ashfield <bruce.ashfield@windriver.com>
---
 arch/powerpc/include/asm/reg_wrhv.h    |  141 +++++
 arch/powerpc/include/asm/wrhv.h        |   31 +
 arch/powerpc/kernel/head_wrhv.S        |  890 +++++++++++++++++++++++++++++
 arch/powerpc/kernel/head_wrhv.h        |  146 +++++
 arch/powerpc/kernel/vbi/util.c         |  142 +++++
 arch/powerpc/kernel/vbi/vmmu_display.c |  140 +++++
 arch/powerpc/kernel/vbi/wrhv.c         |  963 ++++++++++++++++++++++++++++++++
 arch/powerpc/kernel/wrhv_entry_32.S    |  500 +++++++++++++++++
 arch/powerpc/kernel/wrhv_misc_32.S     |   90 +++
 9 files changed, 3043 insertions(+), 0 deletions(-)
 create mode 100644 arch/powerpc/include/asm/reg_wrhv.h
 create mode 100644 arch/powerpc/include/asm/wrhv.h
 create mode 100644 arch/powerpc/kernel/head_wrhv.S
 create mode 100644 arch/powerpc/kernel/head_wrhv.h
 create mode 100644 arch/powerpc/kernel/vbi/util.c
 create mode 100644 arch/powerpc/kernel/vbi/vmmu_display.c
 create mode 100644 arch/powerpc/kernel/vbi/wrhv.c
 create mode 100644 arch/powerpc/kernel/wrhv_entry_32.S
 create mode 100644 arch/powerpc/kernel/wrhv_misc_32.S

diff --git a/arch/powerpc/include/asm/reg_wrhv.h b/arch/powerpc/include/asm/reg_wrhv.h
new file mode 100644
index 0000000..77f4514
--- /dev/null
+++ b/arch/powerpc/include/asm/reg_wrhv.h
@@ -0,0 +1,141 @@
+/*
+ *  This program is free software; you can redistribute it and/or modify it
+ *  under the terms of the GNU General Public License as published by the
+ *  Free Software Foundation; either version 2, or (at your option) any
+ *  later version.
+ *
+ *  This program is distributed in the hope that it will be useful, but
+ *  WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ *  General Public License for more details.
+ *
+ *  Copyright (C) 2009 Wind River Systems, Inc.
+ *
+ * Contains the definition of registers common to all PowerPC variants.
+ * If a register definition has been changed in a different PowerPC
+ * variant, we will case it in #ifndef XXX ... #endif, and have the
+ * number used in the Programming Environments Manual For 32-Bit
+ * Implementations of the PowerPC Architecture (a.k.a. Green Book) here.
+ */
+
+#ifndef _ASM_POWERPC_REG_WRHV_H
+#define _ASM_POWERPC_REG_WRHV_H
+#ifdef __KERNEL__
+
+#include <linux/stringify.h>
+#include <asm/cputable.h>
+#include <vbi/vbInterface.h>
+
+/* macro used in on entry_32.S */
+#define PARAVIRT_ENABLE_MSR_EE      WRHV_INT_UNLOCK(r10,r11)
+#define PARAVIRT_DISABLE_MSR_EE     WRHV_INT_LOCK(r10,r3)
+
+/* macro used in misc.S */
+#undef PARAVIRT_MFSPR_SPRG3
+#define PARAVIRT_MFSPR_SPRG3(a)  WRHV_MFSPRG3(a)
+
+/* macro used in arch/powerpc/kernel/traps.c */
+#define PARAVIRT_DISABLE_INST_COMPLETION       do{ } while (0)
+#define PARAVIRT_CLEAR_INST_COMPLETION                 do{ } while (0)
+
+#ifdef __ASSEMBLY__
+.extern	var(wrhv_sprg3)
+.extern	var(wrhv_user)
+.extern var(wrhv_pir)
+
+#define WRHV_MFSPRG3(rd)                        \
+	lis	rd,wrhv_sprg3@ha;               \
+	lwz	rd,wrhv_sprg3@l(rd)
+
+#define WRHV_MTSPRG3(rs,tmpr)                   \
+	lis	tmpr,wrhv_sprg3@ha;             \
+	stw	rs,wrhv_sprg3@l(tmpr)
+
+#ifdef CONFIG_SMP
+#define WRHV_MFPIR(rd)				\
+	lis	rd,wrhv_pir@ha;		\
+	lwz	rd,wrhv_pir@l(rd);
+#endif
+
+#define WRHV_INT_LOCK(tmpr1,tmpr2)                      \
+	li	tmpr2,-1;                               \
+	lis	tmpr1,wr_control@ha;                   \
+	lwz	tmpr1,wr_control@l(tmpr1);             \
+	stw	tmpr2,VB_CONTROL_INT_DISABLE(tmpr1)
+
+#define WRHV_INT_UNLOCK(tmpr1,tmpr2)                    \
+	lis	tmpr1,wr_control@ha;                   \
+	lwz	tmpr1,wr_control@l(tmpr1);             \
+	li	tmpr2,0;                                \
+	stw	tmpr2,VB_CONTROL_INT_DISABLE(tmpr1);    \
+	lis	tmpr1,wr_status@ha;                    \
+	lwz	tmpr1,wr_status@l(tmpr1);              \
+	lwz	tmpr1,VB_STATUS_INT_PENDING(tmpr1);     \
+	cmpwi	0,tmpr1,0;                              \
+	beq	1f;                                     \
+	mr	tmpr1,r0;                               \
+	lis	r0,VBI_SYS_int_enable@h;                \
+	ori	r0,r0,VBI_SYS_int_enable@l;             \
+	sc;                                             \
+	mr	r0,tmpr1;                               \
+1:
+
+#define WRHV_INT_LVL_GET(rd)                            \
+	lis	rd,wr_control@ha;                      \
+	lwz	rd,wr_control@l(rd);                   \
+	lwz	rd,VB_CONTROL_INT_DISABLE(rd)
+
+#define WRHV_FIX_MSR(msr,tmpr)                                  \
+	rlwinm	msr,msr,0,18,15; /* Clear EE & PR bits */       \
+	WRHV_INT_LVL_GET(tmpr);                         \
+	cmpwi	0,tmpr,0;                                       \
+	bne	1f;                                             \
+	ori	msr,msr,MSR_EE;                                 \
+1:	lis	tmpr,wrhv_supervisor@ha;                        \
+	lwz	tmpr,wrhv_supervisor@l(tmpr);                   \
+	cmpwi	0,tmpr,0;                                       \
+	bne	2f;                                             \
+	ori	msr,msr,MSR_PR;                                 \
+2:
+
+#define WRHV_LOAD_MSR(msr,tmpr1,tmpr2)                          \
+	li	tmpr2,0;                                        \
+	rlwinm.	tmpr1,msr,0,16,16;      /* test EE bit */       \
+	bne	1f;                     /* IT unlocked? */      \
+	li	tmpr2,-1;                                       \
+1:	lis	tmpr1,wr_control@ha;                           \
+	lwz	tmpr1,wr_control@l(tmpr1);                     \
+	stw	tmpr2,VB_CONTROL_NEW_INT_DISABLE(tmpr1);        \
+	stw	msr,VB_CONTROL_SRR1(tmpr1);                     \
+	li	tmpr2,1;                                        \
+	rlwinm.	tmpr1,msr,0,17,17;      /* test PR bit */       \
+	beq	2f;                     /* priv. mode? */       \
+	li	tmpr2,0;                                        \
+2:	WRHV_SET_SUP_MODE(tmpr1,tmpr2)
+
+#define WRHV_FIX_MSR2(msr,tmpr)                         \
+	rlwinm	msr,msr,0,18,15; /* Clear EE & PR bits */       \
+	lis	tmpr,wr_status@ha;                             \
+	lwz	tmpr,wr_status@l(tmpr);                        \
+	lwz	tmpr,VB_STATUS_OLD_INT_DISABLE(tmpr);           \
+	cmpwi	0,tmpr,0;                                       \
+	bne	1f;                                             \
+	ori	msr,msr,MSR_EE;                                 \
+1:	lis	tmpr,wrhv_supervisor@ha;                        \
+	lwz	tmpr,wrhv_supervisor@l(tmpr);                   \
+	cmpwi	0,tmpr,0;                                       \
+	bne	2f;                                             \
+	ori	msr,msr,MSR_PR;                                 \
+2:
+
+#define WRHV_SET_SUP_MODE(tmpr,rs)                              \
+	lis	tmpr,wrhv_supervisor@ha;                        \
+	stw	rs,wrhv_supervisor@l(tmpr)
+
+#define WRHV_SUP_MODE_GET(rd)                                   \
+	lis	rd,wrhv_supervisor@ha;                          \
+	lwz	rd,wrhv_supervisor@l(rd)
+#endif /* __ASSEMBLY__ */
+
+#endif /* __KERNEL__ */
+#endif /* _ASM_POWERPC_REG_WRHV_H */
diff --git a/arch/powerpc/include/asm/wrhv.h b/arch/powerpc/include/asm/wrhv.h
new file mode 100644
index 0000000..84a4881
--- /dev/null
+++ b/arch/powerpc/include/asm/wrhv.h
@@ -0,0 +1,31 @@
+/*
+ *  This program is free software; you can redistribute it and/or modify it
+ *  under the terms of the GNU General Public License as published by the
+ *  Free Software Foundation; either version 2, or (at your option) any
+ *  later version.
+ *
+ *  This program is distributed in the hope that it will be useful, but
+ *  WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ *  General Public License for more details.
+ *
+ *  Copyright (C) 2008 Wind River Systems, Inc.
+ */
+
+#ifndef __ASM_WRHV_H
+#define __ASM_WRHV_H
+
+#ifdef CONFIG_WRHV
+extern void wrhv_mapping(void);
+extern void wrhv_restart(void);
+extern unsigned long __init wrhv_find_end_of_memory(void);
+extern void wrhv_power_save(void);
+extern unsigned int wrhv_vioapic_get_irq(void);
+extern void wrhv_init_irq(void);
+extern void __init wrhv_calibrate_decr(void);
+extern void __init wrhv_time_init(void);
+
+extern unsigned long wrhv_cpu_freq;
+
+#endif /* CONFIG_WRHV */
+#endif /* __ASM_WRHV_H */
diff --git a/arch/powerpc/kernel/head_wrhv.S b/arch/powerpc/kernel/head_wrhv.S
new file mode 100644
index 0000000..cdc3bdf
--- /dev/null
+++ b/arch/powerpc/kernel/head_wrhv.S
@@ -0,0 +1,890 @@
+/*
+ * Kernel execution entry point code.
+ *
+ *    Copyright (c) 1995-1996 Gary Thomas <gdt@linuxppc.org>
+ *	Initial PowerPC version.
+ *    Copyright (c) 1996 Cort Dougan <cort@cs.nmt.edu>
+ *	Rewritten for PReP
+ *    Copyright (c) 1996 Paul Mackerras <paulus@cs.anu.edu.au>
+ *	Low-level exception handers, MMU support, and rewrite.
+ *    Copyright (c) 1997 Dan Malek <dmalek@jlc.net>
+ *	PowerPC 8xx modifications.
+ *    Copyright (c) 1998-1999 TiVo, Inc.
+ *	PowerPC 403GCX modifications.
+ *    Copyright (c) 1999 Grant Erickson <grant@lcse.umn.edu>
+ *	PowerPC 403GCX/405GP modifications.
+ *    Copyright 2000 MontaVista Software Inc.
+ *	PPC405 modifications
+ *	PowerPC 403GCX/405GP modifications.
+ *	Author: MontaVista Software, Inc.
+ *		frank_rowand@mvista.com or source@mvista.com
+ *		debbie_chu@mvista.com
+ *    Copyright 2002-2004 MontaVista Software, Inc.
+ *	PowerPC 44x support, Matt Porter <mporter@kernel.crashing.org>
+ *    Copyright 2004 Freescale Semiconductor, Inc
+ *	PowerPC e500 modifications, Kumar Gala <galak@kernel.crashing.org>
+ *    Copyright 2009 Wind River Systems, Inc
+ *	clone from head_fsl_booke.S and implement guest/hypervisor changes
+ *
+ * This program is free software; you can redistribute  it and/or modify it
+ * under  the terms of  the GNU General  Public License as published by the
+ * Free Software Foundation;  either version 2 of the  License, or (at your
+ * option) any later version.
+ */
+
+#include <linux/threads.h>
+#include <asm/processor.h>
+#include <asm/page.h>
+#include <asm/mmu.h>
+#include <asm/pgtable.h>
+#include <asm/cputable.h>
+#include <asm/thread_info.h>
+#include <asm/ppc_asm.h>
+#include <asm/asm-offsets.h>
+#include <asm/cache.h>
+#include "head_booke.h"
+#include "head_wrhv.h"
+#include <vbi/vbInterface.h>
+#include <vbi/sys/vmmu.h>
+
+/* As with the other PowerPC ports, it is expected that when code
+ * execution begins here, the following registers contain valid, yet
+ * optional, information:
+ *
+ *   r3 - Board info structure pointer (DRAM, frequency, MAC address, etc.)
+ *   r4 - Starting address of the init RAM disk
+ *   r5 - Ending address of the init RAM disk
+ *   r6 - Start of kernel command line string (e.g. "mem=128")
+ *   r7 - End of kernel command line string
+ *
+ */
+	.section	.text.head, "ax"
+_ENTRY(_stext);
+_ENTRY(_start);
+	/*
+	 * Reserve a word at a fixed location to store the address
+	 * of abatron_pteptrs
+	 */
+	nop
+/*
+ * Save parameters we are passed
+ */
+	mr	r31,r3
+	mr	r30,r4
+	mr	r29,r5
+	mr	r28,r6
+	mr	r27,r7
+	li	r25,0		/* phys kernel start (low) */
+	li	r24,0		/* CPU number */
+	li	r23,0		/* phys kernel start (high) */
+
+/* We try to not make any assumptions about how the boot loader
+ * setup or used the TLBs.  We invalidate all mappings from the
+ * boot loader and load a single entry in TLB1[0] to map the
+ * first 64M of kernel memory.  Any boot info passed from the
+ * bootloader needs to live in this first 64M.
+ *
+ * Requirement on bootloader:
+ *  - The page we're executing in needs to reside in TLB1 and
+ *    have IPROT=1.  If not an invalidate broadcast could
+ *    evict the entry we're currently executing in.
+ *
+ *  r3 = Index of TLB1 were executing in
+ *  r4 = Current MSR[IS]
+ *  r5 = Index of TLB1 temp mapping
+ *
+ * Later in mapin_ram we will correctly map lowmem, and resize TLB1[0]
+ * if needed
+ */
+
+_ENTRY(__early_start)
+	/*
+	 * This is where the main kernel code starts.
+ 	 */
+
+#ifdef CONFIG_SMP
+	/* Check to see if we're the second processor, and jump
+	 * to the secondary_start code if so
+	 */
+ 	/* Need get the real PIR from Hypervisor, now this
+ 	 * is just an emulation. PIR is defined in enrey_32.S,
+ 	 * the default value is zero.
+ 	 */
+ 	WRHV_MFPIR(r24)
+	cmpwi	r24,0
+	bne	__secondary_start
+#endif
+
+	/*
+	 * This is where the main kernel code starts.
+	 */
+
+	/* ptr to current */
+	lis	r2,init_task@h
+	ori	r2,r2,init_task@l
+
+	/* ptr to current thread */
+	addi	r4,r2,THREAD	/* init task's THREAD */
+	WRHV_MTSPRG3(r4,r1)
+	/* Establish the interrupt vector offsets */
+	lis	r0,VBI_SYS_hyIoctl@h
+	ori     r0,r0,VBI_SYS_hyIoctl@l
+	lis	r3,VBI_HYIOCTL_EXCBASE@h
+	ori	r3,r3,VBI_HYIOCTL_EXCBASE@l
+	lis	r4,_start@h
+	ori	r4,r4,_start@l
+	sc
+
+	/* stack */
+	lis	r1,init_thread_union@h
+	ori	r1,r1,init_thread_union@l
+	li	r0,0
+	stwu	r0,THREAD_SIZE-STACK_FRAME_OVERHEAD(r1)
+
+	bl	early_init
+
+#ifdef CONFIG_RELOCATABLE
+	lis	r3,kernstart_addr@ha
+	la	r3,kernstart_addr@l(r3)
+#ifdef CONFIG_PHYS_64BIT
+	stw	r23,0(r3)
+	stw	r25,4(r3)
+#else
+	stw	r25,0(r3)
+#endif
+#endif
+
+/*
+ * Decide what sort of machine this is and initialize the MMU.
+ */
+	mr	r3,r31
+	mr	r4,r30
+	mr	r5,r29
+	mr	r6,r28
+	mr	r7,r27
+	bl	machine_init
+	bl	MMU_init
+	b	start_kernel
+
+/* Macros to hide the PTE size differences
+ *
+ * FIND_PTE_ADDR -- walks the page tables given EA & pgdir pointer
+ *   r10 -- EA of fault
+ *   r11 -- PGDIR pointer
+ *   r12 -- free
+ *   label 2: is the bailout case
+ *
+ * if we find the pte (fall through):
+ *   r12 is pointer to the pte
+ */
+
+#if defined(CONFIG_PTE_64BIT) || defined(CONFIG_WRHV)
+#  define FIND_PTE_ADDR \
+	rlwinm	r12, r10, 13, 19, 29;	/* Compute pgdir/pmd offset */	\
+	lwzx	r11, r12, r11;		/* Get pgd/pmd entry */		\
+	rlwinm.	r12, r11, 0, 0, 20;	/* Extract pt base address */	\
+	rlwimi	r12, r10, 23, 20, 28;	/* Compute pte address */
+
+#    define LOAD_PTE \
+	lwz	r11, 4(r12);
+
+#  ifdef CONFIG_SMP
+#    define LWARX_PTE \
+	li	r11, 4;							\
+	lwarx	r11, r11, r12;		/* lwarx pte */
+
+#    define STWCX_PTE \
+	addi	r12, r12, 4;	\
+	stwcx.	r11, 0, r12;	\
+	addi	r12, r12, -4;
+#else
+#    define LWARX_PTE \
+	lwz	r11, 4(r12)
+
+#    define STWCX_PTE \
+	stw	r11, 4(r12)
+#endif /* CONFIG_SMP */
+
+#else /* 32-bit PTEs */
+#  define FIND_PTE_ADDR	\
+	rlwinm	r12, r10, 12, 20, 29;	/* Compute pgdir/pmd offset */	\
+	lwzx	r11, r12, r11;		/* Get L1 entry */		\
+	rlwinm.	r12, r11, 0, 0, 19;	/* Extract pte base address */	\
+	rlwimi	r12, r10, 22, 20, 29;	/* Compute PTE address */	\
+
+#    define LOAD_PTE \
+	lwz	r11, 0(r12);
+
+#  ifdef CONFIG_SMP
+#    define LWARX_PTE \
+	lwarx	r11, 0, r12;		/* lwarx pte */
+#    define STWCX_PTE \
+	stwcx.	r11, 0, r12;
+#  else
+#    define LWARX_PTE \
+	lwz	r11, 0(r12);
+#    define STWCX_PTE \
+	stw	r11, 0(r12);
+#  endif
+#endif
+
+/*
+ * Interrupt vector entry code
+ *
+ * The Book E MMUs are always on so we don't need to handle
+ * interrupts in real mode as with previous PPC processors. In
+ * this case we handle interrupts in the kernel virtual address
+ * space.
+ *
+ * Interrupt vectors are dynamically placed relative to the
+ * interrupt prefix as determined by the address of interrupt_base.
+ * The interrupt vectors offsets are programmed using the labels
+ * for each interrupt vector entry.
+ *
+ * Interrupt vectors must be aligned on a 16 byte boundary.
+ * We align on a 32 byte cache line boundary for good measure.
+ */
+
+	.align 8
+interrupt_base:
+	/* Critical Input Interrupt */
+	CRITICAL_EXCEPTION(0x0100, CriticalInput, unknown_exception)
+
+	/* Machine Check Interrupt */
+#ifdef CONFIG_E200
+	/* no RFMCI, MCSRRs on E200 */
+	CRITICAL_EXCEPTION(0x0200, MachineCheck, machine_check_exception)
+#else
+	MCHECK_EXCEPTION(0x0200, MachineCheck, machine_check_exception)
+#endif
+
+	/* Data Storage Interrupt */
+	START_EXCEPTION(DataStorage)
+        /*    only  r3, r4, CR are saved in vb_status */
+       	lis	r4,wr_control@ha
+       	lwz	r4,wr_control@l(r4)
+	stw	r10,VB_CONTROL_R10(r4)
+	stw	r11,VB_CONTROL_R11(r4)
+	stw	r12,VB_CONTROL_R12(r4)
+	stw	r13,VB_CONTROL_R13(r4)
+
+	/*
+	 * Check if it was a store fault, if not then bail
+	 * because a user tried to access a kernel or
+	 * read-protected page.  Otherwise, get the
+	 * offending address and handle it.
+	 */
+        lis     r11,wr_status@ha
+        lwz     r11,wr_status@l(r11)
+	lwz	r10,VB_STATUS_ESR(r11)
+	srwi	r10,r10,16    /* get the hibit of ESR_ST */
+	andis.	r10, r10, ESR_ST@h
+	beq	2f
+
+	lwz	r10,VB_STATUS_DEAR(r11)
+
+	/* If we are faulting a kernel address, we have to use the
+	 * kernel page tables.
+	 */
+	lis	r11, PAGE_OFFSET@h
+	cmplw	0, r10, r11
+	bge	2f
+
+	/* Get the PGD for the current thread */
+3:
+	WRHV_MFSPRG3(r11)
+	lwz	r11,PGDIR(r11)
+4:
+	FIND_PTE_ADDR
+
+	beq	2f	/* Bail if there's no entry */
+
+5:
+	LOAD_PTE
+
+	/* Are _PAGE_USER & _PAGE_RW set & _PAGE_HWWRITE not? */
+	andi.	r13, r11, _PAGE_RW|_PAGE_USER|_PAGE_HWWRITE
+	cmpwi	0, r13, _PAGE_RW|_PAGE_USER
+	bne	2f			/* Bail if not */
+
+	/* update search PID in MAS6, AS = 0 */
+	mfspr	r13, SPRN_PID0
+	slwi	r13, r13, 16
+	mtspr	SPRN_MAS6, r13
+
+	/* find the TLB index that caused the fault. */
+	tlbsx	0, r10
+
+#if defined(CONFIG_SMP) && !defined(CONFIG_WRHV)
+	/*
+	 * It's possible another processor kicked out the entry
+	 * before we did our tlbsx, so check if we hit
+	 */
+	mfspr	r13, SPRN_MAS1
+	rlwinm.	r13,r13, 0, 0, 0;	/* Check the Valid bit */
+	beq	2f
+#endif /* CONFIG_SMP */
+
+	/*
+	 * MAS2 not updated as the entry does exist in the tlb, this
+	 * fault taken to detect state transition (eg: COW -> DIRTY)
+	 */
+	andi.	r13, r11, _PAGE_HWEXEC
+	rlwimi	r13, r13, 31, 27, 27	/* SX <- _PAGE_HWEXEC */
+	ori	r13, r13, (MAS3_UW|MAS3_SW|MAS3_UR|MAS3_SR)@l /* set static perms */
+
+	/* only update the perm bits, assume the RPN is fine */
+	mfspr	r10, SPRN_MAS3
+	rlwimi	r10, r13, 0, 20, 31
+	mtspr	SPRN_MAS3,r10
+	tlbwe
+
+#if defined(CONFIG_SMP) && !defined(CONFIG_WRHV)
+	mr	r13, r11
+	LWARX_PTE
+	cmpw	r13, r11
+	bne-	7f
+#endif
+
+	/* Update 'changed'. */
+	ori	r11, r11, _PAGE_DIRTY|_PAGE_ACCESSED|_PAGE_HWWRITE
+	STWCX_PTE	/* r11 and r12 must be PTE and &PTE */
+
+#if defined(CONFIG_SMP) && !defined(CONFIG_WRHV)
+	/*
+	 * If the stwcx. failed, we invalidate the entry we just wrote,
+	 * and start over
+	 */
+	beq+	6f
+7:	mfspr	r13, SPRN_MAS1
+	rlwinm	r13, r13, 0, 1, 31	/* Clear Valid bit */
+	mtspr	SPRN_MAS1, r13
+	tlbwe
+
+	b	5b		/* Try again */
+#endif	/* CONFIG_SMP */
+6:
+
+	/* Done...restore registers and get out of here.  */
+	mfspr	r11, SPRN_SPRG7R
+	mtcr	r11
+	mfspr	r13, SPRN_SPRG5R
+	mfspr	r12, SPRN_SPRG4R
+	mfspr	r11, SPRN_SPRG1
+	mfspr	r10, SPRN_SPRG0
+	rfi			/* Force context change */
+
+2:
+#if defined(CONFIG_SMP) && !defined(CONFIG_WRHV)
+	/* Clear the reservation */
+	lis	r11, dummy_stwcx@h
+	ori	r11,r11, dummy_stwcx@l
+	stwcx.	r11, 0, r11
+#endif
+
+	/*
+	 * The bailout.  Restore registers to pre-exception conditions
+	 * and call the heavyweights to help us out.
+	 */
+       	lis	r11,wr_control@ha
+       	lwz	r11,wr_control@l(r11)
+	lwz     r13,VB_CONTROL_R13(r11)
+	lwz     r12,VB_CONTROL_R12(r11)
+	lwz     r10,VB_CONTROL_R10(r11)
+	lwz     r11,VB_CONTROL_R11(r11)
+	b	data_access
+
+	/* Instruction Storage Interrupt */
+	INSTRUCTION_STORAGE_EXCEPTION
+
+	/* External Input Interrupt */
+	EXCEPTION(0x0500, ExternalInput, do_IRQ, EXC_XFER_LITE)
+
+	/* Alignment Interrupt */
+	ALIGNMENT_EXCEPTION
+
+	/* Program Interrupt */
+	PROGRAM_EXCEPTION
+
+	/* Floating Point Unavailable Interrupt */
+#ifdef CONFIG_PPC_FPU
+	FP_UNAVAILABLE_EXCEPTION
+#else
+#ifdef CONFIG_E200
+	/* E200 treats 'normal' floating point instructions as FP Unavail exception */
+	EXCEPTION(0x0800, FloatingPointUnavailable, program_check_exception, EXC_XFER_EE)
+#else
+	EXCEPTION(0x0800, FloatingPointUnavailable, unknown_exception, EXC_XFER_EE)
+#endif
+#endif
+
+	/* System Call Interrupt */
+	START_EXCEPTION(SystemCall)
+	NORMAL_EXCEPTION_PROLOG
+	EXC_XFER_EE_LITE(0x0c00, DoSyscall)
+
+	/* Auxillary Processor Unavailable Interrupt */
+	EXCEPTION(0x2900, AuxillaryProcessorUnavailable, unknown_exception, EXC_XFER_EE)
+
+	/* Decrementer Interrupt */
+	DECREMENTER_EXCEPTION
+
+	/* Fixed Internal Timer Interrupt */
+	/* TODO: Add FIT support */
+	EXCEPTION(0x3100, FixedIntervalTimer, unknown_exception, EXC_XFER_EE)
+
+	/* Watchdog Timer Interrupt */
+#ifdef CONFIG_BOOKE_WDT
+	CRITICAL_EXCEPTION(0x3200, WatchdogTimer, WatchdogException)
+#else
+	CRITICAL_EXCEPTION(0x3200, WatchdogTimer, unknown_exception)
+#endif
+
+	/* Data TLB Error Interrupt */
+	START_EXCEPTION(DataTLBError)
+	b	data_access
+
+	/* Instruction TLB Error Interrupt */
+	/*
+	 * Nearly the same as above, except we get our
+	 * information from different registers and bailout
+	 * to a different point.
+	 */
+	START_EXCEPTION(InstructionTLBError)
+	b	InstructionStorage
+
+	DEBUG_DEBUG_EXCEPTION
+#if defined(CONFIG_E500) && !defined(CONFIG_PPC_E500MC)
+	DEBUG_CRIT_EXCEPTION
+#endif
+
+#ifdef CONFIG_SPE
+	/* SPE Unavailable */
+	START_EXCEPTION(SPEUnavailable)
+	NORMAL_EXCEPTION_PROLOG
+	bne	load_up_spe
+	addi	r3,r1,STACK_FRAME_OVERHEAD
+	EXC_XFER_EE_LITE(0x2010, KernelSPE)
+#else
+	EXCEPTION(0x2020, SPEUnavailable, unknown_exception, EXC_XFER_EE)
+#endif /* CONFIG_SPE */
+
+	/* SPE Floating Point Data */
+#ifdef CONFIG_SPE
+	EXCEPTION(0x2030, SPEFloatingPointData, SPEFloatingPointException, EXC_XFER_EE);
+#else
+	EXCEPTION(0x2040, SPEFloatingPointData, unknown_exception, EXC_XFER_EE)
+#endif /* CONFIG_SPE */
+
+	/* SPE Floating Point Round */
+	EXCEPTION(0x2050, SPEFloatingPointRound, SPEFloatingPointException_Round, EXC_XFER_EE)
+
+	/* Performance Monitor */
+	EXCEPTION(0x2060, PerformanceMonitor, performance_monitor_exception, EXC_XFER_STD)
+
+#ifdef CONFIG_PPC_E500MC
+	EXCEPTION(0x2070, Doorbell, unknown_exception, EXC_XFER_EE)
+#endif
+
+
+/*
+ * Local functions
+ */
+
+	/*
+	 * Data TLB exceptions will bail out to this point
+	 * if they can't resolve the lightweight TLB fault.
+	 */
+data_access:
+	NORMAL_EXCEPTION_PROLOG
+	lis	r6,wr_status@ha
+	lwz	r6,wr_status@l(r6)
+	lwz	r5, VB_STATUS_ESR(r6)
+	stw	r5,_ESR(r11)
+	lwz	r4, VB_STATUS_DEAR(r6)
+	andis.	r10,r5,(ESR_ILK|ESR_DLK)@h
+	bne	1f
+	EXC_XFER_EE_LITE(0x0300, handle_page_fault)
+1:
+	addi	r3,r1,STACK_FRAME_OVERHEAD
+	EXC_XFER_EE_LITE(0x0300, CacheLockingException)
+
+#ifdef CONFIG_SPE
+/* Note that the SPE support is closely modeled after the AltiVec
+ * support.  Changes to one are likely to be applicable to the
+ * other!  */
+load_up_spe:
+/*
+ * Disable SPE for the task which had SPE previously,
+ * and save its SPE registers in its thread_struct.
+ * Enables SPE for use in the kernel on return.
+ * On SMP we know the SPE units are free, since we give it up every
+ * switch.  -- Kumar
+ */
+	mfmsr	r5
+	oris	r5,r5,MSR_SPE@h
+	mtmsr	r5			/* enable use of SPE now */
+	isync
+	li	r5,(SPEFSCR_FINVE | SPEFSCR_FDBZE | SPEFSCR_FUNFE | SPEFSCR_FOVFE)
+	mtspr   SPRN_SPEFSCR,r5
+/*
+ * For SMP, we don't do lazy SPE switching because it just gets too
+ * horrendously complex, especially when a task switches from one CPU
+ * to another.  Instead we call giveup_spe in switch_to.
+ */
+#ifndef CONFIG_SMP
+	lis	r3,last_task_used_spe@ha
+	lwz	r4,last_task_used_spe@l(r3)
+	cmpi	0,r4,0
+	beq	1f
+	addi	r4,r4,THREAD	/* want THREAD of last_task_used_spe */
+	SAVE_32EVRS(0,r10,r4)
+	evxor	evr10, evr10, evr10	/* clear out evr10 */
+	evmwumiaa evr10, evr10, evr10	/* evr10 <- ACC = 0 * 0 + ACC */
+	li	r5,THREAD_ACC
+	evstddx	evr10, r4, r5		/* save off accumulator */
+	lwz	r5,PT_REGS(r4)
+	lwz	r4,_MSR-STACK_FRAME_OVERHEAD(r5)
+	lis	r10,MSR_SPE@h
+	andc	r4,r4,r10	/* disable SPE for previous task */
+	stw	r4,_MSR-STACK_FRAME_OVERHEAD(r5)
+1:
+#endif /* !CONFIG_SMP */
+	/* enable use of SPE after return */
+	oris	r9,r9,MSR_SPE@h
+	WRHV_MFSPRG3(r5)
+	li	r4,1
+	li	r10,THREAD_ACC
+	stw	r4,THREAD_USED_SPE(r5)
+	evlddx	evr4,r10,r5
+	evmra	evr4,evr4
+	REST_32EVRS(0,r10,r5)
+#ifndef CONFIG_SMP
+	subi	r4,r5,THREAD
+	stw	r4,last_task_used_spe@l(r3)
+#endif /* !CONFIG_SMP */
+	/* restore registers and return */
+2:	REST_4GPRS(3, r11)
+	lwz	r10,_CCR(r11)
+	REST_GPR(1, r11)
+	mtcr	r10
+	lwz	r10,_LINK(r11)
+	mtlr	r10
+	REST_GPR(10, r11)
+	mtspr	SPRN_SRR1,r9
+	mtspr	SPRN_SRR0,r12
+	REST_GPR(9, r11)
+	REST_GPR(12, r11)
+	lwz	r11,GPR11(r11)
+	rfi
+
+/*
+ * SPE unavailable trap from kernel - print a message, but let
+ * the task use SPE in the kernel until it returns to user mode.
+ */
+KernelSPE:
+	lwz	r3,_MSR(r1)
+	oris	r3,r3,MSR_SPE@h
+	stw	r3,_MSR(r1)	/* enable use of SPE after return */
+	lis	r3,87f@h
+	ori	r3,r3,87f@l
+	mr	r4,r2		/* current */
+	lwz	r5,_NIP(r1)
+	bl	printk
+	b	ret_from_except
+87:	.string	"SPE used in kernel  (task=%p, pc=%x)  \n"
+	.align	4,0
+
+#endif /* CONFIG_SPE */
+
+/*
+ * Global functions
+ */
+
+/*
+ * extern void loadcam_entry(unsigned int index)
+ *
+ * Load TLBCAM[index] entry in to the L2 CAM MMU
+ */
+_GLOBAL(loadcam_entry)
+	lis	r4,TLBCAM@ha
+	addi	r4,r4,TLBCAM@l
+	mulli	r5,r3,20
+	add	r3,r5,r4
+	lwz	r4,0(r3)
+	mtspr	SPRN_MAS0,r4
+	lwz	r4,4(r3)
+	mtspr	SPRN_MAS1,r4
+	lwz	r4,8(r3)
+	mtspr	SPRN_MAS2,r4
+	lwz	r4,12(r3)
+	mtspr	SPRN_MAS3,r4
+	tlbwe
+	isync
+	blr
+
+/*
+ * extern void giveup_altivec(struct task_struct *prev)
+ *
+ * The e500 core does not have an AltiVec unit.
+ */
+_GLOBAL(giveup_altivec)
+	blr
+
+/*
+ * extern void giveup_fpu(struct task_struct *prev)
+ *
+ * Not all FSL Book-E cores have an FPU
+ */
+#ifndef CONFIG_PPC_FPU
+_GLOBAL(giveup_fpu)
+	blr
+#endif
+
+/*
+ * extern void abort(void)
+ *
+ * At present, this routine just applies a system reset.
+ */
+_GLOBAL(abort)
+	li	r13,0
+	mtspr	SPRN_DBCR0,r13		/* disable all debug events */
+	isync
+	mfmsr	r13
+	ori	r13,r13,MSR_DE@l	/* Enable Debug Events */
+	mtmsr	r13
+	isync
+	mfspr	r13,SPRN_DBCR0
+	lis	r13,(DBCR0_IDM|DBCR0_RST_CHIP)@h
+	mtspr	SPRN_DBCR0,r13
+	isync
+
+_GLOBAL(flush_dcache_L1)
+	mfspr	r3,SPRN_L1CFG0
+
+	rlwinm	r5,r3,9,3	/* Extract cache block size */
+	twlgti	r5,1		/* Only 32 and 64 byte cache blocks
+				 * are currently defined.
+				 */
+	li	r4,32
+	subfic	r6,r5,2		/* r6 = log2(1KiB / cache block size) -
+				 *      log2(number of ways)
+				 */
+	slw	r5,r4,r5	/* r5 = cache block size */
+
+	rlwinm	r7,r3,0,0xff	/* Extract number of KiB in the cache */
+	mulli	r7,r7,13	/* An 8-way cache will require 13
+				 * loads per set.
+				 */
+	slw	r7,r7,r6
+
+	/* save off HID0 and set DCFA */
+	mfspr	r8,SPRN_HID0
+	ori	r9,r8,HID0_DCFA@l
+	mtspr	SPRN_HID0,r9
+	isync
+
+	lis	r4,KERNELBASE@h
+	mtctr	r7
+
+1:	lwz	r3,0(r4)	/* Load... */
+	add	r4,r4,r5
+	bdnz	1b
+
+	msync
+	lis	r4,KERNELBASE@h
+	mtctr	r7
+
+1:	dcbf	0,r4		/* ...and flush. */
+	add	r4,r4,r5
+	bdnz	1b
+
+	/* restore HID0 */
+	mtspr	SPRN_HID0,r8
+	isync
+
+	blr
+
+#ifdef CONFIG_SMP
+#ifdef CONFIG_MPC8572DS_UBOOT_COMPAT
+/* To boot secondary cpus, we need a place for them to start up.
+ * Normally, they start at 0xfffffffc, but that's usually the
+ * firmware, and we don't want to have to run the firmware again.
+ * Instead, the primary cpu will set the BPTR to point here to
+ * this page.  We then set up the core, and head to
+ * start_secondary.  Note that this means that the code below
+ * must never exceed 1023 instructions (the branch at the end
+ * would then be the 1024th).
+ */
+	.globl	__secondary_start_page
+	.align	12
+__secondary_start_page:
+/* First do some preliminary setup */
+	lis	r3, HID0_EMCP@h /* Enable machine check */
+	ori	r3,r3,0x4000	/* Enable the Time Base */
+#ifdef CONFIG_PHYS_64BIT
+	ori	r3,r3,0x0080	/* Enable MAS7 updates */
+#endif
+	mtspr	SPRN_HID0,r3
+
+	mfspr	r3, SPRN_TCR
+	oris	r3,r3, TCR_DIE@h /* Enable the Decrementer Interrupt */
+	mtspr	SPRN_TCR,r3
+
+	li	r3,0x3000
+	mtspr	SPRN_HID1,r3
+
+	/* Enable branch prediction */
+	li	r3,0x201
+	mtspr	SPRN_BUCSR,r3
+
+	/* Enable/invalidate the I-Cache */
+	mfspr	r0,SPRN_L1CSR1
+	ori	r0,r0,(L1CSR1_ICFI|L1CSR1_ICE)
+	oris	r0,r0,L1CSR1_ICPE@h
+	mtspr	SPRN_L1CSR1,r0
+	isync
+
+	/* Enable/invalidate the D-Cache */
+	mfspr	r0,SPRN_L1CSR0
+	ori	r0,r0,(L1CSR0_DCFI|L1CSR0_DCE)
+	oris	r0,r0,L1CSR0_DCPE@h
+	msync
+	isync
+	mtspr	SPRN_L1CSR0,r0
+	isync
+
+
+/*
+ * Coming here, we know the cpu has one TLB mapping in TLB1[0]
+ * which maps 0xfffff000-0xffffffff one-to-one.  We set up a
+ * second mapping that maps 0 to 0 for 16M, and then we jump to
+ * __early_start
+ */
+	lis	r6,0x1001	/* Set TLB=1 and ESEL=1 */
+	mtspr	SPRN_MAS0,r6
+	lis	r6,(MAS1_VALID|MAS1_IPROT)@h
+	ori	r6,r6,(MAS1_TSIZE(BOOKE_PAGESZ_16M))@l
+	mtspr	SPRN_MAS1,r6
+	li	r6,0
+	mtspr	SPRN_MAS2,r6	/* EPN is 0 */
+	lis	r7,0	/* RPN is 0*/
+	ori	r7,r7,(MAS3_SX|MAS3_SW|MAS3_SR)
+	mtspr	SPRN_MAS3,r7
+	tlbwe
+
+/* Now we have another mapping for this page, so we jump to that
+ * mapping
+ */
+	mfmsr	r4
+	lis	r7,__early_start@h
+	ori	r7,r7,__early_start@l
+	rlwinm	r7,r7,0,20,31
+	mtspr	SPRN_SRR0,r7
+	mtspr	SPRN_SRR1,r4
+	rfi
+#endif  /* CONFIG_MPC8572DS_UBOOT_COMPAT */
+
+/* When we get here, r24 needs to hold the CPU # */
+	.globl __secondary_start
+__secondary_start:
+	lis	r3,__secondary_hold_acknowledge@h
+	ori	r3,r3,__secondary_hold_acknowledge@l
+	stw	r24,0(r3)
+
+	li	r3,0
+	mr	r4,r24		/* Why? */
+	bl	call_setup_cpu
+
+	/* r26 should be safe, here */
+	lis	r26, tlbcam_index@ha
+	lwz	r26, tlbcam_index@l(r26)
+
+	/* Load CAM 0 */
+	li	r3,0
+	bl	loadcam_entry
+
+	/* Load CAM 1, if it's set */
+	li	r3,1
+	cmpw	r3,r26
+	bgt	1f
+	bl	loadcam_entry
+1:
+	/* Load CAM 2, if it's set */
+	li	r3,2
+	cmpw	r3,r26
+	bgt	2f
+	bl	loadcam_entry
+2:
+
+	/* get current_thread_info and current */
+	lis	r1,secondary_ti@ha
+	lwz	r1,secondary_ti@l(r1)
+	lwz	r2,TI_TASK(r1)
+
+	/* stack */
+	addi	r1,r1,THREAD_SIZE-STACK_FRAME_OVERHEAD
+	li	r0,0
+	stw	r0,0(r1)
+
+	/* ptr to current thread */
+	addi	r4,r2,THREAD	/* address of our thread_struct */
+	mtspr	SPRN_SPRG3,r4
+
+	/* Setup the defaults for TLB entries */
+	li	r4,(MAS4_TSIZED(BOOKE_PAGESZ_4K))@l
+	mtspr	SPRN_MAS4,r4
+
+	/* Jump to start_secondary */
+	lis	r4,MSR_KERNEL@h
+	ori	r4,r4,MSR_KERNEL@l
+	lis	r3,start_secondary@h
+	ori	r3,r3,start_secondary@l
+	mtspr	SPRN_SRR0,r3
+	mtspr	SPRN_SRR1,r4
+	sync
+	rfi
+	sync
+
+	.globl __secondary_hold_acknowledge
+__secondary_hold_acknowledge:
+	.long	-1
+
+#ifdef CONFIG_MPC8572DS_UBOOT_COMPAT
+	/* Fill in the empty space.  The actual reset vector is
+	 * the last word of the page */
+__secondary_start_code_end:
+	.space 4092 - (__secondary_start_code_end - __secondary_start_page)
+__secondary_reset_vector:
+	b	__secondary_start_page
+#endif
+#endif
+
+/*
+ * We put a few things here that have to be page-aligned. This stuff
+ * goes at the beginning of the data segment, which is page-aligned.
+ */
+	.data
+	.align	12
+	.globl	sdata
+sdata:
+	.globl	empty_zero_page
+empty_zero_page:
+	.space	4096
+	.globl	swapper_pg_dir
+swapper_pg_dir:
+	.space	8192
+/*
+ * We need a place to stwcx. to when we want to clear a reservation
+ * without knowing where the original was.
+ */
+	.globl	dummy_stwcx
+dummy_stwcx:
+	.space	4
+
+/*
+ * Room for two PTE pointers, usually the kernel and current user pointers
+ * to their respective root page table.
+ */
+abatron_pteptrs:
+	.space	8
diff --git a/arch/powerpc/kernel/head_wrhv.h b/arch/powerpc/kernel/head_wrhv.h
new file mode 100644
index 0000000..cbb439f
--- /dev/null
+++ b/arch/powerpc/kernel/head_wrhv.h
@@ -0,0 +1,146 @@
+/*
+ *  This program is free software; you can redistribute it and/or modify it
+ *  under the terms of the GNU General Public License as published by the
+ *  Free Software Foundation; either version 2, or (at your option) any
+ *  later version.
+ *
+ *  This program is distributed in the hope that it will be useful, but
+ *  WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ *  General Public License for more details.
+ *
+ *  Copyright (C) 2008 Wind River Systems, Inc.
+ */
+
+#ifndef __HEAD_WRHV_H__
+#define __HEAD_WRHV_H__
+
+#include <asm/arch_vbi.h>
+
+	/* Interrupts are disabled by hypervisor at this entry point.
+	 * It puts the following registers into the status page:
+	 *   VB_STATUS_OLD_INT_DISABLE (the INT_DISABLE from Control)
+	 *   CR register
+	 *   SRR0 register - pc at time of interrupt
+	 *   SRR1 register - msr at time of interrupt
+	 *   LR register - link register at time of interrupt
+	 *   R3 register - R3 at time of interrupt
+	 *   R4 register - R4 at time of interrupt
+	 * When code in this macro has been executed, r9 contains MSR, r12 pc
+	 * r10 is trashed and r11 pointer on interrupt frame. All other
+	 * registers contain their value before the system call was executed.
+	 */
+#undef NORMAL_EXCEPTION_PROLOG
+#define NORMAL_EXCEPTION_PROLOG						     \
+        mr      r4,r1;                                                       \
+        WRHV_SUP_MODE_GET(r3);         /* check whether user or kernel */   \
+        cmpwi   0,r3,0;                                                 \
+        bne     1f;                                                          \
+        WRHV_MFSPRG3(r1);              /* if from user, start at top of   */\
+        lwz     r1,THREAD_INFO-THREAD(r1); /* this thread's kernel stack   */\
+        addi    r1,r1,THREAD_SIZE;                                           \
+1:      subi    r1,r1,INT_FRAME_SIZE;   /* Allocate an exception frame     */\
+        mr      r3,r1;                                                       \
+        stw     r0,GPR0(r3);                                                 \
+        stw     r4,GPR1(r3);                                                 \
+        stw     r4,0(r3);                                                    \
+        SAVE_4GPRS(5, r3);                                                   \
+        SAVE_4GPRS(9, r3);                                                   \
+        mr      r11,r3;                                                      \
+        lis     r4,wr_status@ha;                                           \
+        lwz     r4,wr_status@l(r4);                                        \
+        lwz     r12,VB_STATUS_LR(r4);                                        \
+        stw     r12,_LINK(r11);                                              \
+        lwz     r12,VB_STATUS_R3(r4);                                        \
+        stw     r12,GPR3(r11);                                               \
+        lwz     r12,VB_STATUS_R4(r4);                                        \
+        stw     r12,GPR4(r11);                                               \
+        lwz     r12,VB_STATUS_CR(r4);                                        \
+        stw     r12,_CCR(r11);                                               \
+        lwz     r9,VB_STATUS_SRR1(r4);                                       \
+	rlwinm  r9,r9,0,18,15; /* Clear EE & PR bits */                      \
+	mr	r12, r4;                                                     \
+        lwz     r12,VB_STATUS_OLD_INT_DISABLE(r12);                          \
+        cmpwi   0,r12,0;                                                     \
+        bne     2f;                                                          \
+        ori     r9,r9,MSR_EE;                                                \
+2:      lis     r12,wrhv_supervisor@ha;                                     \
+        lwz     r12,wrhv_supervisor@l(r12);                                \
+        cmpwi   0,r12,0;                                                     \
+        bne     3f;                                                          \
+        ori     r9,r9,MSR_PR;                                                \
+3:      li      r12,1;                                                       \
+        WRHV_SET_SUP_MODE(r3,r12);                                          \
+        lwz     r12,VB_STATUS_SRR0(r4);                                      \
+        lwz     r3,VB_STATUS_R3(r4);                                         \
+        mr      r10,r4;                                                      \
+        lwz     r4,VB_STATUS_R4(r4)
+
+
+/*
+ * Exception vectors.
+ */
+#if defined (CONFIG_WR_OCD_DEBUG) && defined (CONFIG_BOOKE)
+#define	START_EXCEPTION(label)						     \
+        .align 5;              						     \
+label:									     \
+	nop;								     \
+	nop;								     \
+	nop;								     \
+	isync;
+#else
+#ifdef CONFIG_WRHV
+#undef START_EXCEPTION
+#define        START_EXCEPTION(label)                                  \
+       .align 8;                                               \
+label:
+#else
+#define        START_EXCEPTION(label)                                  \
+       .align 5;                                               \
+label:
+#endif /* CONFIG_WRHV */
+#endif
+
+#undef DEBUG_DEBUG_EXCEPTION
+#define DEBUG_DEBUG_EXCEPTION						      \
+	START_EXCEPTION(DebugDebug);						\
+	NORMAL_EXCEPTION_PROLOG;					\
+	mr      r4,r12;                /* Pass SRR0 as arg2 */		\
+	lwz     r5,VB_STATUS_ESR(r10);					\
+	stw     r5,_ESR(r11);						\
+	addi    r3,r1,STACK_FRAME_OVERHEAD;				\
+	/* EXC_XFER_STD(0x1000, DebugException)	*/		\
+	EXC_XFER_TEMPLATE(DebugException, 0x2008, (MSR_KERNEL & ~(MSR_ME|MSR_DE|MSR_CE)), NOCOPY, transfer_to_handler_full, ret_from_except_full)
+
+#undef INSTRUCTION_STORAGE_EXCEPTION
+#define INSTRUCTION_STORAGE_EXCEPTION					      \
+	START_EXCEPTION(InstructionStorage)				      \
+	NORMAL_EXCEPTION_PROLOG;					      \
+	mr      r4,r12;                /* Pass SRR0 as arg2 */                \
+        lwz     r5,VB_STATUS_ESR(r10);                                        \
+        stw     r5,_ESR(r11);                                                 \
+        li      r5,0;                   /* Pass zero as arg3 */               \
+	EXC_XFER_EE_LITE(0x0400, handle_page_fault)
+
+#undef PROGRAM_EXCEPTION
+#define PROGRAM_EXCEPTION						      \
+	START_EXCEPTION(Program)					      \
+	NORMAL_EXCEPTION_PROLOG;					      \
+	mr      r4,r12;               /* Pass SRR0 as arg2 */                \
+        lwz     r5,VB_STATUS_ESR(r10);                                        \
+	stw	r5,_ESR(r11);						      \
+	addi	r3,r1,STACK_FRAME_OVERHEAD;				      \
+	EXC_XFER_STD(0x0700, program_check_exception)
+
+#undef DECREMENTER_EXCEPTION
+#define DECREMENTER_EXCEPTION						      \
+	START_EXCEPTION(Decrementer)					      \
+	NORMAL_EXCEPTION_PROLOG;					      \
+	addi    r3,r1,STACK_FRAME_OVERHEAD;				      \
+	EXC_XFER_LITE(0x0900, timer_interrupt)
+
+
+/* ensure this structure is always sized to a multiple of the stack alignment */
+#define STACK_EXC_LVL_FRAME_SIZE	_ALIGN_UP(sizeof (struct exception_regs), 16)
+
+#endif /* __HEAD_BOOKE_H__ */
diff --git a/arch/powerpc/kernel/vbi/util.c b/arch/powerpc/kernel/vbi/util.c
new file mode 100644
index 0000000..62d84cb
--- /dev/null
+++ b/arch/powerpc/kernel/vbi/util.c
@@ -0,0 +1,142 @@
+/*
+ * util.c - utilities routines for guest OS para-virtualization
+ *
+ * Copyright (c) 2009 Wind River Systems, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+ * See the GNU General Public License for more details.
+ *
+ */
+
+/*
+This module implements a library which is handy for para-virtualized
+guest os to use. The routines are developed based on the need while
+para-virtualize linux, therefore, may need some tweaks to be generic.
+*/
+
+#include <asm/page.h>
+#include <linux/module.h>
+#include <vbi/interface.h>
+#include <vbi/vmmu.h>
+#include <vbi/syscall.h>
+#include <vbi/vbi.h>
+
+
+/* defines */
+
+/* globals */
+
+/*
+ * wr_config is initialized as part of the guest os init, before os turns on
+ * MMU. For paravirualized linux, it is initialized in plaform_init().
+ */
+
+extern struct vb_config *wr_config;
+extern struct vb_status *wr_status;
+extern struct vb_control *wr_control;
+
+/* local */
+
+/* extern */
+extern void pteAttrSet(VMMU_PTE * pte, u_int attr);
+extern void vmmuPageTableDisplay(VMMU_LEVEL_1_DESC *l1, int vmmuon);
+
+/* forward declarations */
+
+/*
+ * vb_memsize_get should not be called before wr_config is initialized
+ */
+unsigned int vb_memsize_get(void)
+{
+	if (wr_config == (struct vb_config *)(-1)) 
+		return 0;
+	return VBI_MEM_SIZE_GET();
+}
+
+unsigned int vb_context_get(void)
+{
+	if (wr_config == (struct vb_config *)(-1))
+		return 0xdeadbee0;
+	return VBI_CONTEXT_ID_GET();
+}
+
+void vb_pte_set(void *pPte, unsigned long paddr, int protval)
+{
+
+	/* caller has guaranteed pPte != NULL */
+
+	*(uint *) pPte = (uint) VMMU_PTE_VALID_MASK;
+
+	/* linux uses more than the permission bits, in word1 of PTE */
+
+	*((uint *) ((uint *) pPte) + 1) = (((u_int) paddr & VMMU_PTE_RPN_MASK) | (protval & 0xfff));
+
+	return;
+}
+
+/*
+ * turn on mmu for the particular context
+ *
+ * note, caller must make sure, context switch inside the guest OS must
+ * not happen during this call.
+ */
+
+int vb_context_mmu_on(int pid,	/* context id */
+		      void *pgtable,	/* level 1 page table */
+		      int pagesize, int debug)
+{
+	static VMMU_CONFIG vmmu_cfg;
+
+	if (wr_config == (struct vb_config *)(- 1) || pgtable == NULL || pagesize <= 0)
+		return -1;
+
+	vmmu_cfg.addr = (VMMU_LEVEL_1_DESC *) pgtable;
+	vmmu_cfg.pageSize = pagesize;
+	vmmu_cfg.contextId = pid;
+	vmmu_cfg.vmmuNum = 0;	/* only vmmu 0 is support for the time being */
+
+	if ((vbi_config_vmmu(&vmmu_cfg)) != 0)
+		return -1;
+
+	if (debug) {
+		printk("L1 page table address %p\n", pgtable);
+		vmmuPageTableDisplay(pgtable, 0);
+		printk("End of page table display \n");
+	}
+
+	vbi_enable_vmmu(vmmu_cfg.vmmuNum);
+
+	return 0;
+}
+
+void vb__flush_dcache_icache(void *start)
+{
+	vbi_flush_icache(start, 4096);
+	vbi_flush_dcache(start, 4096);
+}
+
+void vb_flush_dcache_range(unsigned long start, unsigned long stop)
+{
+	vbi_flush_dcache((void *) start, (stop - start + 1));
+}
+
+void vb__flush_icache_range(unsigned long start, unsigned long stop)
+{
+	vbi_flush_icache((void *) start, (stop - start + 1));
+}
+
+void vb__flush_dcache_icache_phys(unsigned long physaddr)
+{
+	vbi_flush_icache((void *) physaddr, 4096);
+	vbi_flush_dcache((void *) physaddr, 4096);
+}
+
+EXPORT_SYMBOL_GPL(wrhv_int_lock);
+EXPORT_SYMBOL_GPL(wrhv_int_unlock);
+EXPORT_SYMBOL_GPL(wrhv_int_lvl_get);
diff --git a/arch/powerpc/kernel/vbi/vmmu_display.c b/arch/powerpc/kernel/vbi/vmmu_display.c
new file mode 100644
index 0000000..d1fc41a
--- /dev/null
+++ b/arch/powerpc/kernel/vbi/vmmu_display.c
@@ -0,0 +1,140 @@
+/*
+ * vmmu_display.c - hypervisor VMMU operations
+ *
+ * Copyright (c) 2009 Wind River Systems, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+ * See the GNU General Public License for more details.
+ *
+ */
+
+#include <linux/kernel.h>
+#include <vbi/interface.h>
+#include <vbi/vmmu.h>
+
+#define __va(paddr) (((unsigned long )(paddr))+0xC0000000)
+#define __pa(vaddr) (((unsigned long )(vaddr))-0xC0000000)
+
+
+/*
+ *
+ * vmmuPageTableDisplay - display information about the specified page table
+ *
+ * This routine display all the VMMU PTE entries in the specified table
+ *
+ */
+
+void vmmuPageTableDisplay(VMMU_LEVEL_1_DESC *l1, int vmmuon)
+{
+	VMMU_LEVEL_2_DESC *l2;
+	VMMU_PTE *pte;
+	VMMU_EFFECTIVE_ADDR ea;
+	u_int l1_index;
+	u_int i,j;
+
+	l1_index = 0;
+	ea.addr = 0;
+
+	printk("Logical           Physical          R C U[01234567] WIMGE S[XWR] U[XWR]\n");
+	printk("----------------- ----------------- - -  ---------- -----  -----  -----\n");
+
+	/* run through all the entries */
+	for (i=0; i<VMMU_L1_ENTRIES; i++) {
+		if (l1->field.v) {
+			ea.field.l1index = l1_index;
+			l2 = (VMMU_LEVEL_2_DESC *)VMMU_LBA_TO_ADDR(l1->field.l2ba);
+			if (vmmuon)
+				l2 = (VMMU_LEVEL_2_DESC *)__va(l2);
+
+			pte = (VMMU_PTE *)l2;
+
+			for (j=0; j<VMMU_L2_ENTRIES; j++) {
+				if (pte->field.v) {
+					ea.field.l2index = j;
+					printk ("%08x-%08x %08x-%08x %d %d ",
+					(u_int)ea.addr, (u_int)ea.addr + 0xfff,
+					pte->field.rpn << VMMU_RPN_SHIFT,
+					(pte->field.rpn << VMMU_RPN_SHIFT) + 0xfff,
+					pte->field.r, pte->field.c);
+					printk ("  %d%d%d%d%d%d%d%d  %d%d%d%d%d   %c%c%c    %c%c%c\n",
+					pte->field.u0, pte->field.u1,
+					pte->field.u2, pte->field.u3,
+					pte->field.u4, pte->field.u5,
+					pte->field.u6, pte->field.u7,
+					pte->field.w, pte->field.i,
+					pte->field.m,
+					pte->field.g, pte->field.e,
+					pte->field.sx ? 'X' : ' ',
+					pte->field.sw ? 'W' : ' ',
+					pte->field.sr ? 'R' : ' ',
+					pte->field.ux ? 'X' : ' ',
+					pte->field.uw ? 'W' : ' ',
+					pte->field.ur ? 'R' : ' ');
+				} /* pte field.v */
+				pte++;
+			} /* j */
+		} /* l1 field.v */
+		l1++;
+		l1_index++;
+	} /* i */
+}
+
+/*
+ * vmmuPteDisplay - display a specific PTE entry
+ *
+ * This routine display the VMMU PTE entrie corresponding to the specified
+ * virtual address.
+ *
+ */
+unsigned int vmmuPteDisplay(VMMU_LEVEL_1_DESC *l1, void *vaddr)
+{
+	VMMU_LEVEL_2_DESC  *l2;
+	VMMU_PTE *pte;
+
+	/* find the level-1 page table descriptor for the virtual address */
+	l1 += VMMU_L1_INDEX(vaddr);
+
+	/* if no level-2 table exists abort and return error */
+	if (!l1->field.v)
+		return -1;
+
+	/* locate correct PTE entry in level-2 table */
+	l2  = (VMMU_LEVEL_2_DESC *)VMMU_LBA_TO_ADDR(l1->field.l2ba) +
+		VMMU_L2_INDEX(vaddr);
+
+	l2 = (VMMU_LEVEL_2_DESC *)__va(l2);
+
+	pte = &l2->pte;
+
+	if (!pte->field.v)
+		return  -1;
+
+	printk("PTE for virtual address 0x%p:\n", vaddr);
+	printk("  Page Number:  0x%08x\n", pte->field.rpn<<VMMU_RPN_SHIFT);
+	printk("  Referenced:   %d\n", pte->field.r);
+	printk("  Changed:      %d\n", pte->field.c);
+	printk("  User bits:    %d%d%d%d%d%d%d%d\n",
+		pte->field.u0, pte->field.u1,
+		pte->field.u2, pte->field.u3,
+		pte->field.u4, pte->field.u5,
+		pte->field.u6, pte->field.u7);
+	printk("  WIMGE:        %d%d%d%d%d\n",
+		pte->field.w, pte->field.i, pte->field.m,
+		pte->field.g, pte->field.e);
+	printk("  Supv Perms:   %c%c%c\n",
+		pte->field.sr ? 'R' : '-',
+		pte->field.sw ? 'W' : '-',
+		pte->field.sx ? 'X' : '-');
+	printk("  User Perms:   %c%c%c\n",
+		pte->field.ur ? 'R' : '-',
+		pte->field.uw ? 'W' : '-',
+		pte->field.ux ? 'X' : '-');
+
+	return 0;
+}
diff --git a/arch/powerpc/kernel/vbi/wrhv.c b/arch/powerpc/kernel/vbi/wrhv.c
new file mode 100644
index 0000000..ac90c65
--- /dev/null
+++ b/arch/powerpc/kernel/vbi/wrhv.c
@@ -0,0 +1,963 @@
+/*
+ *  This program is free software; you can redistribute it and/or modify it
+ *  under the terms of the GNU General Public License as published by the
+ *  Free Software Foundation; either version 2, or (at your option) any
+ *  later version.
+ *
+ *  This program is distributed in the hope that it will be useful, but
+ *  WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ *  General Public License for more details.
+ *
+ *  Copyright (C) 2009 Wind River Systems, Inc.
+ */
+
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/irq.h>
+#include <linux/profile.h>
+#include <linux/wrhv.h>
+#include <linux/interrupt.h>
+#include <vbi/interface.h>
+#include <vbi/interrupt.h>
+#include <vbi/errors.h>
+
+#include <asm/page.h>
+#include <asm/pgtable.h>
+#include <asm/time.h>
+
+#include <linux/threads.h>
+#include <linux/kernel_stat.h>
+#include <linux/signal.h>
+#include <linux/sched.h>
+#include <linux/ptrace.h>
+#include <linux/ioport.h>
+#include <linux/timex.h>
+#include <linux/slab.h>
+#include <linux/delay.h>
+#include <trace/trap.h>
+
+#include <linux/seq_file.h>
+#include <linux/cpumask.h>
+#include <linux/bitops.h>
+#include <linux/list.h>
+#include <linux/radix-tree.h>
+#include <linux/mutex.h>
+#include <linux/bootmem.h>
+#include <linux/pci.h>
+#include <linux/debugfs.h>
+
+#include <asm/uaccess.h>
+#include <asm/system.h>
+#include <asm/io.h>
+#include <asm/cache.h>
+#include <asm/prom.h>
+#include <asm/machdep.h>
+#include <asm/udbg.h>
+#include <asm/firmware.h>
+
+#include <asm/pgalloc.h>
+#include <asm/mmu_context.h>
+#include <asm/mmu.h>
+#include <asm/smp.h>
+#include <asm/btext.h>
+#include <asm/tlb.h>
+#include <asm/sections.h>
+
+#include <linux/sched.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/mm.h>
+#include <linux/stddef.h>
+#include <linux/highmem.h>
+#include <linux/initrd.h>
+#include <linux/pagemap.h>
+
+#include <linux/kprobes.h>
+#include <linux/kexec.h>
+#include <linux/backlight.h>
+#include <linux/bug.h>
+#include <linux/kdebug.h>
+#include <linux/ltt-core.h>
+#include <linux/kallsyms.h>
+
+#include <mm/mmu_decl.h>
+#include <linux/lmb.h>
+
+#include <linux/major.h>
+#include <linux/device.h>
+#include <linux/platform_device.h>
+#include <linux/of_platform.h>
+#include <linux/phy.h>
+#include <linux/phy_fixed.h>
+#include <linux/spi/spi.h>
+#include <linux/fsl_devices.h>
+#include <linux/fs_enet_pd.h>
+#include <linux/fs_uart_pd.h>
+
+#include <asm/irq.h>
+#include <sysdev/fsl_soc.h>
+#include <asm/cpm2.h>
+
+#include <linux/kgdb.h>
+#include <asm/current.h>
+#include <asm/processor.h>
+
+#include <asm/paravirt.h>
+
+
+/* powerpc clocksource/clockevent code */
+
+#include <linux/clockchips.h>
+#include <linux/clocksource.h>
+
+static struct vb_config __wr_config;
+struct vb_config *wr_config;		/* TODO kernel relocation friendly ? */
+struct vb_control *wr_control;
+struct vb_status *wr_status;
+EXPORT_SYMBOL(wr_config);
+
+void wrhv_mapping(void);
+void mpc85xx_power_down(void);
+
+extern int map_page(unsigned long, phys_addr_t, int);
+
+extern int vb_context_mmu_on(int pid,  /* context id */
+			void *pgtable,    /* level 1 page table */
+			int pagesize, int debug);
+
+unsigned long wrhv_cpu_freq = 0;
+
+void wrhv_mapping(void)
+{
+	/* map in vbConfig address */
+
+	/*
+	 * WRHV vb_config should really add a length field for each
+	 * of the objected we mapped.  As the sizes are WRHV
+	 * implementation AND worse, configuration dependent.
+	 *
+	 * For now, we just use 1 page which is fine for the time being.
+	 */
+
+	map_page((unsigned long)wr_config, (unsigned long)wr_config,
+		 _PAGE_RAM);
+
+	map_page((unsigned long)wr_config->vb_status,
+		 (unsigned long)wr_config->vb_status, _PAGE_RAM);
+	map_page((unsigned long)wr_config->vb_control,
+		 (unsigned long)wr_config->vb_control, _PAGE_RAM);
+	map_page((unsigned long)wr_config->interruptConfiguration,
+		 (unsigned long)wr_config->interruptConfiguration, _PAGE_RAM);
+
+	memcpy(&__wr_config, wr_config, sizeof(__wr_config));
+	/* map any shared memory region info */
+
+	if (wr_config->sharedMemoryRegionsConfigAddress != 0)
+		map_page((unsigned long)wr_config->
+			 sharedMemoryRegionsConfigAddress,
+			 (unsigned long)wr_config->
+			 sharedMemoryRegionsConfigAddress, _PAGE_RAM);
+	/* MIPC */
+	map_page(0xfd000000, 0xfd000000, _PAGE_IO);
+	/* devices mapped by the hypervisor xml coqnfiguration */
+
+	return;
+}
+
+unsigned long __init wrhv_find_end_of_memory(void)
+{
+	return wr_config->phys_mem_size;
+}
+
+void wrhv_power_save(void)
+{
+	local_irq_enable();
+	vbi_idle(1);
+}
+
+void wrhv_restart(void)
+{
+	int ret;
+	vbi_vb_mgmt(VBI_VBMGMT_RESET, __wrhvConfig.pid, &ret,
+			VBI_VBMGMT_RESET_CLEAR,0);
+	while (1);
+}
+
+static struct irqaction wrhv_timer_irq = {
+	.handler = wrhv_timer_interrupt,
+	.flags= IRQF_DISABLED,
+	.mask = CPU_MASK_NONE,
+	.name = "timer",
+};
+
+void __init wrhv_calibrate_decr(void)
+{
+	/* The timebase is updated every 8 bus clocks */
+	ppc_tb_freq = wrhv_cpu_freq / 8;
+	printk(KERN_DEBUG "WRHV-TIME: wrhv_cpu_freq=%lu  ppc_tb_freq =%lu\n",
+			wrhv_cpu_freq, ppc_tb_freq);
+
+}
+
+void __init wrhv_time_init(void)
+{
+	return;
+}
+
+void __init wrhv_init_irq(void)
+{
+	int i;
+
+	wrhv_irq_chip.typename = "WRHV-PIC";
+	for (i = 0; i < NR_IRQS; i++) {
+		irq_desc[i].status = IRQ_DISABLED | IRQ_LEVEL;
+		irq_desc[i].action = NULL;
+		irq_desc[i].depth = 1;
+		set_irq_chip_and_handler(i, &wrhv_irq_chip, handle_fasteoi_irq);
+	}
+}
+
+#ifdef CONFIG_DEBUG_VIRTUAL_IRQS
+static irqreturn_t wrhv_vbint(int irq, void * dev_id)
+{
+	printk("[DEBUG VIRTUAL IRQS] Handling the DEBUG IRQ %d\n", irq);
+	return IRQ_HANDLED;
+}
+
+static int __init wrhv_late_init_irq(void)
+{
+	int dev_id = 1;
+	int i;
+
+	/* IRQ 0 is unknown IRQ number for Hypervisor */
+	for (i = 1; i < 32; i++) {
+		if(request_irq(i, wrhv_vbint, IRQF_SHARED, "vbint_single", &dev_id))
+			printk("Unable request IRQ for IRQ %d\n", i);
+	}
+
+	return 0;
+}
+subsys_initcall(wrhv_late_init_irq);
+#endif
+
+unsigned int wrhv_vioapic_get_irq(void)
+{
+	unsigned int irq;
+
+	irq = wr_control->irq_pend;
+
+#ifdef CONFIG_DEBUG_VIRTUAL_IRQS
+	/* Maybe this is useless for real external interrupt */
+	wr_status->irq_pend = 0;
+#endif
+
+	if (irq == 0xffff)
+		irq = NO_IRQ_IGNORE;
+	else
+		wr_control->irq_pend = 0xffff;
+
+	return irq;
+}
+
+/* refer to native implementation in arch/powerpc/kernel/irq.c */
+extern int get_ppc_spurious_interrupts(void);
+extern void set_ppc_spurious_interrupts(int value);
+
+void wrhv_do_IRQ(struct pt_regs *regs)
+{
+	struct pt_regs *old_regs = set_irq_regs(regs);
+	unsigned int irq;
+#ifdef CONFIG_IRQSTACKS
+	struct thread_info *curtp, *irqtp;
+#endif
+
+	irq_enter();
+
+#ifdef CONFIG_DEBUG_STACKOVERFLOW
+	/* Debugging check for stack overflow: is there less than 2KB free? */
+	{
+		long sp;
+
+		sp = __get_SP() & (THREAD_SIZE-1);
+
+		if (unlikely(sp < (sizeof(struct thread_info) + 2048))) {
+			printk("do_IRQ: stack overflow: %ld\n",
+				sp - sizeof(struct thread_info));
+			dump_stack();
+		}
+	}
+#endif
+
+	/*
+	 * Every platform is required to implement ppc_md.get_irq.
+	 * This function will either return an irq number or NO_IRQ to
+	 * indicate there are no more pending.
+	 * The value NO_IRQ_IGNORE is for buggy hardware and means that this
+	 * IRQ has already been handled. -- Tom
+	 */
+#ifdef CONFIG_WRHV
+check_again:
+#endif
+	irq = ppc_md.get_irq();
+
+	if (irq != NO_IRQ && irq != NO_IRQ_IGNORE) {
+#ifdef CONFIG_IRQSTACKS
+		/* Switch to the irq stack to handle this */
+		curtp = current_thread_info();
+		irqtp = hardirq_ctx[smp_processor_id()];
+		if (curtp != irqtp) {
+			struct irq_desc *desc = irq_desc + irq;
+			void *handler = desc->handle_irq;
+			unsigned long saved_sp_limit = current->thread.ksp_limit;
+			if (handler == NULL)
+				handler = &__do_IRQ;
+			irqtp->task = curtp->task;
+			irqtp->flags = 0;
+
+			/* Copy the softirq bits in preempt_count so that the
+			 * softirq checks work in the hardirq context.
+			 */
+			irqtp->preempt_count =
+				(irqtp->preempt_count & ~SOFTIRQ_MASK) |
+				(curtp->preempt_count & SOFTIRQ_MASK);
+
+			current->thread.ksp_limit = (unsigned long)irqtp +
+				_ALIGN_UP(sizeof(struct thread_info), 16);
+			call_handle_irq(irq, desc, irqtp, handler);
+			current->thread.ksp_limit = saved_sp_limit;
+			irqtp->task = NULL;
+
+
+			/* Set any flag that may have been set on the
+			 * alternate stack
+			 */
+			if (irqtp->flags)
+				set_bits(irqtp->flags, &curtp->flags);
+		} else
+#endif
+			generic_handle_irq(irq);
+#ifdef CONFIG_WRHV
+		goto check_again;
+#endif
+	} else if (irq != NO_IRQ_IGNORE)
+		/* That's not SMP safe ... but who cares ? */
+		set_ppc_spurious_interrupts(get_ppc_spurious_interrupts()+1);
+
+	irq_exit();
+	set_irq_regs(old_regs);
+
+#ifdef CONFIG_PPC_ISERIES
+	if (firmware_has_feature(FW_FEATURE_ISERIES) &&
+			get_lppaca()->int_dword.fields.decr_int) {
+		get_lppaca()->int_dword.fields.decr_int = 0;
+		/* Signal a fake decrementer interrupt */
+		timer_interrupt(regs);
+	}
+#endif
+}
+
+unsigned int wrhv_irq_of_parse_and_map(struct device_node *dev, int index)
+{
+	int irq;
+
+	irq = vbi_find_irq(dev->full_name, VB_INPUT_INT);
+	if (irq == VBI_INVALID_IRQ)
+		return NO_IRQ;
+
+	return irq;
+}
+
+unsigned int wrhv_get_pvr(void)
+{
+       return 0x80200000;
+}
+
+
+
+
+/* arch/powerpc/kernel/time.c */
+#define HWTIMER_USE_JIFFY 1
+static void wrhv_set_mode(enum clock_event_mode mode,
+				 struct clock_event_device *dev)
+{
+	return;
+}
+
+static int wrhv_set_next_event(unsigned long evt,
+				      struct clock_event_device *dev)
+{
+	return 0;
+}
+static struct clock_event_device wrhv_clockevent = {
+       .name	   = "wrhv",
+       .shift	  = 32,
+       .irq	    = 0,
+       .mult	   = 1,     /* To be filled in */
+       .set_mode       = wrhv_set_mode,
+       .set_next_event = wrhv_set_next_event,
+       .features       = CLOCK_EVT_FEAT_ONESHOT,
+};
+
+void wrhv_hw_timer_interrupt(struct pt_regs * regs)
+{
+	struct pt_regs *old_regs;
+
+	if (atomic_read(&ppc_n_lost_interrupts) != 0)
+		do_IRQ(regs);
+
+	old_regs = set_irq_regs(regs);
+	irq_enter();
+
+	calculate_steal_time();
+
+	trace_trap_entry(regs, regs->trap);
+
+	wrhv_timer_interrupt(0, NULL);
+
+	irq_exit();
+	set_irq_regs(old_regs);
+	trace_trap_exit();
+}
+
+void __init wrhv_hw_clocksource_init(void)
+{
+	return;
+}
+extern atomic_t hwtimer_cpu_trigger[];
+
+void __init wrhv_hw_time_init(void)
+{
+	return;
+}
+
+
+/* arch/powerpc/mm/fault.c */
+void wrhv_vmmu_restore (void)
+{
+	/*
+	 * this function is called by the end of page fault handling to reinstall
+	 * the vmmu
+	 */
+	wr_control->vmmu0 = wr_status->vmmu0;
+	wr_control->vmmu1 = wr_status->vmmu1;
+	return;
+}
+
+/* arch/powerpc/mm/fsl_booke_mmu.c */
+void __init wrhv_MMU_init_hw(void)
+{
+	return;
+}
+
+unsigned long __init wrhv_mmu_mapin_ram(void)
+{
+       return 0;
+}
+
+/* arch/powerpc/mm/init_32.c */
+void wrhv_MMU_setup(void)
+{
+	__map_without_bats = 1;
+
+#ifdef CONFIG_DEBUG_PAGEALLOC
+	__map_without_bats = 1;
+	__map_without_ltlbs = 1;
+#endif
+}
+
+void __init wrhv_MMU_init(void)
+{
+	if (ppc_md.progress)
+		ppc_md.progress("MMU:enter", 0x111);
+
+	/* parse args from command line */
+	wrhv_MMU_setup();
+
+	if (lmb.memory.cnt > 1) {
+		lmb.memory.cnt = 1;
+		lmb_analyze();
+		printk(KERN_WARNING "Only using first contiguous memory region");
+	}
+
+	total_lowmem = total_memory = lmb_end_of_DRAM() - memstart_addr;
+	lowmem_end_addr = memstart_addr + total_lowmem;
+
+#if defined(CONFIG_FSL_BOOKE) && !defined(CONFIG_WRHV)
+	/* Freescale Book-E parts expect lowmem to be mapped by fixed TLB
+	 * entries, so we need to adjust lowmem to match the amount we can map
+	 * in the fixed entries */
+	adjust_total_lowmem();
+#endif /* CONFIG_FSL_BOOKE && !CONFIG_WRHV*/
+
+	if (total_lowmem > __max_low_memory) {
+		total_lowmem = __max_low_memory;
+		lowmem_end_addr = memstart_addr + total_lowmem;
+#ifndef CONFIG_HIGHMEM
+		total_memory = total_lowmem;
+		lmb_enforce_memory_limit(lowmem_end_addr);
+		lmb_analyze();
+#endif /* CONFIG_HIGHMEM */
+	}
+
+	/* Initialize the MMU hardware */
+	if (ppc_md.progress)
+		ppc_md.progress("MMU:hw init", 0x300);
+	MMU_init_hw();
+
+	/* Map in all of RAM starting at KERNELBASE */
+	if (ppc_md.progress)
+		ppc_md.progress("MMU:mapin", 0x301);
+	mapin_ram();
+#ifdef CONFIG_HIGHMEM
+	ioremap_base = PKMAP_BASE;
+#else
+	ioremap_base = 0xfe000000UL;    /* for now, could be 0xfffff000 */
+#endif /* CONFIG_HIGHMEM */
+	ioremap_bot = ioremap_base;
+
+	/* Map in I/O resources */
+	if (ppc_md.progress)
+		ppc_md.progress("MMU:setio", 0x302);
+
+	/* Initialize the context management stuff */
+	mmu_context_init();
+
+	if (ppc_md.progress)
+		ppc_md.progress("MMU:exit", 0x211);
+
+	/* From now on, btext is no longer BAT mapped if it was at all */
+#ifdef CONFIG_BOOTX_TEXT
+	btext_unmap();
+#endif
+
+	/* we enables the mmu here without having to do this from the caller
+	 * (which is in assembly world)
+	 */
+	vb_context_mmu_on(0, swapper_pg_dir, PAGE_SIZE, 0);
+}
+
+
+/* sysdev/fsl_soc.c */
+extern const char * get_gfar_tx_intr(void);
+extern const char * get_gfar_rx_intr(void);
+extern const char * get_gfar_err_intr(void);
+
+int __init wrhv_gfar_of_init(void)
+{
+	struct device_node *np;
+	unsigned int i;
+	struct platform_device *gfar_dev;
+	struct resource res;
+	int ret;
+
+	for (np = NULL, i = 0;
+	     (np = of_find_compatible_node(np, "network", "gianfar")) != NULL;
+	     i++) {
+		struct resource r[4];
+		struct device_node *phy, *mdio;
+		struct gianfar_platform_data gfar_data;
+		const unsigned int *id;
+		const char *model;
+		const char *ctype;
+		const void *mac_addr;
+		const phandle *ph;
+		int n_res = 2;
+
+		if (!of_device_is_available(np))
+			continue;
+
+		memset(r, 0, sizeof(r));
+		memset(&gfar_data, 0, sizeof(gfar_data));
+
+		ret = of_address_to_resource(np, 0, &r[0]);
+		if (ret)
+			goto err;
+
+		of_irq_to_resource(np, 0, &r[1]);
+
+		model = of_get_property(np, "model", NULL);
+
+		/* If we aren't the FEC we have multiple interrupts */
+		if (model && strcasecmp(model, "FEC")) {
+#if defined(CONFIG_WRHV)
+#include <vbi/vbiInterrupt.h>
+			int vector;
+			char eTsec_tx[10];
+			char eTsec_rx[10];
+			char eTsec_err[10];
+
+			sprintf(eTsec_tx,"etsec%d_tx", i+1);
+			sprintf(eTsec_rx,"etsec%d_rx", i+1);
+			sprintf(eTsec_err,"etsec%d_err", i+1);
+
+			r[1].name = get_gfar_tx_intr();
+			vector = vbi_find_irq(eTsec_tx, VB_INPUT_INT);
+			r[1].start = vector;
+			r[1].end = vector;
+			r[1].flags = IORESOURCE_IRQ;
+
+			r[2].name = get_gfar_rx_intr();
+			vector = vbi_find_irq(eTsec_rx, VB_INPUT_INT);
+			r[2].start = vector;
+			r[2].end = vector;
+			r[2].flags = IORESOURCE_IRQ;
+
+			r[3].name = get_gfar_err_intr();
+			vector = vbi_find_irq(eTsec_err, VB_INPUT_INT);
+			r[3].start = vector;
+			r[3].end = vector;
+			r[3].flags = IORESOURCE_IRQ;
+#else
+			r[1].name = gfar_tx_intr;
+
+			r[2].name = gfar_rx_intr;
+			of_irq_to_resource(np, 1, &r[2]);
+
+			r[3].name = gfar_err_intr;
+			of_irq_to_resource(np, 2, &r[3]);
+#endif
+			n_res += 2;
+		}
+
+		gfar_dev =
+		    platform_device_register_simple("fsl-gianfar", i, &r[0],
+						    n_res);
+
+		if (IS_ERR(gfar_dev)) {
+			ret = PTR_ERR(gfar_dev);
+			goto err;
+		}
+
+		mac_addr = of_get_mac_address(np);
+		if (mac_addr)
+			memcpy(gfar_data.mac_addr, mac_addr, 6);
+
+		if (model && !strcasecmp(model, "TSEC"))
+			gfar_data.device_flags =
+			    FSL_GIANFAR_DEV_HAS_GIGABIT |
+			    FSL_GIANFAR_DEV_HAS_COALESCE |
+			    FSL_GIANFAR_DEV_HAS_RMON |
+			    FSL_GIANFAR_DEV_HAS_MULTI_INTR;
+		if (model && !strcasecmp(model, "eTSEC"))
+			gfar_data.device_flags =
+			    FSL_GIANFAR_DEV_HAS_GIGABIT |
+			    FSL_GIANFAR_DEV_HAS_COALESCE |
+			    FSL_GIANFAR_DEV_HAS_RMON |
+			    FSL_GIANFAR_DEV_HAS_MULTI_INTR |
+			    FSL_GIANFAR_DEV_HAS_CSUM |
+			    FSL_GIANFAR_DEV_HAS_VLAN |
+			    FSL_GIANFAR_DEV_HAS_EXTENDED_HASH;
+
+		ctype = of_get_property(np, "phy-connection-type", NULL);
+
+		/* We only care about rgmii-id.  The rest are autodetected */
+		if (ctype && !strcmp(ctype, "rgmii-id"))
+			gfar_data.interface = PHY_INTERFACE_MODE_RGMII_ID;
+		else
+			gfar_data.interface = PHY_INTERFACE_MODE_MII;
+
+		if (of_get_property(np, "fsl,magic-packet", NULL))
+			gfar_data.device_flags |= FSL_GIANFAR_DEV_HAS_MAGIC_PACKET;
+
+		ph = of_get_property(np, "phy-handle", NULL);
+		if (ph == NULL) {
+			u32 *fixed_link;
+
+			fixed_link = (u32 *)of_get_property(np, "fixed-link",
+							   NULL);
+			if (!fixed_link) {
+				ret = -ENODEV;
+				goto unreg;
+			}
+
+			snprintf(gfar_data.bus_id, MII_BUS_ID_SIZE, "0");
+			gfar_data.phy_id = fixed_link[0];
+		} else {
+			phy = of_find_node_by_phandle(*ph);
+
+			if (phy == NULL) {
+				ret = -ENODEV;
+				goto unreg;
+			}
+
+			mdio = of_get_parent(phy);
+
+			id = of_get_property(phy, "reg", NULL);
+			ret = of_address_to_resource(mdio, 0, &res);
+			if (ret) {
+				of_node_put(phy);
+				of_node_put(mdio);
+				goto unreg;
+			}
+
+			gfar_data.phy_id = *id;
+			snprintf(gfar_data.bus_id, MII_BUS_ID_SIZE, "%llx",
+				 (unsigned long long)res.start&0xfffff);
+
+			of_node_put(phy);
+			of_node_put(mdio);
+		}
+
+		ret =
+		    platform_device_add_data(gfar_dev, &gfar_data,
+					     sizeof(struct
+						    gianfar_platform_data));
+		if (ret)
+			goto unreg;
+	}
+
+	return 0;
+
+unreg:
+	platform_device_unregister(gfar_dev);
+err:
+	return ret;
+}
+
+/* arch/powerpc/mm/mem.c */
+extern void __flush_dcache_icache_phys(unsigned long physaddr);
+void wrhv_flush_dcache_page(struct page *page)
+{
+	if (cpu_has_feature(CPU_FTR_COHERENT_ICACHE))
+		return;
+	/* avoid an atomic op if possible */
+	if (test_bit(PG_arch_1, &page->flags))
+		clear_bit(PG_arch_1, &page->flags);
+	__flush_dcache_icache_phys(page_to_pfn(page) << PAGE_SHIFT);
+}
+
+void wrhv_update_mmu_cache(struct vm_area_struct *vma, unsigned long address,
+		      pte_t pte)
+{
+#ifndef CONFIG_WRHV
+#ifdef CONFIG_PPC_STD_MMU
+	unsigned long access = 0, trap;
+#endif
+#endif /* !CONFIG_WRHV */
+	unsigned long pfn = pte_pfn(pte);
+
+	/* handle i-cache coherency */
+	if (!cpu_has_feature(CPU_FTR_COHERENT_ICACHE) &&
+	    !cpu_has_feature(CPU_FTR_NOEXECUTE) &&
+	    pfn_valid(pfn)) {
+		struct page *page = pfn_to_page(pfn);
+#ifdef CONFIG_8xx
+		/* On 8xx, cache control instructions (particularly
+		 * "dcbst" from flush_dcache_icache) fault as write
+		 * operation if there is an unpopulated TLB entry
+		 * for the address in question. To workaround that,
+		 * we invalidate the TLB here, thus avoiding dcbst
+		 * misbehaviour.
+		 */
+		_tlbie(address, 0 /* 8xx doesn't care about PID */);
+#endif
+		/* The _PAGE_USER test should really be _PAGE_EXEC, but
+		 * older glibc versions execute some code from no-exec
+		 * pages, which for now we are supporting.  If exec-only
+		 * pages are ever implemented, this will have to change.
+		 */
+		if (!PageReserved(page) && (pte_val(pte) & _PAGE_USER)
+		    && !test_bit(PG_arch_1, &page->flags)) {
+			if (vma->vm_mm == current->active_mm) {
+				__flush_dcache_icache((void *) address);
+			} else
+				flush_dcache_icache_page(page);
+			set_bit(PG_arch_1, &page->flags);
+		}
+	}
+
+#ifndef CONFIG_WRHV
+#ifdef CONFIG_PPC_STD_MMU
+	/* We only want HPTEs for linux PTEs that have _PAGE_ACCESSED set */
+	if (!pte_young(pte) || address >= TASK_SIZE)
+		return;
+
+	/* We try to figure out if we are coming from an instruction
+	 * access fault and pass that down to __hash_page so we avoid
+	 * double-faulting on execution of fresh text. We have to test
+	 * for regs NULL since init will get here first thing at boot
+	 *
+	 * We also avoid filling the hash if not coming from a fault
+	 */
+	if (current->thread.regs == NULL)
+		return;
+	trap = TRAP(current->thread.regs);
+	if (trap == 0x400)
+		access |= _PAGE_EXEC;
+	else if (trap != 0x300)
+		return;
+	hash_preload(vma->vm_mm, address, access, trap);
+#endif /* CONFIG_PPC_STD_MMU */
+#endif /* !CONFIG_WRHV */
+}
+
+/* arch/powerpc/mm/mmu_context_32.c */
+void set_context(unsigned long contextId, pgd_t * pgd)
+{
+
+	pgd_t * kpdStart, *kpdEnd, *updStart;
+	/* we attach (copy) kernel page mapping to the user page table
+	 * Note, we only copy the L1 entrys to user L1 pageTable,
+	 * then letting L1 share the same L2 page table
+	 */
+
+	kpdStart = pgd_offset_k(KERNELBASE);
+	kpdEnd =   pgd_offset_k(0xffffffff);
+
+	updStart = pgd + pgd_index(KERNELBASE);
+
+	memcpy(updStart, kpdStart, (kpdEnd - kpdStart + 1) * sizeof (pgd_t));
+
+	/* in linux context, page table entry is not set up yet */
+	vb_context_mmu_on(contextId, pgd, PAGE_SIZE, 0);
+}
+
+/* arch/powerpc/mm/pgtable_32.c */
+int wrhv_map_page(unsigned long va, phys_addr_t pa, int flags)
+{
+	pmd_t *pd;
+	pte_t *pg;
+	int err = -ENOMEM;
+
+	/* Use upper 10 bits of VA to index the first level map */
+	pd = pmd_offset(pud_offset(pgd_offset_k(va), va), va);
+	/* Use middle 10 bits of VA to index the second-level map */
+	pg = pte_alloc_kernel(pd, va);
+	if (pg != 0) {
+		err = 0;
+		/* The PTE should never be already set nor present in the
+		 * hash table
+		 */
+		BUG_ON(pte_val(*pg) & (_PAGE_PRESENT | _PAGE_HASHPTE));
+		set_pte_at(&init_mm, va, pg, pfn_pte(pa >> PAGE_SHIFT,
+						     __pgprot(flags)));
+	}
+	if (mem_init_done)
+		flush_HPTE(0, va, pmd_val(*pmd));
+	return err;
+}
+
+
+/* arch/powerpc/kernel/traps.c */
+void __kprobes wrhv_DebugException(struct pt_regs *regs, unsigned long debug_status)
+{
+	debug_status = wr_control->vb_control_regs.dbsr;
+	wr_control->vb_control_regs.emsr &= ~MSR_DE;
+
+	if (debug_status & DBSR_IC) {   /* instruction completion */
+		regs->msr &= ~MSR_DE;
+		if (notify_die(DIE_SSTEP, "single_step", regs, 5,
+			       5, SIGTRAP) == NOTIFY_STOP) {
+			return;
+		}
+
+		if (debugger_sstep(regs))
+			return;
+
+		if (user_mode(regs)) {
+			current->thread.dbcr0 &= ~DBCR0_IC;
+		}
+
+		_exception(SIGTRAP, regs, TRAP_TRACE, regs->nip);
+	} else if (debug_status & (DBSR_DAC1R | DBSR_DAC1W)) {
+		regs->msr &= ~MSR_DE;
+
+		if (user_mode(regs)) {
+			current->thread.dbcr0 &= ~(DBSR_DAC1R | DBSR_DAC1W |
+								DBCR0_IDM);
+		} else {
+			/* Disable DAC interupts */
+			mtspr(SPRN_DBCR0, mfspr(SPRN_DBCR0) & ~(DBSR_DAC1R |
+						DBSR_DAC1W | DBCR0_IDM));
+
+			/* Clear the DAC event */
+			mtspr(SPRN_DBSR, (DBSR_DAC1R | DBSR_DAC1W));
+		}
+		/* Setup and send the trap to the handler */
+		do_dabr(regs, mfspr(SPRN_DAC1), debug_status);
+	}
+}
+
+/* arch/powerpc/kernel/kgdb.c */
+int wrhv_kgdb_arch_handle_exception(int vector, int signo, int err_code,
+			       char *remcom_in_buffer, char *remcom_out_buffer,
+			       struct pt_regs *linux_regs)
+{
+	char *ptr = &remcom_in_buffer[1];
+	unsigned long addr;
+
+	switch (remcom_in_buffer[0]) {
+		/*
+		 * sAA..AA   Step one instruction from AA..AA
+		 * This will return an error to gdb ..
+		 */
+	case 's':
+	case 'c':
+		/* handle the optional parameter */
+		if (kgdb_hex2long(&ptr, &addr))
+			linux_regs->nip = addr;
+
+		atomic_set(&kgdb_cpu_doing_single_step, -1);
+		/* set the trace bit if we're stepping */
+		if (remcom_in_buffer[0] == 's') {
+#if defined(CONFIG_40x) || defined(CONFIG_BOOKE)
+#ifdef CONFIG_WRHV
+			wr_control->vb_control_regs.dbcr0 |= (DBCR0_IC | DBCR0_IDM);
+#else
+			mtspr(SPRN_DBCR0,
+			      mfspr(SPRN_DBCR0) | DBCR0_IC | DBCR0_IDM);
+#endif
+			linux_regs->msr |= MSR_DE;
+#else
+			linux_regs->msr |= MSR_SE;
+#endif
+			kgdb_single_step = 1;
+			atomic_set(&kgdb_cpu_doing_single_step,
+				   raw_smp_processor_id());
+		}
+		return 0;
+	}
+
+	return -1;
+}
+
+
+void wrhv_init(void)
+{
+	/* initialize wr_config so that we can access
+	 * vbi configuration. The vbi configuration space
+	 * is defined in Hypervisor linux.xml
+	 */
+	wr_config = (struct vb_config *)0xF0000000;
+	wr_control = wr_config->vb_control;
+	wr_status = wr_config->vb_status;
+
+	pv_info.name = "wrhv";
+	pv_info.paravirt_enabled = 1;
+
+	pv_time_ops.time_init_cont = wrhv_time_init_cont;
+	pv_time_ops.timer_interrupt = wrhv_hw_timer_interrupt;
+	pv_time_ops.clocksource_init = wrhv_clocksource_init;
+
+	pv_irq_ops.do_IRQ = wrhv_do_IRQ;
+	pv_irq_ops.irq_of_parse_and_map =
+			wrhv_irq_of_parse_and_map;
+
+	pv_cpu_ops.get_pvr = wrhv_get_pvr;
+	pv_cpu_ops.gfar_of_init =  wrhv_gfar_of_init;
+	pv_cpu_ops.DebugException = wrhv_DebugException;
+	pv_cpu_ops.kgdb_arch_handle_exception =
+		wrhv_kgdb_arch_handle_exception;
+
+	pv_mmu_ops.vmmu_restore = wrhv_vmmu_restore;
+	pv_mmu_ops.MMU_init_hw = wrhv_MMU_init_hw;
+	pv_mmu_ops.mmu_mapin_ram = wrhv_mmu_mapin_ram;
+	pv_mmu_ops.MMU_setup = wrhv_MMU_setup;
+	pv_mmu_ops.MMU_init = wrhv_MMU_init;
+	pv_mmu_ops.flush_dcache_page = wrhv_flush_dcache_page;
+	pv_mmu_ops.update_mmu_cache = wrhv_update_mmu_cache;
+	pv_mmu_ops.map_page = wrhv_map_page;
+}
diff --git a/arch/powerpc/kernel/wrhv_entry_32.S b/arch/powerpc/kernel/wrhv_entry_32.S
new file mode 100644
index 0000000..7a00e7d
--- /dev/null
+++ b/arch/powerpc/kernel/wrhv_entry_32.S
@@ -0,0 +1,500 @@
+/*
+ *  PowerPC version
+ *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)
+ *  Rewritten by Cort Dougan (cort@fsmlabs.com) for PReP
+ *    Copyright (C) 1996 Cort Dougan <cort@fsmlabs.com>
+ *  Adapted for Power Macintosh by Paul Mackerras.
+ *  Low-level exception handlers and MMU support
+ *  rewritten by Paul Mackerras.
+ *    Copyright (C) 1996 Paul Mackerras.
+ *  MPC8xx modifications Copyright (C) 1997 Dan Malek (dmalek@jlc.net).
+ *  
+ *  Fork from entry_32.S for Hypervisor/Guest, Copyright (C) 2009
+ *  Wind River Systems, Inc.
+ *
+ *  This file contains the system call entry code, context switch
+ *  code, and exception/interrupt return code for PowerPC.
+ *
+ *  This program is free software; you can redistribute it and/or
+ *  modify it under the terms of the GNU General Public License
+ *  as published by the Free Software Foundation; either version
+ *  2 of the License, or (at your option) any later version.
+ *
+ */
+
+#include <linux/errno.h>
+#include <linux/sys.h>
+#include <linux/threads.h>
+#include <asm/reg.h>
+#include <asm/page.h>
+#include <asm/mmu.h>
+#include <asm/cputable.h>
+#include <asm/thread_info.h>
+#include <asm/ppc_asm.h>
+#include <asm/asm-offsets.h>
+#include <asm/unistd.h>
+#include <asm/ftrace.h>
+#ifdef CONFIG_WRHV
+#include <vbi/interface.h>
+#include <asm/arch_vbi.h>
+#include <vbi/syscalls.h>
+#endif /* CONFIG_WRHV */
+
+#undef SHOW_SYSCALLS
+#undef SHOW_SYSCALLS_TASK
+#ifdef	CONFIG_WRHV
+#undef VMMU  /* just for debugging */
+#endif /* CONFIG_WRHV */
+
+#ifdef	CONFIG_WRHV
+	.data
+	.globl	wrhv_sprg3
+wrhv_sprg3:
+	.long	0
+	.globl	wrhv_supervisor
+wrhv_supervisor:
+	.long	1
+#ifdef CONFIG_SMP
+	.globl wrhv_pir
+wrhv_pir:
+	.long	0
+#endif
+	.text
+#endif	/* CONFIG_WRHV */
+
+/*
+ * MSR_KERNEL is > 0x10000 on 4xx/Book-E since it include MSR_CE.
+ */
+#if MSR_KERNEL >= 0x10000
+#define LOAD_MSR_KERNEL(r, x)	lis r,(x)@h; ori r,r,(x)@l
+#else
+#define LOAD_MSR_KERNEL(r, x)	li r,(x)
+#endif
+
+	.globl	paravirt_transfer_to_handler
+paravirt_transfer_to_handler:
+	stw	r2,GPR2(r11)
+	stw	r12,_NIP(r11)
+	stw	r9,_MSR(r11)
+	andi.	r2,r9,MSR_PR
+	mfctr	r12
+	mfspr	r2,SPRN_XER
+	stw	r12,_CTR(r11)
+	stw	r2,_XER(r11)
+	WRHV_MFSPRG3(r12)
+	addi	r2,r12,-THREAD
+	tovirt(r2,r2)			/* set r2 to current */
+	beq	2f			/* if from user, fix up THREAD.regs */
+	addi	r11,r1,STACK_FRAME_OVERHEAD
+	stw	r11,PT_REGS(r12)
+#if defined(CONFIG_40x) || defined(CONFIG_BOOKE) && !defined(CONFIG_WRHV)
+	/* Check to see if the dbcr0 register is set up to debug.  Use the
+	   internal debug mode bit to do this. */
+	lwz	r12,THREAD_DBCR0(r12)
+	andis.	r12,r12,DBCR0_IDM@h
+	beq+	3f
+	/* From user and task is ptraced - load up global dbcr0 */
+	li	r12,-1			/* clear all pending debug events */
+	mtspr	SPRN_DBSR,r12
+	lis	r11,global_dbcr0@ha
+	tophys(r11,r11)
+	addi	r11,r11,global_dbcr0@l
+#ifdef CONFIG_SMP
+	rlwinm	r9,r1,0,0,(31-THREAD_SHIFT)
+	lwz	r9,TI_CPU(r9)
+	slwi	r9,r9,3
+	add	r11,r11,r9
+#endif
+	lwz	r12,0(r11)
+	mtspr	SPRN_DBCR0,r12
+	lwz	r12,4(r11)
+	addi	r12,r12,-1
+	stw	r12,4(r11)
+#endif
+	b	3f
+
+2:	/* if from kernel, check interrupted DOZE/NAP mode and
+         * check for stack overflow
+         */
+	lwz	r9,KSP_LIMIT(r12)
+	cmplw	r1,r9			/* if r1 <= ksp_limit */
+	ble-	paravirt_stack_ovf		/* then the kernel stack overflowed */
+5:
+#if defined(CONFIG_6xx) || defined(CONFIG_E500)
+	rlwinm	r9,r1,0,0,31-THREAD_SHIFT
+	tophys(r9,r9)			/* check local flags */
+	lwz	r12,TI_LOCAL_FLAGS(r9)
+	mtcrf	0x01,r12
+	bt-	31-TLF_NAPPING,4f
+	bt-	31-TLF_SLEEPING,7f
+#endif /* CONFIG_6xx || CONFIG_E500 */
+	.globl paravirt_transfer_to_handler_cont
+paravirt_transfer_to_handler_cont:
+3:
+	mflr	r9
+	lwz	r11,0(r9)		/* virtual address of handler */
+	lwz	r9,4(r9)		/* where to go when done */
+	mtlr	r9
+	lis	r9,wr_control@ha
+	lwz	r9,wr_control@l(r9)
+	stw	r11,VB_CONTROL_SRR0(r9)
+	mfcr	r11
+	stw	r11,VB_CONTROL_CR(r9)
+	stw	r0,VB_CONTROL_R0(r9)
+
+	lis	r12,wr_status@ha
+	lwz	r12,wr_status@l(r12)
+
+	lwz	r11,VB_STATUS_OLD_INT_DISABLE(r12)
+	stw	r11,VB_CONTROL_NEW_INT_DISABLE(r9)
+
+/*
+	lwz	r11,VB_STATUS_CR(r12)
+	stw	r11,VB_CONTROL_CR(r9)
+*/
+
+#ifdef VMMU
+        /* restore vmmu from wr_status to wr_control */
+
+	lwz	r11,VB_STATUS_VMMU0(r12)
+	stw	r11,VB_CONTROL_VMMU0(r9)
+
+	lwz	r11,VB_STATUS_VMMU1(r12)
+	stw	r11,VB_CONTROL_VMMU1(r9)
+
+	lwz	r11,VB_STATUS_EMSR(r12)
+	stw	r11,VB_CONTROL_EMSR(r9)
+
+	stw	r1,VB_CONTROL_SP(r9)
+	stw	r2,VB_CONTROL_R2(r9)
+	stw	r3,VB_CONTROL_R3(r9)
+	stw	r4,VB_CONTROL_R4(r9)
+	stw	r5,VB_CONTROL_R5(r9)
+	stw	r6,VB_CONTROL_R6(r9)
+	stw	r7,VB_CONTROL_R7(r9)
+	stw	r8,VB_CONTROL_R8(r9)
+	stw	r10,VB_CONTROL_R10(r9)
+	mflr	r11
+#endif
+
+	WRHV_LOAD_MSR(r10,r9,r11)
+
+#ifdef VMMU
+        /* re-enable vmmu */
+
+	lis	r0,VBI_SYS_ctx_load_vmmu@h
+	ori	r0,r0,VBI_SYS_ctx_load_vmmu@l
+	sc
+
+#else
+	lis	r0,VBI_SYS_ctx_load@h
+	ori	r0,r0,VBI_SYS_ctx_load@l
+	sc
+#endif /* VMMU */
+
+#if defined (CONFIG_6xx) || defined(CONFIG_E500)
+4:	rlwinm	r12,r12,0,~_TLF_NAPPING
+	stw	r12,TI_LOCAL_FLAGS(r9)
+	b	power_save_ppc32_restore
+
+7:	rlwinm	r12,r12,0,~_TLF_SLEEPING
+	stw	r12,TI_LOCAL_FLAGS(r9)
+	lwz	r9,_MSR(r11)		/* if sleeping, clear MSR.EE */
+	rlwinm	r9,r9,0,~MSR_EE
+	lwz	r12,_LINK(r11)		/* and return to address in LR */
+	b	fast_exception_return
+#endif
+
+/*
+ * On kernel stack overflow, load up an initial stack pointer
+ * and call StackOverflow(regs), which should not return.
+ */
+paravirt_stack_ovf:
+	/* sometimes we use a statically-allocated stack, which is OK. */
+	lis	r12,_end@h
+	ori	r12,r12,_end@l
+	cmplw	r1,r12
+	ble	5b			/* r1 <= &_end is OK */
+	SAVE_NVGPRS(r11)
+	addi	r3,r1,STACK_FRAME_OVERHEAD
+	lis	r1,init_thread_union@ha
+	addi	r1,r1,init_thread_union@l
+	addi	r1,r1,THREAD_SIZE-STACK_FRAME_OVERHEAD
+	lis	r9,StackOverflow@ha
+	addi	r9,r9,StackOverflow@l
+	LOAD_MSR_KERNEL(r10,MSR_KERNEL)
+	FIX_SRR1(r10,r12)
+	mtspr	SPRN_SRR0,r9
+	mtspr	SPRN_SRR1,r10
+	SYNC
+	RFI
+
+
+	.globl	paravirt_ret_from_syscall
+paravirt_ret_from_syscall:
+#ifdef SHOW_SYSCALLS
+	bl	do_show_syscall_exit
+#endif
+	mr	r6,r3
+	rlwinm	r12,r1,0,0,(31-THREAD_SHIFT)	/* current_thread_info() */
+	/* disable interrupts so current_thread_info()->flags can't change */
+	WRHV_INT_LOCK(r10,r9)
+	lwz	r9,TI_FLAGS(r12)
+	li	r8,-_LAST_ERRNO
+	andi.	r0,r9,(_TIF_SYSCALL_T_OR_A|_TIF_SINGLESTEP|_TIF_USER_WORK_MASK|_TIF_PERSYSCALL_MASK)
+	bne-	syscall_exit_work
+	cmplw	0,r3,r8
+	blt+	paravirt_syscall_exit_cont
+	lwz	r11,_CCR(r1)			/* Load CR */
+	neg	r3,r3
+	oris	r11,r11,0x1000	/* Set SO bit in CR */
+	stw	r11,_CCR(r1)
+paravirt_syscall_exit_cont:
+#if defined(CONFIG_4xx) || defined(CONFIG_BOOKE) && !(CONFIG_WRHV)
+	/* If the process has its own DBCR0 value, load it up.  The internal
+	   debug mode bit tells us that dbcr0 should be loaded. */
+	lwz	r0,THREAD+THREAD_DBCR0(r2)
+	andis.	r10,r0,DBCR0_IDM@h
+	bnel-	load_dbcr0
+#endif
+#ifdef CONFIG_44x
+	lis	r4,icache_44x_need_flush@ha
+	lwz	r5,icache_44x_need_flush@l(r4)
+	cmplwi	cr0,r5,0
+	bne-	2f
+1:
+#endif /* CONFIG_44x */
+BEGIN_FTR_SECTION
+	lwarx	r7,0,r1
+END_FTR_SECTION_IFSET(CPU_FTR_NEED_PAIRED_STWCX)
+	stwcx.	r0,0,r1			/* to clear the reservation */
+	lwz	r4,_LINK(r1)
+	lwz	r5,_CCR(r1)
+	mtlr	r4
+	lis	r4,wr_control@ha
+	lwz	r4,wr_control@l(r4)
+	stw	r5,VB_CONTROL_CR(r4)
+	lwz	r5,GPR0(r1)
+	stw	r5,VB_CONTROL_R0(r4)
+	lwz	r5,_NIP(r1)
+	stw	r5,VB_CONTROL_SRR0(r4)
+
+	lis	r5,wr_status@ha
+	lwz	r5,wr_status@l(r5)
+	lwz	r5,VB_STATUS_OLD_INT_DISABLE(r5)
+	stw	r5,VB_CONTROL_NEW_INT_DISABLE(r4)
+ 
+	lwz	r5,_MSR(r1)
+	WRHV_LOAD_MSR(r5,r7,r8)
+	lwz	r2,GPR2(r1)
+	lwz	r1,GPR1(r1)
+	lis	r0,VBI_SYS_ctx_load@h
+	sc
+
+66:	li	r3,-ENOSYS
+	b	ret_from_syscall
+
+	.globl	paravirt_syscall_exit_work
+paravirt_syscall_exit_work:
+	andi.	r0,r9,_TIF_RESTOREALL
+	beq+	0f
+	REST_NVGPRS(r1)
+	b	2f
+0:	cmplw	0,r3,r8
+	blt+	1f
+	andi.	r0,r9,_TIF_NOERROR
+	bne-	1f
+	lwz	r11,_CCR(r1)			/* Load CR */
+	neg	r3,r3
+	oris	r11,r11,0x1000	/* Set SO bit in CR */
+	stw	r11,_CCR(r1)
+
+1:	stw	r6,RESULT(r1)	/* Save result */
+	stw	r3,GPR3(r1)	/* Update return value */
+2:	andi.	r0,r9,(_TIF_PERSYSCALL_MASK)
+	beq	4f
+
+	/* Clear per-syscall TIF flags if any are set.  */
+
+	li	r11,_TIF_PERSYSCALL_MASK
+	addi	r12,r12,TI_FLAGS
+3:	lwarx	r8,0,r12
+	andc	r8,r8,r11
+#ifdef CONFIG_IBM405_ERR77
+	dcbt	0,r12
+#endif
+	stwcx.	r8,0,r12
+	bne-	3b
+	subi	r12,r12,TI_FLAGS
+	
+4:	/* Anything which requires enabling interrupts? */
+	andi.	r0,r9,(_TIF_SYSCALL_T_OR_A|_TIF_SINGLESTEP)
+	beq	ret_from_except
+
+	/* Re-enable interrupts */
+	WRHV_INT_UNLOCK(r10,r4)
+
+	/* Save NVGPRS if they're not saved already */
+	lwz	r4,_TRAP(r1)
+	andi.	r4,r4,1
+	beq	5f
+	SAVE_NVGPRS(r1)
+	li	r4,0xc00
+	stw	r4,_TRAP(r1)
+5:
+	addi	r3,r1,STACK_FRAME_OVERHEAD
+	bl	do_syscall_trace_leave
+	b	ret_from_except_full
+
+
+/*
+ * This routine switches between two different tasks.  The process
+ * state of one is saved on its kernel stack.  Then the state
+ * of the other is restored from its kernel stack.  The memory
+ * management hardware is updated to the second process's state.
+ * Finally, we can return to the second process.
+ * On entry, r3 points to the THREAD for the current task, r4
+ * points to the THREAD for the new task.
+ *
+ * This routine is always called with interrupts disabled.
+ *
+ * Note: there are two ways to get to the "going out" portion
+ * of this code; either by coming in via the entry (_switch)
+ * or via "fork" which must set up an environment equivalent
+ * to the "_switch" path.  If you change this , you'll have to
+ * change the fork code also.
+ *
+ * The code which creates the new task context is in 'copy_thread'
+ * in arch/ppc/kernel/process.c
+ */
+_GLOBAL(paravirt_switch)
+	stwu	r1,-INT_FRAME_SIZE(r1)
+	mflr	r0
+	stw	r0,INT_FRAME_SIZE+4(r1)
+	/* r3-r12 are caller saved -- Cort */
+	SAVE_NVGPRS(r1)
+	stw	r0,_NIP(r1)	/* Return to switch caller */
+	LOAD_MSR_KERNEL(r11,MSR_KERNEL)
+	WRHV_FIX_MSR(r11,r10)
+1:	stw	r11,_MSR(r1)
+	mfcr	r10
+	stw	r10,_CCR(r1)
+	stw	r1,KSP(r3)	/* Set old stack pointer */
+
+#ifdef CONFIG_SMP
+	/* We need a sync somewhere here to make sure that if the
+	 * previous task gets rescheduled on another CPU, it sees all
+	 * stores it has performed on this one.
+	 */
+	sync
+#endif /* CONFIG_SMP */
+
+	tophys(r0,r4)
+	CLR_TOP32(r0)
+	WRHV_MTSPRG3(r0,r3)
+	lwz	r1,KSP(r4)	/* Load new stack pointer */
+
+	/* save the old current 'last' for return value */
+	mr	r3,r2
+	addi	r2,r4,-THREAD	/* Update current */
+
+	lwz	r0,_CCR(r1)
+	mtcrf	0xFF,r0
+	/* r3-r12 are destroyed -- Cort */
+	REST_NVGPRS(r1)
+
+	lwz	r4,_NIP(r1)	/* Return to _switch caller in new task */
+	mtlr	r4
+	addi	r1,r1,INT_FRAME_SIZE
+	blr
+
+
+	/* interrupts are hard-disabled at this point */
+	.globl	paravirt_restore
+paravirt_restore:
+#ifdef	CONFIG_WRHV
+	lis	r4,wr_control@ha
+	lwz	r4,wr_control@l(r4)
+	lwz	r0,GPR0(r1)
+	stw	r0,VB_CONTROL_R0(r4)
+	lwz	r2,GPR2(r1)
+	lwz	r3,GPR3(r1)
+	lwz	r6,GPR6(r1)
+	lwz	r7,GPR7(r1)
+	lwz	r8,GPR8(r1)
+	lwz	r9,GPR9(r1)
+	lwz	r10,GPR10(r1)
+	lwz	r11,GPR11(r1)
+
+	lis	r12,wr_status@ha
+	lwz	r12,wr_status@l(r12)
+	lwz	r5,VB_STATUS_OLD_INT_DISABLE(r12)
+	stw	r5,VB_CONTROL_NEW_INT_DISABLE(r4)
+
+#ifdef VMMU
+	stw	r2,VB_CONTROL_R2(r4)
+	stw	r3,VB_CONTROL_R3(r4)
+	stw	r6,VB_CONTROL_R6(r4)
+	stw	r7,VB_CONTROL_R7(r4)
+	stw	r8,VB_CONTROL_R8(r4)
+	stw	r9,VB_CONTROL_R9(r4)
+	stw	r10,VB_CONTROL_R10(r4)
+	stw	r11,VB_CONTROL_R11(r4)
+#endif /* VMMU */
+	lwz	r0,_CCR(r1)
+	stw	r0,VB_CONTROL_CR(r4)
+	lwz	r0,_NIP(r1)
+	stw	r0,VB_CONTROL_SRR0(r4)
+	lwz	r0,_LINK(r1)
+	mtlr	r0
+#if 0
+	stw	r0,VB_CONTROL_LR(r4)
+#endif
+	lwz	r0,_XER(r1)
+	mtspr	SPRN_XER,r0
+	lwz	r0,_CTR(r1)
+	mtctr	r0
+	lwz	r0,_MSR(r1)
+	WRHV_LOAD_MSR(r0,r12,r5)
+	lwz	r12,GPR12(r1)
+	lwz	r5,GPR5(r1)
+#ifdef VMMU
+	stw	r12,VB_CONTROL_R12(r4)
+	stw	r5,VB_CONTROL_R5(r4)
+
+	lwz	r5,GPR1(r1)
+	stw	r5,VB_CONTROL_R1(r4)
+
+	lwz	r5,GPR4(r1)
+	stw	r5,VB_CONTROL_R4(r4)
+
+	lis	r12,wr_status@ha
+	lwz	r12,wr_status@l(r12)
+
+	lwz	r5,VB_STATUS_EMSR(r12)
+	stw	r5,VB_CONTROL_EMSR(r4)
+
+#if 1
+        /* resume VMMU, since we always turn VMMU back on during the
+         * exception entrance, this is really not needed, but we
+         * make them here anyway for consistency
+         */
+	lwz	r5,VB_STATUS_VMMU0(r12)
+	stw	r5,VB_CONTROL_VMMU0(r4)
+
+	lwz	r5,VB_STATUS_VMMU1(r12)
+	stw	r5,VB_CONTROL_VMMU1(r4)
+#endif
+
+	lis	r0,VBI_SYS_ctx_load_vmmu@h
+	ori	r0,r0,VBI_SYS_ctx_load_vmmu@l
+#else
+	lwz	r4,GPR4(r1)
+	lwz	r1,GPR1(r1)
+	lis	r0,VBI_SYS_ctx_load@h
+#endif  /* VMMU */
+	sc
+
+	/* Never back from here */
+#endif /* CONFIG_WRHV */
+
diff --git a/arch/powerpc/kernel/wrhv_misc_32.S b/arch/powerpc/kernel/wrhv_misc_32.S
new file mode 100644
index 0000000..3f96eee
--- /dev/null
+++ b/arch/powerpc/kernel/wrhv_misc_32.S
@@ -0,0 +1,90 @@
+/*
+ * Low level asm functions for guest implementation on powerpc
+ * 
+ * Copyright (c) 2009 Wind River Systems, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ *
+ */
+
+#include <linux/sys.h>
+#include <asm/unistd.h>
+#include <asm/errno.h>
+#include <asm/reg.h>
+#include <asm/page.h>
+#include <asm/cache.h>
+#include <asm/cputable.h>
+#include <asm/mmu.h>
+#include <asm/ppc_asm.h>
+#include <asm/thread_info.h>
+#include <asm/asm-offsets.h>
+#include <asm/processor.h>
+#include <vbi/interface.h>
+#include <vbi/syscalls.h>
+
+	.text
+
+	.align	5
+_GLOBAL(wrhv_int_lock)
+	WRHV_INT_LOCK(r4,r5)
+	blr
+
+_GLOBAL(wrhv_int_lvl_get)
+	WRHV_INT_LVL_GET(r3)
+	blr
+
+_GLOBAL(wrhv_int_unlock)
+	WRHV_INT_UNLOCK(r3,r4)
+	blr
+
+/*
+ * Flush MMU TLB
+ */
+_GLOBAL(paravirt_tlbia)
+	lis	r0, VBI_SYS_tlb_flush@h
+	ori	r0, r0, VBI_SYS_tlb_flush@l
+	sc
+	blr
+
+/*
+ * Flush MMU TLB for a particular address
+ */
+_GLOBAL(paravirt_tlbie)
+	lis	r0, VBI_SYS_tlb_flush@h
+	ori	r0, r0, VBI_SYS_tlb_flush@l
+	sc
+	blr
+
+/*
+ * Write any modified data cache blocks out to memory.
+ * Does not invalidate the corresponding cache lines (especially for
+ * any corresponding instruction cache).
+ *
+ * clean_dcache_range(unsigned long start, unsigned long stop)
+ */
+_GLOBAL(paravirt_clean_dcache_range)
+	/*
+	 * vbi_flush_dcache (void *start_addr, void *end_addr)
+	 */
+	#li	r5, 8
+	subf	r4,r3,r4
+	addi	r4, r4, 1
+	bl	vbi_flush_dcache
+	blr
+
+_GLOBAL(paravirt__flush_dcache_icache)
+	b	vb__flush_dcache_icache
+
+_GLOBAL(paravirt_flush_dcache_range)
+	b	vb_flush_dcache_range
+
+_GLOBAL(paravirt__flush_icache_range)
+	b	vb__flush_icache_range
+
+_GLOBAL(paravirt__flush_dcache_icache_phys)
+	b	vb__flush_dcache_icache_phys
+
+
-- 
1.6.5.2

