From 4c398320d68e054fba8b53f9043f1fa1e1810fd9 Mon Sep 17 00:00:00 2001
From: Weixing Shi <Weixing.Shi@windriver.com>
Date: Wed, 2 Dec 2009 13:26:53 +0800
Subject: [PATCH] sched: Fix granularity of task_u/stime()

Commit-ID:  761b1d26df542fd5eb348837351e4d2f3bc7bffe upstream
Gitweb:     http://git.kernel.org/tip/761b1d26df542fd5eb348837351e4d2f3bc7bffe

author	Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
	Thu, 12 Nov 2009 04:33:45 +0000 (13:33 +0900)
committer Ingo Molnar <mingo@elte.hu>
	Thu, 12 Nov 2009 14:23:47 +0000 (15:23 +0100)

Originally task_s/utime() were designed to return clock_t but
later changed to return cputime_t by following commit:

  commit efe567fc8281661524ffa75477a7c4ca9b466c63
  Author: Christian Borntraeger <borntraeger@de.ibm.com>
  Date:   Thu Aug 23 15:18:02 2007 +0200

It only changed the type of return value, but not the
implementation. As the result the granularity of task_s/utime()
is still that of clock_t, not that of cputime_t.

So using task_s/utime() in __exit_signal() makes values
accumulated to the signal struct to be rounded and coarse
grained.

This patch removes casts to clock_t in task_u/stime(), to keep
granularity of cputime_t over the calculation.

v2:
  Use div_u64() to avoid error "undefined reference to `__udivdi3`"
  on some 32bit systems.

Signed-off-by: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
Acked-by: Peter Zijlstra <peterz@infradead.org>
Cc: xiyou.wangcong@gmail.com
Cc: Spencer Candland <spencer@bluehost.com>
Cc: Oleg Nesterov <oleg@redhat.com>
Cc: Stanislaw Gruszka <sgruszka@redhat.com>
LKML-Reference: <4AFB9029.9000208@jp.fujitsu.com>
Signed-off-by: Ingo Molnar <mingo@elte.hu>
integrated-by: weixing.shi(weixing.shi@windriver.com)
---
 kernel/sched.c |   60 ++++++++++++++++++++++++++++----------------------------
 1 files changed, 30 insertions(+), 30 deletions(-)

diff --git a/kernel/sched.c b/kernel/sched.c
index a466d97..386331b 100644
--- a/kernel/sched.c
+++ b/kernel/sched.c
@@ -4187,43 +4187,43 @@ cputime_t task_stime(struct task_struct *p)
 	return p->stime;
 }
 #else
-cputime_t task_utime(struct task_struct *p)
-{
-	clock_t utime = cputime_to_clock_t(p->utime),
-		total = utime + cputime_to_clock_t(p->stime);
-	u64 temp;
 
-	/*
-	 * Use CFS's precise accounting:
-	 */
-	temp = (u64)nsec_to_clock_t(p->se.sum_exec_runtime);
+#ifndef nsecs_to_cputime
+# define nsecs_to_cputime(__nsecs) \
+        msecs_to_cputime(div_u64((__nsecs), NSEC_PER_MSEC))
+#endif
 
-	if (total) {
-		temp *= utime;
-		do_div(temp, total);
-	}
-	utime = (clock_t)temp;
+cputime_t task_utime(struct task_struct *p)
+{
+        cputime_t utime = p->utime, total = utime + p->stime;
+        u64 temp;
 
-	p->prev_utime = max(p->prev_utime, clock_t_to_cputime(utime));
-	return p->prev_utime;
+        /*
+         * Use CFS's precise accounting:
+         */
+        temp = (u64)nsecs_to_cputime(p->se.sum_exec_runtime);
+        if (total) {
+                temp *= utime;
+                do_div(temp, total);
+        }
+	utime = (cputime_t)temp;
+        p->prev_utime = max(p->prev_utime,utime);
+        return p->prev_utime;
 }
 
 cputime_t task_stime(struct task_struct *p)
 {
-	clock_t stime;
-
-	/*
-	 * Use CFS's precise accounting. (we subtract utime from
-	 * the total, to make sure the total observed by userspace
-	 * grows monotonically - apps rely on that):
-	 */
-	stime = nsec_to_clock_t(p->se.sum_exec_runtime) -
-			cputime_to_clock_t(task_utime(p));
-
-	if (stime >= 0)
-		p->prev_stime = max(p->prev_stime, clock_t_to_cputime(stime));
-
-	return p->prev_stime;
+	cputime_t stime;
+        /*
+         * Use CFS's precise accounting. (we subtract utime from
+         * the total, to make sure the total observed by userspace
+         * grows monotonically - apps rely on that):
+         */
+	stime = nsecs_to_cputime(p->se.sum_exec_runtime) - task_utime(p);
+        if (stime >= 0)
+                p->prev_stime = max(p->prev_stime, stime);
+
+        return p->prev_stime;
 }
 #endif
 
-- 
1.6.5.2

