From 9beb150ed0b4cd35b71ae775fb868180ab7e6388 Mon Sep 17 00:00:00 2001
From: Zhang Xiao <xiao.zhang@windriver.com>
Date: Mon, 26 Dec 2011 13:26:16 +0800
Subject: [PATCH] net: sk_add_backlog() take rmem_alloc into account

commit: c377411f2494a931ff7facdbb3a6839b1266bcf6 upstream

Current socket backlog limit is not enough to really stop DDOS attacks,
because user thread spend many time to process a full backlog each
round, and user might crazy spin on socket lock.

We should add backlog size and receive_queue size (aka rmem_alloc) to
pace writers, and let user run without being slow down too much.

Introduce a sk_rcvqueues_full() helper, to avoid taking socket lock in
stress situations.

Under huge stress from a multiqueue/RPS enabled NIC, a single flow udp
receiver can now process ~200.000 pps (instead of ~100 pps before the
patch) on a 8 core machine.

Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
Signed-off-by: David S. Miller <davem@davemloft.net>
Integrated-by: Zhang Xiao <xiao.zhang@windriver.com>
---
 include/net/sock.h |   13 +++++++++++--
 net/core/sock.c    |    6 +++++-
 net/ipv4/udp.c     |    3 +++
 net/ipv6/udp.c     |   23 +++++++++++++++++++++++
 net/sctp/socket.c  |    3 ---
 5 files changed, 42 insertions(+), 6 deletions(-)

diff --git a/include/net/sock.h b/include/net/sock.h
index 2f05903..3724967 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -227,7 +227,6 @@ struct sock {
 		struct sk_buff *head;
 		struct sk_buff *tail;
 		int len;
-		int limit;
 	} sk_backlog;
 	wait_queue_head_t	*sk_sleep;
 	struct dst_entry	*sk_dst_cache;
@@ -484,10 +483,20 @@ static inline void sk_add_backlog(struct sock *sk, struct sk_buff *skb)
 	skb->next = NULL;
 }
 
+/*
+ * Take into account size of receive queue and backlog queue
+ */
+static inline bool sk_rcvqueues_full(const struct sock *sk, const struct sk_buff *skb)
+{
+	unsigned int qsize = sk->sk_backlog.len + atomic_read(&sk->sk_rmem_alloc);
+
+	return qsize + skb->truesize > sk->sk_rcvbuf;
+}
+
 /* The per-socket spinlock must be held here. */
 static inline __must_check int sk_add_backlog_limited(struct sock *sk, struct sk_buff *skb)
 {
-	if (sk->sk_backlog.len >= max(sk->sk_backlog.limit, sk->sk_rcvbuf << 1))
+	if (sk_rcvqueues_full(sk, skb))
 		return -ENOBUFS;
 	sk_add_backlog(sk, skb);
 	sk->sk_backlog.len += skb->truesize;
diff --git a/net/core/sock.c b/net/core/sock.c
index 562dd5d..619b8e1 100644
--- a/net/core/sock.c
+++ b/net/core/sock.c
@@ -314,6 +314,11 @@ int sk_receive_skb(struct sock *sk, struct sk_buff *skb, const int nested)
 
 	skb->dev = NULL;
 
+	if (sk_rcvqueues_full(sk, skb)) {
+		atomic_inc(&sk->sk_drops);
+		goto discard_and_relse;
+	}
+
 	if (nested)
 		bh_lock_sock_nested(sk);
 	else
@@ -1713,7 +1718,6 @@ void sock_init_data(struct socket *sock, struct sock *sk)
 	sk->sk_allocation	=	GFP_KERNEL;
 	sk->sk_rcvbuf		=	sysctl_rmem_default;
 	sk->sk_sndbuf		=	sysctl_wmem_default;
-	sk->sk_backlog.limit    =       sk->sk_rcvbuf << 1;
 	sk->sk_state		=	TCP_CLOSE;
 	sk_set_socket(sk, sock);
 
diff --git a/net/ipv4/udp.c b/net/ipv4/udp.c
index 440f7d8..87b6e28 100644
--- a/net/ipv4/udp.c
+++ b/net/ipv4/udp.c
@@ -1069,6 +1069,9 @@ int udp_queue_rcv_skb(struct sock * sk, struct sk_buff *skb)
 			goto drop;
 	}
 
+	if (sk_rcvqueues_full(sk, skb))
+		goto drop;
+
 	rc = 0;
 
 	bh_lock_sock(sk);
diff --git a/net/ipv6/udp.c b/net/ipv6/udp.c
index af8d8df..e9242c6 100644
--- a/net/ipv6/udp.c
+++ b/net/ipv6/udp.c
@@ -382,6 +382,15 @@ static int __udp6_lib_mcast_deliver(struct net *net, struct sk_buff *skb,
 					uh->source, saddr, dif))) {
 		struct sk_buff *buff = skb_clone(skb, GFP_ATOMIC);
 		if (buff) {
+			if (sk_rcvqueues_full(sk2, buff)) {
+				kfree_skb(buff);
+				atomic_inc(&sk2->sk_drops);
+				UDP6_INC_STATS_BH(sock_net(sk2),
+					UDP_MIB_RCVBUFERRORS, IS_UDPLITE(sk2));
+				UDP6_INC_STATS_BH(sock_net(sk2),
+					UDP_MIB_INERRORS, IS_UDPLITE(sk2));
+				continue;
+			}
 			bh_lock_sock(sk2);
 			if (!sock_owned_by_user(sk2))
 				udpv6_queue_rcv_skb(sk2, buff);
@@ -396,6 +405,15 @@ static int __udp6_lib_mcast_deliver(struct net *net, struct sk_buff *skb,
 			bh_unlock_sock(sk2);
 		}
 	}
+	if (sk_rcvqueues_full(sk, skb)) {
+		kfree_skb(skb);
+		atomic_inc(&sk->sk_drops);
+		UDP6_INC_STATS_BH(sock_net(sk),
+			UDP_MIB_RCVBUFERRORS, IS_UDPLITE(sk));
+		UDP6_INC_STATS_BH(sock_net(sk),
+			UDP_MIB_INERRORS, IS_UDPLITE(sk));
+		goto out;
+	}
 	bh_lock_sock(sk);
 	if (!sock_owned_by_user(sk))
 		udpv6_queue_rcv_skb(sk, skb);
@@ -523,6 +541,11 @@ int __udp6_lib_rcv(struct sk_buff *skb, struct hlist_head udptable[],
 
 	/* deliver */
 
+	if (sk_rcvqueues_full(sk, skb)) {
+		sock_put(sk);
+		goto discard;
+	}
+
 	bh_lock_sock(sk);
 	if (!sock_owned_by_user(sk))
 		udpv6_queue_rcv_skb(sk, skb);
diff --git a/net/sctp/socket.c b/net/sctp/socket.c
index 6ae32de..2fc26f8 100644
--- a/net/sctp/socket.c
+++ b/net/sctp/socket.c
@@ -3614,9 +3614,6 @@ SCTP_STATIC int sctp_init_sock(struct sock *sk)
 
 	SCTP_DBG_OBJCNT_INC(sock);
 
-	/* Set socket backlog limit. */
-	sk->sk_backlog.limit = sysctl_sctp_rmem[1];
-
 	atomic_inc(&sctp_sockets_allocated);
 	return 0;
 }
-- 
1.7.0.4

