From 719b1f4fef061e65dcae12ecdd92fd9e4db9b0aa Mon Sep 17 00:00:00 2001
From: Zhang Xiao <xiao.zhang@windriver.com>
Date: Mon, 26 Sep 2011 14:39:31 +0800
Subject: [PATCH 11/17] udp: use limited socket backlog

commit 55349790d7cbf0d381873a7ece1dcafcffd4aaa9 from upstream.

Make udp adapt to the limited socket backlog change.

Cc: "David S. Miller" <davem@davemloft.net>
Cc: Alexey Kuznetsov <kuznet@ms2.inr.ac.ru>
Cc: "Pekka Savola (ipv6)" <pekkas@netcore.fi>
Cc: Patrick McHardy <kaber@trash.net>
Signed-off-by: Zhu Yi <yi.zhu@intel.com>
Acked-by: Eric Dumazet <eric.dumazet@gmail.com>
Signed-off-by: David S. Miller <davem@davemloft.net>
Integrate-by: Zhang Xiao <xiao.zhang@windriver.com>
---
 net/ipv4/udp.c |    6 ++++--
 net/ipv6/udp.c |   28 ++++++++++++++++++++++------
 2 files changed, 26 insertions(+), 8 deletions(-)

diff --git a/net/ipv4/udp.c b/net/ipv4/udp.c
index 79d5e0b..440f7d8 100644
--- a/net/ipv4/udp.c
+++ b/net/ipv4/udp.c
@@ -1074,8 +1074,10 @@ int udp_queue_rcv_skb(struct sock * sk, struct sk_buff *skb)
 	bh_lock_sock(sk);
 	if (!sock_owned_by_user(sk))
 		rc = __udp_queue_rcv_skb(sk, skb);
-	else
-		sk_add_backlog(sk, skb);
+	else if (sk_add_backlog_limited(sk, skb)) {
+		bh_unlock_sock(sk);
+		goto drop;
+	}
 	bh_unlock_sock(sk);
 
 	return rc;
diff --git a/net/ipv6/udp.c b/net/ipv6/udp.c
index 560279c..af8d8df 100644
--- a/net/ipv6/udp.c
+++ b/net/ipv6/udp.c
@@ -385,16 +385,28 @@ static int __udp6_lib_mcast_deliver(struct net *net, struct sk_buff *skb,
 			bh_lock_sock(sk2);
 			if (!sock_owned_by_user(sk2))
 				udpv6_queue_rcv_skb(sk2, buff);
-			else
-				sk_add_backlog(sk2, buff);
+			else if (sk_add_backlog_limited(sk2, buff)) {
+				kfree_skb(buff);
+				atomic_inc(&sk2->sk_drops);
+				UDP6_INC_STATS_BH(sock_net(sk2),
+					UDP_MIB_RCVBUFERRORS, IS_UDPLITE(sk2));
+				UDP6_INC_STATS_BH(sock_net(sk2),
+					UDP_MIB_INERRORS, IS_UDPLITE(sk2));
+			}
 			bh_unlock_sock(sk2);
 		}
 	}
 	bh_lock_sock(sk);
 	if (!sock_owned_by_user(sk))
 		udpv6_queue_rcv_skb(sk, skb);
-	else
-		sk_add_backlog(sk, skb);
+	else if (sk_add_backlog_limited(sk, skb)) {
+		kfree_skb(skb);
+		atomic_inc(&sk->sk_drops);
+		UDP6_INC_STATS_BH(sock_net(sk),
+			UDP_MIB_RCVBUFERRORS, IS_UDPLITE(sk));
+		UDP6_INC_STATS_BH(sock_net(sk),
+			UDP_MIB_INERRORS, IS_UDPLITE(sk));
+	}
 	bh_unlock_sock(sk);
 out:
 	read_unlock(&udp_hash_lock);
@@ -514,8 +526,12 @@ int __udp6_lib_rcv(struct sk_buff *skb, struct hlist_head udptable[],
 	bh_lock_sock(sk);
 	if (!sock_owned_by_user(sk))
 		udpv6_queue_rcv_skb(sk, skb);
-	else
-		sk_add_backlog(sk, skb);
+	else if (sk_add_backlog_limited(sk, skb)) {
+		atomic_inc(&sk->sk_drops);
+		bh_unlock_sock(sk);
+		sock_put(sk);
+		goto discard;
+	}
 	bh_unlock_sock(sk);
 	sock_put(sk);
 	return 0;
-- 
1.7.0.4

