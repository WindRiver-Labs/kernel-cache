From bb3e8280b2998e996b6718ef255654e7ced63ef0 Mon Sep 17 00:00:00 2001
From: Mark Asselstine <mark.asselstine@windriver.com>
Date: Mon, 28 Apr 2014 09:28:19 -0400
Subject: [PATCH] openvswitch: create openvswitch-nv to host the out of tree
 ovs src

The out of tree kernel module code maintained by the Open vSwitch project is
both newer and more functional then what is found in the kernel. Merge the out
of tree code in to the kernel to avoid the downsides of an out of tree kernel
module and allow people to use the newer functions found in this source base.

This is based off of OVS v1.9.3.

Signed-off-by: Mark Asselstine <mark.asselstine@windriver.com>

diff --git a/drivers/staging/Kconfig b/drivers/staging/Kconfig
index 069c8eeff99a..0464970d63f7 100644
--- a/drivers/staging/Kconfig
+++ b/drivers/staging/Kconfig
@@ -136,4 +136,6 @@ source "drivers/staging/ozwpan/Kconfig"
 
 source "drivers/staging/netmap/Kconfig"
 
+source "drivers/staging/openvswitch_nv/Kconfig"
+
 endif # STAGING
diff --git a/drivers/staging/Makefile b/drivers/staging/Makefile
index e8943cabecd0..e0f3121d36fe 100644
--- a/drivers/staging/Makefile
+++ b/drivers/staging/Makefile
@@ -59,3 +59,4 @@ obj-$(CONFIG_PHONE)		+= telephony/
 obj-$(CONFIG_RAMSTER)		+= ramster/
 obj-$(CONFIG_USB_WPAN_HCD)	+= ozwpan/
 obj-$(CONFIG_NETMAP)		+= netmap/
+obj-$(CONFIG_OPENVSWITCH_NV)	+= openvswitch_nv/
diff --git a/drivers/staging/openvswitch_nv/Kconfig b/drivers/staging/openvswitch_nv/Kconfig
new file mode 100644
index 000000000000..7c84ef9ecaa4
--- /dev/null
+++ b/drivers/staging/openvswitch_nv/Kconfig
@@ -0,0 +1,32 @@
+#
+# Open vSwitch
+#
+
+config OPENVSWITCH_NV
+	tristate "Open vSwitch (newer version)"
+	depends on OPENVSWITCH=n && BRIDGE=m && EXPERIMENTAL && STAGING
+	---help---
+	  Open vSwitch is a multilayer Ethernet switch targeted at virtualized
+	  environments.  In addition to supporting a variety of features
+	  expected in a traditional hardware switch, it enables fine-grained
+	  programmatic extension and flow-based control of the network.  This
+	  control is useful in a wide variety of applications but is
+	  particularly important in multi-server virtualization deployments,
+	  which are often characterized by highly dynamic endpoints and the
+	  need to maintain logical abstractions for multiple tenants.
+
+	  The Open vSwitch datapath provides an in-kernel fast path for packet
+	  forwarding.  It is complemented by a userspace daemon, ovs-vswitchd,
+	  which is able to accept configuration from a variety of sources and
+	  translate it into packet processing rules.
+
+	  See http://openvswitch.org for more information and userspace
+	  utilities.
+
+	  This enables the latest Open vSwitch code taken from the out of tree
+	  kernel module.
+
+	  To compile this code as a module, choose M here: the module will be
+	  called openvswitch.
+
+	  If unsure, say N.
diff --git a/drivers/staging/openvswitch_nv/Makefile b/drivers/staging/openvswitch_nv/Makefile
new file mode 100644
index 000000000000..756b0a33ded0
--- /dev/null
+++ b/drivers/staging/openvswitch_nv/Makefile
@@ -0,0 +1,22 @@
+#
+# Makefile for Open vSwitch (newer version)
+#
+
+obj-$(CONFIG_OPENVSWITCH_NV) += openvswitch.o
+
+openvswitch-y := \
+	actions.o \
+	datapath.o \
+	dp_notify.o \
+	flow.o \
+	vport.o \
+	vport-internal_dev.o \
+	vport-netdev.o \
+	vport-capwap.o \
+	vport-generic.o \
+	vport-gre.o \
+	tunnel.o \
+	genl_exec.o \
+	dp_sysfs_if.o \
+	vport-patch.o \
+	dp_sysfs_dp.o
diff --git a/drivers/staging/openvswitch_nv/actions.c b/drivers/staging/openvswitch_nv/actions.c
new file mode 100644
index 000000000000..392dac370d7a
--- /dev/null
+++ b/drivers/staging/openvswitch_nv/actions.c
@@ -0,0 +1,606 @@
+/*
+ * Copyright (c) 2007-2012 Nicira, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/skbuff.h>
+#include <linux/in.h>
+#include <linux/ip.h>
+#include <linux/openvswitch-nv.h>
+#include <linux/tcp.h>
+#include <linux/udp.h>
+#include <linux/in6.h>
+#include <linux/if_arp.h>
+#include <linux/if_vlan.h>
+#include <linux/netfilter_ipv6/ip6_tables.h>
+#include <net/ip.h>
+#include <net/ipv6.h>
+#include <net/checksum.h>
+#include <net/dsfield.h>
+
+#include "checksum.h"
+#include "datapath.h"
+#include "vlan.h"
+#include "vport.h"
+
+static int do_execute_actions(struct datapath *dp, struct sk_buff *skb,
+			      const struct nlattr *attr, int len,
+			      struct ovs_key_ipv4_tunnel *tun_key, bool keep_skb);
+
+static int make_writable(struct sk_buff *skb, int write_len)
+{
+	if (!skb_cloned(skb) || skb_clone_writable(skb, write_len))
+		return 0;
+
+	return pskb_expand_head(skb, 0, 0, GFP_ATOMIC);
+}
+
+/* remove VLAN header from packet and update csum accordingly. */
+static int __pop_vlan_tci(struct sk_buff *skb, __be16 *current_tci)
+{
+	struct vlan_hdr *vhdr;
+	int err;
+
+	err = make_writable(skb, VLAN_ETH_HLEN);
+	if (unlikely(err))
+		return err;
+
+	if (get_ip_summed(skb) == OVS_CSUM_COMPLETE)
+		skb->csum = csum_sub(skb->csum, csum_partial(skb->data
+					+ (2 * ETH_ALEN), VLAN_HLEN, 0));
+
+	vhdr = (struct vlan_hdr *)(skb->data + ETH_HLEN);
+	*current_tci = vhdr->h_vlan_TCI;
+
+	memmove(skb->data + VLAN_HLEN, skb->data, 2 * ETH_ALEN);
+	__skb_pull(skb, VLAN_HLEN);
+
+	vlan_set_encap_proto(skb, vhdr);
+	skb->mac_header += VLAN_HLEN;
+	skb_reset_mac_len(skb);
+
+	return 0;
+}
+
+static int pop_vlan(struct sk_buff *skb)
+{
+	__be16 tci;
+	int err;
+
+	if (likely(vlan_tx_tag_present(skb))) {
+		vlan_set_tci(skb, 0);
+	} else {
+		if (unlikely(skb->protocol != htons(ETH_P_8021Q) ||
+			     skb->len < VLAN_ETH_HLEN))
+			return 0;
+
+		err = __pop_vlan_tci(skb, &tci);
+		if (err)
+			return err;
+	}
+	/* move next vlan tag to hw accel tag */
+	if (likely(skb->protocol != htons(ETH_P_8021Q) ||
+		   skb->len < VLAN_ETH_HLEN))
+		return 0;
+
+	err = __pop_vlan_tci(skb, &tci);
+	if (unlikely(err))
+		return err;
+
+	__vlan_hwaccel_put_tag(skb, ntohs(tci));
+	return 0;
+}
+
+static int push_vlan(struct sk_buff *skb, const struct ovs_action_push_vlan *vlan)
+{
+	if (unlikely(vlan_tx_tag_present(skb))) {
+		u16 current_tag;
+
+		/* push down current VLAN tag */
+		current_tag = vlan_tx_tag_get(skb);
+
+		if (!__vlan_put_tag(skb, current_tag))
+			return -ENOMEM;
+
+		if (get_ip_summed(skb) == OVS_CSUM_COMPLETE)
+			skb->csum = csum_add(skb->csum, csum_partial(skb->data
+					+ (2 * ETH_ALEN), VLAN_HLEN, 0));
+
+	}
+	__vlan_hwaccel_put_tag(skb, ntohs(vlan->vlan_tci) & ~VLAN_TAG_PRESENT);
+	return 0;
+}
+
+static int set_eth_addr(struct sk_buff *skb,
+			const struct ovs_key_ethernet *eth_key)
+{
+	int err;
+	err = make_writable(skb, ETH_HLEN);
+	if (unlikely(err))
+		return err;
+
+	memcpy(eth_hdr(skb)->h_source, eth_key->eth_src, ETH_ALEN);
+	memcpy(eth_hdr(skb)->h_dest, eth_key->eth_dst, ETH_ALEN);
+
+	return 0;
+}
+
+static void set_ip_addr(struct sk_buff *skb, struct iphdr *nh,
+				__be32 *addr, __be32 new_addr)
+{
+	int transport_len = skb->len - skb_transport_offset(skb);
+
+	if (nh->protocol == IPPROTO_TCP) {
+		if (likely(transport_len >= sizeof(struct tcphdr)))
+			inet_proto_csum_replace4(&tcp_hdr(skb)->check, skb,
+						 *addr, new_addr, 1);
+	} else if (nh->protocol == IPPROTO_UDP) {
+		if (likely(transport_len >= sizeof(struct udphdr))) {
+			struct udphdr *uh = udp_hdr(skb);
+
+			if (uh->check ||
+			    get_ip_summed(skb) == OVS_CSUM_PARTIAL) {
+				inet_proto_csum_replace4(&uh->check, skb,
+							 *addr, new_addr, 1);
+				if (!uh->check)
+					uh->check = CSUM_MANGLED_0;
+			}
+		}
+	}
+
+	csum_replace4(&nh->check, *addr, new_addr);
+	skb->rxhash = 0;
+	*addr = new_addr;
+}
+
+static void update_ipv6_checksum(struct sk_buff *skb, u8 l4_proto,
+				 __be32 addr[4], const __be32 new_addr[4])
+{
+	int transport_len = skb->len - skb_transport_offset(skb);
+
+	if (l4_proto == IPPROTO_TCP) {
+		if (likely(transport_len >= sizeof(struct tcphdr)))
+			inet_proto_csum_replace16(&tcp_hdr(skb)->check, skb,
+						  addr, new_addr, 1);
+	} else if (l4_proto == IPPROTO_UDP) {
+		if (likely(transport_len >= sizeof(struct udphdr))) {
+			struct udphdr *uh = udp_hdr(skb);
+
+			if (uh->check ||
+			    get_ip_summed(skb) == OVS_CSUM_PARTIAL) {
+				inet_proto_csum_replace16(&uh->check, skb,
+							  addr, new_addr, 1);
+				if (!uh->check)
+					uh->check = CSUM_MANGLED_0;
+			}
+		}
+	}
+}
+
+static void set_ipv6_addr(struct sk_buff *skb, u8 l4_proto,
+			  __be32 addr[4], const __be32 new_addr[4],
+			  bool recalculate_csum)
+{
+	if (recalculate_csum)
+		update_ipv6_checksum(skb, l4_proto, addr, new_addr);
+
+	skb->rxhash = 0;
+	memcpy(addr, new_addr, sizeof(__be32[4]));
+}
+
+static void set_ipv6_tc(struct ipv6hdr *nh, u8 tc)
+{
+	nh->priority = tc >> 4;
+	nh->flow_lbl[0] = (nh->flow_lbl[0] & 0x0F) | ((tc & 0x0F) << 4);
+}
+
+static void set_ipv6_fl(struct ipv6hdr *nh, u32 fl)
+{
+	nh->flow_lbl[0] = (nh->flow_lbl[0] & 0xF0) | (fl & 0x000F0000) >> 16;
+	nh->flow_lbl[1] = (fl & 0x0000FF00) >> 8;
+	nh->flow_lbl[2] = fl & 0x000000FF;
+}
+
+static void set_ip_ttl(struct sk_buff *skb, struct iphdr *nh, u8 new_ttl)
+{
+	csum_replace2(&nh->check, htons(nh->ttl << 8), htons(new_ttl << 8));
+	nh->ttl = new_ttl;
+}
+
+static int set_ipv4(struct sk_buff *skb, const struct ovs_key_ipv4 *ipv4_key)
+{
+	struct iphdr *nh;
+	int err;
+
+	err = make_writable(skb, skb_network_offset(skb) +
+				 sizeof(struct iphdr));
+	if (unlikely(err))
+		return err;
+
+	nh = ip_hdr(skb);
+
+	if (ipv4_key->ipv4_src != nh->saddr)
+		set_ip_addr(skb, nh, &nh->saddr, ipv4_key->ipv4_src);
+
+	if (ipv4_key->ipv4_dst != nh->daddr)
+		set_ip_addr(skb, nh, &nh->daddr, ipv4_key->ipv4_dst);
+
+	if (ipv4_key->ipv4_tos != nh->tos)
+		ipv4_change_dsfield(nh, 0, ipv4_key->ipv4_tos);
+
+	if (ipv4_key->ipv4_ttl != nh->ttl)
+		set_ip_ttl(skb, nh, ipv4_key->ipv4_ttl);
+
+	return 0;
+}
+
+static int set_ipv6(struct sk_buff *skb, const struct ovs_key_ipv6 *ipv6_key)
+{
+	struct ipv6hdr *nh;
+	int err;
+	__be32 *saddr;
+	__be32 *daddr;
+
+	err = make_writable(skb, skb_network_offset(skb) +
+			    sizeof(struct ipv6hdr));
+	if (unlikely(err))
+		return err;
+
+	nh = ipv6_hdr(skb);
+	saddr = (__be32 *)&nh->saddr;
+	daddr = (__be32 *)&nh->daddr;
+
+	if (memcmp(ipv6_key->ipv6_src, saddr, sizeof(ipv6_key->ipv6_src)))
+		set_ipv6_addr(skb, ipv6_key->ipv6_proto, saddr,
+			      ipv6_key->ipv6_src, true);
+
+	if (memcmp(ipv6_key->ipv6_dst, daddr, sizeof(ipv6_key->ipv6_dst))) {
+		unsigned int offset = 0;
+		int flags = OVS_IP6T_FH_F_SKIP_RH;
+		bool recalc_csum = true;
+
+		if (ipv6_ext_hdr(nh->nexthdr))
+			recalc_csum = ipv6_ovs_find_hdr(skb, &offset,
+						    NEXTHDR_ROUTING, NULL,
+						    &flags) != NEXTHDR_ROUTING;
+
+		set_ipv6_addr(skb, ipv6_key->ipv6_proto, daddr,
+			      ipv6_key->ipv6_dst, recalc_csum);
+	}
+
+	set_ipv6_tc(nh, ipv6_key->ipv6_tclass);
+	set_ipv6_fl(nh, ntohl(ipv6_key->ipv6_label));
+	nh->hop_limit = ipv6_key->ipv6_hlimit;
+
+	return 0;
+}
+
+/* Must follow make_writable() since that can move the skb data. */
+static void set_tp_port(struct sk_buff *skb, __be16 *port,
+			 __be16 new_port, __sum16 *check)
+{
+	inet_proto_csum_replace2(check, skb, *port, new_port, 0);
+	*port = new_port;
+	skb->rxhash = 0;
+}
+
+static void set_udp_port(struct sk_buff *skb, __be16 *port, __be16 new_port)
+{
+	struct udphdr *uh = udp_hdr(skb);
+
+	if (uh->check && get_ip_summed(skb) != OVS_CSUM_PARTIAL) {
+		set_tp_port(skb, port, new_port, &uh->check);
+
+		if (!uh->check)
+			uh->check = CSUM_MANGLED_0;
+	} else {
+		*port = new_port;
+		skb->rxhash = 0;
+	}
+}
+
+static int set_udp(struct sk_buff *skb, const struct ovs_key_udp *udp_port_key)
+{
+	struct udphdr *uh;
+	int err;
+
+	err = make_writable(skb, skb_transport_offset(skb) +
+				 sizeof(struct udphdr));
+	if (unlikely(err))
+		return err;
+
+	uh = udp_hdr(skb);
+	if (udp_port_key->udp_src != uh->source)
+		set_udp_port(skb, &uh->source, udp_port_key->udp_src);
+
+	if (udp_port_key->udp_dst != uh->dest)
+		set_udp_port(skb, &uh->dest, udp_port_key->udp_dst);
+
+	return 0;
+}
+
+static int set_tcp(struct sk_buff *skb, const struct ovs_key_tcp *tcp_port_key)
+{
+	struct tcphdr *th;
+	int err;
+
+	err = make_writable(skb, skb_transport_offset(skb) +
+				 sizeof(struct tcphdr));
+	if (unlikely(err))
+		return err;
+
+	th = tcp_hdr(skb);
+	if (tcp_port_key->tcp_src != th->source)
+		set_tp_port(skb, &th->source, tcp_port_key->tcp_src, &th->check);
+
+	if (tcp_port_key->tcp_dst != th->dest)
+		set_tp_port(skb, &th->dest, tcp_port_key->tcp_dst, &th->check);
+
+	return 0;
+}
+
+static int do_output(struct datapath *dp, struct sk_buff *skb, int out_port)
+{
+	struct vport *vport;
+
+	if (unlikely(!skb))
+		return -ENOMEM;
+
+	vport = ovs_vport_rcu(dp, out_port);
+	if (unlikely(!vport)) {
+		kfree_skb(skb);
+		return -ENODEV;
+	}
+
+	ovs_vport_send(vport, skb);
+	return 0;
+}
+
+static int output_userspace(struct datapath *dp, struct sk_buff *skb,
+			    const struct nlattr *attr)
+{
+	struct dp_upcall_info upcall;
+	const struct nlattr *a;
+	int rem;
+
+	upcall.cmd = OVS_PACKET_CMD_ACTION;
+	upcall.key = &OVS_CB(skb)->flow->key;
+	upcall.userdata = NULL;
+	upcall.portid = 0;
+
+	for (a = nla_data(attr), rem = nla_len(attr); rem > 0;
+		 a = nla_next(a, &rem)) {
+		switch (nla_type(a)) {
+		case OVS_USERSPACE_ATTR_USERDATA:
+			upcall.userdata = a;
+			break;
+
+		case OVS_USERSPACE_ATTR_PID:
+			upcall.portid = nla_get_u32(a);
+			break;
+		}
+	}
+
+	return ovs_dp_upcall(dp, skb, &upcall);
+}
+
+static int sample(struct datapath *dp, struct sk_buff *skb,
+		  const struct nlattr *attr,
+		  struct ovs_key_ipv4_tunnel *tun_key)
+{
+	const struct nlattr *acts_list = NULL;
+	const struct nlattr *a;
+	int rem;
+
+	for (a = nla_data(attr), rem = nla_len(attr); rem > 0;
+		 a = nla_next(a, &rem)) {
+		switch (nla_type(a)) {
+		case OVS_SAMPLE_ATTR_PROBABILITY:
+			if (net_random() >= nla_get_u32(a))
+				return 0;
+			break;
+
+		case OVS_SAMPLE_ATTR_ACTIONS:
+			acts_list = a;
+			break;
+		}
+	}
+
+	return do_execute_actions(dp, skb, nla_data(acts_list),
+				  nla_len(acts_list), tun_key, true);
+}
+
+static int execute_set_action(struct sk_buff *skb,
+				 const struct nlattr *nested_attr,
+				 struct ovs_key_ipv4_tunnel *tun_key)
+{
+	int err = 0;
+
+	switch (nla_type(nested_attr)) {
+	case OVS_KEY_ATTR_PRIORITY:
+		skb->priority = nla_get_u32(nested_attr);
+		break;
+
+	case OVS_KEY_ATTR_SKB_MARK:
+		skb->mark = nla_get_u32(nested_attr);
+		break;
+
+	case OVS_KEY_ATTR_TUN_ID:
+		/* If we're only using the TUN_ID action, store the value in a
+		 * temporary instance of struct ovs_key_ipv4_tunnel on the stack.
+		 * If both IPV4_TUNNEL and TUN_ID are being used together we
+		 * can't write into the IPV4_TUNNEL action, so make a copy and
+		 * write into that version.
+		 */
+		if (!OVS_CB(skb)->tun_key)
+			memset(tun_key, 0, sizeof(*tun_key));
+		else if (OVS_CB(skb)->tun_key != tun_key)
+			memcpy(tun_key, OVS_CB(skb)->tun_key, sizeof(*tun_key));
+		OVS_CB(skb)->tun_key = tun_key;
+
+		OVS_CB(skb)->tun_key->tun_id = nla_get_be64(nested_attr);
+		break;
+
+	case OVS_KEY_ATTR_IPV4_TUNNEL:
+		OVS_CB(skb)->tun_key = nla_data(nested_attr);
+		break;
+
+	case OVS_KEY_ATTR_ETHERNET:
+		err = set_eth_addr(skb, nla_data(nested_attr));
+		break;
+
+	case OVS_KEY_ATTR_IPV4:
+		err = set_ipv4(skb, nla_data(nested_attr));
+		break;
+
+	case OVS_KEY_ATTR_IPV6:
+		err = set_ipv6(skb, nla_data(nested_attr));
+		break;
+
+	case OVS_KEY_ATTR_TCP:
+		err = set_tcp(skb, nla_data(nested_attr));
+		break;
+
+	case OVS_KEY_ATTR_UDP:
+		err = set_udp(skb, nla_data(nested_attr));
+		break;
+	}
+
+	return err;
+}
+
+/* Execute a list of actions against 'skb'. */
+static int do_execute_actions(struct datapath *dp, struct sk_buff *skb,
+			const struct nlattr *attr, int len,
+			struct ovs_key_ipv4_tunnel *tun_key, bool keep_skb)
+{
+	/* Every output action needs a separate clone of 'skb', but the common
+	 * case is just a single output action, so that doing a clone and
+	 * then freeing the original skbuff is wasteful.  So the following code
+	 * is slightly obscure just to avoid that. */
+	int prev_port = -1;
+	const struct nlattr *a;
+	int rem;
+
+	for (a = attr, rem = len; rem > 0;
+	     a = nla_next(a, &rem)) {
+		int err = 0;
+
+		if (prev_port != -1) {
+			do_output(dp, skb_clone(skb, GFP_ATOMIC), prev_port);
+			prev_port = -1;
+		}
+
+		switch (nla_type(a)) {
+		case OVS_ACTION_ATTR_OUTPUT:
+			prev_port = nla_get_u32(a);
+			break;
+
+		case OVS_ACTION_ATTR_USERSPACE:
+			output_userspace(dp, skb, a);
+			break;
+
+		case OVS_ACTION_ATTR_PUSH_VLAN:
+			err = push_vlan(skb, nla_data(a));
+			if (unlikely(err)) /* skb already freed. */
+				return err;
+			break;
+
+		case OVS_ACTION_ATTR_POP_VLAN:
+			err = pop_vlan(skb);
+			break;
+
+		case OVS_ACTION_ATTR_SET:
+			err = execute_set_action(skb, nla_data(a), tun_key);
+			break;
+
+		case OVS_ACTION_ATTR_SAMPLE:
+			err = sample(dp, skb, a, tun_key);
+			break;
+		}
+
+		if (unlikely(err)) {
+			kfree_skb(skb);
+			return err;
+		}
+	}
+
+	if (prev_port != -1) {
+		if (keep_skb)
+			skb = skb_clone(skb, GFP_ATOMIC);
+
+		do_output(dp, skb, prev_port);
+	} else if (!keep_skb)
+		consume_skb(skb);
+
+	return 0;
+}
+
+/* We limit the number of times that we pass into execute_actions()
+ * to avoid blowing out the stack in the event that we have a loop. */
+#define MAX_LOOPS 4
+
+struct loop_counter {
+	u8 count;		/* Count. */
+	bool looping;		/* Loop detected? */
+};
+
+static DEFINE_PER_CPU(struct loop_counter, loop_counters);
+
+static int loop_suppress(struct datapath *dp, struct sw_flow_actions *actions)
+{
+	if (net_ratelimit())
+		pr_warn("%s: flow looped %d times, dropping\n",
+				ovs_dp_name(dp), MAX_LOOPS);
+	actions->actions_len = 0;
+	return -ELOOP;
+}
+
+/* Execute a list of actions against 'skb'. */
+int ovs_execute_actions(struct datapath *dp, struct sk_buff *skb)
+{
+	struct sw_flow_actions *acts = rcu_dereference(OVS_CB(skb)->flow->sf_acts);
+	struct loop_counter *loop;
+	int error;
+	struct ovs_key_ipv4_tunnel tun_key;
+
+	/* Check whether we've looped too much. */
+	loop = &__get_cpu_var(loop_counters);
+	if (unlikely(++loop->count > MAX_LOOPS))
+		loop->looping = true;
+	if (unlikely(loop->looping)) {
+		error = loop_suppress(dp, acts);
+		kfree_skb(skb);
+		goto out_loop;
+	}
+
+	OVS_CB(skb)->tun_key = NULL;
+	error = do_execute_actions(dp, skb, acts->actions,
+					 acts->actions_len, &tun_key, false);
+
+	/* Check whether sub-actions looped too much. */
+	if (unlikely(loop->looping))
+		error = loop_suppress(dp, acts);
+
+out_loop:
+	/* Decrement loop counter. */
+	if (!--loop->count)
+		loop->looping = false;
+
+	return error;
+}
diff --git a/drivers/staging/openvswitch_nv/checksum.h b/drivers/staging/openvswitch_nv/checksum.h
new file mode 100644
index 000000000000..a440c59985ca
--- /dev/null
+++ b/drivers/staging/openvswitch_nv/checksum.h
@@ -0,0 +1,173 @@
+/*
+ * Copyright (c) 2007-2011 Nicira, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#ifndef CHECKSUM_H
+#define CHECKSUM_H 1
+
+#include <linux/skbuff.h>
+#include <linux/version.h>
+
+#include <net/checksum.h>
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,22) || \
+	(defined(CONFIG_XEN) && defined(HAVE_PROTO_DATA_VALID))
+#define NEED_CSUM_NORMALIZE
+#endif
+
+/* These are the same values as the checksum constants in 2.6.22+. */
+enum csum_type {
+	OVS_CSUM_NONE = 0,
+	OVS_CSUM_UNNECESSARY = 1,
+	OVS_CSUM_COMPLETE = 2,
+	OVS_CSUM_PARTIAL = 3,
+};
+
+#ifdef NEED_CSUM_NORMALIZE
+int compute_ip_summed(struct sk_buff *skb, bool xmit);
+void forward_ip_summed(struct sk_buff *skb, bool xmit);
+u8 get_ip_summed(struct sk_buff *skb);
+void set_ip_summed(struct sk_buff *skb, u8 ip_summed);
+void get_skb_csum_pointers(const struct sk_buff *skb, u16 *csum_start,
+			   u16 *csum_offset);
+void set_skb_csum_pointers(struct sk_buff *skb, u16 csum_start,
+			   u16 csum_offset);
+#else
+static inline int compute_ip_summed(struct sk_buff *skb, bool xmit)
+{
+	return 0;
+}
+
+static inline void forward_ip_summed(struct sk_buff *skb, bool xmit) { }
+
+static inline u8 get_ip_summed(struct sk_buff *skb)
+{
+	return skb->ip_summed;
+}
+
+static inline void set_ip_summed(struct sk_buff *skb, u8 ip_summed)
+{
+	skb->ip_summed = ip_summed;
+}
+
+static inline void get_skb_csum_pointers(const struct sk_buff *skb,
+					 u16 *csum_start, u16 *csum_offset)
+{
+	*csum_start = skb->csum_start;
+	*csum_offset = skb->csum_offset;
+}
+
+static inline void set_skb_csum_pointers(struct sk_buff *skb, u16 csum_start,
+					 u16 csum_offset)
+{
+	skb->csum_start = csum_start;
+	skb->csum_offset = csum_offset;
+}
+#endif
+
+/* This is really compatibility code that belongs in the compat directory.
+ * However, it needs access to our normalized checksum values, so put it here.
+ */
+#if defined(NEED_CSUM_NORMALIZE) || LINUX_VERSION_CODE < KERNEL_VERSION(2,6,25)
+#define inet_proto_csum_replace4 rpl_inet_proto_csum_replace4
+static inline void inet_proto_csum_replace4(__sum16 *sum, struct sk_buff *skb,
+					    __be32 from, __be32 to,
+					    int pseudohdr)
+{
+	__be32 diff[] = { ~from, to };
+
+	if (get_ip_summed(skb) != OVS_CSUM_PARTIAL) {
+		*sum = csum_fold(csum_partial((char *)diff, sizeof(diff),
+				~csum_unfold(*sum)));
+		if (get_ip_summed(skb) == OVS_CSUM_COMPLETE && pseudohdr)
+			skb->csum = ~csum_partial((char *)diff, sizeof(diff),
+						~skb->csum);
+	} else if (pseudohdr)
+		*sum = ~csum_fold(csum_partial((char *)diff, sizeof(diff),
+				csum_unfold(*sum)));
+}
+#endif
+
+#if defined(NEED_CSUM_NORMALIZE) || LINUX_VERSION_CODE < KERNEL_VERSION(3,7,0)
+#define inet_proto_csum_replace16 rpl_inet_proto_csum_replace16
+static inline void inet_proto_csum_replace16(__sum16 *sum,
+					     struct sk_buff *skb,
+					     const __be32 *from,
+					     const __be32 *to,
+					     int pseudohdr)
+{
+	__be32 diff[] = {
+		~from[0], ~from[1], ~from[2], ~from[3],
+		to[0], to[1], to[2], to[3],
+	};
+	if (get_ip_summed(skb) != OVS_CSUM_PARTIAL) {
+		*sum = csum_fold(csum_partial(diff, sizeof(diff),
+				 ~csum_unfold(*sum)));
+		if (get_ip_summed(skb) == OVS_CSUM_COMPLETE && pseudohdr)
+			skb->csum = ~csum_partial(diff, sizeof(diff),
+						  ~skb->csum);
+	} else if (pseudohdr)
+		*sum = ~csum_fold(csum_partial(diff, sizeof(diff),
+				  csum_unfold(*sum)));
+}
+#endif
+
+#ifdef NEED_CSUM_NORMALIZE
+static inline void update_csum_start(struct sk_buff *skb, int delta)
+{
+	if (get_ip_summed(skb) == OVS_CSUM_PARTIAL) {
+		u16 csum_start, csum_offset;
+
+		get_skb_csum_pointers(skb, &csum_start, &csum_offset);
+		set_skb_csum_pointers(skb, csum_start + delta, csum_offset);
+	}
+}
+
+static inline int rpl_pskb_expand_head(struct sk_buff *skb, int nhead,
+				       int ntail, gfp_t gfp_mask)
+{
+	int err;
+	int old_headroom = skb_headroom(skb);
+
+	err = pskb_expand_head(skb, nhead, ntail, gfp_mask);
+	if (unlikely(err))
+		return err;
+
+	update_csum_start(skb, skb_headroom(skb) - old_headroom);
+
+	return 0;
+}
+#define pskb_expand_head rpl_pskb_expand_head
+
+static inline unsigned char *rpl__pskb_pull_tail(struct sk_buff *skb,
+						  int delta)
+{
+	unsigned char *ret;
+	int old_headroom = skb_headroom(skb);
+
+	ret = __pskb_pull_tail(skb, delta);
+	if (unlikely(!ret))
+		return ret;
+
+	update_csum_start(skb, skb_headroom(skb) - old_headroom);
+
+	return ret;
+}
+#define __pskb_pull_tail rpl__pskb_pull_tail
+#endif
+
+#endif /* checksum.h */
diff --git a/drivers/staging/openvswitch_nv/datapath.c b/drivers/staging/openvswitch_nv/datapath.c
new file mode 100644
index 000000000000..9fe9b412fe20
--- /dev/null
+++ b/drivers/staging/openvswitch_nv/datapath.c
@@ -0,0 +1,2502 @@
+/*
+ * Copyright (c) 2007-2012 Nicira, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/if_arp.h>
+#include <linux/if_vlan.h>
+#include <linux/in.h>
+#include <linux/ip.h>
+#include <linux/jhash.h>
+#include <linux/delay.h>
+#include <linux/time.h>
+#include <linux/etherdevice.h>
+#include <linux/genetlink.h>
+#include <linux/kernel.h>
+#include <linux/kthread.h>
+#include <linux/mutex.h>
+#include <linux/percpu.h>
+#include <linux/rcupdate.h>
+#include <linux/tcp.h>
+#include <linux/udp.h>
+#include <linux/version.h>
+#include <linux/ethtool.h>
+#include <linux/wait.h>
+#include <asm/div64.h>
+#include <linux/highmem.h>
+#include <linux/netfilter_bridge.h>
+#include <linux/netfilter_ipv4.h>
+#include <linux/inetdevice.h>
+#include <linux/list.h>
+#include <linux/openvswitch-nv.h>
+#include <linux/rculist.h>
+#include <linux/dmi.h>
+#include <net/genetlink.h>
+#include <net/net_namespace.h>
+#include <net/netns/generic.h>
+
+#include "checksum.h"
+#include "datapath.h"
+#include "flow.h"
+#include "genl_exec.h"
+#include "vlan.h"
+#include "tunnel.h"
+#include "vport-internal_dev.h"
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,18) || \
+    LINUX_VERSION_CODE >= KERNEL_VERSION(3,9,0)
+#error Kernels before 2.6.18 or after 3.8 are not supported by this version of Open vSwitch.
+#endif
+
+#define REHASH_FLOW_INTERVAL (10 * 60 * HZ)
+static void rehash_flow_table(struct work_struct *work);
+static DECLARE_DELAYED_WORK(rehash_flow_wq, rehash_flow_table);
+
+int ovs_net_id __read_mostly;
+
+int (*ovs_dp_ioctl_hook)(struct net_device *dev, struct ifreq *rq, int cmd);
+EXPORT_SYMBOL(ovs_dp_ioctl_hook);
+
+/**
+ * DOC: Locking:
+ *
+ * Writes to device state (add/remove datapath, port, set operations on vports,
+ * etc.) are protected by RTNL.
+ *
+ * Writes to other state (flow table modifications, set miscellaneous datapath
+ * parameters, etc.) are protected by genl_mutex.  The RTNL lock nests inside
+ * genl_mutex.
+ *
+ * Reads are protected by RCU.
+ *
+ * There are a few special cases (mostly stats) that have their own
+ * synchronization but they nest under all of above and don't interact with
+ * each other.
+ */
+
+static struct vport *new_vport(const struct vport_parms *);
+static int queue_gso_packets(struct net *, int dp_ifindex, struct sk_buff *,
+			     const struct dp_upcall_info *);
+static int queue_userspace_packet(struct net *, int dp_ifindex,
+				  struct sk_buff *,
+				  const struct dp_upcall_info *);
+
+/* Must be called with rcu_read_lock, genl_mutex, or RTNL lock. */
+static struct datapath *get_dp(struct net *net, int dp_ifindex)
+{
+	struct datapath *dp = NULL;
+	struct net_device *dev;
+
+	rcu_read_lock();
+	dev = dev_get_by_index_rcu(net, dp_ifindex);
+	if (dev) {
+		struct vport *vport = ovs_internal_dev_get_vport(dev);
+		if (vport)
+			dp = vport->dp;
+	}
+	rcu_read_unlock();
+
+	return dp;
+}
+
+/* Must be called with rcu_read_lock or RTNL lock. */
+const char *ovs_dp_name(const struct datapath *dp)
+{
+	struct vport *vport = ovs_vport_rtnl_rcu(dp, OVSP_LOCAL);
+	return vport->ops->get_name(vport);
+}
+
+static int get_dpifindex(struct datapath *dp)
+{
+	struct vport *local;
+	int ifindex;
+
+	rcu_read_lock();
+
+	local = ovs_vport_rcu(dp, OVSP_LOCAL);
+	if (local)
+		ifindex = local->ops->get_ifindex(local);
+	else
+		ifindex = 0;
+
+	rcu_read_unlock();
+
+	return ifindex;
+}
+
+static size_t br_nlmsg_size(void)
+{
+	return NLMSG_ALIGN(sizeof(struct ifinfomsg))
+	       + nla_total_size(IFNAMSIZ) /* IFLA_IFNAME */
+	       + nla_total_size(MAX_ADDR_LEN) /* IFLA_ADDRESS */
+	       + nla_total_size(4) /* IFLA_MASTER */
+	       + nla_total_size(4) /* IFLA_MTU */
+	       + nla_total_size(1); /* IFLA_OPERSTATE */
+}
+
+/* Caller must hold RTNL lock. */
+static int dp_fill_ifinfo(struct sk_buff *skb,
+			  const struct vport *port,
+			  int event, unsigned int flags)
+{
+	struct datapath *dp = port->dp;
+	struct ifinfomsg *hdr;
+	struct nlmsghdr *nlh;
+
+	if (!port->ops->get_ifindex)
+		return -ENODEV;
+
+	nlh = nlmsg_put(skb, 0, 0, event, sizeof(*hdr), flags);
+	if (nlh == NULL)
+		return -EMSGSIZE;
+
+	hdr = nlmsg_data(nlh);
+	hdr->ifi_family = AF_BRIDGE;
+	hdr->__ifi_pad = 0;
+	hdr->ifi_type = ARPHRD_ETHER;
+	hdr->ifi_index = port->ops->get_ifindex(port);
+	hdr->ifi_flags = port->ops->get_dev_flags(port);
+	hdr->ifi_change = 0;
+
+	if (nla_put_string(skb, IFLA_IFNAME, port->ops->get_name(port)) ||
+	    nla_put_u32(skb, IFLA_MASTER, get_dpifindex(dp)) ||
+	    nla_put_u32(skb, IFLA_MTU, port->ops->get_mtu(port)) ||
+#ifdef IFLA_OPERSTATE
+	    nla_put_u8(skb, IFLA_OPERSTATE,
+		       port->ops->is_running(port) ?
+				port->ops->get_operstate(port) :
+				IF_OPER_DOWN) ||
+#endif
+	    nla_put(skb, IFLA_ADDRESS, ETH_ALEN, port->ops->get_addr(port)))
+		goto nla_put_failure;
+
+	return nlmsg_end(skb, nlh);
+
+nla_put_failure:
+	nlmsg_cancel(skb, nlh);
+	return -EMSGSIZE;
+}
+
+/* Caller must hold RTNL lock. */
+static void dp_ifinfo_notify(int event, struct vport *port)
+{
+	struct sk_buff *skb;
+	int err;
+
+	skb = nlmsg_new(br_nlmsg_size(), GFP_KERNEL);
+	if (!skb) {
+		err = -ENOBUFS;
+		goto err;
+	}
+
+	err = dp_fill_ifinfo(skb, port, event, 0);
+	if (err < 0) {
+		if (err == -ENODEV) {
+			goto out;
+		} else {
+			/* -EMSGSIZE implies BUG in br_nlmsg_size() */
+			WARN_ON(err == -EMSGSIZE);
+			goto err;
+		}
+	}
+
+	rtnl_notify(skb, ovs_dp_get_net(port->dp), 0, RTNLGRP_LINK, NULL, GFP_KERNEL);
+
+	return;
+err:
+	rtnl_set_sk_err(ovs_dp_get_net(port->dp), RTNLGRP_LINK, err);
+out:
+	kfree_skb(skb);
+}
+
+static void release_dp(struct kobject *kobj)
+{
+	struct datapath *dp = container_of(kobj, struct datapath, ifobj);
+	kfree(dp);
+}
+
+static struct kobj_type dp_ktype = {
+	.release = release_dp
+};
+
+static void destroy_dp_rcu(struct rcu_head *rcu)
+{
+	struct datapath *dp = container_of(rcu, struct datapath, rcu);
+
+	ovs_flow_tbl_destroy((__force struct flow_table *)dp->table);
+	free_percpu(dp->stats_percpu);
+	release_net(ovs_dp_get_net(dp));
+	kfree(dp->ports);
+	kobject_put(&dp->ifobj);
+}
+
+static struct hlist_head *vport_hash_bucket(const struct datapath *dp,
+					    u16 port_no)
+{
+	return &dp->ports[port_no & (DP_VPORT_HASH_BUCKETS - 1)];
+}
+
+struct vport *ovs_lookup_vport(const struct datapath *dp, u16 port_no)
+{
+	struct vport *vport;
+	struct hlist_node *n;
+	struct hlist_head *head;
+
+	head = vport_hash_bucket(dp, port_no);
+	hlist_for_each_entry_rcu(vport, n, head, dp_hash_node) {
+		if (vport->port_no == port_no)
+			return vport;
+	}
+	return NULL;
+}
+
+/* Called with RTNL lock and genl_lock. */
+static struct vport *new_vport(const struct vport_parms *parms)
+{
+	struct vport *vport;
+
+	vport = ovs_vport_add(parms);
+	if (!IS_ERR(vport)) {
+		struct datapath *dp = parms->dp;
+		struct hlist_head *head = vport_hash_bucket(dp, vport->port_no);
+
+		hlist_add_head_rcu(&vport->dp_hash_node, head);
+		dp_ifinfo_notify(RTM_NEWLINK, vport);
+	}
+	return vport;
+}
+
+/* Called with RTNL lock. */
+void ovs_dp_detach_port(struct vport *p)
+{
+	ASSERT_RTNL();
+
+	if (p->port_no != OVSP_LOCAL)
+		ovs_dp_sysfs_del_if(p);
+
+	dp_ifinfo_notify(RTM_DELLINK, p);
+
+	/* First drop references to device. */
+	hlist_del_rcu(&p->dp_hash_node);
+
+	/* Then destroy it. */
+	ovs_vport_del(p);
+}
+
+/* Must be called with rcu_read_lock. */
+void ovs_dp_process_received_packet(struct vport *p, struct sk_buff *skb)
+{
+	struct datapath *dp = p->dp;
+	struct sw_flow *flow;
+	struct dp_stats_percpu *stats;
+	u64 *stats_counter;
+	int error;
+
+	stats = per_cpu_ptr(dp->stats_percpu, smp_processor_id());
+
+	if (!OVS_CB(skb)->flow) {
+		struct sw_flow_key key;
+		int key_len;
+
+		/* Extract flow from 'skb' into 'key'. */
+		error = ovs_flow_extract(skb, p->port_no, &key, &key_len);
+		if (unlikely(error)) {
+			kfree_skb(skb);
+			return;
+		}
+
+		/* Look up flow. */
+		flow = ovs_flow_tbl_lookup(rcu_dereference(dp->table),
+					   &key, key_len);
+		if (unlikely(!flow)) {
+			struct dp_upcall_info upcall;
+
+			upcall.cmd = OVS_PACKET_CMD_MISS;
+			upcall.key = &key;
+			upcall.userdata = NULL;
+			upcall.portid = p->upcall_portid;
+			ovs_dp_upcall(dp, skb, &upcall);
+			consume_skb(skb);
+			stats_counter = &stats->n_missed;
+			goto out;
+		}
+
+		OVS_CB(skb)->flow = flow;
+	}
+
+	stats_counter = &stats->n_hit;
+	ovs_flow_used(OVS_CB(skb)->flow, skb);
+	ovs_execute_actions(dp, skb);
+
+out:
+	/* Update datapath statistics. */
+	u64_stats_update_begin(&stats->sync);
+	(*stats_counter)++;
+	u64_stats_update_end(&stats->sync);
+}
+
+static struct genl_family dp_packet_genl_family = {
+	.id = GENL_ID_GENERATE,
+	.hdrsize = sizeof(struct ovs_header),
+	.name = OVS_PACKET_FAMILY,
+	.version = OVS_PACKET_VERSION,
+	.maxattr = OVS_PACKET_ATTR_MAX,
+	 .netnsok = true,
+};
+
+int ovs_dp_upcall(struct datapath *dp, struct sk_buff *skb,
+		  const struct dp_upcall_info *upcall_info)
+{
+	struct dp_stats_percpu *stats;
+	int dp_ifindex;
+	int err;
+
+	if (upcall_info->portid == 0) {
+		err = -ENOTCONN;
+		goto err;
+	}
+
+	dp_ifindex = get_dpifindex(dp);
+	if (!dp_ifindex) {
+		err = -ENODEV;
+		goto err;
+	}
+
+	forward_ip_summed(skb, true);
+
+	if (!skb_is_gso(skb))
+		err = queue_userspace_packet(ovs_dp_get_net(dp), dp_ifindex, skb, upcall_info);
+	else
+		err = queue_gso_packets(ovs_dp_get_net(dp), dp_ifindex, skb, upcall_info);
+	if (err)
+		goto err;
+
+	return 0;
+
+err:
+	stats = per_cpu_ptr(dp->stats_percpu, smp_processor_id());
+
+	u64_stats_update_begin(&stats->sync);
+	stats->n_lost++;
+	u64_stats_update_end(&stats->sync);
+
+	return err;
+}
+
+static int queue_gso_packets(struct net *net, int dp_ifindex,
+			     struct sk_buff *skb,
+			     const struct dp_upcall_info *upcall_info)
+{
+	unsigned short gso_type = skb_shinfo(skb)->gso_type;
+	struct dp_upcall_info later_info;
+	struct sw_flow_key later_key;
+	struct sk_buff *segs, *nskb;
+	int err;
+
+	segs = skb_gso_segment(skb, NETIF_F_SG | NETIF_F_HW_CSUM);
+	if (IS_ERR(segs))
+		return PTR_ERR(segs);
+
+	/* Queue all of the segments. */
+	skb = segs;
+	do {
+		err = queue_userspace_packet(net, dp_ifindex, skb, upcall_info);
+		if (err)
+			break;
+
+		if (skb == segs && gso_type & SKB_GSO_UDP) {
+			/* The initial flow key extracted by ovs_flow_extract()
+			 * in this case is for a first fragment, so we need to
+			 * properly mark later fragments.
+			 */
+			later_key = *upcall_info->key;
+			later_key.ip.frag = OVS_FRAG_TYPE_LATER;
+
+			later_info = *upcall_info;
+			later_info.key = &later_key;
+			upcall_info = &later_info;
+		}
+	} while ((skb = skb->next));
+
+	/* Free all of the segments. */
+	skb = segs;
+	do {
+		nskb = skb->next;
+		if (err)
+			kfree_skb(skb);
+		else
+			consume_skb(skb);
+	} while ((skb = nskb));
+	return err;
+}
+
+static int queue_userspace_packet(struct net *net, int dp_ifindex,
+				  struct sk_buff *skb,
+				  const struct dp_upcall_info *upcall_info)
+{
+	struct ovs_header *upcall;
+	struct sk_buff *nskb = NULL;
+	struct sk_buff *user_skb; /* to be queued to userspace */
+	struct nlattr *nla;
+	unsigned int len;
+	int err;
+
+	if (vlan_tx_tag_present(skb)) {
+		nskb = skb_clone(skb, GFP_ATOMIC);
+		if (!nskb)
+			return -ENOMEM;
+
+		err = vlan_deaccel_tag(nskb);
+		if (err)
+			return err;
+
+		skb = nskb;
+	}
+
+	if (nla_attr_size(skb->len) > USHRT_MAX) {
+		err = -EFBIG;
+		goto out;
+	}
+
+	len = sizeof(struct ovs_header);
+	len += nla_total_size(skb->len);
+	len += nla_total_size(FLOW_BUFSIZE);
+	if (upcall_info->cmd == OVS_PACKET_CMD_ACTION)
+		len += nla_total_size(8);
+
+	user_skb = genlmsg_new(len, GFP_ATOMIC);
+	if (!user_skb) {
+		err = -ENOMEM;
+		goto out;
+	}
+
+	upcall = genlmsg_put(user_skb, 0, 0, &dp_packet_genl_family,
+			     0, upcall_info->cmd);
+	upcall->dp_ifindex = dp_ifindex;
+
+	nla = nla_nest_start(user_skb, OVS_PACKET_ATTR_KEY);
+	ovs_flow_to_nlattrs(upcall_info->key, user_skb);
+	nla_nest_end(user_skb, nla);
+
+	if (upcall_info->userdata)
+		nla_put_u64(user_skb, OVS_PACKET_ATTR_USERDATA,
+			    nla_get_u64(upcall_info->userdata));
+
+	nla = __nla_reserve(user_skb, OVS_PACKET_ATTR_PACKET, skb->len);
+
+	skb_copy_and_csum_dev(skb, nla_data(nla));
+
+	err = genlmsg_unicast(net, user_skb, upcall_info->portid);
+
+out:
+	kfree_skb(nskb);
+	return err;
+}
+
+/* Called with genl_mutex. */
+static int flush_flows(struct datapath *dp)
+{
+	struct flow_table *old_table;
+	struct flow_table *new_table;
+
+	old_table = genl_dereference(dp->table);
+	new_table = ovs_flow_tbl_alloc(TBL_MIN_BUCKETS);
+	if (!new_table)
+		return -ENOMEM;
+
+	rcu_assign_pointer(dp->table, new_table);
+
+	ovs_flow_tbl_deferred_destroy(old_table);
+	return 0;
+}
+
+static struct nlattr *reserve_sfa_size(struct sw_flow_actions **sfa, int attr_len)
+{
+
+	struct sw_flow_actions *acts;
+	int new_acts_size;
+	int req_size = NLA_ALIGN(attr_len);
+	int next_offset = offsetof(struct sw_flow_actions, actions) +
+					(*sfa)->actions_len;
+
+	if (req_size <= (ksize(*sfa) - next_offset))
+		goto out;
+
+	new_acts_size = ksize(*sfa) * 2;
+
+	if (new_acts_size > MAX_ACTIONS_BUFSIZE) {
+		if ((MAX_ACTIONS_BUFSIZE - next_offset) < req_size)
+			return ERR_PTR(-EMSGSIZE);
+		new_acts_size = MAX_ACTIONS_BUFSIZE;
+	}
+
+	acts = ovs_flow_actions_alloc(new_acts_size);
+	if (IS_ERR(acts))
+		return (void *)acts;
+
+	memcpy(acts->actions, (*sfa)->actions, (*sfa)->actions_len);
+	acts->actions_len = (*sfa)->actions_len;
+	kfree(*sfa);
+	*sfa = acts;
+
+out:
+	(*sfa)->actions_len += req_size;
+	return  (struct nlattr *) ((unsigned char *)(*sfa) + next_offset);
+}
+
+static int add_action(struct sw_flow_actions **sfa, int attrtype, void *data, int len)
+{
+	struct nlattr *a;
+
+	a = reserve_sfa_size(sfa, nla_attr_size(len));
+	if (IS_ERR(a))
+		return PTR_ERR(a);
+
+	a->nla_type = attrtype;
+	a->nla_len = nla_attr_size(len);
+
+	if (data)
+		memcpy(nla_data(a), data, len);
+	memset((unsigned char *) a + a->nla_len, 0, nla_padlen(len));
+
+	return 0;
+}
+
+static inline int add_nested_action_start(struct sw_flow_actions **sfa, int attrtype)
+{
+	int used = (*sfa)->actions_len;
+	int err;
+
+	err = add_action(sfa, attrtype, NULL, 0);
+	if (err)
+		return err;
+
+	return used;
+}
+
+static inline void add_nested_action_end(struct sw_flow_actions *sfa, int st_offset)
+{
+	struct nlattr *a = (struct nlattr *) ((unsigned char *)sfa->actions + st_offset);
+
+	a->nla_len = sfa->actions_len - st_offset;
+}
+
+static int validate_and_copy_actions(const struct nlattr *attr,
+				const struct sw_flow_key *key, int depth,
+				struct sw_flow_actions **sfa);
+
+static int validate_and_copy_sample(const struct nlattr *attr,
+			   const struct sw_flow_key *key, int depth,
+			   struct sw_flow_actions **sfa)
+{
+	const struct nlattr *attrs[OVS_SAMPLE_ATTR_MAX + 1];
+	const struct nlattr *probability, *actions;
+	const struct nlattr *a;
+	int rem, start, err, st_acts;
+
+	memset(attrs, 0, sizeof(attrs));
+	nla_for_each_nested(a, attr, rem) {
+		int type = nla_type(a);
+		if (!type || type > OVS_SAMPLE_ATTR_MAX || attrs[type])
+			return -EINVAL;
+		attrs[type] = a;
+	}
+	if (rem)
+		return -EINVAL;
+
+	probability = attrs[OVS_SAMPLE_ATTR_PROBABILITY];
+	if (!probability || nla_len(probability) != sizeof(u32))
+		return -EINVAL;
+
+	actions = attrs[OVS_SAMPLE_ATTR_ACTIONS];
+	if (!actions || (nla_len(actions) && nla_len(actions) < NLA_HDRLEN))
+		return -EINVAL;
+
+	/* validation done, copy sample action. */
+	start = add_nested_action_start(sfa, OVS_ACTION_ATTR_SAMPLE);
+	if (start < 0)
+		return start;
+	err = add_action(sfa, OVS_SAMPLE_ATTR_PROBABILITY, nla_data(probability), sizeof(u32));
+	if (err)
+		return err;
+	st_acts = add_nested_action_start(sfa, OVS_SAMPLE_ATTR_ACTIONS);
+	if (st_acts < 0)
+		return st_acts;
+
+	err = validate_and_copy_actions(actions, key, depth + 1, sfa);
+	if (err)
+		return err;
+
+	add_nested_action_end(*sfa, st_acts);
+	add_nested_action_end(*sfa, start);
+
+	return 0;
+}
+
+static int validate_tp_port(const struct sw_flow_key *flow_key)
+{
+	if (flow_key->eth.type == htons(ETH_P_IP)) {
+		if (flow_key->ipv4.tp.src || flow_key->ipv4.tp.dst)
+			return 0;
+	} else if (flow_key->eth.type == htons(ETH_P_IPV6)) {
+		if (flow_key->ipv6.tp.src || flow_key->ipv6.tp.dst)
+			return 0;
+	}
+
+	return -EINVAL;
+}
+
+static int validate_and_copy_set_tun(const struct nlattr *attr,
+				     struct sw_flow_actions **sfa)
+{
+	struct ovs_key_ipv4_tunnel tun_key;
+	int err, start;
+
+	err = ipv4_tun_from_nlattr(nla_data(attr), &tun_key);
+	if (err)
+		return err;
+
+	start = add_nested_action_start(sfa, OVS_ACTION_ATTR_SET);
+	if (start < 0)
+		return start;
+
+	err = add_action(sfa, OVS_KEY_ATTR_IPV4_TUNNEL, &tun_key, sizeof(tun_key));
+	add_nested_action_end(*sfa, start);
+
+	return err;
+}
+
+static int validate_set(const struct nlattr *a,
+			const struct sw_flow_key *flow_key,
+			struct sw_flow_actions **sfa,
+			bool *set_tun)
+{
+	const struct nlattr *ovs_key = nla_data(a);
+	int key_type = nla_type(ovs_key);
+
+	/* There can be only one key in a action */
+	if (nla_total_size(nla_len(ovs_key)) != nla_len(a))
+		return -EINVAL;
+
+	if (key_type > OVS_KEY_ATTR_MAX ||
+	    (ovs_key_lens[key_type] != nla_len(ovs_key) &&
+	     ovs_key_lens[key_type] != -1))
+		return -EINVAL;
+
+	switch (key_type) {
+	const struct ovs_key_ipv4 *ipv4_key;
+	const struct ovs_key_ipv6 *ipv6_key;
+	int err;
+
+	case OVS_KEY_ATTR_PRIORITY:
+	case OVS_KEY_ATTR_TUN_ID:
+	case OVS_KEY_ATTR_ETHERNET:
+		break;
+
+	case OVS_KEY_ATTR_SKB_MARK:
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,20) && !defined(CONFIG_NETFILTER)
+		if (nla_get_u32(ovs_key) != 0)
+			return -EINVAL;
+#endif
+		break;
+
+	case OVS_KEY_ATTR_TUNNEL:
+		*set_tun = true;
+		err = validate_and_copy_set_tun(a, sfa);
+		if (err)
+			return err;
+		break;
+
+	case OVS_KEY_ATTR_IPV4:
+		if (flow_key->eth.type != htons(ETH_P_IP))
+			return -EINVAL;
+
+		if (!flow_key->ip.proto)
+			return -EINVAL;
+
+		ipv4_key = nla_data(ovs_key);
+		if (ipv4_key->ipv4_proto != flow_key->ip.proto)
+			return -EINVAL;
+
+		if (ipv4_key->ipv4_frag != flow_key->ip.frag)
+			return -EINVAL;
+
+		break;
+
+	case OVS_KEY_ATTR_IPV6:
+		if (flow_key->eth.type != htons(ETH_P_IPV6))
+			return -EINVAL;
+
+		if (!flow_key->ip.proto)
+			return -EINVAL;
+
+		ipv6_key = nla_data(ovs_key);
+		if (ipv6_key->ipv6_proto != flow_key->ip.proto)
+			return -EINVAL;
+
+		if (ipv6_key->ipv6_frag != flow_key->ip.frag)
+			return -EINVAL;
+
+		if (ntohl(ipv6_key->ipv6_label) & 0xFFF00000)
+			return -EINVAL;
+
+		break;
+
+	case OVS_KEY_ATTR_TCP:
+		if (flow_key->ip.proto != IPPROTO_TCP)
+			return -EINVAL;
+
+		return validate_tp_port(flow_key);
+
+	case OVS_KEY_ATTR_UDP:
+		if (flow_key->ip.proto != IPPROTO_UDP)
+			return -EINVAL;
+
+		return validate_tp_port(flow_key);
+
+	default:
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int validate_userspace(const struct nlattr *attr)
+{
+	static const struct nla_policy userspace_policy[OVS_USERSPACE_ATTR_MAX + 1] =	{
+		[OVS_USERSPACE_ATTR_PID] = {.type = NLA_U32 },
+		[OVS_USERSPACE_ATTR_USERDATA] = {.type = NLA_U64 },
+	};
+	struct nlattr *a[OVS_USERSPACE_ATTR_MAX + 1];
+	int error;
+
+	error = nla_parse_nested(a, OVS_USERSPACE_ATTR_MAX,
+				 attr, userspace_policy);
+	if (error)
+		return error;
+
+	if (!a[OVS_USERSPACE_ATTR_PID] ||
+	    !nla_get_u32(a[OVS_USERSPACE_ATTR_PID]))
+		return -EINVAL;
+
+	return 0;
+}
+
+static int copy_action(const struct nlattr *from,
+		      struct sw_flow_actions **sfa)
+{
+	int totlen = NLA_ALIGN(from->nla_len);
+	struct nlattr *to;
+
+	to = reserve_sfa_size(sfa, from->nla_len);
+	if (IS_ERR(to))
+		return PTR_ERR(to);
+
+	memcpy(to, from, totlen);
+	return 0;
+}
+
+static int validate_and_copy_actions(const struct nlattr *attr,
+				const struct sw_flow_key *key,
+				int depth,
+				struct sw_flow_actions **sfa)
+{
+	const struct nlattr *a;
+	int rem, err;
+
+	if (depth >= SAMPLE_ACTION_DEPTH)
+		return -EOVERFLOW;
+
+	nla_for_each_nested(a, attr, rem) {
+		/* Expected argument lengths, (u32)-1 for variable length. */
+		static const u32 action_lens[OVS_ACTION_ATTR_MAX + 1] = {
+			[OVS_ACTION_ATTR_OUTPUT] = sizeof(u32),
+			[OVS_ACTION_ATTR_USERSPACE] = (u32)-1,
+			[OVS_ACTION_ATTR_PUSH_VLAN] = sizeof(struct ovs_action_push_vlan),
+			[OVS_ACTION_ATTR_POP_VLAN] = 0,
+			[OVS_ACTION_ATTR_SET] = (u32)-1,
+			[OVS_ACTION_ATTR_SAMPLE] = (u32)-1
+		};
+		const struct ovs_action_push_vlan *vlan;
+		int type = nla_type(a);
+		bool skip_copy;
+
+		if (type > OVS_ACTION_ATTR_MAX ||
+		    (action_lens[type] != nla_len(a) &&
+		     action_lens[type] != (u32)-1))
+			return -EINVAL;
+
+		skip_copy = false;
+		switch (type) {
+		case OVS_ACTION_ATTR_UNSPEC:
+			return -EINVAL;
+
+		case OVS_ACTION_ATTR_USERSPACE:
+			err = validate_userspace(a);
+			if (err)
+				return err;
+			break;
+
+		case OVS_ACTION_ATTR_OUTPUT:
+			if (nla_get_u32(a) >= DP_MAX_PORTS)
+				return -EINVAL;
+			break;
+
+
+		case OVS_ACTION_ATTR_POP_VLAN:
+			break;
+
+		case OVS_ACTION_ATTR_PUSH_VLAN:
+			vlan = nla_data(a);
+			if (vlan->vlan_tpid != htons(ETH_P_8021Q))
+				return -EINVAL;
+			if (!(vlan->vlan_tci & htons(VLAN_TAG_PRESENT)))
+				return -EINVAL;
+			break;
+
+		case OVS_ACTION_ATTR_SET:
+			err = validate_set(a, key, sfa, &skip_copy);
+			if (err)
+				return err;
+			break;
+
+		case OVS_ACTION_ATTR_SAMPLE:
+			err = validate_and_copy_sample(a, key, depth, sfa);
+			if (err)
+				return err;
+			skip_copy = true;
+			break;
+
+		default:
+			return -EINVAL;
+		}
+		if (!skip_copy) {
+			err = copy_action(a, sfa);
+			if (err)
+				return err;
+		}
+	}
+
+	if (rem > 0)
+		return -EINVAL;
+
+	return 0;
+}
+
+static void clear_stats(struct sw_flow *flow)
+{
+	flow->used = 0;
+	flow->tcp_flags = 0;
+	flow->packet_count = 0;
+	flow->byte_count = 0;
+}
+
+static int ovs_packet_cmd_execute(struct sk_buff *skb, struct genl_info *info)
+{
+	struct ovs_header *ovs_header = info->userhdr;
+	struct nlattr **a = info->attrs;
+	struct sw_flow_actions *acts;
+	struct sk_buff *packet;
+	struct sw_flow *flow;
+	struct datapath *dp;
+	struct ethhdr *eth;
+	int len;
+	int err;
+	int key_len;
+
+	err = -EINVAL;
+	if (!a[OVS_PACKET_ATTR_PACKET] || !a[OVS_PACKET_ATTR_KEY] ||
+	    !a[OVS_PACKET_ATTR_ACTIONS] ||
+	    nla_len(a[OVS_PACKET_ATTR_PACKET]) < ETH_HLEN)
+		goto err;
+
+	len = nla_len(a[OVS_PACKET_ATTR_PACKET]);
+	packet = __dev_alloc_skb(NET_IP_ALIGN + len, GFP_KERNEL);
+	err = -ENOMEM;
+	if (!packet)
+		goto err;
+	skb_reserve(packet, NET_IP_ALIGN);
+
+	memcpy(__skb_put(packet, len), nla_data(a[OVS_PACKET_ATTR_PACKET]), len);
+
+	skb_reset_mac_header(packet);
+	eth = eth_hdr(packet);
+
+	/* Normally, setting the skb 'protocol' field would be handled by a
+	 * call to eth_type_trans(), but it assumes there's a sending
+	 * device, which we may not have. */
+	if (ntohs(eth->h_proto) >= 1536)
+		packet->protocol = eth->h_proto;
+	else
+		packet->protocol = htons(ETH_P_802_2);
+
+	/* Build an sw_flow for sending this packet. */
+	flow = ovs_flow_alloc();
+	err = PTR_ERR(flow);
+	if (IS_ERR(flow))
+		goto err_kfree_skb;
+
+	err = ovs_flow_extract(packet, -1, &flow->key, &key_len);
+	if (err)
+		goto err_flow_free;
+
+	err = ovs_flow_metadata_from_nlattrs(flow, key_len, a[OVS_PACKET_ATTR_KEY]);
+	if (err)
+		goto err_flow_free;
+	acts = ovs_flow_actions_alloc(nla_len(a[OVS_PACKET_ATTR_ACTIONS]));
+	err = PTR_ERR(acts);
+	if (IS_ERR(acts))
+		goto err_flow_free;
+
+	err = validate_and_copy_actions(a[OVS_PACKET_ATTR_ACTIONS], &flow->key, 0, &acts);
+	rcu_assign_pointer(flow->sf_acts, acts);
+	if (err)
+		goto err_flow_free;
+
+	OVS_CB(packet)->flow = flow;
+	packet->priority = flow->key.phy.priority;
+	packet->mark = flow->key.phy.skb_mark;
+
+	rcu_read_lock();
+	dp = get_dp(sock_net(skb->sk), ovs_header->dp_ifindex);
+	err = -ENODEV;
+	if (!dp)
+		goto err_unlock;
+
+	local_bh_disable();
+	err = ovs_execute_actions(dp, packet);
+	local_bh_enable();
+	rcu_read_unlock();
+
+	ovs_flow_free(flow);
+	return err;
+
+err_unlock:
+	rcu_read_unlock();
+err_flow_free:
+	ovs_flow_free(flow);
+err_kfree_skb:
+	kfree_skb(packet);
+err:
+	return err;
+}
+
+static const struct nla_policy packet_policy[OVS_PACKET_ATTR_MAX + 1] = {
+	[OVS_PACKET_ATTR_PACKET] = { .type = NLA_UNSPEC },
+	[OVS_PACKET_ATTR_KEY] = { .type = NLA_NESTED },
+	[OVS_PACKET_ATTR_ACTIONS] = { .type = NLA_NESTED },
+};
+
+static struct genl_ops dp_packet_genl_ops[] = {
+	{ .cmd = OVS_PACKET_CMD_EXECUTE,
+	  .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */
+	  .policy = packet_policy,
+	  .doit = ovs_packet_cmd_execute
+	}
+};
+
+static void get_dp_stats(struct datapath *dp, struct ovs_dp_stats *stats)
+{
+	int i;
+	struct flow_table *table = genl_dereference(dp->table);
+
+	stats->n_flows = ovs_flow_tbl_count(table);
+
+	stats->n_hit = stats->n_missed = stats->n_lost = 0;
+	for_each_possible_cpu(i) {
+		const struct dp_stats_percpu *percpu_stats;
+		struct dp_stats_percpu local_stats;
+		unsigned int start;
+
+		percpu_stats = per_cpu_ptr(dp->stats_percpu, i);
+
+		do {
+			start = u64_stats_fetch_begin_bh(&percpu_stats->sync);
+			local_stats = *percpu_stats;
+		} while (u64_stats_fetch_retry_bh(&percpu_stats->sync, start));
+
+		stats->n_hit += local_stats.n_hit;
+		stats->n_missed += local_stats.n_missed;
+		stats->n_lost += local_stats.n_lost;
+	}
+}
+
+static const struct nla_policy flow_policy[OVS_FLOW_ATTR_MAX + 1] = {
+	[OVS_FLOW_ATTR_KEY] = { .type = NLA_NESTED },
+	[OVS_FLOW_ATTR_ACTIONS] = { .type = NLA_NESTED },
+	[OVS_FLOW_ATTR_CLEAR] = { .type = NLA_FLAG },
+};
+
+static struct genl_family dp_flow_genl_family = {
+	.id = GENL_ID_GENERATE,
+	.hdrsize = sizeof(struct ovs_header),
+	.name = OVS_FLOW_FAMILY,
+	.version = OVS_FLOW_VERSION,
+	.maxattr = OVS_FLOW_ATTR_MAX,
+	 .netnsok = true,
+};
+
+static struct genl_multicast_group ovs_dp_flow_multicast_group = {
+	.name = OVS_FLOW_MCGROUP
+};
+
+static int actions_to_attr(const struct nlattr *attr, int len, struct sk_buff *skb);
+static int sample_action_to_attr(const struct nlattr *attr, struct sk_buff *skb)
+{
+	const struct nlattr *a;
+	struct nlattr *start;
+	int err = 0, rem;
+
+	start = nla_nest_start(skb, OVS_ACTION_ATTR_SAMPLE);
+	if (!start)
+		return -EMSGSIZE;
+
+	nla_for_each_nested(a, attr, rem) {
+		int type = nla_type(a);
+		struct nlattr *st_sample;
+
+		switch (type) {
+		case OVS_SAMPLE_ATTR_PROBABILITY:
+			if (nla_put(skb, OVS_SAMPLE_ATTR_PROBABILITY, sizeof(u32), nla_data(a)))
+				return -EMSGSIZE;
+			break;
+		case OVS_SAMPLE_ATTR_ACTIONS:
+			st_sample = nla_nest_start(skb, OVS_SAMPLE_ATTR_ACTIONS);
+			if (!st_sample)
+				return -EMSGSIZE;
+			err = actions_to_attr(nla_data(a), nla_len(a), skb);
+			if (err)
+				return err;
+			nla_nest_end(skb, st_sample);
+			break;
+		}
+	}
+
+	nla_nest_end(skb, start);
+	return err;
+}
+
+static int set_action_to_attr(const struct nlattr *a, struct sk_buff *skb)
+{
+	const struct nlattr *ovs_key = nla_data(a);
+	int key_type = nla_type(ovs_key);
+	struct nlattr *start;
+	int err;
+
+	switch (key_type) {
+	case OVS_KEY_ATTR_IPV4_TUNNEL:
+		start = nla_nest_start(skb, OVS_ACTION_ATTR_SET);
+		if (!start)
+			return -EMSGSIZE;
+
+		err = ipv4_tun_to_nlattr(skb, nla_data(ovs_key));
+		if (err)
+			return err;
+		nla_nest_end(skb, start);
+		break;
+	default:
+		if (nla_put(skb, OVS_ACTION_ATTR_SET, nla_len(a), ovs_key))
+			return -EMSGSIZE;
+		break;
+	}
+
+	return 0;
+}
+
+static int actions_to_attr(const struct nlattr *attr, int len, struct sk_buff *skb)
+{
+	const struct nlattr *a;
+	int rem, err;
+
+	nla_for_each_attr(a, attr, len, rem) {
+		int type = nla_type(a);
+
+		switch (type) {
+		case OVS_ACTION_ATTR_SET:
+			err = set_action_to_attr(a, skb);
+			if (err)
+				return err;
+			break;
+
+		case OVS_ACTION_ATTR_SAMPLE:
+			err = sample_action_to_attr(a, skb);
+			if (err)
+				return err;
+			break;
+		default:
+			if (nla_put(skb, type, nla_len(a), nla_data(a)))
+				return -EMSGSIZE;
+			break;
+		}
+	}
+
+	return 0;
+}
+
+/* Called with genl_lock. */
+static int ovs_flow_cmd_fill_info(struct sw_flow *flow, struct datapath *dp,
+				  struct sk_buff *skb, u32 portid,
+				  u32 seq, u32 flags, u8 cmd)
+{
+	const int skb_orig_len = skb->len;
+	const struct sw_flow_actions *sf_acts;
+	struct nlattr *start;
+	struct ovs_flow_stats stats;
+	struct ovs_header *ovs_header;
+	struct nlattr *nla;
+	unsigned long used;
+	u8 tcp_flags;
+	int err;
+
+	sf_acts = rcu_dereference_protected(flow->sf_acts,
+					    lockdep_genl_is_held());
+
+	ovs_header = genlmsg_put(skb, portid, seq, &dp_flow_genl_family, flags, cmd);
+	if (!ovs_header)
+		return -EMSGSIZE;
+
+	ovs_header->dp_ifindex = get_dpifindex(dp);
+
+	nla = nla_nest_start(skb, OVS_FLOW_ATTR_KEY);
+	if (!nla)
+		goto nla_put_failure;
+	err = ovs_flow_to_nlattrs(&flow->key, skb);
+	if (err)
+		goto error;
+	nla_nest_end(skb, nla);
+
+	spin_lock_bh(&flow->lock);
+	used = flow->used;
+	stats.n_packets = flow->packet_count;
+	stats.n_bytes = flow->byte_count;
+	tcp_flags = flow->tcp_flags;
+	spin_unlock_bh(&flow->lock);
+
+	if (used &&
+	    nla_put_u64(skb, OVS_FLOW_ATTR_USED, ovs_flow_used_time(used)))
+		goto nla_put_failure;
+
+	if (stats.n_packets &&
+	    nla_put(skb, OVS_FLOW_ATTR_STATS,
+		    sizeof(struct ovs_flow_stats), &stats))
+		goto nla_put_failure;
+
+	if (tcp_flags &&
+	    nla_put_u8(skb, OVS_FLOW_ATTR_TCP_FLAGS, tcp_flags))
+		goto nla_put_failure;
+
+	/* If OVS_FLOW_ATTR_ACTIONS doesn't fit, skip dumping the actions if
+	 * this is the first flow to be dumped into 'skb'.  This is unusual for
+	 * Netlink but individual action lists can be longer than
+	 * NLMSG_GOODSIZE and thus entirely undumpable if we didn't do this.
+	 * The userspace caller can always fetch the actions separately if it
+	 * really wants them.  (Most userspace callers in fact don't care.)
+	 *
+	 * This can only fail for dump operations because the skb is always
+	 * properly sized for single flows.
+	 */
+	start = nla_nest_start(skb, OVS_FLOW_ATTR_ACTIONS);
+	if (start) {
+		err = actions_to_attr(sf_acts->actions, sf_acts->actions_len, skb);
+		if (!err)
+			nla_nest_end(skb, start);
+		else {
+			if (skb_orig_len)
+				goto error;
+
+			nla_nest_cancel(skb, start);
+		}
+	} else if (skb_orig_len)
+		goto nla_put_failure;
+
+	return genlmsg_end(skb, ovs_header);
+
+nla_put_failure:
+	err = -EMSGSIZE;
+error:
+	genlmsg_cancel(skb, ovs_header);
+	return err;
+}
+
+static struct sk_buff *ovs_flow_cmd_alloc_info(struct sw_flow *flow)
+{
+	const struct sw_flow_actions *sf_acts;
+	int len;
+
+	sf_acts = rcu_dereference_protected(flow->sf_acts,
+					    lockdep_genl_is_held());
+
+	/* OVS_FLOW_ATTR_KEY */
+	len = nla_total_size(FLOW_BUFSIZE);
+	/* OVS_FLOW_ATTR_ACTIONS */
+	len += nla_total_size(sf_acts->actions_len);
+	/* OVS_FLOW_ATTR_STATS */
+	len += nla_total_size(sizeof(struct ovs_flow_stats));
+	/* OVS_FLOW_ATTR_TCP_FLAGS */
+	len += nla_total_size(1);
+	/* OVS_FLOW_ATTR_USED */
+	len += nla_total_size(8);
+
+	len += NLMSG_ALIGN(sizeof(struct ovs_header));
+
+	return genlmsg_new(len, GFP_KERNEL);
+}
+
+static struct sk_buff *ovs_flow_cmd_build_info(struct sw_flow *flow,
+					       struct datapath *dp,
+					       u32 portid, u32 seq, u8 cmd)
+{
+	struct sk_buff *skb;
+	int retval;
+
+	skb = ovs_flow_cmd_alloc_info(flow);
+	if (!skb)
+		return ERR_PTR(-ENOMEM);
+
+	retval = ovs_flow_cmd_fill_info(flow, dp, skb, portid, seq, 0, cmd);
+	BUG_ON(retval < 0);
+	return skb;
+}
+
+static int ovs_flow_cmd_new_or_set(struct sk_buff *skb, struct genl_info *info)
+{
+	struct nlattr **a = info->attrs;
+	struct ovs_header *ovs_header = info->userhdr;
+	struct sw_flow_key key;
+	struct sw_flow *flow;
+	struct sk_buff *reply;
+	struct datapath *dp;
+	struct flow_table *table;
+	struct sw_flow_actions *acts = NULL;
+	int error;
+	int key_len;
+
+	/* Extract key. */
+	error = -EINVAL;
+	if (!a[OVS_FLOW_ATTR_KEY])
+		goto error;
+	error = ovs_flow_from_nlattrs(&key, &key_len, a[OVS_FLOW_ATTR_KEY]);
+	if (error)
+		goto error;
+
+	/* Validate actions. */
+	if (a[OVS_FLOW_ATTR_ACTIONS]) {
+		acts = ovs_flow_actions_alloc(nla_len(a[OVS_FLOW_ATTR_ACTIONS]));
+		error = PTR_ERR(acts);
+		if (IS_ERR(acts))
+			goto error;
+
+		error = validate_and_copy_actions(a[OVS_FLOW_ATTR_ACTIONS], &key,  0, &acts);
+		if (error)
+			goto err_kfree;
+	} else if (info->genlhdr->cmd == OVS_FLOW_CMD_NEW) {
+		error = -EINVAL;
+		goto error;
+	}
+
+	dp = get_dp(sock_net(skb->sk), ovs_header->dp_ifindex);
+	error = -ENODEV;
+	if (!dp)
+		goto err_kfree;
+
+	table = genl_dereference(dp->table);
+	flow = ovs_flow_tbl_lookup(table, &key, key_len);
+	if (!flow) {
+		/* Bail out if we're not allowed to create a new flow. */
+		error = -ENOENT;
+		if (info->genlhdr->cmd == OVS_FLOW_CMD_SET)
+			goto err_kfree;
+
+		/* Expand table, if necessary, to make room. */
+		if (ovs_flow_tbl_need_to_expand(table)) {
+			struct flow_table *new_table;
+
+			new_table = ovs_flow_tbl_expand(table);
+			if (!IS_ERR(new_table)) {
+				rcu_assign_pointer(dp->table, new_table);
+				ovs_flow_tbl_deferred_destroy(table);
+				table = genl_dereference(dp->table);
+			}
+		}
+
+		/* Allocate flow. */
+		flow = ovs_flow_alloc();
+		if (IS_ERR(flow)) {
+			error = PTR_ERR(flow);
+			goto err_kfree;
+		}
+		clear_stats(flow);
+
+		rcu_assign_pointer(flow->sf_acts, acts);
+
+		/* Put flow in bucket. */
+		ovs_flow_tbl_insert(table, flow, &key, key_len);
+
+		reply = ovs_flow_cmd_build_info(flow, dp, info->snd_pid,
+						info->snd_seq,
+						OVS_FLOW_CMD_NEW);
+	} else {
+		/* We found a matching flow. */
+		struct sw_flow_actions *old_acts;
+
+		/* Bail out if we're not allowed to modify an existing flow.
+		 * We accept NLM_F_CREATE in place of the intended NLM_F_EXCL
+		 * because Generic Netlink treats the latter as a dump
+		 * request.  We also accept NLM_F_EXCL in case that bug ever
+		 * gets fixed.
+		 */
+		error = -EEXIST;
+		if (info->genlhdr->cmd == OVS_FLOW_CMD_NEW &&
+		    info->nlhdr->nlmsg_flags & (NLM_F_CREATE | NLM_F_EXCL))
+			goto err_kfree;
+
+		/* Update actions. */
+		old_acts = rcu_dereference_protected(flow->sf_acts,
+						     lockdep_genl_is_held());
+		rcu_assign_pointer(flow->sf_acts, acts);
+		ovs_flow_deferred_free_acts(old_acts);
+
+		reply = ovs_flow_cmd_build_info(flow, dp, info->snd_pid,
+					       info->snd_seq, OVS_FLOW_CMD_NEW);
+
+		/* Clear stats. */
+		if (a[OVS_FLOW_ATTR_CLEAR]) {
+			spin_lock_bh(&flow->lock);
+			clear_stats(flow);
+			spin_unlock_bh(&flow->lock);
+		}
+	}
+
+	if (!IS_ERR(reply))
+		genl_notify(reply, genl_info_net(info), info->snd_pid,
+			   ovs_dp_flow_multicast_group.id, info->nlhdr,
+			   GFP_KERNEL);
+	else
+		netlink_set_err((sock_net(skb->sk))->genl_sock, 0,
+				ovs_dp_flow_multicast_group.id,	PTR_ERR(reply));
+	return 0;
+
+err_kfree:
+	kfree(acts);
+error:
+	return error;
+}
+
+static int ovs_flow_cmd_get(struct sk_buff *skb, struct genl_info *info)
+{
+	struct nlattr **a = info->attrs;
+	struct ovs_header *ovs_header = info->userhdr;
+	struct sw_flow_key key;
+	struct sk_buff *reply;
+	struct sw_flow *flow;
+	struct datapath *dp;
+	struct flow_table *table;
+	int err;
+	int key_len;
+
+	if (!a[OVS_FLOW_ATTR_KEY])
+		return -EINVAL;
+	err = ovs_flow_from_nlattrs(&key, &key_len, a[OVS_FLOW_ATTR_KEY]);
+	if (err)
+		return err;
+
+	dp = get_dp(sock_net(skb->sk), ovs_header->dp_ifindex);
+	if (!dp)
+		return -ENODEV;
+
+	table = genl_dereference(dp->table);
+	flow = ovs_flow_tbl_lookup(table, &key, key_len);
+	if (!flow)
+		return -ENOENT;
+
+	reply = ovs_flow_cmd_build_info(flow, dp, info->snd_pid,
+					info->snd_seq, OVS_FLOW_CMD_NEW);
+	if (IS_ERR(reply))
+		return PTR_ERR(reply);
+
+	return genlmsg_reply(reply, info);
+}
+
+static int ovs_flow_cmd_del(struct sk_buff *skb, struct genl_info *info)
+{
+	struct nlattr **a = info->attrs;
+	struct ovs_header *ovs_header = info->userhdr;
+	struct sw_flow_key key;
+	struct sk_buff *reply;
+	struct sw_flow *flow;
+	struct datapath *dp;
+	struct flow_table *table;
+	int err;
+	int key_len;
+
+	dp = get_dp(sock_net(skb->sk), ovs_header->dp_ifindex);
+	if (!dp)
+		return -ENODEV;
+
+	if (!a[OVS_FLOW_ATTR_KEY])
+		return flush_flows(dp);
+
+	err = ovs_flow_from_nlattrs(&key, &key_len, a[OVS_FLOW_ATTR_KEY]);
+	if (err)
+		return err;
+
+	table = genl_dereference(dp->table);
+	flow = ovs_flow_tbl_lookup(table, &key, key_len);
+	if (!flow)
+		return -ENOENT;
+
+	reply = ovs_flow_cmd_alloc_info(flow);
+	if (!reply)
+		return -ENOMEM;
+
+	ovs_flow_tbl_remove(table, flow);
+
+	err = ovs_flow_cmd_fill_info(flow, dp, reply, info->snd_pid,
+				     info->snd_seq, 0, OVS_FLOW_CMD_DEL);
+	BUG_ON(err < 0);
+
+	ovs_flow_deferred_free(flow);
+
+	genl_notify(reply, genl_info_net(info), info->snd_pid,
+		    ovs_dp_flow_multicast_group.id, info->nlhdr, GFP_KERNEL);
+	return 0;
+}
+
+static int ovs_flow_cmd_dump(struct sk_buff *skb, struct netlink_callback *cb)
+{
+	struct ovs_header *ovs_header = genlmsg_data(nlmsg_data(cb->nlh));
+	struct datapath *dp;
+	struct flow_table *table;
+
+	dp = get_dp(sock_net(skb->sk), ovs_header->dp_ifindex);
+	if (!dp)
+		return -ENODEV;
+
+	table = genl_dereference(dp->table);
+
+	for (;;) {
+		struct sw_flow *flow;
+		u32 bucket, obj;
+
+		bucket = cb->args[0];
+		obj = cb->args[1];
+		flow = ovs_flow_tbl_next(table, &bucket, &obj);
+		if (!flow)
+			break;
+
+		if (ovs_flow_cmd_fill_info(flow, dp, skb,
+					   NETLINK_CB(cb->skb).pid,
+					   cb->nlh->nlmsg_seq, NLM_F_MULTI,
+					   OVS_FLOW_CMD_NEW) < 0)
+			break;
+
+		cb->args[0] = bucket;
+		cb->args[1] = obj;
+	}
+	return skb->len;
+}
+
+static struct genl_ops dp_flow_genl_ops[] = {
+	{ .cmd = OVS_FLOW_CMD_NEW,
+	  .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */
+	  .policy = flow_policy,
+	  .doit = ovs_flow_cmd_new_or_set
+	},
+	{ .cmd = OVS_FLOW_CMD_DEL,
+	  .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */
+	  .policy = flow_policy,
+	  .doit = ovs_flow_cmd_del
+	},
+	{ .cmd = OVS_FLOW_CMD_GET,
+	  .flags = 0,		    /* OK for unprivileged users. */
+	  .policy = flow_policy,
+	  .doit = ovs_flow_cmd_get,
+	  .dumpit = ovs_flow_cmd_dump
+	},
+	{ .cmd = OVS_FLOW_CMD_SET,
+	  .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */
+	  .policy = flow_policy,
+	  .doit = ovs_flow_cmd_new_or_set,
+	},
+};
+
+static const struct nla_policy datapath_policy[OVS_DP_ATTR_MAX + 1] = {
+	[OVS_DP_ATTR_NAME] = { .type = NLA_NUL_STRING, .len = IFNAMSIZ - 1 },
+	[OVS_DP_ATTR_UPCALL_PID] = { .type = NLA_U32 },
+};
+
+static struct genl_family dp_datapath_genl_family = {
+	.id = GENL_ID_GENERATE,
+	.hdrsize = sizeof(struct ovs_header),
+	.name = OVS_DATAPATH_FAMILY,
+	.version = OVS_DATAPATH_VERSION,
+	.maxattr = OVS_DP_ATTR_MAX,
+	 .netnsok = true,
+};
+
+static struct genl_multicast_group ovs_dp_datapath_multicast_group = {
+	.name = OVS_DATAPATH_MCGROUP
+};
+
+static int ovs_dp_cmd_fill_info(struct datapath *dp, struct sk_buff *skb,
+				u32 portid, u32 seq, u32 flags, u8 cmd)
+{
+	struct ovs_header *ovs_header;
+	struct ovs_dp_stats dp_stats;
+	int err;
+
+	ovs_header = genlmsg_put(skb, portid, seq, &dp_datapath_genl_family,
+				   flags, cmd);
+	if (!ovs_header)
+		goto error;
+
+	ovs_header->dp_ifindex = get_dpifindex(dp);
+
+	rcu_read_lock();
+	err = nla_put_string(skb, OVS_DP_ATTR_NAME, ovs_dp_name(dp));
+	rcu_read_unlock();
+	if (err)
+		goto nla_put_failure;
+
+	get_dp_stats(dp, &dp_stats);
+	if (nla_put(skb, OVS_DP_ATTR_STATS, sizeof(struct ovs_dp_stats), &dp_stats))
+		goto nla_put_failure;
+
+	return genlmsg_end(skb, ovs_header);
+
+nla_put_failure:
+	genlmsg_cancel(skb, ovs_header);
+error:
+	return -EMSGSIZE;
+}
+
+static struct sk_buff *ovs_dp_cmd_build_info(struct datapath *dp, u32 portid,
+					     u32 seq, u8 cmd)
+{
+	struct sk_buff *skb;
+	int retval;
+
+	skb = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);
+	if (!skb)
+		return ERR_PTR(-ENOMEM);
+
+	retval = ovs_dp_cmd_fill_info(dp, skb, portid, seq, 0, cmd);
+	if (retval < 0) {
+		kfree_skb(skb);
+		return ERR_PTR(retval);
+	}
+	return skb;
+}
+
+static int ovs_dp_cmd_validate(struct nlattr *a[OVS_DP_ATTR_MAX + 1])
+{
+	return 0;
+}
+
+/* Called with genl_mutex and optionally with RTNL lock also. */
+static struct datapath *lookup_datapath(struct net *net,
+					struct ovs_header *ovs_header,
+					struct nlattr *a[OVS_DP_ATTR_MAX + 1])
+{
+	struct datapath *dp;
+
+	if (!a[OVS_DP_ATTR_NAME])
+		dp = get_dp(net, ovs_header->dp_ifindex);
+	else {
+		struct vport *vport;
+
+		rcu_read_lock();
+		vport = ovs_vport_locate(net, nla_data(a[OVS_DP_ATTR_NAME]));
+		dp = vport && vport->port_no == OVSP_LOCAL ? vport->dp : NULL;
+		rcu_read_unlock();
+	}
+	return dp ? dp : ERR_PTR(-ENODEV);
+}
+
+static int ovs_dp_cmd_new(struct sk_buff *skb, struct genl_info *info)
+{
+	struct nlattr **a = info->attrs;
+	struct vport_parms parms;
+	struct sk_buff *reply;
+	struct datapath *dp;
+	struct vport *vport;
+	struct ovs_net *ovs_net;
+	int err, i;
+
+	err = -EINVAL;
+	if (!a[OVS_DP_ATTR_NAME] || !a[OVS_DP_ATTR_UPCALL_PID])
+		goto err;
+
+	err = ovs_dp_cmd_validate(a);
+	if (err)
+		goto err;
+
+	rtnl_lock();
+
+	err = -ENOMEM;
+	dp = kzalloc(sizeof(*dp), GFP_KERNEL);
+	if (dp == NULL)
+		goto err_unlock_rtnl;
+
+	/* Initialize kobject for bridge.  This will be added as
+	 * /sys/class/net/<devname>/brif later, if sysfs is enabled. */
+	dp->ifobj.kset = NULL;
+	kobject_init(&dp->ifobj, &dp_ktype);
+
+	ovs_dp_set_net(dp, hold_net(sock_net(skb->sk)));
+
+	/* Allocate table. */
+	err = -ENOMEM;
+	rcu_assign_pointer(dp->table, ovs_flow_tbl_alloc(TBL_MIN_BUCKETS));
+	if (!dp->table)
+		goto err_free_dp;
+
+	dp->stats_percpu = alloc_percpu(struct dp_stats_percpu);
+	if (!dp->stats_percpu) {
+		err = -ENOMEM;
+		goto err_destroy_table;
+	}
+
+	dp->ports = kmalloc(DP_VPORT_HASH_BUCKETS * sizeof(struct hlist_head),
+			    GFP_KERNEL);
+	if (!dp->ports) {
+		err = -ENOMEM;
+		goto err_destroy_percpu;
+	}
+
+	for (i = 0; i < DP_VPORT_HASH_BUCKETS; i++)
+		INIT_HLIST_HEAD(&dp->ports[i]);
+
+	/* Set up our datapath device. */
+	parms.name = nla_data(a[OVS_DP_ATTR_NAME]);
+	parms.type = OVS_VPORT_TYPE_INTERNAL;
+	parms.options = NULL;
+	parms.dp = dp;
+	parms.port_no = OVSP_LOCAL;
+	parms.upcall_portid = nla_get_u32(a[OVS_DP_ATTR_UPCALL_PID]);
+
+	vport = new_vport(&parms);
+	if (IS_ERR(vport)) {
+		err = PTR_ERR(vport);
+		if (err == -EBUSY)
+			err = -EEXIST;
+
+		goto err_destroy_ports_array;
+	}
+
+	reply = ovs_dp_cmd_build_info(dp, info->snd_pid,
+				      info->snd_seq, OVS_DP_CMD_NEW);
+	err = PTR_ERR(reply);
+	if (IS_ERR(reply))
+		goto err_destroy_local_port;
+
+	ovs_net = net_generic(ovs_dp_get_net(dp), ovs_net_id);
+	list_add_tail(&dp->list_node, &ovs_net->dps);
+	ovs_dp_sysfs_add_dp(dp);
+
+	rtnl_unlock();
+
+	genl_notify(reply, genl_info_net(info), info->snd_pid,
+		    ovs_dp_datapath_multicast_group.id, info->nlhdr,
+		    GFP_KERNEL);
+	return 0;
+
+err_destroy_local_port:
+	ovs_dp_detach_port(ovs_vport_rtnl(dp, OVSP_LOCAL));
+err_destroy_ports_array:
+	kfree(dp->ports);
+err_destroy_percpu:
+	free_percpu(dp->stats_percpu);
+err_destroy_table:
+	ovs_flow_tbl_destroy(genl_dereference(dp->table));
+err_free_dp:
+	release_net(ovs_dp_get_net(dp));
+	kfree(dp);
+err_unlock_rtnl:
+	rtnl_unlock();
+err:
+	return err;
+}
+
+/* Called with genl_mutex. */
+static void __dp_destroy(struct datapath *dp)
+{
+	int i;
+
+	rtnl_lock();
+
+	for (i = 0; i < DP_VPORT_HASH_BUCKETS; i++) {
+		struct vport *vport;
+		struct hlist_node *node, *n;
+
+		hlist_for_each_entry_safe(vport, node, n, &dp->ports[i], dp_hash_node)
+			if (vport->port_no != OVSP_LOCAL)
+				ovs_dp_detach_port(vport);
+	}
+
+	ovs_dp_sysfs_del_dp(dp);
+	list_del(&dp->list_node);
+	ovs_dp_detach_port(ovs_vport_rtnl(dp, OVSP_LOCAL));
+
+	/* rtnl_unlock() will wait until all the references to devices that
+	 * are pending unregistration have been dropped.  We do it here to
+	 * ensure that any internal devices (which contain DP pointers) are
+	 * fully destroyed before freeing the datapath.
+	 */
+	rtnl_unlock();
+
+	call_rcu(&dp->rcu, destroy_dp_rcu);
+}
+
+static int ovs_dp_cmd_del(struct sk_buff *skb, struct genl_info *info)
+{
+	struct sk_buff *reply;
+	struct datapath *dp;
+	int err;
+
+	err = ovs_dp_cmd_validate(info->attrs);
+	if (err)
+		return err;
+
+	dp = lookup_datapath(sock_net(skb->sk), info->userhdr, info->attrs);
+	err = PTR_ERR(dp);
+	if (IS_ERR(dp))
+		return err;
+
+	reply = ovs_dp_cmd_build_info(dp, info->snd_pid,
+				      info->snd_seq, OVS_DP_CMD_DEL);
+	err = PTR_ERR(reply);
+	if (IS_ERR(reply))
+		return err;
+
+	__dp_destroy(dp);
+
+	genl_notify(reply, genl_info_net(info), info->snd_pid,
+		    ovs_dp_datapath_multicast_group.id, info->nlhdr,
+		    GFP_KERNEL);
+
+	return 0;
+}
+
+static int ovs_dp_cmd_set(struct sk_buff *skb, struct genl_info *info)
+{
+	struct sk_buff *reply;
+	struct datapath *dp;
+	int err;
+
+	err = ovs_dp_cmd_validate(info->attrs);
+	if (err)
+		return err;
+
+	dp = lookup_datapath(sock_net(skb->sk), info->userhdr, info->attrs);
+	if (IS_ERR(dp))
+		return PTR_ERR(dp);
+
+	reply = ovs_dp_cmd_build_info(dp, info->snd_pid,
+				      info->snd_seq, OVS_DP_CMD_NEW);
+	if (IS_ERR(reply)) {
+		err = PTR_ERR(reply);
+		netlink_set_err((sock_net(skb->sk))->genl_sock, 0,
+				ovs_dp_datapath_multicast_group.id, err);
+		return 0;
+	}
+
+	genl_notify(reply, genl_info_net(info), info->snd_pid,
+		    ovs_dp_datapath_multicast_group.id, info->nlhdr,
+		    GFP_KERNEL);
+
+	return 0;
+}
+
+static int ovs_dp_cmd_get(struct sk_buff *skb, struct genl_info *info)
+{
+	struct sk_buff *reply;
+	struct datapath *dp;
+	int err;
+
+	err = ovs_dp_cmd_validate(info->attrs);
+	if (err)
+		return err;
+
+	dp = lookup_datapath(sock_net(skb->sk), info->userhdr, info->attrs);
+	if (IS_ERR(dp))
+		return PTR_ERR(dp);
+
+	reply = ovs_dp_cmd_build_info(dp, info->snd_pid,
+				      info->snd_seq, OVS_DP_CMD_NEW);
+	if (IS_ERR(reply))
+		return PTR_ERR(reply);
+
+	return genlmsg_reply(reply, info);
+}
+
+static int ovs_dp_cmd_dump(struct sk_buff *skb, struct netlink_callback *cb)
+{
+	struct ovs_net *ovs_net = net_generic(sock_net(skb->sk), ovs_net_id);
+	struct datapath *dp;
+	int skip = cb->args[0];
+	int i = 0;
+
+	list_for_each_entry(dp, &ovs_net->dps, list_node) {
+		if (i >= skip &&
+		    ovs_dp_cmd_fill_info(dp, skb, NETLINK_CB(cb->skb).pid,
+					 cb->nlh->nlmsg_seq, NLM_F_MULTI,
+					 OVS_DP_CMD_NEW) < 0)
+			break;
+		i++;
+	}
+
+	cb->args[0] = i;
+
+	return skb->len;
+}
+
+static struct genl_ops dp_datapath_genl_ops[] = {
+	{ .cmd = OVS_DP_CMD_NEW,
+	  .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */
+	  .policy = datapath_policy,
+	  .doit = ovs_dp_cmd_new
+	},
+	{ .cmd = OVS_DP_CMD_DEL,
+	  .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */
+	  .policy = datapath_policy,
+	  .doit = ovs_dp_cmd_del
+	},
+	{ .cmd = OVS_DP_CMD_GET,
+	  .flags = 0,		    /* OK for unprivileged users. */
+	  .policy = datapath_policy,
+	  .doit = ovs_dp_cmd_get,
+	  .dumpit = ovs_dp_cmd_dump
+	},
+	{ .cmd = OVS_DP_CMD_SET,
+	  .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */
+	  .policy = datapath_policy,
+	  .doit = ovs_dp_cmd_set,
+	},
+};
+
+static const struct nla_policy vport_policy[OVS_VPORT_ATTR_MAX + 1] = {
+	[OVS_VPORT_ATTR_NAME] = { .type = NLA_NUL_STRING, .len = IFNAMSIZ - 1 },
+	[OVS_VPORT_ATTR_STATS] = { .len = sizeof(struct ovs_vport_stats) },
+	[OVS_VPORT_ATTR_ADDRESS] = { .len = ETH_ALEN },
+	[OVS_VPORT_ATTR_PORT_NO] = { .type = NLA_U32 },
+	[OVS_VPORT_ATTR_TYPE] = { .type = NLA_U32 },
+	[OVS_VPORT_ATTR_UPCALL_PID] = { .type = NLA_U32 },
+	[OVS_VPORT_ATTR_OPTIONS] = { .type = NLA_NESTED },
+};
+
+static struct genl_family dp_vport_genl_family = {
+	.id = GENL_ID_GENERATE,
+	.hdrsize = sizeof(struct ovs_header),
+	.name = OVS_VPORT_FAMILY,
+	.version = OVS_VPORT_VERSION,
+	.maxattr = OVS_VPORT_ATTR_MAX,
+	 .netnsok = true,
+};
+
+struct genl_multicast_group ovs_dp_vport_multicast_group = {
+	.name = OVS_VPORT_MCGROUP
+};
+
+/* Called with RTNL lock or RCU read lock. */
+static int ovs_vport_cmd_fill_info(struct vport *vport, struct sk_buff *skb,
+				   u32 portid, u32 seq, u32 flags, u8 cmd)
+{
+	struct ovs_header *ovs_header;
+	struct ovs_vport_stats vport_stats;
+	int err;
+
+	ovs_header = genlmsg_put(skb, portid, seq, &dp_vport_genl_family,
+				 flags, cmd);
+	if (!ovs_header)
+		return -EMSGSIZE;
+
+	ovs_header->dp_ifindex = get_dpifindex(vport->dp);
+
+	if (nla_put_u32(skb, OVS_VPORT_ATTR_PORT_NO, vport->port_no) ||
+	    nla_put_u32(skb, OVS_VPORT_ATTR_TYPE, vport->ops->type) ||
+	    nla_put_string(skb, OVS_VPORT_ATTR_NAME, vport->ops->get_name(vport)) ||
+	    nla_put_u32(skb, OVS_VPORT_ATTR_UPCALL_PID, vport->upcall_portid))
+		goto nla_put_failure;
+
+	ovs_vport_get_stats(vport, &vport_stats);
+	if (nla_put(skb, OVS_VPORT_ATTR_STATS, sizeof(struct ovs_vport_stats),
+		    &vport_stats))
+		goto nla_put_failure;
+
+	if (nla_put(skb, OVS_VPORT_ATTR_ADDRESS, ETH_ALEN,
+		    vport->ops->get_addr(vport)))
+		goto nla_put_failure;
+
+	err = ovs_vport_get_options(vport, skb);
+	if (err == -EMSGSIZE)
+		goto error;
+
+	return genlmsg_end(skb, ovs_header);
+
+nla_put_failure:
+	err = -EMSGSIZE;
+error:
+	genlmsg_cancel(skb, ovs_header);
+	return err;
+}
+
+/* Called with RTNL lock or RCU read lock. */
+struct sk_buff *ovs_vport_cmd_build_info(struct vport *vport, u32 portid,
+					 u32 seq, u8 cmd)
+{
+	struct sk_buff *skb;
+	int retval;
+
+	skb = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_ATOMIC);
+	if (!skb)
+		return ERR_PTR(-ENOMEM);
+
+	retval = ovs_vport_cmd_fill_info(vport, skb, portid, seq, 0, cmd);
+	if (retval < 0) {
+		kfree_skb(skb);
+		return ERR_PTR(retval);
+	}
+	return skb;
+}
+
+static int ovs_vport_cmd_validate(struct nlattr *a[OVS_VPORT_ATTR_MAX + 1])
+{
+	return 0;
+}
+
+/* Called with RTNL lock or RCU read lock. */
+static struct vport *lookup_vport(struct net *net,
+				  struct ovs_header *ovs_header,
+				  struct nlattr *a[OVS_VPORT_ATTR_MAX + 1])
+{
+	struct datapath *dp;
+	struct vport *vport;
+
+	if (a[OVS_VPORT_ATTR_NAME]) {
+		vport = ovs_vport_locate(net, nla_data(a[OVS_VPORT_ATTR_NAME]));
+		if (!vport)
+			return ERR_PTR(-ENODEV);
+		if (ovs_header->dp_ifindex &&
+		    ovs_header->dp_ifindex != get_dpifindex(vport->dp))
+			return ERR_PTR(-ENODEV);
+		return vport;
+	} else if (a[OVS_VPORT_ATTR_PORT_NO]) {
+		u32 port_no = nla_get_u32(a[OVS_VPORT_ATTR_PORT_NO]);
+
+		if (port_no >= DP_MAX_PORTS)
+			return ERR_PTR(-EFBIG);
+
+		dp = get_dp(net, ovs_header->dp_ifindex);
+		if (!dp)
+			return ERR_PTR(-ENODEV);
+
+		vport = ovs_vport_rtnl_rcu(dp, port_no);
+		if (!vport)
+			return ERR_PTR(-ENOENT);
+		return vport;
+	} else
+		return ERR_PTR(-EINVAL);
+}
+
+/* Called with RTNL lock. */
+static int change_vport(struct vport *vport,
+			struct nlattr *a[OVS_VPORT_ATTR_MAX + 1])
+{
+	int err = 0;
+
+	if (a[OVS_VPORT_ATTR_STATS])
+		ovs_vport_set_stats(vport, nla_data(a[OVS_VPORT_ATTR_STATS]));
+
+	if (a[OVS_VPORT_ATTR_ADDRESS])
+		err = ovs_vport_set_addr(vport, nla_data(a[OVS_VPORT_ATTR_ADDRESS]));
+
+	return err;
+}
+
+static int ovs_vport_cmd_new(struct sk_buff *skb, struct genl_info *info)
+{
+	struct nlattr **a = info->attrs;
+	struct ovs_header *ovs_header = info->userhdr;
+	struct vport_parms parms;
+	struct sk_buff *reply;
+	struct vport *vport;
+	struct datapath *dp;
+	u32 port_no;
+	int err;
+
+	err = -EINVAL;
+	if (!a[OVS_VPORT_ATTR_NAME] || !a[OVS_VPORT_ATTR_TYPE] ||
+	    !a[OVS_VPORT_ATTR_UPCALL_PID])
+		goto exit;
+
+	err = ovs_vport_cmd_validate(a);
+	if (err)
+		goto exit;
+
+	rtnl_lock();
+	dp = get_dp(sock_net(skb->sk), ovs_header->dp_ifindex);
+	err = -ENODEV;
+	if (!dp)
+		goto exit_unlock;
+
+	if (a[OVS_VPORT_ATTR_PORT_NO]) {
+		port_no = nla_get_u32(a[OVS_VPORT_ATTR_PORT_NO]);
+
+		err = -EFBIG;
+		if (port_no >= DP_MAX_PORTS)
+			goto exit_unlock;
+
+		vport = ovs_vport_rtnl(dp, port_no);
+		err = -EBUSY;
+		if (vport)
+			goto exit_unlock;
+	} else {
+		for (port_no = 1; ; port_no++) {
+			if (port_no >= DP_MAX_PORTS) {
+				err = -EFBIG;
+				goto exit_unlock;
+			}
+			vport = ovs_vport_rtnl(dp, port_no);
+			if (!vport)
+				break;
+		}
+	}
+
+	parms.name = nla_data(a[OVS_VPORT_ATTR_NAME]);
+	parms.type = nla_get_u32(a[OVS_VPORT_ATTR_TYPE]);
+	parms.options = a[OVS_VPORT_ATTR_OPTIONS];
+	parms.dp = dp;
+	parms.port_no = port_no;
+	parms.upcall_portid = nla_get_u32(a[OVS_VPORT_ATTR_UPCALL_PID]);
+
+	vport = new_vport(&parms);
+	err = PTR_ERR(vport);
+	if (IS_ERR(vport))
+		goto exit_unlock;
+
+	ovs_dp_sysfs_add_if(vport);
+
+	err = change_vport(vport, a);
+	if (!err) {
+		reply = ovs_vport_cmd_build_info(vport, info->snd_pid,
+						 info->snd_seq,
+						 OVS_VPORT_CMD_NEW);
+		if (IS_ERR(reply))
+			err = PTR_ERR(reply);
+	}
+	if (err) {
+		ovs_dp_detach_port(vport);
+		goto exit_unlock;
+	}
+	genl_notify(reply, genl_info_net(info), info->snd_pid,
+		    ovs_dp_vport_multicast_group.id, info->nlhdr, GFP_KERNEL);
+
+exit_unlock:
+	rtnl_unlock();
+exit:
+	return err;
+}
+
+static int ovs_vport_cmd_set(struct sk_buff *skb, struct genl_info *info)
+{
+	struct nlattr **a = info->attrs;
+	struct sk_buff *reply;
+	struct vport *vport;
+	int err;
+
+	err = ovs_vport_cmd_validate(a);
+	if (err)
+		goto exit;
+
+	rtnl_lock();
+	vport = lookup_vport(sock_net(skb->sk), info->userhdr, a);
+	err = PTR_ERR(vport);
+	if (IS_ERR(vport))
+		goto exit_unlock;
+
+	err = 0;
+	if (a[OVS_VPORT_ATTR_TYPE] &&
+	    nla_get_u32(a[OVS_VPORT_ATTR_TYPE]) != vport->ops->type)
+		err = -EINVAL;
+
+	if (!err && a[OVS_VPORT_ATTR_OPTIONS])
+		err = ovs_vport_set_options(vport, a[OVS_VPORT_ATTR_OPTIONS]);
+	if (!err)
+		err = change_vport(vport, a);
+	else
+		goto exit_unlock;
+	if (!err && a[OVS_VPORT_ATTR_UPCALL_PID])
+		vport->upcall_portid = nla_get_u32(a[OVS_VPORT_ATTR_UPCALL_PID]);
+
+	reply = ovs_vport_cmd_build_info(vport, info->snd_pid,
+					 info->snd_seq, OVS_VPORT_CMD_NEW);
+	if (IS_ERR(reply)) {
+		netlink_set_err((sock_net(skb->sk))->genl_sock, 0,
+				ovs_dp_vport_multicast_group.id, PTR_ERR(reply));
+		goto exit_unlock;
+	}
+
+	genl_notify(reply, genl_info_net(info), info->snd_pid,
+		    ovs_dp_vport_multicast_group.id, info->nlhdr, GFP_KERNEL);
+
+exit_unlock:
+	rtnl_unlock();
+exit:
+	return err;
+}
+
+static int ovs_vport_cmd_del(struct sk_buff *skb, struct genl_info *info)
+{
+	struct nlattr **a = info->attrs;
+	struct sk_buff *reply;
+	struct vport *vport;
+	int err;
+
+	err = ovs_vport_cmd_validate(a);
+	if (err)
+		goto exit;
+
+	rtnl_lock();
+	vport = lookup_vport(sock_net(skb->sk), info->userhdr, a);
+	err = PTR_ERR(vport);
+	if (IS_ERR(vport))
+		goto exit_unlock;
+
+	if (vport->port_no == OVSP_LOCAL) {
+		err = -EINVAL;
+		goto exit_unlock;
+	}
+
+	reply = ovs_vport_cmd_build_info(vport, info->snd_pid,
+					 info->snd_seq, OVS_VPORT_CMD_DEL);
+	err = PTR_ERR(reply);
+	if (IS_ERR(reply))
+		goto exit_unlock;
+
+	err = 0;
+	ovs_dp_detach_port(vport);
+
+	genl_notify(reply, genl_info_net(info), info->snd_pid,
+		    ovs_dp_vport_multicast_group.id, info->nlhdr, GFP_KERNEL);
+
+exit_unlock:
+	rtnl_unlock();
+exit:
+	return err;
+}
+
+static int ovs_vport_cmd_get(struct sk_buff *skb, struct genl_info *info)
+{
+	struct nlattr **a = info->attrs;
+	struct ovs_header *ovs_header = info->userhdr;
+	struct sk_buff *reply;
+	struct vport *vport;
+	int err;
+
+	err = ovs_vport_cmd_validate(a);
+	if (err)
+		goto exit;
+
+	rcu_read_lock();
+	vport = lookup_vport(sock_net(skb->sk), ovs_header, a);
+	err = PTR_ERR(vport);
+	if (IS_ERR(vport))
+		goto exit_unlock;
+
+	reply = ovs_vport_cmd_build_info(vport, info->snd_pid,
+					 info->snd_seq, OVS_VPORT_CMD_NEW);
+	err = PTR_ERR(reply);
+	if (IS_ERR(reply))
+		goto exit_unlock;
+
+	rcu_read_unlock();
+
+	return genlmsg_reply(reply, info);
+
+exit_unlock:
+	rcu_read_unlock();
+exit:
+	return err;
+}
+
+static int ovs_vport_cmd_dump(struct sk_buff *skb, struct netlink_callback *cb)
+{
+	struct ovs_header *ovs_header = genlmsg_data(nlmsg_data(cb->nlh));
+	struct datapath *dp;
+	int bucket = cb->args[0], skip = cb->args[1];
+	int i, j = 0;
+
+	dp = get_dp(sock_net(skb->sk), ovs_header->dp_ifindex);
+	if (!dp)
+		return -ENODEV;
+
+	rcu_read_lock();
+	for (i = bucket; i < DP_VPORT_HASH_BUCKETS; i++) {
+		struct vport *vport;
+		struct hlist_node *n;
+
+		j = 0;
+		hlist_for_each_entry_rcu(vport, n, &dp->ports[i], dp_hash_node) {
+			if (j >= skip &&
+			    ovs_vport_cmd_fill_info(vport, skb,
+						    NETLINK_CB(cb->skb).pid,
+						    cb->nlh->nlmsg_seq,
+						    NLM_F_MULTI,
+						    OVS_VPORT_CMD_NEW) < 0)
+				goto out;
+
+			j++;
+		}
+		skip = 0;
+	}
+out:
+	rcu_read_unlock();
+
+	cb->args[0] = i;
+	cb->args[1] = j;
+
+	return skb->len;
+}
+
+static struct genl_ops dp_vport_genl_ops[] = {
+	{ .cmd = OVS_VPORT_CMD_NEW,
+	  .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */
+	  .policy = vport_policy,
+	  .doit = ovs_vport_cmd_new
+	},
+	{ .cmd = OVS_VPORT_CMD_DEL,
+	  .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */
+	  .policy = vport_policy,
+	  .doit = ovs_vport_cmd_del
+	},
+	{ .cmd = OVS_VPORT_CMD_GET,
+	  .flags = 0,		    /* OK for unprivileged users. */
+	  .policy = vport_policy,
+	  .doit = ovs_vport_cmd_get,
+	  .dumpit = ovs_vport_cmd_dump
+	},
+	{ .cmd = OVS_VPORT_CMD_SET,
+	  .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */
+	  .policy = vport_policy,
+	  .doit = ovs_vport_cmd_set,
+	},
+};
+
+struct genl_family_and_ops {
+	struct genl_family *family;
+	struct genl_ops *ops;
+	int n_ops;
+	struct genl_multicast_group *group;
+};
+
+static const struct genl_family_and_ops dp_genl_families[] = {
+	{ &dp_datapath_genl_family,
+	  dp_datapath_genl_ops, ARRAY_SIZE(dp_datapath_genl_ops),
+	  &ovs_dp_datapath_multicast_group },
+	{ &dp_vport_genl_family,
+	  dp_vport_genl_ops, ARRAY_SIZE(dp_vport_genl_ops),
+	  &ovs_dp_vport_multicast_group },
+	{ &dp_flow_genl_family,
+	  dp_flow_genl_ops, ARRAY_SIZE(dp_flow_genl_ops),
+	  &ovs_dp_flow_multicast_group },
+	{ &dp_packet_genl_family,
+	  dp_packet_genl_ops, ARRAY_SIZE(dp_packet_genl_ops),
+	  NULL },
+};
+
+static void dp_unregister_genl(int n_families)
+{
+	int i;
+
+	for (i = 0; i < n_families; i++)
+		genl_unregister_family(dp_genl_families[i].family);
+}
+
+static int dp_register_genl(void)
+{
+	int n_registered;
+	int err;
+	int i;
+
+	n_registered = 0;
+	for (i = 0; i < ARRAY_SIZE(dp_genl_families); i++) {
+		const struct genl_family_and_ops *f = &dp_genl_families[i];
+
+		err = genl_register_family_with_ops(f->family, f->ops,
+						    f->n_ops);
+		if (err)
+			goto error;
+		n_registered++;
+
+		if (f->group) {
+			err = genl_register_mc_group(f->family, f->group);
+			if (err)
+				goto error;
+		}
+	}
+
+	return 0;
+
+error:
+	dp_unregister_genl(n_registered);
+	return err;
+}
+
+static int __rehash_flow_table(void *dummy)
+{
+	struct datapath *dp;
+	struct net *net;
+
+	rtnl_lock();
+	for_each_net(net) {
+		struct ovs_net *ovs_net = net_generic(net, ovs_net_id);
+
+		list_for_each_entry(dp, &ovs_net->dps, list_node) {
+			struct flow_table *old_table = genl_dereference(dp->table);
+			struct flow_table *new_table;
+
+			new_table = ovs_flow_tbl_rehash(old_table);
+			if (!IS_ERR(new_table)) {
+				rcu_assign_pointer(dp->table, new_table);
+				ovs_flow_tbl_deferred_destroy(old_table);
+			}
+		}
+	}
+	rtnl_unlock();
+	return 0;
+}
+
+static void rehash_flow_table(struct work_struct *work)
+{
+	genl_exec(__rehash_flow_table, NULL);
+	schedule_delayed_work(&rehash_flow_wq, REHASH_FLOW_INTERVAL);
+}
+
+static int dp_destroy_all(void *data)
+{
+	struct datapath *dp, *dp_next;
+	struct ovs_net *ovs_net = data;
+
+	list_for_each_entry_safe(dp, dp_next, &ovs_net->dps, list_node)
+		__dp_destroy(dp);
+
+	return 0;
+}
+
+static int __net_init ovs_init_net(struct net *net)
+{
+	struct ovs_net *ovs_net = net_generic(net, ovs_net_id);
+
+	INIT_LIST_HEAD(&ovs_net->dps);
+	return 0;
+}
+
+static void __net_exit ovs_exit_net(struct net *net)
+{
+	struct ovs_net *ovs_net = net_generic(net, ovs_net_id);
+
+	genl_exec(dp_destroy_all, ovs_net);
+}
+
+static struct pernet_operations ovs_net_ops = {
+	.init = ovs_init_net,
+	.exit = ovs_exit_net,
+	.id   = &ovs_net_id,
+	.size = sizeof(struct ovs_net),
+};
+
+static int __init dp_init(void)
+{
+	struct sk_buff *dummy_skb;
+	int err;
+
+	BUILD_BUG_ON(sizeof(struct ovs_skb_cb) > sizeof(dummy_skb->cb));
+
+	pr_info("Open vSwitch switching datapath %s, built "__DATE__" "__TIME__"\n",
+		OVS_VERSION);
+
+	err = genl_exec_init();
+	if (err)
+		goto error;
+
+	err = ovs_tnl_init();
+	if (err)
+		goto error_genl_exec;
+
+	err = ovs_flow_init();
+	if (err)
+		goto error_tnl_exit;
+
+	err = ovs_vport_init();
+	if (err)
+		goto error_flow_exit;
+
+	err = register_pernet_device(&ovs_net_ops);
+	if (err)
+		goto error_vport_exit;
+
+	err = register_netdevice_notifier(&ovs_dp_device_notifier);
+	if (err)
+		goto error_netns_exit;
+
+	err = dp_register_genl();
+	if (err < 0)
+		goto error_unreg_notifier;
+
+	schedule_delayed_work(&rehash_flow_wq, REHASH_FLOW_INTERVAL);
+
+	return 0;
+
+error_unreg_notifier:
+	unregister_netdevice_notifier(&ovs_dp_device_notifier);
+error_netns_exit:
+	unregister_pernet_device(&ovs_net_ops);
+error_vport_exit:
+	ovs_vport_exit();
+error_flow_exit:
+	ovs_flow_exit();
+error_tnl_exit:
+	ovs_tnl_exit();
+error_genl_exec:
+	genl_exec_exit();
+error:
+	return err;
+}
+
+static void dp_cleanup(void)
+{
+	cancel_delayed_work_sync(&rehash_flow_wq);
+	dp_unregister_genl(ARRAY_SIZE(dp_genl_families));
+	unregister_netdevice_notifier(&ovs_dp_device_notifier);
+	unregister_pernet_device(&ovs_net_ops);
+	rcu_barrier();
+	ovs_vport_exit();
+	ovs_flow_exit();
+	ovs_tnl_exit();
+	genl_exec_exit();
+}
+
+module_init(dp_init);
+module_exit(dp_cleanup);
+
+MODULE_DESCRIPTION("Open vSwitch switching datapath");
+MODULE_LICENSE("GPL");
+MODULE_VERSION(OVS_VERSION);
diff --git a/drivers/staging/openvswitch_nv/datapath.h b/drivers/staging/openvswitch_nv/datapath.h
new file mode 100644
index 000000000000..6930b0c8129e
--- /dev/null
+++ b/drivers/staging/openvswitch_nv/datapath.h
@@ -0,0 +1,195 @@
+/*
+ * Copyright (c) 2007-2012 Nicira, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#ifndef DATAPATH_H
+#define DATAPATH_H 1
+
+#include <asm/page.h>
+#include <linux/kernel.h>
+#include <linux/mutex.h>
+#include <linux/netdevice.h>
+#include <linux/skbuff.h>
+#include <linux/u64_stats_sync.h>
+
+#include "checksum.h"
+#include "dp_sysfs.h"
+#include "flow.h"
+#include "tunnel.h"
+#include "vlan.h"
+#include "vport.h"
+
+#define DP_MAX_PORTS		USHRT_MAX
+#define DP_VPORT_HASH_BUCKETS	1024
+
+#define SAMPLE_ACTION_DEPTH 3
+
+/**
+ * struct dp_stats_percpu - per-cpu packet processing statistics for a given
+ * datapath.
+ * @n_hit: Number of received packets for which a matching flow was found in
+ * the flow table.
+ * @n_miss: Number of received packets that had no matching flow in the flow
+ * table.  The sum of @n_hit and @n_miss is the number of packets that have
+ * been received by the datapath.
+ * @n_lost: Number of received packets that had no matching flow in the flow
+ * table that could not be sent to userspace (normally due to an overflow in
+ * one of the datapath's queues).
+ */
+struct dp_stats_percpu {
+	u64 n_hit;
+	u64 n_missed;
+	u64 n_lost;
+	struct u64_stats_sync sync;
+};
+
+/**
+ * struct datapath - datapath for flow-based packet switching
+ * @rcu: RCU callback head for deferred destruction.
+ * @list_node: Element in global 'dps' list.
+ * @ifobj: Represents /sys/class/net/<devname>/brif.  Protected by RTNL.
+ * @n_flows: Number of flows currently in flow table.
+ * @table: Current flow table.  Protected by genl_lock and RCU.
+ * @ports: Hash table for ports.  %OVSP_LOCAL port always exists.  Protected by
+ * RTNL and RCU.
+ * @stats_percpu: Per-CPU datapath statistics.
+ * @net: Reference to net namespace.
+ *
+ * Context: See the comment on locking at the top of datapath.c for additional
+ * locking information.
+ */
+struct datapath {
+	struct rcu_head rcu;
+	struct list_head list_node;
+	struct kobject ifobj;
+
+	/* Flow table. */
+	struct flow_table __rcu *table;
+
+	/* Switch ports. */
+	struct hlist_head *ports;
+
+	/* Stats. */
+	struct dp_stats_percpu __percpu *stats_percpu;
+
+#ifdef CONFIG_NET_NS
+	/* Network namespace ref. */
+	struct net *net;
+#endif
+};
+
+/**
+ * struct ovs_skb_cb - OVS data in skb CB
+ * @flow: The flow associated with this packet.  May be %NULL if no flow.
+ * @tun_key: Key for the tunnel that encapsulated this packet. NULL if the
+ * packet is not being tunneled.
+ * @ip_summed: Consistently stores L4 checksumming status across different
+ * kernel versions.
+ * @csum_start: Stores the offset from which to start checksumming independent
+ * of the transport header on all kernel versions.
+ * packet was not received on a tunnel.
+ * @vlan_tci: Provides a substitute for the skb->vlan_tci field on kernels
+ * before 2.6.27.
+ */
+struct ovs_skb_cb {
+	struct sw_flow		*flow;
+	struct ovs_key_ipv4_tunnel  *tun_key;
+#ifdef NEED_CSUM_NORMALIZE
+	enum csum_type		ip_summed;
+	u16			csum_start;
+#endif
+#ifdef NEED_VLAN_FIELD
+	u16			vlan_tci;
+#endif
+};
+#define OVS_CB(skb) ((struct ovs_skb_cb *)(skb)->cb)
+
+/**
+ * struct dp_upcall - metadata to include with a packet to send to userspace
+ * @cmd: One of %OVS_PACKET_CMD_*.
+ * @key: Becomes %OVS_PACKET_ATTR_KEY.  Must be nonnull.
+ * @userdata: If nonnull, its u64 value is extracted and passed to userspace as
+ * %OVS_PACKET_ATTR_USERDATA.
+ * @portid: Netlink PID to which packet should be sent.  If @portid is 0 then no
+ * packet is sent and the packet is accounted in the datapath's @n_lost
+ * counter.
+ */
+struct dp_upcall_info {
+	u8 cmd;
+	const struct sw_flow_key *key;
+	const struct nlattr *userdata;
+	u32 portid;
+};
+
+/**
+ * struct ovs_net - Per net-namespace data for ovs.
+ * @dps: List of datapaths to enable dumping them all out.
+ * Protected by genl_mutex.
+ * @vport_net: Per network namespace data for vport.
+ */
+struct ovs_net {
+	struct list_head dps;
+	struct vport_net vport_net;
+};
+
+extern int ovs_net_id;
+
+static inline struct net *ovs_dp_get_net(struct datapath *dp)
+{
+	return read_pnet(&dp->net);
+}
+
+static inline void ovs_dp_set_net(struct datapath *dp, struct net *net)
+{
+	write_pnet(&dp->net, net);
+}
+
+struct vport *ovs_lookup_vport(const struct datapath *dp, u16 port_no);
+
+static inline struct vport *ovs_vport_rcu(const struct datapath *dp, int port_no)
+{
+	WARN_ON_ONCE(!rcu_read_lock_held());
+	return ovs_lookup_vport(dp, port_no);
+}
+
+static inline struct vport *ovs_vport_rtnl_rcu(const struct datapath *dp, int port_no)
+{
+	WARN_ON_ONCE(!rcu_read_lock_held() && !rtnl_is_locked());
+	return ovs_lookup_vport(dp, port_no);
+}
+
+static inline struct vport *ovs_vport_rtnl(const struct datapath *dp, int port_no)
+{
+	ASSERT_RTNL();
+	return ovs_lookup_vport(dp, port_no);
+}
+
+extern struct notifier_block ovs_dp_device_notifier;
+extern struct genl_multicast_group ovs_dp_vport_multicast_group;
+extern int (*ovs_dp_ioctl_hook)(struct net_device *dev, struct ifreq *rq, int cmd);
+
+void ovs_dp_process_received_packet(struct vport *, struct sk_buff *);
+void ovs_dp_detach_port(struct vport *);
+int ovs_dp_upcall(struct datapath *, struct sk_buff *,
+		  const struct dp_upcall_info *);
+
+const char *ovs_dp_name(const struct datapath *dp);
+struct sk_buff *ovs_vport_cmd_build_info(struct vport *, u32 portid, u32 seq,
+					 u8 cmd);
+
+int ovs_execute_actions(struct datapath *dp, struct sk_buff *skb);
+#endif /* datapath.h */
diff --git a/drivers/staging/openvswitch_nv/dp_notify.c b/drivers/staging/openvswitch_nv/dp_notify.c
new file mode 100644
index 000000000000..f53c77b94d55
--- /dev/null
+++ b/drivers/staging/openvswitch_nv/dp_notify.c
@@ -0,0 +1,75 @@
+/*
+ * Copyright (c) 2007-2012 Nicira, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#include <linux/netdevice.h>
+#include <net/genetlink.h>
+
+#include "datapath.h"
+#include "vport-internal_dev.h"
+#include "vport-netdev.h"
+
+static int dp_device_event(struct notifier_block *unused, unsigned long event,
+			   void *ptr)
+{
+	struct net_device *dev = ptr;
+	struct vport *vport;
+
+	if (ovs_is_internal_dev(dev))
+		vport = ovs_internal_dev_get_vport(dev);
+	else
+		vport = ovs_netdev_get_vport(dev);
+
+	if (!vport)
+		return NOTIFY_DONE;
+
+	switch (event) {
+	case NETDEV_UNREGISTER:
+		if (!ovs_is_internal_dev(dev)) {
+			struct sk_buff *notify;
+			struct datapath *dp = vport->dp;
+
+			notify = ovs_vport_cmd_build_info(vport, 0, 0,
+							  OVS_VPORT_CMD_DEL);
+			ovs_dp_detach_port(vport);
+			if (IS_ERR(notify)) {
+				netlink_set_err((ovs_dp_get_net(dp))->genl_sock, 0,
+						ovs_dp_vport_multicast_group.id,
+						PTR_ERR(notify));
+				break;
+			}
+
+			genlmsg_multicast_netns(ovs_dp_get_net(dp), notify, 0,
+						ovs_dp_vport_multicast_group.id,
+						GFP_KERNEL);
+		}
+		break;
+
+	case NETDEV_CHANGENAME:
+		if (vport->port_no != OVSP_LOCAL) {
+			ovs_dp_sysfs_del_if(vport);
+			ovs_dp_sysfs_add_if(vport);
+		}
+		break;
+	}
+
+	return NOTIFY_DONE;
+}
+
+struct notifier_block ovs_dp_device_notifier = {
+	.notifier_call = dp_device_event
+};
diff --git a/drivers/staging/openvswitch_nv/dp_sysfs.h b/drivers/staging/openvswitch_nv/dp_sysfs.h
new file mode 100644
index 000000000000..938cc2578cb8
--- /dev/null
+++ b/drivers/staging/openvswitch_nv/dp_sysfs.h
@@ -0,0 +1,37 @@
+/*
+ * Copyright (c) 2007-2011 Nicira, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#ifndef DP_SYSFS_H
+#define DP_SYSFS_H 1
+
+struct datapath;
+struct vport;
+
+/* dp_sysfs_dp.c */
+int ovs_dp_sysfs_add_dp(struct datapath *dp);
+int ovs_dp_sysfs_del_dp(struct datapath *dp);
+
+/* dp_sysfs_if.c */
+int ovs_dp_sysfs_add_if(struct vport *p);
+int ovs_dp_sysfs_del_if(struct vport *p);
+
+#ifdef CONFIG_SYSFS
+extern struct sysfs_ops ovs_brport_sysfs_ops;
+#endif
+
+#endif /* dp_sysfs.h */
diff --git a/drivers/staging/openvswitch_nv/dp_sysfs_dp.c b/drivers/staging/openvswitch_nv/dp_sysfs_dp.c
new file mode 100644
index 000000000000..3ecacd779a86
--- /dev/null
+++ b/drivers/staging/openvswitch_nv/dp_sysfs_dp.c
@@ -0,0 +1,419 @@
+/*
+ * Copyright (c) 2007-2012 Nicira, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/version.h>
+
+/*
+ *	Sysfs attributes of bridge for Open vSwitch
+ *
+ *  This has been shamelessly copied from the kernel sources.
+ */
+
+#include <linux/capability.h>
+#include <linux/device.h>
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/if_bridge.h>
+#include <linux/rtnetlink.h>
+#include <linux/version.h>
+
+#include "dp_sysfs.h"
+#include "datapath.h"
+#include "vport-internal_dev.h"
+
+#ifdef CONFIG_SYSFS
+
+/* Hack to attempt to build on more platforms. */
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,21)
+#define INTERNAL_DEVICE_ATTR CLASS_DEVICE_ATTR
+#define DEVICE_PARAMS struct class_device *d
+#define DEVICE_ARGS d
+#define DEV_ATTR(NAME) class_device_attr_##NAME
+#else
+#define INTERNAL_DEVICE_ATTR DEVICE_ATTR
+#define DEVICE_PARAMS struct device *d, struct device_attribute *attr
+#define DEVICE_ARGS d, attr
+#define DEV_ATTR(NAME) dev_attr_##NAME
+#endif
+
+static struct datapath *sysfs_get_dp(struct net_device *netdev)
+{
+	struct vport *vport = ovs_internal_dev_get_vport(netdev);
+	return vport ? vport->dp : NULL;
+}
+/*
+ * Common code for storing bridge parameters.
+ */
+static ssize_t store_bridge_parm(DEVICE_PARAMS,
+				 const char *buf, size_t len,
+				 void (*set)(struct datapath *, unsigned long))
+{
+	char *endp;
+	unsigned long val;
+	ssize_t result = len;
+
+	if (!capable(CAP_NET_ADMIN))
+		return -EPERM;
+
+	val = simple_strtoul(buf, &endp, 0);
+	if (endp == buf)
+		return -EINVAL;
+
+	/* xxx We use a default value of 0 for all fields.  If the caller is
+	 * xxx attempting to set the value to our default, just silently
+	 * xxx ignore the request.
+	 */
+	if (val != 0) {
+		struct datapath *dp;
+
+		rcu_read_lock();
+
+		dp = sysfs_get_dp(to_net_dev(d));
+		if (dp)
+			pr_warning("%s: xxx writing dp parms not supported yet!\n",
+			       ovs_dp_name(dp));
+		else
+			result = -ENODEV;
+
+		rcu_read_unlock();
+	}
+
+	return result;
+}
+
+
+static ssize_t show_forward_delay(DEVICE_PARAMS, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+
+static void set_forward_delay(struct datapath *dp, unsigned long val)
+{
+	pr_info("%s: xxx attempt to set_forward_delay()\n", ovs_dp_name(dp));
+}
+
+static ssize_t store_forward_delay(DEVICE_PARAMS,
+				   const char *buf, size_t len)
+{
+	return store_bridge_parm(DEVICE_ARGS, buf, len, set_forward_delay);
+}
+static INTERNAL_DEVICE_ATTR(forward_delay, S_IRUGO | S_IWUSR,
+		   show_forward_delay, store_forward_delay);
+
+static ssize_t show_hello_time(DEVICE_PARAMS, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+
+static void set_hello_time(struct datapath *dp, unsigned long val)
+{
+	pr_info("%s: xxx attempt to set_hello_time()\n", ovs_dp_name(dp));
+}
+
+static ssize_t store_hello_time(DEVICE_PARAMS,
+				const char *buf,
+				size_t len)
+{
+	return store_bridge_parm(DEVICE_ARGS, buf, len, set_hello_time);
+}
+static INTERNAL_DEVICE_ATTR(hello_time, S_IRUGO | S_IWUSR, show_hello_time,
+		   store_hello_time);
+
+static ssize_t show_max_age(DEVICE_PARAMS, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+
+static void set_max_age(struct datapath *dp, unsigned long val)
+{
+	pr_info("%s: xxx attempt to set_max_age()\n", ovs_dp_name(dp));
+}
+
+static ssize_t store_max_age(DEVICE_PARAMS,
+			     const char *buf, size_t len)
+{
+	return store_bridge_parm(DEVICE_ARGS, buf, len, set_max_age);
+}
+static INTERNAL_DEVICE_ATTR(max_age, S_IRUGO | S_IWUSR, show_max_age, store_max_age);
+
+static ssize_t show_ageing_time(DEVICE_PARAMS, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+
+static void set_ageing_time(struct datapath *dp, unsigned long val)
+{
+	pr_info("%s: xxx attempt to set_ageing_time()\n", ovs_dp_name(dp));
+}
+
+static ssize_t store_ageing_time(DEVICE_PARAMS,
+				 const char *buf, size_t len)
+{
+	return store_bridge_parm(DEVICE_ARGS, buf, len, set_ageing_time);
+}
+static INTERNAL_DEVICE_ATTR(ageing_time, S_IRUGO | S_IWUSR, show_ageing_time,
+		   store_ageing_time);
+
+static ssize_t show_stp_state(DEVICE_PARAMS, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+
+
+static ssize_t store_stp_state(DEVICE_PARAMS,
+			       const char *buf,
+			       size_t len)
+{
+	struct datapath *dp;
+	ssize_t result = len;
+
+	rcu_read_lock();
+
+	dp = sysfs_get_dp(to_net_dev(d));
+	if (dp)
+		pr_info("%s: xxx attempt to set_stp_state()\n", ovs_dp_name(dp));
+	else
+		result = -ENODEV;
+
+	rcu_read_unlock();
+
+	return result;
+}
+static INTERNAL_DEVICE_ATTR(stp_state, S_IRUGO | S_IWUSR, show_stp_state,
+		   store_stp_state);
+
+static ssize_t show_priority(DEVICE_PARAMS, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+
+static void set_priority(struct datapath *dp, unsigned long val)
+{
+	pr_info("%s: xxx attempt to set_priority()\n", ovs_dp_name(dp));
+}
+
+static ssize_t store_priority(DEVICE_PARAMS,
+			       const char *buf, size_t len)
+{
+	return store_bridge_parm(DEVICE_ARGS, buf, len, set_priority);
+}
+static INTERNAL_DEVICE_ATTR(priority, S_IRUGO | S_IWUSR, show_priority, store_priority);
+
+static ssize_t show_root_id(DEVICE_PARAMS, char *buf)
+{
+	return sprintf(buf, "0000.010203040506\n");
+}
+static INTERNAL_DEVICE_ATTR(root_id, S_IRUGO, show_root_id, NULL);
+
+static ssize_t show_bridge_id(DEVICE_PARAMS, char *buf)
+{
+	struct vport *vport;
+	ssize_t result;
+
+	rcu_read_lock();
+
+	vport = ovs_internal_dev_get_vport(to_net_dev(d));
+	if (vport) {
+		const unsigned char *addr;
+
+		addr = vport->ops->get_addr(vport);
+		result = sprintf(buf, "%.2x%.2x.%.2x%.2x%.2x%.2x%.2x%.2x\n",
+				 0, 0, addr[0], addr[1], addr[2], addr[3],
+				 addr[4], addr[5]);
+	} else
+		result = -ENODEV;
+
+	rcu_read_unlock();
+
+	return result;
+}
+static INTERNAL_DEVICE_ATTR(bridge_id, S_IRUGO, show_bridge_id, NULL);
+
+static ssize_t show_root_port(DEVICE_PARAMS, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+static INTERNAL_DEVICE_ATTR(root_port, S_IRUGO, show_root_port, NULL);
+
+static ssize_t show_root_path_cost(DEVICE_PARAMS, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+static INTERNAL_DEVICE_ATTR(root_path_cost, S_IRUGO, show_root_path_cost, NULL);
+
+static ssize_t show_topology_change(DEVICE_PARAMS, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+static INTERNAL_DEVICE_ATTR(topology_change, S_IRUGO, show_topology_change, NULL);
+
+static ssize_t show_topology_change_detected(DEVICE_PARAMS, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+static INTERNAL_DEVICE_ATTR(topology_change_detected, S_IRUGO,
+		   show_topology_change_detected, NULL);
+
+static ssize_t show_hello_timer(DEVICE_PARAMS, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+static INTERNAL_DEVICE_ATTR(hello_timer, S_IRUGO, show_hello_timer, NULL);
+
+static ssize_t show_tcn_timer(DEVICE_PARAMS, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+static INTERNAL_DEVICE_ATTR(tcn_timer, S_IRUGO, show_tcn_timer, NULL);
+
+static ssize_t show_topology_change_timer(DEVICE_PARAMS, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+static INTERNAL_DEVICE_ATTR(topology_change_timer, S_IRUGO, show_topology_change_timer,
+		   NULL);
+
+static ssize_t show_gc_timer(DEVICE_PARAMS, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+static INTERNAL_DEVICE_ATTR(gc_timer, S_IRUGO, show_gc_timer, NULL);
+
+static ssize_t show_group_addr(DEVICE_PARAMS, char *buf)
+{
+	return sprintf(buf, "00:01:02:03:04:05\n");
+}
+
+static ssize_t store_group_addr(DEVICE_PARAMS,
+				const char *buf, size_t len)
+{
+	struct datapath *dp;
+	ssize_t result = len;
+
+	rcu_read_lock();
+
+	dp = sysfs_get_dp(to_net_dev(d));
+	if (dp)
+		pr_info("%s: xxx attempt to store_group_addr()\n",
+		       ovs_dp_name(dp));
+	else
+		result = -ENODEV;
+
+	rcu_read_unlock();
+
+	return result;
+}
+
+static INTERNAL_DEVICE_ATTR(group_addr, S_IRUGO | S_IWUSR,
+		   show_group_addr, store_group_addr);
+
+static struct attribute *bridge_attrs[] = {
+	&DEV_ATTR(forward_delay).attr,
+	&DEV_ATTR(hello_time).attr,
+	&DEV_ATTR(max_age).attr,
+	&DEV_ATTR(ageing_time).attr,
+	&DEV_ATTR(stp_state).attr,
+	&DEV_ATTR(priority).attr,
+	&DEV_ATTR(bridge_id).attr,
+	&DEV_ATTR(root_id).attr,
+	&DEV_ATTR(root_path_cost).attr,
+	&DEV_ATTR(root_port).attr,
+	&DEV_ATTR(topology_change).attr,
+	&DEV_ATTR(topology_change_detected).attr,
+	&DEV_ATTR(hello_timer).attr,
+	&DEV_ATTR(tcn_timer).attr,
+	&DEV_ATTR(topology_change_timer).attr,
+	&DEV_ATTR(gc_timer).attr,
+	&DEV_ATTR(group_addr).attr,
+	NULL
+};
+
+static struct attribute_group bridge_group = {
+	.name = SYSFS_BRIDGE_ATTR, /* "bridge" */
+	.attrs = bridge_attrs,
+};
+
+/*
+ * Add entries in sysfs onto the existing network class device
+ * for the bridge.
+ *   Adds a attribute group "bridge" containing tuning parameters.
+ *   Sub directory to hold links to interfaces.
+ *
+ * Note: the ifobj exists only to be a subdirectory
+ *   to hold links.  The ifobj exists in the same data structure
+ *   as its parent the bridge so reference counting works.
+ */
+int ovs_dp_sysfs_add_dp(struct datapath *dp)
+{
+	struct vport *vport = ovs_vport_rtnl(dp, OVSP_LOCAL);
+	struct kobject *kobj = vport->ops->get_kobj(vport);
+	int err;
+
+#ifdef CONFIG_NET_NS
+	/* Due to bug in 2.6.32 kernel, sysfs_create_group() could panic
+	 * in other namespace than init_net. Following check is to avoid it. */
+	if (!kobj->sd)
+		return -ENOENT;
+#endif
+	/* Create /sys/class/net/<devname>/bridge directory. */
+	err = sysfs_create_group(kobj, &bridge_group);
+	if (err) {
+		pr_info("%s: can't create group %s/%s\n",
+			__func__, ovs_dp_name(dp), bridge_group.name);
+		goto out1;
+	}
+
+	/* Create /sys/class/net/<devname>/brif directory. */
+	err = kobject_add(&dp->ifobj, kobj, SYSFS_BRIDGE_PORT_SUBDIR);
+	if (err) {
+		pr_info("%s: can't add kobject (directory) %s/%s\n",
+			__func__, ovs_dp_name(dp), kobject_name(&dp->ifobj));
+		goto out2;
+	}
+	kobject_uevent(&dp->ifobj, KOBJ_ADD);
+	return 0;
+
+ out2:
+	sysfs_remove_group(kobj, &bridge_group);
+ out1:
+	return err;
+}
+
+int ovs_dp_sysfs_del_dp(struct datapath *dp)
+{
+	struct vport *vport = ovs_vport_rtnl(dp, OVSP_LOCAL);
+	struct kobject *kobj = vport->ops->get_kobj(vport);
+
+#ifdef CONFIG_NET_NS
+	if (!kobj->sd)
+		return 0;
+#endif
+
+	kobject_del(&dp->ifobj);
+	sysfs_remove_group(kobj, &bridge_group);
+
+	return 0;
+}
+#else /* !CONFIG_SYSFS */
+int ovs_dp_sysfs_add_dp(struct datapath *dp) { return 0; }
+int ovs_dp_sysfs_del_dp(struct datapath *dp) { return 0; }
+int dp_sysfs_add_if(struct vport *p) { return 0; }
+int dp_sysfs_del_if(struct vport *p) { return 0; }
+#endif /* !CONFIG_SYSFS */
diff --git a/drivers/staging/openvswitch_nv/dp_sysfs_if.c b/drivers/staging/openvswitch_nv/dp_sysfs_if.c
new file mode 100644
index 000000000000..219a26068390
--- /dev/null
+++ b/drivers/staging/openvswitch_nv/dp_sysfs_if.c
@@ -0,0 +1,275 @@
+/*
+ * Copyright (c) 2007-2012 Nicira, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/capability.h>
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/if_bridge.h>
+#include <linux/rtnetlink.h>
+
+#include "datapath.h"
+#include "dp_sysfs.h"
+#include "vport.h"
+
+#ifdef CONFIG_SYSFS
+
+struct brport_attribute {
+	struct attribute	attr;
+	ssize_t (*show)(struct vport *, char *);
+	ssize_t (*store)(struct vport *, unsigned long);
+};
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,36)
+#define BRPORT_ATTR(_name, _mode, _show, _store)		\
+struct brport_attribute brport_attr_##_name = {		        \
+	.attr = {.name = __stringify(_name),			\
+		 .mode = _mode },				\
+	.show	= _show,					\
+	.store	= _store,					\
+};
+#else
+#define BRPORT_ATTR(_name, _mode, _show, _store)		\
+struct brport_attribute brport_attr_##_name = {			\
+	.attr = {.name = __stringify(_name),			\
+		 .mode = _mode,					\
+		 .owner = THIS_MODULE, },			\
+	.show	= _show,					\
+	.store	= _store,					\
+};
+#endif
+
+static ssize_t show_path_cost(struct vport *p, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+static ssize_t store_path_cost(struct vport *p, unsigned long v)
+{
+	return 0;
+}
+static BRPORT_ATTR(path_cost, S_IRUGO | S_IWUSR,
+		   show_path_cost, store_path_cost);
+
+static ssize_t show_priority(struct vport *p, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+static ssize_t store_priority(struct vport *p, unsigned long v)
+{
+	return 0;
+}
+static BRPORT_ATTR(priority, S_IRUGO | S_IWUSR,
+			 show_priority, store_priority);
+
+static ssize_t show_designated_root(struct vport *p, char *buf)
+{
+	return sprintf(buf, "0000.010203040506\n");
+}
+static BRPORT_ATTR(designated_root, S_IRUGO, show_designated_root, NULL);
+
+static ssize_t show_designated_bridge(struct vport *p, char *buf)
+{
+	return sprintf(buf, "0000.060504030201\n");
+}
+static BRPORT_ATTR(designated_bridge, S_IRUGO, show_designated_bridge, NULL);
+
+static ssize_t show_designated_port(struct vport *p, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+static BRPORT_ATTR(designated_port, S_IRUGO, show_designated_port, NULL);
+
+static ssize_t show_designated_cost(struct vport *p, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+static BRPORT_ATTR(designated_cost, S_IRUGO, show_designated_cost, NULL);
+
+static ssize_t show_port_id(struct vport *p, char *buf)
+{
+	return sprintf(buf, "0x%x\n", 0);
+}
+static BRPORT_ATTR(port_id, S_IRUGO, show_port_id, NULL);
+
+static ssize_t show_port_no(struct vport *p, char *buf)
+{
+	return sprintf(buf, "0x%x\n", p->port_no);
+}
+
+static BRPORT_ATTR(port_no, S_IRUGO, show_port_no, NULL);
+
+static ssize_t show_change_ack(struct vport *p, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+static BRPORT_ATTR(change_ack, S_IRUGO, show_change_ack, NULL);
+
+static ssize_t show_config_pending(struct vport *p, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+static BRPORT_ATTR(config_pending, S_IRUGO, show_config_pending, NULL);
+
+static ssize_t show_port_state(struct vport *p, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+static BRPORT_ATTR(state, S_IRUGO, show_port_state, NULL);
+
+static ssize_t show_message_age_timer(struct vport *p, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+static BRPORT_ATTR(message_age_timer, S_IRUGO, show_message_age_timer, NULL);
+
+static ssize_t show_forward_delay_timer(struct vport *p, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+static BRPORT_ATTR(forward_delay_timer, S_IRUGO, show_forward_delay_timer, NULL);
+
+static ssize_t show_hold_timer(struct vport *p, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+static BRPORT_ATTR(hold_timer, S_IRUGO, show_hold_timer, NULL);
+
+static struct brport_attribute *brport_attrs[] = {
+	&brport_attr_path_cost,
+	&brport_attr_priority,
+	&brport_attr_port_id,
+	&brport_attr_port_no,
+	&brport_attr_designated_root,
+	&brport_attr_designated_bridge,
+	&brport_attr_designated_port,
+	&brport_attr_designated_cost,
+	&brport_attr_state,
+	&brport_attr_change_ack,
+	&brport_attr_config_pending,
+	&brport_attr_message_age_timer,
+	&brport_attr_forward_delay_timer,
+	&brport_attr_hold_timer,
+	NULL
+};
+
+#define to_vport_attr(_at) container_of(_at, struct brport_attribute, attr)
+#define to_vport(obj)	container_of(obj, struct vport, kobj)
+
+static ssize_t brport_show(struct kobject *kobj,
+			   struct attribute *attr, char *buf)
+{
+	struct brport_attribute *brport_attr = to_vport_attr(attr);
+	struct vport *p = to_vport(kobj);
+
+	return brport_attr->show(p, buf);
+}
+
+static ssize_t brport_store(struct kobject *kobj,
+			    struct attribute *attr,
+			    const char *buf, size_t count)
+{
+	struct vport *p = to_vport(kobj);
+	ssize_t ret = -EINVAL;
+
+	if (!capable(CAP_NET_ADMIN))
+		return -EPERM;
+
+	pr_warning("%s: xxx writing port parms not supported yet!\n",
+		   ovs_dp_name(p->dp));
+
+	return ret;
+}
+
+struct sysfs_ops ovs_brport_sysfs_ops = {
+	.show = brport_show,
+	.store = brport_store,
+};
+
+/*
+ * Add sysfs entries to ethernet device added to a bridge.
+ * Creates a brport subdirectory with bridge attributes.
+ * Puts symlink in bridge's brport subdirectory
+ */
+int ovs_dp_sysfs_add_if(struct vport *p)
+{
+	struct datapath *dp = p->dp;
+	struct vport *local_port = ovs_vport_rtnl(dp, OVSP_LOCAL);
+	struct brport_attribute **a;
+	int err;
+
+	/* Create /sys/class/net/<devname>/brport directory. */
+	if (!p->ops->get_kobj)
+		return -ENOENT;
+
+#ifdef CONFIG_NET_NS
+	/* Due to bug in 2.6.32 kernel, sysfs_create_group() could panic
+	 * in other namespace than init_net. Following check is to avoid it. */
+
+	if (!p->kobj.sd)
+		return -ENOENT;
+#endif
+
+	err = kobject_add(&p->kobj, p->ops->get_kobj(p),
+			  SYSFS_BRIDGE_PORT_ATTR);
+	if (err)
+		goto err;
+
+	/* Create symlink from /sys/class/net/<devname>/brport/bridge to
+	 * /sys/class/net/<bridgename>. */
+	err = sysfs_create_link(&p->kobj, local_port->ops->get_kobj(local_port),
+		SYSFS_BRIDGE_PORT_LINK); /* "bridge" */
+	if (err)
+		goto err_del;
+
+	/* Populate /sys/class/net/<devname>/brport directory with files. */
+	for (a = brport_attrs; *a; ++a) {
+		err = sysfs_create_file(&p->kobj, &((*a)->attr));
+		if (err)
+			goto err_del;
+	}
+
+	/* Create symlink from /sys/class/net/<bridgename>/brif/<devname> to
+	 * /sys/class/net/<devname>/brport.  */
+	err = sysfs_create_link(&dp->ifobj, &p->kobj, p->ops->get_name(p));
+	if (err)
+		goto err_del;
+	strcpy(p->linkname, p->ops->get_name(p));
+
+	kobject_uevent(&p->kobj, KOBJ_ADD);
+
+	return 0;
+
+err_del:
+	kobject_del(&p->kobj);
+err:
+	p->linkname[0] = 0;
+	return err;
+}
+
+int ovs_dp_sysfs_del_if(struct vport *p)
+{
+	if (p->linkname[0]) {
+		sysfs_remove_link(&p->dp->ifobj, p->linkname);
+		kobject_uevent(&p->kobj, KOBJ_REMOVE);
+		kobject_del(&p->kobj);
+		p->linkname[0] = '\0';
+	}
+	return 0;
+}
+#endif /* CONFIG_SYSFS */
diff --git a/drivers/staging/openvswitch_nv/flow.c b/drivers/staging/openvswitch_nv/flow.c
new file mode 100644
index 000000000000..df98fab92d78
--- /dev/null
+++ b/drivers/staging/openvswitch_nv/flow.c
@@ -0,0 +1,1568 @@
+/*
+ * Copyright (c) 2007-2011 Nicira, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#include "flow.h"
+#include "datapath.h"
+#include <linux/uaccess.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/if_ether.h>
+#include <linux/if_vlan.h>
+#include <net/llc_pdu.h>
+#include <linux/kernel.h>
+#include <linux/jhash.h>
+#include <linux/jiffies.h>
+#include <linux/llc.h>
+#include <linux/module.h>
+#include <linux/in.h>
+#include <linux/rcupdate.h>
+#include <linux/if_arp.h>
+#include <linux/ip.h>
+#include <linux/ipv6.h>
+#include <linux/tcp.h>
+#include <linux/udp.h>
+#include <linux/icmp.h>
+#include <linux/icmpv6.h>
+#include <linux/rculist.h>
+#include <linux/openvswitch-nv.h>
+#include <net/ip.h>
+#include <net/ipv6.h>
+#include <net/ndisc.h>
+#include <net/netlink.h>
+
+#include "vlan.h"
+
+static struct kmem_cache *flow_cache;
+
+static int check_header(struct sk_buff *skb, int len)
+{
+	if (unlikely(skb->len < len))
+		return -EINVAL;
+	if (unlikely(!pskb_may_pull(skb, len)))
+		return -ENOMEM;
+	return 0;
+}
+
+static bool arphdr_ok(struct sk_buff *skb)
+{
+	return pskb_may_pull(skb, skb_network_offset(skb) +
+				  sizeof(struct arp_eth_header));
+}
+
+static int check_iphdr(struct sk_buff *skb)
+{
+	unsigned int nh_ofs = skb_network_offset(skb);
+	unsigned int ip_len;
+	int err;
+
+	err = check_header(skb, nh_ofs + sizeof(struct iphdr));
+	if (unlikely(err))
+		return err;
+
+	ip_len = ip_hdrlen(skb);
+	if (unlikely(ip_len < sizeof(struct iphdr) ||
+		     skb->len < nh_ofs + ip_len))
+		return -EINVAL;
+
+	skb_set_transport_header(skb, nh_ofs + ip_len);
+	return 0;
+}
+
+static bool tcphdr_ok(struct sk_buff *skb)
+{
+	int th_ofs = skb_transport_offset(skb);
+	int tcp_len;
+
+	if (unlikely(!pskb_may_pull(skb, th_ofs + sizeof(struct tcphdr))))
+		return false;
+
+	tcp_len = tcp_hdrlen(skb);
+	if (unlikely(tcp_len < sizeof(struct tcphdr) ||
+		     skb->len < th_ofs + tcp_len))
+		return false;
+
+	return true;
+}
+
+static bool udphdr_ok(struct sk_buff *skb)
+{
+	return pskb_may_pull(skb, skb_transport_offset(skb) +
+				  sizeof(struct udphdr));
+}
+
+static bool icmphdr_ok(struct sk_buff *skb)
+{
+	return pskb_may_pull(skb, skb_transport_offset(skb) +
+				  sizeof(struct icmphdr));
+}
+
+u64 ovs_flow_used_time(unsigned long flow_jiffies)
+{
+	struct timespec cur_ts;
+	u64 cur_ms, idle_ms;
+
+	ktime_get_ts(&cur_ts);
+	idle_ms = jiffies_to_msecs(jiffies - flow_jiffies);
+	cur_ms = (u64)cur_ts.tv_sec * MSEC_PER_SEC +
+		 cur_ts.tv_nsec / NSEC_PER_MSEC;
+
+	return cur_ms - idle_ms;
+}
+
+#define SW_FLOW_KEY_OFFSET(field)		\
+	(offsetof(struct sw_flow_key, field) +	\
+	 FIELD_SIZEOF(struct sw_flow_key, field))
+
+static int parse_ipv6hdr(struct sk_buff *skb, struct sw_flow_key *key,
+			 int *key_lenp)
+{
+	unsigned int nh_ofs = skb_network_offset(skb);
+	unsigned int nh_len;
+	int payload_ofs;
+	struct ipv6hdr *nh;
+	uint8_t nexthdr;
+	__be16 frag_off;
+	int err;
+
+	*key_lenp = SW_FLOW_KEY_OFFSET(ipv6.label);
+
+	err = check_header(skb, nh_ofs + sizeof(*nh));
+	if (unlikely(err))
+		return err;
+
+	nh = ipv6_hdr(skb);
+	nexthdr = nh->nexthdr;
+	payload_ofs = (u8 *)(nh + 1) - skb->data;
+
+	key->ip.proto = NEXTHDR_NONE;
+	key->ip.tos = ipv6_get_dsfield(nh);
+	key->ip.ttl = nh->hop_limit;
+	key->ipv6.label = *(__be32 *)nh & htonl(IPV6_FLOWINFO_FLOWLABEL);
+	key->ipv6.addr.src = nh->saddr;
+	key->ipv6.addr.dst = nh->daddr;
+
+	payload_ofs = ipv6_skip_exthdr(skb, payload_ofs, &nexthdr, &frag_off);
+	if (unlikely(payload_ofs < 0))
+		return -EINVAL;
+
+	if (frag_off) {
+		if (frag_off & htons(~0x7))
+			key->ip.frag = OVS_FRAG_TYPE_LATER;
+		else
+			key->ip.frag = OVS_FRAG_TYPE_FIRST;
+	}
+
+	nh_len = payload_ofs - nh_ofs;
+	skb_set_transport_header(skb, nh_ofs + nh_len);
+	key->ip.proto = nexthdr;
+	return nh_len;
+}
+
+static bool icmp6hdr_ok(struct sk_buff *skb)
+{
+	return pskb_may_pull(skb, skb_transport_offset(skb) +
+				  sizeof(struct icmp6hdr));
+}
+
+#define TCP_FLAGS_OFFSET 13
+#define TCP_FLAG_MASK 0x3f
+
+void ovs_flow_used(struct sw_flow *flow, struct sk_buff *skb)
+{
+	u8 tcp_flags = 0;
+
+	if ((flow->key.eth.type == htons(ETH_P_IP) ||
+	     flow->key.eth.type == htons(ETH_P_IPV6)) &&
+	    flow->key.ip.proto == IPPROTO_TCP &&
+	    likely(skb->len >= skb_transport_offset(skb) + sizeof(struct tcphdr))) {
+		u8 *tcp = (u8 *)tcp_hdr(skb);
+		tcp_flags = *(tcp + TCP_FLAGS_OFFSET) & TCP_FLAG_MASK;
+	}
+
+	spin_lock(&flow->lock);
+	flow->used = jiffies;
+	flow->packet_count++;
+	flow->byte_count += skb->len;
+	flow->tcp_flags |= tcp_flags;
+	spin_unlock(&flow->lock);
+}
+
+struct sw_flow_actions *ovs_flow_actions_alloc(int size)
+{
+	struct sw_flow_actions *sfa;
+
+	if (size > MAX_ACTIONS_BUFSIZE)
+		return ERR_PTR(-EINVAL);
+
+	sfa = kmalloc(sizeof(*sfa) + size, GFP_KERNEL);
+	if (!sfa)
+		return ERR_PTR(-ENOMEM);
+
+	sfa->actions_len = 0;
+	return sfa;
+}
+
+struct sw_flow *ovs_flow_alloc(void)
+{
+	struct sw_flow *flow;
+
+	flow = kmem_cache_alloc(flow_cache, GFP_KERNEL);
+	if (!flow)
+		return ERR_PTR(-ENOMEM);
+
+	spin_lock_init(&flow->lock);
+	flow->sf_acts = NULL;
+
+	return flow;
+}
+
+static struct hlist_head *find_bucket(struct flow_table *table, u32 hash)
+{
+	hash = jhash_1word(hash, table->hash_seed);
+	return flex_array_get(table->buckets,
+				(hash & (table->n_buckets - 1)));
+}
+
+static struct flex_array *alloc_buckets(unsigned int n_buckets)
+{
+	struct flex_array *buckets;
+	int i, err;
+
+	buckets = flex_array_alloc(sizeof(struct hlist_head *),
+				   n_buckets, GFP_KERNEL);
+	if (!buckets)
+		return NULL;
+
+	err = flex_array_prealloc(buckets, 0, n_buckets, GFP_KERNEL);
+	if (err) {
+		flex_array_free(buckets);
+		return NULL;
+	}
+
+	for (i = 0; i < n_buckets; i++)
+		INIT_HLIST_HEAD((struct hlist_head *)
+					flex_array_get(buckets, i));
+
+	return buckets;
+}
+
+static void free_buckets(struct flex_array *buckets)
+{
+	flex_array_free(buckets);
+}
+
+struct flow_table *ovs_flow_tbl_alloc(int new_size)
+{
+	struct flow_table *table = kmalloc(sizeof(*table), GFP_KERNEL);
+
+	if (!table)
+		return NULL;
+
+	table->buckets = alloc_buckets(new_size);
+
+	if (!table->buckets) {
+		kfree(table);
+		return NULL;
+	}
+	table->n_buckets = new_size;
+	table->count = 0;
+	table->node_ver = 0;
+	table->keep_flows = false;
+	get_random_bytes(&table->hash_seed, sizeof(u32));
+
+	return table;
+}
+
+void ovs_flow_tbl_destroy(struct flow_table *table)
+{
+	int i;
+
+	if (!table)
+		return;
+
+	if (table->keep_flows)
+		goto skip_flows;
+
+	for (i = 0; i < table->n_buckets; i++) {
+		struct sw_flow *flow;
+		struct hlist_head *head = flex_array_get(table->buckets, i);
+		struct hlist_node *node, *n;
+		int ver = table->node_ver;
+
+		hlist_for_each_entry_safe(flow, node, n, head, hash_node[ver]) {
+			hlist_del_rcu(&flow->hash_node[ver]);
+			ovs_flow_free(flow);
+		}
+	}
+
+skip_flows:
+	free_buckets(table->buckets);
+	kfree(table);
+}
+
+static void flow_tbl_destroy_rcu_cb(struct rcu_head *rcu)
+{
+	struct flow_table *table = container_of(rcu, struct flow_table, rcu);
+
+	ovs_flow_tbl_destroy(table);
+}
+
+void ovs_flow_tbl_deferred_destroy(struct flow_table *table)
+{
+	if (!table)
+		return;
+
+	call_rcu(&table->rcu, flow_tbl_destroy_rcu_cb);
+}
+
+struct sw_flow *ovs_flow_tbl_next(struct flow_table *table, u32 *bucket, u32 *last)
+{
+	struct sw_flow *flow;
+	struct hlist_head *head;
+	struct hlist_node *n;
+	int ver;
+	int i;
+
+	ver = table->node_ver;
+	while (*bucket < table->n_buckets) {
+		i = 0;
+		head = flex_array_get(table->buckets, *bucket);
+		hlist_for_each_entry_rcu(flow, n, head, hash_node[ver]) {
+			if (i < *last) {
+				i++;
+				continue;
+			}
+			*last = i + 1;
+			return flow;
+		}
+		(*bucket)++;
+		*last = 0;
+	}
+
+	return NULL;
+}
+
+static void __flow_tbl_insert(struct flow_table *table, struct sw_flow *flow)
+{
+	struct hlist_head *head;
+	head = find_bucket(table, flow->hash);
+	hlist_add_head_rcu(&flow->hash_node[table->node_ver], head);
+	table->count++;
+}
+
+static void flow_table_copy_flows(struct flow_table *old, struct flow_table *new)
+{
+	int old_ver;
+	int i;
+
+	old_ver = old->node_ver;
+	new->node_ver = !old_ver;
+
+	/* Insert in new table. */
+	for (i = 0; i < old->n_buckets; i++) {
+		struct sw_flow *flow;
+		struct hlist_head *head;
+		struct hlist_node *n;
+
+		head = flex_array_get(old->buckets, i);
+
+		hlist_for_each_entry(flow, n, head, hash_node[old_ver])
+			__flow_tbl_insert(new, flow);
+	}
+	old->keep_flows = true;
+}
+
+static struct flow_table *__flow_tbl_rehash(struct flow_table *table, int n_buckets)
+{
+	struct flow_table *new_table;
+
+	new_table = ovs_flow_tbl_alloc(n_buckets);
+	if (!new_table)
+		return ERR_PTR(-ENOMEM);
+
+	flow_table_copy_flows(table, new_table);
+
+	return new_table;
+}
+
+struct flow_table *ovs_flow_tbl_rehash(struct flow_table *table)
+{
+	return __flow_tbl_rehash(table, table->n_buckets);
+}
+
+struct flow_table *ovs_flow_tbl_expand(struct flow_table *table)
+{
+	return __flow_tbl_rehash(table, table->n_buckets * 2);
+}
+
+void ovs_flow_free(struct sw_flow *flow)
+{
+	if (unlikely(!flow))
+		return;
+
+	kfree((struct sf_flow_acts __force *)flow->sf_acts);
+	kmem_cache_free(flow_cache, flow);
+}
+
+/* RCU callback used by ovs_flow_deferred_free. */
+static void rcu_free_flow_callback(struct rcu_head *rcu)
+{
+	struct sw_flow *flow = container_of(rcu, struct sw_flow, rcu);
+
+	ovs_flow_free(flow);
+}
+
+/* Schedules 'flow' to be freed after the next RCU grace period.
+ * The caller must hold rcu_read_lock for this to be sensible. */
+void ovs_flow_deferred_free(struct sw_flow *flow)
+{
+	call_rcu(&flow->rcu, rcu_free_flow_callback);
+}
+
+/* RCU callback used by ovs_flow_deferred_free_acts. */
+static void rcu_free_acts_callback(struct rcu_head *rcu)
+{
+	struct sw_flow_actions *sf_acts = container_of(rcu,
+			struct sw_flow_actions, rcu);
+	kfree(sf_acts);
+}
+
+/* Schedules 'sf_acts' to be freed after the next RCU grace period.
+ * The caller must hold rcu_read_lock for this to be sensible. */
+void ovs_flow_deferred_free_acts(struct sw_flow_actions *sf_acts)
+{
+	call_rcu(&sf_acts->rcu, rcu_free_acts_callback);
+}
+
+static int parse_vlan(struct sk_buff *skb, struct sw_flow_key *key)
+{
+	struct qtag_prefix {
+		__be16 eth_type; /* ETH_P_8021Q */
+		__be16 tci;
+	};
+	struct qtag_prefix *qp;
+
+	if (unlikely(skb->len < sizeof(struct qtag_prefix) + sizeof(__be16)))
+		return 0;
+
+	if (unlikely(!pskb_may_pull(skb, sizeof(struct qtag_prefix) +
+					 sizeof(__be16))))
+		return -ENOMEM;
+
+	qp = (struct qtag_prefix *) skb->data;
+	key->eth.tci = qp->tci | htons(VLAN_TAG_PRESENT);
+	__skb_pull(skb, sizeof(struct qtag_prefix));
+
+	return 0;
+}
+
+static __be16 parse_ethertype(struct sk_buff *skb)
+{
+	struct llc_snap_hdr {
+		u8  dsap;  /* Always 0xAA */
+		u8  ssap;  /* Always 0xAA */
+		u8  ctrl;
+		u8  oui[3];
+		__be16 ethertype;
+	};
+	struct llc_snap_hdr *llc;
+	__be16 proto;
+
+	proto = *(__be16 *) skb->data;
+	__skb_pull(skb, sizeof(__be16));
+
+	if (ntohs(proto) >= 1536)
+		return proto;
+
+	if (skb->len < sizeof(struct llc_snap_hdr))
+		return htons(ETH_P_802_2);
+
+	if (unlikely(!pskb_may_pull(skb, sizeof(struct llc_snap_hdr))))
+		return htons(0);
+
+	llc = (struct llc_snap_hdr *) skb->data;
+	if (llc->dsap != LLC_SAP_SNAP ||
+	    llc->ssap != LLC_SAP_SNAP ||
+	    (llc->oui[0] | llc->oui[1] | llc->oui[2]) != 0)
+		return htons(ETH_P_802_2);
+
+	__skb_pull(skb, sizeof(struct llc_snap_hdr));
+	return llc->ethertype;
+}
+
+static int parse_icmpv6(struct sk_buff *skb, struct sw_flow_key *key,
+			int *key_lenp, int nh_len)
+{
+	struct icmp6hdr *icmp = icmp6_hdr(skb);
+	int error = 0;
+	int key_len;
+
+	/* The ICMPv6 type and code fields use the 16-bit transport port
+	 * fields, so we need to store them in 16-bit network byte order.
+	 */
+	key->ipv6.tp.src = htons(icmp->icmp6_type);
+	key->ipv6.tp.dst = htons(icmp->icmp6_code);
+	key_len = SW_FLOW_KEY_OFFSET(ipv6.tp);
+
+	if (icmp->icmp6_code == 0 &&
+	    (icmp->icmp6_type == NDISC_NEIGHBOUR_SOLICITATION ||
+	     icmp->icmp6_type == NDISC_NEIGHBOUR_ADVERTISEMENT)) {
+		int icmp_len = skb->len - skb_transport_offset(skb);
+		struct nd_msg *nd;
+		int offset;
+
+		key_len = SW_FLOW_KEY_OFFSET(ipv6.nd);
+
+		/* In order to process neighbor discovery options, we need the
+		 * entire packet.
+		 */
+		if (unlikely(icmp_len < sizeof(*nd)))
+			goto out;
+		if (unlikely(skb_linearize(skb))) {
+			error = -ENOMEM;
+			goto out;
+		}
+
+		nd = (struct nd_msg *)skb_transport_header(skb);
+		key->ipv6.nd.target = nd->target;
+		key_len = SW_FLOW_KEY_OFFSET(ipv6.nd);
+
+		icmp_len -= sizeof(*nd);
+		offset = 0;
+		while (icmp_len >= 8) {
+			struct nd_opt_hdr *nd_opt =
+				 (struct nd_opt_hdr *)(nd->opt + offset);
+			int opt_len = nd_opt->nd_opt_len * 8;
+
+			if (unlikely(!opt_len || opt_len > icmp_len))
+				goto invalid;
+
+			/* Store the link layer address if the appropriate
+			 * option is provided.  It is considered an error if
+			 * the same link layer option is specified twice.
+			 */
+			if (nd_opt->nd_opt_type == ND_OPT_SOURCE_LL_ADDR
+			    && opt_len == 8) {
+				if (unlikely(!is_zero_ether_addr(key->ipv6.nd.sll)))
+					goto invalid;
+				memcpy(key->ipv6.nd.sll,
+				    &nd->opt[offset+sizeof(*nd_opt)], ETH_ALEN);
+			} else if (nd_opt->nd_opt_type == ND_OPT_TARGET_LL_ADDR
+				   && opt_len == 8) {
+				if (unlikely(!is_zero_ether_addr(key->ipv6.nd.tll)))
+					goto invalid;
+				memcpy(key->ipv6.nd.tll,
+				    &nd->opt[offset+sizeof(*nd_opt)], ETH_ALEN);
+			}
+
+			icmp_len -= opt_len;
+			offset += opt_len;
+		}
+	}
+
+	goto out;
+
+invalid:
+	memset(&key->ipv6.nd.target, 0, sizeof(key->ipv6.nd.target));
+	memset(key->ipv6.nd.sll, 0, sizeof(key->ipv6.nd.sll));
+	memset(key->ipv6.nd.tll, 0, sizeof(key->ipv6.nd.tll));
+
+out:
+	*key_lenp = key_len;
+	return error;
+}
+
+/**
+ * ovs_flow_extract - extracts a flow key from an Ethernet frame.
+ * @skb: sk_buff that contains the frame, with skb->data pointing to the
+ * Ethernet header
+ * @in_port: port number on which @skb was received.
+ * @key: output flow key
+ * @key_lenp: length of output flow key
+ *
+ * The caller must ensure that skb->len >= ETH_HLEN.
+ *
+ * Returns 0 if successful, otherwise a negative errno value.
+ *
+ * Initializes @skb header pointers as follows:
+ *
+ *    - skb->mac_header: the Ethernet header.
+ *
+ *    - skb->network_header: just past the Ethernet header, or just past the
+ *      VLAN header, to the first byte of the Ethernet payload.
+ *
+ *    - skb->transport_header: If key->dl_type is ETH_P_IP or ETH_P_IPV6
+ *      on output, then just past the IP header, if one is present and
+ *      of a correct length, otherwise the same as skb->network_header.
+ *      For other key->dl_type values it is left untouched.
+ */
+int ovs_flow_extract(struct sk_buff *skb, u16 in_port, struct sw_flow_key *key,
+		 int *key_lenp)
+{
+	int error = 0;
+	int key_len = SW_FLOW_KEY_OFFSET(eth);
+	struct ethhdr *eth;
+
+	memset(key, 0, sizeof(*key));
+
+	key->phy.priority = skb->priority;
+	if (OVS_CB(skb)->tun_key)
+		memcpy(&key->phy.tun.tun_key, OVS_CB(skb)->tun_key, sizeof(key->phy.tun.tun_key));
+	key->phy.in_port = in_port;
+	key->phy.skb_mark = skb->mark;
+
+	skb_reset_mac_header(skb);
+
+	/* Link layer.  We are guaranteed to have at least the 14 byte Ethernet
+	 * header in the linear data area.
+	 */
+	eth = eth_hdr(skb);
+	memcpy(key->eth.src, eth->h_source, ETH_ALEN);
+	memcpy(key->eth.dst, eth->h_dest, ETH_ALEN);
+
+	__skb_pull(skb, 2 * ETH_ALEN);
+
+	if (vlan_tx_tag_present(skb))
+		key->eth.tci = htons(vlan_get_tci(skb));
+	else if (eth->h_proto == htons(ETH_P_8021Q))
+		if (unlikely(parse_vlan(skb, key)))
+			return -ENOMEM;
+
+	key->eth.type = parse_ethertype(skb);
+	if (unlikely(key->eth.type == htons(0)))
+		return -ENOMEM;
+
+	skb_reset_network_header(skb);
+	__skb_push(skb, skb->data - skb_mac_header(skb));
+
+	/* Network layer. */
+	if (key->eth.type == htons(ETH_P_IP)) {
+		struct iphdr *nh;
+		__be16 offset;
+
+		key_len = SW_FLOW_KEY_OFFSET(ipv4.addr);
+
+		error = check_iphdr(skb);
+		if (unlikely(error)) {
+			if (error == -EINVAL) {
+				skb->transport_header = skb->network_header;
+				error = 0;
+			}
+			goto out;
+		}
+
+		nh = ip_hdr(skb);
+		key->ipv4.addr.src = nh->saddr;
+		key->ipv4.addr.dst = nh->daddr;
+
+		key->ip.proto = nh->protocol;
+		key->ip.tos = nh->tos;
+		key->ip.ttl = nh->ttl;
+
+		offset = nh->frag_off & htons(IP_OFFSET);
+		if (offset) {
+			key->ip.frag = OVS_FRAG_TYPE_LATER;
+			goto out;
+		}
+		if (nh->frag_off & htons(IP_MF) ||
+			 skb_shinfo(skb)->gso_type & SKB_GSO_UDP)
+			key->ip.frag = OVS_FRAG_TYPE_FIRST;
+
+		/* Transport layer. */
+		if (key->ip.proto == IPPROTO_TCP) {
+			key_len = SW_FLOW_KEY_OFFSET(ipv4.tp);
+			if (tcphdr_ok(skb)) {
+				struct tcphdr *tcp = tcp_hdr(skb);
+				key->ipv4.tp.src = tcp->source;
+				key->ipv4.tp.dst = tcp->dest;
+			}
+		} else if (key->ip.proto == IPPROTO_UDP) {
+			key_len = SW_FLOW_KEY_OFFSET(ipv4.tp);
+			if (udphdr_ok(skb)) {
+				struct udphdr *udp = udp_hdr(skb);
+				key->ipv4.tp.src = udp->source;
+				key->ipv4.tp.dst = udp->dest;
+			}
+		} else if (key->ip.proto == IPPROTO_ICMP) {
+			key_len = SW_FLOW_KEY_OFFSET(ipv4.tp);
+			if (icmphdr_ok(skb)) {
+				struct icmphdr *icmp = icmp_hdr(skb);
+				/* The ICMP type and code fields use the 16-bit
+				 * transport port fields, so we need to store
+				 * them in 16-bit network byte order. */
+				key->ipv4.tp.src = htons(icmp->type);
+				key->ipv4.tp.dst = htons(icmp->code);
+			}
+		}
+
+	} else if ((key->eth.type == htons(ETH_P_ARP) ||
+		   key->eth.type == htons(ETH_P_RARP)) && arphdr_ok(skb)) {
+		struct arp_eth_header *arp;
+
+		arp = (struct arp_eth_header *)skb_network_header(skb);
+
+		if (arp->ar_hrd == htons(ARPHRD_ETHER)
+				&& arp->ar_pro == htons(ETH_P_IP)
+				&& arp->ar_hln == ETH_ALEN
+				&& arp->ar_pln == 4) {
+
+			/* We only match on the lower 8 bits of the opcode. */
+			if (ntohs(arp->ar_op) <= 0xff)
+				key->ip.proto = ntohs(arp->ar_op);
+			memcpy(&key->ipv4.addr.src, arp->ar_sip, sizeof(key->ipv4.addr.src));
+			memcpy(&key->ipv4.addr.dst, arp->ar_tip, sizeof(key->ipv4.addr.dst));
+			memcpy(key->ipv4.arp.sha, arp->ar_sha, ETH_ALEN);
+			memcpy(key->ipv4.arp.tha, arp->ar_tha, ETH_ALEN);
+			key_len = SW_FLOW_KEY_OFFSET(ipv4.arp);
+		}
+	} else if (key->eth.type == htons(ETH_P_IPV6)) {
+		int nh_len;             /* IPv6 Header + Extensions */
+
+		nh_len = parse_ipv6hdr(skb, key, &key_len);
+		if (unlikely(nh_len < 0)) {
+			if (nh_len == -EINVAL)
+				skb->transport_header = skb->network_header;
+			else
+				error = nh_len;
+			goto out;
+		}
+
+		if (key->ip.frag == OVS_FRAG_TYPE_LATER)
+			goto out;
+		if (skb_shinfo(skb)->gso_type & SKB_GSO_UDP)
+			key->ip.frag = OVS_FRAG_TYPE_FIRST;
+
+		/* Transport layer. */
+		if (key->ip.proto == NEXTHDR_TCP) {
+			key_len = SW_FLOW_KEY_OFFSET(ipv6.tp);
+			if (tcphdr_ok(skb)) {
+				struct tcphdr *tcp = tcp_hdr(skb);
+				key->ipv6.tp.src = tcp->source;
+				key->ipv6.tp.dst = tcp->dest;
+			}
+		} else if (key->ip.proto == NEXTHDR_UDP) {
+			key_len = SW_FLOW_KEY_OFFSET(ipv6.tp);
+			if (udphdr_ok(skb)) {
+				struct udphdr *udp = udp_hdr(skb);
+				key->ipv6.tp.src = udp->source;
+				key->ipv6.tp.dst = udp->dest;
+			}
+		} else if (key->ip.proto == NEXTHDR_ICMP) {
+			key_len = SW_FLOW_KEY_OFFSET(ipv6.tp);
+			if (icmp6hdr_ok(skb)) {
+				error = parse_icmpv6(skb, key, &key_len, nh_len);
+				if (error < 0)
+					goto out;
+			}
+		}
+	}
+
+out:
+	*key_lenp = key_len;
+	return error;
+}
+
+static u32 ovs_flow_hash(const struct sw_flow_key *key, int key_start, int key_len)
+{
+	return jhash2((u32 *)((u8 *)key + key_start),
+		      DIV_ROUND_UP(key_len - key_start, sizeof(u32)), 0);
+}
+
+static int flow_key_start(struct sw_flow_key *key)
+{
+	if (key->phy.tun.tun_key.ipv4_dst)
+		return 0;
+	else
+		return offsetof(struct sw_flow_key, phy.priority);
+}
+
+struct sw_flow *ovs_flow_tbl_lookup(struct flow_table *table,
+				struct sw_flow_key *key, int key_len)
+{
+	struct sw_flow *flow;
+	struct hlist_node *n;
+	struct hlist_head *head;
+	u8 *_key;
+	int key_start;
+	u32 hash;
+
+	key_start = flow_key_start(key);
+	hash = ovs_flow_hash(key, key_start, key_len);
+
+	_key = (u8 *) key + key_start;
+	head = find_bucket(table, hash);
+	hlist_for_each_entry_rcu(flow, n, head, hash_node[table->node_ver]) {
+
+		if (flow->hash == hash &&
+		    !memcmp((u8 *)&flow->key + key_start, _key, key_len - key_start)) {
+			return flow;
+		}
+	}
+	return NULL;
+}
+
+void ovs_flow_tbl_insert(struct flow_table *table, struct sw_flow *flow,
+			 struct sw_flow_key *key, int key_len)
+{
+	flow->hash = ovs_flow_hash(key, flow_key_start(key), key_len);
+	memcpy(&flow->key, key, sizeof(flow->key));
+	__flow_tbl_insert(table, flow);
+}
+
+void ovs_flow_tbl_remove(struct flow_table *table, struct sw_flow *flow)
+{
+	hlist_del_rcu(&flow->hash_node[table->node_ver]);
+	table->count--;
+	BUG_ON(table->count < 0);
+}
+
+/* The size of the argument for each %OVS_KEY_ATTR_* Netlink attribute.  */
+const int ovs_key_lens[OVS_KEY_ATTR_MAX + 1] = {
+	[OVS_KEY_ATTR_ENCAP] = -1,
+	[OVS_KEY_ATTR_PRIORITY] = sizeof(u32),
+	[OVS_KEY_ATTR_IN_PORT] = sizeof(u32),
+	[OVS_KEY_ATTR_SKB_MARK] = sizeof(u32),
+	[OVS_KEY_ATTR_ETHERNET] = sizeof(struct ovs_key_ethernet),
+	[OVS_KEY_ATTR_VLAN] = sizeof(__be16),
+	[OVS_KEY_ATTR_ETHERTYPE] = sizeof(__be16),
+	[OVS_KEY_ATTR_IPV4] = sizeof(struct ovs_key_ipv4),
+	[OVS_KEY_ATTR_IPV6] = sizeof(struct ovs_key_ipv6),
+	[OVS_KEY_ATTR_TCP] = sizeof(struct ovs_key_tcp),
+	[OVS_KEY_ATTR_UDP] = sizeof(struct ovs_key_udp),
+	[OVS_KEY_ATTR_ICMP] = sizeof(struct ovs_key_icmp),
+	[OVS_KEY_ATTR_ICMPV6] = sizeof(struct ovs_key_icmpv6),
+	[OVS_KEY_ATTR_ARP] = sizeof(struct ovs_key_arp),
+	[OVS_KEY_ATTR_ND] = sizeof(struct ovs_key_nd),
+	[OVS_KEY_ATTR_TUNNEL] = -1,
+
+	/* Not upstream. */
+	[OVS_KEY_ATTR_TUN_ID] = sizeof(__be64),
+};
+
+static int ipv4_flow_from_nlattrs(struct sw_flow_key *swkey, int *key_len,
+				  const struct nlattr *a[], u64 *attrs)
+{
+	const struct ovs_key_icmp *icmp_key;
+	const struct ovs_key_tcp *tcp_key;
+	const struct ovs_key_udp *udp_key;
+
+	switch (swkey->ip.proto) {
+	case IPPROTO_TCP:
+		if (!(*attrs & (1 << OVS_KEY_ATTR_TCP)))
+			return -EINVAL;
+		*attrs &= ~(1 << OVS_KEY_ATTR_TCP);
+
+		*key_len = SW_FLOW_KEY_OFFSET(ipv4.tp);
+		tcp_key = nla_data(a[OVS_KEY_ATTR_TCP]);
+		swkey->ipv4.tp.src = tcp_key->tcp_src;
+		swkey->ipv4.tp.dst = tcp_key->tcp_dst;
+		break;
+
+	case IPPROTO_UDP:
+		if (!(*attrs & (1 << OVS_KEY_ATTR_UDP)))
+			return -EINVAL;
+		*attrs &= ~(1 << OVS_KEY_ATTR_UDP);
+
+		*key_len = SW_FLOW_KEY_OFFSET(ipv4.tp);
+		udp_key = nla_data(a[OVS_KEY_ATTR_UDP]);
+		swkey->ipv4.tp.src = udp_key->udp_src;
+		swkey->ipv4.tp.dst = udp_key->udp_dst;
+		break;
+
+	case IPPROTO_ICMP:
+		if (!(*attrs & (1 << OVS_KEY_ATTR_ICMP)))
+			return -EINVAL;
+		*attrs &= ~(1 << OVS_KEY_ATTR_ICMP);
+
+		*key_len = SW_FLOW_KEY_OFFSET(ipv4.tp);
+		icmp_key = nla_data(a[OVS_KEY_ATTR_ICMP]);
+		swkey->ipv4.tp.src = htons(icmp_key->icmp_type);
+		swkey->ipv4.tp.dst = htons(icmp_key->icmp_code);
+		break;
+	}
+
+	return 0;
+}
+
+static int ipv6_flow_from_nlattrs(struct sw_flow_key *swkey, int *key_len,
+				  const struct nlattr *a[], u64 *attrs)
+{
+	const struct ovs_key_icmpv6 *icmpv6_key;
+	const struct ovs_key_tcp *tcp_key;
+	const struct ovs_key_udp *udp_key;
+
+	switch (swkey->ip.proto) {
+	case IPPROTO_TCP:
+		if (!(*attrs & (1 << OVS_KEY_ATTR_TCP)))
+			return -EINVAL;
+		*attrs &= ~(1 << OVS_KEY_ATTR_TCP);
+
+		*key_len = SW_FLOW_KEY_OFFSET(ipv6.tp);
+		tcp_key = nla_data(a[OVS_KEY_ATTR_TCP]);
+		swkey->ipv6.tp.src = tcp_key->tcp_src;
+		swkey->ipv6.tp.dst = tcp_key->tcp_dst;
+		break;
+
+	case IPPROTO_UDP:
+		if (!(*attrs & (1 << OVS_KEY_ATTR_UDP)))
+			return -EINVAL;
+		*attrs &= ~(1 << OVS_KEY_ATTR_UDP);
+
+		*key_len = SW_FLOW_KEY_OFFSET(ipv6.tp);
+		udp_key = nla_data(a[OVS_KEY_ATTR_UDP]);
+		swkey->ipv6.tp.src = udp_key->udp_src;
+		swkey->ipv6.tp.dst = udp_key->udp_dst;
+		break;
+
+	case IPPROTO_ICMPV6:
+		if (!(*attrs & (1 << OVS_KEY_ATTR_ICMPV6)))
+			return -EINVAL;
+		*attrs &= ~(1 << OVS_KEY_ATTR_ICMPV6);
+
+		*key_len = SW_FLOW_KEY_OFFSET(ipv6.tp);
+		icmpv6_key = nla_data(a[OVS_KEY_ATTR_ICMPV6]);
+		swkey->ipv6.tp.src = htons(icmpv6_key->icmpv6_type);
+		swkey->ipv6.tp.dst = htons(icmpv6_key->icmpv6_code);
+
+		if (swkey->ipv6.tp.src == htons(NDISC_NEIGHBOUR_SOLICITATION) ||
+		    swkey->ipv6.tp.src == htons(NDISC_NEIGHBOUR_ADVERTISEMENT)) {
+			const struct ovs_key_nd *nd_key;
+
+			if (!(*attrs & (1 << OVS_KEY_ATTR_ND)))
+				return -EINVAL;
+			*attrs &= ~(1 << OVS_KEY_ATTR_ND);
+
+			*key_len = SW_FLOW_KEY_OFFSET(ipv6.nd);
+			nd_key = nla_data(a[OVS_KEY_ATTR_ND]);
+			memcpy(&swkey->ipv6.nd.target, nd_key->nd_target,
+			       sizeof(swkey->ipv6.nd.target));
+			memcpy(swkey->ipv6.nd.sll, nd_key->nd_sll, ETH_ALEN);
+			memcpy(swkey->ipv6.nd.tll, nd_key->nd_tll, ETH_ALEN);
+		}
+		break;
+	}
+
+	return 0;
+}
+
+static int parse_flow_nlattrs(const struct nlattr *attr,
+			      const struct nlattr *a[], u64 *attrsp)
+{
+	const struct nlattr *nla;
+	u64 attrs;
+	int rem;
+
+	attrs = 0;
+	nla_for_each_nested(nla, attr, rem) {
+		u16 type = nla_type(nla);
+		int expected_len;
+
+		if (type > OVS_KEY_ATTR_MAX || attrs & (1ULL << type))
+			return -EINVAL;
+
+		expected_len = ovs_key_lens[type];
+		if (nla_len(nla) != expected_len && expected_len != -1)
+			return -EINVAL;
+
+		attrs |= 1ULL << type;
+		a[type] = nla;
+	}
+	if (rem)
+		return -EINVAL;
+
+	*attrsp = attrs;
+	return 0;
+}
+
+int ipv4_tun_from_nlattr(const struct nlattr *attr,
+			 struct ovs_key_ipv4_tunnel *tun_key)
+{
+	struct nlattr *a;
+	int rem;
+	bool ttl = false;
+
+	memset(tun_key, 0, sizeof(*tun_key));
+
+	nla_for_each_nested(a, attr, rem) {
+		int type = nla_type(a);
+		static const u32 ovs_tunnel_key_lens[OVS_TUNNEL_KEY_ATTR_MAX + 1] = {
+			[OVS_TUNNEL_KEY_ATTR_ID] = sizeof(u64),
+			[OVS_TUNNEL_KEY_ATTR_IPV4_SRC] = sizeof(u32),
+			[OVS_TUNNEL_KEY_ATTR_IPV4_DST] = sizeof(u32),
+			[OVS_TUNNEL_KEY_ATTR_TOS] = 1,
+			[OVS_TUNNEL_KEY_ATTR_TTL] = 1,
+			[OVS_TUNNEL_KEY_ATTR_DONT_FRAGMENT] = 0,
+			[OVS_TUNNEL_KEY_ATTR_CSUM] = 0,
+		};
+
+		if (type > OVS_TUNNEL_KEY_ATTR_MAX ||
+			ovs_tunnel_key_lens[type] != nla_len(a))
+			return -EINVAL;
+
+		switch (type) {
+		case OVS_TUNNEL_KEY_ATTR_ID:
+			tun_key->tun_id = nla_get_be64(a);
+			tun_key->tun_flags |= OVS_TNL_F_KEY;
+			break;
+		case OVS_TUNNEL_KEY_ATTR_IPV4_SRC:
+			tun_key->ipv4_src = nla_get_be32(a);
+			break;
+		case OVS_TUNNEL_KEY_ATTR_IPV4_DST:
+			tun_key->ipv4_dst = nla_get_be32(a);
+			break;
+		case OVS_TUNNEL_KEY_ATTR_TOS:
+			tun_key->ipv4_tos = nla_get_u8(a);
+			break;
+		case OVS_TUNNEL_KEY_ATTR_TTL:
+			tun_key->ipv4_ttl = nla_get_u8(a);
+			ttl = true;
+			break;
+		case OVS_TUNNEL_KEY_ATTR_DONT_FRAGMENT:
+			tun_key->tun_flags |= OVS_TNL_F_DONT_FRAGMENT;
+			break;
+		case OVS_TUNNEL_KEY_ATTR_CSUM:
+			tun_key->tun_flags |= OVS_TNL_F_CSUM;
+			break;
+		default:
+			return -EINVAL;
+
+		}
+	}
+	if (rem > 0)
+		return -EINVAL;
+
+	if (!tun_key->ipv4_dst)
+		return -EINVAL;
+
+	if (!ttl)
+		return -EINVAL;
+
+	return 0;
+}
+
+int ipv4_tun_to_nlattr(struct sk_buff *skb,
+			const struct ovs_key_ipv4_tunnel *tun_key)
+{
+	struct nlattr *nla;
+
+	nla = nla_nest_start(skb, OVS_KEY_ATTR_TUNNEL);
+	if (!nla)
+		return -EMSGSIZE;
+
+	if (tun_key->tun_flags & OVS_TNL_F_KEY &&
+	    nla_put_be64(skb, OVS_TUNNEL_KEY_ATTR_ID, tun_key->tun_id))
+		return -EMSGSIZE;
+	if (tun_key->ipv4_src &&
+	    nla_put_be32(skb, OVS_TUNNEL_KEY_ATTR_IPV4_SRC, tun_key->ipv4_src))
+		return -EMSGSIZE;
+	if (nla_put_be32(skb, OVS_TUNNEL_KEY_ATTR_IPV4_DST, tun_key->ipv4_dst))
+		return -EMSGSIZE;
+	if (tun_key->ipv4_tos &&
+	    nla_put_u8(skb, OVS_TUNNEL_KEY_ATTR_TOS, tun_key->ipv4_tos))
+		return -EMSGSIZE;
+	if (nla_put_u8(skb, OVS_TUNNEL_KEY_ATTR_TTL, tun_key->ipv4_ttl))
+		return -EMSGSIZE;
+	if ((tun_key->tun_flags & OVS_TNL_F_DONT_FRAGMENT) &&
+		nla_put_flag(skb, OVS_TUNNEL_KEY_ATTR_DONT_FRAGMENT))
+		return -EMSGSIZE;
+	if ((tun_key->tun_flags & OVS_TNL_F_CSUM) &&
+		nla_put_flag(skb, OVS_TUNNEL_KEY_ATTR_CSUM))
+		return -EMSGSIZE;
+
+	nla_nest_end(skb, nla);
+	return 0;
+}
+
+/**
+ * ovs_flow_from_nlattrs - parses Netlink attributes into a flow key.
+ * @swkey: receives the extracted flow key.
+ * @key_lenp: number of bytes used in @swkey.
+ * @attr: Netlink attribute holding nested %OVS_KEY_ATTR_* Netlink attribute
+ * sequence.
+ */
+int ovs_flow_from_nlattrs(struct sw_flow_key *swkey, int *key_lenp,
+		      const struct nlattr *attr)
+{
+	const struct nlattr *a[OVS_KEY_ATTR_MAX + 1];
+	const struct ovs_key_ethernet *eth_key;
+	int key_len;
+	u64 attrs;
+	int err;
+
+	memset(swkey, 0, sizeof(struct sw_flow_key));
+	key_len = SW_FLOW_KEY_OFFSET(eth);
+
+	err = parse_flow_nlattrs(attr, a, &attrs);
+	if (err)
+		return err;
+
+	/* Metadata attributes. */
+	if (attrs & (1 << OVS_KEY_ATTR_PRIORITY)) {
+		swkey->phy.priority = nla_get_u32(a[OVS_KEY_ATTR_PRIORITY]);
+		attrs &= ~(1 << OVS_KEY_ATTR_PRIORITY);
+	}
+	if (attrs & (1 << OVS_KEY_ATTR_IN_PORT)) {
+		u32 in_port = nla_get_u32(a[OVS_KEY_ATTR_IN_PORT]);
+		if (in_port >= DP_MAX_PORTS)
+			return -EINVAL;
+		swkey->phy.in_port = in_port;
+		attrs &= ~(1 << OVS_KEY_ATTR_IN_PORT);
+	} else {
+		swkey->phy.in_port = DP_MAX_PORTS;
+	}
+	if (attrs & (1 << OVS_KEY_ATTR_SKB_MARK)) {
+		uint32_t mark = nla_get_u32(a[OVS_KEY_ATTR_SKB_MARK]);
+		swkey->phy.skb_mark = mark;
+		attrs &= ~(1 << OVS_KEY_ATTR_SKB_MARK);
+	}
+
+	if (attrs & (1ULL << OVS_KEY_ATTR_TUN_ID) &&
+	    attrs & (1ULL << OVS_KEY_ATTR_TUNNEL)) {
+		__be64 tun_id;
+
+		err = ipv4_tun_from_nlattr(a[OVS_KEY_ATTR_TUNNEL], &swkey->phy.tun.tun_key);
+		if (err)
+			return err;
+
+		if (!(swkey->phy.tun.tun_key.tun_flags & OVS_TNL_F_KEY))
+			return -EINVAL;
+
+		tun_id = nla_get_be64(a[OVS_KEY_ATTR_TUN_ID]);
+		if (tun_id != swkey->phy.tun.tun_key.tun_id)
+			return -EINVAL;
+
+		attrs &= ~(1ULL << OVS_KEY_ATTR_TUN_ID);
+		attrs &= ~(1ULL << OVS_KEY_ATTR_TUNNEL);
+	} else if (attrs & (1ULL << OVS_KEY_ATTR_TUNNEL)) {
+
+		err = ipv4_tun_from_nlattr(a[OVS_KEY_ATTR_TUNNEL], &swkey->phy.tun.tun_key);
+		if (err)
+			return err;
+
+		attrs &= ~(1ULL << OVS_KEY_ATTR_TUNNEL);
+	}
+
+	/* Data attributes. */
+	if (!(attrs & (1 << OVS_KEY_ATTR_ETHERNET)))
+		return -EINVAL;
+	attrs &= ~(1 << OVS_KEY_ATTR_ETHERNET);
+
+	eth_key = nla_data(a[OVS_KEY_ATTR_ETHERNET]);
+	memcpy(swkey->eth.src, eth_key->eth_src, ETH_ALEN);
+	memcpy(swkey->eth.dst, eth_key->eth_dst, ETH_ALEN);
+
+	if (attrs & (1u << OVS_KEY_ATTR_ETHERTYPE) &&
+	    nla_get_be16(a[OVS_KEY_ATTR_ETHERTYPE]) == htons(ETH_P_8021Q)) {
+		const struct nlattr *encap;
+		__be16 tci;
+
+		if (attrs != ((1 << OVS_KEY_ATTR_VLAN) |
+			      (1 << OVS_KEY_ATTR_ETHERTYPE) |
+			      (1 << OVS_KEY_ATTR_ENCAP)))
+			return -EINVAL;
+
+		encap = a[OVS_KEY_ATTR_ENCAP];
+		tci = nla_get_be16(a[OVS_KEY_ATTR_VLAN]);
+		if (tci & htons(VLAN_TAG_PRESENT)) {
+			swkey->eth.tci = tci;
+
+			err = parse_flow_nlattrs(encap, a, &attrs);
+			if (err)
+				return err;
+		} else if (!tci) {
+			/* Corner case for truncated 802.1Q header. */
+			if (nla_len(encap))
+				return -EINVAL;
+
+			swkey->eth.type = htons(ETH_P_8021Q);
+			*key_lenp = key_len;
+			return 0;
+		} else {
+			return -EINVAL;
+		}
+	}
+
+	if (attrs & (1 << OVS_KEY_ATTR_ETHERTYPE)) {
+		swkey->eth.type = nla_get_be16(a[OVS_KEY_ATTR_ETHERTYPE]);
+		if (ntohs(swkey->eth.type) < 1536)
+			return -EINVAL;
+		attrs &= ~(1 << OVS_KEY_ATTR_ETHERTYPE);
+	} else {
+		swkey->eth.type = htons(ETH_P_802_2);
+	}
+
+	if (swkey->eth.type == htons(ETH_P_IP)) {
+		const struct ovs_key_ipv4 *ipv4_key;
+
+		if (!(attrs & (1 << OVS_KEY_ATTR_IPV4)))
+			return -EINVAL;
+		attrs &= ~(1 << OVS_KEY_ATTR_IPV4);
+
+		key_len = SW_FLOW_KEY_OFFSET(ipv4.addr);
+		ipv4_key = nla_data(a[OVS_KEY_ATTR_IPV4]);
+		if (ipv4_key->ipv4_frag > OVS_FRAG_TYPE_MAX)
+			return -EINVAL;
+		swkey->ip.proto = ipv4_key->ipv4_proto;
+		swkey->ip.tos = ipv4_key->ipv4_tos;
+		swkey->ip.ttl = ipv4_key->ipv4_ttl;
+		swkey->ip.frag = ipv4_key->ipv4_frag;
+		swkey->ipv4.addr.src = ipv4_key->ipv4_src;
+		swkey->ipv4.addr.dst = ipv4_key->ipv4_dst;
+
+		if (swkey->ip.frag != OVS_FRAG_TYPE_LATER) {
+			err = ipv4_flow_from_nlattrs(swkey, &key_len, a, &attrs);
+			if (err)
+				return err;
+		}
+	} else if (swkey->eth.type == htons(ETH_P_IPV6)) {
+		const struct ovs_key_ipv6 *ipv6_key;
+
+		if (!(attrs & (1 << OVS_KEY_ATTR_IPV6)))
+			return -EINVAL;
+		attrs &= ~(1 << OVS_KEY_ATTR_IPV6);
+
+		key_len = SW_FLOW_KEY_OFFSET(ipv6.label);
+		ipv6_key = nla_data(a[OVS_KEY_ATTR_IPV6]);
+		if (ipv6_key->ipv6_frag > OVS_FRAG_TYPE_MAX)
+			return -EINVAL;
+		swkey->ipv6.label = ipv6_key->ipv6_label;
+		swkey->ip.proto = ipv6_key->ipv6_proto;
+		swkey->ip.tos = ipv6_key->ipv6_tclass;
+		swkey->ip.ttl = ipv6_key->ipv6_hlimit;
+		swkey->ip.frag = ipv6_key->ipv6_frag;
+		memcpy(&swkey->ipv6.addr.src, ipv6_key->ipv6_src,
+		       sizeof(swkey->ipv6.addr.src));
+		memcpy(&swkey->ipv6.addr.dst, ipv6_key->ipv6_dst,
+		       sizeof(swkey->ipv6.addr.dst));
+
+		if (swkey->ip.frag != OVS_FRAG_TYPE_LATER) {
+			err = ipv6_flow_from_nlattrs(swkey, &key_len, a, &attrs);
+			if (err)
+				return err;
+		}
+	} else if (swkey->eth.type == htons(ETH_P_ARP) ||
+		   swkey->eth.type == htons(ETH_P_RARP)) {
+		const struct ovs_key_arp *arp_key;
+
+		if (!(attrs & (1 << OVS_KEY_ATTR_ARP)))
+			return -EINVAL;
+		attrs &= ~(1 << OVS_KEY_ATTR_ARP);
+
+		key_len = SW_FLOW_KEY_OFFSET(ipv4.arp);
+		arp_key = nla_data(a[OVS_KEY_ATTR_ARP]);
+		swkey->ipv4.addr.src = arp_key->arp_sip;
+		swkey->ipv4.addr.dst = arp_key->arp_tip;
+		if (arp_key->arp_op & htons(0xff00))
+			return -EINVAL;
+		swkey->ip.proto = ntohs(arp_key->arp_op);
+		memcpy(swkey->ipv4.arp.sha, arp_key->arp_sha, ETH_ALEN);
+		memcpy(swkey->ipv4.arp.tha, arp_key->arp_tha, ETH_ALEN);
+	}
+
+	if (attrs)
+		return -EINVAL;
+	*key_lenp = key_len;
+
+	return 0;
+}
+
+/**
+ * ovs_flow_metadata_from_nlattrs - parses Netlink attributes into a flow key.
+ * @in_port: receives the extracted input port.
+ * @tun_id: receives the extracted tunnel ID.
+ * @key: Netlink attribute holding nested %OVS_KEY_ATTR_* Netlink attribute
+ * sequence.
+ *
+ * This parses a series of Netlink attributes that form a flow key, which must
+ * take the same form accepted by flow_from_nlattrs(), but only enough of it to
+ * get the metadata, that is, the parts of the flow key that cannot be
+ * extracted from the packet itself.
+ */
+
+int ovs_flow_metadata_from_nlattrs(struct sw_flow *flow, int key_len, const struct nlattr *attr)
+{
+	struct ovs_key_ipv4_tunnel *tun_key = &flow->key.phy.tun.tun_key;
+	const struct nlattr *nla;
+	int rem;
+	__be64 tun_id = 0;
+
+	flow->key.phy.in_port = DP_MAX_PORTS;
+	flow->key.phy.priority = 0;
+	flow->key.phy.skb_mark = 0;
+	memset(tun_key, 0, sizeof(flow->key.phy.tun.tun_key));
+
+	nla_for_each_nested(nla, attr, rem) {
+		int type = nla_type(nla);
+
+		if (type <= OVS_KEY_ATTR_MAX && ovs_key_lens[type] > 0) {
+			int err;
+
+			if (nla_len(nla) != ovs_key_lens[type])
+				return -EINVAL;
+
+			switch (type) {
+			case OVS_KEY_ATTR_PRIORITY:
+				flow->key.phy.priority = nla_get_u32(nla);
+				break;
+
+			case OVS_KEY_ATTR_TUN_ID:
+				tun_id = nla_get_be64(nla);
+
+				if (tun_key->ipv4_dst) {
+					if (!(tun_key->tun_flags & OVS_TNL_F_KEY))
+						return -EINVAL;
+					if (tun_key->tun_id != tun_id)
+						return -EINVAL;
+					break;
+				}
+				tun_key->tun_id = tun_id;
+				tun_key->tun_flags |= OVS_TNL_F_KEY;
+
+				break;
+
+			case OVS_KEY_ATTR_TUNNEL:
+				if (tun_key->tun_flags & OVS_TNL_F_KEY) {
+					tun_id = tun_key->tun_id;
+					err = ipv4_tun_from_nlattr(nla, tun_key);
+					if (err)
+						return err;
+
+					if (!(tun_key->tun_flags & OVS_TNL_F_KEY))
+						return -EINVAL;
+
+					if (tun_key->tun_id != tun_id)
+						return -EINVAL;
+				} else {
+					err = ipv4_tun_from_nlattr(nla, tun_key);
+					if (err)
+						return err;
+				}
+				break;
+
+			case OVS_KEY_ATTR_IN_PORT:
+				if (nla_get_u32(nla) >= DP_MAX_PORTS)
+					return -EINVAL;
+				flow->key.phy.in_port = nla_get_u32(nla);
+				break;
+
+			case OVS_KEY_ATTR_SKB_MARK:
+				flow->key.phy.skb_mark = nla_get_u32(nla);
+				break;
+			}
+		}
+	}
+	if (rem)
+		return -EINVAL;
+
+	flow->hash = ovs_flow_hash(&flow->key,
+				   flow_key_start(&flow->key), key_len);
+
+	return 0;
+}
+
+int ovs_flow_to_nlattrs(const struct sw_flow_key *swkey, struct sk_buff *skb)
+{
+	struct ovs_key_ethernet *eth_key;
+	struct nlattr *nla, *encap;
+
+	if (swkey->phy.priority &&
+	    nla_put_u32(skb, OVS_KEY_ATTR_PRIORITY, swkey->phy.priority))
+		goto nla_put_failure;
+
+	if (swkey->phy.tun.tun_key.ipv4_dst &&
+	    ipv4_tun_to_nlattr(skb, &swkey->phy.tun.tun_key))
+		goto nla_put_failure;
+
+	if ((swkey->phy.tun.tun_key.tun_flags & OVS_TNL_F_KEY) &&
+	    nla_put_be64(skb, OVS_KEY_ATTR_TUN_ID, swkey->phy.tun.tun_key.tun_id))
+		goto nla_put_failure;
+
+	if (swkey->phy.in_port != DP_MAX_PORTS &&
+	    nla_put_u32(skb, OVS_KEY_ATTR_IN_PORT, swkey->phy.in_port))
+		goto nla_put_failure;
+
+	if (swkey->phy.skb_mark &&
+	    nla_put_u32(skb, OVS_KEY_ATTR_SKB_MARK, swkey->phy.skb_mark))
+		goto nla_put_failure;
+
+	nla = nla_reserve(skb, OVS_KEY_ATTR_ETHERNET, sizeof(*eth_key));
+	if (!nla)
+		goto nla_put_failure;
+	eth_key = nla_data(nla);
+	memcpy(eth_key->eth_src, swkey->eth.src, ETH_ALEN);
+	memcpy(eth_key->eth_dst, swkey->eth.dst, ETH_ALEN);
+
+	if (swkey->eth.tci || swkey->eth.type == htons(ETH_P_8021Q)) {
+		if (nla_put_be16(skb, OVS_KEY_ATTR_ETHERTYPE, htons(ETH_P_8021Q)) ||
+		    nla_put_be16(skb, OVS_KEY_ATTR_VLAN, swkey->eth.tci))
+			goto nla_put_failure;
+		encap = nla_nest_start(skb, OVS_KEY_ATTR_ENCAP);
+		if (!swkey->eth.tci)
+			goto unencap;
+	} else {
+		encap = NULL;
+	}
+
+	if (swkey->eth.type == htons(ETH_P_802_2))
+		goto unencap;
+
+	if (nla_put_be16(skb, OVS_KEY_ATTR_ETHERTYPE, swkey->eth.type))
+		goto nla_put_failure;
+
+	if (swkey->eth.type == htons(ETH_P_IP)) {
+		struct ovs_key_ipv4 *ipv4_key;
+
+		nla = nla_reserve(skb, OVS_KEY_ATTR_IPV4, sizeof(*ipv4_key));
+		if (!nla)
+			goto nla_put_failure;
+		ipv4_key = nla_data(nla);
+		ipv4_key->ipv4_src = swkey->ipv4.addr.src;
+		ipv4_key->ipv4_dst = swkey->ipv4.addr.dst;
+		ipv4_key->ipv4_proto = swkey->ip.proto;
+		ipv4_key->ipv4_tos = swkey->ip.tos;
+		ipv4_key->ipv4_ttl = swkey->ip.ttl;
+		ipv4_key->ipv4_frag = swkey->ip.frag;
+	} else if (swkey->eth.type == htons(ETH_P_IPV6)) {
+		struct ovs_key_ipv6 *ipv6_key;
+
+		nla = nla_reserve(skb, OVS_KEY_ATTR_IPV6, sizeof(*ipv6_key));
+		if (!nla)
+			goto nla_put_failure;
+		ipv6_key = nla_data(nla);
+		memcpy(ipv6_key->ipv6_src, &swkey->ipv6.addr.src,
+				sizeof(ipv6_key->ipv6_src));
+		memcpy(ipv6_key->ipv6_dst, &swkey->ipv6.addr.dst,
+				sizeof(ipv6_key->ipv6_dst));
+		ipv6_key->ipv6_label = swkey->ipv6.label;
+		ipv6_key->ipv6_proto = swkey->ip.proto;
+		ipv6_key->ipv6_tclass = swkey->ip.tos;
+		ipv6_key->ipv6_hlimit = swkey->ip.ttl;
+		ipv6_key->ipv6_frag = swkey->ip.frag;
+	} else if (swkey->eth.type == htons(ETH_P_ARP) ||
+		   swkey->eth.type == htons(ETH_P_RARP)) {
+		struct ovs_key_arp *arp_key;
+
+		nla = nla_reserve(skb, OVS_KEY_ATTR_ARP, sizeof(*arp_key));
+		if (!nla)
+			goto nla_put_failure;
+		arp_key = nla_data(nla);
+		memset(arp_key, 0, sizeof(struct ovs_key_arp));
+		arp_key->arp_sip = swkey->ipv4.addr.src;
+		arp_key->arp_tip = swkey->ipv4.addr.dst;
+		arp_key->arp_op = htons(swkey->ip.proto);
+		memcpy(arp_key->arp_sha, swkey->ipv4.arp.sha, ETH_ALEN);
+		memcpy(arp_key->arp_tha, swkey->ipv4.arp.tha, ETH_ALEN);
+	}
+
+	if ((swkey->eth.type == htons(ETH_P_IP) ||
+	     swkey->eth.type == htons(ETH_P_IPV6)) &&
+	     swkey->ip.frag != OVS_FRAG_TYPE_LATER) {
+
+		if (swkey->ip.proto == IPPROTO_TCP) {
+			struct ovs_key_tcp *tcp_key;
+
+			nla = nla_reserve(skb, OVS_KEY_ATTR_TCP, sizeof(*tcp_key));
+			if (!nla)
+				goto nla_put_failure;
+			tcp_key = nla_data(nla);
+			if (swkey->eth.type == htons(ETH_P_IP)) {
+				tcp_key->tcp_src = swkey->ipv4.tp.src;
+				tcp_key->tcp_dst = swkey->ipv4.tp.dst;
+			} else if (swkey->eth.type == htons(ETH_P_IPV6)) {
+				tcp_key->tcp_src = swkey->ipv6.tp.src;
+				tcp_key->tcp_dst = swkey->ipv6.tp.dst;
+			}
+		} else if (swkey->ip.proto == IPPROTO_UDP) {
+			struct ovs_key_udp *udp_key;
+
+			nla = nla_reserve(skb, OVS_KEY_ATTR_UDP, sizeof(*udp_key));
+			if (!nla)
+				goto nla_put_failure;
+			udp_key = nla_data(nla);
+			if (swkey->eth.type == htons(ETH_P_IP)) {
+				udp_key->udp_src = swkey->ipv4.tp.src;
+				udp_key->udp_dst = swkey->ipv4.tp.dst;
+			} else if (swkey->eth.type == htons(ETH_P_IPV6)) {
+				udp_key->udp_src = swkey->ipv6.tp.src;
+				udp_key->udp_dst = swkey->ipv6.tp.dst;
+			}
+		} else if (swkey->eth.type == htons(ETH_P_IP) &&
+			   swkey->ip.proto == IPPROTO_ICMP) {
+			struct ovs_key_icmp *icmp_key;
+
+			nla = nla_reserve(skb, OVS_KEY_ATTR_ICMP, sizeof(*icmp_key));
+			if (!nla)
+				goto nla_put_failure;
+			icmp_key = nla_data(nla);
+			icmp_key->icmp_type = ntohs(swkey->ipv4.tp.src);
+			icmp_key->icmp_code = ntohs(swkey->ipv4.tp.dst);
+		} else if (swkey->eth.type == htons(ETH_P_IPV6) &&
+			   swkey->ip.proto == IPPROTO_ICMPV6) {
+			struct ovs_key_icmpv6 *icmpv6_key;
+
+			nla = nla_reserve(skb, OVS_KEY_ATTR_ICMPV6,
+						sizeof(*icmpv6_key));
+			if (!nla)
+				goto nla_put_failure;
+			icmpv6_key = nla_data(nla);
+			icmpv6_key->icmpv6_type = ntohs(swkey->ipv6.tp.src);
+			icmpv6_key->icmpv6_code = ntohs(swkey->ipv6.tp.dst);
+
+			if (icmpv6_key->icmpv6_type == NDISC_NEIGHBOUR_SOLICITATION ||
+			    icmpv6_key->icmpv6_type == NDISC_NEIGHBOUR_ADVERTISEMENT) {
+				struct ovs_key_nd *nd_key;
+
+				nla = nla_reserve(skb, OVS_KEY_ATTR_ND, sizeof(*nd_key));
+				if (!nla)
+					goto nla_put_failure;
+				nd_key = nla_data(nla);
+				memcpy(nd_key->nd_target, &swkey->ipv6.nd.target,
+							sizeof(nd_key->nd_target));
+				memcpy(nd_key->nd_sll, swkey->ipv6.nd.sll, ETH_ALEN);
+				memcpy(nd_key->nd_tll, swkey->ipv6.nd.tll, ETH_ALEN);
+			}
+		}
+	}
+
+unencap:
+	if (encap)
+		nla_nest_end(skb, encap);
+
+	return 0;
+
+nla_put_failure:
+	return -EMSGSIZE;
+}
+
+/* Initializes the flow module.
+ * Returns zero if successful or a negative error code. */
+int ovs_flow_init(void)
+{
+	flow_cache = kmem_cache_create("sw_flow", sizeof(struct sw_flow), 0,
+					0, NULL);
+	if (flow_cache == NULL)
+		return -ENOMEM;
+
+	return 0;
+}
+
+/* Uninitializes the flow module. */
+void ovs_flow_exit(void)
+{
+	kmem_cache_destroy(flow_cache);
+}
diff --git a/drivers/staging/openvswitch_nv/flow.h b/drivers/staging/openvswitch_nv/flow.h
new file mode 100644
index 000000000000..54d244d392c6
--- /dev/null
+++ b/drivers/staging/openvswitch_nv/flow.h
@@ -0,0 +1,239 @@
+/*
+ * Copyright (c) 2007-2011 Nicira, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#ifndef FLOW_H
+#define FLOW_H 1
+
+#include <linux/kernel.h>
+#include <linux/netlink.h>
+#include <linux/openvswitch-nv.h>
+#include <linux/spinlock.h>
+#include <linux/types.h>
+#include <linux/rcupdate.h>
+#include <linux/if_ether.h>
+#include <linux/in6.h>
+#include <linux/jiffies.h>
+#include <linux/time.h>
+#include <linux/flex_array.h>
+#include <net/inet_ecn.h>
+
+struct sk_buff;
+
+struct sw_flow_actions {
+	struct rcu_head rcu;
+	u32 actions_len;
+	struct nlattr actions[];
+};
+
+/* Tunnel flow flags. */
+#define OVS_TNL_F_DONT_FRAGMENT		(1 << 0)
+#define OVS_TNL_F_CSUM			(1 << 1)
+#define OVS_TNL_F_KEY			(1 << 2)
+
+/* Used to memset ovs_key_ipv4_tunnel padding. */
+#define OVS_TUNNEL_KEY_SIZE					\
+        (offsetof(struct ovs_key_ipv4_tunnel, ipv4_ttl) + 	\
+         FIELD_SIZEOF(struct ovs_key_ipv4_tunnel, ipv4_ttl))
+
+struct ovs_key_ipv4_tunnel {
+	__be64 tun_id;
+	__be32 ipv4_src;
+	__be32 ipv4_dst;
+	u16  tun_flags;
+	u8   ipv4_tos;
+	u8   ipv4_ttl;
+};
+
+struct sw_flow_key {
+	struct {
+		union {
+			struct ovs_key_ipv4_tunnel tun_key;  /* Encapsulating tunnel key. */
+		} tun;
+		u32	priority;	/* Packet QoS priority. */
+		u32	skb_mark;	/* SKB mark. */
+		u16	in_port;	/* Input switch port (or DP_MAX_PORTS). */
+	} phy;
+	struct {
+		u8     src[ETH_ALEN];	/* Ethernet source address. */
+		u8     dst[ETH_ALEN];	/* Ethernet destination address. */
+		__be16 tci;		/* 0 if no VLAN, VLAN_TAG_PRESENT set otherwise. */
+		__be16 type;		/* Ethernet frame type. */
+	} eth;
+	struct {
+		u8     proto;		/* IP protocol or lower 8 bits of ARP opcode. */
+		u8     tos;		/* IP ToS. */
+		u8     ttl;		/* IP TTL/hop limit. */
+		u8     frag;		/* One of OVS_FRAG_TYPE_*. */
+	} ip;
+	union {
+		struct {
+			struct {
+				__be32 src;	/* IP source address. */
+				__be32 dst;	/* IP destination address. */
+			} addr;
+			union {
+				struct {
+					__be16 src;		/* TCP/UDP source port. */
+					__be16 dst;		/* TCP/UDP destination port. */
+				} tp;
+				struct {
+					u8 sha[ETH_ALEN];	/* ARP source hardware address. */
+					u8 tha[ETH_ALEN];	/* ARP target hardware address. */
+				} arp;
+			};
+		} ipv4;
+		struct {
+			struct {
+				struct in6_addr src;	/* IPv6 source address. */
+				struct in6_addr dst;	/* IPv6 destination address. */
+			} addr;
+			__be32 label;			/* IPv6 flow label. */
+			struct {
+				__be16 src;		/* TCP/UDP source port. */
+				__be16 dst;		/* TCP/UDP destination port. */
+			} tp;
+			struct {
+				struct in6_addr target;	/* ND target address. */
+				u8 sll[ETH_ALEN];	/* ND source link layer address. */
+				u8 tll[ETH_ALEN];	/* ND target link layer address. */
+			} nd;
+		} ipv6;
+	};
+};
+
+struct sw_flow {
+	struct rcu_head rcu;
+	struct hlist_node hash_node[2];
+	u32 hash;
+
+	struct sw_flow_key key;
+	struct sw_flow_actions __rcu *sf_acts;
+
+	spinlock_t lock;	/* Lock for values below. */
+	unsigned long used;	/* Last used time (in jiffies). */
+	u64 packet_count;	/* Number of packets matched. */
+	u64 byte_count;		/* Number of bytes matched. */
+	u8 tcp_flags;		/* Union of seen TCP flags. */
+};
+
+struct arp_eth_header {
+	__be16      ar_hrd;	/* format of hardware address   */
+	__be16      ar_pro;	/* format of protocol address   */
+	unsigned char   ar_hln;	/* length of hardware address   */
+	unsigned char   ar_pln;	/* length of protocol address   */
+	__be16      ar_op;	/* ARP opcode (command)     */
+
+	/* Ethernet+IPv4 specific members. */
+	unsigned char       ar_sha[ETH_ALEN];	/* sender hardware address  */
+	unsigned char       ar_sip[4];		/* sender IP address        */
+	unsigned char       ar_tha[ETH_ALEN];	/* target hardware address  */
+	unsigned char       ar_tip[4];		/* target IP address        */
+} __packed;
+
+int ovs_flow_init(void);
+void ovs_flow_exit(void);
+
+struct sw_flow *ovs_flow_alloc(void);
+void ovs_flow_deferred_free(struct sw_flow *);
+void ovs_flow_free(struct sw_flow *);
+
+struct sw_flow_actions *ovs_flow_actions_alloc(int actions_len);
+void ovs_flow_deferred_free_acts(struct sw_flow_actions *);
+
+int ovs_flow_extract(struct sk_buff *, u16 in_port, struct sw_flow_key *,
+		     int *key_lenp);
+void ovs_flow_used(struct sw_flow *, struct sk_buff *);
+u64 ovs_flow_used_time(unsigned long flow_jiffies);
+
+/* Upper bound on the length of a nlattr-formatted flow key.  The longest
+ * nlattr-formatted flow key would be:
+ *
+ *                                     struct  pad  nl hdr  total
+ *                                     ------  ---  ------  -----
+ *  OVS_KEY_ATTR_PRIORITY                4    --     4      8
+ *  OVS_KEY_ATTR_TUN_ID                  8    --     4     12
+ *  OVS_KEY_ATTR_TUNNEL                  0    --     4      4
+ *  - OVS_TUNNEL_KEY_ATTR_ID             8    --     4     12
+ *  - OVS_TUNNEL_KEY_ATTR_IPV4_SRC       4    --     4      8
+ *  - OVS_TUNNEL_KEY_ATTR_IPV4_DST       4    --     4      8
+ *  - OVS_TUNNEL_KEY_ATTR_TOS            1    3      4      8
+ *  - OVS_TUNNEL_KEY_ATTR_TTL            1    3      4      8
+ *  - OVS_TUNNEL_KEY_ATTR_DONT_FRAGMENT  0    --     4      4
+ *  - OVS_TUNNEL_KEY_ATTR_CSUM           0    --     4      4
+ *  OVS_KEY_ATTR_IN_PORT                 4    --     4      8
+ *  OVS_KEY_ATTR_SKB_MARK                4    --     4      8
+ *  OVS_KEY_ATTR_ETHERNET               12    --     4     16
+ *  OVS_KEY_ATTR_ETHERTYPE               2     2     4      8  (outer VLAN ethertype)
+ *  OVS_KEY_ATTR_8021Q                   4    --     4      8
+ *  OVS_KEY_ATTR_ENCAP                   0    --     4      4  (VLAN encapsulation)
+ *  OVS_KEY_ATTR_ETHERTYPE               2     2     4      8  (inner VLAN ethertype)
+ *  OVS_KEY_ATTR_IPV6                   40    --     4     44
+ *  OVS_KEY_ATTR_ICMPV6                  2     2     4      8
+ *  OVS_KEY_ATTR_ND                     28    --     4     32
+ *  ----------------------------------------------------------
+ *  total                                                 220
+ */
+#define FLOW_BUFSIZE 220
+
+int ovs_flow_to_nlattrs(const struct sw_flow_key *, struct sk_buff *);
+int ovs_flow_from_nlattrs(struct sw_flow_key *swkey, int *key_lenp,
+		      const struct nlattr *);
+int ovs_flow_metadata_from_nlattrs(struct sw_flow *flow, int key_len,
+				   const struct nlattr *attr);
+
+#define MAX_ACTIONS_BUFSIZE	(16 * 1024)
+#define TBL_MIN_BUCKETS		1024
+
+struct flow_table {
+	struct flex_array *buckets;
+	unsigned int count, n_buckets;
+	struct rcu_head rcu;
+	int node_ver;
+	u32 hash_seed;
+	bool keep_flows;
+};
+
+static inline int ovs_flow_tbl_count(struct flow_table *table)
+{
+	return table->count;
+}
+
+static inline int ovs_flow_tbl_need_to_expand(struct flow_table *table)
+{
+	return (table->count > table->n_buckets);
+}
+
+struct sw_flow *ovs_flow_tbl_lookup(struct flow_table *table,
+				    struct sw_flow_key *key, int len);
+void ovs_flow_tbl_destroy(struct flow_table *table);
+void ovs_flow_tbl_deferred_destroy(struct flow_table *table);
+struct flow_table *ovs_flow_tbl_alloc(int new_size);
+struct flow_table *ovs_flow_tbl_expand(struct flow_table *table);
+struct flow_table *ovs_flow_tbl_rehash(struct flow_table *table);
+void ovs_flow_tbl_insert(struct flow_table *table, struct sw_flow *flow,
+			 struct sw_flow_key *key, int key_len);
+void ovs_flow_tbl_remove(struct flow_table *table, struct sw_flow *flow);
+
+struct sw_flow *ovs_flow_tbl_next(struct flow_table *table, u32 *bucket, u32 *idx);
+extern const int ovs_key_lens[OVS_KEY_ATTR_MAX + 1];
+int ipv4_tun_from_nlattr(const struct nlattr *attr,
+			 struct ovs_key_ipv4_tunnel *tun_key);
+int ipv4_tun_to_nlattr(struct sk_buff *skb,
+			const struct ovs_key_ipv4_tunnel *tun_key);
+
+#endif /* flow.h */
diff --git a/drivers/staging/openvswitch_nv/genl_exec.c b/drivers/staging/openvswitch_nv/genl_exec.c
new file mode 100644
index 000000000000..ba8dfc09de25
--- /dev/null
+++ b/drivers/staging/openvswitch_nv/genl_exec.c
@@ -0,0 +1,147 @@
+/*
+ * Copyright (c) 2007-2012 Nicira, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/version.h>
+#include <linux/completion.h>
+#include <net/genetlink.h>
+#include "genl_exec.h"
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,35)
+
+static DEFINE_MUTEX(genl_exec_lock);
+
+static genl_exec_func_t	 genl_exec_function;
+static int		 genl_exec_function_ret;
+static void		*genl_exec_data;
+static struct completion done;
+
+static struct sk_buff *genlmsg_skb;
+
+static int genl_exec_cmd(struct sk_buff *dummy, struct genl_info *dummy2)
+{
+	genl_exec_function_ret = genl_exec_function(genl_exec_data);
+	complete(&done);
+	return 0;
+}
+
+enum exec_cmd {
+	GENL_EXEC_UNSPEC,
+	GENL_EXEC_RUN,
+};
+
+static struct genl_family genl_exec_family = {
+	.id = GENL_ID_GENERATE,
+	.name = "ovs_genl_exec",
+	.version = 1,
+};
+
+static struct genl_ops genl_exec_ops[] = {
+	{
+	 .cmd = GENL_EXEC_RUN,
+	 .doit = genl_exec_cmd,
+	 .flags = CAP_NET_ADMIN,
+	},
+};
+
+int genl_exec_init(void)
+{
+	int err;
+
+	err = genl_register_family_with_ops(&genl_exec_family,
+			genl_exec_ops, ARRAY_SIZE(genl_exec_ops));
+
+	if (err)
+		return err;
+
+	genlmsg_skb = genlmsg_new(0, GFP_KERNEL);
+	if (!genlmsg_skb) {
+		genl_unregister_family(&genl_exec_family);
+		return -ENOMEM;
+	}
+	return 0;
+}
+
+void genl_exec_exit(void)
+{
+	kfree_skb(genlmsg_skb);
+	genl_unregister_family(&genl_exec_family);
+}
+
+/* genl_lock() is not exported from older kernel.
+ * Following function allows any function to be executed with
+ * genl_mutex held. */
+
+int genl_exec(genl_exec_func_t func, void *data)
+{
+	int ret;
+
+	mutex_lock(&genl_exec_lock);
+
+	init_completion(&done);
+	skb_get(genlmsg_skb);
+	genlmsg_put(genlmsg_skb, 0, 0, &genl_exec_family,
+		    NLM_F_REQUEST, GENL_EXEC_RUN);
+
+	genl_exec_function = func;
+	genl_exec_data = data;
+
+	/* There is no need to send msg to current namespace. */
+	ret = genlmsg_unicast(&init_net, genlmsg_skb, 0);
+
+	if (!ret) {
+		wait_for_completion(&done);
+		ret = genl_exec_function_ret;
+	} else {
+		pr_err("genl_exec send error %d\n", ret);
+	}
+
+	/* Wait for genetlink to kfree skb. */
+	while (skb_shared(genlmsg_skb))
+		cpu_relax();
+
+	genlmsg_skb->data = genlmsg_skb->head;
+	skb_reset_tail_pointer(genlmsg_skb);
+
+	mutex_unlock(&genl_exec_lock);
+
+	return ret;
+}
+
+#else
+
+int genl_exec(genl_exec_func_t func, void *data)
+{
+	int ret;
+
+	genl_lock();
+	ret = func(data);
+	genl_unlock();
+	return ret;
+}
+
+int genl_exec_init(void)
+{
+	return 0;
+}
+
+void genl_exec_exit(void)
+{
+}
+#endif
diff --git a/drivers/staging/openvswitch_nv/genl_exec.h b/drivers/staging/openvswitch_nv/genl_exec.h
new file mode 100644
index 000000000000..834de8e055c7
--- /dev/null
+++ b/drivers/staging/openvswitch_nv/genl_exec.h
@@ -0,0 +1,27 @@
+/*
+ * Copyright (c) 2007-2012 Nicira, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#ifndef GENL_EXEC_H
+#define GENL_EXEC_H 1
+
+typedef int (*genl_exec_func_t)(void *data);
+int genl_exec(genl_exec_func_t func, void *data);
+int genl_exec_init(void);
+void genl_exec_exit(void);
+
+#endif /* genl_exec.h */
diff --git a/drivers/staging/openvswitch_nv/tunnel.c b/drivers/staging/openvswitch_nv/tunnel.c
new file mode 100644
index 000000000000..d1b28718a28d
--- /dev/null
+++ b/drivers/staging/openvswitch_nv/tunnel.c
@@ -0,0 +1,1339 @@
+/*
+ * Copyright (c) 2007-2012 Nicira, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/if_arp.h>
+#include <linux/if_ether.h>
+#include <linux/ip.h>
+#include <linux/if_vlan.h>
+#include <linux/igmp.h>
+#include <linux/in.h>
+#include <linux/in_route.h>
+#include <linux/inetdevice.h>
+#include <linux/jhash.h>
+#include <linux/list.h>
+#include <linux/kernel.h>
+#include <linux/version.h>
+#include <linux/workqueue.h>
+#include <linux/rculist.h>
+
+#include <net/dsfield.h>
+#include <net/dst.h>
+#include <net/icmp.h>
+#include <net/inet_ecn.h>
+#include <net/ip.h>
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+#include <net/ipv6.h>
+#endif
+#include <net/route.h>
+#include <net/xfrm.h>
+
+#include "checksum.h"
+#include "datapath.h"
+#include "tunnel.h"
+#include "vlan.h"
+#include "vport.h"
+#include "vport-generic.h"
+#include "vport-internal_dev.h"
+
+#define PORT_TABLE_SIZE  1024
+
+static struct hlist_head *port_table __read_mostly;
+
+/*
+ * These are just used as an optimization: they don't require any kind of
+ * synchronization because we could have just as easily read the value before
+ * the port change happened.
+ */
+static unsigned int key_local_remote_ports __read_mostly;
+static unsigned int key_remote_ports __read_mostly;
+static unsigned int key_multicast_ports __read_mostly;
+static unsigned int local_remote_ports __read_mostly;
+static unsigned int remote_ports __read_mostly;
+static unsigned int null_ports __read_mostly;
+static unsigned int multicast_ports __read_mostly;
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,36)
+#define rt_dst(rt) (rt->dst)
+#else
+#define rt_dst(rt) (rt->u.dst)
+#endif
+
+static struct vport *tnl_vport_to_vport(const struct tnl_vport *tnl_vport)
+{
+	return vport_from_priv(tnl_vport);
+}
+
+static void free_config_rcu(struct rcu_head *rcu)
+{
+	struct tnl_mutable_config *c = container_of(rcu, struct tnl_mutable_config, rcu);
+	kfree(c);
+}
+
+/* Frees the portion of 'mutable' that requires RTNL and thus can't happen
+ * within an RCU callback.  Fortunately this part doesn't require waiting for
+ * an RCU grace period.
+ */
+static void free_mutable_rtnl(struct tnl_mutable_config *mutable)
+{
+	ASSERT_RTNL();
+	if (ipv4_is_multicast(mutable->key.daddr) && mutable->mlink) {
+		struct in_device *in_dev;
+		in_dev = inetdev_by_index(port_key_get_net(&mutable->key), mutable->mlink);
+		if (in_dev)
+			ip_mc_dec_group(in_dev, mutable->key.daddr);
+	}
+}
+
+static void assign_config_rcu(struct vport *vport,
+			      struct tnl_mutable_config *new_config)
+{
+	struct tnl_vport *tnl_vport = tnl_vport_priv(vport);
+	struct tnl_mutable_config *old_config;
+
+	old_config = rtnl_dereference(tnl_vport->mutable);
+	rcu_assign_pointer(tnl_vport->mutable, new_config);
+
+	free_mutable_rtnl(old_config);
+	call_rcu(&old_config->rcu, free_config_rcu);
+}
+
+static unsigned int *find_port_pool(const struct tnl_mutable_config *mutable)
+{
+	bool is_multicast = ipv4_is_multicast(mutable->key.daddr);
+
+	if (mutable->flags & TNL_F_IN_KEY_MATCH) {
+		if (mutable->key.saddr)
+			return &local_remote_ports;
+		else if (is_multicast)
+			return &multicast_ports;
+		else
+			return &remote_ports;
+	} else {
+		if (mutable->key.saddr)
+			return &key_local_remote_ports;
+		else if (is_multicast)
+			return &key_multicast_ports;
+		else if (mutable->key.daddr)
+			return &key_remote_ports;
+		else
+			return &null_ports;
+	}
+}
+
+static u32 port_hash(const struct port_lookup_key *key)
+{
+	return jhash2((u32 *)key, (PORT_KEY_LEN / sizeof(u32)), 0);
+}
+
+static struct hlist_head *find_bucket(u32 hash)
+{
+	return &port_table[(hash & (PORT_TABLE_SIZE - 1))];
+}
+
+static void port_table_add_port(struct vport *vport)
+{
+	struct tnl_vport *tnl_vport = tnl_vport_priv(vport);
+	const struct tnl_mutable_config *mutable;
+	u32 hash;
+
+	mutable = rtnl_dereference(tnl_vport->mutable);
+	hash = port_hash(&mutable->key);
+	hlist_add_head_rcu(&tnl_vport->hash_node, find_bucket(hash));
+
+	(*find_port_pool(rtnl_dereference(tnl_vport->mutable)))++;
+}
+
+static void port_table_move_port(struct vport *vport,
+		      struct tnl_mutable_config *new_mutable)
+{
+	struct tnl_vport *tnl_vport = tnl_vport_priv(vport);
+	u32 hash;
+
+	hash = port_hash(&new_mutable->key);
+	hlist_del_init_rcu(&tnl_vport->hash_node);
+	hlist_add_head_rcu(&tnl_vport->hash_node, find_bucket(hash));
+
+	(*find_port_pool(rtnl_dereference(tnl_vport->mutable)))--;
+	assign_config_rcu(vport, new_mutable);
+	(*find_port_pool(rtnl_dereference(tnl_vport->mutable)))++;
+}
+
+static void port_table_remove_port(struct vport *vport)
+{
+	struct tnl_vport *tnl_vport = tnl_vport_priv(vport);
+
+	hlist_del_init_rcu(&tnl_vport->hash_node);
+
+	(*find_port_pool(rtnl_dereference(tnl_vport->mutable)))--;
+}
+
+static struct vport *port_table_lookup(struct port_lookup_key *key,
+				       const struct tnl_mutable_config **pmutable)
+{
+	struct hlist_node *n;
+	struct hlist_head *bucket;
+	u32 hash = port_hash(key);
+	struct tnl_vport *tnl_vport;
+
+	bucket = find_bucket(hash);
+
+	hlist_for_each_entry_rcu(tnl_vport, n, bucket, hash_node) {
+		struct tnl_mutable_config *mutable;
+
+		mutable = rcu_dereference_rtnl(tnl_vport->mutable);
+		if (!memcmp(&mutable->key, key, PORT_KEY_LEN)) {
+			*pmutable = mutable;
+			return tnl_vport_to_vport(tnl_vport);
+		}
+	}
+
+	return NULL;
+}
+
+struct vport *ovs_tnl_find_port(struct net *net, __be32 saddr, __be32 daddr,
+				__be64 key, int tunnel_type,
+				const struct tnl_mutable_config **mutable)
+{
+	struct port_lookup_key lookup;
+	struct vport *vport;
+	bool is_multicast = ipv4_is_multicast(saddr);
+
+	port_key_set_net(&lookup, net);
+	lookup.saddr = saddr;
+	lookup.daddr = daddr;
+
+	/* First try for exact match on in_key. */
+	lookup.in_key = key;
+	lookup.tunnel_type = tunnel_type | TNL_T_KEY_EXACT;
+	if (!is_multicast && key_local_remote_ports) {
+		vport = port_table_lookup(&lookup, mutable);
+		if (vport)
+			return vport;
+	}
+	if (key_remote_ports) {
+		lookup.saddr = 0;
+		vport = port_table_lookup(&lookup, mutable);
+		if (vport)
+			return vport;
+
+		lookup.saddr = saddr;
+	}
+
+	/* Then try matches that wildcard in_key. */
+	lookup.in_key = 0;
+	lookup.tunnel_type = tunnel_type | TNL_T_KEY_MATCH;
+	if (!is_multicast && local_remote_ports) {
+		vport = port_table_lookup(&lookup, mutable);
+		if (vport)
+			return vport;
+	}
+	if (remote_ports) {
+		lookup.saddr = 0;
+		vport = port_table_lookup(&lookup, mutable);
+		if (vport)
+			return vport;
+	}
+
+	if (is_multicast) {
+		lookup.saddr = 0;
+		lookup.daddr = saddr;
+		if (key_multicast_ports) {
+			lookup.tunnel_type = tunnel_type | TNL_T_KEY_EXACT;
+			lookup.in_key = key;
+			vport = port_table_lookup(&lookup, mutable);
+			if (vport)
+				return vport;
+		}
+		if (multicast_ports) {
+			lookup.tunnel_type = tunnel_type | TNL_T_KEY_MATCH;
+			lookup.in_key = 0;
+			vport = port_table_lookup(&lookup, mutable);
+			if (vport)
+				return vport;
+		}
+	}
+
+	if (null_ports) {
+		lookup.daddr = 0;
+		lookup.saddr = 0;
+		lookup.in_key = 0;
+		lookup.tunnel_type = tunnel_type;
+		vport = port_table_lookup(&lookup, mutable);
+		if (vport)
+			return vport;
+	}
+	return NULL;
+}
+
+static void ecn_decapsulate(struct sk_buff *skb)
+{
+	if (unlikely(INET_ECN_is_ce(OVS_CB(skb)->tun_key->ipv4_tos))) {
+		__be16 protocol = skb->protocol;
+
+		skb_set_network_header(skb, ETH_HLEN);
+
+		if (protocol == htons(ETH_P_8021Q)) {
+			if (unlikely(!pskb_may_pull(skb, VLAN_ETH_HLEN)))
+				return;
+
+			protocol = vlan_eth_hdr(skb)->h_vlan_encapsulated_proto;
+			skb_set_network_header(skb, VLAN_ETH_HLEN);
+		}
+
+		if (protocol == htons(ETH_P_IP)) {
+			if (unlikely(!pskb_may_pull(skb, skb_network_offset(skb)
+			    + sizeof(struct iphdr))))
+				return;
+
+			IP_ECN_set_ce(ip_hdr(skb));
+		}
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+		else if (protocol == htons(ETH_P_IPV6)) {
+			if (unlikely(!pskb_may_pull(skb, skb_network_offset(skb)
+			    + sizeof(struct ipv6hdr))))
+				return;
+
+			IP6_ECN_set_ce(ipv6_hdr(skb));
+		}
+#endif
+	}
+}
+
+/**
+ *	ovs_tnl_rcv - ingress point for generic tunnel code
+ *
+ * @vport: port this packet was received on
+ * @skb: received packet
+ * @tos: ToS from encapsulating IP packet, used to copy ECN bits
+ *
+ * Must be called with rcu_read_lock.
+ *
+ * Packets received by this function are in the following state:
+ * - skb->data points to the inner Ethernet header.
+ * - The inner Ethernet header is in the linear data area.
+ * - skb->csum does not include the inner Ethernet header.
+ * - The layer pointers are undefined.
+ */
+void ovs_tnl_rcv(struct vport *vport, struct sk_buff *skb)
+{
+	struct ethhdr *eh;
+
+	skb_reset_mac_header(skb);
+	eh = eth_hdr(skb);
+
+	if (likely(ntohs(eh->h_proto) >= 1536))
+		skb->protocol = eh->h_proto;
+	else
+		skb->protocol = htons(ETH_P_802_2);
+
+	skb_dst_drop(skb);
+	nf_reset(skb);
+	skb->rxhash = 0;
+	secpath_reset(skb);
+
+	ecn_decapsulate(skb);
+	vlan_set_tci(skb, 0);
+
+	if (unlikely(compute_ip_summed(skb, false))) {
+		kfree_skb(skb);
+		return;
+	}
+
+	ovs_vport_receive(vport, skb);
+}
+
+static bool check_ipv4_address(__be32 addr)
+{
+	if (ipv4_is_multicast(addr) || ipv4_is_lbcast(addr)
+	    || ipv4_is_loopback(addr) || ipv4_is_zeronet(addr))
+		return false;
+
+	return true;
+}
+
+static bool ipv4_should_icmp(struct sk_buff *skb)
+{
+	struct iphdr *old_iph = ip_hdr(skb);
+
+	/* Don't respond to L2 broadcast. */
+	if (is_multicast_ether_addr(eth_hdr(skb)->h_dest))
+		return false;
+
+	/* Don't respond to L3 broadcast or invalid addresses. */
+	if (!check_ipv4_address(old_iph->daddr) ||
+	    !check_ipv4_address(old_iph->saddr))
+		return false;
+
+	/* Only respond to the first fragment. */
+	if (old_iph->frag_off & htons(IP_OFFSET))
+		return false;
+
+	/* Don't respond to ICMP error messages. */
+	if (old_iph->protocol == IPPROTO_ICMP) {
+		u8 icmp_type, *icmp_typep;
+
+		icmp_typep = skb_header_pointer(skb, (u8 *)old_iph +
+						(old_iph->ihl << 2) +
+						offsetof(struct icmphdr, type) -
+						skb->data, sizeof(icmp_type),
+						&icmp_type);
+
+		if (!icmp_typep)
+			return false;
+
+		if (*icmp_typep > NR_ICMP_TYPES
+			|| (*icmp_typep <= ICMP_PARAMETERPROB
+				&& *icmp_typep != ICMP_ECHOREPLY
+				&& *icmp_typep != ICMP_ECHO))
+			return false;
+	}
+
+	return true;
+}
+
+static void ipv4_build_icmp(struct sk_buff *skb, struct sk_buff *nskb,
+			    unsigned int mtu, unsigned int payload_length)
+{
+	struct iphdr *iph, *old_iph = ip_hdr(skb);
+	struct icmphdr *icmph;
+	u8 *payload;
+
+	iph = (struct iphdr *)skb_put(nskb, sizeof(struct iphdr));
+	icmph = (struct icmphdr *)skb_put(nskb, sizeof(struct icmphdr));
+	payload = skb_put(nskb, payload_length);
+
+	/* IP */
+	iph->version		=	4;
+	iph->ihl		=	sizeof(struct iphdr) >> 2;
+	iph->tos		=	(old_iph->tos & IPTOS_TOS_MASK) |
+					IPTOS_PREC_INTERNETCONTROL;
+	iph->tot_len		=	htons(sizeof(struct iphdr)
+					      + sizeof(struct icmphdr)
+					      + payload_length);
+	get_random_bytes(&iph->id, sizeof(iph->id));
+	iph->frag_off		=	0;
+	iph->ttl		=	IPDEFTTL;
+	iph->protocol		=	IPPROTO_ICMP;
+	iph->daddr		=	old_iph->saddr;
+	iph->saddr		=	old_iph->daddr;
+
+	ip_send_check(iph);
+
+	/* ICMP */
+	icmph->type		=	ICMP_DEST_UNREACH;
+	icmph->code		=	ICMP_FRAG_NEEDED;
+	icmph->un.gateway	=	htonl(mtu);
+	icmph->checksum		=	0;
+
+	nskb->csum = csum_partial((u8 *)icmph, sizeof(struct icmphdr), 0);
+	nskb->csum = skb_copy_and_csum_bits(skb, (u8 *)old_iph - skb->data,
+					    payload, payload_length,
+					    nskb->csum);
+	icmph->checksum = csum_fold(nskb->csum);
+}
+
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+static bool ipv6_should_icmp(struct sk_buff *skb)
+{
+	struct ipv6hdr *old_ipv6h = ipv6_hdr(skb);
+	int addr_type;
+	int payload_off = (u8 *)(old_ipv6h + 1) - skb->data;
+	u8 nexthdr = ipv6_hdr(skb)->nexthdr;
+	__be16 frag_off;
+
+	/* Check source address is valid. */
+	addr_type = ipv6_addr_type(&old_ipv6h->saddr);
+	if (addr_type & IPV6_ADDR_MULTICAST || addr_type == IPV6_ADDR_ANY)
+		return false;
+
+	/* Don't reply to unspecified addresses. */
+	if (ipv6_addr_type(&old_ipv6h->daddr) == IPV6_ADDR_ANY)
+		return false;
+
+	/* Don't respond to ICMP error messages. */
+	payload_off = ipv6_skip_exthdr(skb, payload_off, &nexthdr, &frag_off);
+	if (payload_off < 0)
+		return false;
+
+	if (nexthdr == NEXTHDR_ICMP) {
+		u8 icmp_type, *icmp_typep;
+
+		icmp_typep = skb_header_pointer(skb, payload_off +
+						offsetof(struct icmp6hdr,
+							icmp6_type),
+						sizeof(icmp_type), &icmp_type);
+
+		if (!icmp_typep || !(*icmp_typep & ICMPV6_INFOMSG_MASK))
+			return false;
+	}
+
+	return true;
+}
+
+static void ipv6_build_icmp(struct sk_buff *skb, struct sk_buff *nskb,
+			    unsigned int mtu, unsigned int payload_length)
+{
+	struct ipv6hdr *ipv6h, *old_ipv6h = ipv6_hdr(skb);
+	struct icmp6hdr *icmp6h;
+	u8 *payload;
+
+	ipv6h = (struct ipv6hdr *)skb_put(nskb, sizeof(struct ipv6hdr));
+	icmp6h = (struct icmp6hdr *)skb_put(nskb, sizeof(struct icmp6hdr));
+	payload = skb_put(nskb, payload_length);
+
+	/* IPv6 */
+	ipv6h->version		=	6;
+	ipv6h->priority		=	0;
+	memset(&ipv6h->flow_lbl, 0, sizeof(ipv6h->flow_lbl));
+	ipv6h->payload_len	=	htons(sizeof(struct icmp6hdr)
+					      + payload_length);
+	ipv6h->nexthdr		=	NEXTHDR_ICMP;
+	ipv6h->hop_limit	=	IPV6_DEFAULT_HOPLIMIT;
+	ipv6h->daddr		=	old_ipv6h->saddr;
+	ipv6h->saddr		=	old_ipv6h->daddr;
+
+	/* ICMPv6 */
+	icmp6h->icmp6_type	=	ICMPV6_PKT_TOOBIG;
+	icmp6h->icmp6_code	=	0;
+	icmp6h->icmp6_cksum	=	0;
+	icmp6h->icmp6_mtu	=	htonl(mtu);
+
+	nskb->csum = csum_partial((u8 *)icmp6h, sizeof(struct icmp6hdr), 0);
+	nskb->csum = skb_copy_and_csum_bits(skb, (u8 *)old_ipv6h - skb->data,
+					    payload, payload_length,
+					    nskb->csum);
+	icmp6h->icmp6_cksum = csum_ipv6_magic(&ipv6h->saddr, &ipv6h->daddr,
+						sizeof(struct icmp6hdr)
+						+ payload_length,
+						ipv6h->nexthdr, nskb->csum);
+}
+#endif /* IPv6 */
+
+bool ovs_tnl_frag_needed(struct vport *vport,
+			 const struct tnl_mutable_config *mutable,
+			 struct sk_buff *skb, unsigned int mtu)
+{
+	unsigned int eth_hdr_len = ETH_HLEN;
+	unsigned int total_length = 0, header_length = 0, payload_length;
+	struct ethhdr *eh, *old_eh = eth_hdr(skb);
+	struct sk_buff *nskb;
+
+	/* Sanity check */
+	if (skb->protocol == htons(ETH_P_IP)) {
+		if (mtu < IP_MIN_MTU)
+			return false;
+
+		if (!ipv4_should_icmp(skb))
+			return true;
+	}
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+	else if (skb->protocol == htons(ETH_P_IPV6)) {
+		if (mtu < IPV6_MIN_MTU)
+			return false;
+
+		/*
+		 * In theory we should do PMTUD on IPv6 multicast messages but
+		 * we don't have an address to send from so just fragment.
+		 */
+		if (ipv6_addr_type(&ipv6_hdr(skb)->daddr) & IPV6_ADDR_MULTICAST)
+			return false;
+
+		if (!ipv6_should_icmp(skb))
+			return true;
+	}
+#endif
+	else
+		return false;
+
+	/* Allocate */
+	if (old_eh->h_proto == htons(ETH_P_8021Q))
+		eth_hdr_len = VLAN_ETH_HLEN;
+
+	payload_length = skb->len - eth_hdr_len;
+	if (skb->protocol == htons(ETH_P_IP)) {
+		header_length = sizeof(struct iphdr) + sizeof(struct icmphdr);
+		total_length = min_t(unsigned int, header_length +
+						   payload_length, 576);
+	}
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+	else {
+		header_length = sizeof(struct ipv6hdr) +
+				sizeof(struct icmp6hdr);
+		total_length = min_t(unsigned int, header_length +
+						  payload_length, IPV6_MIN_MTU);
+	}
+#endif
+
+	payload_length = total_length - header_length;
+
+	nskb = dev_alloc_skb(NET_IP_ALIGN + eth_hdr_len + header_length +
+			     payload_length);
+	if (!nskb)
+		return false;
+
+	skb_reserve(nskb, NET_IP_ALIGN);
+
+	/* Ethernet / VLAN */
+	eh = (struct ethhdr *)skb_put(nskb, eth_hdr_len);
+	memcpy(eh->h_dest, old_eh->h_source, ETH_ALEN);
+	memcpy(eh->h_source, mutable->eth_addr, ETH_ALEN);
+	nskb->protocol = eh->h_proto = old_eh->h_proto;
+	if (old_eh->h_proto == htons(ETH_P_8021Q)) {
+		struct vlan_ethhdr *vh = (struct vlan_ethhdr *)eh;
+
+		vh->h_vlan_TCI = vlan_eth_hdr(skb)->h_vlan_TCI;
+		vh->h_vlan_encapsulated_proto = skb->protocol;
+	} else
+		vlan_set_tci(nskb, vlan_get_tci(skb));
+	skb_reset_mac_header(nskb);
+
+	/* Protocol */
+	if (skb->protocol == htons(ETH_P_IP))
+		ipv4_build_icmp(skb, nskb, mtu, payload_length);
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+	else
+		ipv6_build_icmp(skb, nskb, mtu, payload_length);
+#endif
+
+	if (unlikely(compute_ip_summed(nskb, false))) {
+		kfree_skb(nskb);
+		return false;
+	}
+
+	ovs_vport_receive(vport, nskb);
+
+	return true;
+}
+
+static bool check_mtu(struct sk_buff *skb,
+		      struct vport *vport,
+		      const struct tnl_mutable_config *mutable,
+		      const struct rtable *rt, __be16 *frag_offp,
+		      int tunnel_hlen)
+{
+	bool df_inherit;
+	bool pmtud;
+	__be16 frag_off;
+	int mtu = 0;
+	unsigned int packet_length = skb->len - ETH_HLEN;
+
+	if (OVS_CB(skb)->tun_key->ipv4_dst) {
+		df_inherit = false;
+		pmtud = false;
+		frag_off = OVS_CB(skb)->tun_key->tun_flags & OVS_TNL_F_DONT_FRAGMENT ?
+				  htons(IP_DF) : 0;
+	} else {
+		df_inherit = mutable->flags & TNL_F_DF_INHERIT;
+		pmtud = mutable->flags & TNL_F_PMTUD;
+		frag_off = mutable->flags & TNL_F_DF_DEFAULT ? htons(IP_DF) : 0;
+	}
+
+	/* Allow for one level of tagging in the packet length. */
+	if (!vlan_tx_tag_present(skb) &&
+	    eth_hdr(skb)->h_proto == htons(ETH_P_8021Q))
+		packet_length -= VLAN_HLEN;
+
+	if (pmtud) {
+		int vlan_header = 0;
+
+		/* The tag needs to go in packet regardless of where it
+		 * currently is, so subtract it from the MTU.
+		 */
+		if (vlan_tx_tag_present(skb) ||
+		    eth_hdr(skb)->h_proto == htons(ETH_P_8021Q))
+			vlan_header = VLAN_HLEN;
+
+		mtu = dst_mtu(&rt_dst(rt))
+			- ETH_HLEN
+			- tunnel_hlen
+			- vlan_header;
+	}
+
+	if (skb->protocol == htons(ETH_P_IP)) {
+		struct iphdr *iph = ip_hdr(skb);
+
+		if (df_inherit)
+			frag_off = iph->frag_off & htons(IP_DF);
+
+		if (pmtud && iph->frag_off & htons(IP_DF)) {
+			mtu = max(mtu, IP_MIN_MTU);
+
+			if (packet_length > mtu &&
+			    ovs_tnl_frag_needed(vport, mutable, skb, mtu))
+				return false;
+		}
+	}
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+	else if (skb->protocol == htons(ETH_P_IPV6)) {
+		/* IPv6 requires end hosts to do fragmentation
+		 * if the packet is above the minimum MTU.
+		 */
+		if (df_inherit && packet_length > IPV6_MIN_MTU)
+			frag_off = htons(IP_DF);
+
+		if (pmtud) {
+			mtu = max(mtu, IPV6_MIN_MTU);
+
+			if (packet_length > mtu &&
+			    ovs_tnl_frag_needed(vport, mutable, skb, mtu))
+				return false;
+		}
+	}
+#endif
+
+	*frag_offp = frag_off;
+	return true;
+}
+
+static struct rtable *find_route(struct net *net,
+		__be32 *saddr, __be32 daddr, u8 ipproto,
+		u8 tos)
+{
+	struct rtable *rt;
+	/* Tunnel configuration keeps DSCP part of TOS bits, But Linux
+	 * router expect RT_TOS bits only. */
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,39)
+	struct flowi fl = { .nl_u = { .ip4_u = {
+					.daddr = daddr,
+					.saddr = *saddr,
+					.tos   = RT_TOS(tos) } },
+					.proto = ipproto };
+
+	if (unlikely(ip_route_output_key(net, &rt, &fl)))
+		return ERR_PTR(-EADDRNOTAVAIL);
+	*saddr = fl.nl_u.ip4_u.saddr;
+	return rt;
+#else
+	struct flowi4 fl = { .daddr = daddr,
+			     .saddr = *saddr,
+			     .flowi4_tos = RT_TOS(tos),
+			     .flowi4_proto = ipproto };
+
+	rt = ip_route_output_key(net, &fl);
+	*saddr = fl.saddr;
+	return rt;
+#endif
+}
+
+static bool need_linearize(const struct sk_buff *skb)
+{
+	int i;
+
+	if (unlikely(skb_shinfo(skb)->frag_list))
+		return true;
+
+	/*
+	 * Generally speaking we should linearize if there are paged frags.
+	 * However, if all of the refcounts are 1 we know nobody else can
+	 * change them from underneath us and we can skip the linearization.
+	 */
+	for (i = 0; i < skb_shinfo(skb)->nr_frags; i++)
+		if (unlikely(page_count(skb_frag_page(&skb_shinfo(skb)->frags[i])) > 1))
+			return true;
+
+	return false;
+}
+
+static struct sk_buff *handle_offloads(struct sk_buff *skb,
+				       const struct tnl_mutable_config *mutable,
+				       const struct rtable *rt,
+				       int tunnel_hlen)
+{
+	int min_headroom;
+	int err;
+
+	min_headroom = LL_RESERVED_SPACE(rt_dst(rt).dev) + rt_dst(rt).header_len
+			+ tunnel_hlen
+			+ (vlan_tx_tag_present(skb) ? VLAN_HLEN : 0);
+
+	if (skb_headroom(skb) < min_headroom || skb_header_cloned(skb)) {
+		int head_delta = SKB_DATA_ALIGN(min_headroom -
+						skb_headroom(skb) +
+						16);
+		err = pskb_expand_head(skb, max_t(int, head_delta, 0),
+					0, GFP_ATOMIC);
+		if (unlikely(err))
+			goto error_free;
+	}
+
+	forward_ip_summed(skb, true);
+
+	if (skb_is_gso(skb)) {
+		struct sk_buff *nskb;
+
+		nskb = skb_gso_segment(skb, 0);
+		if (IS_ERR(nskb)) {
+			kfree_skb(skb);
+			err = PTR_ERR(nskb);
+			goto error;
+		}
+
+		consume_skb(skb);
+		skb = nskb;
+	} else if (get_ip_summed(skb) == OVS_CSUM_PARTIAL) {
+		/* Pages aren't locked and could change at any time.
+		 * If this happens after we compute the checksum, the
+		 * checksum will be wrong.  We linearize now to avoid
+		 * this problem.
+		 */
+		if (unlikely(need_linearize(skb))) {
+			err = __skb_linearize(skb);
+			if (unlikely(err))
+				goto error_free;
+		}
+
+		err = skb_checksum_help(skb);
+		if (unlikely(err))
+			goto error_free;
+	}
+
+	set_ip_summed(skb, OVS_CSUM_NONE);
+
+	return skb;
+
+error_free:
+	kfree_skb(skb);
+error:
+	return ERR_PTR(err);
+}
+
+static int send_frags(struct sk_buff *skb,
+		      int tunnel_hlen)
+{
+	int sent_len;
+
+	sent_len = 0;
+	while (skb) {
+		struct sk_buff *next = skb->next;
+		int frag_len = skb->len - tunnel_hlen;
+		int err;
+
+		skb->next = NULL;
+		memset(IPCB(skb), 0, sizeof(*IPCB(skb)));
+
+		err = ip_local_out(skb);
+		skb = next;
+		if (unlikely(net_xmit_eval(err)))
+			goto free_frags;
+		sent_len += frag_len;
+	}
+
+	return sent_len;
+
+free_frags:
+	/*
+	 * There's no point in continuing to send fragments once one has been
+	 * dropped so just free the rest.  This may help improve the congestion
+	 * that caused the first packet to be dropped.
+	 */
+	ovs_tnl_free_linked_skbs(skb);
+	return sent_len;
+}
+
+int ovs_tnl_send(struct vport *vport, struct sk_buff *skb)
+{
+	struct tnl_vport *tnl_vport = tnl_vport_priv(vport);
+	const struct tnl_mutable_config *mutable = rcu_dereference(tnl_vport->mutable);
+	enum vport_err_type err = VPORT_E_TX_ERROR;
+	struct rtable *rt;
+	struct ovs_key_ipv4_tunnel tun_key;
+	int sent_len = 0;
+	int tunnel_hlen;
+	__be16 frag_off = 0;
+	__be32 daddr;
+	__be32 saddr;
+	u8 ttl;
+	u8 tos;
+
+	/* Validate the protocol headers before we try to use them. */
+	if (skb->protocol == htons(ETH_P_8021Q) &&
+	    !vlan_tx_tag_present(skb)) {
+		if (unlikely(!pskb_may_pull(skb, VLAN_ETH_HLEN)))
+			goto error_free;
+
+		skb->protocol = vlan_eth_hdr(skb)->h_vlan_encapsulated_proto;
+		skb_set_network_header(skb, VLAN_ETH_HLEN);
+	}
+
+	if (skb->protocol == htons(ETH_P_IP)) {
+		if (unlikely(!pskb_may_pull(skb, skb_network_offset(skb)
+		    + sizeof(struct iphdr))))
+			skb->protocol = 0;
+	}
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+	else if (skb->protocol == htons(ETH_P_IPV6)) {
+		if (unlikely(!pskb_may_pull(skb, skb_network_offset(skb)
+		    + sizeof(struct ipv6hdr))))
+			skb->protocol = 0;
+	}
+#endif
+
+	/* If OVS_CB(skb)->tun_key is NULL, point it at the local tun_key here,
+	 * and zero it out.
+	 */
+	if (!OVS_CB(skb)->tun_key) {
+		memset(&tun_key, 0, sizeof(tun_key));
+		OVS_CB(skb)->tun_key = &tun_key;
+	}
+
+	tunnel_hlen = tnl_vport->tnl_ops->hdr_len(mutable, OVS_CB(skb)->tun_key);
+	if (unlikely(tunnel_hlen < 0)) {
+		err = VPORT_E_TX_DROPPED;
+		goto error_free;
+	}
+	tunnel_hlen += sizeof(struct iphdr);
+
+	if (OVS_CB(skb)->tun_key->ipv4_dst) {
+		daddr = OVS_CB(skb)->tun_key->ipv4_dst;
+		saddr = OVS_CB(skb)->tun_key->ipv4_src;
+		tos = OVS_CB(skb)->tun_key->ipv4_tos;
+		ttl = OVS_CB(skb)->tun_key->ipv4_ttl;
+	} else {
+		u8 inner_tos;
+		daddr = mutable->key.daddr;
+		saddr = mutable->key.saddr;
+
+		if (unlikely(!daddr)) {
+			/* Trying to sent packet from Null-port without
+			 * tunnel info? Drop this packet. */
+			err = VPORT_E_TX_DROPPED;
+			goto error_free;
+		}
+
+		/* ToS */
+		if (skb->protocol == htons(ETH_P_IP))
+			inner_tos = ip_hdr(skb)->tos;
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+		else if (skb->protocol == htons(ETH_P_IPV6))
+			inner_tos = ipv6_get_dsfield(ipv6_hdr(skb));
+#endif
+		else
+			inner_tos = 0;
+
+		if (mutable->flags & TNL_F_TOS_INHERIT)
+			tos = inner_tos;
+		else
+			tos = mutable->tos;
+
+		tos = INET_ECN_encapsulate(tos, inner_tos);
+
+		/* TTL */
+		ttl = mutable->ttl;
+		if (mutable->flags & TNL_F_TTL_INHERIT) {
+			if (skb->protocol == htons(ETH_P_IP))
+				ttl = ip_hdr(skb)->ttl;
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+			else if (skb->protocol == htons(ETH_P_IPV6))
+				ttl = ipv6_hdr(skb)->hop_limit;
+#endif
+		}
+
+	}
+
+	/* Route lookup */
+	rt = find_route(port_key_get_net(&mutable->key), &saddr, daddr,
+			  tnl_vport->tnl_ops->ipproto, tos);
+	if (IS_ERR(rt))
+		goto error_free;
+
+	/* Reset SKB */
+	nf_reset(skb);
+	secpath_reset(skb);
+	skb_dst_drop(skb);
+	skb->rxhash = 0;
+
+	/* Offloading */
+	skb = handle_offloads(skb, mutable, rt, tunnel_hlen);
+	if (IS_ERR(skb)) {
+		skb = NULL;
+		goto err_free_rt;
+	}
+
+	/* MTU */
+	if (unlikely(!check_mtu(skb, vport, mutable, rt, &frag_off, tunnel_hlen))) {
+		err = VPORT_E_TX_DROPPED;
+		goto err_free_rt;
+	}
+
+	/* TTL Fixup. */
+	if (!OVS_CB(skb)->tun_key->ipv4_dst) {
+		if (!(mutable->flags & TNL_F_TTL_INHERIT)) {
+			if (!ttl)
+				ttl = ip4_dst_hoplimit(&rt_dst(rt));
+		}
+	}
+
+	while (skb) {
+		struct iphdr *iph;
+		struct sk_buff *next_skb = skb->next;
+		skb->next = NULL;
+
+		if (unlikely(vlan_deaccel_tag(skb)))
+			goto next;
+
+		skb_push(skb, tunnel_hlen);
+		skb_reset_network_header(skb);
+		skb_set_transport_header(skb, sizeof(struct iphdr));
+
+		if (next_skb)
+			skb_dst_set(skb, dst_clone(&rt_dst(rt)));
+		else
+			skb_dst_set(skb, &rt_dst(rt));
+
+		/* Push IP header. */
+		iph = ip_hdr(skb);
+		iph->version	= 4;
+		iph->ihl	= sizeof(struct iphdr) >> 2;
+		iph->protocol	= tnl_vport->tnl_ops->ipproto;
+		iph->daddr	= daddr;
+		iph->saddr	= saddr;
+		iph->tos	= tos;
+		iph->ttl	= ttl;
+		iph->frag_off	= frag_off;
+		ip_select_ident(skb, &rt_dst(rt), NULL);
+
+		/* Push Tunnel header. */
+		skb = tnl_vport->tnl_ops->build_header(vport, mutable,
+							&rt_dst(rt), skb, tunnel_hlen);
+		if (unlikely(!skb))
+			goto next;
+
+		sent_len += send_frags(skb, tunnel_hlen);
+
+next:
+		skb = next_skb;
+	}
+
+	if (unlikely(sent_len == 0))
+		ovs_vport_record_error(vport, VPORT_E_TX_DROPPED);
+
+	return sent_len;
+
+err_free_rt:
+	ip_rt_put(rt);
+error_free:
+	ovs_tnl_free_linked_skbs(skb);
+	ovs_vport_record_error(vport, err);
+	return sent_len;
+}
+
+static const struct nla_policy tnl_policy[OVS_TUNNEL_ATTR_MAX + 1] = {
+	[OVS_TUNNEL_ATTR_FLAGS]    = { .type = NLA_U32 },
+	[OVS_TUNNEL_ATTR_DST_IPV4] = { .type = NLA_U32 },
+	[OVS_TUNNEL_ATTR_SRC_IPV4] = { .type = NLA_U32 },
+	[OVS_TUNNEL_ATTR_OUT_KEY]  = { .type = NLA_U64 },
+	[OVS_TUNNEL_ATTR_IN_KEY]   = { .type = NLA_U64 },
+	[OVS_TUNNEL_ATTR_TOS]      = { .type = NLA_U8 },
+	[OVS_TUNNEL_ATTR_TTL]      = { .type = NLA_U8 },
+};
+
+/* Sets OVS_TUNNEL_ATTR_* fields in 'mutable', which must initially be
+ * zeroed. */
+static int tnl_set_config(struct net *net, struct nlattr *options,
+			  const struct tnl_ops *tnl_ops,
+			  const struct vport *cur_vport,
+			  struct tnl_mutable_config *mutable)
+{
+	const struct vport *old_vport;
+	const struct tnl_mutable_config *old_mutable;
+	struct nlattr *a[OVS_TUNNEL_ATTR_MAX + 1];
+	int err;
+
+	port_key_set_net(&mutable->key, net);
+	mutable->key.tunnel_type = tnl_ops->tunnel_type;
+	if (!options)
+		goto out;
+
+	err = nla_parse_nested(a, OVS_TUNNEL_ATTR_MAX, options, tnl_policy);
+	if (err)
+		return err;
+
+	if (a[OVS_TUNNEL_ATTR_DST_IPV4])
+		mutable->key.daddr = nla_get_be32(a[OVS_TUNNEL_ATTR_DST_IPV4]);
+
+	/* Skip the rest if configuring a null_port */
+	if (!mutable->key.daddr)
+		goto out;
+
+	if (a[OVS_TUNNEL_ATTR_FLAGS])
+		mutable->flags = nla_get_u32(a[OVS_TUNNEL_ATTR_FLAGS])
+			& TNL_F_PUBLIC;
+
+	if (a[OVS_TUNNEL_ATTR_SRC_IPV4]) {
+		if (ipv4_is_multicast(mutable->key.daddr))
+			return -EINVAL;
+		mutable->key.saddr = nla_get_be32(a[OVS_TUNNEL_ATTR_SRC_IPV4]);
+	}
+
+	if (a[OVS_TUNNEL_ATTR_TOS]) {
+		mutable->tos = nla_get_u8(a[OVS_TUNNEL_ATTR_TOS]);
+		/* Reject ToS config with ECN bits set. */
+		if (mutable->tos & INET_ECN_MASK)
+			return -EINVAL;
+	}
+
+	if (a[OVS_TUNNEL_ATTR_TTL])
+		mutable->ttl = nla_get_u8(a[OVS_TUNNEL_ATTR_TTL]);
+
+	if (!a[OVS_TUNNEL_ATTR_IN_KEY]) {
+		mutable->key.tunnel_type |= TNL_T_KEY_MATCH;
+		mutable->flags |= TNL_F_IN_KEY_MATCH;
+	} else {
+		mutable->key.tunnel_type |= TNL_T_KEY_EXACT;
+		mutable->key.in_key = nla_get_be64(a[OVS_TUNNEL_ATTR_IN_KEY]);
+	}
+
+	if (!a[OVS_TUNNEL_ATTR_OUT_KEY])
+		mutable->flags |= TNL_F_OUT_KEY_ACTION;
+	else
+		mutable->out_key = nla_get_be64(a[OVS_TUNNEL_ATTR_OUT_KEY]);
+
+	mutable->mlink = 0;
+	if (ipv4_is_multicast(mutable->key.daddr)) {
+		struct net_device *dev;
+		struct rtable *rt;
+		__be32 saddr = mutable->key.saddr;
+
+		rt = find_route(port_key_get_net(&mutable->key),
+			     &saddr, mutable->key.daddr,
+			     tnl_ops->ipproto, mutable->tos);
+		if (IS_ERR(rt))
+			return -EADDRNOTAVAIL;
+		dev = rt_dst(rt).dev;
+		ip_rt_put(rt);
+		if (__in_dev_get_rtnl(dev) == NULL)
+			return -EADDRNOTAVAIL;
+		mutable->mlink = dev->ifindex;
+		ip_mc_inc_group(__in_dev_get_rtnl(dev), mutable->key.daddr);
+	}
+
+out:
+	old_vport = port_table_lookup(&mutable->key, &old_mutable);
+	if (old_vport && old_vport != cur_vport)
+		return -EEXIST;
+
+	return 0;
+}
+
+struct vport *ovs_tnl_create(const struct vport_parms *parms,
+			     const struct vport_ops *vport_ops,
+			     const struct tnl_ops *tnl_ops)
+{
+	struct vport *vport;
+	struct tnl_vport *tnl_vport;
+	struct tnl_mutable_config *mutable;
+	int initial_frag_id;
+	int err;
+
+	vport = ovs_vport_alloc(sizeof(struct tnl_vport), vport_ops, parms);
+	if (IS_ERR(vport)) {
+		err = PTR_ERR(vport);
+		goto error;
+	}
+
+	tnl_vport = tnl_vport_priv(vport);
+
+	strcpy(tnl_vport->name, parms->name);
+	tnl_vport->tnl_ops = tnl_ops;
+
+	mutable = kzalloc(sizeof(struct tnl_mutable_config), GFP_KERNEL);
+	if (!mutable) {
+		err = -ENOMEM;
+		goto error_free_vport;
+	}
+
+	random_ether_addr(mutable->eth_addr);
+
+	get_random_bytes(&initial_frag_id, sizeof(int));
+	atomic_set(&tnl_vport->frag_id, initial_frag_id);
+
+	err = tnl_set_config(ovs_dp_get_net(parms->dp), parms->options, tnl_ops,
+			     NULL, mutable);
+	if (err)
+		goto error_free_mutable;
+
+	rcu_assign_pointer(tnl_vport->mutable, mutable);
+
+	port_table_add_port(vport);
+	return vport;
+
+error_free_mutable:
+	free_mutable_rtnl(mutable);
+	kfree(mutable);
+error_free_vport:
+	ovs_vport_free(vport);
+error:
+	return ERR_PTR(err);
+}
+
+int ovs_tnl_set_options(struct vport *vport, struct nlattr *options)
+{
+	struct tnl_vport *tnl_vport = tnl_vport_priv(vport);
+	const struct tnl_mutable_config *old_mutable;
+	struct tnl_mutable_config *mutable;
+	int err;
+
+	old_mutable = rtnl_dereference(tnl_vport->mutable);
+	if (!old_mutable->key.daddr)
+		return -EINVAL;
+
+	mutable = kzalloc(sizeof(struct tnl_mutable_config), GFP_KERNEL);
+	if (!mutable) {
+		err = -ENOMEM;
+		goto error;
+	}
+
+	/* Copy fields whose values should be retained. */
+	mutable->seq = old_mutable->seq + 1;
+	memcpy(mutable->eth_addr, old_mutable->eth_addr, ETH_ALEN);
+
+	/* Parse the others configured by userspace. */
+	err = tnl_set_config(ovs_dp_get_net(vport->dp), options, tnl_vport->tnl_ops,
+			     vport, mutable);
+	if (err)
+		goto error_free;
+
+	if (port_hash(&mutable->key) != port_hash(&old_mutable->key))
+		port_table_move_port(vport, mutable);
+	else
+		assign_config_rcu(vport, mutable);
+
+	return 0;
+
+error_free:
+	free_mutable_rtnl(mutable);
+	kfree(mutable);
+error:
+	return err;
+}
+
+int ovs_tnl_get_options(const struct vport *vport, struct sk_buff *skb)
+{
+	const struct tnl_vport *tnl_vport = tnl_vport_priv(vport);
+	const struct tnl_mutable_config *mutable = rcu_dereference_rtnl(tnl_vport->mutable);
+
+	/* Skip the rest for null_ports */
+	if (!mutable->key.daddr)
+		return 0;
+
+	if (nla_put_be32(skb, OVS_TUNNEL_ATTR_DST_IPV4, mutable->key.daddr))
+		goto nla_put_failure;
+	if (nla_put_u32(skb, OVS_TUNNEL_ATTR_FLAGS,
+			mutable->flags & TNL_F_PUBLIC))
+		goto nla_put_failure;
+	if (!(mutable->flags & TNL_F_IN_KEY_MATCH) &&
+	    nla_put_be64(skb, OVS_TUNNEL_ATTR_IN_KEY, mutable->key.in_key))
+		goto nla_put_failure;
+	if (!(mutable->flags & TNL_F_OUT_KEY_ACTION) &&
+	    nla_put_be64(skb, OVS_TUNNEL_ATTR_OUT_KEY, mutable->out_key))
+		goto nla_put_failure;
+	if (mutable->key.saddr &&
+	    nla_put_be32(skb, OVS_TUNNEL_ATTR_SRC_IPV4, mutable->key.saddr))
+		goto nla_put_failure;
+	if (mutable->tos && nla_put_u8(skb, OVS_TUNNEL_ATTR_TOS, mutable->tos))
+		goto nla_put_failure;
+	if (mutable->ttl && nla_put_u8(skb, OVS_TUNNEL_ATTR_TTL, mutable->ttl))
+		goto nla_put_failure;
+
+	return 0;
+
+nla_put_failure:
+	return -EMSGSIZE;
+}
+
+static void free_port_rcu(struct rcu_head *rcu)
+{
+	struct tnl_vport *tnl_vport = container_of(rcu,
+						   struct tnl_vport, rcu);
+
+	kfree((struct tnl_mutable __force *)tnl_vport->mutable);
+	ovs_vport_free(tnl_vport_to_vport(tnl_vport));
+}
+
+void ovs_tnl_destroy(struct vport *vport)
+{
+	struct tnl_vport *tnl_vport = tnl_vport_priv(vport);
+	struct tnl_mutable_config *mutable;
+
+	mutable = rtnl_dereference(tnl_vport->mutable);
+	port_table_remove_port(vport);
+	free_mutable_rtnl(mutable);
+	call_rcu(&tnl_vport->rcu, free_port_rcu);
+}
+
+int ovs_tnl_set_addr(struct vport *vport, const unsigned char *addr)
+{
+	struct tnl_vport *tnl_vport = tnl_vport_priv(vport);
+	struct tnl_mutable_config *old_mutable, *mutable;
+
+	old_mutable = rtnl_dereference(tnl_vport->mutable);
+	mutable = kmemdup(old_mutable, sizeof(struct tnl_mutable_config), GFP_KERNEL);
+	if (!mutable)
+		return -ENOMEM;
+
+	old_mutable->mlink = 0;
+
+	memcpy(mutable->eth_addr, addr, ETH_ALEN);
+	assign_config_rcu(vport, mutable);
+
+	return 0;
+}
+
+const char *ovs_tnl_get_name(const struct vport *vport)
+{
+	const struct tnl_vport *tnl_vport = tnl_vport_priv(vport);
+	return tnl_vport->name;
+}
+
+const unsigned char *ovs_tnl_get_addr(const struct vport *vport)
+{
+	const struct tnl_vport *tnl_vport = tnl_vport_priv(vport);
+	return rcu_dereference_rtnl(tnl_vport->mutable)->eth_addr;
+}
+
+void ovs_tnl_free_linked_skbs(struct sk_buff *skb)
+{
+	while (skb) {
+		struct sk_buff *next = skb->next;
+		kfree_skb(skb);
+		skb = next;
+	}
+}
+
+int ovs_tnl_init(void)
+{
+	int i;
+
+	port_table = kmalloc(PORT_TABLE_SIZE * sizeof(struct hlist_head *),
+			     GFP_KERNEL);
+	if (!port_table)
+		return -ENOMEM;
+
+	for (i = 0; i < PORT_TABLE_SIZE; i++)
+		INIT_HLIST_HEAD(&port_table[i]);
+
+	return 0;
+}
+
+void ovs_tnl_exit(void)
+{
+	kfree(port_table);
+}
diff --git a/drivers/staging/openvswitch_nv/tunnel.h b/drivers/staging/openvswitch_nv/tunnel.h
new file mode 100644
index 000000000000..1aa57bc88680
--- /dev/null
+++ b/drivers/staging/openvswitch_nv/tunnel.h
@@ -0,0 +1,229 @@
+/*
+ * Copyright (c) 2007-2012 Nicira, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#ifndef TUNNEL_H
+#define TUNNEL_H 1
+
+#include <linux/version.h>
+#include <net/net_namespace.h>
+#include <net/netns/generic.h>
+
+#include "flow.h"
+#include "openvswitch/tunnel.h"
+#include "vport.h"
+
+/*
+ * The absolute minimum fragment size.  Note that there are many other
+ * definitions of the minimum MTU.
+ */
+#define IP_MIN_MTU 68
+
+/*
+ * One of these goes in struct tnl_ops and in tnl_find_port().
+ * These values are in the same namespace as other TNL_T_* values, so
+ * only the least significant 10 bits are available to define protocol
+ * identifiers.
+ */
+#define TNL_T_PROTO_GRE		0
+#define TNL_T_PROTO_GRE64	1
+#define TNL_T_PROTO_CAPWAP	2
+
+/* These flags are only needed when calling tnl_find_port(). */
+#define TNL_T_KEY_EXACT		(1 << 10)
+#define TNL_T_KEY_MATCH		(1 << 11)
+
+/* Private flags not exposed to userspace in this form. */
+#define TNL_F_IN_KEY_MATCH	(1 << 16) /* Store the key in tun_id to
+					   * match in flow table. */
+#define TNL_F_OUT_KEY_ACTION	(1 << 17) /* Get the key from a SET_TUNNEL
+					   * action. */
+
+/* All public tunnel flags. */
+#define TNL_F_PUBLIC (TNL_F_CSUM | TNL_F_TOS_INHERIT | TNL_F_TTL_INHERIT | \
+		      TNL_F_DF_INHERIT | TNL_F_DF_DEFAULT | TNL_F_PMTUD | \
+		      TNL_F_IPSEC)
+
+/**
+ * struct port_lookup_key - Tunnel port key, used as hash table key.
+ * @in_key: Key to match on input, 0 for wildcard.
+ * @net: Network namespace of the port.
+ * @saddr: IPv4 source address to match, 0 to accept any source address.
+ * @daddr: IPv4 destination of tunnel.
+ * @tunnel_type: Set of TNL_T_* flags that define lookup.
+ */
+struct port_lookup_key {
+	__be64 in_key;
+#ifdef CONFIG_NET_NS
+	struct net *net;
+#endif
+	__be32 saddr;
+	__be32 daddr;
+	u32    tunnel_type;
+};
+
+#define PORT_KEY_LEN	(offsetof(struct port_lookup_key, tunnel_type) + \
+			 FIELD_SIZEOF(struct port_lookup_key, tunnel_type))
+
+static inline struct net *port_key_get_net(const struct port_lookup_key *key)
+{
+	return read_pnet(&key->net);
+}
+
+static inline void port_key_set_net(struct port_lookup_key *key, struct net *net)
+{
+	write_pnet(&key->net, net);
+}
+
+/**
+ * struct tnl_mutable_config - modifiable configuration for a tunnel.
+ * @key: Used as key for tunnel port.  Configured via OVS_TUNNEL_ATTR_*
+ * attributes.
+ * @rcu: RCU callback head for deferred destruction.
+ * @seq: Sequence number for distinguishing configuration versions.
+ * @tunnel_hlen: Tunnel header length.
+ * @eth_addr: Source address for packets generated by tunnel itself
+ * (e.g. ICMP fragmentation needed messages).
+ * @out_key: Key to use on output, 0 if this tunnel has no fixed output key.
+ * @flags: TNL_F_* flags.
+ * @tos: IPv4 TOS value to use for tunnel, 0 if no fixed TOS.
+ * @ttl: IPv4 TTL value to use for tunnel, 0 if no fixed TTL.
+ */
+struct tnl_mutable_config {
+	struct port_lookup_key key;
+	struct rcu_head rcu;
+
+	unsigned seq;
+
+	unsigned char eth_addr[ETH_ALEN];
+
+	/* Configured via OVS_TUNNEL_ATTR_* attributes. */
+	__be64	out_key;
+	u32	flags;
+	u8	tos;
+	u8	ttl;
+
+	/* Multicast configuration. */
+	int	mlink;
+};
+
+struct tnl_ops {
+	u32 tunnel_type;	/* Put the TNL_T_PROTO_* type in here. */
+	u8 ipproto;		/* The IP protocol for the tunnel. */
+
+	/*
+	 * Returns the length of the tunnel header that will be added in
+	 * build_header() (i.e. excludes the IP header).  Returns a negative
+	 * error code if the configuration is invalid.
+	 */
+	int (*hdr_len)(const struct tnl_mutable_config *,
+		       const struct ovs_key_ipv4_tunnel *);
+	/*
+	 * Returns a linked list of SKBs with tunnel headers (multiple
+	 * packets may be generated in the event of fragmentation).  Space
+	 * will have already been allocated at the start of the packet equal
+	 * to sizeof(struct iphdr) + value returned by hdr_len().  The IP
+	 * header will have already been constructed.
+	 */
+	struct sk_buff *(*build_header)(const struct vport *,
+					 const struct tnl_mutable_config *,
+					 struct dst_entry *, struct sk_buff *,
+					 int tunnel_hlen);
+};
+
+struct tnl_vport {
+	struct rcu_head rcu;
+	struct hlist_node hash_node;
+
+	char name[IFNAMSIZ];
+	const struct tnl_ops *tnl_ops;
+
+	struct tnl_mutable_config __rcu *mutable;
+
+	/*
+	 * ID of last fragment sent (for tunnel protocols with direct support
+	 * fragmentation).  If the protocol relies on IP fragmentation then
+	 * this is not needed.
+	 */
+	atomic_t frag_id;
+};
+
+struct vport *ovs_tnl_create(const struct vport_parms *, const struct vport_ops *,
+			     const struct tnl_ops *);
+void ovs_tnl_destroy(struct vport *);
+
+int ovs_tnl_set_options(struct vport *, struct nlattr *);
+int ovs_tnl_get_options(const struct vport *, struct sk_buff *);
+
+int ovs_tnl_set_addr(struct vport *vport, const unsigned char *addr);
+const char *ovs_tnl_get_name(const struct vport *vport);
+const unsigned char *ovs_tnl_get_addr(const struct vport *vport);
+int ovs_tnl_send(struct vport *vport, struct sk_buff *skb);
+void ovs_tnl_rcv(struct vport *vport, struct sk_buff *skb);
+
+struct vport *ovs_tnl_find_port(struct net *net, __be32 saddr, __be32 daddr,
+				__be64 key, int tunnel_type,
+				const struct tnl_mutable_config **mutable);
+bool ovs_tnl_frag_needed(struct vport *vport,
+			 const struct tnl_mutable_config *mutable,
+			 struct sk_buff *skb, unsigned int mtu);
+void ovs_tnl_free_linked_skbs(struct sk_buff *skb);
+
+int ovs_tnl_init(void);
+void ovs_tnl_exit(void);
+static inline struct tnl_vport *tnl_vport_priv(const struct vport *vport)
+{
+	return vport_priv(vport);
+}
+
+static inline void tnl_tun_key_init(struct ovs_key_ipv4_tunnel *tun_key,
+				    const struct iphdr *iph, __be64 tun_id, u32 tun_flags)
+{
+	tun_key->tun_id = tun_id;
+	tun_key->ipv4_src = iph->saddr;
+	tun_key->ipv4_dst = iph->daddr;
+	tun_key->ipv4_tos = iph->tos;
+	tun_key->ipv4_ttl = iph->ttl;
+	tun_key->tun_flags = tun_flags;
+
+	/* clear struct padding. */
+	memset((unsigned char*) tun_key + OVS_TUNNEL_KEY_SIZE, 0,
+	       sizeof(*tun_key) - OVS_TUNNEL_KEY_SIZE);
+}
+
+static inline void tnl_get_param(const struct tnl_mutable_config *mutable,
+				 const struct ovs_key_ipv4_tunnel *tun_key,
+				 u32 *flags,  __be64 *out_key)
+{
+	if (tun_key->ipv4_dst) {
+		*flags = 0;
+
+		if (tun_key->tun_flags & OVS_TNL_F_KEY)
+			*flags = TNL_F_OUT_KEY_ACTION;
+		if (tun_key->tun_flags & OVS_TNL_F_CSUM)
+			*flags |= TNL_F_CSUM;
+		*out_key = tun_key->tun_id;
+	} else {
+		*flags = mutable->flags;
+		if (mutable->flags & TNL_F_OUT_KEY_ACTION)
+			*out_key = tun_key->tun_id;
+		else
+			*out_key = mutable->out_key;
+	}
+}
+
+#endif /* tunnel.h */
diff --git a/drivers/staging/openvswitch_nv/vlan.h b/drivers/staging/openvswitch_nv/vlan.h
new file mode 100644
index 000000000000..5d3573b5b974
--- /dev/null
+++ b/drivers/staging/openvswitch_nv/vlan.h
@@ -0,0 +1,100 @@
+/*
+ * Copyright (c) 2007-2011 Nicira, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#ifndef VLAN_H
+#define VLAN_H 1
+
+#include <linux/if_vlan.h>
+#include <linux/skbuff.h>
+#include <linux/version.h>
+
+/**
+ * DOC: VLAN tag manipulation.
+ *
+ * &struct sk_buff handling of VLAN tags has evolved over time:
+ *
+ * In 2.6.26 and earlier, VLAN tags did not have any generic representation in
+ * an skb, other than as a raw 802.1Q header inside the packet data.
+ *
+ * In 2.6.27 &struct sk_buff added a @vlan_tci member.  Between 2.6.27 and
+ * 2.6.32, its value was the raw contents of the 802.1Q TCI field, or zero if
+ * no 802.1Q header was present.  This worked OK except for the corner case of
+ * an 802.1Q header with an all-0-bits TCI, which could not be represented.
+ *
+ * In 2.6.33, @vlan_tci semantics changed.  Now, if an 802.1Q header is
+ * present, then the VLAN_TAG_PRESENT bit is always set.  This fixes the
+ * all-0-bits TCI corner case.
+ *
+ * For compatibility we emulate the 2.6.33+ behavior on earlier kernel
+ * versions.  The client must not access @vlan_tci directly.  Instead, use
+ * vlan_get_tci() to read it or vlan_set_tci() to write it, with semantics
+ * equivalent to those on 2.6.33+.
+ */
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,27)
+#define NEED_VLAN_FIELD
+#endif
+
+#ifndef NEED_VLAN_FIELD
+static inline void vlan_copy_skb_tci(struct sk_buff *skb) { }
+
+static inline u16 vlan_get_tci(struct sk_buff *skb)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+	if (skb->vlan_tci)
+		return skb->vlan_tci | VLAN_TAG_PRESENT;
+#endif
+	return skb->vlan_tci;
+}
+
+static inline void vlan_set_tci(struct sk_buff *skb, u16 vlan_tci)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+	vlan_tci &= ~VLAN_TAG_PRESENT;
+#endif
+	skb->vlan_tci = vlan_tci;
+}
+#else
+void vlan_copy_skb_tci(struct sk_buff *skb);
+u16 vlan_get_tci(struct sk_buff *skb);
+void vlan_set_tci(struct sk_buff *skb, u16 vlan_tci);
+
+#undef vlan_tx_tag_present
+bool vlan_tx_tag_present(struct sk_buff *skb);
+
+#undef vlan_tx_tag_get
+u16 vlan_tx_tag_get(struct sk_buff *skb);
+
+#define __vlan_hwaccel_put_tag rpl__vlan_hwaccel_put_tag
+struct sk_buff *__vlan_hwaccel_put_tag(struct sk_buff *skb, u16 vlan_tci);
+#endif /* NEED_VLAN_FIELD */
+
+static inline int vlan_deaccel_tag(struct sk_buff *skb)
+{
+	if (!vlan_tx_tag_present(skb))
+		return 0;
+
+	skb = __vlan_put_tag(skb, vlan_tx_tag_get(skb));
+	if (unlikely(!skb))
+		return -ENOMEM;
+
+	vlan_set_tci(skb, 0);
+	return 0;
+}
+
+#endif /* vlan.h */
diff --git a/drivers/staging/openvswitch_nv/vport-capwap.c b/drivers/staging/openvswitch_nv/vport-capwap.c
new file mode 100644
index 000000000000..1a1d0a26cec6
--- /dev/null
+++ b/drivers/staging/openvswitch_nv/vport-capwap.c
@@ -0,0 +1,843 @@
+/*
+ * Copyright (c) 2007-2012 Nicira, Inc.
+ * Distributed under the terms of the GNU GPL version 2.
+ *
+ * Significant portions of this file may be copied from parts of the Linux
+ * kernel, by Linus Torvalds and others.
+ */
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/version.h>
+
+#include <linux/if.h>
+#include <linux/in.h>
+#include <linux/ip.h>
+#include <linux/list.h>
+#include <linux/net.h>
+#include <linux/openvswitch-nv.h>
+#include <net/net_namespace.h>
+
+#include <net/icmp.h>
+#include <net/inet_frag.h>
+#include <net/ip.h>
+#include <net/protocol.h>
+#include <net/udp.h>
+
+#include "datapath.h"
+#include "tunnel.h"
+#include "vport.h"
+#include "vport-generic.h"
+
+#define CAPWAP_SRC_PORT 58881
+#define CAPWAP_DST_PORT 58882
+
+#define CAPWAP_FRAG_TIMEOUT (30 * HZ)
+#define CAPWAP_FRAG_MAX_MEM (256 * 1024)
+#define CAPWAP_FRAG_PRUNE_MEM (192 * 1024)
+#define CAPWAP_FRAG_SECRET_INTERVAL (10 * 60 * HZ)
+
+/*
+ * The CAPWAP header is a mess, with all kinds of odd size bit fields that
+ * cross byte boundaries, which are difficult to represent correctly in
+ * various byte orderings.  Luckily we only care about a few permutations, so
+ * statically create them and we can do very fast parsing by checking all 12
+ * fields in one go.
+ */
+#define CAPWAP_PREAMBLE_MASK __cpu_to_be32(0xFF000000)
+#define CAPWAP_HLEN_SHIFT    17
+#define CAPWAP_HLEN_MASK     __cpu_to_be32(0x00F80000)
+#define CAPWAP_RID_MASK      __cpu_to_be32(0x0007C000)
+#define CAPWAP_WBID_MASK     __cpu_to_be32(0x00003E00)
+#define CAPWAP_F_MASK        __cpu_to_be32(0x000001FF)
+
+#define CAPWAP_F_FRAG        __cpu_to_be32(0x00000080)
+#define CAPWAP_F_LASTFRAG    __cpu_to_be32(0x00000040)
+#define CAPWAP_F_WSI         __cpu_to_be32(0x00000020)
+#define CAPWAP_F_RMAC        __cpu_to_be32(0x00000010)
+
+#define CAPWAP_RMAC_LEN      4
+
+/*  Standard CAPWAP looks for a WBID value of 2.
+ *  When we insert WSI field, use WBID value of 30, which has been
+ *  proposed for all "experimental" usage - users with no reserved WBID value
+ *  of their own.
+*/
+#define CAPWAP_WBID_30   __cpu_to_be32(0x00003C00)
+#define CAPWAP_WBID_2    __cpu_to_be32(0x00000200)
+
+#define FRAG_HDR (CAPWAP_F_FRAG)
+#define FRAG_LAST_HDR (FRAG_HDR | CAPWAP_F_LASTFRAG)
+
+/* Keyed packet, WBID 30, and length long enough to include WSI key */
+#define CAPWAP_KEYED (CAPWAP_WBID_30 | CAPWAP_F_WSI | htonl(20 << CAPWAP_HLEN_SHIFT))
+/* A backward-compatible packet, WBID 2 and length of 2 words (no WSI fields) */
+#define CAPWAP_NO_WSI (CAPWAP_WBID_2 | htonl(8 << CAPWAP_HLEN_SHIFT))
+
+/* Mask for all parts of header that must be 0. */
+#define CAPWAP_ZERO_MASK (CAPWAP_PREAMBLE_MASK | \
+		(CAPWAP_F_MASK ^ (CAPWAP_F_WSI | CAPWAP_F_FRAG | CAPWAP_F_LASTFRAG | CAPWAP_F_RMAC)))
+
+struct capwaphdr {
+	__be32 begin;
+	__be16 frag_id;
+	/* low 3 bits of frag_off are reserved */
+	__be16 frag_off;
+};
+
+/*
+ * We use the WSI field to hold additional tunnel data.
+ * The first eight bits store the size of the wsi data in bytes.
+ */
+struct capwaphdr_wsi {
+	u8 wsi_len;
+	u8 flags;
+	__be16 reserved_padding;
+};
+
+struct capwaphdr_wsi_key {
+	__be64 key;
+};
+
+/* Flag indicating a 64bit key is stored in WSI data field */
+#define CAPWAP_WSI_F_KEY64 0x80
+
+static struct capwaphdr *capwap_hdr(const struct sk_buff *skb)
+{
+	return (struct capwaphdr *)(udp_hdr(skb) + 1);
+}
+
+/*
+ * The fragment offset is actually the high 13 bits of the last 16 bit field,
+ * so we would normally need to right shift 3 places.  However, it stores the
+ * offset in 8 byte chunks, which would involve a 3 place left shift.  So we
+ * just mask off the last 3 bits and be done with it.
+ */
+#define FRAG_OFF_MASK (~0x7U)
+
+/*
+ * The minimum header length.  The header may be longer if the optional
+ * WSI field is used.
+ */
+#define CAPWAP_MIN_HLEN (sizeof(struct udphdr) + sizeof(struct capwaphdr))
+
+struct frag_match {
+	__be32 saddr;
+	__be32 daddr;
+	__be16 id;
+};
+
+struct frag_queue {
+	struct inet_frag_queue ifq;
+	struct frag_match match;
+};
+
+struct frag_skb_cb {
+	u16 offset;
+};
+#define FRAG_CB(skb) ((struct frag_skb_cb *)(skb)->cb)
+
+static struct sk_buff *fragment(struct sk_buff *, const struct vport *,
+				struct dst_entry *dst, unsigned int hlen);
+static struct sk_buff *defrag(struct sk_buff *, bool frag_last);
+
+static void capwap_frag_init(struct inet_frag_queue *, void *match);
+static unsigned int capwap_frag_hash(struct inet_frag_queue *);
+static int capwap_frag_match(struct inet_frag_queue *, void *match);
+static void capwap_frag_expire(unsigned long ifq);
+
+static struct inet_frags frag_state = {
+	.constructor	= capwap_frag_init,
+	.qsize		= sizeof(struct frag_queue),
+	.hashfn		= capwap_frag_hash,
+	.match		= capwap_frag_match,
+	.frag_expire	= capwap_frag_expire,
+	.secret_interval = CAPWAP_FRAG_SECRET_INTERVAL,
+};
+
+static int capwap_hdr_len(const struct tnl_mutable_config *mutable,
+			  const struct ovs_key_ipv4_tunnel *tun_key)
+{
+	int size = CAPWAP_MIN_HLEN;
+	u32 flags;
+	__be64 out_key;
+
+	tnl_get_param(mutable, tun_key, &flags, &out_key);
+
+	/* CAPWAP has no checksums. */
+	if (flags & TNL_F_CSUM)
+		return -EINVAL;
+
+	/* if keys are specified, then add WSI field */
+	if (out_key || (flags & TNL_F_OUT_KEY_ACTION)) {
+		size += sizeof(struct capwaphdr_wsi) +
+			sizeof(struct capwaphdr_wsi_key);
+	}
+
+	return size;
+}
+
+static struct sk_buff *capwap_build_header(const struct vport *vport,
+					    const struct tnl_mutable_config *mutable,
+					    struct dst_entry *dst,
+					    struct sk_buff *skb,
+					    int tunnel_hlen)
+{
+	struct ovs_key_ipv4_tunnel *tun_key = OVS_CB(skb)->tun_key;
+	struct udphdr *udph = udp_hdr(skb);
+	struct capwaphdr *cwh = (struct capwaphdr *)(udph + 1);
+	u32 flags;
+	__be64 out_key;
+
+	tnl_get_param(mutable, tun_key, &flags, &out_key);
+
+	udph->source = htons(CAPWAP_SRC_PORT);
+	udph->dest = htons(CAPWAP_DST_PORT);
+	udph->check = 0;
+
+	cwh->frag_id = 0;
+	cwh->frag_off = 0;
+
+	if (out_key || flags & TNL_F_OUT_KEY_ACTION) {
+		/* first field in WSI is key */
+		struct capwaphdr_wsi *wsi = (struct capwaphdr_wsi *)(cwh + 1);
+
+		cwh->begin = CAPWAP_KEYED;
+
+		/* -1 for wsi_len byte, not included in length as per spec */
+		wsi->wsi_len = sizeof(struct capwaphdr_wsi) - 1
+			+ sizeof(struct capwaphdr_wsi_key);
+		wsi->flags = CAPWAP_WSI_F_KEY64;
+		wsi->reserved_padding = 0;
+
+		if (out_key) {
+			struct capwaphdr_wsi_key *opt = (struct capwaphdr_wsi_key *)(wsi + 1);
+			opt->key = out_key;
+		}
+	} else {
+		/* make packet readable by old capwap code */
+		cwh->begin = CAPWAP_NO_WSI;
+	}
+	udph->len = htons(skb->len - skb_transport_offset(skb));
+
+	if (unlikely(skb->len - skb_network_offset(skb) > dst_mtu(dst))) {
+		unsigned int hlen = skb_transport_offset(skb) + capwap_hdr_len(mutable, tun_key);
+		skb = fragment(skb, vport, dst, hlen);
+	}
+
+	return skb;
+}
+
+static int process_capwap_wsi(struct sk_buff *skb, __be64 *key, bool *key_present)
+{
+	struct capwaphdr *cwh = capwap_hdr(skb);
+	struct capwaphdr_wsi *wsi;
+	int hdr_len;
+	int rmac_len = 0;
+	int wsi_len;
+
+	if (((cwh->begin & CAPWAP_WBID_MASK) != CAPWAP_WBID_30))
+		return 0;
+
+	if (cwh->begin & CAPWAP_F_RMAC)
+		rmac_len = CAPWAP_RMAC_LEN;
+
+	hdr_len = ntohl(cwh->begin & CAPWAP_HLEN_MASK) >> CAPWAP_HLEN_SHIFT;
+
+	if (unlikely(sizeof(struct capwaphdr) + rmac_len + sizeof(struct capwaphdr_wsi) > hdr_len))
+		return -EINVAL;
+
+	/* read wsi header to find out how big it really is */
+	wsi = (struct capwaphdr_wsi *)((u8 *)(cwh + 1) + rmac_len);
+	/* +1 for length byte not included in wsi_len */
+	wsi_len = 1 + wsi->wsi_len;
+
+	if (unlikely(sizeof(struct capwaphdr) + rmac_len + wsi_len != hdr_len))
+		return -EINVAL;
+
+	wsi_len -= sizeof(struct capwaphdr_wsi);
+
+	if (wsi->flags & CAPWAP_WSI_F_KEY64) {
+		struct capwaphdr_wsi_key *opt;
+
+		if (unlikely(wsi_len < sizeof(struct capwaphdr_wsi_key)))
+			return -EINVAL;
+
+		opt = (struct capwaphdr_wsi_key *)(wsi + 1);
+		*key = opt->key;
+		*key_present = true;
+	} else {
+		*key_present = false;
+	}
+
+	return 0;
+}
+
+static struct sk_buff *process_capwap_proto(struct sk_buff *skb, __be64 *key, bool *key_present)
+{
+	struct capwaphdr *cwh = capwap_hdr(skb);
+	int hdr_len = sizeof(struct udphdr);
+
+	if (unlikely((cwh->begin & CAPWAP_ZERO_MASK) != 0))
+		goto error;
+
+	hdr_len += ntohl(cwh->begin & CAPWAP_HLEN_MASK) >> CAPWAP_HLEN_SHIFT;
+	if (unlikely(hdr_len < CAPWAP_MIN_HLEN))
+		goto error;
+
+	if (unlikely(!pskb_may_pull(skb, hdr_len + ETH_HLEN)))
+		goto error;
+
+	cwh = capwap_hdr(skb);
+	__skb_pull(skb, hdr_len);
+	skb_postpull_rcsum(skb, skb_transport_header(skb), hdr_len + ETH_HLEN);
+
+	if (cwh->begin & CAPWAP_F_FRAG) {
+		skb = defrag(skb, (__force bool)(cwh->begin & CAPWAP_F_LASTFRAG));
+		if (!skb)
+			return NULL;
+		cwh = capwap_hdr(skb);
+	}
+
+	if ((cwh->begin & CAPWAP_F_WSI) && process_capwap_wsi(skb, key, key_present))
+		goto error;
+
+	return skb;
+error:
+	kfree_skb(skb);
+	return NULL;
+}
+
+/* Called with rcu_read_lock and BH disabled. */
+static int capwap_rcv(struct sock *sk, struct sk_buff *skb)
+{
+	struct vport *vport;
+	const struct tnl_mutable_config *mutable;
+	struct iphdr *iph;
+	struct ovs_key_ipv4_tunnel tun_key;
+	__be64 key = 0;
+	bool key_present = false;
+
+	if (unlikely(!pskb_may_pull(skb, CAPWAP_MIN_HLEN + ETH_HLEN)))
+		goto error;
+
+	skb = process_capwap_proto(skb, &key, &key_present);
+	if (unlikely(!skb))
+		goto out;
+
+	iph = ip_hdr(skb);
+	vport = ovs_tnl_find_port(sock_net(sk), iph->daddr, iph->saddr, key,
+				  TNL_T_PROTO_CAPWAP, &mutable);
+	if (unlikely(!vport))
+		goto error;
+
+	if (key_present && mutable->key.daddr &&
+			 !(mutable->flags & TNL_F_IN_KEY_MATCH)) {
+		key_present = false;
+		key = 0;
+	}
+
+	tnl_tun_key_init(&tun_key, iph, key, key_present ? OVS_TNL_F_KEY : 0);
+	OVS_CB(skb)->tun_key = &tun_key;
+
+	ovs_tnl_rcv(vport, skb);
+	goto out;
+
+error:
+	kfree_skb(skb);
+out:
+	return 0;
+}
+
+static const struct tnl_ops capwap_tnl_ops = {
+	.tunnel_type	= TNL_T_PROTO_CAPWAP,
+	.ipproto	= IPPROTO_UDP,
+	.hdr_len	= capwap_hdr_len,
+	.build_header	= capwap_build_header,
+};
+
+static inline struct capwap_net *ovs_get_capwap_net(struct net *net)
+{
+	struct ovs_net *ovs_net = net_generic(net, ovs_net_id);
+	return &ovs_net->vport_net.capwap;
+}
+
+/* Arbitrary value.  Irrelevant as long as it's not 0 since we set the handler. */
+#define UDP_ENCAP_CAPWAP 10
+static int init_socket(struct net *net)
+{
+	int err;
+	struct capwap_net *capwap_net = ovs_get_capwap_net(net);
+	struct sockaddr_in sin;
+
+	if (capwap_net->n_tunnels) {
+		capwap_net->n_tunnels++;
+		return 0;
+	}
+
+	err = sock_create_kern(AF_INET, SOCK_DGRAM, 0,
+			       &capwap_net->capwap_rcv_socket);
+	if (err)
+		goto error;
+
+	/* release net ref. */
+	sk_change_net(capwap_net->capwap_rcv_socket->sk, net);
+
+	sin.sin_family = AF_INET;
+	sin.sin_addr.s_addr = htonl(INADDR_ANY);
+	sin.sin_port = htons(CAPWAP_DST_PORT);
+
+	err = kernel_bind(capwap_net->capwap_rcv_socket,
+			  (struct sockaddr *)&sin,
+			  sizeof(struct sockaddr_in));
+	if (err)
+		goto error_sock;
+
+	udp_sk(capwap_net->capwap_rcv_socket->sk)->encap_type = UDP_ENCAP_CAPWAP;
+	udp_sk(capwap_net->capwap_rcv_socket->sk)->encap_rcv = capwap_rcv;
+
+	capwap_net->frag_state.timeout		= CAPWAP_FRAG_TIMEOUT;
+	capwap_net->frag_state.high_thresh	= CAPWAP_FRAG_MAX_MEM;
+	capwap_net->frag_state.low_thresh	= CAPWAP_FRAG_PRUNE_MEM;
+
+	inet_frags_init_net(&capwap_net->frag_state);
+	capwap_net->n_tunnels++;
+	return 0;
+
+error_sock:
+	sk_release_kernel(capwap_net->capwap_rcv_socket->sk);
+error:
+	pr_warn("cannot register capwap protocol handler : %d\n", err);
+	return err;
+}
+
+static void release_socket(struct net *net)
+{
+	struct capwap_net *capwap_net = ovs_get_capwap_net(net);
+
+	capwap_net->n_tunnels--;
+	if (capwap_net->n_tunnels)
+		return;
+
+	inet_frags_exit_net(&capwap_net->frag_state, &frag_state);
+	sk_release_kernel(capwap_net->capwap_rcv_socket->sk);
+}
+
+static struct vport *capwap_create(const struct vport_parms *parms)
+{
+	struct vport *vport;
+	int err;
+
+	err = init_socket(ovs_dp_get_net(parms->dp));
+	if (err)
+		return ERR_PTR(err);
+
+	vport = ovs_tnl_create(parms, &ovs_capwap_vport_ops, &capwap_tnl_ops);
+	if (IS_ERR(vport))
+		release_socket(ovs_dp_get_net(parms->dp));
+
+	return vport;
+}
+
+static void capwap_destroy(struct vport *vport)
+{
+	ovs_tnl_destroy(vport);
+	release_socket(ovs_dp_get_net(vport->dp));
+}
+
+static int capwap_init(void)
+{
+	inet_frags_init(&frag_state);
+	return 0;
+}
+
+static void capwap_exit(void)
+{
+	inet_frags_fini(&frag_state);
+}
+
+static void copy_skb_metadata(struct sk_buff *from, struct sk_buff *to)
+{
+	to->pkt_type = from->pkt_type;
+	to->priority = from->priority;
+	to->protocol = from->protocol;
+	skb_dst_set(to, dst_clone(skb_dst(from)));
+	to->dev = from->dev;
+	to->mark = from->mark;
+
+	if (from->sk)
+		skb_set_owner_w(to, from->sk);
+
+#ifdef CONFIG_NET_SCHED
+	to->tc_index = from->tc_index;
+#endif
+#if defined(CONFIG_IP_VS) || defined(CONFIG_IP_VS_MODULE)
+	to->ipvs_property = from->ipvs_property;
+#endif
+	skb_copy_secmark(to, from);
+}
+
+static struct sk_buff *fragment(struct sk_buff *skb, const struct vport *vport,
+				struct dst_entry *dst, unsigned int hlen)
+{
+	struct tnl_vport *tnl_vport = tnl_vport_priv(vport);
+	unsigned int headroom;
+	unsigned int max_frame_len = dst_mtu(dst) + skb_network_offset(skb);
+	struct sk_buff *result = NULL, *list_cur = NULL;
+	unsigned int remaining;
+	unsigned int offset;
+	__be16 frag_id;
+
+	if (hlen + ~FRAG_OFF_MASK + 1 > max_frame_len) {
+		if (net_ratelimit())
+			pr_warn("capwap link mtu (%d) is less than minimum packet (%d)\n",
+				dst_mtu(dst),
+				hlen - skb_network_offset(skb) + ~FRAG_OFF_MASK + 1);
+		goto error;
+	}
+
+	remaining = skb->len - hlen;
+	offset = 0;
+	frag_id = htons(atomic_inc_return(&tnl_vport->frag_id));
+
+	headroom = dst->header_len + 16;
+	if (!skb_network_offset(skb))
+		headroom += LL_RESERVED_SPACE(dst->dev);
+
+	while (remaining) {
+		struct sk_buff *skb2;
+		int frag_size;
+		struct udphdr *udph;
+		struct capwaphdr *cwh;
+
+		frag_size = min(remaining, max_frame_len - hlen);
+		if (remaining > frag_size)
+			frag_size &= FRAG_OFF_MASK;
+
+		skb2 = alloc_skb(headroom + hlen + frag_size, GFP_ATOMIC);
+		if (!skb2)
+			goto error;
+
+		skb_reserve(skb2, headroom);
+		__skb_put(skb2, hlen + frag_size);
+
+		if (skb_network_offset(skb))
+			skb_reset_mac_header(skb2);
+		skb_set_network_header(skb2, skb_network_offset(skb));
+		skb_set_transport_header(skb2, skb_transport_offset(skb));
+
+		/* Copy (Ethernet)/IP/UDP/CAPWAP header. */
+		copy_skb_metadata(skb, skb2);
+		skb_copy_from_linear_data(skb, skb2->data, hlen);
+
+		/* Copy this data chunk. */
+		if (skb_copy_bits(skb, hlen + offset, skb2->data + hlen, frag_size))
+			BUG();
+
+		udph = udp_hdr(skb2);
+		udph->len = htons(skb2->len - skb_transport_offset(skb2));
+
+		cwh = capwap_hdr(skb2);
+		if (remaining > frag_size)
+			cwh->begin |= FRAG_HDR;
+		else
+			cwh->begin |= FRAG_LAST_HDR;
+		cwh->frag_id = frag_id;
+		cwh->frag_off = htons(offset);
+
+		if (result) {
+			list_cur->next = skb2;
+			list_cur = skb2;
+		} else
+			result = list_cur = skb2;
+
+		offset += frag_size;
+		remaining -= frag_size;
+	}
+
+	consume_skb(skb);
+	return result;
+
+error:
+	ovs_tnl_free_linked_skbs(result);
+	kfree_skb(skb);
+	return NULL;
+}
+
+/* All of the following functions relate to fragmentation reassembly. */
+
+static struct frag_queue *ifq_cast(struct inet_frag_queue *ifq)
+{
+	return container_of(ifq, struct frag_queue, ifq);
+}
+
+static u32 frag_hash(struct frag_match *match)
+{
+	return jhash_3words((__force u16)match->id, (__force u32)match->saddr,
+			    (__force u32)match->daddr,
+			    frag_state.rnd) & (INETFRAGS_HASHSZ - 1);
+}
+
+static struct frag_queue *queue_find(struct netns_frags *ns_frag_state,
+				     struct frag_match *match)
+{
+	struct inet_frag_queue *ifq;
+
+	read_lock(&frag_state.lock);
+
+	ifq = inet_frag_find(ns_frag_state, &frag_state, match, frag_hash(match));
+	if (!ifq)
+		return NULL;
+
+	/* Unlock happens inside inet_frag_find(). */
+
+	return ifq_cast(ifq);
+}
+
+static struct sk_buff *frag_reasm(struct frag_queue *fq, struct net_device *dev)
+{
+	struct sk_buff *head = fq->ifq.fragments;
+	struct sk_buff *frag;
+
+	/* Succeed or fail, we're done with this queue. */
+	inet_frag_kill(&fq->ifq, &frag_state);
+
+	if (fq->ifq.len > 65535)
+		return NULL;
+
+	/* Can't have the head be a clone. */
+	if (skb_cloned(head) && pskb_expand_head(head, 0, 0, GFP_ATOMIC))
+		return NULL;
+
+	/*
+	 * We're about to build frag list for this SKB.  If it already has a
+	 * frag list, alloc a new SKB and put the existing frag list there.
+	 */
+	if (skb_shinfo(head)->frag_list) {
+		int i;
+		int paged_len = 0;
+
+		frag = alloc_skb(0, GFP_ATOMIC);
+		if (!frag)
+			return NULL;
+
+		frag->next = head->next;
+		head->next = frag;
+		skb_shinfo(frag)->frag_list = skb_shinfo(head)->frag_list;
+		skb_shinfo(head)->frag_list = NULL;
+
+		for (i = 0; i < skb_shinfo(head)->nr_frags; i++)
+			paged_len += skb_shinfo(head)->frags[i].size;
+		frag->len = frag->data_len = head->data_len - paged_len;
+		head->data_len -= frag->len;
+		head->len -= frag->len;
+
+		frag->ip_summed = head->ip_summed;
+		atomic_add(frag->truesize, &fq->ifq.net->mem);
+	}
+
+	skb_shinfo(head)->frag_list = head->next;
+	atomic_sub(head->truesize, &fq->ifq.net->mem);
+
+	/* Properly account for data in various packets. */
+	for (frag = head->next; frag; frag = frag->next) {
+		head->data_len += frag->len;
+		head->len += frag->len;
+
+		if (head->ip_summed != frag->ip_summed)
+			head->ip_summed = CHECKSUM_NONE;
+		else if (head->ip_summed == CHECKSUM_COMPLETE)
+			head->csum = csum_add(head->csum, frag->csum);
+
+		head->truesize += frag->truesize;
+		atomic_sub(frag->truesize, &fq->ifq.net->mem);
+	}
+
+	head->next = NULL;
+	head->dev = dev;
+	head->tstamp = fq->ifq.stamp;
+	fq->ifq.fragments = NULL;
+
+	return head;
+}
+
+static struct sk_buff *frag_queue(struct frag_queue *fq, struct sk_buff *skb,
+				  u16 offset, bool frag_last)
+{
+	struct sk_buff *prev, *next;
+	struct net_device *dev;
+	int end;
+
+	if (fq->ifq.last_in & INET_FRAG_COMPLETE)
+		goto error;
+
+	if (!skb->len)
+		goto error;
+
+	end = offset + skb->len;
+
+	if (frag_last) {
+		/*
+		 * Last fragment, shouldn't already have data past our end or
+		 * have another last fragment.
+		 */
+		if (end < fq->ifq.len || fq->ifq.last_in & INET_FRAG_LAST_IN)
+			goto error;
+
+		fq->ifq.last_in |= INET_FRAG_LAST_IN;
+		fq->ifq.len = end;
+	} else {
+		/* Fragments should align to 8 byte chunks. */
+		if (end & ~FRAG_OFF_MASK)
+			goto error;
+
+		if (end > fq->ifq.len) {
+			/*
+			 * Shouldn't have data past the end, if we already
+			 * have one.
+			 */
+			if (fq->ifq.last_in & INET_FRAG_LAST_IN)
+				goto error;
+
+			fq->ifq.len = end;
+		}
+	}
+
+	/* Find where we fit in. */
+	prev = NULL;
+	for (next = fq->ifq.fragments; next != NULL; next = next->next) {
+		if (FRAG_CB(next)->offset >= offset)
+			break;
+		prev = next;
+	}
+
+	/*
+	 * Overlapping fragments aren't allowed.  We shouldn't start before
+	 * the end of the previous fragment.
+	 */
+	if (prev && FRAG_CB(prev)->offset + prev->len > offset)
+		goto error;
+
+	/* We also shouldn't end after the beginning of the next fragment. */
+	if (next && end > FRAG_CB(next)->offset)
+		goto error;
+
+	FRAG_CB(skb)->offset = offset;
+
+	/* Link into list. */
+	skb->next = next;
+	if (prev)
+		prev->next = skb;
+	else
+		fq->ifq.fragments = skb;
+
+	dev = skb->dev;
+	skb->dev = NULL;
+
+	fq->ifq.stamp = skb->tstamp;
+	fq->ifq.meat += skb->len;
+	atomic_add(skb->truesize, &fq->ifq.net->mem);
+	if (offset == 0)
+		fq->ifq.last_in |= INET_FRAG_FIRST_IN;
+
+	/* If we have all fragments do reassembly. */
+	if (fq->ifq.last_in == (INET_FRAG_FIRST_IN | INET_FRAG_LAST_IN) &&
+	    fq->ifq.meat == fq->ifq.len)
+		return frag_reasm(fq, dev);
+
+	write_lock(&frag_state.lock);
+	list_move_tail(&fq->ifq.lru_list, &fq->ifq.net->lru_list);
+	write_unlock(&frag_state.lock);
+
+	return NULL;
+
+error:
+	kfree_skb(skb);
+	return NULL;
+}
+
+static struct sk_buff *defrag(struct sk_buff *skb, bool frag_last)
+{
+	struct iphdr *iph = ip_hdr(skb);
+	struct capwaphdr *cwh = capwap_hdr(skb);
+	struct capwap_net *capwap_net = ovs_get_capwap_net(dev_net(skb->dev));
+	struct netns_frags *ns_frag_state = &capwap_net->frag_state;
+	struct frag_match match;
+	u16 frag_off;
+	struct frag_queue *fq;
+
+	inet_frag_evictor(ns_frag_state, &frag_state);
+
+	match.daddr = iph->daddr;
+	match.saddr = iph->saddr;
+	match.id = cwh->frag_id;
+	frag_off = ntohs(cwh->frag_off) & FRAG_OFF_MASK;
+
+	fq = queue_find(ns_frag_state, &match);
+	if (fq) {
+		spin_lock(&fq->ifq.lock);
+		skb = frag_queue(fq, skb, frag_off, frag_last);
+		spin_unlock(&fq->ifq.lock);
+
+		inet_frag_put(&fq->ifq, &frag_state);
+
+		return skb;
+	}
+
+	kfree_skb(skb);
+	return NULL;
+}
+
+static void capwap_frag_init(struct inet_frag_queue *ifq, void *match_)
+{
+	struct frag_match *match = match_;
+
+	ifq_cast(ifq)->match = *match;
+}
+
+static unsigned int capwap_frag_hash(struct inet_frag_queue *ifq)
+{
+	return frag_hash(&ifq_cast(ifq)->match);
+}
+
+static int capwap_frag_match(struct inet_frag_queue *ifq, void *a_)
+{
+	struct frag_match *a = a_;
+	struct frag_match *b = &ifq_cast(ifq)->match;
+
+	return a->id == b->id && a->saddr == b->saddr && a->daddr == b->daddr;
+}
+
+/* Run when the timeout for a given queue expires. */
+static void capwap_frag_expire(unsigned long ifq)
+{
+	struct frag_queue *fq;
+
+	fq = ifq_cast((struct inet_frag_queue *)ifq);
+
+	spin_lock(&fq->ifq.lock);
+
+	if (!(fq->ifq.last_in & INET_FRAG_COMPLETE))
+		inet_frag_kill(&fq->ifq, &frag_state);
+
+	spin_unlock(&fq->ifq.lock);
+	inet_frag_put(&fq->ifq, &frag_state);
+}
+
+const struct vport_ops ovs_capwap_vport_ops = {
+	.type		= OVS_VPORT_TYPE_CAPWAP,
+	.flags		= VPORT_F_TUN_ID,
+	.init		= capwap_init,
+	.exit		= capwap_exit,
+	.create		= capwap_create,
+	.destroy	= capwap_destroy,
+	.set_addr	= ovs_tnl_set_addr,
+	.get_name	= ovs_tnl_get_name,
+	.get_addr	= ovs_tnl_get_addr,
+	.get_options	= ovs_tnl_get_options,
+	.set_options	= ovs_tnl_set_options,
+	.get_dev_flags	= ovs_vport_gen_get_dev_flags,
+	.is_running	= ovs_vport_gen_is_running,
+	.get_operstate	= ovs_vport_gen_get_operstate,
+	.send		= ovs_tnl_send,
+};
diff --git a/drivers/staging/openvswitch_nv/vport-capwap.h b/drivers/staging/openvswitch_nv/vport-capwap.h
new file mode 100644
index 000000000000..21307a3c0b74
--- /dev/null
+++ b/drivers/staging/openvswitch_nv/vport-capwap.h
@@ -0,0 +1,30 @@
+/*
+ * Copyright (c) 2012 Nicira, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#ifndef VPORT_CAPWAP_H
+#define VPORT_CAPWAP_H 1
+
+#include <linux/net.h>
+
+struct capwap_net {
+	struct socket *capwap_rcv_socket;
+	struct netns_frags frag_state;
+	int n_tunnels;
+};
+
+#endif /* vport-capwap.h */
diff --git a/drivers/staging/openvswitch_nv/vport-generic.c b/drivers/staging/openvswitch_nv/vport-generic.c
new file mode 100644
index 000000000000..09b0b7cfa2a0
--- /dev/null
+++ b/drivers/staging/openvswitch_nv/vport-generic.c
@@ -0,0 +1,36 @@
+/*
+ * Copyright (c) 2007-2011 Nicira, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#include <linux/etherdevice.h>
+
+#include "vport-generic.h"
+
+unsigned ovs_vport_gen_get_dev_flags(const struct vport *vport)
+{
+	return IFF_UP | IFF_RUNNING | IFF_LOWER_UP;
+}
+
+int ovs_vport_gen_is_running(const struct vport *vport)
+{
+	return 1;
+}
+
+unsigned char ovs_vport_gen_get_operstate(const struct vport *vport)
+{
+	return IF_OPER_UP;
+}
diff --git a/drivers/staging/openvswitch_nv/vport-generic.h b/drivers/staging/openvswitch_nv/vport-generic.h
new file mode 100644
index 000000000000..4a295c734d59
--- /dev/null
+++ b/drivers/staging/openvswitch_nv/vport-generic.h
@@ -0,0 +1,28 @@
+/*
+ * Copyright (c) 2007-2011 Nicira, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#ifndef VPORT_GENERIC_H
+#define VPORT_GENERIC_H 1
+
+#include "vport.h"
+
+unsigned ovs_vport_gen_get_dev_flags(const struct vport *);
+int ovs_vport_gen_is_running(const struct vport *);
+unsigned char ovs_vport_gen_get_operstate(const struct vport *);
+
+#endif /* vport-generic.h */
diff --git a/drivers/staging/openvswitch_nv/vport-gre.c b/drivers/staging/openvswitch_nv/vport-gre.c
new file mode 100644
index 000000000000..831323420a2e
--- /dev/null
+++ b/drivers/staging/openvswitch_nv/vport-gre.c
@@ -0,0 +1,537 @@
+/*
+ * Copyright (c) 2007-2012 Nicira, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/if.h>
+#include <linux/skbuff.h>
+#include <linux/ip.h>
+#include <linux/if_tunnel.h>
+#include <linux/if_vlan.h>
+#include <linux/in.h>
+#include <linux/openvswitch-nv.h>
+
+#include <net/icmp.h>
+#include <net/ip.h>
+#include <net/protocol.h>
+
+#include "datapath.h"
+#include "tunnel.h"
+#include "vport.h"
+#include "vport-generic.h"
+
+/*
+ * The GRE header is composed of a series of sections: a base and then a variable
+ * number of options.
+ */
+#define GRE_HEADER_SECTION 4
+
+struct gre_base_hdr {
+	__be16 flags;
+	__be16 protocol;
+};
+
+static int gre_hdr_len(const struct tnl_mutable_config *mutable,
+		       const struct ovs_key_ipv4_tunnel *tun_key)
+{
+	int len;
+	u32 flags;
+	__be64 out_key;
+
+	tnl_get_param(mutable, tun_key, &flags, &out_key);
+	len = GRE_HEADER_SECTION;
+
+	if (flags & TNL_F_CSUM)
+		len += GRE_HEADER_SECTION;
+
+	/* Set key for GRE64 tunnels, even when key if is zero. */
+	if (out_key ||
+	    mutable->key.tunnel_type & TNL_T_PROTO_GRE64 ||
+	    flags & TNL_F_OUT_KEY_ACTION) {
+
+		len += GRE_HEADER_SECTION;
+		if (mutable->key.tunnel_type & TNL_T_PROTO_GRE64)
+			len += GRE_HEADER_SECTION;
+	}
+	return len;
+}
+
+
+/* Returns the least-significant 32 bits of a __be64. */
+static __be32 be64_get_low32(__be64 x)
+{
+#ifdef __BIG_ENDIAN
+	return (__force __be32)x;
+#else
+	return (__force __be32)((__force u64)x >> 32);
+#endif
+}
+
+static __be32 be64_get_high32(__be64 x)
+{
+#ifdef __BIG_ENDIAN
+	return (__force __be32)((__force u64)x >> 32);
+#else
+	return (__force __be32)x;
+#endif
+}
+
+static struct sk_buff *gre_build_header(const struct vport *vport,
+					 const struct tnl_mutable_config *mutable,
+					 struct dst_entry *dst,
+					 struct sk_buff *skb,
+					 int tunnel_hlen)
+{
+	u32 flags;
+	__be64 out_key;
+	const struct ovs_key_ipv4_tunnel *tun_key = OVS_CB(skb)->tun_key;
+	__be32 *options = (__be32 *)(skb_network_header(skb) + tunnel_hlen
+					       - GRE_HEADER_SECTION);
+	struct gre_base_hdr *greh = (struct gre_base_hdr *) skb_transport_header(skb);
+
+	tnl_get_param(mutable, tun_key, &flags, &out_key);
+
+	greh->protocol = htons(ETH_P_TEB);
+	greh->flags = 0;
+
+	/* Work backwards over the options so the checksum is last. */
+	if (out_key || flags & TNL_F_OUT_KEY_ACTION ||
+	    mutable->key.tunnel_type & TNL_T_PROTO_GRE64) {
+		greh->flags |= GRE_KEY;
+		if (mutable->key.tunnel_type & TNL_T_PROTO_GRE64) {
+			/* Set higher 32 bits to seq. */
+			*options = be64_get_high32(out_key);
+			options--;
+			greh->flags |= GRE_SEQ;
+		}
+		*options = be64_get_low32(out_key);
+		options--;
+	}
+
+	if (flags & TNL_F_CSUM) {
+		greh->flags |= GRE_CSUM;
+		*options = 0;
+		*(__sum16 *)options = csum_fold(skb_checksum(skb,
+						skb_transport_offset(skb),
+						skb->len - skb_transport_offset(skb),
+						0));
+	}
+	/*
+	 * Allow our local IP stack to fragment the outer packet even if the
+	 * DF bit is set as a last resort.  We also need to force selection of
+	 * an IP ID here because Linux will otherwise leave it at 0 if the
+	 * packet originally had DF set.
+	 */
+	skb->local_df = 1;
+	__ip_select_ident(ip_hdr(skb), dst, 0);
+
+	return skb;
+}
+
+static __be64 key_to_tunnel_id(__be32 key, __be32 seq)
+{
+#ifdef __BIG_ENDIAN
+	return (__force __be64)((__force u64)seq << 32 | (__force u32)key);
+#else
+	return (__force __be64)((__force u64)key << 32 | (__force u32)seq);
+#endif
+}
+
+static int parse_header(struct iphdr *iph, __be16 *flags, __be64 *tun_id,
+			u32 *tunnel_type)
+{
+	/* IP and ICMP protocol handlers check that the IHL is valid. */
+	struct gre_base_hdr *greh = (struct gre_base_hdr *)((u8 *)iph + (iph->ihl << 2));
+	__be32 *options = (__be32 *)(greh + 1);
+	int hdr_len;
+
+	*flags = greh->flags;
+
+	if (unlikely(greh->flags & (GRE_VERSION | GRE_ROUTING)))
+		return -EINVAL;
+
+	if (unlikely(greh->protocol != htons(ETH_P_TEB)))
+		return -EINVAL;
+
+	hdr_len = GRE_HEADER_SECTION;
+
+	if (greh->flags & GRE_CSUM) {
+		hdr_len += GRE_HEADER_SECTION;
+		options++;
+	}
+
+	if (greh->flags & GRE_KEY) {
+		__be32 seq;
+		__be32 gre_key;
+
+		gre_key = *options;
+		hdr_len += GRE_HEADER_SECTION;
+		options++;
+
+		if (greh->flags & GRE_SEQ) {
+			seq = *options;
+			*tunnel_type = TNL_T_PROTO_GRE64;
+		} else {
+			seq = 0;
+			*tunnel_type = TNL_T_PROTO_GRE;
+		}
+		*tun_id = key_to_tunnel_id(gre_key, seq);
+	} else {
+		*tun_id = 0;
+		/* Ignore GRE seq if there is no key present. */
+		*tunnel_type = TNL_T_PROTO_GRE;
+	}
+
+	if (greh->flags & GRE_SEQ)
+		hdr_len += GRE_HEADER_SECTION;
+
+	return hdr_len;
+}
+
+/* Called with rcu_read_lock and BH disabled. */
+static void gre_err(struct sk_buff *skb, u32 info)
+{
+	struct vport *vport;
+	const struct tnl_mutable_config *mutable;
+	const int type = icmp_hdr(skb)->type;
+	const int code = icmp_hdr(skb)->code;
+	int mtu = ntohs(icmp_hdr(skb)->un.frag.mtu);
+	u32 tunnel_type;
+
+	struct iphdr *iph;
+	__be16 flags;
+	__be64 key;
+	int tunnel_hdr_len, tot_hdr_len;
+	unsigned int orig_mac_header;
+	unsigned int orig_nw_header;
+
+	if (type != ICMP_DEST_UNREACH || code != ICMP_FRAG_NEEDED)
+		return;
+
+	/*
+	 * The mimimum size packet that we would actually be able to process:
+	 * encapsulating IP header, minimum GRE header, Ethernet header,
+	 * inner IPv4 header.
+	 */
+	if (!pskb_may_pull(skb, sizeof(struct iphdr) + GRE_HEADER_SECTION +
+				ETH_HLEN + sizeof(struct iphdr)))
+		return;
+
+	iph = (struct iphdr *)skb->data;
+	if (ipv4_is_multicast(iph->daddr))
+		return;
+
+	tunnel_hdr_len = parse_header(iph, &flags, &key, &tunnel_type);
+	if (tunnel_hdr_len < 0)
+		return;
+
+	vport = ovs_tnl_find_port(dev_net(skb->dev), iph->saddr, iph->daddr, key,
+				  tunnel_type, &mutable);
+	if (!vport)
+		return;
+
+	/*
+	 * Packets received by this function were previously sent by us, so
+	 * any comparisons should be to the output values, not the input.
+	 * However, it's not really worth it to have a hash table based on
+	 * output keys (especially since ICMP error handling of tunneled packets
+	 * isn't that reliable anyways).  Therefore, we do a lookup based on the
+	 * out key as if it were the in key and then check to see if the input
+	 * and output keys are the same.
+	 */
+	if (mutable->key.in_key != mutable->out_key)
+		return;
+
+	if (!!(mutable->flags & TNL_F_IN_KEY_MATCH) !=
+	    !!(mutable->flags & TNL_F_OUT_KEY_ACTION))
+		return;
+
+	if ((mutable->flags & TNL_F_CSUM) && !(flags & GRE_CSUM))
+		return;
+
+	tunnel_hdr_len += iph->ihl << 2;
+
+	orig_mac_header = skb_mac_header(skb) - skb->data;
+	orig_nw_header = skb_network_header(skb) - skb->data;
+	skb_set_mac_header(skb, tunnel_hdr_len);
+
+	tot_hdr_len = tunnel_hdr_len + ETH_HLEN;
+
+	skb->protocol = eth_hdr(skb)->h_proto;
+	if (skb->protocol == htons(ETH_P_8021Q)) {
+		tot_hdr_len += VLAN_HLEN;
+		skb->protocol = vlan_eth_hdr(skb)->h_vlan_encapsulated_proto;
+	}
+
+	skb_set_network_header(skb, tot_hdr_len);
+	mtu -= tot_hdr_len;
+
+	if (skb->protocol == htons(ETH_P_IP))
+		tot_hdr_len += sizeof(struct iphdr);
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+	else if (skb->protocol == htons(ETH_P_IPV6))
+		tot_hdr_len += sizeof(struct ipv6hdr);
+#endif
+	else
+		goto out;
+
+	if (!pskb_may_pull(skb, tot_hdr_len))
+		goto out;
+
+	if (skb->protocol == htons(ETH_P_IP)) {
+		if (mtu < IP_MIN_MTU) {
+			if (ntohs(ip_hdr(skb)->tot_len) >= IP_MIN_MTU)
+				mtu = IP_MIN_MTU;
+			else
+				goto out;
+		}
+
+	}
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+	else if (skb->protocol == htons(ETH_P_IPV6)) {
+		if (mtu < IPV6_MIN_MTU) {
+			unsigned int packet_length = sizeof(struct ipv6hdr) +
+					      ntohs(ipv6_hdr(skb)->payload_len);
+
+			if (packet_length >= IPV6_MIN_MTU
+			    || ntohs(ipv6_hdr(skb)->payload_len) == 0)
+				mtu = IPV6_MIN_MTU;
+			else
+				goto out;
+		}
+	}
+#endif
+
+	__skb_pull(skb, tunnel_hdr_len);
+	ovs_tnl_frag_needed(vport, mutable, skb, mtu);
+	__skb_push(skb, tunnel_hdr_len);
+
+out:
+	skb_set_mac_header(skb, orig_mac_header);
+	skb_set_network_header(skb, orig_nw_header);
+	skb->protocol = htons(ETH_P_IP);
+}
+
+static bool check_checksum(struct sk_buff *skb)
+{
+	struct iphdr *iph = ip_hdr(skb);
+	struct gre_base_hdr *greh = (struct gre_base_hdr *)(iph + 1);
+	__sum16 csum = 0;
+
+	if (greh->flags & GRE_CSUM) {
+		switch (skb->ip_summed) {
+		case CHECKSUM_COMPLETE:
+			csum = csum_fold(skb->csum);
+
+			if (!csum)
+				break;
+			/* Fall through. */
+
+		case CHECKSUM_NONE:
+			skb->csum = 0;
+			csum = __skb_checksum_complete(skb);
+			skb->ip_summed = CHECKSUM_COMPLETE;
+			break;
+		}
+	}
+
+	return (csum == 0);
+}
+
+static u32 gre_flags_to_tunnel_flags(const struct tnl_mutable_config *mutable,
+				     __be16 gre_flags, __be64 *key)
+{
+	u32 tunnel_flags = 0;
+
+	if (gre_flags & GRE_KEY) {
+		if (mutable->flags & TNL_F_IN_KEY_MATCH ||
+		    !mutable->key.daddr)
+			tunnel_flags = OVS_TNL_F_KEY;
+		else
+			*key = 0;
+	}
+
+	if (gre_flags & GRE_CSUM)
+		tunnel_flags |= OVS_TNL_F_CSUM;
+
+	return tunnel_flags;
+}
+
+/* Called with rcu_read_lock and BH disabled. */
+static int gre_rcv(struct sk_buff *skb)
+{
+	struct vport *vport;
+	const struct tnl_mutable_config *mutable;
+	int hdr_len;
+	struct iphdr *iph;
+	struct ovs_key_ipv4_tunnel tun_key;
+	__be16 gre_flags;
+	u32 tnl_flags;
+	__be64 key;
+	u32 tunnel_type;
+
+	if (unlikely(!pskb_may_pull(skb, sizeof(struct gre_base_hdr) + ETH_HLEN)))
+		goto error;
+	if (unlikely(!check_checksum(skb)))
+		goto error;
+
+	hdr_len = parse_header(ip_hdr(skb), &gre_flags, &key, &tunnel_type);
+	if (unlikely(hdr_len < 0))
+		goto error;
+
+	if (unlikely(!pskb_may_pull(skb, hdr_len + ETH_HLEN)))
+		goto error;
+
+	iph = ip_hdr(skb);
+	vport = ovs_tnl_find_port(dev_net(skb->dev), iph->daddr, iph->saddr, key,
+				  tunnel_type, &mutable);
+	if (unlikely(!vport))
+		goto error;
+
+	tnl_flags = gre_flags_to_tunnel_flags(mutable, gre_flags, &key);
+	tnl_tun_key_init(&tun_key, iph, key, tnl_flags);
+	OVS_CB(skb)->tun_key = &tun_key;
+
+	__skb_pull(skb, hdr_len);
+	skb_postpull_rcsum(skb, skb_transport_header(skb), hdr_len + ETH_HLEN);
+
+	ovs_tnl_rcv(vport, skb);
+	return 0;
+
+error:
+	kfree_skb(skb);
+	return 0;
+}
+
+static const struct tnl_ops gre_tnl_ops = {
+	.tunnel_type	= TNL_T_PROTO_GRE,
+	.ipproto	= IPPROTO_GRE,
+	.hdr_len	= gre_hdr_len,
+	.build_header	= gre_build_header,
+};
+
+static struct vport *gre_create(const struct vport_parms *parms)
+{
+	return ovs_tnl_create(parms, &ovs_gre_vport_ops, &gre_tnl_ops);
+}
+
+static struct vport *gre_create_ft(const struct vport_parms *parms)
+{
+	return ovs_tnl_create(parms, &ovs_gre_ft_vport_ops, &gre_tnl_ops);
+}
+
+static const struct tnl_ops gre64_tnl_ops = {
+	.tunnel_type	= TNL_T_PROTO_GRE64,
+	.ipproto	= IPPROTO_GRE,
+	.hdr_len	= gre_hdr_len,
+	.build_header	= gre_build_header,
+};
+
+static struct vport *gre_create64(const struct vport_parms *parms)
+{
+	return ovs_tnl_create(parms, &ovs_gre64_vport_ops, &gre64_tnl_ops);
+}
+
+static const struct net_protocol gre_protocol_handlers = {
+	.handler	=	gre_rcv,
+	.err_handler	=	gre_err,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,32)
+	.netns_ok	=	1,
+#endif
+};
+
+static bool inited;
+
+static int gre_init(void)
+{
+	int err;
+
+	if (inited)
+		return 0;
+
+	inited = true;
+	err = inet_add_protocol(&gre_protocol_handlers, IPPROTO_GRE);
+	if (err)
+		pr_warn("cannot register gre protocol handler\n");
+
+	return err;
+}
+
+static void gre_exit(void)
+{
+	if (!inited)
+		return;
+
+	inited = false;
+
+	inet_del_protocol(&gre_protocol_handlers, IPPROTO_GRE);
+}
+
+const struct vport_ops ovs_gre_ft_vport_ops = {
+	.type		= OVS_VPORT_TYPE_FT_GRE,
+	.flags		= VPORT_F_TUN_ID,
+	.init		= gre_init,
+	.exit		= gre_exit,
+	.create		= gre_create_ft,
+	.destroy	= ovs_tnl_destroy,
+	.set_addr	= ovs_tnl_set_addr,
+	.get_name	= ovs_tnl_get_name,
+	.get_addr	= ovs_tnl_get_addr,
+	.get_options	= ovs_tnl_get_options,
+	.set_options	= ovs_tnl_set_options,
+	.get_dev_flags	= ovs_vport_gen_get_dev_flags,
+	.is_running	= ovs_vport_gen_is_running,
+	.get_operstate	= ovs_vport_gen_get_operstate,
+	.send		= ovs_tnl_send,
+};
+
+const struct vport_ops ovs_gre_vport_ops = {
+	.type		= OVS_VPORT_TYPE_GRE,
+	.flags		= VPORT_F_TUN_ID,
+	.init		= gre_init,
+	.exit		= gre_exit,
+	.create		= gre_create,
+	.destroy	= ovs_tnl_destroy,
+	.set_addr	= ovs_tnl_set_addr,
+	.get_name	= ovs_tnl_get_name,
+	.get_addr	= ovs_tnl_get_addr,
+	.get_options	= ovs_tnl_get_options,
+	.set_options	= ovs_tnl_set_options,
+	.get_dev_flags	= ovs_vport_gen_get_dev_flags,
+	.is_running	= ovs_vport_gen_is_running,
+	.get_operstate	= ovs_vport_gen_get_operstate,
+	.send		= ovs_tnl_send,
+};
+
+const struct vport_ops ovs_gre64_vport_ops = {
+	.type		= OVS_VPORT_TYPE_GRE64,
+	.flags		= VPORT_F_TUN_ID,
+	.init		= gre_init,
+	.exit		= gre_exit,
+	.create		= gre_create64,
+	.destroy	= ovs_tnl_destroy,
+	.set_addr	= ovs_tnl_set_addr,
+	.get_name	= ovs_tnl_get_name,
+	.get_addr	= ovs_tnl_get_addr,
+	.get_options	= ovs_tnl_get_options,
+	.set_options	= ovs_tnl_set_options,
+	.get_dev_flags	= ovs_vport_gen_get_dev_flags,
+	.is_running	= ovs_vport_gen_is_running,
+	.get_operstate	= ovs_vport_gen_get_operstate,
+	.send		= ovs_tnl_send,
+};
diff --git a/drivers/staging/openvswitch_nv/vport-internal_dev.c b/drivers/staging/openvswitch_nv/vport-internal_dev.c
new file mode 100644
index 000000000000..4dc2eb479d70
--- /dev/null
+++ b/drivers/staging/openvswitch_nv/vport-internal_dev.c
@@ -0,0 +1,346 @@
+/*
+ * Copyright (c) 2007-2012 Nicira, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#include <linux/hardirq.h>
+#include <linux/if_vlan.h>
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/ethtool.h>
+#include <linux/skbuff.h>
+#include <linux/version.h>
+
+#include <net/dst.h>
+#include <net/xfrm.h>
+
+#include "checksum.h"
+#include "datapath.h"
+#include "vlan.h"
+#include "vport-generic.h"
+#include "vport-internal_dev.h"
+#include "vport-netdev.h"
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,1,0)
+#define HAVE_NET_DEVICE_OPS
+#endif
+
+struct internal_dev {
+	struct vport *vport;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,22)
+	struct net_device_stats stats;
+#endif
+};
+
+static struct internal_dev *internal_dev_priv(struct net_device *netdev)
+{
+	return netdev_priv(netdev);
+}
+
+/* This function is only called by the kernel network layer.*/
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,36)
+static struct rtnl_link_stats64 *internal_dev_get_stats(struct net_device *netdev,
+							struct rtnl_link_stats64 *stats)
+{
+#else
+static struct net_device_stats *internal_dev_sys_stats(struct net_device *netdev)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,22)
+	struct net_device_stats *stats = &internal_dev_priv(netdev)->stats;
+#else
+	struct net_device_stats *stats = &netdev->stats;
+#endif
+#endif
+	struct vport *vport = ovs_internal_dev_get_vport(netdev);
+	struct ovs_vport_stats vport_stats;
+
+	ovs_vport_get_stats(vport, &vport_stats);
+
+	/* The tx and rx stats need to be swapped because the
+	 * switch and host OS have opposite perspectives. */
+	stats->rx_packets	= vport_stats.tx_packets;
+	stats->tx_packets	= vport_stats.rx_packets;
+	stats->rx_bytes		= vport_stats.tx_bytes;
+	stats->tx_bytes		= vport_stats.rx_bytes;
+	stats->rx_errors	= vport_stats.tx_errors;
+	stats->tx_errors	= vport_stats.rx_errors;
+	stats->rx_dropped	= vport_stats.tx_dropped;
+	stats->tx_dropped	= vport_stats.rx_dropped;
+
+	return stats;
+}
+
+static int internal_dev_mac_addr(struct net_device *dev, void *p)
+{
+	struct sockaddr *addr = p;
+
+	if (!is_valid_ether_addr(addr->sa_data))
+		return -EADDRNOTAVAIL;
+#ifdef NET_ADDR_RANDOM
+	dev->addr_assign_type &= ~NET_ADDR_RANDOM;
+#endif
+	memcpy(dev->dev_addr, addr->sa_data, dev->addr_len);
+	return 0;
+}
+
+/* Called with rcu_read_lock_bh. */
+static int internal_dev_xmit(struct sk_buff *skb, struct net_device *netdev)
+{
+	if (unlikely(compute_ip_summed(skb, true))) {
+		kfree_skb(skb);
+		return 0;
+	}
+
+	vlan_copy_skb_tci(skb);
+	OVS_CB(skb)->flow = NULL;
+
+	rcu_read_lock();
+	ovs_vport_receive(internal_dev_priv(netdev)->vport, skb);
+	rcu_read_unlock();
+	return 0;
+}
+
+static int internal_dev_open(struct net_device *netdev)
+{
+	netif_start_queue(netdev);
+	return 0;
+}
+
+static int internal_dev_stop(struct net_device *netdev)
+{
+	netif_stop_queue(netdev);
+	return 0;
+}
+
+static void internal_dev_getinfo(struct net_device *netdev,
+				 struct ethtool_drvinfo *info)
+{
+	strcpy(info->driver, "openvswitch");
+}
+
+static const struct ethtool_ops internal_dev_ethtool_ops = {
+	.get_drvinfo	= internal_dev_getinfo,
+	.get_link	= ethtool_op_get_link,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,39)
+	.get_sg		= ethtool_op_get_sg,
+	.set_sg		= ethtool_op_set_sg,
+	.get_tx_csum	= ethtool_op_get_tx_csum,
+	.set_tx_csum	= ethtool_op_set_tx_hw_csum,
+	.get_tso	= ethtool_op_get_tso,
+	.set_tso	= ethtool_op_set_tso,
+#endif
+};
+
+static int internal_dev_change_mtu(struct net_device *netdev, int new_mtu)
+{
+	if (new_mtu < 68)
+		return -EINVAL;
+
+	netdev->mtu = new_mtu;
+	return 0;
+}
+
+static int internal_dev_do_ioctl(struct net_device *dev,
+				 struct ifreq *ifr, int cmd)
+{
+	if (ovs_dp_ioctl_hook)
+		return ovs_dp_ioctl_hook(dev, ifr, cmd);
+
+	return -EOPNOTSUPP;
+}
+
+static void internal_dev_destructor(struct net_device *dev)
+{
+	struct vport *vport = ovs_internal_dev_get_vport(dev);
+
+	ovs_vport_free(vport);
+	free_netdev(dev);
+}
+
+#ifdef HAVE_NET_DEVICE_OPS
+static const struct net_device_ops internal_dev_netdev_ops = {
+	.ndo_open = internal_dev_open,
+	.ndo_stop = internal_dev_stop,
+	.ndo_start_xmit = internal_dev_xmit,
+	.ndo_set_mac_address = internal_dev_mac_addr,
+	.ndo_do_ioctl = internal_dev_do_ioctl,
+	.ndo_change_mtu = internal_dev_change_mtu,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,36)
+	.ndo_get_stats64 = internal_dev_get_stats,
+#else
+	.ndo_get_stats = internal_dev_sys_stats,
+#endif
+};
+#endif
+
+static void do_setup(struct net_device *netdev)
+{
+	ether_setup(netdev);
+
+#ifdef HAVE_NET_DEVICE_OPS
+	netdev->netdev_ops = &internal_dev_netdev_ops;
+#else
+	netdev->do_ioctl = internal_dev_do_ioctl;
+	netdev->get_stats = internal_dev_sys_stats;
+	netdev->hard_start_xmit = internal_dev_xmit;
+	netdev->open = internal_dev_open;
+	netdev->stop = internal_dev_stop;
+	netdev->set_mac_address = internal_dev_mac_addr;
+	netdev->change_mtu = internal_dev_change_mtu;
+#endif
+
+	netdev->priv_flags &= ~IFF_TX_SKB_SHARING;
+	netdev->destructor = internal_dev_destructor;
+	SET_ETHTOOL_OPS(netdev, &internal_dev_ethtool_ops);
+	netdev->tx_queue_len = 0;
+
+	netdev->features = NETIF_F_LLTX | NETIF_F_SG | NETIF_F_FRAGLIST |
+			   NETIF_F_HIGHDMA | NETIF_F_HW_CSUM | NETIF_F_TSO;
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,27)
+	netdev->vlan_features = netdev->features;
+	netdev->features |= NETIF_F_HW_VLAN_TX;
+#endif
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,39)
+	netdev->hw_features = netdev->features & ~NETIF_F_LLTX;
+#endif
+	eth_hw_addr_random(netdev);
+}
+
+static struct vport *internal_dev_create(const struct vport_parms *parms)
+{
+	struct vport *vport;
+	struct netdev_vport *netdev_vport;
+	struct internal_dev *internal_dev;
+	int err;
+
+	vport = ovs_vport_alloc(sizeof(struct netdev_vport),
+				&ovs_internal_vport_ops, parms);
+	if (IS_ERR(vport)) {
+		err = PTR_ERR(vport);
+		goto error;
+	}
+
+	netdev_vport = netdev_vport_priv(vport);
+
+	netdev_vport->dev = alloc_netdev(sizeof(struct internal_dev),
+					 parms->name, do_setup);
+	if (!netdev_vport->dev) {
+		err = -ENOMEM;
+		goto error_free_vport;
+	}
+
+	dev_net_set(netdev_vport->dev, ovs_dp_get_net(vport->dp));
+	internal_dev = internal_dev_priv(netdev_vport->dev);
+	internal_dev->vport = vport;
+
+	/* Restrict bridge port to current netns. */
+	if (vport->port_no == OVSP_LOCAL)
+		netdev_vport->dev->features |= NETIF_F_NETNS_LOCAL;
+
+	err = register_netdevice(netdev_vport->dev);
+	if (err)
+		goto error_free_netdev;
+
+	dev_set_promiscuity(netdev_vport->dev, 1);
+	netif_start_queue(netdev_vport->dev);
+
+	return vport;
+
+error_free_netdev:
+	free_netdev(netdev_vport->dev);
+error_free_vport:
+	ovs_vport_free(vport);
+error:
+	return ERR_PTR(err);
+}
+
+static void internal_dev_destroy(struct vport *vport)
+{
+	struct netdev_vport *netdev_vport = netdev_vport_priv(vport);
+
+	netif_stop_queue(netdev_vport->dev);
+	dev_set_promiscuity(netdev_vport->dev, -1);
+
+	/* unregister_netdevice() waits for an RCU grace period. */
+	unregister_netdevice(netdev_vport->dev);
+}
+
+static int internal_dev_recv(struct vport *vport, struct sk_buff *skb)
+{
+	struct net_device *netdev = netdev_vport_priv(vport)->dev;
+	int len;
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,37)
+	if (unlikely(vlan_deaccel_tag(skb)))
+		return 0;
+#endif
+
+	len = skb->len;
+
+	skb_dst_drop(skb);
+	nf_reset(skb);
+	secpath_reset(skb);
+
+	skb->dev = netdev;
+	skb->pkt_type = PACKET_HOST;
+	skb->protocol = eth_type_trans(skb, netdev);
+	forward_ip_summed(skb, false);
+
+	netif_rx(skb);
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,29)
+	netdev->last_rx = jiffies;
+#endif
+
+	return len;
+}
+
+const struct vport_ops ovs_internal_vport_ops = {
+	.type		= OVS_VPORT_TYPE_INTERNAL,
+	.flags		= VPORT_F_REQUIRED | VPORT_F_FLOW,
+	.create		= internal_dev_create,
+	.destroy	= internal_dev_destroy,
+	.set_addr	= ovs_netdev_set_addr,
+	.get_name	= ovs_netdev_get_name,
+	.get_addr	= ovs_netdev_get_addr,
+	.get_kobj	= ovs_netdev_get_kobj,
+	.get_dev_flags	= ovs_netdev_get_dev_flags,
+	.is_running	= ovs_netdev_is_running,
+	.get_operstate	= ovs_netdev_get_operstate,
+	.get_ifindex	= ovs_netdev_get_ifindex,
+	.get_mtu	= ovs_netdev_get_mtu,
+	.send		= internal_dev_recv,
+};
+
+int ovs_is_internal_dev(const struct net_device *netdev)
+{
+#ifdef HAVE_NET_DEVICE_OPS
+	return netdev->netdev_ops == &internal_dev_netdev_ops;
+#else
+	return netdev->open == internal_dev_open;
+#endif
+}
+
+struct vport *ovs_internal_dev_get_vport(struct net_device *netdev)
+{
+	if (!ovs_is_internal_dev(netdev))
+		return NULL;
+
+	return internal_dev_priv(netdev)->vport;
+}
diff --git a/drivers/staging/openvswitch_nv/vport-internal_dev.h b/drivers/staging/openvswitch_nv/vport-internal_dev.h
new file mode 100644
index 000000000000..9a7d30ecc6a2
--- /dev/null
+++ b/drivers/staging/openvswitch_nv/vport-internal_dev.h
@@ -0,0 +1,28 @@
+/*
+ * Copyright (c) 2007-2011 Nicira, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#ifndef VPORT_INTERNAL_DEV_H
+#define VPORT_INTERNAL_DEV_H 1
+
+#include "datapath.h"
+#include "vport.h"
+
+int ovs_is_internal_dev(const struct net_device *);
+struct vport *ovs_internal_dev_get_vport(struct net_device *);
+
+#endif /* vport-internal_dev.h */
diff --git a/drivers/staging/openvswitch_nv/vport-netdev.c b/drivers/staging/openvswitch_nv/vport-netdev.c
new file mode 100644
index 000000000000..eda665688d6d
--- /dev/null
+++ b/drivers/staging/openvswitch_nv/vport-netdev.c
@@ -0,0 +1,462 @@
+/*
+ * Copyright (c) 2007-2012 Nicira, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/if_arp.h>
+#include <linux/if_bridge.h>
+#include <linux/if_vlan.h>
+#include <linux/kernel.h>
+#include <linux/llc.h>
+#include <linux/rtnetlink.h>
+#include <linux/skbuff.h>
+
+#include <net/llc.h>
+
+#include "checksum.h"
+#include "datapath.h"
+#include "vlan.h"
+#include "vport-internal_dev.h"
+#include "vport-netdev.h"
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,37) && \
+	!defined(HAVE_VLAN_BUG_WORKAROUND)
+#include <linux/module.h>
+
+static int vlan_tso __read_mostly;
+module_param(vlan_tso, int, 0644);
+MODULE_PARM_DESC(vlan_tso, "Enable TSO for VLAN packets");
+#else
+#define vlan_tso true
+#endif
+
+#ifdef HAVE_RHEL_OVS_HOOK
+static atomic_t nr_bridges = ATOMIC_INIT(0);
+
+extern struct sk_buff *(*openvswitch_handle_frame_hook)(struct sk_buff *skb);
+#endif
+
+static void netdev_port_receive(struct vport *vport, struct sk_buff *skb);
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,39)
+/* Called with rcu_read_lock and bottom-halves disabled. */
+static rx_handler_result_t netdev_frame_hook(struct sk_buff **pskb)
+{
+	struct sk_buff *skb = *pskb;
+	struct vport *vport;
+
+	if (unlikely(skb->pkt_type == PACKET_LOOPBACK))
+		return RX_HANDLER_PASS;
+
+	vport = ovs_netdev_get_vport(skb->dev);
+
+	netdev_port_receive(vport, skb);
+
+	return RX_HANDLER_CONSUMED;
+}
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,36) || \
+      defined HAVE_RHEL_OVS_HOOK
+/* Called with rcu_read_lock and bottom-halves disabled. */
+static struct sk_buff *netdev_frame_hook(struct sk_buff *skb)
+{
+	struct vport *vport;
+
+	if (unlikely(skb->pkt_type == PACKET_LOOPBACK))
+		return skb;
+
+	vport = ovs_netdev_get_vport(skb->dev);
+
+	netdev_port_receive(vport, skb);
+
+	return NULL;
+}
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,22)
+/*
+ * Used as br_handle_frame_hook.  (Cannot run bridge at the same time, even on
+ * different set of devices!)
+ */
+/* Called with rcu_read_lock and bottom-halves disabled. */
+static struct sk_buff *netdev_frame_hook(struct net_bridge_port *p,
+					 struct sk_buff *skb)
+{
+	netdev_port_receive((struct vport *)p, skb);
+	return NULL;
+}
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,0)
+/*
+ * Used as br_handle_frame_hook.  (Cannot run bridge at the same time, even on
+ * different set of devices!)
+ */
+/* Called with rcu_read_lock and bottom-halves disabled. */
+static int netdev_frame_hook(struct net_bridge_port *p, struct sk_buff **pskb)
+{
+	netdev_port_receive((struct vport *)p, *pskb);
+	return 1;
+}
+#else
+#error
+#endif
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,36) || \
+    defined HAVE_RHEL_OVS_HOOK
+static int netdev_init(void) { return 0; }
+static void netdev_exit(void) { }
+#else
+static int netdev_init(void)
+{
+	/* Hook into callback used by the bridge to intercept packets.
+	 * Parasites we are. */
+	br_handle_frame_hook = netdev_frame_hook;
+
+	return 0;
+}
+
+static void netdev_exit(void)
+{
+	br_handle_frame_hook = NULL;
+}
+#endif
+
+static struct vport *netdev_create(const struct vport_parms *parms)
+{
+	struct vport *vport;
+	struct netdev_vport *netdev_vport;
+	int err;
+
+	vport = ovs_vport_alloc(sizeof(struct netdev_vport),
+				&ovs_netdev_vport_ops, parms);
+	if (IS_ERR(vport)) {
+		err = PTR_ERR(vport);
+		goto error;
+	}
+
+	netdev_vport = netdev_vport_priv(vport);
+
+	netdev_vport->dev = dev_get_by_name(ovs_dp_get_net(vport->dp), parms->name);
+	if (!netdev_vport->dev) {
+		err = -ENODEV;
+		goto error_free_vport;
+	}
+
+	if (netdev_vport->dev->flags & IFF_LOOPBACK ||
+	    netdev_vport->dev->type != ARPHRD_ETHER ||
+	    ovs_is_internal_dev(netdev_vport->dev)) {
+		err = -EINVAL;
+		goto error_put;
+	}
+
+#ifdef HAVE_RHEL_OVS_HOOK
+	rcu_assign_pointer(netdev_vport->dev->ax25_ptr, vport);
+	atomic_inc(&nr_bridges);
+	rcu_assign_pointer(openvswitch_handle_frame_hook, netdev_frame_hook);
+#else
+	err = netdev_rx_handler_register(netdev_vport->dev, netdev_frame_hook,
+					 vport);
+	if (err)
+		goto error_put;
+#endif
+
+	dev_set_promiscuity(netdev_vport->dev, 1);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,24)
+	dev_disable_lro(netdev_vport->dev);
+#endif
+	netdev_vport->dev->priv_flags |= IFF_OVS_DATAPATH;
+
+	return vport;
+
+error_put:
+	dev_put(netdev_vport->dev);
+error_free_vport:
+	ovs_vport_free(vport);
+error:
+	return ERR_PTR(err);
+}
+
+static void free_port_rcu(struct rcu_head *rcu)
+{
+	struct netdev_vport *netdev_vport = container_of(rcu,
+					struct netdev_vport, rcu);
+
+#ifdef HAVE_RHEL_OVS_HOOK
+	rcu_assign_pointer(netdev_vport->dev->ax25_ptr, NULL);
+
+	if (atomic_dec_and_test(&nr_bridges))
+		rcu_assign_pointer(openvswitch_handle_frame_hook, NULL);
+#endif
+	dev_put(netdev_vport->dev);
+	ovs_vport_free(vport_from_priv(netdev_vport));
+}
+
+static void netdev_destroy(struct vport *vport)
+{
+	struct netdev_vport *netdev_vport = netdev_vport_priv(vport);
+
+	netdev_vport->dev->priv_flags &= ~IFF_OVS_DATAPATH;
+	netdev_rx_handler_unregister(netdev_vport->dev);
+	dev_set_promiscuity(netdev_vport->dev, -1);
+
+	call_rcu(&netdev_vport->rcu, free_port_rcu);
+}
+
+int ovs_netdev_set_addr(struct vport *vport, const unsigned char *addr)
+{
+	struct netdev_vport *netdev_vport = netdev_vport_priv(vport);
+	struct sockaddr sa;
+
+	sa.sa_family = ARPHRD_ETHER;
+	memcpy(sa.sa_data, addr, ETH_ALEN);
+
+	return dev_set_mac_address(netdev_vport->dev, &sa);
+}
+
+const char *ovs_netdev_get_name(const struct vport *vport)
+{
+	const struct netdev_vport *netdev_vport = netdev_vport_priv(vport);
+	return netdev_vport->dev->name;
+}
+
+const unsigned char *ovs_netdev_get_addr(const struct vport *vport)
+{
+	const struct netdev_vport *netdev_vport = netdev_vport_priv(vport);
+	return netdev_vport->dev->dev_addr;
+}
+
+struct kobject *ovs_netdev_get_kobj(const struct vport *vport)
+{
+	const struct netdev_vport *netdev_vport = netdev_vport_priv(vport);
+	return &netdev_vport->dev->dev.kobj;
+}
+
+unsigned ovs_netdev_get_dev_flags(const struct vport *vport)
+{
+	const struct netdev_vport *netdev_vport = netdev_vport_priv(vport);
+	return dev_get_flags(netdev_vport->dev);
+}
+
+int ovs_netdev_is_running(const struct vport *vport)
+{
+	const struct netdev_vport *netdev_vport = netdev_vport_priv(vport);
+	return netif_running(netdev_vport->dev);
+}
+
+unsigned char ovs_netdev_get_operstate(const struct vport *vport)
+{
+	const struct netdev_vport *netdev_vport = netdev_vport_priv(vport);
+	return netdev_vport->dev->operstate;
+}
+
+int ovs_netdev_get_ifindex(const struct vport *vport)
+{
+	const struct netdev_vport *netdev_vport = netdev_vport_priv(vport);
+	return netdev_vport->dev->ifindex;
+}
+
+int ovs_netdev_get_mtu(const struct vport *vport)
+{
+	const struct netdev_vport *netdev_vport = netdev_vport_priv(vport);
+	return netdev_vport->dev->mtu;
+}
+
+/* Must be called with rcu_read_lock. */
+static void netdev_port_receive(struct vport *vport, struct sk_buff *skb)
+{
+	if (unlikely(!vport))
+		goto error;
+
+	if (unlikely(skb_warn_if_lro(skb)))
+		goto error;
+
+	/* Make our own copy of the packet.  Otherwise we will mangle the
+	 * packet for anyone who came before us (e.g. tcpdump via AF_PACKET).
+	 * (No one comes after us, since we tell handle_bridge() that we took
+	 * the packet.) */
+	skb = skb_share_check(skb, GFP_ATOMIC);
+	if (unlikely(!skb))
+		return;
+
+	skb_push(skb, ETH_HLEN);
+
+	if (unlikely(compute_ip_summed(skb, false)))
+		goto error;
+
+	vlan_copy_skb_tci(skb);
+
+	ovs_vport_receive(vport, skb);
+	return;
+
+error:
+	kfree_skb(skb);
+}
+
+static unsigned int packet_length(const struct sk_buff *skb)
+{
+	unsigned int length = skb->len - ETH_HLEN;
+
+	if (skb->protocol == htons(ETH_P_8021Q))
+		length -= VLAN_HLEN;
+
+	return length;
+}
+
+static bool dev_supports_vlan_tx(struct net_device *dev)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,37)
+	/* Software fallback means every device supports vlan_tci on TX. */
+	return true;
+#elif defined(HAVE_VLAN_BUG_WORKAROUND)
+	return dev->features & NETIF_F_HW_VLAN_TX;
+#else
+	/* Assume that the driver is buggy. */
+	return false;
+#endif
+}
+
+static int netdev_send(struct vport *vport, struct sk_buff *skb)
+{
+	struct netdev_vport *netdev_vport = netdev_vport_priv(vport);
+	int mtu = netdev_vport->dev->mtu;
+	int len;
+
+	if (unlikely(packet_length(skb) > mtu && !skb_is_gso(skb))) {
+		net_warn_ratelimited("%s: dropped over-mtu packet: %d > %d\n",
+				     netdev_vport->dev->name,
+				     packet_length(skb), mtu);
+		goto error;
+	}
+
+	skb->dev = netdev_vport->dev;
+	forward_ip_summed(skb, true);
+
+	if (vlan_tx_tag_present(skb) && !dev_supports_vlan_tx(skb->dev)) {
+		int features;
+
+		features = netif_skb_features(skb);
+
+		if (!vlan_tso)
+			features &= ~(NETIF_F_TSO | NETIF_F_TSO6 |
+				      NETIF_F_UFO | NETIF_F_FSO);
+
+		if (netif_needs_gso(skb, features)) {
+			struct sk_buff *nskb;
+
+			nskb = skb_gso_segment(skb, features);
+			if (!nskb) {
+				if (unlikely(skb_cloned(skb) &&
+				    pskb_expand_head(skb, 0, 0, GFP_ATOMIC))) {
+					kfree_skb(skb);
+					return 0;
+				}
+
+				skb_shinfo(skb)->gso_type &= ~SKB_GSO_DODGY;
+				goto tag;
+			}
+
+			if (IS_ERR(nskb)) {
+				kfree_skb(skb);
+				return 0;
+			}
+			consume_skb(skb);
+			skb = nskb;
+
+			len = 0;
+			do {
+				nskb = skb->next;
+				skb->next = NULL;
+
+				skb = __vlan_put_tag(skb, vlan_tx_tag_get(skb));
+				if (likely(skb)) {
+					len += skb->len;
+					vlan_set_tci(skb, 0);
+					dev_queue_xmit(skb);
+				}
+
+				skb = nskb;
+			} while (skb);
+
+			return len;
+		}
+
+tag:
+		skb = __vlan_put_tag(skb, vlan_tx_tag_get(skb));
+		if (unlikely(!skb))
+			return 0;
+		vlan_set_tci(skb, 0);
+	}
+
+	len = skb->len;
+	dev_queue_xmit(skb);
+
+	return len;
+
+error:
+	kfree_skb(skb);
+	ovs_vport_record_error(vport, VPORT_E_TX_DROPPED);
+	return 0;
+}
+
+/* Returns null if this device is not attached to a datapath. */
+struct vport *ovs_netdev_get_vport(struct net_device *dev)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,36) || \
+    defined HAVE_RHEL_OVS_HOOK
+#if IFF_OVS_DATAPATH != 0
+	if (likely(dev->priv_flags & IFF_OVS_DATAPATH))
+#else
+	if (likely(rcu_access_pointer(dev->rx_handler) == netdev_frame_hook))
+#endif
+#ifdef HAVE_RHEL_OVS_HOOK
+		return (struct vport *)rcu_dereference_rtnl(dev->ax25_ptr);
+#else
+		return (struct vport *)rcu_dereference_rtnl(dev->rx_handler_data);
+#endif
+	else
+		return NULL;
+#else
+	return (struct vport *)rcu_dereference_rtnl(dev->br_port);
+#endif
+}
+
+const struct vport_ops ovs_netdev_vport_ops = {
+	.type		= OVS_VPORT_TYPE_NETDEV,
+	.flags          = VPORT_F_REQUIRED,
+	.init		= netdev_init,
+	.exit		= netdev_exit,
+	.create		= netdev_create,
+	.destroy	= netdev_destroy,
+	.set_addr	= ovs_netdev_set_addr,
+	.get_name	= ovs_netdev_get_name,
+	.get_addr	= ovs_netdev_get_addr,
+	.get_kobj	= ovs_netdev_get_kobj,
+	.get_dev_flags	= ovs_netdev_get_dev_flags,
+	.is_running	= ovs_netdev_is_running,
+	.get_operstate	= ovs_netdev_get_operstate,
+	.get_ifindex	= ovs_netdev_get_ifindex,
+	.get_mtu	= ovs_netdev_get_mtu,
+	.send		= netdev_send,
+};
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,36) && \
+    !defined HAVE_RHEL_OVS_HOOK
+/*
+ * In kernels earlier than 2.6.36, Open vSwitch cannot safely coexist with the
+ * Linux bridge module, because there is only a single bridge hook function and
+ * only a single br_port member in struct net_device, so this prevents loading
+ * both bridge and openvswitch at the same time.
+ */
+BRIDGE_MUTUAL_EXCLUSION;
+#endif
diff --git a/drivers/staging/openvswitch_nv/vport-netdev.h b/drivers/staging/openvswitch_nv/vport-netdev.h
new file mode 100644
index 000000000000..b4f455dd3943
--- /dev/null
+++ b/drivers/staging/openvswitch_nv/vport-netdev.h
@@ -0,0 +1,52 @@
+/*
+ * Copyright (c) 2007-2011 Nicira, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#ifndef VPORT_NETDEV_H
+#define VPORT_NETDEV_H 1
+
+#include <linux/netdevice.h>
+#include <linux/rcupdate.h>
+
+#include "vport.h"
+
+struct vport *ovs_netdev_get_vport(struct net_device *dev);
+
+struct netdev_vport {
+	struct rcu_head rcu;
+
+	struct net_device *dev;
+};
+
+static inline struct netdev_vport *
+netdev_vport_priv(const struct vport *vport)
+{
+	return vport_priv(vport);
+}
+
+int ovs_netdev_set_addr(struct vport *, const unsigned char *addr);
+const char *ovs_netdev_get_name(const struct vport *);
+const unsigned char *ovs_netdev_get_addr(const struct vport *);
+const char *ovs_netdev_get_config(const struct vport *);
+struct kobject *ovs_netdev_get_kobj(const struct vport *);
+unsigned ovs_netdev_get_dev_flags(const struct vport *);
+int ovs_netdev_is_running(const struct vport *);
+unsigned char ovs_netdev_get_operstate(const struct vport *);
+int ovs_netdev_get_ifindex(const struct vport *);
+int ovs_netdev_get_mtu(const struct vport *);
+
+#endif /* vport_netdev.h */
diff --git a/drivers/staging/openvswitch_nv/vport-patch.c b/drivers/staging/openvswitch_nv/vport-patch.c
new file mode 100644
index 000000000000..c19325d846b1
--- /dev/null
+++ b/drivers/staging/openvswitch_nv/vport-patch.c
@@ -0,0 +1,315 @@
+/*
+ * Copyright (c) 2007-2012 Nicira, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#include <linux/kernel.h>
+#include <linux/jhash.h>
+#include <linux/list.h>
+#include <linux/rtnetlink.h>
+#include <linux/openvswitch-nv.h>
+#include <net/net_namespace.h>
+
+#include "datapath.h"
+#include "vport.h"
+#include "vport-generic.h"
+
+struct patch_config {
+	struct rcu_head rcu;
+
+	char peer_name[IFNAMSIZ];
+	unsigned char eth_addr[ETH_ALEN];
+};
+
+struct patch_vport {
+	struct rcu_head rcu;
+
+	char name[IFNAMSIZ];
+
+	/* Protected by RTNL lock. */
+	struct hlist_node hash_node;
+
+	struct vport __rcu *peer;
+	struct patch_config __rcu *patchconf;
+};
+
+/* Protected by RTNL lock. */
+static struct hlist_head *peer_table;
+#define PEER_HASH_BUCKETS 256
+
+static void update_peers(struct net *, const char *name, struct vport *);
+
+static struct patch_vport *patch_vport_priv(const struct vport *vport)
+{
+	return vport_priv(vport);
+}
+
+/* RCU callback. */
+static void free_config(struct rcu_head *rcu)
+{
+	struct patch_config *c = container_of(rcu, struct patch_config, rcu);
+	kfree(c);
+}
+
+static void assign_config_rcu(struct vport *vport,
+			      struct patch_config *new_config)
+{
+	struct patch_vport *patch_vport = patch_vport_priv(vport);
+	struct patch_config *old_config;
+
+	old_config = rtnl_dereference(patch_vport->patchconf);
+	rcu_assign_pointer(patch_vport->patchconf, new_config);
+	call_rcu(&old_config->rcu, free_config);
+}
+
+static struct hlist_head *hash_bucket(struct net *net, const char *name)
+{
+	unsigned int hash = jhash(name, strlen(name), (unsigned long) net);
+	return &peer_table[hash & (PEER_HASH_BUCKETS - 1)];
+}
+
+static int patch_init(void)
+{
+	peer_table = kzalloc(PEER_HASH_BUCKETS * sizeof(struct hlist_head),
+			    GFP_KERNEL);
+	if (!peer_table)
+		return -ENOMEM;
+
+	return 0;
+}
+
+static void patch_exit(void)
+{
+	kfree(peer_table);
+}
+
+static const struct nla_policy patch_policy[OVS_PATCH_ATTR_MAX + 1] = {
+#ifdef HAVE_NLA_NUL_STRING
+	[OVS_PATCH_ATTR_PEER] = { .type = NLA_NUL_STRING, .len = IFNAMSIZ - 1 },
+#endif
+};
+
+static int patch_set_config(struct vport *vport, const struct nlattr *options,
+			    struct patch_config *patchconf)
+{
+	struct patch_vport *patch_vport = patch_vport_priv(vport);
+	struct nlattr *a[OVS_PATCH_ATTR_MAX + 1];
+	const char *peer_name;
+	int err;
+
+	if (!options)
+		return -EINVAL;
+
+	err = nla_parse_nested(a, OVS_PATCH_ATTR_MAX, options, patch_policy);
+	if (err)
+		return err;
+
+	peer_name = nla_data(a[OVS_PATCH_ATTR_PEER]);
+	if (!strcmp(patch_vport->name, peer_name))
+		return -EINVAL;
+
+	strcpy(patchconf->peer_name, peer_name);
+
+	return 0;
+}
+
+static struct vport *patch_create(const struct vport_parms *parms)
+{
+	struct vport *vport;
+	struct patch_vport *patch_vport;
+	const char *peer_name;
+	struct patch_config *patchconf;
+	struct net *net = ovs_dp_get_net(parms->dp);
+	int err;
+
+	vport = ovs_vport_alloc(sizeof(struct patch_vport),
+				&ovs_patch_vport_ops, parms);
+	if (IS_ERR(vport)) {
+		err = PTR_ERR(vport);
+		goto error;
+	}
+
+	patch_vport = patch_vport_priv(vport);
+
+	strcpy(patch_vport->name, parms->name);
+
+	patchconf = kmalloc(sizeof(struct patch_config), GFP_KERNEL);
+	if (!patchconf) {
+		err = -ENOMEM;
+		goto error_free_vport;
+	}
+
+	err = patch_set_config(vport, parms->options, patchconf);
+	if (err)
+		goto error_free_patchconf;
+
+	random_ether_addr(patchconf->eth_addr);
+
+	rcu_assign_pointer(patch_vport->patchconf, patchconf);
+
+	peer_name = patchconf->peer_name;
+	hlist_add_head(&patch_vport->hash_node, hash_bucket(net, peer_name));
+	rcu_assign_pointer(patch_vport->peer, ovs_vport_locate(net, peer_name));
+	update_peers(net, patch_vport->name, vport);
+
+	return vport;
+
+error_free_patchconf:
+	kfree(patchconf);
+error_free_vport:
+	ovs_vport_free(vport);
+error:
+	return ERR_PTR(err);
+}
+
+static void free_port_rcu(struct rcu_head *rcu)
+{
+	struct patch_vport *patch_vport = container_of(rcu,
+					  struct patch_vport, rcu);
+
+	kfree((struct patch_config __force *)patch_vport->patchconf);
+	ovs_vport_free(vport_from_priv(patch_vport));
+}
+
+static void patch_destroy(struct vport *vport)
+{
+	struct patch_vport *patch_vport = patch_vport_priv(vport);
+
+	update_peers(ovs_dp_get_net(vport->dp), patch_vport->name, NULL);
+	hlist_del(&patch_vport->hash_node);
+	call_rcu(&patch_vport->rcu, free_port_rcu);
+}
+
+static int patch_set_options(struct vport *vport, struct nlattr *options)
+{
+	struct patch_vport *patch_vport = patch_vport_priv(vport);
+	struct patch_config *patchconf;
+	int err;
+
+	patchconf = kmemdup(rtnl_dereference(patch_vport->patchconf),
+			  sizeof(struct patch_config), GFP_KERNEL);
+	if (!patchconf) {
+		err = -ENOMEM;
+		goto error;
+	}
+
+	err = patch_set_config(vport, options, patchconf);
+	if (err)
+		goto error_free;
+
+	assign_config_rcu(vport, patchconf);
+
+	hlist_del(&patch_vport->hash_node);
+
+	rcu_assign_pointer(patch_vport->peer,
+		ovs_vport_locate(ovs_dp_get_net(vport->dp), patchconf->peer_name));
+
+	hlist_add_head(&patch_vport->hash_node,
+		       hash_bucket(ovs_dp_get_net(vport->dp), patchconf->peer_name));
+
+	return 0;
+error_free:
+	kfree(patchconf);
+error:
+	return err;
+}
+
+static void update_peers(struct net *net, const char *name, struct vport *vport)
+{
+	struct hlist_head *bucket = hash_bucket(net, name);
+	struct patch_vport *peer_vport;
+	struct hlist_node *node;
+
+	hlist_for_each_entry(peer_vport, node, bucket, hash_node) {
+		struct vport *curr_vport = vport_from_priv(peer_vport);
+		const char *peer_name;
+
+		peer_name = rtnl_dereference(peer_vport->patchconf)->peer_name;
+		if (!strcmp(peer_name, name) && net_eq(ovs_dp_get_net(curr_vport->dp), net))
+			rcu_assign_pointer(peer_vport->peer, vport);
+	}
+}
+
+static int patch_set_addr(struct vport *vport, const unsigned char *addr)
+{
+	struct patch_vport *patch_vport = patch_vport_priv(vport);
+	struct patch_config *patchconf;
+
+	patchconf = kmemdup(rtnl_dereference(patch_vport->patchconf),
+			  sizeof(struct patch_config), GFP_KERNEL);
+	if (!patchconf)
+		return -ENOMEM;
+
+	memcpy(patchconf->eth_addr, addr, ETH_ALEN);
+	assign_config_rcu(vport, patchconf);
+
+	return 0;
+}
+
+
+static const char *patch_get_name(const struct vport *vport)
+{
+	const struct patch_vport *patch_vport = patch_vport_priv(vport);
+	return patch_vport->name;
+}
+
+static const unsigned char *patch_get_addr(const struct vport *vport)
+{
+	const struct patch_vport *patch_vport = patch_vport_priv(vport);
+	return rcu_dereference_rtnl(patch_vport->patchconf)->eth_addr;
+}
+
+static int patch_get_options(const struct vport *vport, struct sk_buff *skb)
+{
+	struct patch_vport *patch_vport = patch_vport_priv(vport);
+	struct patch_config *patchconf = rcu_dereference_rtnl(patch_vport->patchconf);
+
+	return nla_put_string(skb, OVS_PATCH_ATTR_PEER, patchconf->peer_name);
+}
+
+static int patch_send(struct vport *vport, struct sk_buff *skb)
+{
+	struct patch_vport *patch_vport = patch_vport_priv(vport);
+	struct vport *peer = rcu_dereference(patch_vport->peer);
+	int skb_len = skb->len;
+
+	if (!peer) {
+		kfree_skb(skb);
+		ovs_vport_record_error(vport, VPORT_E_TX_DROPPED);
+
+		return 0;
+	}
+
+	ovs_vport_receive(peer, skb);
+	return skb_len;
+}
+
+const struct vport_ops ovs_patch_vport_ops = {
+	.type		= OVS_VPORT_TYPE_PATCH,
+	.init		= patch_init,
+	.exit		= patch_exit,
+	.create		= patch_create,
+	.destroy	= patch_destroy,
+	.set_addr	= patch_set_addr,
+	.get_name	= patch_get_name,
+	.get_addr	= patch_get_addr,
+	.get_options	= patch_get_options,
+	.set_options	= patch_set_options,
+	.get_dev_flags	= ovs_vport_gen_get_dev_flags,
+	.is_running	= ovs_vport_gen_is_running,
+	.get_operstate	= ovs_vport_gen_get_operstate,
+	.send		= patch_send,
+};
diff --git a/drivers/staging/openvswitch_nv/vport.c b/drivers/staging/openvswitch_nv/vport.c
new file mode 100644
index 000000000000..6184c1af7487
--- /dev/null
+++ b/drivers/staging/openvswitch_nv/vport.c
@@ -0,0 +1,530 @@
+/*
+ * Copyright (c) 2007-2012 Nicira, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#include <linux/etherdevice.h>
+#include <linux/if.h>
+#include <linux/if_vlan.h>
+#include <linux/jhash.h>
+#include <linux/kernel.h>
+#include <linux/list.h>
+#include <linux/mutex.h>
+#include <linux/percpu.h>
+#include <linux/rcupdate.h>
+#include <linux/rtnetlink.h>
+#include <linux/compat.h>
+#include <linux/version.h>
+#include <net/net_namespace.h>
+
+#include "datapath.h"
+#include "vport.h"
+#include "vport-internal_dev.h"
+
+/* List of statically compiled vport implementations.  Don't forget to also
+ * add yours to the list at the bottom of vport.h. */
+static const struct vport_ops *base_vport_ops_list[] = {
+	&ovs_netdev_vport_ops,
+	&ovs_internal_vport_ops,
+	&ovs_patch_vport_ops,
+	&ovs_gre_vport_ops,
+	&ovs_gre_ft_vport_ops,
+	&ovs_gre64_vport_ops,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,26)
+	&ovs_capwap_vport_ops,
+#endif
+};
+
+static const struct vport_ops **vport_ops_list;
+static int n_vport_types;
+
+/* Protected by RCU read lock for reading, RTNL lock for writing. */
+static struct hlist_head *dev_table;
+#define VPORT_HASH_BUCKETS 1024
+
+/**
+ *	ovs_vport_init - initialize vport subsystem
+ *
+ * Called at module load time to initialize the vport subsystem and any
+ * compiled in vport types.
+ */
+int ovs_vport_init(void)
+{
+	int err;
+	int i;
+
+	dev_table = kzalloc(VPORT_HASH_BUCKETS * sizeof(struct hlist_head),
+			    GFP_KERNEL);
+	if (!dev_table) {
+		err = -ENOMEM;
+		goto error;
+	}
+
+	vport_ops_list = kmalloc(ARRAY_SIZE(base_vport_ops_list) *
+				 sizeof(struct vport_ops *), GFP_KERNEL);
+	if (!vport_ops_list) {
+		err = -ENOMEM;
+		goto error_dev_table;
+	}
+
+	for (i = 0; i < ARRAY_SIZE(base_vport_ops_list); i++) {
+		const struct vport_ops *new_ops = base_vport_ops_list[i];
+
+		if (new_ops->init)
+			err = new_ops->init();
+		else
+			err = 0;
+
+		if (!err)
+			vport_ops_list[n_vport_types++] = new_ops;
+		else if (new_ops->flags & VPORT_F_REQUIRED) {
+			ovs_vport_exit();
+			goto error;
+		}
+	}
+
+	return 0;
+
+error_dev_table:
+	kfree(dev_table);
+error:
+	return err;
+}
+
+/**
+ *	ovs_vport_exit - shutdown vport subsystem
+ *
+ * Called at module exit time to shutdown the vport subsystem and any
+ * initialized vport types.
+ */
+void ovs_vport_exit(void)
+{
+	int i;
+
+	for (i = 0; i < n_vport_types; i++) {
+		if (vport_ops_list[i]->exit)
+			vport_ops_list[i]->exit();
+	}
+
+	kfree(vport_ops_list);
+	kfree(dev_table);
+}
+
+static struct hlist_head *hash_bucket(struct net *net, const char *name)
+{
+	unsigned int hash = jhash(name, strlen(name), (unsigned long) net);
+	return &dev_table[hash & (VPORT_HASH_BUCKETS - 1)];
+}
+
+/**
+ *	ovs_vport_locate - find a port that has already been created
+ *
+ * @name: name of port to find
+ *
+ * Must be called with RTNL or RCU read lock.
+ */
+struct vport *ovs_vport_locate(struct net *net, const char *name)
+{
+	struct hlist_head *bucket = hash_bucket(net, name);
+	struct vport *vport;
+	struct hlist_node *node;
+
+	hlist_for_each_entry_rcu(vport, node, bucket, hash_node)
+		if (!strcmp(name, vport->ops->get_name(vport)) &&
+		    net_eq(ovs_dp_get_net(vport->dp), net))
+			return vport;
+
+	return NULL;
+}
+
+static void release_vport(struct kobject *kobj)
+{
+	struct vport *p = container_of(kobj, struct vport, kobj);
+	kfree(p);
+}
+
+static struct kobj_type brport_ktype = {
+#ifdef CONFIG_SYSFS
+	.sysfs_ops = &ovs_brport_sysfs_ops,
+#endif
+	.release = release_vport
+};
+
+/**
+ *	ovs_vport_alloc - allocate and initialize new vport
+ *
+ * @priv_size: Size of private data area to allocate.
+ * @ops: vport device ops
+ *
+ * Allocate and initialize a new vport defined by @ops.  The vport will contain
+ * a private data area of size @priv_size that can be accessed using
+ * vport_priv().  vports that are no longer needed should be released with
+ * ovs_vport_free().
+ */
+struct vport *ovs_vport_alloc(int priv_size, const struct vport_ops *ops,
+			      const struct vport_parms *parms)
+{
+	struct vport *vport;
+	size_t alloc_size;
+
+	alloc_size = sizeof(struct vport);
+	if (priv_size) {
+		alloc_size = ALIGN(alloc_size, VPORT_ALIGN);
+		alloc_size += priv_size;
+	}
+
+	vport = kzalloc(alloc_size, GFP_KERNEL);
+	if (!vport)
+		return ERR_PTR(-ENOMEM);
+
+	vport->dp = parms->dp;
+	vport->port_no = parms->port_no;
+	vport->upcall_portid = parms->upcall_portid;
+	vport->ops = ops;
+	INIT_HLIST_NODE(&vport->dp_hash_node);
+
+	/* Initialize kobject for bridge.  This will be added as
+	 * /sys/class/net/<devname>/brport later, if sysfs is enabled. */
+	vport->kobj.kset = NULL;
+	kobject_init(&vport->kobj, &brport_ktype);
+
+	vport->percpu_stats = alloc_percpu(struct vport_percpu_stats);
+	if (!vport->percpu_stats) {
+		kfree(vport);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	spin_lock_init(&vport->stats_lock);
+
+	return vport;
+}
+
+/**
+ *	ovs_vport_free - uninitialize and free vport
+ *
+ * @vport: vport to free
+ *
+ * Frees a vport allocated with ovs_vport_alloc() when it is no longer needed.
+ *
+ * The caller must ensure that an RCU grace period has passed since the last
+ * time @vport was in a datapath.
+ */
+void ovs_vport_free(struct vport *vport)
+{
+	free_percpu(vport->percpu_stats);
+
+	kobject_put(&vport->kobj);
+}
+
+/**
+ *	ovs_vport_add - add vport device (for kernel callers)
+ *
+ * @parms: Information about new vport.
+ *
+ * Creates a new vport with the specified configuration (which is dependent on
+ * device type).  RTNL lock must be held.
+ */
+struct vport *ovs_vport_add(const struct vport_parms *parms)
+{
+	struct vport *vport;
+	int err = 0;
+	int i;
+
+	ASSERT_RTNL();
+
+	for (i = 0; i < n_vport_types; i++) {
+		if (vport_ops_list[i]->type == parms->type) {
+			struct hlist_head *bucket;
+
+			vport = vport_ops_list[i]->create(parms);
+			if (IS_ERR(vport)) {
+				err = PTR_ERR(vport);
+				goto out;
+			}
+
+			bucket = hash_bucket(ovs_dp_get_net(vport->dp),
+					     vport->ops->get_name(vport));
+			hlist_add_head_rcu(&vport->hash_node, bucket);
+			return vport;
+		}
+	}
+
+	err = -EAFNOSUPPORT;
+
+out:
+	return ERR_PTR(err);
+}
+
+/**
+ *	ovs_vport_set_options - modify existing vport device (for kernel callers)
+ *
+ * @vport: vport to modify.
+ * @port: New configuration.
+ *
+ * Modifies an existing device with the specified configuration (which is
+ * dependent on device type).  RTNL lock must be held.
+ */
+int ovs_vport_set_options(struct vport *vport, struct nlattr *options)
+{
+	ASSERT_RTNL();
+
+	if (!vport->ops->set_options)
+		return -EOPNOTSUPP;
+	return vport->ops->set_options(vport, options);
+}
+
+/**
+ *	ovs_vport_del - delete existing vport device
+ *
+ * @vport: vport to delete.
+ *
+ * Detaches @vport from its datapath and destroys it.  It is possible to fail
+ * for reasons such as lack of memory.  RTNL lock must be held.
+ */
+void ovs_vport_del(struct vport *vport)
+{
+	ASSERT_RTNL();
+
+	hlist_del_rcu(&vport->hash_node);
+
+	vport->ops->destroy(vport);
+}
+
+/**
+ *	ovs_vport_set_addr - set device Ethernet address (for kernel callers)
+ *
+ * @vport: vport on which to set Ethernet address.
+ * @addr: New address.
+ *
+ * Sets the Ethernet address of the given device.  Some devices may not support
+ * setting the Ethernet address, in which case the result will always be
+ * -EOPNOTSUPP.  RTNL lock must be held.
+ */
+int ovs_vport_set_addr(struct vport *vport, const unsigned char *addr)
+{
+	ASSERT_RTNL();
+
+	if (!is_valid_ether_addr(addr))
+		return -EADDRNOTAVAIL;
+
+	if (vport->ops->set_addr)
+		return vport->ops->set_addr(vport, addr);
+	else
+		return -EOPNOTSUPP;
+}
+
+/**
+ *	ovs_vport_set_stats - sets offset device stats
+ *
+ * @vport: vport on which to set stats
+ * @stats: stats to set
+ *
+ * Provides a set of transmit, receive, and error stats to be added as an
+ * offset to the collect data when stats are retreived.  Some devices may not
+ * support setting the stats, in which case the result will always be
+ * -EOPNOTSUPP.
+ *
+ * Must be called with RTNL lock.
+ */
+void ovs_vport_set_stats(struct vport *vport, struct ovs_vport_stats *stats)
+{
+	ASSERT_RTNL();
+
+	spin_lock_bh(&vport->stats_lock);
+	vport->offset_stats = *stats;
+	spin_unlock_bh(&vport->stats_lock);
+}
+
+/**
+ *	ovs_vport_get_stats - retrieve device stats
+ *
+ * @vport: vport from which to retrieve the stats
+ * @stats: location to store stats
+ *
+ * Retrieves transmit, receive, and error stats for the given device.
+ *
+ * Must be called with RTNL lock or rcu_read_lock.
+ */
+void ovs_vport_get_stats(struct vport *vport, struct ovs_vport_stats *stats)
+{
+	int i;
+
+	/* We potentially have 3 sources of stats that need to be
+	 * combined: those we have collected (split into err_stats and
+	 * percpu_stats), offset_stats from set_stats(), and device
+	 * error stats from netdev->get_stats() (for errors that happen
+	 * downstream and therefore aren't reported through our
+	 * vport_record_error() function).
+	 * Stats from first two sources are merged and reported by ovs over
+	 * OVS_VPORT_ATTR_STATS.
+	 * netdev-stats can be directly read over netlink-ioctl.
+	 */
+
+	spin_lock_bh(&vport->stats_lock);
+
+	*stats = vport->offset_stats;
+
+	stats->rx_errors	+= vport->err_stats.rx_errors;
+	stats->tx_errors	+= vport->err_stats.tx_errors;
+	stats->tx_dropped	+= vport->err_stats.tx_dropped;
+	stats->rx_dropped	+= vport->err_stats.rx_dropped;
+
+	spin_unlock_bh(&vport->stats_lock);
+
+	for_each_possible_cpu(i) {
+		const struct vport_percpu_stats *percpu_stats;
+		struct vport_percpu_stats local_stats;
+		unsigned int start;
+
+		percpu_stats = per_cpu_ptr(vport->percpu_stats, i);
+
+		do {
+			start = u64_stats_fetch_begin_bh(&percpu_stats->sync);
+			local_stats = *percpu_stats;
+		} while (u64_stats_fetch_retry_bh(&percpu_stats->sync, start));
+
+		stats->rx_bytes		+= local_stats.rx_bytes;
+		stats->rx_packets	+= local_stats.rx_packets;
+		stats->tx_bytes		+= local_stats.tx_bytes;
+		stats->tx_packets	+= local_stats.tx_packets;
+	}
+}
+
+/**
+ *	ovs_vport_get_options - retrieve device options
+ *
+ * @vport: vport from which to retrieve the options.
+ * @skb: sk_buff where options should be appended.
+ *
+ * Retrieves the configuration of the given device, appending an
+ * %OVS_VPORT_ATTR_OPTIONS attribute that in turn contains nested
+ * vport-specific attributes to @skb.
+ *
+ * Returns 0 if successful, -EMSGSIZE if @skb has insufficient room, or another
+ * negative error code if a real error occurred.  If an error occurs, @skb is
+ * left unmodified.
+ *
+ * Must be called with RTNL lock or rcu_read_lock.
+ */
+int ovs_vport_get_options(const struct vport *vport, struct sk_buff *skb)
+{
+	struct nlattr *nla;
+
+	nla = nla_nest_start(skb, OVS_VPORT_ATTR_OPTIONS);
+	if (!nla)
+		return -EMSGSIZE;
+
+	if (vport->ops->get_options) {
+		int err = vport->ops->get_options(vport, skb);
+		if (err) {
+			nla_nest_cancel(skb, nla);
+			return err;
+		}
+	}
+
+	nla_nest_end(skb, nla);
+	return 0;
+}
+
+/**
+ *	ovs_vport_receive - pass up received packet to the datapath for processing
+ *
+ * @vport: vport that received the packet
+ * @skb: skb that was received
+ *
+ * Must be called with rcu_read_lock.  The packet cannot be shared and
+ * skb->data should point to the Ethernet header.  The caller must have already
+ * called compute_ip_summed() to initialize the checksumming fields.
+ */
+void ovs_vport_receive(struct vport *vport, struct sk_buff *skb)
+{
+	struct vport_percpu_stats *stats;
+
+	stats = per_cpu_ptr(vport->percpu_stats, smp_processor_id());
+
+	u64_stats_update_begin(&stats->sync);
+	stats->rx_packets++;
+	stats->rx_bytes += skb->len;
+	u64_stats_update_end(&stats->sync);
+
+	if (!(vport->ops->flags & VPORT_F_FLOW))
+		OVS_CB(skb)->flow = NULL;
+
+	if (!(vport->ops->flags & VPORT_F_TUN_ID))
+		OVS_CB(skb)->tun_key = NULL;
+
+	ovs_dp_process_received_packet(vport, skb);
+}
+
+/**
+ *	ovs_vport_send - send a packet on a device
+ *
+ * @vport: vport on which to send the packet
+ * @skb: skb to send
+ *
+ * Sends the given packet and returns the length of data sent.  Either RTNL
+ * lock or rcu_read_lock must be held.
+ */
+int ovs_vport_send(struct vport *vport, struct sk_buff *skb)
+{
+	int sent = vport->ops->send(vport, skb);
+
+	if (likely(sent)) {
+		struct vport_percpu_stats *stats;
+
+		stats = per_cpu_ptr(vport->percpu_stats, smp_processor_id());
+
+		u64_stats_update_begin(&stats->sync);
+		stats->tx_packets++;
+		stats->tx_bytes += sent;
+		u64_stats_update_end(&stats->sync);
+	}
+	return sent;
+}
+
+/**
+ *	ovs_vport_record_error - indicate device error to generic stats layer
+ *
+ * @vport: vport that encountered the error
+ * @err_type: one of enum vport_err_type types to indicate the error type
+ *
+ * If using the vport generic stats layer indicate that an error of the given
+ * type has occured.
+ */
+void ovs_vport_record_error(struct vport *vport, enum vport_err_type err_type)
+{
+	spin_lock(&vport->stats_lock);
+
+	switch (err_type) {
+	case VPORT_E_RX_DROPPED:
+		vport->err_stats.rx_dropped++;
+		break;
+
+	case VPORT_E_RX_ERROR:
+		vport->err_stats.rx_errors++;
+		break;
+
+	case VPORT_E_TX_DROPPED:
+		vport->err_stats.tx_dropped++;
+		break;
+
+	case VPORT_E_TX_ERROR:
+		vport->err_stats.tx_errors++;
+		break;
+	}
+
+	spin_unlock(&vport->stats_lock);
+}
diff --git a/drivers/staging/openvswitch_nv/vport.h b/drivers/staging/openvswitch_nv/vport.h
new file mode 100644
index 000000000000..9a3cac297193
--- /dev/null
+++ b/drivers/staging/openvswitch_nv/vport.h
@@ -0,0 +1,261 @@
+/*
+ * Copyright (c) 2007-2012 Nicira, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#ifndef VPORT_H
+#define VPORT_H 1
+
+#include <linux/list.h>
+#include <linux/netlink.h>
+#include <linux/openvswitch-nv.h>
+#include <linux/skbuff.h>
+#include <linux/spinlock.h>
+#include <linux/u64_stats_sync.h>
+
+#include "vport-capwap.h"
+
+struct vport;
+struct vport_parms;
+
+struct vport_net {
+	struct capwap_net capwap;
+};
+
+/* The following definitions are for users of the vport subsytem: */
+
+int ovs_vport_init(void);
+void ovs_vport_exit(void);
+
+struct vport *ovs_vport_add(const struct vport_parms *);
+void ovs_vport_del(struct vport *);
+
+struct vport *ovs_vport_locate(struct net *net, const char *name);
+
+int ovs_vport_set_addr(struct vport *, const unsigned char *);
+void ovs_vport_set_stats(struct vport *, struct ovs_vport_stats *);
+void ovs_vport_get_stats(struct vport *, struct ovs_vport_stats *);
+
+int ovs_vport_set_options(struct vport *, struct nlattr *options);
+int ovs_vport_get_options(const struct vport *, struct sk_buff *);
+
+int ovs_vport_send(struct vport *, struct sk_buff *);
+
+/* The following definitions are for implementers of vport devices: */
+
+struct vport_percpu_stats {
+	u64 rx_bytes;
+	u64 rx_packets;
+	u64 tx_bytes;
+	u64 tx_packets;
+	struct u64_stats_sync sync;
+};
+
+struct vport_err_stats {
+	u64 rx_dropped;
+	u64 rx_errors;
+	u64 tx_dropped;
+	u64 tx_errors;
+};
+
+/**
+ * struct vport - one port within a datapath
+ * @rcu: RCU callback head for deferred destruction.
+ * @port_no: Index into @dp's @ports array.
+ * @dp: Datapath to which this port belongs.
+ * @kobj: Represents /sys/class/net/<devname>/brport.
+ * @linkname: The name of the link from /sys/class/net/<datapath>/brif to this
+ * &struct vport.  (We keep this around so that we can delete it if the
+ * device gets renamed.)  Set to the null string when no link exists.
+ * @upcall_portid: The Netlink port to use for packets received on this port that
+ * miss the flow table.
+ * @hash_node: Element in @dev_table hash table in vport.c.
+ * @dp_hash_node: Element in @datapath->ports hash table in datapath.c.
+ * @ops: Class structure.
+ * @percpu_stats: Points to per-CPU statistics used and maintained by vport
+ * @stats_lock: Protects @err_stats and @offset_stats.
+ * @err_stats: Points to error statistics used and maintained by vport
+ * @offset_stats: Added to actual statistics as a sop to compatibility with
+ * XAPI for Citrix XenServer.  Deprecated.
+ */
+struct vport {
+	struct rcu_head rcu;
+	u16 port_no;
+	struct datapath	*dp;
+	struct kobject kobj;
+	char linkname[IFNAMSIZ];
+	u32 upcall_portid;
+
+	struct hlist_node hash_node;
+	struct hlist_node dp_hash_node;
+	const struct vport_ops *ops;
+
+	struct vport_percpu_stats __percpu *percpu_stats;
+
+	spinlock_t stats_lock;
+	struct vport_err_stats err_stats;
+	struct ovs_vport_stats offset_stats;
+};
+
+#define VPORT_F_REQUIRED	(1 << 0) /* If init fails, module loading fails. */
+#define VPORT_F_FLOW		(1 << 1) /* Sets OVS_CB(skb)->flow. */
+#define VPORT_F_TUN_ID		(1 << 2) /* Sets OVS_CB(skb)->tun_id. */
+
+/**
+ * struct vport_parms - parameters for creating a new vport
+ *
+ * @name: New vport's name.
+ * @type: New vport's type.
+ * @options: %OVS_VPORT_ATTR_OPTIONS attribute from Netlink message, %NULL if
+ * none was supplied.
+ * @dp: New vport's datapath.
+ * @port_no: New vport's port number.
+ */
+struct vport_parms {
+	const char *name;
+	enum ovs_vport_type type;
+	struct nlattr *options;
+
+	/* For ovs_vport_alloc(). */
+	struct datapath *dp;
+	u16 port_no;
+	u32 upcall_portid;
+};
+
+/**
+ * struct vport_ops - definition of a type of virtual port
+ *
+ * @type: %OVS_VPORT_TYPE_* value for this type of virtual port.
+ * @flags: Flags of type VPORT_F_* that influence how the generic vport layer
+ * handles this vport.
+ * @init: Called at module initialization.  If VPORT_F_REQUIRED is set then the
+ * failure of this function will cause the module to not load.  If the flag is
+ * not set and initialzation fails then no vports of this type can be created.
+ * @exit: Called at module unload.
+ * @create: Create a new vport configured as specified.  On success returns
+ * a new vport allocated with ovs_vport_alloc(), otherwise an ERR_PTR() value.
+ * @destroy: Destroys a vport.  Must call vport_free() on the vport but not
+ * before an RCU grace period has elapsed.
+ * @set_options: Modify the configuration of an existing vport.  May be %NULL
+ * if modification is not supported.
+ * @get_options: Appends vport-specific attributes for the configuration of an
+ * existing vport to a &struct sk_buff.  May be %NULL for a vport that does not
+ * have any configuration.
+ * @set_addr: Set the device's MAC address.  May be null if not supported.
+ * @get_name: Get the device's name.
+ * @get_addr: Get the device's MAC address.
+ * @get_config: Get the device's configuration.
+ * @get_kobj: Get the kobj associated with the device (may return null).
+ * @get_dev_flags: Get the device's flags.
+ * @is_running: Checks whether the device is running.
+ * @get_operstate: Get the device's operating state.
+ * @get_ifindex: Get the system interface index associated with the device.
+ * May be null if the device does not have an ifindex.
+ * @get_mtu: Get the device's MTU.  May be %NULL if the device does not have an
+ * MTU (as e.g. some tunnels do not).  Must be implemented if @get_ifindex is
+ * implemented.
+ * @send: Send a packet on the device.  Returns the length of the packet sent.
+ */
+struct vport_ops {
+	enum ovs_vport_type type;
+	u32 flags;
+
+	/* Called at module init and exit respectively. */
+	int (*init)(void);
+	void (*exit)(void);
+
+	/* Called with RTNL lock. */
+	struct vport *(*create)(const struct vport_parms *);
+	void (*destroy)(struct vport *);
+
+	int (*set_options)(struct vport *, struct nlattr *);
+	int (*get_options)(const struct vport *, struct sk_buff *);
+
+	int (*set_addr)(struct vport *, const unsigned char *);
+
+	/* Called with rcu_read_lock or RTNL lock. */
+	const char *(*get_name)(const struct vport *);
+	const unsigned char *(*get_addr)(const struct vport *);
+	void (*get_config)(const struct vport *, void *);
+	struct kobject *(*get_kobj)(const struct vport *);
+
+	unsigned (*get_dev_flags)(const struct vport *);
+	int (*is_running)(const struct vport *);
+	unsigned char (*get_operstate)(const struct vport *);
+
+	int (*get_ifindex)(const struct vport *);
+
+	int (*get_mtu)(const struct vport *);
+
+	int (*send)(struct vport *, struct sk_buff *);
+};
+
+enum vport_err_type {
+	VPORT_E_RX_DROPPED,
+	VPORT_E_RX_ERROR,
+	VPORT_E_TX_DROPPED,
+	VPORT_E_TX_ERROR,
+};
+
+struct vport *ovs_vport_alloc(int priv_size, const struct vport_ops *,
+			      const struct vport_parms *);
+void ovs_vport_free(struct vport *);
+
+#define VPORT_ALIGN 8
+
+/**
+ *	vport_priv - access private data area of vport
+ *
+ * @vport: vport to access
+ *
+ * If a nonzero size was passed in priv_size of vport_alloc() a private data
+ * area was allocated on creation.  This allows that area to be accessed and
+ * used for any purpose needed by the vport implementer.
+ */
+static inline void *vport_priv(const struct vport *vport)
+{
+	return (u8 *)vport + ALIGN(sizeof(struct vport), VPORT_ALIGN);
+}
+
+/**
+ *	vport_from_priv - lookup vport from private data pointer
+ *
+ * @priv: Start of private data area.
+ *
+ * It is sometimes useful to translate from a pointer to the private data
+ * area to the vport, such as in the case where the private data pointer is
+ * the result of a hash table lookup.  @priv must point to the start of the
+ * private data area.
+ */
+static inline struct vport *vport_from_priv(const void *priv)
+{
+	return (struct vport *)(priv - ALIGN(sizeof(struct vport), VPORT_ALIGN));
+}
+
+void ovs_vport_receive(struct vport *, struct sk_buff *);
+void ovs_vport_record_error(struct vport *, enum vport_err_type err_type);
+
+/* List of statically compiled vport implementations.  Don't forget to also
+ * add yours to the list at the top of vport.c. */
+extern const struct vport_ops ovs_netdev_vport_ops;
+extern const struct vport_ops ovs_internal_vport_ops;
+extern const struct vport_ops ovs_patch_vport_ops;
+extern const struct vport_ops ovs_gre_vport_ops;
+extern const struct vport_ops ovs_gre_ft_vport_ops;
+extern const struct vport_ops ovs_gre64_vport_ops;
+extern const struct vport_ops ovs_capwap_vport_ops;
+
+#endif /* vport.h */
diff --git a/include/linux/netfilter_ipv6/ip6_tables.h b/include/linux/netfilter_ipv6/ip6_tables.h
index 1bc898b14a80..c711c70b7298 100644
--- a/include/linux/netfilter_ipv6/ip6_tables.h
+++ b/include/linux/netfilter_ipv6/ip6_tables.h
@@ -301,6 +301,10 @@ ip6t_ext_hdr(u8 nexthdr)
 /* find specified header and get offset to it */
 extern int ipv6_find_hdr(const struct sk_buff *skb, unsigned int *offset,
 			 int target, unsigned short *fragoff);
+#if IS_ENABLED(CONFIG_OPENVSWITCH_NV)
+extern int ipv6_ovs_find_hdr(const struct sk_buff *skb, unsigned int *offset,
+			     int target, unsigned short *fragoff, int *fragflg);
+#endif /* CONFIG_OPENVSWITCH_NV */
 
 #ifdef CONFIG_COMPAT
 #include <net/compat.h>
diff --git a/include/linux/openvswitch-nv.h b/include/linux/openvswitch-nv.h
new file mode 100644
index 000000000000..06d0d8219384
--- /dev/null
+++ b/include/linux/openvswitch-nv.h
@@ -0,0 +1,511 @@
+/*
+ * Copyright (c) 2007-2011 Nicira, Inc.
+ *
+ * This file is offered under your choice of two licenses: Apache 2.0 or GNU
+ * GPL 2.0 or later.  The permission statements for each of these licenses is
+ * given below.  You may license your modifications to this file under either
+ * of these licenses or both.  If you wish to license your modifications under
+ * only one of these licenses, delete the permission text for the other
+ * license.
+ *
+ * ----------------------------------------------------------------------
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at:
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ * ----------------------------------------------------------------------
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ * ----------------------------------------------------------------------
+ */
+
+#ifndef _LINUX_OPENVSWITCH_NV_H
+#define _LINUX_OPENVSWITCH_NV_H 1
+
+#include <linux/types.h>
+
+/**
+ * struct ovs_header - header for OVS Generic Netlink messages.
+ * @dp_ifindex: ifindex of local port for datapath (0 to make a request not
+ * specific to a datapath).
+ *
+ * Attributes following the header are specific to a particular OVS Generic
+ * Netlink family, but all of the OVS families use this header.
+ */
+
+struct ovs_header {
+	int dp_ifindex;
+};
+
+#define OVS_VERSION "v1.9.3"
+
+/* Datapaths. */
+
+#define OVS_DATAPATH_FAMILY  "ovs_datapath"
+#define OVS_DATAPATH_MCGROUP "ovs_datapath"
+#define OVS_DATAPATH_VERSION 0x1
+
+enum ovs_datapath_cmd {
+	OVS_DP_CMD_UNSPEC,
+	OVS_DP_CMD_NEW,
+	OVS_DP_CMD_DEL,
+	OVS_DP_CMD_GET,
+	OVS_DP_CMD_SET
+};
+
+/**
+ * enum ovs_datapath_attr - attributes for %OVS_DP_* commands.
+ * @OVS_DP_ATTR_NAME: Name of the network device that serves as the "local
+ * port".  This is the name of the network device whose dp_ifindex is given in
+ * the &struct ovs_header.  Always present in notifications.  Required in
+ * %OVS_DP_NEW requests.  May be used as an alternative to specifying
+ * dp_ifindex in other requests (with a dp_ifindex of 0).
+ * @OVS_DP_ATTR_UPCALL_PID: The Netlink socket in userspace that is initially
+ * set on the datapath port (for OVS_ACTION_ATTR_MISS).  Only valid on
+ * %OVS_DP_CMD_NEW requests. A value of zero indicates that upcalls should
+ * not be sent.
+ * @OVS_DP_ATTR_STATS: Statistics about packets that have passed through the
+ * datapath.  Always present in notifications.
+ *
+ * These attributes follow the &struct ovs_header within the Generic Netlink
+ * payload for %OVS_DP_* commands.
+ */
+enum ovs_datapath_attr {
+	OVS_DP_ATTR_UNSPEC,
+	OVS_DP_ATTR_NAME,       /* name of dp_ifindex netdev */
+	OVS_DP_ATTR_UPCALL_PID, /* Netlink PID to receive upcalls */
+	OVS_DP_ATTR_STATS,      /* struct ovs_dp_stats */
+	__OVS_DP_ATTR_MAX
+};
+
+#define OVS_DP_ATTR_MAX (__OVS_DP_ATTR_MAX - 1)
+
+struct ovs_dp_stats {
+	__u64 n_hit;             /* Number of flow table matches. */
+	__u64 n_missed;          /* Number of flow table misses. */
+	__u64 n_lost;            /* Number of misses not sent to userspace. */
+	__u64 n_flows;           /* Number of flows present */
+};
+
+struct ovs_vport_stats {
+	__u64   rx_packets;		/* total packets received       */
+	__u64   tx_packets;		/* total packets transmitted    */
+	__u64   rx_bytes;		/* total bytes received         */
+	__u64   tx_bytes;		/* total bytes transmitted      */
+	__u64   rx_errors;		/* bad packets received         */
+	__u64   tx_errors;		/* packet transmit problems     */
+	__u64   rx_dropped;		/* no space in linux buffers    */
+	__u64   tx_dropped;		/* no space available in linux  */
+};
+
+/* Fixed logical ports. */
+#define OVSP_LOCAL      ((__u16)0)
+
+/* Packet transfer. */
+
+#define OVS_PACKET_FAMILY "ovs_packet"
+#define OVS_PACKET_VERSION 0x1
+
+enum ovs_packet_cmd {
+	OVS_PACKET_CMD_UNSPEC,
+
+	/* Kernel-to-user notifications. */
+	OVS_PACKET_CMD_MISS,    /* Flow table miss. */
+	OVS_PACKET_CMD_ACTION,  /* OVS_ACTION_ATTR_USERSPACE action. */
+
+	/* Userspace commands. */
+	OVS_PACKET_CMD_EXECUTE  /* Apply actions to a packet. */
+};
+
+/**
+ * enum ovs_packet_attr - attributes for %OVS_PACKET_* commands.
+ * @OVS_PACKET_ATTR_PACKET: Present for all notifications.  Contains the entire
+ * packet as received, from the start of the Ethernet header onward.  For
+ * %OVS_PACKET_CMD_ACTION, %OVS_PACKET_ATTR_PACKET reflects changes made by
+ * actions preceding %OVS_ACTION_ATTR_USERSPACE, but %OVS_PACKET_ATTR_KEY is
+ * the flow key extracted from the packet as originally received.
+ * @OVS_PACKET_ATTR_KEY: Present for all notifications.  Contains the flow key
+ * extracted from the packet as nested %OVS_KEY_ATTR_* attributes.  This allows
+ * userspace to adapt its flow setup strategy by comparing its notion of the
+ * flow key against the kernel's.
+ * @OVS_PACKET_ATTR_ACTIONS: Contains actions for the packet.  Used
+ * for %OVS_PACKET_CMD_EXECUTE.  It has nested %OVS_ACTION_ATTR_* attributes.
+ * @OVS_PACKET_ATTR_USERDATA: Present for an %OVS_PACKET_CMD_ACTION
+ * notification if the %OVS_ACTION_ATTR_USERSPACE action specified an
+ * %OVS_USERSPACE_ATTR_USERDATA attribute.
+ *
+ * These attributes follow the &struct ovs_header within the Generic Netlink
+ * payload for %OVS_PACKET_* commands.
+ */
+enum ovs_packet_attr {
+	OVS_PACKET_ATTR_UNSPEC,
+	OVS_PACKET_ATTR_PACKET,      /* Packet data. */
+	OVS_PACKET_ATTR_KEY,         /* Nested OVS_KEY_ATTR_* attributes. */
+	OVS_PACKET_ATTR_ACTIONS,     /* Nested OVS_ACTION_ATTR_* attributes. */
+	OVS_PACKET_ATTR_USERDATA,    /* u64 OVS_ACTION_ATTR_USERSPACE arg. */
+	__OVS_PACKET_ATTR_MAX
+};
+
+#define OVS_PACKET_ATTR_MAX (__OVS_PACKET_ATTR_MAX - 1)
+
+/* Virtual ports. */
+
+#define OVS_VPORT_FAMILY  "ovs_vport"
+#define OVS_VPORT_MCGROUP "ovs_vport"
+#define OVS_VPORT_VERSION 0x1
+
+enum ovs_vport_cmd {
+	OVS_VPORT_CMD_UNSPEC,
+	OVS_VPORT_CMD_NEW,
+	OVS_VPORT_CMD_DEL,
+	OVS_VPORT_CMD_GET,
+	OVS_VPORT_CMD_SET
+};
+
+enum ovs_vport_type {
+	OVS_VPORT_TYPE_UNSPEC,
+	OVS_VPORT_TYPE_NETDEV,   /* network device */
+	OVS_VPORT_TYPE_INTERNAL, /* network device implemented by datapath */
+	OVS_VPORT_TYPE_FT_GRE,	 /* Flow based GRE tunnel. */
+	OVS_VPORT_TYPE_PATCH = 100, /* virtual tunnel connecting two vports */
+	OVS_VPORT_TYPE_GRE,      /* GRE tunnel */
+	OVS_VPORT_TYPE_CAPWAP,   /* CAPWAP tunnel */
+	OVS_VPORT_TYPE_GRE64 = 104, /* GRE tunnel with 64-bit keys */
+	__OVS_VPORT_TYPE_MAX
+};
+
+#define OVS_VPORT_TYPE_MAX (__OVS_VPORT_TYPE_MAX - 1)
+
+/**
+ * enum ovs_vport_attr - attributes for %OVS_VPORT_* commands.
+ * @OVS_VPORT_ATTR_PORT_NO: 32-bit port number within datapath.
+ * @OVS_VPORT_ATTR_TYPE: 32-bit %OVS_VPORT_TYPE_* constant describing the type
+ * of vport.
+ * @OVS_VPORT_ATTR_NAME: Name of vport.  For a vport based on a network device
+ * this is the name of the network device.  Maximum length %IFNAMSIZ-1 bytes
+ * plus a null terminator.
+ * @OVS_VPORT_ATTR_OPTIONS: Vport-specific configuration information.
+ * @OVS_VPORT_ATTR_UPCALL_PID: The Netlink socket in userspace that
+ * OVS_PACKET_CMD_MISS upcalls will be directed to for packets received on
+ * this port.  A value of zero indicates that upcalls should not be sent.
+ * @OVS_VPORT_ATTR_STATS: A &struct ovs_vport_stats giving statistics for
+ * packets sent or received through the vport.
+ * @OVS_VPORT_ATTR_ADDRESS: A 6-byte Ethernet address for the vport.
+ *
+ * These attributes follow the &struct ovs_header within the Generic Netlink
+ * payload for %OVS_VPORT_* commands.
+ *
+ * For %OVS_VPORT_CMD_NEW requests, the %OVS_VPORT_ATTR_TYPE and
+ * %OVS_VPORT_ATTR_NAME attributes are required.  %OVS_VPORT_ATTR_PORT_NO is
+ * optional; if not specified a free port number is automatically selected.
+ * Whether %OVS_VPORT_ATTR_OPTIONS is required or optional depends on the type
+ * of vport.  %OVS_VPORT_ATTR_STATS and %OVS_VPORT_ATTR_ADDRESS are optional,
+ * and other attributes are ignored.
+ *
+ * For other requests, if %OVS_VPORT_ATTR_NAME is specified then it is used to
+ * look up the vport to operate on; otherwise dp_idx from the &struct
+ * ovs_header plus %OVS_VPORT_ATTR_PORT_NO determine the vport.
+ */
+enum ovs_vport_attr {
+	OVS_VPORT_ATTR_UNSPEC,
+	OVS_VPORT_ATTR_PORT_NO,	/* u32 port number within datapath */
+	OVS_VPORT_ATTR_TYPE,	/* u32 OVS_VPORT_TYPE_* constant. */
+	OVS_VPORT_ATTR_NAME,	/* string name, up to IFNAMSIZ bytes long */
+	OVS_VPORT_ATTR_OPTIONS, /* nested attributes, varies by vport type */
+	OVS_VPORT_ATTR_UPCALL_PID, /* u32 Netlink PID to receive upcalls */
+	OVS_VPORT_ATTR_STATS,	/* struct ovs_vport_stats */
+	OVS_VPORT_ATTR_ADDRESS = 100, /* hardware address */
+	__OVS_VPORT_ATTR_MAX
+};
+
+#define OVS_VPORT_ATTR_MAX (__OVS_VPORT_ATTR_MAX - 1)
+
+/* OVS_VPORT_ATTR_OPTIONS attributes for patch vports. */
+enum {
+	OVS_PATCH_ATTR_UNSPEC,
+	OVS_PATCH_ATTR_PEER,	/* name of peer vport, as a string */
+	__OVS_PATCH_ATTR_MAX
+};
+
+#define OVS_PATCH_ATTR_MAX (__OVS_PATCH_ATTR_MAX - 1)
+
+/* Flows. */
+
+#define OVS_FLOW_FAMILY  "ovs_flow"
+#define OVS_FLOW_MCGROUP "ovs_flow"
+#define OVS_FLOW_VERSION 0x1
+
+enum ovs_flow_cmd {
+	OVS_FLOW_CMD_UNSPEC,
+	OVS_FLOW_CMD_NEW,
+	OVS_FLOW_CMD_DEL,
+	OVS_FLOW_CMD_GET,
+	OVS_FLOW_CMD_SET
+};
+
+struct ovs_flow_stats {
+	__u64 n_packets;         /* Number of matched packets. */
+	__u64 n_bytes;           /* Number of matched bytes. */
+};
+
+enum ovs_key_attr {
+	OVS_KEY_ATTR_UNSPEC,
+	OVS_KEY_ATTR_ENCAP,	/* Nested set of encapsulated attributes. */
+	OVS_KEY_ATTR_PRIORITY,  /* u32 skb->priority */
+	OVS_KEY_ATTR_IN_PORT,   /* u32 OVS dp port number */
+	OVS_KEY_ATTR_ETHERNET,  /* struct ovs_key_ethernet */
+	OVS_KEY_ATTR_VLAN,	/* be16 VLAN TCI */
+	OVS_KEY_ATTR_ETHERTYPE,	/* be16 Ethernet type */
+	OVS_KEY_ATTR_IPV4,      /* struct ovs_key_ipv4 */
+	OVS_KEY_ATTR_IPV6,      /* struct ovs_key_ipv6 */
+	OVS_KEY_ATTR_TCP,       /* struct ovs_key_tcp */
+	OVS_KEY_ATTR_UDP,       /* struct ovs_key_udp */
+	OVS_KEY_ATTR_ICMP,      /* struct ovs_key_icmp */
+	OVS_KEY_ATTR_ICMPV6,    /* struct ovs_key_icmpv6 */
+	OVS_KEY_ATTR_ARP,       /* struct ovs_key_arp */
+	OVS_KEY_ATTR_ND,        /* struct ovs_key_nd */
+	OVS_KEY_ATTR_SKB_MARK,  /* u32 skb mark */
+	OVS_KEY_ATTR_TUNNEL,	/* Nested set of ovs_tunnel attributes */
+
+#ifdef __KERNEL__
+	OVS_KEY_ATTR_IPV4_TUNNEL,  /* struct ovs_key_ipv4_tunnel */
+#endif
+	OVS_KEY_ATTR_TUN_ID = 63,  /* be64 tunnel ID */
+	__OVS_KEY_ATTR_MAX
+};
+
+#define OVS_KEY_ATTR_MAX (__OVS_KEY_ATTR_MAX - 1)
+
+enum ovs_tunnel_key_attr {
+	OVS_TUNNEL_KEY_ATTR_ID,			/* be64 Tunnel ID */
+	OVS_TUNNEL_KEY_ATTR_IPV4_SRC,		/* be32 src IP address. */
+	OVS_TUNNEL_KEY_ATTR_IPV4_DST,		/* be32 dst IP address. */
+	OVS_TUNNEL_KEY_ATTR_TOS,		/* u8 Tunnel IP ToS. */
+	OVS_TUNNEL_KEY_ATTR_TTL,		/* u8 Tunnel IP TTL. */
+	OVS_TUNNEL_KEY_ATTR_DONT_FRAGMENT,	/* No argument, set DF. */
+	OVS_TUNNEL_KEY_ATTR_CSUM,		/* No argument. CSUM packet. */
+	__OVS_TUNNEL_KEY_ATTR_MAX
+};
+
+#define OVS_TUNNEL_KEY_ATTR_MAX (__OVS_TUNNEL_KEY_ATTR_MAX - 1)
+
+/**
+ * enum ovs_frag_type - IPv4 and IPv6 fragment type
+ * @OVS_FRAG_TYPE_NONE: Packet is not a fragment.
+ * @OVS_FRAG_TYPE_FIRST: Packet is a fragment with offset 0.
+ * @OVS_FRAG_TYPE_LATER: Packet is a fragment with nonzero offset.
+ *
+ * Used as the @ipv4_frag in &struct ovs_key_ipv4 and as @ipv6_frag &struct
+ * ovs_key_ipv6.
+ */
+enum ovs_frag_type {
+	OVS_FRAG_TYPE_NONE,
+	OVS_FRAG_TYPE_FIRST,
+	OVS_FRAG_TYPE_LATER,
+	__OVS_FRAG_TYPE_MAX
+};
+
+#define OVS_FRAG_TYPE_MAX (__OVS_FRAG_TYPE_MAX - 1)
+
+struct ovs_key_ethernet {
+	__u8	 eth_src[6];
+	__u8	 eth_dst[6];
+};
+
+struct ovs_key_ipv4 {
+	__be32 ipv4_src;
+	__be32 ipv4_dst;
+	__u8   ipv4_proto;
+	__u8   ipv4_tos;
+	__u8   ipv4_ttl;
+	__u8   ipv4_frag;	/* One of OVS_FRAG_TYPE_*. */
+};
+
+struct ovs_key_ipv6 {
+	__be32 ipv6_src[4];
+	__be32 ipv6_dst[4];
+	__be32 ipv6_label;	/* 20-bits in least-significant bits. */
+	__u8   ipv6_proto;
+	__u8   ipv6_tclass;
+	__u8   ipv6_hlimit;
+	__u8   ipv6_frag;	/* One of OVS_FRAG_TYPE_*. */
+};
+
+struct ovs_key_tcp {
+	__be16 tcp_src;
+	__be16 tcp_dst;
+};
+
+struct ovs_key_udp {
+	__be16 udp_src;
+	__be16 udp_dst;
+};
+
+struct ovs_key_icmp {
+	__u8 icmp_type;
+	__u8 icmp_code;
+};
+
+struct ovs_key_icmpv6 {
+	__u8 icmpv6_type;
+	__u8 icmpv6_code;
+};
+
+struct ovs_key_arp {
+	__be32 arp_sip;
+	__be32 arp_tip;
+	__be16 arp_op;
+	__u8   arp_sha[6];
+	__u8   arp_tha[6];
+};
+
+struct ovs_key_nd {
+	__u32 nd_target[4];
+	__u8  nd_sll[6];
+	__u8  nd_tll[6];
+};
+
+/**
+ * enum ovs_flow_attr - attributes for %OVS_FLOW_* commands.
+ * @OVS_FLOW_ATTR_KEY: Nested %OVS_KEY_ATTR_* attributes specifying the flow
+ * key.  Always present in notifications.  Required for all requests (except
+ * dumps).
+ * @OVS_FLOW_ATTR_ACTIONS: Nested %OVS_ACTION_ATTR_* attributes specifying
+ * the actions to take for packets that match the key.  Always present in
+ * notifications.  Required for %OVS_FLOW_CMD_NEW requests, optional for
+ * %OVS_FLOW_CMD_SET requests.
+ * @OVS_FLOW_ATTR_STATS: &struct ovs_flow_stats giving statistics for this
+ * flow.  Present in notifications if the stats would be nonzero.  Ignored in
+ * requests.
+ * @OVS_FLOW_ATTR_TCP_FLAGS: An 8-bit value giving the OR'd value of all of the
+ * TCP flags seen on packets in this flow.  Only present in notifications for
+ * TCP flows, and only if it would be nonzero.  Ignored in requests.
+ * @OVS_FLOW_ATTR_USED: A 64-bit integer giving the time, in milliseconds on
+ * the system monotonic clock, at which a packet was last processed for this
+ * flow.  Only present in notifications if a packet has been processed for this
+ * flow.  Ignored in requests.
+ * @OVS_FLOW_ATTR_CLEAR: If present in a %OVS_FLOW_CMD_SET request, clears the
+ * last-used time, accumulated TCP flags, and statistics for this flow.
+ * Otherwise ignored in requests.  Never present in notifications.
+ *
+ * These attributes follow the &struct ovs_header within the Generic Netlink
+ * payload for %OVS_FLOW_* commands.
+ */
+enum ovs_flow_attr {
+	OVS_FLOW_ATTR_UNSPEC,
+	OVS_FLOW_ATTR_KEY,       /* Sequence of OVS_KEY_ATTR_* attributes. */
+	OVS_FLOW_ATTR_ACTIONS,   /* Nested OVS_ACTION_ATTR_* attributes. */
+	OVS_FLOW_ATTR_STATS,     /* struct ovs_flow_stats. */
+	OVS_FLOW_ATTR_TCP_FLAGS, /* 8-bit OR'd TCP flags. */
+	OVS_FLOW_ATTR_USED,      /* u64 msecs last used in monotonic time. */
+	OVS_FLOW_ATTR_CLEAR,     /* Flag to clear stats, tcp_flags, used. */
+	__OVS_FLOW_ATTR_MAX
+};
+
+#define OVS_FLOW_ATTR_MAX (__OVS_FLOW_ATTR_MAX - 1)
+
+/**
+ * enum ovs_sample_attr - Attributes for %OVS_ACTION_ATTR_SAMPLE action.
+ * @OVS_SAMPLE_ATTR_PROBABILITY: 32-bit fraction of packets to sample with
+ * @OVS_ACTION_ATTR_SAMPLE.  A value of 0 samples no packets, a value of
+ * %UINT32_MAX samples all packets and intermediate values sample intermediate
+ * fractions of packets.
+ * @OVS_SAMPLE_ATTR_ACTIONS: Set of actions to execute in sampling event.
+ * Actions are passed as nested attributes.
+ *
+ * Executes the specified actions with the given probability on a per-packet
+ * basis.
+ */
+enum ovs_sample_attr {
+	OVS_SAMPLE_ATTR_UNSPEC,
+	OVS_SAMPLE_ATTR_PROBABILITY, /* u32 number */
+	OVS_SAMPLE_ATTR_ACTIONS,     /* Nested OVS_ACTION_ATTR_* attributes. */
+	__OVS_SAMPLE_ATTR_MAX,
+};
+
+#define OVS_SAMPLE_ATTR_MAX (__OVS_SAMPLE_ATTR_MAX - 1)
+
+/**
+ * enum ovs_userspace_attr - Attributes for %OVS_ACTION_ATTR_USERSPACE action.
+ * @OVS_USERSPACE_ATTR_PID: u32 Netlink PID to which the %OVS_PACKET_CMD_ACTION
+ * message should be sent.  Required.
+ * @OVS_USERSPACE_ATTR_USERDATA: If present, its u64 argument is copied to the
+ * %OVS_PACKET_CMD_ACTION message as %OVS_PACKET_ATTR_USERDATA,
+ */
+enum ovs_userspace_attr {
+	OVS_USERSPACE_ATTR_UNSPEC,
+	OVS_USERSPACE_ATTR_PID,	      /* u32 Netlink PID to receive upcalls. */
+	OVS_USERSPACE_ATTR_USERDATA,  /* u64 optional user-specified cookie. */
+	__OVS_USERSPACE_ATTR_MAX
+};
+
+#define OVS_USERSPACE_ATTR_MAX (__OVS_USERSPACE_ATTR_MAX - 1)
+
+/**
+ * struct ovs_action_push_vlan - %OVS_ACTION_ATTR_PUSH_VLAN action argument.
+ * @vlan_tpid: Tag protocol identifier (TPID) to push.
+ * @vlan_tci: Tag control identifier (TCI) to push.  The CFI bit must be set
+ * (but it will not be set in the 802.1Q header that is pushed).
+ *
+ * The @vlan_tpid value is typically %ETH_P_8021Q.  The only acceptable TPID
+ * values are those that the kernel module also parses as 802.1Q headers, to
+ * prevent %OVS_ACTION_ATTR_PUSH_VLAN followed by %OVS_ACTION_ATTR_POP_VLAN
+ * from having surprising results.
+ */
+struct ovs_action_push_vlan {
+	__be16 vlan_tpid;	/* 802.1Q TPID. */
+	__be16 vlan_tci;	/* 802.1Q TCI (VLAN ID and priority). */
+};
+
+/**
+ * enum ovs_action_attr - Action types.
+ *
+ * @OVS_ACTION_ATTR_OUTPUT: Output packet to port.
+ * @OVS_ACTION_ATTR_USERSPACE: Send packet to userspace according to nested
+ * %OVS_USERSPACE_ATTR_* attributes.
+ * @OVS_ACTION_ATTR_SET: Replaces the contents of an existing header.  The
+ * single nested %OVS_KEY_ATTR_* attribute specifies a header to modify and its
+ * value.
+ * @OVS_ACTION_ATTR_PUSH_VLAN: Push a new outermost 802.1Q header onto the
+ * packet.
+ * @OVS_ACTION_ATTR_POP_VLAN: Pop the outermost 802.1Q header off the packet.
+ * @OVS_ACTION_ATTR_SAMPLE: Probabilitically executes actions, as specified in
+ * the nested %OVS_SAMPLE_ATTR_* attributes.
+ *
+ * Only a single header can be set with a single %OVS_ACTION_ATTR_SET.  Not all
+ * fields within a header are modifiable, e.g. the IPv4 protocol and fragment
+ * type may not be changed.
+ */
+
+enum ovs_action_attr {
+	OVS_ACTION_ATTR_UNSPEC,
+	OVS_ACTION_ATTR_OUTPUT,	      /* u32 port number. */
+	OVS_ACTION_ATTR_USERSPACE,    /* Nested OVS_USERSPACE_ATTR_*. */
+	OVS_ACTION_ATTR_SET,          /* One nested OVS_KEY_ATTR_*. */
+	OVS_ACTION_ATTR_PUSH_VLAN,    /* struct ovs_action_push_vlan. */
+	OVS_ACTION_ATTR_POP_VLAN,     /* No argument. */
+	OVS_ACTION_ATTR_SAMPLE,       /* Nested OVS_SAMPLE_ATTR_*. */
+	__OVS_ACTION_ATTR_MAX
+};
+
+#define OVS_ACTION_ATTR_MAX (__OVS_ACTION_ATTR_MAX - 1)
+
+#endif /* _LINUX_OPENVSWITCH_NV_H */
diff --git a/include/net/ipv6.h b/include/net/ipv6.h
index fa7af9183dc9..ea9477f65ab1 100644
--- a/include/net/ipv6.h
+++ b/include/net/ipv6.h
@@ -230,6 +230,14 @@ struct ip6_flowlabel {
 	struct net		*fl_net;
 };
 
+#if IS_ENABLED(CONFIG_OPENVSWITCH_NV)
+enum {
+  OVS_IP6T_FH_F_FRAG      = (1 << 0),
+  OVS_IP6T_FH_F_AUTH      = (1 << 1),
+  OVS_IP6T_FH_F_SKIP_RH   = (1 << 2),
+};
+#endif /* CONFIG_OPENVSWITCH_NV */
+
 #define IPV6_FLOWINFO_MASK	cpu_to_be32(0x0FFFFFFF)
 #define IPV6_FLOWLABEL_MASK	cpu_to_be32(0x000FFFFF)
 
diff --git a/include/net/netlink.h b/include/net/netlink.h
index f394fe5d7641..6b3663bdd081 100644
--- a/include/net/netlink.h
+++ b/include/net/netlink.h
@@ -1069,4 +1069,21 @@ static inline int nla_validate_nested(const struct nlattr *start, int maxtype,
 #define nla_for_each_nested(pos, nla, rem) \
 	nla_for_each_attr(pos, nla_data(nla), nla_len(nla), rem)
 
+#if IS_ENABLED(CONFIG_OPENVSWITCH_NV)
+static inline int nla_put_be16(struct sk_buff *skb, int attrtype, __be16 value)
+{
+  return nla_put(skb, attrtype, sizeof(__be16), &value);
+}
+
+static inline int nla_put_be32(struct sk_buff *skb, int attrtype, __be32 value)
+{
+  return nla_put(skb, attrtype, sizeof(__be32), &value);
+}
+
+static inline int nla_put_be64(struct sk_buff *skb, int attrtype, __be64 value)
+{
+  return nla_put(skb, attrtype, sizeof(__be64), &value);
+}
+#endif /* CONFIG_OPENVSWITCH_NV */
+
 #endif
diff --git a/include/openvswitch/tunnel.h b/include/openvswitch/tunnel.h
new file mode 100644
index 000000000000..f6e40c8b9039
--- /dev/null
+++ b/include/openvswitch/tunnel.h
@@ -0,0 +1,76 @@
+/*
+ * Copyright (c) 2008, 2009, 2010, 2011 Nicira, Inc.
+ *
+ * This file is offered under your choice of two licenses: Apache 2.0 or GNU
+ * GPL 2.0 or later.  The permission statements for each of these licenses is
+ * given below.  You may license your modifications to this file under either
+ * of these licenses or both.  If you wish to license your modifications under
+ * only one of these licenses, delete the permission text for the other
+ * license.
+ *
+ * ----------------------------------------------------------------------
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at:
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ * ----------------------------------------------------------------------
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
+ * ----------------------------------------------------------------------
+ */
+
+#ifndef OPENVSWITCH_TUNNEL_H
+#define OPENVSWITCH_TUNNEL_H 1
+
+#include <linux/types.h>
+#include <linux/openvswitch-nv.h>
+
+/* OVS_VPORT_ATTR_OPTIONS attributes for tunnels.
+ *
+ * OVS_TUNNEL_ATTR_DST_IPV4 is required for kernel tunnel ports, all other
+ * attributes are optional.
+ * For flow-based tunnels, none of the options apply.
+ */
+enum {
+	OVS_TUNNEL_ATTR_UNSPEC,
+	OVS_TUNNEL_ATTR_FLAGS,    /* 32-bit TNL_F_*. */
+	OVS_TUNNEL_ATTR_DST_IPV4, /* Remote IPv4 address. */
+	OVS_TUNNEL_ATTR_SRC_IPV4, /* Local IPv4 address. */
+	OVS_TUNNEL_ATTR_OUT_KEY,  /* __be64 key to use on output. */
+	OVS_TUNNEL_ATTR_IN_KEY,   /* __be64 key to match on input. */
+	OVS_TUNNEL_ATTR_TOS,      /* 8-bit TOS value. */
+	OVS_TUNNEL_ATTR_TTL,      /* 8-bit TTL value. */
+	__OVS_TUNNEL_ATTR_MAX
+};
+
+#define OVS_TUNNEL_ATTR_MAX (__OVS_TUNNEL_ATTR_MAX - 1)
+
+#define TNL_F_CSUM		(1 << 0) /* Checksum packets. */
+#define TNL_F_TOS_INHERIT	(1 << 1) /* Inherit ToS from inner packet. */
+#define TNL_F_TTL_INHERIT	(1 << 2) /* Inherit TTL from inner packet. */
+#define TNL_F_DF_INHERIT	(1 << 3) /* Inherit DF bit from inner packet. */
+#define TNL_F_DF_DEFAULT	(1 << 4) /* Set DF bit if inherit off or
+					  * not IP. */
+/* Bit 6 is reserved since it was previously used for Tunnel header caching. */
+#define TNL_F_PMTUD		(1 << 5) /* Enable path MTU discovery. */
+#define TNL_F_IPSEC		(1 << 7) /* Traffic is IPsec encrypted. */
+
+#endif /* openvswitch/tunnel.h */
diff --git a/net/ipv6/netfilter/ip6_tables.c b/net/ipv6/netfilter/ip6_tables.c
index 9d4e15559319..913938f5acd3 100644
--- a/net/ipv6/netfilter/ip6_tables.c
+++ b/net/ipv6/netfilter/ip6_tables.c
@@ -2350,6 +2350,132 @@ int ipv6_find_hdr(const struct sk_buff *skb, unsigned int *offset,
 	return nexthdr;
 }
 
+#if IS_ENABLED(CONFIG_OPENVSWITCH_NV)
+/*
+ * find the offset to specified header or the protocol number of last header
+ * if target < 0. "last header" is transport protocol header, ESP, or
+ * "No next header".
+ *
+ * Note that *offset is used as input/output parameter. an if it is not zero,
+ * then it must be a valid offset to an inner IPv6 header. This can be used
+ * to explore inner IPv6 header, eg. ICMPv6 error messages.
+ *
+ * If target header is found, its offset is set in *offset and return protocol
+ * number. Otherwise, return -1.
+ *
+ * If the first fragment doesn't contain the final protocol header or
+ * NEXTHDR_NONE it is considered invalid.
+ *
+ * Note that non-1st fragment is special case that "the protocol number
+ * of last header" is "next header" field in Fragment header. In this case,
+ * *offset is meaningless and fragment offset is stored in *fragoff if fragoff
+ * isn't NULL.
+ *
+ * if flags is not NULL and it's a fragment, then the frag flag
+ * OVS_IP6T_FH_F_FRAG will be set. If it's an AH header, the
+ * OVS_IP6T_FH_F_AUTH flag is set and target < 0, then this function will
+ * stop at the AH header. If OVS_IP6T_FH_F_SKIP_RH flag was passed, then this
+ * function will skip all those routing headers, where segements_left was 0.
+ */
+int ipv6_ovs_find_hdr(const struct sk_buff *skb, unsigned int *offset,
+		      int target, unsigned short *fragoff, int *flags)
+{
+	unsigned int start = skb_network_offset(skb) + sizeof(struct ipv6hdr);
+	u8 nexthdr = ipv6_hdr(skb)->nexthdr;
+	unsigned int len;
+	bool found;
+
+	if (fragoff)
+		*fragoff = 0;
+
+	if (*offset) {
+		struct ipv6hdr _ip6, *ip6;
+
+		ip6 = skb_header_pointer(skb, *offset, sizeof(_ip6), &_ip6);
+		if (!ip6 || (ip6->version != 6)) {
+			printk(KERN_ERR "IPv6 header not found\n");
+			return -EBADMSG;
+		}
+		start = *offset + sizeof(struct ipv6hdr);
+		nexthdr = ip6->nexthdr;
+	}
+	len = skb->len - start;
+
+	do {
+		struct ipv6_opt_hdr _hdr, *hp;
+		unsigned int hdrlen;
+		found = (nexthdr == target);
+
+		if ((!ipv6_ext_hdr(nexthdr)) || nexthdr == NEXTHDR_NONE) {
+			if (target < 0)
+				break;
+			return -ENOENT;
+		}
+
+		hp = skb_header_pointer(skb, start, sizeof(_hdr), &_hdr);
+		if (hp == NULL)
+			return -EBADMSG;
+
+		if (nexthdr == NEXTHDR_ROUTING) {
+			struct ipv6_rt_hdr _rh, *rh;
+
+			rh = skb_header_pointer(skb, start, sizeof(_rh),
+						&_rh);
+			if (rh == NULL)
+				return -EBADMSG;
+
+			if (flags && (*flags & OVS_IP6T_FH_F_SKIP_RH) &&
+			    rh->segments_left == 0)
+				found = false;
+		}
+
+		if (nexthdr == NEXTHDR_FRAGMENT) {
+			unsigned short _frag_off;
+			__be16 *fp;
+
+			if (flags)	/* Indicate that this is a fragment */
+				*flags |= OVS_IP6T_FH_F_FRAG;
+			fp = skb_header_pointer(skb,
+						start+offsetof(struct frag_hdr,
+							       frag_off),
+						sizeof(_frag_off),
+						&_frag_off);
+			if (fp == NULL)
+				return -EBADMSG;
+
+			_frag_off = ntohs(*fp) & ~0x7;
+			if (_frag_off) {
+				if (target < 0 &&
+				    ((!ipv6_ext_hdr(hp->nexthdr)) ||
+				     hp->nexthdr == NEXTHDR_NONE)) {
+					if (fragoff)
+						*fragoff = _frag_off;
+					return hp->nexthdr;
+				}
+				return -ENOENT;
+			}
+			hdrlen = 8;
+		} else if (nexthdr == NEXTHDR_AUTH) {
+			if (flags && (*flags & OVS_IP6T_FH_F_AUTH) &&
+			    (target < 0))
+				break;
+			hdrlen = (hp->hdrlen + 2) << 2;
+		} else
+			hdrlen = ipv6_optlen(hp);
+
+		if (!found) {
+			nexthdr = hp->nexthdr;
+			len -= hdrlen;
+			start += hdrlen;
+		}
+	} while (!found);
+
+	*offset = start;
+	return nexthdr;
+}
+EXPORT_SYMBOL(ipv6_ovs_find_hdr);
+#endif /* CONFIG_OPENVSWITCH_NV */
+
 EXPORT_SYMBOL(ip6t_register_table);
 EXPORT_SYMBOL(ip6t_unregister_table);
 EXPORT_SYMBOL(ip6t_do_table);
-- 
1.9.0

