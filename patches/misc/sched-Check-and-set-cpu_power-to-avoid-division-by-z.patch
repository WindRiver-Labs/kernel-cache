From 48b0a23132de6b1f793ee56c5800fa1edf1b560d Mon Sep 17 00:00:00 2001
From: Zhang Xiao <xiao.zhang@windriver.com>
Date: Tue, 29 Sep 2015 10:30:38 +0800
Subject: [PATCH] sched: Check and set cpu_power to avoid division by zero

In some cases, cpu_power maybe divided without initialization. Check
and set it to avoid it.

Patch comes from
https://bugzilla.redhat.com/attachment.cgi?id=481395&action=diff

Signed-off-by: xiao zhang <xzhang1@ala-lpggp1.wrs.com>
---
 kernel/sched_fair.c |   32 ++++++++++++++++++++++++++++++++
 1 files changed, 32 insertions(+), 0 deletions(-)

diff --git a/kernel/sched_fair.c b/kernel/sched_fair.c
index bc8a05f..929b120 100644
--- a/kernel/sched_fair.c
+++ b/kernel/sched_fair.c
@@ -2514,6 +2514,13 @@ static inline void update_sg_lb_stats(struct sched_domain *sd,
 	update_group_power(sd, this_cpu);
 
 	/* Adjust by relative CPU power of the group */
+
+	/* can happen at 'init 0' power down */
+	if (group->cpu_power == 0) {
+		sched_warn_zero_power(group);
+		group->cpu_power = 1;
+	}
+
 	sgs->avg_load = (sgs->group_load * SCHED_LOAD_SCALE) / sched_group_power(group);
 
 	/*
@@ -2564,6 +2571,13 @@ static inline void update_sd_lb_stats(struct sched_domain *sd, int this_cpu,
 	do {
 		int local_group;
 
+		/* this can happen during 'init 0' power down */
+		if (!group) {
+			WARN_ONCE(1, "%s %d group is not set.\n",
+				  __func__, __LINE__);
+			return;
+		}
+
 		local_group = cpumask_test_cpu(this_cpu,
 					       sched_group_cpus(group));
 		memset(&sgs, 0, sizeof(sgs));
@@ -2631,6 +2645,10 @@ static inline void fix_small_imbalance(struct sd_lb_stats *sds,
 
 	scaled_busy_load_per_task = sds->busiest_load_per_task
 						 * SCHED_LOAD_SCALE;
+	if (sds->busiest->cpu_power == 0) {
+		sched_warn_zero_power(sds->busiest);
+		sds->busiest->cpu_power = 1;
+	}
 	scaled_busy_load_per_task /= sds->busiest->cpu_power;
 
 	if (sds->max_load - sds->this_load + scaled_busy_load_per_task >=
@@ -2687,6 +2705,12 @@ static inline void calculate_imbalance(struct sd_lb_stats *sds, int this_cpu,
 {
 	unsigned long max_pull, load_above_capacity = ~0UL;
 
+	if (sds->busiest_nr_running == 0) {
+		WARN_ONCE(1, "%s %d busiest_nr_running is 0\n",
+			  __func__, __LINE__);
+		sds->busiest_nr_running = 1;
+	}
+
 	sds->busiest_load_per_task /= sds->busiest_nr_running;
 	if (sds->group_imb) {
 		sds->busiest_load_per_task =
@@ -2712,6 +2736,10 @@ static inline void calculate_imbalance(struct sd_lb_stats *sds, int this_cpu,
 
 		load_above_capacity *= (SCHED_LOAD_SCALE * SCHED_LOAD_SCALE);
 
+		if (sds->busiest->cpu_power == 0) {
+			sched_warn_zero_power(sds->busiest);
+			sds->busiest->cpu_power = 1;
+		}
 		load_above_capacity /= sds->busiest->cpu_power;
 	}
 
@@ -2803,6 +2831,10 @@ find_busiest_group(struct sched_domain *sd, int this_cpu,
 	if (sds.this_load >= sds.max_load)
 		goto out_balanced;
 
+	if (sds.total_pwr == 0) {
+		WARN_ONCE(1, "%s %d total_pwr is 0\n", __func__, __LINE__);
+		sds.total_pwr = 1;
+	}
 	sds.avg_load = (SCHED_LOAD_SCALE * sds.total_load) / sds.total_pwr;
 
 	if (sds.this_load >= sds.avg_load)
-- 
1.7.5.4

