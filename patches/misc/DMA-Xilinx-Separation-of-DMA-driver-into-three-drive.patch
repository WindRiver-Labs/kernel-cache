From 06e8840ea60e8148452e8d74bd709be5bfc3eacb Mon Sep 17 00:00:00 2001
From: Srikanth Thokala <srikanth.thokala@xilinx.com>
Date: Wed, 14 Nov 2012 23:40:32 +0530
Subject: [PATCH 103/628] DMA: Xilinx: Separation of DMA driver into three
 drivers

git://github.com/Xilinx/linux-xlnx.git xilinx-v14.7
commit 60ea42affdc49e30cc41a12ff50b53c4871714fe

This patch separates the existing Xilinx DMA driver, which is
currently common driver for the three IPs AXI-DMA, AXI-VDMA, AXI-CDMA,
into three individual IP drivers. It also adds three test clients for
each DMA IP.

Signed-off-by: Srikanth Thokala <sthokal@xilinx.com>
Acked-by: Michal Simek <michal.simek@xilinx.com>
Signed-off-by: Liming Wang <liming.wang@windriver.com>
---
 .../devicetree/bindings/dma/xilinx/axi-cdma.txt    |   18 +
 .../devicetree/bindings/dma/xilinx/axi-dma.txt     |   28 +
 .../devicetree/bindings/dma/xilinx/axi-vdma.txt    |   28 +
 drivers/dma/Kconfig                                |    2 +
 drivers/dma/Makefile                               |    1 +
 drivers/dma/xilinx/Kconfig                         |   53 +
 drivers/dma/xilinx/Makefile                        |    6 +
 drivers/dma/xilinx/cdmatest.c                      |  644 ++++++++++
 drivers/dma/xilinx/dmatest.c                       |  649 ++++++++++
 drivers/dma/xilinx/vdmatest.c                      |  620 +++++++++
 drivers/dma/xilinx/xilinx_axicdma.c                | 1095 ++++++++++++++++
 drivers/dma/xilinx/xilinx_axidma.c                 | 1201 ++++++++++++++++++
 drivers/dma/xilinx/xilinx_axivdma.c                | 1321 ++++++++++++++++++++
 include/linux/amba/xilinx_dma.h                    |   93 +-
 14 files changed, 5709 insertions(+), 50 deletions(-)
 create mode 100644 Documentation/devicetree/bindings/dma/xilinx/axi-cdma.txt
 create mode 100644 Documentation/devicetree/bindings/dma/xilinx/axi-dma.txt
 create mode 100644 Documentation/devicetree/bindings/dma/xilinx/axi-vdma.txt
 create mode 100644 drivers/dma/xilinx/Kconfig
 create mode 100644 drivers/dma/xilinx/Makefile
 create mode 100644 drivers/dma/xilinx/cdmatest.c
 create mode 100644 drivers/dma/xilinx/dmatest.c
 create mode 100644 drivers/dma/xilinx/vdmatest.c
 create mode 100644 drivers/dma/xilinx/xilinx_axicdma.c
 create mode 100644 drivers/dma/xilinx/xilinx_axidma.c
 create mode 100644 drivers/dma/xilinx/xilinx_axivdma.c

diff --git a/Documentation/devicetree/bindings/dma/xilinx/axi-cdma.txt b/Documentation/devicetree/bindings/dma/xilinx/axi-cdma.txt
new file mode 100644
index 0000000..9035626
--- /dev/null
+++ b/Documentation/devicetree/bindings/dma/xilinx/axi-cdma.txt
@@ -0,0 +1,18 @@
+Xilinx AXI CDMA engine, it does transfers between memory and memory
+
+Required properties:
+- compatible: Should be "xlnx,axi-cdma"
+- reg: Should contain CDMA registers location and length.
+- interrupts: Should contain channel CDMA interrupts.
+
+Example:
+++++++++
+
+axi_cdma_0: axicdma@40030000 {
+	compatible = "xlnx,axi-cdma";
+	reg = < 0x40030000 0x10000 >;
+	dma-channel@40030000 {
+		interrupts = < 0 59 4 >;
+	} ;
+} ;
+
diff --git a/Documentation/devicetree/bindings/dma/xilinx/axi-dma.txt b/Documentation/devicetree/bindings/dma/xilinx/axi-dma.txt
new file mode 100644
index 0000000..4b474a2
--- /dev/null
+++ b/Documentation/devicetree/bindings/dma/xilinx/axi-dma.txt
@@ -0,0 +1,28 @@
+Xilinx AXI DMA engine, it does transfers between memory and device. It can be
+configured to have one channel or two channels. If configured as two
+channels, one is to transmit to device and another is to receive from
+device.
+
+Required properties:
+- compatible: Should be "xlnx,axi-dma"
+- reg: Should contain DMA registers location and length.
+- interrupts: Should contain per channel DMA interrupts.
+- compatible (child node): It should be either "xlnx,axi-dma-mm2s-channel" or
+	"xlnx,axi-dma-s2mm-channel". It depends on the hardware design and it
+	can also have both channels.
+
+Example:
+++++++++
+
+axi_dma_0: axidma@40400000 {
+	compatible = "xlnx,axi-dma";
+	reg = < 0x40400000 0x10000 >;
+	dma-channel@40400000 {
+		compatible = "xlnx,axi-dma-mm2s-channel";
+		interrupts = < 0 59 4 >;
+	} ;
+	dma-channel@40030030 {
+		compatible = "xlnx,axi-dma-s2mm-channel";
+		interrupts = < 0 58 4 >;
+	} ;
+} ;
diff --git a/Documentation/devicetree/bindings/dma/xilinx/axi-vdma.txt b/Documentation/devicetree/bindings/dma/xilinx/axi-vdma.txt
new file mode 100644
index 0000000..a9cccf1
--- /dev/null
+++ b/Documentation/devicetree/bindings/dma/xilinx/axi-vdma.txt
@@ -0,0 +1,28 @@
+Xilinx AXI VDMA engine, it does transfers between memory and video devices.
+It can be configured to have one channel or two channels. If configured
+as two channels, one is to transmit to the video device and another is
+to receive from the video device.
+
+Required properties:
+- compatible: Should be "xlnx,axi-vdma"
+- reg: Should contain VDMA registers location and length.
+- interrupts: Should contain per channel VDMA interrupts.
+- compatible (child node): It should be either "xlnx,axi-vdma-mm2s-channel" or
+	"xlnx,axi-vdma-s2mm-channel". It depends on the hardware design and it
+	can also have both channels.
+
+Example:
+++++++++
+
+axi_vdma_0: axivdma@40030000 {
+	compatible = "xlnx,axi-vdma";
+	reg = < 0x40030000 0x10000 >;
+	dma-channel@40030000 {
+		compatible = "xlnx,axi-vdma-mm2s-channel";
+		interrupts = < 0 54 4 >;
+	} ;
+	dma-channel@40030030 {
+		compatible = "xlnx,axi-vdma-s2mm-channel";
+		interrupts = < 0 53 4 >;
+	} ;
+} ;
diff --git a/drivers/dma/Kconfig b/drivers/dma/Kconfig
index 9e12107..b66691e 100644
--- a/drivers/dma/Kconfig
+++ b/drivers/dma/Kconfig
@@ -33,6 +33,8 @@ if DMADEVICES
 
 comment "DMA Devices"
 
+source "drivers/dma/xilinx/Kconfig"
+
 config INTEL_MID_DMAC
 	tristate "Intel MID DMA support for Peripheral DMA controllers"
 	depends on PCI && X86
diff --git a/drivers/dma/Makefile b/drivers/dma/Makefile
index 0f23d94..5517dcd 100644
--- a/drivers/dma/Makefile
+++ b/drivers/dma/Makefile
@@ -41,3 +41,4 @@ obj-$(CONFIG_MMP_PDMA) += mmp_pdma.o
 obj-$(CONFIG_KEYSTONE_DMA) += keystone-pktdma.o
 obj-$(CONFIG_KEYSTONE_UDMA) += keystone-udma.o
 obj-$(CONFIG_TI_CPPI41) += cppi41.o
+obj-$(CONFIG_XILINX_DMA_ENGINES) += xilinx/
diff --git a/drivers/dma/xilinx/Kconfig b/drivers/dma/xilinx/Kconfig
new file mode 100644
index 0000000..704a2b8
--- /dev/null
+++ b/drivers/dma/xilinx/Kconfig
@@ -0,0 +1,53 @@
+#
+# XILINX DMA Engines configuration
+#
+
+menuconfig XILINX_DMA_ENGINES
+	bool "Xilinx DMA Engines"
+	help
+	  Enable support for the Xilinx DMA controllers. It supports three DMA
+	  engines: Axi Central DMA (memory to memory transfer), Axi DMA (memory and
+	  device transfer), and Axi VDMA (memory and video device transfer).
+
+if XILINX_DMA_ENGINES
+
+config XILINX_AXIDMA
+	tristate "Xilinx AXI DMA Engine"
+	select DMA_ENGINE
+	help
+	  Enable support for Xilinx Axi DMA (memory and device transfer).
+
+config XILINX_DMATEST
+	tristate "DMA Test client for AXI DMA"
+	depends on XILINX_AXIDMA
+	help
+	  Simple DMA test client. Say N unless you're debugging a
+	  DMA Device driver.
+
+config XILINX_AXIVDMA
+	tristate "Xilinx AXI VDMA Engine"
+	select DMA_ENGINE
+	help
+	  Enable support for Xilinx Axi VDMA (memory and video device transfer).
+
+config XILINX_VDMATEST
+	tristate "DMA Test client for VDMA"
+	depends on XILINX_AXIVDMA
+	help
+	  Simple DMA test client. Say N unless you're debugging a
+	  DMA Device driver.
+
+config XILINX_AXICDMA
+	tristate "Xilinx AXI CDMA Engine"
+	select DMA_ENGINE
+	help
+	  Enable support for Xilinx Axi Central DMA (memory to memory transfer).
+
+config XILINX_CDMATEST
+	tristate "DMA Test client for CDMA"
+	depends on XILINX_AXICDMA
+	help
+	  Simple DMA test client. Say N unless you're debugging a
+	  DMA Device driver.
+
+endif # XILINX_DMA_ENGINES
diff --git a/drivers/dma/xilinx/Makefile b/drivers/dma/xilinx/Makefile
new file mode 100644
index 0000000..3362ad9
--- /dev/null
+++ b/drivers/dma/xilinx/Makefile
@@ -0,0 +1,6 @@
+obj-$(CONFIG_XILINX_AXIDMA) += xilinx_axidma.o
+obj-$(CONFIG_XILINX_DMATEST) += dmatest.o
+obj-$(CONFIG_XILINX_AXIVDMA) += xilinx_axivdma.o
+obj-$(CONFIG_XILINX_VDMATEST) += vdmatest.o
+obj-$(CONFIG_XILINX_AXICDMA) += xilinx_axicdma.o
+obj-$(CONFIG_XILINX_CDMATEST) += cdmatest.o
diff --git a/drivers/dma/xilinx/cdmatest.c b/drivers/dma/xilinx/cdmatest.c
new file mode 100644
index 0000000..4f2bb0f
--- /dev/null
+++ b/drivers/dma/xilinx/cdmatest.c
@@ -0,0 +1,644 @@
+/*
+ * XILINX CDMA Engine test module
+ *
+ * Copyright (C) 2012 Xilinx, Inc. All rights reserved.
+ *
+ * Based on Atmel DMA Test Client
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+#include <linux/delay.h>
+#include <linux/dmaengine.h>
+#include <linux/init.h>
+#include <linux/kthread.h>
+#include <linux/module.h>
+#include <linux/random.h>
+#include <linux/slab.h>
+#include <linux/wait.h>
+#include <linux/amba/xilinx_dma.h>
+
+static unsigned int test_buf_size = 64;
+module_param(test_buf_size, uint, S_IRUGO);
+MODULE_PARM_DESC(test_buf_size, "Size of the memcpy test buffer");
+
+static char test_channel[20];
+module_param_string(channel, test_channel, sizeof(test_channel), S_IRUGO);
+MODULE_PARM_DESC(channel, "Bus ID of the channel to test (default: any)");
+
+static char test_device[20];
+module_param_string(device, test_device, sizeof(test_device), S_IRUGO);
+MODULE_PARM_DESC(device, "Bus ID of the DMA Engine to test (default: any)");
+
+static unsigned int threads_per_chan = 1;
+module_param(threads_per_chan, uint, S_IRUGO);
+MODULE_PARM_DESC(threads_per_chan,
+		"Number of threads to start per channel (default: 1)");
+
+static unsigned int max_channels;
+module_param(max_channels, uint, S_IRUGO);
+MODULE_PARM_DESC(max_channels,
+		"Maximum number of channels to use (default: all)");
+
+static unsigned int iterations;
+module_param(iterations, uint, S_IRUGO);
+MODULE_PARM_DESC(iterations,
+		"Iterations before stopping test (default: infinite)");
+
+static unsigned int xor_sources = 3;
+module_param(xor_sources, uint, S_IRUGO);
+MODULE_PARM_DESC(xor_sources,
+		"Number of xor source buffers (default: 3)");
+
+static unsigned int pq_sources = 3;
+module_param(pq_sources, uint, S_IRUGO);
+MODULE_PARM_DESC(pq_sources,
+		"Number of p+q source buffers (default: 3)");
+
+/*
+ * Initialization patterns. All bytes in the source buffer has bit 7
+ * set, all bytes in the destination buffer has bit 7 cleared.
+ *
+ * Bit 6 is set for all bytes which are to be copied by the DMA
+ * engine. Bit 5 is set for all bytes which are to be overwritten by
+ * the DMA engine.
+ *
+ * The remaining bits are the inverse of a counter which increments by
+ * one for each byte address.
+ */
+#define PATTERN_SRC		0x80
+#define PATTERN_DST		0x00
+#define PATTERN_COPY		0x40
+#define PATTERN_OVERWRITE	0x20
+#define PATTERN_COUNT_MASK	0x1f
+
+struct cdmatest_thread {
+	struct list_head node;
+	struct task_struct *task;
+	struct dma_chan *chan;
+	u8 **srcs;
+	u8 **dsts;
+	enum dma_transaction_type type;
+};
+
+struct cdmatest_chan {
+	struct list_head node;
+	struct dma_chan *chan;
+	struct list_head threads;
+};
+
+/*
+ * These are protected by dma_list_mutex since they're only used by
+ * the DMA filter function callback
+ */
+static LIST_HEAD(cdmatest_channels);
+static unsigned int nr_channels;
+
+static bool cdmatest_match_channel(struct dma_chan *chan)
+{
+	if (test_channel[0] == '\0')
+		return true;
+	return strcmp(dma_chan_name(chan), test_channel) == 0;
+}
+
+static bool cdmatest_match_device(struct dma_device *device)
+{
+	if (test_device[0] == '\0')
+		return true;
+	return strcmp(dev_name(device->dev), test_device) == 0;
+}
+
+static unsigned long cdmatest_random(void)
+{
+	unsigned long buf;
+
+	get_random_bytes(&buf, sizeof(buf));
+	return buf;
+}
+
+static void cdmatest_init_srcs(u8 **bufs, unsigned int start, unsigned int len)
+{
+	unsigned int i;
+	u8 *buf;
+
+	for (; (buf = *bufs); bufs++) {
+		for (i = 0; i < start; i++)
+			buf[i] = PATTERN_SRC | (~i & PATTERN_COUNT_MASK);
+		for ( ; i < start + len; i++)
+			buf[i] = PATTERN_SRC | PATTERN_COPY
+				| (~i & PATTERN_COUNT_MASK);
+		for ( ; i < test_buf_size; i++)
+			buf[i] = PATTERN_SRC | (~i & PATTERN_COUNT_MASK);
+		buf++;
+	}
+}
+
+static void cdmatest_init_dsts(u8 **bufs, unsigned int start, unsigned int len)
+{
+	unsigned int i;
+	u8 *buf;
+
+	for (; (buf = *bufs); bufs++) {
+		for (i = 0; i < start; i++)
+			buf[i] = PATTERN_DST | (~i & PATTERN_COUNT_MASK);
+		for ( ; i < start + len; i++)
+			buf[i] = PATTERN_DST | PATTERN_OVERWRITE
+				| (~i & PATTERN_COUNT_MASK);
+		for ( ; i < test_buf_size; i++)
+			buf[i] = PATTERN_DST | (~i & PATTERN_COUNT_MASK);
+	}
+}
+
+static void cdmatest_mismatch(u8 actual, u8 pattern, unsigned int index,
+		unsigned int counter, bool is_srcbuf)
+{
+	u8 diff = actual ^ pattern;
+	u8 expected = pattern | (~counter & PATTERN_COUNT_MASK);
+	const char *thread_name = current->comm;
+
+	if (is_srcbuf)
+		pr_warn(
+		"%s: srcbuf[0x%x] overwritten! Expected %02x, got %02x\n",
+				thread_name, index, expected, actual);
+	else if ((pattern & PATTERN_COPY)
+			&& (diff & (PATTERN_COPY | PATTERN_OVERWRITE)))
+		pr_warn(
+		"%s: dstbuf[0x%x] not copied! Expected %02x, got %02x\n",
+				thread_name, index, expected, actual);
+	else if (diff & PATTERN_SRC)
+		pr_warn(
+		"%s: dstbuf[0x%x] was copied! Expected %02x, got %02x\n",
+				thread_name, index, expected, actual);
+	else
+		pr_warn(
+		"%s: dstbuf[0x%x] mismatch! Expected %02x, got %02x\n",
+				thread_name, index, expected, actual);
+}
+
+static unsigned int cdmatest_verify(u8 **bufs, unsigned int start,
+		unsigned int end, unsigned int counter, u8 pattern,
+		bool is_srcbuf)
+{
+	unsigned int i;
+	unsigned int error_count = 0;
+	u8 actual;
+	u8 expected;
+	u8 *buf;
+	unsigned int counter_orig = counter;
+
+	for (; (buf = *bufs); bufs++) {
+		counter = counter_orig;
+		for (i = start; i < end; i++) {
+			actual = buf[i];
+			expected = pattern | (~counter & PATTERN_COUNT_MASK);
+			if (actual != expected) {
+				if (error_count < 32)
+					cdmatest_mismatch(actual, pattern, i,
+							counter, is_srcbuf);
+				error_count++;
+			}
+			counter++;
+		}
+	}
+
+	if (error_count > 32)
+		pr_warn("%s: %u errors suppressed\n",
+			current->comm, error_count - 32);
+
+	return error_count;
+}
+
+static void cdmatest_callback(void *completion)
+{
+	complete(completion);
+}
+
+/*
+ * This function repeatedly tests DMA transfers of various lengths and
+ * offsets for a given operation type until it is told to exit by
+ * kthread_stop(). There may be multiple threads running this function
+ * in parallel for a single channel, and there may be multiple channels
+ * being tested in parallel.
+ *
+ * Before each test, the source and destination buffer is initialized
+ * with a known pattern. This pattern is different depending on
+ * whether it's in an area which is supposed to be copied or
+ * overwritten, and different in the source and destination buffers.
+ * So if the DMA engine doesn't copy exactly what we tell it to copy,
+ * we'll notice.
+ */
+static int cdmatest_func(void *data)
+{
+	struct cdmatest_thread *thread = data;
+	struct dma_chan *chan;
+	const char *thread_name;
+	unsigned int src_off, dst_off, len;
+	unsigned int error_count;
+	unsigned int failed_tests = 0;
+	unsigned int total_tests = 0;
+	dma_cookie_t cookie;
+	enum dma_status status;
+	enum dma_ctrl_flags flags;
+	u8 pq_coefs[pq_sources + 1];
+	int ret;
+	int src_cnt;
+	int dst_cnt;
+	int i;
+
+	thread_name = current->comm;
+
+	ret = -ENOMEM;
+
+	/* JZ: limit testing scope here */
+	iterations = 5;
+
+	smp_rmb();
+	chan = thread->chan;
+	if (thread->type == DMA_MEMCPY)
+		src_cnt = dst_cnt = 1;
+	else if (thread->type == DMA_XOR) {
+		src_cnt = xor_sources | 1;
+				/* force odd to ensure dst = src */
+		dst_cnt = 1;
+	} else if (thread->type == DMA_PQ) {
+		src_cnt = pq_sources | 1;
+				/* force odd to ensure dst = src */
+		dst_cnt = 2;
+		for (i = 0; i < src_cnt; i++)
+			pq_coefs[i] = 1;
+	} else
+		goto err_srcs;
+
+	thread->srcs = kcalloc(src_cnt+1, sizeof(u8 *), GFP_KERNEL);
+	if (!thread->srcs)
+		goto err_srcs;
+	for (i = 0; i < src_cnt; i++) {
+		thread->srcs[i] = kmalloc(test_buf_size, GFP_KERNEL);
+		if (!thread->srcs[i])
+			goto err_srcbuf;
+	}
+	thread->srcs[i] = NULL;
+
+	thread->dsts = kcalloc(dst_cnt+1, sizeof(u8 *), GFP_KERNEL);
+	if (!thread->dsts)
+		goto err_dsts;
+	for (i = 0; i < dst_cnt; i++) {
+		thread->dsts[i] = kmalloc(test_buf_size, GFP_KERNEL);
+		if (!thread->dsts[i])
+			goto err_dstbuf;
+	}
+	thread->dsts[i] = NULL;
+
+	set_user_nice(current, 10);
+
+	flags = DMA_CTRL_ACK | DMA_COMPL_SKIP_DEST_UNMAP | DMA_PREP_INTERRUPT;
+
+	while (!kthread_should_stop()
+		&& !(iterations && total_tests >= iterations)) {
+		struct dma_device *dev = chan->device;
+		struct dma_async_tx_descriptor *tx = NULL;
+		dma_addr_t dma_srcs[src_cnt];
+		dma_addr_t dma_dsts[dst_cnt];
+		struct completion cmp;
+		unsigned long tmo = msecs_to_jiffies(3000);
+		u8 align = 0;
+
+		total_tests++;
+
+		/* honor alignment restrictions */
+		if (thread->type == DMA_MEMCPY)
+			align = dev->copy_align;
+		else if (thread->type == DMA_XOR)
+			align = dev->xor_align;
+		else if (thread->type == DMA_PQ)
+			align = dev->pq_align;
+
+		if (1 << align > test_buf_size) {
+			pr_err("%u-byte buffer too small for %d-byte alignment\n",
+			       test_buf_size, 1 << align);
+			break;
+		}
+
+		len = cdmatest_random() % test_buf_size + 1;
+		len = (len >> align) << align;
+		if (!len)
+			len = 1 << align;
+		src_off = cdmatest_random() % (test_buf_size - len + 1);
+		dst_off = cdmatest_random() % (test_buf_size - len + 1);
+
+		src_off = (src_off >> align) << align;
+		dst_off = (dst_off >> align) << align;
+
+		cdmatest_init_srcs(thread->srcs, src_off, len);
+		cdmatest_init_dsts(thread->dsts, dst_off, len);
+
+		for (i = 0; i < src_cnt; i++) {
+			u8 *buf = thread->srcs[i] + src_off;
+
+			dma_srcs[i] = dma_map_single(dev->dev, buf, len,
+							DMA_MEM_TO_DEV);
+		}
+		/* map with DMA_MEM_TO_MEM to force writeback/invalidate */
+		for (i = 0; i < dst_cnt; i++) {
+			dma_dsts[i] = dma_map_single(dev->dev, thread->dsts[i],
+							test_buf_size,
+							DMA_MEM_TO_MEM);
+		}
+
+		if (thread->type == DMA_MEMCPY) {
+			tx = dev->device_prep_dma_memcpy(chan,
+							dma_dsts[0] + dst_off,
+							dma_srcs[0], len,
+							flags);
+
+		} else if (thread->type == DMA_XOR)
+			tx = dev->device_prep_dma_xor(chan,
+							dma_dsts[0] + dst_off,
+							dma_srcs, src_cnt,
+							len, flags);
+		else if (thread->type == DMA_PQ) {
+			dma_addr_t dma_pq[dst_cnt];
+
+			for (i = 0; i < dst_cnt; i++)
+				dma_pq[i] = dma_dsts[i] + dst_off;
+			tx = dev->device_prep_dma_pq(chan, dma_pq, dma_srcs,
+							src_cnt, pq_coefs,
+							len, flags);
+		}
+
+		if (!tx) {
+			for (i = 0; i < src_cnt; i++)
+				dma_unmap_single(dev->dev, dma_srcs[i], len,
+							DMA_MEM_TO_DEV);
+			for (i = 0; i < dst_cnt; i++)
+				dma_unmap_single(dev->dev, dma_dsts[i],
+							test_buf_size,
+							DMA_MEM_TO_MEM);
+			pr_warn(
+			"%s: #%u: prep error with src_off=0x%x ",
+				thread_name, total_tests - 1, src_off);
+			pr_warn("dst_off=0x%x len=0x%x\n",
+					dst_off, len);
+			msleep(100);
+			failed_tests++;
+			continue;
+		}
+
+		init_completion(&cmp);
+		tx->callback = cdmatest_callback;
+		tx->callback_param = &cmp;
+		cookie = tx->tx_submit(tx);
+
+		if (dma_submit_error(cookie)) {
+			pr_warn(
+			"%s: #%u: submit error %d with src_off=0x%x ",
+					thread_name, total_tests - 1,
+					cookie, src_off);
+			pr_warn("dst_off=0x%x len=0x%x\n",
+					dst_off, len);
+			msleep(100);
+			failed_tests++;
+			continue;
+		}
+		dma_async_issue_pending(chan);
+
+		tmo = wait_for_completion_timeout(&cmp, tmo);
+		status = dma_async_is_tx_complete(chan, cookie, NULL, NULL);
+
+		if (tmo == 0) {
+			pr_warn("%s: #%u: test timed out\n",
+					thread_name, total_tests - 1);
+			failed_tests++;
+			continue;
+		} else if (status != DMA_SUCCESS) {
+			pr_warn(
+			"%s: #%u: got completion callback, ",
+					thread_name, total_tests - 1);
+			pr_warn("but status is \'%s\'\n",
+					status == DMA_ERROR ? "error" :
+							"in progress");
+			failed_tests++;
+			continue;
+		}
+
+		/* Unmap by myself (see DMA_COMPL_SKIP_DEST_UNMAP above) */
+		for (i = 0; i < dst_cnt; i++)
+			dma_unmap_single(dev->dev, dma_dsts[i], test_buf_size,
+					DMA_MEM_TO_MEM);
+
+		error_count = 0;
+
+		pr_debug("%s: verifying source buffer...\n", thread_name);
+		error_count += cdmatest_verify(thread->srcs, 0, src_off,
+				0, PATTERN_SRC, true);
+		error_count += cdmatest_verify(thread->srcs, src_off,
+				src_off + len, src_off,
+				PATTERN_SRC | PATTERN_COPY, true);
+		error_count += cdmatest_verify(thread->srcs, src_off + len,
+				test_buf_size, src_off + len,
+				PATTERN_SRC, true);
+
+		pr_debug("%s: verifying dest buffer...\n",
+				thread->task->comm);
+		error_count += cdmatest_verify(thread->dsts, 0, dst_off,
+				0, PATTERN_DST, false);
+		error_count += cdmatest_verify(thread->dsts, dst_off,
+				dst_off + len, src_off,
+				PATTERN_SRC | PATTERN_COPY, false);
+		error_count += cdmatest_verify(thread->dsts, dst_off + len,
+				test_buf_size, dst_off + len,
+				PATTERN_DST, false);
+
+		if (error_count) {
+			pr_warn("%s: #%u: %u errors with ",
+				thread_name, total_tests - 1, error_count);
+			pr_warn("src_off=0x%x dst_off=0x%x len=0x%x\n",
+				src_off, dst_off, len);
+			failed_tests++;
+		} else {
+			pr_debug("%s: #%u: No errors with ",
+				thread_name, total_tests - 1);
+			pr_debug("src_off=0x%x dst_off=0x%x len=0x%x\n",
+				src_off, dst_off, len);
+		}
+	}
+
+	ret = 0;
+	for (i = 0; thread->dsts[i]; i++)
+		kfree(thread->dsts[i]);
+err_dstbuf:
+	kfree(thread->dsts);
+err_dsts:
+	for (i = 0; thread->srcs[i]; i++)
+		kfree(thread->srcs[i]);
+err_srcbuf:
+	kfree(thread->srcs);
+err_srcs:
+	pr_notice("%s: terminating after %u tests, %u failures (status %d)\n",
+			thread_name, total_tests, failed_tests, ret);
+
+	if (iterations > 0)
+		while (!kthread_should_stop()) {
+			DECLARE_WAIT_QUEUE_HEAD_ONSTACK(wait_cdmatest_exit);
+			interruptible_sleep_on(&wait_cdmatest_exit);
+		}
+
+	return ret;
+}
+
+static void cdmatest_cleanup_channel(struct cdmatest_chan *dtc)
+{
+	struct cdmatest_thread *thread;
+	struct cdmatest_thread *_thread;
+	int ret;
+
+	list_for_each_entry_safe(thread, _thread, &dtc->threads, node) {
+		ret = kthread_stop(thread->task);
+		pr_debug("cdmatest: thread %s exited with status %d\n",
+				thread->task->comm, ret);
+		list_del(&thread->node);
+		kfree(thread);
+	}
+	kfree(dtc);
+}
+
+static int cdmatest_add_threads(struct cdmatest_chan *dtc,
+				enum dma_transaction_type type)
+{
+	struct cdmatest_thread *thread;
+	struct dma_chan *chan = dtc->chan;
+	char *op;
+	unsigned int i;
+
+	if (type == DMA_MEMCPY)
+		op = "copy";
+	else if (type == DMA_XOR)
+		op = "xor";
+	else if (type == DMA_PQ)
+		op = "pq";
+	else
+		return -EINVAL;
+
+	for (i = 0; i < threads_per_chan; i++) {
+		thread = kzalloc(sizeof(struct cdmatest_thread), GFP_KERNEL);
+		if (!thread) {
+			pr_warn("cdmatest: No memory for %s-%s%u\n",
+					dma_chan_name(chan), op, i);
+
+			break;
+		}
+		thread->chan = dtc->chan;
+		thread->type = type;
+		smp_wmb();
+		thread->task = kthread_run(cdmatest_func, thread, "%s-%s%u",
+				dma_chan_name(chan), op, i);
+		if (IS_ERR(thread->task)) {
+			pr_warn("cdmatest: Failed to run thread %s-%s%u\n",
+					dma_chan_name(chan), op, i);
+			kfree(thread);
+			break;
+		}
+
+		/* srcbuf and dstbuf are allocated by the thread itself */
+
+		list_add_tail(&thread->node, &dtc->threads);
+	}
+
+	return i;
+}
+
+static int cdmatest_add_channel(struct dma_chan *chan)
+{
+	struct cdmatest_chan *dtc;
+	struct dma_device *dma_dev = chan->device;
+	unsigned int thread_count = 0;
+	int cnt;
+
+	dtc = kmalloc(sizeof(struct cdmatest_chan), GFP_KERNEL);
+	if (!dtc) {
+		pr_warn("cdmatest: No memory for %s\n", dma_chan_name(chan));
+		return -ENOMEM;
+	}
+
+	dtc->chan = chan;
+	INIT_LIST_HEAD(&dtc->threads);
+
+	if (dma_has_cap(DMA_MEMCPY, dma_dev->cap_mask)) {
+		cnt = cdmatest_add_threads(dtc, DMA_MEMCPY);
+		thread_count += cnt > 0 ? cnt : 0;
+	}
+	if (dma_has_cap(DMA_XOR, dma_dev->cap_mask)) {
+		cnt = cdmatest_add_threads(dtc, DMA_XOR);
+		thread_count += cnt > 0 ? cnt : 0;
+	}
+	if (dma_has_cap(DMA_PQ, dma_dev->cap_mask)) {
+		cnt = cdmatest_add_threads(dtc, DMA_PQ);
+		thread_count += cnt > 0 ? cnt : 0;
+	}
+
+	pr_info("cdmatest: Started %u threads using %s\n",
+		thread_count, dma_chan_name(chan));
+
+	list_add_tail(&dtc->node, &cdmatest_channels);
+	nr_channels++;
+
+	return 0;
+}
+
+static bool filter(struct dma_chan *chan, void *param)
+{
+	if (!cdmatest_match_channel(chan) ||
+			!cdmatest_match_device(chan->device))
+		return false;
+
+	return true;
+}
+
+static int __init cdmatest_init(void)
+{
+	dma_cap_mask_t mask;
+	struct dma_chan *chan;
+	int err = 0;
+
+	dma_cap_zero(mask);
+	dma_cap_set(DMA_MEMCPY, mask);
+	for (;;) {
+		chan = dma_request_channel(mask, filter, NULL);
+
+		if (chan) {
+			err = cdmatest_add_channel(chan);
+			if (err) {
+				dma_release_channel(chan);
+				break; /* add_channel failed, punt */
+			}
+		} else
+			break; /* no more channels available */
+		if (max_channels && nr_channels >= max_channels)
+			break; /* we have all we need */
+	}
+
+	return err;
+}
+/* when compiled-in wait for drivers to load first */
+late_initcall(cdmatest_init);
+
+static void __exit cdmatest_exit(void)
+{
+	struct cdmatest_chan *dtc, *_dtc;
+	struct dma_chan *chan;
+
+	list_for_each_entry_safe(dtc, _dtc, &cdmatest_channels, node) {
+		list_del(&dtc->node);
+		chan = dtc->chan;
+		cdmatest_cleanup_channel(dtc);
+		pr_debug("cdmatest: dropped channel %s\n",
+			 dma_chan_name(chan));
+		dma_release_channel(chan);
+	}
+}
+module_exit(cdmatest_exit);
+
+MODULE_AUTHOR("Xilinx, Inc.");
+MODULE_DESCRIPTION("Xilinx AXI CDMA Test Client");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/dma/xilinx/dmatest.c b/drivers/dma/xilinx/dmatest.c
new file mode 100644
index 0000000..e9104f2
--- /dev/null
+++ b/drivers/dma/xilinx/dmatest.c
@@ -0,0 +1,649 @@
+/*
+ * XILINX AXI DMA Engine test module
+ *
+ * Copyright (C) 2010 Xilinx, Inc. All rights reserved.
+ *
+ * Based on Atmel DMA Test Client
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+#include <linux/delay.h>
+#include <linux/dmaengine.h>
+#include <linux/init.h>
+#include <linux/kthread.h>
+#include <linux/module.h>
+#include <linux/random.h>
+#include <linux/slab.h>
+#include <linux/wait.h>
+#include <linux/amba/xilinx_dma.h>
+
+static unsigned int test_buf_size = 64;
+module_param(test_buf_size, uint, S_IRUGO);
+MODULE_PARM_DESC(test_buf_size, "Size of the memcpy test buffer");
+
+static unsigned int iterations;
+module_param(iterations, uint, S_IRUGO);
+MODULE_PARM_DESC(iterations,
+		"Iterations before stopping test (default: infinite)");
+
+/*
+ * Initialization patterns. All bytes in the source buffer has bit 7
+ * set, all bytes in the destination buffer has bit 7 cleared.
+ *
+ * Bit 6 is set for all bytes which are to be copied by the DMA
+ * engine. Bit 5 is set for all bytes which are to be overwritten by
+ * the DMA engine.
+ *
+ * The remaining bits are the inverse of a counter which increments by
+ * one for each byte address.
+ */
+#define PATTERN_SRC		0x80
+#define PATTERN_DST		0x00
+#define PATTERN_COPY		0x40
+#define PATTERN_OVERWRITE	0x20
+#define PATTERN_COUNT_MASK	0x1f
+
+struct dmatest_slave_thread {
+	struct list_head node;
+	struct task_struct *task;
+	struct dma_chan *tx_chan;
+	struct dma_chan *rx_chan;
+	u8 **srcs;
+	u8 **dsts;
+	enum dma_transaction_type type;
+};
+
+struct dmatest_chan {
+	struct list_head node;
+	struct dma_chan *chan;
+	struct list_head threads;
+};
+
+/*
+ * These are protected by dma_list_mutex since they're only used by
+ * the DMA filter function callback
+ */
+static LIST_HEAD(dmatest_channels);
+static unsigned int nr_channels;
+
+static unsigned long dmatest_random(void)
+{
+	unsigned long buf;
+
+	get_random_bytes(&buf, sizeof(buf));
+	return buf;
+}
+
+static void dmatest_init_srcs(u8 **bufs, unsigned int start, unsigned int len)
+{
+	unsigned int i;
+	u8 *buf;
+
+	for (; (buf = *bufs); bufs++) {
+		for (i = 0; i < start; i++)
+			buf[i] = PATTERN_SRC | (~i & PATTERN_COUNT_MASK);
+		for ( ; i < start + len; i++)
+			buf[i] = PATTERN_SRC | PATTERN_COPY
+				| (~i & PATTERN_COUNT_MASK);
+		for ( ; i < test_buf_size; i++)
+			buf[i] = PATTERN_SRC | (~i & PATTERN_COUNT_MASK);
+		buf++;
+	}
+}
+
+static void dmatest_init_dsts(u8 **bufs, unsigned int start, unsigned int len)
+{
+	unsigned int i;
+	u8 *buf;
+
+	for (; (buf = *bufs); bufs++) {
+		for (i = 0; i < start; i++)
+			buf[i] = PATTERN_DST | (~i & PATTERN_COUNT_MASK);
+		for ( ; i < start + len; i++)
+			buf[i] = PATTERN_DST | PATTERN_OVERWRITE
+				| (~i & PATTERN_COUNT_MASK);
+		for ( ; i < test_buf_size; i++)
+			buf[i] = PATTERN_DST | (~i & PATTERN_COUNT_MASK);
+	}
+}
+
+static void dmatest_mismatch(u8 actual, u8 pattern, unsigned int index,
+		unsigned int counter, bool is_srcbuf)
+{
+	u8 diff = actual ^ pattern;
+	u8 expected = pattern | (~counter & PATTERN_COUNT_MASK);
+	const char *thread_name = current->comm;
+
+	if (is_srcbuf)
+		pr_warn(
+		"%s: srcbuf[0x%x] overwritten! Expected %02x, got %02x\n",
+				thread_name, index, expected, actual);
+	else if ((pattern & PATTERN_COPY)
+			&& (diff & (PATTERN_COPY | PATTERN_OVERWRITE)))
+		pr_warn(
+		"%s: dstbuf[0x%x] not copied! Expected %02x, got %02x\n",
+				thread_name, index, expected, actual);
+	else if (diff & PATTERN_SRC)
+		pr_warn(
+		"%s: dstbuf[0x%x] was copied! Expected %02x, got %02x\n",
+				thread_name, index, expected, actual);
+	else
+		pr_warn(
+		"%s: dstbuf[0x%x] mismatch! Expected %02x, got %02x\n",
+				thread_name, index, expected, actual);
+}
+
+static unsigned int dmatest_verify(u8 **bufs, unsigned int start,
+		unsigned int end, unsigned int counter, u8 pattern,
+		bool is_srcbuf)
+{
+	unsigned int i;
+	unsigned int error_count = 0;
+	u8 actual;
+	u8 expected;
+	u8 *buf;
+	unsigned int counter_orig = counter;
+
+	for (; (buf = *bufs); bufs++) {
+		counter = counter_orig;
+		for (i = start; i < end; i++) {
+			actual = buf[i];
+			expected = pattern | (~counter & PATTERN_COUNT_MASK);
+			if (actual != expected) {
+				if (error_count < 32)
+					dmatest_mismatch(actual, pattern, i,
+							counter, is_srcbuf);
+				error_count++;
+			}
+			counter++;
+		}
+	}
+
+	if (error_count > 32)
+		pr_warn("%s: %u errors suppressed\n",
+			current->comm, error_count - 32);
+
+	return error_count;
+}
+
+static void dmatest_slave_tx_callback(void *completion)
+{
+	complete(completion);
+}
+
+static void dmatest_slave_rx_callback(void *completion)
+{
+	complete(completion);
+}
+
+/* Function for slave transfers
+ * Each thread requires 2 channels, one for transmit, and one for receive
+ */
+static int dmatest_slave_func(void *data)
+{
+	struct dmatest_slave_thread	*thread = data;
+	struct dma_chan *tx_chan;
+	struct dma_chan *rx_chan;
+	const char *thread_name;
+	unsigned int src_off, dst_off, len;
+	unsigned int error_count;
+	unsigned int failed_tests = 0;
+	unsigned int total_tests = 0;
+	dma_cookie_t tx_cookie;
+	dma_cookie_t rx_cookie;
+	enum dma_status status;
+	enum dma_ctrl_flags flags;
+	int ret;
+	int src_cnt;
+	int dst_cnt;
+	int bd_cnt = 11;
+	int i;
+	struct xilinx_dma_config config;
+	thread_name = current->comm;
+
+	ret = -ENOMEM;
+
+	/* JZ: limit testing scope here */
+	iterations = 5;
+	test_buf_size = 700;
+
+	smp_rmb();
+	tx_chan = thread->tx_chan;
+	rx_chan = thread->rx_chan;
+	src_cnt = dst_cnt = bd_cnt;
+
+	thread->srcs = kcalloc(src_cnt+1, sizeof(u8 *), GFP_KERNEL);
+	if (!thread->srcs)
+		goto err_srcs;
+	for (i = 0; i < src_cnt; i++) {
+		thread->srcs[i] = kmalloc(test_buf_size, GFP_KERNEL);
+		if (!thread->srcs[i])
+			goto err_srcbuf;
+	}
+	thread->srcs[i] = NULL;
+
+	thread->dsts = kcalloc(dst_cnt+1, sizeof(u8 *), GFP_KERNEL);
+	if (!thread->dsts)
+		goto err_dsts;
+	for (i = 0; i < dst_cnt; i++) {
+		thread->dsts[i] = kmalloc(test_buf_size, GFP_KERNEL);
+		if (!thread->dsts[i])
+			goto err_dstbuf;
+	}
+	thread->dsts[i] = NULL;
+
+	set_user_nice(current, 10);
+
+	flags = DMA_CTRL_ACK | DMA_COMPL_SKIP_DEST_UNMAP | DMA_PREP_INTERRUPT;
+
+	while (!kthread_should_stop()
+		&& !(iterations && total_tests >= iterations)) {
+		struct dma_device *tx_dev = tx_chan->device;
+		struct dma_device *rx_dev = rx_chan->device;
+		struct dma_async_tx_descriptor *txd = NULL;
+		struct dma_async_tx_descriptor *rxd = NULL;
+		dma_addr_t dma_srcs[src_cnt];
+		dma_addr_t dma_dsts[dst_cnt];
+		struct completion rx_cmp;
+		struct completion tx_cmp;
+		unsigned long rx_tmo =
+				msecs_to_jiffies(300000); /* RX takes longer */
+		unsigned long tx_tmo = msecs_to_jiffies(30000);
+		u8 align = 0;
+		struct scatterlist tx_sg[bd_cnt];
+		struct scatterlist rx_sg[bd_cnt];
+
+		total_tests++;
+
+		/* honor larger alignment restrictions */
+		align = tx_dev->copy_align;
+		if (rx_dev->copy_align > align)
+			align = rx_dev->copy_align;
+
+		if (1 << align > test_buf_size) {
+			pr_err("%u-byte buffer too small for %d-byte alignment\n",
+				test_buf_size, 1 << align);
+			break;
+		}
+
+		len = dmatest_random() % test_buf_size + 1;
+		len = (len >> align) << align;
+		if (!len)
+			len = 1 << align;
+		src_off = dmatest_random() % (test_buf_size - len + 1);
+		dst_off = dmatest_random() % (test_buf_size - len + 1);
+
+		src_off = (src_off >> align) << align;
+		dst_off = (dst_off >> align) << align;
+
+		dmatest_init_srcs(thread->srcs, src_off, len);
+		dmatest_init_dsts(thread->dsts, dst_off, len);
+
+		for (i = 0; i < src_cnt; i++) {
+			u8 *buf = thread->srcs[i] + src_off;
+
+			dma_srcs[i] = dma_map_single(tx_dev->dev, buf, len,
+							DMA_MEM_TO_DEV);
+		}
+
+		for (i = 0; i < dst_cnt; i++) {
+			dma_dsts[i] = dma_map_single(rx_dev->dev,
+							thread->dsts[i],
+							test_buf_size,
+							DMA_MEM_TO_DEV);
+
+			dma_unmap_single(rx_dev->dev, dma_dsts[i],
+							test_buf_size,
+							DMA_MEM_TO_DEV);
+
+			dma_dsts[i] = dma_map_single(rx_dev->dev,
+							thread->dsts[i],
+							test_buf_size,
+							DMA_DEV_TO_MEM);
+		}
+
+		sg_init_table(tx_sg, bd_cnt);
+		sg_init_table(rx_sg, bd_cnt);
+
+		for (i = 0; i < bd_cnt; i++) {
+			sg_dma_address(&tx_sg[i]) = dma_srcs[i];
+			sg_dma_address(&rx_sg[i]) = dma_dsts[i] + dst_off;
+
+			sg_dma_len(&tx_sg[i]) = len;
+			sg_dma_len(&rx_sg[i]) = len;
+
+		}
+
+		/* Only one interrupt */
+		config.coalesc = 1;
+		config.delay = 0;
+		rx_dev->device_control(rx_chan, DMA_SLAVE_CONFIG,
+				(unsigned long)&config);
+
+		config.coalesc = 1;
+		config.delay = 0;
+		tx_dev->device_control(tx_chan, DMA_SLAVE_CONFIG,
+				(unsigned long)&config);
+
+		rxd = rx_dev->device_prep_slave_sg(rx_chan, rx_sg, bd_cnt,
+				DMA_DEV_TO_MEM, flags, NULL);
+
+		txd = tx_dev->device_prep_slave_sg(tx_chan, tx_sg, bd_cnt,
+				DMA_MEM_TO_DEV, flags, NULL);
+
+		if (!rxd || !txd) {
+			for (i = 0; i < src_cnt; i++)
+				dma_unmap_single(tx_dev->dev, dma_srcs[i], len,
+						DMA_MEM_TO_DEV);
+			for (i = 0; i < dst_cnt; i++)
+				dma_unmap_single(rx_dev->dev, dma_dsts[i],
+						test_buf_size,
+						DMA_DEV_TO_MEM);
+			pr_warn(
+			"%s: #%u: prep error with src_off=0x%x ",
+				thread_name, total_tests - 1, src_off);
+			pr_warn("dst_off=0x%x len=0x%x\n",
+					dst_off, len);
+			msleep(100);
+			failed_tests++;
+			continue;
+		}
+
+		init_completion(&rx_cmp);
+		rxd->callback = dmatest_slave_rx_callback;
+		rxd->callback_param = &rx_cmp;
+		rx_cookie = rxd->tx_submit(rxd);
+
+		init_completion(&tx_cmp);
+		txd->callback = dmatest_slave_tx_callback;
+		txd->callback_param = &tx_cmp;
+		tx_cookie = txd->tx_submit(txd);
+
+		if (dma_submit_error(rx_cookie) ||
+				dma_submit_error(tx_cookie)) {
+			pr_warn(
+			"%s: #%u: submit error %d/%d with src_off=0x%x ",
+					thread_name, total_tests - 1,
+					rx_cookie, tx_cookie, src_off);
+			pr_warn("dst_off=0x%x len=0x%x\n",
+					dst_off, len);
+			msleep(100);
+			failed_tests++;
+			continue;
+		}
+		dma_async_issue_pending(tx_chan);
+		dma_async_issue_pending(rx_chan);
+
+		tx_tmo = wait_for_completion_timeout(&tx_cmp, tx_tmo);
+
+		status = dma_async_is_tx_complete(tx_chan, tx_cookie,
+							NULL, NULL);
+
+		if (tx_tmo == 0) {
+			pr_warn("%s: #%u: tx test timed out\n",
+				   thread_name, total_tests - 1);
+			failed_tests++;
+			continue;
+		} else if (status != DMA_SUCCESS) {
+			pr_warn(
+			"%s: #%u: tx got completion callback, ",
+				   thread_name, total_tests - 1);
+			pr_warn("but status is \'%s\'\n",
+				   status == DMA_ERROR ? "error" :
+							"in progress");
+			failed_tests++;
+			continue;
+		}
+
+		rx_tmo = wait_for_completion_timeout(&rx_cmp, rx_tmo);
+		status = dma_async_is_tx_complete(rx_chan, rx_cookie,
+							NULL, NULL);
+
+		if (rx_tmo == 0) {
+			pr_warn("%s: #%u: rx test timed out\n",
+				   thread_name, total_tests - 1);
+			failed_tests++;
+			continue;
+		} else if (status != DMA_SUCCESS) {
+			pr_warn(
+			"%s: #%u: rx got completion callback, ",
+				   thread_name, total_tests - 1);
+			pr_warn("but status is \'%s\'\n",
+				   status == DMA_ERROR ? "error" :
+							"in progress");
+			failed_tests++;
+			continue;
+		}
+
+		/* Unmap by myself (see DMA_COMPL_SKIP_DEST_UNMAP above) */
+		for (i = 0; i < dst_cnt; i++)
+			dma_unmap_single(rx_dev->dev, dma_dsts[i],
+					test_buf_size, DMA_DEV_TO_MEM);
+
+		error_count = 0;
+
+		pr_debug("%s: verifying source buffer...\n", thread_name);
+		error_count += dmatest_verify(thread->srcs, 0, src_off,
+				0, PATTERN_SRC, true);
+		error_count += dmatest_verify(thread->srcs, src_off,
+				src_off + len, src_off,
+				PATTERN_SRC | PATTERN_COPY, true);
+		error_count += dmatest_verify(thread->srcs, src_off + len,
+				test_buf_size, src_off + len,
+				PATTERN_SRC, true);
+
+		pr_debug("%s: verifying dest buffer...\n",
+				thread->task->comm);
+		error_count += dmatest_verify(thread->dsts, 0, dst_off,
+				0, PATTERN_DST, false);
+		error_count += dmatest_verify(thread->dsts, dst_off,
+				dst_off + len, src_off,
+				PATTERN_SRC | PATTERN_COPY, false);
+		error_count += dmatest_verify(thread->dsts, dst_off + len,
+				test_buf_size, dst_off + len,
+				PATTERN_DST, false);
+
+		if (error_count) {
+			pr_warn("%s: #%u: %u errors with ",
+				thread_name, total_tests - 1, error_count);
+			pr_warn("src_off=0x%x dst_off=0x%x len=0x%x\n",
+				src_off, dst_off, len);
+			failed_tests++;
+		} else {
+			pr_debug("%s: #%u: No errors with ",
+				thread_name, total_tests - 1);
+			pr_debug("src_off=0x%x dst_off=0x%x len=0x%x\n",
+				src_off, dst_off, len);
+		}
+	}
+
+	ret = 0;
+	for (i = 0; thread->dsts[i]; i++)
+		kfree(thread->dsts[i]);
+err_dstbuf:
+	kfree(thread->dsts);
+err_dsts:
+	for (i = 0; thread->srcs[i]; i++)
+		kfree(thread->srcs[i]);
+err_srcbuf:
+	kfree(thread->srcs);
+err_srcs:
+	pr_notice("%s: terminating after %u tests, %u failures (status %d)\n",
+			thread_name, total_tests, failed_tests, ret);
+
+	if (iterations > 0)
+		while (!kthread_should_stop()) {
+			DECLARE_WAIT_QUEUE_HEAD_ONSTACK(wait_dmatest_exit);
+			interruptible_sleep_on(&wait_dmatest_exit);
+		}
+
+	return ret;
+}
+
+static void dmatest_cleanup_channel(struct dmatest_chan *dtc)
+{
+	struct dmatest_slave_thread *thread;
+	struct dmatest_slave_thread *_thread;
+	int ret;
+
+	list_for_each_entry_safe(thread, _thread, &dtc->threads, node) {
+		ret = kthread_stop(thread->task);
+		pr_debug("dmatest: thread %s exited with status %d\n",
+				thread->task->comm, ret);
+		list_del(&thread->node);
+		kfree(thread);
+	}
+	kfree(dtc);
+}
+
+static int dmatest_add_slave_threads(struct dmatest_chan *tx_dtc,
+					struct dmatest_chan *rx_dtc)
+{
+	struct dmatest_slave_thread *thread;
+	struct dma_chan *tx_chan = tx_dtc->chan;
+	struct dma_chan *rx_chan = rx_dtc->chan;
+
+	thread = kzalloc(sizeof(struct dmatest_slave_thread), GFP_KERNEL);
+	if (!thread) {
+		pr_warn("dmatest: No memory for slave thread %s-%s\n",
+				dma_chan_name(tx_chan), dma_chan_name(rx_chan));
+
+	}
+
+	thread->tx_chan = tx_chan;
+	thread->rx_chan = rx_chan;
+	thread->type = (enum dma_transaction_type)DMA_SLAVE;
+	smp_wmb();
+	thread->task = kthread_run(dmatest_slave_func, thread, "%s-%s",
+		dma_chan_name(tx_chan), dma_chan_name(rx_chan));
+	if (IS_ERR(thread->task)) {
+		pr_warn("dmatest: Failed to run thread %s-%s\n",
+				dma_chan_name(tx_chan), dma_chan_name(rx_chan));
+		kfree(thread);
+	}
+
+	/* srcbuf and dstbuf are allocated by the thread itself */
+
+	list_add_tail(&thread->node, &tx_dtc->threads);
+
+	/* Added one thread with 2 channels */
+	return 1;
+}
+
+static int dmatest_add_slave_channels(struct dma_chan *tx_chan,
+					struct dma_chan *rx_chan)
+{
+	struct dmatest_chan *tx_dtc;
+	struct dmatest_chan *rx_dtc;
+	unsigned int thread_count = 0;
+
+	tx_dtc = kmalloc(sizeof(struct dmatest_chan), GFP_KERNEL);
+	if (!tx_dtc) {
+		pr_warn("dmatest: No memory for tx %s\n",
+				dma_chan_name(tx_chan));
+		return -ENOMEM;
+	}
+
+	rx_dtc = kmalloc(sizeof(struct dmatest_chan), GFP_KERNEL);
+	if (!rx_dtc) {
+		pr_warn("dmatest: No memory for rx %s\n",
+				dma_chan_name(rx_chan));
+		return -ENOMEM;
+	}
+
+	tx_dtc->chan = tx_chan;
+	rx_dtc->chan = rx_chan;
+	INIT_LIST_HEAD(&tx_dtc->threads);
+	INIT_LIST_HEAD(&rx_dtc->threads);
+
+	dmatest_add_slave_threads(tx_dtc, rx_dtc);
+	thread_count += 1;
+
+	pr_info("dmatest: Started %u threads using %s %s\n",
+		thread_count, dma_chan_name(tx_chan), dma_chan_name(rx_chan));
+
+	list_add_tail(&tx_dtc->node, &dmatest_channels);
+	list_add_tail(&rx_dtc->node, &dmatest_channels);
+	nr_channels += 2;
+
+	return 0;
+}
+
+static bool xdma_filter(struct dma_chan *chan, void *param)
+{
+	pr_debug("dmatest: Private is %x\n", *((int *)chan->private));
+
+	if (*((int *)chan->private) == *(int *)param)
+		return true;
+
+	return false;
+}
+
+static int __init dmatest_init(void)
+{
+	dma_cap_mask_t mask;
+	struct dma_chan *chan;
+	int err = 0;
+
+	/* JZ for slave transfer channels */
+	enum dma_data_direction direction;
+	struct dma_chan *rx_chan;
+	u32 match;
+
+	dma_cap_zero(mask);
+	dma_cap_set(DMA_SLAVE | DMA_PRIVATE, mask);
+
+	direction = DMA_MEM_TO_DEV;
+	match = (direction & 0xFF) | XILINX_DMA_IP_DMA;
+	pr_debug("dmatest: match is %x\n", match);
+
+	chan = dma_request_channel(mask, xdma_filter, (void *)&match);
+
+	if (chan)
+		pr_debug("dmatest: Found tx device\n");
+	else
+		pr_info("dmatest: Did not find tx device\n");
+
+	direction = DMA_DEV_TO_MEM;
+	match = (direction & 0xFF) | XILINX_DMA_IP_DMA;
+	rx_chan = dma_request_channel(mask, xdma_filter, &match);
+
+	if (rx_chan)
+		pr_debug("dmatest: Found rx device\n");
+	else
+		pr_info("dmatest: Did not find rx device\n");
+
+	if (chan && rx_chan) {
+		err = dmatest_add_slave_channels(chan, rx_chan);
+		if (err) {
+			dma_release_channel(chan);
+			dma_release_channel(rx_chan);
+		}
+	}
+
+	return err;
+}
+/* when compiled-in wait for drivers to load first */
+late_initcall(dmatest_init);
+
+static void __exit dmatest_exit(void)
+{
+	struct dmatest_chan *dtc, *_dtc;
+	struct dma_chan *chan;
+
+	list_for_each_entry_safe(dtc, _dtc, &dmatest_channels, node) {
+		list_del(&dtc->node);
+		chan = dtc->chan;
+		dmatest_cleanup_channel(dtc);
+		pr_debug("dmatest: dropped channel %s\n",
+			dma_chan_name(chan));
+		dma_release_channel(chan);
+	}
+}
+module_exit(dmatest_exit);
+
+MODULE_AUTHOR("Xilinx, Inc.");
+MODULE_DESCRIPTION("Xilinx AXI DMA Test Client");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/dma/xilinx/vdmatest.c b/drivers/dma/xilinx/vdmatest.c
new file mode 100644
index 0000000..6c7ada8
--- /dev/null
+++ b/drivers/dma/xilinx/vdmatest.c
@@ -0,0 +1,620 @@
+/*
+ * XILIN VDMA Engine test module
+ *
+ * Copyright (C) 2012 Xilinx, Inc. All rights reserved.
+ *
+ * Based on Atmel DMA Test Client
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+#include <linux/delay.h>
+#include <linux/dmaengine.h>
+#include <linux/init.h>
+#include <linux/kthread.h>
+#include <linux/module.h>
+#include <linux/random.h>
+#include <linux/slab.h>
+#include <linux/wait.h>
+#include <linux/amba/xilinx_dma.h>
+
+static unsigned int test_buf_size = 64;
+module_param(test_buf_size, uint, S_IRUGO);
+MODULE_PARM_DESC(test_buf_size, "Size of the memcpy test buffer");
+
+static unsigned int iterations;
+module_param(iterations, uint, S_IRUGO);
+MODULE_PARM_DESC(iterations,
+		"Iterations before stopping test (default: infinite)");
+
+/*
+ * Initialization patterns. All bytes in the source buffer has bit 7
+ * set, all bytes in the destination buffer has bit 7 cleared.
+ *
+ * Bit 6 is set for all bytes which are to be copied by the DMA
+ * engine. Bit 5 is set for all bytes which are to be overwritten by
+ * the DMA engine.
+ *
+ * The remaining bits are the inverse of a counter which increments by
+ * one for each byte address.
+ */
+#define PATTERN_SRC		0x80
+#define PATTERN_DST		0x00
+#define PATTERN_COPY		0x40
+#define PATTERN_OVERWRITE	0x20
+#define PATTERN_COUNT_MASK	0x1f
+
+struct vdmatest_slave_thread {
+	struct list_head node;
+	struct task_struct *task;
+	struct dma_chan *tx_chan;
+	struct dma_chan *rx_chan;
+	u8 **srcs;
+	u8 **dsts;
+	enum dma_transaction_type type;
+};
+
+struct vdmatest_chan {
+	struct list_head node;
+	struct dma_chan *chan;
+	struct list_head threads;
+};
+
+/*
+ * These are protected by dma_list_mutex since they're only used by
+ * the DMA filter function callback
+ */
+static LIST_HEAD(vdmatest_channels);
+static unsigned int nr_channels;
+
+static void vdmatest_init_srcs(u8 **bufs, unsigned int start, unsigned int len)
+{
+	unsigned int i;
+	u8 *buf;
+
+	for (; (buf = *bufs); bufs++) {
+		for (i = 0; i < start; i++)
+			buf[i] = PATTERN_SRC | (~i & PATTERN_COUNT_MASK);
+		for ( ; i < start + len; i++)
+			buf[i] = PATTERN_SRC | PATTERN_COPY
+				| (~i & PATTERN_COUNT_MASK);
+		for ( ; i < test_buf_size; i++)
+			buf[i] = PATTERN_SRC | (~i & PATTERN_COUNT_MASK);
+		buf++;
+	}
+}
+
+static void vdmatest_init_dsts(u8 **bufs, unsigned int start, unsigned int len)
+{
+	unsigned int i;
+	u8 *buf;
+
+	for (; (buf = *bufs); bufs++) {
+		for (i = 0; i < start; i++)
+			buf[i] = PATTERN_DST | (~i & PATTERN_COUNT_MASK);
+		for ( ; i < start + len; i++)
+			buf[i] = PATTERN_DST | PATTERN_OVERWRITE
+				| (~i & PATTERN_COUNT_MASK);
+		for ( ; i < test_buf_size; i++)
+			buf[i] = PATTERN_DST | (~i & PATTERN_COUNT_MASK);
+	}
+}
+
+static void vdmatest_mismatch(u8 actual, u8 pattern, unsigned int index,
+		unsigned int counter, bool is_srcbuf)
+{
+	u8 diff = actual ^ pattern;
+	u8 expected = pattern | (~counter & PATTERN_COUNT_MASK);
+	const char *thread_name = current->comm;
+
+	if (is_srcbuf)
+		pr_warn(
+		"%s: srcbuf[0x%x] overwritten! Expected %02x, got %02x\n",
+				thread_name, index, expected, actual);
+	else if ((pattern & PATTERN_COPY)
+			&& (diff & (PATTERN_COPY | PATTERN_OVERWRITE)))
+		pr_warn(
+		"%s: dstbuf[0x%x] not copied! Expected %02x, got %02x\n",
+				thread_name, index, expected, actual);
+	else if (diff & PATTERN_SRC)
+		pr_warn(
+		"%s: dstbuf[0x%x] was copied! Expected %02x, got %02x\n",
+				thread_name, index, expected, actual);
+	else
+		pr_warn(
+		"%s: dstbuf[0x%x] mismatch! Expected %02x, got %02x\n",
+				thread_name, index, expected, actual);
+}
+
+static unsigned int vdmatest_verify(u8 **bufs, unsigned int start,
+		unsigned int end, unsigned int counter, u8 pattern,
+		bool is_srcbuf)
+{
+	unsigned int i;
+	unsigned int error_count = 0;
+	u8 actual;
+	u8 expected;
+	u8 *buf;
+	unsigned int counter_orig = counter;
+
+	for (; (buf = *bufs); bufs++) {
+		counter = counter_orig;
+		for (i = start; i < end; i++) {
+			actual = buf[i];
+			expected = pattern | (~counter & PATTERN_COUNT_MASK);
+			if (actual != expected) {
+				if (error_count < 32)
+					vdmatest_mismatch(actual, pattern, i,
+							counter, is_srcbuf);
+				error_count++;
+			}
+			counter++;
+		}
+	}
+
+	if (error_count > 32)
+		pr_warn("%s: %u errors suppressed\n",
+			current->comm, error_count - 32);
+
+	return error_count;
+}
+
+static void vdmatest_slave_tx_callback(void *completion)
+{
+	pr_debug("Got tx callback\n");
+	complete(completion);
+}
+
+static void vdmatest_slave_rx_callback(void *completion)
+{
+	pr_debug("Got rx callback\n");
+	complete(completion);
+}
+
+/*
+ * Function for slave transfers
+ * Each thread requires 2 channels, one for transmit, and one for receive
+ */
+static int vdmatest_slave_func(void *data)
+{
+	struct vdmatest_slave_thread *thread = data;
+	struct dma_chan *tx_chan;
+	struct dma_chan *rx_chan;
+	const char *thread_name;
+	unsigned int len;
+	unsigned int error_count;
+	unsigned int failed_tests = 0;
+	unsigned int total_tests = 0;
+	dma_cookie_t tx_cookie;
+	dma_cookie_t rx_cookie;
+	enum dma_status status;
+	enum dma_ctrl_flags flags;
+	int ret;
+	int frm_cnt = 8;
+	int i;
+	int hsize = 64;
+	int vsize = 32;
+	struct xilinx_vdma_config config;
+	thread_name = current->comm;
+
+	ret = -ENOMEM;
+
+	/* JZ: limit testing scope here */
+	iterations = 1;
+	test_buf_size = hsize * vsize;
+
+	smp_rmb();
+	tx_chan = thread->tx_chan;
+	rx_chan = thread->rx_chan;
+
+	thread->srcs = kcalloc(frm_cnt+1, sizeof(u8 *), GFP_KERNEL);
+	if (!thread->srcs)
+		goto err_srcs;
+	for (i = 0; i < frm_cnt; i++) {
+		thread->srcs[i] = kmalloc(test_buf_size, GFP_KERNEL);
+		if (!thread->srcs[i])
+			goto err_srcbuf;
+	}
+	thread->srcs[i] = NULL;
+
+	thread->dsts = kcalloc(frm_cnt+1, sizeof(u8 *), GFP_KERNEL);
+	if (!thread->dsts)
+		goto err_dsts;
+	for (i = 0; i < frm_cnt; i++) {
+		thread->dsts[i] = kmalloc(test_buf_size, GFP_KERNEL);
+		if (!thread->dsts[i])
+			goto err_dstbuf;
+	}
+	thread->dsts[i] = NULL;
+
+	set_user_nice(current, 10);
+
+	flags = DMA_CTRL_ACK | DMA_COMPL_SKIP_DEST_UNMAP | DMA_PREP_INTERRUPT;
+
+	while (!kthread_should_stop()
+		&& !(iterations && total_tests >= iterations)) {
+		struct dma_device *tx_dev = tx_chan->device;
+		struct dma_device *rx_dev = rx_chan->device;
+		struct dma_async_tx_descriptor *txd = NULL;
+		struct dma_async_tx_descriptor *rxd = NULL;
+		dma_addr_t dma_srcs[frm_cnt];
+		dma_addr_t dma_dsts[frm_cnt];
+		struct completion rx_cmp;
+		struct completion tx_cmp;
+		unsigned long rx_tmo =
+				msecs_to_jiffies(30000); /* RX takes longer */
+		unsigned long tx_tmo = msecs_to_jiffies(30000);
+		u8 align = 0;
+		struct scatterlist tx_sg[frm_cnt];
+		struct scatterlist rx_sg[frm_cnt];
+
+		total_tests++;
+
+		/* honor larger alignment restrictions */
+		align = tx_dev->copy_align;
+		if (rx_dev->copy_align > align)
+			align = rx_dev->copy_align;
+
+		if (1 << align > test_buf_size) {
+			pr_err("%u-byte buffer too small for %d-byte alignment\n",
+			       test_buf_size, 1 << align);
+			break;
+		}
+
+		len = test_buf_size;
+		vdmatest_init_srcs(thread->srcs, 0, len);
+		vdmatest_init_dsts(thread->dsts, 0, len);
+
+		sg_init_table(tx_sg, frm_cnt);
+		sg_init_table(rx_sg, frm_cnt);
+
+		for (i = 0; i < frm_cnt; i++) {
+			u8 *buf = thread->srcs[i];
+
+			dma_srcs[i] = dma_map_single(tx_dev->dev, buf, len,
+							DMA_MEM_TO_DEV);
+			pr_debug("src buf %x dma %x\n", (unsigned int)buf,
+							dma_srcs[i]);
+			sg_dma_address(&tx_sg[i]) = dma_srcs[i];
+			sg_dma_len(&tx_sg[i]) = len;
+		}
+
+		for (i = 0; i < frm_cnt; i++) {
+			dma_dsts[i] = dma_map_single(rx_dev->dev,
+							thread->dsts[i],
+							test_buf_size,
+							DMA_DEV_TO_MEM);
+			pr_debug("dst %x dma %x\n",
+					(unsigned int)thread->dsts[i],
+					dma_dsts[i]);
+			sg_dma_address(&rx_sg[i]) = dma_dsts[i];
+			sg_dma_len(&rx_sg[i]) = len;
+		}
+
+		/* Set up hardware configuration information */
+		config.direction = DMA_MEM_TO_DEV;
+		config.vsize = vsize;
+		config.hsize = hsize;
+		config.stride = hsize;
+		config.frm_cnt_en = 1;
+		config.coalesc = frm_cnt * 10;
+		config.delay = 0;
+		/* The following is do-not-care, need to set to 0 */
+		config.frm_dly = 0;
+		config.park = 1;
+		config.gen_lock = 0;
+		config.master = 0;
+		config.park_frm = 0;
+		config.disable_intr = 0;
+		tx_dev->device_control(tx_chan, DMA_SLAVE_CONFIG,
+					(unsigned long)&config);
+
+		config.direction = DMA_DEV_TO_MEM;
+		config.park = 0;
+		rx_dev->device_control(rx_chan, DMA_SLAVE_CONFIG,
+					(unsigned long)&config);
+
+		rxd = rx_dev->device_prep_slave_sg(rx_chan, rx_sg, frm_cnt,
+				DMA_DEV_TO_MEM, flags, NULL);
+
+		txd = tx_dev->device_prep_slave_sg(tx_chan, tx_sg, frm_cnt,
+				DMA_MEM_TO_DEV, flags, NULL);
+
+		if (!rxd || !txd) {
+			for (i = 0; i < frm_cnt; i++)
+				dma_unmap_single(tx_dev->dev, dma_srcs[i], len,
+						DMA_MEM_TO_DEV);
+			for (i = 0; i < frm_cnt; i++)
+				dma_unmap_single(rx_dev->dev, dma_dsts[i],
+						test_buf_size,
+						DMA_DEV_TO_MEM);
+			pr_warn("%s: #%u: prep error with len=0x%x ",
+					thread_name, total_tests - 1, len);
+			msleep(100);
+			failed_tests++;
+			continue;
+		}
+
+		init_completion(&rx_cmp);
+		rxd->callback = vdmatest_slave_rx_callback;
+		rxd->callback_param = &rx_cmp;
+		rx_cookie = rxd->tx_submit(rxd);
+
+		init_completion(&tx_cmp);
+		txd->callback = vdmatest_slave_tx_callback;
+		txd->callback_param = &tx_cmp;
+		tx_cookie = txd->tx_submit(txd);
+
+		if (dma_submit_error(rx_cookie) ||
+				dma_submit_error(tx_cookie)) {
+			pr_warn("%s: #%u: submit error %d/%d with len=0x%x ",
+					thread_name, total_tests - 1,
+					rx_cookie, tx_cookie, len);
+			msleep(100);
+			failed_tests++;
+			continue;
+		}
+		dma_async_issue_pending(tx_chan);
+		dma_async_issue_pending(rx_chan);
+
+		tx_tmo = wait_for_completion_timeout(&tx_cmp, tx_tmo);
+
+		status = dma_async_is_tx_complete(tx_chan, tx_cookie,
+							NULL, NULL);
+
+		if (tx_tmo == 0) {
+			pr_warn("%s: #%u: tx test timed out\n",
+					thread_name, total_tests - 1);
+			failed_tests++;
+			continue;
+		} else if (status != DMA_SUCCESS) {
+			pr_warn(
+			"%s: #%u: tx got completion callback, ",
+				   thread_name, total_tests - 1);
+			pr_warn("but status is \'%s\'\n",
+				   status == DMA_ERROR ? "error" :
+							"in progress");
+			failed_tests++;
+			continue;
+		}
+
+		rx_tmo = wait_for_completion_timeout(&rx_cmp, rx_tmo);
+		status = dma_async_is_tx_complete(rx_chan, rx_cookie,
+							NULL, NULL);
+
+		if (rx_tmo == 0) {
+			pr_warn("%s: #%u: rx test timed out\n",
+					thread_name, total_tests - 1);
+			failed_tests++;
+			continue;
+		} else if (status != DMA_SUCCESS) {
+			pr_warn(
+			"%s: #%u: rx got completion callback, ",
+					thread_name, total_tests - 1);
+			pr_warn("but status is \'%s\'\n",
+					status == DMA_ERROR ? "error" :
+							"in progress");
+			failed_tests++;
+			continue;
+		}
+
+		/* Unmap by myself (see DMA_COMPL_SKIP_DEST_UNMAP above) */
+		for (i = 0; i < frm_cnt; i++)
+			dma_unmap_single(rx_dev->dev, dma_dsts[i],
+					 test_buf_size, DMA_DEV_TO_MEM);
+
+		error_count = 0;
+
+		pr_debug("%s: verifying source buffer...\n", thread_name);
+		error_count += vdmatest_verify(thread->srcs, 0, 0,
+				0, PATTERN_SRC, true);
+		error_count += vdmatest_verify(thread->srcs, 0,
+				len, 0, PATTERN_SRC | PATTERN_COPY, true);
+		error_count += vdmatest_verify(thread->srcs, len,
+				test_buf_size, len, PATTERN_SRC, true);
+
+		pr_debug("%s: verifying dest buffer...\n",
+				thread->task->comm);
+		error_count += vdmatest_verify(thread->dsts, 0, 0,
+				0, PATTERN_DST, false);
+		error_count += vdmatest_verify(thread->dsts, 0,
+				len, 0, PATTERN_SRC | PATTERN_COPY, false);
+		error_count += vdmatest_verify(thread->dsts, len,
+				test_buf_size, len, PATTERN_DST, false);
+
+		if (error_count) {
+			pr_warn("%s: #%u: %u errors with len=0x%x\n",
+				thread_name, total_tests - 1, error_count, len);
+			failed_tests++;
+		} else {
+			pr_debug("%s: #%u: No errors with len=0x%x\n",
+				thread_name, total_tests - 1, len);
+		}
+	}
+
+	ret = 0;
+	for (i = 0; thread->dsts[i]; i++)
+		kfree(thread->dsts[i]);
+err_dstbuf:
+	kfree(thread->dsts);
+err_dsts:
+	for (i = 0; thread->srcs[i]; i++)
+		kfree(thread->srcs[i]);
+err_srcbuf:
+	kfree(thread->srcs);
+err_srcs:
+	pr_notice("%s: terminating after %u tests, %u failures (status %d)\n",
+			thread_name, total_tests, failed_tests, ret);
+
+	if (iterations > 0)
+		while (!kthread_should_stop()) {
+			DECLARE_WAIT_QUEUE_HEAD_ONSTACK(wait_vdmatest_exit);
+			interruptible_sleep_on(&wait_vdmatest_exit);
+		}
+
+	return ret;
+}
+
+static void vdmatest_cleanup_channel(struct vdmatest_chan *dtc)
+{
+	struct vdmatest_slave_thread *thread;
+	struct vdmatest_slave_thread *_thread;
+	int ret;
+
+	list_for_each_entry_safe(thread, _thread,
+				&dtc->threads, node) {
+		ret = kthread_stop(thread->task);
+		pr_info("vdmatest: thread %s exited with status %d\n",
+				thread->task->comm, ret);
+		list_del(&thread->node);
+		kfree(thread);
+	}
+	kfree(dtc);
+}
+
+static int vdmatest_add_slave_threads(struct vdmatest_chan *tx_dtc,
+					struct vdmatest_chan *rx_dtc)
+{
+	struct vdmatest_slave_thread *thread;
+	struct dma_chan *tx_chan = tx_dtc->chan;
+	struct dma_chan *rx_chan = rx_dtc->chan;
+
+	thread = kzalloc(sizeof(struct vdmatest_slave_thread), GFP_KERNEL);
+	if (!thread) {
+		pr_warn("vdmatest: No memory for slave thread %s-%s\n",
+			   dma_chan_name(tx_chan), dma_chan_name(rx_chan));
+
+	}
+
+	thread->tx_chan = tx_chan;
+	thread->rx_chan = rx_chan;
+	thread->type = (enum dma_transaction_type)DMA_SLAVE;
+	smp_wmb();
+	thread->task = kthread_run(vdmatest_slave_func, thread, "%s-%s",
+		dma_chan_name(tx_chan), dma_chan_name(rx_chan));
+	if (IS_ERR(thread->task)) {
+		pr_warn("vdmatest: Failed to run thread %s-%s\n",
+				dma_chan_name(tx_chan), dma_chan_name(rx_chan));
+		kfree(thread);
+	}
+
+	/* srcbuf and dstbuf are allocated by the thread itself */
+
+	list_add_tail(&thread->node, &tx_dtc->threads);
+
+	/* Added one thread with 2 channels */
+	return 1;
+}
+
+static int vdmatest_add_slave_channels(struct dma_chan *tx_chan,
+					struct dma_chan *rx_chan)
+{
+	struct vdmatest_chan *tx_dtc;
+	struct vdmatest_chan *rx_dtc;
+	unsigned int thread_count = 0;
+
+	tx_dtc = kmalloc(sizeof(struct vdmatest_chan), GFP_KERNEL);
+	if (!tx_dtc) {
+		pr_warn("vdmatest: No memory for tx %s\n",
+					dma_chan_name(tx_chan));
+		return -ENOMEM;
+	}
+
+	rx_dtc = kmalloc(sizeof(struct vdmatest_chan), GFP_KERNEL);
+	if (!rx_dtc) {
+		pr_warn("vdmatest: No memory for rx %s\n",
+					dma_chan_name(rx_chan));
+		return -ENOMEM;
+	}
+
+	tx_dtc->chan = tx_chan;
+	rx_dtc->chan = rx_chan;
+	INIT_LIST_HEAD(&tx_dtc->threads);
+	INIT_LIST_HEAD(&rx_dtc->threads);
+
+	vdmatest_add_slave_threads(tx_dtc, rx_dtc);
+	thread_count += 1;
+
+	pr_info("vdmatest: Started %u threads using %s %s\n",
+		thread_count, dma_chan_name(tx_chan), dma_chan_name(rx_chan));
+
+	list_add_tail(&tx_dtc->node, &vdmatest_channels);
+	list_add_tail(&rx_dtc->node, &vdmatest_channels);
+	nr_channels += 2;
+
+	return 0;
+}
+
+static bool xdma_filter(struct dma_chan *chan, void *param)
+{
+	if (*((int *)chan->private) == *(int *)param)
+		return true;
+
+	return false;
+}
+
+static int __init vdmatest_init(void)
+{
+	dma_cap_mask_t mask;
+	struct dma_chan *chan;
+	int err = 0;
+
+	enum dma_data_direction direction;
+	u32 match;
+	struct dma_chan *rx_chan;
+
+	dma_cap_zero(mask);
+	dma_cap_set(DMA_SLAVE | DMA_PRIVATE, mask);
+
+	direction = DMA_MEM_TO_DEV;
+	match = (direction & 0xFF) | XILINX_DMA_IP_VDMA;
+	pr_debug("vdmatest: match is %x\n", match);
+
+	chan = dma_request_channel(mask, xdma_filter, (void *)&match);
+
+	if (chan)
+		pr_debug("vdmatest: Found tx device\n");
+	else
+		pr_info("vdmatest: Did not find tx device\n");
+
+	direction = DMA_DEV_TO_MEM;
+	match = (direction & 0xFF) | XILINX_DMA_IP_VDMA;
+	rx_chan = dma_request_channel(mask, xdma_filter, &match);
+
+	if (rx_chan)
+		pr_debug("vdmatest: Found rx device\n");
+	else
+		pr_info("vdmatest: Did not find rx device\n");
+
+	if (chan && rx_chan) {
+		err = vdmatest_add_slave_channels(chan, rx_chan);
+		if (err) {
+			dma_release_channel(chan);
+			dma_release_channel(rx_chan);
+		}
+	}
+	return err;
+}
+/* when compiled-in wait for drivers to load first */
+late_initcall(vdmatest_init);
+
+static void __exit vdmatest_exit(void)
+{
+	struct vdmatest_chan *dtc, *_dtc;
+	struct dma_chan *chan;
+
+	list_for_each_entry_safe(dtc, _dtc, &vdmatest_channels, node) {
+		list_del(&dtc->node);
+		chan = dtc->chan;
+		vdmatest_cleanup_channel(dtc);
+		pr_info("vdmatest: dropped channel %s\n",
+			dma_chan_name(chan));
+		dma_release_channel(chan);
+	}
+}
+module_exit(vdmatest_exit);
+
+MODULE_AUTHOR("Xilinx, Inc.");
+MODULE_DESCRIPTION("Xilinx AXI VDMA Test Client");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/dma/xilinx/xilinx_axicdma.c b/drivers/dma/xilinx/xilinx_axicdma.c
new file mode 100644
index 0000000..170c67a
--- /dev/null
+++ b/drivers/dma/xilinx/xilinx_axicdma.c
@@ -0,0 +1,1095 @@
+/*
+ * Xilinx Central DMA Engine support
+ *
+ * Copyright (C) 2010 Xilinx, Inc. All rights reserved.
+ *
+ * Based on the Freescale DMA driver.
+ *
+ * Description:
+ *  . Axi CDMA engine, it does transfers between memory and memory
+ *
+ * This is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/interrupt.h>
+#include <linux/dmapool.h>
+#include <linux/io.h>
+#include <linux/of.h>
+#include <linux/of_platform.h>
+#include <linux/platform_device.h>
+#include <linux/of_address.h>
+#include <linux/of_irq.h>
+#include <linux/amba/xilinx_dma.h>
+
+/* Hw specific definitions */
+#define XILINX_CDMA_MAX_CHANS_PER_DEVICE	0x1
+#define XILINX_CDMA_MAX_TRANS_LEN		0x7FFFFF
+
+/* General register bits definitions */
+#define XILINX_CDMA_CR_RESET_MASK		0x00000004
+						/* Reset DMA engine */
+
+#define XILINX_CDMA_SR_IDLE_MASK		0x00000002
+						/* DMA channel idle */
+
+#define XILINX_CDMA_SR_ERR_INTERNAL_MASK	0x00000010
+						/* Datamover internal err */
+#define XILINX_CDMA_SR_ERR_SLAVE_MASK		0x00000020
+						/* Datamover slave err */
+#define XILINX_CDMA_SR_ERR_DECODE_MASK		0x00000040
+						/* Datamover decode err */
+#define XILINX_CDMA_SR_ERR_SG_INT_MASK		0x00000100
+						/* SG internal err */
+#define XILINX_CDMA_SR_ERR_SG_SLV_MASK		0x00000200
+						/* SG slave err */
+#define XILINX_CDMA_SR_ERR_SG_DEC_MASK		0x00000400
+						/* SG decode err */
+#define XILINX_CDMA_SR_ERR_ALL_MASK		0x00000770
+						/* All errors */
+
+#define XILINX_CDMA_XR_IRQ_IOC_MASK	0x00001000
+						/* Completion interrupt */
+#define XILINX_CDMA_XR_IRQ_DELAY_MASK	0x00002000
+						/* Delay interrupt */
+#define XILINX_CDMA_XR_IRQ_ERROR_MASK	0x00004000
+						/* Error interrupt */
+#define XILINX_CDMA_XR_IRQ_ALL_MASK	0x00007000
+						/* All interrupts */
+
+#define XILINX_CDMA_XR_DELAY_MASK	0xFF000000
+						/* Delay timeout counter */
+#define XILINX_CDMA_XR_COALESCE_MASK	0x00FF0000
+						/* Coalesce counter */
+
+#define XILINX_CDMA_IRQ_SHIFT		12
+#define XILINX_CDMA_DELAY_SHIFT		24
+#define XILINX_CDMA_COALESCE_SHIFT	16
+
+#define XILINX_CDMA_DELAY_MAX		0xFF
+					/* Maximum delay counter value */
+#define XILINX_CDMA_COALESCE_MAX	0xFF
+					/* Maximum coalescing counter value */
+
+#define XILINX_CDMA_CR_SGMODE_MASK	0x00000008
+					/* Scatter gather mode */
+
+#define XILINX_CDMA_SR_SGINCLD_MASK		0x00000008
+					/* Hybrid build */
+#define XILINX_CDMA_XR_IRQ_SIMPLE_ALL_MASK	0x00005000
+					/* All interrupts for simple mode */
+
+/* BD definitions for Axi Cdma */
+#define XILINX_CDMA_BD_STS_COMPL_MASK	0x80000000
+#define XILINX_CDMA_BD_STS_ERR_MASK	0x70000000
+#define XILINX_CDMA_BD_STS_ALL_MASK	0xF0000000
+
+/* Feature encodings */
+#define XILINX_CDMA_FTR_DATA_WIDTH_MASK	0x000000FF
+						/* Data width mask, 1024 */
+#define XILINX_CDMA_FTR_HAS_SG		0x00000100
+						/* Has SG */
+#define XILINX_CDMA_FTR_HAS_SG_SHIFT	8
+						/* Has SG shift */
+
+/* Delay loop counter to prevent hardware failure */
+#define XILINX_CDMA_RESET_LOOP	1000000
+#define XILINX_CDMA_HALT_LOOP	1000000
+
+/* Device Id in the private structure */
+#define XILINX_CDMA_DEVICE_ID_SHIFT	28
+
+/* IO accessors */
+#define CDMA_OUT(addr, val)	(iowrite32(val, addr))
+#define CDMA_IN(addr)		(ioread32(addr))
+
+/* Hardware descriptor */
+struct xilinx_cdma_desc_hw {
+	u32 next_desc;	/* 0x00 */
+	u32 pad1;	/* 0x04 */
+	u32 src_addr;	/* 0x08 */
+	u32 pad2;	/* 0x0C */
+	u32 dest_addr;	/* 0x10 */
+	u32 pad3;	/* 0x14 */
+	u32 control;	/* 0x18 */
+	u32 status;	/* 0x1C */
+} __aligned(64);
+
+/* Software descriptor */
+struct xilinx_cdma_desc_sw {
+	struct xilinx_cdma_desc_hw hw;
+	struct list_head node;
+	struct list_head tx_list;
+	struct dma_async_tx_descriptor async_tx;
+} __aligned(64);
+
+/* AXI CDMA Registers Structure */
+struct xcdma_regs {
+	u32 cr;		/* 0x00 Control Register */
+	u32 sr;		/* 0x04 Status Register */
+	u32 cdr;	/* 0x08 Current Descriptor Register */
+	u32 pad1;
+	u32 tdr;	/* 0x10 Tail Descriptor Register */
+	u32 pad2;
+	u32 src;	/* 0x18 Source Address Register */
+	u32 pad3;
+	u32 dst;	/* 0x20 Destination Address Register */
+	u32 pad4;
+	u32 btt_ref;	/* 0x28 Bytes To Transfer */
+};
+
+/* Per DMA specific operations should be embedded in the channel structure */
+struct xilinx_cdma_chan {
+	struct xcdma_regs __iomem *regs;	/* Control status registers */
+	dma_cookie_t completed_cookie;		/* Maximum cookie completed */
+	dma_cookie_t cookie;			/* The current cookie */
+	spinlock_t lock;			/* Descriptor operation lock */
+	bool sg_waiting;			/* SG transfer waiting */
+	struct list_head active_list;		/* Active descriptors */
+	struct list_head pending_list;		/* Descriptors waiting */
+	struct dma_chan common;			/* DMA common channel */
+	struct dma_pool *desc_pool;		/* Descriptors pool */
+	struct device *dev;			/* The dma device */
+	int irq;				/* Channel IRQ */
+	int id;					/* Channel ID */
+	enum dma_transfer_direction direction;	/* Transfer direction */
+	int max_len;				/* Max data len per transfer */
+	int is_lite;				/* Whether is light build */
+	int has_SG;				/* Support scatter transfers */
+	int has_DRE;				/* For unaligned transfers */
+	int err;				/* Channel has errors */
+	struct tasklet_struct tasklet;		/* Cleanup work after irq */
+	u32 feature;				/* IP feature */
+	u32 private;				/* Match info for
+							channel request */
+	void (*start_transfer)(struct xilinx_cdma_chan *chan);
+	struct xilinx_cdma_config config;	/* Device configuration info */
+};
+
+struct xilinx_cdma_device {
+	void __iomem *regs;
+	struct device *dev;
+	struct dma_device common;
+	struct xilinx_cdma_chan *chan[XILINX_CDMA_MAX_CHANS_PER_DEVICE];
+	u32 feature;
+	int irq;
+};
+
+#define to_xilinx_chan(chan) \
+			container_of(chan, struct xilinx_cdma_chan, common)
+
+/* Required functions */
+
+static int xilinx_cdma_alloc_chan_resources(struct dma_chan *dchan)
+{
+	struct xilinx_cdma_chan *chan = to_xilinx_chan(dchan);
+
+	/* Has this channel already been allocated? */
+	if (chan->desc_pool)
+		return 1;
+
+	/*
+	 * We need the descriptor to be aligned to 64bytes
+	 * for meeting Xilinx DMA specification requirement.
+	 */
+	chan->desc_pool = dma_pool_create("xilinx_cdma_desc_pool",
+				chan->dev,
+				sizeof(struct xilinx_cdma_desc_sw),
+				__alignof__(struct xilinx_cdma_desc_sw), 0);
+	if (!chan->desc_pool) {
+		dev_err(chan->dev,
+			"unable to allocate channel %d descriptor pool\n",
+			chan->id);
+		return -ENOMEM;
+	}
+
+	chan->completed_cookie = 1;
+	chan->cookie = 1;
+
+	/* there is at least one descriptor free to be allocated */
+	return 1;
+}
+
+static void xilinx_cdma_free_desc_list(struct xilinx_cdma_chan *chan,
+					struct list_head *list)
+{
+	struct xilinx_cdma_desc_sw *desc, *_desc;
+
+	list_for_each_entry_safe(desc, _desc, list, node) {
+		list_del(&desc->node);
+		dma_pool_free(chan->desc_pool, desc, desc->async_tx.phys);
+	}
+}
+
+static void xilinx_cdma_free_desc_list_reverse(struct xilinx_cdma_chan *chan,
+						struct list_head *list)
+{
+	struct xilinx_cdma_desc_sw *desc, *_desc;
+
+	list_for_each_entry_safe_reverse(desc, _desc, list, node) {
+		list_del(&desc->node);
+		dma_pool_free(chan->desc_pool, desc, desc->async_tx.phys);
+	}
+}
+
+static void xilinx_cdma_free_chan_resources(struct dma_chan *dchan)
+{
+	struct xilinx_cdma_chan *chan = to_xilinx_chan(dchan);
+	unsigned long flags;
+
+	dev_dbg(chan->dev, "Free all channel resources.\n");
+	spin_lock_irqsave(&chan->lock, flags);
+	xilinx_cdma_free_desc_list(chan, &chan->active_list);
+	xilinx_cdma_free_desc_list(chan, &chan->pending_list);
+	spin_unlock_irqrestore(&chan->lock, flags);
+
+	dma_pool_destroy(chan->desc_pool);
+	chan->desc_pool = NULL;
+}
+
+static enum dma_status xilinx_cdma_desc_status(struct xilinx_cdma_chan *chan,
+					struct xilinx_cdma_desc_sw *desc)
+{
+	return dma_async_is_complete(desc->async_tx.cookie,
+					chan->completed_cookie,
+					chan->cookie);
+}
+
+static void xilinx_cdma_chan_desc_cleanup(struct xilinx_cdma_chan *chan)
+{
+	struct xilinx_cdma_desc_sw *desc, *_desc;
+	unsigned long flags;
+
+	spin_lock_irqsave(&chan->lock, flags);
+
+	list_for_each_entry_safe(desc, _desc, &chan->active_list, node) {
+		dma_async_tx_callback callback;
+		void *callback_param;
+
+		if (xilinx_cdma_desc_status(chan, desc) == DMA_IN_PROGRESS)
+			break;
+
+		/* Remove from the list of running transactions */
+		list_del(&desc->node);
+
+		/* Run the link descriptor callback function */
+		callback = desc->async_tx.callback;
+		callback_param = desc->async_tx.callback_param;
+		if (callback) {
+			spin_unlock_irqrestore(&chan->lock, flags);
+			callback(callback_param);
+			spin_lock_irqsave(&chan->lock, flags);
+		}
+
+		/* Run any dependencies, then free the descriptor */
+		dma_run_dependencies(&desc->async_tx);
+		dma_pool_free(chan->desc_pool, desc, desc->async_tx.phys);
+	}
+
+	spin_unlock_irqrestore(&chan->lock, flags);
+}
+
+static enum dma_status xilinx_tx_status(struct dma_chan *dchan,
+					dma_cookie_t cookie,
+					struct dma_tx_state *txstate)
+{
+	struct xilinx_cdma_chan *chan = to_xilinx_chan(dchan);
+	dma_cookie_t last_used;
+	dma_cookie_t last_complete;
+
+	xilinx_cdma_chan_desc_cleanup(chan);
+
+	last_used = dchan->cookie;
+	last_complete = chan->completed_cookie;
+
+	dma_set_tx_state(txstate, last_complete, last_used, 0);
+
+	return dma_async_is_complete(cookie, last_complete, last_used);
+}
+
+static int cdma_is_idle(struct xilinx_cdma_chan *chan)
+{
+	return CDMA_IN(&chan->regs->sr) & XILINX_CDMA_SR_IDLE_MASK;
+}
+
+/* Only needed for Axi CDMA v2_00_a or earlier core */
+static void cdma_sg_toggle(struct xilinx_cdma_chan *chan)
+{
+	CDMA_OUT(&chan->regs->cr,
+		CDMA_IN(&chan->regs->cr) & ~XILINX_CDMA_CR_SGMODE_MASK);
+
+	CDMA_OUT(&chan->regs->cr,
+		CDMA_IN(&chan->regs->cr) | XILINX_CDMA_CR_SGMODE_MASK);
+}
+
+#define XILINX_CDMA_DRIVER_DEBUG	0
+
+#if (XILINX_CDMA_DRIVER_DEBUG == 1)
+static void desc_dump(struct xilinx_cdma_desc_hw *hw)
+{
+	pr_info("hw desc %x:\n", (unsigned int)hw);
+	pr_info("\tnext_desc %x\n", hw->next_desc);
+	pr_info("\tsrc_addr %x\n", hw->src_addr);
+	pr_info("\tdest_addr %x\n", hw->dest_addr);
+	pr_info("\thsize %x\n", hw->hsize);
+	pr_info("\tcontrol %x\n", hw->control);
+	pr_info("\tstatus %x\n", hw->status);
+}
+#endif
+
+static void xilinx_cdma_start_transfer(struct xilinx_cdma_chan *chan)
+{
+	unsigned long flags;
+	struct xilinx_cdma_desc_sw *desch, *desct;
+	struct xilinx_cdma_desc_hw *hw;
+
+	if (chan->err)
+		return;
+
+	spin_lock_irqsave(&chan->lock, flags);
+
+	if (list_empty(&chan->pending_list))
+		goto out_unlock;
+
+	/* If hardware is busy, cannot submit */
+	if (!cdma_is_idle(chan)) {
+		dev_dbg(chan->dev, "DMA controller still busy %x\n",
+					CDMA_IN(&chan->regs->sr));
+		goto out_unlock;
+	}
+
+	/* Enable interrupts */
+	CDMA_OUT(&chan->regs->cr,
+	    CDMA_IN(&chan->regs->cr) | XILINX_CDMA_XR_IRQ_ALL_MASK);
+
+	desch = list_first_entry(&chan->pending_list,
+			struct xilinx_cdma_desc_sw, node);
+
+	if (chan->has_SG) {
+
+		/* If hybrid mode, append pending list to active list */
+		desct = container_of(chan->pending_list.prev,
+				struct xilinx_cdma_desc_sw, node);
+
+		list_splice_tail_init(&chan->pending_list, &chan->active_list);
+
+		/*
+		 * If hardware is idle, then all descriptors on the active list
+		 * are done, start new transfers
+		 */
+		cdma_sg_toggle(chan);
+
+		CDMA_OUT(&chan->regs->cdr, desch->async_tx.phys);
+
+		/* Update tail ptr register and start the transfer */
+		CDMA_OUT(&chan->regs->tdr, desct->async_tx.phys);
+		goto out_unlock;
+	}
+
+	/* In simple mode */
+	list_del(&desch->node);
+	list_add_tail(&desch->node, &chan->active_list);
+
+	hw = &desch->hw;
+
+	CDMA_OUT(&chan->regs->src, hw->src_addr);
+	CDMA_OUT(&chan->regs->dst, hw->dest_addr);
+
+	/* Start the transfer */
+	CDMA_OUT(&chan->regs->btt_ref,
+		hw->control & XILINX_CDMA_MAX_TRANS_LEN);
+
+out_unlock:
+	spin_unlock_irqrestore(&chan->lock, flags);
+}
+
+/*
+ * If sg mode, link the pending list to running list; if simple mode, get the
+ * head of the pending list and submit it to hw
+ */
+static void xilinx_cdma_issue_pending(struct dma_chan *dchan)
+{
+	struct xilinx_cdma_chan *chan = to_xilinx_chan(dchan);
+
+	xilinx_cdma_start_transfer(chan);
+}
+
+/**
+ * xilinx_cdma_update_completed_cookie - Update the completed cookie.
+ * @chan : xilinx DMA channel
+ *
+ * CONTEXT: hardirq
+ */
+static void xilinx_cdma_update_completed_cookie(struct xilinx_cdma_chan *chan)
+{
+	struct xilinx_cdma_desc_sw *desc = NULL;
+	struct xilinx_cdma_desc_hw *hw = NULL;
+	unsigned long flags;
+	dma_cookie_t cookie = -EBUSY;
+	int done = 0;
+
+	spin_lock_irqsave(&chan->lock, flags);
+
+	if (list_empty(&chan->active_list)) {
+		dev_dbg(chan->dev, "no running descriptors\n");
+		goto out_unlock;
+	}
+
+	/* Get the last completed descriptor, update the cookie to that */
+	list_for_each_entry(desc, &chan->active_list, node) {
+		if (chan->has_SG) {
+			hw = &desc->hw;
+
+			/* If a BD has no status bits set, hw has it */
+			if (!(hw->status & XILINX_CDMA_BD_STS_ALL_MASK)) {
+				break;
+			} else {
+				done = 1;
+				cookie = desc->async_tx.cookie;
+			}
+		} else {
+			/* In non-SG mode, all active entries are done */
+			done = 1;
+			cookie = desc->async_tx.cookie;
+		}
+	}
+
+	if (done)
+		chan->completed_cookie = cookie;
+
+out_unlock:
+	spin_unlock_irqrestore(&chan->lock, flags);
+}
+
+/* Reset hardware */
+static int cdma_init(struct xilinx_cdma_chan *chan)
+{
+	int loop = XILINX_CDMA_RESET_LOOP;
+	u32 tmp;
+
+	CDMA_OUT(&chan->regs->cr,
+		CDMA_IN(&chan->regs->cr) | XILINX_CDMA_CR_RESET_MASK);
+
+	tmp = CDMA_IN(&chan->regs->cr) & XILINX_CDMA_CR_RESET_MASK;
+
+	/* Wait for the hardware to finish reset */
+	while (loop && tmp) {
+		tmp = CDMA_IN(&chan->regs->cr) & XILINX_CDMA_CR_RESET_MASK;
+		loop -= 1;
+	}
+
+	if (!loop) {
+		dev_err(chan->dev, "reset timeout, cr %x, sr %x\n",
+			CDMA_IN(&chan->regs->cr), CDMA_IN(&chan->regs->sr));
+		return 1;
+	}
+
+	/* For Axi CDMA, always do sg transfers if sg mode is built in */
+	if ((chan->feature & XILINX_DMA_IP_CDMA) && chan->has_SG)
+		CDMA_OUT(&chan->regs->cr, tmp | XILINX_CDMA_CR_SGMODE_MASK);
+
+	return 0;
+}
+
+
+static irqreturn_t cdma_intr_handler(int irq, void *data)
+{
+	struct xilinx_cdma_chan *chan = data;
+	int update_cookie = 0;
+	int to_transfer = 0;
+	u32 stat, reg;
+
+	reg = CDMA_IN(&chan->regs->cr);
+
+	/* Disable intr */
+	CDMA_OUT(&chan->regs->cr,
+		reg & ~XILINX_CDMA_XR_IRQ_ALL_MASK);
+
+	stat = CDMA_IN(&chan->regs->sr);
+	if (!(stat & XILINX_CDMA_XR_IRQ_ALL_MASK))
+		return IRQ_NONE;
+
+	/* Ack the interrupts */
+	CDMA_OUT(&chan->regs->sr, XILINX_CDMA_XR_IRQ_ALL_MASK);
+
+	/* Check for only the interrupts which are enabled */
+	stat &= (reg & XILINX_CDMA_XR_IRQ_ALL_MASK);
+
+	if (stat & XILINX_CDMA_XR_IRQ_ERROR_MASK) {
+		dev_err(chan->dev,
+			"Channel %x has errors %x, cdr %x tdr %x\n",
+			(unsigned int)chan,
+			(unsigned int)CDMA_IN(&chan->regs->sr),
+			(unsigned int)CDMA_IN(&chan->regs->cdr),
+			(unsigned int)CDMA_IN(&chan->regs->tdr));
+		chan->err = 1;
+	}
+
+	/*
+	 * Device takes too long to do the transfer when user requires
+	 * responsiveness
+	 */
+	if (stat & XILINX_CDMA_XR_IRQ_DELAY_MASK)
+		dev_dbg(chan->dev, "Inter-packet latency too long\n");
+
+	if (stat & XILINX_CDMA_XR_IRQ_IOC_MASK) {
+		update_cookie = 1;
+		to_transfer = 1;
+	}
+
+	if (update_cookie)
+		xilinx_cdma_update_completed_cookie(chan);
+
+	if (to_transfer)
+		chan->start_transfer(chan);
+
+	tasklet_schedule(&chan->tasklet);
+	return IRQ_HANDLED;
+}
+
+static void cdma_do_tasklet(unsigned long data)
+{
+	struct xilinx_cdma_chan *chan = (struct xilinx_cdma_chan *)data;
+
+	xilinx_cdma_chan_desc_cleanup(chan);
+}
+
+/* Append the descriptor list to the pending list */
+static void append_desc_queue(struct xilinx_cdma_chan *chan,
+			struct xilinx_cdma_desc_sw *desc)
+{
+	struct xilinx_cdma_desc_sw *tail = container_of(chan->pending_list.prev,
+					struct xilinx_cdma_desc_sw, node);
+	struct xilinx_cdma_desc_hw *hw;
+
+	if (list_empty(&chan->pending_list))
+		goto out_splice;
+
+	/*
+	 * Add the hardware descriptor to the chain of hardware descriptors
+	 * that already exists in memory.
+	 */
+	hw = &(tail->hw);
+	hw->next_desc = (u32)desc->async_tx.phys;
+
+	/*
+	 * Add the software descriptor and all children to the list
+	 * of pending transactions
+	 */
+out_splice:
+	list_splice_tail_init(&desc->tx_list, &chan->pending_list);
+}
+
+/*
+ * Assign cookie to each descriptor, and append the descriptors to the pending
+ * list
+ */
+static dma_cookie_t xilinx_cdma_tx_submit(struct dma_async_tx_descriptor *tx)
+{
+	struct xilinx_cdma_chan *chan = to_xilinx_chan(tx->chan);
+	struct xilinx_cdma_desc_sw *desc = container_of(tx,
+				struct xilinx_cdma_desc_sw, async_tx);
+	struct xilinx_cdma_desc_sw *child;
+	unsigned long flags;
+	dma_cookie_t cookie = -EBUSY;
+
+	if (chan->err) {
+		/*
+		 * If reset fails, need to hard reset the system.
+		 * Channel is no longer functional
+		 */
+		if (!cdma_init(chan))
+			chan->err = 0;
+		else
+			return cookie;
+	}
+
+	spin_lock_irqsave(&chan->lock, flags);
+
+	/*
+	 * assign cookies to all of the software descriptors
+	 * that make up this transaction
+	 */
+	cookie = chan->cookie;
+	list_for_each_entry(child, &desc->tx_list, node) {
+		cookie++;
+		if (cookie < 0)
+			cookie = DMA_MIN_COOKIE;
+
+		child->async_tx.cookie = cookie;
+	}
+
+	chan->cookie = cookie;
+
+	/* put this transaction onto the tail of the pending queue */
+	append_desc_queue(chan, desc);
+
+	spin_unlock_irqrestore(&chan->lock, flags);
+
+	return cookie;
+}
+
+static struct xilinx_cdma_desc_sw *xilinx_cdma_alloc_descriptor(
+					struct xilinx_cdma_chan *chan)
+{
+	struct xilinx_cdma_desc_sw *desc;
+	dma_addr_t pdesc;
+
+	desc = dma_pool_alloc(chan->desc_pool, GFP_ATOMIC, &pdesc);
+	if (!desc) {
+		dev_dbg(chan->dev, "out of memory for desc\n");
+		return NULL;
+	}
+
+	memset(desc, 0, sizeof(*desc));
+	INIT_LIST_HEAD(&desc->tx_list);
+	dma_async_tx_descriptor_init(&desc->async_tx, &chan->common);
+	desc->async_tx.tx_submit = xilinx_cdma_tx_submit;
+	desc->async_tx.phys = pdesc;
+
+	return desc;
+}
+
+/**
+ * xilinx_cdma_prep_memcpy - prepare descriptors for a memcpy transaction
+ * @dchan: DMA channel
+ * @dma_dst: destination address
+ * @dma_src: source address
+ * @len: transfer length
+ * @flags: transfer ack flags
+ */
+static struct dma_async_tx_descriptor *xilinx_cdma_prep_memcpy(
+	struct dma_chan *dchan, dma_addr_t dma_dst, dma_addr_t dma_src,
+	size_t len, unsigned long flags)
+{
+	struct xilinx_cdma_chan *chan;
+	struct xilinx_cdma_desc_sw *first = NULL, *prev = NULL, *new;
+	struct xilinx_cdma_desc_hw *hw, *prev_hw;
+	size_t copy;
+	dma_addr_t src = dma_src;
+	dma_addr_t dst = dma_dst;
+
+	if (!dchan)
+		return NULL;
+
+	if (!len)
+		return NULL;
+
+	chan = to_xilinx_chan(dchan);
+
+	if (chan->err) {
+
+		/*
+		 * If reset fails, need to hard reset the system.
+		 * Channel is no longer functional
+		 */
+		if (!cdma_init(chan))
+			chan->err = 0;
+		else
+			return NULL;
+	}
+
+	/*
+	 * If build does not have Data Realignment Engine (DRE),
+	 * src has to be aligned
+	 */
+	if (!chan->has_DRE) {
+		if ((dma_src &
+			(chan->feature & XILINX_CDMA_FTR_DATA_WIDTH_MASK)) ||
+			(dma_dst &
+			(chan->feature & XILINX_CDMA_FTR_DATA_WIDTH_MASK))) {
+
+			dev_err(chan->dev,
+				"Src/Dest address not aligned when no DRE\n");
+
+			return NULL;
+		}
+	}
+
+	do {
+		/* Allocate descriptor from DMA pool */
+		new = xilinx_cdma_alloc_descriptor(chan);
+		if (!new) {
+			dev_err(chan->dev,
+				"No free memory for link descriptor\n");
+			goto fail;
+		}
+
+		copy = min_t(size_t, len, chan->max_len);
+
+		/* if lite build, transfer cannot cross page boundary */
+		if (chan->is_lite)
+			copy = min(copy, (size_t)(PAGE_MASK -
+						(src & PAGE_MASK)));
+
+		if (!copy) {
+			dev_err(chan->dev,
+				"Got zero transfer length for %x\n",
+					(unsigned int)src);
+			goto fail;
+		}
+
+		hw = &(new->hw);
+		hw->control =
+			(hw->control & ~XILINX_CDMA_MAX_TRANS_LEN) | copy;
+		hw->src_addr = src;
+		hw->dest_addr = dst;
+
+		if (!first)
+			first = new;
+		else {
+			prev_hw = &(prev->hw);
+			prev_hw->next_desc = new->async_tx.phys;
+		}
+
+		new->async_tx.cookie = 0;
+		async_tx_ack(&new->async_tx);
+
+		prev = new;
+		len -= copy;
+		src += copy;
+		dst += copy;
+
+		/* Insert the descriptor to the list */
+		list_add_tail(&new->node, &first->tx_list);
+	} while (len);
+
+	/* Link the last BD with the first BD */
+	hw->next_desc = first->async_tx.phys;
+
+	new->async_tx.flags = flags; /* client is in control of this ack */
+	new->async_tx.cookie = -EBUSY;
+
+	return &first->async_tx;
+
+fail:
+	if (!first)
+		return NULL;
+
+	xilinx_cdma_free_desc_list_reverse(chan, &first->tx_list);
+	return NULL;
+}
+
+/* Run-time device configuration for Axi CDMA */
+static int xilinx_cdma_device_control(struct dma_chan *dchan,
+				enum dma_ctrl_cmd cmd, unsigned long arg)
+{
+	struct xilinx_cdma_chan *chan;
+	unsigned long flags;
+
+	if (!dchan)
+		return -EINVAL;
+
+	chan = to_xilinx_chan(dchan);
+
+	if (cmd == DMA_TERMINATE_ALL) {
+		spin_lock_irqsave(&chan->lock, flags);
+
+		/* Remove and free all of the descriptors in the lists */
+		xilinx_cdma_free_desc_list(chan, &chan->pending_list);
+		xilinx_cdma_free_desc_list(chan, &chan->active_list);
+
+		spin_unlock_irqrestore(&chan->lock, flags);
+		return 0;
+	} else if (cmd == DMA_SLAVE_CONFIG) {
+		/*
+		 * Configure interrupt coalescing and delay counter
+		 * Use value XILINX_CDMA_NO_CHANGE to signal no change
+		 */
+		struct xilinx_cdma_config *cfg =
+				(struct xilinx_cdma_config *)arg;
+		u32 reg = CDMA_IN(&chan->regs->cr);
+
+		if (cfg->coalesc <= XILINX_CDMA_COALESCE_MAX) {
+			reg &= ~XILINX_CDMA_XR_COALESCE_MASK;
+			reg |= cfg->coalesc << XILINX_CDMA_COALESCE_SHIFT;
+
+			chan->config.coalesc = cfg->coalesc;
+		}
+
+		if (cfg->delay <= XILINX_CDMA_DELAY_MAX) {
+			reg &= ~XILINX_CDMA_XR_DELAY_MASK;
+			reg |= cfg->delay << XILINX_CDMA_DELAY_SHIFT;
+			chan->config.delay = cfg->delay;
+		}
+
+		CDMA_OUT(&chan->regs->cr, reg);
+
+		return 0;
+	}
+
+	return -ENXIO;
+}
+
+/*
+ * Logarithm function to compute alignment shift
+ *
+ * Only deals with value less than 4096.
+ */
+static int my_log(int value)
+{
+	int i = 0;
+	while ((1 << i) < value) {
+		i++;
+
+		if (i >= 12)
+			return 0;
+	}
+
+	return i;
+}
+
+static void xilinx_cdma_chan_remove(struct xilinx_cdma_chan *chan)
+{
+	irq_dispose_mapping(chan->irq);
+	list_del(&chan->common.device_node);
+	kfree(chan);
+}
+
+/*
+ * Probing channels
+ *
+ * . Get channel features from the device tree entry
+ * . Initialize special channel handling routines
+ */
+static int __devinit xilinx_cdma_chan_probe(struct xilinx_cdma_device *xdev,
+	struct device_node *node, u32 feature)
+{
+	struct xilinx_cdma_chan *chan;
+	int err;
+	int *value;
+	u32 width = 0, device_id = 0;
+
+	/* alloc channel */
+	chan = kzalloc(sizeof(*chan), GFP_KERNEL);
+	if (!chan) {
+		dev_err(xdev->dev, "no free memory for DMA channels!\n");
+		err = -ENOMEM;
+		goto out_return;
+	}
+
+	chan->feature = feature;
+	chan->is_lite = 0;
+	chan->has_DRE = 0;
+	chan->has_SG = 0;
+	chan->max_len = XILINX_CDMA_MAX_TRANS_LEN;
+
+	value = (int *)of_get_property(node, "xlnx,include-dre",
+			NULL);
+	if (value)
+		chan->has_DRE = be32_to_cpup(value);
+
+	value = (int *)of_get_property(node,
+			"xlnx,datawidth",
+			NULL);
+	if (value) {
+		width = be32_to_cpup(value) >> 3; /* convert bits to bytes */
+
+		/* If data width is greater than 8 bytes, DRE is not in hw */
+		if (width > 8)
+			chan->has_DRE = 0;
+
+		chan->feature |= width - 1;
+	}
+
+	value = (int *)of_get_property(node, "xlnx,device-id", NULL);
+	if (value)
+		device_id = be32_to_cpup(value);
+
+	chan->direction = DMA_MEM_TO_MEM;
+	chan->start_transfer = xilinx_cdma_start_transfer;
+
+	chan->has_SG = (xdev->feature & XILINX_CDMA_FTR_HAS_SG) >>
+			XILINX_CDMA_FTR_HAS_SG_SHIFT;
+
+	value = (int *)of_get_property(node,
+			"xlnx,lite-mode", NULL);
+	if (value) {
+		if (be32_to_cpup(value) == 1) {
+			chan->is_lite = 1;
+			value = (int *)of_get_property(node,
+				"xlnx,max-burst-len", NULL);
+			if (value) {
+				if (!width) {
+					dev_err(xdev->dev,
+						"Lite mode w/o data width property\n");
+					goto out_free_chan;
+				}
+				chan->max_len = width *
+					be32_to_cpup(value);
+			}
+		}
+	}
+
+	chan->regs = (struct xcdma_regs *)xdev->regs;
+	chan->id = 0;
+
+	/*
+	 * Used by dmatest channel matching in slave transfers
+	 * Can change it to be a structure to have more matching information
+	 */
+	chan->private = (chan->direction & 0xFF) |
+		(chan->feature & XILINX_DMA_IP_MASK) |
+		(device_id << XILINX_CDMA_DEVICE_ID_SHIFT);
+	chan->common.private = (void *)&(chan->private);
+
+	if (!chan->has_DRE)
+		xdev->common.copy_align = my_log(width);
+
+	chan->dev = xdev->dev;
+	xdev->chan[chan->id] = chan;
+
+	tasklet_init(&chan->tasklet, cdma_do_tasklet, (unsigned long)chan);
+
+	/* Initialize the channel */
+	if (cdma_init(chan)) {
+		dev_err(xdev->dev, "Reset channel failed\n");
+		goto out_free_chan;
+	}
+
+	spin_lock_init(&chan->lock);
+	INIT_LIST_HEAD(&chan->pending_list);
+	INIT_LIST_HEAD(&chan->active_list);
+
+	chan->common.device = &xdev->common;
+
+	/* Find the IRQ line, if it exists in the device tree */
+	chan->irq = irq_of_parse_and_map(node, 0);
+	err = request_irq(chan->irq, cdma_intr_handler, IRQF_SHARED,
+				"xilinx-cdma-controller", chan);
+	if (err) {
+		dev_err(xdev->dev, "unable to request IRQ\n");
+		goto out_free_irq;
+	}
+
+	/* Add the channel to DMA device channel list */
+	list_add_tail(&chan->common.device_node, &xdev->common.channels);
+	xdev->common.chancnt++;
+
+	return 0;
+
+out_free_irq:
+	irq_dispose_mapping(chan->irq);
+out_free_chan:
+	kfree(chan);
+out_return:
+	return err;
+}
+
+static int __devinit xilinx_cdma_of_probe(struct platform_device *op)
+{
+	struct xilinx_cdma_device *xdev;
+	struct device_node *child, *node;
+	int err;
+	int *value;
+
+	dev_info(&op->dev, "Probing xilinx axi cdma engine\n");
+
+	xdev = kzalloc(sizeof(struct xilinx_cdma_device), GFP_KERNEL);
+	if (!xdev) {
+		dev_err(&op->dev, "Not enough memory for device\n");
+		err = -ENOMEM;
+		goto out_return;
+	}
+
+	xdev->dev = &(op->dev);
+	INIT_LIST_HEAD(&xdev->common.channels);
+
+	node = op->dev.of_node;
+	xdev->feature = 0;
+
+	/* iomap registers */
+	xdev->regs = of_iomap(node, 0);
+	if (!xdev->regs) {
+		dev_err(&op->dev, "unable to iomap registers\n");
+		err = -ENOMEM;
+		goto out_free_xdev;
+	}
+
+	/* Axi CDMA only does memcpy */
+	if (of_device_is_compatible(node, "xlnx,axi-cdma")) {
+		xdev->feature |= XILINX_DMA_IP_CDMA;
+
+		value = (int *)of_get_property(node, "xlnx,include-sg",
+				NULL);
+		if (value) {
+			if (be32_to_cpup(value) == 1)
+				xdev->feature |= XILINX_CDMA_FTR_HAS_SG;
+		}
+
+		dma_cap_set(DMA_MEMCPY, xdev->common.cap_mask);
+		xdev->common.device_prep_dma_memcpy = xilinx_cdma_prep_memcpy;
+		xdev->common.device_control = xilinx_cdma_device_control;
+		xdev->common.device_issue_pending = xilinx_cdma_issue_pending;
+	}
+
+	xdev->common.device_alloc_chan_resources =
+				xilinx_cdma_alloc_chan_resources;
+	xdev->common.device_free_chan_resources =
+				xilinx_cdma_free_chan_resources;
+	xdev->common.device_tx_status = xilinx_tx_status;
+	xdev->common.dev = &op->dev;
+
+	dev_set_drvdata(&op->dev, xdev);
+
+	for_each_child_of_node(node, child) {
+		xilinx_cdma_chan_probe(xdev, child, xdev->feature);
+	}
+
+	dma_async_device_register(&xdev->common);
+
+	return 0;
+
+out_free_xdev:
+	kfree(xdev);
+
+out_return:
+	return err;
+}
+
+static int __devexit xilinx_cdma_of_remove(struct platform_device *op)
+{
+	struct xilinx_cdma_device *xdev;
+	int i;
+
+	xdev = dev_get_drvdata(&op->dev);
+	dma_async_device_unregister(&xdev->common);
+
+	for (i = 0; i < XILINX_CDMA_MAX_CHANS_PER_DEVICE; i++) {
+		if (xdev->chan[i])
+			xilinx_cdma_chan_remove(xdev->chan[i]);
+	}
+
+	iounmap(xdev->regs);
+	dev_set_drvdata(&op->dev, NULL);
+	kfree(xdev);
+
+	return 0;
+}
+
+static const struct of_device_id xilinx_cdma_of_ids[] = {
+	{ .compatible = "xlnx,axi-cdma",},
+	{}
+};
+
+static struct platform_driver xilinx_cdma_of_driver = {
+	.driver = {
+		.name = "xilinx-cdma",
+		.owner = THIS_MODULE,
+		.of_match_table = xilinx_cdma_of_ids,
+	},
+	.probe = xilinx_cdma_of_probe,
+	.remove = __devexit_p(xilinx_cdma_of_remove),
+};
+
+module_platform_driver(xilinx_cdma_of_driver);
+
+MODULE_AUTHOR("Xilinx, Inc.");
+MODULE_DESCRIPTION("Xilinx CDMA driver");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/dma/xilinx/xilinx_axidma.c b/drivers/dma/xilinx/xilinx_axidma.c
new file mode 100644
index 0000000..65a16ed
--- /dev/null
+++ b/drivers/dma/xilinx/xilinx_axidma.c
@@ -0,0 +1,1201 @@
+/*
+ * Xilinx AXI DMA Engine support
+ *
+ * Copyright (C) 2012 Xilinx, Inc. All rights reserved.
+ *
+ * Based on the Freescale DMA driver.
+ *
+ * Description:
+ *  . Axi DMA engine, it does transfers between memory and device. It can be
+ *    configured to have one channel or two channels. If configured as two
+ *    channels, one is to transmit to a device and another is to receive from
+ *    a device.
+ *
+ * This is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/interrupt.h>
+#include <linux/dmapool.h>
+#include <linux/io.h>
+#include <linux/of.h>
+#include <linux/of_platform.h>
+#include <linux/platform_device.h>
+#include <linux/of_address.h>
+#include <linux/of_irq.h>
+#include <linux/amba/xilinx_dma.h>
+
+/* Hw specific definitions */
+#define XILINX_DMA_MAX_CHANS_PER_DEVICE	0x2
+#define XILINX_DMA_MAX_TRANS_LEN	0x7FFFFF
+
+/* General register bits definitions */
+#define XILINX_DMA_CR_RESET_MASK	0x00000004
+						/* Reset DMA engine */
+#define XILINX_DMA_CR_RUNSTOP_MASK	0x00000001
+						/* Start/stop DMA engine */
+
+#define XILINX_DMA_SR_HALTED_MASK	0x00000001
+						/* DMA channel halted */
+#define XILINX_DMA_SR_IDLE_MASK		0x00000002
+						/* DMA channel idle */
+
+#define XILINX_DMA_SR_ERR_INTERNAL_MASK	0x00000010
+						/* Datamover internal err */
+#define XILINX_DMA_SR_ERR_SLAVE_MASK	0x00000020
+						/* Datamover slave err */
+#define XILINX_DMA_SR_ERR_DECODE_MASK	0x00000040
+						/* Datamover decode err */
+#define XILINX_DMA_SR_ERR_SG_INT_MASK	0x00000100
+						/* SG internal err */
+#define XILINX_DMA_SR_ERR_SG_SLV_MASK	0x00000200
+						/* SG slave err */
+#define XILINX_DMA_SR_ERR_SG_DEC_MASK	0x00000400
+						/* SG decode err */
+#define XILINX_DMA_SR_ERR_ALL_MASK	0x00000770
+						/* All errors */
+
+#define XILINX_DMA_XR_IRQ_IOC_MASK	0x00001000
+						/* Completion interrupt */
+#define XILINX_DMA_XR_IRQ_DELAY_MASK	0x00002000
+						/* Delay interrupt */
+#define XILINX_DMA_XR_IRQ_ERROR_MASK	0x00004000
+						/* Error interrupt */
+#define XILINX_DMA_XR_IRQ_ALL_MASK	0x00007000
+						/* All interrupts */
+
+#define XILINX_DMA_XR_DELAY_MASK	0xFF000000
+						/* Delay timeout counter */
+#define XILINX_DMA_XR_COALESCE_MASK	0x00FF0000
+						/* Coalesce counter */
+
+#define XILINX_DMA_IRQ_SHIFT		12
+#define XILINX_DMA_DELAY_SHIFT		24
+#define XILINX_DMA_COALESCE_SHIFT	16
+
+#define XILINX_DMA_DELAY_MAX		0xFF
+					/* Maximum delay counter value */
+#define XILINX_DMA_COALESCE_MAX		0xFF
+					/* Maximum coalescing counter value */
+
+#define XILINX_DMA_RX_CHANNEL_OFFSET	0x30
+
+/* BD definitions for AXI Dma */
+#define XILINX_DMA_BD_STS_COMPL_MASK	0x80000000
+#define XILINX_DMA_BD_STS_ERR_MASK	0x70000000
+#define XILINX_DMA_BD_STS_ALL_MASK	0xF0000000
+
+/* Axi DMA BD special bits definitions */
+#define XILINX_DMA_BD_SOP	0x08000000	/* Start of packet bit */
+#define XILINX_DMA_BD_EOP	0x04000000	/* End of packet bit */
+
+/* Feature encodings */
+#define XILINX_DMA_FTR_DATA_WIDTH_MASK	0x000000FF
+						/* Data width mask, 1024 */
+#define XILINX_DMA_FTR_HAS_SG		0x00000100
+						/* Has SG */
+#define XILINX_DMA_FTR_HAS_SG_SHIFT	8
+						/* Has SG shift */
+#define XILINX_DMA_FTR_STSCNTRL_STRM	0x00010000
+						/* Optional feature for dma */
+
+/* Delay loop counter to prevent hardware failure */
+#define XILINX_DMA_RESET_LOOP		1000000
+#define XILINX_DMA_HALT_LOOP		1000000
+
+/* Device Id in the private structure */
+#define XILINX_DMA_DEVICE_ID_SHIFT	28
+
+/* IO accessors */
+#define DMA_OUT(addr, val)	(iowrite32(val, addr))
+#define DMA_IN(addr)		(ioread32(addr))
+
+#ifdef CONFIG_XILINX_DMATEST
+#define TEST_DMA_WITH_LOOPBACK
+#endif
+
+/* Hardware descriptor */
+struct xilinx_dma_desc_hw {
+	u32 next_desc;	/* 0x00 */
+	u32 pad1;	/* 0x04 */
+	u32 buf_addr;	/* 0x08 */
+	u32 pad2;	/* 0x0C */
+	u32 pad3;	/* 0x10 */
+	u32 pad4;	/* 0x14 */
+	u32 control;	/* 0x18 */
+	u32 status;	/* 0x1C */
+	u32 app_0;	/* 0x20 */
+	u32 app_1;	/* 0x24 */
+	u32 app_2;	/* 0x28 */
+	u32 app_3;	/* 0x2C */
+	u32 app_4;	/* 0x30 */
+} __aligned(64);
+
+/* Software descriptor */
+struct xilinx_dma_desc_sw {
+	struct xilinx_dma_desc_hw hw;
+	struct list_head node;
+	struct list_head tx_list;
+	struct dma_async_tx_descriptor async_tx;
+} __aligned(64);
+
+/* AXI DMA Registers Structure */
+struct xdma_regs {
+	u32 cr;		/* 0x00 Control Register */
+	u32 sr;		/* 0x04 Status Register */
+	u32 cdr;	/* 0x08 Current Descriptor Register */
+	u32 pad1;
+	u32 tdr;	/* 0x10 Tail Descriptor Register */
+	u32 pad2;
+	u32 src;	/* 0x18 Source Address Register (sg = 0) */
+	u32 pad3;
+	u32 dst;	/* 0x20 Destination Address Register (sg = 0) */
+	u32 pad4;
+	u32 btt_ref;	/* 0x28 Bytes To Transfer (sg = 0) */
+};
+
+/* Per DMA specific operations should be embedded in the channel structure */
+struct xilinx_dma_chan {
+	struct xdma_regs __iomem *regs;	/* Control status registers */
+	dma_cookie_t completed_cookie;	/* The maximum cookie completed */
+	dma_cookie_t cookie;		/* The current cookie */
+	spinlock_t lock;		/* Descriptor operation lock */
+	bool sg_waiting;		/* Scatter gather transfer waiting */
+	struct list_head active_list;	/* Active descriptors */
+	struct list_head pending_list;	/* Descriptors waiting */
+	struct dma_chan common;		/* DMA common channel */
+	struct dma_pool *desc_pool;	/* Descriptors pool */
+	struct device *dev;		/* The dma device */
+	int irq;			/* Channel IRQ */
+	int id;				/* Channel ID */
+	enum dma_transfer_direction direction;
+					/* Transfer direction */
+	int max_len;			/* Maximum data len per transfer */
+	int is_lite;			/* Whether is light build */
+	int has_SG;			/* Support scatter transfers */
+	int has_DRE;			/* Support unaligned transfers */
+	int err;			/* Channel has errors */
+	struct tasklet_struct tasklet;	/* Cleanup work after irq */
+	u32 feature;			/* IP feature */
+	u32 private;			/* Match info for channel request */
+	void (*start_transfer)(struct xilinx_dma_chan *chan);
+	struct xilinx_dma_config config;
+					/* Device configuration info */
+};
+
+/* DMA Device Structure */
+struct xilinx_dma_device {
+	void __iomem *regs;
+	struct device *dev;
+	struct dma_device common;
+	struct xilinx_dma_chan *chan[XILINX_DMA_MAX_CHANS_PER_DEVICE];
+	u32 feature;
+	int irq;
+};
+
+#define to_xilinx_chan(chan) \
+	container_of(chan, struct xilinx_dma_chan, common)
+
+/* Required functions */
+
+static int xilinx_dma_alloc_chan_resources(struct dma_chan *dchan)
+{
+	struct xilinx_dma_chan *chan = to_xilinx_chan(dchan);
+
+	/* Has this channel already been allocated? */
+	if (chan->desc_pool)
+		return 1;
+
+	/*
+	 * We need the descriptor to be aligned to 64bytes
+	 * for meeting Xilinx DMA specification requirement.
+	 */
+	chan->desc_pool = dma_pool_create("xilinx_dma_desc_pool",
+				chan->dev,
+				sizeof(struct xilinx_dma_desc_sw),
+				__alignof__(struct xilinx_dma_desc_sw), 0);
+	if (!chan->desc_pool) {
+		dev_err(chan->dev,
+			"unable to allocate channel %d descriptor pool\n",
+					chan->id);
+		return -ENOMEM;
+	}
+
+	chan->completed_cookie = 1;
+	chan->cookie = 1;
+
+	/* There is at least one descriptor free to be allocated */
+	return 1;
+}
+
+static void xilinx_dma_free_desc_list(struct xilinx_dma_chan *chan,
+					struct list_head *list)
+{
+	struct xilinx_dma_desc_sw *desc, *_desc;
+
+	list_for_each_entry_safe(desc, _desc, list, node) {
+		list_del(&desc->node);
+		dma_pool_free(chan->desc_pool, desc, desc->async_tx.phys);
+	}
+}
+
+static void xilinx_dma_free_desc_list_reverse(struct xilinx_dma_chan *chan,
+						struct list_head *list)
+{
+	struct xilinx_dma_desc_sw *desc, *_desc;
+
+	list_for_each_entry_safe_reverse(desc, _desc, list, node) {
+		list_del(&desc->node);
+		dma_pool_free(chan->desc_pool, desc, desc->async_tx.phys);
+	}
+}
+
+static void xilinx_dma_free_chan_resources(struct dma_chan *dchan)
+{
+	struct xilinx_dma_chan *chan = to_xilinx_chan(dchan);
+	unsigned long flags;
+
+	dev_dbg(chan->dev, "Free all channel resources.\n");
+	spin_lock_irqsave(&chan->lock, flags);
+	xilinx_dma_free_desc_list(chan, &chan->active_list);
+	xilinx_dma_free_desc_list(chan, &chan->pending_list);
+	spin_unlock_irqrestore(&chan->lock, flags);
+
+	dma_pool_destroy(chan->desc_pool);
+	chan->desc_pool = NULL;
+}
+
+static enum dma_status xilinx_dma_desc_status(struct xilinx_dma_chan *chan,
+						struct xilinx_dma_desc_sw *desc)
+{
+	return dma_async_is_complete(desc->async_tx.cookie,
+					chan->completed_cookie,
+					chan->cookie);
+}
+
+static void xilinx_chan_desc_cleanup(struct xilinx_dma_chan *chan)
+{
+	struct xilinx_dma_desc_sw *desc, *_desc;
+	unsigned long flags;
+
+	spin_lock_irqsave(&chan->lock, flags);
+
+	list_for_each_entry_safe(desc, _desc, &chan->active_list, node) {
+		dma_async_tx_callback callback;
+		void *callback_param;
+
+		if (xilinx_dma_desc_status(chan, desc) == DMA_IN_PROGRESS)
+			break;
+
+		/* Remove from the list of running transactions */
+		list_del(&desc->node);
+
+		/* Run the link descriptor callback function */
+		callback = desc->async_tx.callback;
+		callback_param = desc->async_tx.callback_param;
+		if (callback) {
+			spin_unlock_irqrestore(&chan->lock, flags);
+			callback(callback_param);
+			spin_lock_irqsave(&chan->lock, flags);
+		}
+
+		/* Run any dependencies, then free the descriptor */
+		dma_run_dependencies(&desc->async_tx);
+		dma_pool_free(chan->desc_pool, desc, desc->async_tx.phys);
+	}
+
+	spin_unlock_irqrestore(&chan->lock, flags);
+}
+
+static enum dma_status xilinx_tx_status(struct dma_chan *dchan,
+					dma_cookie_t cookie,
+					struct dma_tx_state *txstate)
+{
+	struct xilinx_dma_chan *chan = to_xilinx_chan(dchan);
+	dma_cookie_t last_used;
+	dma_cookie_t last_complete;
+
+	xilinx_chan_desc_cleanup(chan);
+
+	last_used = dchan->cookie;
+	last_complete = chan->completed_cookie;
+
+	dma_set_tx_state(txstate, last_complete, last_used, 0);
+
+	return dma_async_is_complete(cookie, last_complete, last_used);
+}
+
+static int dma_is_running(struct xilinx_dma_chan *chan)
+{
+	return !(DMA_IN(&chan->regs->sr) & XILINX_DMA_SR_HALTED_MASK) &&
+		(DMA_IN(&chan->regs->cr) & XILINX_DMA_CR_RUNSTOP_MASK);
+}
+
+static int dma_is_idle(struct xilinx_dma_chan *chan)
+{
+	return DMA_IN(&chan->regs->sr) & XILINX_DMA_SR_IDLE_MASK;
+}
+
+#define XILINX_DMA_DRIVER_DEBUG 0
+
+#if (XILINX_DMA_DRIVER_DEBUG == 1)
+static void desc_dump(struct xilinx_dma_desc_hw *hw)
+{
+	pr_info("hw desc %x:\n", (unsigned int)hw);
+	pr_info("\tnext_desc %x\n", hw->next_desc);
+	pr_info("\tbuf_addr %x\n", hw->buf_addr);
+	pr_info("\taddr_vsize %x\n", hw->addr_vsize);
+	pr_info("\thsize %x\n", hw->hsize);
+	pr_info("\tcontrol %x\n", hw->control);
+	pr_info("\tstatus %x\n", hw->status);
+}
+#endif
+
+/* Stop the hardware, the ongoing transfer will be finished */
+static void dma_halt(struct xilinx_dma_chan *chan)
+{
+	int loop = XILINX_DMA_HALT_LOOP;
+
+	DMA_OUT(&chan->regs->cr,
+		DMA_IN(&chan->regs->cr) & ~XILINX_DMA_CR_RUNSTOP_MASK);
+
+	/* Wait for the hardware to halt */
+	while (loop) {
+		if (!(DMA_IN(&chan->regs->cr) & XILINX_DMA_CR_RUNSTOP_MASK))
+			break;
+
+		loop -= 1;
+	}
+
+	if (!loop) {
+		pr_debug("Cannot stop channel %x: %x\n",
+			(unsigned int)chan,
+			(unsigned int)DMA_IN(&chan->regs->cr));
+		chan->err = 1;
+	}
+
+	return;
+}
+
+/* Start the hardware. Transfers are not started yet */
+static void dma_start(struct xilinx_dma_chan *chan)
+{
+	int loop = XILINX_DMA_HALT_LOOP;
+
+	DMA_OUT(&chan->regs->cr,
+		DMA_IN(&chan->regs->cr) | XILINX_DMA_CR_RUNSTOP_MASK);
+
+	/* Wait for the hardware to start */
+	while (loop) {
+		if (DMA_IN(&chan->regs->cr) & XILINX_DMA_CR_RUNSTOP_MASK)
+			break;
+
+		loop -= 1;
+	}
+
+	if (!loop) {
+		pr_debug("Cannot start channel %x: %x\n",
+			(unsigned int)chan,
+			(unsigned int)DMA_IN(&chan->regs->cr));
+
+		chan->err = 1;
+	}
+
+	return;
+}
+
+
+static void xilinx_dma_start_transfer(struct xilinx_dma_chan *chan)
+{
+	unsigned long flags;
+	struct xilinx_dma_desc_sw *desch, *desct;
+	struct xilinx_dma_desc_hw *hw;
+
+	if (chan->err)
+		return;
+
+	spin_lock_irqsave(&chan->lock, flags);
+
+	if (list_empty(&chan->pending_list))
+		goto out_unlock;
+
+	/* If hardware is busy, cannot submit */
+	if (dma_is_running(chan) && !dma_is_idle(chan)) {
+		dev_dbg(chan->dev, "DMA controller still busy\n");
+		goto out_unlock;
+	}
+
+	/*
+	 * If hardware is idle, then all descriptors on active list are
+	 * done, start new transfers
+	 */
+	dma_halt(chan);
+
+	if (chan->err)
+		goto out_unlock;
+
+	if (chan->has_SG) {
+		desch = list_first_entry(&chan->pending_list,
+				struct xilinx_dma_desc_sw, node);
+
+		desct = container_of(chan->pending_list.prev,
+				struct xilinx_dma_desc_sw, node);
+
+		DMA_OUT(&chan->regs->cdr, desch->async_tx.phys);
+
+		dma_start(chan);
+
+		if (chan->err)
+			goto out_unlock;
+		list_splice_tail_init(&chan->pending_list, &chan->active_list);
+
+		/* Enable interrupts */
+		DMA_OUT(&chan->regs->cr,
+			DMA_IN(&chan->regs->cr) | XILINX_DMA_XR_IRQ_ALL_MASK);
+
+		/* Update tail ptr register and start the transfer */
+		DMA_OUT(&chan->regs->tdr, desct->async_tx.phys);
+		goto out_unlock;
+	}
+
+	/* In simple mode */
+	dma_halt(chan);
+
+	if (chan->err)
+		goto out_unlock;
+
+	pr_info("xilinx_dma_start_transfer::simple DMA mode\n");
+
+	desch = list_first_entry(&chan->pending_list,
+			struct xilinx_dma_desc_sw, node);
+
+	list_del(&desch->node);
+	list_add_tail(&desch->node, &chan->active_list);
+
+	dma_start(chan);
+
+	if (chan->err)
+		goto out_unlock;
+
+	hw = &desch->hw;
+
+	/* Enable interrupts */
+	DMA_OUT(&chan->regs->cr,
+		DMA_IN(&chan->regs->cr) | XILINX_DMA_XR_IRQ_ALL_MASK);
+
+	DMA_OUT(&chan->regs->src, hw->buf_addr);
+
+	/* Start the transfer */
+	DMA_OUT(&chan->regs->btt_ref,
+		hw->control & XILINX_DMA_MAX_TRANS_LEN);
+
+out_unlock:
+	spin_unlock_irqrestore(&chan->lock, flags);
+}
+
+static void xilinx_dma_issue_pending(struct dma_chan *dchan)
+{
+	struct xilinx_dma_chan *chan = to_xilinx_chan(dchan);
+
+	xilinx_dma_start_transfer(chan);
+}
+
+/**
+ * xilinx_dma_update_completed_cookie - Update the completed cookie.
+ * @chan : xilinx DMA channel
+ *
+ * CONTEXT: hardirq
+ */
+static void xilinx_dma_update_completed_cookie(struct xilinx_dma_chan *chan)
+{
+	struct xilinx_dma_desc_sw *desc = NULL;
+	struct xilinx_dma_desc_hw *hw = NULL;
+	unsigned long flags;
+	dma_cookie_t cookie = -EBUSY;
+	int done = 0;
+
+	spin_lock_irqsave(&chan->lock, flags);
+
+	if (list_empty(&chan->active_list)) {
+		dev_dbg(chan->dev, "no running descriptors\n");
+		goto out_unlock;
+	}
+
+	/* Get the last completed descriptor, update the cookie to that */
+	list_for_each_entry(desc, &chan->active_list, node) {
+		if (chan->has_SG) {
+			hw = &desc->hw;
+
+			/* If a BD has no status bits set, hw has it */
+			if (!(hw->status & XILINX_DMA_BD_STS_ALL_MASK)) {
+				break;
+			} else {
+				done = 1;
+				cookie = desc->async_tx.cookie;
+			}
+		} else {
+			/* In non-SG mode, all active entries are done */
+			done = 1;
+			cookie = desc->async_tx.cookie;
+		}
+	}
+
+	if (done)
+		chan->completed_cookie = cookie;
+
+out_unlock:
+	spin_unlock_irqrestore(&chan->lock, flags);
+}
+
+/* Reset hardware */
+static int dma_init(struct xilinx_dma_chan *chan)
+{
+	int loop = XILINX_DMA_RESET_LOOP;
+	u32 tmp;
+
+	DMA_OUT(&chan->regs->cr,
+		DMA_IN(&chan->regs->cr) | XILINX_DMA_CR_RESET_MASK);
+
+	tmp = DMA_IN(&chan->regs->cr) & XILINX_DMA_CR_RESET_MASK;
+
+	/* Wait for the hardware to finish reset */
+	while (loop && tmp) {
+		tmp = DMA_IN(&chan->regs->cr) & XILINX_DMA_CR_RESET_MASK;
+		loop -= 1;
+	}
+
+	if (!loop) {
+		dev_err(chan->dev, "reset timeout, cr %x, sr %x\n",
+			DMA_IN(&chan->regs->cr), DMA_IN(&chan->regs->sr));
+		return 1;
+	}
+
+	return 0;
+}
+
+
+static irqreturn_t dma_intr_handler(int irq, void *data)
+{
+	struct xilinx_dma_chan *chan = data;
+	int update_cookie = 0;
+	int to_transfer = 0;
+	u32 stat, reg;
+
+	reg = DMA_IN(&chan->regs->cr);
+
+	/* Disable intr */
+	DMA_OUT(&chan->regs->cr,
+		reg & ~XILINX_DMA_XR_IRQ_ALL_MASK);
+
+	stat = DMA_IN(&chan->regs->sr);
+	if (!(stat & XILINX_DMA_XR_IRQ_ALL_MASK))
+		return IRQ_NONE;
+
+	/* Ack the interrupts */
+	DMA_OUT(&chan->regs->sr, XILINX_DMA_XR_IRQ_ALL_MASK);
+
+	/* Check for only the interrupts which are enabled */
+	stat &= (reg & XILINX_DMA_XR_IRQ_ALL_MASK);
+
+	if (stat & XILINX_DMA_XR_IRQ_ERROR_MASK) {
+		dev_err(chan->dev,
+			"Channel %x has errors %x, cdr %x tdr %x\n",
+			(unsigned int)chan,
+			(unsigned int)DMA_IN(&chan->regs->sr),
+			(unsigned int)DMA_IN(&chan->regs->cdr),
+			(unsigned int)DMA_IN(&chan->regs->tdr));
+		chan->err = 1;
+	}
+
+	/*
+	 * Device takes too long to do the transfer when user requires
+	 * responsiveness
+	 */
+	if (stat & XILINX_DMA_XR_IRQ_DELAY_MASK)
+		dev_dbg(chan->dev, "Inter-packet latency too long\n");
+
+	if (stat & XILINX_DMA_XR_IRQ_IOC_MASK) {
+		update_cookie = 1;
+		to_transfer = 1;
+	}
+
+	if (update_cookie)
+		xilinx_dma_update_completed_cookie(chan);
+
+	if (to_transfer)
+		chan->start_transfer(chan);
+
+	tasklet_schedule(&chan->tasklet);
+	return IRQ_HANDLED;
+}
+
+static void dma_do_tasklet(unsigned long data)
+{
+	struct xilinx_dma_chan *chan = (struct xilinx_dma_chan *)data;
+
+	xilinx_chan_desc_cleanup(chan);
+}
+
+/* Append the descriptor list to the pending list */
+static void append_desc_queue(struct xilinx_dma_chan *chan,
+				struct xilinx_dma_desc_sw *desc)
+{
+	struct xilinx_dma_desc_sw *tail =
+				container_of(chan->pending_list.prev,
+					struct xilinx_dma_desc_sw, node);
+	struct xilinx_dma_desc_hw *hw;
+
+	if (list_empty(&chan->pending_list))
+		goto out_splice;
+
+	/*
+	 * Add the hardware descriptor to the chain of hardware descriptors
+	 * that already exists in memory.
+	 */
+	hw = &(tail->hw);
+	hw->next_desc = (u32)desc->async_tx.phys;
+
+	/*
+	 * Add the software descriptor and all children to the list
+	 * of pending transactions
+	 */
+out_splice:
+	list_splice_tail_init(&desc->tx_list, &chan->pending_list);
+}
+
+/*
+ * Assign cookie to each descriptor, and append the descriptors to the pending
+ * list
+ */
+static dma_cookie_t xilinx_dma_tx_submit(struct dma_async_tx_descriptor *tx)
+{
+	struct xilinx_dma_chan *chan = to_xilinx_chan(tx->chan);
+	struct xilinx_dma_desc_sw *desc = container_of(tx,
+				struct xilinx_dma_desc_sw, async_tx);
+	struct xilinx_dma_desc_sw *child;
+	unsigned long flags;
+	dma_cookie_t cookie = -EBUSY;
+
+	if (chan->err) {
+		/*
+		 * If reset fails, need to hard reset the system.
+		 * Channel is no longer functional
+		 */
+		if (!dma_init(chan))
+			chan->err = 0;
+		else
+			return cookie;
+	}
+
+	spin_lock_irqsave(&chan->lock, flags);
+
+	/*
+	 * Assign cookies to all of the software descriptors
+	 * that make up this transaction
+	 */
+	cookie = chan->cookie;
+	list_for_each_entry(child, &desc->tx_list, node) {
+		cookie++;
+		if (cookie < 0)
+			cookie = DMA_MIN_COOKIE;
+
+		child->async_tx.cookie = cookie;
+	}
+
+	chan->cookie = cookie;
+
+	/* Put this transaction onto the tail of the pending queue */
+	append_desc_queue(chan, desc);
+
+	spin_unlock_irqrestore(&chan->lock, flags);
+
+	return cookie;
+}
+
+static struct xilinx_dma_desc_sw *xilinx_dma_alloc_descriptor(
+					struct xilinx_dma_chan *chan)
+{
+	struct xilinx_dma_desc_sw *desc;
+	dma_addr_t pdesc;
+
+	desc = dma_pool_alloc(chan->desc_pool, GFP_ATOMIC, &pdesc);
+	if (!desc) {
+		dev_dbg(chan->dev, "out of memory for desc\n");
+		return NULL;
+	}
+
+	memset(desc, 0, sizeof(*desc));
+	INIT_LIST_HEAD(&desc->tx_list);
+	dma_async_tx_descriptor_init(&desc->async_tx, &chan->common);
+	desc->async_tx.tx_submit = xilinx_dma_tx_submit;
+	desc->async_tx.phys = pdesc;
+
+	return desc;
+}
+
+/**
+ * xilinx_dma_prep_slave_sg - prepare descriptors for a DMA_SLAVE transaction
+ * @chan: DMA channel
+ * @sgl: scatterlist to transfer to/from
+ * @sg_len: number of entries in @scatterlist
+ * @direction: DMA direction
+ * @flags: transfer ack flags
+ */
+static struct dma_async_tx_descriptor *xilinx_dma_prep_slave_sg(
+	struct dma_chan *dchan, struct scatterlist *sgl, unsigned int sg_len,
+	enum dma_transfer_direction direction, unsigned long flags,
+	void *context)
+{
+	struct xilinx_dma_chan *chan;
+	struct xilinx_dma_desc_sw *first = NULL, *prev = NULL, *new = NULL;
+	struct xilinx_dma_desc_hw *hw = NULL, *prev_hw = NULL;
+
+	size_t copy;
+
+	int i;
+	struct scatterlist *sg;
+	size_t sg_used;
+	dma_addr_t dma_src;
+
+#ifdef TEST_DMA_WITH_LOOPBACK
+	int total_len;
+#endif
+	if (!dchan)
+		return NULL;
+
+	chan = to_xilinx_chan(dchan);
+
+	if (chan->direction != direction)
+		return NULL;
+
+#ifdef TEST_DMA_WITH_LOOPBACK
+	total_len = 0;
+
+	for_each_sg(sgl, sg, sg_len, i) {
+		total_len += sg_dma_len(sg);
+	}
+#endif
+	/* Build transactions using information in the scatter gather list */
+	for_each_sg(sgl, sg, sg_len, i) {
+		sg_used = 0;
+
+		/* Loop until the entire scatterlist entry is used */
+		while (sg_used < sg_dma_len(sg)) {
+
+			/* Allocate the link descriptor from DMA pool */
+			new = xilinx_dma_alloc_descriptor(chan);
+			if (!new) {
+				dev_err(chan->dev,
+					"No free memory for link descriptor\n");
+				goto fail;
+			}
+
+			/*
+			 * Calculate the maximum number of bytes to transfer,
+			 * making sure it is less than the hw limit
+			 */
+			copy = min((size_t)(sg_dma_len(sg) - sg_used),
+				(size_t)chan->max_len);
+			hw = &(new->hw);
+
+			dma_src = sg_dma_address(sg) + sg_used;
+
+			hw->buf_addr = dma_src;
+
+			/* Fill in the descriptor */
+			hw->control = copy;
+
+			/*
+			 * If this is not the first descriptor, chain the
+			 * current descriptor after the previous descriptor
+			 *
+			 * For the first DMA_MEM_TO_DEV transfer, set SOP
+			 */
+			if (!first) {
+				first = new;
+				if (direction == DMA_MEM_TO_DEV) {
+					hw->control |= XILINX_DMA_BD_SOP;
+#ifdef TEST_DMA_WITH_LOOPBACK
+					hw->app_4 = total_len;
+#endif
+				}
+			} else {
+				prev_hw = &(prev->hw);
+				prev_hw->next_desc = new->async_tx.phys;
+			}
+
+			new->async_tx.cookie = 0;
+			async_tx_ack(&new->async_tx);
+
+			prev = new;
+			sg_used += copy;
+
+			/* Insert the link descriptor into the LD ring */
+			list_add_tail(&new->node, &first->tx_list);
+		}
+	}
+
+	/* Link the last BD with the first BD */
+	hw->next_desc = first->async_tx.phys;
+
+	if (direction == DMA_MEM_TO_DEV)
+		hw->control |= XILINX_DMA_BD_EOP;
+
+	/* All scatter gather list entries has length == 0 */
+	if (!first || !new)
+		return NULL;
+
+	new->async_tx.flags = flags;
+	new->async_tx.cookie = -EBUSY;
+
+	/* Set EOP to the last link descriptor of new list */
+	hw->control |= XILINX_DMA_BD_EOP;
+
+	return &first->async_tx;
+
+fail:
+	/*
+	 * If first was not set, then we failed to allocate the very first
+	 * descriptor, and we're done
+	 */
+	if (!first)
+		return NULL;
+
+	/*
+	 * First is set, so all of the descriptors we allocated have been added
+	 * to first->tx_list, INCLUDING "first" itself. Therefore we
+	 * must traverse the list backwards freeing each descriptor in turn
+	 */
+	xilinx_dma_free_desc_list_reverse(chan, &first->tx_list);
+
+	return NULL;
+}
+
+/* Run-time device configuration for Axi DMA */
+static int xilinx_dma_device_control(struct dma_chan *dchan,
+				enum dma_ctrl_cmd cmd, unsigned long arg)
+{
+	struct xilinx_dma_chan *chan;
+	unsigned long flags;
+
+	if (!dchan)
+		return -EINVAL;
+
+	chan = to_xilinx_chan(dchan);
+
+	if (cmd == DMA_TERMINATE_ALL) {
+		/* Halt the DMA engine */
+		dma_halt(chan);
+
+		spin_lock_irqsave(&chan->lock, flags);
+
+		/* Remove and free all of the descriptors in the lists */
+		xilinx_dma_free_desc_list(chan, &chan->pending_list);
+		xilinx_dma_free_desc_list(chan, &chan->active_list);
+
+		spin_unlock_irqrestore(&chan->lock, flags);
+		return 0;
+	} else if (cmd == DMA_SLAVE_CONFIG) {
+		/*
+		 * Configure interrupt coalescing and delay counter
+		 * Use value XILINX_DMA_NO_CHANGE to signal no change
+		 */
+		struct xilinx_dma_config *cfg = (struct xilinx_dma_config *)arg;
+		u32 reg = DMA_IN(&chan->regs->cr);
+
+		if (cfg->coalesc <= XILINX_DMA_COALESCE_MAX) {
+			reg &= ~XILINX_DMA_XR_COALESCE_MASK;
+			reg |= cfg->coalesc << XILINX_DMA_COALESCE_SHIFT;
+
+			chan->config.coalesc = cfg->coalesc;
+		}
+
+		if (cfg->delay <= XILINX_DMA_DELAY_MAX) {
+			reg &= ~XILINX_DMA_XR_DELAY_MASK;
+			reg |= cfg->delay << XILINX_DMA_DELAY_SHIFT;
+			chan->config.delay = cfg->delay;
+		}
+
+		DMA_OUT(&chan->regs->cr, reg);
+
+		return 0;
+	} else
+		return -ENXIO;
+}
+
+/*
+ * Logarithm function to compute alignment shift
+ *
+ * Only deals with value less than 4096.
+ */
+static int my_log(int value)
+{
+	int i = 0;
+	while ((1 << i) < value) {
+		i++;
+
+		if (i >= 12)
+			return 0;
+	}
+
+	return i;
+}
+
+static void xilinx_dma_chan_remove(struct xilinx_dma_chan *chan)
+{
+	irq_dispose_mapping(chan->irq);
+	list_del(&chan->common.device_node);
+	kfree(chan);
+}
+
+/*
+ * Probing channels
+ *
+ * . Get channel features from the device tree entry
+ * . Initialize special channel handling routines
+ */
+static int __devinit xilinx_dma_chan_probe(struct xilinx_dma_device *xdev,
+	struct device_node *node, u32 feature)
+{
+	struct xilinx_dma_chan *chan;
+	int err;
+	int *value;
+	u32 width = 0, device_id = 0;
+
+	/* alloc channel */
+	chan = kzalloc(sizeof(*chan), GFP_KERNEL);
+	if (!chan) {
+		dev_err(xdev->dev, "no free memory for DMA channels!\n");
+		err = -ENOMEM;
+		goto out_return;
+	}
+
+	chan->feature = feature;
+	chan->is_lite = 0;
+	chan->has_DRE = 0;
+	chan->has_SG = 0;
+	chan->max_len = XILINX_DMA_MAX_TRANS_LEN;
+
+	value = (int *)of_get_property(node, "xlnx,include-dre",
+			NULL);
+	if (value)
+		chan->has_DRE = be32_to_cpup(value);
+
+	value = (int *)of_get_property(node,
+			"xlnx,datawidth",
+			NULL);
+	if (value) {
+		width = be32_to_cpup(value) >> 3; /* convert bits to bytes */
+
+		/* If data width is greater than 8 bytes, DRE is not in hw */
+		if (width > 8)
+			chan->has_DRE = 0;
+
+		chan->feature |= width - 1;
+	}
+
+	value = (int *)of_get_property(node, "xlnx,device-id", NULL);
+	if (value)
+		device_id = be32_to_cpup(value);
+
+	if (feature & XILINX_DMA_IP_DMA) {
+		chan->has_SG = (xdev->feature & XILINX_DMA_FTR_HAS_SG) >>
+					XILINX_DMA_FTR_HAS_SG_SHIFT;
+
+		chan->start_transfer = xilinx_dma_start_transfer;
+
+		if (of_device_is_compatible(node,
+			 "xlnx,axi-dma-mm2s-channel"))
+			chan->direction = DMA_MEM_TO_DEV;
+
+		if (of_device_is_compatible(node,
+				"xlnx,axi-dma-s2mm-channel"))
+			chan->direction = DMA_DEV_TO_MEM;
+
+	}
+
+	chan->regs = (struct xdma_regs *)xdev->regs;
+	chan->id = 0;
+
+	if (chan->direction == DMA_DEV_TO_MEM) {
+		chan->regs = (struct xdma_regs *)((u32)xdev->regs +
+					XILINX_DMA_RX_CHANNEL_OFFSET);
+		chan->id = 1;
+	}
+
+	/*
+	 * Used by dmatest channel matching in slave transfers
+	 * Can change it to be a structure to have more matching information
+	 */
+	chan->private = (chan->direction & 0xFF) |
+		(chan->feature & XILINX_DMA_IP_MASK) |
+		(device_id << XILINX_DMA_DEVICE_ID_SHIFT);
+	chan->common.private = (void *)&(chan->private);
+
+	if (!chan->has_DRE)
+		xdev->common.copy_align = my_log(width);
+
+	chan->dev = xdev->dev;
+	xdev->chan[chan->id] = chan;
+
+	tasklet_init(&chan->tasklet, dma_do_tasklet, (unsigned long)chan);
+
+	/* Initialize the channel */
+	if (dma_init(chan)) {
+		dev_err(xdev->dev, "Reset channel failed\n");
+		goto out_free_chan;
+	}
+
+
+	spin_lock_init(&chan->lock);
+	INIT_LIST_HEAD(&chan->pending_list);
+	INIT_LIST_HEAD(&chan->active_list);
+
+	chan->common.device = &xdev->common;
+
+	/* find the IRQ line, if it exists in the device tree */
+	chan->irq = irq_of_parse_and_map(node, 0);
+	err = request_irq(chan->irq, dma_intr_handler, IRQF_SHARED,
+				"xilinx-dma-controller", chan);
+	if (err) {
+		dev_err(xdev->dev, "unable to request IRQ\n");
+		goto out_free_irq;
+	}
+
+	/* Add the channel to DMA device channel list */
+	list_add_tail(&chan->common.device_node, &xdev->common.channels);
+	xdev->common.chancnt++;
+
+	return 0;
+
+out_free_irq:
+	irq_dispose_mapping(chan->irq);
+out_free_chan:
+	kfree(chan);
+out_return:
+	return err;
+}
+
+static int __devinit xilinx_dma_of_probe(struct platform_device *op)
+{
+	struct xilinx_dma_device *xdev;
+	struct device_node *child, *node;
+	int err;
+	int *value;
+
+	dev_info(&op->dev, "Probing xilinx axi dma engine\n");
+
+	xdev = kzalloc(sizeof(struct xilinx_dma_device), GFP_KERNEL);
+	if (!xdev) {
+		dev_err(&op->dev, "Not enough memory for device\n");
+		err = -ENOMEM;
+		goto out_return;
+	}
+
+	xdev->dev = &(op->dev);
+	INIT_LIST_HEAD(&xdev->common.channels);
+
+	node = op->dev.of_node;
+	xdev->feature = 0;
+
+	/* iomap registers */
+	xdev->regs = of_iomap(node, 0);
+	if (!xdev->regs) {
+		dev_err(&op->dev, "unable to iomap registers\n");
+		err = -ENOMEM;
+		goto out_free_xdev;
+	}
+
+	/*
+	 * Axi DMA only do slave transfers
+	 */
+	if (of_device_is_compatible(node, "xlnx,axi-dma")) {
+
+		xdev->feature |= XILINX_DMA_IP_DMA;
+		value = (int *)of_get_property(node,
+				"xlnx,sg-include-stscntrl-strm",
+				NULL);
+		if (value) {
+			if (be32_to_cpup(value) == 1) {
+				xdev->feature |= (XILINX_DMA_FTR_STSCNTRL_STRM |
+							XILINX_DMA_FTR_HAS_SG);
+			}
+		}
+
+		dma_cap_set(DMA_SLAVE, xdev->common.cap_mask);
+		dma_cap_set(DMA_PRIVATE, xdev->common.cap_mask);
+		xdev->common.device_prep_slave_sg = xilinx_dma_prep_slave_sg;
+		xdev->common.device_control = xilinx_dma_device_control;
+		xdev->common.device_issue_pending = xilinx_dma_issue_pending;
+	}
+
+	xdev->common.device_alloc_chan_resources =
+				xilinx_dma_alloc_chan_resources;
+	xdev->common.device_free_chan_resources =
+				xilinx_dma_free_chan_resources;
+	xdev->common.device_tx_status = xilinx_tx_status;
+	xdev->common.dev = &op->dev;
+
+	dev_set_drvdata(&op->dev, xdev);
+
+	for_each_child_of_node(node, child) {
+		xilinx_dma_chan_probe(xdev, child, xdev->feature);
+	}
+
+	dma_async_device_register(&xdev->common);
+
+	return 0;
+
+out_free_xdev:
+	kfree(xdev);
+
+out_return:
+	return err;
+}
+
+static int __devexit xilinx_dma_of_remove(struct platform_device *op)
+{
+	struct xilinx_dma_device *xdev;
+	int i;
+
+	xdev = dev_get_drvdata(&op->dev);
+	dma_async_device_unregister(&xdev->common);
+
+	for (i = 0; i < XILINX_DMA_MAX_CHANS_PER_DEVICE; i++) {
+		if (xdev->chan[i])
+			xilinx_dma_chan_remove(xdev->chan[i]);
+	}
+
+	iounmap(xdev->regs);
+	dev_set_drvdata(&op->dev, NULL);
+	kfree(xdev);
+
+	return 0;
+}
+
+static const struct of_device_id xilinx_dma_of_ids[] = {
+	{ .compatible = "xlnx,axi-dma",},
+	{}
+};
+
+static struct platform_driver xilinx_dma_of_driver = {
+	.driver = {
+		.name = "xilinx-dma",
+		.owner = THIS_MODULE,
+		.of_match_table = xilinx_dma_of_ids,
+	},
+	.probe = xilinx_dma_of_probe,
+	.remove = __devexit_p(xilinx_dma_of_remove),
+};
+
+module_platform_driver(xilinx_dma_of_driver);
+
+MODULE_AUTHOR("Xilinx, Inc.");
+MODULE_DESCRIPTION("Xilinx DMA driver");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/dma/xilinx/xilinx_axivdma.c b/drivers/dma/xilinx/xilinx_axivdma.c
new file mode 100644
index 0000000..e39676c
--- /dev/null
+++ b/drivers/dma/xilinx/xilinx_axivdma.c
@@ -0,0 +1,1321 @@
+/*
+ * Xilinx Video DMA Engine support
+ *
+ * Copyright (C) 2010 Xilinx, Inc. All rights reserved.
+ *
+ * Based on the Freescale DMA driver.
+ *
+ * Description:
+ *  . Axi VDMA engine, it does transfers between memory and video devices.
+ *    It can be configured to have one channel or two channels. If configured
+ *    as two channels, one is to transmit to the video device and another is
+ *    to receive from the video device.
+ *
+ * This is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/interrupt.h>
+#include <linux/dmapool.h>
+#include <linux/io.h>
+#include <linux/of.h>
+#include <linux/of_platform.h>
+#include <linux/platform_device.h>
+#include <linux/of_address.h>
+#include <linux/of_irq.h>
+#include <linux/amba/xilinx_dma.h>
+
+
+/* Hw specific definitions */
+#define XILINX_VDMA_MAX_CHANS_PER_DEVICE	0x2
+#define XILINX_VDMA_MAX_TRANS_LEN		0x7FFFFF
+
+/* General register bits definitions */
+#define XILINX_VDMA_CR_RESET_MASK	0x00000004
+						/* Reset DMA engine */
+#define XILINX_VDMA_CR_RUNSTOP_MASK	0x00000001
+						/* Start/stop DMA engine */
+
+#define XILINX_VDMA_SR_HALTED_MASK	0x00000001
+						/* DMA channel halted */
+#define XILINX_VDMA_SR_IDLE_MASK	0x00000002
+						/* DMA channel idle */
+
+#define XILINX_VDMA_SR_ERR_INTERNAL_MASK	0x00000010
+						/* Datamover internal err */
+#define XILINX_VDMA_SR_ERR_SLAVE_MASK		0x00000020
+						/* Datamover slave err */
+#define XILINX_VDMA_SR_ERR_DECODE_MASK		0x00000040
+						/* Datamover decode err */
+#define XILINX_VDMA_SR_ERR_SG_INT_MASK		0x00000100
+						/* SG internal err */
+#define XILINX_VDMA_SR_ERR_SG_SLV_MASK		0x00000200
+						/* SG slave err */
+#define XILINX_VDMA_SR_ERR_SG_DEC_MASK		0x00000400
+						/* SG decode err */
+#define XILINX_VDMA_SR_ERR_ALL_MASK		0x00000770
+						/* All errors */
+
+#define XILINX_VDMA_XR_IRQ_IOC_MASK	0x00001000
+						/* Completion interrupt */
+#define XILINX_VDMA_XR_IRQ_DELAY_MASK	0x00002000
+						/* Delay interrupt */
+#define XILINX_VDMA_XR_IRQ_ERROR_MASK	0x00004000
+						/* Error interrupt */
+#define XILINX_VDMA_XR_IRQ_ALL_MASK	0x00007000
+						/* All interrupts */
+
+#define XILINX_VDMA_XR_DELAY_MASK	0xFF000000
+						/* Delay timeout counter */
+#define XILINX_VDMA_XR_COALESCE_MASK	0x00FF0000
+						/* Coalesce counter */
+
+#define XILINX_VDMA_IRQ_SHIFT		12
+#define XILINX_VDMA_DELAY_SHIFT		24
+#define XILINX_VDMA_COALESCE_SHIFT	16
+
+#define XILINX_VDMA_DELAY_MAX		0xFF
+					/* Maximum delay counter value */
+#define XILINX_VDMA_COALESCE_MAX	0xFF
+					/* Maximum coalescing counter value */
+
+#define XILINX_VDMA_RX_CHANNEL_OFFSET	0x30
+
+#define XILINX_VDMA_CIRC_EN	0x00000002	/* Circular mode */
+#define XILINX_VDMA_SYNC_EN	0x00000008	/* Sync enable mode */
+#define XILINX_VDMA_FRMCNT_EN	0x00000010	/* Frm Cnt enable mode */
+#define XILINX_VDMA_MSTR_MASK	0x00000F00	/* Master in control */
+
+#define XILINX_VDMA_EXTFSYNC_SHIFT	6
+#define XILINX_VDMA_MSTR_SHIFT		8
+#define XILINX_VDMA_WR_REF_SHIFT	8
+
+#define XILINX_VDMA_FRMDLY_SHIFT	24
+
+#define XILINX_VDMA_DIRECT_REG_OFFSET		0x50
+#define XILINX_VDMA_CHAN_DIRECT_REG_SIZE	0x50
+
+#define XILINX_VDMA_PARK_REG_OFFSET		0x28
+
+#define XILINX_VDMA_SR_ERR_FSIZE_LESS_MASK	0x00000080
+						/* FSize Less Mismatch err */
+#define XILINX_VDMA_SR_ERR_LSIZE_LESS_MASK	0x00000100
+						/* LSize Less Mismatch err */
+#define XILINX_VDMA_SR_ERR_FSIZE_MORE_MASK	0x00000800
+						/* FSize more err */
+
+/*
+ * Recoverable errors are DMA Internal error, FSize Less, LSize Less
+ * and FSize More mismatch errors.  These are only recoverable only
+ * when C_FLUSH_ON_FSYNC is enabled in the hardware system.
+ */
+#define XILINX_VDMA_SR_ERR_RECOVER_MASK	0x00000990
+						/* Recoverable errs */
+
+/* Axi VDMA Flush on Fsync bits */
+#define XILINX_VDMA_FLUSH_S2MM	3
+#define XILINX_VDMA_FLUSH_MM2S	2
+#define XILINX_VDMA_FLUSH_BOTH	1
+
+/* Feature encodings */
+#define XILINX_VDMA_FTR_HAS_SG		0x00000100
+						/* Has SG */
+#define XILINX_VDMA_FTR_HAS_SG_SHIFT	8
+						/* Has SG shift */
+#define XILINX_VDMA_FTR_FLUSH_MASK	0x00000600
+						/* Flush-on-FSync Mask */
+#define XILINX_VDMA_FTR_FLUSH_SHIFT	9
+						/* Flush-on-FSync shift */
+
+/* Delay loop counter to prevent hardware failure */
+#define XILINX_VDMA_RESET_LOOP	1000000
+#define XILINX_VDMA_HALT_LOOP	1000000
+
+/* Device Id in the private structure */
+#define XILINX_VDMA_DEVICE_ID_SHIFT	28
+
+/* IO accessors */
+#define VDMA_OUT(addr, val)	(iowrite32(val, addr))
+#define VDMA_IN(addr)		(ioread32(addr))
+
+/* Hardware descriptor */
+struct xilinx_vdma_desc_hw {
+	u32 next_desc;	/* 0x00 */
+	u32 pad1;	/* 0x04 */
+	u32 buf_addr;	/* 0x08 */
+	u32 pad2;	/* 0x0C */
+	u32 vsize;	/* 0x10 */
+	u32 hsize;	/* 0x14 */
+	u32 stride;	/* 0x18 */
+} __aligned(64);
+
+struct xilinx_vdma_desc_sw {
+	struct xilinx_vdma_desc_hw hw;
+	struct list_head node;
+	struct list_head tx_list;
+	struct dma_async_tx_descriptor async_tx;
+} __aligned(64);
+
+struct xvdma_regs {
+	u32 cr;		/* 0x00 Control Register */
+	u32 sr;		/* 0x04 Status Register */
+	u32 cdr;	/* 0x08 Current Descriptor Register */
+	u32 pad1;
+	u32 tdr;	/* 0x10 Tail Descriptor Register */
+	u32 pad2;
+};
+
+struct vdma_addr_regs {
+	u32 vsize;		/* 0x0 Vertical size */
+	u32 hsize;		/* 0x4 Horizontal size */
+	u32 frmdly_stride;	/* 0x8 Frame delay and stride */
+	u32 buf_addr[16];	/* 0xC - 0x48 Src addresses */
+};
+
+/* Per DMA specific operations should be embedded in the channel structure */
+struct xilinx_vdma_chan {
+	struct xvdma_regs __iomem *regs;	/* Control status registers */
+	struct vdma_addr_regs *addr_regs;	/* Direct address registers */
+	dma_cookie_t completed_cookie;		/* Maximum cookie completed */
+	dma_cookie_t cookie;			/* The current cookie */
+	spinlock_t lock;			/* Descriptor operation lock */
+	bool sg_waiting;			/* SG transfer waiting */
+	struct list_head active_list;		/* Active descriptors */
+	struct list_head pending_list;		/* Descriptors waiting */
+	struct dma_chan common;			/* DMA common channel */
+	struct dma_pool *desc_pool;		/* Descriptors pool */
+	struct device *dev;			/* The dma device */
+	int irq;				/* Channel IRQ */
+	int id;					/* Channel ID */
+	enum dma_transfer_direction direction;	/* Transfer direction */
+	int max_len;				/* Max data len per transfer */
+	int is_lite;				/* Whether is light build */
+	int num_frms;				/* Number of frames */
+	int has_SG;				/* Support scatter transfers */
+	int has_DRE;				/* For unaligned transfers */
+	int genlock;				/* Support genlock mode */
+	int err;				/* Channel has errors */
+	struct tasklet_struct tasklet;		/* Cleanup work after irq */
+	u32 feature;				/* IP feature */
+	u32 private;				/* Match info for
+							channel request */
+	void (*start_transfer)(struct xilinx_vdma_chan *chan);
+	struct xilinx_vdma_config config;	/* Device configuration info */
+	u32 flush_fsync;			/* Flush on Fsync */
+};
+
+struct xilinx_vdma_device {
+	void __iomem *regs;
+	struct device *dev;
+	struct dma_device common;
+	struct xilinx_vdma_chan *chan[XILINX_VDMA_MAX_CHANS_PER_DEVICE];
+	u32 feature;
+	int irq;
+};
+
+#define to_xilinx_chan(chan) \
+			container_of(chan, struct xilinx_vdma_chan, common)
+
+/* Required functions */
+
+static int xilinx_vdma_alloc_chan_resources(struct dma_chan *dchan)
+{
+	struct xilinx_vdma_chan *chan = to_xilinx_chan(dchan);
+
+	/* Has this channel already been allocated? */
+	if (chan->desc_pool)
+		return 1;
+
+	/*
+	 * We need the descriptor to be aligned to 64bytes
+	 * for meeting Xilinx VDMA specification requirement.
+	 */
+	chan->desc_pool = dma_pool_create("xilinx_vdma_desc_pool",
+				chan->dev,
+				sizeof(struct xilinx_vdma_desc_sw),
+				__alignof__(struct xilinx_vdma_desc_sw), 0);
+	if (!chan->desc_pool) {
+		dev_err(chan->dev,
+			"unable to allocate channel %d descriptor pool\n",
+			chan->id);
+		return -ENOMEM;
+	}
+
+	chan->completed_cookie = 1;
+	chan->cookie = 1;
+
+	/* there is at least one descriptor free to be allocated */
+	return 1;
+}
+
+static void xilinx_vdma_free_desc_list(struct xilinx_vdma_chan *chan,
+					struct list_head *list)
+{
+	struct xilinx_vdma_desc_sw *desc, *_desc;
+
+	list_for_each_entry_safe(desc, _desc, list, node) {
+		list_del(&desc->node);
+		dma_pool_free(chan->desc_pool, desc, desc->async_tx.phys);
+	}
+}
+
+static void xilinx_vdma_free_desc_list_reverse(struct xilinx_vdma_chan *chan,
+						struct list_head *list)
+{
+	struct xilinx_vdma_desc_sw *desc, *_desc;
+
+	list_for_each_entry_safe_reverse(desc, _desc, list, node) {
+		list_del(&desc->node);
+		dma_pool_free(chan->desc_pool, desc, desc->async_tx.phys);
+	}
+}
+
+static void xilinx_vdma_free_chan_resources(struct dma_chan *dchan)
+{
+	struct xilinx_vdma_chan *chan = to_xilinx_chan(dchan);
+	unsigned long flags;
+
+	dev_dbg(chan->dev, "Free all channel resources.\n");
+	spin_lock_irqsave(&chan->lock, flags);
+	xilinx_vdma_free_desc_list(chan, &chan->active_list);
+	xilinx_vdma_free_desc_list(chan, &chan->pending_list);
+	spin_unlock_irqrestore(&chan->lock, flags);
+
+	dma_pool_destroy(chan->desc_pool);
+	chan->desc_pool = NULL;
+}
+
+static enum dma_status xilinx_vdma_desc_status(struct xilinx_vdma_chan *chan,
+					struct xilinx_vdma_desc_sw *desc)
+{
+	return dma_async_is_complete(desc->async_tx.cookie,
+					chan->completed_cookie,
+					chan->cookie);
+}
+
+static void xilinx_chan_desc_cleanup(struct xilinx_vdma_chan *chan)
+{
+	struct xilinx_vdma_desc_sw *desc, *_desc;
+	unsigned long flags;
+
+	spin_lock_irqsave(&chan->lock, flags);
+
+	list_for_each_entry_safe(desc, _desc, &chan->active_list, node) {
+		dma_async_tx_callback callback;
+		void *callback_param;
+
+		if (xilinx_vdma_desc_status(chan, desc) == DMA_IN_PROGRESS)
+			break;
+
+		/* Remove from the list of running transactions */
+		list_del(&desc->node);
+
+		/* Run the link descriptor callback function */
+		callback = desc->async_tx.callback;
+		callback_param = desc->async_tx.callback_param;
+		if (callback) {
+			spin_unlock_irqrestore(&chan->lock, flags);
+			callback(callback_param);
+			spin_lock_irqsave(&chan->lock, flags);
+		}
+
+		/* Run any dependencies, then free the descriptor */
+		dma_run_dependencies(&desc->async_tx);
+		dma_pool_free(chan->desc_pool, desc, desc->async_tx.phys);
+	}
+
+	spin_unlock_irqrestore(&chan->lock, flags);
+}
+
+static enum dma_status xilinx_tx_status(struct dma_chan *dchan,
+					dma_cookie_t cookie,
+					struct dma_tx_state *txstate)
+{
+	struct xilinx_vdma_chan *chan = to_xilinx_chan(dchan);
+	dma_cookie_t last_used;
+	dma_cookie_t last_complete;
+
+	xilinx_chan_desc_cleanup(chan);
+
+	last_used = dchan->cookie;
+	last_complete = chan->completed_cookie;
+
+	dma_set_tx_state(txstate, last_complete, last_used, 0);
+
+	return dma_async_is_complete(cookie, last_complete, last_used);
+}
+
+static int dma_is_running(struct xilinx_vdma_chan *chan)
+{
+	return !(VDMA_IN(&chan->regs->sr) & XILINX_VDMA_SR_HALTED_MASK) &&
+		(VDMA_IN(&chan->regs->cr) & XILINX_VDMA_CR_RUNSTOP_MASK);
+}
+
+static int dma_is_idle(struct xilinx_vdma_chan *chan)
+{
+	return VDMA_IN(&chan->regs->sr) & XILINX_VDMA_SR_IDLE_MASK;
+}
+
+#define XILINX_VDMA_DRIVER_DEBUG	0
+
+#if (XILINX_VDMA_DRIVER_DEBUG == 1)
+static void desc_dump(struct xilinx_vdma_desc_hw *hw)
+{
+	pr_info("hw desc %x:\n", (unsigned int)hw);
+	pr_info("\tnext_desc %x\n", hw->next_desc);
+	pr_info("\tbuf_addr %x\n", hw->buf_addr);
+	pr_info("\tvsize %x\n", hw->vsize);
+	pr_info("\thsize %x\n", hw->hsize);
+	pr_info("\tstride %x\n", hw->stride);
+	pr_info("\tstatus %x\n", hw->status);
+
+}
+#endif
+
+/* Stop the hardware, the ongoing transfer will be finished */
+static void vdma_halt(struct xilinx_vdma_chan *chan)
+{
+	int loop = XILINX_VDMA_HALT_LOOP;
+
+	VDMA_OUT(&chan->regs->cr,
+		VDMA_IN(&chan->regs->cr) & ~XILINX_VDMA_CR_RUNSTOP_MASK);
+
+	/* Wait for the hardware to halt */
+	while (loop) {
+		if (!(VDMA_IN(&chan->regs->cr) & XILINX_VDMA_CR_RUNSTOP_MASK))
+			break;
+
+		loop -= 1;
+	}
+
+	if (!loop) {
+		pr_debug("Cannot stop channel %x: %x\n",
+			(unsigned int)chan,
+			(unsigned int)VDMA_IN(&chan->regs->cr));
+		chan->err = 1;
+	}
+
+	return;
+}
+
+/* Start the hardware. Transfers are not started yet */
+static void vdma_start(struct xilinx_vdma_chan *chan)
+{
+	int loop = XILINX_VDMA_HALT_LOOP;
+
+	VDMA_OUT(&chan->regs->cr,
+		VDMA_IN(&chan->regs->cr) | XILINX_VDMA_CR_RUNSTOP_MASK);
+
+	/* Wait for the hardware to start */
+	while (loop) {
+		if (VDMA_IN(&chan->regs->cr) & XILINX_VDMA_CR_RUNSTOP_MASK)
+			break;
+
+		loop -= 1;
+	}
+
+	if (!loop) {
+		pr_debug("Cannot start channel %x: %x\n",
+			(unsigned int)chan,
+			(unsigned int)VDMA_IN(&chan->regs->cr));
+
+		chan->err = 1;
+	}
+
+	return;
+}
+
+static void xilinx_vdma_start_transfer(struct xilinx_vdma_chan *chan)
+{
+	unsigned long flags;
+	struct xilinx_vdma_desc_sw *desch, *desct = NULL;
+	struct xilinx_vdma_config *config;
+	u32 reg;
+	u8 *chan_base;
+
+	if (chan->err)
+		return;
+
+	spin_lock_irqsave(&chan->lock, flags);
+
+	if (list_empty(&chan->pending_list))
+		goto out_unlock;
+
+	/* If it is SG mode and hardware is busy, cannot submit */
+	if (chan->has_SG && dma_is_running(chan) && !dma_is_idle(chan)) {
+		dev_dbg(chan->dev, "DMA controller still busy\n");
+		goto out_unlock;
+	}
+
+	/*
+	 * If hardware is idle, then all descriptors on the running lists are
+	 * done, start new transfers
+	 */
+	if (chan->err)
+		goto out_unlock;
+
+	if (chan->has_SG) {
+		desch = list_first_entry(&chan->pending_list,
+				struct xilinx_vdma_desc_sw, node);
+
+		desct = container_of(chan->pending_list.prev,
+				struct xilinx_vdma_desc_sw, node);
+
+		VDMA_OUT(&chan->regs->cdr, desch->async_tx.phys);
+	}
+
+	/* Configure the hardware using info in the config structure */
+	config = &(chan->config);
+	reg = VDMA_IN(&chan->regs->cr);
+
+	if (config->frm_cnt_en)
+		reg |= XILINX_VDMA_FRMCNT_EN;
+	else
+		reg &= ~XILINX_VDMA_FRMCNT_EN;
+
+	/*
+	 * With SG, start with circular mode, so that BDs can be fetched.
+	 * In direct register mode, if not parking, enable circular mode
+	 */
+	if ((chan->has_SG) || (!config->park))
+		reg |= XILINX_VDMA_CIRC_EN;
+
+	if (config->park)
+		reg &= ~XILINX_VDMA_CIRC_EN;
+
+	VDMA_OUT(&chan->regs->cr, reg);
+
+	if (config->park && (config->park_frm >= 0)
+			&& (config->park_frm < chan->num_frms)) {
+		if (config->direction == DMA_MEM_TO_DEV) {
+			chan_base = (char *)chan->regs;
+			VDMA_OUT((chan_base + XILINX_VDMA_PARK_REG_OFFSET),
+					config->park_frm);
+		} else {
+			chan_base = ((char *)chan->regs -
+					XILINX_VDMA_RX_CHANNEL_OFFSET);
+			VDMA_OUT((chan_base + XILINX_VDMA_PARK_REG_OFFSET),
+				config->park_frm << XILINX_VDMA_WR_REF_SHIFT);
+		}
+	}
+
+	/* Start the hardware */
+	vdma_start(chan);
+
+	if (chan->err)
+		goto out_unlock;
+	list_splice_tail_init(&chan->pending_list, &chan->active_list);
+
+	/*
+	 * Enable interrupts
+	 * park/genlock testing does not use interrupts
+	 */
+	if (!chan->config.disable_intr) {
+		VDMA_OUT(&chan->regs->cr,
+			VDMA_IN(&chan->regs->cr) |
+				XILINX_VDMA_XR_IRQ_ALL_MASK);
+	} else {
+		VDMA_OUT(&chan->regs->cr,
+			(VDMA_IN(&chan->regs->cr) |
+				XILINX_VDMA_XR_IRQ_ALL_MASK) &
+			~((chan->config.disable_intr <<
+				XILINX_VDMA_IRQ_SHIFT)));
+	}
+
+	/* Start the transfer */
+	if (chan->has_SG)
+		VDMA_OUT(&chan->regs->tdr, desct->async_tx.phys);
+	else
+		VDMA_OUT(&chan->addr_regs->vsize, config->vsize);
+
+out_unlock:
+	spin_unlock_irqrestore(&chan->lock, flags);
+}
+
+static void xilinx_vdma_issue_pending(struct dma_chan *dchan)
+{
+	struct xilinx_vdma_chan *chan = to_xilinx_chan(dchan);
+
+	xilinx_vdma_start_transfer(chan);
+}
+
+/**
+ * xilinx_vdma_update_completed_cookie - Update the completed cookie.
+ * @chan : xilinx DMA channel
+ *
+ * CONTEXT: hardirq
+ */
+static void xilinx_vdma_update_completed_cookie(struct xilinx_vdma_chan *chan)
+{
+	struct xilinx_vdma_desc_sw *desc = NULL;
+	unsigned long flags;
+	dma_cookie_t cookie = -EBUSY;
+	int done = 0;
+
+	spin_lock_irqsave(&chan->lock, flags);
+
+	if (list_empty(&chan->active_list)) {
+		dev_dbg(chan->dev, "no running descriptors\n");
+		goto out_unlock;
+	}
+
+	/* Get the last completed descriptor, update the cookie to that */
+	list_for_each_entry(desc, &chan->active_list, node) {
+		/* In non-SG mode, all active entries are done */
+		done = 1;
+		cookie = desc->async_tx.cookie;
+	}
+
+	if (done)
+		chan->completed_cookie = cookie;
+
+out_unlock:
+	spin_unlock_irqrestore(&chan->lock, flags);
+}
+
+/* Reset hardware */
+static int vdma_init(struct xilinx_vdma_chan *chan)
+{
+	int loop = XILINX_VDMA_RESET_LOOP;
+	u32 tmp;
+
+	VDMA_OUT(&chan->regs->cr,
+		VDMA_IN(&chan->regs->cr) | XILINX_VDMA_CR_RESET_MASK);
+
+	tmp = VDMA_IN(&chan->regs->cr) & XILINX_VDMA_CR_RESET_MASK;
+
+	/* Wait for the hardware to finish reset */
+	while (loop && tmp) {
+		tmp = VDMA_IN(&chan->regs->cr) & XILINX_VDMA_CR_RESET_MASK;
+		loop -= 1;
+	}
+
+	if (!loop) {
+		dev_err(chan->dev, "reset timeout, cr %x, sr %x\n",
+			VDMA_IN(&chan->regs->cr), VDMA_IN(&chan->regs->sr));
+		return 1;
+	}
+
+	return 0;
+}
+
+
+static irqreturn_t vdma_intr_handler(int irq, void *data)
+{
+	struct xilinx_vdma_chan *chan = data;
+	int update_cookie = 0;
+	int to_transfer = 0;
+	u32 stat, reg;
+
+	reg = VDMA_IN(&chan->regs->cr);
+
+	/* Disable intr */
+	VDMA_OUT(&chan->regs->cr,
+		reg & ~XILINX_VDMA_XR_IRQ_ALL_MASK);
+
+	stat = VDMA_IN(&chan->regs->sr);
+	if (!(stat & XILINX_VDMA_XR_IRQ_ALL_MASK))
+		return IRQ_NONE;
+
+	/* Ack the interrupts */
+	VDMA_OUT(&chan->regs->sr, XILINX_VDMA_XR_IRQ_ALL_MASK);
+
+	/* Check for only the interrupts which are enabled */
+	stat &= (reg & XILINX_VDMA_XR_IRQ_ALL_MASK);
+
+	if (stat & XILINX_VDMA_XR_IRQ_ERROR_MASK) {
+		if (chan->flush_fsync) {
+			/*
+			 * VDMA Recoverable Errors, only when
+			 * C_FLUSH_ON_FSYNC is enabled
+			 */
+			u32 error = VDMA_IN(&chan->regs->sr) &
+				XILINX_VDMA_SR_ERR_RECOVER_MASK;
+			if (error)
+				VDMA_OUT(&chan->regs->sr, error);
+			else
+				chan->err = 1;
+		} else {
+			dev_err(chan->dev,
+				"Channel %x has errors %x, cdr %x tdr %x\n",
+				(unsigned int)chan,
+				(unsigned int)VDMA_IN(&chan->regs->sr),
+				(unsigned int)VDMA_IN(&chan->regs->cdr),
+				(unsigned int)VDMA_IN(&chan->regs->tdr));
+			chan->err = 1;
+		}
+	}
+
+	/*
+	 * Device takes too long to do the transfer when user requires
+	 * responsiveness
+	 */
+	if (stat & XILINX_VDMA_XR_IRQ_DELAY_MASK)
+		dev_dbg(chan->dev, "Inter-packet latency too long\n");
+
+	if (stat & XILINX_VDMA_XR_IRQ_IOC_MASK) {
+		update_cookie = 1;
+		to_transfer = 1;
+	}
+
+	if (update_cookie)
+		xilinx_vdma_update_completed_cookie(chan);
+
+	if (to_transfer)
+		chan->start_transfer(chan);
+
+	tasklet_schedule(&chan->tasklet);
+	return IRQ_HANDLED;
+}
+
+static void dma_do_tasklet(unsigned long data)
+{
+	struct xilinx_vdma_chan *chan = (struct xilinx_vdma_chan *)data;
+
+	xilinx_chan_desc_cleanup(chan);
+}
+
+/* Append the descriptor list to the pending list */
+static void append_desc_queue(struct xilinx_vdma_chan *chan,
+			struct xilinx_vdma_desc_sw *desc)
+{
+	struct xilinx_vdma_desc_sw *tail = container_of(chan->pending_list.prev,
+					struct xilinx_vdma_desc_sw, node);
+	struct xilinx_vdma_desc_hw *hw;
+
+	if (list_empty(&chan->pending_list))
+		goto out_splice;
+
+	/*
+	 * Add the hardware descriptor to the chain of hardware descriptors
+	 * that already exists in memory.
+	 */
+	hw = &(tail->hw);
+	hw->next_desc = (u32)desc->async_tx.phys;
+
+	/*
+	 * Add the software descriptor and all children to the list
+	 * of pending transactions
+	 */
+out_splice:
+	list_splice_tail_init(&desc->tx_list, &chan->pending_list);
+}
+
+/*
+ * Assign cookie to each descriptor, and append the descriptors to the pending
+ * list
+ */
+static dma_cookie_t xilinx_vdma_tx_submit(struct dma_async_tx_descriptor *tx)
+{
+	struct xilinx_vdma_chan *chan = to_xilinx_chan(tx->chan);
+	struct xilinx_vdma_desc_sw *desc = container_of(tx,
+				struct xilinx_vdma_desc_sw, async_tx);
+	struct xilinx_vdma_desc_sw *child;
+	unsigned long flags;
+	dma_cookie_t cookie = -EBUSY;
+
+	if (chan->err) {
+		/*
+		 * If reset fails, need to hard reset the system.
+		 * Channel is no longer functional
+		 */
+		if (!vdma_init(chan))
+			chan->err = 0;
+		else
+			return cookie;
+	}
+
+	spin_lock_irqsave(&chan->lock, flags);
+
+	/*
+	 * assign cookies to all of the software descriptors
+	 * that make up this transaction
+	 */
+	cookie = chan->cookie;
+	list_for_each_entry(child, &desc->tx_list, node) {
+		cookie++;
+		if (cookie < 0)
+			cookie = DMA_MIN_COOKIE;
+
+		child->async_tx.cookie = cookie;
+	}
+
+	chan->cookie = cookie;
+
+	/* Put this transaction onto the tail of the pending queue */
+	append_desc_queue(chan, desc);
+
+	spin_unlock_irqrestore(&chan->lock, flags);
+
+	return cookie;
+}
+
+static struct xilinx_vdma_desc_sw *xilinx_vdma_alloc_descriptor(
+					struct xilinx_vdma_chan *chan)
+{
+	struct xilinx_vdma_desc_sw *desc;
+	dma_addr_t pdesc;
+
+	desc = dma_pool_alloc(chan->desc_pool, GFP_ATOMIC, &pdesc);
+	if (!desc) {
+		dev_dbg(chan->dev, "out of memory for desc\n");
+		return NULL;
+	}
+
+	memset(desc, 0, sizeof(*desc));
+	INIT_LIST_HEAD(&desc->tx_list);
+	dma_async_tx_descriptor_init(&desc->async_tx, &chan->common);
+	desc->async_tx.tx_submit = xilinx_vdma_tx_submit;
+	desc->async_tx.phys = pdesc;
+
+	return desc;
+}
+
+/**
+ * xilinx_vdma_prep_slave_sg - prepare descriptors for a DMA_SLAVE transaction
+ * @chan: VDMA channel
+ * @sgl: scatterlist to transfer to/from
+ * @sg_len: number of entries in @scatterlist
+ * @direction: DMA direction
+ * @flags: transfer ack flags
+ */
+static struct dma_async_tx_descriptor *xilinx_vdma_prep_slave_sg(
+	struct dma_chan *dchan, struct scatterlist *sgl, unsigned int sg_len,
+	enum dma_transfer_direction direction, unsigned long flags,
+	void *context)
+{
+	struct xilinx_vdma_chan *chan;
+	struct xilinx_vdma_desc_sw *first = NULL, *prev = NULL, *new = NULL;
+	struct xilinx_vdma_desc_hw *hw = NULL, *prev_hw = NULL;
+	int i;
+	struct scatterlist *sg;
+	dma_addr_t dma_src;
+
+	if (!dchan)
+		return NULL;
+
+	chan = to_xilinx_chan(dchan);
+
+	if (chan->direction != direction)
+		return NULL;
+
+	/* Enforce one sg entry for one frame */
+	if (sg_len != chan->num_frms) {
+		dev_err(chan->dev,
+		"number of entries %d not the same as num stores %d\n",
+			sg_len, chan->num_frms);
+
+		return NULL;
+	}
+
+	if (!chan->has_SG) {
+		VDMA_OUT(&chan->addr_regs->hsize, chan->config.hsize);
+		VDMA_OUT(&chan->addr_regs->frmdly_stride,
+			chan->config.frm_dly << XILINX_VDMA_FRMDLY_SHIFT |
+			chan->config.stride);
+	}
+
+	/* Build transactions using information in the scatter gather list */
+	for_each_sg(sgl, sg, sg_len, i) {
+		/* Allocate the link descriptor from DMA pool */
+		new = xilinx_vdma_alloc_descriptor(chan);
+		if (!new) {
+			dev_err(chan->dev,
+				"No free memory for link descriptor\n");
+			goto fail;
+		}
+
+		/*
+		 * Calculate the maximum number of bytes to transfer,
+		 * making sure it is less than the hw limit
+		 */
+		hw = &(new->hw);
+
+		dma_src = sg_dma_address(sg);
+		if (chan->has_SG) {
+			hw->buf_addr = dma_src;
+
+			/* Fill in the descriptor */
+			hw->vsize = chan->config.vsize;
+			hw->hsize = chan->config.hsize;
+			hw->stride = (chan->config.frm_dly <<
+					XILINX_VDMA_FRMDLY_SHIFT) |
+					chan->config.stride;
+		} else {
+			/* Update the registers */
+			VDMA_OUT(&(chan->addr_regs->buf_addr[i]), dma_src);
+		}
+
+		/*
+		 * If this is not the first descriptor, chain the
+		 * current descriptor after the previous descriptor
+		 */
+		if (!first) {
+			first = new;
+		} else {
+			prev_hw = &(prev->hw);
+			prev_hw->next_desc = new->async_tx.phys;
+		}
+
+		new->async_tx.cookie = 0;
+		async_tx_ack(&new->async_tx);
+
+		prev = new;
+
+		/* Insert the link descriptor into the list */
+		list_add_tail(&new->node, &first->tx_list);
+	}
+
+	/* Link the last BD with the first BD */
+	hw->next_desc = first->async_tx.phys;
+
+	if (!first || !new)
+		return NULL;
+
+	new->async_tx.flags = flags;
+	new->async_tx.cookie = -EBUSY;
+
+	return &first->async_tx;
+
+fail:
+	/*
+	 * If first was not set, then we failed to allocate the very first
+	 * descriptor, and we're done */
+	if (!first)
+		return NULL;
+
+	/*
+	 * First is set, so all of the descriptors we allocated have been added
+	 * to first->tx_list, INCLUDING "first" itself. Therefore we
+	 * must traverse the list backwards freeing each descriptor in turn
+	 */
+	xilinx_vdma_free_desc_list_reverse(chan, &first->tx_list);
+	return NULL;
+}
+
+/*
+ * Run-time configuration for Axi VDMA, supports:
+ * . halt the channel
+ * . configure interrupt coalescing and inter-packet delay threshold
+ * . start/stop parking
+ * . enable genlock
+ * . set transfer information using config struct
+ */
+static int xilinx_vdma_device_control(struct dma_chan *dchan,
+				enum dma_ctrl_cmd cmd, unsigned long arg)
+{
+	struct xilinx_vdma_chan *chan;
+	unsigned long flags;
+
+	if (!dchan)
+		return -EINVAL;
+
+	chan = to_xilinx_chan(dchan);
+
+	if (cmd == DMA_TERMINATE_ALL) {
+		/* Halt the DMA engine */
+		vdma_halt(chan);
+
+		spin_lock_irqsave(&chan->lock, flags);
+
+		/* Remove and free all of the descriptors in the lists */
+		xilinx_vdma_free_desc_list(chan, &chan->pending_list);
+		xilinx_vdma_free_desc_list(chan, &chan->active_list);
+
+		spin_unlock_irqrestore(&chan->lock, flags);
+		return 0;
+	} else if (cmd == DMA_SLAVE_CONFIG) {
+		struct xilinx_vdma_config *cfg =
+				(struct xilinx_vdma_config *)arg;
+		u32 reg;
+
+		if (cfg->reset)
+			vdma_init(chan);
+
+		reg = VDMA_IN(&chan->regs->cr);
+
+		/* If vsize is -1, it is park-related operations */
+		if (cfg->vsize == -1) {
+			if (cfg->park)
+				reg &= ~XILINX_VDMA_CIRC_EN;
+			else
+				reg |= XILINX_VDMA_CIRC_EN;
+
+			VDMA_OUT(&chan->regs->cr, reg);
+			return 0;
+		}
+
+		/* If hsize is -1, it is interrupt threshold settings */
+		if (cfg->hsize == -1) {
+			if (cfg->coalesc <= XILINX_VDMA_COALESCE_MAX) {
+				reg &= ~XILINX_VDMA_XR_COALESCE_MASK;
+				reg |= cfg->coalesc <<
+					XILINX_VDMA_COALESCE_SHIFT;
+				chan->config.coalesc = cfg->coalesc;
+			}
+
+			if (cfg->delay <= XILINX_VDMA_DELAY_MAX) {
+				reg &= ~XILINX_VDMA_XR_DELAY_MASK;
+				reg |= cfg->delay << XILINX_VDMA_DELAY_SHIFT;
+				chan->config.delay = cfg->delay;
+			}
+
+			VDMA_OUT(&chan->regs->cr, reg);
+			return 0;
+		}
+
+		/* Transfer information */
+		chan->config.vsize = cfg->vsize;
+		chan->config.hsize = cfg->hsize;
+		chan->config.stride = cfg->stride;
+		chan->config.frm_dly = cfg->frm_dly;
+		chan->config.park = cfg->park;
+		chan->config.direction = cfg->direction;
+
+		/* genlock settings */
+		chan->config.gen_lock = cfg->gen_lock;
+		chan->config.master = cfg->master;
+
+		if (cfg->gen_lock) {
+			if (chan->genlock) {
+				reg |= XILINX_VDMA_SYNC_EN;
+				reg |= cfg->master << XILINX_VDMA_MSTR_SHIFT;
+			}
+		}
+
+		chan->config.frm_cnt_en = cfg->frm_cnt_en;
+		if (cfg->park)
+			chan->config.park_frm = cfg->park_frm;
+		else
+			chan->config.park_frm = -1;
+
+		chan->config.coalesc = cfg->coalesc;
+		chan->config.delay = cfg->delay;
+		if (cfg->coalesc <= XILINX_VDMA_COALESCE_MAX) {
+			reg |= cfg->coalesc << XILINX_VDMA_COALESCE_SHIFT;
+			chan->config.coalesc = cfg->coalesc;
+		}
+
+		if (cfg->delay <= XILINX_VDMA_DELAY_MAX) {
+			reg |= cfg->delay << XILINX_VDMA_DELAY_SHIFT;
+			chan->config.delay = cfg->delay;
+		}
+
+		chan->config.disable_intr = cfg->disable_intr;
+
+		if (cfg->ext_fsync)
+			reg |= cfg->ext_fsync << XILINX_VDMA_EXTFSYNC_SHIFT;
+
+		VDMA_OUT(&chan->regs->cr, reg);
+		return 0;
+	} else
+		return -ENXIO;
+}
+
+
+/*
+ * Logarithm function to compute alignment shift
+ * Only deals with value less than 4096.
+ */
+static int my_log(int value)
+{
+	int i = 0;
+	while ((1 << i) < value) {
+		i++;
+
+		if (i >= 12)
+			return 0;
+	}
+
+	return i;
+}
+
+static void xilinx_vdma_chan_remove(struct xilinx_vdma_chan *chan)
+{
+	irq_dispose_mapping(chan->irq);
+	list_del(&chan->common.device_node);
+	kfree(chan);
+}
+
+/*
+ * Probing channels
+ *
+ * . Get channel features from the device tree entry
+ * . Initialize special channel handling routines
+ */
+static int __devinit xilinx_vdma_chan_probe(struct xilinx_vdma_device *xdev,
+	struct device_node *node, u32 feature)
+{
+	struct xilinx_vdma_chan *chan;
+	int err;
+	const __be32 *value;
+	u32 width = 0, device_id = 0, flush_fsync = 0;
+
+	/* Alloc channel */
+	chan = kzalloc(sizeof(*chan), GFP_KERNEL);
+	if (!chan) {
+		dev_err(xdev->dev, "no free memory for DMA channels!\n");
+		err = -ENOMEM;
+		goto out_return;
+	}
+
+	chan->feature = feature;
+	chan->is_lite = 0;
+	chan->has_DRE = 0;
+	chan->has_SG = 0;
+	chan->max_len = XILINX_VDMA_MAX_TRANS_LEN;
+
+	value = of_get_property(node, "xlnx,include-dre", NULL);
+	if (value)
+		chan->has_DRE = be32_to_cpup(value);
+
+	value = (int *)of_get_property(node, "xlnx,genlock-mode", NULL);
+	if (value)
+		chan->genlock = be32_to_cpup(value);
+
+	value = (int *)of_get_property(node, "xlnx,datawidth", NULL);
+	if (value) {
+		width = be32_to_cpup(value) >> 3; /* Convert bits to bytes */
+
+		/* If data width is greater than 8 bytes, DRE is not in hw */
+		if (width > 8)
+			chan->has_DRE = 0;
+
+		chan->feature |= width - 1;
+	}
+
+	value = (int *)of_get_property(node, "xlnx,device-id", NULL);
+	if (value)
+		device_id = be32_to_cpup(value);
+
+	flush_fsync = (xdev->feature & XILINX_VDMA_FTR_FLUSH_MASK) >>
+			XILINX_VDMA_FTR_FLUSH_SHIFT;
+
+	chan->start_transfer = xilinx_vdma_start_transfer;
+
+	chan->has_SG = (xdev->feature & XILINX_VDMA_FTR_HAS_SG) >>
+		XILINX_VDMA_FTR_HAS_SG_SHIFT;
+
+	if (of_device_is_compatible(node,
+			"xlnx,axi-vdma-mm2s-channel")) {
+		chan->direction = DMA_MEM_TO_DEV;
+		if (!chan->has_SG) {
+			chan->addr_regs = (struct vdma_addr_regs *)
+			    ((u32)xdev->regs +
+				 XILINX_VDMA_DIRECT_REG_OFFSET);
+		}
+
+		if (flush_fsync == XILINX_VDMA_FLUSH_BOTH ||
+			flush_fsync == XILINX_VDMA_FLUSH_MM2S)
+			chan->flush_fsync = 1;
+	}
+
+	if (of_device_is_compatible(node, "xlnx,axi-vdma-s2mm-channel")) {
+		chan->direction = DMA_DEV_TO_MEM;
+		if (!chan->has_SG) {
+			chan->addr_regs = (struct vdma_addr_regs *)
+			    ((u32)xdev->regs +
+				XILINX_VDMA_DIRECT_REG_OFFSET +
+				XILINX_VDMA_CHAN_DIRECT_REG_SIZE);
+		}
+
+		if (flush_fsync == XILINX_VDMA_FLUSH_BOTH ||
+			flush_fsync == XILINX_VDMA_FLUSH_S2MM)
+			chan->flush_fsync = 1;
+	}
+
+	chan->regs = (struct xvdma_regs *)xdev->regs;
+	chan->id = 0;
+
+	if (chan->direction == DMA_DEV_TO_MEM) {
+		chan->regs = (struct xvdma_regs *)((u32)xdev->regs +
+					XILINX_VDMA_RX_CHANNEL_OFFSET);
+		chan->id = 1;
+	}
+
+	/*
+	 * Used by dmatest channel matching in slave transfers
+	 * Can change it to be a structure to have more matching information
+	 */
+	chan->private = (chan->direction & 0xFF) |
+		(chan->feature & XILINX_DMA_IP_MASK) |
+		(device_id << XILINX_VDMA_DEVICE_ID_SHIFT);
+	chan->common.private = (void *)&(chan->private);
+
+	if (!chan->has_DRE)
+		xdev->common.copy_align = my_log(width);
+
+	chan->dev = xdev->dev;
+	xdev->chan[chan->id] = chan;
+
+	tasklet_init(&chan->tasklet, dma_do_tasklet, (unsigned long)chan);
+
+	/* Initialize the channel */
+	if (vdma_init(chan)) {
+		dev_err(xdev->dev, "Reset channel failed\n");
+		goto out_free_chan;
+	}
+
+	spin_lock_init(&chan->lock);
+	INIT_LIST_HEAD(&chan->pending_list);
+	INIT_LIST_HEAD(&chan->active_list);
+
+	chan->common.device = &xdev->common;
+
+	/* Find the IRQ line, if it exists in the device tree */
+	chan->irq = irq_of_parse_and_map(node, 0);
+	err = request_irq(chan->irq, vdma_intr_handler, IRQF_SHARED,
+				"xilinx-vdma-controller", chan);
+	if (err) {
+		dev_err(xdev->dev, "unable to request IRQ\n");
+		goto out_free_irq;
+	}
+
+	/* Add the channel to DMA device channel list */
+	list_add_tail(&chan->common.device_node, &xdev->common.channels);
+	xdev->common.chancnt++;
+
+	return 0;
+
+out_free_irq:
+	irq_dispose_mapping(chan->irq);
+out_free_chan:
+	kfree(chan);
+out_return:
+	return err;
+}
+
+static int __devinit xilinx_vdma_of_probe(struct platform_device *op)
+{
+	struct xilinx_vdma_device *xdev;
+	struct device_node *child, *node;
+	int err, i;
+	int *value;
+	int num_frames = 0;
+
+	dev_info(&op->dev, "Probing xilinx axi vdma engine\n");
+
+	xdev = kzalloc(sizeof(struct xilinx_vdma_device), GFP_KERNEL);
+	if (!xdev) {
+		dev_err(&op->dev, "Not enough memory for device\n");
+		err = -ENOMEM;
+		goto out_return;
+	}
+
+	xdev->dev = &(op->dev);
+	INIT_LIST_HEAD(&xdev->common.channels);
+
+	node = op->dev.of_node;
+	xdev->feature = 0;
+
+	/* iomap registers */
+	xdev->regs = of_iomap(node, 0);
+	if (!xdev->regs) {
+		dev_err(&op->dev, "unable to iomap registers\n");
+		err = -ENOMEM;
+		goto out_free_xdev;
+	}
+
+	/* Axi VDMA only do slave transfers */
+	if (of_device_is_compatible(node, "xlnx,axi-vdma")) {
+		xdev->feature |= XILINX_DMA_IP_VDMA;
+
+		value = (int *)of_get_property(node, "xlnx,include-sg",
+				NULL);
+		if (value) {
+			if (be32_to_cpup(value) == 1)
+				xdev->feature |= XILINX_VDMA_FTR_HAS_SG;
+		}
+
+		value = (int *)of_get_property(node, "xlnx,num-fstores",
+			NULL);
+		if (value)
+			num_frames = be32_to_cpup(value);
+
+		value = (int *)of_get_property(node, "xlnx,flush-fsync", NULL);
+		if (value)
+			xdev->feature |= be32_to_cpup(value) <<
+				XILINX_VDMA_FTR_FLUSH_SHIFT;
+
+		dma_cap_set(DMA_SLAVE, xdev->common.cap_mask);
+		dma_cap_set(DMA_PRIVATE, xdev->common.cap_mask);
+		xdev->common.device_prep_slave_sg = xilinx_vdma_prep_slave_sg;
+		xdev->common.device_control = xilinx_vdma_device_control;
+		xdev->common.device_issue_pending = xilinx_vdma_issue_pending;
+	}
+
+	xdev->common.device_alloc_chan_resources =
+				xilinx_vdma_alloc_chan_resources;
+	xdev->common.device_free_chan_resources =
+				xilinx_vdma_free_chan_resources;
+	xdev->common.device_tx_status = xilinx_tx_status;
+	xdev->common.dev = &op->dev;
+
+	dev_set_drvdata(&op->dev, xdev);
+
+	for_each_child_of_node(node, child) {
+		xilinx_vdma_chan_probe(xdev, child, xdev->feature);
+	}
+
+	for (i = 0; i < XILINX_VDMA_MAX_CHANS_PER_DEVICE; i++) {
+		if (xdev->chan[i])
+			xdev->chan[i]->num_frms = num_frames;
+	}
+
+	dma_async_device_register(&xdev->common);
+
+	return 0;
+
+out_free_xdev:
+	kfree(xdev);
+
+out_return:
+	return err;
+}
+
+static int __devexit xilinx_vdma_of_remove(struct platform_device *op)
+{
+	struct xilinx_vdma_device *xdev;
+	int i;
+
+	xdev = dev_get_drvdata(&op->dev);
+	dma_async_device_unregister(&xdev->common);
+
+	for (i = 0; i < XILINX_VDMA_MAX_CHANS_PER_DEVICE; i++) {
+		if (xdev->chan[i])
+			xilinx_vdma_chan_remove(xdev->chan[i]);
+	}
+
+	iounmap(xdev->regs);
+	dev_set_drvdata(&op->dev, NULL);
+	kfree(xdev);
+
+	return 0;
+}
+
+static const struct of_device_id xilinx_vdma_of_ids[] = {
+	{ .compatible = "xlnx,axi-vdma",},
+	{}
+};
+
+static struct platform_driver xilinx_vdma_of_driver = {
+	.driver = {
+		.name = "xilinx-vdma",
+		.owner = THIS_MODULE,
+		.of_match_table = xilinx_vdma_of_ids,
+	},
+	.probe = xilinx_vdma_of_probe,
+	.remove = __devexit_p(xilinx_vdma_of_remove),
+};
+
+module_platform_driver(xilinx_vdma_of_driver);
+
+MODULE_AUTHOR("Xilinx, Inc.");
+MODULE_DESCRIPTION("Xilinx VDMA driver");
+MODULE_LICENSE("GPL v2");
diff --git a/include/linux/amba/xilinx_dma.h b/include/linux/amba/xilinx_dma.h
index 751a743..dca98af 100644
--- a/include/linux/amba/xilinx_dma.h
+++ b/include/linux/amba/xilinx_dma.h
@@ -1,12 +1,11 @@
 /*
- * Xilinx Central DMA Engine support
+ * Xilinx DMA Engines support header file
  *
  * Copyright (C) 2010 Xilinx, Inc. All rights reserved.
  *
  * Based on the Freescale DMA driver.
  *
  * Description:
- * This driver supports three Xilinx DMA engines:
  *  . Axi CDMA engine, it does transfers between memory and memory, it
  *    only has one channel.
  *  . Axi DMA engine, it does transfers between memory and device. It can be
@@ -31,63 +30,57 @@
 #include <linux/dmaengine.h>
 #include <linux/dma-mapping.h>
 
-/* Specific hardware configuration-related constants
- */
-#define XILINX_DMA_NO_CHANGE             0xFFFF;
+/* Specific hardware configuration-related constants */
+#define XILINX_DMA_NO_CHANGE	0xFFFF;
 
-/* DMA IP masks 
- */
-#define XILINX_DMA_IP_DMA              0x00100000 /* A DMA IP */
-#define XILINX_DMA_IP_CDMA             0x00200000 /* A Central DMA IP */
-#define XILINX_DMA_IP_VDMA             0x00400000 /* A Video DMA IP */
-#define XILINX_DMA_IP_MASK             0x00700000 /* DMA IP MASK */
+/* DMA IP masks */
+#define XILINX_DMA_IP_DMA	0x00100000	/* A DMA IP */
+#define XILINX_DMA_IP_CDMA	0x00200000	/* A Central DMA IP */
+#define XILINX_DMA_IP_VDMA	0x00400000	/* A Video DMA IP */
+#define XILINX_DMA_IP_MASK	0x00700000	/* DMA IP MASK */
 
-/* shared by all Xilinx DMA engines
- */
-/* Device configuration structure
- *
- * Xilinx CDMA and Xilinx DMA only use interrupt coalescing and delay counter
- * settings.
+/*
+ * Device configuration structure
  *
  * If used to start/stop parking mode for Xilinx VDMA, vsize must be -1
  * If used to set interrupt coalescing and delay counter only for
- * Xilinx VDMA, hsize must be -1 */
-struct xilinx_dma_config {
-	enum dma_transfer_direction direction; /* Channel direction */
-	int vsize;                         /* Vertical size */
-	int hsize;                         /* Horizontal size */
-	int stride;                        /* Stride */
-	int frm_dly;                       /* Frame delay */
-	int gen_lock;                      /* Whether in gen-lock mode */
-	int master;                        /* Master that it syncs to */
-	int frm_cnt_en;                    /* Enable frame count enable */
-	int park;                          /* Whether wants to park */
-	int park_frm;                      /* Frame to park on */
-	int coalesc;                       /* Interrupt coalescing threshold */
-	int delay;                         /* Delay counter */
-	int disable_intr;                  /* Whether use interrupts */
-	int reset;			   /* Reset Channel */
-	int ext_fsync;			   /* External Frame Sync */
+ * Xilinx VDMA, hsize must be -1
+ */
+struct xilinx_vdma_config {
+	enum dma_transfer_direction direction;
+					/* Channel direction */
+	int vsize;			/* Vertical size */
+	int hsize;			/* Horizontal size */
+	int stride;			/* Stride */
+	int frm_dly;			/* Frame delay */
+	int gen_lock;			/* Whether in gen-lock mode */
+	int master;			/* Master that it syncs to */
+	int frm_cnt_en;			/* Enable frame count enable */
+	int park;			/* Whether wants to park */
+	int park_frm;			/* Frame to park on */
+	int coalesc;			/* Interrupt coalescing threshold */
+	int delay;			/* Delay counter */
+	int disable_intr;		/* Whether use interrupts */
+	int reset;			/* Reset Channel */
+	int ext_fsync;			/* External Frame Sync */
 };
 
-/* Platform data definition until ARM supports device tree */
-
-struct dma_channel_config {
-	char *type;	
-	unsigned int lite_mode;       /* cdma only */
-	unsigned int include_dre;
-	unsigned int genlock_mode;    /* vdma only */
-	unsigned int datawidth;
-	unsigned int max_burst_len;
+/* Device configuration structure for DMA */
+struct xilinx_dma_config {
+	enum dma_transfer_direction direction;
+					/* Channel direction */
+	int coalesc;			/* Interrupt coalescing threshold */
+	int delay;			/* Delay counter */
+	int reset;			/* Reset Channel */
 };
 
-struct dma_device_config {
-	char *type;	
-	unsigned int include_sg;
-	unsigned int num_fstores;    /* vdma only */
-	unsigned int sg_include_stscntrl_strm;  /* dma only */
-	unsigned int channel_count;
-	struct dma_channel_config *channel_config;
+/* Device configuration structure for CDMA */
+struct xilinx_cdma_config {
+	enum dma_transfer_direction direction;
+					/* Channel direction */
+	int coalesc;			/* Interrupt coalescing threshold */
+	int delay;			/* Delay counter */
+	int reset;			/* Reset Channel */
 };
 
 #endif
-- 
1.7.5.4

