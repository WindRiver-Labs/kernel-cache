From 014a7d6d7c4bd598a4f875fd19eaaa43d119f007 Mon Sep 17 00:00:00 2001
From: Corey Minyard <cminyard@mvista.com>
Date: Mon, 6 May 2013 15:36:11 -0500
Subject: [PATCH] Live application coredump

The original commit taken from: https://github.com/cminyard/linux-live-app-coredump.git.

Add the ability to take a coredump of an application while it is running
with minimal impact on the application itself.  This creates a copy of the
process and coredump the copy.

Signed-off-by: Dmitry Antipov <antipov@mvista.com>
Signed-off-by: Dale Farnsworth <dfarnsworth@mvista.com>
Signed-off-by: Andrei Konovalov <akonovalov@mvista.com>
Signed-off-by: Corey Minyard <cminyard@mvista.com>
Signed-off-by: Jianchuan Wang <jianchuan.wang@windriver.com>

diff --git a/arch/mips/kernel/ptrace32.c b/arch/mips/kernel/ptrace32.c
index 283b5a1967d1..13dbf6a72486 100644
--- a/arch/mips/kernel/ptrace32.c
+++ b/arch/mips/kernel/ptrace32.c
@@ -47,7 +47,14 @@ long compat_arch_ptrace(struct task_struct *child, compat_long_t request,
 	int ret;
 
 	switch (request) {
-
+	case PTRACE_LIVEDUMP:
+#ifdef CONFIG_LIVEDUMP
+		ret = compat_ptrace_livedump(child,
+		      (struct compat_livedump_param __user *)(long)cdata);
+#else
+		ret = -ENOSYS;
+#endif /* CONFIG_LIVEDUMP */
+		break;
 	/*
 	 * Read 4 bytes of the other process' storage
 	 *  data is a pointer specifying where the user wants the
diff --git a/arch/powerpc/include/asm/systbl.h b/arch/powerpc/include/asm/systbl.h
index f1863a138b4a..3eaecaf627fd 100644
--- a/arch/powerpc/include/asm/systbl.h
+++ b/arch/powerpc/include/asm/systbl.h
@@ -4,7 +4,7 @@
  */
 
 SYSCALL(restart_syscall)
-SYSCALL(exit)
+PPC_SYS(exit)
 PPC_SYS(fork)
 SYSCALL_SPU(read)
 SYSCALL_SPU(write)
@@ -29,7 +29,7 @@ SYSX(sys_ni_syscall,sys_oldumount,sys_oldumount)
 SYSCALL_SPU(setuid)
 SYSCALL_SPU(getuid)
 COMPAT_SYS_SPU(stime)
-COMPAT_SYS(ptrace)
+PPC_SYS(ptrace)
 SYSCALL_SPU(alarm)
 OLDSYS(fstat)
 SYSCALL(pause)
@@ -238,7 +238,7 @@ COMPAT_SYS_SPU(io_submit)
 SYSCALL_SPU(io_cancel)
 SYSCALL(set_tid_address)
 SYSX_SPU(sys_fadvise64,ppc32_fadvise64,sys_fadvise64)
-SYSCALL(exit_group)
+PPC_SYS(exit_group)
 COMPAT_SYS(lookup_dcookie)
 SYSCALL_SPU(epoll_create)
 SYSCALL_SPU(epoll_ctl)
diff --git a/arch/powerpc/kernel/entry_32.S b/arch/powerpc/kernel/entry_32.S
index 3d390ac490d9..1dfb3f98172c 100644
--- a/arch/powerpc/kernel/entry_32.S
+++ b/arch/powerpc/kernel/entry_32.S
@@ -552,6 +552,40 @@ ppc_swapcontext:
 	b	sys_swapcontext
 
 /*
+ * Live dumping calls copy_proces from ptrace and may also do so from
+ * exit/exit_group, therefore full register set is required for them too.
+ */
+	.globl	ppc_ptrace
+ppc_ptrace:
+#ifdef CONFIG_LIVEDUMP
+	SAVE_NVGPRS(r1)
+	lwz	r0,_TRAP(r1)
+	rlwinm	r0,r0,0,0,30
+	stw	r0,_TRAP(r1)
+#endif
+	b	sys_ptrace
+
+	.globl	ppc_exit_group
+ppc_exit_group:
+#ifdef CONFIG_LIVEDUMP
+	SAVE_NVGPRS(r1)
+	lwz	r0,_TRAP(r1)
+	rlwinm	r0,r0,0,0,30
+	stw	r0,_TRAP(r1)
+#endif
+	b	sys_exit_group
+
+	.globl	ppc_exit
+ppc_exit:
+#ifdef CONFIG_LIVEDUMP
+	SAVE_NVGPRS(r1)
+	lwz	r0,_TRAP(r1)
+	rlwinm	r0,r0,0,0,30
+	stw	r0,_TRAP(r1)
+#endif
+	b	sys_exit
+
+/*
  * Top-level page fault handling.
  * This is in assembler because if do_page_fault tells us that
  * it is a bad kernel page fault, we want to save the non-volatile
diff --git a/arch/powerpc/kernel/entry_64.S b/arch/powerpc/kernel/entry_64.S
index 5e2d2645d1e0..226a0ce0f938 100644
--- a/arch/powerpc/kernel/entry_64.S
+++ b/arch/powerpc/kernel/entry_64.S
@@ -361,6 +361,27 @@ _GLOBAL(ppc_switch_endian)
 	bl	sys_switch_endian
 	b	.Lsyscall_exit
 
+_GLOBAL(ppc_ptrace)
+#ifdef CONFIG_LIVEDUMP
+	bl	.save_nvgprs
+#endif
+	bl	.sys_ptrace
+	b	syscall_exit
+
+_GLOBAL(ppc_exit_group)
+#ifdef CONFIG_LIVEDUMP
+	bl	.save_nvgprs
+#endif
+	bl	.sys_exit_group
+	b	syscall_exit
+
+_GLOBAL(ppc_exit)
+#ifdef CONFIG_LIVEDUMP
+	bl	.save_nvgprs
+#endif
+	bl	.sys_exit
+	b	syscall_exit
+
 _GLOBAL(ret_from_fork)
 	bl	schedule_tail
 	REST_NVGPRS(r1)
diff --git a/block/ioprio.c b/block/ioprio.c
index 31666c92b46a..0070249ec213 100644
--- a/block/ioprio.c
+++ b/block/ioprio.c
@@ -141,7 +141,7 @@ free_uid:
 	return ret;
 }
 
-static int get_task_ioprio(struct task_struct *p)
+int get_task_ioprio(struct task_struct *p)
 {
 	int ret;
 
@@ -154,6 +154,7 @@ static int get_task_ioprio(struct task_struct *p)
 out:
 	return ret;
 }
+EXPORT_SYMBOL_GPL(get_task_ioprio);
 
 int ioprio_best(unsigned short aprio, unsigned short bprio)
 {
diff --git a/fs/coredump.c b/fs/coredump.c
index 8dd099dc5f9b..173cc76d39d3 100644
--- a/fs/coredump.c
+++ b/fs/coredump.c
@@ -497,7 +497,7 @@ static int umh_pipe_setup(struct subprocess_info *info, struct cred *new)
 	return err;
 }
 
-void do_coredump(const siginfo_t *siginfo)
+int do_coredump(const siginfo_t *siginfo)
 {
 	struct core_state core_state;
 	struct core_name cn;
@@ -719,7 +719,7 @@ fail_unlock:
 fail_creds:
 	put_cred(cred);
 fail:
-	return;
+	return retval;
 }
 
 /*
diff --git a/fs/proc/base.c b/fs/proc/base.c
index fcdeb1eb3921..aa98a1fb30d6 100644
--- a/fs/proc/base.c
+++ b/fs/proc/base.c
@@ -2746,7 +2746,7 @@ void proc_flush_task(struct task_struct *task)
 	for (i = 0; i <= pid->level; i++) {
 		upid = &pid->numbers[i];
 		proc_flush_task_mnt(upid->ns->proc_mnt, upid->nr,
-					tgid->numbers[i].nr);
+				    tgid ? tgid->numbers[i].nr : 0);
 	}
 }
 
diff --git a/include/linux/compat.h b/include/linux/compat.h
index ab25814690bc..bf61ee3625dc 100644
--- a/include/linux/compat.h
+++ b/include/linux/compat.h
@@ -442,6 +442,19 @@ asmlinkage long compat_sys_ptrace(compat_long_t request, compat_long_t pid,
 				  compat_long_t addr, compat_long_t data);
 
 asmlinkage long compat_sys_lookup_dcookie(u32, u32, char __user *, compat_size_t);
+
+#ifdef CONFIG_LIVEDUMP
+struct compat_livedump_param {
+	compat_int_t	sched_nice;
+	compat_int_t	io_prio;
+	compat_int_t	oom_adj;
+	compat_ulong_t	core_limit;
+};
+
+extern long compat_ptrace_livedump(struct task_struct *tsk,
+		struct compat_livedump_param __user *cparam);
+#endif
+
 /*
  * epoll (fs/eventpoll.c) compat bits follow ...
  */
diff --git a/include/linux/coredump.h b/include/linux/coredump.h
index d016a121a8c4..f27032bcb2cc 100644
--- a/include/linux/coredump.h
+++ b/include/linux/coredump.h
@@ -15,9 +15,9 @@ extern int dump_skip(struct coredump_params *cprm, size_t nr);
 extern int dump_emit(struct coredump_params *cprm, const void *addr, int nr);
 extern int dump_align(struct coredump_params *cprm, int align);
 #ifdef CONFIG_COREDUMP
-extern void do_coredump(const siginfo_t *siginfo);
+extern int do_coredump(const siginfo_t *siginfo);
 #else
-static inline void do_coredump(const siginfo_t *siginfo) {}
+static inline int do_coredump(const siginfo_t *siginfo) { return 0; }
 #endif
 
 #endif /* _LINUX_COREDUMP_H */
diff --git a/include/linux/ioprio.h b/include/linux/ioprio.h
index beb9ce1c2c23..1c7cada1da94 100644
--- a/include/linux/ioprio.h
+++ b/include/linux/ioprio.h
@@ -75,5 +75,6 @@ static inline int task_nice_ioclass(struct task_struct *task)
 extern int ioprio_best(unsigned short aprio, unsigned short bprio);
 
 extern int set_task_ioprio(struct task_struct *task, int ioprio);
+extern int get_task_ioprio(struct task_struct *task);
 
 #endif
diff --git a/include/linux/livedump.h b/include/linux/livedump.h
new file mode 100644
index 000000000000..c4eee02e3ba4
--- /dev/null
+++ b/include/linux/livedump.h
@@ -0,0 +1,188 @@
+/*
+ * include/linux/livedump.h
+ *
+ * Live application dump datatype(s) and prototypes.
+ */
+
+#ifndef _LINUX_LIVEDUMP_H_
+#define _LINUX_LIVEDUMP_H_
+
+#include <linux/kref.h>
+#include <linux/mutex.h>
+#include <linux/completion.h>
+#include <linux/slab.h>
+#include <asm/atomic.h>
+#include <asm/siginfo.h>
+
+/* This may be passed as 'data' argument of ptrace(PT_LIVEDUMP,...) call. */
+struct livedump_param {
+	int sched_nice;
+	int io_prio;
+	int oom_adj;
+	unsigned long core_limit;
+};
+
+#ifdef __KERNEL__
+
+#ifdef CONFIG_LIVEDUMP
+
+/* Dumping process stages.  */
+typedef enum {
+	COPY_LEADER,
+	COPY_THREADS,
+	PERFORM_DUMP
+} livedump_stage_t;
+
+/*
+ * This is the dump context. It's created only once when dumping
+ * begins, and referenced from the task_struct of each clone involved
+ * in dumping.
+ */
+struct livedump_context {
+	/* Number of references to dump context. */
+	struct kref ref;
+
+	/* The thread group leader of the task being dumped. */
+	struct task_struct *origin;
+
+	/* The thread group leader of the cloned task. */
+	struct task_struct *leader;
+
+	/* Number of tasks still in the process of being cloned. */
+	atomic_t nr_clone_remains;
+
+	/* Tell the main thread the mm is duplicated. */
+	struct completion dump_ready;
+
+	/* Tell the main thread the dump is complete. */
+	struct completion dump_complete;
+
+	/* If we're requesting the dump of itself, it's performed in
+	   the context of helper kernel thread. This is used to wait
+	   until this thread completes. */
+	struct completion thread_exit;
+
+	/* Parameters from the user. */
+	struct livedump_param param;
+
+	/* The stage of dump we are in. */
+	livedump_stage_t stage;
+
+	/* Return status for the dump. */
+	int status;
+};
+
+static inline void livedump_request(struct task_struct *tsk)
+{
+	force_sig_info(SIGKILL, SEND_SIG_FORCED, tsk);
+}
+
+/*
+ * Same as above, but no locking on sighand->siglock.  Assumes the
+ * signal can't be blocked or ignored.
+ */
+static inline void __livedump_request(struct task_struct *tsk)
+{
+	specific_send_sig_info(SIGKILL, SEND_SIG_FORCED, tsk);
+}
+
+static inline void livedump_set_status(struct livedump_context *dump,
+				       long status)
+{
+	set_mb(dump->status, status);
+}
+
+static inline int livedump_status(struct livedump_context *dump)
+{
+	return dump->status;
+}
+
+static inline void livedump_stage(struct livedump_context *dump,
+				  livedump_stage_t stage)
+{
+	set_mb(dump->stage, stage);
+}
+
+static inline int in_livedump(struct task_struct *tsk,
+			      livedump_stage_t check_stage)
+{
+	return tsk->dump ? (tsk->dump->stage == check_stage) : 0;
+}
+
+extern void livedump_ref_done(struct kref *ref);
+
+static inline struct livedump_context *get_dump(struct livedump_context *dump)
+{
+	kref_get(&dump->ref);
+	return dump;
+}
+
+static inline void put_dump(struct livedump_context *dump)
+{
+	kref_put(&dump->ref, livedump_ref_done);
+}
+
+/*
+ * If we are waiting, we hold a reference to the dump structure;
+ * livedump_wait() decrements the reference and frees if necessary.
+ */
+static inline void livedump_wait(struct livedump_context *dump)
+{
+	wait_for_completion(&dump->dump_ready);
+	current->dump = NULL;
+	put_dump(dump);
+}
+
+static inline void livedump_block_signals(sigset_t *saveset)
+{
+	/* Save original blocked set, block everything except SIGKILL
+	   which must be handled to drive dumping process. */
+	spin_lock_irq(&current->sighand->siglock);
+	*saveset = current->blocked;
+	sigfillset(&current->blocked);
+	sigdelset(&current->blocked, SIGKILL);
+	recalc_sigpending();
+	spin_unlock_irq(&current->sighand->siglock);
+}
+
+static inline void livedump_unblock_signals(sigset_t *restoreset)
+{
+        /* Restore original blocking set. */
+	spin_lock_irq(&current->sighand->siglock);
+	current->blocked = *restoreset;
+	recalc_sigpending();
+	spin_unlock_irq(&current->sighand->siglock);
+}
+
+static inline void livedump_maybe_wait_dump(struct task_struct *tsk) {
+	struct task_struct *leader = tsk->group_leader;
+
+	if (leader->extra_flags & PFE_LIVEDUMP) {
+		sigset_t set;
+
+		/* This thread group performs live dumping.
+		   Wait for it's completion, but be ready to
+		   process SIGKILL to clone current thread. */
+		livedump_block_signals(&set);
+		do {
+			if (schedule_timeout_interruptible(1)) {
+				struct ksignal ks;
+				get_signal(&ks);
+			}
+			barrier();
+		} while (leader->extra_flags & PFE_LIVEDUMP);
+		livedump_unblock_signals(&set);
+	}
+}
+
+extern void livedump_clone_leader(void);
+extern void livedump_clone_thread(void);
+extern void livedump_perform_dump(siginfo_t *);
+extern int livedump_take(struct livedump_context *dump);
+extern long ptrace_livedump(struct task_struct *tsk,
+			    struct livedump_param __user *param);
+
+#endif /* CONFIG_LIVEDUMP */
+
+#endif /* __KERNEL__ */
+#endif /* _LINUX_LIVEDUMP_H_ */
diff --git a/include/linux/sched.h b/include/linux/sched.h
index def8ddd7eb94..07bb3afd169d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -126,6 +126,9 @@ struct sched_attr {
 	u64 sched_period;
 };
 
+#ifdef CONFIG_LIVEDUMP
+struct livedump_context;
+#endif
 struct futex_pi_state;
 struct robust_list_head;
 struct bio_list;
@@ -1270,6 +1273,9 @@ struct sched_rt_entity {
 	/* rq "owned" by this entity/group: */
 	struct rt_rq		*my_q;
 #endif
+#ifdef CONFIG_LIVEDUMP
+	struct livedump_context *dump;
+#endif
 };
 
 struct sched_dl_entity {
@@ -1343,6 +1349,7 @@ struct task_struct {
 	void *stack;
 	atomic_t usage;
 	unsigned int flags;	/* per process flags, defined below */
+	unsigned int extra_flags;
 	unsigned int ptrace;
 
 #ifdef CONFIG_SMP
@@ -2082,6 +2089,9 @@ extern void thread_group_cputime_adjusted(struct task_struct *p, cputime_t *ut,
 #define PF_FREEZER_SKIP	0x40000000	/* Freezer should not count it as freezable */
 #define PF_SUSPEND_TASK 0x80000000      /* this thread called freeze_processes and should not be frozen */
 
+/* Flags in the extra_flags field */
+#define PFE_LIVEDUMP	0x00001000	/* doing live dump */
+
 /*
  * Only the _current_ task can read/write to tsk->flags, but other
  * tasks can access tsk->flags in readonly mode for example
@@ -2622,6 +2632,9 @@ extern int do_execveat(int, struct filename *,
 		       const char __user * const __user *,
 		       int);
 extern long do_fork(unsigned long, unsigned long, unsigned long, int __user *, int __user *);
+extern struct task_struct *copy_process(unsigned long, unsigned long,
+					unsigned long, int __user *,
+					struct pid *, int);
 struct task_struct *fork_idle(int);
 extern pid_t kernel_thread(int (*fn)(void *), void *arg, unsigned long flags);
 
diff --git a/include/linux/signal.h b/include/linux/signal.h
index 66215b20aa8d..a3566fe8d1d5 100644
--- a/include/linux/signal.h
+++ b/include/linux/signal.h
@@ -232,6 +232,7 @@ struct pt_regs;
 extern int next_signal(struct sigpending *pending, sigset_t *mask);
 extern int do_send_sig_info(int sig, struct siginfo *info,
 				struct task_struct *p, bool group);
+extern int specific_send_sig_info(int, struct siginfo *, struct task_struct *);
 extern int group_send_sig_info(int sig, struct siginfo *info, struct task_struct *p);
 extern int __group_send_sig_info(int, struct siginfo *, struct task_struct *);
 extern int do_sigtimedwait(const sigset_t *, siginfo_t *,
diff --git a/include/uapi/linux/ptrace.h b/include/uapi/linux/ptrace.h
index a7a697986614..77afdd935561 100644
--- a/include/uapi/linux/ptrace.h
+++ b/include/uapi/linux/ptrace.h
@@ -28,6 +28,7 @@
 #define PTRACE_GETEVENTMSG	0x4201
 #define PTRACE_GETSIGINFO	0x4202
 #define PTRACE_SETSIGINFO	0x4203
+#define PTRACE_LIVEDUMP 	0x4221
 
 /*
  * Generic ptrace interface that exports the architecture specific regsets
diff --git a/include/uapi/linux/sched.h b/include/uapi/linux/sched.h
index cc89ddefa926..4ea65d998f09 100644
--- a/include/uapi/linux/sched.h
+++ b/include/uapi/linux/sched.h
@@ -9,6 +9,7 @@
 #define CLONE_FS	0x00000200	/* set if fs info shared between processes */
 #define CLONE_FILES	0x00000400	/* set if open files shared between processes */
 #define CLONE_SIGHAND	0x00000800	/* set if signal handlers and blocked signals shared */
+#define CLONE_LIVEDUMP	0x00001000	/* set if cloned for a live dump */
 #define CLONE_PTRACE	0x00002000	/* set if we want to let tracing continue on the child too */
 #define CLONE_VFORK	0x00004000	/* set if the parent wants the child to wake it up on mm_release */
 #define CLONE_PARENT	0x00008000	/* set if we want to have the same parent as the cloner */
diff --git a/init/Kconfig b/init/Kconfig
index a70b5002df06..70f494fd8593 100644
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -1463,6 +1463,13 @@ config ELF_CORE
 	help
 	  Enable support for generating core dumps. Disabling saves about 4k.
 
+config LIVEDUMP
+	default y
+	bool "Enable Live Application Dump"
+	help
+	  Enable support for live application dump, i.e. the capability to
+	  drop core files from the running process without (substantial)
+	  interruption of it.
 
 config PCSPKR_PLATFORM
 	bool "Enable PC-Speaker support" if EXPERT
diff --git a/kernel/Makefile b/kernel/Makefile
index 60c302cfb4d3..553295cbca0e 100644
--- a/kernel/Makefile
+++ b/kernel/Makefile
@@ -99,6 +99,8 @@ obj-$(CONFIG_JUMP_LABEL) += jump_label.o
 obj-$(CONFIG_CONTEXT_TRACKING) += context_tracking.o
 obj-$(CONFIG_TORTURE_TEST) += torture.o
 
+obj-$(CONFIG_LIVEDUMP) += livedump.o
+
 $(obj)/configs.o: $(obj)/config_data.h
 
 # config_data.h contains the same information as ikconfig.h but gzipped.
diff --git a/kernel/exit.c b/kernel/exit.c
index a0cf72b37b72..1e536bee6bc9 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -53,6 +53,7 @@
 #include <linux/oom.h>
 #include <linux/writeback.h>
 #include <linux/shm.h>
+#include <linux/livedump.h>
 
 #include <asm/uaccess.h>
 #include <asm/unistd.h>
@@ -604,6 +605,11 @@ static void exit_notify(struct task_struct *tsk, int group_dead)
 				!ptrace_reparented(tsk) ?
 			tsk->exit_signal : SIGCHLD;
 		autoreap = do_notify_parent(tsk, sig);
+#ifdef CONFIG_LIVEDUMP
+	} else if (tsk->dump) {
+		/* Always autoreap livedumped threads. */
+		autoreap = true;
+#endif
 	} else if (thread_group_leader(tsk)) {
 		autoreap = thread_group_empty(tsk) &&
 			do_notify_parent(tsk, tsk->exit_signal);
@@ -656,6 +662,9 @@ void do_exit(long code)
 	int group_dead;
 	TASKS_RCU(int tasks_rcu_i);
 
+#ifdef CONFIG_LIVEDUMP
+	livedump_maybe_wait_dump(tsk);
+#endif
 	profile_task_exit(tsk);
 
 	WARN_ON(blk_needs_flush_plug(tsk));
@@ -774,6 +783,27 @@ void do_exit(long code)
 	if (unlikely(current->pi_state_cache))
 		kfree(current->pi_state_cache);
 #endif
+#ifdef CONFIG_LIVEDUMP
+	if (unlikely(tsk->dump)) {
+		if (in_livedump(tsk, COPY_THREADS)) {
+			/*
+			 * If livedumping and in COPY_THREADS state,
+			 * that means this thread is expected to clone
+			 * itself.  But at this point, it doesn't
+			 * matter, so just pretend like it was never
+			 * requested.
+			 */
+			if (atomic_dec_and_test(&tsk->dump->nr_clone_remains)){
+				/* All threads are cloned, or at least
+				 * have tried to clone. */
+				livedump_stage(tsk->dump, PERFORM_DUMP);
+				livedump_request(tsk->dump->leader);
+			}
+		}
+		/* Free the dump variable if necessary. */
+		put_dump(tsk->dump);
+	}
+#endif /* CONFIG_LIVEDUMP */
 	/*
 	 * Make sure we are holding no locks:
 	 */
@@ -853,7 +883,9 @@ do_group_exit(int exit_code)
 	struct signal_struct *sig = current->signal;
 
 	BUG_ON(exit_code & 0x80); /* core dumps don't get here */
-
+#ifdef CONFIG_LIVEDUMP
+	livedump_maybe_wait_dump(current);
+#endif
 	if (signal_group_exit(sig))
 		exit_code = sig->group_exit_code;
 	else if (!thread_group_empty(current)) {
diff --git a/kernel/fork.c b/kernel/fork.c
index 1b0e656f60e8..cece0ed27df2 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -75,6 +75,7 @@
 #include <linux/aio.h>
 #include <linux/compiler.h>
 #include <linux/sysctl.h>
+#include <linux/livedump.h>
 
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
@@ -975,7 +976,17 @@ static int copy_mm(unsigned long clone_flags, struct task_struct *tsk)
 
 	tsk->mm = NULL;
 	tsk->active_mm = NULL;
-
+#ifdef CONFIG_LIVEDUMP
+	if ((clone_flags & (CLONE_LIVEDUMP | CLONE_THREAD)) ==
+	    (CLONE_LIVEDUMP | CLONE_THREAD)) {
+		BUG_ON(!current->dump);
+		/* Assign temporary VM to livedumped clone. */
+		tsk->mm = current->dump->leader->mm;
+		tsk->active_mm = tsk->mm;
+		atomic_inc(&tsk->mm->mm_users);
+		return 0;
+	}
+#endif
 	/*
 	 * Are we cloning a kernel thread?
 	 *
@@ -1086,6 +1097,14 @@ static int copy_sighand(unsigned long clone_flags, struct task_struct *tsk)
 	struct sighand_struct *sig;
 
 	if (clone_flags & CLONE_SIGHAND) {
+#ifdef CONFIG_LIVEDUMP
+		if (clone_flags & CLONE_LIVEDUMP) {
+			rcu_assign_pointer(tsk->sighand,
+					   current->dump->leader->sighand);
+			atomic_inc(&tsk->sighand->count);
+			return 0;
+		}
+#endif
 		atomic_inc(&current->sighand->count);
 		return 0;
 	}
@@ -1136,8 +1155,17 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 {
 	struct signal_struct *sig;
 
-	if (clone_flags & CLONE_THREAD)
+	if (clone_flags & CLONE_THREAD) {
+#ifdef CONFIG_LIVEDUMP
+		if (clone_flags & CLONE_LIVEDUMP) {
+			tsk->signal = current->dump->leader->signal;
+			atomic_inc(&tsk->signal->sigcnt);
+			atomic_inc(&tsk->signal->live);
+			return 0;
+		}
+#endif
 		return 0;
+	}
 
 	sig = kmem_cache_zalloc(signal_cachep, GFP_KERNEL);
 	tsk->signal = sig;
@@ -1265,12 +1293,12 @@ init_task_pid(struct task_struct *task, enum pid_type type, struct pid *pid)
  * parts of the process environment (as per the clone
  * flags). The actual kick-off is left to the caller.
  */
-static struct task_struct *copy_process(unsigned long clone_flags,
-					unsigned long stack_start,
-					unsigned long stack_size,
-					int __user *child_tidptr,
-					struct pid *pid,
-					int trace)
+struct task_struct *copy_process(unsigned long clone_flags,
+				 unsigned long stack_start,
+				 unsigned long stack_size,
+				 int __user *child_tidptr,
+				 struct pid *pid,
+				 int trace)
 {
 	int retval;
 	struct task_struct *p;
@@ -1293,7 +1321,13 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	 * thread groups also imply shared VM. Blocking this case allows
 	 * for various simplifications in other code.
 	 */
-	if ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))
+	if ((clone_flags & CLONE_SIGHAND) &&
+#ifdef CONFIG_LIVEDUMP
+	    /* Livedumping clones uses CLONE_THREAD | CLONE_SIGHAND and
+	       not CLONE_VM since VM is handled specially for them. */
+	    !(clone_flags & CLONE_LIVEDUMP) &&
+#endif
+	    !(clone_flags & CLONE_VM))
 		return ERR_PTR(-EINVAL);
 
 	/*
@@ -1360,6 +1394,9 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	delayacct_tsk_init(p);	/* Must remain after dup_task_struct() */
 	p->flags &= ~(PF_SUPERPRIV | PF_WQ_WORKER);
 	p->flags |= PF_FORKNOEXEC;
+#ifdef CONFIG_LIVEDUMP
+	p->extra_flags &= ~PFE_LIVEDUMP;
+#endif
 	INIT_LIST_HEAD(&p->children);
 	INIT_LIST_HEAD(&p->sibling);
 	rcu_copy_process(p);
@@ -1485,7 +1522,12 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	if (retval)
 		goto bad_fork_cleanup_io;
 
-	if (pid != &init_struct_pid) {
+	if (pid != &init_struct_pid
+#ifdef CONFIG_LIVEDUMP
+	    /* Livedumped clones always uses the same PID. */
+	    && !(clone_flags & CLONE_LIVEDUMP)
+#endif
+	    ) {
 		pid = alloc_pid(p->nsproxy->pid_ns_for_children);
 		if (IS_ERR(pid)) {
 			retval = PTR_ERR(pid);
@@ -1526,6 +1568,9 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 #endif
 	clear_all_latency_tracing(p);
 
+#ifdef CONFIG_LIVEDUMP
+	p->dump = NULL;
+#endif
 	/* ok, now we should be set up.. */
 	p->pid = pid_nr(pid);
 	if (clone_flags & CLONE_THREAD) {
@@ -1608,6 +1653,17 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 			attach_pid(p, PIDTYPE_PGID);
 			attach_pid(p, PIDTYPE_SID);
 			__this_cpu_inc(process_counts);
+#ifdef CONFIG_LIVEDUMP
+		} else if ((clone_flags & (CLONE_LIVEDUMP | CLONE_THREAD)) ==
+			   (CLONE_LIVEDUMP | CLONE_THREAD)) {
+			/* This is a thread cloned for livedumping. */
+			BUG_ON(!current->dump);
+			if (current->dump->leader)
+				p->group_leader = current->dump->leader;
+			list_add_tail_rcu(&p->thread_group,
+					  &p->group_leader->thread_group);
+			p->dump = get_dump(current->dump);
+#endif
 		} else {
 			current->signal->nr_threads++;
 			atomic_inc(&current->signal->live);
@@ -1620,7 +1676,19 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		attach_pid(p, PIDTYPE_PID);
 		nr_threads++;
 	}
-
+#ifdef CONFIG_LIVEDUMP
+	if (unlikely(in_livedump(current, COPY_THREADS)) &&
+	    !(clone_flags & CLONE_LIVEDUMP) && (clone_flags & CLONE_THREAD)) {
+		/*
+		 * This thread is currently being livedumped and the
+		 * dumping process is in COPY_THREADS stage. Make sure
+		 * the new thread is livedumped, too.
+		 */
+		atomic_inc(&p->dump->nr_clone_remains);
+		p->dump = get_dump(current->dump);
+		livedump_request(p);
+	}
+#endif
 	total_forks++;
 	spin_unlock(&current->sighand->siglock);
 	syscall_tracepoint_update(p);
diff --git a/kernel/livedump.c b/kernel/livedump.c
new file mode 100644
index 000000000000..8244636a8c51
--- /dev/null
+++ b/kernel/livedump.c
@@ -0,0 +1,308 @@
+/*
+ * kernel/livedump.c
+ *
+ * Core of live application dump.
+ *
+ * Actual dumping is initiated by sending SIGKILL to the thread
+ * group leader, which is done by ptrace(PT_LIVEDUMP, ...). The
+ * rest is signal-driven too, and performed by appropriate calls
+ * from get_signal_to_deliver().
+ *
+ * The various interactions are rather convoluted, but here's the big
+ * picture.  In the following, O is the requesting process, T is the
+ * task leader being cloned, t is a thread being cloned, C is the clone
+ * leader and c is a clone thread.
+ *
+ *
+ * O: allocate dump
+ * O: T->dump = dump
+ * O: signal(T)
+ * O: wait for dump_ready
+ * T: C = clone(T)
+ * T: for each (t) t->dump = dump, signal(t)
+ * T: wait for mm cloned
+ * t: c = clone(c)
+ * t: if (I am last clone) signal(C)
+ * t: wait for mm cloned
+ * C: C->mm = clone mm
+ * C: wake O with dump_ready
+ * O: set mm cloned
+ * O: wait for dump_complete
+ * C: for each (c) c->mm = cloned mm
+ * C: dump core
+ * C: wake O with dump_complete
+ * C: exit
+ */
+
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/signal.h>
+#include <linux/file.h>
+#include <linux/err.h>
+#include <linux/binfmts.h>
+#include <linux/mutex.h>
+#include <linux/livedump.h>
+#include <linux/ioprio.h>
+#include <linux/coredump.h>
+
+#include <asm/mmu_context.h>
+#include <asm/processor.h>
+
+int livedump_take(struct livedump_context *dump)
+{
+	int status;
+
+	/* Initialize the rest of livedump context. */
+	livedump_set_status(dump, 0);
+	livedump_stage(dump, COPY_LEADER);
+	init_completion(&dump->dump_complete);
+	init_completion(&dump->dump_ready);
+
+	/* Kref is 1, so don't use get_dump(). */
+	dump->origin->dump = dump;
+	/* Signal the group leader to clone, thus starting the dump. */
+	livedump_request(dump->origin);
+
+	wait_for_completion(&dump->dump_ready);
+	status = livedump_status(dump);
+	if (unlikely(status)) {
+		/* There was error cloning leader. */
+		if (!dump->leader) {
+			dump->origin->dump = NULL;
+			put_dump(dump);
+		}
+		return status;
+	}
+
+	wait_for_completion(&dump->dump_complete);
+	return livedump_status(dump);
+}
+
+/*
+ * Copy current task_struct, make clone suitable for dumping.
+ */
+static struct task_struct *livedump_clone(struct livedump_param *param,
+					  unsigned long clone_flags)
+{
+	/* We use the same pid as the cloned thread. */
+	struct pid *pid = get_task_pid(current, PIDTYPE_PID);
+	struct pt_regs regs;
+	struct task_struct *clone;
+	int err, prio;
+
+	regs = *task_pt_regs(current);
+	clone = copy_process(CLONE_LIVEDUMP | clone_flags,
+			     KSTK_ESP(current), 0, NULL, pid, 0);
+	if (unlikely(IS_ERR(clone))) {
+		put_pid(pid);
+		return clone;
+	}
+
+	__set_task_state(clone, TASK_STOPPED);
+
+	/* FIXME - do we need this on every thread or just the leader? */
+	set_user_nice(clone, param->sched_nice);
+
+	/* Clone will have inherited I/O scheduling class, but new priority. */
+	prio = get_task_ioprio(clone);
+	if (prio < 0) {
+		put_pid(pid);
+		return ERR_PTR(prio);
+	}
+	err = set_task_ioprio(clone, IOPRIO_PRIO_VALUE
+			      (IOPRIO_PRIO_CLASS(prio), param->io_prio));
+	if (err) {
+		put_pid(pid);
+		return ERR_PTR(err);
+	}
+
+	clone->signal->oom_score_adj = param->oom_adj;
+
+	/* This is for the leader only, as sys_setrlimit() does. */
+	if (!(clone_flags & CLONE_THREAD) && param->core_limit) {
+		clone->signal->rlim[RLIMIT_CORE].rlim_cur = param->core_limit;
+		clone->signal->rlim[RLIMIT_CORE].rlim_max = param->core_limit;
+	}
+
+	return clone;
+}
+
+/*
+ * Clone the thread group leader. If we have more than one
+ * thread in this group, ask other threads to clone themselves.
+ * Otherwise, asking the leader's clone to perform the dump.
+ */
+void livedump_clone_leader(void)
+{
+	struct livedump_context *dump;
+	struct task_struct *p, *clone;
+	int nr_dump_threads = 0;
+
+	dump = current->dump;
+	BUG_ON(!dump);
+
+	clone = livedump_clone(&dump->param, CLONE_PARENT);
+	if (unlikely(IS_ERR(clone))) {
+		livedump_set_status(dump, PTR_ERR(clone));
+		complete_all(&dump->dump_ready);
+		return;
+	}
+
+	clone->dump = get_dump(dump);
+
+	/* siglock protects the thread list. */
+	spin_lock_irq(&current->sighand->siglock);
+	for (p = next_thread(current); p != current; p = next_thread(p))
+		nr_dump_threads++;
+
+	if (nr_dump_threads > 0) {
+		atomic_set(&dump->nr_clone_remains, nr_dump_threads);
+		livedump_stage(dump, COPY_THREADS);
+		dump->leader = clone;
+		for (p = next_thread(current); p != current;
+		     p = next_thread(p)) {
+			p->dump = get_dump(dump);
+			__livedump_request(p);
+		}
+	} else {
+		livedump_stage(dump, PERFORM_DUMP);
+		/* Note: locking validator may warn about recursive locking
+		   since we're asking for the nested lock clone->sighand->siglock,
+		   which is of the same class as current->sighand->siglock. */
+		livedump_request(clone);
+	}
+	spin_unlock_irq(&current->sighand->siglock);
+	livedump_wait(dump);
+}
+
+/*
+ * Clone one thread, force the new thread group leader
+ * to perform the dump if we're the last thread cloned.
+ */
+void livedump_clone_thread(void)
+{
+	struct livedump_context *dump;
+	struct task_struct *clone;
+
+	dump = current->dump;
+	BUG_ON(!dump);
+
+	/*
+	 * For non-leader threads, make sure we clone the signal stuff
+	 * from the cloned leader thread.  Important to keep the thread
+	 * list signal lock the same for all threads.
+	 */
+	clone = livedump_clone(&dump->param, CLONE_THREAD | CLONE_SIGHAND);
+	if (unlikely(IS_ERR(clone))) {
+		/*
+		 * Save first error we've encountered. Other threads
+		 * may have failed to clone themselves too, but their
+		 * errors will be discarded.
+		 */
+		if (!livedump_status(dump))
+			livedump_set_status(dump, PTR_ERR(clone));
+	}
+	if (atomic_dec_and_test(&dump->nr_clone_remains)) {
+		/* All threads are cloned, or at least have tried to clone. */
+		livedump_stage(dump, PERFORM_DUMP);
+		livedump_request(dump->leader);
+	}
+	livedump_wait(dump);
+}
+
+/*
+ * Copy the mm from the original thread group to the clone group.
+ */
+static long livedump_copy_context(void)
+{
+	struct mm_struct *newmm, *oldmm;
+	struct task_struct *p = current;
+	struct livedump_context *dump = p->dump;
+
+	BUG_ON(!dump);
+	BUG_ON(!dump->origin);
+	BUG_ON(p->mm != p->active_mm);
+
+	/*
+	 * All original threads are blocked on dump->dump_ready, so it
+	 * should be safe to duplicate mm and other important things
+	 * from them.
+	 */
+	newmm = dup_mm(dump->origin);
+	if (unlikely(!newmm))
+		return -ENOMEM;
+
+	/* MM is copied, we can now let the original threads go. */
+	complete_all(&dump->dump_ready);
+
+	/* FIXME - refcounts on oldmm? */
+	oldmm = p->active_mm;
+	p->mm = p->active_mm = newmm;
+	preempt_disable();
+	activate_mm(oldmm, newmm);
+	preempt_enable();
+	mmput(oldmm);
+
+	for (p = next_thread(p); p != current; p = next_thread(p)) {
+		mmput(p->mm);
+		p->mm = p->active_mm = newmm;
+		atomic_inc(&newmm->mm_users);
+#ifdef CONFIG_PREEMPT
+		/*
+		 * Restore original preempt count just to allow
+		 * correct exit for this clone.
+		 */
+		init_task_preempt_count(p);
+#endif
+	}
+	return 0;
+}
+
+/*
+ * Complete the dump. Core file is dumped only if dump->status is 0
+ * before dumping, i.e. there was no errors on the previous stages.
+ */
+void livedump_perform_dump(siginfo_t *info)
+{
+	struct livedump_context *dump = current->dump;
+	sigset_t blocked;
+	long status;
+
+	BUG_ON(!dump);
+	current->flags |= PF_SIGNALED;
+
+	status = livedump_status(dump);
+	if (!status)
+		/* There was no errors. Try to copy mm and other
+		   substantial stuff, and allow original threads
+		   to run in case of success. */
+		status = livedump_copy_context();
+	if (status) {
+		/* There was some error, while copying mm or earlier.
+		   Allow original threads to run, but do not dump core. */
+		complete_all(&dump->dump_ready);
+		goto exiting;
+	}
+
+	/*
+	 * Everything looks valid, may dump core. All signals are
+	 * blocked to avoid magic -ERESTARTSYS errors comes due to I/O
+	 * via NFS.
+	 */
+	sigfillset(&blocked);
+	sigprocmask(SIG_BLOCK, &blocked, NULL);
+	flush_signals(current);
+
+	status = do_coredump(info);
+
+exiting:
+	livedump_set_status(dump, status);
+	complete_all(&dump->dump_complete);
+	do_group_exit(SIGKILL);
+	/* NOTREACHED */
+}
+
+void livedump_ref_done(struct kref *ref)
+{
+	kfree(container_of(ref, struct livedump_context, ref));
+}
diff --git a/kernel/pid.c b/kernel/pid.c
index 4fd07d5b7baf..e3c3629313cc 100644
--- a/kernel/pid.c
+++ b/kernel/pid.c
@@ -389,7 +389,18 @@ EXPORT_SYMBOL_GPL(find_vpid);
 void attach_pid(struct task_struct *task, enum pid_type type)
 {
 	struct pid_link *link = &task->pids[type];
-	hlist_add_head_rcu(&link->node, &link->pid->tasks[type]);
+#ifdef CONFIG_LIVEDUMP
+	struct hlist_node *first;
+
+	/* Livedumped clone and it's origin shares pid. Since clone
+	   arrives later, adding it after origin prevents pid_task()
+	   from returning pointer to clone instead of origin. */
+	if ((type == PIDTYPE_PID) &&
+	    (first = rcu_dereference(link->pid->tasks[type].first)))
+		hlist_add_behind_rcu(&link->node, first);
+	else
+#endif
+		hlist_add_head_rcu(&link->node, &link->pid->tasks[type]);
 }
 
 static void __change_pid(struct task_struct *task, enum pid_type type,
diff --git a/kernel/ptrace.c b/kernel/ptrace.c
index d06f90d0ce43..0ea74ffb2cd9 100644
--- a/kernel/ptrace.c
+++ b/kernel/ptrace.c
@@ -26,6 +26,10 @@
 #include <linux/hw_breakpoint.h>
 #include <linux/cn_proc.h>
 #include <linux/compat.h>
+#include <linux/livedump.h>
+#include <linux/kthread.h>
+#include <linux/ioprio.h>
+#include <linux/oom.h>
 
 
 /*
@@ -1047,6 +1051,147 @@ static struct task_struct *ptrace_get_task_struct(pid_t pid)
 #define arch_ptrace_attach(child)	do { } while (0)
 #endif
 
+#ifdef CONFIG_LIVEDUMP
+static int livedump_dumper_thread(void *arg)
+{
+	struct livedump_context *dump = arg;
+
+	/* Block all signals just to be safe. */
+	spin_lock_irq(&current->sighand->siglock);
+	sigfillset(&current->blocked);
+	recalc_sigpending();
+	spin_unlock_irq(&current->sighand->siglock);
+	livedump_take(dump);
+	complete_and_exit(&dump->thread_exit, 0);
+}
+
+long ptrace_livedump(struct task_struct *tsk,
+		     struct livedump_param __user *param)
+{
+	int status = 0;
+	struct livedump_context *dump;
+	struct task_struct *leader;
+
+	read_lock(&tasklist_lock);
+	leader = tsk->group_leader;
+	BUG_ON(!leader);
+	get_task_struct(leader);
+	read_unlock(&tasklist_lock);
+
+	/* FIXME - better security check here ? */
+	if (!ptrace_check_attach(leader, 0)) {
+		status = -EPERM;
+		goto exit;
+	}
+
+	/*
+	 * Do not dump the task being ptraced, kernel thread,
+	 * task which is a clone created for live dumping,
+	 * or task which is exiting or handling fatal signal.
+	 */
+	task_lock(leader);
+	if ((leader->ptrace & PT_PTRACED) ||
+            (!leader->mm) ||
+            unlikely(leader->dump) ||
+            (leader->flags & PF_SIGNALED) ||
+            (leader->signal->flags & SIGNAL_GROUP_EXIT))
+                status = -EINVAL;
+	else if (leader->extra_flags & PFE_LIVEDUMP)
+		status = -EINPROGRESS;
+	else
+		leader->extra_flags |= PFE_LIVEDUMP;
+	task_unlock(leader);
+	if (status)
+		goto exit;
+
+	dump = kmalloc(sizeof *dump, GFP_KERNEL);
+	if (unlikely(!dump)) {
+		status = -ENOMEM;
+		goto exit_flag;
+	}
+
+	if (param) {
+		if (copy_from_user(&dump->param, param, sizeof(dump->param)))
+			status = -EFAULT;
+		else if (dump->param.sched_nice < -20 ||
+			 dump->param.sched_nice > 19 ||
+			 dump->param.io_prio < 0 ||
+			 dump->param.io_prio >= IOPRIO_BE_NR ||
+			 dump->param.oom_adj < OOM_DISABLE ||
+			 dump->param.oom_adj > OOM_ADJUST_MAX ||
+			 dump->param.core_limit > RLIM_INFINITY)
+			status = -EINVAL;
+		else if (((dump->param.sched_nice < 0) && !capable(CAP_SYS_NICE)) ||
+			 (dump->param.core_limit && !capable(CAP_SYS_RESOURCE)))
+			status = -EPERM;
+	} else {
+		/* Make it ultra-low priority by default. */
+		dump->param.sched_nice = 19;
+		dump->param.io_prio = 7;
+		dump->param.oom_adj = OOM_ADJUST_MAX;
+		/* Zero means using default (inherited) value. */
+		dump->param.core_limit = 0;
+	}
+
+	if (unlikely(status)) {
+		kfree(dump);
+		goto exit_flag;
+	}
+	kref_init(&dump->ref);
+	dump->origin = leader;
+	dump->leader = NULL;
+
+	/* We may start the dump. */
+	if (unlikely(current->group_leader == leader)) {
+		struct task_struct *dumper;
+
+		/*
+		 * Current is a member of the thread group lead by the
+		 * dumped task.  Since the task can't dump itself nor
+		 * it's leader, a special kernel thread (dumper) will do
+		 * it.
+		 */
+		init_completion(&dump->thread_exit);
+		dumper = kthread_run(livedump_dumper_thread, dump, "dump_%s",
+				     current->comm);
+		if (IS_ERR(dumper)) {
+			status = PTR_ERR(dumper);
+		} else {
+			sigset_t set;
+
+			livedump_block_signals(&set);
+			do {
+				struct ksignal ks;
+				/*
+				 * Wait for dumper thread. This wait
+				 * will be interrupted by the SIGKILL
+				 * sent by dumper.
+				 */
+				if (!wait_for_completion_interruptible(&dump->thread_exit))
+					break;
+				/*
+				 * Handling signal gives dumping stuff
+				 * a chance to do it's job.
+				 */
+				get_signal(&ks);
+			} while (1);
+			status = livedump_status(dump);
+			livedump_unblock_signals(&set);
+		}
+	} else
+		/*
+		 * When current and leader are not in the same
+		 * thread group, things are much easier...
+		 */
+		status = livedump_take(dump);
+exit_flag:
+	leader->extra_flags &= ~PFE_LIVEDUMP;
+exit:
+	put_task_struct(leader);
+	return status;
+}
+#endif /* CONFIG_LIVEDUMP */
+
 SYSCALL_DEFINE4(ptrace, long, request, long, pid, unsigned long, addr,
 		unsigned long, data)
 {
@@ -1066,6 +1211,16 @@ SYSCALL_DEFINE4(ptrace, long, request, long, pid, unsigned long, addr,
 		goto out;
 	}
 
+	if (request == PTRACE_LIVEDUMP) {
+#ifdef CONFIG_LIVEDUMP
+		ret = ptrace_livedump
+		  (child, (struct livedump_param __user *)data);
+#else
+		ret = -ENOSYS;
+#endif
+		goto out_put_task_struct;
+        }
+
 	if (request == PTRACE_ATTACH || request == PTRACE_SEIZE) {
 		ret = ptrace_attach(child, request, addr, data);
 		/*
@@ -1115,6 +1270,41 @@ int generic_ptrace_pokedata(struct task_struct *tsk, unsigned long addr,
 
 #if defined CONFIG_COMPAT
 
+#ifdef CONFIG_LIVEDUMP
+static inline long
+copy_livedump_param_from_user32(struct livedump_param *to,
+				struct compat_livedump_param __user  *from)
+{
+	long err;
+
+	if (!access_ok (VERIFY_READ, from, sizeof(struct compat_livedump_param)))
+		return -EFAULT;
+
+	err = __get_user(to->sched_nice, &from->sched_nice);
+	err |= __get_user(to->io_prio, &from->io_prio);
+	err |= __get_user(to->oom_adj, &from->oom_adj);
+	err |= __get_user(to->core_limit, &from->core_limit);
+
+	return err;
+}
+
+long compat_ptrace_livedump(struct task_struct *tsk,
+			    struct compat_livedump_param __user *cparam)
+{
+	struct livedump_param __user tmp, *param;
+
+	if (cparam) {
+		param = compat_alloc_user_space(sizeof(struct livedump_param));
+		if (copy_livedump_param_from_user32(&tmp, cparam))
+			return -EFAULT;
+		if (copy_to_user(param, &tmp, sizeof(struct livedump_param)))
+			return -EFAULT;
+	} else
+		param = NULL;
+	return ptrace_livedump(tsk, param);
+}
+#endif	/* CONFIG_LIVEDUMP */
+
 int compat_ptrace_request(struct task_struct *child, compat_long_t request,
 			  compat_ulong_t addr, compat_ulong_t data)
 {
@@ -1210,6 +1400,21 @@ COMPAT_SYSCALL_DEFINE4(ptrace, compat_long_t, request, compat_long_t, pid,
 		goto out;
 	}
 
+	if (request == PTRACE_LIVEDUMP) {
+#ifdef CONFIG_LIVEDUMP
+		struct compat_livedump_param __user * datap;
+
+		datap = compat_ptr(data);
+		if (datap)
+			ret = compat_ptrace_livedump(child, datap);
+		else
+			ret = ptrace_livedump(child, NULL);
+#else
+		ret = -ENOSYS;
+#endif
+		goto out_put_task_struct;
+        }
+
 	if (request == PTRACE_ATTACH || request == PTRACE_SEIZE) {
 		ret = ptrace_attach(child, request, addr, data);
 		/*
diff --git a/kernel/signal.c b/kernel/signal.c
index 1336e4c016ba..56fc16e1d994 100644
--- a/kernel/signal.c
+++ b/kernel/signal.c
@@ -38,6 +38,7 @@
 
 #define CREATE_TRACE_POINTS
 #include <trace/events/signal.h>
+#include <linux/livedump.h>
 
 #include <asm/param.h>
 #include <asm/uaccess.h>
@@ -1105,6 +1106,18 @@ static int __send_signal(int sig, struct siginfo *info, struct task_struct *t,
 
 	assert_spin_locked(&t->sighand->siglock);
 
+#ifdef CONFIG_LIVEDUMP
+	if (unlikely(t->group_leader->extra_flags & PFE_LIVEDUMP)) {
+		/* We're doing live dump, do not receive alien signals. */
+		if (sig != SIGKILL)
+			return 0;
+		if (!thread_group_leader(t) && !in_livedump(t, COPY_THREADS))
+			/* For the thread, we're expecting SIGKILL from
+			   cloning group leader. This is not the one. */
+			return 0;
+	}
+#endif
+
 	result = TRACE_SIGNAL_IGNORED;
 	if (!prepare_signal(sig, t,
 			from_ancestor_ns || (info == SEND_SIG_FORCED)))
@@ -1193,6 +1206,12 @@ static int __send_signal(int sig, struct siginfo *info, struct task_struct *t,
 out_set:
 	signalfd_notify(t, sig);
 	sigaddset(&pending->signal, sig);
+#ifdef CONFIG_LIVEDUMP
+	if (unlikely(t->dump)) {
+		signal_wake_up(t, 1);
+		goto ret;
+	}
+#endif
 	complete_signal(sig, t, group);
 ret:
 	trace_signal_generate(sig, info, t, group, result);
@@ -1251,7 +1270,7 @@ __group_send_sig_info(int sig, struct siginfo *info, struct task_struct *p)
 	return send_signal(sig, info, p, 1);
 }
 
-static int
+int
 specific_send_sig_info(int sig, struct siginfo *info, struct task_struct *t)
 {
 	return send_signal(sig, info, t, 0);
@@ -2360,6 +2379,26 @@ relock:
 				continue;
 		}
 
+#ifdef CONFIG_LIVEDUMP
+		if (signr == SIGKILL) {
+			if (in_livedump(current, COPY_LEADER)) {
+				spin_unlock_irq(&sighand->siglock);
+				livedump_clone_leader();
+				signr = 0;
+				goto exiting;
+			} else if (in_livedump(current, COPY_THREADS)) {
+				spin_unlock_irq(&sighand->siglock);
+				livedump_clone_thread();
+				signr = 0;
+				goto exiting;
+			} else if (in_livedump(current, PERFORM_DUMP) &&
+				   thread_group_leader(current)) {
+				spin_unlock_irq(&sighand->siglock);
+				livedump_perform_dump(&ksig->info);
+			}
+		}
+#endif /* CONFIG_LIVEDUMP */
+
 		ka = &sighand->action[signr-1];
 
 		/* Trace actually delivered signals. */
@@ -2461,6 +2500,9 @@ relock:
 	}
 	spin_unlock_irq(&sighand->siglock);
 
+#ifdef CONFIG_LIVEDUMP
+ exiting:
+#endif
 	ksig->sig = signr;
 	return ksig->sig > 0;
 }
-- 
2.5.0

