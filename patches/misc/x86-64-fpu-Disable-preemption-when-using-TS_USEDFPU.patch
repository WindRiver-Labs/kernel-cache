From 407ecfc927b2e65dca5aa191d21a35df68343449 Mon Sep 17 00:00:00 2001
From: Hu Yadi <Yadi.hu@windriver.com>
Date: Thu, 19 Dec 2013 15:28:13 +0800
Subject: [PATCH] x86-64, fpu: Disable preemption when using TS_USEDFPU

commit a4d4fbc7735bba6654b20f859135f9d3f8fe7f76 upstream

    Consolidates code and fixes the below race for 64-bit.

        commit 9fa2f37bfeb798728241cc4a19578ce6e4258f25
        Author: torvalds <torvalds>
        Date: Tue Sep 2 07:37:25 2003 +0000

        Be a lot more careful about TS_USEDFPU and preemption

        We had some races where we testecd (or set) TS_USEDFPU
        together with sequences that depended on the setting
        (like clearing or setting the TS flag in %cr0) and we
        could be preempted in between, which screws up the FPU state,
        since preemption will itself change USEDFPU and the TS flag.
        This makes it a lot more explicit: the "internal" low-level
        FPU functions ("__xxxx_fpu()") all require preemption to be
        disabled, and the exported "real" functions will make sure
        that is the case. One case - in __switch_to() - was switched
        to the non-preempt-safe internal version, since the scheduler
        itself has already disabled preemption.

    Signed-off-by: Brian Gerst <brgerst@gmail.com>
    Acked-by: Pekka Enberg <penberg@kernel.org>
    Cc: Suresh Siddha <suresh.b.siddha@intel.com>
    LKML-Reference: <1283563039-3466-6-git-send-email-brgerst@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Signed-off-by: Hu Yadi <Yadi.hu@windriver.com>
---
 arch/x86/include/asm/i387.h  |   14 --------------
 arch/x86/kernel/process_64.c |    4 ++--
 2 files changed, 2 insertions(+), 16 deletions(-)

diff --git a/arch/x86/include/asm/i387.h b/arch/x86/include/asm/i387.h
index 55ecaef..4cb3237 100644
--- a/arch/x86/include/asm/i387.h
+++ b/arch/x86/include/asm/i387.h
@@ -365,19 +365,6 @@ static inline void irq_ts_restore(int TS_state)
 		stts();
 }
 
-#ifdef CONFIG_X86_64
-
-static inline void save_init_fpu(struct task_struct *tsk)
-{
-	__save_init_fpu(tsk);
-	stts();
-}
-
-#define unlazy_fpu	__unlazy_fpu
-#define clear_fpu	__clear_fpu
-
-#else  /* CONFIG_X86_32 */
-
 /*
  * These disable preemption on their own and are safe
  */
@@ -403,7 +390,6 @@ static inline void clear_fpu(struct task_struct *tsk)
 	preempt_enable();
 }
 
-#endif	/* CONFIG_X86_64 */
 
 /*
  * i387 state interaction
diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index e01fcc0..ecfbafa 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -426,8 +426,8 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	load_TLS(next, cpu);
 
 	/* Must be after DS reload */
-	unlazy_fpu(prev_p);
-
+	//unlazy_fpu(prev_p);
+        __unlazy_fpu(prev_p);
 	/* Make sure cpu is ready for new context */
 	if (preload_fpu)
 		clts();
-- 
1.7.0

