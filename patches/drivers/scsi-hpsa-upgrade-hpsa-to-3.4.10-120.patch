From afe40c2066efe652f546d503be62b6e8166f9651 Mon Sep 17 00:00:00 2001
From: chunguang yang <chunguang.yang@windriver.com>
Date: Tue, 20 Oct 2015 22:27:43 -0400
Subject: [PATCH 1/5] scsi/hpsa: upgrade hpsa to 3.4.10-120

source http://sourceforge.net/projects/cciss/files/hpsa-3.0-tarballs/hpsa-3.4.10-120.tar.bz2

HP needs the latest driver for HP Smart Array, but it is not merged to
upstream yet. We use one commit to upgrade the whole driver, so that we
can easily revert it when upstream integrate it, and grab upsteam's
commits.

This driver will be verified by HP during the certification process.

Signed-off-by: chunguang yang <chunguang.yang@windriver.com>
---
 drivers/scsi/hpsa.c               | 4938 ++++++++++++++++++++++++-------------
 drivers/scsi/hpsa.h               |   70 +-
 drivers/scsi/hpsa_cmd.h           |  375 +--
 drivers/scsi/hpsa_kernel_compat.h |  738 ++++++
 4 files changed, 4245 insertions(+), 1876 deletions(-)
 create mode 100644 drivers/scsi/hpsa_kernel_compat.h

diff --git a/drivers/scsi/hpsa.c b/drivers/scsi/hpsa.c
index 8eab107..ebe6d47 100644
--- a/drivers/scsi/hpsa.c
+++ b/drivers/scsi/hpsa.c
@@ -1,6 +1,7 @@
 /*
  *    Disk Array driver for HP Smart Array SAS controllers
- *    Copyright 2000, 2014 Hewlett-Packard Development Company, L.P.
+ *    Copyright 2014-2015 PMC-Sierra, Inc.
+ *    Copyright 2000,2009-2015 Hewlett-Packard Development Company, L.P.
  *
  *    This program is free software; you can redistribute it and/or modify
  *    it under the terms of the GNU General Public License as published by
@@ -15,15 +16,15 @@
  *    along with this program; if not, write to the Free Software
  *    Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
  *
- *    Questions/Comments/Bugfixes to iss_storagedev@hp.com
+ *    Questions/Comments/Bugfixes to storagedev@pmcs.com
  *
  */
 
 #include <linux/module.h>
+#include <linux/reboot.h>
 #include <linux/interrupt.h>
 #include <linux/types.h>
 #include <linux/pci.h>
-#include <linux/pci-aspm.h>
 #include <linux/kernel.h>
 #include <linux/slab.h>
 #include <linux/delay.h>
@@ -33,7 +34,6 @@
 #include <linux/spinlock.h>
 #include <linux/compat.h>
 #include <linux/blktrace_api.h>
-#include <linux/uaccess.h>
 #include <linux/io.h>
 #include <linux/dma-mapping.h>
 #include <linux/completion.h>
@@ -46,17 +46,17 @@
 #include <linux/cciss_ioctl.h>
 #include <linux/string.h>
 #include <linux/bitmap.h>
-#include <linux/atomic.h>
+#include <asm/atomic.h>
 #include <linux/jiffies.h>
-#include <linux/percpu-defs.h>
 #include <linux/percpu.h>
-#include <asm/unaligned.h>
+#include <linux/workqueue.h>
 #include <asm/div64.h>
 #include "hpsa_cmd.h"
 #include "hpsa.h"
+#include "hpsa_kernel_compat.h"
 
 /* HPSA_DRIVER_VERSION must be 3 byte values (0-255) separated by '.' */
-#define HPSA_DRIVER_VERSION "3.4.4-1"
+#define HPSA_DRIVER_VERSION "3.4.10-120"
 #define DRIVER_NAME "HP HPSA Driver (v " HPSA_DRIVER_VERSION ")"
 #define HPSA "hpsa"
 
@@ -64,8 +64,8 @@
 #define CLEAR_EVENT_WAIT_INTERVAL 20	/* ms for each msleep() call */
 #define MODE_CHANGE_WAIT_INTERVAL 10	/* ms for each msleep() call */
 #define MAX_CLEAR_EVENT_WAIT 30000	/* times 20 ms = 600 s */
-#define MAX_MODE_CHANGE_WAIT 2000	/* times 10 ms = 20 s */
-#define MAX_IOCTL_CONFIG_WAIT 1000
+#define MAX_MODE_CHANGE_WAIT 2000	/* times 10 ms = 2 s */
+#define MAX_IOCTL_CONFIG_WAIT 10000	/* times 10 ms = 10 s */
 
 /*define how many times we will try a command because of bus resets */
 #define MAX_CMD_RETRIES 3
@@ -73,7 +73,7 @@
 /* Embedded module documentation macros - see modules.h */
 MODULE_AUTHOR("Hewlett-Packard Company");
 MODULE_DESCRIPTION("Driver for HP Smart Array Controller version " \
-	HPSA_DRIVER_VERSION);
+	HPSA_DRIVER_VERSION " (d38/s216)");
 MODULE_SUPPORTED_DEVICE("HP Smart Array Controllers");
 MODULE_VERSION(HPSA_DRIVER_VERSION);
 MODULE_LICENSE("GPL");
@@ -84,19 +84,49 @@ MODULE_PARM_DESC(hpsa_allow_any,
 		"Allow hpsa driver to access unknown HP Smart Array hardware");
 static int hpsa_simple_mode;
 module_param(hpsa_simple_mode, int, S_IRUGO|S_IWUSR);
+static int hpsa_lockup_action = 0; /* 0 = disable device -- 1 = panic -- 2 = reboot */
 MODULE_PARM_DESC(hpsa_simple_mode,
 	"Use 'simple mode' rather than 'performant mode'");
+static int reply_queues = 4;
+module_param(reply_queues, int, S_IRUGO|S_IWUSR);
+MODULE_PARM_DESC(reply_queues,
+	"Specify desired number of reply queues. 1-16, default is 4, not to exceed number of online CPUs.");
+
+#undef PCI_DEVICE_ID_HP_CISSF
+#ifndef PCI_DEVICE_ID_HP_CISSF
+#define PCI_DEVICE_ID_HP_CISSF 0x323B
+#endif
+#undef PCI_DEVICE_ID_HP_CISSG
+#ifndef PCI_DEVICE_ID_HP_CISSG
+#define PCI_DEVICE_ID_HP_CISSG 0x333F
+#endif
+#undef PCI_DEVICE_ID_HP_CISSH
+#ifndef PCI_DEVICE_ID_HP_CISSH
+#define PCI_DEVICE_ID_HP_CISSH 0x323C
+#endif
+#undef PCI_DEVICE_ID_HP_CISSI
+#ifndef PCI_DEVICE_ID_HP_CISSI
+#define PCI_DEVICE_ID_HP_CISSI 0x3239
+#endif
+#ifndef PCI_VENDOR_ID_3PAR
+#define PCI_VENDOR_ID_3PAR 0x1590
+#endif
+#ifndef PCI_DEVICE_ID_3PAR
+#define PCI_DEVICE_ID_3PAR 0x0075
+#endif
 
 /* define the PCI info for the cards we can control */
 static const struct pci_device_id hpsa_pci_device_id[] = {
+#if SA_CONTROLLERS_GEN6
 	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSE,     0x103C, 0x3241},
 	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSE,     0x103C, 0x3243},
 	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSE,     0x103C, 0x3245},
 	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSE,     0x103C, 0x3247},
 	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSE,     0x103C, 0x3249},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSE,     0x103C, 0x324A},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSE,     0x103C, 0x324B},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSE,     0x103C, 0x3233},
+	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSE,     0x103C, 0x324a},
+	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSE,     0x103C, 0x324b},
+#endif
+#if SA_CONTROLLERS_GEN8
 	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSF,     0x103C, 0x3350},
 	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSF,     0x103C, 0x3351},
 	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSF,     0x103C, 0x3352},
@@ -104,38 +134,58 @@ static const struct pci_device_id hpsa_pci_device_id[] = {
 	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSF,     0x103C, 0x3354},
 	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSF,     0x103C, 0x3355},
 	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSF,     0x103C, 0x3356},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSH,     0x103C, 0x1921},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSH,     0x103C, 0x1922},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSH,     0x103C, 0x1923},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSH,     0x103C, 0x1924},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSH,     0x103C, 0x1926},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSH,     0x103C, 0x1928},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSH,     0x103C, 0x1929},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21BD},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21BE},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21BF},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21C0},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21C1},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21C2},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21C3},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21C4},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21C5},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21C6},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21C7},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21C8},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21C9},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21CA},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21CB},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21CC},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21CD},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21CE},
-	{PCI_VENDOR_ID_HP_3PAR, 0x0075, 0x1590, 0x0076},
-	{PCI_VENDOR_ID_HP_3PAR, 0x0075, 0x1590, 0x0087},
-	{PCI_VENDOR_ID_HP_3PAR, 0x0075, 0x1590, 0x007D},
-	{PCI_VENDOR_ID_HP_3PAR, 0x0075, 0x1590, 0x0088},
-	{PCI_VENDOR_ID_HP, 0x333f, 0x103c, 0x333f},
-	{PCI_VENDOR_ID_HP,     PCI_ANY_ID,	PCI_ANY_ID, PCI_ANY_ID,
-		PCI_CLASS_STORAGE_RAID << 8, 0xffff << 8, 0},
+#endif
+#if SA_CONTROLLERS_GEN8_2
+   {PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSF,     0x103C, 0x334D},
+#endif
+#if SA_CONTROLLERS_GEN8_5
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSH,      0x103c, 0x1920},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSH,      0x103c, 0x1921},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSH,      0x103c, 0x1922},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSH,      0x103c, 0x1923},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSH,      0x103c, 0x1924},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSH,      0x103c, 0x1925},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSH,      0x103c, 0x1926},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSH,      0x103c, 0x1928},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSH,      0x103c, 0x1929},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSH,      0x103c, 0x192A},
+#endif
+#if SA_CONTROLLERS_GEN9
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSI,      0x103c, 0x21bd},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSI,      0x103c, 0x21be},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSI,      0x103c, 0x21bf},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSI,      0x103c, 0x21c0},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSI,      0x103c, 0x21c1},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSI,      0x103c, 0x21c2},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSI,      0x103c, 0x21c3},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSI,      0x103c, 0x21c4},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSI,      0x103c, 0x21c5},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSI,      0x103c, 0x21c6},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSI,      0x103c, 0x21c7},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSI,      0x103c, 0x21c8},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSI,      0x103c, 0x21c9},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSI,      0x103c, 0x21ca},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSI,      0x103c, 0x21cb},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSI,      0x103c, 0x21cc},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSI,      0x103c, 0x21cd},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSI,      0x103c, 0x21ce},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSI,      0x103c, 0x2208},
+#endif
+
+#if PMC_CONTROLLERS_GEN1
+	{PCI_VENDOR_ID_ADAPTEC2,              0x0290,      0x9005, 0x0580},
+	{PCI_VENDOR_ID_ADAPTEC2,              0x0290,      0x9005, 0x0581},
+	{PCI_VENDOR_ID_ADAPTEC2,              0x0290,      0x9005, 0x0582},
+	{PCI_VENDOR_ID_ADAPTEC2,              0x0290,      0x9005, 0x0583},
+	{PCI_VENDOR_ID_ADAPTEC2,              0x0290,      0x9005, 0x0584},
+	{PCI_VENDOR_ID_ADAPTEC2,              0x0290,      0x9005, 0x0585},
+#endif
+
+	HPSA_STORAGEWORKS_1210m_PCI_IDS
+        {PCI_VENDOR_ID_HP,     PCI_ANY_ID,       PCI_ANY_ID, PCI_ANY_ID,
+                PCI_CLASS_STORAGE_RAID << 8, 0xffff << 8, 0},
+        {PCI_VENDOR_ID_COMPAQ,     PCI_ANY_ID,   PCI_ANY_ID, PCI_ANY_ID,
+                PCI_CLASS_STORAGE_RAID << 8, 0xffff << 8, 0},
 	{0,}
 };
 
@@ -146,6 +196,7 @@ MODULE_DEVICE_TABLE(pci, hpsa_pci_device_id);
  *  access = Address of the struct of function pointers
  */
 static struct board_type products[] = {
+#if SA_CONTROLLERS_GEN6
 	{0x3241103C, "Smart Array P212", &SA5_access},
 	{0x3243103C, "Smart Array P410", &SA5_access},
 	{0x3245103C, "Smart Array P410i", &SA5_access},
@@ -153,56 +204,76 @@ static struct board_type products[] = {
 	{0x3249103C, "Smart Array P812", &SA5_access},
 	{0x324A103C, "Smart Array P712m", &SA5_access},
 	{0x324B103C, "Smart Array P711m", &SA5_access},
-	{0x3233103C, "HP StorageWorks 1210m", &SA5_access}, /* alias of 333f */
+#endif
+#if SA_CONTROLLERS_GEN8
 	{0x3350103C, "Smart Array P222", &SA5_access},
 	{0x3351103C, "Smart Array P420", &SA5_access},
 	{0x3352103C, "Smart Array P421", &SA5_access},
 	{0x3353103C, "Smart Array P822", &SA5_access},
 	{0x3354103C, "Smart Array P420i", &SA5_access},
 	{0x3355103C, "Smart Array P220i", &SA5_access},
-	{0x3356103C, "Smart Array P721m", &SA5_access},
+	{0x3356103C, "Smart Array P721", &SA5_access},
+#endif
+#if SA_CONTROLLERS_GEN8_2
+	{0x334D103C, "Smart Array", &SA5_access},
+#endif
+#if SA_CONTROLLERS_GEN8_5
+	{0x1920103C, "Smart Array P430i", &SA5_access},
 	{0x1921103C, "Smart Array P830i", &SA5_access},
 	{0x1922103C, "Smart Array P430", &SA5_access},
 	{0x1923103C, "Smart Array P431", &SA5_access},
 	{0x1924103C, "Smart Array P830", &SA5_access},
+	{0x1925103C, "Smart Array P831", &SA5_access},
 	{0x1926103C, "Smart Array P731m", &SA5_access},
 	{0x1928103C, "Smart Array P230i", &SA5_access},
 	{0x1929103C, "Smart Array P530", &SA5_access},
-	{0x21BD103C, "Smart Array P244br", &SA5_access},
-	{0x21BE103C, "Smart Array P741m", &SA5_access},
-	{0x21BF103C, "Smart HBA H240ar", &SA5_access},
-	{0x21C0103C, "Smart Array P440ar", &SA5_access},
-	{0x21C1103C, "Smart Array P840ar", &SA5_access},
-	{0x21C2103C, "Smart Array P440", &SA5_access},
-	{0x21C3103C, "Smart Array P441", &SA5_access},
-	{0x21C4103C, "Smart Array", &SA5_access},
-	{0x21C5103C, "Smart Array P841", &SA5_access},
-	{0x21C6103C, "Smart HBA H244br", &SA5_access},
-	{0x21C7103C, "Smart HBA H240", &SA5_access},
-	{0x21C8103C, "Smart HBA H241", &SA5_access},
-	{0x21C9103C, "Smart Array", &SA5_access},
-	{0x21CA103C, "Smart Array P246br", &SA5_access},
-	{0x21CB103C, "Smart Array P840", &SA5_access},
-	{0x21CC103C, "Smart Array", &SA5_access},
-	{0x21CD103C, "Smart Array", &SA5_access},
-	{0x21CE103C, "Smart HBA", &SA5_access},
-	{0x00761590, "HP Storage P1224 Array Controller", &SA5_access},
-	{0x00871590, "HP Storage P1224e Array Controller", &SA5_access},
-	{0x007D1590, "HP Storage P1228 Array Controller", &SA5_access},
-	{0x00881590, "HP Storage P1228e Array Controller", &SA5_access},
-	{0x333f103c, "HP StorageWorks 1210m Array Controller", &SA5_access},
+	{0x192A103C, "Smart Array P531", &SA5_access},
+#endif
+#if SA_CONTROLLERS_GEN9
+	{0x21bd103C, "Smart Array P244br", &SA5_access},
+	{0x21be103C, "Smart Array P741m", &SA5_access},
+	{0x21bf103C, "Smart HBA H240ar", &SA5_access},
+	{0x21c0103C, "Smart Array P440ar", &SA5_access},
+	{0x21c1103C, "Smart Array P840ar", &SA5_access},
+	{0x21c2103C, "Smart Array P440", &SA5_access},
+	{0x21c3103C, "Smart Array P441", &SA5_access},
+	{0x21c4103C, "Smart Array", &SA5_access},
+	{0x21c5103C, "Smart Array P841", &SA5_access},
+	{0x21c6103C, "Smart HBA H244br", &SA5_access},
+	{0x21c7103C, "Smart HBA H240", &SA5_access},
+	{0x21c8103C, "Smart HBA H241", &SA5_access},
+	{0x21c9103C, "Smart Array", &SA5_access},
+	{0x21ca103C, "Smart Array P246br", &SA5_access},
+	{0x21cb103C, "Smart Array P840", &SA5_access},
+	{0x21cc103C, "Smart Array P542D", &SA5_access},
+	{0x21cd103C, "Smart Array P240nr", &SA5_access},
+	{0x21ce103C, "Smart HBA H240nr", &SA5_access},
+	{0x2208103C, "Smart Array", &SA5_access},
+#endif
+#if PMC_CONTROLLERS_GEN1
+	{0x05809005, "SmartHBA-SA", &SA5_access},
+	{0x05819005, "SmartHBA-SA 8i", &SA5_access},
+	{0x05829005, "SmartHBA-SA 8i8e", &SA5_access},
+	{0x05839005, "SmartHBA-SA 8e", &SA5_access},
+	{0x05849005, "SmartHBA-SA 16i", &SA5_access},
+	{0x05859005, "SmartHBA-SA 4i4e", &SA5_access},
+#endif
+	HPSA_STORAGEWORKS_1210m_PRODUCT_ENTRIES
 	{0xFFFF103C, "Unknown Smart Array", &SA5_access},
 };
 
+#define SCSI_CMD_BUSY ((struct scsi_cmnd *)&hpsa_cmd_busy)
+static const struct scsi_cmnd hpsa_cmd_busy;
+#define SCSI_CMD_IDLE ((struct scsi_cmnd *)&hpsa_cmd_idle)
+static const struct scsi_cmnd hpsa_cmd_idle;
 static int number_of_controllers;
 
-static irqreturn_t do_hpsa_intr_intx(int irq, void *dev_id);
-static irqreturn_t do_hpsa_intr_msi(int irq, void *dev_id);
-static int hpsa_ioctl(struct scsi_device *dev, int cmd, void __user *arg);
+DECLARE_INTERRUPT_HANDLER(do_hpsa_intr_intx);
+DECLARE_INTERRUPT_HANDLER(do_hpsa_intr_msi);
+static int hpsa_ioctl(struct scsi_device *dev, int cmd, void *arg);
 
 #ifdef CONFIG_COMPAT
-static int hpsa_compat_ioctl(struct scsi_device *dev, int cmd,
-	void __user *arg);
+static int hpsa_compat_ioctl(struct scsi_device *dev, int cmd, void *arg);
 #endif
 
 static void cmd_free(struct ctlr_info *h, struct CommandList *c);
@@ -210,18 +281,19 @@ static struct CommandList *cmd_alloc(struct ctlr_info *h);
 static int fill_cmd(struct CommandList *c, u8 cmd, struct ctlr_info *h,
 	void *buff, size_t size, u16 page_code, unsigned char *scsi3addr,
 	int cmd_type);
-static void hpsa_free_cmd_pool(struct ctlr_info *h);
 #define VPD_PAGE (1 << 8)
 
-static int hpsa_scsi_queue_command(struct Scsi_Host *h, struct scsi_cmnd *cmd);
+DECLARE_QUEUECOMMAND(hpsa_scsi_queue_command);
 static void hpsa_scan_start(struct Scsi_Host *);
 static int hpsa_scan_finished(struct Scsi_Host *sh,
 	unsigned long elapsed_time);
-static int hpsa_change_queue_depth(struct scsi_device *sdev, int qdepth);
+DECLARE_CHANGE_QUEUE_DEPTH(hpsa_change_queue_depth);
+static int hpsa_change_queue_type(struct scsi_device *sdev, int type);
 
 static int hpsa_eh_device_reset_handler(struct scsi_cmnd *scsicmd);
 static int hpsa_eh_abort_handler(struct scsi_cmnd *scsicmd);
 static int hpsa_slave_alloc(struct scsi_device *sdev);
+static int hpsa_slave_configure(struct scsi_device *sdev);
 static void hpsa_slave_destroy(struct scsi_device *sdev);
 
 static void hpsa_update_scsi_devices(struct ctlr_info *h, int hostno);
@@ -231,27 +303,31 @@ static void check_ioctl_unit_attention(struct ctlr_info *h,
 	struct CommandList *c);
 /* performant mode helper functions */
 static void calc_bucket_map(int *bucket, int num_buckets,
-	int nsgs, int min_blocks, u32 *bucket_map);
-static void hpsa_put_ctlr_into_performant_mode(struct ctlr_info *h);
+	int nsgs, int min_blocks, int *bucket_map);
+static void hpsa_free_performant_mode(struct ctlr_info *h);
+static __devinit int hpsa_put_ctlr_into_performant_mode(struct ctlr_info *h);
 static inline u32 next_command(struct ctlr_info *h, u8 q);
-static int hpsa_find_cfg_addrs(struct pci_dev *pdev, void __iomem *vaddr,
-			       u32 *cfg_base_addr, u64 *cfg_base_addr_index,
-			       u64 *cfg_offset);
-static int hpsa_pci_find_memory_BAR(struct pci_dev *pdev,
-				    unsigned long *memory_bar);
-static int hpsa_lookup_board_id(struct pci_dev *pdev, u32 *board_id);
-static int hpsa_wait_for_board_state(struct pci_dev *pdev, void __iomem *vaddr,
-				     int wait_for_ready);
+static int hpsa_find_cfg_addrs(struct pci_dev *pdev,
+	void __iomem *vaddr, u32 *cfg_base_addr, u64 *cfg_base_addr_index,
+	u64 *cfg_offset);
+static int __devinit hpsa_pci_find_memory_BAR(struct pci_dev *pdev,
+	unsigned long *memory_bar);
+static int __devinit hpsa_lookup_board_id(struct pci_dev *pdev, u32 *board_id);
+static int hpsa_wait_for_board_state(struct pci_dev *pdev,
+	void __iomem *vaddr, int wait_for_ready);
+static inline u32 hpsa_tag_discard_error_bits(struct ctlr_info *h, u32 tag);
 static inline void finish_cmd(struct CommandList *c);
+static int hpsa_wait_for_clear_event_notify_ack(struct ctlr_info *h);
 static int hpsa_wait_for_mode_change_ack(struct ctlr_info *h);
 #define BOARD_NOT_READY 0
 #define BOARD_READY 1
 static void hpsa_drain_accel_commands(struct ctlr_info *h);
 static void hpsa_flush_cache(struct ctlr_info *h);
 static int hpsa_scsi_ioaccel_queue_command(struct ctlr_info *h,
-	struct CommandList *c, u32 ioaccel_handle, u8 *cdb, int cdb_len,
-	u8 *scsi3addr, struct hpsa_scsi_dev_t *phys_disk);
+        struct CommandList *c, u32 ioaccel_handle, u8 *cdb, int cdb_len,
+        u8 *scsi3addr, struct hpsa_scsi_dev_t *phys_disk);
 static void hpsa_command_resubmit_worker(struct work_struct *work);
+static void detect_controller_lockup(struct ctlr_info *h);
 
 static inline struct ctlr_info *sdev_to_hba(struct scsi_device *sdev)
 {
@@ -265,13 +341,63 @@ static inline struct ctlr_info *shost_to_hba(struct Scsi_Host *sh)
 	return (struct ctlr_info *) *priv;
 }
 
+static inline bool hpsa_is_cmd_idle(struct CommandList *c)
+{
+	return c->scsi_cmd == SCSI_CMD_IDLE;
+}
+
+static inline bool hpsa_is_pending_event(struct CommandList *c)
+{
+	return c->abort_pending || c->reset_pending;
+}
+
+/* extract sense key, asc, and ascq from sense data.  -1 means invalid. */
+static void decode_sense_data(const u8 *sense_data, int sense_data_len,
+			int *sense_key, int *asc, int *ascq)
+{
+	if (sense_data_len < 1) {
+		*sense_key = -1;
+		*ascq = -1;
+		*asc = -1;
+		return;
+	}
+
+	switch (sense_data[0]) {
+	case 0x70: /* old format sense data */
+		*sense_key = (sense_data_len > 2) ? sense_data[2] & 0x0f : -1;
+		*asc = (sense_data_len > 12) ?  sense_data[12] : -1;
+		*ascq = (sense_data_len > 13) ?  sense_data[13] : -1;
+		break;
+	case 0x72: /* descriptor format sense data */
+		*sense_key = (sense_data_len > 1) ? sense_data[1] & 0x0f : -1;
+		*asc = (sense_data_len > 2) ?  sense_data[2] : -1;
+		*ascq = (sense_data_len > 3) ?  sense_data[3] : -1;
+		break;
+	default:
+		*sense_key = -1;
+		*ascq = -1;
+		*asc = -1;
+		break;
+	}
+}
+
 static int check_for_unit_attention(struct ctlr_info *h,
 	struct CommandList *c)
 {
-	if (c->err_info->SenseInfo[2] != UNIT_ATTENTION)
+	int sense_key, asc, ascq;
+	int sense_len;
+
+	if (c->err_info->SenseLen > sizeof(c->err_info->SenseInfo))
+		sense_len = sizeof(c->err_info->SenseInfo);
+	else
+		sense_len = c->err_info->SenseLen;
+
+	decode_sense_data(c->err_info->SenseInfo, sense_len,
+				&sense_key, &asc, &ascq);
+	if (sense_key != UNIT_ATTENTION || asc == -1)
 		return 0;
 
-	switch (c->err_info->SenseInfo[12]) {
+	switch (asc) {
 	case STATE_CHANGED:
 		dev_warn(&h->pdev->dev, HPSA "%d: a state change "
 			"detected, command retried\n", h->ctlr);
@@ -307,16 +433,14 @@ static int check_for_unit_attention(struct ctlr_info *h,
 static int check_for_busy(struct ctlr_info *h, struct CommandList *c)
 {
 	if (c->err_info->CommandStatus != CMD_TARGET_STATUS ||
-		(c->err_info->ScsiStatus != SAM_STAT_BUSY &&
+		(c->err_info->ScsiStatus != SAM_STAT_BUSY && 
 		 c->err_info->ScsiStatus != SAM_STAT_TASK_SET_FULL))
 		return 0;
 	dev_warn(&h->pdev->dev, HPSA "device busy");
 	return 1;
 }
 
-static ssize_t host_store_hp_ssd_smart_path_status(struct device *dev,
-					 struct device_attribute *attr,
-					 const char *buf, size_t count)
+DECLARE_DEVATTR_STORE_FUNC(host_store_hp_ssd_smart_path_status)
 {
 	int status, len;
 	struct ctlr_info *h;
@@ -331,16 +455,45 @@ static ssize_t host_store_hp_ssd_smart_path_status(struct device *dev,
 	if (sscanf(tmpbuf, "%d", &status) != 1)
 		return -EINVAL;
 	h = shost_to_hba(shost);
+	if (h->hba_mode_enabled) {
+		dev_warn(&h->pdev->dev, "hpsa: HBA mode active, "
+			"cannot enable HP SSD Smart Path.\n");
+		return count;
+	}
 	h->acciopath_status = !!status;
-	dev_warn(&h->pdev->dev,
-		"hpsa: HP SSD Smart Path %s via sysfs update.\n",
+	dev_warn(&h->pdev->dev, "hpsa: HP SSD Smart Path %s "
+		"via sysfs update.\n",
 		h->acciopath_status ? "enabled" : "disabled");
 	return count;
 }
 
-static ssize_t host_store_raid_offload_debug(struct device *dev,
-					 struct device_attribute *attr,
-					 const char *buf, size_t count)
+DECLARE_DEVATTR_STORE_FUNC(host_store_disk_rq_timeout)
+{
+	int timeout, len;
+	struct ctlr_info *h;
+	struct Scsi_Host *shost = class_to_shost(dev);
+	char tmpbuf[10];
+
+	if (!capable(CAP_SYS_ADMIN) || !capable(CAP_SYS_RAWIO))
+		return -EACCES;
+	len = count > sizeof(tmpbuf) - 1 ? sizeof(tmpbuf) - 1 : count;
+	strncpy(tmpbuf, buf, len);
+	tmpbuf[len] = '\0';
+	len = sscanf(tmpbuf, "%d", &timeout);
+	if ((len == 0) || (len > 2))
+		return -EINVAL;
+	if (timeout > 30)
+		timeout = 30;
+	if (timeout < 0)
+		timeout = 0;
+	h = shost_to_hba(shost);
+	h->disk_rq_timeout = timeout;
+	dev_warn(&h->pdev->dev, "hpsa: disk request timeout set to %d seconds",
+		h->disk_rq_timeout);
+	return count;
+}
+
+DECLARE_DEVATTR_STORE_FUNC(host_store_raid_offload_debug)
 {
 	int debug_level, len;
 	struct ctlr_info *h;
@@ -354,7 +507,7 @@ static ssize_t host_store_raid_offload_debug(struct device *dev,
 	tmpbuf[len] = '\0';
 	if (sscanf(tmpbuf, "%d", &debug_level) != 1)
 		return -EINVAL;
-	if (debug_level < 0)
+	if (debug_level < 0 )
 		debug_level = 0;
 	h = shost_to_hba(shost);
 	h->raid_offload_debug = debug_level;
@@ -363,9 +516,7 @@ static ssize_t host_store_raid_offload_debug(struct device *dev,
 	return count;
 }
 
-static ssize_t host_store_rescan(struct device *dev,
-				 struct device_attribute *attr,
-				 const char *buf, size_t count)
+DECLARE_DEVATTR_STORE_FUNC(host_store_rescan)
 {
 	struct ctlr_info *h;
 	struct Scsi_Host *shost = class_to_shost(dev);
@@ -374,8 +525,7 @@ static ssize_t host_store_rescan(struct device *dev,
 	return count;
 }
 
-static ssize_t host_show_firmware_revision(struct device *dev,
-	     struct device_attribute *attr, char *buf)
+DECLARE_DEVATTR_SHOW_FUNC(host_show_firmware_revision)
 {
 	struct ctlr_info *h;
 	struct Scsi_Host *shost = class_to_shost(dev);
@@ -389,8 +539,7 @@ static ssize_t host_show_firmware_revision(struct device *dev,
 		fwrev[0], fwrev[1], fwrev[2], fwrev[3]);
 }
 
-static ssize_t host_show_commands_outstanding(struct device *dev,
-	     struct device_attribute *attr, char *buf)
+DECLARE_DEVATTR_SHOW_FUNC(host_show_commands_outstanding)
 {
 	struct Scsi_Host *shost = class_to_shost(dev);
 	struct ctlr_info *h = shost_to_hba(shost);
@@ -399,8 +548,7 @@ static ssize_t host_show_commands_outstanding(struct device *dev,
 			atomic_read(&h->commands_outstanding));
 }
 
-static ssize_t host_show_transport_mode(struct device *dev,
-	struct device_attribute *attr, char *buf)
+DECLARE_DEVATTR_SHOW_FUNC(host_show_transport_mode)
 {
 	struct ctlr_info *h;
 	struct Scsi_Host *shost = class_to_shost(dev);
@@ -411,17 +559,30 @@ static ssize_t host_show_transport_mode(struct device *dev,
 			"performant" : "simple");
 }
 
-static ssize_t host_show_hp_ssd_smart_path_status(struct device *dev,
-	struct device_attribute *attr, char *buf)
+DECLARE_DEVATTR_SHOW_FUNC(host_show_hp_ssd_smart_path_status)
 {
 	struct ctlr_info *h;
 	struct Scsi_Host *shost = class_to_shost(dev);
-
 	h = shost_to_hba(shost);
 	return snprintf(buf, 30, "HP SSD Smart Path %s\n",
-		(h->acciopath_status == 1) ?  "enabled" : "disabled");
+		(h->acciopath_status == 1) ?
+		"enabled" : "disabled");
+}
+
+DECLARE_DEVATTR_SHOW_FUNC(host_show_disk_rq_timeout)
+{
+	struct ctlr_info *h;
+	struct Scsi_Host *shost = class_to_shost(dev);
+	h = shost_to_hba(shost);
+
+	if (h->disk_rq_timeout == 0)
+		return snprintf(buf, 11, "no timeout\n");
+	else
+		return snprintf(buf, 35, "disk request timeout =  %d seconds\n",
+			h->disk_rq_timeout);
 }
 
+
 /* List of controllers which cannot be hard reset on kexec with reset_devices */
 static u32 unresettable_controller[] = {
 	0x324a103C, /* Smart Array P712m */
@@ -436,26 +597,12 @@ static u32 unresettable_controller[] = {
 	0x3215103C, /* Smart Array E200i */
 	0x3237103C, /* Smart Array E500 */
 	0x323D103C, /* Smart Array P700m */
-	0x40800E11, /* Smart Array 5i */
 	0x409C0E11, /* Smart Array 6400 */
 	0x409D0E11, /* Smart Array 6400 EM */
-	0x40700E11, /* Smart Array 5300 */
-	0x40820E11, /* Smart Array 532 */
-	0x40830E11, /* Smart Array 5312 */
-	0x409A0E11, /* Smart Array 641 */
-	0x409B0E11, /* Smart Array 642 */
-	0x40910E11, /* Smart Array 6i */
 };
 
 /* List of controllers which cannot even be soft reset */
 static u32 soft_unresettable_controller[] = {
-	0x40800E11, /* Smart Array 5i */
-	0x40700E11, /* Smart Array 5300 */
-	0x40820E11, /* Smart Array 532 */
-	0x40830E11, /* Smart Array 5312 */
-	0x409A0E11, /* Smart Array 641 */
-	0x409B0E11, /* Smart Array 642 */
-	0x40910E11, /* Smart Array 6i */
 	/* Exclude 640x boards.  These are two pci devices in one slot
 	 * which share a battery backed cache module.  One controls the
 	 * cache, the other accesses the cache through the one that controls
@@ -467,24 +614,31 @@ static u32 soft_unresettable_controller[] = {
 	0x409D0E11, /* Smart Array 6400 EM */
 };
 
-static int ctlr_is_hard_resettable(u32 board_id)
+static u32 needs_abort_tags_swizzled[] = {
+	0x324a103C, /* Smart Array P712m */
+	0x324b103C, /* SmartArray P711m */
+};
+
+static int board_id_in_array(u32 a[], int nelems, u32 board_id)
 {
 	int i;
 
-	for (i = 0; i < ARRAY_SIZE(unresettable_controller); i++)
-		if (unresettable_controller[i] == board_id)
-			return 0;
-	return 1;
+	for (i = 0; i < nelems; i++)
+		if (a[i] == board_id)
+			return 1;
+	return 0;
 }
 
-static int ctlr_is_soft_resettable(u32 board_id)
+static int ctlr_is_hard_resettable(u32 board_id)
 {
-	int i;
+	return !board_id_in_array(unresettable_controller,
+			ARRAY_SIZE(unresettable_controller), board_id);
+}
 
-	for (i = 0; i < ARRAY_SIZE(soft_unresettable_controller); i++)
-		if (soft_unresettable_controller[i] == board_id)
-			return 0;
-	return 1;
+static int ctlr_is_soft_resettable(u32 board_id)
+{
+	return !board_id_in_array(soft_unresettable_controller,
+			ARRAY_SIZE(soft_unresettable_controller), board_id);
 }
 
 static int ctlr_is_resettable(u32 board_id)
@@ -493,8 +647,7 @@ static int ctlr_is_resettable(u32 board_id)
 		ctlr_is_soft_resettable(board_id);
 }
 
-static ssize_t host_show_resettable(struct device *dev,
-	struct device_attribute *attr, char *buf)
+DECLARE_DEVATTR_SHOW_FUNC(host_show_resettable)
 {
 	struct ctlr_info *h;
 	struct Scsi_Host *shost = class_to_shost(dev);
@@ -503,17 +656,31 @@ static ssize_t host_show_resettable(struct device *dev,
 	return snprintf(buf, 20, "%d\n", ctlr_is_resettable(h->board_id));
 }
 
+DECLARE_DEVATTR_SHOW_FUNC(host_show_heartbeat)
+{
+	struct ctlr_info *h;
+	struct Scsi_Host *shost = class_to_shost(dev);
+	u32 heartbeat;
+	unsigned long flags;
+
+	h = shost_to_hba(shost);
+	spin_lock_irqsave(&h->lock, flags);
+	heartbeat = readl(&h->cfgtable->HeartBeat);
+	spin_unlock_irqrestore(&h->lock, flags);
+	return snprintf(buf, 20, "0x%08x\n", heartbeat);
+}
+
 static inline int is_logical_dev_addr_mode(unsigned char scsi3addr[])
 {
 	return (scsi3addr[3] & 0xC0) == 0x40;
 }
 
-static const char * const raid_label[] = { "0", "4", "1(+0)", "5", "5+1", "6",
-	"1(+0)ADM", "UNKNOWN"
+static const char *raid_label[] = { "0", "4", "1(1+0)", "5", "5+1", "ADG",
+	"1(ADM)", "UNKNOWN"
 };
 #define HPSA_RAID_0	0
 #define HPSA_RAID_4	1
-#define HPSA_RAID_1	2	/* also used for RAID 10 */
+#define HPSA_RAID_1	2 	/* also used for RAID 10 */
 #define HPSA_RAID_5	3	/* also used for RAID 50 */
 #define HPSA_RAID_51	4
 #define HPSA_RAID_6	5	/* also used for RAID 60 */
@@ -554,6 +721,44 @@ static ssize_t raid_level_show(struct device *dev,
 	return l;
 }
 
+static ssize_t host_store_lockup_detector(struct device *dev,
+				struct device_attribute *attr,
+				const char *buf, size_t count)
+{
+	struct Scsi_Host *shost;
+	struct ctlr_info *h;
+	int len, enabled;
+	char tmpbuf[10];
+
+	if (!capable(CAP_SYS_ADMIN) || !capable(CAP_SYS_RAWIO))
+		return -EACCES;
+	len = count > sizeof(tmpbuf) - 1 ? sizeof(tmpbuf) - 1 : count;
+	strncpy(tmpbuf, buf, len);
+	tmpbuf[len] = '\0';
+	if (sscanf(tmpbuf, "%d", &enabled) != 1)
+		return -EINVAL;
+	shost = class_to_shost(dev);
+	h = shost_to_hba(shost);
+	h->lockup_detector_enabled = !!enabled;
+	return count;
+}
+
+static int ctlr_needs_abort_tags_swizzled(u32 board_id)
+{
+	return board_id_in_array(needs_abort_tags_swizzled,
+			ARRAY_SIZE(needs_abort_tags_swizzled), board_id);
+}
+
+static ssize_t host_show_lockup_detector(struct device *dev,
+	     struct device_attribute *attr, char *buf)
+{
+	struct ctlr_info *h;
+	struct Scsi_Host *shost = class_to_shost(dev);
+
+	h = shost_to_hba(shost);
+	return snprintf(buf, 20, "%d\n", h->lockup_detector_enabled);
+}
+
 static ssize_t lunid_show(struct device *dev,
 	     struct device_attribute *attr, char *buf)
 {
@@ -606,7 +811,59 @@ static ssize_t unique_id_show(struct device *dev,
 			sn[12], sn[13], sn[14], sn[15]);
 }
 
-static ssize_t host_show_hp_ssd_smart_path_enabled(struct device *dev,
+static ssize_t lockup_action_show (struct device *dev,
+            struct device_attribute *attr, char *buf)
+{
+
+	ssize_t l = 0;
+	/*-------------------------------------------------
+
+	Might want to make this configurble per controller
+
+	int lockup;
+	struct ctlr_info *h;
+	struct scsi_device *sdev;
+	struct hpsa_scsi_dev_t *hdev;
+	unsigned long flags;
+
+	sdev = to_scsi_device(dev);
+	h = sdev_to_hba(sdev);
+	spin_lock_irqsave(&h->lock, flags);
+	hdev = sdev->hostdata;
+	if (!hdev) {
+		spin_unlock_irqrestore(&h->lock, flags);
+		return -ENODEV;
+	}
+	lockup = hdev->hpsa_lockup_action;
+	spin_unlock_irqrestore(&h->lock, flags);
+
+	------------------------------------------------------*/
+
+	l = snprintf(buf, PAGE_SIZE, "LOCKUP ACTION SET TO: %d\n", hpsa_lockup_action);
+	return l;
+}
+
+static ssize_t lockup_action_store (struct device *dev,
+                                   struct device_attribute *attr,
+                                   const char *buf, size_t count)
+{
+	
+	struct ctlr_info *h;
+	struct Scsi_Host *shost = class_to_shost(dev);
+	h = shost_to_hba(shost);
+
+	dev_warn(&h->pdev->dev,"Setting controller lockup response to %s\n", buf);
+
+	if ((strncmp("1",buf,1)) == 1 )
+		hpsa_lockup_action = 1;
+	else if ((strncmp("2",buf,1)) == 2 )
+		hpsa_lockup_action = 2;
+	else
+		hpsa_lockup_action = 0;
+	return count;
+}
+
+static ssize_t hp_ssd_smart_path_enabled(struct device *dev,
 	     struct device_attribute *attr, char *buf)
 {
 	struct ctlr_info *h;
@@ -628,42 +885,120 @@ static ssize_t host_show_hp_ssd_smart_path_enabled(struct device *dev,
 	return snprintf(buf, 20, "%d\n", offload_enabled);
 }
 
+#define MAX_PATHS 8
+#define PATH_STRING_LEN 30
+static ssize_t path_info_show(struct device *dev,
+	     struct device_attribute *attr, char *buf)
+{
+	struct ctlr_info *h;
+	struct scsi_device *sdev;
+	struct hpsa_scsi_dev_t *hdev;
+	unsigned long flags;
+	int i;
+	int output_len = 0;
+	u8 box;
+	u8 bay;
+	u8 path_map_index = 0;
+	unsigned char active;
+	unsigned char phys_connector[2];
+	unsigned char path[MAX_PATHS][PATH_STRING_LEN];
+
+	memset(path, 0, MAX_PATHS * PATH_STRING_LEN);
+	sdev = to_scsi_device(dev);
+	h = sdev_to_hba(sdev);
+	spin_lock_irqsave(&h->devlock, flags);
+	hdev = sdev->hostdata;
+	if (!hdev) {
+		spin_unlock_irqrestore(&h->devlock, flags);
+		return -ENODEV;
+	}
+	bay = hdev->bay;
+	for (i = 0; i < MAX_PATHS; i++) {
+		box = hdev->box[i];
+		memcpy(&phys_connector, &hdev->phys_connector[i],
+			sizeof(phys_connector));
+		path_map_index = 1<<i;
+		if (i == hdev->active_path_index)
+			active = 'A';
+		else if (hdev->path_map & path_map_index)
+			active = 'I';
+		else
+			continue;
+		if (hdev->devtype == TYPE_DISK) {
+			if (box == 0 || box == 0xFF)
+				output_len += snprintf(path[i],
+					PATH_STRING_LEN,
+					"PORT: %.2s BAY: %hhu %c\n",
+					phys_connector, bay, active);
+			else
+				output_len += snprintf(path[i],
+					PATH_STRING_LEN,
+					"PORT: %.2s BOX: %hhu BAY: %hhu %c\n",
+					phys_connector, box, bay, active);
+		} else if (box != 0 && box != 0xFF) {
+			output_len += snprintf(path[i],
+				PATH_STRING_LEN, "PORT: %.2s BOX: %hhu %c\n",
+				phys_connector, box, active);
+		} else {
+			output_len += snprintf(path[i], PATH_STRING_LEN,
+				"PORT: %.2s %c\n", phys_connector, active);
+		}
+	}
+
+	spin_unlock_irqrestore(&h->devlock, flags);
+	return snprintf(buf, output_len+1, "%s%s%s%s%s%s%s%s",
+		path[0], path[1], path[2], path[3],
+		path[4], path[5], path[6], path[7]);
+}
+
 static DEVICE_ATTR(raid_level, S_IRUGO, raid_level_show, NULL);
 static DEVICE_ATTR(lunid, S_IRUGO, lunid_show, NULL);
 static DEVICE_ATTR(unique_id, S_IRUGO, unique_id_show, NULL);
-static DEVICE_ATTR(rescan, S_IWUSR, NULL, host_store_rescan);
 static DEVICE_ATTR(hp_ssd_smart_path_enabled, S_IRUGO,
-			host_show_hp_ssd_smart_path_enabled, NULL);
-static DEVICE_ATTR(hp_ssd_smart_path_status, S_IWUSR|S_IRUGO|S_IROTH,
-		host_show_hp_ssd_smart_path_status,
-		host_store_hp_ssd_smart_path_status);
-static DEVICE_ATTR(raid_offload_debug, S_IWUSR, NULL,
-			host_store_raid_offload_debug);
-static DEVICE_ATTR(firmware_revision, S_IRUGO,
+			hp_ssd_smart_path_enabled, NULL);
+static DEVICE_ATTR(path_info, S_IRUGO, path_info_show, NULL);
+static DEVICE_ATTR(lockup_action, S_IWUSR | S_IRUGO,
+	lockup_action_show, lockup_action_store);
+DECLARE_HOST_DEVICE_ATTR(hp_ssd_smart_path_status, S_IWUSR|S_IRUGO|S_IROTH,
+	host_show_hp_ssd_smart_path_status, host_store_hp_ssd_smart_path_status);
+DECLARE_HOST_DEVICE_ATTR(raid_offload_debug, S_IWUSR, NULL, host_store_raid_offload_debug);
+DECLARE_HOST_DEVICE_ATTR(rescan, S_IWUSR, NULL, host_store_rescan);
+DECLARE_HOST_DEVICE_ATTR(firmware_revision, S_IRUGO,
 	host_show_firmware_revision, NULL);
-static DEVICE_ATTR(commands_outstanding, S_IRUGO,
+DECLARE_HOST_DEVICE_ATTR(commands_outstanding, S_IRUGO,
 	host_show_commands_outstanding, NULL);
-static DEVICE_ATTR(transport_mode, S_IRUGO,
+DECLARE_HOST_DEVICE_ATTR(transport_mode, S_IRUGO,
 	host_show_transport_mode, NULL);
-static DEVICE_ATTR(resettable, S_IRUGO,
+DECLARE_HOST_DEVICE_ATTR(resettable, S_IRUGO,
 	host_show_resettable, NULL);
+DECLARE_HOST_DEVICE_ATTR(heartbeat, S_IRUGO,
+	host_show_heartbeat, NULL);
+DECLARE_HOST_DEVICE_ATTR(lockup_detector, S_IWUSR|S_IRUGO,
+	host_show_lockup_detector, host_store_lockup_detector);
+DECLARE_HOST_DEVICE_ATTR(disk_rq_timeout, S_IWUSR|S_IRUGO|S_IROTH,
+	host_show_disk_rq_timeout, host_store_disk_rq_timeout);
 
 static struct device_attribute *hpsa_sdev_attrs[] = {
 	&dev_attr_raid_level,
 	&dev_attr_lunid,
 	&dev_attr_unique_id,
 	&dev_attr_hp_ssd_smart_path_enabled,
+	&dev_attr_path_info,
 	NULL,
 };
 
-static struct device_attribute *hpsa_shost_attrs[] = {
+DECLARE_HOST_ATTR_LIST(hpsa_shost_attrs) = {
 	&dev_attr_rescan,
 	&dev_attr_firmware_revision,
 	&dev_attr_commands_outstanding,
 	&dev_attr_transport_mode,
 	&dev_attr_resettable,
-	&dev_attr_hp_ssd_smart_path_status,
+	&dev_attr_heartbeat,
 	&dev_attr_raid_offload_debug,
+	&dev_attr_hp_ssd_smart_path_status,
+	&dev_attr_lockup_detector,
+	&dev_attr_disk_rq_timeout,
+	&dev_attr_lockup_action,
 	NULL,
 };
 
@@ -672,15 +1007,18 @@ static struct scsi_host_template hpsa_driver_template = {
 	.name			= HPSA,
 	.proc_name		= HPSA,
 	.queuecommand		= hpsa_scsi_queue_command,
-	.scan_start		= hpsa_scan_start,
-	.scan_finished		= hpsa_scan_finished,
+	INITIALIZE_SCAN_START(hpsa_scan_start)
+	INITIALIZE_SCAN_FINISHED(hpsa_scan_finished)
+	HPSA_SKIP_HOST_LOCK
 	.change_queue_depth	= hpsa_change_queue_depth,
+	.change_queue_type	= hpsa_change_queue_type,
 	.this_id		= -1,
 	.use_clustering		= ENABLE_CLUSTERING,
 	.eh_abort_handler	= hpsa_eh_abort_handler,
 	.eh_device_reset_handler = hpsa_eh_device_reset_handler,
 	.ioctl			= hpsa_ioctl,
 	.slave_alloc		= hpsa_slave_alloc,
+	.slave_configure	= hpsa_slave_configure,
 	.slave_destroy		= hpsa_slave_destroy,
 #ifdef CONFIG_COMPAT
 	.compat_ioctl		= hpsa_compat_ioctl,
@@ -688,7 +1026,6 @@ static struct scsi_host_template hpsa_driver_template = {
 	.sdev_attrs = hpsa_sdev_attrs,
 	.shost_attrs = hpsa_shost_attrs,
 	.max_sectors = 8192,
-	.no_write_same = 1,
 };
 
 static inline u32 next_command(struct ctlr_info *h, u8 q)
@@ -731,7 +1068,7 @@ static inline u32 next_command(struct ctlr_info *h, u8 q)
  * bit 0 = "performant mode" bit.
  * bits 1-3 = block fetch table entry
  * bits 4-6 = command type (== 110)
- * (command type is needed because ioaccel1 mode
+ * (command type is needed because ioaccel1 mode 
  * commands are submitted through the same register as normal
  * mode commands, so this is how the controller knows whether
  * the command is normal mode or ioaccel1 mode.)
@@ -747,25 +1084,35 @@ static inline u32 next_command(struct ctlr_info *h, u8 q)
  * set bit 0 for pull model, bits 3-1 for block fetch
  * register number
  */
-static void set_performant_mode(struct ctlr_info *h, struct CommandList *c)
+#define DEFAULT_REPLY_QUEUE (-1)
+static void set_performant_mode(struct ctlr_info *h, struct CommandList *c,
+					int reply_queue)
 {
 	if (likely(h->transMethod & CFGTBL_Trans_Performant)) {
 		c->busaddr |= 1 | (h->blockFetchTable[c->Header.SGList] << 1);
-		if (likely(h->msix_vector > 0))
+		if (unlikely(!h->msix_vector))
+			return;
+		if (likely(reply_queue == DEFAULT_REPLY_QUEUE))
 			c->Header.ReplyQueue =
-				raw_smp_processor_id() % h->nreply_queues;
+				smp_processor_id() % h->nreply_queues;
+		else
+			c->Header.ReplyQueue = reply_queue % h->nreply_queues;
 	}
 }
 
 static void set_ioaccel1_performant_mode(struct ctlr_info *h,
-						struct CommandList *c)
+						struct CommandList *c,
+						int reply_queue)
 {
 	struct io_accel1_cmd *cp = &h->ioaccel_cmd_pool[c->cmdindex];
 
 	/* Tell the controller to post the reply to the queue for this
 	 * processor.  This seems to give the best I/O throughput.
 	 */
-	cp->ReplyQueue = smp_processor_id() % h->nreply_queues;
+	if (likely(reply_queue == DEFAULT_REPLY_QUEUE))
+		cp->ReplyQueue = smp_processor_id() % h->nreply_queues;
+	else
+		cp->ReplyQueue = reply_queue % h->nreply_queues;
 	/* Set the bits in the address sent down to include:
 	 *  - performant mode bit (bit 0)
 	 *  - pull count (bits 1-3)
@@ -775,15 +1122,41 @@ static void set_ioaccel1_performant_mode(struct ctlr_info *h,
 					IOACCEL1_BUSADDR_CMDTYPE;
 }
 
+static void set_ioaccel2_tmf_performant_mode(struct ctlr_info *h,
+						struct CommandList *c,
+						int reply_queue)
+{
+	struct hpsa_tmf_struct *cp = (struct hpsa_tmf_struct *)
+		&h->ioaccel2_cmd_pool[c->cmdindex];
+
+	/* Tell the controller to post the reply to the queue for this
+	 * processor.  This seems to give the best I/O throughput.
+	 */
+	if (likely(reply_queue == DEFAULT_REPLY_QUEUE))
+		cp->reply_queue = smp_processor_id() % h->nreply_queues;
+	else
+		cp->reply_queue = reply_queue % h->nreply_queues;
+	/* Set the bits in the address sent down to include:
+	 *  - performant mode bit not used in ioaccel mode 2
+	 *  - pull count (bits 0-3)
+	 *  - command type isn't needed for ioaccel2
+	 */
+	c->busaddr |= (h->ioaccel2_blockFetchTable[0]);
+}
+
 static void set_ioaccel2_performant_mode(struct ctlr_info *h,
-						struct CommandList *c)
+						struct CommandList *c,
+						int reply_queue)
 {
 	struct io_accel2_cmd *cp = &h->ioaccel2_cmd_pool[c->cmdindex];
 
 	/* Tell the controller to post the reply to the queue for this
 	 * processor.  This seems to give the best I/O throughput.
 	 */
-	cp->reply_queue = smp_processor_id() % h->nreply_queues;
+	if (likely(reply_queue == DEFAULT_REPLY_QUEUE))
+		cp->reply_queue = smp_processor_id() % h->nreply_queues;
+	else
+		cp->reply_queue = reply_queue % h->nreply_queues;
 	/* Set the bits in the address sent down to include:
 	 *  - performant mode bit not used in ioaccel mode 2
 	 *  - pull count (bits 0-3)
@@ -821,26 +1194,41 @@ static void dial_up_lockup_detection_on_fw_flash_complete(struct ctlr_info *h,
 		h->heartbeat_sample_interval = HEARTBEAT_SAMPLE_INTERVAL;
 }
 
-static void enqueue_cmd_and_start_io(struct ctlr_info *h,
-	struct CommandList *c)
+static void __enqueue_cmd_and_start_io(struct ctlr_info *h,
+	struct CommandList *c, int reply_queue)
 {
+	int count;
+
 	dial_down_lockup_detection_during_fw_flash(h, c);
 	atomic_inc(&h->commands_outstanding);
+	count = atomic_read(&h->commands_outstanding);
 	switch (c->cmd_type) {
 	case CMD_IOACCEL1:
-		set_ioaccel1_performant_mode(h, c);
+		set_ioaccel1_performant_mode(h, c, reply_queue);
 		writel(c->busaddr, h->vaddr + SA5_REQUEST_PORT_OFFSET);
 		break;
 	case CMD_IOACCEL2:
-		set_ioaccel2_performant_mode(h, c);
+		set_ioaccel2_performant_mode(h, c, reply_queue);
+		writel(c->busaddr, h->vaddr + IOACCEL2_INBOUND_POSTQ_32);
+		break;
+	case IOACCEL2_TMF:
+		set_ioaccel2_tmf_performant_mode(h, c, reply_queue);
 		writel(c->busaddr, h->vaddr + IOACCEL2_INBOUND_POSTQ_32);
 		break;
 	default:
-		set_performant_mode(h, c);
+		set_performant_mode(h, c, reply_queue);
 		h->access.submit_command(h, c);
 	}
 }
 
+static void enqueue_cmd_and_start_io(struct ctlr_info *h, struct CommandList *c)
+{
+	if (unlikely(hpsa_is_pending_event(c)))
+		return finish_cmd(c);
+
+	__enqueue_cmd_and_start_io(h, c, DEFAULT_REPLY_QUEUE);
+}
+
 static inline int is_hba_lunid(unsigned char scsi3addr[])
 {
 	return memcmp(scsi3addr, RAID_CTLR_LUNID, 8) == 0;
@@ -905,7 +1293,7 @@ static int hpsa_scsi_add_entry(struct ctlr_info *h, int hostno,
 
 	/* If this device a non-zero lun of a multi-lun device
 	 * byte 4 of the 8-byte LUN addr will contain the logical
-	 * unit no, zero otherwise.
+	 * unit no, zero otherise.
 	 */
 	if (device->scsi3addr[4] == 0) {
 		/* This is not a non-zero lun of a multi-lun device */
@@ -946,17 +1334,23 @@ lun_assigned:
 
 	h->dev[n] = device;
 	h->ndevices++;
+	device->offload_to_be_enabled = device->offload_enabled;
+	device->offload_enabled = 0;
 	added[*nadded] = device;
 	(*nadded)++;
 
-	/* initially, (before registering with scsi layer) we don't
-	 * know our hostno and we don't want to print anything first
-	 * time anyway (the scsi layer's inquiries will show that info)
-	 */
-	/* if (hostno != -1) */
-		dev_info(&h->pdev->dev, "%s device c%db%dt%dl%d added.\n",
-			scsi_device_type(device->devtype), hostno,
-			device->bus, device->target, device->lun);
+	dev_info(&h->pdev->dev,
+		"added scsi %d:%d:%d:%d: %s %.8s %.16s RAID-%s SSDSmartPathCap%c En%c Exp=%d qd=%d\n",
+		hostno, device->bus, device->target, device->lun,
+		scsi_device_type(device->devtype),
+		device->vendor,
+		device->model,
+		device->raid_level > RAID_UNKNOWN ?
+			"RAID-?" : raid_label[device->raid_level],
+		device->offload_config ? '+' : '-',
+		device->offload_enabled ? '+' : '-',
+		device->expose_state,
+		device->queue_depth);
 	return 0;
 }
 
@@ -972,8 +1366,7 @@ static void hpsa_scsi_update_entry(struct ctlr_info *h, int hostno,
 
 	/* Raid offload parameters changed.  Careful about the ordering. */
 	if (new_entry->offload_config && new_entry->offload_enabled) {
-		/*
-		 * if drive is newly offload_enabled, we want to copy the
+		/* if drive is newly offload_enabled, we want to copy the
 		 * raid map data first.  If previously offload_enabled and
 		 * offload_config were set, raid map data had better be
 		 * the same as it was before.  if raid map data is changed
@@ -982,16 +1375,37 @@ static void hpsa_scsi_update_entry(struct ctlr_info *h, int hostno,
 		 */
 		h->dev[entry]->raid_map = new_entry->raid_map;
 		h->dev[entry]->ioaccel_handle = new_entry->ioaccel_handle;
-		wmb(); /* ensure raid map updated prior to ->offload_enabled */
 	}
+	if (new_entry->hba_ioaccel_enabled) {
+		h->dev[entry]->ioaccel_handle = new_entry->ioaccel_handle;
+		wmb(); /* set ioaccel_handle *before* hba_ioaccel_eanbled */
+	}
+	h->dev[entry]->hba_ioaccel_enabled = new_entry->hba_ioaccel_enabled;
 	h->dev[entry]->offload_config = new_entry->offload_config;
 	h->dev[entry]->offload_to_mirror = new_entry->offload_to_mirror;
-	h->dev[entry]->offload_enabled = new_entry->offload_enabled;
+
+	/* We can turn off ioaccel offload now, but need to delay turning
+	 * it on until we can update h->dev[entry]->phys_disk[], but we
+	 * can't do that until all the devices are updated.
+	 */
+	h->dev[entry]->offload_to_be_enabled = new_entry->offload_enabled;
+	if (!new_entry->offload_enabled)
+		h->dev[entry]->offload_enabled = 0;
 	h->dev[entry]->queue_depth = new_entry->queue_depth;
 
-	dev_info(&h->pdev->dev, "%s device c%db%dt%dl%d updated.\n",
-		scsi_device_type(new_entry->devtype), hostno, new_entry->bus,
-		new_entry->target, new_entry->lun);
+	dev_info(&h->pdev->dev,
+		"updated scsi %d:%d:%d:%d: %s %.8s %.16s RAID-%s SSDSmartPathCap%c En%c Exp=%d qd=%d\n",
+		hostno, h->dev[entry]->bus,
+		h->dev[entry]->target, h->dev[entry]->lun,
+		scsi_device_type(h->dev[entry]->devtype),
+		h->dev[entry]->vendor,
+		h->dev[entry]->model,
+		h->dev[entry]->raid_level > RAID_UNKNOWN ?
+			"RAID-?" : raid_label[h->dev[entry]->raid_level],
+		h->dev[entry]->offload_config ? '+' : '-',
+		h->dev[entry]->offload_enabled ? '+' : '-',
+		h->dev[entry]->expose_state,
+		h->dev[entry]->queue_depth);
 }
 
 /* Replace an entry from h->dev[] array. */
@@ -1014,12 +1428,23 @@ static void hpsa_scsi_replace_entry(struct ctlr_info *h, int hostno,
 		new_entry->lun = h->dev[entry]->lun;
 	}
 
+	new_entry->offload_to_be_enabled = new_entry->offload_enabled;
+	new_entry->offload_enabled = 0;
 	h->dev[entry] = new_entry;
 	added[*nadded] = new_entry;
 	(*nadded)++;
-	dev_info(&h->pdev->dev, "%s device c%db%dt%dl%d changed.\n",
-		scsi_device_type(new_entry->devtype), hostno, new_entry->bus,
-			new_entry->target, new_entry->lun);
+	dev_info(&h->pdev->dev,
+		"replaced scsi %d:%d:%d:%d: %s %.8s %.16s RAID-%s SSDSmartPathCap%c En%c Exp=%d qd=%d\n",
+		hostno, new_entry->bus, new_entry->target, new_entry->lun,
+		scsi_device_type(new_entry->devtype),
+		new_entry->vendor,
+		new_entry->model,
+		new_entry->raid_level > RAID_UNKNOWN ?
+			"RAID-?" : raid_label[new_entry->raid_level],
+		new_entry->offload_config ? '+' : '-',
+		new_entry->offload_enabled ? '+' : '-',
+		new_entry->expose_state,
+		new_entry->queue_depth);
 }
 
 /* Remove an entry from h->dev[] array. */
@@ -1039,9 +1464,18 @@ static void hpsa_scsi_remove_entry(struct ctlr_info *h, int hostno, int entry,
 	for (i = entry; i < h->ndevices-1; i++)
 		h->dev[i] = h->dev[i+1];
 	h->ndevices--;
-	dev_info(&h->pdev->dev, "%s device c%db%dt%dl%d removed.\n",
-		scsi_device_type(sd->devtype), hostno, sd->bus, sd->target,
-		sd->lun);
+	dev_info(&h->pdev->dev,
+		"removed scsi %d:%d:%d:%d: %s %.8s %.16s RAID-%s SSDSmartPathCap%c En%c Exp=%d qd=%d\n",
+		hostno, sd->bus, sd->target, sd->lun,
+		scsi_device_type(sd->devtype),
+		sd->vendor,
+		sd->model,
+		sd->raid_level > RAID_UNKNOWN ?
+			"RAID-?" : raid_label[sd->raid_level],
+		sd->offload_config ? '+' : '-',
+		sd->offload_enabled ? '+' : '-',
+		sd->expose_state,
+		sd->queue_depth);
 }
 
 #define SCSI3ADDR_EQ(a, b) ( \
@@ -1146,7 +1580,7 @@ static int hpsa_scsi_find_entry(struct hpsa_scsi_dev_t *needle,
 				return DEVICE_SAME;
 			} else {
 				/* Keep offline devices offline */
-				if (needle->volume_offline)
+				if (needle->volume_offline) 
 					return DEVICE_NOT_FOUND;
 				return DEVICE_CHANGED;
 			}
@@ -1166,7 +1600,7 @@ static void hpsa_monitor_offline_device(struct ctlr_info *h,
 	spin_lock_irqsave(&h->offline_device_lock, flags);
 	list_for_each_entry(device, &h->offline_device_list, offline_list) {
 		if (memcmp(device->scsi3addr, scsi3addr,
-			sizeof(device->scsi3addr)) == 0) {
+				sizeof(device->scsi3addr)) == 0) {
 			spin_unlock_irqrestore(&h->offline_device_lock, flags);
 			return;
 		}
@@ -1191,77 +1625,91 @@ static void hpsa_show_volume_status(struct ctlr_info *h,
 {
 	if (sd->volume_offline == HPSA_VPD_LV_STATUS_UNSUPPORTED)
 		dev_info(&h->pdev->dev,
-			"C%d:B%d:T%d:L%d Volume status is not available through vital product data pages.\n",
+			"C%d:B%d:T%d:L%d Volume status is not "
+			"available through vital product data pages.\n",
 			h->scsi_host->host_no,
 			sd->bus, sd->target, sd->lun);
 	switch (sd->volume_offline) {
-	case HPSA_LV_OK:
-		break;
-	case HPSA_LV_UNDERGOING_ERASE:
-		dev_info(&h->pdev->dev,
-			"C%d:B%d:T%d:L%d Volume is undergoing background erase process.\n",
-			h->scsi_host->host_no,
-			sd->bus, sd->target, sd->lun);
-		break;
-	case HPSA_LV_UNDERGOING_RPI:
-		dev_info(&h->pdev->dev,
-			"C%d:B%d:T%d:L%d Volume is undergoing rapid parity initialization process.\n",
-			h->scsi_host->host_no,
-			sd->bus, sd->target, sd->lun);
-		break;
-	case HPSA_LV_PENDING_RPI:
-		dev_info(&h->pdev->dev,
-				"C%d:B%d:T%d:L%d Volume is queued for rapid parity initialization process.\n",
+		case HPSA_LV_OK:
+			break;
+		case HPSA_LV_UNDERGOING_ERASE:
+			dev_info(&h->pdev->dev,
+				"C%d:B%d:T%d:L%d Volume is undergoing "
+				"background erase process.\n",
 				h->scsi_host->host_no,
 				sd->bus, sd->target, sd->lun);
-		break;
-	case HPSA_LV_ENCRYPTED_NO_KEY:
-		dev_info(&h->pdev->dev,
-			"C%d:B%d:T%d:L%d Volume is encrypted and cannot be accessed because key is not present.\n",
-			h->scsi_host->host_no,
-			sd->bus, sd->target, sd->lun);
-		break;
-	case HPSA_LV_PLAINTEXT_IN_ENCRYPT_ONLY_CONTROLLER:
-		dev_info(&h->pdev->dev,
-			"C%d:B%d:T%d:L%d Volume is not encrypted and cannot be accessed because controller is in encryption-only mode.\n",
-			h->scsi_host->host_no,
-			sd->bus, sd->target, sd->lun);
-		break;
-	case HPSA_LV_UNDERGOING_ENCRYPTION:
-		dev_info(&h->pdev->dev,
-			"C%d:B%d:T%d:L%d Volume is undergoing encryption process.\n",
-			h->scsi_host->host_no,
-			sd->bus, sd->target, sd->lun);
-		break;
-	case HPSA_LV_UNDERGOING_ENCRYPTION_REKEYING:
-		dev_info(&h->pdev->dev,
-			"C%d:B%d:T%d:L%d Volume is undergoing encryption re-keying process.\n",
-			h->scsi_host->host_no,
-			sd->bus, sd->target, sd->lun);
-		break;
-	case HPSA_LV_ENCRYPTED_IN_NON_ENCRYPTED_CONTROLLER:
-		dev_info(&h->pdev->dev,
-			"C%d:B%d:T%d:L%d Volume is encrypted and cannot be accessed because controller does not have encryption enabled.\n",
-			h->scsi_host->host_no,
-			sd->bus, sd->target, sd->lun);
-		break;
-	case HPSA_LV_PENDING_ENCRYPTION:
-		dev_info(&h->pdev->dev,
-			"C%d:B%d:T%d:L%d Volume is pending migration to encrypted state, but process has not started.\n",
-			h->scsi_host->host_no,
-			sd->bus, sd->target, sd->lun);
-		break;
-	case HPSA_LV_PENDING_ENCRYPTION_REKEYING:
-		dev_info(&h->pdev->dev,
-			"C%d:B%d:T%d:L%d Volume is encrypted and is pending encryption rekeying.\n",
-			h->scsi_host->host_no,
-			sd->bus, sd->target, sd->lun);
-		break;
+			break;
+		case HPSA_LV_UNDERGOING_RPI:
+			dev_info(&h->pdev->dev,
+				"C%d:B%d:T%d:L%d Volume is undergoing "
+				"rapid parity initialization process.\n",
+				h->scsi_host->host_no,
+				sd->bus, sd->target, sd->lun);
+			break;
+		case HPSA_LV_PENDING_RPI:
+			dev_info(&h->pdev->dev,
+				"C%d:B%d:T%d:L%d Volume is queued for "
+				"rapid parity initialization process.\n",
+				h->scsi_host->host_no,
+				sd->bus, sd->target, sd->lun);
+			break;
+		case HPSA_LV_ENCRYPTED_NO_KEY:
+			dev_info(&h->pdev->dev,
+				"C%d:B%d:T%d:L%d "
+				"Volume is encrypted and cannot be accessed because "
+				"key is not present.\n",
+				h->scsi_host->host_no,
+				sd->bus, sd->target, sd->lun);
+			break;
+		case HPSA_LV_PLAINTEXT_IN_ENCRYPT_ONLY_CONTROLLER:
+			dev_info(&h->pdev->dev,
+				"C%d:B%d:T%d:L%d Volume is not encrypted "
+				"and cannot be accessed because "
+				"controller is in encryption-only mode.\n",
+				h->scsi_host->host_no,
+				sd->bus, sd->target, sd->lun);
+			break;
+		case HPSA_LV_UNDERGOING_ENCRYPTION:
+			dev_info(&h->pdev->dev,
+				"C%d:B%d:T%d:L%d Volume is undergoing "
+				"encryption process.\n",
+				h->scsi_host->host_no,
+				sd->bus, sd->target, sd->lun);
+			break;
+		case HPSA_LV_UNDERGOING_ENCRYPTION_REKEYING:
+			dev_info(&h->pdev->dev,
+				"C%d:B%d:T%d:L%d Volume is undergoing "
+				"encryption re-keying process.\n",
+				h->scsi_host->host_no,
+				sd->bus, sd->target, sd->lun);
+			break;
+		case HPSA_LV_ENCRYPTED_IN_NON_ENCRYPTED_CONTROLLER:
+			dev_info(&h->pdev->dev,
+				"C%d:B%d:T%d:L%d Volume is encrypted "
+				"and cannot be accessed because "
+				"controller does not have encryption enabled.\n",
+				h->scsi_host->host_no,
+				sd->bus, sd->target, sd->lun);
+			break;
+		case HPSA_LV_PENDING_ENCRYPTION:
+			dev_info(&h->pdev->dev,
+				"C%d:B%d:T%d:L%d Volume is pending migration "
+				"to encrypted state, but process has not "
+				"started.\n",
+				h->scsi_host->host_no,
+				sd->bus, sd->target, sd->lun);
+			break;
+		case HPSA_LV_PENDING_ENCRYPTION_REKEYING:
+			dev_info(&h->pdev->dev,
+				"C%d:B%d:T%d:L%d Volume is encrypted "
+				"and is pending encryption rekeying.\n",
+				h->scsi_host->host_no,
+				sd->bus, sd->target, sd->lun);
+			break;
 	}
 }
 
-/*
- * Figure the list of physical drive pointers for a logical drive with
+/* Figure the list of physical drive pointers for a logical drive with
  * raid offload configured.
  */
 static void hpsa_figure_phys_disk_ptrs(struct ctlr_info *h,
@@ -1271,18 +1719,17 @@ static void hpsa_figure_phys_disk_ptrs(struct ctlr_info *h,
 	struct raid_map_data *map = &logical_drive->raid_map;
 	struct raid_map_disk_data *dd = &map->data[0];
 	int i, j;
-	int total_disks_per_row = le16_to_cpu(map->data_disks_per_row) +
-				le16_to_cpu(map->metadata_disks_per_row);
-	int nraid_map_entries = le16_to_cpu(map->row_cnt) *
-				le16_to_cpu(map->layout_map_count) *
-				total_disks_per_row;
-	int nphys_disk = le16_to_cpu(map->layout_map_count) *
-				total_disks_per_row;
+	int nraid_map_entries = map->row_cnt * map->layout_map_count *
+		(map->data_disks_per_row + map->metadata_disks_per_row);
+	int nphys_disk = map->layout_map_count *
+		(map->data_disks_per_row + map->metadata_disks_per_row);
 	int qdepth;
 
 	if (nraid_map_entries > RAID_MAP_MAX_ENTRIES)
 		nraid_map_entries = RAID_MAP_MAX_ENTRIES;
 
+	logical_drive->nphysical_disks = nraid_map_entries;
+
 	qdepth = 0;
 	for (i = 0; i < nraid_map_entries; i++) {
 		logical_drive->phys_disk[i] = NULL;
@@ -1303,16 +1750,14 @@ static void hpsa_figure_phys_disk_ptrs(struct ctlr_info *h,
 			break;
 		}
 
-		/*
-		 * This can happen if a physical drive is removed and
-		 * the logical drive is degraded.  In that case, the RAID
-		 * map data will refer to a physical disk which isn't actually
-		 * present.  And in that case offload_enabled should already
-		 * be 0, but we'll turn it off here just in case
-		 */
 		if (!logical_drive->phys_disk[i]) {
+			/* Shouldn't happen, barring firmware bugs. */
+			dev_warn(&h->pdev->dev,
+				"Failed to find physical disk ioaccel handle for logical drive %8ph\n",
+				logical_drive->scsi3addr);
 			logical_drive->offload_enabled = 0;
-			logical_drive->queue_depth = h->nr_cmds;
+			logical_drive->offload_to_be_enabled = 0;
+			logical_drive->queue_depth = 8;
 		}
 	}
 	if (nraid_map_entries)
@@ -1335,6 +1780,15 @@ static void hpsa_update_log_drive_phys_drive_ptrs(struct ctlr_info *h,
 			continue;
 		if (!is_logical_dev_addr_mode(dev[i]->scsi3addr))
 			continue;
+
+		/* If offload is currently enabled, the RAID map and
+		 * phys_disk[] assignment *better* not be changing
+		 * and since it isn't changing, we do not need to
+		 * update it.
+		 */
+		if (dev[i]->offload_enabled)
+			continue;
+
 		hpsa_figure_phys_disk_ptrs(h, dev, ndevices, dev[i]);
 	}
 }
@@ -1411,9 +1865,19 @@ static void adjust_hpsa_scsi_table(struct ctlr_info *h, int hostno,
 		 */
 		if (sd[i]->volume_offline) {
 			hpsa_show_volume_status(h, sd[i]);
-			dev_info(&h->pdev->dev, "c%db%dt%dl%d: temporarily offline\n",
-				h->scsi_host->host_no,
-				sd[i]->bus, sd[i]->target, sd[i]->lun);
+			dev_info(&h->pdev->dev,
+				"offline scsi %d:%d:%d:%d: %s %.8s %.16s RAID-%s SSDSmartPathCap%c En%c Exp=%d qd=%d\n",
+				hostno, sd[i]->bus, sd[i]->target, sd[i]->lun,
+				scsi_device_type(sd[i]->devtype),
+				sd[i]->vendor,
+				sd[i]->model,
+				sd[i]->raid_level > RAID_UNKNOWN ?
+					"RAID-?" :
+					raid_label[sd[i]->raid_level],
+				sd[i]->offload_config ? '+' : '-',
+				sd[i]->offload_enabled ? '+' : '-',
+				sd[i]->expose_state,
+				sd[i]->queue_depth);
 			continue;
 		}
 
@@ -1433,6 +1897,14 @@ static void adjust_hpsa_scsi_table(struct ctlr_info *h, int hostno,
 			/* but if it does happen, we just ignore that device */
 		}
 	}
+	hpsa_update_log_drive_phys_drive_ptrs(h, h->dev, h->ndevices);
+
+	/* Now that h->dev[]->phys_disk[] is coherent, we can enable
+	 * any logical drives that need it enabled.
+	 */
+	for (i = 0; i < h->ndevices; i++)
+		h->dev[i]->offload_enabled = h->dev[i]->offload_to_be_enabled;
+
 	spin_unlock_irqrestore(&h->devlock, flags);
 
 	/* Monitor devices which are in one of several NOT READY states to be
@@ -1456,20 +1928,23 @@ static void adjust_hpsa_scsi_table(struct ctlr_info *h, int hostno,
 	sh = h->scsi_host;
 	/* Notify scsi mid layer of any removed devices */
 	for (i = 0; i < nremoved; i++) {
-		struct scsi_device *sdev =
-			scsi_device_lookup(sh, removed[i]->bus,
-				removed[i]->target, removed[i]->lun);
-		if (sdev != NULL) {
-			scsi_remove_device(sdev);
-			scsi_device_put(sdev);
-		} else {
-			/* We don't expect to get here.
-			 * future cmds to this device will get selection
-			 * timeout as if the device was gone.
-			 */
-			dev_warn(&h->pdev->dev, "didn't find c%db%dt%dl%d "
-				" for removal.", hostno, removed[i]->bus,
-				removed[i]->target, removed[i]->lun);
+		if (removed[i]->expose_state & HPSA_SCSI_ADD) {
+			struct scsi_device *sdev =
+				scsi_device_lookup(sh, removed[i]->bus,
+					removed[i]->target, removed[i]->lun);
+			if (sdev != NULL) {
+				scsi_remove_device(sdev);
+				scsi_device_put(sdev);
+			} else {
+				/* We don't expect to get here.
+				 * future cmds to this device will get selection
+				 * timeout as if the device was gone.
+				 */
+				dev_warn(&h->pdev->dev,
+					"didn't find scsi %d:%d:%d:%d for removal.",
+					hostno, removed[i]->bus,
+					removed[i]->target, removed[i]->lun);
+			}
 		}
 		kfree(removed[i]);
 		removed[i] = NULL;
@@ -1477,11 +1952,14 @@ static void adjust_hpsa_scsi_table(struct ctlr_info *h, int hostno,
 
 	/* Notify scsi mid layer of any added devices */
 	for (i = 0; i < nadded; i++) {
+		if (!(added[i]->expose_state & HPSA_SCSI_ADD))
+			continue;
 		if (scsi_add_device(sh, added[i]->bus,
 			added[i]->target, added[i]->lun) == 0)
 			continue;
-		dev_warn(&h->pdev->dev, "scsi_add_device c%db%dt%dl%d failed, "
-			"device not added.\n", hostno, added[i]->bus,
+		dev_warn(&h->pdev->dev,
+			"scsi %d:%d:%d:%d addition failed, device not added.\n",
+			hostno, added[i]->bus,
 			added[i]->target, added[i]->lun);
 		/* now we have to remove it from h->dev,
 		 * since it didn't get added to scsi mid layer
@@ -1495,7 +1973,7 @@ free_and_out:
 }
 
 /*
- * Lookup bus/target/lun and return corresponding struct hpsa_scsi_dev_t *
+ * Lookup bus/target/lun and retrun corresponding struct hpsa_scsi_dev_t *
  * Assume's h->devlock is held.
  */
 static struct hpsa_scsi_dev_t *lookup_hpsa_scsi_dev(struct ctlr_info *h,
@@ -1523,12 +2001,33 @@ static int hpsa_slave_alloc(struct scsi_device *sdev)
 	spin_lock_irqsave(&h->devlock, flags);
 	sd = lookup_hpsa_scsi_dev(h, sdev_channel(sdev),
 		sdev_id(sdev), sdev->lun);
-	if (sd != NULL) {
+	if (sd && (sd->expose_state & HPSA_SCSI_ADD)) {
 		sdev->hostdata = sd;
 		if (sd->queue_depth)
-			scsi_change_queue_depth(sdev, sd->queue_depth);
+			scsi_adjust_queue_depth(sdev, scsi_get_tag_type(sdev),
+				sd->queue_depth);
 		atomic_set(&sd->ioaccel_cmds_out, 0);
+	} else if (sd) {
+		sdev->hostdata = NULL;
 	}
+
+	spin_unlock_irqrestore(&h->devlock, flags);
+	return 0;
+}
+
+/* configure scsi device based on internal per-device structure */
+static int hpsa_slave_configure(struct scsi_device *sdev)
+{
+	struct hpsa_scsi_dev_t *sd;
+	unsigned long flags;
+	struct ctlr_info *h;
+
+	h = sdev_to_hba(sdev);
+	spin_lock_irqsave(&h->devlock, flags);
+	sd = sdev->hostdata;
+	if (sd && sd->expose_state & HPSA_NO_ULD_ATTACH)
+		sdev->no_uld_attach = 1;
+
 	spin_unlock_irqrestore(&h->devlock, flags);
 	return 0;
 }
@@ -1538,6 +2037,46 @@ static void hpsa_slave_destroy(struct scsi_device *sdev)
 	/* nothing to do. */
 }
 
+static void hpsa_free_ioaccel2_sg_chain_blocks(struct ctlr_info *h)
+{
+	int i;
+
+	if (!h->ioaccel2_cmd_sg_list)
+		return;
+	for (i = 0; i < h->nr_cmds; i++) {
+		kfree(h->ioaccel2_cmd_sg_list[i]);
+		h->ioaccel2_cmd_sg_list[i] = NULL;
+	}
+	kfree(h->ioaccel2_cmd_sg_list);
+	h->ioaccel2_cmd_sg_list = NULL;
+}
+
+static int hpsa_allocate_ioaccel2_sg_chain_blocks(struct ctlr_info *h)
+{
+	int i;
+
+	if (h->chainsize <= 0)
+		return 0;
+
+	h->ioaccel2_cmd_sg_list = 
+		kzalloc(sizeof(*h->ioaccel2_cmd_sg_list) * h->nr_cmds,
+		GFP_KERNEL);
+	if (!h->ioaccel2_cmd_sg_list)
+		return -ENOMEM;
+	for (i = 0; i < h->nr_cmds; i++) {
+		h->ioaccel2_cmd_sg_list[i] = 
+			kmalloc(sizeof(*h->ioaccel2_cmd_sg_list[i]) *
+			h->maxsgentries, GFP_KERNEL);
+		if (!h->ioaccel2_cmd_sg_list[i])
+			goto clean;
+	}
+	return 0;
+
+clean:
+	hpsa_free_ioaccel2_sg_chain_blocks(h);
+	return -ENOMEM;
+}
+
 static void hpsa_free_sg_chain_blocks(struct ctlr_info *h)
 {
 	int i;
@@ -1552,7 +2091,7 @@ static void hpsa_free_sg_chain_blocks(struct ctlr_info *h)
 	h->cmd_sg_list = NULL;
 }
 
-static int hpsa_allocate_sg_chain_blocks(struct ctlr_info *h)
+static int hpsa_alloc_sg_chain_blocks(struct ctlr_info *h)
 {
 	int i;
 
@@ -1580,6 +2119,39 @@ clean:
 	return -ENOMEM;
 }
 
+static int hpsa_map_ioaccel2_sg_chain_block(struct ctlr_info *h,
+	struct io_accel2_cmd *cp, struct CommandList *c)
+{
+	struct ioaccel2_sg_element *chain_block;
+	u64 temp64;
+	u32 chain_size;
+
+	chain_block = h->ioaccel2_cmd_sg_list[c->cmdindex];      
+	chain_size = le32_to_cpu(cp->data_len);
+	temp64 = pci_map_single(h->pdev, chain_block, chain_size,
+				PCI_DMA_TODEVICE);
+	if (hpsa_dma_mapping_error(&h->pdev->dev, temp64)) {
+		/* prevent subsequent unmapping */
+		cp->sg->address = 0;
+		return -1;
+	}
+	cp->sg->address = (u64) cpu_to_le64(temp64);
+	return 0;
+}
+
+static void hpsa_unmap_ioaccel2_sg_chain_block(struct ctlr_info *h,
+	struct io_accel2_cmd *cp)
+{
+	struct ioaccel2_sg_element *chain_sg;
+	u64 temp64;
+	u32 chain_size;
+
+	chain_sg = cp->sg;
+	temp64 = le64_to_cpu(chain_sg->address);
+	chain_size = le32_to_cpu(cp->data_len);
+	pci_unmap_single(h->pdev, temp64, chain_size, PCI_DMA_TODEVICE);
+}
+
 static int hpsa_map_sg_chain_block(struct ctlr_info *h,
 	struct CommandList *c)
 {
@@ -1591,13 +2163,13 @@ static int hpsa_map_sg_chain_block(struct ctlr_info *h,
 	chain_block = h->cmd_sg_list[c->cmdindex];
 	chain_sg->Ext = cpu_to_le32(HPSA_SG_CHAIN);
 	chain_len = sizeof(*chain_sg) *
-		(le16_to_cpu(c->Header.SGTotal) - h->max_cmd_sg_entries);
+		(c->Header.SGTotal - h->max_cmd_sg_entries);
 	chain_sg->Len = cpu_to_le32(chain_len);
 	temp64 = pci_map_single(h->pdev, chain_block, chain_len,
 				PCI_DMA_TODEVICE);
-	if (dma_mapping_error(&h->pdev->dev, temp64)) {
+	if (hpsa_dma_mapping_error(&h->pdev->dev, temp64)) {
 		/* prevent subsequent unmapping */
-		chain_sg->Addr = cpu_to_le64(0);
+		chain_sg->Addr = 0;
 		return -1;
 	}
 	chain_sg->Addr = cpu_to_le64(temp64);
@@ -1618,7 +2190,7 @@ static void hpsa_unmap_sg_chain_block(struct ctlr_info *h,
 }
 
 
-/* Decode the various types of errors on ioaccel2 path.
+/* Decode the various types of errors on ioaccel2 path. 
  * Return 1 for any error that should generate a RAID path retry.
  * Return 0 for errors that don't require a RAID path retry.
  */
@@ -1629,6 +2201,7 @@ static int handle_ioaccel_mode2_error(struct ctlr_info *h,
 {
 	int data_len;
 	int retry = 0;
+	u32 ioaccel2_resid = 0;
 
 	switch (c2->error_data.serv_response) {
 	case IOACCEL2_SERV_RESPONSE_COMPLETE:
@@ -1636,9 +2209,6 @@ static int handle_ioaccel_mode2_error(struct ctlr_info *h,
 		case IOACCEL2_STATUS_SR_TASK_COMP_GOOD:
 			break;
 		case IOACCEL2_STATUS_SR_TASK_COMP_CHK_COND:
-			dev_warn(&h->pdev->dev,
-				"%s: task complete with check condition.\n",
-				"HP SSD Smart Path");
 			cmd->result |= SAM_STAT_CHECK_CONDITION;
 			if (c2->error_data.data_present !=
 					IOACCEL2_SENSE_DATA_PRESENT) {
@@ -1651,65 +2221,65 @@ static int handle_ioaccel_mode2_error(struct ctlr_info *h,
 			if (data_len > SCSI_SENSE_BUFFERSIZE)
 				data_len = SCSI_SENSE_BUFFERSIZE;
 			if (data_len > sizeof(c2->error_data.sense_data_buff))
-				data_len =
-					sizeof(c2->error_data.sense_data_buff);
+				data_len = sizeof(c2->error_data.sense_data_buff);
 			memcpy(cmd->sense_buffer,
 				c2->error_data.sense_data_buff, data_len);
 			retry = 1;
 			break;
 		case IOACCEL2_STATUS_SR_TASK_COMP_BUSY:
-			dev_warn(&h->pdev->dev,
-				"%s: task complete with BUSY status.\n",
-				"HP SSD Smart Path");
 			retry = 1;
 			break;
 		case IOACCEL2_STATUS_SR_TASK_COMP_RES_CON:
-			dev_warn(&h->pdev->dev,
-				"%s: task complete with reservation conflict.\n",
-				"HP SSD Smart Path");
 			retry = 1;
 			break;
 		case IOACCEL2_STATUS_SR_TASK_COMP_SET_FULL:
-			/* Make scsi midlayer do unlimited retries */
-			cmd->result = DID_IMM_RETRY << 16;
+			retry = 1;
 			break;
 		case IOACCEL2_STATUS_SR_TASK_COMP_ABORTED:
-			dev_warn(&h->pdev->dev,
-				"%s: task complete with aborted status.\n",
-				"HP SSD Smart Path");
 			retry = 1;
 			break;
 		default:
-			dev_warn(&h->pdev->dev,
-				"%s: task complete with unrecognized status: 0x%02x\n",
-				"HP SSD Smart Path", c2->error_data.status);
 			retry = 1;
 			break;
 		}
 		break;
 	case IOACCEL2_SERV_RESPONSE_FAILURE:
-		/* don't expect to get here. */
-		dev_warn(&h->pdev->dev,
-			"unexpected delivery or target failure, status = 0x%02x\n",
-			c2->error_data.status);
-		retry = 1;
+		switch (c2->error_data.status) {
+		case IOACCEL2_STATUS_SR_IO_ERROR:
+		case IOACCEL2_STATUS_SR_IO_ABORTED:
+		case IOACCEL2_STATUS_SR_OVERRUN:
+			retry = 1;
+			break;
+		case IOACCEL2_STATUS_SR_UNDERRUN:
+			cmd->result = (DID_OK << 16);
+			cmd->result |= (COMMAND_COMPLETE << 8);
+			ioaccel2_resid = c2->error_data.resid_cnt[3] << 24;
+			ioaccel2_resid |= c2->error_data.resid_cnt[2] << 16;
+			ioaccel2_resid |= c2->error_data.resid_cnt[1] << 8;
+			ioaccel2_resid |= c2->error_data.resid_cnt[0];
+			scsi_set_resid(cmd, ioaccel2_resid);
+			break;
+		case IOACCEL2_STATUS_SR_NO_PATH_TO_DEVICE:
+		case IOACCEL2_STATUS_SR_INVALID_DEVICE:
+		case IOACCEL2_STATUS_SR_IOACCEL_DISABLED:
+			/* We will get an event from ctlr to trigger rescan */
+			retry = 1;
+			break;
+		default:
+			retry = 1;
+		}
 		break;
 	case IOACCEL2_SERV_RESPONSE_TMF_COMPLETE:
 		break;
 	case IOACCEL2_SERV_RESPONSE_TMF_SUCCESS:
 		break;
 	case IOACCEL2_SERV_RESPONSE_TMF_REJECTED:
-		dev_warn(&h->pdev->dev, "task management function rejected.\n");
-		retry = 1;
+		/* TODO: Figure out what to do here. */
 		break;
 	case IOACCEL2_SERV_RESPONSE_TMF_WRONG_LUN:
-		dev_warn(&h->pdev->dev, "task management function invalid LUN\n");
+		/* TODO: Figure out what to do here. */
 		break;
 	default:
-		dev_warn(&h->pdev->dev,
-			"%s: Unrecognized server response: 0x%02x\n",
-			"HP SSD Smart Path",
-			c2->error_data.serv_response);
 		retry = 1;
 		break;
 	}
@@ -1717,6 +2287,85 @@ static int handle_ioaccel_mode2_error(struct ctlr_info *h,
 	return retry;	/* retry on raid path? */
 }
 
+static void hpsa_cmd_resolve_events(struct ctlr_info *h,
+		struct CommandList *c)
+{
+	bool do_wake = false;
+
+	/* Prevent the following race in the abort handler:
+	 *
+	 * 1. LLD is requested to abort a SCSI command
+	 * 2. The SCSI command completes
+	 * 3. The struct CommandList associated with step 2 is made available
+	 * 4. New I/O request to LLD to another LUN re-uses struct CommandList
+	 * 5. Abort handler follows scsi_cmnd->host_scribble and
+	 *    finds struct CommandList and tries to aborts it
+	 * Now we have aborted the wrong command.
+	 *
+	 * Reset c->scsi_cmd here so that the abort or reset handler will know
+	 * this command has completed.  Then, check to see if the handler is
+	 * waiting for this command, and, if so, wake it.
+	 */
+	c->scsi_cmd = SCSI_CMD_IDLE;
+	mb();	/* Declare command idle before checking for pending events. */
+	if (c->abort_pending) {
+		do_wake = true;
+		c->abort_pending = false;
+	}
+	if (c->reset_pending) {
+		unsigned long flags;
+		struct hpsa_scsi_dev_t *dev;
+
+		/* There appears to be a reset pending; lock the lock and
+		 * reconfirm.  If so, then decrement the count of outstanding
+		 * commands and wake the reset command if this is the last one.
+		 */
+		spin_lock_irqsave(&h->lock, flags);
+		dev = c->reset_pending;		/* Re-fetch under the lock. */
+		if (dev && atomic_dec_and_test(&dev->reset_cmds_out))
+			 do_wake = true;
+		c->reset_pending = NULL;
+		spin_unlock_irqrestore(&h->lock, flags);
+	}
+
+	if (do_wake)
+		wake_up_all(&h->event_sync_wait_queue);
+}
+
+static void hpsa_cmd_resolve_and_free(struct ctlr_info *h,
+				      struct CommandList *c)
+{
+	hpsa_cmd_resolve_events(h, c);
+	cmd_free(h, c);
+}
+
+static void hpsa_cmd_free_and_done(struct ctlr_info *h,
+		struct CommandList *c, struct scsi_cmnd *cmd)
+{
+	hpsa_cmd_resolve_and_free(h, c);
+	cmd->scsi_done(cmd);
+}
+
+static void hpsa_retry_cmd(struct ctlr_info *h, struct CommandList *c)
+{
+	INIT_WORK(&c->work, hpsa_command_resubmit_worker);
+	queue_work_on(raw_smp_processor_id(), h->resubmit_wq, &c->work);
+}
+
+static void hpsa_set_scsi_cmd_aborted(struct scsi_cmnd *cmd)
+{
+	cmd->result = DID_ABORT << 16;
+}
+
+static void hpsa_cmd_abort_and_free(struct ctlr_info *h, struct CommandList *c,
+				    struct scsi_cmnd *cmd)
+{
+	hpsa_set_scsi_cmd_aborted(cmd);
+	dev_warn(&h->pdev->dev, "CDB %16phN was aborted with status 0x%x\n",
+			 c->Request.CDB, c->err_info->ScsiStatus);
+	hpsa_cmd_resolve_and_free(h, c);
+}
+
 static void process_ioaccel2_completion(struct ctlr_info *h,
 		struct CommandList *c, struct scsi_cmnd *cmd,
 		struct hpsa_scsi_dev_t *dev)
@@ -1725,11 +2374,8 @@ static void process_ioaccel2_completion(struct ctlr_info *h,
 
 	/* check for good status */
 	if (likely(c2->error_data.serv_response == 0 &&
-			c2->error_data.status == 0)) {
-		cmd_free(h, c);
-		cmd->scsi_done(cmd);
-		return;
-	}
+			c2->error_data.status == 0))
+		return hpsa_cmd_free_and_done(h, c, cmd);
 
 	/* Any RAID offload error results in retry which will use
 	 * the normal I/O path so the controller can handle whatever's
@@ -1741,19 +2387,40 @@ static void process_ioaccel2_completion(struct ctlr_info *h,
 		if (c2->error_data.status ==
 			IOACCEL2_STATUS_SR_IOACCEL_DISABLED)
 			dev->offload_enabled = 0;
-		goto retry_cmd;
-	}
 
+		return hpsa_retry_cmd(h, c);
+	}
 	if (handle_ioaccel_mode2_error(h, c, cmd, c2))
-		goto retry_cmd;
+		return hpsa_retry_cmd(h, c);
 
-	cmd_free(h, c);
-	cmd->scsi_done(cmd);
-	return;
+	return hpsa_cmd_free_and_done(h, c, cmd);
+}
 
-retry_cmd:
-	INIT_WORK(&c->work, hpsa_command_resubmit_worker);
-	queue_work_on(raw_smp_processor_id(), h->resubmit_wq, &c->work);
+/* Returns 0 on success, < 0 otherwise. */
+static int hpsa_evaluate_tmf_status(struct ctlr_info *h,
+					struct CommandList *cp)
+{
+	u8 tmf_status = cp->err_info->ScsiStatus;
+
+	switch (tmf_status) {
+	case CISS_TMF_COMPLETE:
+		/* CISS_TMF_COMPLETE never happens, instead,
+		 * ei->CommandStatus == 0 for this case.
+		 */
+	case CISS_TMF_SUCCESS:
+		return 0;
+	case CISS_TMF_INVALID_FRAME:
+	case CISS_TMF_NOT_SUPPORTED:
+	case CISS_TMF_FAILED:
+	case CISS_TMF_WRONG_LUN:
+	case CISS_TMF_OVERLAPPED_TAG:
+		break;
+	default:
+		dev_warn(&h->pdev->dev, "Unknown TMF status: %02x\n",
+				tmf_status);
+		break;
+	}
+	return -tmf_status;
 }
 
 static void complete_scsi_command(struct CommandList *cp)
@@ -1762,61 +2429,66 @@ static void complete_scsi_command(struct CommandList *cp)
 	struct ctlr_info *h;
 	struct ErrorInfo *ei;
 	struct hpsa_scsi_dev_t *dev;
+	struct io_accel2_cmd *c2;
 
-	unsigned char sense_key;
-	unsigned char asc;      /* additional sense code */
-	unsigned char ascq;     /* additional sense code qualifier */
+	int sense_key;
+	int asc;      /* additional sense code */
+	int ascq;     /* additional sense code qualifier */
 	unsigned long sense_data_size;
 
 	ei = cp->err_info;
 	cmd = cp->scsi_cmd;
 	h = cp->h;
 	dev = cmd->device->hostdata;
+	c2 = &h->ioaccel2_cmd_pool[cp->cmdindex];
 
 	scsi_dma_unmap(cmd); /* undo the DMA mappings */
 	if ((cp->cmd_type == CMD_SCSI) &&
-		(le16_to_cpu(cp->Header.SGTotal) > h->max_cmd_sg_entries))
+		(cp->Header.SGTotal > h->max_cmd_sg_entries))
 		hpsa_unmap_sg_chain_block(h, cp);
 
+	if ((cp->cmd_type == CMD_IOACCEL2) &&
+		(c2->sg[0].chain_indicator == IOACCEL2_CHAIN))
+		hpsa_unmap_ioaccel2_sg_chain_block(h, c2);
+
 	cmd->result = (DID_OK << 16); 		/* host byte */
 	cmd->result |= (COMMAND_COMPLETE << 8);	/* msg byte */
 
 	if (cp->cmd_type == CMD_IOACCEL2 || cp->cmd_type == CMD_IOACCEL1)
 		atomic_dec(&cp->phys_disk->ioaccel_cmds_out);
 
-	if (cp->cmd_type == CMD_IOACCEL2)
-		return process_ioaccel2_completion(h, cp, cmd, dev);
-
-	cmd->result |= ei->ScsiStatus;
+	/* We check for lockup status here as it may be set for
+	 * CMD_SCSI, CMD_IOACCEL1 and CMD_IOACCEL2 commands by
+	 * fail_all_oustanding_cmds()
+	 */
+	if (unlikely(ei->CommandStatus == CMD_CTLR_LOCKUP)) {
+		/* DID_NO_CONNECT will prevent a retry */
+		cmd->result = DID_NO_CONNECT << 16;
+		return hpsa_cmd_free_and_done(h, cp, cmd);
+	}
 
-	scsi_set_resid(cmd, ei->ResidualCnt);
-	if (ei->CommandStatus == 0) {
-		if (cp->cmd_type == CMD_IOACCEL1)
-			atomic_dec(&cp->phys_disk->ioaccel_cmds_out);
-		cmd_free(h, cp);
-		cmd->scsi_done(cmd);
-		return;
+	if ((unlikely(hpsa_is_pending_event(cp)))) {
+		if (cp->reset_pending)
+			return hpsa_cmd_resolve_and_free(h, cp);
+		if (cp->abort_pending)
+			return hpsa_cmd_abort_and_free(h, cp, cmd);
 	}
 
-	/* copy the sense data */
-	if (SCSI_SENSE_BUFFERSIZE < sizeof(ei->SenseInfo))
-		sense_data_size = SCSI_SENSE_BUFFERSIZE;
-	else
-		sense_data_size = sizeof(ei->SenseInfo);
-	if (ei->SenseLen < sense_data_size)
-		sense_data_size = ei->SenseLen;
+	if (cp->cmd_type == CMD_IOACCEL2)
+		return process_ioaccel2_completion(h, cp, cmd, dev);
 
-	memcpy(cmd->sense_buffer, ei->SenseInfo, sense_data_size);
+	scsi_set_resid(cmd, ei->ResidualCnt);
+	if (ei->CommandStatus == 0)
+		return hpsa_cmd_free_and_done(h, cp, cmd);
 
 	/* For I/O accelerator commands, copy over some fields to the normal
 	 * CISS header used below for error handling.
 	 */
 	if (cp->cmd_type == CMD_IOACCEL1) {
 		struct io_accel1_cmd *c = &h->ioaccel_cmd_pool[cp->cmdindex];
-		cp->Header.SGList = scsi_sg_count(cmd);
-		cp->Header.SGTotal = cpu_to_le16(cp->Header.SGList);
-		cp->Request.CDBLen = le16_to_cpu(c->io_flags) &
-			IOACCEL1_IOFLAGS_CDBLEN_MASK;
+
+		cp->Header.SGList = cp->Header.SGTotal = scsi_sg_count(cmd);
+		cp->Request.CDBLen = c->io_flags & IOACCEL1_IOFLAGS_CDBLEN_MASK;
 		cp->Header.tag = c->tag;
 		memcpy(cp->Header.LUN.LunAddrBytes, c->CISS_LUN, 8);
 		memcpy(cp->Request.CDB, c->CDB, cp->Request.CDBLen);
@@ -1828,10 +2500,7 @@ static void complete_scsi_command(struct CommandList *cp)
 		if (is_logical_dev_addr_mode(dev->scsi3addr)) {
 			if (ei->CommandStatus == CMD_IOACCEL_DISABLED)
 				dev->offload_enabled = 0;
-			INIT_WORK(&cp->work, hpsa_command_resubmit_worker);
-			queue_work_on(raw_smp_processor_id(),
-					h->resubmit_wq, &cp->work);
-			return;
+			return hpsa_retry_cmd(h, cp);
 		}
 	}
 
@@ -1839,14 +2508,18 @@ static void complete_scsi_command(struct CommandList *cp)
 	switch (ei->CommandStatus) {
 
 	case CMD_TARGET_STATUS:
-		if (ei->ScsiStatus) {
-			/* Get sense key */
-			sense_key = 0xf & ei->SenseInfo[2];
-			/* Get additional sense code */
-			asc = ei->SenseInfo[12];
-			/* Get addition sense code qualifier */
-			ascq = ei->SenseInfo[13];
-		}
+		cmd->result |= ei->ScsiStatus;
+		/* copy the sense data */
+		if (SCSI_SENSE_BUFFERSIZE < sizeof(ei->SenseInfo))
+			sense_data_size = SCSI_SENSE_BUFFERSIZE;
+		else
+			sense_data_size = sizeof(ei->SenseInfo);
+		if (ei->SenseLen < sense_data_size)
+			sense_data_size = ei->SenseLen;
+		memcpy(cmd->sense_buffer, ei->SenseInfo, sense_data_size);
+		if (ei->ScsiStatus)
+			decode_sense_data(ei->SenseInfo, sense_data_size,
+				&sense_key, &asc, &ascq);
 		if (ei->ScsiStatus == SAM_STAT_CHECK_CONDITION) {
 			if (sense_key == ABORTED_COMMAND) {
 				cmd->result |= DID_SOFT_ERROR << 16;
@@ -1888,7 +2561,7 @@ static void complete_scsi_command(struct CommandList *cp)
 		break;
 	case CMD_DATA_OVERRUN:
 		dev_warn(&h->pdev->dev,
-			"CDB %16phN data overrun\n", cp->Request.CDB);
+			"CDB " phN16 " data overrun\n", phNbytes16(cp->Request.CDB));
 		break;
 	case CMD_INVALID: {
 		/* print_bytes(cp, sizeof(*cp), 1, 0);
@@ -1904,43 +2577,45 @@ static void complete_scsi_command(struct CommandList *cp)
 		break;
 	case CMD_PROTOCOL_ERR:
 		cmd->result = DID_ERROR << 16;
-		dev_warn(&h->pdev->dev, "CDB %16phN : protocol error\n",
-				cp->Request.CDB);
+		dev_warn(&h->pdev->dev, "CDB " phN16 " : protocol error\n",
+				phNbytes16(cp->Request.CDB));
 		break;
 	case CMD_HARDWARE_ERR:
 		cmd->result = DID_ERROR << 16;
-		dev_warn(&h->pdev->dev, "CDB %16phN : hardware error\n",
-			cp->Request.CDB);
+		dev_warn(&h->pdev->dev, "CDB " phN16 " : hardware error\n",
+			phNbytes16(cp->Request.CDB));
 		break;
 	case CMD_CONNECTION_LOST:
 		cmd->result = DID_ERROR << 16;
-		dev_warn(&h->pdev->dev, "CDB %16phN : connection lost\n",
-			cp->Request.CDB);
+		dev_warn(&h->pdev->dev, "CDB " phN16 " : connection lost\n",
+			phNbytes16(cp->Request.CDB));
 		break;
 	case CMD_ABORTED:
-		cmd->result = DID_ABORT << 16;
-		dev_warn(&h->pdev->dev, "CDB %16phN was aborted with status 0x%x\n",
-				cp->Request.CDB, ei->ScsiStatus);
-		break;
+		/* Return now to avoid calling scsi_done(). */
+		return hpsa_cmd_abort_and_free(h, cp, cmd);
 	case CMD_ABORT_FAILED:
 		cmd->result = DID_ERROR << 16;
-		dev_warn(&h->pdev->dev, "CDB %16phN : abort failed\n",
-			cp->Request.CDB);
+		dev_warn(&h->pdev->dev, "CDB " phN16 " : abort failed\n",
+			phNbytes16(cp->Request.CDB));
 		break;
 	case CMD_UNSOLICITED_ABORT:
 		cmd->result = DID_SOFT_ERROR << 16; /* retry the command */
-		dev_warn(&h->pdev->dev, "CDB %16phN : unsolicited abort\n",
-			cp->Request.CDB);
+		dev_warn(&h->pdev->dev, "CDB " phN16 " : unsolicited abort\n",
+			phNbytes16(cp->Request.CDB));
 		break;
 	case CMD_TIMEOUT:
 		cmd->result = DID_TIME_OUT << 16;
-		dev_warn(&h->pdev->dev, "CDB %16phN timed out\n",
-			cp->Request.CDB);
+		dev_warn(&h->pdev->dev, "CDB " phN16 " timedout\n",
+			phNbytes16(cp->Request.CDB));
 		break;
 	case CMD_UNABORTABLE:
 		cmd->result = DID_ERROR << 16;
 		dev_warn(&h->pdev->dev, "Command unabortable\n");
 		break;
+	case CMD_TMF_STATUS:
+		if (hpsa_evaluate_tmf_status(h, cp)) /* TMF failed? */
+			cmd->result = DID_ERROR << 16;
+		break;
 	case CMD_IOACCEL_DISABLED:
 		/* This only handles the direct pass-through case since RAID
 		 * offload is handled above.  Just attempt a retry.
@@ -1954,8 +2629,8 @@ static void complete_scsi_command(struct CommandList *cp)
 		dev_warn(&h->pdev->dev, "cp %p returned unknown status %x\n",
 				cp, ei->CommandStatus);
 	}
-	cmd_free(h, cp);
-	cmd->scsi_done(cmd);
+
+	return hpsa_cmd_free_and_done(h, cp, cmd);
 }
 
 static void hpsa_pci_unmap(struct pci_dev *pdev,
@@ -1983,8 +2658,8 @@ static int hpsa_map_one(struct pci_dev *pdev,
 		return 0;
 	}
 
-	addr64 = pci_map_single(pdev, buf, buflen, data_direction);
-	if (dma_mapping_error(&pdev->dev, addr64)) {
+	addr64 = (u64) pci_map_single(pdev, buf, buflen, data_direction);
+	if (hpsa_dma_mapping_error(&pdev->dev, addr64)) {
 		/* Prevent subsequent unmap of something never mapped */
 		cp->Header.SGList = 0;
 		cp->Header.SGTotal = cpu_to_le16(0);
@@ -1993,19 +2668,31 @@ static int hpsa_map_one(struct pci_dev *pdev,
 	cp->SG[0].Addr = cpu_to_le64(addr64);
 	cp->SG[0].Len = cpu_to_le32(buflen);
 	cp->SG[0].Ext = cpu_to_le32(HPSA_SG_LAST); /* we are not chaining */
-	cp->Header.SGList = 1;   /* no. SGs contig in this cmd */
-	cp->Header.SGTotal = cpu_to_le16(1); /* total sgs in cmd list */
+	cp->Header.SGList = (u8) 1;   /* no. SGs contig in this cmd */
+	cp->Header.SGTotal = (u16) cpu_to_le16(1); /* total sgs in cmd list */
 	return 0;
 }
 
-static inline void hpsa_scsi_do_simple_cmd_core(struct ctlr_info *h,
-	struct CommandList *c)
+#define NO_TIMEOUT ((unsigned long) -1)
+#define DEFAULT_TIMEOUT (30000) /* milliseconds */
+static int hpsa_scsi_do_simple_cmd_core(struct ctlr_info *h,
+	struct CommandList *c, int reply_queue, unsigned long timeout_msecs)
 {
 	DECLARE_COMPLETION_ONSTACK(wait);
 
 	c->waiting = &wait;
-	enqueue_cmd_and_start_io(h, c);
-	wait_for_completion(&wait);
+	__enqueue_cmd_and_start_io(h, c, reply_queue);
+	if (timeout_msecs == NO_TIMEOUT) {
+		/* TODO: get rid of this no-timeout thing */
+		wait_for_completion_io(&wait);
+		return 0;
+	}
+	if (!wait_for_completion_io_timeout(&wait,
+					msecs_to_jiffies(timeout_msecs))) {
+		dev_warn(&h->pdev->dev, "Command timed out.\n");
+		return -ETIMEDOUT;
+	}
+	return 0;
 }
 
 static u32 lockup_detected(struct ctlr_info *h)
@@ -2020,25 +2707,29 @@ static u32 lockup_detected(struct ctlr_info *h)
 	return rc;
 }
 
-static void hpsa_scsi_do_simple_cmd_core_if_no_lockup(struct ctlr_info *h,
-	struct CommandList *c)
+static int hpsa_scsi_do_simple_cmd(struct ctlr_info *h, struct CommandList *c,
+				   int reply_queue, unsigned long timeout_msecs)
 {
-	/* If controller lockup detected, fake a hardware error. */
-	if (unlikely(lockup_detected(h)))
-		c->err_info->CommandStatus = CMD_HARDWARE_ERR;
-	else
-		hpsa_scsi_do_simple_cmd_core(h, c);
+	if (unlikely(lockup_detected(h))) {
+		c->err_info->CommandStatus = CMD_CTLR_LOCKUP;
+		return 0;
+	}
+	return hpsa_scsi_do_simple_cmd_core(h, c, reply_queue, timeout_msecs);
 }
 
 #define MAX_DRIVER_CMD_RETRIES 25
-static void hpsa_scsi_do_simple_cmd_with_retry(struct ctlr_info *h,
-	struct CommandList *c, int data_direction)
+static int hpsa_scsi_do_simple_cmd_with_retry(struct ctlr_info *h,
+	struct CommandList *c, int data_direction, unsigned long timeout_msecs)
 {
 	int backoff_time = 10, retry_count = 0;
+	int rc;
 
 	do {
 		memset(c->err_info, 0, sizeof(*c->err_info));
-		hpsa_scsi_do_simple_cmd_core(h, c);
+		rc = hpsa_scsi_do_simple_cmd(h, c, DEFAULT_REPLY_QUEUE,
+						  timeout_msecs);
+		if (rc)
+			break;
 		retry_count++;
 		if (retry_count > 3) {
 			msleep(backoff_time);
@@ -2049,6 +2740,9 @@ static void hpsa_scsi_do_simple_cmd_with_retry(struct ctlr_info *h,
 			check_for_busy(h, c)) &&
 			retry_count <= MAX_DRIVER_CMD_RETRIES);
 	hpsa_pci_unmap(h->pdev, c, 1, data_direction);
+	if (retry_count > MAX_DRIVER_CMD_RETRIES)
+		rc = -1; /* FIXME do something better? */
+	return rc;
 }
 
 static void hpsa_print_cmd(struct ctlr_info *h, char *txt,
@@ -2072,16 +2766,21 @@ static void hpsa_scsi_interpret_error(struct ctlr_info *h,
 {
 	const struct ErrorInfo *ei = cp->err_info;
 	struct device *d = &cp->h->pdev->dev;
-	const u8 *sd = ei->SenseInfo;
+	int sense_key, asc, ascq, sense_len;
 
 	switch (ei->CommandStatus) {
 	case CMD_TARGET_STATUS:
-		hpsa_print_cmd(h, "SCSI status", cp);
+		if (ei->SenseLen > sizeof(ei->SenseInfo))
+			sense_len = sizeof(ei->SenseInfo);
+		else
+			sense_len = ei->SenseLen;
+		decode_sense_data(ei->SenseInfo, sense_len,
+					&sense_key, &asc, &ascq);
 		if (ei->ScsiStatus == SAM_STAT_CHECK_CONDITION)
-			dev_warn(d, "SCSI Status = 02, Sense key = %02x, ASC = %02x, ASCQ = %02x\n",
-				sd[2] & 0x0f, sd[12], sd[13]);
+			dev_warn(d, "SCSI Status = 0x02, Sense key = 0x%02x, ASC = 0x%02x, ASCQ = 0x%02x\n",
+				sense_key, asc, ascq);
 		else
-			dev_warn(d, "SCSI Status = %02x\n", ei->ScsiStatus);
+			dev_warn(d, "SCSI Status = 0x%02x\n", ei->ScsiStatus);
 		if (ei->ScsiStatus == 0)
 			dev_warn(d, "SCSI status is abnormally zero.  "
 			"(probably indicates selection timeout "
@@ -2125,6 +2824,9 @@ static void hpsa_scsi_interpret_error(struct ctlr_info *h,
 	case CMD_UNABORTABLE:
 		hpsa_print_cmd(h, "unabortable", cp);
 		break;
+	case CMD_CTLR_LOCKUP:
+		hpsa_print_cmd(h, "controller lockup detected", cp);
+		break;
 	default:
 		hpsa_print_cmd(h, "unknown status", cp);
 		dev_warn(d, "Unknown command status %x\n",
@@ -2141,18 +2843,15 @@ static int hpsa_scsi_do_inquiry(struct ctlr_info *h, unsigned char *scsi3addr,
 	struct ErrorInfo *ei;
 
 	c = cmd_alloc(h);
-
-	if (c == NULL) {
-		dev_warn(&h->pdev->dev, "cmd_alloc returned NULL!\n");
-		return -ENOMEM;
-	}
-
 	if (fill_cmd(c, HPSA_INQUIRY, h, buf, bufsize,
 			page, scsi3addr, TYPE_CMD)) {
 		rc = -1;
 		goto out;
 	}
-	hpsa_scsi_do_simple_cmd_with_retry(h, c, PCI_DMA_FROMDEVICE);
+	rc = hpsa_scsi_do_simple_cmd_with_retry(h, c,
+		PCI_DMA_FROMDEVICE, NO_TIMEOUT);
+	if (rc)
+		goto out;
 	ei = c->err_info;
 	if (ei->CommandStatus != 0 && ei->CommandStatus != CMD_DATA_UNDERRUN) {
 		hpsa_scsi_interpret_error(h, c);
@@ -2163,26 +2862,26 @@ out:
 	return rc;
 }
 
+
 static int hpsa_bmic_ctrl_mode_sense(struct ctlr_info *h,
-		unsigned char *scsi3addr, unsigned char page,
-		struct bmic_controller_parameters *buf, size_t bufsize)
+			unsigned char *scsi3addr, unsigned char page,
+			struct bmic_controller_parameters *buf, size_t bufsize)
 {
 	int rc = IO_OK;
 	struct CommandList *c;
 	struct ErrorInfo *ei;
 
 	c = cmd_alloc(h);
-	if (c == NULL) {			/* trouble... */
-		dev_warn(&h->pdev->dev, "cmd_alloc returned NULL!\n");
-		return -ENOMEM;
-	}
 
 	if (fill_cmd(c, BMIC_SENSE_CONTROLLER_PARAMETERS, h, buf, bufsize,
 			page, scsi3addr, TYPE_CMD)) {
 		rc = -1;
 		goto out;
 	}
-	hpsa_scsi_do_simple_cmd_with_retry(h, c, PCI_DMA_FROMDEVICE);
+	rc = hpsa_scsi_do_simple_cmd_with_retry(h, c,
+					PCI_DMA_FROMDEVICE, NO_TIMEOUT);
+	if (rc)
+		goto out;
 	ei = c->err_info;
 	if (ei->CommandStatus != 0 && ei->CommandStatus != CMD_DATA_UNDERRUN) {
 		hpsa_scsi_interpret_error(h, c);
@@ -2191,10 +2890,10 @@ static int hpsa_bmic_ctrl_mode_sense(struct ctlr_info *h,
 out:
 	cmd_free(h, c);
 	return rc;
-	}
+}
 
-static int hpsa_send_reset(struct ctlr_info *h, unsigned char *scsi3addr,
-	u8 reset_type)
+static int hpsa_send_reset(struct ctlr_info *h, unsigned char *scsi3addr, 
+	int reset_type, int reply_queue)
 {
 	int rc = IO_OK;
 	struct CommandList *c;
@@ -2202,16 +2901,13 @@ static int hpsa_send_reset(struct ctlr_info *h, unsigned char *scsi3addr,
 
 	c = cmd_alloc(h);
 
-	if (c == NULL) {			/* trouble... */
-		dev_warn(&h->pdev->dev, "cmd_alloc returned NULL!\n");
-		return -ENOMEM;
-	}
-
 	/* fill_cmd can't fail here, no data buffer to map. */
-	(void) fill_cmd(c, HPSA_DEVICE_RESET_MSG, h, NULL, 0, 0,
-			scsi3addr, TYPE_MSG);
-	c->Request.CDB[1] = reset_type; /* fill_cmd defaults to LUN reset */
-	hpsa_scsi_do_simple_cmd_core(h, c);
+	(void) fill_cmd(c, reset_type, h, NULL, 0, 0, scsi3addr, TYPE_MSG);
+	rc = hpsa_scsi_do_simple_cmd(h, c, reply_queue, NO_TIMEOUT);
+	if (rc) {
+		dev_warn(&h->pdev->dev, "Failed to send reset command\n");
+		goto out;
+	}
 	/* no unmap needed here because no data xfer. */
 
 	ei = c->err_info;
@@ -2219,17 +2915,134 @@ static int hpsa_send_reset(struct ctlr_info *h, unsigned char *scsi3addr,
 		hpsa_scsi_interpret_error(h, c);
 		rc = -1;
 	}
+out:
 	cmd_free(h, c);
 	return rc;
 }
 
-static void hpsa_get_raid_level(struct ctlr_info *h,
-	unsigned char *scsi3addr, unsigned char *raid_level)
+static bool hpsa_cmd_dev_match(struct ctlr_info *h, struct CommandList *c,
+			       struct hpsa_scsi_dev_t *dev,
+			       unsigned char *scsi3addr)
 {
-	int rc;
-	unsigned char *buf;
+	int i;
+	bool match = false;
+	struct io_accel2_cmd *c2 = &h->ioaccel2_cmd_pool[c->cmdindex];
+	struct hpsa_tmf_struct *ac = (struct hpsa_tmf_struct *) c2;
 
-	*raid_level = RAID_UNKNOWN;
+	if (hpsa_is_cmd_idle(c))
+		return false;
+
+	switch (c->cmd_type) {
+	case CMD_SCSI:
+	case CMD_IOCTL_PEND:
+		match = !memcmp(scsi3addr, &c->Header.LUN.LunAddrBytes,
+				sizeof(c->Header.LUN.LunAddrBytes));
+		break;
+
+	case CMD_IOACCEL1:
+	case CMD_IOACCEL2:
+		if (c->phys_disk == dev) {
+			/* HBA mode match */
+			match = true;
+		} else {
+			/* Possible RAID mode -- check each phys dev. */
+			/* FIXME:  Do we need to take out a lock here?  If
+			 * so, we could just call hpsa_get_pdisk_of_ioaccel2()
+			 * instead. */
+			for (i = 0; i < dev->nphysical_disks && !match; i++) {
+				/* FIXME: an alternate test might be
+				 *
+				 * match = dev->phys_disk[i]->ioaccel_handle
+				 *              == c2->scsi_nexus;      */
+				match = dev->phys_disk[i] == c->phys_disk;
+			}
+		}
+		break;
+
+	case IOACCEL2_TMF:
+		for (i = 0; i < dev->nphysical_disks && !match; i++) {
+			match = dev->phys_disk[i]->ioaccel_handle ==
+					ac->it_nexus;
+		}
+		break;
+
+	case 0:		/* The command is in the middle of being initialized. */
+		match = false;
+		break;
+
+	default:
+		dev_err(&h->pdev->dev, "unexpected cmd_type: %d\n",
+			c->cmd_type);
+		BUG();
+	}
+
+	return match;
+}
+
+static int hpsa_do_reset(struct ctlr_info *h, struct hpsa_scsi_dev_t *dev,
+	unsigned char *scsi3addr, u8 reset_type, int reply_queue)
+{
+	int i;
+	int rc = 0;
+
+	/* We can really only handle one reset at a time */
+	if (mutex_lock_interruptible(&h->reset_mutex) == -EINTR) {
+		dev_warn(&h->pdev->dev, "concurrent reset wait interrupted.\n");
+		return -EINTR;
+	}
+
+	BUG_ON(atomic_read(&dev->reset_cmds_out) != 0);
+
+	for (i = 0; i < h->nr_cmds; i++) {
+		struct CommandList *c = h->cmd_pool + i;
+		int refcount = atomic_inc_return(&c->refcount);
+
+		if (refcount > 1 && hpsa_cmd_dev_match(h, c, dev, scsi3addr)) {
+			unsigned long flags;
+
+			/*
+			 * Mark the target command as having a reset pending,
+			 * then lock a lock so that the command cannot complete
+			 * while we're considering it.  If the command is not
+			 * idle then count it; otherwise revoke the event.
+			 */
+			c->reset_pending = dev;
+			spin_lock_irqsave(&h->lock, flags);	/* Implied MB */
+			if (!hpsa_is_cmd_idle(c))
+				atomic_inc(&dev->reset_cmds_out);
+			else
+				c->reset_pending = NULL;
+			spin_unlock_irqrestore(&h->lock, flags);
+		}
+
+		cmd_free(h, c);
+	}
+	
+	rc = hpsa_send_reset(h, scsi3addr, reset_type, reply_queue);
+	if (!rc) {
+		wait_event(h->event_sync_wait_queue,
+			atomic_read(&dev->reset_cmds_out) == 0 ||
+			lockup_detected(h));
+		if (lockup_detected(h)) {
+			atomic_set(&dev->reset_cmds_out, 0);
+			dev_warn(&h->pdev->dev, "Controller lockup detected during reset wait\n");
+			mutex_unlock(&h->reset_mutex);
+			rc = -ENODEV;
+		}
+	} else
+		atomic_set(&dev->reset_cmds_out, 0);
+
+	mutex_unlock(&h->reset_mutex);
+	return rc;
+}
+
+static void hpsa_get_raid_level(struct ctlr_info *h,
+	unsigned char *scsi3addr, unsigned char *raid_level)
+{
+	int rc;
+	unsigned char *buf;
+
+	*raid_level = RAID_UNKNOWN;
 	buf = kzalloc(64, GFP_KERNEL);
 	if (!buf)
 		return;
@@ -2282,21 +3095,22 @@ static void hpsa_debug_map_buff(struct ctlr_info *h, int rc,
 			le16_to_cpu(map_buff->row_cnt));
 	dev_info(&h->pdev->dev, "layout_map_count = %u\n",
 			le16_to_cpu(map_buff->layout_map_count));
-	dev_info(&h->pdev->dev, "flags = 0x%x\n",
+	dev_info(&h->pdev->dev, "flags = %u\n",
 			le16_to_cpu(map_buff->flags));
-	dev_info(&h->pdev->dev, "encrypytion = %s\n",
-			le16_to_cpu(map_buff->flags) &
-			RAID_MAP_FLAG_ENCRYPT_ON ?  "ON" : "OFF");
+	if (map_buff->flags & RAID_MAP_FLAG_ENCRYPT_ON ) 
+		dev_info(&h->pdev->dev, "encrypytion = ON\n");
+	else 	
+		dev_info(&h->pdev->dev, "encrypytion = OFF\n");
 	dev_info(&h->pdev->dev, "dekindex = %u\n",
 			le16_to_cpu(map_buff->dekindex));
+
 	map_cnt = le16_to_cpu(map_buff->layout_map_count);
 	for (map = 0; map < map_cnt; map++) {
 		dev_info(&h->pdev->dev, "Map%u:\n", map);
 		row_cnt = le16_to_cpu(map_buff->row_cnt);
 		for (row = 0; row < row_cnt; row++) {
 			dev_info(&h->pdev->dev, "  Row%u:\n", row);
-			disks_per_row =
-				le16_to_cpu(map_buff->data_disks_per_row);
+			disks_per_row = le16_to_cpu(map_buff->data_disks_per_row);
 			for (col = 0; col < disks_per_row; col++, dd++)
 				dev_info(&h->pdev->dev,
 					"    D%02u: h=0x%04x xor=%u,%u\n",
@@ -2328,24 +3142,25 @@ static int hpsa_get_raid_map(struct ctlr_info *h,
 	struct ErrorInfo *ei;
 
 	c = cmd_alloc(h);
-	if (c == NULL) {
-		dev_warn(&h->pdev->dev, "cmd_alloc returned NULL!\n");
-		return -ENOMEM;
-	}
+
 	if (fill_cmd(c, HPSA_GET_RAID_MAP, h, &this_device->raid_map,
 			sizeof(this_device->raid_map), 0,
 			scsi3addr, TYPE_CMD)) {
-		dev_warn(&h->pdev->dev, "Out of memory in hpsa_get_raid_map()\n");
+		dev_warn(&h->pdev->dev, "hpsa_get_raid_map fill_cmd failed\n");
 		cmd_free(h, c);
-		return -ENOMEM;
+		return -1;
 	}
-	hpsa_scsi_do_simple_cmd_with_retry(h, c, PCI_DMA_FROMDEVICE);
+	rc = hpsa_scsi_do_simple_cmd_with_retry(h, c,
+			PCI_DMA_FROMDEVICE, NO_TIMEOUT);
+	if (rc)
+		goto out;
 	ei = c->err_info;
 	if (ei->CommandStatus != 0 && ei->CommandStatus != CMD_DATA_UNDERRUN) {
 		hpsa_scsi_interpret_error(h, c);
 		cmd_free(h, c);
 		return -1;
 	}
+out:
 	cmd_free(h, c);
 
 	/* @todo in the future, dynamically allocate RAID map memory */
@@ -2367,6 +3182,7 @@ static int hpsa_bmic_id_physical_device(struct ctlr_info *h,
 	struct ErrorInfo *ei;
 
 	c = cmd_alloc(h);
+
 	rc = fill_cmd(c, BMIC_IDENTIFY_PHYSICAL_DEVICE, h, buf, bufsize,
 		0, RAID_CTLR_LUNID, TYPE_CMD);
 	if (rc)
@@ -2375,7 +3191,9 @@ static int hpsa_bmic_id_physical_device(struct ctlr_info *h,
 	c->Request.CDB[2] = bmic_device_index & 0xff;
 	c->Request.CDB[9] = (bmic_device_index >> 8) & 0xff;
 
-	hpsa_scsi_do_simple_cmd_with_retry(h, c, PCI_DMA_FROMDEVICE);
+	rc = hpsa_scsi_do_simple_cmd_with_retry(h, c, PCI_DMA_FROMDEVICE, NO_TIMEOUT);
+	if (rc)
+		goto out;
 	ei = c->err_info;
 	if (ei->CommandStatus != 0 && ei->CommandStatus != CMD_DATA_UNDERRUN) {
 		hpsa_scsi_interpret_error(h, c);
@@ -2389,20 +3207,20 @@ out:
 static int hpsa_vpd_page_supported(struct ctlr_info *h,
 	unsigned char scsi3addr[], u8 page)
 {
-	int rc;
+        int rc;
 	int i;
 	int pages;
-	unsigned char *buf, bufsize;
+        unsigned char *buf, bufsize;
 
-	buf = kzalloc(256, GFP_KERNEL);
-	if (!buf)
+        buf = kzalloc(256, GFP_KERNEL);
+        if (!buf)
 		return 0;
 
 	/* Get the size of the page list first */
-	rc = hpsa_scsi_do_inquiry(h, scsi3addr,
+        rc = hpsa_scsi_do_inquiry(h, scsi3addr,
 				VPD_PAGE | HPSA_VPD_SUPPORTED_PAGES,
 				buf, HPSA_VPD_HEADER_SZ);
-	if (rc != 0)
+        if (rc != 0)
 		goto exit_unsupported;
 	pages = buf[3];
 	if ((pages + HPSA_VPD_HEADER_SZ) <= 255)
@@ -2411,10 +3229,10 @@ static int hpsa_vpd_page_supported(struct ctlr_info *h,
 		bufsize = 255;
 
 	/* Get the whole VPD page list */
-	rc = hpsa_scsi_do_inquiry(h, scsi3addr,
+        rc = hpsa_scsi_do_inquiry(h, scsi3addr,
 				VPD_PAGE | HPSA_VPD_SUPPORTED_PAGES,
 				buf, bufsize);
-	if (rc != 0)
+        if (rc != 0)
 		goto exit_unsupported;
 
 	pages = buf[3];
@@ -2438,6 +3256,7 @@ static void hpsa_get_ioaccel_status(struct ctlr_info *h,
 
 	this_device->offload_config = 0;
 	this_device->offload_enabled = 0;
+	this_device->offload_to_be_enabled = 0;
 
 	buf = kzalloc(64, GFP_KERNEL);
 	if (!buf)
@@ -2461,6 +3280,7 @@ static void hpsa_get_ioaccel_status(struct ctlr_info *h,
 		if (hpsa_get_raid_map(h, scsi3addr, this_device))
 			this_device->offload_enabled = 0;
 	}
+	this_device->offload_to_be_enabled = this_device->offload_enabled;
 out:
 	kfree(buf);
 	return;
@@ -2495,10 +3315,7 @@ static int hpsa_scsi_do_report_luns(struct ctlr_info *h, int logical,
 	struct ErrorInfo *ei;
 
 	c = cmd_alloc(h);
-	if (c == NULL) {			/* trouble... */
-		dev_err(&h->pdev->dev, "cmd_alloc returned NULL!\n");
-		return -1;
-	}
+
 	/* address the controller */
 	memset(scsi3addr, 0, sizeof(scsi3addr));
 	if (fill_cmd(c, logical ? HPSA_REPORT_LOG : HPSA_REPORT_PHYS, h,
@@ -2508,7 +3325,10 @@ static int hpsa_scsi_do_report_luns(struct ctlr_info *h, int logical,
 	}
 	if (extended_response)
 		c->Request.CDB[1] = extended_response;
-	hpsa_scsi_do_simple_cmd_with_retry(h, c, PCI_DMA_FROMDEVICE);
+	rc = hpsa_scsi_do_simple_cmd_with_retry(h, c,
+					PCI_DMA_FROMDEVICE, NO_TIMEOUT);
+	if (rc)
+		goto out;
 	ei = c->err_info;
 	if (ei->CommandStatus != 0 &&
 	    ei->CommandStatus != CMD_DATA_UNDERRUN) {
@@ -2516,7 +3336,6 @@ static int hpsa_scsi_do_report_luns(struct ctlr_info *h, int logical,
 		rc = -1;
 	} else {
 		struct ReportLUNdata *rld = buf;
-
 		if (rld->extended_response_flag != extended_response) {
 			dev_err(&h->pdev->dev,
 				"report luns requested format %u, got %u\n",
@@ -2553,15 +3372,15 @@ static inline void hpsa_set_bus_target_lun(struct hpsa_scsi_dev_t *device,
 
 /* Use VPD inquiry to get details of volume status */
 static int hpsa_get_volume_status(struct ctlr_info *h,
-					unsigned char scsi3addr[])
+	unsigned char scsi3addr[])
 {
 	int rc;
-	int status;
+        int status;
 	int size;
-	unsigned char *buf;
+        unsigned char *buf;
 
-	buf = kzalloc(64, GFP_KERNEL);
-	if (!buf)
+        buf = kzalloc(64, GFP_KERNEL);
+        if (!buf)
 		return HPSA_VPD_LV_STATUS_UNSUPPORTED;
 
 	/* Does controller have VPD for logical volume status? */
@@ -2569,14 +3388,14 @@ static int hpsa_get_volume_status(struct ctlr_info *h,
 		goto exit_failed;
 
 	/* Get the size of the VPD return buffer */
-	rc = hpsa_scsi_do_inquiry(h, scsi3addr, VPD_PAGE | HPSA_VPD_LV_STATUS,
+        rc = hpsa_scsi_do_inquiry(h, scsi3addr, VPD_PAGE | HPSA_VPD_LV_STATUS,
 					buf, HPSA_VPD_HEADER_SZ);
 	if (rc != 0)
 		goto exit_failed;
 	size = buf[3];
 
 	/* Now get the whole VPD buffer */
-	rc = hpsa_scsi_do_inquiry(h, scsi3addr, VPD_PAGE | HPSA_VPD_LV_STATUS,
+        rc = hpsa_scsi_do_inquiry(h, scsi3addr, VPD_PAGE | HPSA_VPD_LV_STATUS,
 					buf, size + HPSA_VPD_HEADER_SZ);
 	if (rc != 0)
 		goto exit_failed;
@@ -2585,7 +3404,7 @@ static int hpsa_get_volume_status(struct ctlr_info *h,
 	kfree(buf);
 	return status;
 exit_failed:
-	kfree(buf);
+        kfree(buf);
 	return HPSA_VPD_LV_STATUS_UNSUPPORTED;
 }
 
@@ -2597,11 +3416,12 @@ exit_failed:
  *     describing why a volume is to be kept offline)
  */
 static int hpsa_volume_offline(struct ctlr_info *h,
-					unsigned char scsi3addr[])
+		unsigned char scsi3addr[])
 {
 	struct CommandList *c;
-	unsigned char *sense, sense_key, asc, ascq;
-	int ldstat = 0;
+	unsigned char *sense;
+	int sense_key, asc, ascq, sense_len;
+	int rc, ldstat = 0;
 	u16 cmd_status;
 	u8 scsi_status;
 #define ASC_LUN_NOT_READY 0x04
@@ -2609,51 +3429,104 @@ static int hpsa_volume_offline(struct ctlr_info *h,
 #define ASCQ_LUN_NOT_READY_INITIALIZING_CMD_REQ 0x02
 
 	c = cmd_alloc(h);
-	if (!c)
+
+	fill_cmd(c, TEST_UNIT_READY, h, NULL, 0, 0, scsi3addr, TYPE_CMD);
+	rc = hpsa_scsi_do_simple_cmd(h, c, DEFAULT_REPLY_QUEUE, NO_TIMEOUT);
+	if (rc) {
+		cmd_free(h, c);
 		return 0;
-	(void) fill_cmd(c, TEST_UNIT_READY, h, NULL, 0, 0, scsi3addr, TYPE_CMD);
-	hpsa_scsi_do_simple_cmd_core(h, c);
+	}
 	sense = c->err_info->SenseInfo;
-	sense_key = sense[2];
-	asc = sense[12];
-	ascq = sense[13];
+	if (c->err_info->SenseLen > sizeof(c->err_info->SenseInfo))
+		sense_len = sizeof(c->err_info->SenseInfo);
+	else
+		sense_len = c->err_info->SenseLen;
+	decode_sense_data(sense, sense_len, &sense_key, &asc, &ascq);
 	cmd_status = c->err_info->CommandStatus;
 	scsi_status = c->err_info->ScsiStatus;
 	cmd_free(h, c);
+
 	/* Is the volume 'not ready'? */
-	if (cmd_status != CMD_TARGET_STATUS ||
-		scsi_status != SAM_STAT_CHECK_CONDITION ||
-		sense_key != NOT_READY ||
+	if (cmd_status != CMD_TARGET_STATUS || 
+		scsi_status != SAM_STAT_CHECK_CONDITION || 
+		sense_key != NOT_READY || 
 		asc != ASC_LUN_NOT_READY)  {
 		return 0;
 	}
-
+		
 	/* Determine the reason for not ready state */
 	ldstat = hpsa_get_volume_status(h, scsi3addr);
 
 	/* Keep volume offline in certain cases: */
 	switch (ldstat) {
-	case HPSA_LV_UNDERGOING_ERASE:
-	case HPSA_LV_UNDERGOING_RPI:
-	case HPSA_LV_PENDING_RPI:
-	case HPSA_LV_ENCRYPTED_NO_KEY:
-	case HPSA_LV_PLAINTEXT_IN_ENCRYPT_ONLY_CONTROLLER:
-	case HPSA_LV_UNDERGOING_ENCRYPTION:
-	case HPSA_LV_UNDERGOING_ENCRYPTION_REKEYING:
-	case HPSA_LV_ENCRYPTED_IN_NON_ENCRYPTED_CONTROLLER:
-		return ldstat;
-	case HPSA_VPD_LV_STATUS_UNSUPPORTED:
-		/* If VPD status page isn't available,
-		 * use ASC/ASCQ to determine state
-		 */
-		if ((ascq == ASCQ_LUN_NOT_READY_FORMAT_IN_PROGRESS) ||
-			(ascq == ASCQ_LUN_NOT_READY_INITIALIZING_CMD_REQ))
+		case HPSA_LV_UNDERGOING_ERASE:
+		case HPSA_LV_UNDERGOING_RPI:
+		case HPSA_LV_PENDING_RPI:
+		case HPSA_LV_ENCRYPTED_NO_KEY:
+		case HPSA_LV_PLAINTEXT_IN_ENCRYPT_ONLY_CONTROLLER:
+		case HPSA_LV_UNDERGOING_ENCRYPTION:
+		case HPSA_LV_UNDERGOING_ENCRYPTION_REKEYING:
+		case HPSA_LV_ENCRYPTED_IN_NON_ENCRYPTED_CONTROLLER:
 			return ldstat;
+		case HPSA_VPD_LV_STATUS_UNSUPPORTED:
+			/* If VPD status page isn't available,
+			 * use ASC/ASCQ to determine state
+			 */
+			if ((ascq == ASCQ_LUN_NOT_READY_FORMAT_IN_PROGRESS) ||
+				(ascq == ASCQ_LUN_NOT_READY_INITIALIZING_CMD_REQ))
+				return ldstat;
+			break;
+		default:
+			break;
+	}
+	return 0;
+}
+
+/* Find out if a logical device supports aborts by simply trying one.
+ * Smart Array may claim not to support aborts on logical drives, but
+ * if a MSA2000 * is connected, the drives on that will be presented
+ * by the Smart Array as logical drives, and aborts may be sent to
+ * those devices successfully.  So the simplest way to find out is
+ * to simply try an abort and see how the device responds.
+ */
+static int hpsa_device_supports_aborts(struct ctlr_info *h, int phys_path,
+					unsigned char *scsi3addr)
+{
+	struct CommandList *c;
+	struct ErrorInfo *ei;
+	int rc = 0;
+
+	u64 tag = (u64) -1; /* bogus tag */
+
+	/* Assume that physical devices that expose the ioaccel path
+ 	 * support aborts.
+ 	 */
+	if (!is_logical_dev_addr_mode(scsi3addr))
+		return phys_path;
+
+	c = cmd_alloc(h);
+
+	(void) fill_cmd(c, HPSA_ABORT_MSG, h, &tag, 0, 0, scsi3addr, TYPE_MSG);
+	(void) hpsa_scsi_do_simple_cmd(h, c, 0, NO_TIMEOUT);
+	/* no unmap needed here because no data xfer. */
+	ei = c->err_info;
+	switch (ei->CommandStatus) {
+	case CMD_INVALID:
+		rc = 0;
+		break;
+	case CMD_UNABORTABLE:
+	case CMD_ABORT_FAILED:
+		rc = 1;
+		break;
+	case CMD_TMF_STATUS:
+		rc = hpsa_evaluate_tmf_status(h, c);
 		break;
 	default:
+		rc = 0;
 		break;
 	}
-	return 0;
+	cmd_free(h, c);
+	return rc;
 }
 
 static int hpsa_update_device_info(struct ctlr_info *h,
@@ -2668,10 +3541,13 @@ static int hpsa_update_device_info(struct ctlr_info *h,
 
 	unsigned char *inq_buff;
 	unsigned char *obdr_sig;
+	int rc = 0;
 
 	inq_buff = kzalloc(OBDR_TAPE_INQ_SIZE, GFP_KERNEL);
-	if (!inq_buff)
+	if (!inq_buff) {
+		rc = -ENOMEM;
 		goto bail_out;
+	}
 
 	/* Do an inquiry to the device to see what it is. */
 	if (hpsa_scsi_do_inquiry(h, scsi3addr, 0, inq_buff,
@@ -2679,6 +3555,7 @@ static int hpsa_update_device_info(struct ctlr_info *h,
 		/* Inquiry failed (msg printed already) */
 		dev_err(&h->pdev->dev,
 			"hpsa_update_device_info: inquiry failed\n");
+		rc = 1;
 		goto bail_out;
 	}
 
@@ -2708,6 +3585,8 @@ static int hpsa_update_device_info(struct ctlr_info *h,
 		this_device->raid_level = RAID_UNKNOWN;
 		this_device->offload_config = 0;
 		this_device->offload_enabled = 0;
+		this_device->offload_to_be_enabled = 0;
+		this_device->hba_ioaccel_enabled = 0;
 		this_device->volume_offline = 0;
 		this_device->queue_depth = h->nr_cmds;
 	}
@@ -2721,13 +3600,38 @@ static int hpsa_update_device_info(struct ctlr_info *h,
 					strncmp(obdr_sig, OBDR_TAPE_SIG,
 						OBDR_SIG_LEN) == 0);
 	}
-
 	kfree(inq_buff);
 	return 0;
 
 bail_out:
 	kfree(inq_buff);
-	return 1;
+	return rc;
+}
+
+static void hpsa_update_device_supports_aborts(struct ctlr_info *h,
+			struct hpsa_scsi_dev_t *dev, u8 *scsi3addr)
+{
+	unsigned long flags;
+	int rc, entry;
+	/*
+	 * See if this device supports aborts.  If we already know
+	 * the device, we already know if it supports aborts, otherwise
+	 * we have to find out if it supports aborts by trying one.
+	 */
+	spin_lock_irqsave(&h->devlock, flags);
+	rc = hpsa_scsi_find_entry(dev, h->dev, h->ndevices, &entry);
+	if ((rc == DEVICE_SAME || rc == DEVICE_UPDATED) &&
+		entry >= 0 && entry < h->ndevices) {
+		dev->supports_aborts = h->dev[entry]->supports_aborts;
+		spin_unlock_irqrestore(&h->devlock, flags);
+	} else {
+		spin_unlock_irqrestore(&h->devlock, flags);
+		dev->supports_aborts =
+				hpsa_device_supports_aborts(h,
+					dev->hba_ioaccel_enabled, scsi3addr);
+		if (dev->supports_aborts < 0)
+			dev->supports_aborts = 0;
+	}
 }
 
 static unsigned char *ext_target_model[] = {
@@ -2737,6 +3641,9 @@ static unsigned char *ext_target_model[] = {
 	"MSA2324",
 	"P2000 G3 SAS",
 	"MSA 2040 SAS",
+	"MSA 1040 SAS",
+	"DH3000",
+	"DH4524",
 	NULL,
 };
 
@@ -2835,103 +3742,39 @@ static int add_ext_target_dev(struct ctlr_info *h,
 	(*n_ext_target_devs)++;
 	hpsa_set_bus_target_lun(this_device,
 				tmpdevice->bus, tmpdevice->target, 0);
+	hpsa_update_device_supports_aborts(h, this_device, scsi3addr);
 	set_bit(tmpdevice->target, lunzerobits);
 	return 1;
 }
 
 /*
  * Get address of physical disk used for an ioaccel2 mode command:
- *	1. Extract ioaccel2 handle from the command.
- *	2. Find a matching ioaccel2 handle from list of physical disks.
- *	3. Return:
- *		1 and set scsi3addr to address of matching physical
- *		0 if no matching physical disk was found.
+ * 	1. Extract ioaccel2 handle from the command.
+	2. Find a matching ioaccel2 handle from list of physical disks.
+ *	3. Return: 	
+ * 		1 and set scsi3addr to address of matching physical
+ * 		0 if no matching physical disk was found.
  */
 static int hpsa_get_pdisk_of_ioaccel2(struct ctlr_info *h,
 	struct CommandList *ioaccel2_cmd_to_abort, unsigned char *scsi3addr)
 {
-	struct ReportExtendedLUNdata *physicals = NULL;
-	int responsesize = 24;	/* size of physical extended response */
-	int reportsize = sizeof(*physicals) + HPSA_MAX_PHYS_LUN * responsesize;
-	u32 nphysicals = 0;	/* number of reported physical devs */
-	int found = 0;		/* found match (1) or not (0) */
-	u32 find;		/* handle we need to match */
+	struct io_accel2_cmd *c2 =
+			&h->ioaccel2_cmd_pool[ioaccel2_cmd_to_abort->cmdindex];
+	unsigned long flags;
 	int i;
-	struct scsi_cmnd *scmd;	/* scsi command within request being aborted */
-	struct hpsa_scsi_dev_t *d; /* device of request being aborted */
-	struct io_accel2_cmd *c2a; /* ioaccel2 command to abort */
-	__le32 it_nexus;	/* 4 byte device handle for the ioaccel2 cmd */
-	__le32 scsi_nexus;	/* 4 byte device handle for the ioaccel2 cmd */
-
-	if (ioaccel2_cmd_to_abort->cmd_type != CMD_IOACCEL2)
-		return 0; /* no match */
-
-	/* point to the ioaccel2 device handle */
-	c2a = &h->ioaccel2_cmd_pool[ioaccel2_cmd_to_abort->cmdindex];
-	if (c2a == NULL)
-		return 0; /* no match */
-
-	scmd = (struct scsi_cmnd *) ioaccel2_cmd_to_abort->scsi_cmd;
-	if (scmd == NULL)
-		return 0; /* no match */
-
-	d = scmd->device->hostdata;
-	if (d == NULL)
-		return 0; /* no match */
-
-	it_nexus = cpu_to_le32(d->ioaccel_handle);
-	scsi_nexus = c2a->scsi_nexus;
-	find = le32_to_cpu(c2a->scsi_nexus);
-
-	if (h->raid_offload_debug > 0)
-		dev_info(&h->pdev->dev,
-			"%s: scsi_nexus:0x%08x device id: 0x%02x%02x%02x%02x %02x%02x%02x%02x %02x%02x%02x%02x %02x%02x%02x%02x\n",
-			__func__, scsi_nexus,
-			d->device_id[0], d->device_id[1], d->device_id[2],
-			d->device_id[3], d->device_id[4], d->device_id[5],
-			d->device_id[6], d->device_id[7], d->device_id[8],
-			d->device_id[9], d->device_id[10], d->device_id[11],
-			d->device_id[12], d->device_id[13], d->device_id[14],
-			d->device_id[15]);
-
-	/* Get the list of physical devices */
-	physicals = kzalloc(reportsize, GFP_KERNEL);
-	if (physicals == NULL)
-		return 0;
-	if (hpsa_scsi_do_report_phys_luns(h, physicals, reportsize)) {
-		dev_err(&h->pdev->dev,
-			"Can't lookup %s device handle: report physical LUNs failed.\n",
-			"HP SSD Smart Path");
-		kfree(physicals);
-		return 0;
-	}
-	nphysicals = be32_to_cpu(*((__be32 *)physicals->LUNListLength)) /
-							responsesize;
-
-	/* find ioaccel2 handle in list of physicals: */
-	for (i = 0; i < nphysicals; i++) {
-		struct ext_report_lun_entry *entry = &physicals->LUN[i];
-
-		/* handle is in bytes 28-31 of each lun */
-		if (entry->ioaccel_handle != find)
-			continue; /* didn't match */
-		found = 1;
-		memcpy(scsi3addr, entry->lunid, 8);
-		if (h->raid_offload_debug > 0)
-			dev_info(&h->pdev->dev,
-				"%s: Searched h=0x%08x, Found h=0x%08x, scsiaddr 0x%8phN\n",
-				__func__, find,
-				entry->ioaccel_handle, scsi3addr);
-		break; /* found it */
-	}
-
-	kfree(physicals);
-	if (found)
-		return 1;
-	else
-		return 0;
 
+	spin_lock_irqsave(&h->devlock, flags);
+	for (i = 0; i < h->ndevices; i++)
+		if (h->dev[i]->ioaccel_handle == c2->scsi_nexus) {
+			memcpy(scsi3addr, h->dev[i]->scsi3addr,
+				sizeof(h->dev[i]->scsi3addr));
+			spin_unlock_irqrestore(&h->devlock, flags);
+			return 1;
+		}
+	spin_unlock_irqrestore(&h->devlock, flags);
+	return 0;
 }
+
 /*
  * Do CISS_REPORT_PHYS and CISS_REPORT_LOG.  Data is returned in physdev,
  * logdev.  The number of luns in physdev and logdev are returned in
@@ -2948,8 +3791,9 @@ static int hpsa_gather_lun_info(struct ctlr_info *h,
 	}
 	*nphysicals = be32_to_cpu(*((__be32 *)physdev->LUNListLength)) / 24;
 	if (*nphysicals > HPSA_MAX_PHYS_LUN) {
-		dev_warn(&h->pdev->dev, "maximum physical LUNs (%d) exceeded. %d LUNs ignored.\n",
-			HPSA_MAX_PHYS_LUN, *nphysicals - HPSA_MAX_PHYS_LUN);
+		dev_warn(&h->pdev->dev, "maximum physical LUNs (%d) exceeded."
+			"  %d LUNs ignored.\n", HPSA_MAX_PHYS_LUN,
+			*nphysicals - HPSA_MAX_PHYS_LUN);
 		*nphysicals = HPSA_MAX_PHYS_LUN;
 	}
 	if (hpsa_scsi_do_report_log_luns(h, logdev, sizeof(*logdev))) {
@@ -2975,8 +3819,8 @@ static int hpsa_gather_lun_info(struct ctlr_info *h,
 	return 0;
 }
 
-static u8 *figure_lunaddrbytes(struct ctlr_info *h, int raid_ctlr_position,
-	int i, int nphysicals, int nlogicals,
+u8 *figure_lunaddrbytes(struct ctlr_info *h, int raid_ctlr_position, int i,
+	int nphysicals, int nlogicals,
 	struct ReportExtendedLUNdata *physdev_list,
 	struct ReportLUNdata *logdev_list)
 {
@@ -2991,10 +3835,10 @@ static u8 *figure_lunaddrbytes(struct ctlr_info *h, int raid_ctlr_position,
 	if (i == raid_ctlr_position)
 		return RAID_CTLR_LUNID;
 
-	if (i < logicals_start)
-		return &physdev_list->LUN[i -
-				(raid_ctlr_position == 0)].lunid[0];
-
+	if (i < logicals_start) {
+		const int index = i - (raid_ctlr_position == 0);
+		return &physdev_list->LUN[index].lunid[0];
+	}
 	if (i < last_device)
 		return &logdev_list->LUN[i - nphysicals -
 			(raid_ctlr_position == 0)][0];
@@ -3008,17 +3852,16 @@ static int hpsa_hba_mode_enabled(struct ctlr_info *h)
 	int hba_mode_enabled;
 	struct bmic_controller_parameters *ctlr_params;
 	ctlr_params = kzalloc(sizeof(struct bmic_controller_parameters),
-		GFP_KERNEL);
-
+				GFP_KERNEL);
+	
 	if (!ctlr_params)
 		return -ENOMEM;
 	rc = hpsa_bmic_ctrl_mode_sense(h, RAID_CTLR_LUNID, 0, ctlr_params,
-		sizeof(struct bmic_controller_parameters));
-	if (rc) {
+				sizeof(struct bmic_controller_parameters));
+	if (rc != 0) {
 		kfree(ctlr_params);
 		return rc;
 	}
-
 	hba_mode_enabled =
 		((ctlr_params->nvram_flags & HBA_MODE_ENABLED_FLAG) != 0);
 	kfree(ctlr_params);
@@ -3026,7 +3869,7 @@ static int hpsa_hba_mode_enabled(struct ctlr_info *h)
 }
 
 /* get physical drive ioaccel handle and queue depth */
-static void hpsa_get_ioaccel_drive_info(struct ctlr_info *h,
+void hpsa_get_ioaccel_drive_info(struct ctlr_info *h,
 		struct hpsa_scsi_dev_t *dev,
 		u8 *lunaddrbytes,
 		struct bmic_identify_physical_device *id_phys)
@@ -3036,6 +3879,8 @@ static void hpsa_get_ioaccel_drive_info(struct ctlr_info *h,
 		(struct ext_report_lun_entry *) lunaddrbytes;
 
 	dev->ioaccel_handle = rle->ioaccel_handle;
+	if (PHYS_IOACCEL(lunaddrbytes) && dev->ioaccel_handle)
+		dev->hba_ioaccel_enabled = 1;
 	memset(id_phys, 0, sizeof(*id_phys));
 	rc = hpsa_bmic_id_physical_device(h, lunaddrbytes,
 			GET_BMIC_DRIVE_NUMBER(lunaddrbytes), id_phys,
@@ -3043,13 +3888,40 @@ static void hpsa_get_ioaccel_drive_info(struct ctlr_info *h,
 	if (!rc)
 		/* Reserve space for FW operations */
 #define DRIVE_CMDS_RESERVED_FOR_FW 2
-#define DRIVE_QUEUE_DEPTH 7
 		dev->queue_depth =
 			le16_to_cpu(id_phys->current_queue_depth_limit) -
 				DRIVE_CMDS_RESERVED_FOR_FW;
 	else
-		dev->queue_depth = DRIVE_QUEUE_DEPTH; /* conservative */
+		dev->queue_depth = 7; /* conservative */
 	atomic_set(&dev->ioaccel_cmds_out, 0);
+	atomic_set(&dev->reset_cmds_out, 0);
+}
+
+static void hpsa_get_path_info(u8 *lunaddrbytes, 
+	struct hpsa_scsi_dev_t *this_device,
+	struct bmic_identify_physical_device *id_phys)
+{
+	if (PHYS_IOACCEL(lunaddrbytes)
+		&& this_device->ioaccel_handle)
+		this_device->hba_ioaccel_enabled = 1;
+
+	memcpy(&this_device->active_path_index,
+		&id_phys->active_path_number,
+		sizeof(this_device->active_path_index));
+	memcpy(&this_device->path_map,
+		&id_phys->redundant_path_present_map,
+		sizeof(this_device->path_map));
+	memcpy(&this_device->box,
+		&id_phys->alternate_paths_phys_box_on_port,
+		sizeof(this_device->box));
+	memcpy(&this_device->phys_connector,
+	&id_phys->alternate_paths_phys_connector,
+		sizeof(this_device->phys_connector));
+	memcpy(&this_device->bay,
+		&id_phys->phys_bay_in_box,
+		sizeof(this_device->bay));
+
+	memset(id_phys, 0, sizeof(*id_phys));
 }
 
 static void hpsa_update_scsi_devices(struct ctlr_info *h, int hostno)
@@ -3073,6 +3945,7 @@ static void hpsa_update_scsi_devices(struct ctlr_info *h, int hostno)
 	struct hpsa_scsi_dev_t **currentsd, *this_device, *tmpdevice;
 	int ncurrent = 0;
 	int i, n_ext_target_devs, ndevs_to_allocate;
+	int rc = 0;
 	int raid_ctlr_position;
 	int rescan_hba_mode;
 	DECLARE_BITMAP(lunzerobits, MAX_EXT_TARGETS);
@@ -3091,19 +3964,24 @@ static void hpsa_update_scsi_devices(struct ctlr_info *h, int hostno)
 	memset(lunzerobits, 0, sizeof(lunzerobits));
 
 	rescan_hba_mode = hpsa_hba_mode_enabled(h);
-	if (rescan_hba_mode < 0)
+
+	if (rescan_hba_mode < 0) {
+		h->drv_req_rescan = 1;
 		goto out;
+	}
 
 	if (!h->hba_mode_enabled && rescan_hba_mode)
-		dev_warn(&h->pdev->dev, "HBA mode enabled\n");
+		dev_info(&h->pdev->dev, "HBA mode enabled\n");
 	else if (h->hba_mode_enabled && !rescan_hba_mode)
-		dev_warn(&h->pdev->dev, "HBA mode disabled\n");
+		dev_info(&h->pdev->dev, "HBA mode disabled\n");
 
 	h->hba_mode_enabled = rescan_hba_mode;
 
 	if (hpsa_gather_lun_info(h, physdev_list, &nphysicals,
-			logdev_list, &nlogicals))
+			logdev_list, &nlogicals)) {
+		h->drv_req_rescan = 1;
 		goto out;
+	}
 
 	/* We might see up to the maximum number of logical and physical disks
 	 * plus external target devices, and a device for the local RAID
@@ -3124,6 +4002,7 @@ static void hpsa_update_scsi_devices(struct ctlr_info *h, int hostno)
 		if (!currentsd[i]) {
 			dev_warn(&h->pdev->dev, "out of memory at %s:%d\n",
 				__FILE__, __LINE__);
+			h->drv_req_rescan = 1;
 			goto out;
 		}
 		ndev_allocated++;
@@ -3142,16 +4021,25 @@ static void hpsa_update_scsi_devices(struct ctlr_info *h, int hostno)
 		/* Figure out where the LUN ID info is coming from */
 		lunaddrbytes = figure_lunaddrbytes(h, raid_ctlr_position,
 			i, nphysicals, nlogicals, physdev_list, logdev_list);
-		/* skip masked physical devices. */
-		if (lunaddrbytes[3] & 0xC0 &&
-			i < nphysicals + (raid_ctlr_position == 0))
-			continue;
+		/* skip masked non-disk devices */
+		if (MASKED_DEVICE(lunaddrbytes))
+			if (i < nphysicals + (raid_ctlr_position == 0) &&
+				NON_DISK_PHYS_DEV(lunaddrbytes))
+				continue;
 
 		/* Get device type, vendor, model, device id */
-		if (hpsa_update_device_info(h, lunaddrbytes, tmpdevice,
-							&is_OBDR))
-			continue; /* skip it if we can't talk to it. */
+		rc = hpsa_update_device_info(h, lunaddrbytes, tmpdevice,
+							&is_OBDR);
+		if (rc) {
+			dev_warn(&h->pdev->dev, "%s, rescan stopped.\n",
+				(rc == -ENOMEM) ? "Out of memory" :
+				"Inquiry failed");
+			h->drv_req_rescan = 1;
+			goto out;
+		}
+
 		figure_bus_target_lun(h, lunaddrbytes, tmpdevice);
+		hpsa_update_device_supports_aborts(h, tmpdevice, lunaddrbytes);
 		this_device = currentsd[ncurrent];
 
 		/*
@@ -3170,47 +4058,81 @@ static void hpsa_update_scsi_devices(struct ctlr_info *h, int hostno)
 
 		*this_device = *tmpdevice;
 
+		this_device->bay = 0;
+		this_device->active_path_index = 0;
+		this_device->path_map = 0;
+		memset(this_device->box, 0, sizeof(this_device->box));
+		memset(this_device->phys_connector, 0,
+			sizeof(this_device->phys_connector));
+		this_device->timeout = 0;
+
+		/* do not expose masked devices */
+		if (MASKED_DEVICE(lunaddrbytes) &&
+			i < nphysicals + (raid_ctlr_position == 0)) {
+			if (h->hba_mode_enabled)
+				dev_warn(&h->pdev->dev,
+					"Masked physical device detected\n");
+			this_device->expose_state = HPSA_DO_NOT_EXPOSE;
+		} else {
+			this_device->expose_state = HPSA_EXPOSE;
+		}
+
 		switch (this_device->devtype) {
 		case TYPE_ROM:
 			/* We don't *really* support actual CD-ROM devices,
-			 * just "One Button Disaster Recovery" tape drive
-			 * which temporarily pretends to be a CD-ROM drive.
-			 * So we check that the device is really an OBDR tape
-			 * device by checking for "$DR-10" in bytes 43-48 of
-			 * the inquiry data.
+			 * on Smart Arrays, just "One Button Disaster
+			 * Recovery" tape drive which temporarily pretends to
+			 * be a CD-ROM drive.  So we check that the device is
+			 * really an OBDR tape device by checking for "$DR-10"
+			 * in bytes 43-48 of the inquiry data.
 			 */
 			if (is_OBDR)
 				ncurrent++;
 			break;
 		case TYPE_DISK:
+			/* Use configurable timeout setting for disks.*/
+			this_device->timeout = h->disk_rq_timeout;
+			/* HBA mode supported*/
 			if (h->hba_mode_enabled) {
 				/* never use raid mapper in HBA mode */
 				this_device->offload_enabled = 0;
+				hpsa_get_ioaccel_drive_info(h, this_device,
+						lunaddrbytes, id_phys);
+
+				hpsa_get_path_info(lunaddrbytes, 
+						this_device, id_phys);
 				ncurrent++;
 				break;
-			} else if (h->acciopath_status) {
+			}
+			/* HP SSD Smart Path Mode supported */
+			else if (h->acciopath_status) {
 				if (i >= nphysicals) {
 					ncurrent++;
 					break;
 				}
-			} else {
+				if (h->transMethod & CFGTBL_Trans_io_accel1 ||
+					h->transMethod & CFGTBL_Trans_io_accel2) {
+					hpsa_get_ioaccel_drive_info(h, this_device,
+							lunaddrbytes, id_phys);
+					hpsa_get_path_info(lunaddrbytes, this_device, id_phys);
+					ncurrent++;
+				}
+				break;
+			}
+			else {  /* normal mode: no ioaccel or HBA mode */
 				if (i < nphysicals)
 					break;
 				ncurrent++;
 				break;
-			}
-			if (h->transMethod & CFGTBL_Trans_io_accel1 ||
-				h->transMethod & CFGTBL_Trans_io_accel2) {
-				hpsa_get_ioaccel_drive_info(h, this_device,
-							lunaddrbytes, id_phys);
-				atomic_set(&this_device->ioaccel_cmds_out, 0);
-				ncurrent++;
-			}
-			break;
+                        }
 		case TYPE_TAPE:
 		case TYPE_MEDIUM_CHANGER:
 			ncurrent++;
 			break;
+		case TYPE_ENCLOSURE:
+			if (h->hba_mode_enabled)
+				ncurrent++;
+			break;
 		case TYPE_RAID:
 			/* Only present the Smartarray HBA as a RAID controller.
 			 * If it's a RAID controller other than the HBA itself
@@ -3227,18 +4149,20 @@ static void hpsa_update_scsi_devices(struct ctlr_info *h, int hostno)
 		if (ncurrent >= HPSA_MAX_DEVICES)
 			break;
 	}
-	hpsa_update_log_drive_phys_drive_ptrs(h, currentsd, ncurrent);
 	adjust_hpsa_scsi_table(h, hostno, currentsd, ncurrent);
 out:
 	kfree(tmpdevice);
-	for (i = 0; i < ndev_allocated; i++)
-		kfree(currentsd[i]);
-	kfree(currentsd);
+	if (currentsd) {
+		for (i = 0; i < ndev_allocated; i++)
+			kfree(currentsd[i]);
+		kfree(currentsd);
+	}
 	kfree(physdev_list);
 	kfree(logdev_list);
 	kfree(id_phys);
 }
 
+#if KFEATURE_HAS_SCSI_DMA_FUNCTIONS
 static void hpsa_set_sg_descriptor(struct SGDescriptor *desc,
 				   struct scatterlist *sg)
 {
@@ -3250,8 +4174,7 @@ static void hpsa_set_sg_descriptor(struct SGDescriptor *desc,
 	desc->Ext = 0;
 }
 
-/*
- * hpsa_scatter_gather takes a struct scsi_cmnd, (cmd), and does the pci
+/* hpsa_scatter_gather takes a struct scsi_cmnd, (cmd), and does the pci
  * dma mapping  and fills in the scatter gather entries of the
  * hpsa command, cp.
  */
@@ -3260,7 +4183,7 @@ static int hpsa_scatter_gather(struct ctlr_info *h,
 		struct scsi_cmnd *cmd)
 {
 	struct scatterlist *sg;
-	int use_sg, i, sg_index, chained;
+	int use_sg, i, sg_limit, chained, last_sg;
 	struct SGDescriptor *curr_sg;
 
 	BUG_ON(scsi_sg_count(cmd) > h->maxsgentries);
@@ -3272,29 +4195,42 @@ static int hpsa_scatter_gather(struct ctlr_info *h,
 	if (!use_sg)
 		goto sglist_finished;
 
+	/* If the number of entries is greater than the max for a single list,
+	 * then we have a chained list; we will set up all but one entry in the
+	 * first list (the last entry is saved for link information);
+	 * otherwise, we don't have a chained list and we'll set up at each of
+	 * the entries in the one list. */
 	curr_sg = cp->SG;
-	chained = 0;
-	sg_index = 0;
-	scsi_for_each_sg(cmd, sg, use_sg, i) {
-		if (i == h->max_cmd_sg_entries - 1 &&
-			use_sg > h->max_cmd_sg_entries) {
-			chained = 1;
-			curr_sg = h->cmd_sg_list[cp->cmdindex];
-			sg_index = 0;
-		}
+	chained = use_sg > h->max_cmd_sg_entries;
+	sg_limit = chained ? h->max_cmd_sg_entries - 1 : use_sg;
+	last_sg = scsi_sg_count(cmd) - 1;
+	scsi_for_each_sg(cmd, sg, sg_limit, i) {
 		hpsa_set_sg_descriptor(curr_sg, sg);
 		curr_sg++;
 	}
 
+	if (chained) {
+		/* Continue with the chained list.  Set curr_sg to the chained
+		 * list.  Modify the limit to the total count less the entries
+		 * we've already set up.  Resume the scan at the list entry
+		 * where the previous loop left off. */
+		curr_sg = h->cmd_sg_list[cp->cmdindex];
+		sg_limit = use_sg - sg_limit;
+		for_each_sg(sg, sg, sg_limit, i) {
+			hpsa_set_sg_descriptor(curr_sg, sg);
+			curr_sg++;
+		}
+	}
+
 	/* Back the pointer up to the last entry and mark it as "last". */
-	(--curr_sg)->Ext = cpu_to_le32(HPSA_SG_LAST);
+	(curr_sg - 1)->Ext = cpu_to_le32(HPSA_SG_LAST);
 
 	if (use_sg + chained > h->maxSG)
 		h->maxSG = use_sg + chained;
 
 	if (chained) {
 		cp->Header.SGList = h->max_cmd_sg_entries;
-		cp->Header.SGTotal = cpu_to_le16(use_sg + 1);
+		cp->Header.SGTotal = (u16) (use_sg + 1);
 		if (hpsa_map_sg_chain_block(h, cp)) {
 			scsi_dma_unmap(cmd);
 			return -1;
@@ -3305,9 +4241,12 @@ static int hpsa_scatter_gather(struct ctlr_info *h,
 sglist_finished:
 
 	cp->Header.SGList = (u8) use_sg;   /* no. SGs contig in this cmd */
-	cp->Header.SGTotal = cpu_to_le16(use_sg); /* total sgs in cmd list */
+	cp->Header.SGTotal = (u16) use_sg; /* total sgs in this cmd list */
 	return 0;
 }
+#endif /* if KFEATURE_HAS_SCSI_DMA_FUNCTIONS
+	* otherwise hpsa_scatter_gather() is defined in hpsa_kernel_compat.h
+	*/
 
 #define IO_ACCEL_INELIGIBLE (1)
 static int fixup_ioaccel_cdb(u8 *cdb, int *cdb_len)
@@ -3405,10 +4344,13 @@ static int hpsa_scsi_ioaccel1_queue_command(struct ctlr_info *h,
 			total_len += len;
 			curr_sg->Addr = cpu_to_le64(addr64);
 			curr_sg->Len = cpu_to_le32(len);
-			curr_sg->Ext = cpu_to_le32(0);
+
+			if (i == (scsi_sg_count(cmd) - 1))
+				curr_sg->Ext = cpu_to_le32(HPSA_SG_LAST);
+			else
+				curr_sg->Ext = cpu_to_le32(0); /* not chain */
 			curr_sg++;
 		}
-		(--curr_sg)->Ext = cpu_to_le32(HPSA_SG_LAST);
 
 		switch (cmd->sc_data_direction) {
 		case DMA_TO_DEVICE:
@@ -3432,18 +4374,18 @@ static int hpsa_scsi_ioaccel1_queue_command(struct ctlr_info *h,
 
 	c->Header.SGList = use_sg;
 	/* Fill out the command structure to submit */
-	cp->dev_handle = cpu_to_le16(ioaccel_handle & 0xFFFF);
-	cp->transfer_len = cpu_to_le32(total_len);
-	cp->io_flags = cpu_to_le16(IOACCEL1_IOFLAGS_IO_REQ |
-			(cdb_len & IOACCEL1_IOFLAGS_CDBLEN_MASK));
-	cp->control = cpu_to_le32(control);
+	cp->dev_handle = ioaccel_handle & 0xFFFF;
+	cp->transfer_len = total_len;
+	cp->io_flags = IOACCEL1_IOFLAGS_IO_REQ |
+			(cdb_len & IOACCEL1_IOFLAGS_CDBLEN_MASK);
+	cp->control = control;
 	memcpy(cp->CDB, cdb, cdb_len);
 	memcpy(cp->CISS_LUN, scsi3addr, 8);
 	/* Tag was already set at init time. */
 	enqueue_cmd_and_start_io(h, c);
 	return 0;
 }
-
+ 
 /*
  * Queue a command directly to a device behind the controller using the
  * I/O accelerator path.
@@ -3460,63 +4402,208 @@ static int hpsa_scsi_ioaccel_direct_map(struct ctlr_info *h,
 		cmd->cmnd, cmd->cmd_len, dev->scsi3addr, dev);
 }
 
-/*
- * Set encryption parameters for the ioaccel2 request
- */
-static void set_encrypt_ioaccel2(struct ctlr_info *h,
-	struct CommandList *c, struct io_accel2_cmd *cp)
+static void __attribute__((unused)) verify_offsets(void)
 {
-	struct scsi_cmnd *cmd = c->scsi_cmd;
-	struct hpsa_scsi_dev_t *dev = cmd->device->hostdata;
-	struct raid_map_data *map = &dev->raid_map;
-	u64 first_block;
+#define VERIFY_OFFSET(member, offset) \
+	BUILD_BUG_ON(offsetof(struct raid_map_data, member) != offset)
 
-	/* Are we doing encryption on this device */
-	if (!(le16_to_cpu(map->flags) & RAID_MAP_FLAG_ENCRYPT_ON))
-		return;
-	/* Set the data encryption key index. */
-	cp->dekindex = map->dekindex;
+	VERIFY_OFFSET(structure_size, 0);
+	VERIFY_OFFSET(volume_blk_size, 4);
+	VERIFY_OFFSET(volume_blk_cnt, 8);
+	VERIFY_OFFSET(phys_blk_shift, 16);
+	VERIFY_OFFSET(parity_rotation_shift, 17);
+	VERIFY_OFFSET(strip_size, 18);
+	VERIFY_OFFSET(disk_starting_blk, 20);
+	VERIFY_OFFSET(disk_blk_cnt, 28);
+	VERIFY_OFFSET(data_disks_per_row, 36);
+	VERIFY_OFFSET(metadata_disks_per_row, 38);
+	VERIFY_OFFSET(row_cnt, 40);
+	VERIFY_OFFSET(layout_map_count, 42);
+	VERIFY_OFFSET(flags, 44);
+	VERIFY_OFFSET(dekindex, 46);
+	/* VERIFY_OFFSET(reserved, 48 */
+	VERIFY_OFFSET(data, 64);
 
-	/* Set the encryption enable flag, encoded into direction field. */
-	cp->direction |= IOACCEL2_DIRECTION_ENCRYPT_MASK;
+#undef VERIFY_OFFSET
 
-	/* Set encryption tweak values based on logical block address
-	 * If block size is 512, tweak value is LBA.
-	 * For other block sizes, tweak is (LBA * block size)/ 512)
-	 */
-	switch (cmd->cmnd[0]) {
-	/* Required? 6-byte cdbs eliminated by fixup_ioaccel_cdb */
-	case WRITE_6:
-	case READ_6:
-		first_block = get_unaligned_be16(&cmd->cmnd[2]);
-		break;
-	case WRITE_10:
+#define VERIFY_OFFSET(member, offset) \
+	BUILD_BUG_ON(offsetof(struct io_accel2_cmd, member) != offset)
+
+	VERIFY_OFFSET(IU_type, 0);
+	VERIFY_OFFSET(direction, 1);
+	VERIFY_OFFSET(reply_queue, 2);
+	/* VERIFY_OFFSET(reserved1, 3);  */
+	VERIFY_OFFSET(scsi_nexus, 4);
+	VERIFY_OFFSET(Tag, 8);
+	VERIFY_OFFSET(tweak_lower, 12);
+	VERIFY_OFFSET(cdb, 16);
+	VERIFY_OFFSET(cciss_lun, 32);
+	VERIFY_OFFSET(data_len, 40);
+	VERIFY_OFFSET(cmd_priority_task_attr, 44);
+	VERIFY_OFFSET(sg_count, 45); 
+	VERIFY_OFFSET(dekindex, 46);
+	VERIFY_OFFSET(err_ptr, 48);
+	VERIFY_OFFSET(err_len, 56);
+	VERIFY_OFFSET(tweak_upper, 60);
+	VERIFY_OFFSET(sg, 64);
+
+#undef VERIFY_OFFSET
+
+#define VERIFY_OFFSET(member, offset) \
+	BUILD_BUG_ON(offsetof(struct io_accel1_cmd, member) != offset)
+	
+	VERIFY_OFFSET(dev_handle, 0x00);
+	VERIFY_OFFSET(reserved1, 0x02);
+	VERIFY_OFFSET(function, 0x03);
+	VERIFY_OFFSET(reserved2, 0x04);
+	VERIFY_OFFSET(err_info, 0x0C);
+	VERIFY_OFFSET(reserved3, 0x10);
+	VERIFY_OFFSET(err_info_len, 0x12);
+	VERIFY_OFFSET(reserved4, 0x13);
+	VERIFY_OFFSET(sgl_offset, 0x14);
+	VERIFY_OFFSET(reserved5, 0x15);
+	VERIFY_OFFSET(transfer_len, 0x1C);
+	VERIFY_OFFSET(reserved6, 0x20);
+	VERIFY_OFFSET(io_flags, 0x24);
+	VERIFY_OFFSET(reserved7, 0x26);
+	VERIFY_OFFSET(LUN, 0x34);
+	VERIFY_OFFSET(control, 0x3C);
+	VERIFY_OFFSET(CDB, 0x40);
+	VERIFY_OFFSET(reserved8, 0x50);
+	VERIFY_OFFSET(host_context_flags, 0x60);
+	VERIFY_OFFSET(timeout_sec, 0x62);
+	VERIFY_OFFSET(ReplyQueue, 0x64);
+	VERIFY_OFFSET(reserved9, 0x65);
+	VERIFY_OFFSET(tag, 0x68);
+	VERIFY_OFFSET(host_addr, 0x70);
+	VERIFY_OFFSET(CISS_LUN, 0x78);
+	VERIFY_OFFSET(SG, 0x78 + 8);
+#undef VERIFY_OFFSET
+}
+
+/*
+ * Set encryption parameters for the ioaccel2 request 
+ */
+static void set_encrypt_ioaccel2(struct ctlr_info *h,
+	struct CommandList *c, struct io_accel2_cmd *cp)
+{
+	struct scsi_cmnd *cmd = c->scsi_cmd;
+	struct hpsa_scsi_dev_t *dev = cmd->device->hostdata;
+	struct raid_map_data *map = &dev->raid_map;
+	u64 first_block;
+	/* Are we doing encryption on this device */
+	if (! (map->flags & RAID_MAP_FLAG_ENCRYPT_ON) )
+		return;
+	/* Set the data encryption key index */
+	cp->dekindex = map->dekindex;
+
+	/* Set the encryption enable flag, encoded into direction field. */
+	cp->direction |= IOACCEL2_DIRECTION_ENCRYPT_MASK;
+
+	/* Set encryption tweak values based on logical block address
+	 * If block size is 512, tweak value is LBA.
+	 * For other block sizes, tweak is (LBA * block size)/ 512)
+	 */
+	switch (cmd->cmnd[0]) {
+	/* Required? 6-byte cdbs eliminated by fixup_ioaccel_cdb */
+	case WRITE_6:
+	case READ_6:
+		if (map->volume_blk_size == 512 ) {
+			cp->tweak_lower = 
+				(((u32) cmd->cmnd[2]) << 8) |
+					cmd->cmnd[3];
+			cp->tweak_upper = 0; 
+		}
+		else {
+			first_block = 
+				(((u64) cmd->cmnd[2]) << 8) |
+					cmd->cmnd[3];
+			first_block = (first_block * map->volume_blk_size)/512;
+			cp->tweak_lower = (u32)first_block;
+			cp->tweak_upper = (u32)(first_block >> 32);
+		}
+		break;
+	case WRITE_10:
 	case READ_10:
+		if (map->volume_blk_size == 512 ) {
+			cp->tweak_lower = 
+				(((u32) cmd->cmnd[2]) << 24) |
+				(((u32) cmd->cmnd[3]) << 16) |
+				(((u32) cmd->cmnd[4]) << 8) |
+					cmd->cmnd[5];
+			cp->tweak_upper = 0; 
+		}
+		else {
+			first_block =
+				(((u64) cmd->cmnd[2]) << 24) |
+				(((u64) cmd->cmnd[3]) << 16) |
+				(((u64) cmd->cmnd[4]) << 8) |
+					cmd->cmnd[5];
+			first_block = (first_block * map->volume_blk_size)/512;
+			cp->tweak_lower = (u32)first_block;
+			cp->tweak_upper = (u32)(first_block >> 32);
+		}
+		break;
 	/* Required? 12-byte cdbs eliminated by fixup_ioaccel_cdb */
 	case WRITE_12:
 	case READ_12:
-		first_block = get_unaligned_be32(&cmd->cmnd[2]);
+		if (map->volume_blk_size == 512 ) {
+			cp->tweak_lower = 
+				(((u32) cmd->cmnd[2]) << 24) |
+				(((u32) cmd->cmnd[3]) << 16) |
+				(((u32) cmd->cmnd[4]) << 8) |
+					cmd->cmnd[5];
+			cp->tweak_upper = 0; 
+		}
+		else {
+			first_block =
+				(((u64) cmd->cmnd[2]) << 24) |
+				(((u64) cmd->cmnd[3]) << 16) |
+				(((u64) cmd->cmnd[4]) << 8) |
+					cmd->cmnd[5];
+			first_block = (first_block * map->volume_blk_size)/512;
+			cp->tweak_lower = (u32)first_block;
+			cp->tweak_upper = (u32)(first_block >> 32);
+		}
 		break;
 	case WRITE_16:
 	case READ_16:
-		first_block = get_unaligned_be64(&cmd->cmnd[2]);
+		if (map->volume_blk_size == 512 ) {
+			cp->tweak_lower = 
+				(((u32) cmd->cmnd[6]) << 24) |
+				(((u32) cmd->cmnd[7]) << 16) |
+				(((u32) cmd->cmnd[8]) << 8) |
+					cmd->cmnd[9];
+			cp->tweak_upper = 
+				(((u32) cmd->cmnd[2]) << 24) |
+				(((u32) cmd->cmnd[3]) << 16) |
+				(((u32) cmd->cmnd[4]) << 8) |
+					cmd->cmnd[5];
+		}
+		else {
+			first_block =
+				(((u64) cmd->cmnd[2]) << 56) |
+				(((u64) cmd->cmnd[3]) << 48) |
+				(((u64) cmd->cmnd[4]) << 40) |
+				(((u64) cmd->cmnd[5]) << 32) |
+				(((u64) cmd->cmnd[6]) << 24) |
+				(((u64) cmd->cmnd[7]) << 16) |
+				(((u64) cmd->cmnd[8]) << 8) |
+					cmd->cmnd[9];
+			first_block = (first_block * map->volume_blk_size)/512;
+			cp->tweak_lower = (u32)first_block;
+			cp->tweak_upper = (u32)(first_block >> 32);
+		}
 		break;
 	default:
-		dev_err(&h->pdev->dev,
-			"ERROR: %s: size (0x%x) not supported for encryption\n",
-			__func__, cmd->cmnd[0]);
-		BUG();
+		dev_err(&h->pdev->dev, "ERROR: %s: IOACCEL request "
+			"CDB size not supported for encryption\n", __func__);
+		BUG();	
 		break;
 	}
-
-	if (le32_to_cpu(map->volume_blk_size) != 512)
-		first_block = first_block *
-				le32_to_cpu(map->volume_blk_size)/512;
-
-	cp->tweak_lower = cpu_to_le32(first_block);
-	cp->tweak_upper = cpu_to_le32(first_block >> 32);
 }
 
+
 static int hpsa_scsi_ioaccel2_queue_command(struct ctlr_info *h,
 	struct CommandList *c, u32 ioaccel_handle, u8 *cdb, int cdb_len,
 	u8 *scsi3addr, struct hpsa_scsi_dev_t *phys_disk)
@@ -3530,6 +4617,7 @@ static int hpsa_scsi_ioaccel2_queue_command(struct ctlr_info *h,
 	u32 len;
 	u32 total_len = 0;
 
+	BUG_ON(scsi_sg_count(cmd) > h->maxsgentries);
 	if (scsi_sg_count(cmd) > h->ioaccel_maxsg) {
 		atomic_dec(&phys_disk->ioaccel_cmds_out);
 		return IO_ACCEL_INELIGIBLE;
@@ -3556,8 +4644,19 @@ static int hpsa_scsi_ioaccel2_queue_command(struct ctlr_info *h,
 	}
 
 	if (use_sg) {
-		BUG_ON(use_sg > IOACCEL2_MAXSGENTRIES);
 		curr_sg = cp->sg;
+		if (use_sg > h->ioaccel_maxsg) {
+			addr64 = h->ioaccel2_cmd_sg_list[c->cmdindex]->address;
+			curr_sg->address = cpu_to_le64(addr64);
+			curr_sg->length = 0;
+			curr_sg->reserved[0] = 0;
+			curr_sg->reserved[1] = 0;
+			curr_sg->reserved[2] = 0;
+			curr_sg->chain_indicator = 0x80;
+
+			curr_sg = h->ioaccel2_cmd_sg_list[c->cmdindex];
+		}
+			
 		scsi_for_each_sg(cmd, sg, use_sg, i) {
 			addr64 = (u64) sg_dma_address(sg);
 			len  = sg_dma_len(sg);
@@ -3573,15 +4672,15 @@ static int hpsa_scsi_ioaccel2_queue_command(struct ctlr_info *h,
 
 		switch (cmd->sc_data_direction) {
 		case DMA_TO_DEVICE:
-			cp->direction &= ~IOACCEL2_DIRECTION_MASK;
+			cp->direction &= ~IOACCEL2_DIRECTION_MASK; 
 			cp->direction |= IOACCEL2_DIR_DATA_OUT;
 			break;
 		case DMA_FROM_DEVICE:
-			cp->direction &= ~IOACCEL2_DIRECTION_MASK;
+			cp->direction &= ~IOACCEL2_DIRECTION_MASK; 
 			cp->direction |= IOACCEL2_DIR_DATA_IN;
 			break;
 		case DMA_NONE:
-			cp->direction &= ~IOACCEL2_DIRECTION_MASK;
+			cp->direction &= ~IOACCEL2_DIRECTION_MASK; 
 			cp->direction |= IOACCEL2_DIR_NO_DATA;
 			break;
 		default:
@@ -3591,24 +4690,40 @@ static int hpsa_scsi_ioaccel2_queue_command(struct ctlr_info *h,
 			break;
 		}
 	} else {
-		cp->direction &= ~IOACCEL2_DIRECTION_MASK;
+		cp->direction &= ~IOACCEL2_DIRECTION_MASK; 
 		cp->direction |= IOACCEL2_DIR_NO_DATA;
 	}
 
 	/* Set encryption parameters, if necessary */
 	set_encrypt_ioaccel2(h, c, cp);
-
-	cp->scsi_nexus = cpu_to_le32(ioaccel_handle);
-	cp->Tag = cpu_to_le32(c->cmdindex << DIRECT_LOOKUP_SHIFT);
+		
+	cp->scsi_nexus = ioaccel_handle; /* FIXME: is this correct? */
+	cp->Tag = c->cmdindex << DIRECT_LOOKUP_SHIFT;
 	memcpy(cp->cdb, cdb, sizeof(cp->cdb));
+	memset(cp->cciss_lun, 0, sizeof(cp->cciss_lun)); /* FIXME: cciss_lun doesn't make sense here */
+	cp->cmd_priority_task_attr = 0;
 
-	/* fill in sg elements */
-	cp->sg_count = (u8) use_sg;
 
 	cp->data_len = cpu_to_le32(total_len);
-	cp->err_ptr = cpu_to_le64(c->busaddr +
-			offsetof(struct io_accel2_cmd, error_data));
-	cp->err_len = cpu_to_le32(sizeof(cp->error_data));
+	cp->err_ptr = cpu_to_le64(c->busaddr + offsetof(struct io_accel2_cmd, error_data));
+	cp->err_len = cpu_to_le32((u32) sizeof(cp->error_data));
+
+	/* fill in sg elements */
+	if (use_sg > h->ioaccel_maxsg) {
+		cp->sg_count = 1;
+		if (hpsa_map_ioaccel2_sg_chain_block(h, cp, c)) {
+			scsi_dma_unmap(cmd);
+			return -1;
+		}
+	} else {
+		cp->sg_count = (u8) use_sg;
+	}
+	if (unlikely(lockup_detected(h))) {
+		cmd->result = DID_NO_CONNECT << 16;
+		cmd_free(h, c);
+		cmd->scsi_done(cmd);
+		return 0;
+	}
 
 	enqueue_cmd_and_start_io(h, c);
 	return 0;
@@ -3637,32 +4752,6 @@ static int hpsa_scsi_ioaccel_queue_command(struct ctlr_info *h,
 						phys_disk);
 }
 
-static void raid_map_helper(struct raid_map_data *map,
-		int offload_to_mirror, u32 *map_index, u32 *current_group)
-{
-	if (offload_to_mirror == 0)  {
-		/* use physical disk in the first mirrored group. */
-		*map_index %= le16_to_cpu(map->data_disks_per_row);
-		return;
-	}
-	do {
-		/* determine mirror group that *map_index indicates */
-		*current_group = *map_index /
-			le16_to_cpu(map->data_disks_per_row);
-		if (offload_to_mirror == *current_group)
-			continue;
-		if (*current_group < le16_to_cpu(map->layout_map_count) - 1) {
-			/* select map index from next group */
-			*map_index += le16_to_cpu(map->data_disks_per_row);
-			(*current_group)++;
-		} else {
-			/* select map index from first group */
-			*map_index %= le16_to_cpu(map->data_disks_per_row);
-			*current_group = 0;
-		}
-	} while (offload_to_mirror != *current_group);
-}
-
 /*
  * Attempt to perform offload RAID mapping for a logical volume I/O.
  */
@@ -3695,7 +4784,6 @@ static int hpsa_scsi_ioaccel_raid_map(struct ctlr_info *h,
 	u32 disk_block_cnt;
 	u8 cdb[16];
 	u8 cdb_len;
-	u16 strip_size;
 #if BITS_PER_LONG == 32
 	u64 tmpdiv;
 #endif
@@ -3710,8 +4798,6 @@ static int hpsa_scsi_ioaccel_raid_map(struct ctlr_info *h,
 			(((u64) cmd->cmnd[2]) << 8) |
 			cmd->cmnd[3];
 		block_cnt = cmd->cmnd[4];
-		if (block_cnt == 0)
-			block_cnt = 256;
 		break;
 	case WRITE_10:
 		is_write = 1;
@@ -3760,6 +4846,7 @@ static int hpsa_scsi_ioaccel_raid_map(struct ctlr_info *h,
 	default:
 		return IO_ACCEL_INELIGIBLE; /* process via normal I/O path */
 	}
+	BUG_ON(block_cnt == 0);
 	last_block = first_block + block_cnt - 1;
 
 	/* check for write to non-RAID-0 */
@@ -3767,14 +4854,11 @@ static int hpsa_scsi_ioaccel_raid_map(struct ctlr_info *h,
 		return IO_ACCEL_INELIGIBLE;
 
 	/* check for invalid block or wraparound */
-	if (last_block >= le64_to_cpu(map->volume_blk_cnt) ||
-		last_block < first_block)
+	if (last_block >= map->volume_blk_cnt || last_block < first_block)
 		return IO_ACCEL_INELIGIBLE;
 
 	/* calculate stripe information for the request */
-	blocks_per_row = le16_to_cpu(map->data_disks_per_row) *
-				le16_to_cpu(map->strip_size);
-	strip_size = le16_to_cpu(map->strip_size);
+	blocks_per_row = map->data_disks_per_row * map->strip_size;
 #if BITS_PER_LONG == 32
 	tmpdiv = first_block;
 	(void) do_div(tmpdiv, blocks_per_row);
@@ -3785,18 +4869,18 @@ static int hpsa_scsi_ioaccel_raid_map(struct ctlr_info *h,
 	first_row_offset = (u32) (first_block - (first_row * blocks_per_row));
 	last_row_offset = (u32) (last_block - (last_row * blocks_per_row));
 	tmpdiv = first_row_offset;
-	(void) do_div(tmpdiv, strip_size);
+	(void) do_div(tmpdiv,  map->strip_size);
 	first_column = tmpdiv;
 	tmpdiv = last_row_offset;
-	(void) do_div(tmpdiv, strip_size);
+	(void) do_div(tmpdiv, map->strip_size);
 	last_column = tmpdiv;
 #else
 	first_row = first_block / blocks_per_row;
 	last_row = last_block / blocks_per_row;
 	first_row_offset = (u32) (first_block - (first_row * blocks_per_row));
 	last_row_offset = (u32) (last_block - (last_row * blocks_per_row));
-	first_column = first_row_offset / strip_size;
-	last_column = last_row_offset / strip_size;
+	first_column = first_row_offset / map->strip_size;
+	last_column = last_row_offset / map->strip_size;
 #endif
 
 	/* if this isn't a single row/column then give to the controller */
@@ -3804,73 +4888,94 @@ static int hpsa_scsi_ioaccel_raid_map(struct ctlr_info *h,
 		return IO_ACCEL_INELIGIBLE;
 
 	/* proceeding with driver mapping */
-	total_disks_per_row = le16_to_cpu(map->data_disks_per_row) +
-				le16_to_cpu(map->metadata_disks_per_row);
-	map_row = ((u32)(first_row >> map->parity_rotation_shift)) %
-				le16_to_cpu(map->row_cnt);
-	map_index = (map_row * total_disks_per_row) + first_column;
-
-	switch (dev->raid_level) {
-	case HPSA_RAID_0:
-		break; /* nothing special to do */
-	case HPSA_RAID_1:
+	total_disks_per_row = map->data_disks_per_row + map->metadata_disks_per_row;
+	map_row = ((u32)(first_row >> map->parity_rotation_shift)) % map->row_cnt;
+	map_index = (map_row * total_disks_per_row ) + first_column;
+
+
+	/* RAID 1 */ 
+	if (dev->raid_level == HPSA_RAID_1) {
 		/* Handles load balance across RAID 1 members.
 		 * (2-drive R1 and R10 with even # of drives.)
-		 * Appropriate for SSDs, not optimal for HDDs
+		 * Appropriate for SSDs, not optimal for HDDs 
 		 */
-		BUG_ON(le16_to_cpu(map->layout_map_count) != 2);
+		BUG_ON(map->layout_map_count != 2 );
 		if (dev->offload_to_mirror)
-			map_index += le16_to_cpu(map->data_disks_per_row);
+			map_index += map->data_disks_per_row;
 		dev->offload_to_mirror = !dev->offload_to_mirror;
-		break;
-	case HPSA_RAID_ADM:
+	}
+	/* RAID ADM */
+        else if (dev->raid_level == HPSA_RAID_ADM) {
 		/* Handles N-way mirrors  (R1-ADM)
 		 * and R10 with # of drives divisible by 3.)
 		 */
-		BUG_ON(le16_to_cpu(map->layout_map_count) != 3);
-
+		BUG_ON(map->layout_map_count != 3 );
+		
 		offload_to_mirror = dev->offload_to_mirror;
-		raid_map_helper(map, offload_to_mirror,
-				&map_index, &current_group);
-		/* set mirror group to use next time */
-		offload_to_mirror =
-			(offload_to_mirror >=
-			le16_to_cpu(map->layout_map_count) - 1)
-			? 0 : offload_to_mirror + 1;
-		dev->offload_to_mirror = offload_to_mirror;
-		/* Avoid direct use of dev->offload_to_mirror within this
-		 * function since multiple threads might simultaneously
-		 * increment it beyond the range of dev->layout_map_count -1.
-		 */
-		break;
-	case HPSA_RAID_5:
-	case HPSA_RAID_6:
-		if (le16_to_cpu(map->layout_map_count) <= 1)
-			break;
+		if (offload_to_mirror == 0)  {
+			/* use physical disk in the first mirrored group. */
+			map_index %= map->data_disks_per_row;
+		} else {
+			
+			do {
+				/* determine mirror group that map_index indicates */
+				current_group = map_index / map->data_disks_per_row;
+	
+				if (offload_to_mirror != current_group ) {
+	
+					if (current_group < (map->layout_map_count - 1)) {
+						/* select map index from next group */
+						map_index += map->data_disks_per_row;
+						current_group++;	
+					}
+					else {
+						/* select map index from first group */
+						map_index %= map->data_disks_per_row;
+						current_group = 0;
+					}
+					
+				}
+			} while (offload_to_mirror != current_group);
+
+		}
+
+                /* set mirror group to use next time */
+                offload_to_mirror = (offload_to_mirror >= map->layout_map_count-1) 
+                        ? 0 : offload_to_mirror+1;
+                BUG_ON(offload_to_mirror >= map->layout_map_count); /* FIXME: remove after debug/dev */
+		//dev_warn(&h->pdev->dev, "DEBUG: Using physical disk map index %d "
+		//	"from mirror group %d\n", map_index, offload_to_mirror);
+                dev->offload_to_mirror = offload_to_mirror;
+                /* Avoid direct use of dev->offload_to_mirror within this function since multiple threads
+                 * might simultaneously increment it beyond the range of dev->layout_map_count -1.
+                 */
+        }
+
+	/* RAID 50/60 */
+        else if ((dev->raid_level == HPSA_RAID_5 || 
+		dev->raid_level == HPSA_RAID_6) &&
+		map->layout_map_count > 1) {
 
 		/* Verify first and last block are in same RAID group */
-		r5or6_blocks_per_row =
-			le16_to_cpu(map->strip_size) *
-			le16_to_cpu(map->data_disks_per_row);
+		r5or6_blocks_per_row = map->strip_size * map->data_disks_per_row;
 		BUG_ON(r5or6_blocks_per_row == 0);
-		stripesize = r5or6_blocks_per_row *
-			le16_to_cpu(map->layout_map_count);
+		stripesize = r5or6_blocks_per_row * map->layout_map_count;
 #if BITS_PER_LONG == 32
 		tmpdiv = first_block;
 		first_group = do_div(tmpdiv, stripesize);
 		tmpdiv = first_group;
-		(void) do_div(tmpdiv, r5or6_blocks_per_row);
+		(void) do_div(tmpdiv, r5or6_blocks_per_row);					
 		first_group = tmpdiv;
 		tmpdiv = last_block;
 		last_group = do_div(tmpdiv, stripesize);
 		tmpdiv = last_group;
-		(void) do_div(tmpdiv, r5or6_blocks_per_row);
+		(void) do_div(tmpdiv, r5or6_blocks_per_row);					
 		last_group = tmpdiv;
 #else
 		first_group = (first_block % stripesize) / r5or6_blocks_per_row;
 		last_group = (last_block % stripesize) / r5or6_blocks_per_row;
 #endif
-		if (first_group != last_group)
+		if (first_group != last_group )  
 			return IO_ACCEL_INELIGIBLE;
 
 		/* Verify request is in a single row of RAID 5/6 */
@@ -3881,60 +4986,55 @@ static int hpsa_scsi_ioaccel_raid_map(struct ctlr_info *h,
 		tmpdiv = last_block;
 		(void) do_div(tmpdiv, stripesize);
 		r5or6_last_row = r0_last_row = tmpdiv;
-#else
-		first_row = r5or6_first_row = r0_first_row =
-						first_block / stripesize;
+#else 
+		first_row = r5or6_first_row = r0_first_row = first_block / stripesize;
 		r5or6_last_row = r0_last_row = last_block / stripesize;
 #endif
-		if (r5or6_first_row != r5or6_last_row)
+		if (r5or6_first_row != r5or6_last_row )
 			return IO_ACCEL_INELIGIBLE;
-
-
+		
+	
 		/* Verify request is in a single column */
 #if BITS_PER_LONG == 32
 		tmpdiv = first_block;
-		first_row_offset = do_div(tmpdiv, stripesize);
+		first_row_offset = do_div(tmpdiv, stripesize); 
 		tmpdiv = first_row_offset;
-		first_row_offset = (u32) do_div(tmpdiv, r5or6_blocks_per_row);
+		first_row_offset = (u32) do_div(tmpdiv, r5or6_blocks_per_row); 
 		r5or6_first_row_offset = first_row_offset;
 		tmpdiv = last_block;
-		r5or6_last_row_offset = do_div(tmpdiv, stripesize);
+		r5or6_last_row_offset = do_div(tmpdiv, stripesize); 
 		tmpdiv = r5or6_last_row_offset;
 		r5or6_last_row_offset = do_div(tmpdiv, r5or6_blocks_per_row);
 		tmpdiv = r5or6_first_row_offset;
 		(void) do_div(tmpdiv, map->strip_size);
-		first_column = r5or6_first_column = tmpdiv;
+		first_column = r5or6_first_column = tmpdiv; 
 		tmpdiv = r5or6_last_row_offset;
 		(void) do_div(tmpdiv, map->strip_size);
-		r5or6_last_column = tmpdiv;
+		r5or6_last_column = tmpdiv; 
 #else
-		first_row_offset = r5or6_first_row_offset =
-			(u32)((first_block % stripesize) %
-						r5or6_blocks_per_row);
-
-		r5or6_last_row_offset =
-			(u32)((last_block % stripesize) %
-						r5or6_blocks_per_row);
-
-		first_column = r5or6_first_column =
-			r5or6_first_row_offset / le16_to_cpu(map->strip_size);
-		r5or6_last_column =
-			r5or6_last_row_offset / le16_to_cpu(map->strip_size);
+		first_row_offset = r5or6_first_row_offset = 
+			(u32)((first_block % stripesize ) % r5or6_blocks_per_row);	
+
+		r5or6_last_row_offset = 
+			(u32)((last_block % stripesize ) % r5or6_blocks_per_row);	
+
+		first_column = r5or6_first_column = 
+			r5or6_first_row_offset / map->strip_size; 
+		r5or6_last_column = 
+			r5or6_last_row_offset / map->strip_size;
 #endif
-		if (r5or6_first_column != r5or6_last_column)
+		if (r5or6_first_column != r5or6_last_column ) 
 			return IO_ACCEL_INELIGIBLE;
 
 		/* Request is eligible */
 		map_row = ((u32)(first_row >> map->parity_rotation_shift)) %
-			le16_to_cpu(map->row_cnt);
-
-		map_index = (first_group *
-			(le16_to_cpu(map->row_cnt) * total_disks_per_row)) +
-			(map_row * total_disks_per_row) + first_column;
-		break;
-	default:
-		return IO_ACCEL_INELIGIBLE;
-	}
+			map->row_cnt;
+			
+		map_index = (first_group * 
+			(map->row_cnt * total_disks_per_row)) + 
+			(map_row * total_disks_per_row) + first_column; 
+		
+	} /* end RAID 50/60 */
 
 	if (unlikely(map_index >= RAID_MAP_MAX_ENTRIES))
 		return IO_ACCEL_INELIGIBLE;
@@ -3942,10 +5042,8 @@ static int hpsa_scsi_ioaccel_raid_map(struct ctlr_info *h,
 	c->phys_disk = dev->phys_disk[map_index];
 
 	disk_handle = dd[map_index].ioaccel_handle;
-	disk_block = le64_to_cpu(map->disk_starting_blk) +
-			first_row * le16_to_cpu(map->strip_size) +
-			(first_row_offset - first_column *
-			le16_to_cpu(map->strip_size));
+	disk_block = map->disk_starting_blk + (first_row * map->strip_size) +
+			(first_row_offset - (first_column * map->strip_size));
 	disk_block_cnt = block_cnt;
 
 	/* handle differing logical/physical block sizes */
@@ -3993,21 +5091,24 @@ static int hpsa_scsi_ioaccel_raid_map(struct ctlr_info *h,
 }
 
 /* Submit commands down the "normal" RAID stack path */
+/* All callers to hpsa_ciss_submit must check lockup_detected
+ * beforehand, before (opt.) and after calling cmd_alloc
+ */
 static int hpsa_ciss_submit(struct ctlr_info *h,
 	struct CommandList *c, struct scsi_cmnd *cmd,
 	unsigned char scsi3addr[])
 {
+	struct hpsa_scsi_dev_t *dev = cmd->device->hostdata;
 	cmd->host_scribble = (unsigned char *) c;
 	c->cmd_type = CMD_SCSI;
 	c->scsi_cmd = cmd;
 	c->Header.ReplyQueue = 0;  /* unused in simple mode */
 	memcpy(&c->Header.LUN.LunAddrBytes[0], &scsi3addr[0], 8);
-	c->Header.tag = cpu_to_le64((c->cmdindex << DIRECT_LOOKUP_SHIFT));
+	c->Header.tag = cpu_to_le64((u64) c->cmdindex << DIRECT_LOOKUP_SHIFT);
 
 	/* Fill in the request block... */
+	c->Request.Timeout = dev->timeout;
 
-	c->Request.Timeout = 0;
-	memset(c->Request.CDB, 0, sizeof(c->Request.CDB));
 	BUG_ON(cmd->cmd_len > sizeof(c->Request.CDB));
 	c->Request.CDBLen = cmd->cmd_len;
 	memcpy(c->Request.CDB, cmd->cmnd, cmd->cmd_len);
@@ -4050,7 +5151,7 @@ static int hpsa_ciss_submit(struct ctlr_info *h,
 	}
 
 	if (hpsa_scatter_gather(h, c, cmd) < 0) { /* Fill SG list */
-		cmd_free(h, c);
+		hpsa_cmd_resolve_and_free(h, c);
 		return SCSI_MLQUEUE_HOST_BUSY;
 	}
 	enqueue_cmd_and_start_io(h, c);
@@ -4058,33 +5159,126 @@ static int hpsa_ciss_submit(struct ctlr_info *h,
 	return 0;
 }
 
+static void hpsa_cmd_init(struct ctlr_info *h, int index,
+				struct CommandList *c)
+{
+	dma_addr_t cmd_dma_handle, err_dma_handle;
+
+	/* Zero out all of commandlist except the last field, refcount */
+	memset(c, 0, offsetof(struct CommandList, refcount));
+	c->Header.tag = cpu_to_le64((u64) (index << DIRECT_LOOKUP_SHIFT));
+	cmd_dma_handle = h->cmd_pool_dhandle + index * sizeof(*c);
+	c->err_info = h->errinfo_pool + index;
+	memset(c->err_info, 0, sizeof(*c->err_info));
+	err_dma_handle = h->errinfo_pool_dhandle
+	    + index * sizeof(*c->err_info);
+	c->cmdindex = index;
+	c->busaddr = (u32) cmd_dma_handle;
+	c->ErrDesc.Addr = cpu_to_le64((u64) err_dma_handle);
+	c->ErrDesc.Len = cpu_to_le32((u32) sizeof(*c->err_info));
+	c->h = h;
+	c->scsi_cmd = SCSI_CMD_IDLE;
+}
+
+static int hpsa_ioaccel_submit(struct ctlr_info *h,
+		struct CommandList *c, struct scsi_cmnd *cmd,
+		unsigned char *scsi3addr)
+{
+	struct hpsa_scsi_dev_t *dev = cmd->device->hostdata;
+	int rc = IO_ACCEL_INELIGIBLE;
+
+	cmd->host_scribble = (unsigned char *) c;
+	c->scsi_cmd = cmd;
+	c->cmd_type = CMD_SCSI;
+
+	if (dev->offload_enabled &&
+		cmd->request->cmd_type == REQ_TYPE_FS) {
+		rc = hpsa_scsi_ioaccel_raid_map(h, c);
+		if (rc < 0)     /* scsi_dma_map failed. */
+			rc = SCSI_MLQUEUE_HOST_BUSY;
+	} else if (dev->hba_ioaccel_enabled) {
+		rc = hpsa_scsi_ioaccel_direct_map(h, c);
+		if (rc < 0)     /* scsi_dma_map failed. */
+			rc = SCSI_MLQUEUE_HOST_BUSY;
+	}
+	return rc;
+}
+
+static void hpsa_preinitialize_commands(struct ctlr_info *h)
+{
+	int i;
+
+	for (i = 0; i < h->nr_cmds; i++) {
+		struct CommandList *c = h->cmd_pool + i;
+		hpsa_cmd_init(h, i, c);
+		atomic_set(&c->refcount, 0);
+	}
+}
+
+static inline void hpsa_cmd_partial_init(struct ctlr_info *h, int index,
+				struct CommandList *c)
+{
+	dma_addr_t cmd_dma_handle = h->cmd_pool_dhandle + index * sizeof(*c);
+
+	memset(c->Request.CDB, 0, sizeof(c->Request.CDB));
+	memset(c->err_info, 0, sizeof(*c->err_info));
+	c->busaddr = (u32) cmd_dma_handle;
+}
+
 static void hpsa_command_resubmit_worker(struct work_struct *work)
 {
 	struct scsi_cmnd *cmd;
 	struct hpsa_scsi_dev_t *dev;
-	struct CommandList *c =
-			container_of(work, struct CommandList, work);
+	struct CommandList *c = container_of(work, struct CommandList, work);
 
 	cmd = c->scsi_cmd;
 	dev = cmd->device->hostdata;
 	if (!dev) {
 		cmd->result = DID_NO_CONNECT << 16;
-		cmd->scsi_done(cmd);
-		return;
+		return hpsa_cmd_free_and_done(c->h, c, cmd);
+	}
+	if (c->reset_pending)
+		return hpsa_cmd_resolve_and_free(c->h, c);
+	if (c->abort_pending)
+		return hpsa_cmd_abort_and_free(c->h, c, cmd);
+	if (c->cmd_type == CMD_IOACCEL2) {
+		struct ctlr_info *h = c->h;
+		struct io_accel2_cmd *c2 = &h->ioaccel2_cmd_pool[c->cmdindex];
+		int rc;
+
+		if (c2->error_data.serv_response ==
+				IOACCEL2_STATUS_SR_TASK_COMP_SET_FULL) {
+			rc = hpsa_ioaccel_submit(h, c, cmd, dev->scsi3addr);
+			if (rc == 0)
+				return;
+			if (rc == SCSI_MLQUEUE_HOST_BUSY) {
+				/*
+				 * If we get here, it means dma mapping failed.
+				 * Try again via scsi mid layer, which will
+				 * then get SCSI_MLQUEUE_HOST_BUSY.
+				 */
+				cmd->result = DID_IMM_RETRY << 16;
+				return hpsa_cmd_free_and_done(h, c, cmd);
+			}
+			/* else, fall thru and resubmit down CISS path */
+		}
 	}
+	hpsa_cmd_partial_init(c->h, c->cmdindex, c);
 	if (hpsa_ciss_submit(c->h, c, cmd, dev->scsi3addr)) {
 		/*
 		 * If we get here, it means dma mapping failed. Try
 		 * again via scsi mid layer, which will then get
 		 * SCSI_MLQUEUE_HOST_BUSY.
+		 *
+		 * hpsa_ciss_submit will have already freed c
+		 * if it encountered a dma mapping failure.
 		 */
 		cmd->result = DID_IMM_RETRY << 16;
 		cmd->scsi_done(cmd);
 	}
 }
 
-/* Running in struct Scsi_Host->host_lock less mode */
-static int hpsa_scsi_queue_command(struct Scsi_Host *sh, struct scsi_cmnd *cmd)
+DECLARE_QUEUECOMMAND(hpsa_scsi_queue_command)
 {
 	struct ctlr_info *h;
 	struct hpsa_scsi_dev_t *dev;
@@ -4097,69 +5291,65 @@ static int hpsa_scsi_queue_command(struct Scsi_Host *sh, struct scsi_cmnd *cmd)
 	dev = cmd->device->hostdata;
 	if (!dev) {
 		cmd->result = DID_NO_CONNECT << 16;
-		cmd->scsi_done(cmd);
+		hpsa_scsi_done(cmd, done);
 		return 0;
 	}
+
+	if (bail_on_report_luns_if_no_scan_start(cmd, done))
+		return 0;
+
 	memcpy(scsi3addr, dev->scsi3addr, sizeof(scsi3addr));
 
 	if (unlikely(lockup_detected(h))) {
-		cmd->result = DID_ERROR << 16;
-		cmd->scsi_done(cmd);
+		cmd->result = DID_NO_CONNECT << 16;
+		hpsa_scsi_done(cmd, done);
 		return 0;
 	}
 	c = cmd_alloc(h);
-	if (c == NULL) {			/* trouble... */
-		dev_err(&h->pdev->dev, "cmd_alloc returned NULL!\n");
-		return SCSI_MLQUEUE_HOST_BUSY;
-	}
+
 	if (unlikely(lockup_detected(h))) {
-		cmd->result = DID_ERROR << 16;
-		cmd_free(h, c);
+		cmd->result = DID_NO_CONNECT << 16;
+		cmd_free(h, c); /* FIXME may not be necessary, as lockup detector also frees everything */
 		cmd->scsi_done(cmd);
-		return 0;
 	}
 
-	/*
-	 * Call alternate submit routine for I/O accelerated commands.
-	 * Retries always go down the normal I/O path.
-	 */
-	if (likely(cmd->retries == 0 &&
-		cmd->request->cmd_type == REQ_TYPE_FS &&
-		h->acciopath_status)) {
-
-		cmd->host_scribble = (unsigned char *) c;
-		c->cmd_type = CMD_SCSI;
-		c->scsi_cmd = cmd;
+	/* Fill in the command list header */
+	hpsa_save_scsi_done(cmd, done);
 
-		if (dev->offload_enabled) {
-			rc = hpsa_scsi_ioaccel_raid_map(h, c);
-			if (rc == 0)
-				return 0; /* Sent on ioaccel path */
-			if (rc < 0) {   /* scsi_dma_map failed. */
-				cmd_free(h, c);
-				return SCSI_MLQUEUE_HOST_BUSY;
-			}
-		} else if (dev->ioaccel_handle) {
-			rc = hpsa_scsi_ioaccel_direct_map(h, c);
-			if (rc == 0)
-				return 0; /* Sent on direct map path */
-			if (rc < 0) {   /* scsi_dma_map failed. */
-				cmd_free(h, c);
-				return SCSI_MLQUEUE_HOST_BUSY;
-			}
+	/* Call alternate submit routine for I/O accelerated commands.
+ 	 * Retries always go down the normal I/O path.
+	 */
+	if (likely(h->acciopath_status) && cmd->retries == 0) {
+		rc = hpsa_ioaccel_submit(h, c, cmd, scsi3addr);
+		if (rc == 0)
+			return 0;
+		if (rc == SCSI_MLQUEUE_HOST_BUSY) {
+			hpsa_cmd_resolve_and_free(h, c);
+			return SCSI_MLQUEUE_HOST_BUSY;
 		}
 	}
 	return hpsa_ciss_submit(h, c, cmd, scsi3addr);
 }
 
-static void hpsa_scan_complete(struct ctlr_info *h)
+static int do_not_scan_if_controller_locked_up(struct ctlr_info *h)
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&h->scan_lock, flags);
-	h->scan_finished = 1;
-	wake_up_all(&h->scan_wait_queue);
-	spin_unlock_irqrestore(&h->scan_lock, flags);
+	/*
+	 * Don't let rescans be initiated on a controller known
+	 * to be locked up.  If the controller locks up *during*
+	 * a rescan, that thread is probably hosed, but at least
+	 * we can prevent new rescan threads from piling up on a
+	 * locked up controller.
+	 */
+	if (unlikely(lockup_detected(h))) {
+		spin_lock_irqsave(&h->scan_lock, flags);
+		h->scan_finished = 1;
+		wake_up_all(&h->scan_wait_queue);
+		spin_unlock_irqrestore(&h->scan_lock, flags);
+		return 1;
+	}
+	return 0;
 }
 
 static void hpsa_scan_start(struct Scsi_Host *sh)
@@ -4167,14 +5357,8 @@ static void hpsa_scan_start(struct Scsi_Host *sh)
 	struct ctlr_info *h = shost_to_hba(sh);
 	unsigned long flags;
 
-	/*
-	 * Don't let rescans be initiated on a controller known to be locked
-	 * up.  If the controller locks up *during* a rescan, that thread is
-	 * probably hosed, but at least we can prevent new rescan threads from
-	 * piling up on a locked up controller.
-	 */
-	if (unlikely(lockup_detected(h)))
-		return hpsa_scan_complete(h);
+	if (do_not_scan_if_controller_locked_up(h))
+		return;
 
 	/* wait until any scan already in progress is finished. */
 	while (1) {
@@ -4192,27 +5376,15 @@ static void hpsa_scan_start(struct Scsi_Host *sh)
 	h->scan_finished = 0; /* mark scan as in progress */
 	spin_unlock_irqrestore(&h->scan_lock, flags);
 
-	if (unlikely(lockup_detected(h)))
-		return hpsa_scan_complete(h);
+	if (do_not_scan_if_controller_locked_up(h))
+		return;
 
 	hpsa_update_scsi_devices(h, h->scsi_host->host_no);
 
-	hpsa_scan_complete(h);
-}
-
-static int hpsa_change_queue_depth(struct scsi_device *sdev, int qdepth)
-{
-	struct hpsa_scsi_dev_t *logical_drive = sdev->hostdata;
-
-	if (!logical_drive)
-		return -ENODEV;
-
-	if (qdepth < 1)
-		qdepth = 1;
-	else if (qdepth > logical_drive->queue_depth)
-		qdepth = logical_drive->queue_depth;
-
-	return scsi_change_queue_depth(sdev, qdepth);
+	spin_lock_irqsave(&h->scan_lock, flags);
+	h->scan_finished = 1; /* mark scan as finished. */
+	wake_up_all(&h->scan_wait_queue);
+	spin_unlock_irqrestore(&h->scan_lock, flags);
 }
 
 static int hpsa_scan_finished(struct Scsi_Host *sh,
@@ -4228,6 +5400,47 @@ static int hpsa_scan_finished(struct Scsi_Host *sh,
 	return finished;
 }
 
+/* scsi host template change_queue_depth function */
+DECLARE_CHANGE_QUEUE_DEPTH(hpsa_change_queue_depth)
+{
+	int reason = -1;
+
+	SET_QUEUE_DEPTH_REASON(reason); /* This sets reason */
+
+	if (reason == SCSI_QDEPTH_DEFAULT || reason == SCSI_QDEPTH_RAMP_UP) {
+		struct hpsa_scsi_dev_t *logical_drive = sdev->hostdata;
+
+		if (!logical_drive)
+			return -ENODEV;
+
+		if (qdepth < 1)
+			qdepth = 1;
+		else if (qdepth > logical_drive->queue_depth)
+			qdepth = logical_drive->queue_depth;
+
+		scsi_adjust_queue_depth(sdev, scsi_get_tag_type(sdev), qdepth);
+	} else if (reason == SCSI_QDEPTH_QFULL)
+		scsi_track_queue_full(sdev, qdepth);
+	else
+		return -ENOTSUPP;
+
+	return sdev->queue_depth;
+}
+
+static int hpsa_change_queue_type(struct scsi_device *sdev, int tag_type)
+{
+	if (sdev->tagged_supported) {
+		scsi_set_tag_type(sdev, tag_type);
+		if (tag_type)
+			scsi_activate_tcq(sdev, sdev->queue_depth);
+		else
+			scsi_deactivate_tcq(sdev, sdev->queue_depth);
+	} else
+		tag_type = 0;
+
+	return tag_type;
+}
+
 static void hpsa_unregister_scsi(struct ctlr_info *h)
 {
 	/* we are being forcibly unloaded, and may not refuse. */
@@ -4241,6 +5454,9 @@ static int hpsa_register_scsi(struct ctlr_info *h)
 	struct Scsi_Host *sh;
 	int error;
 
+	/* This is a no-op on kernels with working ->scan_start() */
+	hpsa_initial_update_scsi_devices(h);
+
 	sh = scsi_host_alloc(&hpsa_driver_template, sizeof(h));
 	if (sh == NULL)
 		goto fail;
@@ -4279,53 +5495,99 @@ static int hpsa_register_scsi(struct ctlr_info *h)
 	return -ENOMEM;
 }
 
-static int wait_for_device_to_become_ready(struct ctlr_info *h,
-	unsigned char lunaddr[])
+/* Send a TEST_UNIT_READY command to the specified LUN using the specified
+ * reply queue; returns zero if the unit is ready, and non-zero otherwise. */
+static int hpsa_send_test_unit_ready(struct ctlr_info *h,
+				struct CommandList *c, unsigned char lunaddr[],
+				int reply_queue)
+{
+	int rc;
+
+	/* Send the Test Unit Ready, fill_cmd can't fail, no mapping */
+	(void) fill_cmd(c, TEST_UNIT_READY, h,
+			NULL, 0, 0, lunaddr, TYPE_CMD);
+	rc = hpsa_scsi_do_simple_cmd(h, c, reply_queue, NO_TIMEOUT);
+	if (rc)
+		return rc;
+	/* no unmap needed here because no data xfer. */
+
+	/* Check if the unit is already ready. */
+	if (c->err_info->CommandStatus == CMD_SUCCESS)
+		return 0;
+
+	/* The first command sent after reset will receive "unit attention" to
+	 * indicate that the LUN has been reset...this is actually what we're
+	 * looking for (but, success is good too). */
+	if (c->err_info->CommandStatus == CMD_TARGET_STATUS &&
+		c->err_info->ScsiStatus == SAM_STAT_CHECK_CONDITION &&
+			(c->err_info->SenseInfo[2] == NO_SENSE ||
+			 c->err_info->SenseInfo[2] == UNIT_ATTENTION))
+		return 0;
+
+	return 1;
+}
+
+/* Wait for a TEST_UNIT_READY command to complete, retrying as necessary;
+ * returns zero when the unit is ready, and non-zero when giving up. */
+static int hpsa_wait_for_test_unit_ready(struct ctlr_info *h, struct CommandList *c,
+				    unsigned char lunaddr[], int reply_queue)
 {
 	int rc;
 	int count = 0;
 	int waittime = 1; /* seconds */
-	struct CommandList *c;
-
-	c = cmd_alloc(h);
-	if (!c) {
-		dev_warn(&h->pdev->dev, "out of memory in "
-			"wait_for_device_to_become_ready.\n");
-		return IO_ERROR;
-	}
 
 	/* Send test unit ready until device ready, or give up. */
-	while (count < HPSA_TUR_RETRY_LIMIT) {
+	for (count = 0; count < HPSA_TUR_RETRY_LIMIT; count++) {
 
 		/* Wait for a bit.  do this first, because if we send
 		 * the TUR right away, the reset will just abort it.
 		 */
 		msleep(1000 * waittime);
-		count++;
-		rc = 0; /* Device ready. */
+
+		rc = hpsa_send_test_unit_ready(h, c, lunaddr, reply_queue);
+		if (!rc)
+			break;
 
 		/* Increase wait time with each try, up to a point. */
 		if (waittime < HPSA_MAX_WAIT_INTERVAL_SECS)
 			waittime = waittime * 2;
 
-		/* Send the Test Unit Ready, fill_cmd can't fail, no mapping */
-		(void) fill_cmd(c, TEST_UNIT_READY, h,
-				NULL, 0, 0, lunaddr, TYPE_CMD);
-		hpsa_scsi_do_simple_cmd_core(h, c);
-		/* no unmap needed here because no data xfer. */
+		dev_warn(&h->pdev->dev,
+			 "waiting %d secs for device to become ready.\n",
+			 waittime);
+	}
 
-		if (c->err_info->CommandStatus == CMD_SUCCESS)
-			break;
+	return rc;
+}
 
-		if (c->err_info->CommandStatus == CMD_TARGET_STATUS &&
-			c->err_info->ScsiStatus == SAM_STAT_CHECK_CONDITION &&
-			(c->err_info->SenseInfo[2] == NO_SENSE ||
-			c->err_info->SenseInfo[2] == UNIT_ATTENTION))
-			break;
+static int wait_for_device_to_become_ready(struct ctlr_info *h,
+					   unsigned char lunaddr[],
+					   int reply_queue)
+{
+	int first_queue;
+	int last_queue;
+	int rq;
+	int rc = 0;
+	struct CommandList *c;
+
+	c = cmd_alloc(h);
+
+	/* If no specific reply queue was requested, then send the TUR
+	 * repeatedly, requesting a reply on each reply queue; otherwise execute
+	 * the loop exactly once using only the specified queue. */
+	if (likely(reply_queue == DEFAULT_REPLY_QUEUE)) {
+		first_queue = 0;
+		last_queue = h->nreply_queues - 1;
+	} else {
+		first_queue = reply_queue;
+		last_queue = reply_queue;
+	}
 
-		dev_warn(&h->pdev->dev, "waiting %d secs "
-			"for device to become ready.\n", waittime);
-		rc = 1; /* device not ready. */
+	for (rq = first_queue; rq <= last_queue; rq++) {
+		rc = hpsa_wait_for_test_unit_ready(h, c, lunaddr,
+			reply_queue == DEFAULT_REPLY_QUEUE ? rq : reply_queue);
+		if (rc)
+			break;
 	}
 
 	if (rc)
@@ -4337,9 +5599,6 @@ static int wait_for_device_to_become_ready(struct ctlr_info *h,
 	return rc;
 }
 
-/* Need at least one of these error handlers to keep ../scsi/hosts.c from
- * complaining.  Doing a host- or bus-reset can't do anything good here.
- */
 static int hpsa_eh_device_reset_handler(struct scsi_cmnd *scsicmd)
 {
 	int rc;
@@ -4351,24 +5610,56 @@ static int hpsa_eh_device_reset_handler(struct scsi_cmnd *scsicmd)
 	if (h == NULL) /* paranoia */
 		return FAILED;
 
-	if (lockup_detected(h))
-		return FAILED;
-
 	dev = scsicmd->device->hostdata;
 	if (!dev) {
-		dev_err(&h->pdev->dev, "hpsa_eh_device_reset_handler: "
-			"device lookup failed.\n");
+		dev_err(&h->pdev->dev, "%s: device lookup failed\n", __func__);
 		return FAILED;
 	}
-	dev_warn(&h->pdev->dev, "resetting device %d:%d:%d:%d\n",
-		h->scsi_host->host_no, dev->bus, dev->target, dev->lun);
-	/* send a reset to the SCSI LUN which the command was sent to */
-	rc = hpsa_send_reset(h, dev->scsi3addr, HPSA_RESET_TYPE_LUN);
-	if (rc == 0 && wait_for_device_to_become_ready(h, dev->scsi3addr) == 0)
+
+	/* if controller locked up, we can guarantee command won't complete */
+	if (lockup_detected(h)) {
+		dev_warn(&h->pdev->dev,
+			 "scsi %d:%d:%d:%d RESET FAILED, lockup detected\n",
+			 h->scsi_host->host_no, dev->bus, dev->target, dev->lun);
+		return FAILED;
+	} else {
+		/* this reset request might be the result of a lockup; check */
+		detect_controller_lockup(h);
+
+		/* check to see if one just occurred */
+		if (lockup_detected(h)) {
+			dev_warn(&h->pdev->dev,
+				 "scsi %d:%d:%d:%d RESET FAILED, new lockup detected\n",
+				 h->scsi_host->host_no, dev->bus, dev->target,
+				 dev->lun);
+			return FAILED;
+		}
+	}
+
+	/* Do not attempt on controller */
+	if (is_hba_lunid(dev->scsi3addr))
 		return SUCCESS;
 
-	dev_warn(&h->pdev->dev, "resetting device failed.\n");
-	return FAILED;
+	dev_warn(&h->pdev->dev,
+		"resetting scsi %d:%d:%d:%d: %s %.8s %.16s RAID-%s SSDSmartPathCap%c En%c Exp=%d\n",
+		h->scsi_host->host_no, dev->bus, dev->target, dev->lun,
+		scsi_device_type(dev->devtype),
+		dev->vendor,
+		dev->model,
+		dev->raid_level > RAID_UNKNOWN ?
+			"RAID-?" : raid_label[dev->raid_level],
+		dev->offload_config ? '+' : '-',
+		dev->offload_enabled ? '+' : '-',
+		dev->expose_state);
+
+	/* send a reset to the SCSI LUN which the command was sent to */
+	rc = hpsa_do_reset(h, dev, dev->scsi3addr, HPSA_DEVICE_RESET_MSG,
+			     DEFAULT_REPLY_QUEUE);
+	dev_warn(&h->pdev->dev,
+		 "resetting scsi %d:%d:%d:%d %s\n",
+		 h->scsi_host->host_no, dev->bus, dev->target, dev->lun,
+		 rc == 0 ? "completed successfully" : "failed");
+	return rc == 0 ? SUCCESS : FAILED;
 }
 
 static void swizzle_abort_tag(u8 *tag)
@@ -4387,52 +5678,45 @@ static void swizzle_abort_tag(u8 *tag)
 }
 
 static void hpsa_get_tag(struct ctlr_info *h,
-	struct CommandList *c, __le32 *taglower, __le32 *tagupper)
+	struct CommandList *c, u32 *taglower, u32 *tagupper)
 {
-	u64 tag;
+
 	if (c->cmd_type == CMD_IOACCEL1) {
 		struct io_accel1_cmd *cm1 = (struct io_accel1_cmd *)
 			&h->ioaccel_cmd_pool[c->cmdindex];
-		tag = le64_to_cpu(cm1->tag);
-		*tagupper = cpu_to_le32(tag >> 32);
-		*taglower = cpu_to_le32(tag);
+		*tagupper = (u32) (cm1->tag >> 32);
+		*taglower = (u32) (cm1->tag & 0x0ffffffffULL);
 		return;
 	}
 	if (c->cmd_type == CMD_IOACCEL2) {
 		struct io_accel2_cmd *cm2 = (struct io_accel2_cmd *)
 			&h->ioaccel2_cmd_pool[c->cmdindex];
-		/* upper tag not used in ioaccel2 mode */
-		memset(tagupper, 0, sizeof(*tagupper));
+		memset(tagupper, 0, sizeof(*tagupper)); /* upper tag isn't used in ioaccel2 mode */	
 		*taglower = cm2->Tag;
 		return;
 	}
-	tag = le64_to_cpu(c->Header.tag);
-	*tagupper = cpu_to_le32(tag >> 32);
-	*taglower = cpu_to_le32(tag);
+	*tagupper = (u32) (c->Header.tag >> 32);
+	*taglower = (u32) (c->Header.tag & 0x0ffffffffULL);
 }
 
 static int hpsa_send_abort(struct ctlr_info *h, unsigned char *scsi3addr,
-	struct CommandList *abort, int swizzle)
+	struct CommandList *abort, int reply_queue)
 {
 	int rc = IO_OK;
 	struct CommandList *c;
 	struct ErrorInfo *ei;
-	__le32 tagupper, taglower;
+	u32 tagupper, taglower;
 
 	c = cmd_alloc(h);
-	if (c == NULL) {	/* trouble... */
-		dev_warn(&h->pdev->dev, "cmd_alloc returned NULL!\n");
-		return -ENOMEM;
-	}
 
 	/* fill_cmd can't fail here, no buffer to map */
-	(void) fill_cmd(c, HPSA_ABORT_MSG, h, abort,
+	(void) fill_cmd(c, HPSA_ABORT_MSG, h, &abort->Header.tag,
 		0, 0, scsi3addr, TYPE_MSG);
-	if (swizzle)
+	if (h->needs_abort_tags_swizzled)
 		swizzle_abort_tag(&c->Request.CDB[4]);
-	hpsa_scsi_do_simple_cmd_core(h, c);
+	(void) hpsa_scsi_do_simple_cmd(h, c, reply_queue, NO_TIMEOUT);
 	hpsa_get_tag(h, abort, &taglower, &tagupper);
-	dev_dbg(&h->pdev->dev, "%s: Tag:0x%08x:%08x: do_simple_cmd_core completed.\n",
+	dev_dbg(&h->pdev->dev, "%s: Tag:0x%08x:%08x: do_simple_cmd(abort) completed.\n",
 		__func__, tagupper, taglower);
 	/* no unmap needed here because no data xfer. */
 
@@ -4440,118 +5724,272 @@ static int hpsa_send_abort(struct ctlr_info *h, unsigned char *scsi3addr,
 	switch (ei->CommandStatus) {
 	case CMD_SUCCESS:
 		break;
+	case CMD_TMF_STATUS:
+		rc = hpsa_evaluate_tmf_status(h, c);
+		break;
 	case CMD_UNABORTABLE: /* Very common, don't make noise. */
 		rc = -1;
 		break;
 	default:
-		dev_dbg(&h->pdev->dev, "%s: Tag:0x%08x:%08x: interpreting error.\n",
+		dev_dbg(&h->pdev->dev, "%s: Tag:0x%08x:%08x: "
+			"interpreting error.\n",
 			__func__, tagupper, taglower);
 		hpsa_scsi_interpret_error(h, c);
 		rc = -1;
 		break;
 	}
 	cmd_free(h, c);
-	dev_dbg(&h->pdev->dev, "%s: Tag:0x%08x:%08x: Finished.\n",
-		__func__, tagupper, taglower);
+	dev_dbg(&h->pdev->dev, "%s: Tag:0x%08x:%08x: "
+		"Finished.\n", __func__, tagupper, taglower);
 	return rc;
 }
 
+static void setup_ioaccel2_abort_cmd(struct CommandList *c, struct ctlr_info *h,
+	struct CommandList *command_to_abort, int reply_queue)
+{
+	struct io_accel2_cmd *c2 = &h->ioaccel2_cmd_pool[c->cmdindex];
+	struct hpsa_tmf_struct *ac = (struct hpsa_tmf_struct *) c2;
+	struct io_accel2_cmd *c2a =
+		&h->ioaccel2_cmd_pool[command_to_abort->cmdindex];
+	struct scsi_cmnd *scmd = command_to_abort->scsi_cmd;
+	struct hpsa_scsi_dev_t *dev = scmd->device->hostdata;
+
+	/*
+	 * We're overlaying struct hpsa_tmf_struct on top of something which
+	 * was allocated as a struct io_accel2_cmd, so we better be sure it
+	 * actually fits, and doesn't overrun the error info space.
+	 */
+	BUILD_BUG_ON(sizeof(struct hpsa_tmf_struct) >
+			sizeof(struct io_accel2_cmd)); 
+	BUG_ON(offsetof(struct io_accel2_cmd, error_data) <
+			offsetof(struct hpsa_tmf_struct, error_len) +
+				sizeof(ac->error_len));
+
+	c->cmd_type = IOACCEL2_TMF;
+	c->scsi_cmd = SCSI_CMD_BUSY;
+
+	/* Adjust the DMA address to point to the accelerated command buffer */
+	c->busaddr = (u32) h->ioaccel2_cmd_pool_dhandle +
+				(c->cmdindex * sizeof(struct io_accel2_cmd));
+	BUG_ON(c->busaddr & 0x0000007F);
+
+	memset(ac, 0, sizeof(*c2)); /* yes this is correct */
+	ac->iu_type = IOACCEL2_IU_TMF_TYPE;
+	ac->reply_queue = reply_queue;
+	ac->tmf = IOACCEL2_TMF_ABORT;
+	ac->it_nexus = cpu_to_le32((u32) dev->ioaccel_handle);
+	memset(ac->lun_id, 0, sizeof(ac->lun_id));
+	ac->tag = c->cmdindex << DIRECT_LOOKUP_SHIFT;
+	ac->abort_tag = c2a->Tag;
+	ac->error_ptr = cpu_to_le64((u64) c->busaddr +
+			offsetof(struct io_accel2_cmd, error_data));
+	ac->error_len = cpu_to_le32((u32) sizeof(c2->error_data));
+	/* Set the bits in the address sent down to include:
+	 *  - performant mode bit not used in ioaccel mode 2
+	 *  - pull count (bits 0-3)
+	 */
+}
+
 /* ioaccel2 path firmware cannot handle abort task requests.
- * Change abort requests to physical target reset, and send to the
+ * Change abort requests to physical target reset, and send to the 
  * address of the physical disk used for the ioaccel 2 command.
  * Return 0 on success (IO_OK)
  *	 -1 on failure
  */
 
 static int hpsa_send_reset_as_abort_ioaccel2(struct ctlr_info *h,
-	unsigned char *scsi3addr, struct CommandList *abort)
+	unsigned char *scsi3addr, struct CommandList *abort, int reply_queue)
 {
 	int rc = IO_OK;
-	struct scsi_cmnd *scmd; /* scsi command within request being aborted */
+        struct scsi_cmnd *scmd; /* scsi command within request being aborted */
 	struct hpsa_scsi_dev_t *dev; /* device to which scsi cmd was sent */
-	unsigned char phys_scsi3addr[8]; /* addr of phys disk with volume */
+	unsigned char phys_scsi3addr[8]; /* address of physical disk with volume */
 	unsigned char *psa = &phys_scsi3addr[0];
 
-	/* Get a pointer to the hpsa logical device. */
+        /* Get a pointer to the hpsa logical device. */
 	scmd = abort->scsi_cmd;
 	dev = (struct hpsa_scsi_dev_t *)(scmd->device->hostdata);
-	if (dev == NULL) {
-		dev_warn(&h->pdev->dev,
+	if ( dev == NULL ) {
+		printk(KERN_WARNING 
 			"Cannot abort: no device pointer for command.\n");
 			return -1; /* not abortable */
 	}
-
-	if (h->raid_offload_debug > 0)
+	
+	if (h->raid_offload_debug > 0 )
 		dev_info(&h->pdev->dev,
-			"Reset as abort: Abort requested on C%d:B%d:T%d:L%d scsi3addr 0x%02x%02x%02x%02x%02x%02x%02x%02x\n",
-			h->scsi_host->host_no, dev->bus, dev->target, dev->lun,
+			"Reset as abort: scsi %d:%d:%d:%d "
+			"scsi3addr 0x%02x%02x%02x%02x%02x%02x%02x%02x\n",
+               		 h->scsi_host->host_no, dev->bus, dev->target, dev->lun,
 			scsi3addr[0], scsi3addr[1], scsi3addr[2], scsi3addr[3],
 			scsi3addr[4], scsi3addr[5], scsi3addr[6], scsi3addr[7]);
 
 	if (!dev->offload_enabled) {
-		dev_warn(&h->pdev->dev,
-			"Can't abort: device is not operating in HP SSD Smart Path mode.\n");
+		dev_warn(&h->pdev->dev, "Can't abort: device is not operating "
+			"in HP SSD Smart Path mode.\n"); 
 		return -1; /* not abortable */
 	}
 
 	/* Incoming scsi3addr is logical addr. We need physical disk addr. */
-	if (!hpsa_get_pdisk_of_ioaccel2(h, abort, psa)) {
-		dev_warn(&h->pdev->dev, "Can't abort: Failed lookup of physical address.\n");
+	if (! hpsa_get_pdisk_of_ioaccel2(h, abort, psa)) {
+		dev_warn(&h->pdev->dev, "Can't abort: Failed lookup of physical address.\n"); 
 		return -1; /* not abortable */
 	}
 
 	/* send the reset */
-	if (h->raid_offload_debug > 0)
+	if (h->raid_offload_debug > 0 )
 		dev_info(&h->pdev->dev,
-			"Reset as abort: Resetting physical device at scsi3addr 0x%02x%02x%02x%02x%02x%02x%02x%02x\n",
-			psa[0], psa[1], psa[2], psa[3],
+			"Reset as abort: Resetting physical device at "
+			"scsi3addr 0x%02x%02x%02x%02x%02x%02x%02x%02x\n",
+			psa[0], psa[1], psa[2], psa[3], 
 			psa[4], psa[5], psa[6], psa[7]);
-	rc = hpsa_send_reset(h, psa, HPSA_RESET_TYPE_TARGET);
+	rc = hpsa_do_reset(h, dev, psa, HPSA_PHYS_TARGET_RESET, reply_queue);
 	if (rc != 0) {
 		dev_warn(&h->pdev->dev,
-			"Reset as abort: Failed on physical device at scsi3addr 0x%02x%02x%02x%02x%02x%02x%02x%02x\n",
-			psa[0], psa[1], psa[2], psa[3],
+			"Reset as abort: Failed on physical device at "
+			"scsi3addr 0x%02x%02x%02x%02x%02x%02x%02x%02x\n",
+			psa[0], psa[1], psa[2], psa[3], 
 			psa[4], psa[5], psa[6], psa[7]);
 		return rc; /* failed to reset */
 	}
 
 	/* wait for device to recover */
-	if (wait_for_device_to_become_ready(h, psa) != 0) {
+	if (wait_for_device_to_become_ready(h, psa, reply_queue) != 0) {
 		dev_warn(&h->pdev->dev,
-			"Reset as abort: Failed: Device never recovered from reset: 0x%02x%02x%02x%02x%02x%02x%02x%02x\n",
-			psa[0], psa[1], psa[2], psa[3],
+			"Reset as abort: Failed: Device never recovered "
+			"from reset: 0x%02x%02x%02x%02x%02x%02x%02x%02x\n",
+			psa[0], psa[1], psa[2], psa[3], 
 			psa[4], psa[5], psa[6], psa[7]);
 		return -1;  /* failed to recover */
 	}
 
 	/* device recovered */
 	dev_info(&h->pdev->dev,
-		"Reset as abort: Device recovered from reset: scsi3addr 0x%02x%02x%02x%02x%02x%02x%02x%02x\n",
-		psa[0], psa[1], psa[2], psa[3],
+		"Reset as abort: Device recovered from reset: "
+		"scsi3addr 0x%02x%02x%02x%02x%02x%02x%02x%02x\n",
+		psa[0], psa[1], psa[2], psa[3], 
 		psa[4], psa[5], psa[6], psa[7]);
 
 	return rc; /* success */
 }
 
-/* Some Smart Arrays need the abort tag swizzled, and some don't.  It's hard to
- * tell which kind we're dealing with, so we send the abort both ways.  There
- * shouldn't be any collisions between swizzled and unswizzled tags due to the
- * way we construct our tags but we check anyway in case the assumptions which
- * make this true someday become false.
- */
+static int hpsa_send_abort_ioaccel2(struct ctlr_info *h,
+	struct CommandList *abort, int reply_queue)
+{
+	int rc = IO_OK;
+	struct CommandList *c;
+	u32 taglower, tagupper;
+	struct hpsa_scsi_dev_t *dev;
+	struct io_accel2_cmd* c2;
+
+	dev = abort->scsi_cmd->device->hostdata;
+	if (!dev->offload_enabled && !dev->hba_ioaccel_enabled)
+		return -1;
+
+	c = cmd_alloc(h);
+
+	setup_ioaccel2_abort_cmd(c, h, abort, reply_queue);
+ 	c2 = &h->ioaccel2_cmd_pool[c->cmdindex];
+	(void) hpsa_scsi_do_simple_cmd(h, c, reply_queue, NO_TIMEOUT);
+	hpsa_get_tag(h, abort, &taglower, &tagupper);
+	dev_dbg(&h->pdev->dev, "%s: Tag:0x%08x:%08x: do_simple_cmd(ioaccel2 abort) completed.\n",
+		__func__, tagupper, taglower);
+	/* no unmap needed here because no data xfer. */
+
+	dev_dbg(&h->pdev->dev, "%s: Tag:0x%08x:%08x: abort service response = 0x%02x.\n",
+		__func__, tagupper, taglower, c2->error_data.serv_response);
+	switch (c2->error_data.serv_response) {
+	case IOACCEL2_SERV_RESPONSE_TMF_COMPLETE:
+	case IOACCEL2_SERV_RESPONSE_TMF_SUCCESS:
+		rc = 0;
+		break;
+	case IOACCEL2_SERV_RESPONSE_TMF_REJECTED:
+	case IOACCEL2_SERV_RESPONSE_FAILURE:
+	case IOACCEL2_SERV_RESPONSE_TMF_WRONG_LUN:
+		rc = -1;
+		break;
+	default:
+		dev_warn(&h->pdev->dev, "%s: Tag:0x%08x:%08x: unknown abort service response x0%02x\n",
+			__func__, tagupper, taglower, c2->error_data.serv_response);
+		rc = -1;
+	}
+	cmd_free(h, c);
+	dev_dbg(&h->pdev->dev, "%s: Tag:0x%08x:%08x: Finished.\n", __func__,
+		tagupper, taglower);
+	return rc;
+	
+}
+
 static int hpsa_send_abort_both_ways(struct ctlr_info *h,
-	unsigned char *scsi3addr, struct CommandList *abort)
+	unsigned char *scsi3addr, struct CommandList *abort, int reply_queue)
 {
-	/* ioccelerator mode 2 commands should be aborted via the
-	 * accelerated path, since RAID path is unaware of these commands,
-	 * but underlying firmware can't handle abort TMF.
-	 * Change abort to physical device reset.
+	/* ioccelerator mode 2 commands should be aborted via the 
+	 * accelerated path, since RAID path is unaware of these commands, 
+	 * but not all underlying firmware can handle abort TMF.
+	 * Change abort to physical device reset when abort TMF is unsupported.
 	 */
-	if (abort->cmd_type == CMD_IOACCEL2)
-		return hpsa_send_reset_as_abort_ioaccel2(h, scsi3addr, abort);
+	if (abort->cmd_type == CMD_IOACCEL2) {
+		if (HPSATMF_IOACCEL_ENABLED & h->TMFSupportFlags)
+			return hpsa_send_abort_ioaccel2(h, abort,
+						reply_queue);
+		else
+			return hpsa_send_reset_as_abort_ioaccel2(h, scsi3addr,
+						abort, reply_queue);
+	}
+	return hpsa_send_abort(h, scsi3addr, abort, reply_queue);
+}
+
+/* Find out which reply queue a command was meant to return on */
+static int hpsa_extract_reply_queue(struct ctlr_info *h,
+					struct CommandList *c)
+{
+	if (c->cmd_type == CMD_IOACCEL2)
+		return h->ioaccel2_cmd_pool[c->cmdindex].reply_queue;
+	return c->Header.ReplyQueue;
+}
 
-	return hpsa_send_abort(h, scsi3addr, abort, 0) &&
-			hpsa_send_abort(h, scsi3addr, abort, 1);
+/*
+ * Limit concurrency of abort commands to prevent
+ * over-subscription of commands
+ */
+static inline int wait_for_available_abort_cmd(struct ctlr_info *h)
+{
+#define ABORT_CMD_WAIT_MSECS 5000
+	return !wait_event_timeout(h->abort_cmd_wait_queue,
+			atomic_dec_if_positive(&h->abort_cmds_available) >= 0,
+			msecs_to_jiffies(ABORT_CMD_WAIT_MSECS));
+}
+
+/* Handle aborting a command which hasn't left the driver yet;  returns true if
+ * the command was in the driver and should be considered successfully
+ * aborted; returns false otherwise. */
+static int hpsa_do_local_abort(struct CommandList *c, struct ctlr_info *h)
+{
+	c->abort_pending = true;
+	mb();	/* Write the flag, then check if it's on the queue. */
+	if (work_pending(&c->work)) {
+		/* We set the abort-pending flag and then saw that the command
+		 * was on the retry queue, so the command should see the flag
+		 * when it is removed from the work queue.  Wait for that to
+		 * happen. */
+		flush_workqueue(h->resubmit_wq);
+
+		/* By now, the work item has been removed from the work queue
+		 * and is winding its way back to the SCSI midlayer.  We don't
+		 * know if it has made it there, yet, but adding additional
+		 * synchronization probably isn't worthwhile -- just assume
+		 * that it has made it and tell the caller to return success.
+		 */
+		return true;
+	} else {
+		/* The command was not on the work queue when we checked:  if it
+		 * is in the driver on its way to the controller, it will likely
+		 * see the flag and abort itself -- in any case, because of the
+		 * flag, it won't be retried; however, without implementing more
+		 * logic (which will be unhelpful in the normal case), we can't
+		 * be sure that it wasn't already sent to the controller, so
+		 * tell the caller to proceed with sending an abort. */
+		return false;
+	}
 }
 
 /* Send an abort for the specified command.
@@ -4561,24 +5999,47 @@ static int hpsa_send_abort_both_ways(struct ctlr_info *h,
 static int hpsa_eh_abort_handler(struct scsi_cmnd *sc)
 {
 
-	int i, rc;
+	int rc;
 	struct ctlr_info *h;
 	struct hpsa_scsi_dev_t *dev;
 	struct CommandList *abort; /* pointer to command to be aborted */
 	struct scsi_cmnd *as;	/* ptr to scsi cmd inside aborted command. */
 	char msg[256];		/* For debug messaging. */
 	int ml = 0;
-	__le32 tagupper, taglower;
-	int refcount;
+	u32 tagupper, taglower;
+	int refcount, reply_queue;
 
 	/* Find the controller of the command to be aborted */
+	if (sc->device == NULL)
+		return FAILED;
+
 	h = sdev_to_hba(sc->device);
-	if (WARN(h == NULL,
-			"ABORT REQUEST FAILED, Controller lookup failed.\n"))
+	if (h == NULL)
 		return FAILED;
 
-	if (lockup_detected(h))
+	/* if controller locked up, we can guarantee command won't complete */
+	if (lockup_detected(h)) {
+		dev_warn(&h->pdev->dev,
+			"scsi %d:%d:%d:%llu scmd %p ABORT FAILED, lockup detected\n",
+			h->scsi_host->host_no, sc->device->channel,
+			sc->device->id, (u64) sc->device->lun, sc);
 		return FAILED;
+	} else {
+		/* good time to check if controller lockup has occurred */
+		/* FIXME eh_timeout_handler would be even better.
+		 * for testing, abort is just being used for timeouts,
+		 * so is equivalent */
+		detect_controller_lockup(h);
+
+		/* check again in case one just occurred */
+		if (lockup_detected(h)) {
+			dev_warn(&h->pdev->dev,
+				"scsi %d:%d:%d:%llu scmd %p ABORT FAILED, lockup detected\n",
+				h->scsi_host->host_no, sc->device->channel,
+				sc->device->id, (u64) sc->device->lun, sc);
+			return FAILED;
+		}
+	}
 
 	/* Check that controller supports some kind of task abort */
 	if (!(HPSATMF_PHYS_TASK_ABORT & h->TMFSupportFlags) &&
@@ -4586,15 +6047,15 @@ static int hpsa_eh_abort_handler(struct scsi_cmnd *sc)
 		return FAILED;
 
 	memset(msg, 0, sizeof(msg));
-	ml += sprintf(msg+ml, "ABORT REQUEST on C%d:B%d:T%d:L%llu ",
+	ml += sprintf(msg+ml, "scsi %d:%d:%d:%llu scmd %p ABORT ",
 		h->scsi_host->host_no, sc->device->channel,
-		sc->device->id, sc->device->lun);
+		sc->device->id, (u64) sc->device->lun, sc);
 
 	/* Find the device of the command to be aborted */
 	dev = sc->device->hostdata;
 	if (!dev) {
-		dev_err(&h->pdev->dev, "%s FAILED, Device lookup failed.\n",
-				msg);
+		dev_err(&h->pdev->dev, "%s FAILED, "
+			"Device lookup failed.\n", msg);
 		return FAILED;
 	}
 
@@ -4609,50 +6070,63 @@ static int hpsa_eh_abort_handler(struct scsi_cmnd *sc)
 		cmd_free(h, abort);
 		return SUCCESS;
 	}
+
+	/* Don't bother trying the abort if we know it won't work. */
+	if (abort->cmd_type != CMD_IOACCEL2 &&
+		abort->cmd_type != CMD_IOACCEL1 && !dev->supports_aborts) {
+		cmd_free(h, abort);
+		return FAILED;
+	}
+
+	/* Check that we're aborting the right command.
+	 * It's possible the CommandList already completed and got re-used.
+	 */
+	if (abort->scsi_cmd != sc) {
+		cmd_free(h, abort);
+		return SUCCESS;
+	}
+
+	/*
+	 * If the command is still being handled by the driver, try to abort it
+	 * before it can be sent to the controller.
+	 */
+	if (hpsa_do_local_abort(abort, h))
+		return SUCCESS;
+
 	hpsa_get_tag(h, abort, &taglower, &tagupper);
+	reply_queue = hpsa_extract_reply_queue(h, abort);
 	ml += sprintf(msg+ml, "Tag:0x%08x:%08x ", tagupper, taglower);
 	as  = abort->scsi_cmd;
 	if (as != NULL)
-		ml += sprintf(msg+ml, "Command:0x%x SN:0x%lx ",
-			as->cmnd[0], as->serial_number);
-	dev_dbg(&h->pdev->dev, "%s\n", msg);
-	dev_warn(&h->pdev->dev, "Abort request on C%d:B%d:T%d:L%d\n",
-		h->scsi_host->host_no, dev->bus, dev->target, dev->lun);
+		ml += sprintf(msg+ml,
+			"CDBLen: %d CDB: 0x%02x%02x... SN: 0x%lx ",
+			as->cmd_len, as->cmnd[0], as->cmnd[1],
+			as->serial_number);
+	dev_warn(&h->pdev->dev, "%s BEING SENT\n", msg);
 	/*
 	 * Command is in flight, or possibly already completed
 	 * by the firmware (but not to the scsi mid layer) but we can't
 	 * distinguish which.  Send the abort down.
 	 */
-	rc = hpsa_send_abort_both_ways(h, dev->scsi3addr, abort);
-	if (rc != 0) {
-		dev_dbg(&h->pdev->dev, "%s Request FAILED.\n", msg);
-		dev_warn(&h->pdev->dev, "FAILED abort on device C%d:B%d:T%d:L%d\n",
-			h->scsi_host->host_no,
-			dev->bus, dev->target, dev->lun);
+	if (wait_for_available_abort_cmd(h)) {
+		dev_warn(&h->pdev->dev,
+			"%s FAILED, timeout waiting for an abort command to become available.\n",
+			msg);
 		cmd_free(h, abort);
 		return FAILED;
 	}
-	dev_info(&h->pdev->dev, "%s REQUEST SUCCEEDED.\n", msg);
-
-	/* If the abort(s) above completed and actually aborted the
-	 * command, then the command to be aborted should already be
-	 * completed.  If not, wait around a bit more to see if they
-	 * manage to complete normally.
-	 */
-#define ABORT_COMPLETE_WAIT_SECS 30
-	for (i = 0; i < ABORT_COMPLETE_WAIT_SECS * 10; i++) {
-		refcount = atomic_read(&abort->refcount);
-		if (refcount < 2) {
-			cmd_free(h, abort);
-			return SUCCESS;
-		} else {
-			msleep(100);
-		}
+	rc = hpsa_send_abort_both_ways(h, dev->scsi3addr, abort, reply_queue);
+	atomic_inc(&h->abort_cmds_available);
+	wake_up_all(&h->abort_cmd_wait_queue);
+	if (rc != 0) {
+		dev_warn(&h->pdev->dev, "%s SENT, FAILED\n", msg);
+		cmd_free(h, abort);
+		return FAILED;
 	}
-	dev_warn(&h->pdev->dev, "%s FAILED. Aborted command has not completed after %d seconds.\n",
-		msg, ABORT_COMPLETE_WAIT_SECS);
+	dev_info(&h->pdev->dev, "%s SENT, SUCCESS\n", msg);
+	wait_event(h->event_sync_wait_queue, abort->scsi_cmd != sc);
 	cmd_free(h, abort);
-	return FAILED;
+	return SUCCESS;
 }
 
 /*
@@ -4660,19 +6134,16 @@ static int hpsa_eh_abort_handler(struct scsi_cmnd *sc)
  * and managed by cmd_alloc() and cmd_free() using a simple bitmap to track
  * which ones are free or in use.  Lock must be held when calling this.
  * cmd_free() is the complement.
+ * This function never gives up and returns NULL.  If it hangs,
+ * another thread must call cmd_free() to free some tags.
  */
-
 static struct CommandList *cmd_alloc(struct ctlr_info *h)
 {
-	struct CommandList *c;
-	int i;
-	union u64bit temp64;
-	dma_addr_t cmd_dma_handle, err_dma_handle;
-	int refcount;
-	unsigned long offset;
+	struct CommandList *c;
+	int refcount, i;
+	unsigned long offset = 0;
 
-	/*
-	 * There is some *extremely* small but non-zero chance that that
+	/* There is some *extremely* small but non-zero chance that that
 	 * multiple threads could get in here, and one thread could
 	 * be scanning through the list of bits looking for a free
 	 * one, but the free ones are always behind him, and other
@@ -4683,7 +6154,7 @@ static struct CommandList *cmd_alloc(struct ctlr_info *h)
 	 * infrequently as to be indistinguishable from never.
 	 */
 
-	offset = h->last_allocation; /* benignly racy */
+	offset = h->last_allocation; /* benighly racy */
 	for (;;) {
 		i = find_next_zero_bit(h->cmd_pool_bits, h->nr_cmds, offset);
 		if (unlikely(i == h->nr_cmds)) {
@@ -4702,24 +6173,7 @@ static struct CommandList *cmd_alloc(struct ctlr_info *h)
 		break; /* it's ours now. */
 	}
 	h->last_allocation = i; /* benignly racy */
-
-	/* Zero out all of commandlist except the last field, refcount */
-	memset(c, 0, offsetof(struct CommandList, refcount));
-	c->Header.tag = cpu_to_le64((u64) (i << DIRECT_LOOKUP_SHIFT));
-	cmd_dma_handle = h->cmd_pool_dhandle + i * sizeof(*c);
-	c->err_info = h->errinfo_pool + i;
-	memset(c->err_info, 0, sizeof(*c->err_info));
-	err_dma_handle = h->errinfo_pool_dhandle
-	    + i * sizeof(*c->err_info);
-
-	c->cmdindex = i;
-
-	c->busaddr = (u32) cmd_dma_handle;
-	temp64.val = (u64) err_dma_handle;
-	c->ErrDesc.Addr = cpu_to_le64((u64) err_dma_handle);
-	c->ErrDesc.Len = cpu_to_le32((u32) sizeof(*c->err_info));
-
-	c->h = h;
+	hpsa_cmd_partial_init(h, i, c);
 	return c;
 }
 
@@ -4736,8 +6190,7 @@ static void cmd_free(struct ctlr_info *h, struct CommandList *c)
 
 #ifdef CONFIG_COMPAT
 
-static int hpsa_ioctl32_passthru(struct scsi_device *dev, int cmd,
-	void __user *arg)
+static int hpsa_ioctl32_passthru(struct scsi_device *dev, int cmd, void *arg)
 {
 	IOCTL32_Command_struct __user *arg32 =
 	    (IOCTL32_Command_struct __user *) arg;
@@ -4762,7 +6215,7 @@ static int hpsa_ioctl32_passthru(struct scsi_device *dev, int cmd,
 	if (err)
 		return -EFAULT;
 
-	err = hpsa_ioctl(dev, CCISS_PASSTHRU, p);
+	err = hpsa_ioctl(dev, CCISS_PASSTHRU, (void *)p);
 	if (err)
 		return err;
 	err |= copy_in_user(&arg32->error_info, &p->error_info,
@@ -4773,7 +6226,7 @@ static int hpsa_ioctl32_passthru(struct scsi_device *dev, int cmd,
 }
 
 static int hpsa_ioctl32_big_passthru(struct scsi_device *dev,
-	int cmd, void __user *arg)
+	int cmd, void *arg)
 {
 	BIG_IOCTL32_Command_struct __user *arg32 =
 	    (BIG_IOCTL32_Command_struct __user *) arg;
@@ -4800,7 +6253,7 @@ static int hpsa_ioctl32_big_passthru(struct scsi_device *dev,
 	if (err)
 		return -EFAULT;
 
-	err = hpsa_ioctl(dev, CCISS_BIG_PASSTHRU, p);
+	err = hpsa_ioctl(dev, CCISS_BIG_PASSTHRU, (void *)p);
 	if (err)
 		return err;
 	err |= copy_in_user(&arg32->error_info, &p->error_info,
@@ -4810,7 +6263,7 @@ static int hpsa_ioctl32_big_passthru(struct scsi_device *dev,
 	return err;
 }
 
-static int hpsa_compat_ioctl(struct scsi_device *dev, int cmd, void __user *arg)
+static int hpsa_compat_ioctl(struct scsi_device *dev, int cmd, void *arg)
 {
 	switch (cmd) {
 	case CCISS_GETPCIINFO:
@@ -4818,7 +6271,6 @@ static int hpsa_compat_ioctl(struct scsi_device *dev, int cmd, void __user *arg)
 	case CCISS_SETINTINFO:
 	case CCISS_GETNODENAME:
 	case CCISS_SETNODENAME:
-	case CCISS_GETHEARTBEAT:
 	case CCISS_GETBUSTYPES:
 	case CCISS_GETFIRMVER:
 	case CCISS_GETDRIVVER:
@@ -4900,7 +6352,7 @@ static int hpsa_passthru_ioctl(struct ctlr_info *h, void __user *argp)
 	if (iocommand.buf_size > 0) {
 		buff = kmalloc(iocommand.buf_size, GFP_KERNEL);
 		if (buff == NULL)
-			return -EFAULT;
+			return -ENOMEM;
 		if (iocommand.Request.Type.Direction & XFER_WRITE) {
 			/* Copy the data into the buffer we created */
 			if (copy_from_user(buff, iocommand.buf,
@@ -4913,16 +6365,14 @@ static int hpsa_passthru_ioctl(struct ctlr_info *h, void __user *argp)
 		}
 	}
 	c = cmd_alloc(h);
-	if (c == NULL) {
-		rc = -ENOMEM;
-		goto out_kfree;
-	}
+
 	/* Fill in the command type */
 	c->cmd_type = CMD_IOCTL_PEND;
+	c->scsi_cmd = SCSI_CMD_BUSY;
 	/* Fill in Command Header */
 	c->Header.ReplyQueue = 0; /* unused in simple mode */
 	if (iocommand.buf_size > 0) {	/* buffer to fill */
-		c->Header.SGList = 1;
+		c->Header.SGList = (u8) 1;
 		c->Header.SGTotal = cpu_to_le16(1);
 	} else	{ /* no buffers to fill */
 		c->Header.SGList = 0;
@@ -4936,9 +6386,10 @@ static int hpsa_passthru_ioctl(struct ctlr_info *h, void __user *argp)
 
 	/* Fill in the scatter gather information */
 	if (iocommand.buf_size > 0) {
-		temp64 = pci_map_single(h->pdev, buff,
+		temp64 = (u64) pci_map_single(h->pdev, buff,
 			iocommand.buf_size, PCI_DMA_BIDIRECTIONAL);
-		if (dma_mapping_error(&h->pdev->dev, (dma_addr_t) temp64)) {
+		if (hpsa_dma_mapping_error(&h->pdev->dev,
+						(dma_addr_t) temp64)) {
 			c->SG[0].Addr = cpu_to_le64(0);
 			c->SG[0].Len = cpu_to_le32(0);
 			rc = -ENOMEM;
@@ -4948,7 +6399,9 @@ static int hpsa_passthru_ioctl(struct ctlr_info *h, void __user *argp)
 		c->SG[0].Len = cpu_to_le32(iocommand.buf_size);
 		c->SG[0].Ext = cpu_to_le32(HPSA_SG_LAST); /* not chaining */
 	}
-	hpsa_scsi_do_simple_cmd_core_if_no_lockup(h, c);
+	rc = hpsa_scsi_do_simple_cmd(h, c, DEFAULT_REPLY_QUEUE, NO_TIMEOUT);
+	if (rc)
+		rc = -EIO;
 	if (iocommand.buf_size > 0)
 		hpsa_pci_unmap(h->pdev, c, 1, PCI_DMA_BIDIRECTIONAL);
 	check_ioctl_unit_attention(h, c);
@@ -4984,6 +6437,7 @@ static int hpsa_big_passthru_ioctl(struct ctlr_info *h, void __user *argp)
 	u64 temp64;
 	BYTE sg_used = 0;
 	int status = 0;
+	int i;
 	u32 left;
 	u32 sz;
 	BYTE __user *data_ptr;
@@ -5048,11 +6502,9 @@ static int hpsa_big_passthru_ioctl(struct ctlr_info *h, void __user *argp)
 		sg_used++;
 	}
 	c = cmd_alloc(h);
-	if (c == NULL) {
-		status = -ENOMEM;
-		goto cleanup1;
-	}
+
 	c->cmd_type = CMD_IOCTL_PEND;
+	c->scsi_cmd = SCSI_CMD_BUSY;
 	c->Header.ReplyQueue = 0;
 	c->Header.SGList = (u8) sg_used;
 	c->Header.SGTotal = cpu_to_le16(sg_used);
@@ -5061,12 +6513,12 @@ static int hpsa_big_passthru_ioctl(struct ctlr_info *h, void __user *argp)
 	if (ioc->buf_size > 0) {
 		int i;
 		for (i = 0; i < sg_used; i++) {
-			temp64 = pci_map_single(h->pdev, buff[i],
+			temp64 = (u64) pci_map_single(h->pdev, buff[i],
 				    buff_size[i], PCI_DMA_BIDIRECTIONAL);
-			if (dma_mapping_error(&h->pdev->dev,
+			if (hpsa_dma_mapping_error(&h->pdev->dev,
 							(dma_addr_t) temp64)) {
-				c->SG[i].Addr = cpu_to_le64(0);
-				c->SG[i].Len = cpu_to_le32(0);
+				c->SG[i].Addr = 0;
+				c->SG[i].Len = 0;
 				hpsa_pci_unmap(h->pdev, c, i,
 					PCI_DMA_BIDIRECTIONAL);
 				status = -ENOMEM;
@@ -5074,11 +6526,16 @@ static int hpsa_big_passthru_ioctl(struct ctlr_info *h, void __user *argp)
 			}
 			c->SG[i].Addr = cpu_to_le64(temp64);
 			c->SG[i].Len = cpu_to_le32(buff_size[i]);
-			c->SG[i].Ext = cpu_to_le32(0);
+			/* we are not chaining */
+			c->SG[i].Ext =
+				cpu_to_le32((i == sg_used) * HPSA_SG_LAST);
 		}
-		c->SG[--i].Ext = cpu_to_le32(HPSA_SG_LAST);
 	}
-	hpsa_scsi_do_simple_cmd_core_if_no_lockup(h, c);
+	status = hpsa_scsi_do_simple_cmd(h, c, DEFAULT_REPLY_QUEUE, NO_TIMEOUT);
+	if (status) {
+		status = -EIO;
+		goto cleanup0;
+	}
 	if (sg_used)
 		hpsa_pci_unmap(h->pdev, c, sg_used, PCI_DMA_BIDIRECTIONAL);
 	check_ioctl_unit_attention(h, c);
@@ -5089,8 +6546,6 @@ static int hpsa_big_passthru_ioctl(struct ctlr_info *h, void __user *argp)
 		goto cleanup0;
 	}
 	if ((ioc->Request.Type.Direction & XFER_READ) && ioc->buf_size > 0) {
-		int i;
-
 		/* Copy the data out of the buffer we created */
 		BYTE __user *ptr = ioc->buf;
 		for (i = 0; i < sg_used; i++) {
@@ -5106,8 +6561,6 @@ cleanup0:
 	cmd_free(h, c);
 cleanup1:
 	if (buff) {
-		int i;
-
 		for (i = 0; i < sg_used; i++)
 			kfree(buff[i]);
 		kfree(buff);
@@ -5128,7 +6581,7 @@ static void check_ioctl_unit_attention(struct ctlr_info *h,
 /*
  * ioctl
  */
-static int hpsa_ioctl(struct scsi_device *dev, int cmd, void __user *arg)
+static int hpsa_ioctl(struct scsi_device *dev, int cmd, void *arg)
 {
 	struct ctlr_info *h;
 	void __user *argp = (void __user *)arg;
@@ -5163,14 +6616,13 @@ static int hpsa_ioctl(struct scsi_device *dev, int cmd, void __user *arg)
 	}
 }
 
-static int hpsa_send_host_reset(struct ctlr_info *h, unsigned char *scsi3addr,
-				u8 reset_type)
+static void __devinit hpsa_send_host_reset(struct ctlr_info *h, unsigned char *scsi3addr,
+	u8 reset_type)
 {
 	struct CommandList *c;
 
 	c = cmd_alloc(h);
-	if (!c)
-		return -ENOMEM;
+
 	/* fill_cmd can't fail here, no data buffer to map */
 	(void) fill_cmd(c, HPSA_DEVICE_RESET_MSG, h, NULL, 0, 0,
 		RAID_CTLR_LUNID, TYPE_MSG);
@@ -5181,7 +6633,7 @@ static int hpsa_send_host_reset(struct ctlr_info *h, unsigned char *scsi3addr,
 	 * the command either.  This is the last command we will send before
 	 * re-initializing everything, so it doesn't matter and won't leak.
 	 */
-	return 0;
+	return;
 }
 
 static int fill_cmd(struct CommandList *c, u8 cmd, struct ctlr_info *h,
@@ -5189,16 +6641,18 @@ static int fill_cmd(struct CommandList *c, u8 cmd, struct ctlr_info *h,
 	int cmd_type)
 {
 	int pci_dir = XFER_NONE;
-	struct CommandList *a; /* for commands to be aborted */
+	u64 tag; /* for commands to be aborted */
+	u32 tupper, tlower;
 
 	c->cmd_type = CMD_IOCTL_PEND;
+	c->scsi_cmd = SCSI_CMD_BUSY;
 	c->Header.ReplyQueue = 0;
 	if (buff != NULL && size > 0) {
-		c->Header.SGList = 1;
-		c->Header.SGTotal = cpu_to_le16(1);
+		c->Header.SGList = (u8) 1;
+		c->Header.SGTotal = cpu_to_le32(1);
 	} else {
 		c->Header.SGList = 0;
-		c->Header.SGTotal = cpu_to_le16(0);
+		c->Header.SGTotal = 0;
 	}
 	memcpy(c->Header.LUN.LunAddrBytes, scsi3addr, 8);
 
@@ -5289,6 +6743,20 @@ static int fill_cmd(struct CommandList *c, u8 cmd, struct ctlr_info *h,
 	} else if (cmd_type == TYPE_MSG) {
 		switch (cmd) {
 
+		case  HPSA_PHYS_TARGET_RESET:
+			c->Request.CDBLen = 16;
+			c->Request.type_attr_dir =
+				TYPE_ATTR_DIR(cmd_type, ATTR_SIMPLE, XFER_NONE);
+			c->Request.Timeout = 0; /* Don't time out */
+			memset(&c->Request.CDB[0], 0, sizeof(c->Request.CDB));
+			c->Request.CDB[0] = HPSA_RESET;
+			c->Request.CDB[1] = HPSA_TARGET_RESET_TYPE;
+			/* Physical target reset needs no control bytes 4-7*/
+			c->Request.CDB[4] = 0x00;
+			c->Request.CDB[5] = 0x00;
+			c->Request.CDB[6] = 0x00;
+			c->Request.CDB[7] = 0x00;
+			break;
 		case  HPSA_DEVICE_RESET_MSG:
 			c->Request.CDBLen = 16;
 			c->Request.type_attr_dir =
@@ -5296,7 +6764,7 @@ static int fill_cmd(struct CommandList *c, u8 cmd, struct ctlr_info *h,
 			c->Request.Timeout = 0; /* Don't time out */
 			memset(&c->Request.CDB[0], 0, sizeof(c->Request.CDB));
 			c->Request.CDB[0] =  cmd;
-			c->Request.CDB[1] = HPSA_RESET_TYPE_LUN;
+			c->Request.CDB[1] = HPSA_LUN_RESET_TYPE;
 			/* If bytes 4-7 are zero, it means reset the */
 			/* LunID device */
 			c->Request.CDB[4] = 0x00;
@@ -5305,10 +6773,16 @@ static int fill_cmd(struct CommandList *c, u8 cmd, struct ctlr_info *h,
 			c->Request.CDB[7] = 0x00;
 			break;
 		case  HPSA_ABORT_MSG:
-			a = buff;       /* point to command to be aborted */
+			memcpy(&tag, buff, sizeof(tag));
+			if (buff == NULL) {
+				dev_warn(&h->pdev->dev, "Null Abort request.\n");
+				BUG();
+			}
 			dev_dbg(&h->pdev->dev,
-				"Abort Tag:0x%016llx request Tag:0x%016llx",
-				a->Header.tag, c->Header.tag);
+				"Abort Tag:0x%016llx using request Tag:0x%016llx",
+				tag, c->Header.tag);
+			tupper = (u32) (tag >> 32);
+			tlower = (u32) (tag & 0x0ffffffffULL);
 			c->Request.CDBLen = 16;
 			c->Request.type_attr_dir =
 					TYPE_ATTR_DIR(cmd_type,
@@ -5319,8 +6793,17 @@ static int fill_cmd(struct CommandList *c, u8 cmd, struct ctlr_info *h,
 			c->Request.CDB[2] = 0x00; /* reserved */
 			c->Request.CDB[3] = 0x00; /* reserved */
 			/* Tag to abort goes in CDB[4]-CDB[11] */
-			memcpy(&c->Request.CDB[4], &a->Header.tag,
-				sizeof(a->Header.tag));
+
+			/* FIXME: Double check the byte swapping here. */
+			c->Request.CDB[4] = tlower & 0xFF;
+			c->Request.CDB[5] = (tlower >> 8) & 0xFF;
+			c->Request.CDB[6] = (tlower >> 16) & 0xFF;
+			c->Request.CDB[7] = (tlower >> 24) & 0xFF;
+			c->Request.CDB[8] = tupper & 0xFF;
+			c->Request.CDB[9] = (tupper >> 8) & 0xFF;
+			c->Request.CDB[10] = (tupper >> 16) & 0xFF;
+			c->Request.CDB[11] = (tupper >> 24) & 0xFF;
+
 			c->Request.CDB[12] = 0x00; /* reserved */
 			c->Request.CDB[13] = 0x00; /* reserved */
 			c->Request.CDB[14] = 0x00; /* reserved */
@@ -5361,8 +6844,7 @@ static void __iomem *remap_pci_mem(ulong base, ulong size)
 {
 	ulong page_base = ((ulong) base) & PAGE_MASK;
 	ulong page_offs = ((ulong) base) - page_base;
-	void __iomem *page_remapped = ioremap_nocache(page_base,
-		page_offs + size);
+	void __iomem *page_remapped = ioremap_nocache(page_base, page_offs + size);
 
 	return page_remapped ? (page_remapped + page_offs) : NULL;
 }
@@ -5372,7 +6854,7 @@ static inline unsigned long get_next_completion(struct ctlr_info *h, u8 q)
 	return h->access.command_completed(h, q);
 }
 
-static inline bool interrupt_pending(struct ctlr_info *h)
+static inline int interrupt_pending(struct ctlr_info *h)
 {
 	return h->access.intr_pending(h);
 }
@@ -5399,11 +6881,10 @@ static inline void finish_cmd(struct CommandList *c)
 	if (likely(c->cmd_type == CMD_IOACCEL1 || c->cmd_type == CMD_SCSI
 			|| c->cmd_type == CMD_IOACCEL2))
 		complete_scsi_command(c);
-	else if (c->cmd_type == CMD_IOCTL_PEND)
+	else if (c->cmd_type == CMD_IOCTL_PEND || c->cmd_type == IOACCEL2_TMF)
 		complete(c->waiting);
 }
 
-
 static inline u32 hpsa_tag_discard_error_bits(struct ctlr_info *h, u32 tag)
 {
 #define HPSA_PERF_ERROR_BITS ((1 << DIRECT_LOOKUP_SHIFT) - 1)
@@ -5449,14 +6930,14 @@ static int ignore_bogus_interrupt(struct ctlr_info *h)
 /*
  * Convert &h->q[x] (passed to interrupt handlers) back to h.
  * Relies on (h-q[x] == x) being true for x such that
- * 0 <= x < MAX_REPLY_QUEUES.
+ * 0 <= x < h->nreply_queues.
  */
 static struct ctlr_info *queue_to_hba(u8 *queue)
 {
 	return container_of((queue - *queue), struct ctlr_info, q[0]);
 }
 
-static irqreturn_t hpsa_intx_discard_completions(int irq, void *queue)
+DECLARE_INTERRUPT_HANDLER(hpsa_intx_discard_completions)
 {
 	struct ctlr_info *h = queue_to_hba(queue);
 	u8 q = *(u8 *) queue;
@@ -5476,7 +6957,7 @@ static irqreturn_t hpsa_intx_discard_completions(int irq, void *queue)
 	return IRQ_HANDLED;
 }
 
-static irqreturn_t hpsa_msix_discard_completions(int irq, void *queue)
+DECLARE_INTERRUPT_HANDLER(hpsa_msix_discard_completions)
 {
 	struct ctlr_info *h = queue_to_hba(queue);
 	u32 raw_tag;
@@ -5492,9 +6973,9 @@ static irqreturn_t hpsa_msix_discard_completions(int irq, void *queue)
 	return IRQ_HANDLED;
 }
 
-static irqreturn_t do_hpsa_intr_intx(int irq, void *queue)
+DECLARE_INTERRUPT_HANDLER(do_hpsa_intr_intx)
 {
-	struct ctlr_info *h = queue_to_hba((u8 *) queue);
+	struct ctlr_info *h = queue_to_hba(queue);
 	u32 raw_tag;
 	u8 q = *(u8 *) queue;
 
@@ -5511,7 +6992,7 @@ static irqreturn_t do_hpsa_intr_intx(int irq, void *queue)
 	return IRQ_HANDLED;
 }
 
-static irqreturn_t do_hpsa_intr_msi(int irq, void *queue)
+DECLARE_INTERRUPT_HANDLER(do_hpsa_intr_msi)
 {
 	struct ctlr_info *h = queue_to_hba(queue);
 	u32 raw_tag;
@@ -5526,12 +7007,12 @@ static irqreturn_t do_hpsa_intr_msi(int irq, void *queue)
 	return IRQ_HANDLED;
 }
 
-/* Send a message CDB to the firmware. Careful, this only works
+/* Send a message CDB to the firmware.  Careful, this only works
  * in simple mode, not performant mode due to the tag lookup.
  * We only ever use this immediately after a controller reset.
  */
-static int hpsa_message(struct pci_dev *pdev, unsigned char opcode,
-			unsigned char type)
+static __devinit int hpsa_message(struct pci_dev *pdev, unsigned char opcode,
+						unsigned char type)
 {
 	struct Command {
 		struct CommandListHeader CommandHeader;
@@ -5542,12 +7023,18 @@ static int hpsa_message(struct pci_dev *pdev, unsigned char opcode,
 	static const size_t cmd_sz = sizeof(*cmd) +
 					sizeof(cmd->ErrorDescriptor);
 	dma_addr_t paddr64;
-	__le32 paddr32;
-	u32 tag;
+	uint32_t paddr32, tag;
 	void __iomem *vaddr;
 	int i, err;
+	unsigned long paddr;
+
+	/* kernel.org uses pci_remap_bar() here, but 2.6.27 doesn't have it.*/
+	err = hpsa_pci_find_memory_BAR(pdev, &paddr);
+	if (err)
+		return err;
+	vaddr = ioremap_nocache(paddr, 0x250);
+
 
-	vaddr = pci_ioremap_bar(pdev, 0);
 	if (vaddr == NULL)
 		return -ENOMEM;
 
@@ -5571,12 +7058,12 @@ static int hpsa_message(struct pci_dev *pdev, unsigned char opcode,
 	 * although there's no guarantee, we assume that the address is at
 	 * least 4-byte aligned (most likely, it's page-aligned).
 	 */
-	paddr32 = cpu_to_le32(paddr64);
+	paddr32 = paddr64;
 
 	cmd->CommandHeader.ReplyQueue = 0;
 	cmd->CommandHeader.SGList = 0;
-	cmd->CommandHeader.SGTotal = cpu_to_le16(0);
-	cmd->CommandHeader.tag = cpu_to_le64(paddr64);
+	cmd->CommandHeader.SGTotal = 0;
+	cmd->CommandHeader.tag = (u64) paddr32;
 	memset(&cmd->CommandHeader.LUN.LunAddrBytes, 0, 8);
 
 	cmd->Request.CDBLen = 16;
@@ -5587,14 +7074,14 @@ static int hpsa_message(struct pci_dev *pdev, unsigned char opcode,
 	cmd->Request.CDB[1] = type;
 	memset(&cmd->Request.CDB[2], 0, 14); /* rest of the CDB is reserved */
 	cmd->ErrorDescriptor.Addr =
-			cpu_to_le64((le32_to_cpu(paddr32) + sizeof(*cmd)));
+			cpu_to_le64((u64) (paddr32 + sizeof(*cmd)));
 	cmd->ErrorDescriptor.Len = cpu_to_le32(sizeof(struct ErrorInfo));
 
-	writel(le32_to_cpu(paddr32), vaddr + SA5_REQUEST_PORT_OFFSET);
+	writel(paddr32, vaddr + SA5_REQUEST_PORT_OFFSET);
 
 	for (i = 0; i < HPSA_MSG_SEND_RETRY_LIMIT; i++) {
 		tag = readl(vaddr + SA5_REPLY_PORT_OFFSET);
-		if ((tag & ~HPSA_SIMPLE_ERROR_BITS) == paddr64)
+		if ((tag & ~HPSA_SIMPLE_ERROR_BITS) == paddr32)
 			break;
 		msleep(HPSA_MSG_SEND_RETRY_INTERVAL_MSECS);
 	}
@@ -5626,8 +7113,10 @@ static int hpsa_message(struct pci_dev *pdev, unsigned char opcode,
 #define hpsa_noop(p) hpsa_message(p, 3, 0)
 
 static int hpsa_controller_hard_reset(struct pci_dev *pdev,
-	void __iomem *vaddr, u32 use_doorbell)
+	void * __iomem vaddr, u32 use_doorbell)
 {
+	u16 pmcsr;
+	int pos;
 
 	if (use_doorbell) {
 		/* For everything after the P600, the PCI power state method
@@ -5653,39 +7142,63 @@ static int hpsa_controller_hard_reset(struct pci_dev *pdev,
 		 * this causes a secondary PCI reset which will reset the
 		 * controller." */
 
-		int rc = 0;
-
+		pos = pci_find_capability(pdev, PCI_CAP_ID_PM);
+		if (pos == 0) {
+			dev_err(&pdev->dev,
+				"hpsa_reset_controller: "
+				"PCI PM not supported\n");
+			return -ENODEV;
+		}
 		dev_info(&pdev->dev, "using PCI PM to reset controller\n");
-
 		/* enter the D3hot power management state */
-		rc = pci_set_power_state(pdev, PCI_D3hot);
-		if (rc)
-			return rc;
+		pci_read_config_word(pdev, pos + PCI_PM_CTRL, &pmcsr);
+		pmcsr &= ~PCI_PM_CTRL_STATE_MASK;
+		pmcsr |= PCI_D3hot;
+		pci_write_config_word(pdev, pos + PCI_PM_CTRL, pmcsr);
 
 		msleep(500);
 
 		/* enter the D0 power management state */
-		rc = pci_set_power_state(pdev, PCI_D0);
-		if (rc)
-			return rc;
-
-		/*
-		 * The P600 requires a small delay when changing states.
-		 * Otherwise we may think the board did not reset and we bail.
-		 * This for kdump only and is particular to the P600.
-		 */
-		msleep(500);
+		pmcsr &= ~PCI_PM_CTRL_STATE_MASK;
+		pmcsr |= PCI_D0;
+		pci_write_config_word(pdev, pos + PCI_PM_CTRL, pmcsr);
 	}
 	return 0;
 }
 
-static void init_driver_version(char *driver_version, int len)
+static int hpsa_wait_for_board_state(struct pci_dev *pdev,
+	void __iomem *vaddr, int wait_for_ready)
+{
+	int i, iterations;
+	u32 scratchpad;
+	if (wait_for_ready)
+		iterations = HPSA_BOARD_READY_ITERATIONS;
+	else
+		iterations = HPSA_BOARD_NOT_READY_ITERATIONS;
+
+	for (i = 0; i < iterations; i++) {
+		scratchpad = readl(vaddr + SA5_SCRATCHPAD_OFFSET);
+		if (wait_for_ready) {
+			if (scratchpad == HPSA_FIRMWARE_READY)
+				return 0;
+		} else {
+			if (scratchpad != HPSA_FIRMWARE_READY)
+				return 0;
+		}
+		msleep(HPSA_BOARD_READY_POLL_INTERVAL_MSECS);
+	}
+	dev_warn(&pdev->dev, "board not ready, timed out.\n");
+	return -ENODEV;
+}
+
+static __devinit void init_driver_version(char *driver_version, int len)
 {
 	memset(driver_version, 0, len);
 	strncpy(driver_version, HPSA " " HPSA_DRIVER_VERSION, len - 1);
 }
 
-static int write_driver_ver_to_cfgtable(struct CfgTable __iomem *cfgtable)
+static __devinit int write_driver_ver_to_cfgtable(
+	struct CfgTable __iomem *cfgtable)
 {
 	char *driver_version;
 	int i, size = sizeof(cfgtable->driver_version);
@@ -5701,8 +7214,8 @@ static int write_driver_ver_to_cfgtable(struct CfgTable __iomem *cfgtable)
 	return 0;
 }
 
-static void read_driver_ver_from_cfgtable(struct CfgTable __iomem *cfgtable,
-					  unsigned char *driver_ver)
+static __devinit void read_driver_ver_from_cfgtable(
+	struct CfgTable __iomem *cfgtable, unsigned char *driver_ver)
 {
 	int i;
 
@@ -5710,15 +7223,15 @@ static void read_driver_ver_from_cfgtable(struct CfgTable __iomem *cfgtable,
 		driver_ver[i] = readb(&cfgtable->driver_version[i]);
 }
 
-static int controller_reset_failed(struct CfgTable __iomem *cfgtable)
+static __devinit int controller_reset_failed(
+	struct CfgTable __iomem *cfgtable)
 {
-
 	char *driver_ver, *old_driver_ver;
 	int rc, size = sizeof(cfgtable->driver_version);
 
 	old_driver_ver = kmalloc(2 * size, GFP_KERNEL);
-	if (!old_driver_ver)
-		return -ENOMEM;
+	if (!old_driver_ver ) 
+		return 1; 
 	driver_ver = old_driver_ver + size;
 
 	/* After a reset, the 32 bytes of "driver version" in the cfgtable
@@ -5730,10 +7243,11 @@ static int controller_reset_failed(struct CfgTable __iomem *cfgtable)
 	kfree(old_driver_ver);
 	return rc;
 }
+
 /* This does a hard reset of the controller using PCI power management
  * states or the using the doorbell register.
  */
-static int hpsa_kdump_hard_reset_controller(struct pci_dev *pdev)
+static __devinit int hpsa_kdump_hard_reset_controller(struct pci_dev *pdev)
 {
 	u64 cfg_offset;
 	u32 cfg_base_addr;
@@ -5761,12 +7275,8 @@ static int hpsa_kdump_hard_reset_controller(struct pci_dev *pdev)
 	 */
 
 	rc = hpsa_lookup_board_id(pdev, &board_id);
-	if (rc < 0) {
-		dev_warn(&pdev->dev, "Board ID not found\n");
-		return rc;
-	}
-	if (!ctlr_is_resettable(board_id)) {
-		dev_warn(&pdev->dev, "Controller not resettable\n");
+	if (rc < 0 || !ctlr_is_resettable(board_id)) {
+		dev_warn(&pdev->dev, "Not resetting device.\n");
 		return -ENODEV;
 	}
 
@@ -5776,6 +7286,10 @@ static int hpsa_kdump_hard_reset_controller(struct pci_dev *pdev)
 
 	/* Save the PCI command register */
 	pci_read_config_word(pdev, 4, &command_register);
+	/* Turn the board off.  This is so that later pci_restore_state()
+	 * won't turn the board on before the rest of config space is ready.
+	 */
+	pci_disable_device(pdev);
 	pci_save_state(pdev);
 
 	/* find the first memory BAR, so we can find the cfg table */
@@ -5799,7 +7313,7 @@ static int hpsa_kdump_hard_reset_controller(struct pci_dev *pdev)
 	}
 	rc = write_driver_ver_to_cfgtable(cfgtable);
 	if (rc)
-		goto unmap_cfgtable;
+		return rc;
 
 	/* If reset via doorbell register is supported, use that.
 	 * There are two such methods.  Favor the newest method.
@@ -5811,8 +7325,10 @@ static int hpsa_kdump_hard_reset_controller(struct pci_dev *pdev)
 	} else {
 		use_doorbell = misc_fw_support & MISC_FW_DOORBELL_RESET;
 		if (use_doorbell) {
-			dev_warn(&pdev->dev,
-				"Soft reset not supported. Firmware update is required.\n");
+			dev_warn(&pdev->dev, "Controller claims that "
+				"'Bit 2 doorbell reset' is "
+				"supported, but not 'bit 5 doorbell reset'.  "
+				"Firmware update is recommended.\n");
 			rc = -ENOTSUPP; /* try soft reset */
 			goto unmap_cfgtable;
 		}
@@ -5823,16 +7339,44 @@ static int hpsa_kdump_hard_reset_controller(struct pci_dev *pdev)
 		goto unmap_cfgtable;
 
 	pci_restore_state(pdev);
+	rc = pci_enable_device(pdev);
+	if (rc) {
+		dev_warn(&pdev->dev, "Failed to enable PCI device.\n");
+		goto unmap_cfgtable;
+	}
 	pci_write_config_word(pdev, 4, command_register);
 
 	/* Some devices (notably the HP Smart Array 5i Controller)
-	   need a little pause here */
-	msleep(HPSA_POST_RESET_PAUSE_MSECS);
-
+	 * need a little pause here.  Not all controllers though,
+	 * and for some we will miss the transition to NOT READY
+	 * if we wait here.   As a heuristic, if either doorbell
+	 * reset method is supported, we'll skip this sleep.
+	 */
+	if (!use_doorbell)
+		msleep(HPSA_POST_RESET_PAUSE_MSECS);
+
+	if (!use_doorbell) {
+		/* Wait for board to become not ready, then ready.
+		 * (if we used the doorbell, then we already waited 5 secs
+		 * so the "not ready" state is already gone by so we
+		 * won't catch it.)
+		 */
+		dev_info(&pdev->dev, "Waiting for board to reset.\n");
+		rc = hpsa_wait_for_board_state(pdev, vaddr, BOARD_NOT_READY);
+		if (rc) {
+			dev_warn(&pdev->dev,
+				"failed waiting for board to reset."
+				" Will try soft reset.\n");
+			/* Not expected, but try soft reset later */
+			rc = -ENOTSUPP;
+			goto unmap_cfgtable;
+		}
+	}
 	rc = hpsa_wait_for_board_state(pdev, vaddr, BOARD_READY);
 	if (rc) {
 		dev_warn(&pdev->dev,
-			"Failed waiting for board to become ready after hard reset\n");
+			"Failed waiting for board to become ready "
+			"after hard reset\n");
 		goto unmap_cfgtable;
 	}
 
@@ -5860,7 +7404,7 @@ unmap_vaddr:
  *   the io functions.
  *   This is for debug only.
  */
-static void print_cfg_table(struct device *dev, struct CfgTable __iomem *tb)
+static void print_cfg_table(struct device *dev, struct CfgTable *tb)
 {
 #ifdef HPSA_DEBUG
 	int i;
@@ -5883,7 +7427,7 @@ static void print_cfg_table(struct device *dev, struct CfgTable __iomem *tb)
 	       readl(&(tb->HostWrite.CoalIntDelay)));
 	dev_info(dev, "   Coalesce Interrupt Count = 0x%x\n",
 	       readl(&(tb->HostWrite.CoalIntCount)));
-	dev_info(dev, "   Max outstanding commands = %d\n",
+	dev_info(dev, "   Max outstanding commands = 0x%d\n",
 	       readl(&(tb->CmdsOutMax)));
 	dev_info(dev, "   Bus Types = 0x%x\n", readl(&(tb->BusTypes)));
 	for (i = 0; i < 16; i++)
@@ -5930,16 +7474,43 @@ static int find_PCI_BAR_index(struct pci_dev *pdev, unsigned long pci_bar_addr)
 	return -1;
 }
 
+static int calculate_nreply_queues(void)
+{
+	/* Make sure reply_queues module param is in bounds */
+	if (reply_queues < 1)
+		return 1;
+	if (reply_queues > MAX_REPLY_QUEUES)
+		return MAX_REPLY_QUEUES;
+	/* Cap reply queues to number of CPUs, more is a waste */
+	if (num_online_cpus() < reply_queues)
+		return num_online_cpus();
+	return reply_queues;
+}
+
+static void hpsa_disable_interrupt_mode(struct ctlr_info *h)
+{
+#ifdef CONFIG_PCI_MSI
+	if (h->msix_vector) {
+		if (h->pdev->msix_enabled)
+			pci_disable_msix(h->pdev);
+	} else if (h->msi_vector) {
+		if (h->pdev->msi_enabled)
+			pci_disable_msi(h->pdev);
+	}
+#endif /* CONFIG_PCI_MSI */
+}
+
 /* If MSI/MSI-X is supported by the kernel we will try to enable it on
  * controllers that are capable. If not, we use legacy INTx mode.
  */
-
-static void hpsa_interrupt_mode(struct ctlr_info *h)
+static void __devinit hpsa_interrupt_mode(struct ctlr_info *h)
 {
 #ifdef CONFIG_PCI_MSI
 	int err, i;
 	struct msix_entry hpsa_msix_entries[MAX_REPLY_QUEUES];
 
+	h->nreply_queues = calculate_nreply_queues();
+
 	for (i = 0; i < MAX_REPLY_QUEUES; i++) {
 		hpsa_msix_entries[i].vector = 0;
 		hpsa_msix_entries[i].entry = i;
@@ -5951,25 +7522,31 @@ static void hpsa_interrupt_mode(struct ctlr_info *h)
 		goto default_int_mode;
 	if (pci_find_capability(h->pdev, PCI_CAP_ID_MSIX)) {
 		dev_info(&h->pdev->dev, "MSI-X capable controller\n");
+
 		h->msix_vector = MAX_REPLY_QUEUES;
 		if (h->msix_vector > num_online_cpus())
 			h->msix_vector = num_online_cpus();
-		err = pci_enable_msix_range(h->pdev, hpsa_msix_entries,
-					    1, h->msix_vector);
-		if (err < 0) {
-			dev_warn(&h->pdev->dev, "MSI-X init failed %d\n", err);
-			h->msix_vector = 0;
-			goto single_msi_mode;
-		} else if (err < h->msix_vector) {
+
+		err = pci_enable_msix(h->pdev, hpsa_msix_entries,
+					h->nreply_queues);
+		if (!err) {
+			for (i = 0; i < h->nreply_queues; i++)
+				h->intr[i] = hpsa_msix_entries[i].vector;
+			h->msix_vector = 1;
+			dev_warn(&h->pdev->dev, "Using %d reply queue(s)\n",
+					h->nreply_queues);
+			return;
+		}
+		if (err > 0) {
 			dev_warn(&h->pdev->dev, "only %d MSI-X vectors "
 			       "available\n", err);
+			goto default_int_mode;
+		} else {
+			dev_warn(&h->pdev->dev, "MSI-X init failed %d\n",
+			       err);
+			goto default_int_mode;
 		}
-		h->msix_vector = err;
-		for (i = 0; i < h->msix_vector; i++)
-			h->intr[i] = hpsa_msix_entries[i].vector;
-		return;
 	}
-single_msi_mode:
 	if (pci_find_capability(h->pdev, PCI_CAP_ID_MSI)) {
 		dev_info(&h->pdev->dev, "MSI capable controller\n");
 		if (!pci_enable_msi(h->pdev))
@@ -5978,12 +7555,14 @@ single_msi_mode:
 			dev_warn(&h->pdev->dev, "MSI init failed\n");
 	}
 default_int_mode:
+	h->nreply_queues = 1;
 #endif				/* CONFIG_PCI_MSI */
 	/* if we get here we're going to use the default interrupt mode */
+	dev_info(&h->pdev->dev, "Using %d reply queue(s)\n", h->nreply_queues);
 	h->intr[h->intr_mode] = h->pdev->irq;
 }
 
-static int hpsa_lookup_board_id(struct pci_dev *pdev, u32 *board_id)
+static int __devinit hpsa_lookup_board_id(struct pci_dev *pdev, u32 *board_id)
 {
 	int i;
 	u32 subsystem_vendor_id, subsystem_device_id;
@@ -6002,13 +7581,13 @@ static int hpsa_lookup_board_id(struct pci_dev *pdev, u32 *board_id)
 		!hpsa_allow_any) {
 		dev_warn(&pdev->dev, "unrecognized board ID: "
 			"0x%08x, ignoring.\n", *board_id);
-			return -ENODEV;
+		return -ENODEV;
 	}
 	return ARRAY_SIZE(products) - 1; /* generic unknown smart array */
 }
 
-static int hpsa_pci_find_memory_BAR(struct pci_dev *pdev,
-				    unsigned long *memory_bar)
+static int __devinit hpsa_pci_find_memory_BAR(struct pci_dev *pdev,
+	unsigned long *memory_bar)
 {
 	int i;
 
@@ -6024,34 +7603,9 @@ static int hpsa_pci_find_memory_BAR(struct pci_dev *pdev,
 	return -ENODEV;
 }
 
-static int hpsa_wait_for_board_state(struct pci_dev *pdev, void __iomem *vaddr,
-				     int wait_for_ready)
-{
-	int i, iterations;
-	u32 scratchpad;
-	if (wait_for_ready)
-		iterations = HPSA_BOARD_READY_ITERATIONS;
-	else
-		iterations = HPSA_BOARD_NOT_READY_ITERATIONS;
-
-	for (i = 0; i < iterations; i++) {
-		scratchpad = readl(vaddr + SA5_SCRATCHPAD_OFFSET);
-		if (wait_for_ready) {
-			if (scratchpad == HPSA_FIRMWARE_READY)
-				return 0;
-		} else {
-			if (scratchpad != HPSA_FIRMWARE_READY)
-				return 0;
-		}
-		msleep(HPSA_BOARD_READY_POLL_INTERVAL_MSECS);
-	}
-	dev_warn(&pdev->dev, "board not ready, timed out.\n");
-	return -ENODEV;
-}
-
-static int hpsa_find_cfg_addrs(struct pci_dev *pdev, void __iomem *vaddr,
-			       u32 *cfg_base_addr, u64 *cfg_base_addr_index,
-			       u64 *cfg_offset)
+static int hpsa_find_cfg_addrs(struct pci_dev *pdev,
+	void __iomem *vaddr, u32 *cfg_base_addr, u64 *cfg_base_addr_index,
+	u64 *cfg_offset)
 {
 	*cfg_base_addr = readl(vaddr + SA5_CTCFG_OFFSET);
 	*cfg_offset = readl(vaddr + SA5_CTMEM_OFFSET);
@@ -6064,7 +7618,16 @@ static int hpsa_find_cfg_addrs(struct pci_dev *pdev, void __iomem *vaddr,
 	return 0;
 }
 
-static int hpsa_find_cfgtables(struct ctlr_info *h)
+static void hpsa_free_cfgtables(struct ctlr_info *h)
+{
+	iounmap(h->transtable);
+	iounmap(h->cfgtable);
+}
+
+/* Find and map CISS config table and transfer table
++ * several items must be unmapped (freed) later
++ * */
+static int __devinit hpsa_find_cfgtables(struct ctlr_info *h)
 {
 	u64 cfg_offset;
 	u32 cfg_base_addr;
@@ -6084,18 +7647,21 @@ static int hpsa_find_cfgtables(struct ctlr_info *h)
 	}
 	rc = write_driver_ver_to_cfgtable(h->cfgtable);
 	if (rc)
-		return rc;
+		return -ENOMEM;
 	/* Find performant mode table. */
 	trans_offset = readl(&h->cfgtable->TransMethodOffset);
 	h->transtable = remap_pci_mem(pci_resource_start(h->pdev,
 				cfg_base_addr_index)+cfg_offset+trans_offset,
 				sizeof(*h->transtable));
-	if (!h->transtable)
+	if (!h->transtable) {
+		dev_err(&h->pdev->dev, "Failed mapping transfer table\n");
+		hpsa_free_cfgtables(h);
 		return -ENOMEM;
+	}
 	return 0;
 }
 
-static void hpsa_get_max_perf_mode_cmds(struct ctlr_info *h)
+static void __devinit hpsa_get_max_perf_mode_cmds(struct ctlr_info *h)
 {
 	h->max_commands = readl(&(h->cfgtable->MaxPerformantModeCommands));
 
@@ -6125,7 +7691,7 @@ static int hpsa_supports_chained_sg_blocks(struct ctlr_info *h)
  * max commands, max SG elements without chaining, and with chaining,
  * SG chain block size, etc.
  */
-static void hpsa_find_board_params(struct ctlr_info *h)
+static void __devinit hpsa_find_board_params(struct ctlr_info *h)
 {
 	hpsa_get_max_perf_mode_cmds(h);
 	h->nr_cmds = h->max_commands;
@@ -6134,47 +7700,51 @@ static void hpsa_find_board_params(struct ctlr_info *h)
 	if (hpsa_supports_chained_sg_blocks(h)) {
 		/* Limit in-command s/g elements to 32 save dma'able memory. */
 		h->max_cmd_sg_entries = 32;
-		h->chainsize = h->maxsgentries - h->max_cmd_sg_entries;
+		h->chainsize = h->maxsgentries - h->max_cmd_sg_entries + 1;
 		h->maxsgentries--; /* save one for chain pointer */
 	} else {
-		/*
-		 * Original smart arrays supported at most 31 s/g entries
-		 * embedded inline in the command (trying to use more
-		 * would lock up the controller)
+		/* Original smart arrays supported at most 31 scatter gather
+		 * entries embedded inline in the command (trying to use more
+		 * would lock up the controller, see
+		 * https://lkml.org/lkml/2001/12/4/139 ).
 		 */
 		h->max_cmd_sg_entries = 31;
-		h->maxsgentries = 31; /* default to traditional values */
+ 		h->maxsgentries = 31; /* default to traditional values */
 		h->chainsize = 0;
 	}
 
 	/* Find out what task management functions are supported and cache */
 	h->TMFSupportFlags = readl(&(h->cfgtable->TMFSupportFlags));
-	if (!(HPSATMF_PHYS_TASK_ABORT & h->TMFSupportFlags))
-		dev_warn(&h->pdev->dev, "Physical aborts not supported\n");
-	if (!(HPSATMF_LOG_TASK_ABORT & h->TMFSupportFlags))
-		dev_warn(&h->pdev->dev, "Logical aborts not supported\n");
+	printk(KERN_WARNING "Physical aborts supported: %s\n",
+		(HPSATMF_PHYS_TASK_ABORT & h->TMFSupportFlags) ? "yes" : "no");
+	printk(KERN_WARNING "Logical aborts supported: %s\n",
+		(HPSATMF_LOG_TASK_ABORT & h->TMFSupportFlags) ? "yes" : "no");
+	printk(KERN_WARNING "HP SSD Smart Path aborts supported: %s\n",
+		(HPSATMF_IOACCEL_ENABLED & h->TMFSupportFlags) ? "yes" : "no");
 }
 
-static inline bool hpsa_CISS_signature_present(struct ctlr_info *h)
+static inline int hpsa_CISS_signature_present(struct ctlr_info *h)
 {
-	if (!check_signature(h->cfgtable->Signature, "CISS", 4)) {
+	if ((readb(&h->cfgtable->Signature[0]) != 'C') ||
+	    (readb(&h->cfgtable->Signature[1]) != 'I') ||
+	    (readb(&h->cfgtable->Signature[2]) != 'S') ||
+	    (readb(&h->cfgtable->Signature[3]) != 'S')) {
 		dev_err(&h->pdev->dev, "not a valid CISS config table\n");
-		return false;
+		return 0;
 	}
-	return true;
+	return 1;
 }
 
-static inline void hpsa_set_driver_support_bits(struct ctlr_info *h)
+/* Need to enable prefetch in the SCSI core for 6400 in x86 */
+static inline void hpsa_enable_scsi_prefetch(struct ctlr_info *h)
 {
-	u32 driver_support;
-
-	driver_support = readl(&(h->cfgtable->driver_support));
-	/* Need to enable prefetch in the SCSI core for 6400 in x86 */
 #ifdef CONFIG_X86
-	driver_support |= ENABLE_SCSI_PREFETCH;
+	u32 prefetch;
+
+	prefetch = readl(&(h->cfgtable->driver_support));
+	prefetch |= ENABLE_SCSI_PREFETCH;
+	writel(prefetch, &(h->cfgtable->driver_support));
 #endif
-	driver_support |= ENABLE_UNIT_ATTN;
-	writel(driver_support, &(h->cfgtable->driver_support));
 }
 
 /* Disable DMA prefetch for the P600.  Otherwise an ASIC bug may result
@@ -6236,7 +7806,7 @@ done:
 }
 
 /* return -ENODEV or other reason on error, 0 on success */
-static int hpsa_enter_simple_mode(struct ctlr_info *h)
+static int __devinit hpsa_enter_simple_mode(struct ctlr_info *h)
 {
 	u32 trans_support;
 
@@ -6262,71 +7832,86 @@ error:
 	return -ENODEV;
 }
 
-static int hpsa_pci_init(struct ctlr_info *h)
+/* free items allocated or mapped by hpsa_pci_init */
+static void hpsa_free_pci_init(struct ctlr_info *h)
+{
+	hpsa_free_cfgtables(h);			/* pci_init 4 */
+	iounmap(h->vaddr);			/* pci_init 3 */
+	hpsa_disable_interrupt_mode(h);		/* pci_init 2 */
+	pci_release_regions(h->pdev);		/* pci_init 2 */
+	pci_disable_device(h->pdev);		/* pci_init 1 */
+}
+
+/* several items must be freed later */
+static int __devinit hpsa_pci_init(struct ctlr_info *h)
 {
 	int prod_index, err;
 
 	prod_index = hpsa_lookup_board_id(h->pdev, &h->board_id);
 	if (prod_index < 0)
-		return prod_index;
+		return -ENODEV;
+
 	h->product_name = products[prod_index].product_name;
 	h->access = *(products[prod_index].access);
 
-	pci_disable_link_state(h->pdev, PCIE_LINK_STATE_L0S |
-			       PCIE_LINK_STATE_L1 | PCIE_LINK_STATE_CLKPM);
+	h->needs_abort_tags_swizzled =
+		ctlr_needs_abort_tags_swizzled(h->board_id);
 
 	err = pci_enable_device(h->pdev);
 	if (err) {
-		dev_warn(&h->pdev->dev, "unable to enable PCI device\n");
+		dev_err(&h->pdev->dev, "failed to enable PCI device\n");
 		return err;
 	}
 
+	/* Enable bus mastering (pci_disable_device may disable this) */
+	pci_set_master(h->pdev);
+
 	err = pci_request_regions(h->pdev, HPSA);
 	if (err) {
 		dev_err(&h->pdev->dev,
-			"cannot obtain PCI resources, aborting\n");
-		return err;
+			"failed to obtain PCI resources\n");
+		goto clean1;	/* pci */
 	}
 
-	pci_set_master(h->pdev);
-
 	hpsa_interrupt_mode(h);
+
 	err = hpsa_pci_find_memory_BAR(h->pdev, &h->paddr);
 	if (err)
-		goto err_out_free_res;
+		goto clean2;	/* intmode+region, pci */
 	h->vaddr = remap_pci_mem(h->paddr, 0x250);
 	if (!h->vaddr) {
+		dev_err(&h->pdev->dev, "failed to remap PCI mem\n");
 		err = -ENOMEM;
-		goto err_out_free_res;
+		goto clean2;	/* intmode+region, pci */
 	}
 	err = hpsa_wait_for_board_state(h->pdev, h->vaddr, BOARD_READY);
 	if (err)
-		goto err_out_free_res;
+		goto clean3;	/* vaddr, intmode+region, pci */
 	err = hpsa_find_cfgtables(h);
 	if (err)
-		goto err_out_free_res;
+		goto clean3;	/* vaddr, intmode+region, pci */
 	hpsa_find_board_params(h);
 
 	if (!hpsa_CISS_signature_present(h)) {
 		err = -ENODEV;
-		goto err_out_free_res;
+		goto clean4;	/* cfgtables, vaddr, intmode+region, pci */
 	}
-	hpsa_set_driver_support_bits(h);
+	hpsa_enable_scsi_prefetch(h);
 	hpsa_p600_dma_prefetch_quirk(h);
 	err = hpsa_enter_simple_mode(h);
 	if (err)
-		goto err_out_free_res;
+		goto clean4;	/* cfgtables, vaddr, intmode+region, pci */
 	return 0;
 
-err_out_free_res:
-	if (h->transtable)
-		iounmap(h->transtable);
-	if (h->cfgtable)
-		iounmap(h->cfgtable);
-	if (h->vaddr)
-		iounmap(h->vaddr);
-	pci_disable_device(h->pdev);
+clean4:	/* cfgtables, vaddr, intmode+region, pci */
+	hpsa_free_cfgtables(h);
+clean3:	/* vaddr, intmode+region, pci */
+	iounmap(h->vaddr);
+clean2:	/* intmode+region, pci */
+	hpsa_disable_interrupt_mode(h);
 	pci_release_regions(h->pdev);
+clean1:	/* pci */
+	pci_disable_device(h->pdev);
 	return err;
 }
 
@@ -6346,41 +7931,13 @@ static void hpsa_hba_inquiry(struct ctlr_info *h)
 	}
 }
 
-static int hpsa_init_reset_devices(struct pci_dev *pdev)
+static __devinit int hpsa_init_reset_devices(struct pci_dev *pdev)
 {
 	int rc, i;
-	void __iomem *vaddr;
 
 	if (!reset_devices)
 		return 0;
 
-	/* kdump kernel is loading, we don't know in which state is
-	 * the pci interface. The dev->enable_cnt is equal zero
-	 * so we call enable+disable, wait a while and switch it on.
-	 */
-	rc = pci_enable_device(pdev);
-	if (rc) {
-		dev_warn(&pdev->dev, "Failed to enable PCI device\n");
-		return -ENODEV;
-	}
-	pci_disable_device(pdev);
-	msleep(260);			/* a randomly chosen number */
-	rc = pci_enable_device(pdev);
-	if (rc) {
-		dev_warn(&pdev->dev, "failed to enable device.\n");
-		return -ENODEV;
-	}
-
-	pci_set_master(pdev);
-
-	vaddr = pci_ioremap_bar(pdev, 0);
-	if (vaddr == NULL) {
-		rc = -ENOMEM;
-		goto out_disable;
-	}
-	writel(SA5_INTR_OFF, vaddr + SA5_REPLY_INTR_MASK_OFFSET);
-	iounmap(vaddr);
-
 	/* Reset the controller with a PCI power-cycle or via doorbell */
 	rc = hpsa_kdump_hard_reset_controller(pdev);
 
@@ -6389,11 +7946,13 @@ static int hpsa_init_reset_devices(struct pci_dev *pdev)
 	 * "performant mode".  Or, it might be 640x, which can't reset
 	 * due to concerns about shared bbwc between 6402/6404 pair.
 	 */
+	if (rc == -ENOTSUPP)
+		return rc; /* just try to do the kdump anyhow. */
 	if (rc)
-		goto out_disable;
+		return -ENODEV;
 
 	/* Now try to get the controller to respond to a no-op */
-	dev_info(&pdev->dev, "Waiting for controller to respond to no-op\n");
+	dev_warn(&pdev->dev, "Waiting for controller to respond to no-op\n");
 	for (i = 0; i < HPSA_POST_RESET_NOOP_RETRIES; i++) {
 		if (hpsa_noop(pdev) == 0)
 			break;
@@ -6401,14 +7960,24 @@ static int hpsa_init_reset_devices(struct pci_dev *pdev)
 			dev_warn(&pdev->dev, "no-op failed%s\n",
 					(i < 11 ? "; re-trying" : ""));
 	}
+	return 0;
+}
 
-out_disable:
-
-	pci_disable_device(pdev);
-	return rc;
+static void hpsa_free_cmd_pool(struct ctlr_info *h)
+{
+	kfree(h->cmd_pool_bits);
+	if (h->cmd_pool)
+		pci_free_consistent(h->pdev,
+			    h->nr_cmds * sizeof(struct CommandList),
+			    h->cmd_pool, h->cmd_pool_dhandle);
+	if (h->errinfo_pool)
+		pci_free_consistent(h->pdev,
+			    h->nr_cmds * sizeof(struct ErrorInfo),
+			    h->errinfo_pool,
+			    h->errinfo_pool_dhandle);
 }
 
-static int hpsa_allocate_cmd_pool(struct ctlr_info *h)
+static __devinit int hpsa_alloc_cmd_pool(struct ctlr_info *h)
 {
 	h->cmd_pool_bits = kzalloc(
 		DIV_ROUND_UP(h->nr_cmds, BITS_PER_LONG) *
@@ -6425,41 +7994,20 @@ static int hpsa_allocate_cmd_pool(struct ctlr_info *h)
 		dev_err(&h->pdev->dev, "out of memory in %s", __func__);
 		goto clean_up;
 	}
+	hpsa_preinitialize_commands(h);
 	return 0;
 clean_up:
 	hpsa_free_cmd_pool(h);
 	return -ENOMEM;
 }
 
-static void hpsa_free_cmd_pool(struct ctlr_info *h)
-{
-	kfree(h->cmd_pool_bits);
-	if (h->cmd_pool)
-		pci_free_consistent(h->pdev,
-			    h->nr_cmds * sizeof(struct CommandList),
-			    h->cmd_pool, h->cmd_pool_dhandle);
-	if (h->ioaccel2_cmd_pool)
-		pci_free_consistent(h->pdev,
-			h->nr_cmds * sizeof(*h->ioaccel2_cmd_pool),
-			h->ioaccel2_cmd_pool, h->ioaccel2_cmd_pool_dhandle);
-	if (h->errinfo_pool)
-		pci_free_consistent(h->pdev,
-			    h->nr_cmds * sizeof(struct ErrorInfo),
-			    h->errinfo_pool,
-			    h->errinfo_pool_dhandle);
-	if (h->ioaccel_cmd_pool)
-		pci_free_consistent(h->pdev,
-			h->nr_cmds * sizeof(struct io_accel1_cmd),
-			h->ioaccel_cmd_pool, h->ioaccel_cmd_pool_dhandle);
-}
-
 static void hpsa_irq_affinity_hints(struct ctlr_info *h)
 {
-	int i, cpu;
+	int i, cpu, rc;
 
 	cpu = cpumask_first(cpu_online_mask);
 	for (i = 0; i < h->msix_vector; i++) {
-		irq_set_affinity_hint(h->intr[i], get_cpu_mask(cpu));
+		rc = irq_set_affinity_hint(h->intr[i], get_cpu_mask(cpu));
 		cpu = cpumask_next(cpu, cpu_online_mask);
 	}
 }
@@ -6477,20 +8025,18 @@ static void hpsa_free_irqs(struct ctlr_info *h)
 		return;
 	}
 
-	for (i = 0; i < h->msix_vector; i++) {
+	for (i = 0; i < h->nreply_queues; i++) {
 		irq_set_affinity_hint(h->intr[i], NULL);
 		free_irq(h->intr[i], &h->q[i]);
 	}
-	for (; i < MAX_REPLY_QUEUES; i++)
-		h->q[i] = 0;
 }
 
 /* returns 0 on success; cleans up and returns -Enn on error */
 static int hpsa_request_irqs(struct ctlr_info *h,
-	irqreturn_t (*msixhandler)(int, void *),
-	irqreturn_t (*intxhandler)(int, void *))
+	INTERRUPT_HANDLER_TYPE(msixhandler),
+	INTERRUPT_HANDLER_TYPE(intxhandler))
 {
-	int rc, i;
+	int rc = -1, i;
 
 	/*
 	 * initialize h->q[x] = x so that interrupt handlers know which
@@ -6499,31 +8045,16 @@ static int hpsa_request_irqs(struct ctlr_info *h,
 	for (i = 0; i < MAX_REPLY_QUEUES; i++)
 		h->q[i] = (u8) i;
 
-	if (h->intr_mode == PERF_MODE_INT && h->msix_vector > 0) {
+	if (h->intr_mode == PERF_MODE_INT && h->msix_vector) {
 		/* If performant mode and MSI-X, use multiple reply queues */
-		for (i = 0; i < h->msix_vector; i++) {
+		for (i = 0; i < h->nreply_queues; i++)
 			rc = request_irq(h->intr[i], msixhandler,
-					0, h->devname,
-					&h->q[i]);
-			if (rc) {
-				int j;
-
-				dev_err(&h->pdev->dev,
-					"failed to get irq %d for %s\n",
-				       h->intr[i], h->devname);
-				for (j = 0; j < i; j++) {
-					free_irq(h->intr[j], &h->q[j]);
-					h->q[j] = 0;
-				}
-				for (; j < MAX_REPLY_QUEUES; j++)
-					h->q[j] = 0;
-				return rc;
-			}
-		}
+					0, h->devname,
+					&h->q[i]);
 		hpsa_irq_affinity_hints(h);
 	} else {
 		/* Use single reply pool */
-		if (h->msix_vector > 0 || h->msi_vector) {
+		if (h->msix_vector || h->msi_vector) {
 			rc = request_irq(h->intr[h->intr_mode],
 				msixhandler, 0, h->devname,
 				&h->q[h->intr_mode]);
@@ -6534,8 +8065,9 @@ static int hpsa_request_irqs(struct ctlr_info *h,
 		}
 	}
 	if (rc) {
-		dev_err(&h->pdev->dev, "unable to get irq %d for %s\n",
+		dev_err(&h->pdev->dev, "failed to get irq %d for %s\n",
 		       h->intr[h->intr_mode], h->devname);
+		hpsa_free_irqs(h);
 		return -ENODEV;
 	}
 	return 0;
@@ -6543,11 +8075,7 @@ static int hpsa_request_irqs(struct ctlr_info *h,
 
 static int hpsa_kdump_soft_reset(struct ctlr_info *h)
 {
-	if (hpsa_send_host_reset(h, RAID_CTLR_LUNID,
-		HPSA_RESET_TYPE_CONTROLLER)) {
-		dev_warn(&h->pdev->dev, "Resetting array controller failed.\n");
-		return -EIO;
-	}
+	hpsa_send_host_reset(h, RAID_CTLR_LUNID, HPSA_RESET_TYPE_CONTROLLER);
 
 	dev_info(&h->pdev->dev, "Waiting for board to soft reset.\n");
 	if (hpsa_wait_for_board_state(h->pdev, h->vaddr, BOARD_NOT_READY)) {
@@ -6565,20 +8093,6 @@ static int hpsa_kdump_soft_reset(struct ctlr_info *h)
 	return 0;
 }
 
-static void hpsa_free_irqs_and_disable_msix(struct ctlr_info *h)
-{
-	hpsa_free_irqs(h);
-#ifdef CONFIG_PCI_MSI
-	if (h->msix_vector) {
-		if (h->pdev->msix_enabled)
-			pci_disable_msix(h->pdev);
-	} else if (h->msi_vector) {
-		if (h->pdev->msi_enabled)
-			pci_disable_msi(h->pdev);
-	}
-#endif /* CONFIG_PCI_MSI */
-}
-
 static void hpsa_free_reply_queues(struct ctlr_info *h)
 {
 	int i;
@@ -6595,21 +8109,16 @@ static void hpsa_free_reply_queues(struct ctlr_info *h)
 
 static void hpsa_undo_allocations_after_kdump_soft_reset(struct ctlr_info *h)
 {
-	hpsa_free_irqs_and_disable_msix(h);
-	hpsa_free_sg_chain_blocks(h);
-	hpsa_free_cmd_pool(h);
-	kfree(h->ioaccel1_blockFetchTable);
-	kfree(h->blockFetchTable);
-	hpsa_free_reply_queues(h);
-	if (h->vaddr)
-		iounmap(h->vaddr);
-	if (h->transtable)
-		iounmap(h->transtable);
-	if (h->cfgtable)
-		iounmap(h->cfgtable);
-	pci_disable_device(h->pdev);
-	pci_release_regions(h->pdev);
-	kfree(h);
+	hpsa_free_performant_mode(h);		/* init_one 7 */
+	hpsa_free_ioaccel2_sg_chain_blocks(h);
+	hpsa_free_sg_chain_blocks(h);		/* init_one 6 */
+	hpsa_free_cmd_pool(h);			/* init_one 5 */
+	hpsa_free_irqs(h);			/* init_one 4 */
+	hpsa_free_cfgtables(h);			/* pci_init 4 */
+	iounmap(h->vaddr);			/* pci_init 3 */
+	hpsa_disable_interrupt_mode(h);		/* pci_init 2 */
+	pci_release_regions(h->pdev);		/* pci_init 2 */
+	kfree(h);				/* init_one 1 */
 }
 
 /* Called when controller lockup detected. */
@@ -6617,27 +8126,37 @@ static void fail_all_outstanding_cmds(struct ctlr_info *h)
 {
 	int i, refcount;
 	struct CommandList *c;
+	int failcount = 0;
 
 	flush_workqueue(h->resubmit_wq); /* ensure all cmds are fully built */
 	for (i = 0; i < h->nr_cmds; i++) {
 		c = h->cmd_pool + i;
 		refcount = atomic_inc_return(&c->refcount);
 		if (refcount > 1) {
-			c->err_info->CommandStatus = CMD_HARDWARE_ERR;
+			c->err_info->CommandStatus = CMD_CTLR_LOCKUP;
+			/* CMD_CTLR_LOCKUP gets finish_cmd to return
+			 * DID_NO_CONNECT which doesn't get retried
+			 */
 			finish_cmd(c);
+			atomic_dec(&h->commands_outstanding);
+			failcount++;
 		}
 		cmd_free(h, c);
 	}
+	dev_warn(&h->pdev->dev,
+		"failed %d commands in fail_all\n", failcount);
 }
 
 static void set_lockup_detected_for_all_cpus(struct ctlr_info *h, u32 value)
 {
-	int cpu;
+	int i, cpu;
 
-	for_each_online_cpu(cpu) {
+	cpu = cpumask_first(cpu_online_mask);
+	for (i = 0; i < num_online_cpus(); i++) {
 		u32 *lockup_detected;
 		lockup_detected = per_cpu_ptr(h->lockup_detected, cpu);
 		*lockup_detected = value;
+		cpu = cpumask_next(cpu, cpu_online_mask);
 	}
 	wmb(); /* be sure the per-cpu variables are out to memory */
 }
@@ -6653,15 +8172,25 @@ static void controller_lockup_detected(struct ctlr_info *h)
 	if (!lockup_detected) {
 		/* no heartbeat, but controller gave us a zero. */
 		dev_warn(&h->pdev->dev,
-			"lockup detected but scratchpad register is zero\n");
+			"lockup detected after %d but scratchpad register is zero\n",
+			h->heartbeat_sample_interval / HZ);
 		lockup_detected = 0xffffffff;
 	}
 	set_lockup_detected_for_all_cpus(h, lockup_detected);
 	spin_unlock_irqrestore(&h->lock, flags);
-	dev_warn(&h->pdev->dev, "Controller lockup detected: 0x%08x\n",
-			lockup_detected);
-	pci_disable_device(h->pdev);
-	fail_all_outstanding_cmds(h);
+	dev_warn(&h->pdev->dev, "Controller lockup detected: 0x%08x after %d\n",
+			lockup_detected, h->heartbeat_sample_interval / HZ);
+	/* If enabled explicitly panic or reset the system */
+	if(hpsa_lockup_action == 1)
+		panic("FATAL -- Smart Array Controller Lockup detected"); // not advised with kdump 
+									  // enabled as it leads to hang
+	else if(hpsa_lockup_action == 2)
+		emergency_restart();
+	else {
+		pci_disable_device(h->pdev);
+		fail_all_outstanding_cmds(h);
+		dev_warn(&h->pdev->dev, "Controller Lockup detected, controller disabled.\n");
+	}
 }
 
 static void detect_controller_lockup(struct ctlr_info *h)
@@ -6670,6 +8199,9 @@ static void detect_controller_lockup(struct ctlr_info *h)
 	u32 heartbeat;
 	unsigned long flags;
 
+	if (!h->lockup_detector_enabled)
+		return;
+
 	now = get_jiffies_64();
 	/* If we've received an interrupt recently, we're ok. */
 	if (time_after64(h->last_intr_timestamp +
@@ -6713,16 +8245,16 @@ static void hpsa_ack_ctlr_events(struct ctlr_info *h)
 		(h->events & HPSA_EVENT_NOTIFY_ACCEL_IO_PATH_STATE_CHANGE ||
 		 h->events & HPSA_EVENT_NOTIFY_ACCEL_IO_PATH_CONFIG_CHANGE)) {
 
-		if (h->events & HPSA_EVENT_NOTIFY_ACCEL_IO_PATH_STATE_CHANGE)
+		if (h->events & HPSA_EVENT_NOTIFY_ACCEL_IO_PATH_STATE_CHANGE )
 			event_type = "state change";
-		if (h->events & HPSA_EVENT_NOTIFY_ACCEL_IO_PATH_CONFIG_CHANGE)
+		if (h->events & HPSA_EVENT_NOTIFY_ACCEL_IO_PATH_CONFIG_CHANGE )
 			event_type = "configuration change";
 		/* Stop sending new RAID offload reqs via the IO accelerator */
 		scsi_block_requests(h->scsi_host);
 		for (i = 0; i < h->ndevices; i++)
-			h->dev[i]->offload_enabled = 0;
+			h->dev[i]->offload_enabled = 0; 
 		hpsa_drain_accel_commands(h);
-		/* Set 'accelerator path config change' bit */
+		/* Set 'accelerator path config change' bit in clear_event_notify field */
 		dev_warn(&h->pdev->dev,
 			"Acknowledging event: 0x%08x (HP SSD Smart Path %s)\n",
 			h->events, event_type);
@@ -6747,11 +8279,16 @@ static void hpsa_ack_ctlr_events(struct ctlr_info *h)
 
 /* Check a register on the controller to see if there are configuration
  * changes (added/changed/removed logical drives, etc.) which mean that
- * we should rescan the controller for devices.
- * Also check flag for driver-initiated rescan.
+ * we should rescan the controller for devices.  
+ * Also check flag for driver-initiated rescan. 
  */
 static int hpsa_ctlr_needs_rescan(struct ctlr_info *h)
 {
+	if (h->drv_req_rescan) {
+		h->drv_req_rescan = 0;
+		return 1;
+	}
+
 	if (!(h->fw_support & MISC_FW_EVENT_NOTIFY))
 		return 0;
 
@@ -6785,15 +8322,20 @@ static int hpsa_offline_devices_ready(struct ctlr_info *h)
 	return 0;
 }
 
-static void hpsa_rescan_ctlr_worker(struct work_struct *work)
+static void hpsa_requeue_worker(struct ctlr_info *h, struct delayed_work *wi)
 {
 	unsigned long flags;
-	struct ctlr_info *h = container_of(to_delayed_work(work),
-					struct ctlr_info, rescan_ctlr_work);
 
+	spin_lock_irqsave(&h->lock, flags);
+	if (!h->remove_in_progress)
+		schedule_delayed_work(wi, h->heartbeat_sample_interval);
+	spin_unlock_irqrestore(&h->lock, flags);
+}
 
-	if (h->remove_in_progress)
-		return;
+static void hpsa_rescan_ctlr_worker(struct work_struct *work)
+{
+	struct ctlr_info *h = container_of(to_delayed_work(work),
+					struct ctlr_info, rescan_ctlr_work);
 
 	if (hpsa_ctlr_needs_rescan(h) || hpsa_offline_devices_ready(h)) {
 		scsi_host_get(h->scsi_host);
@@ -6801,49 +8343,106 @@ static void hpsa_rescan_ctlr_worker(struct work_struct *work)
 		hpsa_scan_start(h->scsi_host);
 		scsi_host_put(h->scsi_host);
 	}
-	spin_lock_irqsave(&h->lock, flags);
-	if (!h->remove_in_progress)
-		queue_delayed_work(h->rescan_ctlr_wq, &h->rescan_ctlr_work,
-				h->heartbeat_sample_interval);
-	spin_unlock_irqrestore(&h->lock, flags);
+	hpsa_requeue_worker(h, &h->rescan_ctlr_work);
 }
 
 static void hpsa_monitor_ctlr_worker(struct work_struct *work)
 {
-	unsigned long flags;
 	struct ctlr_info *h = container_of(to_delayed_work(work),
 					struct ctlr_info, monitor_ctlr_work);
-
 	detect_controller_lockup(h);
 	if (lockup_detected(h))
 		return;
 
-	spin_lock_irqsave(&h->lock, flags);
-	if (!h->remove_in_progress)
-		schedule_delayed_work(&h->monitor_ctlr_work,
-				h->heartbeat_sample_interval);
-	spin_unlock_irqrestore(&h->lock, flags);
+	hpsa_requeue_worker(h, &h->monitor_ctlr_work);
 }
 
-static struct workqueue_struct *hpsa_create_controller_wq(struct ctlr_info *h,
-						char *name)
+static void check_board_id_tables(void)
 {
-	struct workqueue_struct *wq = NULL;
-
-	wq = alloc_ordered_workqueue("%s_%d_hpsa", 0, name, h->ctlr);
-	if (!wq)
-		dev_err(&h->pdev->dev, "failed to create %s workqueue\n", name);
+	int i, j, found;
+	uint32_t board_id;
+	int count;
 
-	return wq;
+	for (i = 0; i < ARRAY_SIZE(products); i++) {
+		board_id = products[i].board_id;
+		if (board_id == 0xFFFF103C) /* sentinel entry */
+			continue;
+		found = 0;
+		count = 0;
+		for (j = 0; j < ARRAY_SIZE(hpsa_pci_device_id); j++) {
+			uint32_t bid;
+
+			bid = (hpsa_pci_device_id[j].subdevice << 16) |
+				hpsa_pci_device_id[j].subvendor;
+			if (bid == board_id) {
+				found = 1;
+				if (i != j) {
+					printk(KERN_WARNING HPSA
+						": products[%d] (%s) found at pci table entry %d\n",
+						i, products[i].product_name, j);
+				}
+				count++;
+			}
+		}
+		if (!found)
+			printk(KERN_WARNING HPSA
+				": products[%d] (%s) not found in pci table\n",
+				i, products[i].product_name);
+		if (count > 1)
+			printk(KERN_WARNING HPSA
+				": products[%d] (%s) has duplicate entries in pci table\n",
+				i, products[i].product_name);
+	}
+
+	for (i = 0; i < ARRAY_SIZE(hpsa_pci_device_id); i++) {
+		board_id = (hpsa_pci_device_id[i].subdevice << 16) |
+			hpsa_pci_device_id[i].subvendor;
+		if (hpsa_pci_device_id[i].vendor == 0) /* sentinel */
+			continue;
+		if (hpsa_pci_device_id[i].subvendor == PCI_ANY_ID &&
+			hpsa_pci_device_id[i].subdevice == PCI_ANY_ID)
+			continue;
+		count = 0;
+		found = 0;
+		for (j = 0; j < ARRAY_SIZE(products); j++) {
+			if (board_id == products[j].board_id) {
+				if (j != i) {
+					printk(KERN_WARNING HPSA
+						": pci table entry %d found at product entry %d (%s)\n",
+						i, j, products[j].product_name);
+				}
+				found = 1;
+				count++;
+			}
+		}
+		if (!found)
+			printk(KERN_WARNING HPSA
+				": pci table entry %d (%04x:%04x) not found in product table\n",
+				i, hpsa_pci_device_id[i].subvendor,
+				hpsa_pci_device_id[i].subdevice);
+		if (count > 1)
+			printk(KERN_WARNING HPSA
+				": pci table entry %d (%04x:%04x) has duplicate product[] entries\n",
+				i, hpsa_pci_device_id[i].subvendor,
+				hpsa_pci_device_id[i].subdevice);
+	}
+	if (ARRAY_SIZE(products) != ARRAY_SIZE(hpsa_pci_device_id) - 2) {
+		printk(KERN_WARNING HPSA
+			": suspicious relative cardinality of products vs hpsa_pci_device_id (%lu/%lu)\n",
+			(unsigned long) ARRAY_SIZE(products),
+			(unsigned long) ARRAY_SIZE(hpsa_pci_device_id)-2);
+	}
 }
 
-static int hpsa_init_one(struct pci_dev *pdev, const struct pci_device_id *ent)
+static int __devinit hpsa_init_one(struct pci_dev *pdev,
+				    const struct pci_device_id *ent)
 {
 	int dac, rc;
 	struct ctlr_info *h;
 	int try_soft_reset = 0;
 	unsigned long flags;
 
+	check_board_id_tables();
 	if (number_of_controllers == 0)
 		printk(KERN_INFO DRIVER_NAME "\n");
 
@@ -6868,40 +8467,38 @@ reinit_after_soft_reset:
 	 */
 	BUILD_BUG_ON(sizeof(struct CommandList) % COMMANDLIST_ALIGNMENT);
 	h = kzalloc(sizeof(*h), GFP_KERNEL);
-	if (!h)
+	if (!h) {
+		dev_err(&h->pdev->dev, "Failed to allocate controller head\n");
 		return -ENOMEM;
+	}
 
 	h->pdev = pdev;
+
 	h->intr_mode = hpsa_simple_mode ? SIMPLE_MODE_INT : PERF_MODE_INT;
 	INIT_LIST_HEAD(&h->offline_device_list);
 	spin_lock_init(&h->lock);
-	spin_lock_init(&h->offline_device_lock);
 	spin_lock_init(&h->scan_lock);
+	spin_lock_init(&h->offline_device_lock);
 	atomic_set(&h->passthru_cmds_avail, HPSA_MAX_CONCURRENT_PASSTHRUS);
+	atomic_set(&h->abort_cmds_available, HPSA_CMDS_RESERVED_FOR_ABORTS);
 
-	h->rescan_ctlr_wq = hpsa_create_controller_wq(h, "rescan");
-	if (!h->rescan_ctlr_wq) {
-		rc = -ENOMEM;
-		goto clean1;
-	}
-
-	h->resubmit_wq = hpsa_create_controller_wq(h, "resubmit");
+	h->resubmit_wq = alloc_workqueue("hpsa", WQ_MEM_RECLAIM, 0);
 	if (!h->resubmit_wq) {
-		rc = -ENOMEM;
-		goto clean1;
+		dev_err(&h->pdev->dev, "Failed to allocate work queue\n");
+		goto clean1;	/* aer/h */
 	}
-
 	/* Allocate and clear per-cpu variable lockup_detected */
 	h->lockup_detected = alloc_percpu(u32);
 	if (!h->lockup_detected) {
+		dev_err(&h->pdev->dev, "Failed to allocate lockup detector\n");
 		rc = -ENOMEM;
-		goto clean1;
+		goto clean1;	/* wq/aer/h */
 	}
 	set_lockup_detected_for_all_cpus(h, 0);
 
 	rc = hpsa_pci_init(h);
-	if (rc != 0)
-		goto clean1;
+	if (rc)
+		goto clean2;	/* lockup, wq/aer/h */
 
 	sprintf(h->devname, HPSA "%d", number_of_controllers);
 	h->ctlr = number_of_controllers;
@@ -6917,32 +8514,40 @@ reinit_after_soft_reset:
 			dac = 0;
 		} else {
 			dev_err(&pdev->dev, "no suitable DMA available\n");
-			goto clean1;
+			goto clean3;	/* pci, lockup, wq/aer/h */
 		}
 	}
 
 	/* make sure the board interrupts are off */
 	h->access.set_intr_mask(h, HPSA_INTR_OFF);
 
-	if (hpsa_request_irqs(h, do_hpsa_intr_msi, do_hpsa_intr_intx))
-		goto clean2;
+	rc = hpsa_request_irqs(h, do_hpsa_intr_msi, do_hpsa_intr_intx);
+	if (rc)
+		goto clean3;	/* pci, lockup, wq/aer/h */
 	dev_info(&pdev->dev, "%s: <0x%x> at IRQ %d%s using DAC\n",
 	       h->devname, pdev->device,
 	       h->intr[h->intr_mode], dac ? "" : " not");
-	rc = hpsa_allocate_cmd_pool(h);
+	rc = hpsa_alloc_cmd_pool(h);
+	if (rc)
+		goto clean4;	/* irq, pci, lockup, wq/aer/h */
+	rc = hpsa_alloc_sg_chain_blocks(h);
 	if (rc)
-		goto clean2_and_free_irqs;
-	if (hpsa_allocate_sg_chain_blocks(h))
-		goto clean4;
+		goto clean5;	/* cmd, irq, pci, lockup, wq/aer/h */
 	init_waitqueue_head(&h->scan_wait_queue);
+	init_waitqueue_head(&h->abort_cmd_wait_queue);
+	init_waitqueue_head(&h->event_sync_wait_queue);
+	mutex_init(&h->reset_mutex);
 	h->scan_finished = 1; /* no scan currently in progress */
 
 	pci_set_drvdata(pdev, h);
 	h->ndevices = 0;
 	h->hba_mode_enabled = 0;
+	h->disk_rq_timeout = 0;
 	h->scsi_host = NULL;
 	spin_lock_init(&h->devlock);
-	hpsa_put_ctlr_into_performant_mode(h);
+	rc = hpsa_put_ctlr_into_performant_mode(h);
+	if (rc)
+		goto clean6;	/* sg, cmd, irq, pci, lockup, wq/aer/h */
 
 	/* At this point, the controller is ready to take commands.
 	 * Now, if reset_devices and the hard reset didn't work, try
@@ -6959,6 +8564,7 @@ reinit_after_soft_reset:
 		 */
 		spin_lock_irqsave(&h->lock, flags);
 		h->access.set_intr_mask(h, HPSA_INTR_OFF);
+		cancel_delayed_work(&h->rescan_ctlr_work);
 		spin_unlock_irqrestore(&h->lock, flags);
 		hpsa_free_irqs(h);
 		rc = hpsa_request_irqs(h, hpsa_msix_discard_completions,
@@ -6999,49 +8605,58 @@ reinit_after_soft_reset:
 		goto reinit_after_soft_reset;
 	}
 
-		/* Enable Accelerated IO path at driver layer */
-		h->acciopath_status = 1;
+	/* Enable Accelerated IO path at driver layer */
+	h->acciopath_status = 1;
 
+	h->lockup_detector_enabled = 1;
 
 	/* Turn the interrupts on so we can service requests */
 	h->access.set_intr_mask(h, HPSA_INTR_ON);
 
 	hpsa_hba_inquiry(h);
-	hpsa_register_scsi(h);	/* hook ourselves into SCSI subsystem */
+	rc = hpsa_register_scsi(h);	/* hook ourselves into SCSI subsystem */
+	if (rc)
+		goto clean7;
 
 	/* Monitor the controller for firmware lockups */
 	h->heartbeat_sample_interval = HEARTBEAT_SAMPLE_INTERVAL;
 	INIT_DELAYED_WORK(&h->monitor_ctlr_work, hpsa_monitor_ctlr_worker);
 	schedule_delayed_work(&h->monitor_ctlr_work,
-				h->heartbeat_sample_interval);
+			h->heartbeat_sample_interval);
 	INIT_DELAYED_WORK(&h->rescan_ctlr_work, hpsa_rescan_ctlr_worker);
-	queue_delayed_work(h->rescan_ctlr_wq, &h->rescan_ctlr_work,
-				h->heartbeat_sample_interval);
+	schedule_delayed_work(&h->rescan_ctlr_work,
+			h->heartbeat_sample_interval);
 	return 0;
 
-clean4:
-	hpsa_free_sg_chain_blocks(h);
+clean7: /* perf, sg, cmd, irq, pci, lockup, wq/aer/h */
+	hpsa_free_performant_mode(h);
+clean6: /* sg, cmd, irq, pci, lockup, wq/aer/h */	hpsa_free_sg_chain_blocks(h);
+clean5: /* cmd, irq, pci, lockup, wq/aer/h */
 	hpsa_free_cmd_pool(h);
-clean2_and_free_irqs:
+clean4: /* irq, pci, lockup, wq/aer/h */
 	hpsa_free_irqs(h);
-clean2:
-clean1:
-	if (h->resubmit_wq)
-		destroy_workqueue(h->resubmit_wq);
-	if (h->rescan_ctlr_wq)
-		destroy_workqueue(h->rescan_ctlr_wq);
+clean3: /* pci, lockup, wq/aer/h */
+	hpsa_free_pci_init(h);
+clean2: /* lockup, wq/aer/h */
 	if (h->lockup_detected)
 		free_percpu(h->lockup_detected);
+clean1:	/* wq/aer/h */
+	if (h->resubmit_wq)
+		destroy_workqueue(h->resubmit_wq);
+	/* (void) pci_disable_pcie_error_reporting(pdev); */
 	kfree(h);
 	return rc;
 }
+EXPORT_SYMBOL(hpsa_init_one);
 
 static void hpsa_flush_cache(struct ctlr_info *h)
 {
 	char *flush_buf;
 	struct CommandList *c;
+	int rc;
 
 	/* Don't bother trying to flush the cache if locked up */
+	/* FIXME not necessary if do_simple_cmd does the check */
 	if (unlikely(lockup_detected(h)))
 		return;
 	flush_buf = kzalloc(4, GFP_KERNEL);
@@ -7049,21 +8664,20 @@ static void hpsa_flush_cache(struct ctlr_info *h)
 		return;
 
 	c = cmd_alloc(h);
-	if (!c) {
-		dev_warn(&h->pdev->dev, "cmd_alloc returned NULL!\n");
-		goto out_of_memory;
-	}
+
 	if (fill_cmd(c, HPSA_CACHE_FLUSH, h, flush_buf, 4, 0,
 		RAID_CTLR_LUNID, TYPE_CMD)) {
 		goto out;
 	}
-	hpsa_scsi_do_simple_cmd_with_retry(h, c, PCI_DMA_TODEVICE);
+	rc = hpsa_scsi_do_simple_cmd_with_retry(h, c,
+					PCI_DMA_TODEVICE, NO_TIMEOUT);
+	if (rc)
+		goto out;
 	if (c->err_info->CommandStatus != 0)
 out:
 		dev_warn(&h->pdev->dev,
 			"error flushing cache on controller\n");
 	cmd_free(h, c);
-out_of_memory:
 	kfree(flush_buf);
 }
 
@@ -7078,10 +8692,11 @@ static void hpsa_shutdown(struct pci_dev *pdev)
 	 */
 	hpsa_flush_cache(h);
 	h->access.set_intr_mask(h, HPSA_INTR_OFF);
-	hpsa_free_irqs_and_disable_msix(h);
+	hpsa_free_irqs(h);			/* init_one 4 */
+	hpsa_disable_interrupt_mode(h);		/* pci_init 2 */
 }
 
-static void hpsa_free_device_info(struct ctlr_info *h)
+static void __devexit hpsa_free_device_info(struct ctlr_info *h)
 {
 	int i;
 
@@ -7089,7 +8704,7 @@ static void hpsa_free_device_info(struct ctlr_info *h)
 		kfree(h->dev[i]);
 }
 
-static void hpsa_remove_one(struct pci_dev *pdev)
+static void __devexit hpsa_remove_one(struct pci_dev *pdev)
 {
 	struct ctlr_info *h;
 	unsigned long flags;
@@ -7103,34 +8718,33 @@ static void hpsa_remove_one(struct pci_dev *pdev)
 	/* Get rid of any controller monitoring work items */
 	spin_lock_irqsave(&h->lock, flags);
 	h->remove_in_progress = 1;
-	spin_unlock_irqrestore(&h->lock, flags);
 	cancel_delayed_work_sync(&h->monitor_ctlr_work);
 	cancel_delayed_work_sync(&h->rescan_ctlr_work);
-	destroy_workqueue(h->rescan_ctlr_wq);
-	destroy_workqueue(h->resubmit_wq);
-	hpsa_unregister_scsi(h);	/* unhook from SCSI subsystem */
+	spin_unlock_irqrestore(&h->lock, flags);
+
+	/* includes hpsa_free_irqs - init_one 4 */
+	/* includes hpsa_disable_interrupt_mode - pci_init 2 */
 	hpsa_shutdown(pdev);
-	iounmap(h->vaddr);
-	iounmap(h->transtable);
-	iounmap(h->cfgtable);
-	hpsa_free_device_info(h);
-	hpsa_free_sg_chain_blocks(h);
-	pci_free_consistent(h->pdev,
-		h->nr_cmds * sizeof(struct CommandList),
-		h->cmd_pool, h->cmd_pool_dhandle);
-	pci_free_consistent(h->pdev,
-		h->nr_cmds * sizeof(struct ErrorInfo),
-		h->errinfo_pool, h->errinfo_pool_dhandle);
-	hpsa_free_reply_queues(h);
-	kfree(h->cmd_pool_bits);
-	kfree(h->blockFetchTable);
-	kfree(h->ioaccel1_blockFetchTable);
-	kfree(h->ioaccel2_blockFetchTable);
-	kfree(h->hba_inquiry_data);
-	pci_disable_device(pdev);
-	pci_release_regions(pdev);
-	free_percpu(h->lockup_detected);
-	kfree(h);
+
+	hpsa_free_device_info(h);		/* scan */
+
+	hpsa_unregister_scsi(h);			/* init_one "8" */
+	hpsa_free_ioaccel2_sg_chain_blocks(h);
+	kfree(h->hba_inquiry_data);			/* init_one "8" */
+	hpsa_free_performant_mode(h);			/* init_one 7 */
+	hpsa_free_sg_chain_blocks(h);			/* init_one 6 */
+	hpsa_free_cmd_pool(h);				/* init_one 5 */
+
+	/* hpsa_free_irqs already called via hpsa_shutdown init_one 4 */
+
+	/* includes hpsa_disable_interrupt_mode - pci_init 2 */
+	hpsa_free_pci_init(h);				/* init_one 3 */
+
+	free_percpu(h->lockup_detected);		/* init_one 2 */
+	if (h->resubmit_wq)
+		destroy_workqueue(h->resubmit_wq);	/* init_one 1 */
+	/* (void) pci_disable_pcie_error_reporting(pdev); */	/* init_one 1 */
+	kfree(h);					/* init_one 1 */
 }
 
 static int hpsa_suspend(__attribute__((unused)) struct pci_dev *pdev,
@@ -7147,7 +8761,7 @@ static int hpsa_resume(__attribute__((unused)) struct pci_dev *pdev)
 static struct pci_driver hpsa_pci_driver = {
 	.name = HPSA,
 	.probe = hpsa_init_one,
-	.remove = hpsa_remove_one,
+	.remove = __devexit_p(hpsa_remove_one),
 	.id_table = hpsa_pci_device_id,	/* id_table */
 	.shutdown = hpsa_shutdown,
 	.suspend = hpsa_suspend,
@@ -7167,7 +8781,7 @@ static struct pci_driver hpsa_pci_driver = {
  * bits of the command address.
  */
 static void  calc_bucket_map(int bucket[], int num_buckets,
-	int nsgs, int min_blocks, u32 *bucket_map)
+	int nsgs, int min_blocks, int *bucket_map)
 {
 	int i, j, b, size;
 
@@ -7189,7 +8803,8 @@ static void  calc_bucket_map(int bucket[], int num_buckets,
 }
 
 /* return -ENODEV or other reason on error, 0 on success */
-static int hpsa_enter_performant_mode(struct ctlr_info *h, u32 trans_support)
+static int hpsa_enter_performant_mode(struct ctlr_info *h,
+	u32 trans_support)
 {
 	int i;
 	unsigned long register_value;
@@ -7220,8 +8835,8 @@ static int hpsa_enter_performant_mode(struct ctlr_info *h, u32 trans_support)
 	int bft[8] = {5, 6, 8, 10, 12, 20, 28, SG_ENTRIES_IN_CMD + 4};
 #define MIN_IOACCEL2_BFT_ENTRY 5
 #define HPSA_IOACCEL2_HEADER_SZ 4
-	int bft2[16] = {MIN_IOACCEL2_BFT_ENTRY, 6, 7, 8, 9, 10, 11, 12,
-			13, 14, 15, 16, 17, 18, 19,
+	int bft2[16] = {MIN_IOACCEL2_BFT_ENTRY, 6, 7, 8, 9, 10, 11, 12, 
+			13, 14, 15, 16, 17, 18, 19, 
 			HPSA_IOACCEL2_HEADER_SZ + IOACCEL2_MAXSGENTRIES};
 	BUILD_BUG_ON(ARRAY_SIZE(bft2) != 16);
 	BUILD_BUG_ON(ARRAY_SIZE(bft) != 8);
@@ -7237,7 +8852,7 @@ static int hpsa_enter_performant_mode(struct ctlr_info *h, u32 trans_support)
 
 	/* If the controller supports either ioaccel method then
 	 * we can also use the RAID stack submit path that does not
-	 * perform the superfluous readl() after each command submission.
+	 * the superfluous readl() after each command submission.
 	 */
 	if (trans_support & (CFGTBL_Trans_io_accel1 | CFGTBL_Trans_io_accel2))
 		access = SA5_performant_access_no_read;
@@ -7286,6 +8901,7 @@ static int hpsa_enter_performant_mode(struct ctlr_info *h, u32 trans_support)
 			"performant mode problem - doorbell timeout\n");
 		return -ENODEV;
 	}
+	print_cfg_table(&h->pdev->dev, h->cfgtable);
 	register_value = readl(&(h->cfgtable->TransportActive));
 	if (!(register_value & CFGTBL_Trans_Performant)) {
 		dev_err(&h->pdev->dev,
@@ -7328,15 +8944,14 @@ static int hpsa_enter_performant_mode(struct ctlr_info *h, u32 trans_support)
 					(i * sizeof(struct ErrorInfo)));
 			cp->err_info_len = sizeof(struct ErrorInfo);
 			cp->sgl_offset = IOACCEL1_SGLOFFSET;
-			cp->host_context_flags =
-				cpu_to_le16(IOACCEL1_HCFLAGS_CISS_FORMAT);
+			cp->host_context_flags = IOACCEL1_HCFLAGS_CISS_FORMAT;
 			cp->timeout_sec = 0;
 			cp->ReplyQueue = 0;
 			cp->tag =
-				cpu_to_le64((i << DIRECT_LOOKUP_SHIFT));
+				cpu_to_le64((u64) (i << DIRECT_LOOKUP_SHIFT));
 			cp->host_addr =
-				cpu_to_le64(h->ioaccel_cmd_pool_dhandle +
-					(i * sizeof(struct io_accel1_cmd)));
+				cpu_to_le64((u64) (h->ioaccel_cmd_pool_dhandle +
+					(i * sizeof(struct io_accel1_cmd))));
 		}
 	} else if (trans_support & CFGTBL_Trans_io_accel2) {
 		u64 cfg_offset, cfg_base_addr_index;
@@ -7350,14 +8965,13 @@ static int hpsa_enter_performant_mode(struct ctlr_info *h, u32 trans_support)
 		calc_bucket_map(bft2, ARRAY_SIZE(bft2), h->ioaccel_maxsg,
 				4, h->ioaccel2_blockFetchTable);
 		bft2_offset = readl(&h->cfgtable->io_accel_request_size_offset);
-		BUILD_BUG_ON(offsetof(struct CfgTable,
-				io_accel_request_size_offset) != 0xb8);
+		BUILD_BUG_ON(offsetof(struct CfgTable, io_accel_request_size_offset)
+			!= 0xb8); 
+		printk(KERN_WARNING "bft2_offset = 0x%x\n", bft2_offset);
 		h->ioaccel2_bft2_regs =
-			remap_pci_mem(pci_resource_start(h->pdev,
-					cfg_base_addr_index) +
-					cfg_offset + bft2_offset,
-					ARRAY_SIZE(bft2) *
-					sizeof(*h->ioaccel2_bft2_regs));
+			remap_pci_mem(pci_resource_start(h->pdev, cfg_base_addr_index) + 
+					cfg_offset +
+					bft2_offset, ARRAY_SIZE(bft2) * sizeof(*h->ioaccel2_bft2_regs));
 		for (i = 0; i < ARRAY_SIZE(bft2); i++)
 			writel(bft2[i], &h->ioaccel2_bft2_regs[i]);
 	}
@@ -7370,7 +8984,18 @@ static int hpsa_enter_performant_mode(struct ctlr_info *h, u32 trans_support)
 	return 0;
 }
 
-static int hpsa_alloc_ioaccel_cmd_and_bft(struct ctlr_info *h)
+/* Free ioaccel1 mode command blocks and block fetch table */
+static void hpsa_free_ioaccel1_cmd_and_bft(struct ctlr_info *h)
+{
+	if (h->ioaccel_cmd_pool)
+		pci_free_consistent(h->pdev,
+			h->nr_cmds * sizeof(*h->ioaccel_cmd_pool),
+			h->ioaccel_cmd_pool, h->ioaccel_cmd_pool_dhandle);
+	kfree(h->ioaccel1_blockFetchTable);
+}
+
+/* Allocate ioaccel1 mode command blocks and block fetch table */
+static int hpsa_alloc_ioaccel1_cmd_and_bft(struct ctlr_info *h)
 {
 	h->ioaccel_maxsg =
 		readl(&(h->cfgtable->io_accel_max_embedded_sg_count));
@@ -7383,6 +9008,7 @@ static int hpsa_alloc_ioaccel_cmd_and_bft(struct ctlr_info *h)
 	 */
 	BUILD_BUG_ON(sizeof(struct io_accel1_cmd) %
 			IOACCEL1_COMMANDLIST_ALIGNMENT);
+
 	h->ioaccel_cmd_pool =
 		pci_alloc_consistent(h->pdev,
 			h->nr_cmds * sizeof(*h->ioaccel_cmd_pool),
@@ -7401,15 +9027,22 @@ static int hpsa_alloc_ioaccel_cmd_and_bft(struct ctlr_info *h)
 	return 0;
 
 clean_up:
-	if (h->ioaccel_cmd_pool)
+	hpsa_free_ioaccel1_cmd_and_bft(h);
+	return -ENOMEM;
+}
+
+/* Free ioaccel2 mode command blocks and block fetch table */
+static void hpsa_free_ioaccel2_cmd_and_bft(struct ctlr_info *h)
+{
+	if (h->ioaccel2_cmd_pool)
 		pci_free_consistent(h->pdev,
-			h->nr_cmds * sizeof(*h->ioaccel_cmd_pool),
-			h->ioaccel_cmd_pool, h->ioaccel_cmd_pool_dhandle);
-	kfree(h->ioaccel1_blockFetchTable);
-	return 1;
+			h->nr_cmds * sizeof(*h->ioaccel2_cmd_pool),
+			h->ioaccel2_cmd_pool, h->ioaccel2_cmd_pool_dhandle);
+	kfree(h->ioaccel2_blockFetchTable);
 }
 
-static int ioaccel2_alloc_cmds_and_bft(struct ctlr_info *h)
+/* Allocate ioaccel2 mode command blocks and block fetch table */
+static int hpsa_alloc_ioaccel2_cmd_and_bft(struct ctlr_info *h)
 {
 	/* Allocate ioaccel2 mode command blocks and block fetch table */
 
@@ -7433,49 +9066,59 @@ static int ioaccel2_alloc_cmds_and_bft(struct ctlr_info *h)
 		(h->ioaccel2_blockFetchTable == NULL))
 		goto clean_up;
 
+	if (hpsa_allocate_ioaccel2_sg_chain_blocks(h))
+		goto clean_up;
+
 	memset(h->ioaccel2_cmd_pool, 0,
 		h->nr_cmds * sizeof(*h->ioaccel2_cmd_pool));
 	return 0;
 
 clean_up:
-	if (h->ioaccel2_cmd_pool)
-		pci_free_consistent(h->pdev,
-			h->nr_cmds * sizeof(*h->ioaccel2_cmd_pool),
-			h->ioaccel2_cmd_pool, h->ioaccel2_cmd_pool_dhandle);
-	kfree(h->ioaccel2_blockFetchTable);
-	return 1;
+	hpsa_free_ioaccel2_cmd_and_bft(h);
+	return -ENOMEM;
+}
+
+/* Free items allocated by hpsa_put_ctlr_into_performant_mode */
+static void hpsa_free_performant_mode(struct ctlr_info *h)
+{
+	kfree(h->blockFetchTable);
+	hpsa_free_reply_queues(h);
+	hpsa_free_ioaccel1_cmd_and_bft(h);
+	hpsa_free_ioaccel2_cmd_and_bft(h);
 }
 
-static void hpsa_put_ctlr_into_performant_mode(struct ctlr_info *h)
+/* return -ENODEV on error, 0 on success (or no action)
+ * allocates numerous items that must be freed later
+ */
+static __devinit int hpsa_put_ctlr_into_performant_mode(struct ctlr_info *h)
 {
 	u32 trans_support;
 	unsigned long transMethod = CFGTBL_Trans_Performant |
 					CFGTBL_Trans_use_short_tags;
-	int i;
+	int i, rc;
 
-	if (hpsa_simple_mode)
-		return;
+	if (h->intr_mode == SIMPLE_MODE_INT)
+		return 0;
 
 	trans_support = readl(&(h->cfgtable->TransportSupport));
 	if (!(trans_support & PERFORMANT_MODE))
-		return;
+		return 0;
 
 	/* Check for I/O accelerator mode support */
 	if (trans_support & CFGTBL_Trans_io_accel1) {
 		transMethod |= CFGTBL_Trans_io_accel1 |
 				CFGTBL_Trans_enable_directed_msix;
-		if (hpsa_alloc_ioaccel_cmd_and_bft(h))
-			goto clean_up;
-	} else {
-		if (trans_support & CFGTBL_Trans_io_accel2) {
-				transMethod |= CFGTBL_Trans_io_accel2 |
+		rc = hpsa_alloc_ioaccel1_cmd_and_bft(h);
+		if (rc)
+			return rc;
+	} else if (trans_support & CFGTBL_Trans_io_accel2) {
+		transMethod |= CFGTBL_Trans_io_accel2 |
 				CFGTBL_Trans_enable_directed_msix;
-		if (ioaccel2_alloc_cmds_and_bft(h))
-			goto clean_up;
-		}
+		rc = hpsa_alloc_ioaccel2_cmd_and_bft(h);
+		if (rc)
+			return rc;
 	}
 
-	h->nreply_queues = h->msix_vector > 0 ? h->msix_vector : 1;
 	hpsa_get_max_perf_mode_cmds(h);
 	/* Performant mode ring buffer and supporting data structures */
 	h->reply_queue_size = h->max_commands * sizeof(u64);
@@ -7484,8 +9127,10 @@ static void hpsa_put_ctlr_into_performant_mode(struct ctlr_info *h)
 		h->reply_queue[i].head = pci_alloc_consistent(h->pdev,
 						h->reply_queue_size,
 						&(h->reply_queue[i].busaddr));
-		if (!h->reply_queue[i].head)
-			goto clean_up;
+		if (!h->reply_queue[i].head) {
+			rc = -ENOMEM;
+			goto clean1;	/* rq, ioaccel */
+		}
 		h->reply_queue[i].size = h->max_commands;
 		h->reply_queue[i].wraparound = 1;  /* spec: init to 1 */
 		h->reply_queue[i].current_entry = 0;
@@ -7494,20 +9139,29 @@ static void hpsa_put_ctlr_into_performant_mode(struct ctlr_info *h)
 	/* Need a block fetch table for performant mode */
 	h->blockFetchTable = kmalloc(((SG_ENTRIES_IN_CMD + 1) *
 				sizeof(u32)), GFP_KERNEL);
-	if (!h->blockFetchTable)
-		goto clean_up;
+	if (!h->blockFetchTable) {
+		rc = -ENOMEM;
+		goto clean1;	/* rq, ioaccel */
+	}
 
-	hpsa_enter_performant_mode(h, trans_support);
-	return;
+	rc = hpsa_enter_performant_mode(h, trans_support);
+	if (rc)
+		goto clean2;	/* bft, rq, ioaccel */
+	return 0;
 
-clean_up:
-	hpsa_free_reply_queues(h);
+clean2:	/* bft, rq, ioaccel */
 	kfree(h->blockFetchTable);
+clean1:	/* rq, ioaccel */
+	hpsa_free_reply_queues(h);
+	hpsa_free_ioaccel1_cmd_and_bft(h);
+	hpsa_free_ioaccel2_cmd_and_bft(h);
+	return rc;
 }
 
 static int is_accelerated_cmd(struct CommandList *c)
 {
-	return c->cmd_type == CMD_IOACCEL1 || c->cmd_type == CMD_IOACCEL2;
+	return (c->cmd_type == CMD_IOACCEL1 ||
+		c->cmd_type == CMD_IOACCEL2);
 }
 
 static void hpsa_drain_accel_commands(struct ctlr_info *h)
@@ -7545,83 +9199,5 @@ static void __exit hpsa_cleanup(void)
 	pci_unregister_driver(&hpsa_pci_driver);
 }
 
-static void __attribute__((unused)) verify_offsets(void)
-{
-#define VERIFY_OFFSET(member, offset) \
-	BUILD_BUG_ON(offsetof(struct raid_map_data, member) != offset)
-
-	VERIFY_OFFSET(structure_size, 0);
-	VERIFY_OFFSET(volume_blk_size, 4);
-	VERIFY_OFFSET(volume_blk_cnt, 8);
-	VERIFY_OFFSET(phys_blk_shift, 16);
-	VERIFY_OFFSET(parity_rotation_shift, 17);
-	VERIFY_OFFSET(strip_size, 18);
-	VERIFY_OFFSET(disk_starting_blk, 20);
-	VERIFY_OFFSET(disk_blk_cnt, 28);
-	VERIFY_OFFSET(data_disks_per_row, 36);
-	VERIFY_OFFSET(metadata_disks_per_row, 38);
-	VERIFY_OFFSET(row_cnt, 40);
-	VERIFY_OFFSET(layout_map_count, 42);
-	VERIFY_OFFSET(flags, 44);
-	VERIFY_OFFSET(dekindex, 46);
-	/* VERIFY_OFFSET(reserved, 48 */
-	VERIFY_OFFSET(data, 64);
-
-#undef VERIFY_OFFSET
-
-#define VERIFY_OFFSET(member, offset) \
-	BUILD_BUG_ON(offsetof(struct io_accel2_cmd, member) != offset)
-
-	VERIFY_OFFSET(IU_type, 0);
-	VERIFY_OFFSET(direction, 1);
-	VERIFY_OFFSET(reply_queue, 2);
-	/* VERIFY_OFFSET(reserved1, 3);  */
-	VERIFY_OFFSET(scsi_nexus, 4);
-	VERIFY_OFFSET(Tag, 8);
-	VERIFY_OFFSET(cdb, 16);
-	VERIFY_OFFSET(cciss_lun, 32);
-	VERIFY_OFFSET(data_len, 40);
-	VERIFY_OFFSET(cmd_priority_task_attr, 44);
-	VERIFY_OFFSET(sg_count, 45);
-	/* VERIFY_OFFSET(reserved3 */
-	VERIFY_OFFSET(err_ptr, 48);
-	VERIFY_OFFSET(err_len, 56);
-	/* VERIFY_OFFSET(reserved4  */
-	VERIFY_OFFSET(sg, 64);
-
-#undef VERIFY_OFFSET
-
-#define VERIFY_OFFSET(member, offset) \
-	BUILD_BUG_ON(offsetof(struct io_accel1_cmd, member) != offset)
-
-	VERIFY_OFFSET(dev_handle, 0x00);
-	VERIFY_OFFSET(reserved1, 0x02);
-	VERIFY_OFFSET(function, 0x03);
-	VERIFY_OFFSET(reserved2, 0x04);
-	VERIFY_OFFSET(err_info, 0x0C);
-	VERIFY_OFFSET(reserved3, 0x10);
-	VERIFY_OFFSET(err_info_len, 0x12);
-	VERIFY_OFFSET(reserved4, 0x13);
-	VERIFY_OFFSET(sgl_offset, 0x14);
-	VERIFY_OFFSET(reserved5, 0x15);
-	VERIFY_OFFSET(transfer_len, 0x1C);
-	VERIFY_OFFSET(reserved6, 0x20);
-	VERIFY_OFFSET(io_flags, 0x24);
-	VERIFY_OFFSET(reserved7, 0x26);
-	VERIFY_OFFSET(LUN, 0x34);
-	VERIFY_OFFSET(control, 0x3C);
-	VERIFY_OFFSET(CDB, 0x40);
-	VERIFY_OFFSET(reserved8, 0x50);
-	VERIFY_OFFSET(host_context_flags, 0x60);
-	VERIFY_OFFSET(timeout_sec, 0x62);
-	VERIFY_OFFSET(ReplyQueue, 0x64);
-	VERIFY_OFFSET(reserved9, 0x65);
-	VERIFY_OFFSET(tag, 0x68);
-	VERIFY_OFFSET(host_addr, 0x70);
-	VERIFY_OFFSET(CISS_LUN, 0x78);
-	VERIFY_OFFSET(SG, 0x78 + 8);
-#undef VERIFY_OFFSET
-}
-
 module_init(hpsa_init);
 module_exit(hpsa_cleanup);
diff --git a/drivers/scsi/hpsa.h b/drivers/scsi/hpsa.h
index 6577130..5724342 100644
--- a/drivers/scsi/hpsa.h
+++ b/drivers/scsi/hpsa.h
@@ -1,6 +1,7 @@
 /*
  *    Disk Array driver for HP Smart Array SAS controllers
- *    Copyright 2000, 2014 Hewlett-Packard Development Company, L.P.
+ *    Copyright 2014-2015 PMC-Sierra, Inc.
+ *    Copyright 2000,2009-2015 Hewlett-Packard Development Company, L.P.
  *
  *    This program is free software; you can redistribute it and/or modify
  *    it under the terms of the GNU General Public License as published by
@@ -15,7 +16,7 @@
  *    along with this program; if not, write to the Free Software
  *    Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
  *
- *    Questions/Comments/Bugfixes to iss_storagedev@hp.com
+ *    Questions/Comments/Bugfixes to storagedev@pmcs.com
  *
  */
 #ifndef HPSA_H
@@ -32,7 +33,7 @@ struct access_method {
 	void (*submit_command)(struct ctlr_info *h,
 		struct CommandList *c);
 	void (*set_intr_mask)(struct ctlr_info *h, unsigned long val);
-	bool (*intr_pending)(struct ctlr_info *h);
+	int (*intr_pending)(struct ctlr_info *h);
 	unsigned long (*command_completed)(struct ctlr_info *h, u8 q);
 };
 
@@ -47,20 +48,30 @@ struct hpsa_scsi_dev_t {
 	unsigned char raid_level;	/* from inquiry page 0xC1 */
 	unsigned char volume_offline;	/* discovered via TUR or VPD */
 	u16 queue_depth;		/* max queue_depth for this device */
-	atomic_t ioaccel_cmds_out;	/* Only used for physical devices
-					 * counts commands sent to physical
-					 * device via "ioaccel" path.
-					 */
+	atomic_t reset_cmds_out;	/* Count of commands to-be affected */
+	atomic_t ioaccel_cmds_out;
 	u32 ioaccel_handle;
+	u8 active_path_index;
+	u8 path_map;
+	u8 bay;
+	u8 box[8];
+	u16 phys_connector[8];
 	int offload_config;		/* I/O accel RAID offload configured */
 	int offload_enabled;		/* I/O accel RAID offload enabled */
+	int offload_to_be_enabled;
+	int hba_ioaccel_enabled;
 	int offload_to_mirror;		/* Send next I/O accelerator RAID
 					 * offload request to mirror drive
 					 */
 	struct raid_map_data raid_map;	/* I/O accelerator RAID map */
-
-	/*
-	 * Pointers from logical drive map indices to the phys drives that
+	int supports_aborts;
+#define HPSA_NO_ULD_ATTACH	0x1
+#define HPSA_DO_NOT_EXPOSE	0x2
+#define HPSA_EXPOSE		0x4
+#define HPSA_SCSI_ADD		(HPSA_NO_ULD_ATTACH | HPSA_EXPOSE)
+	u8 expose_state;
+
+	/* Pointers from logical drive map indices to the phys drives that
 	 * make those logical drives.  Note, multiple logical drives may
 	 * share physical drives.  You can have for instance 5 physical
 	 * drives with 3 logical drives each using those same 5 physical
@@ -68,6 +79,8 @@ struct hpsa_scsi_dev_t {
 	 * devices in order to honor physical device queue depth limits.
 	 */
 	struct hpsa_scsi_dev_t *phys_disk[RAID_MAP_MAX_ENTRIES];
+	int nphysical_disks;
+	int timeout; /* specify command timeout per dev type, at least */
 };
 
 struct reply_queue_buffer {
@@ -154,6 +167,7 @@ struct ctlr_info {
 	u8 max_cmd_sg_entries;
 	int chainsize;
 	struct SGDescriptor **cmd_sg_list;
+	struct ioaccel2_sg_element **ioaccel2_cmd_sg_list;
 
 	/* pointers to command and error info pool */
 	struct CommandList 	*cmd_pool;
@@ -178,7 +192,7 @@ struct ctlr_info {
 	 */
 	u32 trans_support;
 	u32 trans_offset;
-	struct TransTable_struct __iomem *transtable;
+	struct TransTable_struct *transtable;
 	unsigned long transMethod;
 
 	/* cap concurrent passthrus at some reasonable maximum */
@@ -194,7 +208,7 @@ struct ctlr_info {
 	u32 *blockFetchTable;
 	u32 *ioaccel1_blockFetchTable;
 	u32 *ioaccel2_blockFetchTable;
-	u32 __iomem *ioaccel2_bft2_regs;
+	u32 *ioaccel2_bft2_regs;
 	unsigned char *hba_inquiry_data;
 	u32 driver_support;
 	u32 fw_support;
@@ -205,10 +219,12 @@ struct ctlr_info {
 	u64 last_heartbeat_timestamp;
 	u32 heartbeat_sample_interval;
 	atomic_t firmware_flash_in_progress;
-	u32 __percpu *lockup_detected;
+	u32 *lockup_detected;
 	struct delayed_work monitor_ctlr_work;
 	struct delayed_work rescan_ctlr_work;
 	int remove_in_progress;
+	struct list_head scan_list;
+	u32 fifo_recently_full;
 	/* Address of h->q[x] is passed to intr handler to know which queue */
 	u8 q[MAX_REPLY_QUEUES];
 	u32 TMFSupportFlags; /* cache what task mgmt funcs are supported. */
@@ -222,6 +238,7 @@ struct ctlr_info {
 #define HPSATMF_PHYS_QRY_TASK   (1 << 7)
 #define HPSATMF_PHYS_QRY_TSET   (1 << 8)
 #define HPSATMF_PHYS_QRY_ASYNC  (1 << 9)
+#define HPSATMF_IOACCEL_ENABLED (1 << 15)
 #define HPSATMF_MASK_SUPPORTED  (1 << 16)
 #define HPSATMF_LOG_LUN_RESET   (1 << 17)
 #define HPSATMF_LOG_NEX_RESET   (1 << 18)
@@ -247,12 +264,20 @@ struct ctlr_info {
 		CTLR_STATE_CHANGE_EVENT_LOGICAL_DRV | \
 		CTLR_STATE_CHANGE_EVENT_AIO_ENABLED_DISABLED | \
 		CTLR_STATE_CHANGE_EVENT_AIO_CONFIG_CHANGE)
+
 	spinlock_t offline_device_lock;
 	struct list_head offline_device_list;
-	int	acciopath_status;
-	int	raid_offload_debug;
+	int	raid_offload_debug;	
+	int	acciopath_status;	
+	int 	drv_req_rescan;
+	int	lockup_detector_enabled;
+	int	needs_abort_tags_swizzled;
 	struct workqueue_struct *resubmit_wq;
-	struct workqueue_struct *rescan_ctlr_wq;
+	atomic_t abort_cmds_available;
+	wait_queue_head_t abort_cmd_wait_queue;
+	wait_queue_head_t event_sync_wait_queue;
+	struct mutex reset_mutex;
+	int	disk_rq_timeout;
 };
 
 struct offline_device_entry {
@@ -292,7 +317,7 @@ struct offline_device_entry {
  * HPSA_BOARD_READY_ITERATIONS are derived from those.
  */
 #define HPSA_BOARD_READY_WAIT_SECS (120)
-#define HPSA_BOARD_NOT_READY_WAIT_SECS (100)
+#define HPSA_BOARD_NOT_READY_WAIT_SECS (120)
 #define HPSA_BOARD_READY_POLL_INTERVAL_MSECS (100)
 #define HPSA_BOARD_READY_POLL_INTERVAL \
 	((HPSA_BOARD_READY_POLL_INTERVAL_MSECS * HZ) / 1000)
@@ -311,8 +336,6 @@ struct offline_device_entry {
  */
 #define SA5_DOORBELL	0x20
 #define SA5_REQUEST_PORT_OFFSET	0x40
-#define SA5_REQUEST_PORT64_LO_OFFSET 0xC0
-#define SA5_REQUEST_PORT64_HI_OFFSET 0xC4
 #define SA5_REPLY_INTR_MASK_OFFSET	0x34
 #define SA5_REPLY_PORT_OFFSET		0x44
 #define SA5_INTR_STATUS		0x30
@@ -461,22 +484,23 @@ static unsigned long SA5_completed(struct ctlr_info *h,
 
 	return register_value;
 }
+
 /*
  *	Returns true if an interrupt is pending..
  */
-static bool SA5_intr_pending(struct ctlr_info *h)
+static int SA5_intr_pending(struct ctlr_info *h)
 {
 	unsigned long register_value  =
 		readl(h->vaddr + SA5_INTR_STATUS);
 	return register_value & SA5_INTR_PENDING;
 }
 
-static bool SA5_performant_intr_pending(struct ctlr_info *h)
+static int SA5_performant_intr_pending(struct ctlr_info *h)
 {
 	unsigned long register_value = readl(h->vaddr + SA5_INTR_STATUS);
 
 	if (!register_value)
-		return false;
+		return 0;
 
 	/* Read outbound doorbell to flush */
 	register_value = readl(h->vaddr + SA5_OUTDB_STATUS);
@@ -485,7 +509,7 @@ static bool SA5_performant_intr_pending(struct ctlr_info *h)
 
 #define SA5_IOACCEL_MODE1_INTR_STATUS_CMP_BIT    0x100
 
-static bool SA5_ioaccel_mode1_intr_pending(struct ctlr_info *h)
+static int SA5_ioaccel_mode1_intr_pending(struct ctlr_info *h)
 {
 	unsigned long register_value = readl(h->vaddr + SA5_INTR_STATUS);
 
diff --git a/drivers/scsi/hpsa_cmd.h b/drivers/scsi/hpsa_cmd.h
index 3a621c7..4dbbbdb 100644
--- a/drivers/scsi/hpsa_cmd.h
+++ b/drivers/scsi/hpsa_cmd.h
@@ -1,6 +1,7 @@
 /*
  *    Disk Array driver for HP Smart Array SAS controllers
- *    Copyright 2000, 2014 Hewlett-Packard Development Company, L.P.
+ *    Copyright 2014-2015 PMC-Sierra, Inc.
+ *    Copyright 2000,2009-2015 Hewlett-Packard Development Company, L.P.
  *
  *    This program is free software; you can redistribute it and/or modify
  *    it under the terms of the GNU General Public License as published by
@@ -15,7 +16,7 @@
  *    along with this program; if not, write to the Free Software
  *    Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
  *
- *    Questions/Comments/Bugfixes to iss_storagedev@hp.com
+ *    Questions/Comments/Bugfixes to storagedev@pmcs.com
  *
  */
 #ifndef HPSA_CMD_H
@@ -42,8 +43,22 @@
 #define CMD_UNSOLICITED_ABORT   0x000A
 #define CMD_TIMEOUT             0x000B
 #define CMD_UNABORTABLE		0x000C
+#define CMD_TMF_STATUS		0x000D
 #define CMD_IOACCEL_DISABLED	0x000E
+#define CMD_CTLR_LOCKUP		0xffff
+/* Note: CMD_CTLR_LOCKUP is not a value defined by the CISS spec
+ * it is a value defined by the driver that commands can be marked
+ * with when a controller lockup has been detected by the driver
+ */
 
+/* TMF function status values */
+#define CISS_TMF_COMPLETE	0x00
+#define CISS_TMF_INVALID_FRAME	0x02
+#define CISS_TMF_NOT_SUPPORTED	0x04
+#define CISS_TMF_FAILED		0x05
+#define CISS_TMF_SUCCESS	0x08
+#define CISS_TMF_WRONG_LUN	0x09
+#define CISS_TMF_OVERLAPPED_TAG 0x0a
 
 /* Unit Attentions ASC's as defined for the MSA2012sa */
 #define POWER_OR_RESET			0x29
@@ -84,13 +99,15 @@
 /* cdb type */
 #define TYPE_CMD		0x00
 #define TYPE_MSG		0x01
-#define TYPE_IOACCEL2_CMD	0x81 /* 0x81 is not used by hardware */
+#define TYPE_IOACCEL2_CMD	0x81 /* 0x81 is not a value hardware uses */ 
 
 /* Message Types  */
 #define HPSA_TASK_MANAGEMENT    0x00
 #define HPSA_RESET              0x01
 #define HPSA_SCAN               0x02
 #define HPSA_NOOP               0x03
+#define HPSA_PHYS_TARGET_RESET 	0x99 /* not defined by cciss spec */
+
 
 #define HPSA_CTLR_RESET_TYPE    0x00
 #define HPSA_BUS_RESET_TYPE     0x01
@@ -144,15 +161,17 @@
 #define CFGTBL_BusType_Fibre2G  0x00000200l
 
 /* VPD Inquiry types */
-#define HPSA_VPD_SUPPORTED_PAGES        0x00
-#define HPSA_VPD_LV_DEVICE_GEOMETRY     0xC1
-#define HPSA_VPD_LV_IOACCEL_STATUS      0xC2
+#define HPSA_VPD_SUPPORTED_PAGES	0x00
+#define HPSA_VPD_LV_DEVICE_ID		0x83
+#define HPSA_VPD_PHYS_DEVICE_ID		0xC0
+#define HPSA_VPD_LV_DEVICE_GEOMETRY	0xC1
+#define HPSA_VPD_LV_IOACCEL_STATUS	0xC2
 #define HPSA_VPD_LV_STATUS		0xC3
-#define HPSA_VPD_HEADER_SZ              4
+#define HPSA_VPD_HEADER_SZ		4
 
 /* Logical volume states */
 #define HPSA_VPD_LV_STATUS_UNSUPPORTED			0xff
-#define HPSA_LV_OK                                      0x0
+#define HPSA_LV_OK					0x0
 #define HPSA_LV_UNDERGOING_ERASE			0x0F
 #define HPSA_LV_UNDERGOING_RPI				0x12
 #define HPSA_LV_PENDING_RPI				0x13
@@ -195,7 +214,7 @@ struct InquiryData {
 #define HPSA_CISS_READ	0xc0	/* CISS Read */
 #define HPSA_GET_RAID_MAP 0xc8	/* CISS Get RAID Layout Map */
 
-#define RAID_MAP_MAX_ENTRIES   256
+#define RAID_MAP_MAX_ENTRIES   1024
 
 struct raid_map_disk_data {
 	u32   ioaccel_handle;         /**< Handle to access this disk via the
@@ -206,27 +225,27 @@ struct raid_map_disk_data {
 };
 
 struct raid_map_data {
-	__le32   structure_size;	/* Size of entire structure in bytes */
-	__le32   volume_blk_size;	/* bytes / block in the volume */
-	__le64   volume_blk_cnt;	/* logical blocks on the volume */
+	u32   structure_size;		/* Size of entire structure in bytes */
+	u32   volume_blk_size;		/* bytes / block in the volume */
+	u64   volume_blk_cnt;		/* logical blocks on the volume */
 	u8    phys_blk_shift;		/* Shift factor to convert between
 					 * units of logical blocks and physical
 					 * disk blocks */
-	u8    parity_rotation_shift;	/* Shift factor to convert between units
+	u8    parity_rotation_shift;	/*  Shift factor to convert between units
 					 * of logical stripes and physical
 					 * stripes */
-	__le16   strip_size;		/* blocks used on each disk / stripe */
-	__le64   disk_starting_blk;	/* First disk block used in volume */
-	__le64   disk_blk_cnt;		/* disk blocks used by volume / disk */
-	__le16   data_disks_per_row;	/* data disk entries / row in the map */
-	__le16   metadata_disks_per_row;/* mirror/parity disk entries / row
+	u16   strip_size;		/* blocks used on each disk / stripe */
+	u64   disk_starting_blk;	/* First disk block used in volume */
+	u64   disk_blk_cnt;		/* disk blocks used by volume / disk */
+	u16   data_disks_per_row;	/* data disk entries / row in the map */
+	u16   metadata_disks_per_row;	/* mirror/parity disk entries / row
 					 * in the map */
-	__le16   row_cnt;		/* rows in each layout map */
-	__le16   layout_map_count;	/* layout maps (1 map per mirror/parity
+	u16   row_cnt;			/* rows in each layout map */
+	u16   layout_map_count;		/* layout maps (one map per mirror/parity
 					 * group) */
-	__le16   flags;			/* Bit 0 set if encryption enabled */
-#define RAID_MAP_FLAG_ENCRYPT_ON  0x01
-	__le16   dekindex;		/* Data encryption key index. */
+	u16   flags;			/* Bit 0 set if encryption is enabled for volume */
+#define RAID_MAP_FLAG_ENCRYPT_ON  0x01 	
+	u16   dekindex;			/* Data encryption key index. */  
 	u8    reserved[16];
 	struct raid_map_disk_data data[RAID_MAP_MAX_ENTRIES];
 };
@@ -240,6 +259,7 @@ struct ReportLUNdata {
 
 struct ext_report_lun_entry {
 	u8 lunid[8];
+#define MASKED_DEVICE(x) ((x)[3] & 0xC0)
 #define GET_BMIC_BUS(lunid) ((lunid)[7] & 0x3F)
 #define GET_BMIC_LEVEL_TWO_TARGET(lunid) ((lunid)[6])
 #define GET_BMIC_DRIVE_NUMBER(lunid) (((GET_BMIC_BUS((lunid)) - 1) << 8) + \
@@ -247,6 +267,8 @@ struct ext_report_lun_entry {
 	u8 wwid[8];
 	u8 device_type;
 	u8 device_flags;
+#define NON_DISK_PHYS_DEV(x) ((x)[17] & 0x01)
+#define PHYS_IOACCEL(x) ((x)[17] & 0x08)
 	u8 lun_count; /* multi-lun device, how many luns */
 	u8 redundant_paths;
 	u32 ioaccel_handle; /* ioaccel1 only uses lower 16 bits */
@@ -318,8 +340,8 @@ union LUNAddr {
 struct CommandListHeader {
 	u8              ReplyQueue;
 	u8              SGList;
-	__le16          SGTotal;
-	__le64		tag;
+	u16             SGTotal;
+	u64		tag;
 	union LUNAddr     LUN;
 };
 
@@ -343,14 +365,14 @@ struct RequestBlock {
 };
 
 struct ErrDescriptor {
-	__le64 Addr;
-	__le32 Len;
+	u64 Addr;
+	u32  Len;
 };
 
 struct SGDescriptor {
-	__le64 Addr;
-	__le32 Len;
-	__le32 Ext;
+	u64 Addr;
+	u32  Len;
+	u32  Ext;
 };
 
 union MoreErrInfo {
@@ -379,9 +401,9 @@ struct ErrorInfo {
 #define CMD_SCSI	0x03
 #define CMD_IOACCEL1	0x04
 #define CMD_IOACCEL2	0x05
+#define IOACCEL2_TMF	0x06
 
 #define DIRECT_LOOKUP_SHIFT 4
-#define DIRECT_LOOKUP_MASK (~((1 << DIRECT_LOOKUP_SHIFT) - 1))
 
 #define HPSA_ERROR_BIT          0x02
 struct ctlr_info; /* defined in hpsa.h */
@@ -407,12 +429,12 @@ struct CommandList {
 	struct ctlr_info	   *h;
 	int			   cmd_type;
 	long			   cmdindex;
+	struct request *rq;
 	struct completion *waiting;
 	struct scsi_cmnd *scsi_cmd;
 	struct work_struct work;
 
-	/*
-	 * For commands using either of the two "ioaccel" paths to
+	/* For commands using either of the two "ioaccel" paths to
 	 * bypass the RAID stack and go directly to the physical disk
 	 * phys_disk is a pointer to the hpsa_scsi_dev_t to which the
 	 * i/o is destined.  We need to store that here because the command
@@ -421,12 +443,15 @@ struct CommandList {
 	 * not used.
 	 */
 	struct hpsa_scsi_dev_t *phys_disk;
-	atomic_t refcount; /* Must be last to avoid memset in cmd_alloc */
-} __aligned(COMMANDLIST_ALIGNMENT);
+
+	int abort_pending;
+	struct hpsa_scsi_dev_t *reset_pending;
+	atomic_t refcount;
+} __attribute__((aligned(COMMANDLIST_ALIGNMENT)));
 
 /* Max S/G elements in I/O accelerator command */
 #define IOACCEL1_MAXSGENTRIES           24
-#define IOACCEL2_MAXSGENTRIES		28
+#define IOACCEL2_MAXSGENTRIES		28 
 
 /*
  * Structure for I/O accelerator (mode 1) commands.
@@ -434,7 +459,7 @@ struct CommandList {
  */
 #define IOACCEL1_COMMANDLIST_ALIGNMENT 128
 struct io_accel1_cmd {
-	__le16 dev_handle;		/* 0x00 - 0x01 */
+	u16 dev_handle;			/* 0x00 - 0x01 */
 	u8  reserved1;			/* 0x02 */
 	u8  function;			/* 0x03 */
 	u8  reserved2[8];		/* 0x04 - 0x0B */
@@ -444,23 +469,23 @@ struct io_accel1_cmd {
 	u8  reserved4;			/* 0x13 */
 	u8  sgl_offset;			/* 0x14 */
 	u8  reserved5[7];		/* 0x15 - 0x1B */
-	__le32 transfer_len;		/* 0x1C - 0x1F */
+	u32 transfer_len;		/* 0x1C - 0x1F */
 	u8  reserved6[4];		/* 0x20 - 0x23 */
-	__le16 io_flags;		/* 0x24 - 0x25 */
+	u16 io_flags;			/* 0x24 - 0x25 */
 	u8  reserved7[14];		/* 0x26 - 0x33 */
 	u8  LUN[8];			/* 0x34 - 0x3B */
-	__le32 control;			/* 0x3C - 0x3F */
+	u32 control;			/* 0x3C - 0x3F */
 	u8  CDB[16];			/* 0x40 - 0x4F */
 	u8  reserved8[16];		/* 0x50 - 0x5F */
-	__le16 host_context_flags;	/* 0x60 - 0x61 */
-	__le16 timeout_sec;		/* 0x62 - 0x63 */
+	u16 host_context_flags;		/* 0x60 - 0x61 */
+	u16 timeout_sec;		/* 0x62 - 0x63 */
 	u8  ReplyQueue;			/* 0x64 */
 	u8  reserved9[3];		/* 0x65 - 0x67 */
-	__le64 tag;			/* 0x68 - 0x6F */
-	__le64 host_addr;		/* 0x70 - 0x77 */
+	u64 tag;			/* 0x68 - 0x6F */
+	u64 host_addr;			/* 0x70 - 0x77 */
 	u8  CISS_LUN[8];		/* 0x78 - 0x7F */
 	struct SGDescriptor SG[IOACCEL1_MAXSGENTRIES];
-} __aligned(IOACCEL1_COMMANDLIST_ALIGNMENT);
+} __attribute__((aligned(IOACCEL1_COMMANDLIST_ALIGNMENT)));
 
 #define IOACCEL1_FUNCTION_SCSIIO        0x00
 #define IOACCEL1_SGLOFFSET              32
@@ -484,8 +509,8 @@ struct io_accel1_cmd {
 #define IOACCEL1_BUSADDR_CMDTYPE        0x00000060
 
 struct ioaccel2_sg_element {
-	__le64 address;
-	__le32 length;
+	u64 address;
+	u32 length;
 	u8 reserved[3];
 	u8 chain_indicator;
 #define IOACCEL2_CHAIN 0x80
@@ -515,6 +540,12 @@ struct io_accel2_scsi_response {
 #define IOACCEL2_STATUS_SR_TASK_COMP_SET_FULL	0x28
 #define IOACCEL2_STATUS_SR_TASK_COMP_ABORTED	0x40
 #define IOACCEL2_STATUS_SR_IOACCEL_DISABLED	0x0E
+#define IOACCEL2_STATUS_SR_IO_ERROR		0x01
+#define IOACCEL2_STATUS_SR_IO_ABORTED		0x02
+#define IOACCEL2_STATUS_SR_NO_PATH_TO_DEVICE	0x03
+#define IOACCEL2_STATUS_SR_INVALID_DEVICE	0x04
+#define IOACCEL2_STATUS_SR_UNDERRUN		0x51
+#define IOACCEL2_STATUS_SR_OVERRUN		0x75
 	u8 data_present;		/* low 2 bits */
 #define IOACCEL2_NO_DATAPRESENT		0x000
 #define IOACCEL2_RESPONSE_DATAPRESENT	0x001
@@ -535,28 +566,28 @@ struct io_accel2_cmd {
 	u8  direction;			/* direction, memtype, and encryption */
 #define IOACCEL2_DIRECTION_MASK		0x03 /* bits 0,1: direction  */
 #define IOACCEL2_DIRECTION_MEMTYPE_MASK	0x04 /* bit 2: memtype source/dest */
-					     /*     0b=PCIe, 1b=DDR */
+					     /*     0b=PCIe, 1b=DDR */ 
 #define IOACCEL2_DIRECTION_ENCRYPT_MASK	0x08 /* bit 3: encryption flag */
-					     /*     0=off, 1=on */
+					     /*     0=off, 1=on */					
 	u8  reply_queue;		/* Reply Queue ID */
 	u8  reserved1;			/* Reserved */
-	__le32 scsi_nexus;		/* Device Handle */
-	__le32 Tag;			/* cciss tag, lower 4 bytes only */
-	__le32 tweak_lower;		/* Encryption tweak, lower 4 bytes */
+	u32 scsi_nexus;			/* Device Handle */
+	u32 Tag;			/* cciss tag, lower 4 bytes only */
+	u32 tweak_lower;		/* Encryption tweak, lower 4 bytes */	
 	u8  cdb[16];			/* SCSI Command Descriptor Block */
 	u8  cciss_lun[8];		/* 8 byte SCSI address */
-	__le32 data_len;		/* Total bytes to transfer */
+	u32 data_len;			/* Total bytes to transfer */
 	u8  cmd_priority_task_attr;	/* priority and task attrs */
 #define IOACCEL2_PRIORITY_MASK 0x78
 #define IOACCEL2_ATTR_MASK 0x07
 	u8  sg_count;			/* Number of sg elements */
-	__le16 dekindex;		/* Data encryption key index */
-	__le64 err_ptr;			/* Error Pointer */
-	__le32 err_len;			/* Error Length*/
-	__le32 tweak_upper;		/* Encryption tweak, upper 4 bytes */
+	u16 dekindex;			/* Data encryption key index */
+	u64 err_ptr;			/* Error Pointer */
+	u32 err_len;			/* Error Length*/
+	u32 tweak_upper;		/* Encryption tweak, upper 4 bytes */
 	struct ioaccel2_sg_element sg[IOACCEL2_MAXSGENTRIES];
 	struct io_accel2_scsi_response error_data;
-} __aligned(IOACCEL2_COMMANDLIST_ALIGNMENT);
+} __attribute__((aligned(IOACCEL2_COMMANDLIST_ALIGNMENT)));
 
 /*
  * defines for Mode 2 command struct
@@ -567,6 +598,7 @@ struct io_accel2_cmd {
 #define IOACCEL2_DIR_NO_DATA	0x00
 #define IOACCEL2_DIR_DATA_IN	0x01
 #define IOACCEL2_DIR_DATA_OUT	0x02
+#define IOACCEL2_TMF_ABORT	0x01
 /*
  * SCSI Task Management Request format for Accelerator Mode 2
  */
@@ -577,18 +609,18 @@ struct hpsa_tmf_struct {
 	u8 reserved1;		/* byte 3 Reserved */
 	u32 it_nexus;		/* SCSI I-T Nexus */
 	u8 lun_id[8];		/* LUN ID for TMF request */
-	__le64 tag;		/* cciss tag associated w/ request */
-	__le64 abort_tag;	/* cciss tag of SCSI cmd or TMF to abort */
-	__le64 error_ptr;		/* Error Pointer */
-	__le32 error_len;		/* Error Length */
-};
+	u64 tag;		/* cciss tag associated w/ request */
+	u64 abort_tag;		/* cciss tag of SCSI cmd or task to abort */
+	u64 error_ptr;		/* Error Pointer */
+	u32 error_len;		/* Error Length */
+} __attribute__((aligned(IOACCEL2_COMMANDLIST_ALIGNMENT)));
 
 /* Configuration Table Structure */
 struct HostWrite {
-	__le32		TransportRequest;
-	__le32		command_pool_addr_hi;
-	__le32		CoalIntDelay;
-	__le32		CoalIntCount;
+	u32 TransportRequest;
+	u32 command_pool_addr_hi;
+	u32 CoalIntDelay;
+	u32 CoalIntCount;
 };
 
 #define SIMPLE_MODE     0x02
@@ -599,54 +631,53 @@ struct HostWrite {
 #define DRIVER_SUPPORT_UA_ENABLE        0x00000001
 
 struct CfgTable {
-	u8		Signature[4];
-	__le32		SpecValence;
-	__le32		TransportSupport;
-	__le32		TransportActive;
-	struct HostWrite HostWrite;
-	__le32		CmdsOutMax;
-	__le32		BusTypes;
-	__le32		TransMethodOffset;
-	u8		ServerName[16];
-	__le32		HeartBeat;
-	__le32		driver_support;
-#define			ENABLE_SCSI_PREFETCH		0x100
-#define			ENABLE_UNIT_ATTN		0x01
-	__le32		MaxScatterGatherElements;
-	__le32		MaxLogicalUnits;
-	__le32		MaxPhysicalDevices;
-	__le32		MaxPhysicalDrivesPerLogicalUnit;
-	__le32		MaxPerformantModeCommands;
-	__le32		MaxBlockFetch;
-	__le32		PowerConservationSupport;
-	__le32		PowerConservationEnable;
-	__le32		TMFSupportFlags;
+	u8            Signature[4];
+	u32		SpecValence;
+	u32           TransportSupport;
+	u32           TransportActive;
+	struct 		HostWrite HostWrite;
+	u32           CmdsOutMax;
+	u32           BusTypes;
+	u32           TransMethodOffset;
+	u8            ServerName[16];
+	u32           HeartBeat;
+	u32           driver_support;
+#define			ENABLE_SCSI_PREFETCH 0x100
+	u32	 	MaxScatterGatherElements;
+	u32		MaxLogicalUnits;
+	u32		MaxPhysicalDevices;
+	u32		MaxPhysicalDrivesPerLogicalUnit;
+	u32		MaxPerformantModeCommands;
+	u32		MaxBlockFetch;
+	u32		PowerConservationSupport;
+	u32		PowerConservationEnable;
+	u32		TMFSupportFlags;
 	u8		TMFTagMask[8];
 	u8		reserved[0x78 - 0x70];
-	__le32		misc_fw_support;		/* offset 0x78 */
-#define			MISC_FW_DOORBELL_RESET		0x02
-#define			MISC_FW_DOORBELL_RESET2		0x010
-#define			MISC_FW_RAID_OFFLOAD_BASIC	0x020
-#define			MISC_FW_EVENT_NOTIFY		0x080
+	u32		misc_fw_support; /* offset 0x78 */
+#define			MISC_FW_DOORBELL_RESET (0x02)
+#define			MISC_FW_DOORBELL_RESET2 (0x010)
+#define			MISC_FW_RAID_OFFLOAD_BASIC (0x020)
+#define			MISC_FW_EVENT_NOTIFY (0x080)
 	u8		driver_version[32];
-	__le32		max_cached_write_size;
-	u8		driver_scratchpad[16];
-	__le32		max_error_info_length;
-	__le32		io_accel_max_embedded_sg_count;
-	__le32		io_accel_request_size_offset;
-	__le32		event_notify;
-#define		HPSA_EVENT_NOTIFY_ACCEL_IO_PATH_STATE_CHANGE (1 << 30)
-#define		HPSA_EVENT_NOTIFY_ACCEL_IO_PATH_CONFIG_CHANGE (1 << 31)
-	__le32		clear_event_notify;
+	u32             max_cached_write_size;
+	u8              driver_scratchpad[16];
+	u32             max_error_info_length;
+	u32		io_accel_max_embedded_sg_count;
+	u32		io_accel_request_size_offset;
+	u32		event_notify;
+#define HPSA_EVENT_NOTIFY_ACCEL_IO_PATH_STATE_CHANGE (1 << 30)
+#define HPSA_EVENT_NOTIFY_ACCEL_IO_PATH_CONFIG_CHANGE (1 << 31)
+	u32		clear_event_notify;
 };
 
 #define NUM_BLOCKFETCH_ENTRIES 8
 struct TransTable_struct {
-	__le32		BlockFetch[NUM_BLOCKFETCH_ENTRIES];
-	__le32		RepQSize;
-	__le32		RepQCount;
-	__le32		RepQCtrAddrLow32;
-	__le32		RepQCtrAddrHigh32;
+	u32            BlockFetch[NUM_BLOCKFETCH_ENTRIES];
+	u32            RepQSize;
+	u32            RepQCount;
+	u32            RepQCtrAddrLow32;
+	u32            RepQCtrAddrHigh32;
 #define MAX_REPLY_QUEUES 64
 	struct vals32  RepQAddr[MAX_REPLY_QUEUES];
 };
@@ -659,17 +690,17 @@ struct hpsa_pci_info {
 };
 
 struct bmic_identify_physical_device {
-	u8    scsi_bus;          /* SCSI Bus number on controller */
-	u8    scsi_id;           /* SCSI ID on this bus */
-	__le16 block_size;	     /* sector size in bytes */
-	__le32 total_blocks;	     /* number for sectors on drive */
-	__le32 reserved_blocks;   /* controller reserved (RIS) */
-	u8    model[40];         /* Physical Drive Model */
-	u8    serial_number[40]; /* Drive Serial Number */
-	u8    firmware_revision[8]; /* drive firmware revision */
-	u8    scsi_inquiry_bits; /* inquiry byte 7 bits */
-	u8    compaq_drive_stamp; /* 0 means drive not stamped */
-	u8    last_failure_reason;
+	u8 scsi_bus;          /* SCSI Bus number on controller */
+	u8 scsi_id;           /* SCSI ID on this bus */
+	u16 block_size;	     /* sector size in bytes */
+	u32 total_blocks;	     /* number for sectors on drive */
+	u32 reserved_blocks;   /* controller reserved (RIS) */
+	u8 model[40];         /* Physical Drive Model */
+	u8 serial_number[40]; /* Drive Serial Number */
+	u8 firmware_revision[8]; /* drive firmware revision */
+	u8 scsi_inquiry_bits; /* inquiry byte 7 bits */
+	u8 compaq_drive_stamp; /* 0 means drive not stamped */
+	u8 last_failure_reason;
 #define BMIC_LAST_FAILURE_TOO_SMALL_IN_LOAD_CONFIG		0x01
 #define BMIC_LAST_FAILURE_ERROR_ERASING_RIS			0x02
 #define BMIC_LAST_FAILURE_ERROR_SAVING_RIS			0x03
@@ -724,70 +755,70 @@ struct bmic_identify_physical_device {
 #define BMIC_LAST_FAILURE_OFFLINE_DRIVE_TYPE_MIX		0x82
 #define BMIC_LAST_FAILURE_OFFLINE_ERASE_COMPLETE		0x83
 
-	u8     flags;
-	u8     more_flags;
-	u8     scsi_lun;          /* SCSI LUN for phys drive */
-	u8     yet_more_flags;
-	u8     even_more_flags;
-	__le32 spi_speed_rules;/* SPI Speed data:Ultra disable diagnose */
-	u8     phys_connector[2];         /* connector number on controller */
-	u8     phys_box_on_bus;  /* phys enclosure this drive resides */
-	u8     phys_bay_in_box;  /* phys drv bay this drive resides */
-	__le32 rpm;              /* Drive rotational speed in rpm */
-	u8     device_type;       /* type of drive */
-	u8     sata_version;     /* only valid when drive_type is SATA */
-	__le64 big_total_block_count;
-	__le64 ris_starting_lba;
-	__le32 ris_size;
-	u8     wwid[20];
-	u8     controller_phy_map[32];
-	__le16 phy_count;
-	u8     phy_connected_dev_type[256];
-	u8     phy_to_drive_bay_num[256];
-	__le16 phy_to_attached_dev_index[256];
-	u8     box_index;
-	u8     reserved;
-	__le16 extra_physical_drive_flags;
+	u8  flags;
+	u8  more_flags;
+	u8  scsi_lun;          /* SCSI LUN for phys drive */
+	u8  yet_more_flags;
+	u8  even_more_flags;
+	u32 spi_speed_rules;/* SPI Speed data:Ultra disable diagnose */
+	u8  phys_connector[2];         /* connector number on controller */
+	u8  phys_box_on_bus;  /* phys enclosure this drive resides */
+	u8  phys_bay_in_box;  /* phys drv bay this drive resides */
+	u32 rpm;              /* Drive rotational speed in rpm */
+	u8  device_type;       /* type of drive */
+	u8  sata_version;     /* only valid when drive_type is SATA */
+	u64 big_total_block_count;
+	u64 ris_starting_lba;
+	u32 ris_size;
+	u8  wwid[20];
+	u8  controller_phy_map[32];
+	u16 phy_count;
+	u8  phy_connected_dev_type[256];
+	u8  phy_to_drive_bay_num[256];
+	u16 phy_to_attached_dev_index[256];
+	u8  box_index;
+	u8  reserved;
+	u16 extra_physical_drive_flags;
 #define BMIC_PHYS_DRIVE_SUPPORTS_GAS_GAUGE(idphydrv) \
 	(idphydrv->extra_physical_drive_flags & (1 << 10))
-	u8     negotiated_link_rate[256];
-	u8     phy_to_phy_map[256];
-	u8     redundant_path_present_map;
-	u8     redundant_path_failure_map;
-	u8     active_path_number;
-	__le16 alternate_paths_phys_connector[8];
-	u8     alternate_paths_phys_box_on_port[8];
-	u8     multi_lun_device_lun_count;
-	u8     minimum_good_fw_revision[8];
-	u8     unique_inquiry_bytes[20];
-	u8     current_temperature_degreesC;
-	u8     temperature_threshold_degreesC;
-	u8     max_temperature_degreesC;
-	u8     logical_blocks_per_phys_block_exp; /* phyblocksize = 512*2^exp */
-	__le16 current_queue_depth_limit;
-	u8     switch_name[10];
-	__le16 switch_port;
-	u8     alternate_paths_switch_name[40];
-	u8     alternate_paths_switch_port[8];
-	__le16 power_on_hours; /* valid only if gas gauge supported */
-	__le16 percent_endurance_used; /* valid only if gas gauge supported. */
+	u8  negotiated_link_rate[256];
+	u8  phy_to_phy_map[256];
+	u8  redundant_path_present_map;
+	u8  redundant_path_failure_map;
+	u8  active_path_number;
+	u16 alternate_paths_phys_connector[8];
+	u8  alternate_paths_phys_box_on_port[8];
+	u8  multi_lun_device_lun_count;
+	u8  minimum_good_fw_revision[8];
+	u8  unique_inquiry_bytes[20];
+	u8  current_temperature_degreesC;
+	u8  temperature_threshold_degreesC;
+	u8  max_temperature_degreesC;
+	u8  logical_blocks_per_phys_block_exp; /* phyblocksize = 512 * 2^exp */
+	u16 current_queue_depth_limit;
+	u8  switch_name[10];
+	u16 switch_port;
+	u8  alternate_paths_switch_name[40];
+	u8  alternate_paths_switch_port[8];
+	u16 power_on_hours; /* valid only if gas gauge supported */
+	u16 percent_endurance_used; /* valid only if gas gauge supported. */
 #define BMIC_PHYS_DRIVE_SSD_WEAROUT(idphydrv) \
 	((idphydrv->percent_endurance_used & 0x80) || \
 	 (idphydrv->percent_endurance_used > 10000))
-	u8     drive_authentication;
+	u8  drive_authentication;
 #define BMIC_PHYS_DRIVE_AUTHENTICATED(idphydrv) \
 	(idphydrv->drive_authentication == 0x80)
-	u8     smart_carrier_authentication;
+	u8  smart_carrier_authentication;
 #define BMIC_SMART_CARRIER_AUTHENTICATION_SUPPORTED(idphydrv) \
 	(idphydrv->smart_carrier_authentication != 0x0)
 #define BMIC_SMART_CARRIER_AUTHENTICATED(idphydrv) \
 	(idphydrv->smart_carrier_authentication == 0x01)
-	u8     smart_carrier_app_fw_version;
-	u8     smart_carrier_bootloader_fw_version;
-	u8     encryption_key_name[64];
-	__le32 misc_drive_flags;
-	__le16 dek_index;
-	u8     padding[112];
+	u8  smart_carrier_app_fw_version;
+	u8  smart_carrier_bootloader_fw_version;
+	u8  encryption_key_name[64];
+	u32 misc_drive_flags;
+	u16 dek_index;
+	u8  padding[112];
 };
 
 #pragma pack()
diff --git a/drivers/scsi/hpsa_kernel_compat.h b/drivers/scsi/hpsa_kernel_compat.h
new file mode 100644
index 0000000..7b84753
--- /dev/null
+++ b/drivers/scsi/hpsa_kernel_compat.h
@@ -0,0 +1,738 @@
+/*
+ *    Disk Array driver for HP Smart Array SAS controllers
+ *    Copyright 2014-2015 PMC-Sierra, Inc.
+ *    Copyright 2000,2009-2015 Hewlett-Packard Development Company, L.P.
+ *
+ *    This program is free software; you can redistribute it and/or modify
+ *    it under the terms of the GNU General Public License as published by
+ *    the Free Software Foundation; version 2 of the License.
+ *
+ *    This program is distributed in the hope that it will be useful,
+ *    but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *    MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or
+ *    NON INFRINGEMENT.  See the GNU General Public License for more details.
+ *
+ *    You should have received a copy of the GNU General Public License
+ *    along with this program; if not, write to the Free Software
+ *    Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ *    Questions/Comments/Bugfixes to storagedev@pmcs.com
+ *
+ */
+
+/*
+ * The following #defines allow the hpsa driver to be compiled for a 
+ * variety of kernels.  Despite having names like RHEL5, SLES11, these
+ * are more about the kernel than about the OS.  So for instance, if
+ * you're running RHEL5 (typically 2.6.18-ish kernel), but you've compiled
+ * a custom 2.6.38 or 3.x kernel and you're running that, then you don't want
+ * the RHEL5 define, you probably want the default kernel.org (as of this
+ * writing circa March 2012)  If you're running the OS vendor's kernel
+ * or a kernel that is of roughly the same vintage as the OS vendor's kernel
+ * then you can go by the OS name.
+ *
+ * If you have some intermediate kernel which doesn't quite match any of
+ * the predefined sets of kernel features here, you may have to make your own 
+ * define for your particular kernel and mix and match the kernel features
+ * to fit the kernel you're compiling for.  How can you tell?  By studying
+ * the source of this file and the source of the kernel you're compiling for
+ * and understanding which "KFEATURES" your kernel has.
+ *
+ * Usually, if you get it wrong, it won't compile, but there are no doubt
+ * some cases in which, if you get it wrong, it will compile, but won't
+ * work right.  In any case, if you're compiling this, you're on your own
+ * and likely nobody has tested this particular code with your particular
+ * kernel, so, good luck, and pay attention to the compiler warnings.
+ *
+ */
+
+/* #define SLES11sp1 */
+/* #define SLES11sp2plus */
+/* #define RHEL5 */
+/* #define RHEL5u2 */
+/* #define RHEL6 */
+/* #define RHEL7 */
+/* Default is kernel.org */
+
+
+/* ----- RHEL5 variants --------- */
+#if \
+	defined(RHEL5U0) || \
+	defined(RHEL5U1) || \
+	defined(RHEL5U2) || \
+	defined(RHEL5U2) || \
+	defined(RHEL5U3) || \
+	defined(RHEL5U4) || \
+	defined(RHEL5U5) || \
+	defined(RHEL5U6) || \
+	defined(RHEL5U7) || \
+	defined(RHEL5U8) || \
+	defined(RHEL5U9) || \
+	defined(RHEL5U10)
+#error "Sorry, hpsa will no longer work on RHEL5.  The scatter gather DMA mapping API of RHEL5 is too old."
+#endif
+
+
+/* ----- RHEL6 variants --------- */
+#if \
+	defined (RHEL6U0) || \
+	defined (RHEL6U1) || \
+	defined (RHEL6U2) || \
+	defined (RHEL6U3) || \
+	defined (RHEL6U4) || \
+	defined (RHEL6U5) || \
+	defined (RHEL6U6)
+#define RHEL6
+#endif
+
+
+/* ----- RHEL7 variants --------- */
+#if \
+	defined (RHEL7U0)
+#define RHEL7
+#endif
+
+
+/* ----- SLES11 variants --------- */
+#if \
+	defined (SLES11SP0) || \
+	defined (SLES11SP1) || \
+	defined (SLES11SP2) || \
+	defined (SLES11SP3) || \
+	defined (SLES11SP4)
+
+#define SLES11
+#define SLES11sp2plus
+
+#if defined (SLES11SP0)
+#undef SLES11sp1
+#undef SLES11sp2plus
+#endif
+
+#if defined (SLES11SP1)
+#define SLES11sp1
+#undef SLES11sp2plus
+#endif
+#endif
+
+
+/* ----- SLES12 variants --------- */
+#if \
+	defined (SLES12SP0)
+#define SLES12
+#endif
+
+
+/* Define kernel features per distro... */
+
+#ifdef RHEL6 /************ RHEL 6 ************/
+#define KFEATURE_HAS_WAIT_FOR_COMPLETION_IO_TIMEOUT 0
+#define KFEATURE_HAS_WAIT_FOR_COMPLETION_IO 0
+#define KFEATURE_HAS_2011_03_INTERRUPT_HANDLER 1
+#define KFEATURE_CHANGE_QDEPTH_HAS_REASON 1
+#define KFEATURE_HAS_2011_03_STYLE_DEVICE_ATTR 1
+#define KFEATURE_HAS_SCSI_QDEPTH_DEFAULT 1
+#define KFEATURE_HAS_SCSI_FOR_EACH_SG 1
+#define KFEATURE_HAS_SCSI_DEVICE_TYPE 1
+#define KFEATURE_SCAN_START_PRESENT 1
+#define KFEATURE_SCAN_START_IMPLEMENTED 1
+#define KFEATURE_HAS_2011_03_QUEUECOMMAND 0
+#define KFEATURE_HAS_SHOST_PRIV 1
+#define KFEATURE_HAS_SCSI_DMA_FUNCTIONS 1
+#define KFEATURE_HAS_SCSI_SET_RESID 1
+#define KFEATURE_HAS_UACCESS_H_FILE 1
+#define KFEATURE_HAS_SMP_LOCK_H 1
+#define KFEATURE_HAS_NEW_DMA_MAPPING_ERROR 1
+#define KFEATURE_HAS_IRQ_SET_AFFINITY_HINTS 1
+#define KFEATURE_HAS_ALLOC_WORKQUEUE 0
+#define KFEATURE_HAS_ATOMIC_DEC_IF_POSITIVE 0
+#define HPSA_SUPPORTS_STORAGEWORKS_1210m 1
+#define SA_CONTROLLERS_GEN6 1
+#define SA_CONTROLLERS_GEN8 1
+#define SA_CONTROLLERS_GEN8_2 1
+#define SA_CONTROLLERS_GEN8_5 1
+#define SA_CONTROLLERS_GEN9 1
+#define PMC_CONTROLLERS_GEN1 1
+
+#if defined (RHEL6U0) || defined (RHEL6U1)
+/* RHEL6 base and update 1 do not have .lockless field in SCSI host template
+ * but RHEL6u2 and onwards do.
+ */
+#define KFEATURE_HAS_HOST_LOCKLESS_FIELD 0
+#else
+#define KFEATURE_HAS_HOST_LOCKLESS_FIELD 1
+#endif
+
+#else /* ... else it's not RHEL6 */
+
+#ifdef SLES11
+#define KFEATURE_HAS_WAIT_FOR_COMPLETION_IO_TIMEOUT 0
+#define KFEATURE_HAS_WAIT_FOR_COMPLETION_IO 0
+#define KFEATURE_HAS_2011_03_INTERRUPT_HANDLER 1
+#define KFEATURE_CHANGE_QDEPTH_HAS_REASON 1
+#define KFEATURE_HAS_2011_03_STYLE_DEVICE_ATTR 1
+#define KFEATURE_HAS_SCSI_QDEPTH_DEFAULT 1
+#define KFEATURE_HAS_SCSI_FOR_EACH_SG 1
+#define KFEATURE_HAS_SCSI_DEVICE_TYPE 1
+#define KFEATURE_SCAN_START_PRESENT 1
+#define KFEATURE_SCAN_START_IMPLEMENTED 1
+#define KFEATURE_HAS_HOST_LOCKLESS_FIELD 0
+#define KFEATURE_HAS_SHOST_PRIV 1
+#define KFEATURE_HAS_SCSI_DMA_FUNCTIONS 1
+#define KFEATURE_HAS_SCSI_SET_RESID 1
+#define KFEATURE_HAS_UACCESS_H_FILE 1
+#define KFEATURE_HAS_NEW_DMA_MAPPING_ERROR 1
+#define HPSA_SUPPORTS_STORAGEWORKS_1210m 1
+#define SA_CONTROLLERS_GEN6 0
+#define SA_CONTROLLERS_GEN8 1
+#define SA_CONTROLLERS_GEN8_2 1
+#define SA_CONTROLLERS_GEN8_5 1
+#define SA_CONTROLLERS_GEN9 1
+#define PMC_CONTROLLERS_GEN1 1
+
+#ifdef SLES11sp1 /************* SLES11 sp1 ********/
+#define KFEATURE_HAS_SMP_LOCK_H 1
+#define KFEATURE_HAS_2011_03_QUEUECOMMAND 0
+#define KFEATURE_HAS_IRQ_SET_AFFINITY_HINTS 0
+#define KFEATURE_HAS_ALLOC_WORKQUEUE 0
+#define KFEATURE_HAS_ATOMIC_DEC_IF_POSITIVE 0
+#endif
+
+#ifdef SLES11sp2plus /************* SLES11 sp2 and after ********/
+#define KFEATURE_HAS_SMP_LOCK_H 0
+#define KFEATURE_HAS_2011_03_QUEUECOMMAND 1
+#define KFEATURE_HAS_IRQ_SET_AFFINITY_HINTS 1
+#define KFEATURE_HAS_ALLOC_WORKQUEUE 1
+#define KFEATURE_HAS_ATOMIC_DEC_IF_POSITIVE 1
+#endif
+
+#else /* ... else not SLES11 */
+
+#ifdef RHEL7 /************ RHEL 7 ************/
+#define KFEATURE_HAS_WAIT_FOR_COMPLETION_IO_TIMEOUT 1
+#define KFEATURE_HAS_WAIT_FOR_COMPLETION_IO 1
+#define KFEATURE_HAS_2011_03_INTERRUPT_HANDLER 1
+#define KFEATURE_CHANGE_QDEPTH_HAS_REASON 1
+#define KFEATURE_HAS_2011_03_STYLE_DEVICE_ATTR 1
+#define KFEATURE_HAS_SCSI_QDEPTH_DEFAULT 1
+#define KFEATURE_HAS_SCSI_FOR_EACH_SG 1
+#define KFEATURE_HAS_SCSI_DEVICE_TYPE 1
+#define KFEATURE_SCAN_START_PRESENT 1
+#define KFEATURE_SCAN_START_IMPLEMENTED 1
+#define KFEATURE_HAS_2011_03_QUEUECOMMAND 1
+#define KFEATURE_HAS_HOST_LOCKLESS_FIELD 0
+#define KFEATURE_HAS_SHOST_PRIV 1
+#define KFEATURE_HAS_SCSI_DMA_FUNCTIONS 1
+#define KFEATURE_HAS_SCSI_SET_RESID 1
+#define KFEATURE_HAS_UACCESS_H_FILE 1
+#define KFEATURE_HAS_SMP_LOCK_H 0 /* include/linux/smp_lock.h removed between 2.6.38 and 2.6.39 */
+#define KFEATURE_HAS_NEW_DMA_MAPPING_ERROR 1
+#define KFEATURE_HAS_IRQ_SET_AFFINITY_HINTS 1
+#define KFEATURE_HAS_ALLOC_WORKQUEUE 1
+#define KFEATURE_HAS_ATOMIC_DEC_IF_POSITIVE 1
+#define HPSA_SUPPORTS_STORAGEWORKS_1210m 1
+#define SA_CONTROLLERS_GEN6 1
+#define SA_CONTROLLERS_GEN8 1
+#define SA_CONTROLLERS_GEN8_2 1
+#define SA_CONTROLLERS_GEN8_5 1
+#define SA_CONTROLLERS_GEN9 1
+#define PMC_CONTROLLERS_GEN1 1
+
+#else /* ... else Default, kernel.org */
+#define KFEATURE_HAS_WAIT_FOR_COMPLETION_IO_TIMEOUT 1
+#define KFEATURE_HAS_WAIT_FOR_COMPLETION_IO 1
+#define KFEATURE_HAS_2011_03_INTERRUPT_HANDLER 1
+#define KFEATURE_CHANGE_QDEPTH_HAS_REASON 1
+#define KFEATURE_HAS_2011_03_STYLE_DEVICE_ATTR 1
+#define KFEATURE_HAS_SCSI_QDEPTH_DEFAULT 1
+#define KFEATURE_HAS_SCSI_FOR_EACH_SG 1
+#define KFEATURE_HAS_SCSI_DEVICE_TYPE 1
+#define KFEATURE_SCAN_START_PRESENT 1
+#define KFEATURE_SCAN_START_IMPLEMENTED 1
+#define KFEATURE_HAS_2011_03_QUEUECOMMAND 1
+#define KFEATURE_HAS_HOST_LOCKLESS_FIELD 0
+#define KFEATURE_HAS_SHOST_PRIV 1
+#define KFEATURE_HAS_SCSI_DMA_FUNCTIONS 1
+#define KFEATURE_HAS_SCSI_SET_RESID 1
+#define KFEATURE_HAS_UACCESS_H_FILE 1
+#define KFEATURE_HAS_SMP_LOCK_H 0 /* include/linux/smp_lock.h removed between 2.6.38 and 2.6.39 */
+#define KFEATURE_HAS_NEW_DMA_MAPPING_ERROR 1
+#define KFEATURE_HAS_IRQ_SET_AFFINITY_HINTS 1
+#define KFEATURE_HAS_ALLOC_WORKQUEUE 1
+#define KFEATURE_HAS_ATOMIC_DEC_IF_POSITIVE 1
+#define HPSA_SUPPORTS_STORAGEWORKS_1210m 1
+#define SA_CONTROLLERS_GEN6 1
+#define SA_CONTROLLERS_GEN8 1
+#define SA_CONTROLLERS_GEN8_2 1
+#define SA_CONTROLLERS_GEN8_5 1
+#define SA_CONTROLLERS_GEN9 1
+#define PMC_CONTROLLERS_GEN1 1
+/* --- end of default kernel.org --- */
+
+#endif /* RHEL7 */
+#endif /* SLES11 */
+#endif /* RHEL6 */
+
+#if !KFEATURE_HAS_WAIT_FOR_COMPLETION_IO_TIMEOUT
+static inline unsigned long wait_for_completion_io_timeout(struct completion *x,
+			__attribute__((unused)) unsigned long timeout)
+{
+	return wait_for_completion_timeout(x, timeout);
+}
+#endif
+
+#if !KFEATURE_HAS_WAIT_FOR_COMPLETION_IO
+static inline unsigned long wait_for_completion_io(struct completion *x)
+{
+	wait_for_completion(x);
+	return 0;
+}
+#endif
+
+#if KFEATURE_HAS_2011_03_INTERRUPT_HANDLER
+	/* new style interrupt handler */
+#	define DECLARE_INTERRUPT_HANDLER(handler) \
+		static irqreturn_t handler(int irq, void *queue)
+#	define INTERRUPT_HANDLER_TYPE(handler) \
+		irqreturn_t (*handler)(int, void *)
+#else
+	/* old style interrupt handler */
+#	define DECLARE_INTERRUPT_HANDLER(handler) \
+		static irqreturn_t handler(int irq, void *queue, \
+			struct pt_regs *regs)
+#	define INTERRUPT_HANDLER_TYPE(handler) \
+		irqreturn_t (*handler)(int, void *, struct pt_regs *)
+#endif
+
+
+#if KFEATURE_CHANGE_QDEPTH_HAS_REASON
+#	define DECLARE_CHANGE_QUEUE_DEPTH(func) \
+	static int func(struct scsi_device *sdev, \
+		int qdepth, int the_reason)
+#	define SET_QUEUE_DEPTH_REASON(reason) \
+		((reason) = (the_reason))
+#else
+#	define DECLARE_CHANGE_QUEUE_DEPTH(func) \
+	static int func(struct scsi_device *sdev, int qdepth)
+#	define SET_QUEUE_DEPTH_REASON(reason) \
+		((reason) = -1)
+#endif
+
+
+#if KFEATURE_HAS_2011_03_STYLE_DEVICE_ATTR
+
+#	define DECLARE_DEVATTR_SHOW_FUNC(func) \
+		static ssize_t func(struct device *dev, \
+			struct device_attribute *attr, char *buf)
+
+#	define DECLARE_DEVATTR_STORE_FUNC(func) \
+	static ssize_t func(struct device *dev, \
+		struct device_attribute *attr, const char *buf, size_t count)
+
+#	define DECLARE_HOST_DEVICE_ATTR(xname, xmode, xshow, xstore) \
+		DEVICE_ATTR(xname, xmode, xshow, xstore)
+
+#	define DECLARE_HOST_ATTR_LIST(xlist) \
+	static struct device_attribute *xlist[]
+#else /* not KFEATURE_HAS_2011_03_STYLE_DEVICE_ATTR */
+
+#	define DECLARE_DEVATTR_SHOW_FUNC(func) \
+	static ssize_t func(struct class_device *dev, char *buf)
+
+#	define DECLARE_DEVATTR_STORE_FUNC(func) \
+	static ssize_t func(struct class_device *dev, \
+		const char *buf, size_t count)
+
+#	define DECLARE_HOST_DEVICE_ATTR(xname, xmode, xshow, xstore) \
+	struct class_device_attribute dev_attr_##xname = {\
+		.attr = { \
+			.name = #xname, \
+			.mode = xmode, \
+		}, \
+		.show = xshow, \
+		.store = xstore, \
+	};
+
+#	define DECLARE_HOST_ATTR_LIST(xlist) \
+	static struct class_device_attribute *xlist[]
+
+#endif /* KFEATURE_HAS_2011_03_STYLE_DEVICE_ATTR */
+
+#ifndef SCSI_QDEPTH_DEFAULT
+#	define SCSI_QDEPTH_DEFAULT 0
+#endif
+
+#if !KFEATURE_HAS_SCSI_FOR_EACH_SG
+#	define scsi_for_each_sg(cmd, sg, nseg, __i) \
+	for (__i = 0, sg = scsi_sglist(cmd); __i < (nseg); __i++, (sg)++)
+#endif
+
+#if !KFEATURE_HAS_SHOST_PRIV
+	static inline void *shost_priv(struct Scsi_Host *shost)
+	{
+		return (void *) shost->hostdata;
+	}
+#endif
+
+#if !KFEATURE_HAS_SCSI_DMA_FUNCTIONS
+	/* Does not have things like scsi_dma_map, scsi_dma_unmap, scsi_sg_count,
+	 * sg_dma_address, sg_dma_len...
+	 */
+
+static void hpsa_map_sg_chain_block(struct ctlr_info *h,
+	struct CommandList *c);
+
+/* It is not reasonably possible to retrofit the new scsi dma interfaces
+ * onto the old code.  So we retrofit at a higher level, at the dma mapping
+ * function of the hpsa driver itself.
+ *
+ * hpsa_scatter_gather takes a struct scsi_cmnd, (cmd), and does the pci
+ * dma mapping  and fills in the scatter gather entries of the
+ * hpsa command, cp.
+ */
+static int hpsa_scatter_gather(struct ctlr_info *h,
+		struct CommandList *cp,
+		struct scsi_cmnd *cmd)
+{
+	unsigned int len;
+	u64 addr64;
+	int use_sg, i, sg_index, chained = 0;
+	struct SGDescriptor *curr_sg;
+	struct scatterlist *sg = (struct scatterlist *) cmd->request_buffer;
+
+	if (!cmd->use_sg) {
+		if (cmd->request_bufflen) { /* Just one scatter gather entry */
+			addr64 = (__u64) pci_map_single(h->pdev,
+				cmd->request_buffer, cmd->request_bufflen,
+				cmd->sc_data_direction);
+
+			cp->SG[0].Addr.lower =
+				(__u32) (addr64 & (__u64) 0x0FFFFFFFF);
+			cp->SG[0].Addr.upper =
+				(__u32) ((addr64 >> 32) & (__u64) 0x0FFFFFFFF);
+			cp->SG[0].Len = cmd->request_bufflen;
+			use_sg = 1;
+		} else /* Zero sg entries */
+			use_sg = 0;
+	} else {
+		BUG_ON(cmd->use_sg > h->maxsgentries);
+
+		/* Many sg entries */
+		use_sg = pci_map_sg(h->pdev, cmd->request_buffer, cmd->use_sg,
+				cmd->sc_data_direction);
+
+		if (use_sg < 0)
+			return use_sg;
+
+		sg_index = 0;
+		curr_sg = cp->SG;
+		use_sg = cmd->use_sg;
+
+		for (i = 0; i < use_sg; i++) {
+			if (i == h->max_cmd_sg_entries - 1 &&
+				use_sg > h->max_cmd_sg_entries) {
+				chained = 1;
+				curr_sg = h->cmd_sg_list[cp->cmdindex];
+				sg_index = 0;
+			}
+			addr64 = (__u64) sg_dma_address(&sg[i]);
+			len  = sg_dma_len(&sg[i]);
+			curr_sg->Addr.lower =
+				(u32) (addr64 & 0x0FFFFFFFFULL);
+			curr_sg->Addr.upper =
+				(u32) ((addr64 >> 32) & 0x0FFFFFFFFULL);
+			curr_sg->Len = len;
+			curr_sg->Ext = 0;  /* we are not chaining */
+			curr_sg++;
+		}
+	}
+
+	if (use_sg + chained > h->maxSG)
+		h->maxSG = use_sg + chained;
+
+	if (chained) {
+		cp->Header.SGList = h->max_cmd_sg_entries;
+		cp->Header.SGTotal = (u16) (use_sg + 1);
+		hpsa_map_sg_chain_block(h, cp);
+		return 0;
+	}
+
+	cp->Header.SGList = (u8) use_sg;   /* no. SGs contig in this cmd */
+	cp->Header.SGTotal = (u16) use_sg; /* total sgs in this cmd list */
+	return 0;
+}
+
+static void hpsa_unmap_sg_chain_block(struct ctlr_info *h,
+	struct CommandList *c);
+static void hpsa_scatter_gather_unmap(struct ctlr_info *h,
+	struct CommandList *c, struct scsi_cmnd *cmd)
+{
+	union u64bit addr64;
+
+	if (cmd->use_sg) {
+		pci_unmap_sg(h->pdev, cmd->request_buffer, cmd->use_sg,
+			cmd->sc_data_direction);
+		if (c->Header.SGTotal > h->max_cmd_sg_entries)
+			hpsa_unmap_sg_chain_block(h, c);
+		return;
+	}
+	if (cmd->request_bufflen) {
+		addr64.val32.lower = c->SG[0].Addr.lower;
+		addr64.val32.upper = c->SG[0].Addr.upper;
+		pci_unmap_single(h->pdev, (dma_addr_t) addr64.val,
+		cmd->request_bufflen, cmd->sc_data_direction);
+	}
+}
+
+static inline void scsi_dma_unmap(struct scsi_cmnd *cmd)
+{
+	struct CommandList *c = (struct CommandList *) cmd->host_scribble;
+
+	hpsa_scatter_gather_unmap(c->h, c, cmd);
+}
+
+#endif
+
+#if !KFEATURE_HAS_SCSI_DEVICE_TYPE
+	/**
+	 * scsi_device_type - Return 17 char string indicating device type.
+	 * @type: type number to look up
+	 */
+	const char *scsi_device_type(unsigned type)
+	{
+		if (type == 0x1e)
+			return "Well-known LUN   ";
+		if (type == 0x1f)
+			return "No Device        ";
+		if (type >= ARRAY_SIZE(scsi_device_types))
+			return "Unknown          ";
+		return scsi_device_types[type];
+	}
+#endif
+
+#if KFEATURE_SCAN_START_IMPLEMENTED
+	/* .scan_start is present in scsi host template AND implemented.
+	 * Used to bail out of queuecommand if no scan_start and REPORT_LUNS
+	 * encountered
+	 */
+	static inline int hpsa_dummy_function_returning_zero(void)
+	{
+		return 0;
+	}
+
+#define bail_on_report_luns_if_no_scan_start(cmd, done) \
+		hpsa_dummy_function_returning_zero()
+
+	/* RHEL6, kernel.org have functioning ->scan_start() method in kernel
+	 * so this is no-op.
+	 */
+	static inline void hpsa_initial_update_scsi_devices(
+		__attribute__((unused)) struct ctlr_info *h)
+	{
+		return;
+	}
+#else /* not KFEATURE_SCAN_START_IMPLEMENTED */
+	static inline int bail_on_report_luns_if_no_scan_start(
+		struct scsi_cmnd *cmd, void (*done)(struct scsi_cmnd *))
+	{
+		/*
+		 * This thing bails out of our queue command early on SCSI
+		 * REPORT_LUNS This is needed when the kernel doesn't really
+		 * support the scan_start method of the scsi host template.
+		 *
+		 * Since we do our own mapping in our driver, and we handle
+		 * adding/removing of our own devices.
+		 *
+		 * We want to prevent the mid-layer from doing it's own
+		 * adding/removing of drives which is what it would do
+		 * if we allow REPORT_LUNS to be processed.
+		 *
+		 * On RHEL5, scsi mid-layer never calls scan_start and
+		 * scan_finished even though they exist in scsi_host_template.
+		 *
+		 * On RHEL6 we use scan_start and scan_finished to tell
+		 * mid-layer that we do our own device adding/removing
+		 * therefore we can handle REPORT_LUNS.
+		 */
+
+		if (cmd->cmnd[0] == REPORT_LUNS) {
+			cmd->result = (DID_OK << 16);           /* host byte */
+			cmd->result |= (COMMAND_COMPLETE << 8); /* msg byte */
+			cmd->result |= SAM_STAT_CHECK_CONDITION;
+			memset(cmd->sense_buffer, 0, sizeof(cmd->sense_buffer));
+			cmd->sense_buffer[2] = ILLEGAL_REQUEST;
+			done(cmd);
+			return 1;
+		}
+		return 0;
+	}
+
+	/* Need this if no functioning ->scan_start() method in kernel. */
+	static void hpsa_update_scsi_devices(struct ctlr_info *h, int hostno);
+	static inline void hpsa_initial_update_scsi_devices(
+				struct ctlr_info *h)
+	{
+		hpsa_update_scsi_devices(h, -1);
+	}
+#endif /* KFEATURE_SCAN_START_IMPLEMENTED */
+
+#if KFEATURE_SCAN_START_PRESENT
+	/* .scan_start is present in scsi host template */
+	#define INITIALIZE_SCAN_START(funcptr) .scan_start = funcptr,
+	#define INITIALIZE_SCAN_FINISHED(funcptr) .scan_finished = funcptr,
+#else /* .scan start is not even present in scsi host template */
+	#define INITIALIZE_SCAN_START(funcptr)
+	#define INITIALIZE_SCAN_FINISHED(funcptr)
+#endif
+
+#if KFEATURE_HAS_2011_03_QUEUECOMMAND
+#	define DECLARE_QUEUECOMMAND(func) \
+		static int func(struct Scsi_Host *sh, struct scsi_cmnd *cmd)
+#	define hpsa_scsi_done(cmd, done) \
+		(cmd)->scsi_done((cmd))
+#	define hpsa_save_scsi_done(cmd, done)
+#else
+#	define DECLARE_QUEUECOMMAND(func) \
+	static int func(struct scsi_cmnd *cmd, void (*done)(struct scsi_cmnd *))
+#	define hpsa_scsi_done(cmd, done) \
+		done((cmd))
+#	define hpsa_save_scsi_done(cmd, done) \
+		(cmd)->scsi_done = (done)
+#endif
+
+#if KFEATURE_HAS_HOST_LOCKLESS_FIELD
+#	define HPSA_SKIP_HOST_LOCK .lockless = 1,
+#else
+#	define HPSA_SKIP_HOST_LOCK
+#endif
+
+#if !KFEATURE_HAS_SCSI_SET_RESID
+	static inline void scsi_set_resid(struct scsi_cmnd *cmd, int resid)
+	{
+		cmd->resid = resid;
+	}
+#endif
+
+#ifndef DMA_BIT_MASK
+#define DMA_BIT_MASK(n) (((n) == 64) ? ~0ULL : ((1ULL<<(n))-1))
+#endif
+
+/* Define old style irq flags SA_* if the IRQF_* ones are missing. */
+#ifndef IRQF_DISABLED
+#define IRQF_DISABLED (SA_INTERRUPT | SA_SAMPLE_RANDOM)
+#endif
+
+#if KFEATURE_HAS_UACCESS_H_FILE
+#include <linux/uaccess.h>
+#endif
+
+#if KFEATURE_HAS_SMP_LOCK_H
+#include <linux/smp_lock.h>
+#endif
+
+/*
+ * Support for packaged storage solutions.
+ * Enabled by default for kernel.org 
+ * Enable above as required for distros.
+ */
+#if HPSA_SUPPORTS_STORAGEWORKS_1210m
+#define HPSA_STORAGEWORKS_1210m_PCI_IDS \
+	{PCI_VENDOR_ID_HP, PCI_DEVICE_ID_HP_CISSE, 0x103C, 0x3233}, \
+	{PCI_VENDOR_ID_HP, PCI_DEVICE_ID_HP_CISSF, 0x103C, 0x333F},	\
+	{PCI_VENDOR_ID_3PAR,	PCI_DEVICE_ID_3PAR,	0x1590, 0x0076},\
+	{PCI_VENDOR_ID_3PAR,	PCI_DEVICE_ID_3PAR,	0x1590, 0x007d},\
+	{PCI_VENDOR_ID_3PAR,	PCI_DEVICE_ID_3PAR,	0x1590, 0x0077},\
+	{PCI_VENDOR_ID_3PAR,	PCI_DEVICE_ID_3PAR,	0x1590, 0x0087},\
+	{PCI_VENDOR_ID_3PAR,	PCI_DEVICE_ID_3PAR,	0x1590, 0x0088},\
+	{PCI_VENDOR_ID_3PAR,	PCI_DEVICE_ID_3PAR,	0x1590, 0x0089},
+
+#define HPSA_STORAGEWORKS_1210m_PRODUCT_ENTRIES \
+	{0x3233103C, "HP StorageWorks 1210m", &SA5_access}, \
+	{0x333F103C, "HP StorageWorks 1210m", &SA5_access}, \
+   {0x00761590, "HP Storage P1224 Array Controller", &SA5_access}, \
+   {0x007d1590, "HP Storage P1228 Array Controller", &SA5_access}, \
+   {0x00771590, "HP Storage P1228m Array Controller", &SA5_access}, \
+   {0x00871590, "HP Storage P1224e Array Controller", &SA5_access}, \
+   {0x00881590, "HP Storage P1228e Array Controller", &SA5_access}, \
+   {0x00891590, "HP Storage P1228em Array Controller", &SA5_access},
+   
+
+#else
+#define HPSA_STORAGEWORKS_1210m_PCI_IDS	
+#define HPSA_STORAGEWORKS_1210m_PRODUCT_ENTRIES
+#endif
+
+/* sles10sp4 apparently doesn't have DIV_ROUND_UP.  Normally it comes
+ * from include/linux/kernel.h.  Other sles10's have it I think.
+ */
+#if !defined(DIV_ROUND_UP)
+#define DIV_ROUND_UP(n,d) (((n) + (d) - 1) / (d))
+#endif
+
+/* Newer dma_mapping_error function takes 2 args, older version only takes 1 arg.
+ * This macro makes the code do the right thing depending on which variant we have.
+ */
+#if KFEATURE_HAS_NEW_DMA_MAPPING_ERROR
+#define hpsa_dma_mapping_error(x, y) dma_mapping_error(x, y)
+#else
+#define hpsa_dma_mapping_error(x, y) dma_mapping_error(y)
+#endif
+
+#if !KFEATURE_HAS_IRQ_SET_AFFINITY_HINTS
+#define irq_set_affinity_hint(irq, cpu) (0)
+#endif
+
+#if !KFEATURE_HAS_ALLOC_WORKQUEUE
+/* Earlier implementations had no concept of WQ_MEM_RECLAIM so far as I know.
+ * FIXME: RHEL6 does not have WQ_MEM_RECLAIM flag.  What to do about this?
+ * WQ_MEM_RECLAIM is supposed to reserve a "rescue thread" so that the work
+ * queue can always make forward progress even in low memory situations. E.g.
+ * if OS is scrambling for memory and trying to swap out to disk, it is bad if
+ * the thread that is doing the swapping i/o needs to allocate.
+ */
+#define WQ_MEM_RECLAIM (0)
+#define alloc_workqueue(name, flags, max_active) __create_workqueue(name, flags, max_active, 0)
+#endif
+
+#if !KFEATURE_HAS_ATOMIC_DEC_IF_POSITIVE
+static inline int atomic_dec_if_positive(atomic_t *v)
+{
+	int c, old, dec;
+	c = atomic_read(v);
+	for (;;) {
+		dec = c - 1;
+		if (unlikely(dec < 0))
+			break;
+		old = atomic_cmpxchg((v), c, dec);
+		if (likely(old == c))
+			break;
+		c = old;
+	}
+	return dec;
+}
+#endif
+
+/* these next three disappeared in 3.8-rc4 */
+#ifndef __devinit
+#define __devinit
+#endif
+
+#ifndef __devexit
+#define __devexit
+#endif
+
+#ifndef __devexit_p
+#define __devexit_p(x) x
+#endif
+
+/* Kernel.org since about July 2013 has nice %XphN formatting for bytes
+ * Older kernels don't.  So we have this instead.
+ */
+#define phnbyte(x, n) ((int) ((x)[(n)]))
+#define phN16 "%02hhx%02hhx%02hhx%02hhx%02hhx%02hhx%02hhx%02hhx%02hhx%02hhx%02hhx%02hhx%02hhx%02hhx%02hhx%02hhx"
+#define phNbytes16(x) \
+	phnbyte((x), 0), phnbyte((x), 1), phnbyte((x), 2), phnbyte((x), 3), \
+	phnbyte((x), 4), phnbyte((x), 5), phnbyte((x), 6), phnbyte((x), 7), \
+	phnbyte((x), 8), phnbyte((x), 9), phnbyte((x), 10), phnbyte((x), 11), \
+	phnbyte((x), 12), phnbyte((x), 13), phnbyte((x), 14), phnbyte((x), 15)
+
-- 
2.0.2

