From 3567b2ade2464ba95c1281ce3211e000951bca8e Mon Sep 17 00:00:00 2001
From: Xufeng Zhang <xufeng.zhang@windriver.com>
Date: Tue, 9 Aug 2011 19:23:01 +0800
Subject: [PATCH 19/47] gianfar eTSEC TSO

Add TCP segmentation offload support in eTSEC driver.
Enable it by "ethtool -K eth0 tso on"

Extracted from vendor drop QorIQ-NONDPAA-SDK-V1-20110429_ltib.iso
linux-2.6.35-qoriq-gianfar-eTSEC-TSO.patch.

Signed-off-by: Jiajun Wu <b06378@freescale.com>

[Remove the obsolete do_tstamp variable.]

Signed-off-by: Xufeng Zhang <xufeng.zhang@windriver.com>
---
 drivers/net/gianfar.c |  290 +++++++++++++++++++++++++++++++++++++++++++++++++
 1 files changed, 290 insertions(+), 0 deletions(-)

diff --git a/drivers/net/gianfar.c b/drivers/net/gianfar.c
index 6648fe4..35942e6 100644
--- a/drivers/net/gianfar.c
+++ b/drivers/net/gianfar.c
@@ -2582,6 +2582,293 @@ static inline struct txbd8 *next_txbd(struct txbd8 *bdp, struct txbd8 *base,
 	return skip_txbd(bdp, 1, base, ring_size);
 }
 
+static int gfar_xmit_skb(struct sk_buff *skb, struct net_device *dev, int rq)
+{
+		struct gfar_private *priv = netdev_priv(dev);
+		struct gfar_priv_tx_q *tx_queue = NULL;
+		struct netdev_queue *txq;
+		struct gfar __iomem *regs = NULL;
+		struct txfcb *fcb = NULL;
+		struct txbd8 *txbdp, *txbdp_start, *base;
+		u32 lstatus;
+		int i;
+		u32 bufaddr;
+		unsigned long flags;
+		unsigned int nr_frags, length;
+
+		tx_queue = priv->tx_queue[rq];
+		txq = netdev_get_tx_queue(dev, rq);
+		base = tx_queue->tx_bd_base;
+		regs = tx_queue->grp->regs;
+
+		/* total number of fragments in the SKB */
+		nr_frags = skb_shinfo(skb)->nr_frags;
+
+		/* check if there is space to queue this packet */
+		if ((nr_frags+1) > tx_queue->num_txbdfree) {
+			/* no space, stop the queue */
+			netif_tx_stop_queue(txq);
+			dev->stats.tx_fifo_errors++;
+			return NETDEV_TX_BUSY;
+		}
+
+		/* Update transmit stats */
+		txq->tx_bytes += skb->len;
+		txq->tx_packets++;
+
+		txbdp = txbdp_start = tx_queue->cur_tx;
+
+		if (nr_frags == 0) {
+			lstatus = txbdp->lstatus | BD_LFLAG(TXBD_LAST | TXBD_INTERRUPT);
+		} else {
+			/* Place the fragment addresses and lengths into the TxBDs */
+			for (i = 0; i < nr_frags; i++) {
+				/* Point at the next BD, wrapping as needed */
+				txbdp = next_txbd(txbdp, base, tx_queue->tx_ring_size);
+
+				length = skb_shinfo(skb)->frags[i].size;
+
+				lstatus = txbdp->lstatus | length |
+					BD_LFLAG(TXBD_READY);
+
+				/* Handle the last BD specially */
+				if (i == nr_frags - 1)
+					lstatus |= BD_LFLAG(TXBD_LAST | TXBD_INTERRUPT);
+
+				bufaddr = dma_map_page(&priv->ofdev->dev,
+						skb_shinfo(skb)->frags[i].page,
+						skb_shinfo(skb)->frags[i].page_offset,
+						length,
+						DMA_TO_DEVICE);
+
+				/* set the TxBD length and buffer pointer */
+				txbdp->bufPtr = bufaddr;
+				txbdp->lstatus = lstatus;
+			}
+
+			lstatus = txbdp_start->lstatus;
+		}
+
+		/* Set up checksumming */
+		if (CHECKSUM_PARTIAL == skb->ip_summed) {
+			fcb = gfar_add_fcb(skb);
+			lstatus |= BD_LFLAG(TXBD_TOE);
+			gfar_tx_checksum(skb, fcb);
+		}
+
+		if (priv->vlgrp && vlan_tx_tag_present(skb)) {
+			if (unlikely(NULL == fcb)) {
+				fcb = gfar_add_fcb(skb);
+				lstatus |= BD_LFLAG(TXBD_TOE);
+			}
+
+			gfar_tx_vlan(skb, fcb);
+		}
+
+		/* setup the TxBD length and buffer pointer for the first BD */
+		tx_queue->tx_skbuff[tx_queue->skb_curtx] = skb;
+		txbdp_start->bufPtr = dma_map_single(&priv->ofdev->dev, skb->data,
+				skb_headlen(skb), DMA_TO_DEVICE);
+
+		lstatus |= BD_LFLAG(TXBD_CRC | TXBD_READY) | skb_headlen(skb);
+
+		/*
+		* We can work in parallel with gfar_clean_tx_ring(), except
+		* when modifying num_txbdfree. Note that we didn't grab the lock
+		* when we were reading the num_txbdfree and checking for available
+		* space, that's because outside of this function it can only grow,
+		* and once we've got needed space, it cannot suddenly disappear.
+		*
+		* The lock also protects us from gfar_error(), which can modify
+		* regs->tstat and thus retrigger the transfers, which is why we
+		* also must grab the lock before setting ready bit for the first
+		* to be transmitted BD.
+		*/
+		spin_lock_irqsave(&tx_queue->txlock, flags);
+
+		/*
+		 * The powerpc-specific eieio() is used, as wmb() has too strong
+		 * semantics (it requires synchronization between cacheable and
+		 * uncacheable mappings, which eieio doesn't provide and which we
+		 * don't need), thus requiring a more expensive sync instruction.  At
+		 * some point, the set of architecture-independent barrier functions
+		 * should be expanded to include weaker barriers.
+		 */
+		eieio();
+
+		txbdp_start->lstatus = lstatus;
+
+		/* Update the current skb pointer to the next entry we will use
+		 * (wrapping if necessary) */
+		tx_queue->skb_curtx = (tx_queue->skb_curtx + 1) &
+			TX_RING_MOD_MASK(tx_queue->tx_ring_size);
+
+		tx_queue->cur_tx = next_txbd(txbdp, base, tx_queue->tx_ring_size);
+
+		/* reduce TxBD free count */
+		tx_queue->num_txbdfree -= (nr_frags + 1);
+
+		txq->trans_start = jiffies;
+
+		/* If the next BD still needs to be cleaned up, then the bds
+		   are full.  We need to tell the kernel to stop sending us stuff. */
+		if (!tx_queue->num_txbdfree) {
+			netif_stop_subqueue(dev, tx_queue->qindex);
+
+			dev->stats.tx_fifo_errors++;
+		}
+
+		/* Tell the DMA to go go go */
+		gfar_write(&regs->tstat, TSTAT_CLEAR_THALT >> tx_queue->qindex);
+
+		/* Unlock priv */
+		spin_unlock_irqrestore(&tx_queue->txlock, flags);
+
+		return NETDEV_TX_OK;
+
+}
+
+/*software TCP segmentation offload*/
+static int gfar_tso(struct sk_buff *skb, struct net_device *dev, int rq)
+{
+	int i = 0;
+	struct iphdr *iph;
+	int ihl;
+	int id;
+	unsigned int offset = 0;
+	struct tcphdr *th;
+	unsigned thlen;
+	unsigned int seq;
+	__be32 delta;
+	unsigned int oldlen;
+	unsigned int mss;
+	unsigned int doffset;
+	unsigned int headroom;
+	unsigned int len;
+	int nfrags;
+	int pos;
+	int hsize;
+	int ret;
+
+	/*processing mac header*/
+	skb_reset_mac_header(skb);
+	skb->mac_len = skb->network_header - skb->mac_header;
+	__skb_pull(skb, skb->mac_len);
+
+	/*processing IP header*/
+	iph = ip_hdr(skb);
+	ihl = iph->ihl * 4;
+	__skb_pull(skb, ihl);
+	skb_reset_transport_header(skb);
+	iph = ip_hdr(skb);
+	id = ntohs(iph->id);
+
+	/*processing TCP header*/
+	th = tcp_hdr(skb);
+	thlen = th->doff * 4;
+	oldlen = (u16)~skb->len;
+	__skb_pull(skb, thlen);
+	mss = skb_shinfo(skb)->gso_size;
+	seq = ntohl(th->seq);
+	delta = htonl(oldlen + (thlen + mss));
+
+	/*processing SKB*/
+	doffset = skb->data - skb_mac_header(skb);
+	offset = doffset;
+	nfrags = skb_shinfo(skb)->nr_frags;
+	__skb_push(skb, doffset);
+	headroom = skb_headroom(skb);
+	pos = skb_headlen(skb);
+
+	/*duplicating SKB*/
+	hsize = skb_headlen(skb) - offset;
+	if (hsize < 0)
+		hsize = 0;
+
+	do {
+		struct sk_buff *nskb;
+		skb_frag_t *frag;
+		int size;
+
+		len = skb->len - offset;
+		if (len > mss)
+			len = mss;
+
+		nskb = alloc_skb(hsize + doffset + headroom,
+					 GFP_ATOMIC);
+		skb_reserve(nskb, headroom);
+		__skb_put(nskb, doffset+hsize);
+
+		nskb->ip_summed = skb->ip_summed;
+		nskb->vlan_tci = skb->vlan_tci;
+		nskb->mac_len = skb->mac_len;
+
+		skb_reset_mac_header(nskb);
+		skb_set_network_header(nskb, skb->mac_len);
+		nskb->transport_header = (nskb->network_header +
+					  skb_network_header_len(skb));
+		skb_copy_from_linear_data(skb, nskb->data, doffset+hsize);
+		frag = skb_shinfo(nskb)->frags;
+
+		/*move skb data*/
+		while (pos < offset + len && i < nfrags) {
+			*frag = skb_shinfo(skb)->frags[i];
+			get_page(frag->page);
+			size = frag->size;
+
+			if (pos < offset) {
+				frag->page_offset += offset - pos;
+				frag->size -= offset - pos;
+			}
+
+			skb_shinfo(nskb)->nr_frags++;
+
+			if (pos + size <= offset + len) {
+				i++;
+				pos += size;
+			} else {
+				frag->size -= pos + size - (offset + len);
+				goto skip_fraglist;
+			}
+
+			frag++;
+		}
+
+skip_fraglist:
+		nskb->data_len = len - hsize;
+		nskb->len += nskb->data_len;
+
+		/*update TCP header*/
+		if ((offset + len) >= skb->len)
+			delta = htonl(oldlen + (nskb->tail -
+				nskb->transport_header) + nskb->data_len);
+
+		th = tcp_hdr(nskb);
+		th->fin = th->psh = 0;
+		th->seq = htonl(seq);
+		th->cwr = 0;
+		seq += mss;
+		th->check = ~csum_fold((__force __wsum)((__force u32)th->check
+				+ (__force u32)delta));
+
+		/*update IP header*/
+		iph = ip_hdr(nskb);
+		iph->id = htons(id++);
+		iph->tot_len = htons(nskb->len - nskb->mac_len);
+		iph->check = 0;
+		iph->check = ip_fast_csum(skb_network_header(nskb), iph->ihl);
+		ret = gfar_xmit_skb(nskb, dev, rq);
+		if (unlikely(ret != NETDEV_TX_OK)) {
+			skb = nskb;
+			goto out_tso;
+		}
+	} while ((offset += len) < skb->len);
+
+out_tso:
+	dev_kfree_skb_any(skb);
+	return ret;
+}
+
 /* This is called by the kernel when a frame is ready for transmission. */
 /* It is pointed to by the dev->hard_start_xmit function pointer */
 static int gfar_start_xmit(struct sk_buff *skb, struct net_device *dev)
@@ -2635,6 +2922,9 @@ static int gfar_start_xmit(struct sk_buff *skb, struct net_device *dev)
 		skb = skb_new;
 	}
 
+	if (skb_shinfo(skb)->gso_size)
+		return gfar_tso(skb, dev, rq);
+
 	/* total number of fragments in the SKB */
 	nr_frags = skb_shinfo(skb)->nr_frags;
 
-- 
1.7.0.2

