From b9a0d9db73b2f4a009995f48e311cb447519b777 Mon Sep 17 00:00:00 2001
From: Lans Zhang <jia.zhang@windriver.com>
Date: Thu, 18 Dec 2014 10:34:22 +0800
Subject: [PATCH] Update hpsa driver to 3.4.6-170.

source http://softlayer-dal.dl.sourceforge.net/project/cciss/hpsa-3.0-tarballs/hpsa-3.4.6-170.tar.bz2

HP requests the integration for the latest HP SAS array driver and doesn't
have the patience to wait the upstream commits. This driver have been validated
by hp and us. Just simply replace the existing codes with this single patch and
once the upstream catches up this version we can revert this commit easily.

Signed-off-by: Lans Zhang <jia.zhang@windriver.com>

diff --git a/drivers/scsi/hpsa.c b/drivers/scsi/hpsa.c
index e98b41d9fad1..0a0408a397a1 100644
--- a/drivers/scsi/hpsa.c
+++ b/drivers/scsi/hpsa.c
@@ -1,6 +1,6 @@
 /*
  *    Disk Array driver for HP Smart Array SAS controllers
- *    Copyright 2000, 2014 Hewlett-Packard Development Company, L.P.
+ *    Copyright 2000, 2013 Hewlett-Packard Development Company, L.P.
  *
  *    This program is free software; you can redistribute it and/or modify
  *    it under the terms of the GNU General Public License as published by
@@ -23,7 +23,6 @@
 #include <linux/interrupt.h>
 #include <linux/types.h>
 #include <linux/pci.h>
-#include <linux/pci-aspm.h>
 #include <linux/kernel.h>
 #include <linux/slab.h>
 #include <linux/delay.h>
@@ -33,7 +32,6 @@
 #include <linux/spinlock.h>
 #include <linux/compat.h>
 #include <linux/blktrace_api.h>
-#include <linux/uaccess.h>
 #include <linux/io.h>
 #include <linux/dma-mapping.h>
 #include <linux/completion.h>
@@ -46,15 +44,17 @@
 #include <linux/cciss_ioctl.h>
 #include <linux/string.h>
 #include <linux/bitmap.h>
-#include <linux/atomic.h>
+#include <asm/atomic.h>
 #include <linux/jiffies.h>
 #include <linux/percpu.h>
+#include <linux/workqueue.h>
 #include <asm/div64.h>
 #include "hpsa_cmd.h"
 #include "hpsa.h"
+#include "hpsa_kernel_compat.h"
 
 /* HPSA_DRIVER_VERSION must be 3 byte values (0-255) separated by '.' */
-#define HPSA_DRIVER_VERSION "3.4.4-1"
+#define HPSA_DRIVER_VERSION "3.4.6-170"
 #define DRIVER_NAME "HP HPSA Driver (v " HPSA_DRIVER_VERSION ")"
 #define HPSA "hpsa"
 
@@ -68,7 +68,7 @@
 /* Embedded module documentation macros - see modules.h */
 MODULE_AUTHOR("Hewlett-Packard Company");
 MODULE_DESCRIPTION("Driver for HP Smart Array Controller version " \
-	HPSA_DRIVER_VERSION);
+	HPSA_DRIVER_VERSION " (d1059/s1248)");
 MODULE_SUPPORTED_DEVICE("HP Smart Array Controllers");
 MODULE_VERSION(HPSA_DRIVER_VERSION);
 MODULE_LICENSE("GPL");
@@ -81,17 +81,46 @@ static int hpsa_simple_mode;
 module_param(hpsa_simple_mode, int, S_IRUGO|S_IWUSR);
 MODULE_PARM_DESC(hpsa_simple_mode,
 	"Use 'simple mode' rather than 'performant mode'");
+static int reply_queues = 4;
+module_param(reply_queues, int, S_IRUGO|S_IWUSR);
+MODULE_PARM_DESC(reply_queues,
+	"Specify desired number of reply queues. 1-16, default is 4, not to exceed number of online CPUs.");
+
+#undef PCI_DEVICE_ID_HP_CISSF
+#ifndef PCI_DEVICE_ID_HP_CISSF
+#define PCI_DEVICE_ID_HP_CISSF 0x323B
+#endif
+#undef PCI_DEVICE_ID_HP_CISSG
+#ifndef PCI_DEVICE_ID_HP_CISSG
+#define PCI_DEVICE_ID_HP_CISSG 0x333F
+#endif
+#undef PCI_DEVICE_ID_HP_CISSH
+#ifndef PCI_DEVICE_ID_HP_CISSH
+#define PCI_DEVICE_ID_HP_CISSH 0x323C
+#endif
+#undef PCI_DEVICE_ID_HP_CISSI
+#ifndef PCI_DEVICE_ID_HP_CISSI
+#define PCI_DEVICE_ID_HP_CISSI 0x3239
+#endif
+#ifndef PCI_VENDOR_ID_3PAR
+#define PCI_VENDOR_ID_3PAR 0x1590
+#endif
+#ifndef PCI_DEVICE_ID_3PAR
+#define PCI_DEVICE_ID_3PAR 0x0075
+#endif
 
 /* define the PCI info for the cards we can control */
 static const struct pci_device_id hpsa_pci_device_id[] = {
+#if SA_CONTROLLERS_GEN6
 	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSE,     0x103C, 0x3241},
 	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSE,     0x103C, 0x3243},
 	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSE,     0x103C, 0x3245},
 	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSE,     0x103C, 0x3247},
 	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSE,     0x103C, 0x3249},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSE,     0x103C, 0x324A},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSE,     0x103C, 0x324B},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSE,     0x103C, 0x3233},
+	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSE,     0x103C, 0x324a},
+	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSE,     0x103C, 0x324b},
+#endif
+#if SA_CONTROLLERS_GEN8
 	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSF,     0x103C, 0x3350},
 	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSF,     0x103C, 0x3351},
 	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSF,     0x103C, 0x3352},
@@ -99,39 +128,47 @@ static const struct pci_device_id hpsa_pci_device_id[] = {
 	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSF,     0x103C, 0x3354},
 	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSF,     0x103C, 0x3355},
 	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSF,     0x103C, 0x3356},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSH,     0x103C, 0x1921},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSH,     0x103C, 0x1922},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSH,     0x103C, 0x1923},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSH,     0x103C, 0x1924},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSH,     0x103C, 0x1925},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSH,     0x103C, 0x1926},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSH,     0x103C, 0x1928},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSH,     0x103C, 0x1929},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21BD},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21BE},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21BF},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21C0},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21C1},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21C2},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21C3},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21C4},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21C5},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21C6},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21C7},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21C8},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21C9},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21CA},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21CB},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21CC},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21CD},
-	{PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSI,     0x103C, 0x21CE},
-	{PCI_VENDOR_ID_HP_3PAR, 0x0075, 0x1590, 0x0076},
-	{PCI_VENDOR_ID_HP_3PAR, 0x0075, 0x1590, 0x0087},
-	{PCI_VENDOR_ID_HP_3PAR, 0x0075, 0x1590, 0x007D},
-	{PCI_VENDOR_ID_HP_3PAR, 0x0075, 0x1590, 0x0088},
-	{PCI_VENDOR_ID_HP, 0x333f, 0x103c, 0x333f},
-	{PCI_VENDOR_ID_HP,     PCI_ANY_ID,	PCI_ANY_ID, PCI_ANY_ID,
-		PCI_CLASS_STORAGE_RAID << 8, 0xffff << 8, 0},
+#endif
+#if SA_CONTROLLERS_GEN8_2
+   {PCI_VENDOR_ID_HP,     PCI_DEVICE_ID_HP_CISSF,     0x103C, 0x334D},
+#endif
+#if SA_CONTROLLERS_GEN8_5
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSH,      0x103c, 0x1920},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSH,      0x103c, 0x1921},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSH,      0x103c, 0x1922},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSH,      0x103c, 0x1923},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSH,      0x103c, 0x1924},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSH,      0x103c, 0x1925},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSH,      0x103c, 0x1926},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSH,      0x103c, 0x1928},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSH,      0x103c, 0x1929},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSH,      0x103c, 0x192A},
+#endif
+#if SA_CONTROLLERS_GEN9
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSI,      0x103c, 0x21bd},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSI,      0x103c, 0x21be},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSI,      0x103c, 0x21bf},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSI,      0x103c, 0x21c0},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSI,      0x103c, 0x21c1},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSI,      0x103c, 0x21c2},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSI,      0x103c, 0x21c3},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSI,      0x103c, 0x21c4},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSI,      0x103c, 0x21c5},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSI,      0x103c, 0x21c6},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSI,      0x103c, 0x21c7},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSI,      0x103c, 0x21c8},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSI,      0x103c, 0x21c9},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSI,      0x103c, 0x21ca},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSI,      0x103c, 0x21cb},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSI,      0x103c, 0x21cc},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSI,      0x103c, 0x21cd},
+	{PCI_VENDOR_ID_HP,    PCI_DEVICE_ID_HP_CISSI,      0x103c, 0x21ce},
+#endif
+	HPSA_STORAGEWORKS_1210m_PCI_IDS
+        {PCI_VENDOR_ID_HP,     PCI_ANY_ID,       PCI_ANY_ID, PCI_ANY_ID,
+                PCI_CLASS_STORAGE_RAID << 8, 0xffff << 8, 0},
+        {PCI_VENDOR_ID_COMPAQ,     PCI_ANY_ID,   PCI_ANY_ID, PCI_ANY_ID,
+                PCI_CLASS_STORAGE_RAID << 8, 0xffff << 8, 0},
 	{0,}
 };
 
@@ -142,6 +179,7 @@ MODULE_DEVICE_TABLE(pci, hpsa_pci_device_id);
  *  access = Address of the struct of function pointers
  */
 static struct board_type products[] = {
+#if SA_CONTROLLERS_GEN6
 	{0x3241103C, "Smart Array P212", &SA5_access},
 	{0x3243103C, "Smart Array P410", &SA5_access},
 	{0x3245103C, "Smart Array P410i", &SA5_access},
@@ -149,77 +187,82 @@ static struct board_type products[] = {
 	{0x3249103C, "Smart Array P812", &SA5_access},
 	{0x324A103C, "Smart Array P712m", &SA5_access},
 	{0x324B103C, "Smart Array P711m", &SA5_access},
+#endif
+#if SA_CONTROLLERS_GEN8
 	{0x3350103C, "Smart Array P222", &SA5_access},
 	{0x3351103C, "Smart Array P420", &SA5_access},
 	{0x3352103C, "Smart Array P421", &SA5_access},
 	{0x3353103C, "Smart Array P822", &SA5_access},
 	{0x3354103C, "Smart Array P420i", &SA5_access},
 	{0x3355103C, "Smart Array P220i", &SA5_access},
-	{0x3356103C, "Smart Array P721m", &SA5_access},
+	{0x3356103C, "Smart Array P721", &SA5_access},
+#endif
+#if SA_CONTROLLERS_GEN8_2
+   {0x334D103C, "Smart Array", &SA5_access},
+#endif
+#if SA_CONTROLLERS_GEN8_5
+	{0x1920103C, "Smart Array P430i", &SA5_access},
 	{0x1921103C, "Smart Array P830i", &SA5_access},
 	{0x1922103C, "Smart Array P430", &SA5_access},
 	{0x1923103C, "Smart Array P431", &SA5_access},
 	{0x1924103C, "Smart Array P830", &SA5_access},
+	{0x1925103C, "Smart Array P831", &SA5_access},
 	{0x1926103C, "Smart Array P731m", &SA5_access},
 	{0x1928103C, "Smart Array P230i", &SA5_access},
 	{0x1929103C, "Smart Array P530", &SA5_access},
-	{0x21BD103C, "Smart Array", &SA5_access},
-	{0x21BE103C, "Smart Array", &SA5_access},
-	{0x21BF103C, "Smart Array", &SA5_access},
-	{0x21C0103C, "Smart Array", &SA5_access},
-	{0x21C1103C, "Smart Array", &SA5_access},
-	{0x21C2103C, "Smart Array", &SA5_access},
-	{0x21C3103C, "Smart Array", &SA5_access},
-	{0x21C4103C, "Smart Array", &SA5_access},
-	{0x21C5103C, "Smart Array", &SA5_access},
-	{0x21C6103C, "Smart Array", &SA5_access},
-	{0x21C7103C, "Smart Array", &SA5_access},
-	{0x21C8103C, "Smart Array", &SA5_access},
-	{0x21C9103C, "Smart Array", &SA5_access},
-	{0x21CA103C, "Smart Array", &SA5_access},
-	{0x21CB103C, "Smart Array", &SA5_access},
-	{0x21CC103C, "Smart Array", &SA5_access},
-	{0x21CD103C, "Smart Array", &SA5_access},
-	{0x21CE103C, "Smart Array", &SA5_access},
-	{0x00761590, "HP Storage P1224 Array Controller", &SA5_access},
-	{0x00871590, "HP Storage P1224e Array Controller", &SA5_access},
-	{0x007D1590, "HP Storage P1228 Array Controller", &SA5_access},
-	{0x00881590, "HP Storage P1228e Array Controller", &SA5_access},
-	{0x333f103c, "HP StorageWorks 1210m Array Controller", &SA5_access},
+	{0x192A103C, "Smart Array P531", &SA5_access},
+#endif
+#if SA_CONTROLLERS_GEN9
+	{0x21bd103C, "Smart Array P244br", &SA5_access},
+	{0x21be103C, "Smart Array P741m", &SA5_access},
+	{0x21bf103C, "Smart HBA H240ar", &SA5_access},
+	{0x21c0103C, "Smart Array P440ar", &SA5_access},
+	{0x21c1103C, "Smart Array", &SA5_access},
+	{0x21c2103C, "Smart Array P440", &SA5_access},
+	{0x21c3103C, "Smart Array P441", &SA5_access},
+	{0x21c4103C, "Smart Array", &SA5_access},
+	{0x21c5103C, "Smart Array", &SA5_access},
+	{0x21c6103C, "Smart HBA H244br", &SA5_access},
+	{0x21c7103C, "Smart HBA H240", &SA5_access},
+	{0x21c8103C, "Smart HBA H241", &SA5_access},
+	{0x21c9103C, "Smart Array", &SA5_access},
+	{0x21ca103C, "Smart Array", &SA5_access},
+	{0x21cb103C, "Smart Array P840", &SA5_access},
+	{0x21cc103C, "Smart Array", &SA5_access},
+	{0x21cd103C, "Smart Array", &SA5_access},
+	{0x21ce103C, "Smart HBA", &SA5_access},
+#endif
+	HPSA_STORAGEWORKS_1210m_PRODUCT_ENTRIES
 	{0xFFFF103C, "Unknown Smart Array", &SA5_access},
 };
 
 static int number_of_controllers;
 
-static irqreturn_t do_hpsa_intr_intx(int irq, void *dev_id);
-static irqreturn_t do_hpsa_intr_msi(int irq, void *dev_id);
+DECLARE_INTERRUPT_HANDLER(do_hpsa_intr_intx);
+DECLARE_INTERRUPT_HANDLER(do_hpsa_intr_msi);
 static int hpsa_ioctl(struct scsi_device *dev, int cmd, void *arg);
-static void lock_and_start_io(struct ctlr_info *h);
-static void start_io(struct ctlr_info *h, unsigned long *flags);
 
 #ifdef CONFIG_COMPAT
 static int hpsa_compat_ioctl(struct scsi_device *dev, int cmd, void *arg);
 #endif
 
 static void cmd_free(struct ctlr_info *h, struct CommandList *c);
-static void cmd_special_free(struct ctlr_info *h, struct CommandList *c);
 static struct CommandList *cmd_alloc(struct ctlr_info *h);
-static struct CommandList *cmd_special_alloc(struct ctlr_info *h);
 static int fill_cmd(struct CommandList *c, u8 cmd, struct ctlr_info *h,
 	void *buff, size_t size, u16 page_code, unsigned char *scsi3addr,
 	int cmd_type);
 #define VPD_PAGE (1 << 8)
 
-static int hpsa_scsi_queue_command(struct Scsi_Host *h, struct scsi_cmnd *cmd);
+DECLARE_QUEUECOMMAND(hpsa_scsi_queue_command);
 static void hpsa_scan_start(struct Scsi_Host *);
 static int hpsa_scan_finished(struct Scsi_Host *sh,
 	unsigned long elapsed_time);
-static int hpsa_change_queue_depth(struct scsi_device *sdev,
-	int qdepth, int reason);
+DECLARE_CHANGE_QUEUE_DEPTH(hpsa_change_queue_depth);
 
 static int hpsa_eh_device_reset_handler(struct scsi_cmnd *scsicmd);
 static int hpsa_eh_abort_handler(struct scsi_cmnd *scsicmd);
 static int hpsa_slave_alloc(struct scsi_device *sdev);
+static int hpsa_slave_configure(struct scsi_device *sdev);
 static void hpsa_slave_destroy(struct scsi_device *sdev);
 
 static void hpsa_update_scsi_devices(struct ctlr_info *h, int hostno);
@@ -230,25 +273,28 @@ static void check_ioctl_unit_attention(struct ctlr_info *h,
 /* performant mode helper functions */
 static void calc_bucket_map(int *bucket, int num_buckets,
 	int nsgs, int min_blocks, int *bucket_map);
-static void hpsa_put_ctlr_into_performant_mode(struct ctlr_info *h);
+static __devinit void hpsa_put_ctlr_into_performant_mode(struct ctlr_info *h);
 static inline u32 next_command(struct ctlr_info *h, u8 q);
-static int hpsa_find_cfg_addrs(struct pci_dev *pdev, void __iomem *vaddr,
-			       u32 *cfg_base_addr, u64 *cfg_base_addr_index,
-			       u64 *cfg_offset);
-static int hpsa_pci_find_memory_BAR(struct pci_dev *pdev,
-				    unsigned long *memory_bar);
-static int hpsa_lookup_board_id(struct pci_dev *pdev, u32 *board_id);
-static int hpsa_wait_for_board_state(struct pci_dev *pdev, void __iomem *vaddr,
-				     int wait_for_ready);
+static int hpsa_find_cfg_addrs(struct pci_dev *pdev,
+	void __iomem *vaddr, u32 *cfg_base_addr, u64 *cfg_base_addr_index,
+	u64 *cfg_offset);
+static int __devinit hpsa_pci_find_memory_BAR(struct pci_dev *pdev,
+	unsigned long *memory_bar);
+static int __devinit hpsa_lookup_board_id(struct pci_dev *pdev, u32 *board_id);
+static int hpsa_wait_for_board_state(struct pci_dev *pdev,
+	void __iomem *vaddr, int wait_for_ready);
+static inline u32 hpsa_tag_discard_error_bits(struct ctlr_info *h, u32 tag);
 static inline void finish_cmd(struct CommandList *c);
+static void hpsa_wait_for_clear_event_notify_ack(struct ctlr_info *h);
 static void hpsa_wait_for_mode_change_ack(struct ctlr_info *h);
 #define BOARD_NOT_READY 0
 #define BOARD_READY 1
 static void hpsa_drain_accel_commands(struct ctlr_info *h);
 static void hpsa_flush_cache(struct ctlr_info *h);
 static int hpsa_scsi_ioaccel_queue_command(struct ctlr_info *h,
-	struct CommandList *c, u32 ioaccel_handle, u8 *cdb, int cdb_len,
-	u8 *scsi3addr);
+        struct CommandList *c, u32 ioaccel_handle, u8 *cdb, int cdb_len,
+        u8 *scsi3addr);
+static void hpsa_command_resubmit_worker(struct work_struct *work);
 
 static inline struct ctlr_info *sdev_to_hba(struct scsi_device *sdev)
 {
@@ -262,24 +308,64 @@ static inline struct ctlr_info *shost_to_hba(struct Scsi_Host *sh)
 	return (struct ctlr_info *) *priv;
 }
 
+/* extract sense key, asc, and ascq from sense data.  -1 means invalid. */
+static void decode_sense_data(const u8 *sense_data, int sense_data_len,
+			int *sense_key, int *asc, int *ascq)
+{
+	if (sense_data_len < 1) {
+		*sense_key = -1;
+		*ascq = -1;
+		*asc = -1;
+		return;
+	}
+
+	switch (sense_data[0]) {
+	case 0x70: /* old format sense data */
+		*sense_key = (sense_data_len > 2) ? sense_data[2] & 0x0f : -1;
+		*ascq = (sense_data_len > 13) ?  sense_data[13] : -1;
+		*asc = (sense_data_len > 12) ?  sense_data[12] : -1;
+		break;
+	case 0x72: /* descriptor format sense data */
+		*sense_key = (sense_data_len > 1) ? sense_data[1] & 0x0f : -1;
+		*ascq = (sense_data_len > 2) ?  sense_data[2] : -1;
+		*asc = (sense_data_len > 3) ?  sense_data[3] : -1;
+		break;
+	default:
+		*sense_key = -1;
+		*ascq = -1;
+		*asc = -1;
+		break;
+	}
+}
+
 static int check_for_unit_attention(struct ctlr_info *h,
 	struct CommandList *c)
 {
-	if (c->err_info->SenseInfo[2] != UNIT_ATTENTION)
+	int sense_key, asc, ascq;
+	int sense_len;
+
+	if (c->err_info->SenseLen > sizeof(c->err_info->SenseInfo))
+		sense_len = sizeof(c->err_info->SenseInfo);
+	else
+		sense_len = c->err_info->SenseLen;
+
+	decode_sense_data(c->err_info->SenseInfo, sense_len,
+				&sense_key, &asc, &ascq);
+	if (sense_key != UNIT_ATTENTION || asc == -1)
 		return 0;
 
-	switch (c->err_info->SenseInfo[12]) {
+	switch (asc) {
 	case STATE_CHANGED:
 		dev_warn(&h->pdev->dev, HPSA "%d: a state change "
 			"detected, command retried\n", h->ctlr);
 		break;
 	case LUN_FAILED:
-		dev_warn(&h->pdev->dev, HPSA "%d: LUN failure "
-			"detected, action required\n", h->ctlr);
+		dev_warn(&h->pdev->dev,
+			HPSA "%d: LUN failure detected\n", h->ctlr);
 		break;
 	case REPORT_LUNS_CHANGED:
-		dev_warn(&h->pdev->dev, HPSA "%d: report LUN data "
-			"changed, action required\n", h->ctlr);
+		dev_warn(&h->pdev->dev,
+			HPSA "%d: report LUN data changed\n", h->ctlr);
 	/*
 	 * Note: this REPORT_LUNS_CHANGED condition only occurs on the external
 	 * target (array) devices.
@@ -304,16 +390,14 @@ static int check_for_unit_attention(struct ctlr_info *h,
 static int check_for_busy(struct ctlr_info *h, struct CommandList *c)
 {
 	if (c->err_info->CommandStatus != CMD_TARGET_STATUS ||
-		(c->err_info->ScsiStatus != SAM_STAT_BUSY &&
+		(c->err_info->ScsiStatus != SAM_STAT_BUSY && 
 		 c->err_info->ScsiStatus != SAM_STAT_TASK_SET_FULL))
 		return 0;
 	dev_warn(&h->pdev->dev, HPSA "device busy");
 	return 1;
 }
 
-static ssize_t host_store_hp_ssd_smart_path_status(struct device *dev,
-					 struct device_attribute *attr,
-					 const char *buf, size_t count)
+DECLARE_DEVATTR_STORE_FUNC(host_store_hp_ssd_smart_path_status)
 {
 	int status, len;
 	struct ctlr_info *h;
@@ -328,16 +412,19 @@ static ssize_t host_store_hp_ssd_smart_path_status(struct device *dev,
 	if (sscanf(tmpbuf, "%d", &status) != 1)
 		return -EINVAL;
 	h = shost_to_hba(shost);
+	if (h->hba_mode_enabled) {
+		dev_warn(&h->pdev->dev, "hpsa: HBA mode active, "
+			"cannot enable HP SSD Smart Path.\n");
+		return count;
+	}
 	h->acciopath_status = !!status;
-	dev_warn(&h->pdev->dev,
-		"hpsa: HP SSD Smart Path %s via sysfs update.\n",
+	dev_warn(&h->pdev->dev, "hpsa: HP SSD Smart Path %s "
+		"via sysfs update.\n",
 		h->acciopath_status ? "enabled" : "disabled");
 	return count;
 }
 
-static ssize_t host_store_raid_offload_debug(struct device *dev,
-					 struct device_attribute *attr,
-					 const char *buf, size_t count)
+DECLARE_DEVATTR_STORE_FUNC(host_store_raid_offload_debug)
 {
 	int debug_level, len;
 	struct ctlr_info *h;
@@ -351,7 +438,7 @@ static ssize_t host_store_raid_offload_debug(struct device *dev,
 	tmpbuf[len] = '\0';
 	if (sscanf(tmpbuf, "%d", &debug_level) != 1)
 		return -EINVAL;
-	if (debug_level < 0)
+	if (debug_level < 0 )
 		debug_level = 0;
 	h = shost_to_hba(shost);
 	h->raid_offload_debug = debug_level;
@@ -360,9 +447,7 @@ static ssize_t host_store_raid_offload_debug(struct device *dev,
 	return count;
 }
 
-static ssize_t host_store_rescan(struct device *dev,
-				 struct device_attribute *attr,
-				 const char *buf, size_t count)
+DECLARE_DEVATTR_STORE_FUNC(host_store_rescan)
 {
 	struct ctlr_info *h;
 	struct Scsi_Host *shost = class_to_shost(dev);
@@ -371,8 +456,7 @@ static ssize_t host_store_rescan(struct device *dev,
 	return count;
 }
 
-static ssize_t host_show_firmware_revision(struct device *dev,
-	     struct device_attribute *attr, char *buf)
+DECLARE_DEVATTR_SHOW_FUNC(host_show_firmware_revision)
 {
 	struct ctlr_info *h;
 	struct Scsi_Host *shost = class_to_shost(dev);
@@ -386,17 +470,16 @@ static ssize_t host_show_firmware_revision(struct device *dev,
 		fwrev[0], fwrev[1], fwrev[2], fwrev[3]);
 }
 
-static ssize_t host_show_commands_outstanding(struct device *dev,
-	     struct device_attribute *attr, char *buf)
+DECLARE_DEVATTR_SHOW_FUNC(host_show_commands_outstanding)
 {
 	struct Scsi_Host *shost = class_to_shost(dev);
 	struct ctlr_info *h = shost_to_hba(shost);
 
-	return snprintf(buf, 20, "%d\n", h->commands_outstanding);
+	return snprintf(buf, 20, "%d\n",
+			atomic_read(&h->commands_outstanding));
 }
 
-static ssize_t host_show_transport_mode(struct device *dev,
-	struct device_attribute *attr, char *buf)
+DECLARE_DEVATTR_SHOW_FUNC(host_show_transport_mode)
 {
 	struct ctlr_info *h;
 	struct Scsi_Host *shost = class_to_shost(dev);
@@ -407,17 +490,17 @@ static ssize_t host_show_transport_mode(struct device *dev,
 			"performant" : "simple");
 }
 
-static ssize_t host_show_hp_ssd_smart_path_status(struct device *dev,
-	struct device_attribute *attr, char *buf)
+DECLARE_DEVATTR_SHOW_FUNC(host_show_hp_ssd_smart_path_status)
 {
 	struct ctlr_info *h;
 	struct Scsi_Host *shost = class_to_shost(dev);
-
 	h = shost_to_hba(shost);
 	return snprintf(buf, 30, "HP SSD Smart Path %s\n",
-		(h->acciopath_status == 1) ?  "enabled" : "disabled");
+		(h->acciopath_status == 1) ?
+		"enabled" : "disabled");
 }
 
+
 /* List of controllers which cannot be hard reset on kexec with reset_devices */
 static u32 unresettable_controller[] = {
 	0x324a103C, /* Smart Array P712m */
@@ -432,26 +515,12 @@ static u32 unresettable_controller[] = {
 	0x3215103C, /* Smart Array E200i */
 	0x3237103C, /* Smart Array E500 */
 	0x323D103C, /* Smart Array P700m */
-	0x40800E11, /* Smart Array 5i */
 	0x409C0E11, /* Smart Array 6400 */
 	0x409D0E11, /* Smart Array 6400 EM */
-	0x40700E11, /* Smart Array 5300 */
-	0x40820E11, /* Smart Array 532 */
-	0x40830E11, /* Smart Array 5312 */
-	0x409A0E11, /* Smart Array 641 */
-	0x409B0E11, /* Smart Array 642 */
-	0x40910E11, /* Smart Array 6i */
 };
 
 /* List of controllers which cannot even be soft reset */
 static u32 soft_unresettable_controller[] = {
-	0x40800E11, /* Smart Array 5i */
-	0x40700E11, /* Smart Array 5300 */
-	0x40820E11, /* Smart Array 532 */
-	0x40830E11, /* Smart Array 5312 */
-	0x409A0E11, /* Smart Array 641 */
-	0x409B0E11, /* Smart Array 642 */
-	0x40910E11, /* Smart Array 6i */
 	/* Exclude 640x boards.  These are two pci devices in one slot
 	 * which share a battery backed cache module.  One controls the
 	 * cache, the other accesses the cache through the one that controls
@@ -463,24 +532,31 @@ static u32 soft_unresettable_controller[] = {
 	0x409D0E11, /* Smart Array 6400 EM */
 };
 
-static int ctlr_is_hard_resettable(u32 board_id)
+static u32 needs_abort_tags_swizzled[] = {
+	0x324a103C, /* Smart Array P712m */
+	0x324b103C, /* SmartArray P711m */
+};
+
+static int board_id_in_array(u32 a[], int nelems, u32 board_id)
 {
 	int i;
 
-	for (i = 0; i < ARRAY_SIZE(unresettable_controller); i++)
-		if (unresettable_controller[i] == board_id)
-			return 0;
-	return 1;
+	for (i = 0; i < nelems; i++)
+		if (a[i] == board_id)
+			return 1;
+	return 0;
 }
 
-static int ctlr_is_soft_resettable(u32 board_id)
+static int ctlr_is_hard_resettable(u32 board_id)
 {
-	int i;
+	return !board_id_in_array(unresettable_controller,
+			ARRAY_SIZE(unresettable_controller), board_id);
+}
 
-	for (i = 0; i < ARRAY_SIZE(soft_unresettable_controller); i++)
-		if (soft_unresettable_controller[i] == board_id)
-			return 0;
-	return 1;
+static int ctlr_is_soft_resettable(u32 board_id)
+{
+	return !board_id_in_array(soft_unresettable_controller,
+			ARRAY_SIZE(soft_unresettable_controller), board_id);
 }
 
 static int ctlr_is_resettable(u32 board_id)
@@ -489,8 +565,7 @@ static int ctlr_is_resettable(u32 board_id)
 		ctlr_is_soft_resettable(board_id);
 }
 
-static ssize_t host_show_resettable(struct device *dev,
-	struct device_attribute *attr, char *buf)
+DECLARE_DEVATTR_SHOW_FUNC(host_show_resettable)
 {
 	struct ctlr_info *h;
 	struct Scsi_Host *shost = class_to_shost(dev);
@@ -499,6 +574,20 @@ static ssize_t host_show_resettable(struct device *dev,
 	return snprintf(buf, 20, "%d\n", ctlr_is_resettable(h->board_id));
 }
 
+DECLARE_DEVATTR_SHOW_FUNC(host_show_heartbeat)
+{
+	struct ctlr_info *h;
+	struct Scsi_Host *shost = class_to_shost(dev);
+	u32 heartbeat;
+	unsigned long flags;
+
+	h = shost_to_hba(shost);
+	spin_lock_irqsave(&h->lock, flags);
+	heartbeat = readl(&h->cfgtable->HeartBeat);
+	spin_unlock_irqrestore(&h->lock, flags);
+	return snprintf(buf, 20, "0x%08x\n", heartbeat);
+}
+
 static inline int is_logical_dev_addr_mode(unsigned char scsi3addr[])
 {
 	return (scsi3addr[3] & 0xC0) == 0x40;
@@ -509,7 +598,7 @@ static const char *raid_label[] = { "0", "4", "1(1+0)", "5", "5+1", "ADG",
 };
 #define HPSA_RAID_0	0
 #define HPSA_RAID_4	1
-#define HPSA_RAID_1	2	/* also used for RAID 10 */
+#define HPSA_RAID_1	2 	/* also used for RAID 10 */
 #define HPSA_RAID_5	3	/* also used for RAID 50 */
 #define HPSA_RAID_51	4
 #define HPSA_RAID_6	5	/* also used for RAID 60 */
@@ -550,6 +639,44 @@ static ssize_t raid_level_show(struct device *dev,
 	return l;
 }
 
+static ssize_t host_store_lockup_detector(struct device *dev,
+				struct device_attribute *attr,
+				const char *buf, size_t count)
+{
+	struct Scsi_Host *shost;
+	struct ctlr_info *h;
+	int len, enabled;
+	char tmpbuf[10];
+
+	if (!capable(CAP_SYS_ADMIN) || !capable(CAP_SYS_RAWIO))
+		return -EACCES;
+	len = count > sizeof(tmpbuf) - 1 ? sizeof(tmpbuf) - 1 : count;
+	strncpy(tmpbuf, buf, len);
+	tmpbuf[len] = '\0';
+	if (sscanf(tmpbuf, "%d", &enabled) != 1)
+		return -EINVAL;
+	shost = class_to_shost(dev);
+	h = shost_to_hba(shost);
+	h->lockup_detector_enabled = !!enabled;
+	return count;
+}
+
+static int ctlr_needs_abort_tags_swizzled(u32 board_id)
+{
+	return board_id_in_array(needs_abort_tags_swizzled,
+			ARRAY_SIZE(needs_abort_tags_swizzled), board_id);
+}
+
+static ssize_t host_show_lockup_detector(struct device *dev,
+	     struct device_attribute *attr, char *buf)
+{
+	struct ctlr_info *h;
+	struct Scsi_Host *shost = class_to_shost(dev);
+
+	h = shost_to_hba(shost);
+	return snprintf(buf, 20, "%d\n", h->lockup_detector_enabled);
+}
+
 static ssize_t lunid_show(struct device *dev,
 	     struct device_attribute *attr, char *buf)
 {
@@ -602,7 +729,7 @@ static ssize_t unique_id_show(struct device *dev,
 			sn[12], sn[13], sn[14], sn[15]);
 }
 
-static ssize_t host_show_hp_ssd_smart_path_enabled(struct device *dev,
+static ssize_t hp_ssd_smart_path_enabled(struct device *dev,
 	     struct device_attribute *attr, char *buf)
 {
 	struct ctlr_info *h;
@@ -624,42 +751,114 @@ static ssize_t host_show_hp_ssd_smart_path_enabled(struct device *dev,
 	return snprintf(buf, 20, "%d\n", offload_enabled);
 }
 
+#define MAX_PATHS 8
+#define PATH_STRING_LEN 30
+static ssize_t path_info_show(struct device *dev,
+	     struct device_attribute *attr, char *buf)
+{
+	struct ctlr_info *h;
+	struct scsi_device *sdev;
+	struct hpsa_scsi_dev_t *hdev;
+	unsigned long flags;
+	int i;
+	int output_len = 0;
+	u8 box;
+	u8 bay;
+	u8 path_map_index = 0;
+	unsigned char active;
+	unsigned char phys_connector[2];
+	unsigned char path[MAX_PATHS][PATH_STRING_LEN];
+
+	memset(path, 0, MAX_PATHS * PATH_STRING_LEN);
+	sdev = to_scsi_device(dev);
+	h = sdev_to_hba(sdev);
+	spin_lock_irqsave(&h->devlock, flags);
+	hdev = sdev->hostdata;
+	if (!hdev) {
+		spin_unlock_irqrestore(&h->devlock, flags);
+		return -ENODEV;
+	}
+	bay = hdev->bay;
+	for (i = 0; i < MAX_PATHS; i++) {
+		box = hdev->box[i];
+		memcpy(&phys_connector, &hdev->phys_connector[i],
+			sizeof(phys_connector));
+		path_map_index = 1<<i;
+		if (i == hdev->active_path_index)
+			active = 'A';
+		else if (hdev->path_map & path_map_index)
+			active = 'I';
+		else
+			continue;
+		if (hdev->devtype == TYPE_DISK) {
+			if (box == 0 || box == 0xFF)
+				output_len += snprintf(path[i],
+					PATH_STRING_LEN,
+					"PORT: %.2s BAY: %hhu %c\n",
+					phys_connector, bay, active);
+			else
+				output_len += snprintf(path[i],
+					PATH_STRING_LEN,
+					"PORT: %.2s BOX: %hhu BAY: %hhu %c\n",
+					phys_connector, box, bay, active);
+		} else if (box != 0 && box != 0xFF) {
+			output_len += snprintf(path[i],
+				PATH_STRING_LEN, "PORT: %.2s BOX: %hhu %c\n",
+				phys_connector, box, active);
+		} else {
+			output_len += snprintf(path[i], PATH_STRING_LEN,
+				"PORT: %.2s %c\n", phys_connector, active);
+		}
+	}
+
+	spin_unlock_irqrestore(&h->devlock, flags);
+	return snprintf(buf, output_len+1, "%s%s%s%s%s%s%s%s",
+		path[0], path[1], path[2], path[3],
+		path[4], path[5], path[6], path[7]);
+}
+
 static DEVICE_ATTR(raid_level, S_IRUGO, raid_level_show, NULL);
 static DEVICE_ATTR(lunid, S_IRUGO, lunid_show, NULL);
 static DEVICE_ATTR(unique_id, S_IRUGO, unique_id_show, NULL);
-static DEVICE_ATTR(rescan, S_IWUSR, NULL, host_store_rescan);
 static DEVICE_ATTR(hp_ssd_smart_path_enabled, S_IRUGO,
-			host_show_hp_ssd_smart_path_enabled, NULL);
-static DEVICE_ATTR(hp_ssd_smart_path_status, S_IWUSR|S_IRUGO|S_IROTH,
-		host_show_hp_ssd_smart_path_status,
-		host_store_hp_ssd_smart_path_status);
-static DEVICE_ATTR(raid_offload_debug, S_IWUSR, NULL,
-			host_store_raid_offload_debug);
-static DEVICE_ATTR(firmware_revision, S_IRUGO,
+			hp_ssd_smart_path_enabled, NULL);
+static DEVICE_ATTR(path_info, S_IRUGO, path_info_show, NULL);
+DECLARE_HOST_DEVICE_ATTR(hp_ssd_smart_path_status, S_IWUSR|S_IRUGO|S_IROTH,
+	host_show_hp_ssd_smart_path_status, host_store_hp_ssd_smart_path_status);
+DECLARE_HOST_DEVICE_ATTR(raid_offload_debug, S_IWUSR, NULL, host_store_raid_offload_debug);
+DECLARE_HOST_DEVICE_ATTR(rescan, S_IWUSR, NULL, host_store_rescan);
+DECLARE_HOST_DEVICE_ATTR(firmware_revision, S_IRUGO,
 	host_show_firmware_revision, NULL);
-static DEVICE_ATTR(commands_outstanding, S_IRUGO,
+DECLARE_HOST_DEVICE_ATTR(commands_outstanding, S_IRUGO,
 	host_show_commands_outstanding, NULL);
-static DEVICE_ATTR(transport_mode, S_IRUGO,
+DECLARE_HOST_DEVICE_ATTR(transport_mode, S_IRUGO,
 	host_show_transport_mode, NULL);
-static DEVICE_ATTR(resettable, S_IRUGO,
+DECLARE_HOST_DEVICE_ATTR(resettable, S_IRUGO,
 	host_show_resettable, NULL);
+DECLARE_HOST_DEVICE_ATTR(heartbeat, S_IRUGO,
+	host_show_heartbeat, NULL);
+DECLARE_HOST_DEVICE_ATTR(lockup_detector, S_IWUSR|S_IRUGO,
+	host_show_lockup_detector, host_store_lockup_detector);
 
 static struct device_attribute *hpsa_sdev_attrs[] = {
 	&dev_attr_raid_level,
 	&dev_attr_lunid,
 	&dev_attr_unique_id,
 	&dev_attr_hp_ssd_smart_path_enabled,
+	&dev_attr_path_info,
 	NULL,
 };
 
-static struct device_attribute *hpsa_shost_attrs[] = {
+DECLARE_HOST_ATTR_LIST(hpsa_shost_attrs) = {
 	&dev_attr_rescan,
 	&dev_attr_firmware_revision,
 	&dev_attr_commands_outstanding,
 	&dev_attr_transport_mode,
 	&dev_attr_resettable,
-	&dev_attr_hp_ssd_smart_path_status,
+	&dev_attr_heartbeat,
 	&dev_attr_raid_offload_debug,
+	&dev_attr_hp_ssd_smart_path_status,
+	&dev_attr_lockup_detector,
 	NULL,
 };
 
@@ -668,8 +867,9 @@ static struct scsi_host_template hpsa_driver_template = {
 	.name			= HPSA,
 	.proc_name		= HPSA,
 	.queuecommand		= hpsa_scsi_queue_command,
-	.scan_start		= hpsa_scan_start,
-	.scan_finished		= hpsa_scan_finished,
+	INITIALIZE_SCAN_START(hpsa_scan_start)
+	INITIALIZE_SCAN_FINISHED(hpsa_scan_finished)
+	HPSA_SKIP_HOST_LOCK
 	.change_queue_depth	= hpsa_change_queue_depth,
 	.this_id		= -1,
 	.use_clustering		= ENABLE_CLUSTERING,
@@ -677,6 +877,7 @@ static struct scsi_host_template hpsa_driver_template = {
 	.eh_device_reset_handler = hpsa_eh_device_reset_handler,
 	.ioctl			= hpsa_ioctl,
 	.slave_alloc		= hpsa_slave_alloc,
+	.slave_configure	= hpsa_slave_configure,
 	.slave_destroy		= hpsa_slave_destroy,
 #ifdef CONFIG_COMPAT
 	.compat_ioctl		= hpsa_compat_ioctl,
@@ -684,21 +885,12 @@ static struct scsi_host_template hpsa_driver_template = {
 	.sdev_attrs = hpsa_sdev_attrs,
 	.shost_attrs = hpsa_shost_attrs,
 	.max_sectors = 8192,
-	.no_write_same = 1,
 };
 
-
-/* Enqueuing and dequeuing functions for cmdlists. */
-static inline void addQ(struct list_head *list, struct CommandList *c)
-{
-	list_add_tail(&c->list, list);
-}
-
 static inline u32 next_command(struct ctlr_info *h, u8 q)
 {
 	u32 a;
 	struct reply_queue_buffer *rq = &h->reply_queue[q];
-	unsigned long flags;
 
 	if (h->transMethod & CFGTBL_Trans_io_accel1)
 		return h->access.command_completed(h, q);
@@ -709,9 +901,7 @@ static inline u32 next_command(struct ctlr_info *h, u8 q)
 	if ((rq->head[rq->current_entry] & 1) == rq->wraparound) {
 		a = rq->head[rq->current_entry];
 		rq->current_entry++;
-		spin_lock_irqsave(&h->lock, flags);
-		h->commands_outstanding--;
-		spin_unlock_irqrestore(&h->lock, flags);
+		atomic_dec(&h->commands_outstanding);
 	} else {
 		a = FIFO_EMPTY;
 	}
@@ -737,7 +927,7 @@ static inline u32 next_command(struct ctlr_info *h, u8 q)
  * bit 0 = "performant mode" bit.
  * bits 1-3 = block fetch table entry
  * bits 4-6 = command type (== 110)
- * (command type is needed because ioaccel1 mode
+ * (command type is needed because ioaccel1 mode 
  * commands are submitted through the same register as normal
  * mode commands, so this is how the controller knows whether
  * the command is normal mode or ioaccel1 mode.)
@@ -753,25 +943,35 @@ static inline u32 next_command(struct ctlr_info *h, u8 q)
  * set bit 0 for pull model, bits 3-1 for block fetch
  * register number
  */
-static void set_performant_mode(struct ctlr_info *h, struct CommandList *c)
+#define DEFAULT_REPLY_QUEUE (-1)
+static void set_performant_mode(struct ctlr_info *h, struct CommandList *c,
+					int reply_queue)
 {
 	if (likely(h->transMethod & CFGTBL_Trans_Performant)) {
 		c->busaddr |= 1 | (h->blockFetchTable[c->Header.SGList] << 1);
-		if (likely(h->msix_vector > 0))
+		if (unlikely(!h->msix_vector))
+			return;
+		if (likely(reply_queue == DEFAULT_REPLY_QUEUE))
 			c->Header.ReplyQueue =
 				raw_smp_processor_id() % h->nreply_queues;
+		else
+			c->Header.ReplyQueue = reply_queue % h->nreply_queues;
 	}
 }
 
 static void set_ioaccel1_performant_mode(struct ctlr_info *h,
-						struct CommandList *c)
+						struct CommandList *c,
+						int reply_queue)
 {
 	struct io_accel1_cmd *cp = &h->ioaccel_cmd_pool[c->cmdindex];
 
 	/* Tell the controller to post the reply to the queue for this
 	 * processor.  This seems to give the best I/O throughput.
 	 */
-	cp->ReplyQueue = smp_processor_id() % h->nreply_queues;
+	if (likely(reply_queue == DEFAULT_REPLY_QUEUE))
+		cp->ReplyQueue = smp_processor_id() % h->nreply_queues;
+	else
+		cp->ReplyQueue = reply_queue % h->nreply_queues;
 	/* Set the bits in the address sent down to include:
 	 *  - performant mode bit (bit 0)
 	 *  - pull count (bits 1-3)
@@ -782,14 +982,18 @@ static void set_ioaccel1_performant_mode(struct ctlr_info *h,
 }
 
 static void set_ioaccel2_performant_mode(struct ctlr_info *h,
-						struct CommandList *c)
+						struct CommandList *c,
+						int reply_queue)
 {
 	struct io_accel2_cmd *cp = &h->ioaccel2_cmd_pool[c->cmdindex];
 
 	/* Tell the controller to post the reply to the queue for this
 	 * processor.  This seems to give the best I/O throughput.
 	 */
-	cp->reply_queue = smp_processor_id() % h->nreply_queues;
+	if (likely(reply_queue == DEFAULT_REPLY_QUEUE))
+		cp->reply_queue = smp_processor_id() % h->nreply_queues;
+	else
+		cp->reply_queue = reply_queue % h->nreply_queues;
 	/* Set the bits in the address sent down to include:
 	 *  - performant mode bit not used in ioaccel mode 2
 	 *  - pull count (bits 0-3)
@@ -827,34 +1031,33 @@ static void dial_up_lockup_detection_on_fw_flash_complete(struct ctlr_info *h,
 		h->heartbeat_sample_interval = HEARTBEAT_SAMPLE_INTERVAL;
 }
 
-static void enqueue_cmd_and_start_io(struct ctlr_info *h,
-	struct CommandList *c)
+static void __enqueue_cmd_and_start_io(struct ctlr_info *h,
+	struct CommandList *c, int reply_queue)
 {
-	unsigned long flags;
+	int count;
 
+	dial_down_lockup_detection_during_fw_flash(h, c);
+	atomic_inc(&h->commands_outstanding);
+	count = atomic_read(&h->commands_outstanding);
 	switch (c->cmd_type) {
 	case CMD_IOACCEL1:
-		set_ioaccel1_performant_mode(h, c);
+		set_ioaccel1_performant_mode(h, c, reply_queue);
+		writel(c->busaddr, h->vaddr + SA5_REQUEST_PORT_OFFSET);
 		break;
 	case CMD_IOACCEL2:
-		set_ioaccel2_performant_mode(h, c);
+		set_ioaccel2_performant_mode(h, c, reply_queue);
+		writel(c->busaddr, h->vaddr + IOACCEL2_INBOUND_POSTQ_32);
 		break;
 	default:
-		set_performant_mode(h, c);
+		set_performant_mode(h, c, reply_queue);
+		h->access.submit_command(h, c);
 	}
-	dial_down_lockup_detection_during_fw_flash(h, c);
-	spin_lock_irqsave(&h->lock, flags);
-	addQ(&h->reqQ, c);
-	h->Qdepth++;
-	start_io(h, &flags);
-	spin_unlock_irqrestore(&h->lock, flags);
 }
 
-static inline void removeQ(struct CommandList *c)
+static void enqueue_cmd_and_start_io(struct ctlr_info *h,
+					struct CommandList *c)
 {
-	if (WARN_ON(list_empty(&c->list)))
-		return;
-	list_del_init(&c->list);
+	__enqueue_cmd_and_start_io(h, c, DEFAULT_REPLY_QUEUE);
 }
 
 static inline int is_hba_lunid(unsigned char scsi3addr[])
@@ -986,12 +1189,27 @@ static void hpsa_scsi_update_entry(struct ctlr_info *h, int hostno,
 	/* Raid level changed. */
 	h->dev[entry]->raid_level = new_entry->raid_level;
 
-	/* Raid offload parameters changed. */
+	/* Raid offload parameters changed.  Careful about the ordering. */
+	if (new_entry->offload_config && new_entry->offload_enabled) {
+		/* if drive is newly offload_enabled, we want to copy the
+		 * raid map data first.  If previously offload_enabled and
+		 * offload_config were set, raid map data had better be
+		 * the same as it was before.  if raid map data is changed
+		 * then it had better be the case that
+		 * h->dev[entry]->offload_enabled is currently 0.
+		 */
+		h->dev[entry]->raid_map = new_entry->raid_map;
+		h->dev[entry]->ioaccel_handle = new_entry->ioaccel_handle;
+		wmb(); /* ensure raid map updated prior to ->offload_enabled */
+	}
+	if (new_entry->hba_ioaccel_enabled) {
+		h->dev[entry]->ioaccel_handle = new_entry->ioaccel_handle;
+		wmb(); /* set ioaccel_handle *before* hba_ioaccel_eanbled */
+	}
+	h->dev[entry]->hba_ioaccel_enabled = new_entry->hba_ioaccel_enabled;
 	h->dev[entry]->offload_config = new_entry->offload_config;
-	h->dev[entry]->offload_enabled = new_entry->offload_enabled;
-	h->dev[entry]->ioaccel_handle = new_entry->ioaccel_handle;
 	h->dev[entry]->offload_to_mirror = new_entry->offload_to_mirror;
-	h->dev[entry]->raid_map = new_entry->raid_map;
+	h->dev[entry]->offload_enabled = new_entry->offload_enabled;
 
 	dev_info(&h->pdev->dev, "%s device c%db%dt%dl%d updated.\n",
 		scsi_device_type(new_entry->devtype), hostno, new_entry->bus,
@@ -1148,7 +1366,7 @@ static int hpsa_scsi_find_entry(struct hpsa_scsi_dev_t *needle,
 				return DEVICE_SAME;
 			} else {
 				/* Keep offline devices offline */
-				if (needle->volume_offline)
+				if (needle->volume_offline) 
 					return DEVICE_NOT_FOUND;
 				return DEVICE_CHANGED;
 			}
@@ -1168,7 +1386,7 @@ static void hpsa_monitor_offline_device(struct ctlr_info *h,
 	spin_lock_irqsave(&h->offline_device_lock, flags);
 	list_for_each_entry(device, &h->offline_device_list, offline_list) {
 		if (memcmp(device->scsi3addr, scsi3addr,
-			sizeof(device->scsi3addr)) == 0) {
+				sizeof(device->scsi3addr)) == 0) {
 			spin_unlock_irqrestore(&h->offline_device_lock, flags);
 			return;
 		}
@@ -1193,72 +1411,87 @@ static void hpsa_show_volume_status(struct ctlr_info *h,
 {
 	if (sd->volume_offline == HPSA_VPD_LV_STATUS_UNSUPPORTED)
 		dev_info(&h->pdev->dev,
-			"C%d:B%d:T%d:L%d Volume status is not available through vital product data pages.\n",
+			"C%d:B%d:T%d:L%d Volume status is not "
+			"available through vital product data pages.\n",
 			h->scsi_host->host_no,
 			sd->bus, sd->target, sd->lun);
 	switch (sd->volume_offline) {
-	case HPSA_LV_OK:
-		break;
-	case HPSA_LV_UNDERGOING_ERASE:
-		dev_info(&h->pdev->dev,
-			"C%d:B%d:T%d:L%d Volume is undergoing background erase process.\n",
-			h->scsi_host->host_no,
-			sd->bus, sd->target, sd->lun);
-		break;
-	case HPSA_LV_UNDERGOING_RPI:
-		dev_info(&h->pdev->dev,
-			"C%d:B%d:T%d:L%d Volume is undergoing rapid parity initialization process.\n",
-			h->scsi_host->host_no,
-			sd->bus, sd->target, sd->lun);
-		break;
-	case HPSA_LV_PENDING_RPI:
-		dev_info(&h->pdev->dev,
-				"C%d:B%d:T%d:L%d Volume is queued for rapid parity initialization process.\n",
+		case HPSA_LV_OK:
+			break;
+		case HPSA_LV_UNDERGOING_ERASE:
+			dev_info(&h->pdev->dev,
+				"C%d:B%d:T%d:L%d Volume is undergoing "
+				"background erase process.\n",
 				h->scsi_host->host_no,
 				sd->bus, sd->target, sd->lun);
-		break;
-	case HPSA_LV_ENCRYPTED_NO_KEY:
-		dev_info(&h->pdev->dev,
-			"C%d:B%d:T%d:L%d Volume is encrypted and cannot be accessed because key is not present.\n",
-			h->scsi_host->host_no,
-			sd->bus, sd->target, sd->lun);
-		break;
-	case HPSA_LV_PLAINTEXT_IN_ENCRYPT_ONLY_CONTROLLER:
-		dev_info(&h->pdev->dev,
-			"C%d:B%d:T%d:L%d Volume is not encrypted and cannot be accessed because controller is in encryption-only mode.\n",
-			h->scsi_host->host_no,
-			sd->bus, sd->target, sd->lun);
-		break;
-	case HPSA_LV_UNDERGOING_ENCRYPTION:
-		dev_info(&h->pdev->dev,
-			"C%d:B%d:T%d:L%d Volume is undergoing encryption process.\n",
-			h->scsi_host->host_no,
-			sd->bus, sd->target, sd->lun);
-		break;
-	case HPSA_LV_UNDERGOING_ENCRYPTION_REKEYING:
-		dev_info(&h->pdev->dev,
-			"C%d:B%d:T%d:L%d Volume is undergoing encryption re-keying process.\n",
-			h->scsi_host->host_no,
-			sd->bus, sd->target, sd->lun);
-		break;
-	case HPSA_LV_ENCRYPTED_IN_NON_ENCRYPTED_CONTROLLER:
-		dev_info(&h->pdev->dev,
-			"C%d:B%d:T%d:L%d Volume is encrypted and cannot be accessed because controller does not have encryption enabled.\n",
-			h->scsi_host->host_no,
-			sd->bus, sd->target, sd->lun);
-		break;
-	case HPSA_LV_PENDING_ENCRYPTION:
-		dev_info(&h->pdev->dev,
-			"C%d:B%d:T%d:L%d Volume is pending migration to encrypted state, but process has not started.\n",
-			h->scsi_host->host_no,
-			sd->bus, sd->target, sd->lun);
-		break;
-	case HPSA_LV_PENDING_ENCRYPTION_REKEYING:
-		dev_info(&h->pdev->dev,
-			"C%d:B%d:T%d:L%d Volume is encrypted and is pending encryption rekeying.\n",
-			h->scsi_host->host_no,
-			sd->bus, sd->target, sd->lun);
-		break;
+			break;
+		case HPSA_LV_UNDERGOING_RPI:
+			dev_info(&h->pdev->dev,
+				"C%d:B%d:T%d:L%d Volume is undergoing "
+				"rapid parity initialization process.\n",
+				h->scsi_host->host_no,
+				sd->bus, sd->target, sd->lun);
+			break;
+		case HPSA_LV_PENDING_RPI:
+			dev_info(&h->pdev->dev,
+				"C%d:B%d:T%d:L%d Volume is queued for "
+				"rapid parity initialization process.\n",
+				h->scsi_host->host_no,
+				sd->bus, sd->target, sd->lun);
+			break;
+		case HPSA_LV_ENCRYPTED_NO_KEY:
+			dev_info(&h->pdev->dev,
+				"C%d:B%d:T%d:L%d "
+				"Volume is encrypted and cannot be accessed because "
+				"key is not present.\n",
+				h->scsi_host->host_no,
+				sd->bus, sd->target, sd->lun);
+			break;
+		case HPSA_LV_PLAINTEXT_IN_ENCRYPT_ONLY_CONTROLLER:
+			dev_info(&h->pdev->dev,
+				"C%d:B%d:T%d:L%d Volume is not encrypted "
+				"and cannot be accessed because "
+				"controller is in encryption-only mode.\n",
+				h->scsi_host->host_no,
+				sd->bus, sd->target, sd->lun);
+			break;
+		case HPSA_LV_UNDERGOING_ENCRYPTION:
+			dev_info(&h->pdev->dev,
+				"C%d:B%d:T%d:L%d Volume is undergoing "
+				"encryption process.\n",
+				h->scsi_host->host_no,
+				sd->bus, sd->target, sd->lun);
+			break;
+		case HPSA_LV_UNDERGOING_ENCRYPTION_REKEYING:
+			dev_info(&h->pdev->dev,
+				"C%d:B%d:T%d:L%d Volume is undergoing "
+				"encryption re-keying process.\n",
+				h->scsi_host->host_no,
+				sd->bus, sd->target, sd->lun);
+			break;
+		case HPSA_LV_ENCRYPTED_IN_NON_ENCRYPTED_CONTROLLER:
+			dev_info(&h->pdev->dev,
+				"C%d:B%d:T%d:L%d Volume is encrypted "
+				"and cannot be accessed because "
+				"controller does not have encryption enabled.\n",
+				h->scsi_host->host_no,
+				sd->bus, sd->target, sd->lun);
+			break;
+		case HPSA_LV_PENDING_ENCRYPTION:
+			dev_info(&h->pdev->dev,
+				"C%d:B%d:T%d:L%d Volume is pending migration "
+				"to encrypted state, but process has not "
+				"started.\n",
+				h->scsi_host->host_no,
+				sd->bus, sd->target, sd->lun);
+			break;
+		case HPSA_LV_PENDING_ENCRYPTION_REKEYING:
+			dev_info(&h->pdev->dev,
+				"C%d:B%d:T%d:L%d Volume is encrypted "
+				"and is pending encryption rekeying.\n",
+				h->scsi_host->host_no,
+				sd->bus, sd->target, sd->lun);
+			break;
 	}
 }
 
@@ -1379,20 +1612,23 @@ static void adjust_hpsa_scsi_table(struct ctlr_info *h, int hostno,
 	sh = h->scsi_host;
 	/* Notify scsi mid layer of any removed devices */
 	for (i = 0; i < nremoved; i++) {
-		struct scsi_device *sdev =
-			scsi_device_lookup(sh, removed[i]->bus,
-				removed[i]->target, removed[i]->lun);
-		if (sdev != NULL) {
-			scsi_remove_device(sdev);
-			scsi_device_put(sdev);
-		} else {
-			/* We don't expect to get here.
-			 * future cmds to this device will get selection
-			 * timeout as if the device was gone.
-			 */
-			dev_warn(&h->pdev->dev, "didn't find c%db%dt%dl%d "
-				" for removal.", hostno, removed[i]->bus,
-				removed[i]->target, removed[i]->lun);
+		if (removed[i]->expose_state & HPSA_SCSI_ADD) {
+			struct scsi_device *sdev =
+				scsi_device_lookup(sh, removed[i]->bus,
+					removed[i]->target, removed[i]->lun);
+			if (sdev != NULL) {
+				scsi_remove_device(sdev);
+				scsi_device_put(sdev);
+			} else {
+				/* We don't expect to get here.
+				 * future cmds to this device will get selection
+				 * timeout as if the device was gone.
+				 */
+				dev_warn(&h->pdev->dev,
+					"didn't find c%db%dt%dl%d for removal.",
+					hostno, removed[i]->bus,
+					removed[i]->target, removed[i]->lun);
+			}
 		}
 		kfree(removed[i]);
 		removed[i] = NULL;
@@ -1400,6 +1636,8 @@ static void adjust_hpsa_scsi_table(struct ctlr_info *h, int hostno,
 
 	/* Notify scsi mid layer of any added devices */
 	for (i = 0; i < nadded; i++) {
+		if (!(added[i]->expose_state & HPSA_SCSI_ADD))
+			continue;
 		if (scsi_add_device(sh, added[i]->bus,
 			added[i]->target, added[i]->lun) == 0)
 			continue;
@@ -1446,8 +1684,33 @@ static int hpsa_slave_alloc(struct scsi_device *sdev)
 	spin_lock_irqsave(&h->devlock, flags);
 	sd = lookup_hpsa_scsi_dev(h, sdev_channel(sdev),
 		sdev_id(sdev), sdev->lun);
-	if (sd != NULL)
+	if (sd && (sd->expose_state & HPSA_SCSI_ADD)) {
 		sdev->hostdata = sd;
+		if (sd->queue_depth)
+			scsi_adjust_queue_depth(sdev, scsi_get_tag_type(sdev),
+				sd->queue_depth);
+		atomic_set(&sd->ioaccel_cmds_out, 0);
+	} else if (sd) {
+		sdev->hostdata = NULL;
+	}
+
+	spin_unlock_irqrestore(&h->devlock, flags);
+	return 0;
+}
+
+/* configure scsi device based on internal per-device structure */
+static int hpsa_slave_configure(struct scsi_device *sdev)
+{
+	struct hpsa_scsi_dev_t *sd;
+	unsigned long flags;
+	struct ctlr_info *h;
+
+	h = sdev_to_hba(sdev);
+	spin_lock_irqsave(&h->devlock, flags);
+	sd = sdev->hostdata;
+	if (sd && sd->expose_state & HPSA_NO_ULD_ATTACH)
+		sdev->no_uld_attach = 1;
+
 	spin_unlock_irqrestore(&h->devlock, flags);
 	return 0;
 }
@@ -1457,35 +1720,75 @@ static void hpsa_slave_destroy(struct scsi_device *sdev)
 	/* nothing to do. */
 }
 
-static void hpsa_free_sg_chain_blocks(struct ctlr_info *h)
+static void hpsa_free_ioaccel2_sg_chain_blocks(struct ctlr_info *h)
 {
 	int i;
 
-	if (!h->cmd_sg_list)
+	if (!h->ioaccel2_cmd_sg_list)
 		return;
 	for (i = 0; i < h->nr_cmds; i++) {
-		kfree(h->cmd_sg_list[i]);
-		h->cmd_sg_list[i] = NULL;
+		kfree(h->ioaccel2_cmd_sg_list[i]);
+		h->ioaccel2_cmd_sg_list[i] = NULL;
 	}
-	kfree(h->cmd_sg_list);
-	h->cmd_sg_list = NULL;
+	kfree(h->ioaccel2_cmd_sg_list);
+	h->ioaccel2_cmd_sg_list = NULL;
 }
 
-static int hpsa_allocate_sg_chain_blocks(struct ctlr_info *h)
+static int hpsa_allocate_ioaccel2_sg_chain_blocks(struct ctlr_info *h)
 {
 	int i;
 
 	if (h->chainsize <= 0)
 		return 0;
 
-	h->cmd_sg_list = kzalloc(sizeof(*h->cmd_sg_list) * h->nr_cmds,
-				GFP_KERNEL);
-	if (!h->cmd_sg_list)
+	h->ioaccel2_cmd_sg_list = 
+		kzalloc(sizeof(*h->ioaccel2_cmd_sg_list) * h->nr_cmds,
+		GFP_KERNEL);
+	if (!h->ioaccel2_cmd_sg_list)
 		return -ENOMEM;
 	for (i = 0; i < h->nr_cmds; i++) {
-		h->cmd_sg_list[i] = kmalloc(sizeof(*h->cmd_sg_list[i]) *
-						h->chainsize, GFP_KERNEL);
-		if (!h->cmd_sg_list[i])
+		h->ioaccel2_cmd_sg_list[i] = 
+			kmalloc(sizeof(*h->ioaccel2_cmd_sg_list[i]) *
+			h->maxsgentries, GFP_KERNEL);
+		if (!h->ioaccel2_cmd_sg_list[i])
+			goto clean;
+	}
+	return 0;
+
+clean:
+	hpsa_free_ioaccel2_sg_chain_blocks(h);
+	return -ENOMEM;
+}
+
+static void hpsa_free_sg_chain_blocks(struct ctlr_info *h)
+{
+	int i;
+
+	if (!h->cmd_sg_list)
+		return;
+	for (i = 0; i < h->nr_cmds; i++) {
+		kfree(h->cmd_sg_list[i]);
+		h->cmd_sg_list[i] = NULL;
+	}
+	kfree(h->cmd_sg_list);
+	h->cmd_sg_list = NULL;
+}
+
+static int hpsa_allocate_sg_chain_blocks(struct ctlr_info *h)
+{
+	int i;
+
+	if (h->chainsize <= 0)
+		return 0;
+
+	h->cmd_sg_list = kzalloc(sizeof(*h->cmd_sg_list) * h->nr_cmds,
+				GFP_KERNEL);
+	if (!h->cmd_sg_list)
+		return -ENOMEM;
+	for (i = 0; i < h->nr_cmds; i++) {
+		h->cmd_sg_list[i] = kmalloc(sizeof(*h->cmd_sg_list[i]) *
+						h->chainsize, GFP_KERNEL);
+		if (!h->cmd_sg_list[i])
 			goto clean;
 	}
 	return 0;
@@ -1495,27 +1798,60 @@ clean:
 	return -ENOMEM;
 }
 
+static int hpsa_map_ioaccel2_sg_chain_block(struct ctlr_info *h,
+	struct io_accel2_cmd *cp, struct CommandList *c)
+{
+	struct ioaccel2_sg_element *chain_block;
+	u64 temp64;
+	u32 chain_size;
+
+	chain_block = h->ioaccel2_cmd_sg_list[c->cmdindex];      
+	chain_size = le32_to_cpu(cp->data_len);
+	temp64 = pci_map_single(h->pdev, chain_block, chain_size,
+				PCI_DMA_TODEVICE);
+	if (hpsa_dma_mapping_error(&h->pdev->dev, temp64)) {
+		/* prevent subsequent unmapping */
+		cp->sg->address = 0;
+		return -1;
+	}
+	cp->sg->address = (u64) cpu_to_le64(temp64);
+	return 0;
+}
+
+static void hpsa_unmap_ioaccel2_sg_chain_block(struct ctlr_info *h,
+	struct io_accel2_cmd *cp)
+{
+	struct ioaccel2_sg_element *chain_sg;
+	u64 temp64;
+	u32 chain_size;
+
+	chain_sg = cp->sg;
+	temp64 = le64_to_cpu(chain_sg->address);
+	chain_size = le32_to_cpu(cp->data_len);
+	pci_unmap_single(h->pdev, temp64, chain_size, PCI_DMA_TODEVICE);
+}
+
 static int hpsa_map_sg_chain_block(struct ctlr_info *h,
 	struct CommandList *c)
 {
 	struct SGDescriptor *chain_sg, *chain_block;
 	u64 temp64;
+	u32 chain_len;
 
 	chain_sg = &c->SG[h->max_cmd_sg_entries - 1];
 	chain_block = h->cmd_sg_list[c->cmdindex];
-	chain_sg->Ext = HPSA_SG_CHAIN;
-	chain_sg->Len = sizeof(*chain_sg) *
+	chain_sg->Ext = cpu_to_le32(HPSA_SG_CHAIN);
+	chain_len = sizeof(*chain_sg) *
 		(c->Header.SGTotal - h->max_cmd_sg_entries);
+	chain_sg->Len = cpu_to_le32(chain_len);
 	temp64 = pci_map_single(h->pdev, chain_block, chain_sg->Len,
 				PCI_DMA_TODEVICE);
-	if (dma_mapping_error(&h->pdev->dev, temp64)) {
+	if (hpsa_dma_mapping_error(&h->pdev->dev, temp64)) {
 		/* prevent subsequent unmapping */
-		chain_sg->Addr.lower = 0;
-		chain_sg->Addr.upper = 0;
+		chain_sg->Addr = 0;
 		return -1;
 	}
-	chain_sg->Addr.lower = (u32) (temp64 & 0x0FFFFFFFFULL);
-	chain_sg->Addr.upper = (u32) ((temp64 >> 32) & 0x0FFFFFFFFULL);
+	chain_sg->Addr = cpu_to_le64(temp64);
 	return 0;
 }
 
@@ -1523,19 +1859,17 @@ static void hpsa_unmap_sg_chain_block(struct ctlr_info *h,
 	struct CommandList *c)
 {
 	struct SGDescriptor *chain_sg;
-	union u64bit temp64;
 
-	if (c->Header.SGTotal <= h->max_cmd_sg_entries)
+	if (le16_to_cpu(c->Header.SGTotal) <= h->max_cmd_sg_entries)
 		return;
 
 	chain_sg = &c->SG[h->max_cmd_sg_entries - 1];
-	temp64.val32.lower = chain_sg->Addr.lower;
-	temp64.val32.upper = chain_sg->Addr.upper;
-	pci_unmap_single(h->pdev, temp64.val, chain_sg->Len, PCI_DMA_TODEVICE);
+	pci_unmap_single(h->pdev, le64_to_cpu(chain_sg->Addr),
+			le32_to_cpu(chain_sg->Len), PCI_DMA_TODEVICE);
 }
 
 
-/* Decode the various types of errors on ioaccel2 path.
+/* Decode the various types of errors on ioaccel2 path. 
  * Return 1 for any error that should generate a RAID path retry.
  * Return 0 for errors that don't require a RAID path retry.
  */
@@ -1546,6 +1880,7 @@ static int handle_ioaccel_mode2_error(struct ctlr_info *h,
 {
 	int data_len;
 	int retry = 0;
+	u32 ioaccel2_resid = 0;
 
 	switch (c2->error_data.serv_response) {
 	case IOACCEL2_SERV_RESPONSE_COMPLETE:
@@ -1553,9 +1888,6 @@ static int handle_ioaccel_mode2_error(struct ctlr_info *h,
 		case IOACCEL2_STATUS_SR_TASK_COMP_GOOD:
 			break;
 		case IOACCEL2_STATUS_SR_TASK_COMP_CHK_COND:
-			dev_warn(&h->pdev->dev,
-				"%s: task complete with check condition.\n",
-				"HP SSD Smart Path");
 			cmd->result |= SAM_STAT_CHECK_CONDITION;
 			if (c2->error_data.data_present !=
 					IOACCEL2_SENSE_DATA_PRESENT) {
@@ -1568,64 +1900,81 @@ static int handle_ioaccel_mode2_error(struct ctlr_info *h,
 			if (data_len > SCSI_SENSE_BUFFERSIZE)
 				data_len = SCSI_SENSE_BUFFERSIZE;
 			if (data_len > sizeof(c2->error_data.sense_data_buff))
-				data_len =
-					sizeof(c2->error_data.sense_data_buff);
+				data_len = sizeof(c2->error_data.sense_data_buff);
 			memcpy(cmd->sense_buffer,
 				c2->error_data.sense_data_buff, data_len);
 			retry = 1;
 			break;
 		case IOACCEL2_STATUS_SR_TASK_COMP_BUSY:
-			dev_warn(&h->pdev->dev,
-				"%s: task complete with BUSY status.\n",
-				"HP SSD Smart Path");
+			dev_warn(&h->pdev->dev, "%s: task complete with "
+				"BUSY status.\n", "HP SSD Smart Path");
 			retry = 1;
 			break;
 		case IOACCEL2_STATUS_SR_TASK_COMP_RES_CON:
-			dev_warn(&h->pdev->dev,
-				"%s: task complete with reservation conflict.\n",
-				"HP SSD Smart Path");
+			dev_warn(&h->pdev->dev, "%s: task complete with "
+				"reservation conflict.\n", "HP SSD Smart Path");
 			retry = 1;
 			break;
 		case IOACCEL2_STATUS_SR_TASK_COMP_SET_FULL:
-			/* Make scsi midlayer do unlimited retries */
-			cmd->result = DID_IMM_RETRY << 16;
+			retry = 1;
 			break;
 		case IOACCEL2_STATUS_SR_TASK_COMP_ABORTED:
-			dev_warn(&h->pdev->dev,
-				"%s: task complete with aborted status.\n",
-				"HP SSD Smart Path");
+			dev_warn(&h->pdev->dev, "%s: task complete with "
+				"aborted status.\n", "HP SSD Smart Path");
 			retry = 1;
 			break;
 		default:
-			dev_warn(&h->pdev->dev,
-				"%s: task complete with unrecognized status: 0x%02x\n",
+			dev_warn(&h->pdev->dev, "%s: task complete with "
+				"unrecognized status: 0x%02x\n", 
 				"HP SSD Smart Path", c2->error_data.status);
 			retry = 1;
 			break;
 		}
 		break;
 	case IOACCEL2_SERV_RESPONSE_FAILURE:
-		/* don't expect to get here. */
-		dev_warn(&h->pdev->dev,
-			"unexpected delivery or target failure, status = 0x%02x\n",
-			c2->error_data.status);
-		retry = 1;
+		switch (c2->error_data.status) {
+		case IOACCEL2_STATUS_SR_IO_ERROR:
+		case IOACCEL2_STATUS_SR_IO_ABORTED:
+		case IOACCEL2_STATUS_SR_OVERRUN:
+			retry = 1;
+			break;
+		case IOACCEL2_STATUS_SR_UNDERRUN:
+			cmd->result = (DID_OK << 16);
+			cmd->result |= (COMMAND_COMPLETE << 8);
+			ioaccel2_resid = c2->error_data.resid_cnt[3] << 24;
+			ioaccel2_resid |= c2->error_data.resid_cnt[2] << 16;
+			ioaccel2_resid |= c2->error_data.resid_cnt[1] << 8;
+			ioaccel2_resid |= c2->error_data.resid_cnt[0];
+			scsi_set_resid(cmd, ioaccel2_resid);
+			break;
+		case IOACCEL2_STATUS_SR_NO_PATH_TO_DEVICE:
+		case IOACCEL2_STATUS_SR_INVALID_DEVICE:
+		case IOACCEL2_STATUS_SR_IOACCEL_DISABLED:
+			/* We will get an event from ctlr to trigger rescan */
+			retry = 1;
+			break;
+		default:
+			retry = 1;
+			dev_warn(&h->pdev->dev,
+				"unexpected delivery or target failure, "
+				"status = 0x%02x\n", c2->error_data.status);
+		}
 		break;
 	case IOACCEL2_SERV_RESPONSE_TMF_COMPLETE:
 		break;
 	case IOACCEL2_SERV_RESPONSE_TMF_SUCCESS:
 		break;
 	case IOACCEL2_SERV_RESPONSE_TMF_REJECTED:
+		/* TODO: Figure out what to do here. */
 		dev_warn(&h->pdev->dev, "task management function rejected.\n");
-		retry = 1;
 		break;
 	case IOACCEL2_SERV_RESPONSE_TMF_WRONG_LUN:
+		/* TODO: Figure out what to do here. */
 		dev_warn(&h->pdev->dev, "task management function invalid LUN\n");
 		break;
 	default:
-		dev_warn(&h->pdev->dev,
-			"%s: Unrecognized server response: 0x%02x\n",
-			"HP SSD Smart Path",
+		dev_warn(&h->pdev->dev, "%s: Unrecognized TMF response: "
+			"0x%02x\n", "HP SSD Smart Path", 
 			c2->error_data.serv_response);
 		retry = 1;
 		break;
@@ -1639,7 +1988,11 @@ static void process_ioaccel2_completion(struct ctlr_info *h,
 		struct hpsa_scsi_dev_t *dev)
 {
 	struct io_accel2_cmd *c2 = &h->ioaccel2_cmd_pool[c->cmdindex];
-	int raid_retry = 0;
+
+	if (c2->sg[0].chain_indicator == IOACCEL2_CHAIN)
+		hpsa_unmap_ioaccel2_sg_chain_block(h, c2);	
+
+	atomic_dec(&dev->ioaccel_cmds_out);
 
 	/* check for good status */
 	if (likely(c2->error_data.serv_response == 0 &&
@@ -1656,26 +2009,48 @@ static void process_ioaccel2_completion(struct ctlr_info *h,
 	if (is_logical_dev_addr_mode(dev->scsi3addr) &&
 		c2->error_data.serv_response ==
 			IOACCEL2_SERV_RESPONSE_FAILURE) {
-		dev->offload_enabled = 0;
-		h->drv_req_rescan = 1;	/* schedule controller for a rescan */
-		cmd->result = DID_SOFT_ERROR << 16;
-		cmd_free(h, c);
-		cmd->scsi_done(cmd);
-		return;
-	}
-	raid_retry = handle_ioaccel_mode2_error(h, c, cmd, c2);
-	/* If error found, disable Smart Path, schedule a rescan,
-	 * and force a retry on the standard path.
-	 */
-	if (raid_retry) {
-		dev_warn(&h->pdev->dev, "%s: Retrying on standard path.\n",
-			"HP SSD Smart Path");
-		dev->offload_enabled = 0; /* Disable Smart Path */
-		h->drv_req_rescan = 1;	  /* schedule controller rescan */
-		cmd->result = DID_SOFT_ERROR << 16;
+		if (c2->error_data.status ==
+			IOACCEL2_STATUS_SR_IOACCEL_DISABLED)
+			dev->offload_enabled = 0;
+		goto retry_cmd;
 	}
+	if (handle_ioaccel_mode2_error(h, c, cmd, c2))
+		goto retry_cmd;
+
 	cmd_free(h, c);
 	cmd->scsi_done(cmd);
+	return;
+
+retry_cmd:
+	INIT_WORK(&c->work, hpsa_command_resubmit_worker);
+	queue_work_on(raw_smp_processor_id(), h->resubmit_wq, &c->work);
+}
+
+/* Returns 0 on success, < 0 otherwise. */
+static int hpsa_evaluate_tmf_status(struct ctlr_info *h,
+					struct CommandList *cp)
+{
+	u8 tmf_status = cp->err_info->ScsiStatus;
+
+	switch (tmf_status) {
+	case CISS_TMF_COMPLETE:
+		/* CISS_TMF_COMPLETE never happens, instead,
+		 * ei->CommandStatus == 0 for this case.
+		 */
+	case CISS_TMF_SUCCESS:
+		return 0;
+	case CISS_TMF_INVALID_FRAME:
+	case CISS_TMF_NOT_SUPPORTED:
+	case CISS_TMF_FAILED:
+	case CISS_TMF_WRONG_LUN:
+	case CISS_TMF_OVERLAPPED_TAG:
+		break;
+	default:
+		dev_warn(&h->pdev->dev, "Unknown TMF status: %02x\n",
+				tmf_status);
+		break;
+	}
+	return -tmf_status;
 }
 
 static void complete_scsi_command(struct CommandList *cp)
@@ -1685,13 +2060,13 @@ static void complete_scsi_command(struct CommandList *cp)
 	struct ErrorInfo *ei;
 	struct hpsa_scsi_dev_t *dev;
 
-	unsigned char sense_key;
-	unsigned char asc;      /* additional sense code */
-	unsigned char ascq;     /* additional sense code qualifier */
+	int sense_key;
+	int asc;      /* additional sense code */
+	int ascq;     /* additional sense code qualifier */
 	unsigned long sense_data_size;
 
 	ei = cp->err_info;
-	cmd = (struct scsi_cmnd *) cp->scsi_cmd;
+	cmd = cp->scsi_cmd;
 	h = cp->h;
 	dev = cmd->device->hostdata;
 
@@ -1706,19 +2081,7 @@ static void complete_scsi_command(struct CommandList *cp)
 	if (cp->cmd_type == CMD_IOACCEL2)
 		return process_ioaccel2_completion(h, cp, cmd, dev);
 
-	cmd->result |= ei->ScsiStatus;
-
-	/* copy the sense data whether we need to or not. */
-	if (SCSI_SENSE_BUFFERSIZE < sizeof(ei->SenseInfo))
-		sense_data_size = SCSI_SENSE_BUFFERSIZE;
-	else
-		sense_data_size = sizeof(ei->SenseInfo);
-	if (ei->SenseLen < sense_data_size)
-		sense_data_size = ei->SenseLen;
-
-	memcpy(cmd->sense_buffer, ei->SenseInfo, sense_data_size);
 	scsi_set_resid(cmd, ei->ResidualCnt);
-
 	if (ei->CommandStatus == 0) {
 		cmd_free(h, cp);
 		cmd->scsi_done(cmd);
@@ -1730,10 +2093,10 @@ static void complete_scsi_command(struct CommandList *cp)
 	 */
 	if (cp->cmd_type == CMD_IOACCEL1) {
 		struct io_accel1_cmd *c = &h->ioaccel_cmd_pool[cp->cmdindex];
+		atomic_dec(&dev->ioaccel_cmds_out);
 		cp->Header.SGList = cp->Header.SGTotal = scsi_sg_count(cmd);
 		cp->Request.CDBLen = c->io_flags & IOACCEL1_IOFLAGS_CDBLEN_MASK;
-		cp->Header.Tag.lower = c->Tag.lower;
-		cp->Header.Tag.upper = c->Tag.upper;
+		cp->Header.tag = c->tag;
 		memcpy(cp->Header.LUN.LunAddrBytes, c->CISS_LUN, 8);
 		memcpy(cp->Request.CDB, c->CDB, cp->Request.CDBLen);
 
@@ -1744,9 +2107,9 @@ static void complete_scsi_command(struct CommandList *cp)
 		if (is_logical_dev_addr_mode(dev->scsi3addr)) {
 			if (ei->CommandStatus == CMD_IOACCEL_DISABLED)
 				dev->offload_enabled = 0;
-			cmd->result = DID_SOFT_ERROR << 16;
-			cmd_free(h, cp);
-			cmd->scsi_done(cmd);
+			INIT_WORK(&cp->work, hpsa_command_resubmit_worker);
+			queue_work_on(raw_smp_processor_id(),
+					h->resubmit_wq, &cp->work);
 			return;
 		}
 	}
@@ -1755,80 +2118,25 @@ static void complete_scsi_command(struct CommandList *cp)
 	switch (ei->CommandStatus) {
 
 	case CMD_TARGET_STATUS:
-		if (ei->ScsiStatus) {
-			/* Get sense key */
-			sense_key = 0xf & ei->SenseInfo[2];
-			/* Get additional sense code */
-			asc = ei->SenseInfo[12];
-			/* Get addition sense code qualifier */
-			ascq = ei->SenseInfo[13];
-		}
-
+		cmd->result |= ei->ScsiStatus;
+		/* copy the sense data */
+		if (SCSI_SENSE_BUFFERSIZE < sizeof(ei->SenseInfo))
+			sense_data_size = SCSI_SENSE_BUFFERSIZE;
+		else
+			sense_data_size = sizeof(ei->SenseInfo);
+		if (ei->SenseLen < sense_data_size)
+			sense_data_size = ei->SenseLen;
+		memcpy(cmd->sense_buffer, ei->SenseInfo, sense_data_size);
+		if (ei->ScsiStatus)
+			decode_sense_data(ei->SenseInfo, sense_data_size,
+				&sense_key, &asc, &ascq);
 		if (ei->ScsiStatus == SAM_STAT_CHECK_CONDITION) {
-			if (check_for_unit_attention(h, cp))
-				break;
-			if (sense_key == ILLEGAL_REQUEST) {
-				/*
-				 * SCSI REPORT_LUNS is commonly unsupported on
-				 * Smart Array.  Suppress noisy complaint.
-				 */
-				if (cp->Request.CDB[0] == REPORT_LUNS)
-					break;
-
-				/* If ASC/ASCQ indicate Logical Unit
-				 * Not Supported condition,
-				 */
-				if ((asc == 0x25) && (ascq == 0x0)) {
-					dev_warn(&h->pdev->dev, "cp %p "
-						"has check condition\n", cp);
-					break;
-				}
-			}
-
-			if (sense_key == NOT_READY) {
-				/* If Sense is Not Ready, Logical Unit
-				 * Not ready, Manual Intervention
-				 * required
-				 */
-				if ((asc == 0x04) && (ascq == 0x03)) {
-					dev_warn(&h->pdev->dev, "cp %p "
-						"has check condition: unit "
-						"not ready, manual "
-						"intervention required\n", cp);
-					break;
-				}
-			}
 			if (sense_key == ABORTED_COMMAND) {
-				/* Aborted command is retryable */
-				dev_warn(&h->pdev->dev, "cp %p "
-					"has check condition: aborted command: "
-					"ASC: 0x%x, ASCQ: 0x%x\n",
-					cp, asc, ascq);
 				cmd->result |= DID_SOFT_ERROR << 16;
 				break;
 			}
-			/* Must be some other type of check condition */
-			dev_dbg(&h->pdev->dev, "cp %p has check condition: "
-					"unknown type: "
-					"Sense: 0x%x, ASC: 0x%x, ASCQ: 0x%x, "
-					"Returning result: 0x%x, "
-					"cmd=[%02x %02x %02x %02x %02x "
-					"%02x %02x %02x %02x %02x %02x "
-					"%02x %02x %02x %02x %02x]\n",
-					cp, sense_key, asc, ascq,
-					cmd->result,
-					cmd->cmnd[0], cmd->cmnd[1],
-					cmd->cmnd[2], cmd->cmnd[3],
-					cmd->cmnd[4], cmd->cmnd[5],
-					cmd->cmnd[6], cmd->cmnd[7],
-					cmd->cmnd[8], cmd->cmnd[9],
-					cmd->cmnd[10], cmd->cmnd[11],
-					cmd->cmnd[12], cmd->cmnd[13],
-					cmd->cmnd[14], cmd->cmnd[15]);
 			break;
 		}
-
-
 		/* Problem was not a check condition
 		 * Pass it up to the upper layers...
 		 */
@@ -1862,9 +2170,8 @@ static void complete_scsi_command(struct CommandList *cp)
 	case CMD_DATA_UNDERRUN: /* let mid layer handle it. */
 		break;
 	case CMD_DATA_OVERRUN:
-		dev_warn(&h->pdev->dev, "cp %p has"
-			" completed with data overrun "
-			"reported\n", cp);
+		dev_warn(&h->pdev->dev,
+			"CDB " phN16 " data overrun\n", phNbytes16(cp->Request.CDB));
 		break;
 	case CMD_INVALID: {
 		/* print_bytes(cp, sizeof(*cp), 1, 0);
@@ -1880,39 +2187,47 @@ static void complete_scsi_command(struct CommandList *cp)
 		break;
 	case CMD_PROTOCOL_ERR:
 		cmd->result = DID_ERROR << 16;
-		dev_warn(&h->pdev->dev, "cp %p has "
-			"protocol error\n", cp);
+		dev_warn(&h->pdev->dev, "CDB " phN16 " : protocol error\n",
+				phNbytes16(cp->Request.CDB));
 		break;
 	case CMD_HARDWARE_ERR:
 		cmd->result = DID_ERROR << 16;
-		dev_warn(&h->pdev->dev, "cp %p had  hardware error\n", cp);
+		dev_warn(&h->pdev->dev, "CDB " phN16 " : hardware error\n",
+			phNbytes16(cp->Request.CDB));
 		break;
 	case CMD_CONNECTION_LOST:
 		cmd->result = DID_ERROR << 16;
-		dev_warn(&h->pdev->dev, "cp %p had connection lost\n", cp);
+		dev_warn(&h->pdev->dev, "CDB " phN16 " : connection lost\n",
+			phNbytes16(cp->Request.CDB));
 		break;
 	case CMD_ABORTED:
 		cmd->result = DID_ABORT << 16;
-		dev_warn(&h->pdev->dev, "cp %p was aborted with status 0x%x\n",
-				cp, ei->ScsiStatus);
+		dev_warn(&h->pdev->dev, "CDB " phN16 " was aborted with status 0x%x\n",
+				phNbytes16(cp->Request.CDB), ei->ScsiStatus);
 		break;
 	case CMD_ABORT_FAILED:
 		cmd->result = DID_ERROR << 16;
-		dev_warn(&h->pdev->dev, "cp %p reports abort failed\n", cp);
+		dev_warn(&h->pdev->dev, "CDB " phN16 " : abort failed\n",
+			phNbytes16(cp->Request.CDB));
 		break;
 	case CMD_UNSOLICITED_ABORT:
 		cmd->result = DID_SOFT_ERROR << 16; /* retry the command */
-		dev_warn(&h->pdev->dev, "cp %p aborted due to an unsolicited "
-			"abort\n", cp);
+		dev_warn(&h->pdev->dev, "CDB " phN16 " : unsolicited abort\n",
+			phNbytes16(cp->Request.CDB));
 		break;
 	case CMD_TIMEOUT:
 		cmd->result = DID_TIME_OUT << 16;
-		dev_warn(&h->pdev->dev, "cp %p timedout\n", cp);
+		dev_warn(&h->pdev->dev, "CDB " phN16 " timedout\n",
+			phNbytes16(cp->Request.CDB));
 		break;
 	case CMD_UNABORTABLE:
 		cmd->result = DID_ERROR << 16;
 		dev_warn(&h->pdev->dev, "Command unabortable\n");
 		break;
+	case CMD_TMF_STATUS:
+		if (hpsa_evaluate_tmf_status(h, cp)) /* TMF failed? */
+			cmd->result = DID_ERROR << 16;
+		break;
 	case CMD_IOACCEL_DISABLED:
 		/* This only handles the direct pass-through case since RAID
 		 * offload is handled above.  Just attempt a retry.
@@ -1926,6 +2241,21 @@ static void complete_scsi_command(struct CommandList *cp)
 		dev_warn(&h->pdev->dev, "cp %p returned unknown status %x\n",
 				cp, ei->CommandStatus);
 	}
+
+	/* Prevent the following race in the abort handler:
+	 *
+	 * 1. LLD is requested to abort a scsi command
+	 * 2. scsi command completes
+	 * 3. The struct CommandList associated with 2 is made available.
+	 * 4. new io request to LLD to another LUN re-uses struct CommandList
+	 * 5. abort handler follows scsi_cmnd->host_scribble and
+	 *    finds struct CommandList and tries to aborts it.
+	 * Now we have aborted the wrong command.
+	 * Clear cp->scsi_cmd here so that if this get re-used, the abort
+	 * handler will know it's a different scsi_cmnd.
+	 */
+	cp->scsi_cmd = NULL;
+
 	cmd_free(h, cp);
 	cmd->scsi_done(cmd);
 }
@@ -1934,14 +2264,11 @@ static void hpsa_pci_unmap(struct pci_dev *pdev,
 	struct CommandList *c, int sg_used, int data_direction)
 {
 	int i;
-	union u64bit addr64;
 
-	for (i = 0; i < sg_used; i++) {
-		addr64.val32.lower = c->SG[i].Addr.lower;
-		addr64.val32.upper = c->SG[i].Addr.upper;
-		pci_unmap_single(pdev, (dma_addr_t) addr64.val, c->SG[i].Len,
-			data_direction);
-	}
+	for (i = 0; i < sg_used; i++)
+		pci_unmap_single(pdev, (dma_addr_t) le64_to_cpu(c->SG[i].Addr),
+				le32_to_cpu(c->SG[i].Len),
+				data_direction);
 }
 
 static int hpsa_map_one(struct pci_dev *pdev,
@@ -1954,36 +2281,52 @@ static int hpsa_map_one(struct pci_dev *pdev,
 
 	if (buflen == 0 || data_direction == PCI_DMA_NONE) {
 		cp->Header.SGList = 0;
-		cp->Header.SGTotal = 0;
+		cp->Header.SGTotal = cpu_to_le16(0);
 		return 0;
 	}
 
 	addr64 = (u64) pci_map_single(pdev, buf, buflen, data_direction);
-	if (dma_mapping_error(&pdev->dev, addr64)) {
+	if (hpsa_dma_mapping_error(&pdev->dev, addr64)) {
 		/* Prevent subsequent unmap of something never mapped */
 		cp->Header.SGList = 0;
-		cp->Header.SGTotal = 0;
+		cp->Header.SGTotal = cpu_to_le16(0);
 		return -1;
 	}
-	cp->SG[0].Addr.lower =
-	  (u32) (addr64 & (u64) 0x00000000FFFFFFFF);
-	cp->SG[0].Addr.upper =
-	  (u32) ((addr64 >> 32) & (u64) 0x00000000FFFFFFFF);
-	cp->SG[0].Len = buflen;
-	cp->SG[0].Ext = HPSA_SG_LAST; /* we are not chaining */
+	cp->SG[0].Addr = cpu_to_le64(addr64);
+	cp->SG[0].Len = cpu_to_le32(buflen);
+	cp->SG[0].Ext = cpu_to_le32(HPSA_SG_LAST); /* we are not chaining */
 	cp->Header.SGList = (u8) 1;   /* no. SGs contig in this cmd */
-	cp->Header.SGTotal = (u16) 1; /* total sgs in this cmd list */
+	cp->Header.SGTotal = (u16) cpu_to_le16(1); /* total sgs in cmd list */
 	return 0;
 }
 
-static inline void hpsa_scsi_do_simple_cmd_core(struct ctlr_info *h,
-	struct CommandList *c)
+#define NO_TIMEOUT ((unsigned long) -1)
+#define DEFAULT_TIMEOUT (30000) /* milliseconds */
+static int __hpsa_scsi_do_simple_cmd_core(struct ctlr_info *h,
+	struct CommandList *c, int reply_queue, unsigned long timeout_msecs)
 {
 	DECLARE_COMPLETION_ONSTACK(wait);
 
 	c->waiting = &wait;
-	enqueue_cmd_and_start_io(h, c);
-	wait_for_completion(&wait);
+	__enqueue_cmd_and_start_io(h, c, reply_queue);
+	if (timeout_msecs == NO_TIMEOUT) {
+		/* TODO: get rid of this no-timeout thing */
+		wait_for_completion_io(&wait);
+		return 0;
+	}
+	if (!wait_for_completion_io_timeout(&wait,
+					msecs_to_jiffies(timeout_msecs))) {
+		dev_warn(&h->pdev->dev, "Command timed out.\n");
+		return -ETIMEDOUT;
+	}
+	return 0;
+}
+
+static int hpsa_scsi_do_simple_cmd_core(struct ctlr_info *h,
+	struct CommandList *c, unsigned long timeout_msecs)
+{
+	return __hpsa_scsi_do_simple_cmd_core(h, c,
+					DEFAULT_REPLY_QUEUE, timeout_msecs);
 }
 
 static u32 lockup_detected(struct ctlr_info *h)
@@ -1998,25 +2341,29 @@ static u32 lockup_detected(struct ctlr_info *h)
 	return rc;
 }
 
-static void hpsa_scsi_do_simple_cmd_core_if_no_lockup(struct ctlr_info *h,
-	struct CommandList *c)
+static int hpsa_scsi_do_simple_cmd_core_if_no_lockup(struct ctlr_info *h,
+	struct CommandList *c, unsigned long timeout_msecs)
 {
 	/* If controller lockup detected, fake a hardware error. */
-	if (unlikely(lockup_detected(h)))
+	if (unlikely(lockup_detected(h))) {
 		c->err_info->CommandStatus = CMD_HARDWARE_ERR;
-	else
-		hpsa_scsi_do_simple_cmd_core(h, c);
+		return 0;
+	}
+	return hpsa_scsi_do_simple_cmd_core(h, c, timeout_msecs);
 }
 
 #define MAX_DRIVER_CMD_RETRIES 25
-static void hpsa_scsi_do_simple_cmd_with_retry(struct ctlr_info *h,
-	struct CommandList *c, int data_direction)
+static int hpsa_scsi_do_simple_cmd_with_retry(struct ctlr_info *h,
+	struct CommandList *c, int data_direction, unsigned long timeout_msecs)
 {
 	int backoff_time = 10, retry_count = 0;
+	int rc;
 
 	do {
 		memset(c->err_info, 0, sizeof(*c->err_info));
-		hpsa_scsi_do_simple_cmd_core(h, c);
+		rc = hpsa_scsi_do_simple_cmd_core(h, c, timeout_msecs);
+		if (rc)
+			break;
 		retry_count++;
 		if (retry_count > 3) {
 			msleep(backoff_time);
@@ -2027,39 +2374,30 @@ static void hpsa_scsi_do_simple_cmd_with_retry(struct ctlr_info *h,
 			check_for_busy(h, c)) &&
 			retry_count <= MAX_DRIVER_CMD_RETRIES);
 	hpsa_pci_unmap(h->pdev, c, 1, data_direction);
+	if (retry_count > MAX_DRIVER_CMD_RETRIES)
+		rc = -1; /* FIXME do something better? */
+	return rc;
 }
 
-static void hpsa_print_cmd(struct ctlr_info *h, char *txt,
-				struct CommandList *c)
-{
-	const u8 *cdb = c->Request.CDB;
-	const u8 *lun = c->Header.LUN.LunAddrBytes;
-
-	dev_warn(&h->pdev->dev, "%s: LUN:%02x%02x%02x%02x%02x%02x%02x%02x"
-	" CDB:%02x%02x%02x%02x%02x%02x%02x%02x%02x%02x%02x%02x%02x%02x%02x%02x\n",
-		txt, lun[0], lun[1], lun[2], lun[3],
-		lun[4], lun[5], lun[6], lun[7],
-		cdb[0], cdb[1], cdb[2], cdb[3],
-		cdb[4], cdb[5], cdb[6], cdb[7],
-		cdb[8], cdb[9], cdb[10], cdb[11],
-		cdb[12], cdb[13], cdb[14], cdb[15]);
-}
-
-static void hpsa_scsi_interpret_error(struct ctlr_info *h,
-			struct CommandList *cp)
+static void hpsa_scsi_interpret_error(struct CommandList *cp)
 {
 	const struct ErrorInfo *ei = cp->err_info;
 	struct device *d = &cp->h->pdev->dev;
-	const u8 *sd = ei->SenseInfo;
+	int sense_key, asc, ascq, sense_len;
 
 	switch (ei->CommandStatus) {
 	case CMD_TARGET_STATUS:
-		hpsa_print_cmd(h, "SCSI status", cp);
+		if (ei->SenseLen > sizeof(ei->SenseInfo))
+			sense_len = sizeof(ei->SenseInfo);
+		else
+			sense_len = ei->SenseLen;
+		decode_sense_data(ei->SenseInfo, sense_len,
+					&sense_key, &asc, &ascq);
 		if (ei->ScsiStatus == SAM_STAT_CHECK_CONDITION)
-			dev_warn(d, "SCSI Status = 02, Sense key = %02x, ASC = %02x, ASCQ = %02x\n",
-				sd[2] & 0x0f, sd[12], sd[13]);
+			dev_warn(d, "SCSI Status = 0x02, Sense key = 0x%02x, ASC = 0x%02x, ASCQ = 0x%02x\n",
+				sense_key, asc, ascq);
 		else
-			dev_warn(d, "SCSI Status = %02x\n", ei->ScsiStatus);
+			dev_warn(d, "SCSI Status = 0x%02x\n", ei->ScsiStatus);
 		if (ei->ScsiStatus == 0)
 			dev_warn(d, "SCSI status is abnormally zero.  "
 			"(probably indicates selection timeout "
@@ -2067,45 +2405,48 @@ static void hpsa_scsi_interpret_error(struct ctlr_info *h,
 			"firmware bug, circa July, 2001.)\n");
 		break;
 	case CMD_DATA_UNDERRUN: /* let mid layer handle it. */
+			dev_info(d, "UNDERRUN\n");
 		break;
 	case CMD_DATA_OVERRUN:
-		hpsa_print_cmd(h, "overrun condition", cp);
+		dev_warn(d, "cp %p has completed with data overrun\n", cp);
 		break;
 	case CMD_INVALID: {
 		/* controller unfortunately reports SCSI passthru's
 		 * to non-existent targets as invalid commands.
 		 */
-		hpsa_print_cmd(h, "invalid command", cp);
-		dev_warn(d, "probably means device no longer present\n");
+		dev_warn(d, "cp %p is reported invalid (probably means "
+			"target device no longer present)\n", cp);
+		/* print_bytes((unsigned char *) cp, sizeof(*cp), 1, 0);
+		print_cmd(cp);  */
 		}
 		break;
 	case CMD_PROTOCOL_ERR:
-		hpsa_print_cmd(h, "protocol error", cp);
+		dev_warn(d, "cp %p has protocol error \n", cp);
 		break;
 	case CMD_HARDWARE_ERR:
-		hpsa_print_cmd(h, "hardware error", cp);
+		/* cmd->result = DID_ERROR << 16; */
+		dev_warn(d, "cp %p had hardware error\n", cp);
 		break;
 	case CMD_CONNECTION_LOST:
-		hpsa_print_cmd(h, "connection lost", cp);
+		dev_warn(d, "cp %p had connection lost\n", cp);
 		break;
 	case CMD_ABORTED:
-		hpsa_print_cmd(h, "aborted", cp);
+		dev_warn(d, "cp %p was aborted\n", cp);
 		break;
 	case CMD_ABORT_FAILED:
-		hpsa_print_cmd(h, "abort failed", cp);
+		dev_warn(d, "cp %p reports abort failed\n", cp);
 		break;
 	case CMD_UNSOLICITED_ABORT:
-		hpsa_print_cmd(h, "unsolicited abort", cp);
+		dev_warn(d, "cp %p aborted due to an unsolicited abort\n", cp);
 		break;
 	case CMD_TIMEOUT:
-		hpsa_print_cmd(h, "timed out", cp);
+		dev_warn(d, "cp %p timed out\n", cp);
 		break;
 	case CMD_UNABORTABLE:
-		hpsa_print_cmd(h, "unabortable", cp);
+		dev_warn(d, "Command unabortable\n");
 		break;
 	default:
-		hpsa_print_cmd(h, "unknown status", cp);
-		dev_warn(d, "Unknown command status %x\n",
+		dev_warn(d, "cp %p returned unknown status %x\n", cp,
 				ei->CommandStatus);
 	}
 }
@@ -2118,10 +2459,9 @@ static int hpsa_scsi_do_inquiry(struct ctlr_info *h, unsigned char *scsi3addr,
 	struct CommandList *c;
 	struct ErrorInfo *ei;
 
-	c = cmd_special_alloc(h);
-
-	if (c == NULL) {			/* trouble... */
-		dev_warn(&h->pdev->dev, "cmd_special_alloc returned NULL!\n");
+	c = cmd_alloc(h);
+	if (!c) {			/* trouble... */
+		dev_warn(&h->pdev->dev, "cmd_alloc returned NULL!\n");
 		return -ENOMEM;
 	}
 
@@ -2130,29 +2470,33 @@ static int hpsa_scsi_do_inquiry(struct ctlr_info *h, unsigned char *scsi3addr,
 		rc = -1;
 		goto out;
 	}
-	hpsa_scsi_do_simple_cmd_with_retry(h, c, PCI_DMA_FROMDEVICE);
+	rc = hpsa_scsi_do_simple_cmd_with_retry(h, c,
+		PCI_DMA_FROMDEVICE, NO_TIMEOUT);
+	if (rc)
+		goto out;
 	ei = c->err_info;
 	if (ei->CommandStatus != 0 && ei->CommandStatus != CMD_DATA_UNDERRUN) {
-		hpsa_scsi_interpret_error(h, c);
+		hpsa_scsi_interpret_error(c);
 		rc = -1;
 	}
 out:
-	cmd_special_free(h, c);
+	cmd_free(h, c);
 	return rc;
 }
 
+
 static int hpsa_bmic_ctrl_mode_sense(struct ctlr_info *h,
-		unsigned char *scsi3addr, unsigned char page,
-		struct bmic_controller_parameters *buf, size_t bufsize)
+			unsigned char *scsi3addr, unsigned char page,
+			struct bmic_controller_parameters *buf, size_t bufsize)
 {
 	int rc = IO_OK;
 	struct CommandList *c;
 	struct ErrorInfo *ei;
 
-	c = cmd_special_alloc(h);
+	c = cmd_alloc(h);
 
 	if (c == NULL) {			/* trouble... */
-		dev_warn(&h->pdev->dev, "cmd_special_alloc returned NULL!\n");
+		dev_warn(&h->pdev->dev, "cmd_alloc returned NULL!\n");
 		return -ENOMEM;
 	}
 
@@ -2161,44 +2505,49 @@ static int hpsa_bmic_ctrl_mode_sense(struct ctlr_info *h,
 		rc = -1;
 		goto out;
 	}
-	hpsa_scsi_do_simple_cmd_with_retry(h, c, PCI_DMA_FROMDEVICE);
+	rc = hpsa_scsi_do_simple_cmd_with_retry(h, c,
+					PCI_DMA_FROMDEVICE, NO_TIMEOUT);
+	if (rc)
+		goto out;
 	ei = c->err_info;
 	if (ei->CommandStatus != 0 && ei->CommandStatus != CMD_DATA_UNDERRUN) {
-		hpsa_scsi_interpret_error(h, c);
+		hpsa_scsi_interpret_error(c);
 		rc = -1;
 	}
 out:
-	cmd_special_free(h, c);
+	cmd_free(h, c);
 	return rc;
-	}
+}
 
-static int hpsa_send_reset(struct ctlr_info *h, unsigned char *scsi3addr,
-	u8 reset_type)
+static int hpsa_send_reset(struct ctlr_info *h, unsigned char *scsi3addr, 
+	int reset_type)
 {
 	int rc = IO_OK;
 	struct CommandList *c;
 	struct ErrorInfo *ei;
 
-	c = cmd_special_alloc(h);
-
+	c = cmd_alloc(h);
 	if (c == NULL) {			/* trouble... */
-		dev_warn(&h->pdev->dev, "cmd_special_alloc returned NULL!\n");
+		dev_warn(&h->pdev->dev, "cmd_alloc returned NULL!\n");
 		return -ENOMEM;
 	}
 
 	/* fill_cmd can't fail here, no data buffer to map. */
-	(void) fill_cmd(c, HPSA_DEVICE_RESET_MSG, h, NULL, 0, 0,
-			scsi3addr, TYPE_MSG);
-	c->Request.CDB[1] = reset_type; /* fill_cmd defaults to LUN reset */
-	hpsa_scsi_do_simple_cmd_core(h, c);
+	(void) fill_cmd(c, reset_type, h, NULL, 0, 0, scsi3addr, TYPE_MSG);
+	rc = hpsa_scsi_do_simple_cmd_core(h, c, NO_TIMEOUT);
+	if (rc) {
+		dev_warn(&h->pdev->dev, "Failed to send reset command\n");
+		goto out;
+	}
 	/* no unmap needed here because no data xfer. */
 
 	ei = c->err_info;
 	if (ei->CommandStatus != 0) {
-		hpsa_scsi_interpret_error(h, c);
+		hpsa_scsi_interpret_error(c);
 		rc = -1;
 	}
-	cmd_special_free(h, c);
+out:
+	cmd_free(h, c);
 	return rc;
 }
 
@@ -2263,9 +2612,9 @@ static void hpsa_debug_map_buff(struct ctlr_info *h, int rc,
 			le16_to_cpu(map_buff->layout_map_count));
 	dev_info(&h->pdev->dev, "flags = %u\n",
 			le16_to_cpu(map_buff->flags));
-	if (map_buff->flags & RAID_MAP_FLAG_ENCRYPT_ON)
+	if (map_buff->flags & RAID_MAP_FLAG_ENCRYPT_ON ) 
 		dev_info(&h->pdev->dev, "encrypytion = ON\n");
-	else
+	else 	
 		dev_info(&h->pdev->dev, "encrypytion = OFF\n");
 	dev_info(&h->pdev->dev, "dekindex = %u\n",
 			le16_to_cpu(map_buff->dekindex));
@@ -2276,8 +2625,7 @@ static void hpsa_debug_map_buff(struct ctlr_info *h, int rc,
 		row_cnt = le16_to_cpu(map_buff->row_cnt);
 		for (row = 0; row < row_cnt; row++) {
 			dev_info(&h->pdev->dev, "  Row%u:\n", row);
-			disks_per_row =
-				le16_to_cpu(map_buff->data_disks_per_row);
+			disks_per_row = le16_to_cpu(map_buff->data_disks_per_row);
 			for (col = 0; col < disks_per_row; col++, dd++)
 				dev_info(&h->pdev->dev,
 					"    D%02u: h=0x%04x xor=%u,%u\n",
@@ -2308,26 +2656,28 @@ static int hpsa_get_raid_map(struct ctlr_info *h,
 	struct CommandList *c;
 	struct ErrorInfo *ei;
 
-	c = cmd_special_alloc(h);
+	c = cmd_alloc(h);
 	if (c == NULL) {
-		dev_warn(&h->pdev->dev, "cmd_special_alloc returned NULL!\n");
+		dev_warn(&h->pdev->dev, "cmd_alloc returned NULL!\n");
 		return -ENOMEM;
 	}
 	if (fill_cmd(c, HPSA_GET_RAID_MAP, h, &this_device->raid_map,
-			sizeof(this_device->raid_map), 0,
-			scsi3addr, TYPE_CMD)) {
-		dev_warn(&h->pdev->dev, "Out of memory in hpsa_get_raid_map()\n");
-		cmd_special_free(h, c);
+			sizeof(this_device->raid_map), 0, scsi3addr, TYPE_CMD)) {
+		cmd_free(h, c);
 		return -ENOMEM;
 	}
-	hpsa_scsi_do_simple_cmd_with_retry(h, c, PCI_DMA_FROMDEVICE);
+	rc = hpsa_scsi_do_simple_cmd_with_retry(h, c,
+			PCI_DMA_FROMDEVICE, NO_TIMEOUT);
+	if (rc)
+		goto out;
 	ei = c->err_info;
 	if (ei->CommandStatus != 0 && ei->CommandStatus != CMD_DATA_UNDERRUN) {
-		hpsa_scsi_interpret_error(h, c);
-		cmd_special_free(h, c);
+		hpsa_scsi_interpret_error(c);
+		cmd_free(h, c);
 		return -1;
 	}
-	cmd_special_free(h, c);
+out:
+	cmd_free(h, c);
 
 	/* @todo in the future, dynamically allocate RAID map memory */
 	if (le32_to_cpu(this_device->raid_map.structure_size) >
@@ -2339,23 +2689,59 @@ static int hpsa_get_raid_map(struct ctlr_info *h,
 	return rc;
 }
 
+static int hpsa_bmic_id_physical_device(struct ctlr_info *h,
+		unsigned char scsi3addr[], u16 bmic_device_index,
+		struct bmic_identify_physical_device *buf, size_t bufsize)
+{
+	int rc = IO_OK;
+	struct CommandList *c;
+	struct ErrorInfo *ei;
+
+	c = cmd_alloc(h);
+
+	if (c == NULL) {			/* trouble... */
+		dev_warn(&h->pdev->dev, "cmd_alloc returned NULL!\n");
+		return -ENOMEM;
+	}
+
+	rc = fill_cmd(c, BMIC_IDENTIFY_PHYSICAL_DEVICE, h, buf, bufsize,
+		0, RAID_CTLR_LUNID, TYPE_CMD);
+	if (rc)
+		goto out;
+
+	c->Request.CDB[2] = bmic_device_index & 0xff;
+	c->Request.CDB[9] = (bmic_device_index >> 8) & 0xff;
+
+	rc = hpsa_scsi_do_simple_cmd_with_retry(h, c, PCI_DMA_FROMDEVICE, NO_TIMEOUT);
+	if (rc)
+		goto out;
+	ei = c->err_info;
+	if (ei->CommandStatus != 0 && ei->CommandStatus != CMD_DATA_UNDERRUN) {
+		hpsa_scsi_interpret_error(c);
+		rc = -1;
+	}
+out:
+	cmd_free(h, c);
+	return rc;
+}
+
 static int hpsa_vpd_page_supported(struct ctlr_info *h,
 	unsigned char scsi3addr[], u8 page)
 {
-	int rc;
+        int rc;
 	int i;
 	int pages;
-	unsigned char *buf, bufsize;
+        unsigned char *buf, bufsize;
 
-	buf = kzalloc(256, GFP_KERNEL);
-	if (!buf)
+        buf = kzalloc(256, GFP_KERNEL);
+        if (!buf)
 		return 0;
 
 	/* Get the size of the page list first */
-	rc = hpsa_scsi_do_inquiry(h, scsi3addr,
+        rc = hpsa_scsi_do_inquiry(h, scsi3addr,
 				VPD_PAGE | HPSA_VPD_SUPPORTED_PAGES,
 				buf, HPSA_VPD_HEADER_SZ);
-	if (rc != 0)
+        if (rc != 0)
 		goto exit_unsupported;
 	pages = buf[3];
 	if ((pages + HPSA_VPD_HEADER_SZ) <= 255)
@@ -2364,10 +2750,10 @@ static int hpsa_vpd_page_supported(struct ctlr_info *h,
 		bufsize = 255;
 
 	/* Get the whole VPD page list */
-	rc = hpsa_scsi_do_inquiry(h, scsi3addr,
+        rc = hpsa_scsi_do_inquiry(h, scsi3addr,
 				VPD_PAGE | HPSA_VPD_SUPPORTED_PAGES,
 				buf, bufsize);
-	if (rc != 0)
+        if (rc != 0)
 		goto exit_unsupported;
 
 	pages = buf[3];
@@ -2439,7 +2825,7 @@ static int hpsa_get_device_id(struct ctlr_info *h, unsigned char *scsi3addr,
 }
 
 static int hpsa_scsi_do_report_luns(struct ctlr_info *h, int logical,
-		struct ReportLUNdata *buf, int bufsize,
+		void *buf, int bufsize,
 		int extended_response)
 {
 	int rc = IO_OK;
@@ -2447,9 +2833,9 @@ static int hpsa_scsi_do_report_luns(struct ctlr_info *h, int logical,
 	unsigned char scsi3addr[8];
 	struct ErrorInfo *ei;
 
-	c = cmd_special_alloc(h);
+	c = cmd_alloc(h);
 	if (c == NULL) {			/* trouble... */
-		dev_err(&h->pdev->dev, "cmd_special_alloc returned NULL!\n");
+		dev_err(&h->pdev->dev, "cmd_alloc returned NULL!\n");
 		return -1;
 	}
 	/* address the controller */
@@ -2461,31 +2847,35 @@ static int hpsa_scsi_do_report_luns(struct ctlr_info *h, int logical,
 	}
 	if (extended_response)
 		c->Request.CDB[1] = extended_response;
-	hpsa_scsi_do_simple_cmd_with_retry(h, c, PCI_DMA_FROMDEVICE);
+	rc = hpsa_scsi_do_simple_cmd_with_retry(h, c,
+					PCI_DMA_FROMDEVICE, NO_TIMEOUT);
+	if (rc)
+		goto out;
 	ei = c->err_info;
 	if (ei->CommandStatus != 0 &&
 	    ei->CommandStatus != CMD_DATA_UNDERRUN) {
-		hpsa_scsi_interpret_error(h, c);
+		hpsa_scsi_interpret_error(c);
 		rc = -1;
 	} else {
-		if (buf->extended_response_flag != extended_response) {
+		struct ReportLUNdata *rld = buf;
+		if (rld->extended_response_flag != extended_response) {
 			dev_err(&h->pdev->dev,
 				"report luns requested format %u, got %u\n",
 				extended_response,
-				buf->extended_response_flag);
+				rld->extended_response_flag);
 			rc = -1;
 		}
 	}
 out:
-	cmd_special_free(h, c);
+	cmd_free(h, c);
 	return rc;
 }
 
 static inline int hpsa_scsi_do_report_phys_luns(struct ctlr_info *h,
-		struct ReportLUNdata *buf,
-		int bufsize, int extended_response)
+		struct ReportExtendedLUNdata *buf, int bufsize)
 {
-	return hpsa_scsi_do_report_luns(h, 0, buf, bufsize, extended_response);
+	return hpsa_scsi_do_report_luns(h, 0, buf, bufsize,
+						HPSA_REPORT_PHYS_EXTENDED);
 }
 
 static inline int hpsa_scsi_do_report_log_luns(struct ctlr_info *h,
@@ -2504,15 +2894,15 @@ static inline void hpsa_set_bus_target_lun(struct hpsa_scsi_dev_t *device,
 
 /* Use VPD inquiry to get details of volume status */
 static int hpsa_get_volume_status(struct ctlr_info *h,
-					unsigned char scsi3addr[])
+	unsigned char scsi3addr[])
 {
 	int rc;
-	int status;
+        int status;
 	int size;
-	unsigned char *buf;
+        unsigned char *buf;
 
-	buf = kzalloc(64, GFP_KERNEL);
-	if (!buf)
+        buf = kzalloc(64, GFP_KERNEL);
+        if (!buf)
 		return HPSA_VPD_LV_STATUS_UNSUPPORTED;
 
 	/* Does controller have VPD for logical volume status? */
@@ -2520,14 +2910,14 @@ static int hpsa_get_volume_status(struct ctlr_info *h,
 		goto exit_failed;
 
 	/* Get the size of the VPD return buffer */
-	rc = hpsa_scsi_do_inquiry(h, scsi3addr, VPD_PAGE | HPSA_VPD_LV_STATUS,
+        rc = hpsa_scsi_do_inquiry(h, scsi3addr, VPD_PAGE | HPSA_VPD_LV_STATUS,
 					buf, HPSA_VPD_HEADER_SZ);
 	if (rc != 0)
 		goto exit_failed;
 	size = buf[3];
 
 	/* Now get the whole VPD buffer */
-	rc = hpsa_scsi_do_inquiry(h, scsi3addr, VPD_PAGE | HPSA_VPD_LV_STATUS,
+        rc = hpsa_scsi_do_inquiry(h, scsi3addr, VPD_PAGE | HPSA_VPD_LV_STATUS,
 					buf, size + HPSA_VPD_HEADER_SZ);
 	if (rc != 0)
 		goto exit_failed;
@@ -2536,7 +2926,7 @@ static int hpsa_get_volume_status(struct ctlr_info *h,
 	kfree(buf);
 	return status;
 exit_failed:
-	kfree(buf);
+        kfree(buf);
 	return HPSA_VPD_LV_STATUS_UNSUPPORTED;
 }
 
@@ -2548,11 +2938,12 @@ exit_failed:
  *     describing why a volume is to be kept offline)
  */
 static int hpsa_volume_offline(struct ctlr_info *h,
-					unsigned char scsi3addr[])
+		unsigned char scsi3addr[])
 {
 	struct CommandList *c;
-	unsigned char *sense, sense_key, asc, ascq;
-	int ldstat = 0;
+	unsigned char *sense;
+	int sense_key, asc, ascq, sense_len;
+	int rc, ldstat = 0;
 	u16 cmd_status;
 	u8 scsi_status;
 #define ASC_LUN_NOT_READY 0x04
@@ -2562,67 +2953,125 @@ static int hpsa_volume_offline(struct ctlr_info *h,
 	c = cmd_alloc(h);
 	if (!c)
 		return 0;
-	(void) fill_cmd(c, TEST_UNIT_READY, h, NULL, 0, 0, scsi3addr, TYPE_CMD);
-	hpsa_scsi_do_simple_cmd_core(h, c);
+	fill_cmd(c, TEST_UNIT_READY, h, NULL, 0, 0, scsi3addr, TYPE_CMD);
+	rc = hpsa_scsi_do_simple_cmd_core(h, c, NO_TIMEOUT);
+	if (rc) {
+		cmd_free(h, c);
+		return 0;
+	}
 	sense = c->err_info->SenseInfo;
-	sense_key = sense[2];
-	asc = sense[12];
-	ascq = sense[13];
+	if (c->err_info->SenseLen > sizeof(c->err_info->SenseInfo))
+		sense_len = sizeof(c->err_info->SenseInfo);
+	else
+		sense_len = c->err_info->SenseLen;
+	decode_sense_data(sense, sense_len, &sense_key, &asc, &ascq);
 	cmd_status = c->err_info->CommandStatus;
 	scsi_status = c->err_info->ScsiStatus;
 	cmd_free(h, c);
+
 	/* Is the volume 'not ready'? */
-	if (cmd_status != CMD_TARGET_STATUS ||
-		scsi_status != SAM_STAT_CHECK_CONDITION ||
-		sense_key != NOT_READY ||
+	if (cmd_status != CMD_TARGET_STATUS || 
+		scsi_status != SAM_STAT_CHECK_CONDITION || 
+		sense_key != NOT_READY || 
 		asc != ASC_LUN_NOT_READY)  {
 		return 0;
 	}
-
+		
 	/* Determine the reason for not ready state */
 	ldstat = hpsa_get_volume_status(h, scsi3addr);
 
 	/* Keep volume offline in certain cases: */
 	switch (ldstat) {
-	case HPSA_LV_UNDERGOING_ERASE:
-	case HPSA_LV_UNDERGOING_RPI:
-	case HPSA_LV_PENDING_RPI:
-	case HPSA_LV_ENCRYPTED_NO_KEY:
-	case HPSA_LV_PLAINTEXT_IN_ENCRYPT_ONLY_CONTROLLER:
-	case HPSA_LV_UNDERGOING_ENCRYPTION:
-	case HPSA_LV_UNDERGOING_ENCRYPTION_REKEYING:
-	case HPSA_LV_ENCRYPTED_IN_NON_ENCRYPTED_CONTROLLER:
-		return ldstat;
-	case HPSA_VPD_LV_STATUS_UNSUPPORTED:
-		/* If VPD status page isn't available,
-		 * use ASC/ASCQ to determine state
-		 */
-		if ((ascq == ASCQ_LUN_NOT_READY_FORMAT_IN_PROGRESS) ||
-			(ascq == ASCQ_LUN_NOT_READY_INITIALIZING_CMD_REQ))
+		case HPSA_LV_UNDERGOING_ERASE:
+		case HPSA_LV_UNDERGOING_RPI:
+		case HPSA_LV_PENDING_RPI:
+		case HPSA_LV_ENCRYPTED_NO_KEY:
+		case HPSA_LV_PLAINTEXT_IN_ENCRYPT_ONLY_CONTROLLER:
+		case HPSA_LV_UNDERGOING_ENCRYPTION:
+		case HPSA_LV_UNDERGOING_ENCRYPTION_REKEYING:
+		case HPSA_LV_ENCRYPTED_IN_NON_ENCRYPTED_CONTROLLER:
 			return ldstat;
-		break;
-	default:
-		break;
+		case HPSA_VPD_LV_STATUS_UNSUPPORTED:
+			/* If VPD status page isn't available,
+			 * use ASC/ASCQ to determine state
+			 */
+			if ((ascq == ASCQ_LUN_NOT_READY_FORMAT_IN_PROGRESS) ||
+				(ascq == ASCQ_LUN_NOT_READY_INITIALIZING_CMD_REQ))
+				return ldstat;
+			break;
+		default:
+			break;
 	}
 	return 0;
 }
 
-static int hpsa_update_device_info(struct ctlr_info *h,
-	unsigned char scsi3addr[], struct hpsa_scsi_dev_t *this_device,
-	unsigned char *is_OBDR_device)
+/* Find out if a logical device supports aborts by simply trying one.
+ * Smart Array may claim not to support aborts on logical drives, but
+ * if a MSA2000 * is connected, the drives on that will be presented
+ * by the Smart Array as logical drives, and aborts may be sent to
+ * those devices successfully.  So the simplest way to find out is
+ * to simply try an abort and see how the device responds.
+ */
+static int hpsa_device_supports_aborts(struct ctlr_info *h,
+					unsigned char *scsi3addr)
 {
+	struct CommandList *c;
+	struct ErrorInfo *ei;
+	int rc = 0;
 
-#define OBDR_SIG_OFFSET 43
-#define OBDR_TAPE_SIG "$DR-10"
-#define OBDR_SIG_LEN (sizeof(OBDR_TAPE_SIG) - 1)
-#define OBDR_TAPE_INQ_SIZE (OBDR_SIG_OFFSET + OBDR_SIG_LEN)
+	u64 tag = (u64) -1; /* bogus tag */
 
-	unsigned char *inq_buff;
-	unsigned char *obdr_sig;
+	/* Assume that physical devices support aborts */
+	if (!is_logical_dev_addr_mode(scsi3addr))
+		return 1;
+
+	c = cmd_alloc(h);
+	if (!c)
+		return -ENOMEM;
+	(void) fill_cmd(c, HPSA_ABORT_MSG, h, &tag, 0, 0, scsi3addr, TYPE_MSG);
+	(void) __hpsa_scsi_do_simple_cmd_core(h, c, 0, NO_TIMEOUT);
+	/* no unmap needed here because no data xfer. */
+	ei = c->err_info;
+	switch (ei->CommandStatus) {
+	case CMD_INVALID:
+		rc = 0;
+		break;
+	case CMD_UNABORTABLE:
+	case CMD_ABORT_FAILED:
+		rc = 1;
+		break;
+	case CMD_TMF_STATUS:
+		rc = hpsa_evaluate_tmf_status(h, c);
+		break;
+	default:
+		rc = 0;
+		break;
+	}
+	cmd_free(h, c);
+	return rc;
+}
+
+static int hpsa_update_device_info(struct ctlr_info *h,
+	unsigned char scsi3addr[], struct hpsa_scsi_dev_t *this_device,
+	unsigned char *is_OBDR_device)
+{
+
+#define OBDR_SIG_OFFSET 43
+#define OBDR_TAPE_SIG "$DR-10"
+#define OBDR_SIG_LEN (sizeof(OBDR_TAPE_SIG) - 1)
+#define OBDR_TAPE_INQ_SIZE (OBDR_SIG_OFFSET + OBDR_SIG_LEN)
+
+	unsigned char *inq_buff;
+	unsigned char *obdr_sig;
+	unsigned long flags;
+	int entry;
+	int rc = 0;
 
 	inq_buff = kzalloc(OBDR_TAPE_INQ_SIZE, GFP_KERNEL);
-	if (!inq_buff)
+	if (!inq_buff) {
+		rc = -ENOMEM;
 		goto bail_out;
+	}
 
 	/* Do an inquiry to the device to see what it is. */
 	if (hpsa_scsi_do_inquiry(h, scsi3addr, 0, inq_buff,
@@ -2630,6 +3079,7 @@ static int hpsa_update_device_info(struct ctlr_info *h,
 		/* Inquiry failed (msg printed already) */
 		dev_err(&h->pdev->dev,
 			"hpsa_update_device_info: inquiry failed\n");
+		rc = 1;
 		goto bail_out;
 	}
 
@@ -2659,6 +3109,7 @@ static int hpsa_update_device_info(struct ctlr_info *h,
 		this_device->raid_level = RAID_UNKNOWN;
 		this_device->offload_config = 0;
 		this_device->offload_enabled = 0;
+		this_device->hba_ioaccel_enabled = 0;
 		this_device->volume_offline = 0;
 	}
 
@@ -2671,13 +3122,31 @@ static int hpsa_update_device_info(struct ctlr_info *h,
 					strncmp(obdr_sig, OBDR_TAPE_SIG,
 						OBDR_SIG_LEN) == 0);
 	}
-
 	kfree(inq_buff);
+
+	/*
+	 * See if this device supports aborts.  If we already know
+	 * the device, we already know if it supports aborts, otherwise
+	 * we have to find out if it supports aborts by trying one.
+	 */
+	spin_lock_irqsave(&h->devlock, flags);
+	rc = hpsa_scsi_find_entry(this_device, h->dev, h->ndevices, &entry);
+	if ((rc == DEVICE_SAME || rc == DEVICE_UPDATED) &&
+		entry >= 0 && entry < h->ndevices) {
+		this_device->supports_aborts = h->dev[entry]->supports_aborts;
+		spin_unlock_irqrestore(&h->devlock, flags);
+	} else {
+		spin_unlock_irqrestore(&h->devlock, flags);
+		this_device->supports_aborts =
+				hpsa_device_supports_aborts(h, scsi3addr);
+		if (this_device->supports_aborts < 0)
+			this_device->supports_aborts = 0;
+	}
 	return 0;
 
 bail_out:
 	kfree(inq_buff);
-	return 1;
+	return rc;
 }
 
 static unsigned char *ext_target_model[] = {
@@ -2791,99 +3260,32 @@ static int add_ext_target_dev(struct ctlr_info *h,
 
 /*
  * Get address of physical disk used for an ioaccel2 mode command:
- *	1. Extract ioaccel2 handle from the command.
- *	2. Find a matching ioaccel2 handle from list of physical disks.
- *	3. Return:
- *		1 and set scsi3addr to address of matching physical
- *		0 if no matching physical disk was found.
+ * 	1. Extract ioaccel2 handle from the command.
+	2. Find a matching ioaccel2 handle from list of physical disks.
+ *	3. Return: 	
+ * 		1 and set scsi3addr to address of matching physical
+ * 		0 if no matching physical disk was found.
  */
 static int hpsa_get_pdisk_of_ioaccel2(struct ctlr_info *h,
 	struct CommandList *ioaccel2_cmd_to_abort, unsigned char *scsi3addr)
 {
-	struct ReportExtendedLUNdata *physicals = NULL;
-	int responsesize = 24;	/* size of physical extended response */
-	int extended = 2;	/* flag forces reporting 'other dev info'. */
-	int reportsize = sizeof(*physicals) + HPSA_MAX_PHYS_LUN * responsesize;
-	u32 nphysicals = 0;	/* number of reported physical devs */
-	int found = 0;		/* found match (1) or not (0) */
-	u32 find;		/* handle we need to match */
+	struct io_accel2_cmd *c2 =
+			&h->ioaccel2_cmd_pool[ioaccel2_cmd_to_abort->cmdindex];
+	unsigned long flags;
 	int i;
-	struct scsi_cmnd *scmd;	/* scsi command within request being aborted */
-	struct hpsa_scsi_dev_t *d; /* device of request being aborted */
-	struct io_accel2_cmd *c2a; /* ioaccel2 command to abort */
-	u32 it_nexus;		/* 4 byte device handle for the ioaccel2 cmd */
-	u32 scsi_nexus;		/* 4 byte device handle for the ioaccel2 cmd */
-
-	if (ioaccel2_cmd_to_abort->cmd_type != CMD_IOACCEL2)
-		return 0; /* no match */
-
-	/* point to the ioaccel2 device handle */
-	c2a = &h->ioaccel2_cmd_pool[ioaccel2_cmd_to_abort->cmdindex];
-	if (c2a == NULL)
-		return 0; /* no match */
-
-	scmd = (struct scsi_cmnd *) ioaccel2_cmd_to_abort->scsi_cmd;
-	if (scmd == NULL)
-		return 0; /* no match */
-
-	d = scmd->device->hostdata;
-	if (d == NULL)
-		return 0; /* no match */
-
-	it_nexus = cpu_to_le32((u32) d->ioaccel_handle);
-	scsi_nexus = cpu_to_le32((u32) c2a->scsi_nexus);
-	find = c2a->scsi_nexus;
-
-	if (h->raid_offload_debug > 0)
-		dev_info(&h->pdev->dev,
-			"%s: scsi_nexus:0x%08x device id: 0x%02x%02x%02x%02x %02x%02x%02x%02x %02x%02x%02x%02x %02x%02x%02x%02x\n",
-			__func__, scsi_nexus,
-			d->device_id[0], d->device_id[1], d->device_id[2],
-			d->device_id[3], d->device_id[4], d->device_id[5],
-			d->device_id[6], d->device_id[7], d->device_id[8],
-			d->device_id[9], d->device_id[10], d->device_id[11],
-			d->device_id[12], d->device_id[13], d->device_id[14],
-			d->device_id[15]);
-
-	/* Get the list of physical devices */
-	physicals = kzalloc(reportsize, GFP_KERNEL);
-	if (physicals == NULL)
-		return 0;
-	if (hpsa_scsi_do_report_phys_luns(h, (struct ReportLUNdata *) physicals,
-		reportsize, extended)) {
-		dev_err(&h->pdev->dev,
-			"Can't lookup %s device handle: report physical LUNs failed.\n",
-			"HP SSD Smart Path");
-		kfree(physicals);
-		return 0;
-	}
-	nphysicals = be32_to_cpu(*((__be32 *)physicals->LUNListLength)) /
-							responsesize;
-
-	/* find ioaccel2 handle in list of physicals: */
-	for (i = 0; i < nphysicals; i++) {
-		struct ext_report_lun_entry *entry = &physicals->LUN[i];
-
-		/* handle is in bytes 28-31 of each lun */
-		if (entry->ioaccel_handle != find)
-			continue; /* didn't match */
-		found = 1;
-		memcpy(scsi3addr, entry->lunid, 8);
-		if (h->raid_offload_debug > 0)
-			dev_info(&h->pdev->dev,
-				"%s: Searched h=0x%08x, Found h=0x%08x, scsiaddr 0x%8phN\n",
-				__func__, find,
-				entry->ioaccel_handle, scsi3addr);
-		break; /* found it */
-	}
-
-	kfree(physicals);
-	if (found)
-		return 1;
-	else
-		return 0;
 
+	spin_lock_irqsave(&h->devlock, flags);
+	for (i = 0; i < h->ndevices; i++)
+		if (h->dev[i]->ioaccel_handle == c2->scsi_nexus) {
+			memcpy(scsi3addr, h->dev[i]->scsi3addr,
+				sizeof(h->dev[i]->scsi3addr));
+			spin_unlock_irqrestore(&h->devlock, flags);
+			return 1;
+		}
+	spin_unlock_irqrestore(&h->devlock, flags);
+	return 0;
 }
+
 /*
  * Do CISS_REPORT_PHYS and CISS_REPORT_LOG.  Data is returned in physdev,
  * logdev.  The number of luns in physdev and logdev are returned in
@@ -2891,34 +3293,21 @@ static int hpsa_get_pdisk_of_ioaccel2(struct ctlr_info *h,
  * Returns 0 on success, -1 otherwise.
  */
 static int hpsa_gather_lun_info(struct ctlr_info *h,
-	int reportlunsize,
-	struct ReportLUNdata *physdev, u32 *nphysicals, int *physical_mode,
+	struct ReportExtendedLUNdata *physdev, u32 *nphysicals,
 	struct ReportLUNdata *logdev, u32 *nlogicals)
 {
-	int physical_entry_size = 8;
-
-	*physical_mode = 0;
-
-	/* For I/O accelerator mode we need to read physical device handles */
-	if (h->transMethod & CFGTBL_Trans_io_accel1 ||
-		h->transMethod & CFGTBL_Trans_io_accel2) {
-		*physical_mode = HPSA_REPORT_PHYS_EXTENDED;
-		physical_entry_size = 24;
-	}
-	if (hpsa_scsi_do_report_phys_luns(h, physdev, reportlunsize,
-							*physical_mode)) {
+	if (hpsa_scsi_do_report_phys_luns(h, physdev, sizeof(*physdev))) {
 		dev_err(&h->pdev->dev, "report physical LUNs failed.\n");
 		return -1;
 	}
-	*nphysicals = be32_to_cpu(*((__be32 *)physdev->LUNListLength)) /
-							physical_entry_size;
+	*nphysicals = be32_to_cpu(*((__be32 *)physdev->LUNListLength)) / 24;
 	if (*nphysicals > HPSA_MAX_PHYS_LUN) {
 		dev_warn(&h->pdev->dev, "maximum physical LUNs (%d) exceeded."
 			"  %d LUNs ignored.\n", HPSA_MAX_PHYS_LUN,
 			*nphysicals - HPSA_MAX_PHYS_LUN);
 		*nphysicals = HPSA_MAX_PHYS_LUN;
 	}
-	if (hpsa_scsi_do_report_log_luns(h, logdev, reportlunsize)) {
+	if (hpsa_scsi_do_report_log_luns(h, logdev, sizeof(*logdev))) {
 		dev_err(&h->pdev->dev, "report logical LUNs failed.\n");
 		return -1;
 	}
@@ -2957,10 +3346,10 @@ u8 *figure_lunaddrbytes(struct ctlr_info *h, int raid_ctlr_position, int i,
 	if (i == raid_ctlr_position)
 		return RAID_CTLR_LUNID;
 
-	if (i < logicals_start)
-		return &physdev_list->LUN[i -
-				(raid_ctlr_position == 0)].lunid[0];
-
+	if (i < logicals_start) {
+		const int index = i - (raid_ctlr_position == 0);
+		return &physdev_list->LUN[index].lunid[0];
+	}
 	if (i < last_device)
 		return &logdev_list->LUN[i - nphysicals -
 			(raid_ctlr_position == 0)][0];
@@ -2974,23 +3363,47 @@ static int hpsa_hba_mode_enabled(struct ctlr_info *h)
 	int hba_mode_enabled;
 	struct bmic_controller_parameters *ctlr_params;
 	ctlr_params = kzalloc(sizeof(struct bmic_controller_parameters),
-		GFP_KERNEL);
-
+				GFP_KERNEL);
+	
 	if (!ctlr_params)
 		return -ENOMEM;
 	rc = hpsa_bmic_ctrl_mode_sense(h, RAID_CTLR_LUNID, 0, ctlr_params,
-		sizeof(struct bmic_controller_parameters));
-	if (rc) {
+				sizeof(struct bmic_controller_parameters));
+	if (rc != 0) {
 		kfree(ctlr_params);
 		return rc;
 	}
-
 	hba_mode_enabled =
 		((ctlr_params->nvram_flags & HBA_MODE_ENABLED_FLAG) != 0);
 	kfree(ctlr_params);
 	return hba_mode_enabled;
 }
 
+/* get physical drive ioaccel handle and queue depth */
+void hpsa_get_ioaccel_drive_info(struct ctlr_info *h,
+		struct hpsa_scsi_dev_t *dev,
+		u8 *lunaddrbytes,
+		struct bmic_identify_physical_device *id_phys)
+{
+	int rc;
+	struct ext_report_lun_entry *rle =
+		(struct ext_report_lun_entry *) lunaddrbytes;
+
+	dev->ioaccel_handle = rle->ioaccel_handle;
+	memset(id_phys, 0, sizeof(*id_phys));
+	rc = hpsa_bmic_id_physical_device(h, lunaddrbytes,
+			GET_BMIC_DRIVE_NUMBER(lunaddrbytes), id_phys,
+			sizeof(*id_phys));
+	if (!rc)
+		/* Reserve space for FW operations */
+#define DRIVE_CMDS_RESERVED_FOR_FW 2
+		dev->queue_depth =
+			le16_to_cpu(id_phys->current_queue_depth_limit) -
+				DRIVE_CMDS_RESERVED_FOR_FW;
+	else
+		dev->queue_depth = 7; /* conservative */
+}
+
 static void hpsa_update_scsi_devices(struct ctlr_info *h, int hostno)
 {
 	/* the idea here is we could get notified
@@ -3005,44 +3418,50 @@ static void hpsa_update_scsi_devices(struct ctlr_info *h, int hostno)
 	 */
 	struct ReportExtendedLUNdata *physdev_list = NULL;
 	struct ReportLUNdata *logdev_list = NULL;
+	struct bmic_identify_physical_device *id_phys = NULL;
 	u32 nphysicals = 0;
 	u32 nlogicals = 0;
-	int physical_mode = 0;
 	u32 ndev_allocated = 0;
 	struct hpsa_scsi_dev_t **currentsd, *this_device, *tmpdevice;
 	int ncurrent = 0;
-	int reportlunsize = sizeof(*physdev_list) + HPSA_MAX_PHYS_LUN * 24;
 	int i, n_ext_target_devs, ndevs_to_allocate;
+	int rc = 0;
 	int raid_ctlr_position;
 	int rescan_hba_mode;
 	DECLARE_BITMAP(lunzerobits, MAX_EXT_TARGETS);
 
 	currentsd = kzalloc(sizeof(*currentsd) * HPSA_MAX_DEVICES, GFP_KERNEL);
-	physdev_list = kzalloc(reportlunsize, GFP_KERNEL);
-	logdev_list = kzalloc(reportlunsize, GFP_KERNEL);
+	physdev_list = kzalloc(sizeof(*physdev_list), GFP_KERNEL);
+	logdev_list = kzalloc(sizeof(*logdev_list), GFP_KERNEL);
 	tmpdevice = kzalloc(sizeof(*tmpdevice), GFP_KERNEL);
+	id_phys = kzalloc(sizeof(*id_phys), GFP_KERNEL);
 
-	if (!currentsd || !physdev_list || !logdev_list || !tmpdevice) {
+	if (!currentsd || !physdev_list || !logdev_list ||
+		!tmpdevice || !id_phys) {
 		dev_err(&h->pdev->dev, "out of memory\n");
 		goto out;
 	}
 	memset(lunzerobits, 0, sizeof(lunzerobits));
 
 	rescan_hba_mode = hpsa_hba_mode_enabled(h);
-	if (rescan_hba_mode < 0)
+
+	if (rescan_hba_mode < 0) {
+		h->drv_req_rescan = 1;
 		goto out;
+	}
 
 	if (!h->hba_mode_enabled && rescan_hba_mode)
-		dev_warn(&h->pdev->dev, "HBA mode enabled\n");
+		dev_info(&h->pdev->dev, "HBA mode enabled\n");
 	else if (h->hba_mode_enabled && !rescan_hba_mode)
-		dev_warn(&h->pdev->dev, "HBA mode disabled\n");
+		dev_info(&h->pdev->dev, "HBA mode disabled\n");
 
 	h->hba_mode_enabled = rescan_hba_mode;
 
-	if (hpsa_gather_lun_info(h, reportlunsize,
-			(struct ReportLUNdata *) physdev_list, &nphysicals,
-			&physical_mode, logdev_list, &nlogicals))
+	if (hpsa_gather_lun_info(h, physdev_list, &nphysicals,
+			logdev_list, &nlogicals)) {
+		h->drv_req_rescan = 1;
 		goto out;
+	}
 
 	/* We might see up to the maximum number of logical and physical disks
 	 * plus external target devices, and a device for the local RAID
@@ -3063,6 +3482,7 @@ static void hpsa_update_scsi_devices(struct ctlr_info *h, int hostno)
 		if (!currentsd[i]) {
 			dev_warn(&h->pdev->dev, "out of memory at %s:%d\n",
 				__FILE__, __LINE__);
+			h->drv_req_rescan = 1;
 			goto out;
 		}
 		ndev_allocated++;
@@ -3081,15 +3501,23 @@ static void hpsa_update_scsi_devices(struct ctlr_info *h, int hostno)
 		/* Figure out where the LUN ID info is coming from */
 		lunaddrbytes = figure_lunaddrbytes(h, raid_ctlr_position,
 			i, nphysicals, nlogicals, physdev_list, logdev_list);
-		/* skip masked physical devices. */
-		if (lunaddrbytes[3] & 0xC0 &&
-			i < nphysicals + (raid_ctlr_position == 0))
-			continue;
+		/* skip masked non-disk devices */
+		if (MASKED_DEVICE(lunaddrbytes))
+			if (i < nphysicals + (raid_ctlr_position == 0) &&
+				NON_DISK_PHYS_DEV(lunaddrbytes))
+				continue;
 
 		/* Get device type, vendor, model, device id */
-		if (hpsa_update_device_info(h, lunaddrbytes, tmpdevice,
-							&is_OBDR))
-			continue; /* skip it if we can't talk to it. */
+		rc = hpsa_update_device_info(h, lunaddrbytes, tmpdevice,
+							&is_OBDR);
+		if (rc) {
+			dev_warn(&h->pdev->dev, "%s, rescan stopped.\n",
+				(rc == -ENOMEM) ? "Out of memory" :
+				"Inquiry failed");
+			h->drv_req_rescan = 1;
+			goto out;
+		}
+
 		figure_bus_target_lun(h, lunaddrbytes, tmpdevice);
 		this_device = currentsd[ncurrent];
 
@@ -3109,46 +3537,96 @@ static void hpsa_update_scsi_devices(struct ctlr_info *h, int hostno)
 
 		*this_device = *tmpdevice;
 
+		this_device->bay = 0;
+		this_device->active_path_index = 0;
+		this_device->path_map = 0;
+		memset(this_device->box, 0, sizeof(this_device->box));
+		memset(this_device->phys_connector, 0,
+			sizeof(this_device->phys_connector));
+
+		/* do not expose masked devices */
+		if (MASKED_DEVICE(lunaddrbytes) &&
+			i < nphysicals + (raid_ctlr_position == 0)) {
+			if (h->hba_mode_enabled)
+				dev_warn(&h->pdev->dev,
+					"Masked physical device detected\n");
+			this_device->expose_state = HPSA_DO_NOT_EXPOSE;
+		} else {
+			this_device->expose_state = HPSA_EXPOSE;
+		}
+
 		switch (this_device->devtype) {
 		case TYPE_ROM:
 			/* We don't *really* support actual CD-ROM devices,
-			 * just "One Button Disaster Recovery" tape drive
-			 * which temporarily pretends to be a CD-ROM drive.
-			 * So we check that the device is really an OBDR tape
-			 * device by checking for "$DR-10" in bytes 43-48 of
-			 * the inquiry data.
+			 * on Smart Arrays, just "One Button Disaster
+			 * Recovery" tape drive which temporarily pretends to
+			 * be a CD-ROM drive.  So we check that the device is
+			 * really an OBDR tape device by checking for "$DR-10"
+			 * in bytes 43-48 of the inquiry data.
 			 */
 			if (is_OBDR)
 				ncurrent++;
 			break;
 		case TYPE_DISK:
+			/* HBA mode supported*/
 			if (h->hba_mode_enabled) {
 				/* never use raid mapper in HBA mode */
 				this_device->offload_enabled = 0;
+				hpsa_get_ioaccel_drive_info(h, this_device,
+						lunaddrbytes, id_phys);
+
+				if (PHYS_IOACCEL(lunaddrbytes)
+					&& this_device->ioaccel_handle)
+					this_device->hba_ioaccel_enabled = 1;
+
+				memcpy(&this_device->active_path_index,
+					&id_phys->active_path_number,
+					sizeof(this_device->active_path_index));
+				memcpy(&this_device->path_map,
+					&id_phys->redundant_path_present_map,
+					sizeof(this_device->path_map));
+				memcpy(&this_device->box,
+					&id_phys->alternate_paths_phys_box_on_port,
+					sizeof(this_device->box));
+				memcpy(&this_device->phys_connector,
+					&id_phys->alternate_paths_phys_connector,
+					sizeof(this_device->phys_connector));
+				memcpy(&this_device->bay,
+					&id_phys->phys_bay_in_box,
+					sizeof(this_device->bay));
+
 				ncurrent++;
+				memset(id_phys, 0, sizeof(*id_phys));
 				break;
-			} else if (h->acciopath_status) {
+			}
+			/* HP SSD Smart Path Mode supported */
+			else if (h->acciopath_status) {
 				if (i >= nphysicals) {
 					ncurrent++;
 					break;
 				}
-			} else {
+				if (h->transMethod & CFGTBL_Trans_io_accel1 ||
+					h->transMethod & CFGTBL_Trans_io_accel2) {
+					hpsa_get_ioaccel_drive_info(h, this_device,
+							lunaddrbytes, id_phys);
+					ncurrent++;
+				}
+				break;
+			}
+			else {  /* normal mode: no ioaccel or HBA mode */
 				if (i < nphysicals)
 					break;
 				ncurrent++;
 				break;
-			}
-			if (physical_mode == HPSA_REPORT_PHYS_EXTENDED) {
-				memcpy(&this_device->ioaccel_handle,
-					&lunaddrbytes[20],
-					sizeof(this_device->ioaccel_handle));
-				ncurrent++;
-			}
-			break;
+                        }
 		case TYPE_TAPE:
 		case TYPE_MEDIUM_CHANGER:
 			ncurrent++;
 			break;
+		case TYPE_ENCLOSURE:
+			if (h->hba_mode_enabled)
+				ncurrent++;
+			break;
 		case TYPE_RAID:
 			/* Only present the Smartarray HBA as a RAID controller.
 			 * If it's a RAID controller other than the HBA itself
@@ -3168,13 +3646,17 @@ static void hpsa_update_scsi_devices(struct ctlr_info *h, int hostno)
 	adjust_hpsa_scsi_table(h, hostno, currentsd, ncurrent);
 out:
 	kfree(tmpdevice);
-	for (i = 0; i < ndev_allocated; i++)
-		kfree(currentsd[i]);
-	kfree(currentsd);
+	if (currentsd) {
+		for (i = 0; i < ndev_allocated; i++)
+			kfree(currentsd[i]);
+		kfree(currentsd);
+	}
 	kfree(physdev_list);
 	kfree(logdev_list);
+	kfree(id_phys);
 }
 
+#if KFEATURE_HAS_SCSI_DMA_FUNCTIONS
 /* hpsa_scatter_gather takes a struct scsi_cmnd, (cmd), and does the pci
  * dma mapping  and fills in the scatter gather entries of the
  * hpsa command, cp.
@@ -3186,7 +3668,7 @@ static int hpsa_scatter_gather(struct ctlr_info *h,
 	unsigned int len;
 	struct scatterlist *sg;
 	u64 addr64;
-	int use_sg, i, sg_index, chained;
+	int use_sg, i, sg_index, chained, last_sg;
 	struct SGDescriptor *curr_sg;
 
 	BUG_ON(scsi_sg_count(cmd) > h->maxsgentries);
@@ -3201,6 +3683,7 @@ static int hpsa_scatter_gather(struct ctlr_info *h,
 	curr_sg = cp->SG;
 	chained = 0;
 	sg_index = 0;
+	last_sg = scsi_sg_count(cmd) - 1;
 	scsi_for_each_sg(cmd, sg, use_sg, i) {
 		if (i == h->max_cmd_sg_entries - 1 &&
 			use_sg > h->max_cmd_sg_entries) {
@@ -3210,10 +3693,9 @@ static int hpsa_scatter_gather(struct ctlr_info *h,
 		}
 		addr64 = (u64) sg_dma_address(sg);
 		len  = sg_dma_len(sg);
-		curr_sg->Addr.lower = (u32) (addr64 & 0x0FFFFFFFFULL);
-		curr_sg->Addr.upper = (u32) ((addr64 >> 32) & 0x0FFFFFFFFULL);
-		curr_sg->Len = len;
-		curr_sg->Ext = (i < scsi_sg_count(cmd) - 1) ? 0 : HPSA_SG_LAST;
+		curr_sg->Addr = cpu_to_le64(addr64);
+		curr_sg->Len = cpu_to_le32(len);
+		curr_sg->Ext = cpu_to_le32((i == last_sg) * HPSA_SG_LAST);
 		curr_sg++;
 	}
 
@@ -3236,6 +3718,9 @@ sglist_finished:
 	cp->Header.SGTotal = (u16) use_sg; /* total sgs in this cmd list */
 	return 0;
 }
+#endif /* if KFEATURE_HAS_SCSI_DMA_FUNCTIONS
+	* otherwise hpsa_scatter_gather() is defined in hpsa_kernel_compat.h
+	*/
 
 #define IO_ACCEL_INELIGIBLE (1)
 static int fixup_ioaccel_cdb(u8 *cdb, int *cdb_len)
@@ -3325,15 +3810,13 @@ static int hpsa_scsi_ioaccel1_queue_command(struct ctlr_info *h,
 			addr64 = (u64) sg_dma_address(sg);
 			len  = sg_dma_len(sg);
 			total_len += len;
-			curr_sg->Addr.lower = (u32) (addr64 & 0x0FFFFFFFFULL);
-			curr_sg->Addr.upper =
-				(u32) ((addr64 >> 32) & 0x0FFFFFFFFULL);
-			curr_sg->Len = len;
+			curr_sg->Addr = cpu_to_le64(addr64);
+			curr_sg->Len = cpu_to_le32(len);
 
 			if (i == (scsi_sg_count(cmd) - 1))
-				curr_sg->Ext = HPSA_SG_LAST;
+				curr_sg->Ext = cpu_to_le32(HPSA_SG_LAST);
 			else
-				curr_sg->Ext = 0;  /* we are not chaining */
+				curr_sg->Ext = cpu_to_le32(0); /* not chain */
 			curr_sg++;
 		}
 
@@ -3370,7 +3853,7 @@ static int hpsa_scsi_ioaccel1_queue_command(struct ctlr_info *h,
 	enqueue_cmd_and_start_io(h, c);
 	return 0;
 }
-
+ 
 /*
  * Queue a command directly to a device behind the controller using the
  * I/O accelerator path.
@@ -3385,8 +3868,87 @@ static int hpsa_scsi_ioaccel_direct_map(struct ctlr_info *h,
 		cmd->cmnd, cmd->cmd_len, dev->scsi3addr);
 }
 
+static void __attribute__((unused)) verify_offsets(void)
+{
+#define VERIFY_OFFSET(member, offset) \
+	BUILD_BUG_ON(offsetof(struct raid_map_data, member) != offset)
+
+	VERIFY_OFFSET(structure_size, 0);
+	VERIFY_OFFSET(volume_blk_size, 4);
+	VERIFY_OFFSET(volume_blk_cnt, 8);
+	VERIFY_OFFSET(phys_blk_shift, 16);
+	VERIFY_OFFSET(parity_rotation_shift, 17);
+	VERIFY_OFFSET(strip_size, 18);
+	VERIFY_OFFSET(disk_starting_blk, 20);
+	VERIFY_OFFSET(disk_blk_cnt, 28);
+	VERIFY_OFFSET(data_disks_per_row, 36);
+	VERIFY_OFFSET(metadata_disks_per_row, 38);
+	VERIFY_OFFSET(row_cnt, 40);
+	VERIFY_OFFSET(layout_map_count, 42);
+	VERIFY_OFFSET(flags, 44);
+	VERIFY_OFFSET(dekindex, 46);
+	/* VERIFY_OFFSET(reserved, 48 */
+	VERIFY_OFFSET(data, 64);
+
+#undef VERIFY_OFFSET
+
+#define VERIFY_OFFSET(member, offset) \
+	BUILD_BUG_ON(offsetof(struct io_accel2_cmd, member) != offset)
+
+	VERIFY_OFFSET(IU_type, 0);
+	VERIFY_OFFSET(direction, 1);
+	VERIFY_OFFSET(reply_queue, 2);
+	/* VERIFY_OFFSET(reserved1, 3);  */
+	VERIFY_OFFSET(scsi_nexus, 4);
+	VERIFY_OFFSET(Tag, 8);
+	VERIFY_OFFSET(tweak_lower, 12);
+	VERIFY_OFFSET(cdb, 16);
+	VERIFY_OFFSET(cciss_lun, 32);
+	VERIFY_OFFSET(data_len, 40);
+	VERIFY_OFFSET(cmd_priority_task_attr, 44);
+	VERIFY_OFFSET(sg_count, 45); 
+	VERIFY_OFFSET(dekindex, 46);
+	VERIFY_OFFSET(err_ptr, 48);
+	VERIFY_OFFSET(err_len, 56);
+	VERIFY_OFFSET(tweak_upper, 60);
+	VERIFY_OFFSET(sg, 64);
+
+#undef VERIFY_OFFSET
+
+#define VERIFY_OFFSET(member, offset) \
+	BUILD_BUG_ON(offsetof(struct io_accel1_cmd, member) != offset)
+	
+	VERIFY_OFFSET(dev_handle, 0x00);
+	VERIFY_OFFSET(reserved1, 0x02);
+	VERIFY_OFFSET(function, 0x03);
+	VERIFY_OFFSET(reserved2, 0x04);
+	VERIFY_OFFSET(err_info, 0x0C);
+	VERIFY_OFFSET(reserved3, 0x10);
+	VERIFY_OFFSET(err_info_len, 0x12);
+	VERIFY_OFFSET(reserved4, 0x13);
+	VERIFY_OFFSET(sgl_offset, 0x14);
+	VERIFY_OFFSET(reserved5, 0x15);
+	VERIFY_OFFSET(transfer_len, 0x1C);
+	VERIFY_OFFSET(reserved6, 0x20);
+	VERIFY_OFFSET(io_flags, 0x24);
+	VERIFY_OFFSET(reserved7, 0x26);
+	VERIFY_OFFSET(LUN, 0x34);
+	VERIFY_OFFSET(control, 0x3C);
+	VERIFY_OFFSET(CDB, 0x40);
+	VERIFY_OFFSET(reserved8, 0x50);
+	VERIFY_OFFSET(host_context_flags, 0x60);
+	VERIFY_OFFSET(timeout_sec, 0x62);
+	VERIFY_OFFSET(ReplyQueue, 0x64);
+	VERIFY_OFFSET(reserved9, 0x65);
+	VERIFY_OFFSET(tag, 0x68);
+	VERIFY_OFFSET(host_addr, 0x70);
+	VERIFY_OFFSET(CISS_LUN, 0x78);
+	VERIFY_OFFSET(SG, 0x78 + 8);
+#undef VERIFY_OFFSET
+}
+
 /*
- * Set encryption parameters for the ioaccel2 request
+ * Set encryption parameters for the ioaccel2 request 
  */
 static void set_encrypt_ioaccel2(struct ctlr_info *h,
 	struct CommandList *c, struct io_accel2_cmd *cp)
@@ -3396,10 +3958,8 @@ static void set_encrypt_ioaccel2(struct ctlr_info *h,
 	struct raid_map_data *map = &dev->raid_map;
 	u64 first_block;
 
-	BUG_ON(!(dev->offload_config && dev->offload_enabled));
-
 	/* Are we doing encryption on this device */
-	if (!(map->flags & RAID_MAP_FLAG_ENCRYPT_ON))
+	if (! (map->flags & RAID_MAP_FLAG_ENCRYPT_ON) )
 		return;
 	/* Set the data encryption key index. */
 	cp->dekindex = map->dekindex;
@@ -3415,13 +3975,14 @@ static void set_encrypt_ioaccel2(struct ctlr_info *h,
 	/* Required? 6-byte cdbs eliminated by fixup_ioaccel_cdb */
 	case WRITE_6:
 	case READ_6:
-		if (map->volume_blk_size == 512) {
-			cp->tweak_lower =
+		if (map->volume_blk_size == 512 ) {
+			cp->tweak_lower = 
 				(((u32) cmd->cmnd[2]) << 8) |
 					cmd->cmnd[3];
-			cp->tweak_upper = 0;
-		} else {
-			first_block =
+			cp->tweak_upper = 0; 
+		}
+		else {
+			first_block = 
 				(((u64) cmd->cmnd[2]) << 8) |
 					cmd->cmnd[3];
 			first_block = (first_block * map->volume_blk_size)/512;
@@ -3431,14 +3992,15 @@ static void set_encrypt_ioaccel2(struct ctlr_info *h,
 		break;
 	case WRITE_10:
 	case READ_10:
-		if (map->volume_blk_size == 512) {
-			cp->tweak_lower =
+		if (map->volume_blk_size == 512 ) {
+			cp->tweak_lower = 
 				(((u32) cmd->cmnd[2]) << 24) |
 				(((u32) cmd->cmnd[3]) << 16) |
 				(((u32) cmd->cmnd[4]) << 8) |
 					cmd->cmnd[5];
-			cp->tweak_upper = 0;
-		} else {
+			cp->tweak_upper = 0; 
+		}
+		else {
 			first_block =
 				(((u64) cmd->cmnd[2]) << 24) |
 				(((u64) cmd->cmnd[3]) << 16) |
@@ -3452,14 +4014,15 @@ static void set_encrypt_ioaccel2(struct ctlr_info *h,
 	/* Required? 12-byte cdbs eliminated by fixup_ioaccel_cdb */
 	case WRITE_12:
 	case READ_12:
-		if (map->volume_blk_size == 512) {
-			cp->tweak_lower =
+		if (map->volume_blk_size == 512 ) {
+			cp->tweak_lower = 
 				(((u32) cmd->cmnd[2]) << 24) |
 				(((u32) cmd->cmnd[3]) << 16) |
 				(((u32) cmd->cmnd[4]) << 8) |
 					cmd->cmnd[5];
-			cp->tweak_upper = 0;
-		} else {
+			cp->tweak_upper = 0; 
+		}
+		else {
 			first_block =
 				(((u64) cmd->cmnd[2]) << 24) |
 				(((u64) cmd->cmnd[3]) << 16) |
@@ -3472,18 +4035,19 @@ static void set_encrypt_ioaccel2(struct ctlr_info *h,
 		break;
 	case WRITE_16:
 	case READ_16:
-		if (map->volume_blk_size == 512) {
-			cp->tweak_lower =
+		if (map->volume_blk_size == 512 ) {
+			cp->tweak_lower = 
 				(((u32) cmd->cmnd[6]) << 24) |
 				(((u32) cmd->cmnd[7]) << 16) |
 				(((u32) cmd->cmnd[8]) << 8) |
 					cmd->cmnd[9];
-			cp->tweak_upper =
+			cp->tweak_upper = 
 				(((u32) cmd->cmnd[2]) << 24) |
 				(((u32) cmd->cmnd[3]) << 16) |
 				(((u32) cmd->cmnd[4]) << 8) |
 					cmd->cmnd[5];
-		} else {
+		}
+		else {
 			first_block =
 				(((u64) cmd->cmnd[2]) << 56) |
 				(((u64) cmd->cmnd[3]) << 48) |
@@ -3499,14 +4063,14 @@ static void set_encrypt_ioaccel2(struct ctlr_info *h,
 		}
 		break;
 	default:
-		dev_err(&h->pdev->dev,
-			"ERROR: %s: IOACCEL request CDB size not supported for encryption\n",
-			__func__);
-		BUG();
+		dev_err(&h->pdev->dev, "ERROR: %s: IOACCEL request "
+			"CDB size not supported for encryption\n", __func__);
+		BUG();	
 		break;
 	}
 }
 
+
 static int hpsa_scsi_ioaccel2_queue_command(struct ctlr_info *h,
 	struct CommandList *c, u32 ioaccel_handle, u8 *cdb, int cdb_len,
 	u8 *scsi3addr)
@@ -3520,8 +4084,7 @@ static int hpsa_scsi_ioaccel2_queue_command(struct ctlr_info *h,
 	u32 len;
 	u32 total_len = 0;
 
-	if (scsi_sg_count(cmd) > h->ioaccel_maxsg)
-		return IO_ACCEL_INELIGIBLE;
+	BUG_ON(scsi_sg_count(cmd) > h->maxsgentries);
 
 	if (fixup_ioaccel_cdb(cdb, &cdb_len))
 		return IO_ACCEL_INELIGIBLE;
@@ -3539,8 +4102,19 @@ static int hpsa_scsi_ioaccel2_queue_command(struct ctlr_info *h,
 		return use_sg;
 
 	if (use_sg) {
-		BUG_ON(use_sg > IOACCEL2_MAXSGENTRIES);
 		curr_sg = cp->sg;
+		if (use_sg > h->ioaccel_maxsg) {
+			addr64 = h->ioaccel2_cmd_sg_list[c->cmdindex]->address;
+			curr_sg->address = cpu_to_le64(addr64);
+			curr_sg->length = 0;
+			curr_sg->reserved[0] = 0;
+			curr_sg->reserved[1] = 0;
+			curr_sg->reserved[2] = 0;
+			curr_sg->chain_indicator = 0x80;
+
+			curr_sg = h->ioaccel2_cmd_sg_list[c->cmdindex];
+		}
+			
 		scsi_for_each_sg(cmd, sg, use_sg, i) {
 			addr64 = (u64) sg_dma_address(sg);
 			len  = sg_dma_len(sg);
@@ -3556,15 +4130,15 @@ static int hpsa_scsi_ioaccel2_queue_command(struct ctlr_info *h,
 
 		switch (cmd->sc_data_direction) {
 		case DMA_TO_DEVICE:
-			cp->direction &= ~IOACCEL2_DIRECTION_MASK;
+			cp->direction &= ~IOACCEL2_DIRECTION_MASK; 
 			cp->direction |= IOACCEL2_DIR_DATA_OUT;
 			break;
 		case DMA_FROM_DEVICE:
-			cp->direction &= ~IOACCEL2_DIRECTION_MASK;
+			cp->direction &= ~IOACCEL2_DIRECTION_MASK; 
 			cp->direction |= IOACCEL2_DIR_DATA_IN;
 			break;
 		case DMA_NONE:
-			cp->direction &= ~IOACCEL2_DIRECTION_MASK;
+			cp->direction &= ~IOACCEL2_DIRECTION_MASK; 
 			cp->direction |= IOACCEL2_DIR_NO_DATA;
 			break;
 		default:
@@ -3574,26 +4148,41 @@ static int hpsa_scsi_ioaccel2_queue_command(struct ctlr_info *h,
 			break;
 		}
 	} else {
-		cp->direction &= ~IOACCEL2_DIRECTION_MASK;
+		cp->direction &= ~IOACCEL2_DIRECTION_MASK; 
 		cp->direction |= IOACCEL2_DIR_NO_DATA;
 	}
 
 	/* Set encryption parameters, if necessary */
 	set_encrypt_ioaccel2(h, c, cp);
-
-	cp->scsi_nexus = ioaccel_handle;
-	cp->Tag = (c->cmdindex << DIRECT_LOOKUP_SHIFT) |
-				DIRECT_LOOKUP_BIT;
+		
+	cp->scsi_nexus = ioaccel_handle; /* FIXME: is this correct? */
+	cp->Tag = c->cmdindex << DIRECT_LOOKUP_SHIFT;
 	memcpy(cp->cdb, cdb, sizeof(cp->cdb));
+	memset(cp->cciss_lun, 0, sizeof(cp->cciss_lun)); /* FIXME: cciss_lun doesn't make sense here */
+	cp->cmd_priority_task_attr = 0;
 
-	/* fill in sg elements */
-	cp->sg_count = (u8) use_sg;
 
 	cp->data_len = cpu_to_le32(total_len);
-	cp->err_ptr = cpu_to_le64(c->busaddr +
-			offsetof(struct io_accel2_cmd, error_data));
+	cp->err_ptr = cpu_to_le64(c->busaddr + offsetof(struct io_accel2_cmd, error_data));
 	cp->err_len = cpu_to_le32((u32) sizeof(cp->error_data));
 
+	/* fill in sg elements */
+	if (use_sg > h->ioaccel_maxsg) {
+		cp->sg_count = 1;
+		if (hpsa_map_ioaccel2_sg_chain_block(h, cp, c)) {
+			scsi_dma_unmap(cmd);
+			return -1;
+		}
+	} else {
+		cp->sg_count = (u8) use_sg;
+	}
+	if (unlikely(lockup_detected(h))) {
+		cmd->result = DID_NO_CONNECT << 16;
+		cmd_free(h, c);
+		cmd->scsi_done(cmd);
+		return 0;
+	}
+
 	enqueue_cmd_and_start_io(h, c);
 	return 0;
 }
@@ -3613,31 +4202,6 @@ static int hpsa_scsi_ioaccel_queue_command(struct ctlr_info *h,
 						cdb, cdb_len, scsi3addr);
 }
 
-static void raid_map_helper(struct raid_map_data *map,
-		int offload_to_mirror, u32 *map_index, u32 *current_group)
-{
-	if (offload_to_mirror == 0)  {
-		/* use physical disk in the first mirrored group. */
-		*map_index %= map->data_disks_per_row;
-		return;
-	}
-	do {
-		/* determine mirror group that *map_index indicates */
-		*current_group = *map_index / map->data_disks_per_row;
-		if (offload_to_mirror == *current_group)
-			continue;
-		if (*current_group < (map->layout_map_count - 1)) {
-			/* select map index from next group */
-			*map_index += map->data_disks_per_row;
-			(*current_group)++;
-		} else {
-			/* select map index from first group */
-			*map_index %= map->data_disks_per_row;
-			*current_group = 0;
-		}
-	} while (offload_to_mirror != *current_group);
-}
-
 /*
  * Attempt to perform offload RAID mapping for a logical volume I/O.
  */
@@ -3776,75 +4340,94 @@ static int hpsa_scsi_ioaccel_raid_map(struct ctlr_info *h,
 		return IO_ACCEL_INELIGIBLE;
 
 	/* proceeding with driver mapping */
-	total_disks_per_row = map->data_disks_per_row +
-				map->metadata_disks_per_row;
-	map_row = ((u32)(first_row >> map->parity_rotation_shift)) %
-				map->row_cnt;
-	map_index = (map_row * total_disks_per_row) + first_column;
-
-	switch (dev->raid_level) {
-	case HPSA_RAID_0:
-		break; /* nothing special to do */
-	case HPSA_RAID_1:
+	total_disks_per_row = map->data_disks_per_row + map->metadata_disks_per_row;
+	map_row = ((u32)(first_row >> map->parity_rotation_shift)) % map->row_cnt;
+	map_index = (map_row * total_disks_per_row ) + first_column;
+
+
+	/* RAID 1 */ 
+	if (dev->raid_level == HPSA_RAID_1) {
 		/* Handles load balance across RAID 1 members.
 		 * (2-drive R1 and R10 with even # of drives.)
-		 * Appropriate for SSDs, not optimal for HDDs
+		 * Appropriate for SSDs, not optimal for HDDs 
 		 */
-		BUG_ON(map->layout_map_count != 2);
+		BUG_ON(map->layout_map_count != 2 );
 		if (dev->offload_to_mirror)
 			map_index += map->data_disks_per_row;
 		dev->offload_to_mirror = !dev->offload_to_mirror;
-		break;
-	case HPSA_RAID_ADM:
+	}
+	/* RAID ADM */
+        else if (dev->raid_level == HPSA_RAID_ADM) {
 		/* Handles N-way mirrors  (R1-ADM)
 		 * and R10 with # of drives divisible by 3.)
 		 */
-		BUG_ON(map->layout_map_count != 3);
-
+		BUG_ON(map->layout_map_count != 3 );
+		
 		offload_to_mirror = dev->offload_to_mirror;
-		raid_map_helper(map, offload_to_mirror,
-				&map_index, &current_group);
-		/* set mirror group to use next time */
-		offload_to_mirror =
-			(offload_to_mirror >= map->layout_map_count - 1)
-			? 0 : offload_to_mirror + 1;
-		/* FIXME: remove after debug/dev */
-		BUG_ON(offload_to_mirror >= map->layout_map_count);
-		dev_warn(&h->pdev->dev,
-			"DEBUG: Using physical disk map index %d from mirror group %d\n",
-			map_index, offload_to_mirror);
-		dev->offload_to_mirror = offload_to_mirror;
-		/* Avoid direct use of dev->offload_to_mirror within this
-		 * function since multiple threads might simultaneously
-		 * increment it beyond the range of dev->layout_map_count -1.
-		 */
-		break;
-	case HPSA_RAID_5:
-	case HPSA_RAID_6:
-		if (map->layout_map_count <= 1)
-			break;
+		if (offload_to_mirror == 0)  {
+			/* use physical disk in the first mirrored group. */
+			map_index %= map->data_disks_per_row;
+		} else {
+			
+			do {
+				/* determine mirror group that map_index indicates */
+				current_group = map_index / map->data_disks_per_row;
+	
+				if (offload_to_mirror != current_group ) {
+	
+					if (current_group < (map->layout_map_count - 1)) {
+						/* select map index from next group */
+						map_index += map->data_disks_per_row;
+						current_group++;	
+					}
+					else {
+						/* select map index from first group */
+						map_index %= map->data_disks_per_row;
+						current_group = 0;
+					}
+					
+				}
+			} while (offload_to_mirror != current_group);
 
-		/* Verify first and last block are in same RAID group */
-		r5or6_blocks_per_row =
-			map->strip_size * map->data_disks_per_row;
+		}
+
+                /* set mirror group to use next time */
+                offload_to_mirror = (offload_to_mirror >= map->layout_map_count-1) 
+                        ? 0 : offload_to_mirror+1;
+                BUG_ON(offload_to_mirror >= map->layout_map_count); /* FIXME: remove after debug/dev */
+		//dev_warn(&h->pdev->dev, "DEBUG: Using physical disk map index %d "
+		//	"from mirror group %d\n", map_index, offload_to_mirror);
+                dev->offload_to_mirror = offload_to_mirror;
+                /* Avoid direct use of dev->offload_to_mirror within this function since multiple threads
+                 * might simultaneously increment it beyond the range of dev->layout_map_count -1.
+                 */
+        }
+
+	/* RAID 50/60 */
+        else if ((dev->raid_level == HPSA_RAID_5 || 
+		dev->raid_level == HPSA_RAID_6) &&
+		map->layout_map_count > 1) {
+
+		/* Verify first and last block are in same RAID group */
+		r5or6_blocks_per_row = map->strip_size * map->data_disks_per_row;
 		BUG_ON(r5or6_blocks_per_row == 0);
 		stripesize = r5or6_blocks_per_row * map->layout_map_count;
 #if BITS_PER_LONG == 32
 		tmpdiv = first_block;
 		first_group = do_div(tmpdiv, stripesize);
 		tmpdiv = first_group;
-		(void) do_div(tmpdiv, r5or6_blocks_per_row);
+		(void) do_div(tmpdiv, r5or6_blocks_per_row);					
 		first_group = tmpdiv;
 		tmpdiv = last_block;
 		last_group = do_div(tmpdiv, stripesize);
 		tmpdiv = last_group;
-		(void) do_div(tmpdiv, r5or6_blocks_per_row);
+		(void) do_div(tmpdiv, r5or6_blocks_per_row);					
 		last_group = tmpdiv;
 #else
 		first_group = (first_block % stripesize) / r5or6_blocks_per_row;
 		last_group = (last_block % stripesize) / r5or6_blocks_per_row;
 #endif
-		if (first_group != last_group)
+		if (first_group != last_group )  
 			return IO_ACCEL_INELIGIBLE;
 
 		/* Verify request is in a single row of RAID 5/6 */
@@ -3855,60 +4438,55 @@ static int hpsa_scsi_ioaccel_raid_map(struct ctlr_info *h,
 		tmpdiv = last_block;
 		(void) do_div(tmpdiv, stripesize);
 		r5or6_last_row = r0_last_row = tmpdiv;
-#else
-		first_row = r5or6_first_row = r0_first_row =
-						first_block / stripesize;
+#else 
+		first_row = r5or6_first_row = r0_first_row = first_block / stripesize;
 		r5or6_last_row = r0_last_row = last_block / stripesize;
 #endif
-		if (r5or6_first_row != r5or6_last_row)
+		if (r5or6_first_row != r5or6_last_row )
 			return IO_ACCEL_INELIGIBLE;
-
-
+		
+	
 		/* Verify request is in a single column */
 #if BITS_PER_LONG == 32
 		tmpdiv = first_block;
-		first_row_offset = do_div(tmpdiv, stripesize);
+		first_row_offset = do_div(tmpdiv, stripesize); 
 		tmpdiv = first_row_offset;
-		first_row_offset = (u32) do_div(tmpdiv, r5or6_blocks_per_row);
+		first_row_offset = (u32) do_div(tmpdiv, r5or6_blocks_per_row); 
 		r5or6_first_row_offset = first_row_offset;
 		tmpdiv = last_block;
-		r5or6_last_row_offset = do_div(tmpdiv, stripesize);
+		r5or6_last_row_offset = do_div(tmpdiv, stripesize); 
 		tmpdiv = r5or6_last_row_offset;
 		r5or6_last_row_offset = do_div(tmpdiv, r5or6_blocks_per_row);
 		tmpdiv = r5or6_first_row_offset;
 		(void) do_div(tmpdiv, map->strip_size);
-		first_column = r5or6_first_column = tmpdiv;
+		first_column = r5or6_first_column = tmpdiv; 
 		tmpdiv = r5or6_last_row_offset;
 		(void) do_div(tmpdiv, map->strip_size);
-		r5or6_last_column = tmpdiv;
+		r5or6_last_column = tmpdiv; 
 #else
-		first_row_offset = r5or6_first_row_offset =
-			(u32)((first_block % stripesize) %
-						r5or6_blocks_per_row);
+		first_row_offset = r5or6_first_row_offset = 
+			(u32)((first_block % stripesize ) % r5or6_blocks_per_row);	
 
-		r5or6_last_row_offset =
-			(u32)((last_block % stripesize) %
-						r5or6_blocks_per_row);
+		r5or6_last_row_offset = 
+			(u32)((last_block % stripesize ) % r5or6_blocks_per_row);	
 
-		first_column = r5or6_first_column =
-			r5or6_first_row_offset / map->strip_size;
-		r5or6_last_column =
+		first_column = r5or6_first_column = 
+			r5or6_first_row_offset / map->strip_size; 
+		r5or6_last_column = 
 			r5or6_last_row_offset / map->strip_size;
 #endif
-		if (r5or6_first_column != r5or6_last_column)
+		if (r5or6_first_column != r5or6_last_column ) 
 			return IO_ACCEL_INELIGIBLE;
 
 		/* Request is eligible */
 		map_row = ((u32)(first_row >> map->parity_rotation_shift)) %
 			map->row_cnt;
-
-		map_index = (first_group *
-			(map->row_cnt * total_disks_per_row)) +
-			(map_row * total_disks_per_row) + first_column;
-		break;
-	default:
-		return IO_ACCEL_INELIGIBLE;
-	}
+			
+		map_index = (first_group * 
+			(map->row_cnt * total_disks_per_row)) + 
+			(map_row * total_disks_per_row) + first_column; 
+		
+	} /* end RAID 50/60 */
 
 	disk_handle = dd[map_index].ioaccel_handle;
 	disk_block = map->disk_starting_blk + (first_row * map->strip_size) +
@@ -3958,75 +4536,17 @@ static int hpsa_scsi_ioaccel_raid_map(struct ctlr_info *h,
 						dev->scsi3addr);
 }
 
-static int hpsa_scsi_queue_command_lck(struct scsi_cmnd *cmd,
-	void (*done)(struct scsi_cmnd *))
+/* Submit commands down the "normal" RAID stack path */
+static int hpsa_ciss_submit(struct ctlr_info *h,
+	struct CommandList *c, struct scsi_cmnd *cmd,
+	unsigned char scsi3addr[])
 {
-	struct ctlr_info *h;
-	struct hpsa_scsi_dev_t *dev;
-	unsigned char scsi3addr[8];
-	struct CommandList *c;
-	int rc = 0;
-
-	/* Get the ptr to our adapter structure out of cmd->host. */
-	h = sdev_to_hba(cmd->device);
-	dev = cmd->device->hostdata;
-	if (!dev) {
-		cmd->result = DID_NO_CONNECT << 16;
-		done(cmd);
-		return 0;
-	}
-	memcpy(scsi3addr, dev->scsi3addr, sizeof(scsi3addr));
-
-	if (unlikely(lockup_detected(h))) {
-		cmd->result = DID_ERROR << 16;
-		done(cmd);
-		return 0;
-	}
-	c = cmd_alloc(h);
-	if (c == NULL) {			/* trouble... */
-		dev_err(&h->pdev->dev, "cmd_alloc returned NULL!\n");
-		return SCSI_MLQUEUE_HOST_BUSY;
-	}
-
-	/* Fill in the command list header */
-
-	cmd->scsi_done = done;    /* save this for use by completion code */
-
-	/* save c in case we have to abort it  */
 	cmd->host_scribble = (unsigned char *) c;
-
 	c->cmd_type = CMD_SCSI;
 	c->scsi_cmd = cmd;
-
-	/* Call alternate submit routine for I/O accelerated commands.
-	 * Retries always go down the normal I/O path.
-	 */
-	if (likely(cmd->retries == 0 &&
-		cmd->request->cmd_type == REQ_TYPE_FS &&
-		h->acciopath_status)) {
-		if (dev->offload_enabled) {
-			rc = hpsa_scsi_ioaccel_raid_map(h, c);
-			if (rc == 0)
-				return 0; /* Sent on ioaccel path */
-			if (rc < 0) {   /* scsi_dma_map failed. */
-				cmd_free(h, c);
-				return SCSI_MLQUEUE_HOST_BUSY;
-			}
-		} else if (dev->ioaccel_handle) {
-			rc = hpsa_scsi_ioaccel_direct_map(h, c);
-			if (rc == 0)
-				return 0; /* Sent on direct map path */
-			if (rc < 0) {   /* scsi_dma_map failed. */
-				cmd_free(h, c);
-				return SCSI_MLQUEUE_HOST_BUSY;
-			}
-		}
-	}
-
 	c->Header.ReplyQueue = 0;  /* unused in simple mode */
 	memcpy(&c->Header.LUN.LunAddrBytes[0], &scsi3addr[0], 8);
-	c->Header.Tag.lower = (c->cmdindex << DIRECT_LOOKUP_SHIFT);
-	c->Header.Tag.lower |= DIRECT_LOOKUP_BIT;
+	c->Header.tag = cpu_to_le64((u64) c->cmdindex << DIRECT_LOOKUP_SHIFT);
 
 	/* Fill in the request block... */
 
@@ -4035,17 +4555,18 @@ static int hpsa_scsi_queue_command_lck(struct scsi_cmnd *cmd,
 	BUG_ON(cmd->cmd_len > sizeof(c->Request.CDB));
 	c->Request.CDBLen = cmd->cmd_len;
 	memcpy(c->Request.CDB, cmd->cmnd, cmd->cmd_len);
-	c->Request.Type.Type = TYPE_CMD;
-	c->Request.Type.Attribute = ATTR_SIMPLE;
 	switch (cmd->sc_data_direction) {
 	case DMA_TO_DEVICE:
-		c->Request.Type.Direction = XFER_WRITE;
+		c->Request.type_attr_dir =
+			TYPE_ATTR_DIR(TYPE_CMD, ATTR_SIMPLE, XFER_WRITE);
 		break;
 	case DMA_FROM_DEVICE:
-		c->Request.Type.Direction = XFER_READ;
+		c->Request.type_attr_dir =
+			TYPE_ATTR_DIR(TYPE_CMD, ATTR_SIMPLE, XFER_READ);
 		break;
 	case DMA_NONE:
-		c->Request.Type.Direction = XFER_NONE;
+		c->Request.type_attr_dir =
+			TYPE_ATTR_DIR(TYPE_CMD, ATTR_SIMPLE, XFER_NONE);
 		break;
 	case DMA_BIDIRECTIONAL:
 		/* This can happen if a buggy application does a scsi passthru
@@ -4053,7 +4574,8 @@ static int hpsa_scsi_queue_command_lck(struct scsi_cmnd *cmd,
 		 * ../scsi/scsi_ioctl.c:scsi_ioctl_send_command() )
 		 */
 
-		c->Request.Type.Direction = XFER_RSVD;
+		c->Request.type_attr_dir =
+			TYPE_ATTR_DIR(TYPE_CMD, ATTR_SIMPLE, XFER_RSVD);
 		/* This is technically wrong, and hpsa controllers should
 		 * reject it with CMD_INVALID, which is the most correct
 		 * response, but non-fibre backends appear to let it
@@ -4080,7 +4602,166 @@ static int hpsa_scsi_queue_command_lck(struct scsi_cmnd *cmd,
 	return 0;
 }
 
-static DEF_SCSI_QCMD(hpsa_scsi_queue_command)
+static inline void hpsa_cmd_init(struct ctlr_info *h, int index,
+				struct CommandList *c)
+{
+	dma_addr_t cmd_dma_handle, err_dma_handle;
+	union u64bit temp64;
+
+	/* Zero out all of commandlist except the last field, refcount */
+	memset(c, 0, offsetof(struct CommandList, refcount));
+	c->Header.tag = cpu_to_le64((u64) (index << DIRECT_LOOKUP_SHIFT));
+	cmd_dma_handle = h->cmd_pool_dhandle + index * sizeof(*c);
+	c->err_info = h->errinfo_pool + index;
+	memset(c->err_info, 0, sizeof(*c->err_info));
+	err_dma_handle = h->errinfo_pool_dhandle
+	    + index * sizeof(*c->err_info);
+	c->cmdindex = index;
+	c->busaddr = (u32) cmd_dma_handle;
+	temp64.val = (u64) err_dma_handle;
+	c->ErrDesc.Addr = cpu_to_le64((u64) err_dma_handle);
+	c->ErrDesc.Len = cpu_to_le32((u32) sizeof(*c->err_info));
+	c->h = h;
+}
+
+static int hpsa_ioaccel_submit(struct ctlr_info *h,
+		struct CommandList *c, struct scsi_cmnd *cmd,
+		unsigned char *scsi3addr)
+{
+	struct hpsa_scsi_dev_t *dev = cmd->device->hostdata;
+	int rc = IO_ACCEL_INELIGIBLE;
+
+	cmd->host_scribble = (unsigned char *) c;
+	c->scsi_cmd = cmd;
+	c->cmd_type = CMD_SCSI;
+
+	/* Try to honor the device's queue depth */
+	atomic_inc(&dev->ioaccel_cmds_out);
+	if (atomic_read(&dev->ioaccel_cmds_out) > dev->queue_depth) {
+		atomic_dec(&dev->ioaccel_cmds_out);
+		return IO_ACCEL_INELIGIBLE;
+	}
+
+	if (dev->offload_enabled &&
+		cmd->request->cmd_type == REQ_TYPE_FS) {
+		rc = hpsa_scsi_ioaccel_raid_map(h, c);
+		if (rc == 0)
+			return 0; /* Sent on ioaccel path */
+		if (rc < 0) {   /* scsi_dma_map failed. */
+			cmd_free(h, c);
+			atomic_dec(&dev->ioaccel_cmds_out);
+			return SCSI_MLQUEUE_HOST_BUSY;
+		}
+	} else if (dev->hba_ioaccel_enabled) {
+		rc = hpsa_scsi_ioaccel_direct_map(h, c);
+		if (rc == 0)
+			return 0; /* Sent on direct map path */
+		if (rc < 0) {   /* scsi_dma_map failed. */
+			cmd_free(h, c);
+			atomic_dec(&dev->ioaccel_cmds_out);
+			return SCSI_MLQUEUE_HOST_BUSY;
+		}
+	}
+	atomic_dec(&dev->ioaccel_cmds_out);
+	return rc;
+}
+
+static void hpsa_command_resubmit_worker(struct work_struct *work)
+{
+	struct scsi_cmnd *cmd;
+	struct hpsa_scsi_dev_t *dev;
+	struct CommandList *c =
+			container_of(work, struct CommandList, work);
+
+	cmd = c->scsi_cmd;
+	dev = cmd->device->hostdata;
+	if (!dev) {
+		cmd->result = DID_NO_CONNECT << 16;
+		cmd->scsi_done(cmd);
+		return;
+	}
+	if (c->cmd_type == CMD_IOACCEL2) {
+		struct ctlr_info *h = c->h;
+		struct io_accel2_cmd *c2 = &h->ioaccel2_cmd_pool[c->cmdindex];
+		int rc;
+
+		if (c2->error_data.serv_response ==
+				IOACCEL2_STATUS_SR_TASK_COMP_SET_FULL) {
+			rc = hpsa_ioaccel_submit(h, c, cmd, dev->scsi3addr);
+			if (rc == 0)
+				return;
+			if (rc < 0) {
+				/*
+				 * If we get here, it means dma mapping failed.
+				 * Try again via scsi mid layer, which will
+				 * then get SCSI_MLQUEUE_HOST_BUSY.
+				 */
+				cmd->result = DID_IMM_RETRY << 16;
+				cmd->scsi_done(cmd);
+			}
+			/* if rc > 0, fall thru and resubmit down CISS path */
+		}
+	}
+	hpsa_cmd_init(c->h, c->cmdindex, c);
+	if (hpsa_ciss_submit(c->h, c, cmd, dev->scsi3addr)) {
+		/*
+		 * If we get here, it means dma mapping failed. Try
+		 * again via scsi mid layer, which will then get
+		 * SCSI_MLQUEUE_HOST_BUSY.
+		 */
+		cmd->result = DID_IMM_RETRY << 16;
+		cmd->scsi_done(cmd);
+	}
+}
+
+DECLARE_QUEUECOMMAND(hpsa_scsi_queue_command)
+{
+	struct ctlr_info *h;
+	struct hpsa_scsi_dev_t *dev;
+	unsigned char scsi3addr[8];
+	struct CommandList *c;
+	int rc = 0;
+
+	/* Get the ptr to our adapter structure out of cmd->host. */
+	h = sdev_to_hba(cmd->device);
+	dev = cmd->device->hostdata;
+	if (!dev) {
+		cmd->result = DID_NO_CONNECT << 16;
+		hpsa_scsi_done(cmd, done);
+		return 0;
+	}
+
+	if (bail_on_report_luns_if_no_scan_start(cmd, done))
+		return 0;
+
+	memcpy(scsi3addr, dev->scsi3addr, sizeof(scsi3addr));
+
+	if (unlikely(lockup_detected(h))) {
+		cmd->result = DID_NO_CONNECT << 16;
+		hpsa_scsi_done(cmd, done);
+		return 0;
+	}
+	c = cmd_alloc(h);
+	if (c == NULL) {			/* trouble... */
+		dev_err(&h->pdev->dev, "cmd_alloc returned NULL!\n");
+		return SCSI_MLQUEUE_HOST_BUSY;
+	}
+
+	/* Fill in the command list header */
+	hpsa_save_scsi_done(cmd, done);
+
+	/* Call alternate submit routine for I/O accelerated commands.
+ 	 * Retries always go down the normal I/O path.
+	 */
+	if (likely(h->acciopath_status) && cmd->retries == 0) {
+		rc = hpsa_ioaccel_submit(h, c, cmd, scsi3addr);
+		if (rc == 0)
+			return 0;
+		if (rc < 0)
+			return SCSI_MLQUEUE_HOST_BUSY;
+	}
+	return hpsa_ciss_submit(h, c, cmd, scsi3addr);
+}
 
 static int do_not_scan_if_controller_locked_up(struct ctlr_info *h)
 {
@@ -4151,13 +4832,11 @@ static int hpsa_scan_finished(struct Scsi_Host *sh,
 	return finished;
 }
 
-static int hpsa_change_queue_depth(struct scsi_device *sdev,
-	int qdepth, int reason)
+DECLARE_CHANGE_QUEUE_DEPTH(hpsa_change_queue_depth)
 {
 	struct ctlr_info *h = sdev_to_hba(sdev);
 
-	if (reason != SCSI_QDEPTH_DEFAULT)
-		return -ENOTSUPP;
+	BAIL_ON_BAD_REASON;
 
 	if (qdepth < 1)
 		qdepth = 1;
@@ -4181,6 +4860,9 @@ static int hpsa_register_scsi(struct ctlr_info *h)
 	struct Scsi_Host *sh;
 	int error;
 
+	/* This is a no-op on kernels with working ->scan_start() */
+	hpsa_initial_update_scsi_devices(h);
+
 	sh = scsi_host_alloc(&hpsa_driver_template, sizeof(h));
 	if (sh == NULL)
 		goto fail;
@@ -4192,11 +4874,11 @@ static int hpsa_register_scsi(struct ctlr_info *h)
 	sh->max_cmd_len = MAX_COMMAND_SIZE;
 	sh->max_lun = HPSA_MAX_LUN;
 	sh->max_id = HPSA_MAX_LUN;
-	sh->can_queue = h->nr_cmds;
-	if (h->hba_mode_enabled)
-		sh->cmd_per_lun = 7;
-	else
-		sh->cmd_per_lun = h->nr_cmds;
+	sh->can_queue = h->nr_cmds -
+			HPSA_CMDS_RESERVED_FOR_ABORTS -
+			HPSA_CMDS_RESERVED_FOR_DRIVER -
+			HPSA_MAX_CONCURRENT_PASSTHRUS;
+	sh->cmd_per_lun = sh->can_queue;
 	sh->sg_tablesize = h->maxsgentries;
 	h->scsi_host = sh;
 	sh->hostdata[0] = (unsigned long) h;
@@ -4227,7 +4909,7 @@ static int wait_for_device_to_become_ready(struct ctlr_info *h,
 	int waittime = 1; /* seconds */
 	struct CommandList *c;
 
-	c = cmd_special_alloc(h);
+	c = cmd_alloc(h);
 	if (!c) {
 		dev_warn(&h->pdev->dev, "out of memory in "
 			"wait_for_device_to_become_ready.\n");
@@ -4242,7 +4924,7 @@ static int wait_for_device_to_become_ready(struct ctlr_info *h,
 		 */
 		msleep(1000 * waittime);
 		count++;
-		rc = 0; /* Device ready. */
+		rc = 0; /* device ready. */
 
 		/* Increase wait time with each try, up to a point. */
 		if (waittime < HPSA_MAX_WAIT_INTERVAL_SECS)
@@ -4251,7 +4933,9 @@ static int wait_for_device_to_become_ready(struct ctlr_info *h,
 		/* Send the Test Unit Ready, fill_cmd can't fail, no mapping */
 		(void) fill_cmd(c, TEST_UNIT_READY, h,
 				NULL, 0, 0, lunaddr, TYPE_CMD);
-		hpsa_scsi_do_simple_cmd_core(h, c);
+		rc = hpsa_scsi_do_simple_cmd_core(h, c, NO_TIMEOUT);
+		if (rc)
+			goto do_it_again;
 		/* no unmap needed here because no data xfer. */
 
 		if (c->err_info->CommandStatus == CMD_SUCCESS)
@@ -4262,7 +4946,7 @@ static int wait_for_device_to_become_ready(struct ctlr_info *h,
 			(c->err_info->SenseInfo[2] == NO_SENSE ||
 			c->err_info->SenseInfo[2] == UNIT_ATTENTION))
 			break;
-
+do_it_again:
 		dev_warn(&h->pdev->dev, "waiting %d secs "
 			"for device to become ready.\n", waittime);
 		rc = 1; /* device not ready. */
@@ -4273,13 +4957,10 @@ static int wait_for_device_to_become_ready(struct ctlr_info *h,
 	else
 		dev_warn(&h->pdev->dev, "device is ready.\n");
 
-	cmd_special_free(h, c);
+	cmd_free(h, c);
 	return rc;
 }
 
-/* Need at least one of these error handlers to keep ../scsi/hosts.c from
- * complaining.  Doing a host- or bus-reset can't do anything good here.
- */
 static int hpsa_eh_device_reset_handler(struct scsi_cmnd *scsicmd)
 {
 	int rc;
@@ -4290,6 +4971,10 @@ static int hpsa_eh_device_reset_handler(struct scsi_cmnd *scsicmd)
 	h = sdev_to_hba(scsicmd->device);
 	if (h == NULL) /* paranoia */
 		return FAILED;
+
+	if (lockup_detected(h))
+		return FAILED;
+
 	dev = scsicmd->device->hostdata;
 	if (!dev) {
 		dev_err(&h->pdev->dev, "hpsa_eh_device_reset_handler: "
@@ -4299,7 +4984,7 @@ static int hpsa_eh_device_reset_handler(struct scsi_cmnd *scsicmd)
 	dev_warn(&h->pdev->dev, "resetting device %d:%d:%d:%d\n",
 		h->scsi_host->host_no, dev->bus, dev->target, dev->lun);
 	/* send a reset to the SCSI LUN which the command was sent to */
-	rc = hpsa_send_reset(h, dev->scsi3addr, HPSA_RESET_TYPE_LUN);
+	rc = hpsa_send_reset(h, dev->scsi3addr, HPSA_DEVICE_RESET_MSG);
 	if (rc == 0 && wait_for_device_to_become_ready(h, dev->scsi3addr) == 0)
 		return SUCCESS;
 
@@ -4325,48 +5010,48 @@ static void swizzle_abort_tag(u8 *tag)
 static void hpsa_get_tag(struct ctlr_info *h,
 	struct CommandList *c, u32 *taglower, u32 *tagupper)
 {
+
 	if (c->cmd_type == CMD_IOACCEL1) {
 		struct io_accel1_cmd *cm1 = (struct io_accel1_cmd *)
 			&h->ioaccel_cmd_pool[c->cmdindex];
-		*tagupper = cm1->Tag.upper;
-		*taglower = cm1->Tag.lower;
+		*tagupper = (u32) (cm1->tag >> 32);
+		*taglower = (u32) (cm1->tag & 0x0ffffffffULL);
 		return;
 	}
 	if (c->cmd_type == CMD_IOACCEL2) {
 		struct io_accel2_cmd *cm2 = (struct io_accel2_cmd *)
 			&h->ioaccel2_cmd_pool[c->cmdindex];
-		/* upper tag not used in ioaccel2 mode */
-		memset(tagupper, 0, sizeof(*tagupper));
+		memset(tagupper, 0, sizeof(*tagupper)); /* upper tag isn't used in ioaccel2 mode */	
 		*taglower = cm2->Tag;
 		return;
 	}
-	*tagupper = c->Header.Tag.upper;
-	*taglower = c->Header.Tag.lower;
+	*tagupper = (u32) (c->Header.tag >> 32);
+	*taglower = (u32) (c->Header.tag & 0x0ffffffffULL);
 }
 
-
 static int hpsa_send_abort(struct ctlr_info *h, unsigned char *scsi3addr,
-	struct CommandList *abort, int swizzle)
+	struct CommandList *abort, int reply_queue)
 {
 	int rc = IO_OK;
 	struct CommandList *c;
 	struct ErrorInfo *ei;
 	u32 tagupper, taglower;
 
-	c = cmd_special_alloc(h);
+	c = cmd_alloc(h);
 	if (c == NULL) {	/* trouble... */
-		dev_warn(&h->pdev->dev, "cmd_special_alloc returned NULL!\n");
+		dev_warn(&h->pdev->dev, "cmd_alloc returned NULL!\n");
 		return -ENOMEM;
 	}
 
 	/* fill_cmd can't fail here, no buffer to map */
-	(void) fill_cmd(c, HPSA_ABORT_MSG, h, abort,
+	(void) fill_cmd(c, HPSA_ABORT_MSG, h, &abort->Header.tag,
 		0, 0, scsi3addr, TYPE_MSG);
-	if (swizzle)
+	if (h->needs_abort_tags_swizzled)
 		swizzle_abort_tag(&c->Request.CDB[4]);
-	hpsa_scsi_do_simple_cmd_core(h, c);
+	(void) __hpsa_scsi_do_simple_cmd_core(h, c, reply_queue, NO_TIMEOUT);
 	hpsa_get_tag(h, abort, &taglower, &tagupper);
-	dev_dbg(&h->pdev->dev, "%s: Tag:0x%08x:%08x: do_simple_cmd_core completed.\n",
+	dev_dbg(&h->pdev->dev, "%s: Tag:0x%08x:%08x: "
+		"do_simple_cmd_core completed.\n",
 		__func__, tagupper, taglower);
 	/* no unmap needed here because no data xfer. */
 
@@ -4374,127 +5059,133 @@ static int hpsa_send_abort(struct ctlr_info *h, unsigned char *scsi3addr,
 	switch (ei->CommandStatus) {
 	case CMD_SUCCESS:
 		break;
+	case CMD_TMF_STATUS:
+		rc = hpsa_evaluate_tmf_status(h, c);
+		break;
 	case CMD_UNABORTABLE: /* Very common, don't make noise. */
 		rc = -1;
 		break;
 	default:
-		dev_dbg(&h->pdev->dev, "%s: Tag:0x%08x:%08x: interpreting error.\n",
+		dev_dbg(&h->pdev->dev, "%s: Tag:0x%08x:%08x: "
+			"interpreting error.\n",
 			__func__, tagupper, taglower);
-		hpsa_scsi_interpret_error(h, c);
+		hpsa_scsi_interpret_error(c);
 		rc = -1;
 		break;
 	}
-	cmd_special_free(h, c);
-	dev_dbg(&h->pdev->dev, "%s: Tag:0x%08x:%08x: Finished.\n",
-		__func__, tagupper, taglower);
+	cmd_free(h, c);
+	dev_dbg(&h->pdev->dev, "%s: Tag:0x%08x:%08x: "
+		"Finished.\n", __func__, tagupper, taglower);
 	return rc;
 }
 
-/*
- * hpsa_find_cmd_in_queue
- *
- * Used to determine whether a command (find) is still present
- * in queue_head.   Optionally excludes the last element of queue_head.
- *
- * This is used to avoid unnecessary aborts.  Commands in h->reqQ have
- * not yet been submitted, and so can be aborted by the driver without
- * sending an abort to the hardware.
- *
- * Returns pointer to command if found in queue, NULL otherwise.
- */
-static struct CommandList *hpsa_find_cmd_in_queue(struct ctlr_info *h,
-			struct scsi_cmnd *find, struct list_head *queue_head)
+#if 0
+static void setup_ioaccel2_abort_cmd(struct CommandList *c, struct ctlr_info *h,
+	unsigned char *scsi3addr, struct CommandList *command_to_abort)
 {
-	unsigned long flags;
-	struct CommandList *c = NULL;	/* ptr into cmpQ */
+	struct io_accel2_cmd *c2 = &h->ioaccel2_cmd_pool[c->cmdindex];
+	struct hpsa_tmf_struct *ac = (struct hpsa_tmf_struct *) c2;
+	struct io_accel2_cmd *c2a =
+		&h->ioaccel2_cmd_pool[command_to_abort->cmdindex];
+	struct scsi_cmnd *scmd =
+		(struct scsi_cmnd *) command_to_abort->scsi_cmd;
+	struct hpsa_scsi_dev_t *dev = scmd->device->hostdata;
 
-	if (!find)
-		return 0;
-	spin_lock_irqsave(&h->lock, flags);
-	list_for_each_entry(c, queue_head, list) {
-		if (c->scsi_cmd == NULL) /* e.g.: passthru ioctl */
-			continue;
-		if (c->scsi_cmd == find) {
-			spin_unlock_irqrestore(&h->lock, flags);
-			return c;
-		}
-	}
-	spin_unlock_irqrestore(&h->lock, flags);
-	return NULL;
-}
+	/*
+	 * We're overlaying struct hpsa_tmf_struct on top of something which
+	 * was allocated as a struct io_accel2_cmd, so we better be sure it
+	 * actually fits, and doesn't overrun the error info space.
+	 */
+	BUILD_BUG_ON(sizeof(struct hpsa_tmf_struct) >
+			sizeof(struct io_accel2_cmd)); 
+	BUG_ON(offsetof(struct io_accel2_cmd, error_data) <
+			offsetof(struct hpsa_tmf_struct, error_len) +
+				sizeof(ac->error_len));
 
-static struct CommandList *hpsa_find_cmd_in_queue_by_tag(struct ctlr_info *h,
-					u8 *tag, struct list_head *queue_head)
-{
-	unsigned long flags;
-	struct CommandList *c;
+	c->cmd_type = CMD_IOACCEL2;
+	/* Adjust the DMA address to point to the accelerated command buffer */
+	c->busaddr = (u32) h->ioaccel2_cmd_pool_dhandle +
+				(c->cmdindex * sizeof(struct io_accel2_cmd));
+	BUG_ON(c->busaddr & 0x0000007F);
 
-	spin_lock_irqsave(&h->lock, flags);
-	list_for_each_entry(c, queue_head, list) {
-		if (memcmp(&c->Header.Tag, tag, 8) != 0)
-			continue;
-		spin_unlock_irqrestore(&h->lock, flags);
-		return c;
-	}
-	spin_unlock_irqrestore(&h->lock, flags);
-	return NULL;
+	memset(ac, 0, sizeof(*c2)); /* yes this is correct */
+	ac->iu_type = IU_TYPE_TMF;
+	ac->reply_queue = smp_processor_id() % h->nreply_queues;
+	ac->tmf = ABORT_TASK; 
+	ac->it_nexus = cpu_to_le32((u32) dev->ioaccel_handle);
+	memcpy(ac->lun_id, c2a->cciss_lun, sizeof(ac->lun_id));
+	ac->Tag.lower = cpu_to_le32((c->cmdindex << DIRECT_LOOKUP_SHIFT) |
+					DIRECT_LOOKUP_BIT);
+	ac->abort_tag = c2a->Tag;
+	ac->error_ptr = cpu_to_le64((u64) c->busaddr +
+			offsetof(struct io_accel2_cmd, error_data));
+	ac->error_len = cpu_to_le32((u32) sizeof(c2->error_data));
+	/* Set the bits in the address sent down to include:
+	 *  - performant mode bit not used in ioaccel mode 2
+	 *  - pull count (bits 0-3)
+	 */
 }
+#endif
 
 /* ioaccel2 path firmware cannot handle abort task requests.
- * Change abort requests to physical target reset, and send to the
+ * Change abort requests to physical target reset, and send to the 
  * address of the physical disk used for the ioaccel 2 command.
  * Return 0 on success (IO_OK)
  *	 -1 on failure
  */
 
 static int hpsa_send_reset_as_abort_ioaccel2(struct ctlr_info *h,
-	unsigned char *scsi3addr, struct CommandList *abort)
+	unsigned char *scsi3addr, struct CommandList *abort,
+	__attribute__((unused)) int reply_queue)
 {
 	int rc = IO_OK;
-	struct scsi_cmnd *scmd; /* scsi command within request being aborted */
+        struct scsi_cmnd *scmd; /* scsi command within request being aborted */
 	struct hpsa_scsi_dev_t *dev; /* device to which scsi cmd was sent */
-	unsigned char phys_scsi3addr[8]; /* addr of phys disk with volume */
+	unsigned char phys_scsi3addr[8]; /* address of physical disk with volume */
 	unsigned char *psa = &phys_scsi3addr[0];
 
-	/* Get a pointer to the hpsa logical device. */
-	scmd = (struct scsi_cmnd *) abort->scsi_cmd;
+        /* Get a pointer to the hpsa logical device. */
+	scmd = abort->scsi_cmd;
 	dev = (struct hpsa_scsi_dev_t *)(scmd->device->hostdata);
-	if (dev == NULL) {
-		dev_warn(&h->pdev->dev,
+	if ( dev == NULL ) {
+		printk(KERN_WARNING 
 			"Cannot abort: no device pointer for command.\n");
 			return -1; /* not abortable */
 	}
-
-	if (h->raid_offload_debug > 0)
+	
+	if (h->raid_offload_debug > 0 )
 		dev_info(&h->pdev->dev,
-			"Reset as abort: Abort requested on C%d:B%d:T%d:L%d scsi3addr 0x%02x%02x%02x%02x%02x%02x%02x%02x\n",
-			h->scsi_host->host_no, dev->bus, dev->target, dev->lun,
+			"Reset as abort: Abort requested on C%d:B%d:T%d:L%d "
+			"scsi3addr 0x%02x%02x%02x%02x%02x%02x%02x%02x\n",
+               		 h->scsi_host->host_no, dev->bus, dev->target, dev->lun,
 			scsi3addr[0], scsi3addr[1], scsi3addr[2], scsi3addr[3],
 			scsi3addr[4], scsi3addr[5], scsi3addr[6], scsi3addr[7]);
 
 	if (!dev->offload_enabled) {
-		dev_warn(&h->pdev->dev,
-			"Can't abort: device is not operating in HP SSD Smart Path mode.\n");
+		dev_warn(&h->pdev->dev, "Can't abort: device is not operating "
+			"in HP SSD Smart Path mode.\n"); 
 		return -1; /* not abortable */
 	}
 
 	/* Incoming scsi3addr is logical addr. We need physical disk addr. */
-	if (!hpsa_get_pdisk_of_ioaccel2(h, abort, psa)) {
-		dev_warn(&h->pdev->dev, "Can't abort: Failed lookup of physical address.\n");
+	if (! hpsa_get_pdisk_of_ioaccel2(h, abort, psa)) {
+		dev_warn(&h->pdev->dev, "Can't abort: Failed lookup of physical address.\n"); 
 		return -1; /* not abortable */
 	}
 
 	/* send the reset */
-	if (h->raid_offload_debug > 0)
+	if (h->raid_offload_debug > 0 )
 		dev_info(&h->pdev->dev,
-			"Reset as abort: Resetting physical device at scsi3addr 0x%02x%02x%02x%02x%02x%02x%02x%02x\n",
-			psa[0], psa[1], psa[2], psa[3],
+			"Reset as abort: Resetting physical device at "
+			"scsi3addr 0x%02x%02x%02x%02x%02x%02x%02x%02x\n",
+			psa[0], psa[1], psa[2], psa[3], 
 			psa[4], psa[5], psa[6], psa[7]);
-	rc = hpsa_send_reset(h, psa, HPSA_RESET_TYPE_TARGET);
+	rc = hpsa_send_reset(h, psa, HPSA_PHYS_TARGET_RESET);
 	if (rc != 0) {
 		dev_warn(&h->pdev->dev,
-			"Reset as abort: Failed on physical device at scsi3addr 0x%02x%02x%02x%02x%02x%02x%02x%02x\n",
-			psa[0], psa[1], psa[2], psa[3],
+			"Reset as abort: Failed on physical device at "
+			"scsi3addr 0x%02x%02x%02x%02x%02x%02x%02x%02x\n",
+			psa[0], psa[1], psa[2], psa[3], 
 			psa[4], psa[5], psa[6], psa[7]);
 		return rc; /* failed to reset */
 	}
@@ -4502,63 +5193,117 @@ static int hpsa_send_reset_as_abort_ioaccel2(struct ctlr_info *h,
 	/* wait for device to recover */
 	if (wait_for_device_to_become_ready(h, psa) != 0) {
 		dev_warn(&h->pdev->dev,
-			"Reset as abort: Failed: Device never recovered from reset: 0x%02x%02x%02x%02x%02x%02x%02x%02x\n",
-			psa[0], psa[1], psa[2], psa[3],
+			"Reset as abort: Failed: Device never recovered "
+			"from reset: 0x%02x%02x%02x%02x%02x%02x%02x%02x\n",
+			psa[0], psa[1], psa[2], psa[3], 
 			psa[4], psa[5], psa[6], psa[7]);
 		return -1;  /* failed to recover */
 	}
 
 	/* device recovered */
 	dev_info(&h->pdev->dev,
-		"Reset as abort: Device recovered from reset: scsi3addr 0x%02x%02x%02x%02x%02x%02x%02x%02x\n",
-		psa[0], psa[1], psa[2], psa[3],
+		"Reset as abort: Device recovered from reset: "
+		"scsi3addr 0x%02x%02x%02x%02x%02x%02x%02x%02x\n",
+		psa[0], psa[1], psa[2], psa[3], 
 		psa[4], psa[5], psa[6], psa[7]);
 
 	return rc; /* success */
 }
 
-/* Some Smart Arrays need the abort tag swizzled, and some don't.  It's hard to
- * tell which kind we're dealing with, so we send the abort both ways.  There
- * shouldn't be any collisions between swizzled and unswizzled tags due to the
- * way we construct our tags but we check anyway in case the assumptions which
- * make this true someday become false.
- */
-static int hpsa_send_abort_both_ways(struct ctlr_info *h,
+#if 0
+static int hpsa_send_abort_ioaccel2(struct ctlr_info *h,
 	unsigned char *scsi3addr, struct CommandList *abort)
 {
-	u8 swizzled_tag[8];
+	int rc = IO_OK;
 	struct CommandList *c;
-	int rc = 0, rc2 = 0;
+	u32 taglower, tagupper;
+	struct io_accel2_scsi_response *ei;
+#if 0
+	struct hpsa_scsi_dev_t *dev;
 
-	/* ioccelerator mode 2 commands should be aborted via the
-	 * accelerated path, since RAID path is unaware of these commands,
+	/*
+	 * FIXME:
+	 * If RAID offload is not enabled on this device, don't try the
+	 * abort instead do ??????
+	 */
+
+	dev = container_of(scsi3addr, struct hpsa_scsi_dev_t, scsi3addr);
+	if (!dev->offload_enabled)
+		return 0;
+#endif
+
+	c = cmd_alloc(h);
+	if (c == NULL) {
+		dev_warn(&h->pdev->dev, "cmd_alloc returned NULL!\n");
+		return -ENOMEM;
+	}
+
+	setup_ioaccel2_abort_cmd(c, h, scsi3addr, abort);
+	hpsa_scsi_do_simple_cmd_core(h, c);
+	hpsa_get_tag(h, abort, &taglower, &tagupper);
+	dev_dbg(&h->pdev->dev, "%s: Tag:0x%08x:%08x: do_simple_cmd_core completed.\n",
+		__func__, tagupper, taglower);
+	/* no unmap needed here because no data xfer. */
+
+	ei = (struct io_accel2_scsi_response *) c->err_info;
+	dev_dbg(&h->pdev->dev, "%s: Tag:0x%08x:%08x: abort service response = 0x%02x.\n",
+		__func__, tagupper, taglower, ei->serv_response);
+	switch (ei->serv_response) {
+	case IOACCEL2_SERV_RESPONSE_TMF_COMPLETE:
+	case IOACCEL2_SERV_RESPONSE_TMF_SUCCESS:
+	case IOACCEL2_SERV_RESPONSE_TMF_REJECTED:
+		rc = 0;
+		break;
+	case IOACCEL2_SERV_RESPONSE_FAILURE:
+	case IOACCEL2_SERV_RESPONSE_TMF_WRONG_LUN:
+		rc = -1;
+		break;
+	default:
+		dev_warn(&h->pdev->dev, "%s: Tag:0x%08x:%08x: unknown abort service response x0%02x\n",
+			__func__, tagupper, taglower, ei->serv_response);
+		rc = -1;
+	}
+	cmd_free(h, c);
+	dev_dbg(&h->pdev->dev, "%s: Tag:0x%08x:%08x: Finished.\n", __func__,
+		tagupper, taglower);
+	return rc;
+	
+}
+#endif 
+
+static int hpsa_send_abort_both_ways(struct ctlr_info *h,
+	unsigned char *scsi3addr, struct CommandList *abort, int reply_queue)
+{
+	/* ioccelerator mode 2 commands should be aborted via the 
+	 * accelerated path, since RAID path is unaware of these commands, 
 	 * but underlying firmware can't handle abort TMF.
 	 * Change abort to physical device reset.
 	 */
-	if (abort->cmd_type == CMD_IOACCEL2)
-		return hpsa_send_reset_as_abort_ioaccel2(h, scsi3addr, abort);
+	if (abort->cmd_type == CMD_IOACCEL2) 
+		return hpsa_send_reset_as_abort_ioaccel2(h, scsi3addr,
+							abort, reply_queue);
+	return hpsa_send_abort(h, scsi3addr, abort, reply_queue);
+}
 
-	/* we do not expect to find the swizzled tag in our queue, but
-	 * check anyway just to be sure the assumptions which make this
-	 * the case haven't become wrong.
-	 */
-	memcpy(swizzled_tag, &abort->Request.CDB[4], 8);
-	swizzle_abort_tag(swizzled_tag);
-	c = hpsa_find_cmd_in_queue_by_tag(h, swizzled_tag, &h->cmpQ);
-	if (c != NULL) {
-		dev_warn(&h->pdev->dev, "Unexpectedly found byte-swapped tag in completion queue.\n");
-		return hpsa_send_abort(h, scsi3addr, abort, 0);
-	}
-	rc = hpsa_send_abort(h, scsi3addr, abort, 0);
-
-	/* if the command is still in our queue, we can't conclude that it was
-	 * aborted (it might have just completed normally) but in any case
-	 * we don't need to try to abort it another way.
-	 */
-	c = hpsa_find_cmd_in_queue(h, abort->scsi_cmd, &h->cmpQ);
-	if (c)
-		rc2 = hpsa_send_abort(h, scsi3addr, abort, 1);
-	return rc && rc2;
+/* Find out which reply queue a command was meant to return on */
+static int hpsa_extract_reply_queue(struct ctlr_info *h,
+					struct CommandList *c)
+{
+	if (c->cmd_type == CMD_IOACCEL2)
+		return h->ioaccel2_cmd_pool[c->cmdindex].reply_queue;
+	return c->Header.ReplyQueue;
+}
+
+/*
+ * Limit concurrency of abort commands to prevent
+ * over-subscription of commands
+ */
+static inline int wait_for_available_abort_cmd(struct ctlr_info *h)
+{
+#define ABORT_CMD_WAIT_MSECS 5000
+	return !wait_event_timeout(h->abort_cmd_wait_queue,
+			atomic_dec_if_positive(&h->abort_cmds_available) >= 0,
+			msecs_to_jiffies(ABORT_CMD_WAIT_MSECS));
 }
 
 /* Send an abort for the specified command.
@@ -4568,20 +5313,25 @@ static int hpsa_send_abort_both_ways(struct ctlr_info *h,
 static int hpsa_eh_abort_handler(struct scsi_cmnd *sc)
 {
 
-	int i, rc;
+	int rc;
 	struct ctlr_info *h;
 	struct hpsa_scsi_dev_t *dev;
 	struct CommandList *abort; /* pointer to command to be aborted */
-	struct CommandList *found;
 	struct scsi_cmnd *as;	/* ptr to scsi cmd inside aborted command. */
 	char msg[256];		/* For debug messaging. */
 	int ml = 0;
 	u32 tagupper, taglower;
+	int refcount, reply_queue;
 
 	/* Find the controller of the command to be aborted */
 	h = sdev_to_hba(sc->device);
-	if (WARN(h == NULL,
-			"ABORT REQUEST FAILED, Controller lookup failed.\n"))
+	if (!h) {
+		printk(KERN_WARNING "hpsa: ABORT REQUEST FAILED, "
+			"controller lookup failed\n");
+		return FAILED;
+	}
+
+	if (lockup_detected(h))
 		return FAILED;
 
 	/* Check that controller supports some kind of task abort */
@@ -4597,21 +5347,42 @@ static int hpsa_eh_abort_handler(struct scsi_cmnd *sc)
 	/* Find the device of the command to be aborted */
 	dev = sc->device->hostdata;
 	if (!dev) {
-		dev_err(&h->pdev->dev, "%s FAILED, Device lookup failed.\n",
-				msg);
+		dev_err(&h->pdev->dev, "%s FAILED, "
+			"Device lookup failed.\n", msg);
 		return FAILED;
 	}
 
 	/* Get SCSI command to be aborted */
 	abort = (struct CommandList *) sc->host_scribble;
 	if (abort == NULL) {
-		dev_err(&h->pdev->dev, "%s FAILED, Command to abort is NULL.\n",
-				msg);
+		/* This can happen if the command already completed. */
+		return SUCCESS;
+	}
+	refcount = atomic_inc_return(&abort->refcount);
+	if (refcount == 1) { /* Command is done already. */
+		cmd_free(h, abort);
+		return SUCCESS;
+	}
+
+	/* Don't bother trying the abort if we know it won't work. */
+	if (abort->cmd_type != CMD_IOACCEL2 &&
+		abort->cmd_type != CMD_IOACCEL1 && !dev->supports_aborts) {
+		cmd_free(h, abort);
 		return FAILED;
 	}
+
+	/* Check that we're aborting the right command.
+	 * It's possible the CommandList already completed and got re-used.
+	 */
+	if (abort->scsi_cmd != sc) {
+		cmd_free(h, abort);
+		return SUCCESS;
+	}
+
 	hpsa_get_tag(h, abort, &taglower, &tagupper);
+	reply_queue = hpsa_extract_reply_queue(h, abort);
 	ml += sprintf(msg+ml, "Tag:0x%08x:%08x ", tagupper, taglower);
-	as  = (struct scsi_cmnd *) abort->scsi_cmd;
+	as  = abort->scsi_cmd;
 	if (as != NULL)
 		ml += sprintf(msg+ml, "Command:0x%x SN:0x%lx ",
 			as->cmnd[0], as->serial_number);
@@ -4619,60 +5390,34 @@ static int hpsa_eh_abort_handler(struct scsi_cmnd *sc)
 	dev_warn(&h->pdev->dev, "Abort request on C%d:B%d:T%d:L%d\n",
 		h->scsi_host->host_no, dev->bus, dev->target, dev->lun);
 
-	/* Search reqQ to See if command is queued but not submitted,
-	 * if so, complete the command with aborted status and remove
-	 * it from the reqQ.
-	 */
-	found = hpsa_find_cmd_in_queue(h, sc, &h->reqQ);
-	if (found) {
-		found->err_info->CommandStatus = CMD_ABORTED;
-		finish_cmd(found);
-		dev_info(&h->pdev->dev, "%s Request SUCCEEDED (driver queue).\n",
-				msg);
-		return SUCCESS;
-	}
-
-	/* not in reqQ, if also not in cmpQ, must have already completed */
-	found = hpsa_find_cmd_in_queue(h, sc, &h->cmpQ);
-	if (!found)  {
-		dev_dbg(&h->pdev->dev, "%s Request SUCCEEDED (not known to driver).\n",
-				msg);
-		return SUCCESS;
-	}
-
 	/*
 	 * Command is in flight, or possibly already completed
 	 * by the firmware (but not to the scsi mid layer) but we can't
 	 * distinguish which.  Send the abort down.
 	 */
-	rc = hpsa_send_abort_both_ways(h, dev->scsi3addr, abort);
+	if (wait_for_available_abort_cmd(h)) {
+		dev_warn(&h->pdev->dev,
+			"Timed out waiting for an abort command to become available.\n");
+		cmd_free(h, abort);
+		return FAILED;
+	}
+	rc = hpsa_send_abort_both_ways(h, dev->scsi3addr, abort, reply_queue);
+	atomic_inc(&h->abort_cmds_available);
+	wake_up_all(&h->abort_cmd_wait_queue);
 	if (rc != 0) {
 		dev_dbg(&h->pdev->dev, "%s Request FAILED.\n", msg);
-		dev_warn(&h->pdev->dev, "FAILED abort on device C%d:B%d:T%d:L%d\n",
+		dev_warn(&h->pdev->dev, "FAILED abort on device "
+			"C%d:B%d:T%d:L%d\n",
 			h->scsi_host->host_no,
 			dev->bus, dev->target, dev->lun);
+		cmd_free(h, abort);
 		return FAILED;
 	}
 	dev_info(&h->pdev->dev, "%s REQUEST SUCCEEDED.\n", msg);
-
-	/* If the abort(s) above completed and actually aborted the
-	 * command, then the command to be aborted should already be
-	 * completed.  If not, wait around a bit more to see if they
-	 * manage to complete normally.
-	 */
-#define ABORT_COMPLETE_WAIT_SECS 30
-	for (i = 0; i < ABORT_COMPLETE_WAIT_SECS * 10; i++) {
-		found = hpsa_find_cmd_in_queue(h, sc, &h->cmpQ);
-		if (!found)
-			return SUCCESS;
-		msleep(100);
-	}
-	dev_warn(&h->pdev->dev, "%s FAILED. Aborted command has not completed after %d seconds.\n",
-		msg, ABORT_COMPLETE_WAIT_SECS);
-	return FAILED;
+	cmd_free(h, abort);
+	return SUCCESS;
 }
 
-
 /*
  * For operations that cannot sleep, a command block is allocated at init,
  * and managed by cmd_alloc() and cmd_free() using a simple bitmap to track
@@ -4682,106 +5427,52 @@ static int hpsa_eh_abort_handler(struct scsi_cmnd *sc)
 static struct CommandList *cmd_alloc(struct ctlr_info *h)
 {
 	struct CommandList *c;
-	int i;
-	union u64bit temp64;
-	dma_addr_t cmd_dma_handle, err_dma_handle;
-	unsigned long flags;
+	int refcount, i;
+	unsigned long offset = 0;
+
+	/* There is some *extremely* small but non-zero chance that that
+	 * multiple threads could get in here, and one thread could
+	 * be scanning through the list of bits looking for a free
+	 * one, but the free ones are always behind him, and other
+	 * threads sneak in behind him and eat them before he can
+	 * get to them, so that while there is always a free one, a
+	 * very unlucky thread might be starved anyway, never able to
+	 * beat the other threads.  In reality, this happens so
+	 * infrequently as to be indistinguishable from never.
+	 */
 
-	spin_lock_irqsave(&h->lock, flags);
-	do {
-		i = find_first_zero_bit(h->cmd_pool_bits, h->nr_cmds);
-		if (i == h->nr_cmds) {
-			spin_unlock_irqrestore(&h->lock, flags);
-			return NULL;
+	offset = h->last_allocation; /* benighly racy */
+	for (;;) {
+		i = find_next_zero_bit(h->cmd_pool_bits, h->nr_cmds, offset);
+		if (unlikely(i == h->nr_cmds)) {
+			offset = 0;
+			continue;
 		}
-	} while (test_and_set_bit
-		 (i & (BITS_PER_LONG - 1),
-		  h->cmd_pool_bits + (i / BITS_PER_LONG)) != 0);
-	spin_unlock_irqrestore(&h->lock, flags);
-
-	c = h->cmd_pool + i;
-	memset(c, 0, sizeof(*c));
-	cmd_dma_handle = h->cmd_pool_dhandle
-	    + i * sizeof(*c);
-	c->err_info = h->errinfo_pool + i;
-	memset(c->err_info, 0, sizeof(*c->err_info));
-	err_dma_handle = h->errinfo_pool_dhandle
-	    + i * sizeof(*c->err_info);
-
-	c->cmdindex = i;
-
-	INIT_LIST_HEAD(&c->list);
-	c->busaddr = (u32) cmd_dma_handle;
-	temp64.val = (u64) err_dma_handle;
-	c->ErrDesc.Addr.lower = temp64.val32.lower;
-	c->ErrDesc.Addr.upper = temp64.val32.upper;
-	c->ErrDesc.Len = sizeof(*c->err_info);
-
-	c->h = h;
-	return c;
-}
-
-/* For operations that can wait for kmalloc to possibly sleep,
- * this routine can be called. Lock need not be held to call
- * cmd_special_alloc. cmd_special_free() is the complement.
- */
-static struct CommandList *cmd_special_alloc(struct ctlr_info *h)
-{
-	struct CommandList *c;
-	union u64bit temp64;
-	dma_addr_t cmd_dma_handle, err_dma_handle;
-
-	c = pci_alloc_consistent(h->pdev, sizeof(*c), &cmd_dma_handle);
-	if (c == NULL)
-		return NULL;
-	memset(c, 0, sizeof(*c));
-
-	c->cmd_type = CMD_SCSI;
-	c->cmdindex = -1;
-
-	c->err_info = pci_alloc_consistent(h->pdev, sizeof(*c->err_info),
-		    &err_dma_handle);
-
-	if (c->err_info == NULL) {
-		pci_free_consistent(h->pdev,
-			sizeof(*c), c, cmd_dma_handle);
-		return NULL;
+		c = h->cmd_pool + i;
+		refcount = atomic_inc_return(&c->refcount);
+		if (unlikely(refcount > 1)) {
+			cmd_free(h, c); /* already in use */
+			offset = (i + 1) % h->nr_cmds;
+			continue;
+		}
+		set_bit(i & (BITS_PER_LONG - 1),
+			h->cmd_pool_bits + (i / BITS_PER_LONG));
+		break; /* it's ours now. */
 	}
-	memset(c->err_info, 0, sizeof(*c->err_info));
-
-	INIT_LIST_HEAD(&c->list);
-	c->busaddr = (u32) cmd_dma_handle;
-	temp64.val = (u64) err_dma_handle;
-	c->ErrDesc.Addr.lower = temp64.val32.lower;
-	c->ErrDesc.Addr.upper = temp64.val32.upper;
-	c->ErrDesc.Len = sizeof(*c->err_info);
-
-	c->h = h;
+	h->last_allocation = i; /* benignly racy */
+	hpsa_cmd_init(h, i, c);
 	return c;
 }
 
 static void cmd_free(struct ctlr_info *h, struct CommandList *c)
 {
-	int i;
-	unsigned long flags;
-
-	i = c - h->cmd_pool;
-	spin_lock_irqsave(&h->lock, flags);
-	clear_bit(i & (BITS_PER_LONG - 1),
-		  h->cmd_pool_bits + (i / BITS_PER_LONG));
-	spin_unlock_irqrestore(&h->lock, flags);
-}
-
-static void cmd_special_free(struct ctlr_info *h, struct CommandList *c)
-{
-	union u64bit temp64;
+	if (atomic_dec_and_test(&c->refcount)) {
+		int i;
 
-	temp64.val32.lower = c->ErrDesc.Addr.lower;
-	temp64.val32.upper = c->ErrDesc.Addr.upper;
-	pci_free_consistent(h->pdev, sizeof(*c->err_info),
-			    c->err_info, (dma_addr_t) temp64.val);
-	pci_free_consistent(h->pdev, sizeof(*c),
-			    c, (dma_addr_t) (c->busaddr & DIRECT_LOOKUP_MASK));
+		i = c - h->cmd_pool;
+		clear_bit(i & (BITS_PER_LONG - 1),
+			  h->cmd_pool_bits + (i / BITS_PER_LONG));
+	}
 }
 
 #ifdef CONFIG_COMPAT
@@ -4867,7 +5558,6 @@ static int hpsa_compat_ioctl(struct scsi_device *dev, int cmd, void *arg)
 	case CCISS_SETINTINFO:
 	case CCISS_GETNODENAME:
 	case CCISS_SETNODENAME:
-	case CCISS_GETHEARTBEAT:
 	case CCISS_GETBUSTYPES:
 	case CCISS_GETFIRMVER:
 	case CCISS_GETDRIVVER:
@@ -4933,7 +5623,7 @@ static int hpsa_passthru_ioctl(struct ctlr_info *h, void __user *argp)
 	IOCTL_Command_struct iocommand;
 	struct CommandList *c;
 	char *buff = NULL;
-	union u64bit temp64;
+	u64 temp64;
 	int rc = 0;
 
 	if (!argp)
@@ -4961,7 +5651,7 @@ static int hpsa_passthru_ioctl(struct ctlr_info *h, void __user *argp)
 			memset(buff, 0, iocommand.buf_size);
 		}
 	}
-	c = cmd_special_alloc(h);
+	c = cmd_alloc(h);
 	if (c == NULL) {
 		rc = -ENOMEM;
 		goto out_kfree;
@@ -4971,15 +5661,13 @@ static int hpsa_passthru_ioctl(struct ctlr_info *h, void __user *argp)
 	/* Fill in Command Header */
 	c->Header.ReplyQueue = 0; /* unused in simple mode */
 	if (iocommand.buf_size > 0) {	/* buffer to fill */
-		c->Header.SGList = 1;
-		c->Header.SGTotal = 1;
+		c->Header.SGList = (u8) 1;
+		c->Header.SGTotal = cpu_to_le16(1);
 	} else	{ /* no buffers to fill */
 		c->Header.SGList = 0;
-		c->Header.SGTotal = 0;
+		c->Header.SGTotal = cpu_to_le16(0);
 	}
 	memcpy(&c->Header.LUN, &iocommand.LUN_info, sizeof(c->Header.LUN));
-	/* use the kernel address the cmd block for tag */
-	c->Header.Tag.lower = c->busaddr;
 
 	/* Fill in Request block */
 	memcpy(&c->Request, &iocommand.Request,
@@ -4987,21 +5675,22 @@ static int hpsa_passthru_ioctl(struct ctlr_info *h, void __user *argp)
 
 	/* Fill in the scatter gather information */
 	if (iocommand.buf_size > 0) {
-		temp64.val = pci_map_single(h->pdev, buff,
+		temp64 = (u64) pci_map_single(h->pdev, buff,
 			iocommand.buf_size, PCI_DMA_BIDIRECTIONAL);
-		if (dma_mapping_error(&h->pdev->dev, temp64.val)) {
-			c->SG[0].Addr.lower = 0;
-			c->SG[0].Addr.upper = 0;
-			c->SG[0].Len = 0;
+		if (hpsa_dma_mapping_error(&h->pdev->dev,
+						(dma_addr_t) temp64)) {
+			c->SG[0].Addr = cpu_to_le64(0);
+			c->SG[0].Len = cpu_to_le32(0);
 			rc = -ENOMEM;
 			goto out;
 		}
-		c->SG[0].Addr.lower = temp64.val32.lower;
-		c->SG[0].Addr.upper = temp64.val32.upper;
-		c->SG[0].Len = iocommand.buf_size;
-		c->SG[0].Ext = HPSA_SG_LAST; /* we are not chaining*/
+		c->SG[0].Addr = cpu_to_le64(temp64);
+		c->SG[0].Len = cpu_to_le32(iocommand.buf_size);
+		c->SG[0].Ext = cpu_to_le32(HPSA_SG_LAST); /* not chaining */
 	}
-	hpsa_scsi_do_simple_cmd_core_if_no_lockup(h, c);
+	rc = hpsa_scsi_do_simple_cmd_core_if_no_lockup(h, c, NO_TIMEOUT);
+	if (rc)
+		rc = -EIO;
 	if (iocommand.buf_size > 0)
 		hpsa_pci_unmap(h->pdev, c, 1, PCI_DMA_BIDIRECTIONAL);
 	check_ioctl_unit_attention(h, c);
@@ -5022,7 +5711,7 @@ static int hpsa_passthru_ioctl(struct ctlr_info *h, void __user *argp)
 		}
 	}
 out:
-	cmd_special_free(h, c);
+	cmd_free(h, c);
 out_kfree:
 	kfree(buff);
 	return rc;
@@ -5034,7 +5723,7 @@ static int hpsa_big_passthru_ioctl(struct ctlr_info *h, void __user *argp)
 	struct CommandList *c;
 	unsigned char **buff = NULL;
 	int *buff_size = NULL;
-	union u64bit temp64;
+	u64 temp64;
 	BYTE sg_used = 0;
 	int status = 0;
 	int i;
@@ -5101,38 +5790,43 @@ static int hpsa_big_passthru_ioctl(struct ctlr_info *h, void __user *argp)
 		data_ptr += sz;
 		sg_used++;
 	}
-	c = cmd_special_alloc(h);
+	c = cmd_alloc(h);
 	if (c == NULL) {
 		status = -ENOMEM;
 		goto cleanup1;
 	}
 	c->cmd_type = CMD_IOCTL_PEND;
 	c->Header.ReplyQueue = 0;
-	c->Header.SGList = c->Header.SGTotal = sg_used;
+	c->Header.SGList = (u8) sg_used;
+	c->Header.SGTotal = cpu_to_le16(sg_used);
 	memcpy(&c->Header.LUN, &ioc->LUN_info, sizeof(c->Header.LUN));
-	c->Header.Tag.lower = c->busaddr;
 	memcpy(&c->Request, &ioc->Request, sizeof(c->Request));
 	if (ioc->buf_size > 0) {
 		int i;
 		for (i = 0; i < sg_used; i++) {
-			temp64.val = pci_map_single(h->pdev, buff[i],
+			temp64 = (u64) pci_map_single(h->pdev, buff[i],
 				    buff_size[i], PCI_DMA_BIDIRECTIONAL);
-			if (dma_mapping_error(&h->pdev->dev, temp64.val)) {
-				c->SG[i].Addr.lower = 0;
-				c->SG[i].Addr.upper = 0;
+			if (hpsa_dma_mapping_error(&h->pdev->dev,
+							(dma_addr_t) temp64)) {
+				c->SG[i].Addr = 0;
 				c->SG[i].Len = 0;
 				hpsa_pci_unmap(h->pdev, c, i,
 					PCI_DMA_BIDIRECTIONAL);
 				status = -ENOMEM;
 				goto cleanup0;
 			}
-			c->SG[i].Addr.lower = temp64.val32.lower;
-			c->SG[i].Addr.upper = temp64.val32.upper;
-			c->SG[i].Len = buff_size[i];
-			c->SG[i].Ext = i < sg_used - 1 ? 0 : HPSA_SG_LAST;
+			c->SG[i].Addr = cpu_to_le64(temp64);
+			c->SG[i].Len = cpu_to_le32(buff_size[i]);
+			/* we are not chaining */
+			c->SG[i].Ext =
+				cpu_to_le32((i == sg_used) * HPSA_SG_LAST);
 		}
 	}
-	hpsa_scsi_do_simple_cmd_core_if_no_lockup(h, c);
+	status = hpsa_scsi_do_simple_cmd_core_if_no_lockup(h, c, NO_TIMEOUT);
+	if (status) {
+		status = -EIO;
+		goto cleanup0;
+	}
 	if (sg_used)
 		hpsa_pci_unmap(h->pdev, c, sg_used, PCI_DMA_BIDIRECTIONAL);
 	check_ioctl_unit_attention(h, c);
@@ -5155,7 +5849,7 @@ static int hpsa_big_passthru_ioctl(struct ctlr_info *h, void __user *argp)
 	}
 	status = 0;
 cleanup0:
-	cmd_special_free(h, c);
+	cmd_free(h, c);
 cleanup1:
 	if (buff) {
 		for (i = 0; i < sg_used; i++)
@@ -5175,35 +5869,6 @@ static void check_ioctl_unit_attention(struct ctlr_info *h,
 		(void) check_for_unit_attention(h, c);
 }
 
-static int increment_passthru_count(struct ctlr_info *h)
-{
-	unsigned long flags;
-
-	spin_lock_irqsave(&h->passthru_count_lock, flags);
-	if (h->passthru_count >= HPSA_MAX_CONCURRENT_PASSTHRUS) {
-		spin_unlock_irqrestore(&h->passthru_count_lock, flags);
-		return -1;
-	}
-	h->passthru_count++;
-	spin_unlock_irqrestore(&h->passthru_count_lock, flags);
-	return 0;
-}
-
-static void decrement_passthru_count(struct ctlr_info *h)
-{
-	unsigned long flags;
-
-	spin_lock_irqsave(&h->passthru_count_lock, flags);
-	if (h->passthru_count <= 0) {
-		spin_unlock_irqrestore(&h->passthru_count_lock, flags);
-		/* not expecting to get here. */
-		dev_warn(&h->pdev->dev, "Bug detected, passthru_count seems to be incorrect.\n");
-		return;
-	}
-	h->passthru_count--;
-	spin_unlock_irqrestore(&h->passthru_count_lock, flags);
-}
-
 /*
  * ioctl
  */
@@ -5226,24 +5891,24 @@ static int hpsa_ioctl(struct scsi_device *dev, int cmd, void *arg)
 	case CCISS_GETDRIVVER:
 		return hpsa_getdrivver_ioctl(h, argp);
 	case CCISS_PASSTHRU:
-		if (increment_passthru_count(h))
+		if (atomic_dec_if_positive(&h->passthru_cmds_avail) < 0)
 			return -EAGAIN;
 		rc = hpsa_passthru_ioctl(h, argp);
-		decrement_passthru_count(h);
+		atomic_inc(&h->passthru_cmds_avail);
 		return rc;
 	case CCISS_BIG_PASSTHRU:
-		if (increment_passthru_count(h))
+		if (atomic_dec_if_positive(&h->passthru_cmds_avail) < 0)
 			return -EAGAIN;
 		rc = hpsa_big_passthru_ioctl(h, argp);
-		decrement_passthru_count(h);
+		atomic_inc(&h->passthru_cmds_avail);
 		return rc;
 	default:
 		return -ENOTTY;
 	}
 }
 
-static int hpsa_send_host_reset(struct ctlr_info *h, unsigned char *scsi3addr,
-				u8 reset_type)
+static int __devinit hpsa_send_host_reset(struct ctlr_info *h, unsigned char *scsi3addr,
+	u8 reset_type)
 {
 	struct CommandList *c;
 
@@ -5268,21 +5933,20 @@ static int fill_cmd(struct CommandList *c, u8 cmd, struct ctlr_info *h,
 	int cmd_type)
 {
 	int pci_dir = XFER_NONE;
-	struct CommandList *a; /* for commands to be aborted */
+	u64 tag; /* for commands to be aborted */
+	u32 tupper, tlower;
 
 	c->cmd_type = CMD_IOCTL_PEND;
 	c->Header.ReplyQueue = 0;
 	if (buff != NULL && size > 0) {
-		c->Header.SGList = 1;
-		c->Header.SGTotal = 1;
+		c->Header.SGList = (u8) 1;
+		c->Header.SGTotal = cpu_to_le32(1);
 	} else {
 		c->Header.SGList = 0;
 		c->Header.SGTotal = 0;
 	}
-	c->Header.Tag.lower = c->busaddr;
 	memcpy(c->Header.LUN.LunAddrBytes, scsi3addr, 8);
 
-	c->Request.Type.Type = cmd_type;
 	if (cmd_type == TYPE_CMD) {
 		switch (cmd) {
 		case HPSA_INQUIRY:
@@ -5292,8 +5956,8 @@ static int fill_cmd(struct CommandList *c, u8 cmd, struct ctlr_info *h,
 				c->Request.CDB[2] = (page_code & 0xff);
 			}
 			c->Request.CDBLen = 6;
-			c->Request.Type.Attribute = ATTR_SIMPLE;
-			c->Request.Type.Direction = XFER_READ;
+			c->Request.type_attr_dir =
+				TYPE_ATTR_DIR(cmd_type, ATTR_SIMPLE, XFER_READ);
 			c->Request.Timeout = 0;
 			c->Request.CDB[0] = HPSA_INQUIRY;
 			c->Request.CDB[4] = size & 0xFF;
@@ -5304,8 +5968,8 @@ static int fill_cmd(struct CommandList *c, u8 cmd, struct ctlr_info *h,
 			   mode = 00 target = 0.  Nothing to write.
 			 */
 			c->Request.CDBLen = 12;
-			c->Request.Type.Attribute = ATTR_SIMPLE;
-			c->Request.Type.Direction = XFER_READ;
+			c->Request.type_attr_dir =
+				TYPE_ATTR_DIR(cmd_type, ATTR_SIMPLE, XFER_READ);
 			c->Request.Timeout = 0;
 			c->Request.CDB[0] = cmd;
 			c->Request.CDB[6] = (size >> 24) & 0xFF; /* MSB */
@@ -5315,8 +5979,9 @@ static int fill_cmd(struct CommandList *c, u8 cmd, struct ctlr_info *h,
 			break;
 		case HPSA_CACHE_FLUSH:
 			c->Request.CDBLen = 12;
-			c->Request.Type.Attribute = ATTR_SIMPLE;
-			c->Request.Type.Direction = XFER_WRITE;
+			c->Request.type_attr_dir =
+					TYPE_ATTR_DIR(cmd_type,
+						ATTR_SIMPLE, XFER_WRITE);
 			c->Request.Timeout = 0;
 			c->Request.CDB[0] = BMIC_WRITE;
 			c->Request.CDB[6] = BMIC_CACHE_FLUSH;
@@ -5325,14 +5990,14 @@ static int fill_cmd(struct CommandList *c, u8 cmd, struct ctlr_info *h,
 			break;
 		case TEST_UNIT_READY:
 			c->Request.CDBLen = 6;
-			c->Request.Type.Attribute = ATTR_SIMPLE;
-			c->Request.Type.Direction = XFER_NONE;
+			c->Request.type_attr_dir =
+				TYPE_ATTR_DIR(cmd_type, ATTR_SIMPLE, XFER_NONE);
 			c->Request.Timeout = 0;
 			break;
 		case HPSA_GET_RAID_MAP:
 			c->Request.CDBLen = 12;
-			c->Request.Type.Attribute = ATTR_SIMPLE;
-			c->Request.Type.Direction = XFER_READ;
+			c->Request.type_attr_dir =
+				TYPE_ATTR_DIR(cmd_type, ATTR_SIMPLE, XFER_READ);
 			c->Request.Timeout = 0;
 			c->Request.CDB[0] = HPSA_CISS_READ;
 			c->Request.CDB[1] = cmd;
@@ -5343,14 +6008,24 @@ static int fill_cmd(struct CommandList *c, u8 cmd, struct ctlr_info *h,
 			break;
 		case BMIC_SENSE_CONTROLLER_PARAMETERS:
 			c->Request.CDBLen = 10;
-			c->Request.Type.Attribute = ATTR_SIMPLE;
-			c->Request.Type.Direction = XFER_READ;
+			c->Request.type_attr_dir =
+				TYPE_ATTR_DIR(cmd_type, ATTR_SIMPLE, XFER_READ);
 			c->Request.Timeout = 0;
 			c->Request.CDB[0] = BMIC_READ;
 			c->Request.CDB[6] = BMIC_SENSE_CONTROLLER_PARAMETERS;
 			c->Request.CDB[7] = (size >> 16) & 0xFF;
 			c->Request.CDB[8] = (size >> 8) & 0xFF;
 			break;
+		case BMIC_IDENTIFY_PHYSICAL_DEVICE:
+			c->Request.CDBLen = 10;
+			c->Request.type_attr_dir =
+				TYPE_ATTR_DIR(cmd_type, ATTR_SIMPLE, XFER_READ);
+			c->Request.Timeout = 0;
+			c->Request.CDB[0] = BMIC_READ;
+			c->Request.CDB[6] = BMIC_IDENTIFY_PHYSICAL_DEVICE;
+			c->Request.CDB[7] = (size >> 16) & 0xFF;
+			c->Request.CDB[8] = (size >> 8) & 0XFF;
+			break;
 		default:
 			dev_warn(&h->pdev->dev, "unknown command 0x%c\n", cmd);
 			BUG();
@@ -5359,15 +6034,28 @@ static int fill_cmd(struct CommandList *c, u8 cmd, struct ctlr_info *h,
 	} else if (cmd_type == TYPE_MSG) {
 		switch (cmd) {
 
+		case  HPSA_PHYS_TARGET_RESET:
+			c->Request.CDBLen = 16;
+			c->Request.type_attr_dir =
+				TYPE_ATTR_DIR(cmd_type, ATTR_SIMPLE, XFER_NONE);
+			c->Request.Timeout = 0; /* Don't time out */
+			memset(&c->Request.CDB[0], 0, sizeof(c->Request.CDB));
+			c->Request.CDB[0] = HPSA_RESET;
+			c->Request.CDB[1] = HPSA_TARGET_RESET_TYPE;
+			/* Physical target reset needs no control bytes 4-7*/
+			c->Request.CDB[4] = 0x00;
+			c->Request.CDB[5] = 0x00;
+			c->Request.CDB[6] = 0x00;
+			c->Request.CDB[7] = 0x00;
+			break;
 		case  HPSA_DEVICE_RESET_MSG:
 			c->Request.CDBLen = 16;
-			c->Request.Type.Type =  1; /* It is a MSG not a CMD */
-			c->Request.Type.Attribute = ATTR_SIMPLE;
-			c->Request.Type.Direction = XFER_NONE;
+			c->Request.type_attr_dir =
+				TYPE_ATTR_DIR(cmd_type, ATTR_SIMPLE, XFER_NONE);
 			c->Request.Timeout = 0; /* Don't time out */
 			memset(&c->Request.CDB[0], 0, sizeof(c->Request.CDB));
 			c->Request.CDB[0] =  cmd;
-			c->Request.CDB[1] = HPSA_RESET_TYPE_LUN;
+			c->Request.CDB[1] = HPSA_LUN_RESET_TYPE;
 			/* If bytes 4-7 are zero, it means reset the */
 			/* LunID device */
 			c->Request.CDB[4] = 0x00;
@@ -5376,28 +6064,37 @@ static int fill_cmd(struct CommandList *c, u8 cmd, struct ctlr_info *h,
 			c->Request.CDB[7] = 0x00;
 			break;
 		case  HPSA_ABORT_MSG:
-			a = buff;       /* point to command to be aborted */
-			dev_dbg(&h->pdev->dev, "Abort Tag:0x%08x:%08x using request Tag:0x%08x:%08x\n",
-				a->Header.Tag.upper, a->Header.Tag.lower,
-				c->Header.Tag.upper, c->Header.Tag.lower);
+			memcpy(&tag, buff, sizeof(tag));
+			if (buff == NULL) {
+				dev_warn(&h->pdev->dev, "Null Abort request.\n");
+				BUG();
+			}
+			dev_dbg(&h->pdev->dev,
+				"Abort Tag:0x%016llx using request Tag:0x%016llx",
+				tag, c->Header.tag);
+			tlower = (u32) (tag >> 32);
+			tupper = (u32) (tag & 0x0ffffffffULL);
 			c->Request.CDBLen = 16;
-			c->Request.Type.Type = TYPE_MSG;
-			c->Request.Type.Attribute = ATTR_SIMPLE;
-			c->Request.Type.Direction = XFER_WRITE;
+			c->Request.type_attr_dir =
+					TYPE_ATTR_DIR(cmd_type,
+						ATTR_SIMPLE, XFER_WRITE);
 			c->Request.Timeout = 0; /* Don't time out */
 			c->Request.CDB[0] = HPSA_TASK_MANAGEMENT;
 			c->Request.CDB[1] = HPSA_TMF_ABORT_TASK;
 			c->Request.CDB[2] = 0x00; /* reserved */
 			c->Request.CDB[3] = 0x00; /* reserved */
 			/* Tag to abort goes in CDB[4]-CDB[11] */
-			c->Request.CDB[4] = a->Header.Tag.lower & 0xFF;
-			c->Request.CDB[5] = (a->Header.Tag.lower >> 8) & 0xFF;
-			c->Request.CDB[6] = (a->Header.Tag.lower >> 16) & 0xFF;
-			c->Request.CDB[7] = (a->Header.Tag.lower >> 24) & 0xFF;
-			c->Request.CDB[8] = a->Header.Tag.upper & 0xFF;
-			c->Request.CDB[9] = (a->Header.Tag.upper >> 8) & 0xFF;
-			c->Request.CDB[10] = (a->Header.Tag.upper >> 16) & 0xFF;
-			c->Request.CDB[11] = (a->Header.Tag.upper >> 24) & 0xFF;
+
+			/* FIXME: Double check the byte swapping here. */
+			c->Request.CDB[4] = tlower & 0xFF;
+			c->Request.CDB[5] = (tlower >> 8) & 0xFF;
+			c->Request.CDB[6] = (tlower >> 16) & 0xFF;
+			c->Request.CDB[7] = (tlower >> 24) & 0xFF;
+			c->Request.CDB[8] = tupper & 0xFF;
+			c->Request.CDB[9] = (tupper >> 8) & 0xFF;
+			c->Request.CDB[10] = (tupper >> 16) & 0xFF;
+			c->Request.CDB[11] = tupper & 0xFF;
+
 			c->Request.CDB[12] = 0x00; /* reserved */
 			c->Request.CDB[13] = 0x00; /* reserved */
 			c->Request.CDB[14] = 0x00; /* reserved */
@@ -5413,7 +6110,7 @@ static int fill_cmd(struct CommandList *c, u8 cmd, struct ctlr_info *h,
 		BUG();
 	}
 
-	switch (c->Request.Type.Direction) {
+	switch (GET_DIR(c->Request.type_attr_dir)) {
 	case XFER_READ:
 		pci_dir = PCI_DMA_FROMDEVICE;
 		break;
@@ -5438,65 +6135,17 @@ static void __iomem *remap_pci_mem(ulong base, ulong size)
 {
 	ulong page_base = ((ulong) base) & PAGE_MASK;
 	ulong page_offs = ((ulong) base) - page_base;
-	void __iomem *page_remapped = ioremap_nocache(page_base,
-		page_offs + size);
+	void __iomem *page_remapped = ioremap_nocache(page_base, page_offs + size);
 
 	return page_remapped ? (page_remapped + page_offs) : NULL;
 }
 
-/* Takes cmds off the submission queue and sends them to the hardware,
- * then puts them on the queue of cmds waiting for completion.
- * Assumes h->lock is held
- */
-static void start_io(struct ctlr_info *h, unsigned long *flags)
-{
-	struct CommandList *c;
-
-	while (!list_empty(&h->reqQ)) {
-		c = list_entry(h->reqQ.next, struct CommandList, list);
-		/* can't do anything if fifo is full */
-		if ((h->access.fifo_full(h))) {
-			h->fifo_recently_full = 1;
-			dev_warn(&h->pdev->dev, "fifo full\n");
-			break;
-		}
-		h->fifo_recently_full = 0;
-
-		/* Get the first entry from the Request Q */
-		removeQ(c);
-		h->Qdepth--;
-
-		/* Put job onto the completed Q */
-		addQ(&h->cmpQ, c);
-
-		/* Must increment commands_outstanding before unlocking
-		 * and submitting to avoid race checking for fifo full
-		 * condition.
-		 */
-		h->commands_outstanding++;
-
-		/* Tell the controller execute command */
-		spin_unlock_irqrestore(&h->lock, *flags);
-		h->access.submit_command(h, c);
-		spin_lock_irqsave(&h->lock, *flags);
-	}
-}
-
-static void lock_and_start_io(struct ctlr_info *h)
-{
-	unsigned long flags;
-
-	spin_lock_irqsave(&h->lock, flags);
-	start_io(h, &flags);
-	spin_unlock_irqrestore(&h->lock, flags);
-}
-
 static inline unsigned long get_next_completion(struct ctlr_info *h, u8 q)
 {
 	return h->access.command_completed(h, q);
 }
 
-static inline bool interrupt_pending(struct ctlr_info *h)
+static inline int interrupt_pending(struct ctlr_info *h)
 {
 	return h->access.intr_pending(h);
 }
@@ -5519,56 +6168,14 @@ static inline int bad_tag(struct ctlr_info *h, u32 tag_index,
 
 static inline void finish_cmd(struct CommandList *c)
 {
-	unsigned long flags;
-	int io_may_be_stalled = 0;
-	struct ctlr_info *h = c->h;
-
-	spin_lock_irqsave(&h->lock, flags);
-	removeQ(c);
-
-	/*
-	 * Check for possibly stalled i/o.
-	 *
-	 * If a fifo_full condition is encountered, requests will back up
-	 * in h->reqQ.  This queue is only emptied out by start_io which is
-	 * only called when a new i/o request comes in.  If no i/o's are
-	 * forthcoming, the i/o's in h->reqQ can get stuck.  So we call
-	 * start_io from here if we detect such a danger.
-	 *
-	 * Normally, we shouldn't hit this case, but pounding on the
-	 * CCISS_PASSTHRU ioctl can provoke it.  Only call start_io if
-	 * commands_outstanding is low.  We want to avoid calling
-	 * start_io from in here as much as possible, and esp. don't
-	 * want to get in a cycle where we call start_io every time
-	 * through here.
-	 */
-	if (unlikely(h->fifo_recently_full) &&
-		h->commands_outstanding < 5)
-		io_may_be_stalled = 1;
-
-	spin_unlock_irqrestore(&h->lock, flags);
-
 	dial_up_lockup_detection_on_fw_flash_complete(c->h, c);
 	if (likely(c->cmd_type == CMD_IOACCEL1 || c->cmd_type == CMD_SCSI
 			|| c->cmd_type == CMD_IOACCEL2))
 		complete_scsi_command(c);
-	else if (c->cmd_type == CMD_IOCTL_PEND)
-		complete(c->waiting);
-	if (unlikely(io_may_be_stalled))
-		lock_and_start_io(h);
-}
-
-static inline u32 hpsa_tag_contains_index(u32 tag)
-{
-	return tag & DIRECT_LOOKUP_BIT;
-}
-
-static inline u32 hpsa_tag_to_index(u32 tag)
-{
-	return tag >> DIRECT_LOOKUP_SHIFT;
+	else if (c->cmd_type == CMD_IOCTL_PEND)
+		complete(c->waiting);
 }
 
-
 static inline u32 hpsa_tag_discard_error_bits(struct ctlr_info *h, u32 tag)
 {
 #define HPSA_PERF_ERROR_BITS ((1 << DIRECT_LOOKUP_SHIFT) - 1)
@@ -5585,34 +6192,13 @@ static inline void process_indexed_cmd(struct ctlr_info *h,
 	u32 tag_index;
 	struct CommandList *c;
 
-	tag_index = hpsa_tag_to_index(raw_tag);
+	tag_index = raw_tag >> DIRECT_LOOKUP_SHIFT;
 	if (!bad_tag(h, tag_index, raw_tag)) {
 		c = h->cmd_pool + tag_index;
 		finish_cmd(c);
 	}
 }
 
-/* process completion of a non-indexed command */
-static inline void process_nonindexed_cmd(struct ctlr_info *h,
-	u32 raw_tag)
-{
-	u32 tag;
-	struct CommandList *c = NULL;
-	unsigned long flags;
-
-	tag = hpsa_tag_discard_error_bits(h, raw_tag);
-	spin_lock_irqsave(&h->lock, flags);
-	list_for_each_entry(c, &h->cmpQ, list) {
-		if ((c->busaddr & 0xFFFFFFE0) == (tag & 0xFFFFFFE0)) {
-			spin_unlock_irqrestore(&h->lock, flags);
-			finish_cmd(c);
-			return;
-		}
-	}
-	spin_unlock_irqrestore(&h->lock, flags);
-	bad_tag(h, h->nr_cmds + 1, raw_tag);
-}
-
 /* Some controllers, like p400, will give us one interrupt
  * after a soft reset, even if we turned interrupts off.
  * Only need to check for this in the hpsa_xxx_discard_completions
@@ -5635,14 +6221,14 @@ static int ignore_bogus_interrupt(struct ctlr_info *h)
 /*
  * Convert &h->q[x] (passed to interrupt handlers) back to h.
  * Relies on (h-q[x] == x) being true for x such that
- * 0 <= x < MAX_REPLY_QUEUES.
+ * 0 <= x < h->nreply_queues.
  */
 static struct ctlr_info *queue_to_hba(u8 *queue)
 {
 	return container_of((queue - *queue), struct ctlr_info, q[0]);
 }
 
-static irqreturn_t hpsa_intx_discard_completions(int irq, void *queue)
+DECLARE_INTERRUPT_HANDLER(hpsa_intx_discard_completions)
 {
 	struct ctlr_info *h = queue_to_hba(queue);
 	u8 q = *(u8 *) queue;
@@ -5662,7 +6248,7 @@ static irqreturn_t hpsa_intx_discard_completions(int irq, void *queue)
 	return IRQ_HANDLED;
 }
 
-static irqreturn_t hpsa_msix_discard_completions(int irq, void *queue)
+DECLARE_INTERRUPT_HANDLER(hpsa_msix_discard_completions)
 {
 	struct ctlr_info *h = queue_to_hba(queue);
 	u32 raw_tag;
@@ -5678,9 +6264,9 @@ static irqreturn_t hpsa_msix_discard_completions(int irq, void *queue)
 	return IRQ_HANDLED;
 }
 
-static irqreturn_t do_hpsa_intr_intx(int irq, void *queue)
+DECLARE_INTERRUPT_HANDLER(do_hpsa_intr_intx)
 {
-	struct ctlr_info *h = queue_to_hba((u8 *) queue);
+	struct ctlr_info *h = queue_to_hba(queue);
 	u32 raw_tag;
 	u8 q = *(u8 *) queue;
 
@@ -5690,17 +6276,14 @@ static irqreturn_t do_hpsa_intr_intx(int irq, void *queue)
 	while (interrupt_pending(h)) {
 		raw_tag = get_next_completion(h, q);
 		while (raw_tag != FIFO_EMPTY) {
-			if (likely(hpsa_tag_contains_index(raw_tag)))
-				process_indexed_cmd(h, raw_tag);
-			else
-				process_nonindexed_cmd(h, raw_tag);
+			process_indexed_cmd(h, raw_tag);
 			raw_tag = next_command(h, q);
 		}
 	}
 	return IRQ_HANDLED;
 }
 
-static irqreturn_t do_hpsa_intr_msi(int irq, void *queue)
+DECLARE_INTERRUPT_HANDLER(do_hpsa_intr_msi)
 {
 	struct ctlr_info *h = queue_to_hba(queue);
 	u32 raw_tag;
@@ -5709,21 +6292,18 @@ static irqreturn_t do_hpsa_intr_msi(int irq, void *queue)
 	h->last_intr_timestamp = get_jiffies_64();
 	raw_tag = get_next_completion(h, q);
 	while (raw_tag != FIFO_EMPTY) {
-		if (likely(hpsa_tag_contains_index(raw_tag)))
-			process_indexed_cmd(h, raw_tag);
-		else
-			process_nonindexed_cmd(h, raw_tag);
+		process_indexed_cmd(h, raw_tag);
 		raw_tag = next_command(h, q);
 	}
 	return IRQ_HANDLED;
 }
 
-/* Send a message CDB to the firmware. Careful, this only works
+/* Send a message CDB to the firmware.  Careful, this only works
  * in simple mode, not performant mode due to the tag lookup.
  * We only ever use this immediately after a controller reset.
  */
-static int hpsa_message(struct pci_dev *pdev, unsigned char opcode,
-			unsigned char type)
+static __devinit int hpsa_message(struct pci_dev *pdev, unsigned char opcode,
+						unsigned char type)
 {
 	struct Command {
 		struct CommandListHeader CommandHeader;
@@ -5738,7 +6318,9 @@ static int hpsa_message(struct pci_dev *pdev, unsigned char opcode,
 	void __iomem *vaddr;
 	int i, err;
 
-	vaddr = pci_ioremap_bar(pdev, 0);
+	/* kernel.org uses pci_remap_bar() here, but 2.6.27 doesn't have it.*/
+	vaddr = ioremap_nocache(pci_resource_start(pdev, 0),
+					pci_resource_len(pdev, 0));
 	if (vaddr == NULL)
 		return -ENOMEM;
 
@@ -5767,21 +6349,19 @@ static int hpsa_message(struct pci_dev *pdev, unsigned char opcode,
 	cmd->CommandHeader.ReplyQueue = 0;
 	cmd->CommandHeader.SGList = 0;
 	cmd->CommandHeader.SGTotal = 0;
-	cmd->CommandHeader.Tag.lower = paddr32;
-	cmd->CommandHeader.Tag.upper = 0;
+	cmd->CommandHeader.tag = (u64) paddr32;
 	memset(&cmd->CommandHeader.LUN.LunAddrBytes, 0, 8);
 
 	cmd->Request.CDBLen = 16;
-	cmd->Request.Type.Type = TYPE_MSG;
-	cmd->Request.Type.Attribute = ATTR_HEADOFQUEUE;
-	cmd->Request.Type.Direction = XFER_NONE;
+	cmd->Request.type_attr_dir =
+			TYPE_ATTR_DIR(TYPE_MSG, ATTR_HEADOFQUEUE, XFER_NONE);
 	cmd->Request.Timeout = 0; /* Don't time out */
 	cmd->Request.CDB[0] = opcode;
 	cmd->Request.CDB[1] = type;
 	memset(&cmd->Request.CDB[2], 0, 14); /* rest of the CDB is reserved */
-	cmd->ErrorDescriptor.Addr.lower = paddr32 + sizeof(*cmd);
-	cmd->ErrorDescriptor.Addr.upper = 0;
-	cmd->ErrorDescriptor.Len = sizeof(struct ErrorInfo);
+	cmd->ErrorDescriptor.Addr =
+			cpu_to_le64((u64) (paddr32 + sizeof(*cmd)));
+	cmd->ErrorDescriptor.Len = cpu_to_le32(sizeof(struct ErrorInfo));
 
 	writel(paddr32, vaddr + SA5_REQUEST_PORT_OFFSET);
 
@@ -5868,24 +6448,43 @@ static int hpsa_controller_hard_reset(struct pci_dev *pdev,
 		pmcsr &= ~PCI_PM_CTRL_STATE_MASK;
 		pmcsr |= PCI_D0;
 		pci_write_config_word(pdev, pos + PCI_PM_CTRL, pmcsr);
-
-		/*
-		 * The P600 requires a small delay when changing states.
-		 * Otherwise we may think the board did not reset and we bail.
-		 * This for kdump only and is particular to the P600.
-		 */
-		msleep(500);
 	}
 	return 0;
 }
 
-static void init_driver_version(char *driver_version, int len)
+static int hpsa_wait_for_board_state(struct pci_dev *pdev,
+	void __iomem *vaddr, int wait_for_ready)
+{
+	int i, iterations;
+	u32 scratchpad;
+	if (wait_for_ready)
+		iterations = HPSA_BOARD_READY_ITERATIONS;
+	else
+		iterations = HPSA_BOARD_NOT_READY_ITERATIONS;
+
+	for (i = 0; i < iterations; i++) {
+		scratchpad = readl(vaddr + SA5_SCRATCHPAD_OFFSET);
+		if (wait_for_ready) {
+			if (scratchpad == HPSA_FIRMWARE_READY)
+				return 0;
+		} else {
+			if (scratchpad != HPSA_FIRMWARE_READY)
+				return 0;
+		}
+		msleep(HPSA_BOARD_READY_POLL_INTERVAL_MSECS);
+	}
+	dev_warn(&pdev->dev, "board not ready, timed out.\n");
+	return -ENODEV;
+}
+
+static __devinit void init_driver_version(char *driver_version, int len)
 {
 	memset(driver_version, 0, len);
 	strncpy(driver_version, HPSA " " HPSA_DRIVER_VERSION, len - 1);
 }
 
-static int write_driver_ver_to_cfgtable(struct CfgTable __iomem *cfgtable)
+static __devinit int write_driver_ver_to_cfgtable(
+	struct CfgTable __iomem *cfgtable)
 {
 	char *driver_version;
 	int i, size = sizeof(cfgtable->driver_version);
@@ -5901,8 +6500,8 @@ static int write_driver_ver_to_cfgtable(struct CfgTable __iomem *cfgtable)
 	return 0;
 }
 
-static void read_driver_ver_from_cfgtable(struct CfgTable __iomem *cfgtable,
-					  unsigned char *driver_ver)
+static __devinit void read_driver_ver_from_cfgtable(
+	struct CfgTable __iomem *cfgtable, unsigned char *driver_ver)
 {
 	int i;
 
@@ -5910,15 +6509,15 @@ static void read_driver_ver_from_cfgtable(struct CfgTable __iomem *cfgtable,
 		driver_ver[i] = readb(&cfgtable->driver_version[i]);
 }
 
-static int controller_reset_failed(struct CfgTable __iomem *cfgtable)
+static __devinit int controller_reset_failed(
+	struct CfgTable __iomem *cfgtable)
 {
-
 	char *driver_ver, *old_driver_ver;
 	int rc, size = sizeof(cfgtable->driver_version);
 
 	old_driver_ver = kmalloc(2 * size, GFP_KERNEL);
-	if (!old_driver_ver)
-		return -ENOMEM;
+	if (!old_driver_ver ) 
+		return 1; 
 	driver_ver = old_driver_ver + size;
 
 	/* After a reset, the 32 bytes of "driver version" in the cfgtable
@@ -5930,10 +6529,11 @@ static int controller_reset_failed(struct CfgTable __iomem *cfgtable)
 	kfree(old_driver_ver);
 	return rc;
 }
+
 /* This does a hard reset of the controller using PCI power management
  * states or the using the doorbell register.
  */
-static int hpsa_kdump_hard_reset_controller(struct pci_dev *pdev)
+static __devinit int hpsa_kdump_hard_reset_controller(struct pci_dev *pdev)
 {
 	u64 cfg_offset;
 	u32 cfg_base_addr;
@@ -5995,7 +6595,7 @@ static int hpsa_kdump_hard_reset_controller(struct pci_dev *pdev)
 	}
 	rc = write_driver_ver_to_cfgtable(cfgtable);
 	if (rc)
-		goto unmap_vaddr;
+		return rc;
 
 	/* If reset via doorbell register is supported, use that.
 	 * There are two such methods.  Favor the newest method.
@@ -6007,8 +6607,10 @@ static int hpsa_kdump_hard_reset_controller(struct pci_dev *pdev)
 	} else {
 		use_doorbell = misc_fw_support & MISC_FW_DOORBELL_RESET;
 		if (use_doorbell) {
-			dev_warn(&pdev->dev, "Soft reset not supported. "
-				"Firmware update is required.\n");
+			dev_warn(&pdev->dev, "Controller claims that "
+				"'Bit 2 doorbell reset' is "
+				"supported, but not 'bit 5 doorbell reset'.  "
+				"Firmware update is recommended.\n");
 			rc = -ENOTSUPP; /* try soft reset */
 			goto unmap_cfgtable;
 		}
@@ -6022,9 +6624,31 @@ static int hpsa_kdump_hard_reset_controller(struct pci_dev *pdev)
 	pci_write_config_word(pdev, 4, command_register);
 
 	/* Some devices (notably the HP Smart Array 5i Controller)
-	   need a little pause here */
-	msleep(HPSA_POST_RESET_PAUSE_MSECS);
-
+	 * need a little pause here.  Not all controllers though,
+	 * and for some we will miss the transition to NOT READY
+	 * if we wait here.   As a heuristic, if either doorbell
+	 * reset method is supported, we'll skip this sleep.
+	 */
+	if (!use_doorbell)
+		msleep(HPSA_POST_RESET_PAUSE_MSECS);
+
+	if (!use_doorbell) {
+		/* Wait for board to become not ready, then ready.
+		 * (if we used the doorbell, then we already waited 5 secs
+		 * so the "not ready" state is already gone by so we
+		 * won't catch it.)
+		 */
+		dev_info(&pdev->dev, "Waiting for board to reset.\n");
+		rc = hpsa_wait_for_board_state(pdev, vaddr, BOARD_NOT_READY);
+		if (rc) {
+			dev_warn(&pdev->dev,
+				"failed waiting for board to reset."
+				" Will try soft reset.\n");
+			/* Not expected, but try soft reset later */
+			rc = -ENOTSUPP;
+			goto unmap_cfgtable;
+		}
+	}
 	rc = hpsa_wait_for_board_state(pdev, vaddr, BOARD_READY);
 	if (rc) {
 		dev_warn(&pdev->dev,
@@ -6127,16 +6751,31 @@ static int find_PCI_BAR_index(struct pci_dev *pdev, unsigned long pci_bar_addr)
 	return -1;
 }
 
+static int calculate_nreply_queues(void)
+{
+	/* Make sure reply_queues module param is in bounds */
+	if (reply_queues < 1)
+		return 1;
+	if (reply_queues > MAX_REPLY_QUEUES)
+		return MAX_REPLY_QUEUES;
+	/* Cap reply queues to number of CPUs, more is a waste */
+	if (num_online_cpus() < reply_queues)
+		return num_online_cpus();
+	return reply_queues;
+}
+
 /* If MSI/MSI-X is supported by the kernel we will try to enable it on
  * controllers that are capable. If not, we use IO-APIC mode.
  */
 
-static void hpsa_interrupt_mode(struct ctlr_info *h)
+static void __devinit hpsa_interrupt_mode(struct ctlr_info *h)
 {
 #ifdef CONFIG_PCI_MSI
 	int err, i;
 	struct msix_entry hpsa_msix_entries[MAX_REPLY_QUEUES];
 
+	h->nreply_queues = calculate_nreply_queues();
+
 	for (i = 0; i < MAX_REPLY_QUEUES; i++) {
 		hpsa_msix_entries[i].vector = 0;
 		hpsa_msix_entries[i].entry = i;
@@ -6148,26 +6787,28 @@ static void hpsa_interrupt_mode(struct ctlr_info *h)
 		goto default_int_mode;
 	if (pci_find_capability(h->pdev, PCI_CAP_ID_MSIX)) {
 		dev_info(&h->pdev->dev, "MSIX\n");
+
 		h->msix_vector = MAX_REPLY_QUEUES;
 		if (h->msix_vector > num_online_cpus())
 			h->msix_vector = num_online_cpus();
+
 		err = pci_enable_msix(h->pdev, hpsa_msix_entries,
-				      h->msix_vector);
-		if (err > 0) {
-			dev_warn(&h->pdev->dev, "only %d MSI-X vectors "
-			       "available\n", err);
-			h->msix_vector = err;
-			err = pci_enable_msix(h->pdev, hpsa_msix_entries,
-					      h->msix_vector);
-		}
+					h->nreply_queues);
 		if (!err) {
-			for (i = 0; i < h->msix_vector; i++)
+			for (i = 0; i < h->nreply_queues; i++)
 				h->intr[i] = hpsa_msix_entries[i].vector;
+			h->msix_vector = 1;
+			dev_warn(&h->pdev->dev, "Using %d reply queue(s)\n",
+					h->nreply_queues);
 			return;
+		}
+		if (err > 0) {
+			dev_warn(&h->pdev->dev, "only %d MSI-X vectors "
+			       "available\n", err);
+			goto default_int_mode;
 		} else {
 			dev_warn(&h->pdev->dev, "MSI-X init failed %d\n",
 			       err);
-			h->msix_vector = 0;
 			goto default_int_mode;
 		}
 	}
@@ -6179,12 +6820,14 @@ static void hpsa_interrupt_mode(struct ctlr_info *h)
 			dev_warn(&h->pdev->dev, "MSI init failed\n");
 	}
 default_int_mode:
+	h->nreply_queues = 1;
 #endif				/* CONFIG_PCI_MSI */
 	/* if we get here we're going to use the default interrupt mode */
+	dev_info(&h->pdev->dev, "Using %d reply queue(s)\n", h->nreply_queues);
 	h->intr[h->intr_mode] = h->pdev->irq;
 }
 
-static int hpsa_lookup_board_id(struct pci_dev *pdev, u32 *board_id)
+static int __devinit hpsa_lookup_board_id(struct pci_dev *pdev, u32 *board_id)
 {
 	int i;
 	u32 subsystem_vendor_id, subsystem_device_id;
@@ -6203,13 +6846,13 @@ static int hpsa_lookup_board_id(struct pci_dev *pdev, u32 *board_id)
 		!hpsa_allow_any) {
 		dev_warn(&pdev->dev, "unrecognized board ID: "
 			"0x%08x, ignoring.\n", *board_id);
-			return -ENODEV;
+		return -ENODEV;
 	}
 	return ARRAY_SIZE(products) - 1; /* generic unknown smart array */
 }
 
-static int hpsa_pci_find_memory_BAR(struct pci_dev *pdev,
-				    unsigned long *memory_bar)
+static int __devinit hpsa_pci_find_memory_BAR(struct pci_dev *pdev,
+	unsigned long *memory_bar)
 {
 	int i;
 
@@ -6225,34 +6868,9 @@ static int hpsa_pci_find_memory_BAR(struct pci_dev *pdev,
 	return -ENODEV;
 }
 
-static int hpsa_wait_for_board_state(struct pci_dev *pdev, void __iomem *vaddr,
-				     int wait_for_ready)
-{
-	int i, iterations;
-	u32 scratchpad;
-	if (wait_for_ready)
-		iterations = HPSA_BOARD_READY_ITERATIONS;
-	else
-		iterations = HPSA_BOARD_NOT_READY_ITERATIONS;
-
-	for (i = 0; i < iterations; i++) {
-		scratchpad = readl(vaddr + SA5_SCRATCHPAD_OFFSET);
-		if (wait_for_ready) {
-			if (scratchpad == HPSA_FIRMWARE_READY)
-				return 0;
-		} else {
-			if (scratchpad != HPSA_FIRMWARE_READY)
-				return 0;
-		}
-		msleep(HPSA_BOARD_READY_POLL_INTERVAL_MSECS);
-	}
-	dev_warn(&pdev->dev, "board not ready, timed out.\n");
-	return -ENODEV;
-}
-
-static int hpsa_find_cfg_addrs(struct pci_dev *pdev, void __iomem *vaddr,
-			       u32 *cfg_base_addr, u64 *cfg_base_addr_index,
-			       u64 *cfg_offset)
+static int hpsa_find_cfg_addrs(struct pci_dev *pdev,
+	void __iomem *vaddr, u32 *cfg_base_addr, u64 *cfg_base_addr_index,
+	u64 *cfg_offset)
 {
 	*cfg_base_addr = readl(vaddr + SA5_CTCFG_OFFSET);
 	*cfg_offset = readl(vaddr + SA5_CTMEM_OFFSET);
@@ -6265,7 +6883,7 @@ static int hpsa_find_cfg_addrs(struct pci_dev *pdev, void __iomem *vaddr,
 	return 0;
 }
 
-static int hpsa_find_cfgtables(struct ctlr_info *h)
+static int __devinit hpsa_find_cfgtables(struct ctlr_info *h)
 {
 	u64 cfg_offset;
 	u32 cfg_base_addr;
@@ -6283,7 +6901,7 @@ static int hpsa_find_cfgtables(struct ctlr_info *h)
 		return -ENOMEM;
 	rc = write_driver_ver_to_cfgtable(h->cfgtable);
 	if (rc)
-		return rc;
+		return -ENOMEM;
 	/* Find performant mode table. */
 	trans_offset = readl(&h->cfgtable->TransMethodOffset);
 	h->transtable = remap_pci_mem(pci_resource_start(h->pdev,
@@ -6294,7 +6912,7 @@ static int hpsa_find_cfgtables(struct ctlr_info *h)
 	return 0;
 }
 
-static void hpsa_get_max_perf_mode_cmds(struct ctlr_info *h)
+static void __devinit hpsa_get_max_perf_mode_cmds(struct ctlr_info *h)
 {
 	h->max_commands = readl(&(h->cfgtable->MaxPerformantModeCommands));
 
@@ -6315,10 +6933,10 @@ static void hpsa_get_max_perf_mode_cmds(struct ctlr_info *h)
  * max commands, max SG elements without chaining, and with chaining,
  * SG chain block size, etc.
  */
-static void hpsa_find_board_params(struct ctlr_info *h)
+static void __devinit hpsa_find_board_params(struct ctlr_info *h)
 {
 	hpsa_get_max_perf_mode_cmds(h);
-	h->nr_cmds = h->max_commands - 4; /* Allow room for some ioctls */
+	h->nr_cmds = h->max_commands;
 	h->maxsgentries = readl(&(h->cfgtable->MaxScatterGatherElements));
 	h->fw_support = readl(&(h->cfgtable->misc_fw_support));
 	/*
@@ -6337,32 +6955,34 @@ static void hpsa_find_board_params(struct ctlr_info *h)
 
 	/* Find out what task management functions are supported and cache */
 	h->TMFSupportFlags = readl(&(h->cfgtable->TMFSupportFlags));
-	if (!(HPSATMF_PHYS_TASK_ABORT & h->TMFSupportFlags))
-		dev_warn(&h->pdev->dev, "Physical aborts not supported\n");
-	if (!(HPSATMF_LOG_TASK_ABORT & h->TMFSupportFlags))
-		dev_warn(&h->pdev->dev, "Logical aborts not supported\n");
+	printk(KERN_WARNING "Physical aborts supported: %s\n",
+		(HPSATMF_PHYS_TASK_ABORT & h->TMFSupportFlags) ? "yes" : "no");
+	printk(KERN_WARNING "Logical aborts supported: %s\n",
+		(HPSATMF_LOG_TASK_ABORT & h->TMFSupportFlags) ? "yes" : "no");
 }
 
-static inline bool hpsa_CISS_signature_present(struct ctlr_info *h)
+static inline int hpsa_CISS_signature_present(struct ctlr_info *h)
 {
-	if (!check_signature(h->cfgtable->Signature, "CISS", 4)) {
+	if ((readb(&h->cfgtable->Signature[0]) != 'C') ||
+	    (readb(&h->cfgtable->Signature[1]) != 'I') ||
+	    (readb(&h->cfgtable->Signature[2]) != 'S') ||
+	    (readb(&h->cfgtable->Signature[3]) != 'S')) {
 		dev_warn(&h->pdev->dev, "not a valid CISS config table\n");
-		return false;
+		return 0;
 	}
-	return true;
+	return 1;
 }
 
-static inline void hpsa_set_driver_support_bits(struct ctlr_info *h)
+/* Need to enable prefetch in the SCSI core for 6400 in x86 */
+static inline void hpsa_enable_scsi_prefetch(struct ctlr_info *h)
 {
-	u32 driver_support;
-
 #ifdef CONFIG_X86
-	/* Need to enable prefetch in the SCSI core for 6400 in x86 */
-	driver_support = readl(&(h->cfgtable->driver_support));
-	driver_support |= ENABLE_SCSI_PREFETCH;
+	u32 prefetch;
+
+	prefetch = readl(&(h->cfgtable->driver_support));
+	prefetch |= ENABLE_SCSI_PREFETCH;
+	writel(prefetch, &(h->cfgtable->driver_support));
 #endif
-	driver_support |= ENABLE_UNIT_ATTN;
-	writel(driver_support, &(h->cfgtable->driver_support));
 }
 
 /* Disable DMA prefetch for the P600.  Otherwise an ASIC bug may result
@@ -6392,7 +7012,7 @@ static void hpsa_wait_for_clear_event_notify_ack(struct ctlr_info *h)
 		if (!(doorbell_value & DOORBELL_CLEAR_EVENTS))
 			break;
 		/* delay and try again */
-		msleep(20);
+		msleep(10);
 	}
 }
 
@@ -6413,11 +7033,11 @@ static void hpsa_wait_for_mode_change_ack(struct ctlr_info *h)
 		if (!(doorbell_value & CFGTBL_ChangeReq))
 			break;
 		/* delay and try again */
-		usleep_range(10000, 20000);
+		msleep(10);
 	}
 }
 
-static int hpsa_enter_simple_mode(struct ctlr_info *h)
+static int __devinit hpsa_enter_simple_mode(struct ctlr_info *h)
 {
 	u32 trans_support;
 
@@ -6442,18 +7062,21 @@ error:
 	return -ENODEV;
 }
 
-static int hpsa_pci_init(struct ctlr_info *h)
+static void __devexit hpsa_remove_one(struct pci_dev *pdev);
+
+static int __devinit hpsa_pci_init(struct ctlr_info *h)
 {
 	int prod_index, err;
 
 	prod_index = hpsa_lookup_board_id(h->pdev, &h->board_id);
 	if (prod_index < 0)
 		return -ENODEV;
+
 	h->product_name = products[prod_index].product_name;
 	h->access = *(products[prod_index].access);
 
-	pci_disable_link_state(h->pdev, PCIE_LINK_STATE_L0S |
-			       PCIE_LINK_STATE_L1 | PCIE_LINK_STATE_CLKPM);
+	h->needs_abort_tags_swizzled =
+		ctlr_needs_abort_tags_swizzled(h->board_id);
 
 	err = pci_enable_device(h->pdev);
 	if (err) {
@@ -6470,7 +7093,9 @@ static int hpsa_pci_init(struct ctlr_info *h)
 			"cannot obtain PCI resources, aborting\n");
 		return err;
 	}
+
 	hpsa_interrupt_mode(h);
+
 	err = hpsa_pci_find_memory_BAR(h->pdev, &h->paddr);
 	if (err)
 		goto err_out_free_res;
@@ -6491,7 +7116,7 @@ static int hpsa_pci_init(struct ctlr_info *h)
 		err = -ENODEV;
 		goto err_out_free_res;
 	}
-	hpsa_set_driver_support_bits(h);
+	hpsa_enable_scsi_prefetch(h);
 	hpsa_p600_dma_prefetch_quirk(h);
 	err = hpsa_enter_simple_mode(h);
 	if (err)
@@ -6526,7 +7151,7 @@ static void hpsa_hba_inquiry(struct ctlr_info *h)
 	}
 }
 
-static int hpsa_init_reset_devices(struct pci_dev *pdev)
+static __devinit int hpsa_init_reset_devices(struct pci_dev *pdev)
 {
 	int rc, i;
 
@@ -6580,7 +7205,7 @@ out_disable:
 	return rc;
 }
 
-static int hpsa_allocate_cmd_pool(struct ctlr_info *h)
+static __devinit int hpsa_allocate_cmd_pool(struct ctlr_info *h)
 {
 	h->cmd_pool_bits = kzalloc(
 		DIV_ROUND_UP(h->nr_cmds, BITS_PER_LONG) *
@@ -6634,10 +7259,10 @@ static void hpsa_irq_affinity_hints(struct ctlr_info *h)
 }
 
 static int hpsa_request_irq(struct ctlr_info *h,
-	irqreturn_t (*msixhandler)(int, void *),
-	irqreturn_t (*intxhandler)(int, void *))
+	INTERRUPT_HANDLER_TYPE(msixhandler),
+	INTERRUPT_HANDLER_TYPE(intxhandler))
 {
-	int rc, i;
+	int rc = -1, i;
 
 	/*
 	 * initialize h->q[x] = x so that interrupt handlers know which
@@ -6646,16 +7271,16 @@ static int hpsa_request_irq(struct ctlr_info *h,
 	for (i = 0; i < MAX_REPLY_QUEUES; i++)
 		h->q[i] = (u8) i;
 
-	if (h->intr_mode == PERF_MODE_INT && h->msix_vector > 0) {
+	if (h->intr_mode == PERF_MODE_INT && h->msix_vector) {
 		/* If performant mode and MSI-X, use multiple reply queues */
-		for (i = 0; i < h->msix_vector; i++)
+		for (i = 0; i < h->nreply_queues; i++)
 			rc = request_irq(h->intr[i], msixhandler,
 					0, h->devname,
 					&h->q[i]);
 		hpsa_irq_affinity_hints(h);
 	} else {
 		/* Use single reply pool */
-		if (h->msix_vector > 0 || h->msi_vector) {
+		if (h->msix_vector || h->msi_vector) {
 			rc = request_irq(h->intr[h->intr_mode],
 				msixhandler, 0, h->devname,
 				&h->q[h->intr_mode]);
@@ -6709,7 +7334,7 @@ static void free_irqs(struct ctlr_info *h)
 		return;
 	}
 
-	for (i = 0; i < h->msix_vector; i++) {
+	for (i = 0; i < h->nreply_queues; i++) {
 		irq_set_affinity_hint(h->intr[i], NULL);
 		free_irq(h->intr[i], &h->q[i]);
 	}
@@ -6720,13 +7345,15 @@ static void hpsa_free_irqs_and_disable_msix(struct ctlr_info *h)
 	free_irqs(h);
 #ifdef CONFIG_PCI_MSI
 	if (h->msix_vector) {
-		if (h->pdev->msix_enabled)
+		if (h->pdev->msix_enabled) {
 			pci_disable_msix(h->pdev);
+		}
 	} else if (h->msi_vector) {
-		if (h->pdev->msi_enabled)
+		if (h->pdev->msi_enabled) {
 			pci_disable_msi(h->pdev);
+		}
 	}
-#endif /* CONFIG_PCI_MSI */
+#endif				/* CONFIG_PCI_MSI */
 }
 
 static void hpsa_free_reply_queues(struct ctlr_info *h)
@@ -6747,6 +7374,7 @@ static void hpsa_undo_allocations_after_kdump_soft_reset(struct ctlr_info *h)
 {
 	hpsa_free_irqs_and_disable_msix(h);
 	hpsa_free_sg_chain_blocks(h);
+	hpsa_free_ioaccel2_sg_chain_blocks(h);
 	hpsa_free_cmd_pool(h);
 	kfree(h->ioaccel1_blockFetchTable);
 	kfree(h->blockFetchTable);
@@ -6763,16 +7391,21 @@ static void hpsa_undo_allocations_after_kdump_soft_reset(struct ctlr_info *h)
 }
 
 /* Called when controller lockup detected. */
-static void fail_all_cmds_on_list(struct ctlr_info *h, struct list_head *list)
+static void fail_all_outstanding_cmds(struct ctlr_info *h)
 {
-	struct CommandList *c = NULL;
+	int i, refcount;
+	struct CommandList *c;
 
-	assert_spin_locked(&h->lock);
-	/* Mark all outstanding commands as failed and complete them. */
-	while (!list_empty(list)) {
-		c = list_entry(list->next, struct CommandList, list);
-		c->err_info->CommandStatus = CMD_HARDWARE_ERR;
-		finish_cmd(c);
+	flush_workqueue(h->resubmit_wq); /* ensure all cmds are fully built */
+	for (i = 0; i < h->nr_cmds; i++) {
+		c = h->cmd_pool + i;
+		refcount = atomic_inc_return(&c->refcount);
+		if (refcount > 1) {
+			c->err_info->CommandStatus = CMD_HARDWARE_ERR;
+			finish_cmd(c);
+			atomic_dec(&h->commands_outstanding);
+		}
+		cmd_free(h, c);
 	}
 }
 
@@ -6809,10 +7442,7 @@ static void controller_lockup_detected(struct ctlr_info *h)
 	dev_warn(&h->pdev->dev, "Controller lockup detected: 0x%08x\n",
 			lockup_detected);
 	pci_disable_device(h->pdev);
-	spin_lock_irqsave(&h->lock, flags);
-	fail_all_cmds_on_list(h, &h->cmpQ);
-	fail_all_cmds_on_list(h, &h->reqQ);
-	spin_unlock_irqrestore(&h->lock, flags);
+	fail_all_outstanding_cmds(h);
 }
 
 static void detect_controller_lockup(struct ctlr_info *h)
@@ -6821,6 +7451,9 @@ static void detect_controller_lockup(struct ctlr_info *h)
 	u32 heartbeat;
 	unsigned long flags;
 
+	if (!h->lockup_detector_enabled)
+		return;
+
 	now = get_jiffies_64();
 	/* If we've received an interrupt recently, we're ok. */
 	if (time_after64(h->last_intr_timestamp +
@@ -6855,25 +7488,22 @@ static void hpsa_ack_ctlr_events(struct ctlr_info *h)
 	int i;
 	char *event_type;
 
-	/* Clear the driver-requested rescan flag */
-	h->drv_req_rescan = 0;
-
 	/* Ask the controller to clear the events we're handling. */
 	if ((h->transMethod & (CFGTBL_Trans_io_accel1
 			| CFGTBL_Trans_io_accel2)) &&
 		(h->events & HPSA_EVENT_NOTIFY_ACCEL_IO_PATH_STATE_CHANGE ||
 		 h->events & HPSA_EVENT_NOTIFY_ACCEL_IO_PATH_CONFIG_CHANGE)) {
 
-		if (h->events & HPSA_EVENT_NOTIFY_ACCEL_IO_PATH_STATE_CHANGE)
+		if (h->events & HPSA_EVENT_NOTIFY_ACCEL_IO_PATH_STATE_CHANGE )
 			event_type = "state change";
-		if (h->events & HPSA_EVENT_NOTIFY_ACCEL_IO_PATH_CONFIG_CHANGE)
+		if (h->events & HPSA_EVENT_NOTIFY_ACCEL_IO_PATH_CONFIG_CHANGE )
 			event_type = "configuration change";
 		/* Stop sending new RAID offload reqs via the IO accelerator */
 		scsi_block_requests(h->scsi_host);
 		for (i = 0; i < h->ndevices; i++)
-			h->dev[i]->offload_enabled = 0;
+			h->dev[i]->offload_enabled = 0; 
 		hpsa_drain_accel_commands(h);
-		/* Set 'accelerator path config change' bit */
+		/* Set 'accelerator path config change' bit in clear_event_notify field */
 		dev_warn(&h->pdev->dev,
 			"Acknowledging event: 0x%08x (HP SSD Smart Path %s)\n",
 			h->events, event_type);
@@ -6898,13 +7528,15 @@ static void hpsa_ack_ctlr_events(struct ctlr_info *h)
 
 /* Check a register on the controller to see if there are configuration
  * changes (added/changed/removed logical drives, etc.) which mean that
- * we should rescan the controller for devices.
- * Also check flag for driver-initiated rescan.
+ * we should rescan the controller for devices.  
+ * Also check flag for driver-initiated rescan. 
  */
 static int hpsa_ctlr_needs_rescan(struct ctlr_info *h)
 {
-	if (h->drv_req_rescan)
+	if (h->drv_req_rescan) {
+		h->drv_req_rescan = 0;
 		return 1;
+	}
 
 	if (!(h->fw_support & MISC_FW_EVENT_NOTIFY))
 		return 0;
@@ -6927,31 +7559,47 @@ static int hpsa_offline_devices_ready(struct ctlr_info *h)
 		d = list_entry(this, struct offline_device_entry,
 				offline_list);
 		spin_unlock_irqrestore(&h->offline_device_lock, flags);
-		if (!hpsa_volume_offline(h, d->scsi3addr))
+		if (!hpsa_volume_offline(h, d->scsi3addr)) {
+			spin_lock_irqsave(&h->offline_device_lock, flags);
+			list_del(&d->offline_list);
+			spin_unlock_irqrestore(&h->offline_device_lock, flags);
 			return 1;
+		}
 		spin_lock_irqsave(&h->offline_device_lock, flags);
 	}
 	spin_unlock_irqrestore(&h->offline_device_lock, flags);
 	return 0;
 }
 
-
-static void hpsa_monitor_ctlr_worker(struct work_struct *work)
+static void hpsa_rescan_ctlr_worker(struct work_struct *work)
 {
 	unsigned long flags;
 	struct ctlr_info *h = container_of(to_delayed_work(work),
-					struct ctlr_info, monitor_ctlr_work);
-	detect_controller_lockup(h);
-	if (lockup_detected(h))
-		return;
+					struct ctlr_info, rescan_ctlr_work);
 
 	if (hpsa_ctlr_needs_rescan(h) || hpsa_offline_devices_ready(h)) {
 		scsi_host_get(h->scsi_host);
-		h->drv_req_rescan = 0;
 		hpsa_ack_ctlr_events(h);
 		hpsa_scan_start(h->scsi_host);
 		scsi_host_put(h->scsi_host);
 	}
+	spin_lock_irqsave(&h->lock, flags);
+	if (h->remove_in_progress)
+		cancel_delayed_work(&h->rescan_ctlr_work);
+	else
+		schedule_delayed_work(&h->rescan_ctlr_work,
+				h->heartbeat_sample_interval);
+	spin_unlock_irqrestore(&h->lock, flags);
+}
+
+static void hpsa_monitor_ctlr_worker(struct work_struct *work)
+{
+	unsigned long flags;
+	struct ctlr_info *h = container_of(to_delayed_work(work),
+					struct ctlr_info, monitor_ctlr_work);
+	detect_controller_lockup(h);
+	if (lockup_detected(h))
+		return;
 
 	spin_lock_irqsave(&h->lock, flags);
 	if (h->remove_in_progress) {
@@ -6963,7 +7611,8 @@ static void hpsa_monitor_ctlr_worker(struct work_struct *work)
 	spin_unlock_irqrestore(&h->lock, flags);
 }
 
-static int hpsa_init_one(struct pci_dev *pdev, const struct pci_device_id *ent)
+static int __devinit hpsa_init_one(struct pci_dev *pdev,
+				    const struct pci_device_id *ent)
 {
 	int dac, rc;
 	struct ctlr_info *h;
@@ -6999,18 +7648,24 @@ reinit_after_soft_reset:
 
 	h->pdev = pdev;
 	h->intr_mode = hpsa_simple_mode ? SIMPLE_MODE_INT : PERF_MODE_INT;
-	INIT_LIST_HEAD(&h->cmpQ);
-	INIT_LIST_HEAD(&h->reqQ);
 	INIT_LIST_HEAD(&h->offline_device_list);
 	spin_lock_init(&h->lock);
-	spin_lock_init(&h->offline_device_lock);
 	spin_lock_init(&h->scan_lock);
-	spin_lock_init(&h->passthru_count_lock);
+	spin_lock_init(&h->offline_device_lock);
+	atomic_set(&h->passthru_cmds_avail, HPSA_MAX_CONCURRENT_PASSTHRUS);
+	atomic_set(&h->abort_cmds_available, HPSA_CMDS_RESERVED_FOR_ABORTS);
 
+	h->resubmit_wq = alloc_workqueue("hpsa", WQ_MEM_RECLAIM, 0);
+	if (!h->resubmit_wq) {
+		dev_warn(&h->pdev->dev, "Failed to allocate work queue\n");
+		goto clean1;
+	}
 	/* Allocate and clear per-cpu variable lockup_detected */
 	h->lockup_detected = alloc_percpu(u32);
-	if (!h->lockup_detected)
+	if (!h->lockup_detected) {
+		rc = -ENOMEM;
 		goto clean1;
+	}
 	set_lockup_detected_for_all_cpus(h, 0);
 
 	rc = hpsa_pci_init(h);
@@ -7048,6 +7703,7 @@ reinit_after_soft_reset:
 	if (hpsa_allocate_sg_chain_blocks(h))
 		goto clean4;
 	init_waitqueue_head(&h->scan_wait_queue);
+	init_waitqueue_head(&h->abort_cmd_wait_queue);
 	h->scan_finished = 1; /* no scan currently in progress */
 
 	pci_set_drvdata(pdev, h);
@@ -7072,6 +7728,7 @@ reinit_after_soft_reset:
 		 */
 		spin_lock_irqsave(&h->lock, flags);
 		h->access.set_intr_mask(h, HPSA_INTR_OFF);
+		cancel_delayed_work(&h->rescan_ctlr_work);
 		spin_unlock_irqrestore(&h->lock, flags);
 		free_irqs(h);
 		rc = hpsa_request_irq(h, hpsa_msix_discard_completions,
@@ -7115,19 +7772,23 @@ reinit_after_soft_reset:
 		/* Enable Accelerated IO path at driver layer */
 		h->acciopath_status = 1;
 
-	h->drv_req_rescan = 0;
+	h->lockup_detector_enabled = 1;
 
 	/* Turn the interrupts on so we can service requests */
 	h->access.set_intr_mask(h, HPSA_INTR_ON);
 
 	hpsa_hba_inquiry(h);
-	hpsa_register_scsi(h);	/* hook ourselves into SCSI subsystem */
+	if (hpsa_register_scsi(h)) /* hook ourselves into SCSI subsystem */
+		goto clean4;
 
 	/* Monitor the controller for firmware lockups */
 	h->heartbeat_sample_interval = HEARTBEAT_SAMPLE_INTERVAL;
 	INIT_DELAYED_WORK(&h->monitor_ctlr_work, hpsa_monitor_ctlr_worker);
 	schedule_delayed_work(&h->monitor_ctlr_work,
-				h->heartbeat_sample_interval);
+			h->heartbeat_sample_interval);
+	INIT_DELAYED_WORK(&h->rescan_ctlr_work, hpsa_rescan_ctlr_worker);
+	schedule_delayed_work(&h->rescan_ctlr_work,
+			h->heartbeat_sample_interval);
 	return 0;
 
 clean4:
@@ -7136,16 +7797,20 @@ clean4:
 	free_irqs(h);
 clean2:
 clean1:
+	if (h->resubmit_wq)
+		destroy_workqueue(h->resubmit_wq);
 	if (h->lockup_detected)
 		free_percpu(h->lockup_detected);
 	kfree(h);
 	return rc;
 }
+EXPORT_SYMBOL(hpsa_init_one);
 
 static void hpsa_flush_cache(struct ctlr_info *h)
 {
 	char *flush_buf;
 	struct CommandList *c;
+	int rc;
 
 	/* Don't bother trying to flush the cache if locked up */
 	if (unlikely(lockup_detected(h)))
@@ -7154,21 +7819,24 @@ static void hpsa_flush_cache(struct ctlr_info *h)
 	if (!flush_buf)
 		return;
 
-	c = cmd_special_alloc(h);
+	c = cmd_alloc(h);
 	if (!c) {
-		dev_warn(&h->pdev->dev, "cmd_special_alloc returned NULL!\n");
+		dev_warn(&h->pdev->dev, "cmd_alloc returned NULL!\n");
 		goto out_of_memory;
 	}
 	if (fill_cmd(c, HPSA_CACHE_FLUSH, h, flush_buf, 4, 0,
 		RAID_CTLR_LUNID, TYPE_CMD)) {
 		goto out;
 	}
-	hpsa_scsi_do_simple_cmd_with_retry(h, c, PCI_DMA_TODEVICE);
+	rc = hpsa_scsi_do_simple_cmd_with_retry(h, c,
+					PCI_DMA_TODEVICE, NO_TIMEOUT);
+	if (rc)
+		goto out;
 	if (c->err_info->CommandStatus != 0)
 out:
 		dev_warn(&h->pdev->dev,
 			"error flushing cache on controller\n");
-	cmd_special_free(h, c);
+	cmd_free(h, c);
 out_of_memory:
 	kfree(flush_buf);
 }
@@ -7187,7 +7855,7 @@ static void hpsa_shutdown(struct pci_dev *pdev)
 	hpsa_free_irqs_and_disable_msix(h);
 }
 
-static void hpsa_free_device_info(struct ctlr_info *h)
+static void __devexit hpsa_free_device_info(struct ctlr_info *h)
 {
 	int i;
 
@@ -7195,7 +7863,7 @@ static void hpsa_free_device_info(struct ctlr_info *h)
 		kfree(h->dev[i]);
 }
 
-static void hpsa_remove_one(struct pci_dev *pdev)
+static void __devexit hpsa_remove_one(struct pci_dev *pdev)
 {
 	struct ctlr_info *h;
 	unsigned long flags;
@@ -7210,29 +7878,26 @@ static void hpsa_remove_one(struct pci_dev *pdev)
 	spin_lock_irqsave(&h->lock, flags);
 	h->remove_in_progress = 1;
 	cancel_delayed_work(&h->monitor_ctlr_work);
+	cancel_delayed_work(&h->rescan_ctlr_work);
 	spin_unlock_irqrestore(&h->lock, flags);
-
 	hpsa_unregister_scsi(h);	/* unhook from SCSI subsystem */
 	hpsa_shutdown(pdev);
+	destroy_workqueue(h->resubmit_wq);
 	iounmap(h->vaddr);
 	iounmap(h->transtable);
 	iounmap(h->cfgtable);
 	hpsa_free_device_info(h);
 	hpsa_free_sg_chain_blocks(h);
-	pci_free_consistent(h->pdev,
-		h->nr_cmds * sizeof(struct CommandList),
-		h->cmd_pool, h->cmd_pool_dhandle);
-	pci_free_consistent(h->pdev,
-		h->nr_cmds * sizeof(struct ErrorInfo),
-		h->errinfo_pool, h->errinfo_pool_dhandle);
+	hpsa_free_ioaccel2_sg_chain_blocks(h);
+	hpsa_free_cmd_pool(h);
+	if (h->intr_mode != SIMPLE_MODE_INT)
 	hpsa_free_reply_queues(h);
-	kfree(h->cmd_pool_bits);
 	kfree(h->blockFetchTable);
 	kfree(h->ioaccel1_blockFetchTable);
 	kfree(h->ioaccel2_blockFetchTable);
 	kfree(h->hba_inquiry_data);
-	pci_disable_device(pdev);
 	pci_release_regions(pdev);
+	pci_disable_device(h->pdev);
 	pci_set_drvdata(pdev, NULL);
 	free_percpu(h->lockup_detected);
 	kfree(h);
@@ -7252,7 +7917,7 @@ static int hpsa_resume(__attribute__((unused)) struct pci_dev *pdev)
 static struct pci_driver hpsa_pci_driver = {
 	.name = HPSA,
 	.probe = hpsa_init_one,
-	.remove = hpsa_remove_one,
+	.remove = __devexit_p(hpsa_remove_one),
 	.id_table = hpsa_pci_device_id,	/* id_table */
 	.shutdown = hpsa_shutdown,
 	.suspend = hpsa_suspend,
@@ -7293,7 +7958,8 @@ static void  calc_bucket_map(int bucket[], int num_buckets,
 	}
 }
 
-static void hpsa_enter_performant_mode(struct ctlr_info *h, u32 trans_support)
+static void hpsa_enter_performant_mode(struct ctlr_info *h,
+	u32 trans_support)
 {
 	int i;
 	unsigned long register_value;
@@ -7324,8 +7990,8 @@ static void hpsa_enter_performant_mode(struct ctlr_info *h, u32 trans_support)
 	int bft[8] = {5, 6, 8, 10, 12, 20, 28, SG_ENTRIES_IN_CMD + 4};
 #define MIN_IOACCEL2_BFT_ENTRY 5
 #define HPSA_IOACCEL2_HEADER_SZ 4
-	int bft2[16] = {MIN_IOACCEL2_BFT_ENTRY, 6, 7, 8, 9, 10, 11, 12,
-			13, 14, 15, 16, 17, 18, 19,
+	int bft2[16] = {MIN_IOACCEL2_BFT_ENTRY, 6, 7, 8, 9, 10, 11, 12, 
+			13, 14, 15, 16, 17, 18, 19, 
 			HPSA_IOACCEL2_HEADER_SZ + IOACCEL2_MAXSGENTRIES};
 	BUILD_BUG_ON(ARRAY_SIZE(bft2) != 16);
 	BUILD_BUG_ON(ARRAY_SIZE(bft) != 8);
@@ -7341,7 +8007,7 @@ static void hpsa_enter_performant_mode(struct ctlr_info *h, u32 trans_support)
 
 	/* If the controller supports either ioaccel method then
 	 * we can also use the RAID stack submit path that does not
-	 * perform the superfluous readl() after each command submission.
+	 * the superfluous readl() after each command submission.
 	 */
 	if (trans_support & (CFGTBL_Trans_io_accel1 | CFGTBL_Trans_io_accel2))
 		access = SA5_performant_access_no_read;
@@ -7431,13 +8097,11 @@ static void hpsa_enter_performant_mode(struct ctlr_info *h, u32 trans_support)
 			cp->host_context_flags = IOACCEL1_HCFLAGS_CISS_FORMAT;
 			cp->timeout_sec = 0;
 			cp->ReplyQueue = 0;
-			cp->Tag.lower = (i << DIRECT_LOOKUP_SHIFT) |
-						DIRECT_LOOKUP_BIT;
-			cp->Tag.upper = 0;
-			cp->host_addr.lower =
-				(u32) (h->ioaccel_cmd_pool_dhandle +
-					(i * sizeof(struct io_accel1_cmd)));
-			cp->host_addr.upper = 0;
+			cp->tag =
+				cpu_to_le64((u64) (i << DIRECT_LOOKUP_SHIFT));
+			cp->host_addr =
+				cpu_to_le64((u64) (h->ioaccel_cmd_pool_dhandle +
+					(i * sizeof(struct io_accel1_cmd))));
 		}
 	} else if (trans_support & CFGTBL_Trans_io_accel2) {
 		u64 cfg_offset, cfg_base_addr_index;
@@ -7451,14 +8115,13 @@ static void hpsa_enter_performant_mode(struct ctlr_info *h, u32 trans_support)
 		calc_bucket_map(bft2, ARRAY_SIZE(bft2), h->ioaccel_maxsg,
 				4, h->ioaccel2_blockFetchTable);
 		bft2_offset = readl(&h->cfgtable->io_accel_request_size_offset);
-		BUILD_BUG_ON(offsetof(struct CfgTable,
-				io_accel_request_size_offset) != 0xb8);
+		BUILD_BUG_ON(offsetof(struct CfgTable, io_accel_request_size_offset)
+			!= 0xb8); 
+		printk(KERN_WARNING "bft2_offset = 0x%x\n", bft2_offset);
 		h->ioaccel2_bft2_regs =
-			remap_pci_mem(pci_resource_start(h->pdev,
-					cfg_base_addr_index) +
-					cfg_offset + bft2_offset,
-					ARRAY_SIZE(bft2) *
-					sizeof(*h->ioaccel2_bft2_regs));
+			remap_pci_mem(pci_resource_start(h->pdev, cfg_base_addr_index) + 
+					cfg_offset +
+					bft2_offset, ARRAY_SIZE(bft2) * sizeof(*h->ioaccel2_bft2_regs));
 		for (i = 0; i < ARRAY_SIZE(bft2); i++)
 			writel(bft2[i], &h->ioaccel2_bft2_regs[i]);
 	}
@@ -7479,6 +8142,7 @@ static int hpsa_alloc_ioaccel_cmd_and_bft(struct ctlr_info *h)
 	 */
 	BUILD_BUG_ON(sizeof(struct io_accel1_cmd) %
 			IOACCEL1_COMMANDLIST_ALIGNMENT);
+
 	h->ioaccel_cmd_pool =
 		pci_alloc_consistent(h->pdev,
 			h->nr_cmds * sizeof(*h->ioaccel_cmd_pool),
@@ -7529,6 +8193,9 @@ static int ioaccel2_alloc_cmds_and_bft(struct ctlr_info *h)
 		(h->ioaccel2_blockFetchTable == NULL))
 		goto clean_up;
 
+	if (hpsa_allocate_ioaccel2_sg_chain_blocks(h))
+		goto clean_up;
+
 	memset(h->ioaccel2_cmd_pool, 0,
 		h->nr_cmds * sizeof(*h->ioaccel2_cmd_pool));
 	return 0;
@@ -7542,14 +8209,14 @@ clean_up:
 	return 1;
 }
 
-static void hpsa_put_ctlr_into_performant_mode(struct ctlr_info *h)
+static __devinit void hpsa_put_ctlr_into_performant_mode(struct ctlr_info *h)
 {
 	u32 trans_support;
 	unsigned long transMethod = CFGTBL_Trans_Performant |
 					CFGTBL_Trans_use_short_tags;
 	int i;
 
-	if (hpsa_simple_mode)
+	if (h->intr_mode == SIMPLE_MODE_INT)
 		return;
 
 	trans_support = readl(&(h->cfgtable->TransportSupport));
@@ -7571,7 +8238,6 @@ static void hpsa_put_ctlr_into_performant_mode(struct ctlr_info *h)
 		}
 	}
 
-	h->nreply_queues = h->msix_vector > 0 ? h->msix_vector : 1;
 	hpsa_get_max_perf_mode_cmds(h);
 	/* Performant mode ring buffer and supporting data structures */
 	h->reply_queue_size = h->max_commands * sizeof(u64);
@@ -7603,23 +8269,25 @@ clean_up:
 
 static int is_accelerated_cmd(struct CommandList *c)
 {
-	return c->cmd_type == CMD_IOACCEL1 || c->cmd_type == CMD_IOACCEL2;
+	return (c->cmd_type == CMD_IOACCEL1 ||
+		c->cmd_type == CMD_IOACCEL2);
 }
 
 static void hpsa_drain_accel_commands(struct ctlr_info *h)
 {
 	struct CommandList *c = NULL;
-	unsigned long flags;
-	int accel_cmds_out;
+	int i, accel_cmds_out;
+	int refcount;
 
-	do { /* wait for all outstanding commands to drain out */
+	do { /* wait for all outstanding ioaccel commands to drain out */
 		accel_cmds_out = 0;
-		spin_lock_irqsave(&h->lock, flags);
-		list_for_each_entry(c, &h->cmpQ, list)
-			accel_cmds_out += is_accelerated_cmd(c);
-		list_for_each_entry(c, &h->reqQ, list)
-			accel_cmds_out += is_accelerated_cmd(c);
-		spin_unlock_irqrestore(&h->lock, flags);
+		for (i = 0; i < h->nr_cmds; i++) {
+			c = h->cmd_pool + i;
+			refcount = atomic_inc_return(&c->refcount);
+			if (refcount > 1) /* Command is allocated */
+				accel_cmds_out += is_accelerated_cmd(c);
+			cmd_free(h, c);
+		}
 		if (accel_cmds_out <= 0)
 			break;
 		msleep(100);
@@ -7640,83 +8308,5 @@ static void __exit hpsa_cleanup(void)
 	pci_unregister_driver(&hpsa_pci_driver);
 }
 
-static void __attribute__((unused)) verify_offsets(void)
-{
-#define VERIFY_OFFSET(member, offset) \
-	BUILD_BUG_ON(offsetof(struct raid_map_data, member) != offset)
-
-	VERIFY_OFFSET(structure_size, 0);
-	VERIFY_OFFSET(volume_blk_size, 4);
-	VERIFY_OFFSET(volume_blk_cnt, 8);
-	VERIFY_OFFSET(phys_blk_shift, 16);
-	VERIFY_OFFSET(parity_rotation_shift, 17);
-	VERIFY_OFFSET(strip_size, 18);
-	VERIFY_OFFSET(disk_starting_blk, 20);
-	VERIFY_OFFSET(disk_blk_cnt, 28);
-	VERIFY_OFFSET(data_disks_per_row, 36);
-	VERIFY_OFFSET(metadata_disks_per_row, 38);
-	VERIFY_OFFSET(row_cnt, 40);
-	VERIFY_OFFSET(layout_map_count, 42);
-	VERIFY_OFFSET(flags, 44);
-	VERIFY_OFFSET(dekindex, 46);
-	/* VERIFY_OFFSET(reserved, 48 */
-	VERIFY_OFFSET(data, 64);
-
-#undef VERIFY_OFFSET
-
-#define VERIFY_OFFSET(member, offset) \
-	BUILD_BUG_ON(offsetof(struct io_accel2_cmd, member) != offset)
-
-	VERIFY_OFFSET(IU_type, 0);
-	VERIFY_OFFSET(direction, 1);
-	VERIFY_OFFSET(reply_queue, 2);
-	/* VERIFY_OFFSET(reserved1, 3);  */
-	VERIFY_OFFSET(scsi_nexus, 4);
-	VERIFY_OFFSET(Tag, 8);
-	VERIFY_OFFSET(cdb, 16);
-	VERIFY_OFFSET(cciss_lun, 32);
-	VERIFY_OFFSET(data_len, 40);
-	VERIFY_OFFSET(cmd_priority_task_attr, 44);
-	VERIFY_OFFSET(sg_count, 45);
-	/* VERIFY_OFFSET(reserved3 */
-	VERIFY_OFFSET(err_ptr, 48);
-	VERIFY_OFFSET(err_len, 56);
-	/* VERIFY_OFFSET(reserved4  */
-	VERIFY_OFFSET(sg, 64);
-
-#undef VERIFY_OFFSET
-
-#define VERIFY_OFFSET(member, offset) \
-	BUILD_BUG_ON(offsetof(struct io_accel1_cmd, member) != offset)
-
-	VERIFY_OFFSET(dev_handle, 0x00);
-	VERIFY_OFFSET(reserved1, 0x02);
-	VERIFY_OFFSET(function, 0x03);
-	VERIFY_OFFSET(reserved2, 0x04);
-	VERIFY_OFFSET(err_info, 0x0C);
-	VERIFY_OFFSET(reserved3, 0x10);
-	VERIFY_OFFSET(err_info_len, 0x12);
-	VERIFY_OFFSET(reserved4, 0x13);
-	VERIFY_OFFSET(sgl_offset, 0x14);
-	VERIFY_OFFSET(reserved5, 0x15);
-	VERIFY_OFFSET(transfer_len, 0x1C);
-	VERIFY_OFFSET(reserved6, 0x20);
-	VERIFY_OFFSET(io_flags, 0x24);
-	VERIFY_OFFSET(reserved7, 0x26);
-	VERIFY_OFFSET(LUN, 0x34);
-	VERIFY_OFFSET(control, 0x3C);
-	VERIFY_OFFSET(CDB, 0x40);
-	VERIFY_OFFSET(reserved8, 0x50);
-	VERIFY_OFFSET(host_context_flags, 0x60);
-	VERIFY_OFFSET(timeout_sec, 0x62);
-	VERIFY_OFFSET(ReplyQueue, 0x64);
-	VERIFY_OFFSET(reserved9, 0x65);
-	VERIFY_OFFSET(Tag, 0x68);
-	VERIFY_OFFSET(host_addr, 0x70);
-	VERIFY_OFFSET(CISS_LUN, 0x78);
-	VERIFY_OFFSET(SG, 0x78 + 8);
-#undef VERIFY_OFFSET
-}
-
 module_init(hpsa_init);
 module_exit(hpsa_cleanup);
diff --git a/drivers/scsi/hpsa.h b/drivers/scsi/hpsa.h
index 24472cec7de3..3c300c939da1 100644
--- a/drivers/scsi/hpsa.h
+++ b/drivers/scsi/hpsa.h
@@ -1,6 +1,6 @@
 /*
  *    Disk Array driver for HP Smart Array SAS controllers
- *    Copyright 2000, 2014 Hewlett-Packard Development Company, L.P.
+ *    Copyright 2000, 2013 Hewlett-Packard Development Company, L.P.
  *
  *    This program is free software; you can redistribute it and/or modify
  *    it under the terms of the GNU General Public License as published by
@@ -32,8 +32,7 @@ struct access_method {
 	void (*submit_command)(struct ctlr_info *h,
 		struct CommandList *c);
 	void (*set_intr_mask)(struct ctlr_info *h, unsigned long val);
-	unsigned long (*fifo_full)(struct ctlr_info *h);
-	bool (*intr_pending)(struct ctlr_info *h);
+	int (*intr_pending)(struct ctlr_info *h);
 	unsigned long (*command_completed)(struct ctlr_info *h, u8 q);
 };
 
@@ -47,14 +46,27 @@ struct hpsa_scsi_dev_t {
 	unsigned char model[16];        /* bytes 16-31 of inquiry data */
 	unsigned char raid_level;	/* from inquiry page 0xC1 */
 	unsigned char volume_offline;	/* discovered via TUR or VPD */
+	u16 queue_depth;
+	atomic_t ioaccel_cmds_out;
 	u32 ioaccel_handle;
+	u8 active_path_index;
+	u8 path_map;
+	u8 bay;
+	u8 box[8];
+	u16 phys_connector[8];
 	int offload_config;		/* I/O accel RAID offload configured */
 	int offload_enabled;		/* I/O accel RAID offload enabled */
+	int hba_ioaccel_enabled;
 	int offload_to_mirror;		/* Send next I/O accelerator RAID
 					 * offload request to mirror drive
 					 */
 	struct raid_map_data raid_map;	/* I/O accelerator RAID map */
-
+	int supports_aborts;
+#define HPSA_NO_ULD_ATTACH	0x1
+#define HPSA_DO_NOT_EXPOSE	0x2
+#define HPSA_EXPOSE		0x4
+#define HPSA_SCSI_ADD		(HPSA_NO_ULD_ATTACH | HPSA_EXPOSE)
+	u8 expose_state;
 };
 
 struct reply_queue_buffer {
@@ -115,10 +127,13 @@ struct ctlr_info {
 	void __iomem *vaddr;
 	unsigned long paddr;
 	int 	nr_cmds; /* Number of commands allowed on this controller */
+#define HPSA_CMDS_RESERVED_FOR_ABORTS 2
+#define HPSA_CMDS_RESERVED_FOR_DRIVER 1
 	struct CfgTable __iomem *cfgtable;
 	int	interrupts_enabled;
 	int 	max_commands;
-	int	commands_outstanding;
+	int last_allocation;
+	atomic_t commands_outstanding;
 #	define PERF_MODE_INT	0
 #	define DOORBELL_INT	1
 #	define SIMPLE_MODE_INT	2
@@ -131,8 +146,6 @@ struct ctlr_info {
 	char hba_mode_enabled;
 
 	/* queue and queue Info */
-	struct list_head reqQ;
-	struct list_head cmpQ;
 	unsigned int Qdepth;
 	unsigned int maxSG;
 	spinlock_t lock;
@@ -140,6 +153,7 @@ struct ctlr_info {
 	u8 max_cmd_sg_entries;
 	int chainsize;
 	struct SGDescriptor **cmd_sg_list;
+	struct ioaccel2_sg_element **ioaccel2_cmd_sg_list;
 
 	/* pointers to command and error info pool */
 	struct CommandList 	*cmd_pool;
@@ -168,9 +182,8 @@ struct ctlr_info {
 	unsigned long transMethod;
 
 	/* cap concurrent passthrus at some reasonable maximum */
-#define HPSA_MAX_CONCURRENT_PASSTHRUS (20)
-	spinlock_t passthru_count_lock; /* protects passthru_count */
-	int passthru_count;
+#define HPSA_MAX_CONCURRENT_PASSTHRUS (10)
+	atomic_t passthru_cmds_avail;
 
 	/*
 	 * Performant mode completion buffers
@@ -194,7 +207,9 @@ struct ctlr_info {
 	atomic_t firmware_flash_in_progress;
 	u32 *lockup_detected;
 	struct delayed_work monitor_ctlr_work;
+	struct delayed_work rescan_ctlr_work;
 	int remove_in_progress;
+	struct list_head scan_list;
 	u32 fifo_recently_full;
 	/* Address of h->q[x] is passed to intr handler to know which queue */
 	u8 q[MAX_REPLY_QUEUES];
@@ -234,11 +249,17 @@ struct ctlr_info {
 		CTLR_STATE_CHANGE_EVENT_LOGICAL_DRV | \
 		CTLR_STATE_CHANGE_EVENT_AIO_ENABLED_DISABLED | \
 		CTLR_STATE_CHANGE_EVENT_AIO_CONFIG_CHANGE)
+
 	spinlock_t offline_device_lock;
 	struct list_head offline_device_list;
-	int	acciopath_status;
-	int	drv_req_rescan;	/* flag for driver to request rescan event */
-	int	raid_offload_debug;
+	int	raid_offload_debug;	
+	int	acciopath_status;	
+	int 	drv_req_rescan;
+	int	lockup_detector_enabled;
+	int	needs_abort_tags_swizzled;
+	struct workqueue_struct *resubmit_wq;
+	atomic_t abort_cmds_available;
+	wait_queue_head_t abort_cmd_wait_queue;
 };
 
 struct offline_device_entry {
@@ -278,7 +299,7 @@ struct offline_device_entry {
  * HPSA_BOARD_READY_ITERATIONS are derived from those.
  */
 #define HPSA_BOARD_READY_WAIT_SECS (120)
-#define HPSA_BOARD_NOT_READY_WAIT_SECS (100)
+#define HPSA_BOARD_NOT_READY_WAIT_SECS (120)
 #define HPSA_BOARD_READY_POLL_INTERVAL_MSECS (100)
 #define HPSA_BOARD_READY_POLL_INTERVAL \
 	((HPSA_BOARD_READY_POLL_INTERVAL_MSECS * HZ) / 1000)
@@ -353,10 +374,7 @@ static void SA5_submit_command_no_read(struct ctlr_info *h,
 static void SA5_submit_command_ioaccel2(struct ctlr_info *h,
 	struct CommandList *c)
 {
-	if (c->cmd_type == CMD_IOACCEL2)
-		writel(c->busaddr, h->vaddr + IOACCEL2_INBOUND_POSTQ_32);
-	else
-		writel(c->busaddr, h->vaddr + SA5_REQUEST_PORT_OFFSET);
+	writel(c->busaddr, h->vaddr + SA5_REQUEST_PORT_OFFSET);
 }
 
 /*
@@ -395,27 +413,25 @@ static void SA5_performant_intr_mask(struct ctlr_info *h, unsigned long val)
 static unsigned long SA5_performant_completed(struct ctlr_info *h, u8 q)
 {
 	struct reply_queue_buffer *rq = &h->reply_queue[q];
-	unsigned long flags, register_value = FIFO_EMPTY;
+	unsigned long register_value = FIFO_EMPTY;
 
 	/* msi auto clears the interrupt pending bit. */
-	if (!(h->msi_vector || h->msix_vector)) {
+	if (unlikely(!(h->msi_vector || h->msix_vector))) {
 		/* flush the controller write of the reply queue by reading
 		 * outbound doorbell status register.
 		 */
-		register_value = readl(h->vaddr + SA5_OUTDB_STATUS);
+		(void) readl(h->vaddr + SA5_OUTDB_STATUS);
 		writel(SA5_OUTDB_CLEAR_PERF_BIT, h->vaddr + SA5_OUTDB_CLEAR);
 		/* Do a read in order to flush the write to the controller
 		 * (as per spec.)
 		 */
-		register_value = readl(h->vaddr + SA5_OUTDB_STATUS);
+		(void) readl(h->vaddr + SA5_OUTDB_STATUS);
 	}
 
-	if ((rq->head[rq->current_entry] & 1) == rq->wraparound) {
+	if ((((u32) rq->head[rq->current_entry]) & 1) == rq->wraparound) {
 		register_value = rq->head[rq->current_entry];
 		rq->current_entry++;
-		spin_lock_irqsave(&h->lock, flags);
-		h->commands_outstanding--;
-		spin_unlock_irqrestore(&h->lock, flags);
+		atomic_dec(&h->commands_outstanding);
 	} else {
 		register_value = FIFO_EMPTY;
 	}
@@ -428,18 +444,6 @@ static unsigned long SA5_performant_completed(struct ctlr_info *h, u8 q)
 }
 
 /*
- *  Returns true if fifo is full.
- *
- */
-static unsigned long SA5_fifo_full(struct ctlr_info *h)
-{
-	if (h->commands_outstanding >= h->max_commands)
-		return 1;
-	else
-		return 0;
-
-}
-/*
  *   returns value read from hardware.
  *     returns FIFO_EMPTY if there is nothing to read
  */
@@ -448,13 +452,9 @@ static unsigned long SA5_completed(struct ctlr_info *h,
 {
 	unsigned long register_value
 		= readl(h->vaddr + SA5_REPLY_PORT_OFFSET);
-	unsigned long flags;
 
-	if (register_value != FIFO_EMPTY) {
-		spin_lock_irqsave(&h->lock, flags);
-		h->commands_outstanding--;
-		spin_unlock_irqrestore(&h->lock, flags);
-	}
+	if (register_value != FIFO_EMPTY)
+		atomic_dec(&h->commands_outstanding);
 
 #ifdef HPSA_DEBUG
 	if (register_value != FIFO_EMPTY)
@@ -466,25 +466,23 @@ static unsigned long SA5_completed(struct ctlr_info *h,
 
 	return register_value;
 }
+
 /*
  *	Returns true if an interrupt is pending..
  */
-static bool SA5_intr_pending(struct ctlr_info *h)
+static int SA5_intr_pending(struct ctlr_info *h)
 {
 	unsigned long register_value  =
 		readl(h->vaddr + SA5_INTR_STATUS);
 	return register_value & SA5_INTR_PENDING;
 }
 
-static bool SA5_performant_intr_pending(struct ctlr_info *h)
+static int SA5_performant_intr_pending(struct ctlr_info *h)
 {
 	unsigned long register_value = readl(h->vaddr + SA5_INTR_STATUS);
 
 	if (!register_value)
-		return false;
-
-	if (h->msi_vector || h->msix_vector)
-		return true;
+		return 0;
 
 	/* Read outbound doorbell to flush */
 	register_value = readl(h->vaddr + SA5_OUTDB_STATUS);
@@ -493,7 +491,7 @@ static bool SA5_performant_intr_pending(struct ctlr_info *h)
 
 #define SA5_IOACCEL_MODE1_INTR_STATUS_CMP_BIT    0x100
 
-static bool SA5_ioaccel_mode1_intr_pending(struct ctlr_info *h)
+static int SA5_ioaccel_mode1_intr_pending(struct ctlr_info *h)
 {
 	unsigned long register_value = readl(h->vaddr + SA5_INTR_STATUS);
 
@@ -510,7 +508,6 @@ static unsigned long SA5_ioaccel_mode1_completed(struct ctlr_info *h, u8 q)
 {
 	u64 register_value;
 	struct reply_queue_buffer *rq = &h->reply_queue[q];
-	unsigned long flags;
 
 	BUG_ON(q >= h->nreply_queues);
 
@@ -528,9 +525,7 @@ static unsigned long SA5_ioaccel_mode1_completed(struct ctlr_info *h, u8 q)
 		wmb();
 		writel((q << 24) | rq->current_entry, h->vaddr +
 				IOACCEL_MODE1_CONSUMER_INDEX);
-		spin_lock_irqsave(&h->lock, flags);
-		h->commands_outstanding--;
-		spin_unlock_irqrestore(&h->lock, flags);
+		atomic_dec(&h->commands_outstanding);
 	}
 	return (unsigned long) register_value;
 }
@@ -538,7 +533,6 @@ static unsigned long SA5_ioaccel_mode1_completed(struct ctlr_info *h, u8 q)
 static struct access_method SA5_access = {
 	SA5_submit_command,
 	SA5_intr_mask,
-	SA5_fifo_full,
 	SA5_intr_pending,
 	SA5_completed,
 };
@@ -546,7 +540,6 @@ static struct access_method SA5_access = {
 static struct access_method SA5_ioaccel_mode1_access = {
 	SA5_submit_command,
 	SA5_performant_intr_mask,
-	SA5_fifo_full,
 	SA5_ioaccel_mode1_intr_pending,
 	SA5_ioaccel_mode1_completed,
 };
@@ -554,7 +547,6 @@ static struct access_method SA5_ioaccel_mode1_access = {
 static struct access_method SA5_ioaccel_mode2_access = {
 	SA5_submit_command_ioaccel2,
 	SA5_performant_intr_mask,
-	SA5_fifo_full,
 	SA5_performant_intr_pending,
 	SA5_performant_completed,
 };
@@ -562,7 +554,6 @@ static struct access_method SA5_ioaccel_mode2_access = {
 static struct access_method SA5_performant_access = {
 	SA5_submit_command,
 	SA5_performant_intr_mask,
-	SA5_fifo_full,
 	SA5_performant_intr_pending,
 	SA5_performant_completed,
 };
@@ -570,7 +561,6 @@ static struct access_method SA5_performant_access = {
 static struct access_method SA5_performant_access_no_read = {
 	SA5_submit_command_no_read,
 	SA5_performant_intr_mask,
-	SA5_fifo_full,
 	SA5_performant_intr_pending,
 	SA5_performant_completed,
 };
diff --git a/drivers/scsi/hpsa_cmd.h b/drivers/scsi/hpsa_cmd.h
index b5125dc31439..e80865270ebb 100644
--- a/drivers/scsi/hpsa_cmd.h
+++ b/drivers/scsi/hpsa_cmd.h
@@ -1,6 +1,6 @@
 /*
  *    Disk Array driver for HP Smart Array SAS controllers
- *    Copyright 2000, 2014 Hewlett-Packard Development Company, L.P.
+ *    Copyright 2000, 2013 Hewlett-Packard Development Company, L.P.
  *
  *    This program is free software; you can redistribute it and/or modify
  *    it under the terms of the GNU General Public License as published by
@@ -42,8 +42,17 @@
 #define CMD_UNSOLICITED_ABORT   0x000A
 #define CMD_TIMEOUT             0x000B
 #define CMD_UNABORTABLE		0x000C
+#define CMD_TMF_STATUS		0x000D
 #define CMD_IOACCEL_DISABLED	0x000E
 
+/* TMF function status values */
+#define CISS_TMF_COMPLETE	0x00
+#define CISS_TMF_INVALID_FRAME	0x02
+#define CISS_TMF_NOT_SUPPORTED	0x04
+#define CISS_TMF_FAILED		0x05
+#define CISS_TMF_SUCCESS	0x08
+#define CISS_TMF_WRONG_LUN	0x09
+#define CISS_TMF_OVERLAPPED_TAG 0x0a
 
 /* Unit Attentions ASC's as defined for the MSA2012sa */
 #define POWER_OR_RESET			0x29
@@ -84,13 +93,15 @@
 /* cdb type */
 #define TYPE_CMD		0x00
 #define TYPE_MSG		0x01
-#define TYPE_IOACCEL2_CMD	0x81 /* 0x81 is not used by hardware */
+#define TYPE_IOACCEL2_CMD	0x81 /* 0x81 is not a value hardware uses */ 
 
 /* Message Types  */
 #define HPSA_TASK_MANAGEMENT    0x00
 #define HPSA_RESET              0x01
 #define HPSA_SCAN               0x02
 #define HPSA_NOOP               0x03
+#define HPSA_PHYS_TARGET_RESET 	0x99 /* not defined by cciss spec */
+
 
 #define HPSA_CTLR_RESET_TYPE    0x00
 #define HPSA_BUS_RESET_TYPE     0x01
@@ -144,15 +155,17 @@
 #define CFGTBL_BusType_Fibre2G  0x00000200l
 
 /* VPD Inquiry types */
-#define HPSA_VPD_SUPPORTED_PAGES        0x00
-#define HPSA_VPD_LV_DEVICE_GEOMETRY     0xC1
-#define HPSA_VPD_LV_IOACCEL_STATUS      0xC2
+#define HPSA_VPD_SUPPORTED_PAGES	0x00
+#define HPSA_VPD_LV_DEVICE_ID		0x83
+#define HPSA_VPD_PHYS_DEVICE_ID		0xC0
+#define HPSA_VPD_LV_DEVICE_GEOMETRY	0xC1
+#define HPSA_VPD_LV_IOACCEL_STATUS	0xC2
 #define HPSA_VPD_LV_STATUS		0xC3
-#define HPSA_VPD_HEADER_SZ              4
+#define HPSA_VPD_HEADER_SZ		4
 
 /* Logical volume states */
 #define HPSA_VPD_LV_STATUS_UNSUPPORTED			0xff
-#define HPSA_LV_OK                                      0x0
+#define HPSA_LV_OK					0x0
 #define HPSA_LV_UNDERGOING_ERASE			0x0F
 #define HPSA_LV_UNDERGOING_RPI				0x12
 #define HPSA_LV_PENDING_RPI				0x13
@@ -195,7 +208,7 @@ struct InquiryData {
 #define HPSA_CISS_READ	0xc0	/* CISS Read */
 #define HPSA_GET_RAID_MAP 0xc8	/* CISS Get RAID Layout Map */
 
-#define RAID_MAP_MAX_ENTRIES   256
+#define RAID_MAP_MAX_ENTRIES   1024
 
 struct raid_map_disk_data {
 	u32   ioaccel_handle;         /**< Handle to access this disk via the
@@ -212,7 +225,7 @@ struct raid_map_data {
 	u8    phys_blk_shift;		/* Shift factor to convert between
 					 * units of logical blocks and physical
 					 * disk blocks */
-	u8    parity_rotation_shift;	/* Shift factor to convert between units
+	u8    parity_rotation_shift;	/*  Shift factor to convert between units
 					 * of logical stripes and physical
 					 * stripes */
 	u16   strip_size;		/* blocks used on each disk / stripe */
@@ -222,11 +235,11 @@ struct raid_map_data {
 	u16   metadata_disks_per_row;	/* mirror/parity disk entries / row
 					 * in the map */
 	u16   row_cnt;			/* rows in each layout map */
-	u16   layout_map_count;		/* layout maps (1 map per mirror/parity
+	u16   layout_map_count;		/* layout maps (one map per mirror/parity
 					 * group) */
-	u16   flags;			/* Bit 0 set if encryption enabled */
-#define RAID_MAP_FLAG_ENCRYPT_ON  0x01
-	u16   dekindex;			/* Data encryption key index. */
+	u16   flags;			/* Bit 0 set if encryption is enabled for volume */
+#define RAID_MAP_FLAG_ENCRYPT_ON  0x01 	
+	u16   dekindex;			/* Data encryption key index. */  
 	u8    reserved[16];
 	struct raid_map_disk_data data[RAID_MAP_MAX_ENTRIES];
 };
@@ -240,9 +253,16 @@ struct ReportLUNdata {
 
 struct ext_report_lun_entry {
 	u8 lunid[8];
+#define MASKED_DEVICE(x) ((x)[3] & 0xC0)
+#define GET_BMIC_BUS(lunid) ((lunid)[7] & 0x3F)
+#define GET_BMIC_LEVEL_TWO_TARGET(lunid) ((lunid)[6])
+#define GET_BMIC_DRIVE_NUMBER(lunid) (((GET_BMIC_BUS((lunid)) - 1) << 8) + \
+			GET_BMIC_LEVEL_TWO_TARGET((lunid)))
 	u8 wwid[8];
 	u8 device_type;
 	u8 device_flags;
+#define NON_DISK_PHYS_DEV(x) ((x)[17] & 0x01)
+#define PHYS_IOACCEL(x) ((x)[17] & 0x08)
 	u8 lun_count; /* multi-lun device, how many luns */
 	u8 redundant_paths;
 	u32 ioaccel_handle; /* ioaccel1 only uses lower 16 bits */
@@ -252,7 +272,7 @@ struct ReportExtendedLUNdata {
 	u8 LUNListLength[4];
 	u8 extended_response_flag;
 	u8 reserved[3];
-	struct ext_report_lun_entry LUN[HPSA_MAX_LUN];
+	struct ext_report_lun_entry LUN[HPSA_MAX_PHYS_LUN];
 };
 
 struct SenseSubsystem_info {
@@ -268,6 +288,7 @@ struct SenseSubsystem_info {
 #define HPSA_CACHE_FLUSH 0x01	/* C2 was already being used by HPSA */
 #define BMIC_FLASH_FIRMWARE 0xF7
 #define BMIC_SENSE_CONTROLLER_PARAMETERS 0x64
+#define BMIC_IDENTIFY_PHYSICAL_DEVICE 0x15
 
 /* Command List Structure */
 union SCSI3Addr {
@@ -314,28 +335,36 @@ struct CommandListHeader {
 	u8              ReplyQueue;
 	u8              SGList;
 	u16             SGTotal;
-	struct vals32     Tag;
+	u64		tag;
 	union LUNAddr     LUN;
 };
 
 struct RequestBlock {
 	u8   CDBLen;
-	struct {
-		u8 Type:3;
-		u8 Attribute:3;
-		u8 Direction:2;
-	} Type;
+	/*
+	 * type_attr_dir:
+	 * type: low 3 bits
+	 * attr: middle 3 bits
+	 * dir: high 2 bits
+	 */
+	u8	type_attr_dir;
+#define TYPE_ATTR_DIR(t, a, d) ((((d) & 0x03) << 6) |\
+				(((a) & 0x07) << 3) |\
+				((t) & 0x07))
+#define GET_TYPE(tad) ((tad) & 0x07)
+#define GET_ATTR(tad) (((tad) >> 3) & 0x07)
+#define GET_DIR(tad) (((tad) >> 6) & 0x03)
 	u16  Timeout;
 	u8   CDB[16];
 };
 
 struct ErrDescriptor {
-	struct vals32 Addr;
+	u64 Addr;
 	u32  Len;
 };
 
 struct SGDescriptor {
-	struct vals32 Addr;
+	u64 Addr;
 	u32  Len;
 	u32  Ext;
 };
@@ -367,22 +396,18 @@ struct ErrorInfo {
 #define CMD_IOACCEL1	0x04
 #define CMD_IOACCEL2	0x05
 
-#define DIRECT_LOOKUP_SHIFT 5
-#define DIRECT_LOOKUP_BIT 0x10
-#define DIRECT_LOOKUP_MASK (~((1 << DIRECT_LOOKUP_SHIFT) - 1))
+#define DIRECT_LOOKUP_SHIFT 4
 
 #define HPSA_ERROR_BIT          0x02
 struct ctlr_info; /* defined in hpsa.h */
-/* The size of this structure needs to be divisible by 32
- * on all architectures because low 5 bits of the addresses
+/* The size of this structure needs to be divisible by 128
+ * on all architectures.  The low 4 bits of the addresses
  * are used as follows:
  *
  * bit 0: to device, used to indicate "performant mode" command
  *        from device, indidcates error status.
  * bit 1-3: to device, indicates block fetch table entry for
  *          reducing DMA in fetching commands from host memory.
- * bit 4: used to indicate whether tag is "direct lookup" (index),
- *        or a bus address.
  */
 
 #define COMMANDLIST_ALIGNMENT 128
@@ -397,14 +422,16 @@ struct CommandList {
 	struct ctlr_info	   *h;
 	int			   cmd_type;
 	long			   cmdindex;
-	struct list_head list;
+	struct request *rq;
 	struct completion *waiting;
-	void   *scsi_cmd;
-} __aligned(COMMANDLIST_ALIGNMENT);
+	struct scsi_cmnd *scsi_cmd;
+	struct work_struct work;
+	atomic_t refcount; /* Must be last to avoid memset in cmd_alloc */
+} __attribute__((aligned(COMMANDLIST_ALIGNMENT)));
 
 /* Max S/G elements in I/O accelerator command */
 #define IOACCEL1_MAXSGENTRIES           24
-#define IOACCEL2_MAXSGENTRIES		28
+#define IOACCEL2_MAXSGENTRIES		28 
 
 /*
  * Structure for I/O accelerator (mode 1) commands.
@@ -434,11 +461,11 @@ struct io_accel1_cmd {
 	u16 timeout_sec;		/* 0x62 - 0x63 */
 	u8  ReplyQueue;			/* 0x64 */
 	u8  reserved9[3];		/* 0x65 - 0x67 */
-	struct vals32 Tag;		/* 0x68 - 0x6F */
-	struct vals32 host_addr;	/* 0x70 - 0x77 */
+	u64 tag;			/* 0x68 - 0x6F */
+	u64 host_addr;			/* 0x70 - 0x77 */
 	u8  CISS_LUN[8];		/* 0x78 - 0x7F */
 	struct SGDescriptor SG[IOACCEL1_MAXSGENTRIES];
-} __aligned(IOACCEL1_COMMANDLIST_ALIGNMENT);
+} __attribute__((aligned(IOACCEL1_COMMANDLIST_ALIGNMENT)));
 
 #define IOACCEL1_FUNCTION_SCSIIO        0x00
 #define IOACCEL1_SGLOFFSET              32
@@ -493,6 +520,12 @@ struct io_accel2_scsi_response {
 #define IOACCEL2_STATUS_SR_TASK_COMP_SET_FULL	0x28
 #define IOACCEL2_STATUS_SR_TASK_COMP_ABORTED	0x40
 #define IOACCEL2_STATUS_SR_IOACCEL_DISABLED	0x0E
+#define IOACCEL2_STATUS_SR_IO_ERROR		0x01
+#define IOACCEL2_STATUS_SR_IO_ABORTED		0x02
+#define IOACCEL2_STATUS_SR_NO_PATH_TO_DEVICE	0x03
+#define IOACCEL2_STATUS_SR_INVALID_DEVICE	0x04
+#define IOACCEL2_STATUS_SR_UNDERRUN		0x51
+#define IOACCEL2_STATUS_SR_OVERRUN		0x75
 	u8 data_present;		/* low 2 bits */
 #define IOACCEL2_NO_DATAPRESENT		0x000
 #define IOACCEL2_RESPONSE_DATAPRESENT	0x001
@@ -513,14 +546,14 @@ struct io_accel2_cmd {
 	u8  direction;			/* direction, memtype, and encryption */
 #define IOACCEL2_DIRECTION_MASK		0x03 /* bits 0,1: direction  */
 #define IOACCEL2_DIRECTION_MEMTYPE_MASK	0x04 /* bit 2: memtype source/dest */
-					     /*     0b=PCIe, 1b=DDR */
+					     /*     0b=PCIe, 1b=DDR */ 
 #define IOACCEL2_DIRECTION_ENCRYPT_MASK	0x08 /* bit 3: encryption flag */
-					     /*     0=off, 1=on */
+					     /*     0=off, 1=on */					
 	u8  reply_queue;		/* Reply Queue ID */
 	u8  reserved1;			/* Reserved */
 	u32 scsi_nexus;			/* Device Handle */
 	u32 Tag;			/* cciss tag, lower 4 bytes only */
-	u32 tweak_lower;		/* Encryption tweak, lower 4 bytes */
+	u32 tweak_lower;		/* Encryption tweak, lower 4 bytes */	
 	u8  cdb[16];			/* SCSI Command Descriptor Block */
 	u8  cciss_lun[8];		/* 8 byte SCSI address */
 	u32 data_len;			/* Total bytes to transfer */
@@ -534,7 +567,7 @@ struct io_accel2_cmd {
 	u32 tweak_upper;		/* Encryption tweak, upper 4 bytes */
 	struct ioaccel2_sg_element sg[IOACCEL2_MAXSGENTRIES];
 	struct io_accel2_scsi_response error_data;
-} __aligned(IOACCEL2_COMMANDLIST_ALIGNMENT);
+} __attribute__((aligned(IOACCEL2_COMMANDLIST_ALIGNMENT)));
 
 /*
  * defines for Mode 2 command struct
@@ -550,13 +583,14 @@ struct io_accel2_cmd {
  */
 struct hpsa_tmf_struct {
 	u8 iu_type;		/* Information Unit Type */
+#define IU_TYPE_TMF		0x41
 	u8 reply_queue;		/* Reply Queue ID */
 	u8 tmf;			/* Task Management Function */
 	u8 reserved1;		/* byte 3 Reserved */
 	u32 it_nexus;		/* SCSI I-T Nexus */
 	u8 lun_id[8];		/* LUN ID for TMF request */
-	struct vals32 Tag;	/* cciss tag associated w/ request */
-	struct vals32 abort_tag;/* cciss tag of SCSI cmd or task to abort */
+	u64 tag;		/* cciss tag associated w/ request */
+	u64 abort_tag;		/* cciss tag of SCSI cmd or task to abort */
 	u64 error_ptr;		/* Error Pointer */
 	u32 error_len;		/* Error Length */
 };
@@ -589,7 +623,6 @@ struct CfgTable {
 	u32           HeartBeat;
 	u32           driver_support;
 #define			ENABLE_SCSI_PREFETCH 0x100
-#define			ENABLE_UNIT_ATTN 0x01
 	u32	 	MaxScatterGatherElements;
 	u32		MaxLogicalUnits;
 	u32		MaxPhysicalDevices;
@@ -636,5 +669,137 @@ struct hpsa_pci_info {
 	u32		board_id;
 };
 
+struct bmic_identify_physical_device {
+	u8 scsi_bus;          /* SCSI Bus number on controller */
+	u8 scsi_id;           /* SCSI ID on this bus */
+	u16 block_size;	     /* sector size in bytes */
+	u32 total_blocks;	     /* number for sectors on drive */
+	u32 reserved_blocks;   /* controller reserved (RIS) */
+	u8 model[40];         /* Physical Drive Model */
+	u8 serial_number[40]; /* Drive Serial Number */
+	u8 firmware_revision[8]; /* drive firmware revision */
+	u8 scsi_inquiry_bits; /* inquiry byte 7 bits */
+	u8 compaq_drive_stamp; /* 0 means drive not stamped */
+	u8 last_failure_reason;
+#define BMIC_LAST_FAILURE_TOO_SMALL_IN_LOAD_CONFIG		0x01
+#define BMIC_LAST_FAILURE_ERROR_ERASING_RIS			0x02
+#define BMIC_LAST_FAILURE_ERROR_SAVING_RIS			0x03
+#define BMIC_LAST_FAILURE_FAIL_DRIVE_COMMAND			0x04
+#define BMIC_LAST_FAILURE_MARK_BAD_FAILED			0x05
+#define BMIC_LAST_FAILURE_MARK_BAD_FAILED_IN_FINISH_REMAP	0x06
+#define BMIC_LAST_FAILURE_TIMEOUT				0x07
+#define BMIC_LAST_FAILURE_AUTOSENSE_FAILED			0x08
+#define BMIC_LAST_FAILURE_MEDIUM_ERROR_1			0x09
+#define BMIC_LAST_FAILURE_MEDIUM_ERROR_2			0x0a
+#define BMIC_LAST_FAILURE_NOT_READY_BAD_SENSE			0x0b
+#define BMIC_LAST_FAILURE_NOT_READY				0x0c
+#define BMIC_LAST_FAILURE_HARDWARE_ERROR			0x0d
+#define BMIC_LAST_FAILURE_ABORTED_COMMAND			0x0e
+#define BMIC_LAST_FAILURE_WRITE_PROTECTED			0x0f
+#define BMIC_LAST_FAILURE_SPIN_UP_FAILURE_IN_RECOVER		0x10
+#define BMIC_LAST_FAILURE_REBUILD_WRITE_ERROR			0x11
+#define BMIC_LAST_FAILURE_TOO_SMALL_IN_HOT_PLUG			0x12
+#define BMIC_LAST_FAILURE_BUS_RESET_RECOVERY_ABORTED		0x13
+#define BMIC_LAST_FAILURE_REMOVED_IN_HOT_PLUG			0x14
+#define BMIC_LAST_FAILURE_INIT_REQUEST_SENSE_FAILED		0x15
+#define BMIC_LAST_FAILURE_INIT_START_UNIT_FAILED		0x16
+#define BMIC_LAST_FAILURE_INQUIRY_FAILED			0x17
+#define BMIC_LAST_FAILURE_NON_DISK_DEVICE			0x18
+#define BMIC_LAST_FAILURE_READ_CAPACITY_FAILED			0x19
+#define BMIC_LAST_FAILURE_INVALID_BLOCK_SIZE			0x1a
+#define BMIC_LAST_FAILURE_HOT_PLUG_REQUEST_SENSE_FAILED		0x1b
+#define BMIC_LAST_FAILURE_HOT_PLUG_START_UNIT_FAILED		0x1c
+#define BMIC_LAST_FAILURE_WRITE_ERROR_AFTER_REMAP		0x1d
+#define BMIC_LAST_FAILURE_INIT_RESET_RECOVERY_ABORTED		0x1e
+#define BMIC_LAST_FAILURE_DEFERRED_WRITE_ERROR			0x1f
+#define BMIC_LAST_FAILURE_MISSING_IN_SAVE_RIS			0x20
+#define BMIC_LAST_FAILURE_WRONG_REPLACE				0x21
+#define BMIC_LAST_FAILURE_GDP_VPD_INQUIRY_FAILED		0x22
+#define BMIC_LAST_FAILURE_GDP_MODE_SENSE_FAILED			0x23
+#define BMIC_LAST_FAILURE_DRIVE_NOT_IN_48BIT_MODE		0x24
+#define BMIC_LAST_FAILURE_DRIVE_TYPE_MIX_IN_HOT_PLUG		0x25
+#define BMIC_LAST_FAILURE_DRIVE_TYPE_MIX_IN_LOAD_CFG		0x26
+#define BMIC_LAST_FAILURE_PROTOCOL_ADAPTER_FAILED		0x27
+#define BMIC_LAST_FAILURE_FAULTY_ID_BAY_EMPTY			0x28
+#define BMIC_LAST_FAILURE_FAULTY_ID_BAY_OCCUPIED		0x29
+#define BMIC_LAST_FAILURE_FAULTY_ID_INVALID_BAY			0x2a
+#define BMIC_LAST_FAILURE_WRITE_RETRIES_FAILED			0x2b
+
+#define BMIC_LAST_FAILURE_SMART_ERROR_REPORTED			0x37
+#define BMIC_LAST_FAILURE_PHY_RESET_FAILED			0x38
+#define BMIC_LAST_FAILURE_ONLY_ONE_CTLR_CAN_SEE_DRIVE		0x40
+#define BMIC_LAST_FAILURE_KC_VOLUME_FAILED			0x41
+#define BMIC_LAST_FAILURE_UNEXPECTED_REPLACEMENT		0x42
+#define BMIC_LAST_FAILURE_OFFLINE_ERASE				0x80
+#define BMIC_LAST_FAILURE_OFFLINE_TOO_SMALL			0x81
+#define BMIC_LAST_FAILURE_OFFLINE_DRIVE_TYPE_MIX		0x82
+#define BMIC_LAST_FAILURE_OFFLINE_ERASE_COMPLETE		0x83
+
+	u8  flags;
+	u8  more_flags;
+	u8  scsi_lun;          /* SCSI LUN for phys drive */
+	u8  yet_more_flags;
+	u8  even_more_flags;
+	u32 spi_speed_rules;/* SPI Speed data:Ultra disable diagnose */
+	u8  phys_connector[2];         /* connector number on controller */
+	u8  phys_box_on_bus;  /* phys enclosure this drive resides */
+	u8  phys_bay_in_box;  /* phys drv bay this drive resides */
+	u32 rpm;              /* Drive rotational speed in rpm */
+	u8  device_type;       /* type of drive */
+	u8  sata_version;     /* only valid when drive_type is SATA */
+	u64 big_total_block_count;
+	u64 ris_starting_lba;
+	u32 ris_size;
+	u8  wwid[20];
+	u8  controller_phy_map[32];
+	u16 phy_count;
+	u8  phy_connected_dev_type[256];
+	u8  phy_to_drive_bay_num[256];
+	u16 phy_to_attached_dev_index[256];
+	u8  box_index;
+	u8  reserved;
+	u16 extra_physical_drive_flags;
+#define BMIC_PHYS_DRIVE_SUPPORTS_GAS_GAUGE(idphydrv) \
+	(idphydrv->extra_physical_drive_flags & (1 << 10))
+	u8  negotiated_link_rate[256];
+	u8  phy_to_phy_map[256];
+	u8  redundant_path_present_map;
+	u8  redundant_path_failure_map;
+	u8  active_path_number;
+	u16 alternate_paths_phys_connector[8];
+	u8  alternate_paths_phys_box_on_port[8];
+	u8  multi_lun_device_lun_count;
+	u8  minimum_good_fw_revision[8];
+	u8  unique_inquiry_bytes[20];
+	u8  current_temperature_degreesC;
+	u8  temperature_threshold_degreesC;
+	u8  max_temperature_degreesC;
+	u8  logical_blocks_per_phys_block_exp; /* phyblocksize = 512 * 2^exp */
+	u16 current_queue_depth_limit;
+	u8  switch_name[10];
+	u16 switch_port;
+	u8  alternate_paths_switch_name[40];
+	u8  alternate_paths_switch_port[8];
+	u16 power_on_hours; /* valid only if gas gauge supported */
+	u16 percent_endurance_used; /* valid only if gas gauge supported. */
+#define BMIC_PHYS_DRIVE_SSD_WEAROUT(idphydrv) \
+	((idphydrv->percent_endurance_used & 0x80) || \
+	 (idphydrv->percent_endurance_used > 10000))
+	u8  drive_authentication;
+#define BMIC_PHYS_DRIVE_AUTHENTICATED(idphydrv) \
+	(idphydrv->drive_authentication == 0x80)
+	u8  smart_carrier_authentication;
+#define BMIC_SMART_CARRIER_AUTHENTICATION_SUPPORTED(idphydrv) \
+	(idphydrv->smart_carrier_authentication != 0x0)
+#define BMIC_SMART_CARRIER_AUTHENTICATED(idphydrv) \
+	(idphydrv->smart_carrier_authentication == 0x01)
+	u8  smart_carrier_app_fw_version;
+	u8  smart_carrier_bootloader_fw_version;
+	u8  encryption_key_name[64];
+	u32 misc_drive_flags;
+	u16 dek_index;
+	u8  padding[112];
+};
+
 #pragma pack()
 #endif /* HPSA_CMD_H */
diff --git a/drivers/scsi/hpsa_kernel_compat.h b/drivers/scsi/hpsa_kernel_compat.h
new file mode 100644
index 000000000000..65467d0ddbf0
--- /dev/null
+++ b/drivers/scsi/hpsa_kernel_compat.h
@@ -0,0 +1,731 @@
+/*
+ *    Disk Array driver for HP Smart Array SAS controllers
+ *    Copyright 2013, Hewlett-Packard Development Company, L.P.
+ *
+ *    This program is free software; you can redistribute it and/or modify
+ *    it under the terms of the GNU General Public License as published by
+ *    the Free Software Foundation; version 2 of the License.
+ *
+ *    This program is distributed in the hope that it will be useful,
+ *    but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *    MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or
+ *    NON INFRINGEMENT.  See the GNU General Public License for more details.
+ *
+ *    You should have received a copy of the GNU General Public License
+ *    along with this program; if not, write to the Free Software
+ *    Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ *    Questions/Comments/Bugfixes to iss_storagedev@hp.com
+ *
+ */
+
+/*
+ * The following #defines allow the hpsa driver to be compiled for a 
+ * variety of kernels.  Despite having names like RHEL5, SLES11, these
+ * are more about the kernel than about the OS.  So for instance, if
+ * you're running RHEL5 (typically 2.6.18-ish kernel), but you've compiled
+ * a custom 2.6.38 or 3.x kernel and you're running that, then you don't want
+ * the RHEL5 define, you probably want the default kernel.org (as of this
+ * writing circa March 2012)  If you're running the OS vendor's kernel
+ * or a kernel that is of roughly the same vintage as the OS vendor's kernel
+ * then you can go by the OS name.
+ *
+ * If you have some intermediate kernel which doesn't quite match any of
+ * the predefined sets of kernel features here, you may have to make your own 
+ * define for your particular kernel and mix and match the kernel features
+ * to fit the kernel you're compiling for.  How can you tell?  By studying
+ * the source of this file and the source of the kernel you're compiling for
+ * and understanding which "KFEATURES" your kernel has.
+ *
+ * Usually, if you get it wrong, it won't compile, but there are no doubt
+ * some cases in which, if you get it wrong, it will compile, but won't
+ * work right.  In any case, if you're compiling this, you're on your own
+ * and likely nobody has tested this particular code with your particular
+ * kernel, so, good luck, and pay attention to the compiler warnings.
+ *
+ */
+
+/* #define SLES11sp1 */
+/* #define SLES11sp2plus */
+/* #define RHEL5 */
+/* #define RHEL5u2 */
+/* #define RHEL6 */
+/* #define RHEL7 */
+/* Default is kernel.org */
+
+
+/* ----- RHEL5 variants --------- */
+#if \
+	defined(RHEL5U0) || \
+	defined(RHEL5U1) || \
+	defined(RHEL5U2) || \
+	defined(RHEL5U2) || \
+	defined(RHEL5U3) || \
+	defined(RHEL5U4) || \
+	defined(RHEL5U5) || \
+	defined(RHEL5U6) || \
+	defined(RHEL5U7) || \
+	defined(RHEL5U8) || \
+	defined(RHEL5U9) || \
+	defined(RHEL5U10)
+#error "Sorry, hpsa will no longer work on RHEL5.  The scatter gather DMA mapping API of RHEL5 is too old."
+#endif
+
+
+/* ----- RHEL6 variants --------- */
+#if \
+	defined (RHEL6U0) || \
+	defined (RHEL6U1) || \
+	defined (RHEL6U2) || \
+	defined (RHEL6U3) || \
+	defined (RHEL6U4) || \
+	defined (RHEL6U5)
+#define RHEL6
+#endif
+
+
+/* ----- RHEL7 variants --------- */
+#if \
+	defined (RHEL7U0)
+#define RHEL7
+#endif
+
+
+/* ----- SLES11 variants --------- */
+#if \
+	defined (SLES11SP0) || \
+	defined (SLES11SP1) || \
+	defined (SLES11SP2) || \
+	defined (SLES11SP3)
+
+#define SLES11
+#define SLES11sp2plus
+
+#if defined (SLES11SP0)
+#undef SLES11sp1
+#undef SLES11sp2plus
+#endif
+
+#if defined (SLES11SP1)
+#define SLES11sp1
+#undef SLES11sp2plus
+#endif
+#endif
+
+
+/* ----- SLES12 variants --------- */
+#if \
+	defined (SLES12SP0)
+#define SLES12
+#endif
+
+
+/* Define kernel features per distro... */
+
+#ifdef RHEL6 /************ RHEL 6 ************/
+#define KFEATURE_HAS_WAIT_FOR_COMPLETION_IO_TIMEOUT 0
+#define KFEATURE_HAS_WAIT_FOR_COMPLETION_IO 0
+#define KFEATURE_HAS_2011_03_INTERRUPT_HANDLER 1
+#define KFEATURE_CHANGE_QDEPTH_HAS_REASON 1
+#define KFEATURE_HAS_2011_03_STYLE_DEVICE_ATTR 1
+#define KFEATURE_HAS_SCSI_QDEPTH_DEFAULT 1
+#define KFEATURE_HAS_SCSI_FOR_EACH_SG 1
+#define KFEATURE_HAS_SCSI_DEVICE_TYPE 1
+#define KFEATURE_SCAN_START_PRESENT 1
+#define KFEATURE_SCAN_START_IMPLEMENTED 1
+#define KFEATURE_HAS_2011_03_QUEUECOMMAND 0
+#define KFEATURE_HAS_SHOST_PRIV 1
+#define KFEATURE_HAS_SCSI_DMA_FUNCTIONS 1
+#define KFEATURE_HAS_SCSI_SET_RESID 1
+#define KFEATURE_HAS_UACCESS_H_FILE 1
+#define KFEATURE_HAS_SMP_LOCK_H 1
+#define KFEATURE_HAS_NEW_DMA_MAPPING_ERROR 1
+#define KFEATURE_HAS_IRQ_SET_AFFINITY_HINTS 1
+#define KFEATURE_HAS_ALLOC_WORKQUEUE 0
+#define KFEATURE_HAS_ATOMIC_DEC_IF_POSITIVE 0
+#define HPSA_SUPPORTS_STORAGEWORKS_1210m 1
+#define SA_CONTROLLERS_GEN6 1
+#define SA_CONTROLLERS_GEN8 1
+#define SA_CONTROLLERS_GEN8_2 1
+#define SA_CONTROLLERS_GEN8_5 1
+#define SA_CONTROLLERS_GEN9 1
+
+#if defined (RHEL6U0) || defined (RHEL6U1)
+/* RHEL6 base and update 1 do not have .lockless field in SCSI host template
+ * but RHEL6u2 and onwards do.
+ */
+#define KFEATURE_HAS_HOST_LOCKLESS_FIELD 0
+#else
+#define KFEATURE_HAS_HOST_LOCKLESS_FIELD 1
+#endif
+
+#else /* ... else it's not RHEL6 */
+
+#ifdef SLES11
+#define KFEATURE_HAS_WAIT_FOR_COMPLETION_IO_TIMEOUT 0
+#define KFEATURE_HAS_WAIT_FOR_COMPLETION_IO 0
+#define KFEATURE_HAS_2011_03_INTERRUPT_HANDLER 1
+#define KFEATURE_CHANGE_QDEPTH_HAS_REASON 1
+#define KFEATURE_HAS_2011_03_STYLE_DEVICE_ATTR 1
+#define KFEATURE_HAS_SCSI_QDEPTH_DEFAULT 1
+#define KFEATURE_HAS_SCSI_FOR_EACH_SG 1
+#define KFEATURE_HAS_SCSI_DEVICE_TYPE 1
+#define KFEATURE_SCAN_START_PRESENT 1
+#define KFEATURE_SCAN_START_IMPLEMENTED 1
+#define KFEATURE_HAS_HOST_LOCKLESS_FIELD 0
+#define KFEATURE_HAS_SHOST_PRIV 1
+#define KFEATURE_HAS_SCSI_DMA_FUNCTIONS 1
+#define KFEATURE_HAS_SCSI_SET_RESID 1
+#define KFEATURE_HAS_UACCESS_H_FILE 1
+#define KFEATURE_HAS_NEW_DMA_MAPPING_ERROR 1
+#define HPSA_SUPPORTS_STORAGEWORKS_1210m 1
+#define SA_CONTROLLERS_GEN6 0
+#define SA_CONTROLLERS_GEN8 1
+#define SA_CONTROLLERS_GEN8_2 1
+#define SA_CONTROLLERS_GEN8_5 1
+#define SA_CONTROLLERS_GEN9 1
+
+#ifdef SLES11sp1 /************* SLES11 sp1 ********/
+#define KFEATURE_HAS_SMP_LOCK_H 1
+#define KFEATURE_HAS_2011_03_QUEUECOMMAND 0
+#define KFEATURE_HAS_IRQ_SET_AFFINITY_HINTS 0
+#define KFEATURE_HAS_ALLOC_WORKQUEUE 0
+#define KFEATURE_HAS_ATOMIC_DEC_IF_POSITIVE 0
+#endif
+
+#ifdef SLES11sp2plus /************* SLES11 sp2 and after ********/
+#define KFEATURE_HAS_SMP_LOCK_H 0
+#define KFEATURE_HAS_2011_03_QUEUECOMMAND 1
+#define KFEATURE_HAS_IRQ_SET_AFFINITY_HINTS 1
+#define KFEATURE_HAS_ALLOC_WORKQUEUE 1
+#define KFEATURE_HAS_ATOMIC_DEC_IF_POSITIVE 1
+#endif
+
+#else /* ... else not SLES11 */
+
+#ifdef RHEL7 /************ RHEL 7 ************/
+#define KFEATURE_HAS_WAIT_FOR_COMPLETION_IO_TIMEOUT 1
+#define KFEATURE_HAS_WAIT_FOR_COMPLETION_IO 1
+#define KFEATURE_HAS_2011_03_INTERRUPT_HANDLER 1
+#define KFEATURE_CHANGE_QDEPTH_HAS_REASON 1
+#define KFEATURE_HAS_2011_03_STYLE_DEVICE_ATTR 1
+#define KFEATURE_HAS_SCSI_QDEPTH_DEFAULT 1
+#define KFEATURE_HAS_SCSI_FOR_EACH_SG 1
+#define KFEATURE_HAS_SCSI_DEVICE_TYPE 1
+#define KFEATURE_SCAN_START_PRESENT 1
+#define KFEATURE_SCAN_START_IMPLEMENTED 1
+#define KFEATURE_HAS_2011_03_QUEUECOMMAND 1
+#define KFEATURE_HAS_HOST_LOCKLESS_FIELD 0
+#define KFEATURE_HAS_SHOST_PRIV 1
+#define KFEATURE_HAS_SCSI_DMA_FUNCTIONS 1
+#define KFEATURE_HAS_SCSI_SET_RESID 1
+#define KFEATURE_HAS_UACCESS_H_FILE 1
+#define KFEATURE_HAS_SMP_LOCK_H 0 /* include/linux/smp_lock.h removed between 2.6.38 and 2.6.39 */
+#define KFEATURE_HAS_NEW_DMA_MAPPING_ERROR 1
+#define KFEATURE_HAS_IRQ_SET_AFFINITY_HINTS 1
+#define KFEATURE_HAS_ALLOC_WORKQUEUE 1
+#define KFEATURE_HAS_ATOMIC_DEC_IF_POSITIVE 1
+#define HPSA_SUPPORTS_STORAGEWORKS_1210m 1
+#define SA_CONTROLLERS_GEN6 1
+#define SA_CONTROLLERS_GEN8 1
+#define SA_CONTROLLERS_GEN8_2 1
+#define SA_CONTROLLERS_GEN8_5 1
+#define SA_CONTROLLERS_GEN9 1
+
+#else /* ... else Default, kernel.org */
+#define KFEATURE_HAS_WAIT_FOR_COMPLETION_IO_TIMEOUT 1
+#define KFEATURE_HAS_WAIT_FOR_COMPLETION_IO 1
+#define KFEATURE_HAS_2011_03_INTERRUPT_HANDLER 1
+#define KFEATURE_CHANGE_QDEPTH_HAS_REASON 1
+#define KFEATURE_HAS_2011_03_STYLE_DEVICE_ATTR 1
+#define KFEATURE_HAS_SCSI_QDEPTH_DEFAULT 1
+#define KFEATURE_HAS_SCSI_FOR_EACH_SG 1
+#define KFEATURE_HAS_SCSI_DEVICE_TYPE 1
+#define KFEATURE_SCAN_START_PRESENT 1
+#define KFEATURE_SCAN_START_IMPLEMENTED 1
+#define KFEATURE_HAS_2011_03_QUEUECOMMAND 1
+#define KFEATURE_HAS_HOST_LOCKLESS_FIELD 0
+#define KFEATURE_HAS_SHOST_PRIV 1
+#define KFEATURE_HAS_SCSI_DMA_FUNCTIONS 1
+#define KFEATURE_HAS_SCSI_SET_RESID 1
+#define KFEATURE_HAS_UACCESS_H_FILE 1
+#define KFEATURE_HAS_SMP_LOCK_H 0 /* include/linux/smp_lock.h removed between 2.6.38 and 2.6.39 */
+#define KFEATURE_HAS_NEW_DMA_MAPPING_ERROR 1
+#define KFEATURE_HAS_IRQ_SET_AFFINITY_HINTS 1
+#define KFEATURE_HAS_ALLOC_WORKQUEUE 1
+#define KFEATURE_HAS_ATOMIC_DEC_IF_POSITIVE 1
+#define HPSA_SUPPORTS_STORAGEWORKS_1210m 1
+#define SA_CONTROLLERS_GEN6 1
+#define SA_CONTROLLERS_GEN8 1
+#define SA_CONTROLLERS_GEN8_2 1
+#define SA_CONTROLLERS_GEN8_5 1
+#define SA_CONTROLLERS_GEN9 1
+/* --- end of default kernel.org --- */
+
+#endif /* RHEL7 */
+#endif /* SLES11 */
+#endif /* RHEL6 */
+
+#if !KFEATURE_HAS_WAIT_FOR_COMPLETION_IO_TIMEOUT
+static inline unsigned long wait_for_completion_io_timeout(struct completion *x,
+			__attribute__((unused)) unsigned long timeout)
+{
+	return wait_for_completion_timeout(x, timeout);
+}
+#endif
+
+#if !KFEATURE_HAS_WAIT_FOR_COMPLETION_IO
+static inline unsigned long wait_for_completion_io(struct completion *x)
+{
+	wait_for_completion(x);
+	return 0;
+}
+#endif
+
+#if KFEATURE_HAS_2011_03_INTERRUPT_HANDLER
+	/* new style interrupt handler */
+#	define DECLARE_INTERRUPT_HANDLER(handler) \
+		static irqreturn_t handler(int irq, void *queue)
+#	define INTERRUPT_HANDLER_TYPE(handler) \
+		irqreturn_t (*handler)(int, void *)
+#else
+	/* old style interrupt handler */
+#	define DECLARE_INTERRUPT_HANDLER(handler) \
+		static irqreturn_t handler(int irq, void *queue, \
+			struct pt_regs *regs)
+#	define INTERRUPT_HANDLER_TYPE(handler) \
+		irqreturn_t (*handler)(int, void *, struct pt_regs *)
+#endif
+
+
+#if KFEATURE_CHANGE_QDEPTH_HAS_REASON
+#	define DECLARE_CHANGE_QUEUE_DEPTH(func) \
+	static int func(struct scsi_device *sdev, \
+		int qdepth, int reason)
+#	define BAIL_ON_BAD_REASON \
+		{ if (reason != SCSI_QDEPTH_DEFAULT) \
+			return -ENOTSUPP; }
+#else
+#	define DECLARE_CHANGE_QUEUE_DEPTH(func) \
+	static int func(struct scsi_device *sdev, int qdepth)
+#	define BAIL_ON_BAD_REASON
+#endif
+
+
+#if KFEATURE_HAS_2011_03_STYLE_DEVICE_ATTR
+
+#	define DECLARE_DEVATTR_SHOW_FUNC(func) \
+		static ssize_t func(struct device *dev, \
+			struct device_attribute *attr, char *buf)
+
+#	define DECLARE_DEVATTR_STORE_FUNC(func) \
+	static ssize_t func(struct device *dev, \
+		struct device_attribute *attr, const char *buf, size_t count)
+
+#	define DECLARE_HOST_DEVICE_ATTR(xname, xmode, xshow, xstore) \
+		DEVICE_ATTR(xname, xmode, xshow, xstore)
+
+#	define DECLARE_HOST_ATTR_LIST(xlist) \
+	static struct device_attribute *xlist[]
+#else /* not KFEATURE_HAS_2011_03_STYLE_DEVICE_ATTR */
+
+#	define DECLARE_DEVATTR_SHOW_FUNC(func) \
+	static ssize_t func(struct class_device *dev, char *buf)
+
+#	define DECLARE_DEVATTR_STORE_FUNC(func) \
+	static ssize_t func(struct class_device *dev, \
+		const char *buf, size_t count)
+
+#	define DECLARE_HOST_DEVICE_ATTR(xname, xmode, xshow, xstore) \
+	struct class_device_attribute dev_attr_##xname = {\
+		.attr = { \
+			.name = #xname, \
+			.mode = xmode, \
+		}, \
+		.show = xshow, \
+		.store = xstore, \
+	};
+
+#	define DECLARE_HOST_ATTR_LIST(xlist) \
+	static struct class_device_attribute *xlist[]
+
+#endif /* KFEATURE_HAS_2011_03_STYLE_DEVICE_ATTR */
+
+#ifndef SCSI_QDEPTH_DEFAULT
+#	define SCSI_QDEPTH_DEFAULT 0
+#endif
+
+#if !KFEATURE_HAS_SCSI_FOR_EACH_SG
+#	define scsi_for_each_sg(cmd, sg, nseg, __i) \
+	for (__i = 0, sg = scsi_sglist(cmd); __i < (nseg); __i++, (sg)++)
+#endif
+
+#if !KFEATURE_HAS_SHOST_PRIV
+	static inline void *shost_priv(struct Scsi_Host *shost)
+	{
+		return (void *) shost->hostdata;
+	}
+#endif
+
+#if !KFEATURE_HAS_SCSI_DMA_FUNCTIONS
+	/* Does not have things like scsi_dma_map, scsi_dma_unmap, scsi_sg_count,
+	 * sg_dma_address, sg_dma_len...
+	 */
+
+static void hpsa_map_sg_chain_block(struct ctlr_info *h,
+	struct CommandList *c);
+
+/* It is not reasonably possible to retrofit the new scsi dma interfaces
+ * onto the old code.  So we retrofit at a higher level, at the dma mapping
+ * function of the hpsa driver itself.
+ *
+ * hpsa_scatter_gather takes a struct scsi_cmnd, (cmd), and does the pci
+ * dma mapping  and fills in the scatter gather entries of the
+ * hpsa command, cp.
+ */
+static int hpsa_scatter_gather(struct ctlr_info *h,
+		struct CommandList *cp,
+		struct scsi_cmnd *cmd)
+{
+	unsigned int len;
+	u64 addr64;
+	int use_sg, i, sg_index, chained = 0;
+	struct SGDescriptor *curr_sg;
+	struct scatterlist *sg = (struct scatterlist *) cmd->request_buffer;
+
+	if (!cmd->use_sg) {
+		if (cmd->request_bufflen) { /* Just one scatter gather entry */
+			addr64 = (__u64) pci_map_single(h->pdev,
+				cmd->request_buffer, cmd->request_bufflen,
+				cmd->sc_data_direction);
+
+			cp->SG[0].Addr.lower =
+				(__u32) (addr64 & (__u64) 0x0FFFFFFFF);
+			cp->SG[0].Addr.upper =
+				(__u32) ((addr64 >> 32) & (__u64) 0x0FFFFFFFF);
+			cp->SG[0].Len = cmd->request_bufflen;
+			use_sg = 1;
+		} else /* Zero sg entries */
+			use_sg = 0;
+	} else {
+		BUG_ON(cmd->use_sg > h->maxsgentries);
+
+		/* Many sg entries */
+		use_sg = pci_map_sg(h->pdev, cmd->request_buffer, cmd->use_sg,
+				cmd->sc_data_direction);
+
+		if (use_sg < 0)
+			return use_sg;
+
+		sg_index = 0;
+		curr_sg = cp->SG;
+		use_sg = cmd->use_sg;
+
+		for (i = 0; i < use_sg; i++) {
+			if (i == h->max_cmd_sg_entries - 1 &&
+				use_sg > h->max_cmd_sg_entries) {
+				chained = 1;
+				curr_sg = h->cmd_sg_list[cp->cmdindex];
+				sg_index = 0;
+			}
+			addr64 = (__u64) sg_dma_address(&sg[i]);
+			len  = sg_dma_len(&sg[i]);
+			curr_sg->Addr.lower =
+				(u32) (addr64 & 0x0FFFFFFFFULL);
+			curr_sg->Addr.upper =
+				(u32) ((addr64 >> 32) & 0x0FFFFFFFFULL);
+			curr_sg->Len = len;
+			curr_sg->Ext = 0;  /* we are not chaining */
+			curr_sg++;
+		}
+	}
+
+	if (use_sg + chained > h->maxSG)
+		h->maxSG = use_sg + chained;
+
+	if (chained) {
+		cp->Header.SGList = h->max_cmd_sg_entries;
+		cp->Header.SGTotal = (u16) (use_sg + 1);
+		hpsa_map_sg_chain_block(h, cp);
+		return 0;
+	}
+
+	cp->Header.SGList = (u8) use_sg;   /* no. SGs contig in this cmd */
+	cp->Header.SGTotal = (u16) use_sg; /* total sgs in this cmd list */
+	return 0;
+}
+
+static void hpsa_unmap_sg_chain_block(struct ctlr_info *h,
+	struct CommandList *c);
+static void hpsa_scatter_gather_unmap(struct ctlr_info *h,
+	struct CommandList *c, struct scsi_cmnd *cmd)
+{
+	union u64bit addr64;
+
+	if (cmd->use_sg) {
+		pci_unmap_sg(h->pdev, cmd->request_buffer, cmd->use_sg,
+			cmd->sc_data_direction);
+		if (c->Header.SGTotal > h->max_cmd_sg_entries)
+			hpsa_unmap_sg_chain_block(h, c);
+		return;
+	}
+	if (cmd->request_bufflen) {
+		addr64.val32.lower = c->SG[0].Addr.lower;
+		addr64.val32.upper = c->SG[0].Addr.upper;
+		pci_unmap_single(h->pdev, (dma_addr_t) addr64.val,
+		cmd->request_bufflen, cmd->sc_data_direction);
+	}
+}
+
+static inline void scsi_dma_unmap(struct scsi_cmnd *cmd)
+{
+	struct CommandList *c = (struct CommandList *) cmd->host_scribble;
+
+	hpsa_scatter_gather_unmap(c->h, c, cmd);
+}
+
+#endif
+
+#if !KFEATURE_HAS_SCSI_DEVICE_TYPE
+	/**
+	 * scsi_device_type - Return 17 char string indicating device type.
+	 * @type: type number to look up
+	 */
+	const char *scsi_device_type(unsigned type)
+	{
+		if (type == 0x1e)
+			return "Well-known LUN   ";
+		if (type == 0x1f)
+			return "No Device        ";
+		if (type >= ARRAY_SIZE(scsi_device_types))
+			return "Unknown          ";
+		return scsi_device_types[type];
+	}
+#endif
+
+#if KFEATURE_SCAN_START_IMPLEMENTED
+	/* .scan_start is present in scsi host template AND implemented.
+	 * Used to bail out of queuecommand if no scan_start and REPORT_LUNS
+	 * encountered
+	 */
+	static inline int hpsa_dummy_function_returning_zero(void)
+	{
+		return 0;
+	}
+
+#define bail_on_report_luns_if_no_scan_start(cmd, done) \
+		hpsa_dummy_function_returning_zero()
+
+	/* RHEL6, kernel.org have functioning ->scan_start() method in kernel
+	 * so this is no-op.
+	 */
+	static inline void hpsa_initial_update_scsi_devices(
+		__attribute__((unused)) struct ctlr_info *h)
+	{
+		return;
+	}
+#else /* not KFEATURE_SCAN_START_IMPLEMENTED */
+	static inline int bail_on_report_luns_if_no_scan_start(
+		struct scsi_cmnd *cmd, void (*done)(struct scsi_cmnd *))
+	{
+		/*
+		 * This thing bails out of our queue command early on SCSI
+		 * REPORT_LUNS This is needed when the kernel doesn't really
+		 * support the scan_start method of the scsi host template.
+		 *
+		 * Since we do our own mapping in our driver, and we handle
+		 * adding/removing of our own devices.
+		 *
+		 * We want to prevent the mid-layer from doing it's own
+		 * adding/removing of drives which is what it would do
+		 * if we allow REPORT_LUNS to be processed.
+		 *
+		 * On RHEL5, scsi mid-layer never calls scan_start and
+		 * scan_finished even though they exist in scsi_host_template.
+		 *
+		 * On RHEL6 we use scan_start and scan_finished to tell
+		 * mid-layer that we do our own device adding/removing
+		 * therefore we can handle REPORT_LUNS.
+		 */
+
+		if (cmd->cmnd[0] == REPORT_LUNS) {
+			cmd->result = (DID_OK << 16);           /* host byte */
+			cmd->result |= (COMMAND_COMPLETE << 8); /* msg byte */
+			cmd->result |= SAM_STAT_CHECK_CONDITION;
+			memset(cmd->sense_buffer, 0, sizeof(cmd->sense_buffer));
+			cmd->sense_buffer[2] = ILLEGAL_REQUEST;
+			done(cmd);
+			return 1;
+		}
+		return 0;
+	}
+
+	/* Need this if no functioning ->scan_start() method in kernel. */
+	static void hpsa_update_scsi_devices(struct ctlr_info *h, int hostno);
+	static inline void hpsa_initial_update_scsi_devices(
+				struct ctlr_info *h)
+	{
+		hpsa_update_scsi_devices(h, -1);
+	}
+#endif /* KFEATURE_SCAN_START_IMPLEMENTED */
+
+#if KFEATURE_SCAN_START_PRESENT
+	/* .scan_start is present in scsi host template */
+	#define INITIALIZE_SCAN_START(funcptr) .scan_start = funcptr,
+	#define INITIALIZE_SCAN_FINISHED(funcptr) .scan_finished = funcptr,
+#else /* .scan start is not even present in scsi host template */
+	#define INITIALIZE_SCAN_START(funcptr)
+	#define INITIALIZE_SCAN_FINISHED(funcptr)
+#endif
+
+#if KFEATURE_HAS_2011_03_QUEUECOMMAND
+#	define DECLARE_QUEUECOMMAND(func) \
+		static int func(struct Scsi_Host *sh, struct scsi_cmnd *cmd)
+#	define hpsa_scsi_done(cmd, done) \
+		(cmd)->scsi_done((cmd))
+#	define hpsa_save_scsi_done(cmd, done)
+#else
+#	define DECLARE_QUEUECOMMAND(func) \
+	static int func(struct scsi_cmnd *cmd, void (*done)(struct scsi_cmnd *))
+#	define hpsa_scsi_done(cmd, done) \
+		done((cmd))
+#	define hpsa_save_scsi_done(cmd, done) \
+		(cmd)->scsi_done = (done)
+#endif
+
+#if KFEATURE_HAS_HOST_LOCKLESS_FIELD
+#	define HPSA_SKIP_HOST_LOCK .lockless = 1,
+#else
+#	define HPSA_SKIP_HOST_LOCK
+#endif
+
+#if !KFEATURE_HAS_SCSI_SET_RESID
+	static inline void scsi_set_resid(struct scsi_cmnd *cmd, int resid)
+	{
+		cmd->resid = resid;
+	}
+#endif
+
+#ifndef DMA_BIT_MASK
+#define DMA_BIT_MASK(n) (((n) == 64) ? ~0ULL : ((1ULL<<(n))-1))
+#endif
+
+/* Define old style irq flags SA_* if the IRQF_* ones are missing. */
+#ifndef IRQF_DISABLED
+#define IRQF_DISABLED (SA_INTERRUPT | SA_SAMPLE_RANDOM)
+#endif
+
+#if KFEATURE_HAS_UACCESS_H_FILE
+#include <linux/uaccess.h>
+#endif
+
+#if KFEATURE_HAS_SMP_LOCK_H
+#include <linux/smp_lock.h>
+#endif
+
+/*
+ * Support for packaged storage solutions.
+ * Enabled by default for kernel.org 
+ * Enable above as required for distros.
+ */
+#if HPSA_SUPPORTS_STORAGEWORKS_1210m
+#define HPSA_STORAGEWORKS_1210m_PCI_IDS \
+	{PCI_VENDOR_ID_HP, PCI_DEVICE_ID_HP_CISSE, 0x103C, 0x3233}, \
+	{PCI_VENDOR_ID_HP, PCI_DEVICE_ID_HP_CISSF, 0x103C, 0x333F},	\
+	{PCI_VENDOR_ID_3PAR,	PCI_DEVICE_ID_3PAR,	0x1590, 0x0076},\
+	{PCI_VENDOR_ID_3PAR,	PCI_DEVICE_ID_3PAR,	0x1590, 0x007d},\
+	{PCI_VENDOR_ID_3PAR,	PCI_DEVICE_ID_3PAR,	0x1590, 0x0077},\
+	{PCI_VENDOR_ID_3PAR,	PCI_DEVICE_ID_3PAR,	0x1590, 0x0087},\
+	{PCI_VENDOR_ID_3PAR,	PCI_DEVICE_ID_3PAR,	0x1590, 0x0088},\
+	{PCI_VENDOR_ID_3PAR,	PCI_DEVICE_ID_3PAR,	0x1590, 0x0089},
+
+#define HPSA_STORAGEWORKS_1210m_PRODUCT_ENTRIES \
+	{0x3233103C, "HP StorageWorks 1210m", &SA5_access}, \
+	{0x333F103C, "HP StorageWorks 1210m", &SA5_access}, \
+   {0x00761590, "HP Storage P1224 Array Controller", &SA5_access}, \
+   {0x007d1590, "HP Storage P1228 Array Controller", &SA5_access}, \
+   {0x00771590, "HP Storage P1228m Array Controller", &SA5_access}, \
+   {0x00871590, "HP Storage P1224e Array Controller", &SA5_access}, \
+   {0x00881590, "HP Storage P1228e Array Controller", &SA5_access}, \
+   {0x00891590, "HP Storage P1228em Array Controller", &SA5_access},
+   
+
+#else
+#define HPSA_STORAGEWORKS_1210m_PCI_IDS	
+#define HPSA_STORAGEWORKS_1210m_PRODUCT_ENTRIES
+#endif
+
+/* sles10sp4 apparently doesn't have DIV_ROUND_UP.  Normally it comes
+ * from include/linux/kernel.h.  Other sles10's have it I think.
+ */
+#if !defined(DIV_ROUND_UP)
+#define DIV_ROUND_UP(n,d) (((n) + (d) - 1) / (d))
+#endif
+
+/* Newer dma_mapping_error function takes 2 args, older version only takes 1 arg.
+ * This macro makes the code do the right thing depending on which variant we have.
+ */
+#if KFEATURE_HAS_NEW_DMA_MAPPING_ERROR
+#define hpsa_dma_mapping_error(x, y) dma_mapping_error(x, y)
+#else
+#define hpsa_dma_mapping_error(x, y) dma_mapping_error(y)
+#endif
+
+#if !KFEATURE_HAS_IRQ_SET_AFFINITY_HINTS
+#define irq_set_affinity_hint(irq, cpu) (0)
+#endif
+
+#if !KFEATURE_HAS_ALLOC_WORKQUEUE
+/* Earlier implementations had no concept of WQ_MEM_RECLAIM so far as I know.
+ * FIXME: RHEL6 does not have WQ_MEM_RECLAIM flag.  What to do about this?
+ * WQ_MEM_RECLAIM is supposed to reserve a "rescue thread" so that the work
+ * queue can always make forward progress even in low memory situations. E.g.
+ * if OS is scrambling for memory and trying to swap out to disk, it is bad if
+ * the thread that is doing the swapping i/o needs to allocate.
+ */
+#define WQ_MEM_RECLAIM (0)
+#define alloc_workqueue(name, flags, max_active) __create_workqueue(name, flags, max_active, 0)
+#endif
+
+#if !KFEATURE_HAS_ATOMIC_DEC_IF_POSITIVE
+static inline int atomic_dec_if_positive(atomic_t *v)
+{
+	int c, old, dec;
+	c = atomic_read(v);
+	for (;;) {
+		dec = c - 1;
+		if (unlikely(dec < 0))
+			break;
+		old = atomic_cmpxchg((v), c, dec);
+		if (likely(old == c))
+			break;
+		c = old;
+	}
+	return dec;
+}
+#endif
+
+/* these next three disappeared in 3.8-rc4 */
+#ifndef __devinit
+#define __devinit
+#endif
+
+#ifndef __devexit
+#define __devexit
+#endif
+
+#ifndef __devexit_p
+#define __devexit_p(x) x
+#endif
+
+/* Kernel.org since about July 2013 has nice %XphN formatting for bytes
+ * Older kernels don't.  So we have this instead.
+ */
+#define phnbyte(x, n) ((int) ((x)[(n)]))
+#define phN16 "%02hhx%02hhx%02hhx%02hhx%02hhx%02hhx%02hhx%02hhx%02hhx%02hhx%02hhx%02hhx%02hhx%02hhx%02hhx%02hhx"
+#define phNbytes16(x) \
+	phnbyte((x), 0), phnbyte((x), 1), phnbyte((x), 2), phnbyte((x), 3), \
+	phnbyte((x), 4), phnbyte((x), 5), phnbyte((x), 6), phnbyte((x), 7), \
+	phnbyte((x), 8), phnbyte((x), 9), phnbyte((x), 10), phnbyte((x), 11), \
+	phnbyte((x), 12), phnbyte((x), 13), phnbyte((x), 14), phnbyte((x), 15)
+
-- 
2.4.3

