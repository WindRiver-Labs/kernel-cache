From f879b9f35a638c54d962caef9208c23eaec26e26 Mon Sep 17 00:00:00 2001
From: Donn Seeley <donn.seeley@windriver.com>
Date: Tue, 15 Jul 2008 15:26:03 -0400
Subject: [PATCH] Arm VFP Context

This patch implements context saving for the ARM VFP hardware floating
point unit in two specific circumstances:

  +     before a fork(), so that the child inherits the parent's floating
        point state; and

  +     across a caught signal, so that the use of floating point in a
        signal handler won't destroy caller-saved floating point state
        in the the interrupted context.

Few real programs actually need this functionality, but when they do
need it, the bugs can lead to data that's corrupted in a way that is
subtle and hard to identify.

Also note: the condition field "NE" in vfp instruction is not
supported. So add a instruction "beq" to replace it.

Besides, when FPEXC.EX is set to zero, the FPINST and FPINST2
registers are not implemented. So FPINST doesn't need to be restored when
FPEXC.EX is not set.

Signed-off-by: Donn Seeley <donn.seeley@windriver.com>
Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
Signed-off-by: Stanley.Miao <stanley.miao@windriver.com>
Signed-off-by: Bruce Ashfield <bruce.ashfield@windriver.com>

---
 arch/arm/include/asm/processor.h |    8 +++-
 arch/arm/include/asm/ucontext.h  |    2 +-
 arch/arm/kernel/process.c        |   16 ++++++
 arch/arm/kernel/signal.c         |   39 ++++++++++++++--
 arch/arm/vfp/vfp.h               |    3 +-
 arch/arm/vfp/vfphw.S             |   23 ++++++++-
 arch/arm/vfp/vfpmodule.c         |   96 ++++++++++++++++++++++++++++++++++++++
 7 files changed, 177 insertions(+), 10 deletions(-)

diff --git a/arch/arm/include/asm/processor.h b/arch/arm/include/asm/processor.h
index 517a4d6..c2a6693 100644
--- a/arch/arm/include/asm/processor.h
+++ b/arch/arm/include/asm/processor.h
@@ -86,7 +86,13 @@ struct task_struct;
 extern void release_thread(struct task_struct *);
 
 /* Prepare to copy thread state - unlazy all lazy status */
-#define prepare_to_copy(tsk)	do { } while (0)
+extern void prepare_to_copy(struct task_struct *);
+#ifdef CONFIG_VFP
+extern void vfp_task_disable(struct task_struct *tsk);
+struct thread_info;
+extern void vfp_task_copy(struct thread_info *, void *);
+extern void vfp_task_restore(struct thread_info *, void *);
+#endif
 
 unsigned long get_wchan(struct task_struct *p);
 
diff --git a/arch/arm/include/asm/ucontext.h b/arch/arm/include/asm/ucontext.h
index bf65e9f..b22b817 100644
--- a/arch/arm/include/asm/ucontext.h
+++ b/arch/arm/include/asm/ucontext.h
@@ -91,7 +91,7 @@ struct aux_sigframe {
 #ifdef CONFIG_IWMMXT
 	struct iwmmxt_sigframe	iwmmxt;
 #endif
-#if 0 && defined CONFIG_VFP /* Not yet saved.  */
+#ifdef CONFIG_VFP
 	struct vfp_sigframe	vfp;
 #endif
 	/* Something that isn't a valid magic number for any coprocessor.  */
diff --git a/arch/arm/kernel/process.c b/arch/arm/kernel/process.c
index 3fd8823..bf04035 100644
--- a/arch/arm/kernel/process.c
+++ b/arch/arm/kernel/process.c
@@ -328,6 +328,22 @@ void release_thread(struct task_struct *dead_task)
 
 asmlinkage void ret_from_fork(void) __asm__("ret_from_fork");
 
+void prepare_to_copy(struct task_struct *tsk)
+{
+#ifdef CONFIG_CRUNCH
+	if (elf_hwcap & HWCAP_CRUNCH)
+		crunch_task_disable(tsk);
+#endif
+#ifdef CONFIG_IWMMXT
+	if (test_ti_thread_flag(task_thread_info(tsk), TIF_USING_IWMMXT))
+		iwmmxt_task_disable(task_thread_info(tsk));
+#endif
+#ifdef CONFIG_VFP
+	if (elf_hwcap & HWCAP_VFP)
+		vfp_task_disable(tsk);
+#endif
+}
+
 int
 copy_thread(int nr, unsigned long clone_flags, unsigned long stack_start,
 	    unsigned long stk_sz, struct task_struct *p, struct pt_regs *regs)
diff --git a/arch/arm/kernel/signal.c b/arch/arm/kernel/signal.c
index ef2f86a..6b96536 100644
--- a/arch/arm/kernel/signal.c
+++ b/arch/arm/kernel/signal.c
@@ -196,6 +196,37 @@ static int restore_iwmmxt_context(struct iwmmxt_sigframe *frame)
 
 #endif
 
+#ifdef CONFIG_VFP
+
+static int preserve_vfp_context(struct vfp_sigframe *frame)
+{
+	char kbuf[sizeof(*frame) + 8];
+	struct vfp_sigframe *kframe;
+
+	kframe = (struct vfp_sigframe *)((unsigned long)(kbuf + 8) & ~7);
+	kframe->magic = VFP_MAGIC;
+	kframe->size = VFP_STORAGE_SIZE;
+	vfp_task_copy(current_thread_info(), &kframe->storage);
+	return __copy_to_user(frame, kframe, sizeof(*frame));
+}
+
+static int restore_vfp_context(struct vfp_sigframe *frame)
+{
+	char kbuf[sizeof(*frame) + 8];
+	struct vfp_sigframe *kframe;
+
+	kframe = (struct vfp_sigframe *)((unsigned long)(kbuf + 8) & ~7);
+	if (__copy_from_user(kframe, frame, sizeof(*frame)))
+		return -1;
+	if (kframe->magic != VFP_MAGIC ||
+	    kframe->size != VFP_STORAGE_SIZE)
+		return -1;
+	vfp_task_restore(current_thread_info(), &kframe->storage);
+	return 0;
+}
+
+#endif
+
 /*
  * Do a signal return; undo the signal stack.  These are aligned to 64-bit.
  */
@@ -254,8 +285,8 @@ static int restore_sigframe(struct pt_regs *regs, struct sigframe __user *sf)
 		err |= restore_iwmmxt_context(&aux->iwmmxt);
 #endif
 #ifdef CONFIG_VFP
-//	if (err == 0)
-//		err |= vfp_restore_state(&sf->aux.vfp);
+	if (err == 0 && (elf_hwcap & HWCAP_VFP) != 0)
+		err |= restore_vfp_context(&aux->vfp);
 #endif
 
 	return err;
@@ -369,8 +400,8 @@ setup_sigframe(struct sigframe __user *sf, struct pt_regs *regs, sigset_t *set)
 		err |= preserve_iwmmxt_context(&aux->iwmmxt);
 #endif
 #ifdef CONFIG_VFP
-//	if (err == 0)
-//		err |= vfp_save_state(&sf->aux.vfp);
+	if (err == 0 && (elf_hwcap & HWCAP_VFP) != 0)
+		err |= preserve_vfp_context(&aux->vfp);
 #endif
 	__put_user_error(0, &aux->end_magic, err);
 
diff --git a/arch/arm/vfp/vfp.h b/arch/arm/vfp/vfp.h
index c85860b..20ce519 100644
--- a/arch/arm/vfp/vfp.h
+++ b/arch/arm/vfp/vfp.h
@@ -377,6 +377,5 @@ struct op {
 	u32 flags;
 };
 
-#ifdef CONFIG_SMP
 extern void vfp_save_state(void *location, u32 fpexc);
-#endif
+extern void vfp_restore_state(void *location);
diff --git a/arch/arm/vfp/vfphw.S b/arch/arm/vfp/vfphw.S
index 353f9e5..a43b69c 100644
--- a/arch/arm/vfp/vfphw.S
+++ b/arch/arm/vfp/vfphw.S
@@ -166,7 +166,6 @@ process_exception:
 					@ required. If not, the user code will
 					@ retry the faulted instruction
 
-#ifdef CONFIG_SMP
 	.globl	vfp_save_state
 	.type	vfp_save_state, %function
 vfp_save_state:
@@ -182,7 +181,27 @@ vfp_save_state:
 	VFPFMRX	r12, FPINST2, NE	@ FPINST2 if needed (and present)
 	stmia	r0, {r1, r2, r3, r12}	@ save FPEXC, FPSCR, FPINST, FPINST2
 	mov	pc, lr
-#endif
+
+	.globl	vfp_restore_state
+	.type	vfp_restore_state,%function
+vfp_restore_state:
+	@ Inverse of vfp_save_state() -- restore VFP register state
+	@ r0 - address of saved register state
+	DBGSTR1 "restore VFP state %p", r0
+	VFPFLDMIA r0, r5                @ restore the working registers
+	ldmia	r0, {r1, r2, r3, r12}	@ load FPEXC, FPSCR, FPINST, FPINST2
+	tst	r1, #FPEXC_EX		@ is there additional state to write?
+	beq	1f
+	VFPFMXR FPINST, r3
+	tst	r1, #FPEXC_FP2V		@ is there an FPINST2 to write?
+	beq	1f
+	VFPFMXR FPINST2, r12	 	@ FPINST2 if needed - avoids writing
+					@ nonexistant reg on rev0
+1:
+	orr	r1, r1, #FPEXC_EN
+	VFPFMXR FPSCR, r2
+	VFPFMXR FPEXC, r1
+	mov	pc, lr
 
 last_VFP_context_address:
 	.word	last_VFP_context
diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index c0d2c9b..fed7421 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -322,6 +322,102 @@ static void vfp_enable(void *unused)
 	set_copro_access(access | CPACC_FULL(10) | CPACC_FULL(11));
 }
 
+/*
+ * Make sure that this task no longer owns the VFP unit
+ * and that its VFP state is saved.
+ */
+void vfp_task_disable(struct task_struct *tsk)
+{
+	struct thread_info *ti = task_thread_info(tsk);
+	u32 fpexc;
+	__u32 cpu;
+
+	preempt_disable();
+	cpu = ti->cpu;
+	fpexc = fmrx(FPEXC);
+	if (last_VFP_context[cpu] == &ti->vfpstate) {
+#ifdef CONFIG_SMP
+		/*
+		 * On SMP systems, if the VFP unit is disabled,
+		 * then our saved state is good.
+		 */
+		if ((fpexc & FPEXC_EN) != 0)
+			vfp_save_state(last_VFP_context[cpu], fpexc);
+#else
+		if ((fpexc & FPEXC_EN) == 0)
+			fmxr(FPEXC, fpexc | FPEXC_EN);
+		vfp_save_state(last_VFP_context[cpu], fpexc);
+#endif
+		last_VFP_context[cpu] = NULL;
+	}
+	fmxr(FPEXC, fpexc & ~FPEXC_EN);
+	preempt_enable();
+}
+
+/*
+ * Take a snapshot of our current VFP state.
+ */
+void vfp_task_copy(struct thread_info *ti, void *storage)
+{
+	u32 fpexc;
+	union vfp_state *vfp = storage;
+
+	preempt_disable();
+	fpexc = fmrx(FPEXC);
+
+#ifdef CONFIG_SMP
+	if ((fpexc & FPEXC_EN) != 0 &&
+	    last_VFP_context[ti->cpu] == &ti->vfpstate) {
+		vfp_save_state(vfp, fpexc);
+		preempt_enable();
+		return;
+	}
+#else
+	if (last_VFP_context[ti->cpu] == &ti->vfpstate) {
+		if ((fpexc & FPEXC_EN) == 0)
+			fmxr(FPEXC, fpexc | FPEXC_EN);
+		vfp_save_state(vfp, fpexc);
+		preempt_enable();
+		return;
+	}
+#endif
+	preempt_enable();
+	memcpy(vfp, &ti->vfpstate, sizeof *vfp);
+}
+
+/*
+ * Restore our VFP state from saved state.  If we own the VFP
+ * unit, we leave it enabled, with valid register contents;
+ * otherwise we just update the values in thread storage.
+ */
+void vfp_task_restore(struct thread_info *ti, void *storage)
+{
+	u32 fpexc;
+	union vfp_state *vfp = storage;
+
+	preempt_disable();
+	fpexc = fmrx(FPEXC);
+
+#ifdef CONFIG_SMP
+	if ((fpexc & FPEXC_EN) != 0 &&
+	    last_VFP_context[ti->cpu] == &ti->vfpstate) {
+		vfp_restore_state(vfp);
+		preempt_enable();
+		return;
+	}
+#else
+	if (last_VFP_context[ti->cpu] == &ti->vfpstate) {
+		if ((fpexc & FPEXC_EN) == 0)
+			fmxr(FPEXC, fpexc | FPEXC_EN);
+		vfp_restore_state(vfp);
+		preempt_enable();
+		return;
+	}
+#endif
+	preempt_enable();
+	memcpy(&ti->vfpstate, vfp, sizeof *vfp);
+}
+
 #include <linux/smp.h>
 
 /*
-- 
1.6.0.3

