From e90b02093754315389aa612b0a321adaf237fed0 Mon Sep 17 00:00:00 2001
From: Benjamin Walsh <benjamin.walsh@windriver.com>
Date: Thu, 16 Sep 2010 13:24:08 -0700
Subject: [PATCH 161/233] kexec/powerpc/SMP: add support for more than two CPUs

Generalize the SMP shutdown code so that it can handle more
than two CPUs. Needed for p4080 board.

Signed-off-by: Benjamin Walsh <benjamin.walsh@windriver.com>
---
 arch/powerpc/kernel/machine_kexec_32.c |   65 +++++++++++++++----------------
 arch/powerpc/kernel/misc_32.S          |   53 +++++++++++++++++++-------
 arch/powerpc/platforms/85xx/smp.c      |    7 +++-
 3 files changed, 76 insertions(+), 49 deletions(-)

diff --git a/arch/powerpc/kernel/machine_kexec_32.c b/arch/powerpc/kernel/machine_kexec_32.c
index c1df33e..98afb29 100644
--- a/arch/powerpc/kernel/machine_kexec_32.c
+++ b/arch/powerpc/kernel/machine_kexec_32.c
@@ -129,17 +129,19 @@ void default_machine_kexec(struct kimage *image)
 }
 
 #ifdef CONFIG_SMP
-/* CPU 1 will always be the one calling this function */
+/* secondary CPUs will always be the ones calling this function */
 static void _smp_kexec_secondary_cpu_down(void *arg)
 {
 	u32 rnkss; 	 /* relocate_new_kernel_secondary_spin */
 	u32 spin;	 /* addr of the relocated start address
-			  * variable on which CPU1 will spin */
+			  * variable on which CPUn will spin */
 	u32 ready;	 /* addr of the relocated ready variable */
 	rnk_t rnk;	 /* relocate_new_kernel() */
 
 	struct kimage *image = (struct kimage *)arg;
 
+	local_irq_disable();
+
 	rnkss = (u32)&relocate_new_kernel_secondary_spin;
 	rnkss = virt_to_phys((void *)kexec_find_reloc(image, rnkss));
 
@@ -152,8 +154,6 @@ static void _smp_kexec_secondary_cpu_down(void *arg)
 	rnk = (rnk_t)kexec_find_reloc((struct kimage *)arg,
 				(u32)relocate_new_kernel);
 
-	local_irq_disable();
-
 	flush_icache_range((u32)rnk, (u32)rnk + KEXEC_CONTROL_PAGE_SIZE);
 
 	rnk((unsigned long *)spin, (unsigned long *)ready, rnkss);
@@ -165,15 +165,17 @@ static void _smp_kexec_wait_for_secondaries(void *arg)
 	volatile u32 *ready;	 /* addr of the relocated ready variable,
 				  * we spin on it, don't want it to be
 				  * optimized out. */
+	unsigned int ncpus;
 
 	local_irq_disable();
 	ready = (void*)kexec_find_reloc((struct kimage *)arg,
 				(u32)&relocate_new_kernel_ready);
-	while(!(*ready)) {
+	ncpus = num_online_cpus()-1;
+	while(*ready != ncpus) {
 		cpu_relax();
 	}
-	mdelay(1);	/* should be plenty for cpu1 to start spinning
-			 * on the start address variable */
+	mdelay(1);	/* should be plenty for secondary CPUs to start
+			 * spinning on the start address variable */
 }
 
 static void _smp_kexec_leave_kernel(void *arg)
@@ -182,15 +184,24 @@ static void _smp_kexec_leave_kernel(void *arg)
 	kexec_leave_kernel(arg);
 }
 
+static void _smp_handle_non_kexecing_cpu(void *arg)
+{
+	/* get hardware CPU# from special Processor Identity Register */
+	if (mfspr(SPRN_PIR) == 0) {
+		_smp_kexec_leave_kernel(arg);
+	} else {
+		_smp_kexec_secondary_cpu_down(arg);
+	}
+	/* NOT REACHED */
+}
+
 static struct kimage * __crash_smp_flag = NULL;
 void default_kexec_stop_cpus(void *arg)
 {
-	int cpu;
-
 	/* Initialization from head_[32|fsl_booke].S expects HW CPU #0 as
-	 * the boot CPU: thus, if we're CPU1, call CPU0 and have it do
+	 * the boot CPU: thus, if we're CPUn, call CPU0 and have it do
 	 * the rest of the shutdown sequence, then put ourselves on
-	 * a spin; if we're CPU0, call CPU1 to put itself on a spin,
+	 * a spin; if we're CPU0, call CPUn to put themselves on a spin,
 	 * then do the rest of the shutdown sequence. */
 
 	/* if this is coming while handling a crash, the CPU that did not
@@ -198,26 +209,19 @@ void default_kexec_stop_cpus(void *arg)
 	 * get it out of that loop and execute its normal kexec shutdown
 	 * sequence */
 	preempt_disable();
+	/* shutdown cpu 1 and wait for it */
+	if(kexec_is_handling_crash()) {
+		__crash_smp_flag = arg;
+		smp_mb();
+	} else {
+		smp_call_function(_smp_handle_non_kexecing_cpu, arg, 0);
+	}
 	/* get hardware CPU# from special Processor Identity Register */
-	cpu = mfspr(SPRN_PIR);
-	if (0 == cpu) {
-		/* shutdown cpu 1 and wait for it */
-		if(kexec_is_handling_crash()) {
-			__crash_smp_flag = arg;
-			smp_mb();
-		} else {
-			smp_call_function(_smp_kexec_secondary_cpu_down, arg, 0);
-		}
+	if (mfspr(SPRN_PIR) == 0) {
 		_smp_kexec_wait_for_secondaries(arg);
 
 		/* was called from default_machine_kexec, continues there */
 	} else {
-		if(kexec_is_handling_crash()) {
-			__crash_smp_flag = arg;
-			smp_mb();
-		} else {
-			smp_call_function(_smp_kexec_leave_kernel, arg, 0);
-		}
 		_smp_kexec_secondary_cpu_down(arg);
 
 		/* not reached, going to wait on
@@ -227,17 +231,10 @@ void default_kexec_stop_cpus(void *arg)
 
 void kexec_smp_wait(void)
 {
-	int cpu;
 	while(!__crash_smp_flag) {
 		cpu_relax();
 	}
-	/* get hardware CPU# from special Processor Identity Register */
-	cpu = mfspr(SPRN_PIR);
-	if(0 == cpu) {
-		_smp_kexec_leave_kernel(__crash_smp_flag);
-	} else {
-		_smp_kexec_secondary_cpu_down(__crash_smp_flag);
-	}
+	_smp_handle_non_kexecing_cpu(__crash_smp_flag);
 }
 
 #endif /* CONFIG_SMP */
diff --git a/arch/powerpc/kernel/misc_32.S b/arch/powerpc/kernel/misc_32.S
index 6fde005..c055d89 100644
--- a/arch/powerpc/kernel/misc_32.S
+++ b/arch/powerpc/kernel/misc_32.S
@@ -727,6 +727,13 @@ _GLOBAL(__main)
 	blr
 
 #ifdef CONFIG_KEXEC
+
+/* e500mc has 64 entries in TLB1, e500vX have 16 */
+#ifdef CONFIG_PPC_E500MC
+#define MAS0_ESEL_MASK_MSB 10
+#else
+#define MAS0_ESEL_MASK_MSB 12
+#endif
 	/*
 	 * Must be relocatable PIC code callable as a C function.
 	 */
@@ -736,7 +743,7 @@ relocate_new_kernel:
 	/* r3 = page_list   */
 	/* r4 = reboot_code_buffer */
 	/* r5 = start_address      */
-	/* CPU1: */
+	/* CPUn: */
 	/* r3 = spin variable   */
 	/* r4 = ready variable */
 	/* r5 = address of relocate_new_kernel_secondary_spin */
@@ -749,6 +756,7 @@ relocate_new_kernel:
 	 *
 	 * First, invalidate the TLB0 entries
 	 */
+
 	li	r6, 0x04
 	tlbivax	0, r6
 #ifdef CONFIG_SMP
@@ -766,8 +774,9 @@ relocate_new_kernel:
 	mr	r8, r6
 	subf	r6, r7, r6
 	mtctr	r6
+
 1:
-	rlwinm	r6, r7, 16, 12, 15
+	rlwinm	r6, r7, 16, MAS0_ESEL_MASK_MSB, 15
 	oris	r6, r6, 0x1000
 	mtspr	SPRN_MAS0, r6
 	tlbre
@@ -789,7 +798,7 @@ relocate_new_kernel:
 	li	r6, 3			/* number of TLBs to copy */
 	mtctr	r6
 1:
-	rlwinm	r6, r7, 16, 12, 15
+	rlwinm	r6, r7, 16, MAS0_ESEL_MASK_MSB, 15
 	oris	r6, r6, 0x1000
 	mtspr	SPRN_MAS0, r6
 	tlbre
@@ -799,7 +808,7 @@ relocate_new_kernel:
 	subf	r6, r0, r6		/* identity map */
 	mtspr	SPRN_MAS2, r6
 
-	rlwinm	r6, r8, 16, 12, 15
+	rlwinm	r6, r8, 16, MAS0_ESEL_MASK_MSB, 15
 	oris	r6, r6, 0x1000
 	mtspr	SPRN_MAS0, r6
 	tlbwe
@@ -824,7 +833,7 @@ relocate_new_kernel:
 	mfspr	r6, SPRN_PIR
 	cmpwi	r6, 0
 	beq	cpu0
-	addi	r8, r4, 1f - relocate_new_kernel_ready
+	addi	r8, r3, 1f - relocate_new_kernel_spin_addr
 	b	all_cpus
 #endif
 cpu0:
@@ -838,6 +847,7 @@ all_cpus:
 	/* from this point address translation is turned off */
 	/* and interrupts are disabled */
 
+
 #ifdef CONFIG_SMP
 	/* if not CPU0, jump to spin */
 	mfspr	r6, SPRN_PIR
@@ -940,9 +950,13 @@ relocate_new_kernel_secondary_spin:
 	/* r4 contains the ready address */
 
 	/* signal CPU0 that we've left the kernel */
-	lis	r5, 1
-	stw	r5, 0(r4)
-	sync
+	li r5, 0
+1:
+	sync	/* relax */
+	lwarx	r5, 0, r4
+	addi	r5, r5, 1
+	stwcx.	r5, 0, r4
+	bne-	1b
 
 	/* spin waiting for a non-zero address to branch to */
 1:	sync	/* relax */
@@ -971,20 +985,31 @@ relocate_new_kernel_size:
 	.globl kexec_secondary_hold_addr
 kexec_secondary_hold_addr:
 	.long 0
+	.globl kexec_secondary_cpu_to_wakeup
+kexec_secondary_cpu_to_wakeup:
+	.long 0
 
 	.globl kexec_secondary_hold
 kexec_secondary_hold:
-	lis	r3, kexec_secondary_hold_addr@h
-	ori	r3, r3, kexec_secondary_hold_addr@l
+	mfspr	r5, SPRN_PIR /* who are we ? */
+
+	lis	r3, kexec_secondary_cpu_to_wakeup@h
+	ori	r3, r3, kexec_secondary_cpu_to_wakeup@l
 	subis	r3, r3, PAGE_OFFSET@h
 
-	/* spin waiting for a non-zero address to branch to */
+	/* spin waiting for our CPU number to come up */
 1:	sync	/* relax */
-	lwz	r5, 0(r3)
-	cmpwi	r5, 0
-	beq	1b
+	lwz	r20, 0(r3)
+	cmpw	r20, r5
+	bne	1b
 	/* end of spin loop */
 
+	/* load address to branch to */
+	lis	r3, kexec_secondary_hold_addr@h
+	ori	r3, r3, kexec_secondary_hold_addr@l
+	subis	r3, r3, PAGE_OFFSET@h
+	lwz	r5, 0(r3)
+
 	isync
 	sync
 	mtlr	r5
diff --git a/arch/powerpc/platforms/85xx/smp.c b/arch/powerpc/platforms/85xx/smp.c
index f6a8735..a59e69a 100644
--- a/arch/powerpc/platforms/85xx/smp.c
+++ b/arch/powerpc/platforms/85xx/smp.c
@@ -108,6 +108,7 @@ smp_85xx_kick_cpu(int nr)
 }
 #else
 extern u32 kexec_secondary_hold_addr;
+extern u32 kexec_secondary_cpu_to_wakeup;
 static void __init smp_85xx_kick_cpu(int nr)
 {
 	unsigned long flags;
@@ -124,7 +125,11 @@ static void __init smp_85xx_kick_cpu(int nr)
 	 * point: release it and make it start its true kernel execution,
 	 * at __early_start() */
 
-	kexec_secondary_hold_addr = (u32)__pa(__early_start);
+	if (!kexec_secondary_hold_addr) {
+		kexec_secondary_hold_addr = (u32)__pa(__early_start);
+		mb();
+	}
+	kexec_secondary_cpu_to_wakeup = (u32)nr;
 	mb();
 
 	/* Wait a bit for the CPU to ack. */
-- 
1.7.0.4

