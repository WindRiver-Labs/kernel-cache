From 89388b893ec6dc1b3a03ab2809a5bb741634d515 Mon Sep 17 00:00:00 2001
From: Andrew Liu <shengping.liu@windriver.com>
Date: Mon, 20 Dec 2010 10:59:45 +0800
Subject: [PATCH 05/28] QMAN: Update Qman subsystem

Patch taken from FSL vendor SDK 2.2.

Update Freescale Queue Manager (datapath) support.

Add Qman error interrupt handler and its Initializaiotn function.
Add Qman debug filesystem in file: qman_debugfs.c
Change qman_low.c to qman_low.h and rerrange its content.
Modify code style.
Add the support of deferring Qman interrupt to tasklet.
Defer Qman initiallization to kernel thread: __init_affine_portal
Update bman self-test files.

Integrated-by: Andrew Liu <shengping.liu@windriver.com>
---
 drivers/hwqueue/Kconfig               |   43 +-
 drivers/hwqueue/Makefile              |    7 +-
 drivers/hwqueue/qman_config.c         |  465 ++++++++++-
 drivers/hwqueue/qman_debugfs.c        |  841 ++++++++++++++++++
 drivers/hwqueue/qman_driver.c         |  197 +++---
 drivers/hwqueue/qman_fqalloc.c        |   11 +-
 drivers/hwqueue/qman_high.c           | 1531 +++++++++++++++++++++------------
 drivers/hwqueue/qman_low.c            | 1189 -------------------------
 drivers/hwqueue/qman_low.h            | 1247 +++++++++++++++++++++++++++
 drivers/hwqueue/qman_private.h        |  314 +++++---
 drivers/hwqueue/qman_sys.h            |   51 +-
 drivers/hwqueue/qman_test.c           |    8 +-
 drivers/hwqueue/qman_test_fqrange.c   |   65 --
 drivers/hwqueue/qman_test_high.c      |   16 +-
 drivers/hwqueue/qman_test_hotpotato.c |   30 +-
 drivers/hwqueue/qman_utility.c        |    2 +-
 include/linux/fsl_qman.h              |  904 +++++++++-----------
 17 files changed, 4321 insertions(+), 2600 deletions(-)
 create mode 100644 drivers/hwqueue/qman_debugfs.c
 delete mode 100644 drivers/hwqueue/qman_low.c
 create mode 100644 drivers/hwqueue/qman_low.h
 delete mode 100644 drivers/hwqueue/qman_test_fqrange.c

diff --git a/drivers/hwqueue/Kconfig b/drivers/hwqueue/Kconfig
index 952dd07..cc375e6 100644
--- a/drivers/hwqueue/Kconfig
+++ b/drivers/hwqueue/Kconfig
@@ -11,7 +11,7 @@ if FSL_QMAN
 
 config FSL_QMAN_CHECKING
 	bool "additional driver checking"
-	default y
+	default n
 	---help---
 	  Compiles in additional checks to sanity-check the Qman driver and any
 	  use of it by other code. Not recommended for performance.
@@ -23,6 +23,15 @@ config FSL_QMAN_PORTAL
 	  Compiles support to detect and support Qman software corenet portals
 	  (as provided by the device-tree).
 
+config FSL_QMAN_PORTAL_TASKLET
+	bool "Defer interrupt processing to a tasklet"
+	depends on FSL_QMAN_PORTAL
+	default n
+	---help---
+	  Rather than processing DQRR and MR ring entries and congestion group
+	  state-change notifications all within the interrupt handler, defer
+	  this work to a tasklet.
+
 config FSL_QMAN_BUG_AND_FEATURE_REV1
 	bool "workarounds for errata and missing features in p4080 rev1"
 	depends on FSL_QMAN_PORTAL
@@ -37,29 +46,12 @@ config FSL_QMAN_BUG_AND_FEATURE_REV1
 
 	  If in doubt, say Y.
 
-# The current driver is interrupt-driven only (poll-driven isn't yet supported).
-config FSL_QMAN_HAVE_POLL
-	bool
-	default n
-
 config FSL_QMAN_POLL_LIMIT
 	int
 	default 32
 
-config FSL_QMAN_PORTAL_DISABLEAUTO
-	bool "disable auto-initialisation of cpu-affine portals"
-	depends on FSL_QMAN_PORTAL
-	default n
-	---help---
-	  The high-level portal API, in its normal usage, requires that each cpu
-	  have a portal assigned to it that is auto-initialised. If an
-	  application is manually initialising portals in a non-cpu-affine
-	  manner (or you are using the low-level portal API), this may need to
-	  be disabled. If in doubt, say N.
-
 config FSL_QMAN_PORTAL_DISABLEAUTO_DCA
 	bool "disable discrete-consumption support on cpu-affine portals"
-	depends on !FSL_QMAN_PORTAL_DISABLEAUTO
 	default n
 	---help---
 	  By default, auto-initialised cpu-affine portals support
@@ -68,7 +60,7 @@ config FSL_QMAN_PORTAL_DISABLEAUTO_DCA
 
 config FSL_QMAN_FQALLOCATOR
 	bool "use Bman buffer pool 0 as a Qman FQ allocator"
-	depends on FSL_QMAN_PORTAL && !FSL_BMAN_PORTAL_DISABLEAUTO
+	depends on FSL_QMAN_PORTAL && FSL_BMAN_PORTAL
 	default y
 	---help---
 	  If enabled, the Qman driver will assume that Bman buffer pool 0 has
@@ -99,7 +91,7 @@ config FSL_QMAN_TEST
 
 config FSL_QMAN_TEST_STASH_POTATO
 	bool "Qman 'hot potato' data-stashing self-test"
-	depends on FSL_QMAN_TEST && FSL_QMAN_FQALLOCATOR && !FSL_QMAN_PORTAL_DISABLEAUTO
+	depends on FSL_QMAN_TEST && FSL_QMAN_FQALLOCATOR
 	default y
 	---help---
 	  This performs a "hot potato" style test enqueuing/dequeuing a frame
@@ -108,7 +100,7 @@ config FSL_QMAN_TEST_STASH_POTATO
 
 config FSL_QMAN_TEST_HIGH
 	bool "Qman high-level self-test"
-	depends on FSL_QMAN_TEST && !FSL_QMAN_PORTAL_DISABLEAUTO
+	depends on FSL_QMAN_TEST
 	default y
 	---help---
 	  This requires the presence of cpu-affine portals, and performs
@@ -117,12 +109,19 @@ config FSL_QMAN_TEST_HIGH
 
 config FSL_QMAN_TEST_ERRATA
 	bool "Qman errata-handling self-test"
-	depends on FSL_QMAN_TEST && !FSL_QMAN_PORTAL_DISABLEAUTO
+	depends on FSL_QMAN_TEST
 	default y
 	---help---
 	  This requires the presence of cpu-affine portals, and performs
 	  testing that handling for known hardware-errata is correct.
 
+config FSL_QMAN_DEBUGFS
+	tristate "Qman debugfs interface"
+	depends on FSL_QMAN_PORTAL
+	default y
+	---help---
+	This option compiles qman debugfs code for Qman.
+
 # H/w settings that can be hard-coded for now.
 
 # Corenet initiator settings. Stash request queues are 4-deep to match cores'
diff --git a/drivers/hwqueue/Makefile b/drivers/hwqueue/Makefile
index d51855b..db35b39 100644
--- a/drivers/hwqueue/Makefile
+++ b/drivers/hwqueue/Makefile
@@ -1,9 +1,10 @@
 obj-$(CONFIG_FSL_QMAN)		+= qman_utility.o
 obj-$(CONFIG_FSL_QMAN_CONFIG)	+= qman_config.o
-obj-$(CONFIG_FSL_QMAN_PORTAL)	+= qman_driver.o qman_low.o qman_high.o
+obj-$(CONFIG_FSL_QMAN_PORTAL)	+= qman_driver.o qman_high.o
 obj-$(CONFIG_FSL_QMAN_FQALLOCATOR) += qman_fqalloc.o
 obj-$(CONFIG_FSL_QMAN_TEST)	+= qman_tester.o
 qman_tester-y			 = qman_test.o qman_test_hotpotato.o \
-				qman_test_high.o
-qman_tester-$(CONFIG_FSL_QMAN_FQRANGE) += qman_test_fqrange.o
+					qman_test_high.o
 qman_tester-$(CONFIG_FSL_QMAN_TEST_ERRATA) += qman_test_errata.o
+obj-$(CONFIG_FSL_QMAN_DEBUGFS) += qman_debugfs_interface.o
+qman_debugfs_interface-y	= qman_debugfs.o
diff --git a/drivers/hwqueue/qman_config.c b/drivers/hwqueue/qman_config.c
index 392196f..a0ac830 100644
--- a/drivers/hwqueue/qman_config.c
+++ b/drivers/hwqueue/qman_config.c
@@ -45,12 +45,14 @@
 #define REG_DD_CFG		0x0200
 #define REG_DCP_CFG(n)		(0x0300 + ((n) * 0x10))
 #define REG_DCP_DD_CFG(n)	(0x0304 + ((n) * 0x10))
+#define REG_DCP_DLM_AVG(n)	(0x030c + ((n) * 0x10))
 #define REG_PFDR_FPC		0x0400
 #define REG_PFDR_FP_HEAD	0x0404
 #define REG_PFDR_FP_TAIL	0x0408
 #define REG_PFDR_FP_LWIT	0x0410
 #define REG_PFDR_CFG		0x0414
 #define REG_SFDR_CFG		0x0500
+#define REG_SFDR_IN_USE		0x0504
 #define REG_WQ_CS_CFG(n)	(0x0600 + ((n) * 0x04))
 #define REG_WQ_DEF_ENC_WQID	0x0630
 #define REG_WQ_SC_DD_CFG(n)	(0x640 + ((n) * 0x04))
@@ -59,9 +61,15 @@
 #define REG_WQ_DC1_DD_CFG(n)	(0x700 + ((n) * 0x04))
 #define REG_WQ_DCn_DD_CFG(n)	(0x6c0 + ((n) * 0x40)) /* n=2,3 */
 #define REG_CM_CFG		0x0800
+#define REG_ECSR		0x0a00
+#define REG_ECIR		0x0a04
+#define REG_EADR		0x0a08
+#define REG_EDATA(n)		(0x0a10 + ((n) * 0x04))
+#define REG_SBEC(n)		(0x0a80 + ((n) * 0x04))
 #define REG_MCR			0x0b00
 #define REG_MCP(n)		(0x0b04 + ((n) * 0x04))
 #define REG_HID_CFG		0x0bf0
+#define REG_IDLE_STAT		0x0bf4
 #define REG_IP_REV_1		0x0bf8
 #define REG_IP_REV_2		0x0bfc
 #define REG_FQD_BARE		0x0c00
@@ -73,6 +81,7 @@
 #define REG_CI_SCHED_CFG	0x0d00
 #define REG_SRCIDR		0x0d04
 #define REG_LIODNR		0x0d08
+#define REG_CI_RLM_AVG		0x0d14
 #define REG_ERR_ISR		0x0e00	/* + "enum qm_isr_reg" */
 
 /* Assists for QMAN_MCR */
@@ -108,8 +117,9 @@ enum qm_memory {
 #define QM_EIRQ_CTDE	0x10000000	/* Corenet Target Data Error */
 #define QM_EIRQ_CITT	0x08000000	/* Corenet Invalid Target Transaction */
 #define QM_EIRQ_PLWI	0x04000000	/* PFDR Low Watermark */
-#define QM_EIRQ_MBEI	0x01000000	/* Multi-bit ECC Error */
-#define QM_EIRQ_SBEI	0x00800000	/* Single-bit ECC Error */
+#define QM_EIRQ_MBEI	0x02000000	/* Multi-bit ECC Error */
+#define QM_EIRQ_SBEI	0x01000000	/* Single-bit ECC Error */
+#define QM_EIRQ_PEBI	0x00800000	/* PFDR Enqueues Blocked Interrupt */
 #define QM_EIRQ_ICVI	0x00010000	/* Invalid Command Verb */
 #define QM_EIRQ_IDDI	0x00000800	/* Invalid Dequeue (Direct-connect) */
 #define QM_EIRQ_IDFI	0x00000400	/* Invalid Dequeue FQ */
@@ -120,6 +130,85 @@ enum qm_memory {
 #define QM_EIRQ_IECI	0x00000002	/* Invalid Enqueue Channel */
 #define QM_EIRQ_IEQI	0x00000001	/* Invalid Enqueue Queue */
 
+/* QMAN_ECIR valid error bit */
+#define PORTAL_ECSR_ERR	(QM_EIRQ_IEQI | QM_EIRQ_IESI | QM_EIRQ_IEOI | \
+				QM_EIRQ_IDQI | QM_EIRQ_IDSI | QM_EIRQ_IDFI | \
+				QM_EIRQ_IDDI | QM_EIRQ_ICVI)
+#define FQID_ECSR_ERR	(QM_EIRQ_IEQI | QM_EIRQ_IECI | QM_EIRQ_IESI | \
+			QM_EIRQ_IEOI | QM_EIRQ_IDQI | QM_EIRQ_IDFI)
+
+union qman_ecir {
+	u32 ecir_raw;
+	struct {
+		u32 __reserved:2;
+		u32 portal_type:1;
+		u32 portal_num:5;
+		u32 fqid:24;
+	} __packed info;
+};
+
+union qman_eadr {
+	u32 eadr_raw;
+	struct {
+		u32 __reserved1:4;
+		u32 memid:4;
+		u32 __reserved2:12;
+		u32 eadr:12;
+	} __packed info;
+};
+
+struct qman_hwerr_txt {
+	u32 mask;
+	const char *txt;
+};
+
+#define QMAN_HWE_TXT(a, b) { .mask = QM_EIRQ_##a, .txt = b }
+
+static const struct qman_hwerr_txt qman_hwerr_txts[] = {
+	QMAN_HWE_TXT(CIDE, "Corenet Initiator Data Error"),
+	QMAN_HWE_TXT(CTDE, "Corenet Target Data Error"),
+	QMAN_HWE_TXT(CITT, "Corenet Invalid Target Transaction"),
+	QMAN_HWE_TXT(PLWI, "PFDR Low Watermark"),
+	QMAN_HWE_TXT(MBEI, "Multi-bit ECC Error"),
+	QMAN_HWE_TXT(SBEI, "Single-bit ECC Error"),
+	QMAN_HWE_TXT(PEBI, "PFDR Enqueues Blocked Interrupt"),
+	QMAN_HWE_TXT(ICVI, "Invalid Command Verb"),
+	QMAN_HWE_TXT(IDDI, "Invalid Dequeue (Direct-connect)"),
+	QMAN_HWE_TXT(IDFI, "Invalid Dequeue FQ"),
+	QMAN_HWE_TXT(IDSI, "Invalid Dequeue Source"),
+	QMAN_HWE_TXT(IDQI, "Invalid Dequeue Queue"),
+	QMAN_HWE_TXT(IEOI, "Invalid Enqueue Overflow"),
+	QMAN_HWE_TXT(IESI, "Invalid Enqueue State"),
+	QMAN_HWE_TXT(IECI, "Invalid Enqueue Channel"),
+	QMAN_HWE_TXT(IEQI, "Invalid Enqueue Queue")
+};
+#define QMAN_HWE_COUNT (sizeof(qman_hwerr_txts)/sizeof(struct qman_hwerr_txt))
+
+struct qman_error_info_mdata {
+	u16 addr_mask;
+	u16 bits;
+	const char *txt;
+};
+
+#define QMAN_ERR_MDATA(a, b, c) { .addr_mask = a, .bits = b, .txt = c}
+static const struct qman_error_info_mdata error_mdata[] = {
+	QMAN_ERR_MDATA(0x01FF, 24, "FQD cache tag memory 0"),
+	QMAN_ERR_MDATA(0x01FF, 24, "FQD cache tag memory 1"),
+	QMAN_ERR_MDATA(0x01FF, 24, "FQD cache tag memory 2"),
+	QMAN_ERR_MDATA(0x01FF, 24, "FQD cache tag memory 3"),
+	QMAN_ERR_MDATA(0x0FFF, 512, "FQD cache memory"),
+	QMAN_ERR_MDATA(0x07FF, 128, "SFDR memory"),
+	QMAN_ERR_MDATA(0x01FF, 72, "WQ context memory"),
+	QMAN_ERR_MDATA(0x00FF, 240, "CGR memory"),
+	QMAN_ERR_MDATA(0x00FF, 302, "Internal Order Restoration List memory"),
+	QMAN_ERR_MDATA(0x01FF, 256, "SW portal ring memory"),
+};
+#define QMAN_ERR_MDATA_COUNT \
+	(sizeof(error_mdata)/sizeof(struct qman_error_info_mdata))
+
+/* Add this in Kconfig */
+#define QMAN_ERRS_TO_UNENABLE (QM_EIRQ_PLWI | QM_EIRQ_PEBI)
+
 /**
  * qm_err_isr_<reg>_<verb> - Manipulate global interrupt registers
  * @v: for accessors that write values, this is the 32-bit value
@@ -144,8 +233,7 @@ enum qm_memory {
  * Keeping a list here of Qman registers I have not yet covered;
  * QCSP_DD_IHRSR, QCSP_DD_IHRFR, QCSP_DD_HASR,
  * DCP_DD_IHRSR, DCP_DD_IHRFR, DCP_DD_HASR, CM_CFG,
- * QMAN_ECSR, QMAN_ECIR, QMAN_EADR, QMAN_EECC, QMAN_EDATA0-7,
- * QMAN_SBET, QMAN_EINJ, QMAN_SBEC0-12
+ * QMAN_EECC, QMAN_SBET, QMAN_EINJ, QMAN_SBEC0-12
  */
 
 /* Encapsulate "struct qman *" as a cast of the register space address. */
@@ -166,8 +254,6 @@ static inline void __qm_out(struct qman *qm, u32 offset, u32 val)
 #define qm_in(reg)		__qm_in(qm, REG_##reg)
 #define qm_out(reg, val)	__qm_out(qm, REG_##reg, val)
 
-#if 0
-
 static u32 __qm_err_isr_read(struct qman *qm, enum qm_isr_reg n)
 {
 	return __qm_in(qm, REG_ERR_ISR + (n << 2));
@@ -178,6 +264,8 @@ static void __qm_err_isr_write(struct qman *qm, enum qm_isr_reg n, u32 val)
 	__qm_out(qm, REG_ERR_ISR + (n << 2), val);
 }
 
+#if 0
+
 static void qm_set_portal(struct qman *qm, u8 swportalID,
 			u16 ec_tp_cfg, u16 ecd_tp_cfg)
 {
@@ -344,8 +432,8 @@ static void qm_get_version(struct qman *qm, u16 *id, u8 *major, u8 *minor)
 	*minor = v & 0xff;
 }
 
-static void qm_set_memory(struct qman *qm, enum qm_memory memory, u16 eba,
-			u32 ba, int enable, int prio, int stash, u32 size)
+static void qm_set_memory(struct qman *qm, enum qm_memory memory, u64 ba,
+			int enable, int prio, int stash, u32 size)
 {
 	u32 offset = (memory == qm_memory_fqd) ? REG_FQD_BARE : REG_PFDR_BARE;
 	u32 exp = ilog2(size);
@@ -354,8 +442,8 @@ static void qm_set_memory(struct qman *qm, enum qm_memory memory, u16 eba,
 			is_power_of_2(size));
 	/* choke if 'ba' has lower-alignment than 'size' */
 	QM_ASSERT(!(ba & (size - 1)));
-	__qm_out(qm, offset, eba);
-	__qm_out(qm, offset + REG_offset_BAR, ba);
+	__qm_out(qm, offset, upper_32_bits(ba));
+	__qm_out(qm, offset + REG_offset_BAR, lower_32_bits(ba));
 	__qm_out(qm, offset + REG_offset_AR,
 		(enable ? 0x80000000 : 0) |
 		(prio ? 0x40000000 : 0) |
@@ -435,14 +523,8 @@ static __init int parse_mem_property(struct device_node *node, const char *name,
 		return 0;
 	}
 	pr_info("Using %s property '%s'\n", node->full_name, name);
-	/* Props are 64-bit, but dma_addr_t is (currently) 32-bit */
-	BUG_ON(sizeof(*addr) != 4);
-	BUG_ON(pint[2]);
-	*addr = pint[1];
-#ifdef CONFIG_PHYS_64BIT
-	*addr |= (u64)pint[0] << 32;
-#endif
-	*sz = pint[3];
+	*addr = ((u64)pint[0] << 32) | (u64)pint[1];
+	*sz = ((u64)pint[2] << 32) | (u64)pint[3];
 	/* Keep things simple, it's either all in the DRAM range or it's all
 	 * outside. */
 	if (*addr < lmb_end_of_DRAM()) {
@@ -482,9 +564,6 @@ static u32 qm_get_fqd_bar(void)
 
 /* TODO:
  * - there is obviously no handling of errors,
- * - the calls to qm_set_memory() pass no upper-bits, the physical addresses
- *   are cast on the assumption that they are <= 32bits. We BUG_ON() to handle
- *   this for now,
  * - the calls to qm_set_memory() hard-code the priority and CPC-stashing for
  *   both memory resources to zero.
  */
@@ -519,15 +598,15 @@ static int __init fsl_qman_init(struct device_node *node)
 		ret = parse_mem_property(node, "fsl,qman-pfdr", &pfdr_a, &pfdr_sz, 0);
 		BUG_ON(ret);
 		/* FQD memory */
-		qm_set_memory(qm, qm_memory_fqd, (u16)((u64)fqd_a >> 32), (u32)fqd_a,
-				1, 0, 0, fqd_sz);
+		qm_set_memory(qm, qm_memory_fqd, fqd_a, 1, 0, 0, fqd_sz);
 		/* PFDR memory */
-		qm_set_memory(qm, qm_memory_pfdr, (u16)((u64)pfdr_a >> 32), (u32)pfdr_a,
-				1, 0, 0, pfdr_sz);
+		qm_set_memory(qm, qm_memory_pfdr, pfdr_a, 1, 0, 0, pfdr_sz);
 		qm_init_pfdr(qm, 8, pfdr_sz / 64 - 8);
 		/* thresholds */
-		qm_set_pfdr_threshold(qm, 32, 32);
+		qm_set_pfdr_threshold(qm, 512, 64);
 		qm_set_sfdr_threshold(qm, 128);
+		/* clear stale PEBI bit from interrupt status register */
+		qm_err_isr_status_clear(qm, QM_EIRQ_PEBI);
 		/* corenet initiator settings */
 		qm_set_corenet_initiator(qm);
 		/* HID settings */
@@ -544,7 +623,7 @@ static int __init fsl_qman_init(struct device_node *node)
 	if (sizeof(dma_addr_t) <= sizeof(u32))
 		qm_out(QCSP_BARE, 0);
 	/* TODO: add interrupt handling here, so that ISR is cleared *after*
-	 * PFDR initialisation. */
+	 * PFDR initialisation. */	
 	return 0;
 }
 
@@ -587,7 +666,7 @@ static void qman_free_all_fq(void)
 		struct qman_fq fq;
 
 		memset(&fq, 0, sizeof(fq));
-		ret = qman_create_fq((u32)base, QMAN_FQ_FLAG_RECOVER, &fq);
+		ret = qman_create_fq((u32)base, QMAN_FQ_FLAG_AS_IS, &fq);
 		if (ret)
 			pr_err("Create fq %d error\n", i);
 
@@ -632,3 +711,333 @@ __init void qman_init_early(void)
 #endif
 }
 
+static void log_edata_bits(u32 bit_count)
+{
+	u32 i, j, mask = 0xffffffff;
+
+	pr_warning("Qman ErrInt, EDATA:\n");
+	i = bit_count/32;
+	if (bit_count%32) {
+		i++;
+		mask = ~(mask << bit_count%32);
+	}
+	j = 16-i;
+	pr_warning("  0x%08x\n", qm_in(EDATA(j)) & mask);
+	j++;
+	for (; j < 16; j++)
+		pr_warning("  0x%08x\n", qm_in(EDATA(j)));
+}
+
+static void log_additional_error_info(u32 isr_val, u32 ecsr_val)
+{
+	union qman_ecir ecir_val;
+	union qman_eadr eadr_val;
+
+	ecir_val.ecir_raw = qm_in(ECIR);
+	/* Is portal info valid */
+	if (ecsr_val & PORTAL_ECSR_ERR) {
+		pr_warning("Qman ErrInt: %s id %d\n",
+			(ecir_val.info.portal_type) ?
+			"DCP" : "SWP", ecir_val.info.portal_num);
+	}
+	if (ecsr_val & FQID_ECSR_ERR) {
+		pr_warning("Qman ErrInt: ecir.fqid 0x%x\n",
+			ecir_val.info.fqid);
+	}
+	if (ecsr_val & (QM_EIRQ_SBEI|QM_EIRQ_MBEI)) {
+		eadr_val.eadr_raw = qm_in(EADR);
+		pr_warning("Qman ErrInt: EADR Memory: %s, 0x%x\n",
+			error_mdata[eadr_val.info.memid].txt,
+			error_mdata[eadr_val.info.memid].addr_mask
+				& eadr_val.info.eadr);
+		log_edata_bits(error_mdata[eadr_val.info.memid].bits);
+	}
+}
+
+/* Qman interrupt handler */
+static irqreturn_t qman_isr(int irq, void *ptr)
+{
+	u32 isr_val, ier_val, ecsr_val, isr_mask, i;
+
+	ier_val = qm_err_isr_enable_read(qm);
+	isr_val = qm_err_isr_status_read(qm);
+	ecsr_val = qm_in(ECSR);
+	isr_mask = isr_val & ier_val;
+
+	if (!isr_mask)
+		return IRQ_NONE;
+	for (i = 0; i < QMAN_HWE_COUNT; i++) {
+		if (qman_hwerr_txts[i].mask & isr_mask) {
+			pr_warning("Qman ErrInt: %s\n", qman_hwerr_txts[i].txt);
+			if (qman_hwerr_txts[i].mask & ecsr_val) {
+				log_additional_error_info(isr_mask, ecsr_val);
+				/* Re-arm error capture registers */
+				qm_out(ECSR, ecsr_val);
+			}
+			if (qman_hwerr_txts[i].mask & QMAN_ERRS_TO_UNENABLE) {
+				pr_devel("Qman un-enabling error 0x%x\n",
+					qman_hwerr_txts[i].mask);
+				ier_val &= ~qman_hwerr_txts[i].mask;
+				qm_err_isr_enable_write(qm, ier_val);
+			}
+		}
+	}
+	qm_err_isr_status_clear(qm, isr_val);
+	return IRQ_HANDLED;
+}
+
+/* Initialise Error Interrupt Handler */
+int qman_init_error_int(struct device_node *node)
+{
+	int ret, err_irq;
+	err_irq = of_irq_to_resource(node, 0, NULL);
+
+	if (err_irq == NO_IRQ) {
+		pr_info("Can't get %s property '%s'\n", node->full_name,
+			"interrupts");
+		return -ENODEV;
+	}
+	ret = request_irq(err_irq, qman_isr, IRQF_SHARED, "qman-err", node);
+	if (ret)  {
+		pr_err("request_irq() failed %d for '%s'\n", ret,
+			node->full_name);
+		return -ENODEV;
+	}
+	/* Write-to-clear any stale bits, (eg. starvation being asserted prior
+	 * to resource allocation during driver init). */
+	qm_err_isr_status_clear(qm, 0xffffffff);
+	/* Enable Error Interrupts */
+	qm_err_isr_enable_write(qm, 0xffffffff);
+	return 0;
+}
+
+#ifdef CONFIG_SYSFS
+
+#define DRV_NAME	"fsl-qman"
+
+static ssize_t show_pfdr_fpc(struct device *dev,
+	struct device_attribute *dev_attr, char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%u\n", qm_in(PFDR_FPC));
+};
+
+static ssize_t show_dlm_avg(struct device *dev,
+	struct device_attribute *dev_attr, char *buf)
+{
+	u32 data;
+	int i;
+
+	if (!sscanf(dev_attr->attr.name, "dcp%d_dlm_avg", &i))
+		return -EINVAL;
+	data = qm_in(DCP_DLM_AVG(i));
+	return snprintf(buf, PAGE_SIZE, "%d.%08d\n", data>>8,
+			(data & 0x000000ff)*390625);
+};
+
+static ssize_t set_dlm_avg(struct device *dev,
+	struct device_attribute *dev_attr, const char *buf, size_t count)
+{
+	unsigned long val;
+	int i;
+
+	if (!sscanf(dev_attr->attr.name, "dcp%d_dlm_avg", &i))
+		return -EINVAL;
+	if (strict_strtoul(buf, 0, &val)) {
+		dev_dbg(dev, "invalid input %s\n", buf);
+		return -EINVAL;
+	}
+	qm_out(DCP_DLM_AVG(i), val);
+	return count;
+};
+
+static ssize_t show_pfdr_cfg(struct device *dev,
+	struct device_attribute *dev_attr, char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%u\n", qm_in(PFDR_CFG));
+};
+
+static ssize_t set_pfdr_cfg(struct device *dev,
+	struct device_attribute *dev_attr, const char *buf, size_t count)
+{
+	unsigned long val;
+
+	if (strict_strtoul(buf, 0, &val)) {
+		dev_dbg(dev, "invalid input %s\n", buf);
+		return -EINVAL;
+	}
+	qm_out(PFDR_CFG, val);
+	return count;
+};
+
+static ssize_t show_sfdr_in_use(struct device *dev,
+	struct device_attribute *dev_attr, char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%u\n", qm_in(SFDR_IN_USE));
+};
+
+static ssize_t show_idle_stat(struct device *dev,
+	struct device_attribute *dev_attr, char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%u\n", qm_in(IDLE_STAT));
+};
+
+static ssize_t show_ci_rlm_avg(struct device *dev,
+	struct device_attribute *dev_attr, char *buf)
+{
+	u32 data = qm_in(CI_RLM_AVG);
+	return snprintf(buf, PAGE_SIZE, "%d.%08d\n", data>>8,
+			(data & 0x000000ff)*390625);
+};
+
+static ssize_t set_ci_rlm_avg(struct device *dev,
+	struct device_attribute *dev_attr, const char *buf, size_t count)
+{
+	unsigned long val;
+
+	if (strict_strtoul(buf, 0, &val)) {
+		dev_dbg(dev, "invalid input %s\n", buf);
+		return -EINVAL;
+	}
+	qm_out(CI_RLM_AVG, val);
+	return count;
+};
+
+static ssize_t show_err_isr(struct device *dev,
+	struct device_attribute *dev_attr, char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "0x%08x\n", qm_in(ERR_ISR));
+};
+
+
+static ssize_t show_sbec(struct device *dev,
+	struct device_attribute *dev_attr, char *buf)
+{
+	int i;
+
+	if (!sscanf(dev_attr->attr.name, "sbec_%d", &i))
+		return -EINVAL;
+	return snprintf(buf, PAGE_SIZE, "%u\n", qm_in(SBEC(i)));
+};
+
+static DEVICE_ATTR(pfdr_fpc, S_IRUSR, show_pfdr_fpc, NULL);
+static DEVICE_ATTR(pfdr_cfg, S_IRUSR, show_pfdr_cfg, set_pfdr_cfg);
+static DEVICE_ATTR(idle_stat, S_IRUSR, show_idle_stat, NULL);
+static DEVICE_ATTR(ci_rlm_avg, (S_IRUSR|S_IWUGO),
+		show_ci_rlm_avg, set_ci_rlm_avg);
+static DEVICE_ATTR(err_isr, S_IRUSR, show_err_isr, NULL);
+static DEVICE_ATTR(sfdr_in_use, S_IRUSR, show_sfdr_in_use, NULL);
+
+static DEVICE_ATTR(dcp0_dlm_avg, (S_IRUSR|S_IWUGO), show_dlm_avg, set_dlm_avg);
+static DEVICE_ATTR(dcp1_dlm_avg, (S_IRUSR|S_IWUGO), show_dlm_avg, set_dlm_avg);
+static DEVICE_ATTR(dcp2_dlm_avg, (S_IRUSR|S_IWUGO), show_dlm_avg, set_dlm_avg);
+static DEVICE_ATTR(dcp3_dlm_avg, (S_IRUSR|S_IWUGO), show_dlm_avg, set_dlm_avg);
+
+static DEVICE_ATTR(sbec_0, S_IRUSR, show_sbec, NULL);
+static DEVICE_ATTR(sbec_1, S_IRUSR, show_sbec, NULL);
+static DEVICE_ATTR(sbec_2, S_IRUSR, show_sbec, NULL);
+static DEVICE_ATTR(sbec_3, S_IRUSR, show_sbec, NULL);
+static DEVICE_ATTR(sbec_4, S_IRUSR, show_sbec, NULL);
+static DEVICE_ATTR(sbec_5, S_IRUSR, show_sbec, NULL);
+static DEVICE_ATTR(sbec_6, S_IRUSR, show_sbec, NULL);
+
+
+static struct attribute *qman_dev_attributes[] = {
+	&dev_attr_pfdr_fpc.attr,
+	&dev_attr_pfdr_cfg.attr,
+	&dev_attr_idle_stat.attr,
+	&dev_attr_ci_rlm_avg.attr,
+	&dev_attr_err_isr.attr,
+	&dev_attr_dcp0_dlm_avg.attr,
+	&dev_attr_dcp1_dlm_avg.attr,
+	&dev_attr_dcp2_dlm_avg.attr,
+	&dev_attr_dcp3_dlm_avg.attr,
+	/* sfdr_in_use will be added if necessary */
+	NULL
+};
+
+static struct attribute *qman_dev_ecr_attributes[] = {
+	&dev_attr_sbec_0.attr,
+	&dev_attr_sbec_1.attr,
+	&dev_attr_sbec_2.attr,
+	&dev_attr_sbec_3.attr,
+	&dev_attr_sbec_4.attr,
+	&dev_attr_sbec_5.attr,
+	&dev_attr_sbec_6.attr,
+	NULL
+};
+
+/* root level */
+static const struct attribute_group qman_dev_attr_grp = {
+	.name = NULL,
+	.attrs = qman_dev_attributes
+};
+static const struct attribute_group qman_dev_ecr_grp = {
+	.name = "error_capture",
+	.attrs = qman_dev_ecr_attributes
+};
+
+static int of_fsl_qman_remove(struct of_device *ofdev)
+{
+	sysfs_remove_group(&ofdev->dev.kobj, &qman_dev_attr_grp);
+	return 0;
+};
+
+static int __devinit of_fsl_qman_probe(struct of_device *ofdev,
+				const struct of_device_id *qman)
+{
+	int ret;
+
+	ret = sysfs_create_group(&ofdev->dev.kobj, &qman_dev_attr_grp);
+	if (ret)
+		goto done;
+	if (qman_ip_rev != QMAN_REV1) {
+		ret = sysfs_add_file_to_group(&ofdev->dev.kobj,
+			&dev_attr_sfdr_in_use.attr, qman_dev_attr_grp.name);
+		if (ret)
+			goto del_group_0;
+	}
+	ret = sysfs_create_group(&ofdev->dev.kobj, &qman_dev_ecr_grp);
+	if (ret)
+		goto del_group_0;
+
+	goto done;
+
+del_group_0:
+	sysfs_remove_group(&ofdev->dev.kobj, &qman_dev_attr_grp);
+done:
+	if (ret)
+		dev_err(&ofdev->dev,
+				"Cannot create dev attributes ret=%d\n", ret);
+	return ret;
+};
+
+static struct of_device_id of_fsl_qman_ids[] = {
+	{
+		.compatible = "fsl,qman",
+	},
+	{}
+};
+MODULE_DEVICE_TABLE(of, of_fsl_qman_ids);
+
+static struct of_platform_driver of_fsl_qman_driver = {
+	.owner = THIS_MODULE,
+	.name = DRV_NAME,
+	.match_table = of_fsl_qman_ids,
+	.probe = of_fsl_qman_probe,
+	.remove      = __devexit_p(of_fsl_qman_remove),
+};
+
+static int qman_ctrl_init(void)
+{
+	return of_register_platform_driver(&of_fsl_qman_driver);
+}
+
+static void qman_ctrl_exit(void)
+{
+	of_unregister_platform_driver(&of_fsl_qman_driver);
+}
+
+module_init(qman_ctrl_init);
+module_exit(qman_ctrl_exit);
+
+#endif /* CONFIG_SYSFS */
diff --git a/drivers/hwqueue/qman_debugfs.c b/drivers/hwqueue/qman_debugfs.c
new file mode 100644
index 0000000..08162e9
--- /dev/null
+++ b/drivers/hwqueue/qman_debugfs.c
@@ -0,0 +1,841 @@
+/* Copyright (c) 2010 Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+#include "qman_private.h"
+
+static const char *state_txt[] = {
+	"Out of Service",
+	"Retired",
+	"Tentatively Scheduled",
+	"Truly Scheduled",
+	"Parked",
+	"Active, Active Held or Held Suspended",
+	"Unknown State 6",
+	"Unknown State 7",
+};
+
+struct mask_to_text {
+	u16 mask;
+	const char *txt;
+};
+
+static const struct mask_to_text fq_ctrl_text_list[] = {
+	{
+		.mask = QM_FQCTRL_PREFERINCACHE,
+		.txt = "Aggressively cache FQD"
+	},
+	{
+		.mask = QM_FQCTRL_HOLDACTIVE,
+		.txt =  "Hold active in portal",
+	},
+	{
+		.mask = QM_FQCTRL_AVOIDBLOCK,
+		.txt = "Don't block active",
+	},
+	{
+		.mask = QM_FQCTRL_FORCESFDR,
+		.txt = "High-priority SFDRs",
+	},
+	{
+		.mask = QM_FQCTRL_CPCSTASH,
+		.txt = "CPC Stash Enable",
+	},
+	{
+		.mask = QM_FQCTRL_CTXASTASHING,
+		.txt =  "Context-A stashing",
+	},
+	{
+		.mask = QM_FQCTRL_ORP,
+		.txt =  "ORP Enable",
+	},
+	{
+		.mask = QM_FQCTRL_TDE,
+		.txt = "Tail-Drop Enable",
+	},
+	{
+		.mask = QM_FQCTRL_CGE,
+		.txt = "Congestion Group Enable",
+	},
+	{
+		.mask = 0,
+		.txt = NULL,
+	},
+};
+
+static const struct mask_to_text stashing_text_list[] = {
+	{
+		.mask = QM_STASHING_EXCL_CTX,
+		.txt = "FQ Ctx Stash"
+	},
+	{
+		.mask = QM_STASHING_EXCL_DATA,
+		.txt =  "Frame Data Stash",
+	},
+	{
+		.mask = QM_STASHING_EXCL_ANNOTATION,
+		.txt = "Frame Annotation Stash",
+	},
+	{
+		.mask = 0,
+		.txt = NULL,
+	},
+};
+
+static struct dentry *dfs_root; /* debugfs root directory */
+
+/*******************************************************************************
+ *  Query Frame Queue Non Programmable Fields
+ ******************************************************************************/
+struct query_fq_np_fields_data_s {
+	u32 fqid;
+};
+
+static struct query_fq_np_fields_data_s query_fq_np_fields_data = {
+	.fqid = 1,
+};
+
+static ssize_t query_fq_np_fields_show(struct seq_file *file, void *offset)
+{
+	int ret;
+	struct qm_mcr_queryfq_np np;
+	struct qman_fq fq;
+
+	fq.fqid = query_fq_np_fields_data.fqid;
+	ret = qman_query_fq_np(&fq, &np);
+
+	if (ret) {
+		seq_printf(file, "Error %d\n", ret);
+		return 0;
+	}
+	/* Print state */
+	seq_printf(file, "Query FQ Non Programmable Fields Result fqid 0x%x\n",
+			fq.fqid);
+	seq_printf(file, " force eligible pending: %s\n",
+		(np.state & QM_MCR_NP_STATE_FE) ? "yes" : "no");
+	seq_printf(file, " retirement pending: %s\n",
+		(np.state & QM_MCR_NP_STATE_R) ? "yes" : "no");
+	seq_printf(file, " state: %s\n",
+		state_txt[np.state & QM_MCR_NP_STATE_MASK]);
+	seq_printf(file, " fq_link: 0x%x\n", np.fqd_link);
+	seq_printf(file, " odp_seq: %u\n", np.odp_seq);
+	seq_printf(file, " orp_nesn: %u\n", np.orp_nesn);
+	seq_printf(file, " orp_ea_hseq: %u\n", np.orp_ea_hseq);
+	seq_printf(file, " orp_ea_tseq: %u\n", np.orp_ea_tseq);
+	seq_printf(file, " orp_ea_hptr: 0x%x\n", np.orp_ea_hptr);
+	seq_printf(file, " orp_ea_tptr: 0x%x\n", np.orp_ea_tptr);
+	seq_printf(file, " pfdr_hptr: 0x%x\n", np.pfdr_hptr);
+	seq_printf(file, " pfdr_tptr: 0x%x\n", np.pfdr_tptr);
+	seq_printf(file, " is: ics_surp contains a %s\n",
+		(np.is) ? "deficit" : "surplus");
+	seq_printf(file, " ics_surp: %u\n", np.ics_surp);
+	seq_printf(file, " byte_cnt: %u\n", np.byte_cnt);
+	seq_printf(file, " frm_cnt: %u\n", np.frm_cnt);
+	seq_printf(file, " ra1_sfdr: 0x%x\n", np.ra1_sfdr);
+	seq_printf(file, " ra2_sfdr: 0x%x\n", np.ra2_sfdr);
+	seq_printf(file, " od1_sfdr: 0x%x\n", np.od1_sfdr);
+	seq_printf(file, " od2_sfdr: 0x%x\n", np.od2_sfdr);
+	seq_printf(file, " od3_sfdr: 0x%x\n", np.od3_sfdr);
+	return 0;
+}
+
+static int query_fq_np_fields_open(struct inode *inode,
+					struct file *file)
+{
+	return single_open(file, query_fq_np_fields_show, NULL);
+}
+
+static ssize_t query_fq_np_fields_write(struct file *f,
+			const char __user *buf, size_t count, loff_t *off)
+{
+	char *pdata;
+	unsigned long val;
+
+	pdata = kzalloc(count, GFP_KERNEL);
+	if (!pdata) {
+		pr_err("Copy from user failed\n");
+		return -ENOMEM;
+	}
+	if (copy_from_user(pdata, buf, count)) {
+		pr_err("Copy from user failed\n");
+		kfree(pdata);
+		return -EINVAL;
+	}
+	if (strict_strtoul(pdata, 0, &val)) {
+		pr_err("invalid input\n");
+		kfree(pdata);
+		return -EINVAL;
+	}
+	if (val > 0xfff) {
+		pr_err("FQ is > 0xfff\n");
+		kfree(pdata);
+		return -EINVAL;
+	}
+	query_fq_np_fields_data.fqid = (u32)val;
+	kfree(pdata);
+	return count;
+}
+
+static const struct file_operations query_fq_np_fields_fops = {
+	.owner          = THIS_MODULE,
+	.open		= query_fq_np_fields_open,
+	.read           = seq_read,
+	.write		= query_fq_np_fields_write,
+	.release	= single_release,
+};
+
+/*******************************************************************************
+ *  Frame Queue Programmable Fields
+ ******************************************************************************/
+struct query_fq_fields_data_s {
+	u32 fqid;
+};
+
+static struct query_fq_fields_data_s query_fq_fields_data = {
+	.fqid = 1,
+};
+
+static ssize_t query_fq_fields_show(struct seq_file *file, void *offset)
+{
+	int ret;
+	struct qm_fqd fqd;
+	struct qman_fq fq;
+	int i = 0;
+
+	memset(&fqd, 0, sizeof(struct qm_fqd));
+	fq.fqid = query_fq_fields_data.fqid;
+	ret = qman_query_fq(&fq, &fqd);
+	if (ret) {
+		seq_printf(file, "Error %d\n", ret);
+		return 0;
+	}
+	seq_printf(file, "Query FQ Programmable Fields Result fqid 0x%x\n",
+			fq.fqid);
+	seq_printf(file, " orprws: %u\n", fqd.orprws);
+	seq_printf(file, " oa: %u\n", fqd.oa);
+	seq_printf(file, " olws: %u\n", fqd.olws);
+
+	seq_printf(file, " cgid: %u\n", fqd.cgid);
+
+	if ((fqd.fq_ctrl & QM_FQCTRL_MASK) == 0)
+		seq_printf(file, " fq_ctrl: None\n");
+	else {
+		i = 0;
+		seq_printf(file, " fq_ctrl:\n");
+		while (fq_ctrl_text_list[i].txt != NULL) {
+			if ((fqd.fq_ctrl & QM_FQCTRL_MASK) &
+					fq_ctrl_text_list[i].mask)
+				seq_printf(file, "  %s\n",
+					fq_ctrl_text_list[i].txt);
+			i++;
+		}
+	}
+	seq_printf(file, " dest_channel: %u\n", fqd.dest.channel);
+	seq_printf(file, " dest_wq: %u\n", fqd.dest.wq);
+	seq_printf(file, " ics_cred: %u\n", fqd.ics_cred);
+	seq_printf(file, " td_mant: %u\n", fqd.td.mant);
+	seq_printf(file, " td_exp: %u\n", fqd.td.exp);
+
+	seq_printf(file, " ctx_b: 0x%x\n", fqd.context_b);
+
+	seq_printf(file, " ctx_a: 0x%llx\n", qm_fqd_stashing_get64(&fqd));
+	/* Any stashing configured */
+	if ((fqd.context_a.stashing.exclusive & 0x7) == 0)
+		seq_printf(file, " ctx_a_stash_exclusive: None\n");
+	else {
+		seq_printf(file, " ctx_a_stash_exclusive:\n");
+		i = 0;
+		while (stashing_text_list[i].txt != NULL) {
+			if ((fqd.fq_ctrl & 0x7) & stashing_text_list[i].mask)
+				seq_printf(file, "  %s\n",
+					stashing_text_list[i].txt);
+			i++;
+		}
+	}
+	seq_printf(file, " ctx_a_stash_annotation_cl: %u\n",
+			fqd.context_a.stashing.annotation_cl);
+	seq_printf(file, " ctx_a_stash_data_cl: %u\n",
+			fqd.context_a.stashing.data_cl);
+	seq_printf(file, " ctx_a_stash_context_cl: %u\n",
+			fqd.context_a.stashing.context_cl);
+	return 0;
+}
+
+static int query_fq_fields_open(struct inode *inode,
+					struct file *file)
+{
+	return single_open(file, query_fq_fields_show, NULL);
+}
+
+static ssize_t query_fq_fields_write(struct file *f,
+			const char __user *buf, size_t count, loff_t *off)
+{
+	char *pdata;
+	unsigned long val;
+
+	pdata = kzalloc(count, GFP_KERNEL);
+	if (!pdata) {
+		pr_err("Copy from user failed\n");
+		return -ENOMEM;
+	}
+	if (copy_from_user(pdata, buf, count)) {
+		pr_err("Copy from user failed\n");
+		kfree(pdata);
+		return -EINVAL;
+	}
+	if (strict_strtoul(pdata, 0, &val)) {
+		pr_err("invalid input\n");
+		kfree(pdata);
+		return -EINVAL;
+	}
+	if (val > 0xfff) {
+		pr_err("FQ is > 0xfff\n");
+		kfree(pdata);
+		return -EINVAL;
+	}
+	query_fq_fields_data.fqid = (u32)val;
+	kfree(pdata);
+	return count;
+}
+
+static const struct file_operations query_fq_fields_fops = {
+	.owner          = THIS_MODULE,
+	.open		= query_fq_fields_open,
+	.read           = seq_read,
+	.write		= query_fq_fields_write,
+	.release	= single_release,
+};
+
+/*******************************************************************************
+ * Query WQ lengths
+ ******************************************************************************/
+struct query_wq_lengths_data_s {
+	union {
+		u16 channel_wq; /* ignores wq (3 lsbits) */
+		struct {
+			u16 id:13; /* enum qm_channel */
+			u16 __reserved:3;
+		} __packed channel;
+	};
+};
+static struct query_wq_lengths_data_s query_wq_lengths_data;
+static ssize_t query_wq_lengths_show(struct seq_file *file, void *offset)
+{
+	int ret;
+	struct qm_mcr_querywq wq;
+	int i;
+
+	memset(&wq, 0, sizeof(struct qm_mcr_querywq));
+	wq.channel.id = query_wq_lengths_data.channel.id;
+	ret = qman_query_wq(0, &wq);
+	if (ret) {
+		seq_printf(file, "Error %d\n", ret);
+		return 0;
+	}
+	seq_printf(file, "Query Result For Channel: 0x%x\n", wq.channel.id);
+	for (i = 0; i < 8; i++)
+		/* mask out upper 4 bits since they are not part of length */
+		seq_printf(file, " wq%d_len : %u\n", i, wq.wq_len[i] & 0x0fff);
+	return 0;
+}
+
+static int query_wq_lengths_open(struct inode *inode,
+					struct file *file)
+{
+	return single_open(file, query_wq_lengths_show, NULL);
+}
+
+static ssize_t query_wq_lengths_write(struct file *f,
+			const char __user *buf, size_t count, loff_t *off)
+{
+	char *pdata;
+	unsigned long val;
+
+	pdata = kzalloc(count, GFP_KERNEL);
+	if (!pdata) {
+		pr_err("Copy from user failed\n");
+		return -ENOMEM;
+	}
+	if (copy_from_user(pdata, buf, count)) {
+		pr_err("Copy from user failed\n");
+		kfree(pdata);
+		return -EINVAL;
+	}
+	if (strict_strtoul(pdata, 0, &val)) {
+		pr_err("invalid input\n");
+		kfree(pdata);
+		return -EINVAL;
+	}
+	if (val > 0xfff8) {
+		pr_err("Channel is > 0xfff8\n");
+		kfree(pdata);
+		return -EINVAL;
+	}
+	query_wq_lengths_data.channel.id = (u16)val;
+	kfree(pdata);
+	return count;
+}
+
+static const struct file_operations query_wq_lengths_fops = {
+	.owner          = THIS_MODULE,
+	.open		= query_wq_lengths_open,
+	.read           = seq_read,
+	.write		= query_wq_lengths_write,
+	.release	= single_release,
+};
+
+/*******************************************************************************
+ *  Query CGR
+ ******************************************************************************/
+struct query_cgr_s {
+	u8 cgid;
+};
+static struct query_cgr_s query_cgr_data;
+
+static ssize_t query_cgr_show(struct seq_file *file, void *offset)
+{
+	int ret;
+	struct qm_mcr_querycgr cgrd;
+	struct qman_cgr cgr;
+
+	memset(&cgr, 0, sizeof(struct qm_mcr_querycgr));
+	cgr.cgrid = query_cgr_data.cgid;
+	ret = qman_query_cgr(&cgr, &cgrd);
+	if (ret) {
+		seq_printf(file, "Error %d\n", ret);
+		return 0;
+	}
+	seq_printf(file, "Query CGR id 0x%x\n", cgr.cgrid);
+	seq_printf(file, " wr_parm_g MA: %u, Mn: %u, SA: %u, Sn: %u, Pn: %u\n",
+		cgrd.cgr.wr_parm_g.MA, cgrd.cgr.wr_parm_g.Mn,
+		cgrd.cgr.wr_parm_g.SA, cgrd.cgr.wr_parm_g.Sn,
+		cgrd.cgr.wr_parm_g.Pn);
+
+	seq_printf(file, " wr_parm_y MA: %u, Mn: %u, SA: %u, Sn: %u, Pn: %u\n",
+		cgrd.cgr.wr_parm_y.MA, cgrd.cgr.wr_parm_y.Mn,
+		cgrd.cgr.wr_parm_y.SA, cgrd.cgr.wr_parm_y.Sn,
+		cgrd.cgr.wr_parm_y.Pn);
+
+	seq_printf(file, " wr_parm_r MA: %u, Mn: %u, SA: %u, Sn: %u, Pn: %u\n",
+		cgrd.cgr.wr_parm_r.MA, cgrd.cgr.wr_parm_r.Mn,
+		cgrd.cgr.wr_parm_r.SA, cgrd.cgr.wr_parm_r.Sn,
+		cgrd.cgr.wr_parm_r.Pn);
+
+	seq_printf(file, " wr_en_g: %u, wr_en_y: %u, we_en_r: %u\n",
+		cgrd.cgr.wr_en_g, cgrd.cgr.wr_en_y, cgrd.cgr.wr_en_r);
+
+	seq_printf(file, " cscn_en: %u\n", cgrd.cgr.cscn_en);
+	seq_printf(file, " cscn_targ: %u\n", cgrd.cgr.cscn_targ);
+	seq_printf(file, " cstd_en: %u\n", cgrd.cgr.cstd_en);
+	seq_printf(file, " cs: %u\n", cgrd.cgr.cs);
+
+	seq_printf(file, " cs_thresh_TA: %u, cs_thresh_Tn: %u\n",
+		cgrd.cgr.cs_thres.TA, cgrd.cgr.cs_thres.Tn);
+
+	if (qman_ip_rev != QMAN_REV1) {
+		seq_printf(file, " mode: %s\n",
+			(cgrd.cgr.mode & QMAN_CGR_MODE_FRAME) ?
+			"frame count" : "byte count");
+	}
+	seq_printf(file, " i_bcnt: %llu\n", qm_mcr_querycgr_i_get64(&cgrd));
+	seq_printf(file, " a_bcnt: %llu\n", qm_mcr_querycgr_a_get64(&cgrd));
+
+	return 0;
+}
+
+static int query_cgr_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, query_cgr_show, NULL);
+}
+
+static ssize_t query_cgr_write(struct file *f, const char __user *buf,
+				size_t count, loff_t *off)
+{
+	char *pdata;
+	unsigned long val;
+
+	pdata = kzalloc(count, GFP_KERNEL);
+	if (!pdata) {
+		pr_err("Copy from user failed\n");
+		return -ENOMEM;
+	}
+	if (copy_from_user(pdata, buf, count)) {
+		pr_err("Copy from user failed\n");
+		kfree(pdata);
+		return -EINVAL;
+	}
+	if (strict_strtoul(pdata, 0, &val)) {
+		pr_err("invalid input\n");
+		kfree(pdata);
+		return -EINVAL;
+	}
+	if (val > 0xff) {
+		pr_err("cgrid is > 0xff\n");
+		kfree(pdata);
+		return -EINVAL;
+	}
+	query_cgr_data.cgid = (u8)val;
+	kfree(pdata);
+	return count;
+}
+
+static const struct file_operations query_cgr_fops = {
+	.owner          = THIS_MODULE,
+	.open		= query_cgr_open,
+	.read           = seq_read,
+	.write		= query_cgr_write,
+	.release	= single_release,
+};
+
+/*******************************************************************************
+ *  Test Write CGR
+ ******************************************************************************/
+struct test_write_cgr_s {
+	u64 i_bcnt;
+	u8 cgid;
+};
+static struct test_write_cgr_s test_write_cgr_data;
+
+static ssize_t testwrite_cgr_show(struct seq_file *file, void *offset)
+{
+	int ret;
+	struct qm_mcr_cgrtestwrite result;
+	struct qman_cgr cgr;
+	u64 i_bcnt;
+
+	memset(&cgr, 0, sizeof(struct qman_cgr));
+	memset(&result, 0, sizeof(struct qm_mcr_cgrtestwrite));
+	cgr.cgrid = test_write_cgr_data.cgid;
+	i_bcnt = test_write_cgr_data.i_bcnt;
+	ret = qman_testwrite_cgr(&cgr, i_bcnt, &result);
+	if (ret) {
+		seq_printf(file, "Error %d\n", ret);
+		return 0;
+	}
+	seq_printf(file, "CGR Test Write CGR id 0x%x\n", cgr.cgrid);
+	seq_printf(file, " wr_parm_g MA: %u, Mn: %u, SA: %u, Sn: %u, Pn: %u\n",
+		result.cgr.wr_parm_g.MA, result.cgr.wr_parm_g.Mn,
+		result.cgr.wr_parm_g.SA, result.cgr.wr_parm_g.Sn,
+		result.cgr.wr_parm_g.Pn);
+	seq_printf(file, " wr_parm_y MA: %u, Mn: %u, SA: %u, Sn: %u, Pn: %u\n",
+		result.cgr.wr_parm_y.MA, result.cgr.wr_parm_y.Mn,
+		result.cgr.wr_parm_y.SA, result.cgr.wr_parm_y.Sn,
+		result.cgr.wr_parm_y.Pn);
+	seq_printf(file, " wr_parm_r MA: %u, Mn: %u, SA: %u, Sn: %u, Pn: %u\n",
+		result.cgr.wr_parm_r.MA, result.cgr.wr_parm_r.Mn,
+		result.cgr.wr_parm_r.SA, result.cgr.wr_parm_r.Sn,
+		result.cgr.wr_parm_r.Pn);
+	seq_printf(file, " wr_en_g: %u, wr_en_y: %u, we_en_r: %u\n",
+		result.cgr.wr_en_g, result.cgr.wr_en_y, result.cgr.wr_en_r);
+	seq_printf(file, " cscn_en: %u\n", result.cgr.cscn_en);
+	seq_printf(file, " cscn_targ: %u\n", result.cgr.cscn_targ);
+	seq_printf(file, " cstd_en: %u\n", result.cgr.cstd_en);
+	seq_printf(file, " cs: %u\n", result.cgr.cs);
+	seq_printf(file, " cs_thresh_TA: %u, cs_thresh_Tn: %u\n",
+		result.cgr.cs_thres.TA, result.cgr.cs_thres.Tn);
+
+	/* Add Mode for Si 2 */
+	if (qman_ip_rev != QMAN_REV1) {
+		seq_printf(file, " mode: %s\n",
+			(result.cgr.mode & QMAN_CGR_MODE_FRAME) ?
+			"frame count" : "byte count");
+	}
+
+	seq_printf(file, " i_bcnt: %llu\n",
+		qm_mcr_cgrtestwrite_i_get64(&result));
+	seq_printf(file, " a_bcnt: %llu\n",
+		qm_mcr_cgrtestwrite_a_get64(&result));
+	seq_printf(file, " wr_prob_g: %u\n", result.wr_prob_g);
+	seq_printf(file, " wr_prob_y: %u\n", result.wr_prob_y);
+	seq_printf(file, " wr_prob_r: %u\n", result.wr_prob_r);
+	return 0;
+}
+
+static int testwrite_cgr_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, testwrite_cgr_show, NULL);
+}
+
+static const struct file_operations testwrite_cgr_fops = {
+	.owner          = THIS_MODULE,
+	.open		= testwrite_cgr_open,
+	.read           = seq_read,
+	.release	= single_release,
+};
+
+
+static ssize_t testwrite_cgr_ibcnt_show(struct seq_file *file, void *offset)
+{
+	seq_printf(file, "i_bcnt: %llu\n", test_write_cgr_data.i_bcnt);
+	return 0;
+}
+static int testwrite_cgr_ibcnt_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, testwrite_cgr_ibcnt_show, NULL);
+}
+
+static ssize_t testwrite_cgr_ibcnt_write(struct file *f, const char __user *buf,
+				size_t count, loff_t *off)
+{
+	char *pdata;
+	unsigned long long val;
+
+	pdata = kzalloc(count, GFP_KERNEL);
+	if (!pdata) {
+		pr_err("Copy from user failed\n");
+		return -ENOMEM;
+	}
+	if (copy_from_user(pdata, buf, count)) {
+		pr_err("Copy from user failed\n");
+		kfree(pdata);
+		return -EINVAL;
+	}
+	if (strict_strtoull(pdata, 0, &val)) {
+		pr_err("invalid input\n");
+		kfree(pdata);
+		return -EINVAL;
+	}
+	test_write_cgr_data.i_bcnt = val;
+	kfree(pdata);
+	return count;
+}
+
+static const struct file_operations teswrite_cgr_ibcnt_fops = {
+	.owner          = THIS_MODULE,
+	.open		= testwrite_cgr_ibcnt_open,
+	.read           = seq_read,
+	.write		= testwrite_cgr_ibcnt_write,
+	.release	= single_release,
+};
+
+static ssize_t testwrite_cgr_cgrid_show(struct seq_file *file, void *offset)
+{
+	seq_printf(file, "cgrid: %u\n", (u32)test_write_cgr_data.cgid);
+	return 0;
+}
+static int testwrite_cgr_cgrid_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, testwrite_cgr_cgrid_show, NULL);
+}
+
+static ssize_t testwrite_cgr_cgrid_write(struct file *f, const char __user *buf,
+				size_t count, loff_t *off)
+{
+	char *pdata;
+	unsigned long val;
+
+	pdata = kzalloc(count, GFP_KERNEL);
+	if (!pdata) {
+		pr_err("Copy from user failed\n");
+		return -ENOMEM;
+	}
+	if (copy_from_user(pdata, buf, count)) {
+		pr_err("Copy from user failed\n");
+		kfree(pdata);
+		return -EINVAL;
+	}
+	if (strict_strtoul(pdata, 0, &val)) {
+		pr_err("invalid input\n");
+		kfree(pdata);
+		return -EINVAL;
+	}
+	if (val > 0xff) {
+		pr_err("cgrid is > 0xff\n");
+		kfree(pdata);
+		return -EINVAL;
+	}
+	test_write_cgr_data.cgid = (u8)val;
+	kfree(pdata);
+	return count;
+}
+
+static const struct file_operations teswrite_cgr_cgrid_fops = {
+	.owner          = THIS_MODULE,
+	.open		= testwrite_cgr_cgrid_open,
+	.read           = seq_read,
+	.write		= testwrite_cgr_cgrid_write,
+	.release	= single_release,
+};
+
+/*******************************************************************************
+ *  Query Congestion State
+ ******************************************************************************/
+static ssize_t query_congestion_show(struct seq_file *file, void *offset)
+{
+	int ret;
+	struct qm_mcr_querycongestion cs;
+	int i, j, in_cong = 0;
+	u32 mask;
+
+	memset(&cs, 0, sizeof(struct qm_mcr_querycongestion));
+	ret = qman_query_congestion(&cs);
+	if (ret) {
+		seq_printf(file, "Error %d\n", ret);
+		return 0;
+	}
+	seq_printf(file, "Query Congestion Result\n");
+	for (i = 0; i < 8; i++) {
+		mask = 0x80000000;
+		for (j = 0; j < 32; j++) {
+			if (cs.state.__state[i] & mask) {
+				in_cong = 1;
+				seq_printf(file, " cg %u: %s\n", (i*32)+j,
+					"in congestion");
+			}
+			mask >>= 1;
+		}
+	}
+	if (!in_cong)
+		seq_printf(file, " All congestion groups not congested.\n");
+	return 0;
+}
+
+static int query_congestion_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, query_congestion_show, NULL);
+}
+
+static const struct file_operations query_congestion_fops = {
+	.owner          = THIS_MODULE,
+	.open		= query_congestion_open,
+	.read           = seq_read,
+	.release	= single_release,
+};
+
+static int __init qman_debugfs_module_init(void)
+{
+	int ret = 0;
+	struct dentry *d;
+
+	dfs_root = debugfs_create_dir("qman", NULL);
+
+	if (dfs_root == NULL) {
+		ret = -ENOMEM;
+		pr_err("Cannot create qman debugfs dir\n");
+		goto _return;
+	}
+	d = debugfs_create_file("query_fq_np_fields",
+		S_IRUGO | S_IWUGO ,
+		dfs_root,
+		&query_fq_np_fields_data,
+		&query_fq_np_fields_fops);
+	if (d == NULL) {
+		ret = -ENOMEM;
+		pr_err("Cannot create  %s\n", "query_fq_np_fields");
+		goto _return;
+	}
+	d = debugfs_create_file("query_fq_fields",
+		S_IRUGO | S_IWUGO ,
+		dfs_root,
+		&query_fq_fields_data,
+		&query_fq_fields_fops);
+	if (d == NULL) {
+		ret = -ENOMEM;
+		pr_err("Cannot create  %s\n", "query_fq_fields");
+		goto _return;
+	}
+	d = debugfs_create_file("query_wq_lengths",
+		S_IRUGO | S_IWUGO ,
+		dfs_root,
+		&query_wq_lengths_data,
+		&query_wq_lengths_fops);
+	if (d == NULL) {
+		ret = -ENOMEM;
+		pr_err("Cannot create query_wq_lengths\n");
+		goto _return;
+	}
+	d = debugfs_create_file("query_cgr",
+		S_IRUGO | S_IWUGO ,
+		dfs_root,
+		&query_cgr_data,
+		&query_cgr_fops);
+	if (d == NULL) {
+		ret = -ENOMEM;
+		pr_err("Cannot create query_cgr\n");
+		goto _return;
+	}
+	d = debugfs_create_file("query_congestion",
+		S_IRUGO ,
+		dfs_root,
+		NULL,
+		&query_congestion_fops);
+	if (d == NULL) {
+		ret = -ENOMEM;
+		pr_err("Cannot create query_congestion\n");
+		goto _return;
+	}
+	d = debugfs_create_file("testwrite_cgr",
+		S_IRUGO ,
+		dfs_root,
+		NULL,
+		&testwrite_cgr_fops);
+	if (d == NULL) {
+		ret = -ENOMEM;
+		pr_err("Cannot create testwrite_cgr\n");
+		goto _return;
+	}
+	d = debugfs_create_file("testwrite_cgr_cgrid",
+		S_IRUGO | S_IWUGO ,
+		dfs_root,
+		NULL,
+		&teswrite_cgr_cgrid_fops);
+	if (d == NULL) {
+		ret = -ENOMEM;
+		pr_err("Cannot create testwrite_cgr_cgrid\n");
+		goto _return;
+	}
+	d = debugfs_create_file("testwrite_cgr_ibcnt",
+		S_IRUGO | S_IWUGO ,
+		dfs_root,
+		NULL,
+		&teswrite_cgr_ibcnt_fops);
+	if (d == NULL) {
+		ret = -ENOMEM;
+		pr_err("Cannot create testwrite_cgr_ibcnt\n");
+		goto _return;
+	}
+	return 0;
+
+_return:
+	if (dfs_root)
+		debugfs_remove_recursive(dfs_root);
+	return ret;
+}
+
+static void __exit qman_debugfs_module_exit(void)
+{
+	debugfs_remove_recursive(dfs_root);
+}
+
+
+module_init(qman_debugfs_module_init);
+module_exit(qman_debugfs_module_exit);
+MODULE_LICENSE("Dual BSD/GPL");
+
diff --git a/drivers/hwqueue/qman_driver.c b/drivers/hwqueue/qman_driver.c
index b01f61c..a6556b3 100644
--- a/drivers/hwqueue/qman_driver.c
+++ b/drivers/hwqueue/qman_driver.c
@@ -53,13 +53,8 @@ struct __pool_channel {
 static struct __pool_channel pools[POOL_MAX];
 static u32 pools_mask;
 
-static struct qm_portal portals[PORTAL_MAX];
+static struct qm_portal_config configs[PORTAL_MAX];
 static u8 num_portals;
-#ifndef CONFIG_FSL_QMAN_PORTAL_DISABLEAUTO
-static u8 num_affine_portals;
-#endif
-static DEFINE_SPINLOCK(bind_lock);
-DEFINE_PER_CPU(struct qman_portal *, qman_affine_portal);
 
 static int __qm_pool_add(u32 idx, enum qm_channel channel, phandle ph)
 {
@@ -76,7 +71,7 @@ static int __qm_pool_add(u32 idx, enum qm_channel channel, phandle ph)
 	return 0;
 }
 
-static int __qm_link(struct qm_portal *portal, phandle pool_ph)
+static int __qm_link(struct qm_portal_config *config, phandle pool_ph)
 {
 	int idx = 0;
 	struct __pool_channel *pool = &pools[0];
@@ -87,78 +82,41 @@ static int __qm_link(struct qm_portal *portal, phandle pool_ph)
 	if (idx == POOL_MAX)
 		return -EINVAL;
 	/* Link the pool to the portal */
-	pool->cfg.portals |= (1 << portal->index);
+	pool->cfg.portals |= (1 << config->index);
 	/* Link the portal to the pool */
-	portal->config.pools |= pool->cfg.pool;
+	config->pools |= pool->cfg.pool;
 	return 0;
 }
 
-static struct qm_portal *__qm_portal_add(const struct qm_addr *addr,
+static struct qm_portal_config *__qm_portal_add(
 				const struct qm_portal_config *config)
 {
-	struct qm_portal *ret;
+	struct qm_portal_config *ret;
 	BUG_ON((num_portals + 1) > PORTAL_MAX);
-	ret = &portals[num_portals];
-	ret->addr = *addr;
-	ret->config = *config;
-	ret->config.bound = 0;
-#ifdef CONFIG_FSL_QMAN_BUG_AND_FEATURE_REV1
-	ret->bugs = (void *)get_zeroed_page(GFP_KERNEL);
-	if (!ret->bugs) {
-		pr_err("Can't get zeroed page for 'bugs'\n");
-		return NULL;
-	}
-#endif
+	ret = &configs[num_portals];
+	*ret = *config;
+	ret->portal = NULL;
 	ret->index = num_portals++;
 	return ret;
 }
 
-int __qm_portal_bind(struct qm_portal *portal, u8 iface)
-{
-	int ret = -EBUSY;
-	spin_lock(&bind_lock);
-	if (!(portal->config.bound & iface)) {
-		portal->config.bound |= iface;
-		ret = 0;
-	}
-	spin_unlock(&bind_lock);
-	return ret;
-}
-
-void __qm_portal_unbind(struct qm_portal *portal, u8 iface)
-{
-	spin_lock(&bind_lock);
-	QM_ASSERT(portal->config.bound & iface);
-	portal->config.bound &= ~iface;
-	spin_unlock(&bind_lock);
-}
-
 u8 qm_portal_num(void)
 {
 	return num_portals;
 }
-EXPORT_SYMBOL(qm_portal_num);
 
-struct qm_portal *qm_portal_get(u8 idx)
+const struct qm_portal_config *qm_portal_config(u8 idx)
 {
 	if (unlikely(idx >= num_portals))
 		return NULL;
 
-	return &portals[idx];
+	return &configs[idx];
 }
-EXPORT_SYMBOL(qm_portal_get);
-
-const struct qm_portal_config *qm_portal_config(const struct qm_portal *portal)
-{
-	return &portal->config;
-}
-EXPORT_SYMBOL(qm_portal_config);
 
 u32 qm_pools(void)
 {
 	return pools_mask;
 }
-EXPORT_SYMBOL(qm_pools);
 
 const struct qm_pool_channel *qm_pool_channel(u32 mask)
 {
@@ -173,7 +131,6 @@ const struct qm_pool_channel *qm_pool_channel(u32 mask)
 		return NULL;
 	return &c->cfg;
 }
-EXPORT_SYMBOL(qm_pool_channel);
 
 static int __init fsl_qman_pool_channel_init(struct device_node *node)
 {
@@ -228,17 +185,73 @@ static const struct qman_fq_cb null_cb = {
 	.fqs = null_cb_mr
 };
 
+struct affine_portal_data {
+	struct completion done;
+	const struct qm_portal_config *pconfig;
+};
+
+static int __init_affine_portal(void *__data)
+{
+	int ret = 0;
+	struct affine_portal_data *data = __data;
+	const struct qm_portal_config *pconfig = data->pconfig;
+	if (!qman_have_affine_portal()) {
+		u32 flags = 0;
+		u32 irq_sources = 0;
+		/* Determine if ring and/or data stashing should be enabled */
+		if (pconfig->has_hv_dma)
+			flags = QMAN_PORTAL_FLAG_RSTASH |
+				QMAN_PORTAL_FLAG_DSTASH;
+		/* Determine what should be interrupt-vs-poll driven */
+#ifdef CONFIG_FSL_QMAN_HAVE_IRQ
+		irq_sources = QM_PIRQ_EQCI | QM_PIRQ_EQRI | QM_PIRQ_MRI |
+				QM_PIRQ_CSCI;
+#ifdef CONFIG_FSL_QMAN_PIRQ_FAST
+		irq_sources |= QM_PIRQ_DQRI;
+#endif
+#endif
+		/* TODO: cgrs ?? */
+		ret = qman_create_affine_portal(pconfig, flags, NULL, &null_cb,
+						irq_sources);
+		if (!ret)
+			/* default: enable all (available) pool channels */
+			qman_static_dequeue_add(~0);
+	}
+	complete(&data->done);
+	return ret;
+}
+
+static void init_affine_portal(const struct qm_portal_config *pconfig)
+{
+	struct affine_portal_data data = {
+		.done = COMPLETION_INITIALIZER(data.done),
+		.pconfig = pconfig
+	};
+	struct task_struct *k = kthread_create(__init_affine_portal, &data,
+		"qman_affine%d", pconfig->cpu);
+	int ret;
+	if (IS_ERR(k)) {
+		pr_err("Failed to init Qman affine portal for cpu %d\n",
+			pconfig->cpu);
+		return;
+	}
+	kthread_bind(k, pconfig->cpu);
+	wake_up_process(k);
+	wait_for_completion(&data.done);
+	ret = kthread_stop(k);
+	if (ret)
+		pr_err("Qman portal initialisation failed, cpu %d, code %d\n",
+			pconfig->cpu, ret);
+	else
+		pr_info("Qman portal initialised, cpu %d\n", pconfig->cpu);
+}
+
 static int __init fsl_qman_portal_init(struct device_node *node)
 {
 	struct resource res[2];
-	struct qm_portal_config cfg;
-	struct qm_addr addr;
-	struct qm_portal *portal;
+	struct qm_portal_config cfg, *pconfig;
 	const u32 *index, *channel;
 	const phandle *ph;
-#ifndef CONFIG_FSL_QMAN_PORTAL_DISABLEAUTO
-	struct qman_portal *affine_portal;
-#endif
 	int irq, ret, numpools;
 	u16 ip_rev = 0;
 
@@ -344,52 +357,30 @@ bad_cpu_ph:
 #else
 		cfg.has_hv_dma = 0;
 #endif
-	addr.addr_ce = ioremap_flags(res[0].start,
+	cfg.addr.addr_ce = ioremap_flags(res[0].start,
 				res[0].end - res[0].start + 1, 0);
-	addr.addr_ci = ioremap_flags(res[1].start,
+	cfg.addr.addr_ci = ioremap_flags(res[1].start,
 				res[1].end - res[1].start + 1,
 				_PAGE_GUARDED | _PAGE_NO_CACHE);
 	cfg.pools = 0;
-	cfg.bound = 0;
-	portal = __qm_portal_add(&addr, &cfg);
-	if (!portal) {
-		iounmap(addr.addr_ce);
-		iounmap(addr.addr_ci);
+	pconfig = __qm_portal_add(&cfg);
+	if (!pconfig) {
+		iounmap(cfg.addr.addr_ce);
+		iounmap(cfg.addr.addr_ci);
 		irq_dispose_mapping(cfg.irq);
 		return -ENOMEM;
 	}
-	pr_info("Qman portal at %p:%p (%d:%d,v%04x)\n", addr.addr_ce,
-		addr.addr_ci, cfg.cpu, cfg.channel, qman_ip_rev);
+	pr_info("Qman portal at %p:%p (%d:%d,v%04x)\n", cfg.addr.addr_ce,
+		cfg.addr.addr_ci, cfg.cpu, cfg.channel, qman_ip_rev);
 	while (numpools--) {
-		int tmp = __qm_link(portal, *(ph++));
+		int tmp = __qm_link(pconfig, *(ph++));
 		if (tmp)
 			panic("Unrecoverable error linking pool channels");
 	}
 	/* If the portal is affine to a cpu and that cpu has no default affine
 	 * portal, auto-initialise this one for the job. */
-#ifndef CONFIG_FSL_QMAN_PORTAL_DISABLEAUTO
-	if (cfg.cpu == -1)
-		return 0;
-	affine_portal = per_cpu(qman_affine_portal, cfg.cpu);
-	if (!affine_portal) {
-		u32 flags = 0;
-		if (cfg.has_hv_dma)
-			flags = QMAN_PORTAL_FLAG_RSTASH |
-				QMAN_PORTAL_FLAG_DSTASH;
-		/* TODO: cgrs ?? */
-		affine_portal = qman_create_portal(portal, flags, NULL,
-						&null_cb);
-		if (!affine_portal) {
-			pr_err("Qman portal auto-initialisation failed\n");
-			return 0;
-		}
-		/* default: enable all (available) pool channels */
-		qman_static_dequeue_add_ex(affine_portal, ~0);
-		pr_info("Qman portal %d auto-initialised\n", cfg.cpu);
-		per_cpu(qman_affine_portal, cfg.cpu) = affine_portal;
-		num_affine_portals++;
-	}
-#endif
+	if (cfg.cpu != -1)
+		init_affine_portal(pconfig);
 	return 0;
 }
 
@@ -401,6 +392,13 @@ static __init int qman_init(void)
 {
 	struct device_node *dn;
 	int ret;
+
+	for_each_compatible_node(dn, NULL, "fsl,qman") {
+		if (!qman_init_error_int(dn))
+			pr_info("Qman err interrupt handler present\n");
+		else
+			pr_err("Qman err interrupt handler missing\n");
+	}
 	for_each_compatible_node(dn, NULL, "fsl,qman-pool-channel") {
 		ret = fsl_qman_pool_channel_init(dn);
 		if (ret)
@@ -411,18 +409,17 @@ static __init int qman_init(void)
 		if (ret)
 			return ret;
 	}
-#ifndef CONFIG_FSL_QMAN_PORTAL_DISABLEAUTO
-	if (num_affine_portals == num_online_cpus()) {
-		u32 cgid;
-		for (cgid = 0; cgid < 256; cgid++)
-			if (qman_init_cgr(cgid))
+	if (cpumask_equal(qman_affine_cpus(), cpu_online_mask)) {
+		struct qman_cgr cgr;
+		for (cgr.cgrid = 0; cgr.cgrid < 256; cgr.cgrid++) {
+			if (qman_modify_cgr(&cgr, QMAN_CGR_FLAG_USE_INIT, NULL))
 				pr_err("CGR init failed on CGID %d\n",
-					cgid);
+					cgr.cgrid);
+		}
 	} else {
 		pr_err("Not all cpus have an affine Qman portal\n");
 		pr_err("Expect Qman-dependent drivers to crash!\n");
 	}
-#endif
 #ifdef CONFIG_FSL_QMAN_FQALLOCATOR
 	ret = __fqalloc_init();
 	if (ret)
diff --git a/drivers/hwqueue/qman_fqalloc.c b/drivers/hwqueue/qman_fqalloc.c
index 07b24b1..8fb0ba6 100644
--- a/drivers/hwqueue/qman_fqalloc.c
+++ b/drivers/hwqueue/qman_fqalloc.c
@@ -1,4 +1,4 @@
-/* Copyright (c) 2009 Freescale Semiconductor, Inc.
+/* Copyright (c) 2009-2010 Freescale Semiconductor, Inc.
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -64,14 +64,12 @@ u32 qm_fq_new(void)
 }
 EXPORT_SYMBOL(qm_fq_new);
 
-int qm_fq_free_flags(u32 fqid, u32 flags)
+int qm_fq_free_flags(u32 fqid, __maybe_unused u32 flags)
 {
-	struct bm_buffer buf = {
-		.hi = 0,
-		.lo = fqid
-	};
+	struct bm_buffer buf = BM_BUFFER_INIT64(fqid);
 	u32 bflags = 0;
 	int ret;
+#ifdef CONFIG_FSL_DPA_CAN_WAIT
 	if (flags & QM_FQ_FREE_WAIT) {
 		bflags |= BMAN_RELEASE_FLAG_WAIT;
 		if (flags & BMAN_RELEASE_FLAG_WAIT_INT)
@@ -79,6 +77,7 @@ int qm_fq_free_flags(u32 fqid, u32 flags)
 		if (flags & BMAN_RELEASE_FLAG_WAIT_SYNC)
 			bflags |= BMAN_RELEASE_FLAG_WAIT_SYNC;
 	}
+#endif
 	ret = bman_release(fq_pool, &buf, 1, bflags);
 	return ret;
 }
diff --git a/drivers/hwqueue/qman_high.c b/drivers/hwqueue/qman_high.c
index cc95f69..538aea0 100644
--- a/drivers/hwqueue/qman_high.c
+++ b/drivers/hwqueue/qman_high.c
@@ -30,26 +30,25 @@
  * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  */
 
-/* TODO:
- *
- * - make RECOVER also handle incomplete mgmt-commands
- */
-
-#include "qman_private.h"
+#include "qman_low.h"
 
 /* Compilation constants */
 #define DQRR_MAXFILL	15
 #define DQRR_STASH_RING	0	/* if enabled, we ought to check SDEST */
 #define DQRR_STASH_DATA	0	/* ditto */
-#define EQCR_THRESH	1	/* reread h/w CI when running out of space */
 #define EQCR_ITHRESH	4	/* if EQCR congests, interrupt threshold */
-#define RECOVER_MSLEEP	100	/* DQRR and MR need to be empty for 0.1s */
+#ifdef CONFIG_FSL_QMAN_ADAPTIVE_EQCR_THROTTLE
+#define EQCR_CI_THROT_PERIOD 500000 /* min # updates b4 throttle adjust */
+#define EQCR_CI_THROT_MAX 4000	/* max # cycles to throttle by */
+#define EQCR_CI_THROT_STEP 100	/* # cycles to inc/dec throttle by */
+#define EQCR_CI_THROT_TARGET_x10 45 /* throttle if (updatesx10) go lower */
+#endif
 #define IRQNAME		"QMan portal %d"
 #define MAX_IRQNAME	16	/* big enough for "QMan portal %d" */
 
 /* Lock/unlock frame queues, subject to the "LOCKED" flag. This is about
  * inter-processor locking only. Note, FQLOCK() is always called either under a
- * local_irq_disable() or from interrupt context - hence there's no need for
+ * local_irq_save() or from interrupt context - hence there's no need for
  * spin_lock_irq() (and indeed, the nesting breaks as the "irq" bit isn't
  * recursive...). */
 #define FQLOCK(fq) \
@@ -82,46 +81,63 @@ static inline int fq_isclear(struct qman_fq *fq, u32 mask)
 	return !(fq->flags & mask);
 }
 
-/**************/
-/* Portal API */
-/**************/
-
-#define PORTAL_BITS_RECOVER	0x00010000	/* use default callbacks */
-#define PORTAL_BITS_VDQCR	0x00008000	/* VDQCR active */
-#define PORTAL_BITS_MASK_V	0x00007fff
-#define PORTAL_BITS_NON_V	~(PORTAL_BITS_VDQCR | PORTAL_BITS_MASK_V)
-#define PORTAL_BITS_GET_V(p)	((p)->bits & PORTAL_BITS_MASK_V)
-#define PORTAL_BITS_INC_V(p) \
-	do { \
-		struct qman_portal *__p793 = (p); \
-		u32 __r793 = __p793->bits & PORTAL_BITS_NON_V; \
-		__r793 |= ((__p793->bits + 1) & PORTAL_BITS_MASK_V); \
-		__p793->bits = __r793; \
-	} while (0)
+#define PORTAL_BITS_CI_PREFETCH	0x00020000	/* EQCR::CI prefetched */
 
 struct qman_portal {
-	struct qm_portal *p;
-	/* 2-element array. cgrs[0] is mask, cgrs[1] is snapshot. */
-	struct qman_cgrs *cgrs;
+	struct qm_portal p;
 	/* To avoid overloading the term "flags", we use these 2; */
 	u32 options;	/* QMAN_PORTAL_FLAG_*** - static, caller-provided */
 	u32 bits;	/* PORTAL_BITS_*** - dynamic, strictly internal */
+	unsigned long irq_sources;
 	u32 slowpoll;	/* only used when interrupts are off */
-	/* The wrap-around eq_[prod|cons] counters are used to support
-	 * QMAN_ENQUEUE_FLAG_WAIT_SYNC. */
-	u32 eq_prod, eq_cons;
+	struct qman_fq *vdqcr_owned; /* only 1 volatile dequeue at a time */
+#ifdef CONFIG_FSL_DPA_CAN_WAIT_SYNC
+	struct qman_fq *eqci_owned; /* only 1 enqueue WAIT_SYNC at a time */
+#endif
+#ifdef CONFIG_FSL_QMAN_PORTAL_TASKLET
+	struct tasklet_struct tasklet;
+#endif
+#ifdef CONFIG_FSL_QMAN_ADAPTIVE_EQCR_THROTTLE
+	/* Throttle EQCR::CI updates under load, see try_eq_start() */
+	u64 eq_throt_last_update;
+	u32 eq_throt_last_evaluation;
+	u32 eq_throt_cycles;
+#endif
 	u32 sdqcr;
-	volatile int disable_count;
+	int dqrr_disable_ref;
+#ifdef CONFIG_FSL_QMAN_NULL_FQ_DEMUX
 	/* If we receive a DQRR or MR ring entry for a "null" FQ, ie. for which
 	 * FQD::contextB is NULL rather than pointing to a FQ object, we use
 	 * these handlers. (This is not considered a fast-path mechanism.) */
 	struct qman_fq_cb null_cb;
+#endif
+	/* When the cpu-affine portal is activated, this is non-NULL */
+	const struct qm_portal_config *config;
 	/* This is needed for providing a non-NULL device to dma_map_***() */
 	struct platform_device *pdev;
 	struct qman_rbtree retire_table;
 	char irqname[MAX_IRQNAME];
+	/* 2-element array. cgrs[0] is mask, cgrs[1] is snapshot. */
+	struct qman_cgrs *cgrs;
+	/* 256-element array, each is a linked-list of CSCN handlers. */
+	struct list_head cgr_cbs[256];
+	/* list lock */
+	spinlock_t cgr_lock;
 };
 
+static cpumask_t affine_mask = CPU_MASK_NONE;
+static DEFINE_SPINLOCK(affine_mask_lock);
+static DEFINE_PER_CPU(struct qman_portal, qman_affine_portal);
+static inline struct qman_portal *get_affine_portal(void)
+{
+	return &get_cpu_var(qman_affine_portal);
+}
+static inline void put_affine_portal(void)
+{
+	put_cpu_var(qman_affine_portal);
+}
+
+
 /* This gives a FQID->FQ lookup to cover the fact that we can't directly demux
  * retirement notifications (the fact they are sometimes h/w-consumed means that
  * contextB isn't always a s/w demux - and as we can't know which case it is
@@ -163,74 +179,129 @@ static inline struct qman_fq *table_find_fq(struct qman_portal *p, u32 fqid)
  * work to do. */
 #define SLOW_POLL_IDLE   1000
 #define SLOW_POLL_BUSY   10
-static u32 __poll_portal_slow(struct qman_portal *p, struct qm_portal *lowp,
-				u32 is);
-static inline void __poll_portal_fast(struct qman_portal *p,
-					struct qm_portal *lowp);
+static u32 __poll_portal_slow(struct qman_portal *p, u32 is);
+static inline unsigned int __poll_portal_fast(struct qman_portal *p,
+					unsigned int poll_limit);
 
 #ifdef CONFIG_FSL_QMAN_HAVE_IRQ
+/* This is called from the ISR or from a deferred tasklet */
+static inline void do_isr_work(struct qman_portal *p)
+{
+	u32 clear = QM_DQAVAIL_MASK | p->irq_sources;
+	u32 is = qm_isr_status_read(&p->p) & p->irq_sources;
+	/* DQRR-handling if it's interrupt-driven */
+	if (is & QM_PIRQ_DQRI)
+		__poll_portal_fast(p, CONFIG_FSL_QMAN_POLL_LIMIT);
+	/* Handling of anything else that's interrupt-driven */
+	clear |= __poll_portal_slow(p, is);
+	qm_isr_status_clear(&p->p, clear);
+}
+#ifdef CONFIG_FSL_QMAN_PORTAL_TASKLET
+static void portal_tasklet(unsigned long __p)
+{
+	struct qman_portal *p = (struct qman_portal *)__p;
+	do_isr_work(p);
+	qm_isr_uninhibit(&p->p);
+}
+#endif
 /* Portal interrupt handler */
 static irqreturn_t portal_isr(__always_unused int irq, void *ptr)
 {
 	struct qman_portal *p = ptr;
-	struct qm_portal *lowp = p->p;
-	u32 clear = 0, is = qm_isr_status_read(lowp);
-	/* Only do fast-path handling if it's required */
-#ifdef CONFIG_FSL_QMAN_PORTAL_FLAG_IRQ_FAST
-	clear |= QM_PIRQ_DQRI;
-	__poll_portal_fast(p, lowp);
-#endif
-#ifdef CONFIG_FSL_QMAN_PORTAL_FLAG_IRQ_SLOW
-	clear |= __poll_portal_slow(p, lowp, is);
+#ifdef CONFIG_FSL_QMAN_PORTAL_TASKLET
+	qm_isr_inhibit(&p->p);
+	tasklet_schedule(&p->tasklet);
+#else
+	do_isr_work(p);
 #endif
-	qm_isr_status_clear(lowp, clear);
 	return IRQ_HANDLED;
 }
 #endif
 
-/* This inner version is used privately by qman_create_portal(), as well as by
- * the exported qman_disable_portal(). */
-static inline void qman_disable_portal_ex(struct qman_portal *p)
+/* This inner version is used privately by qman_create_affine_portal(), as well
+ * as by the exported qman_stop_dequeues(). */
+static inline void qman_stop_dequeues_ex(struct qman_portal *p)
 {
-	local_irq_disable();
-	if (!(p->disable_count++))
-		qm_dqrr_set_maxfill(p->p, 0);
-	local_irq_enable();
+	__maybe_unused unsigned long irqflags;
+	local_irq_save(irqflags);
+	if (!(p->dqrr_disable_ref++))
+		qm_dqrr_set_maxfill(&p->p, 0);
+	local_irq_restore(irqflags);
 }
 
-static int int_dqrr_mr_empty(struct qman_portal *p, int can_wait)
+int qman_have_affine_portal(void)
 {
-	int ret;
-	might_sleep_if(can_wait);
-	ret = (qm_dqrr_current(p->p) == NULL) &&
-		(qm_mr_current(p->p) == NULL);
-	if (ret && can_wait) {
-		/* Stall and recheck to be sure it has quiesced. */
-		msleep(RECOVER_MSLEEP);
-		ret = (qm_dqrr_current(p->p) == NULL) &&
-			(qm_mr_current(p->p) == NULL);
-	}
+	struct qman_portal *qm = get_affine_portal();
+	int ret = (qm->config ? 1 : 0);
+	put_affine_portal();
 	return ret;
 }
 
-struct qman_portal *qman_create_portal(struct qm_portal *__p, u32 flags,
+static int drain_mr_fqrni(struct qm_portal *p)
+{
+	struct qm_mr_entry *msg;
+loop:
+	msg = qm_mr_current(p);
+	if (!msg) {
+		/* if MR was full and h/w had other FQRNI entries to produce, we
+		 * need to allow it time to produce those entries once the
+		 * existing entries are consumed. A worst-case situation
+		 * (fully-loaded system) means h/w sequencers may have to do 3-4
+		 * other things before servicing the portal's MR pump, each of
+		 * which (if slow) may take ~50 qman cycles (which is ~200
+		 * processor cycles). So rounding up and then multiplying this
+		 * worst-case estimate by a factor of 10, just to be
+		 * ultra-paranoid, goes as high as 10,000 cycles. NB, we consume
+		 * one entry at a time, so h/w has an opportunity to produce new
+		 * entries well before the ring has been fully consumed, so
+		 * we're being *really* paranoid here. */
+		u64 now, then = mfatb();
+		do {
+			now = mfatb();
+		} while ((then + 10000) > now);
+		msg = qm_mr_current(p);
+		if (!msg)
+			return 0;
+	}
+	if ((msg->verb & QM_MR_VERB_TYPE_MASK) != QM_MR_VERB_FQRNI)
+		/* We aren't draining anything but FQRNIs */
+		return -1;
+	qm_mr_next(p);
+	qm_mr_cci_consume(p, 1);
+	goto loop;
+}
+
+int qman_create_affine_portal(const struct qm_portal_config *config, u32 flags,
 			const struct qman_cgrs *cgrs,
-			const struct qman_fq_cb *null_cb)
+			const struct qman_fq_cb *null_cb,
+			u32 irq_sources)
 {
-	struct qman_portal *portal;
-	const struct qm_portal_config *config = qm_portal_config(__p);
+	struct qman_portal *portal = get_affine_portal();
+	struct qm_portal *__p = &portal->p;
 	char buf[16];
 	int ret;
 	u32 isdr;
 
-	portal = kmalloc(sizeof(*portal), GFP_KERNEL);
-	if (!portal)
-		return NULL;
+	/* A criteria for calling this function (from qman_driver.c) is that
+	 * we're already affine to the cpu and won't schedule onto another cpu.
+	 * This means we can put_affine_portal() and yet continue to use
+	 * "portal", which in turn means aspects of this routine can sleep. */
+	put_affine_portal();
+#ifndef CONFIG_FSL_QMAN_NULL_FQ_DEMUX
+	if (null_cb) {
+		pr_err("Driver does not support 'NULL FQ' callbacks\n");
+		return -EINVAL;
+	}
+#endif
+	/* prep the low-level portal struct with the mapped addresses from the
+	 * config, everything that follows depends on it and "config" is more
+	 * for (de)reference... */
+	__p->addr = config->addr;
 	if (qm_eqcr_init(__p, qm_eqcr_pvb, qm_eqcr_cce)) {
 		pr_err("Qman EQCR initialisation failed\n");
 		goto fail_eqcr;
 	}
-	if (qm_dqrr_init(__p, qm_dqrr_dpush, qm_dqrr_pvb,
+	if (qm_dqrr_init(__p, config, qm_dqrr_dpush, qm_dqrr_pvb,
 #ifdef CONFIG_FSL_QMAN_PORTAL_DISABLEAUTO_DCA
 			qm_dqrr_cci
 #else
@@ -255,56 +326,72 @@ struct qman_portal *qman_create_portal(struct qm_portal *__p, u32 flags,
 		goto fail_isr;
 	}
 	/* static interrupt-gating controls */
-	qm_dqrr_set_ithresh(__p, 12);
-	qm_mr_set_ithresh(__p, 4);
-	qm_isr_set_iperiod(__p, 100);
-	portal->p = __p;
-	if (!cgrs)
-		portal->cgrs = NULL;
-	else {
-		portal->cgrs = kmalloc(2 * sizeof(*cgrs), GFP_KERNEL);
-		if (!portal->cgrs)
-			goto fail_cgrs;
+	qm_dqrr_set_ithresh(__p, CONFIG_FSL_QMAN_PIRQ_DQRR_ITHRESH);
+	qm_mr_set_ithresh(__p, CONFIG_FSL_QMAN_PIRQ_MR_ITHRESH);
+	qm_isr_set_iperiod(__p, CONFIG_FSL_QMAN_PIRQ_IPERIOD);
+	portal->cgrs = kmalloc(2 * sizeof(*cgrs), GFP_KERNEL);
+	if (!portal->cgrs)
+		goto fail_cgrs;
+	/* initial snapshot is no-depletion */
+	qman_cgrs_init(&portal->cgrs[1]);
+	if (cgrs)
 		portal->cgrs[0] = *cgrs;
-		memset(&portal->cgrs[1], 0, sizeof(*cgrs));
-	}
+	else
+		/* if the given mask is NULL, assume all CGRs can be seen */
+		qman_cgrs_fill(&portal->cgrs[0]);
+	for (ret = 0; ret < 256; ret++)
+		INIT_LIST_HEAD(&portal->cgr_cbs[ret]);
+	spin_lock_init(&portal->cgr_lock);
 	portal->options = flags;
 	portal->bits = 0;
 	portal->slowpoll = 0;
-	portal->eq_prod = portal->eq_cons = 0;
+#ifdef CONFIG_FSL_DPA_CAN_WAIT_SYNC
+	portal->eqci_owned = NULL;
+#endif
+#ifdef CONFIG_FSL_QMAN_PORTAL_TASKLET
+	tasklet_init(&portal->tasklet, portal_tasklet, (unsigned long)portal);
+#endif
+#ifdef CONFIG_FSL_QMAN_ADAPTIVE_EQCR_THROTTLE
+	portal->eq_throt_last_update = 0;
+	portal->eq_throt_last_evaluation = 0;
+	portal->eq_throt_cycles = 0;
+#endif
 	portal->sdqcr = QM_SDQCR_SOURCE_CHANNELS | QM_SDQCR_COUNT_UPTO3 |
 			QM_SDQCR_DEDICATED_PRECEDENCE | QM_SDQCR_TYPE_PRIO_QOS |
 			QM_SDQCR_TOKEN_SET(0xab) | QM_SDQCR_CHANNELS_DEDICATED;
-	portal->disable_count = 0;
+	portal->dqrr_disable_ref = 0;
+#ifdef CONFIG_FSL_QMAN_NULL_FQ_DEMUX
 	if (null_cb)
 		portal->null_cb = *null_cb;
 	else
 		memset(&portal->null_cb, 0, sizeof(*null_cb));
+#endif
 	sprintf(buf, "qportal-%d", config->channel);
 	portal->pdev = platform_device_alloc(buf, -1);
 	if (!portal->pdev) {
 		ret = -ENOMEM;
 		goto fail_devalloc;
 	}
+	if (dma_set_mask(&portal->pdev->dev, DMA_BIT_MASK(40))) {
+		ret = -ENODEV;
+		goto fail_devadd;
+	}
 	ret = platform_device_add(portal->pdev);
 	if (ret)
 		goto fail_devadd;
 	qman_rbtree_init(&portal->retire_table);
 	isdr = 0xffffffff;
-	qm_isr_disable_write(portal->p, isdr);
-#ifdef CONFIG_FSL_QMAN_HAVE_IRQ
-	qm_isr_enable_write(portal->p, QM_PIRQ_EQCI | QM_PIRQ_EQRI |
-#ifdef CONFIG_FSL_QMAN_PORTAL_FLAG_IRQ_FAST
-		QM_PIRQ_DQRI |
-#endif
-		QM_PIRQ_MRI | (cgrs ? QM_PIRQ_CSCI : 0));
-#else
-		qm_isr_enable_write(portal->p, 0);
-#endif
-	qm_isr_status_clear(portal->p, 0xffffffff);
+	qm_isr_disable_write(__p, isdr);
+	portal->irq_sources = irq_sources;
+	qm_isr_enable_write(__p, portal->irq_sources);
+	qm_isr_status_clear(__p, 0xffffffff);
 #ifdef CONFIG_FSL_QMAN_HAVE_IRQ
 	snprintf(portal->irqname, MAX_IRQNAME, IRQNAME, config->cpu);
+#if defined(CONFIG_PREEMPT_HARDIRQS) || defined(CONFIG_PREEMPT_SOFTIRQS)
+	if (request_irq(config->irq, portal_isr, IRQF_NODELAY, portal->irqname, portal)) {
+#else
 	if (request_irq(config->irq, portal_isr, 0, portal->irqname, portal)) {
+#endif
 		pr_err("request_irq() failed\n");
 		goto fail_irq;
 	}
@@ -314,50 +401,41 @@ struct qman_portal *qman_create_portal(struct qm_portal *__p, u32 flags,
 		pr_err("irq_set_affinity() failed\n");
 		goto fail_affinity;
 	}
-	qm_isr_uninhibit(portal->p);
+	qm_isr_uninhibit(__p);
+#else
+	if (irq_sources)
+		panic("No Qman portal IRQ support, mustn't spcify IRQ flags!");
 #endif
 	/* Need EQCR to be empty before continuing */
 	isdr ^= QM_PIRQ_EQCI;
-	qm_isr_disable_write(portal->p, isdr);
-#ifdef CONFIG_FSL_QMAN_PORTAL_FLAG_RECOVER
-	wait_event(affine_queue, !qm_eqcr_get_fill(portal->p));
-	ret = 0;
-#else
-	ret = qm_eqcr_get_fill(portal->p);
-#endif
+	qm_isr_disable_write(__p, isdr);
+	ret = qm_eqcr_get_fill(__p);
 	if (ret) {
 		pr_err("Qman EQCR unclean, need recovery\n");
 		goto fail_eqcr_empty;
 	}
-#ifdef CONFIG_FSL_QMAN_PORTAL_FLAG_RECOVER
-	/* Check DQRR and MR are empty too, subject to RECOVERY logic */
-	if (flags & QMAN_PORTAL_FLAG_RECOVER)
-		portal->bits |= PORTAL_BITS_RECOVER;
-#endif
 	isdr ^= (QM_PIRQ_DQRI | QM_PIRQ_MRI);
-	qm_isr_disable_write(portal->p, isdr);
-#ifndef CONFIG_CRASH_DUMP
-#ifdef CONFIG_FSL_QMAN_PORTAL_FLAG_RECOVER
-	wait_event(affine_queue, int_dqrr_mr_empty(portal, 1));
-	ret = 0;
-	qman_disable_portal_ex(portal);
-	portal->bits ^= PORTAL_BITS_RECOVER;
-#else
-	ret = !int_dqrr_mr_empty(portal, 0);
-#endif
-	if (ret) {
-#ifdef CONFIG_FSL_QMAN_PORTAL_FLAG_RECOVER
-		pr_err("Qman DQRR/MR unclean, recovery failed\n");
-#else
-		pr_err("Qman DQRR/MR unclean, need recovery\n");
-#endif
+	qm_isr_disable_write(__p, isdr);
+	if (qm_dqrr_current(__p) != NULL) {
+		pr_err("Qman DQRR unclean, need recovery\n");
 		goto fail_dqrr_mr_empty;
 	}
-#endif
-	qm_isr_disable_write(portal->p, 0);
+	if (qm_mr_current(__p) != NULL) {
+		/* special handling, drain just in case it's a few FQRNIs */
+		if (drain_mr_fqrni(__p)) {
+			pr_err("Qman MR unclean, need recovery\n");
+			goto fail_dqrr_mr_empty;
+		}
+	}
+	/* Success */
+	portal->config = config;
+	spin_lock(&affine_mask_lock);
+	cpumask_set_cpu(config->cpu, &affine_mask);
+	spin_unlock(&affine_mask_lock);
+	qm_isr_disable_write(__p, 0);
 	/* Write a sane SDQCR */
-	qm_dqrr_sdqcr_set(portal->p, portal->sdqcr);
-	return portal;
+	qm_dqrr_sdqcr_set(__p, portal->sdqcr);
+	return 0;
 fail_dqrr_mr_empty:
 fail_eqcr_empty:
 #ifdef CONFIG_FSL_QMAN_HAVE_IRQ
@@ -382,28 +460,34 @@ fail_mr:
 fail_dqrr:
 	qm_eqcr_finish(__p);
 fail_eqcr:
-	kfree(portal);
-	return NULL;
+	put_affine_portal();
+	return -EINVAL;
 }
 
-void qman_destroy_portal(struct qman_portal *qm)
+void qman_destroy_affine_portal(void)
 {
+	struct qman_portal *qm = get_affine_portal();
 	/* NB we do this to "quiesce" EQCR. If we add enqueue-completions or
 	 * something related to QM_PIRQ_EQCI, this may need fixing. */
-	qm_eqcr_cce_update(qm->p);
+	qm_eqcr_cce_update(&qm->p);
 #ifdef CONFIG_FSL_QMAN_HAVE_IRQ
-	free_irq(qm_portal_config(qm->p)->irq, qm);
+	free_irq(qm->config->irq, qm);
 #endif
 	if (qm->cgrs)
 		kfree(qm->cgrs);
-	qm_isr_finish(qm->p);
-	qm_mc_finish(qm->p);
-	qm_mr_finish(qm->p);
-	qm_dqrr_finish(qm->p);
-	qm_eqcr_finish(qm->p);
-	kfree(qm);
+	qm_isr_finish(&qm->p);
+	qm_mc_finish(&qm->p);
+	qm_mr_finish(&qm->p);
+	qm_dqrr_finish(&qm->p);
+	qm_eqcr_finish(&qm->p);
+	spin_lock(&affine_mask_lock);
+	cpumask_clear_cpu(qm->config->cpu, &affine_mask);
+	spin_unlock(&affine_mask_lock);
+	qm->config = NULL;
+	put_affine_portal();
 }
 
+#ifdef CONFIG_FSL_QMAN_NULL_FQ_DEMUX
 void qman_get_null_cb(struct qman_fq_cb *null_cb)
 {
 	struct qman_portal *p = get_affine_portal();
@@ -419,6 +503,7 @@ void qman_set_null_cb(const struct qman_fq_cb *null_cb)
 	put_affine_portal();
 }
 EXPORT_SYMBOL(qman_set_null_cb);
+#endif
 
 /* Inline helper to reduce nesting in __poll_portal_slow() */
 static inline void fq_state_change(struct qman_portal *p, struct qman_fq *fq,
@@ -452,72 +537,68 @@ static inline void fq_state_change(struct qman_portal *p, struct qman_fq *fq,
 	FQUNLOCK(fq);
 }
 
-static noinline void eqcr_set_thresh(struct qman_portal *p, int check)
-{
-	if (!check || !qm_eqcr_get_ithresh(p->p))
-		qm_eqcr_set_ithresh(p->p, EQCR_ITHRESH);
-}
-
-static u32 __poll_portal_slow(struct qman_portal *p, struct qm_portal *lowp,
-				u32 is)
+static u32 __poll_portal_slow(struct qman_portal *p, u32 is)
 {
 	struct qm_mr_entry *msg;
 
-	qm_mr_pvb_prefetch(lowp);
+	qm_mr_pvb_prefetch(&p->p);
 
 	if (is & QM_PIRQ_CSCI) {
+		struct qman_cgrs rr, c;
 		struct qm_mc_result *mcr;
-		unsigned int i;
-		local_irq_disable();
-		qm_mc_start(lowp);
-		qm_mc_commit(lowp, QM_MCC_VERB_QUERYCONGESTION);
-		while (!(mcr = qm_mc_result(lowp)))
+		struct qman_cgr *cgr;
+		int i;
+		__maybe_unused unsigned long irqflags;
+
+		spin_lock_irqsave(&p->cgr_lock, irqflags);
+		qm_mc_start(&p->p);
+		qm_mc_commit(&p->p, QM_MCC_VERB_QUERYCONGESTION);
+		while (!(mcr = qm_mc_result(&p->p)))
 			cpu_relax();
-		p->cgrs[1].q = mcr->querycongestion.state;
-		local_irq_enable();
-		for (i = 0; i < 8; i++)
-			p->cgrs[1].q.__state[i] &= p->cgrs[0].q.__state[i];
+		/* mask out the ones I'm not interested in */
+		qman_cgrs_and(&rr, (const struct qman_cgrs *)
+			&mcr->querycongestion.state, &p->cgrs[0]);
+		/* check previous snapshot for delta, enter/exit congestion */
+		qman_cgrs_xor(&c, &rr, &p->cgrs[1]);
+		/* update snapshot */
+		qman_cgrs_cp(&p->cgrs[1], &rr);
+		/* Invoke callback */
+		qman_cgrs_for_each_1(i, &c)
+			list_for_each_entry(cgr, &p->cgr_cbs[i], node) {
+				if (cgr->cb)
+					cgr->cb(p, cgr, qman_cgrs_get(&rr, i));
+			}
+		spin_unlock_irqrestore(&p->cgr_lock, irqflags);
 	}
 
-#if 0
-	/* PIRQ_EQCI serves no meaningful purpose for a high-level interface,
-	 * so you can enable it to force interrupt-processing if you want (ie.
-	 * as a consequence of h/w consuming your EQCR entry, despite this
-	 * being unrelated to the work interrupt-processing needs to do).
-	 * Callbacks for enqueue-completion don't make sense (because they can
-	 * get rejected some time after EQCR-consumption, so you'd have to be
-	 * call it enqueue-incompletion...), and even if they did, you'd have
-	 * to call them irrespective of the EQCI interrupt source because it
-	 * can get coalesced. */
-	if (is & QM_PIRQ_EQCI) { ... }
-
+#ifdef CONFIG_FSL_DPA_CAN_WAIT_SYNC
+	if (is & QM_PIRQ_EQCI) {
+		unsigned long irqflags;
+		local_irq_save(irqflags);
+		p->eqci_owned = NULL;
+		local_irq_restore(irqflags);
+		wake_up(&affine_queue);
+	}
 #endif
+
 	if (is & QM_PIRQ_EQRI) {
-		local_irq_disable();
-		p->eq_cons += qm_eqcr_cce_update(lowp);
-		qm_eqcr_set_ithresh(lowp, 0);
+		__maybe_unused unsigned long irqflags;
+		local_irq_save(irqflags);
+		qm_eqcr_cce_update(&p->p);
+		qm_eqcr_set_ithresh(&p->p, 0);
+		local_irq_restore(irqflags);
 		wake_up(&affine_queue);
-		local_irq_enable();
 	}
 
 	if (is & QM_PIRQ_MRI) {
 		u8 num = 0;
 mr_loop:
-		if (qm_mr_pvb_update(lowp))
-			qm_mr_pvb_prefetch(lowp);
-		msg = qm_mr_current(lowp);
+		if (qm_mr_pvb_update(&p->p))
+			qm_mr_pvb_prefetch(&p->p);
+		msg = qm_mr_current(&p->p);
 		if (msg) {
 			struct qman_fq *fq = (void *)msg->ern.tag;
 			u8 verb = msg->verb & QM_MR_VERB_TYPE_MASK;
-			if (unlikely(p->bits & PORTAL_BITS_RECOVER)) {
-				/* use portal default handlers for recovery */
-				if (likely(!(verb & QM_MR_VERB_DC_ERN)))
-					p->null_cb.ern(p, NULL, msg);
-				else if (verb == QM_MR_VERB_DC_ERN)
-					p->null_cb.dc_ern(p, NULL, msg);
-				else if (p->null_cb.fqs)
-					p->null_cb.fqs(p, NULL, msg);
-			}
 			if (verb == QM_MR_VERB_FQRNI) {
 				; /* nada, we drop FQRNIs on the floor */
 			} else if ((verb == QM_MR_VERB_FQRN) ||
@@ -536,7 +617,9 @@ mr_loop:
 					fq->cb.ern(p, fq, msg);
 				else
 					fq->cb.dc_ern(p, fq, msg);
-			} else {
+			}
+#ifdef CONFIG_FSL_QMAN_NULL_FQ_DEMUX
+			else {
 				/* use portal default handlers for 'null's */
 				if (likely(!(verb & QM_MR_VERB_DC_ERN)))
 					p->null_cb.ern(p, NULL, msg);
@@ -545,29 +628,41 @@ mr_loop:
 				else if (p->null_cb.fqs)
 					p->null_cb.fqs(p, NULL, msg);
 			}
+#endif
 			num++;
-			qm_mr_next(lowp);
+			qm_mr_next(&p->p);
 			goto mr_loop;
 		}
-		qm_mr_cci_consume(lowp, num);
+		qm_mr_cci_consume(&p->p, num);
 	}
 
 	return is & (QM_PIRQ_CSCI | QM_PIRQ_EQCI | QM_PIRQ_EQRI | QM_PIRQ_MRI);
 }
 
-/* Look: no locks, no irq_disable()s, no preempt_disable()s! :-) The only states
+/* remove some slowish-path stuff from the "fast path" and make sure it isn't
+ * inlined. */
+static noinline void clear_vdqcr(struct qman_portal *p, struct qman_fq *fq)
+{
+	p->vdqcr_owned = NULL;
+	FQLOCK(fq);
+	fq_clear(fq, QMAN_FQ_STATE_VDQCR);
+	FQUNLOCK(fq);
+	wake_up(&affine_queue);
+}
+
+/* Look: no locks, no irq_save()s, no preempt_disable()s! :-) The only states
  * that would conflict with other things if they ran at the same time on the
  * same cpu are;
  *
- *   (i) clearing/incrementing PORTAL_BITS_*** stuff related to VDQCR, and
+ *   (i) setting/clearing vdqcr_owned, and
  *  (ii) clearing the NE (Not Empty) flag.
  *
  * Both are safe. Because;
  *
- *   (i) this clearing/incrementing can only occur after qman_volatile_dequeue()
- *       has set the PORTAL_BITS_*** stuff (which it does before setting VDQCR),
- *       and qman_volatile_dequeue() blocks interrupts and preemption while this
- *       is done so that we can't interfere.
+ *   (i) this clearing can only occur after qman_volatile_dequeue() has set the
+ *       vdqcr_owned field (which it does before setting VDQCR), and
+ *       qman_volatile_dequeue() blocks interrupts and preemption while this is
+ *       done so that we can't interfere.
  *  (ii) the NE flag is only cleared after qman_retire_fq() has set it, and as
  *       with (i) that API prevents us from interfering until it's safe.
  *
@@ -582,44 +677,67 @@ mr_loop:
  * user callbacks to call into any Qman API *except* qman_poll() (as that's the
  * sole API that could be invoking the callback through this function).
  */
-static inline void __poll_portal_fast(struct qman_portal *p,
-				struct qm_portal *lowp)
+static inline unsigned int __poll_portal_fast(struct qman_portal *p,
+					unsigned int poll_limit)
 {
 	struct qm_dqrr_entry *dq;
 	struct qman_fq *fq;
 	enum qman_cb_dqrr_result res;
+#ifdef CONFIG_FSL_QMAN_DQRR_PREFETCHING
 	int prefetch = !(p->options & QMAN_PORTAL_FLAG_RSTASH);
-	int limit = 0;
+#endif
+	unsigned int limit = 0;
 
 loop:
-	if (qm_dqrr_pvb_update(lowp) && prefetch)
-		qm_dqrr_pvb_prefetch(lowp);
-	dq = qm_dqrr_current(lowp);
+#ifdef CONFIG_FSL_QMAN_DQRR_PREFETCHING
+	if (qm_dqrr_pvb_update(&p->p) && prefetch)
+		qm_dqrr_pvb_prefetch(&p->p);
+#else
+	qm_dqrr_pvb_update(&p->p);
+#endif
+	dq = qm_dqrr_current(&p->p);
 	if (!dq)
 		goto done;
-	fq = (void *)dq->contextB;
-	/* Interpret 'dq' from the owner's perspective. */
-	if (unlikely(
-#ifdef CONFIG_FSL_QMAN_PORTAL_FLAG_RECOVER
-			(p->bits & PORTAL_BITS_RECOVER) ||
-#endif
-			!fq)) {
-		/* use portal default handlers */
-		res = p->null_cb.dqrr(p, NULL, dq);
-		QM_ASSERT(res == qman_cb_dqrr_consume);
-		res = qman_cb_dqrr_consume;
-	} else {
+	if (dq->stat & QM_DQRR_STAT_UNSCHEDULED) {
+		/* VDQCR: don't trust contextB as the FQ may have been
+		 * configured for h/w consumption and we're draining it
+		 * post-retirement. */
+		fq = p->vdqcr_owned;
+		/* We only set QMAN_FQ_STATE_NE when retiring, so we only need
+		 * to check for clearing it when doing volatile dequeues. It's
+		 * one less thing to check in the critical path (SDQCR). */
 		if (dq->stat & QM_DQRR_STAT_FQ_EMPTY)
 			fq_clear(fq, QMAN_FQ_STATE_NE);
-		/* Now let the callback do its stuff */
+		/* this is duplicated from the SDQCR code, but we have stuff to
+		 * do before *and* after this callback, and we don't want
+		 * multiple if()s in the critical path (SDQCR). */
 		res = fq->cb.dqrr(p, fq, dq);
+		if (res == qman_cb_dqrr_stop)
+			goto done;
+		/* Check for VDQCR completion */
+		if (dq->stat & QM_DQRR_STAT_DQCR_EXPIRED)
+			clear_vdqcr(p, fq);
+	} else {
+		/* SDQCR: contextB points to the FQ */
+		fq = (void *)dq->contextB;
+#ifdef CONFIG_FSL_QMAN_NULL_FQ_DEMUX
+		if (unlikely(!fq)) {
+			/* use portal default handlers */
+			res = p->null_cb.dqrr(p, NULL, dq);
+			QM_ASSERT(res == qman_cb_dqrr_consume);
+			res = qman_cb_dqrr_consume;
+		} else
+#endif
+		{
+			/* Now let the callback do its stuff */
+			res = fq->cb.dqrr(p, fq, dq);
+			/* The callback can request that we exit without
+			 * consuming this entry nor advancing; */
+			if (res == qman_cb_dqrr_stop)
+				goto done;
+		}
 	}
 	/* Interpret 'dq' from a driver perspective. */
-#define VDQCR_DONE (QM_DQRR_STAT_UNSCHEDULED | QM_DQRR_STAT_DQCR_EXPIRED)
-	if (unlikely((dq->stat & VDQCR_DONE) == VDQCR_DONE)) {
-		PORTAL_BITS_INC_V(p);
-		wake_up(&affine_queue);
-	}
 	/* Parking isn't possible unless HELDACTIVE was set. NB,
 	 * FORCEELIGIBLE implies HELDACTIVE, so we only need to
 	 * check for HELDACTIVE to cover both. */
@@ -628,84 +746,155 @@ loop:
 #ifdef CONFIG_FSL_QMAN_PORTAL_DISABLEAUTO_DCA
 	if (res == qman_cb_dqrr_park)
 		/* The only thing to do for non-DCA is the park-request */
-		qm_dqrr_park_ci(lowp);
+		qm_dqrr_park_ci(&p->p);
 	/* Move forward */
-	qm_dqrr_next(lowp);
-	qm_dqrr_cci_consume(lowp, 1);
+	qm_dqrr_next(&p->p);
+	qm_dqrr_cci_consume(&p->p, 1);
 #else
 	/* Defer just means "skip it, I'll consume it myself later on" */
 	if (res != qman_cb_dqrr_defer)
-		qm_dqrr_cdc_consume_1ptr(lowp, dq, (res == qman_cb_dqrr_park));
+		qm_dqrr_cdc_consume_1ptr(&p->p, dq, (res == qman_cb_dqrr_park));
 	/* Move forward */
-	qm_dqrr_next(lowp);
+	qm_dqrr_next(&p->p);
 #endif
-	if (++limit < CONFIG_FSL_QMAN_POLL_LIMIT)
+	/* Entry processed and consumed, increment our counter. The callback can
+	 * request that we exit after consuming the entry, and we also exit if
+	 * we reach our processing limit, so loop back only if neither of these
+	 * conditions is met. */
+	if ((++limit < poll_limit) && (res != qman_cb_dqrr_consume_stop))
 		goto loop;
 done:
+#ifdef CONFIG_FSL_QMAN_DQRR_PREFETCHING
 	if (prefetch)
-		qm_dqrr_pvb_prefetch(lowp);
+		qm_dqrr_pvb_prefetch(&p->p);
+#endif
+	return limit;
 }
 
-#ifdef CONFIG_FSL_QMAN_HAVE_POLL
-void qman_poll(void)
+u32 qman_irqsource_get(void)
 {
 	struct qman_portal *p = get_affine_portal();
-	struct qm_portal *lowp = p->p;
-#ifndef CONFIG_FSL_QMAN_PORTAL_FLAG_IRQ_SLOW
-	if (!(p->slowpoll--)) {
-		u32 is = qm_isr_status_read(lowp);
-		u32 active = __poll_portal_slow(p, lowp, is);
-		if (active) {
-			qm_isr_status_clear(lowp, active);
-			p->slowpoll = SLOW_POLL_BUSY;
-		} else
-			p->slowpoll = SLOW_POLL_IDLE;
-	}
-#endif
-#ifndef CONFIG_FSL_QMAN_PORTAL_FLAG_IRQ_FAST
-	__poll_portal_fast(p, lowp);
+	u32 ret = p->irq_sources & QM_PIRQ_VISIBLE;
+	put_affine_portal();
+	return ret;
+}
+EXPORT_SYMBOL(qman_irqsource_get);
+
+void qman_irqsource_add(__maybe_unused u32 bits)
+{
+#ifdef CONFIG_FSL_QMAN_HAVE_IRQ
+	struct qman_portal *p = get_affine_portal();
+	set_bits(bits & QM_PIRQ_VISIBLE, &p->irq_sources);
+	qm_isr_enable_write(&p->p, p->irq_sources);
+	put_affine_portal();
+#else
+	panic("No Qman portal IRQ support, mustn't spcify IRQ flags!");
 #endif
+}
+EXPORT_SYMBOL(qman_irqsource_add);
+
+void qman_irqsource_remove(u32 bits)
+{
+	struct qman_portal *p = get_affine_portal();
+	/* Subtle but important: we need to update the interrupt enable register
+	 * prior to clearing p->irq_sources. If we don't, an interrupt-spin
+	 * might happen between us clearing p->irq_sources and preventing the
+	 * same sources from triggering an interrupt. This means we have to read
+	 * the register back with a data-dependency, to ensure the write reaches
+	 * Qman before we update p->irq_sources. Hence the appearance of
+	 * obfuscation... */
+	u32 newval = p->irq_sources & ~(bits & QM_PIRQ_VISIBLE);
+	qm_isr_enable_write(&p->p, newval);
+	newval = qm_isr_enable_read(&p->p);
+	clear_bits(~newval, &p->irq_sources);
 	put_affine_portal();
 }
-EXPORT_SYMBOL(qman_poll);
+EXPORT_SYMBOL(qman_irqsource_remove);
+
+const cpumask_t *qman_affine_cpus(void)
+{
+	return &affine_mask;
+}
+EXPORT_SYMBOL(qman_affine_cpus);
+
+unsigned int qman_poll_dqrr(unsigned int limit)
+{
+	unsigned int ret;
+	struct qman_portal *p = get_affine_portal();
+	BUG_ON(p->irq_sources & QM_PIRQ_DQRI);
+#ifdef CONFIG_PREEMPT_RT
+	put_cpu();
 #endif
+	ret = __poll_portal_fast(p, limit);
+#ifdef CONFIG_PREEMPT_RT
+	get_cpu();
+#endif
+	put_affine_portal();
+	return ret;
+}
+EXPORT_SYMBOL(qman_poll_dqrr);
 
-void qman_disable_portal(void)
+void qman_poll_slow(void)
 {
 	struct qman_portal *p = get_affine_portal();
-	qman_disable_portal_ex(p);
+	u32 is = qm_isr_status_read(&p->p) & ~p->irq_sources;
+	u32 active = __poll_portal_slow(p, is);
+	qm_isr_status_clear(&p->p, active);
 	put_affine_portal();
 }
-EXPORT_SYMBOL(qman_disable_portal);
+EXPORT_SYMBOL(qman_poll_slow);
 
-void qman_enable_portal(void)
+/* Legacy wrapper */
+void qman_poll(void)
 {
 	struct qman_portal *p = get_affine_portal();
-	local_irq_disable();
-	QM_ASSERT(p->disable_count > 0);
-	if (!(--p->disable_count))
-		qm_dqrr_set_maxfill(p->p, DQRR_MAXFILL);
-	local_irq_enable();
+	if ((~p->irq_sources) & QM_PIRQ_SLOW) {
+		if (!(p->slowpoll--)) {
+			u32 is = qm_isr_status_read(&p->p) & ~p->irq_sources;
+			u32 active = __poll_portal_slow(p, is);
+			if (active) {
+				qm_isr_status_clear(&p->p, active);
+				p->slowpoll = SLOW_POLL_BUSY;
+			} else
+				p->slowpoll = SLOW_POLL_IDLE;
+		}
+	}
+	if ((~p->irq_sources) & QM_PIRQ_DQRI)
+		__poll_portal_fast(p, CONFIG_FSL_QMAN_POLL_LIMIT);
 	put_affine_portal();
 }
-EXPORT_SYMBOL(qman_enable_portal);
+EXPORT_SYMBOL(qman_poll);
 
-/* This isn't a fast-path API, and qman_driver.c setup code needs to be able to
- * set initial SDQCR values to all portals, not just the affine one for the
- * current cpu, so we suction out the "_ex" version as a private hook. */
-void qman_static_dequeue_add_ex(struct qman_portal *p, u32 pools)
+void qman_stop_dequeues(void)
 {
-	local_irq_disable();
-	pools &= p->p->config.pools;
-	p->sdqcr |= pools;
-	qm_dqrr_sdqcr_set(p->p, p->sdqcr);
-	local_irq_enable();
+	struct qman_portal *p = get_affine_portal();
+	qman_stop_dequeues_ex(p);
+	put_affine_portal();
 }
+EXPORT_SYMBOL(qman_stop_dequeues);
+
+void qman_start_dequeues(void)
+{
+	struct qman_portal *p = get_affine_portal();
+	__maybe_unused unsigned long irqflags;
+	local_irq_save(irqflags);
+	QM_ASSERT(p->dqrr_disable_ref > 0);
+	if (!(--p->dqrr_disable_ref))
+		qm_dqrr_set_maxfill(&p->p, DQRR_MAXFILL);
+	local_irq_restore(irqflags);
+	put_affine_portal();
+}
+EXPORT_SYMBOL(qman_start_dequeues);
 
 void qman_static_dequeue_add(u32 pools)
 {
+	__maybe_unused unsigned long irqflags;
 	struct qman_portal *p = get_affine_portal();
-	qman_static_dequeue_add_ex(p, pools);
+	local_irq_save(irqflags);
+	pools &= p->config->pools;
+	p->sdqcr |= pools;
+	qm_dqrr_sdqcr_set(&p->p, p->sdqcr);
+	local_irq_restore(irqflags);
 	put_affine_portal();
 }
 EXPORT_SYMBOL(qman_static_dequeue_add);
@@ -713,11 +902,12 @@ EXPORT_SYMBOL(qman_static_dequeue_add);
 void qman_static_dequeue_del(u32 pools)
 {
 	struct qman_portal *p = get_affine_portal();
-	local_irq_disable();
-	pools &= p->p->config.pools;
+	__maybe_unused unsigned long irqflags;
+	local_irq_save(irqflags);
+	pools &= p->config->pools;
 	p->sdqcr &= ~pools;
-	qm_dqrr_sdqcr_set(p->p, p->sdqcr);
-	local_irq_enable();
+	qm_dqrr_sdqcr_set(&p->p, p->sdqcr);
+	local_irq_restore(irqflags);
 	put_affine_portal();
 }
 EXPORT_SYMBOL(qman_static_dequeue_del);
@@ -734,7 +924,7 @@ EXPORT_SYMBOL(qman_static_dequeue_get);
 void qman_dca(struct qm_dqrr_entry *dq, int park_request)
 {
 	struct qman_portal *p = get_affine_portal();
-	qm_dqrr_cdc_consume_1ptr(p->p, dq, park_request);
+	qm_dqrr_cdc_consume_1ptr(&p->p, dq, park_request);
 	put_affine_portal();
 }
 EXPORT_SYMBOL(qman_dca);
@@ -758,6 +948,8 @@ static const char *mcr_result_str(u8 result)
 		return "QM_MCR_RESULT_ERR_NOTEMPTY";
 	case QM_MCR_RESULT_PENDING:
 		return "QM_MCR_RESULT_PENDING";
+	case QM_MCR_RESULT_ERR_BADCOMMAND:
+		return "QM_MCR_RESULT_ERR_BADCOMMAND";
 	}
 	return "<unknown MCR result>";
 }
@@ -769,6 +961,7 @@ int qman_create_fq(u32 fqid, u32 flags, struct qman_fq *fq)
 	struct qm_mc_command *mcc;
 	struct qm_mc_result *mcr;
 	struct qman_portal *p;
+	__maybe_unused unsigned long irqflags;
 
 	if (flags & QMAN_FQ_FLAG_DYNAMIC_FQID) {
 		fqid = qm_fq_new();
@@ -780,16 +973,15 @@ int qman_create_fq(u32 fqid, u32 flags, struct qman_fq *fq)
 	fq->flags = flags;
 	fq->state = qman_fq_state_oos;
 	fq->cgr_groupid = 0;
-	if (!(flags & QMAN_FQ_FLAG_RECOVER) ||
-			(flags & QMAN_FQ_FLAG_NO_MODIFY))
+	if (!(flags & QMAN_FQ_FLAG_AS_IS) || (flags & QMAN_FQ_FLAG_NO_MODIFY))
 		return 0;
-	/* Everything else is RECOVER support */
+	/* Everything else is AS_IS support */
 	p = get_affine_portal();
-	local_irq_disable();
-	mcc = qm_mc_start(p->p);
+	local_irq_save(irqflags);
+	mcc = qm_mc_start(&p->p);
 	mcc->queryfq.fqid = fqid;
-	qm_mc_commit(p->p, QM_MCC_VERB_QUERYFQ);
-	while (!(mcr = qm_mc_result(p->p)))
+	qm_mc_commit(&p->p, QM_MCC_VERB_QUERYFQ);
+	while (!(mcr = qm_mc_result(&p->p)))
 		cpu_relax();
 	QM_ASSERT((mcr->verb & QM_MCR_VERB_MASK) == QM_MCC_VERB_QUERYFQ);
 	if (mcr->result != QM_MCR_RESULT_OK) {
@@ -797,10 +989,10 @@ int qman_create_fq(u32 fqid, u32 flags, struct qman_fq *fq)
 		goto err;
 	}
 	fqd = mcr->queryfq.fqd;
-	mcc = qm_mc_start(p->p);
+	mcc = qm_mc_start(&p->p);
 	mcc->queryfq_np.fqid = fqid;
-	qm_mc_commit(p->p, QM_MCC_VERB_QUERYFQ_NP);
-	while (!(mcr = qm_mc_result(p->p)))
+	qm_mc_commit(&p->p, QM_MCC_VERB_QUERYFQ_NP);
+	while (!(mcr = qm_mc_result(&p->p)))
 		cpu_relax();
 	QM_ASSERT((mcr->verb & QM_MCR_VERB_MASK) == QM_MCC_VERB_QUERYFQ_NP);
 	if (mcr->result != QM_MCR_RESULT_OK) {
@@ -834,11 +1026,11 @@ int qman_create_fq(u32 fqid, u32 flags, struct qman_fq *fq)
 	}
 	if (fqd.fq_ctrl & QM_FQCTRL_CGE)
 		fq->state |= QMAN_FQ_STATE_CGR_EN;
-	local_irq_enable();
+	local_irq_restore(irqflags);
 	put_affine_portal();
 	return 0;
 err:
-	local_irq_enable();
+	local_irq_restore(irqflags);
 	put_affine_portal();
 	if (flags & QMAN_FQ_FLAG_DYNAMIC_FQID)
 		qm_fq_free(fqid);
@@ -872,7 +1064,8 @@ EXPORT_SYMBOL(qman_fq_fqid);
 
 void qman_fq_state(struct qman_fq *fq, enum qman_fq_state *state, u32 *flags)
 {
-	*state = fq->state;
+	if (state)
+		*state = fq->state;
 	if (flags)
 		*flags = fq->flags;
 }
@@ -883,9 +1076,9 @@ int qman_init_fq(struct qman_fq *fq, u32 flags, struct qm_mcc_initfq *opts)
 	struct qm_mc_command *mcc;
 	struct qm_mc_result *mcr;
 	struct qman_portal *p;
+	__maybe_unused unsigned long irqflags;
 	u8 res, myverb = (flags & QMAN_INITFQ_FLAG_SCHED) ?
 		QM_MCC_VERB_INITFQ_SCHED : QM_MCC_VERB_INITFQ_PARKED;
-	unsigned long f;
 
 	QM_ASSERT((fq->state == qman_fq_state_oos) ||
 		(fq->state == qman_fq_state_parked));
@@ -895,17 +1088,17 @@ int qman_init_fq(struct qman_fq *fq, u32 flags, struct qm_mcc_initfq *opts)
 #endif
 	/* Issue an INITFQ_[PARKED|SCHED] management command */
 	p = get_affine_portal();
-	local_irq_save(f);
+	local_irq_save(irqflags);
 	FQLOCK(fq);
 	if (unlikely((fq_isset(fq, QMAN_FQ_STATE_CHANGING)) ||
 			((fq->state != qman_fq_state_oos) &&
 				(fq->state != qman_fq_state_parked)))) {
 		FQUNLOCK(fq);
-		local_irq_restore(f);
+		local_irq_restore(irqflags);
 		put_affine_portal();
 		return -EBUSY;
 	}
-	mcc = qm_mc_start(p->p);
+	mcc = qm_mc_start(&p->p);
 	if (opts)
 		mcc->initfq = *opts;
 	mcc->initfq.fqid = fq->fqid;
@@ -929,26 +1122,24 @@ int qman_init_fq(struct qman_fq *fq, u32 flags, struct qm_mcc_initfq *opts)
 		} else {
 			phys_fq = dma_map_single(&p->pdev->dev, fq, sizeof(*fq),
 						DMA_TO_DEVICE);
-			mcc->initfq.fqd.context_a.context_hi = 0;
-			mcc->initfq.fqd.context_a.context_lo = (u32)phys_fq;
+			qm_fqd_stashing_set64(&mcc->initfq.fqd, phys_fq);
 		}
 	}
 	if (flags & QMAN_INITFQ_FLAG_LOCAL) {
-		mcc->initfq.fqd.dest.channel = p->p->config.channel;
+		mcc->initfq.fqd.dest.channel = p->config->channel;
 		if (!(mcc->initfq.we_mask & QM_INITFQ_WE_DESTWQ)) {
 			mcc->initfq.we_mask |= QM_INITFQ_WE_DESTWQ;
 			mcc->initfq.fqd.dest.wq = 4;
 		}
 	}
-	qm_mc_commit(p->p, myverb);
-	while (!(mcr = qm_mc_result(p->p)))
+	qm_mc_commit(&p->p, myverb);
+	while (!(mcr = qm_mc_result(&p->p)))
 		cpu_relax();
 	QM_ASSERT((mcr->verb & QM_MCR_VERB_MASK) == myverb);
 	res = mcr->result;
 	if (res != QM_MCR_RESULT_OK) {
-		pr_err("INITFQ failed: %s\n", mcr_result_str(res));
 		FQUNLOCK(fq);
-		local_irq_restore(f);
+		local_irq_restore(irqflags);
 		put_affine_portal();
 		return -EIO;
 	}
@@ -965,7 +1156,7 @@ int qman_init_fq(struct qman_fq *fq, u32 flags, struct qm_mcc_initfq *opts)
 	fq->state = (flags & QMAN_INITFQ_FLAG_SCHED) ?
 			qman_fq_state_sched : qman_fq_state_parked;
 	FQUNLOCK(fq);
-	local_irq_restore(f);
+	local_irq_restore(irqflags);
 	put_affine_portal();
 	return 0;
 }
@@ -976,6 +1167,7 @@ int qman_schedule_fq(struct qman_fq *fq)
 	struct qm_mc_command *mcc;
 	struct qm_mc_result *mcr;
 	struct qman_portal *p;
+	__maybe_unused unsigned long irqflags;
 	int ret = 0;
 	u8 res;
 
@@ -986,29 +1178,28 @@ int qman_schedule_fq(struct qman_fq *fq)
 #endif
 	/* Issue a ALTERFQ_SCHED management command */
 	p = get_affine_portal();
-	local_irq_disable();
+	local_irq_save(irqflags);
 	FQLOCK(fq);
 	if (unlikely((fq_isset(fq, QMAN_FQ_STATE_CHANGING)) ||
 			(fq->state != qman_fq_state_parked))) {
 		ret = -EBUSY;
 		goto out;
 	}
-	mcc = qm_mc_start(p->p);
+	mcc = qm_mc_start(&p->p);
 	mcc->alterfq.fqid = fq->fqid;
-	qm_mc_commit(p->p, QM_MCC_VERB_ALTER_SCHED);
-	while (!(mcr = qm_mc_result(p->p)))
+	qm_mc_commit(&p->p, QM_MCC_VERB_ALTER_SCHED);
+	while (!(mcr = qm_mc_result(&p->p)))
 		cpu_relax();
 	QM_ASSERT((mcr->verb & QM_MCR_VERB_MASK) == QM_MCR_VERB_ALTER_SCHED);
 	res = mcr->result;
 	if (res != QM_MCR_RESULT_OK) {
-		pr_err("ALTER_SCHED failed: %s\n", mcr_result_str(res));
 		ret = -EIO;
 		goto out;
 	}
 	fq->state = qman_fq_state_sched;
 out:
 	FQUNLOCK(fq);
-	local_irq_enable();
+	local_irq_restore(irqflags);
 	put_affine_portal();
 	return ret;
 }
@@ -1019,9 +1210,9 @@ int qman_retire_fq(struct qman_fq *fq, u32 *flags)
 	struct qm_mc_command *mcc;
 	struct qm_mc_result *mcr;
 	struct qman_portal *p;
+	__maybe_unused unsigned long irqflags;
 	int rval;
 	u8 res;
-	unsigned long f;
 
 	QM_ASSERT((fq->state == qman_fq_state_parked) ||
 		(fq->state == qman_fq_state_sched));
@@ -1030,7 +1221,7 @@ int qman_retire_fq(struct qman_fq *fq, u32 *flags)
 		return -EINVAL;
 #endif
 	p = get_affine_portal();
-	local_irq_save(f);
+	local_irq_save(irqflags);
 	FQLOCK(fq);
 	if (unlikely((fq_isset(fq, QMAN_FQ_STATE_CHANGING)) ||
 			(fq->state == qman_fq_state_retired) ||
@@ -1041,10 +1232,10 @@ int qman_retire_fq(struct qman_fq *fq, u32 *flags)
 	rval = table_push_fq(p, fq);
 	if (rval)
 		goto out;
-	mcc = qm_mc_start(p->p);
+	mcc = qm_mc_start(&p->p);
 	mcc->alterfq.fqid = fq->fqid;
-	qm_mc_commit(p->p, QM_MCC_VERB_ALTER_RETIRE);
-	while (!(mcr = qm_mc_result(p->p)))
+	qm_mc_commit(&p->p, QM_MCC_VERB_ALTER_RETIRE);
+	while (!(mcr = qm_mc_result(&p->p)))
 		cpu_relax();
 	QM_ASSERT((mcr->verb & QM_MCR_VERB_MASK) == QM_MCR_VERB_ALTER_RETIRE);
 	res = mcr->result;
@@ -1088,11 +1279,10 @@ int qman_retire_fq(struct qman_fq *fq, u32 *flags)
 	} else {
 		rval = -EIO;
 		table_del_fq(p, fq);
-		pr_err("ALTER_RETIRE failed: %s\n", mcr_result_str(res));
 	}
 out:
 	FQUNLOCK(fq);
-	local_irq_restore(f);
+	local_irq_restore(irqflags);
 	put_affine_portal();
 	return rval;
 }
@@ -1103,9 +1293,9 @@ int qman_oos_fq(struct qman_fq *fq)
 	struct qm_mc_command *mcc;
 	struct qm_mc_result *mcr;
 	struct qman_portal *p;
+	__maybe_unused unsigned long irqflags;
 	int ret = 0;
 	u8 res;
-	unsigned long flags;
 
 	QM_ASSERT(fq->state == qman_fq_state_retired);
 #ifdef CONFIG_FSL_QMAN_CHECKING
@@ -1113,29 +1303,28 @@ int qman_oos_fq(struct qman_fq *fq)
 		return -EINVAL;
 #endif
 	p = get_affine_portal();
-	local_irq_save(flags);
+	local_irq_save(irqflags);
 	FQLOCK(fq);
 	if (unlikely((fq_isset(fq, QMAN_FQ_STATE_BLOCKOOS)) ||
 			(fq->state != qman_fq_state_retired))) {
 		ret = -EBUSY;
 		goto out;
 	}
-	mcc = qm_mc_start(p->p);
+	mcc = qm_mc_start(&p->p);
 	mcc->alterfq.fqid = fq->fqid;
-	qm_mc_commit(p->p, QM_MCC_VERB_ALTER_OOS);
-	while (!(mcr = qm_mc_result(p->p)))
+	qm_mc_commit(&p->p, QM_MCC_VERB_ALTER_OOS);
+	while (!(mcr = qm_mc_result(&p->p)))
 		cpu_relax();
 	QM_ASSERT((mcr->verb & QM_MCR_VERB_MASK) == QM_MCR_VERB_ALTER_OOS);
 	res = mcr->result;
 	if (res != QM_MCR_RESULT_OK) {
-		pr_err("ALTER_OOS failed: %s\n", mcr_result_str(res));
 		ret = -EIO;
 		goto out;
 	}
 	fq->state = qman_fq_state_oos;
 out:
 	FQUNLOCK(fq);
-	local_irq_restore(flags);
+	local_irq_restore(irqflags);
 	put_affine_portal();
 	return ret;
 }
@@ -1146,25 +1335,23 @@ int qman_query_fq(struct qman_fq *fq, struct qm_fqd *fqd)
 	struct qm_mc_command *mcc;
 	struct qm_mc_result *mcr;
 	struct qman_portal *p = get_affine_portal();
+	__maybe_unused unsigned long irqflags;
 	u8 res;
-	unsigned long flags;
 
-	local_irq_save(flags);
-	mcc = qm_mc_start(p->p);
+	local_irq_save(irqflags);
+	mcc = qm_mc_start(&p->p);
 	mcc->queryfq.fqid = fq->fqid;
-	qm_mc_commit(p->p, QM_MCC_VERB_QUERYFQ);
-	while (!(mcr = qm_mc_result(p->p)))
+	qm_mc_commit(&p->p, QM_MCC_VERB_QUERYFQ);
+	while (!(mcr = qm_mc_result(&p->p)))
 		cpu_relax();
 	QM_ASSERT((mcr->verb & QM_MCR_VERB_MASK) == QM_MCR_VERB_QUERYFQ);
 	res = mcr->result;
 	if (res == QM_MCR_RESULT_OK)
 		*fqd = mcr->queryfq.fqd;
-	local_irq_restore(flags);
+	local_irq_restore(irqflags);
 	put_affine_portal();
-	if (res != QM_MCR_RESULT_OK) {
-		pr_err("QUERYFQ failed: %s\n", mcr_result_str(res));
+	if (res != QM_MCR_RESULT_OK)
 		return -EIO;
-	}
 	return 0;
 }
 EXPORT_SYMBOL(qman_query_fq);
@@ -1174,78 +1361,204 @@ int qman_query_fq_np(struct qman_fq *fq, struct qm_mcr_queryfq_np *np)
 	struct qm_mc_command *mcc;
 	struct qm_mc_result *mcr;
 	struct qman_portal *p = get_affine_portal();
+	__maybe_unused unsigned long irqflags;
 	u8 res;
 
-	local_irq_disable();
-	mcc = qm_mc_start(p->p);
+	local_irq_save(irqflags);
+	mcc = qm_mc_start(&p->p);
 	mcc->queryfq.fqid = fq->fqid;
-	qm_mc_commit(p->p, QM_MCC_VERB_QUERYFQ_NP);
-	while (!(mcr = qm_mc_result(p->p)))
+	qm_mc_commit(&p->p, QM_MCC_VERB_QUERYFQ_NP);
+	while (!(mcr = qm_mc_result(&p->p)))
 		cpu_relax();
 	QM_ASSERT((mcr->verb & QM_MCR_VERB_MASK) == QM_MCR_VERB_QUERYFQ_NP);
 	res = mcr->result;
 	if (res == QM_MCR_RESULT_OK)
 		*np = mcr->queryfq_np;
-	local_irq_enable();
+	local_irq_restore(irqflags);
+	put_affine_portal();
+	if (res != QM_MCR_RESULT_OK)
+		return -EIO;
+	return 0;
+}
+EXPORT_SYMBOL(qman_query_fq_np);
+
+int qman_query_wq(u8 query_dedicated, struct qm_mcr_querywq *wq)
+{
+	struct qm_mc_command *mcc;
+	struct qm_mc_result *mcr;
+	struct qman_portal *p = get_affine_portal();
+	__maybe_unused unsigned long irqflags;
+	u8 res, myverb;
+
+	local_irq_save(irqflags);
+	myverb = (query_dedicated) ? QM_MCR_VERB_QUERYWQ_DEDICATED :
+				 QM_MCR_VERB_QUERYWQ;
+	mcc = qm_mc_start(&p->p);
+	mcc->querywq.channel.id = wq->channel.id;
+	qm_mc_commit(&p->p, myverb);
+	while (!(mcr = qm_mc_result(&p->p)))
+		cpu_relax();
+	QM_ASSERT((mcr->verb & QM_MCR_VERB_MASK) == myverb);
+	res = mcr->result;
+	if (res == QM_MCR_RESULT_OK)
+		*wq = mcr->querywq;
+	local_irq_restore(irqflags);
 	put_affine_portal();
 	if (res != QM_MCR_RESULT_OK) {
-		pr_err("QUERYFQ_NP failed: %s\n", mcr_result_str(res));
+		pr_err("QUERYWQ failed: %s\n", mcr_result_str(res));
 		return -EIO;
 	}
 	return 0;
 }
-EXPORT_SYMBOL(qman_query_fq_np);
+EXPORT_SYMBOL(qman_query_wq);
+
+int qman_testwrite_cgr(struct qman_cgr *cgr, u64 i_bcnt,
+			struct qm_mcr_cgrtestwrite *result)
+{
+	struct qm_mc_command *mcc;
+	struct qm_mc_result *mcr;
+	struct qman_portal *p = get_affine_portal();
+	__maybe_unused unsigned long irqflags;
+	u8 res;
+
+	local_irq_save(irqflags);
+	mcc = qm_mc_start(&p->p);
+	mcc->cgrtestwrite.cgid = cgr->cgrid;
+	mcc->cgrtestwrite.i_bcnt_hi = (u8)(i_bcnt >> 32);
+	mcc->cgrtestwrite.i_bcnt_lo = (u32)i_bcnt;
+	qm_mc_commit(&p->p, QM_MCC_VERB_CGRTESTWRITE);
+	while (!(mcr = qm_mc_result(&p->p)))
+		cpu_relax();
+	QM_ASSERT((mcr->verb & QM_MCR_VERB_MASK) == QM_MCC_VERB_CGRTESTWRITE);
+	res = mcr->result;
+	if (res == QM_MCR_RESULT_OK)
+		*result = mcr->cgrtestwrite;
+	local_irq_restore(irqflags);
+	put_affine_portal();
+	if (res != QM_MCR_RESULT_OK) {
+		pr_err("CGR TEST WRITE failed: %s\n", mcr_result_str(res));
+		return -EIO;
+	}
+	return 0;
+}
+EXPORT_SYMBOL(qman_testwrite_cgr);
+
+int qman_query_cgr(struct qman_cgr *cgr, struct qm_mcr_querycgr *cgrd)
+{
+	struct qm_mc_command *mcc;
+	struct qm_mc_result *mcr;
+	struct qman_portal *p = get_affine_portal();
+	__maybe_unused unsigned long irqflags;
+	u8 res;
+
+	local_irq_save(irqflags);
+	mcc = qm_mc_start(&p->p);
+	mcc->querycgr.cgid = cgr->cgrid;
+	qm_mc_commit(&p->p, QM_MCC_VERB_QUERYCGR);
+	while (!(mcr = qm_mc_result(&p->p)))
+		cpu_relax();
+	QM_ASSERT((mcr->verb & QM_MCR_VERB_MASK) == QM_MCC_VERB_QUERYCGR);
+	res = mcr->result;
+	if (res == QM_MCR_RESULT_OK)
+		*cgrd = mcr->querycgr;
+	local_irq_restore(irqflags);
+	put_affine_portal();
+	if (res != QM_MCR_RESULT_OK) {
+		pr_err("QUERY_CGR failed: %s\n", mcr_result_str(res));
+		return -EIO;
+	}
+	return 0;
+}
+EXPORT_SYMBOL(qman_query_cgr);
+
+int qman_query_congestion(struct qm_mcr_querycongestion *congestion)
+{
+	struct qm_mc_command *mcc;
+	struct qm_mc_result *mcr;
+	struct qman_portal *p = get_affine_portal();
+	__maybe_unused unsigned long irqflags;
+	u8 res;
+
+	local_irq_save(irqflags);
+	mcc = qm_mc_start(&p->p);
+	qm_mc_commit(&p->p, QM_MCC_VERB_QUERYCONGESTION);
+	while (!(mcr = qm_mc_result(&p->p)))
+		cpu_relax();
+	QM_ASSERT((mcr->verb & QM_MCR_VERB_MASK) ==
+			QM_MCC_VERB_QUERYCONGESTION);
+	res = mcr->result;
+	if (res == QM_MCR_RESULT_OK)
+		*congestion = mcr->querycongestion;
+	local_irq_restore(irqflags);
+	put_affine_portal();
+	if (res != QM_MCR_RESULT_OK) {
+		pr_err("QUERY_CONGESTION failed: %s\n", mcr_result_str(res));
+		return -EIO;
+	}
+	return 0;
+}
+EXPORT_SYMBOL(qman_query_congestion);
 
 /* internal function used as a wait_event() expression */
-static int set_vdqcr(struct qman_portal **p, u32 vdqcr, u16 *v)
+static int set_vdqcr(struct qman_portal **p, struct qman_fq *fq, u32 vdqcr)
 {
+	__maybe_unused unsigned long irqflags;
 	int ret = -EBUSY;
 	*p = get_affine_portal();
-	local_irq_disable();
-	if (!((*p)->bits & PORTAL_BITS_VDQCR)) {
-		(*p)->bits |= PORTAL_BITS_VDQCR;
+	local_irq_save(irqflags);
+	if (!(*p)->vdqcr_owned) {
+		FQLOCK(fq);
+		if (fq_isset(fq, QMAN_FQ_STATE_VDQCR))
+			goto escape;
+		fq_set(fq, QMAN_FQ_STATE_VDQCR);
+		FQUNLOCK(fq);
+		(*p)->vdqcr_owned = fq;
 		ret = 0;
 	}
-	local_irq_enable();
-	if (!ret) {
-		if (v)
-			*v = PORTAL_BITS_GET_V(*p);
-		qm_dqrr_vdqcr_set((*p)->p, vdqcr);
-	}
+escape:
+	local_irq_restore(irqflags);
+	if (!ret)
+		qm_dqrr_vdqcr_set(&(*p)->p, vdqcr);
 	put_affine_portal();
 	return ret;
 }
 
-static int wait_vdqcr_start(struct qman_portal **p, u32 vdqcr, u16 *v,
-					u32 flags)
+#ifdef CONFIG_FSL_DPA_CAN_WAIT
+static int wait_vdqcr_start(struct qman_portal **p, struct qman_fq *fq,
+				u32 vdqcr, u32 flags)
 {
 	int ret = 0;
 	if (flags & QMAN_VOLATILE_FLAG_WAIT_INT)
 		ret = wait_event_interruptible(affine_queue,
-					!(ret = set_vdqcr(p, vdqcr, v)));
+				!(ret = set_vdqcr(p, fq, vdqcr)));
 	else
-		wait_event(affine_queue, !(ret = set_vdqcr(p, vdqcr, v)));
+		wait_event(affine_queue, !(ret = set_vdqcr(p, fq, vdqcr)));
 	return ret;
 }
+#endif
 
-int qman_volatile_dequeue(struct qman_fq *fq, u32 flags, u32 vdqcr)
+int qman_volatile_dequeue(struct qman_fq *fq, __maybe_unused u32 flags,
+				u32 vdqcr)
 {
 	struct qman_portal *p;
 	int ret;
-	u16 v = 0; /* init not needed, but gcc is dumb */
 
 	QM_ASSERT(!fq || (fq->state == qman_fq_state_parked) ||
 			(fq->state == qman_fq_state_retired));
 	QM_ASSERT(!fq || !(vdqcr & QM_VDQCR_FQID_MASK));
+	QM_ASSERT(!fq || !fq_isset(fq, QMAN_FQ_STATE_VDQCR));
 	if (fq)
 		vdqcr = (vdqcr & ~QM_VDQCR_FQID_MASK) | fq->fqid;
+#ifdef CONFIG_FSL_DPA_CAN_WAIT
 	if (flags & QMAN_VOLATILE_FLAG_WAIT)
-		ret = wait_vdqcr_start(&p, vdqcr, &v, flags);
+		ret = wait_vdqcr_start(&p, fq, vdqcr, flags);
 	else
-		ret = set_vdqcr(&p, vdqcr, &v);
+#endif
+		ret = set_vdqcr(&p, fq, vdqcr);
 	if (ret)
 		return ret;
 	/* VDQCR is set */
+#ifdef CONFIG_FSL_DPA_CAN_WAIT
 	if (flags & QMAN_VOLATILE_FLAG_FINISH) {
 		if (flags & QMAN_VOLATILE_FLAG_WAIT_INT)
 			/* NB: don't propagate any error - the caller wouldn't
@@ -1253,261 +1566,379 @@ int qman_volatile_dequeue(struct qman_fq *fq, u32 flags, u32 vdqcr)
 			 * could arrive after returning anyway, so the caller
 			 * can check signal_pending() if that's an issue. */
 			wait_event_interruptible(affine_queue,
-				PORTAL_BITS_GET_V(p) != v);
+				!fq_isset(fq, QMAN_FQ_STATE_VDQCR));
 		else
-			wait_event(affine_queue, PORTAL_BITS_GET_V(p) != v);
+			wait_event(affine_queue,
+				!fq_isset(fq, QMAN_FQ_STATE_VDQCR));
 	}
+#endif
 	return 0;
 }
 EXPORT_SYMBOL(qman_volatile_dequeue);
 
-static struct qm_eqcr_entry *try_eq_start(struct qman_portal **p)
+static noinline void update_eqcr_ci(struct qman_portal *p, u8 avail)
+{
+#ifdef CONFIG_FSL_QMAN_ADAPTIVE_EQCR_THROTTLE
+	if (avail < 2) {
+		if (p->bits & PORTAL_BITS_CI_PREFETCH) {
+			qm_eqcr_cce_update(&p->p);
+			p->bits &= ~PORTAL_BITS_CI_PREFETCH;
+		} else {
+			if (unlikely(!(p->eq_throt_last_evaluation--)))
+				__throttle_reevaluate(p);
+			if (!p->eq_throt_cycles || __throttle_test(p)) {
+				qm_eqcr_cce_prefetch(&p->p);
+				p->bits |= PORTAL_BITS_CI_PREFETCH;
+			}
+		}
+	}
+#else
+	if (avail)
+		qm_eqcr_cce_prefetch(&p->p);
+	else
+		qm_eqcr_cce_update(&p->p);
+#endif
+}
+
+int qman_eqcr_is_empty(void)
+{
+	__maybe_unused unsigned long irqflags;
+	struct qman_portal *p = get_affine_portal();
+	u8 avail;
+
+	local_irq_save(irqflags);
+	update_eqcr_ci(p, 0);
+#ifdef CONFIG_FSL_QMAN_ADAPTIVE_EQCR_THROTTLE
+	/* the adaptive code ignores the zero-valued 'avail' param we're
+	 * passing, so need to call twice to be sure the CI update occurs. */
+	update_eqcr_ci(p, 0);
+#endif
+	avail = qm_eqcr_get_fill(&p->p);
+	local_irq_restore(irqflags);
+	put_affine_portal();
+	return (avail == 0);
+}
+EXPORT_SYMBOL(qman_eqcr_is_empty);
+
+static inline struct qm_eqcr_entry *try_eq_start(struct qman_portal **p,
+					__maybe_unused unsigned long *irqflags,
+					struct qman_fq *fq,
+					const struct qm_fd *fd,
+					u32 flags)
 {
 	struct qm_eqcr_entry *eq;
-	struct qm_portal *lowp;
 	u8 avail;
+
 	*p = get_affine_portal();
-	lowp = (*p)->p;
-	local_irq_disable();
-	avail = qm_eqcr_get_avail(lowp);
-	if (avail == EQCR_THRESH)
-		/* We don't need EQCR:CI yet, but we will next time */
-		qm_eqcr_cce_prefetch(lowp);
-	else if (avail < EQCR_THRESH)
-		/* The EQCR::CI cacheline is prefetched post-enqueue, so this
-		 * would ideally be in cache from the previous commit. */
-		(*p)->eq_cons += qm_eqcr_cce_update(lowp);
-	eq = qm_eqcr_start(lowp);
+	local_irq_save((*irqflags));
+#ifdef CONFIG_FSL_DPA_CAN_WAIT_SYNC
+	if (unlikely((flags & QMAN_ENQUEUE_FLAG_WAIT) &&
+			(flags & QMAN_ENQUEUE_FLAG_WAIT_SYNC))) {
+		if ((*p)->eqci_owned) {
+			local_irq_restore((*irqflags));
+			put_affine_portal();
+			return NULL;
+		}
+		(*p)->eqci_owned = fq;
+	}
+#endif
+	avail = qm_eqcr_get_avail(&(*p)->p);
+	if (avail < 2)
+		update_eqcr_ci(*p, avail);
+	eq = qm_eqcr_start(&(*p)->p);
 	if (unlikely(!eq)) {
-		local_irq_enable();
+#ifdef CONFIG_FSL_DPA_CAN_WAIT_SYNC
+		if (unlikely((flags & QMAN_ENQUEUE_FLAG_WAIT) &&
+				(flags & QMAN_ENQUEUE_FLAG_WAIT_SYNC)))
+			(*p)->eqci_owned = NULL;
+#endif
+		local_irq_restore((*irqflags));
 		put_affine_portal();
+		return NULL;
+	}
+	if (flags & QMAN_ENQUEUE_FLAG_DCA)
+		eq->dca = QM_EQCR_DCA_ENABLE |
+			((flags & QMAN_ENQUEUE_FLAG_DCA_PARK) ?
+					QM_EQCR_DCA_PARK : 0) |
+			((flags >> 8) & QM_EQCR_DCA_IDXMASK);
+	eq->fqid = fq->fqid;
+	eq->tag = (u32)fq;
+	/* From p4080 rev1 -> rev2, the FD struct's address went from 48-bit to
+	 * 40-bit but rev1 chips will still interpret it as 48-bit, meaning we
+	 * have to scrub the upper 8-bits, just in case the user left noise in
+	 * there. Doing this selectively via a run-time check of the h/w
+	 * revision (as we do for most errata, for example) is too slow in this
+	 * critical path code. The most inexpensive way to handle this is just
+	 * to reinterpret the FD as 4 32-bit words and to mask the first word
+	 * appropriately, irrespecitive of the h/w revision. The struct fields
+	 * corresponding to this word are;
+	 *     u8 dd:2;
+	 *     u8 liodn_offset:6;
+	 *     u8 bpid;
+	 *     u8 eliodn_offset:4;
+	 *     u8 __reserved:4;
+	 *     u8 addr_hi;
+	 * So we mask this word with 0xc0ff00ff, which implicitly scrubs out
+	 * liodn_offset, eliodn_offset, and __reserved - the latter two fields
+	 * are interpreted as the 8 msbits of the 48-bit address in the case of
+	 * rev1.
+	 */
+	{
+		const u32 *src = (const u32 *)fd;
+		u32 *dest = (u32 *)&eq->fd;
+		dest[0] = src[0] & 0xc0ff00ff;
+		dest[1] = src[1];
+		dest[2] = src[2];
+		dest[3] = src[3];
 	}
 	return eq;
 }
 
-static inline struct qm_eqcr_entry *__try_eq(struct qman_portal **p)
+#ifdef CONFIG_FSL_DPA_CAN_WAIT
+static noinline struct qm_eqcr_entry *__wait_eq_start(struct qman_portal **p,
+					__maybe_unused unsigned long *irqflags,
+					struct qman_fq *fq,
+					const struct qm_fd *fd,
+					u32 flags)
 {
-	struct qm_eqcr_entry *eq = try_eq_start(p);
-	if (unlikely(!eq))
-		/* TODO: this used to be in try_eq_start() prior to
-		 * local_irq_enable() - verify that the reorder hasn't created a
-		 * race... */
-		eqcr_set_thresh(*p, 1);
+	struct qm_eqcr_entry *eq = try_eq_start(p, irqflags, fq, fd, flags);
+	if (!eq)
+		qm_eqcr_set_ithresh(&(*p)->p, EQCR_ITHRESH);
 	return eq;
 }
-
 static noinline struct qm_eqcr_entry *wait_eq_start(struct qman_portal **p,
-							u32 flags)
+					__maybe_unused unsigned long *irqflags,
+					struct qman_fq *fq,
+					const struct qm_fd *fd,
+					u32 flags)
 {
 	struct qm_eqcr_entry *eq;
-	int ret = 0;
 	if (flags & QMAN_ENQUEUE_FLAG_WAIT_INT)
-		ret = wait_event_interruptible(affine_queue,
-					(eq = __try_eq(p)));
-	else
-		wait_event(affine_queue, (eq = __try_eq(p)));
-	return eq;
-}
-
-/* Used as a wait_event() condition to determine if eq_cons has caught up to
- * eq_poll. The complication is that they're wrap-arounds, so we use a cyclic
- * comparison. This would a lot simpler if it weren't to work around a
- * theoretical possibility - that the u32 prod/cons counters wrap so fast before
- * this task is woken that it appears the enqueue never completed. We can't wait
- * for it to wrap "another time" because activity might have stopped and the
- * interrupt threshold is no longer set to wake us up (about as improbable as
- * the scenario we're fixing). What we do then is wait until the cons counter
- * reaches a safely-completed distance from 'eq_poll' *or* EQCR becomes empty,
- * and continually reset the interrupt threshold until this happens (and for
- * qman_poll() to do wakeups *after* unsetting the interrupt threshold). */
-static int eqcr_completed(struct qman_portal *p, u32 eq_poll)
-{
-	u32 tr_cons = p->eq_cons;
-	if (eq_poll & 0xc0000000) {
-		eq_poll &= 0x7fffffff;
-		tr_cons ^= 0x80000000;
-	}
-	if (tr_cons >= eq_poll)
-		return 1;
-	if ((eq_poll - tr_cons) > QM_EQCR_SIZE)
-		return 1;
-	if (!qm_eqcr_get_fill(p->p))
-		/* If EQCR is empty, we must have completed */
-		return 1;
-	eqcr_set_thresh(p, 0);
-	return 0;
-}
-
-static noinline void wait_eqcr_commit(struct qman_portal *p, u32 flags,
-					u32 eq_poll)
-{
-	eqcr_set_thresh(p, 1);
-	/* So we're supposed to wait until the commit is consumed */
-	if (flags & QMAN_ENQUEUE_FLAG_WAIT_INT)
-		/* See __enqueue() (where this inline is called) as to why we're
-		 * ignoring return codes from wait_***(). */
 		wait_event_interruptible(affine_queue,
-					eqcr_completed(p, eq_poll));
+			(eq = __wait_eq_start(p, irqflags, fq, fd, flags)));
 	else
-		wait_event(affine_queue, eqcr_completed(p, eq_poll));
+		wait_event(affine_queue,
+			(eq = __wait_eq_start(p, irqflags, fq, fd, flags)));
+	return eq;
 }
+#endif
 
-static inline void eqcr_commit(struct qman_portal *p, u32 flags, int orp)
+int qman_enqueue(struct qman_fq *fq, const struct qm_fd *fd, u32 flags)
 {
-	u32 eq_poll;
-	qm_eqcr_pvb_commit(p->p,
-		(flags & (QM_EQCR_VERB_COLOUR_MASK | QM_EQCR_VERB_INTERRUPT |
-				QM_EQCR_VERB_CMD_ENQUEUE)) |
-		(orp ? QM_EQCR_VERB_ORP : 0));
-	/* increment the producer count and capture it for SYNC */
-	eq_poll = ++p->eq_prod;
-	local_irq_enable();
-	put_affine_portal();
-	if ((flags & QMAN_ENQUEUE_FLAG_WAIT_SYNC) ==
-			QMAN_ENQUEUE_FLAG_WAIT_SYNC)
-		wait_eqcr_commit(p, flags, eq_poll);
-}
+	struct qman_portal *p;
+	struct qm_eqcr_entry *eq;
+	__maybe_unused unsigned long irqflags;
 
-static inline void eqcr_abort(struct qman_portal *p)
-{
-	qm_eqcr_abort(p->p);
-	local_irq_enable();
+#ifdef CONFIG_FSL_DPA_CAN_WAIT
+	if (flags & QMAN_ENQUEUE_FLAG_WAIT)
+		eq = wait_eq_start(&p, &irqflags, fq, fd, flags);
+	else
+#endif
+	eq = try_eq_start(&p, &irqflags, fq, fd, flags);
+	if (!eq)
+		return -EBUSY;
+	/* Note: QM_EQCR_VERB_INTERRUPT == QMAN_ENQUEUE_FLAG_WAIT_SYNC */
+	qm_eqcr_pvb_commit(&p->p, QM_EQCR_VERB_CMD_ENQUEUE |
+		(flags & (QM_EQCR_VERB_COLOUR_MASK | QM_EQCR_VERB_INTERRUPT)));
+	/* Factor the below out, it's used from qman_enqueue_orp() too */
+	local_irq_restore(irqflags);
 	put_affine_portal();
+#ifdef CONFIG_FSL_DPA_CAN_WAIT_SYNC
+	if (unlikely((flags & QMAN_ENQUEUE_FLAG_WAIT) &&
+			(flags & QMAN_ENQUEUE_FLAG_WAIT_SYNC))) {
+		if (flags & QMAN_ENQUEUE_FLAG_WAIT_INT)
+			wait_event_interruptible(affine_queue,
+					(p->eqci_owned != fq));
+		else
+			wait_event(affine_queue, (p->eqci_owned != fq));
+	}
+#endif
+	return 0;
 }
+EXPORT_SYMBOL(qman_enqueue);
 
-/* Internal version of enqueue, used by ORP and non-ORP variants. Inlining
- * should allow each instantiation to optimise appropriately (and this is why
- * the excess 'orp' parameters are not an issue). */
-static inline int __enqueue(struct qman_fq *fq, const struct qm_fd *fd,
-				u32 flags, struct qman_fq *orp_fq,
-				u16 orp_seqnum, int orp)
+int qman_enqueue_orp(struct qman_fq *fq, const struct qm_fd *fd, u32 flags,
+			struct qman_fq *orp, u16 orp_seqnum)
 {
-	register struct qm_eqcr_entry *eq;
 	struct qman_portal *p;
+	struct qm_eqcr_entry *eq;
+	__maybe_unused unsigned long irqflags;
 
-#ifdef CONFIG_FSL_QMAN_CHECKING
-	if (unlikely(fq_isset(fq, QMAN_FQ_FLAG_NO_ENQUEUE)))
-		return -EINVAL;
-	if (unlikely(fq_isclear(fq, QMAN_FQ_FLAG_NO_MODIFY) &&
-			((fq->state == qman_fq_state_retired) ||
-			(fq->state == qman_fq_state_oos))))
-		return -EBUSY;
+#ifdef CONFIG_FSL_DPA_CAN_WAIT
+	if (flags & QMAN_ENQUEUE_FLAG_WAIT)
+		eq = wait_eq_start(&p, &irqflags, fq, fd, flags);
+	else
 #endif
-
-	eq = try_eq_start(&p);
-	if (unlikely(!eq)) {
-		if (flags & QMAN_ENQUEUE_FLAG_WAIT) {
-			eq = wait_eq_start(&p, flags);
-			if (!eq)
-				return -EBUSY;
-		} else
-			return -EBUSY;
+	eq = try_eq_start(&p, &irqflags, fq, fd, flags);
+	if (!eq)
+		return -EBUSY;
+	/* Process ORP-specifics here */
+	if (flags & QMAN_ENQUEUE_FLAG_NLIS)
+		orp_seqnum |= QM_EQCR_SEQNUM_NLIS;
+	else {
+		orp_seqnum &= ~QM_EQCR_SEQNUM_NLIS;
+		if (flags & QMAN_ENQUEUE_FLAG_NESN)
+			orp_seqnum |= QM_EQCR_SEQNUM_NESN;
+		else
+			/* No need to check 4 QMAN_ENQUEUE_FLAG_HOLE */
+			orp_seqnum &= ~QM_EQCR_SEQNUM_NESN;
 	}
-	/* If we're using ORP, it's very unwise to back-off because of
-	 * WATCH_CGR - that would leave a hole in the ORP sequence which could
-	 * block (if the caller doesn't retry). The idea is to enqueue via the
-	 * ORP anyway, and let congestion take effect in h/w once
-	 * order-restoration has occurred. */
-#if 0
-	if (unlikely(!orp && (flags & QMAN_ENQUEUE_FLAG_WATCH_CGR) && p->cgrs &&
-			fq_isset(fq, QMAN_FQ_STATE_CGR_EN) &&
-			qman_cgrs_get(&p->cgrs[1], fq->cgr_groupid))) {
-		eqcr_abort(p);
-		return -EAGAIN;
+	eq->seqnum = orp_seqnum;
+	eq->orp = orp->fqid;
+	/* Note: QM_EQCR_VERB_INTERRUPT == QMAN_ENQUEUE_FLAG_WAIT_SYNC */
+	qm_eqcr_pvb_commit(&p->p,
+		(flags & (QMAN_ENQUEUE_FLAG_HOLE | QMAN_ENQUEUE_FLAG_NESN)) ?
+				0 : QM_EQCR_VERB_CMD_ENQUEUE |
+		(flags & (QM_EQCR_VERB_COLOUR_MASK | QM_EQCR_VERB_INTERRUPT)));
+	local_irq_restore(irqflags);
+	put_affine_portal();
+#ifdef CONFIG_FSL_DPA_CAN_WAIT_SYNC
+	if (unlikely((flags & QMAN_ENQUEUE_FLAG_WAIT) &&
+			(flags & QMAN_ENQUEUE_FLAG_WAIT_SYNC))) {
+		if (flags & QMAN_ENQUEUE_FLAG_WAIT_INT)
+			wait_event_interruptible(affine_queue,
+					(p->eqci_owned != fq));
+		else
+			wait_event(affine_queue, (p->eqci_owned != fq));
 	}
 #endif
-	if (flags & QMAN_ENQUEUE_FLAG_DCA) {
-		u8 dca = QM_EQCR_DCA_ENABLE;
-		if (unlikely(flags & QMAN_ENQUEUE_FLAG_DCA_PARK))
-			dca |= QM_EQCR_DCA_PARK;
-		dca |= ((flags >> 8) & QM_EQCR_DCA_IDXMASK);
-		eq->dca = dca;
-	}
-	if (orp) {
-		if (flags & QMAN_ENQUEUE_FLAG_NLIS)
-			orp_seqnum |= QM_EQCR_SEQNUM_NLIS;
-		else {
-			orp_seqnum &= ~QM_EQCR_SEQNUM_NLIS;
-			if (flags & QMAN_ENQUEUE_FLAG_NESN)
-				orp_seqnum |= QM_EQCR_SEQNUM_NESN;
-			else
-				/* No need to check 4 QMAN_ENQUEUE_FLAG_HOLE */
-				orp_seqnum &= ~QM_EQCR_SEQNUM_NESN;
-		}
-		eq->seqnum = orp_seqnum;
-		eq->orp = orp_fq->fqid;
-	}
-	eq->fqid = fq->fqid;
-	eq->tag = (u32)fq;
-	/* gcc does a dreadful job of the following;
-	 *	eq->fd = *fd;
-	 * It causes the entire function to save/restore a wider range of
-	 * registers, and comes up with instruction-waste galore. This will do
-	 * until we can rework the function for better code-generation. */
-	{
-		int *eqfdptr = (int *)&eq->fd;
-		int *fdptr = (int *)fd;
-		int temp;
-		asm volatile (
-			"lwz %0,0(%2);"
-			"stw %0,0(%1);"
-			"lwz %0,4(%2);"
-			"stw %0,4(%1);"
-			"lwz %0,8(%2);"
-			"stw %0,8(%1);"
-			"lwz %0,12(%2);"
-			"stw %0,12(%1);"
-			: "=&r"(temp) : "b"(eqfdptr), "b"(fdptr)
-		);
-	}
-	/* Issue the enqueue command, and wait for sync if requested.
-	 * NB: design choice - the commit can't fail, only waiting can. Don't
-	 * propogate any failure if a signal arrives. Otherwise the caller can't
-	 * distinguish whether the enqueue was issued or not. Code for
-	 * user-space can check signal_pending() after we return. */
-	eqcr_commit(p, flags, orp);
 	return 0;
 }
+EXPORT_SYMBOL(qman_enqueue_orp);
 
-int qman_enqueue(struct qman_fq *fq, const struct qm_fd *fd, u32 flags)
+int qman_modify_cgr(struct qman_cgr *cgr, u32 flags,
+			struct qm_mcc_initcgr *opts)
 {
-	flags |= QM_EQCR_VERB_CMD_ENQUEUE;
-	return __enqueue(fq, fd, flags, NULL, 0, 0);
+	struct qm_mc_command *mcc;
+	struct qm_mc_result *mcr;
+	struct qman_portal *p = get_affine_portal();
+	__maybe_unused unsigned long irqflags;
+	u8 res;
+	u8 verb = QM_MCC_VERB_MODIFYCGR;
+
+	/* frame mode not supported on rev1 */
+	if (unlikely(qman_ip_rev == QMAN_REV1)) {
+		if (opts && (opts->we_mask & QM_CGR_WE_MODE) &&
+				opts->cgr.mode == QMAN_CGR_MODE_FRAME) {
+			put_affine_portal();
+			return -EIO;
+		}
+	}
+	local_irq_save(irqflags);
+	mcc = qm_mc_start(&p->p);
+	if (opts)
+		mcc->initcgr = *opts;
+	mcc->initcgr.cgid = cgr->cgrid;
+	if (flags & QMAN_CGR_FLAG_USE_INIT)
+		verb = QM_MCC_VERB_INITCGR;
+	qm_mc_commit(&p->p, verb);
+	while (!(mcr = qm_mc_result(&p->p)))
+		cpu_relax();
+	QM_ASSERT((mcr->verb & QM_MCR_VERB_MASK) == verb);
+	res = mcr->result;
+	local_irq_restore(irqflags);
+	put_affine_portal();
+	return (res == QM_MCR_RESULT_OK) ? 0 : -EIO;
 }
-EXPORT_SYMBOL(qman_enqueue);
+EXPORT_SYMBOL(qman_modify_cgr);
 
-int qman_enqueue_orp(struct qman_fq *fq, const struct qm_fd *fd, u32 flags,
-			struct qman_fq *orp, u16 orp_seqnum)
+#define TARG_MASK(n) (0x80000000 >> (n->config->channel - qm_channel_swportal0))
+
+int qman_create_cgr(struct qman_cgr *cgr, u32 flags,
+			struct qm_mcc_initcgr *opts)
 {
-	if (flags & (QMAN_ENQUEUE_FLAG_HOLE | QMAN_ENQUEUE_FLAG_NESN))
-		flags &= ~QM_EQCR_VERB_CMD_ENQUEUE;
+	unsigned long irqflags;
+	struct qm_mcr_querycgr cgr_state;
+	struct qm_mcc_initcgr local_opts;
+	int ret;
+	struct qman_portal *p = get_affine_portal();
+
+	memset(&local_opts, 0, sizeof(struct qm_mcc_initcgr));
+	cgr->chan = p->config->channel;
+	spin_lock_irqsave(&p->cgr_lock, irqflags);
+
+	/* if no opts specified and I'm not the first for this portal, just add
+	 * to the list */
+	if ((opts == NULL) && !list_empty(&p->cgr_cbs[cgr->cgrid]))
+		goto add_list;
+
+	ret = qman_query_cgr(cgr, &cgr_state);
+	if (ret)
+		goto release_lock;
+	if (opts)
+		local_opts = *opts;
+	/* Overwrite TARG */
+	local_opts.cgr.cscn_targ = cgr_state.cgr.cscn_targ | TARG_MASK(p);
+	local_opts.we_mask |= QM_CGR_WE_CSCN_TARG;
+
+	/* send init if flags indicate so */
+	if (opts && (flags & QMAN_CGR_FLAG_USE_INIT))
+		ret = qman_modify_cgr(cgr, QMAN_CGR_FLAG_USE_INIT, &local_opts);
 	else
-		flags |= QM_EQCR_VERB_CMD_ENQUEUE;
-	return __enqueue(fq, fd, flags, orp, orp_seqnum, 1);
+		ret = qman_modify_cgr(cgr, 0, &local_opts);
+	if (ret)
+		goto release_lock;
+add_list:
+	list_add(&cgr->node, &p->cgr_cbs[cgr->cgrid]);
+
+	/* Determine if newly added object requires its callback to be called */
+	ret = qman_query_cgr(cgr, &cgr_state);
+	if (ret) {
+		/* we can't go back, so proceed and return success, but screen
+		 * and wail to the log file */
+		pr_crit("CGR HW state partially modified\n");
+		ret = 0;
+		goto release_lock;
+	}
+	if (cgr->cb && cgr_state.cgr.cscn_en && qman_cgrs_get(&p->cgrs[1],
+							cgr->cgrid))
+		cgr->cb(p, cgr, 1);
+release_lock:
+	spin_unlock_irqrestore(&p->cgr_lock, irqflags);
+	put_affine_portal();
+	return ret;
 }
-EXPORT_SYMBOL(qman_enqueue_orp);
+EXPORT_SYMBOL(qman_create_cgr);
 
-int qman_init_cgr(u32 cgid)
+int qman_delete_cgr(struct qman_cgr *cgr)
 {
-	struct qm_mc_command *mcc;
-	struct qm_mc_result *mcr;
+	unsigned long irqflags;
+	struct qm_mcr_querycgr cgr_state;
+	struct qm_mcc_initcgr local_opts;
+	int ret = 0;
 	struct qman_portal *p = get_affine_portal();
-	u8 res;
 
-	local_irq_disable();
-	mcc = qm_mc_start(p->p);
-	mcc->initcgr.cgid = cgid;
-	qm_mc_commit(p->p, QM_MCC_VERB_INITCGR);
-	while (!(mcr = qm_mc_result(p->p)))
-		cpu_relax();
-	QM_ASSERT((mcr->verb & QM_MCR_VERB_MASK) == QM_MCC_VERB_INITCGR);
-	res = mcr->result;
-	if (res != QM_MCR_RESULT_OK) {
-		pr_err("INITCGR failed: %s\n", mcr_result_str(res));
+	if (cgr->chan != p->config->channel) {
+		pr_crit("Attempting to delete cgr from different portal "
+			"than it was create: create 0x%x, delete 0x%x\n",
+			cgr->chan, p->config->channel);
+		ret = -EINVAL;
+		goto put_portal;
+	}
+	memset(&local_opts, 0, sizeof(struct qm_mcc_initcgr));
+	spin_lock_irqsave(&p->cgr_lock, irqflags);
+	list_del(&cgr->node);
+	/* If last in list, CSCN_TARG must be set accordingly */
+	if (!list_empty(&p->cgr_cbs[cgr->cgrid]))
+		goto release_lock;
+	ret = qman_query_cgr(cgr, &cgr_state);
+	if (ret)  {
+		/* add back to the list */
+		list_add(&cgr->node, &p->cgr_cbs[cgr->cgrid]);
+		goto release_lock;
 	}
-	local_irq_enable();
+	/* Overwrite TARG */
+	local_opts.we_mask = QM_CGR_WE_CSCN_TARG;
+	local_opts.cgr.cscn_targ = cgr_state.cgr.cscn_targ & ~(TARG_MASK(p));
+	ret = qman_modify_cgr(cgr, 0, &local_opts);
+	if (ret)
+		/* add back to the list */
+		list_add(&cgr->node, &p->cgr_cbs[cgr->cgrid]);
+release_lock:
+	spin_unlock_irqrestore(&p->cgr_lock, irqflags);
+put_portal:
 	put_affine_portal();
-	return (res == QM_MCR_RESULT_OK) ? 0 : -EIO;
+	return ret;
 }
-EXPORT_SYMBOL(qman_init_cgr);
+EXPORT_SYMBOL(qman_delete_cgr);
 
diff --git a/drivers/hwqueue/qman_low.c b/drivers/hwqueue/qman_low.c
deleted file mode 100644
index c9e8920..0000000
--- a/drivers/hwqueue/qman_low.c
+++ /dev/null
@@ -1,1189 +0,0 @@
-/* Copyright (c) 2008-2010 Freescale Semiconductor, Inc.
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *     * Redistributions of source code must retain the above copyright
- *       notice, this list of conditions and the following disclaimer.
- *     * Redistributions in binary form must reproduce the above copyright
- *       notice, this list of conditions and the following disclaimer in the
- *       documentation and/or other materials provided with the distribution.
- *     * Neither the name of Freescale Semiconductor nor the
- *       names of its contributors may be used to endorse or promote products
- *       derived from this software without specific prior written permission.
- *
- *
- * ALTERNATIVELY, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") as published by the Free Software
- * Foundation, either version 2 of that License or (at your option) any
- * later version.
- *
- * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
- * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
- * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
- * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
- * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
- * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
- * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
- * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "qman_private.h"
-
-/***************************/
-/* Portal register assists */
-/***************************/
-
-/* Cache-inhibited register offsets */
-#define REG_EQCR_PI_CINH	(void *)0x0000
-#define REG_EQCR_CI_CINH	(void *)0x0004
-#define REG_EQCR_ITR		(void *)0x0008
-#define REG_DQRR_PI_CINH	(void *)0x0040
-#define REG_DQRR_CI_CINH	(void *)0x0044
-#define REG_DQRR_ITR		(void *)0x0048
-#define REG_DQRR_DCAP		(void *)0x0050
-#define REG_DQRR_SDQCR		(void *)0x0054
-#define REG_DQRR_VDQCR		(void *)0x0058
-#define REG_DQRR_PDQCR		(void *)0x005c
-#define REG_MR_PI_CINH		(void *)0x0080
-#define REG_MR_CI_CINH		(void *)0x0084
-#define REG_MR_ITR		(void *)0x0088
-#define REG_CFG			(void *)0x0100
-#define REG_ISR			(void *)0x0e00
-#define REG_ITPR		(void *)0x0e14
-
-/* Cache-enabled register offsets */
-#define CL_EQCR			(void *)0x0000
-#define CL_DQRR			(void *)0x1000
-#define CL_MR			(void *)0x2000
-#define CL_EQCR_PI_CENA		(void *)0x3000
-#define CL_EQCR_CI_CENA		(void *)0x3100
-#define CL_DQRR_PI_CENA		(void *)0x3200
-#define CL_DQRR_CI_CENA		(void *)0x3300
-#define CL_MR_PI_CENA		(void *)0x3400
-#define CL_MR_CI_CENA		(void *)0x3500
-#define CL_CR			(void *)0x3800
-#define CL_RR0			(void *)0x3900
-#define CL_RR1			(void *)0x3940
-
-/* The h/w design requires mappings to be size-aligned so that "add"s can be
- * reduced to "or"s. The primitives below do the same for s/w. */
-
-/* Bitwise-OR two pointers */
-static inline void *ptr_OR(void *a, void *b)
-{
-	return (void *)((unsigned long)a | (unsigned long)b);
-}
-
-/* Cache-inhibited register access */
-static inline u32 __qm_in(struct qm_addr *qm, void *offset)
-{
-	return in_be32(ptr_OR(qm->addr_ci, offset));
-}
-static inline void __qm_out(struct qm_addr *qm, void *offset, u32 val)
-{
-	out_be32(ptr_OR(qm->addr_ci, offset), val);
-}
-#define qm_in(reg)		__qm_in(&portal->addr, REG_##reg)
-#define qm_out(reg, val)	__qm_out(&portal->addr, REG_##reg, val)
-
-/* Convert 'n' cachelines to a pointer value for bitwise OR */
-#define qm_cl(n)		(void *)((n) << 6)
-
-/* Cache-enabled (index) register access */
-static inline void __qm_cl_touch_ro(struct qm_addr *qm, void *offset)
-{
-	dcbt_ro(ptr_OR(qm->addr_ce, offset));
-}
-static inline void __qm_cl_touch_rw(struct qm_addr *qm, void *offset)
-{
-	dcbt_rw(ptr_OR(qm->addr_ce, offset));
-}
-static inline u32 __qm_cl_in(struct qm_addr *qm, void *offset)
-{
-	return in_be32(ptr_OR(qm->addr_ce, offset));
-}
-static inline void __qm_cl_out(struct qm_addr *qm, void *offset, u32 val)
-{
-	out_be32(ptr_OR(qm->addr_ce, offset), val);
-	dcbf(ptr_OR(qm->addr_ce, offset));
-}
-static inline void __qm_cl_invalidate(struct qm_addr *qm, void *offset)
-{
-	dcbi(ptr_OR(qm->addr_ce, offset));
-}
-#define qm_cl_touch_ro(reg)	__qm_cl_touch_ro(&portal->addr, CL_##reg##_CENA)
-#define qm_cl_touch_rw(reg)	__qm_cl_touch_rw(&portal->addr, CL_##reg##_CENA)
-#define qm_cl_in(reg)		__qm_cl_in(&portal->addr, CL_##reg##_CENA)
-#define qm_cl_out(reg, val)	__qm_cl_out(&portal->addr, CL_##reg##_CENA, val)
-#define qm_cl_invalidate(reg) __qm_cl_invalidate(&portal->addr, CL_##reg##_CENA)
-
-/* Cyclic helper for rings. FIXME: once we are able to do fine-grain perf
- * analysis, look at using the "extra" bit in the ring index registers to avoid
- * cyclic issues. */
-static inline u8 cyc_diff(u8 ringsize, u8 first, u8 last)
-{
-	/* 'first' is included, 'last' is excluded */
-	if (first <= last)
-		return last - first;
-	return ringsize + last - first;
-}
-
-
-/* ---------------- */
-/* --- EQCR API --- */
-
-/* Bit-wise logic to wrap a ring pointer by clearing the "carry bit" */
-#define EQCR_CARRYCLEAR(p) \
-	(void *)((unsigned long)(p) & (~(unsigned long)(QM_EQCR_SIZE << 6)))
-
-/* Bit-wise logic to convert a ring pointer to a ring index */
-static inline u8 EQCR_PTR2IDX(struct qm_eqcr_entry *e)
-{
-	return ((u32)e >> 6) & (QM_EQCR_SIZE - 1);
-}
-
-/* Increment the 'cursor' ring pointer, taking 'vbit' into account */
-static inline void EQCR_INC(struct qm_eqcr *eqcr)
-{
-	/* NB: this is odd-looking, but experiments show that it generates fast
-	 * code with essentially no branching overheads. We increment to the
-	 * next EQCR pointer and handle overflow and 'vbit'. */
-	struct qm_eqcr_entry *partial = eqcr->cursor + 1;
-	eqcr->cursor = EQCR_CARRYCLEAR(partial);
-	if (partial != eqcr->cursor)
-		eqcr->vbit ^= QM_EQCR_VERB_VBIT;
-}
-
-int qm_eqcr_init(struct qm_portal *portal, enum qm_eqcr_pmode pmode,
-		__maybe_unused enum qm_eqcr_cmode cmode)
-{
-	/* This use of 'register', as well as all other occurances, is because
-	 * it has been observed to generate much faster code with gcc than is
-	 * otherwise the case. */
-	register struct qm_eqcr *eqcr = &portal->eqcr;
-	u32 cfg;
-	u8 pi;
-
-	if (__qm_portal_bind(portal, QM_BIND_EQCR))
-		return -EBUSY;
-	eqcr->ring = ptr_OR(portal->addr.addr_ce, CL_EQCR);
-	eqcr->ci = qm_in(EQCR_CI_CINH) & (QM_EQCR_SIZE - 1);
-	qm_cl_invalidate(EQCR_CI);
-	pi = qm_in(EQCR_PI_CINH) & (QM_EQCR_SIZE - 1);
-	eqcr->cursor = eqcr->ring + pi;
-	eqcr->vbit = (qm_in(EQCR_PI_CINH) & QM_EQCR_SIZE) ?
-			QM_EQCR_VERB_VBIT : 0;
-	eqcr->available = QM_EQCR_SIZE - 1 -
-			cyc_diff(QM_EQCR_SIZE, eqcr->ci, pi);
-	eqcr->ithresh = qm_in(EQCR_ITR);
-#ifdef CONFIG_FSL_QMAN_CHECKING
-	eqcr->busy = 0;
-	eqcr->pmode = pmode;
-	eqcr->cmode = cmode;
-#endif
-	cfg = (qm_in(CFG) & 0x00ffffff) |
-		((pmode & 0x3) << 24);	/* QCSP_CFG::EPM */
-	qm_out(CFG, cfg);
-	return 0;
-}
-EXPORT_SYMBOL(qm_eqcr_init);
-
-void qm_eqcr_finish(struct qm_portal *portal)
-{
-	register struct qm_eqcr *eqcr = &portal->eqcr;
-	u8 pi = qm_in(EQCR_PI_CINH) & (QM_EQCR_SIZE - 1);
-	u8 ci = qm_in(EQCR_CI_CINH) & (QM_EQCR_SIZE - 1);
-
-	QM_ASSERT(!eqcr->busy);
-	if (pi != EQCR_PTR2IDX(eqcr->cursor))
-		pr_crit("losing uncommited EQCR entries\n");
-	if (ci != eqcr->ci)
-		pr_crit("missing existing EQCR completions\n");
-	if (eqcr->ci != EQCR_PTR2IDX(eqcr->cursor))
-		pr_crit("EQCR destroyed unquiesced\n");
-	__qm_portal_unbind(portal, QM_BIND_EQCR);
-}
-EXPORT_SYMBOL(qm_eqcr_finish);
-
-struct qm_eqcr_entry *qm_eqcr_start(struct qm_portal *portal)
-{
-	register struct qm_eqcr *eqcr = &portal->eqcr;
-	QM_ASSERT(!eqcr->busy);
-	if (!eqcr->available)
-		return NULL;
-#ifdef CONFIG_FSL_QMAN_CHECKING
-	eqcr->busy = 1;
-#endif
-	dcbzl(eqcr->cursor);
-	return eqcr->cursor;
-}
-EXPORT_SYMBOL(qm_eqcr_start);
-
-void qm_eqcr_abort(struct qm_portal *portal)
-{
-	__maybe_unused register struct qm_eqcr *eqcr = &portal->eqcr;
-	QM_ASSERT(eqcr->busy);
-#ifdef CONFIG_FSL_QMAN_CHECKING
-	eqcr->busy = 0;
-#endif
-}
-EXPORT_SYMBOL(qm_eqcr_abort);
-
-struct qm_eqcr_entry *qm_eqcr_pend_and_next(struct qm_portal *portal, u8 myverb)
-{
-	register struct qm_eqcr *eqcr = &portal->eqcr;
-	QM_ASSERT(eqcr->busy);
-	QM_ASSERT(eqcr->pmode != qm_eqcr_pvb);
-	if (eqcr->available == 1)
-		return NULL;
-	eqcr->cursor->__dont_write_directly__verb = myverb | eqcr->vbit;
-	dcbf(eqcr->cursor);
-	EQCR_INC(eqcr);
-	eqcr->available--;
-	dcbzl(eqcr->cursor);
-	return eqcr->cursor;
-}
-EXPORT_SYMBOL(qm_eqcr_pend_and_next);
-
-#define EQCR_COMMIT_CHECKS(eqcr) \
-do { \
-	QM_ASSERT(eqcr->busy); \
-	QM_ASSERT(eqcr->cursor->orp == (eqcr->cursor->orp & 0x00ffffff)); \
-	QM_ASSERT(eqcr->cursor->fqid == (eqcr->cursor->fqid & 0x00ffffff)); \
-} while(0)
-
-#ifdef CONFIG_FSL_QMAN_BUG_AND_FEATURE_REV1
-static inline void eqcr_fd_fixup(struct qm_eqcr_entry *eq)
-{
-	if (qman_ip_rev == QMAN_REV1) {
-		struct qm_fd *fd = &eq->fd;
-		/* The struct's address went from 48-bit to 40-bit but rev1
-		 * chips will still interpret it as 48-bit, meaning we have to
-		 * scrub the upper 8-bits in that case, just in case the user
-		 * left noise in there. NB, this code is the most explicit but
-		 * if the compiler doesn't optimise it (they are 4-bits each of
-		 * the same byte), it may be more efficient to do;
-		 *    ((u8 *)fd)[2] = 0;
-		 */
-		fd->eliodn_offset = 0;
-		fd->__reserved = 0;
-	}
-}
-#else
-#define eqcr_fd_fixup(fd) do { ; } while (0)
-#endif
-
-void qm_eqcr_pci_commit(struct qm_portal *portal, u8 myverb)
-{
-	register struct qm_eqcr *eqcr = &portal->eqcr;
-	EQCR_COMMIT_CHECKS(eqcr);
-	QM_ASSERT(eqcr->pmode == qm_eqcr_pci);
-	eqcr_fd_fixup(eqcr->cursor);
-	eqcr->cursor->__dont_write_directly__verb = myverb | eqcr->vbit;
-	EQCR_INC(eqcr);
-	eqcr->available--;
-	dcbf(eqcr->cursor);
-	hwsync();
-	qm_out(EQCR_PI_CINH, EQCR_PTR2IDX(eqcr->cursor));
-#ifdef CONFIG_FSL_QMAN_CHECKING
-	eqcr->busy = 0;
-#endif
-}
-EXPORT_SYMBOL(qm_eqcr_pci_commit);
-
-void qm_eqcr_pce_prefetch(struct qm_portal *portal)
-{
-	 __maybe_unused register struct qm_eqcr *eqcr = &portal->eqcr;
-	QM_ASSERT(eqcr->pmode == qm_eqcr_pce);
-	qm_cl_invalidate(EQCR_PI);
-	qm_cl_touch_rw(EQCR_PI);
-}
-EXPORT_SYMBOL(qm_eqcr_pce_prefetch);
-
-void qm_eqcr_pce_commit(struct qm_portal *portal, u8 myverb)
-{
-	register struct qm_eqcr *eqcr = &portal->eqcr;
-	EQCR_COMMIT_CHECKS(eqcr);
-	QM_ASSERT(eqcr->pmode == qm_eqcr_pce);
-	eqcr_fd_fixup(eqcr->cursor);
-	eqcr->cursor->__dont_write_directly__verb = myverb | eqcr->vbit;
-	EQCR_INC(eqcr);
-	eqcr->available--;
-	dcbf(eqcr->cursor);
-	lwsync();
-	qm_cl_out(EQCR_PI, EQCR_PTR2IDX(eqcr->cursor));
-#ifdef CONFIG_FSL_QMAN_CHECKING
-	eqcr->busy = 0;
-#endif
-}
-EXPORT_SYMBOL(qm_eqcr_pce_commit);
-
-void qm_eqcr_pvb_commit(struct qm_portal *portal, u8 myverb)
-{
-	register struct qm_eqcr *eqcr = &portal->eqcr;
-	struct qm_eqcr_entry *eqcursor;
-	EQCR_COMMIT_CHECKS(eqcr);
-	QM_ASSERT(eqcr->pmode == qm_eqcr_pvb);
-	eqcr_fd_fixup(eqcr->cursor);
-	lwsync();
-	eqcursor = eqcr->cursor;
-	eqcursor->__dont_write_directly__verb = myverb | eqcr->vbit;
-	dcbf(eqcursor);
-	EQCR_INC(eqcr);
-	eqcr->available--;
-#ifdef CONFIG_FSL_QMAN_CHECKING
-	eqcr->busy = 0;
-#endif
-}
-EXPORT_SYMBOL(qm_eqcr_pvb_commit);
-
-u8 qm_eqcr_cci_update(struct qm_portal *portal)
-{
-	register struct qm_eqcr *eqcr = &portal->eqcr;
-	u8 diff, old_ci = eqcr->ci;
-	QM_ASSERT(eqcr->cmode == qm_eqcr_cci);
-	eqcr->ci = qm_in(EQCR_CI_CINH) & (QM_EQCR_SIZE - 1);
-	diff = cyc_diff(QM_EQCR_SIZE, old_ci, eqcr->ci);
-	eqcr->available += diff;
-	return diff;
-}
-EXPORT_SYMBOL(qm_eqcr_cci_update);
-
-void qm_eqcr_cce_prefetch(struct qm_portal *portal)
-{
-	__maybe_unused register struct qm_eqcr *eqcr = &portal->eqcr;
-	QM_ASSERT(eqcr->cmode == qm_eqcr_cce);
-	qm_cl_touch_ro(EQCR_CI);
-}
-EXPORT_SYMBOL(qm_eqcr_cce_prefetch);
-
-u8 qm_eqcr_cce_update(struct qm_portal *portal)
-{
-	register struct qm_eqcr *eqcr = &portal->eqcr;
-	u8 diff, old_ci = eqcr->ci;
-	QM_ASSERT(eqcr->cmode == qm_eqcr_cce);
-	eqcr->ci = qm_cl_in(EQCR_CI) & (QM_EQCR_SIZE - 1);
-	qm_cl_invalidate(EQCR_CI);
-	diff = cyc_diff(QM_EQCR_SIZE, old_ci, eqcr->ci);
-	eqcr->available += diff;
-	return diff;
-}
-EXPORT_SYMBOL(qm_eqcr_cce_update);
-
-u8 qm_eqcr_get_ithresh(struct qm_portal *portal)
-{
-	register struct qm_eqcr *eqcr = &portal->eqcr;
-	return eqcr->ithresh;
-}
-EXPORT_SYMBOL(qm_eqcr_get_ithresh);
-
-void qm_eqcr_set_ithresh(struct qm_portal *portal, u8 ithresh)
-{
-	register struct qm_eqcr *eqcr = &portal->eqcr;
-	eqcr->ithresh = ithresh;
-	qm_out(EQCR_ITR, ithresh);
-}
-EXPORT_SYMBOL(qm_eqcr_set_ithresh);
-
-u8 qm_eqcr_get_avail(struct qm_portal *portal)
-{
-	register struct qm_eqcr *eqcr = &portal->eqcr;
-	return eqcr->available;
-}
-EXPORT_SYMBOL(qm_eqcr_get_avail);
-
-u8 qm_eqcr_get_fill(struct qm_portal *portal)
-{
-	register struct qm_eqcr *eqcr = &portal->eqcr;
-	return QM_EQCR_SIZE - 1 - eqcr->available;
-}
-EXPORT_SYMBOL(qm_eqcr_get_fill);
-
-
-/* ---------------- */
-/* --- DQRR API --- */
-
-/* FIXME: many possible improvements;
- * - look at changing the API to use pointer rather than index parameters now
- *   that 'cursor' is a pointer,
- * - consider moving other parameters to pointer if it could help (ci)
- */
-
-#define DQRR_CARRYCLEAR(p) \
-	(void *)((unsigned long)(p) & (~(unsigned long)(QM_DQRR_SIZE << 6)))
-
-static inline u8 DQRR_PTR2IDX(struct qm_dqrr_entry *e)
-{
-	return ((u32)e >> 6) & (QM_DQRR_SIZE - 1);
-}
-
-static inline struct qm_dqrr_entry *DQRR_INC(struct qm_dqrr_entry *e)
-{
-	return DQRR_CARRYCLEAR(e + 1);
-}
-
-int qm_dqrr_init(struct qm_portal *portal, enum qm_dqrr_dmode dmode,
-		__maybe_unused enum qm_dqrr_pmode pmode,
-		enum qm_dqrr_cmode cmode, u8 max_fill,
-		int stash_ring, int stash_data)
-{
-	register struct qm_dqrr *dqrr = &portal->dqrr;
-	const struct qm_portal_config *config = qm_portal_config(portal);
-	u32 cfg;
-
-	if (__qm_portal_bind(portal, QM_BIND_DQRR))
-		return -EBUSY;
-	if ((stash_ring || stash_data) &&
-			((config->cpu == -1) || !config->has_hv_dma))
-		return -EINVAL;
-	/* Make sure the DQRR will be idle when we enable */
-	qm_out(DQRR_SDQCR, 0);
-	qm_out(DQRR_VDQCR, 0);
-	qm_out(DQRR_PDQCR, 0);
-	dqrr->ring = ptr_OR(portal->addr.addr_ce, CL_DQRR);
-	dqrr->pi = qm_in(DQRR_PI_CINH) & (QM_DQRR_SIZE - 1);
-	dqrr->ci = qm_in(DQRR_CI_CINH) & (QM_DQRR_SIZE - 1);
-	dqrr->cursor = dqrr->ring + dqrr->ci;
-	dqrr->fill = cyc_diff(QM_DQRR_SIZE, dqrr->ci, dqrr->pi);
-	dqrr->vbit = (qm_in(DQRR_PI_CINH) & QM_DQRR_SIZE) ?
-			QM_DQRR_VERB_VBIT : 0;
-	dqrr->ithresh = qm_in(DQRR_ITR);
-#ifdef CONFIG_FSL_QMAN_CHECKING
-	dqrr->dmode = dmode;
-	dqrr->pmode = pmode;
-	dqrr->cmode = cmode;
-	dqrr->flags = 0;
-	if (stash_ring)
-		dqrr->flags |= QM_DQRR_FLAG_RE;
-	if (stash_data)
-		dqrr->flags |= QM_DQRR_FLAG_SE;
-#endif
-	cfg = (qm_in(CFG) & 0xff000f00) |
-		((max_fill & (QM_DQRR_SIZE - 1)) << 20) | /* DQRR_MF */
-		((dmode & 1) << 18) |			/* DP */
-		((cmode & 3) << 16) |			/* DCM */
-		(stash_ring ? 0x80 : 0) |		/* RE */
-		(0 ? 0x40 : 0) |			/* Ignore RP */
-		(stash_data ? 0x20 : 0) |		/* SE */
-		(0 ? 0x10 : 0);				/* Ignore SP */
-	qm_out(CFG, cfg);
-	qm_dqrr_set_maxfill(portal, max_fill);
-	return 0;
-}
-EXPORT_SYMBOL(qm_dqrr_init);
-
-void qm_dqrr_finish(struct qm_portal *portal)
-{
-	register struct qm_dqrr *dqrr = &portal->dqrr;
-	if (dqrr->ci != DQRR_PTR2IDX(dqrr->cursor))
-		pr_crit("Ignoring completed DQRR entries\n");
-	__qm_portal_unbind(portal, QM_BIND_DQRR);
-}
-EXPORT_SYMBOL(qm_dqrr_finish);
-
-void qm_dqrr_current_prefetch(struct qm_portal *portal)
-{
-	register struct qm_dqrr *dqrr = &portal->dqrr;
-	/* If ring entries get stashed, don't invalidate/prefetch */
-	QM_ASSERT(!(dqrr->flags & QM_DQRR_FLAG_RE));
-	dcbt_ro(dqrr->cursor);
-}
-EXPORT_SYMBOL(qm_dqrr_current_prefetch);
-
-struct qm_dqrr_entry *qm_dqrr_current(struct qm_portal *portal)
-{
-	register struct qm_dqrr *dqrr = &portal->dqrr;
-	if (!dqrr->fill)
-		return NULL;
-	return dqrr->cursor;
-}
-EXPORT_SYMBOL(qm_dqrr_current);
-
-u8 qm_dqrr_cursor(struct qm_portal *portal)
-{
-	register struct qm_dqrr *dqrr = &portal->dqrr;
-	return DQRR_PTR2IDX(dqrr->cursor);
-}
-EXPORT_SYMBOL(qm_dqrr_cursor);
-
-u8 qm_dqrr_next(struct qm_portal *portal)
-{
-	register struct qm_dqrr *dqrr = &portal->dqrr;
-	QM_ASSERT(dqrr->fill);
-	dqrr->cursor = DQRR_INC(dqrr->cursor);
-	return --dqrr->fill;
-}
-EXPORT_SYMBOL(qm_dqrr_next);
-
-u8 qm_dqrr_pci_update(struct qm_portal *portal)
-{
-	register struct qm_dqrr *dqrr = &portal->dqrr;
-	u8 diff, old_pi = dqrr->pi;
-	QM_ASSERT(dqrr->pmode == qm_dqrr_pci);
-	dqrr->pi = qm_in(DQRR_PI_CINH) & (QM_DQRR_SIZE - 1);
-	diff = cyc_diff(QM_DQRR_SIZE, old_pi, dqrr->pi);
-	dqrr->fill += diff;
-	return diff;
-}
-EXPORT_SYMBOL(qm_dqrr_pci_update);
-
-void qm_dqrr_pce_prefetch(struct qm_portal *portal)
-{
-	__maybe_unused register struct qm_dqrr *dqrr = &portal->dqrr;
-	QM_ASSERT(dqrr->pmode == qm_dqrr_pce);
-	qm_cl_invalidate(DQRR_PI);
-	qm_cl_touch_ro(DQRR_PI);
-}
-EXPORT_SYMBOL(qm_dqrr_pce_prefetch);
-
-u8 qm_dqrr_pce_update(struct qm_portal *portal)
-{
-	register struct qm_dqrr *dqrr = &portal->dqrr;
-	u8 diff, old_pi = dqrr->pi;
-	QM_ASSERT(dqrr->pmode == qm_dqrr_pce);
-	dqrr->pi = qm_cl_in(DQRR_PI) & (QM_DQRR_SIZE - 1);
-	diff = cyc_diff(QM_DQRR_SIZE, old_pi, dqrr->pi);
-	dqrr->fill += diff;
-	return diff;
-}
-EXPORT_SYMBOL(qm_dqrr_pce_update);
-
-void qm_dqrr_pvb_prefetch(struct qm_portal *portal)
-{
-	register struct qm_dqrr *dqrr = &portal->dqrr;
-	QM_ASSERT(dqrr->pmode == qm_dqrr_pvb);
-	QM_ASSERT(!(dqrr->flags & QM_DQRR_FLAG_RE));
-	dcbi(ptr_OR(dqrr->ring, qm_cl(dqrr->pi)));
-	dcbt_ro(ptr_OR(dqrr->ring, qm_cl(dqrr->pi)));
-}
-EXPORT_SYMBOL(qm_dqrr_pvb_prefetch);
-
-u8 qm_dqrr_pvb_update(struct qm_portal *portal)
-{
-	register struct qm_dqrr *dqrr = &portal->dqrr;
-	struct qm_dqrr_entry *res = ptr_OR(dqrr->ring, qm_cl(dqrr->pi));
-	QM_ASSERT(dqrr->pmode == qm_dqrr_pvb);
-	if ((res->verb & QM_DQRR_VERB_VBIT) == dqrr->vbit) {
-		dqrr->pi = (dqrr->pi + 1) & (QM_DQRR_SIZE - 1);
-		if (!dqrr->pi)
-			dqrr->vbit ^= QM_DQRR_VERB_VBIT;
-		dqrr->fill++;
-		return 1;
-	}
-	return 0;
-}
-EXPORT_SYMBOL(qm_dqrr_pvb_update);
-
-void qm_dqrr_cci_consume(struct qm_portal *portal, u8 num)
-{
-	register struct qm_dqrr *dqrr = &portal->dqrr;
-	QM_ASSERT(dqrr->cmode == qm_dqrr_cci);
-	dqrr->ci = (dqrr->ci + num) & (QM_DQRR_SIZE - 1);
-	qm_out(DQRR_CI_CINH, dqrr->ci);
-}
-EXPORT_SYMBOL(qm_dqrr_cci_consume);
-
-void qm_dqrr_cci_consume_to_current(struct qm_portal *portal)
-{
-	register struct qm_dqrr *dqrr = &portal->dqrr;
-	QM_ASSERT(dqrr->cmode == qm_dqrr_cci);
-	dqrr->ci = DQRR_PTR2IDX(dqrr->cursor);
-	qm_out(DQRR_CI_CINH, dqrr->ci);
-}
-EXPORT_SYMBOL(qm_dqrr_cci_consume_to_current);
-
-void qm_dqrr_cce_prefetch(struct qm_portal *portal)
-{
-	__maybe_unused register struct qm_dqrr *dqrr = &portal->dqrr;
-	QM_ASSERT(dqrr->cmode == qm_dqrr_cce);
-	qm_cl_invalidate(DQRR_CI);
-	qm_cl_touch_rw(DQRR_CI);
-}
-EXPORT_SYMBOL(qm_dqrr_cce_prefetch);
-
-void qm_dqrr_cce_consume(struct qm_portal *portal, u8 num)
-{
-	register struct qm_dqrr *dqrr = &portal->dqrr;
-	QM_ASSERT(dqrr->cmode == qm_dqrr_cce);
-	dqrr->ci = (dqrr->ci + num) & (QM_DQRR_SIZE - 1);
-	qm_cl_out(DQRR_CI, dqrr->ci);
-}
-EXPORT_SYMBOL(qm_dqrr_cce_consume);
-
-void qm_dqrr_cce_consume_to_current(struct qm_portal *portal)
-{
-	register struct qm_dqrr *dqrr = &portal->dqrr;
-	QM_ASSERT(dqrr->cmode == qm_dqrr_cce);
-	dqrr->ci = DQRR_PTR2IDX(dqrr->cursor);
-	qm_cl_out(DQRR_CI, dqrr->ci);
-}
-EXPORT_SYMBOL(qm_dqrr_cce_consume_to_current);
-
-void qm_dqrr_cdc_consume_1(struct qm_portal *portal, u8 idx, int park)
-{
-	__maybe_unused register struct qm_dqrr *dqrr = &portal->dqrr;
-	QM_ASSERT(dqrr->cmode == qm_dqrr_cdc);
-	QM_ASSERT(idx < QM_DQRR_SIZE);
-	qm_out(DQRR_DCAP, (0 << 8) |	/* S */
-		((park ? 1 : 0) << 6) |	/* PK */
-		idx);			/* DCAP_CI */
-}
-EXPORT_SYMBOL(qm_dqrr_cdc_consume_1);
-
-void qm_dqrr_cdc_consume_1ptr(struct qm_portal *portal, struct qm_dqrr_entry *dq,
-				int park)
-{
-	__maybe_unused register struct qm_dqrr *dqrr = &portal->dqrr;
-	u8 idx = DQRR_PTR2IDX(dq);
-	QM_ASSERT(dqrr->cmode == qm_dqrr_cdc);
-	QM_ASSERT((dqrr->ring + idx) == dq);
-	QM_ASSERT(idx < QM_DQRR_SIZE);
-	qm_out(DQRR_DCAP, (0 << 8) |		/* DQRR_DCAP::S */
-		((park ? 1 : 0) << 6) |		/* DQRR_DCAP::PK */
-		idx);				/* DQRR_DCAP::DCAP_CI */
-}
-EXPORT_SYMBOL(qm_dqrr_cdc_consume_1ptr);
-
-void qm_dqrr_cdc_consume_n(struct qm_portal *portal, u16 bitmask)
-{
-	__maybe_unused register struct qm_dqrr *dqrr = &portal->dqrr;
-	QM_ASSERT(dqrr->cmode == qm_dqrr_cdc);
-	qm_out(DQRR_DCAP, (1 << 8) |		/* DQRR_DCAP::S */
-		((u32)bitmask << 16));		/* DQRR_DCAP::DCAP_CI */
-}
-EXPORT_SYMBOL(qm_dqrr_cdc_consume_n);
-
-u8 qm_dqrr_cdc_cci(struct qm_portal *portal)
-{
-	__maybe_unused register struct qm_dqrr *dqrr = &portal->dqrr;
-	QM_ASSERT(dqrr->cmode == qm_dqrr_cdc);
-	return qm_in(DQRR_CI_CINH) & (QM_DQRR_SIZE - 1);
-}
-EXPORT_SYMBOL(qm_dqrr_cdc_cci);
-
-void qm_dqrr_cdc_cce_prefetch(struct qm_portal *portal)
-{
-	__maybe_unused register struct qm_dqrr *dqrr = &portal->dqrr;
-	QM_ASSERT(dqrr->cmode == qm_dqrr_cdc);
-	qm_cl_invalidate(DQRR_CI);
-	qm_cl_touch_ro(DQRR_CI);
-}
-EXPORT_SYMBOL(qm_dqrr_cdc_cce_prefetch);
-
-u8 qm_dqrr_cdc_cce(struct qm_portal *portal)
-{
-	__maybe_unused register struct qm_dqrr *dqrr = &portal->dqrr;
-	QM_ASSERT(dqrr->cmode == qm_dqrr_cdc);
-	return qm_cl_in(DQRR_CI) & (QM_DQRR_SIZE - 1);
-}
-EXPORT_SYMBOL(qm_dqrr_cdc_cce);
-
-u8 qm_dqrr_get_ci(struct qm_portal *portal)
-{
-	register struct qm_dqrr *dqrr = &portal->dqrr;
-	QM_ASSERT(dqrr->cmode != qm_dqrr_cdc);
-	return dqrr->ci;
-}
-EXPORT_SYMBOL(qm_dqrr_get_ci);
-
-void qm_dqrr_park(struct qm_portal *portal, u8 idx)
-{
-	__maybe_unused register struct qm_dqrr *dqrr = &portal->dqrr;
-	QM_ASSERT(dqrr->cmode != qm_dqrr_cdc);
-	qm_out(DQRR_DCAP, (0 << 8) |		/* S */
-		(1 << 6) |			/* PK */
-		(idx & (QM_DQRR_SIZE - 1)));	/* DCAP_CI */
-}
-EXPORT_SYMBOL(qm_dqrr_park);
-
-void qm_dqrr_park_ci(struct qm_portal *portal)
-{
-	register struct qm_dqrr *dqrr = &portal->dqrr;
-	QM_ASSERT(dqrr->cmode != qm_dqrr_cdc);
-	qm_out(DQRR_DCAP, (0 << 8) |		/* S */
-		(1 << 6) |			/* PK */
-		(dqrr->ci & (QM_DQRR_SIZE - 1)));/* DCAP_CI */
-}
-EXPORT_SYMBOL(qm_dqrr_park_ci);
-
-void qm_dqrr_sdqcr_set(struct qm_portal *portal, u32 sdqcr)
-{
-	qm_out(DQRR_SDQCR, sdqcr);
-}
-EXPORT_SYMBOL(qm_dqrr_sdqcr_set);
-
-u32 qm_dqrr_sdqcr_get(struct qm_portal *portal)
-{
-	return qm_in(DQRR_SDQCR);
-}
-EXPORT_SYMBOL(qm_dqrr_sdqcr_get);
-
-void qm_dqrr_vdqcr_set(struct qm_portal *portal, u32 vdqcr)
-{
-	qm_out(DQRR_VDQCR, vdqcr);
-}
-EXPORT_SYMBOL(qm_dqrr_vdqcr_set);
-
-u32 qm_dqrr_vdqcr_get(struct qm_portal *portal)
-{
-	return qm_in(DQRR_VDQCR);
-}
-EXPORT_SYMBOL(qm_dqrr_vdqcr_get);
-
-void qm_dqrr_pdqcr_set(struct qm_portal *portal, u32 pdqcr)
-{
-	qm_out(DQRR_PDQCR, pdqcr);
-}
-EXPORT_SYMBOL(qm_dqrr_pdqcr_set);
-
-u32 qm_dqrr_pdqcr_get(struct qm_portal *portal)
-{
-	return qm_in(DQRR_PDQCR);
-}
-EXPORT_SYMBOL(qm_dqrr_pdqcr_get);
-
-u8 qm_dqrr_get_ithresh(struct qm_portal *portal)
-{
-	register struct qm_dqrr *dqrr = &portal->dqrr;
-	return dqrr->ithresh;
-}
-EXPORT_SYMBOL(qm_dqrr_get_ithresh);
-
-void qm_dqrr_set_ithresh(struct qm_portal *portal, u8 ithresh)
-{
-	qm_out(DQRR_ITR, ithresh);
-}
-EXPORT_SYMBOL(qm_dqrr_set_ithresh);
-
-u8 qm_dqrr_get_maxfill(struct qm_portal *portal)
-{
-	return (qm_in(CFG) & 0x00f00000) >> 20;
-}
-EXPORT_SYMBOL(qm_dqrr_get_maxfill);
-
-void qm_dqrr_set_maxfill(struct qm_portal *portal, u8 mf)
-{
-	qm_out(CFG, (qm_in(CFG) & 0xff0fffff) |
-		((mf & (QM_DQRR_SIZE - 1)) << 20));
-}
-EXPORT_SYMBOL(qm_dqrr_set_maxfill);
-
-
-/* -------------- */
-/* --- MR API --- */
-
-#define MR_CARRYCLEAR(p) \
-	(void *)((unsigned long)(p) & (~(unsigned long)(QM_MR_SIZE << 6)))
-
-static inline u8 MR_PTR2IDX(struct qm_mr_entry *e)
-{
-	return ((u32)e >> 6) & (QM_MR_SIZE - 1);
-}
-
-static inline struct qm_mr_entry *MR_INC(struct qm_mr_entry *e)
-{
-	return MR_CARRYCLEAR(e + 1);
-}
-
-int qm_mr_init(struct qm_portal *portal, enum qm_mr_pmode pmode,
-		enum qm_mr_cmode cmode)
-{
-	register struct qm_mr *mr = &portal->mr;
-	u32 cfg;
-
-#ifdef CONFIG_FSL_QMAN_BUG_AND_FEATURE_REV1
-	if ((qman_ip_rev == QMAN_REV1) && (pmode != qm_mr_pvb)) {
-		pr_err("Qman is rev1, so QMAN9 workaround requires 'pvb'\n");
-		return -EINVAL;
-	}
-#endif
-	if (__qm_portal_bind(portal, QM_BIND_MR))
-		return -EBUSY;
-	mr->ring = ptr_OR(portal->addr.addr_ce, CL_MR);
-	mr->pi = qm_in(MR_PI_CINH) & (QM_MR_SIZE - 1);
-	mr->ci = qm_in(MR_CI_CINH) & (QM_MR_SIZE - 1);
-#ifdef CONFIG_FSL_QMAN_BUG_AND_FEATURE_REV1
-	if (qman_ip_rev == QMAN_REV1)
-		/* Situate the cursor in the shadow ring */
-		mr->cursor = portal->bugs->mr + mr->ci;
-	else
-#endif
-	mr->cursor = mr->ring + mr->ci;
-	mr->fill = cyc_diff(QM_MR_SIZE, mr->ci, mr->pi);
-	mr->vbit = (qm_in(MR_PI_CINH) & QM_MR_SIZE) ? QM_MR_VERB_VBIT : 0;
-	mr->ithresh = qm_in(MR_ITR);
-#ifdef CONFIG_FSL_QMAN_CHECKING
-	mr->pmode = pmode;
-	mr->cmode = cmode;
-#endif
-	cfg = (qm_in(CFG) & 0xfffff0ff) |
-		((cmode & 1) << 8);		/* QCSP_CFG:MM */
-	qm_out(CFG, cfg);
-	return 0;
-}
-EXPORT_SYMBOL(qm_mr_init);
-
-void qm_mr_finish(struct qm_portal *portal)
-{
-	register struct qm_mr *mr = &portal->mr;
-	if (mr->ci != MR_PTR2IDX(mr->cursor))
-		pr_crit("Ignoring completed MR entries\n");
-	__qm_portal_unbind(portal, QM_BIND_MR);
-}
-EXPORT_SYMBOL(qm_mr_finish);
-
-void qm_mr_current_prefetch(struct qm_portal *portal)
-{
-	register struct qm_mr *mr = &portal->mr;
-	dcbt_ro(mr->cursor);
-}
-EXPORT_SYMBOL(qm_mr_current_prefetch);
-
-struct qm_mr_entry *qm_mr_current(struct qm_portal *portal)
-{
-	register struct qm_mr *mr = &portal->mr;
-	if (!mr->fill)
-		return NULL;
-	return mr->cursor;
-}
-EXPORT_SYMBOL(qm_mr_current);
-
-u8 qm_mr_cursor(struct qm_portal *portal)
-{
-	register struct qm_mr *mr = &portal->mr;
-	return MR_PTR2IDX(mr->cursor);
-}
-EXPORT_SYMBOL(qm_mr_cursor);
-
-u8 qm_mr_next(struct qm_portal *portal)
-{
-	register struct qm_mr *mr = &portal->mr;
-	QM_ASSERT(mr->fill);
-	mr->cursor = MR_INC(mr->cursor);
-	return --mr->fill;
-}
-EXPORT_SYMBOL(qm_mr_next);
-
-u8 qm_mr_pci_update(struct qm_portal *portal)
-{
-	register struct qm_mr *mr = &portal->mr;
-	u8 diff, old_pi = mr->pi;
-	QM_ASSERT(mr->pmode == qm_mr_pci);
-	mr->pi = qm_in(MR_PI_CINH);
-	diff = cyc_diff(QM_MR_SIZE, old_pi, mr->pi);
-	mr->fill += diff;
-	return diff;
-}
-EXPORT_SYMBOL(qm_mr_pci_update);
-
-void qm_mr_pce_prefetch(struct qm_portal *portal)
-{
-	__maybe_unused register struct qm_mr *mr = &portal->mr;
-	QM_ASSERT(mr->pmode == qm_mr_pce);
-	qm_cl_invalidate(MR_PI);
-	qm_cl_touch_ro(MR_PI);
-}
-EXPORT_SYMBOL(qm_mr_pce_prefetch);
-
-u8 qm_mr_pce_update(struct qm_portal *portal)
-{
-	register struct qm_mr *mr = &portal->mr;
-	u8 diff, old_pi = mr->pi;
-	QM_ASSERT(mr->pmode == qm_mr_pce);
-	mr->pi = qm_cl_in(MR_PI) & (QM_MR_SIZE - 1);
-	diff = cyc_diff(QM_MR_SIZE, old_pi, mr->pi);
-	mr->fill += diff;
-	return diff;
-}
-EXPORT_SYMBOL(qm_mr_pce_update);
-
-void qm_mr_pvb_prefetch(struct qm_portal *portal)
-{
-	register struct qm_mr *mr = &portal->mr;
-	QM_ASSERT(mr->pmode == qm_mr_pvb);
-	dcbi(ptr_OR(mr->ring, qm_cl(mr->pi)));
-	dcbt_ro(ptr_OR(mr->ring, qm_cl(mr->pi)));
-}
-EXPORT_SYMBOL(qm_mr_pvb_prefetch);
-
-u8 qm_mr_pvb_update(struct qm_portal *portal)
-{
-	register struct qm_mr *mr = &portal->mr;
-	struct qm_mr_entry *res = ptr_OR(mr->ring, qm_cl(mr->pi));
-	QM_ASSERT(mr->pmode == qm_mr_pvb);
-	if ((res->verb & QM_MR_VERB_VBIT) == mr->vbit) {
-#ifdef CONFIG_FSL_QMAN_BUG_AND_FEATURE_REV1
-		/* New MR entry, on affected chips, copy this to the shadow ring
-		 * and fixup if required. */
-		if (qman_ip_rev == QMAN_REV1) {
-			struct qm_mr_entry *shadow = ptr_OR(portal->bugs->mr,
-							qm_cl(mr->pi));
-			memcpy(shadow, res, sizeof(*res));
-			/* Bypass the QM_MR_RC_*** definitions, and check the
-			 * byte value directly to handle the erratum. */
-			if (shadow->ern.rc == 0x06)
-				shadow->ern.rc = 0x60;
-		}
-#endif
-		mr->pi = (mr->pi + 1) & (QM_MR_SIZE - 1);
-		if (!mr->pi)
-			mr->vbit ^= QM_MR_VERB_VBIT;
-		mr->fill++;
-		return 1;
-	}
-	return 0;
-}
-EXPORT_SYMBOL(qm_mr_pvb_update);
-
-void qm_mr_cci_consume(struct qm_portal *portal, u8 num)
-{
-	register struct qm_mr *mr = &portal->mr;
-	QM_ASSERT(mr->cmode == qm_mr_cci);
-	mr->ci = (mr->ci + num) & (QM_MR_SIZE - 1);
-	qm_out(MR_CI_CINH, mr->ci);
-}
-EXPORT_SYMBOL(qm_mr_cci_consume);
-
-void qm_mr_cci_consume_to_current(struct qm_portal *portal)
-{
-	register struct qm_mr *mr = &portal->mr;
-	QM_ASSERT(mr->cmode == qm_mr_cci);
-	mr->ci = MR_PTR2IDX(mr->cursor);
-	qm_out(MR_CI_CINH, mr->ci);
-}
-EXPORT_SYMBOL(qm_mr_cci_consume_to_current);
-
-void qm_mr_cce_prefetch(struct qm_portal *portal)
-{
-	__maybe_unused register struct qm_mr *mr = &portal->mr;
-	QM_ASSERT(mr->cmode == qm_mr_cce);
-	qm_cl_invalidate(MR_CI);
-	qm_cl_touch_rw(MR_CI);
-}
-EXPORT_SYMBOL(qm_mr_cce_prefetch);
-
-void qm_mr_cce_consume(struct qm_portal *portal, u8 num)
-{
-	register struct qm_mr *mr = &portal->mr;
-	QM_ASSERT(mr->cmode == qm_mr_cce);
-	mr->ci = (mr->ci + num) & (QM_MR_SIZE - 1);
-	qm_cl_out(MR_CI, mr->ci);
-}
-EXPORT_SYMBOL(qm_mr_cce_consume);
-
-void qm_mr_cce_consume_to_current(struct qm_portal *portal)
-{
-	register struct qm_mr *mr = &portal->mr;
-	QM_ASSERT(mr->cmode == qm_mr_cce);
-	mr->ci = MR_PTR2IDX(mr->cursor);
-	qm_cl_out(MR_CI, mr->ci);
-}
-EXPORT_SYMBOL(qm_mr_cce_consume_to_current);
-
-u8 qm_mr_get_ci(struct qm_portal *portal)
-{
-	register struct qm_mr *mr = &portal->mr;
-	return mr->ci;
-}
-EXPORT_SYMBOL(qm_mr_get_ci);
-
-u8 qm_mr_get_ithresh(struct qm_portal *portal)
-{
-	register struct qm_mr *mr = &portal->mr;
-	return mr->ithresh;
-}
-EXPORT_SYMBOL(qm_mr_get_ithresh);
-
-void qm_mr_set_ithresh(struct qm_portal *portal, u8 ithresh)
-{
-	qm_out(MR_ITR, ithresh);
-}
-EXPORT_SYMBOL(qm_mr_set_ithresh);
-
-
-/* ------------------------------ */
-/* --- Management command API --- */
-
-int qm_mc_init(struct qm_portal *portal)
-{
-	register struct qm_mc *mc = &portal->mc;
-	if (__qm_portal_bind(portal, QM_BIND_MC))
-		return -EBUSY;
-	mc->cr = ptr_OR(portal->addr.addr_ce, CL_CR);
-	mc->rr = ptr_OR(portal->addr.addr_ce, CL_RR0);
-	mc->rridx = (mc->cr->__dont_write_directly__verb & QM_MCC_VERB_VBIT) ?
-			0 : 1;
-	mc->vbit = mc->rridx ? QM_MCC_VERB_VBIT : 0;
-#ifdef CONFIG_FSL_QMAN_CHECKING
-	mc->state = mc_idle;
-#endif
-	return 0;
-}
-EXPORT_SYMBOL(qm_mc_init);
-
-void qm_mc_finish(struct qm_portal *portal)
-{
-	__maybe_unused register struct qm_mc *mc = &portal->mc;
-	QM_ASSERT(mc->state == mc_idle);
-#ifdef CONFIG_FSL_QMAN_CHECKING
-	if (mc->state != mc_idle)
-		pr_crit("Losing incomplete MC command\n");
-#endif
-	__qm_portal_unbind(portal, QM_BIND_MC);
-}
-EXPORT_SYMBOL(qm_mc_finish);
-
-struct qm_mc_command *qm_mc_start(struct qm_portal *portal)
-{
-	register struct qm_mc *mc = &portal->mc;
-	QM_ASSERT(mc->state == mc_idle);
-#ifdef CONFIG_FSL_QMAN_CHECKING
-	mc->state = mc_user;
-#endif
-	dcbzl(mc->cr);
-	return mc->cr;
-}
-EXPORT_SYMBOL(qm_mc_start);
-
-void qm_mc_abort(struct qm_portal *portal)
-{
-	__maybe_unused register struct qm_mc *mc = &portal->mc;
-	QM_ASSERT(mc->state == mc_user);
-#ifdef CONFIG_FSL_QMAN_CHECKING
-	mc->state = mc_idle;
-#endif
-}
-EXPORT_SYMBOL(qm_mc_abort);
-
-void qm_mc_commit(struct qm_portal *portal, u8 myverb)
-{
-	register struct qm_mc *mc = &portal->mc;
-	struct qm_mc_result *rr = mc->rr + mc->rridx;
-	QM_ASSERT(mc->state == mc_user);
-	dcbi(rr);
-	lwsync();
-#ifdef CONFIG_FSL_QMAN_BUG_AND_FEATURE_REV1
-	if ((qman_ip_rev == QMAN_REV1) && ((myverb & QM_MCC_VERB_MASK) ==
-					QM_MCC_VERB_INITFQ_SCHED)) {
-		u32 fqid = mc->cr->initfq.fqid;
-		/* Do two commands to avoid the hw bug. Note, we poll locally
-		 * rather than using qm_mc_result() because from a QMAN_CHECKING
-		 * perspective, we don't want to appear to have "finished" until
-		 * both commands are done. */
-		mc->cr->__dont_write_directly__verb = mc->vbit |
-					QM_MCC_VERB_INITFQ_PARKED;
-		dcbf(mc->cr);
-		portal->bugs->initfq_and_sched = 1;
-		do {
-			dcbi(rr);
-			dcbt_ro(rr);
-			barrier();
-		} while (!rr->verb);
-#ifdef CONFIG_FSL_QMAN_CHECKING
-		mc->state = mc_idle;
-#endif
-		if (rr->result != QM_MCR_RESULT_OK) {
-#ifdef CONFIG_FSL_QMAN_CHECKING
-			mc->state = mc_hw;
-#endif
-			return;
-		}
-		mc->rridx ^= 1;
-		mc->vbit ^= QM_MCC_VERB_VBIT;
-		rr = mc->rr + mc->rridx;
-		dcbzl(mc->cr);
-		mc->cr->alterfq.fqid = fqid;
-		dcbi(rr);
-		lwsync();
-		myverb = QM_MCC_VERB_ALTER_SCHED;
-	} else
-		portal->bugs->initfq_and_sched = 0;
-#endif
-	mc->cr->__dont_write_directly__verb = myverb | mc->vbit;
-	dcbf(mc->cr);
-	dcbt_ro(rr);
-#ifdef CONFIG_FSL_QMAN_CHECKING
-	mc->state = mc_hw;
-#endif
-}
-EXPORT_SYMBOL(qm_mc_commit);
-
-struct qm_mc_result *qm_mc_result(struct qm_portal *portal)
-{
-	register struct qm_mc *mc = &portal->mc;
-	struct qm_mc_result *rr = mc->rr + mc->rridx;
-	QM_ASSERT(mc->state == mc_hw);
-	/* The inactive response register's verb byte always returns zero until
-	 * its command is submitted and completed. This includes the valid-bit,
-	 * in case you were wondering... */
-	if (!rr->verb) {
-		dcbi(rr);
-		dcbt_ro(rr);
-		return NULL;
-	}
-#ifdef CONFIG_FSL_QMAN_BUG_AND_FEATURE_REV1
-	if (qman_ip_rev == QMAN_REV1) {
-		if ((rr->verb & QM_MCR_VERB_MASK) == QM_MCR_VERB_QUERYFQ) {
-			void *misplaced = (void *)rr + 50;
-			memcpy(&portal->bugs->result, rr, sizeof(*rr));
-			rr = &portal->bugs->result;
-			memcpy(&rr->queryfq.fqd.td, misplaced,
-				sizeof(rr->queryfq.fqd.td));
-		} else if (portal->bugs->initfq_and_sched) {
-			/* We split the user-requested command, make the final
-			 * result match the requested type. */
-			memcpy(&portal->bugs->result, rr, sizeof(*rr));
-			rr = &portal->bugs->result;
-			rr->verb = (rr->verb & QM_MCR_VERB_RRID) |
-					QM_MCR_VERB_INITFQ_SCHED;
-		}
-	}
-#endif
-	mc->rridx ^= 1;
-	mc->vbit ^= QM_MCC_VERB_VBIT;
-#ifdef CONFIG_FSL_QMAN_CHECKING
-	mc->state = mc_idle;
-#endif
-	return rr;
-}
-EXPORT_SYMBOL(qm_mc_result);
-
-
-/* ------------------------------------- */
-/* --- Portal interrupt register API --- */
-
-int qm_isr_init(struct qm_portal *portal)
-{
-	if (__qm_portal_bind(portal, QM_BIND_ISR))
-		return -EBUSY;
-	return 0;
-}
-EXPORT_SYMBOL(qm_isr_init);
-
-void qm_isr_finish(struct qm_portal *portal)
-{
-	__qm_portal_unbind(portal, QM_BIND_ISR);
-}
-EXPORT_SYMBOL(qm_isr_finish);
-
-void qm_isr_set_iperiod(struct qm_portal *portal, u16 iperiod)
-{
-	qm_out(ITPR, iperiod);
-}
-EXPORT_SYMBOL(qm_isr_set_iperiod);
-
-u32 __qm_isr_read(struct qm_portal *portal, enum qm_isr_reg n)
-{
-	return __qm_in(&portal->addr, REG_ISR + (n << 2));
-}
-EXPORT_SYMBOL(__qm_isr_read);
-
-void __qm_isr_write(struct qm_portal *portal, enum qm_isr_reg n, u32 val)
-{
-	__qm_out(&portal->addr, REG_ISR + (n << 2), val);
-}
-EXPORT_SYMBOL(__qm_isr_write);
-
diff --git a/drivers/hwqueue/qman_low.h b/drivers/hwqueue/qman_low.h
new file mode 100644
index 0000000..cb5b81c
--- /dev/null
+++ b/drivers/hwqueue/qman_low.h
@@ -0,0 +1,1247 @@
+/* Copyright (c) 2008-2010 Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "qman_private.h"
+
+/***************************/
+/* Portal register assists */
+/***************************/
+
+/* Cache-inhibited register offsets */
+#define REG_EQCR_PI_CINH	(void *)0x0000
+#define REG_EQCR_CI_CINH	(void *)0x0004
+#define REG_EQCR_ITR		(void *)0x0008
+#define REG_DQRR_PI_CINH	(void *)0x0040
+#define REG_DQRR_CI_CINH	(void *)0x0044
+#define REG_DQRR_ITR		(void *)0x0048
+#define REG_DQRR_DCAP		(void *)0x0050
+#define REG_DQRR_SDQCR		(void *)0x0054
+#define REG_DQRR_VDQCR		(void *)0x0058
+#define REG_DQRR_PDQCR		(void *)0x005c
+#define REG_MR_PI_CINH		(void *)0x0080
+#define REG_MR_CI_CINH		(void *)0x0084
+#define REG_MR_ITR		(void *)0x0088
+#define REG_CFG			(void *)0x0100
+#define REG_ISR			(void *)0x0e00
+#define REG_ITPR		(void *)0x0e14
+
+/* Cache-enabled register offsets */
+#define CL_EQCR			(void *)0x0000
+#define CL_DQRR			(void *)0x1000
+#define CL_MR			(void *)0x2000
+#define CL_EQCR_PI_CENA		(void *)0x3000
+#define CL_EQCR_CI_CENA		(void *)0x3100
+#define CL_DQRR_PI_CENA		(void *)0x3200
+#define CL_DQRR_CI_CENA		(void *)0x3300
+#define CL_MR_PI_CENA		(void *)0x3400
+#define CL_MR_CI_CENA		(void *)0x3500
+#define CL_CR			(void *)0x3800
+#define CL_RR0			(void *)0x3900
+#define CL_RR1			(void *)0x3940
+
+/* The h/w design requires mappings to be size-aligned so that "add"s can be
+ * reduced to "or"s. The primitives below do the same for s/w. */
+
+/* Bitwise-OR two pointers */
+static inline void *ptr_OR(void *a, void *b)
+{
+	return (void *)((unsigned long)a | (unsigned long)b);
+}
+
+/* Cache-inhibited register access */
+static inline u32 __qm_in(struct qm_addr *qm, void *offset)
+{
+	return in_be32(ptr_OR(qm->addr_ci, offset));
+}
+static inline void __qm_out(struct qm_addr *qm, void *offset, u32 val)
+{
+	out_be32(ptr_OR(qm->addr_ci, offset), val);
+}
+#define qm_in(reg)		__qm_in(&portal->addr, REG_##reg)
+#define qm_out(reg, val)	__qm_out(&portal->addr, REG_##reg, val)
+
+/* Convert 'n' cachelines to a pointer value for bitwise OR */
+#define qm_cl(n)		(void *)((n) << 6)
+
+/* Cache-enabled (index) register access */
+static inline void __qm_cl_touch_ro(struct qm_addr *qm, void *offset)
+{
+	dcbt_ro(ptr_OR(qm->addr_ce, offset));
+}
+static inline void __qm_cl_touch_rw(struct qm_addr *qm, void *offset)
+{
+	dcbt_rw(ptr_OR(qm->addr_ce, offset));
+}
+static inline u32 __qm_cl_in(struct qm_addr *qm, void *offset)
+{
+	return in_be32(ptr_OR(qm->addr_ce, offset));
+}
+static inline void __qm_cl_out(struct qm_addr *qm, void *offset, u32 val)
+{
+	out_be32(ptr_OR(qm->addr_ce, offset), val);
+	dcbf(ptr_OR(qm->addr_ce, offset));
+}
+static inline void __qm_cl_invalidate(struct qm_addr *qm, void *offset)
+{
+	dcbi(ptr_OR(qm->addr_ce, offset));
+}
+#define qm_cl_touch_ro(reg)	__qm_cl_touch_ro(&portal->addr, CL_##reg##_CENA)
+#define qm_cl_touch_rw(reg)	__qm_cl_touch_rw(&portal->addr, CL_##reg##_CENA)
+#define qm_cl_in(reg)		__qm_cl_in(&portal->addr, CL_##reg##_CENA)
+#define qm_cl_out(reg, val)	__qm_cl_out(&portal->addr, CL_##reg##_CENA, val)
+#define qm_cl_invalidate(reg) __qm_cl_invalidate(&portal->addr, CL_##reg##_CENA)
+
+/* Cyclic helper for rings. FIXME: once we are able to do fine-grain perf
+ * analysis, look at using the "extra" bit in the ring index registers to avoid
+ * cyclic issues. */
+static inline u8 cyc_diff(u8 ringsize, u8 first, u8 last)
+{
+	/* 'first' is included, 'last' is excluded */
+	if (first <= last)
+		return last - first;
+	return ringsize + last - first;
+}
+
+/* Portal modes.
+ *   Enum types;
+ *     pmode == production mode
+ *     cmode == consumption mode,
+ *     dmode == h/w dequeue mode.
+ *   Enum values use 3 letter codes. First letter matches the portal mode,
+ *   remaining two letters indicate;
+ *     ci == cache-inhibited portal register
+ *     ce == cache-enabled portal register
+ *     vb == in-band valid-bit (cache-enabled)
+ *     dc == DCA (Discrete Consumption Acknowledgement), DQRR-only
+ *   As for "enum qm_dqrr_dmode", it should be self-explanatory.
+ */
+enum qm_eqcr_pmode {		/* matches QCSP_CFG::EPM */
+	qm_eqcr_pci = 0,	/* PI index, cache-inhibited */
+	qm_eqcr_pce = 1,	/* PI index, cache-enabled */
+	qm_eqcr_pvb = 2		/* valid-bit */
+};
+enum qm_eqcr_cmode {		/* s/w-only */
+	qm_eqcr_cci,		/* CI index, cache-inhibited */
+	qm_eqcr_cce		/* CI index, cache-enabled */
+};
+enum qm_dqrr_dmode {		/* matches QCSP_CFG::DP */
+	qm_dqrr_dpush = 0,	/* SDQCR  + VDQCR */
+	qm_dqrr_dpull = 1	/* PDQCR */
+};
+enum qm_dqrr_pmode {		/* s/w-only */
+	qm_dqrr_pci,		/* reads DQRR_PI_CINH */
+	qm_dqrr_pce,		/* reads DQRR_PI_CENA */
+	qm_dqrr_pvb		/* reads valid-bit */
+};
+enum qm_dqrr_cmode {		/* matches QCSP_CFG::DCM */
+	qm_dqrr_cci = 0,	/* CI index, cache-inhibited */
+	qm_dqrr_cce = 1,	/* CI index, cache-enabled */
+	qm_dqrr_cdc = 2		/* Discrete Consumption Acknowledgement */
+};
+enum qm_mr_pmode {		/* s/w-only */
+	qm_mr_pci,		/* reads MR_PI_CINH */
+	qm_mr_pce,		/* reads MR_PI_CENA */
+	qm_mr_pvb		/* reads valid-bit */
+};
+enum qm_mr_cmode {		/* matches QCSP_CFG::MM */
+	qm_mr_cci = 0,		/* CI index, cache-inhibited */
+	qm_mr_cce = 1		/* CI index, cache-enabled */
+};
+
+
+/* ------------------------- */
+/* --- Portal structures --- */
+
+#define QM_EQCR_SIZE		8
+#define QM_DQRR_SIZE		16
+#define QM_MR_SIZE		8
+
+struct qm_eqcr {
+	struct qm_eqcr_entry *ring, *cursor;
+	u8 ci, available, ithresh, vbit;
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	u32 busy;
+	enum qm_eqcr_pmode pmode;
+	enum qm_eqcr_cmode cmode;
+#endif
+};
+
+struct qm_dqrr {
+	struct qm_dqrr_entry *ring, *cursor;
+	u8 pi, ci, fill, ithresh, vbit;
+#ifdef CONFIG_FSL_QMAN_CHECKING
+#define QM_DQRR_FLAG_RE 0x01 /* Stash ring entries */
+#define QM_DQRR_FLAG_SE 0x02 /* Stash data */
+	u8 flags;
+	enum qm_dqrr_dmode dmode;
+	enum qm_dqrr_pmode pmode;
+	enum qm_dqrr_cmode cmode;
+#endif
+};
+
+struct qm_mr {
+	struct qm_mr_entry *ring, *cursor;
+	u8 pi, ci, fill, ithresh, vbit;
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	enum qm_mr_pmode pmode;
+	enum qm_mr_cmode cmode;
+#endif
+};
+
+struct qm_mc {
+	struct qm_mc_command *cr;
+	struct qm_mc_result *rr;
+	u8 rridx, vbit;
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	enum {
+		/* Can be _mc_start()ed */
+		mc_idle,
+		/* Can be _mc_commit()ed or _mc_abort()ed */
+		mc_user,
+		/* Can only be _mc_retry()ed */
+		mc_hw
+	} state;
+#endif
+};
+
+#ifdef CONFIG_FSL_QMAN_BUG_AND_FEATURE_REV1
+/* For workarounds that require storage. The struct alignment is required for
+ * cases where operations on "shadow" structs need the same alignment as is
+ * present on the corresponding h/w data structs (specifically, there is a
+ * zero-bit present above the range required to address the ring, so that
+ * iteration can be achieved by incrementing a ring pointer and clearing the
+ * carry-bit). The "portal" struct needs the same alignment, as this type goes
+ * at its head. */
+#define QM_PORTAL_ALIGNMENT __attribute__((aligned(16*64)))
+struct qm_portal_bugs {
+	/* shadow MR ring, for QMAN9 workaround, 8-CL-aligned */
+	struct qm_mr_entry mr[QM_MR_SIZE];
+	/* shadow MC result, for QMAN6 and QMAN7 workarounds, CL-aligned */
+	struct qm_mc_result result;
+	/* boolean switch for QMAN7 workaround */
+	int initfq_and_sched;
+	/* histogram to track EQCR_CI updates */
+#ifdef CONFIG_FSL_QMAN_ADAPTIVE_EQCR_THROTTLE
+	u32 ci_histogram[8];
+#endif
+} QM_PORTAL_ALIGNMENT;
+#else
+#define QM_PORTAL_ALIGNMENT ____cacheline_aligned
+#endif
+
+struct qm_portal {
+#ifdef CONFIG_FSL_QMAN_BUG_AND_FEATURE_REV1
+	struct qm_portal_bugs bugs;
+#endif
+	/* In the non-CONFIG_FSL_QMAN_CHECKING case, the following stuff up to
+	 * and including 'mc' fits within a cacheline (yay!). The 'config' part
+	 * is setup-only, so isn't a cause for a concern. In other words, don't
+	 * rearrange this structure on a whim, there be dragons ... */
+	struct qm_addr addr;
+	struct qm_eqcr eqcr;
+	struct qm_dqrr dqrr;
+	struct qm_mr mr;
+	struct qm_mc mc;
+} QM_PORTAL_ALIGNMENT;
+
+
+/* ---------------- */
+/* --- EQCR API --- */
+
+/* Bit-wise logic to wrap a ring pointer by clearing the "carry bit" */
+#define EQCR_CARRYCLEAR(p) \
+	(void *)((unsigned long)(p) & (~(unsigned long)(QM_EQCR_SIZE << 6)))
+
+/* Bit-wise logic to convert a ring pointer to a ring index */
+static inline u8 EQCR_PTR2IDX(struct qm_eqcr_entry *e)
+{
+	return ((u32)e >> 6) & (QM_EQCR_SIZE - 1);
+}
+
+/* Increment the 'cursor' ring pointer, taking 'vbit' into account */
+static inline void EQCR_INC(struct qm_eqcr *eqcr)
+{
+	/* NB: this is odd-looking, but experiments show that it generates fast
+	 * code with essentially no branching overheads. We increment to the
+	 * next EQCR pointer and handle overflow and 'vbit'. */
+	struct qm_eqcr_entry *partial = eqcr->cursor + 1;
+	eqcr->cursor = EQCR_CARRYCLEAR(partial);
+	if (partial != eqcr->cursor)
+		eqcr->vbit ^= QM_EQCR_VERB_VBIT;
+}
+
+static inline int qm_eqcr_init(struct qm_portal *portal,
+				enum qm_eqcr_pmode pmode,
+				__maybe_unused enum qm_eqcr_cmode cmode)
+{
+	/* This use of 'register', as well as all other occurances, is because
+	 * it has been observed to generate much faster code with gcc than is
+	 * otherwise the case. */
+	register struct qm_eqcr *eqcr = &portal->eqcr;
+	u32 cfg;
+	u8 pi;
+
+	eqcr->ring = ptr_OR(portal->addr.addr_ce, CL_EQCR);
+	eqcr->ci = qm_in(EQCR_CI_CINH) & (QM_EQCR_SIZE - 1);
+	qm_cl_invalidate(EQCR_CI);
+	pi = qm_in(EQCR_PI_CINH) & (QM_EQCR_SIZE - 1);
+	eqcr->cursor = eqcr->ring + pi;
+	eqcr->vbit = (qm_in(EQCR_PI_CINH) & QM_EQCR_SIZE) ?
+			QM_EQCR_VERB_VBIT : 0;
+	eqcr->available = QM_EQCR_SIZE - 1 -
+			cyc_diff(QM_EQCR_SIZE, eqcr->ci, pi);
+	eqcr->ithresh = qm_in(EQCR_ITR);
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	eqcr->busy = 0;
+	eqcr->pmode = pmode;
+	eqcr->cmode = cmode;
+#endif
+	cfg = (qm_in(CFG) & 0x00ffffff) |
+		((pmode & 0x3) << 24);	/* QCSP_CFG::EPM */
+	qm_out(CFG, cfg);
+	return 0;
+}
+
+static inline void qm_eqcr_finish(struct qm_portal *portal)
+{
+	register struct qm_eqcr *eqcr = &portal->eqcr;
+	u8 pi = qm_in(EQCR_PI_CINH) & (QM_EQCR_SIZE - 1);
+	u8 ci = qm_in(EQCR_CI_CINH) & (QM_EQCR_SIZE - 1);
+
+	QM_ASSERT(!eqcr->busy);
+	if (pi != EQCR_PTR2IDX(eqcr->cursor))
+		pr_crit("losing uncommited EQCR entries\n");
+	if (ci != eqcr->ci)
+		pr_crit("missing existing EQCR completions\n");
+	if (eqcr->ci != EQCR_PTR2IDX(eqcr->cursor))
+		pr_crit("EQCR destroyed unquiesced\n");
+}
+
+static inline struct qm_eqcr_entry *qm_eqcr_start(struct qm_portal *portal)
+{
+	register struct qm_eqcr *eqcr = &portal->eqcr;
+	QM_ASSERT(!eqcr->busy);
+	if (!eqcr->available)
+		return NULL;
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	eqcr->busy = 1;
+#endif
+	dcbzl(eqcr->cursor);
+	return eqcr->cursor;
+}
+
+static inline void qm_eqcr_abort(struct qm_portal *portal)
+{
+	__maybe_unused register struct qm_eqcr *eqcr = &portal->eqcr;
+	QM_ASSERT(eqcr->busy);
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	eqcr->busy = 0;
+#endif
+}
+
+static inline struct qm_eqcr_entry *qm_eqcr_pend_and_next(
+					struct qm_portal *portal, u8 myverb)
+{
+	register struct qm_eqcr *eqcr = &portal->eqcr;
+	QM_ASSERT(eqcr->busy);
+	QM_ASSERT(eqcr->pmode != qm_eqcr_pvb);
+	if (eqcr->available == 1)
+		return NULL;
+	eqcr->cursor->__dont_write_directly__verb = myverb | eqcr->vbit;
+	dcbf(eqcr->cursor);
+	EQCR_INC(eqcr);
+	eqcr->available--;
+	dcbzl(eqcr->cursor);
+	return eqcr->cursor;
+}
+
+#define EQCR_COMMIT_CHECKS(eqcr) \
+do { \
+	QM_ASSERT(eqcr->busy); \
+	QM_ASSERT(eqcr->cursor->orp == (eqcr->cursor->orp & 0x00ffffff)); \
+	QM_ASSERT(eqcr->cursor->fqid == (eqcr->cursor->fqid & 0x00ffffff)); \
+} while (0)
+
+static inline void qm_eqcr_pci_commit(struct qm_portal *portal, u8 myverb)
+{
+	register struct qm_eqcr *eqcr = &portal->eqcr;
+	EQCR_COMMIT_CHECKS(eqcr);
+	QM_ASSERT(eqcr->pmode == qm_eqcr_pci);
+	eqcr->cursor->__dont_write_directly__verb = myverb | eqcr->vbit;
+	EQCR_INC(eqcr);
+	eqcr->available--;
+	dcbf(eqcr->cursor);
+	hwsync();
+	qm_out(EQCR_PI_CINH, EQCR_PTR2IDX(eqcr->cursor));
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	eqcr->busy = 0;
+#endif
+}
+
+static inline void qm_eqcr_pce_prefetch(struct qm_portal *portal)
+{
+	__maybe_unused register struct qm_eqcr *eqcr = &portal->eqcr;
+	QM_ASSERT(eqcr->pmode == qm_eqcr_pce);
+	qm_cl_invalidate(EQCR_PI);
+	qm_cl_touch_rw(EQCR_PI);
+}
+
+static inline void qm_eqcr_pce_commit(struct qm_portal *portal, u8 myverb)
+{
+	register struct qm_eqcr *eqcr = &portal->eqcr;
+	EQCR_COMMIT_CHECKS(eqcr);
+	QM_ASSERT(eqcr->pmode == qm_eqcr_pce);
+	eqcr->cursor->__dont_write_directly__verb = myverb | eqcr->vbit;
+	EQCR_INC(eqcr);
+	eqcr->available--;
+	dcbf(eqcr->cursor);
+	lwsync();
+	qm_cl_out(EQCR_PI, EQCR_PTR2IDX(eqcr->cursor));
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	eqcr->busy = 0;
+#endif
+}
+
+static inline void qm_eqcr_pvb_commit(struct qm_portal *portal, u8 myverb)
+{
+	register struct qm_eqcr *eqcr = &portal->eqcr;
+	struct qm_eqcr_entry *eqcursor;
+	EQCR_COMMIT_CHECKS(eqcr);
+	QM_ASSERT(eqcr->pmode == qm_eqcr_pvb);
+	lwsync();
+	eqcursor = eqcr->cursor;
+	eqcursor->__dont_write_directly__verb = myverb | eqcr->vbit;
+	dcbf(eqcursor);
+	EQCR_INC(eqcr);
+	eqcr->available--;
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	eqcr->busy = 0;
+#endif
+}
+
+static inline u8 qm_eqcr_cci_update(struct qm_portal *portal)
+{
+	register struct qm_eqcr *eqcr = &portal->eqcr;
+	u8 diff, old_ci = eqcr->ci;
+	QM_ASSERT(eqcr->cmode == qm_eqcr_cci);
+	eqcr->ci = qm_in(EQCR_CI_CINH) & (QM_EQCR_SIZE - 1);
+	diff = cyc_diff(QM_EQCR_SIZE, old_ci, eqcr->ci);
+	eqcr->available += diff;
+	return diff;
+}
+
+static inline void qm_eqcr_cce_prefetch(struct qm_portal *portal)
+{
+	__maybe_unused register struct qm_eqcr *eqcr = &portal->eqcr;
+	QM_ASSERT(eqcr->cmode == qm_eqcr_cce);
+	qm_cl_touch_ro(EQCR_CI);
+}
+
+static inline u8 qm_eqcr_cce_update(struct qm_portal *portal)
+{
+	register struct qm_eqcr *eqcr = &portal->eqcr;
+	u8 diff, old_ci = eqcr->ci;
+	QM_ASSERT(eqcr->cmode == qm_eqcr_cce);
+	eqcr->ci = qm_cl_in(EQCR_CI) & (QM_EQCR_SIZE - 1);
+	qm_cl_invalidate(EQCR_CI);
+	diff = cyc_diff(QM_EQCR_SIZE, old_ci, eqcr->ci);
+	eqcr->available += diff;
+#ifdef CONFIG_FSL_QMAN_ADAPTIVE_EQCR_THROTTLE
+	portal->bugs.ci_histogram[diff]++;
+#endif
+	return diff;
+}
+
+#ifdef CONFIG_FSL_QMAN_ADAPTIVE_EQCR_THROTTLE
+static inline u32 qm_eqcr_cce_avg_x10(struct qm_portal *portal)
+{
+	u32 total = 0, weighted = 0, *p = portal->bugs.ci_histogram;
+	int hist;
+	for (hist = 0; hist < 8; hist++, p++) {
+		total += *p;
+		weighted += (*p) * hist;
+		*p = 0;
+	}
+	if (!total)
+		return 0;
+	return (u32)((weighted * 10 + (total / 2)) / total);
+}
+#endif
+
+static inline u8 qm_eqcr_get_ithresh(struct qm_portal *portal)
+{
+	register struct qm_eqcr *eqcr = &portal->eqcr;
+	return eqcr->ithresh;
+}
+
+static inline void qm_eqcr_set_ithresh(struct qm_portal *portal, u8 ithresh)
+{
+	register struct qm_eqcr *eqcr = &portal->eqcr;
+	eqcr->ithresh = ithresh;
+	qm_out(EQCR_ITR, ithresh);
+}
+
+static inline u8 qm_eqcr_get_avail(struct qm_portal *portal)
+{
+	register struct qm_eqcr *eqcr = &portal->eqcr;
+	return eqcr->available;
+}
+
+static inline u8 qm_eqcr_get_fill(struct qm_portal *portal)
+{
+	register struct qm_eqcr *eqcr = &portal->eqcr;
+	return QM_EQCR_SIZE - 1 - eqcr->available;
+}
+
+
+/* ---------------- */
+/* --- DQRR API --- */
+
+/* FIXME: many possible improvements;
+ * - look at changing the API to use pointer rather than index parameters now
+ *   that 'cursor' is a pointer,
+ * - consider moving other parameters to pointer if it could help (ci)
+ */
+
+#define DQRR_CARRYCLEAR(p) \
+	(void *)((unsigned long)(p) & (~(unsigned long)(QM_DQRR_SIZE << 6)))
+
+static inline u8 DQRR_PTR2IDX(struct qm_dqrr_entry *e)
+{
+	return ((u32)e >> 6) & (QM_DQRR_SIZE - 1);
+}
+
+static inline struct qm_dqrr_entry *DQRR_INC(struct qm_dqrr_entry *e)
+{
+	return DQRR_CARRYCLEAR(e + 1);
+}
+
+static inline void qm_dqrr_set_maxfill(struct qm_portal *portal, u8 mf)
+{
+	qm_out(CFG, (qm_in(CFG) & 0xff0fffff) |
+		((mf & (QM_DQRR_SIZE - 1)) << 20));
+}
+
+static inline int qm_dqrr_init(struct qm_portal *portal,
+				const struct qm_portal_config *config,
+				enum qm_dqrr_dmode dmode,
+				__maybe_unused enum qm_dqrr_pmode pmode,
+				enum qm_dqrr_cmode cmode, u8 max_fill,
+				int stash_ring, int stash_data)
+{
+	register struct qm_dqrr *dqrr = &portal->dqrr;
+	u32 cfg;
+
+	if ((stash_ring || stash_data) &&
+			((config->cpu == -1) || !config->has_hv_dma))
+		return -EINVAL;
+	/* Make sure the DQRR will be idle when we enable */
+	qm_out(DQRR_SDQCR, 0);
+	qm_out(DQRR_VDQCR, 0);
+	qm_out(DQRR_PDQCR, 0);
+	dqrr->ring = ptr_OR(portal->addr.addr_ce, CL_DQRR);
+	dqrr->pi = qm_in(DQRR_PI_CINH) & (QM_DQRR_SIZE - 1);
+	dqrr->ci = qm_in(DQRR_CI_CINH) & (QM_DQRR_SIZE - 1);
+	dqrr->cursor = dqrr->ring + dqrr->ci;
+	dqrr->fill = cyc_diff(QM_DQRR_SIZE, dqrr->ci, dqrr->pi);
+	dqrr->vbit = (qm_in(DQRR_PI_CINH) & QM_DQRR_SIZE) ?
+			QM_DQRR_VERB_VBIT : 0;
+	dqrr->ithresh = qm_in(DQRR_ITR);
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	dqrr->dmode = dmode;
+	dqrr->pmode = pmode;
+	dqrr->cmode = cmode;
+	dqrr->flags = 0;
+	if (stash_ring)
+		dqrr->flags |= QM_DQRR_FLAG_RE;
+	if (stash_data)
+		dqrr->flags |= QM_DQRR_FLAG_SE;
+#endif
+	cfg = (qm_in(CFG) & 0xff000f00) |
+		((max_fill & (QM_DQRR_SIZE - 1)) << 20) | /* DQRR_MF */
+		((dmode & 1) << 18) |			/* DP */
+		((cmode & 3) << 16) |			/* DCM */
+		(stash_ring ? 0x80 : 0) |		/* RE */
+		(0 ? 0x40 : 0) |			/* Ignore RP */
+		(stash_data ? 0x20 : 0) |		/* SE */
+		(0 ? 0x10 : 0);				/* Ignore SP */
+	qm_out(CFG, cfg);
+	qm_dqrr_set_maxfill(portal, max_fill);
+	return 0;
+}
+
+static inline void qm_dqrr_finish(struct qm_portal *portal)
+{
+	register struct qm_dqrr *dqrr = &portal->dqrr;
+	if (dqrr->ci != DQRR_PTR2IDX(dqrr->cursor))
+		pr_crit("Ignoring completed DQRR entries\n");
+}
+
+static inline void qm_dqrr_current_prefetch(struct qm_portal *portal)
+{
+	register struct qm_dqrr *dqrr = &portal->dqrr;
+	/* If ring entries get stashed, don't invalidate/prefetch */
+	QM_ASSERT(!(dqrr->flags & QM_DQRR_FLAG_RE));
+	dcbt_ro(dqrr->cursor);
+}
+
+static inline struct qm_dqrr_entry *qm_dqrr_current(struct qm_portal *portal)
+{
+	register struct qm_dqrr *dqrr = &portal->dqrr;
+	if (!dqrr->fill)
+		return NULL;
+	return dqrr->cursor;
+}
+
+static inline u8 qm_dqrr_cursor(struct qm_portal *portal)
+{
+	register struct qm_dqrr *dqrr = &portal->dqrr;
+	return DQRR_PTR2IDX(dqrr->cursor);
+}
+
+static inline u8 qm_dqrr_next(struct qm_portal *portal)
+{
+	register struct qm_dqrr *dqrr = &portal->dqrr;
+	QM_ASSERT(dqrr->fill);
+	dqrr->cursor = DQRR_INC(dqrr->cursor);
+	return --dqrr->fill;
+}
+
+static inline u8 qm_dqrr_pci_update(struct qm_portal *portal)
+{
+	register struct qm_dqrr *dqrr = &portal->dqrr;
+	u8 diff, old_pi = dqrr->pi;
+	QM_ASSERT(dqrr->pmode == qm_dqrr_pci);
+	dqrr->pi = qm_in(DQRR_PI_CINH) & (QM_DQRR_SIZE - 1);
+	diff = cyc_diff(QM_DQRR_SIZE, old_pi, dqrr->pi);
+	dqrr->fill += diff;
+	return diff;
+}
+
+static inline void qm_dqrr_pce_prefetch(struct qm_portal *portal)
+{
+	__maybe_unused register struct qm_dqrr *dqrr = &portal->dqrr;
+	QM_ASSERT(dqrr->pmode == qm_dqrr_pce);
+	qm_cl_invalidate(DQRR_PI);
+	qm_cl_touch_ro(DQRR_PI);
+}
+
+static inline u8 qm_dqrr_pce_update(struct qm_portal *portal)
+{
+	register struct qm_dqrr *dqrr = &portal->dqrr;
+	u8 diff, old_pi = dqrr->pi;
+	QM_ASSERT(dqrr->pmode == qm_dqrr_pce);
+	dqrr->pi = qm_cl_in(DQRR_PI) & (QM_DQRR_SIZE - 1);
+	diff = cyc_diff(QM_DQRR_SIZE, old_pi, dqrr->pi);
+	dqrr->fill += diff;
+	return diff;
+}
+
+static inline void qm_dqrr_pvb_prefetch(struct qm_portal *portal)
+{
+	register struct qm_dqrr *dqrr = &portal->dqrr;
+	QM_ASSERT(dqrr->pmode == qm_dqrr_pvb);
+	QM_ASSERT(!(dqrr->flags & QM_DQRR_FLAG_RE));
+	dcbi(ptr_OR(dqrr->ring, qm_cl(dqrr->pi)));
+	dcbt_ro(ptr_OR(dqrr->ring, qm_cl(dqrr->pi)));
+}
+
+static inline u8 qm_dqrr_pvb_update(struct qm_portal *portal)
+{
+	register struct qm_dqrr *dqrr = &portal->dqrr;
+	struct qm_dqrr_entry *res = ptr_OR(dqrr->ring, qm_cl(dqrr->pi));
+	QM_ASSERT(dqrr->pmode == qm_dqrr_pvb);
+	if ((readb(&res->verb) & QM_DQRR_VERB_VBIT) == dqrr->vbit) {
+		dqrr->pi = (dqrr->pi + 1) & (QM_DQRR_SIZE - 1);
+		if (!dqrr->pi)
+			dqrr->vbit ^= QM_DQRR_VERB_VBIT;
+		dqrr->fill++;
+		return 1;
+	}
+	return 0;
+}
+
+static inline void qm_dqrr_cci_consume(struct qm_portal *portal, u8 num)
+{
+	register struct qm_dqrr *dqrr = &portal->dqrr;
+	QM_ASSERT(dqrr->cmode == qm_dqrr_cci);
+	dqrr->ci = (dqrr->ci + num) & (QM_DQRR_SIZE - 1);
+	qm_out(DQRR_CI_CINH, dqrr->ci);
+}
+
+static inline void qm_dqrr_cci_consume_to_current(struct qm_portal *portal)
+{
+	register struct qm_dqrr *dqrr = &portal->dqrr;
+	QM_ASSERT(dqrr->cmode == qm_dqrr_cci);
+	dqrr->ci = DQRR_PTR2IDX(dqrr->cursor);
+	qm_out(DQRR_CI_CINH, dqrr->ci);
+}
+
+static inline void qm_dqrr_cce_prefetch(struct qm_portal *portal)
+{
+	__maybe_unused register struct qm_dqrr *dqrr = &portal->dqrr;
+	QM_ASSERT(dqrr->cmode == qm_dqrr_cce);
+	qm_cl_invalidate(DQRR_CI);
+	qm_cl_touch_rw(DQRR_CI);
+}
+
+static inline void qm_dqrr_cce_consume(struct qm_portal *portal, u8 num)
+{
+	register struct qm_dqrr *dqrr = &portal->dqrr;
+	QM_ASSERT(dqrr->cmode == qm_dqrr_cce);
+	dqrr->ci = (dqrr->ci + num) & (QM_DQRR_SIZE - 1);
+	qm_cl_out(DQRR_CI, dqrr->ci);
+}
+
+static inline void qm_dqrr_cce_consume_to_current(struct qm_portal *portal)
+{
+	register struct qm_dqrr *dqrr = &portal->dqrr;
+	QM_ASSERT(dqrr->cmode == qm_dqrr_cce);
+	dqrr->ci = DQRR_PTR2IDX(dqrr->cursor);
+	qm_cl_out(DQRR_CI, dqrr->ci);
+}
+
+static inline void qm_dqrr_cdc_consume_1(struct qm_portal *portal, u8 idx,
+					int park)
+{
+	__maybe_unused register struct qm_dqrr *dqrr = &portal->dqrr;
+	QM_ASSERT(dqrr->cmode == qm_dqrr_cdc);
+	QM_ASSERT(idx < QM_DQRR_SIZE);
+	qm_out(DQRR_DCAP, (0 << 8) |	/* S */
+		((park ? 1 : 0) << 6) |	/* PK */
+		idx);			/* DCAP_CI */
+}
+
+static inline void qm_dqrr_cdc_consume_1ptr(struct qm_portal *portal,
+					struct qm_dqrr_entry *dq,
+				int park)
+{
+	__maybe_unused register struct qm_dqrr *dqrr = &portal->dqrr;
+	u8 idx = DQRR_PTR2IDX(dq);
+	QM_ASSERT(dqrr->cmode == qm_dqrr_cdc);
+	QM_ASSERT((dqrr->ring + idx) == dq);
+	QM_ASSERT(idx < QM_DQRR_SIZE);
+	qm_out(DQRR_DCAP, (0 << 8) |		/* DQRR_DCAP::S */
+		((park ? 1 : 0) << 6) |		/* DQRR_DCAP::PK */
+		idx);				/* DQRR_DCAP::DCAP_CI */
+}
+
+static inline void qm_dqrr_cdc_consume_n(struct qm_portal *portal, u16 bitmask)
+{
+	__maybe_unused register struct qm_dqrr *dqrr = &portal->dqrr;
+	QM_ASSERT(dqrr->cmode == qm_dqrr_cdc);
+	qm_out(DQRR_DCAP, (1 << 8) |		/* DQRR_DCAP::S */
+		((u32)bitmask << 16));		/* DQRR_DCAP::DCAP_CI */
+}
+
+static inline u8 qm_dqrr_cdc_cci(struct qm_portal *portal)
+{
+	__maybe_unused register struct qm_dqrr *dqrr = &portal->dqrr;
+	QM_ASSERT(dqrr->cmode == qm_dqrr_cdc);
+	return qm_in(DQRR_CI_CINH) & (QM_DQRR_SIZE - 1);
+}
+
+static inline void qm_dqrr_cdc_cce_prefetch(struct qm_portal *portal)
+{
+	__maybe_unused register struct qm_dqrr *dqrr = &portal->dqrr;
+	QM_ASSERT(dqrr->cmode == qm_dqrr_cdc);
+	qm_cl_invalidate(DQRR_CI);
+	qm_cl_touch_ro(DQRR_CI);
+}
+
+static inline u8 qm_dqrr_cdc_cce(struct qm_portal *portal)
+{
+	__maybe_unused register struct qm_dqrr *dqrr = &portal->dqrr;
+	QM_ASSERT(dqrr->cmode == qm_dqrr_cdc);
+	return qm_cl_in(DQRR_CI) & (QM_DQRR_SIZE - 1);
+}
+
+static inline u8 qm_dqrr_get_ci(struct qm_portal *portal)
+{
+	register struct qm_dqrr *dqrr = &portal->dqrr;
+	QM_ASSERT(dqrr->cmode != qm_dqrr_cdc);
+	return dqrr->ci;
+}
+
+static inline void qm_dqrr_park(struct qm_portal *portal, u8 idx)
+{
+	__maybe_unused register struct qm_dqrr *dqrr = &portal->dqrr;
+	QM_ASSERT(dqrr->cmode != qm_dqrr_cdc);
+	qm_out(DQRR_DCAP, (0 << 8) |		/* S */
+		(1 << 6) |			/* PK */
+		(idx & (QM_DQRR_SIZE - 1)));	/* DCAP_CI */
+}
+
+static inline void qm_dqrr_park_ci(struct qm_portal *portal)
+{
+	register struct qm_dqrr *dqrr = &portal->dqrr;
+	QM_ASSERT(dqrr->cmode != qm_dqrr_cdc);
+	qm_out(DQRR_DCAP, (0 << 8) |		/* S */
+		(1 << 6) |			/* PK */
+		(dqrr->ci & (QM_DQRR_SIZE - 1)));/* DCAP_CI */
+}
+
+static inline void qm_dqrr_sdqcr_set(struct qm_portal *portal, u32 sdqcr)
+{
+	qm_out(DQRR_SDQCR, sdqcr);
+}
+
+static inline u32 qm_dqrr_sdqcr_get(struct qm_portal *portal)
+{
+	return qm_in(DQRR_SDQCR);
+}
+
+static inline void qm_dqrr_vdqcr_set(struct qm_portal *portal, u32 vdqcr)
+{
+	qm_out(DQRR_VDQCR, vdqcr);
+}
+
+static inline u32 qm_dqrr_vdqcr_get(struct qm_portal *portal)
+{
+	return qm_in(DQRR_VDQCR);
+}
+
+static inline void qm_dqrr_pdqcr_set(struct qm_portal *portal, u32 pdqcr)
+{
+	qm_out(DQRR_PDQCR, pdqcr);
+}
+
+static inline u32 qm_dqrr_pdqcr_get(struct qm_portal *portal)
+{
+	return qm_in(DQRR_PDQCR);
+}
+
+static inline u8 qm_dqrr_get_ithresh(struct qm_portal *portal)
+{
+	register struct qm_dqrr *dqrr = &portal->dqrr;
+	return dqrr->ithresh;
+}
+
+static inline void qm_dqrr_set_ithresh(struct qm_portal *portal, u8 ithresh)
+{
+	qm_out(DQRR_ITR, ithresh);
+}
+
+static inline u8 qm_dqrr_get_maxfill(struct qm_portal *portal)
+{
+	return (qm_in(CFG) & 0x00f00000) >> 20;
+}
+
+
+/* -------------- */
+/* --- MR API --- */
+
+#define MR_CARRYCLEAR(p) \
+	(void *)((unsigned long)(p) & (~(unsigned long)(QM_MR_SIZE << 6)))
+
+static inline u8 MR_PTR2IDX(struct qm_mr_entry *e)
+{
+	return ((u32)e >> 6) & (QM_MR_SIZE - 1);
+}
+
+static inline struct qm_mr_entry *MR_INC(struct qm_mr_entry *e)
+{
+	return MR_CARRYCLEAR(e + 1);
+}
+
+#ifdef CONFIG_FSL_QMAN_BUG_AND_FEATURE_REV1
+static inline void __mr_copy_and_fixup(struct qm_portal *p, u8 idx)
+{
+	if (qman_ip_rev == QMAN_REV1) {
+		struct qm_mr_entry *shadow = ptr_OR(p->bugs.mr, qm_cl(idx));
+		struct qm_mr_entry *res = ptr_OR(p->mr.ring, qm_cl(idx));
+		copy_words(shadow, res, sizeof(*res));
+		/* Bypass the QM_MR_RC_*** definitions, and check the byte value
+		 * directly to handle the erratum. */
+		if (shadow->ern.rc == 0x06)
+			shadow->ern.rc = 0x60;
+	}
+}
+#else
+#define __mr_copy_and_fixup(p, idx) do { ; } while (0)
+#endif
+
+static inline int qm_mr_init(struct qm_portal *portal, enum qm_mr_pmode pmode,
+		enum qm_mr_cmode cmode)
+{
+	register struct qm_mr *mr = &portal->mr;
+	u32 cfg;
+	int loop;
+
+#ifdef CONFIG_FSL_QMAN_BUG_AND_FEATURE_REV1
+	if ((qman_ip_rev == QMAN_REV1) && (pmode != qm_mr_pvb)) {
+		pr_err("Qman is rev1, so QMAN9 workaround requires 'pvb'\n");
+		return -EINVAL;
+	}
+#endif
+	mr->ring = ptr_OR(portal->addr.addr_ce, CL_MR);
+	mr->pi = qm_in(MR_PI_CINH) & (QM_MR_SIZE - 1);
+	mr->ci = qm_in(MR_CI_CINH) & (QM_MR_SIZE - 1);
+#ifdef CONFIG_FSL_QMAN_BUG_AND_FEATURE_REV1
+	if (qman_ip_rev == QMAN_REV1)
+		/* Situate the cursor in the shadow ring */
+		mr->cursor = portal->bugs.mr + mr->ci;
+	else
+#endif
+	mr->cursor = mr->ring + mr->ci;
+	mr->fill = cyc_diff(QM_MR_SIZE, mr->ci, mr->pi);
+	mr->vbit = (qm_in(MR_PI_CINH) & QM_MR_SIZE) ? QM_MR_VERB_VBIT : 0;
+	mr->ithresh = qm_in(MR_ITR);
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	mr->pmode = pmode;
+	mr->cmode = cmode;
+#endif
+	/* Only new entries get the copy-and-fixup treatment from
+	 * qm_mr_pvb_update(), so perform it here for any stale entries. */
+	for (loop = 0; loop < mr->fill; loop++)
+		__mr_copy_and_fixup(portal, (mr->ci + loop) & (QM_MR_SIZE - 1));
+	cfg = (qm_in(CFG) & 0xfffff0ff) |
+		((cmode & 1) << 8);		/* QCSP_CFG:MM */
+	qm_out(CFG, cfg);
+	return 0;
+}
+
+static inline void qm_mr_finish(struct qm_portal *portal)
+{
+	register struct qm_mr *mr = &portal->mr;
+	if (mr->ci != MR_PTR2IDX(mr->cursor))
+		pr_crit("Ignoring completed MR entries\n");
+}
+
+static inline void qm_mr_current_prefetch(struct qm_portal *portal)
+{
+	register struct qm_mr *mr = &portal->mr;
+	dcbt_ro(mr->cursor);
+}
+
+static inline struct qm_mr_entry *qm_mr_current(struct qm_portal *portal)
+{
+	register struct qm_mr *mr = &portal->mr;
+	if (!mr->fill)
+		return NULL;
+	return mr->cursor;
+}
+
+static inline u8 qm_mr_cursor(struct qm_portal *portal)
+{
+	register struct qm_mr *mr = &portal->mr;
+	return MR_PTR2IDX(mr->cursor);
+}
+
+static inline u8 qm_mr_next(struct qm_portal *portal)
+{
+	register struct qm_mr *mr = &portal->mr;
+	QM_ASSERT(mr->fill);
+	mr->cursor = MR_INC(mr->cursor);
+	return --mr->fill;
+}
+
+static inline u8 qm_mr_pci_update(struct qm_portal *portal)
+{
+	register struct qm_mr *mr = &portal->mr;
+	u8 diff, old_pi = mr->pi;
+	QM_ASSERT(mr->pmode == qm_mr_pci);
+	mr->pi = qm_in(MR_PI_CINH);
+	diff = cyc_diff(QM_MR_SIZE, old_pi, mr->pi);
+	mr->fill += diff;
+	return diff;
+}
+
+static inline void qm_mr_pce_prefetch(struct qm_portal *portal)
+{
+	__maybe_unused register struct qm_mr *mr = &portal->mr;
+	QM_ASSERT(mr->pmode == qm_mr_pce);
+	qm_cl_invalidate(MR_PI);
+	qm_cl_touch_ro(MR_PI);
+}
+
+static inline u8 qm_mr_pce_update(struct qm_portal *portal)
+{
+	register struct qm_mr *mr = &portal->mr;
+	u8 diff, old_pi = mr->pi;
+	QM_ASSERT(mr->pmode == qm_mr_pce);
+	mr->pi = qm_cl_in(MR_PI) & (QM_MR_SIZE - 1);
+	diff = cyc_diff(QM_MR_SIZE, old_pi, mr->pi);
+	mr->fill += diff;
+	return diff;
+}
+
+static inline void qm_mr_pvb_prefetch(struct qm_portal *portal)
+{
+	register struct qm_mr *mr = &portal->mr;
+	QM_ASSERT(mr->pmode == qm_mr_pvb);
+	dcbi(ptr_OR(mr->ring, qm_cl(mr->pi)));
+	dcbt_ro(ptr_OR(mr->ring, qm_cl(mr->pi)));
+}
+
+static inline u8 qm_mr_pvb_update(struct qm_portal *portal)
+{
+	register struct qm_mr *mr = &portal->mr;
+	struct qm_mr_entry *res = ptr_OR(mr->ring, qm_cl(mr->pi));
+	QM_ASSERT(mr->pmode == qm_mr_pvb);
+	if ((readb(&res->verb) & QM_MR_VERB_VBIT) == mr->vbit) {
+		__mr_copy_and_fixup(portal, mr->pi);
+		mr->pi = (mr->pi + 1) & (QM_MR_SIZE - 1);
+		if (!mr->pi)
+			mr->vbit ^= QM_MR_VERB_VBIT;
+		mr->fill++;
+		return 1;
+	}
+	return 0;
+}
+
+static inline void qm_mr_cci_consume(struct qm_portal *portal, u8 num)
+{
+	register struct qm_mr *mr = &portal->mr;
+	QM_ASSERT(mr->cmode == qm_mr_cci);
+	mr->ci = (mr->ci + num) & (QM_MR_SIZE - 1);
+	qm_out(MR_CI_CINH, mr->ci);
+}
+
+static inline void qm_mr_cci_consume_to_current(struct qm_portal *portal)
+{
+	register struct qm_mr *mr = &portal->mr;
+	QM_ASSERT(mr->cmode == qm_mr_cci);
+	mr->ci = MR_PTR2IDX(mr->cursor);
+	qm_out(MR_CI_CINH, mr->ci);
+}
+
+static inline void qm_mr_cce_prefetch(struct qm_portal *portal)
+{
+	__maybe_unused register struct qm_mr *mr = &portal->mr;
+	QM_ASSERT(mr->cmode == qm_mr_cce);
+	qm_cl_invalidate(MR_CI);
+	qm_cl_touch_rw(MR_CI);
+}
+
+static inline void qm_mr_cce_consume(struct qm_portal *portal, u8 num)
+{
+	register struct qm_mr *mr = &portal->mr;
+	QM_ASSERT(mr->cmode == qm_mr_cce);
+	mr->ci = (mr->ci + num) & (QM_MR_SIZE - 1);
+	qm_cl_out(MR_CI, mr->ci);
+}
+
+static inline void qm_mr_cce_consume_to_current(struct qm_portal *portal)
+{
+	register struct qm_mr *mr = &portal->mr;
+	QM_ASSERT(mr->cmode == qm_mr_cce);
+	mr->ci = MR_PTR2IDX(mr->cursor);
+	qm_cl_out(MR_CI, mr->ci);
+}
+
+static inline u8 qm_mr_get_ci(struct qm_portal *portal)
+{
+	register struct qm_mr *mr = &portal->mr;
+	return mr->ci;
+}
+
+static inline u8 qm_mr_get_ithresh(struct qm_portal *portal)
+{
+	register struct qm_mr *mr = &portal->mr;
+	return mr->ithresh;
+}
+
+static inline void qm_mr_set_ithresh(struct qm_portal *portal, u8 ithresh)
+{
+	qm_out(MR_ITR, ithresh);
+}
+
+
+/* ------------------------------ */
+/* --- Management command API --- */
+
+static inline int qm_mc_init(struct qm_portal *portal)
+{
+	register struct qm_mc *mc = &portal->mc;
+	mc->cr = ptr_OR(portal->addr.addr_ce, CL_CR);
+	mc->rr = ptr_OR(portal->addr.addr_ce, CL_RR0);
+	mc->rridx = (readb(&mc->cr->__dont_write_directly__verb) &
+			QM_MCC_VERB_VBIT) ?  0 : 1;
+	mc->vbit = mc->rridx ? QM_MCC_VERB_VBIT : 0;
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	mc->state = mc_idle;
+#endif
+	return 0;
+}
+
+static inline void qm_mc_finish(struct qm_portal *portal)
+{
+	__maybe_unused register struct qm_mc *mc = &portal->mc;
+	QM_ASSERT(mc->state == mc_idle);
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	if (mc->state != mc_idle)
+		pr_crit("Losing incomplete MC command\n");
+#endif
+}
+
+static inline struct qm_mc_command *qm_mc_start(struct qm_portal *portal)
+{
+	register struct qm_mc *mc = &portal->mc;
+	QM_ASSERT(mc->state == mc_idle);
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	mc->state = mc_user;
+#endif
+	dcbzl(mc->cr);
+	return mc->cr;
+}
+
+static inline void qm_mc_abort(struct qm_portal *portal)
+{
+	__maybe_unused register struct qm_mc *mc = &portal->mc;
+	QM_ASSERT(mc->state == mc_user);
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	mc->state = mc_idle;
+#endif
+}
+
+static inline void qm_mc_commit(struct qm_portal *portal, u8 myverb)
+{
+	register struct qm_mc *mc = &portal->mc;
+	struct qm_mc_result *rr = mc->rr + mc->rridx;
+	QM_ASSERT(mc->state == mc_user);
+	dcbi(rr);
+	lwsync();
+#ifdef CONFIG_FSL_QMAN_BUG_AND_FEATURE_REV1
+	if ((qman_ip_rev == QMAN_REV1) && ((myverb & QM_MCC_VERB_MASK) ==
+					QM_MCC_VERB_INITFQ_SCHED)) {
+		u32 fqid = mc->cr->initfq.fqid;
+		/* Do two commands to avoid the hw bug. Note, we poll locally
+		 * rather than using qm_mc_result() because from a QMAN_CHECKING
+		 * perspective, we don't want to appear to have "finished" until
+		 * both commands are done. */
+		mc->cr->__dont_write_directly__verb = mc->vbit |
+					QM_MCC_VERB_INITFQ_PARKED;
+		dcbf(mc->cr);
+		portal->bugs.initfq_and_sched = 1;
+		do {
+			dcbi(rr);
+			dcbt_ro(rr);
+			barrier();
+		} while (!readb(&rr->verb));
+#ifdef CONFIG_FSL_QMAN_CHECKING
+		mc->state = mc_idle;
+#endif
+		if (rr->result != QM_MCR_RESULT_OK) {
+#ifdef CONFIG_FSL_QMAN_CHECKING
+			mc->state = mc_hw;
+#endif
+			return;
+		}
+		mc->rridx ^= 1;
+		mc->vbit ^= QM_MCC_VERB_VBIT;
+		rr = mc->rr + mc->rridx;
+		dcbzl(mc->cr);
+		mc->cr->alterfq.fqid = fqid;
+		dcbi(rr);
+		lwsync();
+		myverb = QM_MCC_VERB_ALTER_SCHED;
+	} else
+		portal->bugs.initfq_and_sched = 0;
+#endif
+	mc->cr->__dont_write_directly__verb = myverb | mc->vbit;
+	dcbf(mc->cr);
+	dcbt_ro(rr);
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	mc->state = mc_hw;
+#endif
+}
+
+static inline struct qm_mc_result *qm_mc_result(struct qm_portal *portal)
+{
+	register struct qm_mc *mc = &portal->mc;
+	struct qm_mc_result *rr = mc->rr + mc->rridx;
+	QM_ASSERT(mc->state == mc_hw);
+	/* The inactive response register's verb byte always returns zero until
+	 * its command is submitted and completed. This includes the valid-bit,
+	 * in case you were wondering... */
+	if (!readb(&rr->verb)) {
+		dcbi(rr);
+		dcbt_ro(rr);
+		return NULL;
+	}
+#ifdef CONFIG_FSL_QMAN_BUG_AND_FEATURE_REV1
+	if (qman_ip_rev == QMAN_REV1) {
+		if ((readb(&rr->verb) & QM_MCR_VERB_MASK) ==
+						QM_MCR_VERB_QUERYFQ) {
+			void *misplaced = (void *)rr + 50;
+			copy_words(&portal->bugs.result, rr, sizeof(*rr));
+			rr = &portal->bugs.result;
+			copy_shorts(&rr->queryfq.fqd.td, misplaced,
+				sizeof(rr->queryfq.fqd.td));
+		} else if (portal->bugs.initfq_and_sched) {
+			/* We split the user-requested command, make the final
+			 * result match the requested type. */
+			copy_words(&portal->bugs.result, rr, sizeof(*rr));
+			rr = &portal->bugs.result;
+			rr->verb = (rr->verb & QM_MCR_VERB_RRID) |
+					QM_MCR_VERB_INITFQ_SCHED;
+		}
+	}
+#endif
+	mc->rridx ^= 1;
+	mc->vbit ^= QM_MCC_VERB_VBIT;
+#ifdef CONFIG_FSL_QMAN_CHECKING
+	mc->state = mc_idle;
+#endif
+	return rr;
+}
+
+
+/* ------------------------------------- */
+/* --- Portal interrupt register API --- */
+
+static inline int qm_isr_init(__always_unused struct qm_portal *portal)
+{
+	return 0;
+}
+
+static inline void qm_isr_finish(__always_unused struct qm_portal *portal)
+{
+}
+
+static inline void qm_isr_set_iperiod(struct qm_portal *portal, u16 iperiod)
+{
+	qm_out(ITPR, iperiod);
+}
+
+static inline u32 __qm_isr_read(struct qm_portal *portal, enum qm_isr_reg n)
+{
+	return __qm_in(&portal->addr, REG_ISR + (n << 2));
+}
+
+static inline void __qm_isr_write(struct qm_portal *portal, enum qm_isr_reg n,
+					u32 val)
+{
+	__qm_out(&portal->addr, REG_ISR + (n << 2), val);
+}
+
diff --git a/drivers/hwqueue/qman_private.h b/drivers/hwqueue/qman_private.h
index 176e65a..d85e62b 100644
--- a/drivers/hwqueue/qman_private.h
+++ b/drivers/hwqueue/qman_private.h
@@ -33,102 +33,47 @@
 #include "qman_sys.h"
 #include <linux/fsl_qman.h>
 
+#if defined(CONFIG_FSL_QMAN_ADAPTIVE_EQCR_THROTTLE) && \
+	!defined(CONFIG_FSL_QMAN_BUG_AND_FEATURE_REV1)
+#error "_QMAN_ADAPTIVE_EQCR_THROTTLE requires _QMAN_BUG_AND_FEATURE_REV1"
+#endif
+
 struct qm_addr {
 	void __iomem *addr_ce;	/* cache-enabled */
 	void __iomem *addr_ci;	/* cache-inhibited */
 };
 
-/* EQCR state */
-struct qm_eqcr {
-	struct qm_eqcr_entry *ring, *cursor;
-	u8 ci, available, ithresh, vbit;
-#ifdef CONFIG_FSL_QMAN_CHECKING
-	u32 busy;
-	enum qm_eqcr_pmode pmode;
-	enum qm_eqcr_cmode cmode;
-#endif
-};
-
-/* DQRR state */
-struct qm_dqrr {
-	struct qm_dqrr_entry *ring, *cursor;
-	u8 pi, ci, fill, ithresh, vbit;
-#ifdef CONFIG_FSL_QMAN_CHECKING
-	u8 flags;
-	enum qm_dqrr_dmode dmode;
-	enum qm_dqrr_pmode pmode;
-	enum qm_dqrr_cmode cmode;
-#endif
-};
-#define QM_DQRR_FLAG_RE 0x01 /* Stash ring entries */
-#define QM_DQRR_FLAG_SE 0x02 /* Stash data */
-
-/* MR state */
-struct qm_mr {
-	struct qm_mr_entry *ring, *cursor;
-	u8 pi, ci, fill, ithresh, vbit;
-#ifdef CONFIG_FSL_QMAN_CHECKING
-	enum qm_mr_pmode pmode;
-	enum qm_mr_cmode cmode;
-#endif
+/* used by CCSR and portal interrupt code */
+enum qm_isr_reg {
+	qm_isr_status = 0,
+	qm_isr_enable = 1,
+	qm_isr_disable = 2,
+	qm_isr_inhibit = 3
 };
 
-/* MC state */
-struct qm_mc {
-	struct qm_mc_command *cr;
-	struct qm_mc_result *rr;
-	u8 rridx, vbit;
-#ifdef CONFIG_FSL_QMAN_CHECKING
-	enum {
-		/* Can be _mc_start()ed */
-		mc_idle,
-		/* Can be _mc_commit()ed or _mc_abort()ed */
-		mc_user,
-		/* Can only be _mc_retry()ed */
-		mc_hw
-	} state;
-#endif
-};
-
-/********************/
-/* Portal structure */
-/********************/
-
-#ifdef CONFIG_FSL_QMAN_BUG_AND_FEATURE_REV1
-/* For workarounds that require storage, this struct is overlayed on a
- * get_zeroed_page(), guaranteeing alignment and such. */
-struct qm_portal_bugs {
-	/* shadow MR ring, for QMAN9 workaround, 8-CL aligned */
-	struct qm_mr_entry mr[QM_MR_SIZE];
-	/* shadow MC result, for QMAN6 and QMAN7 workarounds, CL aligned */
-	struct qm_mc_result result;
-	/* boolean switch for QMAN7 workaround */
-	int initfq_and_sched;
-};
-#endif
-
-struct qm_portal {
-	/* In the non-CONFIG_FSL_QMAN_CHECKING case, everything up to and
-	 * including 'mc' fits in a cacheline (yay!). The 'config' part is
-	 * setup-only, so isn't a cause for a concern. In other words, don't
-	 * rearrange this structure on a whim, there be dragons ... */
+struct qm_portal_config {
+	struct qm_portal *portal;
 	struct qm_addr addr;
-	struct qm_eqcr eqcr;
-	struct qm_dqrr dqrr;
-	struct qm_mr mr;
-	struct qm_mc mc;
-	struct qm_portal_config config;
+	/* If the caller enables DQRR stashing (and thus wishes to operate the
+	 * portal from only one cpu), this is the logical CPU that the portal
+	 * will stash to. Whether stashing is enabled or not, this setting is
+	 * also used for any "core-affine" portals, ie. default portals
+	 * associated to the corresponding cpu. -1 implies that there is no core
+	 * affinity configured. */
+	int cpu;
+	/* portal interrupt line */
+	int irq;
+	/* The portal's dedicated channel id, use this value for initialising
+	 * frame queues to target this portal when scheduled. */
+	enum qm_channel channel;
+	/* A mask of which pool channels this portal has dequeue access to
+	 * (using QM_SDQCR_CHANNELS_POOL(n) for the bitmask) */
+	u32 pools;
+	/* does this portal have PAMU assistance from hypervisor? */
+	int has_hv_dma;
 	/* Logical index (not cell-index) */
 	int index;
-#ifdef CONFIG_FSL_QMAN_BUG_AND_FEATURE_REV1
-	struct qm_portal_bugs *bugs;
-#endif
-} ____cacheline_aligned;
-
-/* EQCR/DQRR/[...] code uses this as a locked mechanism to bind/unbind to
- * qm_portal::bound. */
-int __qm_portal_bind(struct qm_portal *portal, u8 iface);
-void __qm_portal_unbind(struct qm_portal *portal, u8 iface);
+};
 
 /* Hooks for driver initialisation */
 #ifdef CONFIG_FSL_QMAN_FQALLOCATOR
@@ -140,29 +85,186 @@ __init int __fqalloc_init(void);
 #define QMAN_REV2 0x0101
 extern u16 qman_ip_rev; /* 0 if uninitialised, otherwise QMAN_REVx */
 
-/* Hooks from qman_high.c in to qman_driver.c */
-extern DEFINE_PER_CPU(struct qman_portal *, qman_affine_portal);
-static inline struct qman_portal *get_affine_portal(void)
-{
-	return get_cpu_var(qman_affine_portal);
-}
-static inline void put_affine_portal(void)
-{
-	put_cpu_var(qman_affine_portal);
-}
+#ifdef CONFIG_FSL_QMAN_CONFIG
+/* Hooks from qman_driver.c to qman_config.c */
+int qman_init_error_int(struct device_node *node);
+#endif
+
+/* This struct represents a pool channel */
+struct qm_pool_channel {
+	/* The QM_SDQCR_CHANNELS_POOL(n) bit that corresponds to this channel */
+	u32 pool;
+	/* The channel id, used for initialising frame queues to target this
+	 * channel. */
+	enum qm_channel channel;
+	/* Bitmask of portal (logical-, not cell-)indices that have dequeue
+	 * access to this channel;
+	 * 0x001 -> qm_portal_get(0)
+	 * 0x002 -> qm_portal_get(1)
+	 * 0x004 -> qm_portal_get(2)
+	 * ...
+	 * 0x200 -> qm_portal_get(9)
+	 */
+	u32 portals;
+};
 
 /* Hooks from qman_driver.c in to qman_high.c */
 #define QMAN_PORTAL_FLAG_RSTASH      0x00000001 /* enable DQRR entry stashing */
 #define QMAN_PORTAL_FLAG_DSTASH      0x00000002 /* enable data stashing */
-struct qman_portal *qman_create_portal(struct qm_portal *portal, u32 flags,
+int qman_have_affine_portal(void);
+int qman_create_affine_portal(const struct qm_portal_config *config, u32 flags,
 			const struct qman_cgrs *cgrs,
-			const struct qman_fq_cb *null_cb);
-void qman_destroy_portal(struct qman_portal *p);
-void qman_static_dequeue_add_ex(struct qman_portal *p, u32 pools);
-
-/* There are no CGR-related APIs exported so far, but due to the
- * uninitialised-data ECC issue in rev1.0 Qman, the driver needs to issue "Init
- * CGR" commands on boot-up. So we're declaring some internal-only APIs to
- * facilitate this for now. */
-int qman_init_cgr(u32 cgid);
+			const struct qman_fq_cb *null_cb,
+			u32 irq_sources);
+void qman_destroy_affine_portal(void);
+
+/* This CGR feature is supported by h/w and required by unit-tests and the
+ * debugfs hooks, so is implemented in the driver. However it allows an explicit
+ * corruption of h/w fields by s/w that are usually incorruptible (because the
+ * counters are usually maintained entirely within h/w). As such, we declare
+ * this API internally. */
+int qman_testwrite_cgr(struct qman_cgr *cgr, u64 i_bcnt,
+	struct qm_mcr_cgrtestwrite *result);
+
+/*************************************************/
+/*   QMan s/w corenet portal, low-level i/face   */
+/*************************************************/
+
+/* Note: most functions are only used by the high-level interface, so are
+ * inlined from qman_low.h. The stuff below is for use by other parts of the
+ * driver. */
+
+/* Obtain the number of portals available */
+u8 qm_portal_num(void);
+
+/* Obtain a portal handle and configuration information about it */
+const struct qm_portal_config *qm_portal_config(u8 idx);
+
+/* Obtain a mask of the available pool channels, expressed using
+ * QM_SDQCR_CHANNELS_POOL(n). */
+u32 qm_pools(void);
+
+/* Retrieve a pool channel configuration, given a QM_SDQCR_CHANNEL_POOL(n)
+ * bit-mask (the least significant bit of 'mask' is used if more than one bit is
+ * set). */
+const struct qm_pool_channel *qm_pool_channel(u32 mask);
+
+/* For qm_dqrr_sdqcr_set(); Choose one SOURCE. Choose one COUNT. Choose one
+ * dequeue TYPE. Choose TOKEN (8-bit).
+ * If SOURCE == CHANNELS,
+ *   Choose CHANNELS_DEDICATED and/or CHANNELS_POOL(n).
+ *   You can choose DEDICATED_PRECEDENCE if the portal channel should have
+ *   priority.
+ * If SOURCE == SPECIFICWQ,
+ *     Either select the work-queue ID with SPECIFICWQ_WQ(), or select the
+ *     channel (SPECIFICWQ_DEDICATED or SPECIFICWQ_POOL()) and specify the
+ *     work-queue priority (0-7) with SPECIFICWQ_WQ() - either way, you get the
+ *     same value.
+ */
+#define QM_SDQCR_SOURCE_CHANNELS	0x0
+#define QM_SDQCR_SOURCE_SPECIFICWQ	0x40000000
+#define QM_SDQCR_COUNT_EXACT1		0x0
+#define QM_SDQCR_COUNT_UPTO3		0x20000000
+#define QM_SDQCR_DEDICATED_PRECEDENCE	0x10000000
+#define QM_SDQCR_TYPE_MASK		0x03000000
+#define QM_SDQCR_TYPE_NULL		0x0
+#define QM_SDQCR_TYPE_PRIO_QOS		0x01000000
+#define QM_SDQCR_TYPE_ACTIVE_QOS	0x02000000
+#define QM_SDQCR_TYPE_ACTIVE		0x03000000
+#define QM_SDQCR_TOKEN_MASK		0x00ff0000
+#define QM_SDQCR_TOKEN_SET(v)		(((v) & 0xff) << 16)
+#define QM_SDQCR_TOKEN_GET(v)		(((v) >> 16) & 0xff)
+#define QM_SDQCR_CHANNELS_DEDICATED	0x00008000
+#if 0 /* These are defined in the external fsl_qman.h API */
+#define QM_SDQCR_CHANNELS_POOL_MASK	0x00007fff
+#define QM_SDQCR_CHANNELS_POOL(n)	(0x00008000 >> (n))
+#endif
+#define QM_SDQCR_SPECIFICWQ_MASK	0x000000f7
+#define QM_SDQCR_SPECIFICWQ_DEDICATED	0x00000000
+#define QM_SDQCR_SPECIFICWQ_POOL(n)	((n) << 4)
+#define QM_SDQCR_SPECIFICWQ_WQ(n)	(n)
+
+/* For qm_dqrr_vdqcr_set(); Choose one PRECEDENCE. EXACT is optional. Use
+ * NUMFRAMES(n) (6-bit) or NUMFRAMES_TILLEMPTY to fill in the frame-count. Use
+ * FQID(n) to fill in the frame queue ID. */
+#if 0 /* These are defined in the external fsl_qman.h API */
+#define QM_VDQCR_PRECEDENCE_VDQCR	0x0
+#define QM_VDQCR_PRECEDENCE_SDQCR	0x80000000
+#define QM_VDQCR_EXACT			0x40000000
+#define QM_VDQCR_NUMFRAMES_MASK		0x3f000000
+#define QM_VDQCR_NUMFRAMES_SET(n)	(((n) & 0x3f) << 24)
+#define QM_VDQCR_NUMFRAMES_GET(n)	(((n) >> 24) & 0x3f)
+#define QM_VDQCR_NUMFRAMES_TILLEMPTY	QM_VDQCR_NUMFRAMES_SET(0)
+#endif
+#define QM_VDQCR_FQID_MASK		0x00ffffff
+#define QM_VDQCR_FQID(n)		((n) & QM_VDQCR_FQID_MASK)
+
+/* For qm_dqrr_pdqcr_set(); Choose one MODE. Choose one COUNT.
+ * If MODE==SCHEDULED
+ *   Choose SCHEDULED_CHANNELS or SCHEDULED_SPECIFICWQ. Choose one dequeue TYPE.
+ *   If CHANNELS,
+ *     Choose CHANNELS_DEDICATED and/or CHANNELS_POOL() channels.
+ *     You can choose DEDICATED_PRECEDENCE if the portal channel should have
+ *     priority.
+ *   If SPECIFICWQ,
+ *     Either select the work-queue ID with SPECIFICWQ_WQ(), or select the
+ *     channel (SPECIFICWQ_DEDICATED or SPECIFICWQ_POOL()) and specify the
+ *     work-queue priority (0-7) with SPECIFICWQ_WQ() - either way, you get the
+ *     same value.
+ * If MODE==UNSCHEDULED
+ *     Choose FQID().
+ */
+#define QM_PDQCR_MODE_SCHEDULED		0x0
+#define QM_PDQCR_MODE_UNSCHEDULED	0x80000000
+#define QM_PDQCR_SCHEDULED_CHANNELS	0x0
+#define QM_PDQCR_SCHEDULED_SPECIFICWQ	0x40000000
+#define QM_PDQCR_COUNT_EXACT1		0x0
+#define QM_PDQCR_COUNT_UPTO3		0x20000000
+#define QM_PDQCR_DEDICATED_PRECEDENCE	0x10000000
+#define QM_PDQCR_TYPE_MASK		0x03000000
+#define QM_PDQCR_TYPE_NULL		0x0
+#define QM_PDQCR_TYPE_PRIO_QOS		0x01000000
+#define QM_PDQCR_TYPE_ACTIVE_QOS	0x02000000
+#define QM_PDQCR_TYPE_ACTIVE		0x03000000
+#define QM_PDQCR_CHANNELS_DEDICATED	0x00008000
+#define QM_PDQCR_CHANNELS_POOL(n)	(0x00008000 >> (n))
+#define QM_PDQCR_SPECIFICWQ_MASK	0x000000f7
+#define QM_PDQCR_SPECIFICWQ_DEDICATED	0x00000000
+#define QM_PDQCR_SPECIFICWQ_POOL(n)	((n) << 4)
+#define QM_PDQCR_SPECIFICWQ_WQ(n)	(n)
+#define QM_PDQCR_FQID(n)		((n) & 0xffffff)
+
+/* Used by all portal interrupt registers except 'inhibit'. NB, some of these
+ * definitions are exported for use by the qman_irqsource_***() APIs, so are
+ * commented-out here. */
+#define QM_PIRQ_DQAVAIL	0x0000ffff	/* Channels with frame availability */
+#if 0
+#define QM_PIRQ_CSCI	0x00100000	/* Congestion State Change */
+#define QM_PIRQ_EQCI	0x00080000	/* Enqueue Command Committed */
+#define QM_PIRQ_EQRI	0x00040000	/* EQCR Ring (below threshold) */
+#define QM_PIRQ_DQRI	0x00020000	/* DQRR Ring (non-empty) */
+#define QM_PIRQ_MRI	0x00010000	/* MR Ring (non-empty) */
+/* This mask contains all the interrupt sources that need handling except DQRI,
+ * ie. that if present should trigger slow-path processing. */
+#define QM_PIRQ_SLOW	(QM_PIRQ_CSCI | QM_PIRQ_EQCI | QM_PIRQ_EQRI | \
+			QM_PIRQ_MRI)
+#endif
+/* The DQAVAIL interrupt fields break down into these bits; */
+#define QM_DQAVAIL_PORTAL	0x8000		/* Portal channel */
+#define QM_DQAVAIL_POOL(n)	(0x8000 >> (n))	/* Pool channel, n==[1..15] */
+#define QM_DQAVAIL_MASK		0xffff
+/* This mask contains all the "irqsource" bits visible to API users */
+#define QM_PIRQ_VISIBLE	(QM_PIRQ_SLOW | QM_PIRQ_DQRI)
+
+/* These are qm_<reg>_<verb>(). So for example, qm_disable_write() means "write
+ * the disable register" rather than "disable the ability to write". */
+#define qm_isr_status_read(qm)		__qm_isr_read(qm, qm_isr_status)
+#define qm_isr_status_clear(qm, m)	__qm_isr_write(qm, qm_isr_status, m)
+#define qm_isr_enable_read(qm)		__qm_isr_read(qm, qm_isr_enable)
+#define qm_isr_enable_write(qm, v)	__qm_isr_write(qm, qm_isr_enable, v)
+#define qm_isr_disable_read(qm)		__qm_isr_read(qm, qm_isr_disable)
+#define qm_isr_disable_write(qm, v)	__qm_isr_write(qm, qm_isr_disable, v)
+/* TODO: unfortunate name-clash here, reword? */
+#define qm_isr_inhibit(qm)		__qm_isr_write(qm, qm_isr_inhibit, 1)
+#define qm_isr_uninhibit(qm)		__qm_isr_write(qm, qm_isr_inhibit, 0)
 
diff --git a/drivers/hwqueue/qman_sys.h b/drivers/hwqueue/qman_sys.h
index 61b02b5..6d007de 100644
--- a/drivers/hwqueue/qman_sys.h
+++ b/drivers/hwqueue/qman_sys.h
@@ -1,4 +1,4 @@
-/* Copyright (c) 2008, 2009 Freescale Semiconductor, Inc.
+/* Copyright (c) 2008-2010 Freescale Semiconductor, Inc.
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -41,25 +41,50 @@
 #define QM_ASSERT(x) BM_ASSERT(x)
 #include "../hwalloc/bman_sys.h"
 
-/* do slow-path processing via IRQ */
-#define CONFIG_FSL_QMAN_PORTAL_FLAG_IRQ_SLOW
+/* CONFIG_FSL_QMAN_BUG_AND_FEATURE_REV1 is defined via Kconfig, because it's a
+ * knob for users. */
 
-/* do not do fast-path processing via IRQ */
-#define CONFIG_FSL_QMAN_PORTAL_FLAG_IRQ_FAST
+/* don't use rev1-specific adaptive "backoff" for EQCR:CI updates */
+#undef CONFIG_FSL_QMAN_ADAPTIVE_EQCR_THROTTLE
 
-/* portals aren't SMP-locked, they're core-affine */
-#undef CONFIG_FSL_QMAN_PORTAL_FLAG_LOCKED
+/* CONFIG_FSL_BMAN_QHECKING is defined via Kconfig, because it's a knob for
+ * users. */
 
-/* portals do not initialise in recovery mode */
-#undef CONFIG_FSL_QMAN_PORTAL_FLAG_RECOVER
+/* CONFIG_FSL_QMAN_FQALLOCATOR is defined via Kconfig, because it's a knob for
+ * users. */
+
+/* CONFIG_FSL_QMAN_PORTAL_DISABLEAUTO_DCA is defined via Kconfig, because it's a
+ * knob for users. */
 
-#if defined(CONFIG_FSL_QMAN_PORTAL_FLAG_IRQ_SLOW) || \
-		defined(CONFIG_FSL_QMAN_PORTAL_FLAG_IRQ_FAST)
+/* portal interrupt settings */
+#define CONFIG_FSL_QMAN_PIRQ_DQRR_ITHRESH 12
+#define CONFIG_FSL_QMAN_PIRQ_MR_ITHRESH 4
+#define CONFIG_FSL_QMAN_PIRQ_IPERIOD 100
+
+/* CONFIG_FSL_QMAN_POLL_LIMIT is defined via Kconfig, because it's a knob for
+ * users. */
+
+/* support portal IRQs, whether or not they're used at run-time */
 #define CONFIG_FSL_QMAN_HAVE_IRQ
-#else
-#undef CONFIG_FSL_QMAN_HAVE_IRQ
+
+#ifdef CONFIG_FSL_QMAN_HAVE_IRQ
+/* do slow-path processing via IRQ */
+#define CONFIG_FSL_QMAN_PIRQ_SLOW
+/* do fast-path processing via IRQ */
+#define CONFIG_FSL_QMAN_PIRQ_FAST
 #endif
 
+/* don't compile support for NULL FQ handling */
+#define CONFIG_FSL_QMAN_NULL_FQ_DEMUX
+
+#ifdef CONFIG_FSL_QMAN_NULL_FQ_DEMUX
+/* portals do not initialise in recovery mode */
+#undef CONFIG_FSL_QMAN_PORTAL_FLAG_RECOVER
+#endif
+
+/* don't compile support for DQRR prefetching (so stashing is required) */
+#define CONFIG_FSL_QMAN_DQRR_PREFETCHING
+
 /************/
 /* RB-trees */
 /************/
diff --git a/drivers/hwqueue/qman_test.c b/drivers/hwqueue/qman_test.c
index 7bc3649..5014ce6 100644
--- a/drivers/hwqueue/qman_test.c
+++ b/drivers/hwqueue/qman_test.c
@@ -1,4 +1,4 @@
-/* Copyright (c) 2008, 2009 Freescale Semiconductor, Inc.
+/* Copyright (c) 2008-2010 Freescale Semiconductor, Inc.
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -38,11 +38,7 @@ MODULE_DESCRIPTION("Qman testing");
 
 static int test_init(void)
 {
-	int loop;
-#ifdef CONFIG_FSL_QMAN_FQRANGE
-	qman_test_fqrange();
-#endif
-	loop = 1;
+	int loop = 1;
 	while(loop--) {
 #ifdef CONFIG_FSL_QMAN_TEST_STASH_POTATO
 		qman_test_hotpotato();
diff --git a/drivers/hwqueue/qman_test_fqrange.c b/drivers/hwqueue/qman_test_fqrange.c
deleted file mode 100644
index 3ce2bb6..0000000
--- a/drivers/hwqueue/qman_test_fqrange.c
+++ /dev/null
@@ -1,65 +0,0 @@
-/* Copyright (c) 2009 Freescale Semiconductor, Inc.
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *     * Redistributions of source code must retain the above copyright
- *       notice, this list of conditions and the following disclaimer.
- *     * Redistributions in binary form must reproduce the above copyright
- *       notice, this list of conditions and the following disclaimer in the
- *       documentation and/or other materials provided with the distribution.
- *     * Neither the name of Freescale Semiconductor nor the
- *       names of its contributors may be used to endorse or promote products
- *       derived from this software without specific prior written permission.
- *
- *
- * ALTERNATIVELY, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") as published by the Free Software
- * Foundation, either version 2 of that License or (at your option) any
- * later version.
- *
- * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
- * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
- * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
- * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
- * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
- * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
- * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
- * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "qman_test.h"
-
-void qman_test_fqrange(void)
-{
-	int num;
-	u32 result1, result2, result3;
-
-	pr_info("Testing \"FQRANGE\" allocator ...\n");
-	/* Seed the allocator*/
-	qman_release_fqid_range(0, 1000);
-
-	num = qman_alloc_fqid_range(&result1, 100, 4, 0);
-	BUG_ON(result1 % 4);
-
-	num = qman_alloc_fqid_range(&result2, 500, 500, 0);
-	BUG_ON((num != 500) || (result2 != 500));
-
-	num = qman_alloc_fqid_range(&result3, 1000, 0, 0);
-	BUG_ON(num >= 0);
-
-	num = qman_alloc_fqid_range(&result3, 1000, 0, 1);
-	BUG_ON(num < 400);
-
-	qman_release_fqid_range(result2, 500);
-	qman_release_fqid_range(result1, 100);
-	qman_release_fqid_range(result3, num);
-
-	/* It should now be possible to drain the allocator empty */
-	num = qman_alloc_fqid_range(&result1, 1000, 0, 0);
-	BUG_ON(num != 1000);
-	pr_info("                              ... SUCCESS!\n");
-}
-
diff --git a/drivers/hwqueue/qman_test_high.c b/drivers/hwqueue/qman_test_high.c
index a940594..793c62f 100644
--- a/drivers/hwqueue/qman_test_high.c
+++ b/drivers/hwqueue/qman_test_high.c
@@ -48,7 +48,6 @@
 			QM_SDQCR_CHANNELS_POOL(POOL_ID))
 #define PORTAL_OPAQUE	(void *)0xf00dbeef
 #define VDQCR_FLAGS	(QMAN_VOLATILE_FLAG_WAIT | QMAN_VOLATILE_FLAG_FINISH)
-#define PORTAL_FLAGS	QMAN_PORTAL_FLAG_IRQ
 
 /*************************************/
 /* Predeclarations (eg. for fq_base) */
@@ -85,8 +84,7 @@ static int retire_complete, sdqcr_complete;
 /* Helpers for initialising and "incrementing" a frame descriptor */
 static void fd_init(struct qm_fd *__fd)
 {
-	__fd->addr_hi = 0xab;		/* high 16-bits */
-	__fd->addr_lo = 0xdeadbeef;	/* low 32-bits */
+	qm_fd_addr_set64(__fd, 0xabdeadbeefLLU);
 	__fd->format = qm_fd_contig_big;
 	__fd->length29 = 0x0000ffff;
 	__fd->cmd = 0xfeedf00d;
@@ -94,8 +92,12 @@ static void fd_init(struct qm_fd *__fd)
 
 static void fd_inc(struct qm_fd *__fd)
 {
-	__fd->addr_lo++;
-	__fd->addr_hi--;
+	u64 t = qm_fd_addr_get64(__fd);
+	int z = t >> 40;
+	t <<= 1;
+	if (z)
+		t |= 1;
+	qm_fd_addr_set64(__fd, t);
 	__fd->length29--;
 	__fd->cmd++;
 }
@@ -103,9 +105,7 @@ static void fd_inc(struct qm_fd *__fd)
 /* The only part of the 'fd' we can't memcmp() is the ppid */
 static int fd_cmp(const struct qm_fd *a, const struct qm_fd *b)
 {
-	int r = a->addr_hi - b->addr_hi;
-	if (!r)
-		r = a->addr_lo - b->addr_lo;
+	int r = (qm_fd_addr_get64(a) == qm_fd_addr_get64(b)) ? 0 : -1;
 	if (!r)
 		r = a->format - b->format;
 	if (!r)
diff --git a/drivers/hwqueue/qman_test_hotpotato.c b/drivers/hwqueue/qman_test_hotpotato.c
index 205b57f..416a7fd 100644
--- a/drivers/hwqueue/qman_test_hotpotato.c
+++ b/drivers/hwqueue/qman_test_hotpotato.c
@@ -1,4 +1,4 @@
-/* Copyright (c) 2009-2010, Freescale Semiconductor, Inc.
+/* Copyright (c) 2009-2010 Freescale Semiconductor, Inc.
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -160,7 +160,7 @@ struct hp_cpu {
 };
 
 /* Each cpu has one of these */
-static DEFINE_PER_CPU(struct hp_cpu, hp_cpu);
+static DEFINE_PER_CPU(struct hp_cpu, hp_cpus);
 
 /* links together the hp_cpu structs, in first-come first-serve order. */
 static LIST_HEAD(hp_cpu_list);
@@ -229,7 +229,7 @@ static inline void process_frame_data(struct hp_handler *handler,
 	u32 *p = handler->frame_ptr;
 	u32 lfsr = HP_FIRST_WORD;
 	int loop;
-	if (fd->addr_lo != (u32)handler->addr)
+	if (qm_fd_addr_get64(fd) != handler->addr)
 		panic("bad frame address");
 	for (loop = 0; loop < HP_NUM_WORDS; loop++, p++) {
 		*p ^= handler->rx_mixer;
@@ -273,37 +273,34 @@ static void create_per_cpu_handlers(void)
 {
 	struct hp_handler *handler;
 	int loop;
-	struct hp_cpu *p = &get_cpu_var(hp_cpu);
-	/* release atomicity (so alloc is ok), we're run core-affine anyway */
-	put_cpu_var(hp_cpu);
+	struct hp_cpu *hp_cpu = &__get_cpu_var(hp_cpus);
 
-	p->processor_id = smp_processor_id();
+	hp_cpu->processor_id = smp_processor_id();
 	spin_lock(&hp_lock);
-	list_add_tail(&p->node, &hp_cpu_list);
+	list_add_tail(&hp_cpu->node, &hp_cpu_list);
 	hp_cpu_list_length++;
 	spin_unlock(&hp_lock);
-	INIT_LIST_HEAD(&p->handlers);
+	INIT_LIST_HEAD(&hp_cpu->handlers);
 	for (loop = 0; loop < HP_PER_CPU; loop++) {
 		handler = kmem_cache_alloc(hp_handler_slab, GFP_KERNEL);
 		if (!handler)
 			panic("kmem_cache_alloc() failed");
-		handler->processor_id = p->processor_id;
+		handler->processor_id = hp_cpu->processor_id;
 		handler->addr = frame_dma;
 		handler->frame_ptr = frame_ptr;
-		list_add_tail(&handler->node, &p->handlers);
+		list_add_tail(&handler->node, &hp_cpu->handlers);
 	}
 }
 
 static void destroy_per_cpu_handlers(void)
 {
 	struct list_head *loop, *tmp;
-	struct hp_cpu *p = &get_cpu_var(hp_cpu);
-	put_cpu_var(hp_cpu);
+	struct hp_cpu *hp_cpu = &__get_cpu_var(hp_cpus);
 
 	spin_lock(&hp_lock);
-	list_del(&p->node);
+	list_del(&hp_cpu->node);
 	spin_unlock(&hp_lock);
-	list_for_each_safe(loop, tmp, &p->handlers) {
+	list_for_each_safe(loop, tmp, &hp_cpu->handlers) {
 		u32 flags;
 		struct hp_handler *handler = list_entry(loop, struct hp_handler,
 							node);
@@ -318,7 +315,6 @@ static void destroy_per_cpu_handlers(void)
 		list_del(&handler->node);
 		kmem_cache_free(hp_handler_slab, handler);
 	}
-
 }
 
 static inline u8 num_cachelines(u32 offset)
@@ -438,7 +434,7 @@ static void send_first_frame(void *ignore)
 
 	BUG_ON(special_handler->processor_id != smp_processor_id());
 	memset(&fd, 0, sizeof(fd));
-	fd.addr_lo = (u32)special_handler->addr;
+	qm_fd_addr_set64(&fd, special_handler->addr);
 	fd.format = qm_fd_contig_big;
 	fd.length29 = HP_NUM_WORDS * 4;
 	for (loop = 0; loop < HP_NUM_WORDS; loop++, p++) {
diff --git a/drivers/hwqueue/qman_utility.c b/drivers/hwqueue/qman_utility.c
index d1cf16e..44fc072 100644
--- a/drivers/hwqueue/qman_utility.c
+++ b/drivers/hwqueue/qman_utility.c
@@ -1,4 +1,4 @@
-/* Copyright (c) 2008, 2009 Freescale Semiconductor, Inc.
+/* Copyright (c) 2008-2010 Freescale Semiconductor, Inc.
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
diff --git a/include/linux/fsl_qman.h b/include/linux/fsl_qman.h
index 686eb8d..d19abf3 100644
--- a/include/linux/fsl_qman.h
+++ b/include/linux/fsl_qman.h
@@ -39,14 +39,6 @@ extern "C" {
 
 /* Last updated for v00.800 of the BG */
 
-/*************************************************/
-/*   QMan s/w corenet portal, low-level i/face   */
-/*************************************************/
-
-/* Portal constants */
-#define QM_EQCR_SIZE		8
-#define QM_DQRR_SIZE		16
-#define QM_MR_SIZE		8
 /* Hardware constants */
 enum qm_channel {
 	qm_channel_swportal0 = 0, qm_channel_swportal1, qm_channel_swportal2,
@@ -69,12 +61,6 @@ enum qm_channel {
 	qm_channel_caam = 0x80,
 	qm_channel_pme = 0xa0,
 };
-enum qm_isr_reg {
-	qm_isr_status = 0,
-	qm_isr_enable = 1,
-	qm_isr_disable = 2,
-	qm_isr_inhibit = 3
-};
 enum qm_dc_portal {
 	qm_dc_portal_fman0 = 0,
 	qm_dc_portal_fman1 = 1,
@@ -82,133 +68,16 @@ enum qm_dc_portal {
 	qm_dc_portal_pme = 3
 };
 
-/* Represents s/w corenet portal mapped data structures */
-struct qm_eqcr_entry;	/* EQCR (EnQueue Command Ring) entries */
-struct qm_dqrr_entry;	/* DQRR (DeQueue Response Ring) entries */
-struct qm_mr_entry;	/* MR (Message Ring) entries */
-struct qm_mc_command;	/* MC (Management Command) command */
-struct qm_mc_result;	/* MC result */
-
-/* This type represents a s/w corenet portal space, and is used for creating the
- * portal objects within it (EQCR, DQRR, etc) */
-struct qm_portal;
-
-/* When iterating the available portals, this is the exposed config structure */
-struct qm_portal_config {
-	/* If the caller enables DQRR stashing (and thus wishes to operate the
-	 * portal from only one cpu), this is the logical CPU that the portal
-	 * will stash to. Whether stashing is enabled or not, this setting is
-	 * also used for any "core-affine" portals, ie. default portals
-	 * associated to the corresponding cpu. -1 implies that there is no core
-	 * affinity configured. */
-	int cpu;
-	/* portal interrupt line */
-	int irq;
-	/* The portal's dedicated channel id, use this value for initialising
-	 * frame queues to target this portal when scheduled. */
-	enum qm_channel channel;
-	/* A mask of which pool channels this portal has dequeue access to
-	 * (using QM_SDQCR_CHANNELS_POOL(n) for the bitmask) */
-	u32 pools;
-	/* which portal sub-interfaces are already bound (ie. "in use") */
-	u8 bound;
-	/* does this portal have PAMU assistance from hypervisor? */
-	int has_hv_dma;
-};
-/* qm_portal_config::bound uses these bit masks */
-#define QM_BIND_EQCR	0x01
-#define QM_BIND_DQRR	0x02
-#define QM_BIND_MR	0x04
-#define QM_BIND_MC	0x08
-#define QM_BIND_ISR	0x10
-
-/* This struct represents a pool channel */
-struct qm_pool_channel {
-	/* The QM_SDQCR_CHANNELS_POOL(n) bit that corresponds to this channel */
-	u32 pool;
-	/* The channel id, used for initialising frame queues to target this
-	 * channel. */
-	enum qm_channel channel;
-	/* Bitmask of portal (logical-, not cell-)indices that have dequeue
-	 * access to this channel;
-	 * 0x001 -> qm_portal_get(0)
-	 * 0x002 -> qm_portal_get(1)
-	 * 0x004 -> qm_portal_get(2)
-	 * ...
-	 * 0x200 -> qm_portal_get(9)
-	 */
-	u32 portals;
-};
-
-/* Portal modes.
- *   Enum types;
- *     pmode == production mode
- *     cmode == consumption mode,
- *     dmode == h/w dequeue mode.
- *   Enum values use 3 letter codes. First letter matches the portal mode,
- *   remaining two letters indicate;
- *     ci == cache-inhibited portal register
- *     ce == cache-enabled portal register
- *     vb == in-band valid-bit (cache-enabled)
- *     dc == DCA (Discrete Consumption Acknowledgement), DQRR-only
- *   As for "enum qm_dqrr_dmode", it should be self-explanatory.
- */
-enum qm_eqcr_pmode {		/* matches QCSP_CFG::EPM */
-	qm_eqcr_pci = 0,	/* PI index, cache-inhibited */
-	qm_eqcr_pce = 1,	/* PI index, cache-enabled */
-	qm_eqcr_pvb = 2		/* valid-bit */
-};
-enum qm_eqcr_cmode {		/* s/w-only */
-	qm_eqcr_cci,		/* CI index, cache-inhibited */
-	qm_eqcr_cce		/* CI index, cache-enabled */
-};
-enum qm_dqrr_dmode {		/* matches QCSP_CFG::DP */
-	qm_dqrr_dpush = 0,	/* SDQCR  + VDQCR */
-	qm_dqrr_dpull = 1	/* PDQCR */
-};
-enum qm_dqrr_pmode {		/* s/w-only */
-	qm_dqrr_pci,		/* reads DQRR_PI_CINH */
-	qm_dqrr_pce,		/* reads DQRR_PI_CENA */
-	qm_dqrr_pvb		/* reads valid-bit */
-};
-enum qm_dqrr_cmode {		/* matches QCSP_CFG::DCM */
-	qm_dqrr_cci = 0,	/* CI index, cache-inhibited */
-	qm_dqrr_cce = 1,	/* CI index, cache-enabled */
-	qm_dqrr_cdc = 2		/* Discrete Consumption Acknowledgement */
-};
-enum qm_mr_pmode {		/* s/w-only */
-	qm_mr_pci,		/* reads MR_PI_CINH */
-	qm_mr_pce,		/* reads MR_PI_CENA */
-	qm_mr_pvb		/* reads valid-bit */
-};
-enum qm_mr_cmode {		/* matches QCSP_CFG::MM */
-	qm_mr_cci = 0,		/* CI index, cache-inhibited */
-	qm_mr_cce = 1		/* CI index, cache-enabled */
-};
-
-
-/* ------------------------------ */
-/* --- Portal enumeration API --- */
-
-/* Obtain the number of portals available */
-u8 qm_portal_num(void);
-
-/* Obtain a portal handle and configuration information about it */
-struct qm_portal *qm_portal_get(u8 idx);
-const struct qm_portal_config *qm_portal_config(const struct qm_portal *portal);
-
-
-/* ------------------------------------ */
-/* --- Pool channel enumeration API --- */
-
-/* Obtain a mask of the available pool channels, expressed using
- * QM_SDQCR_CHANNELS_POOL(n). */
-u32 qm_pools(void);
-
-/* Retrieve a pool channel configuration, given a QM_SDQCR_CHANNEL_POOL(n)
- * bit-mask (the least significant bit of 'mask' is used if more than one bit is
- * set). */
-const struct qm_pool_channel *qm_pool_channel(u32 mask);
+/* Portal processing (interrupt) sources */
+#define QM_PIRQ_CSCI	0x00100000	/* Congestion State Change */
+#define QM_PIRQ_EQCI	0x00080000	/* Enqueue Command Committed */
+#define QM_PIRQ_EQRI	0x00040000	/* EQCR Ring (below threshold) */
+#define QM_PIRQ_DQRI	0x00020000	/* DQRR Ring (non-empty) */
+#define QM_PIRQ_MRI	0x00010000	/* MR Ring (non-empty) */
+/* This mask contains all the interrupt sources that need handling except DQRI,
+ * ie. that if present should trigger slow-path processing. */
+#define QM_PIRQ_SLOW	(QM_PIRQ_CSCI | QM_PIRQ_EQCI | QM_PIRQ_EQRI | \
+			QM_PIRQ_MRI)
 
 
 /* ------------------------ */
@@ -239,135 +108,11 @@ static inline void qm_fq_free(u32 fqid)
 
 #endif /* !CONFIG_FSL_QMAN_FQALLOCATOR */
 
-
-/* ---------------- */
-/* --- EQCR API --- */
-
-/* Create/destroy */
-int qm_eqcr_init(struct qm_portal *portal, enum qm_eqcr_pmode pmode,
-		enum qm_eqcr_cmode cmode);
-void qm_eqcr_finish(struct qm_portal *portal);
-
-/* Start/abort EQCR entry */
-struct qm_eqcr_entry *qm_eqcr_start(struct qm_portal *portal);
-void qm_eqcr_abort(struct qm_portal *portal);
-
-/* For PI modes only. This presumes a started but uncommited EQCR entry. If
- * there's no more room in the EQCR, this function returns NULL. Otherwise it
- * returns the next EQCR entry and increments an internal PI counter without
- * flushing it to h/w. */
-struct qm_eqcr_entry *qm_eqcr_pend_and_next(struct qm_portal *portal, u8 myverb);
-
-/* Commit EQCR entries, including pending ones (aka "write PI") */
-void qm_eqcr_pci_commit(struct qm_portal *portal, u8 myverb);
-void qm_eqcr_pce_prefetch(struct qm_portal *portal);
-void qm_eqcr_pce_commit(struct qm_portal *portal, u8 myverb);
-void qm_eqcr_pvb_commit(struct qm_portal *portal, u8 myverb);
-
-/* Track h/w consumption. Returns non-zero if h/w had consumed previously
- * unconsumed EQCR entries (it returns the number of them in fact). */
-u8 qm_eqcr_cci_update(struct qm_portal *portal);
-void qm_eqcr_cce_prefetch(struct qm_portal *portal);
-u8 qm_eqcr_cce_update(struct qm_portal *portal);
-u8 qm_eqcr_get_ithresh(struct qm_portal *portal);
-void qm_eqcr_set_ithresh(struct qm_portal *portal, u8 ithresh);
-/* Returns the number of available EQCR entries */
-u8 qm_eqcr_get_avail(struct qm_portal *portal);
-/* Returns the number of unconsumed EQCR entries */
-u8 qm_eqcr_get_fill(struct qm_portal *portal);
-
-
-/* ---------------- */
-/* --- DQRR API --- */
-
-/* Create/destroy */
-int qm_dqrr_init(struct qm_portal *portal, enum qm_dqrr_dmode dmode,
-		enum qm_dqrr_pmode pmode, enum qm_dqrr_cmode cmode,
-		/* QCSP_CFG fields; MF, RE, SE (respectively) */
-		u8 max_fill, int stash_ring, int stash_data);
-void qm_dqrr_finish(struct qm_portal *portal);
-
-/* Read 'current' DQRR entry (ie. at the cursor). NB, prefetch generally not
- * required in pvb mode, as pvb_prefetch() will touch the same cacheline. */
-void qm_dqrr_current_prefetch(struct qm_portal *portal);
-struct qm_dqrr_entry *qm_dqrr_current(struct qm_portal *portal);
-u8 qm_dqrr_cursor(struct qm_portal *portal);
-
-/* Increment 'current' cursor, must not already be at "EOF". Returns number of
- * remaining DQRR entries, zero if the 'cursor' is now at "EOF". */
-u8 qm_dqrr_next(struct qm_portal *portal);
-
-/* Track h/w production. Returns non-zero if there are new DQRR entries. */
-u8 qm_dqrr_pci_update(struct qm_portal *portal);
-void qm_dqrr_pce_prefetch(struct qm_portal *portal);
-u8 qm_dqrr_pce_update(struct qm_portal *portal);
-void qm_dqrr_pvb_prefetch(struct qm_portal *portal);
-u8 qm_dqrr_pvb_update(struct qm_portal *portal);
-u8 qm_dqrr_get_ithresh(struct qm_portal *portal);
-void qm_dqrr_set_ithresh(struct qm_portal *portal, u8 ithresh);
-u8 qm_dqrr_get_maxfill(struct qm_portal *portal);
-void qm_dqrr_set_maxfill(struct qm_portal *portal, u8 mf);
-
-/* Consume DQRR entries. NB for 'bitmask', 0x8000 represents idx==0, 0x4000 is
- * idx==1, etc through to 0x0001 being idx==15. */
-void qm_dqrr_cci_consume(struct qm_portal *portal, u8 num);
-void qm_dqrr_cci_consume_to_current(struct qm_portal *portal);
-void qm_dqrr_cce_prefetch(struct qm_portal *portal);
-void qm_dqrr_cce_consume(struct qm_portal *portal, u8 num);
-void qm_dqrr_cce_consume_to_current(struct qm_portal *portal);
-void qm_dqrr_cdc_consume_1(struct qm_portal *portal, u8 idx, int park);
-void qm_dqrr_cdc_consume_1ptr(struct qm_portal *portal, struct qm_dqrr_entry *dq,
-				int park);
-void qm_dqrr_cdc_consume_n(struct qm_portal *portal, u16 bitmask);
-
-/* For CDC; use these to read the effective CI */
-u8 qm_dqrr_cdc_cci(struct qm_portal *portal);
-void qm_dqrr_cdc_cce_prefetch(struct qm_portal *portal);
-u8 qm_dqrr_cdc_cce(struct qm_portal *portal);
-
-/* For CCI/CCE; this returns the s/w-cached CI value */
-u8 qm_dqrr_get_ci(struct qm_portal *portal);
-/*            ; this issues a park-request */
-void qm_dqrr_park(struct qm_portal *portal, u8 idx);
-/*            ; or for the next-to-be-consumed DQRR entry */
-void qm_dqrr_park_ci(struct qm_portal *portal);
-
-/* For qm_dqrr_sdqcr_set(); Choose one SOURCE. Choose one COUNT. Choose one
- * dequeue TYPE. Choose TOKEN (8-bit).
- * If SOURCE == CHANNELS,
- *   Choose CHANNELS_DEDICATED and/or CHANNELS_POOL(n).
- *   You can choose DEDICATED_PRECEDENCE if the portal channel should have
- *   priority.
- * If SOURCE == SPECIFICWQ,
- *     Either select the work-queue ID with SPECIFICWQ_WQ(), or select the
- *     channel (SPECIFICWQ_DEDICATED or SPECIFICWQ_POOL()) and specify the
- *     work-queue priority (0-7) with SPECIFICWQ_WQ() - either way, you get the
- *     same value.
- */
-#define QM_SDQCR_SOURCE_CHANNELS	0x0
-#define QM_SDQCR_SOURCE_SPECIFICWQ	0x40000000
-#define QM_SDQCR_COUNT_EXACT1		0x0
-#define QM_SDQCR_COUNT_UPTO3		0x20000000
-#define QM_SDQCR_DEDICATED_PRECEDENCE	0x10000000
-#define QM_SDQCR_TYPE_MASK		0x03000000
-#define QM_SDQCR_TYPE_NULL		0x0
-#define QM_SDQCR_TYPE_PRIO_QOS		0x01000000
-#define QM_SDQCR_TYPE_ACTIVE_QOS	0x02000000
-#define QM_SDQCR_TYPE_ACTIVE		0x03000000
-#define QM_SDQCR_TOKEN_MASK		0x00ff0000
-#define QM_SDQCR_TOKEN_SET(v)		(((v) & 0xff) << 16)
-#define QM_SDQCR_TOKEN_GET(v)		(((v) >> 16) & 0xff)
-#define QM_SDQCR_CHANNELS_DEDICATED	0x00008000
+/* For qman_static_dequeue_*** APIs */
 #define QM_SDQCR_CHANNELS_POOL_MASK	0x00007fff
 #define QM_SDQCR_CHANNELS_POOL(n)	(0x00008000 >> (n))
-#define QM_SDQCR_SPECIFICWQ_MASK	0x000000f7
-#define QM_SDQCR_SPECIFICWQ_DEDICATED	0x00000000
-#define QM_SDQCR_SPECIFICWQ_POOL(n)	((n) << 4)
-#define QM_SDQCR_SPECIFICWQ_WQ(n)	(n)
-void qm_dqrr_sdqcr_set(struct qm_portal *portal, u32 sdqcr);
-u32 qm_dqrr_sdqcr_get(struct qm_portal *portal);
-
-/* For qm_dqrr_vdqcr_set(); Choose one PRECEDENCE. EXACT is optional. Use
+
+/* For qman_volatile_dequeue(); Choose one PRECEDENCE. EXACT is optional. Use
  * NUMFRAMES(n) (6-bit) or NUMFRAMES_TILLEMPTY to fill in the frame-count. Use
  * FQID(n) to fill in the frame queue ID. */
 #define QM_VDQCR_PRECEDENCE_VDQCR	0x0
@@ -377,171 +122,18 @@ u32 qm_dqrr_sdqcr_get(struct qm_portal *portal);
 #define QM_VDQCR_NUMFRAMES_SET(n)	(((n) & 0x3f) << 24)
 #define QM_VDQCR_NUMFRAMES_GET(n)	(((n) >> 24) & 0x3f)
 #define QM_VDQCR_NUMFRAMES_TILLEMPTY	QM_VDQCR_NUMFRAMES_SET(0)
-#define QM_VDQCR_FQID_MASK		0x00ffffff
-#define QM_VDQCR_FQID(n)		((n) & QM_VDQCR_FQID_MASK)
-void qm_dqrr_vdqcr_set(struct qm_portal *portal, u32 vdqcr);
-u32 qm_dqrr_vdqcr_get(struct qm_portal *portal);
-
-/* For qm_dqrr_pdqcr_set(); Choose one MODE. Choose one COUNT.
- * If MODE==SCHEDULED
- *   Choose SCHEDULED_CHANNELS or SCHEDULED_SPECIFICWQ. Choose one dequeue TYPE.
- *   If CHANNELS,
- *     Choose CHANNELS_DEDICATED and/or CHANNELS_POOL() channels.
- *     You can choose DEDICATED_PRECEDENCE if the portal channel should have
- *     priority.
- *   If SPECIFICWQ,
- *     Either select the work-queue ID with SPECIFICWQ_WQ(), or select the
- *     channel (SPECIFICWQ_DEDICATED or SPECIFICWQ_POOL()) and specify the
- *     work-queue priority (0-7) with SPECIFICWQ_WQ() - either way, you get the
- *     same value.
- * If MODE==UNSCHEDULED
- *     Choose FQID().
- */
-#define QM_PDQCR_MODE_SCHEDULED		0x0
-#define QM_PDQCR_MODE_UNSCHEDULED	0x80000000
-#define QM_PDQCR_SCHEDULED_CHANNELS	0x0
-#define QM_PDQCR_SCHEDULED_SPECIFICWQ	0x40000000
-#define QM_PDQCR_COUNT_EXACT1		0x0
-#define QM_PDQCR_COUNT_UPTO3		0x20000000
-#define QM_PDQCR_DEDICATED_PRECEDENCE	0x10000000
-#define QM_PDQCR_TYPE_MASK		0x03000000
-#define QM_PDQCR_TYPE_NULL		0x0
-#define QM_PDQCR_TYPE_PRIO_QOS		0x01000000
-#define QM_PDQCR_TYPE_ACTIVE_QOS	0x02000000
-#define QM_PDQCR_TYPE_ACTIVE		0x03000000
-#define QM_PDQCR_CHANNELS_DEDICATED	0x00008000
-#define QM_PDQCR_CHANNELS_POOL(n)	(0x00008000 >> (n))
-#define QM_PDQCR_SPECIFICWQ_MASK	0x000000f7
-#define QM_PDQCR_SPECIFICWQ_DEDICATED	0x00000000
-#define QM_PDQCR_SPECIFICWQ_POOL(n)	((n) << 4)
-#define QM_PDQCR_SPECIFICWQ_WQ(n)	(n)
-#define QM_PDQCR_FQID(n)		((n) & 0xffffff)
-void qm_dqrr_pdqcr_set(struct qm_portal *portal, u32 pdqcr);
-u32 qm_dqrr_pdqcr_get(struct qm_portal *portal);
-
-
-/* -------------- */
-/* --- MR API --- */
-
-/* Create/destroy */
-int qm_mr_init(struct qm_portal *portal, enum qm_mr_pmode pmode,
-		enum qm_mr_cmode cmode);
-void qm_mr_finish(struct qm_portal *portal);
-
-/* Read 'current' MR entry (ie. at the cursor) */
-void qm_mr_current_prefetch(struct qm_portal *portal);
-struct qm_mr_entry *qm_mr_current(struct qm_portal *portal);
-u8 qm_mr_cursor(struct qm_portal *portal);
-
-/* Increment 'current' cursor, must not alreday be at "EOF". Returns number of
- * remaining MR entries, zero if the 'cursor' is now at "EOF". */
-u8 qm_mr_next(struct qm_portal *portal);
-
-/* Track h/w production. Returns non-zero if there are new DQRR entries. */
-u8 qm_mr_pci_update(struct qm_portal *portal);
-void qm_mr_pce_prefetch(struct qm_portal *portal);
-u8 qm_mr_pce_update(struct qm_portal *portal);
-void qm_mr_pvb_prefetch(struct qm_portal *portal);
-u8 qm_mr_pvb_update(struct qm_portal *portal);
-u8 qm_mr_get_ithresh(struct qm_portal *portal);
-void qm_mr_set_ithresh(struct qm_portal *portal, u8 ithresh);
-
-/* Consume MR entries */
-void qm_mr_cci_consume(struct qm_portal *portal, u8 num);
-void qm_mr_cci_consume_to_current(struct qm_portal *portal);
-void qm_mr_cce_prefetch(struct qm_portal *portal);
-void qm_mr_cce_consume(struct qm_portal *portal, u8 num);
-void qm_mr_cce_consume_to_current(struct qm_portal *portal);
-
-/* Return the s/w-cached CI value */
-u8 qm_mr_get_ci(struct qm_portal *portal);
-
-
-/* ------------------------------ */
-/* --- Management command API --- */
-
-/* Create/destroy */
-int qm_mc_init(struct qm_portal *portal);
-void qm_mc_finish(struct qm_portal *portal);
-
-/* Start/abort mgmt command */
-struct qm_mc_command *qm_mc_start(struct qm_portal *portal);
-void qm_mc_abort(struct qm_portal *portal);
-
-/* Writes 'verb' with appropriate 'vbit'. Invalidates and pre-fetches the
- * response. */
-void qm_mc_commit(struct qm_portal *portal, u8 myverb);
-
-/* Poll for result. If NULL, invalidates and prefetches for the next call. */
-struct qm_mc_result *qm_mc_result(struct qm_portal *portal);
-
-
-/* ------------------------------------- */
-/* --- Portal interrupt register API --- */
-
-/* Quick explanation of the Qman interrupt model. Each bit has a source
- * condition, that source is asserted iff the condition is true. Eg. Each
- * DQAVAIL source bit tracks whether the corresponding channel's work queues
- * contain any truly scheduled frame queues. That source exists "asserted" if
- * and while there are truly-scheduled FQs available, it is deasserted as/when
- * there are no longer any truly-scheduled FQs available. The same is true for
- * the various other interrupt source conditions (QM_PIRQ_***). The following
- * steps indicate what those source bits affect;
- *    1. if the corresponding bit is set in the disable register, the source
- *       bit is masked off, we never see any effect from it.
- *    2. otherwise, the corresponding bit is set in the status register. Once
- *       asserted in the status register, it must be write-1-to-clear'd - the
- *       status register bit will stay set even if the source condition
- *       deasserts.
- *    3. if a bit is set in the status register but *not* set in the enable
- *       register, it will not cause the interrupt to assert. Other bits may
- *       still cause the interrupt to assert of course, and a read of the
- *       status register can still reveal un-enabled bits - this is why the
- *       enable and disable registers aren't strictly speaking "opposites".
- *       "Un-enabled" means it won't, on its own, trigger an interrupt.
- *       "Disabled" means it won't even show up in the status register.
- *    4. if a bit is set in the status register *and* the enable register, the
- *       interrupt line will assert if and only if the inhibit register is
- *       zero. The inhibit register is the only interrupt-related register that
- *       does not share the bit definitions - it is a boolean on/off register.
- */
-
-/* Create/destroy */
-int qm_isr_init(struct qm_portal *portal);
-void qm_isr_finish(struct qm_portal *portal);
-void qm_isr_set_iperiod(struct qm_portal *portal, u16 iperiod);
-
-/* Used by all portal interrupt registers except 'inhibit' */
-#define QM_PIRQ_CSCI	0x00100000	/* Congestion State Change */
-#define QM_PIRQ_EQCI	0x00080000	/* Enqueue Command Committed */
-#define QM_PIRQ_EQRI	0x00040000	/* EQCR Ring (below threshold) */
-#define QM_PIRQ_DQRI	0x00020000	/* DQRR Ring (non-empty) */
-#define QM_PIRQ_MRI	0x00010000	/* MR Ring (non-empty) */
-#define QM_PIRQ_DQAVAIL	0x0000ffff	/* Channels with frame availability */
-/* The DQAVAIL interrupt fields break down into these bits; */
-#define QM_DQAVAIL_PORTAL	0x8000		/* Portal channel */
-#define QM_DQAVAIL_POOL(n)	(0x8000 >> (n))	/* Pool channel, n==[1..15] */
-
-/* These are qm_<reg>_<verb>(). So for example, qm_disable_write() means "write
- * the disable register" rather than "disable the ability to write". */
-#define qm_isr_status_read(qm)		__qm_isr_read(qm, qm_isr_status)
-#define qm_isr_status_clear(qm, m)	__qm_isr_write(qm, qm_isr_status, m)
-#define qm_isr_enable_read(qm)		__qm_isr_read(qm, qm_isr_enable)
-#define qm_isr_enable_write(qm, v)	__qm_isr_write(qm, qm_isr_enable, v)
-#define qm_isr_disable_read(qm)		__qm_isr_read(qm, qm_isr_disable)
-#define qm_isr_disable_write(qm, v)	__qm_isr_write(qm, qm_isr_disable, v)
-/* TODO: unfortunate name-clash here, reword? */
-#define qm_isr_inhibit(qm)		__qm_isr_write(qm, qm_isr_inhibit, 1)
-#define qm_isr_uninhibit(qm)		__qm_isr_write(qm, qm_isr_inhibit, 0)
-
-/* Don't use these, use the wrappers above*/
-u32 __qm_isr_read(struct qm_portal *portal, enum qm_isr_reg n);
-void __qm_isr_write(struct qm_portal *portal, enum qm_isr_reg n, u32 val);
 
 
 /* ------------------------------------------------------- */
 /* --- Qman data structures (and associated constants) --- */
 
+/* Represents s/w corenet portal mapped data structures */
+struct qm_eqcr_entry;	/* EQCR (EnQueue Command Ring) entries */
+struct qm_dqrr_entry;	/* DQRR (DeQueue Response Ring) entries */
+struct qm_mr_entry;	/* MR (Message Ring) entries */
+struct qm_mc_command;	/* MC (Management Command) command */
+struct qm_mc_result;	/* MC result */
+
 /* See David Lapp's "Frame formats" document, "dpateam", Jan 07, 2008 */
 #define QM_FD_FORMAT_SG		0x4
 #define QM_FD_FORMAT_LONG	0x2
@@ -562,11 +154,11 @@ enum qm_fd_format {
 };
 
 /* Capitalised versions are un-typed but can be used in static expressions */
-#define QM_FD_CONTIG   0
+#define QM_FD_CONTIG	0
 #define QM_FD_CONTIG_BIG QM_FD_FORMAT_LONG
-#define QM_FD_SG       QM_FD_FORMAT_SG
-#define QM_FD_SG_BIG   (QM_FD_FORMAT_SG | QM_FD_FORMAT_LONG)
-#define QM_FD_COMPOUND QM_FD_FORMAT_COMPOUND
+#define QM_FD_SG	QM_FD_FORMAT_SG
+#define QM_FD_SG_BIG	(QM_FD_FORMAT_SG | QM_FD_FORMAT_LONG)
+#define QM_FD_COMPOUND	QM_FD_FORMAT_COMPOUND
 
 /* See 1.5.1.1: "Frame Descriptor (FD)" */
 struct qm_fd {
@@ -583,15 +175,15 @@ struct qm_fd {
 	 * static initialisation under gcc, in which case use the "opaque" form
 	 * with one of the macros. */
 	union {
+		/* For easier/faster copying of this part of the fd (eg. from a
+		 * DQRR entry to an EQCR entry) copy 'opaque' */
+		u32 opaque;
 		/* If 'format' is _contig or _sg, 20b length and 9b offset */
 		struct {
 			enum qm_fd_format format:3;
 			u16 offset:9;
 			u32 length20:20;
 		} __packed;
-		/* For easier/faster copying of this part of the fd (eg. from a
-		 * DQRR entry to an EQCR entry) copy 'opaque' */
-		u32 opaque;
 		/* If 'format' is _contig_big or _sg_big, 29b length */
 		struct {
 			enum qm_fd_format _format1:3;
@@ -607,9 +199,29 @@ struct qm_fd {
 		u32 cmd;
 		u32 status;
 	};
-} __packed;
+} __packed __attribute__((aligned(4)));
 #define QM_FD_DD_NULL		0x00
 #define QM_FD_PID_MASK		0x3f
+static inline u64 qm_fd_addr_get64(const struct qm_fd *fd)
+{
+	return ((u64)fd->addr_hi << 32) | (u64)fd->addr_lo;
+}
+
+static inline dma_addr_t qm_fd_addr(const struct qm_fd *fd)
+{
+#ifdef CONFIG_PHYS_ADDR_T_64BIT
+	return ((dma_addr_t)fd->addr_hi << 32) | (dma_addr_t)fd->addr_lo;
+#else
+	return (dma_addr_t)fd->addr_lo;
+#endif
+}
+/* Macro, so we compile better if 'v' isn't always 64-bit */
+#define qm_fd_addr_set64(fd, v) \
+	do { \
+		struct qm_fd *__fd931 = (fd); \
+		__fd931->addr_hi = upper_32_bits(v); \
+		__fd931->addr_lo = lower_32_bits(v); \
+	} while (0)
 
 /* For static initialisation of FDs (which is complicated by the use of unions
  * in "struct qm_fd"), use the following macros. Note that;
@@ -639,6 +251,17 @@ struct qm_sg_entry {
 	u16 __reserved3:3;
 	u16 offset:13;
 } __packed;
+static inline u64 qm_sg_entry_get64(const struct qm_sg_entry *sg)
+{
+	return ((u64)sg->addr_hi << 32) | (u64)sg->addr_lo;
+}
+/* Macro, so we compile better if 'v' isn't always 64-bit */
+#define qm_sg_entry_set64(sg, v) \
+	do { \
+		struct qm_sg_entry *__sg931 = (sg); \
+		__sg931->addr_hi = upper_32_bits(v); \
+		__sg931->addr_lo = lower_32_bits(v); \
+	} while (0)
 
 /* See 1.5.8.1: "Enqueue Command" */
 struct qm_eqcr_entry {
@@ -752,10 +375,11 @@ struct qm_mr_entry {
 #define QM_MR_DCERN_COLOUR_RED		0x02
 #define QM_MR_DCERN_COLOUR_OVERRIDE	0x03
 
-/* This identical structure of FQD fields is present in the "Init FQ" command
- * and the "Query FQ" result. It's suctioned out here into its own struct. It's
-+ * also used as the qman_query_fq() result structure in the high-level API. NB,
-+ * qm_fqd_stashing is predeclared because of C++ finickyness. */
+/* An identical structure of FQD fields is present in the "Init FQ" command and
+ * the "Query FQ" result, it's suctioned out into the "struct qm_fqd" type.
+ * Within that, the 'stashing' and 'taildrop' pieces are also factored out, the
+ * latter has two inlines to assist with converting to/from the mant+exp
+ * representation. */
 struct qm_fqd_stashing {
 	/* See QM_STASHING_EXCL_<...> */
 	u8 exclusive;
@@ -765,6 +389,11 @@ struct qm_fqd_stashing {
 	u8 data_cl:2;
 	u8 context_cl:2;
 } __packed;
+struct qm_fqd_taildrop {
+	u16 __reserved1:3;
+	u16 mant:8;
+	u16 exp:5;
+} __packed;
 struct qm_fqd {
 	union {
 		u8 orpc;
@@ -786,14 +415,11 @@ struct qm_fqd {
 	};
 	u16 __reserved2:1;
 	u16 ics_cred:15;
-	struct {
-		u16 __reserved1:3;
-		u16 mant:8;
-		u16 exp:5;
-	} __packed td;
+	struct qm_fqd_taildrop td;
 	u32 context_b;
 	union {
 		/* Treat it as 64-bit opaque */
+		u64 opaque;
 		struct {
 			u32 hi;
 			u32 lo;
@@ -809,6 +435,43 @@ struct qm_fqd {
 		} __packed;
 	} context_a;
 } __packed;
+/* 64-bit converters for context_hi/lo */
+static inline u64 qm_fqd_stashing_get64(const struct qm_fqd *fqd)
+{
+	return ((u64)fqd->context_a.context_hi << 32) |
+		(u64)fqd->context_a.context_lo;
+}
+/* Macro, so we compile better when 'v' isn't necessarily 64-bit */
+#define qm_fqd_stashing_set64(fqd, v) \
+	do { \
+		struct qm_fqd *__fqd931 = (fqd); \
+		__fqd931->context_a.context_hi = upper_32_bits(v); \
+		__fqd931->context_a.context_lo = lower_32_bits(v); \
+	} while (0)
+/* convert a threshold value into mant+exp representation */
+static inline int qm_fqd_taildrop_set(struct qm_fqd_taildrop *td, u32 val,
+					int roundup)
+{
+	u32 e = 0;
+	int oddbit = 0;
+	if (val > 0xe0000000)
+		return -ERANGE;
+	while (val > 0xff) {
+		oddbit = val & 1;
+		val >>= 1;
+		e++;
+		if (roundup && oddbit)
+			val++;
+	}
+	td->exp = e;
+	td->mant = val;
+	return 0;
+}
+/* and the other direction */
+static inline u32 qm_fqd_taildrop_get(const struct qm_fqd_taildrop *td)
+{
+	return (u32)td->mant << td->exp;
+}
 
 /* See 1.5.2.2: "Frame Queue Descriptor (FQD)" */
 /* Frame Queue Descriptor (FQD) field 'fq_ctrl' uses these constants */
@@ -821,7 +484,7 @@ struct qm_fqd {
 #define QM_FQCTRL_FORCESFDR	0x0008	/* High-priority SFDRs */
 #define QM_FQCTRL_AVOIDBLOCK	0x0004	/* Don't block active */
 #define QM_FQCTRL_HOLDACTIVE	0x0002	/* Hold active in portal */
-#define QM_FQCTRL_PREFERINCACHE	0x0001  /* Aggressively cache FQD */
+#define QM_FQCTRL_PREFERINCACHE	0x0001	/* Aggressively cache FQD */
 #define QM_FQCTRL_LOCKINCACHE	QM_FQCTRL_PREFERINCACHE /* older naming */
 
 /* See 1.5.6.7.1: "FQD Context_A field used for [...] */
@@ -877,6 +540,7 @@ struct __qm_mc_cgr {
 	u8 cstd_en;	/* boolean, use QM_CGR_EN */
 	u8 cs;		/* boolean, only used in query response */
 	struct qm_cgr_cs_thres cs_thres;
+	u8 mode;	/* QMAN_CGR_MODE_FRAME not supported in rev1.0 */
 } __packed;
 #define QM_CGR_EN		0x01 /* For wr_en_*, cscn_en, cstd_en */
 #define QM_CGR_TARG_PORTAL(n)	(0x80000000 >> (n)) /* s/w portal, 0-9 */
@@ -888,8 +552,9 @@ struct __qm_mc_cgr {
 /* See 1.5.8.5.3: "Query FQ Non-Programmable Fields" */
 /* See 1.5.8.5.4: "Alter FQ State Commands " */
 /* See 1.5.8.6.1: "Initialize/Modify CGR" */
-/* See 1.5.8.6.2: "Query CGR" */
-/* See 1.5.8.6.3: "Query Congestion Group State" */
+/* See 1.5.8.6.2: "CGR Test Write" */
+/* See 1.5.8.6.3: "Query CGR" */
+/* See 1.5.8.6.4: "Query Congestion Group State" */
 struct qm_mcc_initfq {
 	u8 __reserved1;
 	u16 we_mask;	/* Write Enable Mask */
@@ -917,10 +582,18 @@ struct qm_mcc_initcgr {
 	u8 __reserved1;
 	u16 we_mask;	/* Write Enable Mask */
 	struct __qm_mc_cgr cgr;	/* CGR fields */
-	u8 __reserved2[3];
+	u8 __reserved2[2];
 	u8 cgid;
 	u8 __reserved4[32];
 } __packed;
+struct qm_mcc_cgrtestwrite {
+	u8 __reserved1[2];
+	u8 i_bcnt_hi:8;/* high 8-bits of 40-bit "Instant" */
+	u32 i_bcnt_lo;	/* low 32-bits of 40-bit */
+	u8 __reserved2[23];
+	u8 cgid;
+	u8 __reserved3[32];
+} __packed;
 struct qm_mcc_querycgr {
 	u8 __reserved1[30];
 	u8 cgid;
@@ -949,6 +622,7 @@ struct qm_mc_command {
 		struct qm_mcc_queryfq_np queryfq_np;
 		struct qm_mcc_alterfq alterfq;
 		struct qm_mcc_initcgr initcgr;
+		struct qm_mcc_cgrtestwrite cgrtestwrite;
 		struct qm_mcc_querycgr querycgr;
 		struct qm_mcc_querycongestion querycongestion;
 		struct qm_mcc_querywq querywq;
@@ -968,6 +642,7 @@ struct qm_mc_command {
 #define QM_MCC_VERB_ALTER_OOS		0x4b	/* Take FQ out of service */
 #define QM_MCC_VERB_INITCGR		0x50
 #define QM_MCC_VERB_MODIFYCGR		0x51
+#define QM_MCC_VERB_CGRTESTWRITE	0x52
 #define QM_MCC_VERB_QUERYCGR		0x58
 #define QM_MCC_VERB_QUERYCONGESTION	0x59
 /* INITFQ-specific flags */
@@ -992,14 +667,16 @@ struct qm_mc_command {
 #define QM_CGR_WE_CSCN_TARG		0x0008
 #define QM_CGR_WE_CSTD_EN		0x0004
 #define QM_CGR_WE_CS_THRES		0x0002
+#define QM_CGR_WE_MODE			0x0001
 
 /* See 1.5.8.5.1: "Initialize FQ" */
 /* See 1.5.8.5.2: "Query FQ" */
 /* See 1.5.8.5.3: "Query FQ Non-Programmable Fields" */
 /* See 1.5.8.5.4: "Alter FQ State Commands " */
 /* See 1.5.8.6.1: "Initialize/Modify CGR" */
-/* See 1.5.8.6.2: "Query CGR" */
-/* See 1.5.8.6.3: "Query Congestion Group State" */
+/* See 1.5.8.6.2: "CGR Test Write" */
+/* See 1.5.8.6.3: "Query CGR" */
+/* See 1.5.8.6.4: "Query Congestion Group State" */
 struct qm_mcr_initfq {
 	u8 __reserved1[62];
 } __packed;
@@ -1051,19 +728,65 @@ struct qm_mcr_alterfq {
 struct qm_mcr_initcgr {
 	u8 __reserved1[62];
 } __packed;
+struct qm_mcr_cgrtestwrite {
+	u16 __reserved1;
+	struct __qm_mc_cgr cgr; /* CGR fields */
+	u8 __reserved2[3];
+	u32 __reserved3:24;
+	u32 i_bcnt_hi:8;/* high 8-bits of 40-bit "Instant" */
+	u32 i_bcnt_lo;	/* low 32-bits of 40-bit */
+	u32 __reserved4:24;
+	u32 a_bcnt_hi:8;/* high 8-bits of 40-bit "Average" */
+	u32 a_bcnt_lo;	/* low 32-bits of 40-bit */
+	u16 lgt;	/* Last Group Tick */
+	u16 wr_prob_g;
+	u16 wr_prob_y;
+	u16 wr_prob_r;
+	u8 __reserved5[8];
+} __packed;
 struct qm_mcr_querycgr {
 	u16 __reserved1;
 	struct __qm_mc_cgr cgr; /* CGR fields */
-	u32 __reserved2;
+	u8 __reserved2[3];
 	u32 __reserved3:24;
 	u32 i_bcnt_hi:8;/* high 8-bits of 40-bit "Instant" */
 	u32 i_bcnt_lo;	/* low 32-bits of 40-bit */
 	u32 __reserved4:24;
 	u32 a_bcnt_hi:8;/* high 8-bits of 40-bit "Average" */
 	u32 a_bcnt_lo;	/* low 32-bits of 40-bit */
-	u32 lgt;	/* Last Group Tick */
-	u8 __reserved5[12];
+	u8 __reserved5[16];
 } __packed;
+static inline u64 qm_mcr_querycgr_i_get64(const struct qm_mcr_querycgr *q)
+{
+	return ((u64)q->i_bcnt_hi << 32) | (u64)q->i_bcnt_lo;
+}
+static inline u64 qm_mcr_querycgr_a_get64(const struct qm_mcr_querycgr *q)
+{
+	return ((u64)q->a_bcnt_hi << 32) | (u64)q->a_bcnt_lo;
+}
+static inline u64 qm_mcr_cgrtestwrite_i_get64(
+					const struct qm_mcr_cgrtestwrite *q)
+{
+	return ((u64)q->i_bcnt_hi << 32) | (u64)q->i_bcnt_lo;
+}
+static inline u64 qm_mcr_cgrtestwrite_a_get64(
+					const struct qm_mcr_cgrtestwrite *q)
+{
+	return ((u64)q->a_bcnt_hi << 32) | (u64)q->a_bcnt_lo;
+}
+/* Macro, so we compile better if 'v' isn't always 64-bit */
+#define qm_mcr_querycgr_i_set64(q, v) \
+	do { \
+		struct qm_mcr_querycgr *__q931 = (fd); \
+		__q931->i_bcnt_hi = upper_32_bits(v); \
+		__q931->i_bcnt_lo = lower_32_bits(v); \
+	} while (0)
+#define qm_mcr_querycgr_a_set64(q, v) \
+	do { \
+		struct qm_mcr_querycgr *__q931 = (fd); \
+		__q931->a_bcnt_hi = upper_32_bits(v); \
+		__q931->a_bcnt_lo = lower_32_bits(v); \
+	} while (0)
 struct __qm_mcr_querycongestion {
 	u32 __state[8];
 };
@@ -1092,6 +815,7 @@ struct qm_mc_result {
 		struct qm_mcr_queryfq_np queryfq_np;
 		struct qm_mcr_alterfq alterfq;
 		struct qm_mcr_initcgr initcgr;
+		struct qm_mcr_cgrtestwrite cgrtestwrite;
 		struct qm_mcr_querycgr querycgr;
 		struct qm_mcr_querycongestion querycongestion;
 		struct qm_mcr_querywq querywq;
@@ -1184,6 +908,10 @@ static inline void qman_cgrs_init(struct qman_cgrs *c)
 {
 	memset(c, 0, sizeof(*c));
 }
+static inline void qman_cgrs_fill(struct qman_cgrs *c)
+{
+	memset(c, 0xff, sizeof(*c));
+}
 static inline int qman_cgrs_get(struct qman_cgrs *c, int num)
 {
 	return QM_MCR_QUERYCONGESTION(&c->q, num);
@@ -1196,6 +924,39 @@ static inline void qman_cgrs_unset(struct qman_cgrs *c, int num)
 {
 	c->q.__state[__CGR_WORD(num)] &= ~(0x80000000 >> __CGR_SHIFT(num));
 }
+static inline int qman_cgrs_next(struct qman_cgrs *c, int num)
+{
+	while ((++num < 256) && !qman_cgrs_get(c, num))
+		;
+	return num;
+}
+static inline void qman_cgrs_cp(struct qman_cgrs *dest,
+			const struct qman_cgrs *src)
+{
+	memcpy(dest, src, sizeof(*dest));
+}
+static inline void qman_cgrs_and(struct qman_cgrs *dest,
+			const struct qman_cgrs *a, const struct qman_cgrs *b)
+{
+	int ret;
+	u32 *_d = dest->q.__state;
+	const u32 *_a = a->q.__state;
+	const u32 *_b = b->q.__state;
+	for (ret = 0; ret < 8; ret++)
+		*(_d++) = *(_a++) & *(_b++);
+}
+static inline void qman_cgrs_xor(struct qman_cgrs *dest,
+			const struct qman_cgrs *a, const struct qman_cgrs *b)
+{
+	int ret;
+	u32 *_d = dest->q.__state;
+	const u32 *_a = a->q.__state;
+	const u32 *_b = b->q.__state;
+	for (ret = 0; ret < 8; ret++)
+		*(_d++) = *(_a++) ^ *(_b++);
+}
+#define qman_cgrs_for_each_1(cgr, cgrs) \
+	for ((cgr) = -1; (cgr) = qman_cgrs_next((cgrs), (cgr)), (cgr) < 256; )
 
 	/* Portal and Frame Queues */
 	/* ----------------------- */
@@ -1207,6 +968,10 @@ struct qman_portal;
  * defined further down. */
 struct qman_fq;
 
+/* This object type represents a Qman congestion group, it is defined further
+ * down. */
+struct qman_cgr;
+
 /* This enum, and the callback type that returns it, are used when handling
  * dequeued frames via DQRR. Note that for "null" callbacks registered with the
  * portal object (for handling dequeues that do not demux because contextB is
@@ -1219,7 +984,15 @@ enum qman_cb_dqrr_result {
 	/* Does not consume, for DCA mode only. This allows out-of-order
 	 * consumes by explicit calls to qman_dca() and/or the use of implicit
 	 * DCA via EQCR entries. */
-	qman_cb_dqrr_defer
+	qman_cb_dqrr_defer,
+	/* Stop processing without consuming this ring entry. Exits the current
+	 * qman_poll_dqrr() or interrupt-handling, as appropriate. If within an
+	 * interrupt handler, the callback would typically call
+	 * qman_irqsource_remove(QM_PIRQ_DQRI) before returning this value,
+	 * otherwise the interrupt will reassert immediately. */
+	qman_cb_dqrr_stop,
+	/* Like qman_cb_dqrr_stop, but consumes the current entry. */
+	qman_cb_dqrr_consume_stop
 };
 typedef enum qman_cb_dqrr_result (*qman_cb_dqrr)(struct qman_portal *qm,
 					struct qman_fq *fq,
@@ -1279,6 +1052,7 @@ struct qman_fq_cb {
 	qman_cb_mr dc_ern;      /* for diverted h/w ERNs */
 	qman_cb_mr fqs;         /* frame-queue state changes*/
 };
+
 struct qman_fq {
 	/* Caller of qman_create_fq() provides these demux callbacks */
 	struct qman_fq_cb cb;
@@ -1293,12 +1067,26 @@ struct qman_fq {
 	struct rb_node node;
 };
 
+/* This callback type is used when handling congestion group entry/exit.
+ * 'congested' is non-zero on congestion-entry, and zero on congestion-exit. */
+typedef void (*qman_cb_cgr)(struct qman_portal *qm,
+			struct qman_cgr *cgr, int congested);
+
+struct qman_cgr {
+	/* Set these prior to qman_create_cgr() */
+	u32 cgrid; /* 0..255, but u32 to allow specials like -1, 256, etc.*/
+	qman_cb_cgr cb;
+	/* These are private to the driver */
+	enum qm_channel chan; /* portal channel this object is created on */
+	struct list_head node;
+};
+
 /* Flags to qman_create_fq() */
 #define QMAN_FQ_FLAG_NO_ENQUEUE      0x00000001 /* can't enqueue */
 #define QMAN_FQ_FLAG_NO_MODIFY       0x00000002 /* can only enqueue */
 #define QMAN_FQ_FLAG_TO_DCPORTAL     0x00000004 /* consumed by CAAM/PME/Fman */
 #define QMAN_FQ_FLAG_LOCKED          0x00000008 /* multi-core locking */
-#define QMAN_FQ_FLAG_RECOVER         0x00000010 /* recovery mode */
+#define QMAN_FQ_FLAG_AS_IS           0x00000010 /* query h/w state */
 #define QMAN_FQ_FLAG_DYNAMIC_FQID    0x00000020 /* (de)allocate fqid */
 
 /* Flags to qman_destroy_fq() */
@@ -1310,6 +1098,7 @@ struct qman_fq {
 #define QMAN_FQ_STATE_ORL            0x20000000 /* retired FQ has ORL */
 #define QMAN_FQ_STATE_BLOCKOOS       0xe0000000 /* if any are set, no OOS */
 #define QMAN_FQ_STATE_CGR_EN         0x10000000 /* CGR enabled */
+#define QMAN_FQ_STATE_VDQCR          0x08000000 /* being volatile dequeued */
 
 /* Flags to qman_init_fq() */
 #define QMAN_INITFQ_FLAG_SCHED       0x00000001 /* schedule rather than park */
@@ -1326,9 +1115,8 @@ struct qman_fq {
  * any change here should be audited in PME.) */
 #define QMAN_ENQUEUE_FLAG_WAIT       0x00010000 /* wait if EQCR is full */
 #define QMAN_ENQUEUE_FLAG_WAIT_INT   0x00020000 /* if wait, interruptible? */
-#define QMAN_ENQUEUE_FLAG_WAIT_SYNC  0x00040000 /* if wait, until consumed? */
+#define QMAN_ENQUEUE_FLAG_WAIT_SYNC  0x00000004 /* if wait, until consumed? */
 #define QMAN_ENQUEUE_FLAG_WATCH_CGR  0x00080000 /* watch congestion state */
-#define QMAN_ENQUEUE_FLAG_INTERRUPT  0x00000004 /* on command consumption */
 #define QMAN_ENQUEUE_FLAG_DCA        0x00008000 /* perform enqueue-DCA */
 #define QMAN_ENQUEUE_FLAG_DCA_PARK   0x00004000 /* If DCA, requests park */
 #define QMAN_ENQUEUE_FLAG_DCA_PTR(p)		/* If DCA, p is DQRR entry */ \
@@ -1348,8 +1136,13 @@ struct qman_fq {
  *   number. */
 #define QMAN_ENQUEUE_FLAG_NESN       0x04000000
 
+/* Flags to qman_modify_cgr() */
+#define QMAN_CGR_FLAG_USE_INIT       0x00000001
+#define QMAN_CGR_MODE_FRAME          0x00000001
+
 	/* Portal Management */
 	/* ----------------- */
+#ifdef CONFIG_FSL_QMAN_NULL_FQ_DEMUX
 /**
  * qman_get_null_cb - get callbacks currently used for "null" frame queues
  *
@@ -1364,42 +1157,97 @@ void qman_get_null_cb(struct qman_fq_cb *null_cb);
  * a DQRR or MR entry refers to a "null" FQ object. (Eg. zero-conf messaging.)
  */
 void qman_set_null_cb(const struct qman_fq_cb *null_cb);
+#endif
 
 /**
- * qman_poll - Runs portal updates not triggered by interrupts
+ * qman_irqsource_get - return the portal work that is interrupt-driven
+ *
+ * Returns a bitmask of QM_PIRQ_**I processing sources that are currently
+ * enabled for interrupt handling on the current cpu's affine portal. These
+ * sources will trigger the portal interrupt and the interrupt handler (or a
+ * tasklet/bottom-half it defers to) will perform the corresponding processing
+ * work. The qman_poll_***() functions will only process sources that are not in
+ * this bitmask.
+ */
+u32 qman_irqsource_get(void);
+
+/**
+ * qman_irqsource_add - add processing sources to be interrupt-driven
+ * @bits: bitmask of QM_PIRQ_**I processing sources
+ *
+ * Adds processing sources that should be interrupt-driven (rather than
+ * processed via qman_poll_***() functions).
+ */
+void qman_irqsource_add(u32 bits);
+
+/**
+ * qman_irqsource_remove - remove processing sources from being interrupt-driven
+ * @bits: bitmask of QM_PIRQ_**I processing sources
+ *
+ * Removes processing sources from being interrupt-driven, so that they will
+ * instead be processed via qman_poll_***() functions.
+ */
+void qman_irqsource_remove(u32 bits);
+
+/**
+ * qman_affine_cpus - return a mask of cpus that have affine portals
+ */
+const cpumask_t *qman_affine_cpus(void);
+
+/**
+ * qman_poll_dqrr - process DQRR (fast-path) entries
+ * @limit: the maximum number of DQRR entries to process
+ *
+ * Use of this function requires that DQRR processing not be interrupt-driven.
+ * Ie. the value returned by qman_irqsource_get() should not include
+ * QM_PIRQ_DQRI.
+ */
+unsigned int qman_poll_dqrr(unsigned int limit);
+
+/**
+ * qman_poll_slow - process anything (except DQRR) that isn't interrupt-driven.
+ *
+ * This function does any portal processing that isn't interrupt-driven.
+ */
+void qman_poll_slow(void);
+
+/**
+ * qman_poll - legacey wrapper for qman_poll_dqrr() and qman_poll_slow()
  *
  * Dispatcher logic on a cpu can use this to trigger any maintenance of the
  * affine portal. There are two classes of portal processing in question;
  * fast-path (which involves demuxing dequeue ring (DQRR) entries and tracking
  * enqueue ring (EQCR) consumption), and slow-path (which involves EQCR
- * thresholds, congestion state changes, etc). The driver is configured to use
- * interrupts for either (a) all processing, (b) only slow-path processing, or
- * (c) no processing. This function does whatever processing is not triggered by
- * interrupts.
+ * thresholds, congestion state changes, etc). This function does whatever
+ * processing is not triggered by interrupts.
+ *
+ * Note, if DQRR and some slow-path processing are poll-driven (rather than
+ * interrupt-driven) then this function uses a heuristic to determine how often
+ * to run slow-path processing - as slow-path processing introduces at least a
+ * minimum latency each time it is run, whereas fast-path (DQRR) processing is
+ * close to zero-cost if there is no work to be done. Applications can tune this
+ * behaviour themselves by using qman_poll_dqrr() and qman_poll_slow() directly
+ * rather than going via this wrapper.
  */
-#ifdef CONFIG_FSL_QMAN_HAVE_POLL
 void qman_poll(void);
-#else
-#define qman_poll()	do { ; } while (0)
-#endif
 
 /**
- * qman_disable_portal - Cease processing DQRR and MR for a s/w portal
+ * qman_stop_dequeues - Stop h/w dequeuing to the s/w portal
  *
- * Disables DQRR and MR processing of the portal. Portal disabling is
- * reference-counted, so qman_enable_portal() must be called as many times as
- * qman_disable_portal() to truly re-enable the portal.
+ * Disables DQRR processing of the portal. This is reference-counted, so
+ * qman_start_dequeues() must be called as many times as qman_stop_dequeues() to
+ * truly re-enable dequeuing.
  */
-void qman_disable_portal(void);
+void qman_stop_dequeues(void);
 
 /**
- * qman_enable_portal - Commence processing DQRR and MR for a s/w portal
+ * qman_start_dequeues - (Re)start h/w dequeuing to the s/w portal
  *
- * Enables DQRR and MR processing of the portal. Portal disabling is
- * reference-counted, so qman_enable_portal() must be called as many times as
- * qman_disable_portal() to truly re-enable the portal.
+ * Enables DQRR processing of the portal. This is reference-counted, so
+ * qman_start_dequeues() must be called as many times as qman_stop_dequeues() to
+ * truly re-enable dequeuing.
  */
-void qman_enable_portal(void);
+void qman_start_dequeues(void);
 
 /**
  * qman_static_dequeue_add - Add pool channels to the portal SDQCR
@@ -1432,7 +1280,6 @@ u32 qman_static_dequeue_get(void);
 
 /**
  * qman_dca - Perform a Discrete Consumption Acknowledgement
- * @p: the managed portal whose DQRR is targeted (and is in DCA mode)
  * @dq: the DQRR entry to be consumed
  * @park_request: indicates whether the held-active @fq should be parked
  *
@@ -1445,6 +1292,19 @@ u32 qman_static_dequeue_get(void);
  */
 void qman_dca(struct qm_dqrr_entry *dq, int park_request);
 
+/**
+ * qman_eqcr_is_empty - Determine if portal's EQCR is empty
+ *
+ * For use in situations where a cpu-affine caller needs to determine when all
+ * enqueues for the local portal have been processed by Qman but can't use the
+ * QMAN_ENQUEUE_FLAG_WAIT_SYNC flag to do this from the final qman_enqueue().
+ * The function forces tracking of EQCR consumption (which normally doesn't
+ * happen until enqueue processing needs to find space to put new enqueue
+ * commands), and returns zero if the ring still has unprocessed entries,
+ * non-zero if it is empty.
+ */
+int qman_eqcr_is_empty(void);
+
 	/* FQ management */
 	/* ------------- */
 /**
@@ -1465,10 +1325,10 @@ void qman_dca(struct qm_dqrr_entry *dq, int park_request);
  * qm_init_fq() API, as this indicates the frame queue will be consumed by a
  * direct-connect portal (PME, CAAM, or Fman). When frame queues are consumed by
  * software portals, the contextB field is controlled by the driver and can't be
- * modified by the caller. If the RECOVERY flag is specified, management
- * commands will be used on portal @p to query state for frame queue @fqid and
- * construct a frame queue object based on that, rather than assuming/requiring
- * that it be Out of Service.
+ * modified by the caller. If the AS_IS flag is specified, management commands
+ * will be used on portal @p to query state for frame queue @fqid and construct
+ * a frame queue object based on that, rather than assuming/requiring that it be
+ * Out of Service.
  */
 int qman_create_fq(u32 fqid, u32 flags, struct qman_fq *fq);
 
@@ -1597,6 +1457,16 @@ int qman_query_fq(struct qman_fq *fq, struct qm_fqd *fqd);
 int qman_query_fq_np(struct qman_fq *fq, struct qm_mcr_queryfq_np *np);
 
 /**
+ * qman_query_wq - Queries work queue lengths
+ * @query_dedicated: If non-zero, query length of WQs in the channel dedicated
+ *		to this software portal. Otherwise, query length of WQs in a
+ *		channel  specified in wq.
+ * @wq: storage for the queried WQs lengths. Also specified the channel to
+ *	to query if query_dedicated is zero.
+ */
+int qman_query_wq(u8 query_dedicated, struct qm_mcr_querywq *wq);
+
+/**
  * qman_volatile_dequeue - Issue a volatile dequeue command
  * @fq: the frame queue object to dequeue from (or NULL)
  * @flags: a bit-mask of QMAN_VOLATILE_FLAG_*** options
@@ -1607,10 +1477,14 @@ int qman_query_fq_np(struct qman_fq *fq, struct qm_mcr_queryfq_np *np);
  * the VDQCR is already in use, otherwise returns non-zero for failure. If
  * QMAN_VOLATILE_FLAG_FINISH is specified, the function will only return once
  * the VDQCR command has finished executing (ie. once the callback for the last
- * DQRR entry resulting from the VDQCR command has been called). If @fq is
- * non-NULL, the corresponding FQID will be substituted in to the VDQCR command,
- * otherwise it is assumed that @vdqcr already contains the FQID to dequeue
- * from.
+ * DQRR entry resulting from the VDQCR command has been called). If not using
+ * the FINISH flag, completion can be determined either by detecting the
+ * presence of the QM_DQRR_STAT_UNSCHEDULED and QM_DQRR_STAT_DQCR_EXPIRED bits
+ * in the "stat" field of the "struct qm_dqrr_entry" passed to the FQ's dequeue
+ * callback, or by waiting for the QMAN_FQ_STATE_VDQCR bit to disappear from the
+ * "flags" retrieved from qman_fq_state(). If @fq is non-NULL, the corresponding
+ * FQID will be substituted in to the VDQCR command, otherwise it is assumed
+ * that @vdqcr already contains the FQID to dequeue from.
  */
 int qman_volatile_dequeue(struct qman_fq *fq, u32 flags, u32 vdqcr);
 
@@ -1720,6 +1594,64 @@ static inline void qman_release_fqid(u32 fqid)
 	qman_release_fqid_range(fqid, 1);
 }
 
+	/* CGR management */
+	/* -------------- */
+/**
+ * qman_create_cgr - Register a congestion group object
+ * @cgr: the 'cgr' object, with fields filled in
+ * @flags: QMAN_CGR_FLAG_* values
+ * @opts: optional state of CGR settings
+ *
+ * Registers this object to receiving congestion entry/exit callbacks on the
+ * portal affine to the cpu portal on which this API is executed. If opts is
+ * NULL then only the callback (cgr->cb) function is registered. If @flags
+ * contains QMAN_CGR_FLAG_USE_INIT, then an init hw command (which will reset
+ * any unspecified parameters) will be used rather than a modify hw hardware
+ * (which only modifies the specified parameters).
+ */
+int qman_create_cgr(struct qman_cgr *cgr, u32 flags,
+			struct qm_mcc_initcgr *opts);
+
+/**
+ * qman_delete_cgr - Deregisters a congestion group object
+ * @cgr: the 'cgr' object to deregister
+ *
+ * "Unplugs" this CGR object from the portal affine to the cpu on which this API
+ * is executed. This must be excuted on the same affine portal on which it was
+ * created.
+ */
+int qman_delete_cgr(struct qman_cgr *cgr);
+
+/**
+ * qman_modify_cgr - Modify CGR fields
+ * @cgr: the 'cgr' object to modify
+ * @flags: QMAN_CGR_FLAG_* values
+ * @opts: the CGR-modification settings
+ *
+ * The @opts parameter comes from the low-level portal API, and can be NULL.
+ * Note that some fields and options within @opts may be ignored or overwritten
+ * by the driver, in particular the 'cgrid' field is ignored (this operation
+ * only affects the given CGR object). If @flags contains
+ * QMAN_CGR_FLAG_USE_INIT, then an init hw command (which will reset any
+ * unspecified parameters) will be used rather than a modify hw hardware (which
+ * only modifies the specified parameters).
+ */
+int qman_modify_cgr(struct qman_cgr *cgr, u32 flags,
+			struct qm_mcc_initcgr *opts);
+
+/**
+* qman_query_cgr - Queries CGR fields
+* @cgr: the 'cgr' object to query
+* @result: storage for the queried congestion group record
+*/
+int qman_query_cgr(struct qman_cgr *cgr, struct qm_mcr_querycgr *result);
+
+/**
+ * qman_query_congestion - Queries the state of all congestion groups
+ * @congestion: storage for the queried state of all congestion groups
+ */
+int qman_query_congestion(struct qm_mcr_querycongestion *congestion);
+
 	/* Helpers */
 	/* ------- */
 /**
-- 
1.7.0.4

