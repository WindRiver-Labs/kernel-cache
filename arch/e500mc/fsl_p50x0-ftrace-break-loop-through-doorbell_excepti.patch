From d3dc4f5cd3b6eedc3ad6a6790e437855be9b118f Mon Sep 17 00:00:00 2001
From: Yongli He <yongli.he@windriver.com>
Date: Wed, 7 Sep 2011 13:35:18 +0800
Subject: [PATCH] fsl_p50x0:ftrace: break loop through doorbell_exception

raw_local_irq_restore checks if there is pending doorbell exception.
this check makes a dead loop in the ftrace call chain as follows:

	raw_local_irq_restore (en=1) at arch/powerpc/kernel/irq.c:161
	function_trace_call (ip=, parent_ip=32) at kernel/trace/trace_functions.c:105
	ftrace_test_stop_func (ip=, parent_ip=32) at kernel/trace/ftrace.c:146
	unrecov_restore () at arch/powerpc/kernel/entry_64.S:963
	add_preempt_count (val=) at kernel/sched.c:3834
	raw_local_irq_restore (en=1) at arch/powerpc/kernel/irq.c:161

'notrace' is being added to prevent the loop.

Signed-off-by: Yongli He <yongli.he@windriver.com>
---
 arch/powerpc/kernel/dbell.c |    4 ++++
 kernel/sched.c              |   12 ++++++++++++
 2 files changed, 16 insertions(+), 0 deletions(-)

diff --git a/arch/powerpc/kernel/dbell.c b/arch/powerpc/kernel/dbell.c
index 3360392..daeea6b 100644
--- a/arch/powerpc/kernel/dbell.c
+++ b/arch/powerpc/kernel/dbell.c
@@ -63,7 +63,11 @@ void doorbell_message_pass(int target, int msg)
 }
 
 extern void (*crash_ipi_function_ptr)(struct pt_regs *);
+#ifdef CONFIG_PPC64
+notrace void doorbell_exception(struct pt_regs *regs)
+#else
 void doorbell_exception(struct pt_regs *regs)
+#endif
 {
 	struct pt_regs *old_regs = set_irq_regs(regs);
 	struct doorbell_cpu_info *info = &__get_cpu_var(doorbell_cpu_info);
diff --git a/kernel/sched.c b/kernel/sched.c
index c00f1ed..402705d 100644
--- a/kernel/sched.c
+++ b/kernel/sched.c
@@ -3512,7 +3512,11 @@ notrace unsigned long get_parent_ip(unsigned long addr)
 #if defined(CONFIG_PREEMPT) && (defined(CONFIG_DEBUG_PREEMPT) || \
 				defined(CONFIG_PREEMPT_TRACER))
 
+#ifdef CONFIG_PPC64
+notrace void __kprobes add_preempt_count(int val)
+#else
 void __kprobes add_preempt_count(int val)
+#endif
 {
 #ifdef CONFIG_DEBUG_PREEMPT
 	/*
@@ -3534,7 +3538,11 @@ void __kprobes add_preempt_count(int val)
 }
 EXPORT_SYMBOL(add_preempt_count);
 
+#ifdef CONFIG_PPC64
+notrace void __kprobes sub_preempt_count(int val)
+#else
 void __kprobes sub_preempt_count(int val)
+#endif
 {
 #ifdef CONFIG_DEBUG_PREEMPT
 	/*
@@ -3802,7 +3810,11 @@ int mutex_spin_on_owner(struct mutex *lock, struct thread_info *owner)
  * off of preempt_enable. Kernel preemptions off return from interrupt
  * occur there and call schedule directly.
  */
+#ifdef CONFIG_PPC64
+notrace asmlinkage void __sched preempt_schedule(void)
+#else
 asmlinkage void __sched preempt_schedule(void)
+#endif
 {
 	struct thread_info *ti = current_thread_info();
 
-- 
1.7.0.2

