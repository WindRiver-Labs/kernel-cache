From 0bb039b2d2550c812fa40ce878e48be7caca43cf Mon Sep 17 00:00:00 2001
From: Geoff Thorpe <Geoff.Thorpe@freescale.com>
Date: Wed, 14 Oct 2009 16:15:50 -0400
Subject: [PATCH 051/252] qman/bman/pme2: combined optimisations and bugfixes.

This change encompasses a variety of bugfixes and improvements, drawn
from h/w testing, optimisation work that occurred in a different
branch, and some code-reviews.

* moderate the bursts of logging from self-tests.
* optimisation work, including;
  * based on analysis, inline certain fns, and force non-inlining of
    others.
  * use cache-enabled EQCR/RCR consumer index, prefetching one entry
    prior to needing it.
  * precalculate pointers (and mark them 'register') in low-level
    functions rather than re-evaluating expressions.
* pme2 control API improved to no longer have a "ctrl" state. As with
  scanning, control APIs are now async and use a caller-provided token
  with a callback. Control and scan commands can be issued/interlaced
  arbitrarily without blocking. Flow parameters are single-copied so
  stack variables can now be used for issuing async commands.
* when FQ retirement is immediate, the callback is invoked from within
  qman_retire_fq(), rather than when the FQRNI message is seen (so FQRNI
  is now dropped without any lookup or processing). This avoids the race
  where a FQ is OOS'd and then free()'d prior to the FQRNI being
  processed.
* use a per-portal RB-tree for retirement FQ lookups, rather than
  global.
* fixed pme classification of "serious" errors (0x90 and 0x91 are not
  serious).
* fix incorrect pme register offsets.
* apply multiplier to sre_rule_num pme property. The pme user space tool
  expects the sre_rule_num to be displayed with multiplier factor of
  256. (Also fixed masking value, the CNR value is lower 8 bits.)
* fix broken qman/bman self-test Kconfig settings.
* added missing sysfs interfaces.

Signed-off-by: Jeffrey Ladouceur <Jeffrey.Ladouceur@freescale.com>
Signed-off-by: Geoff Thorpe <Geoff.Thorpe@freescale.com>
[Applied FSL SDK 2.0.3 patch
"kernel-2.6.30-qman-bman-pme2-combined-optimisations-and-bugfixes.patch"]
Integrated-by: Yuri Nedel <Yuri.Nedel@windriver.com>
---
 drivers/hwalloc/Kconfig               |    7 +-
 drivers/hwalloc/bman_driver.c         |    4 +-
 drivers/hwalloc/bman_high.c           |  246 +++++++++++---------
 drivers/hwalloc/bman_low.c            |   41 +++-
 drivers/hwalloc/bman_private.h        |    8 +-
 drivers/hwalloc/bman_sys.h            |   22 ++-
 drivers/hwalloc/bman_test_high.c      |    7 -
 drivers/hwqueue/Kconfig               |   13 +
 drivers/hwqueue/qman_driver.c         |   10 +-
 drivers/hwqueue/qman_high.c           |  426 ++++++++++++++++++---------------
 drivers/hwqueue/qman_low.c            |  123 ++++++++---
 drivers/hwqueue/qman_private.h        |   15 +-
 drivers/hwqueue/qman_sys.h            |   19 ++
 drivers/hwqueue/qman_test_high.c      |    5 -
 drivers/hwqueue/qman_test_hotpotato.c |   29 ++-
 drivers/match/pme2_ctrl.c             |  100 ++++++++-
 drivers/match/pme2_db.c               |   26 ++-
 drivers/match/pme2_high.c             |  378 ++++++++++-------------------
 drivers/match/pme2_low.c              |   18 +-
 drivers/match/pme2_private.h          |    7 -
 drivers/match/pme2_regs.h             |    4 +-
 drivers/match/pme2_scan.c             |  185 +++++++++------
 drivers/match/pme2_sysfs.c            |   37 +++
 drivers/match/pme2_test_high.c        |   72 +++++--
 drivers/match/pme2_test_scan.c        |  166 +++++--------
 include/linux/fsl_bman.h              |    4 +
 include/linux/fsl_pme.h               |   64 ++++--
 include/linux/fsl_qman.h              |    4 +
 28 files changed, 1170 insertions(+), 870 deletions(-)

diff --git a/drivers/hwalloc/Kconfig b/drivers/hwalloc/Kconfig
index 7939fee..9125387 100644
--- a/drivers/hwalloc/Kconfig
+++ b/drivers/hwalloc/Kconfig
@@ -23,6 +23,11 @@ config FSL_BMAN_PORTAL
 	  Compiles support to detect and support Bman software corenet portals
 	  (as provided by the device-tree).
 
+# The current driver is interrupt-driven only (poll-driven isn't yet supported).
+config FSL_BMAN_HAVE_POLL
+	bool
+	default n
+
 config FSL_BMAN_PORTAL_DISABLEAUTO
 	bool "disable auto-initialisation of cpu-affine portals"
 	depends on FSL_BMAN_PORTAL
@@ -51,7 +56,7 @@ config FSL_BMAN_TEST
 
 config FSL_BMAN_TEST_LOW
 	bool "Bman low-level self-test"
-	depends on FSL_BMAN_TEST
+	depends on FSL_BMAN_TEST && FSL_BMAN_PORTAL_DISABLEAUTO
 	default y
 	---help---
 	  This takes an unused portal and portal and performs low-level
diff --git a/drivers/hwalloc/bman_driver.c b/drivers/hwalloc/bman_driver.c
index 2455370..389b8a1 100644
--- a/drivers/hwalloc/bman_driver.c
+++ b/drivers/hwalloc/bman_driver.c
@@ -262,9 +262,7 @@ bad_cpu_ph:
 		return 0;
 	affine_portal = per_cpu(bman_affine_portal, cfg.cpu);
 	if (!affine_portal) {
-		affine_portal = bman_create_portal(portal,
-			BMAN_PORTAL_FLAG_IRQ | BMAN_PORTAL_FLAG_IRQ_FAST,
-			&cfg.mask);
+		affine_portal = bman_create_portal(portal, &cfg.mask);
 		if (!affine_portal)
 			pr_err("Bman portal auto-initialisation failed\n");
 		else {
diff --git a/drivers/hwalloc/bman_high.c b/drivers/hwalloc/bman_high.c
index 3c0a4fe..b74ddf6 100644
--- a/drivers/hwalloc/bman_high.c
+++ b/drivers/hwalloc/bman_high.c
@@ -49,7 +49,6 @@ struct bman_portal {
 	struct bm_portal *p;
 	/* 2-element array. pools[0] is mask, pools[1] is snapshot. */
 	struct bman_depletion *pools;
-	u32 flags;	/* BMAN_PORTAL_FLAG_*** - static, caller-provided */
 	int thresh_set;
 	u32 slowpoll;	/* only used when interrupts are off */
 	wait_queue_head_t queue;
@@ -112,29 +111,34 @@ static void depletion_unlink(struct bman_pool *pool)
 	local_irq_enable();
 }
 
-static u32 __poll_portal_slow(struct bman_portal *p);
-static void __poll_portal_fast(struct bman_portal *p);
+static u32 __poll_portal_slow(struct bman_portal *p, struct bm_portal *lowp,
+				u32 is);
+static inline void __poll_portal_fast(struct bman_portal *p,
+				struct bm_portal *lowp);
 
+#ifdef CONFIG_FSL_BMAN_HAVE_IRQ
 /* Portal interrupt handler */
 static irqreturn_t portal_isr(int irq, void *ptr)
 {
 	struct bman_portal *p = ptr;
-#ifdef CONFIG_FSL_BMAN_CHECKING
-	if (unlikely(!(p->flags & BMAN_PORTAL_FLAG_IRQ))) {
-		pr_crit("Portal interrupt is supposed to be disabled!\n");
-		bm_isr_inhibit(p->p);
-		return IRQ_HANDLED;
-	}
+	struct bm_portal *lowp = p->p;
+#ifdef CONFIG_FSL_BMAN_PORTAL_FLAG_IRQ_SLOW
+	u32 clear = 0, is = bm_isr_status_read(lowp);
 #endif
 	/* Only do fast-path handling if it's required */
-	if (p->flags & BMAN_PORTAL_FLAG_IRQ_FAST)
-		__poll_portal_fast(p);
-	__poll_portal_slow(p);
+#ifdef CONFIG_FSL_BMAN_PORTAL_FLAG_IRQ_FAST
+	__poll_portal_fast(p, lowp);
+#endif
+#ifdef CONFIG_FSL_BMAN_PORTAL_FLAG_IRQ_SLOW
+	clear |= __poll_portal_slow(p, lowp, is);
+#endif
+	bm_isr_status_clear(lowp, clear);
 	return IRQ_HANDLED;
 }
+#endif
 
 struct bman_portal *bman_create_portal(struct bm_portal *__p,
-			u32 flags, const struct bman_depletion *pools)
+				const struct bman_depletion *pools)
 {
 	struct bman_portal *portal;
 	const struct bm_portal_config *config = bm_portal_config(__p);
@@ -143,7 +147,7 @@ struct bman_portal *bman_create_portal(struct bm_portal *__p,
 	portal = kmalloc(sizeof(*portal), GFP_KERNEL);
 	if (!portal)
 		return NULL;
-	if (bm_rcr_init(__p, bm_rcr_pvb, bm_rcr_cci)) {
+	if (bm_rcr_init(__p, bm_rcr_pvb, bm_rcr_cce)) {
 		pr_err("Bman RCR initialisation failed\n");
 		goto fail_rcr;
 	}
@@ -172,7 +176,6 @@ struct bman_portal *bman_create_portal(struct bm_portal *__p,
 			bpid++;
 		}
 	}
-	portal->flags = flags;
 	portal->slowpoll = 0;
 	init_waitqueue_head(&portal->queue);
 	portal->rcr_prod = portal->rcr_cons = 0;
@@ -181,35 +184,29 @@ struct bman_portal *bman_create_portal(struct bm_portal *__p,
 	bm_isr_disable_write(portal->p, 0xffffffff);
 	bm_isr_enable_write(portal->p, BM_PIRQ_RCRI | BM_PIRQ_BSCN);
 	bm_isr_status_clear(portal->p, 0xffffffff);
-	if (flags & BMAN_PORTAL_FLAG_IRQ) {
-		if (request_irq(config->irq, portal_isr, 0, "Bman portal 0", portal)) {
-			pr_err("request_irq() failed\n");
-			goto fail_irq;
-		}
-		if ((config->cpu != -1) &&
-				irq_can_set_affinity(config->irq) &&
-				irq_set_affinity(config->irq,
-				     cpumask_of(config->cpu))) {
-			pr_err("irq_set_affinity() failed\n");
-			goto fail_affinity;
-		}
-		/* Enable the bits that make sense */
-		bm_isr_uninhibit(portal->p);
-	} else
-		/* without IRQ, we can't block */
-		flags &= ~BMAN_PORTAL_FLAG_WAIT;
+#ifdef CONFIG_FSL_BMAN_HAVE_IRQ
+	if (request_irq(config->irq, portal_isr, 0, "Bman portal 0", portal)) {
+		pr_err("request_irq() failed\n");
+		goto fail_irq;
+	}
+	if ((config->cpu != -1) &&
+			irq_can_set_affinity(config->irq) &&
+			irq_set_affinity(config->irq,
+			     cpumask_of(config->cpu))) {
+		pr_err("irq_set_affinity() failed\n");
+		goto fail_affinity;
+	}
+	/* Enable the bits that make sense */
+	bm_isr_uninhibit(portal->p);
+#endif
 	/* Need RCR to be empty before continuing */
 	bm_isr_disable_write(portal->p, ~BM_PIRQ_RCRI);
-	if (!(flags & BMAN_PORTAL_FLAG_RECOVER) ||
-			!(flags & BMAN_PORTAL_FLAG_WAIT))
-		ret = bm_rcr_get_fill(portal->p);
-	else if (flags & BMAN_PORTAL_FLAG_WAIT_INT)
-		ret = wait_event_interruptible(portal->queue,
-			!bm_rcr_get_fill(portal->p));
-	else {
-		wait_event(portal->queue, !bm_rcr_get_fill(portal->p));
-		ret = 0;
-	}
+#ifdef CONFIG_FSL_BMAN_PORTAL_FLAG_RECOVER
+	wait_event(portal->queue, !bm_rcr_get_fill(portal->p));
+	ret = 0;
+#else
+	ret = bm_rcr_get_fill(portal->p);
+#endif
 	if (ret) {
 		pr_err("Bman RCR unclean, need recovery\n");
 		goto fail_rcr_empty;
@@ -218,8 +215,9 @@ struct bman_portal *bman_create_portal(struct bm_portal *__p,
 	return portal;
 fail_rcr_empty:
 fail_affinity:
-	if (flags & BMAN_PORTAL_FLAG_IRQ)
-		free_irq(config->irq, portal);
+#ifdef CONFIG_FSL_BMAN_HAVE_IRQ
+	free_irq(config->irq, portal);
+#endif
 fail_irq:
 	if (portal->pools)
 		kfree(portal->pools);
@@ -236,9 +234,10 @@ fail_rcr:
 
 void bman_destroy_portal(struct bman_portal *bm)
 {
-	bm_rcr_cci_update(bm->p);
-	if (bm->flags & BMAN_PORTAL_FLAG_IRQ)
-		free_irq(bm_portal_config(bm->p)->irq, bm);
+	bm_rcr_cce_update(bm->p);
+#ifdef CONFIG_FSL_BMAN_HAVE_IRQ
+	free_irq(bm_portal_config(bm->p)->irq, bm);
+#endif
 	if (bm->pools)
 		kfree(bm->pools);
 	bm_isr_finish(bm->p);
@@ -252,11 +251,11 @@ void bman_destroy_portal(struct bman_portal *bm)
  * different portals - so we can't wait on any per-portal waitqueue). */
 static DECLARE_WAIT_QUEUE_HEAD(affine_queue);
 
-static u32 __poll_portal_slow(struct bman_portal *p)
+static u32 __poll_portal_slow(struct bman_portal *p, struct bm_portal *lowp,
+				u32 is)
 {
 	struct bman_depletion tmp;
-	u32 ret, is = bm_isr_status_read(p->p);
-	ret = is;
+	u32 ret = is;
 
 	/* There is a gotcha to be aware of. If we do the query before clearing
 	 * the status register, we may miss state changes that occur between the
@@ -269,16 +268,16 @@ static u32 __poll_portal_slow(struct bman_portal *p)
 		struct bm_mc_result *mcr;
 		unsigned int i, j;
 		u32 __is;
-		bm_isr_status_clear(p->p, BM_PIRQ_BSCN);
-		while ((__is = bm_isr_status_read(p->p)) & BM_PIRQ_BSCN) {
+		bm_isr_status_clear(lowp, BM_PIRQ_BSCN);
+		while ((__is = bm_isr_status_read(lowp)) & BM_PIRQ_BSCN) {
 			is |= __is;
-			bm_isr_status_clear(p->p, BM_PIRQ_BSCN);
+			bm_isr_status_clear(lowp, BM_PIRQ_BSCN);
 		}
 		is &= ~BM_PIRQ_BSCN;
 		local_irq_disable();
-		bm_mc_start(p->p);
-		bm_mc_commit(p->p, BM_MCC_VERB_CMD_QUERY);
-		while (!(mcr = bm_mc_result(p->p)))
+		bm_mc_start(lowp);
+		bm_mc_commit(lowp, BM_MCC_VERB_CMD_QUERY);
+		while (!(mcr = bm_mc_result(lowp)))
 			cpu_relax();
 		tmp = mcr->query.ds.state;
 		local_irq_enable();
@@ -310,11 +309,11 @@ static u32 __poll_portal_slow(struct bman_portal *p)
 
 	if (is & BM_PIRQ_RCRI) {
 		local_irq_disable();
-		p->rcr_cons += bm_rcr_cci_update(p->p);
-		bm_rcr_set_ithresh(p->p, 0);
+		p->rcr_cons += bm_rcr_cce_update(lowp);
+		bm_rcr_set_ithresh(lowp, 0);
 		wake_up(&p->queue);
 		local_irq_enable();
-		bm_isr_status_clear(p->p, BM_PIRQ_RCRI);
+		bm_isr_status_clear(lowp, BM_PIRQ_RCRI);
 		is &= ~BM_PIRQ_RCRI;
 	}
 
@@ -323,7 +322,8 @@ static u32 __poll_portal_slow(struct bman_portal *p)
 	return ret;
 }
 
-static void __poll_portal_fast(struct bman_portal *p)
+static inline void __poll_portal_fast(struct bman_portal *p,
+				struct bm_portal *lowp)
 {
 	/* nothing yet, this is where we'll put optimised RCR consumption
 	 * tracking */
@@ -338,25 +338,28 @@ static void __poll_portal_fast(struct bman_portal *p)
  * work to do. */
 #define SLOW_POLL_IDLE   1000
 #define SLOW_POLL_BUSY   10
+#ifdef CONFIG_FSL_BMAN_HAVE_POLL
 void bman_poll(void)
 {
 	struct bman_portal *p = get_affine_portal();
-	if (!(p->flags & BMAN_PORTAL_FLAG_IRQ)) {
-		/* we handle slow- and fast-path */
-		__poll_portal_fast(p);
-		if (!(p->slowpoll--)) {
-			u32 active = __poll_portal_slow(p);
-			if (active)
-				p->slowpoll = SLOW_POLL_BUSY;
-			else
-				p->slowpoll = SLOW_POLL_IDLE;
-		}
-	} else if (!(p->flags & BMAN_PORTAL_FLAG_IRQ_FAST))
-		/* we handle fast-path only */
-		__poll_portal_fast(p);
+	struct bm_portal *lowp = p->p;
+#ifndef CONFIG_FSL_BMAN_PORTAL_FLAG_IRQ_SLOW
+	if (!(p->slowpoll--)) {
+		u32 is = qm_isr_status_read(lowp);
+		u32 active = __poll_portal_slow(p, lowp, is);
+		if (active)
+			p->slowpoll = SLOW_POLL_BUSY;
+		else
+			p->slowpoll = SLOW_POLL_IDLE;
+	}
+#endif
+#ifndef CONFIG_FSL_BMAN_PORTAL_FLAG_IRQ_FAST
+	__poll_portal_fast(p, lowp);
+#endif
 	put_affine_portal();
 }
 EXPORT_SYMBOL(bman_poll);
+#endif
 
 static const u32 zero_thresholds[4] = {0, 0, 0, 0};
 
@@ -452,7 +455,7 @@ const struct bman_pool_params *bman_get_params(const struct bman_pool *pool)
 }
 EXPORT_SYMBOL(bman_get_params);
 
-static inline void rel_set_thresh(struct bman_portal *p, int check)
+static noinline void rel_set_thresh(struct bman_portal *p, int check)
 {
 	if (!check || !bm_rcr_get_ithresh(p->p))
 		bm_rcr_set_ithresh(p->p, RCR_ITHRESH);
@@ -463,29 +466,44 @@ static inline void rel_set_thresh(struct bman_portal *p, int check)
 static struct bm_rcr_entry *try_rel_start(struct bman_portal **p)
 {
 	struct bm_rcr_entry *r;
+	struct bm_portal *lowp;
+	u8 avail;
 	*p = get_affine_portal();
+	lowp = (*p)->p;
 	local_irq_disable();
-	if (bm_rcr_get_avail((*p)->p) < RCR_THRESH)
-		bm_rcr_cci_update((*p)->p);
-	r = bm_rcr_start((*p)->p);
+	avail = bm_rcr_get_avail(lowp);
+	if (avail == RCR_THRESH)
+		/* We don't need RCR:CI yet, but we will next time */
+		bm_rcr_cce_prefetch(lowp);
+	else if (avail < RCR_THRESH)
+		(*p)->rcr_cons += bm_rcr_cce_update(lowp);
+	r = bm_rcr_start(lowp);
 	if (unlikely(!r)) {
-		rel_set_thresh(*p, 1);
 		local_irq_enable();
 		put_affine_portal();
 	}
 	return r;
 }
 
-static inline int wait_rel_start(struct bman_portal **p,
-			struct bm_rcr_entry **rel, u32 flags)
+static inline struct bm_rcr_entry *__try_rel(struct bman_portal **p)
 {
+	struct bm_rcr_entry *rcr = try_rel_start(p);
+	if (unlikely(!rcr))
+		rel_set_thresh(*p, 1);
+	return rcr;
+}
+
+static noinline struct bm_rcr_entry *wait_rel_start(struct bman_portal **p,
+							u32 flags)
+{
+	struct bm_rcr_entry *rcr;
 	int ret = 0;
 	if (flags & BMAN_RELEASE_FLAG_WAIT_INT)
 		ret = wait_event_interruptible(affine_queue,
-				(*rel = try_rel_start(p)));
+				(rcr = try_rel_start(p)));
 	else
-		wait_event(affine_queue, (*rel = try_rel_start(p)));
-	return ret;
+		wait_event(affine_queue, (rcr = try_rel_start(p)));
+	return rcr;
 }
 
 /* This copies Qman's eqcr_completed() routine, see that for details */
@@ -507,6 +525,20 @@ static int rel_completed(struct bman_portal *p, u32 rcr_poll)
 	return 0;
 }
 
+static noinline void wait_rel_commit(struct bman_portal *p, u32 flags,
+					u32 rcr_poll)
+{
+	rel_set_thresh(p, 1);
+	/* So we're supposed to wait until the commit is consumed */
+	if (flags & BMAN_RELEASE_FLAG_WAIT_INT)
+		/* See bman_release() as to why we're ignoring return codes
+		 * from wait_***(). */
+		wait_event_interruptible(affine_queue,
+					rel_completed(p, rcr_poll));
+	else
+		wait_event(affine_queue, rel_completed(p, rcr_poll));
+}
+
 static inline void rel_commit(struct bman_portal *p, u32 flags, u8 num)
 {
 	u32 rcr_poll;
@@ -514,22 +546,11 @@ static inline void rel_commit(struct bman_portal *p, u32 flags, u8 num)
 			(num & BM_RCR_VERB_BUFCOUNT_MASK));
 	/* increment the producer count and capture it for SYNC */
 	rcr_poll = ++p->rcr_prod;
-	if ((flags & BMAN_RELEASE_FLAG_WAIT_SYNC) ==
-			BMAN_RELEASE_FLAG_WAIT_SYNC)
-		rel_set_thresh(p, 1);
 	local_irq_enable();
 	put_affine_portal();
-	if ((flags & BMAN_RELEASE_FLAG_WAIT_SYNC) !=
+	if ((flags & BMAN_RELEASE_FLAG_WAIT_SYNC) ==
 			BMAN_RELEASE_FLAG_WAIT_SYNC)
-		return;
-	/* So we're supposed to wait until the commit is consumed */
-	if (flags & BMAN_RELEASE_FLAG_WAIT_INT)
-		/* See bman_release() as to why we're ignoring return codes
-		 * from wait_***(). */
-		wait_event_interruptible(affine_queue,
-					rel_completed(p, rcr_poll));
-	else
-		wait_event(affine_queue, rel_completed(p, rcr_poll));
+		wait_rel_commit(p, flags, rcr_poll);
 }
 
 static inline int __bman_release(struct bman_pool *pool,
@@ -537,24 +558,26 @@ static inline int __bman_release(struct bman_pool *pool,
 {
 	struct bman_portal *p;
 	struct bm_rcr_entry *r;
-	u8 i;
+	u32 i = num - 1;
 
 	/* FIXME: I'm ignoring BMAN_PORTAL_FLAG_COMPACT for now. */
 	r = try_rel_start(&p);
 	if (unlikely(!r)) {
 		if (flags & BMAN_RELEASE_FLAG_WAIT) {
-			int ret = wait_rel_start(&p, &r, flags);
-			if (ret)
-				return ret;
+			r = wait_rel_start(&p, flags);
+			if (!r)
+				return -EBUSY;
 		} else
 			return -EBUSY;
 		BM_ASSERT(r != NULL);
 	}
+	/* We can memcpy() all but the first entry, as this can trigger badness
+	 * with the valid-bit. */
 	r->bpid = pool->params.bpid;
-	for (i = 0; i < num; i++) {
-		r->bufs[i].hi = bufs[i].hi;
-		r->bufs[i].lo = bufs[i].lo;
-	}
+	r->bufs[0].hi = bufs[0].hi;
+	r->bufs[0].lo = bufs[0].lo;
+	if (i)
+		memcpy(&r->bufs[1], &bufs[1], i * sizeof(bufs[0]));
 	/* Issue the release command and wait for sync if requested. NB: the
 	 * commit can't fail, only waiting can. Don't propogate any failure if a
 	 * signal arrives, otherwise the caller can't distinguish whether the
@@ -626,11 +649,7 @@ static inline int __bman_acquire(struct bman_pool *pool, struct bm_buffer *bufs,
 	while (!(mcr = bm_mc_result(p->p)))
 		cpu_relax();
 	ret = num = mcr->verb & BM_MCR_VERB_ACQUIRE_BUFCOUNT;
-	while (num--) {
-		bufs[num].bpid = pool->params.bpid;
-		bufs[num].hi = mcr->acquire.bufs[num].hi;
-		bufs[num].lo = mcr->acquire.bufs[num].lo;
-	}
+	memcpy(&bufs[0], &mcr->acquire.bufs[0], num * sizeof(bufs[0]));
 	local_irq_enable();
 	put_affine_portal();
 	return ret;
@@ -656,11 +675,14 @@ int bman_acquire(struct bman_pool *pool, struct bm_buffer *bufs, u8 num,
 			(pool->sp_fill <= (BMAN_STOCKPILE_LOW + num))) {
 		u8 ret = __bman_acquire(pool, pool->sp + pool->sp_fill, 8);
 		if (!ret)
-			return -ENOMEM;
+			goto hw_starved;
 		BUG_ON(ret != 8);
 		pool->sp_fill += 8;
-	} else if (pool->sp_fill < num)
-		return -ENOMEM;
+	} else {
+hw_starved:
+		if (pool->sp_fill < num)
+			return -ENOMEM;
+	}
 	memcpy(bufs, pool->sp + (pool->sp_fill - num),
 		sizeof(struct bm_buffer) * num);
 	pool->sp_fill -= num;
diff --git a/drivers/hwalloc/bman_low.c b/drivers/hwalloc/bman_low.c
index 84a293d..cd12eea 100644
--- a/drivers/hwalloc/bman_low.c
+++ b/drivers/hwalloc/bman_low.c
@@ -118,6 +118,11 @@ static inline u8 cyc_diff(u8 ringsize, u8 first, u8 last)
 /* --------------- */
 /* --- RCR API --- */
 
+/* It's safer to code in terms of the 'rcr' object than the 'portal' object,
+ * because the latter runs the risk of copy-n-paste errors from other code where
+ * we could manipulate some other structure within 'portal'. */
+#define RCR_API_START()		register struct bm_rcr *rcr = &portal->rcr
+
 /* Bit-wise logic to wrap a ring pointer by clearing the "carry bit" */
 #define RCR_CARRYCLEAR(p) \
 	(void *)((unsigned long)(p) & (~(unsigned long)(BM_RCR_SIZE << 6)))
@@ -140,14 +145,10 @@ static inline void RCR_INC(struct bm_rcr *rcr)
 		rcr->vbit ^= BM_RCR_VERB_VBIT;
 }
 
-/* It's safer to code in terms of the 'rcr' object than the 'portal' object,
- * because the latter runs the risk of copy-n-paste errors from other code where
- * we could manipulate some other structure within 'portal'. */
-#define rcr	(&portal->rcr)
-
 int bm_rcr_init(struct bm_portal *portal, enum bm_rcr_pmode pmode,
 		enum bm_rcr_cmode cmode)
 {
+	RCR_API_START();
 	u32 cfg;
 	u8 pi;
 
@@ -173,6 +174,7 @@ EXPORT_SYMBOL(bm_rcr_init);
 
 void bm_rcr_finish(struct bm_portal *portal)
 {
+	RCR_API_START();
 	u8 pi = bm_in(RCR_PI_CINH) & (BM_RCR_SIZE - 1);
 	u8 ci = bm_in(RCR_CI_CINH) & (BM_RCR_SIZE - 1);
 	BM_ASSERT(!rcr->busy);
@@ -188,6 +190,7 @@ EXPORT_SYMBOL(bm_rcr_finish);
 
 struct bm_rcr_entry *bm_rcr_start(struct bm_portal *portal)
 {
+	RCR_API_START();
 	BM_ASSERT(!rcr->busy);
 	if (!rcr->available)
 		return NULL;
@@ -201,6 +204,7 @@ EXPORT_SYMBOL(bm_rcr_start);
 
 void bm_rcr_abort(struct bm_portal *portal)
 {
+	RCR_API_START();
 	BM_ASSERT(rcr->busy);
 #ifdef CONFIG_FSL_BMAN_CHECKING
 	rcr->busy = 0;
@@ -210,6 +214,7 @@ EXPORT_SYMBOL(bm_rcr_abort);
 
 struct bm_rcr_entry *bm_rcr_pend_and_next(struct bm_portal *portal, u8 myverb)
 {
+	RCR_API_START();
 	BM_ASSERT(rcr->busy);
 	BM_ASSERT(rcr->pmode != bm_rcr_pvb);
 	if (rcr->available == 1)
@@ -225,6 +230,7 @@ EXPORT_SYMBOL(bm_rcr_pend_and_next);
 
 void bm_rcr_pci_commit(struct bm_portal *portal, u8 myverb)
 {
+	RCR_API_START();
 	BM_ASSERT(rcr->busy);
 	BM_ASSERT(rcr->pmode == bm_rcr_pci);
 	rcr->cursor->__dont_write_directly__verb = myverb | rcr->vbit;
@@ -240,6 +246,7 @@ EXPORT_SYMBOL(bm_rcr_pci_commit);
 
 void bm_rcr_pce_prefetch(struct bm_portal *portal)
 {
+	RCR_API_START();
 	BM_ASSERT(rcr->pmode == bm_rcr_pce);
 	bm_cl_invalidate(RCR_PI);
 	bm_cl_touch_rw(RCR_PI);
@@ -248,6 +255,7 @@ EXPORT_SYMBOL(bm_rcr_pce_prefetch);
 
 void bm_rcr_pce_commit(struct bm_portal *portal, u8 myverb)
 {
+	RCR_API_START();
 	BM_ASSERT(rcr->busy);
 	BM_ASSERT(rcr->pmode == bm_rcr_pce);
 	rcr->cursor->__dont_write_directly__verb = myverb | rcr->vbit;
@@ -263,11 +271,14 @@ EXPORT_SYMBOL(bm_rcr_pce_commit);
 
 void bm_rcr_pvb_commit(struct bm_portal *portal, u8 myverb)
 {
+	RCR_API_START();
+	struct bm_rcr_entry *rcursor;
 	BM_ASSERT(rcr->busy);
 	BM_ASSERT(rcr->pmode == bm_rcr_pvb);
 	lwsync();
-	rcr->cursor->__dont_write_directly__verb = myverb | rcr->vbit;
-	dcbf(rcr->cursor);
+	rcursor = rcr->cursor;
+	rcursor->__dont_write_directly__verb = myverb | rcr->vbit;
+	dcbf(rcursor);
 	RCR_INC(rcr);
 	rcr->available--;
 #ifdef CONFIG_FSL_BMAN_CHECKING
@@ -278,6 +289,7 @@ EXPORT_SYMBOL(bm_rcr_pvb_commit);
 
 u8 bm_rcr_cci_update(struct bm_portal *portal)
 {
+	RCR_API_START();
 	u8 diff, old_ci = rcr->ci;
 	BM_ASSERT(rcr->cmode == bm_rcr_cci);
 	rcr->ci = bm_in(RCR_CI_CINH) & (BM_RCR_SIZE - 1);
@@ -289,14 +301,15 @@ EXPORT_SYMBOL(bm_rcr_cci_update);
 
 void bm_rcr_cce_prefetch(struct bm_portal *portal)
 {
+	RCR_API_START();
 	BM_ASSERT(rcr->cmode == bm_rcr_cce);
-	bm_cl_invalidate(RCR_CI);
 	bm_cl_touch_ro(RCR_CI);
 }
 EXPORT_SYMBOL(bm_rcr_cce_prefetch);
 
 u8 bm_rcr_cce_update(struct bm_portal *portal)
 {
+	RCR_API_START();
 	u8 diff, old_ci = rcr->ci;
 	BM_ASSERT(rcr->cmode == bm_rcr_cce);
 	rcr->ci = bm_cl_in(RCR_CI) & (BM_RCR_SIZE - 1);
@@ -309,12 +322,14 @@ EXPORT_SYMBOL(bm_rcr_cce_update);
 
 u8 bm_rcr_get_ithresh(struct bm_portal *portal)
 {
+	RCR_API_START();
 	return rcr->ithresh;
 }
 EXPORT_SYMBOL(bm_rcr_get_ithresh);
 
 void bm_rcr_set_ithresh(struct bm_portal *portal, u8 ithresh)
 {
+	RCR_API_START();
 	rcr->ithresh = ithresh;
 	bm_out(RCR_ITR, ithresh);
 }
@@ -322,12 +337,14 @@ EXPORT_SYMBOL(bm_rcr_set_ithresh);
 
 u8 bm_rcr_get_avail(struct bm_portal *portal)
 {
+	RCR_API_START();
 	return rcr->available;
 }
 EXPORT_SYMBOL(bm_rcr_get_avail);
 
 u8 bm_rcr_get_fill(struct bm_portal *portal)
 {
+	RCR_API_START();
 	return BM_RCR_SIZE - 1 - rcr->available;
 }
 EXPORT_SYMBOL(bm_rcr_get_fill);
@@ -339,10 +356,11 @@ EXPORT_SYMBOL(bm_rcr_get_fill);
 /* It's safer to code in terms of the 'mc' object than the 'portal' object,
  * because the latter runs the risk of copy-n-paste errors from other code where
  * we could manipulate some other structure within 'portal'. */
-#define mc	(&portal->mc)
+#define MC_API_START()		register struct bm_mc *mc = &portal->mc
 
 int bm_mc_init(struct bm_portal *portal)
 {
+	MC_API_START();
 	if (__bm_portal_bind(portal, BM_BIND_MC))
 		return -EBUSY;
 	mc->cr = ptr_OR(portal->addr.addr_ce, CL_CR);
@@ -359,6 +377,7 @@ EXPORT_SYMBOL(bm_mc_init);
 
 void bm_mc_finish(struct bm_portal *portal)
 {
+	MC_API_START();
 	BM_ASSERT(mc->state == mc_idle);
 #ifdef CONFIG_FSL_BMAN_CHECKING
 	if (mc->state != mc_idle)
@@ -370,6 +389,7 @@ EXPORT_SYMBOL(bm_mc_finish);
 
 struct bm_mc_command *bm_mc_start(struct bm_portal *portal)
 {
+	MC_API_START();
 	BM_ASSERT(mc->state == mc_idle);
 #ifdef CONFIG_FSL_BMAN_CHECKING
 	mc->state = mc_user;
@@ -381,6 +401,7 @@ EXPORT_SYMBOL(bm_mc_start);
 
 void bm_mc_abort(struct bm_portal *portal)
 {
+	MC_API_START();
 	BM_ASSERT(mc->state == mc_user);
 #ifdef CONFIG_FSL_BMAN_CHECKING
 	mc->state = mc_idle;
@@ -390,6 +411,7 @@ EXPORT_SYMBOL(bm_mc_abort);
 
 void bm_mc_commit(struct bm_portal *portal, u8 myverb)
 {
+	MC_API_START();
 	BM_ASSERT(mc->state == mc_user);
 	dcbi(mc->rr + mc->rridx);
 	lwsync();
@@ -404,6 +426,7 @@ EXPORT_SYMBOL(bm_mc_commit);
 
 struct bm_mc_result *bm_mc_result(struct bm_portal *portal)
 {
+	MC_API_START();
 	struct bm_mc_result *rr = mc->rr + mc->rridx;
 	BM_ASSERT(mc->state == mc_hw);
 	/* The inactive response register's verb byte always returns zero until
diff --git a/drivers/hwalloc/bman_private.h b/drivers/hwalloc/bman_private.h
index 5f41426..e783d60 100644
--- a/drivers/hwalloc/bman_private.h
+++ b/drivers/hwalloc/bman_private.h
@@ -92,13 +92,7 @@ static inline void put_affine_portal(void)
 {
 	put_cpu_var(bman_affine_portal);
 }
-#define BMAN_PORTAL_FLAG_IRQ         0x00000001 /* use interrupt handler */
-#define BMAN_PORTAL_FLAG_IRQ_FAST    0x00000002 /* ... for fast-path too! */
-#define BMAN_PORTAL_FLAG_COMPACT     0x00000004 /* use compaction algorithm */
-#define BMAN_PORTAL_FLAG_RECOVER     0x00000008 /* recovery mode */
-#define BMAN_PORTAL_FLAG_WAIT        0x00000010 /* wait if RCR is full */
-#define BMAN_PORTAL_FLAG_WAIT_INT    0x00000020 /* if wait, interruptible? */
-struct bman_portal *bman_create_portal(struct bm_portal *portal, u32 flags,
+struct bman_portal *bman_create_portal(struct bm_portal *portal,
 					const struct bman_depletion *pools);
 void bman_destroy_portal(struct bman_portal *p);
 
diff --git a/drivers/hwalloc/bman_sys.h b/drivers/hwalloc/bman_sys.h
index 470d793..d09aa69 100644
--- a/drivers/hwalloc/bman_sys.h
+++ b/drivers/hwalloc/bman_sys.h
@@ -50,6 +50,27 @@
 #include <linux/miscdevice.h>
 #include <linux/uaccess.h>
 
+/* CONFIG_FSL_BMAN_HAVE_POLL is defined via Kconfig, because the API header is
+ * conditional upon it. CONFIG_FSL_BMAN_CHECKING is also defined via Kconfig,
+ * but because it's a knob for users. Everything else affects only
+ * implementation (not interface), so we define it here, internally. */
+
+/* do slow-path processing via IRQ */
+#define CONFIG_FSL_BMAN_PORTAL_FLAG_IRQ_SLOW
+
+/* do fast-path processing via IRQ */
+#define CONFIG_FSL_BMAN_PORTAL_FLAG_IRQ_FAST
+
+/* portals do not initialise in recovery mode */
+#undef CONFIG_FSL_BMAN_PORTAL_FLAG_RECOVER
+
+#if defined(CONFIG_FSL_BMAN_PORTAL_FLAG_IRQ_SLOW) || \
+		defined(CONFIG_FSL_BMAN_PORTAL_FLAG_IRQ_FAST)
+#define CONFIG_FSL_BMAN_HAVE_IRQ
+#else
+#undef CONFIG_FSL_BMAN_HAVE_IRQ
+#endif
+
 /* TODO: NB, we currently assume that hwsync() and lwsync() imply compiler
  * barriers and that dcb*() won't fall victim to compiler or execution
  * reordering with respect to other code/instructions that manipulate the same
@@ -94,4 +115,3 @@
 #define BM_ASSERT(x)
 #endif
 
-
diff --git a/drivers/hwalloc/bman_test_high.c b/drivers/hwalloc/bman_test_high.c
index fa1a9c8..0f0bba4 100644
--- a/drivers/hwalloc/bman_test_high.c
+++ b/drivers/hwalloc/bman_test_high.c
@@ -105,7 +105,6 @@ static void depletion_cb(struct bman_portal *__portal, struct bman_pool *__pool,
 	BUG_ON(__pool != pool);
 	BUG_ON(pool_ctx != POOL_OPAQUE);
 	depleted = __depleted;
-	pr_info("BMAN: depletion_cb: depleted=%d\n", depleted);
 }
 
 void bman_test_high(void)
@@ -127,9 +126,7 @@ void bman_test_high(void)
 
 	bufs_init();
 
-	pr_info("BMAN:  --------------------------------\n");
 	pr_info("BMAN:  --- starting high-level test ---\n");
-	pr_info("BMAN:  --------------------------------\n");
 
 	pool = bman_new_pool(&pparams);
 	BUG_ON(!pool);
@@ -149,7 +146,6 @@ do_loop:
 		if (bman_release(pool, bufs_in + i, num, flags))
 			panic("bman_release() failed\n");
 		i += num;
-		pr_info("BMAN: released %d buffers, total->%d\n", num, i);
 	}
 
 	/*******************/
@@ -162,7 +158,6 @@ do_loop:
 		tmp = bman_acquire(pool, bufs_out + i - num, num, 0);
 		BUG_ON(tmp != num);
 		i -= num;
-		pr_info("BMAN: acquired %d buffers, total->%d\n", num, i);
 	}
 	i = bman_acquire(pool, NULL, 1, 0);
 	BUG_ON(i);
@@ -176,8 +171,6 @@ do_loop:
 	/* Clean up */
 	/************/
 	bman_free_pool(pool);
-	pr_info("BMAN:  --------------------------------\n");
 	pr_info("BMAN:  --- finished high-level test ---\n");
-	pr_info("BMAN:  --------------------------------\n");
 }
 
diff --git a/drivers/hwqueue/Kconfig b/drivers/hwqueue/Kconfig
index d40c846..bb4f530 100644
--- a/drivers/hwqueue/Kconfig
+++ b/drivers/hwqueue/Kconfig
@@ -23,6 +23,11 @@ config FSL_QMAN_PORTAL
 	  Compiles support to detect and support Qman software corenet portals
 	  (as provided by the device-tree).
 
+# The current driver is interrupt-driven only (poll-driven isn't yet supported).
+config FSL_QMAN_HAVE_POLL
+	bool
+	default n
+
 config FSL_QMAN_PORTAL_DISABLEAUTO
 	bool "disable auto-initialisation of cpu-affine portals"
 	depends on FSL_QMAN_PORTAL
@@ -97,6 +102,14 @@ config FSL_QMAN_TEST_STASH_POTATO
 	  across a series of FQs scheduled to different portals (and cpus), with
 	  DQRR, data and context stashing always on.
 
+config FSL_QMAN_TEST_LOW
+	bool "Qman low-level self-test"
+	depends on FSL_QMAN_TEST && FSL_QMAN_PORTAL_DISABLEAUTO
+	default y
+	---help---
+	  This takes an unused portal and portal and performs low-level
+	  API testing with it.
+
 config FSL_QMAN_TEST_LOW_BUG_INTERRUPTS
 	bool "ignore interrupts and interrupt registers"
 	depends on FSL_QMAN_TEST_LOW
diff --git a/drivers/hwqueue/qman_driver.c b/drivers/hwqueue/qman_driver.c
index faa0449..697ae39 100644
--- a/drivers/hwqueue/qman_driver.c
+++ b/drivers/hwqueue/qman_driver.c
@@ -336,15 +336,9 @@ bad_cpu_ph:
 		return 0;
 	affine_portal = per_cpu(qman_affine_portal, cfg.cpu);
 	if (!affine_portal) {
-		u32 flags = QMAN_PORTAL_FLAG_IRQ | QMAN_PORTAL_FLAG_IRQ_FAST;
-#ifndef CONFIG_FSL_QMAN_PORTAL_DISABLEAUTO_DCA
-		flags |= QMAN_PORTAL_FLAG_DCA;
-#endif
-#ifdef CONFIG_FSL_QMAN_PORTAL_REQUIRE_ENABLE
-		flags |= QMAN_PORTAL_FLAG_DISABLE;
-#endif
+		u32 flags = 0;
 		if (cfg.has_hv_dma)
-			flags |= QMAN_PORTAL_FLAG_RSTASH |
+			flags = QMAN_PORTAL_FLAG_RSTASH |
 				QMAN_PORTAL_FLAG_DSTASH;
 		/* TODO: cgrs ?? */
 		affine_portal = qman_create_portal(portal, flags, NULL,
diff --git a/drivers/hwqueue/qman_high.c b/drivers/hwqueue/qman_high.c
index e77b835..283ce8b 100644
--- a/drivers/hwqueue/qman_high.c
+++ b/drivers/hwqueue/qman_high.c
@@ -116,6 +116,7 @@ struct qman_portal {
 	struct qman_fq_cb null_cb;
 	/* This is needed for providing a non-NULL device to dma_map_***() */
 	struct platform_device *pdev;
+	struct qman_rbtree retire_table;
 };
 
 /* This gives a FQID->FQ lookup to cover the fact that we can't directly demux
@@ -127,43 +128,27 @@ struct qman_portal {
  * all FQs - FQs are added when retirement commands are issued, and removed when
  * they complete, which also massively reduces the size of this table. */
 IMPLEMENT_QMAN_RBTREE(fqtree, struct qman_fq, node, fqid);
-static struct qman_rbtree table = QMAN_RBTREE;
-/* This is only locked with local_irq_disable() held or from interrupt context,
- * so spin_lock_irq() shouldn't be used (nesting trouble). */
-static spinlock_t table_lock = SPIN_LOCK_UNLOCKED;
-static unsigned int table_num;
 
 /* This is what everything can wait on, even if it migrates to a different cpu
  * to the one whose affine portal it is waiting on. */
 static DECLARE_WAIT_QUEUE_HEAD(affine_queue);
 
-static inline int table_push_fq(struct qman_fq *fq)
+static inline int table_push_fq(struct qman_portal *p, struct qman_fq *fq)
 {
-	int ret;
-	spin_lock(&table_lock);
-	ret = fqtree_push(&table, fq);
+	int ret = fqtree_push(&p->retire_table, fq);
 	if (ret)
 		pr_err("ERROR: double FQ-retirement %d\n", fq->fqid);
-	else
-		table_num++;
-	spin_unlock(&table_lock);
 	return ret;
 }
 
-static inline void table_del_fq(struct qman_fq *fq)
+static inline void table_del_fq(struct qman_portal *p, struct qman_fq *fq)
 {
-	spin_lock(&table_lock);
-	fqtree_del(&table, fq);
-	spin_unlock(&table_lock);
+	fqtree_del(&p->retire_table, fq);
 }
 
-static inline struct qman_fq *table_find_fq(u32 fqid)
+static inline struct qman_fq *table_find_fq(struct qman_portal *p, u32 fqid)
 {
-	struct qman_fq *fq;
-	spin_lock(&table_lock);
-	fq = fqtree_find(&table, fqid);
-	spin_unlock(&table_lock);
-	return fq;
+	return fqtree_find(&p->retire_table, fqid);
 }
 
 /* In the case that slow- and fast-path handling are both done by qman_poll()
@@ -175,26 +160,30 @@ static inline struct qman_fq *table_find_fq(u32 fqid)
  * work to do. */
 #define SLOW_POLL_IDLE   1000
 #define SLOW_POLL_BUSY   10
-static u32 __poll_portal_slow(struct qman_portal *p, u32 is);
-static void __poll_portal_fast(struct qman_portal *p);
+static u32 __poll_portal_slow(struct qman_portal *p, struct qm_portal *lowp,
+				u32 is);
+static inline void __poll_portal_fast(struct qman_portal *p,
+					struct qm_portal *lowp);
 
+#ifdef CONFIG_FSL_QMAN_HAVE_IRQ
 /* Portal interrupt handler */
 static irqreturn_t portal_isr(int irq, void *ptr)
 {
 	struct qman_portal *p = ptr;
-	u32 clear = QM_PIRQ_DQRI, is = qm_isr_status_read(p->p);
-	if (unlikely(!(p->options & QMAN_PORTAL_FLAG_IRQ))) {
-		pr_crit("Portal interrupt is supposed to be disabled!\n");
-		qm_isr_inhibit(p->p);
-		return IRQ_HANDLED;
-	}
+	struct qm_portal *lowp = p->p;
+	u32 clear = 0, is = qm_isr_status_read(lowp);
 	/* Only do fast-path handling if it's required */
-	if (p->options & QMAN_PORTAL_FLAG_IRQ_FAST)
-		__poll_portal_fast(p);
-	clear |= __poll_portal_slow(p, is);
-	qm_isr_status_clear(p->p, clear);
+#ifdef CONFIG_FSL_QMAN_PORTAL_FLAG_IRQ_FAST
+	clear |= QM_PIRQ_DQRI;
+	__poll_portal_fast(p, lowp);
+#endif
+#ifdef CONFIG_FSL_QMAN_PORTAL_FLAG_IRQ_SLOW
+	clear |= __poll_portal_slow(p, lowp, is);
+#endif
+	qm_isr_status_clear(lowp, clear);
 	return IRQ_HANDLED;
 }
+#endif
 
 /* This inner version is used privately by qman_create_portal(), as well as by
  * the exported qman_disable_portal(). */
@@ -231,19 +220,20 @@ struct qman_portal *qman_create_portal(struct qm_portal *__p, u32 flags,
 	int ret;
 	u32 isdr;
 
-	if (!(flags & QMAN_PORTAL_FLAG_NOTAFFINE))
-		/* core-affine portals don't need multi-core locking */
-		flags &= ~QMAN_PORTAL_FLAG_LOCKED;
 	portal = kmalloc(sizeof(*portal), GFP_KERNEL);
 	if (!portal)
 		return NULL;
-	if (qm_eqcr_init(__p, qm_eqcr_pvb, qm_eqcr_cci)) {
+	if (qm_eqcr_init(__p, qm_eqcr_pvb, qm_eqcr_cce)) {
 		pr_err("Qman EQCR initialisation failed\n");
 		goto fail_eqcr;
 	}
 	if (qm_dqrr_init(__p, qm_dqrr_dpush, qm_dqrr_pvb,
-			(flags & QMAN_PORTAL_FLAG_DCA) ? qm_dqrr_cdc :
-					qm_dqrr_cci, DQRR_MAXFILL,
+#ifdef CONFIG_FSL_QMAN_PORTAL_DISABLEAUTO_DCA
+			qm_dqrr_cci
+#else
+			qm_dqrr_cdc
+#endif
+			, DQRR_MAXFILL,
 			(flags & QMAN_PORTAL_FLAG_RSTASH) ? 1 : 0,
 			(flags & QMAN_PORTAL_FLAG_DSTASH) ? 1 : 0)) {
 		pr_err("Qman DQRR initialisation failed\n");
@@ -292,72 +282,63 @@ struct qman_portal *qman_create_portal(struct qm_portal *__p, u32 flags,
 	ret = platform_device_add(portal->pdev);
 	if (ret)
 		goto fail_devadd;
+	qman_rbtree_init(&portal->retire_table);
 	isdr = 0xffffffff;
 	qm_isr_disable_write(portal->p, isdr);
 	qm_isr_enable_write(portal->p, QM_PIRQ_EQCI | QM_PIRQ_EQRI |
-		((flags & QMAN_PORTAL_FLAG_IRQ_FAST) ? QM_PIRQ_DQRI : 0) |
+#ifdef CONFIG_FSL_QMAN_PORTAL_FLAG_IRQ_FAST
+		QM_PIRQ_DQRI |
+#endif
 		QM_PIRQ_MRI | (cgrs ? QM_PIRQ_CSCI : 0));
 	qm_isr_status_clear(portal->p, 0xffffffff);
-	if (flags & QMAN_PORTAL_FLAG_IRQ) {
-		if (request_irq(config->irq, portal_isr, 0, "Qman portal 0",
+#ifdef CONFIG_FSL_QMAN_HAVE_IRQ
+	if (request_irq(config->irq, portal_isr, 0, "Qman portal 0",
 					portal)) {
-			pr_err("request_irq() failed\n");
-			goto fail_irq;
-		}
-		if ((config->cpu != -1) &&
-				irq_can_set_affinity(config->irq) &&
+		pr_err("request_irq() failed\n");
+		goto fail_irq;
+	}
+	if ((config->cpu != -1) && irq_can_set_affinity(config->irq) &&
 				irq_set_affinity(config->irq,
-				     cpumask_of(config->cpu))) {
-			pr_err("irq_set_affinity() failed\n");
-			goto fail_affinity;
-		}
-		qm_isr_uninhibit(portal->p);
-	} else
-		/* without IRQ, we can't block */
-		flags &= ~QMAN_PORTAL_FLAG_WAIT;
+					cpumask_of(config->cpu))) {
+		pr_err("irq_set_affinity() failed\n");
+		goto fail_affinity;
+	}
+	qm_isr_uninhibit(portal->p);
+#endif
 	/* Need EQCR to be empty before continuing */
 	isdr ^= QM_PIRQ_EQCI;
 	qm_isr_disable_write(portal->p, isdr);
-	if (!(flags & QMAN_PORTAL_FLAG_RECOVER) ||
-			!(flags & QMAN_PORTAL_FLAG_WAIT))
-		ret = qm_eqcr_get_fill(portal->p);
-	else if (flags & QMAN_PORTAL_FLAG_WAIT_INT)
-		ret = wait_event_interruptible(affine_queue,
-			!qm_eqcr_get_fill(portal->p));
-	else {
-		wait_event(affine_queue, !qm_eqcr_get_fill(portal->p));
-		ret = 0;
-	}
+#ifdef CONFIG_FSL_QMAN_PORTAL_FLAG_RECOVER
+	wait_event(affine_queue, !qm_eqcr_get_fill(portal->p));
+	ret = 0;
+#else
+	ret = qm_eqcr_get_fill(portal->p);
+#endif
 	if (ret) {
 		pr_err("Qman EQCR unclean, need recovery\n");
 		goto fail_eqcr_empty;
 	}
+#ifdef CONFIG_FSL_QMAN_PORTAL_FLAG_RECOVER
 	/* Check DQRR and MR are empty too, subject to RECOVERY logic */
 	if (flags & QMAN_PORTAL_FLAG_RECOVER)
 		portal->bits |= PORTAL_BITS_RECOVER;
+#endif
 	isdr ^= (QM_PIRQ_DQRI | QM_PIRQ_MRI);
 	qm_isr_disable_write(portal->p, isdr);
-	if (!(flags & QMAN_PORTAL_FLAG_RECOVER))
-		ret = !int_dqrr_mr_empty(portal, 0);
-	else {
-		if (!(flags & QMAN_PORTAL_FLAG_WAIT))
-			ret = !int_dqrr_mr_empty(portal, 0);
-		else if (flags & QMAN_PORTAL_FLAG_WAIT_INT)
-			ret = wait_event_interruptible(affine_queue,
-				int_dqrr_mr_empty(portal, 1));
-		else {
-			wait_event(affine_queue,
-				int_dqrr_mr_empty(portal, 1));
-			ret = 0;
-		}
-		qman_disable_portal_ex(portal);
-		portal->bits ^= PORTAL_BITS_RECOVER;
-	}
+#ifdef CONFIG_FSL_QMAN_PORTAL_FLAG_RECOVER
+	wait_event(affine_queue, int_dqrr_mr_empty(portal, 1));
+	ret = 0;
+	qman_disable_portal_ex(portal);
+	portal->bits ^= PORTAL_BITS_RECOVER;
+#else
+	ret = !int_dqrr_mr_empty(portal, 0);
+#endif
 	if (ret) {
-		if (flags & QMAN_PORTAL_FLAG_RECOVER)
-			pr_err("Qman DQRR/MR unclean, recovery failed\n");
-		else
-			pr_err("Qman DQRR/MR unclean, need recovery\n");
+#ifdef CONFIG_FSL_QMAN_PORTAL_FLAG_RECOVER
+		pr_err("Qman DQRR/MR unclean, recovery failed\n");
+#else
+		pr_err("Qman DQRR/MR unclean, need recovery\n");
+#endif
 		goto fail_dqrr_mr_empty;
 	}
 	qm_isr_disable_write(portal->p, 0);
@@ -367,8 +348,9 @@ struct qman_portal *qman_create_portal(struct qm_portal *__p, u32 flags,
 fail_dqrr_mr_empty:
 fail_eqcr_empty:
 fail_affinity:
-	if (flags & QMAN_PORTAL_FLAG_IRQ)
-		free_irq(config->irq, portal);
+#ifdef CONFIG_FSL_QMAN_HAVE_IRQ
+	free_irq(config->irq, portal);
+#endif
 fail_irq:
 	platform_device_del(portal->pdev);
 fail_devadd:
@@ -395,9 +377,10 @@ void qman_destroy_portal(struct qman_portal *qm)
 {
 	/* NB we do this to "quiesce" EQCR. If we add enqueue-completions or
 	 * something related to QM_PIRQ_EQCI, this may need fixing. */
-	qm_eqcr_cci_update(qm->p);
-	if (qm->options & QMAN_PORTAL_FLAG_IRQ)
-		free_irq(qm_portal_config(qm->p)->irq, qm);
+	qm_eqcr_cce_update(qm->p);
+#ifdef CONFIG_FSL_QMAN_HAVE_IRQ
+	free_irq(qm_portal_config(qm->p)->irq, qm);
+#endif
 	if (qm->cgrs)
 		kfree(qm->cgrs);
 	qm_isr_finish(qm->p);
@@ -425,15 +408,15 @@ void qman_set_null_cb(const struct qman_fq_cb *null_cb)
 EXPORT_SYMBOL(qman_set_null_cb);
 
 /* Inline helper to reduce nesting in __poll_portal_slow() */
-static inline void fq_state_change(struct qman_fq *fq, struct qm_mr_entry *msg,
-					u8 verb)
+static inline void fq_state_change(struct qman_portal *p, struct qman_fq *fq,
+				struct qm_mr_entry *msg, u8 verb)
 {
 	FQLOCK(fq);
 	switch(verb) {
 	case QM_MR_VERB_FQRL:
 		QM_ASSERT(fq_isset(fq, QMAN_FQ_STATE_ORL));
 		fq_clear(fq, QMAN_FQ_STATE_ORL);
-		table_del_fq(fq);
+		table_del_fq(p, fq);
 		break;
 	case QM_MR_VERB_FQRN:
 		QM_ASSERT((fq->state == qman_fq_state_parked) ||
@@ -445,14 +428,9 @@ static inline void fq_state_change(struct qman_fq *fq, struct qm_mr_entry *msg,
 		if (msg->fq.fqs & QM_MR_FQS_ORLPRESENT)
 			fq_set(fq, QMAN_FQ_STATE_ORL);
 		else
-			table_del_fq(fq);
+			table_del_fq(p, fq);
 		fq->state = qman_fq_state_retired;
 		break;
-	case QM_MR_VERB_FQRNI:
-		/* we processed retirement with the MC command */
-		if (fq_isclear(fq, QMAN_FQ_STATE_ORL))
-			table_del_fq(fq);
-		break;
 	case QM_MR_VERB_FQPN:
 		QM_ASSERT(fq->state == qman_fq_state_sched);
 		QM_ASSERT(fq_isclear(fq, QMAN_FQ_STATE_CHANGING));
@@ -461,25 +439,26 @@ static inline void fq_state_change(struct qman_fq *fq, struct qm_mr_entry *msg,
 	FQUNLOCK(fq);
 }
 
-static inline void eqcr_set_thresh(struct qman_portal *p, int check)
+static noinline void eqcr_set_thresh(struct qman_portal *p, int check)
 {
 	if (!check || !qm_eqcr_get_ithresh(p->p))
 		qm_eqcr_set_ithresh(p->p, EQCR_ITHRESH);
 }
 
-static u32 __poll_portal_slow(struct qman_portal *p, u32 is)
+static u32 __poll_portal_slow(struct qman_portal *p, struct qm_portal *lowp,
+				u32 is)
 {
 	struct qm_mr_entry *msg;
 
-	qm_mr_pvb_prefetch(p->p);
+	qm_mr_pvb_prefetch(lowp);
 
 	if (is & QM_PIRQ_CSCI) {
 		struct qm_mc_result *mcr;
 		unsigned int i;
 		local_irq_disable();
-		qm_mc_start(p->p);
-		qm_mc_commit(p->p, QM_MCC_VERB_QUERYCONGESTION);
-		while (!(mcr = qm_mc_result(p->p)))
+		qm_mc_start(lowp);
+		qm_mc_commit(lowp, QM_MCC_VERB_QUERYCONGESTION);
+		while (!(mcr = qm_mc_result(lowp)))
 			cpu_relax();
 		p->cgrs[1].q = mcr->querycongestion.state;
 		local_irq_enable();
@@ -502,8 +481,8 @@ static u32 __poll_portal_slow(struct qman_portal *p, u32 is)
 #endif
 	if (is & QM_PIRQ_EQRI) {
 		local_irq_disable();
-		p->eq_cons += qm_eqcr_cci_update(p->p);
-		qm_eqcr_set_ithresh(p->p, 0);
+		p->eq_cons += qm_eqcr_cce_update(lowp);
+		qm_eqcr_set_ithresh(lowp, 0);
 		wake_up(&affine_queue);
 		local_irq_enable();
 	}
@@ -511,9 +490,9 @@ static u32 __poll_portal_slow(struct qman_portal *p, u32 is)
 	if (is & QM_PIRQ_MRI) {
 		u8 num = 0;
 mr_loop:
-		if (qm_mr_pvb_update(p->p))
-			qm_mr_pvb_prefetch(p->p);
-		msg = qm_mr_current(p->p);
+		if (qm_mr_pvb_update(lowp))
+			qm_mr_pvb_prefetch(lowp);
+		msg = qm_mr_current(lowp);
 		if (msg) {
 			struct qman_fq *fq = (void *)msg->ern.tag;
 			u8 verb = msg->verb & QM_MR_VERB_TYPE_MASK;
@@ -526,14 +505,15 @@ mr_loop:
 				else if (p->null_cb.fqs)
 					p->null_cb.fqs(p, NULL, msg);
 			}
-			if ((verb == QM_MR_VERB_FQRN) ||
-					(verb == QM_MR_VERB_FQRNI) ||
+			if (verb == QM_MR_VERB_FQRNI) {
+				; /* nada, we drop FQRNIs on the floor */
+			} else if ((verb == QM_MR_VERB_FQRN) ||
 					(verb == QM_MR_VERB_FQRL)) {
 				/* It's retirement related - need a lookup */
-				fq = table_find_fq(msg->fq.fqid);
+				fq = table_find_fq(p, msg->fq.fqid);
 				if (!fq)
 					panic("unexpected FQ retirement");
-				fq_state_change(fq, msg, verb);
+				fq_state_change(p, fq, msg, verb);
 				if (fq->cb.fqs)
 					fq->cb.fqs(p, fq, msg);
 			} else if (likely(fq)) {
@@ -553,10 +533,10 @@ mr_loop:
 					p->null_cb.fqs(p, NULL, msg);
 			}
 			num++;
-			qm_mr_next(p->p);
+			qm_mr_next(lowp);
 			goto mr_loop;
 		}
-		qm_mr_cci_consume(p->p, num);
+		qm_mr_cci_consume(lowp, num);
 	}
 
 	return is & (QM_PIRQ_CSCI | QM_PIRQ_EQCI | QM_PIRQ_EQRI | QM_PIRQ_MRI);
@@ -589,24 +569,26 @@ mr_loop:
  * user callbacks to call into any Qman API *except* qman_poll() (as that's the
  * sole API that could be invoking the callback through this function).
  */
-static void __poll_portal_fast(struct qman_portal *p)
+static inline void __poll_portal_fast(struct qman_portal *p,
+				struct qm_portal *lowp)
 {
 	struct qm_dqrr_entry *dq;
 	struct qman_fq *fq;
 	enum qman_cb_dqrr_result res;
-	u8 num = 0;
+	int prefetch = !(p->options & QMAN_PORTAL_FLAG_RSTASH);
 
-	qm_dqrr_pvb_prefetch(p->p);
-
-dqrr_loop:
-	if (qm_dqrr_pvb_update(p->p))
-		qm_dqrr_pvb_prefetch(p->p);
-	dq = qm_dqrr_current(p->p);
-	if (!dq || (num == CONFIG_FSL_QMAN_POLL_LIMIT))
+	if (qm_dqrr_pvb_update(lowp) && prefetch)
+		qm_dqrr_pvb_prefetch(lowp);
+	dq = qm_dqrr_current(lowp);
+	if (!dq)
 		goto done;
 	fq = (void *)dq->contextB;
 	/* Interpret 'dq' from the owner's perspective. */
-	if (unlikely((p->bits & PORTAL_BITS_RECOVER) || !fq)) {
+	if (unlikely(
+#ifdef CONFIG_FSL_QMAN_PORTAL_FLAG_RECOVER
+			(p->bits & PORTAL_BITS_RECOVER) ||
+#endif
+			!fq)) {
 		/* use portal default handlers */
 		res = p->null_cb.dqrr(p, NULL, dq);
 		QM_ASSERT(res == qman_cb_dqrr_consume);
@@ -619,7 +601,7 @@ dqrr_loop:
 	}
 	/* Interpret 'dq' from a driver perspective. */
 #define VDQCR_DONE (QM_DQRR_STAT_UNSCHEDULED | QM_DQRR_STAT_DQCR_EXPIRED)
-	if ((dq->stat & VDQCR_DONE) == VDQCR_DONE) {
+	if (unlikely((dq->stat & VDQCR_DONE) == VDQCR_DONE)) {
 		PORTAL_BITS_INC_V(p);
 		wake_up(&affine_queue);
 	}
@@ -628,48 +610,48 @@ dqrr_loop:
 	 * check for HELDACTIVE to cover both. */
 	QM_ASSERT((dq->stat & QM_DQRR_STAT_FQ_HELDACTIVE) ||
 		(res != qman_cb_dqrr_park));
-	if (p->options & QMAN_PORTAL_FLAG_DCA) {
-		/* Defer just means "skip it, I'll consume it
-		 * myself later on" */
-		if (res != qman_cb_dqrr_defer)
-			qm_dqrr_cdc_consume_1ptr(p->p, dq,
-				(res == qman_cb_dqrr_park));
-	} else if (res == qman_cb_dqrr_park)
-		/* The only thing to do for non-DCA is the
-		 * park-request */
-		qm_dqrr_park_ci(p->p);
+#ifdef CONFIG_FSL_QMAN_PORTAL_DISABLEAUTO_DCA
+	if (res == qman_cb_dqrr_park)
+		/* The only thing to do for non-DCA is the park-request */
+		qm_dqrr_park_ci(lowp);
+	/* Move forward */
+	qm_dqrr_next(lowp);
+	qm_dqrr_cci_consume(lowp, 1);
+#else
+	/* Defer just means "skip it, I'll consume it myself later on" */
+	if (res != qman_cb_dqrr_defer)
+		qm_dqrr_cdc_consume_1ptr(lowp, dq, (res == qman_cb_dqrr_park));
 	/* Move forward */
-	num++;
-	qm_dqrr_next(p->p);
-	goto dqrr_loop;
+	qm_dqrr_next(lowp);
+#endif
 done:
-	if (num) {
-		if (!(p->options & QMAN_PORTAL_FLAG_DCA))
-			qm_dqrr_cci_consume(p->p, num);
-	}
+	if (prefetch)
+		qm_dqrr_pvb_prefetch(lowp);
 }
 
+#ifdef CONFIG_FSL_QMAN_HAVE_POLL
 void qman_poll(void)
 {
 	struct qman_portal *p = get_affine_portal();
-	if (!(p->options & QMAN_PORTAL_FLAG_IRQ)) {
-		/* we handle slow- and fast-path */
-		__poll_portal_fast(p);
-		if (!(p->slowpoll--)) {
-			u32 is = qm_isr_status_read(p->p);
-			u32 active = __poll_portal_slow(p, is);
-			if (active) {
-				qm_isr_status_clear(p->p, active);
-				p->slowpoll = SLOW_POLL_BUSY;
-			} else
-				p->slowpoll = SLOW_POLL_IDLE;
-		}
-	} else if (!(p->options & QMAN_PORTAL_FLAG_IRQ_FAST))
-		/* we handle fast-path only */
-		__poll_portal_fast(p);
+	struct qm_portal *lowp = p->p;
+#ifndef CONFIG_FSL_QMAN_PORTAL_FLAG_IRQ_SLOW
+	if (!(p->slowpoll--)) {
+		u32 is = qm_isr_status_read(lowp);
+		u32 active = __poll_portal_slow(p, lowp, is);
+		if (active) {
+			qm_isr_status_clear(lowp, active);
+			p->slowpoll = SLOW_POLL_BUSY;
+		} else
+			p->slowpoll = SLOW_POLL_IDLE;
+	}
+#endif
+#ifndef CONFIG_FSL_QMAN_PORTAL_FLAG_IRQ_FAST
+	__poll_portal_fast(p, lowp);
+#endif
 	put_affine_portal();
 }
 EXPORT_SYMBOL(qman_poll);
+#endif
 
 void qman_disable_portal(void)
 {
@@ -1035,7 +1017,7 @@ int qman_retire_fq(struct qman_fq *fq, u32 *flags)
 		rval = -EBUSY;
 		goto out;
 	}
-	rval = table_push_fq(fq);
+	rval = table_push_fq(p, fq);
 	if (rval)
 		goto out;
 	mcc = qm_mc_start(p->p);
@@ -1049,7 +1031,7 @@ int qman_retire_fq(struct qman_fq *fq, u32 *flags)
 	 * and defer the flags until FQRNI or FQRN (respectively) show up. But
 	 * "Friendly" is to process OK immediately, and not set CHANGING. We do
 	 * friendly, otherwise the caller doesn't necessarily have a fully
-	 * "retired" FQ on return even if the retirement was immediate.  However
+	 * "retired" FQ on return even if the retirement was immediate. However
 	 * this does mean some code duplication between here and
 	 * fq_state_change(). */
 	if (likely(res == QM_MCR_RESULT_OK)) {
@@ -1059,15 +1041,32 @@ int qman_retire_fq(struct qman_fq *fq, u32 *flags)
 			fq_set(fq, QMAN_FQ_STATE_NE);
 		if (mcr->alterfq.fqs & QM_MCR_FQS_ORLPRESENT)
 			fq_set(fq, QMAN_FQ_STATE_ORL);
+		else
+			table_del_fq(p, fq);
 		if (flags)
 			*flags = fq->flags;
 		fq->state = qman_fq_state_retired;
+		if (fq->cb.fqs) {
+			/* Another issue with supporting "immediate" retirement
+			 * is that we're forced to drop FQRNIs, because by the
+			 * time they're seen it may already be "too late" (the
+			 * fq may have been OOS'd and free()'d already). But if
+			 * the upper layer wants a callback whether it's
+			 * immediate or not, we have to fake a "MR" entry to
+			 * look like an FQRNI... */
+			struct qm_mr_entry msg;
+			msg.verb = QM_MR_VERB_FQRNI;
+			msg.fq.fqs = mcr->alterfq.fqs;
+			msg.fq.fqid = fq->fqid;
+			msg.fq.contextB = (u32)fq;
+			fq->cb.fqs(p, fq, &msg);
+		}
 	} else if (res == QM_MCR_RESULT_PENDING) {
 		rval = 1;
 		fq_set(fq, QMAN_FQ_STATE_CHANGING);
 	} else {
 		rval = -EIO;
-		table_del_fq(fq);
+		table_del_fq(p, fq);
 		pr_err("ALTER_RETIRE failed: %s\n", mcr_result_str(res));
 	}
 out:
@@ -1194,7 +1193,7 @@ static int set_vdqcr(struct qman_portal **p, u32 vdqcr, u16 *v)
 	return ret;
 }
 
-static inline int wait_vdqcr_start(struct qman_portal **p, u32 vdqcr, u16 *v,
+static int wait_vdqcr_start(struct qman_portal **p, u32 vdqcr, u16 *v,
 					u32 flags)
 {
 	int ret = 0;
@@ -1242,29 +1241,49 @@ EXPORT_SYMBOL(qman_volatile_dequeue);
 static struct qm_eqcr_entry *try_eq_start(struct qman_portal **p)
 {
 	struct qm_eqcr_entry *eq;
+	struct qm_portal *lowp;
+	u8 avail;
 	*p = get_affine_portal();
+	lowp = (*p)->p;
 	local_irq_disable();
-	if (qm_eqcr_get_avail((*p)->p) < EQCR_THRESH)
-		(*p)->eq_cons += qm_eqcr_cci_update((*p)->p);
-	eq = qm_eqcr_start((*p)->p);
+	avail = qm_eqcr_get_avail(lowp);
+	if (avail == EQCR_THRESH)
+		/* We don't need EQCR:CI yet, but we will next time */
+		qm_eqcr_cce_prefetch(lowp);
+	else if (avail < EQCR_THRESH)
+		/* The EQCR::CI cacheline is prefetched post-enqueue, so this
+		 * would ideally be in cache from the previous commit. */
+		(*p)->eq_cons += qm_eqcr_cce_update(lowp);
+	eq = qm_eqcr_start(lowp);
 	if (unlikely(!eq)) {
-		eqcr_set_thresh(*p, 1);
 		local_irq_enable();
 		put_affine_portal();
 	}
 	return eq;
 }
 
-static inline int wait_eq_start(struct qman_portal **p,
-			struct qm_eqcr_entry **eq, u32 flags)
+static inline struct qm_eqcr_entry *__try_eq(struct qman_portal **p)
+{
+	struct qm_eqcr_entry *eq = try_eq_start(p);
+	if (unlikely(!eq))
+		/* TODO: this used to be in try_eq_start() prior to
+		 * local_irq_enable() - verify that the reorder hasn't created a
+		 * race... */
+		eqcr_set_thresh(*p, 1);
+	return eq;
+}
+
+static noinline struct qm_eqcr_entry *wait_eq_start(struct qman_portal **p,
+							u32 flags)
 {
+	struct qm_eqcr_entry *eq;
 	int ret = 0;
 	if (flags & QMAN_ENQUEUE_FLAG_WAIT_INT)
 		ret = wait_event_interruptible(affine_queue,
-					(*eq = try_eq_start(p)));
+					(eq = __try_eq(p)));
 	else
-		wait_event(affine_queue, (*eq = try_eq_start(p)));
-	return ret;
+		wait_event(affine_queue, (eq = __try_eq(p)));
+	return eq;
 }
 
 /* Used as a wait_event() condition to determine if eq_cons has caught up to
@@ -1296,6 +1315,20 @@ static int eqcr_completed(struct qman_portal *p, u32 eq_poll)
 	return 0;
 }
 
+static noinline void wait_eqcr_commit(struct qman_portal *p, u32 flags,
+					u32 eq_poll)
+{
+	eqcr_set_thresh(p, 1);
+	/* So we're supposed to wait until the commit is consumed */
+	if (flags & QMAN_ENQUEUE_FLAG_WAIT_INT)
+		/* See __enqueue() (where this inline is called) as to why we're
+		 * ignoring return codes from wait_***(). */
+		wait_event_interruptible(affine_queue,
+					eqcr_completed(p, eq_poll));
+	else
+		wait_event(affine_queue, eqcr_completed(p, eq_poll));
+}
+
 static inline void eqcr_commit(struct qman_portal *p, u32 flags, int orp)
 {
 	u32 eq_poll;
@@ -1305,22 +1338,11 @@ static inline void eqcr_commit(struct qman_portal *p, u32 flags, int orp)
 		(orp ? QM_EQCR_VERB_ORP : 0));
 	/* increment the producer count and capture it for SYNC */
 	eq_poll = ++p->eq_prod;
-	if ((flags & QMAN_ENQUEUE_FLAG_WAIT_SYNC) ==
-			QMAN_ENQUEUE_FLAG_WAIT_SYNC)
-		eqcr_set_thresh(p, 1);
 	local_irq_enable();
 	put_affine_portal();
-	if ((flags & QMAN_ENQUEUE_FLAG_WAIT_SYNC) !=
+	if ((flags & QMAN_ENQUEUE_FLAG_WAIT_SYNC) ==
 			QMAN_ENQUEUE_FLAG_WAIT_SYNC)
-		return;
-	/* So we're supposed to wait until the commit is consumed */
-	if (flags & QMAN_ENQUEUE_FLAG_WAIT_INT)
-		/* See __enqueue() (where this inline is called) as to why we're
-		 * ignoring return codes from wait_***(). */
-		wait_event_interruptible(affine_queue,
-					eqcr_completed(p, eq_poll));
-	else
-		wait_event(affine_queue, eqcr_completed(p, eq_poll));
+		wait_eqcr_commit(p, flags, eq_poll);
 }
 
 static inline void eqcr_abort(struct qman_portal *p)
@@ -1337,7 +1359,7 @@ static inline int __enqueue(struct qman_fq *fq, const struct qm_fd *fd,
 				u32 flags, struct qman_fq *orp_fq,
 				u16 orp_seqnum, int orp)
 {
-	struct qm_eqcr_entry *eq;
+	register struct qm_eqcr_entry *eq;
 	struct qman_portal *p;
 
 #ifdef CONFIG_FSL_QMAN_CHECKING
@@ -1352,9 +1374,9 @@ static inline int __enqueue(struct qman_fq *fq, const struct qm_fd *fd,
 	eq = try_eq_start(&p);
 	if (unlikely(!eq)) {
 		if (flags & QMAN_ENQUEUE_FLAG_WAIT) {
-			int ret = wait_eq_start(&p, &eq, flags);
-			if (ret)
-				return ret;
+			eq = wait_eq_start(&p, flags);
+			if (!eq)
+				return -EBUSY;
 		} else
 			return -EBUSY;
 	}
@@ -1363,12 +1385,14 @@ static inline int __enqueue(struct qman_fq *fq, const struct qm_fd *fd,
 	 * block (if the caller doesn't retry). The idea is to enqueue via the
 	 * ORP anyway, and let congestion take effect in h/w once
 	 * order-restoration has occurred. */
+#if 0
 	if (unlikely(!orp && (flags & QMAN_ENQUEUE_FLAG_WATCH_CGR) && p->cgrs &&
 			fq_isset(fq, QMAN_FQ_STATE_CGR_EN) &&
 			qman_cgrs_get(&p->cgrs[1], fq->cgr_groupid))) {
 		eqcr_abort(p);
 		return -EAGAIN;
 	}
+#endif
 	if (flags & QMAN_ENQUEUE_FLAG_DCA) {
 		u8 dca = QM_EQCR_DCA_ENABLE;
 		if (unlikely(flags & QMAN_ENQUEUE_FLAG_DCA_PARK))
@@ -1392,7 +1416,27 @@ static inline int __enqueue(struct qman_fq *fq, const struct qm_fd *fd,
 	}
 	eq->fqid = fq->fqid;
 	eq->tag = (u32)fq;
-	eq->fd = *fd;
+	/* gcc does a dreadful job of the following;
+	 *	eq->fd = *fd;
+	 * It causes the entire function to save/restore a wider range of
+	 * registers, and comes up with instruction-waste galore. This will do
+	 * until we can rework the function for better code-generation. */
+	{
+		int *eqfdptr = (int *)&eq->fd;
+		int *fdptr = (int *)fd;
+		int temp;
+		asm volatile (
+			"lwz %0,0(%2);"
+			"stw %0,0(%1);"
+			"lwz %0,4(%2);"
+			"stw %0,4(%1);"
+			"lwz %0,8(%2);"
+			"stw %0,8(%1);"
+			"lwz %0,12(%2);"
+			"stw %0,12(%1);"
+			: "=&r"(temp) : "b"(eqfdptr), "b"(fdptr)
+		);
+	}
 	/* Issue the enqueue command, and wait for sync if requested.
 	 * NB: design choice - the commit can't fail, only waiting can. Don't
 	 * propogate any failure if a signal arrives. Otherwise the caller can't
diff --git a/drivers/hwqueue/qman_low.c b/drivers/hwqueue/qman_low.c
index 10cde3a..a7f0fd3 100644
--- a/drivers/hwqueue/qman_low.c
+++ b/drivers/hwqueue/qman_low.c
@@ -134,6 +134,11 @@ static inline u8 cyc_diff(u8 ringsize, u8 first, u8 last)
 /* ---------------- */
 /* --- EQCR API --- */
 
+/* It's safer to code in terms of the 'eqcr' object than the 'portal' object,
+ * because the latter runs the risk of copy-n-paste errors from other code where
+ * we could manipulate some other structure within 'portal'. */
+#define EQCR_API_START()	register struct qm_eqcr *eqcr = &portal->eqcr
+
 /* Bit-wise logic to wrap a ring pointer by clearing the "carry bit" */
 #define EQCR_CARRYCLEAR(p) \
 	(void *)((unsigned long)(p) & (~(unsigned long)(QM_EQCR_SIZE << 6)))
@@ -156,14 +161,10 @@ static inline void EQCR_INC(struct qm_eqcr *eqcr)
 		eqcr->vbit ^= QM_EQCR_VERB_VBIT;
 }
 
-/* It's safer to code in terms of the 'eqcr' object than the 'portal' object,
- * because the latter runs the risk of copy-n-paste errors from other code where
- * we could manipulate some other structure within 'portal'. */
-#define eqcr	(&portal->eqcr)
-
 int qm_eqcr_init(struct qm_portal *portal, enum qm_eqcr_pmode pmode,
 		enum qm_eqcr_cmode cmode)
 {
+	EQCR_API_START();
 	u32 cfg;
 	u8 pi;
 
@@ -171,6 +172,7 @@ int qm_eqcr_init(struct qm_portal *portal, enum qm_eqcr_pmode pmode,
 		return -EBUSY;
 	eqcr->ring = ptr_OR(portal->addr.addr_ce, CL_EQCR);
 	eqcr->ci = qm_in(EQCR_CI_CINH) & (QM_EQCR_SIZE - 1);
+	qm_cl_invalidate(EQCR_CI);
 	pi = qm_in(EQCR_PI_CINH) & (QM_EQCR_SIZE - 1);
 	eqcr->cursor = eqcr->ring + pi;
 	eqcr->vbit = (qm_in(EQCR_PI_CINH) & QM_EQCR_SIZE) ?
@@ -192,6 +194,7 @@ EXPORT_SYMBOL(qm_eqcr_init);
 
 void qm_eqcr_finish(struct qm_portal *portal)
 {
+	EQCR_API_START();
 	u8 pi = qm_in(EQCR_PI_CINH) & (QM_EQCR_SIZE - 1);
 	u8 ci = qm_in(EQCR_CI_CINH) & (QM_EQCR_SIZE - 1);
 
@@ -208,6 +211,7 @@ EXPORT_SYMBOL(qm_eqcr_finish);
 
 struct qm_eqcr_entry *qm_eqcr_start(struct qm_portal *portal)
 {
+	EQCR_API_START();
 	QM_ASSERT(!eqcr->busy);
 	if (!eqcr->available)
 		return NULL;
@@ -221,6 +225,7 @@ EXPORT_SYMBOL(qm_eqcr_start);
 
 void qm_eqcr_abort(struct qm_portal *portal)
 {
+	EQCR_API_START();
 	QM_ASSERT(eqcr->busy);
 #ifdef CONFIG_FSL_QMAN_CHECKING
 	eqcr->busy = 0;
@@ -230,6 +235,7 @@ EXPORT_SYMBOL(qm_eqcr_abort);
 
 struct qm_eqcr_entry *qm_eqcr_pend_and_next(struct qm_portal *portal, u8 myverb)
 {
+	EQCR_API_START();
 	QM_ASSERT(eqcr->busy);
 	QM_ASSERT(eqcr->pmode != qm_eqcr_pvb);
 	if (eqcr->available == 1)
@@ -252,6 +258,7 @@ do { \
 
 void qm_eqcr_pci_commit(struct qm_portal *portal, u8 myverb)
 {
+	EQCR_API_START();
 	EQCR_COMMIT_CHECKS(eqcr);
 	QM_ASSERT(eqcr->pmode == qm_eqcr_pci);
 	eqcr->cursor->__dont_write_directly__verb = myverb | eqcr->vbit;
@@ -268,6 +275,7 @@ EXPORT_SYMBOL(qm_eqcr_pci_commit);
 
 void qm_eqcr_pce_prefetch(struct qm_portal *portal)
 {
+	EQCR_API_START();
 	QM_ASSERT(eqcr->pmode == qm_eqcr_pce);
 	qm_cl_invalidate(EQCR_PI);
 	qm_cl_touch_rw(EQCR_PI);
@@ -276,6 +284,7 @@ EXPORT_SYMBOL(qm_eqcr_pce_prefetch);
 
 void qm_eqcr_pce_commit(struct qm_portal *portal, u8 myverb)
 {
+	EQCR_API_START();
 	EQCR_COMMIT_CHECKS(eqcr);
 	QM_ASSERT(eqcr->pmode == qm_eqcr_pce);
 	eqcr->cursor->__dont_write_directly__verb = myverb | eqcr->vbit;
@@ -292,11 +301,14 @@ EXPORT_SYMBOL(qm_eqcr_pce_commit);
 
 void qm_eqcr_pvb_commit(struct qm_portal *portal, u8 myverb)
 {
+	EQCR_API_START();
+	struct qm_eqcr_entry *eqcursor;
 	EQCR_COMMIT_CHECKS(eqcr);
 	QM_ASSERT(eqcr->pmode == qm_eqcr_pvb);
 	lwsync();
-	eqcr->cursor->__dont_write_directly__verb = myverb | eqcr->vbit;
-	dcbf(eqcr->cursor);
+	eqcursor = eqcr->cursor;
+	eqcursor->__dont_write_directly__verb = myverb | eqcr->vbit;
+	dcbf(eqcursor);
 	EQCR_INC(eqcr);
 	eqcr->available--;
 #ifdef CONFIG_FSL_QMAN_CHECKING
@@ -307,6 +319,7 @@ EXPORT_SYMBOL(qm_eqcr_pvb_commit);
 
 u8 qm_eqcr_cci_update(struct qm_portal *portal)
 {
+	EQCR_API_START();
 	u8 diff, old_ci = eqcr->ci;
 	QM_ASSERT(eqcr->cmode == qm_eqcr_cci);
 	eqcr->ci = qm_in(EQCR_CI_CINH) & (QM_EQCR_SIZE - 1);
@@ -318,14 +331,15 @@ EXPORT_SYMBOL(qm_eqcr_cci_update);
 
 void qm_eqcr_cce_prefetch(struct qm_portal *portal)
 {
+	EQCR_API_START();
 	QM_ASSERT(eqcr->cmode == qm_eqcr_cce);
-	qm_cl_invalidate(EQCR_CI);
 	qm_cl_touch_ro(EQCR_CI);
 }
 EXPORT_SYMBOL(qm_eqcr_cce_prefetch);
 
 u8 qm_eqcr_cce_update(struct qm_portal *portal)
 {
+	EQCR_API_START();
 	u8 diff, old_ci = eqcr->ci;
 	QM_ASSERT(eqcr->cmode == qm_eqcr_cce);
 	eqcr->ci = qm_cl_in(EQCR_CI) & (QM_EQCR_SIZE - 1);
@@ -338,12 +352,14 @@ EXPORT_SYMBOL(qm_eqcr_cce_update);
 
 u8 qm_eqcr_get_ithresh(struct qm_portal *portal)
 {
+	EQCR_API_START();
 	return eqcr->ithresh;
 }
 EXPORT_SYMBOL(qm_eqcr_get_ithresh);
 
 void qm_eqcr_set_ithresh(struct qm_portal *portal, u8 ithresh)
 {
+	EQCR_API_START();
 	eqcr->ithresh = ithresh;
 	qm_out(EQCR_ITR, ithresh);
 }
@@ -351,12 +367,14 @@ EXPORT_SYMBOL(qm_eqcr_set_ithresh);
 
 u8 qm_eqcr_get_avail(struct qm_portal *portal)
 {
+	EQCR_API_START();
 	return eqcr->available;
 }
 EXPORT_SYMBOL(qm_eqcr_get_avail);
 
 u8 qm_eqcr_get_fill(struct qm_portal *portal)
 {
+	EQCR_API_START();
 	return QM_EQCR_SIZE - 1 - eqcr->available;
 }
 EXPORT_SYMBOL(qm_eqcr_get_fill);
@@ -371,6 +389,11 @@ EXPORT_SYMBOL(qm_eqcr_get_fill);
  * - consider moving other parameters to pointer if it could help (ci)
  */
 
+/* It's safer to code in terms of the 'dqrr' object than the 'portal' object,
+ * because the latter runs the risk of copy-n-paste errors from other code where
+ * we could manipulate some other structure within 'portal'. */
+#define DQRR_API_START()	register struct qm_dqrr *dqrr = &portal->dqrr
+
 #define DQRR_CARRYCLEAR(p) \
 	(void *)((unsigned long)(p) & (~(unsigned long)(QM_DQRR_SIZE << 6)))
 
@@ -384,15 +407,11 @@ static inline struct qm_dqrr_entry *DQRR_INC(struct qm_dqrr_entry *e)
 	return DQRR_CARRYCLEAR(e + 1);
 }
 
-/* It's safer to code in terms of the 'dqrr' object than the 'portal' object,
- * because the latter runs the risk of copy-n-paste errors from other code where
- * we could manipulate some other structure within 'portal'. */
-#define dqrr	(&portal->dqrr)
-
 int qm_dqrr_init(struct qm_portal *portal, enum qm_dqrr_dmode dmode,
 		enum qm_dqrr_pmode pmode, enum qm_dqrr_cmode cmode,
 		u8 max_fill, int stash_ring, int stash_data)
 {
+	DQRR_API_START();
 	const struct qm_portal_config *config = qm_portal_config(portal);
 	u32 cfg;
 
@@ -417,12 +436,12 @@ int qm_dqrr_init(struct qm_portal *portal, enum qm_dqrr_dmode dmode,
 	dqrr->dmode = dmode;
 	dqrr->pmode = pmode;
 	dqrr->cmode = cmode;
-#endif
 	dqrr->flags = 0;
 	if (stash_ring)
 		dqrr->flags |= QM_DQRR_FLAG_RE;
 	if (stash_data)
 		dqrr->flags |= QM_DQRR_FLAG_SE;
+#endif
 	cfg = (qm_in(CFG) & 0xff000f00) |
 		((max_fill & (QM_DQRR_SIZE - 1)) << 20) | /* DQRR_MF */
 		((dmode & 1) << 18) |			/* DP */
@@ -439,6 +458,7 @@ EXPORT_SYMBOL(qm_dqrr_init);
 
 void qm_dqrr_finish(struct qm_portal *portal)
 {
+	DQRR_API_START();
 	if (dqrr->ci != DQRR_PTR2IDX(dqrr->cursor))
 		pr_crit("Ignoring completed DQRR entries\n");
 	__qm_portal_unbind(portal, QM_BIND_DQRR);
@@ -447,16 +467,17 @@ EXPORT_SYMBOL(qm_dqrr_finish);
 
 void qm_dqrr_current_prefetch(struct qm_portal *portal)
 {
+	DQRR_API_START();
 	/* If ring entries get stashed, don't invalidate/prefetch */
-	if (!(dqrr->flags & QM_DQRR_FLAG_RE)) {
-		dcbi(dqrr->cursor);
-		dcbt_ro(dqrr->cursor);
-	}
+	QM_ASSERT(!(dqrr->flags & QM_DQRR_FLAG_RE));
+	dcbi(dqrr->cursor);
+	dcbt_ro(dqrr->cursor);
 }
 EXPORT_SYMBOL(qm_dqrr_current_prefetch);
 
 struct qm_dqrr_entry *qm_dqrr_current(struct qm_portal *portal)
 {
+	DQRR_API_START();
 	if (!dqrr->fill)
 		return NULL;
 	return dqrr->cursor;
@@ -465,12 +486,14 @@ EXPORT_SYMBOL(qm_dqrr_current);
 
 u8 qm_dqrr_cursor(struct qm_portal *portal)
 {
+	DQRR_API_START();
 	return DQRR_PTR2IDX(dqrr->cursor);
 }
 EXPORT_SYMBOL(qm_dqrr_cursor);
 
 u8 qm_dqrr_next(struct qm_portal *portal)
 {
+	DQRR_API_START();
 	QM_ASSERT(dqrr->fill);
 	dqrr->cursor = DQRR_INC(dqrr->cursor);
 	return --dqrr->fill;
@@ -479,6 +502,7 @@ EXPORT_SYMBOL(qm_dqrr_next);
 
 u8 qm_dqrr_pci_update(struct qm_portal *portal)
 {
+	DQRR_API_START();
 	u8 diff, old_pi = dqrr->pi;
 	QM_ASSERT(dqrr->pmode == qm_dqrr_pci);
 	dqrr->pi = qm_in(DQRR_PI_CINH) & (QM_DQRR_SIZE - 1);
@@ -490,6 +514,7 @@ EXPORT_SYMBOL(qm_dqrr_pci_update);
 
 void qm_dqrr_pce_prefetch(struct qm_portal *portal)
 {
+	DQRR_API_START();
 	QM_ASSERT(dqrr->pmode == qm_dqrr_pce);
 	qm_cl_invalidate(DQRR_PI);
 	qm_cl_touch_ro(DQRR_PI);
@@ -498,6 +523,7 @@ EXPORT_SYMBOL(qm_dqrr_pce_prefetch);
 
 u8 qm_dqrr_pce_update(struct qm_portal *portal)
 {
+	DQRR_API_START();
 	u8 diff, old_pi = dqrr->pi;
 	QM_ASSERT(dqrr->pmode == qm_dqrr_pce);
 	dqrr->pi = qm_cl_in(DQRR_PI) & (QM_DQRR_SIZE - 1);
@@ -509,17 +535,17 @@ EXPORT_SYMBOL(qm_dqrr_pce_update);
 
 void qm_dqrr_pvb_prefetch(struct qm_portal *portal)
 {
+	DQRR_API_START();
 	QM_ASSERT(dqrr->pmode == qm_dqrr_pvb);
-	/* If ring entries get stashed, don't invalidate/prefetch */
-	if (!(dqrr->flags & QM_DQRR_FLAG_RE)) {
-		dcbi(ptr_OR(dqrr->ring, qm_cl(dqrr->pi)));
-		dcbt_ro(ptr_OR(dqrr->ring, qm_cl(dqrr->pi)));
-	}
+	QM_ASSERT(!(dqrr->flags & QM_DQRR_FLAG_RE));
+	dcbi(ptr_OR(dqrr->ring, qm_cl(dqrr->pi)));
+	dcbt_ro(ptr_OR(dqrr->ring, qm_cl(dqrr->pi)));
 }
 EXPORT_SYMBOL(qm_dqrr_pvb_prefetch);
 
 u8 qm_dqrr_pvb_update(struct qm_portal *portal)
 {
+	DQRR_API_START();
 	struct qm_dqrr_entry *res = ptr_OR(dqrr->ring, qm_cl(dqrr->pi));
 	QM_ASSERT(dqrr->pmode == qm_dqrr_pvb);
 	if ((res->verb & QM_DQRR_VERB_VBIT) == dqrr->vbit) {
@@ -535,6 +561,7 @@ EXPORT_SYMBOL(qm_dqrr_pvb_update);
 
 void qm_dqrr_cci_consume(struct qm_portal *portal, u8 num)
 {
+	DQRR_API_START();
 	QM_ASSERT(dqrr->cmode == qm_dqrr_cci);
 	dqrr->ci = (dqrr->ci + num) & (QM_DQRR_SIZE - 1);
 	qm_out(DQRR_CI_CINH, dqrr->ci);
@@ -543,6 +570,7 @@ EXPORT_SYMBOL(qm_dqrr_cci_consume);
 
 void qm_dqrr_cci_consume_to_current(struct qm_portal *portal)
 {
+	DQRR_API_START();
 	QM_ASSERT(dqrr->cmode == qm_dqrr_cci);
 	dqrr->ci = DQRR_PTR2IDX(dqrr->cursor);
 	qm_out(DQRR_CI_CINH, dqrr->ci);
@@ -551,6 +579,7 @@ EXPORT_SYMBOL(qm_dqrr_cci_consume_to_current);
 
 void qm_dqrr_cce_prefetch(struct qm_portal *portal)
 {
+	DQRR_API_START();
 	QM_ASSERT(dqrr->cmode == qm_dqrr_cce);
 	qm_cl_invalidate(DQRR_CI);
 	qm_cl_touch_rw(DQRR_CI);
@@ -559,6 +588,7 @@ EXPORT_SYMBOL(qm_dqrr_cce_prefetch);
 
 void qm_dqrr_cce_consume(struct qm_portal *portal, u8 num)
 {
+	DQRR_API_START();
 	QM_ASSERT(dqrr->cmode == qm_dqrr_cce);
 	dqrr->ci = (dqrr->ci + num) & (QM_DQRR_SIZE - 1);
 	qm_cl_out(DQRR_CI, dqrr->ci);
@@ -567,6 +597,7 @@ EXPORT_SYMBOL(qm_dqrr_cce_consume);
 
 void qm_dqrr_cce_consume_to_current(struct qm_portal *portal)
 {
+	DQRR_API_START();
 	QM_ASSERT(dqrr->cmode == qm_dqrr_cce);
 	dqrr->ci = DQRR_PTR2IDX(dqrr->cursor);
 	qm_cl_out(DQRR_CI, dqrr->ci);
@@ -575,6 +606,7 @@ EXPORT_SYMBOL(qm_dqrr_cce_consume_to_current);
 
 void qm_dqrr_cdc_consume_1(struct qm_portal *portal, u8 idx, int park)
 {
+	DQRR_API_START();
 	QM_ASSERT(dqrr->cmode == qm_dqrr_cdc);
 	QM_ASSERT(idx < QM_DQRR_SIZE);
 	qm_out(DQRR_DCAP, (0 << 8) |	/* S */
@@ -586,6 +618,7 @@ EXPORT_SYMBOL(qm_dqrr_cdc_consume_1);
 void qm_dqrr_cdc_consume_1ptr(struct qm_portal *portal, struct qm_dqrr_entry *dq,
 				int park)
 {
+	DQRR_API_START();
 	u8 idx = DQRR_PTR2IDX(dq);
 	QM_ASSERT(dqrr->cmode == qm_dqrr_cdc);
 	QM_ASSERT((dqrr->ring + idx) == dq);
@@ -598,6 +631,7 @@ EXPORT_SYMBOL(qm_dqrr_cdc_consume_1ptr);
 
 void qm_dqrr_cdc_consume_n(struct qm_portal *portal, u16 bitmask)
 {
+	DQRR_API_START();
 	QM_ASSERT(dqrr->cmode == qm_dqrr_cdc);
 	qm_out(DQRR_DCAP, (1 << 8) |		/* DQRR_DCAP::S */
 		((u32)bitmask << 16));		/* DQRR_DCAP::DCAP_CI */
@@ -606,6 +640,7 @@ EXPORT_SYMBOL(qm_dqrr_cdc_consume_n);
 
 u8 qm_dqrr_cdc_cci(struct qm_portal *portal)
 {
+	DQRR_API_START();
 	QM_ASSERT(dqrr->cmode == qm_dqrr_cdc);
 	return qm_in(DQRR_CI_CINH) & (QM_DQRR_SIZE - 1);
 }
@@ -613,6 +648,7 @@ EXPORT_SYMBOL(qm_dqrr_cdc_cci);
 
 void qm_dqrr_cdc_cce_prefetch(struct qm_portal *portal)
 {
+	DQRR_API_START();
 	QM_ASSERT(dqrr->cmode == qm_dqrr_cdc);
 	qm_cl_invalidate(DQRR_CI);
 	qm_cl_touch_ro(DQRR_CI);
@@ -621,6 +657,7 @@ EXPORT_SYMBOL(qm_dqrr_cdc_cce_prefetch);
 
 u8 qm_dqrr_cdc_cce(struct qm_portal *portal)
 {
+	DQRR_API_START();
 	QM_ASSERT(dqrr->cmode == qm_dqrr_cdc);
 	return qm_cl_in(DQRR_CI) & (QM_DQRR_SIZE - 1);
 }
@@ -628,6 +665,7 @@ EXPORT_SYMBOL(qm_dqrr_cdc_cce);
 
 u8 qm_dqrr_get_ci(struct qm_portal *portal)
 {
+	DQRR_API_START();
 	QM_ASSERT(dqrr->cmode != qm_dqrr_cdc);
 	return dqrr->ci;
 }
@@ -635,6 +673,7 @@ EXPORT_SYMBOL(qm_dqrr_get_ci);
 
 void qm_dqrr_park(struct qm_portal *portal, u8 idx)
 {
+	DQRR_API_START();
 	QM_ASSERT(dqrr->cmode != qm_dqrr_cdc);
 	qm_out(DQRR_DCAP, (0 << 8) |		/* S */
 		(1 << 6) |			/* PK */
@@ -644,6 +683,7 @@ EXPORT_SYMBOL(qm_dqrr_park);
 
 void qm_dqrr_park_ci(struct qm_portal *portal)
 {
+	DQRR_API_START();
 	QM_ASSERT(dqrr->cmode != qm_dqrr_cdc);
 	qm_out(DQRR_DCAP, (0 << 8) |		/* S */
 		(1 << 6) |			/* PK */
@@ -689,6 +729,7 @@ EXPORT_SYMBOL(qm_dqrr_pdqcr_get);
 
 u8 qm_dqrr_get_ithresh(struct qm_portal *portal)
 {
+	DQRR_API_START();
 	return dqrr->ithresh;
 }
 EXPORT_SYMBOL(qm_dqrr_get_ithresh);
@@ -716,6 +757,11 @@ EXPORT_SYMBOL(qm_dqrr_set_maxfill);
 /* -------------- */
 /* --- MR API --- */
 
+/* It's safer to code in terms of the 'mr' object than the 'portal' object,
+ * because the latter runs the risk of copy-n-paste errors from other code where
+ * we could manipulate some other structure within 'portal'. */
+#define MR_API_START()		register struct qm_mr *mr = &portal->mr
+
 #define MR_CARRYCLEAR(p) \
 	(void *)((unsigned long)(p) & (~(unsigned long)(QM_MR_SIZE << 6)))
 
@@ -729,14 +775,10 @@ static inline struct qm_mr_entry *MR_INC(struct qm_mr_entry *e)
 	return MR_CARRYCLEAR(e + 1);
 }
 
-/* It's safer to code in terms of the 'mr' object than the 'portal' object,
- * because the latter runs the risk of copy-n-paste errors from other code where
- * we could manipulate some other structure within 'portal'. */
-#define mr	(&portal->mr)
-
 int qm_mr_init(struct qm_portal *portal, enum qm_mr_pmode pmode,
 		enum qm_mr_cmode cmode)
 {
+	MR_API_START();
 	u32 cfg;
 
 	if (__qm_portal_bind(portal, QM_BIND_MR))
@@ -761,6 +803,7 @@ EXPORT_SYMBOL(qm_mr_init);
 
 void qm_mr_finish(struct qm_portal *portal)
 {
+	MR_API_START();
 	if (mr->ci != MR_PTR2IDX(mr->cursor))
 		pr_crit("Ignoring completed MR entries\n");
 	__qm_portal_unbind(portal, QM_BIND_MR);
@@ -769,6 +812,7 @@ EXPORT_SYMBOL(qm_mr_finish);
 
 void qm_mr_current_prefetch(struct qm_portal *portal)
 {
+	MR_API_START();
 	dcbi(mr->cursor);
 	dcbt_ro(mr->cursor);
 }
@@ -776,6 +820,7 @@ EXPORT_SYMBOL(qm_mr_current_prefetch);
 
 struct qm_mr_entry *qm_mr_current(struct qm_portal *portal)
 {
+	MR_API_START();
 	if (!mr->fill)
 		return NULL;
 	return mr->cursor;
@@ -784,12 +829,14 @@ EXPORT_SYMBOL(qm_mr_current);
 
 u8 qm_mr_cursor(struct qm_portal *portal)
 {
+	MR_API_START();
 	return MR_PTR2IDX(mr->cursor);
 }
 EXPORT_SYMBOL(qm_mr_cursor);
 
 u8 qm_mr_next(struct qm_portal *portal)
 {
+	MR_API_START();
 	QM_ASSERT(mr->fill);
 	mr->cursor = MR_INC(mr->cursor);
 	return --mr->fill;
@@ -798,6 +845,7 @@ EXPORT_SYMBOL(qm_mr_next);
 
 u8 qm_mr_pci_update(struct qm_portal *portal)
 {
+	MR_API_START();
 	u8 diff, old_pi = mr->pi;
 	QM_ASSERT(mr->pmode == qm_mr_pci);
 	mr->pi = qm_in(MR_PI_CINH);
@@ -809,6 +857,7 @@ EXPORT_SYMBOL(qm_mr_pci_update);
 
 void qm_mr_pce_prefetch(struct qm_portal *portal)
 {
+	MR_API_START();
 	QM_ASSERT(mr->pmode == qm_mr_pce);
 	qm_cl_invalidate(MR_PI);
 	qm_cl_touch_ro(MR_PI);
@@ -817,6 +866,7 @@ EXPORT_SYMBOL(qm_mr_pce_prefetch);
 
 u8 qm_mr_pce_update(struct qm_portal *portal)
 {
+	MR_API_START();
 	u8 diff, old_pi = mr->pi;
 	QM_ASSERT(mr->pmode == qm_mr_pce);
 	mr->pi = qm_cl_in(MR_PI) & (QM_MR_SIZE - 1);
@@ -828,6 +878,7 @@ EXPORT_SYMBOL(qm_mr_pce_update);
 
 void qm_mr_pvb_prefetch(struct qm_portal *portal)
 {
+	MR_API_START();
 	QM_ASSERT(mr->pmode == qm_mr_pvb);
 	dcbi(ptr_OR(mr->ring, qm_cl(mr->pi)));
 	dcbt_ro(ptr_OR(mr->ring, qm_cl(mr->pi)));
@@ -836,6 +887,7 @@ EXPORT_SYMBOL(qm_mr_pvb_prefetch);
 
 u8 qm_mr_pvb_update(struct qm_portal *portal)
 {
+	MR_API_START();
 	struct qm_mr_entry *res = ptr_OR(mr->ring, qm_cl(mr->pi));
 	QM_ASSERT(mr->pmode == qm_mr_pvb);
 	if ((res->verb & QM_MR_VERB_VBIT) == mr->vbit) {
@@ -851,6 +903,7 @@ EXPORT_SYMBOL(qm_mr_pvb_update);
 
 void qm_mr_cci_consume(struct qm_portal *portal, u8 num)
 {
+	MR_API_START();
 	QM_ASSERT(mr->cmode == qm_mr_cci);
 	mr->ci = (mr->ci + num) & (QM_MR_SIZE - 1);
 	qm_out(MR_CI_CINH, mr->ci);
@@ -859,6 +912,7 @@ EXPORT_SYMBOL(qm_mr_cci_consume);
 
 void qm_mr_cci_consume_to_current(struct qm_portal *portal)
 {
+	MR_API_START();
 	QM_ASSERT(mr->cmode == qm_mr_cci);
 	mr->ci = MR_PTR2IDX(mr->cursor);
 	qm_out(MR_CI_CINH, mr->ci);
@@ -867,6 +921,7 @@ EXPORT_SYMBOL(qm_mr_cci_consume_to_current);
 
 void qm_mr_cce_prefetch(struct qm_portal *portal)
 {
+	MR_API_START();
 	QM_ASSERT(mr->cmode == qm_mr_cce);
 	qm_cl_invalidate(MR_CI);
 	qm_cl_touch_rw(MR_CI);
@@ -875,6 +930,7 @@ EXPORT_SYMBOL(qm_mr_cce_prefetch);
 
 void qm_mr_cce_consume(struct qm_portal *portal, u8 num)
 {
+	MR_API_START();
 	QM_ASSERT(mr->cmode == qm_mr_cce);
 	mr->ci = (mr->ci + num) & (QM_MR_SIZE - 1);
 	qm_cl_out(MR_CI, mr->ci);
@@ -883,6 +939,7 @@ EXPORT_SYMBOL(qm_mr_cce_consume);
 
 void qm_mr_cce_consume_to_current(struct qm_portal *portal)
 {
+	MR_API_START();
 	QM_ASSERT(mr->cmode == qm_mr_cce);
 	mr->ci = MR_PTR2IDX(mr->cursor);
 	qm_cl_out(MR_CI, mr->ci);
@@ -891,12 +948,14 @@ EXPORT_SYMBOL(qm_mr_cce_consume_to_current);
 
 u8 qm_mr_get_ci(struct qm_portal *portal)
 {
+	MR_API_START();
 	return mr->ci;
 }
 EXPORT_SYMBOL(qm_mr_get_ci);
 
 u8 qm_mr_get_ithresh(struct qm_portal *portal)
 {
+	MR_API_START();
 	return mr->ithresh;
 }
 EXPORT_SYMBOL(qm_mr_get_ithresh);
@@ -914,10 +973,11 @@ EXPORT_SYMBOL(qm_mr_set_ithresh);
 /* It's safer to code in terms of the 'mc' object than the 'portal' object,
  * because the latter runs the risk of copy-n-paste errors from other code where
  * we could manipulate some other structure within 'portal'. */
-#define mc	(&portal->mc)
+#define MC_API_START()		register struct qm_mc *mc = &portal->mc
 
 int qm_mc_init(struct qm_portal *portal)
 {
+	MC_API_START();
 	if (__qm_portal_bind(portal, QM_BIND_MC))
 		return -EBUSY;
 	mc->cr = ptr_OR(portal->addr.addr_ce, CL_CR);
@@ -934,6 +994,7 @@ EXPORT_SYMBOL(qm_mc_init);
 
 void qm_mc_finish(struct qm_portal *portal)
 {
+	MC_API_START();
 	QM_ASSERT(mc->state == mc_idle);
 #ifdef CONFIG_FSL_QMAN_CHECKING
 	if (mc->state != mc_idle)
@@ -945,6 +1006,7 @@ EXPORT_SYMBOL(qm_mc_finish);
 
 struct qm_mc_command *qm_mc_start(struct qm_portal *portal)
 {
+	MC_API_START();
 	QM_ASSERT(mc->state == mc_idle);
 #ifdef CONFIG_FSL_QMAN_CHECKING
 	mc->state = mc_user;
@@ -956,6 +1018,7 @@ EXPORT_SYMBOL(qm_mc_start);
 
 void qm_mc_abort(struct qm_portal *portal)
 {
+	MC_API_START();
 	QM_ASSERT(mc->state == mc_user);
 #ifdef CONFIG_FSL_QMAN_CHECKING
 	mc->state = mc_idle;
@@ -965,6 +1028,7 @@ EXPORT_SYMBOL(qm_mc_abort);
 
 void qm_mc_commit(struct qm_portal *portal, u8 myverb)
 {
+	MC_API_START();
 	QM_ASSERT(mc->state == mc_user);
 	dcbi(mc->rr + mc->rridx);
 	lwsync();
@@ -979,6 +1043,7 @@ EXPORT_SYMBOL(qm_mc_commit);
 
 struct qm_mc_result *qm_mc_result(struct qm_portal *portal)
 {
+	MC_API_START();
 	struct qm_mc_result *rr = mc->rr + mc->rridx;
 	QM_ASSERT(mc->state == mc_hw);
 	/* The inactive response register's verb byte always returns zero until
diff --git a/drivers/hwqueue/qman_private.h b/drivers/hwqueue/qman_private.h
index 8875321..79ef384 100644
--- a/drivers/hwqueue/qman_private.h
+++ b/drivers/hwqueue/qman_private.h
@@ -52,8 +52,9 @@ struct qm_eqcr {
 /* DQRR state */
 struct qm_dqrr {
 	struct qm_dqrr_entry *ring, *cursor;
-	u8 pi, ci, fill, ithresh, vbit, flags;
+	u8 pi, ci, fill, ithresh, vbit;
 #ifdef CONFIG_FSL_QMAN_CHECKING
+	u8 flags;
 	enum qm_dqrr_dmode dmode;
 	enum qm_dqrr_pmode pmode;
 	enum qm_dqrr_cmode cmode;
@@ -128,16 +129,8 @@ static inline void put_affine_portal(void)
 {
 	put_cpu_var(qman_affine_portal);
 }
-#define QMAN_PORTAL_FLAG_IRQ         0x00000001 /* use interrupt handler */
-#define QMAN_PORTAL_FLAG_IRQ_FAST    0x00000002 /* ... for fast-path too! */
-#define QMAN_PORTAL_FLAG_DCA         0x00000004 /* use DCA */
-#define QMAN_PORTAL_FLAG_LOCKED      0x00000008 /* multi-core locking */
-#define QMAN_PORTAL_FLAG_NOTAFFINE   0x00000010 /* not cpu-default portal */
-#define QMAN_PORTAL_FLAG_RSTASH      0x00000020 /* enable DQRR entry stashing */
-#define QMAN_PORTAL_FLAG_DSTASH      0x00000040 /* enable data stashing */
-#define QMAN_PORTAL_FLAG_RECOVER     0x00000080 /* recovery mode */
-#define QMAN_PORTAL_FLAG_WAIT        0x00000100 /* for recovery; can wait */
-#define QMAN_PORTAL_FLAG_WAIT_INT    0x00000200 /* for wait; interruptible */
+#define QMAN_PORTAL_FLAG_RSTASH      0x00000001 /* enable DQRR entry stashing */
+#define QMAN_PORTAL_FLAG_DSTASH      0x00000002 /* enable data stashing */
 struct qman_portal *qman_create_portal(struct qm_portal *portal, u32 flags,
 			const struct qman_cgrs *cgrs,
 			const struct qman_fq_cb *null_cb);
diff --git a/drivers/hwqueue/qman_sys.h b/drivers/hwqueue/qman_sys.h
index 9c6c72d..61b02b5 100644
--- a/drivers/hwqueue/qman_sys.h
+++ b/drivers/hwqueue/qman_sys.h
@@ -41,6 +41,25 @@
 #define QM_ASSERT(x) BM_ASSERT(x)
 #include "../hwalloc/bman_sys.h"
 
+/* do slow-path processing via IRQ */
+#define CONFIG_FSL_QMAN_PORTAL_FLAG_IRQ_SLOW
+
+/* do not do fast-path processing via IRQ */
+#define CONFIG_FSL_QMAN_PORTAL_FLAG_IRQ_FAST
+
+/* portals aren't SMP-locked, they're core-affine */
+#undef CONFIG_FSL_QMAN_PORTAL_FLAG_LOCKED
+
+/* portals do not initialise in recovery mode */
+#undef CONFIG_FSL_QMAN_PORTAL_FLAG_RECOVER
+
+#if defined(CONFIG_FSL_QMAN_PORTAL_FLAG_IRQ_SLOW) || \
+		defined(CONFIG_FSL_QMAN_PORTAL_FLAG_IRQ_FAST)
+#define CONFIG_FSL_QMAN_HAVE_IRQ
+#else
+#undef CONFIG_FSL_QMAN_HAVE_IRQ
+#endif
+
 /************/
 /* RB-trees */
 /************/
diff --git a/drivers/hwqueue/qman_test_high.c b/drivers/hwqueue/qman_test_high.c
index 2427c53..ce2584a 100644
--- a/drivers/hwqueue/qman_test_high.c
+++ b/drivers/hwqueue/qman_test_high.c
@@ -123,7 +123,6 @@ static void do_enqueues(struct qman_fq *fq)
 {
 	unsigned int loop;
 	for (loop = 0; loop < NUM_ENQUEUES; loop++) {
-		pr_info("Enqueue: %08x\n", fd.addr_lo);
 		if (qman_enqueue(fq, &fd, QMAN_ENQUEUE_FLAG_WAIT |
 				(((loop + 1) == NUM_ENQUEUES) ?
 				QMAN_ENQUEUE_FLAG_WAIT_SYNC : 0)))
@@ -144,8 +143,6 @@ void qman_test_high(void)
 	qman_cgrs_init(&cgrs);
 	qman_cgrs_set(&cgrs, CGR_ID);
 
-	pr_info("high-level test, start ccmode\n");
-
 	/* Initialise (parked) FQ */
 	if (qman_create_fq(0, FQ_FLAGS, fq))
 		panic("qman_create_fq() failed\n");
@@ -194,8 +191,6 @@ static enum qman_cb_dqrr_result cb_dqrr(struct qman_portal *p,
 					struct qman_fq *fq,
 					const struct qm_dqrr_entry *dq)
 {
-	pr_info("Dequeue: %08x (%s)\n", dq->fd.addr_lo,
-		(dq->stat & QM_DQRR_STAT_UNSCHEDULED) ? "VDQCR" : "SDQCR");
 	if (fd_cmp(&fd_dq, &dq->fd)) {
 		pr_err("BADNESS: dequeued frame doesn't match;\n");
 		BUG();
diff --git a/drivers/hwqueue/qman_test_hotpotato.c b/drivers/hwqueue/qman_test_hotpotato.c
index 28b98bc..017e8b0 100644
--- a/drivers/hwqueue/qman_test_hotpotato.c
+++ b/drivers/hwqueue/qman_test_hotpotato.c
@@ -42,7 +42,7 @@
  * organisation of FQIDs is such that the HP_PER_CPU*NUM_CPUS handlers will
  * shuttle a "hot potato" frame around them such that every forwarding action
  * moves it from one cpu to another. (The use of more than one handler per cpu
- * is to allow enough handlers/FQs to truly test the significance of cachine -
+ * is to allow enough handlers/FQs to truly test the significance of caching -
  * ie. when cache-expiries are occuring.)
  *
  * The "hot potato" frame content will be HP_NUM_WORDS*4 bytes in size, and the
@@ -65,18 +65,18 @@
  *    into 'hp_cpu_list'. Specifically, set processor_id, allocate HP_PER_CPU
  *    handlers and link-list them (but do no other handler setup).
  *
- * 2. scan over 'hp_list' HP_PER_CPU times, the first time sets each hp_cpu's
- *    'iterator' to point to its first handler. With each loop, allocate rx/tx
- *    FQIDs and mixer values to the hp_cpu's iterator handler and advance the
- *    iterator for the next loop. This includes a final fixup, which connects
- *    the last handler to the first (and which is why phase 2 and 3 are
- *    separate).
+ * 2. scan over 'hp_cpu_list' HP_PER_CPU times, the first time sets each
+ *    hp_cpu's 'iterator' to point to its first handler. With each loop,
+ *    allocate rx/tx FQIDs and mixer values to the hp_cpu's iterator handler
+ *    and advance the iterator for the next loop. This includes a final fixup,
+ *    which connects the last handler to the first (and which is why phase 2
+ *    and 3 are separate).
  *
- * 3. scan over 'hp_list' HP_PER_CPU times, the first time sets each hp_cpu's
- *    'iterator' to point to its first handler. With each loop, initialise FQ
- *    objects and advance the iterator for the next loop. Moreover, do this
- *    initialisation on the cpu it applies to so that Rx FQ initialisation
- *    targets the correct cpu.
+ * 3. scan over 'hp_cpu_list' HP_PER_CPU times, the first time sets each
+ *    hp_cpu's 'iterator' to point to its first handler. With each loop,
+ *    initialise FQ objects and advance the iterator for the next loop.
+ *    Moreover, do this initialisation on the cpu it applies to so that Rx FQ
+ *    initialisation targets the correct cpu.
  */
 
 struct hp_handler {
@@ -408,6 +408,11 @@ static void send_first_frame(void *ignore)
 
 void qman_test_hotpotato(void)
 {
+	if (num_online_cpus() < 2) {
+		pr_info("qman_test_hotpotato, skipping - only 1 cpu\n");
+		return;
+	}
+
 	pr_info("qman_test_hotpotato starting\n");
 
 	hp_cpu_list_length = 0;
diff --git a/drivers/match/pme2_ctrl.c b/drivers/match/pme2_ctrl.c
index 5bad83d..ce9fb6c 100644
--- a/drivers/match/pme2_ctrl.c
+++ b/drivers/match/pme2_ctrl.c
@@ -696,6 +696,30 @@ int pme_attr_set(enum pme_attr attr, u32 val)
 	case pme_attr_dxemecc1th:
 		pme_out(global_pme, DXEMECC1TH, val);
 		break;
+	case pme_attr_esr:
+		pme_out(global_pme, ESR, val);
+		break;
+	case pme_attr_pehd:
+		pme_out(global_pme, PEHD, val);
+		break;
+	case pme_attr_ecc1bes:
+		pme_out(global_pme, ECC1BES, val);
+		break;
+	case pme_attr_ecc2bes:
+		pme_out(global_pme, ECC2BES, val);
+		break;
+	case pme_attr_miace:
+		pme_out(global_pme, MIA_CE, val);
+		break;
+	case pme_attr_miacr:
+		pme_out(global_pme, MIA_CR, val);
+		break;
+	case pme_attr_cdcr:
+		pme_out(global_pme, CDCR, val);
+		break;
+	case pme_attr_pmtr:
+		pme_out(global_pme, PMTR, val);
+		break;
 
 	default:
 		pr_err("pme: Unknown attr %u\n", attr);
@@ -837,7 +861,9 @@ int pme_attr_get(enum pme_attr attr, u32 *val)
 		/* bits 24..31 */
 		attr_val = pme_in(global_pme, SREC);
 		/* clear unwanted bits in val*/
-		attr_val &= 0x000003FF;
+		attr_val &= 0x000000FF;
+		/* Multiply by 256 */
+		attr_val <<= 8;
 		break;
 
 	case pme_attr_sre_session_ctx_num: {
@@ -1058,6 +1084,78 @@ int pme_attr_get(enum pme_attr attr, u32 *val)
 		attr_val = pme_in(global_pme, ISR);
 		break;
 
+	case pme_attr_ecr0:
+		attr_val = pme_in(global_pme, ECR0);
+		break;
+
+	case pme_attr_ecr1:
+		attr_val = pme_in(global_pme, ECR1);
+		break;
+
+	case pme_attr_esr:
+		attr_val = pme_in(global_pme, ESR);
+		break;
+
+	case pme_attr_pmstat:
+		attr_val = pme_in(global_pme, PMSTAT);
+		break;
+
+	case pme_attr_pehd:
+		attr_val = pme_in(global_pme, PEHD);
+		break;
+
+	case pme_attr_ecc1bes:
+		attr_val = pme_in(global_pme, ECC1BES);
+		break;
+
+	case pme_attr_ecc2bes:
+		attr_val = pme_in(global_pme, ECC2BES);
+		break;
+
+	case pme_attr_eccaddr:
+		attr_val = pme_in(global_pme, ECCADDR);
+		break;
+
+	case pme_attr_ecccode:
+		attr_val = pme_in(global_pme, ECCCODE);
+		break;
+
+	case pme_attr_miace:
+		attr_val = pme_in(global_pme, MIA_CE);
+		break;
+
+	case pme_attr_miacr:
+		attr_val = pme_in(global_pme, MIA_CR);
+		break;
+
+	case pme_attr_cdcr:
+		attr_val = pme_in(global_pme, CDCR);
+		break;
+
+	case pme_attr_pmtr:
+		attr_val = pme_in(global_pme, PMTR);
+		break;
+
+	case pme_attr_faconf:
+		attr_val = pme_in(global_pme, FACONF);
+		break;
+
+	case pme_attr_pdsrbah:
+		attr_val = pme_in(global_pme, PDSRBAH);
+		break;
+
+	case pme_attr_pdsrbal:
+		attr_val = pme_in(global_pme, PDSRBAL);
+		break;
+
+	case pme_attr_scbarh:
+		attr_val = pme_in(global_pme, SCBARH);
+		break;
+
+	case pme_attr_scbarl:
+		attr_val = pme_in(global_pme, SCBARL);
+		break;
+
 	default:
 		pr_err("pme: Unknown attr %u\n", attr);
 		return -EINVAL;
diff --git a/drivers/match/pme2_db.c b/drivers/match/pme2_db.c
index 2696c80..42fdad4 100644
--- a/drivers/match/pme2_db.c
+++ b/drivers/match/pme2_db.c
@@ -69,6 +69,23 @@ static void db_cb(struct pme_ctx *ctx, const struct qm_fd *fd,
 	complete(&token->cb_done);
 }
 
+struct ctrl_op {
+	struct pme_ctx_ctrl_token ctx_ctr;
+	struct completion cb_done;
+	enum pme_status cmd_status;
+	u8 res_flag;
+};
+
+static void ctrl_cb(struct pme_ctx *ctx, const struct qm_fd *fd,
+		struct pme_ctx_ctrl_token *token)
+{
+	struct ctrl_op *ctrl = (struct ctrl_op *)token;
+	pr_info("pme2_test_high: ctrl_cb() invoked, fd;!\n");
+	ctrl->cmd_status = pme_fd_res_status(fd);
+	ctrl->res_flag = pme_fd_res_flags(fd);
+	complete(&ctrl->cb_done);
+}
+
 static int exclusive_inc(struct file *fp, struct db_session *db)
 {
 	int ret;
@@ -259,7 +276,14 @@ free_tx_data:
 static int execute_nop(struct file *fp, struct db_session *db)
 {
 	int ret = 0;
-	ret = pme_ctx_ctrl_nop(&db->ctx, PME_CTX_OP_WAIT|PME_CTX_OP_WAIT_INT);
+	struct ctrl_op ctx_ctrl =  {
+		.ctx_ctr.cb = ctrl_cb
+	};
+	init_completion(&ctx_ctrl.cb_done);
+
+	ret = pme_ctx_ctrl_nop(&db->ctx, PME_CTX_OP_WAIT|PME_CTX_OP_WAIT_INT,
+			&ctx_ctrl.ctx_ctr);
+	wait_for_completion(&ctx_ctrl.cb_done);
 	/* pme_ctx_ctrl_nop() can be interrupted waiting for the response
 	 * of the NOP. In this scenario, 0 is returned. The only way to
 	 * determine that is was interrupted is to check for signal_pending()
diff --git a/drivers/match/pme2_high.c b/drivers/match/pme2_high.c
index 2d027af..ff9bae4 100644
--- a/drivers/match/pme2_high.c
+++ b/drivers/match/pme2_high.c
@@ -54,33 +54,6 @@
  * is decremented and the context is re-disabled. ENABLING is unset once
  * pme_ctx_enable() completes.
  *
- * CTRL: set by pme_ctx_ctrl_***() at any point that is not dead, disabling,
- * disabled, or already in ctrl (the ctrl state is a one-shot usage that locks
- * in the caller against other attempts to issue ctrl). The function will then
- * atomic_dec_and_test() the ref, and wait (if necessary) for refs==0.  If
- * waiting fails, CTRL is unset and the ref is incremented. The eventual
- * completion event will clear CTRL and increment the reference count.
- *
- * CTRL_ISSUED: set by pme_ctx_ctrl_***() once CTRL is set and refs==0, just
- * before issuing the ctrl command. The eventual completion event uses
- * CTRL_ISSUED to know it is handling a ctrl command response, and will clear
- * CTRL and CTRL_ISSUED. If the enqueue fails, CTRL and CTRL_ISSUED are cleared.
- * NB, the completion handling can't use CTRL to determine the type of result,
- * because CTRL is set before the pipeline is cleared of outstanding non-ctrl
- * work - this is why the CTRL_ISSUED flag exists, it's only set once the
- * pipeline is empty so completion handling can use it as a code point.
- *
- * (Side-note, the pme_ctx_ctrl_***() APIs use a UID in pme_ctx that gets bumped
- * by every ctrl completion, and the caller can wait for this UID to change to
- * indicate that its command has completed. Although the completion also clears
- * CTRL, another caller may be waiting to set CTRL and may wake_up() before the
- * existing caller can detect it was cleared, so the UID mechanism bypasses this
- * issue.)
- *
- * FCW_DEALLOC: set by pme_ctx_ctrl_update_flow() before the ctrl command is
- * issued if the completion callback should deallocate residue. (This means the
- * API can return asynchronously if needed, it's not required to wait.)
- *
  * RECONFIG: set by pme_ctx_reconfigure_[rt]x() provided the context is
  * disabled, not dead, and not already in reconfig. RECONFIG is cleared prior to
  * the function returning.
@@ -89,16 +62,12 @@
  * the ctx 'flags', and callers can rely on the following implications to reduce
  * the number of flags in the masks being passed in;
  * 	DISABLED implies DISABLING (and enable will clear both)
- * 	CTRL_ISSUED implies CTRL (and completion will clear both)
  */
 
 #define PME_CTX_FLAG_DEAD        0x80000000
 #define PME_CTX_FLAG_DISABLING   0x40000000
 #define PME_CTX_FLAG_DISABLED    0x20000000
 #define PME_CTX_FLAG_ENABLING    0x10000000
-#define PME_CTX_FLAG_CTRL        0x08000000
-#define PME_CTX_FLAG_CTRL_ISSUED 0x04000000
-#define PME_CTX_FLAG_FCW_DEALLOC 0x02000000
 #define PME_CTX_FLAG_RECONFIG    0x01000000
 
 #define PME_CTX_FLAG_PRIVATE     0xff000000
@@ -148,6 +117,40 @@ static spinlock_t exclusive_lock = SPIN_LOCK_UNLOCKED;
 static unsigned int exclusive_refs;
 static struct pme_ctx *exclusive_ctx;
 
+/* Index 0..255, bools do indicated which errors are serious
+ * 0x40, 0x41, 0x48, 0x49, 0x4c, 0x4e, 0x4f, 0x50, 0x51, 0x59, 0x5a, 0x5b,
+ * 0x5c, 0x5d, 0x5f, 0x60, 0x80, 0xc0, 0xc1, 0xc2, 0xc4, 0xd2,
+ * 0xd4, 0xd5, 0xd7, 0xd9, 0xda, 0xe0, 0xe7
+ */
+static u8 serious_error_vec[] = {
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x01, 0x01, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x01, 0x01, 0x00, 0x00, 0x01, 0x00, 0x01, 0x01,
+	0x01, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x01,
+	0x01, 0x01, 0x01, 0x01, 0x00, 0x01, 0x01, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x01, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x01, 0x01, 0x01, 0x00, 0x01, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x01, 0x00, 0x01, 0x01, 0x00, 0x01, 0x00, 0x01, 0x01, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	0x00, 0x00, 0x00, 0x00, 0x00, 0x00
+};
+
 /* TODO: this is hitting the rx FQ with a large blunt instrument, ie. park()
  * does a retire, query, oos, and (re)init. It's possible to force-eligible the
  * rx FQ instead, then use a DCA_PK within the cb_dqrr() callback to park it.
@@ -249,7 +252,6 @@ int pme_ctx_init(struct pme_ctx *ctx, u32 flags, u32 bpid, u8 qosin,
 	init_waitqueue_head(&ctx->queue);
 	INIT_LIST_HEAD(&ctx->tokens);
 	ctx->seq_num = 0;
-	ctx->uid = 0xdeadbeef;
 	ctx->fqin = NULL;
 	ctx->hw_flow = NULL;
 	ctx->hw_residue = NULL;
@@ -359,8 +361,8 @@ int pme_ctx_disable(struct pme_ctx *ctx, u32 flags)
 	struct qm_mcc_initfq initfq;
 	int ret;
 
-	ret = do_flags(ctx, 0, PME_CTX_FLAG_DISABLING | PME_CTX_FLAG_CTRL,
-			PME_CTX_FLAG_DISABLING, 0);
+	ret = do_flags(ctx, 0, PME_CTX_FLAG_DISABLING, PME_CTX_FLAG_DISABLING,
+			0);
 	if (ret)
 		return ret;
 	/* Make sure the pipeline is empty */
@@ -495,7 +497,7 @@ static inline int get_exclusive(struct pme_ctx *ctx, u32 flags)
 		ret = __try_exclusive(ctx);
 	return ret;
 }
-/* Used for 'ctrl' and 'work' APIs, convert PME->QMAN wait flags. The PME and
+/* Used for 'work' APIs, convert PME->QMAN wait flags. The PME and
  * QMAN "wait" flags have been aligned so that the below conversion should
  * compile with good straight-line speed. */
 static inline u32 ctrl2eq(u32 flags)
@@ -503,195 +505,6 @@ static inline u32 ctrl2eq(u32 flags)
 	return flags & (QMAN_ENQUEUE_FLAG_WAIT | QMAN_ENQUEUE_FLAG_WAIT_INT);
 }
 
-static inline void release_ctrl(struct pme_ctx *ctx)
-{
-	atomic_inc(&ctx->refs);
-	do_flags(ctx, 0, 0, 0, PME_CTX_FLAG_CTRL | PME_CTX_FLAG_CTRL_ISSUED |
-					PME_CTX_FLAG_FCW_DEALLOC);
-	wake_up(&ctx->queue);
-}
-static int __try_ctrl(struct pme_ctx *ctx)
-{
-	return do_flags(ctx, 0,
-		PME_CTX_FLAG_DEAD | PME_CTX_FLAG_DISABLING | PME_CTX_FLAG_CTRL,
-		PME_CTX_FLAG_CTRL, 0);
-}
-/* Use this macro as the wait expression because we don't want to continue
- * looping if the DEAD/DISABLING flags are set, we only loop if CTRL is held
- * because we wait for it to be cleared.
- * IMPLEMENTATION NOTE: don't use a return code from wait_event_interruptible(),
- * the key is the return value of the last call to __try_ctrl(). */
-#define try_ctrl(ret, ctx) \
-	(!(ret = __try_ctrl(ctx)) || (ctx->flags & (PME_CTX_FLAG_DEAD | \
-					PME_CTX_FLAG_DISABLING)))
-static int get_ctrl(struct pme_ctx *ctx, u32 flags)
-{
-	int ret;
-	/* Lock the CTRL flag in the context */
-	if (flags & PME_CTX_OP_WAIT) {
-		if (flags & PME_CTX_OP_WAIT_INT) {
-			ret = -EINTR;
-			wait_event_interruptible(ctx->queue,
-					try_ctrl(ret, ctx));
-		} else
-			wait_event(ctx->queue, try_ctrl(ret, ctx));
-	} else
-		ret = __try_ctrl(ctx);
-	if (ret)
-		return ret;
-	/* Make sure the pipeline is empty */
-	atomic_dec(&ctx->refs);
-	ret = empty_pipeline(ctx, flags);
-	if (ret)
-		release_ctrl(ctx);
-	return ret;
-}
-/* unlike do_work() (which encapsulates get_work(), get_exclusive(), and
- * qman_enqueue()), the do_ctrl() wrapper only encapsulates get_exclusive() and
- * qman_enqueue(). This is because the ctrl functions need to manipulate 'ctx'
- * between get_ctrl() and qman_enqueue(), so they peel the outer layer of
- * the onion (get_ctrl) themselves. */
-static int do_ctrl(struct pme_ctx *ctx, u32 flags, const struct qm_fd *fd,
-			u32 *uid)
-{
-	int ret;
-	if (ctx->flags & PME_CTX_FLAG_EXCLUSIVE) {
-		ret = get_exclusive(ctx, flags);
-		if (ret)
-			return ret;
-	}
-	*uid = ctx->uid;
-	do_flags(ctx, 0, 0, PME_CTX_FLAG_CTRL_ISSUED, 0);
-	ret = qman_enqueue(ctx->fqin, fd, ctrl2eq(flags));
-	if (ret) {
-		if (ctx->flags & PME_CTX_FLAG_EXCLUSIVE)
-			release_exclusive(ctx);
-	}
-	return ret;
-}
-/* Swallow any wait failure, the ctrl command has already been sent and will go
- * to the PME, so returning a wait error would leave the caller thinking the
- * command didn't happen. If they care about whether they got interrupted
- * before completion, they should check signal_pending() on return. */
-static void wait_ctrl_completion(struct pme_ctx *ctx, u32 flags, u32 uid)
-{
-	if (flags & PME_CTX_OP_WAIT) {
-		if (flags & PME_CTX_OP_WAIT_INT)
-			wait_event_interruptible(ctx->queue,
-					ctx->uid != uid);
-		else
-			wait_event(ctx->queue, ctx->uid != uid);
-	}
-}
-
-int pme_ctx_ctrl_update_flow(struct pme_ctx *ctx, u32 flags,
-			struct pme_flow *params)
-{
-	struct qm_fd fd;
-	u32 uid;
-	int ret, allocres = 0;
-
-	BUG_ON(ctx->flags & (PME_CTX_FLAG_DIRECT | PME_CTX_FLAG_PMTCC));
-	ret = get_ctrl(ctx, flags);
-	if (ret)
-		return ret;
-	if (flags & PME_CTX_OP_RESETRESLEN) {
-		if (ctx->hw_residue) {
-			params->ren = 1;
-			flags |= PME_CMD_FCW_RES;
-		} else
-			flags &= ~PME_CMD_FCW_RES;
-	}
-	/* allocate residue memory if it is being added */
-	if ((flags & PME_CMD_FCW_RES) && params->ren && !ctx->hw_residue) {
-		ctx->hw_residue = pme_hw_residue_new();
-		if (!ctx->hw_residue) {
-			release_ctrl(ctx);
-			return -ENOMEM;
-		}
-		allocres = 1;
-	}
-	/* enqueue the FCW command to PME */
-	memset(&fd, 0, sizeof(fd));
-	if (unlikely((flags & PME_CMD_FCW_RES) && !params->ren &&
-						ctx->hw_residue)) {
-		/* cb_dqrr() needs to deallocate residue on completion */
-		do_flags(ctx, 0, 0, PME_CTX_FLAG_FCW_DEALLOC, 0);
-		pme_fd_cmd_fcw(&fd, flags & PME_CMD_FCW_ALL, params, NULL);
-	} else
-		pme_fd_cmd_fcw(&fd, flags & PME_CMD_FCW_ALL, params,
-					ctx->hw_residue);
-	ret = do_ctrl(ctx, flags, &fd, &uid);
-	if (ret) {
-		if (allocres) {
-			pme_hw_residue_free(ctx->hw_residue);
-			ctx->hw_residue = NULL;
-		}
-		release_ctrl(ctx);
-		return ret;
-	}
-	wait_ctrl_completion(ctx, flags, uid);
-	return 0;
-}
-EXPORT_SYMBOL(pme_ctx_ctrl_update_flow);
-
-int pme_ctx_ctrl_read_flow(struct pme_ctx *ctx, u32 flags,
-			struct pme_flow *params)
-{
-	struct qm_fd fd;
-	u32 uid;
-	int ret;
-
-	BUG_ON(ctx->flags & (PME_CTX_FLAG_DIRECT | PME_CTX_FLAG_PMTCC));
-	/* This has to block, we can't accept a read flow context command being
-	 * orphaned in-flight */
-	might_sleep();
-	flags |= PME_CTX_OP_WAIT;
-	flags &= ~PME_CTX_OP_WAIT_INT;
-	ret = get_ctrl(ctx, flags);
-	if (ret)
-		return ret;
-	/* enqueue the FCR command to PME */
-	memset(&fd, 0, sizeof(fd));
-	pme_fd_cmd_fcr(&fd, params);
-	ret = do_ctrl(ctx, flags, &fd, &uid);
-	if (ret) {
-		release_ctrl(ctx);
-		return ret;
-	}
-	wait_ctrl_completion(ctx, flags, uid);
-	return 0;
-}
-EXPORT_SYMBOL(pme_ctx_ctrl_read_flow);
-
-int pme_ctx_ctrl_nop(struct pme_ctx *ctx, u32 flags)
-{
-	struct qm_fd fd;
-	u32 uid;
-	int ret;
-
-	ret = get_ctrl(ctx, flags);
-	if (ret)
-		return ret;
-	/* enqueue the NOP command to PME */
-	memset(&fd, 0, sizeof(fd));
-	pme_fd_cmd_nop(&fd);
-	ret = do_ctrl(ctx, flags, &fd, &uid);
-	if (ret) {
-		release_ctrl(ctx);
-		return ret;
-	}
-	wait_ctrl_completion(ctx, flags, uid);
-	return 0;
-}
-EXPORT_SYMBOL(pme_ctx_ctrl_nop);
-
-int pme_ctx_in_ctrl(struct pme_ctx *ctx)
-{
-	return ctx->flags & PME_CTX_FLAG_CTRL;
-}
-EXPORT_SYMBOL(pme_ctx_in_ctrl);
-
 static inline void release_work(struct pme_ctx *ctx)
 {
 	if (atomic_dec_and_test(&ctx->refs))
@@ -701,8 +514,8 @@ static inline void release_work(struct pme_ctx *ctx)
 static int __try_work(struct pme_ctx *ctx)
 {
 	atomic_inc(&ctx->refs);
-	if (unlikely(ctx->flags & (PME_CTX_FLAG_DEAD | PME_CTX_FLAG_DISABLING |
-						PME_CTX_FLAG_CTRL))) {
+	if (unlikely(ctx->flags & (PME_CTX_FLAG_DEAD |
+			PME_CTX_FLAG_DISABLING))) {
 		release_work(ctx);
 		if (ctx->flags & (PME_CTX_FLAG_DEAD | PME_CTX_FLAG_DISABLING))
 			return -EIO;
@@ -763,10 +576,74 @@ static int do_work(struct pme_ctx *ctx, u32 flags, struct qm_fd *fd,
 	return ret;
 }
 
+int pme_ctx_ctrl_update_flow(struct pme_ctx *ctx, u32 flags,
+		struct pme_flow *params,  struct pme_ctx_ctrl_token *token)
+{
+	struct qm_fd fd;
+
+	BUG_ON(ctx->flags & PME_CTX_FLAG_DIRECT);
+	token->base_token.cmd_type = pme_cmd_flow_write;
+
+	if (flags & PME_CTX_OP_RESETRESLEN) {
+		if (ctx->hw_residue) {
+			params->ren = 1;
+			flags |= PME_CMD_FCW_RES;
+		} else
+			flags &= ~PME_CMD_FCW_RES;
+	}
+	/* allocate residue memory if it is being added */
+	if ((flags & PME_CMD_FCW_RES) && params->ren && !ctx->hw_residue) {
+		ctx->hw_residue = pme_hw_residue_new();
+		if (!ctx->hw_residue)
+			return -ENOMEM;
+	}
+	/* enqueue the FCW command to PME */
+	memset(&fd, 0, sizeof(fd));
+	token->internal_flow_ptr = pme_hw_flow_new();
+	memcpy(token->internal_flow_ptr, params, sizeof(struct pme_flow));
+	pme_fd_cmd_fcw(&fd, flags & PME_CMD_FCW_ALL,
+			(struct pme_flow *)token->internal_flow_ptr,
+			ctx->hw_residue);
+	return do_work(ctx, flags, &fd, &token->base_token);
+}
+EXPORT_SYMBOL(pme_ctx_ctrl_update_flow);
+
+int pme_ctx_ctrl_read_flow(struct pme_ctx *ctx, u32 flags,
+		struct pme_flow *params, struct pme_ctx_ctrl_token *token)
+{
+	struct qm_fd fd;
+
+	BUG_ON(ctx->flags & (PME_CTX_FLAG_DIRECT | PME_CTX_FLAG_PMTCC));
+	token->base_token.cmd_type = pme_cmd_flow_read;
+	/* enqueue the FCR command to PME */
+	token->usr_flow_ptr = params;
+	token->internal_flow_ptr = pme_hw_flow_new();
+	memset(&fd, 0, sizeof(fd));
+	pme_fd_cmd_fcr(&fd, (struct pme_flow *)token->internal_flow_ptr);
+	return do_work(ctx, flags, &fd, &token->base_token);
+}
+EXPORT_SYMBOL(pme_ctx_ctrl_read_flow);
+
+int pme_ctx_ctrl_nop(struct pme_ctx *ctx, u32 flags,
+		struct pme_ctx_ctrl_token *token)
+{
+	struct qm_fd fd;
+
+	token->base_token.cmd_type = pme_cmd_nop;
+	/* enqueue the NOP command to PME */
+	memset(&fd, 0, sizeof(fd));
+	fd.addr_hi = 0xfeed;
+	fd.addr_lo = (u32)token;
+	pme_fd_cmd_nop(&fd);
+	return do_work(ctx, flags, &fd, &token->base_token);
+}
+EXPORT_SYMBOL(pme_ctx_ctrl_nop);
+
 int pme_ctx_scan(struct pme_ctx *ctx, u32 flags, struct qm_fd *fd, u32 args,
 		struct pme_ctx_token *token)
 {
 	BUG_ON(ctx->flags & PME_CTX_FLAG_PMTCC);
+	token->cmd_type = pme_cmd_scan;
 	pme_fd_cmd_scan(fd, args);
 	return do_work(ctx, flags, fd, token);
 }
@@ -776,6 +653,7 @@ int pme_ctx_pmtcc(struct pme_ctx *ctx, u32 flags, struct qm_fd *fd,
 		struct pme_ctx_token *token)
 {
 	BUG_ON(!(ctx->flags & PME_CTX_FLAG_PMTCC));
+	token->cmd_type = pme_cmd_pmtcc;
 	pme_fd_cmd_pmtcc(fd);
 	return do_work(ctx, flags, fd, token);
 }
@@ -839,28 +717,33 @@ static inline void cb_helper(struct qman_portal *portal, struct pme_ctx *ctx,
 				const struct qm_fd *fd, int error)
 {
 	struct pme_ctx_token *token;
+	struct pme_ctx_ctrl_token *ctrl_token;
 	/* Resist the urge to use "unlikely" - 'error' is a constant param to an
 	 * inline fn, so the compiler can collapse this completely. */
 	if (error)
 		do_flags(ctx, 0, 0, PME_CTX_FLAG_DEAD, 0);
-	/* The 'ctrl' case should be our slow-path */
-	if (unlikely(ctx->flags & PME_CTX_FLAG_CTRL_ISSUED)) {
-		if (ctx->flags & PME_CTX_FLAG_FCW_DEALLOC) {
-			pme_hw_residue_free(ctx->hw_residue);
-			ctx->hw_residue = NULL;
+	token = pop_matching_token(ctx, fd);
+	if (likely(token->cmd_type == pme_cmd_scan))
+		ctx->cb(ctx, fd, token);
+	else if (token->cmd_type == pme_cmd_pmtcc)
+		ctx->cb(ctx, fd, token);
+	else {
+		/* outcast ctx and call supplied callback */
+		ctrl_token = container_of(token, struct pme_ctx_ctrl_token,
+					base_token);
+		if (token->cmd_type == pme_cmd_flow_write) {
+			/* Release the allocated flow context */
+			pme_hw_flow_free(ctrl_token->internal_flow_ptr);
+		} else if (token->cmd_type == pme_cmd_flow_read) {
+			/* Copy read result */
+			memcpy(ctrl_token->usr_flow_ptr,
+				ctrl_token->internal_flow_ptr,
+				sizeof(struct pme_flow));
+			/* Release the allocated flow context */
+			pme_hw_flow_free(ctrl_token->internal_flow_ptr);
 		}
-		/* The caller will be waiting on this... */
-		ctx->uid++;
-		/* Switch out of CTRL mode */
-		release_ctrl(ctx);
-		if (ctx->flags & PME_CTX_FLAG_EXCLUSIVE)
-			release_exclusive(ctx);
-		return;
+		ctrl_token->cb(ctx, fd, ctrl_token);
 	}
-	/* This is a scan or PMTCC - the fast-path, normal, 99.99% case. We
-	 * detach the token for this command and pass it to the owner. */
-	token = pop_matching_token(ctx, fd);
-	ctx->cb(ctx, fd, token);
 	/* Consume the frame */
 	if (ctx->flags & PME_CTX_FLAG_EXCLUSIVE)
 		release_exclusive(ctx);
@@ -875,15 +758,20 @@ static inline void cb_helper(struct qman_portal *portal, struct pme_ctx *ctx,
 static enum qman_cb_dqrr_result cb_dqrr(struct qman_portal *portal,
 			struct qman_fq *fq, const struct qm_dqrr_entry *dq)
 {
-	enum pme_status status = pme_fd_res_status(&dq->fd);
+	u8 status = (u8)pme_fd_res_status(&dq->fd);
 	u8 flags = pme_fd_res_flags(&dq->fd);
 	struct pme_ctx *ctx = (struct pme_ctx *)fq;
 
-	if (unlikely((status != pme_status_ok) ||
-			(flags & PME_STATUS_UNRELIABLE)))
+	/* Put context into dead state is an unreliable or serious error is
+	 * received
+	 */
+	if (unlikely(flags & PME_STATUS_UNRELIABLE))
+		cb_helper(portal, ctx, &dq->fd, 1);
+	else if (unlikely((serious_error_vec[status])))
 		cb_helper(portal, ctx, &dq->fd, 1);
 	else
 		cb_helper(portal, ctx, &dq->fd, 0);
+
 	return qman_cb_dqrr_consume;
 }
 
diff --git a/drivers/match/pme2_low.c b/drivers/match/pme2_low.c
index 9fb47df..d03dce1 100644
--- a/drivers/match/pme2_low.c
+++ b/drivers/match/pme2_low.c
@@ -168,20 +168,11 @@ static const struct pme_flow default_sw_flow = {
 	.mlim = 0xffff
 };
 
-struct pme_flow *pme_sw_flow_new(void)
+void pme_sw_flow_init(struct pme_flow *flow)
 {
-	struct pme_flow *flow = kmem_cache_zalloc(slab_flow, GFP_KERNEL);
-	if (likely(flow))
-		memcpy(flow, &default_sw_flow, sizeof(*flow));
-	return flow;
-}
-EXPORT_SYMBOL(pme_sw_flow_new);
-
-void pme_sw_flow_free(struct pme_flow *p)
-{
-	kmem_cache_free(slab_flow, p);
+	memcpy(flow, &default_sw_flow, sizeof(*flow));
 }
-EXPORT_SYMBOL(pme_sw_flow_free);
+EXPORT_SYMBOL(pme_sw_flow_init);
 
 void pme_initfq(struct qm_mcc_initfq *initfq, struct pme_hw_flow *flow, u8 qos,
 		u8 rbpid, u32 rfqid)
@@ -232,7 +223,7 @@ void pme_fd_cmd_fcw(struct qm_fd *fd, u8 flags, struct pme_flow *flow,
 	fcw->cmd = pme_cmd_flow_write;
 	fcw->flags = flags;
 	if (flags & PME_CMD_FCW_RES) {
-		if (flow->ren) {
+		if (residue) {
 			dma_addr_t rptr;
 			rptr = residue_map(residue);
 			BUG_ON(!residue);
@@ -243,7 +234,6 @@ void pme_fd_cmd_fcw(struct qm_fd *fd, u8 flags, struct pme_flow *flow,
 			flow->rptr_hi = 0;
 			flow->rptr_lo = rptr;
 		} else {
-			BUG_ON(residue);
 			flow->rptr_hi = 0;
 			flow->rptr_lo = 0;
 		}
diff --git a/drivers/match/pme2_private.h b/drivers/match/pme2_private.h
index 83bddb8..07a26c0 100644
--- a/drivers/match/pme2_private.h
+++ b/drivers/match/pme2_private.h
@@ -91,13 +91,6 @@ struct pme_context_b {
 	u32 rfqid:24;
 } __packed;
 
-enum pme_cmd_type {
-	pme_cmd_nop = 0x7,
-	pme_cmd_flow_read = 0x5,	/* aka FCR */
-	pme_cmd_flow_write = 0x4,	/* aka FCW */
-	pme_cmd_pmtcc = 0x1,
-	pme_cmd_scan = 0
-};
 
 /* This is the 32-bit frame "cmd/status" field, sent to PME */
 union pme_cmd {
diff --git a/drivers/match/pme2_regs.h b/drivers/match/pme2_regs.h
index 05799ff..9fd309a 100644
--- a/drivers/match/pme2_regs.h
+++ b/drivers/match/pme2_regs.h
@@ -42,8 +42,8 @@
 #define PME_REG_TRUNCI		0x024
 #define PME_REG_RBC		0x028
 #define PME_REG_ESR		0x02C
-#define PME_REG_ECR1		0x030
-#define PME_REG_ECR0		0x034
+#define PME_REG_ECR0		0x030
+#define PME_REG_ECR1		0x034
 #define PME_REG_EFQC		0x050
 #define PME_REG_FACONF		0x060
 #define PME_REG_PMSTAT		0x064
diff --git a/drivers/match/pme2_scan.c b/drivers/match/pme2_scan.c
index ce23a50..de38ffa 100644
--- a/drivers/match/pme2_scan.c
+++ b/drivers/match/pme2_scan.c
@@ -80,6 +80,23 @@ struct cmd_token {
 	u8 done;
 };
 
+struct ctrl_op {
+	struct pme_ctx_ctrl_token ctx_ctr;
+	struct completion cb_done;
+	enum pme_status cmd_status;
+	u8 res_flag;
+};
+
+static void ctrl_cb(struct pme_ctx *ctx, const struct qm_fd *fd,
+		struct pme_ctx_ctrl_token *token)
+{
+	struct ctrl_op *ctrl = (struct ctrl_op *)token;
+	pr_info("pme2_test_high: ctrl_cb() invoked, fd;!\n");
+	ctrl->cmd_status = pme_fd_res_status(fd);
+	ctrl->res_flag = pme_fd_res_flags(fd);
+	complete(&ctrl->cb_done);
+}
+
 static inline int scan_data_empty(struct scan_session *session)
 {
 	return list_empty(&session->completed_commands);
@@ -176,8 +193,14 @@ static int getscan_cmd(struct file *fp, struct scan_session *session,
 	struct pme_scan_params *user_scan_params)
 {
 	int ret = 0;
-	struct pme_flow *params = NULL;
+	struct pme_flow params;
 	struct pme_scan_params local_scan_params;
+	struct ctrl_op ctx_ctrl =  {
+		.ctx_ctr.cb = ctrl_cb,
+		.cmd_status = 0,
+		.res_flag = 0
+	};
+	init_completion(&ctx_ctrl.cb_done);
 
 	memset(&local_scan_params, 0, sizeof(local_scan_params));
 
@@ -187,25 +210,26 @@ static int getscan_cmd(struct file *fp, struct scan_session *session,
 		ret = -EINVAL;
 		goto done;
 	}
-	params = pme_sw_flow_new();
-	if (!params) {
-		pr_err("pme2_scan: unable to allocate flw_ctx\n");
-		ret = -ENOMEM;
-		goto done;
-	}
 	ret = pme_ctx_ctrl_read_flow(&session->ctx, WAIT_AND_INTERRUPTABLE,
-			params);
+			&params, &ctx_ctrl.ctx_ctr);
 	if (ret) {
 		pr_info("pme2_scan: read flow error %d\n", ret);
 		goto done;
 	}
-	local_scan_params.residue.enable = params->ren;
-	local_scan_params.residue.length = params->rlen;
-	local_scan_params.sre.sessionid = params->sessionid;
-	local_scan_params.sre.verbose = params->srvm;
-	local_scan_params.sre.esee = params->esee;
-	local_scan_params.dxe.clim = params->clim;
-	local_scan_params.dxe.mlim = params->mlim;
+	wait_for_completion(&ctx_ctrl.cb_done);
+
+	if (ctx_ctrl.cmd_status || ctx_ctrl.res_flag) {
+		pr_info("pme2_scan: read flow error %d\n", ctx_ctrl.cmd_status);
+		ret = -EFAULT;
+		goto done;
+	}
+	local_scan_params.residue.enable = params.ren;
+	local_scan_params.residue.length = params.rlen;
+	local_scan_params.sre.sessionid = params.sessionid;
+	local_scan_params.sre.verbose = params.srvm;
+	local_scan_params.sre.esee = params.esee;
+	local_scan_params.dxe.clim = params.clim;
+	local_scan_params.dxe.mlim = params.mlim;
 	spin_lock(&session->set_subset_lock);
 	local_scan_params.pattern.set = session->set;
 	local_scan_params.pattern.subset = session->subset;
@@ -217,8 +241,6 @@ static int getscan_cmd(struct file *fp, struct scan_session *session,
 		ret = -EFAULT;
 	}
 done:
-	if (params)
-		pme_sw_flow_free(params);
 	return ret;
 }
 
@@ -227,16 +249,21 @@ static int setscan_cmd(struct file *fp, struct scan_session *session,
 {
 	int ret = 0;
 	u32 flag = WAIT_AND_INTERRUPTABLE;
-
-	struct pme_flow *params = NULL;
+	struct ctrl_op ctx_ctrl =  {
+		.ctx_ctr.cb = ctrl_cb,
+		.cmd_status = 0,
+		.res_flag = 0
+	};
+	struct pme_flow params;
 	struct pme_scan_params local_params;
 
+	pme_sw_flow_init(&params);
+	init_completion(&ctx_ctrl.cb_done);
 	if (copy_from_user(&local_params, user_params, sizeof(local_params)))
 		return -EFAULT;
 
 	/* must be enabled */
 	if (pme_ctx_is_disabled(&session->ctx)) {
-		pr_err("pme2_scan: ctx is disabled\n");
 		ret = -EINVAL;
 		goto done;
 	}
@@ -244,30 +271,31 @@ static int setscan_cmd(struct file *fp, struct scan_session *session,
 	 * is being done */
 	if (local_params.flags == PME_SCAN_PARAMS_PATTERN)
 		goto set_subset;
-	params = pme_sw_flow_new();
-	if (!params) {
-		pr_err("pme2_scan: unable to allocate flw_ctx\n");
-		ret = -ENOMEM;
-		goto done;
-	}
 	if (local_params.flags & PME_SCAN_PARAMS_RESIDUE)
 		flag |= PME_CMD_FCW_RES;
 	if (local_params.flags & PME_SCAN_PARAMS_SRE)
 		flag |= PME_CMD_FCW_SRE;
 	if (local_params.flags & PME_SCAN_PARAMS_DXE)
 		flag |= PME_CMD_FCW_DXE;
-	params->ren = local_params.residue.enable;
-	params->sessionid = local_params.sre.sessionid;
-	params->srvm = local_params.sre.verbose;
-	params->esee = local_params.sre.esee;
-	params->clim = local_params.dxe.clim;
-	params->mlim = local_params.dxe.mlim;
-
-	ret = pme_ctx_ctrl_update_flow(&session->ctx, flag, params);
+	params.ren = local_params.residue.enable;
+	params.sessionid = local_params.sre.sessionid;
+	params.srvm = local_params.sre.verbose;
+	params.esee = local_params.sre.esee;
+	params.clim = local_params.dxe.clim;
+	params.mlim = local_params.dxe.mlim;
+
+	ret = pme_ctx_ctrl_update_flow(&session->ctx, flag, &params,
+			&ctx_ctrl.ctx_ctr);
 	if (ret) {
 		pr_info("pme2_scan: update flow error %d\n", ret);
 		goto done;
 	}
+	wait_for_completion(&ctx_ctrl.cb_done);
+	if (ctx_ctrl.cmd_status || ctx_ctrl.res_flag) {
+		pr_info("pme2_scan: update flow err %d\n", ctx_ctrl.cmd_status);
+		ret = -EFAULT;
+		goto done;
+	}
 
 set_subset:
 	if (local_params.flags & PME_SCAN_PARAMS_PATTERN) {
@@ -278,15 +306,20 @@ set_subset:
 		goto done;
 	}
 done:
-	if (params)
-		pme_sw_flow_free(params);
 	return ret;
 }
 
 static int resetseq_cmd(struct file *fp, struct scan_session *session)
 {
 	int ret = 0;
-	struct pme_flow *params = NULL;
+	struct pme_flow params;
+	struct ctrl_op ctx_ctrl =  {
+		.ctx_ctr.cb = ctrl_cb,
+		.cmd_status = 0,
+		.res_flag = 0
+	};
+	init_completion(&ctx_ctrl.cb_done);
+	pme_sw_flow_init(&params);
 
 	/* must be enabled */
 	if (pme_ctx_is_disabled(&session->ctx)) {
@@ -294,51 +327,53 @@ static int resetseq_cmd(struct file *fp, struct scan_session *session)
 		ret =  -EINVAL;
 		goto done;
 	}
-	params = pme_sw_flow_new();
-	if (!params) {
-		pr_err("pme2_scan: unable to allocate flw_ctx\n");
-		ret = -ENOMEM;
-		goto done;
-	}
-	params->seqnum_hi = 0;
-	params->seqnum_lo = 0;
-	params->sos = 1;
+	params.seqnum_hi = 0;
+	params.seqnum_lo = 0;
+	params.sos = 1;
 
-	ret = pme_ctx_ctrl_update_flow(&session->ctx, PME_CMD_FCW_SEQ, params);
+	ret = pme_ctx_ctrl_update_flow(&session->ctx, PME_CMD_FCW_SEQ, &params,
+			&ctx_ctrl.ctx_ctr);
 	if (!ret)
 		pr_info("pme2_scan: update flow error %d\n", ret);
+	wait_for_completion(&ctx_ctrl.cb_done);
+	if (ctx_ctrl.cmd_status || ctx_ctrl.res_flag) {
+		pr_info("pme2_scan: update flow err %d\n", ctx_ctrl.cmd_status);
+		ret = -EFAULT;
+	}
 done:
-	if (params)
-		pme_sw_flow_free(params);
 	return ret;
 }
 
 static int resetresidue_cmd(struct file *fp, struct scan_session *session)
 {
 	int ret = 0;
-	struct pme_flow *params = NULL;
-
+	struct pme_flow params;
+	struct ctrl_op ctx_ctrl =  {
+		.ctx_ctr.cb = ctrl_cb,
+		.cmd_status = 0,
+		.res_flag = 0
+	};
+
+	init_completion(&ctx_ctrl.cb_done);
+	pme_sw_flow_init(&params);
 	/* must be enabled */
 	if (pme_ctx_is_disabled(&session->ctx)) {
 		pr_err("pme2_scan: ctx is disabled\n");
 		ret =  -EINVAL;
 		goto done;
 	}
-	params = pme_sw_flow_new();
-	if (!params) {
-		pr_err("pme2_scan: unable to allocate flw_ctx\n");
-		ret = -ENOMEM;
-		goto done;
-	}
-	params->rlen = 0;
+	params.rlen = 0;
 	ret = pme_ctx_ctrl_update_flow(&session->ctx,
 			WAIT_AND_INTERRUPTABLE | PME_CTX_OP_RESETRESLEN,
-			params);
+			&params, &ctx_ctrl.ctx_ctr);
 	if (!ret)
 		pr_info("pme2_scan: update flow error %d\n", ret);
+	wait_for_completion(&ctx_ctrl.cb_done);
+	if (ctx_ctrl.cmd_status || ctx_ctrl.res_flag) {
+		pr_info("pme2_scan: update flow err %d\n", ctx_ctrl.cmd_status);
+		ret = -EFAULT;
+	}
 done:
-	if (params)
-		pme_sw_flow_free(params);
 	return ret;
 }
 
@@ -486,15 +521,20 @@ static int fsl_pme2_scan_open(struct inode *node, struct file *fp)
 {
 	int ret;
 	struct scan_session *session;
-	struct pme_flow *flow = pme_sw_flow_new();
+	struct pme_flow flow;
+	struct ctrl_op ctx_ctrl =  {
+		.ctx_ctr.cb = ctrl_cb,
+		.cmd_status = 0,
+		.res_flag = 0
+	};
+
+	pme_sw_flow_init(&flow);
+	init_completion(&ctx_ctrl.cb_done);
 #ifdef CONFIG_FSL_PME2_SCAN_DEBUG
 	pr_info("pme2_scan: open %d\n", smp_processor_id());
 #endif
-	if (!flow)
-		return -ENOMEM;
 	fp->private_data = kzalloc(sizeof(*session), GFP_KERNEL);
 	if (!fp->private_data) {
-		pme_sw_flow_free(flow);
 		return -ENOMEM;
 	}
 	session = (struct scan_session *)fp->private_data;
@@ -525,21 +565,27 @@ static int fsl_pme2_scan_open(struct inode *node, struct file *fp)
 	/* Update flow to set sane defaults in the flow context */
 	/* TODO: because we free 'flow' here, we need to be uninterruptible. */
 	ret = pme_ctx_ctrl_update_flow(&session->ctx,
-			PME_CTX_OP_WAIT | PME_CMD_FCW_ALL, flow);
+		PME_CTX_OP_WAIT | PME_CMD_FCW_ALL, &flow, &ctx_ctrl.ctx_ctr);
 	if (ret) {
 		pr_info("pme2_scan: error updating flow ctx %d\n", ret);
 		pme_ctx_disable(&session->ctx, PME_CTX_OP_WAIT);
 		pme_ctx_finish(&session->ctx);
 		goto exit;
 	}
-	pme_sw_flow_free(flow);
+	wait_for_completion(&ctx_ctrl.cb_done);
+	if (ctx_ctrl.cmd_status || ctx_ctrl.res_flag) {
+		pr_info("pme2_scan: error updating flow ctx %d\n", ret);
+		pme_ctx_disable(&session->ctx, PME_CTX_OP_WAIT);
+		pme_ctx_finish(&session->ctx);
+		ret = -EFAULT;
+		goto exit;
+	}
 #ifdef CONFIG_FSL_PME2_SCAN_DEBUG
 	/* Set up the structures used for asynchronous requests */
 	pr_info("pme2_scan: Finish pme_scan open %d \n", smp_processor_id());
 #endif
 	return 0;
 exit:
-	pme_sw_flow_free(flow);
 	kfree(fp->private_data);
 	fp->private_data = NULL;
 	return ret;
@@ -770,7 +816,8 @@ static int __init fsl_pme2_scan_init(void)
 		pr_err("fsl-pme2-scan: cannot register device\n");
 		return err;
 	}
-	pr_info("fsl-pme2-san: device %s registered\n", fsl_pme2_scan_dev.name);
+	pr_info("fsl-pme2-scan: device %s registered\n",
+		fsl_pme2_scan_dev.name);
 	return 0;
 }
 
diff --git a/drivers/match/pme2_sysfs.c b/drivers/match/pme2_sysfs.c
index f20115e..8a5daaa 100644
--- a/drivers/match/pme2_sysfs.c
+++ b/drivers/match/pme2_sysfs.c
@@ -202,6 +202,14 @@ PME_SYSFS_ATTR(end_of_sui_reaction_ptr, PRIV_RW, FMT_DEC);
 PME_SYSFS_ATTR(sre_pscl, PRIV_RW, FMT_DEC);
 PME_SYSFS_ATTR(sre_max_block_num, PRIV_RW, FMT_DEC);
 PME_SYSFS_ATTR(sre_max_instruction_limit, PRIV_RW, FMT_DEC);
+PME_SYSFS_ATTR(esr, PRIV_RW, FMT_0HEX);
+PME_SYSFS_ATTR(pehd, PRIV_RW, FMT_0HEX);
+PME_SYSFS_ATTR(ecc1bes, PRIV_RW, FMT_0HEX);
+PME_SYSFS_ATTR(ecc2bes, PRIV_RW, FMT_0HEX);
+PME_SYSFS_ATTR(miace, PRIV_RW, FMT_0HEX);
+PME_SYSFS_ATTR(miacr, PRIV_RW, FMT_0HEX);
+PME_SYSFS_ATTR(cdcr, PRIV_RW, FMT_0HEX);
+PME_SYSFS_ATTR(pmtr, PRIV_RW, FMT_DEC);
 
 /* read-only; */
 PME_SYSFS_ATTR(max_pdsr_index, PRIV_RO, FMT_DEC);
@@ -215,6 +223,17 @@ PME_SYSFS_ATTR(liodnr, PRIV_RO, FMT_DEC);
 PME_SYSFS_ATTR(rev1, PRIV_RO, FMT_0HEX);
 PME_SYSFS_ATTR(rev2, PRIV_RO, FMT_0HEX);
 PME_SYSFS_ATTR(isr, PRIV_RO, FMT_0HEX);
+PME_SYSFS_ATTR(ecr0, PRIV_RO, FMT_0HEX);
+PME_SYSFS_ATTR(ecr1, PRIV_RO, FMT_0HEX);
+PME_SYSFS_ATTR(pmstat, PRIV_RO, FMT_0HEX);
+PME_SYSFS_ATTR(eccaddr, PRIV_RO, FMT_0HEX);
+PME_SYSFS_ATTR(ecccode, PRIV_RO, FMT_0HEX);
+PME_SYSFS_ATTR(faconf, PRIV_RO, FMT_0HEX);
+PME_SYSFS_ATTR(pdsrbah, PRIV_RO, FMT_0HEX);
+PME_SYSFS_ATTR(pdsrbal, PRIV_RO, FMT_0HEX);
+PME_SYSFS_ATTR(scbarh, PRIV_RO, FMT_0HEX);
+PME_SYSFS_ATTR(scbarl, PRIV_RO, FMT_0HEX);
+
 
 /* Buffer Pool Size Configuration */
 PME_SYSFS_BSC_ATTR(0, PRIV_RW, FMT_DEC);
@@ -420,6 +439,24 @@ static struct attribute *pme_dev_attributes[] = {
 	&dev_attr_rev1.attr,
 	&dev_attr_rev2.attr,
 	&dev_attr_isr.attr,
+	&dev_attr_ecr0.attr,
+	&dev_attr_ecr1.attr,
+	&dev_attr_esr.attr,
+	&dev_attr_pmstat.attr,
+	&dev_attr_pehd.attr,
+	&dev_attr_ecc1bes.attr,
+	&dev_attr_ecc2bes.attr,
+	&dev_attr_eccaddr.attr,
+	&dev_attr_ecccode.attr,
+	&dev_attr_miace.attr,
+	&dev_attr_miacr.attr,
+	&dev_attr_cdcr.attr,
+	&dev_attr_pmtr.attr,
+	&dev_attr_faconf.attr,
+	&dev_attr_pdsrbah.attr,
+	&dev_attr_pdsrbal.attr,
+	&dev_attr_scbarh.attr,
+	&dev_attr_scbarl.attr,
 	NULL
 };
 
diff --git a/drivers/match/pme2_test_high.c b/drivers/match/pme2_test_high.c
index 8d2f7d9..96356af 100644
--- a/drivers/match/pme2_test_high.c
+++ b/drivers/match/pme2_test_high.c
@@ -47,56 +47,93 @@ static u8 fl_ctx_exp[]={
 void scan_cb(struct pme_ctx *ctx, const struct qm_fd *fd,
 		struct pme_ctx_token *token)
 {
-	pr_info("pme2_test_high: scan_cb() invoked, fd;!\n");
 	hexdump(fd, sizeof(*fd));
 }
 
+struct ctrl_op {
+	struct pme_ctx_ctrl_token ctx_ctr;
+	struct completion cb_done;
+	enum pme_status cmd_status;
+	u8 res_flag;
+};
+
+static void ctrl_cb(struct pme_ctx *ctx, const struct qm_fd *fd,
+		struct pme_ctx_ctrl_token *token)
+{
+	struct ctrl_op *ctrl = (struct ctrl_op *)token;
+	pr_info("pme2_test_high: ctrl_cb() invoked, fd;!\n");
+	ctrl->cmd_status = pme_fd_res_status(fd);
+	ctrl->res_flag = pme_fd_res_flags(fd);
+	hexdump(fd, sizeof(*fd));
+	complete(&ctrl->cb_done);
+}
+
+
 #define POST_CTRL() \
 do { \
 	BUG_ON(ret); \
 	BUG_ON(pme_ctx_is_dead(&ctx)); \
+	BUG_ON(ctx_ctrl.cmd_status); \
+	BUG_ON(ctx_ctrl.res_flag); \
 } while (0)
 
 void pme2_test_high(void)
 {
-	struct pme_flow *flow;
+	struct pme_flow flow;
 	struct qm_fqd_stashing stashing;
 	struct pme_ctx ctx = {
 		.cb = scan_cb
 	};
 	int ret;
+	struct ctrl_op ctx_ctrl =  {
+		.ctx_ctr.cb = ctrl_cb,
+		.cmd_status = 0,
+		.res_flag = 0
+	};
+
+	pme_sw_flow_init(&flow);
+	init_completion(&ctx_ctrl.cb_done);
 
-	flow = pme_sw_flow_new();
-	BUG_ON(!flow);
 	pr_info("PME2: high-level test starting\n");
 
 	ret = pme_ctx_init(&ctx, PME_CTX_FLAG_LOCAL, 0, 4, 4, 0, NULL);
+	pr_info("PME2: pme_ctx_init done\n");
 	POST_CTRL();
 	/* enable the context */
 	pme_ctx_enable(&ctx);
+	pr_info("PME2: pme_ctx_enable done\n");
 	ret = pme_ctx_ctrl_update_flow(&ctx, PME_CTX_OP_WAIT | PME_CMD_FCW_ALL,
-					flow);
+					&flow, &ctx_ctrl.ctx_ctr);
+	pr_info("PME2: pme_ctx_ctrl_update_flow done\n");
+	wait_for_completion(&ctx_ctrl.cb_done);
 	POST_CTRL();
 	/* read back flow settings */
-	ret = pme_ctx_ctrl_read_flow(&ctx, PME_CTX_OP_WAIT, flow);
+	ret = pme_ctx_ctrl_read_flow(&ctx, PME_CTX_OP_WAIT, &flow,
+			&ctx_ctrl.ctx_ctr);
+	pr_info("PME2: pme_ctx_ctrl_read_flow done\n");
+	wait_for_completion(&ctx_ctrl.cb_done);
 	POST_CTRL();
-	if (memcmp(flow, fl_ctx_exp, sizeof(*flow))) {
+	if (memcmp(&flow, fl_ctx_exp, sizeof(flow))) {
 		pr_info("Default Flow Context Read FAIL\n");
 		pr_info("Expected:\n");
 		hexdump(fl_ctx_exp, sizeof(fl_ctx_exp));
 		pr_info("Received:\n");
-		hexdump(flow, sizeof(*flow));
+		hexdump(&flow, sizeof(flow));
 		BUG_ON(1);
 	} else
 		pr_info("Default Flow Context Read OK\n");
 	/* start a non-blocking NOP */
-	ret = pme_ctx_ctrl_nop(&ctx, 0);
+	ret = pme_ctx_ctrl_nop(&ctx, 0, &ctx_ctrl.ctx_ctr);
+	pr_info("PME2: pme_ctx_ctrl_nop done\n");
+	wait_for_completion(&ctx_ctrl.cb_done);
 	POST_CTRL();
 	/* start a blocking update (implicitly blocks on NOP-completion) to add
 	 * residue to the context */
-	flow->ren = 1;
+	flow.ren = 1;
 	ret = pme_ctx_ctrl_update_flow(&ctx, PME_CTX_OP_WAIT | PME_CMD_FCW_RES,
-					flow);
+					&flow, &ctx_ctrl.ctx_ctr);
+	pr_info("PME2: pme_ctx_ctrl_update_flow done\n");
+	wait_for_completion(&ctx_ctrl.cb_done);
 	POST_CTRL();
 	/* start a blocking disable */
 	ret = pme_ctx_disable(&ctx, PME_CTX_OP_WAIT);
@@ -115,18 +152,21 @@ void pme2_test_high(void)
 	BUG_ON(ret);
 	/* read back flow settings */
 	ret = pme_ctx_ctrl_read_flow(&ctx,
-		PME_CTX_OP_WAIT | PME_CTX_OP_WAIT_INT | PME_CMD_FCW_RES, flow);
+		PME_CTX_OP_WAIT | PME_CTX_OP_WAIT_INT | PME_CMD_FCW_RES, &flow,
+		&ctx_ctrl.ctx_ctr);
+	pr_info("PME2: pme_ctx_ctrl_read_flow done\n");
+	wait_for_completion(&ctx_ctrl.cb_done);
 	POST_CTRL();
-	pr_info("read Flow Context;\n");
-	hexdump(flow, sizeof(*flow));
 	/* blocking NOP */
-	ret = pme_ctx_ctrl_nop(&ctx, PME_CTX_OP_WAIT | PME_CTX_OP_WAIT_INT);
+	ret = pme_ctx_ctrl_nop(&ctx, PME_CTX_OP_WAIT | PME_CTX_OP_WAIT_INT,
+			&ctx_ctrl.ctx_ctr);
+	pr_info("PME2: pme_ctx_ctrl_nop done\n");
+	wait_for_completion(&ctx_ctrl.cb_done);
 	POST_CTRL();
 	/* Disable, and done */
 	ret = pme_ctx_disable(&ctx, PME_CTX_OP_WAIT | PME_CTX_OP_WAIT_INT);
 	BUG_ON(ret);
 	pme_ctx_finish(&ctx);
-	pme_sw_flow_free(flow);
 	pr_info("PME2: high-level test done\n");
 }
 
diff --git a/drivers/match/pme2_test_scan.c b/drivers/match/pme2_test_scan.c
index 7758b37..d26c442 100644
--- a/drivers/match/pme2_test_scan.c
+++ b/drivers/match/pme2_test_scan.c
@@ -57,14 +57,30 @@ struct scan_ctx {
 	struct qm_fd result_fd;
 };
 
+struct ctrl_op {
+	struct pme_ctx_ctrl_token ctx_ctr;
+	struct completion cb_done;
+	enum pme_status cmd_status;
+	u8 res_flag;
+};
+
+static void ctrl_cb(struct pme_ctx *ctx, const struct qm_fd *fd,
+		struct pme_ctx_ctrl_token *token)
+{
+	struct ctrl_op *ctrl = (struct ctrl_op *)token;
+	pr_info("pme2_test_high: ctrl_cb() invoked, fd;!\n");
+	ctrl->cmd_status = pme_fd_res_status(fd);
+	ctrl->res_flag = pme_fd_res_flags(fd);
+	hexdump(fd, sizeof(*fd));
+	complete(&ctrl->cb_done);
+}
+
 static DECLARE_COMPLETION(scan_comp);
 
 static void scan_cb(struct pme_ctx *ctx, const struct qm_fd *fd,
 		struct pme_ctx_token *ctx_token)
 {
 	struct scan_ctx *my_ctx = (struct scan_ctx *)ctx;
-	pr_info("st: scan_cb() invoked, fd;!\n");
-	hexdump(fd, sizeof(*fd));
 	memcpy(&my_ctx->result_fd, fd, sizeof(*fd));
 	complete(&scan_comp);
 }
@@ -93,20 +109,24 @@ static void empty_buffer(void)
 
 	do {
 		ret = bman_acquire(pool, &bufs_in, 1, 0);
-		if (!ret)
-			pr_info("st: Acquired buffer\n");
 	} while (!ret);
 }
 #endif /*CONFIG_FSL_PME2_TEST_SCAN_WITH_BPID*/
 
 void pme2_test_scan(void)
 {
-	struct pme_flow *flow;
+	struct pme_flow flow;
+	struct pme_flow rb_flow;
 	struct scan_ctx a_scan_ctx = {
 		.base_ctx = {
 			.cb = scan_cb
 		}
 	};
+	struct ctrl_op ctx_ctrl =  {
+		.ctx_ctr.cb = ctrl_cb,
+		.cmd_status = 0,
+		.res_flag = 0
+	};
 	struct qm_fd fd;
 	struct qm_sg_entry sg_table[2];
 	int ret;
@@ -122,6 +142,8 @@ void pme2_test_scan(void)
 		0x00,0x00, 0x00,0x00,0x00
 	};
 
+	pme_sw_flow_init(&flow);
+	init_completion(&ctx_ctrl.cb_done);
 	scan_result = scan_result_direct_mode_inc_mode;
 	scan_result_size = sizeof(scan_result_direct_mode_inc_mode);
 
@@ -152,16 +174,11 @@ void pme2_test_scan(void)
 	fd._format2 = qm_fd_compound;
 	fd.addr_lo = pme_map(sg_table);
 
-	pr_info("st: Send scan request\n");
 	ret = pme_ctx_scan(&a_scan_ctx.base_ctx, 0, &fd,
 		PME_SCAN_ARGS(PME_CMD_SCAN_SR | PME_CMD_SCAN_E, 0, 0xff00),
 		&token);
-	pr_info("st: Response scan %d\n", ret);
 	wait_for_completion(&scan_comp);
 
-	pr_info("st: Status is fd.status %x\n",
-			a_scan_ctx.result_fd.status);
-
 	status = pme_fd_res_status(&a_scan_ctx.result_fd);
 	if (status) {
 		pr_info("st: Scan status failed %d\n", status);
@@ -181,27 +198,17 @@ void pme2_test_scan(void)
 	fd.length20 = sizeof(scan_data);
 	fd.addr_lo = pme_map(scan_data);
 
-	pr_info("st: Send scan request\n");
-
 	ret = pme_ctx_scan(&a_scan_ctx.base_ctx, 0, &fd,
 		PME_SCAN_ARGS(PME_CMD_SCAN_SR | PME_CMD_SCAN_E, 0, 0xff00),
 		&token);
-
-	pr_info("st: Response scan %d\n", ret);
 	wait_for_completion(&scan_comp);
 
-	pr_info("st: Status is fd.status %x\n",
-			a_scan_ctx.result_fd.status);
-
 	status = pme_fd_res_status(&a_scan_ctx.result_fd);
-	pr_info("st: Scan status %x\n", status);
-
 	/* Check the response...expect truncation bit to be set */
 	if (!(pme_fd_res_flags(&a_scan_ctx.result_fd) & PME_STATUS_TRUNCATED)) {
 		pr_info("st: Scan result failed...expected trunc\n");
 		BUG_ON(1);
 	}
-	pr_info("st: Simple scan test Passed\n");
 
 	/* Disable */
 	ret = pme_ctx_disable(&a_scan_ctx.base_ctx, PME_CTX_OP_WAIT);
@@ -228,16 +235,11 @@ void pme2_test_scan(void)
 	fd._format2 = qm_fd_compound;
 	fd.addr_lo = pme_map(sg_table);
 
-	pr_info("st: Send scan with bpid response request\n");
 	ret = pme_ctx_scan(&a_scan_ctx.base_ctx, 0, &fd,
 		PME_SCAN_ARGS(PME_CMD_SCAN_SR | PME_CMD_SCAN_E, 0, 0xff00),
 		&token);
-	pr_info("st: Response scan %d\n", ret);
 	wait_for_completion(&scan_comp);
 
-	pr_info("st: Status is fd.status %x\n",
-			a_scan_ctx.result_fd.status);
-
 	status = pme_fd_res_status(&a_scan_ctx.result_fd);
 	if (status) {
 		pr_info("st: Scan status failed %d\n", status);
@@ -245,8 +247,6 @@ void pme2_test_scan(void)
 	}
 
 	/* sg result should point to bman buffer */
-	pr_info("st: sg result should point to bman buffer 0x%x\n",
-			sg_table[0].addr_lo);
 	if (!sg_table[0].addr_lo)
 		BUG_ON(1);
 
@@ -260,7 +260,6 @@ void pme2_test_scan(void)
 	}
 
 	release_buffer(sg_table[0].addr_lo);
-	pr_info("st: Released to bman\n");
 
 	/* Disable */
 	ret = pme_ctx_disable(&a_scan_ctx.base_ctx, PME_CTX_OP_WAIT);
@@ -273,40 +272,31 @@ void pme2_test_scan(void)
 	/*********************** Flow Mode ************************************/
 	/**********************************************************************/
 	/**********************************************************************/
-	pr_info("st: Start Flow Mode Test\n");
 
-	flow = pme_sw_flow_new();
-	BUG_ON(!flow);
 	ret = pme_ctx_init(&a_scan_ctx.base_ctx,
 		PME_CTX_FLAG_LOCAL, 0, 4, 4, 0, NULL);
 	BUG_ON(ret);
 
 	/* enable the context */
 	pme_ctx_enable(&a_scan_ctx.base_ctx);
-	pr_info("st: Context Enabled\n");
 
 	ret = pme_ctx_ctrl_update_flow(&a_scan_ctx.base_ctx,
-		PME_CTX_OP_WAIT | PME_CMD_FCW_ALL, flow);
+		PME_CTX_OP_WAIT | PME_CMD_FCW_ALL, &flow, &ctx_ctrl.ctx_ctr);
+	wait_for_completion(&ctx_ctrl.cb_done);
 	BUG_ON(ret);
 
 	/* read back flow settings */
-	{
-		struct pme_flow *rb_flow = pme_sw_flow_new();
-
-		ret = pme_ctx_ctrl_read_flow(&a_scan_ctx.base_ctx,
-					PME_CTX_OP_WAIT, rb_flow);
-		BUG_ON(ret);
-		if (memcmp(rb_flow,fl_ctx_exp, sizeof(*rb_flow)) != 0) {
-			pr_info("st: Flow Context Read FAIL\n");
-			pr_info("st: Expected\n");
-			hexdump(fl_ctx_exp, sizeof(fl_ctx_exp));
-			pr_info("st: Received...\n");
-			hexdump(rb_flow, sizeof(*rb_flow));
-			BUG_ON(1);
-		} else {
-			pr_info("st: Flow Context Read OK\n");
-		}
-		pme_sw_flow_free(rb_flow);
+	ret = pme_ctx_ctrl_read_flow(&a_scan_ctx.base_ctx,
+			PME_CTX_OP_WAIT, &rb_flow, &ctx_ctrl.ctx_ctr);
+	wait_for_completion(&ctx_ctrl.cb_done);
+	BUG_ON(ret);
+	if (memcmp(&rb_flow, fl_ctx_exp, sizeof(rb_flow)) != 0) {
+		pr_info("st: Flow Context Read FAIL\n");
+		pr_info("st: Expected\n");
+		hexdump(fl_ctx_exp, sizeof(fl_ctx_exp));
+		pr_info("st: Received...\n");
+		hexdump(&rb_flow, sizeof(rb_flow));
+		BUG_ON(1);
 	}
 
 	/* Do a pre-built output, scan with match test */
@@ -324,17 +314,11 @@ void pme2_test_scan(void)
 	fd._format2 = qm_fd_compound;
 	fd.addr_lo = pme_map(sg_table);
 
-	pr_info("st: Send scan request\n");
 	ret = pme_ctx_scan(&a_scan_ctx.base_ctx, 0, &fd,
 		PME_SCAN_ARGS(PME_CMD_SCAN_SR | PME_CMD_SCAN_E, 0, 0xff00),
 		&token);
-
-	pr_info("st: Response scan %d\n", ret);
 	wait_for_completion(&scan_comp);
 
-	pr_info("st: Status is fd.status %x\n",
-			a_scan_ctx.result_fd.status);
-
 	status = pme_fd_res_status(&a_scan_ctx.result_fd);
 	if (status) {
 		pr_info("st: Scan status failed %d\n", status);
@@ -350,26 +334,19 @@ void pme2_test_scan(void)
 	}
 
 	/* read back flow settings */
-	{
-		struct pme_flow *rb_flow = pme_sw_flow_new();
-
-		ret = pme_ctx_ctrl_read_flow(&a_scan_ctx.base_ctx,
-					PME_CTX_OP_WAIT, rb_flow);
-		BUG_ON(ret);
-		if (memcmp(rb_flow, fl_ctx_exp_post_scan,
-					sizeof(*rb_flow)) != 0) {
-			pr_info("st: Flow Context Read FAIL\n");
-			pr_info("st: Expected\n");
-			hexdump(fl_ctx_exp_post_scan,
-				sizeof(fl_ctx_exp_post_scan));
-			pr_info("st: Received\n");
-			hexdump(rb_flow, sizeof(*rb_flow));
-			BUG_ON(1);
-		} else {
-			pr_info("st: Flow Context Read OK\n");
-		}
-		pme_sw_flow_free(rb_flow);
+	ret = pme_ctx_ctrl_read_flow(&a_scan_ctx.base_ctx,
+			PME_CTX_OP_WAIT, &rb_flow, &ctx_ctrl.ctx_ctr);
+	wait_for_completion(&ctx_ctrl.cb_done);
+	BUG_ON(ret);
+	if (memcmp(&rb_flow, fl_ctx_exp_post_scan, sizeof(rb_flow)) != 0) {
+		pr_info("st: Flow Context Read FAIL\n");
+		pr_info("st: Expected\n");
+		hexdump(fl_ctx_exp_post_scan, sizeof(fl_ctx_exp_post_scan));
+		pr_info("st: Received\n");
+		hexdump(&rb_flow, sizeof(rb_flow));
+		BUG_ON(1);
 	}
+
 	/* Test truncation test */
 	/* Build a frame descriptor */
 	memset(&fd, 0, sizeof(struct qm_fd));
@@ -377,53 +354,36 @@ void pme2_test_scan(void)
 	fd.length20 = sizeof(scan_data);
 	fd.addr_lo = pme_map(scan_data);
 
-	pr_info("st: Send scan request\n");
 	ret = pme_ctx_scan(&a_scan_ctx.base_ctx, 0, &fd,
 		PME_SCAN_ARGS(PME_CMD_SCAN_SR | PME_CMD_SCAN_E, 0, 0xff00),
 		&token);
-
-	pr_info("st: Response scan %d\n", ret);
 	wait_for_completion(&scan_comp);
 
-	pr_info("st: Status is fd.status %x\n",
-			a_scan_ctx.result_fd.status);
-
 	status = pme_fd_res_status(&a_scan_ctx.result_fd);
-	 pr_info("Scan status %x\n",status);
-
 	/* Check the response...expect truncation bit to be set */
 	if (!(pme_fd_res_flags(&a_scan_ctx.result_fd) & PME_STATUS_TRUNCATED)) {
 		pr_info("st: Scan result failed...expected trunc\n");
 		BUG_ON(1);
 	}
-	pr_info("st: Simple scan test Passed\n");
 
 	/* read back flow settings */
-	{
-		struct pme_flow *rb_flow = pme_sw_flow_new();
-		ret = pme_ctx_ctrl_read_flow(&a_scan_ctx.base_ctx,
-					PME_CTX_OP_WAIT, rb_flow);
-		BUG_ON(ret);
-		if (memcmp(rb_flow, fl_ctx_exp_post_scan,
-					sizeof(*rb_flow)) != 0) {
-			pr_info("st: Flow Context Read FAIL\n");
-			pr_info("st: Expected\n");
-			hexdump(fl_ctx_exp_post_scan,
-				sizeof(fl_ctx_exp_post_scan));
-			pr_info("st: Received\n");
-			hexdump(rb_flow, sizeof(*rb_flow));
-			BUG_ON(1);
-		} else {
-			pr_info("st: Flow Context Read OK\n");
-		}
-		pme_sw_flow_free(rb_flow);
+	ret = pme_ctx_ctrl_read_flow(&a_scan_ctx.base_ctx,
+			PME_CTX_OP_WAIT, &rb_flow, &ctx_ctrl.ctx_ctr);
+	wait_for_completion(&ctx_ctrl.cb_done);
+	BUG_ON(ret);
+	if (memcmp(&rb_flow, fl_ctx_exp_post_scan, sizeof(rb_flow)) != 0) {
+		pr_info("st: Flow Context Read FAIL\n");
+		pr_info("st: Expected\n");
+		hexdump(fl_ctx_exp_post_scan, sizeof(fl_ctx_exp_post_scan));
+		pr_info("st: Received\n");
+		hexdump(&rb_flow, sizeof(rb_flow));
+		BUG_ON(1);
 	}
 
 	/* Disable */
 	ret = pme_ctx_disable(&a_scan_ctx.base_ctx, PME_CTX_OP_WAIT);
 	BUG_ON(ret);
 	pme_ctx_finish(&a_scan_ctx.base_ctx);
-	pme_sw_flow_free(flow);
 
 	pr_info("st: Scan Test Passed\n");
 }
diff --git a/include/linux/fsl_bman.h b/include/linux/fsl_bman.h
index 37b2de7..55cbfaa 100644
--- a/include/linux/fsl_bman.h
+++ b/include/linux/fsl_bman.h
@@ -410,7 +410,11 @@ struct bman_pool_params {
  * only slow-path processing, or (c) no processing. This function does whatever
  * processing is not triggered by interrupts.
  */
+#ifdef CONFIG_FSL_BMAN_HAVE_POLL
 void bman_poll(void);
+#else
+#define bman_poll()	do { ; } while (0)
+#endif
 
 
 	/* Pool management */
diff --git a/include/linux/fsl_pme.h b/include/linux/fsl_pme.h
index 761cc7b..3bf5477 100644
--- a/include/linux/fsl_pme.h
+++ b/include/linux/fsl_pme.h
@@ -320,10 +320,8 @@ void pme_hw_residue_free(struct pme_hw_residue *);
 struct pme_hw_flow *pme_hw_flow_new(void);
 void pme_hw_flow_free(struct pme_hw_flow *);
 
-/* Software 'flow' structures also have alignment requirements, so use these to
- * allocate them. */
-struct pme_flow *pme_sw_flow_new(void);
-void pme_sw_flow_free(struct pme_flow *);
+/* Initialise a flow context to known default values */
+void pme_sw_flow_init(struct pme_flow *);
 
 /* Fill in an "Initialise FQ" management command for a PME input FQ. NB, the
  * caller is responsible for setting the following fields, they will not be set
@@ -395,6 +393,14 @@ void pme_fd_cmd_scan(struct qm_fd *fd, u32 args);
 dma_addr_t pme_map(void *ptr);
 int pme_map_error(dma_addr_t dma_addr);
 
+enum pme_cmd_type {
+	pme_cmd_nop = 0x7,
+	pme_cmd_flow_read = 0x5,	/* aka FCR */
+	pme_cmd_flow_write = 0x4,	/* aka FCW */
+	pme_cmd_pmtcc = 0x1,
+	pme_cmd_scan = 0
+};
+
 /************************/
 /* high-level functions */
 /************************/
@@ -409,6 +415,16 @@ struct pme_ctx;
 struct pme_ctx_token {
 	u32 blob[4];
 	struct list_head node;
+	enum pme_cmd_type cmd_type;
+};
+
+struct pme_ctx_ctrl_token {
+	void (*cb)(struct pme_ctx *, const struct qm_fd *,
+			struct pme_ctx_ctrl_token *);
+	/* don't touch the rest */
+	struct pme_hw_flow *internal_flow_ptr;
+	struct pme_flow *usr_flow_ptr;
+	struct pme_ctx_token base_token;
 };
 
 /* Scan results invoke a user-provided callback of this type */
@@ -436,7 +452,6 @@ struct pme_ctx {
 	/* TODO: the following "slow-path" values should be bundled into a
 	 * secondary structure so that sizeof(struct pme_ctx) is minimised (for
 	 * stashing of caller-side fast-path state). */
-	u32 uid;
 	struct qman_fq *fqin;
 	struct pme_hw_flow *hw_flow;
 	struct pme_hw_residue *hw_residue;
@@ -551,15 +566,11 @@ int pme_ctx_reconfigure_rx(struct pme_ctx *ctx, u8 qosout,
  * assumed cleared, irrespective of what is specified in 'flags'.
  */
 int pme_ctx_ctrl_update_flow(struct pme_ctx *ctx, u32 flags,
-			struct pme_flow *params);
+		struct pme_flow *params, struct pme_ctx_ctrl_token *token);
 int pme_ctx_ctrl_read_flow(struct pme_ctx *ctx, u32 flags,
-			struct pme_flow *params);
-int pme_ctx_ctrl_nop(struct pme_ctx *ctx, u32 flags);
-/* This function returns non-zero if a pme_ctx_ctrl_***() operation is still in
- * progress. If PME_CTX_OP_WAIT isn't used, this may be required in order to
- * determine completion, eg. to know when one can safely deallocate the 'params'
- * passed to the above APIs. */
-int pme_ctx_in_ctrl(struct pme_ctx *ctx);
+		struct pme_flow *params, struct pme_ctx_ctrl_token *token);
+int pme_ctx_ctrl_nop(struct pme_ctx *ctx, u32 flags,
+		struct pme_ctx_ctrl_token *token);
 
 /* if PME_CTX_OP_WAIT is specified, it'll wait (if it has to) to start the scan
  * but never waits for it to complete. The scan callback serves that purpose.
@@ -651,15 +662,36 @@ enum pme_attr {
 	pme_attr_mia_byc,
 	pme_attr_mia_blc,
 	pme_attr_isr,
-	pme_attr_bsc_first,
-	pme_attr_bsc_last = pme_attr_bsc_first + 63,
 	pme_attr_tbt0ecc1th,
 	pme_attr_tbt1ecc1th,
 	pme_attr_vlt0ecc1th,
 	pme_attr_vlt1ecc1th,
 	pme_attr_cmecc1th,
 	pme_attr_dxcmecc1th,
-	pme_attr_dxemecc1th
+	pme_attr_dxemecc1th,
+	pme_attr_esr,
+	pme_attr_ecr0,
+	pme_attr_ecr1,
+	pme_attr_pmstat,
+	pme_attr_pmtr,
+	pme_attr_pehd,
+	pme_attr_ecc1bes,
+	pme_attr_ecc2bes,
+	pme_attr_eccaddr,
+	pme_attr_ecccode,
+	pme_attr_miace,
+	pme_attr_miacr,
+	pme_attr_cdcr,
+	pme_attr_faconf,
+	pme_attr_ier,
+	pme_attr_isdr,
+	pme_attr_iir,
+	pme_attr_pdsrbah,
+	pme_attr_pdsrbal,
+	pme_attr_scbarh,
+	pme_attr_scbarl,
+	pme_attr_bsc_first, /* create 64-wide space for bsc */
+	pme_attr_bsc_last = pme_attr_bsc_first + 63,
 };
 
 #define pme_attr_bsc(n) (pme_attr_bsc_first + (n))
diff --git a/include/linux/fsl_qman.h b/include/linux/fsl_qman.h
index 0bb71e6..df53e05 100644
--- a/include/linux/fsl_qman.h
+++ b/include/linux/fsl_qman.h
@@ -1323,7 +1323,11 @@ void qman_set_null_cb(const struct qman_fq_cb *null_cb);
  * (c) no processing. This function does whatever processing is not triggered by
  * interrupts.
  */
+#ifdef CONFIG_FSL_QMAN_HAVE_POLL
 void qman_poll(void);
+#else
+#define qman_poll()	do { ; } while (0)
+#endif
 
 /**
  * qman_disable_portal - Cease processing DQRR and MR for a s/w portal
-- 
1.6.5.2

