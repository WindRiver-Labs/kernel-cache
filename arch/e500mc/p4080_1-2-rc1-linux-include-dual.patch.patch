From 7cee628515bb185c423e202e37b214c3ed653b89 Mon Sep 17 00:00:00 2001
From: Roy Pledge <roy.pledge@freescale.com>
Date: Fri, 16 Apr 2010 12:00:25 -0400
Subject: [PATCH 008/252] p4080_1-2-rc1-linux-include-dual.patch

include/linux/fsl_bman.h
include/linux/fsl_pme.h
include/linux/fsl_qman.h

Signed-off-by: Emil Medve <Emilian.Medve@Freescale.com>
Signed-off-by: Geoff Thorpe <geoff@geoffthorpe.net>
Signed-off-by: Jeffrey Ladouceur <Jeffrey.Ladouceur@freescale.com>
Signed-off-by: Roy Pledge <roy.pledge@freescale.com>
[Cleanly applied Freeskale SDK 2.0.3 patch
"p4080_1-2-rc1-linux-include-dual.patch",
original did not have commit text.]
Integrated-by: Yuri Nedel <Yuri.Nedel@windriver.com>
---
 include/linux/fsl_bman.h |  481 ++++++++++++++
 include/linux/fsl_pme.h  |  650 ++++++++++++++++++
 include/linux/fsl_qman.h | 1644 ++++++++++++++++++++++++++++++++++++++++++++++
 3 files changed, 2775 insertions(+), 0 deletions(-)
 create mode 100644 include/linux/fsl_bman.h
 create mode 100644 include/linux/fsl_pme.h
 create mode 100644 include/linux/fsl_qman.h

diff --git a/include/linux/fsl_bman.h b/include/linux/fsl_bman.h
new file mode 100644
index 0000000..37b2de7
--- /dev/null
+++ b/include/linux/fsl_bman.h
@@ -0,0 +1,481 @@
+/* Copyright (c) 2008, 2009 Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef FSL_BMAN_H
+#define FSL_BMAN_H
+
+/* Last updated for v00.79 of the BG */
+
+/*************************************************/
+/*   BMan s/w corenet portal, low-level i/face   */
+/*************************************************/
+
+/* Portal constants */
+#define BM_RCR_SIZE		8
+
+/* Hardware constants */
+enum bm_isr_reg {
+	bm_isr_status = 0,
+	bm_isr_enable = 1,
+	bm_isr_disable = 2,
+	bm_isr_inhibit = 3
+};
+
+/* Represents s/w corenet portal mapped data structures */
+struct bm_rcr_entry;	/* RCR (Release Command Ring) entries */
+struct bm_mc_command;	/* MC (Management Command) command */
+struct bm_mc_result;	/* MC result */
+
+/* This type represents a s/w corenet portal space, and is used for creating the
+ * portal objects within it (RCR, etc) */
+struct bm_portal;
+
+/* This wrapper represents a bit-array for the depletion state of the 64 Bman
+ * buffer pools. */
+struct bman_depletion {
+	u32 __state[2];
+};
+#define __bmdep_word(x) ((x) >> 5)
+#define __bmdep_shift(x) ((x) & 0x1f)
+#define __bmdep_bit(x) (0x80000000 >> __bmdep_shift(x))
+static inline void bman_depletion_init(struct bman_depletion *c)
+{
+	c->__state[0] = c->__state[1] = 0;
+}
+static inline void bman_depletion_fill(struct bman_depletion *c)
+{
+	c->__state[0] = c->__state[1] = ~0;
+}
+static inline int bman_depletion_get(const struct bman_depletion *c, u8 bpid)
+{
+	return c->__state[__bmdep_word(bpid)] & __bmdep_bit(bpid);
+}
+static inline void bman_depletion_set(struct bman_depletion *c, u8 bpid)
+{
+	c->__state[__bmdep_word(bpid)] |= __bmdep_bit(bpid);
+}
+static inline void bman_depletion_unset(struct bman_depletion *c, u8 bpid)
+{
+	c->__state[__bmdep_word(bpid)] &= ~__bmdep_bit(bpid);
+}
+
+/* When iterating the available portals, this is the exposed config structure */
+struct bm_portal_config {
+	/* This is used for any "core-affine" portals, ie. default portals
+	 * associated to the corresponding cpu. -1 implies that there is no core
+	 * affinity configured. */
+	int cpu;
+	/* portal interrupt line */
+	int irq;
+	/* These are the buffer pool IDs that may be used via this portal. NB,
+	 * this is only enforced in the high-level API. Also, BSCN depletion
+	 * state changes will only be unmasked as/when pool objects are created
+	 * with depletion callbacks - the mask is the superset. */
+	struct bman_depletion mask;
+	/* which portal sub-interfaces are already bound (ie. "in use") */
+	u8 bound;
+};
+/* bm_portal_config::bound uses these bit masks */
+#define BM_BIND_RCR	0x01
+#define BM_BIND_MC	0x02
+#define BM_BIND_ISR	0x04
+
+/* Portal modes.
+ *   Enum types;
+ *     pmode == production mode
+ *     cmode == consumption mode,
+ *   Enum values use 3 letter codes. First letter matches the portal mode,
+ *   remaining two letters indicate;
+ *     ci == cache-inhibited portal register
+ *     ce == cache-enabled portal register
+ *     vb == in-band valid-bit (cache-enabled)
+ */
+enum bm_rcr_pmode {		/* matches BCSP_CFG::RPM */
+	bm_rcr_pci = 0,		/* PI index, cache-inhibited */
+	bm_rcr_pce = 1,		/* PI index, cache-enabled */
+	bm_rcr_pvb = 2		/* valid-bit */
+};
+enum bm_rcr_cmode {		/* s/w-only */
+	bm_rcr_cci,		/* CI index, cache-inhibited */
+	bm_rcr_cce		/* CI index, cache-enabled */
+};
+
+
+/* ------------------------------ */
+/* --- Portal enumeration API --- */
+
+/* Obtain the number of portals available */
+u8 bm_portal_num(void);
+
+/* Obtain a portal handle */
+struct bm_portal *bm_portal_get(u8 idx);
+const struct bm_portal_config *bm_portal_config(const struct bm_portal *portal);
+
+
+/* ------------------------------ */
+/* --- Buffer pool allocation --- */
+
+#ifdef CONFIG_FSL_BMAN_CONFIG
+
+/* Allocate/release an unreserved buffer pool id */
+int bm_pool_new(u32 *bpid);
+void bm_pool_free(u32 bpid);
+
+/* Set depletion thresholds associated with a buffer pool. Requires that the
+ * operating system have access to Bman CCSR (ie. compiled in support and
+ * run-time access courtesy of the device-tree). */
+int bm_pool_set(u32 bpid, const u32 *thresholds);
+#define BM_POOL_THRESH_SW_ENTER 0
+#define BM_POOL_THRESH_SW_EXIT  1
+#define BM_POOL_THRESH_HW_ENTER 2
+#define BM_POOL_THRESH_HW_EXIT  3
+
+#endif /* CONFIG_FSL_BMAN_CONFIG */
+
+
+/* --------------- */
+/* --- RCR API --- */
+
+/* Create/destroy */
+int bm_rcr_init(struct bm_portal *portal, enum bm_rcr_pmode pmode,
+		enum bm_rcr_cmode cmode);
+void bm_rcr_finish(struct bm_portal *portal);
+
+/* Start/abort RCR entry */
+struct bm_rcr_entry *bm_rcr_start(struct bm_portal *portal);
+void bm_rcr_abort(struct bm_portal *portal);
+
+/* For PI modes only. This presumes a started but uncommited RCR entry. If
+ * there's no more room in the RCR, this function returns NULL. Otherwise it
+ * returns the next RCR entry and increments an internal PI counter without
+ * flushing it to h/w. */
+struct bm_rcr_entry *bm_rcr_pend_and_next(struct bm_portal *portal, u8 myverb);
+
+/* Commit RCR entries, including pending ones (aka "write PI") */
+void bm_rcr_pci_commit(struct bm_portal *portal, u8 myverb);
+void bm_rcr_pce_prefetch(struct bm_portal *portal);
+void bm_rcr_pce_commit(struct bm_portal *portal, u8 myverb);
+void bm_rcr_pvb_commit(struct bm_portal *portal, u8 myverb);
+
+/* Track h/w consumption. Returns non-zero if h/w had consumed previously
+ * unconsumed RCR entries. */
+u8 bm_rcr_cci_update(struct bm_portal *portal);
+void bm_rcr_cce_prefetch(struct bm_portal *portal);
+u8 bm_rcr_cce_update(struct bm_portal *portal);
+/* Returns the number of available RCR entries */
+u8 bm_rcr_get_avail(struct bm_portal *portal);
+/* Returns the number of unconsumed RCR entries */
+u8 bm_rcr_get_fill(struct bm_portal *portal);
+
+/* Read/write the RCR interrupt threshold */
+u8 bm_rcr_get_ithresh(struct bm_portal *portal);
+void bm_rcr_set_ithresh(struct bm_portal *portal, u8 ithresh);
+
+
+/* ------------------------------ */
+/* --- Management command API --- */
+
+/* Create/destroy */
+int bm_mc_init(struct bm_portal *portal);
+void bm_mc_finish(struct bm_portal *portal);
+
+/* Start/abort mgmt command */
+struct bm_mc_command *bm_mc_start(struct bm_portal *portal);
+void bm_mc_abort(struct bm_portal *portal);
+
+/* Writes 'verb' with appropriate 'vbit'. Invalidates and pre-fetches the
+ * response. */
+void bm_mc_commit(struct bm_portal *portal, u8 myverb);
+
+/* Poll for result. If NULL, invalidates and prefetches for the next call. */
+struct bm_mc_result *bm_mc_result(struct bm_portal *portal);
+
+
+/* ------------------------------------- */
+/* --- Portal interrupt register API --- */
+
+/* For a quick explanation of the Bman interrupt model, see the comments in the
+ * equivalent section of the qman_portal.h header.
+ */
+
+/* Create/destroy */
+int bm_isr_init(struct bm_portal *portal);
+void bm_isr_finish(struct bm_portal *portal);
+
+/* BSCN masking is a per-portal configuration */
+void bm_isr_bscn_mask(struct bm_portal *portal, u8 bpid, int enable);
+
+/* Used by all portal interrupt registers except 'inhibit' */
+#define BM_PIRQ_RCRI	0x00000002	/* RCR Ring (below threshold) */
+#define BM_PIRQ_BSCN	0x00000001	/* Buffer depletion State Change */
+
+/* These are bm_<reg>_<verb>(). So for example, bm_disable_write() means "write
+ * the disable register" rather than "disable the ability to write". */
+#define bm_isr_status_read(bm)		__bm_isr_read(bm, bm_isr_status)
+#define bm_isr_status_clear(bm, m)	__bm_isr_write(bm, bm_isr_status, m)
+#define bm_isr_enable_read(bm)		__bm_isr_read(bm, bm_isr_enable)
+#define bm_isr_enable_write(bm, v)	__bm_isr_write(bm, bm_isr_enable, v)
+#define bm_isr_disable_read(bm)		__bm_isr_read(bm, bm_isr_disable)
+#define bm_isr_disable_write(bm, v)	__bm_isr_write(bm, bm_isr_disable, v)
+#define bm_isr_inhibit(bm)		__bm_isr_write(bm, bm_isr_inhibit, 1)
+#define bm_isr_uninhibit(bm)		__bm_isr_write(bm, bm_isr_inhibit, 0)
+
+/* Don't use these, use the wrappers above*/
+u32 __bm_isr_read(struct bm_portal *portal, enum bm_isr_reg n);
+void __bm_isr_write(struct bm_portal *portal, enum bm_isr_reg n, u32 val);
+
+
+/* ------------------------------------------------------- */
+/* --- Bman data structures (and associated constants) --- */
+
+/* Code-reduction, define a wrapper for 48-bit buffers. In cases where a buffer
+ * pool id specific to this buffer is needed (BM_RCR_VERB_CMD_BPID_MULTI,
+ * BM_MCC_VERB_ACQUIRE), the 'bpid' field is used. */
+struct bm_buffer {
+	u8 __reserved1;
+	u8 bpid;
+	u16 hi;	/* High 16-bits of 48-bit address */
+	u32 lo;	/* Low 32-bits of 48-bit address */
+} __packed;
+
+/* See 1.5.3.5.4: "Release Command" */
+struct bm_rcr_entry {
+	union {
+		struct {
+			u8 __dont_write_directly__verb;
+			u8 bpid; /* used with BM_RCR_VERB_CMD_BPID_SINGLE */
+			u8 __reserved1[62];
+		};
+		struct bm_buffer bufs[8];
+	};
+} __packed;
+#define BM_RCR_VERB_VBIT		0x80
+#define BM_RCR_VERB_CMD_MASK		0x70	/* one of two values; */
+#define BM_RCR_VERB_CMD_BPID_SINGLE	0x20
+#define BM_RCR_VERB_CMD_BPID_MULTI	0x30
+#define BM_RCR_VERB_BUFCOUNT_MASK	0x0f	/* values 1..8 */
+
+/* See 1.5.3.1: "Acquire Command" */
+/* See 1.5.3.2: "Query Command" */
+struct bm_mc_command {
+	u8 __dont_write_directly__verb;
+	union {
+		struct bm_mcc_acquire {
+			u8 bpid;
+			u8 __reserved1[62];
+		} __packed acquire;
+		struct bm_mcc_query {
+			u8 __reserved1[63];
+		} __packed query;
+	};
+} __packed;
+#define BM_MCC_VERB_VBIT		0x80
+#define BM_MCC_VERB_CMD_MASK		0x70	/* where the verb contains; */
+#define BM_MCC_VERB_CMD_ACQUIRE		0x10
+#define BM_MCC_VERB_CMD_QUERY		0x40
+#define BM_MCC_VERB_ACQUIRE_BUFCOUNT	0x0f	/* values 1..8 go here */
+
+/* See 1.5.3.3: "Acquire Reponse" */
+/* See 1.5.3.4: "Query Reponse" */
+struct bm_mc_result {
+	union {
+		struct {
+			u8 verb;
+			u8 __reserved1[63];
+		};
+		union {
+			struct {
+				u8 __reserved1;
+				u8 bpid;
+				u8 __reserved2[62];
+			};
+			struct bm_buffer bufs[8];
+		} acquire;
+		struct {
+			u8 __reserved1[32];
+			/* "availability state" and "depletion state" */
+			struct {
+				u8 __reserved1[8];
+				/* Access using bman_depletion_***() */
+				struct bman_depletion state;
+			} as, ds;
+		} query;
+	};
+} __packed;
+#define BM_MCR_VERB_VBIT		0x80
+#define BM_MCR_VERB_CMD_MASK		BM_MCC_VERB_CMD_MASK
+#define BM_MCR_VERB_CMD_ACQUIRE		BM_MCC_VERB_CMD_ACQUIRE
+#define BM_MCR_VERB_CMD_QUERY		BM_MCC_VERB_CMD_QUERY
+#define BM_MCR_VERB_CMD_ERR_INVALID	0x60
+#define BM_MCR_VERB_CMD_ERR_ECC		0x70
+#define BM_MCR_VERB_ACQUIRE_BUFCOUNT	BM_MCC_VERB_ACQUIRE_BUFCOUNT /* 0..8 */
+/* Determine the "availability state" of pool 'p' from a query result 'r' */
+#define BM_MCR_QUERY_AVAILABILITY(r,p) bman_depletion_get(&r->query.as.state,p)
+/* Determine the "depletion state" of pool 'p' from a query result 'r' */
+#define BM_MCR_QUERY_DEPLETION(r,p) bman_depletion_get(&r->query.ds.state,p)
+
+/*******************************************************************/
+/* Managed (aka "shared" or "mux/demux") portal, high-level i/face */
+/*******************************************************************/
+
+	/* Portal and Buffer Pools */
+	/* ----------------------- */
+/* Represents a managed portal */
+struct bman_portal;
+
+/* This object type represents Bman buffer pools. */
+struct bman_pool;
+
+/* This callback type is used when handling pool depletion entry/exit. The
+ * 'cb_ctx' value is the opaque value associated with the pool object in
+ * bman_new_pool(). 'depleted' is non-zero on depletion-entry, and zero on
+ * depletion-exit. */
+typedef void (*bman_cb_depletion)(struct bman_portal *bm,
+			struct bman_pool *pool, void *cb_ctx, int depleted);
+
+/* This struct specifies parameters for a bman_pool object. */
+struct bman_pool_params {
+	/* index of the buffer pool to encapsulate (0-63), overwritten if
+	 * BMAN_POOL_FLAG_DYNAMIC_BPID is set. */
+	u32 bpid;
+	/* bit-mask of BMAN_POOL_FLAG_*** options */
+	u32 flags;
+	/* depletion-entry/exit callback, if BMAN_POOL_FLAG_DEPLETION is set */
+	bman_cb_depletion cb;
+	/* opaque user value passed as a parameter to 'cb' */
+	void *cb_ctx;
+	/* depletion-entry/exit thresholds, if BMAN_POOL_FLAG_THRESH is set. NB:
+	 * this is only allowed if BMAN_POOL_FLAG_DYNAMIC_BPID is used *and*
+	 * when run in the control plane (which controls Bman CCSR). This array
+	 * matches the definition of bm_pool_set(). */
+	u32 thresholds[4];
+};
+
+/* Flags to bman_new_pool() */
+#define BMAN_POOL_FLAG_NO_RELEASE    0x00000001 /* can't release to pool */
+#define BMAN_POOL_FLAG_ONLY_RELEASE  0x00000002 /* can only release to pool */
+#define BMAN_POOL_FLAG_DEPLETION     0x00000004 /* track depletion entry/exit */
+#define BMAN_POOL_FLAG_DYNAMIC_BPID  0x00000008 /* (de)allocate bpid */
+#define BMAN_POOL_FLAG_THRESH        0x00000010 /* set depletion thresholds */
+#define BMAN_POOL_FLAG_STOCKPILE     0x00000020 /* stockpile to reduce hw ops */
+
+/* Flags to bman_release() */
+#define BMAN_RELEASE_FLAG_WAIT       0x00000001 /* wait if RCR is full */
+#define BMAN_RELEASE_FLAG_WAIT_INT   0x00000002 /* if we wait, interruptible? */
+#define BMAN_RELEASE_FLAG_WAIT_SYNC  0x00000004 /* if wait, until consumed? */
+#define BMAN_RELEASE_FLAG_NOW        0x00000008 /* issue immediate release */
+
+/* Flags to bman_acquire() */
+#define BMAN_ACQUIRE_FLAG_STOCKPILE  0x00000001 /* no hw op, stockpile only */
+
+	/* Portal Management */
+	/* ----------------- */
+/**
+ * bman_poll - Runs portal updates not triggered by interrupts
+ *
+ * Dispatcher logic on a cpu can use this to trigger any maintenance of the
+ * affine portal. There are two classes of portal processing in question;
+ * fast-path (which involves tracking release ring (RCR) consumption), and
+ * slow-path (which involves RCR thresholds, pool depletion state changes, etc).
+ * The driver is configured to use interrupts for either (a) all processing, (b)
+ * only slow-path processing, or (c) no processing. This function does whatever
+ * processing is not triggered by interrupts.
+ */
+void bman_poll(void);
+
+
+	/* Pool management */
+	/* --------------- */
+/**
+ * bman_new_pool - Allocates a Buffer Pool object
+ * @params: parameters specifying the buffer pool ID and behaviour
+ *
+ * Creates a pool object for the given @params. A portal and the depletion
+ * callback field of @params are only used if the BMAN_POOL_FLAG_DEPLETION flag
+ * is set. NB, the fields from @params are copied into the new pool object, so
+ * the structure provided by the caller can be released or reused after the
+ * function returns.
+ */
+struct bman_pool *bman_new_pool(const struct bman_pool_params *params);
+
+/**
+ * bman_free_pool - Deallocates a Buffer Pool object
+ * @pool: the pool object to release
+ *
+ */
+void bman_free_pool(struct bman_pool *pool);
+
+/**
+ * bman_get_params - Returns a pool object's parameters.
+ * @pool: the pool object
+ *
+ * The returned pointer refers to state within the pool object so must not be
+ * modified and can no longer be read once the pool object is destroyed.
+ */
+const struct bman_pool_params *bman_get_params(const struct bman_pool *pool);
+
+/**
+ * bman_release - Release buffer(s) to the buffer pool
+ * @pool: the buffer pool object to release to
+ * @bufs: an array of buffers to release
+ * @num: the number of buffers in @bufs (1-8)
+ * @flags: bit-mask of BMAN_RELEASE_FLAG_*** options
+ *
+ * Adds the given buffers to RCR entries. If the portal @p was created with the
+ * "COMPACT" flag, then it will be using a compaction algorithm to improve
+ * utilisation of RCR. As such, these buffers may join an existing ring entry
+ * and/or it may not be issued right away so as to allow future releases to join
+ * the same ring entry. Use the BMAN_RELEASE_FLAG_NOW flag to override this
+ * behaviour by committing the RCR entry (or entries) right away. If the RCR
+ * ring is full, the function will return -EBUSY unless BMAN_RELEASE_FLAG_WAIT
+ * is selected, in which case it will sleep waiting for space to become
+ * available in RCR. If the function receives a signal before such time (and
+ * BMAN_RELEASE_FLAG_WAIT_INT is set), the function returns -EINTR. Otherwise,
+ * it returns zero.
+ */
+int bman_release(struct bman_pool *pool, const struct bm_buffer *bufs, u8 num,
+			u32 flags);
+
+/**
+ * bman_acquire - Acquire buffer(s) from a buffer pool
+ * @pool: the buffer pool object to acquire from
+ * @bufs: array for storing the acquired buffers
+ * @num: the number of buffers desired (@bufs is at least this big)
+ *
+ * Issues an "Acquire" command via the portal's management command interface.
+ * The return value will be the number of buffers obtained from the pool, or a
+ * negative error code if a h/w error or pool starvation was encountered.
+ */
+int bman_acquire(struct bman_pool *pool, struct bm_buffer *bufs, u8 num,
+			u32 flags);
+
+#endif /* FSL_BMAN_H */
diff --git a/include/linux/fsl_pme.h b/include/linux/fsl_pme.h
new file mode 100644
index 0000000..840f178
--- /dev/null
+++ b/include/linux/fsl_pme.h
@@ -0,0 +1,650 @@
+/* Copyright (c) 2009, Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef FSL_PME_H
+#define FSL_PME_H
+
+/* pme_fd_res_status() returns this enum */
+enum pme_status {
+	pme_status_ok = 0x00,
+	pme_status_kes_ccl = 0x40, /* KES Confidence Collision Limit */
+	pme_status_kes_cme = 0x41, /* KES Confidence Mask Error */
+	pme_status_dxe_ire = 0x48, /* DXE Invalid Repeat Error */
+	pme_status_dxe_tlse = 0x49, /* DXE Test Line Syntax Error */
+	pme_status_dxe_ile = 0x4b, /* DXE Instruction Limit Error */
+	pme_status_dxe_pdsrsore = 0x4c, /* DXE PDSR Space Out Range Error */
+	pme_status_dxe_soe = 0x4d, /* DXE Stack Overflow Error */
+	pme_status_dxe_alse = 0x4e, /* DXE Alternate Link Same Error */
+	pme_status_dxe_slse = 0x4f, /* DXE Subsequent Link Same Error */
+	pme_status_dxe_slre = 0x50, /* DXE Subsequent Link Reverse Error */
+	pme_status_dxe_itlb = 0x51, /* DXE Invalid Test Line Branch */
+	pme_status_dxe_cle = 0x52, /* DXE Compare Limit Exceeded */
+	pme_status_dxe_mle = 0x53, /* DXE Match Limit Exceeded */
+	pme_status_sre_irhbni = 0x59, /* SRE Invalid Reaction Head Block */
+				      /* Number Instructions */
+	pme_status_sre_rl = 0x5a, /* SRE Reaction Limit */
+	pme_status_sre_pdsrsore = 0x5b, /* SRE PDSR Space Out Range Error */
+	pme_status_sre_score = 0x5c, /* SRE Session Context Out Range Error */
+	pme_status_sre_ctore = 0x5d, /* SRE Context Table Out Range Error */
+	pme_status_sre_il = 0x5e, /* SRE Instruction Limit */
+	pme_status_sre_iij = 0x5f, /* SRE Invalid Instruction Jump */
+	pme_status_sre_ise = 0x60, /* SRE Instruction Syntax Error */
+	pme_status_pmfa_pmtcce = 0x80, /* PMFA PCTCC Error */
+	pme_status_pmfa_fcwe = 0x90, /* PMFA Flow Context Write Command Error */
+	pme_status_pmfa_fcre = 0x91, /* PMFA Flow Context Read Command Error */
+	pme_status_pmfa_ume = 0x93, /* PMFA Unrecognized Mode Error */
+	pme_status_pmfa_uce = 0x94, /* PMFA Unrecognized Command Error */
+	pme_status_pmfa_ufe = 0x95, /* PMFA Unrecognized Frame Error */
+	pme_status_sre_csmre = 0xc0, /* SRE Context System Memory Read Error */
+	pme_status_sre_ismre = 0xc1, /* SRE Instruction System Memory Read */
+				     /* Error */
+	pme_status_dxe_smre = 0xc2, /* DXE System Memory Read Error */
+	pme_status_pmfa_pmtccsmre = 0xc4, /* PMFA PMTCC System Memory Read */
+					  /* Error */
+	pme_status_pmfa_csmre = 0xc5, /* PMFA Context System Memory Read */
+				      /* Error */
+	pme_status_pmfa_dsmre = 0xc6, /* PMFA Data System Memory Read Error */
+	pme_status_kes_cmecce = 0xd2, /* KES Confidence Memory ECC Error */
+	pme_status_kes_2btmecce = 0xd4, /*KES 2-Byte Trigger Memory ECC Error */
+	pme_status_kes_vltmecce = 0xd5, /*KES Variable Length Trigger Memory */
+					/* ECC Error */
+	pme_status_pmfa_cmecce = 0xd7, /* PMFA Confidence Memory ECC Error */
+	pme_status_pmfa_2btmecce = 0xd9, /* PMFA 2-Byte Trigger Memory ECC */
+					 /* Error */
+	pme_status_pmfa_vltmecce = 0xda, /* PMFA Variable Length Trigger */
+					  /* Memory ECC Error */
+	pme_status_dxe_iemce = 0xdb, /* DXE Internal Examination Memory */
+				     /* Collision Error */
+	pme_status_dxe_iemecce = 0xdc, /* DXE Internal Examination Memory */
+				       /* ECC Error */
+	pme_status_dxe_icmecce = 0xdd, /* DXE Internal Context Memory ECC */
+				       /* Error */
+	pme_status_sre_ctsmwe = 0xe0, /* SRE Context Table System Memory */
+				      /* Write Error */
+	pme_status_pmfa_pmtccsmwe = 0xe7, /* PMFA PMTCC System Memory Write */
+					  /* Error */
+	pme_status_pmfa_csmwe = 0xe8, /* PMFA Context System Memory Write */
+				      /* Error */
+	pme_status_pmfa_dsmwe = 0xe9, /* PMFA Data System Memory Write Error */
+};
+
+/* pme_fd_res_flags() returns these flags */
+#define PME_STATUS_UNRELIABLE	0x80
+#define PME_STATUS_TRUNCATED	0x10
+
+/**************/
+/* USER SPACE */
+/**************/
+
+#define PME_IOCTL_MAGIC 'p'
+
+/* Wrapper for a pointer and size. */
+struct pme_buffer {
+	void *data;
+	size_t size;
+};
+
+/***************/
+/* SCAN DEVICE */
+/***************/
+/* The /dev/pme_scan device creates a file-descriptor that uses scheduled FQs
+ * serviced by PME's datapath portal. This can only be used for scanning. */
+#define PME_DEV_SCAN_NODE	"pme_scan"
+#define PME_DEV_SCAN_PATH	"/dev/" PME_DEV_SCAN_NODE
+
+/* ioctls for 'scan' device */
+#define PMEIO_SETSCAN	_IOW(PME_IOCTL_MAGIC, 0x06, struct pme_scan_params)
+#define PMEIO_GETSCAN	_IOR(PME_IOCTL_MAGIC, 0x07, struct pme_scan_params)
+#define PMEIO_RESETSEQ	_IO(PME_IOCTL_MAGIC, 0x08)
+#define PMEIO_RESETRES	_IO(PME_IOCTL_MAGIC, 0x09)
+#define PMEIO_SCAN_W1	_IOW(PME_IOCTL_MAGIC, 0x0a, struct pme_scan_cmd)
+#define PMEIO_SCAN_Wn	_IOWR(PME_IOCTL_MAGIC, 0x0b, struct pme_scan_cmds)
+#define PMEIO_SCAN_R1	_IOR(PME_IOCTL_MAGIC, 0x0c, struct pme_scan_result)
+#define PMEIO_SCAN_Rn	_IOWR(PME_IOCTL_MAGIC, 0x0d, struct pme_scan_results)
+#define PMEIO_SCAN	_IOWR(PME_IOCTL_MAGIC, 0x0e, struct pme_scan)
+/* The release_bufs ioctl takes as parameter a (void *) */
+#define PMEIO_RELEASE_BUFS _IOW(PME_IOCTL_MAGIC, 0x0f, void *)
+
+/* Parameters for PMEIO_SETSCAN and PMEIO_GETSCAN ioctl()s. This doesn't cover
+ * "sequence" fields ('soc' and 'seqnum'), they can only be influenced by flags
+ * passed to scan operations, or by PMEIO_RESETSEQ ioctl()s. */
+struct pme_scan_params {
+	__u32 flags; /* PME_SCAN_PARAMS_*** bitmask */
+	struct pme_scan_params_residue {
+		__u8 enable; /* boolean, residue enable */
+		__u8 length; /* read-only for GETSCAN, ignored for SETSCAN */
+	} residue;
+	struct pme_scan_params_sre {
+		__u32 sessionid; /* 27-bit */
+		__u8 verbose; /* 0-3 */
+		__u8 esee; /* boolean, End Of Sui Event Enable */
+	} sre;
+	struct pme_scan_params_dxe {
+		__u16 clim; /* compare limit */
+		__u16 mlim; /* match limit */
+	} dxe;
+	struct pme_scan_params_pattern {
+		__u8 set;
+		__u16 subset;
+	} pattern;
+};
+#define PME_SCAN_PARAMS_RESIDUE	0x00000001
+#define PME_SCAN_PARAMS_SRE	0x00000002
+#define PME_SCAN_PARAMS_DXE	0x00000004
+#define PME_SCAN_PARAMS_PATTERN	0x00000008
+
+/* argument to PMEIO_SCAN_W1 ioctl */
+struct pme_scan_cmd {
+	__u32 flags; /* PME_SCAN_CMD_*** bitmask */
+	void *opaque; /* value carried through in the pme_scan_result */
+	struct pme_buffer input;
+	struct pme_buffer output; /* ignored for 'RES_BMAN' output */
+};
+#define PME_SCAN_CMD_RES_BMAN	0x00000001 /* use Bman for output */
+#define PME_SCAN_CMD_STARTRESET	0x00000002
+#define PME_SCAN_CMD_END	0x00000004
+
+/* argument to PMEIO_SCAN_Wn ioctl
+ * 'num' indicates how many 'cmds' are present on input and is updated on the
+ * response to indicate how many were sent. */
+struct pme_scan_cmds {
+	unsigned num;
+	struct pme_scan_cmd *cmds;
+};
+
+/* argument to PMEIO_SCAN_R1 ioctl. The ioctl doesn't read any of these
+ * fields, they are only written to. If the output comes from BMAN buffer
+ * then 'flags' will have PME_SCAN_RESULT_BMAN set. */
+struct pme_scan_result {
+	__u8 flags; /* PME_SCAN_RESULT_*** bitmask */
+	enum pme_status status;
+	struct pme_buffer output;
+	void *opaque; /* value carried from the pme_scan_cmd */
+};
+#define PME_SCAN_RESULT_UNRELIABLE	PME_STATUS_UNRELIABLE
+#define PME_SCAN_RESULT_TRUNCATED	PME_STATUS_TRUNCATED
+#define PME_SCAN_RESULT_BMAN		0x01
+
+/* argument to PMEIO_SCAN_Rn ioctl.
+ * 'num' indicates how many 'cmds' are present on input and is updated on the
+ * response to indicate how many were retrieved. */
+struct pme_scan_results {
+	unsigned num;
+	struct pme_scan_result *results;
+};
+
+/* argument to PMEIO_SCANWR ioctl. */
+struct pme_scan {
+	struct pme_scan_cmd cmd;
+	struct pme_scan_result result;
+};
+
+/*************/
+/* DB DEVICE */
+/*************/
+/* The /dev/pme_db device creates a file-descriptor that uses parked FQs
+ * serviced by the PME's EFQC (Exclusive Frame Queue Control) mechanism. This is
+ * usually for PMTCC commands for programming the database, though can also be
+ * used for high-priority scanning. This device would typically require root
+ * perms. The EFQC exclusivity is reference-counted, so by default is asserted
+ * on-demand and released when processing quiesces for the context, but
+ * exclusivity can be maintained across inter-frame gaps using the INC and DEC
+ * ioctls, which provide supplementary increments and decrements of the
+ * reference count. */
+#define PME_DEV_DB_NODE	"pme_db"
+#define PME_DEV_DB_PATH	"/dev/" PME_DEV_DB_NODE
+
+/* ioctls for 'db' device */
+#define PMEIO_EXL_INC	_IO(PME_IOCTL_MAGIC, 0x00)
+#define PMEIO_EXL_DEC	_IO(PME_IOCTL_MAGIC, 0x01)
+#define PMEIO_EXL_GET	_IOR(PME_IOCTL_MAGIC, 0x02, int)
+#define PMEIO_PMTCC	_IOWR(PME_IOCTL_MAGIC, 0x03, struct pme_db)
+#define PMEIO_SRE_RESET	_IOR(PME_IOCTL_MAGIC, 0x04, struct pme_db_sre_reset)
+#define PMEIO_NOP	_IO(PME_IOCTL_MAGIC, 0x05)
+
+/* Database structures */
+#define PME_DB_RESULT_UNRELIABLE	PME_STATUS_UNRELIABLE
+#define PME_DB_RESULT_TRUNCATED		PME_STATUS_TRUNCATED
+
+struct pme_db {
+	struct pme_buffer input;
+	struct pme_buffer output;
+	__u8 flags; /* PME_DB_RESULT_*** bitmask */
+	enum pme_status status;
+};
+
+/* This is related to the sre_reset ioctl */
+#define PME_SRE_RULE_VECTOR_SIZE  8
+struct pme_db_sre_reset {
+	__u32 rule_vector[PME_SRE_RULE_VECTOR_SIZE];
+	__u32 rule_index;
+	__u16 rule_increment;
+	__u32 rule_repetitions;
+	__u16 rule_reset_interval;
+	__u8 rule_reset_priority;
+};
+
+/****************/
+/* KERNEL SPACE */
+/****************/
+
+#ifdef __KERNEL__
+
+#include <linux/fsl_qman.h>
+#include <linux/fsl_bman.h>
+
+/* "struct pme_hw_flow" represents a flow-context resource for h/w, whereas
+ * "struct pme_flow" (below) is the s/w type used to provide (and receive)
+ * parameters to(/from) the h/w resource. */
+struct pme_hw_flow;
+
+/* "struct pme_hw_residue" represents a residue resource for h/w. */
+struct pme_hw_residue;
+
+/* This is the pme_flow structure type, used for querying or updating a PME flow
+ * context */
+struct pme_flow {
+	u8 sos:1;
+	u8 __reserved1:1;
+	u8 srvm:2;
+	u8 esee:1;
+	u8 __reserved2:3;
+	u8 ren:1;
+	u8 rlen:7;
+	/* Sequence Number (48-bit) */
+	u16 seqnum_hi;
+	u32 seqnum_lo;
+	u32 __reserved3;
+	u32 sessionid:27;
+	u32 __reserved4:5;
+	u16 __reserved5;
+	/* Residue pointer (48-bit), ignored if ren==0 */
+	u16 rptr_hi;
+	u32 rptr_lo;
+	u16 clim;
+	u16 mlim;
+	u32 __reserved6;
+} __packed;
+
+/* pme_ctx_ctrl_update_flow(), pme_fd_cmd_fcw() and pme_scan_params::flags
+ * use these; */
+#define PME_CMD_FCW_RES	0x80	/* "Residue": ren, rlen */
+#define PME_CMD_FCW_SEQ	0x40	/* "Sequence": sos, sequnum */
+#define PME_CMD_FCW_SRE	0x20	/* "Stateful Rule": srvm, esee, sessionid */
+#define PME_CMD_FCW_DXE	0x10	/* "Data Examination": clim, mlim */
+#define PME_CMD_FCW_ALL 0xf0
+
+/* pme_ctx_scan() and pme_fd_cmd_scan() use these; */
+#define PME_CMD_SCAN_SRVM(n) ((n) << 3) /* n in [0..3] */
+#define PME_CMD_SCAN_FLUSH 0x04
+#define PME_CMD_SCAN_SR    0x02 /* aka "Start of Flow or Reset */
+#define PME_CMD_SCAN_E     0x01 /* aka "End of Flow */
+
+/***********************/
+/* low-level functions */
+/***********************/
+
+/* (De)Allocate PME hardware resources */
+struct pme_hw_residue *pme_hw_residue_new(void);
+void pme_hw_residue_free(struct pme_hw_residue *);
+struct pme_hw_flow *pme_hw_flow_new(void);
+void pme_hw_flow_free(struct pme_hw_flow *);
+
+/* Software 'flow' structures also have alignment requirements, so use these to
+ * allocate them. */
+struct pme_flow *pme_sw_flow_new(void);
+void pme_sw_flow_free(struct pme_flow *);
+
+/* Fill in an "Initialise FQ" management command for a PME input FQ. NB, the
+ * caller is responsible for setting the following fields, they will not be set
+ * by the API;
+ *   - initfq->fqid, the frame queue to be initialised
+ *   - initfq->count, should most likely be zero. A count of 0 initialises 1 FQ,
+ *   			a count of 1 initialises 2 FQs, etc/
+ * The 'qos' parameter indicates which workqueue in the PME channel the
+ * FQ should schedule to for regular scanning (0..7). If 'flow' is non-NULL the
+ * FQ is configured for Flow Mode, otherwise it is configured for Direct Action
+ * Mode. 'bpid' is the buffer pool ID to use when Bman-based output is
+ * produced, and 'rfqid' is the frame queue ID to enqueue output frames to.
+ * Following this api, when calling qm_mc_commit(), use QM_MCC_VERB_INITFQ_SCHED
+ * for regular PMEscanning or QM_MCC_VERB_INITFQ_PARK for exclusive PME
+ * processing (usually PMTCC).*/
+void pme_initfq(struct qm_mcc_initfq *initfq, struct pme_hw_flow *flow, u8 qos,
+		u8 rbpid, u32 rfqid);
+
+/* Given a dequeued frame from PME, return status/flags */
+static inline enum pme_status pme_fd_res_status(const struct qm_fd *fd)
+{
+	return (enum pme_status)(fd->status >> 24);
+}
+static inline u8 pme_fd_res_flags(const struct qm_fd *fd)
+{
+	return (fd->status >> 16) & 0xff;
+}
+
+/* Fill in a frame descriptor for a NOP command. */
+void pme_fd_cmd_nop(struct qm_fd *fd);
+
+/* Fill in a frame descriptor for a Flow Context Write command. NB, the caller
+ * is responsible for setting all the relevant fields in 'flow', only the
+ * following fields are set by the API;
+ *   - flow->rptr_hi
+ *   - flow->rptr_lo
+ * The fields in 'flow' are divided into 4 groups, 'flags' indicates which of
+ * them should be written to the h/w flow context using PME_CMD_FCW_*** defines.
+ * 'residue' should be non-NULL iff flow->ren is non-zero and PME_CMD_FCW_RES is
+ * set. */
+void pme_fd_cmd_fcw(struct qm_fd *fd, u8 flags, struct pme_flow *flow,
+		struct pme_hw_residue *residue);
+
+/* Fill in a frame descriptor for a Flow Context Read command. */
+void pme_fd_cmd_fcr(struct qm_fd *fd, struct pme_flow *flow);
+
+/* Modify a frame descriptor for a PMTCC command (only modifies 'cmd' field) */
+void pme_fd_cmd_pmtcc(struct qm_fd *fd);
+
+/* Modify a frame descriptor for a Scan command (only modifies 'cmd' field).
+ * 'flags' are chosen from PME_CMD_SCAN_*** symbols. NB, the use of the
+ * intermediary representation (and PME_SCAN_ARGS) improves performance - ie.
+ * if the scan params are essentially constant, this compacts them for storage
+ * into the same format used in the interface to h/w. So it reduces parameter
+ * passing, stack-use, and encoding time. */
+#define PME_SCAN_ARGS(flags, set, subset) \
+({ \
+	u8 __flags461 = (flags); \
+	u8 __set461 = (set); \
+	u16 __subset461 = (subset); \
+	u32 __res461 = ((u32)__flags461 << 24) | \
+			((u32)__set461 << 16) | \
+			(u32)__subset461; \
+	__res461; \
+})
+void pme_fd_cmd_scan(struct qm_fd *fd, u32 args);
+
+/* convert pointer to physical address for use by PME */
+dma_addr_t pme_map(void *ptr);
+int pme_map_error(dma_addr_t dma_addr);
+
+/************************/
+/* high-level functions */
+/************************/
+
+struct pme_ctx;
+
+/* Calls to pme_ctx_scan() and pme_ctx_pmtcc() provide these, and they are
+ * provided back in the completion callback. You can embed this within a larger
+ * structure in order to maintain per-command data of your own. The fields are
+ * owned by the driver until the callback is invoked, so for example do not link
+ * this token into a list while the command is in-flight! */
+struct pme_ctx_token {
+	u32 blob[4];
+	struct list_head node;
+};
+
+/* Scan results invoke a user-provided callback of this type */
+typedef void (*pme_scan_cb)(struct pme_ctx *, const struct qm_fd *,
+				struct pme_ctx_token *);
+
+/* PME "association" - ie. connects two frame-queues, with or without a PME flow
+ * (if not, direct action mode), and manages mux/demux of scans and flow-context
+ * updates. To allow state used by your callback to be stashed, as well as
+ * optimising the PME driver and the Qman driver beneath it, embed this
+ * structure as the first field in your own context structure. */
+struct pme_ctx {
+	struct qman_fq fq;
+	/* IMPORTANT: Set (only) this prior to calling pme_ctx_init(); */
+	pme_scan_cb cb;
+	/* These fields should not be manipulated directly. Also the structure
+	 * may change and/or grow, so avoid making any alignment or size
+	 * assumptions. */
+	atomic_t refs;
+	volatile u32 flags;
+	spinlock_t lock;
+	wait_queue_head_t queue;
+	struct list_head tokens;
+	u32 seq_num;
+	/* TODO: the following "slow-path" values should be bundled into a
+	 * secondary structure so that sizeof(struct pme_ctx) is minimised (for
+	 * stashing of caller-side fast-path state). */
+	u32 uid;
+	struct qman_fq *fqin;
+	struct pme_hw_flow *hw_flow;
+	struct pme_hw_residue *hw_residue;
+	struct qm_fqd_stashing stashing;
+	struct qm_fd update_fd;
+};
+
+/* Flags for pme_ctx_init() */
+#define PME_CTX_FLAG_LOCKED      0x00000001 /* use QMAN_FQ_FLAG_LOCKED */
+#define PME_CTX_FLAG_EXCLUSIVE   0x00000002 /* unscheduled, exclusive mode */
+#define PME_CTX_FLAG_PMTCC       0x00000004 /* PMTCC rather than scanning */
+#define PME_CTX_FLAG_DIRECT      0x00000008 /* Direct Action mode (not Flow) */
+#define PME_CTX_FLAG_NO_ORP      0x00000010 /* Using this flags implies there
+					     * is no risk of enqueue misordering
+					     */
+#define PME_CTX_FLAG_LOCAL       0x00000020 /* Ignore dest, use cpu portal */
+
+/* Flags for operations */
+#define PME_CTX_OP_WAIT          QMAN_ENQUEUE_FLAG_WAIT
+#define PME_CTX_OP_WAIT_INT      QMAN_ENQUEUE_FLAG_WAIT_INT
+#define PME_CTX_OP_RESETRESLEN   0x00000001 /* no en/disable, just set len */
+/* Note that pme_ctx_ctrl_update_flow() also uses PME_CMD_FCW flags, so they
+ * mustn't conflict with PME_CTX_OP_***.
+ * Also, the above are defined to match QMAN_ENQUEUE values for optimisation
+ * purposes (ie. fast-path operations that don't _WAIT will not incur PME->QMAN
+ * flag conversion overheads). */
+
+/**
+ * pme_ctx_init - Initialise a PME context
+ * @ctx: the context structure to initialise
+ * @flags: bit-mask of PME_CTX_FLAG_*** options
+ * @bpid: buffer pool ID used for any Bman-generated output
+ * @qosin: workqueue priority on the PME channel (0-7)
+ * @qosout: workqueue priority on the result channel (0-7)
+ * @dest: channel to receive results from PME
+ * @stashing: desired dequeue stashing behaviour
+ *
+ * This creates and initialises a PME context, composed of two FQs, an optional
+ * flow-context, and scheduling parameters for the datapath. The ctx->cb and
+ * ctx->pool fields must have been initialised prior to calling this api. The
+ * initialised context is left 'disabled', meaning that the FQ towards PME is
+ * Parked and no operations are possible. If PME_CTX_INIT_EXCLUSIVE is specified
+ * in @flags, then the input FQ is not scheduled, otherwise enabling the context
+ * will schedule the FQ to PME. Exclusive access is only available if the driver
+ * is built with control functionality and if the operating system has access to
+ * PME's CCSR map. @qosin applies if EXCLUSIVE is not set, and indicates which
+ * of the PME's 8 prioritised workqueues the FQ should schedule to. @dest
+ * indicates the channel that should receive results from PME, unless
+ * PME_CTX_FLAG_LOCAL is set in which case this parameter is ignored and the
+ * dedicated portal channel for the current cpu will be used instead. @qosout
+ * indicates which of the 8 prioritised workqueus the FQ should schedule to on
+ * the s/w portal. @stashing configures whether FQ context, frame data, and/or
+ * frame annotation should be stashed into cpu cache when dequeuing output, and
+ * if so, how many cachelines.  For the FQ context part, set the number of
+ * cachelines to cover; 1. sizeof(struct qman_fq_base), to accelerate only Qman
+ * driver processing, 2. sizeof(struct pme_ctx), to accelerate Qman and PME
+ * driver processing, or 3. sizeof(<user-struct>), where <user-struct> is the
+ * caller's structure of which the pme_ctx is the first member - this will allow
+ * callbacks to operate on state which has a high probability of already being
+ * in-cache.
+ * Returns 0 on success.
+ */
+int pme_ctx_init(struct pme_ctx *ctx, u32 flags, u32 bpid, u8 qosin,
+			u8 qosout, enum qm_channel dest,
+			const struct qm_fqd_stashing *stashing);
+
+/* Cleanup allocated resources */
+void pme_ctx_finish(struct pme_ctx *ctx);
+
+/* disable a context, on return the ctx is fully disabled (quiesced) and returns
+ * zero, or it returns an error and remains enabled. flags can be:
+ * PME_CTX_OP_WAIT and/or PME_CTX_OP_WAIT_INT */
+int pme_ctx_disable(struct pme_ctx *ctx, u32 flags);
+
+/* enable a context */
+int pme_ctx_enable(struct pme_ctx *ctx);
+
+/* query whether a context is disabled. Returns > 0 if the ctx is disabled. */
+int pme_ctx_is_disabled(struct pme_ctx *ctx);
+
+/* A pre-condition for the following APIs is the ctx must be disabled
+ * dest maybe ignored if the flags parameter indicated LOCAL during the
+ * corresponding pme_ctx_init.
+ */
+int pme_ctx_reconfigure_tx(struct pme_ctx *ctx, u32 bpid, u8 qosin);
+int pme_ctx_reconfigure_rx(struct pme_ctx *ctx, u8 qosout,
+		enum qm_channel dest, const struct qm_fqd_stashing *stashing);
+
+/* Precondition: pme_ctx must be enabled */
+/* NB: _update() and _nop() only return failure if their PME commands weren't
+ * sent. If PME_CTX_OP_WAIT_INT was specified and a signal was received while
+ * waiting for the response, it may return prematurely with success. The caller
+ * can use signal_pending() to deal with any corresponding issues, if required.
+ * Also 'params' may be modified by this call. For instance if
+ * PME_CTX_OP_RESETRESLEN was specified and residue is enabled, then the
+ * params->ren will be set to 1 (in order not to disabled residue).
+ * NB: _update() will overwrite the 'params->rptr_[hi/low]' fields since the
+ * residue resource is managed by this layer.
+ * NB: _read_flow() is a blocking/sleeping and uninterruptible API, so it must
+ * not be called in atomic context and will not break due to signals.
+ * PME_CTX_OP_WAIT flag will be assumed set and PME_CTX_OP_WAIT_INT will be
+ * assumed cleared, irrespective of what is specified in 'flags'.
+ */
+int pme_ctx_ctrl_update_flow(struct pme_ctx *ctx, u32 flags,
+			struct pme_flow *params);
+int pme_ctx_ctrl_read_flow(struct pme_ctx *ctx, u32 flags,
+			struct pme_flow *params);
+int pme_ctx_ctrl_nop(struct pme_ctx *ctx, u32 flags);
+/* This function returns non-zero if a pme_ctx_ctrl_***() operation is still in
+ * progress. If PME_CTX_OP_WAIT isn't used, this may be required in order to
+ * determine completion, eg. to know when one can safely deallocate the 'params'
+ * passed to the above APIs. */
+int pme_ctx_in_ctrl(struct pme_ctx *ctx);
+
+/* if PME_CTX_OP_WAIT is specified, it'll wait (if it has to) to start the scan
+ * but never waits for it to complete. The scan callback serves that purpose.
+ * 'fd' is modified by both these calls, but only the 'cmd' field. The 'args'
+ * parameters is produced by the PME_SCAN_ARGS() inline function. */
+int pme_ctx_scan(struct pme_ctx *ctx, u32 flags, struct qm_fd *fd, u32 args,
+		struct pme_ctx_token *token);
+int pme_ctx_pmtcc(struct pme_ctx *ctx, u32 flags, struct qm_fd *fd,
+		struct pme_ctx_token *token);
+
+/* Precondition: must be PME_CTX_FLAG_EXCLUSIVE */
+int pme_ctx_exclusive_inc(struct pme_ctx *ctx, u32 flags);
+void pme_ctx_exclusive_dec(struct pme_ctx *ctx);
+
+/**************************/
+/* control-plane only API */
+/**************************/
+#ifdef CONFIG_FSL_PME2_CTRL
+
+/* Attributes for pme_reg_[set|get]() */
+enum pme_attr {
+	pme_attr_efqc_int,
+	pme_attr_sw_db,
+	pme_attr_kvlts,
+	pme_attr_max_chain_length,
+	pme_attr_pattern_range_counter_idx,
+	pme_attr_pattern_range_counter_mask,
+	pme_attr_max_allowed_test_line_per_pattern,
+	pme_attr_max_pdsr_index,
+	pme_attr_max_pattern_matches_per_sui,
+	pme_attr_max_pattern_evaluations_per_sui,
+	pme_attr_report_length_limit,
+	pme_attr_end_of_simple_sui_report,
+	pme_attr_aim,
+	pme_attr_sre_context_size,
+	pme_attr_sre_rule_num,
+	pme_attr_sre_session_ctx_num,
+	pme_attr_end_of_sui_reaction_ptr,
+	pme_attr_sre_pscl,
+	pme_attr_sre_max_block_num,
+	pme_attr_sre_max_instruction_limit,
+	pme_attr_sre_max_index_size,
+	pme_attr_sre_max_offset_ctrl,
+	pme_attr_src_id,
+	pme_attr_liodnr,
+	pme_attr_rev1,
+	pme_attr_rev2,
+	pme_attr_srrv0,
+	pme_attr_srrv1,
+	pme_attr_srrv2,
+	pme_attr_srrv3,
+	pme_attr_srrv4,
+	pme_attr_srrv5,
+	pme_attr_srrv6,
+	pme_attr_srrv7,
+	pme_attr_srrfi,
+	pme_attr_srri,
+	pme_attr_srrwc,
+	pme_attr_srrr,
+	pme_attr_trunci,
+	pme_attr_rbc,
+	pme_attr_tbt0ecc1ec,
+	pme_attr_tbt1ecc1ec,
+	pme_attr_vlt0ecc1ec,
+	pme_attr_vlt1ecc1ec,
+	pme_attr_cmecc1ec,
+	pme_attr_dxcmecc1ec,
+	pme_attr_dxemecc1ec,
+	pme_attr_stnib,
+	pme_attr_stnis,
+	pme_attr_stnth1,
+	pme_attr_stnth2,
+	pme_attr_stnthv,
+	pme_attr_stnths,
+	pme_attr_stnch,
+	pme_attr_stnpm,
+	pme_attr_stns1m,
+	pme_attr_stnpmr,
+	pme_attr_stndsr,
+	pme_attr_stnesr,
+	pme_attr_stns1r,
+	pme_attr_stnob,
+	pme_attr_mia_byc,
+	pme_attr_mia_blc,
+	pme_attr_isr,
+	pme_attr_bsc_first,
+	pme_attr_bsc_last = pme_attr_bsc_first + 63,
+
+};
+
+#define pme_attr_bsc(n) (pme_attr_bsc_first + (n))
+/* Get/set driver attributes */
+int pme_attr_set(enum pme_attr attr, u32 val);
+u32 pme_attr_get(enum pme_attr attr);
+#endif /* defined(CONFIG_FSL_PME2_CTRL) */
+
+#endif /* __KERNEL__ */
+
+#endif /* FSL_PME_H */
diff --git a/include/linux/fsl_qman.h b/include/linux/fsl_qman.h
new file mode 100644
index 0000000..3ae33b4
--- /dev/null
+++ b/include/linux/fsl_qman.h
@@ -0,0 +1,1644 @@
+/* Copyright (c) 2008, 2009 Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef FSL_QMAN_H
+#define FSL_QMAN_H
+
+/* Last updated for v00.800 of the BG */
+
+/*************************************************/
+/*   QMan s/w corenet portal, low-level i/face   */
+/*************************************************/
+
+/* Portal constants */
+#define QM_EQCR_SIZE		8
+#define QM_DQRR_SIZE		16
+#define QM_MR_SIZE		8
+/* Hardware constants */
+enum qm_channel {
+	qm_channel_swportal0 = 0, qm_channel_swportal1, qm_channel_swportal2,
+	qm_channel_swportal3, qm_channel_swportal4, qm_channel_swportal5,
+	qm_channel_swportal6, qm_channel_swportal7, qm_channel_swportal8,
+	qm_channel_swportal9,
+	qm_channel_pool1 = 0x21, qm_channel_pool2, qm_channel_pool3,
+	qm_channel_pool4, qm_channel_pool5, qm_channel_pool6,
+	qm_channel_pool7, qm_channel_pool8, qm_channel_pool9,
+	qm_channel_pool10, qm_channel_pool11, qm_channel_pool12,
+	qm_channel_pool13, qm_channel_pool14, qm_channel_pool15,
+	qm_channel_fman0_sp0 = 0x40, qm_channel_fman0_sp1, qm_channel_fman0_sp2,
+	qm_channel_fman0_sp3, qm_channel_fman0_sp4, qm_channel_fman0_sp5,
+	qm_channel_fman0_sp6, qm_channel_fman0_sp7, qm_channel_fman0_sp8,
+	qm_channel_fman0_sp9, qm_channel_fman0_sp10, qm_channel_fman0_sp11,
+	qm_channel_fman1_sp0 = 0x60, qm_channel_fman1_sp1, qm_channel_fman1_sp2,
+	qm_channel_fman1_sp3, qm_channel_fman1_sp4, qm_channel_fman1_sp5,
+	qm_channel_fman1_sp6, qm_channel_fman1_sp7, qm_channel_fman1_sp8,
+	qm_channel_fman1_sp9, qm_channel_fman1_sp10, qm_channel_fman1_sp11,
+	qm_channel_caam = 0x80,
+	qm_channel_pme = 0xa0,
+};
+enum qm_isr_reg {
+	qm_isr_status = 0,
+	qm_isr_enable = 1,
+	qm_isr_disable = 2,
+	qm_isr_inhibit = 3
+};
+enum qm_dc_portal {
+	qm_dc_portal_fman0 = 0,
+	qm_dc_portal_fman1 = 1,
+	qm_dc_portal_caam = 2,
+	qm_dc_portal_pme = 3
+};
+
+/* Represents s/w corenet portal mapped data structures */
+struct qm_eqcr_entry;	/* EQCR (EnQueue Command Ring) entries */
+struct qm_dqrr_entry;	/* DQRR (DeQueue Response Ring) entries */
+struct qm_mr_entry;	/* MR (Message Ring) entries */
+struct qm_mc_command;	/* MC (Management Command) command */
+struct qm_mc_result;	/* MC result */
+
+/* This type represents a s/w corenet portal space, and is used for creating the
+ * portal objects within it (EQCR, DQRR, etc) */
+struct qm_portal;
+
+/* When iterating the available portals, this is the exposed config structure */
+struct qm_portal_config {
+	/* If the caller enables DQRR stashing (and thus wishes to operate the
+	 * portal from only one cpu), this is the logical CPU that the portal
+	 * will stash to. Whether stashing is enabled or not, this setting is
+	 * also used for any "core-affine" portals, ie. default portals
+	 * associated to the corresponding cpu. -1 implies that there is no core
+	 * affinity configured. */
+	int cpu;
+	/* portal interrupt line */
+	int irq;
+	/* The portal's dedicated channel id, use this value for initialising
+	 * frame queues to target this portal when scheduled. */
+	enum qm_channel channel;
+	/* A mask of which pool channels this portal has dequeue access to
+	 * (using QM_SDQCR_CHANNELS_POOL(n) for the bitmask) */
+	u32 pools;
+	/* which portal sub-interfaces are already bound (ie. "in use") */
+	u8 bound;
+	/* does this portal have PAMU assistance from hypervisor? */
+	int has_hv_dma;
+};
+/* qm_portal_config::bound uses these bit masks */
+#define QM_BIND_EQCR	0x01
+#define QM_BIND_DQRR	0x02
+#define QM_BIND_MR	0x04
+#define QM_BIND_MC	0x08
+#define QM_BIND_ISR	0x10
+
+/* This struct represents a pool channel */
+struct qm_pool_channel {
+	/* The QM_SDQCR_CHANNELS_POOL(n) bit that corresponds to this channel */
+	u32 pool;
+	/* The channel id, used for initialising frame queues to target this
+	 * channel. */
+	enum qm_channel channel;
+	/* Bitmask of portal (logical-, not cell-)indices that have dequeue
+	 * access to this channel;
+	 * 0x001 -> qm_portal_get(0)
+	 * 0x002 -> qm_portal_get(1)
+	 * 0x004 -> qm_portal_get(2)
+	 * ...
+	 * 0x200 -> qm_portal_get(9)
+	 */
+	u32 portals;
+};
+
+/* Portal modes.
+ *   Enum types;
+ *     pmode == production mode
+ *     cmode == consumption mode,
+ *     dmode == h/w dequeue mode.
+ *   Enum values use 3 letter codes. First letter matches the portal mode,
+ *   remaining two letters indicate;
+ *     ci == cache-inhibited portal register
+ *     ce == cache-enabled portal register
+ *     vb == in-band valid-bit (cache-enabled)
+ *     dc == DCA (Discrete Consumption Acknowledgement), DQRR-only
+ *   As for "enum qm_dqrr_dmode", it should be self-explanatory.
+ */
+enum qm_eqcr_pmode {		/* matches QCSP_CFG::EPM */
+	qm_eqcr_pci = 0,	/* PI index, cache-inhibited */
+	qm_eqcr_pce = 1,	/* PI index, cache-enabled */
+	qm_eqcr_pvb = 2		/* valid-bit */
+};
+enum qm_eqcr_cmode {		/* s/w-only */
+	qm_eqcr_cci,		/* CI index, cache-inhibited */
+	qm_eqcr_cce		/* CI index, cache-enabled */
+};
+enum qm_dqrr_dmode {		/* matches QCSP_CFG::DP */
+	qm_dqrr_dpush = 0,	/* SDQCR  + VDQCR */
+	qm_dqrr_dpull = 1	/* PDQCR */
+};
+enum qm_dqrr_pmode {		/* s/w-only */
+	qm_dqrr_pci,		/* reads DQRR_PI_CINH */
+	qm_dqrr_pce,		/* reads DQRR_PI_CENA */
+	qm_dqrr_pvb		/* reads valid-bit */
+};
+enum qm_dqrr_cmode {		/* matches QCSP_CFG::DCM */
+	qm_dqrr_cci = 0,	/* CI index, cache-inhibited */
+	qm_dqrr_cce = 1,	/* CI index, cache-enabled */
+	qm_dqrr_cdc = 2		/* Discrete Consumption Acknowledgement */
+};
+enum qm_mr_pmode {		/* s/w-only */
+	qm_mr_pci,		/* reads MR_PI_CINH */
+	qm_mr_pce,		/* reads MR_PI_CENA */
+	qm_mr_pvb		/* reads valid-bit */
+};
+enum qm_mr_cmode {		/* matches QCSP_CFG::MM */
+	qm_mr_cci = 0,		/* CI index, cache-inhibited */
+	qm_mr_cce = 1		/* CI index, cache-enabled */
+};
+
+
+/* ------------------------------ */
+/* --- Portal enumeration API --- */
+
+/* Obtain the number of portals available */
+u8 qm_portal_num(void);
+
+/* Obtain a portal handle and configuration information about it */
+struct qm_portal *qm_portal_get(u8 idx);
+const struct qm_portal_config *qm_portal_config(const struct qm_portal *portal);
+
+
+/* ------------------------------------ */
+/* --- Pool channel enumeration API --- */
+
+/* Obtain a mask of the available pool channels, expressed using
+ * QM_SDQCR_CHANNELS_POOL(n). */
+u32 qm_pools(void);
+
+/* Retrieve a pool channel configuration, given a QM_SDQCR_CHANNEL_POOL(n)
+ * bit-mask (the least significant bit of 'mask' is used if more than one bit is
+ * set). */
+const struct qm_pool_channel *qm_pool_channel(u32 mask);
+
+
+/* ------------------------ */
+/* --- FQ allocator API --- */
+
+/* Flags to qm_fq_free_flags() */
+#define QM_FQ_FREE_WAIT       0x00000001 /* wait if RCR is full */
+#define QM_FQ_FREE_WAIT_INT   0x00000002 /* if wait, interruptible? */
+#define QM_FQ_FREE_WAIT_SYNC  0x00000004 /* if wait, until consumed? */
+
+#ifdef CONFIG_FSL_QMAN_FQALLOCATOR
+
+/* Allocate an unused FQID from the FQ allocator, returns zero for failure */
+u32 qm_fq_new(void);
+/* Release a FQID back to the FQ allocator */
+int qm_fq_free_flags(u32 fqid, u32 flags);
+static inline void qm_fq_free(u32 fqid)
+{
+	if (qm_fq_free_flags(fqid, QM_FQ_FREE_WAIT))
+		BUG();
+}
+
+#else /* !CONFIG_FSL_QMAN_FQALLOCATOR */
+
+#define qm_fq_new()                   0
+#define qm_fq_free_flags(fqid,flags)  BUG()
+#define qm_fq_free(fqid)              BUG()
+
+#endif /* !CONFIG_FSL_QMAN_FQALLOCATOR */
+
+
+/* ---------------- */
+/* --- EQCR API --- */
+
+/* Create/destroy */
+int qm_eqcr_init(struct qm_portal *portal, enum qm_eqcr_pmode pmode,
+		enum qm_eqcr_cmode cmode);
+void qm_eqcr_finish(struct qm_portal *portal);
+
+/* Start/abort EQCR entry */
+struct qm_eqcr_entry *qm_eqcr_start(struct qm_portal *portal);
+void qm_eqcr_abort(struct qm_portal *portal);
+
+/* For PI modes only. This presumes a started but uncommited EQCR entry. If
+ * there's no more room in the EQCR, this function returns NULL. Otherwise it
+ * returns the next EQCR entry and increments an internal PI counter without
+ * flushing it to h/w. */
+struct qm_eqcr_entry *qm_eqcr_pend_and_next(struct qm_portal *portal, u8 myverb);
+
+/* Commit EQCR entries, including pending ones (aka "write PI") */
+void qm_eqcr_pci_commit(struct qm_portal *portal, u8 myverb);
+void qm_eqcr_pce_prefetch(struct qm_portal *portal);
+void qm_eqcr_pce_commit(struct qm_portal *portal, u8 myverb);
+void qm_eqcr_pvb_commit(struct qm_portal *portal, u8 myverb);
+
+/* Track h/w consumption. Returns non-zero if h/w had consumed previously
+ * unconsumed EQCR entries (it returns the number of them in fact). */
+u8 qm_eqcr_cci_update(struct qm_portal *portal);
+void qm_eqcr_cce_prefetch(struct qm_portal *portal);
+u8 qm_eqcr_cce_update(struct qm_portal *portal);
+u8 qm_eqcr_get_ithresh(struct qm_portal *portal);
+void qm_eqcr_set_ithresh(struct qm_portal *portal, u8 ithresh);
+/* Returns the number of available EQCR entries */
+u8 qm_eqcr_get_avail(struct qm_portal *portal);
+/* Returns the number of unconsumed EQCR entries */
+u8 qm_eqcr_get_fill(struct qm_portal *portal);
+
+
+/* ---------------- */
+/* --- DQRR API --- */
+
+/* Create/destroy */
+int qm_dqrr_init(struct qm_portal *portal, enum qm_dqrr_dmode dmode,
+		enum qm_dqrr_pmode pmode, enum qm_dqrr_cmode cmode,
+		/* QCSP_CFG fields; MF, RE, SE (respectively) */
+		u8 max_fill, int stash_ring, int stash_data);
+void qm_dqrr_finish(struct qm_portal *portal);
+
+/* Read 'current' DQRR entry (ie. at the cursor). NB, prefetch generally not
+ * required in pvb mode, as pvb_prefetch() will touch the same cacheline. */
+void qm_dqrr_current_prefetch(struct qm_portal *portal);
+struct qm_dqrr_entry *qm_dqrr_current(struct qm_portal *portal);
+u8 qm_dqrr_cursor(struct qm_portal *portal);
+
+/* Increment 'current' cursor, must not already be at "EOF". Returns number of
+ * remaining DQRR entries, zero if the 'cursor' is now at "EOF". */
+u8 qm_dqrr_next(struct qm_portal *portal);
+
+/* Track h/w production. Returns non-zero if there are new DQRR entries. */
+u8 qm_dqrr_pci_update(struct qm_portal *portal);
+void qm_dqrr_pce_prefetch(struct qm_portal *portal);
+u8 qm_dqrr_pce_update(struct qm_portal *portal);
+void qm_dqrr_pvb_prefetch(struct qm_portal *portal);
+u8 qm_dqrr_pvb_update(struct qm_portal *portal);
+u8 qm_dqrr_get_ithresh(struct qm_portal *portal);
+void qm_dqrr_set_ithresh(struct qm_portal *portal, u8 ithresh);
+u8 qm_dqrr_get_maxfill(struct qm_portal *portal);
+void qm_dqrr_set_maxfill(struct qm_portal *portal, u8 mf);
+
+/* Consume DQRR entries. NB for 'bitmask', 0x8000 represents idx==0, 0x4000 is
+ * idx==1, etc through to 0x0001 being idx==15. */
+void qm_dqrr_cci_consume(struct qm_portal *portal, u8 num);
+void qm_dqrr_cci_consume_to_current(struct qm_portal *portal);
+void qm_dqrr_cce_prefetch(struct qm_portal *portal);
+void qm_dqrr_cce_consume(struct qm_portal *portal, u8 num);
+void qm_dqrr_cce_consume_to_current(struct qm_portal *portal);
+void qm_dqrr_cdc_consume_1(struct qm_portal *portal, u8 idx, int park);
+void qm_dqrr_cdc_consume_1ptr(struct qm_portal *portal, struct qm_dqrr_entry *dq,
+				int park);
+void qm_dqrr_cdc_consume_n(struct qm_portal *portal, u16 bitmask);
+
+/* For CDC; use these to read the effective CI */
+u8 qm_dqrr_cdc_cci(struct qm_portal *portal);
+void qm_dqrr_cdc_cce_prefetch(struct qm_portal *portal);
+u8 qm_dqrr_cdc_cce(struct qm_portal *portal);
+
+/* For CCI/CCE; this returns the s/w-cached CI value */
+u8 qm_dqrr_get_ci(struct qm_portal *portal);
+/*            ; this issues a park-request */
+void qm_dqrr_park(struct qm_portal *portal, u8 idx);
+/*            ; or for the next-to-be-consumed DQRR entry */
+void qm_dqrr_park_ci(struct qm_portal *portal);
+
+/* For qm_dqrr_sdqcr_set(); Choose one SOURCE. Choose one COUNT. Choose one
+ * dequeue TYPE. Choose TOKEN (8-bit).
+ * If SOURCE == CHANNELS,
+ *   Choose CHANNELS_DEDICATED and/or CHANNELS_POOL(n).
+ *   You can choose DEDICATED_PRECEDENCE if the portal channel should have
+ *   priority.
+ * If SOURCE == SPECIFICWQ,
+ *     Either select the work-queue ID with SPECIFICWQ_WQ(), or select the
+ *     channel (SPECIFICWQ_DEDICATED or SPECIFICWQ_POOL()) and specify the
+ *     work-queue priority (0-7) with SPECIFICWQ_WQ() - either way, you get the
+ *     same value.
+ */
+#define QM_SDQCR_SOURCE_CHANNELS	0x0
+#define QM_SDQCR_SOURCE_SPECIFICWQ	0x40000000
+#define QM_SDQCR_COUNT_EXACT1		0x0
+#define QM_SDQCR_COUNT_UPTO3		0x20000000
+#define QM_SDQCR_DEDICATED_PRECEDENCE	0x10000000
+#define QM_SDQCR_TYPE_MASK		0x03000000
+#define QM_SDQCR_TYPE_NULL		0x0
+#define QM_SDQCR_TYPE_PRIO_QOS		0x01000000
+#define QM_SDQCR_TYPE_ACTIVE_QOS	0x02000000
+#define QM_SDQCR_TYPE_ACTIVE		0x03000000
+#define QM_SDQCR_TOKEN_MASK		0x00ff0000
+#define QM_SDQCR_TOKEN_SET(v)		(((v) & 0xff) << 16)
+#define QM_SDQCR_TOKEN_GET(v)		(((v) >> 16) & 0xff)
+#define QM_SDQCR_CHANNELS_DEDICATED	0x00008000
+#define QM_SDQCR_CHANNELS_POOL_MASK	0x00007fff
+#define QM_SDQCR_CHANNELS_POOL(n)	(0x00008000 >> (n))
+#define QM_SDQCR_SPECIFICWQ_MASK	0x000000f7
+#define QM_SDQCR_SPECIFICWQ_DEDICATED	0x00000000
+#define QM_SDQCR_SPECIFICWQ_POOL(n)	((n) << 4)
+#define QM_SDQCR_SPECIFICWQ_WQ(n)	(n)
+void qm_dqrr_sdqcr_set(struct qm_portal *portal, u32 sdqcr);
+u32 qm_dqrr_sdqcr_get(struct qm_portal *portal);
+
+/* For qm_dqrr_vdqcr_set(); Choose one PRECEDENCE. EXACT is optional. Use
+ * NUMFRAMES(n) (6-bit) or NUMFRAMES_TILLEMPTY to fill in the frame-count. Use
+ * FQID(n) to fill in the frame queue ID. */
+#define QM_VDQCR_PRECEDENCE_VDQCR	0x0
+#define QM_VDQCR_PRECEDENCE_SDQCR	0x80000000
+#define QM_VDQCR_EXACT			0x40000000
+#define QM_VDQCR_NUMFRAMES_MASK		0x3f000000
+#define QM_VDQCR_NUMFRAMES_SET(n)	(((n) & 0x3f) << 24)
+#define QM_VDQCR_NUMFRAMES_GET(n)	(((n) >> 24) & 0x3f)
+#define QM_VDQCR_NUMFRAMES_TILLEMPTY	QM_VDQCR_NUMFRAMES_SET(0)
+#define QM_VDQCR_FQID_MASK		0x00ffffff
+#define QM_VDQCR_FQID(n)		((n) & QM_VDQCR_FQID_MASK)
+void qm_dqrr_vdqcr_set(struct qm_portal *portal, u32 vdqcr);
+u32 qm_dqrr_vdqcr_get(struct qm_portal *portal);
+
+/* For qm_dqrr_pdqcr_set(); Choose one MODE. Choose one COUNT.
+ * If MODE==SCHEDULED
+ *   Choose SCHEDULED_CHANNELS or SCHEDULED_SPECIFICWQ. Choose one dequeue TYPE.
+ *   If CHANNELS,
+ *     Choose CHANNELS_DEDICATED and/or CHANNELS_POOL() channels.
+ *     You can choose DEDICATED_PRECEDENCE if the portal channel should have
+ *     priority.
+ *   If SPECIFICWQ,
+ *     Either select the work-queue ID with SPECIFICWQ_WQ(), or select the
+ *     channel (SPECIFICWQ_DEDICATED or SPECIFICWQ_POOL()) and specify the
+ *     work-queue priority (0-7) with SPECIFICWQ_WQ() - either way, you get the
+ *     same value.
+ * If MODE==UNSCHEDULED
+ *     Choose FQID().
+ */
+#define QM_PDQCR_MODE_SCHEDULED		0x0
+#define QM_PDQCR_MODE_UNSCHEDULED	0x80000000
+#define QM_PDQCR_SCHEDULED_CHANNELS	0x0
+#define QM_PDQCR_SCHEDULED_SPECIFICWQ	0x40000000
+#define QM_PDQCR_COUNT_EXACT1		0x0
+#define QM_PDQCR_COUNT_UPTO3		0x20000000
+#define QM_PDQCR_DEDICATED_PRECEDENCE	0x10000000
+#define QM_PDQCR_TYPE_MASK		0x03000000
+#define QM_PDQCR_TYPE_NULL		0x0
+#define QM_PDQCR_TYPE_PRIO_QOS		0x01000000
+#define QM_PDQCR_TYPE_ACTIVE_QOS	0x02000000
+#define QM_PDQCR_TYPE_ACTIVE		0x03000000
+#define QM_PDQCR_CHANNELS_DEDICATED	0x00008000
+#define QM_PDQCR_CHANNELS_POOL(n)	(0x00008000 >> (n))
+#define QM_PDQCR_SPECIFICWQ_MASK	0x000000f7
+#define QM_PDQCR_SPECIFICWQ_DEDICATED	0x00000000
+#define QM_PDQCR_SPECIFICWQ_POOL(n)	((n) << 4)
+#define QM_PDQCR_SPECIFICWQ_WQ(n)	(n)
+#define QM_PDQCR_FQID(n)		((n) & 0xffffff)
+void qm_dqrr_pdqcr_set(struct qm_portal *portal, u32 pdqcr);
+u32 qm_dqrr_pdqcr_get(struct qm_portal *portal);
+
+
+/* -------------- */
+/* --- MR API --- */
+
+/* Create/destroy */
+int qm_mr_init(struct qm_portal *portal, enum qm_mr_pmode pmode,
+		enum qm_mr_cmode cmode);
+void qm_mr_finish(struct qm_portal *portal);
+
+/* Read 'current' MR entry (ie. at the cursor) */
+void qm_mr_current_prefetch(struct qm_portal *portal);
+struct qm_mr_entry *qm_mr_current(struct qm_portal *portal);
+u8 qm_mr_cursor(struct qm_portal *portal);
+
+/* Increment 'current' cursor, must not alreday be at "EOF". Returns number of
+ * remaining MR entries, zero if the 'cursor' is now at "EOF". */
+u8 qm_mr_next(struct qm_portal *portal);
+
+/* Track h/w production. Returns non-zero if there are new DQRR entries. */
+u8 qm_mr_pci_update(struct qm_portal *portal);
+void qm_mr_pce_prefetch(struct qm_portal *portal);
+u8 qm_mr_pce_update(struct qm_portal *portal);
+void qm_mr_pvb_prefetch(struct qm_portal *portal);
+u8 qm_mr_pvb_update(struct qm_portal *portal);
+u8 qm_mr_get_ithresh(struct qm_portal *portal);
+void qm_mr_set_ithresh(struct qm_portal *portal, u8 ithresh);
+
+/* Consume MR entries */
+void qm_mr_cci_consume(struct qm_portal *portal, u8 num);
+void qm_mr_cci_consume_to_current(struct qm_portal *portal);
+void qm_mr_cce_prefetch(struct qm_portal *portal);
+void qm_mr_cce_consume(struct qm_portal *portal, u8 num);
+void qm_mr_cce_consume_to_current(struct qm_portal *portal);
+
+/* Return the s/w-cached CI value */
+u8 qm_mr_get_ci(struct qm_portal *portal);
+
+
+/* ------------------------------ */
+/* --- Management command API --- */
+
+/* Create/destroy */
+int qm_mc_init(struct qm_portal *portal);
+void qm_mc_finish(struct qm_portal *portal);
+
+/* Start/abort mgmt command */
+struct qm_mc_command *qm_mc_start(struct qm_portal *portal);
+void qm_mc_abort(struct qm_portal *portal);
+
+/* Writes 'verb' with appropriate 'vbit'. Invalidates and pre-fetches the
+ * response. */
+void qm_mc_commit(struct qm_portal *portal, u8 myverb);
+
+/* Poll for result. If NULL, invalidates and prefetches for the next call. */
+struct qm_mc_result *qm_mc_result(struct qm_portal *portal);
+
+
+/* ------------------------------------- */
+/* --- Portal interrupt register API --- */
+
+/* Quick explanation of the Qman interrupt model. Each bit has a source
+ * condition, that source is asserted iff the condition is true. Eg. Each
+ * DQAVAIL source bit tracks whether the corresponding channel's work queues
+ * contain any truly scheduled frame queues. That source exists "asserted" if
+ * and while there are truly-scheduled FQs available, it is deasserted as/when
+ * there are no longer any truly-scheduled FQs available. The same is true for
+ * the various other interrupt source conditions (QM_PIRQ_***). The following
+ * steps indicate what those source bits affect;
+ *    1. if the corresponding bit is set in the disable register, the source
+ *       bit is masked off, we never see any effect from it.
+ *    2. otherwise, the corresponding bit is set in the status register. Once
+ *       asserted in the status register, it must be write-1-to-clear'd - the
+ *       status register bit will stay set even if the source condition
+ *       deasserts.
+ *    3. if a bit is set in the status register but *not* set in the enable
+ *       register, it will not cause the interrupt to assert. Other bits may
+ *       still cause the interrupt to assert of course, and a read of the
+ *       status register can still reveal un-enabled bits - this is why the
+ *       enable and disable registers aren't strictly speaking "opposites".
+ *       "Un-enabled" means it won't, on its own, trigger an interrupt.
+ *       "Disabled" means it won't even show up in the status register.
+ *    4. if a bit is set in the status register *and* the enable register, the
+ *       interrupt line will assert if and only if the inhibit register is
+ *       zero. The inhibit register is the only interrupt-related register that
+ *       does not share the bit definitions - it is a boolean on/off register.
+ */
+
+/* Create/destroy */
+int qm_isr_init(struct qm_portal *portal);
+void qm_isr_finish(struct qm_portal *portal);
+
+/* Used by all portal interrupt registers except 'inhibit' */
+#define QM_PIRQ_CSCI	0x00100000	/* Congestion State Change */
+#define QM_PIRQ_EQCI	0x00080000	/* Enqueue Command Committed */
+#define QM_PIRQ_EQRI	0x00040000	/* EQCR Ring (below threshold) */
+#define QM_PIRQ_DQRI	0x00020000	/* DQRR Ring (non-empty) */
+#define QM_PIRQ_MRI	0x00010000	/* MR Ring (non-empty) */
+#define QM_PIRQ_DQAVAIL	0x0000ffff	/* Channels with frame availability */
+/* The DQAVAIL interrupt fields break down into these bits; */
+#define QM_DQAVAIL_PORTAL	0x8000		/* Portal channel */
+#define QM_DQAVAIL_POOL(n)	(0x8000 >> (n))	/* Pool channel, n==[1..15] */
+
+/* These are qm_<reg>_<verb>(). So for example, qm_disable_write() means "write
+ * the disable register" rather than "disable the ability to write". */
+#define qm_isr_status_read(qm)		__qm_isr_read(qm, qm_isr_status)
+#define qm_isr_status_clear(qm, m)	__qm_isr_write(qm, qm_isr_status, m)
+#define qm_isr_enable_read(qm)		__qm_isr_read(qm, qm_isr_enable)
+#define qm_isr_enable_write(qm, v)	__qm_isr_write(qm, qm_isr_enable, v)
+#define qm_isr_disable_read(qm)		__qm_isr_read(qm, qm_isr_disable)
+#define qm_isr_disable_write(qm, v)	__qm_isr_write(qm, qm_isr_disable, v)
+/* TODO: unfortunate name-clash here, reword? */
+#define qm_isr_inhibit(qm)		__qm_isr_write(qm, qm_isr_inhibit, 1)
+#define qm_isr_uninhibit(qm)		__qm_isr_write(qm, qm_isr_inhibit, 0)
+
+/* Don't use these, use the wrappers above*/
+u32 __qm_isr_read(struct qm_portal *portal, enum qm_isr_reg n);
+void __qm_isr_write(struct qm_portal *portal, enum qm_isr_reg n, u32 val);
+
+
+/* ------------------------------------------------------- */
+/* --- Qman data structures (and associated constants) --- */
+
+/* See David Lapp's "Frame formats" document, "dpateam", Jan 07, 2008 */
+#define QM_FD_FORMAT_SG		0x4
+#define QM_FD_FORMAT_LONG	0x2
+#define QM_FD_FORMAT_COMPOUND	0x1
+enum qm_fd_format {
+	/* 'contig' implies a contiguous buffer, whereas 'sg' implies a
+	 * scatter-gather table. 'big' implies a 29-bit length with no offset
+	 * field, otherwise length is 20-bit and offset is 9-bit. 'compound'
+	 * implies a s/g-like table, where each entry itself represents a frame
+	 * (contiguous or scatter-gather) and the 29-bit "length" is
+	 * interpreted purely for congestion calculations, ie. a "congestion
+	 * weight". */
+	qm_fd_contig = 0,
+	qm_fd_contig_big = QM_FD_FORMAT_LONG,
+	qm_fd_sg = QM_FD_FORMAT_SG,
+	qm_fd_sg_big = QM_FD_FORMAT_SG | QM_FD_FORMAT_LONG,
+	qm_fd_compound = QM_FD_FORMAT_COMPOUND
+};
+
+/* See 1.5.1.1: "Frame Descriptor (FD)" */
+struct qm_fd {
+	u8 dd:2;	/* dynamic debug */
+	u8 pid:6;	/* Partition ID */
+	u8 bpid;	/* Buffer Pool ID */
+	u16 addr_hi;	/* high 16-bits of 48-bit address */
+	u32 addr_lo;	/* low 32-bits of 48-bit address */
+	/* The 'format' field indicates the interpretation of the remaining 29
+	 * bits of the 32-bit word. For packing reasons, it is duplicated in the
+	 * other union elements. */
+	union {
+		/* If 'format' is _contig or _sg, 20b length and 9b offset */
+		struct {
+			enum qm_fd_format format:3;
+			u16 offset:9;
+			u32 length20:20;
+		} __packed;
+		/* If 'format' is _contig_big or _sg_big, 29b length */
+		struct {
+			enum qm_fd_format _format1:3;
+			u32 length29:29;
+		} __packed;
+		/* If 'format' is _compound, 29b "congestion weight" */
+		struct {
+			enum qm_fd_format _format2:3;
+			u32 cong_weight:29;
+		} __packed;
+		/* For easier/faster copying of this part of the fd (eg. from a
+		 * DQRR entry to an EQCR entry) copy 'opaque' */
+		u32 opaque;
+	} __packed;
+	union {
+		u32 cmd;
+		u32 status;
+	};
+} __packed;
+#define QM_FD_DD_NULL		0x00
+#define QM_FD_PID_MASK		0x3f
+
+/* See 2.2.1.3 Multi-Core Datapath Acceleration Architecture */
+struct qm_sg_entry {
+	u16 __reserved1;
+	u16 addr_hi;		/* high 16-bits of 48-bit address */
+	u32 addr_lo;		/* low 32-bits of 48-bit address */
+	u32 extension:1; 	/* Extension bit */
+	u32 final:1; 		/* Final bit */
+	u32 length:30;
+	u8 __reserved2;
+	u8 bpid;
+	u16 __reserved3:3;
+	u16 offset:13;
+} __packed;
+
+/* See 1.5.8.1: "Enqueue Command" */
+struct qm_eqcr_entry {
+	u8 __dont_write_directly__verb;
+	u8 dca;
+	u16 seqnum;
+	u32 orp;	/* 24-bit */
+	u32 fqid;	/* 24-bit */
+	u32 tag;
+	struct qm_fd fd;
+	u8 __reserved3[32];
+} __packed;
+#define QM_EQCR_VERB_VBIT		0x80
+#define QM_EQCR_VERB_CMD_MASK		0x61	/* but only one value; */
+#define QM_EQCR_VERB_CMD_ENQUEUE	0x01
+#define QM_EQCR_VERB_COLOUR_MASK	0x18	/* 4 possible values; */
+#define QM_EQCR_VERB_COLOUR_GREEN	0x00
+#define QM_EQCR_VERB_COLOUR_YELLOW	0x08
+#define QM_EQCR_VERB_COLOUR_RED		0x10
+#define QM_EQCR_VERB_COLOUR_OVERRIDE	0x18
+#define QM_EQCR_VERB_INTERRUPT		0x04	/* on command consumption */
+#define QM_EQCR_VERB_ORP		0x02	/* enable order restoration */
+#define QM_EQCR_DCA_ENABLE		0x80
+#define QM_EQCR_DCA_PARK		0x40
+#define QM_EQCR_DCA_IDXMASK		0x0f	/* "DQRR::idx" goes here */
+#define QM_EQCR_SEQNUM_NESN		0x8000	/* Advance NESN */
+#define QM_EQCR_SEQNUM_NLIS		0x4000	/* More fragments to come */
+#define QM_EQCR_SEQNUM_SEQMASK		0x3fff	/* sequence number goes here */
+#define QM_EQCR_FQID_NULL		0	/* eg. for an ORP seqnum hole */
+
+/* See 1.5.8.2: "Frame Dequeue Response" */
+struct qm_dqrr_entry {
+	u8 verb;
+	u8 stat;
+	u16 seqnum;	/* 15-bit */
+	u8 tok;
+	u8 __reserved2[3];
+	u32 fqid;	/* 24-bit */
+	u32 contextB;
+	struct qm_fd fd;
+	u8 __reserved4[32];
+} __packed;
+#define QM_DQRR_VERB_VBIT		0x80
+#define QM_DQRR_VERB_MASK		0x7f	/* where the verb contains; */
+#define QM_DQRR_VERB_FRAME_DEQUEUE	0x60	/* "this format" */
+#define QM_DQRR_STAT_FQ_EMPTY		0x80	/* FQ empty */
+#define QM_DQRR_STAT_FQ_HELDACTIVE	0x40	/* FQ held active */
+#define QM_DQRR_STAT_FQ_FORCEELIGIBLE	0x20	/* FQ was force-eligible'd */
+#define QM_DQRR_STAT_FD_VALID		0x10	/* has a non-NULL FD */
+#define QM_DQRR_STAT_UNSCHEDULED	0x02	/* Unscheduled dequeue */
+#define QM_DQRR_STAT_DQCR_EXPIRED	0x01	/* VDQCR or PDQCR expired*/
+
+/* See 1.5.8.3: "ERN Message Response" */
+/* See 1.5.8.4: "FQ State Change Notification" */
+struct qm_mr_entry {
+	u8 verb;
+	union {
+		struct {
+			u8 dca;
+			u16 seqnum;
+			u8 rc;		/* Rejection Code */
+			u32 orp:24;
+			u32 fqid;	/* 24-bit */
+			u32 tag;
+			struct qm_fd fd;
+		} __packed ern;
+		struct {
+			u8 colour:2;	/* See QM_MR_DCERN_COLOUR_* */
+			u8 __reserved1:4;
+			enum qm_dc_portal portal:2;
+			u16 __reserved2;
+			u8 rc;		/* Rejection Code */
+			u32 __reserved3:24;
+			u32 fqid;	/* 24-bit */
+			u32 tag;
+			struct qm_fd fd;
+		} __packed dcern;
+		struct {
+			u8 fqs;		/* Frame Queue Status */
+			u8 __reserved1[6];
+			u32 fqid;	/* 24-bit */
+			u32 contextB;
+			u8 __reserved2[16];
+		} __packed fq;		/* FQRN/FQRNI/FQRL/FQPN */
+	};
+	u8 __reserved2[32];
+} __packed;
+#define QM_MR_VERB_VBIT			0x80
+/* The "ern" VERB bits match QM_EQCR_VERB_*** so aren't reproduced here. ERNs
+ * originating from direct-connect portals ("dcern") use 0x20 as a verb which
+ * would be invalid as a s/w enqueue verb. A s/w ERN can be distinguished from
+ * the other MR types by noting if the 0x20 bit is unset. */
+#define QM_MR_VERB_TYPE_MASK		0x23
+#define QM_MR_VERB_DC_ERN		0x20
+#define QM_MR_VERB_FQRN			0x21
+#define QM_MR_VERB_FQRNI		0x22
+#define QM_MR_VERB_FQRL			0x23
+#define QM_MR_VERB_FQPN			0x24
+#define QM_MR_RC_MASK			0xf0	/* contains one of; */
+#define QM_MR_RC_CGR_TAILDROP		0x00
+#define QM_MR_RC_WRED			0x10
+#define QM_MR_RC_ERROR			0x20
+#define QM_MR_RC_ORPWINDOW_EARLY	0x30
+#define QM_MR_RC_ORPWINDOW_LATE		0x40
+#define QM_MR_RC_FQ_TAILDROP		0x50
+#define QM_MR_RC_ORPWINDOW_RETIRED	0x60
+#define QM_MR_FQS_ORLPRESENT		0x02	/* ORL fragments to come */
+#define QM_MR_FQS_NOTEMPTY		0x01	/* FQ has enqueued frames */
+#define QM_MR_DCERN_COLOUR_GREEN	0x00
+#define QM_MR_DCERN_COLOUR_YELLOW	0x01
+#define QM_MR_DCERN_COLOUR_RED		0x02
+#define QM_MR_DCERN_COLOUR_OVERRIDE	0x03
+
+/* This identical structure of FQD fields is present in the "Init FQ" command
+ * and the "Query FQ" result. It's suctioned out here into its own struct. It's
+ * also used as the qman_query_fq() result structure in the high-level API. */
+struct qm_fqd {
+	union {
+		u8 orpc;
+		struct {
+			u8 __reserved1:2;
+			u8 orprws:3;
+			u8 oa:1;
+			u8 olws:2;
+		} __packed;
+	};
+	u8 cgid;
+	u16 fq_ctrl;	/* See QM_FQCTRL_<...> */
+	union {
+		u16 dest_wq;
+		struct {
+			u16 channel:13; /* enum qm_channel */
+			u16 wq:3;
+		} __packed dest;
+	};
+	u16 __reserved2:1;
+	u16 ics_cred:15;
+	union {
+		u16 td_thresh;
+		struct {
+			u16 __reserved1:3;
+			u16 exp:5;
+			u16 mant:8;
+		} __packed td;
+	};
+	u32 context_b;
+	union {
+		/* Treat it as 64-bit opaque */
+		struct {
+			u32 hi;
+			u32 lo;
+		};
+		/* Treat it as s/w portal stashing config */
+		/* See 1.5.6.7.1: "FQD Context_A field used for [...] */
+		struct {
+			struct qm_fqd_stashing {
+				/* See QM_STASHING_EXCL_<...> */
+				u8 exclusive;
+				u8 __reserved1:2;
+				/* Numbers of cachelines */
+				u8 annotation_cl:2;
+				u8 data_cl:2;
+				u8 context_cl:2;
+			} __packed stashing;
+			/* 48-bit address of FQ context to
+			 * stash, must be cacheline-aligned */
+			u16 context_hi;
+			u32 context_lo;
+		} __packed;
+	} context_a;
+} __packed;
+
+/* See 1.5.2.2: "Frame Queue Descriptor (FQD)" */
+/* Frame Queue Descriptor (FQD) field 'fq_ctrl' uses these constants */
+#define QM_FQCTRL_MASK		0x07ff	/* 'fq_ctrl' flags; */
+#define QM_FQCTRL_CGE		0x0400	/* Congestion Group Enable */
+#define QM_FQCTRL_TDE		0x0200	/* Tail-Drop Enable */
+#define QM_FQCTRL_ORP		0x0100	/* ORP Enable */
+#define QM_FQCTRL_CTXASTASHING	0x0080	/* Context-A stashing */
+#define QM_FQCTRL_CPCSTASH	0x0040	/* CPC Stash Enable */
+#define QM_FQCTRL_FORCESFDR	0x0008	/* High-priority SFDRs */
+#define QM_FQCTRL_AVOIDBLOCK	0x0004	/* Don't block active */
+#define QM_FQCTRL_HOLDACTIVE	0x0002	/* Hold active in portal */
+#define QM_FQCTRL_LOCKINCACHE	0x0001	/* Aggressively cache FQD */
+
+/* See 1.5.6.7.1: "FQD Context_A field used for [...] */
+/* Frame Queue Descriptor (FQD) field 'CONTEXT_A' uses these constants */
+#define QM_STASHING_EXCL_ANNOTATION	0x04
+#define QM_STASHING_EXCL_DATA		0x02
+#define QM_STASHING_EXCL_CTX		0x01
+
+/* See 1.5.8.4: "FQ State Change Notification" */
+/* This struct represents the 32-bit "WR_PARM_[GYR]" parameters in CGR fields
+ * and associated commands/responses. The WRED parameters are calculated from
+ * these fields as follows;
+ *   MaxTH = MA * (2 ^ Mn)
+ *   Slope = SA / (2 ^ Sn)
+ *    MaxP = 4 * (Pn + 1)
+ */
+struct qm_cgr_wr_parm {
+	union {
+		u32 word;
+		struct {
+			u32 MA:8;
+			u32 Mn:5;
+			u32 SA:7; /* must be between 64-127 */
+			u32 Sn:6;
+			u32 Pn:6;
+		} __packed;
+	};
+} __packed;
+/* This struct represents the 13-bit "CS_THRES" CGR field. In the corresponding
+ * management commands, this is padded to a 16-bit structure field, so that's
+ * how we represent it here. The congestion state threshold is calculated from
+ * these fields as follows;
+ *   CS threshold = TA * (2 ^ Tn)
+ */
+struct qm_cgr_cs_thres {
+	u16 __reserved:3;
+	u16 TA:8;
+	u16 Tn:5;
+} __packed;
+
+/* This identical structure of CGR fields is present in the "Init/Modify CGR"
+ * commands and the "Query CGR" result. It's suctioned out here into its own
+ * struct. */
+struct __qm_mc_cgr {
+	struct qm_cgr_wr_parm wr_parm_g;
+	struct qm_cgr_wr_parm wr_parm_y;
+	struct qm_cgr_wr_parm wr_parm_r;
+	u8 wr_en_g;	/* boolean, use QM_CGR_EN */
+	u8 wr_en_y;	/* boolean, use QM_CGR_EN */
+	u8 wr_en_r;	/* boolean, use QM_CGR_EN */
+	u8 cscn_en;	/* boolean, use QM_CGR_EN */
+	u32 cscn_targ;	/* use QM_CGR_TARG_* */
+	u8 cstd_en;	/* boolean, use QM_CGR_EN */
+	u8 cs;		/* boolean, only used in query response */
+	struct qm_cgr_cs_thres cs_thres;
+} __packed;
+#define QM_CGR_EN		0x01 /* For wr_en_*, cscn_en, cstd_en */
+#define QM_CGR_TARG_PORTAL(n)	(0x80000000 >> (n)) /* s/w portal, 0-9 */
+#define QM_CGR_TARG_FMAN0	0x00200000 /* direct-connect portal: fman0 */
+#define QM_CGR_TARG_FMAN1	0x00100000 /*                      : fman1 */
+
+/* See 1.5.8.5.1: "Initialize FQ" */
+/* See 1.5.8.5.2: "Query FQ" */
+/* See 1.5.8.5.3: "Query FQ Non-Programmable Fields" */
+/* See 1.5.8.5.4: "Alter FQ State Commands " */
+/* See 1.5.8.6.1: "Initialize/Modify CGR" */
+/* See 1.5.8.6.2: "Query CGR" */
+/* See 1.5.8.6.3: "Query Congestion Group State" */
+struct qm_mc_command {
+	u8 __dont_write_directly__verb;
+	union {
+		struct qm_mcc_initfq {
+			u8 __reserved1;
+			u16 we_mask;	/* Write Enable Mask */
+			u32 fqid;	/* 24-bit */
+			u16 count;	/* Initialises 'count+1' FQDs */
+			struct qm_fqd fqd; /* the FQD fields go here */
+			u8 __reserved3[32];
+		} __packed initfq;
+		struct qm_mcc_queryfq {
+			u8 __reserved1[3];
+			u32 fqid;	/* 24-bit */
+			u8 __reserved2[56];
+		} __packed queryfq;
+		struct qm_mcc_queryfq_np {
+			u8 __reserved1[3];
+			u32 fqid;	/* 24-bit */
+			u8 __reserved2[56];
+		} __packed queryfq_np;
+		struct qm_mcc_alterfq {
+			u8 __reserved1[3];
+			u32 fqid;	/* 24-bit */
+			u8 __reserved2[56];
+		} __packed alterfq;
+		struct qm_mcc_initcgr {
+			u8 __reserved1;
+			u16 we_mask;	/* Write Enable Mask */
+			struct __qm_mc_cgr cgr;	/* CGR fields */
+			u8 __reserved2[3];
+			u8 cgid;
+			u8 __reserved4[32];
+		} __packed initcgr;
+		struct qm_mcc_querycgr {
+			u8 __reserved1[30];
+			u8 cgid;
+			u8 __reserved2[32];
+		} __packed querycgr;
+		struct qm_mcc_querycongestion {
+			u8 __reserved[63];
+		} __packed querycongestion;
+		struct qm_mcc_querywq {
+			u8 __reserved;
+			/* select channel if verb != QUERYWQ_DEDICATED */
+			union {
+				u16 channel_wq; /* ignores wq (3 lsbits) */
+				struct {
+					u16 id:13; /* enum qm_channel */
+					u16 __reserved1:3;
+				} __packed channel;
+			};
+			u8 __reserved2[60];
+		} __packed querywq;
+	};
+} __packed;
+#define QM_MCC_VERB_VBIT		0x80
+#define QM_MCC_VERB_MASK		0x7f	/* where the verb contains; */
+#define QM_MCC_VERB_INITFQ_PARKED	0x40
+#define QM_MCC_VERB_INITFQ_SCHED	0x41
+#define QM_MCC_VERB_QUERYFQ		0x44
+#define QM_MCC_VERB_QUERYFQ_NP		0x45	/* "non-programmable" fields */
+#define QM_MCC_VERB_QUERYWQ		0x46
+#define QM_MCC_VERB_QUERYWQ_DEDICATED	0x47
+#define QM_MCC_VERB_ALTER_SCHED		0x48	/* Schedule FQ */
+#define QM_MCC_VERB_ALTER_FE		0x49	/* Force Eligible FQ */
+#define QM_MCC_VERB_ALTER_RETIRE	0x4a	/* Retire FQ */
+#define QM_MCC_VERB_ALTER_OOS		0x4b	/* Take FQ out of service */
+#define QM_MCC_VERB_INITCGR		0x50
+#define QM_MCC_VERB_MODIFYCGR		0x51
+#define QM_MCC_VERB_QUERYCGR		0x58
+#define QM_MCC_VERB_QUERYCONGESTION	0x59
+/* INITFQ-specific flags */
+#define QM_INITFQ_WE_MASK		0x00ff	/* 'Write Enable' flags; */
+#define QM_INITFQ_WE_ORPC		0x0080
+#define QM_INITFQ_WE_CGID		0x0040
+#define QM_INITFQ_WE_FQCTRL		0x0020
+#define QM_INITFQ_WE_DESTWQ		0x0010
+#define QM_INITFQ_WE_ICSCRED		0x0008
+#define QM_INITFQ_WE_TDTHRESH		0x0004
+#define QM_INITFQ_WE_CONTEXTB		0x0002
+#define QM_INITFQ_WE_CONTEXTA		0x0001
+/* INITCGR/MODIFYCGR-specific flags */
+#define QM_CGR_WE_MASK			0x07ff	/* 'Write Enable Mask'; */
+#define QM_CGR_WE_WR_PARM_G		0x0400
+#define QM_CGR_WE_WR_PARM_Y		0x0200
+#define QM_CGR_WE_WR_PARM_R		0x0100
+#define QM_CGR_WE_WR_EN_G		0x0080
+#define QM_CGR_WE_WR_EN_Y		0x0040
+#define QM_CGR_WE_WR_EN_R		0x0020
+#define QM_CGR_WE_CSCN_EN		0x0010
+#define QM_CGR_WE_CSCN_TARG		0x0008
+#define QM_CGR_WE_CSTD_EN		0x0004
+#define QM_CGR_WE_CS_THRES		0x0002
+
+/* See 1.5.8.5.1: "Initialize FQ" */
+/* See 1.5.8.5.2: "Query FQ" */
+/* See 1.5.8.5.3: "Query FQ Non-Programmable Fields" */
+/* See 1.5.8.5.4: "Alter FQ State Commands " */
+/* See 1.5.8.6.1: "Initialize/Modify CGR" */
+/* See 1.5.8.6.2: "Query CGR" */
+/* See 1.5.8.6.3: "Query Congestion Group State" */
+struct qm_mc_result {
+	u8 verb;
+	u8 result;
+	union {
+		struct qm_mcr_initfq {
+			u8 __reserved1[62];
+		} __packed initfq;
+		struct qm_mcr_queryfq {
+			u8 __reserved1[8];
+			struct qm_fqd fqd;	/* the FQD fields are here */
+			u8 __reserved2[32];
+		} __packed queryfq;
+		struct qm_mcr_queryfq_np {
+			u8 __reserved1;
+			u8 state;	/* QM_MCR_NP_STATE_*** */
+			u8 __reserved2;
+			u32 fqd_link:24;
+			u16 odp_seq;
+			u16 orp_nesn;
+			u16 orp_ea_hseq;
+			u16 orp_ea_tseq;
+			u8 __reserved3;
+			u32 orp_ea_hptr:24;
+			u8 __reserved4;
+			u32 orp_ea_tptr:24;
+			u8 __reserved5;
+			u32 pfdr_hptr:24;
+			u8 __reserved6;
+			u32 pfdr_tptr:24;
+			u8 __reserved7[5];
+			u8 __reserved8:7;
+			u8 is:1;
+			u16 ics_surp;
+			u32 byte_cnt;
+			u8 __reserved9;
+			u32 frm_cnt:24;
+			u32 __reserved10;
+			u16 ra1_sfdr;	/* QM_MCR_NP_RA1_*** */
+			u16 ra2_sfdr;	/* QM_MCR_NP_RA2_*** */
+			u16 __reserved11;
+			u16 od1_sfdr;	/* QM_MCR_NP_OD1_*** */
+			u16 od2_sfdr;	/* QM_MCR_NP_OD2_*** */
+			u16 od3_sfdr;	/* QM_MCR_NP_OD3_*** */
+		} __packed queryfq_np;
+		struct qm_mcr_alterfq {
+			u8 fqs;		/* Frame Queue Status */
+			u8 __reserved1[61];
+		} __packed alterfq;
+		struct qm_mcr_initcgr {
+			u8 __reserved1[62];
+		} __packed initcgr;
+		struct qm_mcr_querycgr {
+			u16 __reserved1;
+			struct __qm_mc_cgr cgr; /* CGR fields */
+			u32 __reserved2;
+			u32 __reserved3:24;
+			u32 i_bcnt_hi:8;/* high 8-bits of 40-bit "Instant" */
+			u32 i_bcnt_lo;	/* low 32-bits of 40-bit */
+			u32 __reserved4:24;
+			u32 a_bcnt_hi:8;/* high 8-bits of 40-bit "Average" */
+			u32 a_bcnt_lo;	/* low 32-bits of 40-bit */
+			u32 lgt;	/* Last Group Tick */
+			u8 __reserved5[12];
+		} __packed querycgr;
+		struct qm_mcr_querycongestion {
+			u8 __reserved[30];
+			/* Access this struct using QM_MCR_QUERYCONGESTION() */
+			struct __qm_mcr_querycongestion {
+				u32 __state[8];
+			} state;
+		} __packed querycongestion;
+		struct qm_mcr_querywq {
+			union {
+				u16 channel_wq; /* ignores wq (3 lsbits) */
+				struct {
+					u16 id:13; /* enum qm_channel */
+					u16 __reserved:3;
+				} __packed channel;
+			};
+			u8 __reserved[28];
+			u32 wq_len[8];
+		} __packed querywq;
+	};
+} __packed;
+#define QM_MCR_VERB_RRID		0x80
+#define QM_MCR_VERB_MASK		QM_MCC_VERB_MASK
+#define QM_MCR_VERB_INITFQ_PARKED	QM_MCC_VERB_INITFQ_PARKED
+#define QM_MCR_VERB_INITFQ_SCHED	QM_MCC_VERB_INITFQ_SCHED
+#define QM_MCR_VERB_QUERYFQ		QM_MCC_VERB_QUERYFQ
+#define QM_MCR_VERB_QUERYFQ_NP		QM_MCC_VERB_QUERYFQ_NP
+#define QM_MCR_VERB_QUERYWQ		QM_MCC_VERB_QUERYWQ
+#define QM_MCR_VERB_QUERYWQ_DEDICATED	QM_MCC_VERB_QUERYWQ_DEDICATED
+#define QM_MCR_VERB_ALTER_SCHED		QM_MCC_VERB_ALTER_SCHED
+#define QM_MCR_VERB_ALTER_FE		QM_MCC_VERB_ALTER_FE
+#define QM_MCR_VERB_ALTER_RETIRE	QM_MCC_VERB_ALTER_RETIRE
+#define QM_MCR_VERB_ALTER_OOS		QM_MCC_VERB_ALTER_OOS
+#define QM_MCR_RESULT_NULL		0x00
+#define QM_MCR_RESULT_OK		0xf0
+#define QM_MCR_RESULT_ERR_FQID		0xf1
+#define QM_MCR_RESULT_ERR_FQSTATE	0xf2
+#define QM_MCR_RESULT_ERR_NOTEMPTY	0xf3	/* OOS fails if FQ is !empty */
+#define QM_MCR_RESULT_ERR_BADCHANNEL	0xf4
+#define QM_MCR_RESULT_PENDING		0xf8
+#define QM_MCR_RESULT_ERR_BADCOMMAND	0xff
+#define QM_MCR_NP_STATE_FE		0x10
+#define QM_MCR_NP_STATE_R		0x08
+#define QM_MCR_NP_STATE_MASK		0x07	/* Reads FQD::STATE; */
+#define QM_MCR_NP_STATE_OOS		0x00
+#define QM_MCR_NP_STATE_RETIRED		0x01
+#define QM_MCR_NP_STATE_TEN_SCHED	0x02
+#define QM_MCR_NP_STATE_TRU_SCHED	0x03
+#define QM_MCR_NP_STATE_PARKED		0x04
+#define QM_MCR_NP_STATE_ACTIVE		0x05
+#define QM_MCR_NP_PTR_MASK		0x07ff	/* for RA[12] & OD[123] */
+#define QM_MCR_NP_RA1_NRA(v)		(((v) >> 14) & 0x3)	/* FQD::NRA */
+#define QM_MCR_NP_RA2_IT(v)		(((v) >> 14) & 0x1)	/* FQD::IT */
+#define QM_MCR_NP_OD1_NOD(v)		(((v) >> 14) & 0x3)	/* FQD::NOD */
+#define QM_MCR_NP_OD3_NPC(v)		(((v) >> 14) & 0x3)	/* FQD::NPC */
+#define QM_MCR_FQS_ORLPRESENT		0x02	/* ORL fragments to come */
+#define QM_MCR_FQS_NOTEMPTY		0x01	/* FQ has enqueued frames */
+/* This extracts the state for congestion group 'n' from a query response.
+ * Eg.
+ *   u8 cgr = [...];
+ *   struct qm_mc_result *res = [...];
+ *   printf("congestion group %d congestion state: %d\n", cgr,
+ *       QM_MCR_QUERYCONGESTION(&res->querycongestion.state, cgr));
+ */
+#define __CGR_WORD(num)		(num >> 5)
+#define __CGR_SHIFT(num)	(num & 0x1f)
+static inline int QM_MCR_QUERYCONGESTION(struct __qm_mcr_querycongestion *p,
+					u8 cgr)
+{
+	return p->__state[__CGR_WORD(cgr)] & (0x80000000 >> __CGR_SHIFT(cgr));
+}
+
+
+/*********************/
+/* Utility interface */
+/*********************/
+
+/* Represents an allocator over a range of FQIDs. NB, accesses are not locked,
+ * spinlock them yourself if needed. */
+struct qman_fqid_pool;
+
+/* Create/destroy a FQID pool, num must be a multiple of 32. NB, _destroy()
+ * always succeeds, but returns non-zero if there were "leaked" FQID
+ * allocations. */
+struct qman_fqid_pool *qman_fqid_pool_create(u32 fqid_start, u32 num);
+int qman_fqid_pool_destroy(struct qman_fqid_pool *pool);
+/* Alloc/free a FQID from the range. _alloc() returns zero for success. */
+int qman_fqid_pool_alloc(struct qman_fqid_pool *pool, u32 *fqid);
+void qman_fqid_pool_free(struct qman_fqid_pool *pool, u32 fqid);
+u32 qman_fqid_pool_used(struct qman_fqid_pool *pool);
+
+/*******************************************************************/
+/* Managed (aka "shared" or "mux/demux") portal, high-level i/face */
+/*******************************************************************/
+
+	/* Congestion Groups */
+	/* ----------------- */
+/* This wrapper represents a bit-array for the state of the 256 Qman congestion
+ * groups. Is also used as a *mask* for congestion groups, eg. so we ignore
+ * those that don't concern us. We harness the structure and accessor details
+ * already used in the management command to query congestion groups. */
+struct qman_cgrs {
+	struct __qm_mcr_querycongestion q;
+};
+static inline void qman_cgrs_init(struct qman_cgrs *c)
+{
+	memset(c, 0, sizeof(*c));
+}
+static inline int qman_cgrs_get(struct qman_cgrs *c, int num)
+{
+	return QM_MCR_QUERYCONGESTION(&c->q, num);
+}
+static inline void qman_cgrs_set(struct qman_cgrs *c, int num)
+{
+	c->q.__state[__CGR_WORD(num)] |= (0x80000000 >> __CGR_SHIFT(num));
+}
+static inline void qman_cgrs_unset(struct qman_cgrs *c, int num)
+{
+	c->q.__state[__CGR_WORD(num)] &= ~(0x80000000 >> __CGR_SHIFT(num));
+}
+
+	/* Portal and Frame Queues */
+	/* ----------------------- */
+/* Represents a managed portal */
+struct qman_portal;
+
+/* This object type represents Qman frame queue descriptors (FQD), it is
+ * cacheline-aligned, and initialised by qman_create_fq(). The structure is
+ * defined further down. */
+struct qman_fq;
+
+/* This enum, and the callback type that returns it, are used when handling
+ * dequeued frames via DQRR. Note that for "null" callbacks registered with the
+ * portal object (for handling dequeues that do not demux because contextB is
+ * NULL), the return value *MUST* be qman_cb_dqrr_consume. */
+enum qman_cb_dqrr_result {
+	/* DQRR entry can be consumed */
+	qman_cb_dqrr_consume,
+	/* Like _consume, but requests parking - FQ must be held-active */
+	qman_cb_dqrr_park,
+	/* Does not consume, for DCA mode only. This allows out-of-order
+	 * consumes by explicit calls to qman_dca() and/or the use of implicit
+	 * DCA via EQCR entries. */
+	qman_cb_dqrr_defer
+};
+typedef enum qman_cb_dqrr_result (*qman_cb_dqrr)(struct qman_portal *qm,
+					struct qman_fq *fq,
+					const struct qm_dqrr_entry *dqrr);
+
+/* This callback type is used when handling ERNs, FQRNs and FQRLs via MR. They
+ * are always consumed after the callback returns. */
+typedef void (*qman_cb_mr)(struct qman_portal *qm, struct qman_fq *fq,
+				const struct qm_mr_entry *msg);
+
+/* s/w-visible states. Ie. tentatively scheduled + truly scheduled + active +
+ * held-active + held-suspended are just "sched". Things like "retired" will not
+ * be assumed until it is complete (ie. QMAN_FQ_STATE_CHANGING is set until
+ * then, to indicate it's completing and to gate attempts to retry the retire
+ * command). Note, park commands do not set QMAN_FQ_STATE_CHANGING because it's
+ * technically impossible in the case of enqueue DCAs (which refer to DQRR ring
+ * index rather than the FQ that ring entry corresponds to), so repeated park
+ * commands are allowed (if you're silly enough to try) but won't change FQ
+ * state, and the resulting park notifications move FQs from "sched" to
+ * "parked". */
+enum qman_fq_state {
+	qman_fq_state_oos,
+	qman_fq_state_parked,
+	qman_fq_state_sched,
+	qman_fq_state_retired
+};
+
+/* Frame queue objects (struct qman_fq) are stored within memory passed to
+ * qman_create_fq(), as this allows stashing of caller-provided demux callback
+ * pointers at no extra cost to stashing of (driver-internal) FQ state. If the
+ * caller wishes to add per-FQ state and have it benefit from dequeue-stashing,
+ * they should;
+ *
+ * (a) extend the qman_fq structure with their state; eg.
+ *
+ *     // myfq is allocated and driver_fq callbacks filled in;
+ *     struct my_fq {
+ *         struct qman_fq base;
+ *         int an_extra_field;
+ *         [ ... add other fields to be associated with each FQ ...]
+ *     } *myfq = some_my_fq_allocator();
+ *     struct qman_fq *fq = qman_create_fq(fqid, flags, &myfq->base);
+ *
+ *     // in a dequeue callback, access extra fields from 'fq' via a cast;
+ *     struct my_fq *myfq = (struct my_fq *)fq;
+ *     do_something_with(myfq->an_extra_field);
+ *     [...]
+ *
+ * (b) when and if configuring the FQ for context stashing, specify how ever
+ *     many cachelines are required to stash 'struct my_fq', to accelerate not
+ *     only the Qman driver but the callback as well.
+ */
+struct qman_fq {
+	/* Caller of qman_create_fq() provides these demux callbacks */
+	struct qman_fq_cb {
+		qman_cb_dqrr dqrr;	/* for dequeued frames */
+		qman_cb_mr ern;		/* for s/w ERNs */
+		qman_cb_mr dc_ern;	/* for diverted h/w ERNs */
+		qman_cb_mr fqs;		/* frame-queue state changes*/
+	} cb;
+	/* These are internal to the driver, don't touch. In particular, they
+	 * may change, be removed, or extended (so you shouldn't rely on
+	 * sizeof(qman_fq) being a constant). */
+	spinlock_t fqlock;
+	u32 fqid;
+	volatile unsigned long flags;
+	enum qman_fq_state state;
+	int cgr_groupid;
+	struct rb_node node;
+} ____cacheline_aligned;
+
+/* Flags to qman_create_fq() */
+#define QMAN_FQ_FLAG_NO_ENQUEUE      0x00000001 /* can't enqueue */
+#define QMAN_FQ_FLAG_NO_MODIFY       0x00000002 /* can only enqueue */
+#define QMAN_FQ_FLAG_TO_DCPORTAL     0x00000004 /* consumed by CAAM/PME/Fman */
+#define QMAN_FQ_FLAG_LOCKED          0x00000008 /* multi-core locking */
+#define QMAN_FQ_FLAG_RECOVER         0x00000010 /* recovery mode */
+#define QMAN_FQ_FLAG_DYNAMIC_FQID    0x00000020 /* (de)allocate fqid */
+
+/* Flags to qman_destroy_fq() */
+#define QMAN_FQ_DESTROY_PARKED       0x00000001 /* FQ can be parked or OOS */
+
+/* Flags from qman_fq_state() */
+#define QMAN_FQ_STATE_CHANGING       0x80000000 /* 'state' is changing */
+#define QMAN_FQ_STATE_NE             0x40000000 /* retired FQ isn't empty */
+#define QMAN_FQ_STATE_ORL            0x20000000 /* retired FQ has ORL */
+#define QMAN_FQ_STATE_BLOCKOOS       0xe0000000 /* if any are set, no OOS */
+#define QMAN_FQ_STATE_CGR_EN         0x10000000 /* CGR enabled */
+
+/* Flags to qman_init_fq() */
+#define QMAN_INITFQ_FLAG_SCHED       0x00000001 /* schedule rather than park */
+#define QMAN_INITFQ_FLAG_NULL        0x00000002 /* zero 'contextB', no demux */
+#define QMAN_INITFQ_FLAG_LOCAL       0x00000004 /* set dest portal */
+
+/* Flags to qman_volatile_dequeue() */
+#define QMAN_VOLATILE_FLAG_WAIT      0x00000001 /* wait if VDQCR is in use */
+#define QMAN_VOLATILE_FLAG_WAIT_INT  0x00000002 /* if wait, interruptible? */
+#define QMAN_VOLATILE_FLAG_FINISH    0x00000004 /* wait till VDQCR completes */
+
+/* Flags to qman_enqueue(). NB, the strange numbering is to align with hardware,
+ * bit-wise. (NB: the PME API is sensitive to these precise numberings too, so
+ * any change here should be audited in PME.) */
+#define QMAN_ENQUEUE_FLAG_WAIT       0x00010000 /* wait if EQCR is full */
+#define QMAN_ENQUEUE_FLAG_WAIT_INT   0x00020000 /* if wait, interruptible? */
+#define QMAN_ENQUEUE_FLAG_WAIT_SYNC  0x00040000 /* if wait, until consumed? */
+#define QMAN_ENQUEUE_FLAG_WATCH_CGR  0x00080000 /* watch congestion state */
+#define QMAN_ENQUEUE_FLAG_INTERRUPT  0x00000004 /* on command consumption */
+#define QMAN_ENQUEUE_FLAG_DCA        0x00008000 /* perform enqueue-DCA */
+#define QMAN_ENQUEUE_FLAG_DCA_PARK   0x00004000 /* If DCA, requests park */
+#define QMAN_ENQUEUE_FLAG_DCA_PTR(p)		/* If DCA, p is DQRR entry */ \
+		(((u32)(p) << 2) & 0x00000f00)
+#define QMAN_ENQUEUE_FLAG_C_GREEN    0x00000000 /* choose one C_*** flag */
+#define QMAN_ENQUEUE_FLAG_C_YELLOW   0x00000008
+#define QMAN_ENQUEUE_FLAG_C_RED      0x00000010
+#define QMAN_ENQUEUE_FLAG_C_OVERRIDE 0x00000018
+/* For the ORP-specific qman_enqueue_orp() variant;
+ * - this flag indicates "Not Last In Sequence", ie. all but the final fragment
+ *   of a frame. */
+#define QMAN_ENQUEUE_FLAG_NLIS       0x01000000
+/* - this flag performs no enqueue but fills in an ORP sequence number that
+ *   would otherwise block it (eg. if a frame has been dropped). */
+#define QMAN_ENQUEUE_FLAG_HOLE       0x02000000
+/* - this flag performs no enqueue but advances NESN to the given sequence
+ *   number. */
+#define QMAN_ENQUEUE_FLAG_NESN       0x04000000
+
+	/* Portal Management */
+	/* ----------------- */
+/**
+ * qman_set_null_cb - set callbacks to use for "null" frame queues
+ *
+ * Sets the callbacks to use for the affine portal of the current cpu, whenever
+ * a DQRR or MR entry refers to a "null" FQ object. (Eg. zero-conf messaging.)
+ */
+void qman_set_null_cb(const struct qman_fq_cb *null_cb);
+
+/**
+ * qman_poll - Runs portal updates not triggered by interrupts
+ *
+ * Dispatcher logic on a cpu can use this to trigger any maintenance of the
+ * affine portal. There are two classes of portal processing in question;
+ * fast-path (which involves demuxing dequeue ring (DQRR) entries and tracking
+ * enqueue ring (EQCR) consumption), and slow-path (which involves EQCR
+ * thresholds, congestion state changes, etc). The driver is configured to use
+ * interrupts for either (a) all processing, (b) only slow-path processing, or
+ * (c) no processing. This function does whatever processing is not triggered by
+ * interrupts.
+ */
+void qman_poll(void);
+
+/**
+ * qman_disable_portal - Cease processing DQRR and MR for a s/w portal
+ *
+ * Disables DQRR and MR processing of the portal. Portal disabling is
+ * reference-counted, so qman_enable_portal() must be called as many times as
+ * qman_disable_portal() to truly re-enable the portal.
+ */
+void qman_disable_portal(void);
+
+/**
+ * qman_enable_portal - Commence processing DQRR and MR for a s/w portal
+ *
+ * Enables DQRR and MR processing of the portal. Portal disabling is
+ * reference-counted, so qman_enable_portal() must be called as many times as
+ * qman_disable_portal() to truly re-enable the portal.
+ */
+void qman_enable_portal(void);
+
+/**
+ * qman_static_dequeue_add - Add pool channels to the portal SDQCR
+ * @pools: bit-mask of pool channels, using QM_SDQCR_CHANNELS_POOL(n)
+ *
+ * Adds a set of pool channels to the portal's static dequeue command register
+ * (SDQCR). The requested pools are limited to those the portal has dequeue
+ * access to.
+ */
+void qman_static_dequeue_add(u32 pools);
+
+/**
+ * qman_static_dequeue_del - Remove pool channels from the portal SDQCR
+ * @pools: bit-mask of pool channels, using QM_SDQCR_CHANNELS_POOL(n)
+ *
+ * Removes a set of pool channels from the portal's static dequeue command
+ * register (SDQCR). The requested pools are limited to those the portal has
+ * dequeue access to.
+ */
+void qman_static_dequeue_del(u32 pools);
+
+/**
+ * qman_static_dequeue_get - return the portal's current SDQCR
+ *
+ * Returns the portal's current static dequeue command register (SDQCR). The
+ * entire register is returned, so if only the currently-enabled pool channels
+ * are desired, mask the return value with QM_SDQCR_CHANNELS_POOL_MASK.
+ */
+u32 qman_static_dequeue_get(void);
+
+/**
+ * qman_dca - Perform a Discrete Consumption Acknowledgement
+ * @p: the managed portal whose DQRR is targeted (and is in DCA mode)
+ * @dq: the DQRR entry to be consumed
+ * @park_request: indicates whether the held-active @fq should be parked
+ *
+ * Only allowed in DCA-mode portals, for DQRR entries whose handler callback had
+ * previously returned 'qman_cb_dqrr_defer'. NB, as with the other APIs, this
+ * does not take a 'portal' argument but implies the core affine portal from the
+ * cpu that is currently executing the function. For reasons of locking, this
+ * function must be called from the same CPU as that which processed the DQRR
+ * entry in the first place.
+ */
+void qman_dca(struct qm_dqrr_entry *dq, int park_request);
+
+
+	/* FQ management */
+	/* ------------- */
+/**
+ * qman_create_fq - Allocates a FQ
+ * @fqid: the index of the FQD to encapsulate, must be "Out of Service"
+ * @flags: bit-mask of QMAN_FQ_FLAG_*** options
+ * @fq: memory for storing the 'fq', with callbacks filled in
+ *
+ * Creates a frame queue object for the given @fqid, unless the
+ * QMAN_FQ_FLAG_DYNAMIC_FQID flag is set in @flags, in which case a FQID is
+ * dynamically allocated (or the function fails if none are available). Once
+ * created, the caller should not touch the memory at 'fq' except as extended to
+ * adjacent memory for user-defined fields (see the definition of "struct
+ * qman_fq" for more info). NO_MODIFY is only intended for enqueuing to
+ * pre-existing frame-queues that aren't to be otherwise interfered with, it
+ * prevents all other modifications to the frame queue. The TO_DCPORTAL flag
+ * causes the driver to honour any contextB modifications requested in the
+ * qm_init_fq() API, as this indicates the frame queue will be consumed by a
+ * direct-connect portal (PME, CAAM, or Fman). When frame queues are consumed by
+ * software portals, the contextB field is controlled by the driver and can't be
+ * modified by the caller. If the RECOVERY flag is specified, management
+ * commands will be used on portal @p to query state for frame queue @fqid and
+ * construct a frame queue object based on that, rather than assuming/requiring
+ * that it be Out of Service.
+ */
+int qman_create_fq(u32 fqid, u32 flags, struct qman_fq *fq);
+
+/**
+ * qman_destroy_fq - Deallocates a FQ
+ * @fq: the frame queue object to release
+ * @flags: bit-mask of QMAN_FQ_FREE_*** options
+ *
+ * The memory for this frame queue object ('fq' provided in qman_create_fq()) is
+ * not deallocated but the caller regains ownership, to do with as desired. The
+ * FQ must be in the 'out-of-service' state unless the QMAN_FQ_FREE_PARKED flag
+ * is specified, in which case it may also be in the 'parked' state.
+ */
+void qman_destroy_fq(struct qman_fq *fq, u32 flags);
+
+/**
+ * qman_fq_fqid - Queries the frame queue ID of a FQ object
+ * @fq: the frame queue object to query
+ */
+u32 qman_fq_fqid(struct qman_fq *fq);
+
+/**
+ * qman_fq_state - Queries the state of a FQ object
+ * @fq: the frame queue object to query
+ * @state: pointer to state enum to return the FQ scheduling state
+ * @flags: pointer to state flags to receive QMAN_FQ_STATE_*** bitmask
+ *
+ * Queries the state of the FQ object, without performing any h/w commands.
+ * This captures the state, as seen by the driver, at the time the function
+ * executes.
+ */
+void qman_fq_state(struct qman_fq *fq, enum qman_fq_state *state, u32 *flags);
+
+/**
+ * qman_init_fq - Initialises FQ fields, leaves the FQ "parked" or "scheduled"
+ * @fq: the frame queue object to modify, must be 'parked' or new.
+ * @flags: bit-mask of QMAN_INITFQ_FLAG_*** options
+ * @opts: the FQ-modification settings, as defined in the low-level API
+ *
+ * The @opts parameter comes from the low-level portal API. Select
+ * QMAN_INITFQ_FLAG_SCHED in @flags to cause the frame queue to be scheduled
+ * rather than parked. Select QMAN_INITFQ_FLAG_NULL in @flags to configure a
+ * frame queue that will not demux to a 'struct qman_fq' object when dequeued
+ * frames or messages arrive at a software portal, but which will instead
+ * trigger the portal's 'null_cb' callbacks (see qman_create_portal()). NB,
+ * @opts can be NULL.
+ *
+ * Note that some fields and options within @opts may be ignored or overwritten
+ * by the driver;
+ * 1. the 'count' and 'fqid' fields are always ignored (this operation only
+ * affects one frame queue: @fq).
+ * 2. the QM_INITFQ_WE_CONTEXTB option of the 'we_mask' field and the associated
+ * 'fqd' structure's 'context_b' field are sometimes overwritten;
+ *   - if @flags contains QMAN_INITFQ_FLAG_NULL, then context_b is initialised
+ *     to zero by the driver,
+ *   - if @fq was not created with QMAN_FQ_FLAG_TO_DCPORTAL, then context_b is
+ *     initialised to a value used by the driver for demux.
+ *   - if context_b is initialised for demux, so is context_a in case stashing
+ *     is requested (see item 4).
+ * (So caller control of context_b is only possible for TO_DCPORTAL frame queue
+ * objects.)
+ * 3. if @flags contains QMAN_INITFQ_FLAG_LOCAL, the 'fqd' structure's
+ * 'dest::channel' field will be overwritten to match the portal used to issue
+ * the command. If the WE_DESTWQ write-enable bit had already been set by the
+ * caller, the channel workqueue will be left as-is, otherwise the write-enable
+ * bit is set and the workqueue is set to a default of 4. If the "LOCAL" flag
+ * isn't set, the destination channel/workqueue fields and the write-enable bit
+ * are left as-is.
+ * 4. if the driver overwrites context_a/b for demux, then if
+ * QM_INITFQ_WE_CONTEXTA is set, the driver will only overwrite
+ * context_a.address fields and will leave the stashing fields provided by the
+ * user alone, otherwise it will zero out the context_a.stashing fields.
+ */
+int qman_init_fq(struct qman_fq *fq, u32 flags, struct qm_mcc_initfq *opts);
+
+/**
+ * qman_schedule_fq - Schedules a FQ
+ * @fq: the frame queue object to schedule, must be 'parked'
+ *
+ * Schedules the frame queue, which must be Parked, which takes it to
+ * Tentatively-Scheduled or Truly-Scheduled depending on its fill-level.
+ */
+int qman_schedule_fq(struct qman_fq *fq);
+
+/**
+ * qman_retire_fq - Retires a FQ
+ * @fq: the frame queue object to retire
+ * @flags: FQ flags (as per qman_fq_state) if retirement completes immediately
+ *
+ * Retires the frame queue. This returns zero if it succeeds immediately, +1 if
+ * the retirement was started asynchronously, otherwise it returns negative for
+ * failure. When this function returns zero, @flags is set to indicate whether
+ * the retired FQ is empty and/or whether it has any ORL fragments (to show up
+ * as ERNs). Otherwise the corresponding flags will be known when a subsequent
+ * FQRN message shows up on the portal's message ring.
+ *
+ * NB, if the retirement is asynchronous (the FQ was in the Truly Scheduled or
+ * Active state), the completion will be via the message ring as a FQRN - but
+ * the corresponding callback may occur before this function returns!! Ie. the
+ * caller should be prepared to accept the callback as the function is called,
+ * not only once it has returned.
+ */
+int qman_retire_fq(struct qman_fq *fq, u32 *flags);
+
+/**
+ * qman_oos_fq - Puts a FQ "out of service"
+ * @fq: the frame queue object to be put out-of-service, must be 'retired'
+ *
+ * The frame queue must be retired and empty, and if any order restoration list
+ * was released as ERNs at the time of retirement, they must all be consumed.
+ */
+int qman_oos_fq(struct qman_fq *fq);
+
+/**
+ * qman_query_fq - Queries FQD fields (via h/w query command)
+ * @fq: the frame queue object to be queried
+ */
+int qman_query_fq(struct qman_fq *fq, struct qm_fqd *fqd);
+
+/**
+ * qman_volatile_dequeue - Issue a volatile dequeue command
+ * @fq: the frame queue object to dequeue from (or NULL)
+ * @flags: a bit-mask of QMAN_VOLATILE_FLAG_*** options
+ * @vdqcr: bit mask of QM_VDQCR_*** options, as per qm_dqrr_vdqcr_set()
+ *
+ * Attempts to lock access to the portal's VDQCR volatile dequeue functionality.
+ * The function will block and sleep if QMAN_VOLATILE_FLAG_WAIT is specified and
+ * the VDQCR is already in use, otherwise returns non-zero for failure. If
+ * QMAN_VOLATILE_FLAG_FINISH is specified, the function will only return once
+ * the VDQCR command has finished executing (ie. once the callback for the last
+ * DQRR entry resulting from the VDQCR command has been called). If @fq is
+ * non-NULL, the corresponding FQID will be substituted in to the VDQCR command,
+ * otherwise it is assumed that @vdqcr already contains the FQID to dequeue
+ * from.
+ */
+int qman_volatile_dequeue(struct qman_fq *fq, u32 flags, u32 vdqcr);
+
+/**
+ * qman_enqueue - Enqueue a frame to a frame queue
+ * @fq: the frame queue object to enqueue to
+ * @fd: a descriptor of the frame to be enqueued
+ * @flags: bit-mask of QMAN_ENQUEUE_FLAG_*** options
+ *
+ * Fills an entry in the EQCR of portal @qm to enqueue the frame described by
+ * @fd. The descriptor details are copied from @fd to the EQCR entry, the 'pid'
+ * field is ignored. The return value is non-zero on error, such as ring full
+ * (and FLAG_WAIT not specified), congestion avoidance (FLAG_WATCH_CGR
+ * specified), etc. If the ring is full and FLAG_WAIT is specified, this
+ * function will block. If FLAG_INTERRUPT is set, the EQCI bit of the portal
+ * interrupt will assert when Qman consumes the EQCR entry (subject to "status
+ * disable", "enable", and "inhibit" registers). If FLAG_DCA is set, Qman will
+ * perform an implied "discrete consumption acknowledgement" on the dequeue
+ * ring's (DQRR) entry, at the ring index specified by the FLAG_DCA_IDX(x)
+ * macro. (As an alternative to issuing explicit DCA actions on DQRR entries,
+ * this implicit DCA can delay the release of a "held active" frame queue
+ * corresponding to a DQRR entry until Qman consumes the EQCR entry - providing
+ * order-preservation semantics in packet-forwarding scenarios.) If FLAG_DCA is
+ * set, then FLAG_DCA_PARK can also be set to imply that the DQRR consumption
+ * acknowledgement should "park request" the "held active" frame queue. Ie.
+ * when the portal eventually releases that frame queue, it will be left in the
+ * Parked state rather than Tentatively Scheduled or Truly Scheduled. If the
+ * portal is watching congestion groups, the QMAN_ENQUEUE_FLAG_WATCH_CGR flag
+ * is requested, and the FQ is a member of a congestion group, then this
+ * function returns -EAGAIN if the congestion group is currently congested.
+ * Note, this does not eliminate ERNs, as the async interface means we can be
+ * sending enqueue commands to an un-congested FQ that becomes congested before
+ * the enqueue commands are processed, but it does minimise needless thrashing
+ * of an already busy hardware resource by throttling many of the to-be-dropped
+ * enqueues "at the source".
+ */
+int qman_enqueue(struct qman_fq *fq, const struct qm_fd *fd, u32 flags);
+
+/**
+ * qman_enqueue_orp - Enqueue a frame to a frame queue using an ORP
+ * @fq: the frame queue object to enqueue to
+ * @fd: a descriptor of the frame to be enqueued
+ * @flags: bit-mask of QMAN_ENQUEUE_FLAG_*** options
+ * @orp: the frame queue object used as an order restoration point.
+ * @orp_seqnum: the sequence number of this frame in the order restoration path
+ *
+ * Similar to qman_enqueue(), but with the addition of an Order Restoration
+ * Point (@orp) and corresponding sequence number (@orp_seqnum) for this
+ * enqueue operation to employ order restoration. Each frame queue object acts
+ * as an Order Definition Point (ODP) by providing each frame dequeued from it
+ * with an incrementing sequence number, this value is generally ignored unless
+ * that sequence of dequeued frames will need order restoration later. Each
+ * frame queue object also encapsulates an Order Restoration Point (ORP), which
+ * is a re-assembly context for re-ordering frames relative to their sequence
+ * numbers as they are enqueued. The ORP does not have to be within the frame
+ * queue that receives the enqueued frame, in fact it is usually the frame
+ * queue from which the frames were originally dequeued. For the purposes of
+ * order restoration, multiple frames (or "fragments") can be enqueued for a
+ * single sequence number by setting the QMAN_ENQUEUE_FLAG_NLIS flag for all
+ * enqueues except the final fragment of a given sequence number. Ordering
+ * between sequence numbers is guaranteed, even if fragments of different
+ * sequence numbers are interlaced with one another. Fragments of the same
+ * sequence number will retain the order in which they are enqueued. If no
+ * enqueue is to performed, QMAN_ENQUEUE_FLAG_HOLE indicates that the given
+ * sequence number is to be "skipped" by the ORP logic (eg. if a frame has been
+ * dropped from a sequence), or QMAN_ENQUEUE_FLAG_NESN indicates that the given
+ * sequence number should become the ORP's "Next Expected Sequence Number".
+ *
+ * Side note: a frame queue object can be used purely as an ORP, without
+ * carrying any frames at all. Care should be taken not to deallocate a frame
+ * queue object that is being actively used as an ORP, as a future allocation
+ * of the frame queue object may start using the internal ORP before the
+ * previous use has finished.
+ */
+int qman_enqueue_orp(struct qman_fq *fq, const struct qm_fd *fd, u32 flags,
+			struct qman_fq *orp, u16 orp_seqnum);
+
+/**
+ * qman_alloc_fq_range - Allocate a contiguous range of FQIDs
+ * @result: is set by the API to the base FQID of the allocated range
+ * @count: the number of FQIDs required
+ * @align: required alignment of the allocated range
+ * @partial: non-zero if the API can return fewer than @count FQIDs
+
+ * Returns the number of frame queues allocated, or a negative error code. If
+ * @partial is non zero, the allocation request may return a smaller range of
+ * FQs than requested (though alignment will be as requested). If @partial is
+ * zero, the return value will either be 'count' or negative.
+ */
+int qman_alloc_fq_range(u32 *result, u32 count, u32 align, int partial);
+
+/**
+ * qman_release_fq_range - Release the specified range of frame queue IDs
+ * @fqid: the base FQID of the range to deallocate
+ * @count: the number of FQIDs in the range
+ *
+ * This function can also be used to seed the allocator with ranges of FQIDs
+ * that it can subsequently use.
+ */
+void qman_release_fq_range(u32 fqid, unsigned int count);
+
+#endif /* FSL_QMAN_H */
+
-- 
1.6.5.2

