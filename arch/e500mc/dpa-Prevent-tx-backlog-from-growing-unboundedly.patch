From 5f4cb01a8b583d79be62c459f0eb55722769eb73 Mon Sep 17 00:00:00 2001
From: Andy Fleming <afleming@freescale.com>
Date: Tue, 23 Feb 2010 21:21:20 -0600
Subject: [PATCH 116/252] dpa: Prevent tx backlog from growing unboundedly

When servicing more than one flow of packets, it is possible for one
core to fall behind, which opens up the opportunity for the other core
to overwhelm it with tx confirmation work.  This backlog will grow
unbounded, until we run out of memory.  If you are using pool channels,
this isn't as much of an issue, as one of the other cores will pick up
the slack, but if you make each ethernet device core-affined, then this
happens quickly.

In order to fix this, we use what were previously debug counters to keep
track of how many frames are currently on the tx confirmation software
queue, and if the number grows beyond some arbitrary value (here, we've
chosen 512), then we halt the tx queues.  As this is now a possibilty,
we now also set net_device's trans_start field to "jiffies" whenever we
successfully transmit a packet.

Signed-off-by: Andy Fleming <afleming@freescale.com>
[Cleanly applied the FSL SDK 2.0.3 patch:
"kernel-2.6.30-dpa-Prevent-tx-backlog-from-growing-unbounde.patch"]
Integrated-by: Yuri Nedel <Yuri.Nedel@windriver.com>
---
 drivers/net/dpa/dpa.c |   32 ++++++++++++++++----------------
 drivers/net/dpa/dpa.h |    2 --
 2 files changed, 16 insertions(+), 18 deletions(-)

diff --git a/drivers/net/dpa/dpa.c b/drivers/net/dpa/dpa.c
index 54edd0f..d18eba2 100644
--- a/drivers/net/dpa/dpa.c
+++ b/drivers/net/dpa/dpa.c
@@ -64,6 +64,7 @@
 
 #define DPA_NETIF_FEATURES	0
 #define DEFAULT_COUNT	64
+#define DPA_MAX_TX_BACKLOG	512
 
 #define DPA_DESCRIPTION "FSL DPA Ethernet driver"
 
@@ -711,22 +712,20 @@ ingress_dqrr(struct qman_portal		*portal,
 	     uint8_t			 ed)
 {
 	int				 _errno;
-	const struct net_device		*net_dev;
-	const struct dpa_priv_s		*priv;
+	struct net_device		*net_dev;
+	struct dpa_priv_s		*priv;
 	struct dpa_percpu_priv_s	*percpu_priv;
 	struct dpa_fd			*dpa_fd;
 
 	net_dev = ((struct dpa_fq *)fq)->net_dev;
-	priv = (typeof(priv))netdev_priv(net_dev);
+	priv = netdev_priv(net_dev);
 
 	if (netif_msg_intr(priv))
 		cpu_netdev_dbg(net_dev, "-> %s:%s[%s][%hu]()\n",
 			       __file__, __func__, rtx[_rtx], ed);
 
 	percpu_priv = per_cpu_ptr(priv->percpu_priv, smp_processor_id());
-#ifdef CONFIG_DEBUG_FS
 	percpu_priv->hwi[_rtx][ed]++;
-#endif
 
 	if (list_empty(&percpu_priv->free_list)) {
 		dpa_fd = devm_kzalloc(net_dev->dev.parent,
@@ -747,11 +746,12 @@ ingress_dqrr(struct qman_portal		*portal,
 	dpa_fd->fd = dq->fd;
 
 	list_add_tail(&dpa_fd->list, &percpu_priv->fd_list[_rtx][ed]);
-#ifdef CONFIG_DEBUG_FS
 	percpu_priv->count[_rtx][ed]++;
 	percpu_priv->max[_rtx][ed] = max(percpu_priv->max[_rtx][ed],
 					 percpu_priv->count[_rtx][ed]);
-#endif
+
+	if (_rtx == TX && percpu_priv->count[_rtx][ed] > DPA_MAX_TX_BACKLOG)
+		netif_tx_stop_all_queues(net_dev);
 
 	_errno = schedule_work(&percpu_priv->fd_work);
 	if (unlikely(_errno < 0))
@@ -1622,12 +1622,10 @@ static void __hot dpa_work(struct work_struct *fd_work)
 	BUG_ON(percpu_priv != per_cpu_ptr(priv->percpu_priv,
 					  smp_processor_id()));
 
-#ifdef CONFIG_DEBUG_FS
 	percpu_priv->swi++;
-#endif
 
 	/* RX, TX */
-	for (i = 0; i < ARRAY_SIZE(percpu_priv->fd_list); i++) {
+	for (i = ARRAY_SIZE(percpu_priv->fd_list) - 1; i >= 0; i--) {
 		/* Error, default*/
 		for (j = 0; j < ARRAY_SIZE(percpu_priv->fd_list[i]); j++) {
 			list_for_each_entry_safe(dpa_fd, tmp,
@@ -1637,10 +1635,8 @@ static void __hot dpa_work(struct work_struct *fd_work)
 
 			local_irq_disable();
 			list_del(&dpa_fd->list);
-#ifdef CONFIG_DEBUG_FS
 			percpu_priv->count[i][j]--;
 			percpu_priv->total[i][j]++;
-#endif
 			list_add_tail(&dpa_fd->list, &percpu_priv->free_list);
 			local_irq_enable();
 
@@ -1653,6 +1649,10 @@ static void __hot dpa_work(struct work_struct *fd_work)
 		}
 	}
 
+	if (netif_queue_stopped(net_dev) &&
+			percpu_priv->count[TX][0] < DPA_MAX_TX_BACKLOG)
+		netif_tx_wake_all_queues(net_dev);
+
 	/* Try again later if we're not done */
 	if (retry) {
 		_errno = schedule_work(fd_work);
@@ -1678,7 +1678,7 @@ static int __hot dpa_tx(struct sk_buff *skb, struct net_device *net_dev)
 	struct bm_buffer	*bmb = NULL;
 	unsigned int	headroom;
 
-	priv = (typeof(priv))netdev_priv(net_dev);
+	priv = netdev_priv(net_dev);
 	dev = net_dev->dev.parent;
 
 	if (netif_msg_tx_queued(priv))
@@ -1759,6 +1759,8 @@ static int __hot dpa_tx(struct sk_buff *skb, struct net_device *net_dev)
 		goto _return_buffer;
 	}
 
+	net_dev->trans_start = jiffies;
+
 	net_dev->stats.tx_packets++;
 	net_dev->stats.tx_bytes += dpa_fd_length(&fd);
 
@@ -1883,7 +1885,7 @@ static void __cold dpa_timeout(struct net_device *net_dev)
 {
 	const struct dpa_priv_s	*priv;
 
-	priv = (typeof(priv))netdev_priv(net_dev);
+	priv = netdev_priv(net_dev);
 
 	if (netif_msg_timer(priv))
 		cpu_netdev_dbg(net_dev, "-> %s:%s()\n", __file__, __func__);
@@ -1893,7 +1895,6 @@ static void __cold dpa_timeout(struct net_device *net_dev)
 				(jiffies - net_dev->trans_start) * 1000 / HZ);
 
 	net_dev->stats.tx_errors++;
-	netif_tx_wake_all_queues(net_dev);
 
 	if (netif_msg_timer(priv))
 		cpu_netdev_dbg(net_dev, "%s:%s() ->\n", __file__, __func__);
@@ -2933,7 +2934,6 @@ static int __devexit __cold dpa_remove(struct of_device *of_dev)
 
 #ifdef CONFIG_DEBUG_FS
 	debugfs_remove(priv->debugfs_file);
-	free_percpu(priv->percpu_priv);
 #endif
 
 	free_netdev(net_dev);
diff --git a/drivers/net/dpa/dpa.h b/drivers/net/dpa/dpa.h
index e252009..cb66a43 100644
--- a/drivers/net/dpa/dpa.h
+++ b/drivers/net/dpa/dpa.h
@@ -59,10 +59,8 @@ struct dpa_percpu_priv_s {
 	struct list_head	 fd_list[2][2];
 	struct list_head	 free_list;
 	struct sk_buff_head	 rx_recycle;
-#ifdef CONFIG_DEBUG_FS
 	size_t			 count[2][2], total[2][2], max[2][2], hwi[2][2];
 	size_t			 swi;
-#endif
 };
 
 struct dpa_priv_s {
-- 
1.6.5.2

