From 1b47bad2d3a7332868944416383e581ae4b88c09 Mon Sep 17 00:00:00 2001
From: Andrew Liu <shengping.liu@windriver.com>
Date: Tue, 14 Dec 2010 15:52:08 +0800
Subject: [PATCH 04/28] BMAN: Update Bman subsystem

Patch taken from FSL vendor SDK 2.2.

Update Freescale Buffer Manager (datapath) support.

Add Bman error interrupt handler and its Initializaiotn function.
Add Bman debug filesystem.
Change bman_low.c to bman_low.c and rerrange its content.
Modify code style.
Add the support of deferring Bman interrupt to tasklet.
Defer Bman initiallization to kernel thread:  _init_affine_portal

Integrated-by: Andrew Liu <shengping.liu@windriver.com>
---
 drivers/hwalloc/Kconfig          |   28 +-
 drivers/hwalloc/Makefile         |    4 +-
 drivers/hwalloc/bman_config.c    |  349 ++++++++++++++++++++++++--
 drivers/hwalloc/bman_debugfs.c   |  122 +++++++++
 drivers/hwalloc/bman_driver.c    |  170 ++++++------
 drivers/hwalloc/bman_high.c      |  533 +++++++++++++++++++++++---------------
 drivers/hwalloc/bman_low.c       |  487 ----------------------------------
 drivers/hwalloc/bman_low.h       |  514 ++++++++++++++++++++++++++++++++++++
 drivers/hwalloc/bman_private.h   |  131 ++++++----
 drivers/hwalloc/bman_sys.h       |  104 +++++++-
 drivers/hwalloc/bman_test_high.c |   16 +-
 include/linux/fsl_bman.h         |  315 ++++++++---------------
 12 files changed, 1673 insertions(+), 1100 deletions(-)
 create mode 100644 drivers/hwalloc/bman_debugfs.c
 delete mode 100644 drivers/hwalloc/bman_low.c
 create mode 100644 drivers/hwalloc/bman_low.h

diff --git a/drivers/hwalloc/Kconfig b/drivers/hwalloc/Kconfig
index 94112a4..ae3c74f 100644
--- a/drivers/hwalloc/Kconfig
+++ b/drivers/hwalloc/Kconfig
@@ -11,7 +11,7 @@ if FSL_BMAN
 
 config FSL_BMAN_CHECKING
 	bool "additional driver checking"
-	default y
+	default n
 	---help---
 	  Compiles in additional checks to sanity-check the Bman driver and any
 	  use of it by other code. Not recommended for performance.
@@ -23,21 +23,14 @@ config FSL_BMAN_PORTAL
 	  Compiles support to detect and support Bman software corenet portals
 	  (as provided by the device-tree).
 
-# The current driver is interrupt-driven only (poll-driven isn't yet supported).
-config FSL_BMAN_HAVE_POLL
-	bool
-	default n
-
-config FSL_BMAN_PORTAL_DISABLEAUTO
-	bool "disable auto-initialisation of cpu-affine portals"
+config FSL_BMAN_PORTAL_TASKLET
+	bool "Defer interrupt processing to a tasklet"
 	depends on FSL_BMAN_PORTAL
 	default n
 	---help---
-	  The high-level portal API, in its normal usage, requires that each cpu
-	  have a portal assigned to it that is auto-initialised. If an
-	  application is manually initialising portals in a non-cpu-affine
-	  manner (or you are using the low-level portal API), this may need to
-	  be disabled. If in doubt, say N.
+	  Rather than processing buffer pool depletion state-change
+	  notifications within the interrupt handler, defer this work to a
+	  tasklet.
 
 config FSL_BMAN_CONFIG
 	bool "Bman device management"
@@ -56,13 +49,20 @@ config FSL_BMAN_TEST
 
 config FSL_BMAN_TEST_HIGH
 	bool "Bman high-level self-test"
-	depends on FSL_BMAN_TEST && !FSL_BMAN_PORTAL_DISABLEAUTO
+	depends on FSL_BMAN_TEST
 	default y
 	---help---
 	  This requires the presence of cpu-affine portals, and performs
 	  high-level API testing with them (whichever portal(s) are affine to
 	  the cpu(s) the test executes on).
 
+config FSL_BMAN_DEBUGFS
+	tristate "Bman debugfs interface"
+	depends on FSL_BMAN_PORTAL && DEBUG_FS
+	default y
+	---help---
+	This option compiles bman debugfs code for Bman.
+
 endif # FSL_BMAN
 
 endmenu
diff --git a/drivers/hwalloc/Makefile b/drivers/hwalloc/Makefile
index 0a950d1..a98512a 100644
--- a/drivers/hwalloc/Makefile
+++ b/drivers/hwalloc/Makefile
@@ -1,4 +1,6 @@
 obj-$(CONFIG_FSL_BMAN_CONFIG)	+= bman_config.o
-obj-$(CONFIG_FSL_BMAN_PORTAL)	+= bman_driver.o bman_low.o bman_high.o
+obj-$(CONFIG_FSL_BMAN_PORTAL)	+= bman_driver.o bman_high.o
 obj-$(CONFIG_FSL_BMAN_TEST)	+= bman_tester.o
+obj-$(CONFIG_FSL_BMAN_DEBUGFS)  += bman_debugfs_interface.o
 bman_tester-y			 = bman_test.o bman_test_high.o
+bman_debugfs_interface-y	 = bman_debugfs.o
diff --git a/drivers/hwalloc/bman_config.c b/drivers/hwalloc/bman_config.c
index 73af407..b31f265 100644
--- a/drivers/hwalloc/bman_config.c
+++ b/drivers/hwalloc/bman_config.c
@@ -1,4 +1,4 @@
-/* Copyright (c) 2009 Freescale Semiconductor, Inc.
+/* Copyright (c) 2009-2010 Freescale Semiconductor, Inc.
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -45,6 +45,13 @@ struct bman;
 #define REG_POOL_HWDET(n)	(0x0100 + ((n) * 0x04))
 #define REG_POOL_SWDXT(n)	(0x0200 + ((n) * 0x04))
 #define REG_POOL_HWDXT(n)	(0x0300 + ((n) * 0x04))
+#define REG_POOL_CONTENT(n)	(0x0600 + ((n) * 0x04))
+#define REG_FBPR_FPC		0x0800
+#define REG_ECSR		0x0a00
+#define REG_ECIR		0x0a04
+#define REG_EADR		0x0a08
+#define REG_EDATA(n)		(0x0a10 + ((n) * 0x04))
+#define REG_SBEC(n)		(0x0a80 + ((n) * 0x04))
 #define REG_IP_REV_1		0x0bf8
 #define REG_IP_REV_2		0x0bfc
 #define REG_FBPR_BARE		0x0c00
@@ -61,6 +68,65 @@ struct bman;
 #define BM_EIRQ_SBEI	0x00000002	/* Single-bit ECC Error */
 #define BM_EIRQ_BSCN	0x00000001	/* pool State Change Notification */
 
+/* BMAN_ECIR valid error bit */
+#define PORTAL_ECSR_ERR	(BM_EIRQ_IVCI)
+
+union bman_ecir {
+	u32 ecir_raw;
+	struct {
+		u32 __reserved1:4;
+		u32 portal_num:4;
+		u32 __reserved2:12;
+		u32 numb:4;
+		u32 __reserved3:2;
+		u32 pid:6;
+	} __packed info;
+};
+
+union bman_eadr {
+	u32 eadr_raw;
+	struct {
+		u32 __reserved1:5;
+		u32 memid:3;
+		u32 __reserved2:14;
+		u32 eadr:10;
+	} __packed info;
+};
+
+struct bman_hwerr_txt {
+	u32 mask;
+	const char *txt;
+};
+
+#define BMAN_HWE_TXT(a, b) { .mask = BM_EIRQ_##a, .txt = b }
+
+static const struct bman_hwerr_txt bman_hwerr_txts[] = {
+	BMAN_HWE_TXT(IVCI, "Invalid Command Verb"),
+	BMAN_HWE_TXT(FLWI, "FBPR Low Watermark"),
+	BMAN_HWE_TXT(MBEI, "Multi-bit ECC Error"),
+	BMAN_HWE_TXT(SBEI, "Single-bit ECC Error"),
+	BMAN_HWE_TXT(BSCN, "Pool State Change Notification"),
+};
+#define BMAN_HWE_COUNT (sizeof(bman_hwerr_txts)/sizeof(struct bman_hwerr_txt))
+
+struct bman_error_info_mdata {
+	u16 addr_mask;
+	u16 bits;
+	const char *txt;
+};
+
+#define BMAN_ERR_MDATA(a, b, c) { .addr_mask = a, .bits = b, .txt = c}
+static const struct bman_error_info_mdata error_mdata[] = {
+	BMAN_ERR_MDATA(0x03FF, 192, "Stockpile memory"),
+	BMAN_ERR_MDATA(0x00FF, 256, "SW portal ring memory port 1"),
+	BMAN_ERR_MDATA(0x00FF, 256, "SW portal ring memory port 2"),
+};
+#define BMAN_ERR_MDATA_COUNT \
+	(sizeof(error_mdata)/sizeof(struct bman_error_info_mdata))
+
+/* Add this in Kconfig */
+#define BMAN_ERRS_TO_UNENABLE (BM_EIRQ_FLWI)
+
 /**
  * bm_err_isr_<reg>_<verb> - Manipulate global interrupt registers
  * @v: for accessors that write values, this is the 32-bit value
@@ -82,9 +148,8 @@ struct bman;
 /*
  * TODO: unimplemented registers
  *
- * BMAN_POOLk_SDCNT, BMAN_POOLk_HDCNT, BMAN_POOLk_CONTENT, BMAN_FULT,
- * BMAN_VLDPL, BMAN_ECSR, BMAN_ECIR, BMAN_EADR, BMAN_EECC, BMAN_EDATA<n>,
- * BMAN_SBET, BMAN_EINJ, BMAN_SBEC[0|1]
+ * BMAN_POOLk_SDCNT, BMAN_POOLk_HDCNT, BMAN_FULT,
+ * BMAN_VLDPL, BMAN_EECC, BMAN_SBET, BMAN_EINJ
  */
 
 /* Encapsulate "struct bman *" as a cast of the register space address. */
@@ -105,8 +170,6 @@ static inline void __bm_out(struct bman *bm, u32 offset, u32 val)
 #define bm_in(reg)		__bm_in(bm, REG_##reg)
 #define bm_out(reg, val)	__bm_out(bm, REG_##reg, val)
 
-#if 0
-
 static u32 __bm_err_isr_read(struct bman *bm, enum bm_isr_reg n)
 {
 	return __bm_in(bm, REG_ERR_ISR + (n << 2));
@@ -117,6 +180,7 @@ static void __bm_err_isr_write(struct bman *bm, enum bm_isr_reg n, u32 val)
 	__bm_out(bm, REG_ERR_ISR + (n << 2), val);
 }
 
+#if 0
 static void bm_get_details(struct bman *bm, u8 *int_options, u8 *errata,
 			u8 *conf_options)
 {
@@ -187,7 +251,8 @@ static u32 bm_get_fbpr_bar(struct bman* bm)
 	return bm_in(FBPR_BAR);
 }
 #endif
-static void bm_set_memory(struct bman *bm, u16 eba, u32 ba, int prio, u32 size)
+
+static void bm_set_memory(struct bman *bm, u64 ba, int prio, u32 size)
 {
 	u32 exp = ilog2(size);
 	/* choke if size isn't within range */
@@ -199,9 +264,10 @@ static void bm_set_memory(struct bman *bm, u16 eba, u32 ba, int prio, u32 size)
 	 * if that value is nonzero.
 	 */
 	if (bm_get_fbpr_bar(bm) == 0) {
-		bm_out(FBPR_BARE, eba);
-		bm_out(FBPR_BAR, ba);
+		bm_out(FBPR_BARE, upper_32_bits(ba));
+		bm_out(FBPR_BAR, lower_32_bits(ba));
 		bm_out(FBPR_AR, (prio ? 0x40000000 : 0) | (exp - 1));
+
 	}
 }
 
@@ -233,11 +299,8 @@ static __init int parse_mem_property(struct device_node *node, const char *name,
 		return 0;
 	}
 	pr_info("Using %s property '%s'\n", node->full_name, name);
-	/* Props are 64-bit, but dma_addr_t is (currently) 32-bit */
-	BUG_ON(sizeof(*addr) != 4);
-	BUG_ON(pint[0] || pint[2]);
-	*addr = pint[1];
-	*sz = pint[3];
+	*addr = ((u64)pint[0] << 32) | (u64)pint[1];
+	*sz = ((u64)pint[2] << 32) | (u64)pint[3];
 	/* Keep things simple, it's either all in the DRAM range or it's all
 	 * outside. */
 	if (*addr < lmb_end_of_DRAM()) {
@@ -283,9 +346,7 @@ static int __init fsl_bman_init(struct device_node *node)
 	bm_get_version(bm, &id, &major, &minor);
 	pr_info("Bman ver:%04x,%02x,%02x\n", id, major, minor);
 	/* FBPR memory */
-	bm_set_memory(bm, 0, (u32)fbpr_a, 0, fbpr_sz);
-	/* TODO: add interrupt handling here, so that ISR is cleared *after*
-	 * FBPR initialisation. */
+	bm_set_memory(bm, fbpr_a, 0, fbpr_sz);
 	return 0;
 }
 
@@ -346,3 +407,257 @@ __init void bman_init_early(void)
 #endif
 }
 
+static void log_edata_bits(u32 bit_count)
+{
+	u32 i, j, mask = 0xffffffff;
+
+	pr_warning("Bman ErrInt, EDATA:\n");
+	i = bit_count/32;
+	if (bit_count%32) {
+		i++;
+		mask = ~(mask << bit_count%32);
+	}
+	j = 16-i;
+	pr_warning("  0x%08x\n", bm_in(EDATA(j)) & mask);
+	j++;
+	for (; j < 16; j++)
+		pr_warning("  0x%08x\n", bm_in(EDATA(j)));
+}
+
+static void log_additional_error_info(u32 isr_val, u32 ecsr_val)
+{
+	union bman_ecir ecir_val;
+	union bman_eadr eadr_val;
+
+	ecir_val.ecir_raw = bm_in(ECIR);
+	/* Is portal info valid */
+	if (ecsr_val & PORTAL_ECSR_ERR) {
+		pr_warning("Bman ErrInt: SWP id %d, numb %d, pid %d\n",
+			ecir_val.info.portal_num, ecir_val.info.numb,
+			ecir_val.info.pid);
+	}
+	if (ecsr_val & (BM_EIRQ_SBEI|BM_EIRQ_MBEI)) {
+		eadr_val.eadr_raw = bm_in(EADR);
+		pr_warning("Bman ErrInt: EADR Memory: %s, 0x%x\n",
+			error_mdata[eadr_val.info.memid].txt,
+			error_mdata[eadr_val.info.memid].addr_mask
+				& eadr_val.info.eadr);
+		log_edata_bits(error_mdata[eadr_val.info.memid].bits);
+	}
+}
+
+/* Bman interrupt handler */
+static irqreturn_t bman_isr(int irq, void *ptr)
+{
+	u32 isr_val, ier_val, ecsr_val, isr_mask, i;
+
+	ier_val = bm_err_isr_enable_read(bm);
+	isr_val = bm_err_isr_status_read(bm);
+	ecsr_val = bm_in(ECSR);
+	isr_mask = isr_val & ier_val;
+
+	if (!isr_mask)
+		return IRQ_NONE;
+	for (i = 0; i < BMAN_HWE_COUNT; i++) {
+		if (bman_hwerr_txts[i].mask & isr_mask) {
+			pr_warning("Bman ErrInt: %s\n", bman_hwerr_txts[i].txt);
+			if (bman_hwerr_txts[i].mask & ecsr_val) {
+				log_additional_error_info(isr_mask, ecsr_val);
+				/* Re-arm error capture registers */
+				bm_out(ECSR, ecsr_val);
+			}
+			if (bman_hwerr_txts[i].mask & BMAN_ERRS_TO_UNENABLE) {
+				pr_devel("Bman un-enabling error 0x%x\n",
+					bman_hwerr_txts[i].mask);
+				ier_val &= ~bman_hwerr_txts[i].mask;
+				bm_err_isr_enable_write(bm, ier_val);
+			}
+		}
+	}
+	bm_err_isr_status_clear(bm, isr_val);
+	return IRQ_HANDLED;
+}
+
+/* Initialise Error Interrupt Handler */
+int bman_init_error_int(struct device_node *node)
+{
+	int ret, err_irq;
+	err_irq = of_irq_to_resource(node, 0, NULL);
+
+	if (!bman_have_ccsr())
+		return 0;
+	if (err_irq == NO_IRQ) {
+		pr_info("Can't get %s property '%s'\n", node->full_name,
+			"interrupts");
+		return -ENODEV;
+	}
+	ret = request_irq(err_irq, bman_isr, IRQF_SHARED, "bman-err", node);
+	if (ret)  {
+		pr_err("request_irq() failed %d for '%s'\n", ret,
+			node->full_name);
+		return -ENODEV;
+	}
+	/* Disable Buffer Pool State Change */
+	bm_err_isr_disable_write(bm, BM_EIRQ_BSCN);
+	/* Write-to-clear any stale bits, (eg. starvation being asserted prior
+	 * to resource allocation during driver init). */
+	bm_err_isr_status_clear(bm, 0xffffffff);
+	/* Enable Error Interrupts */
+	bm_err_isr_enable_write(bm, 0xffffffff);
+	return 0;
+}
+
+#ifdef CONFIG_SYSFS
+
+#define DRV_NAME "fsl-bman"
+#define POOL_MAX 64
+
+static ssize_t show_fbpr_fpc(struct device *dev,
+	struct device_attribute *dev_attr, char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%u\n", bm_in(FBPR_FPC));
+};
+
+static ssize_t show_pool_count(struct device *dev,
+	struct device_attribute *dev_attr, char *buf)
+{
+	u32 data;
+	int i;
+
+	if (!sscanf(dev_attr->attr.name, "%d", &i))
+		return -EINVAL;
+	data = bm_in(POOL_CONTENT(i));
+	return snprintf(buf, PAGE_SIZE, "%d\n", data);
+};
+
+static ssize_t show_err_isr(struct device *dev,
+	struct device_attribute *dev_attr, char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "0x%08x\n", bm_in(ERR_ISR));
+};
+
+static ssize_t show_sbec(struct device *dev,
+	struct device_attribute *dev_attr, char *buf)
+{
+	int i;
+
+	if (!sscanf(dev_attr->attr.name, "sbec_%d", &i))
+		return -EINVAL;
+	return snprintf(buf, PAGE_SIZE, "%u\n", bm_in(SBEC(i)));
+};
+
+static DEVICE_ATTR(err_isr, S_IRUSR, show_err_isr, NULL);
+static DEVICE_ATTR(fbpr_fpc, S_IRUSR, show_fbpr_fpc, NULL);
+
+/* Didn't use DEVICE_ATTR as 64 of this would be required.
+ * Initialize them when needed. */
+static char name_attrs_pool_count[POOL_MAX][3]; /* "xx" + null-terminator */
+static struct device_attribute dev_attr_buffer_pool_count[POOL_MAX];
+
+static DEVICE_ATTR(sbec_0, S_IRUSR, show_sbec, NULL);
+static DEVICE_ATTR(sbec_1, S_IRUSR, show_sbec, NULL);
+
+static struct attribute *bman_dev_attributes[] = {
+	&dev_attr_fbpr_fpc.attr,
+	&dev_attr_err_isr.attr,
+	NULL
+};
+
+static struct attribute *bman_dev_ecr_attributes[] = {
+	&dev_attr_sbec_0.attr,
+	&dev_attr_sbec_1.attr,
+	NULL
+};
+
+static struct attribute *bman_dev_pool_count_attributes[POOL_MAX+1];
+
+/* root level */
+static const struct attribute_group bman_dev_attr_grp = {
+	.name = NULL,
+	.attrs = bman_dev_attributes
+};
+static const struct attribute_group bman_dev_ecr_grp = {
+	.name = "error_capture",
+	.attrs = bman_dev_ecr_attributes
+};
+static const struct attribute_group bman_dev_pool_countent_grp = {
+	.name = "pool_count",
+	.attrs = bman_dev_pool_count_attributes
+};
+
+static int of_fsl_bman_remove(struct of_device *ofdev)
+{
+	sysfs_remove_group(&ofdev->dev.kobj, &bman_dev_attr_grp);
+	return 0;
+};
+
+static int __devinit of_fsl_bman_probe(struct of_device *ofdev,
+				const struct of_device_id *qman)
+{
+	int ret, i;
+
+	ret = sysfs_create_group(&ofdev->dev.kobj, &bman_dev_attr_grp);
+	if (ret)
+		goto done;
+	ret = sysfs_create_group(&ofdev->dev.kobj, &bman_dev_ecr_grp);
+	if (ret)
+		goto del_group_0;
+
+	for (i = 0; i < POOL_MAX; i++) {
+		ret = scnprintf(name_attrs_pool_count[i], 3, "%d", i);
+		if (!ret)
+			goto del_group_0;
+		dev_attr_buffer_pool_count[i].attr.name =
+			name_attrs_pool_count[i];
+		dev_attr_buffer_pool_count[i].attr.mode = S_IRUSR;
+		dev_attr_buffer_pool_count[i].show = show_pool_count;
+		bman_dev_pool_count_attributes[i] =
+			&dev_attr_buffer_pool_count[i].attr;
+	}
+	ret = sysfs_create_group(&ofdev->dev.kobj, &bman_dev_pool_countent_grp);
+	if (ret)
+		goto del_group_1;
+
+	goto done;
+
+del_group_1:
+	sysfs_remove_group(&ofdev->dev.kobj, &bman_dev_ecr_grp);
+del_group_0:
+	sysfs_remove_group(&ofdev->dev.kobj, &bman_dev_attr_grp);
+done:
+	if (ret)
+		dev_err(&ofdev->dev,
+			"Cannot create dev attributes ret=%d\n", ret);
+	return ret;
+};
+
+static struct of_device_id of_fsl_bman_ids[] = {
+	{
+		.compatible = "fsl,bman",
+	},
+	{}
+};
+MODULE_DEVICE_TABLE(of, of_fsl_bman_ids);
+
+static struct of_platform_driver of_fsl_bman_driver = {
+	.owner = THIS_MODULE,
+	.name = DRV_NAME,
+	.match_table = of_fsl_bman_ids,
+	.probe = of_fsl_bman_probe,
+	.remove = __devexit_p(of_fsl_bman_remove),
+};
+
+static int bman_ctrl_init(void)
+{
+	return of_register_platform_driver(&of_fsl_bman_driver);
+}
+
+static void bman_ctrl_exit(void)
+{
+	of_unregister_platform_driver(&of_fsl_bman_driver);
+}
+
+module_init(bman_ctrl_init);
+module_exit(bman_ctrl_exit);
+
+#endif /* CONFIG_SYSFS */
diff --git a/drivers/hwalloc/bman_debugfs.c b/drivers/hwalloc/bman_debugfs.c
new file mode 100644
index 0000000..4ac315b
--- /dev/null
+++ b/drivers/hwalloc/bman_debugfs.c
@@ -0,0 +1,122 @@
+/* Copyright (c) 2010 Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+#include <linux/module.h>
+#include <linux/fsl_bman.h>
+#include <linux/debugfs.h>
+#include <linux/seq_file.h>
+#include <linux/uaccess.h>
+
+static struct dentry *dfs_root; /* debugfs root directory */
+
+/*******************************************************************************
+ *  Query Buffer Pool State
+ ******************************************************************************/
+static ssize_t query_bp_state_show(struct seq_file *file, void *offset)
+{
+	int ret;
+	struct bm_pool_state state;
+	int i, j;
+	u32 mask;
+
+	memset(&state, 0, sizeof(struct bm_pool_state));
+	ret = bman_query_pools(&state);
+	if (ret) {
+		seq_printf(file, "Error %d\n", ret);
+		return 0;
+	}
+	seq_printf(file, "bp_id  free_buffers_avail  bp_depleted\n");
+	for (i = 0; i < 2; i++) {
+		mask = 0x80000000;
+		for (j = 0; j < 32; j++) {
+			seq_printf(file,
+			 "  %-2u           %-3s             %-3s\n",
+			 (i*32)+j,
+			 (state.as.state.__state[i] & mask) ? "no" : "yes",
+			 (state.ds.state.__state[i] & mask) ? "yes" : "no");
+			 mask >>= 1;
+		}
+	}
+	return 0;
+}
+
+static int query_bp_state_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, query_bp_state_show, NULL);
+}
+
+static const struct file_operations query_bp_state_fops = {
+	.owner          = THIS_MODULE,
+	.open		= query_bp_state_open,
+	.read           = seq_read,
+	.release	= single_release,
+};
+
+static int __init bman_debugfs_module_init(void)
+{
+	int ret = 0;
+	struct dentry *d;
+
+	dfs_root = debugfs_create_dir("bman", NULL);
+
+	if (dfs_root == NULL) {
+		ret = -ENOMEM;
+		pr_err("Cannot create bman debugfs dir\n");
+		goto _return;
+	}
+	d = debugfs_create_file("query_bp_state",
+		S_IRUGO,
+		dfs_root,
+		NULL,
+		&query_bp_state_fops);
+	if (d == NULL) {
+		ret = -ENOMEM;
+		pr_err("Cannot create query_bp_state\n");
+		goto _return;
+	}
+	return 0;
+
+_return:
+	if (dfs_root)
+		debugfs_remove_recursive(dfs_root);
+	return ret;
+}
+
+static void __exit bman_debugfs_module_exit(void)
+{
+	debugfs_remove_recursive(dfs_root);
+}
+
+
+module_init(bman_debugfs_module_init);
+module_exit(bman_debugfs_module_exit);
+MODULE_LICENSE("Dual BSD/GPL");
+
diff --git a/drivers/hwalloc/bman_driver.c b/drivers/hwalloc/bman_driver.c
index fe2ee4a..f09267b 100644
--- a/drivers/hwalloc/bman_driver.c
+++ b/drivers/hwalloc/bman_driver.c
@@ -39,29 +39,24 @@
 #define PORTAL_MAX	10
 #define POOL_MAX	64
 
-static struct bm_portal portals[PORTAL_MAX];
+static struct bm_portal_config configs[PORTAL_MAX];
 static u8 num_portals;
-#ifndef CONFIG_FSL_BMAN_PORTAL_DISABLEAUTO
-static u8 num_affine_portals;
-#endif
-static DEFINE_SPINLOCK(bind_lock);
-DEFINE_PER_CPU(struct bman_portal *, bman_affine_portal);
 
 /* The bman_depletion type is a bitfield representation of the 64 BPIDs as
  * booleans. We're not using it here to represent depletion state though, it's
  * to represent reservations. */
 static struct bman_depletion pools;
 static u8 num_pools;
+static DEFINE_SPINLOCK(pools_lock);
 
-static struct bm_portal *__bm_portal_add(const struct bm_addr *addr,
+static struct bm_portal_config *__bm_portal_add(
 				const struct bm_portal_config *config)
 {
-	struct bm_portal *ret;
+	struct bm_portal_config *ret;
 	BUG_ON((num_portals + 1) > PORTAL_MAX);
-	ret = &portals[num_portals++];
-	ret->addr = *addr;
-	ret->config = *config;
-	ret->config.bound = 0;
+	ret = &configs[num_portals++];
+	*ret = *config;
+	ret->portal = NULL;
 	return ret;
 }
 
@@ -90,8 +85,7 @@ static int __bm_pool_add(u32 bpid, u32 *cfg, int triplets)
 			do {
 				BUG_ON(b > 0xffffffffffffull);
 				bufs[num_bufs].bpid = bpid;
-				bufs[num_bufs].hi = (b >> 32);
-				bufs[num_bufs++].lo = b & 0xffffffff;
+				bm_buffer_set64(&bufs[num_bufs++], b);
 				b += d;
 			} while (--c && (num_bufs < 8));
 			ret = bman_release(pobj, bufs, num_bufs,
@@ -115,51 +109,23 @@ static int __bm_pool_add(u32 bpid, u32 *cfg, int triplets)
 	return 0;
 }
 
-int __bm_portal_bind(struct bm_portal *portal, u8 iface)
-{
-	int ret = -EBUSY;
-	spin_lock(&bind_lock);
-	if (!(portal->config.bound & iface)) {
-		portal->config.bound |= iface;
-		ret = 0;
-	}
-	spin_unlock(&bind_lock);
-	return ret;
-}
-
-void __bm_portal_unbind(struct bm_portal *portal, u8 iface)
-{
-	spin_lock(&bind_lock);
-	BM_ASSERT(portal->config.bound & iface);
-	portal->config.bound &= ~iface;
-	spin_unlock(&bind_lock);
-}
-
 u8 bm_portal_num(void)
 {
 	return num_portals;
 }
-EXPORT_SYMBOL(bm_portal_num);
 
-struct bm_portal *bm_portal_get(u8 idx)
+const struct bm_portal_config *bm_portal_config(u8 idx)
 {
 	if (unlikely(idx >= num_portals))
 		return NULL;
 
-	return &portals[idx];
-}
-EXPORT_SYMBOL(bm_portal_get);
-
-const struct bm_portal_config *bm_portal_config(const struct bm_portal *portal)
-{
-	return &portal->config;
+	return &configs[idx];
 }
-EXPORT_SYMBOL(bm_portal_config);
 
 int bm_pool_new(u32 *bpid)
 {
 	int ret = 0, b = 64;
-	spin_lock(&bind_lock);
+	spin_lock(&pools_lock);
 	if (num_pools > 63)
 		ret = -ENOMEM;
 	else {
@@ -170,32 +136,73 @@ int bm_pool_new(u32 *bpid)
 		*bpid = b;
 		num_pools++;
 	}
-	spin_unlock(&bind_lock);
+	spin_unlock(&pools_lock);
 	return ret;
 }
 EXPORT_SYMBOL(bm_pool_new);
 
 void bm_pool_free(u32 bpid)
 {
-	spin_lock(&bind_lock);
+	spin_lock(&pools_lock);
 	BUG_ON(bpid > 63);
 	BUG_ON(!bman_depletion_get(&pools, bpid));
 	bman_depletion_unset(&pools, bpid);
 	num_pools--;
-	spin_unlock(&bind_lock);
+	spin_unlock(&pools_lock);
 }
 EXPORT_SYMBOL(bm_pool_free);
 
+struct affine_portal_data {
+	struct completion done;
+	const struct bm_portal_config *pconfig;
+};
+
+static int __init_affine_portal(void *__data)
+{
+	int ret = 0;
+	struct affine_portal_data *data = __data;
+	const struct bm_portal_config *pconfig = data->pconfig;
+	if (!bman_have_affine_portal()) {
+		u32 irq_sources = 0;
+#ifdef CONFIG_FSL_BMAN_HAVE_IRQ
+		irq_sources = BM_PIRQ_RCRI | BM_PIRQ_BSCN;
+#endif
+		ret = bman_create_affine_portal(pconfig, irq_sources);
+	}
+	complete(&data->done);
+	return ret;
+}
+
+static void init_affine_portal(const struct bm_portal_config *pconfig)
+{
+	struct affine_portal_data data = {
+		.done = COMPLETION_INITIALIZER(data.done),
+		.pconfig = pconfig
+	};
+	struct task_struct *k = kthread_create(__init_affine_portal, &data,
+		"bman_affine%d", pconfig->cpu);
+	int ret;
+	if (IS_ERR(k)) {
+		pr_err("Failed to init Bman affine portal for cpu %d\n",
+			pconfig->cpu);
+		return;
+	}
+	kthread_bind(k, pconfig->cpu);
+	wake_up_process(k);
+	wait_for_completion(&data.done);
+	ret = kthread_stop(k);
+	if (ret)
+		pr_err("Bman portal initialisation failed, cpu %d, code %d\n",
+			pconfig->cpu, ret);
+	else
+		pr_info("Bman portal initialised, cpu %d\n", pconfig->cpu);
+}
+
 static int __init fsl_bman_portal_init(struct device_node *node)
 {
 	struct resource res[2];
-	struct bm_portal_config cfg;
-	struct bm_addr addr;
-	struct bm_portal *portal;
+	struct bm_portal_config cfg, *pconfig;
 	const phandle *cpu_ph = NULL;
-#ifndef CONFIG_FSL_BMAN_PORTAL_DISABLEAUTO
-	struct bman_portal *affine_portal;
-#endif
 	int irq, ret;
 
 	ret = of_address_to_resource(node, 0, &res[0]);
@@ -213,9 +220,9 @@ static int __init fsl_bman_portal_init(struct device_node *node)
 		pr_err("Can't get %s property 'interrupts'\n", node->full_name);
 		return -ENODEV;
 	}
-	addr.addr_ce = ioremap_flags(res[0].start,
+	cfg.addr.addr_ce = ioremap_flags(res[0].start,
 				res[0].end - res[0].start + 1, 0);
-	addr.addr_ci = ioremap_flags(res[1].start,
+	cfg.addr.addr_ci = ioremap_flags(res[1].start,
 				res[1].end - res[1].start + 1,
 				_PAGE_GUARDED | _PAGE_NO_CACHE);
 	cfg.irq = irq;
@@ -253,27 +260,18 @@ static int __init fsl_bman_portal_init(struct device_node *node)
 	}
 bad_cpu_ph:
 	bman_depletion_fill(&cfg.mask);
-	cfg.bound = 0;
-	pr_info("Bman portal at %p:%p (%d)\n", addr.addr_ce, addr.addr_ci,
-		cfg.cpu);
-	portal = __bm_portal_add(&addr, &cfg);
+	pconfig = __bm_portal_add(&cfg);
+	if (!pconfig) {
+		iounmap(cfg.addr.addr_ce);
+		iounmap(cfg.addr.addr_ci);
+		return -ENOMEM;
+	}
+	pr_info("Bman portal at %p:%p (%d)\n", cfg.addr.addr_ce,
+		cfg.addr.addr_ci, cfg.cpu);
 	/* If the portal is affine to a cpu and that cpu has no default affine
 	 * portal, auto-initialise this one for the job. */
-#ifndef CONFIG_FSL_BMAN_PORTAL_DISABLEAUTO
-	if (cfg.cpu == -1)
-		return 0;
-	affine_portal = per_cpu(bman_affine_portal, cfg.cpu);
-	if (!affine_portal) {
-		affine_portal = bman_create_portal(portal, &cfg.mask);
-		if (!affine_portal)
-			pr_err("Bman portal auto-initialisation failed\n");
-		else {
-			pr_info("Bman portal %d auto-initialised\n", cfg.cpu);
-			per_cpu(bman_affine_portal, cfg.cpu) = affine_portal;
-			num_affine_portals++;
-		}
-	}
-#endif
+	if (cfg.cpu != -1)
+		init_affine_portal(pconfig);
 	return 0;
 }
 
@@ -362,6 +360,13 @@ static int __init fsl_bpool_init(struct device_node *node)
 static __init int bman_init(void)
 {
 	struct device_node *dn;
+
+	for_each_compatible_node(dn, NULL, "fsl,bman") {
+		if (!bman_init_error_int(dn))
+			pr_info("Bman err interrupt handler present\n");
+		else
+			pr_err("Bman err interrupt handler missing\n");
+	}
 	if (!bman_have_ccsr()) {
 		/* If there's no CCSR, our bpid allocator is empty */
 		bman_depletion_fill(&pools);
@@ -372,18 +377,17 @@ static __init int bman_init(void)
 		if (ret)
 			return ret;
 	}
-#ifndef CONFIG_FSL_BMAN_PORTAL_DISABLEAUTO
-	for_each_compatible_node(dn, NULL, "fsl,bpool") {
-		int ret = fsl_bpool_init(dn);
-		if (ret)
-			return ret;
-	}
-	if (num_affine_portals != num_online_cpus()) {
+	if (cpumask_equal(bman_affine_cpus(), cpu_online_mask)) {
+		for_each_compatible_node(dn, NULL, "fsl,bpool") {
+			int ret = fsl_bpool_init(dn);
+			if (ret)
+				return ret;
+		}
+	} else {
 		pr_err("Not all cpus have an affine Bman portal\n");
 		pr_err("Ignoring buffer pools\n");
 		pr_err("Expect Bman-dependent drivers to crash!\n");
 	}
-#endif
 	return 0;
 	pr_info("Bman driver initialised\n");
 }
diff --git a/drivers/hwalloc/bman_high.c b/drivers/hwalloc/bman_high.c
index 9c4f7bb..95b138e 100644
--- a/drivers/hwalloc/bman_high.c
+++ b/drivers/hwalloc/bman_high.c
@@ -30,33 +30,29 @@
  * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  */
 
-/* TODO:
- *
- * - make RECOVER also handle incomplete mgmt-commands
- */
-
-#include "bman_private.h"
+#include "bman_low.h"
 
 /* Compilation constants */
 #define RCR_THRESH	2	/* reread h/w CI when running out of space */
-#define RCR_ITHRESH	4	/* if RCR congests, interrupt threshold */
 #define IRQNAME		"BMan portal %d"
 #define MAX_IRQNAME	16	/* big enough for "BMan portal %d" */
 
-/**************/
-/* Portal API */
-/**************/
-
 struct bman_portal {
-	struct bm_portal *p;
+	struct bm_portal p;
 	/* 2-element array. pools[0] is mask, pools[1] is snapshot. */
 	struct bman_depletion *pools;
 	int thresh_set;
+	unsigned long irq_sources;
 	u32 slowpoll;	/* only used when interrupts are off */
 	wait_queue_head_t queue;
-	/* The wrap-around rcr_[prod|cons] counters are used to support
-	 * BMAN_RELEASE_FLAG_WAIT_SYNC. */
-	u32 rcr_prod, rcr_cons;
+#ifdef CONFIG_FSL_DPA_CAN_WAIT_SYNC
+	struct bman_pool *rcri_owned; /* only 1 release WAIT_SYNC at a time */
+#endif
+#ifdef CONFIG_FSL_BMAN_PORTAL_TASKLET
+	struct tasklet_struct tasklet;
+#endif
+	/* When the cpu-affine portal is activated, this is non-NULL */
+	const struct bm_portal_config *config;
 	/* 64-entry hash-table of pool objects that are tracking depletion
 	 * entry/exit (ie. BMAN_POOL_FLAG_DEPLETION). This isn't fast-path, so
 	 * we're not fussy about cache-misses and so forth - whereas the above
@@ -65,9 +61,20 @@ struct bman_portal {
 	 * you'll never guess the hash-function ... */
 	struct bman_pool *cb[64];
 	char irqname[MAX_IRQNAME];
-	spinlock_t lock;
 };
 
+static cpumask_t affine_mask = CPU_MASK_NONE;
+static DEFINE_SPINLOCK(affine_mask_lock);
+static DEFINE_PER_CPU(struct bman_portal, bman_affine_portal);
+static inline struct bman_portal *get_affine_portal(void)
+{
+	return &get_cpu_var(bman_affine_portal);
+}
+static inline void put_affine_portal(void)
+{
+	put_cpu_var(bman_affine_portal);
+}
+
 /* GOTCHA: this object type refers to a pool, it isn't *the* pool. There may be
  * more than one such object per Bman buffer pool, eg. if different users of the
  * pool are operating via different portals. */
@@ -84,26 +91,24 @@ struct bman_pool {
 /* (De)Registration of depletion notification callbacks */
 static void depletion_link(struct bman_portal *portal, struct bman_pool *pool)
 {
-	unsigned long flags;
-
+	__maybe_unused unsigned long irqflags;
 	pool->portal = portal;
-	spin_lock_irqsave(&portal->lock, flags);
+	local_irq_save(irqflags);
 	pool->next = portal->cb[pool->params.bpid];
 	portal->cb[pool->params.bpid] = pool;
 	if (!pool->next)
 		/* First object for that bpid on this portal, enable the BSCN
 		 * mask bit. */
-		bm_isr_bscn_mask(portal->p, pool->params.bpid, 1);
-	spin_unlock_irqrestore(&portal->lock, flags);
+		bm_isr_bscn_mask(&portal->p, pool->params.bpid, 1);
+	local_irq_restore(irqflags);
 }
 static void depletion_unlink(struct bman_pool *pool)
 {
 	struct bman_pool *it, *last = NULL;
 	struct bman_pool **base = &pool->portal->cb[pool->params.bpid];
-	unsigned long flags;
-
-	spin_lock_irqsave(&pool->portal->lock, flags);
-	it = *base;	/* <-- gotcha, don't do this prior to the irq_disable */
+	__maybe_unused unsigned long irqflags;
+	local_irq_save(irqflags);
+	it = *base;	/* <-- gotcha, don't do this prior to the irq_save */
 	while (it != pool) {
 		last = it;
 		it = it->next;
@@ -115,50 +120,65 @@ static void depletion_unlink(struct bman_pool *pool)
 	if (!last && !pool->next)
 		/* Last object for that bpid on this portal, disable the BSCN
 		 * mask bit. */
-		bm_isr_bscn_mask(pool->portal->p, pool->params.bpid, 0);
-	spin_unlock_irqrestore(&pool->portal->lock, flags);
+		bm_isr_bscn_mask(&pool->portal->p, pool->params.bpid, 0);
+	local_irq_restore(irqflags);
 }
 
 static u32 __poll_portal_slow(struct bman_portal *p, struct bm_portal *lowp,
 				u32 is);
-static inline void __poll_portal_fast(struct bman_portal *p,
-				struct bm_portal *lowp);
 
 #ifdef CONFIG_FSL_BMAN_HAVE_IRQ
+/* This is called from the ISR or from a deferred tasklet */
+static inline void do_isr_work(struct bman_portal *p)
+{
+	struct bm_portal *lowp = &p->p;
+	u32 clear = p->irq_sources;
+	u32 is = bm_isr_status_read(lowp) & p->irq_sources;
+	clear |= __poll_portal_slow(p, lowp, is);
+	bm_isr_status_clear(lowp, clear);
+}
+#ifdef CONFIG_FSL_BMAN_PORTAL_TASKLET
+static void portal_tasklet(unsigned long __p)
+{
+	struct bman_portal *p = (struct bman_portal *)__p;
+	do_isr_work(p);
+	bm_isr_uninhibit(&p->p);
+}
+#endif
 /* Portal interrupt handler */
 static irqreturn_t portal_isr(__always_unused int irq, void *ptr)
 {
 	struct bman_portal *p = ptr;
-	struct bm_portal *lowp = p->p;
-	u32 clear = 0;
-#ifdef CONFIG_FSL_BMAN_PORTAL_FLAG_IRQ_SLOW
-	u32 is = bm_isr_status_read(lowp);
-#endif
-	/* Only do fast-path handling if it's required */
-#ifdef CONFIG_FSL_BMAN_PORTAL_FLAG_IRQ_FAST
-	__poll_portal_fast(p, lowp);
-#endif
-#ifdef CONFIG_FSL_BMAN_PORTAL_FLAG_IRQ_SLOW
-	clear |= __poll_portal_slow(p, lowp, is);
+#ifdef CONFIG_FSL_BMAN_PORTAL_TASKLET
+	bm_isr_inhibit(&p->p);
+	tasklet_schedule(&p->tasklet);
+#else
+	do_isr_work(p);
 #endif
-	bm_isr_status_clear(lowp, clear);
 	return IRQ_HANDLED;
 }
 #endif
 
-struct bman_portal *bman_create_portal(struct bm_portal *__p,
-				const struct bman_depletion *pools)
+int bman_have_affine_portal(void)
 {
-	struct bman_portal *portal;
-#ifdef CONFIG_FSL_BMAN_HAVE_IRQ
-	const struct bm_portal_config *config = bm_portal_config(__p);
-#endif
+	struct bman_portal *bm = get_affine_portal();
+	int ret = (bm->config ? 1 : 0);
+	put_affine_portal();
+	return ret;
+}
+
+int bman_create_affine_portal(const struct bm_portal_config *config,
+				u32 irq_sources)
+{
+	struct bman_portal *portal = get_affine_portal();
+	struct bm_portal *__p = &portal->p;
+	const struct bman_depletion *pools = &config->mask;
 	int ret;
 
-	portal = kmalloc(sizeof(*portal), GFP_KERNEL);
-	if (!portal)
-		return NULL;
-	spin_lock_init(&portal->lock);
+	/* prep the low-level portal struct with the mapped addresses from the
+	 * config, everything that follows depends on it and "config" is more
+	 * for (de)reference... */
+	__p->addr = config->addr;
 	if (bm_rcr_init(__p, bm_rcr_pvb, bm_rcr_cce)) {
 		pr_err("Bman RCR initialisation failed\n");
 		goto fail_rcr;
@@ -171,7 +191,6 @@ struct bman_portal *bman_create_portal(struct bm_portal *__p,
 		pr_err("Bman ISR initialisation failed\n");
 		goto fail_isr;
 	}
-	portal->p = __p;
 	if (!pools)
 		portal->pools = NULL;
 	else {
@@ -190,19 +209,25 @@ struct bman_portal *bman_create_portal(struct bm_portal *__p,
 	}
 	portal->slowpoll = 0;
 	init_waitqueue_head(&portal->queue);
-	portal->rcr_prod = portal->rcr_cons = 0;
+#ifdef CONFIG_FSL_DPA_CAN_WAIT_SYNC
+	portal->rcri_owned = NULL;
+#endif
+#ifdef CONFIG_FSL_BMAN_PORTAL_TASKLET
+	tasklet_init(&portal->tasklet, portal_tasklet, (unsigned long)portal);
+#endif
 	memset(&portal->cb, 0, sizeof(portal->cb));
 	/* Write-to-clear any stale interrupt status bits */
-	bm_isr_disable_write(portal->p, 0xffffffff);
-#ifdef CONFIG_FSL_BMAN_HAVE_IRQ
-	bm_isr_enable_write(portal->p, BM_PIRQ_RCRI | BM_PIRQ_BSCN);
-#else
-	bm_isr_enable_write(portal->p, 0);
-#endif
-	bm_isr_status_clear(portal->p, 0xffffffff);
+	bm_isr_disable_write(__p, 0xffffffff);
+	portal->irq_sources = irq_sources;
+	bm_isr_enable_write(__p, portal->irq_sources);
+	bm_isr_status_clear(__p, 0xffffffff);
 #ifdef CONFIG_FSL_BMAN_HAVE_IRQ
 	snprintf(portal->irqname, MAX_IRQNAME, IRQNAME, config->cpu);
+#if defined(CONFIG_PREEMPT_HARDIRQS) || defined(CONFIG_PREEMPT_SOFTIRQS)
+	if (request_irq(config->irq, portal_isr, IRQF_NODELAY, portal->irqname, portal)) {
+#else
 	if (request_irq(config->irq, portal_isr, 0, portal->irqname, portal)) {
+#endif
 		pr_err("request_irq() failed\n");
 		goto fail_irq;
 	}
@@ -214,22 +239,26 @@ struct bman_portal *bman_create_portal(struct bm_portal *__p,
 		goto fail_affinity;
 	}
 	/* Enable the bits that make sense */
-	bm_isr_uninhibit(portal->p);
-#endif
-	/* Need RCR to be empty before continuing */
-	bm_isr_disable_write(portal->p, ~BM_PIRQ_RCRI);
-#ifdef CONFIG_FSL_BMAN_PORTAL_FLAG_RECOVER
-	wait_event(portal->queue, !bm_rcr_get_fill(portal->p));
-	ret = 0;
+	bm_isr_uninhibit(__p);
 #else
-	ret = bm_rcr_get_fill(portal->p);
+	if (irq_sources)
+		panic("No Bman portal IRQ support, mustn't specify IRQ flags!");
 #endif
+	/* Need RCR to be empty before continuing */
+	bm_isr_disable_write(__p, ~BM_PIRQ_RCRI);
+	ret = bm_rcr_get_fill(__p);
 	if (ret) {
 		pr_err("Bman RCR unclean, need recovery\n");
 		goto fail_rcr_empty;
 	}
-	bm_isr_disable_write(portal->p, 0);
-	return portal;
+	/* Success */
+	portal->config = config;
+	spin_lock(&affine_mask_lock);
+	cpumask_set_cpu(config->cpu, &affine_mask);
+	spin_unlock(&affine_mask_lock);
+	bm_isr_disable_write(__p, 0);
+	put_affine_portal();
+	return 0;
 fail_rcr_empty:
 #ifdef CONFIG_FSL_BMAN_HAVE_IRQ
 fail_affinity:
@@ -245,22 +274,26 @@ fail_isr:
 fail_mc:
 	bm_rcr_finish(__p);
 fail_rcr:
-	kfree(portal);
-	return NULL;
+	put_affine_portal();
+	return -EINVAL;
 }
 
-void bman_destroy_portal(struct bman_portal *bm)
+void bman_destroy_affine_portal(void)
 {
-	bm_rcr_cce_update(bm->p);
+	struct bman_portal *bm = get_affine_portal();
+	bm_rcr_cce_update(&bm->p);
 #ifdef CONFIG_FSL_BMAN_HAVE_IRQ
-	free_irq(bm_portal_config(bm->p)->irq, bm);
+	free_irq(bm->config->irq, bm);
 #endif
-	if (bm->pools)
-		kfree(bm->pools);
-	bm_isr_finish(bm->p);
-	bm_mc_finish(bm->p);
-	bm_rcr_finish(bm->p);
-	kfree(bm);
+	kfree(bm->pools);
+	bm_isr_finish(&bm->p);
+	bm_mc_finish(&bm->p);
+	bm_rcr_finish(&bm->p);
+	spin_lock(&affine_mask_lock);
+	cpumask_clear_cpu(bm->config->cpu, &affine_mask);
+	spin_unlock(&affine_mask_lock);
+	bm->config = NULL;
+	put_affine_portal();
 }
 
 /* When release logic waits on available RCR space, we need a global waitqueue
@@ -272,7 +305,6 @@ static u32 __poll_portal_slow(struct bman_portal *p, struct bm_portal *lowp,
 				u32 is)
 {
 	struct bman_depletion tmp;
-	unsigned long flags;
 	u32 ret = is;
 
 	/* There is a gotcha to be aware of. If we do the query before clearing
@@ -284,6 +316,7 @@ static u32 __poll_portal_slow(struct bman_portal *p, struct bm_portal *lowp,
 	 * the query, hence the odd while loop with the 'is' accumulation. */
 	if (is & BM_PIRQ_BSCN) {
 		struct bm_mc_result *mcr;
+		__maybe_unused unsigned long irqflags;
 		unsigned int i, j;
 		u32 __is;
 		bm_isr_status_clear(lowp, BM_PIRQ_BSCN);
@@ -292,13 +325,13 @@ static u32 __poll_portal_slow(struct bman_portal *p, struct bm_portal *lowp,
 			bm_isr_status_clear(lowp, BM_PIRQ_BSCN);
 		}
 		is &= ~BM_PIRQ_BSCN;
-		spin_lock_irqsave(&p->lock, flags);
+		local_irq_save(irqflags);
 		bm_mc_start(lowp);
 		bm_mc_commit(lowp, BM_MCC_VERB_CMD_QUERY);
 		while (!(mcr = bm_mc_result(lowp)))
 			cpu_relax();
 		tmp = mcr->query.ds.state;
-		spin_unlock_irqrestore(&p->lock, flags);
+		local_irq_restore(irqflags);
 		for (i = 0; i < 2; i++) {
 			int idx = i * 32;
 			/* tmp is a mask of currently-depleted pools.
@@ -326,11 +359,22 @@ static u32 __poll_portal_slow(struct bman_portal *p, struct bm_portal *lowp,
 	}
 
 	if (is & BM_PIRQ_RCRI) {
-		spin_lock_irqsave(&p->lock, flags);
-		p->rcr_cons += bm_rcr_cce_update(lowp);
+		__maybe_unused unsigned long irqflags;
+		local_irq_save(irqflags);
+		bm_rcr_cce_update(lowp);
+#ifdef CONFIG_FSL_DPA_CAN_WAIT_SYNC
+		/* If waiting for sync, we only cancel the interrupt threshold
+		 * when the ring utilisation hits zero. */
+		if (p->rcri_owned) {
+			if (!bm_rcr_get_fill(lowp)) {
+				p->rcri_owned = NULL;
+				bm_rcr_set_ithresh(lowp, 0);
+			}
+		} else
+#endif
 		bm_rcr_set_ithresh(lowp, 0);
+		local_irq_restore(irqflags);
 		wake_up(&p->queue);
-		spin_unlock_irqrestore(&p->lock, flags);
 		bm_isr_status_clear(lowp, BM_PIRQ_RCRI);
 		is &= ~BM_PIRQ_RCRI;
 	}
@@ -340,12 +384,51 @@ static u32 __poll_portal_slow(struct bman_portal *p, struct bm_portal *lowp,
 	return ret;
 }
 
-static inline void __poll_portal_fast(__always_unused struct bman_portal *p,
-					__always_unused struct bm_portal *lowp)
+u32 bman_irqsource_get(void)
+{
+	struct bman_portal *p = get_affine_portal();
+	u32 ret = p->irq_sources & BM_PIRQ_VISIBLE;
+	put_affine_portal();
+	return ret;
+}
+EXPORT_SYMBOL(bman_irqsource_get);
+
+void bman_irqsource_add(__maybe_unused u32 bits)
 {
-	/* nothing yet, this is where we'll put optimised RCR consumption
-	 * tracking */
+#ifdef CONFIG_FSL_BMAN_HAVE_IRQ
+	struct bman_portal *p = get_affine_portal();
+	set_bits(bits & BM_PIRQ_VISIBLE, &p->irq_sources);
+	bm_isr_enable_write(&p->p, p->irq_sources);
+	put_affine_portal();
+#else
+	panic("No Bman portal IRQ support, mustn't spcify IRQ flags!");
+#endif
 }
+EXPORT_SYMBOL(bman_irqsource_add);
+
+void bman_irqsource_remove(u32 bits)
+{
+	struct bman_portal *p = get_affine_portal();
+	/* Subtle but important: we need to update the interrupt enable register
+	 * prior to clearing p->irq_sources. If we don't, an interrupt-spin
+	 * might happen between us clearing p->irq_sources and preventing the
+	 * same sources from triggering an interrupt. This means we have to read
+	 * the register back with a data-dependency, to ensure the write reaches
+	 * Bman before we update p->irq_sources. Hence the appearance of
+	 * obfuscation... */
+	u32 newval = p->irq_sources & ~(bits & BM_PIRQ_VISIBLE);
+	bm_isr_enable_write(&p->p, newval);
+	newval = bm_isr_enable_read(&p->p);
+	clear_bits(~newval, &p->irq_sources);
+	put_affine_portal();
+}
+EXPORT_SYMBOL(bman_irqsource_remove);
+
+const cpumask_t *bman_affine_cpus(void)
+{
+	return &affine_mask;
+}
+EXPORT_SYMBOL(bman_affine_cpus);
 
 /* In the case that slow- and fast-path handling are both done by bman_poll()
  * (ie. because there is no interrupt handling), we ought to balance how often
@@ -356,28 +439,21 @@ static inline void __poll_portal_fast(__always_unused struct bman_portal *p,
  * work to do. */
 #define SLOW_POLL_IDLE   1000
 #define SLOW_POLL_BUSY   10
-#ifdef CONFIG_FSL_BMAN_HAVE_POLL
 void bman_poll(void)
 {
 	struct bman_portal *p = get_affine_portal();
-	struct bm_portal *lowp = p->p;
-#ifndef CONFIG_FSL_BMAN_PORTAL_FLAG_IRQ_SLOW
+	struct bm_portal *lowp = &p->p;
 	if (!(p->slowpoll--)) {
-		u32 is = bm_isr_status_read(lowp);
+		u32 is = bm_isr_status_read(lowp) & ~p->irq_sources;
 		u32 active = __poll_portal_slow(p, lowp, is);
 		if (active)
 			p->slowpoll = SLOW_POLL_BUSY;
 		else
 			p->slowpoll = SLOW_POLL_IDLE;
 	}
-#endif
-#ifndef CONFIG_FSL_BMAN_PORTAL_FLAG_IRQ_FAST
-	__poll_portal_fast(p, lowp);
-#endif
 	put_affine_portal();
 }
 EXPORT_SYMBOL(bman_poll);
-#endif
 
 static const u32 zero_thresholds[4] = {0, 0, 0, 0};
 
@@ -489,135 +565,159 @@ const struct bman_pool_params *bman_get_params(const struct bman_pool *pool)
 }
 EXPORT_SYMBOL(bman_get_params);
 
-static noinline void rel_set_thresh(struct bman_portal *p, int check)
+static noinline void update_rcr_ci(struct bman_portal *p, u8 avail)
 {
-	if (!check || !bm_rcr_get_ithresh(p->p))
-		bm_rcr_set_ithresh(p->p, RCR_ITHRESH);
+	if (avail)
+		bm_rcr_cce_prefetch(&p->p);
+	else
+		bm_rcr_cce_update(&p->p);
 }
 
-/* Used as a wait_event() expression. If it returns non-NULL, any lock will
- * remain held. */
-static struct bm_rcr_entry *__try_rel(struct bman_portal **p)
+int bman_rcr_is_empty(void)
+{
+	__maybe_unused unsigned long irqflags;
+	struct bman_portal *p = get_affine_portal();
+	u8 avail;
+
+	local_irq_save(irqflags);
+	update_rcr_ci(p, 0);
+	avail = bm_rcr_get_fill(&p->p);
+	local_irq_restore(irqflags);
+	put_affine_portal();
+	return (avail == 0);
+}
+EXPORT_SYMBOL(bman_rcr_is_empty);
+
+static inline struct bm_rcr_entry *try_rel_start(struct bman_portal **p,
+#ifdef CONFIG_FSL_DPA_CAN_WAIT
+					__maybe_unused struct bman_pool *pool,
+#endif
+					__maybe_unused unsigned long *irqflags,
+					__maybe_unused u32 flags)
 {
 	struct bm_rcr_entry *r;
-	struct bm_portal *lowp;
 	u8 avail;
+
 	*p = get_affine_portal();
-	lowp = (*p)->p;
-	local_irq_disable();
-	avail = bm_rcr_get_avail(lowp);
-	if (avail == RCR_THRESH)
-		/* We don't need RCR:CI yet, but we will next time */
-		bm_rcr_cce_prefetch(lowp);
-	else if (avail < RCR_THRESH)
-		(*p)->rcr_cons += bm_rcr_cce_update(lowp);
-	r = bm_rcr_start(lowp);
+	local_irq_save((*irqflags));
+#ifdef CONFIG_FSL_DPA_CAN_WAIT_SYNC
+	if (unlikely((flags & BMAN_RELEASE_FLAG_WAIT) &&
+			(flags & BMAN_RELEASE_FLAG_WAIT_SYNC))) {
+		if ((*p)->rcri_owned) {
+			local_irq_restore((*irqflags));
+			put_affine_portal();
+			return NULL;
+		}
+		(*p)->rcri_owned = pool;
+	}
+#endif
+	avail = bm_rcr_get_avail(&(*p)->p);
+	if (avail < 2)
+		update_rcr_ci(*p, avail);
+	r = bm_rcr_start(&(*p)->p);
 	if (unlikely(!r)) {
-		local_irq_enable();
+#ifdef CONFIG_FSL_DPA_CAN_WAIT_SYNC
+		if (unlikely((flags & BMAN_RELEASE_FLAG_WAIT) &&
+				(flags & BMAN_RELEASE_FLAG_WAIT_SYNC)))
+			(*p)->rcri_owned = NULL;
+#endif
+		local_irq_restore((*irqflags));
 		put_affine_portal();
 	}
 	return r;
 }
 
-static inline struct bm_rcr_entry *try_rel_start(struct bman_portal **p)
+#ifdef CONFIG_FSL_DPA_CAN_WAIT
+static noinline struct bm_rcr_entry *__wait_rel_start(struct bman_portal **p,
+					struct bman_pool *pool,
+					__maybe_unused unsigned long *irqflags,
+					u32 flags)
 {
-	struct bm_rcr_entry *rcr = __try_rel(p);
-	if (unlikely(!rcr))
-		rel_set_thresh(*p, 1);
+	struct bm_rcr_entry *rcr = try_rel_start(p, pool, irqflags, flags);
+	if (!rcr)
+		bm_rcr_set_ithresh(&(*p)->p, 1);
 	return rcr;
 }
 
 static noinline struct bm_rcr_entry *wait_rel_start(struct bman_portal **p,
-							u32 flags)
+					struct bman_pool *pool,
+					__maybe_unused unsigned long *irqflags,
+					u32 flags)
 {
 	struct bm_rcr_entry *rcr;
-	int ret = 0;
-	if (flags & BMAN_RELEASE_FLAG_WAIT_INT)
-		ret = wait_event_interruptible(affine_queue,
-				(rcr = try_rel_start(p)));
-	else
-		wait_event(affine_queue, (rcr = try_rel_start(p)));
-	return rcr;
-}
-
-/* This copies Qman's eqcr_completed() routine, see that for details */
-static int rel_completed(struct bman_portal *p, u32 rcr_poll)
-{
-	u32 tr_cons = p->rcr_cons;
-	if (rcr_poll & 0xc0000000) {
-		rcr_poll &= 0x7fffffff;
-		tr_cons ^= 0x80000000;
-	}
-	if (tr_cons >= rcr_poll)
-		return 1;
-	if ((rcr_poll - tr_cons) > BM_RCR_SIZE)
-		return 1;
-	if (!bm_rcr_get_fill(p->p))
-		/* If RCR is empty, we must have completed */
-		return 1;
-	rel_set_thresh(p, 0);
-	return 0;
-}
-
-static noinline void wait_rel_commit(struct bman_portal *p, u32 flags,
-					u32 rcr_poll)
-{
-	rel_set_thresh(p, 1);
-	/* So we're supposed to wait until the commit is consumed */
+#ifndef CONFIG_FSL_DPA_CAN_WAIT_SYNC
+	pool = NULL;
+#endif
 	if (flags & BMAN_RELEASE_FLAG_WAIT_INT)
-		/* See bman_release() as to why we're ignoring return codes
-		 * from wait_***(). */
 		wait_event_interruptible(affine_queue,
-					rel_completed(p, rcr_poll));
+			(rcr = __wait_rel_start(p, pool, irqflags, flags)));
 	else
-		wait_event(affine_queue, rel_completed(p, rcr_poll));
+		wait_event(affine_queue,
+			(rcr = __wait_rel_start(p, pool, irqflags, flags)));
+	return rcr;
 }
+#endif
 
-static inline void rel_commit(struct bman_portal *p, u32 flags, u8 num)
-{
-	u32 rcr_poll;
-	bm_rcr_pvb_commit(p->p, BM_RCR_VERB_CMD_BPID_SINGLE |
-			(num & BM_RCR_VERB_BUFCOUNT_MASK));
-	/* increment the producer count and capture it for SYNC */
-	rcr_poll = ++p->rcr_prod;
-	local_irq_enable();
-	put_affine_portal();
-	if ((flags & BMAN_RELEASE_FLAG_WAIT_SYNC) ==
-			BMAN_RELEASE_FLAG_WAIT_SYNC)
-		wait_rel_commit(p, flags, rcr_poll);
-}
+/* to facilitate better copying of bufs into the ring without either (a) copying
+ * noise into the first byte (prematurely triggering the command), nor (b) being
+ * very inefficient by copying small fields using read-modify-write */
+struct overlay_bm_buffer {
+	u32 first;
+	u32 second;
+};
 
 static inline int __bman_release(struct bman_pool *pool,
 			const struct bm_buffer *bufs, u8 num, u32 flags)
 {
 	struct bman_portal *p;
 	struct bm_rcr_entry *r;
+	struct overlay_bm_buffer *o_dest;
+	struct overlay_bm_buffer *o_src = (struct overlay_bm_buffer *)&bufs[0];
+	__maybe_unused unsigned long irqflags;
 	u32 i = num - 1;
 
-	/* FIXME: I'm ignoring BMAN_PORTAL_FLAG_COMPACT for now. */
-	r = try_rel_start(&p);
-	if (unlikely(!r)) {
-		if (flags & BMAN_RELEASE_FLAG_WAIT) {
-			r = wait_rel_start(&p, flags);
-			if (!r)
-				return -EBUSY;
-		} else
-			return -EBUSY;
-		BM_ASSERT(r != NULL);
-	}
-	/* We can memcpy() all but the first entry, as this can trigger badness
-	 * with the valid-bit. */
-	r->bpid = pool->params.bpid;
-	r->bufs[0].hi = bufs[0].hi;
-	r->bufs[0].lo = bufs[0].lo;
+#ifdef CONFIG_FSL_DPA_CAN_WAIT
+	if (flags & BMAN_RELEASE_FLAG_WAIT)
+		r = wait_rel_start(&p, pool, &irqflags, flags);
+	else
+		r = try_rel_start(&p, pool, &irqflags, flags);
+#else
+	r = try_rel_start(&p, &irqflags, flags);
+#endif
+	if (!r)
+		return -EBUSY;
+	/* We can copy all but the first entry, as this can trigger badness
+	 * with the valid-bit. Use the overlay to mask the verb byte. */
+	o_dest = (struct overlay_bm_buffer *)&r->bufs[0];
+	o_dest->first = (o_src->first & 0x0000ffff) |
+		(((u32)pool->params.bpid << 16) & 0x00ff0000);
+	o_dest->second = o_src->second;
 	if (i)
-		memcpy(&r->bufs[1], &bufs[1], i * sizeof(bufs[0]));
-	/* Issue the release command and wait for sync if requested. NB: the
-	 * commit can't fail, only waiting can. Don't propogate any failure if a
-	 * signal arrives, otherwise the caller can't distinguish whether the
-	 * release was issued or not. Code for user-space can check
-	 * signal_pending() after we return. */
-	rel_commit(p, flags, num);
+		copy_words(&r->bufs[1], &bufs[1], i * sizeof(bufs[0]));
+	bm_rcr_pvb_commit(&p->p, BM_RCR_VERB_CMD_BPID_SINGLE |
+			(num & BM_RCR_VERB_BUFCOUNT_MASK));
+#ifdef CONFIG_FSL_DPA_CAN_WAIT_SYNC
+	/* if we wish to sync we need to set the threshold after h/w sees the
+	 * new ring entry. As we're mixing cache-enabled and cache-inhibited
+	 * accesses, this requires a heavy-weight sync. */
+	if (unlikely((flags & BMAN_RELEASE_FLAG_WAIT) &&
+			(flags & BMAN_RELEASE_FLAG_WAIT_SYNC))) {
+		hwsync();
+		bm_rcr_set_ithresh(&p->p, 1);
+	}
+#endif
+	local_irq_restore(irqflags);
+	put_affine_portal();
+#ifdef CONFIG_FSL_DPA_CAN_WAIT_SYNC
+	if (unlikely((flags & BMAN_RELEASE_FLAG_WAIT) &&
+			(flags & BMAN_RELEASE_FLAG_WAIT_SYNC))) {
+		if (flags & BMAN_RELEASE_FLAG_WAIT_INT)
+			wait_event_interruptible(affine_queue,
+					(p->rcri_owned != pool));
+		else
+			wait_event(affine_queue, (p->rcri_owned != pool));
+	}
+#endif
 	return 0;
 }
 
@@ -649,7 +749,7 @@ int bman_release(struct bman_pool *pool, const struct bm_buffer *bufs, u8 num,
 	while (num) {
 		/* Add buffers to stockpile if they fit */
 		if ((pool->sp_fill + num) < BMAN_STOCKPILE_SZ) {
-			memcpy(pool->sp + pool->sp_fill, bufs,
+			copy_words(pool->sp + pool->sp_fill, bufs,
 				sizeof(struct bm_buffer) * num);
 			pool->sp_fill += num;
 			num = 0; /* --> will return success no matter what */
@@ -673,20 +773,21 @@ static inline int __bman_acquire(struct bman_pool *pool, struct bm_buffer *bufs,
 	struct bman_portal *p = get_affine_portal();
 	struct bm_mc_command *mcc;
 	struct bm_mc_result *mcr;
-	unsigned long flags;
+	__maybe_unused unsigned long irqflags;
 	int ret;
 
-	spin_lock_irqsave(&p->lock, flags);
-	mcc = bm_mc_start(p->p);
+	local_irq_save(irqflags);
+	mcc = bm_mc_start(&p->p);
 	mcc->acquire.bpid = pool->params.bpid;
-	bm_mc_commit(p->p, BM_MCC_VERB_CMD_ACQUIRE |
+	bm_mc_commit(&p->p, BM_MCC_VERB_CMD_ACQUIRE |
 			(num & BM_MCC_VERB_ACQUIRE_BUFCOUNT));
-	while (!(mcr = bm_mc_result(p->p)))
+	while (!(mcr = bm_mc_result(&p->p)))
 		cpu_relax();
 	ret = mcr->verb & BM_MCR_VERB_ACQUIRE_BUFCOUNT;
 	if (bufs)
-		 memcpy(&bufs[0], &mcr->acquire.bufs[0], num * sizeof(bufs[0]));
-	spin_unlock_irqrestore(&p->lock, flags);
+		copy_words(&bufs[0], &mcr->acquire.bufs[0],
+				num * sizeof(bufs[0]));
+	local_irq_restore(irqflags);
 	put_affine_portal();
 	if (ret != num)
 		ret = -ENOMEM;
@@ -712,19 +813,39 @@ int bman_acquire(struct bman_pool *pool, struct bm_buffer *bufs, u8 num,
 	if (!(flags & BMAN_ACQUIRE_FLAG_STOCKPILE) &&
 			(pool->sp_fill <= (BMAN_STOCKPILE_LOW + num))) {
 		int ret = __bman_acquire(pool, pool->sp + pool->sp_fill, 8);
-		if (!ret)
+		if (ret < 0)
 			goto hw_starved;
-		BUG_ON(ret != 8);
+		BM_ASSERT(ret == 8);
 		pool->sp_fill += 8;
 	} else {
 hw_starved:
 		if (pool->sp_fill < num)
 			return -ENOMEM;
 	}
-	memcpy(bufs, pool->sp + (pool->sp_fill - num),
+	copy_words(bufs, pool->sp + (pool->sp_fill - num),
 		sizeof(struct bm_buffer) * num);
 	pool->sp_fill -= num;
 	return num;
 }
 EXPORT_SYMBOL(bman_acquire);
 
+int bman_query_pools(struct bm_pool_state *state)
+{
+	struct bman_portal *p = get_affine_portal();
+	struct bm_mc_command *mcc;
+	struct bm_mc_result *mcr;
+	__maybe_unused unsigned long irqflags;
+
+	local_irq_save(irqflags);
+	mcc = bm_mc_start(&p->p);
+	bm_mc_commit(&p->p, BM_MCC_VERB_CMD_QUERY);
+	while (!(mcr = bm_mc_result(&p->p)))
+		cpu_relax();
+	BM_ASSERT((mcr->verb & BM_MCR_VERB_CMD_MASK) == BM_MCR_VERB_CMD_QUERY);
+	*state = mcr->query;
+	local_irq_restore(irqflags);
+	put_affine_portal();
+	return 0;
+}
+EXPORT_SYMBOL(bman_query_pools);
+
diff --git a/drivers/hwalloc/bman_low.c b/drivers/hwalloc/bman_low.c
deleted file mode 100644
index 260c8fe..0000000
--- a/drivers/hwalloc/bman_low.c
+++ /dev/null
@@ -1,487 +0,0 @@
-/* Copyright (c) 2008-2010 Freescale Semiconductor, Inc.
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *     * Redistributions of source code must retain the above copyright
- *       notice, this list of conditions and the following disclaimer.
- *     * Redistributions in binary form must reproduce the above copyright
- *       notice, this list of conditions and the following disclaimer in the
- *       documentation and/or other materials provided with the distribution.
- *     * Neither the name of Freescale Semiconductor nor the
- *       names of its contributors may be used to endorse or promote products
- *       derived from this software without specific prior written permission.
- *
- *
- * ALTERNATIVELY, this software may be distributed under the terms of the
- * GNU General Public License ("GPL") as published by the Free Software
- * Foundation, either version 2 of that License or (at your option) any
- * later version.
- *
- * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
- * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
- * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
- * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
- * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
- * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
- * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
- * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "bman_private.h"
-
-/***************************/
-/* Portal register assists */
-/***************************/
-
-/* Cache-inhibited register offsets */
-#define REG_RCR_PI_CINH		(void *)0x0000
-#define REG_RCR_CI_CINH		(void *)0x0004
-#define REG_RCR_ITR		(void *)0x0008
-#define REG_CFG			(void *)0x0100
-#define REG_SCN(n)		((void *)(0x0200 + ((n) << 2)))
-#define REG_ISR			(void *)0x0e00
-
-/* Cache-enabled register offsets */
-#define CL_CR			(void *)0x0000
-#define CL_RR0			(void *)0x0100
-#define CL_RR1			(void *)0x0140
-#define CL_RCR			(void *)0x1000
-#define CL_RCR_PI_CENA		(void *)0x3000
-#define CL_RCR_CI_CENA		(void *)0x3100
-
-/* The h/w design requires mappings to be size-aligned so that "add"s can be
- * reduced to "or"s. The primitives below do the same for s/w. */
-
-/* Bitwise-OR two pointers */
-static inline void *ptr_OR(void *a, void *b)
-{
-	return (void *)((unsigned long)a | (unsigned long)b);
-}
-
-/* Cache-inhibited register access */
-static inline u32 __bm_in(struct bm_addr *bm, void *offset)
-{
-	return in_be32(ptr_OR(bm->addr_ci, offset));
-}
-static inline void __bm_out(struct bm_addr *bm, void *offset, u32 val)
-{
-	out_be32(ptr_OR(bm->addr_ci, offset), val);
-}
-#define bm_in(reg)		__bm_in(&portal->addr, REG_##reg)
-#define bm_out(reg, val)	__bm_out(&portal->addr, REG_##reg, val)
-
-/* Convert 'n' cachelines to a pointer value for bitwise OR */
-#define bm_cl(n)		(void *)((n) << 6)
-
-/* Cache-enabled (index) register access */
-static inline void __bm_cl_touch_ro(struct bm_addr *bm, void *offset)
-{
-	dcbt_ro(ptr_OR(bm->addr_ce, offset));
-}
-static inline void __bm_cl_touch_rw(struct bm_addr *bm, void *offset)
-{
-	dcbt_rw(ptr_OR(bm->addr_ce, offset));
-}
-static inline u32 __bm_cl_in(struct bm_addr *bm, void *offset)
-{
-	return in_be32(ptr_OR(bm->addr_ce, offset));
-}
-static inline void __bm_cl_out(struct bm_addr *bm, void *offset, u32 val)
-{
-	out_be32(ptr_OR(bm->addr_ce, offset), val);
-	dcbf(ptr_OR(bm->addr_ce, offset));
-}
-static inline void __bm_cl_invalidate(struct bm_addr *bm, void *offset)
-{
-	dcbi(ptr_OR(bm->addr_ce, offset));
-}
-#define bm_cl_touch_ro(reg)	__bm_cl_touch_ro(&portal->addr, CL_##reg##_CENA)
-#define bm_cl_touch_rw(reg)	__bm_cl_touch_rw(&portal->addr, CL_##reg##_CENA)
-#define bm_cl_in(reg)		__bm_cl_in(&portal->addr, CL_##reg##_CENA)
-#define bm_cl_out(reg, val)	__bm_cl_out(&portal->addr, CL_##reg##_CENA, val)
-#define bm_cl_invalidate(reg) __bm_cl_invalidate(&portal->addr, CL_##reg##_CENA)
-
-/* Cyclic helper for rings. FIXME: once we are able to do fine-grain perf
- * analysis, look at using the "extra" bit in the ring index registers to avoid
- * cyclic issues. */
-static inline u8 cyc_diff(u8 ringsize, u8 first, u8 last)
-{
-	/* 'first' is included, 'last' is excluded */
-	if (first <= last)
-		return last - first;
-	return ringsize + last - first;
-}
-
-/* --------------- */
-/* --- RCR API --- */
-
-/* Bit-wise logic to wrap a ring pointer by clearing the "carry bit" */
-#define RCR_CARRYCLEAR(p) \
-	(void *)((unsigned long)(p) & (~(unsigned long)(BM_RCR_SIZE << 6)))
-
-/* Bit-wise logic to convert a ring pointer to a ring index */
-static inline u8 RCR_PTR2IDX(struct bm_rcr_entry *e)
-{
-	return ((u32)e >> 6) & (BM_RCR_SIZE - 1);
-}
-
-/* Increment the 'cursor' ring pointer, taking 'vbit' into account */
-static inline void RCR_INC(struct bm_rcr *rcr)
-{
-	/* NB: this is odd-looking, but experiments show that it generates
-	 * fast code with essentially no branching overheads. We increment to
-	 * the next RCR pointer and handle overflow and 'vbit'. */
-	struct bm_rcr_entry *partial = rcr->cursor + 1;
-	rcr->cursor = RCR_CARRYCLEAR(partial);
-	if (partial != rcr->cursor)
-		rcr->vbit ^= BM_RCR_VERB_VBIT;
-}
-
-int bm_rcr_init(struct bm_portal *portal, enum bm_rcr_pmode pmode,
-		__maybe_unused enum bm_rcr_cmode cmode)
-{
-	/* This use of 'register', as well as all other occurances, is because
-	 * it has been observed to generate much faster code with gcc than is
-	* otherwise the case. */
-	register struct bm_rcr *rcr = &portal->rcr;
-	u32 cfg;
-	u8 pi;
-
-	if (__bm_portal_bind(portal, BM_BIND_RCR))
-		return -EBUSY;
-	rcr->ring = ptr_OR(portal->addr.addr_ce, CL_RCR);
-	rcr->ci = bm_in(RCR_CI_CINH) & (BM_RCR_SIZE - 1);
-	pi = bm_in(RCR_PI_CINH) & (BM_RCR_SIZE - 1);
-	rcr->cursor = rcr->ring + pi;
-	rcr->vbit = (bm_in(RCR_PI_CINH) & BM_RCR_SIZE) ?  BM_RCR_VERB_VBIT : 0;
-	rcr->available = BM_RCR_SIZE - 1 - cyc_diff(BM_RCR_SIZE, rcr->ci, pi);
-	rcr->ithresh = bm_in(RCR_ITR);
-#ifdef CONFIG_FSL_BMAN_CHECKING
-	rcr->busy = 0;
-	rcr->pmode = pmode;
-	rcr->cmode = cmode;
-#endif
-	cfg = (bm_in(CFG) & 0xffffffe0) | (pmode & 0x3); /* BCSP_CFG::RPM */
-	bm_out(CFG, cfg);
-	return 0;
-}
-EXPORT_SYMBOL(bm_rcr_init);
-
-void bm_rcr_finish(struct bm_portal *portal)
-{
-	register struct bm_rcr *rcr = &portal->rcr;
-	u8 pi = bm_in(RCR_PI_CINH) & (BM_RCR_SIZE - 1);
-	u8 ci = bm_in(RCR_CI_CINH) & (BM_RCR_SIZE - 1);
-	BM_ASSERT(!rcr->busy);
-	if (pi != RCR_PTR2IDX(rcr->cursor))
-		pr_crit("losing uncommited RCR entries\n");
-	if (ci != rcr->ci)
-		pr_crit("missing existing RCR completions\n");
-	if (rcr->ci != RCR_PTR2IDX(rcr->cursor))
-		pr_crit("RCR destroyed unquiesced\n");
-	__bm_portal_unbind(portal, BM_BIND_RCR);
-}
-EXPORT_SYMBOL(bm_rcr_finish);
-
-struct bm_rcr_entry *bm_rcr_start(struct bm_portal *portal)
-{
-	register struct bm_rcr *rcr = &portal->rcr;
-	BM_ASSERT(!rcr->busy);
-	if (!rcr->available)
-		return NULL;
-#ifdef CONFIG_FSL_BMAN_CHECKING
-	rcr->busy = 1;
-#endif
-	dcbzl(rcr->cursor);
-	return rcr->cursor;
-}
-EXPORT_SYMBOL(bm_rcr_start);
-
-void bm_rcr_abort(struct bm_portal *portal)
-{
-	__maybe_unused register struct bm_rcr *rcr = &portal->rcr;
-	BM_ASSERT(rcr->busy);
-#ifdef CONFIG_FSL_BMAN_CHECKING
-	rcr->busy = 0;
-#endif
-}
-EXPORT_SYMBOL(bm_rcr_abort);
-
-struct bm_rcr_entry *bm_rcr_pend_and_next(struct bm_portal *portal, u8 myverb)
-{
-	register struct bm_rcr *rcr = &portal->rcr;
-	BM_ASSERT(rcr->busy);
-	BM_ASSERT(rcr->pmode != bm_rcr_pvb);
-	if (rcr->available == 1)
-		return NULL;
-	rcr->cursor->__dont_write_directly__verb = myverb | rcr->vbit;
-	dcbf(rcr->cursor);
-	RCR_INC(rcr);
-	rcr->available--;
-	dcbzl(rcr->cursor);
-	return rcr->cursor;
-}
-EXPORT_SYMBOL(bm_rcr_pend_and_next);
-
-void bm_rcr_pci_commit(struct bm_portal *portal, u8 myverb)
-{
-	register struct bm_rcr *rcr = &portal->rcr;
-	BM_ASSERT(rcr->busy);
-	BM_ASSERT(rcr->pmode == bm_rcr_pci);
-	rcr->cursor->__dont_write_directly__verb = myverb | rcr->vbit;
-	RCR_INC(rcr);
-	rcr->available--;
-	hwsync();
-	bm_out(RCR_PI_CINH, RCR_PTR2IDX(rcr->cursor));
-#ifdef CONFIG_FSL_BMAN_CHECKING
-	rcr->busy = 0;
-#endif
-}
-EXPORT_SYMBOL(bm_rcr_pci_commit);
-
-void bm_rcr_pce_prefetch(struct bm_portal *portal)
-{
-	 __maybe_unused register struct bm_rcr *rcr = &portal->rcr;
-	BM_ASSERT(rcr->pmode == bm_rcr_pce);
-	bm_cl_invalidate(RCR_PI);
-	bm_cl_touch_rw(RCR_PI);
-}
-EXPORT_SYMBOL(bm_rcr_pce_prefetch);
-
-void bm_rcr_pce_commit(struct bm_portal *portal, u8 myverb)
-{
-	register struct bm_rcr *rcr = &portal->rcr;
-	BM_ASSERT(rcr->busy);
-	BM_ASSERT(rcr->pmode == bm_rcr_pce);
-	rcr->cursor->__dont_write_directly__verb = myverb | rcr->vbit;
-	RCR_INC(rcr);
-	rcr->available--;
-	lwsync();
-	bm_cl_out(RCR_PI, RCR_PTR2IDX(rcr->cursor));
-#ifdef CONFIG_FSL_BMAN_CHECKING
-	rcr->busy = 0;
-#endif
-}
-EXPORT_SYMBOL(bm_rcr_pce_commit);
-
-void bm_rcr_pvb_commit(struct bm_portal *portal, u8 myverb)
-{
-	register struct bm_rcr *rcr = &portal->rcr;
-	struct bm_rcr_entry *rcursor;
-	BM_ASSERT(rcr->busy);
-	BM_ASSERT(rcr->pmode == bm_rcr_pvb);
-	lwsync();
-	rcursor = rcr->cursor;
-	rcursor->__dont_write_directly__verb = myverb | rcr->vbit;
-	dcbf(rcursor);
-	RCR_INC(rcr);
-	rcr->available--;
-#ifdef CONFIG_FSL_BMAN_CHECKING
-	rcr->busy = 0;
-#endif
-}
-EXPORT_SYMBOL(bm_rcr_pvb_commit);
-
-u8 bm_rcr_cci_update(struct bm_portal *portal)
-{
-	register struct bm_rcr *rcr = &portal->rcr;
-	u8 diff, old_ci = rcr->ci;
-	BM_ASSERT(rcr->cmode == bm_rcr_cci);
-	rcr->ci = bm_in(RCR_CI_CINH) & (BM_RCR_SIZE - 1);
-	diff = cyc_diff(BM_RCR_SIZE, old_ci, rcr->ci);
-	rcr->available += diff;
-	return diff;
-}
-EXPORT_SYMBOL(bm_rcr_cci_update);
-
-void bm_rcr_cce_prefetch(struct bm_portal *portal)
-{
-	__maybe_unused register struct bm_rcr *rcr = &portal->rcr;
-	BM_ASSERT(rcr->cmode == bm_rcr_cce);
-	bm_cl_touch_ro(RCR_CI);
-}
-EXPORT_SYMBOL(bm_rcr_cce_prefetch);
-
-u8 bm_rcr_cce_update(struct bm_portal *portal)
-{
-	register struct bm_rcr *rcr = &portal->rcr;
-	u8 diff, old_ci = rcr->ci;
-	BM_ASSERT(rcr->cmode == bm_rcr_cce);
-	rcr->ci = bm_cl_in(RCR_CI) & (BM_RCR_SIZE - 1);
-	bm_cl_invalidate(RCR_CI);
-	diff = cyc_diff(BM_RCR_SIZE, old_ci, rcr->ci);
-	rcr->available += diff;
-	return diff;
-}
-EXPORT_SYMBOL(bm_rcr_cce_update);
-
-u8 bm_rcr_get_ithresh(struct bm_portal *portal)
-{
-	register struct bm_rcr *rcr = &portal->rcr;
-	return rcr->ithresh;
-}
-EXPORT_SYMBOL(bm_rcr_get_ithresh);
-
-void bm_rcr_set_ithresh(struct bm_portal *portal, u8 ithresh)
-{
-	register struct bm_rcr *rcr = &portal->rcr;
-	rcr->ithresh = ithresh;
-	bm_out(RCR_ITR, ithresh);
-}
-EXPORT_SYMBOL(bm_rcr_set_ithresh);
-
-u8 bm_rcr_get_avail(struct bm_portal *portal)
-{
-	register struct bm_rcr *rcr = &portal->rcr;
-	return rcr->available;
-}
-EXPORT_SYMBOL(bm_rcr_get_avail);
-
-u8 bm_rcr_get_fill(struct bm_portal *portal)
-{
-	register struct bm_rcr *rcr = &portal->rcr;
-	return BM_RCR_SIZE - 1 - rcr->available;
-}
-EXPORT_SYMBOL(bm_rcr_get_fill);
-
-
-/* ------------------------------ */
-/* --- Management command API --- */
-
-int bm_mc_init(struct bm_portal *portal)
-{
-	register struct bm_mc *mc = &portal->mc;
-	if (__bm_portal_bind(portal, BM_BIND_MC))
-		return -EBUSY;
-	mc->cr = ptr_OR(portal->addr.addr_ce, CL_CR);
-	mc->rr = ptr_OR(portal->addr.addr_ce, CL_RR0);
-	mc->rridx = (mc->cr->__dont_write_directly__verb & BM_MCC_VERB_VBIT) ?
-			0 : 1;
-	mc->vbit = mc->rridx ? BM_MCC_VERB_VBIT : 0;
-#ifdef CONFIG_FSL_BMAN_CHECKING
-	mc->state = mc_idle;
-#endif
-	return 0;
-}
-EXPORT_SYMBOL(bm_mc_init);
-
-void bm_mc_finish(struct bm_portal *portal)
-{
-	__maybe_unused register struct bm_mc *mc = &portal->mc;
-	BM_ASSERT(mc->state == mc_idle);
-#ifdef CONFIG_FSL_BMAN_CHECKING
-	if (mc->state != mc_idle)
-		pr_crit("Losing incomplete MC command\n");
-#endif
-	__bm_portal_unbind(portal, BM_BIND_MC);
-}
-EXPORT_SYMBOL(bm_mc_finish);
-
-struct bm_mc_command *bm_mc_start(struct bm_portal *portal)
-{
-	register struct bm_mc *mc = &portal->mc;
-	BM_ASSERT(mc->state == mc_idle);
-#ifdef CONFIG_FSL_BMAN_CHECKING
-	mc->state = mc_user;
-#endif
-	dcbzl(mc->cr);
-	return mc->cr;
-}
-EXPORT_SYMBOL(bm_mc_start);
-
-void bm_mc_abort(struct bm_portal *portal)
-{
-	__maybe_unused register struct bm_mc *mc = &portal->mc;
-	BM_ASSERT(mc->state == mc_user);
-#ifdef CONFIG_FSL_BMAN_CHECKING
-	mc->state = mc_idle;
-#endif
-}
-EXPORT_SYMBOL(bm_mc_abort);
-
-void bm_mc_commit(struct bm_portal *portal, u8 myverb)
-{
-	register struct bm_mc *mc = &portal->mc;
-	BM_ASSERT(mc->state == mc_user);
-	dcbi(mc->rr + mc->rridx);
-	lwsync();
-	mc->cr->__dont_write_directly__verb = myverb | mc->vbit;
-	dcbf(mc->cr);
-	dcbt_ro(mc->rr + mc->rridx);
-#ifdef CONFIG_FSL_BMAN_CHECKING
-	mc->state = mc_hw;
-#endif
-}
-EXPORT_SYMBOL(bm_mc_commit);
-
-struct bm_mc_result *bm_mc_result(struct bm_portal *portal)
-{
-	register struct bm_mc *mc = &portal->mc;
-	struct bm_mc_result *rr = mc->rr + mc->rridx;
-	BM_ASSERT(mc->state == mc_hw);
-	/* The inactive response register's verb byte always returns zero until
-	 * its command is submitted and completed. This includes the valid-bit,
-	 * in case you were wondering... */
-	if (!rr->verb) {
-		dcbi(rr);
-		dcbt_ro(rr);
-		return NULL;
-	}
-	mc->rridx ^= 1;
-	mc->vbit ^= BM_MCC_VERB_VBIT;
-#ifdef CONFIG_FSL_BMAN_CHECKING
-	mc->state = mc_idle;
-#endif
-	return rr;
-}
-EXPORT_SYMBOL(bm_mc_result);
-
-
-/* ------------------------------------- */
-/* --- Portal interrupt register API --- */
-
-int bm_isr_init(struct bm_portal *portal)
-{
-	if (__bm_portal_bind(portal, BM_BIND_ISR))
-		return -EBUSY;
-	return 0;
-}
-EXPORT_SYMBOL(bm_isr_init);
-
-void bm_isr_finish(struct bm_portal *portal)
-{
-	__bm_portal_unbind(portal, BM_BIND_ISR);
-}
-EXPORT_SYMBOL(bm_isr_finish);
-
-#define SCN_REG(bpid) REG_SCN((bpid) / 32)
-#define SCN_BIT(bpid) (0x80000000 >> (bpid & 31))
-void bm_isr_bscn_mask(struct bm_portal *portal, u8 bpid, int enable)
-{
-	u32 val;
-	BM_ASSERT(bpid < 64);
-	/* REG_SCN for bpid=0..31, REG_SCN+4 for bpid=32..63 */
-	val = __bm_in(&portal->addr, SCN_REG(bpid));
-	if (enable)
-		val |= SCN_BIT(bpid);
-	else
-		val &= ~SCN_BIT(bpid);
-	__bm_out(&portal->addr, SCN_REG(bpid), val);
-}
-EXPORT_SYMBOL(bm_isr_bscn_mask);
-
-u32 __bm_isr_read(struct bm_portal *portal, enum bm_isr_reg n)
-{
-	return __bm_in(&portal->addr, REG_ISR + (n << 2));
-}
-EXPORT_SYMBOL(__bm_isr_read);
-
-void __bm_isr_write(struct bm_portal *portal, enum bm_isr_reg n, u32 val)
-{
-	__bm_out(&portal->addr, REG_ISR + (n << 2), val);
-}
-EXPORT_SYMBOL(__bm_isr_write);
-
diff --git a/drivers/hwalloc/bman_low.h b/drivers/hwalloc/bman_low.h
new file mode 100644
index 0000000..656df14
--- /dev/null
+++ b/drivers/hwalloc/bman_low.h
@@ -0,0 +1,514 @@
+/* Copyright (c) 2008-2010 Freescale Semiconductor, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "bman_private.h"
+
+/***************************/
+/* Portal register assists */
+/***************************/
+
+/* Cache-inhibited register offsets */
+#define REG_RCR_PI_CINH		(void *)0x0000
+#define REG_RCR_CI_CINH		(void *)0x0004
+#define REG_RCR_ITR		(void *)0x0008
+#define REG_CFG			(void *)0x0100
+#define REG_SCN(n)		((void *)(0x0200 + ((n) << 2)))
+#define REG_ISR			(void *)0x0e00
+
+/* Cache-enabled register offsets */
+#define CL_CR			(void *)0x0000
+#define CL_RR0			(void *)0x0100
+#define CL_RR1			(void *)0x0140
+#define CL_RCR			(void *)0x1000
+#define CL_RCR_PI_CENA		(void *)0x3000
+#define CL_RCR_CI_CENA		(void *)0x3100
+
+/* The h/w design requires mappings to be size-aligned so that "add"s can be
+ * reduced to "or"s. The primitives below do the same for s/w. */
+
+/* Bitwise-OR two pointers */
+static inline void *ptr_OR(void *a, void *b)
+{
+	return (void *)((unsigned long)a | (unsigned long)b);
+}
+
+/* Cache-inhibited register access */
+static inline u32 __bm_in(struct bm_addr *bm, void *offset)
+{
+	return in_be32(ptr_OR(bm->addr_ci, offset));
+}
+static inline void __bm_out(struct bm_addr *bm, void *offset, u32 val)
+{
+	out_be32(ptr_OR(bm->addr_ci, offset), val);
+}
+#define bm_in(reg)		__bm_in(&portal->addr, REG_##reg)
+#define bm_out(reg, val)	__bm_out(&portal->addr, REG_##reg, val)
+
+/* Convert 'n' cachelines to a pointer value for bitwise OR */
+#define bm_cl(n)		(void *)((n) << 6)
+
+/* Cache-enabled (index) register access */
+static inline void __bm_cl_touch_ro(struct bm_addr *bm, void *offset)
+{
+	dcbt_ro(ptr_OR(bm->addr_ce, offset));
+}
+static inline void __bm_cl_touch_rw(struct bm_addr *bm, void *offset)
+{
+	dcbt_rw(ptr_OR(bm->addr_ce, offset));
+}
+static inline u32 __bm_cl_in(struct bm_addr *bm, void *offset)
+{
+	return in_be32(ptr_OR(bm->addr_ce, offset));
+}
+static inline void __bm_cl_out(struct bm_addr *bm, void *offset, u32 val)
+{
+	out_be32(ptr_OR(bm->addr_ce, offset), val);
+	dcbf(ptr_OR(bm->addr_ce, offset));
+}
+static inline void __bm_cl_invalidate(struct bm_addr *bm, void *offset)
+{
+	dcbi(ptr_OR(bm->addr_ce, offset));
+}
+#define bm_cl_touch_ro(reg)	__bm_cl_touch_ro(&portal->addr, CL_##reg##_CENA)
+#define bm_cl_touch_rw(reg)	__bm_cl_touch_rw(&portal->addr, CL_##reg##_CENA)
+#define bm_cl_in(reg)		__bm_cl_in(&portal->addr, CL_##reg##_CENA)
+#define bm_cl_out(reg, val)	__bm_cl_out(&portal->addr, CL_##reg##_CENA, val)
+#define bm_cl_invalidate(reg) __bm_cl_invalidate(&portal->addr, CL_##reg##_CENA)
+
+/* Cyclic helper for rings. FIXME: once we are able to do fine-grain perf
+ * analysis, look at using the "extra" bit in the ring index registers to avoid
+ * cyclic issues. */
+static inline u8 cyc_diff(u8 ringsize, u8 first, u8 last)
+{
+	/* 'first' is included, 'last' is excluded */
+	if (first <= last)
+		return last - first;
+	return ringsize + last - first;
+}
+
+/* Portal modes.
+ *   Enum types;
+ *     pmode == production mode
+ *     cmode == consumption mode,
+ *   Enum values use 3 letter codes. First letter matches the portal mode,
+ *   remaining two letters indicate;
+ *     ci == cache-inhibited portal register
+ *     ce == cache-enabled portal register
+ *     vb == in-band valid-bit (cache-enabled)
+ */
+enum bm_rcr_pmode {		/* matches BCSP_CFG::RPM */
+	bm_rcr_pci = 0,		/* PI index, cache-inhibited */
+	bm_rcr_pce = 1,		/* PI index, cache-enabled */
+	bm_rcr_pvb = 2		/* valid-bit */
+};
+enum bm_rcr_cmode {		/* s/w-only */
+	bm_rcr_cci,		/* CI index, cache-inhibited */
+	bm_rcr_cce		/* CI index, cache-enabled */
+};
+
+
+/* ------------------------- */
+/* --- Portal structures --- */
+
+#define BM_RCR_SIZE		8
+
+struct bm_rcr {
+	struct bm_rcr_entry *ring, *cursor;
+	u8 ci, available, ithresh, vbit;
+#ifdef CONFIG_FSL_BMAN_CHECKING
+	u32 busy;
+	enum bm_rcr_pmode pmode;
+	enum bm_rcr_cmode cmode;
+#endif
+};
+
+struct bm_mc {
+	struct bm_mc_command *cr;
+	struct bm_mc_result *rr;
+	u8 rridx, vbit;
+#ifdef CONFIG_FSL_BMAN_CHECKING
+	enum {
+		/* Can only be _mc_start()ed */
+		mc_idle,
+		/* Can only be _mc_commit()ed or _mc_abort()ed */
+		mc_user,
+		/* Can only be _mc_retry()ed */
+		mc_hw
+	} state;
+#endif
+};
+
+struct bm_portal {
+	struct bm_addr addr;
+	struct bm_rcr rcr;
+	struct bm_mc mc;
+	struct bm_portal_config config;
+} ____cacheline_aligned;
+
+
+/* --------------- */
+/* --- RCR API --- */
+
+/* Bit-wise logic to wrap a ring pointer by clearing the "carry bit" */
+#define RCR_CARRYCLEAR(p) \
+	(void *)((unsigned long)(p) & (~(unsigned long)(BM_RCR_SIZE << 6)))
+
+/* Bit-wise logic to convert a ring pointer to a ring index */
+static inline u8 RCR_PTR2IDX(struct bm_rcr_entry *e)
+{
+	return ((u32)e >> 6) & (BM_RCR_SIZE - 1);
+}
+
+/* Increment the 'cursor' ring pointer, taking 'vbit' into account */
+static inline void RCR_INC(struct bm_rcr *rcr)
+{
+	/* NB: this is odd-looking, but experiments show that it generates
+	 * fast code with essentially no branching overheads. We increment to
+	 * the next RCR pointer and handle overflow and 'vbit'. */
+	struct bm_rcr_entry *partial = rcr->cursor + 1;
+	rcr->cursor = RCR_CARRYCLEAR(partial);
+	if (partial != rcr->cursor)
+		rcr->vbit ^= BM_RCR_VERB_VBIT;
+}
+
+static inline int bm_rcr_init(struct bm_portal *portal, enum bm_rcr_pmode pmode,
+		__maybe_unused enum bm_rcr_cmode cmode)
+{
+	/* This use of 'register', as well as all other occurances, is because
+	 * it has been observed to generate much faster code with gcc than is
+	 * otherwise the case. */
+	register struct bm_rcr *rcr = &portal->rcr;
+	u32 cfg;
+	u8 pi;
+
+	rcr->ring = ptr_OR(portal->addr.addr_ce, CL_RCR);
+	rcr->ci = bm_in(RCR_CI_CINH) & (BM_RCR_SIZE - 1);
+	pi = bm_in(RCR_PI_CINH) & (BM_RCR_SIZE - 1);
+	rcr->cursor = rcr->ring + pi;
+	rcr->vbit = (bm_in(RCR_PI_CINH) & BM_RCR_SIZE) ?  BM_RCR_VERB_VBIT : 0;
+	rcr->available = BM_RCR_SIZE - 1 - cyc_diff(BM_RCR_SIZE, rcr->ci, pi);
+	rcr->ithresh = bm_in(RCR_ITR);
+#ifdef CONFIG_FSL_BMAN_CHECKING
+	rcr->busy = 0;
+	rcr->pmode = pmode;
+	rcr->cmode = cmode;
+#endif
+	cfg = (bm_in(CFG) & 0xffffffe0) | (pmode & 0x3); /* BCSP_CFG::RPM */
+	bm_out(CFG, cfg);
+	return 0;
+}
+
+static inline void bm_rcr_finish(struct bm_portal *portal)
+{
+	register struct bm_rcr *rcr = &portal->rcr;
+	u8 pi = bm_in(RCR_PI_CINH) & (BM_RCR_SIZE - 1);
+	u8 ci = bm_in(RCR_CI_CINH) & (BM_RCR_SIZE - 1);
+	BM_ASSERT(!rcr->busy);
+	if (pi != RCR_PTR2IDX(rcr->cursor))
+		pr_crit("losing uncommited RCR entries\n");
+	if (ci != rcr->ci)
+		pr_crit("missing existing RCR completions\n");
+	if (rcr->ci != RCR_PTR2IDX(rcr->cursor))
+		pr_crit("RCR destroyed unquiesced\n");
+}
+
+static inline struct bm_rcr_entry *bm_rcr_start(struct bm_portal *portal)
+{
+	register struct bm_rcr *rcr = &portal->rcr;
+	BM_ASSERT(!rcr->busy);
+	if (!rcr->available)
+		return NULL;
+#ifdef CONFIG_FSL_BMAN_CHECKING
+	rcr->busy = 1;
+#endif
+	dcbzl(rcr->cursor);
+	return rcr->cursor;
+}
+
+static inline void bm_rcr_abort(struct bm_portal *portal)
+{
+	__maybe_unused register struct bm_rcr *rcr = &portal->rcr;
+	BM_ASSERT(rcr->busy);
+#ifdef CONFIG_FSL_BMAN_CHECKING
+	rcr->busy = 0;
+#endif
+}
+
+static inline struct bm_rcr_entry *bm_rcr_pend_and_next(
+					struct bm_portal *portal, u8 myverb)
+{
+	register struct bm_rcr *rcr = &portal->rcr;
+	BM_ASSERT(rcr->busy);
+	BM_ASSERT(rcr->pmode != bm_rcr_pvb);
+	if (rcr->available == 1)
+		return NULL;
+	rcr->cursor->__dont_write_directly__verb = myverb | rcr->vbit;
+	dcbf(rcr->cursor);
+	RCR_INC(rcr);
+	rcr->available--;
+	dcbzl(rcr->cursor);
+	return rcr->cursor;
+}
+
+static inline void bm_rcr_pci_commit(struct bm_portal *portal, u8 myverb)
+{
+	register struct bm_rcr *rcr = &portal->rcr;
+	BM_ASSERT(rcr->busy);
+	BM_ASSERT(rcr->pmode == bm_rcr_pci);
+	rcr->cursor->__dont_write_directly__verb = myverb | rcr->vbit;
+	RCR_INC(rcr);
+	rcr->available--;
+	hwsync();
+	bm_out(RCR_PI_CINH, RCR_PTR2IDX(rcr->cursor));
+#ifdef CONFIG_FSL_BMAN_CHECKING
+	rcr->busy = 0;
+#endif
+}
+
+static inline void bm_rcr_pce_prefetch(struct bm_portal *portal)
+{
+	__maybe_unused register struct bm_rcr *rcr = &portal->rcr;
+	BM_ASSERT(rcr->pmode == bm_rcr_pce);
+	bm_cl_invalidate(RCR_PI);
+	bm_cl_touch_rw(RCR_PI);
+}
+
+static inline void bm_rcr_pce_commit(struct bm_portal *portal, u8 myverb)
+{
+	register struct bm_rcr *rcr = &portal->rcr;
+	BM_ASSERT(rcr->busy);
+	BM_ASSERT(rcr->pmode == bm_rcr_pce);
+	rcr->cursor->__dont_write_directly__verb = myverb | rcr->vbit;
+	RCR_INC(rcr);
+	rcr->available--;
+	lwsync();
+	bm_cl_out(RCR_PI, RCR_PTR2IDX(rcr->cursor));
+#ifdef CONFIG_FSL_BMAN_CHECKING
+	rcr->busy = 0;
+#endif
+}
+
+static inline void bm_rcr_pvb_commit(struct bm_portal *portal, u8 myverb)
+{
+	register struct bm_rcr *rcr = &portal->rcr;
+	struct bm_rcr_entry *rcursor;
+	BM_ASSERT(rcr->busy);
+	BM_ASSERT(rcr->pmode == bm_rcr_pvb);
+	lwsync();
+	rcursor = rcr->cursor;
+	rcursor->__dont_write_directly__verb = myverb | rcr->vbit;
+	dcbf(rcursor);
+	RCR_INC(rcr);
+	rcr->available--;
+#ifdef CONFIG_FSL_BMAN_CHECKING
+	rcr->busy = 0;
+#endif
+}
+
+static inline u8 bm_rcr_cci_update(struct bm_portal *portal)
+{
+	register struct bm_rcr *rcr = &portal->rcr;
+	u8 diff, old_ci = rcr->ci;
+	BM_ASSERT(rcr->cmode == bm_rcr_cci);
+	rcr->ci = bm_in(RCR_CI_CINH) & (BM_RCR_SIZE - 1);
+	diff = cyc_diff(BM_RCR_SIZE, old_ci, rcr->ci);
+	rcr->available += diff;
+	return diff;
+}
+
+static inline void bm_rcr_cce_prefetch(struct bm_portal *portal)
+{
+	__maybe_unused register struct bm_rcr *rcr = &portal->rcr;
+	BM_ASSERT(rcr->cmode == bm_rcr_cce);
+	bm_cl_touch_ro(RCR_CI);
+}
+
+static inline u8 bm_rcr_cce_update(struct bm_portal *portal)
+{
+	register struct bm_rcr *rcr = &portal->rcr;
+	u8 diff, old_ci = rcr->ci;
+	BM_ASSERT(rcr->cmode == bm_rcr_cce);
+	rcr->ci = bm_cl_in(RCR_CI) & (BM_RCR_SIZE - 1);
+	bm_cl_invalidate(RCR_CI);
+	diff = cyc_diff(BM_RCR_SIZE, old_ci, rcr->ci);
+	rcr->available += diff;
+	return diff;
+}
+
+static inline u8 bm_rcr_get_ithresh(struct bm_portal *portal)
+{
+	register struct bm_rcr *rcr = &portal->rcr;
+	return rcr->ithresh;
+}
+
+static inline void bm_rcr_set_ithresh(struct bm_portal *portal, u8 ithresh)
+{
+	register struct bm_rcr *rcr = &portal->rcr;
+	rcr->ithresh = ithresh;
+	bm_out(RCR_ITR, ithresh);
+}
+
+static inline u8 bm_rcr_get_avail(struct bm_portal *portal)
+{
+	register struct bm_rcr *rcr = &portal->rcr;
+	return rcr->available;
+}
+
+static inline u8 bm_rcr_get_fill(struct bm_portal *portal)
+{
+	register struct bm_rcr *rcr = &portal->rcr;
+	return BM_RCR_SIZE - 1 - rcr->available;
+}
+
+
+/* ------------------------------ */
+/* --- Management command API --- */
+
+static inline int bm_mc_init(struct bm_portal *portal)
+{
+	register struct bm_mc *mc = &portal->mc;
+	mc->cr = ptr_OR(portal->addr.addr_ce, CL_CR);
+	mc->rr = ptr_OR(portal->addr.addr_ce, CL_RR0);
+	mc->rridx = (readb(&mc->cr->__dont_write_directly__verb) &
+			BM_MCC_VERB_VBIT) ?  0 : 1;
+	mc->vbit = mc->rridx ? BM_MCC_VERB_VBIT : 0;
+#ifdef CONFIG_FSL_BMAN_CHECKING
+	mc->state = mc_idle;
+#endif
+	return 0;
+}
+
+static inline void bm_mc_finish(struct bm_portal *portal)
+{
+	__maybe_unused register struct bm_mc *mc = &portal->mc;
+	BM_ASSERT(mc->state == mc_idle);
+#ifdef CONFIG_FSL_BMAN_CHECKING
+	if (mc->state != mc_idle)
+		pr_crit("Losing incomplete MC command\n");
+#endif
+}
+
+static inline struct bm_mc_command *bm_mc_start(struct bm_portal *portal)
+{
+	register struct bm_mc *mc = &portal->mc;
+	BM_ASSERT(mc->state == mc_idle);
+#ifdef CONFIG_FSL_BMAN_CHECKING
+	mc->state = mc_user;
+#endif
+	dcbzl(mc->cr);
+	return mc->cr;
+}
+
+static inline void bm_mc_abort(struct bm_portal *portal)
+{
+	__maybe_unused register struct bm_mc *mc = &portal->mc;
+	BM_ASSERT(mc->state == mc_user);
+#ifdef CONFIG_FSL_BMAN_CHECKING
+	mc->state = mc_idle;
+#endif
+}
+
+static inline void bm_mc_commit(struct bm_portal *portal, u8 myverb)
+{
+	register struct bm_mc *mc = &portal->mc;
+	BM_ASSERT(mc->state == mc_user);
+	dcbi(mc->rr + mc->rridx);
+	lwsync();
+	mc->cr->__dont_write_directly__verb = myverb | mc->vbit;
+	dcbf(mc->cr);
+	dcbt_ro(mc->rr + mc->rridx);
+#ifdef CONFIG_FSL_BMAN_CHECKING
+	mc->state = mc_hw;
+#endif
+}
+
+static inline struct bm_mc_result *bm_mc_result(struct bm_portal *portal)
+{
+	register struct bm_mc *mc = &portal->mc;
+	struct bm_mc_result *rr = mc->rr + mc->rridx;
+	BM_ASSERT(mc->state == mc_hw);
+	/* The inactive response register's verb byte always returns zero until
+	 * its command is submitted and completed. This includes the valid-bit,
+	 * in case you were wondering... */
+	if (!readb(&rr->verb)) {
+		dcbi(rr);
+		dcbt_ro(rr);
+		return NULL;
+	}
+	mc->rridx ^= 1;
+	mc->vbit ^= BM_MCC_VERB_VBIT;
+#ifdef CONFIG_FSL_BMAN_CHECKING
+	mc->state = mc_idle;
+#endif
+	return rr;
+}
+
+
+/* ------------------------------------- */
+/* --- Portal interrupt register API --- */
+
+static inline int bm_isr_init(__always_unused struct bm_portal *portal)
+{
+	return 0;
+}
+
+static inline void bm_isr_finish(__always_unused struct bm_portal *portal)
+{
+}
+
+#define SCN_REG(bpid) REG_SCN((bpid) / 32)
+#define SCN_BIT(bpid) (0x80000000 >> (bpid & 31))
+static inline void bm_isr_bscn_mask(struct bm_portal *portal, u8 bpid,
+					int enable)
+{
+	u32 val;
+	BM_ASSERT(bpid < 64);
+	/* REG_SCN for bpid=0..31, REG_SCN+4 for bpid=32..63 */
+	val = __bm_in(&portal->addr, SCN_REG(bpid));
+	if (enable)
+		val |= SCN_BIT(bpid);
+	else
+		val &= ~SCN_BIT(bpid);
+	__bm_out(&portal->addr, SCN_REG(bpid), val);
+}
+
+static inline u32 __bm_isr_read(struct bm_portal *portal, enum bm_isr_reg n)
+{
+	return __bm_in(&portal->addr, REG_ISR + (n << 2));
+}
+
+static inline void __bm_isr_write(struct bm_portal *portal, enum bm_isr_reg n,
+					u32 val)
+{
+	__bm_out(&portal->addr, REG_ISR + (n << 2), val);
+}
+
diff --git a/drivers/hwalloc/bman_private.h b/drivers/hwalloc/bman_private.h
index e783d60..bf36060 100644
--- a/drivers/hwalloc/bman_private.h
+++ b/drivers/hwalloc/bman_private.h
@@ -1,4 +1,4 @@
-/* Copyright (c) 2008, 2009 Freescale Semiconductor, Inc.
+/* Copyright (c) 2008-2010 Freescale Semiconductor, Inc.
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -38,63 +38,39 @@ struct bm_addr {
 	void __iomem *addr_ci;	/* cache-inhibited */
 };
 
-/* RCR state */
-struct bm_rcr {
-	struct bm_rcr_entry *ring, *cursor;
-	u8 ci, available, ithresh, vbit;
-#ifdef CONFIG_FSL_BMAN_CHECKING
-	u32 busy;
-	enum bm_rcr_pmode pmode;
-	enum bm_rcr_cmode cmode;
-#endif
+/* used by CCSR and portal interrupt code */
+enum bm_isr_reg {
+	bm_isr_status = 0,
+	bm_isr_enable = 1,
+	bm_isr_disable = 2,
+	bm_isr_inhibit = 3
 };
 
-/* MC state */
-struct bm_mc {
-	struct bm_mc_command *cr;
-	struct bm_mc_result *rr;
-	u8 rridx, vbit;
-#ifdef CONFIG_FSL_BMAN_CHECKING
-	enum {
-		/* Can only be _mc_start()ed */
-		mc_idle,
-		/* Can only be _mc_commit()ed or _mc_abort()ed */
-		mc_user,
-		/* Can only be _mc_retry()ed */
-		mc_hw
-	} state;
-#endif
+struct bm_portal_config {
+	struct bm_portal *portal;
+	struct bm_addr addr;
+	/* This is used for any "core-affine" portals, ie. default portals
+	 * associated to the corresponding cpu. -1 implies that there is no core
+	 * affinity configured. */
+	int cpu;
+	/* portal interrupt line */
+	int irq;
+	/* These are the buffer pool IDs that may be used via this portal. NB,
+	 * this is only enforced in the high-level API. Also, BSCN depletion
+	 * state changes will only be unmasked as/when pool objects are created
+	 * with depletion callbacks - the mask is the superset. */
+	struct bman_depletion mask;
 };
 
-/********************/
-/* Portal structure */
-/********************/
+#ifdef CONFIG_FSL_BMAN_CONFIG
+/* Hooks from bman_driver.c to bman_config.c */
+int bman_init_error_int(struct device_node *node);
+#endif
 
-struct bm_portal {
-	struct bm_addr addr;
-	struct bm_rcr rcr;
-	struct bm_mc mc;
-	struct bm_portal_config config;
-} ____cacheline_aligned;
-
-/* RCR/MC/ISR code uses this as a locked mechanism to bind/unbind to
- * bm_portal::config::bound. */
-int __bm_portal_bind(struct bm_portal *portal, u8 iface);
-void __bm_portal_unbind(struct bm_portal *portal, u8 iface);
-
-/* Hooks between qman_driver.c and qman_high.c */
-extern DEFINE_PER_CPU(struct bman_portal *, bman_affine_portal);
-static inline struct bman_portal *get_affine_portal(void)
-{
-	return get_cpu_var(bman_affine_portal);
-}
-static inline void put_affine_portal(void)
-{
-	put_cpu_var(bman_affine_portal);
-}
-struct bman_portal *bman_create_portal(struct bm_portal *portal,
-					const struct bman_depletion *pools);
-void bman_destroy_portal(struct bman_portal *p);
+int bman_have_affine_portal(void);
+int bman_create_affine_portal(const struct bm_portal_config *config,
+				u32 irq_sources);
+void bman_destroy_affine_portal(void);
 
 /* Pool logic in the portal driver, during initialisation, needs to know if
  * there's access to CCSR or not (if not, it'll cripple the pool allocator). */
@@ -125,3 +101,52 @@ int bman_have_ccsr(void);
 #define BMAN_STOCKPILE_SZ   16u /* number of bufs in per-pool cache */
 #define BMAN_STOCKPILE_LOW  2u  /* when fill is <= this, acquire from hw */
 #define BMAN_STOCKPILE_HIGH 14u /* when fill is >= this, release to hw */
+
+/*************************************************/
+/*   BMan s/w corenet portal, low-level i/face   */
+/*************************************************/
+
+/* Obtain the number of portals available */
+u8 bm_portal_num(void);
+
+/* Obtain a portal handle */
+const struct bm_portal_config *bm_portal_config(u8 idx);
+
+/* Used by all portal interrupt registers except 'inhibit'. NB, some of these
+ * definitions are exported for use by the bman_irqsource_***() APIs, so are
+ * commented-out here. */
+#if 0
+#define BM_PIRQ_RCRI	0x00000002	/* RCR Ring (below threshold) */
+#define BM_PIRQ_BSCN	0x00000001	/* Buffer depletion State Change */
+#endif
+/* This mask contains all the "irqsource" bits visible to API users */
+#define BM_PIRQ_VISIBLE	(BM_PIRQ_RCRI | BM_PIRQ_BSCN)
+
+/* These are bm_<reg>_<verb>(). So for example, bm_disable_write() means "write
+ * the disable register" rather than "disable the ability to write". */
+#define bm_isr_status_read(bm)		__bm_isr_read(bm, bm_isr_status)
+#define bm_isr_status_clear(bm, m)	__bm_isr_write(bm, bm_isr_status, m)
+#define bm_isr_enable_read(bm)		__bm_isr_read(bm, bm_isr_enable)
+#define bm_isr_enable_write(bm, v)	__bm_isr_write(bm, bm_isr_enable, v)
+#define bm_isr_disable_read(bm)		__bm_isr_read(bm, bm_isr_disable)
+#define bm_isr_disable_write(bm, v)	__bm_isr_write(bm, bm_isr_disable, v)
+#define bm_isr_inhibit(bm)		__bm_isr_write(bm, bm_isr_inhibit, 1)
+#define bm_isr_uninhibit(bm)		__bm_isr_write(bm, bm_isr_inhibit, 0)
+
+#ifdef CONFIG_FSL_BMAN_CONFIG
+
+/* Allocate/release an unreserved buffer pool id */
+int bm_pool_new(u32 *bpid);
+void bm_pool_free(u32 bpid);
+
+/* Set depletion thresholds associated with a buffer pool. Requires that the
+ * operating system have access to Bman CCSR (ie. compiled in support and
+ * run-time access courtesy of the device-tree). */
+int bm_pool_set(u32 bpid, const u32 *thresholds);
+#define BM_POOL_THRESH_SW_ENTER 0
+#define BM_POOL_THRESH_SW_EXIT  1
+#define BM_POOL_THRESH_HW_ENTER 2
+#define BM_POOL_THRESH_HW_EXIT  3
+
+#endif /* CONFIG_FSL_BMAN_CONFIG */
+
diff --git a/drivers/hwalloc/bman_sys.h b/drivers/hwalloc/bman_sys.h
index 3b6e05e..38d6069 100644
--- a/drivers/hwalloc/bman_sys.h
+++ b/drivers/hwalloc/bman_sys.h
@@ -1,4 +1,4 @@
-/* Copyright (c) 2008, 2009 Freescale Semiconductor, Inc.
+/* Copyright (c) 2008-2010 Freescale Semiconductor, Inc.
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -50,28 +50,53 @@
 #include <linux/miscdevice.h>
 #include <linux/uaccess.h>
 #include <asm/kexec.h>
+#include <linux/debugfs.h>
+#include <linux/seq_file.h>
+#include <asm/smp.h>
 
-/* CONFIG_FSL_BMAN_HAVE_POLL is defined via Kconfig, because the API header is
- * conditional upon it. CONFIG_FSL_BMAN_CHECKING is also defined via Kconfig,
- * but because it's a knob for users. Everything else affects only
- * implementation (not interface), so we define it here, internally. */
+/*************************************/
+/* Generic definitions for Qman+Bman */
+/*************************************/
 
-/* do slow-path processing via IRQ */
-#define CONFIG_FSL_BMAN_PORTAL_FLAG_IRQ_SLOW
+/* When copying aligned words or shorts, try to avoid memcpy() */
+#define CONFIG_TRY_BETTER_MEMCPY
 
-/* do fast-path processing via IRQ */
-#define CONFIG_FSL_BMAN_PORTAL_FLAG_IRQ_FAST
+/* support blocking (so, WAIT flags will be #define'd) */
+#define CONFIG_FSL_DPA_CAN_WAIT
 
-/* portals do not initialise in recovery mode */
-#undef CONFIG_FSL_BMAN_PORTAL_FLAG_RECOVER
+#ifdef CONFIG_FSL_DPA_CAN_WAIT
+/* if we can "WAIT" - can we "WAIT_SYNC" too? */
+#define CONFIG_FSL_DPA_CAN_WAIT_SYNC
+#endif
+
+/* permit portal logic in interrupts (compiles in irq_save/restore) */
+#define CONFIG_FSL_DPA_IRQ_SAFETY
 
-#if defined(CONFIG_FSL_BMAN_PORTAL_FLAG_IRQ_SLOW) || \
-		defined(CONFIG_FSL_BMAN_PORTAL_FLAG_IRQ_FAST)
+/*****************************/
+/* Bman-specific definitions */
+/*****************************/
+
+/* CONFIG_FSL_BMAN_CHECKING is defined via Kconfig, because it's a knob for
+ * users. Everything else affects only implementation (not interface), so we
+ * define it here, internally. */
+
+#ifdef CONFIG_FSL_DPA_IRQ_SAFETY
+/* support IRQs, whether or not they're used at run-time */
 #define CONFIG_FSL_BMAN_HAVE_IRQ
-#else
-#undef CONFIG_FSL_BMAN_HAVE_IRQ
 #endif
 
+#ifdef CONFIG_FSL_BMAN_HAVE_IRQ
+/* do slow-path processing via IRQ */
+#define CONFIG_FSL_BMAN_PIRQ_SLOW
+#endif
+
+/* portals do not initialise in recovery mode */
+#undef CONFIG_FSL_BMAN_PORTAL_FLAG_RECOVER
+
+/***********************/
+/* Misc inline assists */
+/***********************/
+
 /* TODO: NB, we currently assume that hwsync() and lwsync() imply compiler
  * barriers and that dcb*() won't fall victim to compiler or execution
  * reordering with respect to other code/instructions that manipulate the same
@@ -102,6 +127,17 @@
 	} while(0)
 #define dcbi(p) dcbf(p)
 
+static inline u64 mfatb(void)
+{
+	u32 hi, lo, chk;
+	do {
+		hi = mfspr(SPRN_ATBU);
+		lo = mfspr(SPRN_ATBL);
+		chk = mfspr(SPRN_ATBU);
+	} while (unlikely(hi != chk));
+	return ((u64)hi << 32) | (u64)lo;
+}
+
 #ifdef CONFIG_FSL_BMAN_CHECKING
 #define BM_ASSERT(x) \
 	do { \
@@ -115,3 +151,41 @@
 #else
 #define BM_ASSERT(x)
 #endif
+
+/* memcpy() stuff - when you know alignments in advance */
+#ifdef CONFIG_TRY_BETTER_MEMCPY
+static inline void copy_words(void *dest, const void *src, size_t sz)
+{
+	u32 *__dest = dest;
+	const u32 *__src = src;
+	size_t __sz = sz >> 2;
+	BUG_ON((unsigned long)dest & 0x3);
+	BUG_ON((unsigned long)src & 0x3);
+	BUG_ON(sz & 0x3);
+	while (__sz--)
+		*(__dest++) = *(__src++);
+}
+static inline void copy_shorts(void *dest, const void *src, size_t sz)
+{
+	u16 *__dest = dest;
+	const u16 *__src = src;
+	size_t __sz = sz >> 1;
+	BUG_ON((unsigned long)dest & 0x1);
+	BUG_ON((unsigned long)src & 0x1);
+	BUG_ON(sz & 0x1);
+	while (__sz--)
+		*(__dest++) = *(__src++);
+}
+static inline void copy_bytes(void *dest, const void *src, size_t sz)
+{
+	u8 *__dest = dest;
+	const u8 *__src = src;
+	while (sz--)
+		*(__dest++) = *(__src++);
+}
+#else
+#define copy_words memcpy
+#define copy_shorts memcpy
+#define copy_bytes memcpy
+#endif
+
diff --git a/drivers/hwalloc/bman_test_high.c b/drivers/hwalloc/bman_test_high.c
index 3f418b5..78f338b 100644
--- a/drivers/hwalloc/bman_test_high.c
+++ b/drivers/hwalloc/bman_test_high.c
@@ -1,4 +1,4 @@
-/* Copyright (c) 2008, 2009 Freescale Semiconductor, Inc.
+/* Copyright (c) 2008-2010 Freescale Semiconductor, Inc.
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -63,22 +63,16 @@ static void depletion_cb(struct bman_portal *, struct bman_pool *, void *, int);
 static void bufs_init(void)
 {
 	int i;
-	for (i = 0; i < NUM_BUFS; i++) {
-		bufs_in[i].hi = 0xfedc - i;
-		bufs_in[i].lo = 0xcccccccc + (0x11111111 * i);
-	}
+	for (i = 0; i < NUM_BUFS; i++)
+		bm_buffer_set64(&bufs_in[i], 0xfedc01234567LLU * i);
 	bufs_received = 0;
 }
 
 static inline int bufs_cmp(const struct bm_buffer *a, const struct bm_buffer *b)
 {
-	if (a->hi < b->hi)
-		return -1;
-	if (a->hi > b->hi)
-		return 1;
-	if (a->lo < b->lo)
+	if (bm_buffer_get64(a) < bm_buffer_get64(b))
 		return -1;
-	if (a->lo > b->lo)
+	if (bm_buffer_get64(a) > bm_buffer_get64(b))
 		return 1;
 	return 0;
 }
diff --git a/include/linux/fsl_bman.h b/include/linux/fsl_bman.h
index e139644..fba1cec 100644
--- a/include/linux/fsl_bman.h
+++ b/include/linux/fsl_bman.h
@@ -39,29 +39,9 @@ extern "C" {
 
 /* Last updated for v00.79 of the BG */
 
-/*************************************************/
-/*   BMan s/w corenet portal, low-level i/face   */
-/*************************************************/
-
-/* Portal constants */
-#define BM_RCR_SIZE		8
-
-/* Hardware constants */
-enum bm_isr_reg {
-	bm_isr_status = 0,
-	bm_isr_enable = 1,
-	bm_isr_disable = 2,
-	bm_isr_inhibit = 3
-};
-
-/* Represents s/w corenet portal mapped data structures */
-struct bm_rcr_entry;	/* RCR (Release Command Ring) entries */
-struct bm_mc_command;	/* MC (Management Command) command */
-struct bm_mc_result;	/* MC result */
-
-/* This type represents a s/w corenet portal space, and is used for creating the
- * portal objects within it (RCR, etc) */
-struct bm_portal;
+/* Portal processing (interrupt) sources */
+#define BM_PIRQ_RCRI	0x00000002	/* RCR Ring (below threshold) */
+#define BM_PIRQ_BSCN	0x00000001	/* Buffer depletion State Change */
 
 /* This wrapper represents a bit-array for the depletion state of the 64 Bman
  * buffer pools. */
@@ -96,184 +76,47 @@ static inline void bman_depletion_unset(struct bman_depletion *c, u8 bpid)
 	c->__state[__bmdep_word(bpid)] &= ~__bmdep_bit(bpid);
 }
 
-/* When iterating the available portals, this is the exposed config structure */
-struct bm_portal_config {
-	/* This is used for any "core-affine" portals, ie. default portals
-	 * associated to the corresponding cpu. -1 implies that there is no core
-	 * affinity configured. */
-	int cpu;
-	/* portal interrupt line */
-	int irq;
-	/* These are the buffer pool IDs that may be used via this portal. NB,
-	 * this is only enforced in the high-level API. Also, BSCN depletion
-	 * state changes will only be unmasked as/when pool objects are created
-	 * with depletion callbacks - the mask is the superset. */
-	struct bman_depletion mask;
-	/* which portal sub-interfaces are already bound (ie. "in use") */
-	u8 bound;
-};
-/* bm_portal_config::bound uses these bit masks */
-#define BM_BIND_RCR	0x01
-#define BM_BIND_MC	0x02
-#define BM_BIND_ISR	0x04
-
-/* Portal modes.
- *   Enum types;
- *     pmode == production mode
- *     cmode == consumption mode,
- *   Enum values use 3 letter codes. First letter matches the portal mode,
- *   remaining two letters indicate;
- *     ci == cache-inhibited portal register
- *     ce == cache-enabled portal register
- *     vb == in-band valid-bit (cache-enabled)
- */
-enum bm_rcr_pmode {		/* matches BCSP_CFG::RPM */
-	bm_rcr_pci = 0,		/* PI index, cache-inhibited */
-	bm_rcr_pce = 1,		/* PI index, cache-enabled */
-	bm_rcr_pvb = 2		/* valid-bit */
-};
-enum bm_rcr_cmode {		/* s/w-only */
-	bm_rcr_cci,		/* CI index, cache-inhibited */
-	bm_rcr_cce		/* CI index, cache-enabled */
-};
-
-
-/* ------------------------------ */
-/* --- Portal enumeration API --- */
-
-/* Obtain the number of portals available */
-u8 bm_portal_num(void);
-
-/* Obtain a portal handle */
-struct bm_portal *bm_portal_get(u8 idx);
-const struct bm_portal_config *bm_portal_config(const struct bm_portal *portal);
-
-
-/* ------------------------------ */
-/* --- Buffer pool allocation --- */
-
-#ifdef CONFIG_FSL_BMAN_CONFIG
-
-/* Allocate/release an unreserved buffer pool id */
-int bm_pool_new(u32 *bpid);
-void bm_pool_free(u32 bpid);
-
-/* Set depletion thresholds associated with a buffer pool. Requires that the
- * operating system have access to Bman CCSR (ie. compiled in support and
- * run-time access courtesy of the device-tree). */
-int bm_pool_set(u32 bpid, const u32 *thresholds);
-#define BM_POOL_THRESH_SW_ENTER 0
-#define BM_POOL_THRESH_SW_EXIT  1
-#define BM_POOL_THRESH_HW_ENTER 2
-#define BM_POOL_THRESH_HW_EXIT  3
-
-#endif /* CONFIG_FSL_BMAN_CONFIG */
-
-
-/* --------------- */
-/* --- RCR API --- */
-
-/* Create/destroy */
-int bm_rcr_init(struct bm_portal *portal, enum bm_rcr_pmode pmode,
-		enum bm_rcr_cmode cmode);
-void bm_rcr_finish(struct bm_portal *portal);
-
-/* Start/abort RCR entry */
-struct bm_rcr_entry *bm_rcr_start(struct bm_portal *portal);
-void bm_rcr_abort(struct bm_portal *portal);
-
-/* For PI modes only. This presumes a started but uncommited RCR entry. If
- * there's no more room in the RCR, this function returns NULL. Otherwise it
- * returns the next RCR entry and increments an internal PI counter without
- * flushing it to h/w. */
-struct bm_rcr_entry *bm_rcr_pend_and_next(struct bm_portal *portal, u8 myverb);
-
-/* Commit RCR entries, including pending ones (aka "write PI") */
-void bm_rcr_pci_commit(struct bm_portal *portal, u8 myverb);
-void bm_rcr_pce_prefetch(struct bm_portal *portal);
-void bm_rcr_pce_commit(struct bm_portal *portal, u8 myverb);
-void bm_rcr_pvb_commit(struct bm_portal *portal, u8 myverb);
-
-/* Track h/w consumption. Returns non-zero if h/w had consumed previously
- * unconsumed RCR entries. */
-u8 bm_rcr_cci_update(struct bm_portal *portal);
-void bm_rcr_cce_prefetch(struct bm_portal *portal);
-u8 bm_rcr_cce_update(struct bm_portal *portal);
-/* Returns the number of available RCR entries */
-u8 bm_rcr_get_avail(struct bm_portal *portal);
-/* Returns the number of unconsumed RCR entries */
-u8 bm_rcr_get_fill(struct bm_portal *portal);
-
-/* Read/write the RCR interrupt threshold */
-u8 bm_rcr_get_ithresh(struct bm_portal *portal);
-void bm_rcr_set_ithresh(struct bm_portal *portal, u8 ithresh);
-
-
-/* ------------------------------ */
-/* --- Management command API --- */
-
-/* Create/destroy */
-int bm_mc_init(struct bm_portal *portal);
-void bm_mc_finish(struct bm_portal *portal);
-
-/* Start/abort mgmt command */
-struct bm_mc_command *bm_mc_start(struct bm_portal *portal);
-void bm_mc_abort(struct bm_portal *portal);
-
-/* Writes 'verb' with appropriate 'vbit'. Invalidates and pre-fetches the
- * response. */
-void bm_mc_commit(struct bm_portal *portal, u8 myverb);
-
-/* Poll for result. If NULL, invalidates and prefetches for the next call. */
-struct bm_mc_result *bm_mc_result(struct bm_portal *portal);
-
-
-/* ------------------------------------- */
-/* --- Portal interrupt register API --- */
-
-/* For a quick explanation of the Bman interrupt model, see the comments in the
- * equivalent section of the qman_portal.h header.
- */
-
-/* Create/destroy */
-int bm_isr_init(struct bm_portal *portal);
-void bm_isr_finish(struct bm_portal *portal);
-
-/* BSCN masking is a per-portal configuration */
-void bm_isr_bscn_mask(struct bm_portal *portal, u8 bpid, int enable);
-
-/* Used by all portal interrupt registers except 'inhibit' */
-#define BM_PIRQ_RCRI	0x00000002	/* RCR Ring (below threshold) */
-#define BM_PIRQ_BSCN	0x00000001	/* Buffer depletion State Change */
-
-/* These are bm_<reg>_<verb>(). So for example, bm_disable_write() means "write
- * the disable register" rather than "disable the ability to write". */
-#define bm_isr_status_read(bm)		__bm_isr_read(bm, bm_isr_status)
-#define bm_isr_status_clear(bm, m)	__bm_isr_write(bm, bm_isr_status, m)
-#define bm_isr_enable_read(bm)		__bm_isr_read(bm, bm_isr_enable)
-#define bm_isr_enable_write(bm, v)	__bm_isr_write(bm, bm_isr_enable, v)
-#define bm_isr_disable_read(bm)		__bm_isr_read(bm, bm_isr_disable)
-#define bm_isr_disable_write(bm, v)	__bm_isr_write(bm, bm_isr_disable, v)
-#define bm_isr_inhibit(bm)		__bm_isr_write(bm, bm_isr_inhibit, 1)
-#define bm_isr_uninhibit(bm)		__bm_isr_write(bm, bm_isr_inhibit, 0)
-
-/* Don't use these, use the wrappers above*/
-u32 __bm_isr_read(struct bm_portal *portal, enum bm_isr_reg n);
-void __bm_isr_write(struct bm_portal *portal, enum bm_isr_reg n, u32 val);
-
-
 /* ------------------------------------------------------- */
 /* --- Bman data structures (and associated constants) --- */
 
+/* Represents s/w corenet portal mapped data structures */
+struct bm_rcr_entry;	/* RCR (Release Command Ring) entries */
+struct bm_mc_command;	/* MC (Management Command) command */
+struct bm_mc_result;	/* MC result */
+
 /* Code-reduction, define a wrapper for 48-bit buffers. In cases where a buffer
  * pool id specific to this buffer is needed (BM_RCR_VERB_CMD_BPID_MULTI,
  * BM_MCC_VERB_ACQUIRE), the 'bpid' field is used. */
 struct bm_buffer {
 	u8 __reserved1;
 	u8 bpid;
-	u16 hi;	/* High 16-bits of 48-bit address */
-	u32 lo;	/* Low 32-bits of 48-bit address */
+	u16 hi; /* High 16-bits of 48-bit address */
+	u32 lo; /* Low 32-bits of 48-bit address */
 } __packed;
+static inline u64 bm_buffer_get64(const struct bm_buffer *buf)
+{
+	return ((u64)buf->hi << 32) | (u64)buf->lo;
+}
+static inline dma_addr_t bm_buf_addr(const struct bm_buffer *buf)
+{
+#ifdef CONFIG_PHYS_ADDR_T_64BIT
+	return ((dma_addr_t)buf->hi << 32) | (dma_addr_t)buf->lo;
+#else
+	return (dma_addr_t)buf->lo;
+#endif
+}
+/* Macro, so we compile better if 'v' isn't always 64-bit */
+#define bm_buffer_set64(buf, v) \
+	do { \
+		struct bm_buffer *__buf931 = (buf); \
+		__buf931->hi = upper_32_bits(v); \
+		__buf931->lo = lower_32_bits(v); \
+	} while (0)
+#define BM_BUFFER_INIT64(v) \
+	(struct bm_buffer) { \
+		.hi = upper_32_bits(v), \
+		.lo = lower_32_bits(v) \
+	}
 
 /* See 1.5.3.5.4: "Release Command" */
 struct bm_rcr_entry {
@@ -298,11 +141,9 @@ struct bm_mcc_acquire {
 	u8 bpid;
 	u8 __reserved1[62];
 } __packed;
-
 struct bm_mcc_query {
 	u8 __reserved2[63];
 } __packed;
-
 struct bm_mc_command {
 	u8 __dont_write_directly__verb;
 	union {
@@ -318,6 +159,15 @@ struct bm_mc_command {
 
 /* See 1.5.3.3: "Acquire Reponse" */
 /* See 1.5.3.4: "Query Reponse" */
+struct bm_pool_state {
+	u8 __reserved1[32];
+	/* "availability state" and "depletion state" */
+	struct {
+		u8 __reserved1[8];
+		/* Access using bman_depletion_***() */
+		struct bman_depletion state;
+	} as, ds;
+};
 struct bm_mc_result {
 	union {
 		struct {
@@ -332,15 +182,7 @@ struct bm_mc_result {
 			};
 			struct bm_buffer bufs[8];
 		} acquire;
-		struct {
-			u8 __reserved1[32];
-			/* "availability state" and "depletion state" */
-			struct {
-				u8 __reserved1[8];
-				/* Access using bman_depletion_***() */
-				struct bman_depletion state;
-			} as, ds;
-		} query;
+		struct bm_pool_state query;
 	};
 } __packed;
 #define BM_MCR_VERB_VBIT		0x80
@@ -412,21 +254,61 @@ struct bman_pool_params {
 	/* Portal Management */
 	/* ----------------- */
 /**
- * bman_poll - Runs portal updates not triggered by interrupts
+ * bman_irqsource_get - return the portal work that is interrupt-driven
+ *
+ * Returns a bitmask of BM_PIRQ_**I processing sources that are currently
+ * enabled for interrupt handling on the current cpu's affine portal. These
+ * sources will trigger the portal interrupt and the interrupt handler (or a
+ * tasklet/bottom-half it defers to) will perform the corresponding processing
+ * work. The bman_poll_***() functions will only process sources that are not in
+ * this bitmask.
+ */
+u32 bman_irqsource_get(void);
+
+/**
+ * bman_irqsource_add - add processing sources to be interrupt-driven
+ * @bits: bitmask of BM_PIRQ_**I processing sources
+ *
+ * Adds processing sources that should be interrupt-driven, rather than
+ * processed via bman_poll().
+ */
+void bman_irqsource_add(u32 bits);
+
+/**
+ * bman_irqsource_remove - remove processing sources from being interrupt-driven
+ * @bits: bitmask of BM_PIRQ_**I processing sources
+ *
+ * Removes processing sources from being interrupt-driven, so that they will
+ * instead be processed via bman_poll().
+ */
+void bman_irqsource_remove(u32 bits);
+
+/**
+ * bman_affine_cpus - return a mask of cpus that have affine portals
+ */
+const cpumask_t *bman_affine_cpus(void);
+
+/**
+ * bman_poll - process anything that isn't interrupt-driven.
  *
  * Dispatcher logic on a cpu can use this to trigger any maintenance of the
- * affine portal. There are two classes of portal processing in question;
- * fast-path (which involves tracking release ring (RCR) consumption), and
- * slow-path (which involves RCR thresholds, pool depletion state changes, etc).
- * The driver is configured to use interrupts for either (a) all processing, (b)
- * only slow-path processing, or (c) no processing. This function does whatever
- * processing is not triggered by interrupts.
+ * affine portal. This function does whatever processing is not triggered by
+ * interrupts.
  */
-#ifdef CONFIG_FSL_BMAN_HAVE_POLL
 void bman_poll(void);
-#else
-#define bman_poll()	do { ; } while (0)
-#endif
+
+/**
+ * bman_rcr_is_empty - Determine if portal's RCR is empty
+ *
+ * For use in situations where a cpu-affine caller needs to determine when all
+ * releases for the local portal have been processed by Bman but can't use the
+ * BMAN_RELEASE_FLAG_WAIT_SYNC flag to do this from the final bman_release().
+ * The function forces tracking of RCR consumption (which normally doesn't
+ * happen until release processing needs to find space to put new release
+ * commands), and returns zero if the ring still has unprocessed entries,
+ * non-zero if it is empty.
+ */
+int bman_rcr_is_empty(void);
 
 
 	/* Pool management */
@@ -493,6 +375,13 @@ int bman_release(struct bman_pool *pool, const struct bm_buffer *bufs, u8 num,
  */
 int bman_acquire(struct bman_pool *pool, struct bm_buffer *bufs, u8 num,
 			u32 flags);
+
+/**
+ * bman_query_pools - Query all buffer pool states
+ * @state: storage for the queried availability and depletion states
+ */
+int bman_query_pools(struct bm_pool_state *state);
+
 #ifdef __cplusplus
 }
 #endif
-- 
1.7.0.2

