From 006a7a8720b1a13970c08a9d2be3dc9b70915b53 Mon Sep 17 00:00:00 2001
From: Jeffrey Ladouceur <Jeffrey.Ladouceur@freescale.com>
Date: Thu, 3 Dec 2009 13:49:32 +0800
Subject: [PATCH 081/252] pme2: ERN handling, internal seq_num removal

Added callback to handle Error Rejection on input
frame queue.

Removed internal sequence number used for Order
Restoration. This didn't work correctly. There
is now an api for the user to supply the ORP.

Fix sample db not to kernel panic, but rather
print an error and exit.

Changed default sample time for stats accumulator.

Signed-off-by: Jeffrey Ladouceur <Jeffrey.Ladouceur@freescale.com>
[Applied FSL SDK 2.0.3 patch
"kernel-2.6.30-pme2-ERN-handling-internal-seq_num-removal.patch"]
Integrated-by: Yuri Nedel <Yuri.Nedel@windriver.com>
---
 drivers/match/Kconfig          |    2 +-
 drivers/match/pme2_db.c        |   45 ++++++++---
 drivers/match/pme2_high.c      |  123 +++++++++++++++++-----------
 drivers/match/pme2_sample_db.c |   64 +++++++++++----
 drivers/match/pme2_scan.c      |  173 ++++++++++++++++++++--------------------
 include/linux/fsl_pme.h        |   39 ++++-----
 6 files changed, 259 insertions(+), 187 deletions(-)

diff --git a/drivers/match/Kconfig b/drivers/match/Kconfig
index 6629283..e6ed953 100644
--- a/drivers/match/Kconfig
+++ b/drivers/match/Kconfig
@@ -237,7 +237,7 @@ config FSL_PME2_STAT_ACCUMULATOR_UPDATE_INTERVAL
 	int "Configure the pme2 statistics update interval in milliseconds"
 	depends on FSL_PME2_CTRL
 	range 0 10000
-	default 4000
+	default 3400
 	help
 	  The pme accumulator reads the current device statistics and add it
 	  to a running counter. The frequency of these updates may be
diff --git a/drivers/match/pme2_db.c b/drivers/match/pme2_db.c
index 1ea7427..1fd6d16 100644
--- a/drivers/match/pme2_db.c
+++ b/drivers/match/pme2_db.c
@@ -54,6 +54,7 @@ struct cmd_token {
 	struct qm_fd rx_fd;
 	/* Completion interface */
 	struct completion cb_done;
+	u8 ern;
 };
 
 /* PME Compound Frame Index */
@@ -69,11 +70,21 @@ static void db_cb(struct pme_ctx *ctx, const struct qm_fd *fd,
 	complete(&token->cb_done);
 }
 
+static void db_ern_cb(struct pme_ctx *ctx, const struct qm_mr_entry *mr,
+		struct pme_ctx_token *ctx_token)
+{
+	struct cmd_token *token = (struct cmd_token *)ctx_token;
+	token->ern = 1;
+	token->rx_fd = mr->ern.fd;
+	complete(&token->cb_done);
+}
+
 struct ctrl_op {
 	struct pme_ctx_ctrl_token ctx_ctr;
 	struct completion cb_done;
 	enum pme_status cmd_status;
 	u8 res_flag;
+	u8 ern;
 };
 
 static void ctrl_cb(struct pme_ctx *ctx, const struct qm_fd *fd,
@@ -85,6 +96,14 @@ static void ctrl_cb(struct pme_ctx *ctx, const struct qm_fd *fd,
 	complete(&ctrl->cb_done);
 }
 
+static void ctrl_ern_cb(struct pme_ctx *ctx, const struct qm_mr_entry *mr,
+		struct pme_ctx_ctrl_token *token)
+{
+	struct ctrl_op *ctrl = (struct ctrl_op *)token;
+	ctrl->ern = 1;
+	complete(&ctrl->cb_done);
+}
+
 static int exclusive_inc(struct file *fp, struct db_session *db)
 {
 	int ret;
@@ -149,7 +168,6 @@ static int execute_cmd(struct file *fp, struct db_session *db,
 		PMEPRERR("Err alloc %d byte \n", kernel_op.input.size);
 		return -ENOMEM;
 	}
-	PMEPRINFO("kmalloc tx %p\n", tx_data);
 
 	if (copy_from_user(tx_data,
 			(void __user *)kernel_op.input.data,
@@ -158,7 +176,6 @@ static int execute_cmd(struct file *fp, struct db_session *db,
 		ret = -EFAULT;
 		goto free_tx_data;
 	}
-	PMEPRINFO("Copied contiguous user data\n");
 
 	/* Setup input frame */
 	tx_comp[INPUT_FRM].final = 1;
@@ -179,8 +196,6 @@ static int execute_cmd(struct file *fp, struct db_session *db,
 			ret = -ENOMEM;
 			goto unmap_input_frame;
 		}
-		PMEPRINFO("kmalloc rx %p, size %d\n", rx_data,
-				kernel_op.output.size);
 		/* Setup output frame */
 		tx_comp[OUTPUT_FRM].length = kernel_op.output.size;
 		dma_addr = pme_map(rx_data);
@@ -211,7 +226,6 @@ static int execute_cmd(struct file *fp, struct db_session *db,
 		}
 		set_fd_addr(&tx_fd, dma_addr);
 	}
-	PMEPRINFO("About to call pme_ctx_pmtcc\n");
 	ret = pme_ctx_pmtcc(&db->ctx, PME_CTX_OP_WAIT, &tx_fd,
 				(struct pme_ctx_token *)&token);
 	if (unlikely(ret)) {
@@ -222,6 +236,11 @@ static int execute_cmd(struct file *fp, struct db_session *db,
 	/* Wait for the command to complete */
 	wait_for_completion(&token.cb_done);
 
+	if (token.ern) {
+		ret = -EIO;
+		goto unmap_frame;
+	}
+
 	PMEPRINFO("pme2_db: process_completed_token\n");
 	PMEPRINFO("pme2_db: received %d frame type\n", token.rx_fd.format);
 	if (token.rx_fd.format == qm_fd_compound) {
@@ -276,19 +295,18 @@ static int execute_nop(struct file *fp, struct db_session *db)
 {
 	int ret = 0;
 	struct ctrl_op ctx_ctrl =  {
-		.ctx_ctr.cb = ctrl_cb
+		.ctx_ctr.cb = ctrl_cb,
+		.ctx_ctr.ern_cb = ctrl_ern_cb
 	};
 	init_completion(&ctx_ctrl.cb_done);
 
 	ret = pme_ctx_ctrl_nop(&db->ctx, PME_CTX_OP_WAIT|PME_CTX_OP_WAIT_INT,
 			&ctx_ctrl.ctx_ctr);
-	wait_for_completion(&ctx_ctrl.cb_done);
-	/* pme_ctx_ctrl_nop() can be interrupted waiting for the response
-	 * of the NOP. In this scenario, 0 is returned. The only way to
-	 * determine that is was interrupted is to check for signal_pending()
-	 */
-	if (!ret && signal_pending(current))
-		ret = -ERESTARTSYS;
+	if (!ret)
+		wait_for_completion(&ctx_ctrl.cb_done);
+
+	if (ctx_ctrl.ern)
+		ret = -EIO;
 	return ret;
 }
 
@@ -372,6 +390,7 @@ static int fsl_pme2_db_open(struct inode *node, struct file *fp)
 		return -ENOMEM;
 	fp->private_data = db;
 	db->ctx.cb = db_cb;
+	db->ctx.ern_cb = db_ern_cb;
 
 	ret = pme_ctx_init(&db->ctx,
 			PME_CTX_FLAG_EXCLUSIVE |
diff --git a/drivers/match/pme2_high.c b/drivers/match/pme2_high.c
index ed389f5..6a96009 100644
--- a/drivers/match/pme2_high.c
+++ b/drivers/match/pme2_high.c
@@ -72,6 +72,11 @@
 
 #define PME_CTX_FLAG_PRIVATE     0xff000000
 
+struct pme_nostash {
+	struct qman_fq fqin;
+	struct pme_ctx *parent;
+};
+
 /* This wrapper simplifies conditional (and locked) read-modify-writes to
  * 'flags'. Inlining should allow the compiler to optimise it based on the
  * parameters, eg. if 'must_be_set'/'must_not_be_set' are zero it will
@@ -102,11 +107,11 @@ static void cb_dc_ern(struct qman_portal *, struct qman_fq *,
 static void cb_fqs(struct qman_portal *, struct qman_fq *,
 				const struct qm_mr_entry *);
 static const struct qman_fq_cb pme_fq_base_in = {
-	.fqs = cb_fqs
+	.fqs = cb_fqs,
+	.ern = cb_ern
 };
 static const struct qman_fq_cb pme_fq_base_out = {
 	.dqrr = cb_dqrr,
-	.ern = cb_ern,
 	.dc_ern = cb_dc_ern,
 	.fqs = cb_fqs
 };
@@ -241,7 +246,7 @@ int pme_ctx_init(struct pme_ctx *ctx, u32 flags, u32 bpid, u8 qosin,
 			u8 qosout, enum qm_channel dest,
 			const struct qm_fqd_stashing *stashing)
 {
-	u32 fqid_rx, fqid_tx;
+	u32 fqid_rx = 0, fqid_tx = 0;
 	int rxinit = 0, ret = -ENOMEM, fqin_inited = 0;
 
 	ctx->fq.cb = pme_fq_base_out;
@@ -251,20 +256,22 @@ int pme_ctx_init(struct pme_ctx *ctx, u32 flags, u32 bpid, u8 qosin,
 	spin_lock_init(&ctx->lock);
 	init_waitqueue_head(&ctx->queue);
 	INIT_LIST_HEAD(&ctx->tokens);
-	ctx->seq_num = 0;
-	ctx->fqin = NULL;
 	ctx->hw_flow = NULL;
 	ctx->hw_residue = NULL;
 
+	ctx->us_data = kzalloc(sizeof(struct pme_nostash), GFP_KERNEL);
+	if (!ctx->us_data)
+		goto err;
+	ctx->us_data->parent = ctx;
 	fqid_rx = qm_fq_new();
 	fqid_tx = qm_fq_new();
-	ctx->fqin = slabfq_alloc();
-	if (!fqid_rx || !fqid_tx || !ctx->fqin)
+	if (!fqid_rx || !fqid_tx || !ctx->us_data)
 		goto err;
-	ctx->fqin->cb = pme_fq_base_in;
+	ctx->us_data->fqin.cb = pme_fq_base_in;
 	if (qman_create_fq(fqid_rx, QMAN_FQ_FLAG_TO_DCPORTAL |
 			((flags & PME_CTX_FLAG_LOCKED) ?
-				QMAN_FQ_FLAG_LOCKED : 0), ctx->fqin))
+				QMAN_FQ_FLAG_LOCKED : 0),
+				&ctx->us_data->fqin))
 		goto err;
 	fqin_inited = 1;
 	if (qman_create_fq(fqid_tx, QMAN_FQ_FLAG_NO_ENQUEUE |
@@ -285,7 +292,7 @@ int pme_ctx_init(struct pme_ctx *ctx, u32 flags, u32 bpid, u8 qosin,
 	ret = reconfigure_rx(ctx, 0, qosout, dest, stashing);
 	if (ret) {
 		/* Need to OOS the FQ before it gets free'd */
-		ret = qman_oos_fq(ctx->fqin);
+		ret = qman_oos_fq(&ctx->us_data->fqin);
 		BUG_ON(ret);
 		goto err;
 	}
@@ -297,10 +304,10 @@ err:
 		qm_fq_free(fqid_tx);
 	if (ctx->hw_flow)
 		pme_hw_flow_free(ctx->hw_flow);
-	if (ctx->fqin) {
+	if (ctx->us_data) {
 		if (fqin_inited)
-			qman_destroy_fq(ctx->fqin, 0);
-		slabfq_free(ctx->fqin);
+			qman_destroy_fq(&ctx->us_data->fqin, 0);
+		kfree(ctx->us_data);
 	}
 	if (rxinit)
 		qman_destroy_fq(&ctx->fq, 0);
@@ -319,24 +326,24 @@ void pme_ctx_finish(struct pme_ctx *ctx)
 	BUG_ON(ret);
 	/* Rx/Tx are empty (coz ctx is disabled) so retirement should be
 	 * immediate */
-	ret = qman_retire_fq(ctx->fqin, &flags);
+	ret = qman_retire_fq(&ctx->us_data->fqin, &flags);
 	BUG_ON(ret);
 	BUG_ON(flags & QMAN_FQ_STATE_BLOCKOOS);
 	ret = qman_retire_fq(&ctx->fq, &flags);
 	BUG_ON(ret);
 	BUG_ON(flags & QMAN_FQ_STATE_BLOCKOOS);
 	/* OOS and free (don't kfree fq, it's a static ctx member) */
-	ret = qman_oos_fq(ctx->fqin);
+	ret = qman_oos_fq(&ctx->us_data->fqin);
 	BUG_ON(ret);
 	ret = qman_oos_fq(&ctx->fq);
 	BUG_ON(ret);
-	fqid_rx = qman_fq_fqid(ctx->fqin);
+	fqid_rx = qman_fq_fqid(&ctx->us_data->fqin);
 	fqid_tx = qman_fq_fqid(&ctx->fq);
-	qman_destroy_fq(ctx->fqin, 0);
+	qman_destroy_fq(&ctx->us_data->fqin, 0);
 	qman_destroy_fq(&ctx->fq, 0);
 	qm_fq_free(fqid_rx);
 	qm_fq_free(fqid_tx);
-	slabfq_free(ctx->fqin); /* the fq was dynamically allocated */
+	kfree(ctx->us_data);
 	if (ctx->hw_flow)
 		pme_hw_flow_free(ctx->hw_flow);
 	if (ctx->hw_residue)
@@ -370,15 +377,13 @@ int pme_ctx_disable(struct pme_ctx *ctx, u32 flags)
 	ret = empty_pipeline(ctx, flags);
 	if (!ret && !(ctx->flags & PME_CTX_FLAG_EXCLUSIVE))
 		/* Park fqin (exclusive is always parked) */
-		ret = park(ctx->fqin, &initfq);
+		ret = park(&ctx->us_data->fqin, &initfq);
 	if (ret) {
 		atomic_inc(&ctx->refs);
 		do_flags(ctx, 0, 0, 0, PME_CTX_FLAG_DISABLING);
 		wake_up(&ctx->queue);
 		return ret;
 	}
-	/* Our ORP got reset too, so reset the sequence number */
-	ctx->seq_num = 0;
 	do_flags(ctx, 0, 0, PME_CTX_FLAG_DISABLED, 0);
 	return 0;
 }
@@ -394,7 +399,8 @@ int pme_ctx_enable(struct pme_ctx *ctx)
 	if (ret)
 		return ret;
 	if (!(ctx->flags & PME_CTX_FLAG_EXCLUSIVE)) {
-		ret = qman_init_fq(ctx->fqin, QMAN_INITFQ_FLAG_SCHED, NULL);
+		ret = qman_init_fq(&ctx->us_data->fqin,
+				QMAN_INITFQ_FLAG_SCHED, NULL);
 		if (ret) {
 			do_flags(ctx, 0, 0, 0, PME_CTX_FLAG_ENABLING);
 			return ret;
@@ -419,19 +425,7 @@ int pme_ctx_reconfigure_tx(struct pme_ctx *ctx, u32 bpid, u8 qosin)
 		return ret;
 	memset(&initfq,0,sizeof(initfq));
 	pme_initfq(&initfq, ctx->hw_flow, qosin, bpid, qman_fq_fqid(&ctx->fq));
-	if (!(ctx->flags & PME_CTX_FLAG_NO_ORP)) {
-		initfq.we_mask |= QM_INITFQ_WE_FQCTRL | QM_INITFQ_WE_ORPC;
-		/* ORPRWS==1 means the ORP window is max 64 frames. Given that
-		 * the out-of-order problem is (usually?) limited to spraying
-		 * over different EQCRs from different cores, it shouldn't be
-		 * possible to get more than this far behind (8 full EQCRs is 56
-		 * frames). */
-		initfq.fqd.orprws = 1;
-		initfq.fqd.oa = 0;
-		initfq.fqd.olws = 0;
-		initfq.fqd.fq_ctrl |= QM_FQCTRL_ORP;
-	}
-	ret = qman_init_fq(ctx->fqin, 0, &initfq);
+	ret = qman_init_fq(&ctx->us_data->fqin, 0, &initfq);
 	do_flags(ctx, 0, 0, 0, PME_CTX_FLAG_RECONFIG);
 	return ret;
 }
@@ -468,7 +462,7 @@ static int __try_exclusive(struct pme_ctx *ctx)
 			ret = -EBUSY;
 	} else {
 		/* it's not currently held */
-		ret = pme2_exclusive_set(ctx->fqin);
+		ret = pme2_exclusive_set(&ctx->us_data->fqin);
 		if (!ret)
 			exclusive_ctx = ctx;
 	}
@@ -560,19 +554,17 @@ static inline int do_work(struct pme_ctx *ctx, u32 flags, struct qm_fd *fd,
 
 	spin_lock_irq(&ctx->lock);
 	list_add_tail(&token->node, &ctx->tokens);
-	if (!orp_fq) {
-		seqnum = ctx->seq_num++;
-		ctx->seq_num &= QM_EQCR_SEQNUM_SEQMASK; /* rollover at 2^14 */
-		orp_fq = ctx->fqin;
-	}
 	spin_unlock_irq(&ctx->lock);
 
-	if (ctx->flags & PME_CTX_FLAG_NO_ORP)
-		ret = qman_enqueue(ctx->fqin, fd, ctrl2eq(flags));
+	if (!orp_fq)
+		ret = qman_enqueue(&ctx->us_data->fqin, fd, ctrl2eq(flags));
 	else
-		ret = qman_enqueue_orp(ctx->fqin, fd, ctrl2eq(flags),
+		ret = qman_enqueue_orp(&ctx->us_data->fqin, fd, ctrl2eq(flags),
 					orp_fq, seqnum);
 	if (ret) {
+		spin_lock_irq(&ctx->lock);
+		list_del(&token->node);
+		spin_unlock_irq(&ctx->lock);
 		if (ctx->flags & PME_CTX_FLAG_EXCLUSIVE)
 			release_exclusive(ctx);
 		release_work(ctx);
@@ -722,7 +714,7 @@ static inline struct pme_ctx_token *pop_matching_token(struct pme_ctx *ctx,
 	}
 	token = NULL;
 	pr_err("PME2 Could not find matching token!\n");
-	BUG_ON(1);
+	BUG();
 found:
 	spin_unlock_irq(&ctx->lock);
 	return token;
@@ -793,9 +785,44 @@ static enum qman_cb_dqrr_result cb_dqrr(struct qman_portal *portal,
 static void cb_ern(struct qman_portal *portal, struct qman_fq *fq,
 				const struct qm_mr_entry *mr)
 {
-	/* Give this the same handling as the error case through cb_dqrr(). */
-	struct pme_ctx *ctx = (struct pme_ctx *)fq;
-	cb_helper(portal, ctx, &mr->ern.fd, 1);
+	struct pme_ctx *ctx;
+	struct pme_nostash *data;
+	struct pme_ctx_token *token;
+
+	data = container_of(fq, struct pme_nostash, fqin);
+	ctx = data->parent;
+
+	token = pop_matching_token(ctx, &mr->ern.fd);
+	if (likely(token->cmd_type == pme_cmd_scan)) {
+		BUG_ON(!ctx->ern_cb);
+		ctx->ern_cb(ctx, mr, token);
+	} else if (token->cmd_type == pme_cmd_pmtcc) {
+		BUG_ON(!ctx->ern_cb);
+		ctx->ern_cb(ctx, mr, token);
+	} else {
+		struct pme_ctx_ctrl_token *ctrl_token;
+		/* outcast ctx and call supplied callback */
+		ctrl_token = container_of(token, struct pme_ctx_ctrl_token,
+					base_token);
+		if (token->cmd_type == pme_cmd_flow_write) {
+			/* Release the allocated flow context */
+			pme_hw_flow_free(ctrl_token->internal_flow_ptr);
+		} else if (token->cmd_type == pme_cmd_flow_read) {
+			/* Copy read result */
+			memcpy(ctrl_token->usr_flow_ptr,
+				ctrl_token->internal_flow_ptr,
+				sizeof(struct pme_flow));
+			/* Release the allocated flow context */
+			pme_hw_flow_free(ctrl_token->internal_flow_ptr);
+		}
+		BUG_ON(!ctrl_token->ern_cb);
+		ctrl_token->ern_cb(ctx, mr, ctrl_token);
+	}
+	/* Consume the frame */
+	if (ctx->flags & PME_CTX_FLAG_EXCLUSIVE)
+		release_exclusive(ctx);
+	if (atomic_dec_and_test(&ctx->refs))
+		wake_up(&ctx->queue);
 }
 
 static void cb_dc_ern(struct qman_portal *portal, struct qman_fq *fq,
diff --git a/drivers/match/pme2_sample_db.c b/drivers/match/pme2_sample_db.c
index 61a12c5..07ead46 100644
--- a/drivers/match/pme2_sample_db.c
+++ b/drivers/match/pme2_sample_db.c
@@ -100,6 +100,7 @@ struct pmtcc_ctx {
 	struct pme_ctx base_ctx;
 	struct qm_fd result_fd;
 	struct completion done;
+	u8 ern;
 };
 
 static void pmtcc_cb(struct pme_ctx *ctx, const struct qm_fd *fd,
@@ -110,10 +111,22 @@ static void pmtcc_cb(struct pme_ctx *ctx, const struct qm_fd *fd,
 	complete(&my_ctx->done);
 }
 
+static void pmtcc_ern_cb(struct pme_ctx *ctx, const struct qm_mr_entry *mr,
+		struct pme_ctx_token *ctx_token)
+{
+	struct pmtcc_ctx *my_ctx = (struct pmtcc_ctx *)ctx;
+	my_ctx->result_fd = mr->ern.fd;
+	my_ctx->ern = 1;
+	complete(&my_ctx->done);
+}
+
+
 void pme2_sample_db(void)
 {
 	struct pmtcc_ctx ctx = {
 		.base_ctx.cb = pmtcc_cb,
+		.base_ctx.ern_cb = pmtcc_ern_cb,
+		.ern = 0
 	};
 	struct qm_fd fd;
 	struct qm_sg_entry sg_table[2];
@@ -130,10 +143,17 @@ void pme2_sample_db(void)
 		PME_CTX_FLAG_EXCLUSIVE |
 		PME_CTX_FLAG_PMTCC |
 		PME_CTX_FLAG_LOCAL, 0, 4, 4, 0, NULL);
-	BUG_ON(ret);
+	if (ret) {
+		pr_err("sample_db: can't init ctx\n");
+		return;
+	}
+
 	/* enable the context */
 	ret = pme_ctx_enable(&ctx.base_ctx);
-	BUG_ON(ret);
+	if (ret) {
+		pr_err("sample_db: can't enable ctx\n");
+		goto _finish_1;
+	}
 
 	/* Write the database */
 	memset(&fd, 0, sizeof(struct qm_fd));
@@ -146,21 +166,23 @@ void pme2_sample_db(void)
 	ret = pme_ctx_pmtcc(&ctx.base_ctx, PME_CTX_OP_WAIT, &fd, &token);
 	if (ret == -ENODEV) {
 		pr_err("sample_db: not the control plane, bailing\n");
-		ret = pme_ctx_disable(&ctx.base_ctx,
-			PME_CTX_OP_WAIT | PME_CTX_OP_WAIT_INT);
-		BUG_ON(ret);
-		pme_ctx_finish(&ctx.base_ctx);
-		return;
+		goto _finish_2;
+	}
+	if (ret) {
+		pr_err("sample_db: error with pmtcc\n");
+		goto _finish_2;
 	}
-	BUG_ON(ret);
 	wait_for_completion(&ctx.done);
-	kfree(mem);
+	if (ctx.ern) {
+		pr_err("sample_db: Rx ERN from pmtcc\n");
+		goto _finish_2;
+	}
 	status = pme_fd_res_status(&ctx.result_fd);
 	if (status) {
 		pr_info("sample_db: PMTCC write status failed %d\n", status);
-		BUG_ON(1);
+		goto _finish_2;
 	}
-
+	kfree(mem);
 	/* Read back the database */
 	init_completion(&ctx.done);
 	memset(&fd, 0, sizeof(struct qm_fd));
@@ -178,18 +200,24 @@ void pme2_sample_db(void)
 	fd.format = qm_fd_compound;
 	fd.addr_lo = pme_map(sg_table);
 	ret = pme_ctx_pmtcc(&ctx.base_ctx, PME_CTX_OP_WAIT, &fd, &token);
-	BUG_ON(ret);
+	if (ret) {
+		pr_err("sample_db: error with pmtcc\n");
+		goto _finish_3;
+	}
 	wait_for_completion(&ctx.done);
-
+	if (ctx.ern) {
+		pr_err("sample_db: Rx ERN from pmtcc\n");
+		goto _finish_3;
+	}
 	status = pme_fd_res_status(&ctx.result_fd);
 	if (status) {
-		pr_info("sample_db: PMTCC read status failed %d\n", status);
-		BUG_ON(1);
+		pr_err("sample_db: PMTCC read status failed %d\n", status);
+		goto _finish_3;
 	}
 	if (pme_fd_res_flags(&ctx.result_fd)) {
 		pr_err("sample_db: flags result set %x\n",
 			pme_fd_res_flags(&ctx.result_fd));
-		BUG_ON(1);
+		goto _finish_3;
 	}
 	if (memcmp(db_read_expected_result, mem_result,	28) != 0) {
 		pr_err("sample_db: DB read result not expected\n");
@@ -198,14 +226,16 @@ void pme2_sample_db(void)
 				sizeof(db_read_expected_result));
 		pr_info("Received\n");
 		hexdump(mem_result, 28);
-		BUG_ON(1);
 	}
+_finish_3:
 	kfree(mem_result);
+_finish_2:
 	kfree(mem);
 	/* Disable */
 	ret = pme_ctx_disable(&ctx.base_ctx,
 		PME_CTX_OP_WAIT | PME_CTX_OP_WAIT_INT);
 	BUG_ON(ret);
+_finish_1:
 	pme_ctx_finish(&ctx.base_ctx);
 	pr_info("sample_db: pme2 sample DB initialised\n");
 }
diff --git a/drivers/match/pme2_scan.c b/drivers/match/pme2_scan.c
index ba27c86..9a09547 100644
--- a/drivers/match/pme2_scan.c
+++ b/drivers/match/pme2_scan.c
@@ -78,6 +78,7 @@ struct cmd_token {
 	/* List management for completed async requests */
 	struct list_head completed_list;
 	u8 done;
+	u8 ern;
 };
 
 struct ctrl_op {
@@ -85,6 +86,7 @@ struct ctrl_op {
 	struct completion cb_done;
 	enum pme_status cmd_status;
 	u8 res_flag;
+	u8 ern;
 };
 
 static void ctrl_cb(struct pme_ctx *ctx, const struct qm_fd *fd,
@@ -96,6 +98,14 @@ static void ctrl_cb(struct pme_ctx *ctx, const struct qm_fd *fd,
 	complete(&ctrl->cb_done);
 }
 
+static void ctrl_ern_cb(struct pme_ctx *ctx, const struct qm_mr_entry *mr,
+		struct pme_ctx_ctrl_token *token)
+{
+	struct ctrl_op *ctrl = (struct ctrl_op *)token;
+	ctrl->ern = 1;
+	complete(&ctrl->cb_done);
+}
+
 static inline int scan_data_empty(struct scan_session *session)
 {
 	return list_empty(&session->completed_commands);
@@ -131,6 +141,28 @@ static void scan_cb(struct pme_ctx *ctx, const struct qm_fd *fd,
 	return;
 }
 
+static void scan_ern_cb(struct pme_ctx *ctx, const struct qm_mr_entry *mr,
+		struct pme_ctx_token *ctx_token)
+{
+	struct cmd_token *token = (struct cmd_token *)ctx_token;
+	struct scan_session *session = (struct scan_session *)ctx;
+
+	token->ern = 1;
+	token->rx_fd = mr->ern.fd;
+	/* If this is a asynchronous command, queue the token */
+	if (!token->synchronous) {
+		spin_lock(&session->completed_commands_lock);
+		list_add_tail(&token->completed_list,
+			      &session->completed_commands);
+		session->completed_count++;
+		spin_unlock(&session->completed_commands_lock);
+	}
+	/* Wake up the thread that's waiting for us */
+	token->done = 1;
+	wake_up(token->queue);
+	return;
+}
+
 static int process_completed_token(struct file *fp,
 				struct cmd_token *token_p,
 				struct pme_scan_result *user_result)
@@ -139,21 +171,22 @@ static int process_completed_token(struct file *fp,
 	struct pme_scan_result local_result;
 	u32 src_sz, dst_sz;
 
-#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
-	pr_info("pme2_scan: process_completed_token\n");
-	pr_info("pme2_scan: received %d frame type\n", token_p->rx_fd.format);
-#endif
+	PMEPRINFO("pme2_scan: process_completed_token \n");
+
 	memset(&local_result, 0, sizeof(local_result));
+	if (token_p->ern) {
+		PMEPRINFO("pme2_scan: ern in scan\n");
+		ret = -EIO;
+		goto done;
+	}
 	local_result.output.data = token_p->kernel_op.output.data;
 
 	if (token_p->rx_fd.format == qm_fd_compound) {
 		/* Need to copy  output */
 		src_sz = token_p->tx_comp[OUTPUT_FRM].length;
 		dst_sz = token_p->kernel_op.output.size;
-#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
-		pr_info("pme2_scan: pme gen %u data, have space for %u\n",
+		PMEPRINFO("pme2_scan: pme gen %u data, have space for %u\n",
 				src_sz, dst_sz);
-#endif
 		local_result.output.size = min(dst_sz, src_sz);
 		/* Doesn't make sense we generated more than available space
 		 * should have got truncation.
@@ -172,18 +205,16 @@ static int process_completed_token(struct file *fp,
 
 	local_result.flags |= pme_fd_res_flags(&token_p->rx_fd);
 	local_result.status |= pme_fd_res_status(&token_p->rx_fd);
+done:
 	local_result.opaque = token_p->kernel_op.opaque;
-#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
-	pr_info("pme2_scan: process_completed_token, cpy to user\n");
-#endif
+	PMEPRINFO("pme2_scan: process_completed_token, cpy to user\n");
+
 	/* Update the used values */
 	if (copy_to_user(user_result, &local_result, sizeof(local_result))) {
 		ret = -EFAULT;
 		cleanup_token(token_p);
 	}
-#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
-	pr_info("pme2_scan: process_completed_token, free token\n");
-#endif
+	PMEPRINFO("pme2_scan: process_completed_token, free token\n");
 	cleanup_token(token_p);
 	return ret;
 }
@@ -196,8 +227,10 @@ static int getscan_cmd(struct file *fp, struct scan_session *session,
 	struct pme_scan_params local_scan_params;
 	struct ctrl_op ctx_ctrl =  {
 		.ctx_ctr.cb = ctrl_cb,
+		.ctx_ctr.ern_cb = ctrl_ern_cb,
 		.cmd_status = 0,
-		.res_flag = 0
+		.res_flag = 0,
+		.ern = 0
 	};
 	init_completion(&ctx_ctrl.cb_done);
 
@@ -212,13 +245,13 @@ static int getscan_cmd(struct file *fp, struct scan_session *session,
 	ret = pme_ctx_ctrl_read_flow(&session->ctx, WAIT_AND_INTERRUPTABLE,
 			&params, &ctx_ctrl.ctx_ctr);
 	if (ret) {
-		pr_info("pme2_scan: read flow error %d\n", ret);
+		PMEPRINFO("read flow error %d\n", ret);
 		goto done;
 	}
 	wait_for_completion(&ctx_ctrl.cb_done);
 
-	if (ctx_ctrl.cmd_status || ctx_ctrl.res_flag) {
-		pr_info("pme2_scan: read flow error %d\n", ctx_ctrl.cmd_status);
+	if (ctx_ctrl.ern || ctx_ctrl.cmd_status || ctx_ctrl.res_flag) {
+		PMEPRINFO("read flow error %d\n", ctx_ctrl.cmd_status);
 		ret = -EFAULT;
 		goto done;
 	}
@@ -250,8 +283,10 @@ static int setscan_cmd(struct file *fp, struct scan_session *session,
 	u32 flag = WAIT_AND_INTERRUPTABLE;
 	struct ctrl_op ctx_ctrl =  {
 		.ctx_ctr.cb = ctrl_cb,
+		.ctx_ctr.ern_cb = ctrl_ern_cb,
 		.cmd_status = 0,
-		.res_flag = 0
+		.res_flag = 0,
+		.ern = 0
 	};
 	struct pme_flow params;
 	struct pme_scan_params local_params;
@@ -286,12 +321,12 @@ static int setscan_cmd(struct file *fp, struct scan_session *session,
 	ret = pme_ctx_ctrl_update_flow(&session->ctx, flag, &params,
 			&ctx_ctrl.ctx_ctr);
 	if (ret) {
-		pr_info("pme2_scan: update flow error %d\n", ret);
+		PMEPRINFO("update flow error %d\n", ret);
 		goto done;
 	}
 	wait_for_completion(&ctx_ctrl.cb_done);
-	if (ctx_ctrl.cmd_status || ctx_ctrl.res_flag) {
-		pr_info("pme2_scan: update flow err %d\n", ctx_ctrl.cmd_status);
+	if (ctx_ctrl.ern || ctx_ctrl.cmd_status || ctx_ctrl.res_flag) {
+		PMEPRINFO("update flow err %d\n", ctx_ctrl.cmd_status);
 		ret = -EFAULT;
 		goto done;
 	}
@@ -314,8 +349,10 @@ static int resetseq_cmd(struct file *fp, struct scan_session *session)
 	struct pme_flow params;
 	struct ctrl_op ctx_ctrl =  {
 		.ctx_ctr.cb = ctrl_cb,
+		.ctx_ctr.ern_cb = ctrl_ern_cb,
 		.cmd_status = 0,
-		.res_flag = 0
+		.res_flag = 0,
+		.ern = 0
 	};
 	init_completion(&ctx_ctrl.cb_done);
 	pme_sw_flow_init(&params);
@@ -335,8 +372,8 @@ static int resetseq_cmd(struct file *fp, struct scan_session *session)
 	if (!ret)
 		pr_info("pme2_scan: update flow error %d\n", ret);
 	wait_for_completion(&ctx_ctrl.cb_done);
-	if (ctx_ctrl.cmd_status || ctx_ctrl.res_flag) {
-		pr_info("pme2_scan: update flow err %d\n", ctx_ctrl.cmd_status);
+	if (ctx_ctrl.ern || ctx_ctrl.cmd_status || ctx_ctrl.res_flag) {
+		PMEPRINFO("update flow err %d\n", ctx_ctrl.cmd_status);
 		ret = -EFAULT;
 	}
 done:
@@ -349,8 +386,10 @@ static int resetresidue_cmd(struct file *fp, struct scan_session *session)
 	struct pme_flow params;
 	struct ctrl_op ctx_ctrl =  {
 		.ctx_ctr.cb = ctrl_cb,
+		.ctx_ctr.ern_cb = ctrl_ern_cb,
 		.cmd_status = 0,
-		.res_flag = 0
+		.res_flag = 0,
+		.ern = 0
 	};
 
 	init_completion(&ctx_ctrl.cb_done);
@@ -368,8 +407,8 @@ static int resetresidue_cmd(struct file *fp, struct scan_session *session)
 	if (!ret)
 		pr_info("pme2_scan: update flow error %d\n", ret);
 	wait_for_completion(&ctx_ctrl.cb_done);
-	if (ctx_ctrl.cmd_status || ctx_ctrl.res_flag) {
-		pr_info("pme2_scan: update flow err %d\n", ctx_ctrl.cmd_status);
+	if (ctx_ctrl.ern || ctx_ctrl.cmd_status || ctx_ctrl.res_flag) {
+		PMEPRINFO("update flow err %d\n", ctx_ctrl.cmd_status);
 		ret = -EFAULT;
 	}
 done:
@@ -403,10 +442,6 @@ static int process_scan_cmd(struct file *fp, struct scan_session *session,
 	if (copy_from_user(&token_p->kernel_op, user_cmd, sizeof(*user_cmd)))
 		return -EFAULT;
 	/* Copy the input */
-#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
-	pr_info("Received User Space Contiguous mem \n");
-	pr_info("length = %d \n", token_p->kernel_op.input.size);
-#endif
 	token_p->synchronous = synchronous;
 	token_p->tx_size = token_p->kernel_op.input.size;
 	token_p->tx_data = kmalloc(token_p->kernel_op.input.size, GFP_KERNEL);
@@ -414,10 +449,6 @@ static int process_scan_cmd(struct file *fp, struct scan_session *session,
 		pr_err("pme2_scan: Err alloc %d byte", token_p->tx_size);
 		cleanup_token(token_p);
 		return -ENOMEM;
-	} else {
-#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
-		pr_info("kmalloc tx %p\n", token_p->tx_data);
-#endif
 	}
 	if (copy_from_user(token_p->tx_data,
 			token_p->kernel_op.input.data,
@@ -426,9 +457,7 @@ static int process_scan_cmd(struct file *fp, struct scan_session *session,
 		cleanup_token(token_p);
 		return -EFAULT;
 	}
-#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
-	pr_info("Copied contiguous user data\n");
-#endif
+	PMEPRINFO("Copied contiguous user data\n");
 
 	/* Setup input frame */
 	token_p->tx_comp[INPUT_FRM].final = 1;
@@ -438,20 +467,13 @@ static int process_scan_cmd(struct file *fp, struct scan_session *session,
 	/* setup output frame, if output is expected */
 	if (token_p->kernel_op.output.size) {
 		token_p->rx_size = token_p->kernel_op.output.size;
-	#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
-		pr_info("pme2_scan: expect output %d\n", token_p->rx_size);
-	#endif
+		PMEPRINFO("pme2_scan: expect output %d\n", token_p->rx_size);
 		token_p->rx_data = kmalloc(token_p->rx_size, GFP_KERNEL);
 		if (!token_p->rx_data) {
 			pr_err("pme2_scan: Err alloc %d byte",
 					token_p->rx_size);
 			cleanup_token(token_p);
 			return -ENOMEM;
-		} else {
-#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
-			pr_info("kmalloc rx %p, size %d\n", token_p->rx_data,
-					token_p->rx_size);
-#endif
 		}
 		/* Setup output frame */
 		token_p->tx_comp[OUTPUT_FRM].length = token_p->rx_size;
@@ -475,9 +497,7 @@ static int process_scan_cmd(struct file *fp, struct scan_session *session,
 		token_p->queue = &session->waiting_for_completion;
 	token_p->done = 0;
 
-#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
-	pr_info("About to call pme_ctx_scan\n");
-#endif
+	PMEPRINFO("About to call pme_ctx_scan\n");
 	if (token_p->kernel_op.flags & PME_SCAN_CMD_STARTRESET)
 		scan_flags |= PME_CMD_SCAN_SR;
 	if (token_p->kernel_op.flags & PME_SCAN_CMD_END)
@@ -497,10 +517,7 @@ static int process_scan_cmd(struct file *fp, struct scan_session *session,
 		/* Don't wait.  The command is away */
 		return 0;
 
-#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
-	pr_info("Wait for completion\n");
-#endif
-
+	PMEPRINFO("Wait for completion\n");
 	/* Wait for the command to complete */
 	/* TODO: Should this be wait_event_interruptible ?
 	 * If so, will need logic to indicate */
@@ -523,15 +540,15 @@ static int fsl_pme2_scan_open(struct inode *node, struct file *fp)
 	struct pme_flow flow;
 	struct ctrl_op ctx_ctrl =  {
 		.ctx_ctr.cb = ctrl_cb,
+		.ctx_ctr.ern_cb = ctrl_ern_cb,
 		.cmd_status = 0,
-		.res_flag = 0
+		.res_flag = 0,
+		.ern = 0
 	};
 
 	pme_sw_flow_init(&flow);
 	init_completion(&ctx_ctrl.cb_done);
-#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
-	pr_info("pme2_scan: open %d\n", smp_processor_id());
-#endif
+	PMEPRINFO("pme2_scan: open %d\n", smp_processor_id());
 	fp->private_data = kzalloc(sizeof(*session), GFP_KERNEL);
 	if (!fp->private_data) {
 		return -ENOMEM;
@@ -542,11 +559,10 @@ static int fsl_pme2_scan_open(struct inode *node, struct file *fp)
 	INIT_LIST_HEAD(&session->completed_commands);
 	spin_lock_init(&session->completed_commands_lock);
 	spin_lock_init(&session->set_subset_lock);
-#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
-	pr_info("kmalloc session %p\n", fp->private_data);
-#endif
+	PMEPRINFO("kmalloc session %p\n", fp->private_data);
 	session = fp->private_data;
 	session->ctx.cb = scan_cb;
+	session->ctx.ern_cb = scan_ern_cb;
 
 	/* qosin, qosout should be driver attributes */
 	ret = pme_ctx_init(&session->ctx, PME_CTX_FLAG_LOCAL, 0, 4, 4, 0, NULL);
@@ -557,32 +573,29 @@ static int fsl_pme2_scan_open(struct inode *node, struct file *fp)
 	/* enable the context */
 	ret = pme_ctx_enable(&session->ctx);
 	if (ret) {
-		pr_info("pme2_scan: error enabling ctx %d\n", ret);
+		PMEPRINFO("error enabling ctx %d\n", ret);
 		pme_ctx_finish(&session->ctx);
 		goto exit;
 	}
 	/* Update flow to set sane defaults in the flow context */
-	/* TODO: because we free 'flow' here, we need to be uninterruptible. */
 	ret = pme_ctx_ctrl_update_flow(&session->ctx,
 		PME_CTX_OP_WAIT | PME_CMD_FCW_ALL, &flow, &ctx_ctrl.ctx_ctr);
 	if (ret) {
-		pr_info("pme2_scan: error updating flow ctx %d\n", ret);
+		PMEPRINFO("error updating flow ctx %d\n", ret);
 		pme_ctx_disable(&session->ctx, PME_CTX_OP_WAIT);
 		pme_ctx_finish(&session->ctx);
 		goto exit;
 	}
 	wait_for_completion(&ctx_ctrl.cb_done);
-	if (ctx_ctrl.cmd_status || ctx_ctrl.res_flag) {
-		pr_info("pme2_scan: error updating flow ctx %d\n", ret);
+	if (ctx_ctrl.ern || ctx_ctrl.cmd_status || ctx_ctrl.res_flag) {
+		PMEPRINFO("error updating flow ctx %d\n", ret);
 		pme_ctx_disable(&session->ctx, PME_CTX_OP_WAIT);
 		pme_ctx_finish(&session->ctx);
 		ret = -EFAULT;
 		goto exit;
 	}
-#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
 	/* Set up the structures used for asynchronous requests */
-	pr_info("pme2_scan: Finish pme_scan open %d \n", smp_processor_id());
-#endif
+	PMEPRINFO("pme2_scan: Finish pme_scan open %d \n", smp_processor_id());
 	return 0;
 exit:
 	kfree(fp->private_data);
@@ -608,9 +621,7 @@ static int fsl_pme2_scan_close(struct inode *node, struct file *fp)
 
 	pme_ctx_finish(&session->ctx);
 	kfree(session);
-#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
-	pr_info("pme2_scan: Finish pme_session close\n");
-#endif
+	PMEPRINFO("pme2_scan: Finish pme_session close\n");
 	return 0;
 }
 
@@ -704,9 +715,7 @@ static int fsl_pme2_scan_ioctl(struct inode *inode, struct file *fp,
 		/* Copy the command to kernel space */
 		if (copy_from_user(&scan_cmds, (void *)arg, sizeof(scan_cmds)))
 			return -EFAULT;
-#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
-		pr_info("pme2_scan: Received Wn for %d cmds \n", scan_cmds.num);
-#endif
+		PMEPRINFO("Received Wn for %d cmds \n", scan_cmds.num);
 		for (i = 0; i < scan_cmds.num; i++) {
 			ret = process_scan_cmd(fp, session, &scan_cmds.cmds[i],
 					NULL, 0);
@@ -734,9 +743,7 @@ static int fsl_pme2_scan_ioctl(struct inode *inode, struct file *fp,
 		/* Copy the command to kernel space */
 		if (copy_from_user(&results, (void *)arg, sizeof(results)))
 			return -EFAULT;
-#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
-		pr_info("pme2_scan: Received Rn for %d res \n", results.num);
-#endif
+		PMEPRINFO("pme2_scan: Received Rn for %d res \n", results.num);
 		if (!results.num)
 			return 0;
 		do {
@@ -746,9 +753,7 @@ static int fsl_pme2_scan_ioctl(struct inode *inode, struct file *fp,
 			spin_lock(&session->completed_commands_lock);
 			if (!list_empty(&session->completed_commands)) {
 				/* Move to a different list */
-#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
-				pr_info("pme2_scan: Pop response\n");
-#endif
+				PMEPRINFO("pme2_scan: Pop response\n");
 				completed_cmd = list_first_entry(
 						&session->completed_commands,
 						struct cmd_token,
@@ -766,13 +771,9 @@ static int fsl_pme2_scan_ioctl(struct inode *inode, struct file *fp,
 		} while (!ret && completed_cmd && (i != results.num));
 
 		if (i != results.num) {
-#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
-			pr_info("pme2_scan: Only filled %d responses\n", i);
-#endif
+			PMEPRINFO("pme2_scan: Only filled %d responses\n", i);
 			results.num = i;
-#ifdef CONFIG_FSL_PME2_SCAN_DEBUG
-			pr_info("pme2_scan: results.num = %d\n", results.num);
-#endif
+			PMEPRINFO("pme2_scan: results.num = %d\n", results.num);
 			if (copy_to_user((void *)arg, &results,
 					sizeof(results))) {
 				pr_err("Error copying to user data \n");
diff --git a/include/linux/fsl_pme.h b/include/linux/fsl_pme.h
index c8271df..d63b3dd 100644
--- a/include/linux/fsl_pme.h
+++ b/include/linux/fsl_pme.h
@@ -405,7 +405,9 @@ enum pme_cmd_type {
 /* high-level functions */
 /************************/
 
+/* predeclaration of a private structure" */
 struct pme_ctx;
+struct pme_nostash;
 
 /* Calls to pme_ctx_scan() and pme_ctx_pmtcc() provide these, and they are
  * provided back in the completion callback. You can embed this within a larger
@@ -421,6 +423,8 @@ struct pme_ctx_token {
 struct pme_ctx_ctrl_token {
 	void (*cb)(struct pme_ctx *, const struct qm_fd *,
 			struct pme_ctx_ctrl_token *);
+	void (*ern_cb)(struct pme_ctx *, const struct qm_mr_entry *,
+			struct pme_ctx_ctrl_token *);
 	/* don't touch the rest */
 	struct pme_hw_flow *internal_flow_ptr;
 	struct pme_flow *usr_flow_ptr;
@@ -430,6 +434,11 @@ struct pme_ctx_ctrl_token {
 /* Scan results invoke a user-provided callback of this type */
 typedef void (*pme_scan_cb)(struct pme_ctx *, const struct qm_fd *,
 				struct pme_ctx_token *);
+/* Enqueue rejections may happen before order-restoration or after (eg. if due
+ * to congestion or tail-drop). Use * 'rc' code of the 'mr_entry' to
+ * determine. */
+typedef void (*pme_scan_ern_cb)(struct pme_ctx *, const struct qm_mr_entry *,
+				struct pme_ctx_token *);
 
 /* PME "association" - ie. connects two frame-queues, with or without a PME flow
  * (if not, direct action mode), and manages mux/demux of scans and flow-context
@@ -438,8 +447,11 @@ typedef void (*pme_scan_cb)(struct pme_ctx *, const struct qm_fd *,
  * structure as the first field in your own context structure. */
 struct pme_ctx {
 	struct qman_fq fq;
-	/* IMPORTANT: Set (only) this prior to calling pme_ctx_init(); */
+	/* IMPORTANT: Set (only) these two fields prior to calling *
+	 * pme_ctx_init(). 'ern_cb' can be NULL if you know you will not
+	 * receive enqueue rejections. */
 	pme_scan_cb cb;
+	pme_scan_ern_cb ern_cb;
 	/* These fields should not be manipulated directly. Also the structure
 	 * may change and/or grow, so avoid making any alignment or size
 	 * assumptions. */
@@ -448,15 +460,14 @@ struct pme_ctx {
 	spinlock_t lock;
 	wait_queue_head_t queue;
 	struct list_head tokens;
-	u32 seq_num;
 	/* TODO: the following "slow-path" values should be bundled into a
 	 * secondary structure so that sizeof(struct pme_ctx) is minimised (for
 	 * stashing of caller-side fast-path state). */
-	struct qman_fq *fqin;
 	struct pme_hw_flow *hw_flow;
 	struct pme_hw_residue *hw_residue;
 	struct qm_fqd_stashing stashing;
 	struct qm_fd update_fd;
+	struct pme_nostash *us_data;
 };
 
 /* Flags for pme_ctx_init() */
@@ -464,9 +475,6 @@ struct pme_ctx {
 #define PME_CTX_FLAG_EXCLUSIVE   0x00000002 /* unscheduled, exclusive mode */
 #define PME_CTX_FLAG_PMTCC       0x00000004 /* PMTCC rather than scanning */
 #define PME_CTX_FLAG_DIRECT      0x00000008 /* Direct Action mode (not Flow) */
-#define PME_CTX_FLAG_NO_ORP      0x00000010 /* Using this flags implies there
-					     * is no risk of enqueue misordering
-					     */
 #define PME_CTX_FLAG_LOCAL       0x00000020 /* Ignore dest, use cpu portal */
 
 /* Flags for operations */
@@ -543,27 +551,14 @@ int pme_ctx_reconfigure_tx(struct pme_ctx *ctx, u32 bpid, u8 qosin);
 int pme_ctx_reconfigure_rx(struct pme_ctx *ctx, u8 qosout,
 		enum qm_channel dest, const struct qm_fqd_stashing *stashing);
 
-/* Precondition: pme_ctx must be enabled */
-/* NB: _update() and _nop() only return failure if their PME commands weren't
- * sent. If PME_CTX_OP_WAIT_INT was specified and a signal was received while
- * waiting for the response, it may return prematurely with success. The caller
- * can use signal_pending() to deal with any corresponding issues, if required.
- * If WAIT isn't used, or if WAIT_INT is specified and a signal may have
- * returned prematurely before the PME replied to the control command, then
- * pme_ctx_in_ctrl() should be used to determine when the command is complete.
- * NB: as the return value indicates whether the command was issued (and not
- * what the device did in reaction to the command), device errors caused by
- * pme_ctx_ctrl_***() APIs should be detected by calling pme_ctx_is_dead() after
- * the operation completes.
+/* Precondition: pme_ctx must be enabled
+ * if PME_CTX_OP_WAIT is specified, it'll wait (if it has to) to start the ctrl
+ * command but never waits for it to complete. The callback serves that purpose.
  * NB: 'params' may be modified by this call. For instance if
  * PME_CTX_OP_RESETRESLEN was specified and residue is enabled, then the
  * params->ren will be set to 1 (in order not to disabled residue).
  * NB: _update() will overwrite the 'params->rptr_[hi/low]' fields since the
  * residue resource is managed by this layer.
- * NB: _read_flow() is a blocking/sleeping and uninterruptible API, so it must
- * not be called in atomic context and will not break due to signals.
- * PME_CTX_OP_WAIT flag will be assumed set and PME_CTX_OP_WAIT_INT will be
- * assumed cleared, irrespective of what is specified in 'flags'.
  */
 int pme_ctx_ctrl_update_flow(struct pme_ctx *ctx, u32 flags,
 		struct pme_flow *params, struct pme_ctx_ctrl_token *token);
-- 
1.6.5.2

