From 7b8884c4e1ee1431e4ba7ec0a692b0a70e0623a2 Mon Sep 17 00:00:00 2001
From: Sunil Goutham <sgoutham@cavium.com>
Date: Fri, 8 Aug 2014 16:26:12 +0200
Subject: [PATCH 425/430] VNIC: Adding support for Cavium ThunderX network
 controller

This commit comes from:
  git://git.kernel.org/pub/scm/linux/kernel/git/rric/linux.git

This patch adds support for the Cavium ThunderX network controller.
The driver is on the pci bus and thus requires the Thunder PCIe host
controller driver to be enabled.

Signed-off-by: Sunil Goutham <sgoutham@cavium.com>
Signed-off-by: Robert Richter <rrichter@cavium.com>
(cherry picked from commit 361ba940435dfc94dc8d974615b73e7faf039ebc)
Signed-off-by: Zumeng Chen <zumeng.chen@windriver.com>
---
 drivers/base/Kconfig                               |    1 +
 drivers/net/ethernet/Kconfig                       |    1 +
 drivers/net/ethernet/Makefile                      |    1 +
 drivers/net/ethernet/cavium/Kconfig                |   33 +
 drivers/net/ethernet/cavium/Makefile               |    5 +
 drivers/net/ethernet/cavium/thunder/Makefile       |   13 +
 drivers/net/ethernet/cavium/thunder/nic.h          |  335 ++++++
 drivers/net/ethernet/cavium/thunder/nic_main.c     |  616 +++++++++++
 drivers/net/ethernet/cavium/thunder/nic_reg.h      |  162 +++
 .../net/ethernet/cavium/thunder/nicvf_ethtool.c    |  198 ++++
 drivers/net/ethernet/cavium/thunder/nicvf_main.c   | 1122 ++++++++++++++++++++
 drivers/net/ethernet/cavium/thunder/nicvf_queues.c | 1052 ++++++++++++++++++
 drivers/net/ethernet/cavium/thunder/nicvf_queues.h |  182 ++++
 drivers/net/ethernet/cavium/thunder/q_struct.h     |  546 ++++++++++
 drivers/net/ethernet/cavium/thunder/thunder_bgx.c  |  248 +++++
 drivers/net/ethernet/cavium/thunder/thunder_bgx.h  |   27 +
 16 files changed, 4542 insertions(+), 0 deletions(-)
 create mode 100644 drivers/net/ethernet/cavium/Kconfig
 create mode 100644 drivers/net/ethernet/cavium/Makefile
 create mode 100644 drivers/net/ethernet/cavium/thunder/Makefile
 create mode 100644 drivers/net/ethernet/cavium/thunder/nic.h
 create mode 100644 drivers/net/ethernet/cavium/thunder/nic_main.c
 create mode 100644 drivers/net/ethernet/cavium/thunder/nic_reg.h
 create mode 100644 drivers/net/ethernet/cavium/thunder/nicvf_ethtool.c
 create mode 100644 drivers/net/ethernet/cavium/thunder/nicvf_main.c
 create mode 100644 drivers/net/ethernet/cavium/thunder/nicvf_queues.c
 create mode 100644 drivers/net/ethernet/cavium/thunder/nicvf_queues.h
 create mode 100644 drivers/net/ethernet/cavium/thunder/q_struct.h
 create mode 100644 drivers/net/ethernet/cavium/thunder/thunder_bgx.c
 create mode 100644 drivers/net/ethernet/cavium/thunder/thunder_bgx.h

diff --git a/drivers/base/Kconfig b/drivers/base/Kconfig
index 8fa8dea..87f8328 100644
--- a/drivers/base/Kconfig
+++ b/drivers/base/Kconfig
@@ -220,6 +220,7 @@ comment "Default contiguous memory area size:"
 config CMA_SIZE_MBYTES
 	int "Size in Mega Bytes"
 	depends on !CMA_SIZE_SEL_PERCENTAGE
+	default 256 if NET_VENDOR_CAVIUM
 	default 16
 	help
 	  Defines the size (in MiB) of the default memory area for Contiguous
diff --git a/drivers/net/ethernet/Kconfig b/drivers/net/ethernet/Kconfig
index 506b024..a22a248 100644
--- a/drivers/net/ethernet/Kconfig
+++ b/drivers/net/ethernet/Kconfig
@@ -31,6 +31,7 @@ source "drivers/net/ethernet/adi/Kconfig"
 source "drivers/net/ethernet/broadcom/Kconfig"
 source "drivers/net/ethernet/brocade/Kconfig"
 source "drivers/net/ethernet/calxeda/Kconfig"
+source "drivers/net/ethernet/cavium/Kconfig"
 source "drivers/net/ethernet/chelsio/Kconfig"
 source "drivers/net/ethernet/cirrus/Kconfig"
 source "drivers/net/ethernet/cisco/Kconfig"
diff --git a/drivers/net/ethernet/Makefile b/drivers/net/ethernet/Makefile
index c0b8789..ad56952 100644
--- a/drivers/net/ethernet/Makefile
+++ b/drivers/net/ethernet/Makefile
@@ -17,6 +17,7 @@ obj-$(CONFIG_NET_BFIN) += adi/
 obj-$(CONFIG_NET_VENDOR_BROADCOM) += broadcom/
 obj-$(CONFIG_NET_VENDOR_BROCADE) += brocade/
 obj-$(CONFIG_NET_CALXEDA_XGMAC) += calxeda/
+obj-$(CONFIG_NET_VENDOR_CAVIUM) += cavium/
 obj-$(CONFIG_NET_VENDOR_CHELSIO) += chelsio/
 obj-$(CONFIG_NET_VENDOR_CIRRUS) += cirrus/
 obj-$(CONFIG_NET_VENDOR_CISCO) += cisco/
diff --git a/drivers/net/ethernet/cavium/Kconfig b/drivers/net/ethernet/cavium/Kconfig
new file mode 100644
index 0000000..15004b9
--- /dev/null
+++ b/drivers/net/ethernet/cavium/Kconfig
@@ -0,0 +1,33 @@
+#
+# Cavium ethernet device configuration
+#
+
+config NET_VENDOR_CAVIUM
+	tristate "Cavium ethernet drivers"
+	depends on PCI
+	---help---
+	  If you have Cavium Thunder board, say Y.
+
+if NET_VENDOR_CAVIUM
+
+config THUNDER_NIC_PF
+	tristate "Thunder Physical function driver"
+	default NET_VENDOR_CAVIUM
+	select THUNDER_NIC_BGX
+	---help---
+	  This driver supports Thunder's NIC physical function.
+
+config THUNDER_NIC_VF
+	tristate "Thunder Virtual function driver"
+	default NET_VENDOR_CAVIUM
+	---help---
+	  This driver supports Thunder's NIC virtual function
+
+config	THUNDER_NIC_BGX
+	tristate "Thunder MAC interface driver(BGX)"
+	default NET_VENDOR_CAVIUM
+	---help---
+	  This driver supports programming and controlling of MAC
+	  interface from NIC physical function driver.
+
+endif # NET_VENDOR_CAVIUM
diff --git a/drivers/net/ethernet/cavium/Makefile b/drivers/net/ethernet/cavium/Makefile
new file mode 100644
index 0000000..7aac478
--- /dev/null
+++ b/drivers/net/ethernet/cavium/Makefile
@@ -0,0 +1,5 @@
+#
+# Makefile for the Cavium ethernet device drivers.
+#
+
+obj-$(CONFIG_NET_VENDOR_CAVIUM) += thunder/
diff --git a/drivers/net/ethernet/cavium/thunder/Makefile b/drivers/net/ethernet/cavium/thunder/Makefile
new file mode 100644
index 0000000..3998567
--- /dev/null
+++ b/drivers/net/ethernet/cavium/thunder/Makefile
@@ -0,0 +1,13 @@
+#
+# Makefile for Cavium's Thunder ethernet device
+#
+
+obj-$(CONFIG_THUNDER_NIC_PF) += nicpf.o
+obj-$(CONFIG_THUNDER_NIC_VF) += nicvf.o
+obj-$(CONFIG_THUNDER_NIC_BGX) += thunder_bgx.o
+
+nicpf-y := nic_main.o
+nicvf-y := nicvf_main.o nicvf_queues.o
+#ifdef NICVF_ETHTOOL_ENABLE
+nicvf-y += nicvf_ethtool.o
+#endif
diff --git a/drivers/net/ethernet/cavium/thunder/nic.h b/drivers/net/ethernet/cavium/thunder/nic.h
new file mode 100644
index 0000000..7417bcb
--- /dev/null
+++ b/drivers/net/ethernet/cavium/thunder/nic.h
@@ -0,0 +1,335 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2014 Cavium, Inc.
+ */
+
+#ifndef NIC_H
+#define	NIC_H
+
+#include <linux/netdevice.h>
+#include <linux/interrupt.h>
+
+/* PCI device IDs */
+#define	PCI_DEVICE_ID_THUNDER_NIC_PF	0xA01E
+#define	PCI_DEVICE_ID_THUNDER_NIC_VF	0x0011
+#define	PCI_DEVICE_ID_THUNDER_BGX	0xA026
+
+/* PCI BAR nos */
+#define	PCI_CFG_REG_BAR_NUM		0
+#define	PCI_MSIX_REG_BAR_NUM		4
+
+/* NIC SRIOV VF count */
+#define	MAX_NUM_VFS_SUPPORTED		128
+#define	DEFAULT_NUM_VF_ENABLED		8
+
+/* NIC priv flags */
+#define	NIC_SRIOV_ENABLED		(1 << 0)
+
+/* VNIC HW optimiation features */
+#undef	VNIC_RX_CSUM_OFFLOAD_SUPPORT
+#undef	VNIC_TX_CSUM_OFFLOAD_SUPPORT
+#define	VNIC_SG_SUPPORT
+#define	VNIC_TSO_SUPPORT
+#define	VNIC_LRO_SUPPORT
+
+/* TSO not supported in Thunder pass1 */
+#ifdef	VNIC_TSO_SUPPORT
+#define	VNIC_SW_TSO_SUPPORT
+#undef	VNIC_HW_TSO_SUPPORT
+#endif
+
+/* LRO not supported even in Thunder pass2 */
+#ifdef	VNIC_LRO_SUPPORT
+#define	VNIC_SW_LRO_SUPPORT
+#undef	VNIC_HW_LRO_SUPPORT
+#endif
+
+
+/* ETHTOOL enable or disable, undef this to disable */
+#define	NICVF_ETHTOOL_ENABLE
+
+/* NAPI enable or disable, undef this to disable */
+#define	NICVF_NAPI_ENABLE
+
+/* Min/Max packet size */
+#define	NIC_HW_MIN_FRS			64
+#define	NIC_HW_MAX_FRS			1500
+
+/* Max pkinds */
+#define	NIC_MAX_PKIND			16
+
+/* Rx Channels */
+#define	NIC_MAX_BGX			2
+#define	NIC_CHANS_PER_BGX_INF		128
+#define	NIC_MAX_CHANS			(NIC_MAX_BGX * NIC_CHANS_PER_BGX_INF)
+#define	NIC_MAX_CPI			2048 /* Channel parse index */
+#define	NIC_MAX_RSSI			4096 /* Receive side scaling index */
+
+/* TNS bi-pass mode: 1-1 mapping between VNIC and LMAC */
+#define	NIC_CPI_PER_BGX			(NIC_MAX_CPI / NIC_MAX_BGX)
+#define	NIC_CPI_PER_LMAC		(NIC_MAX_CPI / NIC_MAX_CHANS)
+#define	NIC_RSSI_PER_BGX		(NIC_MAX_RSSI / NIC_MAX_BGX)
+#define	NIC_RSSI_PER_LMAC		(NIC_MAX_RSSI / NIC_MAX_CHANS)
+
+/* Tx scheduling */
+#define	NIC_MAX_TL4			1024
+#define	NIC_MAX_TL4_SHAPERS		256 /* 1 shaper for 4 TL4s */
+#define	NIC_MAX_TL3			256
+#define	NIC_MAX_TL3_SHAPERS		64  /* 1 shaper for 4 TL3s */
+#define	NIC_MAX_TL2			64
+#define	NIC_MAX_TL2_SHAPERS		2  /* 1 shaper for 32 TL2s */
+#define	NIC_MAX_TL1			2
+
+/* TNS bi-pass mode */
+#define	NIC_TL4_PER_BGX			(NIC_MAX_TL4 / NIC_MAX_BGX)
+#define	NIC_TL4_PER_LMAC		(NIC_MAX_TL4 / NIC_CHANS_PER_BGX_INF)
+
+/* NIC VF Interrupts */
+#define	NICVF_INTR_CQ			0
+#define	NICVF_INTR_SQ			1
+#define	NICVF_INTR_RBDR			2
+#define	NICVF_INTR_PKT_DROP		3
+#define	NICVF_INTR_TCP_TIMER		4
+#define	NICVF_INTR_MBOX			5
+#define	NICVF_INTR_QS_ERR		6
+
+#define	NICVF_INTR_CQ_SHIFT		0
+#define	NICVF_INTR_SQ_SHIFT		8
+#define	NICVF_INTR_RBDR_SHIFT		16
+#define	NICVF_INTR_PKT_DROP_SHIFT	20
+#define	NICVF_INTR_TCP_TIMER_SHIFT	21
+#define	NICVF_INTR_MBOX_SHIFT		22
+#define	NICVF_INTR_QS_ERR_SHIFT		23
+
+#define	NICVF_INTR_CQ_MASK		(0xFF << NICVF_INTR_CQ_SHIFT)
+#define	NICVF_INTR_SQ_MASK		(0xFF << NICVF_INTR_SQ_SHIFT)
+#define	NICVF_INTR_RBDR_MASK		(0x03 << NICVF_INTR_RBDR_SHIFT)
+#define	NICVF_INTR_PKT_DROP_MASK	(1 << NICVF_INTR_PKT_DROP_SHIFT)
+#define	NICVF_INTR_TCP_TIMER_MASK	(1 << NICVF_INTR_TCP_TIMER_SHIFT)
+#define	NICVF_INTR_MBOX_MASK		(1 << NICVF_INTR_MBOX_SHIFT)
+#define	NICVF_INTR_QS_ERR_MASK		(1 << NICVF_INTR_QS_ERR_SHIFT)
+
+/* MSI-X interrupts */
+#define	NIC_PF_MSIX_VECTORS		10
+#define	NIC_VF_MSIX_VECTORS		20
+
+#define	NICVF_CQ_INTR_ID		0
+#define	NICVF_SQ_INTR_ID		8
+#define	NICVF_RBDR_INTR_ID		16
+#define	NICVF_MISC_INTR_ID		18
+#define	NICVF_QS_ERR_INTR_ID		19
+
+#define for_each_cq_irq(irq) for (irq = NICVF_CQ_INTR_ID; \
+					irq < NICVF_SQ_INTR_ID; irq++)
+#define for_each_sq_irq(irq) for (irq = NICVF_SQ_INTR_ID; \
+					irq < NICVF_RBDR_INTR_ID; irq++)
+#define for_each_rbdr_irq(irq) for (irq = NICVF_RBDR_INTR_ID; \
+					irq < NICVF_MISC_INTR_ID; irq++)
+
+struct nicvf_cq_poll {
+	uint8_t	cq_idx;		/* Completion queue index */
+	struct napi_struct napi;
+};
+
+#ifdef NICVF_ETHTOOL_ENABLE
+
+/* Stats */
+
+/* Tx statistics */
+struct nicvf_tx_stats {
+	u64 tx_frames_ok;
+	u64 tx_unicast_frames_ok;
+	u64 tx_multicast_frames_ok;
+	u64 tx_broadcast_frames_ok;
+	u64 tx_bytes_ok;
+	u64 tx_unicast_bytes_ok;
+	u64 tx_multicast_bytes_ok;
+	u64 tx_broadcast_bytes_ok;
+	u64 tx_drops;
+	u64 tx_errors;
+	u64 tx_tso;
+	u64 rsvd[16];
+};
+
+/* Rx statistics */
+struct nicvf_rx_stats {
+	u64 rx_frames_ok;
+	u64 rx_frames_total;
+	u64 rx_unicast_frames_ok;
+	u64 rx_multicast_frames_ok;
+	u64 rx_broadcast_frames_ok;
+	u64 rx_bytes_ok;
+	u64 rx_unicast_bytes_ok;
+	u64 rx_multicast_bytes_ok;
+	u64 rx_broadcast_bytes_ok;
+	u64 rx_drop;
+	u64 rx_no_bufs;
+	u64 rx_errors;
+	u64 rx_rss;
+	u64 rx_crc_errors;
+	u64 rx_frames_64;
+	u64 rx_frames_127;
+	u64 rx_frames_255;
+	u64 rx_frames_511;
+	u64 rx_frames_1023;
+	u64 rx_frames_1518;
+	u64 rx_frames_jumbo;
+	u64 rsvd[16];
+};
+
+struct eth_stats {
+	struct nicvf_tx_stats tx;
+	struct nicvf_rx_stats rx;
+};
+
+#endif /* NICVF_ETHTOOL_ENABLE */
+
+struct nicvf {
+	struct net_device	*netdev;
+	struct pci_dev		*pdev;
+	uint16_t		mtu;
+	uint8_t			vnic_id;
+	struct queue_set	*qs;		/* Queue set this VNIC is pointing to */
+	uint8_t			num_qs;		/* No of QSs assigned to this VNIC */
+	void			*addnl_qs;	/* Pointer to QSs additional to default 1 QS */
+	uint16_t		vf_mtu;
+	uint64_t		reg_base;	/* Register start address */
+	struct tasklet_struct	rbdr_task;	/* Tasklet to refill RBDR */
+	struct tasklet_struct	qs_err_task;	/* Tasklet to handle Qset err */
+#ifdef NICVF_NAPI_ENABLE
+	struct nicvf_cq_poll	*napi[8];	/* NAPI */
+#endif
+	/* MSI-X  */
+	bool			msix_enabled;
+	uint16_t		num_vec;
+	struct msix_entry	msix_entries[NIC_VF_MSIX_VECTORS];
+	char			irq_name[NIC_VF_MSIX_VECTORS][20];
+	uint8_t			irq_allocated[NIC_VF_MSIX_VECTORS];
+#ifdef NICVF_ETHTOOL_ENABLE
+	struct eth_stats	vstats;
+#endif
+};
+
+struct nicpf {
+	struct net_device	*netdev;
+	struct pci_dev		*pdev;
+	unsigned int		flags;
+	uint16_t		total_vf_cnt;   /* Total num of VF supported */
+	uint16_t		num_vf_en;      /* No of VF enabled */
+	uint64_t		reg_base;       /* Register start address */
+	struct pkind_cfg	pkind;
+	/* MSI-X */
+	bool			msix_enabled;
+	uint16_t		num_vec;
+	struct msix_entry	msix_entries[NIC_PF_MSIX_VECTORS];
+	uint8_t			irq_allocated[NIC_PF_MSIX_VECTORS];
+};
+
+struct nicvf_stats {
+	struct {
+		uint32_t partial_pkts;
+		uint32_t jabber_errs;
+		uint32_t fcs_errs;
+		uint32_t terminate_errs;
+		uint32_t bgx_rx_errs;
+		uint32_t prel2_errs;
+		uint32_t l2_frags;
+		uint32_t l2_overruns;
+		uint32_t l2_pfcs;
+		uint32_t l2_puny;
+		uint32_t l2_mal;
+		uint32_t l2_oversize;
+		uint32_t l2_len_mismatch;
+		uint32_t l2_pclp;
+		uint32_t not_ip;
+		uint32_t ip_csum_err;
+		uint32_t ip_mal;
+		uint32_t ip_mal_payload;
+		uint32_t ip_hop;
+		uint32_t l3_icrc;
+		uint32_t l3_pclp;
+		uint32_t l4_mal;
+		uint32_t l4_csum_err;
+		uint32_t udp_len_err;
+		uint32_t bad_l4_port;
+		uint32_t bad_tcp_flag;
+		uint32_t tcp_offset_err;
+		uint32_t l4_pclp;
+		uint32_t no_rbdr;
+	} rx;
+	struct {
+	} tx;
+};
+
+/* PF <--> Mailbox communication
+ * Eight 64bit registers are shared between PF and VF.
+ * Separate set for each VF.
+ * Writing '1' into last register mbx7 means end of message.
+ */
+
+/* PF <--> VF mailbox communication */
+#define	NIC_PF_VF_MAILBOX_SIZE		8
+
+/* Mailbox message types */
+#define	NIC_PF_VF_MSG_READY		0x01	/* Is PF ready to rcv msgs */
+#define	NIC_PF_VF_MSG_ACK		0x02	/* ACK the message received */
+#define	NIC_PF_VF_MSG_NACK		0x03	/* NACK the message received */
+#define	NIC_PF_VF_MSG_QS_CFG		0x04	/* Configure Qset */
+#define	NIC_PF_VF_MSG_RQ_CFG		0x05	/* Configure receive queue */
+#define	NIC_PF_VF_MSG_SQ_CFG		0x06	/* Configure Send queue */
+#define	NIC_PF_VF_MSG_RQ_DROP_CFG	0x07	/* Configure receive queue */
+#define	NIC_PF_VF_MSG_SET_MAC		0x08	/* Add MAC ID to BGX's DMAC filter */
+#define	NIC_VF_SET_MAX_FRS		0x09	/* Set max frame size */
+
+struct nic_mbx {
+	uint64_t	   msg;
+	union	{
+		uint64_t	vnic_id;
+		struct {			/* Qset configuration */
+			uint64_t   num;
+			uint64_t   cfg;
+		} qs;
+		struct {			/* Receive queue configuration */
+			uint64_t   qs_num;
+			uint64_t   rq_num;
+			uint64_t   cfg;
+		} rq;
+		struct {			/* Send queue configuration */
+			uint64_t   qs_num;
+			uint64_t   sq_num;
+			uint64_t   cfg;
+		} sq;
+		struct {			/* VF's MAC address */
+			uint64_t   vnic_id;
+			uint64_t   addr;
+		} mac;
+		uint64_t	max_frs; /* Max frame size */
+	} data;
+	uint64_t	   mbx4;
+	uint64_t	   mbx5;
+	uint64_t	   mbx6;
+	uint64_t	   mbx_trigger_intr;
+};
+
+#ifdef NICVF_ETHTOOL_ENABLE
+void nicvf_set_ethtool_ops(struct net_device *netdev);
+#endif
+
+struct nic_mbx *nicvf_get_mbx(void);
+int nicvf_send_msg_to_pf(struct nicvf *vf, struct nic_mbx *mbx);
+void nicvf_free_skb(struct nicvf *nic, struct sk_buff *skb);
+
+/* Debug */
+#undef	NIC_DEBUG
+
+#ifdef	NIC_DEBUG
+#define	nic_dbg(dev, fmt, arg...) \
+		dev_info(dev, fmt, ##arg)
+#else
+#define	nic_dbg(dev, fmt, arg...) do {} while (0)
+#endif
+
+#endif /* NIC_H */
diff --git a/drivers/net/ethernet/cavium/thunder/nic_main.c b/drivers/net/ethernet/cavium/thunder/nic_main.c
new file mode 100644
index 0000000..0f7098c
--- /dev/null
+++ b/drivers/net/ethernet/cavium/thunder/nic_main.c
@@ -0,0 +1,616 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2014 Cavium, Inc.
+ */
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/string.h>
+#include <linux/errno.h>
+#include <linux/types.h>
+#include <linux/init.h>
+#include <linux/interrupt.h>
+#include <linux/workqueue.h>
+#include <linux/pci.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/if.h>
+#include <linux/if_ether.h>
+#include <linux/if_vlan.h>
+#include <linux/ethtool.h>
+#include <linux/aer.h>
+
+#include "nic_reg.h"
+#include "nic.h"
+#include "thunder_bgx.h"
+
+#define DRV_NAME	"thunder-nic"
+#define DRV_VERSION	"1.0"
+
+static void nic_channel_cfg(struct nicpf *nic, int vnic);
+static int nic_update_hw_frs(struct nicpf *nic, int new_frs, int vf);
+
+/* Supported devices */
+static DEFINE_PCI_DEVICE_TABLE(nic_id_table) = {
+	{ PCI_DEVICE(PCI_VENDOR_ID_CAVIUM, PCI_DEVICE_ID_THUNDER_NIC_PF) },
+	{ 0, }  /* end of table */
+};
+
+MODULE_AUTHOR("Cavium Inc");
+MODULE_DESCRIPTION("Cavium Thunder Physical Function Driver");
+MODULE_LICENSE("GPL");
+MODULE_VERSION(DRV_VERSION);
+MODULE_DEVICE_TABLE(pci, nic_id_table);
+
+/* Register read/write APIs */
+static void nic_reg_write(struct nicpf *nic, uint64_t offset, uint64_t val)
+{
+	uint64_t addr = nic->reg_base + offset;
+
+	writeq_relaxed(val, (void *)addr);
+}
+
+static uint64_t nic_reg_read(struct nicpf *nic, uint64_t offset)
+{
+	uint64_t addr = nic->reg_base + offset;
+
+	return readq_relaxed((void *)addr);
+}
+
+/* PF -> VF mailbox communication APIs */
+static void nic_enable_mbx_intr(struct nicpf *nic)
+{
+	int	 irq;
+	uint64_t vf_mbx_intr_enable = 0;
+
+	/* TBD: Need to support runtime SRIOV VF count configuratuon */
+	/* Or consider enabling all VF's interrupts, since there is no harm */
+	for (irq = 0; irq < 64; irq++)
+		if (irq < nic->num_vf_en)
+			vf_mbx_intr_enable |= (1 << irq);
+	nic_reg_write(nic, NIC_PF_MAILBOX_ENA_W1S, vf_mbx_intr_enable);
+
+	if (nic->num_vf_en < 64)
+		return;
+
+	vf_mbx_intr_enable = 0;
+	for (irq = 0; irq < 64; irq++)
+		if (irq < (nic->num_vf_en - 64))
+			vf_mbx_intr_enable |= (1 << irq);
+	nic_reg_write(nic, NIC_PF_MAILBOX_ENA_W1S + (1 << 3), vf_mbx_intr_enable);
+}
+
+static uint64_t nic_get_mbx_intr_status(struct nicpf *nic, int mbx_reg)
+{
+	if (!mbx_reg)	/* first 64 VFs */
+		return nic_reg_read(nic, NIC_PF_MAILBOX_INT);
+	else		/* Next 64 VFs */
+		return nic_reg_read(nic, NIC_PF_MAILBOX_INT + (1 << 3));
+}
+
+static void nic_clear_mbx_intr(struct nicpf *nic, int vf)
+{
+	if (!(vf / 64))	/* first 64 VFs */
+		nic_reg_write(nic, NIC_PF_MAILBOX_INT, (1ULL << vf));
+	else		/* Next 64 VFs */
+		nic_reg_write(nic, NIC_PF_MAILBOX_INT + (1 << 3), (1ULL << (vf - 64)));
+}
+
+static uint64_t nic_get_mbx_addr(int vf)
+{
+	return NIC_PF_VF_0_127_MAILBOX_0_7 + (vf << NIC_VF_NUM_SHIFT);
+}
+
+static void nic_mbx_done(struct nicpf *nic, uint64_t mbx_addr)
+{
+	/* write 1 to last MBX reg */
+	mbx_addr += (NIC_PF_VF_MAILBOX_SIZE - 1) * 8;
+	nic_reg_write(nic, mbx_addr, 1ULL);
+}
+
+static void nic_mbx_send_ready(struct nicpf *nic, int vf)
+{
+	uint64_t mbx_addr;
+
+	mbx_addr = nic_get_mbx_addr(vf);
+
+	/* Respond with VNIC ID */
+	nic_reg_write(nic, mbx_addr, NIC_PF_VF_MSG_READY);
+	nic_reg_write(nic, mbx_addr + 8, vf);
+	nic_mbx_done(nic, mbx_addr);
+}
+
+static void nic_mbx_send_ack(struct nicpf *nic, int vf)
+{
+	uint64_t mbx_addr;
+
+	mbx_addr = nic_get_mbx_addr(vf);
+
+	nic_reg_write(nic, mbx_addr, NIC_PF_VF_MSG_ACK);
+	nic_mbx_done(nic, mbx_addr);
+}
+
+static void nic_mbx_send_nack(struct nicpf *nic, int vf)
+{
+	uint64_t mbx_addr;
+
+	mbx_addr = nic_get_mbx_addr(vf);
+
+	nic_reg_write(nic, mbx_addr, NIC_PF_VF_MSG_NACK);
+	nic_mbx_done(nic, mbx_addr);
+}
+
+/* Handle Mailbox messgaes from VF and ack the message. */
+static void nic_handle_mbx_intr(struct nicpf *nic, int vf)
+{
+	int i, ret = 0;
+	struct nic_mbx *mbx;
+	uint64_t *mbx_data;
+	uint64_t reg_addr;
+	uint64_t mbx_addr;
+
+	mbx_addr = NIC_PF_VF_0_127_MAILBOX_0_7;
+	mbx_addr += (vf << NIC_VF_NUM_SHIFT);
+
+	mbx = kzalloc(sizeof(*mbx), GFP_KERNEL);
+	mbx_data = (uint64_t *)mbx;
+
+	for (i = 0; i < NIC_PF_VF_MAILBOX_SIZE; i++) {
+		*mbx_data = nic_reg_read(nic, mbx_addr + (i * NIC_PF_VF_MAILBOX_SIZE));
+		mbx_data++;
+	}
+
+	mbx->msg &= 0xFF;
+	nic_dbg(&nic->pdev->dev, "%s: Mailbox msg %d from VF%d\n",
+		__func__, mbx->msg, vf);
+	switch (mbx->msg) {
+	case NIC_PF_VF_MSG_READY:
+		nic_mbx_send_ready(nic, vf);
+		ret = 1;
+		break;
+	case NIC_PF_VF_MSG_QS_CFG:
+		reg_addr = NIC_PF_QSET_0_127_CFG | (mbx->data.qs.num << NIC_QS_ID_SHIFT);
+		nic_reg_write(nic, reg_addr, mbx->data.qs.cfg);
+		nic_channel_cfg(nic, mbx->data.qs.num);
+		if (!mbx->data.qs.cfg)
+			bgx_lmac_disable(mbx->data.qs.num);
+		else
+			bgx_lmac_enable(mbx->data.qs.num);
+		break;
+	case NIC_PF_VF_MSG_RQ_CFG:
+		reg_addr = NIC_PF_QSET_0_127_RQ_0_7_CFG | (mbx->data.rq.qs_num << NIC_QS_ID_SHIFT) |
+							  (mbx->data.rq.rq_num << NIC_Q_NUM_SHIFT);
+		nic_reg_write(nic, reg_addr, mbx->data.rq.cfg);
+		break;
+	case NIC_PF_VF_MSG_RQ_DROP_CFG:
+		reg_addr = NIC_PF_QSET_0_127_RQ_0_7_DROP_CFG | (mbx->data.rq.qs_num << NIC_QS_ID_SHIFT) |
+								(mbx->data.rq.rq_num << NIC_Q_NUM_SHIFT);
+		nic_reg_write(nic, reg_addr, mbx->data.rq.cfg);
+		break;
+	case NIC_PF_VF_MSG_SQ_CFG:
+		reg_addr = NIC_PF_QSET_0_127_SQ_0_7_CFG | (mbx->data.sq.qs_num << NIC_QS_ID_SHIFT) |
+							  (mbx->data.sq.sq_num << NIC_Q_NUM_SHIFT);
+		nic_reg_write(nic, reg_addr, mbx->data.sq.cfg);
+		break;
+	case NIC_PF_VF_MSG_SET_MAC:
+		bgx_add_dmac_addr(mbx->data.mac.addr, mbx->data.mac.vnic_id);
+		break;
+	case NIC_VF_SET_MAX_FRS:
+		ret = nic_update_hw_frs(nic, mbx->data.max_frs, vf);
+		break;
+	default:
+		netdev_err(nic->netdev, "Invalid message from VF%d, msg 0x%llx\n", vf, mbx->msg);
+		break;
+	}
+
+	if (!ret)
+		nic_mbx_send_ack(nic, vf);
+	else if (mbx->msg != NIC_PF_VF_MSG_READY)
+		nic_mbx_send_nack(nic, vf);
+	kfree(mbx);
+}
+
+static int nic_update_hw_frs(struct nicpf *nic, int new_frs, int vf)
+{
+	if ((new_frs > NIC_HW_MAX_FRS) || (new_frs < NIC_HW_MIN_FRS)) {
+		netdev_err(nic->netdev,
+			   "Invalid MTU setting from VF%d rejected"
+			   "should be between %d and %d\n", vf,
+			   NIC_HW_MIN_FRS, NIC_HW_MAX_FRS);
+		return 1;
+	}
+	new_frs += ETH_HLEN;
+	if (new_frs <= nic->pkind.maxlen)
+		return 0;
+
+	nic->pkind.maxlen = new_frs;
+	nic_reg_write(nic, NIC_PF_PKIND_0_15_CFG, *(uint64_t *)&nic->pkind);
+	return 0;
+}
+
+static void nic_init_hw(struct nicpf *nic)
+{
+	int i;
+
+	/* Reset NIC, incase if driver is repeatedly inserted and removed */
+	nic_reg_write(nic, NIC_PF_SOFT_RESET, 1);
+
+	/* Enable NIC HW block */
+	nic_reg_write(nic, NIC_PF_CFG, 1);
+
+	/* Disable TNS mode */
+	nic_reg_write(nic, NIC_PF_INTF_0_1_SEND_CFG, 0);
+	nic_reg_write(nic, NIC_PF_INTF_0_1_SEND_CFG | (1 << 8), 0);
+
+	/* PKIND configuration */
+	nic->pkind.minlen = 0;
+	nic->pkind.maxlen = NIC_HW_MAX_FRS + ETH_HLEN;
+	nic->pkind.lenerr_en = 1;
+	nic->pkind.rx_hdr = 0;
+	nic->pkind.hdr_sl = 0;
+
+	for (i = 0; i < NIC_MAX_PKIND; i++)
+		nic_reg_write(nic, NIC_PF_PKIND_0_15_CFG | (i << 3),
+			       *(uint64_t *)&nic->pkind);
+
+	/* Disable backpressure for now */
+	for (i = 0; i < NIC_MAX_CHANS; i++)
+		nic_reg_write(nic, NIC_PF_CHAN_0_255_TX_CFG | (i << 3), 0);
+}
+
+static void nic_channel_cfg(struct nicpf *nic, int vnic)
+{
+	uint8_t  rq_idx = 0;
+	uint8_t  sq_idx = 0;
+	uint32_t bgx, lmac, chan, tl3, tl4;
+	uint64_t cpi_base, rssi_base;
+
+	/* Below are the channel mappings
+	 * BGX0-LMAC0-CHAN0 - VNIC CHAN0
+	 * BGX0-LMAC1-CHAN0 - VNIC CHAN16
+	 * ...
+	 * BGX1-LMAC0-CHAN0 - VNIC CHAN128
+	 * ...
+	 * BGX1-LMAC3-CHAN0 - VNIC CHAN174
+	 */
+	bgx = vnic / MAX_LMAC_PER_BGX;
+	lmac = vnic - (bgx * MAX_LMAC_PER_BGX);
+	chan = (lmac * MAX_BGX_CHANS_PER_LMAC) + (bgx * NIC_CHANS_PER_BGX_INF);
+	cpi_base = (lmac * NIC_CPI_PER_LMAC) + (bgx * NIC_CPI_PER_BGX);
+	rssi_base = (lmac * NIC_RSSI_PER_LMAC) + (bgx * NIC_RSSI_PER_BGX);
+
+	nic_reg_write(nic, NIC_PF_CHAN_0_255_RX_CFG | (chan << 3),
+		      cpi_base << 48);
+	nic_reg_write(nic, NIC_PF_CPI_0_2047_CFG | (cpi_base << 3),
+		      (vnic << 24) | rssi_base);
+	/* RQ's QS & RQ idx within QS */
+	nic_reg_write(nic, NIC_PF_RSSI_0_4097_RQ | (rssi_base << 3),
+		      (vnic << 3) | rq_idx);
+
+	/* Transmit Channel config (TL4 -> TL3 -> Chan) */
+	/* VNIC0-SQ0 -> TL4(0)  -> TL4A(0) -> TL3[0] -> BGX0/LMAC0/Chan0
+	 * VNIC1-SQ0 -> TL4(8)  -> TL4A(2) -> TL3[2] -> BGX0/LMAC1/Chan0
+	 * VNIC2-SQ0 -> TL4(16) -> TL4A(4) -> TL3[4] -> BGX0/LMAC2/Chan0
+	 * VNIC3-SQ0 -> TL4(32) -> TL4A(6) -> TL3[6] -> BGX0/LMAC3/Chan0
+	 * VNIC4-SQ0 -> TL4(512)  -> TL4A(128) -> TL3[128] -> BGX1/LMAC0/Chan0
+	 * VNIC5-SQ0 -> TL4(520)  -> TL4A(130) -> TL3[130] -> BGX1/LMAC1/Chan0
+	 * VNIC6-SQ0 -> TL4(528)  -> TL4A(132) -> TL3[132] -> BGX1/LMAC2/Chan0
+	 * VNIC7-SQ0 -> TL4(536)  -> TL4A(134) -> TL3[134] -> BGX1/LMAC3/Chan0
+	 */
+	tl4 = (lmac * NIC_TL4_PER_LMAC) + (bgx * NIC_TL4_PER_BGX);
+
+	for (sq_idx = 0; sq_idx < 8; sq_idx++) {
+		tl4 = tl4 + sq_idx;
+		tl3 = tl4 / (NIC_MAX_TL4 / NIC_MAX_TL3);
+		nic_reg_write(nic, NIC_PF_QSET_0_127_SQ_0_7_CFG2 |
+					(vnic << NIC_QS_ID_SHIFT) |
+					(sq_idx << NIC_Q_NUM_SHIFT), tl4);
+		nic_reg_write(nic, NIC_PF_TL4_0_1023_CFG | (tl4 << 3),
+			      (vnic << 27) | (sq_idx << 24));
+		nic_reg_write(nic, NIC_PF_TL4A_0_255_CFG | (tl3 << 3), tl3);
+		nic_reg_write(nic, NIC_PF_TL3_0_255_CHAN | (tl3 << 3),
+			      (lmac << 4));
+	}
+}
+
+static irqreturn_t nic_intr_handler (int irq, void *nic_irq)
+{
+	int vf;
+	uint64_t intr;
+	struct nicpf *nic = (struct nicpf *)nic_irq;
+
+	intr = nic_get_mbx_intr_status(nic, 0); /* Mbox 0 */
+	nic_dbg(&nic->pdev->dev, "PF MSIX interrupt 0x%llx\n", intr);
+	for (vf = 0; vf < nic->num_vf_en; vf++) {
+		if (intr & (1 << vf)) {
+			nic_dbg(&nic->pdev->dev, "Intr from VF %d\n", vf);
+			nic_handle_mbx_intr(nic, vf);
+			nic_clear_mbx_intr(nic, vf);
+		}
+	}
+
+	return IRQ_HANDLED;
+}
+
+static int nic_enable_msix(struct nicpf *nic)
+{
+	int i, ret;
+
+	nic->num_vec = NIC_PF_MSIX_VECTORS;
+
+	for (i = 0; i < nic->num_vec; i++)
+		nic->msix_entries[i].entry = i;
+
+	ret = pci_enable_msix(nic->pdev, nic->msix_entries, nic->num_vec);
+	if (ret < 0) {
+		netdev_err(nic->netdev,
+			"Request for #%d msix vectors failed\n", nic->num_vec);
+		return 0;
+	} else if (ret > 0) {
+		netdev_err(nic->netdev,
+			"Request for #%d msix vectors failed, requesting #%d\n",
+			nic->num_vec, ret);
+
+		nic->num_vec = ret;
+		ret = pci_enable_msix(nic->pdev, nic->msix_entries, nic->num_vec);
+		if (ret) {
+			netdev_warn(nic->netdev, "Request for msix vectors failed\n");
+			return 0;
+		}
+	}
+
+	nic->msix_enabled = 1;
+	return 1;
+}
+
+static void nic_disable_msix(struct nicpf *nic)
+{
+	if (nic->msix_enabled) {
+		pci_disable_msix(nic->pdev);
+		nic->msix_enabled = 0;
+		nic->num_vec = 0;
+	}
+}
+
+static int nic_register_interrupts(struct nicpf *nic)
+{
+	int irq, free, ret = 0;
+
+	/* Enable MSI-X */
+	if (!nic_enable_msix(nic))
+		return 1;
+
+	/* Register interrupts */
+	/* For now skip ECC interrupts, register only Mbox interrupts */
+	for (irq = 8; irq < nic->num_vec; irq++) {
+		ret = request_irq(nic->msix_entries[irq].vector,
+				nic_intr_handler, 0 , "NIC PF", nic);
+		if (ret)
+			break;
+	}
+
+	if (ret) {
+		netdev_err(nic->netdev, "Request irq failed\n");
+		for (free = 0; free < irq; free++)
+			free_irq(nic->msix_entries[free].vector, nic);
+		return 1;
+	}
+
+	/* Enable mailbox interrupt */
+	nic_enable_mbx_intr(nic);
+
+	return 0;
+}
+
+static void nic_unregister_interrupts(struct nicpf *nic)
+{
+	int irq;
+
+	/* Free registered interrupts */
+	for (irq = 0; irq < nic->num_vec; irq++)
+		free_irq(nic->msix_entries[irq].vector, nic);
+
+	/* Disable MSI-X */
+	nic_disable_msix(nic);
+}
+
+void nic_set_sriov_enable(struct nicpf *nic)
+{
+	nic->flags |= NIC_SRIOV_ENABLED;
+}
+
+void nic_clear_sriov_enable(struct nicpf *nic)
+{
+	nic->flags &= ~NIC_SRIOV_ENABLED;
+}
+
+bool nic_is_sriov_enabled(struct nicpf *nic)
+{
+	if (nic->flags & NIC_SRIOV_ENABLED)
+		return true;
+	return false;
+}
+
+int nic_sriov_configure(struct pci_dev *pdev, int num_vfs_requested)
+{
+	struct net_device *netdev = pci_get_drvdata(pdev);
+	struct nicpf *nic = netdev_priv(netdev);
+	int err;
+
+	if (nic->num_vf_en == num_vfs_requested)
+		return num_vfs_requested;
+
+	if (nic_is_sriov_enabled(nic)) {
+		pci_disable_sriov(pdev);
+		nic_clear_sriov_enable(nic);
+	}
+
+	nic->num_vf_en = 0;
+	if (num_vfs_requested > MAX_NUM_VFS_SUPPORTED)
+		return -EPERM;
+
+	if (num_vfs_requested) {
+		if ((err = pci_enable_sriov(pdev, num_vfs_requested))) {
+			dev_err(&pdev->dev, "SRIOV, Failed to enable %d VFs\n", num_vfs_requested);
+			return err;
+		}
+		nic->num_vf_en = num_vfs_requested;
+		nic_set_sriov_enable(nic);
+	}
+
+	return num_vfs_requested;
+}
+
+static int  nic_sriov_init(struct pci_dev *pdev, struct nicpf *nic)
+{
+	int    pos = 0;
+
+	pos = pci_find_ext_capability(pdev, PCI_EXT_CAP_ID_SRIOV);
+	if (!pos) {
+		dev_err(&pdev->dev, "SRIOV capability is not found in PCIe config space\n");
+		return 0;
+	}
+
+	pci_read_config_word(pdev, (pos + PCI_SRIOV_TOTAL_VF), &nic->total_vf_cnt);
+	if (nic->total_vf_cnt < DEFAULT_NUM_VF_ENABLED)
+		nic->num_vf_en = nic->total_vf_cnt;
+	else
+		nic->num_vf_en = DEFAULT_NUM_VF_ENABLED;
+
+	if (nic->total_vf_cnt && pci_enable_sriov(pdev, nic->num_vf_en)) {
+		dev_err(&pdev->dev, "SRIOV enable failed, num VF is %d\n", nic->num_vf_en);
+		nic->num_vf_en = 0;
+		return 0;
+	}
+	dev_info(&pdev->dev, "SRIOV enabled, numer of VF available %d\n", nic->num_vf_en);
+
+	nic_set_sriov_enable(nic);
+	return 1;
+}
+
+static int nic_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
+{
+	struct device *dev = &pdev->dev;
+	struct net_device *netdev;
+	struct nicpf *nic;
+	int    err;
+
+	netdev = alloc_etherdev(sizeof(struct nicpf));
+	if (!netdev)
+		return -ENOMEM;
+
+	pci_set_drvdata(pdev, netdev);
+
+	SET_NETDEV_DEV(netdev, &pdev->dev);
+
+	nic = netdev_priv(netdev);
+	nic->netdev = netdev;
+	nic->pdev = pdev;
+
+	err = pci_enable_device(pdev);
+	if (err) {
+		dev_err(dev, "Failed to enable PCI device\n");
+		goto exit;
+	}
+
+	err = pci_request_regions(pdev, DRV_NAME);
+	if (err) {
+		dev_err(dev, "PCI request regions failed 0x%x\n", err);
+		goto err_disable_device;
+	}
+
+	err = pci_set_dma_mask(pdev, DMA_BIT_MASK(48));
+	if (!err) {
+		err = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(48));
+		if (err) {
+			dev_err(dev, "unable to get 40-bit DMA for consistent allocations\n");
+			goto err_release_regions;
+		}
+	} else {
+		dev_err(dev, "Unable to get usable DMA configuration\n");
+		goto err_release_regions;
+	}
+
+	/* MAP PF's configuration registers */
+	nic->reg_base = (uint64_t)pci_ioremap_bar(pdev, PCI_CFG_REG_BAR_NUM);
+	if (!nic->reg_base) {
+		dev_err(dev, "Cannot map config register space, aborting\n");
+		err = -ENOMEM;
+		goto err_release_regions;
+	}
+
+	/* Initialize hardware */
+	nic_init_hw(nic);
+
+	/* Configure SRIOV */
+	if (!nic_sriov_init(pdev, nic))
+		goto err_unmap_resources;
+
+	/* Register interrupts */
+	if (nic_register_interrupts(nic))
+		goto err_unmap_resources;
+
+	goto exit;
+
+err_unmap_resources:
+	if (nic->reg_base)
+		iounmap((void *)nic->reg_base);
+err_release_regions:
+	pci_release_regions(pdev);
+err_disable_device:
+	pci_disable_device(pdev);
+exit:
+	return err;
+}
+
+static void nic_remove(struct pci_dev *pdev)
+{
+	struct net_device *netdev = pci_get_drvdata(pdev);
+	struct nicpf *nic;
+
+	if (!netdev)
+		return;
+
+	nic = netdev_priv(netdev);
+
+	nic_unregister_interrupts(nic);
+
+	if (nic_is_sriov_enabled(nic))
+		pci_disable_sriov(pdev);
+
+	pci_set_drvdata(pdev, NULL);
+
+	if (nic->reg_base)
+		iounmap((void *)nic->reg_base);
+
+	pci_release_regions(pdev);
+	pci_disable_device(pdev);
+	free_netdev(netdev);
+}
+
+static struct pci_driver nic_driver = {
+	.name = DRV_NAME,
+	.id_table = nic_id_table,
+	.probe = nic_probe,
+	.remove = nic_remove,
+	.sriov_configure = nic_sriov_configure,
+};
+
+static int __init nic_init_module(void)
+{
+	pr_info("%s, ver %s\n", DRV_NAME, DRV_VERSION);
+
+	return pci_register_driver(&nic_driver);
+}
+
+static void __exit nic_cleanup_module(void)
+{
+	pci_unregister_driver(&nic_driver);
+}
+
+module_init(nic_init_module);
+module_exit(nic_cleanup_module);
+
diff --git a/drivers/net/ethernet/cavium/thunder/nic_reg.h b/drivers/net/ethernet/cavium/thunder/nic_reg.h
new file mode 100644
index 0000000..46bcd412
--- /dev/null
+++ b/drivers/net/ethernet/cavium/thunder/nic_reg.h
@@ -0,0 +1,162 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2013 Cavium, Inc.
+ */
+
+#ifndef NIC_REG_H
+#define NIC_REG_H
+
+/* Physical function register offsets */
+#define   NIC_PF_CFG				(0x0000)
+#define   NIC_PF_STATUS				(0x0010)
+#define   NIC_PF_INTR_TIMER_CFG			(0x0030)
+#define   NIC_PF_BIST_STATUS			(0x0040)
+#define   NIC_PF_SOFT_RESET			(0x0050)
+#define   NIC_PF_TCP_TIMER			(0x0060)
+#define   NIC_PF_BP_CFG				(0x0080)
+#define   NIC_PF_RRM_CFG			(0x0088)
+#define   NIC_PF_CQM_CF				(0x00A0)
+#define   NIC_PF_CNM_CF				(0x00A8)
+#define   NIC_PF_CNM_STATUS			(0x00B0)
+#define   NIC_PF_CQ_AVG_CFG			(0x00C0)
+#define   NIC_PF_RRM_AVG_CFG			(0x00C8)
+#define   NIC_PF_INTF_0_1_SEND_CFG		(0x0200) /* (0..1) << 8 */
+#define   NIC_PF_INTF_0_1_BP_CFG		(0x0208)
+#define   NIC_PF_INTF_0_1_BP_DIS_0_1		(0x0210) /* (0..1) << 8 + (0..1) << 3 */
+#define   NIC_PF_INTF_0_1_BP_SW_0_1		(0x0220)
+#define   NIC_PF_ECC_INT			(0x0400)
+#define   NIC_PF_MAILBOX_INT			(0x0410) /* (0..1) << 3 */
+#define   NIC_PF_ECC_INT_W1S			(0x0420)
+#define   NIC_PF_MAILBOX_INT_W1S		(0x0430)
+#define   NIC_PF_ECC_ENA_W1C			(0x0440)
+#define   NIC_PF_MAILBOX_ENA_W1C		(0x0450)
+#define   NIC_PF_ECC_ENA_W1S			(0x0460)
+#define   NIC_PF_MAILBOX_ENA_W1S		(0x0470)
+#define   NIC_PF_ECC_CTL			(0x0480)
+#define   NIC_PF_RX_ETYPE_0_7			(0x0500) /* + (0..7) << 3 */
+#define   NIC_PF_PKIND_0_15_CFG			(0x0600)
+#define   NIC_PF_CPI_0_2047_CFG			(0x200000)
+#define   NIC_PF_RSSI_0_4097_RQ			(0x220000)
+#define   NIC_PF_LMAC_0_7_CFG			(0x240000)
+#define   NIC_PF_LMAC_0_7_SW_XOFF		(0x242000)
+#define   NIC_PF_LMAC_0_7_CREDIT		(0x244000)
+#define   NIC_PF_CHAN_0_255_TX_CFG		(0x400000)
+#define   NIC_PF_CHAN_0_255_RX_CFG		(0x420000)
+#define   NIC_PF_CHAN_0_255_SW_XOFF		(0x440000)
+#define   NIC_PF_CHAN_0_255_CREDIT		(0x460000)
+#define   NIC_PF_CHAN_0_255_RX_BP_CFG		(0x480000)
+#define   NIC_PF_TL2_0_63_CFG			(0x500000)
+#define   NIC_PF_TL2_0_63_PRI			(0x520000)
+#define   NIC_PF_TL2_0_63_SH_STATUS		(0x580000)
+#define   NIC_PF_TL3A_0_63_CFG			(0x5F0000)
+#define   NIC_PF_TL3_0_255_CFG			(0x600000)
+#define   NIC_PF_TL3_0_255_CHAN			(0x620000)
+#define   NIC_PF_TL3_0_255_PIR			(0x640000)
+#define   NIC_PF_TL3_0_255_SW_XOFF		(0x660000)
+#define   NIC_PF_TL3_0_255_CNM_RATE		(0x680000)
+#define   NIC_PF_TL3_0_255_SH_STATUS		(0x6A0000)
+#define   NIC_PF_TL4A_0_255_CFG			(0x6F0000)
+#define   NIC_PF_TL4_0_1023_CFG			(0x800000)
+#define   NIC_PF_TL4_0_1023_SW_XOFF		(0x820000)
+#define   NIC_PF_TL4_0_1023_SH_STATUS		(0x880000)
+#define   NIC_PF_TL4A_0_1023_CNM_STATUS		(0x8A0000)
+#define   NIC_PF_VF_0_127_MAILBOX_0_7		(0x20002000) /* + (0..127) << 21 + (0..7) << 3 */
+#define   NIC_PF_VNIC_0_127_TX_STAT_0_4		(0x20004000) /* + (0..127) << 21 + (0..4) << 3 */
+#define   NIC_PF_VNIC_0_127_RX_STAT_0_13	(0x20004100)
+#define   NIC_PF_QSET_0_127_LOCK_0_15		(0x20006000)
+#define   NIC_PF_QSET_0_127_CFG			(0x20010000) /* + (0..127) << 21 */
+#define   NIC_PF_QSET_0_127_RQ_0_7_CFG		(0x20010400) /* + (0..127) << 21 + (0..7) << 18 */
+#define   NIC_PF_QSET_0_127_RQ_0_7_DROP_CFG	(0x20010420)
+#define   NIC_PF_QSET_0_127_RQ_0_7_BP_CFG	(0x20010500)
+#define   NIC_PF_QSET_0_127_RQ_0_7_STAT_0_1	(0x20010600) /* + (0..127) << 21 + (0..7) << 18 + (0..1) << 3 */
+#define   NIC_PF_QSET_0_127_SQ_0_7_CFG		(0x20010C00) /* + (0..127) << 21 + (0..7) << 18 */
+#define   NIC_PF_QSET_0_127_SQ_0_7_CFG2		(0x20010C08)
+#define   NIC_PF_QSET_0_127_SQ_0_7_STAT_0_1	(0x20010D00) /* + (0..127) << 21 + (0..7) << 18 + (0..1) << 3 */
+
+#define   NIC_PF_MSIX_VEC_0_18_ADDR		(0x000000) /* + (0..18) << 4 */
+#define   NIC_PF_MSIX_VEC_0_CTL			(0x000008)
+#define   NIC_PF_MSIX_PBA_0			(0x010000)
+
+/* Virtual function register offsets */
+#define   NIC_VNIC_CFG				(0x000020)
+#define   NIC_VF_PF_MAILBOX_0_7			(0x000100) /* + (0..7) << 3 */
+#define   NIC_VF_INT				(0x000200)
+#define   NIC_VF_INT_W1S			(0x000220)
+#define   NIC_VF_ENA_W1C			(0x000240)
+#define   NIC_VF_ENA_W1S			(0x000260)
+
+#define   NIC_VNIC_RSS_CFG			(0x0020E0)
+#define   NIC_VNIC_RSS_KEY_0_4			(0x002200) /* + (0..4) << 3*/
+#define   NIC_VNIC_TX_STAT_0_5			(0x004000)
+#define   NIC_VNIC_RX_STAT_0_13			(0x004100)
+
+#define   NIC_QSET_CQ_0_7_CFG			(0x010400)
+#define   NIC_QSET_CQ_0_7_CFG2			(0x010408)
+#define   NIC_QSET_CQ_0_7_THRESH		(0x010410)
+#define   NIC_QSET_CQ_0_7_BASE			(0x010420)
+#define   NIC_QSET_CQ_0_7_HEAD			(0x010428)
+#define   NIC_QSET_CQ_0_7_TAIL			(0x010430)
+#define   NIC_QSET_CQ_0_7_DOOR			(0x010438)
+#define   NIC_QSET_CQ_0_7_STATUS		(0x010440)
+#define   NIC_QSET_CQ_0_7_STATUS2		(0x010448)
+#define   NIC_QSET_CQ_0_7_DEBUG			(0x010450)
+
+#define   NIC_QSET_RQ_GEN_CFG			(0x010010)
+#define   NIC_QSET_RQ_0_7_CFG			(0x010600)
+#define   NIC_QSET_RQ_0_7_STAT_0_1		(0x010700)
+
+#define   NIC_QSET_SQ_0_7_CFG			(0x010800)
+#define   NIC_QSET_SQ_0_7_THRESH		(0x010810)
+#define   NIC_QSET_SQ_0_7_BASE			(0x010820)
+#define   NIC_QSET_SQ_0_7_HEAD			(0x010828)
+#define   NIC_QSET_SQ_0_7_TAIL			(0x010830)
+#define   NIC_QSET_SQ_0_7_DOOR			(0x010838)
+#define   NIC_QSET_SQ_0_7_STATUS		(0x010840)
+#define   NIC_QSET_SQ_0_7_DEBUG			(0x010848)
+#define   NIC_QSET_SQ_0_7_CNM_CHG		(0x010860)
+#define   NIC_QSET_SQ_0_7_STAT_0_1		(0x010900)
+
+#define   NIC_QSET_RBDR_0_1_CFG			(0x010C00)
+#define   NIC_QSET_RBDR_0_1_THRESH		(0x010C10)
+#define   NIC_QSET_RBDR_0_1_BASE		(0x010C20)
+#define   NIC_QSET_RBDR_0_1_HEAD		(0x010C28)
+#define   NIC_QSET_RBDR_0_1_TAIL		(0x010C30)
+#define   NIC_QSET_RBDR_0_1_DOOR		(0x010C38)
+#define   NIC_QSET_RBDR_0_1_STATUS0		(0x010C40)
+#define   NIC_QSET_RBDR_0_1_STATUS1		(0x010C48)
+
+#define   NIC_VF_MSIX_VECTOR_0_19_ADDR		(0x000000)
+#define   NIC_VF_MSIX_VECTOR_0_19_CTL		(0x000008)
+#define   NIC_VF_MSIX_PBA			(0x010000)
+
+/* Offsets within registers */
+#define   NIC_MSIX_VEC_SHIFT			4
+#define   NIC_Q_NUM_SHIFT			18
+#define   NIC_QS_ID_SHIFT			21
+#define   NIC_VF_NUM_SHIFT			21
+
+/* Port kind configuration */
+struct pkind_cfg {
+#if defined(__BIG_ENDIAN_BITFIELD)
+	uint64_t reserved_42_63:22;
+	uint64_t hdr_sl:5;	/* Header skip length */
+	uint64_t rx_hdr:3;	/* TNS Receive header present */
+	uint64_t lenerr_en:1;	/* L2 length error check enable */
+	uint64_t reserved_32_32:1;
+	uint64_t maxlen:16;	/* Max frame size */
+	uint64_t minlen:16;	/* Min frame size */
+#elif defined(__LITTLE_ENDIAN_BITFIELD)
+	uint64_t minlen:16;
+	uint64_t maxlen:16;
+	uint64_t reserved_32_32:1;
+	uint64_t lenerr_en:1;
+	uint64_t rx_hdr:3;
+	uint64_t hdr_sl:5;
+	uint64_t reserved_42_63:22;
+#endif
+};
+
+#endif /* NIC_REG_H */
diff --git a/drivers/net/ethernet/cavium/thunder/nicvf_ethtool.c b/drivers/net/ethernet/cavium/thunder/nicvf_ethtool.c
new file mode 100644
index 0000000..bd724c3
--- /dev/null
+++ b/drivers/net/ethernet/cavium/thunder/nicvf_ethtool.c
@@ -0,0 +1,198 @@
+/****************************************************************************
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2013 Cavium, Inc.
+ ******************************************************************************/
+
+/* ETHTOOL Support for VNIC_VF Device*/
+
+#include <linux/pci.h>
+
+#include "nic_reg.h"
+#include "nic.h"
+
+#define DRV_NAME	"thunder-nicvf"
+#define DRV_VERSION     "1.0"
+
+struct nicvf_stat {
+	char name[ETH_GSTRING_LEN];
+	unsigned int index;
+};
+
+#define NICVF_TX_STAT(stat) { \
+	.name = #stat, \
+	.index = offsetof(struct nicvf_tx_stats, stat) / sizeof(u64) \
+}
+
+#define NICVF_RX_STAT(stat) { \
+	.name = #stat, \
+	.index = offsetof(struct nicvf_rx_stats, stat) / sizeof(u64) \
+}
+
+static const struct nicvf_stat nicvf_tx_stats[] = {
+	NICVF_TX_STAT(tx_frames_ok),
+	NICVF_TX_STAT(tx_unicast_frames_ok),
+	NICVF_TX_STAT(tx_multicast_frames_ok),
+	NICVF_TX_STAT(tx_broadcast_frames_ok),
+	NICVF_TX_STAT(tx_bytes_ok),
+	NICVF_TX_STAT(tx_unicast_bytes_ok),
+	NICVF_TX_STAT(tx_multicast_bytes_ok),
+	NICVF_TX_STAT(tx_broadcast_bytes_ok),
+	NICVF_TX_STAT(tx_drops),
+	NICVF_TX_STAT(tx_errors),
+	NICVF_TX_STAT(tx_tso),
+};
+
+static const struct nicvf_stat nicvf_rx_stats[] = {
+	NICVF_RX_STAT(rx_frames_ok),
+	NICVF_RX_STAT(rx_frames_total),
+	NICVF_RX_STAT(rx_unicast_frames_ok),
+	NICVF_RX_STAT(rx_multicast_frames_ok),
+	NICVF_RX_STAT(rx_broadcast_frames_ok),
+	NICVF_RX_STAT(rx_bytes_ok),
+	NICVF_RX_STAT(rx_unicast_bytes_ok),
+	NICVF_RX_STAT(rx_multicast_bytes_ok),
+	NICVF_RX_STAT(rx_broadcast_bytes_ok),
+	NICVF_RX_STAT(rx_drop),
+	NICVF_RX_STAT(rx_no_bufs),
+	NICVF_RX_STAT(rx_errors),
+	NICVF_RX_STAT(rx_rss),
+	NICVF_RX_STAT(rx_crc_errors),
+	NICVF_RX_STAT(rx_frames_64),
+	NICVF_RX_STAT(rx_frames_127),
+	NICVF_RX_STAT(rx_frames_255),
+	NICVF_RX_STAT(rx_frames_511),
+	NICVF_RX_STAT(rx_frames_1023),
+	NICVF_RX_STAT(rx_frames_1518),
+	NICVF_RX_STAT(rx_frames_jumbo),
+};
+
+static const unsigned int nicvf_n_tx_stats = ARRAY_SIZE(nicvf_tx_stats);
+static const unsigned int nicvf_n_rx_stats = ARRAY_SIZE(nicvf_rx_stats);
+
+static int nicvf_get_settings(struct net_device *netdev,
+			     struct ethtool_cmd *cmd)
+{
+	cmd->supported = (SUPPORTED_1000baseT_Full |
+			SUPPORTED_100baseT_Full |
+			SUPPORTED_10baseT_Full |
+			SUPPORTED_10000baseT_Full | SUPPORTED_FIBRE);
+
+	cmd->advertising = (ADVERTISED_1000baseT_Full |
+			ADVERTISED_100baseT_Full |
+			ADVERTISED_10baseT_Full |
+			ADVERTISED_10000baseT_Full | ADVERTISED_FIBRE);
+
+	cmd->port = PORT_FIBRE;
+	cmd->transceiver = XCVR_EXTERNAL;
+	if (netif_carrier_ok(netdev)) {
+		ethtool_cmd_speed_set(cmd, SPEED_10000);
+		cmd->duplex = DUPLEX_FULL;
+	} else {
+		ethtool_cmd_speed_set(cmd, -1);
+		cmd->duplex = -1;
+	}
+
+	cmd->autoneg = AUTONEG_DISABLE;
+	ethtool_cmd_speed_set(cmd, SPEED_1000);
+	return 0;
+}
+
+static int nicvf_set_settings(struct net_device *netdev,
+			     struct ethtool_cmd *cmd)
+{
+	return -EOPNOTSUPP;
+
+	/* 10G full duplex setting supported only */
+	if (cmd->autoneg == AUTONEG_ENABLE)
+		return -EOPNOTSUPP;
+
+	if (ethtool_cmd_speed(cmd) != SPEED_10000)
+		return -EOPNOTSUPP;
+
+	if (cmd->duplex != DUPLEX_FULL)
+		return -EOPNOTSUPP;
+
+	return 0;
+}
+
+static void nicvf_get_drvinfo(struct net_device *netdev,
+			     struct ethtool_drvinfo *info)
+{
+	struct nicvf *nic = netdev_priv(netdev);
+
+	strlcpy(info->driver, DRV_NAME, sizeof(info->driver));
+	strlcpy(info->version, DRV_VERSION, sizeof(info->version));
+	strlcpy(info->bus_info, pci_name(nic->pdev), sizeof(info->bus_info));
+}
+
+static void nicvf_get_strings(struct net_device *netdev, u32 stringset,
+			     u8 *data)
+{
+	int stats;
+
+	for (stats = 0; stats < nicvf_n_tx_stats; stats++) {
+		memcpy(data, nicvf_tx_stats[stats].name, ETH_GSTRING_LEN);
+		data += ETH_GSTRING_LEN;
+	}
+	for (stats = 0; stats < nicvf_n_rx_stats; stats++) {
+		memcpy(data, nicvf_rx_stats[stats].name, ETH_GSTRING_LEN);
+		data += ETH_GSTRING_LEN;
+	}
+}
+
+static int nicvf_get_sset_count(struct net_device *netdev, int sset)
+{
+	return nicvf_n_tx_stats + nicvf_n_rx_stats;
+}
+
+static void nicvf_get_ethtool_stats(struct net_device *netdev,
+				   struct ethtool_stats *stats, u64 *data)
+{
+	struct nicvf *nic = netdev_priv(netdev);
+	struct eth_stats vstats = nic->vstats;
+	int stat;
+
+	memset(&vstats, 0, sizeof(struct eth_stats));
+
+	nic->vstats.tx.tx_frames_ok = netdev->stats.tx_packets;
+	nic->vstats.tx.tx_bytes_ok = netdev->stats.tx_bytes;
+	nic->vstats.tx.tx_errors = netdev->stats.tx_errors;
+	nic->vstats.tx.tx_drops = netdev->stats.tx_dropped;
+	nic->vstats.rx.rx_frames_ok = netdev->stats.rx_packets;
+	nic->vstats.rx.rx_bytes_ok = netdev->stats.rx_bytes;
+	nic->vstats.rx.rx_errors = netdev->stats.rx_errors;
+	nic->vstats.rx.rx_drop = netdev->stats.rx_dropped;
+
+	for (stat = 0; stat < nicvf_n_tx_stats; stat++)
+		*(data++) = ((u64 *)&nic->vstats.tx)
+				[nicvf_tx_stats[stat].index];
+	for (stat = 0; stat < nicvf_n_rx_stats; stat++)
+		*(data++) = ((u64 *)&nic->vstats.rx)
+				[nicvf_rx_stats[stat].index];
+}
+
+static const struct ethtool_ops nicvf_ethtool_ops = {
+	.get_settings		= nicvf_get_settings,
+	.set_settings		= nicvf_set_settings,
+	.get_link		= ethtool_op_get_link,
+	.get_drvinfo		= nicvf_get_drvinfo,
+	.get_strings		= nicvf_get_strings,
+	.get_sset_count		= nicvf_get_sset_count,
+	.get_ethtool_stats	= nicvf_get_ethtool_stats,
+	.get_ts_info		= ethtool_op_get_ts_info
+#if 0
+	.get_coalesce		= nicvf_get_coalesce,
+	.set_coalesce		= nicvf_set_coalesce,
+	.get_ringparam		= nicvf_get_ringparam,
+	.set_ringparam		= nicvf_set_ringparam,
+#endif
+};
+
+void nicvf_set_ethtool_ops(struct net_device *netdev)
+{
+	netdev->ethtool_ops = &nicvf_ethtool_ops;
+}
+
diff --git a/drivers/net/ethernet/cavium/thunder/nicvf_main.c b/drivers/net/ethernet/cavium/thunder/nicvf_main.c
new file mode 100644
index 0000000..9d82766
--- /dev/null
+++ b/drivers/net/ethernet/cavium/thunder/nicvf_main.c
@@ -0,0 +1,1122 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2014 Cavium, Inc.
+ */
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/string.h>
+#include <linux/errno.h>
+#include <linux/types.h>
+#include <linux/init.h>
+#include <linux/interrupt.h>
+#include <linux/workqueue.h>
+#include <linux/pci.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/if.h>
+#include <linux/if_ether.h>
+#include <linux/if_vlan.h>
+#include <linux/ethtool.h>
+#include <linux/aer.h>
+#include <linux/ip.h>
+#include <net/tcp.h>
+
+#include "nic_reg.h"
+#include "nic.h"
+#include "nicvf_queues.h"
+
+#define DRV_NAME	"thunder-nicvf"
+#define DRV_VERSION	"1.0"
+
+/* Supported devices */
+static DEFINE_PCI_DEVICE_TABLE(nicvf_id_table) = {
+	{ PCI_DEVICE(PCI_VENDOR_ID_CAVIUM, PCI_DEVICE_ID_THUNDER_NIC_VF) },
+	{ 0, }  /* end of table */
+};
+
+MODULE_AUTHOR("Cavium Inc");
+MODULE_DESCRIPTION("Cavium Thunder Virtual Function Network Driver");
+MODULE_LICENSE("GPL");
+MODULE_VERSION(DRV_VERSION);
+MODULE_DEVICE_TABLE(pci, nicvf_id_table);
+
+static int nicvf_enable_msix(struct nicvf *nic);
+static netdev_tx_t nicvf_xmit(struct sk_buff *skb, struct net_device *netdev);
+
+static void nicvf_dump_packet(struct sk_buff *skb)
+{
+#ifdef NICVF_DUMP_PACKET
+	int i;
+
+	for (i = 0; i < skb->len; i++) {
+		if (!(i % 16))
+			pr_cont("\n");
+		pr_debug("%02x ", (u_char)skb->data[i]);
+	}
+#endif
+}
+
+static void nicvf_update_stats(struct nicvf *nic, struct sk_buff *skb)
+{
+	if (skb->len <= 64)
+		atomic64_add(1, (atomic64_t *)&nic->vstats.rx.rx_frames_64);
+	else if ((skb->len > 64) && (skb->len <= 127))
+		atomic64_add(1, (atomic64_t *)&nic->vstats.rx.rx_frames_127);
+	else if ((skb->len > 127) && (skb->len <= 255))
+		atomic64_add(1, (atomic64_t *)&nic->vstats.rx.rx_frames_255);
+	else if ((skb->len > 255) && (skb->len <= 511))
+		atomic64_add(1, (atomic64_t *)&nic->vstats.rx.rx_frames_511);
+	else if ((skb->len > 511) && (skb->len <= 1023))
+		atomic64_add(1, (atomic64_t *)&nic->vstats.rx.rx_frames_1023);
+	else if ((skb->len > 1023) && (skb->len <= 1518))
+		atomic64_add(1, (atomic64_t *)&nic->vstats.rx.rx_frames_1518);
+	else if (skb->len > 1518)
+		atomic64_add(1, (atomic64_t *)&nic->vstats.rx.rx_frames_jumbo);
+}
+
+/* Register read/write APIs */
+void nicvf_reg_write(struct nicvf *nic, uint64_t offset, uint64_t val)
+{
+	uint64_t addr = nic->reg_base + offset;
+
+	writeq_relaxed(val, (void *)addr);
+}
+
+uint64_t nicvf_reg_read(struct nicvf *nic, uint64_t offset)
+{
+	uint64_t addr = nic->reg_base + offset;
+
+	return readq_relaxed((void *)addr);
+}
+
+void nicvf_qset_reg_write(struct nicvf *nic, uint64_t offset, uint64_t val)
+{
+	uint64_t addr = nic->reg_base + offset;
+
+	writeq_relaxed(val, (void *)(addr));
+}
+
+uint64_t nicvf_qset_reg_read(struct nicvf *nic, uint64_t offset)
+{
+	uint64_t addr = nic->reg_base + offset;
+
+	return readq_relaxed((void *)(addr));
+}
+
+void nicvf_queue_reg_write(struct nicvf *nic, uint64_t offset,
+				uint64_t qidx, uint64_t val)
+{
+	uint64_t addr = nic->reg_base + offset;
+
+	writeq_relaxed(val, (void *)(addr + (qidx << NIC_Q_NUM_SHIFT)));
+}
+
+uint64_t nicvf_queue_reg_read(struct nicvf *nic, uint64_t offset, uint64_t qidx)
+{
+	uint64_t addr = nic->reg_base + offset;
+
+	return readq_relaxed((void *)(addr + (qidx << NIC_Q_NUM_SHIFT)));
+}
+
+/* VF -> PF mailbox communication */
+static bool pf_ready_to_rcv_msg;
+static bool pf_acked;
+static bool pf_nacked;
+
+struct nic_mbx *nicvf_get_mbx(void)
+{
+	struct nic_mbx *mbx = kzalloc(sizeof(*mbx), GFP_KERNEL);
+
+	return mbx;
+}
+
+static void nicvf_enable_mbx_intr(struct nicvf *nic)
+{
+	nicvf_enable_intr(nic, NICVF_INTR_MBOX, 0);
+}
+
+static void nicvf_disable_mbx_intr(struct nicvf *nic)
+{
+	nicvf_disable_intr(nic, NICVF_INTR_MBOX, 0);
+}
+
+int nicvf_send_msg_to_pf(struct nicvf *nic, struct nic_mbx *mbx)
+{
+	int i, timeout = 5000, sleep = 10;
+	uint64_t *msg;
+	uint64_t mbx_addr;
+
+	pf_acked = false;
+	pf_nacked = false;
+	mbx->mbx_trigger_intr = 1;
+	msg = (uint64_t *)mbx;
+	mbx_addr = nic->reg_base + NIC_VF_PF_MAILBOX_0_7;
+
+	for (i = 0; i < NIC_PF_VF_MAILBOX_SIZE; i++)
+		writeq_relaxed(*(msg + i), (void *)(mbx_addr + (i * 8)));
+
+	/* Wait for previous message to be acked, timeout 5sec */
+	while (!pf_acked) {
+		if (pf_nacked)
+			return -EINVAL;
+		msleep(sleep);
+		if (pf_acked)
+			break;
+		else
+			timeout -= sleep;
+		if (!timeout) {
+			netdev_err(nic->netdev,
+				"PF didn't ack to mailbox msg %lld from VF%d\n",
+						(mbx->msg & 0xFF), nic->vnic_id);
+			return -EBUSY;
+		}
+	}
+	return 0;
+}
+
+/* Checks if VF is able to comminicate with PF
+* and also gets the VNIC number this VF is associated to.
+*/
+static int nicvf_check_pf_ready(struct nicvf *nic)
+{
+	int timeout = 5000, sleep = 20;
+	uint64_t mbx_addr = NIC_VF_PF_MAILBOX_0_7;
+
+	pf_ready_to_rcv_msg = false;
+
+	nicvf_reg_write(nic, mbx_addr, NIC_PF_VF_MSG_READY);
+
+	mbx_addr += (NIC_PF_VF_MAILBOX_SIZE - 1) * 8;
+	nicvf_reg_write(nic, mbx_addr, 1ULL);
+
+	while (!pf_ready_to_rcv_msg) {
+		msleep(sleep);
+		if (pf_ready_to_rcv_msg)
+			break;
+		else
+			timeout -= sleep;
+		if (!timeout) {
+			netdev_err(nic->netdev,
+				"PF didn't respond to READY msg\n");
+			return 0;
+		}
+	}
+	return 1;
+}
+
+static void  nicvf_handle_mbx_intr(struct nicvf *nic)
+{
+	int i;
+	struct nic_mbx *mbx;
+	uint64_t *mbx_data;
+	uint64_t mbx_addr;
+
+	mbx_addr = NIC_VF_PF_MAILBOX_0_7;
+
+	mbx = kzalloc(sizeof(*mbx), GFP_KERNEL);
+	mbx_data = (uint64_t *)mbx;
+
+	for (i = 0; i < NIC_PF_VF_MAILBOX_SIZE; i++) {
+		*mbx_data = nicvf_reg_read(nic, mbx_addr + (i * NIC_PF_VF_MAILBOX_SIZE));
+		mbx_data++;
+	}
+
+	switch (mbx->msg & 0xFF) {
+	case NIC_PF_VF_MSG_READY:
+		pf_ready_to_rcv_msg = true;
+		nic->vnic_id = mbx->data.vnic_id & 0x7F;
+		break;
+	case NIC_PF_VF_MSG_ACK:
+		pf_acked = true;
+		break;
+	case NIC_PF_VF_MSG_NACK:
+		pf_nacked = true;
+		break;
+	default:
+		netdev_err(nic->netdev, "Invalid message from PF, msg 0x%llx\n",
+								mbx->msg);
+		break;
+	}
+	nicvf_clear_intr(nic, NICVF_INTR_MBOX, 0);
+	kfree(mbx);
+}
+
+static int nicvf_hw_set_mac_addr(struct nicvf *nic, struct net_device *netdev)
+{
+	int i;
+	int ret;
+	struct  nic_mbx *mbx;
+
+	mbx = nicvf_get_mbx();
+	mbx->msg = NIC_PF_VF_MSG_SET_MAC;
+	mbx->data.mac.vnic_id = nic->vnic_id;
+	for (i = 0; i < ETH_ALEN; i++)
+		mbx->data.mac.addr = (mbx->data.mac.addr << 8) | netdev->dev_addr[i];
+
+	ret = nicvf_send_msg_to_pf(nic, mbx);
+	kfree(mbx);
+
+	return ret;
+}
+
+static int nicvf_is_link_active(struct nicvf *nic)
+{
+	return 1;
+}
+
+static int nicvf_init_resources(struct nicvf *nic)
+{
+	int err;
+
+	nic->num_qs = 1;
+
+	/* Initialize queues and HW for data transfer */
+	if ((err = nicvf_config_data_transfer(nic, true))) {
+		netdev_err(nic->netdev,
+			"Failed to allocate/configure VF's QSet resources, err %d\n", err);
+		return err;
+	}
+	/* Enable Qset */
+	nicvf_qset_config(nic, true);
+
+	return 0;
+}
+
+void nicvf_free_skb(struct nicvf *nic, struct sk_buff *skb)
+{
+	int i;
+
+	if (!skb_shinfo(skb)->nr_frags)
+		goto free_skb;
+
+	for (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {
+		const struct skb_frag_struct *frag;
+
+		frag = &skb_shinfo(skb)->frags[i];
+		pci_unmap_single(nic->pdev, (dma_addr_t)skb_frag_address(frag),
+						skb_frag_size(frag), PCI_DMA_TODEVICE);
+	}
+free_skb:
+	pci_unmap_single(nic->pdev, (dma_addr_t)skb->data, skb_headlen(skb), PCI_DMA_TODEVICE);
+	dev_kfree_skb_any(skb);
+}
+
+static void nicvf_snd_pkt_handler(struct net_device *netdev,
+				  void *cq_desc, int cqe_type)
+{
+	struct sk_buff *skb = NULL;
+	struct cqe_send_t *cqe_tx;
+	struct nicvf *nic = netdev_priv(netdev);
+	struct snd_queue *sq;
+	struct sq_hdr_subdesc *hdr;
+
+	cqe_tx = (struct cqe_send_t *)cq_desc;
+	sq = &nic->qs->sq[cqe_tx->sq_idx];
+
+	hdr  = (struct sq_hdr_subdesc *)(sq->desc_mem.base +
+				(cqe_tx->sqe_ptr * SND_QUEUE_DESC_SIZE));
+	if (hdr->subdesc_type != SQ_DESC_TYPE_HEADER)
+		return;
+
+	nic_dbg(&nic->pdev->dev, "%s Qset #%d SQ #%d SQ ptr #%d Subdesc count %d\n",
+				__func__, cqe_tx->sq_qs, cqe_tx->sq_idx,
+					cqe_tx->sqe_ptr, hdr->subdesc_cnt);
+
+	skb = (struct sk_buff *)sq->skbuff[cqe_tx->sqe_ptr];
+	atomic64_add(1, (atomic64_t *)&netdev->stats.tx_packets);
+	atomic64_add(hdr->tot_len, (atomic64_t *)&netdev->stats.tx_bytes);
+	nicvf_free_skb(nic, skb);
+	nicvf_put_sq_desc(sq, hdr->subdesc_cnt + 1);
+}
+
+static void nicvf_rcv_pkt_handler(struct net_device *netdev,
+			struct napi_struct *napi, void *cq_desc, int cqe_type)
+{
+	struct sk_buff *skb;
+	struct nicvf *nic = netdev_priv(netdev);
+
+	if (!((cqe_type == CQE_TYPE_RX) || (cqe_type == CQE_TYPE_RX_SPLIT) ||
+						(cqe_type == CQE_TYPE_RX_TCP))) {
+		atomic64_add(1, (atomic64_t *)&netdev->stats.rx_dropped);
+		return;
+	}
+
+	/* Check for errors */
+	if (nicvf_cq_check_errs(nic, cq_desc)) {
+		atomic64_add(1, (atomic64_t *)&netdev->stats.rx_errors);
+		return;
+	}
+
+	skb = nicvf_get_rcv_skb(nic, cq_desc);
+	if (!skb) {
+		nic_dbg(&nic->pdev->dev, "Packet not received\n");
+		return;
+	}
+
+	nicvf_dump_packet(skb);
+
+	/* Update stats */
+	atomic64_add(1, (atomic64_t *)&netdev->stats.rx_packets);
+	atomic64_add(skb->len, (atomic64_t *)&netdev->stats.rx_bytes);
+
+#ifdef NICVF_ETHTOOL_ENABLE
+	nicvf_update_stats(nic, skb);
+#endif
+
+	skb->protocol = eth_type_trans(skb, netdev);
+
+#ifdef VNIC_RX_CHKSUM_SUPPORTED
+	skb->ip_summed = CHECKSUM_UNNECESSARY;
+#else
+	skb_checksum_none_assert(skb);
+#endif
+
+#ifdef	NICVF_NAPI_ENABLE
+#ifdef	VNIC_SW_LRO_SUPPORT
+	if (napi && (netdev->features & NETIF_F_GRO))
+		napi_gro_receive(napi, skb);
+	else
+#endif
+		netif_receive_skb(skb);
+#else
+	netif_rx(skb);
+#endif
+}
+
+static int nicvf_cq_intr_handler(struct net_device *netdev, uint8_t cq_idx,
+					struct napi_struct *napi, int budget)
+{
+	int processed_cqe = 0, work_done = 0;
+	int cqe_count, cqe_head;
+	struct nicvf *nic = netdev_priv(netdev);
+	struct queue_set *qs = nic->qs;
+	struct cmp_queue *cq = &qs->cq[cq_idx];
+	struct cqe_rx_t *cq_desc;
+
+	spin_lock(&cq->cq_lock);
+	/* Get no of valid CQ entries to process */
+	cqe_count = nicvf_queue_reg_read(nic, NIC_QSET_CQ_0_7_STATUS, cq_idx);
+	cqe_count &= CQ_CQE_COUNT;
+	if (!cqe_count)
+		goto done;
+
+	/* Get head of the valid CQ entries */
+	cqe_head = nicvf_queue_reg_read(nic, NIC_QSET_CQ_0_7_HEAD, cq_idx) >> 9;
+	cqe_head &= 0xFFFF;
+
+	nic_dbg(&nic->pdev->dev, "%s cqe_count %d cqe_head %d\n", __func__, cqe_count, cqe_head);
+	while (processed_cqe < cqe_count) {
+		/* Get the CQ descriptor */
+		cq_desc = (struct cqe_rx_t *)(cq->desc_mem.base +
+				(cqe_head * CMP_QUEUE_DESC_SIZE));
+
+		if (napi && (work_done >= budget) &&
+			(cq_desc->cqe_type != CQE_TYPE_SEND)) {
+			break;
+		}
+
+		nic_dbg(&nic->pdev->dev, "cq_desc->cqe_type %d\n", cq_desc->cqe_type);
+		switch (cq_desc->cqe_type) {
+		case CQE_TYPE_RX:
+			nicvf_rcv_pkt_handler(netdev, napi, cq_desc, CQE_TYPE_RX);
+			work_done++;
+		break;
+		case CQE_TYPE_SEND:
+			nicvf_snd_pkt_handler(netdev, cq_desc, CQE_TYPE_SEND);
+		break;
+		case CQE_TYPE_INVALID:
+		case CQE_TYPE_RX_SPLIT:
+		case CQE_TYPE_RX_TCP:
+		case CQE_TYPE_SEND_PTP:
+			/* Ignore for now */
+		break;
+		}
+		cq_desc->cqe_type = CQE_TYPE_INVALID;
+		processed_cqe++;
+		cqe_head++;
+		cqe_head &= (cq->desc_mem.q_len - 1);
+	}
+	nic_dbg(&nic->pdev->dev, "%s processed_cqe %d work_done %d budget %d\n",
+			__func__, processed_cqe, work_done, budget);
+
+	/* Ring doorbell to inform H/W to reuse processed CQEs */
+	nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_DOOR,
+					cq_idx, processed_cqe);
+done:
+	spin_unlock(&cq->cq_lock);
+	return work_done;
+}
+
+#ifdef	NICVF_NAPI_ENABLE
+static int nicvf_poll(struct napi_struct *napi, int budget)
+{
+	int  work_done = 0;
+	struct net_device *netdev = napi->dev;
+	struct nicvf *nic = netdev_priv(netdev);
+	struct nicvf_cq_poll *cq = container_of(napi, struct nicvf_cq_poll, napi);
+	struct netdev_queue *txq;
+
+	work_done = nicvf_cq_intr_handler(netdev, cq->cq_idx, napi, budget);
+
+	txq = netdev_get_tx_queue(netdev, cq->cq_idx);
+	if (netif_tx_queue_stopped(txq))
+		netif_tx_wake_queue(txq);
+
+	if (work_done < budget) {
+		/* Slow packet rate, exit polling */
+		napi_complete(napi);
+		/* Re-enable interrupts */
+		nicvf_enable_intr(nic, NICVF_INTR_CQ, cq->cq_idx);
+	}
+	return work_done;
+}
+#endif
+
+/* Qset error interrupt handler
+ *
+ * As of now only CQ errors are handled
+ */
+void nicvf_handle_qs_err(unsigned long data)
+{
+	struct nicvf *nic = (struct nicvf *)data;
+	struct queue_set *qs = nic->qs;
+	int qidx;
+	uint64_t status;
+
+	netif_tx_disable(nic->netdev);
+
+	/* Check if it is CQ err */
+	for (qidx = 0; qidx < qs->cq_cnt; qidx++) {
+		status = nicvf_queue_reg_read(nic,
+					NIC_QSET_CQ_0_7_STATUS, qidx);
+		if (!(status & CQ_ERR_MASK))
+			continue;
+		/* Process already queued CQEs and reconfig CQ */
+		nicvf_disable_intr(nic, NICVF_INTR_CQ, qidx);
+		nicvf_sq_disable(nic, qidx);
+		nicvf_cq_intr_handler(nic->netdev, qidx, NULL, 0);
+		nicvf_cmp_queue_config(nic, qs, qidx, true);
+		nicvf_sq_free_used_descs(nic->netdev, &qs->sq[qidx], qidx);
+		nicvf_sq_enable(nic, &qs->sq[qidx], qidx);
+
+		nicvf_enable_intr(nic, NICVF_INTR_CQ, qidx);
+	}
+
+	netif_tx_start_all_queues(nic->netdev);
+	/* Re-enable Qset error interrupt */
+	nicvf_enable_intr(nic, NICVF_INTR_QS_ERR, 0);
+}
+
+static irqreturn_t nicvf_misc_intr_handler(int irq, void *nicvf_irq)
+{
+	struct nicvf *nic = (struct nicvf *)nicvf_irq;
+
+	nicvf_handle_mbx_intr(nic);
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t nicvf_intr_handler(int irq, void *nicvf_irq)
+{
+	uint64_t qidx, intr;
+	uint64_t cq_intr, rbdr_intr, qs_err_intr;
+	struct nicvf *nic = (struct nicvf *)nicvf_irq;
+	struct queue_set *qs = nic->qs;
+
+	intr = nicvf_qset_reg_read(nic, NIC_VF_INT);
+	nic_dbg(&nic->pdev->dev, "%s intr status 0x%llx\n", __func__, intr);
+
+	cq_intr = (intr & NICVF_INTR_CQ_MASK) >> NICVF_INTR_CQ_SHIFT;
+	qs_err_intr = intr & NICVF_INTR_QS_ERR_MASK;
+	if (qs_err_intr) {
+		/* Disable Qset err interrupt and schedule softirq */
+		nicvf_disable_intr(nic, NICVF_INTR_QS_ERR, 0);
+		tasklet_hi_schedule(&nic->qs_err_task);
+	}
+
+#ifdef	NICVF_NAPI_ENABLE
+	{
+		struct nicvf_cq_poll *cq_poll = NULL;
+		/* Disable interrupts and start polling */
+		for (qidx = 0; qidx < qs->cq_cnt; qidx++) {
+			if (!(cq_intr & (1 << qidx)))
+				continue;
+			if (!nicvf_is_intr_enabled(nic, NICVF_INTR_CQ, qidx))
+				continue;
+			nicvf_disable_intr(nic, NICVF_INTR_CQ, qidx);
+			cq_poll = nic->napi[qidx];
+			/* Schedule NAPI */
+			napi_schedule(&cq_poll->napi);
+		}
+	}
+#else
+	for (qidx = 0; qidx < qs->cq_cnt; qidx++) {
+		if (cq_intr & (1 << qidx))
+			nicvf_cq_intr_handler(nic->netdev, qidx, NULL, 0);
+	}
+#endif
+	/* Handle RBDR interrupts */
+	rbdr_intr = (intr & NICVF_INTR_RBDR_MASK) >> NICVF_INTR_RBDR_SHIFT;
+	if (rbdr_intr) {
+		/* Disable RBDR interrupt and schedule softirq */
+		for (qidx = 0; qidx < qs->rbdr_cnt; qidx++)
+			nicvf_disable_intr(nic, NICVF_INTR_RBDR, qidx);
+
+		tasklet_hi_schedule(&nic->rbdr_task);
+	}
+
+	/* Clear interrupts */
+	nicvf_qset_reg_write(nic, NIC_VF_INT, intr);
+	return IRQ_HANDLED;
+}
+
+static int nicvf_enable_msix(struct nicvf *nic)
+{
+	int i, ret, vec;
+	struct queue_set *qs = nic->qs;
+
+	nic->num_vec = NIC_VF_MSIX_VECTORS;
+	vec = qs->cq_cnt + qs->rbdr_cnt + qs->sq_cnt;
+	vec = NIC_VF_MSIX_VECTORS;
+	if (vec > NIC_VF_MSIX_VECTORS)
+		nic->num_vec = vec;
+
+	for (i = 0; i < nic->num_vec; i++)
+		nic->msix_entries[i].entry = i;
+
+	ret = pci_enable_msix(nic->pdev, nic->msix_entries, nic->num_vec);
+	if (ret < 0) {
+		netdev_err(nic->netdev,
+			"Request for #%d msix vectors failed\n", nic->num_vec);
+		return 0;
+	} else if (ret > 0) {
+		netdev_err(nic->netdev,
+			"Request for #%d msix vectors failed, requesting #%d\n",
+			nic->num_vec, ret);
+
+		nic->num_vec = ret;
+		ret = pci_enable_msix(nic->pdev, nic->msix_entries, nic->num_vec);
+		if (ret) {
+			netdev_warn(nic->netdev, "Request for msix vectors failed\n");
+			return 0;
+		}
+	}
+	nic->msix_enabled = 1;
+	return 1;
+}
+
+static void nicvf_disable_msix(struct nicvf *nic)
+{
+	if (nic->msix_enabled) {
+		pci_disable_msix(nic->pdev);
+		nic->msix_enabled = 0;
+		nic->num_vec = 0;
+	}
+}
+
+static int nicvf_register_interrupts(struct nicvf *nic)
+{
+	int irq, free, ret = 0;
+
+	for_each_cq_irq(irq)
+		sprintf(nic->irq_name[irq], "%s%d CQ%d", "NICVF",
+						nic->vnic_id, irq);
+
+	for_each_sq_irq(irq)
+		sprintf(nic->irq_name[irq], "%s%d SQ%d", "NICVF",
+			nic->vnic_id, irq - NICVF_SQ_INTR_ID);
+
+	for_each_rbdr_irq(irq)
+		sprintf(nic->irq_name[irq], "%s%d RBDR%d", "NICVF",
+			nic->vnic_id, irq - NICVF_RBDR_INTR_ID);
+
+	/* Register all interrupts except mailbox */
+	for (irq = 0; irq < NICVF_MISC_INTR_ID; irq++) {
+		if ((ret = request_irq(nic->msix_entries[irq].vector,
+				nicvf_intr_handler, 0 , nic->irq_name[irq], nic)))
+			break;
+		nic->irq_allocated[irq] = 1;
+	}
+
+	sprintf(nic->irq_name[NICVF_QS_ERR_INTR_ID],
+				"%s%d Qset error", "VNICVF", nic->vnic_id);
+	if (!ret) {
+		if (!(ret = request_irq(nic->msix_entries[NICVF_QS_ERR_INTR_ID].vector,
+				nicvf_intr_handler, 0 , nic->irq_name[NICVF_QS_ERR_INTR_ID], nic)))
+			nic->irq_allocated[NICVF_QS_ERR_INTR_ID] = 1;
+	}
+
+	if (ret) {
+		netdev_err(nic->netdev, "Request irq failed\n");
+		for (free = 0; free < irq; free++)
+			free_irq(nic->msix_entries[free].vector, nic);
+		return 1;
+	}
+
+	return 0;
+}
+
+static void nicvf_unregister_interrupts(struct nicvf *nic)
+{
+	int irq;
+
+	/* Free registered interrupts */
+	for (irq = 0; irq < nic->num_vec; irq++) {
+		if (nic->irq_allocated[irq])
+			free_irq(nic->msix_entries[irq].vector, nic);
+		nic->irq_allocated[irq] = 0;
+	}
+
+	/* Disable MSI-X */
+	nicvf_disable_msix(nic);
+}
+
+static int nicvf_register_misc_interrupt(struct nicvf *nic)
+{
+	int  ret = 0;
+	int irq = NICVF_MISC_INTR_ID;
+
+	/* Enable MSI-X */
+	if (!nicvf_enable_msix(nic))
+		return 1;
+
+	sprintf(nic->irq_name[irq], "%s%d Mbox", "VNIC", nic->vnic_id);
+	/* Register Misc interrupt */
+	ret = request_irq(nic->msix_entries[irq].vector, nicvf_misc_intr_handler,
+						0, nic->irq_name[irq], nic);
+
+	if (ret)
+		return 1;
+	nic->irq_allocated[irq] = 1;
+
+	/* Enable mailbox interrupt */
+	nicvf_enable_mbx_intr(nic);
+
+	/* Check if VF is able to communicate with PF */
+	if (!nicvf_check_pf_ready(nic)) {
+		nicvf_disable_mbx_intr(nic);
+		nicvf_unregister_interrupts(nic);
+		return 1;
+	}
+
+	return 0;
+}
+
+static void nicvf_update_tx_stats(struct nicvf *nic, struct sk_buff *skb) { }
+
+#ifdef VNIC_SW_TSO_SUPPORT
+static int nicvf_sw_tso(struct sk_buff *skb, struct net_device *netdev)
+{
+	struct sk_buff *segs, *nskb;
+
+	if (!skb_shinfo(skb)->gso_size)
+		return 1;
+
+	/* Segment the large frame */
+	segs = skb_gso_segment(skb, netdev->features & ~NETIF_F_TSO);
+	if (IS_ERR(segs))
+		goto gso_err;
+
+	do {
+		nskb = segs;
+		segs = segs->next;
+		nskb->next = NULL;
+		nicvf_xmit(nskb, netdev);
+	} while (segs);
+
+gso_err:
+	dev_kfree_skb(skb);
+
+	return NETDEV_TX_OK;
+}
+#endif
+
+static netdev_tx_t nicvf_xmit(struct sk_buff *skb, struct net_device *netdev)
+{
+	struct nicvf *nic = netdev_priv(netdev);
+	int qid = skb_get_queue_mapping(skb);
+	struct netdev_queue *txq = netdev_get_tx_queue(netdev, qid);
+	int ret = 1;
+
+	/* Check for minimum packet length */
+	if (skb->len <= ETH_HLEN) {
+		atomic64_add(1, (atomic64_t *)&netdev->stats.tx_errors);
+		dev_kfree_skb(skb);
+		return NETDEV_TX_OK;
+	}
+
+#ifdef VNIC_SW_TSO_SUPPORT
+	if (netdev->features & NETIF_F_TSO)
+		ret = nicvf_sw_tso(skb, netdev);
+#endif
+	if (ret == NETDEV_TX_OK)
+		return NETDEV_TX_OK;
+
+#ifndef VNIC_TX_CSUM_OFFLOAD_SUPPORT
+	/* Calculate checksum in software */
+	if (skb->ip_summed == CHECKSUM_PARTIAL) {
+		if (unlikely(skb_checksum_help(skb))) {
+			netdev_dbg(netdev, "unable to do checksum\n");
+			dev_kfree_skb_any(skb);
+			return NETDEV_TX_OK;
+		}
+	}
+#endif
+#ifdef VNIC_HW_TSO_SUPPORT
+	if (skb_shinfo(skb)->gso_size && ((skb->protocol == ETH_P_IP) &&
+				(ip_hdr(skb)->protocol != IPPROTO_TCP))) {
+		netdev_dbg(netdev,
+			   "Only TCP segmentation is supported, dropping packet\n");
+		dev_kfree_skb_any(skb);
+		return NETDEV_TX_OK;
+	}
+#endif
+	if (!nicvf_sq_append_skb(nic, skb) && !netif_tx_queue_stopped(txq)) {
+		netif_tx_stop_queue(txq);
+		atomic64_add(1, (atomic64_t *)&netdev->stats.tx_dropped);
+		nic_dbg(&nic->pdev->dev,
+			"VF%d: TX ring full, stop transmitting packets\n", nic->vnic_id);
+		return NETDEV_TX_BUSY;
+	}
+
+	nicvf_update_tx_stats(nic, skb);
+	return NETDEV_TX_OK;
+}
+
+static int nicvf_stop(struct net_device *netdev)
+{
+	int qidx;
+	struct nicvf *nic = netdev_priv(netdev);
+	struct queue_set *qs = nic->qs;
+
+	netif_carrier_off(netdev);
+	netif_tx_disable(netdev);
+
+	/* Disable HW Qset, to stop receiving packets */
+	nicvf_qset_config(nic, false);
+
+	/* disable mailbox interrupt */
+	nicvf_disable_mbx_intr(nic);
+
+	/* Disable interrupts */
+	for (qidx = 0; qidx < qs->cq_cnt; qidx++)
+		nicvf_disable_intr(nic, NICVF_INTR_CQ, qidx);
+	for (qidx = 0; qidx < qs->rbdr_cnt; qidx++)
+		nicvf_disable_intr(nic, NICVF_INTR_RBDR, qidx);
+	nicvf_disable_intr(nic, NICVF_INTR_QS_ERR, 0);
+
+	tasklet_kill(&nic->rbdr_task);
+	tasklet_kill(&nic->qs_err_task);
+
+	nicvf_unregister_interrupts(nic);
+#ifdef	NICVF_NAPI_ENABLE
+	for (qidx = 0; qidx < nic->qs->cq_cnt; qidx++) {
+		napi_synchronize(&nic->napi[qidx]->napi);
+		napi_disable(&nic->napi[qidx]->napi);
+		netif_napi_del(&nic->napi[qidx]->napi);
+		kfree(nic->napi[qidx]);
+		nic->napi[qidx] = NULL;
+	}
+#endif
+	/* Free resources */
+	nicvf_config_data_transfer(nic, false);
+
+	/* Free Qset */
+	kfree(qs);
+
+	return 0;
+}
+
+static int nicvf_open(struct net_device *netdev)
+{
+	int err, qidx;
+	struct nicvf *nic = netdev_priv(netdev);
+	struct queue_set *qs;
+	struct nicvf_cq_poll *cq_poll = NULL;
+
+	nic->mtu = netdev->mtu;
+
+	netif_carrier_off(netdev);
+
+	if ((err = nicvf_register_misc_interrupt(nic)))
+		return -EIO;
+
+	if ((err = nicvf_init_resources(nic)))
+		return err;
+
+	qs = nic->qs;
+
+	if ((err = netif_set_real_num_tx_queues(netdev, qs->sq_cnt))) {
+		netdev_err(netdev,
+			"Failed to set real number of Tx queues: %d\n", err);
+		return err;
+	}
+	if ((err = netif_set_real_num_rx_queues(netdev, qs->rq_cnt))) {
+		netdev_err(netdev,
+			"Failed to set real number of Rx queues: %d\n", err);
+		return err;
+	}
+
+	if ((err = nicvf_register_interrupts(nic))) {
+		nicvf_stop(netdev);
+		return -EIO;
+	}
+
+#ifdef	NICVF_NAPI_ENABLE
+	for (qidx = 0; qidx < qs->cq_cnt; qidx++) {
+		cq_poll = kzalloc(sizeof(*cq_poll), GFP_KERNEL);
+		if (!cq_poll)
+			goto napi_del;
+		cq_poll->cq_idx = qidx;
+		netif_napi_add(netdev, &cq_poll->napi, nicvf_poll,
+			       NAPI_POLL_WEIGHT);
+		napi_enable(&cq_poll->napi);
+		nic->napi[qidx] = cq_poll;
+	}
+	goto no_err;
+napi_del:
+	while (qidx) {
+		qidx--;
+		cq_poll = nic->napi[qidx];
+		napi_disable(&cq_poll->napi);
+		netif_napi_del(&cq_poll->napi);
+		kfree(cq_poll);
+		nic->napi[qidx] = NULL;
+	}
+	return -ENOMEM;
+no_err:
+#endif
+
+	/* Set MAC-ID */
+	if (is_zero_ether_addr(netdev->dev_addr))
+		eth_hw_addr_random(netdev);
+
+	nicvf_hw_set_mac_addr(nic, netdev);
+
+	/* Init tasklet for handling Qset err interrupt */
+	tasklet_init(&nic->qs_err_task, nicvf_handle_qs_err, (unsigned long)nic);
+
+	/* Enable Qset err interrupt */
+	nicvf_enable_intr(nic, NICVF_INTR_QS_ERR, 0);
+
+	/* Enable completion queue interrupt */
+	for (qidx = 0; qidx < qs->cq_cnt; qidx++)
+		nicvf_enable_intr(nic, NICVF_INTR_CQ, qidx);
+
+	/* Init RBDR tasklet and enable RBDR threshold interrupt */
+	tasklet_init(&nic->rbdr_task, nicvf_refill_rbdr, (unsigned long)nic);
+
+	for (qidx = 0; qidx < qs->rbdr_cnt; qidx++)
+		nicvf_enable_intr(nic, NICVF_INTR_RBDR, qidx);
+
+	if (nicvf_is_link_active(nic)) {
+		netif_carrier_on(netdev);
+		netif_tx_start_all_queues(netdev);
+	}
+
+	return 0;
+}
+
+static int nicvf_update_hw_max_frs(struct nicvf *nic, int mtu)
+{
+	int ret;
+	struct  nic_mbx *mbx;
+
+	mbx = nicvf_get_mbx();
+	mbx->msg = NIC_VF_SET_MAX_FRS;
+	mbx->data.max_frs = mtu;
+	ret = nicvf_send_msg_to_pf(nic, mbx);
+	kfree(mbx);
+
+	return ret;
+}
+
+static int nicvf_change_mtu(struct net_device *netdev, int new_mtu)
+{
+	struct nicvf *nic = netdev_priv(netdev);
+
+	if (new_mtu > NIC_HW_MAX_FRS)
+		return -EINVAL;
+
+	if (new_mtu < NIC_HW_MIN_FRS)
+		return -EINVAL;
+
+	if (nicvf_update_hw_max_frs(nic, new_mtu))
+		return -EINVAL;
+	netdev->mtu = new_mtu;
+
+	return 0;
+}
+
+static int nicvf_set_mac_address(struct net_device *netdev, void *p)
+{
+	struct sockaddr *addr = p;
+	struct nicvf *nic = netdev_priv(netdev);
+
+	if (!is_valid_ether_addr(addr->sa_data))
+		return -EADDRNOTAVAIL;
+
+	if (nicvf_hw_set_mac_addr(nic, netdev))
+		return -EBUSY;
+
+	memcpy(netdev->dev_addr, addr->sa_data, netdev->addr_len);
+
+	return 0;
+}
+
+static const struct net_device_ops nicvf_netdev_ops = {
+	.ndo_open		= nicvf_open,
+	.ndo_stop		= nicvf_stop,
+	.ndo_start_xmit		= nicvf_xmit,
+	.ndo_change_mtu		= nicvf_change_mtu,
+	.ndo_set_mac_address	= nicvf_set_mac_address,
+#if 0
+	.ndo_get_stats64	= nicvf_get_stats,
+	.ndo_validate_addr	= eth_validate_addr,
+	.ndo_set_rx_mode	= nicvf_set_rx_mode,
+	.ndo_vlan_rx_add_vid	= nicvf_vlan_rx_add_vid,
+	.ndo_vlan_rx_kill_vid	= nicvf_vlan_rx_kill_vid,
+	.ndo_tx_timeout		= nicvf_tx_timeout,
+#endif
+};
+
+static int nicvf_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
+{
+	struct device *dev = &pdev->dev;
+	struct net_device *netdev;
+	struct nicvf *nic;
+	int    err;
+
+	netdev = alloc_etherdev_mqs(sizeof(struct nicvf),
+			MAX_RCV_QUEUES_PER_QS, MAX_SND_QUEUES_PER_QS);
+
+	if (!netdev)
+		return -ENOMEM;
+
+	pci_set_drvdata(pdev, netdev);
+
+	SET_NETDEV_DEV(netdev, &pdev->dev);
+
+	nic = netdev_priv(netdev);
+	nic->netdev = netdev;
+	nic->pdev = pdev;
+
+	err = pci_enable_device(pdev);
+	if (err) {
+		dev_err(dev, "Failed to enable PCI device\n");
+		goto exit;
+	}
+
+	err = pci_request_regions(pdev, DRV_NAME);
+	if (err) {
+		dev_err(dev, "PCI request regions failed 0x%x\n", err);
+		goto err_disable_device;
+	}
+
+	err = pci_set_dma_mask(pdev, DMA_BIT_MASK(48));
+	if (!err) {
+		err = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(48));
+		if (err) {
+			dev_err(dev, "unable to get 48-bit DMA for consistent allocations\n");
+			goto err_release_regions;
+		}
+	} else {
+		dev_err(dev, "Unable to get usable DMA configuration\n");
+		goto err_release_regions;
+	}
+
+	/* MAP VF's configuration registers */
+	nic->reg_base = (uint64_t)pci_ioremap_bar(pdev, PCI_CFG_REG_BAR_NUM);
+	if (!nic->reg_base) {
+		dev_err(dev, "Cannot map config register space, aborting\n");
+		err = -ENOMEM;
+		goto err_release_regions;
+	}
+
+#ifdef VNIC_RX_CSUM_OFFLOAD_SUPPORT
+	netdev->hw_features |= NETIF_F_RXCSUM;
+#endif
+#ifdef VNIC_TX_CSUM_OFFLOAD_SUPPORT
+	netdev->hw_features |= NETIF_F_IP_CSUM;
+#endif
+#ifdef VNIC_SG_SUPPORT
+	netdev->hw_features |= NETIF_F_SG;
+#endif
+#ifdef VNIC_TSO_SUPPORT
+	netdev->hw_features |= NETIF_F_TSO | NETIF_F_SG | NETIF_F_IP_CSUM;
+#endif
+#ifdef VNIC_HW_LRO_SUPPORT
+	netdev->hw_features |= NETIF_F_LRO;
+#endif
+
+	netdev->features |= netdev->hw_features;
+	netdev->netdev_ops = &nicvf_netdev_ops;
+
+	if ((err = register_netdev(netdev))) {
+		dev_err(dev, "Failed to register netdevice\n");
+		goto err_unmap_resources;
+	}
+
+#ifdef NICVF_ETHTOOL_ENABLE
+	nicvf_set_ethtool_ops(netdev);
+#endif
+	goto exit;
+
+err_unmap_resources:
+	if (nic->reg_base)
+		iounmap((void *)nic->reg_base);
+err_release_regions:
+	pci_release_regions(pdev);
+err_disable_device:
+	pci_disable_device(pdev);
+exit:
+	return err;
+}
+
+static void nicvf_remove(struct pci_dev *pdev)
+{
+	struct net_device *netdev = pci_get_drvdata(pdev);
+	struct nicvf *nic;
+
+	if (!netdev)
+		return;
+
+	nic = netdev_priv(netdev);
+	unregister_netdev(netdev);
+
+	pci_set_drvdata(pdev, NULL);
+
+	if (nic->reg_base)
+		iounmap((void *)nic->reg_base);
+
+	pci_release_regions(pdev);
+	pci_disable_device(pdev);
+	free_netdev(netdev);
+}
+
+static struct pci_driver nicvf_driver = {
+	.name = DRV_NAME,
+	.id_table = nicvf_id_table,
+	.probe = nicvf_probe,
+	.remove = nicvf_remove,
+};
+
+static int __init nicvf_init_module(void)
+{
+	pr_info("%s, ver %s\n", DRV_NAME, DRV_VERSION);
+
+	return pci_register_driver(&nicvf_driver);
+}
+
+static void __exit nicvf_cleanup_module(void)
+{
+	pci_unregister_driver(&nicvf_driver);
+}
+
+module_init(nicvf_init_module);
+module_exit(nicvf_cleanup_module);
+
diff --git a/drivers/net/ethernet/cavium/thunder/nicvf_queues.c b/drivers/net/ethernet/cavium/thunder/nicvf_queues.c
new file mode 100644
index 0000000..6131f76
--- /dev/null
+++ b/drivers/net/ethernet/cavium/thunder/nicvf_queues.c
@@ -0,0 +1,1052 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2014 Cavium, Inc.
+ */
+
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/types.h>
+#include <linux/pci.h>
+#include <linux/netdevice.h>
+#include <linux/if_vlan.h>
+#include <linux/ip.h>
+#include <net/checksum.h>
+
+#include "nic_reg.h"
+#include "nic.h"
+#include "q_struct.h"
+#include "nicvf_queues.h"
+
+#define  MIN_SND_QUEUE_DESC_FOR_PKT_XMIT 2
+
+static int nicvf_alloc_q_desc_mem(struct nicvf *nic,
+				struct q_desc_mem *desc_mem,
+				int q_len, int desc_size, int align_bytes)
+{
+	desc_mem->q_len = q_len;
+	desc_mem->size = (desc_size * q_len) + align_bytes;
+	desc_mem->unalign_base = dma_alloc_coherent(&nic->pdev->dev, desc_mem->size,
+							&desc_mem->dma, GFP_ATOMIC);
+	if (!desc_mem->unalign_base)
+		return -1;
+
+	desc_mem->phys_base = NICVF_ALIGNED_ADDR((uint64_t)desc_mem->dma, align_bytes);
+	desc_mem->base = (void *)((u8 *)desc_mem->unalign_base +
+					(desc_mem->phys_base - desc_mem->dma));
+	return 0;
+}
+
+static void nicvf_free_q_desc_mem(struct nicvf *nic, struct q_desc_mem *desc_mem)
+{
+	if (!desc_mem)
+		return;
+
+	dma_free_coherent(&nic->pdev->dev, desc_mem->size,
+			desc_mem->unalign_base, desc_mem->dma);
+	desc_mem->unalign_base = NULL;
+	desc_mem->base = NULL;
+}
+
+static int nicvf_alloc_rcv_buffer(struct nicvf *nic, uint64_t buf_len,
+					unsigned char **rcv_buffer)
+{
+	struct sk_buff *skb = NULL;
+
+	buf_len += NICVF_RCV_BUF_ALIGN_BYTES + sizeof(void *);
+
+	if (!(skb = netdev_alloc_skb(nic->netdev, buf_len))) {
+		netdev_err(nic->netdev, "Failed to allocate new rcv buffer\n");
+		return -ENOMEM;
+	}
+
+	/* Reserve bytes for storing skb address */
+	skb_reserve(skb, sizeof(void *));
+	/* Align buffer addr to cache line i.e 128 bytes */
+	skb_reserve(skb, NICVF_RCV_BUF_ALIGN_LEN((uint64_t)skb->data));
+
+	/* Store skb address */
+	*(struct sk_buff **)(skb->data - sizeof(void *)) = skb;
+
+	/* Return buffer address */
+	*rcv_buffer = skb->data;
+	return 0;
+}
+
+static struct sk_buff *nicvf_rb_ptr_to_skb(uint64_t rb_ptr)
+{
+	struct sk_buff *skb;
+
+	rb_ptr = (uint64_t)phys_to_virt(rb_ptr);
+	skb = (struct sk_buff *)*(uint64_t *)(rb_ptr - sizeof(void *));
+	return skb;
+}
+
+static int  nicvf_init_rbdr(struct nicvf *nic, struct rbdr *rbdr,
+						int ring_len, int buf_size)
+{
+	int idx;
+	unsigned char *rcv_buffer;
+
+	if (nicvf_alloc_q_desc_mem(nic, &rbdr->desc_mem, ring_len,
+				sizeof(struct rbdr_entry_t), NICVF_RCV_BUF_ALIGN_BYTES)) {
+		netdev_err(nic->netdev,
+			"Unable to allocate memory for rcv buffer ring\n");
+		return -ENOMEM;
+	}
+
+	/* Buffer size has to be in multiples of 128 bytes */
+	rbdr->buf_size = buf_size;
+	rbdr->enable = true;
+	rbdr->thresh = ring_len / 2;
+
+	for (idx = 0; idx < ring_len; idx++) {
+		rbdr->desc[idx] = &((struct rbdr_entry_t *)rbdr->desc_mem.base)[idx];
+
+		if (nicvf_alloc_rcv_buffer(nic, rbdr->buf_size, &rcv_buffer))
+			return -ENOMEM;
+
+		rbdr->desc[idx]->buf_addr = pci_map_single(nic->pdev, rcv_buffer,
+				rbdr->buf_size, PCI_DMA_FROMDEVICE) >> NICVF_RCV_BUF_ALIGN;
+	}
+	return 0;
+}
+
+static void nicvf_free_rbdr(struct nicvf *nic, struct rbdr *rbdr, int rbdr_qidx)
+{
+	int head, tail;
+	struct sk_buff *skb;
+
+	if (!rbdr)
+		return;
+
+	rbdr->enable = false;
+	if (!rbdr->desc_mem.base)
+		return;
+
+	head = nicvf_queue_reg_read(nic, NIC_QSET_RBDR_0_1_HEAD,
+							rbdr_qidx) >> 3;
+	tail = nicvf_queue_reg_read(nic, NIC_QSET_RBDR_0_1_TAIL,
+							rbdr_qidx) >> 3;
+	/* Free SKBs */
+	while (head != tail) {
+		skb = nicvf_rb_ptr_to_skb(rbdr->desc[head]->buf_addr << NICVF_RCV_BUF_ALIGN);
+		pci_unmap_single(nic->pdev, (dma_addr_t)skb->data,
+				rbdr->buf_size, PCI_DMA_FROMDEVICE);
+		dev_kfree_skb(skb);
+		head++;
+		head &= (rbdr->desc_mem.q_len - 1);
+	}
+	/* Free RBDR ring */
+	nicvf_free_q_desc_mem(nic, &rbdr->desc_mem);
+}
+
+/* Refill receive buffer descriptors with new buffers.
+ * This runs in softirq context .
+ */
+void nicvf_refill_rbdr(unsigned long data)
+{
+	struct nicvf *nic = (struct nicvf *)data;
+	struct queue_set *qs = nic->qs;
+	int rbdr_idx = qs->rbdr_cnt;
+	int tail_ptr, qcount;
+	int refill_rb_cnt, rb_cnt = 0;
+	struct rbdr *rbdr;
+	unsigned char *rcv_buffer;
+
+refill:
+	if (!rbdr_idx)
+		return;
+	rbdr_idx--;
+	rbdr = &qs->rbdr[rbdr_idx];
+	/* Check if it's enabled */
+	if (!rbdr->enable)
+		goto next_rbdr;
+
+	/* check if valid descs reached or crossed threshold level */
+	qcount = nicvf_queue_reg_read(nic, NIC_QSET_RBDR_0_1_STATUS0,
+							rbdr_idx) & 0x7FFFF;
+	if (qcount > rbdr->thresh)
+		goto next_rbdr;
+
+	/* Get no of desc's to be refilled */
+	refill_rb_cnt = rbdr->thresh;
+	rb_cnt = refill_rb_cnt;
+
+	/* Start filling descs from tail */
+	tail_ptr = nicvf_queue_reg_read(nic, NIC_QSET_RBDR_0_1_TAIL,
+							rbdr_idx) >> 3;
+	while (refill_rb_cnt) {
+		tail_ptr++;
+		tail_ptr &= (rbdr->desc_mem.q_len - 1);
+
+		if (nicvf_alloc_rcv_buffer(nic, rbdr->buf_size, &rcv_buffer))
+			break;
+
+		rbdr->desc[tail_ptr]->buf_addr = pci_map_single(nic->pdev,
+						rcv_buffer, rbdr->buf_size,
+				PCI_DMA_FROMDEVICE) >> NICVF_RCV_BUF_ALIGN;
+		refill_rb_cnt--;
+	}
+	/* Notify HW */
+	if (rb_cnt)
+		nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_DOOR,
+						rbdr_idx, rb_cnt);
+next_rbdr:
+	if (rbdr_idx)
+		goto refill;
+
+	/* Re-enable RBDR interrupts */
+	for (rbdr_idx = 0; rbdr_idx < qs->rbdr_cnt; rbdr_idx++)
+		nicvf_enable_intr(nic, NICVF_INTR_RBDR, rbdr_idx);
+}
+
+static int nicvf_init_rcv_queue(struct rcv_queue *rq)
+{
+	/* Nothing to do */
+	return 0;
+}
+
+static void nicvf_free_rcv_queue(struct rcv_queue *rq)
+{
+	/* Nothing to do */
+}
+
+/* TBD: how to handle full packets received in CQ
+ * i.e conversion of buffers into SKBs
+ */
+static int nicvf_init_cmp_queue(struct nicvf *nic, struct cmp_queue *cq,
+								int q_len)
+{
+	if (nicvf_alloc_q_desc_mem(nic, &cq->desc_mem, q_len,
+				CMP_QUEUE_DESC_SIZE, NICVF_CQ_BASE_ALIGN_BYTES)) {
+		netdev_err(nic->netdev,
+			"Unable to allocate memory for Tx/Rx notification queue\n");
+		return -ENOMEM;
+	}
+	cq->intr_timer_thresh = 0;
+	cq->thresh = CMP_QUEUE_THRESH;
+
+	return 0;
+}
+
+static void nicvf_free_cmp_queue(struct nicvf *nic, struct cmp_queue *cq)
+{
+	if (!cq)
+		return;
+	if (!cq->desc_mem.base)
+		return;
+
+	nicvf_free_q_desc_mem(nic, &cq->desc_mem);
+}
+
+static int nicvf_init_snd_queue(struct nicvf *nic, struct snd_queue *sq,
+								int q_len)
+{
+	if (nicvf_alloc_q_desc_mem(nic, &sq->desc_mem, q_len,
+				SND_QUEUE_DESC_SIZE, NICVF_SQ_BASE_ALIGN_BYTES)) {
+		netdev_err(nic->netdev,
+			"Unable to allocate memory for transmit queue\n");
+		return -ENOMEM;
+	}
+
+	sq->skbuff = kcalloc(q_len, sizeof(uint64_t), GFP_ATOMIC);
+	sq->head = 0;
+	sq->tail = 0;
+	sq->free_cnt = q_len;
+	sq->thresh = SND_QUEUE_THRESH;
+
+	return 0;
+}
+
+static void nicvf_free_snd_queue(struct nicvf *nic, struct snd_queue *sq)
+{
+	if (!sq)
+		return;
+	if (!sq->desc_mem.base)
+		return;
+
+	kfree(sq->skbuff);
+	nicvf_free_q_desc_mem(nic, &sq->desc_mem);
+}
+
+static void nicvf_rcv_queue_config(struct nicvf *nic, struct queue_set *qs,
+							int qidx, bool enable)
+{
+	struct  nic_mbx *mbx;
+	struct rcv_queue *rq;
+
+	rq = &qs->rq[qidx];
+
+	if (!enable) {
+		/* Disable receive queue */
+		nicvf_queue_reg_write(nic, NIC_QSET_RQ_0_7_CFG, qidx, 0);
+		return;
+	}
+
+	rq->cq_qs = qs->vnic_id;
+	rq->cq_idx = qidx;
+	rq->start_rbdr_qs = qs->vnic_id;
+	rq->start_qs_rbdr_idx = qs->rbdr_cnt - 1;
+	rq->cont_rbdr_qs = qs->vnic_id;
+	rq->cont_qs_rbdr_idx = qs->rbdr_cnt - 1;
+
+	/* Send a mailbox msg to PF to config RQ */
+	mbx = nicvf_get_mbx();
+	mbx->msg = NIC_PF_VF_MSG_RQ_CFG;
+	mbx->data.rq.qs_num = qs->vnic_id;
+	mbx->data.rq.rq_num = qidx;
+	mbx->data.rq.cfg = (rq->cq_qs << 19) | (rq->cq_idx << 16) |
+			(rq->cont_rbdr_qs << 9) | (rq->cont_qs_rbdr_idx << 8) |
+			(rq->start_rbdr_qs << 1) | (rq->start_qs_rbdr_idx);
+	nicvf_send_msg_to_pf(nic, mbx);
+
+	/* RQ drop config
+	 * Enable CQ drop to reserve sufficient CQEs for all tx packets
+	 */
+	mbx->msg = NIC_PF_VF_MSG_RQ_DROP_CFG;
+	mbx->data.rq.cfg = (1ULL << 62) | (RQ_CQ_DROP << 8);
+	nicvf_send_msg_to_pf(nic, mbx);
+
+	/* Enable Receive queue */
+	nicvf_queue_reg_write(nic, NIC_QSET_RQ_0_7_CFG, qidx, (1ULL << 1));
+
+	kfree(mbx);
+}
+
+void nicvf_cmp_queue_config(struct nicvf *nic, struct queue_set *qs,
+						int qidx, bool enable)
+{
+	struct cmp_queue *cq;
+
+	cq = &qs->cq[qidx];
+	if (!enable) {
+		/* Disable completion queue */
+		nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_CFG, qidx, 0);
+		return;
+	}
+
+	/* Reset completion queue */
+	nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_CFG, qidx, (1ULL << 41));
+
+	/* Enable Completion queue */
+	nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_CFG, qidx,
+			(1ULL << 42) | (((qs->cq_len >> 10) - 1) << 32));
+
+	/* Set threshold value for interrupt generation */
+	nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_THRESH, qidx,
+							cq->thresh);
+
+	/* Set completion queue base address */
+	nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_BASE, qidx,
+					(uint64_t)(cq->desc_mem.phys_base));
+
+	/* Set CQ's head entry */
+	nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_HEAD, qidx, 0);
+}
+
+/* TBD
+ * - Set TL3 index
+ */
+static void nicvf_snd_queue_config(struct nicvf *nic, struct queue_set *qs,
+							int qidx, bool enable)
+{
+	struct snd_queue *sq;
+	struct nic_mbx *mbx;
+
+	sq = &qs->sq[qidx];
+	if (!enable) {
+		/* Disable send queue */
+		nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_CFG, qidx, 0);
+		return;
+	}
+
+	/* Reset send queue */
+	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_CFG, qidx, (1ULL << 17));
+
+	sq->cq_qs = qs->vnic_id;
+	sq->cq_idx = qidx;
+
+	/* Send a mailbox msg to PF to config SQ */
+	mbx = nicvf_get_mbx();
+	mbx->msg = NIC_PF_VF_MSG_SQ_CFG;
+	mbx->data.sq.qs_num = qs->vnic_id;
+	mbx->data.sq.sq_num = qidx;
+	mbx->data.sq.cfg = (sq->cq_qs << 3) | sq->cq_idx;
+	nicvf_send_msg_to_pf(nic, mbx);
+
+	/* Enable send queue  & set queue size */
+	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_CFG, qidx,
+			(1ULL << 19) | (((qs->sq_len >> 10) - 1) << 8));
+
+	/* Set threshold value for interrupt generation */
+	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_THRESH, qidx,
+							sq->thresh);
+
+	/* Set queue base address */
+	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_BASE, qidx,
+					(uint64_t)(sq->desc_mem.phys_base));
+
+	/* Set SQ's head entry */
+	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_HEAD, qidx, 0);
+	kfree(mbx);
+}
+
+static void nicvf_rbdr_config(struct nicvf *nic, struct queue_set *qs,
+						int qidx, bool enable)
+{
+	struct rbdr *rbdr;
+
+	rbdr = &qs->rbdr[qidx];
+	if (!enable) {
+		/* Disable RBDR */
+		nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_CFG, qidx, 0);
+		return;
+	}
+
+	/* Reset RBDR */
+	nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_CFG, qidx, (1ULL << 43));
+
+	/* Enable RBDR  & set queue size */
+	/* Buffer size should be in multiples of 128 bytes */
+	nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_CFG, qidx, (1ULL << 44) |
+		(((qs->rbdr_len >> 13) - 1) << 32) | (rbdr->buf_size / 128));
+
+	/* Set descriptor base address */
+	nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_BASE, qidx,
+					(uint64_t)(rbdr->desc_mem.phys_base));
+
+	/* Set RBDR head entry */
+	nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_HEAD, qidx, 0);
+
+	/* Notify HW */
+	nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_DOOR, qidx,
+						qs->rbdr_len - 1);
+
+	/* Set threshold value for interrupt generation */
+	nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_THRESH, qidx,
+						rbdr->thresh - 1);
+}
+
+void nicvf_qset_config(struct nicvf *nic, bool enable)
+{
+	struct  nic_mbx *mbx;
+	struct queue_set *qs = nic->qs;
+
+	/* Send a mailbox msg to PF to config Qset */
+	mbx = nicvf_get_mbx();
+	mbx->msg = NIC_PF_VF_MSG_QS_CFG;
+	mbx->data.qs.num = qs->vnic_id;
+
+	if (enable) {
+		mbx->data.qs.cfg = 0x80000000 | qs->vnic_id;
+		nicvf_send_msg_to_pf(nic, mbx);
+	} else {  /* disable Qset */
+		mbx->data.qs.cfg = 0;
+		nicvf_send_msg_to_pf(nic, mbx);
+	}
+	kfree(mbx);
+}
+
+static void nicvf_free_resources(struct nicvf *nic)
+{
+	int qidx;
+	struct queue_set *qs = nic->qs;
+
+	if (!qs)
+		return;
+
+	/* Free receive buffer descriptor ring */
+	for (qidx = 0; qidx < qs->rbdr_cnt; qidx++)
+		nicvf_free_rbdr(nic, &qs->rbdr[qidx], qidx);
+
+	/* Free receive queue */
+	for (qidx = 0; qidx < qs->rq_cnt; qidx++)
+		nicvf_free_rcv_queue(&qs->rq[qidx]);
+
+	/* Free completion queue */
+	for (qidx = 0; qidx < qs->cq_cnt; qidx++)
+		nicvf_free_cmp_queue(nic, &qs->cq[qidx]);
+
+	/* Free send queue */
+	for (qidx = 0; qidx < qs->sq_cnt; qidx++)
+		nicvf_free_snd_queue(nic, &qs->sq[qidx]);
+}
+
+static int nicvf_init_resources(struct nicvf *nic)
+{
+	int qidx;
+	struct queue_set *qs = nic->qs;
+
+	/* Set queue count */
+	qs->rbdr_cnt = RBDR_CNT;
+	qs->rq_cnt = RCV_QUEUE_CNT;
+	qs->sq_cnt = SND_QUEUE_CNT;
+	qs->cq_cnt = CMP_QUEUE_CNT;
+
+	/* Set queue lengths */
+	qs->rbdr_len = RCV_BUF_COUNT;
+	qs->sq_len = SND_QUEUE_LEN;
+	qs->cq_len = CMP_QUEUE_LEN;
+
+	/* Alloc receive buffer descriptor ring */
+	for (qidx = 0; qidx < qs->rbdr_cnt; qidx++) {
+		if (nicvf_init_rbdr(nic, &qs->rbdr[qidx], qs->rbdr_len,
+							RCV_BUFFER_LEN))
+			goto alloc_fail;
+	}
+
+	/* Alloc receive queue */
+	for (qidx = 0; qidx < qs->rq_cnt; qidx++) {
+		if (nicvf_init_rcv_queue(&qs->rq[qidx]))
+			goto alloc_fail;
+	}
+
+	/* Alloc send queue */
+	for (qidx = 0; qidx < qs->sq_cnt; qidx++) {
+		if (nicvf_init_snd_queue(nic, &qs->sq[qidx], qs->sq_len))
+			goto alloc_fail;
+	}
+
+	/* Alloc completion queue */
+	for (qidx = 0; qidx < qs->cq_cnt; qidx++) {
+		if (nicvf_init_cmp_queue(nic, &qs->cq[qidx], qs->cq_len))
+			goto alloc_fail;
+	}
+
+	return 0;
+alloc_fail:
+	nicvf_free_resources(nic);
+	return -ENOMEM;
+}
+
+int nicvf_config_data_transfer(struct nicvf *nic, bool enable)
+{
+	bool disable = false;
+	struct queue_set *qs;
+	int qidx;
+
+	if (enable) {
+		if (!(qs = kzalloc(sizeof(*qs), GFP_ATOMIC)))
+			return -ENOMEM;
+
+		qs->vnic_id = nic->vnic_id;
+		nic->qs = qs;
+
+		if (nicvf_init_resources(nic))
+			return -ENOMEM;
+
+		for (qidx = 0; qidx < qs->rbdr_cnt; qidx++)
+			nicvf_rbdr_config(nic, qs, qidx, enable);
+		for (qidx = 0; qidx < qs->rq_cnt; qidx++)
+			nicvf_rcv_queue_config(nic, qs, qidx, enable);
+		for (qidx = 0; qidx < qs->sq_cnt; qidx++)
+			nicvf_snd_queue_config(nic, qs, qidx, enable);
+		for (qidx = 0; qidx < qs->cq_cnt; qidx++)
+			nicvf_cmp_queue_config(nic, qs, qidx, enable);
+
+	} else {
+		if (!(qs = nic->qs))
+			return 0;
+
+		for (qidx = 0; qidx < qs->rbdr_cnt; qidx++)
+			nicvf_rbdr_config(nic, qs, qidx, disable);
+		for (qidx = 0; qidx < qs->rq_cnt; qidx++)
+			nicvf_rcv_queue_config(nic, qs, qidx, disable);
+		for (qidx = 0; qidx < qs->sq_cnt; qidx++)
+			nicvf_snd_queue_config(nic, qs, qidx, disable);
+		for (qidx = 0; qidx < qs->cq_cnt; qidx++)
+			nicvf_cmp_queue_config(nic, qs, qidx, disable);
+
+		nicvf_free_resources(nic);
+	}
+
+	return 0;
+}
+
+/* Check for errors in the cmp.queue entry */
+int nicvf_cq_check_errs(struct nicvf *nic, void *cq_desc)
+{
+	uint32_t ret = false;
+	struct cqe_rx_t *cqe_rx;
+
+	cqe_rx = (struct cqe_rx_t *)cq_desc;
+	if (cqe_rx->err_level || cqe_rx->err_opcode)
+		ret = true;
+
+	return ret;
+}
+
+/* Get a free desc from send queue
+ * @qs:   Qset from which to get a SQ descriptor
+ * @qnum: SQ number (0...7) in the Qset
+ *
+ * returns descriptor ponter & descriptor number
+ */
+static int nicvf_get_sq_desc(struct queue_set *qs, int qnum, void **desc)
+{
+	int qentry;
+	struct snd_queue *sq = &qs->sq[qnum];
+
+	if (!sq->free_cnt)
+		return 0;
+
+	qentry = sq->tail++;
+	sq->free_cnt--;
+	sq->tail &= (sq->desc_mem.q_len - 1);
+	*desc = sq->desc_mem.base + (qentry * SND_QUEUE_DESC_SIZE);
+	return qentry;
+}
+
+void nicvf_put_sq_desc(struct snd_queue *sq, int desc_cnt)
+{
+	while (desc_cnt--) {
+		sq->free_cnt++;
+		sq->head++;
+		sq->head &= (sq->desc_mem.q_len - 1);
+	}
+}
+
+void nicvf_sq_enable(struct nicvf *nic, struct snd_queue *sq, int qidx)
+{
+	uint64_t reg_val;
+
+	reg_val = nicvf_queue_reg_read(nic, NIC_QSET_SQ_0_7_CFG, qidx);
+	reg_val |= (1ULL << 19);
+	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_CFG, qidx, reg_val);
+	/* Ring doorbell so that H/W restarts processing SQEs */
+	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_DOOR, qidx, 0);
+}
+
+void nicvf_sq_disable(struct nicvf *nic, int qidx)
+{
+	uint64_t reg_val;
+
+	reg_val = nicvf_queue_reg_read(nic, NIC_QSET_SQ_0_7_CFG, qidx);
+	reg_val &= ~(1ULL << 19);
+	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_CFG, qidx, reg_val);
+}
+
+void nicvf_sq_free_used_descs(struct net_device *netdev, struct snd_queue *sq, int qidx)
+{
+	uint64_t head, tail;
+	struct sk_buff *skb;
+	struct nicvf *nic = netdev_priv(netdev);
+	struct sq_hdr_subdesc *hdr;
+
+	head = nicvf_queue_reg_read(nic, NIC_QSET_SQ_0_7_HEAD, qidx) >> 4;
+	tail = nicvf_queue_reg_read(nic, NIC_QSET_SQ_0_7_TAIL, qidx) >> 4;
+	while (sq->head != head) {
+		hdr  = (struct sq_hdr_subdesc *)(sq->desc_mem.base +
+			(sq->head * SND_QUEUE_DESC_SIZE));
+		if (hdr->subdesc_type != SQ_DESC_TYPE_HEADER) {
+			nicvf_put_sq_desc(sq, 1);
+			continue;
+		}
+		skb = (struct sk_buff *)sq->skbuff[sq->head];
+		atomic64_add(1, (atomic64_t *)&netdev->stats.tx_packets);
+		atomic64_add(hdr->tot_len, (atomic64_t *)&netdev->stats.tx_bytes);
+		nicvf_free_skb(nic, skb);
+		nicvf_put_sq_desc(sq, hdr->subdesc_cnt + 1);
+	}
+}
+
+static int nicvf_sq_subdesc_required(struct nicvf *nic, struct sk_buff *skb)
+{
+	int subdesc_cnt = MIN_SND_QUEUE_DESC_FOR_PKT_XMIT;
+
+	if (skb_shinfo(skb)->nr_frags)
+		subdesc_cnt += skb_shinfo(skb)->nr_frags;
+
+#ifdef VNIC_TX_CSUM_OFFLOAD_SUPPORT
+	if (skb->ip_summed == CHECKSUM_PARTIAL) {
+		if (skb->protocol == htons(ETH_P_IP))
+			subdesc_cnt++;
+		if ((ip_hdr(skb)->protocol == IPPROTO_TCP) ||
+			(ip_hdr(skb)->protocol == IPPROTO_UDP))
+			subdesc_cnt++;
+	}
+#endif
+
+	return subdesc_cnt;
+}
+
+/* Add SQ HEADER subdescriptor.
+ * First subdescriptor for every send descriptor.
+ */
+struct sq_hdr_subdesc *
+nicvf_sq_add_hdr_subdesc(struct queue_set *qs, int sq_num,
+				int subdesc_cnt, struct sk_buff *skb)
+{
+	int qentry;
+	void *desc;
+	struct snd_queue *sq;
+	struct sq_hdr_subdesc *hdr;
+
+	sq = &qs->sq[sq_num];
+	qentry = nicvf_get_sq_desc(qs, sq_num, &desc);
+	sq->skbuff[qentry] = (uint64_t)skb;
+
+	hdr = (struct sq_hdr_subdesc *)desc;
+
+	memset(hdr, 0, SND_QUEUE_DESC_SIZE); /* TBD: Need to remove these memset */
+	hdr->subdesc_type = SQ_DESC_TYPE_HEADER;
+	hdr->post_cqe = 1;
+	hdr->subdesc_cnt = subdesc_cnt;
+	hdr->tot_len = skb->len;
+
+#ifdef VNIC_HW_TSO_SUPPORT
+	if (!skb_shinfo(skb)->gso_size)
+		return hdr;
+
+	/* Packet to be subjected to TSO */
+	hdr->tso = 1;
+	hdr->tso_l4_offset = (int)(skb_transport_header(skb) - skb->data) +
+				tcp_hdrlen(skb);
+	hdr->tso_max_paysize = skb_shinfo(skb)->gso_size + hdr->tso_l4_offset;
+	/* TBD: These fields have to be setup properly */
+	hdr->tso_sdc_first	= 0;
+	hdr->tso_sdc_cont	= 0;
+	hdr->tso_flags_first	= 0;
+	hdr->tso_flags_last	= 0;
+#endif
+	return hdr;
+}
+
+/* SQ GATHER subdescriptor
+ * Must follow HDR descriptor
+ */
+static void nicvf_sq_add_gather_subdesc(struct nicvf *nic, struct queue_set *qs,
+						int sq_num, struct sk_buff *skb)
+{
+	int i;
+	void *desc;
+	struct sq_gather_subdesc *gather;
+
+	nicvf_get_sq_desc(qs, sq_num, &desc);
+	gather = (struct sq_gather_subdesc *)desc;
+
+	memset(gather, 0, SND_QUEUE_DESC_SIZE);
+	gather->subdesc_type = SQ_DESC_TYPE_GATHER;
+	gather->ld_type = 1;
+	gather->size = skb_is_nonlinear(skb) ? skb_headlen(skb) : skb->len;
+	gather->addr = pci_map_single(nic->pdev, skb->data,
+				gather->size, PCI_DMA_TODEVICE);
+
+	if (!skb_is_nonlinear(skb))
+		return;
+
+	for (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {
+		const struct skb_frag_struct *frag;
+
+		frag = &skb_shinfo(skb)->frags[i];
+
+		nicvf_get_sq_desc(qs, sq_num, &desc);
+		gather = (struct sq_gather_subdesc *)desc;
+
+		memset(gather, 0, SND_QUEUE_DESC_SIZE);
+		gather->subdesc_type = SQ_DESC_TYPE_GATHER;
+		gather->ld_type = 1;
+		gather->size = skb_frag_size(frag);
+		gather->addr = pci_map_single(nic->pdev, skb_frag_address(frag),
+						gather->size, PCI_DMA_TODEVICE);
+	}
+}
+
+static void nicvf_fill_l3_crc_subdesc(struct sq_crc_subdesc *l3,
+					struct sk_buff *skb)
+{
+	int crc_pos;
+
+	crc_pos = skb_network_header(skb) - skb_mac_header(skb);
+	crc_pos += offsetof(struct iphdr, check);
+
+	l3->subdesc_type = SQ_DESC_TYPE_CRC;
+	l3->crc_alg = SEND_CRCALG_CRC32;
+	l3->crc_insert_pos = crc_pos;
+	l3->hdr_start = skb_network_offset(skb);
+	l3->crc_len = skb_transport_header(skb) - skb_network_header(skb);
+	l3->crc_ival = 0;
+}
+
+static void nicvf_fill_l4_crc_subdesc(struct sq_crc_subdesc *l4,
+					struct sk_buff *skb)
+{
+	l4->subdesc_type = SQ_DESC_TYPE_CRC;
+	l4->crc_alg = SEND_CRCALG_CRC32;
+	l4->crc_insert_pos = skb->csum_start + skb->csum_offset;
+	l4->hdr_start = skb->csum_start;
+	l4->crc_len = skb->len - skb_transport_offset(skb);
+	l4->crc_ival = 0;
+}
+
+/* SQ CRC subdescriptor
+ * Must follow HDR and precede GATHER, IMM subdescriptors
+ */
+static void nicvf_sq_add_crc_subdesc(struct nicvf *nic, struct queue_set *qs,
+						int sq_num, struct sk_buff *skb)
+{
+	int proto;
+	void *desc;
+	struct sq_crc_subdesc *crc;
+	struct snd_queue *sq;
+
+	if (skb->ip_summed != CHECKSUM_PARTIAL)
+		return;
+
+	if (skb->protocol != htons(ETH_P_IP))
+		return;
+
+	sq = &qs->sq[sq_num];
+	nicvf_get_sq_desc(qs, sq_num, &desc);
+
+	crc = (struct sq_crc_subdesc *)desc;
+
+	nicvf_fill_l3_crc_subdesc(crc, skb);
+
+	proto = ip_hdr(skb)->protocol;
+	if ((proto == IPPROTO_TCP) || (proto == IPPROTO_UDP)) {
+		nicvf_get_sq_desc(qs, sq_num, &desc);
+		crc = (struct sq_crc_subdesc *)desc;
+		nicvf_fill_l4_crc_subdesc(crc, skb);
+	}
+}
+
+/* Append an skb to a SQ for packet transfer. */
+int nicvf_sq_append_skb(struct nicvf *nic, struct sk_buff *skb)
+{
+	int subdesc_cnt;
+	int sq_num;
+	struct queue_set *qs = nic->qs;
+	struct snd_queue *sq;
+	struct sq_hdr_subdesc *hdr_desc;
+
+	sq_num = skb_get_queue_mapping(skb);
+	sq = &qs->sq[sq_num];
+
+	subdesc_cnt = nicvf_sq_subdesc_required(nic, skb);
+
+	if (subdesc_cnt > sq->free_cnt)
+		goto append_fail;
+
+	/* Add SQ header subdesc */
+	hdr_desc = nicvf_sq_add_hdr_subdesc(qs, sq_num, subdesc_cnt - 1, skb);
+
+#ifdef VNIC_TX_CSUM_OFFLOAD_SUPPORT
+	/* Add CRC subdescriptor for IP/TCP/UDP (L3/L4) crc calculation */
+	if (skb->ip_summed == CHECKSUM_PARTIAL)
+		nicvf_sq_add_crc_subdesc(nic, qs, sq_num, skb);
+#endif
+
+	/* Add SQ gather subdesc */
+	nicvf_sq_add_gather_subdesc(nic, qs, sq_num, skb);
+
+	/* Inform HW to xmit new packet */
+	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_DOOR,
+						sq_num, subdesc_cnt);
+	return 1;
+
+append_fail:
+	nic_dbg(&nic->pdev->dev, "Not enough SQ descriptors to xmit pkt\n");
+	return 0;
+}
+
+struct sk_buff *nicvf_get_rcv_skb(struct nicvf *nic, void *cq_desc)
+{
+	int frag;
+	int payload_len = 0;
+	struct sk_buff *skb = NULL;
+	struct sk_buff *skb_frag = NULL;
+	struct sk_buff *prev_frag = NULL;
+	struct cqe_rx_t *cqe_rx;
+	struct rbdr *rbdr;
+	struct rcv_queue *rq;
+	struct queue_set *qs = nic->qs;
+	uint16_t *rb_lens = NULL;
+	uint64_t *rb_ptrs = NULL;
+
+	cqe_rx = (struct cqe_rx_t *)cq_desc;
+
+	rq = &qs->rq[cqe_rx->rq_idx];
+	rbdr = &qs->rbdr[rq->start_qs_rbdr_idx];
+	rb_lens = cq_desc + (3 * sizeof(uint64_t)); /* Use offsetof */
+	rb_ptrs = cq_desc + (6 * sizeof(uint64_t));
+	nic_dbg(&nic->pdev->dev, "%s rb_cnt %d rb0_ptr %llx rb0_sz %d\n",
+		__func__, cqe_rx->rb_cnt, cqe_rx->rb0_ptr, cqe_rx->rb0_sz);
+
+	for (frag = 0; frag < cqe_rx->rb_cnt; frag++) {
+		payload_len = *rb_lens;
+		if (!frag) {
+			skb = nicvf_rb_ptr_to_skb(*rb_ptrs);
+			skb_put(skb, payload_len);
+			/* First fragment */
+			pci_unmap_single(nic->pdev, (dma_addr_t)skb->data, rbdr->buf_size, PCI_DMA_FROMDEVICE);
+		} else {
+			/* Add fragments */
+			skb_frag = nicvf_rb_ptr_to_skb(*rb_ptrs);
+			pci_unmap_single(nic->pdev, (dma_addr_t)skb_frag->data, rbdr->buf_size, PCI_DMA_FROMDEVICE);
+
+			if (!skb_shinfo(skb)->frag_list)
+				skb_shinfo(skb)->frag_list = skb_frag;
+			else
+				prev_frag->next = skb_frag;
+
+			prev_frag = skb_frag;
+			skb->len += payload_len;
+			skb->data_len += payload_len;
+			skb_frag->len = payload_len;
+		}
+		/* Next buffer pointer */
+		rb_lens++;
+		rb_ptrs++;
+	}
+	return skb;
+}
+
+/* Enable interrupt */
+void nicvf_enable_intr(struct nicvf *nic, int int_type, int q_idx)
+{
+	uint64_t reg_val;
+
+	reg_val = nicvf_qset_reg_read(nic, NIC_VF_ENA_W1S);
+
+	switch (int_type) {
+	case NICVF_INTR_CQ:
+		reg_val |= ((1ULL << q_idx) << NICVF_INTR_CQ_SHIFT);
+	break;
+	case NICVF_INTR_SQ:
+		reg_val |= ((1ULL << q_idx) << NICVF_INTR_SQ_SHIFT);
+	break;
+	case NICVF_INTR_RBDR:
+		reg_val |= ((1ULL << q_idx) << NICVF_INTR_RBDR_SHIFT);
+	break;
+	case NICVF_INTR_PKT_DROP:
+		reg_val |= (1ULL << NICVF_INTR_PKT_DROP_SHIFT);
+	break;
+	case NICVF_INTR_TCP_TIMER:
+		reg_val |= (1ULL << NICVF_INTR_TCP_TIMER_SHIFT);
+	break;
+	case NICVF_INTR_MBOX:
+		reg_val |= (1ULL << NICVF_INTR_MBOX_SHIFT);
+	break;
+	case NICVF_INTR_QS_ERR:
+		reg_val |= (1ULL << NICVF_INTR_QS_ERR_SHIFT);
+	break;
+	default:
+		netdev_err(nic->netdev, "Failed to enable interrupt: unknown interrupt type\n");
+	break;
+	}
+
+	nicvf_qset_reg_write(nic, NIC_VF_ENA_W1S, reg_val);
+}
+
+/* Disable interrupt */
+void nicvf_disable_intr(struct nicvf *nic, int int_type, int q_idx)
+{
+	uint64_t reg_val = 0;
+
+	switch (int_type) {
+	case NICVF_INTR_CQ:
+		reg_val |= ((1ULL << q_idx) << NICVF_INTR_CQ_SHIFT);
+	break;
+	case NICVF_INTR_SQ:
+		reg_val |= ((1ULL << q_idx) << NICVF_INTR_SQ_SHIFT);
+	break;
+	case NICVF_INTR_RBDR:
+		reg_val |= ((1ULL << q_idx) << NICVF_INTR_RBDR_SHIFT);
+	break;
+	case NICVF_INTR_PKT_DROP:
+		reg_val |= (1ULL << NICVF_INTR_PKT_DROP_SHIFT);
+	break;
+	case NICVF_INTR_TCP_TIMER:
+		reg_val |= (1ULL << NICVF_INTR_TCP_TIMER_SHIFT);
+	break;
+	case NICVF_INTR_MBOX:
+		reg_val |= (1ULL << NICVF_INTR_MBOX_SHIFT);
+	break;
+	case NICVF_INTR_QS_ERR:
+		reg_val |= (1ULL << NICVF_INTR_QS_ERR_SHIFT);
+	break;
+	default:
+		netdev_err(nic->netdev, "Failed to disable interrupt: unknown interrupt type\n");
+	break;
+	}
+
+	nicvf_qset_reg_write(nic, NIC_VF_ENA_W1C, reg_val);
+}
+
+/* Clear interrupt */
+void nicvf_clear_intr(struct nicvf *nic, int int_type, int q_idx)
+{
+	uint64_t reg_val = 0;
+
+	switch (int_type) {
+	case NICVF_INTR_CQ:
+		reg_val = ((1ULL << q_idx) << NICVF_INTR_CQ_SHIFT);
+	break;
+	case NICVF_INTR_SQ:
+		reg_val = ((1ULL << q_idx) << NICVF_INTR_SQ_SHIFT);
+	break;
+	case NICVF_INTR_RBDR:
+		reg_val = ((1ULL << q_idx) << NICVF_INTR_RBDR_SHIFT);
+	break;
+	case NICVF_INTR_PKT_DROP:
+		reg_val = (1ULL << NICVF_INTR_PKT_DROP_SHIFT);
+	break;
+	case NICVF_INTR_TCP_TIMER:
+		reg_val = (1ULL << NICVF_INTR_TCP_TIMER_SHIFT);
+	break;
+	case NICVF_INTR_MBOX:
+		reg_val = (1ULL << NICVF_INTR_MBOX_SHIFT);
+	break;
+	case NICVF_INTR_QS_ERR:
+		reg_val |= (1ULL << NICVF_INTR_QS_ERR_SHIFT);
+	break;
+	default:
+		netdev_err(nic->netdev, "Failed to clear interrupt: unknown interrupt type\n");
+	break;
+	}
+
+	nicvf_qset_reg_write(nic, NIC_VF_INT, reg_val);
+}
+
+/* Check if interrupt is enabled */
+int nicvf_is_intr_enabled(struct nicvf *nic, int int_type, int q_idx)
+{
+	uint64_t reg_val;
+	uint64_t mask = 0xff;
+
+	reg_val = nicvf_qset_reg_read(nic, NIC_VF_ENA_W1S);
+
+	switch (int_type) {
+	case NICVF_INTR_CQ:
+		mask = ((1ULL << q_idx) << NICVF_INTR_CQ_SHIFT);
+	break;
+	case NICVF_INTR_SQ:
+		mask = ((1ULL << q_idx) << NICVF_INTR_SQ_SHIFT);
+	break;
+	case NICVF_INTR_RBDR:
+		mask = ((1ULL << q_idx) << NICVF_INTR_RBDR_SHIFT);
+	break;
+	case NICVF_INTR_PKT_DROP:
+		mask = NICVF_INTR_PKT_DROP_MASK;
+	break;
+	case NICVF_INTR_TCP_TIMER:
+		mask = NICVF_INTR_TCP_TIMER_MASK;
+	break;
+	case NICVF_INTR_MBOX:
+		mask = NICVF_INTR_MBOX_MASK;
+	break;
+	case NICVF_INTR_QS_ERR:
+		mask = NICVF_INTR_QS_ERR_MASK;
+	break;
+	default:
+		netdev_err(nic->netdev,
+			   "Failed to check interrupt enable: unknown interrupt type\n");
+	break;
+	}
+
+	return (reg_val & mask);
+}
diff --git a/drivers/net/ethernet/cavium/thunder/nicvf_queues.h b/drivers/net/ethernet/cavium/thunder/nicvf_queues.h
new file mode 100644
index 0000000..06770f9
--- /dev/null
+++ b/drivers/net/ethernet/cavium/thunder/nicvf_queues.h
@@ -0,0 +1,182 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2013 Cavium, Inc.
+ */
+
+#ifndef NICVF_QUEUES_H
+#define NICVF_QUEUES_H
+
+#include "q_struct.h"
+
+#define    MAX_QUEUE_SET			128
+#define    MAX_RCV_QUEUES_PER_QS		8
+#define    MAX_RCV_BUF_DESC_RINGS_PER_QS	2
+#define    MAX_SND_QUEUES_PER_QS		8
+#define    MAX_CMP_QUEUES_PER_QS		8
+
+#define    RBDR_SIZE0	 0ULL /* 8K entries */
+#define    RBDR_SIZE1	 1ULL /* 16K entries */
+#define    RBDR_SIZE2	 2ULL /* 32K entries */
+#define    RBDR_SIZE3	 3ULL /* 64K entries */
+#define    RBDR_SIZE4	 4ULL /* 126K entries */
+#define    RBDR_SIZE5	 5ULL /* 256K entries */
+#define    RBDR_SIZE6	 6ULL /* 512K entries */
+
+#define    SND_QUEUE_SIZE0	 0ULL /* 1K entries */
+#define    SND_QUEUE_SIZE1	 1ULL /* 2K entries */
+#define    SND_QUEUE_SIZE2	 2ULL /* 4K entries */
+#define    SND_QUEUE_SIZE3	 3ULL /* 8K entries */
+#define    SND_QUEUE_SIZE4	 4ULL /* 16K entries */
+#define    SND_QUEUE_SIZE5	 5ULL /* 32K entries */
+#define    SND_QUEUE_SIZE6	 6ULL /* 64K entries */
+
+#define    CMP_QUEUE_SIZE0	 0ULL /* 1K entries */
+#define    CMP_QUEUE_SIZE1	 1ULL /* 2K entries */
+#define    CMP_QUEUE_SIZE2	 2ULL /* 4K entries */
+#define    CMP_QUEUE_SIZE3	 3ULL /* 8K entries */
+#define    CMP_QUEUE_SIZE4	 4ULL /* 16K entries */
+#define    CMP_QUEUE_SIZE5	 5ULL /* 32K entries */
+#define    CMP_QUEUE_SIZE6	 6ULL /* 64K entries */
+
+/* Default queue count per QS, its lengths and threshold values */
+#define    RBDR_CNT		1
+#define    RCV_QUEUE_CNT	1
+#define    SND_QUEUE_CNT	8
+#define    CMP_QUEUE_CNT	8 /* Max of RCV and SND qcount */
+
+#define    SND_QUEUE_LEN	(1ULL << (SND_QUEUE_SIZE0 + 10))
+#define    SND_QUEUE_THRESH	2ULL
+
+#define    CMP_QUEUE_LEN	(1ULL << (CMP_QUEUE_SIZE1 + 10))
+#define    CMP_QUEUE_THRESH	0
+
+#define    RCV_BUF_COUNT	(1ULL << (RBDR_SIZE0 + 13))
+#define    RBDR_THRESH		2048
+#define    RCV_BUFFER_LEN	2048 /* In multiples of 128bytes */
+#define    RQ_CQ_DROP		((CMP_QUEUE_LEN - SND_QUEUE_LEN) / 256) /* To ensure CQEs for all transmitted pkts */
+#define    RQ_RBDR_DROP		((256) / 256) /* Drop rx pkts if free buffers fall below 256 */
+
+/* Descriptor size */
+#define    SND_QUEUE_DESC_SIZE		16   /* 128 bits */
+#define    CMP_QUEUE_DESC_SIZE		512
+
+/* Buffer / descriptor alignments */
+#define    NICVF_RCV_BUF_ALIGN		7
+#define    NICVF_RCV_BUF_ALIGN_BYTES	(1ULL << NICVF_RCV_BUF_ALIGN)
+#define    NICVF_CQ_BASE_ALIGN_BYTES	512  /* 9 bits */
+#define    NICVF_SQ_BASE_ALIGN_BYTES	128  /* 7 bits */
+
+#define    NICVF_ALIGNED_ADDR(ADDR, ALIGN_BYTES)		ALIGN(ADDR, ALIGN_BYTES)
+#define    NICVF_ADDR_ALIGN_LEN(ADDR, BYTES)	(NICVF_ALIGNED_ADDR(ADDR, BYTES) - BYTES)
+#define    NICVF_RCV_BUF_ALIGN_LEN(X)		(NICVF_ALIGNED_ADDR(X, NICVF_RCV_BUF_ALIGN_BYTES) - X)
+
+struct q_desc_mem {
+	dma_addr_t	dma;
+	uint64_t	size;
+	uint16_t	q_len;
+	dma_addr_t	phys_base;
+	void		*base;
+	void		*unalign_base;
+};
+
+struct rbdr {
+	bool		enable;
+	uint32_t	buf_size;
+	uint32_t	thresh;      /* Threshold level for interrupt */
+	struct q_desc_mem   desc_mem;
+	struct rbdr_entry_t    *desc[RCV_BUF_COUNT];
+};
+
+struct rcv_queue {
+	struct	rbdr  *rbdr_start;
+	struct	rbdr  *rbdr_cont;
+	bool	en_tcp_reassembly;
+	uint8_t cq_qs;  /* CQ's QS to which this RQ is assigned */
+	uint8_t cq_idx; /* CQ index (0 to 7) in the QS */
+	uint8_t cont_rbdr_qs;      /* Continue buffer pointers - QS num */
+	uint8_t cont_qs_rbdr_idx;  /* RBDR idx in the cont QS */
+	uint8_t start_rbdr_qs;     /* First buffer pointers - QS num */
+	uint8_t start_qs_rbdr_idx; /* RBDR idx in the above QS */
+};
+
+struct cmp_queue {
+	struct q_desc_mem   desc_mem;
+	uint8_t    intr_timer_thresh;
+	uint16_t   thresh;
+	spinlock_t cq_lock;  /* lock to serialize processing CQEs */
+};
+
+struct sq_desc {
+	bool   free;
+	struct sq_desc  *next;
+};
+
+struct snd_queue {
+	struct    q_desc_mem   desc_mem;
+	uint8_t   cq_qs;  /* CQ's QS to which this SQ is pointing */
+	uint8_t   cq_idx; /* CQ index (0 to 7) in the above QS */
+	uint16_t  thresh;
+	uint16_t  free_cnt;
+	uint64_t  head;
+	uint64_t  tail;
+	uint64_t  *skbuff;
+};
+
+struct queue_set {
+	bool      enabled;
+	bool      be_en;
+	uint8_t   vnic_id;
+	uint8_t   rq_cnt;
+	struct	  rcv_queue rq[MAX_RCV_QUEUES_PER_QS];
+	uint8_t   cq_cnt;
+	struct    cmp_queue cq[MAX_CMP_QUEUES_PER_QS];
+	uint64_t  cq_len;
+	uint8_t   sq_cnt;
+	struct    snd_queue sq[MAX_SND_QUEUES_PER_QS];
+	uint64_t  sq_len;
+	uint8_t   rbdr_cnt;
+	struct    rbdr  rbdr[MAX_RCV_BUF_DESC_RINGS_PER_QS];
+	uint64_t  rbdr_len;
+};
+
+/* CQ status bits */
+#define	CQ_WR_FULL	(1 << 26)
+#define	CQ_WR_DISABLE	(1 << 25)
+#define	CQ_WR_FAULT	(1 << 24)
+#define	CQ_CQE_COUNT	(0xFFFF << 0)
+
+/* CQ err mask */
+#define		CQ_ERR_MASK	(CQ_WR_FULL | CQ_WR_DISABLE | CQ_WR_FAULT)
+
+int nicvf_config_data_transfer(struct nicvf *nic, bool enable);
+void nicvf_qset_config(struct nicvf *nic, bool enable);
+void nicvf_sq_enable(struct nicvf *nic, struct snd_queue *sq, int qidx);
+void nicvf_sq_disable(struct nicvf *nic, int qidx);
+void nicvf_put_sq_desc(struct snd_queue *sq, int desc_cnt);
+void nicvf_sq_free_used_descs(struct net_device *netdev, struct snd_queue *sq, int qidx);
+int nicvf_sq_append_skb(struct nicvf *nic, struct sk_buff *skb);
+
+int nicvf_cq_check_errs(struct nicvf *nic, void *cq_desc);
+struct sk_buff *nicvf_get_rcv_skb(struct nicvf *nic, void *cq_desc);
+void nicvf_refill_rbdr(unsigned long data);
+
+void nicvf_enable_intr(struct nicvf *nic, int int_type, int q_idx);
+void nicvf_disable_intr(struct nicvf *nic, int int_type, int q_idx);
+void nicvf_clear_intr(struct nicvf *nic, int int_type, int q_idx);
+int nicvf_is_intr_enabled(struct nicvf *nic, int int_type, int q_idx);
+
+/* Register access APIs */
+void nicvf_reg_write(struct nicvf *nic, uint64_t offset, uint64_t val);
+uint64_t nicvf_reg_read(struct nicvf *nic, uint64_t offset);
+
+void nicvf_qset_reg_write(struct nicvf *nic, uint64_t offset, uint64_t val);
+uint64_t nicvf_qset_reg_read(struct nicvf *nic, uint64_t offset);
+
+void nicvf_queue_reg_write(struct nicvf *nic, uint64_t offset,
+				uint64_t qidx, uint64_t val);
+uint64_t nicvf_queue_reg_read(struct nicvf *nic, uint64_t offset, uint64_t qidx);
+void nicvf_cmp_queue_config(struct nicvf *nic, struct queue_set *qs, int qidx, bool enable);
+#endif /* NICVF_QUEUES_H */
diff --git a/drivers/net/ethernet/cavium/thunder/q_struct.h b/drivers/net/ethernet/cavium/thunder/q_struct.h
new file mode 100644
index 0000000..7e42e2eb
--- /dev/null
+++ b/drivers/net/ethernet/cavium/thunder/q_struct.h
@@ -0,0 +1,546 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2013 Cavium, Inc.
+ */
+
+#ifndef Q_STRUCT_H
+#define Q_STRUCT_H
+
+enum ether_type_algorithm {
+	ETYPE_ALG_NONE = 0x0,
+	ETYPE_ALG_SKIP = 0x1,
+	ETYPE_ALG_ENDPARSE = 0x2,
+	ETYPE_ALG_VLAN = 0x3,
+	ETYPE_ALG_VLAN_STRIP = 0x4,
+};
+
+enum layer3_type {
+	L3TYPE_NONE = 0x00,
+	L3TYPE_GRH = 0x01,
+	L3TYPE_IPV4 = 0x04,
+	L3TYPE_IPV4_OPTIONS = 0x05,
+	L3TYPE_IPV6 = 0x06,
+	L3TYPE_IPV6_OPTIONS = 0x07,
+	L3TYPE_ET_STOP = 0x0D,
+	L3TYPE_OTHER = 0x0E,
+};
+
+enum layer4_type {
+	L4TYPE_NONE = 0x00,
+	L4TYPE_IPSEC_ESP = 0x01,
+	L4TYPE_IPFRAG = 0x02,
+	L4TYPE_IPCOMP = 0x03,
+	L4TYPE_TCP = 0x04,
+	L4TYPE_UDP = 0x05,
+	L4TYPE_SCTP = 0x06,
+	L4TYPE_GRE = 0x07,
+	L4TYPE_ROCE_BTH = 0x08,
+	L4TYPE_OTHER = 0x0E,
+};
+
+enum rss_algorithm_type {
+	RSS_ALG_NONE = 0x00,
+	RSS_ALG_PORT = 0x01,
+	RSS_ALG_IP = 0x02,
+	RSS_ALG_TCP_IP = 0x03,
+	RSS_ALG_UDP_IP = 0x04,
+	RSS_ALG_SCTP_IP = 0x05,
+	RSS_ALG_GRE_IP = 0x06,
+	RSS_ALG_ROCE = 0x07,
+};
+
+/* Completion queue entry types */
+enum cqe_type {
+	CQE_TYPE_INVALID = 0x0,
+	CQE_TYPE_RX = 0x2,
+	CQE_TYPE_RX_SPLIT = 0x3,
+	CQE_TYPE_RX_TCP = 0x4,
+	CQE_TYPE_SEND = 0x8,
+	CQE_TYPE_SEND_PTP = 0x9,
+};
+
+enum cqe_rx_tcp_status {
+	CQE_RX_STATUS_VALID_TCP_CNXT = 0x00,
+	CQE_RX_STATUS_INVALID_TCP_CNXT = 0x0F,
+};
+
+enum cqe_send_status {
+	CQE_SEND_STATUS_GOOD = 0x00,
+	CQE_SEND_STATUS_DESC_FAULT = 0x01,
+	CQE_SEND_STATUS_HDR_CONS_ERR = 0x11,
+	CQE_SEND_STATUS_SUBDESC_ERR = 0x12,
+	CQE_SEND_STATUS_IMM_SIZE_OFLOW = 0x80,
+	CQE_SEND_STATUS_CRC_SEQ_ERR = 0x81,
+	CQE_SEND_STATUS_DATA_SEQ_ERR = 0x82,
+	CQE_SEND_STATUS_MEM_SEQ_ERR = 0x83,
+	CQE_SEND_STATUS_LOCK_VIOL = 0x84,
+	CQE_SEND_STATUS_LOCK_UFLOW = 0x85,
+	CQE_SEND_STATUS_DATA_FAULT = 0x86,
+	CQE_SEND_STATUS_TSTMP_CONFLICT = 0x87,
+	CQE_SEND_STATUS_TSTMP_TIMEOUT = 0x88,
+	CQE_SEND_STATUS_MEM_FAULT = 0x89,
+	CQE_SEND_STATUS_CSUM_OVERLAP = 0x8A,
+	CQE_SEND_STATUS_CSUM_OVERFLOW = 0x8B,
+};
+
+enum cqe_rx_tcp_end_reason {
+	CQE_RX_TCP_END_FIN_FLAG_DET = 0,
+	CQE_RX_TCP_END_INVALID_FLAG = 1,
+	CQE_RX_TCP_END_TIMEOUT = 2,
+	CQE_RX_TCP_END_OUT_OF_SEQ = 3,
+	CQE_RX_TCP_END_PKT_ERR = 4,
+	CQE_RX_TCP_END_QS_DISABLED = 0x0F,
+};
+
+/* Packet protocol level error enumeration */
+enum cqe_rx_err_level {
+	CQE_RX_ERRLVL_RE = 0x0,
+	CQE_RX_ERRLVL_L2 = 0x1,
+	CQE_RX_ERRLVL_L3 = 0x2,
+	CQE_RX_ERRLVL_L4 = 0x3,
+};
+
+/* Packet protocol level error type enumeration */
+enum cqe_rx_err_opcode {
+	CQE_RX_ERR_RE_NONE = 0x0,
+	CQE_RX_ERR_RE_PARTIAL = 0x1,
+	CQE_RX_ERR_RE_JABBER = 0x2,
+	CQE_RX_ERR_RE_FCS = 0x7,
+	CQE_RX_ERR_RE_TERMINATE = 0x9,
+	CQE_RX_ERR_RE_RX_CTL = 0xb,
+	CQE_RX_ERR_PREL2_ERR = 0x1f,
+	CQE_RX_ERR_L2_FRAGMENT = 0x20,
+	CQE_RX_ERR_L2_OVERRUN = 0x21,
+	CQE_RX_ERR_L2_PFCS = 0x22,
+	CQE_RX_ERR_L2_PUNY = 0x23,
+	CQE_RX_ERR_L2_MAL = 0x24,
+	CQE_RX_ERR_L2_OVERSIZE = 0x25,
+	CQE_RX_ERR_L2_UNDERSIZE = 0x26,
+	CQE_RX_ERR_L2_LENMISM = 0x27,
+	CQE_RX_ERR_L2_PCLP = 0x28,
+	CQE_RX_ERR_IP_NOT = 0x41,
+	CQE_RX_ERR_IP_CHK = 0x42,
+	CQE_RX_ERR_IP_MAL = 0x43,
+	CQE_RX_ERR_IP_MALD = 0x44,
+	CQE_RX_ERR_IP_HOP = 0x45,
+	CQE_RX_ERR_L3_ICRC = 0x46,
+	CQE_RX_ERR_L3_PCLP = 0x47,
+	CQE_RX_ERR_L4_MAL = 0x61,
+	CQE_RX_ERR_L4_CHK = 0x62,
+	CQE_RX_ERR_UDP_LEN = 0x63,
+	CQE_RX_ERR_L4_PORT = 0x64,
+	CQE_RX_ERR_TCP_FLAG = 0x65,
+	CQE_RX_ERR_TCP_OFFSET = 0x66,
+	CQE_RX_ERR_L4_PCLP = 0x67,
+	CQE_RX_ERR_RBDR_TRUNC = 0x70,
+};
+
+struct cqe_rx_t {
+#if defined(__BIG_ENDIAN_BITFIELD)
+	uint64_t   rb11_ptr:64;
+	uint64_t   rb10_ptr:64;
+	uint64_t   rb9_ptr:64;
+	uint64_t   rb8_ptr:64;
+	uint64_t   rb7_ptr:64;
+	uint64_t   rb6_ptr:64;
+	uint64_t   rb5_ptr:64;
+	uint64_t   rb4_ptr:64;
+	uint64_t   rb3_ptr:64;
+	uint64_t   rb2_ptr:64;
+	uint64_t   rb1_ptr:64;
+	uint64_t   rb0_ptr:64;
+	uint64_t   rb11_sz:16; /* W5 */
+	uint64_t   rb10_sz:16;
+	uint64_t   rb9_sz:16;
+	uint64_t   rb8_sz:16;
+	uint64_t   rb7_sz:16; /* W4 */
+	uint64_t   rb6_sz:16;
+	uint64_t   rb5_sz:16;
+	uint64_t   rb4_sz:16;
+	uint64_t   rb3_sz:16; /* W3 */
+	uint64_t   rb2_sz:16;
+	uint64_t   rb1_sz:16;
+	uint64_t   rb0_sz:16;
+	uint64_t   rss_tag:32; /* W2 */
+	uint64_t   vlan_tci:16;
+	uint64_t   vlan_ptr:8;
+	uint64_t   vlan2_ptr:8;
+	uint64_t   pkt_len:16; /* W1 */
+	uint64_t   l2_ptr:8;
+	uint64_t   l3_ptr:8;
+	uint64_t   l4_ptr:8;
+	uint64_t   cq_pkt_len:8;
+	uint64_t   align_len:3;
+	uint64_t   rsvd3:1;
+	uint64_t   chan:12;
+	uint64_t   cqe_type:4; /* W0 */
+	uint64_t   stdn_fault:1;
+	uint64_t   rsvd0:1;
+	uint64_t   rq_qs:7;
+	uint64_t   rq_idx:3;
+	uint64_t   rsvd1:12;
+	uint64_t   rss_alg:4;
+	uint64_t   rsvd2:4;
+	uint64_t   rb_cnt:4;
+	uint64_t   vlan_found:1;
+	uint64_t   vlan_stripped:1;
+	uint64_t   vlan2_found:1;
+	uint64_t   vlan2_stripped:1;
+	uint64_t   l4_type:4;
+	uint64_t   l3_type:4;
+	uint64_t   l2_present:1;
+	uint64_t   err_level:3;
+	uint64_t   err_opcode:8;
+#elif defined(__LITTLE_ENDIAN_BITFIELD)
+	uint64_t   err_opcode:8;
+	uint64_t   err_level:3;
+	uint64_t   l2_present:1;
+	uint64_t   l3_type:4;
+	uint64_t   l4_type:4;
+	uint64_t   vlan2_stripped:1;
+	uint64_t   vlan2_found:1;
+	uint64_t   vlan_stripped:1;
+	uint64_t   vlan_found:1;
+	uint64_t   rb_cnt:4;
+	uint64_t   rsvd2:4;
+	uint64_t   rss_alg:4;
+	uint64_t   rsvd1:12;
+	uint64_t   rq_idx:3;
+	uint64_t   rq_qs:7;
+	uint64_t   rsvd0:1;
+	uint64_t   stdn_fault:1;
+	uint64_t   cqe_type:4; /* W0 */
+	uint64_t   chan:12;
+	uint64_t   rsvd3:1;
+	uint64_t   align_len:3;
+	uint64_t   cq_pkt_len:8;
+	uint64_t   l4_ptr:8;
+	uint64_t   l3_ptr:8;
+	uint64_t   l2_ptr:8;
+	uint64_t   pkt_len:16; /* W1 */
+	uint64_t   vlan2_ptr:8;
+	uint64_t   vlan_ptr:8;
+	uint64_t   vlan_tci:16;
+	uint64_t   rss_tag:32; /* W2 */
+	uint64_t   rb0_sz:16;
+	uint64_t   rb1_sz:16;
+	uint64_t   rb2_sz:16;
+	uint64_t   rb3_sz:16; /* W3 */
+	uint64_t   rb4_sz:16;
+	uint64_t   rb5_sz:16;
+	uint64_t   rb6_sz:16;
+	uint64_t   rb7_sz:16; /* W4 */
+	uint64_t   rb8_sz:16;
+	uint64_t   rb9_sz:16;
+	uint64_t   rb10_sz:16;
+	uint64_t   rb11_sz:16; /* W5 */
+	uint64_t   rb0_ptr:64;
+	uint64_t   rb1_ptr:64;
+	uint64_t   rb2_ptr:64;
+	uint64_t   rb3_ptr:64;
+	uint64_t   rb4_ptr:64;
+	uint64_t   rb5_ptr:64;
+	uint64_t   rb6_ptr:64;
+	uint64_t   rb7_ptr:64;
+	uint64_t   rb8_ptr:64;
+	uint64_t   rb9_ptr:64;
+	uint64_t   rb10_ptr:64;
+	uint64_t   rb11_ptr:64;
+#endif
+};
+
+struct cqe_rx_tcp_err_t {
+#if defined(__BIG_ENDIAN_BITFIELD)
+	uint64_t   rsvd1:4;
+	uint64_t   partial_first:1;
+	uint64_t   rsvd2:27;
+	uint64_t   rbdr_bytes:8;
+	uint64_t   rsvd3:24;
+	uint64_t   cqe_type:4;
+	uint64_t   rsvd0:60;
+#elif defined(__LITTLE_ENDIAN_BITFIELD)
+	uint64_t   rsvd0:60;
+	uint64_t   cqe_type:4;
+	uint64_t   rsvd3:24;
+	uint64_t   rbdr_bytes:8;
+	uint64_t   rsvd2:27;
+	uint64_t   partial_first:1;
+	uint64_t   rsvd1:4;
+#endif
+};
+
+struct cqe_rx_tcp_t {
+#if defined(__BIG_ENDIAN_BITFIELD)
+	uint64_t   rsvd1:32;
+	uint64_t   tcp_cntx_bytes:8;
+	uint64_t   rsvd2:8;
+	uint64_t   tcp_err_bytes:16;
+	uint64_t   cqe_type:4;
+	uint64_t   rsvd0:52;
+	uint64_t   cq_tcp_status:8;
+#elif defined(__LITTLE_ENDIAN_BITFIELD)
+	uint64_t   cq_tcp_status:8;
+	uint64_t   rsvd0:52;
+	uint64_t   cqe_type:4;
+	uint64_t   tcp_err_bytes:16;
+	uint64_t   rsvd2:8;
+	uint64_t   tcp_cntx_bytes:8;
+	uint64_t   rsvd1:32;
+#endif
+};
+
+struct cqe_send_t {
+#if defined(__BIG_ENDIAN_BITFIELD)
+	uint64_t   ptp_timestamp:64;
+	uint64_t   cqe_type:4;
+	uint64_t   rsvd0:4;
+	uint64_t   sqe_ptr:16;
+	uint64_t   rsvd1:4;
+	uint64_t   rsvd2:10;
+	uint64_t   sq_qs:7;
+	uint64_t   sq_idx:3;
+	uint64_t   rsvd3:8;
+	uint64_t   send_status:8;
+#elif defined(__LITTLE_ENDIAN_BITFIELD)
+	uint64_t   send_status:8;
+	uint64_t   rsvd3:8;
+	uint64_t   sq_idx:3;
+	uint64_t   sq_qs:7;
+	uint64_t   rsvd2:10;
+	uint64_t   rsvd1:4;
+	uint64_t   sqe_ptr:16;
+	uint64_t   rsvd0:4;
+	uint64_t   cqe_type:4;
+	uint64_t   ptp_timestamp:64;
+#endif
+};
+
+union cq_desc_t {
+	uint64_t u[64];
+	struct cqe_send_t snd_hdr;
+	struct cqe_rx_t rx_hdr;
+	struct cqe_rx_tcp_t rx_tcp_hdr;
+	struct cqe_rx_tcp_err_t rx_tcp_err_hdr;
+};
+
+struct rbdr_entry_t {
+#if defined(__BIG_ENDIAN_BITFIELD)
+	uint64_t   rsvd0:15;
+	uint64_t   buf_addr:42;
+	uint64_t   cache_align:7;
+#elif defined(__LITTLE_ENDIAN_BITFIELD)
+	uint64_t   cache_align:7;
+	uint64_t   buf_addr:42;
+	uint64_t   rsvd0:15;
+#endif
+};
+
+/* TCP reassembly context */
+struct rbe_tcp_cnxt_t {
+#if defined(__BIG_ENDIAN_BITFIELD)
+	uint64_t   tcp_pkt_cnt:12;
+	uint64_t   rsvd1:4;
+	uint64_t   align_hdr_bytes:4;
+	uint64_t   align_ptr_bytes:4;
+	uint64_t   ptr_bytes:16;
+	uint64_t   rsvd2:24;
+	uint64_t   cqe_type:4;
+	uint64_t   rsvd0:54;
+	uint64_t   tcp_end_reason:2;
+	uint64_t   tcp_status:4;
+#elif defined(__LITTLE_ENDIAN_BITFIELD)
+	uint64_t   tcp_status:4;
+	uint64_t   tcp_end_reason:2;
+	uint64_t   rsvd0:54;
+	uint64_t   cqe_type:4;
+	uint64_t   rsvd2:24;
+	uint64_t   ptr_bytes:16;
+	uint64_t   align_ptr_bytes:4;
+	uint64_t   align_hdr_bytes:4;
+	uint64_t   rsvd1:4;
+	uint64_t   tcp_pkt_cnt:12;
+#endif
+};
+
+/* Always Big endian */
+struct rx_hdr_t {
+	uint64_t   opaque:32;
+	uint64_t   rss_flow:8;
+	uint64_t   skip_length:6;
+	uint64_t   disable_rss:1;
+	uint64_t   disable_tcp_reassembly:1;
+	uint64_t   nodrop:1;
+	uint64_t   dest_alg:2;
+	uint64_t   rsvd0:2;
+	uint64_t   dest_rq:11;
+};
+
+enum send_l4_csum_type {
+	SEND_L4_CSUM_DISABLE = 0x00,
+	SEND_L4_CSUM_UDP = 0x01,
+	SEND_L4_CSUM_TCP = 0x02,
+	SEND_L4_CSUM_SCTP = 0x03,
+};
+
+enum send_crc_alg {
+	SEND_CRCALG_CRC32 = 0x00,
+	SEND_CRCALG_CRC32C = 0x01,
+	SEND_CRCALG_ICRC = 0x02,
+};
+
+enum send_load_type {
+	SEND_LD_TYPE_LDD = 0x00,
+	SEND_LD_TYPE_LDT = 0x01,
+	SEND_LD_TYPE_LDWB = 0x02,
+};
+
+enum send_mem_alg_type {
+	SEND_MEMALG_SET = 0x00,
+	SEND_MEMALG_ADD = 0x08,
+	SEND_MEMALG_SUB = 0x09,
+	SEND_MEMALG_ADDLEN = 0x0A,
+	SEND_MEMALG_SUBLEN = 0x0B,
+};
+
+enum send_mem_dsz_type {
+	SEND_MEMDSZ_B64 = 0x00,
+	SEND_MEMDSZ_B32 = 0x01,
+	SEND_MEMDSZ_B8 = 0x03,
+};
+
+enum sq_subdesc_type {
+	SQ_DESC_TYPE_INVALID = 0x00,
+	SQ_DESC_TYPE_HEADER = 0x01,
+	SQ_DESC_TYPE_CRC = 0x02,
+	SQ_DESC_TYPE_IMMEDIATE = 0x03,
+	SQ_DESC_TYPE_GATHER = 0x04,
+	SQ_DESC_TYPE_MEMORY = 0x05,
+};
+
+struct sq_crc_subdesc {
+#if defined(__BIG_ENDIAN_BITFIELD)
+	uint64_t    rsvd1:32;
+	uint64_t    crc_ival:32;
+	uint64_t    subdesc_type:4;
+	uint64_t    crc_alg:2;
+	uint64_t    rsvd0:10;
+	uint64_t    crc_insert_pos:16;
+	uint64_t    hdr_start:16;
+	uint64_t    crc_len:16;
+#elif defined(__LITTLE_ENDIAN_BITFIELD)
+	uint64_t    crc_len:16;
+	uint64_t    hdr_start:16;
+	uint64_t    crc_insert_pos:16;
+	uint64_t    rsvd0:10;
+	uint64_t    crc_alg:2;
+	uint64_t    subdesc_type:4;
+	uint64_t    crc_ival:32;
+	uint64_t    rsvd1:32;
+#endif
+};
+
+struct sq_gather_subdesc {
+#if defined(__BIG_ENDIAN_BITFIELD)
+	uint64_t    rsvd1:15;
+	uint64_t    addr:49;
+	uint64_t    subdesc_type:4;
+	uint64_t    ld_type:2;
+	uint64_t    rsvd0:42;
+	uint64_t    size:16;
+#elif defined(__LITTLE_ENDIAN_BITFIELD)
+	uint64_t    size:16;
+	uint64_t    rsvd0:42;
+	uint64_t    ld_type:2;
+	uint64_t    subdesc_type:4;
+	uint64_t    addr:49;
+	uint64_t    rsvd1:15;
+#endif
+};
+
+/* SQ immediate subdescriptor */
+struct sq_imm_subdesc {
+#if defined(__BIG_ENDIAN_BITFIELD)
+	uint64_t    data:64;
+	uint64_t    subdesc_type:4;
+	uint64_t    rsvd0:46;
+	uint64_t    len:14;
+#elif defined(__LITTLE_ENDIAN_BITFIELD)
+	uint64_t    len:14;
+	uint64_t    rsvd0:46;
+	uint64_t    subdesc_type:4;
+	uint64_t    data:64;
+#endif
+};
+
+struct sq_mem_subdesc {
+#if defined(__BIG_ENDIAN_BITFIELD)
+	uint64_t    rsvd1:15;
+	uint64_t    addr:49;
+	uint64_t    subdesc_type:4;
+	uint64_t    mem_alg:4;
+	uint64_t    mem_dsz:2;
+	uint64_t    wmem:1;
+	uint64_t    rsvd0:21;
+	uint64_t    offset:32;
+#elif defined(__LITTLE_ENDIAN_BITFIELD)
+	uint64_t    offset:32;
+	uint64_t    rsvd0:21;
+	uint64_t    wmem:1;
+	uint64_t    mem_dsz:2;
+	uint64_t    mem_alg:4;
+	uint64_t    subdesc_type:4;
+	uint64_t    addr:49;
+	uint64_t    rsvd1:15;
+#endif
+};
+
+struct sq_hdr_subdesc {
+#if defined(__BIG_ENDIAN_BITFIELD)
+	uint64_t    tso_sdc_cont:8;
+	uint64_t    tso_sdc_first:8;
+	uint64_t    tso_l4_offset:8;
+	uint64_t    tso_flags_last:12;
+	uint64_t    tso_flags_first:12;
+	uint64_t    rsvd2:2;
+	uint64_t    tso_max_paysize:14;
+	uint64_t    subdesc_type:4;
+	uint64_t    tso:1;
+	uint64_t    post_cqe:1; /* Post CQE on no error also */
+	uint64_t    dont_send:1;
+	uint64_t    tstmp:1;
+	uint64_t    subdesc_cnt:8;
+	uint64_t    csum_l4:2;
+	uint64_t    csum_l3:1;
+	uint64_t    rsvd0:5;
+	uint64_t    l4_offset:8;
+	uint64_t    l3_offset:8;
+	uint64_t    rsvd1:4;
+	uint64_t    tot_len:20;
+#elif defined(__LITTLE_ENDIAN_BITFIELD)
+	uint64_t    tot_len:20;
+	uint64_t    rsvd1:4;
+	uint64_t    l3_offset:8;
+	uint64_t    l4_offset:8;
+	uint64_t    rsvd0:5;
+	uint64_t    csum_l3:1;
+	uint64_t    csum_l4:2;
+	uint64_t    subdesc_cnt:8;
+	uint64_t    tstmp:1;
+	uint64_t    dont_send:1;
+	uint64_t    post_cqe:1; /* Post CQE on no error also */
+	uint64_t    tso:1;
+	uint64_t    subdesc_type:4;
+	uint64_t    tso_max_paysize:14;
+	uint64_t    rsvd2:2;
+	uint64_t    tso_flags_first:12;
+	uint64_t    tso_flags_last:12;
+	uint64_t    tso_l4_offset:8;
+	uint64_t    tso_sdc_first:8;
+	uint64_t    tso_sdc_cont:8;
+#endif
+};
+
+#endif /* Q_STRUCT_H */
diff --git a/drivers/net/ethernet/cavium/thunder/thunder_bgx.c b/drivers/net/ethernet/cavium/thunder/thunder_bgx.c
new file mode 100644
index 0000000..a090d50
--- /dev/null
+++ b/drivers/net/ethernet/cavium/thunder/thunder_bgx.c
@@ -0,0 +1,248 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2014 Cavium, Inc.
+ */
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/string.h>
+#include <linux/errno.h>
+#include <linux/types.h>
+#include <linux/init.h>
+#include <linux/interrupt.h>
+#include <linux/workqueue.h>
+#include <linux/pci.h>
+
+#include "nic_reg.h"
+#include "nic.h"
+#include "thunder_bgx.h"
+
+#define DRV_NAME	"thunder-BGX"
+#define DRV_VERSION	"1.0"
+
+struct lmac {
+	int dmac;
+} lmac;
+
+struct bgx {
+	uint8_t	bgx_id;
+	struct lmac lmac[MAX_LMAC_PER_BGX];
+	uint64_t reg_base;
+	struct pci_dev *pdev;
+} bgx;
+
+struct bgx *bgx_vnic[MAX_BGX_PER_CN88XX];
+
+/* Supported devices */
+static DEFINE_PCI_DEVICE_TABLE(bgx_id_table) = {
+	{ PCI_DEVICE(PCI_VENDOR_ID_CAVIUM, PCI_DEVICE_ID_THUNDER_BGX) },
+	{ 0, }  /* end of table */
+};
+
+MODULE_AUTHOR("Cavium Inc");
+MODULE_DESCRIPTION("Cavium Thunder BGX/MAC Driver");
+MODULE_LICENSE("GPL");
+MODULE_VERSION(DRV_VERSION);
+MODULE_DEVICE_TABLE(pci, bgx_id_table);
+
+/* Register read/write APIs */
+#if 0
+static uint64_t bgx_reg_read(struct bgx *bgx, uint8_t lmac, uint64_t offset)
+{
+	uint64_t addr = bgx->reg_base + (lmac << 20) + offset;
+
+	return readq_relaxed((void *)addr);
+}
+#endif
+static void bgx_reg_write(struct bgx *bgx, uint8_t lmac,
+			uint64_t offset, uint64_t val)
+{
+	uint64_t addr = bgx->reg_base + (lmac << 20) + offset;
+
+	writeq_relaxed(val, (void *)addr);
+}
+
+static void bgx_flush_dmac_addrs(struct bgx *bgx, uint64_t lmac)
+{
+	uint64_t dmac = 0x00;
+	uint64_t offset, addr;
+
+	while (bgx->lmac[lmac].dmac > 0) {
+		offset = ((bgx->lmac[lmac].dmac - 1) * sizeof(dmac)) +
+					(lmac * MAX_DMAC_PER_LMAC * sizeof(dmac));
+		addr = bgx->reg_base + BGX_CMR_RX_DMACX_CAM + offset;
+		writeq_relaxed(dmac, (void *)addr);
+		bgx->lmac[lmac].dmac--;
+	}
+}
+void bgx_add_dmac_addr(uint64_t dmac, uint64_t lmac)
+{
+	int bgx_index;
+	uint64_t offset, addr;
+	struct bgx *bgx;
+
+	bgx_index = lmac / MAX_LMAC_PER_BGX;
+	bgx = bgx_vnic[bgx_index];
+	if (!bgx) {
+		pr_err("BGX%d not yet initialized, ignoring DMAC addition\n",
+								 bgx_index);
+		return;
+	}
+	lmac = lmac % MAX_LMAC_PER_BGX;
+	dmac = dmac | (1ULL << 48) | (lmac << 49); /* Enable DMAC */
+	if (bgx->lmac[lmac].dmac == MAX_DMAC_PER_LMAC) {
+		pr_err("Max DMAC filters for LMAC%lld reached, ignoring DMAC addition\n", lmac);
+		return;
+	}
+	/* Simulator supports only TNS by pass mode */
+	if (bgx->lmac[lmac].dmac == MAX_DMAC_PER_LMAC_TNS_BYPASS_MODE)
+		bgx->lmac[lmac].dmac = 1;
+
+	offset = (bgx->lmac[lmac].dmac * sizeof(dmac)) +
+					(lmac * MAX_DMAC_PER_LMAC * sizeof(dmac));
+	addr = bgx->reg_base + BGX_CMR_RX_DMACX_CAM + offset;
+	writeq_relaxed(dmac, (void *)addr);
+	bgx->lmac[lmac].dmac++;
+}
+
+void bgx_lmac_enable(uint64_t lmac)
+{
+	int bgx_index;
+	struct bgx *bgx;
+
+	bgx_index = lmac / MAX_LMAC_PER_BGX;
+	bgx = bgx_vnic[bgx_index];
+	if (!bgx) {
+		pr_err("BGX%d not yet initialized, ignoring LMAC disable\n",
+								 bgx_index);
+		return;
+	}
+	lmac = lmac % MAX_LMAC_PER_BGX;
+	bgx_reg_write(bgx, lmac, BGX_CMRX_CFG,
+			(1 << 15) | (1 << 14) | (1 << 13));
+}
+
+void bgx_lmac_disable(uint64_t lmac)
+{
+	int bgx_index;
+	struct bgx *bgx;
+
+	bgx_index = lmac / MAX_LMAC_PER_BGX;
+	bgx = bgx_vnic[bgx_index];
+	if (!bgx) {
+		pr_err("BGX%d not yet initialized, ignoring LMAC disable\n",
+								 bgx_index);
+		return;
+	}
+	lmac = lmac % MAX_LMAC_PER_BGX;
+	bgx_reg_write(bgx, lmac, BGX_CMRX_CFG, 0x00);
+}
+
+static void bgx_init_hw(struct bgx *bgx)
+{
+	int lmac;
+	uint64_t enable = 0;
+	uint64_t dmac_bcast = (1ULL << 48) - 1;
+
+	/* Enable all LMACs */
+	/* Enable LMAC, Pkt Rx enable, Pkt Tx enable */
+	enable = (1 << 15) | (1 << 14) | (1 << 13);
+	for (lmac = 0; lmac < MAX_LMAC_PER_BGX; lmac++)
+		bgx_reg_write(bgx, lmac, BGX_CMRX_CFG, enable);
+
+	/* Add broadcast MAC into all LMAC's DMAC filters */
+	for (lmac = 0; lmac < MAX_LMAC_PER_BGX; lmac++)
+		bgx_add_dmac_addr(dmac_bcast,
+				  lmac + bgx->bgx_id * MAX_LMAC_PER_BGX);
+}
+
+static int bgx_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
+{
+	struct device *dev = &pdev->dev;
+	struct bgx *bgx;
+	int    err;
+
+	bgx = kzalloc(sizeof(*bgx), GFP_KERNEL);
+	bgx->pdev = pdev;
+
+	pci_set_drvdata(pdev, bgx);
+
+	err = pci_enable_device(pdev);
+	if (err) {
+		dev_err(dev, "Failed to enable PCI device\n");
+		goto exit;
+	}
+
+	err = pci_request_regions(pdev, DRV_NAME);
+	if (err) {
+		dev_err(dev, "PCI request regions failed 0x%x\n", err);
+		goto err_disable_device;
+	}
+
+	/* MAP configuration registers */
+	bgx->reg_base = (uint64_t) pci_ioremap_bar(pdev, PCI_CFG_REG_BAR_NUM);
+	if (!bgx->reg_base) {
+		dev_err(dev, "BGX: Cannot map CSR memory space, aborting\n");
+		err = -ENOMEM;
+		goto err_release_regions;
+	}
+	bgx->bgx_id = (pci_resource_start(pdev, PCI_CFG_REG_BAR_NUM) >> 24) & 1;
+	bgx_vnic[bgx->bgx_id] = bgx;
+
+	/* Initialize BGX hardware */
+	bgx_init_hw(bgx);
+
+	goto exit;
+
+	if (bgx->reg_base)
+		iounmap((void *)bgx->reg_base);
+err_release_regions:
+	pci_release_regions(pdev);
+err_disable_device:
+	pci_disable_device(pdev);
+exit:
+	return err;
+}
+
+static void bgx_remove(struct pci_dev *pdev)
+{
+	struct bgx *bgx = pci_get_drvdata(pdev);
+
+	if (!bgx)
+		return;
+
+	pci_set_drvdata(pdev, NULL);
+
+	if (bgx->reg_base)
+		iounmap((void *)bgx->reg_base);
+
+	pci_release_regions(pdev);
+	pci_disable_device(pdev);
+	kfree(bgx);
+}
+
+static struct pci_driver bgx_driver = {
+	.name = DRV_NAME,
+	.id_table = bgx_id_table,
+	.probe = bgx_probe,
+	.remove = bgx_remove,
+};
+
+static int __init bgx_init_module(void)
+{
+	pr_info("%s, ver %s\n", DRV_NAME, DRV_VERSION);
+
+	return pci_register_driver(&bgx_driver);
+}
+
+static void __exit bgx_cleanup_module(void)
+{
+	pci_unregister_driver(&bgx_driver);
+}
+
+module_init(bgx_init_module);
+module_exit(bgx_cleanup_module);
+
diff --git a/drivers/net/ethernet/cavium/thunder/thunder_bgx.h b/drivers/net/ethernet/cavium/thunder/thunder_bgx.h
new file mode 100644
index 0000000..4ba925e
--- /dev/null
+++ b/drivers/net/ethernet/cavium/thunder/thunder_bgx.h
@@ -0,0 +1,27 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2014 Cavium, Inc.
+ */
+
+#ifndef THUNDER_BGX_H
+#define THUNDER_BGX_H
+
+#define    MAX_BGX_PER_CN88XX			2
+#define    MAX_LMAC_PER_BGX			4
+#define    MAX_BGX_CHANS_PER_LMAC		16
+#define    MAX_DMAC_PER_LMAC			8
+
+#define    MAX_DMAC_PER_LMAC_TNS_BYPASS_MODE	2
+
+/* Registers */
+#define BGX_CMRX_CFG				0x00
+#define BGX_CMR_RX_DMACX_CAM			0x200
+
+void bgx_add_dmac_addr(uint64_t dmac, uint64_t lmac);
+void bgx_lmac_disable(uint64_t lmac);
+void bgx_lmac_enable(uint64_t lmac);
+
+#endif /* THUNDER_BGX_H */
-- 
1.7.5.4

