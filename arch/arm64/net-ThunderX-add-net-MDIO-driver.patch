From e55b5a15dc9ee7101a975688d42db24a2826a135 Mon Sep 17 00:00:00 2001
From: Jiang Lu <lu.jiang@windriver.com>
Date: Wed, 24 Jun 2015 09:08:30 +0000
Subject: [PATCH 21/23] net:ThunderX:add net & MDIO driver

Add drivers for ethernet controller & MDIO on ThunderX SOC.
Extracted from ThunderX-SDK pre-release 0.4.

Signed-off-by: Jiang Lu <lu.jiang@windriver.com>
---
 drivers/net/ethernet/cavium/thunder/Makefile       |    2 +-
 drivers/net/ethernet/cavium/thunder/nic.h          |  523 ++++++----
 drivers/net/ethernet/cavium/thunder/nic_main.c     |  923 +++++++++++-----
 drivers/net/ethernet/cavium/thunder/nic_reg.h      |  138 ++-
 .../net/ethernet/cavium/thunder/nicvf_ethtool.c    |  641 +++++++++--
 drivers/net/ethernet/cavium/thunder/nicvf_main.c   | 1182 ++++++++++++--------
 drivers/net/ethernet/cavium/thunder/nicvf_queues.c | 1213 +++++++++++++-------
 drivers/net/ethernet/cavium/thunder/nicvf_queues.h |  436 +++++--
 drivers/net/ethernet/cavium/thunder/q_struct.h     |  732 +++++++-----
 drivers/net/ethernet/cavium/thunder/thunder_bgx.c  | 1004 +++++++++++++++--
 drivers/net/ethernet/cavium/thunder/thunder_bgx.h  |  216 ++++-
 drivers/net/phy/Kconfig                            |    2 +-
 drivers/net/phy/cvmx-smix-defs.h                   |  254 ++++
 drivers/net/phy/mdio-octeon.c                      |   33 +-
 14 files changed, 5286 insertions(+), 2013 deletions(-)
 create mode 100644 drivers/net/phy/cvmx-smix-defs.h

diff --git a/drivers/net/ethernet/cavium/thunder/Makefile b/drivers/net/ethernet/cavium/thunder/Makefile
index 3998567..3588162 100644
--- a/drivers/net/ethernet/cavium/thunder/Makefile
+++ b/drivers/net/ethernet/cavium/thunder/Makefile
@@ -2,9 +2,9 @@
 # Makefile for Cavium's Thunder ethernet device
 #
 
+obj-$(CONFIG_THUNDER_NIC_BGX) += thunder_bgx.o
 obj-$(CONFIG_THUNDER_NIC_PF) += nicpf.o
 obj-$(CONFIG_THUNDER_NIC_VF) += nicvf.o
-obj-$(CONFIG_THUNDER_NIC_BGX) += thunder_bgx.o
 
 nicpf-y := nic_main.o
 nicvf-y := nicvf_main.o nicvf_queues.o
diff --git a/drivers/net/ethernet/cavium/thunder/nic.h b/drivers/net/ethernet/cavium/thunder/nic.h
index 7417bcb..236663c 100644
--- a/drivers/net/ethernet/cavium/thunder/nic.h
+++ b/drivers/net/ethernet/cavium/thunder/nic.h
@@ -1,9 +1,10 @@
 /*
- * This file is subject to the terms and conditions of the GNU General Public
- * License.  See the file "COPYING" in the main directory of this archive
- * for more details.
+ * Copyright (C) 2015 Cavium, Inc.
  *
- * Copyright (C) 2014 Cavium, Inc.
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of
+ * the License, or (at your option) any later version.
  */
 
 #ifndef NIC_H
@@ -11,11 +12,13 @@
 
 #include <linux/netdevice.h>
 #include <linux/interrupt.h>
+#include "thunder_bgx.h"
 
 /* PCI device IDs */
-#define	PCI_DEVICE_ID_THUNDER_NIC_PF	0xA01E
-#define	PCI_DEVICE_ID_THUNDER_NIC_VF	0x0011
-#define	PCI_DEVICE_ID_THUNDER_BGX	0xA026
+#define	PCI_DEVICE_ID_THUNDER_NIC_PF		0xA01E
+#define	PCI_DEVICE_ID_THUNDER_PASS1_NIC_VF	0x0011
+#define	PCI_DEVICE_ID_THUNDER_NIC_VF		0xA034
+#define	PCI_DEVICE_ID_THUNDER_BGX		0xA026
 
 /* PCI BAR nos */
 #define	PCI_CFG_REG_BAR_NUM		0
@@ -25,54 +28,43 @@
 #define	MAX_NUM_VFS_SUPPORTED		128
 #define	DEFAULT_NUM_VF_ENABLED		8
 
+#define	NIC_TNS_BYPASS_MODE		0
+#define	NIC_TNS_MODE			1
+
 /* NIC priv flags */
 #define	NIC_SRIOV_ENABLED		(1 << 0)
+#define	NIC_TNS_ENABLED			(1 << 1)
 
 /* VNIC HW optimiation features */
-#undef	VNIC_RX_CSUM_OFFLOAD_SUPPORT
-#undef	VNIC_TX_CSUM_OFFLOAD_SUPPORT
-#define	VNIC_SG_SUPPORT
-#define	VNIC_TSO_SUPPORT
-#define	VNIC_LRO_SUPPORT
-
-/* TSO not supported in Thunder pass1 */
-#ifdef	VNIC_TSO_SUPPORT
-#define	VNIC_SW_TSO_SUPPORT
-#undef	VNIC_HW_TSO_SUPPORT
-#endif
-
-/* LRO not supported even in Thunder pass2 */
-#ifdef	VNIC_LRO_SUPPORT
-#define	VNIC_SW_LRO_SUPPORT
-#undef	VNIC_HW_LRO_SUPPORT
-#endif
-
-
-/* ETHTOOL enable or disable, undef this to disable */
-#define	NICVF_ETHTOOL_ENABLE
-
-/* NAPI enable or disable, undef this to disable */
-#define	NICVF_NAPI_ENABLE
+#define VNIC_RSS_SUPPORT
 
 /* Min/Max packet size */
 #define	NIC_HW_MIN_FRS			64
-#define	NIC_HW_MAX_FRS			1500
+#define	NIC_HW_MAX_FRS			9200 /* 9216 max packet including FCS */
 
 /* Max pkinds */
 #define	NIC_MAX_PKIND			16
 
 /* Rx Channels */
-#define	NIC_MAX_BGX			2
-#define	NIC_CHANS_PER_BGX_INF		128
-#define	NIC_MAX_CHANS			(NIC_MAX_BGX * NIC_CHANS_PER_BGX_INF)
-#define	NIC_MAX_CPI			2048 /* Channel parse index */
-#define	NIC_MAX_RSSI			4096 /* Receive side scaling index */
-
-/* TNS bi-pass mode: 1-1 mapping between VNIC and LMAC */
-#define	NIC_CPI_PER_BGX			(NIC_MAX_CPI / NIC_MAX_BGX)
-#define	NIC_CPI_PER_LMAC		(NIC_MAX_CPI / NIC_MAX_CHANS)
-#define	NIC_RSSI_PER_BGX		(NIC_MAX_RSSI / NIC_MAX_BGX)
-#define	NIC_RSSI_PER_LMAC		(NIC_MAX_RSSI / NIC_MAX_CHANS)
+/* Receive channel configuration in TNS bypass mode
+ * Below is configuration in TNS bypass mode
+ * BGX0-LMAC0-CHAN0 - VNIC CHAN0
+ * BGX0-LMAC1-CHAN0 - VNIC CHAN16
+ * ...
+ * BGX1-LMAC0-CHAN0 - VNIC CHAN128
+ * ...
+ * BGX1-LMAC3-CHAN0 - VNIC CHAN174
+ */
+#define	NIC_INTF_COUNT			2  /* Interfaces btw VNIC and TNS/BGX */
+#define	NIC_CHANS_PER_INF		128
+#define	NIC_MAX_CHANS			(NIC_INTF_COUNT * NIC_CHANS_PER_INF)
+#define	NIC_CPI_COUNT			2048 /* No of channel parse indices */
+
+/* TNS bypass mode: 1-1 mapping between VNIC and BGX:LMAC */
+#define NIC_MAX_BGX			MAX_BGX_PER_CN88XX
+#define	NIC_CPI_PER_BGX			(NIC_CPI_COUNT / NIC_MAX_BGX)
+#define	NIC_MAX_CPI_PER_LMAC		64 /* Max when CPI_ALG is IP diffserv */
+#define	NIC_RSSI_PER_BGX		(NIC_RSSI_COUNT / NIC_MAX_BGX)
 
 /* Tx scheduling */
 #define	NIC_MAX_TL4			1024
@@ -83,9 +75,10 @@
 #define	NIC_MAX_TL2_SHAPERS		2  /* 1 shaper for 32 TL2s */
 #define	NIC_MAX_TL1			2
 
-/* TNS bi-pass mode */
+/* TNS bypass mode */
+#define	NIC_TL2_PER_BGX			32
 #define	NIC_TL4_PER_BGX			(NIC_MAX_TL4 / NIC_MAX_BGX)
-#define	NIC_TL4_PER_LMAC		(NIC_MAX_TL4 / NIC_CHANS_PER_BGX_INF)
+#define	NIC_TL4_PER_LMAC		(NIC_MAX_TL4 / NIC_CHANS_PER_INF)
 
 /* NIC VF Interrupts */
 #define	NICVF_INTR_CQ			0
@@ -116,60 +109,107 @@
 #define	NIC_PF_MSIX_VECTORS		10
 #define	NIC_VF_MSIX_VECTORS		20
 
-#define	NICVF_CQ_INTR_ID		0
-#define	NICVF_SQ_INTR_ID		8
-#define	NICVF_RBDR_INTR_ID		16
-#define	NICVF_MISC_INTR_ID		18
-#define	NICVF_QS_ERR_INTR_ID		19
-
-#define for_each_cq_irq(irq) for (irq = NICVF_CQ_INTR_ID; \
-					irq < NICVF_SQ_INTR_ID; irq++)
-#define for_each_sq_irq(irq) for (irq = NICVF_SQ_INTR_ID; \
-					irq < NICVF_RBDR_INTR_ID; irq++)
-#define for_each_rbdr_irq(irq) for (irq = NICVF_RBDR_INTR_ID; \
-					irq < NICVF_MISC_INTR_ID; irq++)
+#define NIC_PF_INTR_ID_ECC0_SBE		0
+#define NIC_PF_INTR_ID_ECC0_DBE		1
+#define NIC_PF_INTR_ID_ECC1_SBE		2
+#define NIC_PF_INTR_ID_ECC1_DBE		3
+#define NIC_PF_INTR_ID_ECC2_SBE		4
+#define NIC_PF_INTR_ID_ECC2_DBE		5
+#define NIC_PF_INTR_ID_ECC3_SBE		6
+#define NIC_PF_INTR_ID_ECC3_DBE		7
+#define NIC_PF_INTR_ID_MBOX0		8
+#define NIC_PF_INTR_ID_MBOX1		9
+
+/* Global timer for CQ timer thresh interrupts
+ * Calculated for SCLK of 700Mhz
+ * value written should be a 1/16th of what is expected
+ *
+ * 1 tick per usec
+ */
+#define NICPF_CLK_PER_INT_TICK		44
 
 struct nicvf_cq_poll {
-	uint8_t	cq_idx;		/* Completion queue index */
-	struct napi_struct napi;
+	u8	cq_idx;		/* Completion queue index */
+	struct	napi_struct napi;
 };
 
-#ifdef NICVF_ETHTOOL_ENABLE
+#define	NIC_RSSI_COUNT			4096 /* Total no of RSS indices */
+#define NIC_MAX_RSS_HASH_BITS		8
+#define NIC_MAX_RSS_IDR_TBL_SIZE	(1 << NIC_MAX_RSS_HASH_BITS)
+#define RSS_HASH_KEY_SIZE		5 /* 320 bit key */
+
+#ifdef VNIC_RSS_SUPPORT
+struct nicvf_rss_info {
+	bool enable;
+#define	RSS_L2_EXTENDED_HASH_ENA	(1 << 0)
+#define	RSS_IP_HASH_ENA			(1 << 1)
+#define	RSS_TCP_HASH_ENA		(1 << 2)
+#define	RSS_TCP_SYN_DIS			(1 << 3)
+#define	RSS_UDP_HASH_ENA		(1 << 4)
+#define RSS_L4_EXTENDED_HASH_ENA	(1 << 5)
+#define	RSS_ROCE_ENA			(1 << 6)
+#define	RSS_L3_BI_DIRECTION_ENA		(1 << 7)
+#define	RSS_L4_BI_DIRECTION_ENA		(1 << 8)
+	u64 cfg;
+	u8  hash_bits;
+	u16 rss_size;
+	u8  ind_tbl[NIC_MAX_RSS_IDR_TBL_SIZE];
+	u64 key[RSS_HASH_KEY_SIZE];
+} ____cacheline_aligned_in_smp;
+#endif
 
-/* Stats */
+enum rx_stats_reg_offset {
+	RX_OCTS = 0x0,
+	RX_UCAST = 0x1,
+	RX_BCAST = 0x2,
+	RX_MCAST = 0x3,
+	RX_RED = 0x4,
+	RX_RED_OCTS = 0x5,
+	RX_ORUN = 0x6,
+	RX_ORUN_OCTS = 0x7,
+	RX_FCS = 0x8,
+	RX_L2ERR = 0x9,
+	RX_DRP_BCAST = 0xa,
+	RX_DRP_MCAST = 0xb,
+	RX_DRP_L3BCAST = 0xc,
+	RX_DRP_L3MCAST = 0xd,
+	RX_STATS_ENUM_LAST,
+};
 
-/* Tx statistics */
-struct nicvf_tx_stats {
-	u64 tx_frames_ok;
-	u64 tx_unicast_frames_ok;
-	u64 tx_multicast_frames_ok;
-	u64 tx_broadcast_frames_ok;
+enum tx_stats_reg_offset {
+	TX_OCTS = 0x0,
+	TX_UCAST = 0x1,
+	TX_BCAST = 0x2,
+	TX_MCAST = 0x3,
+	TX_DROP = 0x4,
+	TX_STATS_ENUM_LAST,
+};
+
+struct nicvf_hw_stats {
+	u64 rx_bytes_ok;
+	u64 rx_ucast_frames_ok;
+	u64 rx_bcast_frames_ok;
+	u64 rx_mcast_frames_ok;
+	u64 rx_fcs_errors;
+	u64 rx_l2_errors;
+	u64 rx_drop_red;
+	u64 rx_drop_red_bytes;
+	u64 rx_drop_overrun;
+	u64 rx_drop_overrun_bytes;
+	u64 rx_drop_bcast;
+	u64 rx_drop_mcast;
+	u64 rx_drop_l3_bcast;
+	u64 rx_drop_l3_mcast;
 	u64 tx_bytes_ok;
-	u64 tx_unicast_bytes_ok;
-	u64 tx_multicast_bytes_ok;
-	u64 tx_broadcast_bytes_ok;
+	u64 tx_ucast_frames_ok;
+	u64 tx_bcast_frames_ok;
+	u64 tx_mcast_frames_ok;
 	u64 tx_drops;
-	u64 tx_errors;
-	u64 tx_tso;
-	u64 rsvd[16];
 };
 
-/* Rx statistics */
-struct nicvf_rx_stats {
+struct nicvf_drv_stats {
+	/* Rx */
 	u64 rx_frames_ok;
-	u64 rx_frames_total;
-	u64 rx_unicast_frames_ok;
-	u64 rx_multicast_frames_ok;
-	u64 rx_broadcast_frames_ok;
-	u64 rx_bytes_ok;
-	u64 rx_unicast_bytes_ok;
-	u64 rx_multicast_bytes_ok;
-	u64 rx_broadcast_bytes_ok;
-	u64 rx_drop;
-	u64 rx_no_bufs;
-	u64 rx_errors;
-	u64 rx_rss;
-	u64 rx_crc_errors;
 	u64 rx_frames_64;
 	u64 rx_frames_127;
 	u64 rx_frames_255;
@@ -177,150 +217,239 @@ struct nicvf_rx_stats {
 	u64 rx_frames_1023;
 	u64 rx_frames_1518;
 	u64 rx_frames_jumbo;
-	u64 rsvd[16];
-};
-
-struct eth_stats {
-	struct nicvf_tx_stats tx;
-	struct nicvf_rx_stats rx;
+	u64 rx_drops;
+	/* Tx */
+	u64 tx_frames_ok;
+	u64 tx_drops;
+	u64 tx_busy;
+	u64 tx_tso;
 };
 
-#endif /* NICVF_ETHTOOL_ENABLE */
-
 struct nicvf {
 	struct net_device	*netdev;
 	struct pci_dev		*pdev;
-	uint16_t		mtu;
-	uint8_t			vnic_id;
-	struct queue_set	*qs;		/* Queue set this VNIC is pointing to */
-	uint8_t			num_qs;		/* No of QSs assigned to this VNIC */
-	void			*addnl_qs;	/* Pointer to QSs additional to default 1 QS */
-	uint16_t		vf_mtu;
-	uint64_t		reg_base;	/* Register start address */
-	struct tasklet_struct	rbdr_task;	/* Tasklet to refill RBDR */
-	struct tasklet_struct	qs_err_task;	/* Tasklet to handle Qset err */
-#ifdef NICVF_NAPI_ENABLE
-	struct nicvf_cq_poll	*napi[8];	/* NAPI */
+	u8			vf_id;
+	u8			node;
+	u8			tns_mode;
+	u16			mtu;
+	struct queue_set	*qs;
+	u64			reg_base;
+	bool			link_up;
+	u8			duplex;
+	u32			speed;
+	struct tasklet_struct	rbdr_task;
+	struct tasklet_struct	qs_err_task;
+	struct tasklet_struct	cq_task;
+	struct nicvf_cq_poll	*napi[8];
+#ifdef VNIC_RSS_SUPPORT
+	struct nicvf_rss_info	rss_info;
 #endif
+	u8			cpi_alg;
+	/* Interrupt coalescing settings */
+	u32			cq_coalesce_usecs;
+
+	u32			msg_enable;
+	struct nicvf_hw_stats   stats;
+	struct nicvf_drv_stats  drv_stats;
+	struct bgx_stats	bgx_stats;
+	struct work_struct	reset_task;
+
 	/* MSI-X  */
 	bool			msix_enabled;
-	uint16_t		num_vec;
+	u8			num_vec;
 	struct msix_entry	msix_entries[NIC_VF_MSIX_VECTORS];
 	char			irq_name[NIC_VF_MSIX_VECTORS][20];
-	uint8_t			irq_allocated[NIC_VF_MSIX_VECTORS];
-#ifdef NICVF_ETHTOOL_ENABLE
-	struct eth_stats	vstats;
-#endif
-};
+	bool			irq_allocated[NIC_VF_MSIX_VECTORS];
+} ____cacheline_aligned_in_smp;
 
 struct nicpf {
 	struct net_device	*netdev;
 	struct pci_dev		*pdev;
+	u8			rev_id;
+#define NIC_NODE_ID_MASK	0x300000000000
+#define NIC_NODE_ID(x)		((x & NODE_ID_MASK) >> 44)
+	u8			node;
 	unsigned int		flags;
-	uint16_t		total_vf_cnt;   /* Total num of VF supported */
-	uint16_t		num_vf_en;      /* No of VF enabled */
-	uint64_t		reg_base;       /* Register start address */
+	u16			total_vf_cnt;   /* Total num of VF supported */
+	u8			num_vf_en;      /* No of VF enabled */
+	bool			vf_enabled[MAX_NUM_VFS_SUPPORTED];
+	u64			reg_base;       /* Register start address */
 	struct pkind_cfg	pkind;
+	u8			bgx_cnt;
+#define	NIC_SET_VF_LMAC_MAP(bgx, lmac)	(((bgx & 0xF) << 4) | (lmac & 0xF))
+#define	NIC_GET_BGX_FROM_VF_LMAC_MAP(map)	((map >> 4) & 0xF)
+#define	NIC_GET_LMAC_FROM_VF_LMAC_MAP(map)	(map & 0xF)
+	u8			vf_lmac_map[MAX_LMAC];
+	struct delayed_work     dwork;
+	struct workqueue_struct *check_link;
+	u8			link[MAX_LMAC];
+	u8			duplex[MAX_LMAC];
+	u32			speed[MAX_LMAC];
+	u16			cpi_base[MAX_NUM_VFS_SUPPORTED];
+	u16			rss_ind_tbl_size;
+	u64			mac[MAX_NUM_VFS_SUPPORTED];
+	bool			mbx_lock[MAX_NUM_VFS_SUPPORTED];
+
 	/* MSI-X */
 	bool			msix_enabled;
-	uint16_t		num_vec;
+	u8			num_vec;
 	struct msix_entry	msix_entries[NIC_PF_MSIX_VECTORS];
-	uint8_t			irq_allocated[NIC_PF_MSIX_VECTORS];
-};
+	bool			irq_allocated[NIC_PF_MSIX_VECTORS];
+} ____cacheline_aligned_in_smp;
 
-struct nicvf_stats {
-	struct {
-		uint32_t partial_pkts;
-		uint32_t jabber_errs;
-		uint32_t fcs_errs;
-		uint32_t terminate_errs;
-		uint32_t bgx_rx_errs;
-		uint32_t prel2_errs;
-		uint32_t l2_frags;
-		uint32_t l2_overruns;
-		uint32_t l2_pfcs;
-		uint32_t l2_puny;
-		uint32_t l2_mal;
-		uint32_t l2_oversize;
-		uint32_t l2_len_mismatch;
-		uint32_t l2_pclp;
-		uint32_t not_ip;
-		uint32_t ip_csum_err;
-		uint32_t ip_mal;
-		uint32_t ip_mal_payload;
-		uint32_t ip_hop;
-		uint32_t l3_icrc;
-		uint32_t l3_pclp;
-		uint32_t l4_mal;
-		uint32_t l4_csum_err;
-		uint32_t udp_len_err;
-		uint32_t bad_l4_port;
-		uint32_t bad_tcp_flag;
-		uint32_t tcp_offset_err;
-		uint32_t l4_pclp;
-		uint32_t no_rbdr;
-	} rx;
-	struct {
-	} tx;
-};
-
-/* PF <--> Mailbox communication
+/* PF <--> VF Mailbox communication
  * Eight 64bit registers are shared between PF and VF.
  * Separate set for each VF.
  * Writing '1' into last register mbx7 means end of message.
  */
 
 /* PF <--> VF mailbox communication */
-#define	NIC_PF_VF_MAILBOX_SIZE		8
+#define	NIC_PF_VF_MAILBOX_SIZE		2
+#define	NIC_MBOX_MSG_TIMEOUT		2000 /* ms */
 
 /* Mailbox message types */
-#define	NIC_PF_VF_MSG_READY		0x01	/* Is PF ready to rcv msgs */
-#define	NIC_PF_VF_MSG_ACK		0x02	/* ACK the message received */
-#define	NIC_PF_VF_MSG_NACK		0x03	/* NACK the message received */
-#define	NIC_PF_VF_MSG_QS_CFG		0x04	/* Configure Qset */
-#define	NIC_PF_VF_MSG_RQ_CFG		0x05	/* Configure receive queue */
-#define	NIC_PF_VF_MSG_SQ_CFG		0x06	/* Configure Send queue */
-#define	NIC_PF_VF_MSG_RQ_DROP_CFG	0x07	/* Configure receive queue */
-#define	NIC_PF_VF_MSG_SET_MAC		0x08	/* Add MAC ID to BGX's DMAC filter */
-#define	NIC_VF_SET_MAX_FRS		0x09	/* Set max frame size */
+#define	NIC_MBOX_MSG_READY		0x01	/* Is PF ready to rcv msgs */
+#define	NIC_MBOX_MSG_ACK		0x02	/* ACK the message received */
+#define	NIC_MBOX_MSG_NACK		0x03	/* NACK the message received */
+#define	NIC_MBOX_MSG_QS_CFG		0x04	/* Configure Qset */
+#define	NIC_MBOX_MSG_RQ_CFG		0x05	/* Configure receive queue */
+#define	NIC_MBOX_MSG_SQ_CFG		0x06	/* Configure Send queue */
+#define	NIC_MBOX_MSG_RQ_DROP_CFG	0x07	/* Configure receive queue */
+#define	NIC_MBOX_MSG_SET_MAC		0x08	/* Add MAC ID to DMAC filter */
+#define	NIC_MBOX_MSG_SET_MAX_FRS	0x09	/* Set max frame size */
+#define	NIC_MBOX_MSG_CPI_CFG		0x0A	/* Config CPI, RSSI */
+#define	NIC_MBOX_MSG_RSS_SIZE		0x0B	/* Get RSS indir_tbl size */
+#define	NIC_MBOX_MSG_RSS_CFG		0x0C	/* Config RSS table */
+#define	NIC_MBOX_MSG_RSS_CFG_CONT	0x0D	/* RSS config continuation */
+#define	NIC_MBOX_MSG_RQ_BP_CFG		0x0E	/* RQ backpressure config */
+#define	NIC_MBOX_MSG_RQ_SW_SYNC		0x0F	/* Flush inflight pkts to RQ */
+#define	NIC_MBOX_MSG_BGX_STATS		0x10	/* Get stats from BGX */
+#define	NIC_MBOX_MSG_BGX_LINK_CHANGE	0x11	/* BGX:LMAC link status */
+
+struct nic_cfg_msg {
+	u8    vf_id;
+	u8    tns_mode;
+	u8    node_id;
+	u8    unused0;
+	u16   unused1;
+	u64   mac_addr;
+} __packed;
+
+/* Qset configuration */
+struct qs_cfg_msg {
+	u8    num;
+	u8    unused0;
+	u32   unused1;
+	u64   cfg;
+} __packed;
+
+/* Receive queue configuration */
+struct rq_cfg_msg {
+	u8    qs_num;
+	u8    rq_num;
+	u32   unused0;
+	u64   cfg;
+} __packed;
+
+/* Send queue configuration */
+struct sq_cfg_msg {
+	u8    qs_num;
+	u8    sq_num;
+	u32   unused0;
+	u64   cfg;
+} __packed;
+
+/* Set VF's MAC address */
+struct set_mac_msg {
+	u8    vf_id;
+	u8    unused0;
+	u32   unused1;
+	u64   addr;
+} __packed;
+
+/* Set Maximum frame size */
+struct set_frs_msg {
+	u8    vf_id;
+	u16   max_frs;
+};
 
-struct nic_mbx {
-	uint64_t	   msg;
-	union	{
-		uint64_t	vnic_id;
-		struct {			/* Qset configuration */
-			uint64_t   num;
-			uint64_t   cfg;
-		} qs;
-		struct {			/* Receive queue configuration */
-			uint64_t   qs_num;
-			uint64_t   rq_num;
-			uint64_t   cfg;
-		} rq;
-		struct {			/* Send queue configuration */
-			uint64_t   qs_num;
-			uint64_t   sq_num;
-			uint64_t   cfg;
-		} sq;
-		struct {			/* VF's MAC address */
-			uint64_t   vnic_id;
-			uint64_t   addr;
-		} mac;
-		uint64_t	max_frs; /* Max frame size */
-	} data;
-	uint64_t	   mbx4;
-	uint64_t	   mbx5;
-	uint64_t	   mbx6;
-	uint64_t	   mbx_trigger_intr;
+/* Set CPI algorithm type */
+struct cpi_cfg_msg {
+	u8    vf_id;
+	u8    rq_cnt;
+	u8    cpi_alg;
 };
 
-#ifdef NICVF_ETHTOOL_ENABLE
-void nicvf_set_ethtool_ops(struct net_device *netdev);
+#ifdef VNIC_RSS_SUPPORT
+/* Get RSS table size */
+struct rss_sz_msg {
+	u8    vf_id;
+	u16   ind_tbl_size;
+};
+
+/* Set RSS configuration */
+struct rss_cfg_msg {
+	u8    vf_id;
+	u8    hash_bits;
+	u8    tbl_len;
+	u8    tbl_offset;
+	u16   unused0;
+#define RSS_IND_TBL_LEN_PER_MBX_MSG	8
+	u8    ind_tbl[RSS_IND_TBL_LEN_PER_MBX_MSG];
+} __packed;
+#endif
+
+struct bgx_stats_msg {
+	u8    vf_id;
+	u8    rx;
+	u8    idx;
+	u8    unused0;
+	u16   unused1;
+	u64   stats;
+} __packed;
+
+/* Physical interface link status */
+struct bgx_link_status {
+	u8    link_up;
+	u8    duplex;
+	u32   speed;
+};
+
+/* 128 bit shared memory between PF and each VF */
+struct nic_mbx {
+	u8    msg;
+	u8    unused;
+	union	{
+		struct nic_cfg_msg	nic_cfg;
+		struct qs_cfg_msg	qs;
+		struct rq_cfg_msg	rq;
+		struct sq_cfg_msg	sq;
+		struct set_mac_msg	mac;
+		struct set_frs_msg	frs;
+		struct cpi_cfg_msg	cpi_cfg;
+#ifdef VNIC_RSS_SUPPORT
+		struct rss_sz_msg	rss_size;
+		struct rss_cfg_msg	rss_cfg;
 #endif
+		struct bgx_stats_msg    bgx_stats;
+		struct bgx_link_status  link_status;
+	} data;
+} __packed;
 
-struct nic_mbx *nicvf_get_mbx(void);
+int nicvf_set_real_num_queues(struct net_device *netdev,
+			      int tx_queues, int rx_queues);
+int nicvf_open(struct net_device *netdev);
+int nicvf_stop(struct net_device *netdev);
 int nicvf_send_msg_to_pf(struct nicvf *vf, struct nic_mbx *mbx);
+void nicvf_config_cpi(struct nicvf *nic);
+#ifdef VNIC_RSS_SUPPORT
+void nicvf_config_rss(struct nicvf *nic);
+void nicvf_set_rss_key(struct nicvf *nic);
+#endif
 void nicvf_free_skb(struct nicvf *nic, struct sk_buff *skb);
+void nicvf_set_ethtool_ops(struct net_device *netdev);
+void nicvf_update_stats(struct nicvf *nic);
+void nicvf_update_lmac_stats(struct nicvf *nic);
 
 /* Debug */
 #undef	NIC_DEBUG
diff --git a/drivers/net/ethernet/cavium/thunder/nic_main.c b/drivers/net/ethernet/cavium/thunder/nic_main.c
index 2d2d842..d23df0d 100644
--- a/drivers/net/ethernet/cavium/thunder/nic_main.c
+++ b/drivers/net/ethernet/cavium/thunder/nic_main.c
@@ -1,61 +1,60 @@
 /*
- * This file is subject to the terms and conditions of the GNU General Public
- * License.  See the file "COPYING" in the main directory of this archive
- * for more details.
+ * Copyright (C) 2015 Cavium, Inc.
  *
- * Copyright (C) 2014 Cavium, Inc.
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of
+ * the License, or (at your option) any later version.
  */
 
 #include <linux/module.h>
-#include <linux/kernel.h>
-#include <linux/string.h>
-#include <linux/errno.h>
-#include <linux/types.h>
-#include <linux/init.h>
 #include <linux/interrupt.h>
-#include <linux/workqueue.h>
 #include <linux/pci.h>
 #include <linux/netdevice.h>
 #include <linux/etherdevice.h>
-#include <linux/if.h>
-#include <linux/if_ether.h>
-#include <linux/if_vlan.h>
-#include <linux/ethtool.h>
-#include <linux/aer.h>
+#include <linux/of.h>
 
 #include "nic_reg.h"
 #include "nic.h"
+#include "q_struct.h"
 #include "thunder_bgx.h"
 
 #define DRV_NAME	"thunder-nic"
 #define DRV_VERSION	"1.0"
 
-static void nic_channel_cfg(struct nicpf *nic, int vnic);
+static void nic_config_cpi(struct nicpf *nic, struct cpi_cfg_msg *cfg);
+#ifdef VNIC_RSS_SUPPORT
+static void nic_send_rss_size(struct nicpf *nic, int vf);
+static void nic_config_rss(struct nicpf *nic, struct rss_cfg_msg *cfg);
+#endif
+static void nic_tx_channel_cfg(struct nicpf *nic, int vnic, int sq_idx);
 static int nic_update_hw_frs(struct nicpf *nic, int new_frs, int vf);
+static int nic_rcv_queue_sw_sync(struct nicpf *nic);
+static void nic_get_bgx_stats(struct nicpf *nic, struct bgx_stats_msg *bgx);
 
 /* Supported devices */
-static DEFINE_PCI_DEVICE_TABLE(nic_id_table) = {
+static const struct pci_device_id nic_id_table[] = {
 	{ PCI_DEVICE(PCI_VENDOR_ID_CAVIUM, PCI_DEVICE_ID_THUNDER_NIC_PF) },
 	{ 0, }  /* end of table */
 };
 
-MODULE_AUTHOR("Cavium Inc");
-MODULE_DESCRIPTION("Cavium Thunder Physical Function Driver");
-MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Sunil Goutham");
+MODULE_DESCRIPTION("Cavium Thunder NIC Physical Function Driver");
+MODULE_LICENSE("GPL v2");
 MODULE_VERSION(DRV_VERSION);
 MODULE_DEVICE_TABLE(pci, nic_id_table);
 
 /* Register read/write APIs */
-static void nic_reg_write(struct nicpf *nic, uint64_t offset, uint64_t val)
+static void nic_reg_write(struct nicpf *nic, u64 offset, u64 val)
 {
-	uint64_t addr = nic->reg_base + offset;
+	u64 addr = nic->reg_base + offset;
 
 	writeq_relaxed(val, (void *)addr);
 }
 
-static uint64_t nic_reg_read(struct nicpf *nic, uint64_t offset)
+static u64 nic_reg_read(struct nicpf *nic, u64 offset)
 {
-	uint64_t addr = nic->reg_base + offset;
+	u64 addr = nic->reg_base + offset;
 
 	return readq_relaxed((void *)addr);
 }
@@ -63,162 +62,312 @@ static uint64_t nic_reg_read(struct nicpf *nic, uint64_t offset)
 /* PF -> VF mailbox communication APIs */
 static void nic_enable_mbx_intr(struct nicpf *nic)
 {
-	int	 irq;
-	uint64_t vf_mbx_intr_enable = 0;
-
-	/* TBD: Need to support runtime SRIOV VF count configuratuon */
-	/* Or consider enabling all VF's interrupts, since there is no harm */
-	for (irq = 0; irq < 64; irq++)
-		if (irq < nic->num_vf_en)
-			vf_mbx_intr_enable |= (1 << irq);
-	nic_reg_write(nic, NIC_PF_MAILBOX_ENA_W1S, vf_mbx_intr_enable);
-
-	if (nic->num_vf_en < 64)
-		return;
-
-	vf_mbx_intr_enable = 0;
-	for (irq = 0; irq < 64; irq++)
-		if (irq < (nic->num_vf_en - 64))
-			vf_mbx_intr_enable |= (1 << irq);
-	nic_reg_write(nic, NIC_PF_MAILBOX_ENA_W1S + (1 << 3), vf_mbx_intr_enable);
+	/* Enable mailbox interrupt for all 128 VFs */
+	nic_reg_write(nic, NIC_PF_MAILBOX_ENA_W1S, ~0x00ull);
+	nic_reg_write(nic, NIC_PF_MAILBOX_ENA_W1S + sizeof(u64), ~0x00ull);
 }
 
-static uint64_t nic_get_mbx_intr_status(struct nicpf *nic, int mbx_reg)
+static void nic_clear_mbx_intr(struct nicpf *nic, int vf, int mbx_reg)
 {
-	if (!mbx_reg)	/* first 64 VFs */
-		return nic_reg_read(nic, NIC_PF_MAILBOX_INT);
-	else		/* Next 64 VFs */
-		return nic_reg_read(nic, NIC_PF_MAILBOX_INT + (1 << 3));
+	nic_reg_write(nic, NIC_PF_MAILBOX_INT + (mbx_reg << 3), (1ULL << vf));
 }
 
-static void nic_clear_mbx_intr(struct nicpf *nic, int vf)
+static u64 nic_get_mbx_addr(int vf)
 {
-	if (!(vf / 64))	/* first 64 VFs */
-		nic_reg_write(nic, NIC_PF_MAILBOX_INT, (1ULL << vf));
-	else		/* Next 64 VFs */
-		nic_reg_write(nic, NIC_PF_MAILBOX_INT + (1 << 3), (1ULL << (vf - 64)));
+	return NIC_PF_VF_0_127_MAILBOX_0_1 + (vf << NIC_VF_NUM_SHIFT);
 }
 
-static uint64_t nic_get_mbx_addr(int vf)
+/* Send a mailbox message to VF
+ * @vf: vf to which this message to be sent
+ * @mbx: Message to be sent
+ */
+static int nic_send_msg_to_vf(struct nicpf *nic, int vf, struct nic_mbx *mbx)
 {
-	return NIC_PF_VF_0_127_MAILBOX_0_7 + (vf << NIC_VF_NUM_SHIFT);
+	u64 *msg;
+	u64 mbx_addr;
+
+	msg = (u64 *)mbx;
+	mbx_addr = nic->reg_base + nic_get_mbx_addr(vf);
+
+	/* In first revision HW, mbox interrupt is triggerred
+	 * when PF writes to MBOX(1), in next revisions when
+	 * PF writes to MBOX(0)
+	 */
+	if (nic->rev_id == 0) {
+		writeq_relaxed(*(msg), (void *)mbx_addr);
+		writeq_relaxed(*(msg + 1), (void *)(mbx_addr + 8));
+	} else {
+		writeq_relaxed(*(msg + 1), (void *)(mbx_addr + 8));
+		writeq_relaxed(*(msg), (void *)mbx_addr);
+	}
+
+	return 0;
 }
 
-static void nic_mbx_done(struct nicpf *nic, uint64_t mbx_addr)
+/* Function to read MAC addresses from DTS.
+ * If not found random MACs will be used by VF interface
+ */
+static int nic_get_mac_addresses(struct nicpf *nic)
 {
-	/* write 1 to last MBX reg */
-	mbx_addr += (NIC_PF_VF_MAILBOX_SIZE - 1) * 8;
-	nic_reg_write(nic, mbx_addr, 1ULL);
+	struct device_node *np, *np_child;
+	u32  nplen;
+	u8  *np_ptr = NULL, node[10], next_range_off = 10;
+	u32  i, mac_count = 0, range, mac_range;
+	u64  start_mac = 0, next_mac  = 0;
+
+	/* Check if MAC ID list is present in DTS */
+	np = of_find_node_by_name(NULL, "ethernet-macs");
+	if (!np)
+		return 0;
+
+	/* Check if MAC ID list is available for this node (NUMA) */
+	sprintf(node, "node%d", nic->node);
+	np_child = of_get_child_by_name(np, node);
+	if (!np_child)
+		return 0;
+
+	np_ptr = (u8 *)of_get_property(np_child, "mac", &nplen);
+	if (!np_ptr)
+		return 0;
+
+	for (range = 0; range < nplen; range = range + next_range_off) {
+		/* get first mac into cpu format */
+		memcpy(&start_mac, np_ptr + range, 8);
+#ifdef __BIG_ENDIAN
+		start_mac = start_mac >> 16; /* MACADDR is only 48bits */
+#else
+		start_mac = start_mac << 16;
+#endif
+		start_mac = be64_to_cpu(start_mac);
+		next_mac  = start_mac;
+		/* get range length for given range */
+		mac_range = be32_to_cpup((u32 *)(np_ptr + range +
+					ETH_ALEN));
+		for (i = 0; i < mac_range; i++) {
+#ifdef __BIG_ENDIAN
+			nic->mac[mac_count++] =
+				cpu_to_be64(next_mac) << 16;
+#else
+			nic->mac[mac_count++] =
+				cpu_to_be64(next_mac) >> 16;
+#endif
+			next_mac++;
+		}
+	}
+
+	return 1;
 }
 
+/* Responds to VF's READY message with VF's
+ * ID, node, MAC address e.t.c
+ * @vf: VF which sent READY message
+ */
 static void nic_mbx_send_ready(struct nicpf *nic, int vf)
 {
-	uint64_t mbx_addr;
+	struct nic_mbx mbx = {};
 
-	mbx_addr = nic_get_mbx_addr(vf);
+	mbx.msg = NIC_MBOX_MSG_READY;
+	mbx.data.nic_cfg.vf_id = vf;
 
-	/* Respond with VNIC ID */
-	nic_reg_write(nic, mbx_addr, NIC_PF_VF_MSG_READY);
-	nic_reg_write(nic, mbx_addr + 8, vf);
-	nic_mbx_done(nic, mbx_addr);
+	if (nic->flags & NIC_TNS_ENABLED)
+		mbx.data.nic_cfg.tns_mode = NIC_TNS_MODE;
+	else
+		mbx.data.nic_cfg.tns_mode = NIC_TNS_BYPASS_MODE;
+	ether_addr_copy((u8 *)&mbx.data.nic_cfg.mac_addr,
+			(u8 *)&nic->mac[vf]);
+	mbx.data.nic_cfg.node_id = nic->node;
+	nic_send_msg_to_vf(nic, vf, &mbx);
 }
 
+/* ACKs VF's mailbox message
+ * @vf: VF to which ACK to be sent
+ */
 static void nic_mbx_send_ack(struct nicpf *nic, int vf)
 {
-	uint64_t mbx_addr;
-
-	mbx_addr = nic_get_mbx_addr(vf);
+	struct nic_mbx mbx = {};
 
-	nic_reg_write(nic, mbx_addr, NIC_PF_VF_MSG_ACK);
-	nic_mbx_done(nic, mbx_addr);
+	mbx.msg = NIC_MBOX_MSG_ACK;
+	nic_send_msg_to_vf(nic, vf, &mbx);
 }
 
+/* NACKs VF's mailbox message that PF is not able to
+ * complete the action
+ * @vf: VF to which ACK to be sent
+ */
 static void nic_mbx_send_nack(struct nicpf *nic, int vf)
 {
-	uint64_t mbx_addr;
-
-	mbx_addr = nic_get_mbx_addr(vf);
+	struct nic_mbx mbx = {};
 
-	nic_reg_write(nic, mbx_addr, NIC_PF_VF_MSG_NACK);
-	nic_mbx_done(nic, mbx_addr);
+	mbx.msg = NIC_MBOX_MSG_NACK;
+	nic_send_msg_to_vf(nic, vf, &mbx);
 }
 
-/* Handle Mailbox messgaes from VF and ack the message. */
+/* Interrupt handler to handle mailbox messages from VFs */
 static void nic_handle_mbx_intr(struct nicpf *nic, int vf)
 {
-	int i, ret = 0;
-	struct nic_mbx mbx_tmp, *mbx;
-	uint64_t *mbx_data;
-	uint64_t reg_addr;
-	uint64_t mbx_addr;
+	struct nic_mbx mbx = {};
+	u64 *mbx_data;
+	u64 mbx_addr;
+	u64 reg_addr;
+	u64 mac_addr;
+	int bgx, lmac;
+	int i;
+	int ret = 0;
 
-	mbx_addr = NIC_PF_VF_0_127_MAILBOX_0_7;
-	mbx_addr += (vf << NIC_VF_NUM_SHIFT);
+	nic->mbx_lock[vf] = true;
 
-	mbx = &mbx_tmp;
-	mbx_data = (uint64_t *)mbx;
+	mbx_addr = nic_get_mbx_addr(vf);
+	mbx_data = (u64 *)&mbx;
 
 	for (i = 0; i < NIC_PF_VF_MAILBOX_SIZE; i++) {
-		*mbx_data = nic_reg_read(nic, mbx_addr + (i * NIC_PF_VF_MAILBOX_SIZE));
+		*mbx_data = nic_reg_read(nic, mbx_addr);
 		mbx_data++;
+		mbx_addr += sizeof(u64);
 	}
 
-	mbx->msg &= 0xFF;
 	nic_dbg(&nic->pdev->dev, "%s: Mailbox msg %d from VF%d\n",
-		__func__, mbx->msg, vf);
-	switch (mbx->msg) {
-	case NIC_PF_VF_MSG_READY:
+		__func__, mbx.msg, vf);
+	switch (mbx.msg) {
+	case NIC_MBOX_MSG_READY:
 		nic_mbx_send_ready(nic, vf);
+		nic->link[vf] = 0;
+		nic->duplex[vf] = 0;
+		nic->speed[vf] = 0;
 		ret = 1;
 		break;
-	case NIC_PF_VF_MSG_QS_CFG:
-		reg_addr = NIC_PF_QSET_0_127_CFG | (mbx->data.qs.num << NIC_QS_ID_SHIFT);
-		nic_reg_write(nic, reg_addr, mbx->data.qs.cfg);
-		nic_channel_cfg(nic, mbx->data.qs.num);
-		if (!mbx->data.qs.cfg)
-			bgx_lmac_disable(mbx->data.qs.num);
-		else
-			bgx_lmac_enable(mbx->data.qs.num);
+	case NIC_MBOX_MSG_QS_CFG:
+		reg_addr = NIC_PF_QSET_0_127_CFG |
+			   (mbx.data.qs.num << NIC_QS_ID_SHIFT);
+		nic_reg_write(nic, reg_addr, mbx.data.qs.cfg);
 		break;
-	case NIC_PF_VF_MSG_RQ_CFG:
-		reg_addr = NIC_PF_QSET_0_127_RQ_0_7_CFG | (mbx->data.rq.qs_num << NIC_QS_ID_SHIFT) |
-							  (mbx->data.rq.rq_num << NIC_Q_NUM_SHIFT);
-		nic_reg_write(nic, reg_addr, mbx->data.rq.cfg);
+	case NIC_MBOX_MSG_RQ_CFG:
+		reg_addr = NIC_PF_QSET_0_127_RQ_0_7_CFG |
+			   (mbx.data.rq.qs_num << NIC_QS_ID_SHIFT) |
+			   (mbx.data.rq.rq_num << NIC_Q_NUM_SHIFT);
+		nic_reg_write(nic, reg_addr, mbx.data.rq.cfg);
 		break;
-	case NIC_PF_VF_MSG_RQ_DROP_CFG:
-		reg_addr = NIC_PF_QSET_0_127_RQ_0_7_DROP_CFG | (mbx->data.rq.qs_num << NIC_QS_ID_SHIFT) |
-								(mbx->data.rq.rq_num << NIC_Q_NUM_SHIFT);
-		nic_reg_write(nic, reg_addr, mbx->data.rq.cfg);
+	case NIC_MBOX_MSG_RQ_BP_CFG:
+		reg_addr = NIC_PF_QSET_0_127_RQ_0_7_BP_CFG |
+			   (mbx.data.rq.qs_num << NIC_QS_ID_SHIFT) |
+			   (mbx.data.rq.rq_num << NIC_Q_NUM_SHIFT);
+		nic_reg_write(nic, reg_addr, mbx.data.rq.cfg);
 		break;
-	case NIC_PF_VF_MSG_SQ_CFG:
-		reg_addr = NIC_PF_QSET_0_127_SQ_0_7_CFG | (mbx->data.sq.qs_num << NIC_QS_ID_SHIFT) |
-							  (mbx->data.sq.sq_num << NIC_Q_NUM_SHIFT);
-		nic_reg_write(nic, reg_addr, mbx->data.sq.cfg);
+	case NIC_MBOX_MSG_RQ_SW_SYNC:
+		ret = nic_rcv_queue_sw_sync(nic);
+		/* Last message of VF teardown msg sequence */
+		nic->vf_enabled[vf] = false;
 		break;
-	case NIC_PF_VF_MSG_SET_MAC:
-		bgx_add_dmac_addr(mbx->data.mac.addr, mbx->data.mac.vnic_id);
+	case NIC_MBOX_MSG_RQ_DROP_CFG:
+		reg_addr = NIC_PF_QSET_0_127_RQ_0_7_DROP_CFG |
+			   (mbx.data.rq.qs_num << NIC_QS_ID_SHIFT) |
+			   (mbx.data.rq.rq_num << NIC_Q_NUM_SHIFT);
+		nic_reg_write(nic, reg_addr, mbx.data.rq.cfg);
+		/* Last message of VF config msg sequence */
+		nic->vf_enabled[vf] = true;
 		break;
-	case NIC_VF_SET_MAX_FRS:
-		ret = nic_update_hw_frs(nic, mbx->data.max_frs, vf);
+	case NIC_MBOX_MSG_SQ_CFG:
+		reg_addr = NIC_PF_QSET_0_127_SQ_0_7_CFG |
+			   (mbx.data.sq.qs_num << NIC_QS_ID_SHIFT) |
+			   (mbx.data.sq.sq_num << NIC_Q_NUM_SHIFT);
+		nic_reg_write(nic, reg_addr, mbx.data.sq.cfg);
+		nic_tx_channel_cfg(nic, mbx.data.qs.num, mbx.data.sq.sq_num);
 		break;
+	case NIC_MBOX_MSG_SET_MAC:
+		lmac = mbx.data.mac.vf_id;
+		bgx = NIC_GET_BGX_FROM_VF_LMAC_MAP(nic->vf_lmac_map[lmac]);
+		lmac = NIC_GET_LMAC_FROM_VF_LMAC_MAP(nic->vf_lmac_map[lmac]);
+#ifdef __BIG_ENDIAN
+		mac_addr = cpu_to_be64(mbx.data.nic_cfg.mac_addr) << 16;
+#else
+		mac_addr = cpu_to_be64(mbx.data.nic_cfg.mac_addr) >> 16;
+#endif
+		ether_addr_copy((u8 *)&nic->mac[vf],
+				(u8 *)&mac_addr);
+		break;
+	case NIC_MBOX_MSG_SET_MAX_FRS:
+		ret = nic_update_hw_frs(nic, mbx.data.frs.max_frs,
+					mbx.data.frs.vf_id);
+		break;
+	case NIC_MBOX_MSG_CPI_CFG:
+		nic_config_cpi(nic, &mbx.data.cpi_cfg);
+		break;
+#ifdef VNIC_RSS_SUPPORT
+	case NIC_MBOX_MSG_RSS_SIZE:
+		nic_send_rss_size(nic, vf);
+		goto unlock;
+	case NIC_MBOX_MSG_RSS_CFG:
+	case NIC_MBOX_MSG_RSS_CFG_CONT:
+		nic_config_rss(nic, &mbx.data.rss_cfg);
+		break;
+#endif
+	case NIC_MBOX_MSG_BGX_STATS:
+		nic_get_bgx_stats(nic, &mbx.data.bgx_stats);
+		goto unlock;
 	default:
-		netdev_err(nic->netdev, "Invalid message from VF%d, msg 0x%llx\n", vf, mbx->msg);
+		netdev_err(nic->netdev,
+			   "Invalid msg from VF%d, msg 0x%x\n", vf, mbx.msg);
 		break;
 	}
 
 	if (!ret)
 		nic_mbx_send_ack(nic, vf);
-	else if (mbx->msg != NIC_PF_VF_MSG_READY)
+	else if (mbx.msg != NIC_MBOX_MSG_READY)
 		nic_mbx_send_nack(nic, vf);
+unlock:
+	nic->mbx_lock[vf] = false;
 }
 
+/* Flush all in flight receive packets to memory and
+ * bring down an active RQ
+ */
+static int nic_rcv_queue_sw_sync(struct nicpf *nic)
+{
+	u16 timeout = ~0x00;
+
+	nic_reg_write(nic, NIC_PF_SW_SYNC_RX, 0x01);
+	/* Wait till sync cycle is finished */
+	while (timeout) {
+		if (nic_reg_read(nic, NIC_PF_SW_SYNC_RX_DONE) & 0x1)
+			break;
+		timeout--;
+	}
+	nic_reg_write(nic, NIC_PF_SW_SYNC_RX, 0x00);
+	if (!timeout) {
+		netdev_err(nic->netdev, "Recevie queue software sync failed");
+		return 1;
+	}
+	return 0;
+}
+
+/* Get BGX Rx/Tx stats and respond to VF's request */
+static void nic_get_bgx_stats(struct nicpf *nic, struct bgx_stats_msg *bgx)
+{
+	int bgx_idx, lmac;
+	struct nic_mbx mbx = {};
+
+	bgx_idx = NIC_GET_BGX_FROM_VF_LMAC_MAP(nic->vf_lmac_map[bgx->vf_id]);
+	lmac = NIC_GET_LMAC_FROM_VF_LMAC_MAP(nic->vf_lmac_map[bgx->vf_id]);
+
+	mbx.msg = NIC_MBOX_MSG_BGX_STATS;
+	mbx.data.bgx_stats.vf_id = bgx->vf_id;
+	mbx.data.bgx_stats.rx = bgx->rx;
+	mbx.data.bgx_stats.idx = bgx->idx;
+	if (bgx->rx)
+		mbx.data.bgx_stats.stats = bgx_get_rx_stats(bgx_idx,
+							    lmac, bgx->idx);
+	else
+		mbx.data.bgx_stats.stats = bgx_get_tx_stats(bgx_idx,
+							    lmac, bgx->idx);
+	nic_send_msg_to_vf(nic, bgx->vf_id, &mbx);
+}
+
+/* Update hardware min/max frame size */
 static int nic_update_hw_frs(struct nicpf *nic, int new_frs, int vf)
 {
 	if ((new_frs > NIC_HW_MAX_FRS) || (new_frs < NIC_HW_MIN_FRS)) {
 		netdev_err(nic->netdev,
-			   "Invalid MTU setting from VF%d rejected"
-			   "should be between %d and %d\n", vf,
-			   NIC_HW_MIN_FRS, NIC_HW_MAX_FRS);
+			   "Invalid MTU setting from VF%d rejected, should be between %d and %d\n",
+			   vf, NIC_HW_MIN_FRS, NIC_HW_MAX_FRS);
 		return 1;
 	}
 	new_frs += ETH_HLEN;
@@ -226,23 +375,101 @@ static int nic_update_hw_frs(struct nicpf *nic, int new_frs, int vf)
 		return 0;
 
 	nic->pkind.maxlen = new_frs;
-	nic_reg_write(nic, NIC_PF_PKIND_0_15_CFG, *(uint64_t *)&nic->pkind);
+	nic_reg_write(nic, NIC_PF_PKIND_0_15_CFG, *(u64 *)&nic->pkind);
 	return 0;
 }
 
+/* Set minimum transmit packet size */
+static void nic_set_tx_pkt_pad(struct nicpf *nic, int size)
+{
+	int lmac;
+	u64 lmac_cfg;
+
+	/* Max value that can be set is 60 */
+	if (size > 60)
+		size = 60;
+
+	for (lmac = 0; lmac < (MAX_BGX_PER_CN88XX * MAX_LMAC_PER_BGX); lmac++) {
+		lmac_cfg = nic_reg_read(nic, NIC_PF_LMAC_0_7_CFG | (lmac << 3));
+		lmac_cfg &= ~(0xF << 2);
+		lmac_cfg |= ((size / 4) << 2);
+		nic_reg_write(nic, NIC_PF_LMAC_0_7_CFG | (lmac << 3), lmac_cfg);
+	}
+}
+
+/* Function to check number of LMACs present and set VF::LMAC mapping.
+ * Mapping will be used while initializing channels.
+ */
+static void nic_set_lmac_vf_mapping(struct nicpf *nic)
+{
+	int bgx, bgx_count, next_bgx_lmac = 0;
+	int lmac, lmac_cnt = 0;
+	u64 lmac_credit;
+
+	nic->num_vf_en = 0;
+	if (nic->flags & NIC_TNS_ENABLED) {
+		nic->num_vf_en = DEFAULT_NUM_VF_ENABLED;
+		return;
+	}
+
+	bgx_get_count(nic->node, &bgx_count);
+	for (bgx = 0; bgx < NIC_MAX_BGX; bgx++) {
+		if (!(bgx_count & (1 << bgx)))
+			continue;
+		nic->bgx_cnt++;
+		lmac_cnt = bgx_get_lmac_count(nic->node, bgx);
+		for (lmac = 0; lmac < lmac_cnt; lmac++)
+			nic->vf_lmac_map[next_bgx_lmac++] =
+						NIC_SET_VF_LMAC_MAP(bgx, lmac);
+		nic->num_vf_en += lmac_cnt;
+
+		/* Program LMAC credits */
+		lmac_credit = (1ull << 1); /* channel credit enable */
+		lmac_credit |= (0x1ff << 2); /* Max outstanding pkt count */
+		/* 48KB BGX Tx buffer size, each unit is of size 16bytes */
+		lmac_credit |= (((((48 * 1024) / lmac_cnt) -
+				NIC_HW_MAX_FRS) / 16) << 12);
+		lmac = bgx * MAX_LMAC_PER_BGX;
+		for (; lmac < lmac_cnt + (bgx * MAX_LMAC_PER_BGX); lmac++)
+			nic_reg_write(nic,
+				      NIC_PF_LMAC_0_7_CREDIT + (lmac * 8),
+				      lmac_credit);
+	}
+}
+
 static void nic_init_hw(struct nicpf *nic)
 {
 	int i;
+	u64 reg;
 
 	/* Reset NIC, incase if driver is repeatedly inserted and removed */
 	nic_reg_write(nic, NIC_PF_SOFT_RESET, 1);
 
 	/* Enable NIC HW block */
-	nic_reg_write(nic, NIC_PF_CFG, 1);
-
-	/* Disable TNS mode */
-	nic_reg_write(nic, NIC_PF_INTF_0_1_SEND_CFG, 0);
-	nic_reg_write(nic, NIC_PF_INTF_0_1_SEND_CFG | (1 << 8), 0);
+	nic_reg_write(nic, NIC_PF_CFG, 0x3);
+
+	/* Enable backpressure */
+	nic_reg_write(nic, NIC_PF_BP_CFG, (1ULL << 6) | 0x03);
+	nic_reg_write(nic, NIC_PF_INTF_0_1_BP_CFG, (1ULL << 63) | 0x08);
+	nic_reg_write(nic,
+		      NIC_PF_INTF_0_1_BP_CFG + (1 << 8), (1ULL << 63) | 0x09);
+
+	if (nic->flags & NIC_TNS_ENABLED) {
+		reg = NIC_TNS_MODE << 7;
+		reg |= 0x06;
+		nic_reg_write(nic, NIC_PF_INTF_0_1_SEND_CFG, reg);
+		reg &= ~0xFull;
+		reg |= 0x07;
+		nic_reg_write(nic, NIC_PF_INTF_0_1_SEND_CFG | (1 << 8), reg);
+	} else {
+		/* Disable TNS mode on both interfaces */
+		reg = NIC_TNS_BYPASS_MODE << 7;
+		reg |= 0x08; /* Block identifier */
+		nic_reg_write(nic, NIC_PF_INTF_0_1_SEND_CFG, reg);
+		reg &= ~0xFull;
+		reg |= 0x09;
+		nic_reg_write(nic, NIC_PF_INTF_0_1_SEND_CFG | (1 << 8), reg);
+	}
 
 	/* PKIND configuration */
 	nic->pkind.minlen = 0;
@@ -253,83 +480,203 @@ static void nic_init_hw(struct nicpf *nic)
 
 	for (i = 0; i < NIC_MAX_PKIND; i++)
 		nic_reg_write(nic, NIC_PF_PKIND_0_15_CFG | (i << 3),
-			       *(uint64_t *)&nic->pkind);
+			      *(u64 *)&nic->pkind);
 
-	/* Disable backpressure for now */
-	for (i = 0; i < NIC_MAX_CHANS; i++)
-		nic_reg_write(nic, NIC_PF_CHAN_0_255_TX_CFG | (i << 3), 0);
+	nic_set_tx_pkt_pad(nic, NIC_HW_MIN_FRS);
+
+	/* Timer config */
+	nic_reg_write(nic, NIC_PF_INTR_TIMER_CFG, NICPF_CLK_PER_INT_TICK);
 }
 
-static void nic_channel_cfg(struct nicpf *nic, int vnic)
+/* Channel parse index configuration */
+static void nic_config_cpi(struct nicpf *nic, struct cpi_cfg_msg *cfg)
 {
-	uint8_t  rq_idx = 0;
-	uint8_t  sq_idx = 0;
-	uint32_t bgx, lmac, chan, tl3, tl4;
-	uint64_t cpi_base, rssi_base;
+	u32 vnic, bgx, lmac, chan;
+	u32 padd, cpi_count = 0;
+	u64 cpi_base, cpi, rssi_base, rssi;
+	u8  qset, rq_idx = 0;
+
+	vnic = cfg->vf_id;
+	bgx = NIC_GET_BGX_FROM_VF_LMAC_MAP(nic->vf_lmac_map[vnic]);
+	lmac = NIC_GET_LMAC_FROM_VF_LMAC_MAP(nic->vf_lmac_map[vnic]);
+
+	chan = (lmac * MAX_BGX_CHANS_PER_LMAC) + (bgx * NIC_CHANS_PER_INF);
+	cpi_base = (lmac * NIC_MAX_CPI_PER_LMAC) + (bgx * NIC_CPI_PER_BGX);
+	rssi_base = (lmac * nic->rss_ind_tbl_size) + (bgx * NIC_RSSI_PER_BGX);
+
+	/* Rx channel configuration */
+	nic_reg_write(nic, NIC_PF_CHAN_0_255_RX_BP_CFG | (chan << 3),
+		      (1ull << 63) | (vnic << 0));
+	nic_reg_write(nic, NIC_PF_CHAN_0_255_RX_CFG | (chan << 3),
+		      ((u64)cfg->cpi_alg << 62) | (cpi_base << 48));
+
+	if (cfg->cpi_alg == CPI_ALG_NONE)
+		cpi_count = 1;
+	else if (cfg->cpi_alg == CPI_ALG_VLAN) /* 3 bits of PCP */
+		cpi_count = 8;
+	else if (cfg->cpi_alg == CPI_ALG_VLAN16) /* 3 bits PCP + DEI */
+		cpi_count = 16;
+	else if (cfg->cpi_alg == CPI_ALG_DIFF) /* 6bits DSCP */
+		cpi_count = NIC_MAX_CPI_PER_LMAC;
+
+	/* RSS Qset, Qidx mapping */
+	qset = cfg->vf_id;
+	rssi = rssi_base;
+	for (; rssi < (rssi_base + cfg->rq_cnt); rssi++) {
+		nic_reg_write(nic, NIC_PF_RSSI_0_4097_RQ | (rssi << 3),
+			      (qset << 3) | rq_idx);
+		rq_idx++;
+	}
 
-	/* Below are the channel mappings
-	 * BGX0-LMAC0-CHAN0 - VNIC CHAN0
-	 * BGX0-LMAC1-CHAN0 - VNIC CHAN16
-	 * ...
-	 * BGX1-LMAC0-CHAN0 - VNIC CHAN128
-	 * ...
-	 * BGX1-LMAC3-CHAN0 - VNIC CHAN174
-	 */
-	bgx = vnic / MAX_LMAC_PER_BGX;
-	lmac = vnic - (bgx * MAX_LMAC_PER_BGX);
-	chan = (lmac * MAX_BGX_CHANS_PER_LMAC) + (bgx * NIC_CHANS_PER_BGX_INF);
-	cpi_base = (lmac * NIC_CPI_PER_LMAC) + (bgx * NIC_CPI_PER_BGX);
-	rssi_base = (lmac * NIC_RSSI_PER_LMAC) + (bgx * NIC_RSSI_PER_BGX);
+	rssi = 0;
+	cpi = cpi_base;
+	for (; cpi < (cpi_base + cpi_count); cpi++) {
+		/* Determine port to channel adder */
+		if (cfg->cpi_alg != CPI_ALG_DIFF)
+			padd = cpi % cpi_count;
+		else
+			padd = cpi % 8; /* 3 bits CS out of 6bits DSCP */
 
-	nic_reg_write(nic, NIC_PF_CHAN_0_255_RX_CFG | (chan << 3),
-		      cpi_base << 48);
-	nic_reg_write(nic, NIC_PF_CPI_0_2047_CFG | (cpi_base << 3),
-		      (vnic << 24) | rssi_base);
-	/* RQ's QS & RQ idx within QS */
-	nic_reg_write(nic, NIC_PF_RSSI_0_4097_RQ | (rssi_base << 3),
-		      (vnic << 3) | rq_idx);
-
-	/* Transmit Channel config (TL4 -> TL3 -> Chan) */
-	/* VNIC0-SQ0 -> TL4(0)  -> TL4A(0) -> TL3[0] -> BGX0/LMAC0/Chan0
-	 * VNIC1-SQ0 -> TL4(8)  -> TL4A(2) -> TL3[2] -> BGX0/LMAC1/Chan0
-	 * VNIC2-SQ0 -> TL4(16) -> TL4A(4) -> TL3[4] -> BGX0/LMAC2/Chan0
-	 * VNIC3-SQ0 -> TL4(32) -> TL4A(6) -> TL3[6] -> BGX0/LMAC3/Chan0
-	 * VNIC4-SQ0 -> TL4(512)  -> TL4A(128) -> TL3[128] -> BGX1/LMAC0/Chan0
-	 * VNIC5-SQ0 -> TL4(520)  -> TL4A(130) -> TL3[130] -> BGX1/LMAC1/Chan0
-	 * VNIC6-SQ0 -> TL4(528)  -> TL4A(132) -> TL3[132] -> BGX1/LMAC2/Chan0
-	 * VNIC7-SQ0 -> TL4(536)  -> TL4A(134) -> TL3[134] -> BGX1/LMAC3/Chan0
-	 */
-	tl4 = (lmac * NIC_TL4_PER_LMAC) + (bgx * NIC_TL4_PER_BGX);
+		/* Leave RSS_SIZE as '0' to disable RSS */
+		nic_reg_write(nic, NIC_PF_CPI_0_2047_CFG | (cpi << 3),
+			      (vnic << 24) | (padd << 16) | (rssi_base + rssi));
 
-	for (sq_idx = 0; sq_idx < 8; sq_idx++) {
-		tl4 = tl4 + sq_idx;
-		tl3 = tl4 / (NIC_MAX_TL4 / NIC_MAX_TL3);
-		nic_reg_write(nic, NIC_PF_QSET_0_127_SQ_0_7_CFG2 |
-					(vnic << NIC_QS_ID_SHIFT) |
-					(sq_idx << NIC_Q_NUM_SHIFT), tl4);
-		nic_reg_write(nic, NIC_PF_TL4_0_1023_CFG | (tl4 << 3),
-			      (vnic << 27) | (sq_idx << 24));
-		nic_reg_write(nic, NIC_PF_TL4A_0_255_CFG | (tl3 << 3), tl3);
-		nic_reg_write(nic, NIC_PF_TL3_0_255_CHAN | (tl3 << 3),
-			      (lmac << 4));
+		if ((rssi + 1) >= cfg->rq_cnt)
+			continue;
+
+		if (cfg->cpi_alg == CPI_ALG_VLAN)
+			rssi++;
+		else if (cfg->cpi_alg == CPI_ALG_VLAN16)
+			rssi = ((cpi - cpi_base) & 0xe) >> 1;
+		else if (cfg->cpi_alg == CPI_ALG_DIFF)
+			rssi = ((cpi - cpi_base) & 0x38) >> 3;
 	}
+	nic->cpi_base[cfg->vf_id] = cpi_base;
 }
 
-static irqreturn_t nic_intr_handler (int irq, void *nic_irq)
+#ifdef VNIC_RSS_SUPPORT
+/* Responsds to VF with its RSS indirection table size */
+static void nic_send_rss_size(struct nicpf *nic, int vf)
 {
-	int vf;
-	uint64_t intr;
-	struct nicpf *nic = (struct nicpf *)nic_irq;
+	struct nic_mbx mbx = {};
+	u64  *msg;
 
-	intr = nic_get_mbx_intr_status(nic, 0); /* Mbox 0 */
-	nic_dbg(&nic->pdev->dev, "PF MSIX interrupt 0x%llx\n", intr);
-	for (vf = 0; vf < nic->num_vf_en; vf++) {
-		if (intr & (1 << vf)) {
-			nic_dbg(&nic->pdev->dev, "Intr from VF %d\n", vf);
-			nic_handle_mbx_intr(nic, vf);
-			nic_clear_mbx_intr(nic, vf);
+	msg = (u64 *)&mbx;
+
+	mbx.msg = NIC_MBOX_MSG_RSS_SIZE;
+	mbx.data.rss_size.ind_tbl_size = nic->rss_ind_tbl_size;
+	nic_send_msg_to_vf(nic, vf, &mbx);
+}
+
+/* Receive side scaling configuration
+ * configure:
+ * - RSS index
+ * - indir table i.e hash::RQ mapping
+ * - no of hash bits to consider
+ */
+static void nic_config_rss(struct nicpf *nic, struct rss_cfg_msg *cfg)
+{
+	u8  qset, idx = 0;
+	u64 cpi_cfg, cpi_base, rssi_base, rssi;
+
+	cpi_base = nic->cpi_base[cfg->vf_id];
+	cpi_cfg = nic_reg_read(nic, NIC_PF_CPI_0_2047_CFG | (cpi_base << 3));
+	rssi_base = (cpi_cfg & 0x0FFF) + cfg->tbl_offset;
+
+	rssi = rssi_base;
+	qset = cfg->vf_id;
+
+	for (; rssi < (rssi_base + cfg->tbl_len); rssi++) {
+		nic_reg_write(nic, NIC_PF_RSSI_0_4097_RQ | (rssi << 3),
+			      (qset << 3) | (cfg->ind_tbl[idx] & 0x7));
+		idx++;
+	}
+
+	cpi_cfg &= ~(0xFULL << 20);
+	cpi_cfg |= (cfg->hash_bits << 20);
+	nic_reg_write(nic, NIC_PF_CPI_0_2047_CFG | (cpi_base << 3), cpi_cfg);
+}
+#endif
+
+/* 4 level transmit side scheduler configutation
+ * for TNS bypass mode
+ *
+ * Sample configuration for SQ0
+ * VNIC0-SQ0 -> TL4(0)   -> TL3[0]   -> TL2[0]  -> TL1[0] -> BGX0
+ * VNIC1-SQ0 -> TL4(8)   -> TL3[2]   -> TL2[0]  -> TL1[0] -> BGX0
+ * VNIC2-SQ0 -> TL4(16)  -> TL3[4]   -> TL2[1]  -> TL1[0] -> BGX0
+ * VNIC3-SQ0 -> TL4(24)  -> TL3[6]   -> TL2[1]  -> TL1[0] -> BGX0
+ * VNIC4-SQ0 -> TL4(512) -> TL3[128] -> TL2[32] -> TL1[1] -> BGX1
+ * VNIC5-SQ0 -> TL4(520) -> TL3[130] -> TL2[32] -> TL1[1] -> BGX1
+ * VNIC6-SQ0 -> TL4(528) -> TL3[132] -> TL2[33] -> TL1[1] -> BGX1
+ * VNIC7-SQ0 -> TL4(536) -> TL3[134] -> TL2[33] -> TL1[1] -> BGX1
+ */
+static void nic_tx_channel_cfg(struct nicpf *nic, int vnic, int sq_idx)
+{
+	u32 bgx, lmac, chan;
+	u32 tl2, tl3, tl4;
+	u32 rr_quantum;
+
+	bgx = NIC_GET_BGX_FROM_VF_LMAC_MAP(nic->vf_lmac_map[vnic]);
+	lmac = NIC_GET_LMAC_FROM_VF_LMAC_MAP(nic->vf_lmac_map[vnic]);
+	/* 24 bytes for FCS, IPG and preamble */
+	rr_quantum = ((NIC_HW_MAX_FRS + 24) / 4);
+
+	tl4 = (lmac * NIC_TL4_PER_LMAC) + (bgx * NIC_TL4_PER_BGX);
+	tl4 += sq_idx;
+	tl3 = tl4 / (NIC_MAX_TL4 / NIC_MAX_TL3);
+	nic_reg_write(nic, NIC_PF_QSET_0_127_SQ_0_7_CFG2 |
+		      (vnic << NIC_QS_ID_SHIFT) |
+		      (sq_idx << NIC_Q_NUM_SHIFT), tl4);
+	nic_reg_write(nic, NIC_PF_TL4_0_1023_CFG | (tl4 << 3),
+		      (vnic << 27) | (sq_idx << 24) | rr_quantum);
+
+	nic_reg_write(nic, NIC_PF_TL3_0_255_CFG | (tl3 << 3), rr_quantum);
+	chan = (lmac * MAX_BGX_CHANS_PER_LMAC) + (bgx * NIC_CHANS_PER_INF);
+	nic_reg_write(nic, NIC_PF_TL3_0_255_CHAN | (tl3 << 3), chan);
+	/* Enable backpressure on the channel */
+	nic_reg_write(nic, NIC_PF_CHAN_0_255_TX_CFG | (chan << 3), 1);
+
+	tl2 = tl3 >> 2;
+	nic_reg_write(nic, NIC_PF_TL3A_0_63_CFG | (tl2 << 3), tl2);
+	nic_reg_write(nic, NIC_PF_TL2_0_63_CFG | (tl2 << 3), rr_quantum);
+	/* No priorities as of now */
+	nic_reg_write(nic, NIC_PF_TL2_0_63_PRI | (tl2 << 3), 0x00);
+}
+
+static void nic_mbx_intr_handler (struct nicpf *nic, int mbx)
+{
+	u64 intr;
+	u8  vf, vf_per_mbx_reg = 64;
+
+	intr = nic_reg_read(nic, NIC_PF_MAILBOX_INT + (mbx << 3));
+	nic_dbg(&nic->pdev->dev, "PF interrupt Mbox%d 0x%llx\n", mbx, intr);
+	for (vf = 0; vf < vf_per_mbx_reg; vf++) {
+		if (intr & (1ULL << vf)) {
+			nic_dbg(&nic->pdev->dev, "Intr from VF %d\n",
+				vf + (mbx * vf_per_mbx_reg));
+			if ((vf + (mbx * vf_per_mbx_reg)) > nic->num_vf_en)
+				break;
+			nic_handle_mbx_intr(nic, vf + (mbx * vf_per_mbx_reg));
+			nic_clear_mbx_intr(nic, vf, mbx);
 		}
 	}
+}
+
+static irqreturn_t nic_mbx0_intr_handler (int irq, void *nic_irq)
+{
+	struct nicpf *nic = (struct nicpf *)nic_irq;
+
+	nic_mbx_intr_handler(nic, 0);
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t nic_mbx1_intr_handler (int irq, void *nic_irq)
+{
+	struct nicpf *nic = (struct nicpf *)nic_irq;
+
+	nic_mbx_intr_handler(nic, 1);
 
 	return IRQ_HANDLED;
 }
@@ -344,21 +691,11 @@ static int nic_enable_msix(struct nicpf *nic)
 		nic->msix_entries[i].entry = i;
 
 	ret = pci_enable_msix(nic->pdev, nic->msix_entries, nic->num_vec);
-	if (ret < 0) {
+	if (ret) {
 		netdev_err(nic->netdev,
-			"Request for #%d msix vectors failed\n", nic->num_vec);
+			   "Request for #%d msix vectors failed\n",
+			   nic->num_vec);
 		return 0;
-	} else if (ret > 0) {
-		netdev_err(nic->netdev,
-			"Request for #%d msix vectors failed, requesting #%d\n",
-			nic->num_vec, ret);
-
-		nic->num_vec = ret;
-		ret = pci_enable_msix(nic->pdev, nic->msix_entries, nic->num_vec);
-		if (ret) {
-			netdev_warn(nic->netdev, "Request for msix vectors failed\n");
-			return 0;
-		}
 	}
 
 	nic->msix_enabled = 1;
@@ -376,32 +713,38 @@ static void nic_disable_msix(struct nicpf *nic)
 
 static int nic_register_interrupts(struct nicpf *nic)
 {
-	int irq, free, ret = 0;
+	int irq, ret = 0;
 
 	/* Enable MSI-X */
 	if (!nic_enable_msix(nic))
 		return 1;
 
-	/* Register interrupts */
-	/* For now skip ECC interrupts, register only Mbox interrupts */
-	for (irq = 8; irq < nic->num_vec; irq++) {
-		ret = request_irq(nic->msix_entries[irq].vector,
-				nic_intr_handler, 0 , "NIC PF", nic);
-		if (ret)
-			break;
-	}
+	/* Register mailbox interrupt handlers */
+	ret = request_irq(nic->msix_entries[NIC_PF_INTR_ID_MBOX0].vector,
+			  nic_mbx0_intr_handler, 0 , "NIC Mbox0", nic);
+	if (ret)
+		goto fail;
+	else
+		nic->irq_allocated[NIC_PF_INTR_ID_MBOX0] = true;
 
-	if (ret) {
-		netdev_err(nic->netdev, "Request irq failed\n");
-		for (free = 0; free < irq; free++)
-			free_irq(nic->msix_entries[free].vector, nic);
-		return 1;
-	}
+	ret = request_irq(nic->msix_entries[NIC_PF_INTR_ID_MBOX1].vector,
+			  nic_mbx1_intr_handler, 0 , "NIC Mbox1", nic);
+	if (ret)
+		goto fail;
+	else
+		nic->irq_allocated[NIC_PF_INTR_ID_MBOX1] = true;
 
 	/* Enable mailbox interrupt */
 	nic_enable_mbx_intr(nic);
-
 	return 0;
+fail:
+	netdev_err(nic->netdev, "Request irq failed\n");
+	for (irq = 0; irq < nic->num_vec; irq++) {
+		if (nic->irq_allocated[irq])
+			free_irq(nic->msix_entries[irq].vector, nic);
+		nic->irq_allocated[irq] = false;
+	}
+	return 1;
 }
 
 static void nic_unregister_interrupts(struct nicpf *nic)
@@ -409,30 +752,16 @@ static void nic_unregister_interrupts(struct nicpf *nic)
 	int irq;
 
 	/* Free registered interrupts */
-	for (irq = 0; irq < nic->num_vec; irq++)
-		free_irq(nic->msix_entries[irq].vector, nic);
+	for (irq = 0; irq < nic->num_vec; irq++) {
+		if (nic->irq_allocated[irq])
+			free_irq(nic->msix_entries[irq].vector, nic);
+		nic->irq_allocated[irq] = false;
+	}
 
 	/* Disable MSI-X */
 	nic_disable_msix(nic);
 }
 
-void nic_set_sriov_enable(struct nicpf *nic)
-{
-	nic->flags |= NIC_SRIOV_ENABLED;
-}
-
-void nic_clear_sriov_enable(struct nicpf *nic)
-{
-	nic->flags &= ~NIC_SRIOV_ENABLED;
-}
-
-bool nic_is_sriov_enabled(struct nicpf *nic)
-{
-	if (nic->flags & NIC_SRIOV_ENABLED)
-		return true;
-	return false;
-}
-
 int nic_sriov_configure(struct pci_dev *pdev, int num_vfs_requested)
 {
 	struct net_device *netdev = pci_get_drvdata(pdev);
@@ -442,9 +771,9 @@ int nic_sriov_configure(struct pci_dev *pdev, int num_vfs_requested)
 	if (nic->num_vf_en == num_vfs_requested)
 		return num_vfs_requested;
 
-	if (nic_is_sriov_enabled(nic)) {
+	if (nic->flags & NIC_SRIOV_ENABLED) {
 		pci_disable_sriov(pdev);
-		nic_clear_sriov_enable(nic);
+		nic->flags &= ~NIC_SRIOV_ENABLED;
 	}
 
 	nic->num_vf_en = 0;
@@ -452,18 +781,20 @@ int nic_sriov_configure(struct pci_dev *pdev, int num_vfs_requested)
 		return -EPERM;
 
 	if (num_vfs_requested) {
-		if ((err = pci_enable_sriov(pdev, num_vfs_requested))) {
-			dev_err(&pdev->dev, "SRIOV, Failed to enable %d VFs\n", num_vfs_requested);
+		err = pci_enable_sriov(pdev, num_vfs_requested);
+		if (err) {
+			dev_err(&pdev->dev, "SRIOV, Failed to enable %d VFs\n",
+				num_vfs_requested);
 			return err;
 		}
 		nic->num_vf_en = num_vfs_requested;
-		nic_set_sriov_enable(nic);
+		nic->flags |= NIC_SRIOV_ENABLED;
 	}
 
 	return num_vfs_requested;
 }
 
-static int  nic_sriov_init(struct pci_dev *pdev, struct nicpf *nic)
+static int nic_sriov_init(struct pci_dev *pdev, struct nicpf *nic)
 {
 	int    pos = 0;
 
@@ -473,23 +804,69 @@ static int  nic_sriov_init(struct pci_dev *pdev, struct nicpf *nic)
 		return 0;
 	}
 
-	pci_read_config_word(pdev, (pos + PCI_SRIOV_TOTAL_VF), &nic->total_vf_cnt);
-	if (nic->total_vf_cnt < DEFAULT_NUM_VF_ENABLED)
+	pci_read_config_word(pdev, (pos + PCI_SRIOV_TOTAL_VF),
+			     &nic->total_vf_cnt);
+	if (nic->total_vf_cnt < nic->num_vf_en)
 		nic->num_vf_en = nic->total_vf_cnt;
-	else
-		nic->num_vf_en = DEFAULT_NUM_VF_ENABLED;
 
 	if (nic->total_vf_cnt && pci_enable_sriov(pdev, nic->num_vf_en)) {
-		dev_err(&pdev->dev, "SRIOV enable failed, num VF is %d\n", nic->num_vf_en);
+		dev_err(&pdev->dev, "SRIOV enable failed, num VF is %d\n",
+			nic->num_vf_en);
 		nic->num_vf_en = 0;
 		return 0;
 	}
-	dev_info(&pdev->dev, "SRIOV enabled, numer of VF available %d\n", nic->num_vf_en);
+	dev_info(&pdev->dev, "SRIOV enabled, numer of VF available %d\n",
+		 nic->num_vf_en);
 
-	nic_set_sriov_enable(nic);
+	nic->flags |= NIC_SRIOV_ENABLED;
 	return 1;
 }
 
+/* Poll for BGX LMAC link status and update corresponding VF
+ * if there is a change, valid only if internal L2 switch
+ * is not present otherwise VF link is always treated as up
+ */
+static void nic_poll_for_link(struct work_struct *work)
+{
+	struct nic_mbx mbx = {};
+	struct nicpf *nic;
+	struct bgx_link_status link;
+	u8 vf, bgx, lmac;
+
+	nic = container_of(work, struct nicpf, dwork.work);
+
+	mbx.msg = NIC_MBOX_MSG_BGX_LINK_CHANGE;
+
+	for (vf = 0; vf < nic->num_vf_en; vf++) {
+		/* Poll only if VF is UP */
+		if (!nic->vf_enabled[vf])
+			continue;
+
+		/* Get BGX, LMAC indices for the VF */
+		bgx = NIC_GET_BGX_FROM_VF_LMAC_MAP(nic->vf_lmac_map[vf]);
+		lmac = NIC_GET_LMAC_FROM_VF_LMAC_MAP(nic->vf_lmac_map[vf]);
+		/* Get interface link status */
+		bgx_get_lmac_link_state(nic->node, bgx, lmac, &link);
+
+		/* Inform VF only if link status changed */
+		if (nic->link[vf] == link.link_up)
+			continue;
+
+		if (!nic->mbx_lock[vf]) {
+			nic->link[vf] = link.link_up;
+			nic->duplex[vf] = link.duplex;
+			nic->speed[vf] = link.speed;
+
+			/* Send a mbox message to VF with current link status */
+			mbx.data.link_status.link_up = link.link_up;
+			mbx.data.link_status.duplex = link.duplex;
+			mbx.data.link_status.speed = link.speed;
+			nic_send_msg_to_vf(nic, vf, &mbx);
+		}
+	}
+	queue_delayed_work(nic->check_link, &nic->dwork, HZ * 2);
+}
+
 static int nic_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 {
 	struct device *dev = &pdev->dev;
@@ -522,36 +899,61 @@ static int nic_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 	}
 
 	err = pci_set_dma_mask(pdev, DMA_BIT_MASK(48));
-	if (!err) {
-		err = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(48));
-		if (err) {
-			dev_err(dev, "unable to get 40-bit DMA for consistent allocations\n");
-			goto err_release_regions;
-		}
-	} else {
+	if (err) {
 		dev_err(dev, "Unable to get usable DMA configuration\n");
 		goto err_release_regions;
 	}
 
+	err = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(48));
+	if (err) {
+		dev_err(dev, "unable to get 48-bit DMA for consistent allocations\n");
+		goto err_release_regions;
+	}
+
 	/* MAP PF's configuration registers */
-	nic->reg_base = (uint64_t)pci_ioremap_bar(pdev, PCI_CFG_REG_BAR_NUM);
+	nic->reg_base = (u64)pci_ioremap_bar(pdev, PCI_CFG_REG_BAR_NUM);
 	if (!nic->reg_base) {
 		dev_err(dev, "Cannot map config register space, aborting\n");
 		err = -ENOMEM;
 		goto err_release_regions;
 	}
 
+	pci_read_config_byte(pdev, PCI_REVISION_ID, &nic->rev_id);
+
+	nic->node = NIC_NODE_ID(pci_resource_start(pdev, PCI_CFG_REG_BAR_NUM));
+
+	/* By default set NIC in TNS bypass mode */
+	nic->flags &= ~NIC_TNS_ENABLED;
+	nic_set_lmac_vf_mapping(nic);
+
 	/* Initialize hardware */
 	nic_init_hw(nic);
 
-	/* Configure SRIOV */
-	if (!nic_sriov_init(pdev, nic))
-		goto err_unmap_resources;
+	/* Set RSS TBL size for each VF */
+	nic->rss_ind_tbl_size = NIC_MAX_RSS_IDR_TBL_SIZE;
 
 	/* Register interrupts */
 	if (nic_register_interrupts(nic))
 		goto err_unmap_resources;
 
+	/* Configure SRIOV */
+	if (!nic_sriov_init(pdev, nic))
+		goto err_unmap_resources;
+
+	if (!nic_get_mac_addresses(nic))
+		dev_info(&pdev->dev, " Mac node not present in dts\n");
+
+	if (nic->flags & NIC_TNS_ENABLED)
+		goto exit;
+
+	/* Register a physical link status poll fn() */
+	nic->check_link = alloc_workqueue("check_link_status",
+					  WQ_UNBOUND | WQ_MEM_RECLAIM, 1);
+	if (!nic->check_link)
+		return -ENOMEM;
+	INIT_DELAYED_WORK(&nic->dwork, nic_poll_for_link);
+	queue_delayed_work(nic->check_link, &nic->dwork, 0);
+
 	goto exit;
 
 err_unmap_resources:
@@ -577,7 +979,7 @@ static void nic_remove(struct pci_dev *pdev)
 
 	nic_unregister_interrupts(nic);
 
-	if (nic_is_sriov_enabled(nic))
+	if (nic->flags & NIC_SRIOV_ENABLED)
 		pci_disable_sriov(pdev);
 
 	pci_set_drvdata(pdev, NULL);
@@ -612,4 +1014,3 @@ static void __exit nic_cleanup_module(void)
 
 module_init(nic_init_module);
 module_exit(nic_cleanup_module);
-
diff --git a/drivers/net/ethernet/cavium/thunder/nic_reg.h b/drivers/net/ethernet/cavium/thunder/nic_reg.h
index 46bcd412..c214a4c 100644
--- a/drivers/net/ethernet/cavium/thunder/nic_reg.h
+++ b/drivers/net/ethernet/cavium/thunder/nic_reg.h
@@ -1,14 +1,18 @@
 /*
- * This file is subject to the terms and conditions of the GNU General Public
- * License.  See the file "COPYING" in the main directory of this archive
- * for more details.
+ * Copyright (C) 2015 Cavium, Inc.
  *
- * Copyright (C) 2013 Cavium, Inc.
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of
+ * the License, or (at your option) any later version.
  */
 
 #ifndef NIC_REG_H
 #define NIC_REG_H
 
+#define   NIC_PF_REG_COUNT			29573
+#define   NIC_VF_REG_COUNT			249
+
 /* Physical function register offsets */
 #define   NIC_PF_CFG				(0x0000)
 #define   NIC_PF_STATUS				(0x0010)
@@ -23,21 +27,65 @@
 #define   NIC_PF_CNM_STATUS			(0x00B0)
 #define   NIC_PF_CQ_AVG_CFG			(0x00C0)
 #define   NIC_PF_RRM_AVG_CFG			(0x00C8)
-#define   NIC_PF_INTF_0_1_SEND_CFG		(0x0200) /* (0..1) << 8 */
+#define   NIC_PF_INTF_0_1_SEND_CFG		(0x0200)
 #define   NIC_PF_INTF_0_1_BP_CFG		(0x0208)
-#define   NIC_PF_INTF_0_1_BP_DIS_0_1		(0x0210) /* (0..1) << 8 + (0..1) << 3 */
+#define   NIC_PF_INTF_0_1_BP_DIS_0_1		(0x0210)
 #define   NIC_PF_INTF_0_1_BP_SW_0_1		(0x0220)
-#define   NIC_PF_ECC_INT			(0x0400)
-#define   NIC_PF_MAILBOX_INT			(0x0410) /* (0..1) << 3 */
-#define   NIC_PF_ECC_INT_W1S			(0x0420)
+#define   NIC_PF_RBDR_BP_STATE_0_3		(0x0240)
+#define   NIC_PF_MAILBOX_INT			(0x0410)
 #define   NIC_PF_MAILBOX_INT_W1S		(0x0430)
-#define   NIC_PF_ECC_ENA_W1C			(0x0440)
 #define   NIC_PF_MAILBOX_ENA_W1C		(0x0450)
-#define   NIC_PF_ECC_ENA_W1S			(0x0460)
 #define   NIC_PF_MAILBOX_ENA_W1S		(0x0470)
-#define   NIC_PF_ECC_CTL			(0x0480)
-#define   NIC_PF_RX_ETYPE_0_7			(0x0500) /* + (0..7) << 3 */
+#define   NIC_PF_RX_ETYPE_0_7			(0x0500)
 #define   NIC_PF_PKIND_0_15_CFG			(0x0600)
+#define   NIC_PF_ECC0_FLIP0			(0x1000)
+#define   NIC_PF_ECC1_FLIP0			(0x1008)
+#define   NIC_PF_ECC2_FLIP0			(0x1010)
+#define   NIC_PF_ECC3_FLIP0			(0x1018)
+#define   NIC_PF_ECC0_FLIP1			(0x1080)
+#define   NIC_PF_ECC1_FLIP1			(0x1088)
+#define   NIC_PF_ECC2_FLIP1			(0x1090)
+#define   NIC_PF_ECC3_FLIP1			(0x1098)
+#define   NIC_PF_ECC0_CDIS			(0x1100)
+#define   NIC_PF_ECC1_CDIS			(0x1108)
+#define   NIC_PF_ECC2_CDIS			(0x1110)
+#define   NIC_PF_ECC3_CDIS			(0x1118)
+#define   NIC_PF_BIST0_STATUS			(0x1280)
+#define   NIC_PF_BIST1_STATUS			(0x1288)
+#define   NIC_PF_BIST2_STATUS			(0x1290)
+#define   NIC_PF_BIST3_STATUS			(0x1298)
+#define   NIC_PF_ECC0_SBE_INT			(0x2000)
+#define   NIC_PF_ECC0_SBE_INT_W1S		(0x2008)
+#define   NIC_PF_ECC0_SBE_ENA_W1C		(0x2010)
+#define   NIC_PF_ECC0_SBE_ENA_W1S		(0x2018)
+#define   NIC_PF_ECC0_DBE_INT			(0x2100)
+#define   NIC_PF_ECC0_DBE_INT_W1S		(0x2108)
+#define   NIC_PF_ECC0_DBE_ENA_W1C		(0x2110)
+#define   NIC_PF_ECC0_DBE_ENA_W1S		(0x2118)
+#define   NIC_PF_ECC1_SBE_INT			(0x2200)
+#define   NIC_PF_ECC1_SBE_INT_W1S		(0x2208)
+#define   NIC_PF_ECC1_SBE_ENA_W1C		(0x2210)
+#define   NIC_PF_ECC1_SBE_ENA_W1S		(0x2218)
+#define   NIC_PF_ECC1_DBE_INT			(0x2300)
+#define   NIC_PF_ECC1_DBE_INT_W1S		(0x2308)
+#define   NIC_PF_ECC1_DBE_ENA_W1C		(0x2310)
+#define   NIC_PF_ECC1_DBE_ENA_W1S		(0x2318)
+#define   NIC_PF_ECC2_SBE_INT			(0x2400)
+#define   NIC_PF_ECC2_SBE_INT_W1S		(0x2408)
+#define   NIC_PF_ECC2_SBE_ENA_W1C		(0x2410)
+#define   NIC_PF_ECC2_SBE_ENA_W1S		(0x2418)
+#define   NIC_PF_ECC2_DBE_INT			(0x2500)
+#define   NIC_PF_ECC2_DBE_INT_W1S		(0x2508)
+#define   NIC_PF_ECC2_DBE_ENA_W1C		(0x2510)
+#define   NIC_PF_ECC2_DBE_ENA_W1S		(0x2518)
+#define   NIC_PF_ECC3_SBE_INT			(0x2600)
+#define   NIC_PF_ECC3_SBE_INT_W1S		(0x2608)
+#define   NIC_PF_ECC3_SBE_ENA_W1C		(0x2610)
+#define   NIC_PF_ECC3_SBE_ENA_W1S		(0x2618)
+#define   NIC_PF_ECC3_DBE_INT			(0x2700)
+#define   NIC_PF_ECC3_DBE_INT_W1S		(0x2708)
+#define   NIC_PF_ECC3_DBE_ENA_W1C		(0x2710)
+#define   NIC_PF_ECC3_DBE_ENA_W1S		(0x2718)
 #define   NIC_PF_CPI_0_2047_CFG			(0x200000)
 #define   NIC_PF_RSSI_0_4097_RQ			(0x220000)
 #define   NIC_PF_LMAC_0_7_CFG			(0x240000)
@@ -48,6 +96,8 @@
 #define   NIC_PF_CHAN_0_255_SW_XOFF		(0x440000)
 #define   NIC_PF_CHAN_0_255_CREDIT		(0x460000)
 #define   NIC_PF_CHAN_0_255_RX_BP_CFG		(0x480000)
+#define   NIC_PF_SW_SYNC_RX			(0x490000)
+#define   NIC_PF_SW_SYNC_RX_DONE		(0x490008)
 #define   NIC_PF_TL2_0_63_CFG			(0x500000)
 #define   NIC_PF_TL2_0_63_PRI			(0x520000)
 #define   NIC_PF_TL2_0_63_SH_STATUS		(0x580000)
@@ -61,37 +111,39 @@
 #define   NIC_PF_TL4A_0_255_CFG			(0x6F0000)
 #define   NIC_PF_TL4_0_1023_CFG			(0x800000)
 #define   NIC_PF_TL4_0_1023_SW_XOFF		(0x820000)
-#define   NIC_PF_TL4_0_1023_SH_STATUS		(0x880000)
+#define   NIC_PF_TL4_0_1023_SH_STATUS		(0x840000)
+#define   NIC_PF_TL4A_0_1023_CNM_RATE		(0x880000)
 #define   NIC_PF_TL4A_0_1023_CNM_STATUS		(0x8A0000)
-#define   NIC_PF_VF_0_127_MAILBOX_0_7		(0x20002000) /* + (0..127) << 21 + (0..7) << 3 */
-#define   NIC_PF_VNIC_0_127_TX_STAT_0_4		(0x20004000) /* + (0..127) << 21 + (0..4) << 3 */
+#define   NIC_PF_VF_0_127_MAILBOX_0_1		(0x20002030)
+#define   NIC_PF_VNIC_0_127_TX_STAT_0_4		(0x20004000)
 #define   NIC_PF_VNIC_0_127_RX_STAT_0_13	(0x20004100)
 #define   NIC_PF_QSET_0_127_LOCK_0_15		(0x20006000)
-#define   NIC_PF_QSET_0_127_CFG			(0x20010000) /* + (0..127) << 21 */
-#define   NIC_PF_QSET_0_127_RQ_0_7_CFG		(0x20010400) /* + (0..127) << 21 + (0..7) << 18 */
+#define   NIC_PF_QSET_0_127_CFG			(0x20010000)
+#define   NIC_PF_QSET_0_127_RQ_0_7_CFG		(0x20010400)
 #define   NIC_PF_QSET_0_127_RQ_0_7_DROP_CFG	(0x20010420)
 #define   NIC_PF_QSET_0_127_RQ_0_7_BP_CFG	(0x20010500)
-#define   NIC_PF_QSET_0_127_RQ_0_7_STAT_0_1	(0x20010600) /* + (0..127) << 21 + (0..7) << 18 + (0..1) << 3 */
-#define   NIC_PF_QSET_0_127_SQ_0_7_CFG		(0x20010C00) /* + (0..127) << 21 + (0..7) << 18 */
+#define   NIC_PF_QSET_0_127_RQ_0_7_STAT_0_1	(0x20010600)
+#define   NIC_PF_QSET_0_127_SQ_0_7_CFG		(0x20010C00)
 #define   NIC_PF_QSET_0_127_SQ_0_7_CFG2		(0x20010C08)
-#define   NIC_PF_QSET_0_127_SQ_0_7_STAT_0_1	(0x20010D00) /* + (0..127) << 21 + (0..7) << 18 + (0..1) << 3 */
+#define   NIC_PF_QSET_0_127_SQ_0_7_STAT_0_1	(0x20010D00)
 
-#define   NIC_PF_MSIX_VEC_0_18_ADDR		(0x000000) /* + (0..18) << 4 */
+#define   NIC_PF_MSIX_VEC_0_18_ADDR		(0x000000)
 #define   NIC_PF_MSIX_VEC_0_CTL			(0x000008)
-#define   NIC_PF_MSIX_PBA_0			(0x010000)
+#define   NIC_PF_MSIX_PBA_0			(0x0F0000)
 
 /* Virtual function register offsets */
 #define   NIC_VNIC_CFG				(0x000020)
-#define   NIC_VF_PF_MAILBOX_0_7			(0x000100) /* + (0..7) << 3 */
+#define   NIC_VF_PF_MAILBOX_0_1			(0x000130)
 #define   NIC_VF_INT				(0x000200)
 #define   NIC_VF_INT_W1S			(0x000220)
 #define   NIC_VF_ENA_W1C			(0x000240)
 #define   NIC_VF_ENA_W1S			(0x000260)
 
 #define   NIC_VNIC_RSS_CFG			(0x0020E0)
-#define   NIC_VNIC_RSS_KEY_0_4			(0x002200) /* + (0..4) << 3*/
-#define   NIC_VNIC_TX_STAT_0_5			(0x004000)
+#define   NIC_VNIC_RSS_KEY_0_4			(0x002200)
+#define   NIC_VNIC_TX_STAT_0_4			(0x004000)
 #define   NIC_VNIC_RX_STAT_0_13			(0x004100)
+#define   NIC_QSET_RQ_GEN_CFG			(0x010010)
 
 #define   NIC_QSET_CQ_0_7_CFG			(0x010400)
 #define   NIC_QSET_CQ_0_7_CFG2			(0x010408)
@@ -104,7 +156,6 @@
 #define   NIC_QSET_CQ_0_7_STATUS2		(0x010448)
 #define   NIC_QSET_CQ_0_7_DEBUG			(0x010450)
 
-#define   NIC_QSET_RQ_GEN_CFG			(0x010010)
 #define   NIC_QSET_RQ_0_7_CFG			(0x010600)
 #define   NIC_QSET_RQ_0_7_STAT_0_1		(0x010700)
 
@@ -127,10 +178,11 @@
 #define   NIC_QSET_RBDR_0_1_DOOR		(0x010C38)
 #define   NIC_QSET_RBDR_0_1_STATUS0		(0x010C40)
 #define   NIC_QSET_RBDR_0_1_STATUS1		(0x010C48)
+#define   NIC_QSET_RBDR_0_1_PREFETCH_STATUS	(0x010C50)
 
 #define   NIC_VF_MSIX_VECTOR_0_19_ADDR		(0x000000)
 #define   NIC_VF_MSIX_VECTOR_0_19_CTL		(0x000008)
-#define   NIC_VF_MSIX_PBA			(0x010000)
+#define   NIC_VF_MSIX_PBA			(0x0F0000)
 
 /* Offsets within registers */
 #define   NIC_MSIX_VEC_SHIFT			4
@@ -138,24 +190,24 @@
 #define   NIC_QS_ID_SHIFT			21
 #define   NIC_VF_NUM_SHIFT			21
 
-/* Port kind configuration */
+/* Port kind configuration register */
 struct pkind_cfg {
 #if defined(__BIG_ENDIAN_BITFIELD)
-	uint64_t reserved_42_63:22;
-	uint64_t hdr_sl:5;	/* Header skip length */
-	uint64_t rx_hdr:3;	/* TNS Receive header present */
-	uint64_t lenerr_en:1;	/* L2 length error check enable */
-	uint64_t reserved_32_32:1;
-	uint64_t maxlen:16;	/* Max frame size */
-	uint64_t minlen:16;	/* Min frame size */
+	u64 reserved_42_63:22;
+	u64 hdr_sl:5;	/* Header skip length */
+	u64 rx_hdr:3;	/* TNS Receive header present */
+	u64 lenerr_en:1;/* L2 length error check enable */
+	u64 reserved_32_32:1;
+	u64 maxlen:16;	/* Max frame size */
+	u64 minlen:16;	/* Min frame size */
 #elif defined(__LITTLE_ENDIAN_BITFIELD)
-	uint64_t minlen:16;
-	uint64_t maxlen:16;
-	uint64_t reserved_32_32:1;
-	uint64_t lenerr_en:1;
-	uint64_t rx_hdr:3;
-	uint64_t hdr_sl:5;
-	uint64_t reserved_42_63:22;
+	u64 minlen:16;
+	u64 maxlen:16;
+	u64 reserved_32_32:1;
+	u64 lenerr_en:1;
+	u64 rx_hdr:3;
+	u64 hdr_sl:5;
+	u64 reserved_42_63:22;
 #endif
 };
 
diff --git a/drivers/net/ethernet/cavium/thunder/nicvf_ethtool.c b/drivers/net/ethernet/cavium/thunder/nicvf_ethtool.c
index bd724c3..653700a 100644
--- a/drivers/net/ethernet/cavium/thunder/nicvf_ethtool.c
+++ b/drivers/net/ethernet/cavium/thunder/nicvf_ethtool.c
@@ -1,10 +1,10 @@
-/****************************************************************************
- * This file is subject to the terms and conditions of the GNU General Public
- * License.  See the file "COPYING" in the main directory of this archive
- * for more details.
+/* Copyright (C) 2015 Cavium, Inc.
  *
- * Copyright (C) 2013 Cavium, Inc.
- ******************************************************************************/
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of
+ * the License, or (at your option) any later version.
+ */
 
 /* ETHTOOL Support for VNIC_VF Device*/
 
@@ -12,6 +12,9 @@
 
 #include "nic_reg.h"
 #include "nic.h"
+#include "nicvf_queues.h"
+#include "q_struct.h"
+#include "thunder_bgx.h"
 
 #define DRV_NAME	"thunder-nicvf"
 #define DRV_VERSION     "1.0"
@@ -21,178 +24,588 @@ struct nicvf_stat {
 	unsigned int index;
 };
 
-#define NICVF_TX_STAT(stat) { \
+#define NICVF_HW_STAT(stat) { \
 	.name = #stat, \
-	.index = offsetof(struct nicvf_tx_stats, stat) / sizeof(u64) \
+	.index = offsetof(struct nicvf_hw_stats, stat) / sizeof(u64), \
 }
 
-#define NICVF_RX_STAT(stat) { \
+#define NICVF_DRV_STAT(stat) { \
 	.name = #stat, \
-	.index = offsetof(struct nicvf_rx_stats, stat) / sizeof(u64) \
-}
-
-static const struct nicvf_stat nicvf_tx_stats[] = {
-	NICVF_TX_STAT(tx_frames_ok),
-	NICVF_TX_STAT(tx_unicast_frames_ok),
-	NICVF_TX_STAT(tx_multicast_frames_ok),
-	NICVF_TX_STAT(tx_broadcast_frames_ok),
-	NICVF_TX_STAT(tx_bytes_ok),
-	NICVF_TX_STAT(tx_unicast_bytes_ok),
-	NICVF_TX_STAT(tx_multicast_bytes_ok),
-	NICVF_TX_STAT(tx_broadcast_bytes_ok),
-	NICVF_TX_STAT(tx_drops),
-	NICVF_TX_STAT(tx_errors),
-	NICVF_TX_STAT(tx_tso),
+	.index = offsetof(struct nicvf_drv_stats, stat) / sizeof(u64), \
+}
+
+static const struct nicvf_stat nicvf_hw_stats[] = {
+	NICVF_HW_STAT(rx_bytes_ok),
+	NICVF_HW_STAT(rx_ucast_frames_ok),
+	NICVF_HW_STAT(rx_bcast_frames_ok),
+	NICVF_HW_STAT(rx_mcast_frames_ok),
+	NICVF_HW_STAT(rx_fcs_errors),
+	NICVF_HW_STAT(rx_l2_errors),
+	NICVF_HW_STAT(rx_drop_red),
+	NICVF_HW_STAT(rx_drop_red_bytes),
+	NICVF_HW_STAT(rx_drop_overrun),
+	NICVF_HW_STAT(rx_drop_overrun_bytes),
+	NICVF_HW_STAT(rx_drop_bcast),
+	NICVF_HW_STAT(rx_drop_mcast),
+	NICVF_HW_STAT(rx_drop_l3_bcast),
+	NICVF_HW_STAT(rx_drop_l3_mcast),
+	NICVF_HW_STAT(tx_bytes_ok),
+	NICVF_HW_STAT(tx_ucast_frames_ok),
+	NICVF_HW_STAT(tx_bcast_frames_ok),
+	NICVF_HW_STAT(tx_mcast_frames_ok),
 };
 
-static const struct nicvf_stat nicvf_rx_stats[] = {
-	NICVF_RX_STAT(rx_frames_ok),
-	NICVF_RX_STAT(rx_frames_total),
-	NICVF_RX_STAT(rx_unicast_frames_ok),
-	NICVF_RX_STAT(rx_multicast_frames_ok),
-	NICVF_RX_STAT(rx_broadcast_frames_ok),
-	NICVF_RX_STAT(rx_bytes_ok),
-	NICVF_RX_STAT(rx_unicast_bytes_ok),
-	NICVF_RX_STAT(rx_multicast_bytes_ok),
-	NICVF_RX_STAT(rx_broadcast_bytes_ok),
-	NICVF_RX_STAT(rx_drop),
-	NICVF_RX_STAT(rx_no_bufs),
-	NICVF_RX_STAT(rx_errors),
-	NICVF_RX_STAT(rx_rss),
-	NICVF_RX_STAT(rx_crc_errors),
-	NICVF_RX_STAT(rx_frames_64),
-	NICVF_RX_STAT(rx_frames_127),
-	NICVF_RX_STAT(rx_frames_255),
-	NICVF_RX_STAT(rx_frames_511),
-	NICVF_RX_STAT(rx_frames_1023),
-	NICVF_RX_STAT(rx_frames_1518),
-	NICVF_RX_STAT(rx_frames_jumbo),
+static const struct nicvf_stat nicvf_drv_stats[] = {
+	NICVF_DRV_STAT(rx_frames_ok),
+	NICVF_DRV_STAT(rx_frames_64),
+	NICVF_DRV_STAT(rx_frames_127),
+	NICVF_DRV_STAT(rx_frames_255),
+	NICVF_DRV_STAT(rx_frames_511),
+	NICVF_DRV_STAT(rx_frames_1023),
+	NICVF_DRV_STAT(rx_frames_1518),
+	NICVF_DRV_STAT(rx_frames_jumbo),
+	NICVF_DRV_STAT(rx_drops),
+	NICVF_DRV_STAT(tx_frames_ok),
+	NICVF_DRV_STAT(tx_busy),
+	NICVF_DRV_STAT(tx_tso),
+	NICVF_DRV_STAT(tx_drops),
 };
 
-static const unsigned int nicvf_n_tx_stats = ARRAY_SIZE(nicvf_tx_stats);
-static const unsigned int nicvf_n_rx_stats = ARRAY_SIZE(nicvf_rx_stats);
+static const struct nicvf_stat nicvf_queue_stats[] = {
+	{ "bytes", 0 },
+	{ "frames", 1 },
+};
+
+static const unsigned int nicvf_n_hw_stats = ARRAY_SIZE(nicvf_hw_stats);
+static const unsigned int nicvf_n_drv_stats = ARRAY_SIZE(nicvf_drv_stats);
+static const unsigned int nicvf_n_queue_stats = ARRAY_SIZE(nicvf_queue_stats);
 
 static int nicvf_get_settings(struct net_device *netdev,
-			     struct ethtool_cmd *cmd)
+			      struct ethtool_cmd *cmd)
 {
-	cmd->supported = (SUPPORTED_1000baseT_Full |
-			SUPPORTED_100baseT_Full |
-			SUPPORTED_10baseT_Full |
-			SUPPORTED_10000baseT_Full | SUPPORTED_FIBRE);
-
-	cmd->advertising = (ADVERTISED_1000baseT_Full |
-			ADVERTISED_100baseT_Full |
-			ADVERTISED_10baseT_Full |
-			ADVERTISED_10000baseT_Full | ADVERTISED_FIBRE);
+	struct nicvf *nic = netdev_priv(netdev);
 
-	cmd->port = PORT_FIBRE;
+	cmd->supported = 0;
 	cmd->transceiver = XCVR_EXTERNAL;
-	if (netif_carrier_ok(netdev)) {
-		ethtool_cmd_speed_set(cmd, SPEED_10000);
-		cmd->duplex = DUPLEX_FULL;
+	if (nic->speed <= 1000) {
+		cmd->port = PORT_MII;
+		cmd->autoneg = AUTONEG_ENABLE;
 	} else {
-		ethtool_cmd_speed_set(cmd, -1);
-		cmd->duplex = -1;
+		cmd->port = PORT_FIBRE;
+		cmd->autoneg = AUTONEG_DISABLE;
 	}
+	cmd->duplex = nic->duplex;
+	ethtool_cmd_speed_set(cmd, nic->speed);
 
-	cmd->autoneg = AUTONEG_DISABLE;
-	ethtool_cmd_speed_set(cmd, SPEED_1000);
 	return 0;
 }
 
-static int nicvf_set_settings(struct net_device *netdev,
-			     struct ethtool_cmd *cmd)
+static void nicvf_get_drvinfo(struct net_device *netdev,
+			      struct ethtool_drvinfo *info)
 {
-	return -EOPNOTSUPP;
-
-	/* 10G full duplex setting supported only */
-	if (cmd->autoneg == AUTONEG_ENABLE)
-		return -EOPNOTSUPP;
+	struct nicvf *nic = netdev_priv(netdev);
 
-	if (ethtool_cmd_speed(cmd) != SPEED_10000)
-		return -EOPNOTSUPP;
+	strlcpy(info->driver, DRV_NAME, sizeof(info->driver));
+	strlcpy(info->version, DRV_VERSION, sizeof(info->version));
+	strlcpy(info->bus_info, pci_name(nic->pdev), sizeof(info->bus_info));
+}
 
-	if (cmd->duplex != DUPLEX_FULL)
-		return -EOPNOTSUPP;
+static u32 nicvf_get_msglevel(struct net_device *netdev)
+{
+	struct nicvf *nic = netdev_priv(netdev);
 
-	return 0;
+	return nic->msg_enable;
 }
 
-static void nicvf_get_drvinfo(struct net_device *netdev,
-			     struct ethtool_drvinfo *info)
+static void nicvf_set_msglevel(struct net_device *netdev, u32 lvl)
 {
 	struct nicvf *nic = netdev_priv(netdev);
 
-	strlcpy(info->driver, DRV_NAME, sizeof(info->driver));
-	strlcpy(info->version, DRV_VERSION, sizeof(info->version));
-	strlcpy(info->bus_info, pci_name(nic->pdev), sizeof(info->bus_info));
+	nic->msg_enable = lvl;
 }
 
-static void nicvf_get_strings(struct net_device *netdev, u32 stringset,
-			     u8 *data)
+static void nicvf_get_strings(struct net_device *netdev, u32 sset, u8 *data)
 {
-	int stats;
+	int stats, qidx;
+
+	if (sset != ETH_SS_STATS)
+		return;
+
+	for (stats = 0; stats < nicvf_n_hw_stats; stats++) {
+		memcpy(data, nicvf_hw_stats[stats].name, ETH_GSTRING_LEN);
+		data += ETH_GSTRING_LEN;
+	}
+
+	for (stats = 0; stats < nicvf_n_drv_stats; stats++) {
+		memcpy(data, nicvf_drv_stats[stats].name, ETH_GSTRING_LEN);
+		data += ETH_GSTRING_LEN;
+	}
 
-	for (stats = 0; stats < nicvf_n_tx_stats; stats++) {
-		memcpy(data, nicvf_tx_stats[stats].name, ETH_GSTRING_LEN);
+	for (qidx = 0; qidx < MAX_RCV_QUEUES_PER_QS; qidx++) {
+		for (stats = 0; stats < nicvf_n_queue_stats; stats++) {
+			sprintf(data, "rxq%d: %s", qidx,
+				nicvf_queue_stats[stats].name);
+			data += ETH_GSTRING_LEN;
+		}
+	}
+
+	for (qidx = 0; qidx < MAX_SND_QUEUES_PER_QS; qidx++) {
+		for (stats = 0; stats < nicvf_n_queue_stats; stats++) {
+			sprintf(data, "txq%d: %s", qidx,
+				nicvf_queue_stats[stats].name);
+			data += ETH_GSTRING_LEN;
+		}
+	}
+
+	for (stats = 0; stats < BGX_RX_STATS_COUNT; stats++) {
+		sprintf(data, "bgx_rxstat%d: ", stats);
 		data += ETH_GSTRING_LEN;
 	}
-	for (stats = 0; stats < nicvf_n_rx_stats; stats++) {
-		memcpy(data, nicvf_rx_stats[stats].name, ETH_GSTRING_LEN);
+
+	for (stats = 0; stats < BGX_TX_STATS_COUNT; stats++) {
+		sprintf(data, "bgx_txstat%d: ", stats);
 		data += ETH_GSTRING_LEN;
 	}
 }
 
 static int nicvf_get_sset_count(struct net_device *netdev, int sset)
 {
-	return nicvf_n_tx_stats + nicvf_n_rx_stats;
+	if (sset != ETH_SS_STATS)
+		return -EINVAL;
+
+	return nicvf_n_hw_stats + nicvf_n_drv_stats +
+		(nicvf_n_queue_stats *
+		 (MAX_RCV_QUEUES_PER_QS + MAX_SND_QUEUES_PER_QS)) +
+		BGX_RX_STATS_COUNT + BGX_TX_STATS_COUNT;
 }
 
 static void nicvf_get_ethtool_stats(struct net_device *netdev,
-				   struct ethtool_stats *stats, u64 *data)
+				    struct ethtool_stats *stats, u64 *data)
+{
+	struct nicvf *nic = netdev_priv(netdev);
+	int stat, qidx;
+
+	nicvf_update_stats(nic);
+
+	/* Update LMAC stats */
+	nicvf_update_lmac_stats(nic);
+
+	for (stat = 0; stat < nicvf_n_hw_stats; stat++)
+		*(data++) = ((u64 *)&nic->stats)
+				[nicvf_hw_stats[stat].index];
+	for (stat = 0; stat < nicvf_n_drv_stats; stat++)
+		*(data++) = ((u64 *)&nic->drv_stats)
+				[nicvf_drv_stats[stat].index];
+
+	for (qidx = 0; qidx < MAX_RCV_QUEUES_PER_QS; qidx++) {
+		for (stat = 0; stat < nicvf_n_queue_stats; stat++)
+			*(data++) = ((u64 *)&nic->qs->rq[qidx].stats)
+					[nicvf_queue_stats[stat].index];
+	}
+
+	for (qidx = 0; qidx < MAX_SND_QUEUES_PER_QS; qidx++) {
+		for (stat = 0; stat < nicvf_n_queue_stats; stat++)
+			*(data++) = ((u64 *)&nic->qs->sq[qidx].stats)
+					[nicvf_queue_stats[stat].index];
+	}
+
+	for (stat = 0; stat < BGX_RX_STATS_COUNT; stat++)
+		*(data++) = nic->bgx_stats.rx_stats[stat];
+	for (stat = 0; stat < BGX_TX_STATS_COUNT; stat++)
+		*(data++) = nic->bgx_stats.tx_stats[stat];
+}
+
+static int nicvf_get_regs_len(struct net_device *dev)
+{
+	return sizeof(u64) * NIC_VF_REG_COUNT;
+}
+
+static void nicvf_get_regs(struct net_device *dev,
+			   struct ethtool_regs *regs, void *reg)
+{
+	struct nicvf *nic = netdev_priv(dev);
+	u64 *p = (u64 *)reg;
+	u64 reg_offset;
+	int mbox, key, stat, q;
+	int i = 0;
+
+	regs->version = 0;
+	memset(p, 0, NIC_VF_REG_COUNT);
+
+	p[i++] = nicvf_reg_read(nic, NIC_VNIC_CFG);
+	/* Mailbox registers */
+	for (mbox = 0; mbox < NIC_PF_VF_MAILBOX_SIZE; mbox++)
+		p[i++] = nicvf_reg_read(nic,
+					NIC_VF_PF_MAILBOX_0_1 | (mbox << 3));
+
+	p[i++] = nicvf_reg_read(nic, NIC_VF_INT);
+	p[i++] = nicvf_reg_read(nic, NIC_VF_INT_W1S);
+	p[i++] = nicvf_reg_read(nic, NIC_VF_ENA_W1C);
+	p[i++] = nicvf_reg_read(nic, NIC_VF_ENA_W1S);
+	p[i++] = nicvf_reg_read(nic, NIC_VNIC_RSS_CFG);
+
+	for (key = 0; key < RSS_HASH_KEY_SIZE; key++)
+		p[i++] = nicvf_reg_read(nic, NIC_VNIC_RSS_KEY_0_4 | (key << 3));
+
+	/* Tx/Rx statistics */
+	for (stat = 0; stat < TX_STATS_ENUM_LAST; stat++)
+		p[i++] = nicvf_reg_read(nic,
+					NIC_VNIC_TX_STAT_0_4 | (stat << 3));
+
+	for (i = 0; i < RX_STATS_ENUM_LAST; i++)
+		p[i++] = nicvf_reg_read(nic,
+					NIC_VNIC_RX_STAT_0_13 | (stat << 3));
+
+	p[i++] = nicvf_reg_read(nic, NIC_QSET_RQ_GEN_CFG);
+
+	/* All completion queue's registers */
+	for (q = 0; q < MAX_CMP_QUEUES_PER_QS; q++) {
+		p[i++] = nicvf_queue_reg_read(nic, NIC_QSET_CQ_0_7_CFG, q);
+		p[i++] = nicvf_queue_reg_read(nic, NIC_QSET_CQ_0_7_CFG2, q);
+		p[i++] = nicvf_queue_reg_read(nic, NIC_QSET_CQ_0_7_THRESH, q);
+		p[i++] = nicvf_queue_reg_read(nic, NIC_QSET_CQ_0_7_BASE, q);
+		p[i++] = nicvf_queue_reg_read(nic, NIC_QSET_CQ_0_7_HEAD, q);
+		p[i++] = nicvf_queue_reg_read(nic, NIC_QSET_CQ_0_7_TAIL, q);
+		p[i++] = nicvf_queue_reg_read(nic, NIC_QSET_CQ_0_7_DOOR, q);
+		p[i++] = nicvf_queue_reg_read(nic, NIC_QSET_CQ_0_7_STATUS, q);
+		p[i++] = nicvf_queue_reg_read(nic, NIC_QSET_CQ_0_7_STATUS2, q);
+		p[i++] = nicvf_queue_reg_read(nic, NIC_QSET_CQ_0_7_DEBUG, q);
+	}
+
+	/* All receive queue's registers */
+	for (q = 0; q < MAX_RCV_QUEUES_PER_QS; q++) {
+		p[i++] = nicvf_queue_reg_read(nic, NIC_QSET_RQ_0_7_CFG, q);
+		p[i++] = nicvf_queue_reg_read(nic,
+						  NIC_QSET_RQ_0_7_STAT_0_1, q);
+		reg_offset = NIC_QSET_RQ_0_7_STAT_0_1 | (1 << 3);
+		p[i++] = nicvf_queue_reg_read(nic, reg_offset, q);
+	}
+
+	for (q = 0; q < MAX_SND_QUEUES_PER_QS; q++) {
+		p[i++] = nicvf_queue_reg_read(nic, NIC_QSET_SQ_0_7_CFG, q);
+		p[i++] = nicvf_queue_reg_read(nic, NIC_QSET_SQ_0_7_THRESH, q);
+		p[i++] = nicvf_queue_reg_read(nic, NIC_QSET_SQ_0_7_BASE, q);
+		p[i++] = nicvf_queue_reg_read(nic, NIC_QSET_SQ_0_7_HEAD, q);
+		p[i++] = nicvf_queue_reg_read(nic, NIC_QSET_SQ_0_7_TAIL, q);
+		p[i++] = nicvf_queue_reg_read(nic, NIC_QSET_SQ_0_7_DOOR, q);
+		p[i++] = nicvf_queue_reg_read(nic, NIC_QSET_SQ_0_7_STATUS, q);
+		p[i++] = nicvf_queue_reg_read(nic, NIC_QSET_SQ_0_7_DEBUG, q);
+		p[i++] = nicvf_queue_reg_read(nic, NIC_QSET_SQ_0_7_CNM_CHG, q);
+		p[i++] = nicvf_queue_reg_read(nic, NIC_QSET_SQ_0_7_STAT_0_1, q);
+		reg_offset = NIC_QSET_SQ_0_7_STAT_0_1 | (1 << 3);
+		p[i++] = nicvf_queue_reg_read(nic, reg_offset, q);
+	}
+
+	for (q = 0; q < MAX_RCV_BUF_DESC_RINGS_PER_QS; q++) {
+		p[i++] = nicvf_queue_reg_read(nic, NIC_QSET_RBDR_0_1_CFG, q);
+		p[i++] = nicvf_queue_reg_read(nic, NIC_QSET_RBDR_0_1_THRESH, q);
+		p[i++] = nicvf_queue_reg_read(nic, NIC_QSET_RBDR_0_1_BASE, q);
+		p[i++] = nicvf_queue_reg_read(nic, NIC_QSET_RBDR_0_1_HEAD, q);
+		p[i++] = nicvf_queue_reg_read(nic, NIC_QSET_RBDR_0_1_TAIL, q);
+		p[i++] = nicvf_queue_reg_read(nic, NIC_QSET_RBDR_0_1_DOOR, q);
+		p[i++] = nicvf_queue_reg_read(nic,
+					      NIC_QSET_RBDR_0_1_STATUS0, q);
+		p[i++] = nicvf_queue_reg_read(nic,
+					      NIC_QSET_RBDR_0_1_STATUS1, q);
+		reg_offset = NIC_QSET_RBDR_0_1_PREFETCH_STATUS;
+		p[i++] = nicvf_queue_reg_read(nic, reg_offset, q);
+	}
+}
+
+static int nicvf_get_coalesce(struct net_device *netdev,
+			      struct ethtool_coalesce *cmd)
+{
+	struct nicvf *nic = netdev_priv(netdev);
+
+	cmd->rx_coalesce_usecs = nic->cq_coalesce_usecs;
+	return 0;
+}
+
+static int nicvf_set_coalesce(struct net_device *netdev,
+			      struct ethtool_coalesce *cmd)
+{
+	struct nicvf *nic = netdev_priv(netdev);
+	struct queue_set *qs = nic->qs;
+	int qidx;
+
+	nic->cq_coalesce_usecs = cmd->rx_coalesce_usecs;
+	for (qidx = 0; qidx < qs->cq_cnt; qidx++)
+		nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_CFG2,
+				      qidx, nic->cq_coalesce_usecs);
+	return 0;
+}
+
+static void nicvf_get_ringparam(struct net_device *netdev,
+				struct ethtool_ringparam *ring)
 {
 	struct nicvf *nic = netdev_priv(netdev);
-	struct eth_stats vstats = nic->vstats;
-	int stat;
+	struct queue_set *qs = nic->qs;
+
+	ring->rx_max_pending = MAX_RCV_BUF_COUNT;
+	ring->rx_pending = qs->rbdr_len;
+	ring->tx_max_pending = MAX_SND_QUEUE_LEN;
+	ring->tx_pending = qs->sq_len;
+}
+
+#ifdef VNIC_RSS_SUPPORT
+static int nicvf_get_rss_hash_opts(struct nicvf *nic,
+				   struct ethtool_rxnfc *info)
+{
+	info->data = 0;
+
+	switch (info->flow_type) {
+	case TCP_V4_FLOW:
+	case TCP_V6_FLOW:
+	case UDP_V4_FLOW:
+	case UDP_V6_FLOW:
+	case SCTP_V4_FLOW:
+	case SCTP_V6_FLOW:
+		info->data |= RXH_L4_B_0_1 | RXH_L4_B_2_3;
+	case IPV4_FLOW:
+	case IPV6_FLOW:
+		info->data |= RXH_IP_SRC | RXH_IP_DST;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int nicvf_get_rxnfc(struct net_device *dev,
+			   struct ethtool_rxnfc *info, u32 *rules)
+{
+	struct nicvf *nic = netdev_priv(dev);
+	int ret = -EOPNOTSUPP;
+
+	switch (info->cmd) {
+	case ETHTOOL_GRXRINGS:
+		info->data = nic->qs->rq_cnt;
+		ret = 0;
+		break;
+	case ETHTOOL_GRXFH:
+		return nicvf_get_rss_hash_opts(nic, info);
+	default:
+		break;
+	}
+	return ret;
+}
+
+static int nicvf_set_rss_hash_opts(struct nicvf *nic,
+				   struct ethtool_rxnfc *info)
+{
+	struct nicvf_rss_info *rss = &nic->rss_info;
+	u64 rss_cfg = nicvf_reg_read(nic, NIC_VNIC_RSS_CFG);
+
+	if (!rss->enable)
+		netdev_err(nic->netdev,
+			   "RSS is disabled, hash cannot be set\n");
+
+	netdev_info(nic->netdev, "Set RSS flow type = %d, data = %lld\n",
+		    info->flow_type, info->data);
+
+	if (!(info->data & RXH_IP_SRC) || !(info->data & RXH_IP_DST))
+		return -EINVAL;
+
+	switch (info->flow_type) {
+	case TCP_V4_FLOW:
+	case TCP_V6_FLOW:
+		switch (info->data & (RXH_L4_B_0_1 | RXH_L4_B_2_3)) {
+		case 0:
+			rss_cfg &= ~(1ULL << RSS_HASH_TCP);
+			break;
+		case (RXH_L4_B_0_1 | RXH_L4_B_2_3):
+			rss_cfg |= (1ULL << RSS_HASH_TCP);
+			break;
+		default:
+			return -EINVAL;
+		}
+		break;
+	case UDP_V4_FLOW:
+	case UDP_V6_FLOW:
+		switch (info->data & (RXH_L4_B_0_1 | RXH_L4_B_2_3)) {
+		case 0:
+			rss_cfg &= ~(1ULL << RSS_HASH_UDP);
+			break;
+		case (RXH_L4_B_0_1 | RXH_L4_B_2_3):
+			rss_cfg |= (1ULL << RSS_HASH_UDP);
+			break;
+		default:
+			return -EINVAL;
+		}
+		break;
+	case SCTP_V4_FLOW:
+	case SCTP_V6_FLOW:
+		switch (info->data & (RXH_L4_B_0_1 | RXH_L4_B_2_3)) {
+		case 0:
+			rss_cfg &= ~(1ULL << RSS_HASH_L4ETC);
+			break;
+		case (RXH_L4_B_0_1 | RXH_L4_B_2_3):
+			rss_cfg |= (1ULL << RSS_HASH_L4ETC);
+			break;
+		default:
+			return -EINVAL;
+		}
+		break;
+	case IPV4_FLOW:
+	case IPV6_FLOW:
+		rss_cfg = RSS_HASH_IP;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	nicvf_reg_write(nic, NIC_VNIC_RSS_CFG, rss_cfg);
+	return 0;
+}
+
+static int nicvf_set_rxnfc(struct net_device *dev, struct ethtool_rxnfc *info)
+{
+	struct nicvf *nic = netdev_priv(dev);
+
+	switch (info->cmd) {
+	case ETHTOOL_SRXFH:
+		return nicvf_set_rss_hash_opts(nic, info);
+	default:
+		break;
+	}
+	return -EOPNOTSUPP;
+}
+
+static u32 nicvf_get_rxfh_key_size(struct net_device *netdev)
+{
+	return RSS_HASH_KEY_SIZE * sizeof(u64);
+}
+
+static u32 nicvf_get_rxfh_indir_size(struct net_device *dev)
+{
+	struct nicvf *nic = netdev_priv(dev);
+
+	return nic->rss_info.rss_size;
+}
 
-	memset(&vstats, 0, sizeof(struct eth_stats));
+static int nicvf_get_rxfh(struct net_device *dev, u32 *indir, u8 *hkey)
+{
+	struct nicvf *nic = netdev_priv(dev);
+	struct nicvf_rss_info *rss = &nic->rss_info;
+	int idx;
+
+	if (indir) {
+		for (idx = 0; idx < rss->rss_size; idx++)
+			indir[idx] = rss->ind_tbl[idx];
+	}
 
-	nic->vstats.tx.tx_frames_ok = netdev->stats.tx_packets;
-	nic->vstats.tx.tx_bytes_ok = netdev->stats.tx_bytes;
-	nic->vstats.tx.tx_errors = netdev->stats.tx_errors;
-	nic->vstats.tx.tx_drops = netdev->stats.tx_dropped;
-	nic->vstats.rx.rx_frames_ok = netdev->stats.rx_packets;
-	nic->vstats.rx.rx_bytes_ok = netdev->stats.rx_bytes;
-	nic->vstats.rx.rx_errors = netdev->stats.rx_errors;
-	nic->vstats.rx.rx_drop = netdev->stats.rx_dropped;
+	if (hkey)
+		memcpy(hkey, rss->key, RSS_HASH_KEY_SIZE * sizeof(u64));
 
-	for (stat = 0; stat < nicvf_n_tx_stats; stat++)
-		*(data++) = ((u64 *)&nic->vstats.tx)
-				[nicvf_tx_stats[stat].index];
-	for (stat = 0; stat < nicvf_n_rx_stats; stat++)
-		*(data++) = ((u64 *)&nic->vstats.rx)
-				[nicvf_rx_stats[stat].index];
+	return 0;
+}
+
+static int nicvf_set_rxfh(struct net_device *dev, const u32 *indir,
+			  const u8 *hkey)
+{
+	struct nicvf *nic = netdev_priv(dev);
+	struct nicvf_rss_info *rss = &nic->rss_info;
+	int idx;
+
+	if ((nic->qs->rq_cnt <= 1) || (nic->cpi_alg != CPI_ALG_NONE)) {
+		rss->enable = false;
+		rss->hash_bits = 0;
+		return -EIO;
+	}
+
+	rss->enable = true;
+	if (indir) {
+		for (idx = 0; idx < rss->rss_size; idx++)
+			rss->ind_tbl[idx] = indir[idx];
+	}
+
+	if (hkey) {
+		memcpy(rss->key, hkey, RSS_HASH_KEY_SIZE * sizeof(u64));
+		nicvf_set_rss_key(nic);
+	}
+
+	nicvf_config_rss(nic);
+	return 0;
+}
+#endif
+
+/* Get no of queues device supports and current queue count */
+static void nicvf_get_channels(struct net_device *dev,
+			       struct ethtool_channels *channel)
+{
+	struct nicvf *nic = netdev_priv(dev);
+
+	memset(channel, 0, sizeof(*channel));
+
+	channel->max_rx = MAX_RCV_QUEUES_PER_QS;
+	channel->max_tx = MAX_SND_QUEUES_PER_QS;
+
+	channel->rx_count = nic->qs->rq_cnt;
+	channel->tx_count = nic->qs->sq_cnt;
+}
+
+/* Set no of Tx, Rx queues to be used */
+static int nicvf_set_channels(struct net_device *dev,
+			      struct ethtool_channels *channel)
+{
+	struct nicvf *nic = netdev_priv(dev);
+	int err = 0;
+
+	if (!channel->rx_count || !channel->tx_count)
+		return -EINVAL;
+	if (channel->rx_count > MAX_RCV_QUEUES_PER_QS)
+		return -EINVAL;
+	if (channel->tx_count > MAX_SND_QUEUES_PER_QS)
+		return -EINVAL;
+
+	nic->qs->rq_cnt = channel->rx_count;
+	nic->qs->sq_cnt = channel->tx_count;
+	nic->qs->cq_cnt = max(nic->qs->rq_cnt, nic->qs->sq_cnt);
+
+	err = nicvf_set_real_num_queues(dev, nic->qs->sq_cnt, nic->qs->rq_cnt);
+	if (err)
+		return err;
+
+	if (!netif_running(dev))
+		return err;
+
+	nicvf_stop(dev);
+	nicvf_open(dev);
+	netdev_info(dev, "Setting num Tx rings to %d, Rx rings to %d success\n",
+		    nic->qs->sq_cnt, nic->qs->rq_cnt);
+
+	return err;
 }
 
 static const struct ethtool_ops nicvf_ethtool_ops = {
 	.get_settings		= nicvf_get_settings,
-	.set_settings		= nicvf_set_settings,
 	.get_link		= ethtool_op_get_link,
 	.get_drvinfo		= nicvf_get_drvinfo,
+	.get_msglevel		= nicvf_get_msglevel,
+	.set_msglevel		= nicvf_set_msglevel,
 	.get_strings		= nicvf_get_strings,
 	.get_sset_count		= nicvf_get_sset_count,
 	.get_ethtool_stats	= nicvf_get_ethtool_stats,
-	.get_ts_info		= ethtool_op_get_ts_info
-#if 0
+	.get_regs_len		= nicvf_get_regs_len,
+	.get_regs		= nicvf_get_regs,
 	.get_coalesce		= nicvf_get_coalesce,
 	.set_coalesce		= nicvf_set_coalesce,
 	.get_ringparam		= nicvf_get_ringparam,
-	.set_ringparam		= nicvf_set_ringparam,
+#ifdef VNIC_RSS_SUPPORT
+	.get_rxnfc		= nicvf_get_rxnfc,
+	.set_rxnfc		= nicvf_set_rxnfc,
+	.get_rxfh_key_size	= nicvf_get_rxfh_key_size,
+	.get_rxfh_indir_size	= nicvf_get_rxfh_indir_size,
+	.get_rxfh_indir		= nicvf_get_rxfh,
+	.set_rxfh_indir		= nicvf_set_rxfh,
 #endif
+	.get_channels		= nicvf_get_channels,
+	.set_channels		= nicvf_set_channels,
+	.get_ts_info		= ethtool_op_get_ts_info,
 };
 
 void nicvf_set_ethtool_ops(struct net_device *netdev)
 {
 	netdev->ethtool_ops = &nicvf_ethtool_ops;
 }
-
diff --git a/drivers/net/ethernet/cavium/thunder/nicvf_main.c b/drivers/net/ethernet/cavium/thunder/nicvf_main.c
index 8333c94..16f83de 100644
--- a/drivers/net/ethernet/cavium/thunder/nicvf_main.c
+++ b/drivers/net/ethernet/cavium/thunder/nicvf_main.c
@@ -1,123 +1,122 @@
 /*
- * This file is subject to the terms and conditions of the GNU General Public
- * License.  See the file "COPYING" in the main directory of this archive
- * for more details.
+ * Copyright (C) 2015 Cavium, Inc.
  *
- * Copyright (C) 2014 Cavium, Inc.
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of
+ * the License, or (at your option) any later version.
  */
 
 #include <linux/module.h>
-#include <linux/kernel.h>
-#include <linux/string.h>
-#include <linux/errno.h>
-#include <linux/types.h>
-#include <linux/init.h>
 #include <linux/interrupt.h>
-#include <linux/workqueue.h>
 #include <linux/pci.h>
 #include <linux/netdevice.h>
 #include <linux/etherdevice.h>
-#include <linux/if.h>
-#include <linux/if_ether.h>
-#include <linux/if_vlan.h>
 #include <linux/ethtool.h>
-#include <linux/aer.h>
-#include <linux/ip.h>
-#include <net/tcp.h>
+#include <linux/log2.h>
+#include <linux/prefetch.h>
+#include <linux/irq.h>
 
 #include "nic_reg.h"
 #include "nic.h"
 #include "nicvf_queues.h"
+#include "thunder_bgx.h"
 
 #define DRV_NAME	"thunder-nicvf"
 #define DRV_VERSION	"1.0"
 
 /* Supported devices */
-static DEFINE_PCI_DEVICE_TABLE(nicvf_id_table) = {
-	{ PCI_DEVICE(PCI_VENDOR_ID_CAVIUM, PCI_DEVICE_ID_THUNDER_NIC_VF) },
+static const struct pci_device_id nicvf_id_table[] = {
+	{ PCI_DEVICE_SUB(PCI_VENDOR_ID_CAVIUM,
+			 PCI_DEVICE_ID_THUNDER_NIC_VF,
+			 PCI_VENDOR_ID_CAVIUM, 0xA11E) },
+	{ PCI_DEVICE_SUB(PCI_VENDOR_ID_CAVIUM,
+			 PCI_DEVICE_ID_THUNDER_PASS1_NIC_VF,
+			 PCI_VENDOR_ID_CAVIUM, 0xA11E) },
 	{ 0, }  /* end of table */
 };
 
-MODULE_AUTHOR("Cavium Inc");
-MODULE_DESCRIPTION("Cavium Thunder Virtual Function Network Driver");
-MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Sunil Goutham");
+MODULE_DESCRIPTION("Cavium Thunder NIC Virtual Function Driver");
+MODULE_LICENSE("GPL v2");
 MODULE_VERSION(DRV_VERSION);
 MODULE_DEVICE_TABLE(pci, nicvf_id_table);
 
+static int debug = 0x00;
+module_param(debug, int, 0644);
+MODULE_PARM_DESC(debug, "Debug message level bitmap");
+
+static int cpi_alg = CPI_ALG_NONE;
+module_param(cpi_alg, int, S_IRUGO);
+MODULE_PARM_DESC(cpi_alg,
+		 "PFC algorithm (0=none, 1=VLAN, 2=VLAN16, 3=IP Diffserv)");
+#ifdef	VNIC_RSS_SUPPORT
+static int rss_config = RSS_IP_HASH_ENA | RSS_TCP_HASH_ENA | RSS_UDP_HASH_ENA;
+#endif
+
 static int nicvf_enable_msix(struct nicvf *nic);
 static netdev_tx_t nicvf_xmit(struct sk_buff *skb, struct net_device *netdev);
+static void nicvf_read_bgx_stats(struct nicvf *nic, struct bgx_stats_msg *bgx);
 
-static void nicvf_dump_packet(struct sk_buff *skb)
+static void nicvf_dump_packet(struct net_device *netdev, struct sk_buff *skb)
 {
-#ifdef NICVF_DUMP_PACKET
 	int i;
 
+	pr_info("%s: skb 0x%p, len=%d\n",
+		netdev->name, skb, skb->len);
 	for (i = 0; i < skb->len; i++) {
-		if (!(i % 16))
-			pr_cont("\n");
-		pr_debug("%02x ", (u_char)skb->data[i]);
+		if ((i % 16) == 0)
+			pr_info("\n");
+		pr_info(" %02x", ((u8 *)skb->data)[i]);
 	}
-#endif
+	pr_info("\n");
 }
 
-static void nicvf_update_stats(struct nicvf *nic, struct sk_buff *skb)
+static inline void nicvf_set_rx_frame_cnt(struct nicvf *nic,
+					  struct sk_buff *skb)
 {
 	if (skb->len <= 64)
-		atomic64_add(1, (atomic64_t *)&nic->vstats.rx.rx_frames_64);
+		nic->drv_stats.rx_frames_64++;
 	else if ((skb->len > 64) && (skb->len <= 127))
-		atomic64_add(1, (atomic64_t *)&nic->vstats.rx.rx_frames_127);
+		nic->drv_stats.rx_frames_127++;
 	else if ((skb->len > 127) && (skb->len <= 255))
-		atomic64_add(1, (atomic64_t *)&nic->vstats.rx.rx_frames_255);
+		nic->drv_stats.rx_frames_255++;
 	else if ((skb->len > 255) && (skb->len <= 511))
-		atomic64_add(1, (atomic64_t *)&nic->vstats.rx.rx_frames_511);
+		nic->drv_stats.rx_frames_511++;
 	else if ((skb->len > 511) && (skb->len <= 1023))
-		atomic64_add(1, (atomic64_t *)&nic->vstats.rx.rx_frames_1023);
+		nic->drv_stats.rx_frames_1023++;
 	else if ((skb->len > 1023) && (skb->len <= 1518))
-		atomic64_add(1, (atomic64_t *)&nic->vstats.rx.rx_frames_1518);
+		nic->drv_stats.rx_frames_1518++;
 	else if (skb->len > 1518)
-		atomic64_add(1, (atomic64_t *)&nic->vstats.rx.rx_frames_jumbo);
+		nic->drv_stats.rx_frames_jumbo++;
 }
 
 /* Register read/write APIs */
-void nicvf_reg_write(struct nicvf *nic, uint64_t offset, uint64_t val)
+void nicvf_reg_write(struct nicvf *nic, u64 offset, u64 val)
 {
-	uint64_t addr = nic->reg_base + offset;
+	u64 addr = nic->reg_base + offset;
 
 	writeq_relaxed(val, (void *)addr);
 }
 
-uint64_t nicvf_reg_read(struct nicvf *nic, uint64_t offset)
+u64 nicvf_reg_read(struct nicvf *nic, u64 offset)
 {
-	uint64_t addr = nic->reg_base + offset;
+	u64 addr = nic->reg_base + offset;
 
 	return readq_relaxed((void *)addr);
 }
 
-void nicvf_qset_reg_write(struct nicvf *nic, uint64_t offset, uint64_t val)
+void nicvf_queue_reg_write(struct nicvf *nic, u64 offset,
+			   u64 qidx, u64 val)
 {
-	uint64_t addr = nic->reg_base + offset;
-
-	writeq_relaxed(val, (void *)(addr));
-}
-
-uint64_t nicvf_qset_reg_read(struct nicvf *nic, uint64_t offset)
-{
-	uint64_t addr = nic->reg_base + offset;
-
-	return readq_relaxed((void *)(addr));
-}
-
-void nicvf_queue_reg_write(struct nicvf *nic, uint64_t offset,
-				uint64_t qidx, uint64_t val)
-{
-	uint64_t addr = nic->reg_base + offset;
+	u64 addr = nic->reg_base + offset;
 
 	writeq_relaxed(val, (void *)(addr + (qidx << NIC_Q_NUM_SHIFT)));
 }
 
-uint64_t nicvf_queue_reg_read(struct nicvf *nic, uint64_t offset, uint64_t qidx)
+u64 nicvf_queue_reg_read(struct nicvf *nic, u64 offset, u64 qidx)
 {
-	uint64_t addr = nic->reg_base + offset;
+	u64 addr = nic->reg_base + offset;
 
 	return readq_relaxed((void *)(addr + (qidx << NIC_Q_NUM_SHIFT)));
 }
@@ -126,52 +125,35 @@ uint64_t nicvf_queue_reg_read(struct nicvf *nic, uint64_t offset, uint64_t qidx)
 static bool pf_ready_to_rcv_msg;
 static bool pf_acked;
 static bool pf_nacked;
-
-struct nic_mbx *nicvf_get_mbx(void)
-{
-	struct nic_mbx *mbx = kzalloc(sizeof(*mbx), GFP_KERNEL);
-
-	return mbx;
-}
-
-static void nicvf_enable_mbx_intr(struct nicvf *nic)
-{
-	nicvf_enable_intr(nic, NICVF_INTR_MBOX, 0);
-}
-
-static void nicvf_disable_mbx_intr(struct nicvf *nic)
-{
-	nicvf_disable_intr(nic, NICVF_INTR_MBOX, 0);
-}
+static bool bgx_stats_acked;
 
 int nicvf_send_msg_to_pf(struct nicvf *nic, struct nic_mbx *mbx)
 {
-	int i, timeout = 5000, sleep = 10;
-	uint64_t *msg;
-	uint64_t mbx_addr;
+	int timeout = NIC_MBOX_MSG_TIMEOUT;
+	int sleep = 10;
+	u64 *msg;
+	u64 mbx_addr;
 
 	pf_acked = false;
 	pf_nacked = false;
-	mbx->mbx_trigger_intr = 1;
-	msg = (uint64_t *)mbx;
-	mbx_addr = nic->reg_base + NIC_VF_PF_MAILBOX_0_7;
+	msg = (u64 *)mbx;
+	mbx_addr = nic->reg_base + NIC_VF_PF_MAILBOX_0_1;
 
-	for (i = 0; i < NIC_PF_VF_MAILBOX_SIZE; i++)
-		writeq_relaxed(*(msg + i), (void *)(mbx_addr + (i * 8)));
+	writeq_relaxed(*(msg), (void *)mbx_addr);
+	writeq_relaxed(*(msg + 1), (void *)(mbx_addr + 8));
 
-	/* Wait for previous message to be acked, timeout 5sec */
+	/* Wait for previous message to be acked, timeout 2sec */
 	while (!pf_acked) {
 		if (pf_nacked)
 			return -EINVAL;
 		msleep(sleep);
 		if (pf_acked)
 			break;
-		else
-			timeout -= sleep;
+		timeout -= sleep;
 		if (!timeout) {
 			netdev_err(nic->netdev,
-				"PF didn't ack to mailbox msg %lld from VF%d\n",
-						(mbx->msg & 0xFF), nic->vnic_id);
+				   "PF didn't ack to mbox msg %d from VF%d\n",
+				   (mbx->msg & 0xFF), nic->vf_id);
 			return -EBUSY;
 		}
 	}
@@ -184,11 +166,11 @@ int nicvf_send_msg_to_pf(struct nicvf *nic, struct nic_mbx *mbx)
 static int nicvf_check_pf_ready(struct nicvf *nic)
 {
 	int timeout = 5000, sleep = 20;
-	uint64_t mbx_addr = NIC_VF_PF_MAILBOX_0_7;
+	u64 mbx_addr = NIC_VF_PF_MAILBOX_0_1;
 
 	pf_ready_to_rcv_msg = false;
 
-	nicvf_reg_write(nic, mbx_addr, NIC_PF_VF_MSG_READY);
+	nicvf_reg_write(nic, mbx_addr, le64_to_cpu(NIC_MBOX_MSG_READY));
 
 	mbx_addr += (NIC_PF_VF_MAILBOX_SIZE - 1) * 8;
 	nicvf_reg_write(nic, mbx_addr, 1ULL);
@@ -197,11 +179,10 @@ static int nicvf_check_pf_ready(struct nicvf *nic)
 		msleep(sleep);
 		if (pf_ready_to_rcv_msg)
 			break;
-		else
-			timeout -= sleep;
+		timeout -= sleep;
 		if (!timeout) {
 			netdev_err(nic->netdev,
-				"PF didn't respond to READY msg\n");
+				   "PF didn't respond to READY msg\n");
 			return 0;
 		}
 	}
@@ -210,35 +191,72 @@ static int nicvf_check_pf_ready(struct nicvf *nic)
 
 static void  nicvf_handle_mbx_intr(struct nicvf *nic)
 {
+	struct nic_mbx mbx = {};
+	u64 *mbx_data;
+	u64 mbx_addr;
 	int i;
-	struct nic_mbx  mbx_tmp, *mbx;
-	uint64_t *mbx_data;
-	uint64_t mbx_addr;
 
-	mbx_addr = NIC_VF_PF_MAILBOX_0_7;
-
-	mbx = &mbx_tmp;
-	mbx_data = (uint64_t *)mbx;
+	mbx_addr = NIC_VF_PF_MAILBOX_0_1;
+	mbx_data = (u64 *)&mbx;
 
 	for (i = 0; i < NIC_PF_VF_MAILBOX_SIZE; i++) {
-		*mbx_data = nicvf_reg_read(nic, mbx_addr + (i * NIC_PF_VF_MAILBOX_SIZE));
+		*mbx_data = nicvf_reg_read(nic, mbx_addr);
 		mbx_data++;
+		mbx_addr += sizeof(u64);
 	}
 
-	switch (mbx->msg & 0xFF) {
-	case NIC_PF_VF_MSG_READY:
+	nic_dbg(&nic->pdev->dev,
+		"Mbox message from PF, msg 0x%x\n", mbx.msg);
+	switch (mbx.msg) {
+	case NIC_MBOX_MSG_READY:
 		pf_ready_to_rcv_msg = true;
-		nic->vnic_id = mbx->data.vnic_id & 0x7F;
+		nic->vf_id = mbx.data.nic_cfg.vf_id & 0x7F;
+		nic->tns_mode = mbx.data.nic_cfg.tns_mode & 0x7F;
+		nic->node = mbx.data.nic_cfg.node_id;
+		ether_addr_copy(nic->netdev->dev_addr,
+				(u8 *)&mbx.data.nic_cfg.mac_addr);
+		nic->link_up = false;
+		nic->duplex = 0;
+		nic->speed = 0;
 		break;
-	case NIC_PF_VF_MSG_ACK:
+	case NIC_MBOX_MSG_ACK:
 		pf_acked = true;
 		break;
-	case NIC_PF_VF_MSG_NACK:
+	case NIC_MBOX_MSG_NACK:
 		pf_nacked = true;
 		break;
+#ifdef VNIC_RSS_SUPPORT
+	case NIC_MBOX_MSG_RSS_SIZE:
+		nic->rss_info.rss_size = mbx.data.rss_size.ind_tbl_size;
+		pf_acked = true;
+		break;
+#endif
+	case NIC_MBOX_MSG_BGX_STATS:
+		nicvf_read_bgx_stats(nic, &mbx.data.bgx_stats);
+		pf_acked = true;
+		bgx_stats_acked = true;
+		break;
+	case NIC_MBOX_MSG_BGX_LINK_CHANGE:
+		pf_acked = true;
+		nic->link_up = mbx.data.link_status.link_up;
+		nic->duplex = mbx.data.link_status.duplex;
+		nic->speed = mbx.data.link_status.speed;
+		if (nic->link_up) {
+			pr_info("%s: Link is Up %d Mbps %s\n",
+				nic->netdev->name,
+				nic->speed, nic->duplex == DUPLEX_FULL ?
+				"Full duplex" : "Half duplex");
+			netif_carrier_on(nic->netdev);
+			netif_tx_wake_all_queues(nic->netdev);
+		} else {
+			pr_info("%s: Link is Down\n", nic->netdev->name);
+			netif_carrier_off(nic->netdev);
+			netif_tx_stop_all_queues(nic->netdev);
+		}
+		break;
 	default:
-		netdev_err(nic->netdev, "Invalid message from PF, msg 0x%llx\n",
-								mbx->msg);
+		netdev_err(nic->netdev,
+			   "Invalid message from PF, msg 0x%x\n", mbx.msg);
 		break;
 	}
 	nicvf_clear_intr(nic, NICVF_INTR_MBOX, 0);
@@ -246,157 +264,231 @@ static void  nicvf_handle_mbx_intr(struct nicvf *nic)
 
 static int nicvf_hw_set_mac_addr(struct nicvf *nic, struct net_device *netdev)
 {
+	struct nic_mbx mbx = {};
 	int i;
-	int ret;
-	struct  nic_mbx *mbx;
 
-	mbx = nicvf_get_mbx();
-	mbx->msg = NIC_PF_VF_MSG_SET_MAC;
-	mbx->data.mac.vnic_id = nic->vnic_id;
+	mbx.msg = NIC_MBOX_MSG_SET_MAC;
+	mbx.data.mac.vf_id = nic->vf_id;
 	for (i = 0; i < ETH_ALEN; i++)
-		mbx->data.mac.addr = (mbx->data.mac.addr << 8) | netdev->dev_addr[i];
+		mbx.data.mac.addr = (mbx.data.mac.addr << 8) |
+				     netdev->dev_addr[i];
+
+	return nicvf_send_msg_to_pf(nic, &mbx);
+}
+
+void nicvf_config_cpi(struct nicvf *nic)
+{
+	struct nic_mbx mbx = {};
 
-	ret = nicvf_send_msg_to_pf(nic, mbx);
-	kfree(mbx);
+	mbx.msg = NIC_MBOX_MSG_CPI_CFG;
+	mbx.data.cpi_cfg.vf_id = nic->vf_id;
+	mbx.data.cpi_cfg.cpi_alg = nic->cpi_alg;
+	mbx.data.cpi_cfg.rq_cnt = nic->qs->rq_cnt;
 
-	return ret;
+	nicvf_send_msg_to_pf(nic, &mbx);
 }
 
-static int nicvf_is_link_active(struct nicvf *nic)
+#ifdef	VNIC_RSS_SUPPORT
+void nicvf_get_rss_size(struct nicvf *nic)
 {
-	return 1;
+	struct nic_mbx mbx = {};
+
+	mbx.msg = NIC_MBOX_MSG_RSS_SIZE;
+	mbx.data.rss_size.vf_id = nic->vf_id;
+	nicvf_send_msg_to_pf(nic, &mbx);
 }
 
-static int nicvf_init_resources(struct nicvf *nic)
+void nicvf_config_rss(struct nicvf *nic)
 {
-	int err;
+	struct nic_mbx mbx = {};
+	struct nicvf_rss_info *rss = &nic->rss_info;
+	int ind_tbl_len = rss->rss_size;
+	int i, nextq = 0;
+
+	mbx.data.rss_cfg.vf_id = nic->vf_id;
+	mbx.data.rss_cfg.hash_bits = rss->hash_bits;
+	while (ind_tbl_len) {
+		mbx.data.rss_cfg.tbl_offset = nextq;
+		mbx.data.rss_cfg.tbl_len = min(ind_tbl_len,
+					       RSS_IND_TBL_LEN_PER_MBX_MSG);
+		mbx.msg = mbx.data.rss_cfg.tbl_offset ?
+			  NIC_MBOX_MSG_RSS_CFG_CONT : NIC_MBOX_MSG_RSS_CFG;
+
+		for (i = 0; i < mbx.data.rss_cfg.tbl_len; i++)
+			mbx.data.rss_cfg.ind_tbl[i] = rss->ind_tbl[nextq++];
+
+		nicvf_send_msg_to_pf(nic, &mbx);
+
+		ind_tbl_len -= mbx.data.rss_cfg.tbl_len;
+	}
+}
 
-	nic->num_qs = 1;
+void nicvf_set_rss_key(struct nicvf *nic)
+{
+	struct nicvf_rss_info *rss = &nic->rss_info;
+	u64 key_addr = NIC_VNIC_RSS_KEY_0_4;
+	int idx;
 
-	/* Initialize queues and HW for data transfer */
-	if ((err = nicvf_config_data_transfer(nic, true))) {
-		netdev_err(nic->netdev,
-			"Failed to allocate/configure VF's QSet resources, err %d\n", err);
-		return err;
+	for (idx = 0; idx < RSS_HASH_KEY_SIZE; idx++) {
+		nicvf_reg_write(nic, key_addr, rss->key[idx]);
+		key_addr += sizeof(u64);
 	}
-	/* Enable Qset */
-	nicvf_qset_config(nic, true);
+}
 
-	return 0;
+static int nicvf_rss_init(struct nicvf *nic)
+{
+	struct nicvf_rss_info *rss = &nic->rss_info;
+	int idx;
+
+	nicvf_get_rss_size(nic);
+
+	if ((nic->qs->rq_cnt <= 1) || (cpi_alg != CPI_ALG_NONE)) {
+		rss->enable = false;
+		rss->hash_bits = 0;
+		return 0;
+	}
+
+	rss->enable = true;
+
+	/* Using the HW reset value for now */
+	rss->key[0] = 0xFEED0BADFEED0BAD;
+	rss->key[1] = 0xFEED0BADFEED0BAD;
+	rss->key[2] = 0xFEED0BADFEED0BAD;
+	rss->key[3] = 0xFEED0BADFEED0BAD;
+	rss->key[4] = 0xFEED0BADFEED0BAD;
+
+	nicvf_set_rss_key(nic);
+
+	rss->cfg = rss_config;
+	nicvf_reg_write(nic, NIC_VNIC_RSS_CFG, rss->cfg);
+
+	rss->hash_bits =  ilog2(rounddown_pow_of_two(rss->rss_size));
+
+	for (idx = 0; idx < rss->rss_size; idx++)
+		rss->ind_tbl[idx] = ethtool_rxfh_indir_default(idx,
+							       nic->qs->rq_cnt);
+	nicvf_config_rss(nic);
+	return 1;
 }
+#endif
 
-void nicvf_free_skb(struct nicvf *nic, struct sk_buff *skb)
+int nicvf_set_real_num_queues(struct net_device *netdev,
+			      int tx_queues, int rx_queues)
 {
-	int i;
+	int err = 0;
+
+	err = netif_set_real_num_tx_queues(netdev, tx_queues);
+	if (err) {
+		netdev_err(netdev,
+			   "Failed to set no of Tx queues: %d\n", tx_queues);
+		return err;
+	}
+
+	err = netif_set_real_num_rx_queues(netdev, rx_queues);
+	if (err)
+		netdev_err(netdev,
+			   "Failed to set no of Rx queues: %d\n", rx_queues);
+	return err;
+}
 
-	if (!skb_shinfo(skb)->nr_frags)
-		goto free_skb;
+static int nicvf_init_resources(struct nicvf *nic)
+{
+	int err;
 
-	for (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {
-		const struct skb_frag_struct *frag;
+	/* Enable Qset */
+	nicvf_qset_config(nic, true);
 
-		frag = &skb_shinfo(skb)->frags[i];
-		pci_unmap_single(nic->pdev, (dma_addr_t)skb_frag_address(frag),
-						skb_frag_size(frag), PCI_DMA_TODEVICE);
+	/* Initialize queues and HW for data transfer */
+	err = nicvf_config_data_transfer(nic, true);
+	if (err) {
+		netdev_err(nic->netdev,
+			   "Failed to alloc/config VF's QSet resources\n");
+		return err;
 	}
-free_skb:
-	pci_unmap_single(nic->pdev, (dma_addr_t)skb->data, skb_headlen(skb), PCI_DMA_TODEVICE);
-	dev_kfree_skb_any(skb);
+	return 0;
 }
 
 static void nicvf_snd_pkt_handler(struct net_device *netdev,
-				  void *cq_desc, int cqe_type)
+				  struct cmp_queue *cq,
+				  struct cqe_send_t *cqe_tx, int cqe_type)
 {
 	struct sk_buff *skb = NULL;
-	struct cqe_send_t *cqe_tx;
 	struct nicvf *nic = netdev_priv(netdev);
 	struct snd_queue *sq;
 	struct sq_hdr_subdesc *hdr;
 
-	cqe_tx = (struct cqe_send_t *)cq_desc;
 	sq = &nic->qs->sq[cqe_tx->sq_idx];
 
-	hdr  = (struct sq_hdr_subdesc *)(sq->desc_mem.base +
-				(cqe_tx->sqe_ptr * SND_QUEUE_DESC_SIZE));
+	hdr = (struct sq_hdr_subdesc *)GET_SQ_DESC(sq, cqe_tx->sqe_ptr);
 	if (hdr->subdesc_type != SQ_DESC_TYPE_HEADER)
 		return;
 
-	nic_dbg(&nic->pdev->dev, "%s Qset #%d SQ #%d SQ ptr #%d Subdesc count %d\n",
-				__func__, cqe_tx->sq_qs, cqe_tx->sq_idx,
-					cqe_tx->sqe_ptr, hdr->subdesc_cnt);
+	nic_dbg(&nic->pdev->dev,
+		"%s Qset #%d SQ #%d SQ ptr #%d subdesc count %d\n",
+		__func__, cqe_tx->sq_qs, cqe_tx->sq_idx,
+		cqe_tx->sqe_ptr, hdr->subdesc_cnt);
 
 	skb = (struct sk_buff *)sq->skbuff[cqe_tx->sqe_ptr];
-	atomic64_add(1, (atomic64_t *)&netdev->stats.tx_packets);
-	atomic64_add(hdr->tot_len, (atomic64_t *)&netdev->stats.tx_bytes);
-	nicvf_free_skb(nic, skb);
+	prefetch(skb);
+	nicvf_check_cqe_tx_errs(nic, cq, cqe_tx);
 	nicvf_put_sq_desc(sq, hdr->subdesc_cnt + 1);
+	dev_consume_skb_any(skb);
 }
 
 static void nicvf_rcv_pkt_handler(struct net_device *netdev,
-			struct napi_struct *napi, void *cq_desc, int cqe_type)
+				  struct napi_struct *napi,
+				  struct cmp_queue *cq,
+				  struct cqe_rx_t *cqe_rx, int cqe_type)
 {
 	struct sk_buff *skb;
 	struct nicvf *nic = netdev_priv(netdev);
-
-	if (!((cqe_type == CQE_TYPE_RX) || (cqe_type == CQE_TYPE_RX_SPLIT) ||
-						(cqe_type == CQE_TYPE_RX_TCP))) {
-		atomic64_add(1, (atomic64_t *)&netdev->stats.rx_dropped);
-		return;
-	}
+	int err = 0;
 
 	/* Check for errors */
-	if (nicvf_cq_check_errs(nic, cq_desc)) {
-		atomic64_add(1, (atomic64_t *)&netdev->stats.rx_errors);
+	err = nicvf_check_cqe_rx_errs(nic, cq, cqe_rx);
+	if (err && !cqe_rx->rb_cnt)
 		return;
-	}
 
-	skb = nicvf_get_rcv_skb(nic, cq_desc);
+	skb = nicvf_get_rcv_skb(nic, cqe_rx);
 	if (!skb) {
 		nic_dbg(&nic->pdev->dev, "Packet not received\n");
 		return;
 	}
 
-	nicvf_dump_packet(skb);
+	if (netif_msg_pktdata(nic))
+		nicvf_dump_packet(netdev, skb);
 
-	/* Update stats */
-	atomic64_add(1, (atomic64_t *)&netdev->stats.rx_packets);
-	atomic64_add(skb->len, (atomic64_t *)&netdev->stats.rx_bytes);
+	nicvf_set_rx_frame_cnt(nic, skb);
 
-#ifdef NICVF_ETHTOOL_ENABLE
-	nicvf_update_stats(nic, skb);
-#endif
+	skb_record_rx_queue(skb, cqe_rx->rq_idx);
+	if (netdev->hw_features & NETIF_F_RXCSUM) {
+		/* HW by default verifies TCP/UDP/SCTP checksums */
+		skb->ip_summed = CHECKSUM_UNNECESSARY;
+	} else {
+		skb_checksum_none_assert(skb);
+	}
 
 	skb->protocol = eth_type_trans(skb, netdev);
 
-#ifdef VNIC_RX_CHKSUM_SUPPORTED
-	skb->ip_summed = CHECKSUM_UNNECESSARY;
-#else
-	skb_checksum_none_assert(skb);
-#endif
-
-#ifdef	NICVF_NAPI_ENABLE
-#ifdef	VNIC_SW_LRO_SUPPORT
 	if (napi && (netdev->features & NETIF_F_GRO))
 		napi_gro_receive(napi, skb);
 	else
-#endif
 		netif_receive_skb(skb);
-#else
-	netif_rx(skb);
-#endif
 }
 
-static int nicvf_cq_intr_handler(struct net_device *netdev, uint8_t cq_idx,
-					struct napi_struct *napi, int budget)
+static int nicvf_cq_intr_handler(struct net_device *netdev, u8 cq_idx,
+				 struct napi_struct *napi, int budget)
 {
-	int processed_cqe = 0, work_done = 0;
+	int processed_cqe, work_done = 0;
 	int cqe_count, cqe_head;
 	struct nicvf *nic = netdev_priv(netdev);
 	struct queue_set *qs = nic->qs;
 	struct cmp_queue *cq = &qs->cq[cq_idx];
 	struct cqe_rx_t *cq_desc;
 
-	spin_lock(&cq->cq_lock);
+	spin_lock_bh(&cq->lock);
+loop:
+	processed_cqe = 0;
 	/* Get no of valid CQ entries to process */
 	cqe_count = nicvf_queue_reg_read(nic, NIC_QSET_CQ_0_7_STATUS, cq_idx);
 	cqe_count &= CQ_CQE_COUNT;
@@ -407,25 +499,32 @@ static int nicvf_cq_intr_handler(struct net_device *netdev, uint8_t cq_idx,
 	cqe_head = nicvf_queue_reg_read(nic, NIC_QSET_CQ_0_7_HEAD, cq_idx) >> 9;
 	cqe_head &= 0xFFFF;
 
-	nic_dbg(&nic->pdev->dev, "%s cqe_count %d cqe_head %d\n", __func__, cqe_count, cqe_head);
+	nic_dbg(&nic->pdev->dev, "%s cqe_count %d cqe_head %d\n",
+		__func__, cqe_count, cqe_head);
 	while (processed_cqe < cqe_count) {
 		/* Get the CQ descriptor */
-		cq_desc = (struct cqe_rx_t *)(cq->desc_mem.base +
-				(cqe_head * CMP_QUEUE_DESC_SIZE));
+		cq_desc = (struct cqe_rx_t *)GET_CQ_DESC(cq, cqe_head);
+		cqe_head++;
+		cqe_head &= (cq->dmem.q_len - 1);
+		/* Initiate prefetch for next descriptor */
+		prefetch((struct cqe_rx_t *)GET_CQ_DESC(cq, cqe_head));
 
-		if (napi && (work_done >= budget) &&
-			(cq_desc->cqe_type != CQE_TYPE_SEND)) {
+		if ((work_done >= budget) && napi &&
+		    (cq_desc->cqe_type != CQE_TYPE_SEND)) {
 			break;
 		}
 
-		nic_dbg(&nic->pdev->dev, "cq_desc->cqe_type %d\n", cq_desc->cqe_type);
+		nic_dbg(&nic->pdev->dev, "cq_desc->cqe_type %d\n",
+			cq_desc->cqe_type);
 		switch (cq_desc->cqe_type) {
 		case CQE_TYPE_RX:
-			nicvf_rcv_pkt_handler(netdev, napi, cq_desc, CQE_TYPE_RX);
+			nicvf_rcv_pkt_handler(netdev, napi, cq,
+					      cq_desc, CQE_TYPE_RX);
 			work_done++;
 		break;
 		case CQE_TYPE_SEND:
-			nicvf_snd_pkt_handler(netdev, cq_desc, CQE_TYPE_SEND);
+			nicvf_snd_pkt_handler(netdev, cq,
+					      (void *)cq_desc, CQE_TYPE_SEND);
 		break;
 		case CQE_TYPE_INVALID:
 		case CQE_TYPE_RX_SPLIT:
@@ -434,31 +533,33 @@ static int nicvf_cq_intr_handler(struct net_device *netdev, uint8_t cq_idx,
 			/* Ignore for now */
 		break;
 		}
-		cq_desc->cqe_type = CQE_TYPE_INVALID;
 		processed_cqe++;
-		cqe_head++;
-		cqe_head &= (cq->desc_mem.q_len - 1);
 	}
 	nic_dbg(&nic->pdev->dev, "%s processed_cqe %d work_done %d budget %d\n",
-			__func__, processed_cqe, work_done, budget);
+		__func__, processed_cqe, work_done, budget);
 
 	/* Ring doorbell to inform H/W to reuse processed CQEs */
 	nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_DOOR,
-					cq_idx, processed_cqe);
+			      cq_idx, processed_cqe);
+
+	if ((work_done < budget) && napi)
+		goto loop;
+
 done:
-	spin_unlock(&cq->cq_lock);
+	spin_unlock_bh(&cq->lock);
 	return work_done;
 }
 
-#ifdef	NICVF_NAPI_ENABLE
 static int nicvf_poll(struct napi_struct *napi, int budget)
 {
+	u64  cq_head;
 	int  work_done = 0;
 	struct net_device *netdev = napi->dev;
 	struct nicvf *nic = netdev_priv(netdev);
-	struct nicvf_cq_poll *cq = container_of(napi, struct nicvf_cq_poll, napi);
+	struct nicvf_cq_poll *cq;
 	struct netdev_queue *txq;
 
+	cq = container_of(napi, struct nicvf_cq_poll, napi);
 	work_done = nicvf_cq_intr_handler(netdev, cq->cq_idx, napi, budget);
 
 	txq = netdev_get_tx_queue(netdev, cq->cq_idx);
@@ -469,11 +570,15 @@ static int nicvf_poll(struct napi_struct *napi, int budget)
 		/* Slow packet rate, exit polling */
 		napi_complete(napi);
 		/* Re-enable interrupts */
+		cq_head = nicvf_queue_reg_read(nic, NIC_QSET_CQ_0_7_HEAD,
+					       cq->cq_idx);
+		nicvf_clear_intr(nic, NICVF_INTR_CQ, cq->cq_idx);
+		nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_HEAD,
+				      cq->cq_idx, cq_head);
 		nicvf_enable_intr(nic, NICVF_INTR_CQ, cq->cq_idx);
 	}
 	return work_done;
 }
-#endif
 
 /* Qset error interrupt handler
  *
@@ -484,14 +589,14 @@ void nicvf_handle_qs_err(unsigned long data)
 	struct nicvf *nic = (struct nicvf *)data;
 	struct queue_set *qs = nic->qs;
 	int qidx;
-	uint64_t status;
+	u64 status;
 
 	netif_tx_disable(nic->netdev);
 
 	/* Check if it is CQ err */
 	for (qidx = 0; qidx < qs->cq_cnt; qidx++) {
-		status = nicvf_queue_reg_read(nic,
-					NIC_QSET_CQ_0_7_STATUS, qidx);
+		status = nicvf_queue_reg_read(nic, NIC_QSET_CQ_0_7_STATUS,
+					      qidx);
 		if (!(status & CQ_ERR_MASK))
 			continue;
 		/* Process already queued CQEs and reconfig CQ */
@@ -513,6 +618,12 @@ void nicvf_handle_qs_err(unsigned long data)
 static irqreturn_t nicvf_misc_intr_handler(int irq, void *nicvf_irq)
 {
 	struct nicvf *nic = (struct nicvf *)nicvf_irq;
+	u64 intr;
+
+	intr = nicvf_reg_read(nic, NIC_VF_INT);
+	/* Check for spurious interrupt */
+	if (!(intr & NICVF_INTR_MBOX_MASK))
+		return IRQ_HANDLED;
 
 	nicvf_handle_mbx_intr(nic);
 
@@ -521,13 +632,17 @@ static irqreturn_t nicvf_misc_intr_handler(int irq, void *nicvf_irq)
 
 static irqreturn_t nicvf_intr_handler(int irq, void *nicvf_irq)
 {
-	uint64_t qidx, intr;
-	uint64_t cq_intr, rbdr_intr, qs_err_intr;
+	u64 qidx, intr, clear_intr = 0;
+	u64 cq_intr, rbdr_intr, qs_err_intr;
 	struct nicvf *nic = (struct nicvf *)nicvf_irq;
 	struct queue_set *qs = nic->qs;
+	struct nicvf_cq_poll *cq_poll = NULL;
+	struct cmp_queue *cq;
 
-	intr = nicvf_qset_reg_read(nic, NIC_VF_INT);
-	nic_dbg(&nic->pdev->dev, "%s intr status 0x%llx\n", __func__, intr);
+	intr = nicvf_reg_read(nic, NIC_VF_INT);
+	if (netif_msg_intr(nic))
+		dev_info(&nic->pdev->dev, "%s: interrupt status 0x%llx\n",
+			 nic->netdev->name, intr);
 
 	cq_intr = (intr & NICVF_INTR_CQ_MASK) >> NICVF_INTR_CQ_SHIFT;
 	qs_err_intr = intr & NICVF_INTR_QS_ERR_MASK;
@@ -535,29 +650,31 @@ static irqreturn_t nicvf_intr_handler(int irq, void *nicvf_irq)
 		/* Disable Qset err interrupt and schedule softirq */
 		nicvf_disable_intr(nic, NICVF_INTR_QS_ERR, 0);
 		tasklet_hi_schedule(&nic->qs_err_task);
+		clear_intr = qs_err_intr;
 	}
 
-#ifdef	NICVF_NAPI_ENABLE
-	{
-		struct nicvf_cq_poll *cq_poll = NULL;
-		/* Disable interrupts and start polling */
-		for (qidx = 0; qidx < qs->cq_cnt; qidx++) {
-			if (!(cq_intr & (1 << qidx)))
-				continue;
-			if (!nicvf_is_intr_enabled(nic, NICVF_INTR_CQ, qidx))
-				continue;
-			nicvf_disable_intr(nic, NICVF_INTR_CQ, qidx);
-			cq_poll = nic->napi[qidx];
-			/* Schedule NAPI */
-			napi_schedule(&cq_poll->napi);
-		}
-	}
-#else
+	/* Disable interrupts and start polling */
 	for (qidx = 0; qidx < qs->cq_cnt; qidx++) {
-		if (cq_intr & (1 << qidx))
-			nicvf_cq_intr_handler(nic->netdev, qidx, NULL, 0);
+		if (!(cq_intr & (1 << qidx)))
+			continue;
+		if (!nicvf_is_intr_enabled(nic, NICVF_INTR_CQ, qidx))
+			continue;
+
+		/* Makesure NAPI is scheduled on CPU to which
+		 * CQ's IRQ affinity is set.
+		 */
+		cq = &nic->qs->cq[qidx];
+		if (smp_processor_id() != cpumask_first(&cq->affinity_mask))
+			continue;
+
+		nicvf_disable_intr(nic, NICVF_INTR_CQ, qidx);
+		clear_intr |= ((1 << qidx) << NICVF_INTR_CQ_SHIFT);
+
+		cq_poll = nic->napi[qidx];
+		/* Schedule NAPI */
+		napi_schedule(&cq_poll->napi);
 	}
-#endif
+
 	/* Handle RBDR interrupts */
 	rbdr_intr = (intr & NICVF_INTR_RBDR_MASK) >> NICVF_INTR_RBDR_SHIFT;
 	if (rbdr_intr) {
@@ -565,44 +682,29 @@ static irqreturn_t nicvf_intr_handler(int irq, void *nicvf_irq)
 		for (qidx = 0; qidx < qs->rbdr_cnt; qidx++)
 			nicvf_disable_intr(nic, NICVF_INTR_RBDR, qidx);
 
+		clear_intr |= (rbdr_intr << NICVF_INTR_RBDR_SHIFT);
 		tasklet_hi_schedule(&nic->rbdr_task);
 	}
 
 	/* Clear interrupts */
-	nicvf_qset_reg_write(nic, NIC_VF_INT, intr);
+	nicvf_reg_write(nic, NIC_VF_INT, clear_intr);
 	return IRQ_HANDLED;
 }
 
 static int nicvf_enable_msix(struct nicvf *nic)
 {
-	int i, ret, vec;
-	struct queue_set *qs = nic->qs;
+	int ret, vec;
 
 	nic->num_vec = NIC_VF_MSIX_VECTORS;
-	vec = qs->cq_cnt + qs->rbdr_cnt + qs->sq_cnt;
-	vec = NIC_VF_MSIX_VECTORS;
-	if (vec > NIC_VF_MSIX_VECTORS)
-		nic->num_vec = vec;
 
-	for (i = 0; i < nic->num_vec; i++)
-		nic->msix_entries[i].entry = i;
+	for (vec = 0; vec < nic->num_vec; vec++)
+		nic->msix_entries[vec].entry = vec;
 
 	ret = pci_enable_msix(nic->pdev, nic->msix_entries, nic->num_vec);
-	if (ret < 0) {
+	if (ret) {
 		netdev_err(nic->netdev,
-			"Request for #%d msix vectors failed\n", nic->num_vec);
+			   "Req for #%d msix vectors failed\n", nic->num_vec);
 		return 0;
-	} else if (ret > 0) {
-		netdev_err(nic->netdev,
-			"Request for #%d msix vectors failed, requesting #%d\n",
-			nic->num_vec, ret);
-
-		nic->num_vec = ret;
-		ret = pci_enable_msix(nic->pdev, nic->msix_entries, nic->num_vec);
-		if (ret) {
-			netdev_warn(nic->netdev, "Request for msix vectors failed\n");
-			return 0;
-		}
 	}
 	nic->msix_enabled = 1;
 	return 1;
@@ -617,43 +719,82 @@ static void nicvf_disable_msix(struct nicvf *nic)
 	}
 }
 
+static void nicvf_set_cq_irq_affinity(struct nicvf *nic, int qidx, int irq)
+{
+	int cpu, first_cpu, num_online_cpus;
+	struct cmp_queue *cq = &nic->qs->cq[qidx];
+
+	num_online_cpus = cpumask_weight(cpumask_of_node(nic->node));
+	first_cpu = cpumask_first(cpumask_of_node(nic->node));
+
+	if (num_online_cpus > nic->netdev->real_num_rx_queues)
+		cpu = first_cpu + qidx + 1; /* Leave CPU0 for RBDR interrupt */
+	else
+		cpu = first_cpu + (qidx % num_online_cpus);
+
+	if (!(cpu_online(cpu) && irq_can_set_affinity(cpu)))
+		cpu = first_cpu;
+
+	cpumask_clear(&cq->affinity_mask);
+	cpumask_set_cpu(cpu, &cq->affinity_mask);
+	__irq_set_affinity(irq, &cq->affinity_mask, false);
+}
+
 static int nicvf_register_interrupts(struct nicvf *nic)
 {
 	int irq, free, ret = 0;
+	int vector;
 
 	for_each_cq_irq(irq)
 		sprintf(nic->irq_name[irq], "%s%d CQ%d", "NICVF",
-						nic->vnic_id, irq);
+			nic->vf_id, irq);
 
 	for_each_sq_irq(irq)
 		sprintf(nic->irq_name[irq], "%s%d SQ%d", "NICVF",
-			nic->vnic_id, irq - NICVF_SQ_INTR_ID);
+			nic->vf_id, irq - NICVF_INTR_ID_SQ);
 
 	for_each_rbdr_irq(irq)
 		sprintf(nic->irq_name[irq], "%s%d RBDR%d", "NICVF",
-			nic->vnic_id, irq - NICVF_RBDR_INTR_ID);
+			nic->vf_id, irq - NICVF_INTR_ID_RBDR);
 
 	/* Register all interrupts except mailbox */
-	for (irq = 0; irq < NICVF_MISC_INTR_ID; irq++) {
-		if ((ret = request_irq(nic->msix_entries[irq].vector,
-				nicvf_intr_handler, 0 , nic->irq_name[irq], nic)))
+	for (irq = 0; irq < NICVF_INTR_ID_SQ; irq++) {
+		vector = nic->msix_entries[irq].vector;
+		ret = request_irq(vector, nicvf_intr_handler,
+				  0, nic->irq_name[irq], nic);
+		if (ret)
 			break;
-		nic->irq_allocated[irq] = 1;
+		nic->irq_allocated[irq] = true;
+
+		/* Set CQ irq affinity */
+		nicvf_set_cq_irq_affinity(nic, irq, vector);
 	}
 
-	sprintf(nic->irq_name[NICVF_QS_ERR_INTR_ID],
-				"%s%d Qset error", "VNICVF", nic->vnic_id);
+	for (irq = NICVF_INTR_ID_SQ; irq < NICVF_INTR_ID_MISC; irq++) {
+		vector = nic->msix_entries[irq].vector;
+		ret = request_irq(vector, nicvf_intr_handler,
+				  0, nic->irq_name[irq], nic);
+		if (ret)
+			break;
+		nic->irq_allocated[irq] = true;
+	}
+
+	sprintf(nic->irq_name[NICVF_INTR_ID_QS_ERR],
+		"%s%d Qset error", "NICVF", nic->vf_id);
 	if (!ret) {
-		if (!(ret = request_irq(nic->msix_entries[NICVF_QS_ERR_INTR_ID].vector,
-				nicvf_intr_handler, 0 , nic->irq_name[NICVF_QS_ERR_INTR_ID], nic)))
-			nic->irq_allocated[NICVF_QS_ERR_INTR_ID] = 1;
+		vector = nic->msix_entries[NICVF_INTR_ID_QS_ERR].vector;
+		irq = NICVF_INTR_ID_QS_ERR;
+		ret = request_irq(vector, nicvf_intr_handler,
+				  0, nic->irq_name[irq], nic);
+		if (!ret)
+			nic->irq_allocated[irq] = true;
 	}
 
 	if (ret) {
 		netdev_err(nic->netdev, "Request irq failed\n");
 		for (free = 0; free < irq; free++)
 			free_irq(nic->msix_entries[free].vector, nic);
-		return 1;
+		return ret;
 	}
 
 	return 0;
@@ -667,37 +808,44 @@ static void nicvf_unregister_interrupts(struct nicvf *nic)
 	for (irq = 0; irq < nic->num_vec; irq++) {
 		if (nic->irq_allocated[irq])
 			free_irq(nic->msix_entries[irq].vector, nic);
-		nic->irq_allocated[irq] = 0;
+		nic->irq_allocated[irq] = false;
 	}
 
 	/* Disable MSI-X */
 	nicvf_disable_msix(nic);
 }
 
+/* Initialize MSIX vectors and register MISC interrupt.
+ * Send READY message to PF to check if its alive
+ */
 static int nicvf_register_misc_interrupt(struct nicvf *nic)
 {
-	int  ret = 0;
-	int irq = NICVF_MISC_INTR_ID;
+	int ret = 0;
+	int irq = NICVF_INTR_ID_MISC;
+
+	/* Return if mailbox interrupt is already registered */
+	if (nic->msix_enabled)
+		return 0;
 
 	/* Enable MSI-X */
 	if (!nicvf_enable_msix(nic))
 		return 1;
 
-	sprintf(nic->irq_name[irq], "%s%d Mbox", "VNIC", nic->vnic_id);
+	sprintf(nic->irq_name[irq], "%s Mbox", "NICVF");
 	/* Register Misc interrupt */
-	ret = request_irq(nic->msix_entries[irq].vector, nicvf_misc_intr_handler,
-						0, nic->irq_name[irq], nic);
+	ret = request_irq(nic->msix_entries[irq].vector,
+			  nicvf_misc_intr_handler, 0, nic->irq_name[irq], nic);
 
 	if (ret)
-		return 1;
-	nic->irq_allocated[irq] = 1;
+		return ret;
+	nic->irq_allocated[irq] = true;
 
 	/* Enable mailbox interrupt */
-	nicvf_enable_mbx_intr(nic);
+	nicvf_enable_intr(nic, NICVF_INTR_MBOX, 0);
 
 	/* Check if VF is able to communicate with PF */
 	if (!nicvf_check_pf_ready(nic)) {
-		nicvf_disable_mbx_intr(nic);
+		nicvf_disable_intr(nic, NICVF_INTR_MBOX, 0);
 		nicvf_unregister_interrupts(nic);
 		return 1;
 	}
@@ -705,102 +853,42 @@ static int nicvf_register_misc_interrupt(struct nicvf *nic)
 	return 0;
 }
 
-static void nicvf_update_tx_stats(struct nicvf *nic, struct sk_buff *skb) { }
-
-#ifdef VNIC_SW_TSO_SUPPORT
-static int nicvf_sw_tso(struct sk_buff *skb, struct net_device *netdev)
-{
-	struct sk_buff *segs, *nskb;
-
-	if (!skb_shinfo(skb)->gso_size)
-		return 1;
-
-	/* Segment the large frame */
-	segs = skb_gso_segment(skb, netdev->features & ~NETIF_F_TSO);
-	if (IS_ERR(segs))
-		goto gso_err;
-
-	do {
-		nskb = segs;
-		segs = segs->next;
-		nskb->next = NULL;
-		nicvf_xmit(nskb, netdev);
-	} while (segs);
-
-gso_err:
-	dev_kfree_skb(skb);
-
-	return NETDEV_TX_OK;
-}
-#endif
-
 static netdev_tx_t nicvf_xmit(struct sk_buff *skb, struct net_device *netdev)
 {
 	struct nicvf *nic = netdev_priv(netdev);
 	int qid = skb_get_queue_mapping(skb);
 	struct netdev_queue *txq = netdev_get_tx_queue(netdev, qid);
-	int ret = 1;
 
 	/* Check for minimum packet length */
 	if (skb->len <= ETH_HLEN) {
-		atomic64_add(1, (atomic64_t *)&netdev->stats.tx_errors);
 		dev_kfree_skb(skb);
 		return NETDEV_TX_OK;
 	}
 
-#ifdef VNIC_SW_TSO_SUPPORT
-	if (netdev->features & NETIF_F_TSO)
-		ret = nicvf_sw_tso(skb, netdev);
-#endif
-	if (ret == NETDEV_TX_OK)
-		return NETDEV_TX_OK;
-
-#ifndef VNIC_TX_CSUM_OFFLOAD_SUPPORT
-	/* Calculate checksum in software */
-	if (skb->ip_summed == CHECKSUM_PARTIAL) {
-		if (unlikely(skb_checksum_help(skb))) {
-			netdev_dbg(netdev, "unable to do checksum\n");
-			dev_kfree_skb_any(skb);
-			return NETDEV_TX_OK;
-		}
-	}
-#endif
-#ifdef VNIC_HW_TSO_SUPPORT
-	if (skb_shinfo(skb)->gso_size && ((skb->protocol == ETH_P_IP) &&
-				(ip_hdr(skb)->protocol != IPPROTO_TCP))) {
-		netdev_dbg(netdev,
-			   "Only TCP segmentation is supported, dropping packet\n");
-		dev_kfree_skb_any(skb);
-		return NETDEV_TX_OK;
-	}
-#endif
 	if (!nicvf_sq_append_skb(nic, skb) && !netif_tx_queue_stopped(txq)) {
 		netif_tx_stop_queue(txq);
-		atomic64_add(1, (atomic64_t *)&netdev->stats.tx_dropped);
-		nic_dbg(&nic->pdev->dev,
-			"VF%d: TX ring full, stop transmitting packets\n", nic->vnic_id);
+		nic->drv_stats.tx_busy++;
+		if (netif_msg_tx_err(nic))
+			netdev_warn(netdev,
+				    "%s: Transmit ring full, stopping SQ%d\n",
+				    netdev->name, qid);
+
 		return NETDEV_TX_BUSY;
 	}
 
-	nicvf_update_tx_stats(nic, skb);
 	return NETDEV_TX_OK;
 }
 
-static int nicvf_stop(struct net_device *netdev)
+int nicvf_stop(struct net_device *netdev)
 {
-	int qidx;
+	int irq, qidx;
 	struct nicvf *nic = netdev_priv(netdev);
 	struct queue_set *qs = nic->qs;
+	struct nicvf_cq_poll *cq_poll = NULL;
 
 	netif_carrier_off(netdev);
 	netif_tx_disable(netdev);
 
-	/* Disable HW Qset, to stop receiving packets */
-	nicvf_qset_config(nic, false);
-
-	/* disable mailbox interrupt */
-	nicvf_disable_mbx_intr(nic);
-
 	/* Disable interrupts */
 	for (qidx = 0; qidx < qs->cq_cnt; qidx++)
 		nicvf_disable_intr(nic, NICVF_INTR_CQ, qidx);
@@ -808,97 +896,105 @@ static int nicvf_stop(struct net_device *netdev)
 		nicvf_disable_intr(nic, NICVF_INTR_RBDR, qidx);
 	nicvf_disable_intr(nic, NICVF_INTR_QS_ERR, 0);
 
+	/* Wait for pending IRQ handlers to finish */
+	for (irq = 0; irq < nic->num_vec; irq++)
+		synchronize_irq(nic->msix_entries[irq].vector);
+
 	tasklet_kill(&nic->rbdr_task);
 	tasklet_kill(&nic->qs_err_task);
 
-	nicvf_unregister_interrupts(nic);
-#ifdef	NICVF_NAPI_ENABLE
 	for (qidx = 0; qidx < nic->qs->cq_cnt; qidx++) {
-		napi_synchronize(&nic->napi[qidx]->napi);
-		napi_disable(&nic->napi[qidx]->napi);
-		netif_napi_del(&nic->napi[qidx]->napi);
-		kfree(nic->napi[qidx]);
+		cq_poll = nic->napi[qidx];
+		if (!cq_poll)
+			continue;
+		napi_synchronize(&cq_poll->napi);
+		napi_disable(&cq_poll->napi);
+		napi_hash_del(&cq_poll->napi);
+		netif_napi_del(&cq_poll->napi);
+		kfree(cq_poll);
 		nic->napi[qidx] = NULL;
 	}
-#endif
+
 	/* Free resources */
 	nicvf_config_data_transfer(nic, false);
 
-	/* Free Qset */
-	kfree(qs);
+	/* Disable HW Qset */
+	nicvf_qset_config(nic, false);
+
+	/* disable mailbox interrupt */
+	nicvf_disable_intr(nic, NICVF_INTR_MBOX, 0);
+
+	nicvf_unregister_interrupts(nic);
 
 	return 0;
 }
 
-static int nicvf_open(struct net_device *netdev)
+int nicvf_open(struct net_device *netdev)
 {
 	int err, qidx;
 	struct nicvf *nic = netdev_priv(netdev);
-	struct queue_set *qs;
+	struct queue_set *qs = nic->qs;
 	struct nicvf_cq_poll *cq_poll = NULL;
 
 	nic->mtu = netdev->mtu;
 
 	netif_carrier_off(netdev);
 
-	if ((err = nicvf_register_misc_interrupt(nic)))
-		return -EIO;
-
-	if ((err = nicvf_init_resources(nic)))
+	err = nicvf_register_misc_interrupt(nic);
+	if (err)
 		return err;
 
-	qs = nic->qs;
-
-	if ((err = netif_set_real_num_tx_queues(netdev, qs->sq_cnt))) {
-		netdev_err(netdev,
-			"Failed to set real number of Tx queues: %d\n", err);
-		return err;
-	}
-	if ((err = netif_set_real_num_rx_queues(netdev, qs->rq_cnt))) {
-		netdev_err(netdev,
-			"Failed to set real number of Rx queues: %d\n", err);
-		return err;
-	}
-
-	if ((err = nicvf_register_interrupts(nic))) {
-		nicvf_stop(netdev);
-		return -EIO;
-	}
-
-#ifdef	NICVF_NAPI_ENABLE
+	/* Register NAPI handler for processing CQEs */
 	for (qidx = 0; qidx < qs->cq_cnt; qidx++) {
 		cq_poll = kzalloc(sizeof(*cq_poll), GFP_KERNEL);
-		if (!cq_poll)
+		if (!cq_poll) {
+			err = -ENOMEM;
 			goto napi_del;
+		}
 		cq_poll->cq_idx = qidx;
 		netif_napi_add(netdev, &cq_poll->napi, nicvf_poll,
 			       NAPI_POLL_WEIGHT);
+		napi_hash_add(&cq_poll->napi);
 		napi_enable(&cq_poll->napi);
 		nic->napi[qidx] = cq_poll;
 	}
-	goto no_err;
-napi_del:
-	while (qidx) {
-		qidx--;
-		cq_poll = nic->napi[qidx];
-		napi_disable(&cq_poll->napi);
-		netif_napi_del(&cq_poll->napi);
-		kfree(cq_poll);
-		nic->napi[qidx] = NULL;
+
+	/* Check if we got MAC address from PF or else generate a radom MAC */
+	if (is_zero_ether_addr(netdev->dev_addr)) {
+		eth_hw_addr_random(netdev);
+		nicvf_hw_set_mac_addr(nic, netdev);
 	}
-	return -ENOMEM;
-no_err:
+
+	/* Init tasklet for handling Qset err interrupt */
+	tasklet_init(&nic->qs_err_task, nicvf_handle_qs_err,
+		     (unsigned long)nic);
+
+	/* Init RBDR tasklet which will refill RBDR */
+	tasklet_init(&nic->rbdr_task, nicvf_refill_rbdr,
+		     (unsigned long)nic);
+
+	/* Configure CPI alorithm */
+	nic->cpi_alg = cpi_alg;
+	nicvf_config_cpi(nic);
+
+#ifdef	VNIC_RSS_SUPPORT
+	/* Configure receive side scaling */
+	nicvf_rss_init(nic);
 #endif
 
-	/* Set MAC-ID */
-	if (is_zero_ether_addr(netdev->dev_addr))
-		eth_hw_addr_random(netdev);
+	err = nicvf_register_interrupts(nic);
+	if (err)
+		goto cleanup;
 
-	nicvf_hw_set_mac_addr(nic, netdev);
+	/* Initialize the queues */
+	err = nicvf_init_resources(nic);
+	if (err)
+		goto cleanup;
 
-	/* Init tasklet for handling Qset err interrupt */
-	tasklet_init(&nic->qs_err_task, nicvf_handle_qs_err, (unsigned long)nic);
+	/* Make sure queue initialization is written */
+	wmb();
 
+	nicvf_reg_write(nic, NIC_VF_INT, -1);
 	/* Enable Qset err interrupt */
 	nicvf_enable_intr(nic, NICVF_INTR_QS_ERR, 0);
 
@@ -906,32 +1002,40 @@ no_err:
 	for (qidx = 0; qidx < qs->cq_cnt; qidx++)
 		nicvf_enable_intr(nic, NICVF_INTR_CQ, qidx);
 
-	/* Init RBDR tasklet and enable RBDR threshold interrupt */
-	tasklet_init(&nic->rbdr_task, nicvf_refill_rbdr, (unsigned long)nic);
-
+	/* Enable RBDR threshold interrupt */
 	for (qidx = 0; qidx < qs->rbdr_cnt; qidx++)
 		nicvf_enable_intr(nic, NICVF_INTR_RBDR, qidx);
 
-	if (nicvf_is_link_active(nic)) {
-		netif_carrier_on(netdev);
-		netif_tx_start_all_queues(netdev);
-	}
+	netif_carrier_on(netdev);
+	netif_tx_start_all_queues(netdev);
 
 	return 0;
+cleanup:
+	nicvf_disable_intr(nic, NICVF_INTR_MBOX, 0);
+	nicvf_unregister_interrupts(nic);
+napi_del:
+	for (qidx = 0; qidx < qs->cq_cnt; qidx++) {
+		cq_poll = nic->napi[qidx];
+		if (!cq_poll)
+			continue;
+		napi_disable(&cq_poll->napi);
+		napi_hash_del(&cq_poll->napi);
+		netif_napi_del(&cq_poll->napi);
+		kfree(cq_poll);
+		nic->napi[qidx] = NULL;
+	}
+	return err;
 }
 
 static int nicvf_update_hw_max_frs(struct nicvf *nic, int mtu)
 {
-	int ret;
-	struct  nic_mbx *mbx;
+	struct nic_mbx mbx = {};
 
-	mbx = nicvf_get_mbx();
-	mbx->msg = NIC_VF_SET_MAX_FRS;
-	mbx->data.max_frs = mtu;
-	ret = nicvf_send_msg_to_pf(nic, mbx);
-	kfree(mbx);
+	mbx.msg = NIC_MBOX_MSG_SET_MAX_FRS;
+	mbx.data.frs.max_frs = mtu;
+	mbx.data.frs.vf_id = nic->vf_id;
 
-	return ret;
+	return nicvf_send_msg_to_pf(nic, &mbx);
 }
 
 static int nicvf_change_mtu(struct net_device *netdev, int new_mtu)
@@ -947,6 +1051,7 @@ static int nicvf_change_mtu(struct net_device *netdev, int new_mtu)
 	if (nicvf_update_hw_max_frs(nic, new_mtu))
 		return -EINVAL;
 	netdev->mtu = new_mtu;
+	nic->mtu = new_mtu;
 
 	return 0;
 }
@@ -959,28 +1064,166 @@ static int nicvf_set_mac_address(struct net_device *netdev, void *p)
 	if (!is_valid_ether_addr(addr->sa_data))
 		return -EADDRNOTAVAIL;
 
-	if (nicvf_hw_set_mac_addr(nic, netdev))
-		return -EBUSY;
-
 	memcpy(netdev->dev_addr, addr->sa_data, netdev->addr_len);
 
+	if (nic->msix_enabled)
+		if (nicvf_hw_set_mac_addr(nic, netdev))
+			return -EBUSY;
+
 	return 0;
 }
 
+static void nicvf_read_bgx_stats(struct nicvf *nic, struct bgx_stats_msg *bgx)
+{
+	if (bgx->rx)
+		nic->bgx_stats.rx_stats[bgx->idx] = bgx->stats;
+	else
+		nic->bgx_stats.tx_stats[bgx->idx] = bgx->stats;
+}
+
+void nicvf_update_lmac_stats(struct nicvf *nic)
+{
+	int stat = 0;
+	struct nic_mbx mbx = {};
+	int timeout;
+
+	if (!netif_running(nic->netdev))
+		return;
+
+	mbx.msg = NIC_MBOX_MSG_BGX_STATS;
+	mbx.data.bgx_stats.vf_id = nic->vf_id;
+	/* Rx stats */
+	mbx.data.bgx_stats.rx = 1;
+	while (stat < BGX_RX_STATS_COUNT) {
+		bgx_stats_acked = 0;
+		mbx.data.bgx_stats.idx = stat;
+		nicvf_send_msg_to_pf(nic, &mbx);
+		timeout = 0;
+		while ((!bgx_stats_acked) && (timeout < 10)) {
+			msleep(2);
+			timeout++;
+		}
+		stat++;
+	}
+
+	stat = 0;
+
+	/* Tx stats */
+	mbx.data.bgx_stats.rx = 0;
+	while (stat < BGX_TX_STATS_COUNT) {
+		bgx_stats_acked = 0;
+		mbx.data.bgx_stats.idx = stat;
+		nicvf_send_msg_to_pf(nic, &mbx);
+		timeout = 0;
+		while ((!bgx_stats_acked) && (timeout < 10)) {
+			msleep(2);
+			timeout++;
+		}
+		stat++;
+	}
+}
+
+void nicvf_update_stats(struct nicvf *nic)
+{
+	int qidx;
+	struct nicvf_hw_stats *stats = &nic->stats;
+	struct nicvf_drv_stats *drv_stats = &nic->drv_stats;
+	struct queue_set *qs = nic->qs;
+
+#define GET_RX_STATS(reg) \
+	nicvf_reg_read(nic, NIC_VNIC_RX_STAT_0_13 | (reg << 3))
+#define GET_TX_STATS(reg) \
+	nicvf_reg_read(nic, NIC_VNIC_TX_STAT_0_4 | (reg << 3))
+
+	stats->rx_bytes_ok = GET_RX_STATS(RX_OCTS);
+	stats->rx_ucast_frames_ok = GET_RX_STATS(RX_UCAST);
+	stats->rx_bcast_frames_ok = GET_RX_STATS(RX_BCAST);
+	stats->rx_mcast_frames_ok = GET_RX_STATS(RX_MCAST);
+	stats->rx_fcs_errors = GET_RX_STATS(RX_FCS);
+	stats->rx_l2_errors = GET_RX_STATS(RX_L2ERR);
+	stats->rx_drop_red = GET_RX_STATS(RX_RED);
+	stats->rx_drop_overrun = GET_RX_STATS(RX_ORUN);
+	stats->rx_drop_bcast = GET_RX_STATS(RX_DRP_BCAST);
+	stats->rx_drop_mcast = GET_RX_STATS(RX_DRP_MCAST);
+	stats->rx_drop_l3_bcast = GET_RX_STATS(RX_DRP_L3BCAST);
+	stats->rx_drop_l3_mcast = GET_RX_STATS(RX_DRP_L3MCAST);
+
+	stats->tx_bytes_ok = GET_TX_STATS(TX_OCTS);
+	stats->tx_ucast_frames_ok = GET_TX_STATS(TX_UCAST);
+	stats->tx_bcast_frames_ok = GET_TX_STATS(TX_BCAST);
+	stats->tx_mcast_frames_ok = GET_TX_STATS(TX_MCAST);
+	stats->tx_drops = GET_TX_STATS(TX_DROP);
+
+	drv_stats->rx_frames_ok = stats->rx_ucast_frames_ok +
+				  stats->rx_bcast_frames_ok +
+				  stats->rx_mcast_frames_ok;
+	drv_stats->tx_frames_ok = stats->tx_ucast_frames_ok +
+				  stats->tx_bcast_frames_ok +
+				  stats->tx_mcast_frames_ok;
+	drv_stats->rx_drops = stats->rx_drop_red +
+			      stats->rx_drop_overrun;
+	drv_stats->tx_drops = stats->tx_drops;
+
+	/* Update RQ and SQ stats */
+	for (qidx = 0; qidx < qs->rq_cnt; qidx++)
+		nicvf_update_rq_stats(nic, qidx);
+	for (qidx = 0; qidx < qs->sq_cnt; qidx++)
+		nicvf_update_sq_stats(nic, qidx);
+}
+
+struct rtnl_link_stats64 *nicvf_get_stats64(struct net_device *netdev,
+					    struct rtnl_link_stats64 *stats)
+{
+	struct nicvf *nic = netdev_priv(netdev);
+	struct nicvf_hw_stats *hw_stats = &nic->stats;
+	struct nicvf_drv_stats *drv_stats = &nic->drv_stats;
+
+	nicvf_update_stats(nic);
+
+	stats->rx_bytes = hw_stats->rx_bytes_ok;
+	stats->rx_packets = drv_stats->rx_frames_ok;
+	stats->rx_dropped = drv_stats->rx_drops;
+
+	stats->tx_bytes = hw_stats->tx_bytes_ok;
+	stats->tx_packets = drv_stats->tx_frames_ok;
+	stats->tx_dropped = drv_stats->tx_drops;
+
+	return stats;
+}
+
+static void nicvf_tx_timeout(struct net_device *dev)
+{
+	struct nicvf *nic = netdev_priv(dev);
+
+	if (netif_msg_tx_err(nic))
+		netdev_warn(dev, "%s: Transmit timed out, resetting\n",
+			    dev->name);
+
+	schedule_work(&nic->reset_task);
+}
+
+static void nicvf_reset_task(struct work_struct *work)
+{
+	struct nicvf *nic;
+
+	nic = container_of(work, struct nicvf, reset_task);
+
+	if (!netif_running(nic->netdev))
+		return;
+
+	nicvf_stop(nic->netdev);
+	nicvf_open(nic->netdev);
+	nic->netdev->trans_start = jiffies;
+}
+
 static const struct net_device_ops nicvf_netdev_ops = {
 	.ndo_open		= nicvf_open,
 	.ndo_stop		= nicvf_stop,
 	.ndo_start_xmit		= nicvf_xmit,
 	.ndo_change_mtu		= nicvf_change_mtu,
 	.ndo_set_mac_address	= nicvf_set_mac_address,
-#if 0
-	.ndo_get_stats64	= nicvf_get_stats,
-	.ndo_validate_addr	= eth_validate_addr,
-	.ndo_set_rx_mode	= nicvf_set_rx_mode,
-	.ndo_vlan_rx_add_vid	= nicvf_vlan_rx_add_vid,
-	.ndo_vlan_rx_kill_vid	= nicvf_vlan_rx_kill_vid,
-	.ndo_tx_timeout		= nicvf_tx_timeout,
-#endif
+	.ndo_get_stats64	= nicvf_get_stats64,
+	.ndo_tx_timeout         = nicvf_tx_timeout,
 };
 
 static int nicvf_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
@@ -988,22 +1231,9 @@ static int nicvf_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 	struct device *dev = &pdev->dev;
 	struct net_device *netdev;
 	struct nicvf *nic;
+	struct queue_set *qs;
 	int    err;
 
-	netdev = alloc_etherdev_mqs(sizeof(struct nicvf),
-			MAX_RCV_QUEUES_PER_QS, MAX_SND_QUEUES_PER_QS);
-
-	if (!netdev)
-		return -ENOMEM;
-
-	pci_set_drvdata(pdev, netdev);
-
-	SET_NETDEV_DEV(netdev, &pdev->dev);
-
-	nic = netdev_priv(netdev);
-	nic->netdev = netdev;
-	nic->pdev = pdev;
-
 	err = pci_enable_device(pdev);
 	if (err) {
 		dev_err(dev, "Failed to enable PCI device\n");
@@ -1017,52 +1247,74 @@ static int nicvf_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 	}
 
 	err = pci_set_dma_mask(pdev, DMA_BIT_MASK(48));
-	if (!err) {
-		err = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(48));
-		if (err) {
-			dev_err(dev, "unable to get 48-bit DMA for consistent allocations\n");
-			goto err_release_regions;
-		}
-	} else {
+	if (err) {
 		dev_err(dev, "Unable to get usable DMA configuration\n");
 		goto err_release_regions;
 	}
 
+	err = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(48));
+	if (err) {
+		dev_err(dev, "unable to get 48-bit DMA for consistent allocations\n");
+		goto err_release_regions;
+	}
+
+	netdev = alloc_etherdev_mqs(sizeof(struct nicvf),
+				    MAX_RCV_QUEUES_PER_QS,
+				    MAX_SND_QUEUES_PER_QS);
+	if (!netdev) {
+		err = -ENOMEM;
+		goto err_release_regions;
+	}
+
+	pci_set_drvdata(pdev, netdev);
+
+	SET_NETDEV_DEV(netdev, &pdev->dev);
+
+	nic = netdev_priv(netdev);
+	nic->netdev = netdev;
+	nic->pdev = pdev;
+
 	/* MAP VF's configuration registers */
-	nic->reg_base = (uint64_t)pci_ioremap_bar(pdev, PCI_CFG_REG_BAR_NUM);
+	nic->reg_base = (u64)pci_ioremap_bar(pdev, PCI_CFG_REG_BAR_NUM);
 	if (!nic->reg_base) {
 		dev_err(dev, "Cannot map config register space, aborting\n");
 		err = -ENOMEM;
 		goto err_release_regions;
 	}
 
-#ifdef VNIC_RX_CSUM_OFFLOAD_SUPPORT
-	netdev->hw_features |= NETIF_F_RXCSUM;
-#endif
-#ifdef VNIC_TX_CSUM_OFFLOAD_SUPPORT
-	netdev->hw_features |= NETIF_F_IP_CSUM;
-#endif
-#ifdef VNIC_SG_SUPPORT
-	netdev->hw_features |= NETIF_F_SG;
-#endif
-#ifdef VNIC_TSO_SUPPORT
-	netdev->hw_features |= NETIF_F_TSO | NETIF_F_SG | NETIF_F_IP_CSUM;
-#endif
-#ifdef VNIC_HW_LRO_SUPPORT
-	netdev->hw_features |= NETIF_F_LRO;
-#endif
+	err = nicvf_set_qset_resources(nic);
+	if (err)
+		goto err_unmap_resources;
+
+	qs = nic->qs;
+
+	err = nicvf_set_real_num_queues(netdev, qs->sq_cnt, qs->rq_cnt);
+	if (err)
+		goto err_unmap_resources;
+
+	/* Check if PF is alive and get MAC address for this VF */
+	err = nicvf_register_misc_interrupt(nic);
+	if (err)
+		goto err_unmap_resources;
+
+	netdev->features |= (NETIF_F_RXCSUM | NETIF_F_IP_CSUM | NETIF_F_SG |
+			     NETIF_F_GSO | NETIF_F_GRO);
+	netdev->hw_features = netdev->features;
 
-	netdev->features |= netdev->hw_features;
 	netdev->netdev_ops = &nicvf_netdev_ops;
 
-	if ((err = register_netdev(netdev))) {
+	INIT_WORK(&nic->reset_task, nicvf_reset_task);
+
+	err = register_netdev(netdev);
+	if (err) {
 		dev_err(dev, "Failed to register netdevice\n");
 		goto err_unmap_resources;
 	}
 
-#ifdef NICVF_ETHTOOL_ENABLE
+	nic->msg_enable = debug;
+
 	nicvf_set_ethtool_ops(netdev);
-#endif
+
 	goto exit;
 
 err_unmap_resources:
@@ -1092,6 +1344,9 @@ static void nicvf_remove(struct pci_dev *pdev)
 	if (nic->reg_base)
 		iounmap((void *)nic->reg_base);
 
+	/* Free Qset */
+	kfree(nic->qs);
+
 	pci_release_regions(pdev);
 	pci_disable_device(pdev);
 	free_netdev(netdev);
@@ -1118,4 +1373,3 @@ static void __exit nicvf_cleanup_module(void)
 
 module_init(nicvf_init_module);
 module_exit(nicvf_cleanup_module);
-
diff --git a/drivers/net/ethernet/cavium/thunder/nicvf_queues.c b/drivers/net/ethernet/cavium/thunder/nicvf_queues.c
index ad0d80c..c6fde30 100644
--- a/drivers/net/ethernet/cavium/thunder/nicvf_queues.c
+++ b/drivers/net/ethernet/cavium/thunder/nicvf_queues.c
@@ -1,146 +1,204 @@
 /*
- * This file is subject to the terms and conditions of the GNU General Public
- * License.  See the file "COPYING" in the main directory of this archive
- * for more details.
+ * Copyright (C) 2015 Cavium, Inc.
  *
- * Copyright (C) 2014 Cavium, Inc.
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of
+ * the License, or (at your option) any later version.
  */
 
-#include <linux/kernel.h>
-#include <linux/errno.h>
-#include <linux/types.h>
 #include <linux/pci.h>
 #include <linux/netdevice.h>
-#include <linux/if_vlan.h>
 #include <linux/ip.h>
-#include <net/checksum.h>
+#include <linux/etherdevice.h>
 
 #include "nic_reg.h"
 #include "nic.h"
 #include "q_struct.h"
 #include "nicvf_queues.h"
 
-#define  MIN_SND_QUEUE_DESC_FOR_PKT_XMIT 2
+struct rbuf_info {
+	void	*data;
+	u64	offset;
+};
 
-static int nicvf_alloc_q_desc_mem(struct nicvf *nic,
-				struct q_desc_mem *desc_mem,
-				int q_len, int desc_size, int align_bytes)
+#define GET_RBUF_INFO(x) ((struct rbuf_info *)(x - NICVF_RCV_BUF_ALIGN_BYTES))
+
+/* Poll a register for a specific value */
+static int nicvf_poll_reg(struct nicvf *nic, int qidx,
+			  u64 reg, int bit_pos, int bits, int val)
+{
+	u64 bit_mask;
+	u64 reg_val;
+	int timeout = 10;
+
+	bit_mask = (1ULL << bits) - 1;
+	bit_mask = (bit_mask << bit_pos);
+
+	while (timeout) {
+		reg_val = nicvf_queue_reg_read(nic, reg, qidx);
+		if (((reg_val & bit_mask) >> bit_pos) == val)
+			return 0;
+		usleep_range(1000, 2000);
+		timeout--;
+	}
+	netdev_err(nic->netdev, "Poll on reg 0x%llx failed\n", reg);
+	return 1;
+}
+
+/* Allocate memory for a queue's descriptors */
+static int nicvf_alloc_q_desc_mem(struct nicvf *nic, struct q_desc_mem *dmem,
+				  int q_len, int desc_size, int align_bytes)
 {
-	desc_mem->q_len = q_len;
-	desc_mem->size = (desc_size * q_len) + align_bytes;
-	desc_mem->unalign_base = dma_alloc_coherent(&nic->pdev->dev, desc_mem->size,
-							&desc_mem->dma, GFP_ATOMIC);
-	if (!desc_mem->unalign_base)
+	dmem->q_len = q_len;
+	dmem->size = (desc_size * q_len) + align_bytes;
+	/* Save address, need it while freeing */
+	dmem->unalign_base = dma_zalloc_coherent(&nic->pdev->dev, dmem->size,
+						&dmem->dma, GFP_KERNEL);
+	if (!dmem->unalign_base)
 		return -1;
 
-	desc_mem->phys_base = NICVF_ALIGNED_ADDR((uint64_t)desc_mem->dma, align_bytes);
-	desc_mem->base = (void *)((u8 *)desc_mem->unalign_base +
-					(desc_mem->phys_base - desc_mem->dma));
+	/* Align memory address for 'align_bytes' */
+	dmem->phys_base = NICVF_ALIGNED_ADDR((u64)dmem->dma, align_bytes);
+	dmem->base = (void *)((u8 *)dmem->unalign_base +
+			      (dmem->phys_base - dmem->dma));
 	return 0;
 }
 
-static void nicvf_free_q_desc_mem(struct nicvf *nic, struct q_desc_mem *desc_mem)
+/* Free queue's descriptor memory */
+static void nicvf_free_q_desc_mem(struct nicvf *nic, struct q_desc_mem *dmem)
 {
-	if (!desc_mem)
+	if (!dmem)
 		return;
 
-	dma_free_coherent(&nic->pdev->dev, desc_mem->size,
-			desc_mem->unalign_base, desc_mem->dma);
-	desc_mem->unalign_base = NULL;
-	desc_mem->base = NULL;
+	dma_free_coherent(&nic->pdev->dev, dmem->size,
+			  dmem->unalign_base, dmem->dma);
+	dmem->unalign_base = NULL;
+	dmem->base = NULL;
 }
 
-static int nicvf_alloc_rcv_buffer(struct nicvf *nic, uint64_t buf_len,
-					unsigned char **rcv_buffer)
+/* Allocate buffer for packet reception
+ * HW returns memory address where packet is DMA'ed but not a pointer
+ * into RBDR ring, so save buffer address at the start of fragment and
+ * align the start address to a cache aligned address
+ */
+static inline int nicvf_alloc_rcv_buffer(struct nicvf *nic,
+					 u64 buf_len, u64 **rbuf)
 {
-	struct sk_buff *skb = NULL;
-
-	buf_len += NICVF_RCV_BUF_ALIGN_BYTES + sizeof(void *);
+	u64 data;
+	struct rbuf_info *rinfo;
 
-	if (!(skb = netdev_alloc_skb(nic->netdev, buf_len))) {
+	/* SKB is built after packet is received */
+	data = (u64)netdev_alloc_frag(buf_len);
+	if (!data) {
 		netdev_err(nic->netdev, "Failed to allocate new rcv buffer\n");
 		return -ENOMEM;
 	}
 
-	/* Reserve bytes for storing skb address */
-	skb_reserve(skb, sizeof(void *));
 	/* Align buffer addr to cache line i.e 128 bytes */
-	skb_reserve(skb, NICVF_RCV_BUF_ALIGN_LEN((uint64_t)skb->data));
+	rinfo = (struct rbuf_info *)(data + NICVF_RCV_BUF_ALIGN_LEN(data));
+	/* Store start address for later retrieval */
+	rinfo->data = (void *)data;
+	/* Store alignment offset */
+	rinfo->offset = NICVF_RCV_BUF_ALIGN_LEN(data);
 
-	/* Store skb address */
-	*(struct sk_buff **)(skb->data - sizeof(void *)) = skb;
+	data += rinfo->offset;
 
-	/* Return buffer address */
-	*rcv_buffer = skb->data;
+	/* Give next aligned address to hw for DMA */
+	*rbuf = (u64 *)(data + NICVF_RCV_BUF_ALIGN_BYTES);
 	return 0;
 }
 
-static struct sk_buff *nicvf_rb_ptr_to_skb(uint64_t rb_ptr)
+/* Retrieve actual buffer start address and build skb for received packet */
+static struct sk_buff *nicvf_rb_ptr_to_skb(struct nicvf *nic, u64 rb_ptr)
 {
 	struct sk_buff *skb;
+	struct rbuf_info *rinfo;
+
+	rb_ptr = (u64)phys_to_virt(rb_ptr);
+	/* Get buffer start address and alignment offset */
+	rinfo = GET_RBUF_INFO(rb_ptr);
+
+	/* Now build an skb to give to stack */
+	skb = build_skb(rinfo->data, RCV_FRAG_LEN);
+	if (!skb) {
+		put_page(virt_to_head_page(rinfo->data));
+		return NULL;
+	}
+	/* Set correct skb->data */
+	skb_reserve(skb, rinfo->offset + NICVF_RCV_BUF_ALIGN_BYTES);
 
-	rb_ptr = (uint64_t)phys_to_virt(rb_ptr);
-	skb = (struct sk_buff *)*(uint64_t *)(rb_ptr - sizeof(void *));
+	prefetch((void *)rb_ptr);
 	return skb;
 }
 
+/* Allocate RBDR ring and populate receive buffers */
 static int  nicvf_init_rbdr(struct nicvf *nic, struct rbdr *rbdr,
-						int ring_len, int buf_size)
+			    int ring_len, int buf_size)
 {
 	int idx;
-	unsigned char *rcv_buffer;
+	u64 *rbuf;
+	struct rbdr_entry_t *desc;
 
-	if (nicvf_alloc_q_desc_mem(nic, &rbdr->desc_mem, ring_len,
-				sizeof(struct rbdr_entry_t), NICVF_RCV_BUF_ALIGN_BYTES)) {
+	if (nicvf_alloc_q_desc_mem(nic, &rbdr->dmem, ring_len,
+				   sizeof(struct rbdr_entry_t),
+				   NICVF_RCV_BUF_ALIGN_BYTES)) {
 		netdev_err(nic->netdev,
-			"Unable to allocate memory for rcv buffer ring\n");
+			   "Unable to allocate memory for rcv buffer ring\n");
 		return -ENOMEM;
 	}
 
+	rbdr->desc = rbdr->dmem.base;
 	/* Buffer size has to be in multiples of 128 bytes */
-	rbdr->buf_size = buf_size;
+	rbdr->dma_size = buf_size;
 	rbdr->enable = true;
-	rbdr->thresh = ring_len / 2;
+	rbdr->thresh = RBDR_THRESH;
 
 	for (idx = 0; idx < ring_len; idx++) {
-		rbdr->desc[idx] = &((struct rbdr_entry_t *)rbdr->desc_mem.base)[idx];
-
-		if (nicvf_alloc_rcv_buffer(nic, rbdr->buf_size, &rcv_buffer))
+		if (nicvf_alloc_rcv_buffer(nic, RCV_FRAG_LEN, &rbuf))
 			return -ENOMEM;
 
-		rbdr->desc[idx]->buf_addr = pci_map_single(nic->pdev, rcv_buffer,
-				rbdr->buf_size, PCI_DMA_FROMDEVICE) >> NICVF_RCV_BUF_ALIGN;
+		desc = GET_RBDR_DESC(rbdr, idx);
+		desc->buf_addr = virt_to_phys(rbuf) >> NICVF_RCV_BUF_ALIGN;
 	}
 	return 0;
 }
 
-static void nicvf_free_rbdr(struct nicvf *nic, struct rbdr *rbdr, int rbdr_qidx)
+/* Free RBDR ring and its receive buffers */
+static void nicvf_free_rbdr(struct nicvf *nic, struct rbdr *rbdr)
 {
 	int head, tail;
-	struct sk_buff *skb;
+	u64 buf_addr;
+	struct rbdr_entry_t *desc;
+	struct rbuf_info *rinfo;
 
 	if (!rbdr)
 		return;
 
 	rbdr->enable = false;
-	if (!rbdr->desc_mem.base)
+	if (!rbdr->dmem.base)
 		return;
 
-	head = nicvf_queue_reg_read(nic, NIC_QSET_RBDR_0_1_HEAD,
-							rbdr_qidx) >> 3;
-	tail = nicvf_queue_reg_read(nic, NIC_QSET_RBDR_0_1_TAIL,
-							rbdr_qidx) >> 3;
+	head = rbdr->head;
+	tail = rbdr->tail;
+
 	/* Free SKBs */
 	while (head != tail) {
-		skb = nicvf_rb_ptr_to_skb(rbdr->desc[head]->buf_addr << NICVF_RCV_BUF_ALIGN);
-		pci_unmap_single(nic->pdev, (dma_addr_t)skb->data,
-				rbdr->buf_size, PCI_DMA_FROMDEVICE);
-		dev_kfree_skb(skb);
+		desc = GET_RBDR_DESC(rbdr, head);
+		buf_addr = desc->buf_addr << NICVF_RCV_BUF_ALIGN;
+		rinfo = GET_RBUF_INFO((u64)phys_to_virt(buf_addr));
+		put_page(virt_to_head_page(rinfo->data));
 		head++;
-		head &= (rbdr->desc_mem.q_len - 1);
+		head &= (rbdr->dmem.q_len - 1);
 	}
+	/* Free SKB of tail desc */
+	desc = GET_RBDR_DESC(rbdr, tail);
+	buf_addr = desc->buf_addr << NICVF_RCV_BUF_ALIGN;
+	rinfo = GET_RBUF_INFO((u64)phys_to_virt(buf_addr));
+	put_page(virt_to_head_page(rinfo->data));
+
 	/* Free RBDR ring */
-	nicvf_free_q_desc_mem(nic, &rbdr->desc_mem);
+	nicvf_free_q_desc_mem(nic, &rbdr->dmem);
 }
 
 /* Refill receive buffer descriptors with new buffers.
@@ -151,10 +209,11 @@ void nicvf_refill_rbdr(unsigned long data)
 	struct nicvf *nic = (struct nicvf *)data;
 	struct queue_set *qs = nic->qs;
 	int rbdr_idx = qs->rbdr_cnt;
-	int tail_ptr, qcount;
-	int refill_rb_cnt, rb_cnt = 0;
+	int tail, qcount;
+	int refill_rb_cnt;
 	struct rbdr *rbdr;
-	unsigned char *rcv_buffer;
+	struct rbdr_entry_t *desc;
+	u64 *rbuf;
 
 refill:
 	if (!rbdr_idx)
@@ -165,35 +224,29 @@ refill:
 	if (!rbdr->enable)
 		goto next_rbdr;
 
-	/* check if valid descs reached or crossed threshold level */
-	qcount = nicvf_queue_reg_read(nic, NIC_QSET_RBDR_0_1_STATUS0,
-							rbdr_idx) & 0x7FFFF;
-	if (qcount > rbdr->thresh)
-		goto next_rbdr;
+	/* Get no of desc's to be refilled */
+	qcount = nicvf_queue_reg_read(nic, NIC_QSET_RBDR_0_1_STATUS0, rbdr_idx);
+	qcount &= 0x7FFFF;
 
 	/* Get no of desc's to be refilled */
-	refill_rb_cnt = rbdr->thresh;
-	rb_cnt = refill_rb_cnt;
+	refill_rb_cnt = qs->rbdr_len - qcount - 1;
 
 	/* Start filling descs from tail */
-	tail_ptr = nicvf_queue_reg_read(nic, NIC_QSET_RBDR_0_1_TAIL,
-							rbdr_idx) >> 3;
+	tail = nicvf_queue_reg_read(nic, NIC_QSET_RBDR_0_1_TAIL, rbdr_idx) >> 3;
 	while (refill_rb_cnt) {
-		tail_ptr++;
-		tail_ptr &= (rbdr->desc_mem.q_len - 1);
+		tail++;
+		tail &= (rbdr->dmem.q_len - 1);
 
-		if (nicvf_alloc_rcv_buffer(nic, rbdr->buf_size, &rcv_buffer))
+		if (nicvf_alloc_rcv_buffer(nic, RCV_FRAG_LEN, &rbuf))
 			break;
 
-		rbdr->desc[tail_ptr]->buf_addr = pci_map_single(nic->pdev,
-						rcv_buffer, rbdr->buf_size,
-				PCI_DMA_FROMDEVICE) >> NICVF_RCV_BUF_ALIGN;
+		desc = GET_RBDR_DESC(rbdr, tail);
+		desc->buf_addr = virt_to_phys(rbuf) >> NICVF_RCV_BUF_ALIGN;
 		refill_rb_cnt--;
 	}
 	/* Notify HW */
-	if (rb_cnt)
-		nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_DOOR,
-						rbdr_idx, rb_cnt);
+	nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_DOOR,
+			      rbdr_idx, qs->rbdr_len - qcount - 1);
 next_rbdr:
 	if (rbdr_idx)
 		goto refill;
@@ -203,31 +256,20 @@ next_rbdr:
 		nicvf_enable_intr(nic, NICVF_INTR_RBDR, rbdr_idx);
 }
 
-static int nicvf_init_rcv_queue(struct rcv_queue *rq)
-{
-	/* Nothing to do */
-	return 0;
-}
-
-static void nicvf_free_rcv_queue(struct rcv_queue *rq)
-{
-	/* Nothing to do */
-}
-
-/* TBD: how to handle full packets received in CQ
- * i.e conversion of buffers into SKBs
- */
-static int nicvf_init_cmp_queue(struct nicvf *nic, struct cmp_queue *cq,
-								int q_len)
+/* Initialize completion queue */
+static int nicvf_init_cmp_queue(struct nicvf *nic,
+				struct cmp_queue *cq, int q_len)
 {
-	if (nicvf_alloc_q_desc_mem(nic, &cq->desc_mem, q_len,
-				CMP_QUEUE_DESC_SIZE, NICVF_CQ_BASE_ALIGN_BYTES)) {
+	if (nicvf_alloc_q_desc_mem(nic, &cq->dmem, q_len,
+				   CMP_QUEUE_DESC_SIZE,
+				   NICVF_CQ_BASE_ALIGN_BYTES)) {
 		netdev_err(nic->netdev,
-			"Unable to allocate memory for Tx/Rx notification queue\n");
+			   "Unable to allocate memory for completion queue\n");
 		return -ENOMEM;
 	}
-	cq->intr_timer_thresh = 0;
-	cq->thresh = CMP_QUEUE_THRESH;
+	cq->desc = cq->dmem.base;
+	cq->thresh = CMP_QUEUE_CQE_THRESH;
+	nic->cq_coalesce_usecs = CMP_QUEUE_TIMER_THRESH;
 
 	return 0;
 }
@@ -236,26 +278,29 @@ static void nicvf_free_cmp_queue(struct nicvf *nic, struct cmp_queue *cq)
 {
 	if (!cq)
 		return;
-	if (!cq->desc_mem.base)
+	if (!cq->dmem.base)
 		return;
 
-	nicvf_free_q_desc_mem(nic, &cq->desc_mem);
+	nicvf_free_q_desc_mem(nic, &cq->dmem);
 }
 
-static int nicvf_init_snd_queue(struct nicvf *nic, struct snd_queue *sq,
-								int q_len)
+/* Initialize transmit queue */
+static int nicvf_init_snd_queue(struct nicvf *nic,
+				struct snd_queue *sq, int q_len)
 {
-	if (nicvf_alloc_q_desc_mem(nic, &sq->desc_mem, q_len,
-				SND_QUEUE_DESC_SIZE, NICVF_SQ_BASE_ALIGN_BYTES)) {
+	if (nicvf_alloc_q_desc_mem(nic, &sq->dmem, q_len,
+				   SND_QUEUE_DESC_SIZE,
+				   NICVF_SQ_BASE_ALIGN_BYTES)) {
 		netdev_err(nic->netdev,
-			"Unable to allocate memory for transmit queue\n");
+			   "Unable to allocate memory for send queue\n");
 		return -ENOMEM;
 	}
 
-	sq->skbuff = kcalloc(q_len, sizeof(uint64_t), GFP_ATOMIC);
+	sq->desc = sq->dmem.base;
+	sq->skbuff = kcalloc(q_len, sizeof(u64), GFP_ATOMIC);
 	sq->head = 0;
 	sq->tail = 0;
-	sq->free_cnt = q_len;
+	atomic_set(&sq->free_cnt, q_len - 1);
 	sq->thresh = SND_QUEUE_THRESH;
 
 	return 0;
@@ -265,24 +310,112 @@ static void nicvf_free_snd_queue(struct nicvf *nic, struct snd_queue *sq)
 {
 	if (!sq)
 		return;
-	if (!sq->desc_mem.base)
+	if (!sq->dmem.base)
 		return;
 
 	kfree(sq->skbuff);
-	nicvf_free_q_desc_mem(nic, &sq->desc_mem);
+	nicvf_free_q_desc_mem(nic, &sq->dmem);
+}
+
+static void nicvf_reclaim_snd_queue(struct nicvf *nic,
+				    struct queue_set *qs, int qidx)
+{
+	/* Disable send queue */
+	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_CFG, qidx, 0);
+	/* Check if SQ is stopped */
+	if (nicvf_poll_reg(nic, qidx, NIC_QSET_SQ_0_7_STATUS, 21, 1, 0x01))
+		return;
+	/* Reset send queue */
+	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_CFG, qidx, NICVF_SQ_RESET);
+}
+
+static void nicvf_reclaim_rcv_queue(struct nicvf *nic,
+				    struct queue_set *qs, int qidx)
+{
+	struct nic_mbx mbx = {};
+
+	/* Make sure all packets in the pipeline are written back into mem */
+	mbx.msg = NIC_MBOX_MSG_RQ_SW_SYNC;
+	mbx.data.rq.cfg = 0;
+	nicvf_send_msg_to_pf(nic, &mbx);
 }
 
+static void nicvf_reclaim_cmp_queue(struct nicvf *nic,
+				    struct queue_set *qs, int qidx)
+{
+	/* Disable timer threshold (doesn't get reset upon CQ reset */
+	nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_CFG2, qidx, 0);
+	/* Disable completion queue */
+	nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_CFG, qidx, 0);
+	/* Reset completion queue */
+	nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_CFG, qidx, NICVF_CQ_RESET);
+}
+
+static void nicvf_reclaim_rbdr(struct nicvf *nic,
+			       struct rbdr *rbdr, int qidx)
+{
+	u64 tmp;
+	int timeout = 10;
+
+	/* Save head and tail pointers for feeing up buffers */
+	rbdr->head = nicvf_queue_reg_read(nic,
+					  NIC_QSET_RBDR_0_1_HEAD,
+					  qidx) >> 3;
+	rbdr->tail = nicvf_queue_reg_read(nic,
+					  NIC_QSET_RBDR_0_1_TAIL,
+					  qidx) >> 3;
+
+	/* Disable RBDR */
+	nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_CFG, qidx, 0);
+	if (nicvf_poll_reg(nic, qidx, NIC_QSET_RBDR_0_1_STATUS0, 62, 2, 0x00))
+		return;
+	while (1) {
+		tmp = nicvf_queue_reg_read(nic,
+					   NIC_QSET_RBDR_0_1_PREFETCH_STATUS,
+					   qidx);
+		if ((tmp & 0xFFFFFFFF) == ((tmp >> 32) & 0xFFFFFFFF))
+			break;
+		usleep_range(1000, 2000);
+		timeout--;
+		if (!timeout) {
+			netdev_err(nic->netdev,
+				   "Failed polling on prefetch status\n");
+			return;
+		}
+	}
+	nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_CFG,
+			      qidx, NICVF_RBDR_RESET);
+
+	if (nicvf_poll_reg(nic, qidx, NIC_QSET_RBDR_0_1_STATUS0, 62, 2, 0x02))
+		return;
+	nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_CFG, qidx, 0x00);
+	if (nicvf_poll_reg(nic, qidx, NIC_QSET_RBDR_0_1_STATUS0, 62, 2, 0x00))
+		return;
+}
+
+/* Configures receive queue */
 static void nicvf_rcv_queue_config(struct nicvf *nic, struct queue_set *qs,
-							int qidx, bool enable)
+				   int qidx, bool enable)
 {
-	struct  nic_mbx *mbx;
+	struct nic_mbx mbx = {};
 	struct rcv_queue *rq;
+	struct cmp_queue *cq;
+	struct rq_cfg rq_cfg;
+#ifdef CONFIG_RPS
+	struct netdev_rx_queue *rxqueue;
+	struct rps_map *map, *oldmap;
+	cpumask_var_t rps_mask;
+	int i, cpu;
+#endif
 
 	rq = &qs->rq[qidx];
+	rq->enable = enable;
+
+	/* Disable receive queue */
+	nicvf_queue_reg_write(nic, NIC_QSET_RQ_0_7_CFG, qidx, 0);
 
-	if (!enable) {
-		/* Disable receive queue */
-		nicvf_queue_reg_write(nic, NIC_QSET_RQ_0_7_CFG, qidx, 0);
+	if (!rq->enable) {
+		nicvf_reclaim_rcv_queue(nic, qs, qidx);
 		return;
 	}
 
@@ -292,162 +425,225 @@ static void nicvf_rcv_queue_config(struct nicvf *nic, struct queue_set *qs,
 	rq->start_qs_rbdr_idx = qs->rbdr_cnt - 1;
 	rq->cont_rbdr_qs = qs->vnic_id;
 	rq->cont_qs_rbdr_idx = qs->rbdr_cnt - 1;
+	/* all writes of RBDR data to be loaded into L2 Cache as well*/
+	rq->caching = 1;
 
 	/* Send a mailbox msg to PF to config RQ */
-	mbx = nicvf_get_mbx();
-	mbx->msg = NIC_PF_VF_MSG_RQ_CFG;
-	mbx->data.rq.qs_num = qs->vnic_id;
-	mbx->data.rq.rq_num = qidx;
-	mbx->data.rq.cfg = (rq->cq_qs << 19) | (rq->cq_idx << 16) |
-			(rq->cont_rbdr_qs << 9) | (rq->cont_qs_rbdr_idx << 8) |
-			(rq->start_rbdr_qs << 1) | (rq->start_qs_rbdr_idx);
-	nicvf_send_msg_to_pf(nic, mbx);
+	mbx.msg = NIC_MBOX_MSG_RQ_CFG;
+	mbx.data.rq.qs_num = qs->vnic_id;
+	mbx.data.rq.rq_num = qidx;
+	mbx.data.rq.cfg = (rq->caching << 26) | (rq->cq_qs << 19) |
+			  (rq->cq_idx << 16) | (rq->cont_rbdr_qs << 9) |
+			  (rq->cont_qs_rbdr_idx << 8) |
+			  (rq->start_rbdr_qs << 1) | (rq->start_qs_rbdr_idx);
+	nicvf_send_msg_to_pf(nic, &mbx);
+
+	mbx.msg = NIC_MBOX_MSG_RQ_BP_CFG;
+	mbx.data.rq.cfg = (1ULL << 63) | (1ULL << 62) | (qs->vnic_id << 0);
+	nicvf_send_msg_to_pf(nic, &mbx);
 
 	/* RQ drop config
 	 * Enable CQ drop to reserve sufficient CQEs for all tx packets
 	 */
-	mbx->msg = NIC_PF_VF_MSG_RQ_DROP_CFG;
-	mbx->data.rq.cfg = (1ULL << 62) | (RQ_CQ_DROP << 8);
-	nicvf_send_msg_to_pf(nic, mbx);
+	mbx.msg = NIC_MBOX_MSG_RQ_DROP_CFG;
+	mbx.data.rq.cfg = (1ULL << 62) | (RQ_CQ_DROP << 8);
+	nicvf_send_msg_to_pf(nic, &mbx);
+
+	nicvf_queue_reg_write(nic, NIC_QSET_RQ_GEN_CFG, qidx, 0x00);
 
 	/* Enable Receive queue */
-	nicvf_queue_reg_write(nic, NIC_QSET_RQ_0_7_CFG, qidx, (1ULL << 1));
+	rq_cfg.ena = 1;
+	rq_cfg.tcp_ena = 0;
+	nicvf_queue_reg_write(nic, NIC_QSET_RQ_0_7_CFG, qidx, *(u64 *)&rq_cfg);
 
-	kfree(mbx);
+#ifdef CONFIG_RPS
+	cq = &qs->cq[qidx];
+	/* Set RPS CPU map */
+	cpumask_copy(rps_mask, cpu_online_mask);
+	cpumask_clear_cpu(cpumask_first(&cq->affinity_mask), rps_mask);
+
+	rxqueue = nic->netdev->_rx + qidx;
+	oldmap = rcu_dereference(rxqueue->rps_map);
+
+	map = kzalloc(max_t(unsigned int,
+			    RPS_MAP_SIZE(cpumask_weight(rps_mask)),
+			    L1_CACHE_BYTES), GFP_KERNEL);
+	if (!map)
+		return;
+
+	i = 0;
+	for_each_cpu_and(cpu, rps_mask, cpu_online_mask)
+		map->cpus[i++] = cpu;
+	map->len = i;
+
+	static_key_slow_inc(&rps_needed);
+	rcu_assign_pointer(rxqueue->rps_map, map);
+
+	if (oldmap) {
+		kfree_rcu(oldmap, rcu);
+		static_key_slow_dec(&rps_needed);
+	}
+#endif
 }
 
+/* Configures completion queue */
 void nicvf_cmp_queue_config(struct nicvf *nic, struct queue_set *qs,
-						int qidx, bool enable)
+			    int qidx, bool enable)
 {
 	struct cmp_queue *cq;
+	struct cq_cfg cq_cfg;
 
 	cq = &qs->cq[qidx];
-	if (!enable) {
-		/* Disable completion queue */
-		nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_CFG, qidx, 0);
+	cq->enable = enable;
+
+	if (!cq->enable) {
+		nicvf_reclaim_cmp_queue(nic, qs, qidx);
 		return;
 	}
 
 	/* Reset completion queue */
-	nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_CFG, qidx, (1ULL << 41));
+	nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_CFG, qidx, NICVF_CQ_RESET);
 
-	/* Enable Completion queue */
-	nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_CFG, qidx,
-			(1ULL << 42) | (((qs->cq_len >> 10) - 1) << 32));
-
-	/* Set threshold value for interrupt generation */
-	nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_THRESH, qidx,
-							cq->thresh);
+	if (!cq->enable)
+		return;
 
+	spin_lock_init(&cq->lock);
 	/* Set completion queue base address */
-	nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_BASE, qidx,
-					(uint64_t)(cq->desc_mem.phys_base));
+	nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_BASE,
+			      qidx, (u64)(cq->dmem.phys_base));
 
-	/* Set CQ's head entry */
-	nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_HEAD, qidx, 0);
+	/* Enable Completion queue */
+	cq_cfg.ena = 1;
+	cq_cfg.reset = 0;
+	cq_cfg.caching = 0;
+	cq_cfg.qsize = CMP_QSIZE;
+	cq_cfg.avg_con = 0;
+	nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_CFG, qidx, *(u64 *)&cq_cfg);
+
+	/* Set threshold value for interrupt generation */
+	nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_THRESH, qidx, cq->thresh);
+	nicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_CFG2,
+			      qidx, nic->cq_coalesce_usecs);
 }
 
-/* TBD
- * - Set TL3 index
- */
+/* Configures transmit queue */
 static void nicvf_snd_queue_config(struct nicvf *nic, struct queue_set *qs,
-							int qidx, bool enable)
+				   int qidx, bool enable)
 {
+	struct nic_mbx mbx = {};
 	struct snd_queue *sq;
-	struct nic_mbx *mbx;
+	struct sq_cfg sq_cfg;
 
 	sq = &qs->sq[qidx];
-	if (!enable) {
-		/* Disable send queue */
-		nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_CFG, qidx, 0);
+	sq->enable = enable;
+
+	if (!sq->enable) {
+		nicvf_reclaim_snd_queue(nic, qs, qidx);
 		return;
 	}
 
 	/* Reset send queue */
-	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_CFG, qidx, (1ULL << 17));
+	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_CFG, qidx, NICVF_SQ_RESET);
 
 	sq->cq_qs = qs->vnic_id;
 	sq->cq_idx = qidx;
 
 	/* Send a mailbox msg to PF to config SQ */
-	mbx = nicvf_get_mbx();
-	mbx->msg = NIC_PF_VF_MSG_SQ_CFG;
-	mbx->data.sq.qs_num = qs->vnic_id;
-	mbx->data.sq.sq_num = qidx;
-	mbx->data.sq.cfg = (sq->cq_qs << 3) | sq->cq_idx;
-	nicvf_send_msg_to_pf(nic, mbx);
+	mbx.msg = NIC_MBOX_MSG_SQ_CFG;
+	mbx.data.sq.qs_num = qs->vnic_id;
+	mbx.data.sq.sq_num = qidx;
+	mbx.data.sq.cfg = (sq->cq_qs << 3) | sq->cq_idx;
+	nicvf_send_msg_to_pf(nic, &mbx);
+
+	/* Set queue base address */
+	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_BASE,
+			      qidx, (u64)(sq->dmem.phys_base));
 
 	/* Enable send queue  & set queue size */
-	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_CFG, qidx,
-			(1ULL << 19) | (((qs->sq_len >> 10) - 1) << 8));
+	sq_cfg.ena = 1;
+	sq_cfg.reset = 0;
+	sq_cfg.ldwb = 0;
+	sq_cfg.qsize = SND_QSIZE;
+	sq_cfg.tstmp_bgx_intf = 0;
+	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_CFG, qidx, *(u64 *)&sq_cfg);
 
 	/* Set threshold value for interrupt generation */
-	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_THRESH, qidx,
-							sq->thresh);
+	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_THRESH, qidx, sq->thresh);
 
-	/* Set queue base address */
-	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_BASE, qidx,
-					(uint64_t)(sq->desc_mem.phys_base));
-
-	/* Set SQ's head entry */
-	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_HEAD, qidx, 0);
-	kfree(mbx);
+	/* Set queue:cpu affinity for better load distribution */
+	if (cpu_online(qidx)) {
+		cpumask_set_cpu(qidx, &sq->affinity_mask);
+		netif_set_xps_queue(nic->netdev,
+				    &sq->affinity_mask, qidx);
+	}
 }
 
+/* Configures receive buffer descriptor ring */
 static void nicvf_rbdr_config(struct nicvf *nic, struct queue_set *qs,
-						int qidx, bool enable)
+			      int qidx, bool enable)
 {
 	struct rbdr *rbdr;
+	struct rbdr_cfg rbdr_cfg;
 
 	rbdr = &qs->rbdr[qidx];
-	if (!enable) {
-		/* Disable RBDR */
-		nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_CFG, qidx, 0);
+	nicvf_reclaim_rbdr(nic, rbdr, qidx);
+	if (!enable)
 		return;
-	}
 
-	/* Reset RBDR */
-	nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_CFG, qidx, (1ULL << 43));
+	/* Set descriptor base address */
+	nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_BASE,
+			      qidx, (u64)(rbdr->dmem.phys_base));
 
 	/* Enable RBDR  & set queue size */
 	/* Buffer size should be in multiples of 128 bytes */
-	nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_CFG, qidx, (1ULL << 44) |
-		(((qs->rbdr_len >> 13) - 1) << 32) | (rbdr->buf_size / 128));
-
-	/* Set descriptor base address */
-	nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_BASE, qidx,
-					(uint64_t)(rbdr->desc_mem.phys_base));
-
-	/* Set RBDR head entry */
-	nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_HEAD, qidx, 0);
+	rbdr_cfg.ena = 1;
+	rbdr_cfg.reset = 0;
+	rbdr_cfg.ldwb = 0;
+	rbdr_cfg.qsize = RBDR_SIZE;
+	rbdr_cfg.avg_con = 0;
+	rbdr_cfg.lines = rbdr->dma_size / 128;
+	nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_CFG,
+			      qidx, *(u64 *)&rbdr_cfg);
 
 	/* Notify HW */
-	nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_DOOR, qidx,
-						qs->rbdr_len - 1);
+	nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_DOOR,
+			      qidx, qs->rbdr_len - 1);
 
 	/* Set threshold value for interrupt generation */
-	nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_THRESH, qidx,
-						rbdr->thresh - 1);
+	nicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_THRESH,
+			      qidx, rbdr->thresh - 1);
 }
 
+/* Requests PF to assign and enable Qset */
 void nicvf_qset_config(struct nicvf *nic, bool enable)
 {
-	struct  nic_mbx *mbx;
+	struct  nic_mbx mbx = {};
 	struct queue_set *qs = nic->qs;
+	struct qs_cfg *qs_cfg;
 
-	/* Send a mailbox msg to PF to config Qset */
-	mbx = nicvf_get_mbx();
-	mbx->msg = NIC_PF_VF_MSG_QS_CFG;
-	mbx->data.qs.num = qs->vnic_id;
+	if (!qs) {
+		netdev_warn(nic->netdev,
+			    "Qset is still not allocated, don't init queues\n");
+		return;
+	}
 
-	if (enable) {
-		mbx->data.qs.cfg = 0x80000000 | qs->vnic_id;
-		nicvf_send_msg_to_pf(nic, mbx);
-	} else {  /* disable Qset */
-		mbx->data.qs.cfg = 0;
-		nicvf_send_msg_to_pf(nic, mbx);
+	qs->enable = enable;
+	qs->vnic_id = nic->vf_id;
+
+	/* Send a mailbox msg to PF to config Qset */
+	mbx.msg = NIC_MBOX_MSG_QS_CFG;
+	mbx.data.qs.num = qs->vnic_id;
+
+	mbx.data.qs.cfg = 0;
+	qs_cfg = (struct qs_cfg *)&mbx.data.qs.cfg;
+	if (qs->enable) {
+		qs_cfg->ena = 1;
+#ifdef __BIG_ENDIAN
+		qs_cfg->be = 1;
+#endif
+		qs_cfg->vnic = qs->vnic_id;
 	}
-	kfree(mbx);
+	nicvf_send_msg_to_pf(nic, &mbx);
 }
 
 static void nicvf_free_resources(struct nicvf *nic)
@@ -455,16 +651,9 @@ static void nicvf_free_resources(struct nicvf *nic)
 	int qidx;
 	struct queue_set *qs = nic->qs;
 
-	if (!qs)
-		return;
-
 	/* Free receive buffer descriptor ring */
 	for (qidx = 0; qidx < qs->rbdr_cnt; qidx++)
-		nicvf_free_rbdr(nic, &qs->rbdr[qidx], qidx);
-
-	/* Free receive queue */
-	for (qidx = 0; qidx < qs->rq_cnt; qidx++)
-		nicvf_free_rcv_queue(&qs->rq[qidx]);
+		nicvf_free_rbdr(nic, &qs->rbdr[qidx]);
 
 	/* Free completion queue */
 	for (qidx = 0; qidx < qs->cq_cnt; qidx++)
@@ -475,32 +664,15 @@ static void nicvf_free_resources(struct nicvf *nic)
 		nicvf_free_snd_queue(nic, &qs->sq[qidx]);
 }
 
-static int nicvf_init_resources(struct nicvf *nic)
+static int nicvf_alloc_resources(struct nicvf *nic)
 {
 	int qidx;
 	struct queue_set *qs = nic->qs;
 
-	/* Set queue count */
-	qs->rbdr_cnt = RBDR_CNT;
-	qs->rq_cnt = RCV_QUEUE_CNT;
-	qs->sq_cnt = SND_QUEUE_CNT;
-	qs->cq_cnt = CMP_QUEUE_CNT;
-
-	/* Set queue lengths */
-	qs->rbdr_len = RCV_BUF_COUNT;
-	qs->sq_len = SND_QUEUE_LEN;
-	qs->cq_len = CMP_QUEUE_LEN;
-
 	/* Alloc receive buffer descriptor ring */
 	for (qidx = 0; qidx < qs->rbdr_cnt; qidx++) {
 		if (nicvf_init_rbdr(nic, &qs->rbdr[qidx], qs->rbdr_len,
-							RCV_BUFFER_LEN))
-			goto alloc_fail;
-	}
-
-	/* Alloc receive queue */
-	for (qidx = 0; qidx < qs->rq_cnt; qidx++) {
-		if (nicvf_init_rcv_queue(&qs->rq[qidx]))
+				    DMA_BUFFER_LEN))
 			goto alloc_fail;
 	}
 
@@ -522,39 +694,58 @@ alloc_fail:
 	return -ENOMEM;
 }
 
+int nicvf_set_qset_resources(struct nicvf *nic)
+{
+	struct queue_set *qs;
+
+	qs = kzalloc(sizeof(*qs), GFP_ATOMIC);
+	if (!qs)
+		return -ENOMEM;
+	nic->qs = qs;
+
+	/* Set count of each queue */
+	qs->rbdr_cnt = RBDR_CNT;
+#ifdef VNIC_RSS_SUPPORT
+	qs->rq_cnt = RCV_QUEUE_CNT;
+#else
+	qs->rq_cnt = 1;
+#endif
+	qs->sq_cnt = SND_QUEUE_CNT;
+	qs->cq_cnt = CMP_QUEUE_CNT;
+
+	/* Set queue lengths */
+	qs->rbdr_len = RCV_BUF_COUNT;
+	qs->sq_len = SND_QUEUE_LEN;
+	qs->cq_len = CMP_QUEUE_LEN;
+	return 0;
+}
+
 int nicvf_config_data_transfer(struct nicvf *nic, bool enable)
 {
 	bool disable = false;
-	struct queue_set *qs;
+	struct queue_set *qs = nic->qs;
 	int qidx;
 
-	if (enable) {
-		if (!(qs = kzalloc(sizeof(*qs), GFP_ATOMIC)))
-			return -ENOMEM;
-
-		qs->vnic_id = nic->vnic_id;
-		nic->qs = qs;
+	if (!qs)
+		return 0;
 
-		if (nicvf_init_resources(nic))
+	if (enable) {
+		if (nicvf_alloc_resources(nic))
 			return -ENOMEM;
 
-		for (qidx = 0; qidx < qs->rbdr_cnt; qidx++)
-			nicvf_rbdr_config(nic, qs, qidx, enable);
-		for (qidx = 0; qidx < qs->rq_cnt; qidx++)
-			nicvf_rcv_queue_config(nic, qs, qidx, enable);
 		for (qidx = 0; qidx < qs->sq_cnt; qidx++)
 			nicvf_snd_queue_config(nic, qs, qidx, enable);
 		for (qidx = 0; qidx < qs->cq_cnt; qidx++)
 			nicvf_cmp_queue_config(nic, qs, qidx, enable);
-
-	} else {
-		if (!(qs = nic->qs))
-			return 0;
-
 		for (qidx = 0; qidx < qs->rbdr_cnt; qidx++)
-			nicvf_rbdr_config(nic, qs, qidx, disable);
+			nicvf_rbdr_config(nic, qs, qidx, enable);
+		for (qidx = 0; qidx < qs->rq_cnt; qidx++)
+			nicvf_rcv_queue_config(nic, qs, qidx, enable);
+	} else {
 		for (qidx = 0; qidx < qs->rq_cnt; qidx++)
 			nicvf_rcv_queue_config(nic, qs, qidx, disable);
+		for (qidx = 0; qidx < qs->rbdr_cnt; qidx++)
+			nicvf_rbdr_config(nic, qs, qidx, disable);
 		for (qidx = 0; qidx < qs->sq_cnt; qidx++)
 			nicvf_snd_queue_config(nic, qs, qidx, disable);
 		for (qidx = 0; qidx < qs->cq_cnt; qidx++)
@@ -566,72 +757,59 @@ int nicvf_config_data_transfer(struct nicvf *nic, bool enable)
 	return 0;
 }
 
-/* Check for errors in the cmp.queue entry */
-int nicvf_cq_check_errs(struct nicvf *nic, void *cq_desc)
-{
-	uint32_t ret = false;
-	struct cqe_rx_t *cqe_rx;
-
-	cqe_rx = (struct cqe_rx_t *)cq_desc;
-	if (cqe_rx->err_level || cqe_rx->err_opcode)
-		ret = true;
-
-	return ret;
-}
-
-/* Get a free desc from send queue
- * @qs:   Qset from which to get a SQ descriptor
- * @qnum: SQ number (0...7) in the Qset
- *
+/* Get a free desc from SQ
  * returns descriptor ponter & descriptor number
  */
-static int nicvf_get_sq_desc(struct queue_set *qs, int qnum, void **desc)
+static inline int nicvf_get_sq_desc(struct queue_set *qs, int qnum, void **desc)
 {
 	int qentry;
 	struct snd_queue *sq = &qs->sq[qnum];
 
-	if (!sq->free_cnt)
+	if (!atomic_read(&sq->free_cnt))
 		return 0;
 
 	qentry = sq->tail++;
-	sq->free_cnt--;
-	sq->tail &= (sq->desc_mem.q_len - 1);
-	*desc = sq->desc_mem.base + (qentry * SND_QUEUE_DESC_SIZE);
+	atomic_dec(&sq->free_cnt);
+
+	sq->tail &= (sq->dmem.q_len - 1);
+	*desc = GET_SQ_DESC(sq, qentry);
 	return qentry;
 }
 
+/* Free descriptor back to SQ for future use */
 void nicvf_put_sq_desc(struct snd_queue *sq, int desc_cnt)
 {
 	while (desc_cnt--) {
-		sq->free_cnt++;
+		atomic_inc(&sq->free_cnt);
 		sq->head++;
-		sq->head &= (sq->desc_mem.q_len - 1);
+		sq->head &= (sq->dmem.q_len - 1);
 	}
 }
 
 void nicvf_sq_enable(struct nicvf *nic, struct snd_queue *sq, int qidx)
 {
-	uint64_t reg_val;
+	u64 sq_cfg;
 
-	reg_val = nicvf_queue_reg_read(nic, NIC_QSET_SQ_0_7_CFG, qidx);
-	reg_val |= (1ULL << 19);
-	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_CFG, qidx, reg_val);
+	sq_cfg = nicvf_queue_reg_read(nic, NIC_QSET_SQ_0_7_CFG, qidx);
+	sq_cfg |= NICVF_SQ_EN;
+	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_CFG, qidx, sq_cfg);
 	/* Ring doorbell so that H/W restarts processing SQEs */
 	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_DOOR, qidx, 0);
 }
 
 void nicvf_sq_disable(struct nicvf *nic, int qidx)
 {
-	uint64_t reg_val;
+	u64 sq_cfg;
 
-	reg_val = nicvf_queue_reg_read(nic, NIC_QSET_SQ_0_7_CFG, qidx);
-	reg_val &= ~(1ULL << 19);
-	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_CFG, qidx, reg_val);
+	sq_cfg = nicvf_queue_reg_read(nic, NIC_QSET_SQ_0_7_CFG, qidx);
+	sq_cfg &= ~NICVF_SQ_EN;
+	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_CFG, qidx, sq_cfg);
 }
 
-void nicvf_sq_free_used_descs(struct net_device *netdev, struct snd_queue *sq, int qidx)
+void nicvf_sq_free_used_descs(struct net_device *netdev, struct snd_queue *sq,
+			      int qidx)
 {
-	uint64_t head, tail;
+	u64 head, tail;
 	struct sk_buff *skb;
 	struct nicvf *nic = netdev_priv(netdev);
 	struct sq_hdr_subdesc *hdr;
@@ -639,37 +817,34 @@ void nicvf_sq_free_used_descs(struct net_device *netdev, struct snd_queue *sq, i
 	head = nicvf_queue_reg_read(nic, NIC_QSET_SQ_0_7_HEAD, qidx) >> 4;
 	tail = nicvf_queue_reg_read(nic, NIC_QSET_SQ_0_7_TAIL, qidx) >> 4;
 	while (sq->head != head) {
-		hdr  = (struct sq_hdr_subdesc *)(sq->desc_mem.base +
-			(sq->head * SND_QUEUE_DESC_SIZE));
+		hdr = (struct sq_hdr_subdesc *)GET_SQ_DESC(sq, sq->head);
 		if (hdr->subdesc_type != SQ_DESC_TYPE_HEADER) {
 			nicvf_put_sq_desc(sq, 1);
 			continue;
 		}
 		skb = (struct sk_buff *)sq->skbuff[sq->head];
 		atomic64_add(1, (atomic64_t *)&netdev->stats.tx_packets);
-		atomic64_add(hdr->tot_len, (atomic64_t *)&netdev->stats.tx_bytes);
-		nicvf_free_skb(nic, skb);
+		atomic64_add(hdr->tot_len,
+			     (atomic64_t *)&netdev->stats.tx_bytes);
+		dev_kfree_skb_any(skb);
 		nicvf_put_sq_desc(sq, hdr->subdesc_cnt + 1);
 	}
 }
 
+/* Get the number of SQ descriptors needed to xmit this skb */
 static int nicvf_sq_subdesc_required(struct nicvf *nic, struct sk_buff *skb)
 {
-	int subdesc_cnt = MIN_SND_QUEUE_DESC_FOR_PKT_XMIT;
+	int subdesc_cnt = MIN_SQ_DESC_PER_PKT_XMIT;
+
+	if (skb_shinfo(skb)->gso_size) {
+		subdesc_cnt = skb_shinfo(skb)->gso_size / nic->mtu;
+		subdesc_cnt *= MIN_SQ_DESC_PER_PKT_XMIT;
+		return subdesc_cnt;
+	}
 
 	if (skb_shinfo(skb)->nr_frags)
 		subdesc_cnt += skb_shinfo(skb)->nr_frags;
 
-#ifdef VNIC_TX_CSUM_OFFLOAD_SUPPORT
-	if (skb->ip_summed == CHECKSUM_PARTIAL) {
-		if (skb->protocol == htons(ETH_P_IP))
-			subdesc_cnt++;
-		if ((ip_hdr(skb)->protocol == IPPROTO_TCP) ||
-			(ip_hdr(skb)->protocol == IPPROTO_UDP))
-			subdesc_cnt++;
-	}
-#endif
-
 	return subdesc_cnt;
 }
 
@@ -678,40 +853,48 @@ static int nicvf_sq_subdesc_required(struct nicvf *nic, struct sk_buff *skb)
  */
 struct sq_hdr_subdesc *
 nicvf_sq_add_hdr_subdesc(struct queue_set *qs, int sq_num,
-				int subdesc_cnt, struct sk_buff *skb)
+			 int subdesc_cnt, struct sk_buff *skb)
 {
-	int qentry;
-	void *desc;
+	int qentry, proto;
+	void *desc = NULL;
 	struct snd_queue *sq;
 	struct sq_hdr_subdesc *hdr;
 
 	sq = &qs->sq[sq_num];
 	qentry = nicvf_get_sq_desc(qs, sq_num, &desc);
-	sq->skbuff[qentry] = (uint64_t)skb;
+	sq->skbuff[qentry] = (u64)skb;
 
 	hdr = (struct sq_hdr_subdesc *)desc;
 
-	memset(hdr, 0, SND_QUEUE_DESC_SIZE); /* TBD: Need to remove these memset */
+	memset(hdr, 0, SND_QUEUE_DESC_SIZE);
 	hdr->subdesc_type = SQ_DESC_TYPE_HEADER;
+	/* Enable notification via CQE after processing SQE */
 	hdr->post_cqe = 1;
+	/* No of subdescriptors following this */
 	hdr->subdesc_cnt = subdesc_cnt;
 	hdr->tot_len = skb->len;
 
-#ifdef VNIC_HW_TSO_SUPPORT
-	if (!skb_shinfo(skb)->gso_size)
-		return hdr;
-
-	/* Packet to be subjected to TSO */
-	hdr->tso = 1;
-	hdr->tso_l4_offset = (int)(skb_transport_header(skb) - skb->data) +
-				tcp_hdrlen(skb);
-	hdr->tso_max_paysize = skb_shinfo(skb)->gso_size + hdr->tso_l4_offset;
-	/* TBD: These fields have to be setup properly */
-	hdr->tso_sdc_first	= 0;
-	hdr->tso_sdc_cont	= 0;
-	hdr->tso_flags_first	= 0;
-	hdr->tso_flags_last	= 0;
-#endif
+	/* Offload checksum calculation to HW */
+	if (skb->ip_summed == CHECKSUM_PARTIAL) {
+		if (skb->protocol != htons(ETH_P_IP))
+			return hdr;
+		hdr->csum_l3 = 0;
+		hdr->l3_offset = skb_network_offset(skb);
+		hdr->l4_offset = skb_transport_offset(skb);
+		proto = ip_hdr(skb)->protocol;
+		switch (proto) {
+		case IPPROTO_TCP:
+			hdr->csum_l4 = SEND_L4_CSUM_TCP;
+			break;
+		case IPPROTO_UDP:
+			hdr->csum_l4 = SEND_L4_CSUM_UDP;
+			break;
+		case IPPROTO_SCTP:
+			hdr->csum_l4 = SEND_L4_CSUM_SCTP;
+			break;
+		}
+	}
+
 	return hdr;
 }
 
@@ -719,10 +902,10 @@ nicvf_sq_add_hdr_subdesc(struct queue_set *qs, int sq_num,
  * Must follow HDR descriptor
  */
 static void nicvf_sq_add_gather_subdesc(struct nicvf *nic, struct queue_set *qs,
-						int sq_num, struct sk_buff *skb)
+					int sq_num, struct sk_buff *skb)
 {
 	int i;
-	void *desc;
+	void *desc = NULL;
 	struct sq_gather_subdesc *gather;
 
 	nicvf_get_sq_desc(qs, sq_num, &desc);
@@ -730,11 +913,11 @@ static void nicvf_sq_add_gather_subdesc(struct nicvf *nic, struct queue_set *qs,
 
 	memset(gather, 0, SND_QUEUE_DESC_SIZE);
 	gather->subdesc_type = SQ_DESC_TYPE_GATHER;
-	gather->ld_type = 1;
+	gather->ld_type = NIC_SEND_LD_TYPE_E_LDD;
 	gather->size = skb_is_nonlinear(skb) ? skb_headlen(skb) : skb->len;
-	gather->addr = pci_map_single(nic->pdev, skb->data,
-				gather->size, PCI_DMA_TODEVICE);
+	gather->addr = virt_to_phys(skb->data);
 
+	/* Check for scattered buffer */
 	if (!skb_is_nonlinear(skb))
 		return;
 
@@ -748,73 +931,11 @@ static void nicvf_sq_add_gather_subdesc(struct nicvf *nic, struct queue_set *qs,
 
 		memset(gather, 0, SND_QUEUE_DESC_SIZE);
 		gather->subdesc_type = SQ_DESC_TYPE_GATHER;
-		gather->ld_type = 1;
+		gather->ld_type = NIC_SEND_LD_TYPE_E_LDD;
 		gather->size = skb_frag_size(frag);
-		gather->addr = pci_map_single(nic->pdev, skb_frag_address(frag),
-						gather->size, PCI_DMA_TODEVICE);
-	}
-}
-
-#ifdef VNIC_TX_CSUM_OFFLOAD_SUPPORT
-static void nicvf_fill_l3_crc_subdesc(struct sq_crc_subdesc *l3,
-					struct sk_buff *skb)
-{
-	int crc_pos;
-
-	crc_pos = skb_network_header(skb) - skb_mac_header(skb);
-	crc_pos += offsetof(struct iphdr, check);
-
-	l3->subdesc_type = SQ_DESC_TYPE_CRC;
-	l3->crc_alg = SEND_CRCALG_CRC32;
-	l3->crc_insert_pos = crc_pos;
-	l3->hdr_start = skb_network_offset(skb);
-	l3->crc_len = skb_transport_header(skb) - skb_network_header(skb);
-	l3->crc_ival = 0;
-}
-
-static void nicvf_fill_l4_crc_subdesc(struct sq_crc_subdesc *l4,
-					struct sk_buff *skb)
-{
-	l4->subdesc_type = SQ_DESC_TYPE_CRC;
-	l4->crc_alg = SEND_CRCALG_CRC32;
-	l4->crc_insert_pos = skb->csum_start + skb->csum_offset;
-	l4->hdr_start = skb->csum_start;
-	l4->crc_len = skb->len - skb_transport_offset(skb);
-	l4->crc_ival = 0;
-}
-
-/* SQ CRC subdescriptor
- * Must follow HDR and precede GATHER, IMM subdescriptors
- */
-static void nicvf_sq_add_crc_subdesc(struct nicvf *nic, struct queue_set *qs,
-						int sq_num, struct sk_buff *skb)
-{
-	int proto;
-	void *desc;
-	struct sq_crc_subdesc *crc;
-	struct snd_queue *sq;
-
-	if (skb->ip_summed != CHECKSUM_PARTIAL)
-		return;
-
-	if (skb->protocol != htons(ETH_P_IP))
-		return;
-
-	sq = &qs->sq[sq_num];
-	nicvf_get_sq_desc(qs, sq_num, &desc);
-
-	crc = (struct sq_crc_subdesc *)desc;
-
-	nicvf_fill_l3_crc_subdesc(crc, skb);
-
-	proto = ip_hdr(skb)->protocol;
-	if ((proto == IPPROTO_TCP) || (proto == IPPROTO_UDP)) {
-		nicvf_get_sq_desc(qs, sq_num, &desc);
-		crc = (struct sq_crc_subdesc *)desc;
-		nicvf_fill_l4_crc_subdesc(crc, skb);
+		gather->addr = virt_to_phys(skb_frag_address(frag));
 	}
 }
-#endif
 
 /* Append an skb to a SQ for packet transfer. */
 int nicvf_sq_append_skb(struct nicvf *nic, struct sk_buff *skb)
@@ -829,25 +950,21 @@ int nicvf_sq_append_skb(struct nicvf *nic, struct sk_buff *skb)
 	sq = &qs->sq[sq_num];
 
 	subdesc_cnt = nicvf_sq_subdesc_required(nic, skb);
-
-	if (subdesc_cnt > sq->free_cnt)
+	if (subdesc_cnt > atomic_read(&sq->free_cnt))
 		goto append_fail;
 
 	/* Add SQ header subdesc */
 	hdr_desc = nicvf_sq_add_hdr_subdesc(qs, sq_num, subdesc_cnt - 1, skb);
 
-#ifdef VNIC_TX_CSUM_OFFLOAD_SUPPORT
-	/* Add CRC subdescriptor for IP/TCP/UDP (L3/L4) crc calculation */
-	if (skb->ip_summed == CHECKSUM_PARTIAL)
-		nicvf_sq_add_crc_subdesc(nic, qs, sq_num, skb);
-#endif
-
 	/* Add SQ gather subdesc */
 	nicvf_sq_add_gather_subdesc(nic, qs, sq_num, skb);
 
+	/* make sure all memory stores are done before ringing doorbell */
+	smp_wmb();
+
 	/* Inform HW to xmit new packet */
 	nicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_DOOR,
-						sq_num, subdesc_cnt);
+			      sq_num, subdesc_cnt);
 	return 1;
 
 append_fail:
@@ -855,40 +972,49 @@ append_fail:
 	return 0;
 }
 
-struct sk_buff *nicvf_get_rcv_skb(struct nicvf *nic, void *cq_desc)
+static inline unsigned frag_num(unsigned i)
+{
+#ifdef __BIG_ENDIAN
+	return (i & ~3) + 3 - (i & 3);
+#else
+	return i;
+#endif
+}
+
+/* Returns SKB for a received packet */
+struct sk_buff *nicvf_get_rcv_skb(struct nicvf *nic, struct cqe_rx_t *cqe_rx)
 {
 	int frag;
 	int payload_len = 0;
 	struct sk_buff *skb = NULL;
 	struct sk_buff *skb_frag = NULL;
 	struct sk_buff *prev_frag = NULL;
-	struct cqe_rx_t *cqe_rx;
-	struct rbdr *rbdr;
-	struct rcv_queue *rq;
-	struct queue_set *qs = nic->qs;
-	uint16_t *rb_lens = NULL;
-	uint64_t *rb_ptrs = NULL;
+	u16 *rb_lens = NULL;
+	u64 *rb_ptrs = NULL;
 
-	cqe_rx = (struct cqe_rx_t *)cq_desc;
+	rb_lens = (void *)cqe_rx + (3 * sizeof(u64));
+	rb_ptrs = (void *)cqe_rx + (6 * sizeof(u64));
 
-	rq = &qs->rq[cqe_rx->rq_idx];
-	rbdr = &qs->rbdr[rq->start_qs_rbdr_idx];
-	rb_lens = cq_desc + (3 * sizeof(uint64_t)); /* Use offsetof */
-	rb_ptrs = cq_desc + (6 * sizeof(uint64_t));
 	nic_dbg(&nic->pdev->dev, "%s rb_cnt %d rb0_ptr %llx rb0_sz %d\n",
 		__func__, cqe_rx->rb_cnt, cqe_rx->rb0_ptr, cqe_rx->rb0_sz);
 
 	for (frag = 0; frag < cqe_rx->rb_cnt; frag++) {
-		payload_len = *rb_lens;
+		payload_len = rb_lens[frag_num(frag)];
 		if (!frag) {
-			skb = nicvf_rb_ptr_to_skb(*rb_ptrs);
-			skb_put(skb, payload_len);
 			/* First fragment */
-			pci_unmap_single(nic->pdev, (dma_addr_t)skb->data, rbdr->buf_size, PCI_DMA_FROMDEVICE);
+			skb = nicvf_rb_ptr_to_skb(nic,
+						  *rb_ptrs - cqe_rx->align_pad);
+			if (!skb)
+				return NULL;
+			skb_reserve(skb, cqe_rx->align_pad);
+			skb_put(skb, payload_len);
 		} else {
 			/* Add fragments */
-			skb_frag = nicvf_rb_ptr_to_skb(*rb_ptrs);
-			pci_unmap_single(nic->pdev, (dma_addr_t)skb_frag->data, rbdr->buf_size, PCI_DMA_FROMDEVICE);
+			skb_frag = nicvf_rb_ptr_to_skb(nic, *rb_ptrs);
+			if (!skb_frag) {
+				dev_kfree_skb(skb);
+				return NULL;
+			}
 
 			if (!skb_shinfo(skb)->frag_list)
 				skb_shinfo(skb)->frag_list = skb_frag;
@@ -901,7 +1027,6 @@ struct sk_buff *nicvf_get_rcv_skb(struct nicvf *nic, void *cq_desc)
 			skb_frag->len = payload_len;
 		}
 		/* Next buffer pointer */
-		rb_lens++;
 		rb_ptrs++;
 	}
 	return skb;
@@ -910,9 +1035,9 @@ struct sk_buff *nicvf_get_rcv_skb(struct nicvf *nic, void *cq_desc)
 /* Enable interrupt */
 void nicvf_enable_intr(struct nicvf *nic, int int_type, int q_idx)
 {
-	uint64_t reg_val;
+	u64 reg_val;
 
-	reg_val = nicvf_qset_reg_read(nic, NIC_VF_ENA_W1S);
+	reg_val = nicvf_reg_read(nic, NIC_VF_ENA_W1S);
 
 	switch (int_type) {
 	case NICVF_INTR_CQ:
@@ -937,17 +1062,18 @@ void nicvf_enable_intr(struct nicvf *nic, int int_type, int q_idx)
 		reg_val |= (1ULL << NICVF_INTR_QS_ERR_SHIFT);
 	break;
 	default:
-		netdev_err(nic->netdev, "Failed to enable interrupt: unknown interrupt type\n");
+		netdev_err(nic->netdev,
+			   "Failed to enable interrupt: unknown type\n");
 	break;
 	}
 
-	nicvf_qset_reg_write(nic, NIC_VF_ENA_W1S, reg_val);
+	nicvf_reg_write(nic, NIC_VF_ENA_W1S, reg_val);
 }
 
 /* Disable interrupt */
 void nicvf_disable_intr(struct nicvf *nic, int int_type, int q_idx)
 {
-	uint64_t reg_val = 0;
+	u64 reg_val = 0;
 
 	switch (int_type) {
 	case NICVF_INTR_CQ:
@@ -972,17 +1098,18 @@ void nicvf_disable_intr(struct nicvf *nic, int int_type, int q_idx)
 		reg_val |= (1ULL << NICVF_INTR_QS_ERR_SHIFT);
 	break;
 	default:
-		netdev_err(nic->netdev, "Failed to disable interrupt: unknown interrupt type\n");
+		netdev_err(nic->netdev,
+			   "Failed to disable interrupt: unknown type\n");
 	break;
 	}
 
-	nicvf_qset_reg_write(nic, NIC_VF_ENA_W1C, reg_val);
+	nicvf_reg_write(nic, NIC_VF_ENA_W1C, reg_val);
 }
 
 /* Clear interrupt */
 void nicvf_clear_intr(struct nicvf *nic, int int_type, int q_idx)
 {
-	uint64_t reg_val = 0;
+	u64 reg_val = 0;
 
 	switch (int_type) {
 	case NICVF_INTR_CQ:
@@ -1007,20 +1134,21 @@ void nicvf_clear_intr(struct nicvf *nic, int int_type, int q_idx)
 		reg_val |= (1ULL << NICVF_INTR_QS_ERR_SHIFT);
 	break;
 	default:
-		netdev_err(nic->netdev, "Failed to clear interrupt: unknown interrupt type\n");
+		netdev_err(nic->netdev,
+			   "Failed to clear interrupt: unknown type\n");
 	break;
 	}
 
-	nicvf_qset_reg_write(nic, NIC_VF_INT, reg_val);
+	nicvf_reg_write(nic, NIC_VF_INT, reg_val);
 }
 
 /* Check if interrupt is enabled */
 int nicvf_is_intr_enabled(struct nicvf *nic, int int_type, int q_idx)
 {
-	uint64_t reg_val;
-	uint64_t mask = 0xff;
+	u64 reg_val;
+	u64 mask = 0xff;
 
-	reg_val = nicvf_qset_reg_read(nic, NIC_VF_ENA_W1S);
+	reg_val = nicvf_reg_read(nic, NIC_VF_ENA_W1S);
 
 	switch (int_type) {
 	case NICVF_INTR_CQ:
@@ -1046,9 +1174,218 @@ int nicvf_is_intr_enabled(struct nicvf *nic, int int_type, int q_idx)
 	break;
 	default:
 		netdev_err(nic->netdev,
-			   "Failed to check interrupt enable: unknown interrupt type\n");
+			   "Failed to check interrupt enable: unknown type\n");
 	break;
 	}
 
 	return (reg_val & mask);
 }
+
+void nicvf_update_rq_stats(struct nicvf *nic, int rq_idx)
+{
+	struct rcv_queue *rq;
+
+#define GET_RQ_STATS(reg) \
+	nicvf_reg_read(nic, NIC_QSET_RQ_0_7_STAT_0_1 |\
+			    (rq_idx << NIC_Q_NUM_SHIFT) | (reg << 3))
+
+	rq = &nic->qs->rq[rq_idx];
+	rq->stats.bytes = GET_RQ_STATS(RQ_SQ_STATS_OCTS);
+	rq->stats.pkts = GET_RQ_STATS(RQ_SQ_STATS_PKTS);
+}
+
+void nicvf_update_sq_stats(struct nicvf *nic, int sq_idx)
+{
+	struct snd_queue *sq;
+
+#define GET_SQ_STATS(reg) \
+	nicvf_reg_read(nic, NIC_QSET_SQ_0_7_STAT_0_1 |\
+			    (sq_idx << NIC_Q_NUM_SHIFT) | (reg << 3))
+
+	sq = &nic->qs->sq[sq_idx];
+	sq->stats.bytes = GET_SQ_STATS(RQ_SQ_STATS_OCTS);
+	sq->stats.pkts = GET_SQ_STATS(RQ_SQ_STATS_PKTS);
+}
+
+/* Check for errors in the receive cmp.queue entry */
+int nicvf_check_cqe_rx_errs(struct nicvf *nic,
+			    struct cmp_queue *cq, struct cqe_rx_t *cqe_rx)
+{
+	struct cmp_queue_stats *stats = &cq->stats;
+
+	if (!cqe_rx->err_level && !cqe_rx->err_opcode) {
+		stats->rx.errop.good++;
+		return 0;
+	}
+
+	if (netif_msg_rx_err(nic))
+		netdev_err(nic->netdev,
+			   "%s: RX error CQE err_level 0x%x err_opcode 0x%x\n",
+			   nic->netdev->name,
+			   cqe_rx->err_level, cqe_rx->err_opcode);
+
+	switch (cqe_rx->err_level) {
+	case CQ_ERRLVL_MAC:
+		stats->rx.errlvl.mac_errs++;
+	break;
+	case CQ_ERRLVL_L2:
+		stats->rx.errlvl.l2_errs++;
+	break;
+	case CQ_ERRLVL_L3:
+		stats->rx.errlvl.l3_errs++;
+	break;
+	case CQ_ERRLVL_L4:
+		stats->rx.errlvl.l4_errs++;
+	break;
+	}
+
+	switch (cqe_rx->err_opcode) {
+	case CQ_RX_ERROP_RE_PARTIAL:
+		stats->rx.errop.partial_pkts++;
+	break;
+	case CQ_RX_ERROP_RE_JABBER:
+		stats->rx.errop.jabber_errs++;
+	break;
+	case CQ_RX_ERROP_RE_FCS:
+		stats->rx.errop.fcs_errs++;
+	break;
+	case CQ_RX_ERROP_RE_TERMINATE:
+		stats->rx.errop.terminate_errs++;
+	break;
+	case CQ_RX_ERROP_RE_RX_CTL:
+		stats->rx.errop.bgx_rx_errs++;
+	break;
+	case CQ_RX_ERROP_PREL2_ERR:
+		stats->rx.errop.prel2_errs++;
+	break;
+	case CQ_RX_ERROP_L2_FRAGMENT:
+		stats->rx.errop.l2_frags++;
+	break;
+	case CQ_RX_ERROP_L2_OVERRUN:
+		stats->rx.errop.l2_overruns++;
+	break;
+	case CQ_RX_ERROP_L2_PFCS:
+		stats->rx.errop.l2_pfcs++;
+	break;
+	case CQ_RX_ERROP_L2_PUNY:
+		stats->rx.errop.l2_puny++;
+	break;
+	case CQ_RX_ERROP_L2_MAL:
+		stats->rx.errop.l2_hdr_malformed++;
+	break;
+	case CQ_RX_ERROP_L2_OVERSIZE:
+		stats->rx.errop.l2_oversize++;
+	break;
+	case CQ_RX_ERROP_L2_UNDERSIZE:
+		stats->rx.errop.l2_undersize++;
+	break;
+	case CQ_RX_ERROP_L2_LENMISM:
+		stats->rx.errop.l2_len_mismatch++;
+	break;
+	case CQ_RX_ERROP_L2_PCLP:
+		stats->rx.errop.l2_pclp++;
+	break;
+	case CQ_RX_ERROP_IP_NOT:
+		stats->rx.errop.non_ip++;
+	break;
+	case CQ_RX_ERROP_IP_CSUM_ERR:
+		stats->rx.errop.ip_csum_err++;
+	break;
+	case CQ_RX_ERROP_IP_MAL:
+		stats->rx.errop.ip_hdr_malformed++;
+	break;
+	case CQ_RX_ERROP_IP_MALD:
+		stats->rx.errop.ip_payload_malformed++;
+	break;
+	case CQ_RX_ERROP_IP_HOP:
+		stats->rx.errop.ip_hop_errs++;
+	break;
+	case CQ_RX_ERROP_L3_ICRC:
+		stats->rx.errop.l3_icrc_errs++;
+	break;
+	case CQ_RX_ERROP_L3_PCLP:
+		stats->rx.errop.l3_pclp++;
+	break;
+	case CQ_RX_ERROP_L4_MAL:
+		stats->rx.errop.l4_malformed++;
+	break;
+	case CQ_RX_ERROP_L4_CHK:
+		stats->rx.errop.l4_csum_errs++;
+	break;
+	case CQ_RX_ERROP_UDP_LEN:
+		stats->rx.errop.udp_len_err++;
+	break;
+	case CQ_RX_ERROP_L4_PORT:
+		stats->rx.errop.bad_l4_port++;
+	break;
+	case CQ_RX_ERROP_TCP_FLAG:
+		stats->rx.errop.bad_tcp_flag++;
+	break;
+	case CQ_RX_ERROP_TCP_OFFSET:
+		stats->rx.errop.tcp_offset_errs++;
+	break;
+	case CQ_RX_ERROP_L4_PCLP:
+		stats->rx.errop.l4_pclp++;
+	break;
+	case CQ_RX_ERROP_RBDR_TRUNC:
+		stats->rx.errop.pkt_truncated++;
+	break;
+	}
+
+	return 1;
+}
+
+/* Check for errors in the send cmp.queue entry */
+int nicvf_check_cqe_tx_errs(struct nicvf *nic,
+			    struct cmp_queue *cq, struct cqe_send_t *cqe_tx)
+{
+	struct cmp_queue_stats *stats = &cq->stats;
+
+	switch (cqe_tx->send_status) {
+	case CQ_TX_ERROP_GOOD:
+		stats->tx.good++;
+		return 0;
+	break;
+	case CQ_TX_ERROP_DESC_FAULT:
+		stats->tx.desc_fault++;
+	break;
+	case CQ_TX_ERROP_HDR_CONS_ERR:
+		stats->tx.hdr_cons_err++;
+	break;
+	case CQ_TX_ERROP_SUBDC_ERR:
+		stats->tx.subdesc_err++;
+	break;
+	case CQ_TX_ERROP_IMM_SIZE_OFLOW:
+		stats->tx.imm_size_oflow++;
+	break;
+	case CQ_TX_ERROP_DATA_SEQUENCE_ERR:
+		stats->tx.data_seq_err++;
+	break;
+	case CQ_TX_ERROP_MEM_SEQUENCE_ERR:
+		stats->tx.mem_seq_err++;
+	break;
+	case CQ_TX_ERROP_LOCK_VIOL:
+		stats->tx.lock_viol++;
+	break;
+	case CQ_TX_ERROP_DATA_FAULT:
+		stats->tx.data_fault++;
+	break;
+	case CQ_TX_ERROP_TSTMP_CONFLICT:
+		stats->tx.tstmp_conflict++;
+	break;
+	case CQ_TX_ERROP_TSTMP_TIMEOUT:
+		stats->tx.tstmp_timeout++;
+	break;
+	case CQ_TX_ERROP_MEM_FAULT:
+		stats->tx.mem_fault++;
+	break;
+	case CQ_TX_ERROP_CK_OVERLAP:
+		stats->tx.csum_overlap++;
+	break;
+	case CQ_TX_ERROP_CK_OFLOW:
+		stats->tx.csum_overflow++;
+	break;
+	}
+
+	return 1;
+}
diff --git a/drivers/net/ethernet/cavium/thunder/nicvf_queues.h b/drivers/net/ethernet/cavium/thunder/nicvf_queues.h
index 06770f9..d7c16aa 100644
--- a/drivers/net/ethernet/cavium/thunder/nicvf_queues.h
+++ b/drivers/net/ethernet/cavium/thunder/nicvf_queues.h
@@ -1,82 +1,252 @@
 /*
- * This file is subject to the terms and conditions of the GNU General Public
- * License.  See the file "COPYING" in the main directory of this archive
- * for more details.
+ * Copyright (C) 2015 Cavium, Inc.
  *
- * Copyright (C) 2013 Cavium, Inc.
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of
+ * the License, or (at your option) any later version.
  */
 
 #ifndef NICVF_QUEUES_H
 #define NICVF_QUEUES_H
 
+#include <linux/netdevice.h>
 #include "q_struct.h"
 
-#define    MAX_QUEUE_SET			128
-#define    MAX_RCV_QUEUES_PER_QS		8
-#define    MAX_RCV_BUF_DESC_RINGS_PER_QS	2
-#define    MAX_SND_QUEUES_PER_QS		8
-#define    MAX_CMP_QUEUES_PER_QS		8
-
-#define    RBDR_SIZE0	 0ULL /* 8K entries */
-#define    RBDR_SIZE1	 1ULL /* 16K entries */
-#define    RBDR_SIZE2	 2ULL /* 32K entries */
-#define    RBDR_SIZE3	 3ULL /* 64K entries */
-#define    RBDR_SIZE4	 4ULL /* 126K entries */
-#define    RBDR_SIZE5	 5ULL /* 256K entries */
-#define    RBDR_SIZE6	 6ULL /* 512K entries */
-
-#define    SND_QUEUE_SIZE0	 0ULL /* 1K entries */
-#define    SND_QUEUE_SIZE1	 1ULL /* 2K entries */
-#define    SND_QUEUE_SIZE2	 2ULL /* 4K entries */
-#define    SND_QUEUE_SIZE3	 3ULL /* 8K entries */
-#define    SND_QUEUE_SIZE4	 4ULL /* 16K entries */
-#define    SND_QUEUE_SIZE5	 5ULL /* 32K entries */
-#define    SND_QUEUE_SIZE6	 6ULL /* 64K entries */
-
-#define    CMP_QUEUE_SIZE0	 0ULL /* 1K entries */
-#define    CMP_QUEUE_SIZE1	 1ULL /* 2K entries */
-#define    CMP_QUEUE_SIZE2	 2ULL /* 4K entries */
-#define    CMP_QUEUE_SIZE3	 3ULL /* 8K entries */
-#define    CMP_QUEUE_SIZE4	 4ULL /* 16K entries */
-#define    CMP_QUEUE_SIZE5	 5ULL /* 32K entries */
-#define    CMP_QUEUE_SIZE6	 6ULL /* 64K entries */
+#define MAX_QUEUE_SET			128
+#define MAX_RCV_QUEUES_PER_QS		8
+#define MAX_RCV_BUF_DESC_RINGS_PER_QS	2
+#define MAX_SND_QUEUES_PER_QS		8
+#define MAX_CMP_QUEUES_PER_QS		8
+
+/* VF's queue interrupt ranges */
+#define	NICVF_INTR_ID_CQ		0
+#define	NICVF_INTR_ID_SQ		8
+#define	NICVF_INTR_ID_RBDR		16
+#define	NICVF_INTR_ID_MISC		18
+#define	NICVF_INTR_ID_QS_ERR		19
+
+#define	for_each_cq_irq(irq)	\
+	for (irq = NICVF_INTR_ID_CQ; irq < NICVF_INTR_ID_SQ; irq++)
+#define	for_each_sq_irq(irq)	\
+	for (irq = NICVF_INTR_ID_SQ; irq < NICVF_INTR_ID_RBDR; irq++)
+#define	for_each_rbdr_irq(irq)	\
+	for (irq = NICVF_INTR_ID_RBDR; irq < NICVF_INTR_ID_MISC; irq++)
+
+#define RBDR_SIZE0		0ULL /* 8K entries */
+#define RBDR_SIZE1		1ULL /* 16K entries */
+#define RBDR_SIZE2		2ULL /* 32K entries */
+#define RBDR_SIZE3		3ULL /* 64K entries */
+#define RBDR_SIZE4		4ULL /* 126K entries */
+#define RBDR_SIZE5		5ULL /* 256K entries */
+#define RBDR_SIZE6		6ULL /* 512K entries */
+
+#define SND_QUEUE_SIZE0		0ULL /* 1K entries */
+#define SND_QUEUE_SIZE1		1ULL /* 2K entries */
+#define SND_QUEUE_SIZE2		2ULL /* 4K entries */
+#define SND_QUEUE_SIZE3		3ULL /* 8K entries */
+#define SND_QUEUE_SIZE4		4ULL /* 16K entries */
+#define SND_QUEUE_SIZE5		5ULL /* 32K entries */
+#define SND_QUEUE_SIZE6		6ULL /* 64K entries */
+
+#define CMP_QUEUE_SIZE0		0ULL /* 1K entries */
+#define CMP_QUEUE_SIZE1		1ULL /* 2K entries */
+#define CMP_QUEUE_SIZE2		2ULL /* 4K entries */
+#define CMP_QUEUE_SIZE3		3ULL /* 8K entries */
+#define CMP_QUEUE_SIZE4		4ULL /* 16K entries */
+#define CMP_QUEUE_SIZE5		5ULL /* 32K entries */
+#define CMP_QUEUE_SIZE6		6ULL /* 64K entries */
 
 /* Default queue count per QS, its lengths and threshold values */
-#define    RBDR_CNT		1
-#define    RCV_QUEUE_CNT	1
-#define    SND_QUEUE_CNT	8
-#define    CMP_QUEUE_CNT	8 /* Max of RCV and SND qcount */
+#define RBDR_CNT		1
+#define RCV_QUEUE_CNT		8
+#define SND_QUEUE_CNT		8
+#define CMP_QUEUE_CNT		8 /* Max of RCV and SND qcount */
+
+#define SND_QSIZE		SND_QUEUE_SIZE4
+#define SND_QUEUE_LEN		(1ULL << (SND_QSIZE + 10))
+#define MAX_SND_QUEUE_LEN	(1ULL << (SND_QUEUE_SIZE6 + 10))
+#define SND_QUEUE_THRESH	2ULL
+#define MIN_SQ_DESC_PER_PKT_XMIT	2
+/* Since timestamp not enabled, otherwise 2 */
+#define MAX_CQE_PER_PKT_XMIT		1
 
-#define    SND_QUEUE_LEN	(1ULL << (SND_QUEUE_SIZE0 + 10))
-#define    SND_QUEUE_THRESH	2ULL
+#define CMP_QSIZE		CMP_QUEUE_SIZE4
+#define CMP_QUEUE_LEN		(1ULL << (CMP_QSIZE + 10))
+#define CMP_QUEUE_CQE_THRESH	0
+#define CMP_QUEUE_TIMER_THRESH	20 /* 20 usecs */
 
-#define    CMP_QUEUE_LEN	(1ULL << (CMP_QUEUE_SIZE1 + 10))
-#define    CMP_QUEUE_THRESH	0
+#define RBDR_SIZE		RBDR_SIZE0
+#define RCV_BUF_COUNT		(1ULL << (RBDR_SIZE + 13))
+#define MAX_RCV_BUF_COUNT	(1ULL << (RBDR_SIZE6 + 13))
+#define RBDR_THRESH		(RCV_BUF_COUNT / 2)
+#define DMA_BUFFER_LEN		2048 /* In multiples of 128bytes */
+#define RCV_FRAG_LEN		(SKB_DATA_ALIGN(DMA_BUFFER_LEN + NET_SKB_PAD) +\
+				SKB_DATA_ALIGN(sizeof(struct skb_shared_info))+\
+				NICVF_RCV_BUF_ALIGN_BYTES)
+#define RCV_DATA_OFFSET		NICVF_RCV_BUF_ALIGN_BYTES
 
-#define    RCV_BUF_COUNT	(1ULL << (RBDR_SIZE0 + 13))
-#define    RBDR_THRESH		2048
-#define    RCV_BUFFER_LEN	2048 /* In multiples of 128bytes */
-#define    RQ_CQ_DROP		((CMP_QUEUE_LEN - SND_QUEUE_LEN) / 256) /* To ensure CQEs for all transmitted pkts */
-#define    RQ_RBDR_DROP		((256) / 256) /* Drop rx pkts if free buffers fall below 256 */
+#define MAX_CQES_FOR_TX		((SND_QUEUE_LEN / MIN_SQ_DESC_PER_PKT_XMIT) *\
+				 MAX_CQE_PER_PKT_XMIT)
+#define RQ_CQ_DROP		((CMP_QUEUE_LEN - MAX_CQES_FOR_TX) / 256)
 
-/* Descriptor size */
-#define    SND_QUEUE_DESC_SIZE		16   /* 128 bits */
-#define    CMP_QUEUE_DESC_SIZE		512
+/* Descriptor size in bytes */
+#define SND_QUEUE_DESC_SIZE	16
+#define CMP_QUEUE_DESC_SIZE	512
 
 /* Buffer / descriptor alignments */
-#define    NICVF_RCV_BUF_ALIGN		7
-#define    NICVF_RCV_BUF_ALIGN_BYTES	(1ULL << NICVF_RCV_BUF_ALIGN)
-#define    NICVF_CQ_BASE_ALIGN_BYTES	512  /* 9 bits */
-#define    NICVF_SQ_BASE_ALIGN_BYTES	128  /* 7 bits */
+#define NICVF_RCV_BUF_ALIGN		7
+#define NICVF_RCV_BUF_ALIGN_BYTES	(1ULL << NICVF_RCV_BUF_ALIGN)
+#define NICVF_CQ_BASE_ALIGN_BYTES	512  /* 9 bits */
+#define NICVF_SQ_BASE_ALIGN_BYTES	128  /* 7 bits */
+
+#define NICVF_ALIGNED_ADDR(ADDR, ALIGN_BYTES)	ALIGN(ADDR, ALIGN_BYTES)
+#define NICVF_ADDR_ALIGN_LEN(ADDR, BYTES)\
+	(NICVF_ALIGNED_ADDR(ADDR, BYTES) - BYTES)
+#define NICVF_RCV_BUF_ALIGN_LEN(X)\
+	(NICVF_ALIGNED_ADDR(X, NICVF_RCV_BUF_ALIGN_BYTES) - X)
+
+/* Queue enable/disable */
+#define NICVF_SQ_EN            (1ULL << 19)
+
+/* Queue reset */
+#define NICVF_CQ_RESET		(1ULL << 41)
+#define NICVF_SQ_RESET		(1ULL << 17)
+#define NICVF_RBDR_RESET	(1ULL << 43)
+
+enum CQ_RX_ERRLVL_E {
+	CQ_ERRLVL_MAC,
+	CQ_ERRLVL_L2,
+	CQ_ERRLVL_L3,
+	CQ_ERRLVL_L4,
+};
+
+enum CQ_RX_ERROP_E {
+	CQ_RX_ERROP_RE_NONE = 0x0,
+	CQ_RX_ERROP_RE_PARTIAL = 0x1,
+	CQ_RX_ERROP_RE_JABBER = 0x2,
+	CQ_RX_ERROP_RE_FCS = 0x7,
+	CQ_RX_ERROP_RE_TERMINATE = 0x9,
+	CQ_RX_ERROP_RE_RX_CTL = 0xb,
+	CQ_RX_ERROP_PREL2_ERR = 0x1f,
+	CQ_RX_ERROP_L2_FRAGMENT = 0x20,
+	CQ_RX_ERROP_L2_OVERRUN = 0x21,
+	CQ_RX_ERROP_L2_PFCS = 0x22,
+	CQ_RX_ERROP_L2_PUNY = 0x23,
+	CQ_RX_ERROP_L2_MAL = 0x24,
+	CQ_RX_ERROP_L2_OVERSIZE = 0x25,
+	CQ_RX_ERROP_L2_UNDERSIZE = 0x26,
+	CQ_RX_ERROP_L2_LENMISM = 0x27,
+	CQ_RX_ERROP_L2_PCLP = 0x28,
+	CQ_RX_ERROP_IP_NOT = 0x41,
+	CQ_RX_ERROP_IP_CSUM_ERR = 0x42,
+	CQ_RX_ERROP_IP_MAL = 0x43,
+	CQ_RX_ERROP_IP_MALD = 0x44,
+	CQ_RX_ERROP_IP_HOP = 0x45,
+	CQ_RX_ERROP_L3_ICRC = 0x46,
+	CQ_RX_ERROP_L3_PCLP = 0x47,
+	CQ_RX_ERROP_L4_MAL = 0x61,
+	CQ_RX_ERROP_L4_CHK = 0x62,
+	CQ_RX_ERROP_UDP_LEN = 0x63,
+	CQ_RX_ERROP_L4_PORT = 0x64,
+	CQ_RX_ERROP_TCP_FLAG = 0x65,
+	CQ_RX_ERROP_TCP_OFFSET = 0x66,
+	CQ_RX_ERROP_L4_PCLP = 0x67,
+	CQ_RX_ERROP_RBDR_TRUNC = 0x70,
+};
+
+enum CQ_TX_ERROP_E {
+	CQ_TX_ERROP_GOOD = 0x0,
+	CQ_TX_ERROP_DESC_FAULT = 0x10,
+	CQ_TX_ERROP_HDR_CONS_ERR = 0x11,
+	CQ_TX_ERROP_SUBDC_ERR = 0x12,
+	CQ_TX_ERROP_IMM_SIZE_OFLOW = 0x80,
+	CQ_TX_ERROP_DATA_SEQUENCE_ERR = 0x81,
+	CQ_TX_ERROP_MEM_SEQUENCE_ERR = 0x82,
+	CQ_TX_ERROP_LOCK_VIOL = 0x83,
+	CQ_TX_ERROP_DATA_FAULT = 0x84,
+	CQ_TX_ERROP_TSTMP_CONFLICT = 0x85,
+	CQ_TX_ERROP_TSTMP_TIMEOUT = 0x86,
+	CQ_TX_ERROP_MEM_FAULT = 0x87,
+	CQ_TX_ERROP_CK_OVERLAP = 0x88,
+	CQ_TX_ERROP_CK_OFLOW = 0x89,
+	CQ_TX_ERROP_ENUM_LAST = 0x8a,
+};
+
+struct cmp_queue_stats {
+	struct rx_stats {
+		struct {
+			u64 mac_errs;
+			u64 l2_errs;
+			u64 l3_errs;
+			u64 l4_errs;
+		} errlvl;
+		struct {
+			u64 good;
+			u64 partial_pkts;
+			u64 jabber_errs;
+			u64 fcs_errs;
+			u64 terminate_errs;
+			u64 bgx_rx_errs;
+			u64 prel2_errs;
+			u64 l2_frags;
+			u64 l2_overruns;
+			u64 l2_pfcs;
+			u64 l2_puny;
+			u64 l2_hdr_malformed;
+			u64 l2_oversize;
+			u64 l2_undersize;
+			u64 l2_len_mismatch;
+			u64 l2_pclp;
+			u64 non_ip;
+			u64 ip_csum_err;
+			u64 ip_hdr_malformed;
+			u64 ip_payload_malformed;
+			u64 ip_hop_errs;
+			u64 l3_icrc_errs;
+			u64 l3_pclp;
+			u64 l4_malformed;
+			u64 l4_csum_errs;
+			u64 udp_len_err;
+			u64 bad_l4_port;
+			u64 bad_tcp_flag;
+			u64 tcp_offset_errs;
+			u64 l4_pclp;
+			u64 pkt_truncated;
+		} errop;
+	} rx;
+	struct tx_stats {
+		u64 good;
+		u64 desc_fault;
+		u64 hdr_cons_err;
+		u64 subdesc_err;
+		u64 imm_size_oflow;
+		u64 data_seq_err;
+		u64 mem_seq_err;
+		u64 lock_viol;
+		u64 data_fault;
+		u64 tstmp_conflict;
+		u64 tstmp_timeout;
+		u64 mem_fault;
+		u64 csum_overlap;
+		u64 csum_overflow;
+	} tx;
+} ____cacheline_aligned_in_smp;
 
-#define    NICVF_ALIGNED_ADDR(ADDR, ALIGN_BYTES)		ALIGN(ADDR, ALIGN_BYTES)
-#define    NICVF_ADDR_ALIGN_LEN(ADDR, BYTES)	(NICVF_ALIGNED_ADDR(ADDR, BYTES) - BYTES)
-#define    NICVF_RCV_BUF_ALIGN_LEN(X)		(NICVF_ALIGNED_ADDR(X, NICVF_RCV_BUF_ALIGN_BYTES) - X)
+enum RQ_SQ_STATS {
+	RQ_SQ_STATS_OCTS,
+	RQ_SQ_STATS_PKTS,
+};
+
+struct rx_tx_queue_stats {
+	u64	bytes;
+	u64	pkts;
+} ____cacheline_aligned_in_smp;
 
 struct q_desc_mem {
 	dma_addr_t	dma;
-	uint64_t	size;
-	uint16_t	q_len;
+	u64		size;
+	u16		q_len;
 	dma_addr_t	phys_base;
 	void		*base;
 	void		*unalign_base;
@@ -84,63 +254,78 @@ struct q_desc_mem {
 
 struct rbdr {
 	bool		enable;
-	uint32_t	buf_size;
-	uint32_t	thresh;      /* Threshold level for interrupt */
-	struct q_desc_mem   desc_mem;
-	struct rbdr_entry_t    *desc[RCV_BUF_COUNT];
-};
+	u32		dma_size;
+	u32		frag_len;
+	u32		thresh;      /* Threshold level for interrupt */
+	void		*desc;
+	u32		head;
+	u32		tail;
+	struct q_desc_mem   dmem;
+} ____cacheline_aligned_in_smp;
 
 struct rcv_queue {
-	struct	rbdr  *rbdr_start;
-	struct	rbdr  *rbdr_cont;
-	bool	en_tcp_reassembly;
-	uint8_t cq_qs;  /* CQ's QS to which this RQ is assigned */
-	uint8_t cq_idx; /* CQ index (0 to 7) in the QS */
-	uint8_t cont_rbdr_qs;      /* Continue buffer pointers - QS num */
-	uint8_t cont_qs_rbdr_idx;  /* RBDR idx in the cont QS */
-	uint8_t start_rbdr_qs;     /* First buffer pointers - QS num */
-	uint8_t start_qs_rbdr_idx; /* RBDR idx in the above QS */
-};
+	bool		enable;
+	struct	rbdr	*rbdr_start;
+	struct	rbdr	*rbdr_cont;
+	bool		en_tcp_reassembly;
+	u8		cq_qs;  /* CQ's QS to which this RQ is assigned */
+	u8		cq_idx; /* CQ index (0 to 7) in the QS */
+	u8		cont_rbdr_qs;      /* Continue buffer ptrs - QS num */
+	u8		cont_qs_rbdr_idx;  /* RBDR idx in the cont QS */
+	u8		start_rbdr_qs;     /* First buffer ptrs - QS num */
+	u8		start_qs_rbdr_idx; /* RBDR idx in the above QS */
+	u8		caching;
+	struct		rx_tx_queue_stats stats;
+} ____cacheline_aligned_in_smp;
 
 struct cmp_queue {
-	struct q_desc_mem   desc_mem;
-	uint8_t    intr_timer_thresh;
-	uint16_t   thresh;
-	spinlock_t cq_lock;  /* lock to serialize processing CQEs */
-};
-
-struct sq_desc {
-	bool   free;
-	struct sq_desc  *next;
-};
+	bool		enable;
+	u16		thresh;
+	spinlock_t	lock;  /* lock to serialize processing CQEs */
+	void		*desc;
+	struct q_desc_mem   dmem;
+	struct cmp_queue_stats	stats;
+	cpumask_t	affinity_mask;
+} ____cacheline_aligned_in_smp;
 
 struct snd_queue {
-	struct    q_desc_mem   desc_mem;
-	uint8_t   cq_qs;  /* CQ's QS to which this SQ is pointing */
-	uint8_t   cq_idx; /* CQ index (0 to 7) in the above QS */
-	uint16_t  thresh;
-	uint16_t  free_cnt;
-	uint64_t  head;
-	uint64_t  tail;
-	uint64_t  *skbuff;
-};
+	bool		enable;
+	u8		cq_qs;  /* CQ's QS to which this SQ is pointing */
+	u8		cq_idx; /* CQ index (0 to 7) in the above QS */
+	u16		thresh;
+	atomic_t	free_cnt;
+	u32		head;
+	u32		tail;
+	u64		*skbuff;
+	void		*desc;
+	cpumask_t	affinity_mask;
+	struct q_desc_mem   dmem;
+	struct rx_tx_queue_stats stats;
+} ____cacheline_aligned_in_smp;
 
 struct queue_set {
-	bool      enabled;
-	bool      be_en;
-	uint8_t   vnic_id;
-	uint8_t   rq_cnt;
-	struct	  rcv_queue rq[MAX_RCV_QUEUES_PER_QS];
-	uint8_t   cq_cnt;
-	struct    cmp_queue cq[MAX_CMP_QUEUES_PER_QS];
-	uint64_t  cq_len;
-	uint8_t   sq_cnt;
-	struct    snd_queue sq[MAX_SND_QUEUES_PER_QS];
-	uint64_t  sq_len;
-	uint8_t   rbdr_cnt;
-	struct    rbdr  rbdr[MAX_RCV_BUF_DESC_RINGS_PER_QS];
-	uint64_t  rbdr_len;
-};
+	bool		enable;
+	bool		be_en;
+	u8		vnic_id;
+	u8		rq_cnt;
+	u8		cq_cnt;
+	u64		cq_len;
+	u8		sq_cnt;
+	u64		sq_len;
+	u8		rbdr_cnt;
+	u64		rbdr_len;
+	struct	rcv_queue	rq[MAX_RCV_QUEUES_PER_QS];
+	struct	cmp_queue	cq[MAX_CMP_QUEUES_PER_QS];
+	struct	snd_queue	sq[MAX_SND_QUEUES_PER_QS];
+	struct	rbdr		rbdr[MAX_RCV_BUF_DESC_RINGS_PER_QS];
+} ____cacheline_aligned_in_smp;
+
+#define GET_RBDR_DESC(RING, idx)\
+		(&(((struct rbdr_entry_t *)((RING)->desc))[idx]))
+#define GET_SQ_DESC(RING, idx)\
+		(&(((struct sq_hdr_subdesc *)((RING)->desc))[idx]))
+#define GET_CQ_DESC(RING, idx)\
+		(&(((union cq_desc_t *)((RING)->desc))[idx]))
 
 /* CQ status bits */
 #define	CQ_WR_FULL	(1 << 26)
@@ -148,19 +333,22 @@ struct queue_set {
 #define	CQ_WR_FAULT	(1 << 24)
 #define	CQ_CQE_COUNT	(0xFFFF << 0)
 
-/* CQ err mask */
-#define		CQ_ERR_MASK	(CQ_WR_FULL | CQ_WR_DISABLE | CQ_WR_FAULT)
+#define	CQ_ERR_MASK	(CQ_WR_FULL | CQ_WR_DISABLE | CQ_WR_FAULT)
 
+int nicvf_set_qset_resources(struct nicvf *nic);
 int nicvf_config_data_transfer(struct nicvf *nic, bool enable);
 void nicvf_qset_config(struct nicvf *nic, bool enable);
+void nicvf_cmp_queue_config(struct nicvf *nic, struct queue_set *qs,
+			    int qidx, bool enable);
+
 void nicvf_sq_enable(struct nicvf *nic, struct snd_queue *sq, int qidx);
 void nicvf_sq_disable(struct nicvf *nic, int qidx);
 void nicvf_put_sq_desc(struct snd_queue *sq, int desc_cnt);
-void nicvf_sq_free_used_descs(struct net_device *netdev, struct snd_queue *sq, int qidx);
+void nicvf_sq_free_used_descs(struct net_device *netdev,
+			      struct snd_queue *sq, int qidx);
 int nicvf_sq_append_skb(struct nicvf *nic, struct sk_buff *skb);
 
-int nicvf_cq_check_errs(struct nicvf *nic, void *cq_desc);
-struct sk_buff *nicvf_get_rcv_skb(struct nicvf *nic, void *cq_desc);
+struct sk_buff *nicvf_get_rcv_skb(struct nicvf *nic, struct cqe_rx_t *cqe_rx);
 void nicvf_refill_rbdr(unsigned long data);
 
 void nicvf_enable_intr(struct nicvf *nic, int int_type, int q_idx);
@@ -169,14 +357,20 @@ void nicvf_clear_intr(struct nicvf *nic, int int_type, int q_idx);
 int nicvf_is_intr_enabled(struct nicvf *nic, int int_type, int q_idx);
 
 /* Register access APIs */
-void nicvf_reg_write(struct nicvf *nic, uint64_t offset, uint64_t val);
-uint64_t nicvf_reg_read(struct nicvf *nic, uint64_t offset);
-
-void nicvf_qset_reg_write(struct nicvf *nic, uint64_t offset, uint64_t val);
-uint64_t nicvf_qset_reg_read(struct nicvf *nic, uint64_t offset);
+void nicvf_reg_write(struct nicvf *nic, u64 offset, u64 val);
+u64  nicvf_reg_read(struct nicvf *nic, u64 offset);
+void nicvf_qset_reg_write(struct nicvf *nic, u64 offset, u64 val);
+u64 nicvf_qset_reg_read(struct nicvf *nic, u64 offset);
+void nicvf_queue_reg_write(struct nicvf *nic, u64 offset,
+			   u64 qidx, u64 val);
+u64  nicvf_queue_reg_read(struct nicvf *nic,
+			  u64 offset, u64 qidx);
 
-void nicvf_queue_reg_write(struct nicvf *nic, uint64_t offset,
-				uint64_t qidx, uint64_t val);
-uint64_t nicvf_queue_reg_read(struct nicvf *nic, uint64_t offset, uint64_t qidx);
-void nicvf_cmp_queue_config(struct nicvf *nic, struct queue_set *qs, int qidx, bool enable);
+/* Stats */
+void nicvf_update_rq_stats(struct nicvf *nic, int rq_idx);
+void nicvf_update_sq_stats(struct nicvf *nic, int sq_idx);
+int nicvf_check_cqe_rx_errs(struct nicvf *nic,
+			    struct cmp_queue *cq, struct cqe_rx_t *cqe_rx);
+int nicvf_check_cqe_tx_errs(struct nicvf *nic,
+			    struct cmp_queue *cq, struct cqe_send_t *cqe_tx);
 #endif /* NICVF_QUEUES_H */
diff --git a/drivers/net/ethernet/cavium/thunder/q_struct.h b/drivers/net/ethernet/cavium/thunder/q_struct.h
index 7e42e2eb..7d3f102 100644
--- a/drivers/net/ethernet/cavium/thunder/q_struct.h
+++ b/drivers/net/ethernet/cavium/thunder/q_struct.h
@@ -1,14 +1,28 @@
 /*
- * This file is subject to the terms and conditions of the GNU General Public
- * License.  See the file "COPYING" in the main directory of this archive
- * for more details.
+ * This file contains HW queue descriptor formats, config register
+ * structures e.t.c
  *
- * Copyright (C) 2013 Cavium, Inc.
+ * Copyright (C) 2015 Cavium, Inc.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of
+ * the License, or (at your option) any later version.
  */
 
 #ifndef Q_STRUCT_H
 #define Q_STRUCT_H
 
+/* Load transaction types for reading segment bytes specified by
+ * NIC_SEND_GATHER_S[LD_TYPE].
+ */
+enum nic_send_ld_type_e {
+	NIC_SEND_LD_TYPE_E_LDD = 0x0,
+	NIC_SEND_LD_TYPE_E_LDT = 0x1,
+	NIC_SEND_LD_TYPE_E_LDWB = 0x2,
+	NIC_SEND_LD_TYPE_E_ENUM_LAST = 0x3,
+};
+
 enum ether_type_algorithm {
 	ETYPE_ALG_NONE = 0x0,
 	ETYPE_ALG_SKIP = 0x1,
@@ -41,6 +55,14 @@ enum layer4_type {
 	L4TYPE_OTHER = 0x0E,
 };
 
+/* CPI and RSSI configuration */
+enum cpi_algorithm_type {
+	CPI_ALG_NONE = 0x0,
+	CPI_ALG_VLAN = 0x1,
+	CPI_ALG_VLAN16 = 0x2,
+	CPI_ALG_DIFF = 0x3,
+};
+
 enum rss_algorithm_type {
 	RSS_ALG_NONE = 0x00,
 	RSS_ALG_PORT = 0x01,
@@ -52,6 +74,18 @@ enum rss_algorithm_type {
 	RSS_ALG_ROCE = 0x07,
 };
 
+enum rss_hash_cfg {
+	RSS_HASH_L2ETC = 0x00,
+	RSS_HASH_IP = 0x01,
+	RSS_HASH_TCP = 0x02,
+	RSS_HASH_TCP_SYN_DIS = 0x03,
+	RSS_HASH_UDP = 0x04,
+	RSS_HASH_L4ETC = 0x05,
+	RSS_HASH_ROCE = 0x06,
+	RSS_L3_BIDI = 0x07,
+	RSS_L4_BIDI = 0x08,
+};
+
 /* Completion queue entry types */
 enum cqe_type {
 	CQE_TYPE_INVALID = 0x0,
@@ -140,186 +174,185 @@ enum cqe_rx_err_opcode {
 
 struct cqe_rx_t {
 #if defined(__BIG_ENDIAN_BITFIELD)
-	uint64_t   rb11_ptr:64;
-	uint64_t   rb10_ptr:64;
-	uint64_t   rb9_ptr:64;
-	uint64_t   rb8_ptr:64;
-	uint64_t   rb7_ptr:64;
-	uint64_t   rb6_ptr:64;
-	uint64_t   rb5_ptr:64;
-	uint64_t   rb4_ptr:64;
-	uint64_t   rb3_ptr:64;
-	uint64_t   rb2_ptr:64;
-	uint64_t   rb1_ptr:64;
-	uint64_t   rb0_ptr:64;
-	uint64_t   rb11_sz:16; /* W5 */
-	uint64_t   rb10_sz:16;
-	uint64_t   rb9_sz:16;
-	uint64_t   rb8_sz:16;
-	uint64_t   rb7_sz:16; /* W4 */
-	uint64_t   rb6_sz:16;
-	uint64_t   rb5_sz:16;
-	uint64_t   rb4_sz:16;
-	uint64_t   rb3_sz:16; /* W3 */
-	uint64_t   rb2_sz:16;
-	uint64_t   rb1_sz:16;
-	uint64_t   rb0_sz:16;
-	uint64_t   rss_tag:32; /* W2 */
-	uint64_t   vlan_tci:16;
-	uint64_t   vlan_ptr:8;
-	uint64_t   vlan2_ptr:8;
-	uint64_t   pkt_len:16; /* W1 */
-	uint64_t   l2_ptr:8;
-	uint64_t   l3_ptr:8;
-	uint64_t   l4_ptr:8;
-	uint64_t   cq_pkt_len:8;
-	uint64_t   align_len:3;
-	uint64_t   rsvd3:1;
-	uint64_t   chan:12;
-	uint64_t   cqe_type:4; /* W0 */
-	uint64_t   stdn_fault:1;
-	uint64_t   rsvd0:1;
-	uint64_t   rq_qs:7;
-	uint64_t   rq_idx:3;
-	uint64_t   rsvd1:12;
-	uint64_t   rss_alg:4;
-	uint64_t   rsvd2:4;
-	uint64_t   rb_cnt:4;
-	uint64_t   vlan_found:1;
-	uint64_t   vlan_stripped:1;
-	uint64_t   vlan2_found:1;
-	uint64_t   vlan2_stripped:1;
-	uint64_t   l4_type:4;
-	uint64_t   l3_type:4;
-	uint64_t   l2_present:1;
-	uint64_t   err_level:3;
-	uint64_t   err_opcode:8;
+	u64   cqe_type:4; /* W0 */
+	u64   stdn_fault:1;
+	u64   rsvd0:1;
+	u64   rq_qs:7;
+	u64   rq_idx:3;
+	u64   rsvd1:12;
+	u64   rss_alg:4;
+	u64   rsvd2:4;
+	u64   rb_cnt:4;
+	u64   vlan_found:1;
+	u64   vlan_stripped:1;
+	u64   vlan2_found:1;
+	u64   vlan2_stripped:1;
+	u64   l4_type:4;
+	u64   l3_type:4;
+	u64   l2_present:1;
+	u64   err_level:3;
+	u64   err_opcode:8;
+
+	u64   pkt_len:16; /* W1 */
+	u64   l2_ptr:8;
+	u64   l3_ptr:8;
+	u64   l4_ptr:8;
+	u64   cq_pkt_len:8;
+	u64   align_pad:3;
+	u64   rsvd3:1;
+	u64   chan:12;
+
+	u64   rss_tag:32; /* W2 */
+	u64   vlan_tci:16;
+	u64   vlan_ptr:8;
+	u64   vlan2_ptr:8;
+
+	u64   rb3_sz:16; /* W3 */
+	u64   rb2_sz:16;
+	u64   rb1_sz:16;
+	u64   rb0_sz:16;
+
+	u64   rb7_sz:16; /* W4 */
+	u64   rb6_sz:16;
+	u64   rb5_sz:16;
+	u64   rb4_sz:16;
+
+	u64   rb11_sz:16; /* W5 */
+	u64   rb10_sz:16;
+	u64   rb9_sz:16;
+	u64   rb8_sz:16;
 #elif defined(__LITTLE_ENDIAN_BITFIELD)
-	uint64_t   err_opcode:8;
-	uint64_t   err_level:3;
-	uint64_t   l2_present:1;
-	uint64_t   l3_type:4;
-	uint64_t   l4_type:4;
-	uint64_t   vlan2_stripped:1;
-	uint64_t   vlan2_found:1;
-	uint64_t   vlan_stripped:1;
-	uint64_t   vlan_found:1;
-	uint64_t   rb_cnt:4;
-	uint64_t   rsvd2:4;
-	uint64_t   rss_alg:4;
-	uint64_t   rsvd1:12;
-	uint64_t   rq_idx:3;
-	uint64_t   rq_qs:7;
-	uint64_t   rsvd0:1;
-	uint64_t   stdn_fault:1;
-	uint64_t   cqe_type:4; /* W0 */
-	uint64_t   chan:12;
-	uint64_t   rsvd3:1;
-	uint64_t   align_len:3;
-	uint64_t   cq_pkt_len:8;
-	uint64_t   l4_ptr:8;
-	uint64_t   l3_ptr:8;
-	uint64_t   l2_ptr:8;
-	uint64_t   pkt_len:16; /* W1 */
-	uint64_t   vlan2_ptr:8;
-	uint64_t   vlan_ptr:8;
-	uint64_t   vlan_tci:16;
-	uint64_t   rss_tag:32; /* W2 */
-	uint64_t   rb0_sz:16;
-	uint64_t   rb1_sz:16;
-	uint64_t   rb2_sz:16;
-	uint64_t   rb3_sz:16; /* W3 */
-	uint64_t   rb4_sz:16;
-	uint64_t   rb5_sz:16;
-	uint64_t   rb6_sz:16;
-	uint64_t   rb7_sz:16; /* W4 */
-	uint64_t   rb8_sz:16;
-	uint64_t   rb9_sz:16;
-	uint64_t   rb10_sz:16;
-	uint64_t   rb11_sz:16; /* W5 */
-	uint64_t   rb0_ptr:64;
-	uint64_t   rb1_ptr:64;
-	uint64_t   rb2_ptr:64;
-	uint64_t   rb3_ptr:64;
-	uint64_t   rb4_ptr:64;
-	uint64_t   rb5_ptr:64;
-	uint64_t   rb6_ptr:64;
-	uint64_t   rb7_ptr:64;
-	uint64_t   rb8_ptr:64;
-	uint64_t   rb9_ptr:64;
-	uint64_t   rb10_ptr:64;
-	uint64_t   rb11_ptr:64;
+	u64   err_opcode:8;
+	u64   err_level:3;
+	u64   l2_present:1;
+	u64   l3_type:4;
+	u64   l4_type:4;
+	u64   vlan2_stripped:1;
+	u64   vlan2_found:1;
+	u64   vlan_stripped:1;
+	u64   vlan_found:1;
+	u64   rb_cnt:4;
+	u64   rsvd2:4;
+	u64   rss_alg:4;
+	u64   rsvd1:12;
+	u64   rq_idx:3;
+	u64   rq_qs:7;
+	u64   rsvd0:1;
+	u64   stdn_fault:1;
+	u64   cqe_type:4; /* W0 */
+	u64   chan:12;
+	u64   rsvd3:1;
+	u64   align_pad:3;
+	u64   cq_pkt_len:8;
+	u64   l4_ptr:8;
+	u64   l3_ptr:8;
+	u64   l2_ptr:8;
+	u64   pkt_len:16; /* W1 */
+	u64   vlan2_ptr:8;
+	u64   vlan_ptr:8;
+	u64   vlan_tci:16;
+	u64   rss_tag:32; /* W2 */
+	u64   rb0_sz:16;
+	u64   rb1_sz:16;
+	u64   rb2_sz:16;
+	u64   rb3_sz:16; /* W3 */
+	u64   rb4_sz:16;
+	u64   rb5_sz:16;
+	u64   rb6_sz:16;
+	u64   rb7_sz:16; /* W4 */
+	u64   rb8_sz:16;
+	u64   rb9_sz:16;
+	u64   rb10_sz:16;
+	u64   rb11_sz:16; /* W5 */
 #endif
+	u64   rb0_ptr:64;
+	u64   rb1_ptr:64;
+	u64   rb2_ptr:64;
+	u64   rb3_ptr:64;
+	u64   rb4_ptr:64;
+	u64   rb5_ptr:64;
+	u64   rb6_ptr:64;
+	u64   rb7_ptr:64;
+	u64   rb8_ptr:64;
+	u64   rb9_ptr:64;
+	u64   rb10_ptr:64;
+	u64   rb11_ptr:64;
 };
 
 struct cqe_rx_tcp_err_t {
 #if defined(__BIG_ENDIAN_BITFIELD)
-	uint64_t   rsvd1:4;
-	uint64_t   partial_first:1;
-	uint64_t   rsvd2:27;
-	uint64_t   rbdr_bytes:8;
-	uint64_t   rsvd3:24;
-	uint64_t   cqe_type:4;
-	uint64_t   rsvd0:60;
+	u64   cqe_type:4; /* W0 */
+	u64   rsvd0:60;
+
+	u64   rsvd1:4; /* W1 */
+	u64   partial_first:1;
+	u64   rsvd2:27;
+	u64   rbdr_bytes:8;
+	u64   rsvd3:24;
 #elif defined(__LITTLE_ENDIAN_BITFIELD)
-	uint64_t   rsvd0:60;
-	uint64_t   cqe_type:4;
-	uint64_t   rsvd3:24;
-	uint64_t   rbdr_bytes:8;
-	uint64_t   rsvd2:27;
-	uint64_t   partial_first:1;
-	uint64_t   rsvd1:4;
+	u64   rsvd0:60;
+	u64   cqe_type:4;
+
+	u64   rsvd3:24;
+	u64   rbdr_bytes:8;
+	u64   rsvd2:27;
+	u64   partial_first:1;
+	u64   rsvd1:4;
 #endif
 };
 
 struct cqe_rx_tcp_t {
 #if defined(__BIG_ENDIAN_BITFIELD)
-	uint64_t   rsvd1:32;
-	uint64_t   tcp_cntx_bytes:8;
-	uint64_t   rsvd2:8;
-	uint64_t   tcp_err_bytes:16;
-	uint64_t   cqe_type:4;
-	uint64_t   rsvd0:52;
-	uint64_t   cq_tcp_status:8;
+	u64   cqe_type:4; /* W0 */
+	u64   rsvd0:52;
+	u64   cq_tcp_status:8;
+
+	u64   rsvd1:32; /* W1 */
+	u64   tcp_cntx_bytes:8;
+	u64   rsvd2:8;
+	u64   tcp_err_bytes:16;
 #elif defined(__LITTLE_ENDIAN_BITFIELD)
-	uint64_t   cq_tcp_status:8;
-	uint64_t   rsvd0:52;
-	uint64_t   cqe_type:4;
-	uint64_t   tcp_err_bytes:16;
-	uint64_t   rsvd2:8;
-	uint64_t   tcp_cntx_bytes:8;
-	uint64_t   rsvd1:32;
+	u64   cq_tcp_status:8;
+	u64   rsvd0:52;
+	u64   cqe_type:4; /* W0 */
+
+	u64   tcp_err_bytes:16;
+	u64   rsvd2:8;
+	u64   tcp_cntx_bytes:8;
+	u64   rsvd1:32; /* W1 */
 #endif
 };
 
 struct cqe_send_t {
 #if defined(__BIG_ENDIAN_BITFIELD)
-	uint64_t   ptp_timestamp:64;
-	uint64_t   cqe_type:4;
-	uint64_t   rsvd0:4;
-	uint64_t   sqe_ptr:16;
-	uint64_t   rsvd1:4;
-	uint64_t   rsvd2:10;
-	uint64_t   sq_qs:7;
-	uint64_t   sq_idx:3;
-	uint64_t   rsvd3:8;
-	uint64_t   send_status:8;
+	u64   cqe_type:4; /* W0 */
+	u64   rsvd0:4;
+	u64   sqe_ptr:16;
+	u64   rsvd1:4;
+	u64   rsvd2:10;
+	u64   sq_qs:7;
+	u64   sq_idx:3;
+	u64   rsvd3:8;
+	u64   send_status:8;
+
+	u64   ptp_timestamp:64; /* W1 */
 #elif defined(__LITTLE_ENDIAN_BITFIELD)
-	uint64_t   send_status:8;
-	uint64_t   rsvd3:8;
-	uint64_t   sq_idx:3;
-	uint64_t   sq_qs:7;
-	uint64_t   rsvd2:10;
-	uint64_t   rsvd1:4;
-	uint64_t   sqe_ptr:16;
-	uint64_t   rsvd0:4;
-	uint64_t   cqe_type:4;
-	uint64_t   ptp_timestamp:64;
+	u64   send_status:8;
+	u64   rsvd3:8;
+	u64   sq_idx:3;
+	u64   sq_qs:7;
+	u64   rsvd2:10;
+	u64   rsvd1:4;
+	u64   sqe_ptr:16;
+	u64   rsvd0:4;
+	u64   cqe_type:4; /* W0 */
+
+	u64   ptp_timestamp:64; /* W1 */
 #endif
 };
 
 union cq_desc_t {
-	uint64_t u[64];
+	u64    u[64];
 	struct cqe_send_t snd_hdr;
 	struct cqe_rx_t rx_hdr;
 	struct cqe_rx_tcp_t rx_tcp_hdr;
@@ -328,54 +361,54 @@ union cq_desc_t {
 
 struct rbdr_entry_t {
 #if defined(__BIG_ENDIAN_BITFIELD)
-	uint64_t   rsvd0:15;
-	uint64_t   buf_addr:42;
-	uint64_t   cache_align:7;
+	u64   rsvd0:15;
+	u64   buf_addr:42;
+	u64   cache_align:7;
 #elif defined(__LITTLE_ENDIAN_BITFIELD)
-	uint64_t   cache_align:7;
-	uint64_t   buf_addr:42;
-	uint64_t   rsvd0:15;
+	u64   cache_align:7;
+	u64   buf_addr:42;
+	u64   rsvd0:15;
 #endif
 };
 
 /* TCP reassembly context */
 struct rbe_tcp_cnxt_t {
 #if defined(__BIG_ENDIAN_BITFIELD)
-	uint64_t   tcp_pkt_cnt:12;
-	uint64_t   rsvd1:4;
-	uint64_t   align_hdr_bytes:4;
-	uint64_t   align_ptr_bytes:4;
-	uint64_t   ptr_bytes:16;
-	uint64_t   rsvd2:24;
-	uint64_t   cqe_type:4;
-	uint64_t   rsvd0:54;
-	uint64_t   tcp_end_reason:2;
-	uint64_t   tcp_status:4;
+	u64   tcp_pkt_cnt:12;
+	u64   rsvd1:4;
+	u64   align_hdr_bytes:4;
+	u64   align_ptr_bytes:4;
+	u64   ptr_bytes:16;
+	u64   rsvd2:24;
+	u64   cqe_type:4;
+	u64   rsvd0:54;
+	u64   tcp_end_reason:2;
+	u64   tcp_status:4;
 #elif defined(__LITTLE_ENDIAN_BITFIELD)
-	uint64_t   tcp_status:4;
-	uint64_t   tcp_end_reason:2;
-	uint64_t   rsvd0:54;
-	uint64_t   cqe_type:4;
-	uint64_t   rsvd2:24;
-	uint64_t   ptr_bytes:16;
-	uint64_t   align_ptr_bytes:4;
-	uint64_t   align_hdr_bytes:4;
-	uint64_t   rsvd1:4;
-	uint64_t   tcp_pkt_cnt:12;
+	u64   tcp_status:4;
+	u64   tcp_end_reason:2;
+	u64   rsvd0:54;
+	u64   cqe_type:4;
+	u64   rsvd2:24;
+	u64   ptr_bytes:16;
+	u64   align_ptr_bytes:4;
+	u64   align_hdr_bytes:4;
+	u64   rsvd1:4;
+	u64   tcp_pkt_cnt:12;
 #endif
 };
 
 /* Always Big endian */
 struct rx_hdr_t {
-	uint64_t   opaque:32;
-	uint64_t   rss_flow:8;
-	uint64_t   skip_length:6;
-	uint64_t   disable_rss:1;
-	uint64_t   disable_tcp_reassembly:1;
-	uint64_t   nodrop:1;
-	uint64_t   dest_alg:2;
-	uint64_t   rsvd0:2;
-	uint64_t   dest_rq:11;
+	u64   opaque:32;
+	u64   rss_flow:8;
+	u64   skip_length:6;
+	u64   disable_rss:1;
+	u64   disable_tcp_reassembly:1;
+	u64   nodrop:1;
+	u64   dest_alg:2;
+	u64   rsvd0:2;
+	u64   dest_rq:11;
 };
 
 enum send_l4_csum_type {
@@ -422,124 +455,247 @@ enum sq_subdesc_type {
 
 struct sq_crc_subdesc {
 #if defined(__BIG_ENDIAN_BITFIELD)
-	uint64_t    rsvd1:32;
-	uint64_t    crc_ival:32;
-	uint64_t    subdesc_type:4;
-	uint64_t    crc_alg:2;
-	uint64_t    rsvd0:10;
-	uint64_t    crc_insert_pos:16;
-	uint64_t    hdr_start:16;
-	uint64_t    crc_len:16;
+	u64    rsvd1:32;
+	u64    crc_ival:32;
+	u64    subdesc_type:4;
+	u64    crc_alg:2;
+	u64    rsvd0:10;
+	u64    crc_insert_pos:16;
+	u64    hdr_start:16;
+	u64    crc_len:16;
 #elif defined(__LITTLE_ENDIAN_BITFIELD)
-	uint64_t    crc_len:16;
-	uint64_t    hdr_start:16;
-	uint64_t    crc_insert_pos:16;
-	uint64_t    rsvd0:10;
-	uint64_t    crc_alg:2;
-	uint64_t    subdesc_type:4;
-	uint64_t    crc_ival:32;
-	uint64_t    rsvd1:32;
+	u64    crc_len:16;
+	u64    hdr_start:16;
+	u64    crc_insert_pos:16;
+	u64    rsvd0:10;
+	u64    crc_alg:2;
+	u64    subdesc_type:4;
+	u64    crc_ival:32;
+	u64    rsvd1:32;
 #endif
 };
 
 struct sq_gather_subdesc {
 #if defined(__BIG_ENDIAN_BITFIELD)
-	uint64_t    rsvd1:15;
-	uint64_t    addr:49;
-	uint64_t    subdesc_type:4;
-	uint64_t    ld_type:2;
-	uint64_t    rsvd0:42;
-	uint64_t    size:16;
+	u64    subdesc_type:4; /* W0 */
+	u64    ld_type:2;
+	u64    rsvd0:42;
+	u64    size:16;
+
+	u64    rsvd1:15; /* W1 */
+	u64    addr:49;
 #elif defined(__LITTLE_ENDIAN_BITFIELD)
-	uint64_t    size:16;
-	uint64_t    rsvd0:42;
-	uint64_t    ld_type:2;
-	uint64_t    subdesc_type:4;
-	uint64_t    addr:49;
-	uint64_t    rsvd1:15;
+	u64    size:16;
+	u64    rsvd0:42;
+	u64    ld_type:2;
+	u64    subdesc_type:4; /* W0 */
+
+	u64    addr:49;
+	u64    rsvd1:15; /* W1 */
 #endif
 };
 
 /* SQ immediate subdescriptor */
 struct sq_imm_subdesc {
 #if defined(__BIG_ENDIAN_BITFIELD)
-	uint64_t    data:64;
-	uint64_t    subdesc_type:4;
-	uint64_t    rsvd0:46;
-	uint64_t    len:14;
+	u64    subdesc_type:4; /* W0 */
+	u64    rsvd0:46;
+	u64    len:14;
+
+	u64    data:64; /* W1 */
 #elif defined(__LITTLE_ENDIAN_BITFIELD)
-	uint64_t    len:14;
-	uint64_t    rsvd0:46;
-	uint64_t    subdesc_type:4;
-	uint64_t    data:64;
+	u64    len:14;
+	u64    rsvd0:46;
+	u64    subdesc_type:4; /* W0 */
+
+	u64    data:64; /* W1 */
 #endif
 };
 
 struct sq_mem_subdesc {
 #if defined(__BIG_ENDIAN_BITFIELD)
-	uint64_t    rsvd1:15;
-	uint64_t    addr:49;
-	uint64_t    subdesc_type:4;
-	uint64_t    mem_alg:4;
-	uint64_t    mem_dsz:2;
-	uint64_t    wmem:1;
-	uint64_t    rsvd0:21;
-	uint64_t    offset:32;
+	u64    subdesc_type:4; /* W0 */
+	u64    mem_alg:4;
+	u64    mem_dsz:2;
+	u64    wmem:1;
+	u64    rsvd0:21;
+	u64    offset:32;
+
+	u64    rsvd1:15; /* W1 */
+	u64    addr:49;
 #elif defined(__LITTLE_ENDIAN_BITFIELD)
-	uint64_t    offset:32;
-	uint64_t    rsvd0:21;
-	uint64_t    wmem:1;
-	uint64_t    mem_dsz:2;
-	uint64_t    mem_alg:4;
-	uint64_t    subdesc_type:4;
-	uint64_t    addr:49;
-	uint64_t    rsvd1:15;
+	u64    offset:32;
+	u64    rsvd0:21;
+	u64    wmem:1;
+	u64    mem_dsz:2;
+	u64    mem_alg:4;
+	u64    subdesc_type:4; /* W0 */
+
+	u64    addr:49;
+	u64    rsvd1:15; /* W1 */
 #endif
 };
 
 struct sq_hdr_subdesc {
 #if defined(__BIG_ENDIAN_BITFIELD)
-	uint64_t    tso_sdc_cont:8;
-	uint64_t    tso_sdc_first:8;
-	uint64_t    tso_l4_offset:8;
-	uint64_t    tso_flags_last:12;
-	uint64_t    tso_flags_first:12;
-	uint64_t    rsvd2:2;
-	uint64_t    tso_max_paysize:14;
-	uint64_t    subdesc_type:4;
-	uint64_t    tso:1;
-	uint64_t    post_cqe:1; /* Post CQE on no error also */
-	uint64_t    dont_send:1;
-	uint64_t    tstmp:1;
-	uint64_t    subdesc_cnt:8;
-	uint64_t    csum_l4:2;
-	uint64_t    csum_l3:1;
-	uint64_t    rsvd0:5;
-	uint64_t    l4_offset:8;
-	uint64_t    l3_offset:8;
-	uint64_t    rsvd1:4;
-	uint64_t    tot_len:20;
+	u64    subdesc_type:4;
+	u64    tso:1;
+	u64    post_cqe:1; /* Post CQE on no error also */
+	u64    dont_send:1;
+	u64    tstmp:1;
+	u64    subdesc_cnt:8;
+	u64    csum_l4:2;
+	u64    csum_l3:1;
+	u64    rsvd0:5;
+	u64    l4_offset:8;
+	u64    l3_offset:8;
+	u64    rsvd1:4;
+	u64    tot_len:20; /* W0 */
+
+	u64    tso_sdc_cont:8;
+	u64    tso_sdc_first:8;
+	u64    tso_l4_offset:8;
+	u64    tso_flags_last:12;
+	u64    tso_flags_first:12;
+	u64    rsvd2:2;
+	u64    tso_max_paysize:14; /* W1 */
+#elif defined(__LITTLE_ENDIAN_BITFIELD)
+	u64    tot_len:20;
+	u64    rsvd1:4;
+	u64    l3_offset:8;
+	u64    l4_offset:8;
+	u64    rsvd0:5;
+	u64    csum_l3:1;
+	u64    csum_l4:2;
+	u64    subdesc_cnt:8;
+	u64    tstmp:1;
+	u64    dont_send:1;
+	u64    post_cqe:1; /* Post CQE on no error also */
+	u64    tso:1;
+	u64    subdesc_type:4; /* W0 */
+
+	u64    tso_max_paysize:14;
+	u64    rsvd2:2;
+	u64    tso_flags_first:12;
+	u64    tso_flags_last:12;
+	u64    tso_l4_offset:8;
+	u64    tso_sdc_first:8;
+	u64    tso_sdc_cont:8; /* W1 */
+#endif
+};
+
+/* Queue config register formats */
+struct rq_cfg {
+#if defined(__BIG_ENDIAN_BITFIELD)
+	u64 reserved_2_63:62;
+	u64 ena:1;
+	u64 tcp_ena:1;
+#elif defined(__LITTLE_ENDIAN_BITFIELD)
+	u64 tcp_ena:1;
+	u64 ena:1;
+	u64 reserved_2_63:62;
+#endif
+};
+
+struct cq_cfg {
+#if defined(__BIG_ENDIAN_BITFIELD)
+	u64 reserved_43_63:21;
+	u64 ena:1;
+	u64 reset:1;
+	u64 caching:1;
+	u64 reserved_35_39:5;
+	u64 qsize:3;
+	u64 reserved_25_31:7;
+	u64 avg_con:9;
+	u64 reserved_0_15:16;
+#elif defined(__LITTLE_ENDIAN_BITFIELD)
+	u64 reserved_0_15:16;
+	u64 avg_con:9;
+	u64 reserved_25_31:7;
+	u64 qsize:3;
+	u64 reserved_35_39:5;
+	u64 caching:1;
+	u64 reset:1;
+	u64 ena:1;
+	u64 reserved_43_63:21;
+#endif
+};
+
+struct sq_cfg {
+#if defined(__BIG_ENDIAN_BITFIELD)
+	u64 reserved_20_63:44;
+	u64 ena:1;
+	u64 reserved_18_18:1;
+	u64 reset:1;
+	u64 ldwb:1;
+	u64 reserved_11_15:5;
+	u64 qsize:3;
+	u64 reserved_3_7:5;
+	u64 tstmp_bgx_intf:3;
+#elif defined(__LITTLE_ENDIAN_BITFIELD)
+	u64 tstmp_bgx_intf:3;
+	u64 reserved_3_7:5;
+	u64 qsize:3;
+	u64 reserved_11_15:5;
+	u64 ldwb:1;
+	u64 reset:1;
+	u64 reserved_18_18:1;
+	u64 ena:1;
+	u64 reserved_20_63:44;
+#endif
+};
+
+struct rbdr_cfg {
+#if defined(__BIG_ENDIAN_BITFIELD)
+	u64 reserved_45_63:19;
+	u64 ena:1;
+	u64 reset:1;
+	u64 ldwb:1;
+	u64 reserved_36_41:6;
+	u64 qsize:4;
+	u64 reserved_25_31:7;
+	u64 avg_con:9;
+	u64 reserved_12_15:4;
+	u64 lines:12;
+#elif defined(__LITTLE_ENDIAN_BITFIELD)
+	u64 lines:12;
+	u64 reserved_12_15:4;
+	u64 avg_con:9;
+	u64 reserved_25_31:7;
+	u64 qsize:4;
+	u64 reserved_36_41:6;
+	u64 ldwb:1;
+	u64 reset:1;
+	u64 ena:1;
+	u64 reserved_45_63:19;
+#endif
+};
+
+struct qs_cfg {
+#if defined(__BIG_ENDIAN_BITFIELD)
+	u64 reserved_32_63:32;
+	u64 ena:1;
+	u64 reserved_27_30:4;
+	u64 sq_ins_ena:1;
+	u64 sq_ins_pos:6;
+	u64 lock_ena:1;
+	u64 lock_viol_cqe_ena:1;
+	u64 send_tstmp_ena:1;
+	u64 be:1;
+	u64 reserved_7_15:9;
+	u64 vnic:7;
 #elif defined(__LITTLE_ENDIAN_BITFIELD)
-	uint64_t    tot_len:20;
-	uint64_t    rsvd1:4;
-	uint64_t    l3_offset:8;
-	uint64_t    l4_offset:8;
-	uint64_t    rsvd0:5;
-	uint64_t    csum_l3:1;
-	uint64_t    csum_l4:2;
-	uint64_t    subdesc_cnt:8;
-	uint64_t    tstmp:1;
-	uint64_t    dont_send:1;
-	uint64_t    post_cqe:1; /* Post CQE on no error also */
-	uint64_t    tso:1;
-	uint64_t    subdesc_type:4;
-	uint64_t    tso_max_paysize:14;
-	uint64_t    rsvd2:2;
-	uint64_t    tso_flags_first:12;
-	uint64_t    tso_flags_last:12;
-	uint64_t    tso_l4_offset:8;
-	uint64_t    tso_sdc_first:8;
-	uint64_t    tso_sdc_cont:8;
+	u64 vnic:7;
+	u64 reserved_7_15:9;
+	u64 be:1;
+	u64 send_tstmp_ena:1;
+	u64 lock_viol_cqe_ena:1;
+	u64 lock_ena:1;
+	u64 sq_ins_pos:6;
+	u64 sq_ins_ena:1;
+	u64 reserved_27_30:4;
+	u64 ena:1;
+	u64 reserved_32_63:32;
 #endif
 };
 
diff --git a/drivers/net/ethernet/cavium/thunder/thunder_bgx.c b/drivers/net/ethernet/cavium/thunder/thunder_bgx.c
index dfa285b..82b77b4 100644
--- a/drivers/net/ethernet/cavium/thunder/thunder_bgx.c
+++ b/drivers/net/ethernet/cavium/thunder/thunder_bgx.c
@@ -1,20 +1,20 @@
 /*
- * This file is subject to the terms and conditions of the GNU General Public
- * License.  See the file "COPYING" in the main directory of this archive
- * for more details.
+ * Copyright (C) 2015 Cavium, Inc.
  *
- * Copyright (C) 2014 Cavium, Inc.
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of
+ * the License, or (at your option) any later version.
  */
 
 #include <linux/module.h>
-#include <linux/kernel.h>
-#include <linux/string.h>
-#include <linux/errno.h>
-#include <linux/types.h>
-#include <linux/init.h>
 #include <linux/interrupt.h>
-#include <linux/workqueue.h>
 #include <linux/pci.h>
+#include <linux/netdevice.h>
+#include <linux/phy.h>
+#include <linux/of.h>
+#include <linux/of_mdio.h>
+#include <linux/acpi.h>
 
 #include "nic_reg.h"
 #include "nic.h"
@@ -24,126 +24,961 @@
 #define DRV_VERSION	"1.0"
 
 struct lmac {
-	int dmac;
+	struct bgx		*bgx;
+	int			dmac;
+	bool			link_up;
+	int			lmacid; /* ID within BGX */
+	int			lmacid_bd; /* ID on board */
+	struct net_device       netdev;
+	struct phy_device       *phydev;
+	unsigned int            last_duplex;
+	unsigned int            last_link;
+	unsigned int            last_speed;
+	bool			is_sgmii;
+	struct delayed_work	dwork;
+	struct workqueue_struct *check_link;
 } lmac;
 
 struct bgx {
-	uint8_t	bgx_id;
-	struct lmac lmac[MAX_LMAC_PER_BGX];
-	uint64_t reg_base;
-	struct pci_dev *pdev;
+	u8			bgx_id;
+	u8			qlm_mode;
+	struct	lmac		lmac[MAX_LMAC_PER_BGX];
+	int			lmac_count;
+	int                     lmac_type;
+	int                     lane_to_sds;
+	int			use_training;
+	u64			reg_base;
+	struct pci_dev		*pdev;
 } bgx;
 
-struct bgx *bgx_vnic[MAX_BGX_PER_CN88XX];
+struct bgx *bgx_vnic[MAX_BGX_THUNDER];
+static int lmac_count; /* Total no of LMACs in system */
+
+static int bgx_xaui_check_link(struct lmac *lmac);
 
 /* Supported devices */
-static DEFINE_PCI_DEVICE_TABLE(bgx_id_table) = {
+static const struct pci_device_id bgx_id_table[] = {
 	{ PCI_DEVICE(PCI_VENDOR_ID_CAVIUM, PCI_DEVICE_ID_THUNDER_BGX) },
 	{ 0, }  /* end of table */
 };
 
 MODULE_AUTHOR("Cavium Inc");
 MODULE_DESCRIPTION("Cavium Thunder BGX/MAC Driver");
-MODULE_LICENSE("GPL");
+MODULE_LICENSE("GPL v2");
 MODULE_VERSION(DRV_VERSION);
 MODULE_DEVICE_TABLE(pci, bgx_id_table);
 
-static void bgx_reg_write(struct bgx *bgx, uint8_t lmac,
-			uint64_t offset, uint64_t val)
+/* Register read/write APIs */
+static u64 bgx_reg_read(struct bgx *bgx, u8 lmac, u64 offset)
+{
+	u64 addr = bgx->reg_base + ((u32)lmac << 20) + offset;
+
+	return readq_relaxed((void *)addr);
+}
+
+static void bgx_reg_write(struct bgx *bgx, u8 lmac,
+			  u64 offset, u64 val)
 {
-	uint64_t addr = bgx->reg_base + (lmac << 20) + offset;
+	u64 addr = bgx->reg_base + ((u32)lmac << 20) + offset;
 
 	writeq_relaxed(val, (void *)addr);
 }
 
-void bgx_add_dmac_addr(uint64_t dmac, uint64_t lmac)
+static void bgx_reg_modify(struct bgx *bgx, u8 lmac,
+			   u64 offset, u64 val)
+{
+	u64 addr = bgx->reg_base + ((u32)lmac << 20) + offset;
+
+	writeq_relaxed(val | bgx_reg_read(bgx, lmac, offset), (void *)addr);
+}
+
+static int bgx_poll_reg(struct bgx *bgx, u8 lmac,
+			u64 reg, u64 mask, bool zero)
+{
+	int timeout = 100;
+	u64 reg_val;
+
+	while (timeout) {
+		reg_val = bgx_reg_read(bgx, lmac, reg);
+		if (zero && !(reg_val & mask))
+			return 0;
+		if (!zero && (reg_val & mask))
+			return 0;
+		usleep_range(1000, 2000);
+		timeout--;
+	}
+	return 1;
+}
+
+/* Return number of BGX present in HW */
+void bgx_get_count(int node, int *bgx_count)
+{
+	int i;
+	struct bgx *bgx;
+
+	*bgx_count = 0;
+	for (i = 0; i < MAX_BGX_PER_CN88XX; i++) {
+		bgx = bgx_vnic[(node * MAX_BGX_PER_CN88XX) + i];
+		if (bgx)
+			*bgx_count |= (1 << i);
+	}
+}
+EXPORT_SYMBOL(bgx_get_count);
+
+/* Return number of LMAC configured for this BGX */
+int bgx_get_lmac_count(int node, int bgx_idx)
+{
+	struct bgx *bgx;
+
+	bgx = bgx_vnic[(node * MAX_BGX_PER_CN88XX) + bgx_idx];
+	if (bgx)
+		return bgx->lmac_count;
+
+	return 0;
+}
+EXPORT_SYMBOL(bgx_get_lmac_count);
+
+/* Returns the current link status of LMAC */
+void bgx_get_lmac_link_state(int node, int bgx_idx, int lmacid, void *status)
+{
+	struct bgx_link_status *link = (struct bgx_link_status *)status;
+	struct bgx *bgx;
+	struct lmac *lmac;
+
+	bgx = bgx_vnic[(node * MAX_BGX_PER_CN88XX) + bgx_idx];
+	if (!bgx)
+		return 0;
+
+	lmac = &bgx->lmac[lmacid];
+	link->link_up = lmac->link_up;
+	link->duplex = lmac->last_duplex;
+	link->speed = lmac->last_speed;
+}
+EXPORT_SYMBOL(bgx_get_lmac_link_state);
+
+static void bgx_sgmii_change_link_state(struct lmac *lmac)
+{
+	struct bgx *bgx = lmac->bgx;
+	u64 cmr_cfg;
+	u64 port_cfg = 0;
+	u64 misc_ctl = 0;
+
+	cmr_cfg = bgx_reg_read(bgx, lmac->lmacid, BGX_CMRX_CFG);
+	cmr_cfg &= ~CMR_EN;
+	bgx_reg_write(bgx, lmac->lmacid, BGX_CMRX_CFG, cmr_cfg);
+
+	port_cfg = bgx_reg_read(bgx, lmac->lmacid, BGX_GMP_GMI_PRTX_CFG);
+	misc_ctl = bgx_reg_read(bgx, lmac->lmacid, BGX_GMP_PCS_MISCX_CTL);
+
+	if (lmac->link_up) {
+		misc_ctl &= ~PCS_MISC_CTL_GMX_ENO;
+		port_cfg &= ~GMI_PORT_CFG_DUPLEX;
+		port_cfg |=  (lmac->last_duplex << 2);
+	} else {
+		misc_ctl |= PCS_MISC_CTL_GMX_ENO;
+	}
+
+	switch (lmac->last_speed) {
+	case 10:
+		port_cfg &= ~GMI_PORT_CFG_SPEED; /* speed 0 */
+		port_cfg |= GMI_PORT_CFG_SPEED_MSB;  /* speed_msb 1 */
+		port_cfg &= ~GMI_PORT_CFG_SLOT_TIME; /* slottime 0 */
+		misc_ctl &= ~PCS_MISC_CTL_SAMP_PT_MASK;
+		misc_ctl |= 50; /* samp_pt */
+		bgx_reg_write(bgx, lmac->lmacid, BGX_GMP_GMI_TXX_SLOT, 64);
+		bgx_reg_write(bgx, lmac->lmacid, BGX_GMP_GMI_TXX_BURST, 0);
+		break;
+	case 100:
+		port_cfg &= ~GMI_PORT_CFG_SPEED; /* speed 0 */
+		port_cfg &= ~GMI_PORT_CFG_SPEED_MSB; /* speed_msb 0 */
+		port_cfg &= ~GMI_PORT_CFG_SLOT_TIME; /* slottime 0 */
+		misc_ctl &= ~PCS_MISC_CTL_SAMP_PT_MASK;
+		misc_ctl |= 5; /* samp_pt */
+		bgx_reg_write(bgx, lmac->lmacid, BGX_GMP_GMI_TXX_SLOT, 64);
+		bgx_reg_write(bgx, lmac->lmacid, BGX_GMP_GMI_TXX_BURST, 0);
+		break;
+	case 1000:
+		port_cfg |= GMI_PORT_CFG_SPEED; /* speed 1 */
+		port_cfg &= ~GMI_PORT_CFG_SPEED_MSB; /* speed_msb 0 */
+		port_cfg |= GMI_PORT_CFG_SLOT_TIME; /* slottime 1 */
+		misc_ctl &= ~PCS_MISC_CTL_SAMP_PT_MASK;
+		misc_ctl |= 1; /* samp_pt */
+		bgx_reg_write(bgx, lmac->lmacid, BGX_GMP_GMI_TXX_SLOT, 512);
+		if (lmac->last_duplex)
+			bgx_reg_write(bgx, lmac->lmacid,
+				      BGX_GMP_GMI_TXX_BURST, 0);
+		else
+			bgx_reg_write(bgx, lmac->lmacid,
+				      BGX_GMP_GMI_TXX_BURST, 8192);
+		break;
+	default:
+		break;
+	}
+	bgx_reg_write(bgx, lmac->lmacid, BGX_GMP_PCS_MISCX_CTL, misc_ctl);
+	bgx_reg_write(bgx, lmac->lmacid, BGX_GMP_GMI_PRTX_CFG, port_cfg);
+
+	port_cfg = bgx_reg_read(bgx, lmac->lmacid, BGX_GMP_GMI_PRTX_CFG);
+
+	/* renable lmac */
+	cmr_cfg |= CMR_EN;
+	bgx_reg_write(bgx, lmac->lmacid, BGX_CMRX_CFG, cmr_cfg);
+}
+
+void bgx_lmac_handler(struct net_device *netdev)
+{
+	struct lmac *lmac = container_of(netdev, struct lmac, netdev);
+	struct phy_device *phydev = lmac->phydev;
+	int link_changed = 0;
+
+	if (!lmac)
+		return;
+
+	if (!phydev->link && lmac->last_link)
+		link_changed = -1;
+
+	if (phydev->link
+	    && (lmac->last_duplex != phydev->duplex
+		|| lmac->last_link != phydev->link
+		|| lmac->last_speed != phydev->speed)) {
+			link_changed = 1;
+	}
+
+	lmac->last_link = phydev->link;
+	lmac->last_speed = phydev->speed;
+	lmac->last_duplex = phydev->duplex;
+
+	if (!link_changed)
+		return;
+
+	if (link_changed > 0)
+		lmac->link_up = true;
+	else
+		lmac->link_up = false;
+
+	if (lmac->is_sgmii)
+		bgx_sgmii_change_link_state(lmac);
+	else
+		bgx_xaui_check_link(lmac);
+}
+
+u64 bgx_get_rx_stats(int bgx_idx, int lmac, int idx)
+{
+	struct bgx *bgx;
+
+	bgx = bgx_vnic[bgx_idx];
+	if (idx > 8)
+		lmac = 0;
+	return bgx_reg_read(bgx, lmac, BGX_CMRX_RX_STAT0 + (idx * 8));
+}
+EXPORT_SYMBOL(bgx_get_rx_stats);
+
+u64 bgx_get_tx_stats(int bgx_idx, int lmac, int idx)
 {
-	int bgx_index;
-	uint64_t offset, addr;
 	struct bgx *bgx;
 
-	bgx_index = lmac / MAX_LMAC_PER_BGX;
-	bgx = bgx_vnic[bgx_index];
+	bgx = bgx_vnic[bgx_idx];
+	return bgx_reg_read(bgx, lmac, BGX_CMRX_TX_STAT0 + (idx * 8));
+}
+EXPORT_SYMBOL(bgx_get_tx_stats);
+
+static void bgx_flush_dmac_addrs(struct bgx *bgx, int lmac)
+{
+	u64 dmac = 0x00;
+	u64 offset, addr;
+
+	while (bgx->lmac[lmac].dmac > 0) {
+		offset = ((bgx->lmac[lmac].dmac - 1) * sizeof(dmac)) +
+			(lmac * MAX_DMAC_PER_LMAC * sizeof(dmac));
+		addr = bgx->reg_base + BGX_CMR_RX_DMACX_CAM + offset;
+		writeq_relaxed(dmac, (void *)addr);
+		bgx->lmac[lmac].dmac--;
+	}
+}
+
+void bgx_add_dmac_addr(u64 dmac, int node, int bgx_idx, int lmac)
+{
+	u64 offset, addr;
+	struct bgx *bgx;
+
+#ifdef BGX_IN_PROMISCUOUS_MODE
+	return;
+#endif
+
+	bgx_idx += node * MAX_BGX_PER_CN88XX;
+	bgx = bgx_vnic[bgx_idx];
+
 	if (!bgx) {
-		pr_err("BGX%d not yet initialized, ignoring DMAC addition\n",
-								 bgx_index);
+		dev_err(&bgx->pdev->dev,
+			"BGX%d not yet initialized, ignoring DMAC addition\n",
+			bgx_idx);
 		return;
 	}
-	lmac = lmac % MAX_LMAC_PER_BGX;
-	dmac = dmac | (1ULL << 48) | (lmac << 49); /* Enable DMAC */
+
+	dmac = dmac | (1ULL << 48) | ((u64)lmac << 49); /* Enable DMAC */
 	if (bgx->lmac[lmac].dmac == MAX_DMAC_PER_LMAC) {
-		pr_err("Max DMAC filters for LMAC%lld reached, ignoring DMAC addition\n", lmac);
+		dev_err(&bgx->pdev->dev,
+			"Max DMAC filters for LMAC%d reached, ignoring\n",
+			lmac);
 		return;
 	}
-	/* Simulator supports only TNS by pass mode */
+
 	if (bgx->lmac[lmac].dmac == MAX_DMAC_PER_LMAC_TNS_BYPASS_MODE)
 		bgx->lmac[lmac].dmac = 1;
 
 	offset = (bgx->lmac[lmac].dmac * sizeof(dmac)) +
-					(lmac * MAX_DMAC_PER_LMAC * sizeof(dmac));
+		(lmac * MAX_DMAC_PER_LMAC * sizeof(dmac));
 	addr = bgx->reg_base + BGX_CMR_RX_DMACX_CAM + offset;
 	writeq_relaxed(dmac, (void *)addr);
 	bgx->lmac[lmac].dmac++;
+
+	bgx_reg_write(bgx, lmac, BGX_CMRX_RX_DMAC_CTL,
+		      (CAM_ACCEPT << 3) | (MCAST_MODE_CAM_FILTER << 1)
+		      | (BCAST_ACCEPT << 0));
 }
+EXPORT_SYMBOL(bgx_add_dmac_addr);
 
-void bgx_lmac_enable(uint64_t lmac)
+static int bgx_lmac_sgmii_init(struct bgx *bgx, int lmacid)
 {
-	int bgx_index;
-	struct bgx *bgx;
+	u64 cfg;
 
-	bgx_index = lmac / MAX_LMAC_PER_BGX;
-	bgx = bgx_vnic[bgx_index];
-	if (!bgx) {
-		pr_err("BGX%d not yet initialized, ignoring LMAC disable\n",
-								 bgx_index);
-		return;
+	bgx_reg_modify(bgx, lmacid, BGX_GMP_GMI_TXX_THRESH, 0x30);
+	/* max packet size */
+	bgx_reg_modify(bgx, lmacid, BGX_GMP_GMI_RXX_JABBER, MAX_FRAME_SIZE);
+
+	/* Disable frame alignment if using preamble */
+	cfg = bgx_reg_read(bgx, lmacid, BGX_GMP_GMI_TXX_APPEND);
+	if (cfg & 1)
+		bgx_reg_write(bgx, lmacid, BGX_GMP_GMI_TXX_SGMII_CTL, 0);
+
+	/* Enable lmac */
+	bgx_reg_modify(bgx, lmacid, BGX_CMRX_CFG, CMR_EN);
+
+	/* PCS reset */
+	bgx_reg_modify(bgx, lmacid, BGX_GMP_PCS_MRX_CTL, PCS_MRX_CTL_RESET);
+	if (bgx_poll_reg(bgx, lmacid, BGX_GMP_PCS_MRX_CTL,
+			 PCS_MRX_CTL_RESET, true)) {
+		dev_err(&bgx->pdev->dev, "BGX PCS reset not completed\n");
+		return -1;
 	}
-	lmac = lmac % MAX_LMAC_PER_BGX;
-	bgx_reg_write(bgx, lmac, BGX_CMRX_CFG,
-			(1 << 15) | (1 << 14) | (1 << 13));
+
+	/* power down, reset autoneg, autoneg enable */
+	cfg = bgx_reg_read(bgx, lmacid, BGX_GMP_PCS_MRX_CTL);
+	cfg &= ~PCS_MRX_CTL_PWR_DN;
+	cfg |= (PCS_MRX_CTL_RST_AN | PCS_MRX_CTL_AN_EN);
+	bgx_reg_write(bgx, lmacid, BGX_GMP_PCS_MRX_CTL, cfg);
+
+	if (bgx_poll_reg(bgx, lmacid, BGX_GMP_PCS_MRX_STATUS,
+			 PCS_MRX_STATUS_AN_CPT, false)) {
+		dev_err(&bgx->pdev->dev, "BGX AN_CPT not completed\n");
+		return -1;
+	}
+
+	return 0;
 }
 
-void bgx_lmac_disable(uint64_t lmac)
+static int bgx_lmac_xaui_init(struct bgx *bgx, int lmacid, int lmac_type)
 {
-	int bgx_index;
-	struct bgx *bgx;
+	u64 cfg;
 
-	bgx_index = lmac / MAX_LMAC_PER_BGX;
-	bgx = bgx_vnic[bgx_index];
-	if (!bgx) {
-		pr_err("BGX%d not yet initialized, ignoring LMAC disable\n",
-								 bgx_index);
-		return;
+	/* Reset SPU */
+	bgx_reg_modify(bgx, lmacid, BGX_SPUX_CONTROL1, SPU_CTL_RESET);
+	if (bgx_poll_reg(bgx, lmacid, BGX_SPUX_CONTROL1, SPU_CTL_RESET, true)) {
+		dev_err(&bgx->pdev->dev, "BGX SPU reset not completed\n");
+		return -1;
+	}
+
+	/* Disable LMAC */
+	cfg = bgx_reg_read(bgx, lmacid, BGX_CMRX_CFG);
+	cfg &= ~CMR_EN;
+	bgx_reg_write(bgx, lmacid, BGX_CMRX_CFG, cfg);
+
+	bgx_reg_modify(bgx, lmacid, BGX_SPUX_CONTROL1, SPU_CTL_LOW_POWER);
+	/* Set interleaved running disparity for RXAUI */
+	if (bgx->lmac_type != BGX_MODE_RXAUI)
+		bgx_reg_modify(bgx, lmacid,
+			       BGX_SPUX_MISC_CONTROL, SPU_MISC_CTL_RX_DIS);
+	else
+		bgx_reg_modify(bgx, lmacid, BGX_SPUX_MISC_CONTROL,
+			       SPU_MISC_CTL_RX_DIS | SPU_MISC_CTL_INTLV_RDISP);
+
+	/* clear all interrupts */
+	cfg = bgx_reg_read(bgx, lmacid, BGX_SMUX_RX_INT);
+	bgx_reg_write(bgx, lmacid, BGX_SMUX_RX_INT, cfg);
+	cfg = bgx_reg_read(bgx, lmacid, BGX_SMUX_TX_INT);
+	bgx_reg_write(bgx, lmacid, BGX_SMUX_TX_INT, cfg);
+	cfg = bgx_reg_read(bgx, lmacid, BGX_SPUX_INT);
+	bgx_reg_write(bgx, lmacid, BGX_SPUX_INT, cfg);
+
+	if (bgx->use_training) {
+		bgx_reg_write(bgx, lmacid, BGX_SPUX_BR_PMD_LP_CUP, 0x00);
+		bgx_reg_write(bgx, lmacid, BGX_SPUX_BR_PMD_LD_CUP, 0x00);
+		bgx_reg_write(bgx, lmacid, BGX_SPUX_BR_PMD_LD_REP, 0x00);
+		/* training enable */
+		bgx_reg_modify(bgx, lmacid,
+			       BGX_SPUX_BR_PMD_CRTL, SPU_PMD_CRTL_TRAIN_EN);
 	}
-	lmac = lmac % MAX_LMAC_PER_BGX;
-	bgx_reg_write(bgx, lmac, BGX_CMRX_CFG, 0x00);
+
+	/* Append FCS to each packet */
+	bgx_reg_modify(bgx, lmacid, BGX_SMUX_TX_APPEND, SMU_TX_APPEND_FCS_D);
+
+	/* Disable forward error correction */
+	cfg = bgx_reg_read(bgx, lmacid, BGX_SPUX_FEC_CONTROL);
+	cfg &= ~SPU_FEC_CTL_FEC_EN;
+	bgx_reg_write(bgx, lmacid, BGX_SPUX_FEC_CONTROL, cfg);
+
+	/* Disable autoneg */
+	cfg = bgx_reg_read(bgx, lmacid, BGX_SPUX_AN_CONTROL);
+	cfg = cfg & ~(SPU_AN_CTL_AN_EN | SPU_AN_CTL_XNP_EN);
+	bgx_reg_write(bgx, lmacid, BGX_SPUX_AN_CONTROL, cfg);
+
+	cfg = bgx_reg_read(bgx, lmacid, BGX_SPUX_AN_ADV);
+	if (bgx->lmac_type == BGX_MODE_10G_KR)
+		cfg |= (1 << 23);
+	else if (bgx->lmac_type == BGX_MODE_40G_KR)
+		cfg |= (1 << 24);
+	else
+		cfg &= ~((1 << 23) | (1 << 24));
+	cfg = cfg & (~((1ULL << 25) | (1ULL << 22) | (1ULL << 12)));
+	bgx_reg_write(bgx, lmacid, BGX_SPUX_AN_ADV, cfg);
+
+	cfg = bgx_reg_read(bgx, 0, BGX_SPU_DBG_CONTROL);
+	cfg &= ~SPU_DBG_CTL_AN_ARB_LINK_CHK_EN;
+	bgx_reg_write(bgx, 0, BGX_SPU_DBG_CONTROL, cfg);
+
+	/* Enable lmac */
+	bgx_reg_modify(bgx, lmacid, BGX_CMRX_CFG, CMR_EN);
+
+	cfg = bgx_reg_read(bgx, lmacid, BGX_SPUX_CONTROL1);
+	cfg &= ~SPU_CTL_LOW_POWER;
+	bgx_reg_write(bgx, lmacid, BGX_SPUX_CONTROL1, cfg);
+
+	cfg = bgx_reg_read(bgx, lmacid, BGX_SMUX_TX_CTL);
+	cfg &= ~SMU_TX_CTL_UNI_EN;
+	cfg |= SMU_TX_CTL_DIC_EN;
+	bgx_reg_write(bgx, lmacid, BGX_SMUX_TX_CTL, cfg);
+
+	/* take lmac_count into account */
+	bgx_reg_modify(bgx, lmacid, BGX_SMUX_TX_THRESH, (0x100 - 1));
+	/* max packet size */
+	bgx_reg_modify(bgx, lmacid, BGX_SMUX_RX_JABBER, MAX_FRAME_SIZE);
+
+	return 0;
 }
 
-static void bgx_init_hw(struct bgx *bgx)
+static int bgx_xaui_check_link(struct lmac *lmac)
 {
-	int lmac;
-	uint64_t enable = 0;
-	uint64_t dmac_bcast = (1ULL << 48) - 1;
+	struct bgx *bgx = lmac->bgx;
+	int lmacid = lmac->lmacid;
+	int lmac_type = bgx->lmac_type;
+	u64 cfg;
 
-	/* Enable all LMACs */
-	/* Enable LMAC, Pkt Rx enable, Pkt Tx enable */
-	enable = (1 << 15) | (1 << 14) | (1 << 13);
-	for (lmac = 0; lmac < MAX_LMAC_PER_BGX; lmac++)
-		bgx_reg_write(bgx, lmac, BGX_CMRX_CFG, enable);
+	bgx_reg_modify(bgx, lmacid, BGX_SPUX_MISC_CONTROL, SPU_MISC_CTL_RX_DIS);
+	if (bgx->use_training) {
+		cfg = bgx_reg_read(bgx, lmacid, BGX_SPUX_INT);
+		if (!(cfg & (1ull << 13))) {
+			cfg = (1ull << 13) | (1ull << 14);
+			bgx_reg_write(bgx, lmacid, BGX_SPUX_INT, cfg);
+			cfg = bgx_reg_read(bgx, lmacid, BGX_SPUX_BR_PMD_CRTL);
+			cfg |= (1ull << 0);
+			bgx_reg_write(bgx, lmacid, BGX_SPUX_BR_PMD_CRTL, cfg);
+			return -1;
+		}
+	}
+
+	/* wait for PCS to come out of reset */
+	if (bgx_poll_reg(bgx, lmacid, BGX_SPUX_CONTROL1, SPU_CTL_RESET, true)) {
+		dev_err(&bgx->pdev->dev, "BGX SPU reset not completed\n");
+		return -1;
+	}
+
+	if ((lmac_type == BGX_MODE_10G_KR) || (lmac_type == BGX_MODE_XFI) ||
+	    (lmac_type == BGX_MODE_40G_KR) || (lmac_type == BGX_MODE_XLAUI)) {
+		if (bgx_poll_reg(bgx, lmacid, BGX_SPUX_BR_STATUS1,
+				 SPU_BR_STATUS_BLK_LOCK, false)) {
+			dev_err(&bgx->pdev->dev,
+				"SPU_BR_STATUS_BLK_LOCK not completed\n");
+			return -1;
+		}
+	} else {
+		if (bgx_poll_reg(bgx, lmacid, BGX_SPUX_BX_STATUS,
+				 SPU_BX_STATUS_RX_ALIGN, false)) {
+			dev_err(&bgx->pdev->dev,
+				"SPU_BX_STATUS_RX_ALIGN not completed\n");
+			return -1;
+		}
+	}
+
+	/* Clear rcvflt bit (latching high) and read it back */
+	bgx_reg_modify(bgx, lmacid, BGX_SPUX_STATUS2, SPU_STATUS2_RCVFLT);
+	if (bgx_reg_read(bgx, lmacid, BGX_SPUX_STATUS2) & SPU_STATUS2_RCVFLT) {
+		dev_err(&bgx->pdev->dev, "Receive fault, retry training\n");
+		if (bgx->use_training) {
+			cfg = bgx_reg_read(bgx, lmacid, BGX_SPUX_INT);
+			if (!(cfg & (1ull << 13))) {
+				cfg = (1ull << 13) | (1ull << 14);
+				bgx_reg_write(bgx, lmacid, BGX_SPUX_INT, cfg);
+				cfg = bgx_reg_read(bgx, lmacid,
+						   BGX_SPUX_BR_PMD_CRTL);
+				cfg |= (1ull << 0);
+				bgx_reg_write(bgx, lmacid,
+					      BGX_SPUX_BR_PMD_CRTL, cfg);
+				return -1;
+			}
+		}
+		return -1;
+	}
+
+	/* Wait for MAC RX to be ready */
+	if (bgx_poll_reg(bgx, lmacid, BGX_SMUX_RX_CTL,
+			 SMU_RX_CTL_STATUS, true)) {
+		dev_err(&bgx->pdev->dev, "SMU RX link not okay\n");
+		return -1;
+	}
+
+	/* Wait for BGX RX to be idle */
+	if (bgx_poll_reg(bgx, lmacid, BGX_SMUX_CTL, SMU_CTL_RX_IDLE, false)) {
+		dev_err(&bgx->pdev->dev, "SMU RX not idle\n");
+		return -1;
+	}
+
+	/* Wait for BGX TX to be idle */
+	if (bgx_poll_reg(bgx, lmacid, BGX_SMUX_CTL, SMU_CTL_TX_IDLE, false)) {
+		dev_err(&bgx->pdev->dev, "SMU TX not idle\n");
+		return -1;
+	}
+
+	if (bgx_reg_read(bgx, lmacid, BGX_SPUX_STATUS2) & SPU_STATUS2_RCVFLT) {
+		dev_err(&bgx->pdev->dev, "Receive fault\n");
+		return -1;
+	}
+
+	/* Receive link is latching low. Force it high and verify it */
+	bgx_reg_modify(bgx, lmacid, BGX_SPUX_STATUS1, SPU_STATUS1_RCV_LNK);
+	if (bgx_poll_reg(bgx, lmacid, BGX_SPUX_STATUS1,
+			 SPU_STATUS1_RCV_LNK, false)) {
+		dev_err(&bgx->pdev->dev, "SPU receive link down\n");
+		return -1;
+	}
+
+	cfg = bgx_reg_read(bgx, lmacid, BGX_SPUX_MISC_CONTROL);
+	cfg &= ~SPU_MISC_CTL_RX_DIS;
+	bgx_reg_write(bgx, lmacid, BGX_SPUX_MISC_CONTROL, cfg);
+	return 0;
+}
+
+static void bgx_poll_for_link(struct work_struct *work)
+{
+	struct lmac *lmac;
+	u64 link;
+
+	lmac = container_of(work, struct lmac, dwork.work);
+
+	/* Receive link is latching low. Force it high and verify it */
+	bgx_reg_modify(lmac->bgx, lmac->lmacid,
+		       BGX_SPUX_STATUS1, SPU_STATUS1_RCV_LNK);
+	bgx_poll_reg(lmac->bgx, lmac->lmacid, BGX_SPUX_STATUS1,
+		     SPU_STATUS1_RCV_LNK, false);
+
+	link = bgx_reg_read(lmac->bgx, lmac->lmacid, BGX_SPUX_STATUS1);
+	if (link & SPU_STATUS1_RCV_LNK) {
+		lmac->link_up = 1;
+		if (lmac->bgx->lmac_type == BGX_MODE_XLAUI)
+			lmac->last_speed = 40000;
+		else
+			lmac->last_speed = 10000;
+		lmac->last_duplex = 1;
+	} else {
+		lmac->link_up = 0;
+	}
+
+	if (lmac->last_link != lmac->link_up) {
+		lmac->last_link = lmac->link_up;
+		if (lmac->link_up)
+			bgx_xaui_check_link(lmac);
+	}
+
+	queue_delayed_work(lmac->check_link, &lmac->dwork, HZ * 2);
+}
+
+static int bgx_lmac_enable(struct bgx *bgx, int8_t lmacid)
+{
+	u64 dmac_bcast = (1ULL << 48) - 1;
+	struct lmac *lmac;
+	u64 cfg;
+
+	lmac = &bgx->lmac[lmacid];
+	lmac->bgx = bgx;
+
+	if (bgx->lmac_type == BGX_MODE_SGMII) {
+		lmac->is_sgmii = 1;
+		if (bgx_lmac_sgmii_init(bgx, lmacid))
+			return -1;
+	} else {
+		lmac->is_sgmii = 0;
+		if (bgx_lmac_xaui_init(bgx, lmacid, bgx->lmac_type))
+			return -1;
+	}
+
+	if (lmac->is_sgmii) {
+		cfg = bgx_reg_read(bgx, lmacid, BGX_GMP_GMI_TXX_APPEND);
+		cfg |= ((1ull << 2) | (1ull << 1)); /* FCS and PAD */
+		bgx_reg_modify(bgx, lmacid, BGX_GMP_GMI_TXX_APPEND, cfg);
+		bgx_reg_write(bgx, lmacid, BGX_GMP_GMI_TXX_MIN_PKT, 60 - 1);
+	} else {
+		cfg = bgx_reg_read(bgx, lmacid, BGX_SMUX_TX_APPEND);
+		cfg |= ((1ull << 2) | (1ull << 1)); /* FCS and PAD */
+		bgx_reg_modify(bgx, lmacid, BGX_SMUX_TX_APPEND, cfg);
+		bgx_reg_write(bgx, lmacid, BGX_SMUX_TX_MIN_PKT, 60 + 4);
+	}
+
+	/* Enable lmac */
+	bgx_reg_modify(bgx, lmacid, BGX_CMRX_CFG,
+		       CMR_EN | CMR_PKT_RX_EN | CMR_PKT_TX_EN);
+
+	/* Restore default cfg, incase low level firmware changed it */
+	bgx_reg_write(bgx, lmacid, BGX_CMRX_RX_DMAC_CTL, 0x03);
 
 	/* Add broadcast MAC into all LMAC's DMAC filters */
-	for (lmac = 0; lmac < MAX_LMAC_PER_BGX; lmac++)
-		bgx_add_dmac_addr(dmac_bcast,
-				  lmac + bgx->bgx_id * MAX_LMAC_PER_BGX);
+	bgx_add_dmac_addr(dmac_bcast, 0, bgx->bgx_id, lmacid);
+
+	if ((bgx->lmac_type != BGX_MODE_XFI) &&
+	    (bgx->lmac_type != BGX_MODE_XLAUI) &&
+	    (bgx->lmac_type != BGX_MODE_40G_KR) &&
+	    (bgx->lmac_type != BGX_MODE_10G_KR)) {
+		if (!lmac->phydev)
+			return -ENODEV;
+
+		lmac->phydev->dev_flags = 0;
+
+		if (phy_connect_direct(&lmac->netdev, lmac->phydev,
+				       bgx_lmac_handler,
+				       PHY_INTERFACE_MODE_SGMII))
+			return -ENODEV;
+
+		phy_start_aneg(lmac->phydev);
+	} else {
+		lmac->check_link = alloc_workqueue("check_link", WQ_UNBOUND |
+						   WQ_MEM_RECLAIM, 1);
+		if (!lmac->check_link)
+			return -ENOMEM;
+		INIT_DELAYED_WORK(&lmac->dwork, bgx_poll_for_link);
+		queue_delayed_work(lmac->check_link, &lmac->dwork, 0);
+	}
+
+	return 0;
+}
+
+void bgx_lmac_disable(struct bgx *bgx, u8 lmacid)
+{
+	struct lmac *lmac;
+	u64 cmrx_cfg;
+
+	lmac = &bgx->lmac[lmacid];
+
+	cmrx_cfg = bgx_reg_read(bgx, lmacid, BGX_CMRX_CFG);
+	cmrx_cfg &= ~(1 << 15);
+	bgx_reg_write(bgx, lmacid, BGX_CMRX_CFG, cmrx_cfg);
+	bgx_flush_dmac_addrs(bgx, lmacid);
+
+	if (lmac->phydev)
+		phy_disconnect(lmac->phydev);
+
+	lmac->phydev = NULL;
+}
+
+static void bgx_set_num_ports(struct bgx *bgx)
+{
+	u64 lmac_count;
+
+	switch (bgx->qlm_mode) {
+	case QLM_MODE_SGMII:
+		bgx->lmac_count = 4;
+		bgx->lmac_type = BGX_MODE_SGMII;
+		bgx->lane_to_sds = 0;
+		break;
+	case QLM_MODE_XAUI_1X4:
+		bgx->lmac_count = 1;
+		bgx->lmac_type = BGX_MODE_XAUI;
+		bgx->lane_to_sds = 0xE4;
+			break;
+	case QLM_MODE_RXAUI_2X2:
+		bgx->lmac_count = 2;
+		bgx->lmac_type = BGX_MODE_RXAUI;
+		bgx->lane_to_sds = 0xE4;
+			break;
+	case QLM_MODE_XFI_4X1:
+		bgx->lmac_count = 4;
+		bgx->lmac_type = BGX_MODE_XFI;
+		bgx->lane_to_sds = 0;
+		break;
+	case QLM_MODE_XLAUI_1X4:
+		bgx->lmac_count = 1;
+		bgx->lmac_type = BGX_MODE_XLAUI;
+		bgx->lane_to_sds = 0xE4;
+		break;
+	case QLM_MODE_10G_KR_4X1:
+		bgx->lmac_count = 4;
+		bgx->lmac_type = BGX_MODE_10G_KR;
+		bgx->lane_to_sds = 0;
+		bgx->use_training = 1;
+		break;
+	case QLM_MODE_40G_KR4_1X4:
+		bgx->lmac_count = 1;
+		bgx->lmac_type = BGX_MODE_40G_KR;
+		bgx->lane_to_sds = 0xE4;
+		bgx->use_training = 1;
+		break;
+	default:
+		bgx->lmac_count = 0;
+		break;
+	}
+
+	/* Check if low level firmware has programmed LMAC count
+	 * based on board type, if yes consider that otherwise
+	 * the default static values
+	 */
+	lmac_count = bgx_reg_read(bgx, 0, BGX_CMR_RX_LMACS) & 0x7;
+	if (lmac_count != 4)
+		bgx->lmac_count = lmac_count;
+}
+
+static void bgx_init_hw(struct bgx *bgx)
+{
+	int i;
+
+	bgx_set_num_ports(bgx);
+
+	bgx_reg_modify(bgx, 0, BGX_CMR_GLOBAL_CFG, CMR_GLOBAL_CFG_FCS_STRIP);
+	if (bgx_reg_read(bgx, 0, BGX_CMR_BIST_STATUS))
+		dev_err(&bgx->pdev->dev, "BGX%d BIST failed\n", bgx->bgx_id);
+
+	/* Set lmac type and lane2serdes mapping */
+	for (i = 0; i < bgx->lmac_count; i++) {
+		if (bgx->lmac_type == BGX_MODE_RXAUI) {
+			if (i)
+				bgx->lane_to_sds = 0x0e;
+			else
+				bgx->lane_to_sds = 0x04;
+			bgx_reg_write(bgx, i, BGX_CMRX_CFG,
+				      (bgx->lmac_type << 8) | bgx->lane_to_sds);
+			continue;
+		}
+		bgx_reg_write(bgx, i, BGX_CMRX_CFG,
+			      (bgx->lmac_type << 8) | (bgx->lane_to_sds + i));
+		bgx->lmac[i].lmacid_bd = lmac_count;
+		lmac_count++;
+	}
+
+	bgx_reg_write(bgx, 0, BGX_CMR_TX_LMACS, bgx->lmac_count);
+	bgx_reg_write(bgx, 0, BGX_CMR_RX_LMACS, bgx->lmac_count);
+
+	/* Set the backpressure AND mask */
+	for (i = 0; i < bgx->lmac_count; i++)
+		bgx_reg_modify(bgx, 0, BGX_CMR_CHAN_MSK_AND,
+			       ((1ULL << MAX_BGX_CHANS_PER_LMAC) - 1) <<
+			       (i * MAX_BGX_CHANS_PER_LMAC));
+
+	/* Disable all MAC filtering */
+	for (i = 0; i < RX_DMAC_COUNT; i++)
+		bgx_reg_write(bgx, 0, BGX_CMR_RX_DMACX_CAM + (i * 8), 0x00);
+
+	/* Disable MAC steering (NCSI traffic) */
+	for (i = 0; i < RX_TRAFFIC_STEER_RULE_COUNT; i++)
+		bgx_reg_write(bgx, 0, BGX_CMR_RX_STREERING + (i * 8), 0x00);
+}
+
+static void bgx_get_qlm_mode(struct bgx *bgx)
+{
+	struct device *dev = &bgx->pdev->dev;
+	int lmac_type;
+	int train_en;
+
+	/* Read LMAC0 type to figure out QLM mode
+	 * This is configured by low level firmware
+	 **/
+	lmac_type = bgx_reg_read(bgx, 0, BGX_CMRX_CFG);
+	lmac_type = (lmac_type >> 8) & 0x07;
+
+	train_en = bgx_reg_read(bgx, 0, BGX_SPUX_BR_PMD_CRTL) &
+				SPU_PMD_CRTL_TRAIN_EN;
+
+	switch (lmac_type) {
+	case BGX_MODE_SGMII:
+		bgx->qlm_mode = QLM_MODE_SGMII;
+		dev_info(dev, "BGX%d QLM mode: SGMII\n", bgx->bgx_id);
+		break;
+	case BGX_MODE_XAUI:
+		bgx->qlm_mode = QLM_MODE_XAUI_1X4;
+		dev_info(dev, "BGX%d QLM mode: XAUI\n", bgx->bgx_id);
+		break;
+	case BGX_MODE_RXAUI:
+		bgx->qlm_mode = QLM_MODE_RXAUI_2X2;
+		dev_info(dev, "BGX%d QLM mode: RXAUI\n", bgx->bgx_id);
+		break;
+	case BGX_MODE_XFI:
+		if (!train_en) {
+			bgx->qlm_mode = QLM_MODE_XFI_4X1;
+			dev_info(dev, "BGX%d QLM mode: XFI\n", bgx->bgx_id);
+		} else {
+			bgx->qlm_mode = QLM_MODE_10G_KR_4X1;
+			dev_info(dev, "BGX%d QLM mode: 10G_KR\n", bgx->bgx_id);
+		}
+		break;
+	case BGX_MODE_XLAUI:
+		if (!train_en) {
+			bgx->qlm_mode = QLM_MODE_XLAUI_1X4;
+			dev_info(dev, "BGX%d QLM mode: XLAUI\n", bgx->bgx_id);
+		} else {
+			bgx->qlm_mode = QLM_MODE_40G_KR4_1X4;
+			dev_info(dev, "BGX%d QLM mode: 40G_KR4\n", bgx->bgx_id);
+		}
+		break;
+	default:
+		bgx->qlm_mode = QLM_MODE_SGMII;
+		dev_info(dev, "BGX%d QLM default mode: SGMII\n", bgx->bgx_id);
+	}
+}
+
+#ifdef CONFIG_ACPI
+static int
+bgx_match_phy_id(struct device *dev, void *data)
+{
+	struct phy_device *phydev = to_phy_device(dev);
+	u32 *phy_id = data;
+
+	if (phydev->addr == *phy_id)
+		return 1;
+
+	return 0;
+}
+
+static acpi_status
+bgx_acpi_register_phy(acpi_handle handle, u32 lvl, void *context, void **rv)
+{
+	struct acpi_reference_args args;
+	struct bgx *bgx = context;
+	struct acpi_device *adev;
+	struct device *phy_dev;
+	u32 phy_id;
+
+	if (acpi_bus_get_device(handle, &adev))
+		return AE_OK;
+
+	if (acpi_dev_get_property_reference(adev, "phy-handle", NULL, 0, &args))
+		return AE_OK;
+
+	if (acpi_dev_prop_read(args.adev, "phy-channel", DEV_PROP_U32, &phy_id))
+		return AE_OK;
+
+	phy_dev = bus_find_device(&mdio_bus_type, NULL, (void *)&phy_id,
+				  bgx_match_phy_id);
+	if (!phy_dev)
+		return AE_OK;
+
+	SET_NETDEV_DEV(&bgx->lmac[bgx->lmac_count].netdev, &bgx->pdev->dev);
+	bgx->lmac[bgx->lmac_count].phydev = to_phy_device(phy_dev);
+
+	bgx->lmac[bgx->lmac_count].lmacid = bgx->lmac_count;
+	bgx->lmac_count++;
+
+	return AE_OK;
+}
+
+static acpi_status
+bgx_acpi_match_id(acpi_handle handle, u32 lvl, void *context, void **ret_val)
+{
+	struct acpi_buffer string = { ACPI_ALLOCATE_BUFFER, NULL };
+	struct bgx *bgx = context;
+	char bgx_sel[5];
+
+	snprintf(bgx_sel, 5, "BGX%d", bgx->bgx_id);
+	if (ACPI_FAILURE(acpi_get_name(handle, ACPI_SINGLE_NAME, &string))) {
+		pr_warn("Invalid link device\n");
+		return AE_OK;
+	}
+
+	if (strncmp(string.pointer, bgx_sel, 4))
+		return AE_OK;
+
+	acpi_walk_namespace(ACPI_TYPE_DEVICE, handle, 1,
+			    bgx_acpi_register_phy, NULL, bgx, NULL);
+
+	kfree(string.pointer);
+	return AE_CTRL_TERMINATE;
+}
+static int
+bgx_init_acpi_phy(struct bgx *bgx)
+{
+	acpi_get_devices(NULL, bgx_acpi_match_id, bgx, (void **)NULL);
+	return 0;
+}
+
+#else
+static int
+bgx_init_acpi_phy(struct bgx *bgx)
+{
+	return -ENODEV;
+}
+#endif
+
+#ifdef CONFIG_OF_MDIO
+static int
+bgx_init_of_phy(struct bgx *bgx)
+{
+	struct device_node *np;
+	struct device_node *np_child;
+	u8 lmac = 0;
+	char bgx_sel[5];
+
+	/* Get BGX node from DT */
+	snprintf(bgx_sel, 5, "bgx%d", bgx->bgx_id);
+	np = of_find_node_by_name(NULL, bgx_sel);
+	if (!np)
+		return -ENODEV;
+
+	for_each_child_of_node(np, np_child) {
+		struct device_node *phy_np = of_parse_phandle(np_child,
+							      "phy-handle", 0);
+		if (!phy_np)
+			continue;
+		bgx->lmac[lmac].phydev = of_phy_find_device(phy_np);
+
+		SET_NETDEV_DEV(&bgx->lmac[lmac].netdev, &bgx->pdev->dev);
+		bgx->lmac[lmac].lmacid = lmac;
+		lmac++;
+		if (lmac == MAX_LMAC_PER_BGX)
+			break;
+	}
+	return 0;
+}
+#else
+static int
+bgx_init_of_phy(struct bgx *bgx)
+{
+	return -ENODEV;
+}
+#endif
+
+static int bgx_init_phy(struct bgx *bgx)
+{
+	int err = bgx_init_of_phy(bgx);
+
+	if (err != -ENODEV)
+		return err;
+
+	return bgx_init_acpi_phy(bgx);
 }
 
 static int bgx_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 {
+	int err;
 	struct device *dev = &pdev->dev;
-	struct bgx *bgx;
-	int    err;
+	struct bgx *bgx = NULL;
+	u8 lmac;
 
 	bgx = kzalloc(sizeof(*bgx), GFP_KERNEL);
+	if (!bgx)
+		return -ENOMEM;
 	bgx->pdev = pdev;
 
 	pci_set_drvdata(pdev, bgx);
@@ -161,20 +996,36 @@ static int bgx_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 	}
 
 	/* MAP configuration registers */
-	bgx->reg_base = (uint64_t) pci_ioremap_bar(pdev, PCI_CFG_REG_BAR_NUM);
+	bgx->reg_base = (u64)pci_ioremap_bar(pdev, PCI_CFG_REG_BAR_NUM);
 	if (!bgx->reg_base) {
 		dev_err(dev, "BGX: Cannot map CSR memory space, aborting\n");
 		err = -ENOMEM;
 		goto err_release_regions;
 	}
 	bgx->bgx_id = (pci_resource_start(pdev, PCI_CFG_REG_BAR_NUM) >> 24) & 1;
+	bgx->bgx_id += NODE_ID(pci_resource_start(pdev, PCI_CFG_REG_BAR_NUM))
+							* MAX_BGX_PER_CN88XX;
 	bgx_vnic[bgx->bgx_id] = bgx;
+	bgx_get_qlm_mode(bgx);
+
+	err = bgx_init_phy(bgx);
+	if (err)
+		goto err_enable;
 
-	/* Initialize BGX hardware */
 	bgx_init_hw(bgx);
 
-	goto exit;
+	/* Enable all LMACs */
+	for (lmac = 0; lmac < bgx->lmac_count; lmac++) {
+		err = bgx_lmac_enable(bgx, lmac);
+		if (err) {
+			dev_err(dev, "BGX%d failed to enable lmac%d\n",
+				bgx->bgx_id, lmac);
+			goto err_enable;
+		}
+	}
 
+	return 0;
+err_enable:
 	if (bgx->reg_base)
 		iounmap((void *)bgx->reg_base);
 err_release_regions:
@@ -182,15 +1033,21 @@ err_release_regions:
 err_disable_device:
 	pci_disable_device(pdev);
 exit:
+	bgx_vnic[bgx->bgx_id] = NULL;
+	kfree(bgx);
 	return err;
 }
 
 static void bgx_remove(struct pci_dev *pdev)
 {
 	struct bgx *bgx = pci_get_drvdata(pdev);
+	u8 lmac;
 
 	if (!bgx)
 		return;
+	/* Disable all LMACs */
+	for (lmac = 0; lmac < bgx->lmac_count; lmac++)
+		bgx_lmac_disable(bgx, lmac);
 
 	pci_set_drvdata(pdev, NULL);
 
@@ -223,4 +1080,3 @@ static void __exit bgx_cleanup_module(void)
 
 module_init(bgx_init_module);
 module_exit(bgx_cleanup_module);
-
diff --git a/drivers/net/ethernet/cavium/thunder/thunder_bgx.h b/drivers/net/ethernet/cavium/thunder/thunder_bgx.h
index 4ba925e..371d85b 100644
--- a/drivers/net/ethernet/cavium/thunder/thunder_bgx.h
+++ b/drivers/net/ethernet/cavium/thunder/thunder_bgx.h
@@ -1,27 +1,225 @@
 /*
- * This file is subject to the terms and conditions of the GNU General Public
- * License.  See the file "COPYING" in the main directory of this archive
- * for more details.
+ * Copyright (C) 2015 Cavium, Inc.
  *
- * Copyright (C) 2014 Cavium, Inc.
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of
+ * the License, or (at your option) any later version.
  */
 
 #ifndef THUNDER_BGX_H
 #define THUNDER_BGX_H
 
+#define    MAX_BGX_THUNDER			8 /* Max 4 nodes, 2 per node */
 #define    MAX_BGX_PER_CN88XX			2
 #define    MAX_LMAC_PER_BGX			4
 #define    MAX_BGX_CHANS_PER_LMAC		16
 #define    MAX_DMAC_PER_LMAC			8
+#define    MAX_FRAME_SIZE			9216
 
 #define    MAX_DMAC_PER_LMAC_TNS_BYPASS_MODE	2
 
+#define    MAX_LMAC	(MAX_BGX_PER_CN88XX * MAX_LMAC_PER_BGX)
+
+#define    NODE_ID_MASK				0x300000000000
+#define    NODE_ID(x)				((x & NODE_ID_MASK) >> 44)
+
 /* Registers */
-#define BGX_CMRX_CFG				0x00
-#define BGX_CMR_RX_DMACX_CAM			0x200
+#define BGX_CMRX_CFG			0x00
+#define CMR_PKT_TX_EN				(1ull << 13)
+#define CMR_PKT_RX_EN				(1ull << 14)
+#define CMR_EN					(1ull << 15)
+#define BGX_CMR_GLOBAL_CFG		0x08
+#define CMR_GLOBAL_CFG_FCS_STRIP		(1ull << 6)
+#define BGX_CMRX_RX_ID_MAP		0x60
+#define BGX_CMRX_RX_STAT0		0x70
+#define BGX_CMRX_RX_STAT1		0x78
+#define BGX_CMRX_RX_STAT2		0x80
+#define BGX_CMRX_RX_STAT3		0x88
+#define BGX_CMRX_RX_STAT4		0x90
+#define BGX_CMRX_RX_STAT5		0x98
+#define BGX_CMRX_RX_STAT6		0xA0
+#define BGX_CMRX_RX_STAT7		0xA8
+#define BGX_CMRX_RX_STAT8		0xB0
+#define BGX_CMRX_RX_STAT9		0xB8
+#define BGX_CMRX_RX_STAT10		0xC0
+#define BGX_CMRX_RX_BP_DROP		0xC8
+#define BGX_CMRX_RX_DMAC_CTL		0x0E8
+#define BGX_CMR_RX_DMACX_CAM		0x200
+#define RX_DMACX_CAM_EN				(1ull << 48)
+#define RX_DMACX_CAM_LMACID(x)			(x << 49)
+#define RX_DMAC_COUNT				32
+#define BGX_CMR_RX_STREERING		0x300
+#define RX_TRAFFIC_STEER_RULE_COUNT		8
+#define BGX_CMR_CHAN_MSK_AND		0x450
+#define BGX_CMR_BIST_STATUS		0x460
+#define BGX_CMR_RX_LMACS		0x468
+#define BGX_CMRX_TX_STAT0		0x600
+#define BGX_CMRX_TX_STAT1		0x608
+#define BGX_CMRX_TX_STAT2		0x610
+#define BGX_CMRX_TX_STAT3		0x618
+#define BGX_CMRX_TX_STAT4		0x620
+#define BGX_CMRX_TX_STAT5		0x628
+#define BGX_CMRX_TX_STAT6		0x630
+#define BGX_CMRX_TX_STAT7		0x638
+#define BGX_CMRX_TX_STAT8		0x640
+#define BGX_CMRX_TX_STAT9		0x648
+#define BGX_CMRX_TX_STAT10		0x650
+#define BGX_CMRX_TX_STAT11		0x658
+#define BGX_CMRX_TX_STAT12		0x660
+#define BGX_CMRX_TX_STAT13		0x668
+#define BGX_CMRX_TX_STAT14		0x670
+#define BGX_CMRX_TX_STAT15		0x678
+#define BGX_CMRX_TX_STAT16		0x680
+#define BGX_CMRX_TX_STAT17		0x688
+#define BGX_CMR_TX_LMACS		0x1000
+
+#define BGX_SPUX_CONTROL1		0x10000
+#define SPU_CTL_LOW_POWER			(1ull << 11)
+#define SPU_CTL_RESET				(1ull << 15)
+#define BGX_SPUX_STATUS1		0x10008
+#define SPU_STATUS1_RCV_LNK			(1ull << 2)
+#define BGX_SPUX_STATUS2		0x10020
+#define SPU_STATUS2_RCVFLT			(1ull << 10)
+#define BGX_SPUX_BX_STATUS		0x10028
+#define SPU_BX_STATUS_RX_ALIGN                  (1ull << 12)
+#define BGX_SPUX_BR_STATUS1		0x10030
+#define SPU_BR_STATUS_BLK_LOCK			(1ull << 0)
+#define SPU_BR_STATUS_RCV_LNK			(1ull << 12)
+#define BGX_SPUX_BR_PMD_CRTL		0x10068
+#define SPU_PMD_CRTL_TRAIN_EN			(1ull << 1)
+#define BGX_SPUX_BR_PMD_LP_CUP		0x10078
+#define BGX_SPUX_BR_PMD_LD_CUP		0x10088
+#define BGX_SPUX_BR_PMD_LD_REP		0x10090
+#define BGX_SPUX_FEC_CONTROL		0x100A0
+#define SPU_FEC_CTL_FEC_EN			(1ull << 0)
+#define SPU_FEC_CTL_ERR_EN			(1ull << 1)
+#define BGX_SPUX_AN_CONTROL		0x100C8
+#define SPU_AN_CTL_AN_EN			(1ull << 12)
+#define SPU_AN_CTL_XNP_EN			(1ull << 13)
+#define BGX_SPUX_AN_ADV			0x100D8
+#define BGX_SPUX_MISC_CONTROL		0x10218
+#define SPU_MISC_CTL_INTLV_RDISP		(1ull << 10)
+#define SPU_MISC_CTL_RX_DIS			(1ull << 12)
+#define BGX_SPUX_INT			0x10220	/* +(0..3) << 20 */
+#define BGX_SPUX_INT_W1S		0x10228
+#define BGX_SPUX_INT_ENA_W1C		0x10230
+#define BGX_SPUX_INT_ENA_W1S		0x10238
+#define BGX_SPU_DBG_CONTROL		0x10300
+#define SPU_DBG_CTL_AN_ARB_LINK_CHK_EN		(1ull << 18)
+#define SPU_DBG_CTL_AN_NONCE_MCT_DIS		(1ull << 29)
+
+#define BGX_SMUX_RX_INT			0x20000
+#define BGX_SMUX_RX_JABBER		0x20030
+#define BGX_SMUX_RX_CTL			0x20048
+#define SMU_RX_CTL_STATUS			(3ull << 0)
+#define BGX_SMUX_TX_APPEND		0x20100
+#define SMU_TX_APPEND_FCS_D			(1ull << 2)
+#define BGX_SMUX_TX_MIN_PKT		0x20118
+#define BGX_SMUX_TX_INT			0x20140
+#define BGX_SMUX_TX_CTL			0x20178
+#define SMU_TX_CTL_DIC_EN			(1ull << 0)
+#define SMU_TX_CTL_UNI_EN			(1ull << 1)
+#define SMU_TX_CTL_LNK_STATUS			(3ull << 4)
+#define BGX_SMUX_TX_THRESH		0x20180
+#define BGX_SMUX_CTL			0x20200
+#define SMU_CTL_RX_IDLE				(1ull << 0)
+#define SMU_CTL_TX_IDLE				(1ull << 1)
+
+#define BGX_GMP_PCS_MRX_CTL		0x30000
+#define	PCS_MRX_CTL_RST_AN			(1ull << 9)
+#define	PCS_MRX_CTL_PWR_DN			(1ull << 11)
+#define	PCS_MRX_CTL_AN_EN			(1ull << 12)
+#define	PCS_MRX_CTL_RESET			(1ull << 15)
+#define BGX_GMP_PCS_MRX_STATUS		0x30008
+#define	PCS_MRX_STATUS_AN_CPT			(1ull << 5)
+#define BGX_GMP_PCS_ANX_AN_RESULTS	0x30020
+#define BGX_GMP_PCS_SGM_AN_ADV		0x30068
+#define BGX_GMP_PCS_MISCX_CTL		0x30078
+#define PCS_MISC_CTL_GMX_ENO			(1ull << 11)
+#define PCS_MISC_CTL_SAMP_PT_MASK		0x7Full
+#define BGX_GMP_GMI_PRTX_CFG		0x38020
+#define GMI_PORT_CFG_SPEED			(1ull << 1)
+#define GMI_PORT_CFG_DUPLEX			(1ull << 2)
+#define GMI_PORT_CFG_SLOT_TIME			(1ull << 3)
+#define GMI_PORT_CFG_SPEED_MSB			(1ull << 8)
+#define BGX_GMP_GMI_RXX_JABBER		0x38038
+#define BGX_GMP_GMI_TXX_THRESH		0x38210
+#define BGX_GMP_GMI_TXX_APPEND		0x38218
+#define BGX_GMP_GMI_TXX_SLOT		0x38220
+#define BGX_GMP_GMI_TXX_BURST		0x38228
+#define BGX_GMP_GMI_TXX_MIN_PKT		0x38240
+#define BGX_GMP_GMI_TXX_SGMII_CTL	0x38300
+
+#define BGX_MSIX_VEC_0_29_ADDR		0x400000 /* +(0..29) << 4 */
+#define BGX_MSIX_VEC_0_29_CTL		0x400008
+#define BGX_MSIX_PBA_0			0x4F0000
+
+/* MSI-X interrupts */
+#define BGX_MSIX_VECTORS	30
+#define BGX_LMAC_VEC_OFFSET	7
+#define BGX_MSIX_VEC_SHIFT	4
+
+#define CMRX_INT		0
+#define SPUX_INT		1
+#define SMUX_RX_INT		2
+#define SMUX_TX_INT		3
+#define GMPX_PCS_INT		4
+#define GMPX_GMI_RX_INT		5
+#define GMPX_GMI_TX_INT		6
+#define CMR_MEM_INT		28
+#define SPU_MEM_INT		29
+
+#define LMAC_INTR_LINK_UP	(1 << 0)
+#define LMAC_INTR_LINK_DOWN	(1 << 1)
+
+/*  RX_DMAC_CTL configuration*/
+enum MCAST_MODE {
+		MCAST_MODE_REJECT,
+		MCAST_MODE_ACCEPT,
+		MCAST_MODE_CAM_FILTER,
+		RSVD
+};
+
+#define BCAST_ACCEPT	1
+#define CAM_ACCEPT	1
+
+void bgx_add_dmac_addr(u64 dmac, int node, int bgx_idx, int lmac);
+void bgx_get_count(int node, int *bgx_count);
+int bgx_get_lmac_count(int node, int bgx);
+void bgx_get_lmac_link_state(int node, int bgx_idx, int lmacid, void *status);
+u64 bgx_get_rx_stats(int bgx_idx, int lmac, int idx);
+u64 bgx_get_tx_stats(int bgx_idx, int lmac, int idx);
+#define BGX_RX_STATS_COUNT 11
+#define BGX_TX_STATS_COUNT 18
+
+struct bgx_stats {
+	u64 rx_stats[BGX_RX_STATS_COUNT];
+	u64 tx_stats[BGX_TX_STATS_COUNT];
+};
+
+#undef LINK_INTR_ENABLE
+#define BGX_IN_PROMISCUOUS_MODE 1
+
+enum LMAC_TYPE {
+	BGX_MODE_SGMII = 0, /* 1 lane, 1.250 Gbaud */
+	BGX_MODE_XAUI = 1,  /* 4 lanes, 3.125 Gbaud */
+	BGX_MODE_DXAUI = 1, /* 4 lanes, 6.250 Gbaud */
+	BGX_MODE_RXAUI = 2, /* 2 lanes, 6.250 Gbaud */
+	BGX_MODE_XFI = 3,   /* 1 lane, 10.3125 Gbaud */
+	BGX_MODE_XLAUI = 4, /* 4 lanes, 10.3125 Gbaud */
+	BGX_MODE_10G_KR = 3,/* 1 lane, 10.3125 Gbaud */
+	BGX_MODE_40G_KR = 4,/* 4 lanes, 10.3125 Gbaud */
+};
 
-void bgx_add_dmac_addr(uint64_t dmac, uint64_t lmac);
-void bgx_lmac_disable(uint64_t lmac);
-void bgx_lmac_enable(uint64_t lmac);
+enum qlm_mode {
+	QLM_MODE_SGMII,         /* SGMII, each lane independent */
+	QLM_MODE_XAUI_1X4,      /* 1 XAUI or DXAUI, 4 lanes */
+	QLM_MODE_RXAUI_2X2,     /* 2 RXAUI, 2 lanes each */
+	QLM_MODE_XFI_4X1,       /* 4 XFI, 1 lane each */
+	QLM_MODE_XLAUI_1X4,     /* 1 XLAUI, 4 lanes each */
+	QLM_MODE_10G_KR_4X1,    /* 4 10GBASE-KR, 1 lane each */
+	QLM_MODE_40G_KR4_1X4,   /* 1 40GBASE-KR4, 4 lanes each */
+};
 
 #endif /* THUNDER_BGX_H */
diff --git a/drivers/net/phy/Kconfig b/drivers/net/phy/Kconfig
index 9b5d46c..99fa2e3 100644
--- a/drivers/net/phy/Kconfig
+++ b/drivers/net/phy/Kconfig
@@ -135,7 +135,7 @@ config MDIO_GPIO
 
 config MDIO_OCTEON
 	tristate "Support for MDIO buses on Octeon SOCs"
-	depends on CAVIUM_OCTEON_SOC
+	depends on (CAVIUM_OCTEON_SOC || ARCH_THUNDER)
 	default y
 	help
 
diff --git a/drivers/net/phy/cvmx-smix-defs.h b/drivers/net/phy/cvmx-smix-defs.h
new file mode 100644
index 0000000..ef9a2e6
--- /dev/null
+++ b/drivers/net/phy/cvmx-smix-defs.h
@@ -0,0 +1,254 @@
+/***********************license start***************
+ * Author: Cavium Networks
+ *
+ * Contact: support@caviumnetworks.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2012 Cavium Networks
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium Networks for more information
+ ***********************license end**************************************/
+
+#ifndef __CVMX_SMIX_DEFS_H__
+#define __CVMX_SMIX_DEFS_H__
+
+union cvmx_smix_clk {
+	uint64_t u64;
+	struct cvmx_smix_clk_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+		uint64_t reserved_25_63:39;
+		uint64_t mode:1;
+		uint64_t reserved_21_23:3;
+		uint64_t sample_hi:5;
+		uint64_t sample_mode:1;
+		uint64_t reserved_14_14:1;
+		uint64_t clk_idle:1;
+		uint64_t preamble:1;
+		uint64_t sample:4;
+		uint64_t phase:8;
+#else
+		uint64_t phase:8;
+		uint64_t sample:4;
+		uint64_t preamble:1;
+		uint64_t clk_idle:1;
+		uint64_t reserved_14_14:1;
+		uint64_t sample_mode:1;
+		uint64_t sample_hi:5;
+		uint64_t reserved_21_23:3;
+		uint64_t mode:1;
+		uint64_t reserved_25_63:39;
+#endif
+	} s;
+	struct cvmx_smix_clk_cn30xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+		uint64_t reserved_21_63:43;
+		uint64_t sample_hi:5;
+		uint64_t sample_mode:1;
+		uint64_t reserved_14_14:1;
+		uint64_t clk_idle:1;
+		uint64_t preamble:1;
+		uint64_t sample:4;
+		uint64_t phase:8;
+#else
+		uint64_t phase:8;
+		uint64_t sample:4;
+		uint64_t preamble:1;
+		uint64_t clk_idle:1;
+		uint64_t reserved_14_14:1;
+		uint64_t sample_mode:1;
+		uint64_t sample_hi:5;
+		uint64_t reserved_21_63:43;
+#endif
+	} cn30xx;
+	struct cvmx_smix_clk_cn30xx cn31xx;
+	struct cvmx_smix_clk_cn30xx cn38xx;
+	struct cvmx_smix_clk_cn30xx cn38xxp2;
+	struct cvmx_smix_clk_s cn50xx;
+	struct cvmx_smix_clk_s cn52xx;
+	struct cvmx_smix_clk_s cn52xxp1;
+	struct cvmx_smix_clk_s cn56xx;
+	struct cvmx_smix_clk_s cn56xxp1;
+	struct cvmx_smix_clk_cn30xx cn58xx;
+	struct cvmx_smix_clk_cn30xx cn58xxp1;
+	struct cvmx_smix_clk_s cn61xx;
+	struct cvmx_smix_clk_s cn63xx;
+	struct cvmx_smix_clk_s cn63xxp1;
+	struct cvmx_smix_clk_s cn66xx;
+	struct cvmx_smix_clk_s cn68xx;
+	struct cvmx_smix_clk_s cn68xxp1;
+	struct cvmx_smix_clk_s cnf71xx;
+};
+
+union cvmx_smix_cmd {
+	uint64_t u64;
+	struct cvmx_smix_cmd_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+		uint64_t reserved_18_63:46;
+		uint64_t phy_op:2;
+		uint64_t reserved_13_15:3;
+		uint64_t phy_adr:5;
+		uint64_t reserved_5_7:3;
+		uint64_t reg_adr:5;
+#else
+		uint64_t reg_adr:5;
+		uint64_t reserved_5_7:3;
+		uint64_t phy_adr:5;
+		uint64_t reserved_13_15:3;
+		uint64_t phy_op:2;
+		uint64_t reserved_18_63:46;
+#endif
+	} s;
+	struct cvmx_smix_cmd_cn30xx {
+#ifdef __BIG_ENDIAN_BITFIELD
+		uint64_t reserved_17_63:47;
+		uint64_t phy_op:1;
+		uint64_t reserved_13_15:3;
+		uint64_t phy_adr:5;
+		uint64_t reserved_5_7:3;
+		uint64_t reg_adr:5;
+#else
+		uint64_t reg_adr:5;
+		uint64_t reserved_5_7:3;
+		uint64_t phy_adr:5;
+		uint64_t reserved_13_15:3;
+		uint64_t phy_op:1;
+		uint64_t reserved_17_63:47;
+#endif
+	} cn30xx;
+	struct cvmx_smix_cmd_cn30xx cn31xx;
+	struct cvmx_smix_cmd_cn30xx cn38xx;
+	struct cvmx_smix_cmd_cn30xx cn38xxp2;
+	struct cvmx_smix_cmd_s cn50xx;
+	struct cvmx_smix_cmd_s cn52xx;
+	struct cvmx_smix_cmd_s cn52xxp1;
+	struct cvmx_smix_cmd_s cn56xx;
+	struct cvmx_smix_cmd_s cn56xxp1;
+	struct cvmx_smix_cmd_cn30xx cn58xx;
+	struct cvmx_smix_cmd_cn30xx cn58xxp1;
+	struct cvmx_smix_cmd_s cn61xx;
+	struct cvmx_smix_cmd_s cn63xx;
+	struct cvmx_smix_cmd_s cn63xxp1;
+	struct cvmx_smix_cmd_s cn66xx;
+	struct cvmx_smix_cmd_s cn68xx;
+	struct cvmx_smix_cmd_s cn68xxp1;
+	struct cvmx_smix_cmd_s cnf71xx;
+};
+
+union cvmx_smix_en {
+	uint64_t u64;
+	struct cvmx_smix_en_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+		uint64_t reserved_1_63:63;
+		uint64_t en:1;
+#else
+		uint64_t en:1;
+		uint64_t reserved_1_63:63;
+#endif
+	} s;
+	struct cvmx_smix_en_s cn30xx;
+	struct cvmx_smix_en_s cn31xx;
+	struct cvmx_smix_en_s cn38xx;
+	struct cvmx_smix_en_s cn38xxp2;
+	struct cvmx_smix_en_s cn50xx;
+	struct cvmx_smix_en_s cn52xx;
+	struct cvmx_smix_en_s cn52xxp1;
+	struct cvmx_smix_en_s cn56xx;
+	struct cvmx_smix_en_s cn56xxp1;
+	struct cvmx_smix_en_s cn58xx;
+	struct cvmx_smix_en_s cn58xxp1;
+	struct cvmx_smix_en_s cn61xx;
+	struct cvmx_smix_en_s cn63xx;
+	struct cvmx_smix_en_s cn63xxp1;
+	struct cvmx_smix_en_s cn66xx;
+	struct cvmx_smix_en_s cn68xx;
+	struct cvmx_smix_en_s cn68xxp1;
+	struct cvmx_smix_en_s cnf71xx;
+};
+
+union cvmx_smix_rd_dat {
+	uint64_t u64;
+	struct cvmx_smix_rd_dat_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+		uint64_t reserved_18_63:46;
+		uint64_t pending:1;
+		uint64_t val:1;
+		uint64_t dat:16;
+#else
+		uint64_t dat:16;
+		uint64_t val:1;
+		uint64_t pending:1;
+		uint64_t reserved_18_63:46;
+#endif
+	} s;
+	struct cvmx_smix_rd_dat_s cn30xx;
+	struct cvmx_smix_rd_dat_s cn31xx;
+	struct cvmx_smix_rd_dat_s cn38xx;
+	struct cvmx_smix_rd_dat_s cn38xxp2;
+	struct cvmx_smix_rd_dat_s cn50xx;
+	struct cvmx_smix_rd_dat_s cn52xx;
+	struct cvmx_smix_rd_dat_s cn52xxp1;
+	struct cvmx_smix_rd_dat_s cn56xx;
+	struct cvmx_smix_rd_dat_s cn56xxp1;
+	struct cvmx_smix_rd_dat_s cn58xx;
+	struct cvmx_smix_rd_dat_s cn58xxp1;
+	struct cvmx_smix_rd_dat_s cn61xx;
+	struct cvmx_smix_rd_dat_s cn63xx;
+	struct cvmx_smix_rd_dat_s cn63xxp1;
+	struct cvmx_smix_rd_dat_s cn66xx;
+	struct cvmx_smix_rd_dat_s cn68xx;
+	struct cvmx_smix_rd_dat_s cn68xxp1;
+	struct cvmx_smix_rd_dat_s cnf71xx;
+};
+
+union cvmx_smix_wr_dat {
+	uint64_t u64;
+	struct cvmx_smix_wr_dat_s {
+#ifdef __BIG_ENDIAN_BITFIELD
+		uint64_t reserved_18_63:46;
+		uint64_t pending:1;
+		uint64_t val:1;
+		uint64_t dat:16;
+#else
+		uint64_t dat:16;
+		uint64_t val:1;
+		uint64_t pending:1;
+		uint64_t reserved_18_63:46;
+#endif
+	} s;
+	struct cvmx_smix_wr_dat_s cn30xx;
+	struct cvmx_smix_wr_dat_s cn31xx;
+	struct cvmx_smix_wr_dat_s cn38xx;
+	struct cvmx_smix_wr_dat_s cn38xxp2;
+	struct cvmx_smix_wr_dat_s cn50xx;
+	struct cvmx_smix_wr_dat_s cn52xx;
+	struct cvmx_smix_wr_dat_s cn52xxp1;
+	struct cvmx_smix_wr_dat_s cn56xx;
+	struct cvmx_smix_wr_dat_s cn56xxp1;
+	struct cvmx_smix_wr_dat_s cn58xx;
+	struct cvmx_smix_wr_dat_s cn58xxp1;
+	struct cvmx_smix_wr_dat_s cn61xx;
+	struct cvmx_smix_wr_dat_s cn63xx;
+	struct cvmx_smix_wr_dat_s cn63xxp1;
+	struct cvmx_smix_wr_dat_s cn66xx;
+	struct cvmx_smix_wr_dat_s cn68xx;
+	struct cvmx_smix_wr_dat_s cn68xxp1;
+	struct cvmx_smix_wr_dat_s cnf71xx;
+};
+
+#endif
diff --git a/drivers/net/phy/mdio-octeon.c b/drivers/net/phy/mdio-octeon.c
index a51ed92..54628e6 100644
--- a/drivers/net/phy/mdio-octeon.c
+++ b/drivers/net/phy/mdio-octeon.c
@@ -7,6 +7,8 @@
  */
 
 #include <linux/platform_device.h>
+#include <linux/of_address.h>
+#include <linux/of.h>
 #include <linux/of_mdio.h>
 #include <linux/delay.h>
 #include <linux/module.h>
@@ -14,8 +16,10 @@
 #include <linux/phy.h>
 #include <linux/io.h>
 
+#ifndef CONFIG_ARCH_THUNDER
 #include <asm/octeon/octeon.h>
-#include <asm/octeon/cvmx-smix-defs.h>
+#endif
+#include "cvmx-smix-defs.h"
 
 #define DRV_VERSION "1.0"
 #define DRV_DESCRIPTION "Cavium Networks Octeon SMI/MDIO driver"
@@ -41,6 +45,16 @@ struct octeon_mdiobus {
 	int phy_irq[PHY_MAX_ADDR];
 };
 
+static void cvmx_write_csr(uint64_t addr, uint64_t val)
+{
+	writeq_relaxed(val, (void *)addr);
+}
+
+static uint64_t cvmx_read_csr(uint64_t addr)
+{
+	return readq_relaxed((void *)addr);
+}
+
 static void octeon_mdiobus_set_mode(struct octeon_mdiobus *p,
 				    enum octeon_mdiobus_mode m)
 {
@@ -178,14 +192,29 @@ static int octeon_mdiobus_write(struct mii_bus *bus, int phy_id,
 static int octeon_mdiobus_probe(struct platform_device *pdev)
 {
 	struct octeon_mdiobus *bus;
+#ifdef CONFIG_ARCH_THUNDER
+	const __be32 *reg;
+	uint64_t  addr, size;
+#else
 	struct resource *res_mem;
+#endif
 	union cvmx_smix_en smi_en;
 	int err = -ENOENT;
 
+
 	bus = devm_kzalloc(&pdev->dev, sizeof(*bus), GFP_KERNEL);
 	if (!bus)
 		return -ENOMEM;
 
+#ifdef CONFIG_ARCH_THUNDER
+	reg = of_get_property(pdev->dev.of_node, "reg", NULL);
+	addr = of_translate_address(pdev->dev.of_node, reg);
+	pr_err("%s: mdio addr 0x%llx\n", __func__, addr);
+	size = of_read_number(reg + 2, 2);
+	pr_err("%s: size 0x%llx\n", __func__, size);
+
+	bus->register_base = (u64) devm_ioremap(&pdev->dev, addr, size);
+#else
 	res_mem = platform_get_resource(pdev, IORESOURCE_MEM, 0);
 
 	if (res_mem == NULL) {
@@ -202,7 +231,7 @@ static int octeon_mdiobus_probe(struct platform_device *pdev)
 	}
 	bus->register_base =
 		(u64)devm_ioremap(&pdev->dev, bus->mdio_phys, bus->regsize);
-
+#endif
 	bus->mii_bus = mdiobus_alloc();
 
 	if (!bus->mii_bus)
-- 
1.7.5.4

