From be1d59cdc89213fc796341091b06e7688fd9d744 Mon Sep 17 00:00:00 2001
From: Donn Seeley <donn.seeley@windriver.com>
Date: Tue, 15 Jul 2008 15:26:03 -0400
Subject: [PATCH 4/4] arm: Arm VFP Context

This patch implements context saving for the ARM VFP hardware floating
point unit in two specific circumstances:

  +     before a fork(), so that the child inherits the parent's floating
        point state; and

  +     across a caught signal, so that the use of floating point in a
        signal handler won't destroy caller-saved floating point state
        in the the interrupted context.

Few real programs actually need this functionality, but when they do
need it, the bugs can lead to data that's corrupted in a way that is
subtle and hard to identify.

Also note: the condition field "NE" in vfp instruction is not
supported. So add a instruction "beq" to replace it.

Besides, when FPEXC.EX is set to zero, the FPINST and FPINST2
registers are not implemented. So FPINST doesn't need to be restored when
FPEXC.EX is not set.

Signed-off-by: Donn Seeley <donn.seeley@windriver.com>
Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
Signed-off-by: Stanley.Miao <stanley.miao@windriver.com>
Signed-off-by: Bruce Ashfield <bruce.ashfield@windriver.com>
---
 arch/arm/include/asm/processor.h |    8 +++-
 arch/arm/include/asm/ucontext.h  |    2 +-
 arch/arm/kernel/process.c        |   16 ++++++
 arch/arm/kernel/signal.c         |   39 ++++++++++++++--
 arch/arm/vfp/vfp.h               |    1 +
 arch/arm/vfp/vfphw.S             |   20 ++++++++
 arch/arm/vfp/vfpmodule.c         |   96 ++++++++++++++++++++++++++++++++++++++
 7 files changed, 176 insertions(+), 6 deletions(-)

diff --git a/arch/arm/include/asm/processor.h b/arch/arm/include/asm/processor.h
index 6a89567..4b8d2af 100644
--- a/arch/arm/include/asm/processor.h
+++ b/arch/arm/include/asm/processor.h
@@ -87,7 +87,13 @@ struct task_struct;
 extern void release_thread(struct task_struct *);
 
 /* Prepare to copy thread state - unlazy all lazy status */
-#define prepare_to_copy(tsk)	do { } while (0)
+extern void prepare_to_copy(struct task_struct *);
+#ifdef CONFIG_VFP
+extern void vfp_task_disable(struct task_struct *tsk);
+struct thread_info;
+extern void vfp_task_copy(struct thread_info *, void *);
+extern void vfp_task_restore(struct thread_info *, void *);
+#endif
 
 unsigned long get_wchan(struct task_struct *p);
 
diff --git a/arch/arm/include/asm/ucontext.h b/arch/arm/include/asm/ucontext.h
index bf65e9f..b22b817 100644
--- a/arch/arm/include/asm/ucontext.h
+++ b/arch/arm/include/asm/ucontext.h
@@ -91,7 +91,7 @@ struct aux_sigframe {
 #ifdef CONFIG_IWMMXT
 	struct iwmmxt_sigframe	iwmmxt;
 #endif
-#if 0 && defined CONFIG_VFP /* Not yet saved.  */
+#ifdef CONFIG_VFP
 	struct vfp_sigframe	vfp;
 #endif
 	/* Something that isn't a valid magic number for any coprocessor.  */
diff --git a/arch/arm/kernel/process.c b/arch/arm/kernel/process.c
index 0e12e0a..dba757a 100644
--- a/arch/arm/kernel/process.c
+++ b/arch/arm/kernel/process.c
@@ -304,6 +304,22 @@ void release_thread(struct task_struct *dead_task)
 
 asmlinkage void ret_from_fork(void) __asm__("ret_from_fork");
 
+void prepare_to_copy(struct task_struct *tsk)
+{
+#ifdef CONFIG_CRUNCH
+	if (elf_hwcap & HWCAP_CRUNCH)
+		crunch_task_disable(tsk);
+#endif
+#ifdef CONFIG_IWMMXT
+	if (test_ti_thread_flag(task_thread_info(tsk), TIF_USING_IWMMXT))
+		iwmmxt_task_disable(task_thread_info(tsk));
+#endif
+#ifdef CONFIG_VFP
+	if (elf_hwcap & HWCAP_VFP)
+		vfp_task_disable(tsk);
+#endif
+}
+
 int
 copy_thread(unsigned long clone_flags, unsigned long stack_start,
 	    unsigned long stk_sz, struct task_struct *p, struct pt_regs *regs)
diff --git a/arch/arm/kernel/signal.c b/arch/arm/kernel/signal.c
index e7714f3..a45ae9e 100644
--- a/arch/arm/kernel/signal.c
+++ b/arch/arm/kernel/signal.c
@@ -175,6 +175,37 @@ static int restore_iwmmxt_context(struct iwmmxt_sigframe *frame)
 
 #endif
 
+#ifdef CONFIG_VFP
+
+static int preserve_vfp_context(struct vfp_sigframe *frame)
+{
+	char kbuf[sizeof(*frame) + 8];
+	struct vfp_sigframe *kframe;
+
+	kframe = (struct vfp_sigframe *)((unsigned long)(kbuf + 8) & ~7);
+	kframe->magic = VFP_MAGIC;
+	kframe->size = VFP_STORAGE_SIZE;
+	vfp_task_copy(current_thread_info(), &kframe->storage);
+	return __copy_to_user(frame, kframe, sizeof(*frame));
+}
+
+static int restore_vfp_context(struct vfp_sigframe *frame)
+{
+	char kbuf[sizeof(*frame) + 8];
+	struct vfp_sigframe *kframe;
+
+	kframe = (struct vfp_sigframe *)((unsigned long)(kbuf + 8) & ~7);
+	if (__copy_from_user(kframe, frame, sizeof(*frame)))
+		return -1;
+	if (kframe->magic != VFP_MAGIC ||
+	    kframe->size != VFP_STORAGE_SIZE)
+		return -1;
+	vfp_task_restore(current_thread_info(), &kframe->storage);
+	return 0;
+}
+
+#endif
+
 /*
  * Do a signal return; undo the signal stack.  These are aligned to 64-bit.
  */
@@ -233,8 +264,8 @@ static int restore_sigframe(struct pt_regs *regs, struct sigframe __user *sf)
 		err |= restore_iwmmxt_context(&aux->iwmmxt);
 #endif
 #ifdef CONFIG_VFP
-//	if (err == 0)
-//		err |= vfp_restore_state(&sf->aux.vfp);
+	if (err == 0 && (elf_hwcap & HWCAP_VFP) != 0)
+		err |= restore_vfp_context(&aux->vfp);
 #endif
 
 	return err;
@@ -348,8 +379,8 @@ setup_sigframe(struct sigframe __user *sf, struct pt_regs *regs, sigset_t *set)
 		err |= preserve_iwmmxt_context(&aux->iwmmxt);
 #endif
 #ifdef CONFIG_VFP
-//	if (err == 0)
-//		err |= vfp_save_state(&sf->aux.vfp);
+	if (err == 0 && (elf_hwcap & HWCAP_VFP) != 0)
+		err |= preserve_vfp_context(&aux->vfp);
 #endif
 	__put_user_error(0, &aux->end_magic, err);
 
diff --git a/arch/arm/vfp/vfp.h b/arch/arm/vfp/vfp.h
index c8c98dd..20ce519 100644
--- a/arch/arm/vfp/vfp.h
+++ b/arch/arm/vfp/vfp.h
@@ -378,3 +378,4 @@ struct op {
 };
 
 extern void vfp_save_state(void *location, u32 fpexc);
+extern void vfp_restore_state(void *location);
diff --git a/arch/arm/vfp/vfphw.S b/arch/arm/vfp/vfphw.S
index 66dc2d0..b33ccf7 100644
--- a/arch/arm/vfp/vfphw.S
+++ b/arch/arm/vfp/vfphw.S
@@ -206,6 +206,26 @@ ENTRY(vfp_save_state)
 	mov	pc, lr
 ENDPROC(vfp_save_state)
 
+ENTRY(vfp_restore_state)
+	@ Inverse of vfp_save_state() -- restore VFP register state
+	@ r0 - address of saved register state
+	DBGSTR1 "restore VFP state %p", r0
+	VFPFLDMIA r0, r2                @ restore the working registers
+	ldmia	r0, {r1, r2, r3, r12}	@ load FPEXC, FPSCR, FPINST, FPINST2
+	tst	r1, #FPEXC_EX		@ is there additional state to write?
+	beq	1f
+	VFPFMXR FPINST, r3
+	tst	r1, #FPEXC_FP2V		@ is there an FPINST2 to write?
+	beq	1f
+	VFPFMXR FPINST2, r12	 	@ FPINST2 if needed - avoids writing
+					@ nonexistant reg on rev0
+1:
+	orr	r1, r1, #FPEXC_EN
+	VFPFMXR FPSCR, r2
+	VFPFMXR FPEXC, r1
+	mov	pc, lr
+ENDPROC(vfp_restore_state)
+
 last_VFP_context_address:
 	.word	last_VFP_context
 
diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index a420cb9..5143c4f 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -494,6 +494,102 @@ void vfp_flush_hwstate(struct thread_info *thread)
 }
 #endif
 
+/*
+ * Make sure that this task no longer owns the VFP unit
+ * and that its VFP state is saved.
+ */
+void vfp_task_disable(struct task_struct *tsk)
+{
+	struct thread_info *ti = task_thread_info(tsk);
+	u32 fpexc;
+	__u32 cpu;
+
+	preempt_disable();
+	cpu = ti->cpu;
+	fpexc = fmrx(FPEXC);
+	if (last_VFP_context[cpu] == &ti->vfpstate) {
+#ifdef CONFIG_SMP
+		/*
+		 * On SMP systems, if the VFP unit is disabled,
+		 * then our saved state is good.
+		 */
+		if ((fpexc & FPEXC_EN) != 0)
+			vfp_save_state(last_VFP_context[cpu], fpexc);
+#else
+		if ((fpexc & FPEXC_EN) == 0)
+			fmxr(FPEXC, fpexc | FPEXC_EN);
+		vfp_save_state(last_VFP_context[cpu], fpexc);
+#endif
+		last_VFP_context[cpu] = NULL;
+	}
+	fmxr(FPEXC, fpexc & ~FPEXC_EN);
+	preempt_enable();
+}
+
+/*
+ * Take a snapshot of our current VFP state.
+ */
+void vfp_task_copy(struct thread_info *ti, void *storage)
+{
+	u32 fpexc;
+	union vfp_state *vfp = storage;
+
+	preempt_disable();
+	fpexc = fmrx(FPEXC);
+
+#ifdef CONFIG_SMP
+	if ((fpexc & FPEXC_EN) != 0 &&
+	    last_VFP_context[ti->cpu] == &ti->vfpstate) {
+		vfp_save_state(vfp, fpexc);
+		preempt_enable();
+		return;
+	}
+#else
+	if (last_VFP_context[ti->cpu] == &ti->vfpstate) {
+		if ((fpexc & FPEXC_EN) == 0)
+			fmxr(FPEXC, fpexc | FPEXC_EN);
+		vfp_save_state(vfp, fpexc);
+		preempt_enable();
+		return;
+	}
+#endif
+	preempt_enable();
+	memcpy(vfp, &ti->vfpstate, sizeof *vfp);
+}
+
+/*
+ * Restore our VFP state from saved state.  If we own the VFP
+ * unit, we leave it enabled, with valid register contents;
+ * otherwise we just update the values in thread storage.
+ */
+void vfp_task_restore(struct thread_info *ti, void *storage)
+{
+	u32 fpexc;
+	union vfp_state *vfp = storage;
+
+	preempt_disable();
+	fpexc = fmrx(FPEXC);
+
+#ifdef CONFIG_SMP
+	if ((fpexc & FPEXC_EN) != 0 &&
+	    last_VFP_context[ti->cpu] == &ti->vfpstate) {
+		vfp_restore_state(vfp);
+		preempt_enable();
+		return;
+	}
+#else
+	if (last_VFP_context[ti->cpu] == &ti->vfpstate) {
+		if ((fpexc & FPEXC_EN) == 0)
+			fmxr(FPEXC, fpexc | FPEXC_EN);
+		vfp_restore_state(vfp);
+		preempt_enable();
+		return;
+	}
+#endif
+	preempt_enable();
+	memcpy(&ti->vfpstate, vfp, sizeof *vfp);
+}
+
 #include <linux/smp.h>
 
 /*
-- 
1.6.5.2

