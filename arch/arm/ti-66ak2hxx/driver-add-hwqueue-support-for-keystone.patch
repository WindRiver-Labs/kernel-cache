From 49aa20b526e4688a37da2e9a09c9e8b9d91da61d Mon Sep 17 00:00:00 2001
From: Guoqing Jiang <Guoqing.Jiang@windriver.com>
Date: Thu, 18 Jul 2013 14:31:45 +0800
Subject: [PATCH 56/57] driver: add hwqueue support for keystone

Add hwqueue core driver and keystone specific hwqueue driver.

Refer to the following commits from arago tree
git://arago-project.org/git/projects/linux-keystone.git releases/03.00.00.11/master

d7f3f0b hwqueue: keystone: multi qos tree support
0e3e564 hwqueue: keystone: add multi tree support
3499922 hwqueue: keystone: write pdsp number to scratch ram
8eb5d40 hwqueue: keystone: remove pushing to reg C
97f9e6d hwqueue: keystone: fix queue id calculation
04ad96c hwqueue: keystone: qos: update parameter calculation
65f4150 hwqueue: keystone: acc: use of infrastructure for interrupts
6ad5bcf hwqueue: keystone: use of infrastructure for interrupts
8225962 hwqueue: keystone: qos: use of_get_child_by_name()
3f7eaa7 hwqueue: keystone: use of_get_child_by_name()
d98c350 hwqueue: modify test code to match API change
8f2a954 hwqueue: keystone: use qos specific push operation
fd35267 hwqueue: keystone: use hwqueue instance operations
d1b70be hwqueue: separate operations into instance specific operations
9efb30b hwqueue: keystone: qos debugfs support
187b3f1 hwqueue: keystone: sysfs imlpementation for qos tree
b5a2245 hwqueue: keystone: check for return values
2430fbe hwqueue: keystone: qos: lock around shadow writes at init
57d7a5e hwqueue: keystone: free qos resources
c4e3107 hwqueue: keystone: update normalization routines for weights
00ef196 hwqueue: keystone: qos: add quality of service driver
8cd6182 hwqueue: keystone: use extended hwqueue push and pop api's
7c3a666 hwqueue: extend push and pop api's to take an additional argument
41745e8 hwqueue: Region should necessarily contain link-index information.
8ee9e4c hwqueue: keystone: add accumulator support
9bbe6ed hwqueue: keystone: add support for pdsps
c73bfeb hwqueue: keystone: add keystone hwqueue driver
e7af039 hwqueue: add hwqueue subsystem test cases
b878dd9 hwqueue: add hwqueue core infrastructure

Signed-off-by: Guoqing Jiang <Guoqing.Jiang@windriver.com>
---
 .../devicetree/bindings/hwqueue/keystone-qos.txt   |  186 ++
 drivers/Kconfig                                    |    2 +
 drivers/Makefile                                   |    1 +
 drivers/hwqueue/Kconfig                            |   29 +
 drivers/hwqueue/Makefile                           |    9 +
 drivers/hwqueue/hwqueue_core.c                     |  745 +++++
 drivers/hwqueue/hwqueue_internal.h                 |  125 +
 drivers/hwqueue/hwqueue_test.c                     |  267 ++
 drivers/hwqueue/keystone_hwqueue.c                 | 1314 ++++++++
 drivers/hwqueue/keystone_hwqueue.h                 |  309 ++
 drivers/hwqueue/keystone_hwqueue_acc.c             |  579 ++++
 drivers/hwqueue/keystone_hwqueue_qos.c             | 3517 ++++++++++++++++++++
 drivers/hwqueue/keystone_qos.h                     |  393 +++
 include/linux/hwqueue.h                            |  223 ++
 14 files changed, 7699 insertions(+)
 create mode 100644 Documentation/devicetree/bindings/hwqueue/keystone-qos.txt
 create mode 100644 drivers/hwqueue/Kconfig
 create mode 100644 drivers/hwqueue/Makefile
 create mode 100644 drivers/hwqueue/hwqueue_core.c
 create mode 100644 drivers/hwqueue/hwqueue_internal.h
 create mode 100644 drivers/hwqueue/hwqueue_test.c
 create mode 100644 drivers/hwqueue/keystone_hwqueue.c
 create mode 100644 drivers/hwqueue/keystone_hwqueue.h
 create mode 100644 drivers/hwqueue/keystone_hwqueue_acc.c
 create mode 100644 drivers/hwqueue/keystone_hwqueue_qos.c
 create mode 100644 drivers/hwqueue/keystone_qos.h
 create mode 100644 include/linux/hwqueue.h

diff --git a/Documentation/devicetree/bindings/hwqueue/keystone-qos.txt b/Documentation/devicetree/bindings/hwqueue/keystone-qos.txt
new file mode 100644
index 0000000..61d25b48
--- /dev/null
+++ b/Documentation/devicetree/bindings/hwqueue/keystone-qos.txt
@@ -0,0 +1,186 @@
+QoS Tree Configuration
+----------------------
+
+The QoS implementation allows for an abstracted tree of scheduler nodes
+represented in device tree form.  For example:
+
+  +-----------+
+  |  inputs   |  . . .
+  +-----------+              +-----------+        +-----------+
+        |       |      |     |  inputs   |        |  inputs   |
+        +-------+------+     +-----------+        +-----------+
+                |                  |                    |
+        +--------------+   +---------------+    +--------------+
+        |     prio 0   |   |    prio 1     |    |    prio 2    |
+        |   unordered  |   |    inputs     |    |    inputs    |
+        +--------------+   +---------------+    +--------------+
+                |                 |                    |
+                +-----------------+--------------------+
+                                  |
+                        +--------------------+
+                        |  strict prio node  |
+                        +--------------------+
+                                  |
+			output to network transmit
+
+At each node, shaping and dropping parameters may be specified, within limits
+of the constraints outlined in this document.  The following sections detail
+the device tree attributes applicable for this implementation.
+
+QoS Node Attributes
+-------------------
+
+The following attributes are recognized within QoS configuration nodes:
+
+  - "strict-priority" and "weighted-round-robin"
+
+    e.g. strict-priority;
+
+    This attribute specifies the type of scheduling performed at a node.  It
+    is an error to specify both of these attributes in a particular node.  The
+    absence of both of these attributes defaults the node type to unordered
+    (first come first serve).
+
+  - "weight"
+
+    e.g. weight = <50>;
+
+    This attribute specifies the weight attached to the child node of a
+    weighted-round-robin node.  It is an error to specify this attribute on a
+    node whose parent is not a weighted-round-robin node.
+
+  - "priority"
+
+    e.g. priority = <1>;
+
+    This attribute specifies the priority attached to the child node of a
+    strict-priority node.  It is an error to specify this attribute on a
+    node whose parent is not a strict-priority node.  It is also an error for
+    child nodes of a strict-priority node to have the same priority specified.
+
+  - "byte-units" or "packet-units"
+
+    e.g. byte-units;
+
+    The presence of this attribute indicates that the scheduler accounts for
+    traffic in byte or packet units.  If this attribute is not specified for a
+    given node, the accounting mode is inherited from its parent node.  If
+    this attribute is not specified for the root node, the accounting mode
+    defaults to byte units.
+
+  - "output-rate"
+
+    e.g. output-rate = <100000 1000>;
+
+    The first element of this attribute specifies the output shaped rate in
+    bytes/second or packets/second (depending on the accounting mode for the
+    node).  If this attribute is absent, it defaults to infinity (i.e., no
+    shaping).
+
+    The second element of this attribute specifies the maximum accumulated
+    credits in bytes or packets (depending on the accounting mode for the
+    node).  If this attribute is absent, it defaults to infinity (i.e.,
+    accumulate as many credits as possible).
+
+  - "overhead-bytes"
+
+    e.g. overhead-bytes = <24>;
+
+    This attribute specifies a per-packet overhead (in bytes) applied in the byte
+    accounting mode.  This can be used to account for framing overhead on the wire.
+    This attribute is inherited from parent nodes if absent.  If not defined for the
+    root node, a default value of 24 will be used.  This attribute is passed
+    through by inheritence (but ignored) on packet accounted nodes.
+
+  - "output-queue"
+
+    e.g. output-queue = <645>;
+
+    This specifies the QMSS queue on which output packets are pushed.  This
+    attribute must be defined only for the root node in the qos tree.  Child
+    nodes in the tree will ignore this attribute if specified.
+
+  - "input-queues"
+
+    e.g. input-queues = <0 4 8 12 56>;
+
+    This specifies a set of ingress queues that feed into a QoS node. This attribute
+    must be defined only for leaf nodes in the QoS tree.  Specifying input queues 
+    on non-leaf nodes is treated as an error.  The absence of input queues on
+    a leaf node is also treated as an error.
+
+  - "stats-class"
+
+    e.g. stats-class = "linux-best-effort";
+
+    The stats-class attribute ties one or more input stage nodes to a set of
+    traffic statistics (forwarded/discarded bytes, etc.).  The system has a
+    limited set of statistics blocks (up to 48), and an attempt to exceed this
+    count is an error.  This attribute is legal only for leaf nodes, and a
+    stats-class attribute on an intermediate node will be treated as an error.
+
+  - "drop-policy"
+
+    The drop-policy attribute specifies a drop policy to apply to a QoS
+    node (tail drop, random early drop, no drop, etc.) when the traffic
+    pattern exceeds specifies parameters.  The drop-policy parameters are
+    configured separately within device tree (see "Traffic Police Policy
+    Attributes section below).  This attribute defaults to "no drop" for
+    applicable input stage nodes.
+
+    If a node in the QoS tree specifies a drop-policy, it is an error if
+    any of its descendent nodes (children, children of children, ...) are of
+    weighted-round-robin or strict-priority types.
+
+
+1.5 Traffic Police Policy Attributes
+------------------------------------
+
+The following attributes are recognized within traffic drop policy nodes:
+
+  - "byte-units" or "packet-units"
+
+    e.g. byte-units;
+
+    The presence of this attribute indicates that the dropr accounts for
+    traffic in byte or packet units.  If this attribute is not specified, it
+    defaults to byte units.  Policies that use random early drop must be of
+    byte unit type.
+
+  - "limit"
+
+    e.g. limit = <10000>;
+
+    Instantaneous queue depth limit (in bytes or packets) at which tail drop
+    takes effect.  This may be specified in combination with random early
+    drop, which operates on average queue depth (instead of instantaneous).
+    The absence of this attribute, or a zero value for this attribute disables
+    tail drop behavior.
+
+  - "random-early-drop"
+
+    e.g. random-early-drop = <1000 50000 2000>;
+
+    The random-early-drop attribute specifies the following three parameters
+    in order:
+
+      - low threshold:	No packets are dropped when the average queue depth is
+			below this threshold (in bytes). This parameter must
+			be specified.
+			
+      - high threshold:	All packets are dropped when the average queue depth
+			above this threshold (in bytes). This parameter is
+			optional, and defaults to twice the low threshold.
+
+      - half-life:	Specified in milli seconds. This is used to calculate
+			the average queue depth. This parameter
+			is optional and defaults to 2000.
+
+
+Internally Calculated
+---------------------
+ - throttle rate - might want to provide device tree override as well
+ - wrr/sp/be queue counts
+ - egress queue number (except final output)
+ - RNG seeds
+
diff --git a/drivers/Kconfig b/drivers/Kconfig
index 9953a42..c57b60b 100644
--- a/drivers/Kconfig
+++ b/drivers/Kconfig
@@ -136,6 +136,8 @@ source "drivers/clk/Kconfig"
 
 source "drivers/hwspinlock/Kconfig"
 
+source "drivers/hwqueue/Kconfig"
+
 source "drivers/clocksource/Kconfig"
 
 source "drivers/mailbox/Kconfig"
diff --git a/drivers/Makefile b/drivers/Makefile
index 130abc1..0dd8827 100644
--- a/drivers/Makefile
+++ b/drivers/Makefile
@@ -136,6 +136,7 @@ obj-y				+= clk/
 
 obj-$(CONFIG_MAILBOX)		+= mailbox/
 obj-$(CONFIG_HWSPINLOCK)	+= hwspinlock/
+obj-$(CONFIG_HWQUEUE)		+= hwqueue/
 obj-$(CONFIG_NFC)		+= nfc/
 obj-$(CONFIG_IOMMU_SUPPORT)	+= iommu/
 obj-$(CONFIG_REMOTEPROC)	+= remoteproc/
diff --git a/drivers/hwqueue/Kconfig b/drivers/hwqueue/Kconfig
new file mode 100644
index 0000000..83640ee
--- /dev/null
+++ b/drivers/hwqueue/Kconfig
@@ -0,0 +1,29 @@
+#
+# Generic HWQUEUE framework
+#
+
+# HWQUEUE always gets selected by whoever wants it.
+config HWQUEUE
+	tristate
+
+menu "Hardware Queue drivers"
+
+config HWQUEUE_TEST
+	tristate "Hardware Queue test driver"
+	depends on HWQUEUE
+	help
+	  Say y here to enable test code for the hardware queue subsystem
+
+	  If unsure, say N.
+
+config HWQUEUE_KEYSTONE
+	tristate "Keystone Hardware Queue device"
+	depends on TI_KEYSTONE
+	default y if TI_KEYSTONE
+	select HWQUEUE
+	help
+	  Say y here to support the Keystone Hardware queue device
+
+	  If unsure, say N.
+
+endmenu
diff --git a/drivers/hwqueue/Makefile b/drivers/hwqueue/Makefile
new file mode 100644
index 0000000..3d7b761
--- /dev/null
+++ b/drivers/hwqueue/Makefile
@@ -0,0 +1,9 @@
+#
+# Generic Hardware Queue framework
+#
+
+obj-$(CONFIG_HWQUEUE)		+= hwqueue_core.o
+obj-$(CONFIG_HWQUEUE_TEST)	+= hwqueue_test.o
+obj-$(CONFIG_HWQUEUE_KEYSTONE)	+= keystone_hwqueue.o		\
+				   keystone_hwqueue_acc.o	\
+				   keystone_hwqueue_qos.o
diff --git a/drivers/hwqueue/hwqueue_core.c b/drivers/hwqueue/hwqueue_core.c
new file mode 100644
index 0000000..c6d3d54
--- /dev/null
+++ b/drivers/hwqueue/hwqueue_core.c
@@ -0,0 +1,745 @@
+/*
+ * Hardware queue framework
+ *
+ * Copyright (C) 2011 Texas Instruments Incorporated - http://www.ti.com
+ *
+ * Contact: Prabhu Kuttiyam <pkuttiyam@ti.com>
+ *	    Cyril Chemparathy <cyril@ti.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/kernel.h>
+#include <linux/list.h>
+#include <linux/module.h>
+#include <linux/types.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/mutex.h>
+#include <linux/debugfs.h>
+#include <linux/seq_file.h>
+
+#include "hwqueue_internal.h"
+
+static LIST_HEAD(hwqueue_devices);
+static DEFINE_MUTEX(hwqueue_devices_lock);
+
+#define for_each_handle_rcu(qh, inst)			\
+	list_for_each_entry_rcu(qh, &inst->handles, list)
+
+#define for_each_device(hdev)				\
+	list_for_each_entry(hdev, &hwqueue_devices, list)
+
+#define for_each_instance(id, inst, hdev)		\
+	for (id = 0, inst = hdev->instances;		\
+	     id < (hdev)->num_queues;			\
+	     id++, inst = hwqueue_id_to_inst(hdev, id))
+
+static void __hwqueue_poll(unsigned long data);
+
+static int hwqueue_poll_interval = 100;
+
+static inline bool hwqueue_is_busy(struct hwqueue_instance *inst)
+{
+	return !list_empty(&inst->handles);
+}
+
+static inline bool hwqueue_is_exclusive(struct hwqueue_instance *inst)
+{
+	struct hwqueue *tmp;
+
+	rcu_read_lock();
+
+	for_each_handle_rcu(tmp, inst) {
+		if (tmp->flags & O_EXCL) {
+			rcu_read_unlock();
+			return true;
+		}
+	}
+
+	rcu_read_unlock();
+
+	return false;
+}
+
+static inline bool hwqueue_is_writable(struct hwqueue *qh)
+{
+	unsigned acc = qh->flags & O_ACCMODE;
+	return (acc == O_RDWR || acc == O_WRONLY);
+}
+
+static inline bool hwqueue_is_readable(struct hwqueue *qh)
+{
+	unsigned acc = qh->flags & O_ACCMODE;
+	return (acc == O_RDWR || acc == O_RDONLY);
+}
+
+static inline struct hwqueue_instance *hwqueue_find_by_id(int id)
+{
+	struct hwqueue_device *hdev;
+
+	for_each_device(hdev) {
+		if (hdev->base_id <= id &&
+		    hdev->base_id + hdev->num_queues > id) {
+			id -= hdev->base_id;
+			return hwqueue_id_to_inst(hdev, id);
+		}
+	}
+	return NULL;
+}
+
+int hwqueue_device_register(struct hwqueue_device *hdev)
+{
+	struct hwqueue_instance *inst;
+	int id, size, ret = -EEXIST;
+	struct hwqueue_device *b;
+
+	if (!hdev->ops || !hdev->dev)
+		return -EINVAL;
+
+	mutex_lock(&hwqueue_devices_lock);
+
+	for_each_device(b) {
+		if (b->base_id + b->num_queues >= hdev->base_id &&
+		    hdev->base_id + hdev->num_queues >= b->base_id) {
+			dev_err(hdev->dev, "id collision with %s\n",
+				dev_name(b->dev));
+			goto unlock_ret;
+		}
+	}
+	ret = -ENOMEM;
+
+	/* how much do we need for instance data? */
+	size  = sizeof(struct hwqueue_instance) + hdev->priv_size;
+
+	/* round this up to a power of 2, keep the push/pop arithmetic fast */
+	hdev->inst_shift = order_base_2(size);
+	size = (1 << hdev->inst_shift) * hdev->num_queues;
+
+	hdev->instances = kzalloc(size, GFP_KERNEL);
+	if (!hdev->instances)
+		goto unlock_ret;
+
+	ret = 0;
+	for_each_instance(id, inst, hdev) {
+		inst->hdev = hdev;
+		INIT_LIST_HEAD(&inst->handles);
+		setup_timer(&inst->poll_timer, __hwqueue_poll,
+			    (unsigned long)inst);
+		init_waitqueue_head(&inst->wait);
+	}
+
+	list_add(&hdev->list, &hwqueue_devices);
+
+	dev_info(hdev->dev, "registered queues %d-%d\n",
+		 hdev->base_id, hdev->base_id + hdev->num_queues - 1);
+
+unlock_ret:
+	mutex_unlock(&hwqueue_devices_lock);
+	return ret;
+}
+EXPORT_SYMBOL(hwqueue_device_register);
+
+int hwqueue_device_unregister(struct hwqueue_device *hdev)
+{
+	struct hwqueue_instance *inst;
+	int id, ret = -EBUSY;
+
+	mutex_lock(&hwqueue_devices_lock);
+
+	for_each_instance(id, inst, hdev) {
+		if (hwqueue_is_busy(inst)) {
+			dev_err(hdev->dev, "cannot unregister busy dev\n");
+			goto unlock_ret;
+		}
+	}
+
+	list_del(&hdev->list);
+	dev_info(hdev->dev, "unregistered queues %d-%d\n",
+		 hdev->base_id, hdev->base_id + hdev->num_queues);
+	kfree(hdev->instances);
+	ret = 0;
+
+unlock_ret:
+	mutex_unlock(&hwqueue_devices_lock);
+	return ret;
+}
+EXPORT_SYMBOL(hwqueue_device_unregister);
+
+static struct hwqueue *__hwqueue_open(struct hwqueue_instance *inst,
+				      const char *name, unsigned flags,
+				      void *caller)
+{
+	struct hwqueue_device *hdev = inst->hdev;
+	struct hwqueue *qh;
+	int ret;
+
+	if (!try_module_get(hdev->dev->driver->owner))
+		return ERR_PTR(-ENODEV);
+
+	qh = kzalloc(sizeof(struct hwqueue), GFP_KERNEL);
+	if (!qh) {
+		module_put(hdev->dev->driver->owner);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	qh->flags = flags;
+	qh->inst = inst;
+
+	/* first opener? */
+	if (!hwqueue_is_busy(inst)) {
+		strncpy(inst->name, name, sizeof(inst->name));
+		ret = hdev->ops->open(inst, flags);
+		if (ret) {
+			kfree(qh);
+			module_put(hdev->dev->driver->owner);
+			return ERR_PTR(ret);
+		}
+	}
+
+	if (hwqueue_is_readable(qh)) {
+		qh->get_count	= inst->ops->get_count;
+		qh->pop		= inst->ops->pop;
+		qh->unmap	= inst->ops->unmap;
+	}
+
+	if (hwqueue_is_writable(qh)) {
+		qh->flush	= inst->ops->flush;
+		qh->push	= inst->ops->push;
+		qh->map		= inst->ops->map;
+	}
+
+	list_add_tail_rcu(&qh->list, &inst->handles);
+
+	return qh;
+}
+
+static struct hwqueue *hwqueue_open_by_id(const char *name, unsigned id,
+					  unsigned flags, void *caller)
+{
+	struct hwqueue_instance *inst;
+	struct hwqueue_device *hdev;
+	struct hwqueue *qh;
+	int match;
+
+	mutex_lock(&hwqueue_devices_lock);
+
+	qh = ERR_PTR(-ENODEV);
+	inst = hwqueue_find_by_id(id);
+	if (!inst)
+		goto unlock_ret;
+	hdev = inst->hdev;
+
+
+	qh = ERR_PTR(-EINVAL);
+	match = hdev->ops->match(inst, flags);
+
+	if (match < 0)
+		goto unlock_ret;
+
+	qh = ERR_PTR(-EBUSY);
+	if (hwqueue_is_exclusive(inst))
+		goto unlock_ret;
+
+	qh = ERR_PTR(-EEXIST);
+	if ((flags & O_CREAT) && hwqueue_is_busy(inst))
+		goto unlock_ret;
+
+	qh = __hwqueue_open(inst, name, flags, caller);
+
+unlock_ret:
+	mutex_unlock(&hwqueue_devices_lock);
+
+	return qh;
+}
+
+static struct hwqueue *hwqueue_open_any(const char *name, unsigned flags,
+					void *caller)
+{
+	struct hwqueue_device *hdev;
+	int match = INT_MAX, _match, id;
+	struct hwqueue_instance *inst = NULL, *_inst;
+	struct hwqueue *qh = ERR_PTR(-ENODEV);
+
+	mutex_lock(&hwqueue_devices_lock);
+
+	for_each_device(hdev) {
+		for_each_instance(id, _inst, hdev) {
+			_match = hdev->ops->match(_inst, flags);
+			if (_match < 0) /* match error */
+				continue;
+			if (_match >= match) /* match is no better */
+				continue;
+			if (hwqueue_is_exclusive(_inst))
+				continue;
+			if ((flags & O_CREAT) && hwqueue_is_busy(_inst))
+				continue;
+
+			match = _match;
+			inst = _inst;
+
+			if (!match) /* made for each other */
+				break;
+		}
+	}
+
+	if (inst)
+		qh = __hwqueue_open(inst, name, flags, caller);
+
+	mutex_unlock(&hwqueue_devices_lock);
+	return qh;
+}
+
+static struct hwqueue *hwqueue_open_by_name(const char *name, unsigned flags,
+					    void *caller)
+{
+	struct hwqueue_device *hdev;
+	struct hwqueue_instance *inst;
+	struct hwqueue *qh = ERR_PTR(-EINVAL);
+	int id;
+
+	mutex_lock(&hwqueue_devices_lock);
+
+	for_each_device(hdev) {
+		for_each_instance(id, inst, hdev) {
+			int match = hdev->ops->match(inst, flags);
+			if (match < 0)
+				continue;
+			if (!hwqueue_is_busy(inst))
+				continue;
+			if (hwqueue_is_exclusive(inst))
+				continue;
+			if (strcmp(inst->name, name))
+				continue;
+			qh = __hwqueue_open(inst, name, flags, caller);
+			goto unlock_ret;
+		}
+	}
+
+unlock_ret:
+	mutex_unlock(&hwqueue_devices_lock);
+	return qh;
+}
+
+/**
+ * hwqueue_open() - open a hardware queue
+ * @name	- name to give the queue handle
+ * @id		- desired queue number if any
+ *		  HWQUEUE_ANY: allocate any free queue, implies O_CREAT
+ *		  HWQUEUE_BYNAME: open existing queue by name, implies !O_CREAT
+ * @flags	- the following flags are applicable to queues:
+ *	O_EXCL		- insist on exclusive ownership - will fail if queue is
+ *			  already open.  Subsequent attempts to open the same
+ *			  queue will also fail. O_EXCL => O_CREAT here.
+ *	O_CREAT		- insist that queue not be already open - will fail if
+ *			  queue is already open. Subsequent attempts to open
+ *			  the same queue may succeed. O_CREAT is implied if
+ *			  queue id == HWQUEUE_ANY.
+ *	O_RDONLY	- pop only access
+ *	O_WRONLY	- push only access
+ *	O_RDWR		- push and pop access
+ *	O_NONBLOCK	- never block on pushes and pops
+ *   In addition, the following "hints" to the driver/hardware may be passed
+ *   in at open:
+ *	O_HIGHTHROUGHPUT - hint high throughput usage
+ *	O_LOWLATENCY	 - hint low latency usage
+ *
+ * Returns a handle to the open hardware queue if successful.  Use IS_ERR()
+ * to check the returned value for error codes.
+ */
+struct hwqueue *hwqueue_open(const char *name, unsigned id, unsigned flags)
+{
+	struct hwqueue *qh = ERR_PTR(-EINVAL);
+	void *caller = __builtin_return_address(0);
+
+	if (flags & O_EXCL)
+		flags |= O_CREAT;
+
+	switch (id) {
+	case HWQUEUE_ANY:
+		qh = hwqueue_open_any(name, flags, caller);
+		break;
+	case HWQUEUE_BYNAME:
+		if (WARN_ON(flags & (O_EXCL | O_CREAT)))
+			break;
+		qh = hwqueue_open_by_name(name, flags, caller);
+		break;
+	default:
+		qh = hwqueue_open_by_id(name, id, flags, caller);
+		break;
+	}
+
+	return qh;
+}
+EXPORT_SYMBOL(hwqueue_open);
+
+static void devm_hwqueue_release(struct device *dev, void *res)
+{
+	hwqueue_close(*(struct hwqueue **)res);
+}
+
+struct hwqueue *devm_hwqueue_open(struct device *dev, const char *name,
+				  unsigned id, unsigned flags)
+{
+	struct hwqueue **ptr, *queue;
+
+	ptr = devres_alloc(devm_hwqueue_release, sizeof(*ptr), GFP_KERNEL);
+	if (!ptr)
+		return NULL;
+
+	queue = hwqueue_open(name, id, flags);
+	if (queue) {
+		*ptr = queue;
+		devres_add(dev, ptr);
+	} else
+		devres_free(ptr);
+
+	return queue;
+}
+EXPORT_SYMBOL(devm_hwqueue_open);
+
+/**
+ * hwqueue_close() - close a hardware queue handle
+ * @qh	- handle to close
+ */
+void hwqueue_close(struct hwqueue *qh)
+{
+	struct hwqueue_instance *inst = qh->inst;
+	struct hwqueue_device *hdev = inst->hdev;
+
+	while (atomic_read(&qh->notifier_enabled) > 0)
+		hwqueue_disable_notifier(qh);
+
+	mutex_lock(&hwqueue_devices_lock);
+	list_del_rcu(&qh->list);
+	mutex_unlock(&hwqueue_devices_lock);
+
+	synchronize_rcu();
+
+	module_put(hdev->dev->driver->owner);
+
+	if (!hwqueue_is_busy(inst))
+		hdev->ops->close(inst);
+	kfree(qh);
+}
+EXPORT_SYMBOL(hwqueue_close);
+
+static int devm_hwqueue_match(struct device *dev, void *res, void *match_data)
+{
+	return *(void **)res == match_data;
+}
+
+void devm_hwqueue_close(struct device *dev, struct hwqueue *qh)
+{
+	WARN_ON(devres_destroy(dev, devm_hwqueue_release, devm_hwqueue_match,
+			       (void *)qh));
+	hwqueue_close(qh);
+}
+EXPORT_SYMBOL(devm_hwqueue_close);
+
+/**
+ * hwqueue_get_id() - get an ID number for an open queue.  This ID may be
+ *		      passed to another part of the kernel, which then opens the
+ *		      queue by ID.
+ * @qh	- queue handle
+ *
+ * Returns queue id (>= 0) on success, negative return value is an error.
+ */
+int hwqueue_get_id(struct hwqueue *qh)
+{
+	struct hwqueue_instance *inst = qh->inst;
+	unsigned base_id = inst->hdev->base_id;
+
+	return base_id + hwqueue_inst_to_id(inst);
+}
+EXPORT_SYMBOL(hwqueue_get_id);
+
+/**
+ * hwqueue_get_hw_id() - get an ID number for an open queue.  This ID may be
+ *			 passed to hardware modules as a part of
+ *			 descriptor/buffer content.
+ * @qh	- queue handle
+ *
+ * Returns queue id (>= 0) on success, negative return value is an error.
+ */
+int hwqueue_get_hw_id(struct hwqueue *qh)
+{
+	struct hwqueue_instance *inst = qh->inst;
+	struct hwqueue_device *hdev = inst->hdev;
+
+	if (!hdev->ops->get_hw_id)
+		return -EINVAL;
+
+	return hdev->ops->get_hw_id(inst);
+}
+EXPORT_SYMBOL(hwqueue_get_hw_id);
+
+/**
+ * hwqueue_enable_notifier() - Enable notifier callback for a queue handle.
+ * @qh	- hardware queue handle
+ *
+ * Returns 0 on success, errno otherwise.
+ */
+int hwqueue_enable_notifier(struct hwqueue *qh)
+{
+	struct hwqueue_instance *inst = qh->inst;
+	struct hwqueue_device *hdev = inst->hdev;
+	bool first;
+
+	if (!hwqueue_is_readable(qh))
+		return -EINVAL;
+
+	if (WARN_ON(!qh->notifier_fn))
+		return -EINVAL;
+
+	/* Adjust the per handle notifier count */
+	first = (atomic_inc_return(&qh->notifier_enabled) == 1);
+	if (!first)
+		return 0; /* nothing to do */
+
+	/* Now adjust the per instance notifier count */
+	first = (atomic_inc_return(&inst->num_notifiers) == 1);
+	if (first)
+		hdev->ops->set_notify(inst, true);
+
+	return 0;
+}
+EXPORT_SYMBOL(hwqueue_enable_notifier);
+
+/**
+ * hwqueue_disable_notifier() - Disable notifier callback for a queue handle.
+ * @qh	- hardware queue handle
+ *
+ * Returns 0 on success, errno otherwise.
+ */
+int hwqueue_disable_notifier(struct hwqueue *qh)
+{
+	struct hwqueue_instance *inst = qh->inst;
+	struct hwqueue_device *hdev = inst->hdev;
+	bool last;
+
+	if (!hwqueue_is_readable(qh))
+		return -EINVAL;
+
+	last = (atomic_dec_return(&qh->notifier_enabled) == 0);
+	if (!last)
+		return 0; /* nothing to do */
+
+	last = (atomic_dec_return(&inst->num_notifiers) == 0);
+	if (last)
+		hdev->ops->set_notify(inst, false);
+
+	return 0;
+}
+EXPORT_SYMBOL(hwqueue_disable_notifier);
+
+/**
+ * hwqueue_set_notifier() - Set a notifier callback to a queue handle.  This
+ *			    notifier is called whenever the queue has
+ *			    something to pop.
+ * @qh	- hardware queue handle
+ * @fn		- callback function
+ * @fn_arg	- argument for the callback function
+ *
+ * Hardware queues can have multiple notifiers attached to them.
+ * The underlying notification mechanism may vary from queue to queue.  For
+ * example, some queues may issue notify callbacks on timer expiry, and some
+ * may do so in interrupt context.  Notifier callbacks may be called from an
+ * atomic context, and _must not_ block ever.
+ *
+ * Returns 0 on success, errno otherwise.
+ */
+int hwqueue_set_notifier(struct hwqueue *qh, hwqueue_notify_fn fn,
+			 void *fn_arg)
+{
+	hwqueue_notify_fn old_fn = qh->notifier_fn;
+
+	if (!hwqueue_is_readable(qh))
+		return -EINVAL;
+
+	if (!fn && old_fn)
+		hwqueue_disable_notifier(qh);
+
+	qh->notifier_fn = fn;
+	qh->notifier_fn_arg = fn_arg;
+
+	if (fn && !old_fn)
+		hwqueue_enable_notifier(qh);
+
+	return 0;
+}
+EXPORT_SYMBOL(hwqueue_set_notifier);
+
+dma_addr_t __hwqueue_pop_slow(struct hwqueue *qh, unsigned *size,
+			      struct timeval *timeout, unsigned flags)
+{
+	struct hwqueue_instance *inst = qh->inst;
+	dma_addr_t dma_addr = 0;
+	int ret;
+
+	if (timeout) {
+		unsigned long expires = timeval_to_jiffies(timeout);
+
+		ret = wait_event_interruptible_timeout(inst->wait,
+				(dma_addr = qh->pop(inst, size, flags)),
+				expires);
+		if (ret < 0)
+			return 0;
+		if (!ret && !dma_addr)
+			return 0;
+		jiffies_to_timeval(ret, timeout);
+	} else {
+		ret = wait_event_interruptible(inst->wait,
+				(dma_addr = qh->pop(inst, size, flags)));
+		if (ret < 0)
+			return 0;
+		if (WARN_ON(!ret && !dma_addr))
+			return 0;
+	}
+
+	return dma_addr;
+}
+EXPORT_SYMBOL(__hwqueue_pop_slow);
+
+/**
+ * hwqueue_notify() - notify users on data availability
+ * @inst	- hardware queue instance
+ *
+ * Walk through the notifier list for a hardware queue instance and issue
+ * callbacks.  This function is called by drivers when data is available on a
+ * hardware queue, either when notified via interrupt or on timer poll.
+ */
+void hwqueue_notify(struct hwqueue_instance *inst)
+{
+	struct hwqueue *qh;
+
+	rcu_read_lock();
+
+	for_each_handle_rcu(qh, inst) {
+		if (atomic_read(&qh->notifier_enabled) <= 0)
+			continue;
+		if (WARN_ON(!qh->notifier_fn))
+			continue;
+		atomic_inc(&qh->stats.notifies);
+		qh->notifier_fn(qh->notifier_fn_arg);
+	}
+
+	rcu_read_unlock();
+
+	wake_up_interruptible(&inst->wait);
+}
+EXPORT_SYMBOL(hwqueue_notify);
+
+static void __hwqueue_poll(unsigned long data)
+{
+	struct hwqueue_instance *inst = (struct hwqueue_instance *)data;
+	struct hwqueue *qh;
+
+	rcu_read_lock();
+
+	for_each_handle_rcu(qh, inst) {
+		if (hwqueue_get_count(qh) > 0)
+			hwqueue_notify(inst);
+		break;
+	}
+
+	rcu_read_unlock();
+
+	mod_timer(&inst->poll_timer, jiffies +
+		  msecs_to_jiffies(hwqueue_poll_interval));
+}
+
+void hwqueue_set_poll(struct hwqueue_instance *inst, bool enabled)
+{
+	unsigned long expires;
+
+	if (!enabled) {
+		del_timer(&inst->poll_timer);
+		return;
+	}
+
+	expires = jiffies + msecs_to_jiffies(hwqueue_poll_interval);
+	mod_timer(&inst->poll_timer, expires);
+}
+EXPORT_SYMBOL(hwqueue_set_poll);
+
+static void hwqueue_debug_show_instance(struct seq_file *s,
+					struct hwqueue_instance *inst)
+{
+	struct hwqueue_device *hdev = inst->hdev;
+	struct hwqueue *qh;
+
+	if (!hwqueue_is_busy(inst))
+		return;
+
+	seq_printf(s, "\tqueue id %d (%s)\n",
+		   hdev->base_id + hwqueue_inst_to_id(inst), inst->name);
+
+	for_each_handle_rcu(qh, inst) {
+		seq_printf(s, "\t\thandle %p: ", qh);
+		seq_printf(s, "pushes %8d, ",
+			   atomic_read(&qh->stats.pushes));
+		seq_printf(s, "pops %8d, ",
+			   atomic_read(&qh->stats.pops));
+		seq_printf(s, "count %8d, ",
+			   hwqueue_get_count(qh));
+		seq_printf(s, "notifies %8d, ",
+			   atomic_read(&qh->stats.notifies));
+		seq_printf(s, "push errors %8d, ",
+			   atomic_read(&qh->stats.push_errors));
+		seq_printf(s, "pop errors %8d\n",
+			   atomic_read(&qh->stats.pop_errors));
+	}
+}
+
+static int hwqueue_debug_show(struct seq_file *s, void *v)
+{
+	struct hwqueue_device *hdev;
+	struct hwqueue_instance *inst;
+	int id;
+
+	mutex_lock(&hwqueue_devices_lock);
+
+	for_each_device(hdev) {
+		seq_printf(s, "hdev %s: %u-%u\n",
+			   dev_name(hdev->dev), hdev->base_id,
+			   hdev->base_id + hdev->num_queues - 1);
+
+		for_each_instance(id, inst, hdev)
+			hwqueue_debug_show_instance(s, inst);
+	}
+
+	mutex_unlock(&hwqueue_devices_lock);
+
+	return 0;
+}
+
+static int hwqueue_debug_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, hwqueue_debug_show, NULL);
+}
+
+static const struct file_operations hwqueue_debug_ops = {
+	.open		= hwqueue_debug_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+static int __init hwqueue_debug_init(void)
+{
+	debugfs_create_file("hwqueues", S_IFREG | S_IRUGO, NULL, NULL,
+			    &hwqueue_debug_ops);
+	return 0;
+}
+device_initcall(hwqueue_debug_init);
+
+MODULE_LICENSE("GPL v2");
+MODULE_DESCRIPTION("Hardware queue interface");
diff --git a/drivers/hwqueue/hwqueue_internal.h b/drivers/hwqueue/hwqueue_internal.h
new file mode 100644
index 0000000..ce99b54
--- /dev/null
+++ b/drivers/hwqueue/hwqueue_internal.h
@@ -0,0 +1,125 @@
+/*
+ * Hardware queues handle header
+ *
+ * Copyright (C) 2011 Texas Instruments Incorporated - http://www.ti.com
+ *
+ * Contact: Prabhu Kuttiyam <pkuttiyam@ti.com>
+ *	    Cyril Chemparathy <cyril@ti.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef __HWQUEUE_HWQUEUE_H
+#define __HWQUEUE_HWQUEUE_H
+
+#include <linux/device.h>
+#include <linux/wait.h>
+#include <linux/hwqueue.h>
+
+
+struct hwqueue_instance {
+	struct list_head	 handles;
+	struct hwqueue_device	*hdev;
+	struct timer_list	 poll_timer;
+	wait_queue_head_t	 wait;
+	void			*priv;
+	char			 name[32];
+	atomic_t		 num_notifiers;
+	struct hwqueue_inst_ops	*ops;
+};
+
+struct hwqueue_device_ops {
+	/*
+	 * Return a match quotient (0 = best .. UINT_MAX-1) for a set of
+	 * option flags.  Negative error values imply "do not allocate"
+	 */
+	int	 (*match)(struct hwqueue_instance *inst, unsigned flags);
+
+	/* Initialize a queue inst when opened for the first time */
+	int	 (*open)(struct hwqueue_instance *inst, unsigned flags);
+
+	/* Close a queue inst when closed by the last user */
+	void	 (*close)(struct hwqueue_instance *inst);
+
+	/* Enable or disable notification */
+	void	 (*set_notify)(struct hwqueue_instance *inst, bool enabled);
+
+	/* Get a hardware identifier for a queue */
+	int	 (*get_hw_id)(struct hwqueue_instance *inst);
+};
+
+struct hwqueue_inst_ops {
+	/* Push something into the queue */
+	int	 (*push)(struct hwqueue_instance *inst, dma_addr_t dma,
+			 unsigned size, unsigned flags);
+
+	/* Pop something from the queue */
+	dma_addr_t (*pop)(struct hwqueue_instance *inst, unsigned *size,
+			  unsigned flags);
+
+	/* Flush a queue */
+	int	 (*flush)(struct hwqueue_instance *inst);
+
+	/* Poll number of elements on the queue */
+	int	 (*get_count)(struct hwqueue_instance *inst);
+	
+	/* Perform DMA mapping on objects to be pushed */
+	int	 (*map)(struct hwqueue_instance *inst, void *data,
+			unsigned size, dma_addr_t *dma_ptr, unsigned *size_ptr);
+	
+	/* Perform DMA unmapping on objects that have been pulled */
+	void	*(*unmap)(struct hwqueue_instance *inst, dma_addr_t dma,
+			unsigned desc_size);
+};
+
+struct hwqueue_device {
+	unsigned			 base_id;
+	unsigned			 num_queues;
+	unsigned			 inst_shift;
+	void				*instances;
+	struct hwqueue_device_ops	*ops;
+
+	unsigned			 priv_size;
+	struct list_head		 list;
+	struct device			*dev;
+};
+
+static inline int hwqueue_inst_to_id(struct hwqueue_instance *inst)
+{
+	struct hwqueue_device *hdev = inst->hdev;
+	int offset = (void *)inst - hdev->instances;
+	int inst_size = 1 << hdev->inst_shift;
+
+	BUG_ON(offset & (inst_size - 1));
+	return offset >> hdev->inst_shift;
+}
+
+static inline struct hwqueue_instance *
+hwqueue_id_to_inst(struct hwqueue_device *hdev, unsigned id)
+{
+	return hdev->instances + (id <<  hdev->inst_shift);
+}
+
+static inline void *hwqueue_inst_to_priv(struct hwqueue_instance *inst)
+{
+	return (void *)(inst + 1);
+}
+
+static inline struct hwqueue *rcu_to_handle(struct rcu_head *rcu)
+{
+	return container_of(rcu, struct hwqueue, rcu);
+}
+
+int hwqueue_device_register(struct hwqueue_device *dev);
+int hwqueue_device_unregister(struct hwqueue_device *dev);
+void hwqueue_notify(struct hwqueue_instance *inst);
+void hwqueue_set_poll(struct hwqueue_instance *inst, bool enabled);
+
+#endif /* __HWQUEUE_HWQUEUE_H */
diff --git a/drivers/hwqueue/hwqueue_test.c b/drivers/hwqueue/hwqueue_test.c
new file mode 100644
index 0000000..06a4cb0
--- /dev/null
+++ b/drivers/hwqueue/hwqueue_test.c
@@ -0,0 +1,267 @@
+/*
+ * Hardware queue test framework
+ *
+ * Copyright (C) 2012 Texas Instruments Incorporated - http://www.ti.com
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#define pr_fmt(fmt) "hwqueue-test: " fmt
+#define irqpop_test1
+
+#include <linux/kernel.h>
+#include <linux/hwqueue.h>
+#include <linux/module.h>
+#include <linux/types.h>
+
+struct hwqueue_test_ctx {
+	struct hwqueue *qpool;
+	struct hwqueue *qgenwr;
+	struct hwqueue *qgenrd;
+	struct hwqueue *qirqwr;
+	struct hwqueue *qirqrd;
+	unsigned desc_size;
+	bool notified;
+	bool notified_irq;
+	bool notify_added;
+	bool notify_irq_added;
+	dma_addr_t desc;
+};
+
+static void hwqueue_test_notify(void *arg)
+{
+	struct hwqueue_test_ctx *ctx = arg;
+
+	ctx->notified = true;
+	pr_info("notification fired\n");
+}
+
+static void hwqueue_test_notify_irq(void *arg)
+{
+	struct hwqueue_test_ctx *ctx = arg;
+
+
+	ctx->notified_irq = true;
+	pr_info("notification for irq fired\n");
+
+#ifdef irqpop_test1
+	hwqueue_set_notifier(ctx->qirqrd, NULL, NULL);
+	ctx->notify_irq_added = false;
+#else
+	unsigned desc_size;
+	dma_addr_t desc;
+	desc = hwqueue_pop(ctx->qirqrd, &desc_size, NULL, 0);
+	if (!desc)
+		pr_err("test3: failed to pop desc from read queue\n");
+
+	pr_info("test3: popped desc %08x, size %d\n", desc, desc_size);
+
+	if (desc != ctx->desc || desc_size != ctx->desc_size)
+		pr_err("test3: unexpected descriptor popped on read\n");
+#endif
+}
+
+static int __init hwqueue_test(void)
+{
+	unsigned long end_time, timeout = 1000;
+	struct hwqueue_test_ctx ctx;
+	struct hwqueue *qpool;
+	struct hwqueue *qgenwr, *qgenrd;
+	struct hwqueue *qirqwr, *qirqrd;
+	unsigned desc_size;
+	dma_addr_t desc;
+	int ret;
+
+	memset(&ctx, 0, sizeof(ctx));
+
+	qpool = hwqueue_open("pool-net", HWQUEUE_BYNAME, O_RDWR);
+	if (IS_ERR_OR_NULL(qpool)) {
+		ret = PTR_ERR(qpool);
+		pr_err("failed to open pool queue, errno=%d\n", ret);
+		goto fail;
+	}
+	pr_info("opened qpool, id=%d\n", hwqueue_get_id(qpool));
+	ctx.qpool = qpool;
+
+	qgenwr = hwqueue_open("test", HWQUEUE_ANY, O_WRONLY | O_CREAT);
+	if (IS_ERR_OR_NULL(qgenwr)) {
+		ret = PTR_ERR(qgenwr);
+		pr_err("failed to open write queue, errno=%d\n", ret);
+		goto fail;
+	}
+	pr_info("opened qgenwr, id=%d qh = %p\n",
+		hwqueue_get_id(qgenwr), qgenwr);
+	ctx.qgenwr = qgenwr;
+
+	qgenrd = hwqueue_open("test", hwqueue_get_id(qgenwr), O_RDONLY);
+	if (IS_ERR_OR_NULL(qgenrd)) {
+		ret = PTR_ERR(qgenrd);
+		pr_err("failed to open read queue, errno=%d\n", ret);
+		goto fail;
+	}
+	pr_info("opened qgenrd, id = %d, qh = %p\n",
+		hwqueue_get_id(qgenrd), qgenrd);
+	ctx.qgenrd = qgenrd;
+
+	qirqwr = hwqueue_open("irq1", 655, O_LOWLATENCY | O_RDWR);
+	if (IS_ERR_OR_NULL(qirqwr)) {
+		ret = PTR_ERR(qirqwr);
+		pr_err("failed to open write queue, errno=%d\n", ret);
+		goto fail;
+	}
+	pr_info("opened qirqwr, id = %d qh = %p\n",
+		hwqueue_get_id(qirqwr), qirqwr);
+	ctx.qirqwr = qirqwr;
+
+	qirqrd = hwqueue_open("irq1", hwqueue_get_id(qirqwr), O_RDONLY);
+	if (IS_ERR_OR_NULL(qirqrd)) {
+		ret = PTR_ERR(qirqrd);
+		pr_err("failed to open read queue, errno=%d\n", ret);
+		goto fail;
+	}
+	pr_info("opened qirqrd, id=%d qh = %p\n",
+		hwqueue_get_id(qirqrd), qirqrd);
+	ctx.qirqrd = qirqrd;
+
+	ret = hwqueue_set_notifier(qirqrd, hwqueue_test_notify_irq, &ctx);
+	if (ret < 0) {
+		pr_err("failed to set notifier, errno=%d\n", ret);
+		goto fail;
+	}
+	ctx.notify_irq_added = true;
+
+	ret = hwqueue_set_notifier(qgenrd, hwqueue_test_notify, &ctx);
+	if (ret < 0) {
+		pr_err("failed to set notifier, errno=%d\n", ret);
+		goto fail;
+	}
+	ctx.notify_added = true;
+
+	desc = hwqueue_pop(qpool, &desc_size, NULL, 0);
+	if (!desc) {
+		ret = -ENOENT;
+		pr_err("failed to pop pool desc\n");
+		goto fail;
+	}
+	pr_info("popped desc from qpool %08x, size %d\n", desc, desc_size);
+	ctx.desc = desc;
+	ctx.desc_size = desc_size;
+
+	/* Test 1: Simple push and pop */
+	pr_err("test1: simple pop test:\n");
+	ret = hwqueue_push(qgenwr, desc, desc_size, 0);
+	if (ret < 0) {
+		pr_err("test1: failed to push desc, errno=%d\n", ret);
+		goto fail;
+	}
+	pr_info("test1: pushed desc %08x, size %d\n", desc, desc_size);
+
+	desc = hwqueue_pop(qgenrd, &desc_size, NULL, 0);
+	if (!desc) {
+		ret = -ENOENT;
+		pr_err("test1: failed to pop desc from read queue\n");
+		goto fail;
+	}
+	pr_info("test1: popped desc %08x, size %d\n", desc, desc_size);
+
+	if (desc != ctx.desc || desc_size != ctx.desc_size) {
+		ret = -EINVAL;
+		pr_err("unexpected descriptor popped on read\n");
+	}
+
+	/* Test 2: Notified push and pop */
+	pr_err("test2: poll notified pop test:\n");
+	ret = hwqueue_push(qgenwr, desc, desc_size, 0);
+	if (ret < 0) {
+		pr_err("failed to push desc, errno=%d\n", ret);
+		goto fail;
+	}
+	pr_info("test2: pushed desc %08x, size %d\n", desc, desc_size);
+
+	end_time = jiffies + msecs_to_jiffies(timeout);
+	while (jiffies < end_time) {
+		if (ctx.notified)
+			break;
+		cpu_relax();
+	}
+	if (!ctx.notified)
+		pr_err("test2: timed out waiting for hwqueue notification\n");
+	ctx.notified = false;
+
+	desc = hwqueue_pop(qgenrd, &desc_size, NULL, 0);
+	if (!desc) {
+		ret = -ENOENT;
+		pr_err("test2: failed to pop desc from read queue\n");
+		goto fail;
+	}
+	pr_info("test2: popped desc %08x, size %d\n", desc, desc_size);
+
+	if (desc != ctx.desc || desc_size != ctx.desc_size) {
+		ret = -EINVAL;
+		pr_err("test2: unexpected descriptor popped on read\n");
+	}
+
+	/* Test 3: IRQ push, notify and pop */
+	pr_err("test3: irq notified pop test:\n");
+	pr_info("test3: pushing desc %08x, size %d\n", desc, desc_size);
+	ret = hwqueue_push(qirqwr, desc, desc_size, 0);
+	if (ret < 0) {
+		pr_err("failed to push desc, errno=%d\n", ret);
+		goto fail;
+	}
+
+	end_time = jiffies + msecs_to_jiffies(timeout);
+	while (jiffies < end_time) {
+		if (ctx.notified_irq)
+			break;
+		cpu_relax();
+	}
+	if (!ctx.notified_irq)
+		pr_err("test3: timed out waiting for hwqueue notification\n");
+	ctx.notified_irq = false;
+
+#ifdef irqpop_test1
+	desc = hwqueue_pop(qirqrd, &desc_size, NULL, 0);
+	if (!desc) {
+		ret = -ENOENT;
+		pr_err("test3: failed to pop desc from read queue\n");
+		goto fail;
+	}
+	pr_info("test3: popped desc %08x, size %d\n", desc, desc_size);
+
+	if (desc != ctx.desc || desc_size != ctx.desc_size) {
+		ret = -EINVAL;
+		pr_err("test3: unexpected descriptor popped on read\n");
+	}
+#endif
+
+fail:
+	if (ctx.desc)
+		hwqueue_push(ctx.qpool, ctx.desc, ctx.desc_size, 0);
+	if (ctx.notify_added)
+		hwqueue_set_notifier(ctx.qgenrd, NULL, NULL);
+	if (ctx.notify_irq_added)
+		hwqueue_set_notifier(ctx.qirqrd, NULL, NULL);
+	if (ctx.qgenrd)
+		hwqueue_close(ctx.qgenrd);
+	if (ctx.qgenwr)
+		hwqueue_close(ctx.qgenwr);
+	if (ctx.qirqwr)
+		hwqueue_close(ctx.qirqwr);
+	if (ctx.qirqrd)
+		hwqueue_close(ctx.qirqrd);
+	if (ctx.qpool)
+		hwqueue_close(ctx.qpool);
+	return ret;
+}
+module_init(hwqueue_test);
+
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("Hardware queue test driver");
diff --git a/drivers/hwqueue/keystone_hwqueue.c b/drivers/hwqueue/keystone_hwqueue.c
new file mode 100644
index 0000000..b779d10
--- /dev/null
+++ b/drivers/hwqueue/keystone_hwqueue.c
@@ -0,0 +1,1314 @@
+/*
+ * Keystone hardware queue driver
+ *
+ * Copyright (C) 2011 Texas Instruments Incorporated - http://www.ti.com
+ * Contact: Prabhu Kuttiyam <pkuttiyam@ti.com>
+ *	    Cyril Chemparathy <cyril@ti.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/device.h>
+#include <linux/io.h>
+#include <linux/bitops.h>
+#include <linux/slab.h>
+#include <linux/spinlock.h>
+#include <linux/hwqueue.h>
+#include <linux/platform_device.h>
+#include <linux/dma-mapping.h>
+#include <linux/of.h>
+#include <linux/of_irq.h>
+#include <linux/of_device.h>
+#include <linux/of_address.h>
+#include <linux/firmware.h>
+#include <linux/interrupt.h>
+
+#include "hwqueue_internal.h"
+#include "keystone_hwqueue.h"
+
+static inline int khwq_pdsp_wait(u32 * __iomem addr, unsigned timeout,
+				 u32 flags)
+{
+	unsigned long end_time;
+	u32 val = 0;
+	int ret;
+
+	end_time = jiffies + msecs_to_jiffies(timeout);
+	while (jiffies < end_time) {
+		val = __raw_readl(addr);
+		if (flags)
+			val &= flags;
+		if (!val)
+			break;
+		cpu_relax();
+	}
+	ret = val ? -ETIMEDOUT : 0;
+
+	return ret;
+}
+
+static int khwq_match(struct hwqueue_instance *inst, unsigned flags)
+{
+	struct khwq_instance *kq = hwqueue_inst_to_priv(inst);
+	struct khwq_range_info *range;
+	int score = 0;
+
+	if (!kq)
+		return -ENOENT;
+
+	range = kq->range;
+	if (!range)
+		return -ENOENT;
+
+	if (range->flags & RANGE_RESERVED)
+		score += 1000;
+
+	if ((range->flags & RANGE_HAS_ACCUMULATOR) &&
+	    !(flags & O_HIGHTHROUGHPUT))
+		score += 100;
+	if (!(range->flags & RANGE_HAS_ACCUMULATOR) &&
+	    (flags & O_HIGHTHROUGHPUT))
+		score += 100;
+
+	if ((range->flags & RANGE_HAS_IRQ) &&
+	    !(flags & (O_LOWLATENCY | O_HIGHTHROUGHPUT)))
+		score += 100;
+	if (!(range->flags & RANGE_HAS_IRQ) &&
+	    (flags & (O_LOWLATENCY | O_HIGHTHROUGHPUT)))
+		score += 100;
+
+	return score;
+}
+
+static irqreturn_t khwq_int_handler(int irq, void *_instdata)
+{
+	struct hwqueue_instance *inst = _instdata;
+
+	hwqueue_notify(inst);
+
+	return IRQ_HANDLED;
+}
+
+static void khwq_set_notify(struct hwqueue_instance *inst, bool enabled)
+{
+	struct khwq_range_info *range;
+	struct khwq_instance *kq;
+	unsigned queue;
+
+	kq    = hwqueue_inst_to_priv(inst);
+	range = kq->range;
+
+	if (range->ops) {
+		if (range->ops->set_notify)
+			range->ops->set_notify(range, inst, enabled);
+	} else if (range->flags & RANGE_HAS_IRQ) {
+		queue = hwqueue_inst_to_id(inst) - range->queue_base;
+		if (enabled)
+			enable_irq(range->irqs[queue]);
+		else
+			disable_irq_nosync(range->irqs[queue]);
+	} else
+		hwqueue_set_poll(inst, enabled);
+}
+
+static int khwq_setup_irq(struct khwq_range_info *range,
+			  struct hwqueue_instance *inst)
+{
+	struct khwq_instance *kq = hwqueue_inst_to_priv(inst);
+	unsigned queue = hwqueue_inst_to_id(inst) - range->queue_base;
+	int ret = 0, irq;
+
+	if (range->flags & RANGE_HAS_IRQ) {
+		irq = range->irqs[queue];
+
+		ret = request_irq(irq, khwq_int_handler, 0, kq->irq_name, inst);
+		if (ret >= 0)
+			disable_irq(irq);
+	}
+
+	return ret;
+}
+
+static void khwq_free_irq(struct hwqueue_instance *inst)
+{
+	struct khwq_instance *kq = hwqueue_inst_to_priv(inst);
+	struct khwq_range_info *range = kq->range;
+	unsigned id = hwqueue_inst_to_id(inst) - range->queue_base;
+	int irq;
+
+	if (range->flags & RANGE_HAS_IRQ) {
+		irq = range->irqs[id];
+		free_irq(irq, inst);
+	}
+}
+
+static int khwq_open(struct hwqueue_instance *inst, unsigned flags)
+{
+	struct khwq_instance *kq = hwqueue_inst_to_priv(inst);
+	struct khwq_range_info *range = kq->range;
+
+	if (!range->ops)
+		return khwq_setup_irq(range, inst);
+	else if (range->ops->open_queue)
+		return range->ops->open_queue(range, inst, flags);
+	else
+		return 0;
+}
+
+static void khwq_close(struct hwqueue_instance *inst)
+{
+	struct khwq_instance *kq = hwqueue_inst_to_priv(inst);
+	struct khwq_range_info *range = kq->range;
+
+	if (!range->ops)
+		khwq_free_irq(inst);
+	else if (range->ops->close_queue)
+		range->ops->close_queue(range, inst);
+}
+
+static inline struct khwq_region *
+khwq_find_region_by_virt(struct khwq_device *kdev, struct khwq_instance *kq,
+			 void *virt)
+{
+	struct khwq_region *region;
+
+	if (likely(kq->last && kq->last->virt_start <= virt &&
+		   kq->last->virt_end > virt))
+		return kq->last;
+
+	for_each_region(kdev, region) {
+		if (region->virt_start <= virt && region->virt_end > virt) {
+			kq->last = region;
+			return kq->last;
+		}
+	}
+
+	return NULL;
+}
+
+static inline struct khwq_region *
+khwq_find_region_by_dma(struct khwq_device *kdev, struct khwq_instance *kq,
+			dma_addr_t dma)
+{
+	struct khwq_region *region;
+
+	if (likely(kq->last && kq->last->dma_start <= dma &&
+		   kq->last->dma_end > dma))
+		return kq->last;
+
+	for_each_region(kdev, region) {
+		if (region->dma_start <= dma && region->dma_end > dma) {
+			kq->last = region;
+			return kq->last;
+		}
+	}
+
+	return NULL;
+}
+
+static int khwq_push(struct hwqueue_instance *inst, dma_addr_t dma,
+		     unsigned size, unsigned flags)
+{
+	struct khwq_qmgr_info *qmgr;
+	unsigned id;
+	u32 val;
+
+	qmgr = khwq_find_qmgr(inst);
+	if (!qmgr)
+		return -ENODEV;
+
+	id = hwqueue_inst_to_id(inst) - qmgr->start_queue;
+
+	val = (u32)dma | ((size / 16) - 1);
+
+	__raw_writel(val, &qmgr->reg_push[id].ptr_size_thresh);
+
+	return 0;
+}
+
+static dma_addr_t khwq_pop(struct hwqueue_instance *inst, unsigned *size,
+			   unsigned flags)
+{
+	struct khwq_instance *kq = hwqueue_inst_to_priv(inst);
+	struct khwq_qmgr_info *qmgr;
+	u32 val, desc_size, idx;
+	dma_addr_t dma;
+	unsigned id;
+
+	qmgr = khwq_find_qmgr(inst);
+	if (unlikely(!qmgr))
+		return -ENODEV;
+
+	id = hwqueue_inst_to_id(inst) - qmgr->start_queue;
+
+	/* are we accumulated? */
+	if (kq->descs) {
+		if (unlikely(atomic_dec_return(&kq->desc_count) < 0)) {
+			atomic_inc(&kq->desc_count);
+			return 0;
+		}
+
+		idx  = atomic_inc_return(&kq->desc_head);
+		idx &= ACC_DESCS_MASK;
+
+		val = kq->descs[idx];
+	} else {
+		val = __raw_readl(&qmgr->reg_pop[id].ptr_size_thresh);
+		if (unlikely(!val))
+			return 0;
+	}
+
+	dma = val & DESC_PTR_MASK;
+	desc_size = ((val & DESC_SIZE_MASK) + 1) * 16;
+
+	if (unlikely(size))
+		*size = desc_size;
+
+	return dma;
+}
+
+static int khwq_get_count(struct hwqueue_instance *inst)
+{
+	struct khwq_instance *kq = hwqueue_inst_to_priv(inst);
+	struct khwq_qmgr_info *qmgr;
+	unsigned id;
+
+	qmgr = khwq_find_qmgr(inst);
+	if (unlikely(!qmgr))
+		return -EINVAL;
+
+	id = hwqueue_inst_to_id(inst) - qmgr->start_queue;
+
+	return (__raw_readl(&qmgr->reg_peek[id].entry_count) +
+		atomic_read(&kq->desc_count));
+}
+
+static int khwq_flush(struct hwqueue_instance *inst)
+{
+	struct khwq_instance *kq = hwqueue_inst_to_priv(inst);
+	struct khwq_qmgr_info *qmgr;
+	unsigned id;
+
+	qmgr = khwq_find_qmgr(inst);
+	if (!qmgr)
+		return -ENODEV;
+
+	id = hwqueue_inst_to_id(inst) - qmgr->start_queue;
+
+	atomic_set(&kq->desc_count, 0);
+	__raw_writel(0, &qmgr->reg_push[id].ptr_size_thresh);
+	return 0;
+}
+
+static int khwq_map(struct hwqueue_instance *inst, void *data, unsigned size,
+		    dma_addr_t *dma_ptr, unsigned *size_ptr)
+{
+	struct khwq_device *kdev = from_hdev(inst->hdev);
+	struct khwq_instance *kq = hwqueue_inst_to_priv(inst);
+	struct khwq_region *region;
+	dma_addr_t dma;
+
+	region = khwq_find_region_by_virt(kdev, kq, data);
+	if (!region)
+		return -EINVAL;
+
+	if (unlikely(!region || size > region->desc_size))
+		return -EINVAL;
+
+	size = min(size, region->desc_size);
+	size = ALIGN(size, SMP_CACHE_BYTES);
+	dma = region->dma_start + (data - region->virt_start);
+
+	if (WARN_ON(dma & DESC_SIZE_MASK))
+		return -EINVAL;
+	dma_sync_single_for_device(kdev->dev, dma, size, DMA_TO_DEVICE);
+
+	*dma_ptr = dma;
+	*size_ptr = size;
+	return 0;
+}
+
+static void *khwq_unmap(struct hwqueue_instance *inst, dma_addr_t dma,
+			unsigned desc_size)
+{
+	struct khwq_device *kdev = from_hdev(inst->hdev);
+	struct khwq_instance *kq = hwqueue_inst_to_priv(inst);
+	struct khwq_region *region;
+	void *data;
+
+	region = khwq_find_region_by_dma(kdev, kq, dma);
+	if (WARN_ON(!region))
+		return NULL;
+
+	desc_size = min(desc_size, region->desc_size);
+
+	data = region->virt_start + (dma - region->dma_start);
+
+	dma_sync_single_for_cpu(kdev->dev, dma, desc_size, DMA_FROM_DEVICE);
+
+	prefetch(data);
+
+	return data;
+}
+
+static struct hwqueue_device_ops khdev_ops = {
+	.match		= khwq_match,
+	.open		= khwq_open,
+	.set_notify	= khwq_set_notify,
+	.close		= khwq_close,
+};
+
+static struct hwqueue_inst_ops khdev_inst_ops = {
+	.push		= khwq_push,
+	.pop		= khwq_pop,
+	.get_count	= khwq_get_count,
+	.flush		= khwq_flush,
+	.map		= khwq_map,
+	.unmap		= khwq_unmap,
+};
+
+static struct khwq_region *
+khwq_find_match_region(struct khwq_device *kdev, struct khwq_pool_info *pool)
+{
+	struct khwq_region *region;
+
+	for_each_region(kdev, region) {
+		if (region->id != pool->region_id)
+			continue;
+		if ((region->num_desc - region->used_desc) < pool->num_desc)
+			continue;
+		return region;
+	}
+	return NULL;
+}
+
+/* Map the requested pools to regions, creating regions as we go */
+static void khwq_map_pools(struct khwq_device *kdev)
+{
+	struct khwq_region *region;
+	struct khwq_pool_info *pool;
+
+	for_each_pool(kdev, pool) {
+		/* find a matching region*/
+		region = khwq_find_match_region(kdev, pool);
+		if (!region) {
+			dev_err(kdev->dev, "failed to set up pool %s %d\n",
+				pool->name, pool->num_desc);
+			continue;
+		}
+
+		/* link this pool to the region... */
+		pool->region = region;
+		pool->region_offset = region->used_desc;
+		region->used_desc += pool->num_desc;
+
+		dev_dbg(kdev->dev, "pool %s: num:%d, size:%d, region:%d \n",
+			pool->name, pool->num_desc,
+			pool->desc_size, region->id);
+	}
+}
+
+static int khwq_setup_region(struct khwq_device *kdev,
+				       struct khwq_region *region)
+{
+	unsigned hw_num_desc, hw_desc_size, size;
+	int id = region->id;
+	struct khwq_reg_region __iomem  *regs;
+	struct khwq_qmgr_info *qmgr;
+	struct page *page;
+
+	/* unused region? */
+	if (!region->num_desc) {
+		dev_warn(kdev->dev, "unused region %s\n", region->name);
+		return 0;
+	}
+
+	/* get hardware descriptor value */
+	hw_num_desc = ilog2(region->num_desc - 1) + 1;
+
+	/* did we force fit ourselves into nothingness? */
+	if (region->num_desc < 32) {
+		region->num_desc = 0;
+		dev_warn(kdev->dev, "too few descriptors in region %s\n", region->name);
+		return 0;
+	}
+
+	size = region->num_desc * region->desc_size;
+	region->virt_start = alloc_pages_exact(size, GFP_KERNEL | GFP_DMA);
+	if (!region->virt_start) {
+		region->num_desc = 0;
+		dev_err(kdev->dev, "memory alloc failed for region %s\n", region->name);
+		return 0;
+	}
+	region->virt_end = region->virt_start + size;
+	page = virt_to_page(region->virt_start);
+
+	region->dma_start = dma_map_page(kdev->dev, page, 0, size,
+					 DMA_BIDIRECTIONAL);
+	if (dma_mapping_error(kdev->dev, region->dma_start)) {
+		region->num_desc = 0;
+		free_pages_exact(region->virt_start, size);
+		dev_err(kdev->dev, "dma map failed for region %s\n", region->name);
+		return 0;
+	}
+	region->dma_end  = region->dma_start + size;
+
+	dev_dbg(kdev->dev,
+		"region %s (%d): size:%d, link:%d@%d, phys:%08x-%08x, virt:%p-%p\n",
+		region->name, id, region->desc_size, region->num_desc,
+		region->link_index, region->dma_start, region->dma_end,
+		region->virt_start, region->virt_end);
+
+	hw_desc_size = (region->desc_size / 16) - 1;
+	hw_num_desc -= 5;
+
+	for_each_qmgr(kdev, qmgr) {
+		regs = qmgr->reg_region + id;
+		__raw_writel(region->dma_start, &regs->base);
+		__raw_writel(region->link_index, &regs->start_index);
+		__raw_writel(hw_desc_size << 16 | hw_num_desc,
+			     &regs->size_count);
+	}
+
+	return region->num_desc;
+}
+
+static const char *khwq_find_name(struct device_node *node)
+{
+	const char *name;
+
+	if (of_property_read_string(node, "label", &name) < 0)
+		name = node->name;
+	if (!name)
+		name = "unknown";
+	return name;
+}
+
+static int khwq_setup_regions(struct khwq_device *kdev,
+					struct device_node *regions)
+{
+	struct device *dev = kdev->dev;
+	struct khwq_region *region;
+	struct device_node *child;
+	u32 temp[2];
+	int ret;
+
+	for_each_child_of_node(regions, child) {
+		region = devm_kzalloc(dev, sizeof(*region), GFP_KERNEL);
+		if (!region) {
+			dev_err(dev, "out of memory allocating region\n");
+			return -ENOMEM;
+		}
+
+		region->name = khwq_find_name(child);
+		of_property_read_u32(child, "id", &region->id);
+		ret = of_property_read_u32_array(child, "values", temp, 2);
+		if (!ret) {
+			region->num_desc  = temp[0];
+			region->desc_size = temp[1];
+		} else {
+			dev_err(dev, "invalid region info %s\n", region->name);
+			devm_kfree(dev, region);
+			continue;
+		}
+
+		if (!of_get_property(child, "link-index", NULL)) {
+			dev_err(dev, "No link info for %s\n", region->name);
+			devm_kfree(dev, region);
+			continue;
+		}
+		ret = of_property_read_u32(child, "link-index",
+					   &region->link_index);
+		if (ret) {
+			dev_err(dev, "link index not found for %s\n",
+				region->name);
+			devm_kfree(dev, region);
+			continue;
+		}
+
+		list_add_tail(&region->list, &kdev->regions);
+	}
+	if (list_empty(&kdev->regions)) {
+		dev_err(dev, "no valid region information found\n");
+		return -ENODEV;
+	}
+
+	khwq_map_pools(kdev);
+
+	/* Next, we run through the regions and set things up */
+	for_each_region(kdev, region)
+		khwq_setup_region(kdev, region);
+
+	return 0;
+}
+
+/* carve out descriptors and push into named queues */
+static void khwq_fill_pools(struct khwq_device *kdev)
+{
+	struct khwq_region *region;
+	struct khwq_pool_info *pool;
+	dma_addr_t dma_addr;
+	unsigned dma_size;
+	int ret, i;
+
+	for_each_pool(kdev, pool) {
+		pool->queue = hwqueue_open(pool->name, HWQUEUE_ANY,
+					   O_CREAT | O_RDWR | O_NONBLOCK);
+		if (IS_ERR_OR_NULL(pool->queue)) {
+			dev_err(kdev->dev,
+				"failed to open queue for pool %s, error %ld\n",
+				pool->name, PTR_ERR(pool->queue));
+			pool->queue = NULL;
+			continue;
+		}
+
+		region = pool->region;
+
+		if (!region || !region->num_desc) {
+			dev_err(kdev->dev, "no region for pool %s\n",
+				pool->name);
+			continue;
+		}
+
+		pool->desc_size = region->desc_size;
+		for (i = 0; i < pool->num_desc; i++) {
+			int index = pool->region_offset + i;
+			void *desc;
+
+			desc = region->virt_start + region->desc_size * index;
+			ret = hwqueue_map(pool->queue, desc, pool->desc_size,
+					  &dma_addr, &dma_size);
+			if (ret < 0) {
+				WARN_ONCE(ret, "failed map pool queue %s\n",
+					  pool->name);
+				continue;
+			}
+			ret = hwqueue_push(pool->queue, dma_addr, dma_size, 0);
+			WARN_ONCE(ret, "failed push to pool queue %s\n",
+				  pool->name);
+		}
+	}
+}
+
+static int khwq_get_link_ram(struct khwq_device *kdev,
+				       const char *name,
+				       struct khwq_link_ram_block *block)
+{
+	struct platform_device *pdev = to_platform_device(kdev->dev);
+	struct device_node *node = pdev->dev.of_node;
+	u32 temp[2];
+
+	/*
+	 * Note: link ram resources are specified in "entry" sized units. In
+	 * reality, although entries are ~40bits in hardware, we treat them as
+	 * 64-bit entities here.
+	 *
+	 * For example, to specify the internal link ram for Keystone-I class
+	 * devices, we would set the linkram0 resource to 0x80000-0x83fff.
+	 *
+	 * This gets a bit weird when other link rams are used.  For example,
+	 * if the range specified is 0x0c000000-0x0c003fff (i.e., 16K entries
+	 * in MSMC SRAM), the actual memory used is 0x0c000000-0x0c020000,
+	 * which accounts for 64-bits per entry, for 16K entries.
+	 */
+	if (!of_property_read_u32_array(node, name , temp, 2)) {
+		if (temp[0]) {
+			/*
+			 * queue_base specified => using internal or onchip
+			 * link ram WARNING - we do not "reserve" this block
+			 */
+			block->phys = (dma_addr_t)temp[0];
+			block->virt = NULL;
+			block->size = temp[1];
+		} else {
+			block->size = temp[1];
+			/* queue_base not specific => allocate requested size */
+			block->virt = dmam_alloc_coherent(kdev->dev,
+							  8 * block->size, &block->phys,
+							  GFP_KERNEL);
+			if (!block->virt) {
+				dev_err(kdev->dev, "failed to alloc linkram\n");
+				return -ENOMEM;
+			}
+		}
+	} else
+		return -ENODEV;
+	return 0;
+}
+
+static int khwq_setup_link_ram(struct khwq_device *kdev)
+{
+	struct khwq_link_ram_block *block;
+	struct khwq_qmgr_info *qmgr;
+
+	for_each_qmgr(kdev, qmgr) {
+		block = &kdev->link_rams[0];
+		dev_dbg(kdev->dev, "linkram0: phys:%x, virt:%p, size:%x\n",
+			block->phys, block->virt, block->size);
+		__raw_writel(block->phys, &qmgr->reg_config->link_ram_base0);
+		__raw_writel(block->size, &qmgr->reg_config->link_ram_size0);
+
+		block++;
+		if (!block->size)
+			return 0;
+
+		dev_dbg(kdev->dev, "linkram1: phys:%x, virt:%p, size:%x\n",
+			block->phys, block->virt, block->size);
+		__raw_writel(block->phys, &qmgr->reg_config->link_ram_base1);
+	}
+
+	return 0;
+}
+
+static int khwq_setup_queue_range(struct khwq_device *kdev,
+				 struct device_node *node)
+{
+	struct device *dev = kdev->dev;
+	struct khwq_range_info *range;
+	struct khwq_qmgr_info *qmgr;
+	u32 temp[2], start, end, id, index;
+	int ret, i;
+
+	range = devm_kzalloc(dev, sizeof(*range), GFP_KERNEL);
+	if (!range) {
+		dev_err(dev, "out of memory allocating range\n");
+		return -ENOMEM;
+	}
+
+	range->kdev = kdev;
+	range->name = khwq_find_name(node);
+
+	ret = of_property_read_u32_array(node, "values", temp, 2);
+	if (!ret) {
+		range->queue_base = temp[0] - kdev->base_id;
+		range->num_queues = temp[1];
+	} else {
+		dev_err(dev, "invalid queue range %s\n", range->name);
+		devm_kfree(dev, range);
+		return -EINVAL;
+	}
+
+	for (i = 0; i < RANGE_MAX_IRQS; i++) {
+		range->irqs[i] = irq_of_parse_and_map(node, i);
+		if (range->irqs[i] == IRQ_NONE)
+			break;
+		range->num_irqs++;
+	}
+	range->num_irqs = min(range->num_irqs, range->num_queues);
+	if (range->num_irqs)
+		range->flags |= RANGE_HAS_IRQ;
+
+	if (of_get_property(node, "reserved", NULL))
+		range->flags |= RANGE_RESERVED;
+
+	if (of_get_property(node, "accumulator", NULL)) {
+		ret = khwq_init_acc_range(kdev, node, range);
+		if (ret < 0) {
+			devm_kfree(dev, range);
+			return ret;
+		}
+	}
+
+	if (of_get_property(node, "qos-cfg", NULL)) {
+		ret = khwq_init_qos_range(kdev, node, range);
+		if (ret < 0) {
+			devm_kfree(dev, range);
+			return ret;
+		}
+	}
+
+	/* set threshold to 1, and flush out the queues */
+	for_each_qmgr(kdev, qmgr) {
+		start = max(qmgr->start_queue, range->queue_base);
+		end   = min(qmgr->start_queue + qmgr->num_queues,
+			    range->queue_base + range->num_queues);
+		for (id = start; id < end; id++) {
+			index = id - qmgr->start_queue;
+			__raw_writel(THRESH_GTE | 1,
+				     &qmgr->reg_peek[index].ptr_size_thresh);
+			__raw_writel(0, &qmgr->reg_push[index].ptr_size_thresh);
+		}
+	}
+
+	list_add_tail(&range->list, &kdev->queue_ranges);
+
+	dev_dbg(dev, "added range %s: %d-%d, %d irqs%s%s%s\n",
+		range->name, range->queue_base,
+		range->queue_base + range->num_queues - 1,
+		range->num_irqs,
+		(range->flags & RANGE_HAS_IRQ) ? ", has irq" : "",
+		(range->flags & RANGE_RESERVED) ? ", reserved" : "",
+		(range->flags & RANGE_HAS_ACCUMULATOR) ? ", acc" : "");
+
+	return 0;
+}
+
+static void khwq_free_queue_range(struct khwq_device *kdev,
+				  struct khwq_range_info *range)
+{
+	if (range->ops && range->ops->free_range)
+		range->ops->free_range(range);
+	list_del(&range->list);
+	devm_kfree(kdev->dev, range);
+}
+
+static int khwq_setup_queue_ranges(struct khwq_device *kdev,
+				  struct device_node *queues)
+{
+	struct device_node *child;
+	int ret;
+
+	for_each_child_of_node(queues, child) {
+		ret = khwq_setup_queue_range(kdev, child);
+		/* return value ignored, we init the rest... */
+	}
+
+	/* ... and barf if they all failed! */
+	if (list_empty(&kdev->queue_ranges)) {
+		dev_err(kdev->dev, "no valid queue range found\n");
+		return -ENODEV;
+	}
+
+	return 0;
+}
+
+static void khwq_free_queue_ranges(struct khwq_device *kdev)
+{
+	struct khwq_range_info *range;
+
+	for (;;) {
+		range = first_queue_range(kdev);
+		if (!range)
+			break;
+		khwq_free_queue_range(kdev, range);
+	}
+}
+
+static int khwq_setup_pools(struct khwq_device *kdev, struct device_node *pools)
+{
+	struct device *dev = kdev->dev;
+	struct khwq_pool_info *pool;
+	struct device_node *child;
+	u32 temp[2];
+	int ret;
+
+	for_each_child_of_node(pools, child) {
+
+		pool = devm_kzalloc(dev, sizeof(*pool), GFP_KERNEL);
+		if (!pool) {
+			dev_err(dev, "out of memory allocating pool\n");
+			return -ENOMEM;
+		}
+
+		pool->name = khwq_find_name(child);
+
+		ret = of_property_read_u32_array(child, "values", temp, 2);
+		if (!ret) {
+			pool->num_desc  = temp[0];
+			pool->desc_size = temp[1];
+		} else {
+			dev_err(dev, "invalid queue pool %s\n", pool->name);
+			devm_kfree(dev, pool);
+			continue;
+		}
+
+		ret = of_property_read_u32(child, "region-id", &pool->region_id);
+		if (ret < 0) {
+			dev_err(dev, "invalid region id for pool %s\n", pool->name);
+			devm_kfree(dev, pool);
+			continue;
+		}
+
+		list_add_tail(&pool->list, &kdev->pools);
+
+		dev_info(dev, "added pool %s: %d descriptors of size %d\n",
+			pool->name, pool->num_desc, pool->desc_size);
+	}
+
+	if (list_empty(&kdev->pools)) {
+		dev_err(dev, "no valid descriptor pool found\n");
+		return -ENODEV;
+	}
+
+	return 0;
+}
+
+static int khwq_init_qmgrs(struct khwq_device *kdev, struct device_node *qmgrs)
+{
+	struct device *dev = kdev->dev;
+	struct khwq_qmgr_info *qmgr;
+	struct device_node *child;
+	u32 temp[2];
+	int ret;
+
+	for_each_child_of_node(qmgrs, child) {
+		qmgr = devm_kzalloc(dev, sizeof(*qmgr), GFP_KERNEL);
+		if (!qmgr) {
+			dev_err(dev, "out of memory allocating qmgr\n");
+			return -ENOMEM;
+		}
+
+		ret = of_property_read_u32_array(child, "managed-queues",
+						 temp, 2);
+		if (!ret) {
+			qmgr->start_queue = temp[0];
+			qmgr->num_queues = temp[1];
+		} else {
+			dev_err(dev, "invalid qmgr queue range\n");
+			devm_kfree(dev, qmgr);
+			continue;
+		}
+
+		dev_info(dev, "qmgr start queue %d, number of queues %d\n",
+		       qmgr->start_queue, qmgr->num_queues);
+
+		qmgr->reg_peek		= of_iomap(child, 0);
+		qmgr->reg_status	= of_iomap(child, 1);
+		qmgr->reg_config	= of_iomap(child, 2);
+		qmgr->reg_region	= of_iomap(child, 3);
+		qmgr->reg_push		= of_iomap(child, 4);
+		qmgr->reg_pop		= of_iomap(child, 5);
+
+		if (!qmgr->reg_peek || !qmgr->reg_status || !qmgr->reg_config ||
+		    !qmgr->reg_region || !qmgr->reg_push || !qmgr->reg_pop) {
+			dev_err(dev, "failed to map qmgr regs\n");
+			if (qmgr->reg_peek)
+				iounmap(qmgr->reg_peek);
+			if (qmgr->reg_status)
+				iounmap(qmgr->reg_status);
+			if (qmgr->reg_config)
+				iounmap(qmgr->reg_config);
+			if (qmgr->reg_region)
+				iounmap(qmgr->reg_region);
+			if (qmgr->reg_push)
+				iounmap(qmgr->reg_push);
+			if (qmgr->reg_pop)
+				iounmap(qmgr->reg_pop);
+			kfree(qmgr);
+			continue;
+		}
+
+		list_add_tail(&qmgr->list, &kdev->qmgrs);
+
+		dev_info(dev, "added qmgr start queue %d, num of queues %d, "
+				"reg_peek %p, reg_status %p, reg_config %p, "
+				"reg_region %p, reg_push %p, reg_pop %p\n",
+				qmgr->start_queue, qmgr->num_queues,
+				qmgr->reg_peek, qmgr->reg_status,
+				qmgr->reg_config, qmgr->reg_region,
+				qmgr->reg_push, qmgr->reg_pop);
+	}
+
+	return 0;
+}
+
+static int khwq_init_pdsps(struct khwq_device *kdev, struct device_node *pdsps)
+{
+	struct device *dev = kdev->dev;
+	struct khwq_pdsp_info *pdsp;
+	struct device_node *child;
+	int ret;
+
+	for_each_child_of_node(pdsps, child) {
+
+		pdsp = devm_kzalloc(dev, sizeof(*pdsp), GFP_KERNEL);
+		if (!pdsp) {
+			dev_err(dev, "out of memory allocating pdsp\n");
+			return -ENOMEM;
+		}
+
+		pdsp->name = khwq_find_name(child);
+
+		ret = of_property_read_string(child, "firmware",
+					      &pdsp->firmware);
+		if (ret < 0 || !pdsp->firmware) {
+			dev_err(dev, "unknown firmware for pdsp %s\n",
+				pdsp->name);
+			kfree(pdsp);
+			continue;
+		}
+		dev_dbg(dev, "pdsp name %s fw name :%s\n",
+			pdsp->name, pdsp->firmware);
+
+		pdsp->iram	= of_iomap(child, 0);
+		pdsp->regs	= of_iomap(child, 1);
+		pdsp->intd	= of_iomap(child, 2);
+		pdsp->command	= of_iomap(child, 3);
+		if (!pdsp->command || !pdsp->iram || !pdsp->regs || !pdsp->intd) {
+			dev_err(dev, "failed to map pdsp %s regs\n",
+				pdsp->name);
+			if (pdsp->command)
+				devm_iounmap(dev, pdsp->command);
+			if (pdsp->iram)
+				devm_iounmap(dev, pdsp->iram);
+			if (pdsp->regs)
+				devm_iounmap(dev, pdsp->regs);
+			if (pdsp->intd)
+				devm_iounmap(dev, pdsp->intd);
+			kfree(pdsp);
+			continue;
+		}
+		of_property_read_u32(child, "id", &pdsp->id);
+
+		list_add_tail(&pdsp->list, &kdev->pdsps);
+
+		dev_dbg(dev, "added pdsp %s: command %p, iram %p, "
+			"regs %p, intd %p, firmware %s\n",
+			pdsp->name, pdsp->command, pdsp->iram, pdsp->regs,
+			pdsp->intd, pdsp->firmware);
+	}
+
+	return 0;
+}
+
+
+static int khwq_stop_pdsp(struct khwq_device *kdev,
+			  struct khwq_pdsp_info *pdsp)
+{
+	u32 val, timeout = 1000;
+	int ret;
+
+	val = __raw_readl(&pdsp->regs->control) & ~PDSP_CTRL_ENABLE;
+	__raw_writel(val, &pdsp->regs->control);
+
+	ret = khwq_pdsp_wait(&pdsp->regs->control, timeout, PDSP_CTRL_RUNNING);
+
+	if (ret < 0) {
+		dev_err(kdev->dev, "timed out on pdsp %s stop\n", pdsp->name);
+		return ret;
+	}
+	return 0;
+}
+
+static int khwq_load_pdsp(struct khwq_device *kdev,
+			  struct khwq_pdsp_info *pdsp)
+{
+	int i, ret, fwlen;
+	const struct firmware *fw;
+	u32 *fwdata;
+
+	ret = request_firmware(&fw, pdsp->firmware, kdev->dev);
+	if (ret) {
+		dev_err(kdev->dev, "failed to get firmware %s for pdsp %s\n",
+			pdsp->firmware, pdsp->name);
+		return ret;
+	}
+
+	__raw_writel(pdsp->id + 1, pdsp->command + 0x18);
+
+	/* download the firmware */
+	fwdata = (u32 *)fw->data;
+	fwlen = (fw->size + sizeof(u32) - 1) / sizeof(u32);
+
+	for (i = 0; i < fwlen; i++)
+		__raw_writel(be32_to_cpu(fwdata[i]), pdsp->iram + i);
+
+	release_firmware(fw);
+
+	return 0;
+}
+
+static int khwq_start_pdsp(struct khwq_device *kdev,
+			   struct khwq_pdsp_info *pdsp)
+{
+	u32 val, timeout = 1000;
+	int ret;
+
+	/* write a command for sync */
+	__raw_writel(0xffffffff, pdsp->command);
+	while (__raw_readl(pdsp->command) != 0xffffffff)
+		cpu_relax();
+
+	/* soft reset the PDSP */
+	val  = __raw_readl(&pdsp->regs->control);
+	val &= ~(PDSP_CTRL_PC_MASK | PDSP_CTRL_SOFT_RESET);
+	__raw_writel(val, &pdsp->regs->control);
+
+	/* enable pdsp */
+	val = __raw_readl(&pdsp->regs->control) | PDSP_CTRL_ENABLE;
+	__raw_writel(val, &pdsp->regs->control);
+
+	/* wait for command register to clear */
+	ret = khwq_pdsp_wait(pdsp->command, timeout, 0);
+
+	if (ret < 0) {
+		dev_err(kdev->dev, "timed out on pdsp %s command register wait\n",
+			pdsp->name);
+		return ret;
+	}
+	return 0;
+}
+
+static void khwq_stop_pdsps(struct khwq_device *kdev)
+{
+	struct khwq_pdsp_info *pdsp;
+
+	/* disable all pdsps */
+	for_each_pdsp(kdev, pdsp)
+		khwq_stop_pdsp(kdev, pdsp);
+}
+
+static int khwq_start_pdsps(struct khwq_device *kdev)
+{
+	struct khwq_pdsp_info *pdsp;
+	int ret;
+
+	khwq_stop_pdsps(kdev);
+
+	/* now load them all */
+	for_each_pdsp(kdev, pdsp) {
+		ret = khwq_load_pdsp(kdev, pdsp);
+		if (ret < 0)
+			return ret;
+	}
+
+	for_each_pdsp(kdev, pdsp) {
+		ret = khwq_start_pdsp(kdev, pdsp);
+		WARN_ON(ret);
+	}
+
+	return 0;
+}
+
+static int khwq_init_queue(struct khwq_device *kdev,
+			   struct khwq_range_info *range,
+			   struct hwqueue_instance *inst)
+{
+	struct khwq_instance *kq = hwqueue_inst_to_priv(inst);
+	unsigned id = hwqueue_inst_to_id(inst) - range->queue_base;
+
+	kq->kdev = kdev;
+	kq->range = range;
+	kq->irq_num = -1;
+
+	inst->ops = &range->inst_ops;
+
+	scnprintf(kq->irq_name, sizeof(kq->irq_name),
+		  "hwqueue-%d", range->queue_base + id);
+
+	if (range->ops && range->ops->init_queue)
+		return range->ops->init_queue(range, inst);
+	else
+		return 0;
+}
+
+static int khwq_init_queues(struct khwq_device *kdev)
+{
+	struct hwqueue_device *hdev = to_hdev(kdev);
+	struct khwq_range_info *range;
+	int id, ret;
+
+	for_each_queue_range(kdev, range) {
+		range->inst_ops = khdev_inst_ops;
+		if (range->ops && range->ops->init_range)
+			range->ops->init_range(range);
+		for (id = range->queue_base;
+		     id < range->queue_base + range->num_queues; id++) {
+			ret = khwq_init_queue(kdev, range,
+					      hwqueue_id_to_inst(hdev, id));
+			if (ret < 0)
+				return ret;
+		}
+	}
+	return 0;
+}
+
+static int khwq_probe(struct platform_device *pdev)
+{
+	struct device_node *node = pdev->dev.of_node;
+	struct device_node *qmgrs, *descs, *queues, *regions, *pdsps;
+	struct device *dev = &pdev->dev;
+	struct hwqueue_device *hdev;
+	struct khwq_device *kdev;
+	u32 temp[2];
+	int ret;
+
+	if (!node) {
+		dev_err(dev, "device tree info unavailable\n");
+		return -ENODEV;
+	}
+
+	kdev = devm_kzalloc(dev, sizeof(struct khwq_device), GFP_KERNEL);
+	if (!kdev) {
+		dev_err(dev, "memory allocation failed\n");
+		return -ENOMEM;
+	}
+
+	platform_set_drvdata(pdev, kdev);
+	kdev->dev = dev;
+	INIT_LIST_HEAD(&kdev->queue_ranges);
+	INIT_LIST_HEAD(&kdev->qmgrs);
+	INIT_LIST_HEAD(&kdev->pools);
+	INIT_LIST_HEAD(&kdev->regions);
+	INIT_LIST_HEAD(&kdev->pdsps);
+
+	if (of_property_read_u32_array(node, "range", temp, 2)) {
+		dev_err(dev, "hardware queue range not specified\n");
+		return -ENODEV;
+	}
+	kdev->base_id    = temp[0];
+	kdev->num_queues = temp[1];
+
+	qmgrs =  of_get_child_by_name(node, "qmgrs");
+	if (!qmgrs) {
+		dev_err(dev, "queue manager info not specified\n");
+		return -ENODEV;
+	}
+	/* Initialize queue managers using device tree configuration */
+	ret = khwq_init_qmgrs(kdev, qmgrs);
+	if (ret)
+		return ret;
+	of_node_put(qmgrs);
+
+	/*
+	 * TODO: failure handling in this code is somewhere between moronic
+	 * and non-existant - needs to be fixed
+	 */
+
+	pdsps =  of_get_child_by_name(node, "pdsps");
+	/* get pdsp configuration values from device tree */
+	if (pdsps) {
+		ret = khwq_init_pdsps(kdev, pdsps);
+		if (ret)
+			return ret;
+
+		ret = khwq_start_pdsps(kdev);
+		if (ret)
+			return ret;
+	}
+	of_node_put(pdsps);
+
+	queues = of_get_child_by_name(node, "queues");
+	if (!queues) {
+		dev_err(dev, "queues not specified\n");
+		return -ENODEV;
+	}
+	/* get usable queue range values from device tree */
+	ret = khwq_setup_queue_ranges(kdev, queues);
+	if (ret)
+		return ret;
+	of_node_put(queues);
+
+	descs =  of_get_child_by_name(node, "descriptors");
+	if (!descs) {
+		dev_err(dev, "descriptor pools not specified\n");
+		return -ENODEV;
+	}
+	/* Get descriptor pool values from device tree */
+	ret = khwq_setup_pools(kdev, descs);
+	if (ret) {
+		khwq_free_queue_ranges(kdev);
+		khwq_stop_pdsps(kdev);
+		return ret;
+	}
+	of_node_put(descs);
+
+	ret = khwq_get_link_ram(kdev, "linkram0", &kdev->link_rams[0]);
+	if (ret) {
+		dev_err(kdev->dev, "could not setup linking ram\n");
+		return ret;
+	}
+
+	ret = khwq_get_link_ram(kdev, "linkram1", &kdev->link_rams[1]);
+	if (ret) {
+		/*
+		 * nothing really, we have one linking ram already, so we just
+		 * live within our means
+		 */
+	}
+
+	ret = khwq_setup_link_ram(kdev);
+	if (ret)
+		return ret;
+
+	regions =  of_get_child_by_name(node, "regions");
+	if (!regions) {
+		dev_err(dev, "region table not specified\n");
+		return -ENODEV;
+	}
+	ret = khwq_setup_regions(kdev, regions);
+	if (ret)
+		return ret;
+	of_node_put(regions);
+
+	/* initialize hwqueue device data */
+	hdev = to_hdev(kdev);
+	hdev->dev	 = dev;
+	hdev->base_id	 = kdev->base_id;
+	hdev->num_queues = kdev->num_queues;
+	hdev->priv_size	 = sizeof(struct khwq_instance);
+	hdev->ops	 = &khdev_ops;
+
+	/* register the hwqueue device */
+	ret = hwqueue_device_register(hdev);
+	if (ret < 0) {
+		dev_err(dev, "hwqueue registration failed\n");
+		return ret;
+	}
+
+	ret = khwq_init_queues(kdev);
+	if (ret < 0) {
+		dev_err(dev, "hwqueue initialization failed\n");
+		return ret;
+	}
+
+	khwq_fill_pools(kdev);
+
+	return 0;
+}
+
+static int khwq_remove(struct platform_device *pdev)
+{
+	struct khwq_device *kdev = platform_get_drvdata(pdev);
+	struct hwqueue_device *hdev = to_hdev(kdev);
+	int ret;
+
+	ret = hwqueue_device_unregister(hdev);
+	if (ret < 0) {
+		dev_err(&pdev->dev, "hwqueue unregistration failed\n");
+		return ret;
+	}
+
+	return 0;
+}
+
+/* Match table for of_platform binding */
+static struct of_device_id keystone_hwqueue_of_match[] = {
+	{ .compatible = "ti,keystone-hwqueue", },
+	{},
+};
+MODULE_DEVICE_TABLE(of, keystone_hwqueue_of_match);
+
+static struct platform_driver keystone_hwqueue_driver = {
+	.probe		= khwq_probe,
+	.remove		= khwq_remove,
+	.driver		= {
+		.name	= "keystone-hwqueue",
+		.owner	= THIS_MODULE,
+		.of_match_table = keystone_hwqueue_of_match,
+	},
+};
+
+static int __init keystone_hwqueue_init(void)
+{
+	return platform_driver_register(&keystone_hwqueue_driver);
+}
+subsys_initcall(keystone_hwqueue_init);
+
+static void __exit keystone_hwqueue_exit(void)
+{
+	platform_driver_unregister(&keystone_hwqueue_driver);
+}
+module_exit(keystone_hwqueue_exit);
+
+MODULE_LICENSE("GPL v2");
+MODULE_DESCRIPTION("Hardware queue driver for Keystone devices");
diff --git a/drivers/hwqueue/keystone_hwqueue.h b/drivers/hwqueue/keystone_hwqueue.h
new file mode 100644
index 0000000..82d5314
--- /dev/null
+++ b/drivers/hwqueue/keystone_hwqueue.h
@@ -0,0 +1,309 @@
+/*
+ * Keystone hardware queue driver
+ *
+ * Copyright (C) 2011 Texas Instruments Incorporated - http://www.ti.com
+ * Contact: Prabhu Kuttiyam <pkuttiyam@ti.com>
+ *	    Cyril Chemparathy <cyril@ti.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ */
+
+#ifndef __KEYSTONE_HWQUEUE_H__
+#define __KEYSTONE_HWQUEUE_H__
+
+#define DESC_SIZE_MASK	0xful
+#define DESC_PTR_MASK	(~DESC_SIZE_MASK)
+
+#define BITS(x)		(BIT(x) - 1)
+
+#define THRESH_GTE	BIT(7)
+#define THRESH_LT	0
+
+#define PDSP_CTRL_PC_MASK	0xffff0000
+#define PDSP_CTRL_SOFT_RESET	BIT(0)
+#define PDSP_CTRL_ENABLE	BIT(1)
+#define PDSP_CTRL_RUNNING	BIT(15)
+
+#define ACC_MAX_CHANNEL		48
+#define ACC_DEFAULT_PERIOD		25 /* usecs */
+#define ACC_DESCS_MAX			SZ_1K
+#define ACC_DESCS_MASK		(ACC_DESCS_MAX - 1)
+#define ACC_CHANNEL_INT_BASE		2
+
+#define ACC_LIST_ENTRY_TYPE		1
+#define ACC_LIST_ENTRY_WORDS		(1 << ACC_LIST_ENTRY_TYPE)
+#define ACC_LIST_ENTRY_QUEUE_IDX	0
+#define ACC_LIST_ENTRY_DESC_IDX	(ACC_LIST_ENTRY_WORDS - 1)
+
+#define ACC_CMD_DISABLE_CHANNEL	0x80
+#define ACC_CMD_ENABLE_CHANNEL	0x81
+#define ACC_CFG_MULTI_QUEUE		BIT(21)
+
+#define ACC_INTD_OFFSET_EOI		(0x0010)
+#define ACC_INTD_OFFSET_COUNT(ch)	(0x0300 + 4 * (ch))
+#define ACC_INTD_OFFSET_STATUS(ch)	(0x0200 + 4 * ((ch) / 32))
+
+#define RANGE_MAX_IRQS			64
+
+enum khwq_acc_result {
+	ACC_RET_IDLE,
+	ACC_RET_SUCCESS,
+	ACC_RET_INVALID_COMMAND,
+	ACC_RET_INVALID_CHANNEL,
+	ACC_RET_INACTIVE_CHANNEL,
+	ACC_RET_ACTIVE_CHANNEL,
+	ACC_RET_INVALID_QUEUE,
+
+	ACC_RET_INVALID_RET,
+};
+
+struct khwq_reg_config {
+	u32		revision;
+	u32		__pad1;
+	u32		divert;
+	u32		link_ram_base0;
+	u32		link_ram_size0;
+	u32		link_ram_base1;
+	u32		__pad2[2];
+	u32		starvation[0];
+};
+
+struct khwq_reg_region {
+	u32		base;
+	u32		start_index;
+	u32		size_count;
+	u32		__pad;
+};
+
+struct khwq_reg_queue {
+	u32		entry_count;
+	u32		byte_count;
+	u32		packet_size;
+	u32		ptr_size_thresh;
+};
+
+struct khwq_reg_pdsp_regs {
+	u32		control;
+	u32		status;
+	u32		cycle_count;
+	u32		stall_count;
+};
+
+struct khwq_reg_acc_command {
+	u32		command;
+	u32		queue_mask;
+	u32		list_phys;
+	u32		queue_num;
+	u32		timer_config;
+};
+
+struct khwq_region {
+	const char	*name;
+	unsigned	 used_desc;
+	struct list_head list;
+	unsigned	 id;
+	unsigned	 desc_size;
+	unsigned	 num_desc;
+	unsigned	 fixed_mem;
+	dma_addr_t	 next_pool_addr;
+	dma_addr_t	 dma_start, dma_end;
+	void		*virt_start, *virt_end;
+	unsigned	 link_index;
+};
+
+struct khwq_pool_info {
+	const char		*name;
+	struct khwq_region	*region;
+	int			 region_offset;
+	int			 num_desc;
+	int			 desc_size;
+	int			 region_id;
+	struct hwqueue		*queue;
+	struct list_head	 list;
+};
+
+struct khwq_link_ram_block {
+	dma_addr_t	 phys;
+	void		*virt;
+	size_t		 size;
+};
+
+struct khwq_acc_info {
+	u32			 pdsp_id;
+	u32			 start_channel;
+	u32			 list_entries;
+	u32			 pacing_mode;
+	u32			 timer_count;
+	int			 mem_size;
+	int			 list_size;
+	struct khwq_pdsp_info	*pdsp;
+};
+
+struct khwq_acc_channel {
+	u32			 channel;
+	u32			 list_index;
+	u32			 open_mask;
+	u32			*list_cpu[2];
+	dma_addr_t		 list_dma[2];
+	char			 name[32];
+	atomic_t		 retrigger_count;
+};
+
+struct khwq_pdsp_info {
+	const char				*name;
+	struct khwq_reg_pdsp_regs  __iomem	*regs;
+	union {
+		void __iomem				*command;
+		struct khwq_reg_acc_command __iomem	*acc_command;
+		u32 __iomem				*qos_command;
+	};
+	void __iomem				*intd;
+	u32 __iomem				*iram;
+	const char				*firmware;
+	u32					 id;
+	struct list_head			 list;
+	struct khwq_qos_info			*qos_info;
+};
+
+struct khwq_range_ops;
+
+struct khwq_range_info {
+	const char		*name;
+	struct khwq_device	*kdev;
+	unsigned		 queue_base;
+	unsigned		 num_queues;
+	unsigned		 flags;
+	struct list_head	 list;
+	struct khwq_range_ops	*ops;
+	struct hwqueue_inst_ops	 inst_ops;
+	struct khwq_acc_info	 acc_info;
+	struct khwq_acc_channel	*acc;
+	struct khwq_qos_info	*qos_info;
+	unsigned		 num_irqs;
+	int			 irqs[RANGE_MAX_IRQS];
+};
+
+struct khwq_range_ops {
+	int	(*init_range)(struct khwq_range_info *range);
+	int	(*free_range)(struct khwq_range_info *range);
+	int	(*init_queue)(struct khwq_range_info *range,
+			      struct hwqueue_instance *inst);
+	int	(*open_queue)(struct khwq_range_info *range,
+			      struct hwqueue_instance *inst, unsigned flags);
+	int	(*close_queue)(struct khwq_range_info *range,
+			       struct hwqueue_instance *inst);
+	int	(*set_notify)(struct khwq_range_info *range,
+			      struct hwqueue_instance *inst, bool enabled);
+};
+
+#define RANGE_RESERVED		BIT(0)
+#define RANGE_HAS_IRQ		BIT(1)
+#define RANGE_HAS_ACCUMULATOR	BIT(2)
+#define RANGE_MULTI_QUEUE	BIT(3)
+
+struct khwq_qmgr_info {
+	unsigned			 start_queue;
+	unsigned			 num_queues;
+	struct khwq_reg_config __iomem	*reg_config;
+	struct khwq_reg_region __iomem	*reg_region;
+	struct khwq_reg_queue __iomem	*reg_push, *reg_pop, *reg_peek;
+	void __iomem			*reg_status;
+	struct list_head		 list;
+};
+
+struct khwq_device {
+	struct device			*dev;
+	struct hwqueue_device		 hdev;
+	struct list_head		 regions;
+	struct khwq_link_ram_block	 link_rams[2];
+	unsigned			 num_queues;
+	unsigned			 base_id;
+	struct list_head		 queue_ranges;
+	struct list_head		 pools;
+	struct list_head		 pdsps;
+	struct list_head		 qmgrs;
+};
+
+struct khwq_desc {
+	u32			 val;
+	unsigned		 size;
+	struct list_head	 list;
+};
+
+struct khwq_instance {
+	struct khwq_device	*kdev;
+	struct khwq_range_info	*range;
+	u32			*descs;
+	atomic_t		 desc_head, desc_tail, desc_count;
+	struct khwq_acc_channel	*acc;
+	struct khwq_region	*last; /* cache last region used */
+	struct khwq_qmgr_info	*qmgr; /* cache qmgr for the instance */
+	int			 irq_num; /*irq num -ve for non-irq queues */
+	char			 irq_name[32];
+};
+
+#define to_hdev(_kdev)		(&(_kdev)->hdev)
+#define from_hdev(_hdev)	container_of(_hdev, struct khwq_device, hdev)
+
+#define for_each_region(kdev, region)				\
+	list_for_each_entry(region, &kdev->regions, list)
+
+#define for_each_queue_range(kdev, range)			\
+	list_for_each_entry(range, &kdev->queue_ranges, list)
+
+#define first_queue_range(kdev)					\
+	list_first_entry(&kdev->queue_ranges, struct khwq_range_info, list)
+
+#define for_each_pool(kdev, pool)				\
+	list_for_each_entry(pool, &kdev->pools, list)
+
+#define for_each_pdsp(kdev, pdsp)				\
+	list_for_each_entry(pdsp, &kdev->pdsps, list)
+
+#define for_each_qmgr(kdev, qmgr)				\
+	list_for_each_entry(qmgr, &kdev->qmgrs, list)
+
+#define for_each_policy(info, policy)				\
+	list_for_each_entry(policy, &info->drop_policies, list)
+
+static inline struct khwq_pdsp_info *
+khwq_find_pdsp(struct khwq_device *kdev, unsigned pdsp_id)
+{
+	struct khwq_pdsp_info *pdsp;
+
+	for_each_pdsp(kdev, pdsp)
+		if (pdsp_id == pdsp->id)
+			return pdsp;
+	return NULL;
+}
+
+static inline struct khwq_qmgr_info *
+khwq_find_qmgr(struct hwqueue_instance *inst)
+{
+	struct khwq_device *kdev = from_hdev(inst->hdev);
+	unsigned id = hwqueue_inst_to_id(inst);
+	struct khwq_qmgr_info *qmgr;
+
+	for_each_qmgr(kdev, qmgr) {
+		if ((id >= qmgr->start_queue) &&
+			(id < qmgr->start_queue + qmgr->num_queues))
+			return qmgr;
+	}
+
+	return NULL;
+}
+
+int khwq_init_acc_range(struct khwq_device *kdev, struct device_node *node,
+			struct khwq_range_info *range);
+
+int khwq_init_qos_range(struct khwq_device *kdev, struct device_node *node,
+			struct khwq_range_info *range);
+
+#endif /* __KEYSTONE_HWQUEUE_H__ */
diff --git a/drivers/hwqueue/keystone_hwqueue_acc.c b/drivers/hwqueue/keystone_hwqueue_acc.c
new file mode 100644
index 0000000..bf7d963
--- /dev/null
+++ b/drivers/hwqueue/keystone_hwqueue_acc.c
@@ -0,0 +1,579 @@
+/*
+ * Keystone hardware queue driver
+ *
+ * Copyright (C) 2011 Texas Instruments Incorporated - http://www.ti.com
+ * Contact: Prabhu Kuttiyam <pkuttiyam@ti.com>
+ *	    Cyril Chemparathy <cyril@ti.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/device.h>
+#include <linux/io.h>
+#include <linux/bitops.h>
+#include <linux/slab.h>
+#include <linux/spinlock.h>
+#include <linux/hwqueue.h>
+#include <linux/platform_device.h>
+#include <linux/dma-mapping.h>
+#include <linux/of.h>
+#include <linux/of_device.h>
+#include <linux/of_address.h>
+#include <linux/firmware.h>
+#include <linux/interrupt.h>
+
+#include "hwqueue_internal.h"
+#include "keystone_hwqueue.h"
+
+static void __khwq_acc_notify(struct khwq_range_info *range,
+			      struct khwq_acc_channel *acc)
+{
+	struct khwq_device *kdev = range->kdev;
+	struct hwqueue_device *hdev = to_hdev(kdev);
+	struct hwqueue_instance *inst;
+	int range_base, queue;
+
+	range_base = kdev->base_id + range->queue_base;
+
+	if (range->flags & RANGE_MULTI_QUEUE) {
+		for (queue = 0; queue < range->num_queues; queue++) {
+			inst = hwqueue_id_to_inst(hdev, range_base + queue);
+			dev_dbg(kdev->dev, "acc-irq: notifying %d\n",
+				range_base + queue);
+			hwqueue_notify(inst);
+		}
+	} else {
+		queue = acc->channel - range->acc_info.start_channel;
+		inst = hwqueue_id_to_inst(hdev, range_base + queue);
+		dev_dbg(kdev->dev, "acc-irq: notifying %d\n",
+			range_base + queue);
+		hwqueue_notify(inst);
+	}
+}
+
+static int khwq_acc_set_notify(struct khwq_range_info *range,
+			       struct hwqueue_instance *inst,
+			       bool enabled)
+{
+	struct khwq_pdsp_info *pdsp = range->acc_info.pdsp;
+	struct khwq_instance *kq = hwqueue_inst_to_priv(inst);
+	struct khwq_device *kdev = range->kdev;
+	u32 mask, offset;
+
+	/*
+	 * when enabling, we need to re-trigger an interrupt if we
+	 * have descriptors pending
+	 */
+	if (!enabled || atomic_read(&kq->desc_count) <= 0)
+		return 0;
+
+	atomic_inc(&kq->acc->retrigger_count);
+	mask = BIT(kq->acc->channel % 32);
+	offset = ACC_INTD_OFFSET_STATUS(kq->acc->channel);
+
+	dev_dbg(kdev->dev, "setup-notify: re-triggering irq for %s\n",
+		kq->acc->name);
+	__raw_writel(mask, pdsp->intd + offset);
+
+	return 0;
+}
+
+static irqreturn_t khwq_acc_int_handler(int irq, void *_instdata)
+{
+	struct hwqueue_instance *inst = NULL;
+	struct khwq_acc_channel *acc;
+	struct khwq_instance *kq = NULL;
+	struct khwq_range_info *range;
+	struct hwqueue_device *hdev;
+	struct khwq_pdsp_info *pdsp;
+	struct khwq_acc_info *info;
+	struct khwq_device *kdev;
+
+	u32 *list, *list_cpu, val, idx, notifies;
+	int range_base, channel, queue = 0;
+	dma_addr_t list_dma;
+
+	range = _instdata;
+	info  = &range->acc_info;
+	kdev  = range->kdev;
+	hdev  = to_hdev(kdev);
+	pdsp  = range->acc_info.pdsp;
+	acc   = range->acc;
+
+	range_base = kdev->base_id + range->queue_base;
+
+	if ((range->flags & RANGE_MULTI_QUEUE) == 0) {
+		/* TODO: this needs extent checks */
+		for (queue = 0; queue < range->num_irqs; queue++)
+			if (range->irqs[queue] == irq)
+				break;
+		inst	 = hwqueue_id_to_inst(hdev, range_base + queue);
+		kq	 = hwqueue_inst_to_priv(inst);
+		acc	+= queue;
+	}
+
+	channel = acc->channel;
+	list_dma = acc->list_dma[acc->list_index];
+	list_cpu = acc->list_cpu[acc->list_index];
+
+	dev_dbg(kdev->dev, "acc-irq: channel %d, list %d, virt %p, phys %x\n",
+		channel, acc->list_index, list_cpu, list_dma);
+
+	if (atomic_read(&acc->retrigger_count)) {
+		atomic_dec(&acc->retrigger_count);
+		__khwq_acc_notify(range, acc);
+
+		__raw_writel(1, pdsp->intd + ACC_INTD_OFFSET_COUNT(channel));
+
+		/* ack the interrupt */
+		__raw_writel(ACC_CHANNEL_INT_BASE + channel,
+			     pdsp->intd + ACC_INTD_OFFSET_EOI);
+
+		return IRQ_HANDLED;
+	}
+
+	notifies = __raw_readl(pdsp->intd + ACC_INTD_OFFSET_COUNT(channel));
+	WARN_ON(!notifies);
+
+	dma_sync_single_for_cpu(kdev->dev, list_dma, info->list_size, DMA_FROM_DEVICE);
+
+	for (list = list_cpu; list < list_cpu + (info->list_size / sizeof(u32));
+	     list += ACC_LIST_ENTRY_WORDS) {
+
+		if (ACC_LIST_ENTRY_WORDS == 1) {
+			dev_dbg(kdev->dev, "acc-irq: list %d, entry @%p, "
+				"%08x\n",
+				acc->list_index, list, list[0]);
+		} else if (ACC_LIST_ENTRY_WORDS == 2) {
+			dev_dbg(kdev->dev, "acc-irq: list %d, entry @%p, "
+				"%08x %08x\n",
+				acc->list_index, list, list[0], list[1]);
+		} else if (ACC_LIST_ENTRY_WORDS == 4) {
+			dev_dbg(kdev->dev, "acc-irq: list %d, entry @%p, "
+				"%08x %08x %08x %08x\n",
+				acc->list_index, list,
+				list[0], list[1], list[2], list[3]);
+		}
+
+		val = list[ACC_LIST_ENTRY_DESC_IDX];
+
+		if (!val)
+			break;
+
+		if (range->flags & RANGE_MULTI_QUEUE) {
+			queue = list[ACC_LIST_ENTRY_QUEUE_IDX] >> 16;
+			if (queue < range_base || queue >= range_base + range->num_queues) {
+				dev_err(kdev->dev, "bad queue %d, expecting %d-%d\n",
+					queue, range_base, range_base + range->num_queues);
+				break;
+			}
+			queue -= range_base;
+			inst = hwqueue_id_to_inst(hdev, range_base + queue);
+			kq = hwqueue_inst_to_priv(inst);
+		}
+
+		if (atomic_inc_return(&kq->desc_count) >= ACC_DESCS_MAX) {
+			atomic_dec(&kq->desc_count);
+			/* TODO: need a statistics counter for such drops */
+			continue;
+		}
+
+		idx = atomic_inc_return(&kq->desc_tail) & ACC_DESCS_MASK;
+		kq->descs[idx] = val;
+		dev_dbg(kdev->dev, "acc-irq: enqueue %08x at %d, queue %d\n",
+			val, idx, queue + range_base);
+	}
+
+	__khwq_acc_notify(range, acc);
+
+	memset(list_cpu, 0, info->list_size);
+	dma_sync_single_for_device(kdev->dev, list_dma, info->list_size,
+				   DMA_TO_DEVICE);
+
+	/* flip to the other list */
+	acc->list_index ^= 1;
+
+	/* reset the interrupt counter */
+	__raw_writel(1, pdsp->intd + ACC_INTD_OFFSET_COUNT(channel));
+
+	/* ack the interrupt */
+	__raw_writel(ACC_CHANNEL_INT_BASE + channel,
+		     pdsp->intd + ACC_INTD_OFFSET_EOI);
+
+	return IRQ_HANDLED;
+}
+
+int khwq_range_setup_acc_irq(struct khwq_range_info *range, int queue,
+			     bool enabled)
+{
+	struct khwq_device *kdev = range->kdev;
+	struct khwq_acc_channel *acc;
+	int ret = 0, irq;
+	u32 old, new;
+
+	if (range->flags & RANGE_MULTI_QUEUE) {
+		acc = range->acc;
+		irq = range->irqs[0];
+	} else {
+		acc = range->acc + queue;
+		irq = range->irqs[queue];
+	}
+
+	old = acc->open_mask;
+	if (enabled)
+		new = old | BIT(queue);
+	else
+		new = old & ~BIT(queue);
+	acc->open_mask = new;
+
+	dev_dbg(kdev->dev, "setup-acc-irq: open mask old %08x, new %08x, channel %s\n",
+		old, new, acc->name);
+
+	if (likely(new == old))
+		return 0;
+
+	if (new && !old) {
+		dev_dbg(kdev->dev, "setup-acc-irq: requesting %s for channel %s\n",
+			acc->name, acc->name);
+		ret = request_irq(irq, khwq_acc_int_handler, 0, acc->name,
+				  range);
+	}
+
+	if (old && !new) {
+		dev_dbg(kdev->dev, "setup-acc-irq: freeing %s for channel %s\n",
+			acc->name, acc->name);
+		free_irq(irq, range);
+	}
+
+	return ret;
+}
+
+static const char *khwq_acc_result_str(enum khwq_acc_result result)
+{
+	static const char *result_str[] = {
+		[ACC_RET_IDLE]			= "idle",
+		[ACC_RET_SUCCESS]		= "success",
+		[ACC_RET_INVALID_COMMAND]	= "invalid command",
+		[ACC_RET_INVALID_CHANNEL]	= "invalid channel",
+		[ACC_RET_INACTIVE_CHANNEL]	= "inactive channel",
+		[ACC_RET_ACTIVE_CHANNEL]	= "active channel",
+		[ACC_RET_INVALID_QUEUE]		= "invalid queue",
+
+		[ACC_RET_INVALID_RET]		= "invalid return code",
+	};
+
+	if (result >= ARRAY_SIZE(result_str))
+		return result_str[ACC_RET_INVALID_RET];
+	else
+		return result_str[result];
+}
+
+static enum khwq_acc_result
+khwq_acc_write(struct khwq_device *kdev, struct khwq_pdsp_info *pdsp,
+	       struct khwq_reg_acc_command *cmd)
+{
+	u32 result;
+
+	/* TODO: acquire hwspinlock here */
+
+	dev_dbg(kdev->dev, "acc command %08x %08x %08x %08x %08x\n",
+		cmd->command, cmd->queue_mask, cmd->list_phys,
+		cmd->queue_num, cmd->timer_config);
+
+	__raw_writel(cmd->timer_config,	&pdsp->acc_command->timer_config);
+	__raw_writel(cmd->queue_num,	&pdsp->acc_command->queue_num);
+	__raw_writel(cmd->list_phys,	&pdsp->acc_command->list_phys);
+	__raw_writel(cmd->queue_mask,	&pdsp->acc_command->queue_mask);
+	__raw_writel(cmd->command,	&pdsp->acc_command->command);
+
+	/* wait for the command to clear */
+	do {
+		result = __raw_readl(&pdsp->acc_command->command);
+	} while ((result >> 8) & 0xff);
+
+	/* TODO: release hwspinlock here */
+
+	return (result >> 24) & 0xff;
+}
+
+static void khwq_acc_setup_cmd(struct khwq_device *kdev,
+			       struct khwq_range_info *range,
+			       struct khwq_reg_acc_command *cmd,
+			       int queue)
+{
+	struct khwq_acc_info *info = &range->acc_info;
+	struct khwq_acc_channel *acc;
+	int queue_base;
+	u32 queue_mask;
+
+	if (range->flags & RANGE_MULTI_QUEUE) {
+		acc = range->acc;
+		queue_base = range->queue_base;
+		queue_mask = BIT(range->num_queues) - 1;
+	} else {
+		acc = range->acc + queue;
+		queue_base = range->queue_base + queue;
+		queue_mask = 0;
+	}
+
+	memset(cmd, 0, sizeof(*cmd));
+	cmd->command    = acc->channel;
+	cmd->queue_mask = queue_mask;
+	cmd->list_phys  = acc->list_dma[0];
+	cmd->queue_num  = info->list_entries << 16;
+	cmd->queue_num |= queue_base;
+
+	cmd->timer_config = ACC_LIST_ENTRY_TYPE << 18;
+	if (range->flags & RANGE_MULTI_QUEUE)
+		cmd->timer_config |= ACC_CFG_MULTI_QUEUE;
+	cmd->timer_config |= info->pacing_mode << 16;
+	cmd->timer_config |= info->timer_count;
+}
+
+static void khwq_acc_stop(struct khwq_device *kdev,
+			  struct khwq_range_info *range,
+			  int queue)
+{
+	struct khwq_reg_acc_command cmd;
+	struct khwq_acc_channel *acc;
+	enum khwq_acc_result result;
+
+	acc = range->acc + queue;
+
+	khwq_acc_setup_cmd(kdev, range, &cmd, queue);
+	cmd.command |= ACC_CMD_DISABLE_CHANNEL << 8;
+	result = khwq_acc_write(kdev, range->acc_info.pdsp, &cmd);
+
+	dev_dbg(kdev->dev, "stopped acc channel %s, result %s\n",
+		acc->name, khwq_acc_result_str(result));
+}
+
+static enum khwq_acc_result khwq_acc_start(struct khwq_device *kdev,
+					   struct khwq_range_info *range,
+					   int queue)
+{
+	struct khwq_reg_acc_command cmd;
+	struct khwq_acc_channel *acc;
+	enum khwq_acc_result result;
+
+	acc = range->acc + queue;
+
+	khwq_acc_setup_cmd(kdev, range, &cmd, queue);
+	cmd.command |= ACC_CMD_ENABLE_CHANNEL << 8;
+	result = khwq_acc_write(kdev, range->acc_info.pdsp, &cmd);
+
+	dev_dbg(kdev->dev, "started acc channel %s, result %s\n",
+		acc->name, khwq_acc_result_str(result));
+
+	return result;
+}
+
+static int khwq_acc_init_range(struct khwq_range_info *range)
+{
+	struct khwq_device *kdev = range->kdev;
+	struct khwq_acc_channel *acc;
+	enum khwq_acc_result result;
+	int queue;
+
+	for (queue = 0; queue < range->num_queues; queue++) {
+		acc = range->acc + queue;
+
+		khwq_acc_stop(kdev, range, queue);
+		acc->list_index = 0;
+		result = khwq_acc_start(kdev, range, queue);
+
+		if (result != ACC_RET_SUCCESS)
+			return -EIO;
+
+		if (range->flags & RANGE_MULTI_QUEUE)
+			return 0;
+	}
+	return 0;
+}
+
+static int khwq_acc_init_queue(struct khwq_range_info *range,
+			       struct hwqueue_instance *inst)
+{
+	struct khwq_instance *kq = hwqueue_inst_to_priv(inst);
+	unsigned id = hwqueue_inst_to_id(inst) - range->queue_base;
+
+	kq->descs = kzalloc(ACC_DESCS_MAX * sizeof(u32), GFP_KERNEL);
+	if (!kq->descs)
+		return -ENOMEM;
+
+	kq->acc = range->acc;
+	if ((range->flags & RANGE_MULTI_QUEUE) == 0)
+		kq->acc += id;
+	return 0;
+}
+
+static int khwq_acc_open_queue(struct khwq_range_info *range,
+			       struct hwqueue_instance *inst, unsigned flags)
+{
+	unsigned id = hwqueue_inst_to_id(inst) - range->queue_base;
+
+	return khwq_range_setup_acc_irq(range, id, true);
+}
+
+static int khwq_acc_close_queue(struct khwq_range_info *range,
+				struct hwqueue_instance *inst)
+{
+	unsigned id = hwqueue_inst_to_id(inst) - range->queue_base;
+
+	return khwq_range_setup_acc_irq(range, id, false);
+}
+
+static int khwq_acc_free_range(struct khwq_range_info *range)
+{
+	struct khwq_device *kdev = range->kdev;
+	struct khwq_acc_channel *acc;
+	struct khwq_acc_info *info;
+	int channel, channels;
+
+	info = &range->acc_info;
+
+	if (range->flags & RANGE_MULTI_QUEUE)
+		channels = 1;
+	else
+		channels = range->num_queues;
+
+	for (channel = 0; channel < channels; channel++) {
+		acc = range->acc + channel;
+		if (!acc->list_cpu[0])
+			continue;
+		dma_unmap_single(kdev->dev, acc->list_dma[0],
+				 info->mem_size, DMA_BIDIRECTIONAL);
+		free_pages_exact(acc->list_cpu[0], info->mem_size);
+	}
+	kfree(range->acc);
+	return 0;
+}
+
+struct khwq_range_ops khwq_acc_range_ops = {
+	.set_notify	= khwq_acc_set_notify,
+	.init_queue	= khwq_acc_init_queue,
+	.open_queue	= khwq_acc_open_queue,
+	.close_queue	= khwq_acc_close_queue,
+	.init_range	= khwq_acc_init_range,
+	.free_range	= khwq_acc_free_range,
+};
+
+int khwq_init_acc_range(struct khwq_device *kdev, struct device_node *node,
+			struct khwq_range_info *range)
+{
+	struct khwq_acc_channel *acc;
+	struct khwq_pdsp_info *pdsp;
+	struct khwq_acc_info *info;
+	int ret, channel, channels;
+	int list_size, mem_size;
+	dma_addr_t list_dma;
+	void *list_mem;
+	u32 config[5];
+
+	range->flags |= RANGE_HAS_ACCUMULATOR;
+	info = &range->acc_info;
+
+	ret = of_property_read_u32_array(node, "accumulator", config, 5);
+	if (ret)
+		return ret;
+
+	info->pdsp_id		= config[0];
+	info->start_channel	= config[1];
+	info->list_entries	= config[2];
+	info->pacing_mode	= config[3];
+	info->timer_count	= config[4] / ACC_DEFAULT_PERIOD;
+
+	if (info->start_channel > ACC_MAX_CHANNEL) {
+		dev_err(kdev->dev, "channel %d invalid for range %s\n",
+			info->start_channel, range->name);
+		return -EINVAL;
+	}
+
+	if (info->pacing_mode > 3) {
+		dev_err(kdev->dev, "pacing mode %d invalid for range %s\n",
+			info->pacing_mode, range->name);
+		return -EINVAL;
+	}
+
+	pdsp = khwq_find_pdsp(kdev, info->pdsp_id);
+	if (!pdsp) {
+		dev_err(kdev->dev, "pdsp id %d not found for range %s\n",
+			info->pdsp_id, range->name);
+		return -EINVAL;
+	}
+	info->pdsp = pdsp;
+
+	channels = range->num_queues;
+
+	if (of_get_property(node, "multi-queue", NULL)) {
+		range->flags |= RANGE_MULTI_QUEUE;
+		channels = 1;
+		if (range->queue_base & (32 - 1)) {
+			dev_err(kdev->dev,
+				"misaligned multi-queue accumulator range %s\n",
+				range->name);
+			return -EINVAL;
+		}
+		if (range->num_queues > 32) {
+			dev_err(kdev->dev,
+				"too many queues in accumulator range %s\n",
+				range->name);
+			return -EINVAL;
+		}
+	}
+
+	/* figure out list size */
+	list_size  = info->list_entries;
+	list_size *= ACC_LIST_ENTRY_WORDS * sizeof(u32);
+	info->list_size = list_size;
+
+	mem_size   = PAGE_ALIGN(list_size * 2);
+	info->mem_size  = mem_size;
+
+	range->acc = kzalloc(channels * sizeof(*range->acc), GFP_KERNEL);
+	if (!range->acc)
+		return -ENOMEM;
+
+	for (channel = 0; channel < channels; channel++) {
+		acc = range->acc + channel;
+		acc->channel = info->start_channel + channel;
+
+		/* allocate memory for the two lists */
+		list_mem = alloc_pages_exact(mem_size, GFP_KERNEL | GFP_DMA);
+		if (!list_mem)
+			return -ENOMEM;
+
+		list_dma = dma_map_single(kdev->dev, list_mem, mem_size,
+					  DMA_BIDIRECTIONAL);
+		if (dma_mapping_error(kdev->dev, list_dma)) {
+			free_pages_exact(list_mem, mem_size);
+			return -ENOMEM;
+		}
+
+		memset(list_mem, 0, mem_size);
+		dma_sync_single_for_device(kdev->dev, list_dma, mem_size, DMA_TO_DEVICE);
+
+		scnprintf(acc->name, sizeof(acc->name), "hwqueue-acc-%d", acc->channel);
+
+		acc->list_cpu[0] = list_mem;
+		acc->list_cpu[1] = list_mem + list_size;
+		acc->list_dma[0] = list_dma;
+		acc->list_dma[1] = list_dma + list_size;
+
+		dev_dbg(kdev->dev, "%s: channel %d, phys %08x, virt %8p\n",
+			acc->name, acc->channel, list_dma, list_mem);
+	}
+
+	range->ops = &khwq_acc_range_ops;
+
+	return 0;
+}
diff --git a/drivers/hwqueue/keystone_hwqueue_qos.c b/drivers/hwqueue/keystone_hwqueue_qos.c
new file mode 100644
index 0000000..4ddbf10
--- /dev/null
+++ b/drivers/hwqueue/keystone_hwqueue_qos.c
@@ -0,0 +1,3517 @@
+/*
+ * Keystone hardware queue driver
+ *
+ * Copyright (C) 2011 Texas Instruments Incorporated - http://www.ti.com
+ * Contact: Prabhu Kuttiyam <pkuttiyam@ti.com>
+ *	    Cyril Chemparathy <cyril@ti.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/device.h>
+#include <linux/io.h>
+#include <linux/bitops.h>
+#include <linux/slab.h>
+#include <linux/delay.h>
+#include <linux/debugfs.h>
+#include <linux/spinlock.h>
+#include <linux/hwqueue.h>
+#include <linux/ktree.h>
+#include <linux/platform_device.h>
+#include <linux/dma-mapping.h>
+#include <linux/of.h>
+#include <linux/of_device.h>
+#include <linux/of_address.h>
+#include <linux/firmware.h>
+#include <linux/interrupt.h>
+
+#include "hwqueue_internal.h"
+#include "keystone_hwqueue.h"
+#include "keystone_qos.h"
+
+#define	KHWQ_QOS_TIMER_INTERVAL	(HZ * 10)
+#define BITMASK(s, n)		(BITS(n) << (s))
+
+static int khwq_qos_write_cmd(struct khwq_qos_info *info, u32 cmd)
+{
+	struct khwq_pdsp_info *pdsp;
+	struct khwq_device *kdev;
+	unsigned long timeo;
+	u32 result;
+
+	pdsp = info->pdsp;
+	kdev = info->kdev;
+
+	dev_dbg(kdev->dev, "=== command <-- %08x\n", cmd);
+	__raw_writel(cmd, &pdsp->qos_command[0]);
+
+	timeo = jiffies + msecs_to_jiffies(QOS_COMMAND_TIMEOUT);
+	do {
+		result = __raw_readl(&pdsp->qos_command[0]);
+		udelay(QOS_COMMAND_DELAY);
+	} while ((result & 0xff) && time_before(jiffies, timeo));
+
+	if (result & 0xff) {
+		dev_err(kdev->dev, "=== result --> lockup [%x]!\n",
+			result);
+		return -EBUSY;
+	}
+
+	result = __raw_readl(&pdsp->qos_command[1]);
+
+	if (result != QOS_RETCODE_SUCCESS) {
+		dev_err(kdev->dev, "=== result --> %08x ! failed\n", result);
+		return -EIO;
+	} else {
+		dev_dbg(kdev->dev, "=== result --> %08x\n", result);
+		return 0;
+	}
+}
+
+static void khwq_qos_dump_words(struct khwq_device *kdev, u32 ofs, u32 *data,
+				int words, const char *dir)
+{
+	switch (words) {
+	case 4: dev_dbg(kdev->dev, "==== [%04x] %s %08x %08x %08x %08x\n",
+			ofs, dir, data[0], data[1], data[2], data[3]);
+		break;
+	case 3: dev_dbg(kdev->dev, "==== [%04x] %s %08x %08x %08x\n",
+			ofs, dir, data[0], data[1], data[2]);
+		break;
+	case 2: dev_dbg(kdev->dev, "==== [%04x] %s %08x %08x\n",
+			ofs, dir, data[0], data[1]);
+		break;
+	case 1: dev_dbg(kdev->dev, "==== [%04x] %s %08x\n",
+			ofs, dir, data[0]);
+	case 0:
+		break;
+	default:
+		BUG();
+		break;
+	}
+}
+
+static void khwq_qos_write_words(struct khwq_device *kdev, u32 ofs,
+				 u32 __iomem *out, u32 *in, int words)
+{
+	int i;
+	khwq_qos_dump_words(kdev, ofs, in, words, "<--");
+	for (i = 0; i < words; i++)
+		__raw_writel(in[i], &out[i]);
+}
+
+static void khwq_qos_read_words(struct khwq_device *kdev, u32 ofs, u32 *out,
+				u32 __iomem *in, int words)
+{
+	int i;
+	for (i = 0; i < words; i++)
+		out[i] = __raw_readl(&in[i]);
+	khwq_qos_dump_words(kdev, ofs, out, words, "-->");
+}
+
+static int khwq_qos_write_shadow(struct khwq_qos_info *info, u32 options,
+				 u32 ofs, u32 *data, u32 words)
+{
+	u32 __iomem *out = info->pdsp->command + QOS_SHADOW_OFFSET + ofs;
+	struct khwq_device *kdev = info->kdev;
+	int words_left = words;
+	u32 command;
+	int error;
+
+	assert_spin_locked(&info->lock);
+
+	while (data && words_left) {
+		words = min(words_left, 4);
+		khwq_qos_write_words(kdev, ofs, out, data, words);
+		ofs	   += words * sizeof(u32);
+		out	   += words;
+		data	   += words;
+		words_left -= words;
+	}
+
+	command = QOS_CMD_PORT_SHADOW | QOS_COPY_SHADOW_TO_ACTIVE | options;
+	error = khwq_qos_write_cmd(info, command);
+
+	return error;
+}
+
+static int khwq_qos_read_shadow(struct khwq_qos_info *info, u32 options,
+				u32 ofs, u32 *data, u32 words)
+{
+	u32 __iomem *in = info->pdsp->command + QOS_SHADOW_OFFSET + ofs;
+	struct khwq_device *kdev = info->kdev;
+	int words_left = words;
+	u32 command;
+	int error;
+
+	assert_spin_locked(&info->lock);
+
+	command = QOS_CMD_PORT_SHADOW | QOS_COPY_ACTIVE_TO_SHADOW | options;
+	error = khwq_qos_write_cmd(info, command);
+	if (error)
+		return error;
+
+	while (data && words_left) {
+		words = min(words_left, 4);
+		khwq_qos_read_words(kdev, ofs, data, in, words);
+		ofs	   += words * sizeof(u32);
+		in	   += words;
+		data	   += words;
+		words_left -= words;
+	}
+
+	return 0;
+}
+
+static int khwq_qos_program_drop_sched(struct khwq_qos_info *info)
+{
+	u32 config[4];
+
+	config[0] = (info->drop_cfg.qos_ticks << 8 |
+		     info->drop_cfg.drop_ticks);
+	config[1] = info->drop_cfg.seed[0];
+	config[2] = info->drop_cfg.seed[1];
+	config[3] = info->drop_cfg.seed[2];
+
+	return khwq_qos_write_shadow(info, QOS_DROP_SCHED_CFG << 24, 0,
+				     config, 4);
+}
+
+static int khwq_qos_request_stats(struct khwq_qos_info *info, int index)
+{
+	struct khwq_qos_stats *stats = &info->stats;
+	struct khwq_device *kdev = info->kdev;
+	u32 __iomem *ofs;
+	u64 *to;
+	u32 command;
+	int error;
+	
+	spin_lock_bh(&info->lock);
+
+	ofs = info->pdsp->command + QOS_STATS_OFFSET;
+
+	command = (QOS_CMD_STATS_REQUEST | (0x8f << 8) |
+			(index << 16));
+	error = khwq_qos_write_cmd(info, command);
+	if (error) {
+		dev_err(kdev->dev, "failed to request stats for block %d\n",
+			index);
+		goto out;
+	}
+
+	to = (stats->data + index * 0x20);
+
+	to[0] += __raw_readl(&ofs[0]);
+	to[1] += __raw_readl(&ofs[2]);
+	to[2] += __raw_readl(&ofs[4]);
+	to[3] += __raw_readl(&ofs[5]);
+
+	dev_dbg(kdev->dev, "%d %llx %llx %llx %llx\n",
+				index, to[0], to[1], to[2], to[3]);
+
+out:
+	spin_unlock_bh(&info->lock);
+	return error;
+}
+
+static int khwq_qos_update_stats(struct khwq_qos_info *info)
+{
+	struct khwq_qos_stats *stats = &info->stats;
+	int i, error;
+	
+	for (i = (stats->count - 1); i >= stats->start; i--) {
+		if (!test_bit(i, stats->avail)) {
+			error = khwq_qos_request_stats(info, i);
+			if (error)
+				return error;
+		}
+	}
+
+	return 0;
+}
+
+static void khwq_qos_timer(unsigned long arg)
+{
+	struct khwq_qos_info *info = (struct khwq_qos_info *)arg;
+	struct khwq_device *kdev = info->kdev;
+	int error;
+
+	error = khwq_qos_update_stats(info);
+	if (error) {
+		dev_err(kdev->dev, "error updating stats\n");
+		return;
+	}
+
+	info->timer.expires = jiffies + KHWQ_QOS_TIMER_INTERVAL;
+	add_timer(&info->timer);
+
+	return;
+}
+
+struct khwq_qos_attr {
+	struct attribute	attr;
+	int			offset;
+};
+
+#define __KHWQ_QOS_STATS_ATTR(_name, _offset)				\
+(struct khwq_qos_attr) {					\
+	.attr = { .name = __stringify(_name), .mode = S_IRUGO },	\
+	.offset = _offset,						\
+}
+
+#define KHWQ_QOS_STATS_ATTR(_name, _offset)		\
+	&__KHWQ_QOS_STATS_ATTR(_name, _offset).attr
+
+static struct attribute *khwq_qos_stats_attrs[] = {
+	KHWQ_QOS_STATS_ATTR(bytes_forwarded,	0x00),
+	KHWQ_QOS_STATS_ATTR(bytes_discarded,	0x08),
+	KHWQ_QOS_STATS_ATTR(packets_forwarded,	0x10),
+	KHWQ_QOS_STATS_ATTR(packets_discarded,	0x18),
+	NULL
+};
+
+static ssize_t khwq_qos_stats_attr_show(struct kobject *kobj,
+					struct attribute *_attr, char *buf)
+{
+	struct khwq_qos_attr *attr;
+	struct khwq_qos_stats_class *class;
+	struct khwq_qos_info *info;
+	struct khwq_qos_stats *stats;
+	int offset, index, error;
+	u64 *val;
+
+	class = container_of(kobj, struct khwq_qos_stats_class, kobj);
+	attr = container_of(_attr, struct khwq_qos_attr, attr);
+	info = class->info;
+	stats = &info->stats;
+	index = class->stats_block_idx;
+	offset = attr->offset;
+
+	error = khwq_qos_request_stats(info, index);
+	if (error)
+		return error;
+
+	val = stats->data + (index * 0x20) + offset;
+
+	return snprintf(buf, PAGE_SIZE, "%lld\n", *val);
+}
+
+static struct kobj_type khwq_qos_stats_ktype = {
+	.sysfs_ops = &(struct sysfs_ops) { .show = khwq_qos_stats_attr_show },
+	.default_attrs = khwq_qos_stats_attrs,
+};
+
+static void khwq_qos_free_drop_policy(struct khwq_device *kdev,
+				      struct khwq_qos_drop_policy *policy)
+{
+	list_del(&policy->list);
+	kobject_del(&policy->kobj);
+	kobject_put(&policy->kobj);
+	devm_kfree(kdev->dev, policy);
+}
+
+static void khwq_qos_free_drop_policies(struct khwq_device *kdev,
+					struct khwq_qos_info *info)
+{
+	struct khwq_qos_drop_policy *policy;
+
+	while (!list_empty(&info->drop_policies)) {
+		policy = list_first_entry(&info->drop_policies,
+					  struct khwq_qos_drop_policy,
+					  list);
+		khwq_qos_free_drop_policy(kdev, policy);
+	}
+}
+
+static int khwq_program_drop_policy(struct khwq_qos_info *info,
+				    struct khwq_qos_drop_policy *policy,
+				    bool sync)
+{
+	int error;
+	u32 val, time_constant, diff;
+	u32 thresh_recip;
+
+	val = (policy->acct == QOS_BYTE_ACCT) ? BIT(0) : 0;
+
+	error = khwq_qos_set_drop_cfg_unit_flags(info, policy->drop_cfg_idx,
+						 val, false);
+	if (error)
+		return error;
+
+	error = khwq_qos_set_drop_cfg_mode(info, policy->drop_cfg_idx,
+					   policy->mode, false);
+	if (error)
+		return error;
+
+	error = khwq_qos_set_drop_cfg_tail_thresh(info, policy->drop_cfg_idx,
+						  policy->limit, false);
+	if (error)
+		return error;
+
+	if (policy->mode == QOS_TAILDROP)
+		return 0;
+
+	error = khwq_qos_set_drop_cfg_red_low(info, policy->drop_cfg_idx,
+					      policy->red_low, false);
+	if (error)
+		return error;
+
+	error = khwq_qos_set_drop_cfg_red_high(info, policy->drop_cfg_idx,
+					       policy->red_high, false);
+
+	val = policy->half_life / 100;
+	time_constant = ilog2((3 * val) + 1);
+
+	error = khwq_qos_set_drop_cfg_time_const(info, policy->drop_cfg_idx,
+						   time_constant, false);
+
+	diff = ((policy->red_high - policy->red_low) >> time_constant);
+
+	thresh_recip = 1 << 31;
+	thresh_recip /= (diff >> 1);
+
+	error = khwq_qos_set_drop_cfg_thresh_recip(info, policy->drop_cfg_idx,
+						   thresh_recip, false);
+
+	return error;
+}
+
+#define __KHWQ_QOS_DROP_POLICY_ATTR(_name)			\
+(struct khwq_qos_attr) {					\
+	.attr = { .name = __stringify(_name), .mode = S_IRUGO|S_IWUSR },\
+	.offset = offsetof(struct khwq_qos_drop_policy, _name),	\
+}
+
+#define KHWQ_QOS_DROP_POLICY_ATTR(_name)		\
+	&__KHWQ_QOS_DROP_POLICY_ATTR(_name).attr
+
+static struct attribute *khwq_qos_drop_policy_attrs[] = {
+	KHWQ_QOS_DROP_POLICY_ATTR(limit),
+	KHWQ_QOS_DROP_POLICY_ATTR(red_low),
+	KHWQ_QOS_DROP_POLICY_ATTR(red_high),
+	KHWQ_QOS_DROP_POLICY_ATTR(half_life),
+	KHWQ_QOS_DROP_POLICY_ATTR(max_drop_prob),
+	NULL
+};
+
+static struct attribute *khwq_qos_drop_policy_taildrop_attrs[] = {
+	KHWQ_QOS_DROP_POLICY_ATTR(limit),
+	NULL
+};
+
+static ssize_t khwq_qos_drop_policy_attr_show(struct kobject *kobj,
+					      struct attribute *_attr,
+					      char *buf)
+{
+	struct khwq_qos_drop_policy *policy;
+	struct khwq_qos_attr *attr;
+	int offset;
+	u32 *val;
+
+	policy = container_of(kobj, struct khwq_qos_drop_policy, kobj);
+	attr = container_of(_attr, struct khwq_qos_attr, attr);
+	offset = attr->offset;
+
+	val = (((void *)policy) + offset);
+
+	return snprintf(buf, PAGE_SIZE, "%d\n", *val);
+}
+
+static ssize_t khwq_qos_drop_policy_attr_store(struct kobject *kobj,
+					       struct attribute *_attr,
+					       const char *buf, size_t size)
+{
+	struct khwq_qos_drop_policy *policy;
+	struct khwq_qos_info *info;
+	struct khwq_device *kdev;
+	struct khwq_qos_attr *attr;
+	int offset, error, field;
+	u32 *val;
+
+	policy = container_of(kobj, struct khwq_qos_drop_policy, kobj);
+	attr = container_of(_attr, struct khwq_qos_attr, attr);
+	offset = attr->offset;
+	info = policy->info;
+	kdev = info->kdev;
+
+	error = kstrtouint(buf, 0, &field);
+	if (error)
+		return error;
+
+	val = (((void *)policy) + offset);
+	*val = field;
+
+	error = khwq_program_drop_policy(info, policy, false);
+	if (error) {
+		khwq_qos_free_drop_policy(kdev, policy);
+		policy->drop_cfg_idx = -1;
+		return error;
+	}
+
+	error = khwq_qos_sync_drop_cfg(info, -1);
+	if (error)
+		dev_err(kdev->dev, "failed to sync drop configs\n");
+
+	return size;
+}
+
+static struct kobj_type khwq_qos_policy_taildrop_ktype = {
+	.sysfs_ops = &(struct sysfs_ops) {
+		.show	= khwq_qos_drop_policy_attr_show,
+		.store	= khwq_qos_drop_policy_attr_store,
+	},
+	.default_attrs = khwq_qos_drop_policy_taildrop_attrs,
+};
+
+static struct kobj_type khwq_qos_policy_ktype = {
+	.sysfs_ops = &(struct sysfs_ops) {
+		.show	= khwq_qos_drop_policy_attr_show,
+		.store	= khwq_qos_drop_policy_attr_store,
+	},
+	.default_attrs = khwq_qos_drop_policy_attrs,
+};
+
+static int khwq_program_drop_policies(struct khwq_qos_info *info)
+{
+	struct khwq_device *kdev = info->kdev;
+	struct khwq_qos_drop_policy *policy;
+	int error = 0;
+
+	for_each_policy(info, policy) {
+		if (!policy->usecount && policy != info->default_drop_policy)
+			continue;
+
+		policy->drop_cfg_idx = khwq_qos_alloc_drop_cfg(info);
+		if (policy->drop_cfg_idx < 0) {
+			dev_err(kdev->dev, "too many drop policies\n");
+			error = -EOVERFLOW;
+			break;
+		}
+
+		error = khwq_program_drop_policy(info, policy, false);
+		if (error) {
+			khwq_qos_free_drop_policy(kdev, policy);
+			policy->drop_cfg_idx = -1;
+			break;
+		}
+
+		dev_dbg(kdev->dev, "added policy %s\n", policy->name);
+	}
+
+	if (!error) {
+		error = khwq_qos_sync_drop_cfg(info, -1);
+		if (error)
+			dev_err(kdev->dev, "failed to sync drop configs\n");
+	}
+
+	return error;
+}
+
+static struct khwq_qos_stats_class *
+khwq_qos_init_stats_class(struct khwq_qos_info *info, const char *name)
+{
+	struct khwq_device *kdev = info->kdev;
+	struct khwq_qos_stats_class *class;
+	struct khwq_qos_stats *stats = &info->stats;
+	int idx, error;
+
+	class = devm_kzalloc(kdev->dev, sizeof(*class), GFP_KERNEL);
+	if (!class)
+		return NULL;
+	class->name = name;
+	class->info = info;
+
+	spin_lock_bh(&info->lock);
+
+	idx = find_last_bit(stats->avail, stats->count);
+	if (idx < stats->count)
+		clear_bit(idx, stats->avail);
+	else {
+		spin_unlock_bh(&info->lock);
+		return NULL;
+	}
+
+	spin_unlock_bh(&info->lock);
+
+	class->stats_block_idx = idx;
+
+	list_add_tail(&class->list, &info->stats_classes);
+
+	error = kobject_init_and_add(&class->kobj, &khwq_qos_stats_ktype,
+				info->kobj_stats, class->name);
+	if (error) {
+		dev_err(kdev->dev, "failed to create sysfs file\n");
+		return NULL;
+	}
+
+	return class;
+}
+
+static void khwq_qos_free_stats_class(struct khwq_qos_info *info,
+				      struct khwq_qos_stats_class *class)
+{
+	struct khwq_device *kdev = info->kdev;
+	struct khwq_qos_stats *stats = &info->stats;
+	
+	spin_lock_bh(&info->lock);
+
+	list_del(&class->list);
+	set_bit(class->stats_block_idx, stats->avail);
+
+	spin_unlock_bh(&info->lock);
+
+	kobject_del(&class->kobj);
+	kobject_put(&class->kobj);
+	devm_kfree(kdev->dev, class);
+}
+
+static void khwq_qos_free_stats_classes(struct khwq_device *kdev,
+					struct khwq_qos_info *info)
+{
+	struct khwq_qos_stats_class *class;
+
+	while (!list_empty(&info->stats_classes)) {
+		class = list_first_entry(&info->stats_classes,
+					 struct khwq_qos_stats_class,
+					 list);
+		khwq_qos_free_stats_class(info, class);
+	}
+}
+
+static struct khwq_qos_stats_class *
+khwq_qos_find_stats_class(struct khwq_qos_info *info, const char *name)
+{
+	struct khwq_qos_stats_class *class;
+	list_for_each_entry(class, &info->stats_classes, list)
+		if (!strcmp(class->name, name))
+			return class;
+	return NULL;
+}
+
+static struct khwq_qos_drop_policy *
+khwq_qos_find_drop_policy(struct khwq_qos_info *info, const char *name)
+{
+	struct khwq_qos_drop_policy *policy;
+	list_for_each_entry(policy, &info->drop_policies, list)
+		if (!strcmp(policy->name, name))
+			return policy;
+	return NULL;
+}
+
+static struct khwq_qos_drop_policy *
+khwq_qos_get_drop_policy(struct khwq_device *kdev, struct khwq_qos_info *info,
+			 struct device_node *node)
+{
+	struct khwq_qos_drop_policy *policy;
+	u32 length, elements, temp[4];
+	const char *name;
+	int error;
+
+	policy = devm_kzalloc(kdev->dev, sizeof(*policy), GFP_KERNEL);
+	if (!policy)
+		return ERR_PTR(-ENOMEM);
+
+	policy->info = info;
+
+	error = of_property_read_string(node, "label", &name);
+	policy->name = (error < 0) ? node->name : name;
+	if (khwq_qos_find_drop_policy(info, policy->name)) {
+		dev_err(kdev->dev, "duplicate drop policy %s\n", policy->name);
+		devm_kfree(kdev->dev, policy);
+		return ERR_PTR(-EINVAL);
+	}
+
+	policy->mode = QOS_TAILDROP;
+
+	policy->acct = QOS_BYTE_ACCT;
+	if (of_find_property(node, "packet-units", NULL))
+		policy->acct = QOS_PACKET_ACCT;
+
+	error = of_property_read_u32(node, "limit", &policy->limit);
+	if (error < 0)
+		policy->limit = 0;
+
+	if (!of_find_property(node, "random-early-drop", &length))
+		goto done;
+
+	policy->mode = QOS_RED;
+
+	if (policy->acct == QOS_PACKET_ACCT) {
+		dev_err(kdev->dev, "red policy must account bytes\n");
+		devm_kfree(kdev->dev, policy);
+		return ERR_PTR(-EINVAL);
+	}
+
+	elements = length / sizeof(u32);
+	if (elements < 1 || elements > 4) {
+		dev_err(kdev->dev, "invalid number of elements in red info\n");
+		devm_kfree(kdev->dev, policy);
+		return ERR_PTR(-EINVAL);
+	}
+
+	error = of_property_read_u32_array(node, "random-early-drop", temp,
+					   elements);
+	if (error < 0) {
+		dev_err(kdev->dev, "could not obtain red info\n");
+		devm_kfree(kdev->dev, policy);
+		return ERR_PTR(-EINVAL);
+	}
+
+	policy->red_low = temp[0];
+
+	policy->red_high = 2 * policy->red_low;
+	if (elements > 1)
+		policy->red_high = temp[1];
+
+	policy->max_drop_prob = 2;
+	if (elements > 2)
+		policy->max_drop_prob = temp[2];
+	if (policy->max_drop_prob >= 100) {
+		dev_warn(kdev->dev, "invalid max drop prob %d on policy %s, taking defaults\n",
+			 policy->max_drop_prob, policy->name);
+		policy->max_drop_prob = 2;
+	}
+
+	policy->half_life = 2000;
+	if (elements > 3)
+		policy->half_life = temp[3];
+
+done:
+	if (of_find_property(node, "default", NULL)) {
+		if (info->default_drop_policy) {
+			dev_warn(kdev->dev, "duplicate default policy %s\n",
+				 policy->name);
+		} else
+			info->default_drop_policy = policy;
+	}
+	
+	if (policy->mode == QOS_RED)
+		error = kobject_init_and_add(&policy->kobj,
+				     &khwq_qos_policy_ktype,
+				     info->kobj_policies, policy->name);
+	else
+		error = kobject_init_and_add(&policy->kobj,
+				     &khwq_qos_policy_taildrop_ktype,
+				     info->kobj_policies, policy->name);
+
+	if (error) {
+		dev_err(kdev->dev, "failed to create sysfs "
+				"entries for policy %s\n", policy->name);
+		devm_kfree(kdev->dev, policy);
+		return ERR_PTR(-EINVAL);;
+	}
+
+	return policy;
+}
+
+static int khwq_qos_get_drop_policies(struct khwq_device *kdev,
+				      struct khwq_qos_info *info,
+				      struct device_node *node)
+{
+	struct khwq_qos_drop_policy *policy;
+	struct device_node *child;
+	int error = 0;
+
+	for_each_child_of_node(node, child) {
+		policy = khwq_qos_get_drop_policy(kdev, info, child);
+		if (IS_ERR_OR_NULL(policy)) {
+			error = PTR_ERR(policy);
+			break;
+		}
+		list_add_tail(&policy->list, &info->drop_policies);
+	}
+
+	if (!error && !info->default_drop_policy) {
+		dev_err(kdev->dev, "must specify a default drop policy!\n");
+		error = -ENOENT;
+	}
+
+	if (!error)
+		return 0;
+
+	khwq_qos_free_drop_policies(kdev, info);
+
+	return error;
+}
+
+static struct khwq_qos_shadow *
+khwq_find_shadow(struct khwq_qos_info *info, enum khwq_qos_shadow_type type,
+		 int idx, int offset, bool internal)
+{
+	struct khwq_qos_shadow *shadow;
+	struct khwq_device *kdev = info->kdev;
+
+	shadow = &info->shadows[type];
+
+	idx = khwq_qos_id_to_idx(idx);
+	if (idx >= shadow->count || offset >= shadow->size) {
+		dev_err(kdev->dev, "bad shadow access, idx %d, count %d, "
+			"offset %d, size %d\n",
+			idx, shadow->count, offset, shadow->size);
+		return NULL;
+	}
+
+	if (!internal && test_bit(idx, shadow->avail)) {
+		dev_err(kdev->dev, "idx %d not in use\n", idx);
+		return NULL;
+	}
+
+	return shadow;
+}
+
+int khwq_qos_get(struct khwq_qos_info *info, enum khwq_qos_shadow_type type,
+		   const char *name, int idx, int offset, int startbit,
+		   int nbits, u32 *value)
+{
+	struct khwq_device *kdev = info->kdev;
+	struct khwq_qos_shadow *shadow;
+	u32 *element;
+
+	shadow = khwq_find_shadow(info, type, idx, offset, false);
+	if (WARN_ON(!shadow))
+		return -EINVAL;
+
+	idx = khwq_qos_id_to_idx(idx);
+	offset += idx * shadow->size;
+	element = shadow->data + offset;
+	*value = (*element >> startbit) & BITS(nbits);
+	dev_dbg(kdev->dev, "=== %s(%d) --> %x\n", name, idx, *value);
+	return 0;
+}
+
+int khwq_qos_set(struct khwq_qos_info *info, enum khwq_qos_shadow_type type,
+		   const char *name, int idx, int offset, int startbit,
+		   int nbits, bool sync, u32 value, bool internal)
+{
+	struct khwq_device *kdev = info->kdev;
+	struct khwq_qos_shadow *shadow;
+	u32 *element;
+	u32 outval;
+
+	shadow = khwq_find_shadow(info, type, idx, offset, internal);
+	if (WARN_ON(!shadow))
+		return -EINVAL;
+
+	idx = khwq_qos_id_to_idx(idx);
+	offset += idx * shadow->size;
+	element = shadow->data + offset;
+	outval  = *element & ~BITMASK(startbit, nbits);
+	WARN_ON(value & ~BITS(nbits));
+	outval |= (value & BITS(nbits)) << startbit;
+	dev_dbg(kdev->dev, "=== %s(%d) <-- %x [%08x --> %08x]\n", name, idx,
+		value, *element, outval);
+	*element = outval;
+
+	set_bit(idx, shadow->dirty);
+	if (sync)
+		return shadow->sync ? shadow->sync(shadow, idx) : - ENOSYS;
+	return 0;
+}
+
+int khwq_qos_control(struct khwq_qos_info *info, enum khwq_qos_shadow_type type,
+		       enum khwq_qos_control_type ctrl, int idx, u32 arg,
+		       bool internal)
+{
+	struct khwq_qos_shadow *shadow;
+
+	shadow = khwq_find_shadow(info, type, idx, 0, internal);
+	if (WARN_ON(!shadow))
+		return -EINVAL;
+
+	idx = khwq_qos_id_to_idx(idx);
+	if (!shadow->control)
+		return -ENOSYS;
+	return shadow->control(shadow, ctrl, idx, arg);
+}
+
+int khwq_qos_sync(struct khwq_qos_info *info, enum khwq_qos_shadow_type type,
+		    int idx, bool internal)
+{
+	struct khwq_pdsp_info *pdsp;
+	struct khwq_qos_shadow *shadow;
+	struct khwq_device *kdev = info->kdev;
+	int error = 0;
+
+	if (type < 0 || type >= QOS_MAX_SHADOW) {
+		dev_err(kdev->dev, "bad shadow type %d\n", type);
+		return -EINVAL;
+	}
+
+	if (idx >= 0) {
+		shadow = khwq_find_shadow(info, type, idx, 0, internal);
+		if (WARN_ON(!shadow))
+			return -EINVAL;
+		idx = khwq_qos_id_to_idx(idx);
+		return shadow->sync ? shadow->sync(shadow, idx) : - ENOSYS;
+	}
+
+	/* sync all */
+	for_each_pdsp(kdev, pdsp) {
+		info = pdsp->qos_info;
+		if (!info)
+			continue;
+		shadow = &info->shadows[type];
+
+		error = shadow->sync ? shadow->sync(shadow, idx) : 0;
+		if (error)
+			break;
+	}
+
+	return error;
+}
+
+int khwq_qos_alloc(struct khwq_qos_info *info, enum khwq_qos_shadow_type type)
+{
+	struct khwq_pdsp_info *pdsp = info->pdsp;
+	struct khwq_qos_shadow *shadow;
+	int idx;
+
+	shadow = &info->shadows[type];
+	spin_lock_bh(&info->lock);
+
+	idx = find_last_bit(shadow->avail, shadow->count);
+	if (idx < shadow->count) {
+		clear_bit(idx, shadow->avail);
+		spin_unlock_bh(&info->lock);
+		return khwq_qos_make_id(pdsp->id, idx);
+	}
+
+	spin_unlock_bh(&info->lock);
+
+	return -ENOSPC;
+}
+
+int khwq_qos_free(struct khwq_qos_info *info, enum khwq_qos_shadow_type type,
+		    int idx)
+{
+	struct khwq_qos_shadow *shadow;
+
+	shadow = khwq_find_shadow(info, type, idx, 0, false);
+	if (WARN_ON(!shadow))
+		return -EINVAL;
+
+	idx = khwq_qos_id_to_idx(idx);
+	if (WARN_ON(test_bit(idx, shadow->dirty)	||
+		    test_bit(idx, shadow->running)	||
+		    test_bit(idx, shadow->avail)))
+	    return -EBUSY;
+
+	set_bit(idx, shadow->avail);
+
+	return 0;
+}
+
+static int khwq_qos_alloc_drop_queue(struct khwq_qos_info *info, int _idx)
+{
+	struct khwq_pdsp_info *pdsp = info->pdsp;
+	struct khwq_qos_shadow *shadow;
+	int idx, base, count;
+
+	shadow = &info->shadows[QOS_DROP_QUEUE_CFG];
+	base   = info->drop_sched_queue_base;
+	count  = shadow->count;
+
+
+	idx = _idx - base;
+
+	if (test_and_clear_bit(idx, shadow->avail))
+		return khwq_qos_make_id(pdsp->id, idx);
+	else
+		return -EBUSY;
+
+	return -ENODEV;
+}
+
+static int khwq_qos_free_drop_queue(struct khwq_qos_info *info, int idx)
+{
+	return khwq_qos_free(info, QOS_DROP_QUEUE_CFG, idx);
+}
+
+static int khwq_qos_sched_port_enable(struct khwq_qos_shadow *shadow, int idx,
+				       bool enable)
+{
+	struct khwq_qos_info *info = shadow->info;
+	struct khwq_device *kdev = info->kdev;
+	int error = 0, start = idx, end = idx + 1;
+	u32 command;
+
+	if (WARN_ON(idx >= shadow->count))
+		return -EINVAL;
+
+	if (idx < 0) {
+		start = 0;
+		end = shadow->count;
+	}
+
+	/* fail if our state is dirty */
+	for (idx = start; idx < end; idx ++) {
+		if (WARN_ON(test_bit(idx, shadow->dirty)))
+			return -EBUSY;
+	}
+
+	for (idx = start; idx < end; idx ++) {
+		if (enable && test_bit(idx, shadow->running)) {
+			dev_warn(kdev->dev, "forced enable on running %s %d\n",
+				 shadow->name, idx);
+		}
+		if (!enable && !test_bit(idx, shadow->running)) {
+			dev_warn(kdev->dev, "forced disable on halted %s %d\n",
+				 shadow->name, idx);
+		}
+		command = (QOS_CMD_ENABLE_PORT			|
+			   (enable ? QOS_ENABLE : QOS_DISABLE)	|
+			   (shadow->start + idx) << 16);
+		error = khwq_qos_write_cmd(info, command);
+		if (error) {
+			dev_err(kdev->dev, "failed to %s %s index %d\n",
+				enable ? "enable" : "disable", shadow->name, idx);
+			break;
+		}
+		if (enable)
+			set_bit(idx, shadow->running);
+		else
+			clear_bit(idx, shadow->running);
+	}
+
+	return 0;
+}
+
+static int khwq_qos_sched_port_get_input(struct khwq_qos_shadow *shadow,
+					 int idx, u32 queue)
+{
+	struct khwq_qos_info *info = shadow->info;
+	struct khwq_device *kdev = info->kdev;
+
+	if (WARN_ON(idx < 0 || idx >= shadow->count ||
+		    test_bit(idx, shadow->avail)))
+		return -EINVAL;
+	if (queue >= shadow->info->inputs_per_port) {
+		dev_err(kdev->dev, "requested queue %d out of range\n",
+			queue);
+		return -EINVAL;
+	}
+
+	return (shadow->start + idx) * info->inputs_per_port +
+		queue + info->sched_port_queue_base;
+}
+
+static int __khwq_qos_control_sched_port(struct khwq_qos_shadow *shadow,
+					 enum khwq_qos_control_type ctrl,
+					 int idx, u32 arg)
+{
+	struct khwq_qos_info *info = shadow->info;
+	int error = 0;
+
+	spin_lock_bh(&info->lock);
+
+	switch (ctrl) {
+
+	case QOS_CONTROL_ENABLE:
+		error = khwq_qos_sched_port_enable(shadow, idx, !!arg);
+		break;
+
+	case QOS_CONTROL_GET_INPUT:
+		error = khwq_qos_sched_port_get_input(shadow, idx, arg);
+		break;
+
+	default:
+		error = -ENOSYS;
+		break;
+	}
+
+	spin_unlock_bh(&info->lock);
+
+	return error;
+}
+
+static int khwq_qos_sync_shadow_unified(struct khwq_qos_shadow *shadow, int idx)
+{
+	struct khwq_qos_info *info = shadow->info;
+	int count, error, offset;
+	u32 *from;
+
+	if (WARN_ON(idx >= shadow->count))
+		return -EINVAL;
+
+	spin_lock_bh(&info->lock);
+
+	/* now fill in our data */
+	if (idx < 0) {
+		offset	= shadow->start * shadow->size;
+		from	= shadow->data;
+		count	= (shadow->size * shadow->count) / sizeof(u32);
+	} else {
+		offset	= (shadow->start + idx) * shadow->size;
+		from	= shadow->data + idx * shadow->size;
+		count	= shadow->size / sizeof(u32);
+	}
+
+	/* first get the pdsp to copy active config to shadow area */
+	error = khwq_qos_read_shadow(info, shadow->type << 24, 0, NULL, 0);
+	if (error)
+		goto bail;
+
+	/* now fill in our data and write it back */
+	error = khwq_qos_write_shadow(info, shadow->type << 24, offset,
+				      from, count);
+	if (error)
+		goto bail;
+
+	if (idx < 0) {
+		count = BITS_TO_LONGS(shadow->count) * sizeof(long);
+		memset(shadow->dirty, 0, count);
+	} else
+		clear_bit(idx, shadow->dirty);
+
+	error = 0;
+
+bail:
+	spin_unlock_bh(&info->lock);
+
+	return error;
+}
+
+static int khwq_qos_sync_shadow_single(struct khwq_qos_shadow *shadow, int idx)
+{
+	struct khwq_qos_info *info = shadow->info;
+	struct khwq_pdsp_info *pdsp = info->pdsp;
+	int count, error = 0, start = idx, end = idx + 1;
+	u32 __iomem *to;
+	u32 *from;
+	u32 command;
+
+	if (WARN_ON(idx >= shadow->count))
+		return -EINVAL;
+
+	if (idx < 0) {
+		start = 0;
+		end = shadow->count;
+	}
+
+	spin_lock_bh(&info->lock);
+
+	for (idx = start; idx < end; idx ++) {
+		from	= shadow->data + (idx * shadow->size);
+		to	= pdsp->command + QOS_SHADOW_OFFSET;
+		count	= shadow->size / sizeof(u32);
+
+		command = (shadow->start + idx) << 16 | shadow->type << 24;
+		error = khwq_qos_write_shadow(info, command, 0, from, count);
+		if (error)
+			break;
+
+		clear_bit(idx, shadow->dirty);
+	}
+
+	spin_unlock_bh(&info->lock);
+
+	return error;
+}
+
+static void khwq_qos_free_shadow(struct khwq_qos_info *info,
+				 enum khwq_qos_shadow_type type)
+{
+	struct khwq_qos_shadow *shadow = &info->shadows[type];
+	struct khwq_device *kdev = info->kdev;
+
+	if (shadow->data)
+		devm_kfree(kdev->dev, shadow->data);
+}
+
+static int khwq_qos_init_shadow(struct khwq_qos_info *info,
+				enum khwq_qos_shadow_type type, const char *name,
+				struct device_node *node, bool unified)
+{
+	struct khwq_qos_shadow *shadow = &info->shadows[type];
+	struct khwq_device *kdev = info->kdev;
+	int error, size, alloc_size;
+	u32 temp[3];
+
+	shadow->info = info;
+	shadow->name = name;
+
+	error = of_property_read_u32_array(node, name, temp, 3);
+	if (error < 0) {
+		dev_err(kdev->dev, "invalid shadow config for %s\n",
+			name);
+		return -ENODEV;
+	}
+
+	shadow->start	= temp[0];
+	shadow->count	= temp[1];
+	shadow->size	= temp[2];
+	shadow->type	= type;
+	shadow->sync	= (unified ? khwq_qos_sync_shadow_unified :
+				     khwq_qos_sync_shadow_single);
+	if (type == QOS_SCHED_PORT_CFG)
+		shadow->control	= __khwq_qos_control_sched_port;
+
+	if (shadow->size % 4) {
+		dev_err(kdev->dev, "misaligned shadow size for %s\n",
+			name);
+		return -ENODEV;
+	}
+
+	size = shadow->size * shadow->count;
+	alloc_size = size + 3 * BITS_TO_LONGS(shadow->count) * sizeof(long);
+
+	shadow->data = devm_kzalloc(kdev->dev, alloc_size, GFP_KERNEL);
+	if (!shadow->data) {
+		dev_err(kdev->dev, "shadow alloc failed for %s\n",
+			name);
+		return -ENOMEM;
+	}
+	shadow->dirty	= shadow->data + size;
+	shadow->avail	= shadow->dirty + BITS_TO_LONGS(shadow->count);
+	shadow->running	= shadow->avail  + BITS_TO_LONGS(shadow->count);
+
+	/* mark all as available */
+	memset(shadow->avail, 0xff,
+	       BITS_TO_LONGS(shadow->count) * sizeof(long));
+
+	return 0;
+}
+
+static int khwq_qos_init_stats(struct khwq_qos_info *info,
+				struct device_node *node)
+{
+	struct khwq_qos_stats *stats = &info->stats;
+	struct khwq_device *kdev = info->kdev;
+	int error, size, alloc_size;
+	u32 temp[2];
+
+	error = of_property_read_u32_array(node, "statistics-profiles",
+					   temp, 2);
+	if (error < 0) {
+		dev_err(kdev->dev, "invalid statistics config\n");
+		return -ENODEV;
+	}
+
+	stats->start	= temp[0];
+	stats->count	= temp[1];
+
+	size = stats->count * sizeof(u64) * 4;
+	alloc_size = size + 3 * BITS_TO_LONGS(stats->count) * sizeof(long);
+
+	stats->data = devm_kzalloc(kdev->dev, alloc_size, GFP_KERNEL);
+	if (!stats->data) {
+		dev_err(kdev->dev, "stats alloc failed\n");
+		return -ENOMEM;
+	}
+
+	stats->dirty	= stats->data + size;
+	stats->avail	= stats->dirty + BITS_TO_LONGS(stats->count);
+	stats->running	= stats->avail  + BITS_TO_LONGS(stats->count);
+
+	/* mark all as available */
+	memset(stats->avail, 0xff,
+	       BITS_TO_LONGS(stats->count) * sizeof(long));
+
+	return 0;
+}
+
+static void khwq_qos_free_stats(struct khwq_qos_info *info)
+{
+	struct khwq_qos_stats *stats = &info->stats;
+	struct khwq_device *kdev = info->kdev;
+
+	if (stats->data)
+		devm_kfree(kdev->dev, stats->data);
+}
+
+static void __khwq_free_qos_range(struct khwq_device *kdev,
+				  struct khwq_qos_info *info)
+{
+	khwq_qos_free_shadow(info, QOS_SCHED_PORT_CFG);
+	khwq_qos_free_shadow(info, QOS_DROP_CFG_PROF);
+	khwq_qos_free_shadow(info, QOS_DROP_OUT_PROF);
+	khwq_qos_free_shadow(info, QOS_DROP_QUEUE_CFG);
+	khwq_qos_free_stats(info);
+	khwq_qos_free_drop_policies(kdev, info);
+	khwq_qos_free_stats_classes(kdev, info);
+	ktree_remove_tree(&info->qos_tree);
+	kobject_del(info->kobj);
+	kobject_put(info->kobj);
+	kobject_del(info->kobj_stats);
+	kobject_put(info->kobj_stats);
+	kobject_del(info->kobj_policies);
+	kobject_put(info->kobj_policies);
+	devm_kfree(info->kdev->dev, info);
+}
+
+void khwq_free_qos_range(struct khwq_device *kdev,
+			 struct khwq_range_info *range)
+{
+	if (range->qos_info)
+		__khwq_free_qos_range(kdev, range->qos_info);
+}
+
+static int khwq_qos_prio_check(struct ktree_node *child, void *arg)
+{
+	struct khwq_qos_tree_node *node = arg;
+	struct khwq_qos_tree_node *sibling = to_qnode(child);
+
+	return (sibling->priority == node->priority) ? -EINVAL : 0;
+}
+
+struct khwq_qos_drop_policy *
+khwq_qos_inherited_drop_policy(struct khwq_qos_tree_node *node)
+{
+	for (; node; node = node->parent)
+		if (node->drop_policy)
+			return node->drop_policy;
+	return NULL;
+}
+
+static int khwq_qos_drop_policy_check(struct ktree_node *node, void *arg)
+{
+	struct khwq_qos_info *info = arg;
+	struct khwq_qos_tree_node *qnode = to_qnode(node);
+
+	if (qnode->type != QOS_NODE_DEFAULT || qnode->drop_policy)
+		return -EINVAL;
+	return ktree_for_each_child(&qnode->node, khwq_qos_drop_policy_check,
+				    info);
+}
+
+static void khwq_qos_get_tree_node(struct ktree_node *node)
+{
+	/* nothing for now */
+}
+
+static void khwq_qos_put_tree_node(struct ktree_node *node)
+{
+	struct khwq_qos_tree_node *qnode = to_qnode(node);
+	kfree(qnode);
+}
+
+static ssize_t qnode_stats_class_show(struct khwq_qos_tree_node *qnode,
+					     char *buf)
+{
+	struct khwq_qos_stats_class *class;
+
+	class = qnode->stats_class;
+
+	if (!class)
+		return -ENODEV;
+
+	return snprintf(buf, PAGE_SIZE, "%s\n", class->name);
+}
+
+static ssize_t qnode_drop_policy_show(struct khwq_qos_tree_node *qnode,
+					     char *buf)
+{
+	struct khwq_qos_drop_policy *policy;
+
+	policy = qnode->drop_policy;
+
+	if (!policy)
+		return -ENODEV;
+
+	return snprintf(buf, PAGE_SIZE, "%s\n", policy->name);
+}
+
+static ssize_t qnode_weight_show(struct khwq_qos_tree_node *qnode,
+					     char *buf)
+{
+	if (qnode->weight == -1)
+		return -EINVAL;
+
+	return snprintf(buf, PAGE_SIZE, "%d\n", qnode->weight);
+}
+
+static ssize_t qnode_priority_show(struct khwq_qos_tree_node *qnode,
+					     char *buf)
+{
+	if (qnode->priority == -1)
+		return -EINVAL;
+
+	return snprintf(buf, PAGE_SIZE, "%d\n", qnode->priority);
+}
+
+static ssize_t qnode_output_rate_show(struct khwq_qos_tree_node *qnode,
+					     char *buf)
+{
+	if (qnode->output_rate == -1)
+		return -EINVAL;
+
+	return snprintf(buf, PAGE_SIZE, "%d\n", qnode->output_rate);
+}
+
+static ssize_t qnode_output_rate_store(struct khwq_qos_tree_node *qnode,
+				       const char *buf, size_t size)
+{
+	struct khwq_qos_info *info = qnode->info;
+	int error, field;
+
+	error = kstrtouint(buf, 0, &field);
+	if (error)
+		return error;
+	
+	qnode->output_rate = field;
+
+	khwq_qos_stop(info);
+
+	khwq_qos_start(info);
+
+	return size;
+}
+
+static ssize_t qnode_burst_size_show(struct khwq_qos_tree_node *qnode,
+					     char *buf)
+{
+	if (qnode->burst_size == -1)
+		return -EINVAL;
+
+	return snprintf(buf, PAGE_SIZE, "%d\n", qnode->burst_size);
+}
+
+static ssize_t qnode_burst_size_store(struct khwq_qos_tree_node *qnode,
+				      const char *buf, size_t size)
+{
+	struct khwq_qos_info *info = qnode->info;
+	int error, field;
+
+	error = kstrtouint(buf, 0, &field);
+	if (error)
+		return error;
+	
+	qnode->burst_size = field;
+
+	khwq_qos_stop(info);
+
+	khwq_qos_start(info);
+
+	return size;
+}
+
+static ssize_t qnode_overhead_bytes_show(struct khwq_qos_tree_node *qnode,
+					     char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%d\n", qnode->overhead_bytes);
+}
+
+static ssize_t qnode_overhead_bytes_store(struct khwq_qos_tree_node *qnode,
+					  const char *buf, size_t size)
+{
+	struct khwq_qos_info *info = qnode->info;
+	int error, field;
+
+	error = kstrtouint(buf, 0, &field);
+	if (error)
+		return error;
+	
+	qnode->overhead_bytes = field;
+
+	khwq_qos_stop(info);
+
+	khwq_qos_start(info);
+
+	return size;
+}
+
+static ssize_t qnode_output_queue_show(struct khwq_qos_tree_node *qnode,
+					     char *buf)
+{
+	if (qnode->output_queue == -1)
+		return -EINVAL;
+
+	return snprintf(buf, PAGE_SIZE, "%d\n", qnode->output_queue);
+}
+
+static ssize_t qnode_input_queues_show(struct khwq_qos_tree_node *qnode,
+					     char *buf)
+{
+	ssize_t l = 0;
+	int i;
+
+	for (i = 0; i < qnode->num_input_queues; i++)
+		if (qnode->input_queue[i].valid == true)
+			l += snprintf(buf + l, PAGE_SIZE - l, "%d ",
+					qnode->input_queue[i].queue);
+
+	l += snprintf(buf + l, PAGE_SIZE - l, "\n");
+
+	return l;
+}
+
+static ssize_t qnode_input_queues_store(struct khwq_qos_tree_node *qnode,
+					  const char *buf, size_t size)
+{
+	struct khwq_qos_info *info = qnode->info;
+	struct khwq_device *kdev = info->kdev;
+	int error, field, i;
+
+	error = kstrtoint(buf, 0, &field);
+	if (error)
+		return error;
+
+	if (field < 0) {
+		for (i = 0; i < QOS_MAX_INPUTS; i++) {
+			if (qnode->input_queue[i].queue == -field) {
+				qnode->input_queue[i].valid = false;
+				khwq_qos_free_drop_queue(info,
+					qnode->input_queue[i].drop_queue_idx);
+			}
+		}
+	} else {
+		error = khwq_qos_alloc_drop_queue(info, field);
+		if (error < 0) {
+		dev_err(kdev->dev,
+			"failed to alloc input queue %d on node %s\n",
+			field, qnode->name);
+			return error;
+		}
+		for (i = 0; i < QOS_MAX_INPUTS; i++) {
+			if (qnode->input_queue[i].valid == false) {
+				qnode->input_queue[i].drop_queue_idx = error;
+				qnode->input_queue[i].valid = true;
+				qnode->input_queue[i].queue = field;
+			}
+		}
+		qnode->num_input_queues++;
+	}
+
+	khwq_qos_stop(info);
+
+	khwq_qos_start(info);
+
+	return size;
+}
+
+struct khwq_qos_qnode_attribute {
+	struct attribute attr;
+	ssize_t (*show)(struct khwq_qos_tree_node *qnode, char *);
+	ssize_t	(*store)(struct khwq_qos_tree_node *qnode,
+			 const char *, size_t);
+};
+
+#define KHWQ_QOS_QNODE_ATTR(_name, _mode, _show, _store) \
+	struct khwq_qos_qnode_attribute attr_qnode_##_name = \
+	__ATTR(_name, _mode, _show, _store)
+
+static KHWQ_QOS_QNODE_ATTR(stats_class, S_IRUGO, qnode_stats_class_show,
+			   NULL);
+static KHWQ_QOS_QNODE_ATTR(drop_policy, S_IRUGO, qnode_drop_policy_show,
+			   NULL);
+static KHWQ_QOS_QNODE_ATTR(priority, S_IRUGO,
+			   qnode_priority_show, NULL);
+static KHWQ_QOS_QNODE_ATTR(output_queue, S_IRUGO,
+			   qnode_output_queue_show,
+			   NULL);
+static KHWQ_QOS_QNODE_ATTR(weight, S_IRUGO,
+			   qnode_weight_show, NULL);
+static KHWQ_QOS_QNODE_ATTR(output_rate, S_IRUGO|S_IWUSR,
+			   qnode_output_rate_show, qnode_output_rate_store);
+static KHWQ_QOS_QNODE_ATTR(burst_size, S_IRUGO|S_IWUSR,
+			   qnode_burst_size_show, qnode_burst_size_store);
+static KHWQ_QOS_QNODE_ATTR(overhead_bytes, S_IRUGO|S_IWUSR,
+			   qnode_overhead_bytes_show,
+			   qnode_overhead_bytes_store);
+static KHWQ_QOS_QNODE_ATTR(input_queues, S_IRUGO|S_IWUSR,
+			   qnode_input_queues_show,
+			   qnode_input_queues_store);
+
+static struct attribute *khwq_qos_qnode_sysfs_default_attrs[] = {
+	&attr_qnode_output_rate.attr,
+	&attr_qnode_burst_size.attr,
+	&attr_qnode_overhead_bytes.attr,
+	&attr_qnode_output_queue.attr,
+	&attr_qnode_input_queues.attr,
+	NULL
+};
+
+static struct attribute *khwq_qos_qnode_sysfs_priority_attrs[] = {
+	&attr_qnode_stats_class.attr,
+	&attr_qnode_drop_policy.attr,
+	&attr_qnode_priority.attr,
+	&attr_qnode_output_rate.attr,
+	&attr_qnode_burst_size.attr,
+	&attr_qnode_overhead_bytes.attr,
+	&attr_qnode_input_queues.attr,
+	NULL
+};
+
+static struct attribute *khwq_qos_qnode_sysfs_wrr_attrs[] = {
+	&attr_qnode_stats_class.attr,
+	&attr_qnode_drop_policy.attr,
+	&attr_qnode_weight.attr,
+	&attr_qnode_output_rate.attr,
+	&attr_qnode_burst_size.attr,
+	&attr_qnode_overhead_bytes.attr,
+	&attr_qnode_input_queues.attr,
+	NULL
+};
+
+static ssize_t khwq_qos_qnode_attr_show(struct kobject *kobj,
+				 struct attribute *attr, char *buf)
+{
+	struct khwq_qos_qnode_attribute *qnode_attr;
+	struct khwq_qos_tree_node *qnode;
+
+	qnode = container_of(kobj, struct khwq_qos_tree_node, kobj);
+	qnode_attr = container_of(attr, struct khwq_qos_qnode_attribute,
+				   attr);
+
+	if (!qnode_attr->show)
+		return -ENOENT;
+
+	return qnode_attr->show(qnode, buf);
+}
+
+static ssize_t khwq_qos_qnode_attr_store(struct kobject *kobj,
+					  struct attribute *attr,
+					  const char *buf, size_t size)
+{
+	struct khwq_qos_qnode_attribute *qnode_attr;
+	struct khwq_qos_tree_node *qnode;
+
+	qnode = container_of(kobj, struct khwq_qos_tree_node, kobj);
+	qnode_attr = container_of(attr, struct khwq_qos_qnode_attribute,
+				   attr);
+
+	if (!qnode_attr->store)
+		return -ENOENT;
+
+	return qnode_attr->store(qnode, buf, size);
+}
+
+static struct kobj_type khwq_qos_qnode_default_ktype = {
+	.sysfs_ops = &(struct sysfs_ops) {
+		.show = khwq_qos_qnode_attr_show,
+		.store = khwq_qos_qnode_attr_store},
+	.default_attrs = khwq_qos_qnode_sysfs_default_attrs,
+};
+
+static struct kobj_type khwq_qos_qnode_priority_ktype = {
+	.sysfs_ops = &(struct sysfs_ops) {
+		.show = khwq_qos_qnode_attr_show,
+		.store = khwq_qos_qnode_attr_store},
+	.default_attrs = khwq_qos_qnode_sysfs_priority_attrs,
+};
+
+static struct kobj_type khwq_qos_qnode_wrr_ktype = {
+	.sysfs_ops = &(struct sysfs_ops) {
+		.show = khwq_qos_qnode_attr_show,
+		.store = khwq_qos_qnode_attr_store},
+	.default_attrs = khwq_qos_qnode_sysfs_wrr_attrs,
+};
+
+static int khwq_qos_cmp(struct ktree_node *_a, struct ktree_node *_b,
+			void *arg)
+{
+	void *a = to_qnode(_a);
+	void *b = to_qnode(_b);
+	int offset = (int)arg;
+	return *((u32 *)(a + offset)) - *((u32 *)(b + offset));
+}
+
+static int khwq_qos_check_overflow(struct khwq_qos_tree_node *qnode,
+				   int val)
+{
+	int tmp;
+
+	tmp = (qnode->acct == QOS_BYTE_ACCT) ?
+			(val & ~BITS(32 - QOS_CREDITS_BYTE_SHIFT)) :
+			(val & ~BITS(32 - QOS_CREDITS_PACKET_SHIFT));
+
+	if (tmp)
+		return -EINVAL;
+	
+	return 0;
+}
+
+static int khwq_qos_tree_parse(struct khwq_qos_info *info,
+				    struct device_node *node,
+				    struct khwq_qos_tree_node *parent)
+{
+	struct khwq_qos_tree_node *qnode;
+	struct khwq_device *kdev = info->kdev;
+	int length, i, error = 0, elements;
+	struct device_node *child;
+	bool has_children;
+	const char *name;
+	struct kobject *parent_kobj;
+	u32 temp[QOS_MAX_INPUTS];
+
+	/* first find out if we are a leaf node */
+	child = of_get_next_child(node, NULL);
+	has_children = !!child;
+	of_node_put(child);
+
+	qnode = devm_kzalloc(kdev->dev, sizeof(*qnode), GFP_KERNEL);
+	if (!qnode) {
+		dev_err(kdev->dev, "failed to alloc qos node\n");
+		return -ENOMEM;
+	}
+
+	if (!parent)
+		parent_kobj = info->kobj;
+	else
+		parent_kobj = &parent->kobj;
+
+	qnode->info = info;
+	qnode->parent = parent;
+	qnode->name = node->name;
+
+	if (!parent)
+		error = kobject_init_and_add(&qnode->kobj,
+					     &khwq_qos_qnode_default_ktype,
+					     parent_kobj, qnode->name);
+	else {
+		if (parent->type == QOS_NODE_PRIO)
+			error = kobject_init_and_add(&qnode->kobj,
+					     &khwq_qos_qnode_priority_ktype,
+					     parent_kobj, qnode->name);
+		else if (parent->type == QOS_NODE_WRR)
+			error = kobject_init_and_add(&qnode->kobj,
+					     &khwq_qos_qnode_wrr_ktype,
+					     parent_kobj, qnode->name);
+		else
+			error = kobject_init_and_add(&qnode->kobj,
+					     &khwq_qos_qnode_default_ktype,
+					     parent_kobj, qnode->name);
+	}
+	if (error) {
+		dev_err(kdev->dev, "failed to create sysfs "
+			"entries for qnode %s\n", qnode->name);
+		goto error_destroy;
+	}
+
+	of_property_read_string(node, "label", &qnode->name);
+	dev_dbg(kdev->dev, "processing node %s, parent %s%s\n",
+		qnode->name, parent ? parent->name : "(none)",
+		has_children ? "" : ", leaf");
+
+	qnode->type = QOS_NODE_DEFAULT;
+	if (of_find_property(node, "strict-priority", NULL))
+		qnode->type = QOS_NODE_PRIO;
+	if (of_find_property(node, "weighted-round-robin", NULL)) {
+		if (qnode->type != QOS_NODE_DEFAULT) {
+			dev_err(kdev->dev, "multiple node types in %s\n",
+				qnode->name);
+			error = -EINVAL;
+			goto error_free;
+		}
+		qnode->type = QOS_NODE_WRR;
+	}
+	if (!parent && qnode->type == QOS_NODE_DEFAULT) {
+		dev_err(kdev->dev, "root node %s must be wrr/prio\n",
+			qnode->name);
+		error = -EINVAL;
+		goto error_free;
+	}
+	if (!has_children && qnode->type != QOS_NODE_DEFAULT) {
+		dev_err(kdev->dev, "leaf node %s must not be wrr/prio\n",
+			qnode->name);
+		error = -EINVAL;
+		goto error_free;
+	}
+
+	dev_dbg(kdev->dev, "node %s: type %d\n", qnode->name, qnode->type);
+
+	qnode->weight = -1;
+	of_property_read_u32(node, "weight", &qnode->weight);
+	if (qnode->weight != -1) {
+		if (qnode->weight > QOS_MAX_WEIGHT) {
+			dev_err(kdev->dev, "cannot have weight more than 1M\n");
+			error = -EINVAL;
+			goto error_free;
+		}
+		if (!parent || parent->type != QOS_NODE_WRR) {
+			dev_err(kdev->dev, "unexpected weight on node %s\n",
+				qnode->name);
+			error = -EINVAL;
+			goto error_free;
+		}
+	} else if (parent && parent->type == QOS_NODE_WRR) {
+		dev_err(kdev->dev, "expected weight on wrr child node %s\n",
+			qnode->name);
+		error = -EINVAL;
+		goto error_free;
+	}
+	dev_dbg(kdev->dev, "node %s: weight %d\n", qnode->name, qnode->weight);
+
+	qnode->priority = -1;
+	of_property_read_u32(node, "priority", &qnode->priority);
+	if (qnode->priority != -1) {
+		if (!parent || parent->type != QOS_NODE_PRIO) {
+			dev_err(kdev->dev, "unexpected priority on node %s\n",
+				qnode->name);
+			error = -EINVAL;
+			goto error_free;
+		}
+		error = ktree_for_each_child(&parent->node,
+					     khwq_qos_prio_check, qnode);
+		if (error) {
+			dev_err(kdev->dev, "duplicate priority %d on node %s\n",
+				qnode->priority, qnode->name);
+			error = -EINVAL;
+			goto error_free;
+		}
+	} else if (parent && parent->type == QOS_NODE_PRIO) {
+		dev_err(kdev->dev, "expected prio on strict prio child %s\n",
+			qnode->name);
+		error = -EINVAL;
+		goto error_free;
+	}
+	dev_dbg(kdev->dev, "node %s: priority %d\n", qnode->name,
+		qnode->priority);
+
+	qnode->acct = QOS_BYTE_ACCT;
+	if (of_find_property(node, "byte-units", NULL))
+		qnode->acct = QOS_BYTE_ACCT;
+	else if (of_find_property(node, "packet-units", NULL))
+		qnode->acct = QOS_PACKET_ACCT;
+	else if (parent)
+		qnode->acct = parent->acct;
+	dev_dbg(kdev->dev, "node %s: accounting %s\n", qnode->name,
+		qnode->acct == QOS_PACKET_ACCT ? "packet" : "bytes");
+
+	qnode->output_queue = -1;
+	error = of_property_read_u32(node, "output-queue", &qnode->output_queue);
+	if (error && !parent) {
+		dev_err(kdev->dev, "root qos node %s needs an output queue\n",
+			qnode->name);
+		goto error_free;
+	}
+	if (!error && parent) {
+		dev_warn(kdev->dev, "output queue ignored on node %s\n",
+			 qnode->name);
+		qnode->output_queue = -1;
+	}
+	dev_dbg(kdev->dev, "node %s: output queue %d\n", qnode->name,
+		qnode->output_queue);
+
+	qnode->overhead_bytes = parent ? parent->overhead_bytes : 24;
+	error = of_property_read_u32(node, "overhead-bytes",
+				     &qnode->overhead_bytes);
+	if (!error)
+		dev_dbg(kdev->dev, "node %s: overhead bytes %d\n", qnode->name,
+			qnode->overhead_bytes);
+
+	error = of_property_read_string(node, "drop-policy", &name);
+	if (!error) {
+		qnode->drop_policy = khwq_qos_find_drop_policy(info, name);
+		if (!qnode->drop_policy) {
+			dev_err(kdev->dev, "invalid drop policy %s\n", name);
+			error = -EINVAL;
+			goto error_free;
+		}
+		qnode->drop_policy->usecount ++;
+	}
+	if (!has_children && !khwq_qos_inherited_drop_policy(qnode))
+		qnode->drop_policy = info->default_drop_policy;
+
+	dev_dbg(kdev->dev, "node %s: drop policy %s\n", qnode->name,
+		qnode->drop_policy ? qnode->drop_policy->name : "(none)");
+
+	error = of_property_read_string(node, "stats-class", &name);
+	if (!error) {
+		qnode->stats_class = khwq_qos_find_stats_class(info, name);
+		if (!qnode->stats_class)
+			qnode->stats_class = khwq_qos_init_stats_class(info, name);
+		if (!qnode->stats_class) {
+			dev_err(kdev->dev, "failed to create stats class %s\n", name);
+			error = -ENODEV;
+			goto error_free;
+		}
+		qnode->stats_class->usecount ++;
+	}
+	if (has_children && qnode->stats_class) {
+		dev_err(kdev->dev, "unexpected stats class on non-leaf %s",
+			qnode->name);
+		error = -EINVAL;
+		goto error_free;
+	}
+	dev_dbg(kdev->dev, "node %s: stats class %s\n", qnode->name,
+		qnode->stats_class ? qnode->stats_class->name : "(none)");
+
+	qnode->output_rate = parent ? parent->output_rate : -1;
+	qnode->burst_size = parent ? parent->burst_size : -1;
+	if (of_find_property(node, "output-rate", &length)) {
+		elements = length / sizeof(u32);
+		elements = max(elements, 2);
+		error = of_property_read_u32_array(node, "output-rate", temp,
+						   elements);
+		if (error) {
+			dev_err(kdev->dev, "error reading output-rate on %s\n",
+				qnode->name);
+			goto error_free;
+		}
+
+		error = khwq_qos_check_overflow(qnode, (temp[0] /
+							info->ticks_per_sec));
+		if (error) {
+			dev_err(kdev->dev, "burst rate credit overflow\n");
+			goto error_free;
+		}
+
+		qnode->output_rate = temp[0];
+
+		if (elements > 1) {
+			error = khwq_qos_check_overflow(qnode, temp[1]);
+			if (error) {
+				dev_err(kdev->dev, "burst size credit overflow\n");
+				goto error_free;
+			}
+
+			qnode->burst_size = temp[1];
+		}
+	}
+	dev_dbg(kdev->dev, "node %s: output rate %d, burst %d\n", qnode->name,
+		qnode->output_rate, qnode->burst_size);
+
+	if (of_find_property(node, "input-queues", &length)) {
+		qnode->num_input_queues = length / sizeof(u32);
+		if (qnode->num_input_queues >= QOS_MAX_INPUTS) {
+			dev_err(kdev->dev, "too many input_queues to node %s\n",
+				qnode->name);
+			error = -EOVERFLOW;
+			goto error_free;
+		}
+		error = of_property_read_u32_array(node, "input-queues",
+						   temp,
+						   qnode->num_input_queues);
+		if (error) {
+			dev_err(kdev->dev, "error getting input_queues on node %s\n",
+				qnode->name);
+			goto error_free;
+		}
+	}
+	if (has_children && qnode->num_input_queues) {
+		dev_err(kdev->dev, "unexpected input-queues on non-leaf %s",
+			qnode->name);
+		error = -EINVAL;
+		goto error_free;
+	}
+	if (!has_children && !qnode->num_input_queues) {
+		dev_err(kdev->dev, "expected input-queues on leaf %s",
+			qnode->name);
+		error = -EINVAL;
+		goto error_free;
+	}
+	for (i = 0; i < qnode->num_input_queues; i++) {
+		qnode->input_queue[i].queue = temp[i];
+		qnode->input_queue[i].valid = false;
+	}
+
+	if (!parent)
+		ktree_set_root(&info->qos_tree, &qnode->node);
+	else
+		ktree_add_child_last(&parent->node, &qnode->node);
+
+	for_each_child_of_node(node, child) {
+		error = khwq_qos_tree_parse(info, child, qnode);
+		if (error)
+			goto error_destroy;
+	}
+
+	if (qnode->drop_policy) {
+		error = ktree_for_each_child(&qnode->node,
+					     khwq_qos_drop_policy_check,
+					     info);
+		if (error)
+			goto error_destroy;
+	}
+
+	if (qnode->type == QOS_NODE_PRIO) {
+		int o = offsetof(struct khwq_qos_tree_node, priority);
+		ktree_sort_children(&qnode->node, khwq_qos_cmp, (void *)o);
+	}
+	else if (qnode->type == QOS_NODE_WRR) {
+		int o = offsetof(struct khwq_qos_tree_node, weight);
+		ktree_sort_children(&qnode->node, khwq_qos_cmp, (void *)o);
+	}
+
+	return 0;
+
+error_destroy:
+	ktree_remove_node(&qnode->node);
+error_free:
+	if (qnode && qnode->drop_policy)
+		qnode->drop_policy->usecount--;
+	if (qnode && qnode->stats_class)
+		qnode->stats_class->usecount--;
+	devm_kfree(kdev->dev, qnode);
+	return error;
+}
+
+static int khwq_qos_tree_map_nodes(struct ktree_node *node, void *arg)
+{
+	struct khwq_qos_tree_node *qnode = to_qnode(node);
+	struct khwq_qos_tree_node *parent = qnode->parent;
+	struct khwq_qos_info *info = arg;
+	struct khwq_device *kdev = info->kdev;
+
+	qnode->child_port_count	=  0;
+	qnode->child_count	=  0;
+	qnode->parent_input	=  0;
+	qnode->child_weight_sum	=  0;
+	qnode->child_weight_max	=  0;
+	qnode->child_weight_min	= -1;
+	qnode->is_drop_input	= false;
+
+	if (qnode->drop_policy)
+		qnode->is_drop_input = true;
+
+	if (parent) {
+		/* where do we plugin into our parent? */
+		qnode->parent_input = parent->child_count;
+
+		parent->child_weight[parent->child_count] = qnode->weight;
+		/* provide our parent with info */
+		parent->child_count ++;
+		parent->child_weight_sum += qnode->weight;
+		if (qnode->weight > parent->child_weight_max)
+			parent->child_weight_max = qnode->weight;
+		if (qnode->weight < parent->child_weight_min)
+			parent->child_weight_min = qnode->weight;
+
+		/* inherit if parent is an input to drop sched */
+		if (parent->is_drop_input)
+			qnode->is_drop_input = true;
+	}
+
+	ktree_for_each_child(&qnode->node, khwq_qos_tree_map_nodes, info);
+
+	qnode->has_sched_port = (qnode->type == QOS_NODE_PRIO	||
+				 qnode->type == QOS_NODE_WRR	||
+				 qnode->child_port_count);
+
+	if (qnode->has_sched_port && parent)
+		parent->child_port_count ++;
+
+	if (qnode->child_count > info->inputs_per_port) {
+		dev_err(kdev->dev, "too many input_queues (%d) to node %s\n",
+			qnode->child_count, qnode->name);
+		return -EOVERFLOW;
+	}
+	return 0;
+}
+
+static int khwq_qos_tree_alloc_nodes(struct ktree_node *node, void *arg)
+{
+	struct khwq_qos_info *info = arg;
+	struct khwq_device *kdev = info->kdev;
+	struct khwq_qos_tree_node *qnode = to_qnode(node);
+	struct khwq_qos_tree_node *parent = qnode->parent;
+	int error, i;
+
+	if (qnode->has_sched_port) {
+		error = khwq_qos_alloc_sched_port(info);
+		if (error < 0) {
+			dev_err(kdev->dev, "node %s: failed to alloc sched port [%d]\n",
+				qnode->name, error);
+			return error;
+		}
+		qnode->sched_port_idx = error;
+	} else
+		qnode->sched_port_idx = qnode->parent->sched_port_idx;
+
+
+	if (parent) {
+		if (WARN_ON(qnode->output_queue != -1))
+			return -EINVAL;
+		if (parent->type == QOS_NODE_DEFAULT)
+			qnode->parent_input = qnode->parent->parent_input;
+		error = khwq_qos_control_sched_port(info, QOS_CONTROL_GET_INPUT,
+						    parent->sched_port_idx,
+						    qnode->parent_input);
+		if (WARN_ON(error < 0))
+			return error;
+		qnode->output_queue = error;
+	}
+
+	dev_dbg(kdev->dev, "node %s: mapped to output queue %d (port %d)\n",
+		qnode->name, qnode->output_queue,
+		khwq_qos_id_to_idx(qnode->sched_port_idx));
+
+	if (qnode->drop_policy) {
+		error = khwq_qos_alloc_drop_out(info);
+		if (error < 0) {
+			dev_err(kdev->dev, "node %s: failed to alloc sched port [%d]\n",
+				qnode->name, error);
+			return error;
+		}
+		qnode->drop_out_idx = error;
+		dev_dbg(kdev->dev, "allocated drop out %d for node %s\n",
+			khwq_qos_id_to_idx(qnode->drop_out_idx), qnode->name);
+	}
+
+	if (qnode->is_drop_input) {
+
+		if (!qnode->drop_out_idx)
+			qnode->drop_out_idx = parent->drop_out_idx;
+
+		for (i = 0; i < qnode->num_input_queues; i++) {
+			error = khwq_qos_alloc_drop_queue(info,
+							  qnode->input_queue[i].queue);
+			if (error < 0) {
+				dev_err(kdev->dev,
+					"failed to alloc input queue %d on node %s\n",
+					qnode->input_queue[i].queue, qnode->name);
+				return error;
+			}
+			qnode->input_queue[i].drop_queue_idx = error;
+			qnode->input_queue[i].valid = true;
+			dev_dbg(kdev->dev, "allocated drop queue %d for node %s\n",
+				qnode->input_queue[i].queue, qnode->name);
+		}
+	}
+
+	error = ktree_for_each_child(&qnode->node, khwq_qos_tree_alloc_nodes,
+				     info);
+
+	return error;
+}
+
+static int khwq_qos_tree_start_port(struct khwq_qos_info *info,
+				      struct khwq_qos_tree_node *qnode)
+{
+	int error, val, idx = qnode->sched_port_idx, temp;
+	struct khwq_device *kdev = info->kdev;
+	bool sync = false;
+	int inputs, i;
+	u64 scale, tmp;
+
+	if (!qnode->has_sched_port)
+		return 0;
+
+	dev_dbg(kdev->dev, "programming sched port index %d for node %s\n",
+		khwq_qos_id_to_idx(idx), qnode->name);
+
+	val = (qnode->acct == QOS_BYTE_ACCT) ? 0xf : 0;
+	error = khwq_qos_set_sched_unit_flags(info, idx, val, sync);
+	if (WARN_ON(error))
+		return error;
+
+	error = khwq_qos_set_sched_group_count(info, idx, 1, sync);
+	if (WARN_ON(error))
+		return error;
+
+	val = qnode->output_queue;
+	error = khwq_qos_set_sched_out_queue(info, idx, val, sync);
+	if (WARN_ON(error))
+		return error;
+
+	val = qnode->overhead_bytes;
+	error = khwq_qos_set_sched_overhead_bytes(info, idx, val, sync);
+	if (WARN_ON(error))
+		return error;
+
+	val = qnode->output_rate / info->ticks_per_sec;
+	error = khwq_qos_set_sched_out_throttle(info, idx, val, sync);
+	if (WARN_ON(error))
+		return error;
+
+	temp = qnode->output_rate / info->ticks_per_sec;
+	val = (qnode->acct == QOS_BYTE_ACCT) ?
+		(temp << QOS_CREDITS_BYTE_SHIFT) :
+		(temp << QOS_CREDITS_PACKET_SHIFT);
+	error = khwq_qos_set_sched_cir_credit(info, idx, val, sync);
+	if (WARN_ON(error))
+		return error;
+
+	temp = qnode->burst_size;
+	val = (qnode->acct == QOS_BYTE_ACCT) ?
+		(temp << QOS_CREDITS_BYTE_SHIFT) :
+		(temp << QOS_CREDITS_PACKET_SHIFT);
+	error = khwq_qos_set_sched_cir_max(info, idx, val, sync);
+	if (WARN_ON(error))
+		return error;
+
+	inputs = (qnode->type == QOS_NODE_DEFAULT) ? 1 : qnode->child_count;
+
+	error = khwq_qos_set_sched_total_q_count(info, idx, inputs, sync);
+	if (WARN_ON(error))
+		return error;
+
+	val = (qnode->type == QOS_NODE_PRIO) ? inputs : 0;
+	error = khwq_qos_set_sched_sp_q_count(info, idx, val, sync);
+	if (WARN_ON(error))
+		return error;
+
+	val = (qnode->type == QOS_NODE_WRR) ? inputs : 0;
+	error = khwq_qos_set_sched_wrr_q_count(info, idx, val, sync);
+	if (WARN_ON(error))
+		return error;
+
+	for (i = 0; i < inputs; i++) {
+		val = 0;
+		error = khwq_qos_set_sched_cong_thresh(info, idx, i, val, sync);
+		if (WARN_ON(error))
+			return error;
+
+		val = 0;
+		if (qnode->type == QOS_NODE_WRR) {
+			tmp = 0;
+			tmp = (qnode->child_weight[i] * inputs * 10) /
+				qnode->child_weight_sum;
+
+			if (qnode->acct == QOS_BYTE_ACCT) {
+				scale = QOS_BYTE_NORMALIZATION_FACTOR;
+				scale <<= 48;
+				tmp *= scale;
+				tmp += 1ll << (47- QOS_CREDITS_BYTE_SHIFT);
+				tmp >>= (48 - QOS_CREDITS_BYTE_SHIFT);
+			} else {
+				scale = QOS_PACKET_NORMALIZATION_FACTOR;
+				scale <<= 48;
+				tmp *= scale;
+				tmp += 1ll << (47 - QOS_CREDITS_PACKET_SHIFT);
+				tmp >>= (48 - QOS_CREDITS_PACKET_SHIFT);
+			}
+			val = (u32)(tmp);
+			val /= 10;
+
+			dev_dbg(kdev->dev, "node weight = %d, weight "
+				 "credits = %d\n", qnode->child_weight[i], val);
+		}
+
+		error = khwq_qos_set_sched_wrr_credit(info, idx, i, val, sync);
+		if (WARN_ON(error))
+			return error;
+	}
+
+	error = khwq_qos_sync_sched_port(info, idx);
+	if (error) {
+		dev_err(kdev->dev, "error writing sched config for %s\n",
+			qnode->name);
+		return error;
+	}
+
+	error = khwq_qos_control_sched_port(info, QOS_CONTROL_ENABLE, idx,
+					    true);
+	if (error) {
+		dev_err(kdev->dev, "error enabling sched port for %s\n",
+			qnode->name);
+		return error;
+	}
+	return 0;
+}
+
+static int khwq_qos_tree_start_drop_out(struct khwq_qos_info *info,
+				      struct khwq_qos_tree_node *qnode)
+{
+	struct khwq_qos_drop_policy *policy = qnode->drop_policy;
+	int error, val, idx = qnode->drop_out_idx;
+	struct khwq_device *kdev = info->kdev;
+	bool sync = false;
+
+	if (!policy)
+		return 0;
+
+	dev_dbg(kdev->dev, "programming drop out index %d for node %s\n",
+		khwq_qos_id_to_idx(idx), qnode->name);
+
+	val = qnode->output_queue;
+	error = khwq_qos_set_drop_out_queue_number(info, idx, val, sync);
+	if (WARN_ON(error))
+		return error;
+
+	val = (policy->max_drop_prob << 16) / 100;
+	error = khwq_qos_set_drop_out_red_prob(info, idx, val, sync);
+	if (WARN_ON(error))
+		return error;
+
+	val = khwq_qos_id_to_idx(policy->drop_cfg_idx);
+	error = khwq_qos_set_drop_out_cfg_prof_idx(info, idx, val, sync);
+	if (WARN_ON(error))
+		return error;
+
+	error = khwq_qos_set_drop_out_enable(info, idx, 1, sync);
+	if (WARN_ON(error))
+		return error;
+
+	error = khwq_qos_set_drop_out_avg_depth(info, idx, 0, sync);
+	if (WARN_ON(error))
+		return error;
+
+	return 0;
+}
+
+static int khwq_qos_tree_start_drop_queue(struct khwq_qos_info *info,
+					    struct khwq_qos_tree_node *qnode)
+{
+	struct khwq_qos_stats_class *class = qnode->stats_class;
+	struct khwq_device *kdev = info->kdev;
+	int i, idx, error;
+	bool sync = false;
+	u32 val;
+
+	if (!qnode->is_drop_input)
+		return 0;
+
+	for (i = 0; i < qnode->num_input_queues; i++) {
+		if (qnode->input_queue[i].valid == false)
+			continue;
+
+		idx = qnode->input_queue[i].drop_queue_idx;
+
+		dev_dbg(kdev->dev, "programming drop queue %d for node %s\n",
+			khwq_qos_id_to_idx(idx), qnode->name);
+
+		val = khwq_qos_id_to_idx(qnode->drop_out_idx);
+		error = khwq_qos_set_drop_q_out_prof_idx(info, idx, val, sync);
+		if (WARN_ON(error))
+			return error;
+
+		error = khwq_qos_set_drop_q_stat_blk_idx(info, idx,
+							 class->stats_block_idx,
+							 sync);
+		if (WARN_ON(error))
+			return error;
+
+		error = khwq_qos_set_drop_q_stat_irq_pair_idx(info, idx,
+							      1, sync);
+		if (WARN_ON(error))
+			return error;
+
+
+		error = khwq_qos_set_drop_q_valid(info, idx, 1, sync);
+		if (WARN_ON(error))
+			return error;
+	}
+	return 0;
+}
+
+static int khwq_qos_tree_start_nodes(struct ktree_node *node, void *arg)
+{
+	struct khwq_qos_info *info = arg;
+	struct khwq_qos_tree_node *qnode = to_qnode(node);
+	struct khwq_device *kdev = info->kdev;
+	int error;
+
+	error = khwq_qos_tree_start_port(info, qnode);
+	if (error)
+		return error;
+
+	error = khwq_qos_tree_start_drop_out(info, qnode);
+	if (error)
+		return error;
+
+	error = khwq_qos_tree_start_drop_queue(info, qnode);
+	if (error)
+		return error;
+
+	error = ktree_for_each_child(&qnode->node,
+				     khwq_qos_tree_start_nodes, info);
+	if (error)
+		dev_err(kdev->dev, "error programming subtree at %s\n",
+			qnode->name);
+	return error;
+
+}
+
+static int khwq_qos_tree_init(struct khwq_qos_info *info)
+{
+	struct ktree_node *root;
+	int error;
+
+	root = ktree_get_root(&info->qos_tree);
+	if (WARN_ON(!root))
+		return -ENODEV;
+
+	error = khwq_qos_tree_map_nodes(root, info);
+	if (WARN_ON(error))
+		return error;
+
+	error = khwq_qos_tree_alloc_nodes(root, info);
+	if (error)
+		goto bail;
+
+	error = 0;
+bail:
+	ktree_put_node(root);
+	return error;
+}
+
+int khwq_qos_tree_start(struct khwq_qos_info *info)
+{
+	struct ktree_node *root;
+	int error;
+
+	root = ktree_get_root(&info->qos_tree);
+	if (WARN_ON(!root))
+		return -ENODEV;
+
+	error = khwq_qos_tree_start_nodes(root, info);
+	if (WARN_ON(error))
+		goto bail;
+
+	error = khwq_qos_sync_drop_queue(info, -1);
+	if (error) {
+		dev_err(info->kdev->dev, "error syncing drop queues\n");
+		goto bail;
+	}
+
+	error = khwq_qos_sync_drop_out(info, -1);
+	if (error) {
+		dev_err(info->kdev->dev, "error syncing drop outs\n");
+		goto bail;
+	}
+	error = 0;
+
+bail:
+	ktree_put_node(root);
+	return error;
+}
+
+static int khwq_qos_stop_drop_queues(struct khwq_qos_info *info)
+{
+	struct khwq_qos_shadow *shadow = &info->shadows[QOS_DROP_QUEUE_CFG];
+	struct khwq_device *kdev = info->kdev;
+	struct khwq_pdsp_info *pdsp;
+	int i, error, idx;
+
+	pdsp = info->pdsp;
+
+	for (i = shadow->start; i < (shadow->start + shadow->count); i++) {
+		if (!test_bit(i, shadow->avail)) {
+			idx = khwq_qos_make_id(pdsp->id, i);
+			error = khwq_qos_set_drop_q_valid(info, idx, 0, false);
+			if (WARN_ON(error))
+				return error;
+
+			error = khwq_qos_set_drop_q_stat_blk_idx(info, idx, 0,
+								 false);
+			if (WARN_ON(error))
+				return error;
+
+			error = khwq_qos_set_drop_q_stat_irq_pair_idx(info, idx,
+								      0, false);
+			if (WARN_ON(error))
+			return error;
+
+			error = khwq_qos_set_drop_q_out_prof_idx(info, idx, 0,
+								 false);
+			if (WARN_ON(error))
+				return error;
+		}
+	}
+
+	error = khwq_qos_sync_drop_queue(info, -1);
+	if (error) {
+		dev_err(kdev->dev, "error syncing drop queues\n");
+		return error;
+	}
+
+	return 0;
+}
+
+static int khwq_qos_stop_drop_outs(struct khwq_qos_info *info)
+{
+	struct khwq_qos_shadow *shadow = &info->shadows[QOS_DROP_OUT_PROF];
+	struct khwq_device *kdev = info->kdev;
+	struct khwq_pdsp_info *pdsp;
+	int i, error, idx;
+
+	pdsp = info->pdsp;
+
+	for (i = shadow->start; i < (shadow->start + shadow->count); i++) {
+		if (!test_bit(i, shadow->avail)) {
+			idx = khwq_qos_make_id(pdsp->id, i);
+			error = khwq_qos_set_drop_out_enable(info, idx, 0,
+							     false);
+			if (WARN_ON(error))
+				return error;
+
+			error = khwq_qos_set_drop_out_queue_number(info, idx, 0,
+							     false);
+			if (WARN_ON(error))
+				return error;
+
+			error = khwq_qos_set_drop_out_red_prob(info, idx, 0,
+							     false);
+			if (WARN_ON(error))
+				return error;
+
+			error = khwq_qos_set_drop_out_cfg_prof_idx(info, idx, 0,
+							     false);
+			if (WARN_ON(error))
+				return error;
+
+			error = khwq_qos_set_drop_out_avg_depth(info, idx, 0,
+							     false);
+			if (WARN_ON(error))
+				return error;
+		}
+	}
+
+	error = khwq_qos_sync_drop_out(info, -1);
+	if (error) {
+		dev_err(kdev->dev, "error syncing drop out\n");
+		return error;
+	}
+
+	return 0;
+}
+
+static int khwq_qos_stop_sched_port_queues(struct khwq_qos_info *info)
+{
+	struct khwq_qos_shadow *shadow = &info->shadows[QOS_SCHED_PORT_CFG];
+	struct khwq_device *kdev = info->kdev;
+	struct khwq_pdsp_info *pdsp;
+	int i, j, error, idx;
+	u32 queues;
+
+	pdsp = info->pdsp;
+
+	for (i = shadow->start; i < (shadow->start + shadow->count); i++) {
+		if (!test_bit(i, shadow->avail)) {
+			idx = khwq_qos_make_id(pdsp->id, i);
+			error = khwq_qos_set_sched_unit_flags(info, idx, 0xf,
+							      false);
+			if (WARN_ON(error))
+				return error;
+
+			error = khwq_qos_get_sched_total_q_count(info, idx,
+								&queues);
+			if (WARN_ON(error))
+				return error;
+
+			for (j = 0; j < queues; j++) {
+				error = khwq_qos_set_sched_cong_thresh(info,
+								       idx, j,
+								       1, false);
+				if (WARN_ON(error))
+					return error;
+
+				error = khwq_qos_set_sched_wrr_credit(info, idx,
+								      j, 0,
+								      false);
+				if (WARN_ON(error))
+					return error;
+			}
+
+			error = khwq_qos_set_sched_out_queue(info, idx, 0,
+							     false);
+			if (WARN_ON(error))
+				return error;
+
+			error = khwq_qos_set_sched_overhead_bytes(info, idx, 0,
+								  false);
+			if (WARN_ON(error))
+				return error;
+
+			error = khwq_qos_set_sched_out_throttle(info, idx, 0,
+								false);
+			if (WARN_ON(error))
+				return error;
+
+			error = khwq_qos_set_sched_cir_credit(info, idx, 0,
+							      false);
+			if (WARN_ON(error))
+				return error;
+
+			error = khwq_qos_set_sched_cir_max(info, idx, 0, false);
+			if (WARN_ON(error))
+				return error;
+
+			error = khwq_qos_set_sched_out_throttle(info, idx, 0,
+								false);
+			if (WARN_ON(error))
+				return error;
+
+			error = khwq_qos_set_sched_total_q_count(info, idx, 0,
+								false);
+			if (WARN_ON(error))
+				return error;
+
+			error = khwq_qos_set_sched_sp_q_count(info, idx, 0,
+								false);
+			if (WARN_ON(error))
+				return error;
+
+			error = khwq_qos_set_sched_wrr_q_count(info, idx, 0,
+								false);
+			if (WARN_ON(error))
+				return error;
+
+			error = khwq_qos_sync_sched_port(info, idx);
+			if (error)
+				return error;
+
+			error = khwq_qos_control_sched_port(info,
+							    QOS_CONTROL_ENABLE,
+							    idx,
+							    false);
+			if (error) {
+				dev_err(kdev->dev, "error disabling sched port "
+					"%d\n", i);
+				return error;
+			}
+		}
+	}
+
+	return 0;
+}
+
+int khwq_qos_tree_stop(struct khwq_qos_info *info)
+{
+	int error;
+
+	error = khwq_qos_stop_sched_port_queues(info);
+	if (error)
+		return error;
+
+	error = khwq_qos_stop_drop_queues(info);
+	if (error)
+		return error;
+
+	error = khwq_qos_stop_drop_outs(info);
+	if (error)
+		return error;
+
+	return 0;
+}
+
+static int khwq_qos_init_base(struct khwq_qos_info *info, int base, int num)
+{
+	struct khwq_device *kdev = info->kdev;
+	int end = base + num;
+	int sched_port_queues, drop_sched_queues;
+
+	base = ALIGN(base, 32);
+	info->drop_sched_queue_base = base;
+	drop_sched_queues = info->shadows[QOS_DROP_QUEUE_CFG].count;
+	base += drop_sched_queues;
+
+	base = ALIGN(base, 32);
+	info->sched_port_queue_base = base;
+	sched_port_queues = (info->shadows[QOS_SCHED_PORT_CFG].count *
+			     info->inputs_per_port);
+	base += sched_port_queues;
+
+	if (base >= end) {
+		dev_err(kdev->dev, "too few queues (%d), need %d + %d\n",
+			num, sched_port_queues, drop_sched_queues);
+		return -ENODEV;
+	} else {
+		dev_info(kdev->dev, "qos: sched port @%d, drop sched @%d\n",
+			 info->sched_port_queue_base,
+			 info->drop_sched_queue_base);
+		return 0;
+	}
+}
+
+static int khwq_qos_init_queue(struct khwq_range_info *range,
+			       struct hwqueue_instance *inst)
+{
+	return 0;
+}
+
+int khwq_qos_start(struct khwq_qos_info *info)
+{
+	struct khwq_device *kdev;
+	u32 command;
+	int error = 0;
+
+	kdev = info->kdev;
+
+	error = khwq_qos_tree_start(info);
+	if (error) {
+		dev_err(kdev->dev, "failed to program qos tree\n");
+		return error;
+	}
+
+	/* Enable the drop scheduler */
+	command = (QOS_CMD_ENABLE_PORT |
+		   QOS_DROP_SCHED_ENABLE | QOS_ENABLE);
+	error = khwq_qos_write_cmd(info, command);
+	if (error)
+		dev_err(kdev->dev, "failed to enable drop scheduler\n");
+
+	init_timer(&info->timer);
+	info->timer.data		= (unsigned long)info;
+	info->timer.function		= khwq_qos_timer;
+	info->timer.expires		= jiffies +
+						KHWQ_QOS_TIMER_INTERVAL;
+	add_timer(&info->timer);
+
+	return error;
+}
+
+static ssize_t khwq_qos_out_prof_read(struct file *filp, char __user *buffer,
+				   size_t count, loff_t *ppos)
+{
+	struct khwq_qos_info *info = filp->private_data;
+	struct khwq_qos_shadow *shadow = &info->shadows[QOS_DROP_OUT_PROF];
+	struct khwq_pdsp_info *pdsp;
+	int i, buf_len = 8192, idx, error;
+	unsigned long flags;
+	size_t len = 0;
+	ssize_t ret;
+	char *buf;
+	u32 temp;
+
+	if (*ppos != 0)
+		return 0;
+	if (count < sizeof(buf))
+		return -ENOSPC;
+
+	buf = kzalloc(buf_len, GFP_KERNEL);
+	if (!buf)
+		return -ENOMEM;
+
+	pdsp = info->pdsp;
+
+	for (i = shadow->start; i < (shadow->start + shadow->count); i++) {
+		if (!test_bit(i, shadow->avail)) {
+			idx = khwq_qos_make_id(pdsp->id, i);
+
+			spin_lock_irqsave(&info->lock, flags);
+
+			len += snprintf(buf + len, buf_len - len,
+					"output profile %d ", i);
+
+			error = khwq_qos_get_drop_out_queue_number(info, idx,
+								   &temp);
+			if (WARN_ON(error))
+				goto free;
+
+			len += snprintf(buf + len, buf_len - len,
+					"output q # %d ", temp);
+
+			error = khwq_qos_get_drop_out_red_prob(info, idx,
+							       &temp);
+			if (WARN_ON(error))
+				goto free;
+
+			len += snprintf(buf + len, buf_len - len,
+					"red prob %d ", temp);
+
+			error = khwq_qos_get_drop_out_enable(info, idx, &temp);
+			if (WARN_ON(error))
+				goto free;
+
+			len += snprintf(buf + len, buf_len - len,
+					"enable %d ", temp);
+
+			error = khwq_qos_get_drop_out_cfg_prof_idx(info, idx,
+								   &temp);
+			if (WARN_ON(error))
+				goto free;
+
+			len += snprintf(buf + len, buf_len - len,
+					"config profile %d ", temp);
+
+			error = khwq_qos_get_drop_out_avg_depth(info, idx,
+								   &temp);
+			if (WARN_ON(error))
+				goto free;
+
+			len += snprintf(buf + len, buf_len - len,
+					"average q depth %d\n", temp);
+
+			spin_unlock_irqrestore(&info->lock, flags);
+		}
+	}
+
+free:
+	ret = simple_read_from_buffer(buffer, len, ppos, buf, buf_len);
+	kfree(buf);
+
+	return ret;
+}
+
+static ssize_t khwq_qos_q_cfg_read(struct file *filp, char __user *buffer,
+				   size_t count, loff_t *ppos)
+{
+	struct khwq_qos_info *info = filp->private_data;
+	struct khwq_qos_shadow *shadow = &info->shadows[QOS_DROP_QUEUE_CFG];
+	struct khwq_pdsp_info *pdsp;
+	int i, buf_len = 4096, idx, error;
+	unsigned long flags;
+	size_t len = 0;
+	ssize_t ret;
+	char *buf;
+	u32 temp;
+
+	if (*ppos != 0)
+		return 0;
+	if (count < sizeof(buf))
+		return -ENOSPC;
+
+	buf = kzalloc(buf_len, GFP_KERNEL);
+	if (!buf)
+		return -ENOMEM;
+
+	pdsp = info->pdsp;
+
+	for (i = shadow->start; i < (shadow->start + shadow->count); i++) {
+		if (!test_bit(i, shadow->avail)) {
+			idx = khwq_qos_make_id(pdsp->id, i);
+
+			spin_lock_irqsave(&info->lock, flags);
+
+			len += snprintf(buf + len, buf_len - len,
+					"q cfg %d ", i);
+
+			error = khwq_qos_get_drop_q_stat_irq_pair_idx(info, idx,
+								      &temp);
+			if (WARN_ON(error))
+				goto free;
+
+			len += snprintf(buf + len, buf_len - len,
+					"stats q pair # %d ", temp);
+
+			error = khwq_qos_get_drop_q_stat_blk_idx(info, idx,
+								 &temp);
+			if (WARN_ON(error))
+				goto free;
+
+			len += snprintf(buf + len, buf_len - len,
+					"block %d ", temp);
+
+			error = khwq_qos_get_drop_q_out_prof_idx(info, idx,
+								 &temp);
+			if (WARN_ON(error))
+				goto free;
+
+			len += snprintf(buf + len, buf_len - len,
+					"out prof %d\n", temp);
+
+			spin_unlock_irqrestore(&info->lock, flags);
+		}
+	}
+
+free:
+	ret = simple_read_from_buffer(buffer, len, ppos, buf, buf_len);
+	kfree(buf);
+
+	return ret;
+}
+
+static ssize_t khwq_qos_drop_prof_read(struct file *filp, char __user *buffer,
+				   size_t count, loff_t *ppos)
+{
+	struct khwq_qos_info *info = filp->private_data;
+	struct khwq_qos_shadow *shadow = &info->shadows[QOS_DROP_CFG_PROF];
+	struct khwq_pdsp_info *pdsp;
+	int i, buf_len = 4096, idx, error;
+	unsigned long flags;
+	size_t len = 0;
+	ssize_t ret;
+	char *buf;
+	u32 temp;
+
+	if (*ppos != 0)
+		return 0;
+	if (count < sizeof(buf))
+		return -ENOSPC;
+
+	buf = kzalloc(buf_len, GFP_KERNEL);
+	if (!buf)
+		return -ENOMEM;
+
+	pdsp = info->pdsp;
+
+	for (i = shadow->start; i < (shadow->start + shadow->count); i++) {
+		if (!test_bit(i, shadow->avail)) {
+			idx = khwq_qos_make_id(pdsp->id, i);
+
+			spin_lock_irqsave(&info->lock, flags);
+
+			len += snprintf(buf + len, buf_len - len,
+					"drop cfg prof %d ", i);
+
+			error = khwq_qos_get_drop_cfg_unit_flags(info, idx,
+								 &temp);
+			if (WARN_ON(error))
+				goto free;
+
+			len += snprintf(buf + len, buf_len - len,
+					"unit flags %d ", temp);
+
+			error = khwq_qos_get_drop_cfg_mode(info, idx, &temp);
+			if (WARN_ON(error))
+				goto free;
+
+			len += snprintf(buf + len, buf_len - len,
+					"mode %d ", temp);
+
+			error = khwq_qos_get_drop_cfg_time_const(info, idx,
+								 &temp);
+			if (WARN_ON(error))
+				goto free;
+
+			len += snprintf(buf + len, buf_len - len,
+					"time const %d ", temp);
+
+			error = khwq_qos_get_drop_cfg_tail_thresh(info, idx,
+								  &temp);
+			if (WARN_ON(error))
+				goto free;
+
+			len += snprintf(buf + len, buf_len - len,
+					"tail thresh %d ", temp);
+
+			error = khwq_qos_get_drop_cfg_red_low(info, idx, &temp);
+			if (WARN_ON(error))
+				goto free;
+
+			len += snprintf(buf + len, buf_len - len,
+					"red low %d ", temp);
+
+			error = khwq_qos_get_drop_cfg_red_high(info, idx,
+								 &temp);
+			if (WARN_ON(error))
+				goto free;
+
+			len += snprintf(buf + len, buf_len - len,
+					"red high %d ", temp);
+
+			error = khwq_qos_get_drop_cfg_thresh_recip(info, idx,
+								 &temp);
+			if (WARN_ON(error))
+				goto free;
+
+			len += snprintf(buf + len, buf_len - len,
+					"thresh recip %d\n", temp);
+
+			spin_unlock_irqrestore(&info->lock, flags);
+		}
+	}
+
+free:
+	ret = simple_read_from_buffer(buffer, len, ppos, buf, buf_len);
+	kfree(buf);
+
+	return ret;
+}
+
+static ssize_t khwq_qos_sched_port_read(struct file *filp, char __user *buffer,
+				   size_t count, loff_t *ppos)
+{
+	struct khwq_qos_info *info = filp->private_data;
+	struct khwq_qos_shadow *shadow = &info->shadows[QOS_SCHED_PORT_CFG];
+	struct khwq_pdsp_info *pdsp;
+	int i, j, buf_len = 4096, idx, error;
+	unsigned long flags;
+	size_t len = 0;
+	ssize_t ret;
+	char *buf;
+	u32 temp, queues;
+
+	if (*ppos != 0)
+		return 0;
+	if (count < sizeof(buf))
+		return -ENOSPC;
+
+	buf = kzalloc(buf_len, GFP_KERNEL);
+	if (!buf)
+		return -ENOMEM;
+
+	pdsp = info->pdsp;
+
+	for (i = shadow->start; i < (shadow->start + shadow->count); i++) {
+		if (!test_bit(i, shadow->avail)) {
+			idx = khwq_qos_make_id(pdsp->id, i);
+
+			spin_lock_irqsave(&info->lock, flags);
+
+			len += snprintf(buf + len, buf_len - len,
+					"port %d\n", i);
+
+			error = khwq_qos_get_sched_unit_flags(info, idx,
+								 &temp);
+			if (WARN_ON(error))
+				goto free;
+
+			len += snprintf(buf + len, buf_len - len,
+					"unit flags %d ", temp);
+
+			error = khwq_qos_get_sched_group_count(info, idx,
+							       &temp);
+			if (WARN_ON(error))
+				goto free;
+
+			len += snprintf(buf + len, buf_len - len,
+					"group # %d ", temp);
+
+			error = khwq_qos_get_sched_out_queue(info, idx,
+								 &temp);
+			if (WARN_ON(error))
+				goto free;
+
+			len += snprintf(buf + len, buf_len - len,
+					"out q %d ", temp);
+
+			error = khwq_qos_get_sched_overhead_bytes(info, idx,
+								  &temp);
+			if (WARN_ON(error))
+				goto free;
+
+			len += snprintf(buf + len, buf_len - len,
+					"overhead bytes %d ", temp);
+
+			error = khwq_qos_get_sched_out_throttle(info, idx,
+								&temp);
+			if (WARN_ON(error))
+				goto free;
+
+			len += snprintf(buf + len, buf_len - len,
+					"throttle thresh %d ", temp);
+
+			error = khwq_qos_get_sched_cir_credit(info, idx,
+								 &temp);
+			if (WARN_ON(error))
+				goto free;
+
+			len += snprintf(buf + len, buf_len - len,
+					"cir credit %d ", temp);
+
+			error = khwq_qos_get_sched_cir_max(info, idx,
+								 &temp);
+			if (WARN_ON(error))
+				goto free;
+
+			len += snprintf(buf + len, buf_len - len,
+					"cir max %d\n", temp);
+
+			error = khwq_qos_get_sched_total_q_count(info, idx,
+								 &queues);
+			if (WARN_ON(error))
+				goto free;
+
+			len += snprintf(buf + len, buf_len - len,
+					"total q's %d ", queues);
+
+			error = khwq_qos_get_sched_sp_q_count(info, idx,
+								 &temp);
+			if (WARN_ON(error))
+				goto free;
+
+			len += snprintf(buf + len, buf_len - len,
+					"sp q's %d ", temp);
+
+			error = khwq_qos_get_sched_wrr_q_count(info, idx,
+								 &temp);
+			if (WARN_ON(error))
+				goto free;
+
+			len += snprintf(buf + len, buf_len - len,
+					"wrr q's %d\n", temp);
+
+			for (j = 0; j < queues; j++) {
+				len += snprintf(buf + len, buf_len - len,
+					"queue %d ", j);
+
+				error = khwq_qos_get_sched_cong_thresh(info,
+								       idx, j,
+								       &temp);
+				if (WARN_ON(error))
+					return error;
+
+				len += snprintf(buf + len, buf_len - len,
+					"cong thresh %d ", temp);
+
+				error = khwq_qos_get_sched_wrr_credit(info, idx,
+								      j, &temp);
+				if (WARN_ON(error))
+					return error;
+
+				len += snprintf(buf + len, buf_len - len,
+					"wrr credit %d\n", temp);
+			}
+
+			len += snprintf(buf + len, buf_len - len, "\n");
+
+			spin_unlock_irqrestore(&info->lock, flags);
+		}
+	}
+free:
+	ret = simple_read_from_buffer(buffer, len, ppos, buf, buf_len);
+	kfree(buf);
+
+	return ret;
+}
+
+static int khwq_qos_debufs_generic_open(struct inode *inode, struct file *filp)
+{
+	filp->private_data = inode->i_private;
+	return 0;
+}
+
+static const struct file_operations khwq_qos_out_profs_fops = {
+	.owner	= THIS_MODULE,
+	.open	= khwq_qos_debufs_generic_open,
+	.read	= khwq_qos_out_prof_read,
+	.llseek	= default_llseek,
+};
+
+static const struct file_operations khwq_qos_q_cfg_fops = {
+	.owner	= THIS_MODULE,
+	.open	= khwq_qos_debufs_generic_open,
+	.read	= khwq_qos_q_cfg_read,
+	.llseek	= default_llseek,
+};
+
+static const struct file_operations khwq_qos_drop_prof_fops = {
+	.owner	= THIS_MODULE,
+	.open	= khwq_qos_debufs_generic_open,
+	.read	= khwq_qos_drop_prof_read,
+	.llseek	= default_llseek,
+};
+
+static const struct file_operations khwq_qos_sched_port_fops = {
+	.owner	= THIS_MODULE,
+	.open	= khwq_qos_debufs_generic_open,
+	.read	= khwq_qos_sched_port_read,
+	.llseek	= default_llseek,
+};
+
+static int khwq_qos_open_queue(struct khwq_range_info *range,
+			       struct hwqueue_instance *inst, unsigned flags)
+{
+	struct khwq_qos_info *info;
+	struct khwq_device *kdev;
+	int error = 0;
+
+	info = range->qos_info;
+	kdev = info->kdev;
+
+	info->refcount++;
+	if (info->refcount == 1) {
+		error = khwq_qos_start(info);
+		if (error)
+			dev_err(kdev->dev, "failed to start qos\n");
+
+		info->port_configs = debugfs_create_file("sched_ports", S_IRUSR,
+							 info->root_dir, info,
+							 &khwq_qos_sched_port_fops);
+
+		info->out_profiles = debugfs_create_file("out_profiles",
+							 S_IRUSR,
+							 info->root_dir, info,
+							 &khwq_qos_out_profs_fops);
+	
+		info->queue_configs = debugfs_create_file("queue_configs",
+							  S_IRUSR,
+							  info->root_dir, info,
+							  &khwq_qos_q_cfg_fops);
+
+		info->config_profiles = debugfs_create_file("config_profiles",
+							    S_IRUSR,
+							    info->root_dir,
+							    info,
+							    &khwq_qos_drop_prof_fops);
+
+	}
+
+	return error;
+}
+
+int khwq_qos_stop(struct khwq_qos_info *info)
+{
+	struct khwq_device *kdev;
+	u32 command;
+	int error = 0;
+
+	kdev = info->kdev;
+
+	/* Disable the drop scheduler */
+	command = (QOS_CMD_ENABLE_PORT |
+		   QOS_DROP_SCHED_ENABLE | QOS_DISABLE);
+	error = khwq_qos_write_cmd(info, command);
+	if (error)
+		dev_err(kdev->dev, "failed to disable "
+			"drop scheduler\n");
+
+	error = khwq_qos_tree_stop(info);
+	if (error) {
+		dev_err(kdev->dev, "failed to close qos tree\n");
+		return error;
+	}
+
+	del_timer_sync(&info->timer);
+
+	return error;
+}
+
+static int khwq_qos_close_queue(struct khwq_range_info *range,
+				struct hwqueue_instance *inst)
+{
+	struct khwq_qos_info *info;
+	struct khwq_device *kdev;
+	int error = 0;
+
+	info = range->qos_info;
+	kdev = info->kdev;
+
+	info->refcount--;
+	if (!info->refcount) {
+		error = khwq_qos_stop(info);
+		if (error)
+			dev_err(kdev->dev, "failed to stop qos\n");
+
+		debugfs_remove(info->port_configs);
+		debugfs_remove(info->queue_configs);
+		debugfs_remove(info->out_profiles);
+		debugfs_remove(info->config_profiles);
+	}
+
+	return error;
+}
+
+static int khwq_qos_free_range(struct khwq_range_info *range)
+{
+	struct khwq_qos_info *info;
+	struct khwq_qos_shadow *shadow;
+	struct khwq_device *kdev;
+	struct khwq_pdsp_info *pdsp;
+	int i, idx;
+
+	info = range->qos_info;
+	pdsp = info->pdsp;
+	kdev = info->kdev;
+
+	debugfs_remove_recursive(info->root_dir);
+
+	shadow = &info->shadows[QOS_SCHED_PORT_CFG];
+	for (i = shadow->start; i < (shadow->start + shadow->count); i++)
+		if (!test_bit(i, shadow->avail)) {
+			idx = khwq_qos_make_id(pdsp->id, i);
+			khwq_qos_free_sched_port(info, idx);
+		}
+
+	shadow = &info->shadows[QOS_DROP_OUT_PROF];
+	for (i = shadow->start; i < (shadow->start + shadow->count); i++)
+		if (!test_bit(i, shadow->avail)) {
+			idx = khwq_qos_make_id(pdsp->id, i);
+			khwq_qos_free_drop_out(info, idx);
+		}
+
+	shadow = &info->shadows[QOS_DROP_QUEUE_CFG];
+	for (i = shadow->start; i < (shadow->start + shadow->count); i++)
+		if (!test_bit(i, shadow->avail)) {
+			idx = khwq_qos_make_id(pdsp->id, i);
+			khwq_qos_free_drop_queue(info, idx);
+		}
+
+	shadow = &info->shadows[QOS_DROP_CFG_PROF];
+	for (i = shadow->start; i < (shadow->start + shadow->count); i++)
+		if (!test_bit(i, shadow->avail)) {
+			idx = khwq_qos_make_id(pdsp->id, i);
+			khwq_qos_free_drop_cfg(info, idx);
+		}
+
+	return 0;
+}
+
+static int khwq_qos_set_notify(struct khwq_range_info *range,
+			       struct hwqueue_instance *inst, bool enabled)
+{
+	return 0;
+}
+
+static int khwq_qos_push(struct hwqueue_instance *inst, dma_addr_t dma,
+			 unsigned size, unsigned flags)
+{
+	struct khwq_instance *kq = hwqueue_inst_to_priv(inst);
+	struct khwq_range_info *range = kq->range;
+	unsigned id = hwqueue_inst_to_id(inst);
+	struct khwq_qmgr_info *qmgr;
+	struct khwq_pdsp_info *pdsp;
+	struct khwq_qos_info *info;
+	unsigned long irq_flags;
+	u32 val;
+
+	qmgr = khwq_find_qmgr(inst);
+	if (!qmgr)
+		return -ENODEV;
+
+	info = range->qos_info;
+	pdsp = info->pdsp;
+
+	spin_lock_irqsave(&info->lock, irq_flags);
+
+	while(__raw_readl(pdsp->command + QOS_PUSH_PROXY_OFFSET + 0x4));
+	val = (id << 16) | (flags & BITS(17));
+	__raw_writel(val, pdsp->command + QOS_PUSH_PROXY_OFFSET);
+	
+	val = (u32)dma | ((size / 16) - 1);
+	__raw_writel(val, pdsp->command + QOS_PUSH_PROXY_OFFSET + 0x4);
+
+	spin_unlock_irqrestore(&info->lock, irq_flags);
+
+	return 0;
+}
+
+static int khwq_qos_init_range(struct khwq_range_info *range)
+{
+	struct khwq_pdsp_info *pdsp;
+	struct khwq_qos_info *info;
+	struct khwq_device *kdev;
+	u32 command, magic, version;
+	int error, i, idx, timer_config;
+	char name[24];
+
+	info = range->qos_info;
+	pdsp = info->pdsp;
+	kdev = info->kdev;
+
+	range->inst_ops.push = khwq_qos_push;
+
+	snprintf(name, sizeof(name), "qos-%d", pdsp->id);
+
+	spin_lock_bh(&info->lock);
+
+	magic = __raw_readl(pdsp->command + QOS_MAGIC_OFFSET);
+	version = __raw_readl(pdsp->command + QOS_VERSION_OFFSET);
+
+	if ((magic >> 16) != QOS_MAGIC_DROPSCHED) {
+		dev_err(kdev->dev, "invalid qos magic word %x\n", magic);
+		error = -EINVAL;
+		goto fail;
+	}
+
+	dev_info(kdev->dev, "qos version 0x%x, magic %s\n", version,
+		 ((magic >> 16) == QOS_MAGIC_DROPSCHED) ? "valid" : "invalid");
+
+	for (i = 0 ; i < info->shadows[QOS_SCHED_PORT_CFG].count; i++) {
+		idx = khwq_qos_make_id(pdsp->id, i);
+		__khwq_qos_set_sched_overhead_bytes(info, idx,
+						    QOS_DEFAULT_OVERHEAD_BYTES,
+						    false);
+	}
+
+	for (i = 0 ; i < info->shadows[QOS_DROP_CFG_PROF].count; i++) {
+		idx = khwq_qos_make_id(pdsp->id, i);
+		__khwq_qos_set_drop_cfg_tail_thresh(info, idx, -1, false);
+	}
+
+	/* command for drop scheduler base */
+	command = (QOS_CMD_SET_QUEUE_BASE | QOS_QUEUE_BASE_DROP_SCHED |
+		   (info->drop_sched_queue_base << 16));
+	error = khwq_qos_write_cmd(info, command);
+	if (error) {
+		dev_err(kdev->dev, "failed to set drop sched base\n");
+		goto fail;
+	}
+
+	/* command for qos scheduler base */
+	command = (QOS_CMD_SET_QUEUE_BASE | QOS_QUEUE_BASE_QOS_SCHED |
+		   (info->sched_port_queue_base << 16));
+	error = khwq_qos_write_cmd(info, command);
+	if (error) {
+		dev_err(kdev->dev, "failed to set qos sched base\n");
+		goto fail;
+	}
+
+	/* calculate the timer config from the pdsp tick */
+	timer_config = (QMSS_CLK_RATE / info->ticks_per_sec);
+	timer_config /= 2;
+	command = (QOS_CMD_SET_TIMER_CONFIG | ((timer_config & 0xffff) << 16));
+	error = khwq_qos_write_cmd(info, command);
+	if (error) {
+		dev_err(kdev->dev, "failed to set timer\n");
+		goto fail;
+	}
+
+	error = khwq_qos_program_drop_sched(info);
+	if (error) {
+		dev_err(kdev->dev, "failed to initialize drop scheduler\n");
+		goto fail;
+	}
+
+	spin_unlock_bh(&info->lock);
+
+	error = khwq_program_drop_policies(info);
+	if (error) {
+		dev_err(kdev->dev, "failed to initialize drop policies\n");
+		goto fail;
+	}
+
+	info->root_dir = debugfs_create_dir(name, NULL);
+	if (!info->root_dir)
+		goto fail;
+
+	return 0;
+fail:
+	spin_unlock_bh(&info->lock);
+	return error;
+}
+
+struct khwq_range_ops khwq_qos_range_ops = {
+	.set_notify	= khwq_qos_set_notify,
+	.init_queue	= khwq_qos_init_queue,
+	.open_queue	= khwq_qos_open_queue,
+	.close_queue	= khwq_qos_close_queue,
+	.init_range	= khwq_qos_init_range,
+	.free_range	= khwq_qos_free_range,
+};
+
+int khwq_init_qos_range(struct khwq_device *kdev, struct device_node *node,
+			struct khwq_range_info *range)
+{
+	struct khwq_pdsp_info *pdsp = NULL;
+	struct khwq_qos_info *info;
+	struct device_node *child;
+	struct device *dev = kdev->dev;
+	u32 temp[7];
+	int error;
+
+	info = devm_kzalloc(kdev->dev, sizeof(*info), GFP_KERNEL);
+	if (!info) {
+		dev_err(kdev->dev, "failed to alloc qos info\n");
+		return -ENOMEM;
+	}
+
+	info->kobj = kobject_create_and_add(node->name,
+					kobject_get(&dev->kobj));
+	if (!info->kobj) {
+		dev_err(kdev->dev, "could not create sysfs entries for qos\n");
+		devm_kfree(kdev->dev, info);
+		return -ENODEV;
+	}
+
+	info->kobj_stats = kobject_create_and_add("statistics", info->kobj);
+	if (!info->kobj_stats) {
+		dev_err(kdev->dev, "could not create sysfs entries for "
+			"qos statistics\n");
+		devm_kfree(kdev->dev, info);
+		return -ENODEV;
+	}
+
+	info->kobj_policies = kobject_create_and_add("drop-policies",
+						     info->kobj);
+	if (!info->kobj_policies) {
+		dev_err(kdev->dev, "could not create sysfs entries for "
+			"qos drop policies\n");
+		devm_kfree(kdev->dev, info);
+		return -ENODEV;
+	}
+	
+	error = of_property_read_u32(node, "pdsp-id", &info->pdsp_id);
+	if (error < 0) {
+		dev_err(kdev->dev, "pdsp id must be specified\n");
+		devm_kfree(kdev->dev, info);
+		return error;
+	}
+
+	pdsp = khwq_find_pdsp(kdev, info->pdsp_id);
+	if (!pdsp) {
+		dev_err(kdev->dev, "pdsp id %d not found for range %s\n",
+			info->pdsp_id, range->name);
+		devm_kfree(kdev->dev, info);
+		return -ENODEV;
+	}
+
+	if (pdsp->qos_info) {
+		dev_err(kdev->dev, "pdsp id %d is busy\n", info->pdsp_id);
+		devm_kfree(kdev->dev, info);
+		return -EBUSY;
+	}
+
+	info->pdsp = pdsp;
+	info->kdev = kdev;
+	INIT_LIST_HEAD(&info->drop_policies);
+	INIT_LIST_HEAD(&info->stats_classes);
+	spin_lock_init(&info->lock);
+	/* TODO: add refcount handlers */
+	ktree_init(&info->qos_tree, khwq_qos_get_tree_node,
+		   khwq_qos_put_tree_node);
+
+	error = of_property_read_u32_array(node, "qos-cfg", temp, 7);
+	if (error < 0) {
+		dev_err(kdev->dev, "failed to obtain qos scheduler config\n");
+		goto bail;
+	}
+	info->inputs_per_port	  = temp[0];
+	info->drop_cfg.int_num	  = temp[1];
+	info->drop_cfg.qos_ticks  = temp[2];
+	info->drop_cfg.drop_ticks = temp[3];
+	info->drop_cfg.seed[0]	  = temp[4];
+	info->drop_cfg.seed[1]	  = temp[5];
+	info->drop_cfg.seed[2]	  = temp[6];
+
+	error = of_property_read_u32(node, "tick-per-sec",
+				     &info->ticks_per_sec);
+	if (error < 0)
+		info->ticks_per_sec = 10000;
+
+	error = khwq_qos_init_shadow(info, QOS_SCHED_PORT_CFG,
+				     "sched-port-configs", node, false);
+	if (error)
+		goto bail;
+
+	error = khwq_qos_init_shadow(info, QOS_DROP_CFG_PROF,
+				     "drop-cfg-profiles", node, true);
+	if (error)
+		goto bail;
+
+	error = khwq_qos_init_shadow(info, QOS_DROP_OUT_PROF,
+				     "drop-out-profiles", node, true);
+	if (error)
+		goto bail;
+
+	error = khwq_qos_init_shadow(info, QOS_DROP_QUEUE_CFG,
+				     "drop-queue-configs", node, true);
+	if (error)
+		goto bail;
+
+	error = khwq_qos_init_stats(info, node);
+	if (error)
+		goto bail;
+
+	error = khwq_qos_init_base(info, range->queue_base, range->num_queues);
+	if (error)
+		goto bail;
+
+	pdsp->qos_info  = info;
+	range->qos_info = info;
+
+	child = of_parse_phandle(node, "drop-policies", 0);
+	if (!child)
+		child = of_get_child_by_name(node, "drop-policies");
+	if (!child) {
+		dev_err(kdev->dev, "could not find drop policies\n");
+		goto bail;
+	}
+	error = khwq_qos_get_drop_policies(kdev, info, child);
+	if (error)
+		goto bail;
+	of_node_put(child);
+
+	child = of_parse_phandle(node, "qos-tree", 0);
+	if (!child)
+		child = of_get_child_by_name(node, "qos-tree");
+	if (!child) {
+		dev_err(kdev->dev, "could not find qos tree\n");
+		goto bail;
+	}
+	error = khwq_qos_tree_parse(info, child, NULL);
+	if (error)
+		goto bail;
+	of_node_put(child);
+
+	error = khwq_qos_tree_init(info);
+	if (error)
+		goto bail;
+
+	range->ops = &khwq_qos_range_ops;
+
+	return 0;
+
+bail:
+	__khwq_free_qos_range(kdev, info);
+
+	range->qos_info	= NULL;
+	range->ops	= NULL;
+	if (pdsp)
+		pdsp->qos_info	= NULL;
+
+	return error;
+}
diff --git a/drivers/hwqueue/keystone_qos.h b/drivers/hwqueue/keystone_qos.h
new file mode 100644
index 0000000..251673e
--- /dev/null
+++ b/drivers/hwqueue/keystone_qos.h
@@ -0,0 +1,393 @@
+/*
+ * Keystone Accumulator driver
+ *
+ * Copyright (C) 2012 Texas Instruments Incorporated - http://www.ti.com
+ * Contact: Sandeep Paulraj <s-paulraj@ti.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ */
+
+#ifndef __KEYSTONE_QOS_FW_H
+#define __KEYSTONE_QOS_FW_H
+
+#define QOS_MAX_INPUTS			128
+
+#define QMSS_CLK_RATE			327680000
+
+#define QOS_RETCODE_SUCCESS		1
+
+#define QOS_COMMAND_TIMEOUT		20	/* msecs */
+#define QOS_COMMAND_DELAY		10	/* usecs */
+
+#define QOS_CMD_GET_QUEUE_BASE		0x80
+#define QOS_CMD_SET_QUEUE_BASE		0x81
+#define QOS_CMD_SET_TIMER_CONFIG	0x82
+#define QOS_CMD_ENABLE_PORT		0x90
+#define QOS_CMD_PORT_SHADOW		0x91
+#define QOS_CMD_STATS_REQUEST		0x92
+
+#define QOS_MAGIC_DROPSCHED		0x8020
+
+#define QOS_QUEUE_BASE_DROP_SCHED	BIT(8)
+#define QOS_QUEUE_BASE_QOS_SCHED	0
+
+#define QOS_ENABLE			BIT(8)
+#define QOS_DISABLE			0
+
+#define QOS_COPY_ACTIVE_TO_SHADOW	0
+#define	QOS_COPY_SHADOW_TO_ACTIVE	BIT(8)
+
+#define QOS_DROP_SCHED_CFG		4
+#define QOS_DROP_SCHED_ENABLE		BIT(24)
+
+#define QOS_DEFAULT_OVERHEAD_BYTES	24
+
+#define QOS_CREDITS_PACKET_SHIFT	20
+#define QOS_CREDITS_BYTE_SHIFT		11
+
+#define QOS_BYTE_NORMALIZATION_FACTOR	3000
+#define QOS_PACKET_NORMALIZATION_FACTOR	2
+
+#define	QOS_MAX_WEIGHT			SZ_1M
+
+#define to_qnode(_n)	container_of(_n, struct khwq_qos_tree_node, node)
+
+enum khwq_qos_normalize {
+	DIVIDE,
+	MULTIPLY
+};
+
+enum khwq_qos_accounting_type {
+	QOS_PACKET_ACCT,
+	QOS_BYTE_ACCT
+};
+
+enum khwq_qos_drop_mode {
+	QOS_TAILDROP,
+	QOS_RED,
+};
+
+enum khwq_qos_tree_node_type {
+	QOS_NODE_PRIO,
+	QOS_NODE_WRR,
+	QOS_NODE_DEFAULT,
+};
+
+enum khwq_qos_shadow_type {
+	QOS_SCHED_PORT_CFG,
+	QOS_DROP_CFG_PROF,
+	QOS_DROP_QUEUE_CFG,
+	QOS_DROP_OUT_PROF,
+
+	QOS_MAX_SHADOW	/* last */
+};
+
+enum khwq_qos_control_type {
+	QOS_CONTROL_ENABLE,
+	QOS_CONTROL_GET_INPUT,
+};
+
+struct khwq_qos_shadow {
+	enum khwq_qos_shadow_type	  type;
+	struct khwq_qos_info		 *info;
+	const char			 *name;
+	void				 *data;
+	unsigned long			 *dirty;
+	unsigned long			 *avail;
+	unsigned long			 *running;
+	int				  start, count, size;
+	int		(*sync)(struct khwq_qos_shadow *shadow, int idx);
+	int		(*control)(struct khwq_qos_shadow *shadow,
+				   enum khwq_qos_control_type ctrl,
+				   int idx, u32 arg);
+};
+
+struct khwq_qos_stats {
+	enum khwq_qos_shadow_type	  type;
+	const char			 *name;
+	void				 *data;
+	unsigned long			 *dirty;
+	unsigned long			 *avail;
+	unsigned long			 *running;
+	int				  start, count, size;
+	int		(*get_stats)(struct khwq_qos_stats *stats, int idx);
+};
+
+struct khwq_qos_drop_policy {
+	const char			*name;
+	struct khwq_qos_info		*info;
+	bool				 usecount;
+	enum khwq_qos_accounting_type	 acct;
+	enum khwq_qos_drop_mode		 mode;
+	u32				 limit;
+	u32				 red_low;
+	u32				 red_high;
+	u32				 half_life;
+	u32				 max_drop_prob;
+	int				 drop_cfg_idx;
+	struct list_head		 list;
+	struct kobject			 kobj;
+};
+
+struct khwq_qos_info {
+	spinlock_t			 lock;
+	struct khwq_device		*kdev;
+	struct khwq_pdsp_info		*pdsp;
+	u32				 refcount;
+	struct khwq_qos_shadow		 shadows[QOS_MAX_SHADOW];
+	struct khwq_qos_stats		 stats;
+	struct ktree			 qos_tree;
+	struct list_head		 drop_policies;
+	struct list_head		 stats_classes;
+	struct khwq_qos_drop_policy	*default_drop_policy;
+	
+	struct dentry			*root_dir;
+	struct dentry			*config_profiles;
+	struct dentry			*out_profiles;
+	struct dentry			*queue_configs;
+	struct dentry			*port_configs;
+
+	int	 sched_port_queue_base,
+		 drop_sched_queue_base,
+		 inputs_per_port,
+		 ticks_per_sec,
+		 pdsp_id;
+
+	struct {
+		u8		 int_num;
+		u8		 qos_ticks;
+		u8		 drop_ticks;
+		u32		 seed[3];
+	} drop_cfg;
+
+	struct timer_list		 timer;
+
+	struct kobject			 *kobj;
+	struct kobject			 *kobj_stats;
+	struct kobject			 *kobj_policies;
+};
+
+struct khwq_qos_stats_class {
+	const char			*name;
+	struct khwq_qos_info		*info;
+	struct list_head		 list;
+	int				 stats_block_idx;
+	bool				 usecount;
+	struct kobject			 kobj;
+};
+
+struct khwq_qos_input_queue {
+	bool				 valid;
+	u32				 queue;
+	int				 drop_queue_idx;
+};
+
+struct khwq_qos_tree_node {
+	struct khwq_qos_tree_node	*parent;
+	struct khwq_qos_info		*info;
+	struct ktree_node		 node;
+	enum khwq_qos_tree_node_type	 type;
+	u32				 weight;
+	u32				 priority;
+	enum khwq_qos_accounting_type	 acct;
+	const char			*name;
+	int				 overhead_bytes;
+	int				 output_rate;
+	int				 burst_size;
+	int				 num_input_queues;
+	u32				 input_queues[QOS_MAX_INPUTS];
+	struct khwq_qos_input_queue	 input_queue[QOS_MAX_INPUTS];
+	struct khwq_qos_drop_policy	*drop_policy;
+	struct khwq_qos_stats_class	*stats_class;
+
+	int	 child_port_count;	/* children that need ports	*/
+	int	 child_count;		/* number of children		*/
+	int	 parent_input;		/* input number of parent	*/
+	u32	 child_weight_min;	/* min of child weights		*/
+	u32	 child_weight_max;	/* max of child weights		*/
+	u32	 child_weight[4];
+	u32	 child_weight_sum;	/* sum of child weights		*/
+	bool	 is_drop_input;		/* indicates that child's output
+					   feeds to the drop sched	*/
+	bool	 has_sched_port;	/* does this port need a sched?	*/
+	int	 output_queue;		/* from DT or calculated	*/
+
+	/* allocated resources */
+	int	 sched_port_idx;	/* inherited by default nodes	*/
+	int	 drop_out_idx;		/* inherited by default nodes	*/
+	int	 drop_queue_idx[QOS_MAX_INPUTS];
+
+	struct kobject			 kobj;
+};
+
+struct khwq_semaphore_regs {
+	u32		sem;
+};
+
+struct khwq_push_stats_regs {
+	u32		stats_info;
+	u32		bytes_forwarded;
+	u32		bytes_discarded;
+	u32		packets_forwarded;
+	u32		packets_discarded;
+};
+
+struct khwq_query_stats_regs {
+	u32		bytes_forwarded_lsw;
+	u32		bytes_forwarded_msw;
+	u32		bytes_discarded_lsw;
+	u32		bytes_discarded_msw;
+	u32		packets_forwarded;
+	u32		packets_discarded;
+};
+
+#define QOS_SHADOW_OFFSET	0x40
+#define QOS_PUSH_PROXY_OFFSET	0x2e0
+#define QOS_STATS_OFFSET	0x300
+#define QOS_MAGIC_OFFSET	0x1ff8
+#define QOS_VERSION_OFFSET	0x1ffc
+
+#define khwq_qos_id_to_idx(idx)		((idx) & 0xffff)
+#define khwq_qos_id_to_pdsp(idx)	((idx) >> 16)
+#define khwq_qos_make_id(pdsp, idx)	((pdsp) << 16 | (idx))
+#define khwq_qos_id_to_queue(info, idx)		\
+	((info)->drop_sched_queue_base + khwq_qos_id_to_idx(idx))
+
+int khwq_qos_alloc(struct khwq_qos_info *info, enum khwq_qos_shadow_type type);
+int khwq_qos_free(struct khwq_qos_info *info, enum khwq_qos_shadow_type type,
+		    int idx);
+int khwq_qos_control(struct khwq_qos_info *info, enum khwq_qos_shadow_type type,
+		       enum khwq_qos_control_type ctrl, int idx, u32 arg,
+		       bool internal);
+int khwq_qos_sync(struct khwq_qos_info *info, enum khwq_qos_shadow_type type,
+		    int idx, bool internal);
+int khwq_qos_get(struct khwq_qos_info *info, enum khwq_qos_shadow_type type,
+		   const char *name, int idx, int offset, int startbit,
+		   int nbits, u32 *value);
+int khwq_qos_set(struct khwq_qos_info *info, enum khwq_qos_shadow_type type,
+		   const char *name, int idx, int offset, int startbit,
+		   int nbits, bool sync, u32 value, bool internal);
+
+#define DEFINE_SHADOW(_type, _field)					       \
+static inline int khwq_qos_control_##_field(struct khwq_qos_info *info,        \
+					    enum khwq_qos_control_type ctrl,   \
+					    int idx, u32 arg)		       \
+{									       \
+	return khwq_qos_control(info, _type, ctrl, idx, arg, false);	       \
+}									       \
+static inline int khwq_qos_sync_##_field(struct khwq_qos_info *info,	       \
+					 int idx)			       \
+{									       \
+	return khwq_qos_sync(info, _type, idx, false);		       \
+}
+
+DEFINE_SHADOW(QOS_DROP_CFG_PROF,	drop_cfg);
+DEFINE_SHADOW(QOS_DROP_OUT_PROF,	drop_out);
+DEFINE_SHADOW(QOS_SCHED_PORT_CFG,	sched_port);
+DEFINE_SHADOW(QOS_DROP_QUEUE_CFG,	drop_queue);
+
+#define DEFINE_ALLOC(_type, _field)					       \
+static inline int khwq_qos_alloc_##_field(struct khwq_qos_info *info)	       \
+{									       \
+	return khwq_qos_alloc(info, _type);				       \
+}									       \
+static inline int khwq_qos_free_##_field(struct khwq_qos_info *info,	       \
+					 int idx)			       \
+{									       \
+	return khwq_qos_free(info, _type, idx);			       \
+}
+
+DEFINE_ALLOC(QOS_DROP_CFG_PROF,	 drop_cfg);
+DEFINE_ALLOC(QOS_DROP_OUT_PROF,	 drop_out);
+DEFINE_ALLOC(QOS_SCHED_PORT_CFG, sched_port);
+
+#define DEFINE_FIELD_U32(_type, _field, _offset, _startbit, _nbits)	 \
+static inline int khwq_qos_get_##_field(struct khwq_qos_info *info,	 \
+					int idx, u32 *value)		 \
+{									 \
+	return khwq_qos_get(info, _type, #_field, idx, _offset,	 \
+			      _startbit, _nbits, value);		 \
+}									 \
+static inline int khwq_qos_set_##_field(struct khwq_qos_info *info,	 \
+					int idx, u32 value, bool sync)	 \
+{									 \
+	return khwq_qos_set(info, _type, #_field, idx, _offset,	 \
+			      _startbit, _nbits, sync, value, false);	 \
+}									 \
+static inline int __khwq_qos_set_##_field(struct khwq_qos_info *info,	 \
+					int idx, u32 value, bool sync)	 \
+{									 \
+	return khwq_qos_set(info, _type, #_field, idx, _offset,	 \
+			      _startbit, _nbits, sync, value, true);	 \
+}
+
+#define DEFINE_FIELD_U32_ARRAY(_type, _field, _offset, _size)		 \
+static inline int khwq_qos_get_##_field(struct khwq_qos_info *info,	 \
+					int idx, int elem, u32 *value)	 \
+{									 \
+	int ofs = _offset + elem * _size;				 \
+	return khwq_qos_get(info, _type, #_field, idx, ofs, 0, 32,	 \
+			      value);					 \
+}									 \
+static inline int khwq_qos_set_##_field(struct khwq_qos_info *info,	 \
+				int idx, int elem, u32 value, bool sync) \
+{									 \
+	int ofs = _offset + elem * _size;				 \
+	return khwq_qos_set(info, _type, #_field, idx, ofs, 0, 32,	 \
+			      sync, value, false);			 \
+}									 \
+static inline int __khwq_qos_set_##_field(struct khwq_qos_info *info,	 \
+				int idx, int elem, u32 value, bool sync) \
+{									 \
+	int ofs = _offset + elem * _size;				 \
+	return khwq_qos_set(info, _type, #_field, idx, ofs, 0, 32,	 \
+			      sync, value, true);			 \
+}
+
+DEFINE_FIELD_U32(QOS_DROP_CFG_PROF, drop_cfg_unit_flags,   0x00,  0,  8)
+DEFINE_FIELD_U32(QOS_DROP_CFG_PROF, drop_cfg_mode,         0x00,  8,  8)
+DEFINE_FIELD_U32(QOS_DROP_CFG_PROF, drop_cfg_time_const,   0x00, 16,  8)
+DEFINE_FIELD_U32(QOS_DROP_CFG_PROF, drop_cfg_tail_thresh,  0x04,  0, 32)
+DEFINE_FIELD_U32(QOS_DROP_CFG_PROF, drop_cfg_red_low,      0x08,  0, 32)
+DEFINE_FIELD_U32(QOS_DROP_CFG_PROF, drop_cfg_red_high,     0x0c,  0, 32)
+DEFINE_FIELD_U32(QOS_DROP_CFG_PROF, drop_cfg_thresh_recip, 0x10,  0, 32)
+
+DEFINE_FIELD_U32(QOS_DROP_OUT_PROF, drop_out_queue_number, 0x00,  0, 16)
+DEFINE_FIELD_U32(QOS_DROP_OUT_PROF, drop_out_red_prob,     0x00, 16, 16)
+DEFINE_FIELD_U32(QOS_DROP_OUT_PROF, drop_out_cfg_prof_idx, 0x04,  0,  8)
+DEFINE_FIELD_U32(QOS_DROP_OUT_PROF, drop_out_enable,       0x04,  8,  8)
+DEFINE_FIELD_U32(QOS_DROP_OUT_PROF, drop_out_avg_depth,    0x08,  0, 32)
+
+DEFINE_FIELD_U32(QOS_DROP_QUEUE_CFG, drop_q_out_prof_idx,  0x00,  0,  8)
+DEFINE_FIELD_U32(QOS_DROP_QUEUE_CFG, drop_q_stat_blk_idx,  0x00,  8,  8)
+DEFINE_FIELD_U32(QOS_DROP_QUEUE_CFG, drop_q_stat_irq_pair_idx, 0x00, 16,  8)
+DEFINE_FIELD_U32(QOS_DROP_QUEUE_CFG, drop_q_valid,         0x00, 24,  8)
+
+DEFINE_FIELD_U32(QOS_SCHED_PORT_CFG, sched_unit_flags,     0x00,  0,  8)
+DEFINE_FIELD_U32(QOS_SCHED_PORT_CFG, sched_group_count,    0x00,  8,  8)
+DEFINE_FIELD_U32(QOS_SCHED_PORT_CFG, sched_out_queue,      0x00, 16, 16)
+DEFINE_FIELD_U32(QOS_SCHED_PORT_CFG, sched_overhead_bytes, 0x04,  0,  8)
+DEFINE_FIELD_U32(QOS_SCHED_PORT_CFG, sched_out_throttle,   0x04, 16, 16)
+DEFINE_FIELD_U32(QOS_SCHED_PORT_CFG, sched_cir_credit,     0x08,  0, 32)
+DEFINE_FIELD_U32(QOS_SCHED_PORT_CFG, sched_cir_max,        0x0c,  0, 32)
+DEFINE_FIELD_U32(QOS_SCHED_PORT_CFG, sched_total_q_count,  0x24,  0,  8)
+DEFINE_FIELD_U32(QOS_SCHED_PORT_CFG, sched_sp_q_count,     0x24,  8,  8)
+DEFINE_FIELD_U32(QOS_SCHED_PORT_CFG, sched_wrr_q_count,    0x24, 16,  8)
+
+DEFINE_FIELD_U32_ARRAY(QOS_SCHED_PORT_CFG, sched_wrr_credit,  0x28, 0x8)
+DEFINE_FIELD_U32_ARRAY(QOS_SCHED_PORT_CFG, sched_cong_thresh, 0x2c, 0x8)
+
+int khwq_qos_start(struct khwq_qos_info *info);
+int khwq_qos_stop(struct khwq_qos_info *info);
+int khwq_qos_tree_start(struct khwq_qos_info *info);
+int khwq_qos_tree_stop(struct khwq_qos_info *info);
+
+#endif
+
diff --git a/include/linux/hwqueue.h b/include/linux/hwqueue.h
new file mode 100644
index 0000000..3b4e3f2
--- /dev/null
+++ b/include/linux/hwqueue.h
@@ -0,0 +1,223 @@
+/*
+ * Hardware queue framework
+ *
+ * Copyright (C) 2011 Texas Instruments Incorporated - http://www.ti.com
+ *
+ * Contact: Prabhu Kuttiyam <pkuttiyam@ti.com>
+ *	    Cyril Chemparathy <cyril@ti.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation version 2.
+ *
+ * This program is distributed "as is" WITHOUT ANY WARRANTY of any
+ * kind, whether express or implied; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef __LINUX_HWQUEUE_H
+#define __LINUX_HWQUEUE_H
+
+#include <linux/err.h>
+#include <linux/time.h>
+#include <linux/atomic.h>
+#include <linux/device.h>
+#include <linux/fcntl.h>
+
+#define HWQUEUE_ANY			((unsigned)-1)
+#define HWQUEUE_BYNAME			((unsigned)-2)
+
+/* file fcntl flags repurposed for hwqueues */
+#define O_HIGHTHROUGHPUT	O_DIRECT
+#define O_LOWLATENCY		O_LARGEFILE
+
+/* hardware queue notifier callback prototype */
+typedef void (*hwqueue_notify_fn)(void *arg);
+
+struct hwqueue_stats {
+	atomic_t	 pushes;
+	atomic_t	 pops;
+	atomic_t	 push_errors;
+	atomic_t	 pop_errors;
+	atomic_t	 notifies;
+};
+
+struct hwqueue_instance;
+
+struct hwqueue {
+	int		 (*push)(struct hwqueue_instance *inst, dma_addr_t dma,
+				 unsigned size, unsigned flags);
+	dma_addr_t	 (*pop)(struct hwqueue_instance *inst, unsigned *size,
+				unsigned flags);
+	int		 (*flush)(struct hwqueue_instance *inst);
+	int		 (*get_count)(struct hwqueue_instance *inst);
+	int		 (*map)(struct hwqueue_instance *inst, void *data,
+				unsigned size, dma_addr_t *dma_ptr, unsigned *size_ptr);
+	void		*(*unmap)(struct hwqueue_instance *inst, dma_addr_t dma,
+				unsigned desc_size);
+
+	struct hwqueue_instance	*inst;
+	struct hwqueue_stats	 stats;
+	hwqueue_notify_fn	 notifier_fn;
+	void			*notifier_fn_arg;
+	atomic_t		 notifier_enabled;
+	struct rcu_head		 rcu;
+	unsigned		 flags;
+	struct list_head	 list;
+};
+
+struct hwqueue *hwqueue_open(const char *name, unsigned id, unsigned flags);
+
+void hwqueue_close(struct hwqueue *queue);
+
+struct hwqueue *devm_hwqueue_open(struct device *dev, const char *name,
+				  unsigned id, unsigned flags);
+void devm_hwqueue_close(struct device *dev, struct hwqueue *queue);
+
+int hwqueue_get_id(struct hwqueue *queue);
+
+int hwqueue_get_hw_id(struct hwqueue *queue);
+
+int hwqueue_set_notifier(struct hwqueue *queue, hwqueue_notify_fn fn,
+			 void *fn_arg);
+int hwqueue_enable_notifier(struct hwqueue *queue);
+
+int hwqueue_disable_notifier(struct hwqueue *queue);
+
+dma_addr_t __hwqueue_pop_slow(struct hwqueue *qh, unsigned *size,
+			      struct timeval *timeout, unsigned flags);
+
+/**
+ * hwqueue_get_count() - poll a hardware queue and check if empty
+ *			 and return number of elements in a queue
+ * @qh	- hardware queue handle
+ *
+ * Returns 'true' if empty, and 'false' otherwise
+ */
+static inline int hwqueue_get_count(struct hwqueue *qh)
+{
+	if (unlikely(!qh->get_count))
+		return -EINVAL;
+	return qh->get_count(qh->inst);
+}
+
+/**
+ * hwqueue_flush() - forcibly empty a queue if possible
+ * @qh	- hardware queue handle
+ *
+ * Returns 0 on success, errno otherwise.  This may not be universally
+ * supported on all hardware queue implementations.
+ */
+static inline int hwqueue_flush(struct hwqueue *qh)
+{
+	if (unlikely(!qh->flush))
+		return -EINVAL;
+	return qh->flush(qh->inst);
+}
+
+/**
+ * hwqueue_push() - push data (or descriptor) to the tail of a queue
+ * @qh	- hardware queue handle
+ * @data	- data to push
+ * @size	- size of data to push
+ * @flags	- can be used to pass additional information
+ *
+ * Returns 0 on success, errno otherwise.
+ */
+static inline int hwqueue_push(struct hwqueue *qh, dma_addr_t dma,
+			       unsigned size, unsigned flags)
+{
+	int ret = 0;
+
+	do {
+		if (unlikely(!qh->push)) {
+			ret = -EINVAL;
+			break;
+		}
+
+		ret = qh->push(qh->inst, dma, size, flags);
+	} while (0);
+
+	if (unlikely(ret < 0))
+		atomic_inc(&qh->stats.push_errors);
+	else
+		atomic_inc(&qh->stats.pushes);
+	return ret;
+}
+
+/**
+ * hwqueue_pop() - pop data (or descriptor) from the head of a queue
+ * @qh	- hardware queue handle
+ * @size	- hwqueue_pop fills this parameter on success
+ * @timeout	- timeout value to use if blocking
+ *
+ * Returns a DMA address on success, 0 on failure.
+ */
+static inline dma_addr_t hwqueue_pop(struct hwqueue *qh, unsigned *size,
+				struct timeval *timeout , unsigned flags)
+{
+	dma_addr_t ret = 0;
+
+	do {
+		if (unlikely(!qh->pop)) {
+			ret = -EINVAL;
+			break;
+		}
+
+		ret = qh->pop(qh->inst, size, flags);
+		if (likely(ret))
+			break;
+
+		if (likely((qh->flags & O_NONBLOCK) ||
+			   (timeout && !timeout->tv_sec && !timeout->tv_usec)))
+			break;
+
+		ret = __hwqueue_pop_slow(qh, size, timeout, flags);
+	} while (0);
+
+	if (likely(ret)) {
+		if (IS_ERR_VALUE(ret))
+			atomic_inc(&qh->stats.pop_errors);
+		else
+			atomic_inc(&qh->stats.pops);
+	}
+	return ret;
+}
+
+/**
+ * hwqueue_map() - Map data (or descriptor) for DMA transfer
+ * @qh	- hardware queue handle
+ * @data	- address of data to map
+ * @size	- size of data to map
+ * @dma_ptr	- DMA address return pointer
+ * @size_ptr	- adjusted size return pointer
+ *
+ * Returns 0 on success, errno otherwise.
+ */
+static inline int hwqueue_map(struct hwqueue *qh, void *data, unsigned size,
+				dma_addr_t *dma_ptr, unsigned *size_ptr)
+{
+	if (unlikely(!qh->map))
+		return -EINVAL;
+	return qh->map(qh->inst, data, size, dma_ptr, size_ptr);
+}
+
+/**
+ * hwqueue_unmap() - Unmap data (or descriptor) after DMA transfer
+ * @qh	- hardware queue handle
+ * @dma	- DMA address to be unmapped
+ * @size	- size of data to unmap
+ *
+ * Returns a data/descriptor pointer on success.  Use IS_ERR_OR_NULL() to identify
+ * error values on return.
+ */
+static inline void *hwqueue_unmap(struct hwqueue *qh, dma_addr_t dma,
+				unsigned size)
+{
+	if (unlikely(!qh->unmap))
+		return NULL;
+	return qh->unmap(qh->inst, dma, size);
+}
+
+#endif /* __LINUX_HWQUEUE_H */
-- 
1.8.4.93.g57e4c17

