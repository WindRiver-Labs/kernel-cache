From 18e65c9d18305c036f4bf2de71556bbe261e16ff Mon Sep 17 00:00:00 2001
From: Yang Shi <yang.shi@windriver.com>
Date: Wed, 30 Jan 2013 15:52:22 -0800
Subject: [PATCH] intel-iommu: Extend PFN to 64 bits when DMAR >= 40 bits

On x86_32, when the DMAR width is >= 40 bits and IOMMU is enabled,
unbinding a device from the host PCI driver will hang the associated
userspace process on a wait notifier that is never triggered.

This hang is caused on x86_32 when the DMAR width is >= 40 bits, since it
also means that the PFN is more than 32 bits. This cases the kernel to
infinitely loop in dma_pte_free_pagetable as follows:

  while (tmp && tmp + level_size(level) - 1 <= last_pfn)

Since the condition is always true.

Promote start_pfn and last_pfn parameter to 64 bit for dma_pte_clear_range
and dma_pte_free_pagetable. And, pass __DOMAIN_MAX_PFN to them as last_pfn.

Signed-off-by: Yang Shi <yang.shi@windriver.com>
[PG: adjust for stable cherry pick of 3269ee0bd6 which introduces new
 helper function dma_pte_free_level() using start_pfn and last_pfn.]
Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/drivers/iommu/intel-iommu.c b/drivers/iommu/intel-iommu.c
index a60a54d..6e51b2e 100644
--- a/drivers/iommu/intel-iommu.c
+++ b/drivers/iommu/intel-iommu.c
@@ -851,8 +851,8 @@ static struct dma_pte *dma_pfn_level_pte(struct dmar_domain *domain,
 
 /* clear last level pte, a tlb flush should be followed */
 static int dma_pte_clear_range(struct dmar_domain *domain,
-				unsigned long start_pfn,
-				unsigned long last_pfn)
+				uint64_t start_pfn,
+				uint64_t last_pfn)
 {
 	int addr_width = agaw_to_width(domain->agaw) - VTD_PAGE_SHIFT;
 	unsigned int large_page = 1;
@@ -887,14 +887,14 @@ static int dma_pte_clear_range(struct dmar_domain *domain,
 }
 
 static void dma_pte_free_level(struct dmar_domain *domain, int level,
-			       struct dma_pte *pte, unsigned long pfn,
-			       unsigned long start_pfn, unsigned long last_pfn)
+			       struct dma_pte *pte, uint64_t pfn,
+			       uint64_t start_pfn, uint64_t last_pfn)
 {
 	pfn = max(start_pfn, pfn);
 	pte = &pte[pfn_level_offset(pfn, level)];
 
 	do {
-		unsigned long level_pfn;
+		uint64_t level_pfn;
 		struct dma_pte *level_pte;
 
 		if (!dma_pte_present(pte) || dma_pte_superpage(pte))
@@ -921,8 +921,8 @@ next:
 
 /* free page table pages. last level pte should already be cleared */
 static void dma_pte_free_pagetable(struct dmar_domain *domain,
-				   unsigned long start_pfn,
-				   unsigned long last_pfn)
+				   uint64_t start_pfn,
+				   uint64_t last_pfn)
 {
 	int addr_width = agaw_to_width(domain->agaw) - VTD_PAGE_SHIFT;
 
@@ -935,7 +935,7 @@ static void dma_pte_free_pagetable(struct dmar_domain *domain,
 			   domain->pgd, 0, start_pfn, last_pfn);
 
 	/* free pgd */
-	if (start_pfn == 0 && last_pfn == DOMAIN_MAX_PFN(domain->gaw)) {
+	if (start_pfn == 0 && last_pfn == __DOMAIN_MAX_PFN(domain->gaw)) {
 		free_pgtable_page(domain->pgd);
 		domain->pgd = NULL;
 	}
@@ -1527,10 +1527,10 @@ static void domain_exit(struct dmar_domain *domain)
 	put_iova_domain(&domain->iovad);
 
 	/* clear ptes */
-	dma_pte_clear_range(domain, 0, DOMAIN_MAX_PFN(domain->gaw));
+	dma_pte_clear_range(domain, 0, __DOMAIN_MAX_PFN(domain->gaw));
 
 	/* free page tables */
-	dma_pte_free_pagetable(domain, 0, DOMAIN_MAX_PFN(domain->gaw));
+	dma_pte_free_pagetable(domain, 0, __DOMAIN_MAX_PFN(domain->gaw));
 
 	for_each_active_iommu(iommu, drhd)
 		if (test_bit(iommu->seq_id, domain->iommu_bmp))
-- 
1.8.3.1

