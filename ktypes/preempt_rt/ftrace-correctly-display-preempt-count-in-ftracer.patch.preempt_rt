From: Carsten Emde <Carsten.Emde@osadl.org>
Subject: ftrace:  display real preempt_count in ftracer
Date: Wed, 28 Jan 2009 14:39:51 +0100

Ftrace determined the preempt_count after preemption was disabled
instead of the original preemption count.

Signed-off-by: Carsten Emde <C.Emde@osadl.org>
Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

---
 kernel/trace/trace.c              |   52 +++++++++++++++++++-----------------
 kernel/trace/trace.h              |    3 +-
 kernel/trace/trace_hist.c         |    2 +-
 kernel/trace/trace_irqsoff.c      |   13 +++++---
 kernel/trace/trace_sched_wakeup.c |    9 ++++--
 5 files changed, 44 insertions(+), 35 deletions(-)

diff --git a/kernel/trace/trace.c b/kernel/trace/trace.c
index 1cec9e8..6da78b2 100644
--- a/kernel/trace/trace.c
+++ b/kernel/trace/trace.c
@@ -822,12 +822,10 @@ tracing_get_trace_entry(struct trace_array *tr, struct trace_array_cpu *data)
 }
 
 static inline void
-tracing_generic_entry_update(struct trace_entry *entry, unsigned long flags)
+tracing_generic_entry_update(struct trace_entry *entry, unsigned long flags,
+			     unsigned long pc)
 {
 	struct task_struct *tsk = current;
-	unsigned long pc;
-
-	pc = preempt_count();
 
 	entry->preempt_count	= pc & 0xff;
 	entry->pid		= (tsk) ? tsk->pid : 0;
@@ -841,7 +839,8 @@ tracing_generic_entry_update(struct trace_entry *entry, unsigned long flags)
 
 void
 trace_function(struct trace_array *tr, struct trace_array_cpu *data,
-	       unsigned long ip, unsigned long parent_ip, unsigned long flags)
+	       unsigned long ip, unsigned long parent_ip, unsigned long flags,
+	       unsigned long pc)
 {
 	struct trace_entry *entry;
 	unsigned long irq_flags;
@@ -849,7 +848,7 @@ trace_function(struct trace_array *tr, struct trace_array_cpu *data,
 	raw_local_irq_save(irq_flags);
 	__raw_spin_lock(&data->lock);
 	entry			= tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, pc);
 	entry->type		= TRACE_FN;
 	entry->fn.ip		= ip;
 	entry->fn.parent_ip	= parent_ip;
@@ -862,7 +861,8 @@ ftrace(struct trace_array *tr, struct trace_array_cpu *data,
        unsigned long ip, unsigned long parent_ip, unsigned long flags)
 {
 	if (likely(!atomic_read(&data->disabled)))
-		trace_function(tr, data, ip, parent_ip, flags);
+		trace_function(tr, data, ip, parent_ip, flags,
+			       preempt_count());
 }
 
 #ifdef CONFIG_MMIOTRACE
@@ -876,7 +876,7 @@ void __trace_mmiotrace_rw(struct trace_array *tr, struct trace_array_cpu *data,
 	__raw_spin_lock(&data->lock);
 
 	entry			= tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, 0);
+	tracing_generic_entry_update(entry, 0, preempt_count());
 	entry->type		= TRACE_MMIO_RW;
 	entry->mmiorw		= *rw;
 
@@ -896,7 +896,7 @@ void __trace_mmiotrace_map(struct trace_array *tr, struct trace_array_cpu *data,
 	__raw_spin_lock(&data->lock);
 
 	entry			= tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, 0);
+	tracing_generic_entry_update(entry, 0, preempt_count());
 	entry->type		= TRACE_MMIO_MAP;
 	entry->mmiomap		= *map;
 
@@ -919,7 +919,7 @@ void __trace_stack(struct trace_array *tr,
 		return;
 
 	entry			= tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type		= TRACE_STACK;
 
 	memset(&entry->stack, 0, sizeof(entry->stack));
@@ -944,7 +944,7 @@ __trace_special(void *__tr, void *__data,
 	raw_local_irq_save(irq_flags);
 	__raw_spin_lock(&data->lock);
 	entry			= tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, 0);
+	tracing_generic_entry_update(entry, 0, preempt_count());
 	entry->type		= TRACE_SPECIAL;
 	entry->special.arg1	= arg1;
 	entry->special.arg2	= arg2;
@@ -969,7 +969,7 @@ tracing_sched_switch_trace(struct trace_array *tr,
 	raw_local_irq_save(irq_flags);
 	__raw_spin_lock(&data->lock);
 	entry			= tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type		= TRACE_CTX;
 	entry->ctx.prev_pid	= prev->pid;
 	entry->ctx.prev_prio	= prev->prio;
@@ -995,7 +995,7 @@ tracing_sched_wakeup_trace(struct trace_array *tr,
 	raw_local_irq_save(irq_flags);
 	__raw_spin_lock(&data->lock);
 	entry			= tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type		= TRACE_WAKE;
 	entry->ctx.prev_pid	= curr->pid;
 	entry->ctx.prev_prio	= curr->prio;
@@ -1044,7 +1044,7 @@ void tracing_event_irq(struct trace_array *tr,
 	struct trace_entry *entry;
 
 	entry = tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type			= TRACE_IRQ;
 	entry->irq.ip		= ip;
 	entry->irq.irq		= irq;
@@ -1063,7 +1063,7 @@ void tracing_event_fault(struct trace_array *tr,
 	struct trace_entry *entry;
 
 	entry = tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type			= TRACE_FAULT;
 	entry->fault.ip		= ip;
 	entry->fault.ret_ip	= retip;
@@ -1080,7 +1080,7 @@ void tracing_event_timer_set(struct trace_array *tr,
 	struct trace_entry *entry;
 
 	entry = tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type			= TRACE_TIMER_SET;
 	entry->timer.ip		= ip;
 	entry->timer.expire	= *expires;
@@ -1096,7 +1096,7 @@ void tracing_event_program_event(struct trace_array *tr,
 	struct trace_entry *entry;
 
 	entry = tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type		= TRACE_PROGRAM_EVENT;
 	entry->program.ip	= ip;
 	entry->program.expire	= *expires;
@@ -1113,7 +1113,7 @@ void tracing_event_resched_task(struct trace_array *tr,
 	struct trace_entry *entry;
 
 	entry = tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type		= TRACE_RESCHED_TASK;
 	entry->task.ip	= ip;
 	entry->task.prio	= p->prio;
@@ -1130,7 +1130,7 @@ void tracing_event_timer_triggered(struct trace_array *tr,
 	struct trace_entry *entry;
 
 	entry = tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type			= TRACE_TIMER_TRIG;
 	entry->timer.ip		= ip;
 	entry->timer.expire	= *expired;
@@ -1146,7 +1146,7 @@ void tracing_event_timestamp(struct trace_array *tr,
 	struct trace_entry *entry;
 
 	entry = tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type			= TRACE_TIMESTAMP;
 	entry->timestamp.ip		= ip;
 	entry->timestamp.now		= *now;
@@ -1162,7 +1162,7 @@ void tracing_event_task_activate(struct trace_array *tr,
 	struct trace_entry *entry;
 
 	entry = tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type			= TRACE_TASK_ACT;
 	entry->task.ip		= ip;
 	entry->task.pid		= p->pid;
@@ -1180,7 +1180,7 @@ void tracing_event_task_deactivate(struct trace_array *tr,
 	struct trace_entry *entry;
 
 	entry = tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type			= TRACE_TASK_DEACT;
 	entry->task.ip		= ip;
 	entry->task.pid		= p->pid;
@@ -1200,7 +1200,7 @@ void tracing_event_syscall(struct trace_array *tr,
 	struct trace_entry *entry;
 
 	entry = tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type			= TRACE_SYSCALL;
 	entry->syscall.ip		= ip;
 	entry->syscall.nr		= nr;
@@ -1218,7 +1218,7 @@ void tracing_event_sysret(struct trace_array *tr,
 	struct trace_entry *entry;
 
 	entry = tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type			= TRACE_SYSRET;
 	entry->sysret.ip		= ip;
 	entry->sysret.ret		= ret;
@@ -1233,6 +1233,7 @@ function_trace_call(unsigned long ip, unsigned long parent_ip)
 	unsigned long flags;
 	long disabled;
 	int cpu, resched;
+	unsigned long pc;
 
 	if (unlikely(!ftrace_function_enabled))
 		return;
@@ -1240,6 +1241,7 @@ function_trace_call(unsigned long ip, unsigned long parent_ip)
 	if (skip_trace(ip))
 		return;
 
+	pc = preempt_count();
 	resched = ftrace_preempt_disable();
 	cpu = raw_smp_processor_id();
 	data = tr->data[cpu];
@@ -1247,7 +1249,7 @@ function_trace_call(unsigned long ip, unsigned long parent_ip)
 
 	if (likely(disabled == 1)) {
 		local_save_flags(flags);
-		trace_function(tr, data, ip, parent_ip, flags);
+		trace_function(tr, data, ip, parent_ip, flags, pc);
 	}
 
 	atomic_dec(&data->disabled);
diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index b8590a2..7f8d04d 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -300,7 +300,8 @@ void trace_function(struct trace_array *tr,
 		    struct trace_array_cpu *data,
 		    unsigned long ip,
 		    unsigned long parent_ip,
-		    unsigned long flags);
+		    unsigned long flags,
+		    unsigned long pc);
 void tracing_event_irq(struct trace_array *tr,
 		       struct trace_array_cpu *data,
 		       unsigned long flags,
diff --git a/kernel/trace/trace_hist.c b/kernel/trace/trace_hist.c
index f3f49eb..8be4719 100644
--- a/kernel/trace/trace_hist.c
+++ b/kernel/trace/trace_hist.c
@@ -354,7 +354,7 @@ notrace void tracing_hist_preempt_stop(int irqs_on)
 	cpu = raw_smp_processor_id();
 
 #ifdef CONFIG_INTERRUPT_OFF_HIST
-	if (irqs_on  &&
+	if (irqs_on &&
 	    per_cpu(hist_irqsoff_tracing, cpu)) {
 		stop = ftrace_now(cpu);
 		stop_set++;
diff --git a/kernel/trace/trace_irqsoff.c b/kernel/trace/trace_irqsoff.c
index c9b0eed..72506d0 100644
--- a/kernel/trace/trace_irqsoff.c
+++ b/kernel/trace/trace_irqsoff.c
@@ -103,7 +103,8 @@ irqsoff_tracer_call(unsigned long ip, unsigned long parent_ip)
 	disabled = atomic_inc_return(&data->disabled);
 
 	if (likely(disabled == 1))
-		trace_function(tr, data, ip, parent_ip, flags);
+		trace_function(tr, data, ip, parent_ip, flags,
+			       preempt_count());
 
 	atomic_dec(&data->disabled);
 }
@@ -161,7 +162,8 @@ check_critical_timing(struct trace_array *tr,
 	if (!report_latency(delta))
 		goto out_unlock;
 
-	trace_function(tr, data, CALLER_ADDR0, parent_ip, flags);
+	trace_function(tr, data, CALLER_ADDR0, parent_ip, flags,
+		       preempt_count());
 
 	latency = nsecs_to_usecs(delta);
 
@@ -186,7 +188,8 @@ out:
 	data->preempt_timestamp = ftrace_now(cpu);
 	if (!(trace_type & TRACER_IRQ_TRACK))
 		tracing_reset(data);
-	trace_function(tr, data, CALLER_ADDR0, parent_ip, flags);
+	trace_function(tr, data, CALLER_ADDR0, parent_ip, flags,
+		       preempt_count());
 }
 
 extern void die_nmi(char *, struct pt_regs *, int);
@@ -226,7 +229,7 @@ start_critical_timing(unsigned long ip, unsigned long parent_ip)
 
 	local_save_flags(flags);
 
-	trace_function(tr, data, ip, parent_ip, flags);
+	trace_function(tr, data, ip, parent_ip, flags, preempt_count());
 
 	per_cpu(tracing_cpu, cpu) = 1;
 
@@ -265,7 +268,7 @@ stop_critical_timing(unsigned long ip, unsigned long parent_ip)
 	atomic_inc(&data->disabled);
 
 	local_save_flags(flags);
-	trace_function(tr, data, ip, parent_ip, flags);
+	trace_function(tr, data, ip, parent_ip, flags, preempt_count());
 	check_critical_timing(tr, data, parent_ip ? : ip, cpu);
 	data->critical_start = 0;
 	atomic_dec(&data->disabled);
diff --git a/kernel/trace/trace_sched_wakeup.c b/kernel/trace/trace_sched_wakeup.c
index 76a12d9..cc5ea4e 100644
--- a/kernel/trace/trace_sched_wakeup.c
+++ b/kernel/trace/trace_sched_wakeup.c
@@ -44,10 +44,12 @@ wakeup_tracer_call(unsigned long ip, unsigned long parent_ip)
 	long disabled;
 	int resched;
 	int cpu;
+	unsigned long pc;
 
 	if (likely(!wakeup_task) || !ftrace_enabled)
 		return;
 
+	pc = preempt_count();
 	resched = ftrace_preempt_disable();
 
 	cpu = raw_smp_processor_id();
@@ -71,7 +73,7 @@ wakeup_tracer_call(unsigned long ip, unsigned long parent_ip)
 
 	/* interrupts are disabled, no worry about scheduling */
 	preempt_enable_no_resched_notrace();
-	trace_function(tr, data, ip, parent_ip, flags);
+	trace_function(tr, data, ip, parent_ip, flags, pc);
 	preempt_disable_notrace();
 
  unlock:
@@ -149,7 +151,8 @@ probe_wakeup_sched_switch(struct rq *rq, struct task_struct *prev,
 	if (unlikely(!tracer_enabled || next != wakeup_task))
 		goto out_unlock;
 
-	trace_function(wakeup_trace, data, CALLER_ADDR1, CALLER_ADDR2, flags);
+	trace_function(wakeup_trace, data, CALLER_ADDR1, CALLER_ADDR2, flags,
+		       preempt_count());
 
 	/*
 	 * usecs conversion is slow so we try to delay the conversion
@@ -250,7 +253,7 @@ probe_wakeup(struct rq *rq, struct task_struct *p)
 
 	wakeup_trace->data[wakeup_cpu]->preempt_timestamp = ftrace_now(cpu);
 	trace_function(wakeup_trace, wakeup_trace->data[wakeup_cpu],
-		       CALLER_ADDR1, CALLER_ADDR2, flags);
+		       CALLER_ADDR1, CALLER_ADDR2, flags, preempt_count());
 
 out_locked:
 	__raw_spin_unlock(&wakeup_lock);
