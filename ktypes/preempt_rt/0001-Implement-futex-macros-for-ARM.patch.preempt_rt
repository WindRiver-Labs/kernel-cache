From 33cf28740e5fafc21e4297af1d7a1c0cca549b92 Mon Sep 17 00:00:00 2001
From: auto commit <unknown@unknown>
Date: Mon, 6 Oct 2008 09:25:20 -0400
Subject: [PATCH] Linux/ARM currently doesn't support robust or PI futexes.

The problem is that the kernel wants to perform certain ops
(cmpxchg, set, add, or, andn, xor) atomically on user-space
addresses, and ARM's futex.h doesn't support that.

This patch adds that support, but only for uniprocessor machines.
For UP it's enough to disable preemption to ensure mutual exclusion
with other software agents (futexes don't need to care about other
hardware agents, fortunately).

This patch is based on a patch posted to linux-arm-kernel by Khem Raj on
2007-08-01 <http://marc.info/?l=linux-arm-kernel&m=118599407413016&w=2>.
Changes since that version include:
* corrected implementation of FUTEX_OP_ANDN (must complement oparg)
* added missing memory clobber to futex_atomic_cmpxchg_inatomic() asm()
* removed spinlock because it's unnecessary for UP and insufficient
  for SMP, instead the code is restricted to UP and relies on the fact
  that pagefault_disable() also disables preemption
* coding style cleanups

Tested successfully on ARMv5 XScales with the glibc-2.6 nptl test suite.

Signed-off-by: Mikael Pettersson <mikpe@it.uu.se>

PATCH FOLLOWS
KernelVersion: 2.6.27-rc1
---
 arch/arm/include/asm/futex.h |  113 +++++++++++++++++++++---------------------
 1 files changed, 56 insertions(+), 57 deletions(-)

diff --git a/arch/arm/include/asm/futex.h b/arch/arm/include/asm/futex.h
index 3b0861b..9ee743b 100644
--- a/arch/arm/include/asm/futex.h
+++ b/arch/arm/include/asm/futex.h
@@ -3,29 +3,34 @@
 
 #ifdef __KERNEL__
 
+#ifdef CONFIG_SMP
+
+#include <asm-generic/futex.h>
+
+#else /* !SMP, we can work around lack of atomic ops by disabling preemption */
+
 #include <linux/futex.h>
-#include <linux/errno.h>
+#include <linux/preempt.h>
 #include <linux/uaccess.h>
-
-extern raw_spinlock_t  futex_atomic_lock;
-
-#define __futex_atomic_op(insn, ret, oldval, uaddr, oparg) \
-	__asm__ __volatile__ (						\
-	"1:	ldrt	%1, [%2]				\n"	\
-		insn							\
-	"2:	strt	%0, [%2]				\n"	\
-	"	mov	%0, #0					\n"	\
-	"3:							\n"	\
-	"	.section __ex_table, \"a\"			\n"	\
-	"	.align	3					\n"	\
-	"	.long	1b, 4f, 2b, 4f				\n"	\
-	"	.previous					\n"	\
-	"	.section .fixup,\"ax\"				\n"	\
-	"4:	mov	%0, %4					\n"	\
-	"	b	3b					\n"	\
-	"	.previous"						\
-	: "=&r" (ret), "=&r" (oldval)					\
-	: "r" (uaddr), "r" (oparg), "Ir" (-EFAULT)			\
+#include <asm/errno.h>
+
+#define __futex_atomic_op(insn, ret, oldval, uaddr, oparg)	\
+	__asm__ __volatile__(					\
+	"1:	ldrt	%1, [%2]\n"				\
+	"	" insn "\n"					\
+	"2:	strt	%0, [%2]\n"				\
+	"	mov	%0, #0\n"				\
+	"3:\n"							\
+	"	.section __ex_table,\"a\"\n"			\
+	"	.align	3\n"					\
+	"	.long	1b, 4f, 2b, 4f\n"			\
+	"	.previous\n"					\
+	"	.section .fixup,\"ax\"\n"			\
+	"4:	mov	%0, %4\n"				\
+	"	b	3b\n"					\
+	"	.previous"					\
+	: "=&r" (ret), "=&r" (oldval)				\
+	: "r" (uaddr), "r" (oparg), "Ir" (-EFAULT)		\
 	: "cc", "memory")
 
 static inline int
@@ -36,44 +41,36 @@ futex_atomic_op_inuser (int encoded_op, int __user *uaddr)
 	int oparg = (encoded_op << 8) >> 20;
 	int cmparg = (encoded_op << 20) >> 20;
 	int oldval = 0, ret;
+
 	if (encoded_op & (FUTEX_OP_OPARG_SHIFT << 28))
 		oparg = 1 << oparg;
 
-	if (!access_ok (VERIFY_WRITE, uaddr, sizeof(int)))
+	if (!access_ok(VERIFY_WRITE, uaddr, sizeof(int)))
 		return -EFAULT;
 
-	pagefault_disable();
-
-	spin_lock(&futex_atomic_lock);
+	pagefault_disable();	/* implies preempt_disable() */
 
 	switch (op) {
 	case FUTEX_OP_SET:
-		__futex_atomic_op("	mov	%0, %3\n",
-				  ret, oldval, uaddr, oparg);
+		__futex_atomic_op("mov	%0, %3", ret, oldval, uaddr, oparg);
 		break;
 	case FUTEX_OP_ADD:
-		__futex_atomic_op("	add	%0, %1, %3\n",
-				  ret, oldval, uaddr, oparg);
+		__futex_atomic_op("add	%0, %1, %3", ret, oldval, uaddr, oparg);
 		break;
 	case FUTEX_OP_OR:
-		__futex_atomic_op("	orr	%0, %1, %3\n",
-				  ret, oldval, uaddr, oparg);
+		__futex_atomic_op("orr	%0, %1, %3", ret, oldval, uaddr, oparg);
 		break;
 	case FUTEX_OP_ANDN:
-		__futex_atomic_op("	and	%0, %1, %3\n",
-				  ret, oldval, uaddr, oparg);
+		__futex_atomic_op("and	%0, %1, %3", ret, oldval, uaddr, ~oparg);
 		break;
 	case FUTEX_OP_XOR:
-		__futex_atomic_op("	eor	%0, %1, %3\n",
-				  ret, oldval, uaddr, oparg);
+		__futex_atomic_op("eor	%0, %1, %3", ret, oldval, uaddr, oparg);
 		break;
 	default:
 		ret = -ENOSYS;
 	}
 
-	spin_unlock(&futex_atomic_lock);
-
-	pagefault_enable();
+	pagefault_enable();	/* subsumes preempt_enable() */
 
 	if (!ret) {
 		switch (cmp) {
@@ -97,29 +94,31 @@ futex_atomic_cmpxchg_inatomic(int __user *uaddr, int oldval, int newval)
 	if (!access_ok(VERIFY_WRITE, uaddr, sizeof(int)))
 		return -EFAULT;
 
-	spin_lock(&futex_atomic_lock);
-
-	__asm__ __volatile__( "@futex_atomic_cmpxchg_inatomic	\n"
-	"1:     ldrt    %0, [%3]				\n"
-	"       teq     %0, %1					\n"
-	"2:     streqt  %2, [%3]				\n"
-	"3:							\n"
-	"       .section __ex_table, \"a\"			\n"
-	"       .align 3					\n"
-	"       .long   1b, 4f, 2b, 4f				\n"
-	"       .previous					\n"
-	"       .section .fixup,\"ax\"				\n"
-	"4:     mov     %0, %4					\n"
-	"       b       3b					\n"
-	"       .previous"
+	pagefault_disable();	/* implies preempt_disable() */
+
+	__asm__ __volatile__("@futex_atomic_cmpxchg_inatomic\n"
+	"1:	ldrt	%0, [%3]\n"
+	"	teq	%0, %1\n"
+	"2:	streqt	%2, [%3]\n"
+	"3:\n"
+	"	.section __ex_table,\"a\"\n"
+	"	.align	3\n"
+	"	.long	1b, 4f, 2b, 4f\n"
+	"	.previous\n"
+	"	.section .fixup,\"ax\"\n"
+	"4:	mov	%0, %4\n"
+	"	b	3b\n"
+	"	.previous"
 	: "=&r" (val)
 	: "r" (oldval), "r" (newval), "r" (uaddr), "Ir" (-EFAULT)
-	: "cc");
+	: "cc", "memory");
 
-	spin_unlock(&futex_atomic_lock);
+	pagefault_enable();	/* subsumes preempt_enable() */
 
 	return val;
 }
 
-#endif
-#endif
+#endif /* !SMP */
+
+#endif /* __KERNEL__ */
+#endif /* _ASM_ARM_FUTEX_H */
-- 
1.5.5.1

