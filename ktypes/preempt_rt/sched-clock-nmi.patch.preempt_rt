From: Peter Zijlstra

---
 include/linux/hardirq.h |   11 ++++++-----
 kernel/sched_clock.c    |    9 +++++++++
 2 files changed, 15 insertions(+), 5 deletions(-)

diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index bff12ef..ab63f58 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -49,28 +49,28 @@
 #define PREEMPT_SHIFT		0
 #define SOFTIRQ_SHIFT		(PREEMPT_SHIFT + PREEMPT_BITS)
 #define HARDIRQ_SHIFT		(SOFTIRQ_SHIFT + SOFTIRQ_BITS)
-#define PREEMPT_ACTIVE_SHIFT	(HARDIRQ_SHIFT + HARDIRQ_BITS)
-#define HARDNMI_SHIFT	(30)
+#define HARDNMI_SHIFT  		(30)
 
 #define __IRQ_MASK(x)		((1UL << (x))-1)
 
 #define PREEMPT_MASK		(__IRQ_MASK(PREEMPT_BITS) << PREEMPT_SHIFT)
 #define SOFTIRQ_MASK		(__IRQ_MASK(SOFTIRQ_BITS) << SOFTIRQ_SHIFT)
 #define HARDIRQ_MASK		(__IRQ_MASK(HARDIRQ_BITS) << HARDIRQ_SHIFT)
-#define HARDNMI_MASK	(__IRQ_MASK(HARDNMI_BITS) << HARDNMI_SHIFT)
+#define HARDNMI_MASK   		(__IRQ_MASK(HARDNMI_BITS) << HARDNMI_SHIFT)
 
 #define PREEMPT_OFFSET		(1UL << PREEMPT_SHIFT)
 #define SOFTIRQ_OFFSET		(1UL << SOFTIRQ_SHIFT)
 #define HARDIRQ_OFFSET		(1UL << HARDIRQ_SHIFT)
-#define HARDNMI_OFFSET	(1UL << HARDNMI_SHIFT)
+#define HARDNMI_OFFSET 		(1UL << HARDNMI_SHIFT)
 
 #if PREEMPT_ACTIVE < (1 << (HARDIRQ_SHIFT + HARDIRQ_BITS))
 # error PREEMPT_ACTIVE is too low!
 #endif
 
+#define hardnmi_count()	(preempt_count() & HARDNMI_MASK)
 #define hardirq_count()	(preempt_count() & HARDIRQ_MASK)
 #define softirq_count()	(preempt_count() & SOFTIRQ_MASK)
-#define irq_count() \
+#define irq_count()	\
 	(preempt_count() & (HARDNMI_MASK | HARDIRQ_MASK | SOFTIRQ_MASK))
 #define hardnmi_count()	(preempt_count() & HARDNMI_MASK)
 
@@ -78,6 +78,7 @@
  * Are we doing bottom half or hardware interrupt processing?
  * Are we in a softirq context? Interrupt context?
  */
+#define in_nmi()	(hardnmi_count())
 #define in_irq()	(hardirq_count() || (current->flags & PF_HARDIRQ))
 #define in_softirq()	(softirq_count() || (current->flags & PF_SOFTIRQ))
 #define in_interrupt()	(irq_count())
diff --git a/kernel/sched_clock.c b/kernel/sched_clock.c
index 7f4ddd1..5eb5c64 100644
--- a/kernel/sched_clock.c
+++ b/kernel/sched_clock.c
@@ -157,6 +157,7 @@ u64 sched_clock_cpu(int cpu)
 	WARN_ON_ONCE(!irqs_disabled());
 	now = sched_clock();
 
+#if 0
 	if (cpu != raw_smp_processor_id()) {
 		struct sched_clock_data *my_scd = this_scd();
 
@@ -189,6 +190,14 @@ u64 sched_clock_cpu(int cpu)
 	}
 
 	__raw_spin_unlock(&scd->lock);
+#else
+	if (cpu == smp_processor_id() && __raw_spin_trylock(&scd->lock)) {
+		__update_sched_clock(scd, now);
+		clock = scd->clock;
+		__raw_spin_unlock(&scd->lock);
+	} else
+		clock = scd->clock;
+#endif
 
 	return clock;
 }
