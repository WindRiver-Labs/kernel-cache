From 667158745e026621fca95304f55ada8f98f76e1c Mon Sep 17 00:00:00 2001
From: Liu Changhui <changhui.liu@windriver.com>
Date: Sun, 6 Jun 2010 14:36:14 +0800
Subject: [PATCH] bcm5300x high memory support

Add 256M memory support to bcm5300x

Source: from broadcom SDK

Signed-off-by: Liu Changhui <changhui.liu@windriver.com>
---
 arch/mips/Kconfig            |    8 +++
 arch/mips/bcm53000/prom.c    |  114 +++++++++++++++++++++++++++++++++++++++++-
 arch/mips/kernel/setup.c     |    3 +
 arch/mips/kernel/syscall.c   |    3 +
 arch/mips/mm/c-r4k.c         |   27 ++++++++++-
 arch/mips/mm/cache.c         |   36 ++++++++++++-
 arch/mips/mm/highmem.c       |   99 ++++++++++++++++++++++++++++++++++++-
 arch/mips/mm/init.c          |   19 ++++++-
 arch/mips/mm/pgtable-32.c    |    4 ++
 include/asm-mips/fixmap.h    |   15 ++++++
 include/asm-mips/page.h      |   11 ++++-
 include/asm-mips/sparsemem.h |    5 ++
 mm/Kconfig                   |    2 +-
 mm/highmem.c                 |   17 ++++++
 mm/page_alloc.c              |    4 ++
 15 files changed, 357 insertions(+), 10 deletions(-)

diff --git a/arch/mips/Kconfig b/arch/mips/Kconfig
index 96aa6a3..09cf286 100644
--- a/arch/mips/Kconfig
+++ b/arch/mips/Kconfig
@@ -584,6 +584,12 @@ config BCM53000
         help
           Support Broadcom BCM53000 board
 
+config BCM53000_HIGHMEM
+       bool "Support BCM53000 High MEM"
+         select SYS_SUPPORTS_HIGHMEM
+         select ARCH_SPARSEMEM_ENABLE
+       depends on BCM53000
+
 config MACH_TX39XX
 	bool "Toshiba TX39 series based machines"
 
@@ -1667,6 +1673,7 @@ config IRQ_PER_CPU
 config HIGHMEM
 	bool "High Memory Support"
 	depends on 32BIT && CPU_SUPPORTS_HIGHMEM && SYS_SUPPORTS_HIGHMEM
+        default y if BCM53000_HIGHMEM
 
 config CPU_SUPPORTS_HIGHMEM
 	bool
@@ -1678,6 +1685,7 @@ config SYS_SUPPORTS_SMARTMIPS
 	bool
 
 config ARCH_FLATMEM_ENABLE
+	bool
 	def_bool y
 	depends on !NUMA && !CPU_LOONGSON2
 
diff --git a/arch/mips/bcm53000/prom.c b/arch/mips/bcm53000/prom.c
index e5a3812..339fb7c 100644
--- a/arch/mips/bcm53000/prom.c
+++ b/arch/mips/bcm53000/prom.c
@@ -56,6 +56,69 @@ extern unsigned long initrd_start, initrd_end;
 #endif
 
 
+#define MB << 20
+
+#ifdef CONFIG_BCM53000_HIGHMEM
+
+#define EXTVBASE        0xc0000000
+#define ENTRYLO(x)      ((pte_val(pfn_pte((x) >> PAGE_SHIFT, PAGE_KERNEL_UNCACHED)) >> 6) | 1)
+#define UNIQUE_ENTRYHI(idx) (CKSEG0 + ((idx) << (PAGE_SHIFT + 1)))
+
+
+static unsigned long tmp_tlb_ent __initdata;
+
+/* Initialize the wired register and all tlb entries to
+ * known good state.
+ */
+  void __init
+early_tlb_init(void)
+{
+  unsigned long  index;
+  struct cpuinfo_mips *c = &current_cpu_data;
+
+  tmp_tlb_ent = c->tlbsize;
+
+  /*
+   * initialize entire TLB to uniqe virtual addresses
+   * but with the PAGE_VALID bit not set
+   */
+  write_c0_wired(0);
+  write_c0_pagemask(PM_DEFAULT_MASK);
+
+  write_c0_entrylo0(0);   /* not _PAGE_VALID */
+  write_c0_entrylo1(0);
+
+  for (index = 0; index < c->tlbsize; index++) {
+    /* Make sure all entries differ. */
+    write_c0_entryhi(UNIQUE_ENTRYHI(index+32));
+    write_c0_index(index);
+    mtc0_tlbw_hazard();
+    tlb_write_indexed();
+  }
+
+  tlbw_use_hazard();
+
+}
+
+void __init
+add_tmptlb_entry(unsigned long entrylo0, unsigned long entrylo1,
+    unsigned long entryhi, unsigned long pagemask)
+{
+  /* write one tlb entry */
+  --tmp_tlb_ent;
+  write_c0_index(tmp_tlb_ent);
+  write_c0_pagemask(pagemask);
+  write_c0_entryhi(entryhi);
+  write_c0_entrylo0(entrylo0);
+  write_c0_entrylo1(entrylo1);
+  mtc0_tlbw_hazard();
+  tlb_write_indexed();
+  tlbw_use_hazard();
+}
+
+#endif
+
+
 static void ATTRIB_NORET cfe_linux_exit(void *arg)
 {
     int warm = *(int *) arg;
@@ -90,13 +153,16 @@ static void ATTRIB_NORET cfe_linux_halt(void)
 
     cfe_linux_exit((void *) &one);
 }
-
+#ifdef CONFIG_BCM53000_HIGHMEM
+#define SI_SDRAM_R2 0x80000000
+#endif
 static __init void prom_meminit(void)
 {
     u64 addr, size, type;	/* regardless of 64BIT_PHYS_ADDR */
     int mem_flags = 0;
     unsigned int idx;
     int rd_flag;
+	unsigned long mem, extmem = 0, off, data;
 #ifdef CONFIG_BLK_DEV_INITRD
     unsigned long initrd_pstart;
     unsigned long initrd_pend;
@@ -108,6 +174,38 @@ static __init void prom_meminit(void)
 	panic("initrd out of addressable memory");
     }
 #endif				/* INITRD */
+	off = (unsigned long)prom_init;
+	data = *(unsigned long *)prom_init;
+	/* Figure out memory size by finding aliases */
+	for (mem = (1 MB); mem < (128 MB); mem <<= 1) {
+		if (*(unsigned long *)(off + mem) == data)
+			break;
+		 }
+#ifdef CONFIG_BCM53000_HIGHMEM
+	if (mem == 128 MB) {
+		early_tlb_init();
+		/* Add one temporary TLB entries to map SDRAM Region 2.
+		    *      Physical        Virtual
+			*      0x80000000      0xc0000000      (1st: 256MB)
+			*      0x90000000      0xd0000000      (2nd: 256MB)
+			*/
+		add_tmptlb_entry(ENTRYLO(SI_SDRAM_R2),
+				ENTRYLO(SI_SDRAM_R2 + (256 MB)),
+				EXTVBASE, PM_256M);
+		off = EXTVBASE + __pa(off);
+		for (extmem = (128 MB); extmem < (512 MB); extmem <<= 1) {
+			if (*(unsigned long *)(off + extmem) == data)
+				break;
+
+		}
+		extmem -= mem;
+
+		/* Keep tlb entries back in consistent state */
+		 early_tlb_init();
+
+	}
+#endif  /*  CONFIG_BCM53000_HIGHMEM */
+
 
     for (idx = 0;
 	 cfe_enummem(idx, mem_flags, &addr, &size,
@@ -167,6 +265,20 @@ static __init void prom_meminit(void)
 			  BOOT_MEM_RESERVED);
     }
 #endif
+#ifdef CONFIG_BCM53000_HIGHMEM
+	if (extmem) {
+		/* We should deduct 0x1000 from the second memory
+		 * region, because of the fact that processor does prefetch.
+		 * Now that we are deducting a page from second memory
+		 * region, we could add the earlier deducted 4KB (from first bank)
+		 * to the second region (the fact that 0x80000000 -> 0x88000000
+		 * shadows 0x0 -> 0x8000000)
+		 */
+		if (MIPS74K(current_cpu_data.processor_id) && (mem == (128 MB)))
+			extmem -= 0x1000;
+		add_memory_region(SI_SDRAM_R2 + (128 MB) - 0x1000 , extmem, BOOT_MEM_RAM);
+	}
+#endif
 }
 
 #ifdef CONFIG_BLK_DEV_INITRD
diff --git a/arch/mips/kernel/setup.c b/arch/mips/kernel/setup.c
index 35ffffa..4db6aea 100644
--- a/arch/mips/kernel/setup.c
+++ b/arch/mips/kernel/setup.c
@@ -500,6 +500,9 @@ static void __init arch_mem_init(char **cmdline_p)
 	}
 
 	bootmem_init();
+#ifdef CONFIG_BCM53000_HIGHMEM
+	sparse_memory_present_with_active_regions(MAX_NUMNODES);
+#endif
 #ifdef CONFIG_KEXEC
 	pr_info("Crashkernel info:\n");
 	pr_info("\tstart=%llu end=%llu\n", crashk_res.start, crashk_res.end);
diff --git a/arch/mips/kernel/syscall.c b/arch/mips/kernel/syscall.c
index 75228f0..f649465 100644
--- a/arch/mips/kernel/syscall.c
+++ b/arch/mips/kernel/syscall.c
@@ -65,6 +65,9 @@ out:
 }
 
 unsigned long shm_align_mask = PAGE_SIZE - 1;	/* Sane caches */
+#ifdef CONFIG_BCM53000_HIGHMEM
+unsigned char shm_align_shift = PAGE_SHIFT;
+#endif
 
 EXPORT_SYMBOL(shm_align_mask);
 
diff --git a/arch/mips/mm/c-r4k.c b/arch/mips/mm/c-r4k.c
index 98951fb..7d5e227 100644
--- a/arch/mips/mm/c-r4k.c
+++ b/arch/mips/mm/c-r4k.c
@@ -1057,7 +1057,31 @@ static void __cpuinit probe_pcache(void)
 			"cache aliases" : "no aliases",
 	       c->dcache.linesz);
 }
+#ifdef CONFIG_BCM53000_HIGHMEM
+void __init r4k_probe_cache(void)
+{
+	unsigned long config1 = read_c0_config1();
+	unsigned int lsize, ways, sets;
+
+	if ((lsize = ((config1 >> 10) & 7)))
+		lsize = 2 << lsize;
+
+	sets = 64 << ((config1 >> 13) & 7);
+	ways = 1 + ((config1 >> 7) & 7);
+
+	if (lsize) {
+		shm_align_mask = max_t(unsigned long,
+								sets * lsize - 1,
+								PAGE_SIZE - 1);
+
+		if (shm_align_mask != (PAGE_SIZE - 1))
+			shm_align_shift = ffs((shm_align_mask + 1)) - 1;
+   } else
+		shm_align_mask = PAGE_SIZE-1;
 
+
+}
+#endif
 /*
  * If you even _breathe_ on this function, look at the gcc output and make sure
  * it does not pop things on and off the stack for the cache sizing loop that
@@ -1379,13 +1403,14 @@ void __cpuinit r4k_cache_init(void)
 	 * This code supports virtually indexed processors and will be
 	 * unnecessarily inefficient on physically indexed processors.
 	 */
+#ifndef CONFIG_BCM53000_HIGHMEM
 	if (c->dcache.linesz)
 		shm_align_mask = max_t( unsigned long,
 					c->dcache.sets * c->dcache.linesz - 1,
 					PAGE_SIZE - 1);
 	else
 		shm_align_mask = PAGE_SIZE-1;
-
+#endif
 	__flush_cache_vmap	= r4k__flush_cache_vmap;
 	__flush_cache_vunmap	= r4k__flush_cache_vunmap;
 
diff --git a/arch/mips/mm/cache.c b/arch/mips/mm/cache.c
index 6613746..1c7098e 100644
--- a/arch/mips/mm/cache.c
+++ b/arch/mips/mm/cache.c
@@ -54,7 +54,7 @@ void (*_dma_cache_inv)(unsigned long start, unsigned long size);
 EXPORT_SYMBOL(_dma_cache_wback_inv);
 
 #endif /* CONFIG_DMA_NONCOHERENT */
-
+extern void *kmap_atomic_page_address(struct page *page);
 /*
  * We could optimize the case where the cache argument is not BCACHE but
  * that seems very atypical use ...
@@ -67,6 +67,19 @@ SYSCALL_DEFINE3(cacheflush, unsigned long, addr, unsigned long, bytes,
 	if (!access_ok(VERIFY_WRITE, (void __user *) addr, bytes))
 		return -EFAULT;
 
+#ifdef CONFIG_BCM53000_HIGHMEM
+	if (cache & DCACHE) {
+
+		struct vm_area_struct *vma;
+		vma = find_vma(current->mm, (unsigned long) addr);
+		if (vma)
+			flush_cache_range(vma, (unsigned long) addr, ((unsigned long)addr) + bytes);
+		else
+			__flush_cache_all();
+
+    }
+  if (cache & ICACHE)
+#endif
 	flush_icache_range(addr, addr + bytes);
 
 	return 0;
@@ -75,10 +88,18 @@ SYSCALL_DEFINE3(cacheflush, unsigned long, addr, unsigned long, bytes,
 void __flush_dcache_page(struct page *page)
 {
 	struct address_space *mapping = page_mapping(page);
+
 	unsigned long addr;
 
-	if (PageHighMem(page))
+	if (PageHighMem(page)) {
+#ifdef CONFIG_BCM53000_HIGHMEM
+		addr = (unsigned long) kmap_atomic_page_address(page);
+		if (addr)
+			flush_data_cache_page(addr);
+#endif
 		return;
+	}
+
 	if (mapping && !mapping_mapped(mapping)) {
 		SetPageDcacheDirty(page);
 		return;
@@ -155,6 +176,17 @@ static inline void setup_protection_map(void)
 	protection_map[15] = PAGE_SHARED;
 }
 
+#ifdef CONFIG_BCM53000_HIGHMEM
+void __init cpu_early_probe_cache(void)
+{
+	if (cpu_has_4k_cache) {
+		extern void __weak r4k_probe_cache(void);
+
+		return r4k_probe_cache();
+	}
+}
+#endif
+
 void __cpuinit cpu_cache_init(void)
 {
 	if (cpu_has_3k_cache) {
diff --git a/arch/mips/mm/highmem.c b/arch/mips/mm/highmem.c
index 8afa9ab..7d5d0e7 100644
--- a/arch/mips/mm/highmem.c
+++ b/arch/mips/mm/highmem.c
@@ -40,9 +40,38 @@ EXPORT_SYMBOL(__kunmap);
  * kmaps are appropriate for short, tight code paths only.
  */
 
+#ifdef CONFIG_BCM53000_HIGHMEM
+/*
+ * need an array per cpu, and each array has to be cache aligned
+ */
+struct kmap_map {
+  struct page *page;
+  void        *vaddr;
+};
+
+struct {
+  struct kmap_map map[KM_TYPE_NR];
+} ____cacheline_aligned_in_smp kmap_atomic_maps[NR_CPUS];
+
+void *
+kmap_atomic_page_address(struct page *page)
+{
+  int i;
+
+	for (i = 0; i < KM_TYPE_NR; i++)
+		if (kmap_atomic_maps[smp_processor_id()].map[i].page == page)
+			return kmap_atomic_maps[smp_processor_id()].map[i].vaddr;
+
+	return (struct page *)0;
+}
+#endif
 void *__kmap_atomic(struct page *page, enum km_type type)
 {
+#ifdef CONFIG_BCM53000_HIGHMEM
+    unsigned int idx;
+#else
 	enum fixed_addresses idx;
+#endif
 	unsigned long vaddr;
 
 	/* even !CONFIG_PREEMPT needs this, for in_atomic in do_page_fault */
@@ -51,20 +80,65 @@ void *__kmap_atomic(struct page *page, enum km_type type)
 		return page_address(page);
 
 	idx = type + KM_TYPE_NR*smp_processor_id();
+#ifdef CONFIG_BCM53000_HIGHMEM
+	vaddr = fix_to_virt_noalias(VALIAS_IDX(FIX_KMAP_BEGIN + idx), page_to_pfn(page));
+#ifdef CONFIG_DEBUG_HIGHMEM
+	if (!pte_none(*(kmap_pte-(virt_to_fix(vaddr) - VALIAS_IDX(FIX_KMAP_BEGIN)))))
+		BUG();
+#endif
+	set_pte(kmap_pte-(virt_to_fix(vaddr) - VALIAS_IDX(FIX_KMAP_BEGIN)), \
+		 mk_pte(page, kmap_prot));
+#else
 	vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
 #ifdef CONFIG_DEBUG_HIGHMEM
 	if (!pte_none(*(kmap_pte-idx)))
 		BUG();
 #endif
 	set_pte(kmap_pte-idx, mk_pte(page, PAGE_KERNEL));
+#endif
 	local_flush_tlb_one((unsigned long)vaddr);
 
+#ifdef CONFIG_BCM53000_HIGHMEM
+	kmap_atomic_maps[smp_processor_id()].map[type].page = page;
+	kmap_atomic_maps[smp_processor_id()].map[type].vaddr = (void *)vaddr;
+#endif
 	return (void*) vaddr;
 }
 EXPORT_SYMBOL(__kmap_atomic);
 
 void __kunmap_atomic(void *kvaddr, enum km_type type)
 {
+#ifdef CONFIG_BCM53000_HIGHMEM
+    unsigned long vaddr = (unsigned long) kvaddr & PAGE_MASK;
+    unsigned int idx = type + KM_TYPE_NR*smp_processor_id();
+    struct page *page = kmap_atomic_maps[smp_processor_id()].map[type].page;
+
+	if (vaddr < FIXADDR_START) {
+		pagefault_enable();
+		return;
+	}
+
+	if (vaddr != fix_to_virt_noalias(VALIAS_IDX(FIX_KMAP_BEGIN + idx), page_to_pfn(page)))
+		BUG();
+	/*
+	 * Protect against multiple unmaps
+	 * Can't cache flush an unmapped page.
+	 */
+	if (kmap_atomic_maps[smp_processor_id()].map[type].vaddr) {
+		kmap_atomic_maps[smp_processor_id()].map[type].page = (struct page *)0;
+		kmap_atomic_maps[smp_processor_id()].map[type].vaddr = (void *) 0;
+
+		flush_data_cache_page((unsigned long)vaddr);
+	}
+#ifdef CONFIG_DEBUG_HIGHMEM
+	/*
+	 * force other mappings to Oops if they'll try to access
+	 * this pte without first remap it
+	 */
+    pte_clear(&init_mm, vaddr, kmap_pte-(virt_to_fix(vaddr) - VALIAS_IDX(FIX_KMAP_BEGIN)));
+    local_flush_tlb_one(vaddr);
+#endif
+#else
 #ifdef CONFIG_DEBUG_HIGHMEM
 	unsigned long vaddr = (unsigned long) kvaddr & PAGE_MASK;
 	enum fixed_addresses idx = type + KM_TYPE_NR*smp_processor_id();
@@ -84,7 +158,7 @@ void __kunmap_atomic(void *kvaddr, enum km_type type)
 	pte_clear(&init_mm, vaddr, kmap_pte-idx);
 	local_flush_tlb_one(vaddr);
 #endif
-
+#endif
 	pagefault_enable();
 }
 EXPORT_SYMBOL(__kunmap_atomic);
@@ -95,14 +169,25 @@ EXPORT_SYMBOL(__kunmap_atomic);
  */
 void *kmap_atomic_pfn(unsigned long pfn, enum km_type type)
 {
+#ifdef CONFIG_BCM53000_HIGHMEM
+    unsigned int idx;
+#else
 	enum fixed_addresses idx;
+#endif
 	unsigned long vaddr;
 
 	pagefault_disable();
 
 	idx = type + KM_TYPE_NR*smp_processor_id();
+#ifdef CONFIG_BCM53000_HIGHMEM
+	vaddr = fix_to_virt_noalias(VALIAS_IDX(FIX_KMAP_BEGIN + idx), pfn);
+
+	set_pte(kmap_pte-(virt_to_fix(vaddr) - VALIAS_IDX(FIX_KMAP_BEGIN)), \
+		 pfn_pte(pfn, kmap_prot));
+#else
 	vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
 	set_pte(kmap_pte-idx, pfn_pte(pfn, PAGE_KERNEL));
+#endif
 	flush_tlb_one(vaddr);
 
 	return (void*) vaddr;
@@ -110,14 +195,22 @@ void *kmap_atomic_pfn(unsigned long pfn, enum km_type type)
 
 struct page *__kmap_atomic_to_page(void *ptr)
 {
+#ifdef CONFIG_BCM53000_HIGHMEM
+        unsigned long vaddr = (unsigned long)ptr;
+#else
 	unsigned long idx, vaddr = (unsigned long)ptr;
+#endif
 	pte_t *pte;
 
 	if (vaddr < FIXADDR_START)
 		return virt_to_page(ptr);
 
+#ifdef CONFIG_BCM53000_HIGHMEM
+        pte = kmap_pte - (virt_to_fix(vaddr) - VALIAS_IDX(FIX_KMAP_BEGIN));
+#else
 	idx = virt_to_fix(vaddr);
 	pte = kmap_pte - (idx - FIX_KMAP_BEGIN);
+#endif
 	return pte_page(*pte);
 }
 
@@ -126,6 +219,10 @@ void __init kmap_init(void)
 	unsigned long kmap_vstart;
 
 	/* cache the first kmap pte */
+#ifdef CONFIG_BCM53000_HIGHMEM
+	kmap_vstart = __fix_to_virt(VALIAS_IDX(FIX_KMAP_BEGIN));
+#else
 	kmap_vstart = __fix_to_virt(FIX_KMAP_BEGIN);
+#endif
 	kmap_pte = kmap_get_fixmap_pte(kmap_vstart);
 }
diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index d05d4b4..42cb1a0 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -63,7 +63,9 @@
 #endif /* CONFIG_MIPS_MT_SMTC */
 
 DEFINE_PER_CPU(struct mmu_gather, mmu_gathers);
-
+#ifdef CONFIG_BCM53000_HIGHMEM
+extern void cpu_early_probe_cache(void);
+#endif
 /*
  * We have up to 8 empty zeroed pages so we can map one of the right colour
  * when needed.  This is necessary only on R4000 / R4400 SC and MC versions
@@ -234,6 +236,9 @@ void copy_to_user_page(struct vm_area_struct *vma,
 	    page_mapped(page) && !Page_dcache_dirty(page)) {
 		void *vto = kmap_coherent(page, vaddr) + (vaddr & ~PAGE_MASK);
 		memcpy(vto, src, len);
+#ifdef CONFIG_BCM53000_HIGHMEM
+	flush_data_cache_page((unsigned long)vto & PAGE_MASK);
+#endif
 		kunmap_coherent();
 	} else {
 		memcpy(dst, src, len);
@@ -324,7 +329,9 @@ void __init paging_init(void)
 {
 	unsigned long max_zone_pfns[MAX_NR_ZONES];
 	unsigned long lastpfn;
-
+#ifdef CONFIG_BCM53000_HIGHMEM
+	cpu_early_probe_cache();
+#endif
 	pagetable_init();
 
 #ifdef CONFIG_HIGHMEM
@@ -370,7 +377,11 @@ void __init mem_init(void)
 #ifdef CONFIG_DISCONTIGMEM
 #error "CONFIG_HIGHMEM and CONFIG_DISCONTIGMEM dont work together yet"
 #endif
-	max_mapnr = highend_pfn;
+#ifdef CONFIG_BCM53000_HIGHMEM
+    max_mapnr = max(max_low_pfn, highend_pfn);
+#else
+    max_mapnr = highend_pfn;
+#endif
 #else
 	max_mapnr = max_low_pfn;
 #endif
@@ -393,7 +404,9 @@ void __init mem_init(void)
 		struct page *page = pfn_to_page(tmp);
 
 		if (!page_is_ram(tmp)) {
+#ifndef CONFIG_BCM53000_HIGHMEM
 			SetPageReserved(page);
+#endif
 			continue;
 		}
 		ClearPageReserved(page);
diff --git a/arch/mips/mm/pgtable-32.c b/arch/mips/mm/pgtable-32.c
index 575e401..e09f987 100644
--- a/arch/mips/mm/pgtable-32.c
+++ b/arch/mips/mm/pgtable-32.c
@@ -51,7 +51,11 @@ void __init pagetable_init(void)
 	/*
 	 * Fixed mappings:
 	 */
+#ifdef CONFIG_BCM53000_HIGHMEM
+	vaddr = __fix_to_virt(VALIAS_IDX(__end_of_fixed_addresses - 1)) & PMD_MASK;
+#else
 	vaddr = __fix_to_virt(__end_of_fixed_addresses - 1) & PMD_MASK;
+#endif
 	fixrange_init(vaddr, 0, pgd_base);
 
 #ifdef CONFIG_HIGHMEM
diff --git a/include/asm-mips/fixmap.h b/include/asm-mips/fixmap.h
index dd924b9..cde2bce 100644
--- a/include/asm-mips/fixmap.h
+++ b/include/asm-mips/fixmap.h
@@ -72,13 +72,28 @@ enum fixed_addresses {
 #else
 #define FIXADDR_TOP	((unsigned long)(long)(int)0xfffe0000)
 #endif
+
+#ifdef CONFIG_BCM53000_HIGHMEM
+#define FIXADDR_SIZE	(__end_of_fixed_addresses << (VALIAS_PAGE_SHIFT))
+#else
 #define FIXADDR_SIZE	(__end_of_fixed_addresses << PAGE_SHIFT)
+#endif
 #define FIXADDR_START	(FIXADDR_TOP - FIXADDR_SIZE)
 
 #define __fix_to_virt(x)	(FIXADDR_TOP - ((x) << PAGE_SHIFT))
 #define __virt_to_fix(x)	((FIXADDR_TOP - ((x)&PAGE_MASK)) >> PAGE_SHIFT)
 
 extern void __this_fixmap_does_not_exist(void);
+#ifdef CONFIG_BCM53000_HIGHMEM
+static inline unsigned long fix_to_virt_noalias(const unsigned int x, unsigned long pfn)
+{
+	unsigned long vaddr = __fix_to_virt(x);
+	unsigned long poffset = (pfn << PAGE_SHIFT) & VALIAS_PAGE_OFFSET_MASK;
+	unsigned long voffset = vaddr & VALIAS_PAGE_OFFSET_MASK;
+
+	return ((!voffset || (poffset >= voffset)) ? (vaddr | poffset) : (vaddr | poffset | VALIAS_PAGE_SIZE));
+}
+#endif
 
 /*
  * 'index to address' translation. If anyone tries to use the idx
diff --git a/include/asm-mips/page.h b/include/asm-mips/page.h
index 5d2279f..34e1ee9 100644
--- a/include/asm-mips/page.h
+++ b/include/asm-mips/page.h
@@ -48,7 +48,16 @@ extern void clear_page(void * page);
 extern void copy_page(void * to, void * from);
 
 extern unsigned long shm_align_mask;
-
+#ifdef CONFIG_BCM53000_HIGHMEM
+extern unsigned char shm_align_shift;
+
+#define VALIAS_PAGE_OFFSET_MASK	(shm_align_mask)
+#define VALIAS_PAGE_MASK	(~VALIAS_PAGE_OFFSET_MASK)
+#define VALIAS_PAGE_SHIFT	(shm_align_shift)
+#define VALIAS_SHIFT		(VALIAS_PAGE_SHIFT - PAGE_SHIFT)
+#define VALIAS_PAGE_SIZE	(1UL << VALIAS_PAGE_SHIFT)
+#define VALIAS_IDX(x)		((x) << VALIAS_SHIFT)
+#endif
 static inline unsigned long pages_do_alias(unsigned long addr1,
 	unsigned long addr2)
 {
diff --git a/include/asm-mips/sparsemem.h b/include/asm-mips/sparsemem.h
index 795ac6c..a44c67a 100644
--- a/include/asm-mips/sparsemem.h
+++ b/include/asm-mips/sparsemem.h
@@ -6,8 +6,13 @@
  * SECTION_SIZE_BITS		2^N: how big each section will be
  * MAX_PHYSMEM_BITS		2^N: how much memory we can have in that space
  */
+#ifdef CONFIG_BCM53000_HIGHMEM
+#define SECTION_SIZE_BITS       27
+#define MAX_PHYSMEM_BITS        32
+#else
 #define SECTION_SIZE_BITS       28
 #define MAX_PHYSMEM_BITS        35
+#endif
 
 #endif /* CONFIG_SPARSEMEM */
 #endif /* _MIPS_SPARSEMEM_H */
diff --git a/mm/Kconfig b/mm/Kconfig
index 07b4ec4..6adebaa 100644
--- a/mm/Kconfig
+++ b/mm/Kconfig
@@ -11,7 +11,7 @@ choice
 
 config FLATMEM_MANUAL
 	bool "Flat Memory"
-	depends on !(ARCH_DISCONTIGMEM_ENABLE || ARCH_SPARSEMEM_ENABLE) || ARCH_FLATMEM_ENABLE
+	depends on !(ARCH_DISCONTIGMEM_ENABLE || ARCH_SPARSEMEM_ENABLE) || ARCH_FLATMEM_ENABLE && !BCM53000_HIGHMEM
 	help
 	  This option allows you to change some of the ways that
 	  Linux manages its memory internally.  Most users will
diff --git a/mm/highmem.c b/mm/highmem.c
index e16e152..e757e7e 100644
--- a/mm/highmem.c
+++ b/mm/highmem.c
@@ -129,8 +129,25 @@ start:
 			flush_all_zero_pkmaps();
 			count = LAST_PKMAP;
 		}
+#ifdef CONFIG_BCM53000_HIGHMEM
+		if (!pkmap_count[last_pkmap_nr]) {
+			if (cpu_has_dc_aliases) {
+				unsigned int pfn, map_pfn;
+
+				/* check page color */
+				pfn = page_to_pfn(page);
+				map_pfn = PKMAP_ADDR(last_pkmap_nr) >> PAGE_SHIFT;
+
+				/* Avoide possibility of cache Aliasing */
+				if (!pages_do_alias((map_pfn << PAGE_SHIFT), (pfn << PAGE_SHIFT)))
+					break;  /* Found a usable entry */
+			} else
+				break;  /* Found a usable entry */
+		}
+#else
 		if (!pkmap_count[last_pkmap_nr])
 			break;	/* Found a usable entry */
+#endif
 		if (--count)
 			continue;
 
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 12fdae0..1204673 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -3492,8 +3492,12 @@ static void __paginginit free_area_init_core(struct pglist_data *pgdat,
 		 * is used by this zone for memmap. This affects the watermark
 		 * and per-cpu initialisations
 		 */
+#ifdef CONFIG_BCM53000_HIGHMEM
+		memmap_pages = 0;
+#else
 		memmap_pages =
 			PAGE_ALIGN(size * sizeof(struct page)) >> PAGE_SHIFT;
+#endif
 		if (realsize >= memmap_pages) {
 			realsize -= memmap_pages;
 			mminit_dprintk(MMINIT_TRACE, "memmap_init",
-- 
1.7.0.4

