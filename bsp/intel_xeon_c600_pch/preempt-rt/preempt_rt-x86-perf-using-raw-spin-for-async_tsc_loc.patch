From fdc70117bc2958ed2a453f00acf64cb6fd4bba38 Mon Sep 17 00:00:00 2001
From: Zumeng Chen <zumeng.chen@windriver.com>
Date: Fri, 16 Mar 2012 15:40:26 +0800
Subject: [PATCH 477/478] preempt_rt:x86:perf using raw spin for async_tsc_lock

With the current code, it is possible to trigger the below warning
sequence by initiating a CPU hotplug event.

Call Trace:
 [<ffffffff810316a5>] __might_sleep+0xe5/0x110
 [<ffffffff8160c7b2>] ? _raw_spin_unlock_irqrestore+0x12/0x40
 [<ffffffff8160c514>] rt_spin_lock+0x34/0x80
 [<ffffffff81602c14>] hotcpu_callback+0x1f/0x12f
 [<ffffffff8106c407>] notifier_call_chain+0x47/0x90
 [<ffffffff81090a20>] ? stop_cpu+0x0/0xf0
 [<ffffffff8106c4f1>] raw_notifier_call_chain+0x11/0x20
 [<ffffffff815f594d>] take_cpu_down+0x3d/0x70
 [<ffffffff81090a20>] ? stop_cpu+0x0/0xf0
 [<ffffffff81090ac9>] stop_cpu+0xa9/0xf0
 [<ffffffff81061541>] worker_thread+0x1a1/0x330
 [<ffffffff810667d0>] ? autoremove_wake_function+0x0/0x40
 [<ffffffff810613a0>] ? worker_thread+0x0/0x330
 [<ffffffff810613a0>] ? worker_thread+0x0/0x330
 [<ffffffff8106639e>] kthread+0x8e/0xa0
 [<ffffffff810392a4>] ? finish_task_switch+0x54/0xc0
 [<ffffffff81003374>] kernel_thread_helper+0x4/0x10
 [<ffffffff81066310>] ? kthread+0x0/0xa0
 [<ffffffff81003370>] ? kernel_thread_helper+0x0/0x10
CPU 4 is now offline
Booting Node 0 Processor 4 APIC 0x4
checking TSC synchronization [CPU#0 -> CPU#4]: passed.

This is caused by the use of the async_tsc_lock [which gets
converted in the RT instance] in an invalid context (might sleep).

As with all similar type issues in RT, changing it to a
raw lock resolves the symptom, but opens the door for
introducing a potential latency spike on hot paths.

For the async_tsc_lock, the callers are as follows:

  tracing_enabled ----\
                        ---->
  register_tracer ----/           # This is just for test

          register_trace_clock->
                 get_trace_clock->
                     raw_spin_lock(async_tsc_lock)

Since it is just a registration operation during a debug/test
code path, it seems reasonable to convert the lock to raw
without having large concerns about latency introductions.

Signed-off-by: Zumeng Chen <zumeng.chen@windriver.com>
Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
---
 arch/x86/kernel/trace-clock.c |   14 +++++++-------
 1 files changed, 7 insertions(+), 7 deletions(-)

diff --git a/arch/x86/kernel/trace-clock.c b/arch/x86/kernel/trace-clock.c
index 47539e2..e7c5984 100644
--- a/arch/x86/kernel/trace-clock.c
+++ b/arch/x86/kernel/trace-clock.c
@@ -16,7 +16,7 @@
 
 static cycles_t trace_clock_last_tsc;
 static DEFINE_PER_CPU(struct timer_list, update_timer);
-static DEFINE_SPINLOCK(async_tsc_lock);
+static DEFINE_RAW_SPINLOCK(async_tsc_lock);
 static int async_tsc_refcount;	/* Number of readers */
 static int async_tsc_enabled;	/* Async TSC enabled on all online CPUs */
 
@@ -145,7 +145,7 @@ static int __cpuinit hotcpu_callback(struct notifier_block *nb,
 	unsigned int hotcpu = (unsigned long)hcpu;
 	int cpu;
 
-	spin_lock(&async_tsc_lock);
+	raw_spin_lock(&async_tsc_lock);
 	switch (action) {
 	case CPU_UP_PREPARE:
 	case CPU_UP_PREPARE_FROZEN:
@@ -189,7 +189,7 @@ static int __cpuinit hotcpu_callback(struct notifier_block *nb,
 		break;
 #endif /* CONFIG_HOTPLUG_CPU */
 	}
-	spin_unlock(&async_tsc_lock);
+	raw_spin_unlock(&async_tsc_lock);
 
 	return NOTIFY_OK;
 }
@@ -210,7 +210,7 @@ int get_trace_clock(void)
 	}
 
 	get_online_cpus();
-	spin_lock(&async_tsc_lock);
+	raw_spin_lock(&async_tsc_lock);
 	if (async_tsc_refcount++ || trace_clock_is_sync())
 		goto end;
 
@@ -218,7 +218,7 @@ int get_trace_clock(void)
 	for_each_online_cpu(cpu)
 		enable_trace_clock(cpu);
 end:
-	spin_unlock(&async_tsc_lock);
+	raw_spin_unlock(&async_tsc_lock);
 	put_online_cpus();
 	return 0;
 }
@@ -229,7 +229,7 @@ void put_trace_clock(void)
 	int cpu;
 
 	get_online_cpus();
-	spin_lock(&async_tsc_lock);
+	raw_spin_lock(&async_tsc_lock);
 	WARN_ON(async_tsc_refcount <= 0);
 	if (async_tsc_refcount != 1 || !async_tsc_enabled)
 		goto end;
@@ -241,7 +241,7 @@ end:
 	async_tsc_refcount--;
 	if (!async_tsc_refcount && num_online_cpus() == 1)
 		set_trace_clock_is_sync(1);
-	spin_unlock(&async_tsc_lock);
+	raw_spin_unlock(&async_tsc_lock);
 	put_online_cpus();
 }
 EXPORT_SYMBOL_GPL(put_trace_clock);
-- 
1.7.0

