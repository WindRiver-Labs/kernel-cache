From de3f16f1793c040e21a3b48fd1ef1da5293939cd Mon Sep 17 00:00:00 2001
From: Jack Tan <jack.tan@windriver.com>
Date: Tue, 16 Dec 2008 19:07:00 +0800
Subject: [PATCH] rmi xlr cache operations support

RMI XLR is a multi-core processor. It has hardware machanism to make each core's
cache coherent. In spite of it's a MIPS64 implementation it has a diffrent cache
style against R4K. So it need own cache flush operations.

Signed-off-by: Jack Tan <jack.tan@windriver.com>
---
 arch/mips/mm/c-phoenix.c      |  508 +++++++++++++++++++++++++++++++++++++++++
 arch/mips/mm/cache.c          |   20 ++
 include/asm-mips/cacheflush.h |    6 +-
 3 files changed, 533 insertions(+), 1 deletions(-)
 create mode 100644 arch/mips/mm/c-phoenix.c

diff --git a/arch/mips/mm/c-phoenix.c b/arch/mips/mm/c-phoenix.c
new file mode 100644
index 0000000..fb26c3d
--- /dev/null
+++ b/arch/mips/mm/c-phoenix.c
@@ -0,0 +1,508 @@
+/************************************************************************
+
+  Copyright 2003-2006 Raza Microelectronics, Inc.(RMI).
+
+  This is a derived work from software originally provided by the external
+  entity identified below. The licensing terms and warranties specified in
+  the header of the original work apply to this derived work.
+
+  Contribution by RMI: 
+
+  *****************************#RMI_1#************************************/
+/*
+ * Copyright (C) 1996 David S. Miller (dm@engr.sgi.com)
+ * Copyright (C) 1997, 2001 Ralf Baechle (ralf@gnu.org)
+ * 
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version 2
+ * of the License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ * 
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
+ */
+#include <linux/init.h>
+#include <asm/asm.h>
+#include <asm/mmu_context.h>
+#include <asm/bootinfo.h>
+#include <asm/cacheops.h>
+#include <asm/cpu.h>
+#include <asm/uaccess.h>
+#include <linux/smp.h>
+#include <linux/kallsyms.h>
+#include <linux/mm.h>
+#include <linux/module.h>
+
+#include <asm/rmi/debug.h>
+
+static unsigned int icache_linesz;
+static unsigned int icache_lines;
+
+#define cacheop(op, base) do {								\
+	__asm__ __volatile__ (									\
+							".set push\n\t"					\
+							".set mips4\n\t"				\
+							"cache %0, 0(%1)\n\t"			\
+							".set pop\n\t" 					\
+							: : "i"(op), "r"(base));		\
+	} while (0)
+
+#define cacheop_extable(op, base) do {						\
+	__asm__ __volatile__ (									\
+							".set push\n\t"					\
+							".set noreorder\n\t"			\
+							".set mips4\n\t"				\
+							"1: cache %0, 0(%1)\n\t"		\
+							"2: .set pop\n\t"				\
+							".section __ex_table,\"a\"\n\t"	\
+							STR(PTR)"\t1b, 2b\n\t"			\
+							".previous\n\t"					\
+							: : "i" (op), "r" (base));		\
+	} while (0)
+
+static __inline__ void sync_istream(void)
+{
+	__asm__ __volatile__(
+							".set push\n\t"
+							".set noreorder\n\t"
+							"nop\n\t"
+							"nop\n\t"
+							"nop\n\t"
+							"nop\n\t"
+							"nop\n\t"
+							"nop\n\t"
+							"nop\n\t"
+							"nop\n\t"
+							"nop\n\t"
+							"nop\n\t"
+							".set pop\n\t"
+							:::"$8"
+						);
+}
+
+static __inline__ void cacheop_hazard(void)
+{
+	__asm__ __volatile__(
+							".set push\n\t"
+						 	".set noreorder\n\t"
+						 	"nop;nop;nop;nop\n\t"
+						 	"nop;nop;nop;nop\n\t"
+						 	".set pop\n\t"
+						);
+}
+
+static __inline__ void cacheop_sync_istream(void)
+{
+	cacheop_hazard();
+	sync_istream();
+}
+
+#define optimize_thread_flush()
+
+extern unsigned long phnx_ebase;
+
+void phoenix_flush_dcache_page(struct page *page)
+{
+	ClearPageDcacheDirty(page);
+}
+
+EXPORT_SYMBOL(phoenix_flush_dcache_page);
+
+static void phoenix_local_flush_icache_range(unsigned long start,
+											 unsigned long end)
+{
+	unsigned long addr;
+
+	for (addr = (start & ~((unsigned long) (icache_linesz - 1))); addr < end;
+		 addr += icache_linesz) {
+		cacheop_extable(Hit_Invalidate_I, addr);
+	}
+
+	cacheop_sync_istream();
+}
+
+struct flush_icache_range_args
+{
+	unsigned long start;
+	unsigned long end;
+};
+
+struct flush_icache_range_args_paddr
+{
+	phys_t start;
+	phys_t end;
+};
+
+static void phoenix_flush_icache_range_ipi(void *info)
+{
+	struct flush_icache_range_args *args = info;
+
+	optimize_thread_flush();
+
+	phoenix_local_flush_icache_range(args->start, args->end);
+}
+
+void phoenix_flush_icache_range(unsigned long start, unsigned long end)
+{
+	struct flush_icache_range_args args;
+
+#ifdef CONFIG_PHOENIX_VM_DEBUG
+	dbg_msg("return address: ");
+	print_symbol("ra[0]=%s\n", (unsigned long) __builtin_return_address(0));
+#endif
+
+	if ((end - start) > PAGE_SIZE) {
+		dbg_msg("flushing more than page size of icache \
+					addresses starting @ %lx\n", start);
+	}
+
+	args.start = start;
+	args.end = end;
+	/*
+	 * TODO: don't even send ipi to non-zero thread ids 
+	 * This may require some changes to smp_call_function interface, for now 
+	 * just avoid redundant cache ops
+	*/
+	on_each_cpu(phoenix_flush_icache_range_ipi, &args, 1);
+}
+
+static void phoenix_flush_cache_sigtramp_ipi(void *info)
+{
+	unsigned long addr = (unsigned long) info;
+
+	optimize_thread_flush();
+
+	addr = addr & ~(icache_linesz - 1);
+	cacheop_extable(Hit_Invalidate_I, addr);
+	cacheop_sync_istream();
+}
+
+static void phoenix_flush_cache_sigtramp(unsigned long addr)
+{
+	on_each_cpu(phoenix_flush_cache_sigtramp_ipi, (void *) addr, 1);
+}
+
+static void phoenix_local_flush_icache(void)
+{
+	int i = 0;
+	unsigned long base = CKSEG0;
+
+	/* Index Invalidate all the lines and the ways */
+	for (i = 0; i < icache_lines; i++) {
+		cacheop(Index_Invalidate_I, base);
+		base += icache_linesz;
+	}
+
+	cacheop_sync_istream();
+
+}
+
+static void phoenix_local_flush_dcache(void)
+{
+	int i = 0;
+	unsigned long base = CKSEG0;
+	unsigned int lines;
+
+	lines = current_cpu_data.dcache.ways * current_cpu_data.dcache.sets;
+
+	/* Index Invalidate all the lines and the ways */
+	for (i = 0; i < lines; i++) {
+		cacheop(Index_Writeback_Inv_D, base);
+		base += current_cpu_data.dcache.linesz;
+	}
+
+	cacheop_hazard();
+
+}
+
+#ifdef CONFIG_KGDB
+void phoenix_flush_l1_caches_ipi(void *info)
+#else
+static void phoenix_flush_l1_caches_ipi(void *info)
+#endif
+{
+	optimize_thread_flush();
+
+	phoenix_local_flush_dcache();
+	phoenix_local_flush_icache();
+}
+
+static void phoenix_flush_l1_caches(void)
+{
+	on_each_cpu(phoenix_flush_l1_caches_ipi, (void *) NULL, 1);
+}
+
+static void phoenix_noflush(void)
+{
+	/* do nothing */
+}
+
+static __init void probe_l1_cache(void)
+{
+	struct cpuinfo_mips *c = &current_cpu_data;
+	unsigned int config1 = read_c0_config1();
+	int lsize = 0;
+	int icache_size = 0, dcache_size = 0;
+
+	if ((lsize = ((config1 >> 19) & 7)))
+		c->icache.linesz = 2 << lsize;
+	else
+		c->icache.linesz = lsize;
+	c->icache.sets = 64 << ((config1 >> 22) & 7);
+	c->icache.ways = 1 + ((config1 >> 16) & 7);
+
+	icache_size = c->icache.sets * c->icache.ways * c->icache.linesz;
+	c->icache.waybit = ffs(icache_size / c->icache.ways) - 1;
+
+	c->dcache.flags = 0;
+
+	if ((lsize = ((config1 >> 10) & 7)))
+		c->dcache.linesz = 2 << lsize;
+	else
+		c->dcache.linesz = lsize;
+	c->dcache.sets = 64 << ((config1 >> 13) & 7);
+	c->dcache.ways = 1 + ((config1 >> 7) & 7);
+
+	dcache_size = c->dcache.sets * c->dcache.ways * c->dcache.linesz;
+	c->dcache.waybit = ffs(dcache_size / c->dcache.ways) - 1;
+
+	if (smp_processor_id() == 0) {
+		printk("Primary instruction cache %dkB, %d-way, %s, linesize %d \
+				bytes.\n",
+				icache_size >> 10, c->icache.ways, 
+				cpu_has_vtag_icache ? "VIVT" : "VIPT",
+				c->icache.linesz);
+
+		printk("Primary data cache %dkB %d-way, %s, %s, linesize %d bytes.\n",
+				dcache_size >> 10, c->dcache.ways, 
+				(c->dcache.flags & MIPS_CACHE_PINDEX) ? "PIPT" : "VIPT",
+				(c->dcache.flags & MIPS_CACHE_ALIASES) ?
+				"cache aliases" : "no aliases",
+				c->dcache.linesz);
+	}
+
+}
+
+static __inline__ void install_cerr_handler(void)
+{
+	extern char except_vec2_generic;
+
+	memcpy((void *) (phnx_ebase + 0x100), &except_vec2_generic, 0x80);
+}
+
+static int __cpuinitdata cca = -1;
+
+static int __init cca_setup(char *str)
+{
+	get_option(&str, &cca);
+
+	return 1;
+}
+__setup("cca=", cca_setup);
+
+static void update_kseg0_coherency(void)
+{
+	if (cca < 0 || cca > 7)
+		cca = read_c0_config() & CONF_CM_CMASK;
+	_page_cachable_default = cca << _CACHE_SHIFT;
+
+	pr_debug("Using cache attribute %d\n", cca);
+
+	phoenix_local_flush_dcache();
+	phoenix_local_flush_icache();
+
+	change_c0_config(CONF_CM_CMASK, cca);
+
+	sync_istream();
+}
+
+void ld_mmu_phoenix(void)
+{
+	/* update cpu_data */
+	probe_l1_cache();
+
+	if (smp_processor_id()) {
+
+		/*
+		 * flush the exception vector region to make sure 
+		 * not to execute bootloader's exception code 
+		*/
+		phoenix_local_flush_icache_range(phnx_ebase, phnx_ebase + 0x400);
+
+		update_kseg0_coherency();
+
+		return;
+	}
+
+	/* These values are assumed to be the same for all cores */
+	icache_lines =
+		current_cpu_data.icache.ways * current_cpu_data.icache.sets;
+	icache_linesz = current_cpu_data.icache.linesz;
+
+	/*
+	 * When does this function get called? Looks like MIPS has some syscalls
+	 * to flush the caches. 
+	*/
+	__flush_cache_all = phoenix_flush_l1_caches;
+
+	/*
+	 * flush_cache_all: makes all kernel data coherent.
+	 * This gets called just before changing or removing
+	 * a mapping in the page-table-mapped kernel segment (kmap). 
+	 * Physical Cache -> do nothing
+	*/
+	flush_cache_all = phoenix_noflush;
+
+	/*
+	 * flush_icache_range: makes the range of addresses coherent w.r.t I-cache 
+	 * and D-cache. This gets called after the instructions are written to 
+	 * memory. All addresses are valid kernel or mapped user-space virtual
+	 * addresses.
+	*/
+	flush_icache_range = phoenix_flush_icache_range;
+	local_flush_icache_range = phoenix_local_flush_icache_range;
+
+	/*
+	 * flush_cache_{mm, range, page}: make these memory locations, that may 
+	 * have been written by a user process, coherent.
+	 * These get called when virtual->physical translation of a user address
+	 * space is about to be changed. These are closely related to TLB coherency
+	 * (flush_tlb_{mm, range, page})
+	*/
+	flush_cache_mm = (void (*)(struct mm_struct *)) phoenix_noflush;
+	flush_cache_range = (void *) phoenix_noflush;
+	flush_cache_page = (void *) phoenix_noflush;
+
+	flush_data_cache_page = (void *) phoenix_noflush;
+
+	/* flush the single I-cache line with the proper fixup code */
+	flush_cache_sigtramp = phoenix_flush_cache_sigtramp;
+
+	/* This should get called only for Virtuall Tagged I-Caches */
+	flush_icache_all = (void *) phoenix_noflush;
+
+	install_cerr_handler();
+
+	update_kseg0_coherency();
+}
+
+#define cacheop_paddr(op, base)							\
+	__asm__ __volatile__ (								\
+					".set push\n\t"						\
+					".set noreorder\n\t"				\
+					".set mips64\n\t"					\
+					"dli $8, 0x9800000000000000\n\t"	\
+					"daddu $8, $8, %1\n\t"				\
+					"cache %0, 0($8)\n\t"				\
+					".set pop\n\t"						\
+					: : "i"(op), "r"(base) : "$8"		\
+			);
+
+#define enable_KX(flags)					\
+	preempt_disable();						\
+	__asm__ __volatile__ (					\
+					".set push\n\t"			\
+					".set noat\n\t"			\
+					".set noreorder\n\t"	\
+					"mfc0 %0, $12\n\t"		\
+					"ori $1, %0, 0x81\n\t"	\
+					"xori $1, 1\n\t"		\
+					"mtc0 $1, $12\n\t"		\
+					".set pop\n\t"			\
+					: "=r"(flags)			\
+				);							\
+	preempt_enable();
+
+#define disable_KX(flags)					\
+	__asm__ __volatile__ (					\
+					".set push\n\t"			\
+					"mtc0 %0, $12\n\t"		\
+					".set pop\n\t"			\
+					: : "r"(flags)			\
+			);
+
+static void phoenix_local_flush_icache_range_paddr(phys_t start, phys_t end)
+{
+	phys_t addr;
+#ifdef CONFIG_32BIT
+	unsigned long flags;
+	phys_t temp, temp1;
+	uint64_t temp_msb, temp_lsb;
+#endif
+
+#ifdef CONFIG_64BIT
+	for (addr = (start & ~(phys_t)(icache_linesz - 1)); addr < end; 
+			addr += icache_linesz) {
+		cacheop_paddr(Hit_Invalidate_I, addr);
+	}
+#else
+	enable_KX(flags);
+	for (addr = (start & ~(phys_t) (icache_linesz - 1)); addr < end;
+		 addr += icache_linesz) {
+		temp = addr;
+		temp_msb = (uint64_t) temp >> 32;
+		temp_lsb = (uint64_t) temp & 0xffffffff;
+		__asm__ volatile (
+							".set push\n\t"
+							".set noreorder\n\t"
+							".set mips64\n\t"
+							"dli $8,0x9800000000000000\n\t"
+							"dsll32 %0, %2,0\n\t"
+							"or %0,%0,%3\n\t"
+							"daddu $8, $8, %0\n\t"
+							"cache %1, 0($8)\n\t"
+							".set pop\n\t"
+							".set reorder\n\t"
+							:"=&r" (temp1)
+							:"i"(Hit_Invalidate_I), "r"(temp_msb), "r"(temp_lsb)
+							:"$8"
+						);
+	}
+	disable_KX(flags);
+#endif
+
+	cacheop_sync_istream();
+}
+
+static void phoenix_flush_icache_range_paddr_ipi(void *info)
+{
+	struct flush_icache_range_args_paddr *args = info;
+
+	optimize_thread_flush();
+
+	phoenix_local_flush_icache_range_paddr(args->start, args->end);
+}
+
+void phoenix_flush_icache_range_paddr(phys_t start)
+{
+	struct flush_icache_range_args_paddr args;
+
+#ifdef CONFIG_PHOENIX_VM_DEBUG
+	dbg_msg("return address: ");
+	print_symbol("ra[0]=%s\n", (unsigned long) __builtin_return_address(0));
+#endif
+
+	args.start = start;
+	args.end = start + PAGE_SIZE;
+	/*
+	 * TODO: don't even send ipi to non-zero thread ids 
+	 * This may require some changes to smp_call_function interface, for now
+	 * just avoid redundant cache ops.
+	*/
+	on_each_cpu(phoenix_flush_icache_range_paddr_ipi, &args, 1);
+}
+
+void btlb_phoenix_flush_icache_range_paddr(phys_t start, phys_t end)
+{
+	struct flush_icache_range_args_paddr args;
+
+	args.start = start;
+	args.end = end;
+	on_each_cpu(phoenix_flush_icache_range_paddr_ipi, &args, 1);
+}
diff --git a/arch/mips/mm/cache.c b/arch/mips/mm/cache.c
index 789c97a..24ae1cf 100644
--- a/arch/mips/mm/cache.c
+++ b/arch/mips/mm/cache.c
@@ -113,10 +113,24 @@ void __flush_anon_page(struct page *page, unsigned long vmaddr)
 
 EXPORT_SYMBOL(__flush_anon_page);
 
+extern void phoenix_flush_icache_range_paddr(phys_t start);
 void __update_cache(struct vm_area_struct *vma, unsigned long address,
 	pte_t pte)
 {
 	struct page *page;
+
+#ifdef CONFIG_RMI_PHOENIX
+	phys_t start;
+	if (!(vma->vm_flags & VM_EXEC)) return;
+
+	page = pte_page(pte);
+
+	if (Page_dcache_dirty(page)) return;
+
+	start = (phys_t)pte_pfn(pte);
+	phoenix_flush_icache_range_paddr(start << PAGE_SHIFT);
+	SetPageDcacheDirty(page);
+#else
 	unsigned long pfn, addr;
 	int exec = (vma->vm_flags & VM_EXEC) && !cpu_has_ic_fills_f_dc;
 
@@ -130,6 +144,7 @@ void __update_cache(struct vm_area_struct *vma, unsigned long address,
 			flush_data_cache_page(addr);
 		ClearPageDcacheDirty(page);
 	}
+#endif
 }
 
 unsigned long _page_cachable_default;
@@ -182,6 +197,11 @@ void __devinit cpu_cache_init(void)
 
 		tx39_cache_init();
 	}
+	if (cpu_has_phoenix_cache) {
+		extern void __weak ld_mmu_phoenix(void);
+
+		ld_mmu_phoenix();
+	}
 
 	setup_protection_map();
 }
diff --git a/include/asm-mips/cacheflush.h b/include/asm-mips/cacheflush.h
index 03b1d69..3277baf 100644
--- a/include/asm-mips/cacheflush.h
+++ b/include/asm-mips/cacheflush.h
@@ -40,9 +40,13 @@ extern void __flush_dcache_page(struct page *page);
 
 static inline void flush_dcache_page(struct page *page)
 {
+#ifdef CONFIG_RMI_PHOENIX
+	extern void phoenix_flush_dcache_page(struct page *page);
+	phoenix_flush_dcache_page(page);  
+#else
 	if (cpu_has_dc_aliases || !cpu_has_ic_fills_f_dc)
 		__flush_dcache_page(page);
-
+#endif
 }
 
 #define flush_dcache_mmap_lock(mapping)		do { } while (0)
-- 
1.6.0.4

