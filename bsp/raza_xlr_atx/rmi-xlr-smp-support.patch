From c41b2f4a8f6734d8f252f7444b8a7e0a0786c35b Mon Sep 17 00:00:00 2001
From: Jack Tan <jack.tan@windriver.com>
Date: Tue, 16 Dec 2008 19:06:58 +0800
Subject: [PATCH] rmi xlr smp support

RMI XLR732 has 8 cores and 4 hardware threads per core. Following is the smp
support for it.

Signed-off-by: Jack Tan <jack.tan@windriver.com>
---
 arch/mips/kernel/smp.c          |   27 ++++-
 arch/mips/rmi/phoenix/smp.c     |  303 +++++++++++++++++++++++++++++++++++++++
 arch/mips/rmi/phoenix/smpboot.S |   71 +++++++++
 3 files changed, 400 insertions(+), 1 deletions(-)
 create mode 100644 arch/mips/rmi/phoenix/smp.c
 create mode 100644 arch/mips/rmi/phoenix/smpboot.S

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 5b3c1d7..33bee50 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -44,6 +44,11 @@
 #include <asm/mipsmtregs.h>
 #endif /* CONFIG_MIPS_MT_SMTC */
 
+#ifdef CONFIG_RMI_PHOENIX
+#include <asm/rmi/mips-exts.h>
+#include <asm/rmi/debug.h>
+#endif
+
 cpumask_t phys_cpu_present_map;		/* Bitmask of available CPUs */
 volatile cpumask_t cpu_callin_map;	/* Bitmask of started secondaries */
 cpumask_t cpu_online_map;		/* Bitmask of currently online CPUs */
@@ -101,6 +106,12 @@ asmlinkage __cpuinit void start_secondary(void)
 {
 	unsigned int cpu;
 
+#ifdef CONFIG_RMI_PHOENIX
+	dbg_msg("Initializing CPU:%d (%d), current=%lx, gp = %lx\n", 
+			smp_processor_id(), hard_smp_processor_id(),
+			(unsigned long)current, (unsigned long)current_thread_info());
+#endif
+
 #ifdef CONFIG_MIPS_MT_SMTC
 	/* Only do cpu_probe for first TC of CPU */
 	if ((read_c0_tcbind() & TCBIND_CURTC) == 0)
@@ -115,7 +126,9 @@ asmlinkage __cpuinit void start_secondary(void)
 	 * XXX parity protection should be folded in here when it's converted
 	 * to an option instead of something based on .cputype
 	 */
-
+#ifdef CONFIG_RMI_PHOENIX
+	local_irq_enable();
+#endif
 	calibrate_delay();
 	preempt_disable();
 	cpu = smp_processor_id();
@@ -193,6 +206,17 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 /* preload SMP state for boot cpu */
 void __devinit smp_prepare_boot_cpu(void)
 {
+
+#ifdef CONFIG_RMI_PHOENIX
+	__u32 boot_cpu;
+	preempt_disable();
+	boot_cpu = hard_smp_processor_id();
+
+	__cpu_number_map[boot_cpu] = 0;
+	__cpu_logical_map[0] = boot_cpu;
+	cpu_set(boot_cpu, phys_cpu_present_map);
+	preempt_enable();
+#else
 	/*
 	 * This assumes that bootup is always handled by the processor
 	 * with the logic and physical number 0.
@@ -200,6 +224,7 @@ void __devinit smp_prepare_boot_cpu(void)
 	__cpu_number_map[0] = 0;
 	__cpu_logical_map[0] = 0;
 	cpu_set(0, phys_cpu_present_map);
+#endif
 	cpu_set(0, cpu_online_map);
 	cpu_set(0, cpu_callin_map);
 }
diff --git a/arch/mips/rmi/phoenix/smp.c b/arch/mips/rmi/phoenix/smp.c
new file mode 100644
index 0000000..87c4f7d
--- /dev/null
+++ b/arch/mips/rmi/phoenix/smp.c
@@ -0,0 +1,303 @@
+/*********************************************************************
+
+  Copyright 2003-2006 Raza Microelectronics, Inc. (RMI). All rights
+  reserved.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+  1. Redistributions of source code must retain the above copyright
+  notice, this list of conditions and the following disclaimer.
+  2. Redistributions in binary form must reproduce the above copyright
+  notice, this list of conditions and the following disclaimer in
+  the documentation and/or other materials provided with the
+  distribution.
+
+  THIS SOFTWARE IS PROVIDED BY Raza Microelectronics, Inc. ``AS IS'' AND
+  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL RMI OR CONTRIBUTORS BE LIABLE
+  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+  THE POSSIBILITY OF SUCH DAMAGE.
+
+  *****************************#RMI_2#**********************************/
+
+#include <linux/sched.h>
+#include <linux/types.h>
+#include <linux/hardirq.h>
+
+#include <asm/addrspace.h>
+#include <asm/smp.h>
+#include <asm/mipsregs.h>
+#include <asm/mmu_context.h>
+#include <asm/atomic.h>
+
+#include <asm/rmi/sim.h>
+#include <asm/rmi/mips-exts.h>
+#include <asm/rmi/pic.h>
+#include <asm/rmi/msgring.h>
+#include <asm/rmi/phnx_user_mac.h>
+#include <asm/rmi/phnx_loader.h>
+
+/* ipi statistics counters for debugging */
+__u32 ipi_3_counter_tx[NR_CPUS][NR_CPUS];
+__u32 ipi_3_counter_rx[NR_CPUS];
+
+extern void save_epc(unsigned long *epc);
+extern void smp_call_function_interrupt(void);
+extern void phoenix_smp_time_init(void);
+
+static int phoenix_ipi_stats[NR_CPUS];
+
+static void phoenix_send_ipi_single(int logical_cpu, unsigned int action)
+{
+	int cpu = cpu_logical_map(logical_cpu);
+	__u32 ipi = 0;
+	__u32 tid = cpu & 0x3;
+	__u32 pid = (cpu >> 2) & 0x07;
+
+	if (action & SMP_CALL_FUNCTION) {
+		ipi = (tid << 16) | (pid << 20) | IRQ_IPI_SMP_FUNCTION;
+#ifdef IPI_PRINTK_DEBUG
+		printk("[%s]: cpu_%d sending ipi_3 to cpu_%d \t\t\t[->%u] \n",
+			   __func__, smp_processor_id(), cpu,
+			   ipi_3_counter_tx[smp_processor_id()][cpu] + 1);
+#endif
+		++ipi_3_counter_tx[smp_processor_id()][cpu];
+	}
+	else if (action & SMP_RESCHEDULE_YOURSELF) {
+		ipi = (tid << 16) | (pid << 20) | IRQ_IPI_SMP_RESCHEDULE;
+#ifdef IPI_PRINTK_DEBUG
+		printk("[%s]: cpu_%d sending ipi_4 to cpu_%d\n", __func__,
+			   smp_processor_id(), cpu);
+#endif
+	}
+	else if (action & SMP_CALL_KGDB_HOOK) {
+		ipi = (tid << 16) | (pid << 20) | IRQ_IPI_SMP_KGDB;
+#ifdef IPI_PRINTK_DEBUG
+		printk("Sending KGDB IPI 0x%08x to tid %d pid %d\n", ipi, tid, pid);
+#endif
+	}
+	else if (action & SMP_OPROFILE_IPI) {
+		ipi = (tid << 16) | (pid << 20) | IRQ_IPI_OPROFILE;
+#ifdef IPI_PRINTK_DEBUG
+		printk("Sending OPROFILE IPI 0x%08x to tid %d pid %d\n", ipi, tid,
+			   pid);
+#endif
+	}
+#ifdef CONFIG_PHOENIX_IP_FLOW_AFFINITY
+	else if (action & SMP_NETRX_IPI) {
+		ipi = (tid << 16) | (pid << 20) | IRQ_IPI_NETRX;
+#ifdef IPI_PRINTK_DEBUG
+		printk(KERN_ALERT "%s: Sending NETRX IPI 0x%08x to tid %d pid %d\n",
+			   __func__, ipi, tid, pid);
+#endif
+	}
+#endif
+	else
+		BUG();
+
+	pic_send_ipi(ipi);
+}
+
+static inline void phoenix_send_ipi_mask(cpumask_t mask, unsigned int action)
+{
+	unsigned int i;
+
+	for_each_cpu_mask(i, mask)
+		phoenix_send_ipi_single(i, action);
+}
+
+extern __u64 phnx_irq_mask;
+
+#ifdef CONFIG_PHOENIX_IP_FLOW_AFFINITY
+extern void skb_transfer_finish(void);
+#endif /* CONFIG_PHOENIX_IP_FLOW_AFFINITY */
+
+void phoenix_ipi_handler(int irq, struct pt_regs *regs)
+{
+	phoenix_ipi_stats[smp_processor_id()]++;
+
+	if (irq == IRQ_IPI_SMP_FUNCTION) {
+#ifdef IPI_PRINTK_DEBUG
+		printk("[%s]: cpu_%d processing ipi_%d [->%u]\n", __func__,
+			   smp_processor_id(), irq,
+			   ipi_3_counter_rx[smp_processor_id()]++);
+#endif
+		++ipi_3_counter_rx[smp_processor_id()];
+		smp_call_function_interrupt();
+	}
+#ifdef CONFIG_PHOENIX_IP_FLOW_AFFINITY
+	else if (irq == IRQ_IPI_NETRX) {
+		irq_enter();
+
+		skb_transfer_finish();
+
+		/* run soft IRQ at the end */
+		irq_exit();
+	}
+#endif /* CONFIG_PHOENIX_IP_FLOW_AFFINITY */
+	else {
+#ifdef IPI_PRINTK_DEBUG
+		printk("[%s]: cpu_%d processing ipi_%d\n", __func__,
+			   smp_processor_id(), irq);
+#endif
+		/* Announce that we are for reschduling */
+		set_need_resched();
+
+	}
+	phoenix_ipi_stats[smp_processor_id()]--;
+}
+
+extern void __cpuinit phoenix_clockevent_init(void);
+
+void phoenix_smp_finish(void)
+{
+	phoenix_msgring_cpu_init();
+}
+
+extern void __init phoenix_smp_init(void);
+
+void phoenix_init_secondary(void)
+{
+	phoenix_smp_init();
+}
+
+void phoenix_cpus_done(void)
+{
+}
+
+struct smp_boot_info smp_boot;
+
+extern void asmlinkage smp_bootstrap(void);
+
+/* 
+ * Boot all other cpus in the system, initialize them, and
+ * bring them into the boot fn 
+*/
+void phoenix_boot_secondary(int logical_cpu, struct task_struct *idle)
+{
+	unsigned long gp = (unsigned long)task_thread_info(idle);
+	unsigned long sp = gp + THREAD_SIZE - 32;
+	int cpu = cpu_logical_map(logical_cpu);
+
+	dbg_msg("(PROM): waking up phys cpu# %d, gp = %lx\n", cpu, gp);
+  
+	smp_boot.boot_info[cpu].sp = sp;
+	smp_boot.boot_info[cpu].gp = gp;
+	smp_boot.boot_info[cpu].fn = (unsigned long)&smp_bootstrap;  
+	/* barrier */
+	__sync();
+	smp_boot.boot_info[cpu].ready = 1;
+  
+	dbg_msg("(PROM): sent a wakeup message to cpu %d\n", cpu);
+}
+
+extern int xlr_loader_support;
+extern uint32_t phnx_loader_mask;
+
+unsigned int fast_syscall_cpumask_phy = 0x1;
+
+void __init phoenix_smp_setup(void)
+{
+	int num_cpus = 1;
+	int i=0, j=0;
+
+	__u32 boot_cpu_online_map = 0, boot_cpu = 0x0;
+
+	boot_cpu = hard_smp_processor_id();
+
+	cpus_clear(phys_cpu_present_map);
+	cpus_clear(cpu_possible_map);
+
+	/* Initialize the ipi debug stat variables */
+	for(i=0;i<NR_CPUS;i++) {
+		for(j=0;j<NR_CPUS;j++)
+			ipi_3_counter_tx[i][j] = 0;
+  
+		ipi_3_counter_rx[i] = 0;
+	}
+
+	if(xlr_loader_support) {
+		smp_boot.online_map &= ~phnx_loader_mask;
+	}
+
+	boot_cpu_online_map = smp_boot.online_map;
+	printk("(PROM) CPU present map: %x\n", boot_cpu_online_map);
+
+	/* 
+	 * 0th entry in the logical_map should be the bootcpu and all
+     * others proceeds after that
+	 * Fill the entries for boot cpu 
+	*/
+	boot_cpu_online_map &= (~(1 << boot_cpu));
+	cpu_set(boot_cpu, phys_cpu_present_map);
+	__cpu_number_map[boot_cpu] = 0;
+	__cpu_logical_map[0] = boot_cpu;
+	cpu_set(0, cpu_possible_map);
+
+	for(i = 0;i<NR_CPUS;i++) {
+		if (boot_cpu_online_map & (1<<i)) {
+			cpu_set(i, phys_cpu_present_map);
+			__cpu_number_map[i] = num_cpus;
+			__cpu_logical_map[num_cpus] = i;
+			cpu_set(num_cpus, cpu_possible_map);
+			++num_cpus;
+		}
+	}
+
+	fast_syscall_cpumask_phy = (unsigned int)phys_cpu_present_map.bits[0];
+
+	printk("Phys CPU present map: %lx, possible map %lx\n", 
+	       (unsigned long)phys_cpu_present_map.bits[0], 
+	       (unsigned long)cpu_possible_map.bits[0]);
+
+	printk("Detected %i Slave CPU(s)\n", num_cpus);
+}
+
+void phoenix_prepare_cpus(unsigned int max_cpus)
+{
+}
+
+struct plat_smp_ops phoenix_smp_ops = {
+	.send_ipi_single	= phoenix_send_ipi_single,
+	.send_ipi_mask		= phoenix_send_ipi_mask,
+	.init_secondary		= phoenix_init_secondary,
+	.smp_finish			= phoenix_smp_finish,
+	.cpus_done			= phoenix_cpus_done,
+	.boot_secondary		= phoenix_boot_secondary,
+	.smp_setup			= phoenix_smp_setup,
+	.prepare_cpus		= phoenix_prepare_cpus,
+};
+
+
+static spinlock_t smp_boot_lock;
+extern unsigned long phnx_ebase;
+extern void prom_reconfigure_thr_resources(void);
+extern void ptr_smp_boot(unsigned long, unsigned long, unsigned long);
+
+void prom_boot_cpus_secondary(void *args)
+{
+	int cpu = hard_smp_processor_id();
+	unsigned long flags;
+  
+	write_c0_ebase((uint32_t)phnx_ebase);
+	spin_lock_irqsave(&smp_boot_lock, flags);
+	smp_boot.online_map |= (1<<cpu);
+	spin_unlock_irqrestore(&smp_boot_lock, flags);  
+	for(;;) {
+		if (smp_boot.boot_info[cpu].ready) break;
+	}
+	__sync();
+
+	prom_reconfigure_thr_resources();
+
+	ptr_smp_boot(smp_boot.boot_info[cpu].fn, smp_boot.boot_info[cpu].sp, 
+		     smp_boot.boot_info[cpu].gp);
+}
diff --git a/arch/mips/rmi/phoenix/smpboot.S b/arch/mips/rmi/phoenix/smpboot.S
new file mode 100644
index 0000000..9c01a58
--- /dev/null
+++ b/arch/mips/rmi/phoenix/smpboot.S
@@ -0,0 +1,71 @@
+/*********************************************************************
+ *
+ * Copyright 2003-2006 Raza Microelectronics, Inc. (RMI). All rights
+ * reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ * notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ * notice, this list of conditions and the following disclaimer in
+ * the documentation and/or other materials provided with the
+ * distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Raza Microelectronics, Inc. ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL RMI OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+ * THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ ******************************#RMI_2#**********************************/
+
+#include <asm/asm.h>
+#include <asm/asm-offsets.h>
+#include <asm/regdef.h>
+#include <asm/mipsregs.h>
+
+NESTED(ptr_smp_boot, 16, sp)
+
+	move	sp, a1
+	move	gp, a2
+	jal	a0
+	nop
+	
+END(ptr_smp_boot)
+	
+/* 
+ * Don't jump to linux function from Bootloader stack. Change it 
+ * here. Kernel might allocate bootloader memory before all the CPUs are 
+ * brought up (eg: Inode cache region) and we better don't overwrite this 
+ * memory
+*/
+NESTED(prom_pre_boot_secondary_cpus, 16, sp)
+	.set mips64
+	mfc0	t0, $15, 1 #read ebase
+	andi	t0, 0x1f #t0 has the processor_id()
+	PTR_LA	t1, xlr_stack_pages_temp
+	li	t2, _THREAD_SIZE
+	srl	t2, 2
+	mul	t3, t2, t0
+	nop
+	nop
+	nop
+	nop
+	nop
+	nop
+	PTR_ADDU	gp, t1, t3
+	PTR_ADDU	sp, gp, t2
+	PTR_ADDI	sp, sp, -32
+	PTR_LA	t0, prom_boot_cpus_secondary
+	jr	t0
+	nop
+END(prom_pre_boot_secondary_cpus)
-- 
1.6.0.4

