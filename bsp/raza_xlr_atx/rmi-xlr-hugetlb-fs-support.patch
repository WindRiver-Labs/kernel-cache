From af02c120a5f01fbf9d2ea9e33fdf6afffc804e6f Mon Sep 17 00:00:00 2001
From: jack tan <jack.tan@windriver.com>
Date: Wed, 16 Dec 2009 17:06:46 +0800
Subject: [PATCH] rmi xlr hugetlb fs support

This patch is from sdk 1.7.0. It's for replacing the old BigTLB.

The mainline now has the hugeTLB implementation but it's only support 64bit.
This implementation has also support the 32bit kernel (enable 64bit physical
address support).

This implementaion can configure the hugeTLB page size. This is not supported
by mainline kernel.

This implementation is different from the mainline in TLB refill handler.
Seems like this can not be merged into mainline. We need to implement
the 32-bit support in the future.

Signed-off-by: Jack Tan <jack.tan@windriver.com>
---
 arch/mips/Kconfig                |   26 ++++
 arch/mips/mm/Makefile            |    1 +
 arch/mips/mm/hugetlbpage.c       |  288 ++++++++++++++++++++++++++++++++++++++
 arch/mips/mm/tlb-r4k.c           |   16 ++
 arch/mips/mm/tlbex.c             |   80 +++++++++++
 arch/mips/rmi/Kconfig            |    1 +
 fs/Kconfig                       |    3 +-
 include/asm-mips/hugetlb.h       |   50 +++++++
 include/asm-mips/page.h          |   33 +++++
 include/asm-mips/pgtable-64.h    |    4 +
 include/asm-mips/pgtable-bits.h  |    5 +
 include/asm-mips/pgtable.h       |    8 +
 include/asm-mips/rmi/mips-exts.h |   12 ++
 13 files changed, 526 insertions(+), 1 deletions(-)
 create mode 100644 arch/mips/mm/hugetlbpage.c
 create mode 100644 include/asm-mips/hugetlb.h

diff --git a/arch/mips/Kconfig b/arch/mips/Kconfig
index 187c80d..4e9a8a4 100644
--- a/arch/mips/Kconfig
+++ b/arch/mips/Kconfig
@@ -624,6 +624,32 @@ config RMI_PTR
 
 endchoice
 
+choice
+	prompt "Hugetlb Page Size"
+	depends on HUGETLB_PAGE
+	default HUGETLB_PAGE_SIZE_2MB
+
+config HUGETLB_PAGE_SIZE_32KB
+	bool "32KB"
+	depends on !PAGE_SIZE_16KB && !PAGE_SIZE_64KB
+config HUGETLB_PAGE_SIZE_128KB
+	bool "128KB"
+	depends on !PAGE_SIZE_64KB
+config HUGETLB_PAGE_SIZE_512KB
+	bool "512KB"
+config HUGETLB_PAGE_SIZE_2MB
+	bool "2MB"
+config HUGETLB_PAGE_SIZE_8MB
+	bool "8MB"
+config HUGETLB_PAGE_SIZE_32MB
+	bool "32MB"
+config HUGETLB_PAGE_SIZE_128MB
+	bool "128MB"
+config HUGETLB_PAGE_SIZE_512MB
+	bool "512MB"
+
+endchoice
+
 source "arch/mips/au1000/Kconfig"
 source "arch/mips/basler/excite/Kconfig"
 source "arch/mips/jazz/Kconfig"
diff --git a/arch/mips/mm/Makefile b/arch/mips/mm/Makefile
index e41c424..03ce776 100644
--- a/arch/mips/mm/Makefile
+++ b/arch/mips/mm/Makefile
@@ -29,6 +29,7 @@ obj-$(CONFIG_CPU_VR41XX)	+= c-r4k.o cex-gen.o tlb-r4k.o
 
 obj-$(CONFIG_CPU_PHOENIX)	+= c-phoenix.o tlb-r4k.o \
 							cex-gen.o cerr-phoenix.o 
+obj-$(CONFIG_HUGETLB_PAGE)        += hugetlbpage.o
 
 obj-$(CONFIG_IP22_CPU_SCACHE)	+= sc-ip22.o
 obj-$(CONFIG_R5000_CPU_SCACHE)  += sc-r5k.o
diff --git a/arch/mips/mm/hugetlbpage.c b/arch/mips/mm/hugetlbpage.c
new file mode 100644
index 0000000..e599a0b
--- /dev/null
+++ b/arch/mips/mm/hugetlbpage.c
@@ -0,0 +1,288 @@
+/*
+ * MIPS Huge TLB page support.
+ *
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/fs.h>
+#include <linux/mm.h>
+#include <linux/hugetlb.h>
+#include <linux/pagemap.h>
+#include <linux/smp_lock.h>
+#include <linux/slab.h>
+#include <linux/sysctl.h>
+
+#include <asm/mman.h>
+#include <asm/pgalloc.h>
+#include <asm/tlb.h>
+#include <asm/tlbflush.h>
+#include <asm/cacheflush.h>
+#include <asm/mmu_context.h>
+
+#define Message(a,b...) //printk("\n%s:%d: "a"\n",__FUNCTION__,__LINE__,##b)
+
+
+static pte_t *get_pte_offset(struct mm_struct *mm, unsigned long addr)
+{
+	pgd_t *pgd;
+	pud_t *pud;
+	pmd_t *pmd;
+	pte_t *pte = NULL;
+
+	pgd = pgd_offset(mm, addr);
+	if (!pgd_none(*pgd)) {
+		pud = pud_offset(pgd, addr);
+		if (!pud_none(*pud)) {
+			pmd = pmd_offset(pud, addr);
+			if (!pmd_none(*pmd))
+				pte = pte_offset_map(pmd, addr);
+		}
+	}
+	return pte;
+}
+
+pte_t *huge_pte_offset(struct mm_struct *mm, unsigned long addr)
+{
+	pgd_t *pgd;
+	pud_t *pud;
+	pmd_t *pmd;
+	pte_t *pte = NULL;
+
+	addr &= HPAGE_MASK;
+
+	pgd = pgd_offset(mm, addr);
+	if (!pgd_none(*pgd)) {
+		pud = pud_offset(pgd, addr);
+		if (!pud_none(*pud)) {
+			pmd = pmd_offset(pud, addr);
+			if (!pmd_none(*pmd))
+				pte = pte_offset_map(pmd, addr);
+		}
+	}
+	return pte;
+}
+
+static pte_t *huge_pte_alloc_single(struct mm_struct *mm, unsigned long addr)
+{
+	pgd_t *pgd;
+	pud_t *pud;
+	pmd_t *pmd;
+	pte_t *pte = NULL;
+
+	pgd = pgd_offset(mm, addr);
+	pud = pud_alloc(mm, pgd, addr);
+	if (pud) {
+		pmd = pmd_alloc(mm, pud, addr);
+		if (pmd)
+			pte = pte_alloc_map(mm, pmd, addr);
+	}
+	return pte;
+}
+
+pte_t *huge_pte_alloc(struct mm_struct *mm, unsigned long addr, unsigned long sz)
+{
+	pte_t *first_pte = NULL;
+	pte_t *pte = NULL;
+	int i = 0;
+	uint32_t htlb_entries = 1 << HUGETLB_PAGE_ORDER;
+	uint32_t total_pte_required = htlb_entries / PTRS_PER_PTE;
+
+	/*
+	 *increment number of pte required if htlb_entries
+	 *is not multiple of PTRS_PER_PTE
+	*/
+	if(htlb_entries % PTRS_PER_PTE)
+		total_pte_required++;
+
+	Message(" allocating pte for before mask - addr %#lx",addr);
+	addr &= HPAGE_MASK;
+	Message(" allocating pte for after mask - addr %#lx",addr);
+	Message(" total pte required %d",total_pte_required);
+	for(i=0; i<total_pte_required; i++){
+		pte = huge_pte_alloc_single(mm, addr);
+		if(!pte)
+			return NULL;
+		if(!first_pte)
+			first_pte = pte;
+		addr = addr + (PTRS_PER_PTE*PAGE_SIZE);
+		Message("New Addr %#lx",addr);
+	}
+	Message("Returning first_pte %#lx",(unsigned long)first_pte);
+	return first_pte;
+}
+
+void set_huge_pte_at(struct mm_struct *mm, unsigned long addr, pte_t *ptep,
+		pte_t entry)
+{
+	int i;
+	uint32_t htlb_entries = 1 << HUGETLB_PAGE_ORDER;
+	struct vm_area_struct *vma = NULL;
+	uint32_t len = htlb_entries*PAGE_SIZE;
+	pte_t first_entry = entry;
+	unsigned long orig_addr ;
+
+	/*
+	 * We must align the address, because our caller will run
+	 * set_huge_pte_at() on whatever we return, which writes out
+	 * all of the sub-ptes for the hugepage range.  So we have
+	 * to give it the first such sub-pte.
+	 */
+	addr &= HPAGE_MASK;
+	orig_addr = addr;
+	Message("\nSetting hugepte for address %#lx\n",addr);	
+	Message("htlb entries %d",htlb_entries);
+	/* Fill each entry with its own physical address map */	
+	for (i = 0; i < htlb_entries; i++) {
+		/* Get the pte offset, we may cross the pte table */
+		ptep = get_pte_offset(mm,addr);
+
+		if(!(i % 1024))
+		Message("Entry [%d] Virt Addr [%#lx] Phys Addr[%#llx]\n",
+			i,(unsigned long)addr,
+			(unsigned long long)((pte_val(entry))));
+
+		set_pte_at(mm, addr, ptep, entry);
+		addr += PAGE_SIZE;
+		pte_val(entry) += PAGE_SIZE;
+	}
+
+	addr = orig_addr;
+	vma = find_vma(mm, addr);
+
+	/* addr must belong to this vma */
+	if(!((addr >= vma->vm_start) && ((addr+len) <= vma->vm_end)))
+		panic("set_huge_pte_at: No vma found for hugtlb page!!");
+
+	/* Don't know below loop is required or not */
+	pte_val(entry) = pte_val(first_entry);
+	for (i = 0 ; i < htlb_entries; i++){
+		__update_cache(vma, addr, entry);
+		addr += PAGE_SIZE;
+		pte_val(entry) += PAGE_SIZE;
+	}
+	Message("Returning...");
+}
+
+pte_t huge_ptep_get_and_clear(struct mm_struct *mm, unsigned long addr,
+			      pte_t *ptep)
+{
+	pte_t entry;
+	int i;
+
+	entry = *ptep;
+	pte_val(entry) = pte_val(entry) & ~(_PAGE_HUGE);
+
+	addr &= HPAGE_MASK;
+	Message("Called");
+	for (i = 0; i < (1 << HUGETLB_PAGE_ORDER); i++) {
+		ptep = get_pte_offset(mm,addr);
+		if(!(i %1024))
+			Message("\nClearing ptep %d\n",i);
+		pte_clear(mm, addr, ptep);
+		addr += PAGE_SIZE;
+	}
+	Message("Returning");
+	return entry;
+}
+
+struct page *follow_huge_addr(struct mm_struct *mm,
+			      unsigned long address, int write)
+{
+	return ERR_PTR(-EINVAL);
+}
+
+int pmd_huge(pmd_t pmd)
+{
+	return 0;
+}
+
+struct page *follow_huge_pmd(struct mm_struct *mm, unsigned long address,
+			     pmd_t *pmd, int write)
+{
+	return NULL;
+}
+
+int huge_pmd_unshare(struct mm_struct *mm, unsigned long *addr, pte_t *ptep)
+{
+	return 0;
+}
+
+/*
+ * If the arch doesn't supply something else, assume that hugepage
+ * size aligned regions are ok without further preparation.
+ */
+static inline int prepare_hugepage_range(struct file *file,
+			unsigned long addr, unsigned long len)
+{
+	struct hstate *h = hstate_file(file);
+	if (len & ~huge_page_mask(h))
+		return -EINVAL;
+	if (addr & ~huge_page_mask(h))
+		return -EINVAL;
+	return 0;
+}
+
+unsigned long
+hugetlb_get_unmapped_area(struct file *file, unsigned long addr,
+		unsigned long len, unsigned long pgoff, unsigned long flags)
+{
+	struct mm_struct *mm = current->mm;
+	struct vm_area_struct *vma;
+	unsigned long start_addr;
+	unsigned long task_size = TASK_SIZE;
+#ifdef CONFIG_64BIT
+	if (test_thread_flag(TIF_32BIT)) {
+		task_size = TASK_SIZE32;
+	}
+#endif
+
+	if (len & ~HPAGE_MASK)
+		return -EINVAL;
+	
+	if (len > task_size)
+		return -ENOMEM;
+
+	if (flags & MAP_FIXED) {
+		if (prepare_hugepage_range(file, addr, len))
+			return -EINVAL;
+		return addr;
+	}
+
+	if (addr) {
+		addr = ALIGN(addr, HPAGE_SIZE);
+		vma = find_vma(mm, addr);
+		if (task_size - len >= addr &&
+		    (!vma || addr + len <= vma->vm_start)){
+			return addr;
+		}
+	}
+
+	start_addr = mm->free_area_cache;
+
+	if (len <= mm->cached_hole_size)
+		start_addr = TASK_UNMAPPED_BASE;
+
+full_search:
+	addr = ALIGN(start_addr, HPAGE_SIZE);
+
+	for (vma = find_vma(mm, addr); ; vma = vma->vm_next) {
+		/* At this point:  (!vma || addr < vma->vm_end). */
+		if (task_size - len < addr) {
+			/*
+			 * Start a new search - just in case we missed
+			 * some holes.
+			 */
+			if (start_addr != TASK_UNMAPPED_BASE) {
+				start_addr = TASK_UNMAPPED_BASE;
+				goto full_search;
+			}
+			return -ENOMEM;
+		}
+
+		if (!vma || addr + len <= vma->vm_start){
+			return addr;
+		}
+		addr = ALIGN(vma->vm_end, HPAGE_SIZE);
+	}
+}
diff --git a/arch/mips/mm/tlb-r4k.c b/arch/mips/mm/tlb-r4k.c
index 5ce2fa7..2c26b29 100644
--- a/arch/mips/mm/tlb-r4k.c
+++ b/arch/mips/mm/tlb-r4k.c
@@ -17,6 +17,9 @@
 #include <asm/mmu_context.h>
 #include <asm/pgtable.h>
 #include <asm/system.h>
+#ifdef CONFIG_RMI_PHOENIX
+#include <asm/rmi/mips-exts.h>
+#endif
 
 extern void build_tlb_refill_handler(void);
 
@@ -64,6 +67,15 @@ extern void build_tlb_refill_handler(void);
 
 #endif
 
+#if defined(CONFIG_HUGETLBFS) && defined(CONFIG_RMI_PHOENIX)
+void rmi_tlb_entrylo0_mask_init(void);
+void rmi_tlb_entrylo0_mask_init()
+{
+	unsigned long long mask = ~(((1ULL<<HUGETLB_PAGE_ORDER)-1)<<6);
+	rmi_write_os_scratch_3(mask);
+}
+#endif
+
 void local_flush_tlb_all(void)
 {
 	unsigned long flags;
@@ -496,5 +508,9 @@ void __cpuinit tlb_init(void)
 			printk("Ignoring invalid argument ntlb=%d\n", ntlb);
 	}
 
+#if defined(CONFIG_HUGETLBFS) && defined(CONFIG_RMI_PHOENIX)
+	rmi_tlb_entrylo0_mask_init();
+#endif
+
 	build_tlb_refill_handler();
 }
diff --git a/arch/mips/mm/tlbex.c b/arch/mips/mm/tlbex.c
index 54a6648..0a3a5cf 100644
--- a/arch/mips/mm/tlbex.c
+++ b/arch/mips/mm/tlbex.c
@@ -26,6 +26,9 @@
 
 #include <asm/mmu_context.h>
 #include <asm/war.h>
+#ifdef CONFIG_RMI_PHOENIX
+#include <asm/rmi/mips-exts.h>
+#endif
 
 #include "uasm.h"
 
@@ -82,6 +85,10 @@ enum label_id {
 	label_nopage_tlbm,
 	label_smp_pgtable_change,
 	label_r3000_write_probe_fail,
+#ifdef CONFIG_HUGETLBFS
+	label_nohpage_tlbs,
+	label_hpage_tlb_leave,
+#endif
 };
 
 UASM_L_LA(_second_part)
@@ -98,6 +105,10 @@ UASM_L_LA(_nopage_tlbs)
 UASM_L_LA(_nopage_tlbm)
 UASM_L_LA(_smp_pgtable_change)
 UASM_L_LA(_r3000_write_probe_fail)
+#ifdef CONFIG_HUGETLBFS
+UASM_L_LA(_nohpage_tlbs)
+UASM_L_LA(_hpage_tlb_leave)
+#endif
 
 /*
  * For debug purposes.
@@ -118,6 +129,7 @@ static inline void dump_handler(const u32 *handler, int count)
 /* The only general purpose registers allowed in TLB handlers. */
 #define K0		26
 #define K1		27
+#define ZERO    0
 
 /* Some CP0 registers */
 #define C0_INDEX	0, 0
@@ -125,6 +137,7 @@ static inline void dump_handler(const u32 *handler, int count)
 #define C0_TCBIND	2, 2
 #define C0_ENTRYLO1	3, 0
 #define C0_CONTEXT	4, 0
+#define C0_PAGEMASK 5, 0
 #define C0_BADVADDR	8, 0
 #define C0_ENTRYHI	10, 0
 #define C0_EPC		14, 0
@@ -629,8 +642,14 @@ static void __cpuinit build_get_ptep(u32 **p, unsigned int tmp, unsigned int ptr
 	UASM_i_ADDU(p, ptr, ptr, tmp); /* add in offset */
 }
 
+#ifdef CONFIG_HUGETLBFS
+static void __cpuinit build_update_entries(u32 **p, struct uasm_label **l,
+                struct uasm_reloc **r, unsigned int tmp,
+                unsigned int ptep)
+#else
 static void __cpuinit build_update_entries(u32 **p, unsigned int tmp,
 					unsigned int ptep)
+#endif
 {
 	/*
 	 * 64bit address support (36bit on a 32bit CPU) in a 32bit
@@ -639,11 +658,44 @@ static void __cpuinit build_update_entries(u32 **p, unsigned int tmp,
 #ifdef CONFIG_64BIT_PHYS_ADDR
 	if (cpu_has_64bits) {
 		uasm_i_ld(p, tmp, 0, ptep); /* get even pte */
+#ifdef CONFIG_HUGETLBFS
+		/*continue if htlb found otherwise jump to label_nohpage_tlbs*/
+		uasm_i_dsrl32(p, tmp, tmp, 31);	
+		uasm_il_beqz(p, r, tmp, label_nohpage_tlbs);
+		uasm_i_ld(p, tmp, 0, ptep);	/*load tmp with even entry again*/
+		uasm_i_dsrl(p, tmp, tmp, 6);	/*shift by 6 to remove s/w bits*/
+		/*get the entrylo0 mask*/
+		uasm_i_dmfc0(p, ptep, OS_SCRATCH_REG3);
+		/*mask lower paddr bits*/
+		uasm_i_and(p, tmp, tmp, ptep);
+		uasm_i_dmtc0(p, tmp, C0_ENTRYLO0); /* load it */
+
+		/*Add (HUGE_PAGE_SIZE/2)>>6 to entrylo0 to compute entrlylo1*/
+		uasm_i_daddiu(p, ptep, ZERO, 1);
+		uasm_i_sll(p, ptep, ptep, HPAGE_SHIFT-7);
+		uasm_i_daddu(p, tmp, tmp, ptep); /*compute entrylo1*/
+
+		uasm_i_dmtc0(p, tmp, C0_ENTRYLO1);
+		uasm_i_daddiu(p, ptep, ZERO, HUGETLB_PAGE_MASK);
+		uasm_i_sll(p, ptep, ptep, 13);
+#ifndef CONFIG_PAGE_SIZE_4KB
+		uasm_i_mfc0(p, tmp, C0_PAGEMASK);
+#endif
+		uasm_il_b(p, r, label_hpage_tlb_leave);
+		uasm_i_mtc0(p, ptep, C0_PAGEMASK);	/*delay slot*/
+		uasm_l_nohpage_tlbs(l, *p);
+#endif
 		uasm_i_ld(p, ptep, sizeof(pte_t), ptep); /* get odd pte */
 		uasm_i_dsrl(p, tmp, tmp, 6); /* convert to entrylo0 */
 		uasm_i_mtc0(p, tmp, C0_ENTRYLO0); /* load it */
 		uasm_i_dsrl(p, ptep, ptep, 6); /* convert to entrylo1 */
 		uasm_i_mtc0(p, ptep, C0_ENTRYLO1); /* load it */
+#ifdef CONFIG_HUGETLBFS
+#ifndef CONFIG_PAGE_SIZE_4KB
+		uasm_i_mfc0(p, tmp, C0_PAGEMASK);
+#endif
+		uasm_l_hpage_tlb_leave(l, *p);
+#endif
 	} else {
 		int pte_off_even = sizeof(pte_t) / 2;
 		int pte_off_odd = pte_off_even + sizeof(pte_t);
@@ -691,6 +743,12 @@ static void __cpuinit build_r4000_tlb_refill_handler(void)
 	if (bcm1250_m3_war())
 		build_bcm1250_m3_war(&p, &r);
 
+#ifdef CONFIG_RMI_PHOENIX
+    uasm_i_dmfc0(&p, K0, OS_SCRATCH_REG2);
+    uasm_i_daddiu(&p, K0, K0, 1);
+    uasm_i_dmtc0(&p, K0, OS_SCRATCH_REG2);
+#endif
+
 #ifdef CONFIG_64BIT
 	build_get_pmde64(&p, &l, &r, K0, K1); /* get pmd in K1 */
 #else
@@ -698,8 +756,19 @@ static void __cpuinit build_r4000_tlb_refill_handler(void)
 #endif
 
 	build_get_ptep(&p, K0, K1);
+#ifdef CONFIG_HUGETLBFS
+	build_update_entries(&p, &l, &r, K0, K1);
+#else
 	build_update_entries(&p, K0, K1);
+#endif
 	build_tlb_write_entry(&p, &l, &r, tlb_random);
+#ifdef CONFIG_HUGETLBFS
+#ifdef CONFIG_PAGE_SIZE_4KB
+	uasm_i_mtc0(&p, ZERO, C0_PAGEMASK);
+#else
+	uasm_i_mtc0(&p, K0, C0_PAGEMASK);
+#endif
+#endif
 	uasm_l_leave(&l, p);
 	uasm_i_eret(&p); /* return from trap */
 
@@ -1125,8 +1194,19 @@ build_r4000_tlbchange_handler_tail(u32 **p, struct uasm_label **l,
 {
 	uasm_i_ori(p, ptr, ptr, sizeof(pte_t));
 	uasm_i_xori(p, ptr, ptr, sizeof(pte_t));
+#ifdef CONFIG_HUGETLBFS
+	build_update_entries(p, l, r, tmp, ptr);
+#else
 	build_update_entries(p, tmp, ptr);
+#endif
 	build_tlb_write_entry(p, l, r, tlb_indexed);
+#ifdef CONFIG_HUGETLBFS
+#ifdef CONFIG_PAGE_SIZE_4KB
+	uasm_i_mtc0(p, ZERO, C0_PAGEMASK);
+#else
+	uasm_i_mtc0(p, K0, C0_PAGEMASK);
+#endif
+#endif
 	uasm_l_leave(l, *p);
 	uasm_i_eret(p); /* return from trap */
 
diff --git a/arch/mips/rmi/Kconfig b/arch/mips/rmi/Kconfig
index 1ee3e2d..d5b6dbf 100644
--- a/arch/mips/rmi/Kconfig
+++ b/arch/mips/rmi/Kconfig
@@ -1,5 +1,6 @@
 config RMI_PHOENIX
 	bool 
+	select 64BIT_PHYS_ADDR
 
 config PHOENIX_PCIX_GEN_DRIVER
 	bool
diff --git a/fs/Kconfig b/fs/Kconfig
index 5af291a..5a05e58 100644
--- a/fs/Kconfig
+++ b/fs/Kconfig
@@ -956,7 +956,8 @@ config TMPFS_POSIX_ACL
 config HUGETLBFS
 	bool "HugeTLB file system support"
 	depends on X86 || IA64 || PPC64 || SPARC64 || (SUPERH && MMU) || \
-		   (S390 && 64BIT) || BROKEN
+		   (S390 && 64BIT) || BROKEN || \
+		   (RMI_PHOENIX && (64BIT_PHYS_ADDR || 64BIT))
 	help
 	  hugetlbfs is a filesystem backing for HugeTLB pages, based on
 	  ramfs. For architectures that support it, say Y here and read
diff --git a/include/asm-mips/hugetlb.h b/include/asm-mips/hugetlb.h
new file mode 100644
index 0000000..f6ff12f
--- /dev/null
+++ b/include/asm-mips/hugetlb.h
@@ -0,0 +1,50 @@
+#ifndef _ASM_MIPS_HUGETLB_H
+#define _ASM_MIPS_HUGETLB_H
+
+#include <asm/page.h>
+#include <asm/pgtable.h>
+
+#define is_hugepage_only_range(mm, addr, len)	0
+#define hugetlb_free_pgd_range			free_pgd_range
+#define arch_release_hugepage(page)		0
+#define arch_prepare_hugepage(page)		0
+#define huge_pte_none				pte_none
+#define pud_huge(x)				0
+#define hugetlb_prefault_arch_hook(mm)		do { } while (0)
+
+extern void set_huge_pte_at(struct mm_struct *mm, unsigned long addr,
+				pte_t *ptep, pte_t entry);
+extern pte_t huge_ptep_get_and_clear(struct mm_struct *mm, unsigned long addr,
+			      pte_t *ptep);
+static inline pte_t huge_pte_wrprotect(pte_t pte)
+{
+	return pte_wrprotect(pte);
+}
+
+		
+static inline void huge_ptep_clear_flush(struct vm_area_struct *vma,
+					 unsigned long addr, pte_t *ptep)
+{
+}
+
+
+static inline pte_t huge_ptep_get(pte_t *ptep)
+{
+	return *ptep;
+}
+
+static inline void huge_ptep_set_wrprotect(struct mm_struct *mm,
+					   unsigned long addr, pte_t *ptep)
+{
+	set_huge_pte_at(mm, addr, ptep, pte_wrprotect(huge_ptep_get(ptep)));
+}
+
+static inline int huge_ptep_set_access_flags(struct vm_area_struct *vma,
+		unsigned long addr, pte_t *ptep, pte_t entry, int dirty)
+{
+	set_huge_pte_at(vma->vm_mm, addr, ptep, entry);	
+	flush_tlb_page(vma, addr);
+	return 1;
+}
+
+#endif
diff --git a/include/asm-mips/page.h b/include/asm-mips/page.h
index abd5ef5..f3e352f 100644
--- a/include/asm-mips/page.h
+++ b/include/asm-mips/page.h
@@ -29,6 +29,39 @@
 #define PAGE_SIZE	(1UL << PAGE_SHIFT)
 #define PAGE_MASK       (~((1 << PAGE_SHIFT) - 1))
 
+
+#ifdef CONFIG_HUGETLB_PAGE
+#define HAVE_ARCH_HUGETLB_UNMAPPED_AREA
+#if defined(CONFIG_HUGETLB_PAGE_SIZE_32KB)
+#define HPAGE_SHIFT		15
+#elif defined(CONFIG_HUGETLB_PAGE_SIZE_128KB)
+#define HPAGE_SHIFT		17
+#elif defined(CONFIG_HUGETLB_PAGE_SIZE_512KB)
+#define HPAGE_SHIFT		19
+#elif defined(CONFIG_HUGETLB_PAGE_SIZE_2MB)
+#define HPAGE_SHIFT		21
+#elif defined(CONFIG_HUGETLB_PAGE_SIZE_8MB)
+#define CONFIG_FORCE_MAX_ZONEORDER 12
+#define HPAGE_SHIFT		23
+#elif defined(CONFIG_HUGETLB_PAGE_SIZE_32MB)
+#define CONFIG_FORCE_MAX_ZONEORDER 14
+#define HPAGE_SHIFT		25
+#elif defined(CONFIG_HUGETLB_PAGE_SIZE_128MB)
+#define CONFIG_FORCE_MAX_ZONEORDER 16
+#define HPAGE_SHIFT		27
+#elif defined(CONFIG_HUGETLB_PAGE_SIZE_512MB)
+#define CONFIG_FORCE_MAX_ZONEORDER 18
+#define HPAGE_SHIFT		29
+#endif
+
+#define HPAGE_SIZE		((1UL) << HPAGE_SHIFT)
+#define HPAGE_MASK		(~(HPAGE_SIZE - 1UL))
+#define HUGETLB_PAGE_ORDER	(HPAGE_SHIFT - PAGE_SHIFT)
+
+#define HUGETLB_PAGE_MASK	(((HPAGE_SIZE/8192)-1))
+#define ARCH_HAS_SETCLEAR_HUGE_PTE 1
+#endif
+
 #ifndef __ASSEMBLY__
 
 #include <linux/pfn.h>
diff --git a/include/asm-mips/pgtable-64.h b/include/asm-mips/pgtable-64.h
index 943515f..f1d8d4d 100644
--- a/include/asm-mips/pgtable-64.h
+++ b/include/asm-mips/pgtable-64.h
@@ -173,7 +173,11 @@ static inline void pud_clear(pud_t *pudp)
 #define pte_pfn(x)		((unsigned long)((x).pte >> (PAGE_SHIFT + 2)))
 #define pfn_pte(pfn, prot)	__pte(((pfn) << (PAGE_SHIFT + 2)) | pgprot_val(prot))
 #else
+#ifdef CONFIG_HUGETLBFS
+#define pte_pfn(x)      ((unsigned long)((((x).pte & ~(_PAGE_HUGE)) >> PAGE_SHIFT)))
+#else
 #define pte_pfn(x)		((unsigned long)((x).pte >> PAGE_SHIFT))
+#endif
 #define pfn_pte(pfn, prot)	__pte(((pfn) << PAGE_SHIFT) | pgprot_val(prot))
 #endif
 
diff --git a/include/asm-mips/pgtable-bits.h b/include/asm-mips/pgtable-bits.h
index 1efa42b..894441a 100644
--- a/include/asm-mips/pgtable-bits.h
+++ b/include/asm-mips/pgtable-bits.h
@@ -59,6 +59,11 @@
 #define _PAGE_MODIFIED              (1<<4)  /* implemented in software */
 #define _PAGE_FILE                  (1<<4)  /* set:pagecache unset:swap */
 
+#ifdef CONFIG_HUGETLB_PAGE
+#define HUGE_BIT           63
+#define _PAGE_HUGE         (1ULL<<HUGE_BIT) /* mark pte as a hugepage */
+#endif
+
 #if defined(CONFIG_CPU_R3000) || defined(CONFIG_CPU_TX39XX)
 
 #define _PAGE_GLOBAL                (1<<8)
diff --git a/include/asm-mips/pgtable.h b/include/asm-mips/pgtable.h
index 6a0edf7..3d24350 100644
--- a/include/asm-mips/pgtable.h
+++ b/include/asm-mips/pgtable.h
@@ -292,6 +292,14 @@ static inline pte_t pte_mkyoung(pte_t pte)
 		pte_val(pte) |= _PAGE_SILENT_READ;
 	return pte;
 }
+
+#ifdef CONFIG_HUGETLBFS
+static inline pte_t pte_mkhuge(pte_t pte)
+{
+	pte_val(pte) |= _PAGE_HUGE;
+	return pte;
+}
+#endif
 #endif
 static inline int pte_special(pte_t pte)	{ return 0; }
 static inline pte_t pte_mkspecial(pte_t pte)	{ return pte; }
diff --git a/include/asm-mips/rmi/mips-exts.h b/include/asm-mips/rmi/mips-exts.h
index 4d437c1..95af089 100644
--- a/include/asm-mips/rmi/mips-exts.h
+++ b/include/asm-mips/rmi/mips-exts.h
@@ -40,6 +40,15 @@
 #define PHOENIX_OSS_SEL_K0 6
 #define PHOENIX_OSS_SEL_K1 7
 
+#define OS_SCRATCH_REG0 22, 0
+#define OS_SCRATCH_REG1 22, 1
+#define OS_SCRATCH_REG2 22, 2
+#define OS_SCRATCH_REG3 22, 3
+#define OS_SCRATCH_REG4 22, 4
+#define OS_SCRATCH_REG5 22, 5
+#define OS_SCRATCH_REG6 22, 6
+#define OS_SCRATCH_REG7 22, 7
+
 #ifndef __ASSEMBLY__
 
 #include <linux/types.h>
@@ -316,6 +325,9 @@ static __inline__ int phnx_test_and_set(phnx_atomic_t *lock)
 	return (oldval == 0 ? 1 : 0);
 }
 
+#define rmi_write_os_scratch_3(val)	__write_64bit_c0_register($22, 3, val)
+#define rmi_read_os_scratch_3()		__read_64bit_c0_register($22, 3)
+
 #endif
 
 #endif /* _ASM_RMI_MIPS_EXTS_H */
-- 
1.6.0.4

