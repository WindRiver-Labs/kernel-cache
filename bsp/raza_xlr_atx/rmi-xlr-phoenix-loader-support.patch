From d2d2191cc40efa8207a6f2897e6ede26e8e5f04c Mon Sep 17 00:00:00 2001
From: Jack Tan <jack.tan@windriver.com>
Date: Tue, 16 Dec 2008 19:07:15 +0800
Subject: [PATCH] rmi xlr phoenix loader support

A framework for loading of RMIOS applications form linux userspace. It's
detached from SDK 1.5

This framework allows to stop and reload applications on various threads.
It also supports shared memory between RMIOS applications and Linux userland
applications.

With this support, all RMIOS applications will use UART1 port (default) as
their console and Linux will use the UART0 port.

Signed-off-by: Jack Tan <jack.tan@windriver.com>
---
 arch/mips/rmi/ptr/loader/Makefile               |    5 +
 arch/mips/rmi/ptr/loader/console.c              |   73 +++
 arch/mips/rmi/ptr/loader/entry.S                |  137 ++++++
 arch/mips/rmi/ptr/loader/fifo.h                 |   95 ++++
 arch/mips/rmi/ptr/loader/loader.c               |  578 +++++++++++++++++++++++
 arch/mips/rmi/ptr/loader/reload_irq_handler.S   |   80 ++++
 arch/mips/rmi/ptr/loader/traps.c                |  219 +++++++++
 arch/mips/rmi/ptr/loader/uart.c                 |  332 +++++++++++++
 arch/mips/rmi/ptr/loader/uart.h                 |   72 +++
 arch/mips/rmi/ptr/loader/xlr_boot_lib.h         |  392 +++++++++++++++
 arch/mips/rmi/ptr/loader/xlr_lib_launch.c       |  523 ++++++++++++++++++++
 arch/mips/rmi/ptr/loader/xlr_lib_platform.h     |   92 ++++
 arch/mips/rmi/ptr/loader/xlr_rmios_stackframe.h |  191 ++++++++
 13 files changed, 2789 insertions(+), 0 deletions(-)
 create mode 100644 arch/mips/rmi/ptr/loader/Makefile
 create mode 100644 arch/mips/rmi/ptr/loader/console.c
 create mode 100644 arch/mips/rmi/ptr/loader/entry.S
 create mode 100644 arch/mips/rmi/ptr/loader/fifo.h
 create mode 100644 arch/mips/rmi/ptr/loader/loader.c
 create mode 100644 arch/mips/rmi/ptr/loader/reload_irq_handler.S
 create mode 100644 arch/mips/rmi/ptr/loader/traps.c
 create mode 100644 arch/mips/rmi/ptr/loader/uart.c
 create mode 100644 arch/mips/rmi/ptr/loader/uart.h
 create mode 100644 arch/mips/rmi/ptr/loader/xlr_boot_lib.h
 create mode 100644 arch/mips/rmi/ptr/loader/xlr_lib_launch.c
 create mode 100644 arch/mips/rmi/ptr/loader/xlr_lib_platform.h
 create mode 100644 arch/mips/rmi/ptr/loader/xlr_rmios_stackframe.h

diff --git a/arch/mips/rmi/ptr/loader/Makefile b/arch/mips/rmi/ptr/loader/Makefile
new file mode 100644
index 0000000..e8fb9c4
--- /dev/null
+++ b/arch/mips/rmi/ptr/loader/Makefile
@@ -0,0 +1,5 @@
+
+obj-y  = loader.o uart.o console.o entry.o traps.o xlr_lib_launch.o reload_irq_handler.o 
+
+EXTRA_AFLAGS := $(CFLAGS)
+
diff --git a/arch/mips/rmi/ptr/loader/console.c b/arch/mips/rmi/ptr/loader/console.c
new file mode 100644
index 0000000..a8a76b2
--- /dev/null
+++ b/arch/mips/rmi/ptr/loader/console.c
@@ -0,0 +1,73 @@
+/*********************************************************************
+
+  Copyright 2003-2006 Raza Microelectronics, Inc. (RMI). All rights
+  reserved.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+  1. Redistributions of source code must retain the above copyright
+  notice, this list of conditions and the following disclaimer.
+  2. Redistributions in binary form must reproduce the above copyright
+  notice, this list of conditions and the following disclaimer in
+  the documentation and/or other materials provided with the
+  distribution.
+
+  THIS SOFTWARE IS PROVIDED BY Raza Microelectronics, Inc. ``AS IS'' AND
+  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL RMI OR CONTRIBUTORS BE LIABLE
+  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+  THE POSSIBILITY OF SUCH DAMAGE.
+
+  *****************************#RMI_2#**********************************/
+
+/*
+ *  Derived in part from linux/lib/vsprintf.c
+ *
+ *  Copyright (C) 1991, 1992  Linus Torvalds
+ */
+
+/* vsprintf.c -- Lars Wirzenius & Linus Torvalds. */
+
+#include <linux/console.h>
+
+#define CFG_PBSIZE 128
+
+extern void serial_puts(const char *s);
+extern int outbyte(char c);
+
+int puts(const char *s)
+{
+	serial_puts(s);
+	return 0;
+}
+
+int putchar(int c)
+{
+	outbyte(c);
+	return 0;
+}
+
+/* we use this so that we can do without the ctype library */
+#define is_digit(c)	((c) >= '0' && (c) <= '9')
+
+#define ZEROPAD	1				/* pad with zero */
+#define SIGN	2				/* unsigned/signed long */
+#define PLUS	4				/* show plus */
+#define SPACE	8				/* space if plus */
+#define LEFT	16				/* left justified */
+#define SPECIAL	32				/* 0x */
+#define LARGE	64				/* use 'ABCDEF' instead of 'abcdef' */
+
+#define do_div(n,base) ({ \
+	int __res; \
+	__res = ((unsigned long) n) % (unsigned) base; \
+	n = ((unsigned long) n) / (unsigned) base; \
+	__res; })
diff --git a/arch/mips/rmi/ptr/loader/entry.S b/arch/mips/rmi/ptr/loader/entry.S
new file mode 100644
index 0000000..c854b4a
--- /dev/null
+++ b/arch/mips/rmi/ptr/loader/entry.S
@@ -0,0 +1,137 @@
+/*********************************************************************
+ *
+ * Copyright 2003-2006 Raza Microelectronics, Inc. (RMI). All rights
+ * reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ * notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ * notice, this list of conditions and the following disclaimer in
+ * the documentation and/or other materials provided with the
+ * distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Raza Microelectronics, Inc. ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL RMI OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+ * THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ *****************************#RMI_2#**********************************/
+
+#include <asm/mipsregs.h>
+#include <asm/regdef.h>
+#include <asm/asm.h> 
+#include "xlr_rmios_stackframe.h"
+#include <asm/stackframe.h>
+#include <asm/asm-offsets.h>
+
+
+#define KU_USER 0x10
+                                    
+    .text
+    .align 4
+    .set push
+    .set reorder
+
+FEXPORT(r_ret_from_exception)
+FEXPORT(r_ret_from_irq)
+    .set    noat
+	/* STI */
+    restore_stack_frame
+    eret
+ /*   RESTORE_ALL_AND_RET*/
+    .set    at
+	.set	mips64
+NESTED(xlr_tlb_refill_secondary, K_STACK_SIZE, sp)
+	save_stack_frame
+	move a0, sp
+	jal xlr_tlb_panic
+	nop
+END(xlr_tlb_refill_secondary)
+
+NESTED(reload_except_vec_tlbrefill, 0, sp)
+	jal xlr_tlb_refill_secondary 
+	nop    
+END(reload_except_vec_tlbrefill)
+
+NESTED(xlr_xtlb_refill_secondary, K_STACK_SIZE, sp)
+	save_stack_frame
+	move a0, sp
+	jal xlr_xtlb_panic
+	nop
+END(xlr_xtlb_refill_secondary)
+
+
+NESTED(reload_except_vec_xtlbrefill, 0, sp)
+    jal xlr_xtlb_refill_secondary
+    nop
+END(reload_except_vec_xtlbrefill)
+
+NESTED(reload_except_vec_cacheerr, 0, sp)
+    PANIC("Unhandled Cache Err Exception, Reloading CPU\n")
+    nop
+END(reload_except_vec_cacheerr)
+
+NESTED(xlr_vecinit_secondary, K_STACK_SIZE, sp)
+	save_stack_frame
+	move a0, sp
+	jal xlr_vecint_panic
+	nop
+END(xlr_vecinit_secondary)
+
+NESTED(reload_except_vec_vecint, 0, sp)
+    jal xlr_vecinit_secondary
+    nop
+END(reload_except_vec_vecint)
+
+NESTED(reload_except_vec_genex, 0, sp)
+    mfc0    k1, CP0_CAUSE
+    PTR_LA      k0, r_exception_handlers 
+    andi    k1, k1, 0x7c
+    addu    k0, k0, k1
+    lw      k0, (k0)
+    jr      k0
+    nop
+END(reload_except_vec_genex)
+
+    .set pop
+
+	.text
+	.set    push
+	.set    noreorder
+	.set    mips4
+	.align    5
+NESTED(reload_handle_reserved, K_STACK_SIZE, sp)    
+    .set    noat
+    save_stack_frame 
+    .set    at
+    KMODE
+    dmfc0   t0, CP0_STATUS
+    ori t0, 2
+    xori t0, 2
+    dmtc0   t0, CP0_STATUS
+    jal     do_reload_setup
+    move 	a0, sp
+    j   	r_ret_from_exception
+    nop
+END(reload_handle_reserved)    
+
+    .set pop
+
+
+	.text
+	.set    push
+	.set    noreorder
+	.set    mips4
+	.align    5
+    .set pop
diff --git a/arch/mips/rmi/ptr/loader/fifo.h b/arch/mips/rmi/ptr/loader/fifo.h
new file mode 100644
index 0000000..034832b
--- /dev/null
+++ b/arch/mips/rmi/ptr/loader/fifo.h
@@ -0,0 +1,95 @@
+/*********************************************************************
+
+  Copyright 2003-2006 Raza Microelectronics, Inc. (RMI). All rights
+  reserved.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+  1. Redistributions of source code must retain the above copyright
+  notice, this list of conditions and the following disclaimer.
+  2. Redistributions in binary form must reproduce the above copyright
+  notice, this list of conditions and the following disclaimer in
+  the documentation and/or other materials provided with the
+  distribution.
+
+  THIS SOFTWARE IS PROVIDED BY Raza Microelectronics, Inc. ``AS IS'' AND
+  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL RMI OR CONTRIBUTORS BE LIABLE
+  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+  THE POSSIBILITY OF SUCH DAMAGE.
+
+  *****************************#RMI_2#**********************************/
+
+#ifndef _FIFO_H
+#define _FIFO_H
+
+#define FIFO_SIZE 32
+
+struct fifo {
+	int data[FIFO_SIZE];
+	int size;
+	int head;
+	int tail;
+};
+
+static __inline__ int fifo_next_head(struct fifo *fifo)
+{
+	return (fifo->head + 1) % fifo->size;
+}
+
+static __inline__ int fifo_next_tail(struct fifo *fifo)
+{
+	return (fifo->tail + 1) % fifo->size;
+}
+
+static __inline__ int fifo_empty(struct fifo *fifo)
+{
+	return fifo->head == fifo->tail ? 1 : 0;
+}
+
+static __inline__ int fifo_full(struct fifo *fifo)
+{
+	return fifo_next_tail(fifo) == fifo->head ? 1 : 0;
+}
+
+static __inline__ int fifo_size(struct fifo *fifo)
+{
+	if (fifo->head <= fifo->tail)
+		return fifo->tail - fifo->head;
+	else
+		return (fifo->size - fifo->head) + (fifo->tail - 0);
+}
+
+static __inline__ int fifo_dequeue(struct fifo *fifo, int *data)
+{
+	if (fifo_empty(fifo))
+		return 0;
+	*data = (fifo->data)[fifo->head];
+	fifo->head = fifo_next_head(fifo);
+	return 1;
+}
+
+static __inline__ int fifo_enqueue(struct fifo *fifo, int data)
+{
+	if (fifo_full(fifo))
+		return 0;
+	fifo->data[fifo->tail] = data;
+	fifo->tail = fifo_next_tail(fifo);
+	return 1;
+}
+
+static __inline__ void fifo_init(struct fifo *f)
+{
+	f->head = f->tail = 0;
+	f->size = FIFO_SIZE;
+}
+
+#endif
diff --git a/arch/mips/rmi/ptr/loader/loader.c b/arch/mips/rmi/ptr/loader/loader.c
new file mode 100644
index 0000000..3e2f0c0
--- /dev/null
+++ b/arch/mips/rmi/ptr/loader/loader.c
@@ -0,0 +1,578 @@
+/*********************************************************************
+
+  Copyright 2003-2006 Raza Microelectronics, Inc. (RMI). All rights
+  reserved.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+  1. Redistributions of source code must retain the above copyright
+  notice, this list of conditions and the following disclaimer.
+  2. Redistributions in binary form must reproduce the above copyright
+  notice, this list of conditions and the following disclaimer in
+  the documentation and/or other materials provided with the
+  distribution.
+
+  THIS SOFTWARE IS PROVIDED BY Raza Microelectronics, Inc. ``AS IS'' AND
+  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL RMI OR CONTRIBUTORS BE LIABLE
+  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+  THE POSSIBILITY OF SUCH DAMAGE.
+
+  *****************************#RMI_2#**********************************/
+
+#include <linux/kernel.h>
+#include <linux/delay.h>
+#include <linux/init.h>
+#include <linux/smp.h>
+#include <linux/interrupt.h>
+
+#include <asm/mipsregs.h>
+#include <asm/mmu_context.h>
+#include <asm/atomic.h>
+
+#include <asm/rmi/sim.h>
+#include <asm/rmi/msgring.h>
+#include <asm/rmi/mips-exts.h>
+#include <asm/bootinfo.h>
+#include <asm/asm.h>
+#include <asm/mipsregs.h>
+#include <asm/rmi/phnx_loader.h>
+#include <asm/rmi/user/phnx_loader.h>
+#include "xlr_boot_lib.h"
+
+#define GPIO_SWRESET_REG 8
+#define Message(a,b...)
+#define ErrorMsg(a,b...) xlr_loader_print(a,##b)
+
+#define MB(x) (unsigned long)(x<<20)
+
+int xlr_loader_print(const char *fmt, ...);
+
+int xlr_loader_put_char(char c);
+char xlr_loader_get_char(void);
+
+void xlr_uart_init(void);
+
+extern struct environment xlr_bldr_env;
+extern struct kuseg_mem_info kuseg_mem_map[];
+
+extern uint32_t phnx_loader_kseg_start, phnx_loader_kseg_size;
+
+struct xlr_vcpu_wakeup_info *xlr_wakeup_info;
+/*Setup this structure and pass it to lib_launch_init. */
+struct xlr_lib_launch_import *xlr_launch;
+struct xlr_lib_load_import *xlr_load;
+extern struct psb_info *prom_info;
+struct psb_info loader_prom_info;
+/* Loader will use this as the boot1_info structure */
+struct psb_info *xlr_linux_boot_info;
+struct psb_mem_map xlr_loader_mem_map;
+struct psb_io_map xlr_loader_io_map;
+static spinlock_t phnx_loader_lock;	/*Remove This. - */
+
+extern void reload_generic_trap_init(void);
+
+extern uint32_t phnx_loader_mask;
+extern uint32_t xlr_linux_cpu_mask;
+volatile extern phnx_loader_shared_struct_t *phnx_loader_sh_mem;
+extern struct smp_boot_info smp_boot;
+extern unsigned long phnx_app_shmem_start;
+extern uint32_t phnx_app_sh_mem_sz;
+
+extern struct r_exception_region *r_exception_vectors;
+
+/* This address is passed in K0 ($26) for all loaded apps */
+phnx_loader_info_t phnx_loader_info;
+
+volatile int xlr_wakeup_ipi[32];
+
+void prom_check_image(void);
+
+unsigned char *xlr_lib_shmem_start = NULL;
+uint32_t xlr_lib_shmem_size = 0;
+
+void reload_trap_init(void);
+static unsigned char printk_lock[16];
+static unsigned char loader_lock[16];
+static unsigned int global_wakeup_mask;
+#include <asm/cacheops.h>
+#include <asm/r4kcache.h>
+
+void xlr_loader_show_regs(struct xlr_rmios_pt_regs *regs)
+{
+	/* Saved main processor registers */
+	ErrorMsg("\n$0 :0x%016llx 0x%016llx 0x%016llx 0x%016llx",
+			 0ULL, regs->regs[1], regs->regs[2], regs->regs[3]);
+	ErrorMsg("\n$4 :0x%016llx 0x%016llx 0x%016llx 0x%016llx",
+			 regs->regs[4], regs->regs[5], regs->regs[6], regs->regs[7]);
+	ErrorMsg("\n$8 :0x%016llx 0x%016llx 0x%016llx 0x%016llx", regs->regs[8],
+			 regs->regs[9], regs->regs[10], regs->regs[11]);
+	ErrorMsg("\n$12 :0x%016llx 0x%016llx 0x%016llx 0x%016llx",
+			 regs->regs[12], regs->regs[13], regs->regs[14], regs->regs[15]);
+	ErrorMsg("\n$16 :0x%016llx 0x%016llx 0x%016llx 0x%016llx",
+			 regs->regs[16], regs->regs[17], regs->regs[18], regs->regs[19]);
+	ErrorMsg("\n$20 :0x%016llx 0x%016llx 0x%016llx 0x%016llx",
+			 regs->regs[20], regs->regs[21], regs->regs[22], regs->regs[23]);
+	ErrorMsg("\n$24 :0x%016llx 0x%016llx 0x%016llx 0x%016llx",
+			 regs->regs[24], regs->regs[25], regs->regs[26], regs->regs[27]);
+	ErrorMsg("\n$28 :0x%016llx 0x%016llx 0x%016llx 0x%016llx\n",
+			 regs->regs[28], regs->regs[29], regs->regs[30], regs->regs[31]);
+	/* Saved cp0 registers */
+	ErrorMsg("Hi : 0x%016llx\n", regs->hi);
+	ErrorMsg("Lo : 0x%016llx\n", regs->lo);
+	ErrorMsg("epc  : 0x%016llx    \nStatus: 0x%016llx\nCause : 0x%016llx\n",
+			 regs->cp0_epc, regs->cp0_status, regs->cp0_cause);
+}
+
+void xlr_flush_dcache_all(void)
+{
+	int i = 0;
+	int dcache_lines, dcache_sets, dcache_assoc, dcache_line_size;
+	uint32_t config1;
+	unsigned long base = (unsigned long) CKSEG0;
+
+	config1 = read_c0_config1();
+
+	dcache_sets = 1 << (((config1 >> 13) & 0x7) + 6);
+	dcache_assoc = ((config1 >> 7) & 0x7) + 1;
+	dcache_line_size = 1 << (((config1 >> 10) & 0x7) + 1);
+	dcache_lines = dcache_sets * dcache_assoc;
+
+	for (i = 0; i < dcache_lines; i++) {
+		cache_op(Index_Writeback_Inv_D, base);
+		base += dcache_line_size;
+	}
+}
+void local_flush_icache_all(void)
+{
+	int i = 0;
+	int icache_lines, icache_sets, icache_assoc, icache_line_size;
+	uint32_t config1;
+	unsigned long base = (unsigned long) CKSEG0;
+
+	config1 = read_c0_config1();
+	icache_sets = 1 << (((config1 >> 22) & 0x7) + 6);
+	icache_assoc = ((config1 >> 16) & 0x7) + 1;
+	icache_line_size = 1 << (((config1 >> 19) & 0x7) + 1);
+
+	icache_lines = icache_sets * icache_assoc;
+
+	for (i = 0; i < icache_lines; i++) {
+		cache_op(Index_Invalidate_I, base);
+		base += icache_line_size;
+	}
+
+}
+
+void phnx_local_flush_tlb_all(void)
+{
+	unsigned long old_ctx;
+	int entry;
+	int tlbsize;
+	unsigned int config1;
+
+	config1 = read_c0_config1();
+	tlbsize = ((config1 >> 25) & 0x3f) + 1;
+
+	/* Save old context and create impossible VPN2 value */
+	old_ctx = (read_c0_entryhi() & 0xff);
+	write_c0_entrylo0(0);
+	write_c0_entrylo1(0);
+	for (entry = 0; entry < tlbsize; entry++) {
+		write_c0_entryhi(((unsigned long) CKSEG0 + (PAGE_SIZE << 1) * entry));
+		write_c0_index(entry);
+		tlb_write_indexed();
+	}
+	write_c0_entryhi(old_ctx);
+}
+
+static uint32_t tlb_size_to_page_size(uint32_t size)
+{
+	if (size <= (4 * 1024))
+		return 4 * 1024;
+	if (size <= (16 * 1024))
+		return 16 * 1024;
+	if (size <= (64 * 1024))
+		return 64 * 1024;
+	if (size <= (256 * 1024))
+		return 256 * 1024;
+	if (size <= (1024 * 1024))
+		return 1024 * 1024;
+	if (size <= (4 * 1024 * 1024))
+		return 4 * 1024 * 1024;
+	if (size <= (16 * 1024 * 1024))
+		return 16 * 1024 * 1024;
+	if (size <= (64 * 1024 * 1024))
+		return 64 * 1024 * 1024;
+
+	return 256 * 1024 * 1024;
+}
+
+static uint32_t tlb_size_to_mask(uint32_t size)
+{
+	if (size <= (4 * 1024))
+		return 0x0 << 13;
+	if (size <= (16 * 1024))
+		return 0x03 << 13;
+	if (size <= (64 * 1024))
+		return 0x0f << 13;
+	if (size <= (256 * 1024))
+		return 0x3f << 13;
+	if (size <= (1024 * 1024))
+		return 0xff << 13;
+	if (size <= (4 * 1024 * 1024))
+		return 0x3ff << 13;
+	if (size <= (16 * 1024 * 1024))
+		return 0xfff << 13;
+	if (size <= (64 * 1024 * 1024))
+		return 0x3fff << 13;
+
+	return 0xffff << 13;
+}
+
+
+void phnx_setup_tlb(unsigned int virt, uint64_t phys, int size)
+{
+	uint64_t value = 0;
+	uint64_t attr = (3 << 3) | (1 << 2) | (1 << 1) | (1 << 0);
+	int wired = 0;
+	int page_size = tlb_size_to_page_size(size);
+	int page_mask = tlb_size_to_mask(page_size);
+
+	write_c0_pagemask(page_mask);
+	write_c0_entryhi((virt & 0xffffe000));
+	// write_64bit_cp0_register_sel(CP0_ENTRYHI, tmp, 0);
+
+	value = (((phys & 0xffffffffffULL) >> 12) << 6) | attr;
+	write_c0_entrylo0(value);
+
+	value = ((((phys + page_size) & 0xffffffffffULL) >> 12) << 6) | attr;
+	write_c0_entrylo1(value);
+	wired = read_c0_wired();
+
+	write_c0_index(wired);
+	tlb_write_indexed();
+
+	write_c0_wired(wired + 1);
+}
+
+void phnx_get_sp_gp(void)
+{
+	unsigned long sp, gp;
+	__asm__ __volatile__(".set push\n"
+						 ".set noreorder\n"
+						 "move %1, $28\n"
+						 "move %0, $29\n"
+						 "nop\n" ".set pop\n":"=r"(sp), "=r"(gp)
+		);
+	Message("SP = 0x%lx GP = 0x%lx", sp, gp);
+}
+
+void phnx_update_args(phnx_loader_shared_struct_t * sh_mem)
+{
+	struct cpu_wakeup_info *p = &sh_mem->run_info;
+	int i;
+
+	for (i = 0; i < p->argc; i++) {
+		p->argv[i] = (char *) ((unsigned long) p->argv[i] +
+							   (unsigned long) p->buf);
+	}
+}
+
+void phnx_prepare_cpu(void)
+{
+	uint32_t status;
+	uint64_t eirr;
+
+	write_c0_pagemask(0);
+	write_c0_wired(0);
+
+	write_c0_status(ST0_KX | ST0_CU0 | ST0_CU2);
+	write_c0_cause(0);
+	write_c0_compare(0);
+
+	status = read_c0_status();
+	write_c0_status((status & ~ST0_KX));
+
+	eirr = read_64bit_cp0_eirr();
+	write_64bit_cp0_eirr(eirr);
+}
+
+static void xlr_loader_not_implemented(void)
+{
+	ErrorMsg("xlr_loader: Unimplemented service requested");
+	while (1);
+}
+
+static void xlr_loader_shutdown(void)
+{
+	phoenix_reg_t *mmio = phoenix_io_mmio(PHOENIX_IO_GPIO_OFFSET);
+
+	/* trigger a chip reset */
+	phoenix_write_reg(mmio, GPIO_SWRESET_REG, 1);
+	for (;;)
+		cpu_wait();
+}
+
+void prom_init_xlr_loader_setup(struct psb_info *prom_info)
+{
+	loader_prom_info = *prom_info;
+	loader_prom_info.size = sizeof(loader_prom_info);
+	loader_prom_info.bldr_envp = (uint64_t) (unsigned long) &xlr_bldr_env;
+}
+
+void prom_init_xlr_loader(struct psb_info *prom_info)
+{
+	unsigned long shared_mem_size = 0;
+
+	shared_mem_size = sizeof(struct xlr_vcpu_wakeup_info) * 32;
+	shared_mem_size += sizeof(struct psb_info);
+	shared_mem_size += sizeof(struct xlr_lib_load_import);
+	shared_mem_size += sizeof(struct xlr_lib_launch_import);
+
+	Message("**************RQD SharedMemSize**************");
+	Message("VcpuWakeupInfo [%lx]",
+			(unsigned long) sizeof(struct xlr_vcpu_wakeup_info) * 32);
+	Message("PsbInfo [%lx]", (unsigned long) sizeof(struct psb_info));
+	Message("LibLoadImport [%lx]",
+			(unsigned long) sizeof(struct xlr_lib_load_import));
+	Message("LibLaunchImport [%lx]",
+			(unsigned long) sizeof(struct xlr_lib_launch_import));
+	Message("SharedMemSize [%lx]", (unsigned long) phnx_app_sh_mem_sz);
+	Message("TotalSize REquired B4 MAking It PageAlign [%#lx]",
+			(unsigned long) shared_mem_size);
+	/*Make shared mem size aligned. */
+	if (shared_mem_size & (PAGE_SIZE - 1)) {
+		shared_mem_size += PAGE_SIZE;
+		shared_mem_size = shared_mem_size & ~(PAGE_SIZE - 1);
+	}
+
+#ifdef CONFIG_64BIT
+	xlr_lib_shmem_start = (unsigned char *) (MB(49) | 0xffffffff80000000ULL);
+#else
+	xlr_lib_shmem_start = phys_to_virt(MB(49));
+#endif
+	xlr_lib_shmem_size = shared_mem_size;
+
+	/*SANITY CHECK */
+	if (shared_mem_size > MB(2)) {
+		printk("Required Shared Mem Size %#lx", shared_mem_size);
+		panic("---PANIC---");
+	}
+
+	if (!xlr_lib_shmem_start) {
+		panic("Couldnt Allocate Memory For shared DataStructure\n");
+	}
+	Message("Before Alignment Addr %#lx, Size %#lx",
+			(unsigned long) xlr_lib_shmem_start,
+			(unsigned long) xlr_lib_shmem_size);
+
+	if (((unsigned long) xlr_lib_shmem_start) & (PAGE_SIZE - 1)) {
+		xlr_lib_shmem_start += PAGE_SIZE;
+		xlr_lib_shmem_start = (unsigned char *)
+			(((unsigned long) xlr_lib_shmem_start) & ~(PAGE_SIZE - 1));
+	}
+	Message("Got the memory @ %#lx, size %#lx",
+			(unsigned long) xlr_lib_shmem_start,
+			(unsigned long) xlr_lib_shmem_size);
+
+	xlr_wakeup_info = (struct xlr_vcpu_wakeup_info *) xlr_lib_shmem_start;
+	xlr_linux_boot_info = (struct psb_info *) (xlr_wakeup_info + 32);
+	xlr_load = (struct xlr_lib_load_import *) (xlr_linux_boot_info + 1);
+	xlr_launch = (struct xlr_lib_launch_import *) (xlr_load + 1);
+
+	Message("xlr_wakeup_info @ %#lx, Size %#lx",
+			(unsigned long) xlr_wakeup_info,
+			sizeof(struct xlr_vcpu_wakeup_info));
+	Message("xlr_linux_boot_info @ %#lx, Size %#lx",
+			(unsigned long) xlr_linux_boot_info, sizeof(struct psb_info));
+	Message("xlr_load @ %#lx, Size %#lx", (unsigned long) xlr_load,
+			sizeof(struct xlr_lib_load_import));
+	Message("xlr_launch @ %#lx, size %#lx", (unsigned long) xlr_launch,
+			sizeof(struct xlr_lib_launch_import));
+	Message(" shared mem size is %#lx", (unsigned long) phnx_app_sh_mem_sz);
+
+	Message("sanity access %#x", *xlr_lib_shmem_start);
+	memset((void *) xlr_lib_shmem_start, 0, xlr_lib_shmem_size);
+	memcpy((void *) xlr_linux_boot_info, prom_info, sizeof(struct psb_info));
+
+	xlr_uart_init();
+	Message("UartInit Done.");
+	spin_lock_init(&phnx_loader_lock);
+
+	xlr_linux_boot_info->uart_print = PTR2U64(&xlr_loader_not_implemented);
+	xlr_linux_boot_info->led_output = PTR2U64(&xlr_loader_not_implemented);
+	xlr_linux_boot_info->init = PTR2U64(&xlr_loader_not_implemented);
+	xlr_linux_boot_info->exit = PTR2U64(&xlr_loader_not_implemented);
+	xlr_linux_boot_info->warm_reset = PTR2U64(&xlr_loader_shutdown);	/*Make this unimplemented. */
+	xlr_linux_boot_info->wakeup = PTR2U64(&xlr_loader_not_implemented);
+	xlr_linux_boot_info->cpu_online_map &= ~xlr_linux_cpu_mask;
+	xlr_linux_boot_info->master_reentry_sp = 0;
+	xlr_linux_boot_info->master_reentry_gp = 0;
+	xlr_linux_boot_info->master_reentry_fn =
+		PTR2U64(&xlr_loader_not_implemented);
+	xlr_linux_boot_info->slave_reentry_fn =
+		PTR2U64(&xlr_loader_not_implemented);
+	xlr_linux_boot_info->uart_putchar = PTR2U64(&xlr_loader_put_char);	/*Make this not implemented */
+	xlr_linux_boot_info->uart_getchar = PTR2U64(&xlr_loader_get_char);	/*Make this not implemented. */
+	xlr_linux_boot_info->nmi_handler = PTR2U64(&xlr_loader_not_implemented);
+	xlr_linux_boot_info->malloc = PTR2U64(&xlr_loader_not_implemented);
+	xlr_linux_boot_info->free = PTR2U64(&xlr_loader_not_implemented);
+	xlr_linux_boot_info->global_shmem_addr = prom_info->global_shmem_addr;
+	xlr_linux_boot_info->global_shmem_size = prom_info->global_shmem_size;
+	xlr_linux_boot_info->wakeup_os = PTR2U64(&xlr_loader_not_implemented);
+	/*
+	 * Yet xlr_loader_mem_map is not filled up with proper values, 
+	 * we will do it once we are done with parsing of all arguments. 
+	*/
+	xlr_linux_boot_info->psb_mem_map = PTR2U64(&xlr_loader_mem_map);
+
+	memcpy((void *) &xlr_loader_io_map,
+		   (void *) (unsigned long) prom_info->psb_physaddr_map,
+		   sizeof(struct psb_io_map));
+
+	xlr_linux_boot_info->psb_physaddr_map = PTR2U64(&xlr_loader_io_map);
+}
+
+void prom_start_loader(int cpu, unsigned long sp, unsigned long gp)
+{
+	/* Load Linux SP and GP for this thread and jump to loader function */
+	smp_boot.boot_info[cpu].sp = sp;
+	smp_boot.boot_info[cpu].gp = gp;
+	smp_boot.boot_info[cpu].fn = (unsigned long) &xlr_lib_entry;
+	/* barrier */
+	__sync();
+	smp_boot.boot_info[cpu].ready = 1;
+}
+
+int loader_processor_id(void)
+{
+	unsigned int id;
+	id = __read_32bit_c0_register($15, 1);
+	return (id & 0x1f);
+}
+
+void phnx_start_loader_threads(void)
+{
+	int i, index = 0;
+	uint32_t mask = phnx_loader_mask;
+	unsigned long long sp = 0, gp = 0;
+	uint64_t stack_start = 0;
+
+	Message("\nCallin prominit_xlrloader\n");
+	prom_init_xlr_loader(&loader_prom_info);
+
+	stack_start = (uint64_t) (unsigned long)
+		(xlr_lib_shmem_start + xlr_lib_shmem_size);
+	stack_start = (stack_start + PAGE_SIZE) & ~(PAGE_SIZE - 1);
+	xlr_launch->xlr_wakeup_info = (uint32_t) (unsigned long) xlr_wakeup_info;
+	xlr_launch->xlr_lib_boot1_info = (uint32_t) (unsigned long)
+		xlr_linux_boot_info;
+
+	/*
+	 * Below routine ll fill up 'r_exception_vectors+32' with proper exception 
+	 * handlers
+	*/
+	Message("");
+	reload_generic_trap_init();
+
+	/*'r_exception_vectors + 1' is filled up with proper exception hadnerls. */
+	xlr_launch->default_ebase = (uint32_t) (unsigned long)
+		(r_exception_vectors + 32);
+
+	Message("Default Ebase %#x", xlr_launch->default_ebase);
+	for (i = 0; i < 32; i++) {
+		if (((1U << i) & mask) == 0)
+			continue;
+		gp = stack_start + ((PAGE_SIZE << 1) * i);
+		if (gp == 0) {
+			panic("%s:No memory available for launching xlr loader threads\n",
+				  __func__);
+		}
+		sp = gp + (PAGE_SIZE << 1);
+		xlr_launch->gp[i] = gp;
+		xlr_launch->sp[i] = sp - 32;
+		Message("Thread %d ==> Sp [%#llx], GP [%#llx]",
+				i, (unsigned long long) xlr_launch->sp[i],
+				(unsigned long long) xlr_launch->gp[i]);
+	}
+	xlr_launch->cb_prelaunch_init_kuseg = 0;
+	xlr_launch->cb_prelaunch_init_kseg = 0;
+	xlr_launch->flush_icache_all = (int32_t) local_flush_icache_all;
+	xlr_launch->flush_dcache_all = (int32_t) xlr_flush_dcache_all;
+	xlr_launch->flush_tlb_all = (int32_t) phnx_local_flush_tlb_all;
+	xlr_launch->xlr_setup_tlb = (int32_t) phnx_setup_tlb;
+	xlr_launch->xlr_hard_vcpu_id = (int32_t) loader_processor_id;
+	xlr_launch->loader_lock = (int32_t) loader_lock;
+	xlr_launch->global_wakeup_mask = (int32_t) & global_wakeup_mask;
+	xlr_launch->trap_init = (int32_t) reload_trap_init;
+	xlr_launch->cpu_init = (int32_t) phnx_prepare_cpu;
+	xlr_launch->print = (int32_t) xlr_loader_print;
+	memset(loader_lock, 0, 16);
+	/*Do launch Init. */
+	Message("\nCallin Launch Init\n");
+	xlr_lib_launch_init(xlr_launch);
+
+	/*Put all loader threads in "wait" inst loop.. */
+	for (i = 0; i < 32; i++) {
+		if (((1U << i) & mask) == 0)
+			continue;
+		sp = xlr_launch->sp[i];
+		gp = xlr_launch->gp[i];
+		prom_start_loader(i, sp, gp);
+	}
+	/*Setup xlr_loader_mem_map with argument passed by user */
+	xlr_loader_mem_map.nr_map = 1;
+
+	xlr_loader_mem_map.map[index].addr = (uint64_t) phnx_loader_kseg_start;
+	xlr_loader_mem_map.map[index].size = (uint64_t) phnx_loader_kseg_size;
+	xlr_loader_mem_map.map[index].type = BOOT_MEM_RAM;
+	index++;
+
+	for (i = 0; i < MAX_NUM_KUSEG_BLOCKS; i++) {
+		if ((kuseg_mem_map[i].start_addr == 0)
+			&& (kuseg_mem_map[i].size == 0))
+			continue;
+
+		xlr_loader_mem_map.map[index].addr = kuseg_mem_map[i].start_addr;
+		xlr_loader_mem_map.map[index].size = kuseg_mem_map[i].size;
+		xlr_loader_mem_map.map[index].type = BOOT_MEM_RAM;
+		index++;
+		xlr_loader_mem_map.nr_map = xlr_loader_mem_map.nr_map + 1;
+	}
+
+	/*Setup load_init data structure. */
+	memcpy((void *) &xlr_load->default_map, (void *) &xlr_loader_mem_map,
+		   sizeof(struct psb_mem_map));
+	memcpy((void *) &xlr_load->recent_map, (void *) &xlr_loader_mem_map,
+		   sizeof(struct psb_mem_map));
+	xlr_load->userapp_cpu_mask = phnx_loader_mask;
+	xlr_load->total_avail_cpu = phnx_loader_mask;
+	xlr_load->recent_avail_cpu = phnx_loader_mask;
+	xlr_load->coredump_support = 0;
+	xlr_load->persistent_data = 0;
+	Message("");
+	memset(printk_lock, 0, 16);
+	xlr_load->lib_base_lock = (int32_t) printk_lock;
+	Message("*****XLR LOAD INFO*****");
+	Message("Total Avail Vcpu %#x", xlr_load->total_avail_cpu);
+	Message("Recent Avail Vcpu %#x", xlr_load->recent_avail_cpu);
+	Message("Kseg Mem %#llx @ %#llx",
+			(unsigned long long) phnx_loader_kseg_size,
+			(unsigned long long) phnx_loader_kseg_start);
+	for (i = 0; i < MAX_NUM_KUSEG_BLOCKS; i++) {
+		Message("Kuseg Mem %#llx @ %#llx",
+				(unsigned long long) kuseg_mem_map[i].size,
+				(unsigned long long) kuseg_mem_map[i].start_addr);
+	}
+	Message("**********************");
+	Message("Main Thread REturning.");
+}
diff --git a/arch/mips/rmi/ptr/loader/reload_irq_handler.S b/arch/mips/rmi/ptr/loader/reload_irq_handler.S
new file mode 100644
index 0000000..d7a78eb
--- /dev/null
+++ b/arch/mips/rmi/ptr/loader/reload_irq_handler.S
@@ -0,0 +1,80 @@
+/*********************************************************************
+ *
+ * Copyright 2003-2006 Raza Microelectronics, Inc. (RMI). All rights
+ * reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright
+ * notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ * notice, this list of conditions and the following disclaimer in
+ * the documentation and/or other materials provided with the
+ * distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Raza Microelectronics, Inc. ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL RMI OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+ * THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ *****************************#RMI_2#**********************************/
+
+#include <asm/mipsregs.h>
+#include <asm/regdef.h>
+#include <asm/asm.h> 
+#include <asm/stackframe.h>
+#include <asm/asm-offsets.h>
+#include "xlr_rmios_stackframe.h"
+
+#define KU_USER 0x10
+     
+	.text
+        .set    push
+        .set    noreorder
+        .set    mips4
+        .align  5
+        NESTED(reload_irq_handler, K_STACK_SIZE, sp)
+        save_stack_frame 
+        CLI
+
+        /* Read EIRR :   */
+        .word   0x40304806        /* dmfc0 s0, eirr */
+
+        /* If no interrupts, return */
+        beqz    s0, 2f
+        nop
+
+1:
+        /* retrieve the highest priority interrupt */
+        .word   0x72118824        /* dclz s1 s0 */
+        dsubu   a0, zero, s1
+        daddiu  a0, a0, 63
+        /* a0 now has irq# */
+        move    a1, sp
+        /* a1 now has sp */
+        /* first things first : clear the irq in eirr
+        *  note that setting a bit in eirr actually clears it!
+        */
+        li      s0, 1
+        dsllv   s0, s0, a0
+        .word   0x40b04806        /* dmtc0 s0, eirr */
+
+
+        /* a0 = irq, a1 = sp (regs) */
+        jal     reload_do_IRQ
+        nop
+2:      j       r_ret_from_irq
+        nop
+
+        .set pop
+        END(reload_irq_handler)
+
diff --git a/arch/mips/rmi/ptr/loader/traps.c b/arch/mips/rmi/ptr/loader/traps.c
new file mode 100644
index 0000000..c0faa44
--- /dev/null
+++ b/arch/mips/rmi/ptr/loader/traps.c
@@ -0,0 +1,219 @@
+/*********************************************************************
+
+  Copyright 2003-2006 Raza Microelectronics, Inc. (RMI). All rights
+  reserved.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+  1. Redistributions of source code must retain the above copyright
+  notice, this list of conditions and the following disclaimer.
+  2. Redistributions in binary form must reproduce the above copyright
+  notice, this list of conditions and the following disclaimer in
+  the documentation and/or other materials provided with the
+  distribution.
+
+  THIS SOFTWARE IS PROVIDED BY Raza Microelectronics, Inc. ``AS IS'' AND
+  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL RMI OR CONTRIBUTORS BE LIABLE
+  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+  THE POSSIBILITY OF SUCH DAMAGE.
+
+  *****************************#RMI_2#**********************************/
+
+#include <linux/kernel.h>
+#include <linux/delay.h>
+#include <linux/init.h>
+#include <linux/smp.h>
+
+#include <asm/mipsregs.h>
+#include <asm/asm.h>
+#include <asm/addrspace.h>
+#include "xlr_boot_lib.h"
+#include <asm/rmi/phnx_loader.h>
+#include <asm/rmi/mips-exts.h>
+#include "xlr_rmios_stackframe.h"
+
+extern int xlr_loader_print(const char *fmt, ...);
+extern void xlr_loader_show_regs(struct xlr_rmios_pt_regs *regs);
+
+#define Message(a,b...)
+#define ErrorMsg(a,b...) xlr_loader_print(a,##b)
+
+unsigned int r_exception_handlers[32];
+
+struct r_exception_region *r_exception_vectors =
+	(struct r_exception_region *) CKSEG0;
+
+
+void local_flush_icache_all(void);
+void xlr_flush_dcache_all(void);
+
+extern void reload_handle_reserved(void);
+extern void reload_irq_handler(void);
+
+extern char reload_except_vec_tlbrefill[], reload_except_vec_xtlbrefill[];
+extern char reload_except_vec_genex[], reload_except_vec_cacheerr[];
+extern char reload_except_vec_vecint[];
+
+
+static void *r_set_except_vector(int n, void *addr)
+{
+	unsigned long handler = (unsigned long) addr;
+	unsigned long old_handler = r_exception_handlers[n];
+
+	r_exception_handlers[n] = (unsigned int) handler;
+	return (void *) old_handler;
+}
+
+#define CP0_EBASE     $15
+
+void reload_generic_trap_init(void)
+{
+	int i;
+	unsigned long ebase;
+	int cpu = 32;
+	for (i = 1; i < 32; i++) {
+		r_set_except_vector(i, reload_handle_reserved);
+	}
+
+	/* Set the Interrupt - Exception Handler (Ex Code 0) */
+	r_set_except_vector(0, reload_irq_handler);
+	ebase = (unsigned long) &r_exception_vectors[cpu];
+
+	memcpy((void *) (ebase), reload_except_vec_tlbrefill, 0x80);
+	memcpy((void *) (ebase + 0x80), reload_except_vec_xtlbrefill, 0x80);
+	memcpy((void *) (ebase + 0x100), reload_except_vec_cacheerr, 0x80);
+	memcpy((void *) (ebase + 0x180), reload_except_vec_genex, 0x80);
+	memcpy((void *) (ebase + 0x200), reload_except_vec_vecint, 0x80);
+	local_flush_icache_all();
+	xlr_flush_dcache_all();
+}
+
+void reload_trap_init(void)
+{
+	int i;
+	unsigned long ebase;
+	int cpu;
+	cpu = hard_smp_processor_id();
+
+	/* Setup default vectors */
+	for (i = 1; i < 32; i++) {
+		r_set_except_vector(i, reload_handle_reserved);
+	}
+
+	/* Set the Interrupt - Exception Handler (Ex Code 0) */
+	r_set_except_vector(0, reload_irq_handler);
+
+	/* Copy the generic exception handler code to it's final destination. */
+
+	ebase = (unsigned long) &r_exception_vectors[cpu];
+
+	memcpy((void *) (ebase), reload_except_vec_tlbrefill, 0x80);
+	memcpy((void *) (ebase + 0x80), reload_except_vec_xtlbrefill, 0x80);
+	memcpy((void *) (ebase + 0x100), reload_except_vec_cacheerr, 0x80);
+	memcpy((void *) (ebase + 0x180), reload_except_vec_genex, 0x80);
+	memcpy((void *) (ebase + 0x200), reload_except_vec_vecint, 0x80);
+
+	/* set up the ebase register */
+	__write_32bit_c0_register($15, 1, (uint32_t) (ebase & 0x3ffff000));
+	/* Flush I-Cache */
+	local_flush_icache_all();
+
+	/* 
+	 * Note: Need to do this only because the cacheerr exception vector may be still in
+	 * D-cache when cpu takes the exception
+	 */
+	xlr_flush_dcache_all();
+
+}
+
+unsigned int reload_do_IRQ(int irq, struct xlr_rmios_pt_regs *regs)
+{
+	int cpu;
+	cpu = hard_smp_processor_id();
+	switch (irq) {
+		case IRQ_WAKEUP_CPU_IPI:
+		case IRQ_STOP_CPU_IPI:
+			xlr_lib_intr_handler(irq);
+			break;
+		default:
+			ErrorMsg(" Received Unhandled Interrupt = %d!\n", irq);
+			break;
+	}
+	return 0;
+}
+
+void xlr_tlb_panic(struct xlr_rmios_pt_regs *pt_regs)
+{
+	int id = hard_smp_processor_id();
+	reset_printk_base_lock();
+	xlr_loader_show_regs(pt_regs);
+	ErrorMsg("\nUnahandled TLB Refill Exception!!");
+	ErrorMsg("\ncpu_%d: PANIC!!", id);
+	ErrorMsg("\nStop this vcpu using \"stop_vcpu -m <mask>\" cmd.\n");
+	__asm__ __volatile__(
+			"mtc0 %0, $14\n"
+			"move $4,%1\n"
+			"nop\n" "eret\n" "nop\n"
+			::"r"(xlr_lib_entry), "r"(id)
+		);
+}
+
+void xlr_xtlb_panic(struct xlr_rmios_pt_regs *pt_regs)
+{
+	int id = hard_smp_processor_id();
+
+	reset_printk_base_lock();
+	xlr_loader_show_regs(pt_regs);
+	ErrorMsg("\nUnahandled XTLB Refill Exception!!");
+	ErrorMsg("\nStop this vcpu using \"stop_vcpu -m <mask>\" cmd.\n");
+
+	__asm__ __volatile__(
+			"mtc0 %0, $14\n"
+			"move $4,%1\n"
+			"nop\n" "eret\n" "nop\n"
+			::"r"(xlr_lib_entry), "r"(id)
+		);
+}
+
+void xlr_vecint_panic(struct xlr_rmios_pt_regs *pt_regs)
+{
+	int id = hard_smp_processor_id();
+
+	reset_printk_base_lock();
+	xlr_loader_show_regs(pt_regs);
+	ErrorMsg("\nUnahandled Vectored Interrupt !!");
+	ErrorMsg("\nStop this vcpu using \"stop_vcpu -m <mask>\" cmd.\n");
+
+	__asm__ __volatile__(
+			"mtc0 %0, $14\n"
+			"move $4,%1\n"
+			"nop\n" "eret\n" "nop\n"
+			::"r"(xlr_lib_entry), "r"(id)
+		);
+}
+
+void do_reload_setup(struct xlr_rmios_pt_regs *regs)
+{
+	int id = hard_smp_processor_id();
+
+	reset_printk_base_lock();
+	xlr_loader_show_regs(regs);
+	ErrorMsg("\ncpu_%d: PANIC!!", id);
+	ErrorMsg("Stop this vcpu using stop -m <mask> cmd\n");
+
+	__asm__ __volatile__(
+			"mtc0 %0, $14\n"
+			"move $4,%1\n"
+			"nop\n" "eret\n" "nop\n"
+			::"r"(xlr_lib_entry), "r"(id)
+		);
+}
diff --git a/arch/mips/rmi/ptr/loader/uart.c b/arch/mips/rmi/ptr/loader/uart.c
new file mode 100644
index 0000000..78d7d61
--- /dev/null
+++ b/arch/mips/rmi/ptr/loader/uart.c
@@ -0,0 +1,332 @@
+/*********************************************************************
+
+  Copyright 2003-2006 Raza Microelectronics, Inc. (RMI). All rights
+  reserved.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+  1. Redistributions of source code must retain the above copyright
+  notice, this list of conditions and the following disclaimer.
+  2. Redistributions in binary form must reproduce the above copyright
+  notice, this list of conditions and the following disclaimer in
+  the documentation and/or other materials provided with the
+  distribution.
+
+  THIS SOFTWARE IS PROVIDED BY Raza Microelectronics, Inc. ``AS IS'' AND
+  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL RMI OR CONTRIBUTORS BE LIABLE
+  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+  THE POSSIBILITY OF SUCH DAMAGE.
+
+  *****************************#RMI_2#**********************************/
+
+#include <linux/spinlock.h>
+#include <linux/mm.h>
+#include <linux/bootmem.h>
+#include <linux/init.h>
+
+#include <asm/irq.h>
+#include <asm/io.h>
+#include <asm/bootinfo.h>
+#include <asm/addrspace.h>
+#include <asm/reboot.h>
+#include <asm/time.h>
+#include <linux/interrupt.h>
+#include <asm/atomic.h>
+#include <asm/bootinfo.h>
+
+#include <asm/rmi/sim.h>
+#include <asm/rmi/mips-exts.h>
+#include <asm/rmi/iomap.h>
+#include <asm/rmi/xlr_virt_uart.h>
+#include "uart.h"
+#include "fifo.h"
+
+
+static struct fifo tx_fifo;
+static struct fifo rx_fifo;
+
+virt_uart virt_uart_outbuf[32];
+
+extern unsigned char load_env[32][6];
+extern int loader_processor_id(void);
+
+#define TX_BURST_SIZE 1
+
+#define DMESG_BUFSIZE 0x1000
+
+
+int tstbyte(void)
+{
+	int data = 0;
+	if (fifo_dequeue(&rx_fifo, &data)) {
+		/* FIFO not empty */
+		return 1;
+	}
+	return 0;
+}
+
+
+char inbyte(void)
+{
+	int data = 0;
+	int lsr = 0;
+	int i = 0;
+	volatile uint32_t *mmio = phoenix_io_mmio(PHOENIX_IO_UART_1_OFFSET);
+
+	if (fifo_dequeue(&rx_fifo, &data)) {
+		/* characters to be read already in fifo */
+		return (char) data;
+	}
+
+	for (;;) {
+
+		lsr = be32_to_cpu(phoenix_read_reg(mmio, UART_LSR));
+
+		if (lsr & 0x20) {
+			/* Tx available, try to send any characters */
+			for (i = 0;
+				 i < TX_BURST_SIZE && fifo_dequeue(&tx_fifo, &data); i++) {
+				phoenix_write_reg(mmio, UART_THR, cpu_to_be32(data));
+			}
+		}
+		if (lsr & 0x80) {
+			/* parity/frame/break - push a 0! */
+			if (!fifo_enqueue(&rx_fifo, 0))
+				break;
+		}
+		if (lsr & 0x01) {
+			/* Rx Data */
+			data = be32_to_cpu(phoenix_read_reg(mmio, UART_RHR));
+			if (!fifo_enqueue(&rx_fifo, data))
+				break;
+		}
+		if (!fifo_empty(&rx_fifo))
+			break;
+	}
+
+	fifo_dequeue(&rx_fifo, &data);
+
+	return (char) data;
+}
+
+
+void virt_uart_outbyte(char c, int thrd_id)
+{
+
+	if (*(virt_uart_outbuf[thrd_id].status) != VIRT_UART_OPENED)
+		return;
+
+	while (((*(virt_uart_outbuf[thrd_id].tx_pro) + 1) % (USER_RESULT_SIZE)) ==
+		   (*(virt_uart_outbuf[thrd_id].tx_con)));
+
+	*((virt_uart_outbuf[thrd_id].tx_fifo) +
+			*(virt_uart_outbuf[thrd_id].tx_pro)) = c;
+	*(virt_uart_outbuf[thrd_id].tx_pro) =
+			(*(virt_uart_outbuf[thrd_id].tx_pro) + 1) % (USER_RESULT_SIZE);
+
+	if (c == '\n') {
+		while (((*(virt_uart_outbuf[thrd_id].tx_pro) + 1)
+					% (USER_RESULT_SIZE)) ==
+						(*(virt_uart_outbuf[thrd_id].tx_con)));
+		*((virt_uart_outbuf[thrd_id].tx_fifo) +
+				*(virt_uart_outbuf[thrd_id].tx_pro)) = '\r';
+		*(virt_uart_outbuf[thrd_id].tx_pro) =
+				(*(virt_uart_outbuf[thrd_id].tx_pro) + 1) % (USER_RESULT_SIZE);
+	}
+	return;
+}
+
+int outbyte(char c)
+{
+	int data = 0;
+	int lsr = 0;
+	int i = 0;
+	int thrd_id;
+
+	volatile uint32_t *mmio = phoenix_io_mmio(PHOENIX_IO_UART_1_OFFSET);
+
+	thrd_id = loader_processor_id();
+
+	if (strcmp(load_env[thrd_id], "vuart") == 0) {
+		virt_uart_outbyte(c, thrd_id);
+		return 0;
+	}
+	if (c == '\n') {
+		char cr = '\r';
+		if (!fifo_enqueue(&tx_fifo, (int) cr)) {
+			/* Too bad tx fifo is full, drop the character */
+			return 1;
+		}
+	}
+	if (!fifo_enqueue(&tx_fifo, (int) c)) {
+		/* Too bad tx fifo is full, drop the character */
+		return 1;
+	}
+
+	/* accumalate some more characters */
+	if ((fifo_size(&tx_fifo) < TX_BURST_SIZE) && (c != '\n') && (c != '\r'))
+		return 0;
+	for (;;) {
+
+		if (fifo_empty(&tx_fifo))
+			break;
+
+		/* wait for Tx empty indication */
+		for (;;) {
+
+			lsr = be32_to_cpu(phoenix_read_reg(mmio, UART_LSR));
+
+			if (lsr & 0x80) {
+				/* parity/frame/break - push a 0! */
+				fifo_enqueue(&rx_fifo, 0);
+			}
+			if (lsr & 0x01) {
+				/* Rx Data */
+				data = be32_to_cpu(phoenix_read_reg(mmio, UART_RHR));
+				fifo_enqueue(&rx_fifo, data);
+			}
+			/* Tx Fifo empty */
+			if (lsr & 0x20)
+				break;
+		}
+
+		/* transmit upto TX_BURST_SIZE char */
+		for (i = 0; i < TX_BURST_SIZE && fifo_dequeue(&tx_fifo, &data); i++) {
+			phoenix_write_reg(mmio, UART_THR, cpu_to_be32(data));
+		}
+	}
+
+	return 0;
+}
+
+void serial_puts(const char *s)
+{
+	while (*s) {
+		outbyte(*s++);
+	}
+}
+
+int putch(int ch)
+{
+	return outbyte(ch);
+}
+
+void uart_flush_tx_buf(void)
+{
+	int data = 0;
+	int lsr = 0;
+	volatile uint32_t *mmio = phoenix_io_mmio(PHOENIX_IO_UART_1_OFFSET);
+
+	for (;;) {
+
+		lsr = be32_to_cpu(phoenix_read_reg(mmio, UART_LSR));
+
+		if (lsr & 0x20) {
+
+			if (!fifo_dequeue(&tx_fifo, &data))
+				break;
+
+			/* Tx available, try to send more characters */
+			phoenix_write_reg(mmio, UART_THR, cpu_to_be32(data));
+		}
+	}
+}
+
+spinlock_t printf_lock;
+
+void xlr_virt_uart_init(void)
+{
+	int i, size;
+
+	size = USER_CMD_SIZE + USER_RESULT_SIZE + sizeof(unsigned int) * 5;
+
+	for (i = 0; i < 32; i++) {
+		virt_uart_outbuf[i].rx_fifo =
+			i * size + (unsigned char *) VIRT_UART_BUF_START;
+		virt_uart_outbuf[i].rx_pro =
+			(unsigned int *) (virt_uart_outbuf[i].rx_fifo + USER_CMD_SIZE);
+		virt_uart_outbuf[i].rx_con =
+			(unsigned int *) (virt_uart_outbuf[i].rx_pro + 1);
+		virt_uart_outbuf[i].tx_fifo =
+			(unsigned char *) (virt_uart_outbuf[i].rx_con + 1);
+		virt_uart_outbuf[i].tx_pro =
+			(unsigned int *) (virt_uart_outbuf[i].tx_fifo + USER_RESULT_SIZE);
+		virt_uart_outbuf[i].tx_con =
+			(unsigned int *) (virt_uart_outbuf[i].tx_pro + 1);
+		virt_uart_outbuf[i].status =
+			(unsigned int *) (virt_uart_outbuf[i].tx_con + 1);
+	}
+}
+
+void xlr_uart_init(void)
+{
+	volatile uint32_t *mmio = phoenix_io_mmio(PHOENIX_IO_UART_1_OFFSET);
+
+	fifo_init(&rx_fifo);
+	fifo_init(&tx_fifo);
+	spin_lock_init(&printf_lock);
+	xlr_virt_uart_init();
+
+	/* Set up the baud rate */
+	phoenix_write_reg(mmio, UART_LCR,
+					  cpu_to_be32(be32_to_cpu
+								  (phoenix_read_reg(mmio, UART_LCR))
+								  | (1 << 7)));
+	phoenix_write_reg(mmio, UART_DLB_1, cpu_to_be32(UART_BR_DLB1));
+	phoenix_write_reg(mmio, UART_DLB_2, cpu_to_be32(UART_BR_DLB2));
+	phoenix_write_reg(mmio, UART_LCR,
+					  cpu_to_be32(be32_to_cpu
+								  (phoenix_read_reg(mmio, UART_LCR)) & 
+								  ~(1 << 7)));
+
+}
+
+char xlr_loader_get_char(void)
+{
+	return inbyte();
+}
+
+#define CFG_PBSIZE 128
+int puts(const char *s);
+
+int xlr_loader_print(const char *fmt, ...)
+{
+	va_list args;
+	unsigned long flags;
+	char printbuffer[CFG_PBSIZE];
+	int i;
+
+	local_irq_save(flags);
+	spin_lock(&printf_lock);
+
+	va_start(args, fmt);
+	/* 
+	 * For this to work, printbuffer must be larger than
+	 * anything we ever want to print.
+	*/
+	i = vsprintf(printbuffer, fmt, args);
+	va_end(args);
+
+	/* Print the string */
+	puts(printbuffer);
+	/* flush the transmit buffer */
+	uart_flush_tx_buf();
+	spin_unlock(&printf_lock);
+	local_irq_restore(flags);
+	return 0;
+}
+
+int xlr_loader_put_char(char c)
+{
+	outbyte(c);
+	return 0;
+}
diff --git a/arch/mips/rmi/ptr/loader/uart.h b/arch/mips/rmi/ptr/loader/uart.h
new file mode 100644
index 0000000..88c4519
--- /dev/null
+++ b/arch/mips/rmi/ptr/loader/uart.h
@@ -0,0 +1,72 @@
+/*********************************************************************
+
+  Copyright 2003-2006 Raza Microelectronics, Inc. (RMI). All rights
+  reserved.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+  1. Redistributions of source code must retain the above copyright
+  notice, this list of conditions and the following disclaimer.
+  2. Redistributions in binary form must reproduce the above copyright
+  notice, this list of conditions and the following disclaimer in
+  the documentation and/or other materials provided with the
+  distribution.
+
+  THIS SOFTWARE IS PROVIDED BY Raza Microelectronics, Inc. ``AS IS'' AND
+  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL RMI OR CONTRIBUTORS BE LIABLE
+  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+  THE POSSIBILITY OF SUCH DAMAGE.
+
+  *****************************#RMI_2#**********************************/
+
+#ifndef _UART_H
+#define _UART_H
+
+#define UART_RHR 0
+#define UART_THR 0
+#define UART_IER 1
+#define UART_IIR 2
+#define UART_FCR 2
+#define UART_LCR 3
+#define UART_MCR 4
+#define UART_LSR 5
+#define UART_MSR 6
+
+#define UART_DLB_1 0
+#define UART_DLB_2 1
+
+#define UART_DEBUG_1 8
+#define UART_DEBUG_2 9
+
+#define UART_BR9600_DLB1 0xad
+#define UART_BR9600_DLB2 0x01
+
+#define UART_BR38400_DLB1 0x6b
+#define UART_BR38400_DLB2 0x00
+
+#define UART_BR115200_DLB1 0x23
+#define UART_BR115200_DLB2 0x00
+
+#ifndef PHOENIX_SIM
+#define UART_BR_DLB1 UART_BR38400_DLB1
+#define UART_BR_DLB2 UART_BR38400_DLB2
+#else
+#define UART_BR_DLB1 UART_BR115200_DLB1
+#define UART_BR_DLB2 UART_BR115200_DLB2
+#endif
+
+#ifndef __ASSEMBLY__
+extern int outbyte(char c);
+extern char inbyte(void);
+#endif
+
+#endif
diff --git a/arch/mips/rmi/ptr/loader/xlr_boot_lib.h b/arch/mips/rmi/ptr/loader/xlr_boot_lib.h
new file mode 100644
index 0000000..2b16380
--- /dev/null
+++ b/arch/mips/rmi/ptr/loader/xlr_boot_lib.h
@@ -0,0 +1,392 @@
+/***********************************************************************
+Copyright 2003-2006 Raza Microelectronics, Inc.(RMI). All rights
+reserved.
+Use of this software shall be governed in all respects by the terms and
+conditions of the RMI software license agreement ("SLA") that was
+accepted by the user as a condition to opening the attached files.
+Without limiting the foregoing, use of this software in source and
+binary code forms, with or without modification, and subject to all
+other SLA terms and conditions, is permitted.
+Any transfer or redistribution of the source code, with or without
+modification, IS PROHIBITED,unless specifically allowed by the SLA.
+Any transfer or redistribution of the binary code, with or without
+modification, is permitted, provided that the following condition is
+met:
+Redistributions in binary form must reproduce the above copyright
+notice, the SLA, this list of conditions and the following disclaimer
+in the documentation and/or other materials provided with the
+distribution:
+THIS SOFTWARE IS PROVIDED BY Raza Microelectronics, Inc. `AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL RMI OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#RMI_3#***********************************/
+
+#ifndef __XLR_BOOT_LIB_H
+
+#define __XLR_BOOT_LIB_H
+
+typedef struct
+{
+	volatile unsigned int lock;
+} lib_spinlock_t;
+
+#define CPU_RUNNING 0x1
+#define CPU_STOPPED 0x2
+#define MAX_ELF_SEGMENTS 16
+#define MAX_TLB_MAPPINGS 16
+
+#include "xlr_lib_platform.h"
+
+enum { KUSEG_APP, KSEG0_APP };
+
+typedef volatile uint32_t xlr_lib_phnx_reg_t;
+
+#define IRQ_STOP_CPU_IPI 51
+#define IRQ_WAKEUP_CPU_IPI 52
+
+#define XLR_PERSISTENT_MEMORY_MAGIC 0xdead900d
+#define NMI_BL_SHARED_REGION 0x1f000000
+
+#define KSEG0_MEM_MAP 0x10000001
+#define KUSEG_MEM_MAP 0x10000002
+
+#define XLR_ARGV_BUF_SIZE 256
+#define XLR_ENV_BUF_SIZE 256
+#define XLR_MAX_ARGV 32
+#define XLR_MAX_ENV 32
+#define XLR_NUM_CPU 32
+
+struct proc_section_info
+{
+	int mode;
+	uint32_t vaddr[MAX_ELF_SEGMENTS];
+	uint32_t size[MAX_ELF_SEGMENTS];
+	uint32_t tlb_vaddr[MAX_TLB_MAPPINGS];
+	uint32_t tlb_size[MAX_TLB_MAPPINGS];
+	uint64_t tlb_phys[MAX_TLB_MAPPINGS];
+};
+
+
+struct lib_cpu_tlb_mapping
+{
+	int page_size;
+	int asid;
+	int coherency;
+	int attr;
+	uint32_t virt;
+	uint64_t phys;
+};
+
+
+struct core_segment_info
+{
+	uint64_t vaddr;
+	uint64_t memsz;
+	uint32_t flags;
+};
+
+
+struct xlr_vcpu_wakeup_info
+{
+	/*RUNNING or STOPPED */
+	volatile uint32_t cpu_status;
+
+	/*Validity for this structure. */
+	int valid;
+
+	/*
+	 * For master vcpu this is an elf_entry, for buddy vcpus this points to
+	 * the function pointer passed in "wakeup" call. 
+	*/
+	int32_t func, args;
+
+	/*Stack pointer for this cpu. */
+	uint64_t sp, gp;
+
+	/*Master CPU No. */
+	int master_cpu;
+
+	/*Master CPU Mask. */
+	uint32_t master_mask;
+
+	/*Buddy CPU Mask */
+	uint32_t buddy_mask;
+
+	uint32_t psb_os_cpu_map;
+
+	/*
+	 * Argv passed while launching application. These all are pointers in 
+	 * argv_buf.
+	*/
+	int argc;							/*No. of arg */
+	uint32_t argv[XLR_MAX_ARGV];
+	char argv_buf[XLR_ARGV_BUF_SIZE];	/*Actual Argv */
+
+	/*No. Of Valid TLB Entries */
+	int map_count;
+
+	/*Valid only in case of kuseg applications. Kuseg master and buddy vcpus 
+	   both will have valid entries here. */
+	struct lib_cpu_tlb_mapping map[MAX_TLB_MAPPINGS];
+	struct core_segment_info seg_info[MAX_ELF_SEGMENTS];
+
+	/* Environment variables, envs is an array of pointers. All pointers points 
+	 * in to env_buf. Output io device is specfied in one of the environment 
+	 * variale as ttyS0 or ttyS1. Generic bootloader always sets this to ttyS0
+	*/
+	int env;					/*No. Of env */
+	uint32_t envs[XLR_MAX_ENV];
+	char env_buf[XLR_ENV_BUF_SIZE];
+
+	/*Mode to decide Kuseg/Kseg applications */
+	uint32_t mode;
+
+	/* 
+	 * This points to same physical memory (kseg0), for all applications. 
+	 * "printk" routine of rmios_lib will hold this lock while dumping 
+	*/
+	int32_t printk_lock;
+
+	/*Set when we are launching an kseg application on other than vcpu0 */
+	int kseg_master;
+
+	/*
+	 * Reentry Function For KUSEG Applications, Each vcpu can have its own 
+	 * reentry function. 
+	*/
+	int32_t reentry_function;
+	uint32_t reentry_args;
+
+	/*Reserved shared memory between all application launched by bootloader */
+	uint64_t app_shared_mem_addr;
+	uint64_t app_shared_mem_size;
+	uint64_t app_shared_mem_orig;
+
+	/*Lock used to avoid window between wakeup and stop */
+	int32_t loader_lock;
+
+	/*
+	 * This must contain same values in all xlr_vcpu_wakeup_info structures.
+	 * This is used to avoid window while freeing memory.
+	 * unsigned long *global_wakeup_mask; 
+	*/
+	int32_t global_wakeup_mask;
+
+	/*This ll be set to 1 if memory being used by this vcpu can be freed */
+	uint32_t can_free_memory;
+};
+
+
+/*
+ * Foll. structure contains implementation dependent routines and variables. 
+ * This must be filled by the loader implementation. Strucure should be defined
+ * in KSEG0 region, and memory has to be persistent. Library may change values
+ * of this structure variables, and also expects value to be same when 
+ * load_init/launch_init gets call second time.
+	
+ * Linux Loader: In case of linux loader, "loading" is done from user space and
+ * "launhing" is done from kernel space, so foll routines will have mixed 
+ * kernel/user space address. This will be mmaped structure for linux loader, 
+ * whenver any application is launched using "load" cmd, he has to set this
+ * structure with proper function pointers. Kernel Implementation of linux 
+ * loader has to take care of setting "prelaunch_init" routines.
+
+ * Implementation has to fill up foll. structures with appropriate values.
+ * This strucure must be defined in KSEG0 Region. Linux Loader loading 
+ * applications must have to mmap this structure.
+ * Generic Bootloader: All pointers will have kseg0 address.
+ * Linux Loader: All pointers will have kuseg address.
+ *
+ * GLOBAL STRUCTURES SHARED BETWEEN ALL VCPUS
+ *		xlr_lib_load_import:	Used For Elfload.
+ *		xxlr_lib_launch_import:	Used During Launching any application, 
+								for setup tlb, wakeup, prelaunchinit etc.
+*/
+struct xlr_lib_load_import
+{
+	/*
+	 * CopyData - (DestPhysAddr, SrcVirtAddr, Size). Must contain "kuseg" 
+	 * address for linux loader. 
+	*/
+	int32_t copy_data;
+	int32_t copy_data_uncached;
+
+	/* 
+	 * SetData - (DestPhysAddr, SetValue, Size). Must contain "kuseg" 
+	 * address for linux loader. 
+	*/
+	int32_t set_data;
+
+	/*
+	 * Pointer to function to call wakeup_cpu. Proceduer is implementation 
+	 * dependent. 
+	 * LinuxLoader: Linux loader sends an ioctl to the driver and driver sends
+	 * an ipi to the specified cpu and wakes up it.
+	 * GenericLoader: Directly sends an ipi to the specified vcpu.
+	*/
+
+	int32_t call_wakeup_cpu;
+	int32_t call_stop_vcpu;
+
+	/*call_xlr_start_app: valid only in case of generic bootlaoder. */
+	int32_t call_xlr_start_app;
+
+	/*Implementation must have to fill up this region. */
+	struct xlr_lib_psb_mem_map default_map;
+	struct xlr_lib_psb_mem_map recent_map;
+
+	/* 
+	 * Befor Loading Verify whether application can be launched on specified 
+	 * cpu or not.
+     * Linux Loader: Before loading any kuseg/kseg application verify whether
+	 * specified vcpu is available or not. If available then update the mask.
+	 * Generic Loader: Before loading any kuseg application verify whether 
+	 * specified vcpu is availble or not, if yes then update it while 
+	 * loading only. Kseg application never fails in case of generic 
+	 * bootloader, atleast you will find one vcpu free (vcpu0) which can 
+	 * launch this application. Once kseg is launched we are mot much 
+	 * bothered about any other stuffs .. as we wont have any other 
+	 * control.. though update boot1_info as it is.
+	*/
+	uint32_t total_avail_cpu;
+	uint32_t recent_avail_cpu;
+	uint32_t userapp_cpu_mask;
+
+	/*Pointer To Base of WakeupInfo Structure. */
+	int32_t xlr_wakeup_info;
+
+	/*Pointer To Generic Boot1Info */
+	int32_t xlr_lib_boot1_info;
+
+	/*
+	 * Define This as a pointer to buffer of 16 bytes this must be defined in 
+	 * kseg0 region. Implementation must have to call memset on this b4 calling
+	 * library routines. This must be done only once, linux loader has to take
+	 * care of this with some additional logic. This ll be used for printk 
+	 * spin_lock
+	*/
+	int32_t lib_base_lock;
+
+	int coredump_support;
+
+	/*
+	 * Below routine must return hard vcpu id if and only if launching is 
+	 * allowed on threads running loader, Otherwise return -1
+	*/
+	uint32_t xlr_hard_vcpu_id;
+
+	/*
+	 * Library can call this routine to store some data, this memory has to be
+	 * persistent, next time while calling lib_load_init, implementation has 
+	 * to set persistent_data to this memory 
+	*/
+	int32_t xlr_get_persistent_memory;
+	int32_t xlr_free_persistent_memory;
+
+	/*
+	 * If lib has ever stored anything in persistent memory then this pointer 
+	 * will point to that memory. Implementation need not worry about the way 
+	 * data is stored in to this memory, it must mmap(in case of linux loader) 
+	 * this memory and pass it to loader library. 
+	*/
+	int32_t persistent_data;
+	uint32_t persistent_data_length;
+	int32_t print;
+};
+
+/*
+ * Implementation has to fill up foll. structures with appropriate values.
+ * This strucure must be defined in KSEG0 Region. Linux Loader loading 
+ * applications must have to mmap this structure.
+ * Generic Bootloader: All pointers will have kseg0 address.
+ * Linux Loader: All pointers will have kseg0/kseg2 address.
+*/
+struct xlr_lib_launch_import
+{
+
+	/* Implementation has to set this to some kseg0 values for each cpu. */
+	uint64_t sp[32];
+	uint64_t gp[32];
+
+	/*
+	 * PreLaunchInit, Before launching any application on the kuseg/kseg 
+	 * "master" cpu respective routine will get call. In linux loader this 
+	 * ll be called frm kernel context and in generic bootloader this ll 
+	 * get call in context of vcpu0.
+	*/
+	int32_t cb_prelaunch_init_kuseg;
+	int32_t cb_prelaunch_init_kseg;
+
+	/* 
+	 * Foll. Args will be passed to callbacks. Make sure if it is an address 
+	 * of any variable then it must be kseg0 address. 
+	*/
+	uint32_t kuseg_prelaunch_init_args;
+	uint32_t kseg_prelaunch_init_args;
+
+	/* 
+	 * Foll. is the pointer to the boot1_info, this address will be passed to
+	 * the all applications (in scratch-0) 
+	*/
+	int32_t xlr_lib_boot1_info;
+
+
+	/* Pointer To Base of WakeupInfo Structure. */
+	int32_t xlr_wakeup_info;
+
+	uint32_t default_ebase;
+	int32_t global_wakeup_mask;
+	int32_t loader_lock;
+
+	int32_t flush_icache_all;
+	int32_t flush_dcache_all;
+	int32_t flush_tlb_all;
+	int32_t xlr_setup_tlb;
+	int32_t xlr_hard_vcpu_id;
+	int32_t trap_init;
+	int32_t cpu_init;
+	int32_t print;
+};
+
+struct xlr_kuseg_mem_blk
+{
+	uint64_t phys;
+	uint64_t size;
+	struct xlr_kuseg_mem_blk *next;
+};
+
+extern void xlr_start_app(void);
+extern void wakeup_cpu(uint32_t cpu, unsigned long fn, unsigned long args);
+extern void xlr_send_stop_ipi(unsigned long mask);
+extern void xlr_lib_load_init(struct xlr_lib_load_import *lib_load);
+extern void xlr_lib_launch_init(struct xlr_lib_launch_import *lib_launch);
+extern void xlr_lib_prelaunch_init_kseg(unsigned long data);
+extern int xlr_lib_stop_vcpu(unsigned long mask);
+extern void xlr_lib_load_exit(void);
+int setup_lib_env(void);
+extern int xlr_load_kuseg_app(struct xlr_lib_boot_file *file, uint32_t master,
+							  char *loadaddr, char *user_app_addr,
+							  uint32_t buddy, int override, int nargs,
+							  char **cmdline_args, char **, int);
+extern void xlr_lib_get_avail_mem(unsigned long *kuseg_mem,
+								  unsigned long *kseg_mem);
+extern uint64_t xlr_lib_reserve_shmem(uint64_t);
+extern void xlr_lib_intr_handler(int ipi);
+extern void xlr_lib_entry(unsigned long args);
+extern void reset_printk_base_lock(void);
+extern int xlr_lib_launch_userapp(int cpu);
+extern int xlr_load_kseg_app(struct xlr_lib_boot_file *file, uint32_t master,
+							 int argc, char *argv[], int nmiload);
+extern int xlr_lib_load_userapp(uint32_t cpu, int nargs, char *cmdline_args[],
+								char *env_args[]);
+
+
+#endif
diff --git a/arch/mips/rmi/ptr/loader/xlr_lib_launch.c b/arch/mips/rmi/ptr/loader/xlr_lib_launch.c
new file mode 100644
index 0000000..bc75dbc
--- /dev/null
+++ b/arch/mips/rmi/ptr/loader/xlr_lib_launch.c
@@ -0,0 +1,523 @@
+/***********************************************************************
+Copyright 2003-2006 Raza Microelectronics, Inc.(RMI). All rights
+reserved.
+Use of this software shall be governed in all respects by the terms and
+conditions of the RMI software license agreement ("SLA") that was
+accepted by the user as a condition to opening the attached files.
+Without limiting the foregoing, use of this software in source and
+binary code forms, with or without modification, and subject to all
+other SLA terms and conditions, is permitted.
+Any transfer or redistribution of the source code, with or without
+modification, IS PROHIBITED,unless specifically allowed by the SLA.
+Any transfer or redistribution of the binary code, with or without
+modification, is permitted, provided that the following condition is
+met:
+Redistributions in binary form must reproduce the above copyright
+notice, the SLA, this list of conditions and the following disclaimer
+in the documentation and/or other materials provided with the
+distribution:
+THIS SOFTWARE IS PROVIDED BY Raza Microelectronics, Inc. `AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL RMI OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#RMI_3#***********************************/
+
+/*This is the main bootloader lib file.*/
+
+#include "xlr_boot_lib.h"
+
+#define Message(a,b...) print("\n[%s]-[%d] "a"\n",__func__,__LINE__,##b)
+
+static volatile struct xlr_vcpu_wakeup_info *xlr_vcpu_wakeup_info = NULL;
+static struct xlr_lib_psb_info *xlr_lib_boot1_info=NULL;
+static struct xlr_lib_launch_import *xlr_lib_launch = NULL;
+static void xlr_park(unsigned long args);
+static void xlr_lib_kuseg_helper(void);
+static void load_scratch_info(void);
+static void disable_ie(void);
+static void enable_ie(void);
+
+struct exception_region {
+	unsigned long data[1024];
+};
+
+static volatile int xlr_wakeup_ipi[XLR_NUM_CPU] = {0};
+static int loading_kseg0 = 0;
+
+#ifndef __STR
+#define __STR(x) #x
+#endif
+#ifndef STR
+#define STR(x) __STR(x)
+#endif
+
+
+static void (*flush_icache_all)(void);
+static void (*flush_dcache_all) (void);
+static void (*flush_tlb_all) (void);
+static void (*xlr_setup_tlb)(unsigned int, uint64_t , int );
+static int (*xlr_hard_vcpu_id)(void);
+static void (*trap_init) (void);
+static void (*cpu_init) (void);
+static int (*print)(const char *fmt, ...);
+static void (*cb_prelaunch_init_kseg)(unsigned long);
+
+static __inline__ void load_scratch_info(void)
+{
+	struct xlr_vcpu_wakeup_info *t = (struct xlr_vcpu_wakeup_info *)
+									xlr_vcpu_wakeup_info + xlr_hard_vcpu_id();
+	__asm__ volatile(
+		".set push\n"
+		".set noreorder\n"
+		".set mips64\n"	
+		"dmtc0 %0, $22, 0\n"	/* move (&boot1_info) into $22    */
+		"dmtc0 %1, $22, 1\n"	/* move (master cpu + buddy mask) into $22 */
+		"dmtc0 %2, $22, 3\n"
+		"dmtc0 %3, $22, 4\n"
+		"dmtc0 %4, $22, 5\n"
+		"dmtc0 %5, $22, 6\n"
+		"dmtc0 %6, $22, 7\n"
+	    ".set pop\n"
+		:: "r"(xlr_lib_boot1_info),"r"(xlr_lib_boot1_info->userapp_cpu_map),
+		"r"(1),"r"((int)t->argc),
+		"r"((unsigned long)t->argv),
+		"r"((unsigned long)t->envs),"r"(xlr_vcpu_wakeup_info)
+	);
+}
+
+static __inline__ void disable_ie()
+{
+	__asm__ volatile (
+		  ".set push     \n"
+		  ".set noreorder        \n"
+		  ".set noat             \n"
+		  "mfc0 $1, $12          \n"
+		  "ori  $1, 0x1          \n"
+		  "xori $1, 0x1          \n"
+		  "mtc0 $1, $12          \n"
+		  "nop                   \n"
+		  "nop                   \n"
+		  "nop                   \n"
+		  "nop                   \n"
+		  ".set pop              \n"
+	);
+}
+
+static __inline__ void enable_ie()
+{
+	__asm__ volatile (
+		  ".set push             \n"
+		  ".set noreorder        \n"
+		  ".set noat             \n"
+		  "mfc0 $1, $12          \n"
+		  "ori  $1, 0x1f         \n"
+		  "xori $1, 0x1e         \n"
+		  "mtc0 $1, $12          \n"
+		  "nop                   \n"
+		  "nop                   \n"
+		  "nop                   \n"
+		  "nop                   \n"
+		  ".set pop              \n"
+	);
+}
+void xlr_lib_entry(unsigned long args)
+{
+	struct xlr_vcpu_wakeup_info *t = (struct xlr_vcpu_wakeup_info *)
+					xlr_vcpu_wakeup_info + xlr_hard_vcpu_id();
+
+	register unsigned long __sp asm("$29") = t->sp;
+	register unsigned long __gp asm("$28") = t->gp;
+	register unsigned long __func asm("$25") = (unsigned long) xlr_park;
+	register unsigned long __a0 asm("$4") = 0;
+	
+	__asm__ volatile(
+		".set noreorder\n"
+		"jr $25\n"
+		"nop\n"
+		".set reorder\n"
+		:
+		: "r"(__sp), "r"(__a0), "r"(__gp),"r"(__func)
+	);
+}
+
+void set_argv_envs(struct xlr_vcpu_wakeup_info *t)
+{
+	int i=0;
+	unsigned char *tmp=NULL;
+	tmp = t->argv_buf;
+
+	for(i=0;i<t->argc;i++){
+		t->argv[i] = (int32_t)(long)tmp;
+		while(*tmp != ' ' && *tmp)
+			tmp++;
+		tmp++;
+	}
+	for(;i<XLR_MAX_ARGV;i++)
+		t->argv[i] = (int32_t)NULL;
+
+	tmp = t->env_buf;
+	for(i=0;i<t->env;i++){
+		t->envs[i] = (int32_t)(long)tmp;
+		while(*tmp != ' ' && *tmp)
+			tmp++;
+		tmp++;
+	}
+	for(;i<XLR_MAX_ARGV;i++)
+		t->envs[i] = (int32_t)NULL;
+}
+
+void xlr_start_app(void)
+{
+	int vcpu = xlr_hard_vcpu_id();
+	struct xlr_vcpu_wakeup_info *t = (struct xlr_vcpu_wakeup_info *)
+						(xlr_vcpu_wakeup_info + vcpu);
+	unsigned long ebase = xlr_lib_launch->default_ebase;
+	volatile int i=0;
+	uint32_t *gbl_wakeup_mask = (uint32_t *)(long)t->global_wakeup_mask;
+
+	t->cpu_status = CPU_RUNNING;
+	lib_spin_lock(((lib_spinlock_t *)(long)t->loader_lock));
+	*gbl_wakeup_mask = (*gbl_wakeup_mask) & ~(1UL<<vcpu);
+	lib_spin_unlock(((lib_spinlock_t *)(long)t->loader_lock));
+	
+	flush_tlb_all();
+
+	/*Reset EBASE Value*/
+	xlr_fill_cop0_reg_32($15,1,ebase&0x3ffff000);
+	/*Reset STATUS Register*/
+	xlr_fill_cop0_reg_32($12,0,0x50000000);
+
+	/*Call Helper And Setup TLB For KUSEG Apps*/
+	if (t->mode == KUSEG_APP) {
+		xlr_lib_kuseg_helper();
+	}
+
+	/*Set argv and envs with proper pointers.*/
+	set_argv_envs(t);
+
+	flush_icache_all();
+	flush_dcache_all();
+
+	/*
+	 * 1)Setup $22,0 with boot1_info structure. This may required for 
+	 *   initialization process.
+	 * 2)Setup $22,7 with "xlr_vcpu_wakeup_info" base address. Rmios 
+	 * lib startup routine will find the appropriate offset based on this.
+	*/
+	load_scratch_info();
+	if (!loading_kseg0)
+		for(i=0;i<10000000;i++);
+	if (t->kseg_master == 1) {
+		/*
+		 * Case where we are launching kseg application on vcpu not running 
+		 * loader, Setup all the required arguments accordingly.
+		*/
+        __asm__ volatile(
+			".set noreorder\n"
+			".set mips64\n"
+			"move $4, %0\n"
+			"move $5, %1\n"
+			"move $6, %2\n"
+			"move $7, %3\n"
+			"nop\n"
+			"nop\n"
+			".set reorder\n"
+			::"r"(t->argc),"r"(t->argv),"r"(t->envs),
+			"r"((unsigned long)xlr_lib_boot1_info)
+			:"$4","$5","$6","$7"
+        );
+	} else {
+		__asm__ volatile(
+			".set noreorder\n"
+			".set mips64\n"
+			"move $4, %0\n"
+			"move $5, %0\n"
+			"nop\n"
+			"nop\n"
+			".set reorder\n"
+			::"r"(t->args)
+			:"$4","$5"
+		);
+	}
+
+	/*Setup gp,sp and jump to the function.*/
+	register unsigned long __func asm("$25") = t->func;
+    register unsigned long __sp asm("$29") = t->sp;
+    register unsigned long __gp asm("$28") = t->gp;
+    register unsigned long __rentry_func asm("$31") = (unsigned long) t->reentry_function;
+
+	__asm__ volatile(
+		".set noreorder\n\t"
+		"jr $25\n\t"
+		"nop\n\t"
+		".set  reorder\n\t"
+		:
+		:"r"(__sp), "r"(__gp), "r"(__func), "r"(__rentry_func)
+	);
+	/*We will never come back here.*/
+}
+
+void xlr_send_stop_ipi(unsigned long mask)
+{
+	int ipi;
+	volatile unsigned long k = 0;
+	int pid, tid;
+	xlr_lib_phnx_reg_t *mmio = xlr_lib_phnx_io_mmio(PHOENIX_IO_PIC_OFFSET);
+	volatile struct xlr_vcpu_wakeup_info *m = xlr_vcpu_wakeup_info;
+	volatile struct xlr_vcpu_wakeup_info *t,*b;
+	volatile int i,j;
+	uint32_t *gbl_wakeup_mask = (uint32_t *)(long)m->global_wakeup_mask;
+
+	lib_spin_lock(((lib_spinlock_t *)(long)m->loader_lock));
+
+	/*Send stop ipi to all requested vcpus.*/
+	for(i=0; i<XLR_NUM_CPU; i++){
+		if(mask & (1U<<i)){
+			pid = i >> 2;
+			tid = i % 4;
+			ipi = (pid << 20) | (tid << 16) | IRQ_STOP_CPU_IPI ;
+			xlr_lib_phnx_write_reg(mmio, PIC_IPI, ipi);
+		}
+	}
+
+	/*wait for some time.... let cpus come back to park mode.*/
+	for(k=0;k<100000000UL;k++);	
+
+	/*
+	 * Okk... we sent ipi to all vcpus .. now check if we can free memory or 
+	 * not.
+	*/
+	for(i=0;i<XLR_NUM_CPU;i++){
+		if(!(mask & (1U<<i)))
+			continue;
+		t = xlr_vcpu_wakeup_info+i;
+		if(t->cpu_status == CPU_RUNNING)
+			continue;
+		for(j=0;j<XLR_NUM_CPU;j++){
+			if(j==i)
+				continue;
+			if((t->buddy_mask & (1U<<j)) || (t->master_mask & (1U<<j))){
+				b = (struct xlr_vcpu_wakeup_info *)xlr_vcpu_wakeup_info + j;
+				if(b->cpu_status == CPU_RUNNING)
+					goto next;
+			}
+		}
+
+		/*
+		 * Vcpu/s set in which i is the member is/are in STOPPED state.
+		 * Check if any of the vcpu/s has any pending wakeup request. If so we
+		 * cant free the memory or otherwise mark this vcpu set as free 
+		 * "xlr_boot_lib_load" will do the rest of the job for us.
+		*/
+		if(*(gbl_wakeup_mask) & (t->buddy_mask | t->master_mask)){
+			/*cant help.. some one has issued wakeup on one of the buddy cpus.*/
+			goto next;
+		}
+		/*Bingo!!!! Mark this vcpu set as CAN_FREE*/
+		for(j=0;j<XLR_NUM_CPU;j++){
+			if((t->buddy_mask|t->master_mask) & (1U<<j)){
+				b = (struct xlr_vcpu_wakeup_info *)xlr_vcpu_wakeup_info + j;
+				b->can_free_memory = 1;
+			}
+		}
+		mask = mask & ~(t->buddy_mask | t->master_mask);
+next:;
+	}
+	lib_spin_unlock(((lib_spinlock_t *)(long)m->loader_lock));
+	/*handover to xlr_boot_lib_load.c ...*/
+}
+
+void wakeup_cpu(unsigned int cpu, unsigned long fn, unsigned long args)
+{
+	volatile struct xlr_vcpu_wakeup_info *t;
+	int ipi;
+	int pid = cpu >> 2;
+	int tid = cpu % 4;
+	uint32_t *gbl_wakeup_mask;
+	xlr_lib_phnx_reg_t *mmio = xlr_lib_phnx_io_mmio(PHOENIX_IO_PIC_OFFSET);
+
+	t = xlr_vcpu_wakeup_info + cpu;
+	gbl_wakeup_mask = (uint32_t *)(long)t->global_wakeup_mask;
+	lib_spin_lock(((lib_spinlock_t *)(long)t->loader_lock));
+	t = xlr_vcpu_wakeup_info + cpu;
+	t->func = fn;
+	t->args = args;
+	ipi = (pid << 20) | (tid << 16) | IRQ_WAKEUP_CPU_IPI ;
+	*gbl_wakeup_mask = (*gbl_wakeup_mask) | (1UL<<cpu);
+	xlr_lib_phnx_write_reg(mmio, PIC_IPI, ipi);
+	lib_spin_unlock(((lib_spinlock_t *)(long)t->loader_lock));
+}
+
+static void xlr_lib_kuseg_helper(void)
+{
+	struct xlr_vcpu_wakeup_info *t = (struct xlr_vcpu_wakeup_info *)
+					xlr_vcpu_wakeup_info + xlr_hard_vcpu_id();
+	int i;
+	xlr_fill_cop0_reg_32($6,0,0);
+
+	if (!t->map_count) {
+		print("cpu_%d: No TLB mappings?\n", xlr_hard_vcpu_id());
+		return;
+	}
+	for (i = 0; i < t->map_count; i++){
+		xlr_setup_tlb(t->map[i].virt, t->map[i].phys, t->map[i].page_size);
+	}
+	trap_init();
+}
+
+void xlr_lib_launch_init(struct xlr_lib_launch_import *lib_launch)
+{
+	int i=0;
+	struct xlr_vcpu_wakeup_info *t = NULL;
+	xlr_lib_launch = lib_launch;
+	
+	Message("");
+	/*Initialize All Callbacks*/
+	flush_icache_all = (void(*)(void))(long)(xlr_lib_launch->flush_icache_all);
+	flush_dcache_all = (void(*)(void))(long)(xlr_lib_launch->flush_dcache_all);
+	flush_tlb_all = (void(*)(void))(long)(xlr_lib_launch->flush_tlb_all);
+	xlr_setup_tlb = (void (*)(unsigned int, uint64_t , int ))
+							(long)(xlr_lib_launch->xlr_setup_tlb);
+	xlr_hard_vcpu_id = (int(*)(void))(long)(xlr_lib_launch->xlr_hard_vcpu_id);
+	trap_init = (void(*)(void))(long)(xlr_lib_launch->trap_init);
+	cpu_init = (void(*)(void))(long)(xlr_lib_launch->cpu_init);
+	print = (int (*)(const char *fmt, ...))(long)(xlr_lib_launch->print);
+	cb_prelaunch_init_kseg = (void (*)(unsigned long))
+								(long)(xlr_lib_launch->cb_prelaunch_init_kseg);
+	
+	t = (struct xlr_vcpu_wakeup_info *)(long)lib_launch->xlr_wakeup_info;
+	xlr_vcpu_wakeup_info = t;
+	xlr_lib_boot1_info = (struct xlr_lib_psb_info *)(long)
+							lib_launch->xlr_lib_boot1_info;
+	Message("");
+	memset((void *)(long)lib_launch->loader_lock,0,sizeof(t->loader_lock));
+	Message("");
+
+	for(i=0;i<XLR_NUM_CPU;i++){
+		t->sp = lib_launch->sp[i];
+		t->gp = lib_launch->gp[i];
+		t->global_wakeup_mask = lib_launch->global_wakeup_mask;
+		t->loader_lock = lib_launch->loader_lock;
+		t->can_free_memory = 0;
+		t++;
+	}
+	Message("");
+}
+
+int xlr_lib_launch_userapp(int cpu)
+{
+	struct xlr_vcpu_wakeup_info *t = (struct xlr_vcpu_wakeup_info *)
+					xlr_vcpu_wakeup_info + cpu;
+
+	loading_kseg0 = 1;
+
+	if(cb_prelaunch_init_kseg)
+		cb_prelaunch_init_kseg(xlr_lib_launch->kseg_prelaunch_init_args);
+
+	t->cpu_status = CPU_RUNNING;
+
+	if(cpu != xlr_hard_vcpu_id()){
+		t->kseg_master = 1;
+		wakeup_cpu(cpu,t->func,t->args);
+		return 0;
+	}
+	flush_tlb_all();
+	load_scratch_info();
+	__asm__ volatile(
+		".set noreorder\n"
+		".set mips64\n"
+		"move $29, %0\n"
+		"move $28, %1\n"
+		"nop\n"
+		".set reorder\n"
+		::"r"(t->sp), "r"(t->gp)
+	);
+	((void (*)(int, char *, char *, uint64_t))(long)t->func)
+			(t->argc,(char *)t->argv,(char *)t->envs,
+			 (unsigned long)xlr_lib_boot1_info);
+	/*We will never come back here.*/
+	return 0;	
+}
+
+void xlr_lib_intr_handler(int irq)
+{
+	int cpu = xlr_hard_vcpu_id();
+	switch(irq){
+		case IRQ_WAKEUP_CPU_IPI:
+			xlr_wakeup_ipi[cpu] = 1;
+			break;
+		case IRQ_STOP_CPU_IPI:
+			reset_printk_base_lock();
+			__asm__ __volatile__(
+				"mtc0 %0, $14\n"
+				"nop\n"
+				"eret\n"
+				"nop\n"
+				::"r"(xlr_lib_entry)
+			);
+			break;
+		default:
+			print("Lib Doesnt support Interupt %d\n",irq);
+			break;
+	}
+	return;
+}
+
+static void xlr_park(unsigned long args)
+{
+	uint64_t new_mask = (1ULL<<IRQ_WAKEUP_CPU_IPI) | (1ULL<<IRQ_STOP_CPU_IPI);
+	uint64_t old_mask = (1ULL<<4);
+	volatile struct xlr_vcpu_wakeup_info *t;
+	int cpu = xlr_hard_vcpu_id();
+
+	/*
+	 * Dont Read EIMR and or with newmask, we may come here from rmios 
+	 * applications, which may have all interrupts enabled, here we have to 
+	 * reset all interrupts except wakeup/stop ipi.
+	*/
+	new_mask = new_mask | old_mask;
+	cpu_init();
+
+	/* Set Proper Status Reg Value. */
+	t = (struct xlr_vcpu_wakeup_info *)xlr_vcpu_wakeup_info + cpu;
+	t->reentry_function = (int32_t)(long)xlr_lib_entry;
+	t->reentry_args = cpu;
+	disable_ie();
+	t->cpu_status = CPU_STOPPED;
+	trap_init();
+
+	xlr_fill_cop0_reg_64($9,7,new_mask);
+
+	enable_ie();
+
+	while(!xlr_wakeup_ipi[cpu])
+		__asm__ volatile(
+			".set noreorder\n"
+			"wait\n"
+			".set reorder\n"	
+		);
+
+	disable_ie();
+
+	xlr_wakeup_ipi[cpu] = 0;
+	xlr_fill_cop0_reg_64($9,6,0xfffffffffffffffcULL);
+	xlr_fill_cop0_reg_64($9,7,old_mask);
+	xlr_start_app();
+}
+
+void reset_printk_base_lock(void)
+{
+	struct xlr_vcpu_wakeup_info *t = (struct xlr_vcpu_wakeup_info *)
+					xlr_vcpu_wakeup_info + xlr_hard_vcpu_id();
+	uint32_t *ptr = (uint32_t *)(long)t->printk_lock;
+
+	if(*ptr == (xlr_hard_vcpu_id()+1))
+		*ptr = 0;
+}
diff --git a/arch/mips/rmi/ptr/loader/xlr_lib_platform.h b/arch/mips/rmi/ptr/loader/xlr_lib_platform.h
new file mode 100644
index 0000000..fcacd1c
--- /dev/null
+++ b/arch/mips/rmi/ptr/loader/xlr_lib_platform.h
@@ -0,0 +1,92 @@
+/***********************************************************************
+Copyright 2003-2006 Raza Microelectronics, Inc. (RMI). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Raza Microelectronics, Inc. ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL RMI OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#RMI_2#**********************************/
+
+#ifndef __XLR_LIB_PLATEFORM_H
+#define __XLR_LIB_PLATEFORM_H
+
+#include <linux/types.h>
+#include <asm/rmi/phnx_loader.h> 
+#include <asm/rmi/sim.h>
+#include <asm/rmi/iomap.h>
+#include <asm/rmi/pic.h>
+#include <linux/kernel.h>
+#include <linux/string.h>
+#include "xlr_boot_lib.h"
+
+#define xlr_lib_psb_mem_map psb_mem_map
+#define xlr_lib_psb_info psb_info
+#define xlr_lib_phnx_io_mmio(offset) phoenix_io_mmio(offset)
+#define xlr_lib_phnx_write_reg(base,offset,val) phoenix_write_reg(base,offset,val)
+#define xlr_lib_phnx_read_reg(base,offset) phoenix_read_reg(base,offset)
+
+#define xlr_fill_cop0_reg_32(reg,sel,value) 	\
+				__asm__ __volatile__(\
+					".set\tpush\n\t"\
+					".set mips32\n\t"\
+					"mtc0\t%0,"STR(reg)", %1\n\t"\
+					".set\tpop"\
+					:: "r" (value), "i" (sel) );
+
+#define xlr_fill_cop0_reg_64(source,sel,val) __write_64bit_c0_register(source,sel,val)
+
+struct xlr_lib_boot_file{
+        char *name;
+        unsigned char *start;
+        int size;
+        unsigned int max_size;
+        int valid;
+};
+
+static __inline__ void lib_spin_lock(lib_spinlock_t *lock)
+{
+	unsigned int tmp;
+
+	__asm__ __volatile__(
+	    ".set\tnoreorder\t\t\t# spin_lock\n"
+	    "1:\tll\t%1, %2\n\t"
+	    "bnez\t%1, 1b\n\t"
+	    " li\t%1, 1\n\t"
+	    "sc\t%1, %0\n\t"
+	    "beqz\t%1, 1b\n\t"
+	    " sync\n\t"
+	    ".set\treorder"
+	    : "=m" (lock->lock), "=&r" (tmp)
+	    : "m" (lock->lock)
+	    : "memory");
+}
+
+static __inline__ void lib_spin_unlock(lib_spinlock_t *lock)
+{
+	__asm__ __volatile__(
+	    ".set\tnoreorder\t\t\t# spin_unlock\n\t"
+	    "sync\n\t"
+	    "sw\t$0, %0\n\t"
+	    ".set\treorder"
+	    : "=m" (lock->lock)
+	    : "m" (lock->lock)
+	    : "memory");
+}
+
+#endif
diff --git a/arch/mips/rmi/ptr/loader/xlr_rmios_stackframe.h b/arch/mips/rmi/ptr/loader/xlr_rmios_stackframe.h
new file mode 100644
index 0000000..8e263e9
--- /dev/null
+++ b/arch/mips/rmi/ptr/loader/xlr_rmios_stackframe.h
@@ -0,0 +1,191 @@
+/***********************************************************************
+Copyright 2003-2006 Raza Microelectronics, Inc.(RMI). All rights
+reserved.
+Use of this software shall be governed in all respects by the terms and
+conditions of the RMI software license agreement ("SLA") that was
+accepted by the user as a condition to opening the attached files.
+Without limiting the foregoing, use of this software in source and
+binary code forms, with or without modification, and subject to all
+other SLA terms and conditions, is permitted.
+Any transfer or redistribution of the source code, with or without
+modification, IS PROHIBITED.
+Any transfer or redistribution of the binary code, with or without
+modification, is permitted, provided that the following condition is
+met:
+Redistributions in binary form must reproduce the above copyright
+notice, the SLA, this list of conditions and the following disclaimer
+in the documentation and/or other materials provided with the
+distribution:
+THIS SOFTWARE IS PROVIDED BY Raza Microelectronics, Inc. `AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL RMI OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#RMI_3#***********************************/
+#ifndef __RMIOS_STACKFRAME_H__
+#define __RMIOS_STACKFRAME_H__
+
+#include <asm/regdef.h>
+#include <asm/asm.h>
+#include <asm/types.h>
+
+
+#define COP_0_STATUS      $12
+#define COP_0_CAUSE       $13
+#define COP_0_EPC         $14
+
+#define STACK_OFF_R0  		    48
+#define STACK_OFF_R1     	    56
+#define STACK_OFF_R2     	    64
+#define STACK_OFF_R3     	    72
+#define STACK_OFF_R4     	    80
+#define STACK_OFF_R5     	    88
+#define STACK_OFF_R6     	    96
+#define STACK_OFF_R7     	   104
+#define STACK_OFF_R8     	   112
+#define STACK_OFF_R9     	   120
+#define STACK_OFF_R10    	   128
+#define STACK_OFF_R11    	   136
+#define STACK_OFF_R12    	   144
+#define STACK_OFF_R13    	   152
+#define STACK_OFF_R14    	   160
+#define STACK_OFF_R15    	   168
+#define STACK_OFF_R16    	   176
+#define STACK_OFF_R17    	   184
+#define STACK_OFF_R18    	   192
+#define STACK_OFF_R19    	   200
+#define STACK_OFF_R20    	   208
+#define STACK_OFF_R21    	   216
+#define STACK_OFF_R22    	   224
+#define STACK_OFF_R23    	   232
+#define STACK_OFF_R24    	   240
+#define STACK_OFF_R25    	   248
+#define STACK_OFF_R26    	   256
+#define STACK_OFF_R27    	   264
+#define STACK_OFF_R28    	   272
+#define STACK_OFF_R29    	   280
+#define STACK_OFF_R30    	   288
+#define STACK_OFF_R31    	   296
+
+#define STACK_OFF_STATUS 	   304
+#define STACK_OFF_HI     	   312
+#define STACK_OFF_LO     	   320
+
+#define STACK_OFF_BVADDR 	   328
+#define STACK_OFF_CAUSE  	   336
+#define STACK_OFF_EPC    	   344
+
+#define K_STACK_SIZE   	           352
+
+//#define _PAGE_SIZE  0x1000
+
+#define save_stack_frame           \
+	.set push;                 \
+	.set noat;                 \
+	.set reorder;              \
+	move	k1, sp;            \
+        move	k0, sp;            \
+	dsubu	sp, k1, K_STACK_SIZE;     \
+	sd	k0, STACK_OFF_R29(sp);	  \
+        sd	$3, STACK_OFF_R3(sp);	  \
+	sd	$0, STACK_OFF_R0(sp);	  \
+	dmfc0	v1, COP_0_STATUS;         \
+	sd	$2, STACK_OFF_R2(sp);	  \
+	sd	v1, STACK_OFF_STATUS(sp); \
+	sd	$4, STACK_OFF_R4(sp);	  \
+	dmfc0	v1, COP_0_CAUSE;          \
+	sd	$5, STACK_OFF_R5(sp);	  \
+	sd	v1, STACK_OFF_CAUSE(sp);  \
+	sd	$6, STACK_OFF_R6(sp);	  \
+	dmfc0	v1, COP_0_EPC;            \
+	sd	$7, STACK_OFF_R7(sp);	  \
+	sd	v1, STACK_OFF_EPC(sp);	  \
+	sd	$25, STACK_OFF_R25(sp);   \
+	sd	$28, STACK_OFF_R28(sp);   \
+	sd	$31, STACK_OFF_R31(sp);   \
+	ori	$28, sp, 0x1fff;          \
+	xori	$28, 0x1fff;              \
+	sd	$1, STACK_OFF_R1(sp);	  \
+	mfhi	v1;		          \
+	sd	$8, STACK_OFF_R8(sp);	  \
+	sd	$9, STACK_OFF_R9(sp);	  \
+	sd	v1, STACK_OFF_HI(sp);	  \
+	mflo	v1;		          \
+	sd	$10,STACK_OFF_R10(sp);	  \
+	sd	$11, STACK_OFF_R11(sp);   \
+	sd	v1,  STACK_OFF_LO(sp);	  \
+	sd	$12, STACK_OFF_R12(sp);   \
+	sd	$13, STACK_OFF_R13(sp);   \
+	sd	$14, STACK_OFF_R14(sp);   \
+	sd	$15, STACK_OFF_R15(sp);   \
+	sd	$24, STACK_OFF_R24(sp);   \
+	sd	$16, STACK_OFF_R16(sp);   \
+	sd	$17, STACK_OFF_R17(sp);   \
+	sd	$18, STACK_OFF_R18(sp);   \
+	sd	$19, STACK_OFF_R19(sp);   \
+	sd	$20, STACK_OFF_R20(sp);   \
+	sd	$21, STACK_OFF_R21(sp);   \
+	sd	$22, STACK_OFF_R22(sp);   \
+	sd	$23, STACK_OFF_R23(sp);   \
+	sd	$30, STACK_OFF_R30(sp);   \
+.set pop;			   
+
+#define restore_stack_frame                  \
+	.set	push;		             \
+	.set    noat;                        \
+	.set	reorder;	             \
+	mfc0	t0, COP_0_STATUS;            \
+	ori	t0, 0x1f;	             \
+	xori	t0, 0x1f;	             \
+	mtc0	t0, COP_0_STATUS;            \
+	li	v1, 0xff00;	             \
+	and	t0, v1;		             \
+	ld	v0, STACK_OFF_STATUS(sp);    \
+	nor	v1, $0, v1;	             \
+	and	v0, v1;		             \
+	or	v0, t0;		             \
+	mtc0	v0, COP_0_STATUS;            \
+	ld	v1, STACK_OFF_EPC(sp);	     \
+	mtc0	v1, COP_0_EPC;	             \
+	ld	$31, STACK_OFF_R31(sp);      \
+	ld	$28, STACK_OFF_R28(sp);      \
+	ld	$25, STACK_OFF_R25(sp);      \
+	ld	$7,  STACK_OFF_R7(sp);	     \
+	ld	$6,  STACK_OFF_R6(sp);	     \
+	ld	$5,  STACK_OFF_R5(sp);	     \
+	ld	$4,  STACK_OFF_R4(sp);	     \
+	ld	$3,  STACK_OFF_R3(sp);	     \
+	ld	$2,  STACK_OFF_R2(sp);	     \
+	ld	$1,  STACK_OFF_R1(sp);       \
+	ld	$24, STACK_OFF_LO(sp);	     \
+	ld	$8, STACK_OFF_R8(sp);	     \
+	ld	$9, STACK_OFF_R9(sp);	     \
+	mtlo	$24;		             \
+	ld	$24, STACK_OFF_HI(sp);	     \
+	ld	$10,STACK_OFF_R10(sp);	     \
+	ld	$11, STACK_OFF_R11(sp);      \
+	mthi	$24;		             \
+	ld	$12, STACK_OFF_R12(sp);      \
+	ld	$13, STACK_OFF_R13(sp);      \
+	ld	$14, STACK_OFF_R14(sp);      \
+	ld	$15, STACK_OFF_R15(sp);      \
+	ld	$24, STACK_OFF_R24(sp);      \
+	ld	$16, STACK_OFF_R16(sp);      \
+	ld	$17, STACK_OFF_R17(sp);      \
+	ld	$18, STACK_OFF_R18(sp);      \
+	ld	$19, STACK_OFF_R19(sp);      \
+	ld	$20, STACK_OFF_R20(sp);      \
+	ld	$21, STACK_OFF_R21(sp);      \
+	ld	$22, STACK_OFF_R22(sp);      \
+	ld	$23, STACK_OFF_R23(sp);      \
+	ld	$30, STACK_OFF_R30(sp);      \
+	ld	sp,  STACK_OFF_R29(sp);      \
+.set pop;			   
+
+#endif
-- 
1.6.0.4

