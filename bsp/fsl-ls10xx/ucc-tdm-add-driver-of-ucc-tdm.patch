From 36c390e3c837957d8bc054ed2362ff36b84821d2 Mon Sep 17 00:00:00 2001
From: Zhao Qiang <B45475@freescale.com>
Date: Wed, 16 Apr 2014 13:19:04 +0800
Subject: [PATCH 123/255] ucc-tdm: add driver of ucc-tdm

add driver of ucc-tdm for powerpc and ls1.

Signed-off-by: Zhao Qiang <B45475@freescale.com>
[Kevin: Original patch taken from
LS1021A-SDK-V1.1-ARM-SOURCE-20140815-yocto.iso]
Signed-off-by: Kevin Hao <kexin.hao@windriver.com>
---
 arch/arm/include/asm/io.h        |   11 +
 drivers/Kconfig                  |    1 +
 drivers/Makefile                 |    1 +
 drivers/qe/Kconfig               |    2 +-
 drivers/qe/qe.c                  |    6 +
 drivers/qe/ucc.c                 |  771 ++++++++++++++++++++++++
 drivers/qe/ucc_fast.c            |  105 ++--
 drivers/tdm/Kconfig              |   18 +
 drivers/tdm/Makefile             |    5 +
 drivers/tdm/device/Kconfig       |   13 +
 drivers/tdm/device/Makefile      |    5 +
 drivers/tdm/device/fsl_ucc_tdm.c | 1212 ++++++++++++++++++++++++++++++++++++++
 drivers/tdm/device/fsl_ucc_tdm.h |  173 ++++++
 drivers/tdm/tdm-core.c           | 1188 +++++++++++++++++++++++++++++++++++++
 include/linux/fsl/immap_qe.h     |    5 +-
 include/linux/fsl/qe.h           |   10 +
 include/linux/fsl/ucc.h          |    5 +
 include/linux/fsl/ucc_fast.h     |    6 +
 include/linux/idr.h              |    3 +
 include/linux/mod_devicetable.h  |   11 +
 include/linux/tdm.h              |  355 +++++++++++
 lib/idr.c                        |   49 ++
 22 files changed, 3916 insertions(+), 39 deletions(-)
 create mode 100644 drivers/tdm/Kconfig
 create mode 100644 drivers/tdm/Makefile
 create mode 100644 drivers/tdm/device/Kconfig
 create mode 100644 drivers/tdm/device/Makefile
 create mode 100644 drivers/tdm/device/fsl_ucc_tdm.c
 create mode 100644 drivers/tdm/device/fsl_ucc_tdm.h
 create mode 100644 drivers/tdm/tdm-core.c
 create mode 100644 include/linux/tdm.h

diff --git a/arch/arm/include/asm/io.h b/arch/arm/include/asm/io.h
index c003455..efec19f 100644
--- a/arch/arm/include/asm/io.h
+++ b/arch/arm/include/asm/io.h
@@ -375,6 +375,17 @@ extern void _memset_io(volatile void __iomem *, int, size_t);
 #define setbits8(_addr, _v) iowrite8(ioread8(_addr) |  (_v), (_addr))
 #define clrbits8(_addr, _v) iowrite8(ioread8(_addr) & ~(_v), (_addr))
 
+#define clrsetbits(type, addr, clear, set) \
+	iowrite##type((ioread##type(addr) & ~(clear)) | (set), (addr))
+
+#define clrsetbits_be32(addr, clear, set) clrsetbits(32be, addr, clear, set)
+#define clrsetbits_le32(addr, clear, set) clrsetbits(32le, addr, clear, set)
+
+#define clrsetbits_be16(addr, clear, set) clrsetbits(16be, addr, clear, set)
+#define clrsetbits_le16(addr, clear, set) clrsetbits(16le, addr, clear, set)
+
+#define clrsetbits_8(addr, clear, set) clrsetbits(8, addr, clear, set)
+
 extern void __iomem *ioport_map(unsigned long port, unsigned int nr);
 extern void ioport_unmap(void __iomem *addr);
 #endif
diff --git a/drivers/Kconfig b/drivers/Kconfig
index f31d988..f07df24 100644
--- a/drivers/Kconfig
+++ b/drivers/Kconfig
@@ -172,4 +172,5 @@ source "drivers/phy/Kconfig"
 
 source "drivers/powercap/Kconfig"
 
+source "drivers/tdm/Kconfig"
 endmenu
diff --git a/drivers/Makefile b/drivers/Makefile
index 26896a2..200f039 100644
--- a/drivers/Makefile
+++ b/drivers/Makefile
@@ -120,6 +120,7 @@ obj-$(CONFIG_INFINIBAND)	+= infiniband/
 obj-$(CONFIG_SGI_SN)		+= sn/
 obj-y				+= firmware/
 obj-$(CONFIG_CRYPTO)		+= crypto/
+obj-$(CONFIG_TDM)               += tdm/
 obj-$(CONFIG_SUPERH)		+= sh/
 obj-$(CONFIG_ARCH_SHMOBILE_LEGACY)	+= sh/
 ifndef CONFIG_ARCH_USES_GETTIMEOFFSET
diff --git a/drivers/qe/Kconfig b/drivers/qe/Kconfig
index 5035b58..f2c2c68 100644
--- a/drivers/qe/Kconfig
+++ b/drivers/qe/Kconfig
@@ -30,7 +30,7 @@ config UCC_SLOW
 
 config UCC_FAST
 	bool
-	default y if UCC_GETH
+	default y if UCC_GETH || FSL_UCC_TDM
 	help
 	  This option provides qe_lib support to UCC fast
 	  protocols: HDLC, Ethernet, ATM, transparent
diff --git a/drivers/qe/qe.c b/drivers/qe/qe.c
index 8459c4f..6bb255e 100644
--- a/drivers/qe/qe.c
+++ b/drivers/qe/qe.c
@@ -240,6 +240,12 @@ enum qe_clock qe_clock_source(const char *source)
 	if (strcasecmp(source, "none") == 0)
 		return QE_CLK_NONE;
 
+	if (strcasecmp(source, "tsync_pin") == 0)
+		return QE_TSYNC_PIN;
+
+	if (strcasecmp(source, "rsync_pin") == 0)
+		return QE_RSYNC_PIN;
+
 	if (strncasecmp(source, "brg", 3) == 0) {
 		i = simple_strtoul(source + 3, NULL, 10);
 		if ((i >= 1) && (i <= 16))
diff --git a/drivers/qe/ucc.c b/drivers/qe/ucc.c
index 36a206f..99fcd86 100644
--- a/drivers/qe/ucc.c
+++ b/drivers/qe/ucc.c
@@ -210,3 +210,774 @@ int ucc_set_qe_mux_rxtx(unsigned int ucc_num, enum qe_clock clock,
 
 	return 0;
 }
+
+/* tdm_num: TDM A-H port num is 0-7 */
+int ucc_set_tdm_rxtx_clk(u32 tdm_num, enum qe_clock clock,
+		enum comm_dir mode)
+{
+	u32 clock_bits, shift;
+	struct qe_mux *qe_mux_reg;
+	 __be32 __iomem *cmxs1cr;
+
+	clock_bits = 0;
+	qe_mux_reg = &qe_immr->qmx;
+
+	if ((tdm_num > 7 || tdm_num < 0))
+		return -EINVAL;
+
+	/* The communications direction must be RX or TX */
+	if (!((mode == COMM_DIR_RX) || (mode == COMM_DIR_TX)))
+		return -EINVAL;
+
+	switch (mode) {
+	case COMM_DIR_RX:
+		switch (tdm_num) {
+		case 0:
+			switch (clock) {
+			case QE_BRG3:
+					clock_bits = 1;
+					break;
+			case QE_BRG4:
+					clock_bits = 2;
+					break;
+			case QE_CLK1:
+					clock_bits = 4;
+					break;
+			case QE_CLK2:
+					clock_bits = 5;
+					break;
+			case QE_CLK3:
+					clock_bits = 6;
+					break;
+			case QE_CLK8:
+					clock_bits = 7;
+					break;
+			default:
+					break;
+			}
+			shift = 28;
+			break;
+		case 1:
+			switch (clock) {
+			case QE_BRG3:
+					clock_bits = 1;
+					break;
+			case QE_BRG4:
+					clock_bits = 2;
+					break;
+			case QE_CLK1:
+					clock_bits = 4;
+					break;
+			case QE_CLK2:
+					clock_bits = 5;
+					break;
+			case QE_CLK5:
+					clock_bits = 6;
+					break;
+			case QE_CLK10:
+					clock_bits = 7;
+					break;
+			default:
+					break;
+			}
+			shift = 24;
+			break;
+		case 2:
+			switch (clock) {
+			case QE_BRG3:
+					clock_bits = 1;
+					break;
+			case QE_BRG4:
+					clock_bits = 2;
+					break;
+			case QE_CLK1:
+					clock_bits = 4;
+					break;
+			case QE_CLK2:
+					clock_bits = 5;
+					break;
+			case QE_CLK7:
+					clock_bits = 6;
+					break;
+			case QE_CLK12:
+					clock_bits = 7;
+					break;
+			default:
+					break;
+			}
+			shift = 20;
+			break;
+		case 3:
+			switch (clock) {
+			case QE_BRG3:
+					clock_bits = 1;
+					break;
+			case QE_BRG4:
+					clock_bits = 2;
+					break;
+			case QE_CLK1:
+					clock_bits = 4;
+					break;
+			case QE_CLK2:
+					clock_bits = 5;
+					break;
+			case QE_CLK9:
+					clock_bits = 6;
+					break;
+			case QE_CLK14:
+					clock_bits = 7;
+					break;
+			default:
+					break;
+			}
+			shift = 16;
+			break;
+		case 4:
+			switch (clock) {
+			case QE_BRG12:
+					clock_bits = 1;
+					break;
+			case QE_BRG13:
+					clock_bits = 2;
+					break;
+			case QE_CLK23:
+					clock_bits = 4;
+					break;
+			case QE_CLK24:
+					clock_bits = 5;
+					break;
+			case QE_CLK11:
+					clock_bits = 6;
+					break;
+			case QE_CLK16:
+					clock_bits = 7;
+					break;
+			default:
+					break;
+			}
+			shift = 28;
+			break;
+		case 5:
+			switch (clock) {
+			case QE_BRG12:
+					clock_bits = 1;
+					break;
+			case QE_BRG13:
+					clock_bits = 2;
+					break;
+			case QE_CLK23:
+					clock_bits = 4;
+					break;
+			case QE_CLK24:
+					clock_bits = 5;
+					break;
+			case QE_CLK13:
+					clock_bits = 6;
+					break;
+			case QE_CLK18:
+					clock_bits = 7;
+					break;
+			default:
+					break;
+			}
+			shift = 24;
+			break;
+		case 6:
+			switch (clock) {
+			case QE_BRG12:
+					clock_bits = 1;
+					break;
+			case QE_BRG13:
+					clock_bits = 2;
+					break;
+			case QE_CLK23:
+					clock_bits = 4;
+					break;
+			case QE_CLK24:
+					clock_bits = 5;
+					break;
+			case QE_CLK15:
+					clock_bits = 6;
+					break;
+			case QE_CLK20:
+					clock_bits = 7;
+					break;
+			default:
+					break;
+			}
+			shift = 20;
+			break;
+		case 7:
+			switch (clock) {
+			case QE_BRG12:
+					clock_bits = 1;
+					break;
+			case QE_BRG13:
+					clock_bits = 2;
+					break;
+			case QE_CLK23:
+					clock_bits = 4;
+					break;
+			case QE_CLK24:
+					clock_bits = 5;
+					break;
+			case QE_CLK17:
+					clock_bits = 6;
+					break;
+			case QE_CLK22:
+					clock_bits = 7;
+					break;
+			default:
+					break;
+			}
+			shift = 16;
+			break;
+		default:
+				break;
+		}
+		break;
+	case COMM_DIR_TX:
+		switch (tdm_num) {
+		case 0:
+			switch (clock) {
+			case QE_BRG3:
+					clock_bits = 1;
+					break;
+			case QE_BRG4:
+					clock_bits = 2;
+					break;
+			case QE_CLK1:
+					clock_bits = 4;
+					break;
+			case QE_CLK2:
+					clock_bits = 5;
+					break;
+			case QE_CLK4:
+					clock_bits = 6;
+					break;
+			case QE_CLK9:
+					clock_bits = 7;
+					break;
+			default:
+					break;
+			}
+			shift = 12;
+			break;
+		case 1:
+			switch (clock) {
+			case QE_BRG3:
+					clock_bits = 1;
+					break;
+			case QE_BRG4:
+					clock_bits = 2;
+					break;
+			case QE_CLK1:
+					clock_bits = 4;
+					break;
+			case QE_CLK2:
+					clock_bits = 5;
+					break;
+			case QE_CLK6:
+					clock_bits = 6;
+					break;
+			case QE_CLK11:
+					clock_bits = 7;
+					break;
+			default:
+					break;
+			}
+			shift = 8;
+			break;
+		case 2:
+			switch (clock) {
+			case QE_BRG3:
+					clock_bits = 1;
+					break;
+			case QE_BRG4:
+					clock_bits = 2;
+					break;
+			case QE_CLK1:
+					clock_bits = 4;
+					break;
+			case QE_CLK2:
+					clock_bits = 5;
+					break;
+			case QE_CLK8:
+					clock_bits = 6;
+					break;
+			case QE_CLK13:
+					clock_bits = 7;
+					break;
+			default:
+					break;
+			}
+			shift = 4;
+			break;
+		case 3:
+			switch (clock) {
+			case QE_BRG3:
+					clock_bits = 1;
+					break;
+			case QE_BRG4:
+					clock_bits = 2;
+					break;
+			case QE_CLK1:
+					clock_bits = 4;
+					break;
+			case QE_CLK2:
+					clock_bits = 5;
+					break;
+			case QE_CLK10:
+					clock_bits = 6;
+					break;
+			case QE_CLK15:
+					clock_bits = 7;
+					break;
+			default:
+					break;
+			}
+			shift = 0;
+			break;
+		case 4:
+			switch (clock) {
+			case QE_BRG12:
+					clock_bits = 1;
+					break;
+			case QE_BRG13:
+					clock_bits = 2;
+					break;
+			case QE_CLK23:
+					clock_bits = 4;
+					break;
+			case QE_CLK24:
+					clock_bits = 5;
+					break;
+			case QE_CLK12:
+					clock_bits = 6;
+					break;
+			case QE_CLK17:
+					clock_bits = 7;
+					break;
+			default:
+					break;
+			}
+			shift = 12;
+			break;
+		case 5:
+			switch (clock) {
+			case QE_BRG12:
+					clock_bits = 1;
+					break;
+			case QE_BRG13:
+					clock_bits = 2;
+					break;
+			case QE_CLK23:
+					clock_bits = 4;
+					break;
+			case QE_CLK24:
+					clock_bits = 5;
+					break;
+			case QE_CLK14:
+					clock_bits = 6;
+					break;
+			case QE_CLK19:
+					clock_bits = 7;
+					break;
+			default:
+					break;
+			}
+			shift = 8;
+			break;
+		case 6:
+			switch (clock) {
+			case QE_BRG12:
+					clock_bits = 1;
+					break;
+			case QE_BRG13:
+					clock_bits = 2;
+					break;
+			case QE_CLK23:
+					clock_bits = 4;
+					break;
+			case QE_CLK24:
+					clock_bits = 5;
+					break;
+			case QE_CLK16:
+					clock_bits = 6;
+					break;
+			case QE_CLK21:
+					clock_bits = 7;
+					break;
+			default:
+					break;
+			}
+			shift = 4;
+			break;
+		case 7:
+			switch (clock) {
+			case QE_BRG12:
+					clock_bits = 1;
+					break;
+			case QE_BRG13:
+					clock_bits = 2;
+					break;
+			case QE_CLK23:
+					clock_bits = 4;
+					break;
+			case QE_CLK24:
+					clock_bits = 5;
+					break;
+			case QE_CLK18:
+					clock_bits = 6;
+					break;
+			case QE_CLK3:
+					clock_bits = 7;
+					break;
+			default:
+					break;
+			}
+			shift = 0;
+			break;
+		default:
+			break;
+		}
+		break;
+	default:
+		break;
+	}
+
+	if (!clock_bits)
+		return -ENOENT;
+
+	cmxs1cr = (tdm_num < 4) ? (&qe_mux_reg->cmxsi1cr_l) :
+				  (&qe_mux_reg->cmxsi1cr_h);
+
+	clrsetbits_be32(cmxs1cr, QE_CMXUCR_TX_CLK_SRC_MASK << shift,
+		   clock_bits << shift);
+
+	return 0;
+}
+
+
+int ucc_set_tdm_rxtx_sync(u32 tdm_num, enum qe_clock clock,
+		enum comm_dir mode)
+{
+	u32 shift, clock_bits;
+	struct qe_mux *qe_mux_reg;
+	int source;
+
+	source = 0;
+	shift = 0;
+	qe_mux_reg = &qe_immr->qmx;
+
+	if ((tdm_num > 7 || tdm_num < 0))
+		return -EINVAL;
+
+	/* The communications direction must be RX or TX */
+	if (!((mode == COMM_DIR_RX) || (mode == COMM_DIR_TX)))
+		return -EINVAL;
+
+	switch (mode) {
+	case COMM_DIR_RX:
+		switch (tdm_num) {
+		case 0:
+			switch (clock) {
+			case QE_RSYNC_PIN:
+					source = 0;
+					break;
+			case QE_BRG9:
+					source = 1;
+					break;
+			case QE_BRG10:
+					source = 2;
+					break;
+			default:
+					source = -1;
+					break;
+			}
+			shift = 30;
+			break;
+		case 1:
+			switch (clock) {
+			case QE_RSYNC_PIN:
+					source = 0;
+					break;
+			case QE_BRG9:
+					source = 1;
+					break;
+			case QE_BRG10:
+					source = 2;
+					break;
+			default:
+					source = -1;
+					break;
+			}
+			shift = 28;
+			break;
+		case 2:
+			switch (clock) {
+			case QE_RSYNC_PIN:
+					source = 0;
+					break;
+			case QE_BRG9:
+					source = 1;
+					break;
+			case QE_BRG11:
+					source = 2;
+					break;
+			default:
+					source = -1;
+					break;
+			}
+			shift = 26;
+			break;
+		case 3:
+			switch (clock) {
+			case QE_RSYNC_PIN:
+					source = 0;
+					break;
+			case QE_BRG9:
+					source = 1;
+					break;
+			case QE_BRG11:
+					source = 2;
+					break;
+			default:
+					source = -1;
+					break;
+			}
+			shift = 24;
+			break;
+		case 4:
+			switch (clock) {
+			case QE_RSYNC_PIN:
+					source = 0;
+					break;
+			case QE_BRG13:
+					source = 1;
+					break;
+			case QE_BRG14:
+					source = 2;
+					break;
+			default:
+					source = -1;
+					break;
+			}
+			shift = 22;
+			break;
+		case 5:
+			switch (clock) {
+			case QE_RSYNC_PIN:
+					source = 0;
+					break;
+			case QE_BRG13:
+					source = 1;
+					break;
+			case QE_BRG14:
+					source = 2;
+					break;
+			default:
+					source = -1;
+					break;
+			}
+			shift = 20;
+			break;
+		case 6:
+			switch (clock) {
+			case QE_RSYNC_PIN:
+					source = 0;
+					break;
+			case QE_BRG13:
+					source = 1;
+					break;
+			case QE_BRG15:
+					source = 2;
+					break;
+			default:
+					source = -1;
+					break;
+			}
+			shift = 18;
+			break;
+		case 7:
+			switch (clock) {
+			case QE_RSYNC_PIN:
+					source = 0;
+					break;
+			case QE_BRG13:
+					source = 1;
+					break;
+			case QE_BRG15:
+					source = 2;
+					break;
+			default:
+					source = -1;
+					break;
+			}
+			shift = 16;
+			break;
+		default:
+			source = -1;
+			break;
+		}
+		break;
+	case COMM_DIR_TX:
+		switch (tdm_num) {
+		case 0:
+			switch (clock) {
+			case QE_TSYNC_PIN:
+					source = 0;
+					break;
+			case QE_BRG9:
+					source = 1;
+					break;
+			case QE_BRG10:
+					source = 2;
+					break;
+			default:
+					source = -1;
+					break;
+			}
+			shift = 14;
+			break;
+		case 1:
+			switch (clock) {
+			case QE_TSYNC_PIN:
+					source = 0;
+					break;
+			case QE_BRG9:
+					source = 1;
+					break;
+			case QE_BRG10:
+					source = 2;
+					break;
+			default:
+					source = -1;
+					break;
+			}
+			shift = 12;
+			break;
+		case 2:
+			switch (clock) {
+			case QE_TSYNC_PIN:
+					source = 0;
+					break;
+			case QE_BRG9:
+					source = 1;
+					break;
+			case QE_BRG11:
+					source = 2;
+					break;
+			default:
+					source = -1;
+					break;
+			}
+			shift = 10;
+			break;
+		case 3:
+			switch (clock) {
+			case QE_TSYNC_PIN:
+					source = 0;
+					break;
+			case QE_BRG9:
+					source = 1;
+					break;
+			case QE_BRG11:
+					source = 2;
+					break;
+			default:
+					source = -1;
+					break;
+			}
+			shift = 8;
+			break;
+		case 4:
+			switch (clock) {
+			case QE_TSYNC_PIN:
+					source = 0;
+					break;
+			case QE_BRG13:
+					source = 1;
+					break;
+			case QE_BRG14:
+					source = 2;
+					break;
+			default:
+					source = -1;
+					break;
+			}
+			shift = 6;
+			break;
+		case 5:
+			switch (clock) {
+			case QE_TSYNC_PIN:
+					source = 0;
+					break;
+			case QE_BRG13:
+					source = 1;
+					break;
+			case QE_BRG14:
+					source = 2;
+					break;
+			default:
+					source = -1;
+					break;
+			}
+			shift = 4;
+			break;
+		case 6:
+			switch (clock) {
+			case QE_TSYNC_PIN:
+					source = 0;
+					break;
+			case QE_BRG13:
+					source = 1;
+					break;
+			case QE_BRG15:
+					source = 2;
+					break;
+			default:
+					source = -1;
+					break;
+			}
+			shift = 2;
+			break;
+		case 7:
+			switch (clock) {
+			case QE_TSYNC_PIN:
+					source = 0;
+					break;
+			case QE_BRG13:
+					source = 1;
+					break;
+			case QE_BRG15:
+					source = 2;
+					break;
+			default:
+					source = -1;
+					break;
+			}
+			shift = 0;
+			break;
+
+		default:
+			source = -1;
+			break;
+		}
+		break;
+	default:
+		source = -1;
+		break;
+	}
+
+	if (source == -1)
+		return -ENOENT;
+
+	clock_bits = (u32) source;
+
+	clrsetbits_be32(&qe_mux_reg->cmxsi1syr,
+			QE_CMXUCR_TX_CLK_SRC_MASK << shift,
+			clock_bits << shift);
+
+	return 0;
+}
diff --git a/drivers/qe/ucc_fast.c b/drivers/qe/ucc_fast.c
index 223bb36..98a6434 100644
--- a/drivers/qe/ucc_fast.c
+++ b/drivers/qe/ucc_fast.c
@@ -33,41 +33,41 @@ void ucc_fast_dump_regs(struct ucc_fast_private * uccf)
 	printk(KERN_INFO "Base address: 0x%p\n", uccf->uf_regs);
 
 	printk(KERN_INFO "gumr  : addr=0x%p, val=0x%08x\n",
-		  &uccf->uf_regs->gumr, in_be32(&uccf->uf_regs->gumr));
+		  &uccf->uf_regs->gumr, ioread32be(&uccf->uf_regs->gumr));
 	printk(KERN_INFO "upsmr : addr=0x%p, val=0x%08x\n",
-		  &uccf->uf_regs->upsmr, in_be32(&uccf->uf_regs->upsmr));
+		  &uccf->uf_regs->upsmr, ioread32be(&uccf->uf_regs->upsmr));
 	printk(KERN_INFO "utodr : addr=0x%p, val=0x%04x\n",
-		  &uccf->uf_regs->utodr, in_be16(&uccf->uf_regs->utodr));
+		  &uccf->uf_regs->utodr, ioread16be(&uccf->uf_regs->utodr));
 	printk(KERN_INFO "udsr  : addr=0x%p, val=0x%04x\n",
-		  &uccf->uf_regs->udsr, in_be16(&uccf->uf_regs->udsr));
+		  &uccf->uf_regs->udsr, ioread16be(&uccf->uf_regs->udsr));
 	printk(KERN_INFO "ucce  : addr=0x%p, val=0x%08x\n",
-		  &uccf->uf_regs->ucce, in_be32(&uccf->uf_regs->ucce));
+		  &uccf->uf_regs->ucce, ioread32be(&uccf->uf_regs->ucce));
 	printk(KERN_INFO "uccm  : addr=0x%p, val=0x%08x\n",
-		  &uccf->uf_regs->uccm, in_be32(&uccf->uf_regs->uccm));
+		  &uccf->uf_regs->uccm, ioread32be(&uccf->uf_regs->uccm));
 	printk(KERN_INFO "uccs  : addr=0x%p, val=0x%02x\n",
-		  &uccf->uf_regs->uccs, in_8(&uccf->uf_regs->uccs));
+		  &uccf->uf_regs->uccs, ioread8(&uccf->uf_regs->uccs));
 	printk(KERN_INFO "urfb  : addr=0x%p, val=0x%08x\n",
-		  &uccf->uf_regs->urfb, in_be32(&uccf->uf_regs->urfb));
+		  &uccf->uf_regs->urfb, ioread32be(&uccf->uf_regs->urfb));
 	printk(KERN_INFO "urfs  : addr=0x%p, val=0x%04x\n",
-		  &uccf->uf_regs->urfs, in_be16(&uccf->uf_regs->urfs));
+		  &uccf->uf_regs->urfs, ioread16be(&uccf->uf_regs->urfs));
 	printk(KERN_INFO "urfet : addr=0x%p, val=0x%04x\n",
-		  &uccf->uf_regs->urfet, in_be16(&uccf->uf_regs->urfet));
+		  &uccf->uf_regs->urfet, ioread16be(&uccf->uf_regs->urfet));
 	printk(KERN_INFO "urfset: addr=0x%p, val=0x%04x\n",
-		  &uccf->uf_regs->urfset, in_be16(&uccf->uf_regs->urfset));
+		  &uccf->uf_regs->urfset, ioread16be(&uccf->uf_regs->urfset));
 	printk(KERN_INFO "utfb  : addr=0x%p, val=0x%08x\n",
-		  &uccf->uf_regs->utfb, in_be32(&uccf->uf_regs->utfb));
+		  &uccf->uf_regs->utfb, ioread32be(&uccf->uf_regs->utfb));
 	printk(KERN_INFO "utfs  : addr=0x%p, val=0x%04x\n",
-		  &uccf->uf_regs->utfs, in_be16(&uccf->uf_regs->utfs));
+		  &uccf->uf_regs->utfs, ioread16be(&uccf->uf_regs->utfs));
 	printk(KERN_INFO "utfet : addr=0x%p, val=0x%04x\n",
-		  &uccf->uf_regs->utfet, in_be16(&uccf->uf_regs->utfet));
+		  &uccf->uf_regs->utfet, ioread16be(&uccf->uf_regs->utfet));
 	printk(KERN_INFO "utftt : addr=0x%p, val=0x%04x\n",
-		  &uccf->uf_regs->utftt, in_be16(&uccf->uf_regs->utftt));
+		  &uccf->uf_regs->utftt, ioread16be(&uccf->uf_regs->utftt));
 	printk(KERN_INFO "utpt  : addr=0x%p, val=0x%04x\n",
-		  &uccf->uf_regs->utpt, in_be16(&uccf->uf_regs->utpt));
+		  &uccf->uf_regs->utpt, ioread16be(&uccf->uf_regs->utpt));
 	printk(KERN_INFO "urtry : addr=0x%p, val=0x%08x\n",
-		  &uccf->uf_regs->urtry, in_be32(&uccf->uf_regs->urtry));
+		  &uccf->uf_regs->urtry, ioread32be(&uccf->uf_regs->urtry));
 	printk(KERN_INFO "guemr : addr=0x%p, val=0x%02x\n",
-		  &uccf->uf_regs->guemr, in_8(&uccf->uf_regs->guemr));
+		  &uccf->uf_regs->guemr, ioread8(&uccf->uf_regs->guemr));
 }
 EXPORT_SYMBOL(ucc_fast_dump_regs);
 
@@ -89,7 +89,7 @@ EXPORT_SYMBOL(ucc_fast_get_qe_cr_subblock);
 
 void ucc_fast_transmit_on_demand(struct ucc_fast_private * uccf)
 {
-	out_be16(&uccf->uf_regs->utodr, UCC_FAST_TOD);
+	iowrite16be(UCC_FAST_TOD, &uccf->uf_regs->utodr);
 }
 EXPORT_SYMBOL(ucc_fast_transmit_on_demand);
 
@@ -101,7 +101,7 @@ void ucc_fast_enable(struct ucc_fast_private * uccf, enum comm_dir mode)
 	uf_regs = uccf->uf_regs;
 
 	/* Enable reception and/or transmission on this UCC. */
-	gumr = in_be32(&uf_regs->gumr);
+	gumr = ioread32be(&uf_regs->gumr);
 	if (mode & COMM_DIR_TX) {
 		gumr |= UCC_FAST_GUMR_ENT;
 		uccf->enabled_tx = 1;
@@ -110,7 +110,7 @@ void ucc_fast_enable(struct ucc_fast_private * uccf, enum comm_dir mode)
 		gumr |= UCC_FAST_GUMR_ENR;
 		uccf->enabled_rx = 1;
 	}
-	out_be32(&uf_regs->gumr, gumr);
+	iowrite32be(gumr, &uf_regs->gumr);
 }
 EXPORT_SYMBOL(ucc_fast_enable);
 
@@ -122,7 +122,7 @@ void ucc_fast_disable(struct ucc_fast_private * uccf, enum comm_dir mode)
 	uf_regs = uccf->uf_regs;
 
 	/* Disable reception and/or transmission on this UCC. */
-	gumr = in_be32(&uf_regs->gumr);
+	gumr = ioread32be(&uf_regs->gumr);
 	if (mode & COMM_DIR_TX) {
 		gumr &= ~UCC_FAST_GUMR_ENT;
 		uccf->enabled_tx = 0;
@@ -131,7 +131,7 @@ void ucc_fast_disable(struct ucc_fast_private * uccf, enum comm_dir mode)
 		gumr &= ~UCC_FAST_GUMR_ENR;
 		uccf->enabled_rx = 0;
 	}
-	out_be32(&uf_regs->gumr, gumr);
+	iowrite32be(gumr, &uf_regs->gumr);
 }
 EXPORT_SYMBOL(ucc_fast_disable);
 
@@ -263,7 +263,8 @@ int ucc_fast_init(struct ucc_fast_info * uf_info, struct ucc_fast_private ** ucc
 	gumr |= uf_info->tenc;
 	gumr |= uf_info->tcrc;
 	gumr |= uf_info->mode;
-	out_be32(&uf_regs->gumr, gumr);
+	gumr |= uf_info->diag;
+	iowrite32be(gumr, &uf_regs->gumr);
 
 	/* Allocate memory for Tx Virtual Fifo */
 	uccf->ucc_fast_tx_virtual_fifo_base_offset =
@@ -290,15 +291,15 @@ int ucc_fast_init(struct ucc_fast_info * uf_info, struct ucc_fast_private ** ucc
 	}
 
 	/* Set Virtual Fifo registers */
-	out_be16(&uf_regs->urfs, uf_info->urfs);
-	out_be16(&uf_regs->urfet, uf_info->urfet);
-	out_be16(&uf_regs->urfset, uf_info->urfset);
-	out_be16(&uf_regs->utfs, uf_info->utfs);
-	out_be16(&uf_regs->utfet, uf_info->utfet);
-	out_be16(&uf_regs->utftt, uf_info->utftt);
+	iowrite16be(uf_info->urfs, &uf_regs->urfs);
+	iowrite16be(uf_info->urfet, &uf_regs->urfet);
+	iowrite16be(uf_info->urfset, &uf_regs->urfset);
+	iowrite16be(uf_info->utfs, &uf_regs->utfs);
+	iowrite16be(uf_info->utfet, &uf_regs->utfet);
+	iowrite16be(uf_info->utftt, &uf_regs->utftt);
 	/* utfb, urfb are offsets from MURAM base */
-	out_be32(&uf_regs->utfb, uccf->ucc_fast_tx_virtual_fifo_base_offset);
-	out_be32(&uf_regs->urfb, uccf->ucc_fast_rx_virtual_fifo_base_offset);
+	iowrite32be(uccf->ucc_fast_tx_virtual_fifo_base_offset, &uf_regs->utfb);
+	iowrite32be(uccf->ucc_fast_rx_virtual_fifo_base_offset, &uf_regs->urfb);
 
 	/* Mux clocking */
 	/* Grant Support */
@@ -327,17 +328,53 @@ int ucc_fast_init(struct ucc_fast_info * uf_info, struct ucc_fast_private ** ucc
 			ucc_fast_free(uccf);
 			return -EINVAL;
 		}
+	} else {
+		/* tdm Rx clock routing */
+		if ((uf_info->rx_clock != QE_CLK_NONE) &&
+			ucc_set_tdm_rxtx_clk(uf_info->tdm_num,
+				uf_info->rx_clock, COMM_DIR_RX)) {
+			pr_err("%s: illegal value for RX clock", __func__);
+			ucc_fast_free(uccf);
+			return -EINVAL;
+		}
+
+		/* tdm Tx clock routing */
+		if ((uf_info->tx_clock != QE_CLK_NONE) &&
+			ucc_set_tdm_rxtx_clk(uf_info->tdm_num,
+				uf_info->tx_clock, COMM_DIR_TX)) {
+			pr_err("%s:illegal value for TX clock", __func__);
+			ucc_fast_free(uccf);
+			return -EINVAL;
+		}
+
+		/* tdm Rx sync clock routing */
+		if ((uf_info->rx_sync != QE_CLK_NONE) &&
+			ucc_set_tdm_rxtx_sync(uf_info->tdm_num,
+				uf_info->rx_sync, COMM_DIR_RX)) {
+			pr_err("%s:illegal value for TX clock", __func__);
+			ucc_fast_free(uccf);
+			return -EINVAL;
+		}
+
+		/* tdm Tx sync clock routing */
+		if ((uf_info->tx_sync != QE_CLK_NONE) &&
+			ucc_set_tdm_rxtx_sync(uf_info->tdm_num,
+				uf_info->tx_sync, COMM_DIR_TX)) {
+			pr_err("%s:illegal value for TX clock", __func__);
+			ucc_fast_free(uccf);
+			return -EINVAL;
+		}
 	}
 
 	/* Set interrupt mask register at UCC level. */
-	out_be32(&uf_regs->uccm, uf_info->uccm_mask);
+	iowrite32be(uf_info->uccm_mask, &uf_regs->uccm);
 
 	/* First, clear anything pending at UCC level,
 	 * otherwise, old garbage may come through
 	 * as soon as the dam is opened. */
 
 	/* Writing '1' clears */
-	out_be32(&uf_regs->ucce, 0xffffffff);
+	iowrite32be(0xffffffff, &uf_regs->ucce);
 
 	*uccf_ret = uccf;
 	return 0;
diff --git a/drivers/tdm/Kconfig b/drivers/tdm/Kconfig
new file mode 100644
index 0000000..b0c218d
--- /dev/null
+++ b/drivers/tdm/Kconfig
@@ -0,0 +1,18 @@
+#
+# TDM subsystem configuration
+#
+
+menuconfig TDM
+	tristate "TDM support"
+	---help---
+	  More information is contained in the directory <file:Documentation/tdm/>,
+	  especially in the file called "summary" there.
+	  If you want TDM support, you should say Y here and also to the
+	  specific driver for your bus adapter(s) below.
+
+	  This TDM support can also be built as a module.  If so, the module
+	  will be called tdm-core.
+
+if TDM
+source drivers/tdm/device/Kconfig
+endif # TDM
diff --git a/drivers/tdm/Makefile b/drivers/tdm/Makefile
new file mode 100644
index 0000000..a605b3d
--- /dev/null
+++ b/drivers/tdm/Makefile
@@ -0,0 +1,5 @@
+#
+# Makefile for the TDM core.
+#
+
+obj-$(CONFIG_TDM)		+= tdm-core.o device/
diff --git a/drivers/tdm/device/Kconfig b/drivers/tdm/device/Kconfig
new file mode 100644
index 0000000..1e23fab
--- /dev/null
+++ b/drivers/tdm/device/Kconfig
@@ -0,0 +1,13 @@
+#
+# TDM device configuration
+#
+
+menu "TDM Device support"
+
+config FSL_UCC_TDM
+       tristate "UCC TDM driver for Freescale QE engine"
+       depends on FSL_SOC || CONFIG_QE
+       ---help---
+         This is a driver for Freescale QE UCC working with TDM interface.
+
+endmenu
diff --git a/drivers/tdm/device/Makefile b/drivers/tdm/device/Makefile
new file mode 100644
index 0000000..4521d20
--- /dev/null
+++ b/drivers/tdm/device/Makefile
@@ -0,0 +1,5 @@
+#
+# Makefile for the TDM device drivers.
+#
+
+obj-$(CONFIG_FSL_UCC_TDM)	+= fsl_ucc_tdm.o
diff --git a/drivers/tdm/device/fsl_ucc_tdm.c b/drivers/tdm/device/fsl_ucc_tdm.c
new file mode 100644
index 0000000..7599aa2
--- /dev/null
+++ b/drivers/tdm/device/fsl_ucc_tdm.c
@@ -0,0 +1,1212 @@
+/*
+ * Freescale QUICC Engine TDM Device Driver
+ *
+ * Copyright 2011-2012 Freescale Semiconductor, Inc.
+ *
+ * Author:	Haiying Wang	<Haiying.Wang@freescale.com>
+ *		Kai Jiang	<Kai.Jiang@freescale.com>
+ *
+ * This program is free software; you can redistribute  it and/or modify it
+ * under  the terms of  the GNU General  Public License as published by the
+ * Free Software Foundation;  either version 2 of the  License, or (at your
+ * option) any later version.
+ *
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the  GNU General Public License along
+ * with this program; if not, write  to the Free Software Foundation, Inc.,
+ * 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ * This driver adds support for TDM devices via Freescale's QUICC Engine.
+ */
+
+#include <linux/delay.h>
+#include <linux/dma-mapping.h>
+#include <linux/fsl/qe.h>
+#include <linux/init.h>
+#include <linux/interrupt.h>
+#include <linux/io.h>
+#include <linux/irq.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/of_irq.h>
+#include <linux/platform_device.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/spinlock.h>
+#include <linux/stddef.h>
+#include <linux/tdm.h>
+#include "fsl_ucc_tdm.h"
+
+#define DRV_DESC "Freescale QE UCC TDM Driver"
+#define DRV_NAME "ucc_tdm"
+
+#undef DEBUG
+
+static struct ucc_tdm_info utdm_primary_info = {
+	.uf_info = {
+		.tsa = 0,
+		.cdp = 0,
+		.cds = 1,
+		.ctsp = 0,
+		.ctss = 1,
+		.revd = 0,
+		.urfs = 256,
+		.utfs = 256,
+		.urfet = 128,
+		.urfset = 192,
+		.utfet = 128,
+		.utftt = 0x40,
+		.ufpt = 256,
+		.ttx_trx = UCC_FAST_GUMR_TRANSPARENT_TTX_TRX_TRANSPARENT,
+		.tenc = UCC_FAST_TX_ENCODING_NRZ,
+		.renc = UCC_FAST_RX_ENCODING_NRZ,
+		.tcrc = UCC_FAST_16_BIT_CRC,
+		.synl = UCC_FAST_SYNC_LEN_AUTOMATIC,
+		.diag = UCC_FAST_DIAGNOSTIC_LOCAL_LOOP_BACK,
+	},
+
+	.si_info = {
+		.simr_rfsd = 1,		/* pq_mds_t1 card need 1 bit delay */
+		.simr_tfsd = 0,
+		.simr_crt = 0,
+		.simr_sl = 0,
+		.simr_ce = 1,
+		.simr_fe = 1,
+		.simr_gm = 0,
+	},
+};
+
+static struct ucc_tdm_info utdm_info[MAX_TDM_NUM];
+static int siram_init_flag;
+
+#ifdef DEBUG
+static void dump_siram(struct ucc_tdm_private *priv)
+{
+	int i;
+	u16 *siram = priv->siram;
+
+	dev_info(priv->dev, "Dump the SI RX RAM\n");
+	for (i = 0; i < priv->num_of_ts; i++) {
+		pr_info("%04x ", siram[priv->siram_entry_id * 32 + i]);
+		if ((i + 1) % 4)
+			pr_info("\n");
+	}
+
+	dev_info(priv->dev, "Dump the SI TX RAM\n");
+	for (i = 0; i < priv->num_of_ts; i++) {
+		pr_info("%04x ", siram[priv->siram_entry_id * 32 + 0x200 + i]);
+		if ((i + 1) % 4)
+			pr_info("\n");
+	}
+}
+
+static void mem_disp(u8 *addr, int size)
+{
+	void *i;
+	int size16_aling = (size >> 4) << 4;
+	int size4_aling = (size >> 2) << 2;
+	int not_align = 0;
+	if (size % 16)
+		not_align = 1;
+
+	for (i = addr;  i < addr + size16_aling; i += 16) {
+		u32 *i32 = i;
+
+		pr_info("0x%08p: %08x %08x %08x %08x\r\n",
+			i32, i32[0], i32[1], i32[2], i32[3]);
+	}
+
+	if (not_align == 1)
+		pr_info("0x%08p: ", i);
+	for (; i < addr + size4_aling; i += 4)
+		pr_info("%08x ", *((u32 *) (i)));
+	for (; i < addr + size; i++)
+		pr_info("%02x", *((u8 *) (i)));
+	if (not_align == 1)
+		pr_info("\r\n");
+}
+
+static void dump_ucc(struct ucc_tdm_private *priv)
+{
+	struct ucc_transparent_param *ucc_pram;
+	ucc_pram = priv->ucc_pram;
+
+	dev_info(priv->dev, "DumpiniCC %d Registers\n",
+			priv->ut_info->uf_info.ucc_num);
+	ucc_fast_dump_regs(priv->uccf);
+	dev_info(priv->dev, "Dumping UCC %d Parameter RAM\n",
+			priv->ut_info->uf_info.ucc_num);
+	dev_info(priv->dev, "rbase = 0x%x\n", ioread32be(&ucc_pram->rbase));
+	dev_info(priv->dev, "rbptr = 0x%x\n", ioread32be(&ucc_pram->rbptr));
+	dev_info(priv->dev, "mrblr = 0x%x\n", ioread16be(&ucc_pram->mrblr));
+	dev_info(priv->dev, "rbdlen = 0x%x\n", ioread16be(&ucc_pram->rbdlen));
+	dev_info(priv->dev, "rbdstat = 0x%x\n", ioread16be(&ucc_pram->rbdstat));
+	dev_info(priv->dev, "rstate = 0x%x\n", ioread32be(&ucc_pram->rstate));
+	dev_info(priv->dev, "rdptr = 0x%x\n", ioread32be(&ucc_pram->rdptr));
+	dev_info(priv->dev, "riptr = 0x%x\n", ioread16be(&ucc_pram->riptr));
+	dev_info(priv->dev, "tbase = 0x%x\n", ioread32be(&ucc_pram->tbase));
+	dev_info(priv->dev, "tbptr = 0x%x\n", ioread32be(&ucc_pram->tbptr));
+	dev_info(priv->dev, "tbdlen = 0x%x\n", ioread16be(&ucc_pram->tbdlen));
+	dev_info(priv->dev, "tbdstat = 0x%x\n", ioread16be(&ucc_pram->tbdstat));
+	dev_info(priv->dev, "tstate = 0x%x\n", ioread32be(&ucc_pram->tstate));
+	dev_info(priv->dev, "tdptr = 0x%x\n", ioread32be(&ucc_pram->tdptr));
+	dev_info(priv->dev, "tiptr = 0x%x\n", ioread16be(&ucc_pram->tiptr));
+	dev_info(priv->dev, "rcrc = 0x%x\n", ioread32be(&ucc_pram->rcrc));
+	dev_info(priv->dev, "tcrc = 0x%x\n", ioread32be(&ucc_pram->tcrc));
+	dev_info(priv->dev, "c_mask = 0x%x\n", ioread32be(&ucc_pram->c_mask));
+	dev_info(priv->dev, "c_pers = 0x%x\n", ioread32be(&ucc_pram->c_pres));
+	dev_info(priv->dev, "disfc = 0x%x\n", ioread16be(&ucc_pram->disfc));
+	dev_info(priv->dev, "crcec = 0x%x\n", ioread16be(&ucc_pram->crcec));
+}
+
+static void dump_bds(struct ucc_tdm_private *priv)
+{
+	int length;
+
+	if (priv->tx_bd) {
+		length = sizeof(struct qe_bd) * NUM_OF_BUF;
+		dev_info(priv->dev, " Dump tx BDs\n");
+		mem_disp((u8 *)priv->tx_bd, length);
+	}
+
+	if (priv->rx_bd) {
+		length = sizeof(struct qe_bd) * NUM_OF_BUF;
+		dev_info(priv->dev, " Dump rx BDs\n");
+		mem_disp((u8 *)priv->rx_bd, length);
+	}
+
+}
+
+static void dump_priv(struct ucc_tdm_private *priv)
+{
+	dev_info(priv->dev, "ut_info = 0x%x\n", (u32)priv->ut_info);
+	dev_info(priv->dev, "uccf = 0x%x\n", (u32)priv->uccf);
+	dev_info(priv->dev, "uf_regs = 0x%x\n", (u32)priv->uf_regs);
+	dev_info(priv->dev, "si_regs = 0x%x\n", (u32)priv->si_regs);
+	dev_info(priv->dev, "ucc_pram = 0x%x\n", (u32)priv->ucc_pram);
+	dev_info(priv->dev, "tdm_port = 0x%x\n", (u32)priv->tdm_port);
+	dev_info(priv->dev, "siram_entry_id = 0x%x\n", priv->siram_entry_id);
+	dev_info(priv->dev, "siram = 0x%x\n", (u32)priv->siram);
+	dev_info(priv->dev, "tdm_mode = 0x%x\n", (u32)priv->tdm_mode);
+	dev_info(priv->dev, "tdm_framer_type; = 0x%x\n",
+			(u32)priv->tdm_framer_type);
+	dev_info(priv->dev, "rx_buffer; = 0x%x\n", (u32)priv->rx_buffer);
+	dev_info(priv->dev, "tx_buffer; = 0x%x\n", (u32)priv->tx_buffer);
+	dev_info(priv->dev, "dma_rx_addr; = 0x%x\n", (u32)priv->dma_rx_addr);
+	dev_info(priv->dev, "dma_tx_addr; = 0x%x\n", (u32)priv->dma_tx_addr);
+	dev_info(priv->dev, "tx_bd; = 0x%x\n", (u32)priv->tx_bd);
+	dev_info(priv->dev, "rx_bd; = 0x%x\n", (u32)priv->rx_bd);
+	dev_info(priv->dev, "phase_rx = 0x%x\n", (u32)priv->phase_rx);
+	dev_info(priv->dev, "phase_tx = 0x%x\n", (u32)priv->phase_tx);
+	dev_info(priv->dev, "ucc_pram_offset = 0x%x\n", priv->ucc_pram_offset);
+	dev_info(priv->dev, "tx_bd_offset = 0x%x\n", priv->tx_bd_offset);
+	dev_info(priv->dev, "rx_bd_offset = 0x%x\n", priv->rx_bd_offset);
+
+}
+
+#endif /* DEBUG */
+
+static void init_si(struct ucc_tdm_private *priv)
+{
+	struct si1 __iomem *si_regs;
+	u16 __iomem *siram;
+	u16 siram_entry_valid;
+	u16 siram_entry_closed;
+	u16 ucc_num;
+	u8 csel;
+	u16 sixmr;
+	u16 tdm_port;
+	u32 siram_entry_id;
+	u32 mask;
+	int i;
+
+	si_regs = priv->si_regs;
+	siram = priv->siram;
+	ucc_num = priv->ut_info->uf_info.ucc_num;
+	tdm_port = priv->tdm_port;
+	siram_entry_id = priv->siram_entry_id;
+
+	/* set siram table */
+	csel = (ucc_num < 4) ? ucc_num + 9 : ucc_num - 3;
+
+	siram_entry_valid = SIR_CSEL(csel) | SIR_BYTE | SIR_CNT(0);
+	siram_entry_closed = SIR_IDLE | SIR_BYTE | SIR_CNT(0);
+
+	for (i = 0; i < priv->num_of_ts; i++) {
+		mask = 0x01 << i;
+
+		if (priv->tx_ts_mask & mask)
+			iowrite16be(siram_entry_valid,
+				    &siram[siram_entry_id * 32 + i]);
+		else
+			iowrite16be(siram_entry_closed,
+				    &siram[siram_entry_id * 32 + i]);
+
+		if (priv->rx_ts_mask & mask)
+			iowrite16be(siram_entry_valid,
+				    &siram[siram_entry_id * 32 + 0x200 +  i]);
+		else
+			iowrite16be(siram_entry_closed,
+				    &siram[siram_entry_id * 32 + 0x200 +  i]);
+	}
+
+	setbits16(&siram[(siram_entry_id * 32) + (priv->num_of_ts - 1)],
+			SIR_LAST);
+	setbits16(&siram[(siram_entry_id * 32) + 0x200 + (priv->num_of_ts - 1)],
+			SIR_LAST);
+
+	/* Set SIxMR register */
+	sixmr = SIMR_SAD(siram_entry_id);
+
+	sixmr &= ~SIMR_SDM_MASK;
+
+	if (priv->tdm_mode == TDM_INTERNAL_LOOPBACK)
+		sixmr |= SIMR_SDM_INTERNAL_LOOPBACK;
+	else
+		sixmr |= SIMR_SDM_NORMAL;
+
+	sixmr |= SIMR_RFSD(priv->ut_info->si_info.simr_rfsd) |
+			SIMR_TFSD(priv->ut_info->si_info.simr_tfsd);
+
+	if (priv->ut_info->si_info.simr_crt)
+		sixmr |= SIMR_CRT;
+	if (priv->ut_info->si_info.simr_sl)
+		sixmr |= SIMR_SL;
+	if (priv->ut_info->si_info.simr_ce)
+		sixmr |= SIMR_CE;
+	if (priv->ut_info->si_info.simr_fe)
+		sixmr |= SIMR_FE;
+	if (priv->ut_info->si_info.simr_gm)
+		sixmr |= SIMR_GM;
+
+	switch (tdm_port) {
+	case 0:
+		iowrite16be(sixmr, &si_regs->sixmr1[0]);
+		break;
+	case 1:
+		iowrite16be(sixmr, &si_regs->sixmr1[1]);
+		break;
+	case 2:
+		iowrite16be(sixmr, &si_regs->sixmr1[2]);
+		break;
+	case 3:
+		iowrite16be(sixmr, &si_regs->sixmr1[3]);
+		break;
+	default:
+		dev_err(priv->dev, "can not find tdm sixmr reg\n");
+		break;
+	}
+
+#ifdef DEBUG
+	dump_siram(priv);
+#endif
+
+}
+static int utdm_init(struct ucc_tdm_private *priv)
+{
+	struct ucc_tdm_info *ut_info;
+	struct ucc_fast_info *uf_info;
+	u32 cecr_subblock;
+	u32 bd_status;
+	int ret, i;
+	void *bd_buffer;
+	dma_addr_t bd_dma_addr;
+	u32 riptr;
+	u32 tiptr;
+
+	ut_info = priv->ut_info;
+	uf_info = &ut_info->uf_info;
+
+	if (priv->tdm_framer_type == TDM_FRAMER_T1)
+		priv->num_of_ts = 24;
+	if (priv->tdm_framer_type == TDM_FRAMER_E1)
+		priv->num_of_ts = 32;
+
+	uf_info->uccm_mask = (u32) (UCC_TRANS_UCCE_RXB << 16);
+
+	if (ucc_fast_init(uf_info, &priv->uccf)) {
+		dev_err(priv->dev, "Failed to init uccf.");
+		return -ENOMEM;
+	}
+
+	priv->uf_regs = priv->uccf->uf_regs;
+	ucc_fast_disable(priv->uccf, COMM_DIR_RX | COMM_DIR_TX);
+
+	/* Initialize SI */
+	init_si(priv);
+
+	/* Write to QE CECR, UCCx channel to Stop Transmission */
+	cecr_subblock = ucc_fast_get_qe_cr_subblock(uf_info->ucc_num);
+	ret = qe_issue_cmd(QE_STOP_TX, cecr_subblock,
+		(u8) QE_CR_PROTOCOL_UNSPECIFIED, 0);
+
+	/* Set UPSMR normal mode */
+	iowrite32be(0, &priv->uf_regs->upsmr);
+
+	priv->tx_bd = dma_alloc_coherent(priv->dev,
+			NUM_OF_BUF * MAX_RX_BUF_LENGTH,
+			&priv->dma_tx_bd, GFP_KERNEL);
+
+	if (!priv->tx_bd) {
+		dev_err(priv->dev, "Could not allocate buffer descriptors\n");
+		ret = -ENOMEM;
+		goto rxbd_alloc_error;
+	}
+
+	priv->rx_bd = dma_alloc_coherent(priv->dev,
+			NUM_OF_BUF * MAX_RX_BUF_LENGTH,
+			&priv->dma_rx_bd, GFP_KERNEL);
+	if (!priv->rx_bd) {
+		dev_err(priv->dev, "Could not allocate buffer descriptors\n");
+		ret = -ENOMEM;
+		goto txbd_alloc_error;
+	}
+
+	/* Alloc parameter ram for ucc transparent */
+	priv->ucc_pram_offset = qe_muram_alloc(sizeof(*priv->ucc_pram),
+				ALIGNMENT_OF_UCC_TRANS_PRAM);
+
+	if (IS_ERR_VALUE(priv->ucc_pram_offset)) {
+		dev_err(priv->dev, "Can not allocate MURAM for hdlc prameter.\n");
+		ret = -ENOMEM;
+		goto pram_alloc_error;
+	}
+
+	/* init parameter base */
+	cecr_subblock = ucc_fast_get_qe_cr_subblock(uf_info->ucc_num);
+	ret = qe_issue_cmd(QE_ASSIGN_PAGE_TO_DEVICE, cecr_subblock,
+			QE_CR_PROTOCOL_UNSPECIFIED, priv->ucc_pram_offset);
+
+	priv->ucc_pram = (struct ucc_transparent_param __iomem *)
+					qe_muram_addr(priv->ucc_pram_offset);
+
+	/* Zero out parameter ram */
+	memset_io(priv->ucc_pram, 0, sizeof(struct ucc_transparent_param));
+
+	/* Alloc riptr, tiptr */
+	riptr = qe_muram_alloc(32, 32);
+	if (IS_ERR_VALUE(riptr)) {
+		dev_err(priv->dev, "Cannot allocate MURAM mem for Receive");
+		pr_info(" internal temp data pointer\n");
+		ret = -ENOMEM;
+		goto riptr_alloc_error;
+	}
+
+	tiptr = qe_muram_alloc(32, 32);
+	if (IS_ERR_VALUE(tiptr)) {
+		dev_err(priv->dev, "Cannot allocate MURAM mem for transmit");
+		pr_info(" internal temp data pointer\n");
+		ret = -ENOMEM;
+		goto tiptr_alloc_error;
+	}
+
+	/* Set RIPTR, TIPTR */
+	iowrite16be((u16)riptr, &priv->ucc_pram->riptr);
+	iowrite16be((u16)tiptr, &priv->ucc_pram->tiptr);
+
+	/* Set MRBLR */
+	iowrite16be((u16)MAX_RX_BUF_LENGTH, &priv->ucc_pram->mrblr);
+
+	/* Set RBASE, TBASE */
+	iowrite32be((u32)priv->dma_rx_bd, &priv->ucc_pram->rbase);
+	iowrite32be((u32)priv->dma_tx_bd, &priv->ucc_pram->tbase);
+
+	/* Set RSTATE, TSTATE */
+	iowrite32be(0x30000000, &priv->ucc_pram->rstate);
+	iowrite32be(0x30000000, &priv->ucc_pram->tstate);
+
+	/* Set C_MASK, C_PRES for 16bit CRC */
+	iowrite32be(0x0000F0B8, &priv->ucc_pram->c_mask);
+	iowrite32be(0x0000FFFF, &priv->ucc_pram->c_pres);
+
+	iowrite16be(0, &priv->ucc_pram->res0);
+	for (i = 0; i < 4; i++)
+		iowrite32be(0x0, &priv->ucc_pram->res4[i]);
+
+	/* Get BD buffer */
+	bd_buffer = dma_alloc_coherent(priv->dev,
+			2 * NUM_OF_BUF * MAX_RX_BUF_LENGTH,
+			&bd_dma_addr, GFP_KERNEL);
+
+	if (!bd_buffer) {
+		dev_err(priv->dev, "Could not allocate buffer descriptors\n");
+		return -ENOMEM;
+	}
+
+	memset(bd_buffer, 0, 2 * NUM_OF_BUF * MAX_RX_BUF_LENGTH);
+
+	priv->rx_buffer = bd_buffer;
+	priv->tx_buffer = bd_buffer + NUM_OF_BUF * MAX_RX_BUF_LENGTH;
+
+	priv->dma_rx_addr = bd_dma_addr;
+	priv->dma_tx_addr = bd_dma_addr + NUM_OF_BUF * MAX_RX_BUF_LENGTH;
+
+	for (i = 0; i < NUM_OF_BUF; i++) {
+		if (i < (NUM_OF_BUF - 1))
+			bd_status = R_E | R_I | R_CM;
+		else
+			bd_status = R_E | R_I | R_W | R_CM;
+
+		iowrite32be(bd_status, (u32 *)(priv->rx_bd + i));
+		iowrite32be(priv->dma_rx_addr + i * MAX_RX_BUF_LENGTH,
+			    &priv->rx_bd[i].buf);
+
+		if (i < (NUM_OF_BUF - 1))
+			bd_status =  T_I;
+		else
+			bd_status =  T_I | T_W;
+
+		iowrite32be(bd_status, (u32 *)(priv->tx_bd + i));
+		iowrite32be(priv->dma_tx_addr + i * MAX_RX_BUF_LENGTH,
+			    &priv->tx_bd[i].buf);
+	}
+
+	priv->phase_rx = 0;
+	priv->phase_tx = 0;
+
+	return 0;
+
+tiptr_alloc_error:
+	qe_muram_free(riptr);
+riptr_alloc_error:
+	qe_muram_free(priv->ucc_pram_offset);
+pram_alloc_error:
+	dma_free_coherent(priv->dev,
+		NUM_OF_BUF * MAX_RX_BUF_LENGTH,
+		priv->rx_bd, priv->dma_rx_bd);
+txbd_alloc_error:
+	dma_free_coherent(priv->dev,
+		NUM_OF_BUF * MAX_RX_BUF_LENGTH,
+		priv->tx_bd, priv->dma_tx_bd);
+rxbd_alloc_error:
+	ucc_fast_free(priv->uccf);
+
+	return ret;
+}
+
+static int ucc_tdm_read(struct tdm_adapter *adap, u8 *tdm_buffer, u32 len)
+{
+
+	struct ucc_tdm_private *priv = tdm_get_adapdata(adap);
+	u8 phase_rx;
+	u32 byte_copy;
+	u8 *recv_buf;
+
+	wait_event_interruptible(priv->tdm_queue,
+			priv->tdm_queue_flag != false);
+	priv->tdm_queue_flag = false;
+
+	if (priv->phase_rx == 0)
+		phase_rx = NUM_OF_BUF - 1;
+	else
+		phase_rx = priv->phase_rx - 1;
+
+	recv_buf = priv->rx_buffer + phase_rx * MAX_RX_BUF_LENGTH;
+
+	if (len > MAX_RX_BUF_LENGTH)
+		byte_copy = MAX_RX_BUF_LENGTH;
+	else
+		byte_copy = len;
+
+	memcpy(tdm_buffer, recv_buf, byte_copy);
+
+	return byte_copy;
+
+}
+
+
+static int ucc_tdm_write(struct tdm_adapter *adap, u8 *write_buf,
+		 unsigned int len)
+{
+	struct ucc_tdm_private *priv = tdm_get_adapdata(adap);
+	struct qe_bd __iomem *bd;
+	u32 bd_stat_len;
+	u8 *tdm_send_buf;
+	u32 copy_len;
+	int i, ret;
+	u32 buf_num;
+
+	buf_num = len / MAX_RX_BUF_LENGTH;
+	if (len % MAX_RX_BUF_LENGTH)
+		buf_num += 1;
+
+	if (buf_num > NUM_OF_BUF)
+		return -EINVAL;
+
+	for (i = 0; i < buf_num; i++) {
+
+		if (priv->phase_tx == NUM_OF_BUF)
+			priv->phase_tx = 0;
+
+		bd = (priv->tx_bd + priv->phase_tx);
+		bd_stat_len = ioread32be((u32 __iomem *)bd);
+		tdm_send_buf = priv->tx_buffer +
+				priv->phase_tx * MAX_RX_BUF_LENGTH;
+
+		/* the last buf to copy */
+		if (i == (buf_num - 1))
+			copy_len = len - i * MAX_RX_BUF_LENGTH;
+		else
+			copy_len = MAX_RX_BUF_LENGTH;
+
+		ret = spin_event_timeout(((bd_stat_len =
+				ioread32be((u32 __iomem *)bd)) & T_R) != T_R ,
+				1000000, 500);
+		if (!ret) {
+			dev_err(priv->dev, "TDM write data error!\n");
+			return -EFAULT;
+		}
+
+		memset(tdm_send_buf, 0xff, MAX_RX_BUF_LENGTH);
+
+		memcpy(tdm_send_buf, write_buf, copy_len);
+
+		bd_stat_len &= ~(T_L | BD_LEN_MASK);
+		if (i == (buf_num - 1))
+			iowrite32be(bd_stat_len | T_R | T_L | T_I | copy_len,
+				    (u32 __iomem *)(bd));
+		else
+			iowrite32be(bd_stat_len | T_R | T_I | copy_len,
+				    (u32 __iomem *)(bd));
+
+		priv->phase_tx++;
+	}
+
+	return 0;
+}
+
+static irqreturn_t ucc_tdm_irq_handler(int irq, void *dev_id)
+{
+	struct ucc_tdm_private *priv = (struct ucc_tdm_private *)dev_id;
+	struct ucc_fast_private *uccf;
+	struct ucc_tdm_info *ut_info;
+	u32 ucce;
+	u32 uccm;
+
+	ut_info = priv->ut_info;
+	uccf = priv->uccf;
+
+	ucce = ioread32be(uccf->p_ucce);
+	uccm = ioread32be(uccf->p_uccm);
+
+	if ((ucce >> 16) & UCC_TRANS_UCCE_RXB) {
+		if (priv->phase_rx  == NUM_OF_BUF - 1)
+			priv->phase_rx = 0;
+		else
+			priv->phase_rx++;
+
+		priv->tdm_queue_flag = true;
+		wake_up_interruptible(&priv->tdm_queue);
+
+	}
+
+	iowrite32be(ucce, uccf->p_ucce);
+
+	return IRQ_HANDLED;
+
+}
+
+static int utdm_start(struct tdm_adapter *adap)
+{
+	u32 cecr_subblock;
+	struct ucc_tdm_private *priv = tdm_get_adapdata(adap);
+
+	if (priv->tdm_busy != 1) {
+		if (request_irq(priv->ut_info->uf_info.irq, ucc_tdm_irq_handler,
+					0, "tdm", (void *)priv)) {
+			dev_err(priv->dev, "request_irq for ucc tdm failed\n");
+			return -ENODEV;
+		}
+		cecr_subblock = ucc_fast_get_qe_cr_subblock(
+					priv->ut_info->uf_info.ucc_num);
+
+		qe_issue_cmd(QE_INIT_TX_RX, cecr_subblock,
+			(u8) QE_CR_PROTOCOL_UNSPECIFIED, 0);
+
+		ucc_fast_enable(priv->uccf, COMM_DIR_RX | COMM_DIR_TX);
+
+		/* Enable the TDM port */
+		priv->si_regs->siglmr1_h |= (0x1 << priv->tdm_port);
+		priv->phase_rx = 0;
+		priv->phase_tx = 0;
+		priv->tdm_busy = 1;
+	} else
+		dev_err(priv->dev, "TDM IS RUNNING!\n");
+
+#ifdef DEBUG
+	dump_priv(priv);
+	dump_ucc(priv);
+	dump_bds(priv);
+#endif
+
+	return 0;
+}
+
+static void utdm_memclean(struct ucc_tdm_private *priv)
+{
+	qe_muram_free(priv->ucc_pram->riptr);
+	qe_muram_free(priv->ucc_pram->tiptr);
+
+	if (priv->rx_bd) {
+		dma_free_coherent(priv->dev,
+			NUM_OF_BUF * MAX_RX_BUF_LENGTH,
+			priv->rx_bd, priv->dma_rx_bd);
+		priv->rx_bd = NULL;
+		priv->dma_rx_bd = 0;
+	}
+
+	if (priv->tx_bd) {
+		dma_free_coherent(priv->dev,
+			NUM_OF_BUF * MAX_RX_BUF_LENGTH,
+			priv->tx_bd, priv->dma_tx_bd);
+		priv->tx_bd = NULL;
+		priv->dma_tx_bd = 0;
+	}
+
+	if (priv->ucc_pram) {
+		qe_muram_free(priv->ucc_pram_offset);
+		priv->ucc_pram = NULL;
+		priv->ucc_pram_offset = 0;
+	 }
+
+	if (priv->uf_regs) {
+		iounmap(priv->uf_regs);
+		priv->uf_regs = NULL;
+	}
+
+	if (priv->uccf) {
+		ucc_fast_free(priv->uccf);
+		priv->uccf = NULL;
+	}
+
+	if (priv->rx_buffer) {
+		dma_free_coherent(priv->dev,
+			2 * NUM_OF_BUF * MAX_RX_BUF_LENGTH,
+			priv->rx_buffer, priv->dma_rx_addr);
+		priv->rx_buffer = NULL;
+		priv->dma_rx_addr = 0;
+	}
+}
+
+static int utdm_stop(struct tdm_adapter *adap)
+{
+	struct ucc_tdm_private *priv = tdm_get_adapdata(adap);
+	u32 cecr_subblock;
+
+	cecr_subblock = ucc_fast_get_qe_cr_subblock(
+				priv->ut_info->uf_info.ucc_num);
+
+	qe_issue_cmd(QE_GRACEFUL_STOP_TX, cecr_subblock,
+			(u8) QE_CR_PROTOCOL_UNSPECIFIED, 0);
+	qe_issue_cmd(QE_CLOSE_RX_BD, cecr_subblock,
+			(u8) QE_CR_PROTOCOL_UNSPECIFIED, 0);
+
+	priv->si_regs->siglmr1_h &= ~(0x1 << priv->tdm_port);
+	ucc_fast_disable(priv->uccf, COMM_DIR_RX | COMM_DIR_TX);
+
+	free_irq(priv->ut_info->uf_info.irq, priv);
+	priv->tdm_busy = 0;
+
+	return 0;
+}
+
+static const struct tdm_adapt_algorithm tdm_algo = {
+	.tdm_read_simple	= ucc_tdm_read,
+	.tdm_write_simple	= ucc_tdm_write,
+	.tdm_enable		= utdm_start,
+	.tdm_disable		= utdm_stop,
+};
+
+static struct tdm_adapter ucc_tdm_ops = {
+	.owner	= THIS_MODULE,
+	.algo	= &tdm_algo,
+};
+
+static enum tdm_mode_t set_tdm_mode(const char *tdm_mode_type)
+{
+	if (strcasecmp(tdm_mode_type, "internal-loopback") == 0)
+		return TDM_INTERNAL_LOOPBACK;
+	else
+		return TDM_NORMAL;
+}
+
+
+static enum tdm_framer_t set_tdm_framer(const char *tdm_framer_type)
+{
+	if (strcasecmp(tdm_framer_type, "e1") == 0)
+		return TDM_FRAMER_E1;
+	else
+		return TDM_FRAMER_T1;
+}
+
+static void set_si_param(struct ucc_tdm_private *priv)
+{
+	struct si_mode_info *si_info = &priv->ut_info->si_info;
+
+	if (priv->tdm_mode == TDM_INTERNAL_LOOPBACK) {
+		si_info->simr_crt = 1;
+		si_info->simr_rfsd = 0;
+	}
+}
+
+static int ucc_tdm_probe(struct platform_device *pdev)
+{
+	struct device_node *np = pdev->dev.of_node;
+	struct ucc_tdm_private *utdm_priv = NULL;
+	struct ucc_tdm_info *ut_info;
+	struct resource res;
+	int ucc_num;
+	const unsigned int *prop;
+	const char *sprop;
+	struct device_node *np2;
+	int ret;
+
+	prop = of_get_property(np, "cell-index", NULL);
+	if (!prop) {
+		dev_err(&pdev->dev, "Invalid ucc property\n");
+		return -ENODEV;
+	}
+
+	ucc_num = *prop - 1;
+	if ((ucc_num > 7) || (ucc_num < 0)) {
+		dev_err(&pdev->dev, ": Invalid UCC num\n");
+		return -EINVAL;
+	}
+
+	memcpy(&(utdm_info[ucc_num]), &utdm_primary_info,
+		sizeof(utdm_primary_info));
+
+	ut_info = &utdm_info[ucc_num];
+	ut_info->uf_info.ucc_num = ucc_num;
+
+	sprop = of_get_property(np, "rx-clock-name", NULL);
+	if (sprop) {
+		ut_info->uf_info.rx_clock = qe_clock_source(sprop);
+		if ((ut_info->uf_info.rx_clock < QE_CLK_NONE) ||
+			(ut_info->uf_info.rx_clock > QE_CLK24)) {
+			dev_err(&pdev->dev, "Invalid rx-clock-name property\n");
+			return -EINVAL;
+		}
+	} else {
+		dev_err(&pdev->dev, "Invalid rx-clock-name property\n");
+		return -EINVAL;
+	}
+
+	sprop = of_get_property(np, "tx-clock-name", NULL);
+	if (sprop) {
+		ut_info->uf_info.tx_clock = qe_clock_source(sprop);
+		if ((ut_info->uf_info.tx_clock < QE_CLK_NONE) ||
+			(ut_info->uf_info.tx_clock > QE_CLK24)) {
+			dev_err(&pdev->dev, "Invalid tx-clock-name property\n");
+		return -EINVAL;
+		}
+	} else {
+		dev_err(&pdev->dev, "Invalid tx-clock-name property\n");
+		return -EINVAL;
+	}
+
+	/* use the same clock when work in loopback */
+	if (ut_info->uf_info.rx_clock == ut_info->uf_info.tx_clock)
+		qe_setbrg(ut_info->uf_info.rx_clock, 2000000, 1);
+
+	sprop = of_get_property(np, "fsl,rx-sync-clock", NULL);
+	if (sprop) {
+		ut_info->uf_info.rx_sync = qe_clock_source(sprop);
+		if ((ut_info->uf_info.rx_sync < QE_CLK_NONE) ||
+			(ut_info->uf_info.rx_sync > QE_RSYNC_PIN)) {
+			dev_err(&pdev->dev, "Invalid rx-sync-clock property\n");
+		return -EINVAL;
+		}
+	} else {
+		dev_err(&pdev->dev, "Invalid rx-sync-clock property\n");
+		return -EINVAL;
+	}
+
+	sprop = of_get_property(np, "fsl,tx-sync-clock", NULL);
+	if (sprop) {
+		ut_info->uf_info.tx_sync = qe_clock_source(sprop);
+		if ((ut_info->uf_info.tx_sync < QE_CLK_NONE) ||
+			(ut_info->uf_info.tx_sync > QE_TSYNC_PIN)) {
+			dev_err(&pdev->dev, "Invalid tx-sync-clock property\n");
+		return -EINVAL;
+		}
+	} else {
+		dev_err(&pdev->dev, "Invalid tx-sync-clock property\n");
+		return -EINVAL;
+	}
+
+	ret = of_address_to_resource(np, 0, &res);
+	if (ret)
+		return -EINVAL;
+
+	ut_info->uf_info.regs = res.start;
+	ut_info->uf_info.irq = irq_of_parse_and_map(np, 0);
+
+	utdm_priv = kzalloc(sizeof(struct ucc_tdm_private), GFP_KERNEL);
+	if (!utdm_priv) {
+		ret = -ENOMEM;
+		dev_err(&pdev->dev, "No mem to alloc tdm private data\n");
+		goto err_alloc_priv;
+	}
+
+	dev_set_drvdata(&pdev->dev, utdm_priv);
+	utdm_priv->dev = &pdev->dev;
+
+	prop = of_get_property(np, "fsl,tx-timeslot", NULL);
+	if (!prop) {
+		ret = -EINVAL;
+		dev_err(&pdev->dev, "Invalid tx-timeslot property\n");
+		goto err_miss_property;
+	}
+	utdm_priv->tx_ts_mask = *prop;
+
+	prop = of_get_property(np, "fsl,rx-timeslot", NULL);
+	if (!prop) {
+		ret = -EINVAL;
+		dev_err(&pdev->dev, "Invalid rx-timeslot property\n");
+		goto err_miss_property;
+	}
+	utdm_priv->rx_ts_mask = *prop;
+
+	prop = of_get_property(np, "fsl,tdm-id", NULL);
+	if (!prop) {
+		ret = -EINVAL;
+		dev_err(&pdev->dev, "No fsl,tdm-id property for this UCC\n");
+		goto err_miss_property;
+	}
+	utdm_priv->tdm_port = *prop;
+	ut_info->uf_info.tdm_num = utdm_priv->tdm_port;
+
+	prop = of_get_property(np, "fsl,tdm-mode", NULL);
+	if (!prop) {
+		ret = -EINVAL;
+		dev_err(&pdev->dev, "No tdm-mode property for UCC\n");
+		goto err_miss_property;
+	}
+	utdm_priv->tdm_mode = set_tdm_mode((const char *)prop);
+
+	prop = of_get_property(np, "fsl,tdm-framer-type", NULL);
+	if (!prop) {
+		ret = -EINVAL;
+		dev_err(&pdev->dev, "No tdm-framer-type property for UCC\n");
+		goto err_miss_property;
+	}
+	utdm_priv->tdm_framer_type = set_tdm_framer((const char *)prop);
+
+	prop = of_get_property(np, "fsl,siram-entry-id", NULL);
+	if (!prop) {
+		ret = -EINVAL;
+		dev_err(&pdev->dev, "No siram entry id for UCC\n");
+		goto err_miss_property;
+	}
+	utdm_priv->siram_entry_id = *(const u32 *)prop;
+
+	np2 = of_find_node_by_name(NULL, "si");
+	if (!np2) {
+		dev_err(&pdev->dev, "No si property\n");
+		goto err_miss_property;
+	}
+	of_address_to_resource(np2, 0, &res);
+	utdm_priv->si_regs = ioremap(res.start, res.end - res.start + 1);
+	of_node_put(np2);
+
+
+	np2 = of_find_node_by_name(NULL, "siram");
+	if (!np2) {
+		ret = -EINVAL;
+		dev_err(&pdev->dev, "No siramproperty\n");
+		goto err_miss_si_property;
+	}
+	of_address_to_resource(np2, 0 , &res);
+	utdm_priv->siram = ioremap(res.start, res.end - res.start + 1);
+	of_node_put(np2);
+
+	if (siram_init_flag == 0) {
+		memset(utdm_priv->siram, 0,  res.end - res.start + 1);
+		siram_init_flag = 1;
+	}
+
+	utdm_priv->ut_info = ut_info;
+	set_si_param(utdm_priv);
+
+	sprintf(ucc_tdm_ops.name, "%s%d", "tdm_ucc_", ucc_num + 1);
+	memcpy(&utdm_priv->adap, &ucc_tdm_ops, sizeof(struct tdm_adapter));
+
+	tdm_set_adapdata(&utdm_priv->adap, utdm_priv);
+	utdm_priv->adap.parent = &pdev->dev;
+
+	init_waitqueue_head(&utdm_priv->tdm_queue);
+	utdm_priv->tdm_queue_flag = false;
+
+	ret = utdm_init(utdm_priv);
+	if (ret) {
+		dev_err(&pdev->dev, "Failed to init utdm\n");
+		goto err_utdm_init;
+	}
+
+	ret = tdm_add_adapter(&utdm_priv->adap);
+	if (ret < 0) {
+		dev_err(&pdev->dev, "Failed to add adapter\n");
+		goto err_utdm_init;
+	}
+
+	spin_lock_init(&utdm_priv->tdmlock);
+
+#ifdef DEBUG
+	dump_priv(utdm_priv);
+	dump_ucc(utdm_priv);
+	dump_bds(utdm_priv);
+	mem_disp((u8 *)utdm_priv->si_regs, 0x20);
+#endif
+
+	return 0;
+
+err_utdm_init:
+	iounmap(utdm_priv->siram);
+err_miss_si_property:
+	iounmap(utdm_priv->si_regs);
+err_miss_property:
+	kfree(utdm_priv);
+err_alloc_priv:
+	return ret;
+
+}
+
+static int ucc_tdm_remove(struct platform_device *pdev)
+{
+	struct ucc_tdm_private *priv = dev_get_drvdata(&pdev->dev);
+
+	utdm_stop(&priv->adap);
+	utdm_memclean(priv);
+
+	if (priv->si_regs) {
+		iounmap(priv->si_regs);
+		priv->si_regs = NULL;
+	}
+
+	if (priv->siram) {
+		iounmap(priv->siram);
+		priv->siram = NULL;
+	}
+	kfree(priv);
+
+	dev_info(&pdev->dev, "UCC based tdm module removed\n");
+
+	return 0;
+}
+
+#ifdef CONFIG_PM
+static void store_clk_config(struct ucc_tdm_private *utdm_priv)
+{
+	struct qe_mux *qe_mux_reg = &qe_immr->qmx;
+
+	/* store si clk */
+	utdm_priv->cmxsi1cr_h = ioread32be(&qe_mux_reg->cmxsi1cr_h);
+	utdm_priv->cmxsi1cr_l = ioread32be(&qe_mux_reg->cmxsi1cr_l);
+
+	/* store si sync */
+	utdm_priv->cmxsi1syr = ioread32be(&qe_mux_reg->cmxsi1syr);
+
+	/* store ucc clk */
+	memcpy_fromio(utdm_priv->cmxucr, qe_mux_reg->cmxucr, 4 * sizeof(u32));
+}
+
+static void resume_clk_config(struct ucc_tdm_private *utdm_priv)
+{
+	struct qe_mux *qe_mux_reg = &qe_immr->qmx;
+
+	memcpy_toio(qe_mux_reg->cmxucr, utdm_priv->cmxucr, 4 * sizeof(u32));
+
+	iowrite32be(utdm_priv->cmxsi1cr_h, &qe_mux_reg->cmxsi1cr_h);
+	iowrite32be(utdm_priv->cmxsi1cr_l, &qe_mux_reg->cmxsi1cr_l);
+
+	iowrite32be(utdm_priv->cmxsi1syr, &qe_mux_reg->cmxsi1syr);
+
+}
+
+static int ucc_tdm_suspend(struct device *dev)
+{
+	struct ucc_tdm_private *utdm_priv = dev_get_drvdata(dev);
+	struct ucc_tdm_info *ut_info;
+	struct ucc_fast __iomem *uf_regs;
+
+	if (!utdm_priv)
+		return -EINVAL;
+
+	ut_info = utdm_priv->ut_info;
+	uf_regs = utdm_priv->uf_regs;
+
+	/* backup gumr guemr*/
+	utdm_priv->gumr = ioread32be(&uf_regs->gumr);
+	utdm_priv->guemr = ioread8(&uf_regs->guemr);
+
+	utdm_priv->ucc_pram_bak = kmalloc(sizeof(struct ucc_transparent_param),
+					GFP_KERNEL);
+	if (!utdm_priv->ucc_pram_bak)
+		return -ENOMEM;
+
+	/* backup transparent parameter */
+	memcpy_fromio(utdm_priv->ucc_pram_bak, utdm_priv->ucc_pram,
+			sizeof(struct ucc_transparent_param));
+
+	/* store the clk configuration */
+	store_clk_config(utdm_priv);
+
+	/* save power */
+	ucc_fast_disable(utdm_priv->uccf, COMM_DIR_RX | COMM_DIR_TX);
+
+	dev_dbg(dev, "ucc tdm suspend\n");
+
+	return 0;
+}
+
+static int ucc_tdm_resume(struct device *dev)
+{
+	struct ucc_tdm_private *utdm_priv = dev_get_drvdata(dev);
+	struct ucc_tdm_info *ut_info;
+	struct ucc_fast __iomem *uf_regs;
+	struct ucc_fast_private *uccf;
+	struct ucc_fast_info *uf_info;
+	int ret, i;
+	u32 cecr_subblock, bd_status;
+
+	if (!utdm_priv)
+		return -EINVAL;
+
+	ut_info = utdm_priv->ut_info;
+	uf_info = &ut_info->uf_info;
+	uf_regs = utdm_priv->uf_regs;
+	uccf = utdm_priv->uccf;
+
+	/* restore gumr guemr */
+	iowrite8(utdm_priv->guemr, &uf_regs->guemr);
+	iowrite32be(utdm_priv->gumr, &uf_regs->gumr);
+
+	/* Set Virtual Fifo registers */
+	iowrite16be(uf_info->urfs, &uf_regs->urfs);
+	iowrite16be(uf_info->urfet, &uf_regs->urfet);
+	iowrite16be(uf_info->urfset, &uf_regs->urfset);
+	iowrite16be(uf_info->utfs, &uf_regs->utfs);
+	iowrite16be(uf_info->utfet, &uf_regs->utfet);
+	iowrite16be(uf_info->utftt, &uf_regs->utftt);
+	/* utfb, urfb are offsets from MURAM base */
+	iowrite32be(uccf->ucc_fast_tx_virtual_fifo_base_offset, &uf_regs->utfb);
+	iowrite32be(uccf->ucc_fast_rx_virtual_fifo_base_offset, &uf_regs->urfb);
+
+	/* tdm Rx Tx and sync clock routing */
+	resume_clk_config(utdm_priv);
+
+	iowrite32be(uf_info->uccm_mask, &uf_regs->uccm);
+	iowrite32be(0xffffffff, &uf_regs->ucce);
+
+	ucc_fast_disable(utdm_priv->uccf, COMM_DIR_RX | COMM_DIR_TX);
+
+	/* rebuild SIRAM */
+	init_si(utdm_priv);
+
+	/* Write to QE CECR, UCCx channel to Stop Transmission */
+	cecr_subblock = ucc_fast_get_qe_cr_subblock(uf_info->ucc_num);
+	ret = qe_issue_cmd(QE_STOP_TX, cecr_subblock,
+		(u8)QE_CR_PROTOCOL_UNSPECIFIED, 0);
+
+	/* Set UPSMR normal mode */
+	iowrite32be(0, &uf_regs->upsmr);
+
+	/* init parameter base */
+	cecr_subblock = ucc_fast_get_qe_cr_subblock(uf_info->ucc_num);
+	ret = qe_issue_cmd(QE_ASSIGN_PAGE_TO_DEVICE, cecr_subblock,
+			QE_CR_PROTOCOL_UNSPECIFIED, utdm_priv->ucc_pram_offset);
+
+	utdm_priv->ucc_pram = (struct ucc_transparent_param __iomem *)
+				qe_muram_addr(utdm_priv->ucc_pram_offset);
+
+	/* restore ucc parameter */
+	memcpy_toio(utdm_priv->ucc_pram, utdm_priv->ucc_pram_bak,
+			sizeof(struct ucc_transparent_param));
+	kfree(utdm_priv->ucc_pram_bak);
+
+	/* rebuild BD entry */
+	for (i = 0; i < NUM_OF_BUF; i++) {
+		if (i < (NUM_OF_BUF - 1))
+			bd_status = R_E | R_I | R_CM;
+		else
+			bd_status = R_E | R_I | R_W | R_CM;
+
+		iowrite32be(bd_status, (u32 *)(utdm_priv->rx_bd + i));
+		iowrite32be(utdm_priv->dma_rx_addr + i * MAX_RX_BUF_LENGTH,
+			    &utdm_priv->rx_bd[i].buf);
+
+		if (i < (NUM_OF_BUF - 1))
+			bd_status =  T_I;
+		else
+			bd_status =  T_I | T_W;
+
+		iowrite32be(bd_status, (u32 *)(utdm_priv->tx_bd + i));
+		iowrite32be(utdm_priv->dma_tx_addr + i * MAX_RX_BUF_LENGTH,
+			    &utdm_priv->tx_bd[i].buf);
+	}
+
+	/* if tdm is busy enable TX and RX */
+	if (utdm_priv->tdm_busy == 1) {
+		cecr_subblock = ucc_fast_get_qe_cr_subblock(
+					utdm_priv->ut_info->uf_info.ucc_num);
+
+		qe_issue_cmd(QE_INIT_TX_RX, cecr_subblock,
+			(u8)QE_CR_PROTOCOL_UNSPECIFIED, 0);
+
+		ucc_fast_enable(utdm_priv->uccf, COMM_DIR_RX | COMM_DIR_TX);
+
+		/* Enable the TDM port */
+		utdm_priv->si_regs->siglmr1_h |= (0x1 << utdm_priv->tdm_port);
+	}
+
+	return 0;
+}
+
+SIMPLE_DEV_PM_OPS(ucc_tdm_pm_ops, ucc_tdm_suspend, ucc_tdm_resume);
+#endif
+
+static const struct of_device_id fsl_ucc_tdm_of_match[] = {
+	{
+	.compatible = "fsl,ucc-tdm",
+	},
+	{},
+};
+
+MODULE_DEVICE_TABLE(of, fsl_ucc_tdm_of_match);
+
+static struct platform_driver ucc_tdm_driver = {
+	.probe	= ucc_tdm_probe,
+	.remove	= ucc_tdm_remove,
+	.driver	= {
+		.owner		= THIS_MODULE,
+		.name		= DRV_NAME,
+		.of_match_table	= fsl_ucc_tdm_of_match,
+#ifdef CONFIG_PM
+		.pm = &ucc_tdm_pm_ops,
+#endif
+	},
+};
+
+static int __init ucc_tdm_init(void)
+{
+	return platform_driver_register(&ucc_tdm_driver);
+}
+
+static void __exit ucc_tdm_exit(void)
+{
+	platform_driver_unregister(&ucc_tdm_driver);
+}
+
+module_init(ucc_tdm_init);
+module_exit(ucc_tdm_exit);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Freescale Semiconductor Inc.");
+MODULE_DESCRIPTION("Driver For Freescale QE UCC TDM controller");
+MODULE_VERSION("1.0");
diff --git a/drivers/tdm/device/fsl_ucc_tdm.h b/drivers/tdm/device/fsl_ucc_tdm.h
new file mode 100644
index 0000000..7c7898d
--- /dev/null
+++ b/drivers/tdm/device/fsl_ucc_tdm.h
@@ -0,0 +1,173 @@
+/*
+ * Freescale QUICC Engine TDM Device Driver
+ *
+ * Copyright 2011-2012 Freescale Semiconductor, Inc.
+ *
+ * Author: Haiying Wang <Haiying.Wang@freescale.com>
+ *		    Kai Jiang	   <Kai.Jiang@freescale.com>
+ *
+ * This program is free software; you can redistribute  it and/or modify it
+ * under  the terms of  the GNU General  Public License as published by the
+ * Free Software Foundation;  either version 2 of the  License, or (at your
+ * option) any later version.
+ *
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the  GNU General Public License along
+ * with this program; if not, write  to the Free Software Foundation, Inc.,
+ * 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ * This driver adds support for TDM devices via Freescale's QUICC Engine.
+ */
+
+
+#ifndef CONFIG_UCC_TDM_H
+#define CONFIG_UCC_TDM_H
+
+#include <linux/fsl/immap_qe.h>
+#include <linux/fsl/qe.h>
+
+#include <linux/fsl/ucc.h>
+#include <linux/fsl/ucc_fast.h>
+
+#include <linux/list.h>
+#include <linux/kernel.h>
+
+/* SI RAM entries */
+#define SIR_LAST	0x0001
+#define SIR_BYTE	0x0002
+#define SIR_CNT(x)	((x) << 2)
+#define SIR_CSEL(x)	((x) << 5)
+#define SIR_SGS		0x0200
+#define SIR_SWTR	0x4000
+#define SIR_MCC		0x8000
+#define SIR_IDLE	0
+
+/* SIxMR fields */
+#define SIMR_SAD(x) ((x) << 12)
+#define SIMR_SDM_NORMAL	0x0000
+#define SIMR_SDM_INTERNAL_LOOPBACK	0x0800
+#define SIMR_SDM_MASK	0x0c00
+#define SIMR_CRT	0x0040
+#define SIMR_SL		0x0020
+#define SIMR_CE		0x0010
+#define SIMR_FE		0x0008
+#define SIMR_GM		0x0004
+#define SIMR_TFSD(n)	(n)
+#define SIMR_RFSD(n)	((n) << 8)
+
+enum tdm_ts_t {
+	TDM_TX_TS,
+	TDM_RX_TS
+};
+
+
+enum tdm_framer_t {
+	TDM_FRAMER_T1,
+	TDM_FRAMER_E1
+};
+
+enum tdm_mode_t {
+	TDM_INTERNAL_LOOPBACK,
+	TDM_NORMAL
+};
+
+struct ucc_transparent_param {
+	__be16 riptr;
+	__be16 tiptr;
+	__be16 res0;
+	__be16 mrblr;
+	__be32 rstate;
+	__be32 rbase;
+	__be16 rbdstat;
+	__be16 rbdlen;
+	__be32 rdptr;
+	__be32 tstate;
+	__be32 tbase;
+	__be16 tbdstat;
+	__be16 tbdlen;
+	__be32 tdptr;
+	__be32 rbptr;
+	__be32 tbptr;
+	__be32 rcrc;
+	__be32 res1;
+	__be32 tcrc;
+	__be32 res2;
+	__be32 res3;
+	__be32 c_mask;
+	__be32 c_pres;
+	__be16 disfc;
+	__be16 crcec;
+	__be32 res4[4];
+	__be16 ts_tmp;
+	__be16 tmp_mb;
+} __attribute__ ((__packed__));
+
+struct si_mode_info {
+	u8 simr_rfsd;
+	u8 simr_tfsd;
+	u8 simr_crt;
+	u8 simr_sl;
+	u8 simr_ce;
+	u8 simr_fe;
+	u8 simr_gm;
+};
+
+struct ucc_tdm_info {
+	struct ucc_fast_info uf_info;
+	struct si_mode_info si_info;
+};
+
+struct ucc_tdm_private {
+	struct ucc_tdm_info *ut_info;
+	struct ucc_fast_private *uccf;
+	struct device *dev;
+	struct ucc_fast __iomem *uf_regs;	/* UCC Fast registers */
+	struct si1 __iomem *si_regs;
+	struct ucc_transparent_param __iomem *ucc_pram;
+	u16 tdm_port;		/* port for this tdm:TDMA,TDMB,TDMC,TDMD */
+	u32 siram_entry_id;
+	u16 __iomem *siram;
+	enum tdm_mode_t tdm_mode;
+	enum tdm_framer_t tdm_framer_type;
+	bool tdm_busy;
+	u8 num_of_ts;		/* the number of timeslots in this tdm frame */
+	u32 tx_ts_mask;		/* tx time slot mask */
+	u32 rx_ts_mask;		/*rx time slot mask */
+	u8 *rx_buffer;		/* buffer used for Rx by the tdm */
+	u8 *tx_buffer;		/* buffer used for Tx by the tdm */
+	dma_addr_t dma_rx_addr;	/* dma mapped buffer for TDM Rx */
+	dma_addr_t dma_tx_addr;	/* dma mapped buffer for TDM Tx */
+	struct qe_bd *tx_bd;
+	struct qe_bd *rx_bd;
+	u8 phase_rx;
+	u8 phase_tx;
+	u32 ucc_pram_offset;
+	dma_addr_t dma_rx_bd;
+	dma_addr_t dma_tx_bd;
+	spinlock_t tdmlock;
+	wait_queue_head_t tdm_queue;
+	bool tdm_queue_flag;
+	struct tdm_adapter adap;
+#ifdef CONFIG_PM
+	struct ucc_transparent_param *ucc_pram_bak;
+	u32 gumr;
+	u8 guemr;
+	u32 cmxsi1cr_l, cmxsi1cr_h;
+	u32 cmxsi1syr;
+	u32 cmxucr[4];
+#endif
+};
+
+#define NUM_OF_BUF	4
+#define MAX_RX_BUF_LENGTH	(72*0x20)
+#define ALIGNMENT_OF_UCC_TRANS_PRAM	64
+#define SI_BANK_SIZE	128
+#define MAX_TDM_NUM	8
+#define BD_LEN_MASK	0xffff
+
+#endif
diff --git a/drivers/tdm/tdm-core.c b/drivers/tdm/tdm-core.c
new file mode 100644
index 0000000..a16444e
--- /dev/null
+++ b/drivers/tdm/tdm-core.c
@@ -0,0 +1,1188 @@
+/* driver/tdm/tdm-core.c
+ *
+ * Copyright 2012 Freescale Semiconductor, Inc.
+ *
+ * TDM core is the interface between TDM clients and TDM devices.
+ * It is also intended to serve as an interface for line controld
+ * devices later on.
+ *
+ * Author:Hemant Agrawal <hemant@freescale.com>
+ *	Rajesh Gumasta <rajesh.gumasta@freescale.com>
+ *
+ * Modified by Sandeep Kr Singh <sandeep@freescale.com>
+ *		Poonam Aggarwal <poonam.aggarwal@freescale.com>
+ * 1. Added framework based initilization of device.
+ * 2. All the init/run time configuration is now done by framework.
+ * 3. Added channel level operations.
+ *
+ * This program is free software; you can redistribute  it and/or modify it
+ * under  the terms of  the GNU General  Public License as published by the
+ * Free Software Foundation;  either version 2 of the  License, or (at your
+ * option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the  GNU General Public License along
+ * with this program; if not, write  to the Free Software Foundation, Inc.,
+ * 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+/* if read write debug required */
+#undef TDM_CORE_DEBUG
+
+#include <linux/completion.h>
+#include <linux/errno.h>
+#include <linux/hardirq.h>
+#include <linux/idr.h>
+#include <linux/init.h>
+#include <linux/io.h>
+#include <linux/irqflags.h>
+#include <linux/kernel.h>
+#include <linux/list.h>
+#include <linux/module.h>
+#include <linux/mutex.h>
+#include <linux/slab.h>
+#include <linux/tdm.h>
+#include <linux/uaccess.h>
+
+static DEFINE_MUTEX(tdm_core_lock);
+static DEFINE_IDR(tdm_adapter_idr);
+/* List of TDM adapters registered with TDM framework */
+LIST_HEAD(adapter_list);
+
+/* List of TDM clients registered with TDM framework */
+LIST_HEAD(driver_list);
+
+/* In case the previous data is not fetched by the client driver, the
+ * de-interleaving function will  discard the old data and rewrite the
+ * new data */
+static int use_latest_tdm_data = 1;
+
+/* this tasklet is created for each adapter instance */
+static void tdm_data_tasklet_fn(unsigned long);
+
+/* tries to match client driver with the adapter */
+static int tdm_device_match(struct tdm_driver *driver, struct tdm_adapter *adap)
+{
+	/* match on an id table if there is one */
+	if (driver->id_table && driver->id_table->name[0]) {
+		if (!(strcmp(driver->id_table->name, adap->name)))
+			return (int)driver->id_table;
+	}
+	return TDM_E_OK;
+}
+
+static int tdm_attach_driver_adap(struct tdm_driver *driver,
+					struct tdm_adapter *adap)
+{
+	int ret = TDM_E_OK;
+	/* if driver is already attached to any other adapter, return*/
+	if (driver->adapter && (driver->adapter != adap))
+		return ret;
+
+	driver->adapter = adap;
+
+	if (driver->attach_adapter) {
+		ret = driver->attach_adapter(adap);
+		if (ret < 0) {
+			pr_err("attach_adapter failed for driver [%s] err:%d\n"
+				, driver->name, ret);
+			return ret;
+		}
+	}
+	adap->drv_count++;
+
+	if (!adap->tasklet_conf) {
+		tasklet_init(&adap->tdm_data_tasklet, tdm_data_tasklet_fn,
+						(unsigned long)adap);
+		adap->tasklet_conf = 1;
+	}
+
+	return ret;
+}
+
+/* Detach client driver and adapter */
+static int tdm_detach_driver_adap(struct tdm_driver *driver,
+					struct tdm_adapter *adap)
+{
+	int res = TDM_E_OK;
+
+	if (!driver->adapter || (driver->adapter != adap))
+		return TDM_E_OK;
+
+	if (!driver->detach_adapter)
+		return TDM_E_OK;
+
+	adap->drv_count--;
+
+	/* If no more driver is registed with the adapter*/
+	if (!adap->drv_count && adap->tasklet_conf) {
+		tasklet_disable(&adap->tdm_data_tasklet);
+		tasklet_kill(&adap->tdm_data_tasklet);
+		adap->tasklet_conf = 0;
+	}
+
+	if (driver->detach_adapter) {
+		if (driver->detach_adapter(adap))
+			pr_err("detach_adapter failed for driver [%s]\n",
+				driver->name);
+	}
+
+	driver->adapter = NULL;
+	return res;
+}
+
+/* TDM adapter Registration/De-registration with TDM framework */
+
+static int tdm_register_adapter(struct tdm_adapter *adap)
+{
+	int res = TDM_E_OK;
+	struct tdm_driver *driver, *next;
+
+	if (!adap) {
+		pr_err("%s:Invalid handle\n", __func__);
+		return -EINVAL;
+	}
+
+	mutex_init(&adap->adap_lock);
+	INIT_LIST_HEAD(&adap->myports);
+	spin_lock_init(&adap->portlist_lock);
+
+	adap->drv_count = 0;
+	adap->tasklet_conf = 0;
+
+	list_add_tail(&adap->list, &adapter_list);
+
+	/* initialization of driver by framework in default configuration */
+	init_config_adapter(adap);
+
+	/* Notify drivers */
+	pr_info("adapter [%s] registered\n", adap->name);
+	mutex_lock(&tdm_core_lock);
+	list_for_each_entry_safe(driver, next, &driver_list, list) {
+		if (tdm_device_match(driver, adap)) {
+			res = tdm_attach_driver_adap(driver, adap);
+			if (res == TDM_E_OK) {
+				pr_info("Driver(ID=%d) is ", driver->id);
+				pr_info("attached with Adapter %s(ID = %d)\n",
+					adap->name, adap->id);
+			} else {
+				pr_err("Driver(ID=%d) is unable ", driver->id);
+				pr_err("to attach with Adapter %s(ID = %d)\n",
+				adap->name, adap->id);
+			}
+		}
+	}
+	mutex_unlock(&tdm_core_lock);
+
+	return res;
+}
+
+/*
+ * tdm_add_adapter - declare tdm adapter, use dynamic device number
+ * @adapter: the adapter to add
+ * Context: can sleep
+ *
+ * This routine is used to declare a TDM adapter
+ * When this returns zero, a new device number will be allocated and stored
+ * in adap->id, and the specified adapter became available for the clients.
+ * Otherwise, a negative errno value is returned.
+ */
+int tdm_add_adapter(struct tdm_adapter *adapter)
+{
+	int id, res = TDM_E_OK;
+	if (!adapter) {
+		pr_err("%s:Invalid handle\n", __func__);
+		return -EINVAL;
+	}
+
+retry:
+	if (__idr_pre_get(&tdm_adapter_idr, GFP_KERNEL) == 0)
+		return -ENOMEM;
+
+	mutex_lock(&tdm_core_lock);
+	res = __idr_get_new(&tdm_adapter_idr, adapter, &id);
+	mutex_unlock(&tdm_core_lock);
+
+	if (res < 0) {
+		if (res == -EAGAIN)
+			goto retry;
+		return res;
+	}
+
+	adapter->id = id;
+	return tdm_register_adapter(adapter);
+}
+EXPORT_SYMBOL(tdm_add_adapter);
+
+
+/**
+ * tdm_del_adapter - unregister TDM adapter
+ * @adap: the adapter being unregistered
+ *
+ * This unregisters an TDM adapter which was previously registered
+ * by @tdm_add_adapter.
+ */
+int tdm_del_adapter(struct tdm_adapter *adap)
+{
+	int res = TDM_E_OK;
+	struct tdm_adapter *found;
+	struct tdm_driver *driver, *next;
+
+	if (!adap) {
+		pr_err("%s:Invalid handle\n", __func__);
+		return -EINVAL;
+	}
+
+	/* First make sure that this adapter was ever added */
+	mutex_lock(&tdm_core_lock);
+	found = idr_find(&tdm_adapter_idr, adap->id);
+	mutex_unlock(&tdm_core_lock);
+	if (found != adap) {
+		pr_err("tdm-core: attempting to delete unregistered ");
+		pr_err("adapter [%s]\n", adap->name);
+		return -EINVAL;
+	}
+
+	/*disable and kill the data processing tasklet */
+	if (adap->tasklet_conf) {
+		tasklet_disable(&adap->tdm_data_tasklet);
+		tasklet_kill(&adap->tdm_data_tasklet);
+		adap->tasklet_conf = 0;
+	}
+
+	/* Detach any active ports. This can't fail, thus we do not
+	   checking the returned value. */
+	mutex_lock(&tdm_core_lock);
+	list_for_each_entry_safe(driver, next, &driver_list, list) {
+		if (tdm_device_match(driver, adap)) {
+			tdm_detach_driver_adap(driver, adap);
+			pr_info(
+			"Driver(ID=%d) is detached from Adapter %s(ID = %d)\n",
+				 driver->id, adap->name, adap->id);
+		}
+	}
+	mutex_unlock(&tdm_core_lock);
+
+	mutex_lock(&tdm_core_lock);
+	idr_remove(&tdm_adapter_idr, adap->id);
+	mutex_unlock(&tdm_core_lock);
+
+	pr_debug("adapter [%s] unregistered\n", adap->name);
+
+	list_del(&adap->list);
+	/* Clear the device structure in case this adapter is ever going to be
+	   added again */
+	adap->parent = NULL;
+
+	return res;
+}
+EXPORT_SYMBOL(tdm_del_adapter);
+
+/* TDM Client Drivers Registration/De-registration Functions */
+int tdm_register_driver(struct tdm_driver *driver)
+{
+	int res = TDM_E_OK;
+	struct tdm_adapter *adap, *next;
+
+	list_add_tail(&driver->list, &driver_list);
+
+	mutex_lock(&tdm_core_lock);
+	/* Walk the adapters that are already present */
+	list_for_each_entry_safe(adap, next, &adapter_list, list) {
+		if (tdm_device_match(driver, adap)) {
+			res = tdm_attach_driver_adap(driver, adap);
+			if (res == TDM_E_OK) {
+				pr_info("TDM Driver(ID=%d)is attached with ",
+					driver->id);
+				pr_info("Adapter%s(ID = %d) drv_count=%d",
+					adap->name, adap->id, adap->drv_count);
+			} else {
+				pr_err("TDM Driver(ID=%d) unable to attach ",
+					driver->id);
+				pr_err("to Adapter%s(ID = %d) drv_count=%d",
+					adap->name, adap->id, adap->drv_count);
+			}
+		break;
+		}
+	}
+	mutex_unlock(&tdm_core_lock);
+
+	return res;
+}
+EXPORT_SYMBOL(tdm_register_driver);
+
+/*
+ * tdm_unregister_driver - unregister TDM client driver from TDM framework
+ * @driver: the driver being unregistered
+ */
+void tdm_unregister_driver(struct tdm_driver *driver)
+{
+	if (!driver) {
+		pr_err("%s:Invalid handle\n", __func__);
+		return;
+	}
+       /* A driver can register to only one adapter,
+	* so no need to browse the list */
+	mutex_lock(&tdm_core_lock);
+	tdm_detach_driver_adap(driver, driver->adapter);
+	mutex_unlock(&tdm_core_lock);
+
+	list_del(&driver->list);
+
+	pr_debug("tdm-core: driver [%s] unregistered\n", driver->name);
+}
+EXPORT_SYMBOL(tdm_unregister_driver);
+
+/* TDM Framework init and exit */
+static int __init tdm_init(void)
+{
+	pr_info("%s\n", __func__);
+	return TDM_E_OK;
+}
+
+static void __exit tdm_exit(void)
+{
+	pr_info("%s\n", __func__);
+	return;
+}
+
+/* We must initialize early, because some subsystems register tdm drivers
+ * in subsys_initcall() code, but are linked (and initialized) before tdm.
+ */
+postcore_initcall(tdm_init);
+module_exit(tdm_exit);
+
+
+/* Interface to the tdm device/adapter */
+
+/* tdm_read_direct - issue a TDM read
+ * @adap: Handle to TDM device
+ * @buf: Data that will be read from the TDM device
+ * @len: How many bytes to read
+ *
+ * Returns negative errno, or else 0.
+ */
+int tdm_read_direct(struct tdm_adapter *adap, u8 *buf, u32 len)
+{
+	int res;
+
+	if (adap->algo->tdm_read_simple)
+		res = adap->algo->tdm_read_simple(adap, buf, len);
+	else {
+		pr_err("TDM level read not supported\n");
+		return -EOPNOTSUPP;
+	}
+	/* If everything went ok (i.e. frame received), return #bytes
+	transmitted, else error code. */
+
+	return res;
+
+
+}
+EXPORT_SYMBOL(tdm_read_direct);
+
+/* tdm_write_direct - issue a TDM write
+ * @adap: Handle to TDM device
+ * @buf: Data that will be written to the TDM device
+ * @len: How many bytes to write
+ *
+ * Returns negative errno, or else 0.
+ */
+int tdm_write_direct(struct tdm_adapter *adap, u8 *buf, u32 len)
+{
+	int res;
+
+	if (adap->algo->tdm_write_simple)
+		res = adap->algo->tdm_write_simple(adap, buf, len);
+	else {
+		pr_err("TDM level write not supported\n");
+		return -EOPNOTSUPP;
+	}
+
+	return res;
+}
+EXPORT_SYMBOL(tdm_write_direct);
+
+/* tdm_adap_send - issue a TDM write
+ * @adap: Handle to TDM device
+ * @buf: Data that will be written to the TDM device
+ * @count: How many bytes to write
+ *
+ * Returns negative errno, or else the number of bytes written.
+ */
+int tdm_adap_send(struct tdm_adapter *adap, void **buf, int count)
+{
+	int res;
+
+	if ((adap == NULL) || (buf == NULL)) { /* invalid handle*/
+		pr_err("%s: Invalid Handle\n", __func__);
+		return -ENXIO;
+	}
+
+	if (adap->algo->tdm_write)
+		res = adap->algo->tdm_write(adap, buf, count);
+	else {
+		pr_err("TDM level write not supported\n");
+		return -EOPNOTSUPP;
+	}
+
+	/* If everything went ok (i.e. frame transmitted), return #bytes
+	   transmitted, else error code. */
+	return (res == 1) ? count : res;
+}
+EXPORT_SYMBOL(tdm_adap_send);
+
+/**
+ * tdm_adap_recv - issue a TDM read
+ * @adap: Handle to TDM device
+ * @buf: Where to store data read from TDM device
+ *
+ * Returns negative errno, or else the number of bytes read.
+ */
+int tdm_adap_recv(struct tdm_adapter *adap, void **buf)
+{
+	int res;
+
+	if (adap->algo->tdm_read)
+		res = adap->algo->tdm_read(adap, (u16 **)buf);
+	else {
+		pr_err("TDM level read not supported\n");
+		return -EOPNOTSUPP;
+	}
+	/* If everything went ok (i.e. frame received), return #bytes
+	   transmitted, else error code. */
+	return res;
+}
+
+/**
+ * tdm_adap_get_write_buf - get next write TDM device buffer
+ * @adap: Handle to TDM device
+ * @buf: pointer to TDM device buffer
+ *
+ * Returns negative errno, or else size of the write buffer.
+ */
+int tdm_adap_get_write_buf(struct tdm_adapter *adap, void **buf)
+{
+	int res;
+
+	if (adap->algo->tdm_get_write_buf) {
+		res = adap->algo->tdm_get_write_buf(adap, (u16 **)buf);
+	} else {
+		pr_err("TDM level write buf get not supported\n");
+		return -EOPNOTSUPP;
+	}
+	/* If everything went ok (i.e. 1 msg received), return #bytes
+	   transmitted, else error code. */
+	return res;
+}
+EXPORT_SYMBOL(tdm_adap_get_write_buf);
+
+int tdm_adap_enable(struct tdm_driver *drv)
+{
+	int res;
+	struct tdm_adapter *adap;
+	if (drv == NULL) { /* invalid handle*/
+		pr_err("%s: Invalid Handle\n", __func__);
+		return -ENXIO;
+	}
+	adap = drv->adapter;
+
+	if (adap->algo->tdm_enable) {
+		res = adap->algo->tdm_enable(adap);
+	} else {
+		pr_err("TDM level enable not supported\n");
+		return -EOPNOTSUPP;
+	}
+	return res;
+}
+EXPORT_SYMBOL(tdm_adap_enable);
+
+int tdm_adap_disable(struct tdm_driver *drv)
+{
+	int res;
+	struct tdm_adapter *adap;
+	if (drv == NULL) { /* invalid handle*/
+		pr_err("%s: Invalid Handle\n", __func__);
+		return -ENXIO;
+	}
+	adap = drv->adapter;
+
+	if (adap->algo->tdm_disable) {
+		res = adap->algo->tdm_disable(adap);
+	} else {
+		pr_err("TDM level enable not supported\n");
+		return -EOPNOTSUPP;
+	}
+	return res;
+}
+EXPORT_SYMBOL(tdm_adap_disable);
+
+struct tdm_adapter *tdm_get_adapter(int id)
+{
+	struct tdm_adapter *adapter;
+
+	mutex_lock(&tdm_core_lock);
+	adapter = idr_find(&tdm_adapter_idr, id);
+	if (adapter && !try_module_get(adapter->owner))
+		adapter = NULL;
+
+	mutex_unlock(&tdm_core_lock);
+
+	return adapter;
+}
+EXPORT_SYMBOL(tdm_get_adapter);
+
+void tdm_put_adapter(struct tdm_adapter *adap)
+{
+	module_put(adap->owner);
+}
+EXPORT_SYMBOL(tdm_put_adapter);
+
+
+/* Port Level APIs of TDM Framework */
+unsigned int tdm_port_open(struct tdm_driver *driver, void **h_port)
+{
+	struct tdm_port *port;
+	struct tdm_adapter *adap;
+	unsigned long		flags;
+	int res = TDM_E_OK;
+
+	if (driver == NULL) {
+		pr_err("driver NULL\n");
+		return -ENODEV;
+	}
+	if (driver->adapter == NULL) {
+		pr_err("adapter NULL\n");
+		return -ENODEV;
+	}
+
+	adap = tdm_get_adapter(driver->adapter->id);
+	if (!adap)
+		return -ENODEV;
+
+	/* This creates an anonymous tdm_port, which may later be
+	 * pointed to some slot.
+	 *
+	 */
+	port = kzalloc(sizeof(*port), GFP_KERNEL);
+	if (!port) {
+		res = -ENOMEM;
+		goto out;
+	}
+
+	port->rx_max_frames = NUM_SAMPLES_PER_FRAME;
+	port->port_cfg.port_mode = e_TDM_PORT_CHANNELIZED;
+
+	port->in_use = 1;
+
+	snprintf(driver->name, TDM_NAME_SIZE, "tdm-dev");
+	port->driver = driver;
+	port->adapter = adap;
+
+	spin_lock_irqsave(&adap->portlist_lock, flags);
+	list_add_tail(&port->list, &adap->myports);
+	spin_unlock_irqrestore(&adap->portlist_lock, flags);
+
+	INIT_LIST_HEAD(&port->mychannels);
+
+	*h_port = port;
+
+out:
+	return res;
+}
+EXPORT_SYMBOL(tdm_port_open);
+
+unsigned int tdm_port_close(void *h_port)
+{
+	struct tdm_adapter *adap;
+	struct tdm_driver *driver;
+	struct tdm_port *port;
+	struct tdm_channel *temp, *channel;
+	unsigned long		flags;
+	int res = TDM_E_OK;
+	port = (struct tdm_port *)h_port;
+
+	if (port == NULL) { /* invalid handle*/
+		pr_err("Invalid Handle");
+		return -ENXIO;
+	}
+
+	driver =  port->driver;
+
+	if (driver == NULL) {
+		pr_err("driver NULL\n");
+		res = -ENODEV;
+		goto out;
+	}
+	if (driver->adapter == NULL) {
+		pr_err("adapter NULL\n");
+		res = -ENODEV;
+		goto out;
+	}
+
+	list_for_each_entry_safe(channel, temp, &port->mychannels, list) {
+	if (channel)
+		if (channel->in_use) {
+			pr_err("%s: Cannot close port. Channel in use\n",
+								__func__);
+			res = -ENXIO;
+			goto out;
+			}
+	}
+	adap = driver->adapter;
+
+	spin_lock_irqsave(&adap->portlist_lock, flags);
+	list_del(&port->list);
+	spin_unlock_irqrestore(&adap->portlist_lock, flags);
+
+	if (port->p_port_data != NULL) {
+		int i;
+		struct tdm_bd *ch_bd;
+
+		/* If the tdm is in channelised mode,
+		de-allocate the channelised buffer */
+		ch_bd = &(port->p_port_data->rx_data_fifo[0]);
+		for (i = 0; ch_bd && i < TDM_CH_RX_BD_RING_SIZE; i++) {
+			ch_bd->flag = 0;
+			ch_bd++;
+		}
+		ch_bd = &(port->p_port_data->tx_data_fifo[0]);
+		for (i = 0; ch_bd && i < TDM_CH_TX_BD_RING_SIZE; i++) {
+			ch_bd->flag = 0;
+			ch_bd++;
+		}
+		kfree(port->p_port_data);
+	}
+	kfree(port);
+	return res;
+out:
+	if (port)
+		kfree(port->p_port_data);
+	kfree(port);
+	return res;
+}
+EXPORT_SYMBOL(tdm_port_close);
+
+unsigned int tdm_channel_read(void *h_port, void *h_channel,
+				void *p_data, u16 *size)
+{
+	struct tdm_port *port;
+	struct tdm_channel *channel;
+	struct tdm_bd *rx_bd;
+	unsigned long flags;
+	int i, res = TDM_E_OK;
+	unsigned short *buf, *buf1;
+	port = (struct tdm_port *)h_port;
+	channel = (struct tdm_channel *)h_channel;
+
+	if ((port && channel) == 0) { /* invalid handle*/
+		pr_err("%s:Invalid Handle\n", __func__);
+		return -ENXIO;
+	}
+
+	if (!port->in_use)
+		return -EIO;
+	if (!channel->p_ch_data || !channel->in_use)
+		return -EIO;
+
+	spin_lock_irqsave(&channel->p_ch_data->rx_channel_lock, flags);
+	rx_bd = channel->p_ch_data->rx_out_data;
+
+	if (rx_bd->flag) {
+		*size = rx_bd->length;
+		buf = (u16 *) p_data;
+		buf1 = (u16 *)rx_bd->p_data;
+		for (i = 0; i < NUM_SAMPLES_PER_FRAME; i++)
+			buf[i] = buf1[i];
+		rx_bd->flag = 0;
+		rx_bd->offset = 0;
+		channel->p_ch_data->rx_out_data = (rx_bd->wrap) ?
+				channel->p_ch_data->rx_data_fifo : rx_bd + 1;
+
+	} else {
+		spin_unlock_irqrestore(&channel->p_ch_data->rx_channel_lock,
+						flags);
+		pr_info("No Data Available");
+		return -EAGAIN;
+	}
+	spin_unlock_irqrestore(&channel->p_ch_data->rx_channel_lock, flags);
+
+	return res;
+}
+EXPORT_SYMBOL(tdm_channel_read);
+
+
+unsigned int tdm_channel_write(void *h_port, void *h_channel,
+				void *p_data, u16 size)
+{
+	struct tdm_port *port;
+	struct tdm_channel *channel;
+	struct tdm_bd *tx_bd;
+	unsigned long flags;
+	int err = TDM_E_OK;
+	port = (struct tdm_port *)h_port;
+	channel = (struct tdm_channel *)h_channel;
+#ifdef TDM_CORE_DEBUG
+	bool data_flag = 0;
+#endif
+
+	if ((port && channel) == 0) { /* invalid handle*/
+		pr_err("Invalid Handle");
+		return -ENXIO;
+	}
+
+	if (p_data == NULL) { /* invalid data*/
+		pr_err("Invalid Data");
+		return -EFAULT;
+	}
+
+	if (!port->in_use)
+		return -EIO;
+	if (!channel->p_ch_data || !channel->in_use)
+		return -EIO;
+
+	spin_lock_irqsave(&channel->p_ch_data->tx_channel_lock, flags);
+	tx_bd = channel->p_ch_data->tx_in_data;
+
+	if (!tx_bd->flag) {
+		tx_bd->length = size;
+		memcpy(tx_bd->p_data, p_data,
+			size * port->adapter->adapt_cfg.slot_width);
+		tx_bd->flag = 1;
+		tx_bd->offset = 0;
+		channel->p_ch_data->tx_in_data = (tx_bd->wrap) ?
+				channel->p_ch_data->tx_data_fifo : tx_bd+1;
+		port->port_stat.tx_pkt_count++;
+#ifdef TDM_CORE_DEBUG
+		data_flag = 1;
+#endif
+	} else {
+		spin_unlock_irqrestore(&channel->p_ch_data->tx_channel_lock,
+						flags);
+		port->port_stat.tx_pkt_drop_count++;
+		pr_err("E_NO_MEMORY -Failed Transmit");
+		return -ENOMEM;
+	}
+	spin_unlock_irqrestore(&channel->p_ch_data->tx_channel_lock, flags);
+
+#ifdef	TDM_CORE_DEBUG
+	if (data_flag) {
+		int k;
+		pr_info("\nTX port:%d - Write - Port TX-%d\n",
+						port->port_id, size);
+		for (k = 0; k < size; k++)
+			pr_info("%x", p_data[k]);
+		pr_info("\n");
+	}
+#endif
+	return err;
+}
+EXPORT_SYMBOL(tdm_channel_write);
+
+/* Driver Function for select and poll. Based on Channel, it sleeps on
+ * waitqueue */
+unsigned int tdm_ch_poll(void *h_channel, unsigned int wait_time)
+{
+	struct tdm_channel *channel;
+	unsigned long timeout = msecs_to_jiffies(wait_time);
+	channel = h_channel;
+
+	if (!channel->p_ch_data || !channel->in_use)
+		return -EIO;
+
+	if (channel->p_ch_data->rx_out_data->flag) {
+		pr_debug("Data Available");
+		return TDM_E_OK;
+	}
+	if (timeout) {
+		wait_event_interruptible_timeout(channel->ch_wait_queue,
+					  channel->p_ch_data->rx_out_data->flag,
+					  timeout);
+
+		if (channel->p_ch_data->rx_out_data->flag) {
+			pr_debug("Data Available");
+			return TDM_E_OK;
+		}
+	}
+	return -EAGAIN;
+}
+EXPORT_SYMBOL(tdm_ch_poll);
+
+unsigned int tdm_port_get_stats(void *h_port, struct tdm_port_stats *portStat)
+{
+	struct tdm_port *port;
+	int port_num;
+	port = (struct tdm_port *)h_port;
+
+	if (port == NULL || portStat == NULL) { /* invalid handle*/
+		pr_err("Invalid Handle");
+		return -ENXIO;
+	}
+	port_num =  port->port_id;
+
+	memcpy(portStat, &port->port_stat, sizeof(struct tdm_port_stats));
+
+	pr_info("TDM Port %d Get Stats", port_num);
+
+	return TDM_E_OK;
+}
+EXPORT_SYMBOL(tdm_port_get_stats);
+
+/* Data handling functions */
+
+static int tdm_data_rx_deinterleave(struct tdm_adapter *adap)
+{
+	struct tdm_port *port, *next;
+	struct tdm_channel *channel, *temp;
+	struct tdm_bd	*ch_bd;
+
+	int i, buf_size, ch_data_len;
+	u16 *input_tdm_buffer;
+	u16 *pcm_buffer;
+	int slot_width;
+	int frame_ch_data_size;
+	bool ch_data;
+	int bytes_in_fifo_per_frame;
+	int bytes_slot_offset;
+
+	ch_data_len = NUM_SAMPLES_PER_FRAME;
+	frame_ch_data_size = NUM_SAMPLES_PER_FRAME;
+	ch_data = 0;
+
+	if (!adap) { /* invalid handle*/
+		pr_err("%s: Invalid Handle\n", __func__);
+		return -ENXIO;
+	}
+
+	slot_width = adap->adapt_cfg.slot_width;
+	buf_size = tdm_adap_recv(adap, (void **)&input_tdm_buffer);
+	if (buf_size <= 0 || !input_tdm_buffer)
+		return -EINVAL;
+
+	bytes_in_fifo_per_frame = buf_size/frame_ch_data_size;
+	bytes_slot_offset = bytes_in_fifo_per_frame/slot_width;
+
+	/* de-interleaving for all ports*/
+	list_for_each_entry_safe(port, next, &adap->myports, list) {
+
+		/* if the port is not open */
+		if (!port->in_use)
+			continue;
+
+		list_for_each_entry_safe(channel, temp, &port->mychannels,
+							list) {
+		/* if the channel is not open */
+		if (!channel->in_use || !channel->p_ch_data)
+			continue;
+		ch_bd = channel->p_ch_data->rx_in_data;
+		spin_lock(&channel->p_ch_data->rx_channel_lock);
+			/*if old data is to be discarded */
+		if (use_latest_tdm_data)
+			if (ch_bd->flag) {
+				ch_bd->flag = 0;
+				ch_bd->offset = 0;
+				if (ch_bd == channel->p_ch_data->rx_out_data)
+					channel->p_ch_data->rx_out_data =
+						ch_bd->wrap ?
+						channel->p_ch_data->rx_data_fifo
+						: ch_bd+1;
+					port->port_stat.rx_pkt_drop_count++;
+				}
+			/* if the bd is empty */
+			if (!ch_bd->flag) {
+				if (ch_bd->offset == 0)
+					ch_bd->length = port->rx_max_frames;
+
+				pcm_buffer = ch_bd->p_data + ch_bd->offset;
+				/* De-interleaving the data */
+				for (i = 0; i < ch_data_len; i++) {
+					pcm_buffer[i]
+					= input_tdm_buffer[i*bytes_slot_offset +
+						channel->ch_id];
+				}
+				ch_bd->offset += ch_data_len * slot_width;
+
+				if (ch_bd->offset >=
+					(ch_bd->length - frame_ch_data_size)*
+						(adap->adapt_cfg.slot_width)) {
+					ch_bd->flag = 1;
+					ch_bd->offset = 0;
+					channel->p_ch_data->rx_in_data =
+						ch_bd->wrap ?
+						channel->p_ch_data->rx_data_fifo
+						: ch_bd+1;
+					ch_data = 1;
+					wake_up_interruptible
+						(&channel->ch_wait_queue);
+				}
+			} else {
+				port->port_stat.rx_pkt_drop_count++;
+			}
+		spin_unlock(&channel->p_ch_data->rx_channel_lock);
+		}
+
+		if (ch_data) {
+			/*	Wake up the Port Data Poll event */
+#ifdef	TDM_CORE_DEBUG
+			pr_info("Port RX-%d-%d\n", channel->ch_id, ch_data_len);
+			for (i = 0; i < ch_data_len; i++)
+				pr_info("%x", pcm_buffer[i]);
+			pr_info("\n");
+#endif
+			port->port_stat.rx_pkt_count++;
+			ch_data = 0;
+		}
+	}
+	return TDM_E_OK;
+}
+
+static int tdm_data_tx_interleave(struct tdm_adapter *adap)
+{
+	struct tdm_port *port, *next;
+	struct tdm_channel *channel, *temp;
+	struct tdm_bd	*ch_bd;
+	int i, buf_size, ch_data_len = NUM_SAMPLES_PER_FRAME;
+	bool last_data = 0;
+	u16 *output_tdm_buffer;
+	u16 *pcm_buffer;
+	int frame_ch_data_size = NUM_SAMPLES_PER_FRAME;
+	int bytes_in_fifo_per_frame;
+	int bytes_slot_offset;
+
+#ifdef TDM_CORE_DEBUG
+	u8	data_flag = 0;
+#endif
+
+	if (adap == NULL) { /* invalid handle*/
+		pr_err("%s: Invalid Handle\n", __func__);
+		return -ENXIO;
+	}
+
+	buf_size = tdm_adap_get_write_buf(adap, (void **)&output_tdm_buffer);
+	if (buf_size <= 0 || !output_tdm_buffer)
+		return -EINVAL;
+
+	bytes_in_fifo_per_frame = buf_size/frame_ch_data_size;
+	bytes_slot_offset = bytes_in_fifo_per_frame/adap->adapt_cfg.slot_width;
+
+
+	memset(output_tdm_buffer, 0, sizeof(buf_size));
+
+	list_for_each_entry_safe(port, next, &adap->myports, list) {
+
+		/* check if the port is open */
+		if (!port->in_use)
+			continue;
+
+		list_for_each_entry_safe(channel, temp, &port->mychannels,
+								list) {
+		pr_debug("TX-Tdm %d (slots-)", channel->ch_id);
+
+
+		/* if the channel is open */
+		if (!channel->in_use || !channel->p_ch_data)
+			continue;
+
+		spin_lock(&channel->p_ch_data->tx_channel_lock);
+		if (!channel->in_use || !channel->p_ch_data)
+			continue;
+			ch_bd = channel->p_ch_data->tx_out_data;
+			if (ch_bd->flag) {
+				pcm_buffer = (u16 *)((uint8_t *)ch_bd->p_data +
+						ch_bd->offset);
+				/*if the buffer has less frames than required */
+				if (frame_ch_data_size >=
+					((ch_bd->length) - (ch_bd->offset/
+						adap->adapt_cfg.slot_width))) {
+					ch_data_len =
+					(ch_bd->length) - (ch_bd->offset/
+						adap->adapt_cfg.slot_width);
+					last_data = 1;
+				} else {
+					ch_data_len = frame_ch_data_size;
+				}
+				/* Interleaving the data */
+				for (i = 0; i < ch_data_len; i++) {
+					/* TODO- need to be genric for any size
+					   assignment*/
+					output_tdm_buffer[channel->ch_id +
+						bytes_slot_offset * i] =
+								pcm_buffer[i];
+				}
+				/* If all the data of this buffer is
+							transmitted */
+				if (last_data) {
+					ch_bd->flag = 0;
+					ch_bd->offset = 0;
+					channel->p_ch_data->tx_out_data =
+						ch_bd->wrap ?
+						channel->p_ch_data->tx_data_fifo
+						: ch_bd+1;
+					port->port_stat.tx_pkt_conf_count++;
+				} else {
+					ch_bd->offset += ch_data_len *
+						(adap->adapt_cfg.slot_width);
+				}
+#ifdef	TDM_CORE_DEBUG
+				data_flag = 1;
+#endif
+			}
+		spin_unlock(&channel->p_ch_data->tx_channel_lock);
+		}
+	}
+
+#ifdef	TDM_CORE_DEBUG
+	if (data_flag) {
+		pr_info("TX-TDM Interleaved Data-\n");
+		for (i = 0; i < 64; i++)
+			pr_info("%x", output_tdm_buffer[i]);
+		pr_info("\n");
+	  }
+#endif
+	return TDM_E_OK;
+}
+
+/* Channel Level APIs of TDM Framework */
+int tdm_channel_open(u16 chanid, u16 ch_width, struct tdm_port *port,
+				void **h_channel)
+{
+	struct tdm_channel *channel, *temp;
+	unsigned long		flags;
+	struct tdm_ch_data	*p_ch_data;
+	int res = TDM_E_OK;
+
+	if (!(port && h_channel)) {
+		pr_err("%s: Invalid handle\n", __func__);
+		return -EINVAL;
+	}
+
+	if (ch_width != 1) {
+		pr_err("%s: Mode not supported\n", __func__);
+		return -EINVAL;
+	}
+
+	list_for_each_entry_safe(channel, temp, &port->mychannels, list) {
+		if (channel->ch_id == chanid) {
+			pr_err("%s: Channel %d already open\n",
+						__func__, chanid);
+			return -EINVAL;
+		}
+	}
+
+	channel = kzalloc(sizeof(*channel), GFP_KERNEL);
+	if (!channel) {
+		res = -ENOMEM;
+		goto out;
+	}
+
+	init_waitqueue_head(&channel->ch_wait_queue);
+	p_ch_data = kzalloc(sizeof(struct tdm_ch_data), GFP_KERNEL);
+	if (!p_ch_data) {
+		res = -ENOMEM;
+		goto outdata;
+	}
+
+	p_ch_data->rx_data_fifo[TDM_CH_RX_BD_RING_SIZE-1].wrap = 1;
+	p_ch_data->tx_data_fifo[TDM_CH_TX_BD_RING_SIZE-1].wrap = 1;
+
+	p_ch_data->rx_in_data = p_ch_data->rx_data_fifo;
+	p_ch_data->rx_out_data = p_ch_data->rx_data_fifo;
+	p_ch_data->tx_in_data = p_ch_data->tx_data_fifo;
+	p_ch_data->tx_out_data = p_ch_data->tx_data_fifo;
+	spin_lock_init(&p_ch_data->rx_channel_lock);
+	spin_lock_init(&p_ch_data->tx_channel_lock);
+
+	channel->p_ch_data = p_ch_data;
+
+	channel->ch_id = chanid;
+	channel->ch_cfg.first_slot = chanid;
+	channel->ch_cfg.num_slots = 1;	/* This is 1 for channelized mode and
+						configurable for other modes */
+	channel->port = port;
+	channel->in_use = 1;
+
+	spin_lock_irqsave(&port->ch_list_lock, flags);
+	list_add_tail(&channel->list, &port->mychannels);
+	spin_unlock_irqrestore(&port->ch_list_lock, flags);
+
+	*h_channel = channel;
+
+	return res;
+
+outdata:
+	kfree(channel);
+out:
+	return res;
+}
+EXPORT_SYMBOL(tdm_channel_open);
+
+int tdm_channel_close(u16 chanid, u16 ch_width, struct tdm_port *port,
+				struct tdm_channel *h_channel)
+{
+	struct tdm_channel *channel;
+	unsigned long		flags;
+	int res = TDM_E_OK;
+	channel = h_channel;
+
+	if (!(port && channel)) {
+		pr_err("%s: Invalid handle\n", __func__);
+		res = -EINVAL;
+		goto out;
+	}
+
+	if (ch_width != 1) {
+		pr_err("%s: Mode not supported\n", __func__);
+		res = -EINVAL;
+		goto out;
+	}
+
+	spin_lock_irqsave(&port->ch_list_lock, flags);
+	list_del(&channel->list);
+	spin_unlock_irqrestore(&port->ch_list_lock, flags);
+
+out:
+	if (channel)
+		kfree(channel->p_ch_data);
+	kfree(channel);
+	return res;
+}
+EXPORT_SYMBOL(tdm_channel_close);
+
+void init_config_adapter(struct tdm_adapter *adap)
+{
+	struct fsl_tdm_adapt_cfg default_adapt_cfg = {
+		.loopback = e_TDM_PROCESS_NORMAL,
+		.num_ch = NUM_CHANNELS,
+		.ch_size_type = CHANNEL_16BIT_LIN,
+		.frame_len = NUM_SAMPLES_PER_FRAME,
+		.num_frames = NUM_SAMPLES_PER_FRAME,
+		.adap_mode = e_TDM_ADAPTER_MODE_NONE
+			 };
+
+	default_adapt_cfg.slot_width = default_adapt_cfg.ch_size_type/3 + 1;
+
+	memcpy(&adap->adapt_cfg, &default_adapt_cfg,
+		sizeof(struct fsl_tdm_adapt_cfg));
+
+	return;
+}
+EXPORT_SYMBOL(init_config_adapter);
+
+static void tdm_data_tasklet_fn(unsigned long data)
+{
+	struct tdm_adapter *adapter;
+	adapter = (struct tdm_adapter *)data;
+	if (adapter != NULL) {
+		tdm_data_tx_interleave(adapter);
+		tdm_data_rx_deinterleave(adapter);
+	}
+}
+
+
+MODULE_AUTHOR("Hemant Agrawal <hemant@freescale.com>");
+MODULE_DESCRIPTION("TDM Driver Framework Core");
+MODULE_LICENSE("GPL");
diff --git a/include/linux/fsl/immap_qe.h b/include/linux/fsl/immap_qe.h
index bedbff8..238d3c8 100644
--- a/include/linux/fsl/immap_qe.h
+++ b/include/linux/fsl/immap_qe.h
@@ -159,10 +159,7 @@ struct spi {
 
 /* SI */
 struct si1 {
-	__be16	siamr1;		/* SI1 TDMA mode register */
-	__be16	sibmr1;		/* SI1 TDMB mode register */
-	__be16	sicmr1;		/* SI1 TDMC mode register */
-	__be16	sidmr1;		/* SI1 TDMD mode register */
+	__be16	sixmr1[4];		/* SI1 TDMD mode register */
 	u8	siglmr1_h;	/* SI1 global mode register high */
 	u8	res0[0x1];
 	u8	sicmdr1_h;	/* SI1 command register high */
diff --git a/include/linux/fsl/qe.h b/include/linux/fsl/qe.h
index 1bd9bd1..a28817a 100644
--- a/include/linux/fsl/qe.h
+++ b/include/linux/fsl/qe.h
@@ -76,6 +76,8 @@ enum qe_clock {
 	QE_CLK22,		/* Clock 22 */
 	QE_CLK23,		/* Clock 23 */
 	QE_CLK24,		/* Clock 24 */
+	QE_RSYNC_PIN,           /* RSYNC from pin */
+	QE_TSYNC_PIN,           /* TSYNC from pin */
 	QE_CLK_DUMMY
 };
 
@@ -692,6 +694,14 @@ struct ucc_slow_pram {
 #define UCC_BISYNC_UCCE_TXB	0x0002
 #define UCC_BISYNC_UCCE_RXB	0x0001
 
+/* Transparent UCC Event Register (UCCE) */
+#define UCC_TRANS_UCCE_GRA     0x0080
+#define UCC_TRANS_UCCE_TXE     0x0010
+#define UCC_TRANS_UCCE_RXF     0x0008
+#define UCC_TRANS_UCCE_BSY     0x0004
+#define UCC_TRANS_UCCE_TXB     0x0002
+#define UCC_TRANS_UCCE_RXB     0x0001
+
 /* Gigabit Ethernet Fast UCC Event Register (UCCE) */
 #define UCC_GETH_UCCE_MPD       0x80000000
 #define UCC_GETH_UCCE_SCAR      0x40000000
diff --git a/include/linux/fsl/ucc.h b/include/linux/fsl/ucc.h
index d448813..9278a6f 100644
--- a/include/linux/fsl/ucc.h
+++ b/include/linux/fsl/ucc.h
@@ -42,6 +42,11 @@ int ucc_set_qe_mux_mii_mng(unsigned int ucc_num);
 int ucc_set_qe_mux_rxtx(unsigned int ucc_num, enum qe_clock clock,
 	enum comm_dir mode);
 
+int ucc_set_tdm_rxtx_clk(unsigned int tdm_num, enum qe_clock clock,
+			enum comm_dir mode);
+int ucc_set_tdm_rxtx_sync(unsigned int tdm_num, enum qe_clock clock,
+			enum comm_dir mode);
+
 int ucc_mux_set_grant_tsa_bkpt(unsigned int ucc_num, int set, u32 mask);
 
 /* QE MUX clock routing for UCC
diff --git a/include/linux/fsl/ucc_fast.h b/include/linux/fsl/ucc_fast.h
index 101fae7..1862819 100644
--- a/include/linux/fsl/ucc_fast.h
+++ b/include/linux/fsl/ucc_fast.h
@@ -27,12 +27,14 @@
 #define R_I	0x10000000	/* interrupt on reception */
 #define R_L	0x08000000	/* last */
 #define R_F	0x04000000	/* first */
+#define R_CM	0x02000000      /* CM */
 
 /* transmit BD's status */
 #define T_R	0x80000000	/* ready bit */
 #define T_W	0x20000000	/* wrap bit */
 #define T_I	0x10000000	/* interrupt on completion */
 #define T_L	0x08000000	/* last */
+#define T_CM	0x02000000      /* CM */
 
 /* Rx Data buffer must be 4 bytes aligned in most cases */
 #define UCC_FAST_RX_ALIGN			4
@@ -118,8 +120,11 @@ enum ucc_fast_transparent_tcrc {
 /* Fast UCC initialization structure */
 struct ucc_fast_info {
 	int ucc_num;
+	int tdm_num;
 	enum qe_clock rx_clock;
 	enum qe_clock tx_clock;
+	enum qe_clock rx_sync;
+	enum qe_clock tx_sync;
 	u32 regs;
 	int irq;
 	u32 uccm_mask;
@@ -150,6 +155,7 @@ struct ucc_fast_info {
 	enum ucc_fast_rx_decoding_method renc;
 	enum ucc_fast_transparent_tcrc tcrc;
 	enum ucc_fast_sync_len synl;
+	enum ucc_fast_diag_mode diag;
 };
 
 struct ucc_fast_private {
diff --git a/include/linux/idr.h b/include/linux/idr.h
index 871a213..20ab55d 100644
--- a/include/linux/idr.h
+++ b/include/linux/idr.h
@@ -52,6 +52,8 @@ struct idr {
 }
 #define DEFINE_IDR(name)	struct idr name = IDR_INIT(name)
 
+#define _idr_rc_to_errno(rc) ((rc) == -1 ? -EAGAIN : -ENOSPC)
+
 /**
  * DOC: idr sync
  * idr synchronization (stolen from radix-tree.h)
@@ -137,6 +139,7 @@ static inline void *idr_find(struct idr *idr, int id)
  * deprecated warnings on EXPORT_SYMBOL()s.
  */
 int __idr_pre_get(struct idr *idp, gfp_t gfp_mask);
+int __idr_get_new(struct idr *idp, void *ptr, int *id);
 int __idr_get_new_above(struct idr *idp, void *ptr, int starting_id, int *id);
 void __idr_remove_all(struct idr *idp);
 
diff --git a/include/linux/mod_devicetable.h b/include/linux/mod_devicetable.h
index f2ac87c..fcdb23a 100644
--- a/include/linux/mod_devicetable.h
+++ b/include/linux/mod_devicetable.h
@@ -422,6 +422,17 @@ struct i2c_device_id {
 	kernel_ulong_t driver_data;	/* Data private to the driver */
 };
 
+/* tdm */
+
+#define TDM_NAME_SIZE   20
+#define TDM_MODULE_PREFIX "tdm:"
+
+struct tdm_device_id {
+	char name[TDM_NAME_SIZE];
+	kernel_ulong_t driver_data
+		__attribute__((aligned(sizeof(kernel_ulong_t))));
+};
+
 /* spi */
 
 #define SPI_NAME_SIZE	32
diff --git a/include/linux/tdm.h b/include/linux/tdm.h
new file mode 100644
index 0000000..d6d3826
--- /dev/null
+++ b/include/linux/tdm.h
@@ -0,0 +1,355 @@
+/* include/linux/tdm.h
+ *
+ * Copyright 2012 Freescale Semiconductor, Inc.
+ *
+ * tdm.h - definitions for the tdm-device framework interface
+ *
+ * Author:Hemant Agrawal <hemant@freescale.com>
+ *	Rajesh Gumasta <rajesh.gumasta@freescale.com>
+ *
+ * This program is free software; you can redistribute  it and/or modify it
+ * under  the terms of  the GNU General  Public License as published by the
+ * Free Software Foundation;  either version 2 of the  License, or (at your
+ * option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the  GNU General Public License along
+ * with this program; if not, write  to the Free Software Foundation, Inc.,
+ * 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+
+#ifndef _LINUX_TDM_H
+#define _LINUX_TDM_H
+
+#ifdef __KERNEL__
+#include <linux/device.h>	/* for struct device */
+#include <linux/interrupt.h>
+#include <linux/mod_devicetable.h>
+#include <linux/module.h>
+#include <linux/mutex.h>
+#include <linux/sched.h>	/* for completion */
+#include <linux/types.h>
+
+#define CHANNEL_8BIT_LIN	0	/* 8 bit linear */
+#define CHANNEL_8BIT_ULAW	1	/* 8 bit Mu-law */
+#define CHANNEL_8BIT_ALAW	2	/* 8 bit A-law */
+#define CHANNEL_16BIT_LIN	3	/* 16 bit Linear */
+
+#define NUM_CHANNELS		16
+#define NUM_SAMPLES_PER_MS	8		/* 8 samples per milli sec per
+						 channel. Req for voice data */
+#define NUM_MS			10
+#define NUM_SAMPLES_PER_FRAME	(NUM_MS * NUM_SAMPLES_PER_MS) /* Number of
+						samples for 1 client buffer */
+#define NUM_OF_TDM_BUF		3
+
+/* General options */
+
+struct tdm_adapt_algorithm;
+struct tdm_adapter;
+struct tdm_port;
+struct tdm_driver;
+
+/* Align addr on a size boundary - adjust address up if needed */
+/* returns min value greater than size which is multiple of alignment */
+static inline int ALIGN_SIZE(u64 size, u32 alignment)
+{
+	return (size + alignment - 1) & (~(alignment - 1));
+}
+
+int tdm_master_send(struct tdm_adapter *adap, void **buf, int count);
+int tdm_master_recv(struct tdm_adapter *adap, void **buf);
+int tdm_read_direct(struct tdm_adapter *adap, u8 *buf, u32 len);
+int tdm_write_direct(struct tdm_adapter *adap, u8 *buf, u32 len);
+
+/**
+ * struct tdm_driver - represent an TDM device driver
+ * @class: What kind of tdm device we instantiate (for detect)
+ * @id:Driver id
+ * @name: Name of the driver
+ * @attach_adapter: Callback for device addition (for legacy drivers)
+ * @detach_adapter: Callback for device removal (for legacy drivers)
+ * @probe: Callback for device binding
+ * @remove: Callback for device unbinding
+ * @shutdown: Callback for device shutdown
+ * @suspend: Callback for device suspend
+ * @resume: Callback for device resume
+ * @command: Callback for sending commands to device
+ * @id_table: List of TDM devices supported by this driver
+ * @list: List of drivers created (for tdm-core use only)
+ */
+struct tdm_driver {
+	unsigned int class;
+	unsigned int id;
+	char name[TDM_NAME_SIZE];
+
+	int (*attach_adapter)(struct tdm_adapter *);
+	int (*detach_adapter)(struct tdm_adapter *);
+
+	/* Standard driver model interfaces */
+	int (*probe)(const struct tdm_device_id *);
+	int (*remove)(void);
+
+	/* driver model interfaces that don't relate to enumeration */
+	void (*shutdown)(void);
+	int (*suspend)(pm_message_t mesg);
+	int (*resume)(void);
+
+	/* a ioctl like command that can be used to perform specific functions
+	 * with the device.
+	 */
+	int (*command)(unsigned int cmd, void *arg);
+
+	const struct tdm_device_id *id_table;
+
+	/* The associated adapter for this driver */
+	struct tdm_adapter *adapter;
+	struct list_head list;
+};
+
+/* tdm per port statistics structure, used for providing and storing tdm port
+ * statistics.
+ */
+struct tdm_port_stats {
+	unsigned int rx_pkt_count;	/* Rx frame count per channel */
+	unsigned int rx_pkt_drop_count;	/* Rx drop count per channel to
+					 clean space for new buffer */
+	unsigned int tx_pkt_count;	/* Tx frame count per channel */
+	unsigned int tx_pkt_conf_count;	/* Tx frame confirmation count per
+					 channel */
+	unsigned int tx_pkt_drop_count;	/* Tx drop count per channel due to
+					 queue full */
+};
+
+
+/* tdm Buffer Descriptor, used for Creating Interleaved and De-interleaved
+ * FIFOs
+ */
+struct tdm_bd {
+	unsigned char flag;		/* BD is full or empty */
+	unsigned char wrap;		/* BD is last in the queue */
+	unsigned short length;	/* Length of Data in BD */
+	/*TODO: use dyanmic memory */
+	unsigned short p_data[NUM_SAMPLES_PER_FRAME];	/* Data Pointer */
+	unsigned long offset;	/* Offset of the Data Pointer to be used */
+};
+
+#define TDM_CH_RX_BD_RING_SIZE	3
+#define TDM_CH_TX_BD_RING_SIZE	3
+
+/* tdm RX-TX Channelised Data */
+struct tdm_port_data {
+	struct tdm_bd rx_data_fifo[TDM_CH_RX_BD_RING_SIZE]; /* Rx Channel Data
+								BD Ring */
+	struct tdm_bd *rx_in_data;	/* Current Channel Rx BD to be filled by
+						de-interleave function */
+	struct tdm_bd *rx_out_data;	/* Current Channel Rx BD to be
+							read by App */
+	struct tdm_bd tx_data_fifo[TDM_CH_TX_BD_RING_SIZE]; /* Tx Channel Data
+								BD Ring */
+	struct tdm_bd *tx_in_data;	/* Current Channel Tx BD to be
+						 filled by App */
+	struct tdm_bd *tx_out_data;	/* Current Channel Tx BD to be read by
+						interleave function */
+	spinlock_t rx_channel_lock;	/* Spin Lock for Rx Channel */
+	spinlock_t tx_channel_lock;	/* Spin Lock for Tx Channel */
+};
+
+/* structure tdm_port_cfg - contains configuration params for a port */
+struct tdm_port_cfg {
+	unsigned short port_mode;
+};
+
+/* struct tdm_port - represent an TDM ports for a device */
+struct tdm_port {
+	unsigned short port_id;
+	unsigned short in_use;		/* Port is enabled? */
+	uint16_t rx_max_frames;		/* Received Port frames
+					 before allowing Read Operation in
+					 Port Mode */
+
+	struct tdm_port_stats port_stat;/* A structure parameters defining
+					 TDM port statistics. */
+	struct tdm_port_data *p_port_data;	/* a structure parameters
+						defining tdm channelised data */
+
+	struct tdm_driver *driver;	/* driver for this port */
+	struct tdm_adapter *adapter;	/* adapter for this port */
+	struct list_head list;		/* list of ports */
+	struct list_head mychannels;	/* list of channels, created on this
+					 port*/
+	spinlock_t ch_list_lock;	/* Spin Lock for channel_list */
+	struct tdm_port_cfg port_cfg;/* A structure parameters defining
+					 TDM port configuration. */
+};
+
+/* tdm RX-TX Channelised Data */
+struct tdm_ch_data {
+	struct tdm_bd rx_data_fifo[TDM_CH_RX_BD_RING_SIZE]; /* Rx Port Data BD
+								Ring */
+	struct tdm_bd *rx_in_data;	/* Current Port Rx BD to be filled by
+						de-interleave function */
+	struct tdm_bd *rx_out_data; /* Current Port Rx BD to be read by App */
+	struct tdm_bd tx_data_fifo[TDM_CH_TX_BD_RING_SIZE]; /* Tx Port Data BD
+								Ring */
+	struct tdm_bd *tx_in_data;	/* Current Port Tx BD to be filled by
+						App */
+	struct tdm_bd *tx_out_data;	/* Current Port Tx BD to be read by
+						interleave function */
+	spinlock_t rx_channel_lock;	/* Spin Lock for Rx Port */
+	spinlock_t tx_channel_lock;	/* Spin Lock for Tx Port */
+};
+
+/* Channel config params */
+struct tdm_ch_cfg {
+	unsigned short num_slots;
+	unsigned short first_slot;
+};
+
+/* struct tdm_channel- represent a TDM channel for a port */
+struct tdm_channel {
+	u16 ch_id;			/* logical channel number */
+	struct list_head list;		/* list of channels in a port*/
+	struct tdm_port *port;		/* port for this channel */
+	u16 in_use;			/* channel is enabled? */
+	struct tdm_ch_cfg ch_cfg;	/* channel configuration */
+	struct tdm_ch_data *p_ch_data;	/* data storage space for channel */
+	wait_queue_head_t ch_wait_queue;/* waitQueue for RX Channel Data */
+};
+
+/* tdm_adapt_algorithm is for accessing the routines of device */
+struct tdm_adapt_algorithm {
+	u32 (*tdm_read)(struct tdm_adapter *, u16 **);
+	u32 (*tdm_get_write_buf)(struct tdm_adapter *, u16 **);
+	int (*tdm_read_simple)(struct tdm_adapter *, u8 *, u32 len);
+	int (*tdm_write_simple)(struct tdm_adapter *, u8 *, u32 len);
+	u32 (*tdm_write)(struct tdm_adapter *, void * , unsigned int len);
+	int (*tdm_enable)(struct tdm_adapter *);
+	int (*tdm_disable)(struct tdm_adapter *);
+};
+
+/* tdm_adapter_mode is to define in mode of the device */
+enum tdm_adapter_mode {
+	e_TDM_ADAPTER_MODE_NONE = 0x00,
+	e_TDM_ADAPTER_MODE_T1 = 0x01,
+	e_TDM_ADAPTER_MODE_E1 = 0x02,
+	e_TDM_ADAPTER_MODE_T1_RAW = 0x10,
+	e_TDM_ADAPTER_MODE_E1_RAW = 0x20,
+};
+
+/* tdm_port_mode defines the mode in which the port is configured to operate
+ * It can be channelized/full/fractional.
+ */
+enum tdm_port_mode {
+	e_TDM_PORT_CHANNELIZED = 0	/* Channelized mode */
+	, e_TDM_PORT_FULL = 1		/* Full mode */
+	, e_TDM_PORT_FRACTIONAL = 2	/* Fractional mode */
+};
+
+/* tdm_process_mode used for testing the tdm device in normal mode or internal
+ * loopback or external loopback
+ */
+enum tdm_process_mode {
+	e_TDM_PROCESS_NORMAL = 0	/* Normal mode */
+	, e_TDM_PROCESS_INT_LPB = 1	/* Internal loop mode */
+	, e_TDM_PROCESS_EXT_LPB = 2	/* External Loopback mode */
+};
+
+
+/* TDM configuration parameters */
+struct fsl_tdm_adapt_cfg {
+	u8 num_ch;		/* Number of channels in this adpater */
+	u8 ch_size_type;		/* reciever/transmit channel
+						size for all channels */
+	u8 slot_width;		/* 1 or 2 Is defined by channel type */
+	u8 frame_len;		/* Length of frame in samples */
+	u32 num_frames;
+	u8 loopback;			/* loopback or normal */
+	u8 adap_mode;			/* 0=None, 1= T1, 2= T1-FULL, 3=E1,
+						4 = E1-FULL */
+	int max_num_ports;		/* Not Used: Max Number of ports that
+					can be created on this adapter */
+	int max_timeslots;		/* Max Number of timeslots that are
+					supported on this adapter */
+};
+
+/*
+ * tdm_adapter is the structure used to identify a physical tdm device along
+ * with the access algorithms necessary to access it.
+ */
+struct tdm_adapter {
+	struct module *owner;	/* owner of the adapter module */
+	unsigned int id;	/* Adapter Id */
+	unsigned int class;	/* classes to allow probing for */
+	unsigned int drv_count;	/* Number of drivers associated with the
+				 adapter */
+
+	const struct tdm_adapt_algorithm *algo;	/* the algorithm to access the
+						 adapter*/
+
+	char name[TDM_NAME_SIZE];	/* Name of Adapter */
+	struct mutex adap_lock;
+	struct device *parent;		/*Not Used*/
+
+	struct tasklet_struct tdm_data_tasklet;	/* tasklet handle to perform
+						 data processing*/
+	int tasklet_conf;	/* flag for tasklet configuration */
+	int tdm_rx_flag;
+
+	struct list_head myports;	/* list of ports, created on this
+					 adapter */
+	struct list_head list;
+	spinlock_t portlist_lock;	/* Spin Lock for port_list */
+	void *data;
+	struct fsl_tdm_adapt_cfg adapt_cfg;
+};
+
+static inline void *tdm_get_adapdata(const struct tdm_adapter *dev)
+{
+	return dev->data;
+}
+
+static inline void tdm_set_adapdata(struct tdm_adapter *dev, void *data)
+{
+	dev->data = data;
+}
+
+/* functions exported by tdm.o */
+
+extern int tdm_add_adapter(struct tdm_adapter *);
+extern int tdm_del_adapter(struct tdm_adapter *);
+extern int tdm_register_driver(struct tdm_driver *);
+extern void tdm_del_driver(struct tdm_driver *);
+extern void tdm_unregister_driver(struct tdm_driver *);
+extern void init_config_adapter(struct tdm_adapter *);
+extern int tdm_adap_enable(struct tdm_driver *drv);
+extern int tdm_adap_disable(struct tdm_driver *drv);
+extern unsigned int tdm_port_open(struct tdm_driver *, void **);
+extern unsigned int tdm_port_close(void *);
+extern unsigned int tdm_port_ioctl(void *, unsigned int, unsigned long);
+extern unsigned int tdm_channel_read(void *, void *, void *, u16 *);
+extern unsigned int tdm_channel_write(void *, void * , void *, u16);
+extern unsigned int tdm_ch_poll(void *, unsigned int);
+
+extern int tdm_channel_open(u16, u16, struct tdm_port *, void **);
+extern int tdm_channel_close(u16, u16, struct tdm_port *,
+						struct tdm_channel *);
+
+static inline int tdm_add_driver(struct tdm_driver *driver)
+{
+	return tdm_register_driver(driver);
+}
+
+extern struct tdm_adapter *tdm_get_adapter(int id);
+extern void tdm_put_adapter(struct tdm_adapter *adap);
+
+#endif /* __KERNEL__ */
+
+#define TDM_E_OK 0
+
+#endif /* _LINUX_TDM_H */
diff --git a/lib/idr.c b/lib/idr.c
index 674c30b..ac96b4a 100644
--- a/lib/idr.c
+++ b/lib/idr.c
@@ -359,6 +359,25 @@ build_up:
 	return(v);
 }
 
+static int idr_get_new_above_int(struct idr *idp, void *ptr, int starting_id)
+{
+	struct idr_layer *pa[MAX_IDR_LEVEL];
+	int id;
+
+	id = idr_get_empty_slot(idp, starting_id, pa, 0, idp);
+	if (id >= 0) {
+		/*
+		 * Successfully found an empty slot.  Install the user
+		 * pointer and mark the slot full.
+		 */
+		rcu_assign_pointer(pa[0]->ary[id & IDR_MASK],
+				(struct idr_layer *)ptr);
+		pa[0]->count++;
+		idr_mark_full(pa, id);
+	}
+
+	return id;
+}
 /*
  * @id and @pa are from a successful allocation from idr_get_empty_slot().
  * Install the user pointer @ptr and mark the slot full.
@@ -374,6 +393,36 @@ static void idr_fill_slot(struct idr *idr, void *ptr, int id,
 	idr_mark_full(pa, id);
 }
 
+/**
+ * __idr_get_new - allocate new idr entry
+ * @idp: idr handle
+ * @ptr: pointer you want associated with the id
+ * @id: pointer to the allocated handle
+ *
+ * If allocation from IDR's private freelist fails, idr_get_new_above() will
+ * return %-EAGAIN.  The caller should retry the idr_pre_get() call to refill
+ * IDR's preallocation and then retry the idr_get_new_above() call.
+ *
+ * If the idr is full idr_get_new_above() will return %-ENOSPC.
+ *
+ * @id returns a value in the range %0 ... %0x7fffffff
+ */
+int __idr_get_new(struct idr *idp, void *ptr, int *id)
+{
+	int rv;
+
+	rv = idr_get_new_above_int(idp, ptr, 0);
+	/*
+	 * This is a cheap hack until the IDR code can be fixed to
+	 * return proper error values.
+	 */
+	if (rv < 0)
+		return _idr_rc_to_errno(rv);
+	*id = rv;
+	return 0;
+}
+EXPORT_SYMBOL(__idr_get_new);
+
 int __idr_get_new_above(struct idr *idp, void *ptr, int starting_id, int *id)
 {
 	struct idr_layer *pa[MAX_IDR_LEVEL + 1];
-- 
2.0.2

