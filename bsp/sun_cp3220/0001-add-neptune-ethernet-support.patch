From 878d94a43046bd2b6637425928e659513f58415b Mon Sep 17 00:00:00 2001
From: Jack Tan <jack.tan@windriver.com>
Date: Mon, 1 Sep 2008 16:34:15 +0800
Subject: [PATCH] add neptune ethernet support

This patch is originally taken from SUN and merged the patch to fix the
bugs which result in rmmod failure in cgl kernel, and can't set the port
to 10GB mode.

Signed-off-by: Jack Tan <jack.tan@windriver.com>
Signed-off-by: Fei Wu <fei.wu@windriver.com>
---
 drivers/net/Kconfig                         |   14 +
 drivers/net/Makefile                        |    1 +
 drivers/net/nxge/Makefile                   |   43 +
 drivers/net/nxge/README                     |  312 +
 drivers/net/nxge/include/nxge.h             | 1743 ++++
 drivers/net/nxge/include/nxge_common.h      |  458 +
 drivers/net/nxge/include/nxge_common_impl.h |  419 +
 drivers/net/nxge/include/nxge_defs.h        |  477 +
 drivers/net/nxge/include/nxge_espc.h        |  247 +
 drivers/net/nxge/include/nxge_espc_hw.h     |   75 +
 drivers/net/nxge/include/nxge_fflp.h        |  244 +
 drivers/net/nxge/include/nxge_fflp_hash.h   |   72 +
 drivers/net/nxge/include/nxge_fflp_hw.h     | 1680 ++++
 drivers/net/nxge/include/nxge_flow.h        |  212 +
 drivers/net/nxge/include/nxge_fzc.h         |  109 +
 drivers/net/nxge/include/nxge_hw.h          | 1068 ++
 drivers/net/nxge/include/nxge_impl.h        |  507 +
 drivers/net/nxge/include/nxge_ipp.h         |   94 +
 drivers/net/nxge/include/nxge_ipp_hw.h      |  262 +
 drivers/net/nxge/include/nxge_mac.h         |  256 +
 drivers/net/nxge/include/nxge_mac_hw.h      | 2452 +++++
 drivers/net/nxge/include/nxge_mii.h         |  431 +
 drivers/net/nxge/include/nxge_n2_esr_hw.h   |  374 +
 drivers/net/nxge/include/nxge_phy_hw.h      |  642 ++
 drivers/net/nxge/include/nxge_rxdma_hw.h    | 1963 ++++
 drivers/net/nxge/include/nxge_sr_hw.h       |  804 ++
 drivers/net/nxge/include/nxge_txc.h         |   94 +
 drivers/net/nxge/include/nxge_txc_hw.h      | 1281 +++
 drivers/net/nxge/include/nxge_txdma.h       |  311 +
 drivers/net/nxge/include/nxge_txdma_hw.h    |  942 ++
 drivers/net/nxge/include/nxge_virtual.h     |   91 +
 drivers/net/nxge/include/nxge_zcp.h         |   86 +
 drivers/net/nxge/include/nxge_zcp_hw.h      |  782 ++
 drivers/net/nxge/npi/npi.c                  |  104 +
 drivers/net/nxge/npi/npi.h                  |  277 +
 drivers/net/nxge/npi/npi_espc.c             |  651 ++
 drivers/net/nxge/npi/npi_espc.h             |  100 +
 drivers/net/nxge/npi/npi_fflp.c             | 2987 ++++++
 drivers/net/nxge/npi/npi_fflp.h             | 1199 +++
 drivers/net/nxge/npi/npi_ipp.c              |  704 ++
 drivers/net/nxge/npi/npi_ipp.h              |  200 +
 drivers/net/nxge/npi/npi_mac.c              | 3896 ++++++++
 drivers/net/nxge/npi/npi_mac.h              |  588 ++
 drivers/net/nxge/npi/npi_rxdma.c            | 2362 +++++
 drivers/net/nxge/npi/npi_rxdma.h            | 1348 +++
 drivers/net/nxge/npi/npi_txc.c              | 1157 +++
 drivers/net/nxge/npi/npi_txc.h              |  151 +
 drivers/net/nxge/npi/npi_txdma.c            | 2067 ++++
 drivers/net/nxge/npi/npi_txdma.h            |  303 +
 drivers/net/nxge/npi/npi_vir.c              | 1499 +++
 drivers/net/nxge/npi/npi_vir.h              |  702 ++
 drivers/net/nxge/npi/npi_zcp.c              |  771 ++
 drivers/net/nxge/npi/npi_zcp.h              |  199 +
 drivers/net/nxge/nxge_espc.c                |  399 +
 drivers/net/nxge/nxge_ethtool.c             | 2062 ++++
 drivers/net/nxge/nxge_fflp.c                | 2719 ++++++
 drivers/net/nxge/nxge_fzc.c                 |  733 ++
 drivers/net/nxge/nxge_mac.c                 | 3718 +++++++
 drivers/net/nxge/nxge_main.c                |14023 +++++++++++++++++++++++++++
 drivers/net/nxge/nxge_param.c               | 2967 ++++++
 drivers/net/nxge/nxge_rxport.c              | 1168 +++
 drivers/net/nxge/nxge_txc.c                 |  397 +
 62 files changed, 67997 insertions(+), 0 deletions(-)

diff --git a/drivers/net/Kconfig b/drivers/net/Kconfig
index 268314f..0b6037e 100644
--- a/drivers/net/Kconfig
+++ b/drivers/net/Kconfig
@@ -2377,6 +2377,20 @@ config EHEA
 	  To compile the driver as a module, choose M here. The module
 	  will be called ehea.
 
+config NXGE
+	tristate "Sun Multithreaded 10GbE support"
+	depends on PCI
+	---help---
+	  This driver supports Sun Multithreaded 10GbE family of
+	  Networking products.
+
+	   For general information, feedback and support  please contact
+	   neptune_sw@sun.com
+
+	  To compile this driver as a module, choose M here and read
+	  <file:Documentation/networking/net-modules.txt>.  The module
+	  will be called nxge.
+
 config IXGBE
 	tristate "Intel(R) 10GbE PCI Express adapters support"
 	depends on PCI && INET
diff --git a/drivers/net/Makefile b/drivers/net/Makefile
index de81c8a..c9bea2c 100644
--- a/drivers/net/Makefile
+++ b/drivers/net/Makefile
@@ -11,6 +11,7 @@ obj-$(CONFIG_IXGB) += ixgb/
 obj-$(CONFIG_IP1000) += ipg.o
 obj-$(CONFIG_CHELSIO_T1) += chelsio/
 obj-$(CONFIG_CHELSIO_T3) += cxgb3/
+obj-$(CONFIG_NXGE) += nxge/
 obj-$(CONFIG_EHEA) += ehea/
 obj-$(CONFIG_CAN) += can/
 obj-$(CONFIG_BONDING) += bonding/
diff --git a/drivers/net/nxge/Makefile b/drivers/net/nxge/Makefile
new file mode 100644
index 0000000..f090432
--- /dev/null
+++ b/drivers/net/nxge/Makefile
@@ -0,0 +1,43 @@
+NXGE_DIR	:= drivers/net/nxge
+
+EXTRA_CFLAGS	+= -I$(NXGE_DIR)/include -I$(NXGE_DIR)/npi
+EXTRA_CFLAGS	+= -D__KERNEL_ -D__KERNEL__
+EXTRA_CFLAGS	+= -DLINUX
+# -D_LITTLE_ENDIAN -D_BIT_FIELDS_LTOH
+#EXTRA_CFLAGS	+= -DRX_POLL
+#EXTRA_CFLAGS	+= -DLB_MODE_SET
+#EXTRA_CFLAGS	+= -DCONFIG_NXGE_NAPI
+#EXTRA_CFLAGS   += -DNXGE_NEPTUNE_ASIC_1_0
+EXTRA_CFLAGS   	+= -DCONFIG_NXGE_HW_CKSUM
+#EXTRA_CFLAGS	+= -DNXGE_DEBUG
+#EXTRA_CFLAGS   += -DNXGE_DEBUG_INFO
+
+EXTRA_CFLAGS += -Wno-unknown-pragmas
+EXTRA_CFLAGS += -Wno-unused
+EXTRA_CFLAGS += -O2 -pipe
+
+ifeq ($(SUBARCH),x86_64)
+EXTRA_CFLAGS += -mcmodel=kernel -mno-red-zone
+endif
+
+obj-$(CONFIG_NXGE) += nxge.o
+
+nxge-objs :=	nxge_main.o	\
+		nxge_ethtool.o	\
+		nxge_rxport.o	\
+		nxge_txc.o	\
+		nxge_fflp.o	\
+		nxge_mac.o	\
+		nxge_fzc.o	\
+		nxge_param.o	\
+		nxge_espc.o	\
+		npi/npi.o	\
+		npi/npi_espc.o	\
+		npi/npi_fflp.o	\
+		npi/npi_ipp.o	\
+		npi/npi_mac.o	\
+		npi/npi_rxdma.o	\
+		npi/npi_txc.o	\
+		npi/npi_txdma.o	\
+		npi/npi_vir.o	\
+		npi/npi_zcp.o
diff --git a/drivers/net/nxge/README b/drivers/net/nxge/README
new file mode 100644
index 0000000..00d87c7
--- /dev/null
+++ b/drivers/net/nxge/README
@@ -0,0 +1,312 @@
+Linux* Base Driver for the Sun Neptune NIU Family 10 Gigabit Ethernet Adapters
+===============================================================================
+
+November 15, 2007
+
+
+Contents
+========
+- Description
+- Supported Features
+- Building and Installation
+- Configuration
+- Ethtool support
+- Known Issues
+- Support
+
+Description
+============
+
+The nxge Gigabit Ethernet driver is a multi-threaded, loadable driver
+module for the Sun Neptune NIU family 10-Gigabit Ethernet hardware. It
+supports the following models:
+
+1. Sun x8 Express Dual 10 Gigabit Ethernet fiber XFP low  profile  adapter
+2. 10/100/1000BASE-T PCI-express low profile adapter
+
+This driver supports kernel versions 2.6.x. It is only supported as a
+loadable module at this time.
+
+The nxge driver functions include chip initialization, frame transmit
+and  receive,  flow  classification, multicast and promiscuous support
+and error recovery.
+
+The nxge device provides fully compliant IEEE 802.3ae 10Gb/s full duplex
+operation using XFP based 10GigE optics (dual 10 Gigabit fiber XFP adapter).
+The  Neptune  NIU  hardware  can auto-negotiate  for  IEEE  802.3x  frame
+based flow control capabilities. It only supports full duplex  operation.
+For the  10/100/1000BASE-T adapter, the nxge driver and hardware support
+auto-negotiation, a protocol specified by  the  1000 Base-T  standard.
+Auto-negotiation  allows  each  device to advertise its capabilities and discover
+those  of  its  peer (link  partner). The highest common denominator supported by
+both link partners is automatically selected,  yielding  the greatest  available
+throughput,  while  requiring no manual configuration. The nxge driver also
+allows you to  configure the  advertised capabilities to less than the maximum
+(where the full speed of the interface  is  not  required),  or  to
+force a specific mode of operation, irrespective of the link partner's
+advertised capabilities.
+
+Supported Features
+===================
+
+- Speeds
+	- Dual 10 Gigabit Ethernet fiber XFP - 10Gb/s Full Dupex
+	- 10/100/1000BASE-T adapter - 10b/s Full Duplex, 100b/s Full Duplex,
+		1000b/s Full duplex
+
+- Full Autonegotiation
+- Native VLANs
+- Channel Bonding
+- Jumbo Frames
+
+Channel Bonding documentation can be found in the Linux kernel source:
+/Documentation/networking/bonding.txt
+
+Building and Installation
+=========================
+
+The following RPM packages will be provided in the given directories for RedHat
+and SuSE respectively:
+
+RedHat:
+/usr/src/redhat/RPMS/x86_64/sun-nxge-1.0-1.x86_64.rpm
+
+SuSE:
+/usr/src/packages/RPMS/x86_64/sun-nxge-1.0-1.x86_64.rpm
+
+sun-nxge-1.0-1.x86_64.rpm contains nxge.ko, nxge_config and nxge.7.gz.
+
+1. To install the binaries, run the following command:
+	rpm -ivh sun-nxge-1.0-1.x86_64.rpm
+
+The driver binary will be installed as:
+     /lib/modules/<KERNEL VERSION>/kernel/drivers/net/nxge.ko
+The config tool binary will be installed as:
+	/usr/local/bin/nxge_config
+The man page will be installed as:
+	/usr/share/man/man7/nxge.7.gz
+
+2. Load the module using either the modprobe or insmod command:
+
+     modprobe nxge
+
+     insmod /lib/modules/<KERNEL VERSION>/kernel/drivers/net/nxge.ko
+
+3. Add the nxge interfaces to the /etc/modprobe.conf file for loading at boot time.
+	alias <if_name> nxge
+
+4. Use ethtool command to check the properties of each interface:
+	ethtool -i <if_name>
+
+3. Assign an IP address to the interface by entering the following:
+
+     ifconfig <if_name> <IP_address>
+
+4. Verify that the interface works.  Enter the following, where <IP_address>
+   is the IP address for another machine on the same subnet as the
+   interface that is being tested:
+
+     ping  <IP_address>
+
+
+Configuration
+==============
+
+There are three ways to configure the driver.
+
+1. During load time
+--------------------
+
+The parameters can be specified while loading the driver in one of the following
+ two ways:
+
+	- Command Line
+		With insmod or modprobe commands, the parameters can be specified on
+		the command line using the following syntax:
+
+		insmod nxge.ko [<param_name>=<param_val>, ...]
+		or
+		modprobe nxge [<param_name>=<param_val>, ...]
+
+	- Using conf file
+		With the modprobe command, the parameters can be specified in a conf
+		file which can be used with the modprobe command using the following
+		 syntax:
+
+		modprobe nxge -C <conf_file_name>
+
+		Each entry in the conf file would look like this:
+			options nxge <param_name>=<param_val>
+
+For a list and description of the parameters available for configuration on the
+ command line, refer the sub-section "Configurable Parameters".
+
+2. During run-time using the config tool nxge_config
+-----------------------------------------------------
+
+The utility nxge_config is provided along with the driver for configuring certain
+driver parameters during run time. The following syntax can be used with nxge_config
+to set a parameter:
+	nxge_config <if_name> set <param_name> <param_val>
+
+Other options to nxge_config are:
+	nxge_config <if_name> get  -- this gets a list of all the configurable
+					parameters.
+	nxge_config <if_name> get <param_name>  -- this is used to get the value
+				 of a particular parameter.
+
+For a list and description of the parameters available for configuration using
+ nxge_config, refer the section "Configurable Parameters".
+
+3. Using Ethtool
+------------------
+
+Refer to the "Ethtool Support" section.
+
+Configurable Parameters
+-------------------------
+
+The list of parameters that can be configured using insmod/modprobe or nxge_config are:
+
+	Classification and Load Distribution  variables :
+	------------------------------------------------------------------
+	class_opt_ipv4_tcp
+	class_opt_ipv4_udp
+	class_opt_ipv4_ah
+	class_opt_ipv4_sctp
+	class_opt_ipv6_tcp
+	class_opt_ipv6_udp
+	class_opt_ipv6_ah
+	class_opt_ipv6_sctp
+	tx_lb_policy
+
+	Bit-wise explanation for the values of these (class-opt-***) variables :
+	-------------------------------------------------------------------------
+	These variables define how each IP class is configured. Configuration options
+	range from whether TCAM lookup is enabled to flow hash generation. This
+	parameter also controls how the flow template is constructed and how packets
+	are distriubuted within RDC groups. Configuration bits:
+
+	0x0010:         use MAC Port (for flow key)
+	0x0020:         use L2DA (for flow key)
+	0x0040:         use VLAN (for flow key)
+	0x0080:         use proto (for flow key)
+	0x0100:         use IP src addr (for flow key)
+	0x0200:         use IP dest addr (for flow key)
+	0x0400:         use Src Port (for flow key)
+	0x0800:         use Dest Port (for flow key)
+
+	NOTE: The class-opt-*** classification variables are modified on an adapter
+	basis, that is, if any of these variables is modified for one port, the change
+	carries over to all other ports of that adapter.
+
+	Bit-wise explanation for the values of these tx_lb_policy variable :
+	-------------------------------------------------------------------------
+	This variables defines how outgoing IP class packets are load balanced among
+	multiple Transmit rings. Configuration bits:
+
+
+	0x000:         use L4 Source and Destination Ports
+	0x001:         use CPU ID (could cause out of order transmit)
+	0x002:         use L2DA
+	0x004:         use VLAN ID (if any)
+	0x008:         use IP source address
+	0x010:         use IP destination address
+	0x020:         use IP source and destination addresses
+	0x800:         use Single TX ring (ring 0)
+
+
+Ethtool Support
+===================
+
+The driver utilizes the ethtool interface for driver configuration as well as
+displaying statistical information.  Ethtool version 1.8 or later is required for this
+functionality.
+
+The latest release of ethtool can be found from http://sourceforge.net/projects/gkernel.
+
+Ethtool can be used to configure the following parameters:
+
+Interrupt Coalescing
+---------------------
+
+Ref: Use the following command to read the interrupt coalescing parameters for an
+ interface:
+	ethtool -c <if_name>
+
+Parameters rx-usecs and rx-frames control the RX interrupt rate per RX DMA channel.
+RX interrupt will be generated after rx-frames have been received or after rx-usecs
+time interval if fewer than rx-frames have been received within the interval. For low
+latency applications, it is recommened to set rx-usecs to smaller value. For bulk
+traffic, it is recommended to use larger values of rx-usecs and control the rate with
+rx-frames. rx-frames-irq controls the maximum number of rx packets processed with
+a single RX interrupt.
+
+To change RX interrupt Coalesce parameters use the following command
+	ethtool -C <if_name> <param_name> <param_val>
+	example: ethtool -C eth8 rx-usecs 20
+
+Layer 4 HW Checksuming
+----------------------
+
+Ref: Use the following command to read the L4 HW checksumming status for an interface:
+	ethtool -k <if_name>
+
+The tx checksumming and/or the rx checksumming can be turned on/off as follows:
+	ethtool -K <if_name> <tx | rx> <on | off>
+	example: ethtool -K eth8 tx off  -- this turns off L4 HW checksumming on the
+	TX side.
+
+
+IPv4 TCP Segmentation Offloading (LSO)
+----------------------
+
+Ref: Use the following command to read the tso status for an interface:
+	ethtool -k <if_name>
+
+TCP Segmentation offloading (tso) can be turned on/off as follows:
+	ethtool -K <if_name> <tso> <on | off>
+	example: ethtool -K eth8 tso off  -- this turns off tso.
+
+
+Known Issues
+============
+
+Multiple Interfaces on Same Ethernet Broadcast Network
+------------------------------------------------------
+Due to the default ARP behavior on Linux, it is not possible to have
+one system on two IP networks in the same Ethernet broadcast domain
+(non-partitioned switch) behave as expected.  All Ethernet interfaces
+will respond to IP traffic for any IP address assigned to the system.
+This results in unbalanced receive traffic.
+
+If you have multiple interfaces in a server, either turn on ARP
+filtering by entering:
+
+    echo 1 > /proc/sys/net/ipv4/conf/all/arp_filter
+
+NOTE: This setting is not saved across reboots.  The configuration
+change can be made permanent by adding the line:
+    net.ipv4.conf.all.arp_filter = 1
+to the file /etc/sysctl.conf
+
+      or,
+
+install the interfaces in separate broadcast domains (either in
+different switches or in a switch partitioned to VLANs).
+
+
+Support
+=======
+
+For general information, go to www.sun.com
+For support, please send your requests to neptune_sw_dev@sun.com
+
+
+
+Change Log:
+
+11/15/2007
+	Added description for TX Load balancing configuration
+	Cleanup
+
diff --git a/drivers/net/nxge/include/nxge.h b/drivers/net/nxge/include/nxge.h
new file mode 100644
index 0000000..e91b044
--- /dev/null
+++ b/drivers/net/nxge/include/nxge.h
@@ -0,0 +1,1743 @@
+/*
+ * nxge.h	Neptune main header file with the driver data structures
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef	_NXGE_H
+#define	_NXGE_H
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+
+#define NXGE_DEBUG_IOC				(SIOCDEVPRIVATE + 5)
+#define NXGE_TCAM_COMPARE_IOC			(SIOCDEVPRIVATE + 4)
+#define NXGE_TCAM_READ_IOC			(SIOCDEVPRIVATE + 4)
+#define NXGE_TCAM_ADD_IOC			(SIOCDEVPRIVATE + 3)
+#define NXGE_CFGTOOL_IOC			(SIOCDEVPRIVATE + 2)
+
+#define	NXGE_CFGTOOL_DUMP_BUFSZ			4096
+
+
+typedef struct _nxge_cfg_cmd {
+	uint64_t	cmd;
+	uint64_t	value;
+	char		param_name[32];
+	uint8_t		*param_list;
+} nxge_cfg_cmd_t;
+
+typedef enum {
+	ro,
+	rw
+} nxge_cfg_t;
+
+typedef struct nxge_cfg_attr_s {
+	nxge_cfg_t cfg_type;
+	uint8_t	   is_dump_str;
+	char param_name[32];
+} nxge_cfg_attr_t;
+
+
+#define	CFG_SET_CMD		1
+#define	CFG_GET_ONE_CMD		2
+#define CFG_GET_SIZE_CMD	3
+#define CFG_GET_ALL_CMD		4
+
+typedef enum {
+	invalid,
+	txmit,
+	rcv,
+	intr,
+	mac,
+	ipp,
+	vlan,
+	fflp,
+	pci
+} nxge_dump_t;
+
+
+#define SET_REG_VAL			1
+#define GET_REG_VAL			2
+#define SET_TX_LB_POLICY		3
+#define GET_TX_LB_POLICY		4
+#define DUMP_TX_REGS			5
+#define DUMP_RX_REGS			6
+#define DUMP_INTR_REGS			7
+#define DUMP_MAC_REGS			8
+#define DUMP_IPP_REGS			9
+#define DUMP_FFLP_REGS			10
+#define DUMP_VLAN_TABLE			11
+#define DUMP_PCI_REGS			12
+
+typedef struct nxge_dbg_reg {
+	uint8_t 	opcode;
+	uint64_t	addr;
+	uint64_t	val;
+	int		tx_lb_policy;
+} nxge_dbg_reg_t;
+
+#define LB_IOC			(SIOCDEVPRIVATE + 15)
+
+#define LB_SUCCESS		1
+#define LB_DEV_OPEN_ERR		-1
+#define LB_NO_MODE_ERR		-2
+#define	LB_BUF_ALLOC_ERR	-3
+
+#define OPEN_LB_DEVICE		1
+#define CLOSE_LB_DEVICE		2
+#define	SET_LB_MODE		3
+#define	GET_LB_MODE		4
+#define GET_LB_MODES_SZ		5
+#define GET_LB_MODES_LST	6
+#define	RUN_LB_TEST_RX 		7
+#define	RUN_LB_TEST_TX		8
+
+#define MAX_LB_PKTS		64
+#define MAX_LB_PKT_LEN		1514
+#define MIN_LB_PKT_LEN		60
+
+typedef struct lb_cmd_s {
+	uint8_t 	opcode;
+	int		err_code;
+} lb_cmd_t;
+
+typedef lb_cmd_t *p_lb_cmd_t;
+
+typedef struct lb_mode_s {
+	uint8_t 	opcode;
+	int		err_code;
+	uint32_t	lb_mode_size;
+} lb_mode_t;
+
+typedef lb_mode_t *p_lb_mode_t;
+
+typedef struct lb_supported_modes_s {
+	uint8_t 	opcode;
+	int		err_code;
+	uint8_t		*lb_mode_list;
+	uint32_t	list_size;
+} lb_supported_modes_t;
+
+typedef lb_supported_modes_t *p_lb_supported_modes_t;
+
+typedef struct lb_pkt_s {
+	uint8_t 	opcode;
+	int		err_code;
+        unsigned char	buffer[MAX_LB_PKT_LEN];
+	int		buf_len;
+	int		pkt_cnt;
+} lb_pkt_t;
+
+typedef lb_pkt_t *p_lb_pkt_t;
+
+typedef struct lb_pkt_buf_s *p_lb_pkt_buf_t;
+typedef struct lb_pkt_buf_s {
+	char			buffer[MAX_LB_PKT_LEN];
+	int			size;
+	int			index;
+	p_lb_pkt_buf_t	next;
+} lb_pkt_buf_t;
+
+typedef enum {
+	normal,
+	external,
+	internal
+} lb_type_t;
+
+typedef struct lb_property_s {
+	lb_type_t lb_type;
+	char key[16];
+	uint32_t value;
+} lb_property_t;
+
+typedef lb_property_t *p_lb_property_t;
+
+#define NXGE_TX_LB_TCP_PORT	0x00
+#define NXGE_TX_LB_L4_PORT	0x00
+#define NXGE_TX_LB_CPU_ID	0x01
+#define NXGE_TX_LB_DEST_MAC	0x02
+#define NXGE_TX_LB_VLAN	0x04
+#define NXGE_TX_LB_IP_SRC	0x08
+#define NXGE_TX_LB_IP_DEST	0x10
+#define NXGE_TX_LB_IP_ADDR	0x20
+#define NXGE_TX_LB_L4_SRC	0x40
+#define NXGE_TX_LB_L4_DEST	0x80
+#define NXGE_TX_LB_NONE	0x800
+
+#if defined(__KERNEL__) || defined(__KERNEL_)
+
+#include <nxge_mac_hw.h>
+#include <nxge_ipp_hw.h>
+#include <nxge_rxdma_hw.h>
+#include <nxge_txc_hw.h>
+#include <nxge_txdma_hw.h>
+#include <nxge_espc_hw.h>
+#include <nxge_fflp_hw.h>
+#include <nxge_zcp_hw.h>
+
+#include <nxge_mac.h>
+#include <nxge_ipp.h>
+#include <nxge_zcp.h>
+#include <nxge_espc.h>
+
+#include <nxge_common.h>
+
+#include <npi.h>
+#include <npi_ipp.h>
+#include <npi_txc.h>
+#include <npi_txdma.h>
+#include <npi_rxdma.h>
+#include <npi_vir.h>
+#include <npi_espc.h>
+
+
+#define DRV_MODULE_NAME		"nxge"
+#define PFX DRV_MODULE_NAME	":LSO"
+#define DRV_MODULE_VERSION	"1.3.1.cp3220"
+#define DRV_MODULE_RELDATE	"11152007"
+
+#ifndef PCI_DEVICE_ID_SUN_NEPTUNE
+#define PCI_DEVICE_ID_SUN_NEPTUNE 0xABCD
+#endif
+
+
+/* length of time before we decide the hardware is broke,
+ * and dev->tx_timeout() should be called to fix the problem */
+#define NXGE_TX_TIMEOUT			(HZ)
+/* #define NXGE_LINK_TIMEOUT		(22*HZ/10) */
+#define NXGE_LINK_TIMEOUT		(HZ)	/* 3 sec */
+#define NXGE_LINK_FAST_TIMEOUT		(1)
+
+#define	NXGE_MSG_MIN			ETH_ZLEN
+#define NXGE_MIN_MTU			(60)
+#define NXGE_MAX_MTU			(TX_JUMBO_MTU - 22) /* 9500 */
+
+#define	RXDMA_CK_DIV_DEFAULT		7500 	/* 25 usec */
+/*
+ * Hardware RDC designer: 8 cache lines during Atlas bringup.
+ */
+#define	RXDMA_RED_LESS_BYTES		(8 * 64) /* 8 cache line */
+#define	RXDMA_RED_LESS_ENTRIES		(RXDMA_RED_LESS_BYTES/8)
+#define	RXDMA_RED_WINDOW_DEFAULT	0
+#define	RXDMA_RED_THRES_DEFAULT		0
+
+#define	RXDMA_RCR_MAX_PKTS_INTR	0x200
+#define	RXDMA_RCR_PTHRES_DEFAULT 0x200 /* recomm. 8 */
+#define	RXDMA_RCR_TO_DEFAULT	0x8 /*	recomm. 4 */
+#define TX_FULL_MARK 3 /* TODO */
+#define TX_MIN_FREE 32 /* TODO */
+#define TX_MARK_FREE (TX_MIN_FREE << 1)
+#define TX_MARK_GSO_FREE (TX_MIN_FREE << 3)
+
+
+/*
+ * Maximum number of contiguous bytes that can be allocated by the system.
+ * Should be a power of 2.
+ */
+#define NXGE_MAX_CONT_BYTES		65536
+#define NXGE_MAX_CONT_BYTES_SHIFT	16
+
+#define NXGE_RTRACE (SIOCDEVPRIVATE + 1) /* dev private ioctl */
+
+
+/*************************************************************************/
+/* These defs are included from solaris to be able to compile nxge_mac.c.
+ * TODO - map these to linux.
+ */
+
+typedef uint64_t debug_level_t, *p_debug_level_t;
+
+
+typedef enum {
+	param_function_number = 0,
+	param_partition_id,
+	param_read_write_mode,
+	param_niu_cfg_type,
+	param_tx_qcfg_type,
+	param_rx_qcfg_type,
+	param_master_cfg_enable,
+	param_master_cfg_value,
+
+	param_autoneg,
+	param_anar_10gfdx,
+	param_anar_10ghdx,
+	param_anar_1000fdx,
+	param_anar_1000hdx,
+	param_anar_100T4,
+	param_anar_100fdx,
+	param_anar_100hdx,
+	param_anar_10fdx,
+	param_anar_10hdx,
+
+	param_anar_asmpause,
+	param_anar_pause,
+	param_use_int_xcvr,
+	param_enable_ipg0,
+	param_ipg0,
+	param_ipg1,
+	param_ipg2,
+	param_accept_jumbo,
+	param_txdma_weight,
+	param_tx_dma_channels_begin,
+
+	param_tx_dma_channels,
+	param_tx_dma_info,
+	param_rx_dma_channels_begin,
+	param_rx_dma_channels,
+	param_rx_drr_weight,
+	param_rx_full_header,
+	param_rx_dma_info,
+	param_rx_rbr_size,
+	param_rx_rcr_size,
+	param_default_port_rdc,
+
+	param_rx_rdc_grps_begin,
+	param_rx_rdc_grps,
+	param_default_grp0_rdc,
+	param_default_grp1_rdc,
+	param_default_grp2_rdc,
+	param_default_grp3_rdc,
+	param_default_grp4_rdc,
+	param_default_grp5_rdc,
+	param_default_grp6_rdc,
+	param_default_grp7_rdc,
+
+	param_rdc_groups_info,
+	param_start_ldg,
+	param_max_ldg,
+ 	param_mac_2rdc_grp,
+ 	param_vlan_2rdc_grp,
+	param_fcram_part_cfg,
+	param_fcram_access_ratio,
+	param_tcam_access_ratio,
+	param_tcam_enable,
+	param_hash_lookup_enable,
+	param_llc_snap_enable,
+
+	param_h1_init_value,
+	param_h2_init_value,
+	param_class_cfg_ether_usr1,
+	param_class_cfg_ether_usr2,
+	param_class_cfg_ip_usr4,
+	param_class_cfg_ip_usr5,
+	param_class_cfg_ip_usr6,
+	param_class_cfg_ip_usr7,
+	param_class_opt_ip_usr4,
+	param_class_opt_ip_usr5,
+	param_class_opt_ip_usr6,
+	param_class_opt_ip_usr7,
+	param_class_opt_ipv4_tcp,
+	param_class_opt_ipv4_udp,
+	param_class_opt_ipv4_ah,
+	param_class_opt_ipv4_sctp,
+	param_class_opt_ipv6_tcp,
+	param_class_opt_ipv6_udp,
+	param_class_opt_ipv6_ah,
+	param_class_opt_ipv6_sctp,
+	param_tx_lb_policy,
+	param_rx_allow_all,
+	param_nxge_debug_flag,
+	param_npi_debug_flag,
+	param_dump_rdc,
+	param_dump_tdc,
+	param_dump_mac_regs,
+	param_dump_ipp_regs,
+	param_dump_fflp_regs,
+	param_dump_vlan_table,
+	param_dump_rdc_table,
+	param_dump_ptrs,
+	param_end
+} nxge_param_index_t;
+
+#define NXGE_STR(x)	#x
+
+/*
+ * Named Dispatch Parameter Management Structure
+ */
+
+
+typedef	int (*nxge_pgetf_t)(p_nxge_t, caddr_t, uint64_t *, uint8_t *);
+typedef	int (*nxge_psetf_t)(p_nxge_t, uint64_t, caddr_t);
+
+#define	NXGE_PARAM_READ			0x00000001ULL
+#define	NXGE_PARAM_WRITE		0x00000002ULL
+#define	NXGE_PARAM_SHARED		0x00000004ULL
+#define	NXGE_PARAM_PRIV			0x00000008ULL
+#define	NXGE_PARAM_RW			NXGE_PARAM_READ | NXGE_PARAM_WRITE
+#define	NXGE_PARAM_RWS			NXGE_PARAM_RW | NXGE_PARAM_SHARED
+#define	NXGE_PARAM_RWP			NXGE_PARAM_RW | NXGE_PARAM_PRIV
+
+#define	NXGE_PARAM_RXDMA		0x00000010ULL
+#define	NXGE_PARAM_TXDMA		0x00000020ULL
+#define	NXGE_PARAM_CLASS_GEN	0x00000040ULL
+#define	NXGE_PARAM_MAC			0x00000080ULL
+#define	NXGE_PARAM_CLASS_BIN	NXGE_PARAM_CLASS_GEN | NXGE_PARAM_BASE_BIN
+#define	NXGE_PARAM_CLASS_HEX	NXGE_PARAM_CLASS_GEN | NXGE_PARAM_BASE_HEX
+#define	NXGE_PARAM_CLASS		NXGE_PARAM_CLASS_HEX
+
+#define	NXGE_PARAM_DUMP_STR		0x00000100ULL
+#define NXGE_PARAM_PER_PORT_CHK		0x00000200ULL
+
+#define	NXGE_PARAM_CMPLX		0x00010000ULL
+#define	NXGE_PARAM_NDD_WR_OK		0x00020000ULL
+#define	NXGE_PARAM_INIT_ONLY		0x00040000ULL
+#define	NXGE_PARAM_INIT_CONFIG		0x00080000ULL
+
+#define	NXGE_PARAM_READ_PROP		0x00100000ULL
+#define	NXGE_PARAM_PROP_ARR32		0x00200000ULL
+#define	NXGE_PARAM_PROP_ARR64		0x00400000ULL
+#define	NXGE_PARAM_PROP_STR		0x00800000ULL
+
+#define	NXGE_PARAM_BASE_DEC		0x00000000ULL
+#define	NXGE_PARAM_BASE_BIN		0x10000000ULL
+#define	NXGE_PARAM_BASE_HEX		0x20000000ULL
+#define	NXGE_PARAM_BASE_STR		0x40000000ULL
+#define	NXGE_PARAM_DONT_SHOW		0x80000000ULL
+
+#define	NXGE_PARAM_ARRAY_CNT_MASK	0x0000ffff00000000ULL
+#define	NXGE_PARAM_ARRAY_CNT_SHIFT	32ULL
+#define	NXGE_PARAM_ARRAY_ALLOC_MASK	0xffff000000000000ULL
+#define	NXGE_PARAM_ARRAY_ALLOC_SHIFT	48ULL
+
+#define NXGE_PARAM_NOT_CONF		0xffffffffUL
+
+#define NXGE_IS_PARAM_CONFIG(n, p)	(n->param_arr[p].conf_value != NXGE_PARAM_NOT_CONF)
+
+typedef struct _nxge_param_t {
+	nxge_pgetf_t getf;
+	nxge_psetf_t setf;   /* null for read only */
+	uint32_t type;  /* R/W/ Common/Port/ .... */
+	uint32_t minimum;
+	uint32_t maximum;
+	uint64_t value;
+	uint64_t old_value;
+	uint32_t conf_value;
+	char   *name;
+} nxge_param_t, *p_nxge_param_t;
+
+
+/* #define NXGE_WEB_SVR_CFG_STR	"web-server" */
+/* #define NXGE_GEN_SVR_CFG_STR	"generic-server" */
+/* #define NXGE_DMA_EQUAL_CFG_STR	"equal" */
+/* #define NXGE_DMA_FAIR_CFG_STR	"fair" */
+/* typedef enum { */
+/* 	rx_quick_cfg_str, */
+/* 	dma_cfg_str */
+/* } nxge_str_param_index_t; */
+
+/* typedef struct _nxge_str_param_t { */
+/* 	char *value; */
+/* 	char *conf_value; */
+/* 	char *name; */
+/* } nxge_str_param_t, *p_nxge_str_param_t; */
+
+
+typedef enum {
+	rxdma_intr_time,
+	rxdma_intr_pkts,
+	mac_2rdc_grp,
+	vlan_2rdc_grp,
+	txdmas_pport,
+	st_txdma_pport,
+	rxdmas_pport,
+	st_rxdma_pport,
+	rdcgrps_pport,
+	st_rdcgrp_pport
+} nxge_arr_param_index_t;
+
+
+
+typedef enum {
+	nxge_lb_normal = 0,
+	nxge_lb_ext10g,
+	nxge_lb_ext1000,
+	nxge_lb_ext100,
+	nxge_lb_ext10,
+	nxge_lb_phy10g,
+	nxge_lb_phy1000,
+	nxge_lb_phy,
+	nxge_lb_serdes10g,
+	nxge_lb_serdes1000,
+	nxge_lb_serdes,
+	nxge_lb_mac10g,
+	nxge_lb_mac1000,
+	nxge_lb_mac
+} nxge_lb_t;
+
+/*
+ * TX  Statistics.
+ */
+typedef struct _nxge_tx_ring_stats_t {
+	uint64_t	opackets;
+	uint64_t	oerrors;
+	uint64_t	obytes;
+	uint64_t	tx_descs_pending;
+	uint64_t	descs_kicked;
+	uint64_t	tx_packets_tso;
+	uint64_t 	jumbo;
+	uint64_t	tx_tso_nobuff;
+
+	uint32_t	tx_no_desc;
+	uint32_t	tx_tso_no_desc;
+	uint32_t	channel_num;
+	uint32_t	ring_size;
+	uint32_t	tx_inits;
+	uint32_t	tx_no_buf;
+	uint32_t	mbox_err;
+	uint32_t	pkt_size_err;
+	uint32_t 	tx_ring_oflow;
+	uint32_t 	pre_buf_par_err;
+	uint32_t 	nack_pref;
+	uint32_t 	nack_pkt_rd;
+	uint32_t 	conf_part_err;
+	uint32_t 	pkt_part_err;
+
+	uint32_t	tx_blocked;
+	uint32_t	tx_intr;
+	uint32_t	tx_unknown_fail;
+
+	txdma_ring_errlog_t	errlog;
+
+	/* for debug purposes only */
+
+#ifdef NXGE_DEBUG_INFO
+	uint64_t	stk_tx_pkts;
+
+	uint32_t 	frag0;
+	uint32_t 	frag1;
+	uint32_t 	frag2;
+	uint32_t 	frag3;
+	uint32_t 	frag4;
+	uint32_t 	frag5;
+	uint32_t 	frag_many;
+	uint32_t 	cksum;
+#endif
+} nxge_tx_ring_stats_t, *p_nxge_tx_ring_stats_t;
+
+
+typedef	struct _rdc_errlog {
+	rdmc_par_err_log_t	pre_par;
+	rdmc_par_err_log_t	sha_par;
+	uint8_t			compl_err_type;
+} rdc_errlog_t;
+/*
+ * Receive  Statistics.
+ */
+typedef struct _nxge_rx_ring_stats_t {
+
+	uint64_t	ipackets;
+	uint64_t	ibytes;
+	uint64_t	rx_rcvd_bufsz[4];
+	uint64_t	rx_bufsz0_rcvd;
+	uint64_t	rx_bufsz1_rcvd;
+	uint64_t	rx_bufsz2_rcvd;
+	uint64_t	rx_sgl_blk_rcvd;
+	uint64_t	rx_jumbo_pkts;
+	uint32_t	rx_multi_pkts;
+	uint32_t	multircv;
+	uint32_t	brdcstrcv;
+
+	uint32_t	ierrors;
+	uint32_t	norcvbuf;
+
+	uint32_t	channel_num;
+	uint32_t	rx_inits;
+
+	uint32_t	rx_mtu_pkts;
+	uint32_t	rx_no_buf;
+
+	/*
+	 * Error event stats.
+	 */
+	uint32_t	rx_rbr_tmout;
+	uint32_t	l2_err;
+	uint32_t	l4_cksum_err;
+	uint32_t	fflp_soft_err;
+	uint32_t	zcp_soft_err;
+	uint32_t	dcf_err;
+	uint32_t 	rbr_tmout;
+	uint32_t 	rsp_cnt_err;
+	uint32_t 	byte_en_err;
+	uint32_t 	byte_en_bus;
+	uint32_t 	rsp_dat_err;
+	uint32_t 	rcr_ack_err;
+	uint32_t 	dc_fifo_err;
+	uint32_t 	rcr_sha_par;
+	uint32_t 	rbr_pre_par;
+	uint32_t 	port_drop_pkt;
+	uint32_t 	wred_drop;
+	uint32_t 	rbr_pre_empty;
+	uint32_t 	rcr_shadow_full;
+	uint32_t 	config_err;
+	uint32_t 	rcrincon;
+	uint32_t 	rcrfull;
+	uint32_t 	rbr_empty;
+	uint32_t 	rbrfull;
+	uint32_t 	rbrlogpage;
+	uint32_t 	cfiglogpage;
+	uint32_t 	rcrto;
+	uint32_t 	rcrthres;
+	uint32_t 	mex;
+	rdc_errlog_t	errlog;
+} nxge_rx_ring_stats_t, *p_nxge_rx_ring_stats_t;
+
+typedef struct _nxge_rdc_sys_stats {
+	uint32_t	pre_par;
+	uint32_t	sha_par;
+	uint32_t	id_mismatch;
+	uint32_t	ipp_eop_err;
+	uint32_t	zcp_eop_err;
+} nxge_rdc_sys_stats_t, *p_nxge_rdc_sys_stats_t;
+
+typedef struct _nxge_stats_t {
+	/*
+	 *  Overall structure size
+	 */
+	size_t			stats_size;
+
+	/*
+	 * Link Input/Output stats
+	 */
+	uint64_t		ipackets;
+	uint64_t		ierrors;
+	uint64_t		opackets;
+	uint64_t		oerrors;
+	uint64_t		collisions;
+
+	/*
+	 * MIB II variables
+	 */
+	uint64_t		rbytes;    /* # bytes received */
+	uint64_t		obytes;    /* # bytes transmitted */
+	uint32_t		multircv;  /* # multicast packets received */
+	uint32_t		multixmt;  /* # multicast packets for xmit */
+	uint32_t		brdcstrcv; /* # broadcast packets received */
+	uint32_t		brdcstxmt; /* # broadcast packets for xmit */
+	uint32_t		norcvbuf;  /* # rcv packets discarded */
+	uint32_t		noxmtbuf;  /* # xmit packets discarded */
+
+	nxge_mac_stats_t	mac_stats;	/* Common MAC Statistics */
+	nxge_xmac_stats_t	xmac_stats;	/* XMAC Statistics */
+	nxge_bmac_stats_t	bmac_stats;	/* BMAC Statistics */
+
+/* 	nxge_rx_ring_stats_t	rx_stats; */	/* per port RX stats */
+	nxge_ipp_stats_t	ipp_stats;	/* per port IPP stats */
+	nxge_zcp_stats_t	zcp_stats;	/* per port IPP stats */
+	nxge_rx_ring_stats_t	rdc_stats[NXGE_MAX_RDCS]; /* per rdc stats */
+	nxge_rdc_sys_stats_t	rdc_sys_stats;	/* per port RDC stats */
+
+/* 	nxge_tx_ring_stats_t	tx_stats; */	/* per port TX stats */
+	nxge_txc_stats_t	txc_stats;	/* per port TX stats */
+	nxge_tx_ring_stats_t	tdc_stats[NXGE_MAX_TDCS]; /* per tdc stats */
+	nxge_fflp_stats_t	fflp_stats;	/* fflp stats */
+
+	/*
+	 * Lets the user know the MTU currently in use by
+	 * the physical MAC port.
+	 */
+	nxge_lb_t		lb_mode;
+	uint32_t		qos_mode;
+	uint32_t		trunk_mode;
+	uint32_t		poll_mode;
+
+	/*
+	 * Tx Statistics.
+	 */
+	uint32_t		tx_skb_errs;
+	uint32_t		tx_inits;
+	uint32_t		tx_starts;
+	uint32_t		tx_nocanput;
+	uint32_t		tx_no_desc;
+	uint32_t		tx_dma_bind_fail;
+	uint32_t		tx_uflo;
+/* TODO */
+	uint32_t		tx_queue[NXGE_MAX_TDCS];
+
+	uint32_t		tx_hdr_pkts;
+	uint32_t		tx_max_pend;
+
+	/*
+	 * Rx Statistics.
+	 */
+	uint32_t		rx_inits;
+	uint32_t		rx_hdr_pkts;
+	uint32_t		rx_mtu_pkts;
+	uint32_t		rx_split_pkts;
+	uint32_t		rx_no_buf;
+	uint32_t		rx_no_comp_wb;
+	uint32_t		rx_ov_flow;
+	uint32_t		rx_len_mm;
+	uint32_t		rx_tag_err;
+	uint32_t		rx_nocanput;
+	uint32_t		rx_msgdup_fail;
+	uint32_t		rx_allocb_fail;
+
+	/*
+	 * Receive buffer management statistics.
+	 */
+	uint32_t		rx_new_pages;
+	uint32_t		rx_new_hdr_pgs;
+	uint32_t		rx_new_mtu_pgs;
+	uint32_t		rx_new_nxt_pgs;
+	uint32_t		rx_reused_pgs;
+	uint32_t		rx_hdr_drops;
+	uint32_t		rx_mtu_drops;
+	uint32_t		rx_nxt_drops;
+
+	/*
+	 * Receive flow statistics
+	 */
+	uint32_t		rx_rel_flow;
+	uint32_t		rx_rel_bit;
+
+	uint32_t		rx_pkts_dropped;
+
+/* FIXME */
+	/*
+	 * PCI-E Bus Statistics.
+	 */
+	uint32_t		pci_bus_speed;
+	uint32_t		pci_err;
+	uint32_t		pci_rta_err;
+	uint32_t		pci_rma_err;
+	uint32_t		pci_parity_err;
+	uint32_t		pci_bad_ack_err;
+	uint32_t		pci_drto_err;
+	uint32_t		pci_dmawz_err;
+	uint32_t		pci_dmarz_err;
+
+	uint32_t		rx_taskq_waits;
+
+	uint64_t		tx_jumbo_pkts;
+	uint64_t		tx_tso;
+	uint64_t		rx_jumbo_pkts;
+
+	/*
+	 * Some statistics added to support bringup, these
+	 * should be removed.
+	 */
+	uint32_t		user_defined;
+} nxge_stats_t, *p_nxge_stats_t;
+
+
+/*
+ * Neptune Virtualization Control Operations
+ */
+typedef enum {
+	NXGE_CTLOPS_NIUTYPE,
+	NXGE_CTLOPS_GET_ATTRIBUTES,
+	NXGE_CTLOPS_GET_HWPROPERTIES,
+	NXGE_CTLOPS_SET_HWPROPERTIES,
+	NXGE_CTLOPS_GET_SHARED_REG,
+	NXGE_CTLOPS_SET_SHARED_REG,
+	NXGE_CTLOPS_UPDATE_SHARED_REG,
+	NXGE_CTLOPS_GET_LOCK_BLOCK,
+	NXGE_CTLOPS_GET_LOCK_TRY,
+	NXGE_CTLOPS_FREE_LOCK,
+	NXGE_CTLOPS_SET_SHARED_REG_LOCK,
+	NXGE_CTLOPS_CLEAR_BIT_SHARED_REG,
+	NXGE_CTLOPS_CLEAR_BIT_SHARED_REG_UL,
+	NXGE_CTLOPS_END
+} nxge_ctl_enum_t;
+
+/* 12 bits are available */
+#define	COMMON_CFG_VALID	0x01
+#define	COMMON_CFG_BUSY	0x02
+#define	COMMON_INIT_START	0x04
+#define	COMMON_INIT_DONE	0x08
+#define	COMMON_TCAM_BUSY	0x10
+#define	COMMON_VLAN_BUSY	0x20
+
+#define	NXGE_SR_FUNC_BUSY_SHIFT	0x8
+#define	NXGE_SR_FUNC_BUSY_MASK	0xf00
+
+
+#define	COMMON_TXDMA_CFG	1
+#define	COMMON_RXDMA_CFG	2
+#define	COMMON_RXDMA_GRP_CFG	4
+#define	COMMON_CLASS_CFG	8
+#define	COMMON_QUICK_CFG	0x10
+
+/* Interrupts */
+
+#define	INTR_TYPE_INVALID	0
+#define	INTR_TYPE_INTX		1
+#define INTR_TYPE_MSI		2
+#define INTR_TYPE_MSIX		3
+typedef struct _nxge_intr_name {
+	char name[12];
+} nxge_intr_name_t;
+
+typedef struct _nxge_msi_t {
+	boolean_t		intr_registered;/* interrupts are registered */
+	boolean_t		intr_enabled; 	/* interrupts are enabled */
+	boolean_t		niu_msi_enable;	/* debug or configurable? */
+	uint8_t			nldevs;		/* # of logical devices */
+	uint8_t			intr_type;	/* interrupt type to add */
+	uint8_t			max_int_cnt;	/* max MSIX/INT HW supports */
+	uint8_t			msix_alloced;	/* Allocated MSIX */
+	int			start_inum;	/* start inum (in sequence?) */
+	int			msi_intx_cnt;	/* # msi/intx ints returned */
+	int			intr_added;	/* # interrupts actually needed */
+	int			intr_cap;	/* interrupt capabilities */
+	size_t			intr_size;	/* size of array to allocate */
+	struct msix_entry	*msix_vector;	/* For array of interrupts */
+	/* Add interrupt number for each interrupt vector */
+	int			pri;
+	nxge_intr_name_t irq_name[16];
+} nxge_msix_t, *p_nxge_msix_t;
+
+/*
+ * Each logical device (69):
+ *	- LDG #
+ *	- flag bits
+ *	- masks.
+ *	- interrupt handler function.
+ *
+ * Generic system interrupt handler with two arguments:
+ *	(nxge_sys_intr_t)
+ *	Per device instance data structure
+ *	Logical group data structure.
+ *
+ * Logical device interrupt handler with two arguments:
+ *	(nxge_ldv_intr_t)
+ *	Per device instance data structure
+ *	Logical device number
+ */
+typedef struct	_nxge_ldg_t nxge_ldg_t, *p_nxge_ldg_t;
+typedef struct	_nxge_ldv_t nxge_ldv_t, *p_nxge_ldv_t;
+
+typedef struct _nxge_intr_arg_t {
+	struct net_device	*dev;
+	p_nxge_ldv_t		ldvp;
+} nxge_intr_arg_t, *p_nxge_intr_arg_t;
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,18)
+#define NXGE_IRQ_SHARED IRQF_SHARED
+#define NXGE_IRQ_MSI IRQF_SHARED
+#define NXGE_IRQ_MSIX IRQF_TRIGGER_HIGH
+typedef irqreturn_t (*nxge_sys_intr_t)(int, void *);
+typedef irqreturn_t (*nxge_ldv_intr_t)(int, void *);
+#else
+#define NXGE_IRQ_SHARED SA_SHIRQ
+#define NXGE_IRQ_MSI SA_SHIRQ
+#define NXGE_IRQ_MSIX SA_SHIRQ
+typedef irqreturn_t (*nxge_sys_intr_t)(int, void *, struct pt_regs *);
+typedef irqreturn_t (*nxge_ldv_intr_t)(int, void *, struct pt_regs *);
+#endif
+
+/*
+ * Each logical device Group (64) needs to have the following
+ * configurations:
+ *	- timer counter (6 bits)
+ *	- timer resolution (20 bits, number of system clocks)
+ *	- system data (7 bits)
+ */
+struct _nxge_ldg_t {
+	uint8_t			ldg;		/* logical group number */
+	uint8_t			vldg_index;
+	boolean_t		arm;
+	boolean_t		interrupted;
+	uint16_t		ldg_timer;	/* counter */
+	uint8_t			func;
+	uint8_t			vector;
+	uint8_t			intdata;
+	uint8_t			nldvs;
+	p_nxge_ldv_t		ldvp;	/* pointer to the first ldv struct */
+	nxge_sys_intr_t		sys_intr_handler;
+	p_nxge_t		nxgep;
+	boolean_t		has_rxdma;
+	boolean_t		has_txdma;
+};
+
+struct _nxge_ldv_t {
+	uint8_t			ldg_assigned;
+	uint8_t			ldv;
+	boolean_t		is_rxdma;
+	boolean_t		is_txdma;
+	boolean_t		is_mif;
+	boolean_t		is_mac;
+	boolean_t		is_syserr;
+	boolean_t		use_timer;
+	uint8_t			channel;
+	uint8_t			vdma_index;
+	uint8_t			func;
+	p_nxge_ldg_t		ldgp;
+	uint8_t			ldv_flags;
+	boolean_t		is_leve;
+	boolean_t		is_edge;
+	uint8_t			ldv_ldf_masks;
+	nxge_ldv_intr_t		ldv_intr_handler;
+	nxge_intr_arg_t		intr_hdlr_arg;
+	uint64_t csr;
+#if defined(CONFIG_NXGE_NAPI)
+	int budget;
+#endif
+
+#ifdef NXGE_USE_SOFT_RX
+	struct 		tasklet_struct soft_intr;
+	boolean_t tasklet_sched;
+#endif
+	p_nxge_t		nxgep;
+};
+
+typedef struct _nxge_ldgv_t {
+	uint8_t			ndma_ldvs;
+	uint8_t			nldvs;
+	uint8_t			start_ldg;
+	uint8_t			start_ldg_tx;
+	uint8_t			start_ldg_rx;
+	uint8_t			maxldgs;
+	uint8_t			maxldvs;
+	uint8_t			ldg_intrs;
+	boolean_t		own_sys_err;
+	boolean_t		own_max_ldv;
+	uint32_t		tmres;
+	p_nxge_ldg_t		ldgp;
+	p_nxge_ldv_t		ldvp;
+	p_nxge_ldv_t		ldvp_syserr;
+} nxge_ldgv_t, *p_nxge_ldgv_t;
+
+
+/*************************************************************************/
+
+typedef struct _pci_cfg_t {
+	uint16_t vendorid;
+	uint16_t devid;
+	uint16_t command;
+	uint16_t status;
+	uint8_t  revid;
+	uint8_t  res0;
+	uint16_t junk1;
+	uint8_t  cache_line;
+	uint8_t  latency;
+	uint8_t  header;
+	uint8_t  bist;
+	uint32_t base;
+	uint32_t base14;
+	uint32_t base18;
+	uint32_t base1c;
+	uint32_t base20;
+	uint32_t base24;
+	uint32_t base28;
+	uint32_t base2c;
+	uint32_t base30;
+	uint32_t res1[2];
+	uint8_t int_line;
+	uint8_t int_pin;
+	uint8_t	min_gnt;
+	uint8_t max_lat;
+} pci_cfg_t, *p_pci_cfg_t;
+
+typedef struct _dev_regs_t {
+	caddr_t			nxge_pciregp;	/* mapped PCI registers */
+	uint32_t		pci_reg_len;
+
+	caddr_t			nxge_regp;	/* mapped device registers */
+	uint32_t		dev_reg_len;
+
+	caddr_t			nxge_msix_regp;/* MSI/X register */
+	uint32_t		msi_reg_len;
+
+	caddr_t			nxge_romp;	/* expansion ROM pointer */
+	uint32_t		rom_len;
+} dev_regs_t, *p_dev_regs_t;
+
+/**********************************************************************/
+
+/*
+ * Struct to keep track of the individual Neptune cards.
+ */
+typedef struct _nxge_dev_list_t {
+	struct list_head	list;
+	unsigned short		pci_dev_id;
+	nxge_hw_list_t		dev_list;
+} nxge_dev_list_t, *p_nxge_dev_list_t;
+
+/*
+ * The page structure used for the Rx buffer list.
+ */
+typedef struct _nxge_page_t {
+	struct list_head list;
+	struct page *buffer;
+	dma_addr_t dma_addr;
+	int used;
+} nxge_page_t, *p_nxge_page_t;
+
+/******************************************************************/
+
+/*
+ * Common DMA data elements.
+ */
+typedef struct _nxge_dma_common_t {
+	caddr_t			cpu_addr;
+	dma_addr_t		dma_addr;
+	size_t			alloc_len;
+	uint16_t		dma_channel;
+} nxge_dma_common_t, *p_nxge_dma_common_t; /* 32 byte structure */
+
+typedef struct _nxge_dma_buf_t {
+	struct page		*page_p;
+	struct sk_buff	*skb;
+	dma_addr_t		dma_addr;
+	size_t			alloc_len;
+	size_t			block_size;
+	uint_t			dma_chunk_index;
+	uint16_t		dma_channel;
+} nxge_dma_buf_t, *p_nxge_dma_buf_t;
+
+typedef struct _nxge_dma_pool_t {
+	p_nxge_dma_common_t	dma_buf_pool_p;
+	uint32_t		ndmas;
+	boolean_t		buf_allocated;
+} nxge_dma_pool_t, *p_nxge_dma_pool_t;
+
+
+
+
+#define NXGE_TX_SKB_REG 0x00
+#define NXGE_TX_SKB_TINY 0x01
+#define NXGE_TX_SKB_COPY 0x02
+
+
+#define NXGE_TX_RECLAIM_FAIL 0
+#define NXGE_TX_RECLAIM_OK	  1
+#define NXGE_TX_RECLAIM_SLOW 2
+
+typedef struct _skb_page_mapping_info {
+	dma_addr_t addr;
+	uint32_t length;
+} skb_page_mapping_info_t, *p_skb_page_mapping_info_t;
+
+typedef struct _skb_mapping_info {
+	uint64_t hdr_buffer_cpu_addr;
+	dma_addr_t hdr_buffer_dma_addr;
+	p_skb_page_mapping_info_t data_buffer_mapping_info;
+	uint32_t mappings;
+	uint32_t hdr_buffer_length;
+} skb_mapping_info_t, *p_skb_mapping_info_t;
+
+
+typedef struct _skb_hdr_info {
+	tx_pkt_header_t	ctl_hdrp;
+	void * map_info;
+	uint32_t mappings_len;
+	uint8_t skb_process_info;
+	uint8_t l3_offset;
+	uint8_t l4_offset;
+	uint8_t data_offset;
+	uint8_t l4_type;
+	uint8_t l3_type;
+#define NXGE_LB_L3_IPV4 1
+#define NXGE_LB_L3_IPV6 2
+#define NXGE_LB_L4_UDP 1
+#define NXGE_LB_L4_TCP 2
+} skb_hdr_info_t, *p_skb_hdr_info_t;
+
+
+typedef struct _tx_msg_t {
+	struct sk_buff      	*buf;
+	dma_addr_t		addr;
+	caddr_t			cpu_addr;
+	dma_addr_t		cp_dma_addr;
+	caddr_t			cp_cpu_addr;
+	uint32_t 		length;
+	uint32_t		flags;
+#define NXGE_TX_BUF_MAPPED_NONE		0x00
+#define NXGE_TX_BUF_MAPPED_SKB		0x01
+#define NXGE_TX_BUF_MAPPED_SKB_FRAG	0x02
+#define NXGE_TX_BUF_MAPPED_SKB_PART	0x04
+#define NXGE_TX_BUF_MAPPED_COPY_BUFF	0x08
+} tx_msg_t, *p_tx_msg_t;
+
+
+
+typedef struct _tx_ring_t {
+	nxge_dma_common_t	tdc_desc;
+	struct _nxge_t		*nep;
+	p_tx_msg_t 		tx_msg_ring;
+	caddr_t			cp_cpu_addr;
+	dma_addr_t		cp_dma_addr;
+
+	nxge_os_mutex_t 	lock;
+	nxge_os_mutex_t 	stats_lock;
+	p_nxge_tx_ring_stats_t	tdc_stats;
+	p_nxge_dma_buf_t	dma_buf_p;
+
+	uint16_t 			in;
+	uint16_t 			out;
+	uint16_t 			descs_pending;
+	uint16_t 			wrap_mask;
+	uint16_t 			ring_size;
+	uint16_t 			head_index;
+
+	uint16_t		tdc; /* real channel */
+	uint16_t		ring; /* relative index */
+	boolean_t		intr_marked;
+	boolean_t		in_wrap;
+	boolean_t		head_wrap;
+
+	tx_rng_cfig_t		tx_ring_cfig;
+	tx_ring_hdl_t		tx_ring_hdl;
+	tx_ring_kick_t		tx_ring_kick;
+	tx_cs_t			tx_cs;
+	tx_dma_ent_msk_t	tx_evmask;
+	txdma_mbh_t		tx_mbox_mbh;
+	txdma_mbl_t		tx_mbox_mbl;
+	log_page_vld_t		page_valid;
+	log_page_mask_t		page_mask_1;
+	log_page_mask_t		page_mask_2;
+	log_page_value_t	page_value_1;
+	log_page_value_t	page_value_2;
+	log_page_relo_t		page_reloc_1;
+	log_page_relo_t		page_reloc_2;
+	log_page_hdl_t		page_hdl;
+
+
+	tx_ring_hdl_t		ring_head;
+	tx_ring_kick_t		ring_kick_tail;
+	txdma_mailbox_t         tx_mbox;
+
+	txc_dma_max_burst_t	max_burst;
+
+
+
+	uint16_t chunk_cnt;
+	uint16_t		ldg_group_id;
+	boolean_t		cfg_set;
+} tx_ring_t, *p_tx_ring_t;
+
+
+/* Transmit Mailbox */
+typedef struct _tx_mbox_t {
+	nxge_os_mutex_t 		lock;
+	uint16_t		index;
+	struct _nxge_t		*nep;
+	uint16_t		tdc;
+	nxge_dma_common_t	tx_mbox;
+	txdma_mbl_t		tx_mbox_l;
+	txdma_mbh_t		tx_mbox_h;
+} tx_mbox_t, *p_tx_mbox_t;
+
+typedef struct _tx_rings_t {
+	p_tx_ring_t 		*rings;
+	boolean_t		txdesc_allocated;
+
+	p_nxge_dma_common_t	tdc_dma;
+	p_nxge_dma_common_t	tdc_mbox;
+} tx_rings_t, *p_tx_rings_t;
+
+typedef struct _tx_buf_rings_t {
+	struct _tx_buf_ring_t 	*txbuf_rings;
+	boolean_t		txbuf_allocated;
+} tx_buf_rings_t, *p_tx_buf_rings_t;
+
+typedef struct _tx_mbox_areas_t {
+	p_tx_mbox_t 		*txmbox_areas;
+	boolean_t		txmbox_allocated;
+} tx_mbox_areas_t, *p_tx_mbox_areas_t;
+
+typedef struct _nxge_logical_page_t {
+	uint16_t		dma;
+	uint16_t		page;
+	boolean_t		valid;
+	uint64_t		mask;
+	uint64_t		value;
+	uint64_t		reloc;
+	uint32_t		handle;
+} nxge_logical_page_t, *p_nxge_logical_page_t;
+
+typedef struct _tx_param_t {
+	nxge_logical_page_t tx_logical_pages[NXGE_MAX_LOGICAL_PAGES];
+} tx_param_t, *p_tx_param_t;
+
+typedef struct _tx_params {
+	struct _tx_param_t 	*tx_param_p;
+} tx_params_t, *p_tx_params_t;
+
+typedef struct _rx_tx_param_t {
+	nxge_logical_page_t logical_pages[NXGE_MAX_LOGICAL_PAGES];
+} rx_tx_param_t, *p_rx_tx_param_t;
+
+typedef struct _rx_tx_params {
+	struct _tx_param_t 	*tx_param_p;
+} rx_tx_params_t, *p_rx_tx_params_t;
+
+
+/*
+ * Software reserved buffer offset
+ */
+typedef struct _nxge_rxbuf_off_hdr_t {
+	uint32_t		index;
+} nxge_rxbuf_off_hdr_t, *p_nxge_rxbuf_off_hdr_t;
+
+/*
+ * Receive buffer management attributes.
+ */
+typedef struct _nxge_rx_buf_block_attr_t {
+	uint32_t	index;
+} nxge_rx_buf_attr_t, *p_nxge_rx_buf_block_attr_t;
+
+
+
+typedef struct _rx_msg_t {
+	struct _nxge_t		*nxgep;
+	struct _rx_rbr_ring_t	*rx_rbr_p;
+	caddr_t			cpu_addr;
+	dma_addr_t		dma_addr;
+	uint32_t 		ref_cnt;
+	uint32_t		cur_usage_cnt;
+	uint32_t		max_usage_cnt;
+	uint32_t		block_size;
+	uint32_t		block_index;
+	uint32_t 		pkt_buf_size;
+	uint32_t 		shifted_addr;
+	boolean_t 		free;
+} rx_msg_t, *p_rx_msg_t;
+
+
+#define RXCOMP_HIST_ELEMENTS 100000
+
+typedef struct _nxge_rxcomphist_t {
+	uint_t comp_cnt;
+	uint64_t rx_comp_entry;
+} nxge_rxcomphist_t, *p_nxge_rxcomphist_t;
+
+/* Receive Completion Ring */
+typedef struct _rx_rcr_ring_t {
+	struct _nxge_t		*nep;
+	struct _rx_rbr_ring_t	*rx_rbr_p;
+	nxge_os_mutex_t 		lock;
+	caddr_t			cpu_addr;
+	p_nxge_rx_ring_stats_t	rdc_stats;
+
+	uint32_t		max_receive_pkts;
+	uint32_t 		wrap_mask;
+
+	uint16_t 		out;
+	uint16_t 		ring_size;	/* # of RCR entries */
+	uint16_t		sw_priv_hdr_len;/* 0 - 192 bytes (code) */
+	uint16_t		sw_offset_bytes;/* 0 - 192 bytes (bytes) */
+
+	uint16_t		index;
+	uint16_t		rdc;
+	uint16_t		rdc_grp_id;
+	uint16_t		full_hdr_flag;	/* 1: 18 bytes header */
+
+	uint32_t		intr_timeout;
+	uint32_t		intr_thresh;
+	nxge_dma_common_t	rcr_desc;
+	uint64_t		rcr_tail_pp;
+	uint64_t		rcr_head_pp;
+	uint64_t		rcr_addr;
+	rcrcfig_a_t		rcr_cfga;
+	rcrcfig_b_t		rcr_cfgb;
+	uint16_t		ldg_group_id;
+	boolean_t		cfg_set;
+} rx_rcr_ring_t, *p_rx_rcr_ring_t;
+
+/* Buffer index information */
+typedef struct _rxbuf_index_info_t {
+	dma_addr_t dma_addr;
+	struct page *page_p;
+	uint32_t block_index;
+	uint32_t start_buf_index;
+	uint32_t block_size;
+} rxbuf_index_info_t, *p_rxbuf_index_info_t;
+
+/* Buffer index information */
+
+typedef struct _rxring_info_t {
+	uint16_t search_hint[4];
+	uint32_t block_size_mask;
+	uint16_t max_iterations;
+	rxbuf_index_info_t *buffer_arr;
+} rxring_info_t, *p_rxring_info_t;
+
+#define RING_HASH_SIZE 0x200
+#define RING_HASH_BUCKETS 0x18
+
+typedef struct _rxring_info_hash_t {
+	uint32_t key;
+	uint32_t index;
+} rxring_info_hash_t, *p_rxring_info_hash_t;
+
+
+/* Receive Buffer Block Ring */
+typedef struct _rx_rbr_ring_t {
+	nxge_dma_common_t	rbr_desc;
+	p_nxge_dma_buf_t	dma_buf_p;
+	p_rx_msg_t 		rx_msg_ring;
+	struct _nxge_t		*nep;
+	uint32_t		*rbr_desc_vp;
+
+	uint32_t tmp_buffers[RBR_POST_BATCH];
+	uint16_t max_usage[4];
+	uint32_t bsize[4];
+
+	uint16_t		tmp_count;
+	uint16_t		rdc;
+	uint16_t		index;
+	uint16_t 		wrap_mask;
+	uint16_t 		rbr_wr_index;
+	uint16_t 		ring_size;
+	uint16_t		block_size;
+
+	uint16_t		pkt_buf_size0_bytes;
+	uint16_t		pkt_buf_size1_bytes;
+	uint16_t		pkt_buf_size2_bytes;
+
+	uint16_t		pkt_buf_size0;
+	uint16_t		pkt_buf_size1;
+	uint16_t		pkt_buf_size2;
+
+	nxge_os_mutex_t 		lock;
+	p_rxring_info_hash_t ring_hash;
+	rxring_info_t  *ring_info;
+	uint16_t search_hint[4];
+
+	uint16_t 		rbb_max;
+	uint16_t		rdc_grp_id;
+	uint16_t		blks_per_chunk;
+	uint16_t		chunk_cnt;
+
+
+	uint16_t		npi_pkt_buf_size0;
+	uint16_t		npi_pkt_buf_size1;
+	uint16_t		npi_pkt_buf_size2;
+	uint64_t		rbr_head_pp;
+	uint64_t		rbr_tail_pp;
+	p_rx_rcr_ring_t		rx_rcr_p;
+
+		/* ctl path */
+	uint64_t		rbr_addr;
+	rbr_cfig_a_t		rbr_cfga;
+	rbr_cfig_b_t		rbr_cfgb;
+	rbr_kick_t		rbr_kick;
+	log_page_vld_t		page_valid;
+	log_page_mask_t		page_mask_1;
+	log_page_mask_t		page_mask_2;
+	log_page_value_t	page_value_1;
+	log_page_value_t	page_value_2;
+	log_page_relo_t		page_reloc_1;
+	log_page_relo_t		page_reloc_2;
+	log_page_hdl_t		page_hdl;
+} rx_rbr_ring_t, *p_rx_rbr_ring_t;
+
+/* Receive Mailbox */
+typedef struct _rx_mbox_t {
+	nxge_dma_common_t	rx_mbox;
+	rxdma_cfig1_t		rx_cfg1;
+	rxdma_cfig2_t		rx_cfg2;
+	uint64_t		mbox_addr;
+	boolean_t		cfg_set;
+
+	nxge_os_mutex_t 		lock;
+	uint16_t		index;
+	struct _nxge_t		*nep;
+	uint16_t		rdc;
+} rx_mbox_t, *p_rx_mbox_t;
+
+typedef struct _rx_rbr_rings_t {
+	p_rx_rbr_ring_t 	*rings;
+	boolean_t		rxbuf_allocated;
+} rx_rbr_rings_t, *p_rx_rbr_rings_t;
+
+typedef struct _rx_rcr_rings_t {
+	p_rx_rcr_ring_t 	*rings;
+	boolean_t		cntl_buf_allocated;
+} rx_rcr_rings_t, *p_rx_rcr_rings_t;
+
+typedef struct _rx_mbox_areas_t {
+	p_rx_mbox_t 		*rxmbox_areas;
+	boolean_t		mbox_allocated;
+} rx_mbox_areas_t, *p_rx_mbox_areas_t;
+
+/*
+ * Global register definitions per chip and they are initialized
+ * using the function zero control registers.
+ * .
+ */
+typedef struct _txdma_globals {
+	boolean_t		mode32;
+} txdma_globals_t, *p_txdma_globals;
+
+
+typedef struct _rxdma_globals {
+	boolean_t		mode32;
+	uint16_t		rxdma_ck_div_cnt;
+	uint16_t		rxdma_red_ran_init;
+	uint32_t		rxdma_eing_timeout;
+} rxdma_globals_t, *p_rxdma_globals;
+
+
+
+typedef struct _nxge_neptune_t {
+	uint32_t flags;
+	uint32_t neptune_id;
+	uint32_t magic;
+	uint32_t syserr;
+#define NXGE_SYSERR_SMX		0x0001
+#define NXGE_SYSERR_MAC		0x0002
+#define NXGE_SYSERR_IPP		0x0004
+#define NXGE_SYSERR_FFLP	0x0008
+#define NXGE_SYSERR_ZCP		0x0010
+#define NXGE_SYSERR_TDMC	0x0020
+#define NXGE_SYSERR_RDMC	0x0040
+#define NXGE_SYSERR_TXC		0x0080
+#define NXGE_SYSERR_PEU		0x0100
+#define NXGE_SYSERR_META1	0x0200
+#define NXGE_SYSERR_META2	0x0400
+
+	nxge_os_mutex_t nxge_cfg_lock;
+	nxge_os_mutex_t nxge_syserr_lock;
+	nxge_os_mutex_t nxge_mdio_lock;
+	nxge_os_mutex_t nxge_tcam_lock;
+	nxge_os_mutex_t nxge_vlan_lock;
+	union {
+		uint32_t all;
+		uint8_t func[4];
+	} active;
+} nxge_neptune_t, *p_nxge_neptune_t;
+
+
+typedef struct _mc_addr_t {
+	ether_addr_st multcast_addr;
+	uint_t mc_addr_cnt;
+} mc_addr_t, *p_mc_addr_t;
+
+typedef struct _mc_bucket_t {
+	p_mc_addr_t addr_list;
+	uint_t list_size;
+} mc_bucket_t, *p_mc_bucket_t;
+
+typedef struct _mc_table_t {
+	p_mc_bucket_t bucket_list;
+	uint_t buckets_used;
+} mc_table_t, *p_mc_table_t;
+
+typedef struct _filter_t {
+	uint32_t all_phys_cnt;
+	uint32_t all_multicast_cnt;
+	uint32_t all_sap_cnt;
+} filter_t, *p_filter_t;
+
+
+/**********************************************************************/
+
+/*
+ * Neptune Device instance state information.
+ *
+ * Each instance is dynamically allocated on first attach.
+ */
+struct _nxge_t {
+
+	struct 		net_device *dev;
+	p_rx_rbr_rings_t 	rx_rbr_rings;
+	p_rx_rcr_rings_t 	rx_rcr_rings;
+	uint32_t	intr_timeout;
+	uint32_t	intr_thresh;
+	atomic_t tx_intr_set;
+
+	uint8_t		rdc[NXGE_MAX_RDCS];
+	uint8_t		tdc[NXGE_MAX_TDCS];
+
+	nxge_os_mutex_t	lock;
+	nxge_os_mutex_t	txq_lock;
+	nxge_os_mutex_t	*link_lock;
+	nxge_os_mutex_t	*syserr_lock;
+	nxge_os_mutex_t	*cfg_lock;
+	nxge_os_mutex_t	*tcam_lock;
+	nxge_os_mutex_t	*vlan_lock;
+	uint32_t *syserr;
+
+	/* Set when chip is actually in operational state
+	 * (ie. not power managed) */
+
+
+	struct 		pci_dev *pdev;
+
+	uint32_t	max_receive_pkts;
+	uint32_t	drv_state;	/* driver state bit flags */
+	uint64_t	nxge_debug_level; /* driver state bit flags */
+	uint8_t		hw_running;
+	uint8_t		opened;
+	uint8_t		nrdc;
+	uint8_t		def_rdc;
+	uint8_t		ntdc;
+	uint8_t		nports;
+	uint8_t		niu_type;
+	boolean_t	tx_softintr_sched;
+	uint8_t	function_num;
+	uint8_t	neptune_id;
+	uint8_t	neptune_index;
+
+	nxge_hw_list_t	*nxge_hw_p; 	/* pointer to per Neptune */
+	struct semaphore	pm_sem; /* open/close/suspend/resume */
+	struct 		tasklet_struct tx_task;
+
+	boolean_t	os_addr_mode32;	/* set to 1 for 32 bit mode */
+	uint32_t	hw_cfg_type;	/* HW resource configuration type*/
+	char		model[MAX_MOD_STR_LEN];
+	char		bd_model[MAX_BD_MOD_STR_LEN];
+	npi_vpd_info_t	vpd_info;
+
+
+	dev_regs_t	dev_regs;
+	npi_handle_t	npi_handle;
+	nxge_mac_t	mac;
+	nxge_ipp_t	ipp;
+	nxge_classify_t	classifier;
+	nxge_txc_t	txc;
+
+#define NXGE_FLAG_TX_HW_CSUM     0x00000001
+#define NXGE_FLAG_RX_HW_CSUM     0x00000002
+#define NXGE_FLAG_TSO_ENABLED    0x00000004
+
+	uint32_t	flags;
+
+	uint32_t	msg_enable;
+
+	/* TODO: Use pointers */
+	nxge_msix_t		nxge_intr_msix;
+	nxge_dma_pt_cfg_t 	pt_config;
+	nxge_class_pt_cfg_t 	class_config;
+
+	/* Logical device and group data structures. */
+	p_nxge_ldgv_t	ldgvp;
+
+	/* page size allocation */
+	uint32_t	page_size;
+	uint32_t	page_order;
+
+	/*
+	 * Blocks of memory are pre-allocated by the driver. They may include
+	 * blocks for configuration and buffers. These blocks then
+	 * will be broken up to a fixed number of blocks with
+	 * each block having the same block size (4K, 8K, 16K or
+	 * 32K) in the case of buffer blocks. For systems that
+	 * do not support DVMA, more than one big block will be
+	 * allocated.
+	 */
+	uint32_t		rx_default_block_size;
+	nxge_rx_block_size_t	rx_bksize_code;
+
+/* 	p_nxge_dma_pool_t	rx_buf_pool_p; */
+	p_nxge_page_t		*rx_buf_pool_p;
+	p_nxge_dma_pool_t	rx_cntl_pool_p;
+
+/* 	p_nxge_dma_pool_t	tx_buf_pool_p; */
+	struct sk_buff		**tx_buf_pool_p;
+	p_nxge_dma_pool_t	tx_cntl_pool_p;
+
+	/* Receive buffer block ring and completion ring. */
+	p_rx_mbox_areas_t 	rx_mbox_areas_p;
+
+	/* TODO:
+	 * Misc. configuration parameters of DMA channels.
+	 * (interrupt events masks. logical pages)
+	 */
+	p_rx_tx_params_t	rx_params;
+
+	uint32_t		start_rdc;
+	uint32_t		max_rdcs;
+	uint32_t		rdc_mask;
+
+	/* Transmit descriptors rings */
+	p_tx_rings_t 		tx_rings;
+	p_tx_mbox_areas_t	tx_mbox_areas_p;
+
+	uint32_t		start_tdc;
+	uint32_t		max_tdcs;
+	uint32_t		tx_lb_policy;
+	uint32_t		tx_reclaim_pending;
+
+	/* TODO:
+	 * Misc. configuration parameters of DMA channels.
+	 * (interrupt events masks. logical pages)
+	 */
+	p_rx_tx_params_t	tx_params;
+	uint32_t		param_count;
+	p_nxge_param_t		param_arr;
+
+
+	nxge_os_mutex_t		stats_lock;
+	p_nxge_stats_t		statsp;
+	p_nxge_stats_t		old_statsp;
+	struct net_device_stats	net_stats;
+	struct net_device_stats	old_net_stats;
+
+	mii_bmsr_t 		bmsr;		/* xcvr status at last poll. */
+	mii_bmsr_t 		soft_bmsr;	/* xcvr status kept by software. */
+
+	struct ethtool_cmd	*ep;	/* for ethertool settings */
+	uint16_t		seeprom_len;
+
+	/* TODO: add mac/phy related data structures */
+
+	npi_mac_addr_t	alt_addr_arr[MAC_MAX_ALT_ADDR_ENTRY];
+	uint16_t		hash_table[MAC_MAX_HASH_ENTRY];
+	uint16_t		if_multi_cnt;
+#define	PROMISC_F	0x1
+#define ALLMULTI_F	0x2
+#define HASHMATCH_F	0x4
+	uint16_t		if_flags;
+
+	filter_t 		filter;		/* Current instance filter */
+	p_hash_filter_t 	hash_filter;	/* Multicast hash filter. */
+
+	ulong_t 		sys_burst_sz;
+
+/*	uint32_t	pci_cfg[64 >> 2]; */
+
+	struct timer_list	wd_timer;
+	boolean_t		link_timer_stopped;
+	boolean_t		hw_st_timer_stopped;
+
+#ifdef CONFIG_NXGE_NAPI
+	struct sk_buff_head rx_pktq;
+#endif
+
+	uint			timer_ticks;
+	atomic_t		rst_pend_cnt;
+	atomic_t		rst_an_pend_cnt;
+
+	uint8_t 		msg_min;
+	boolean_t 		tx_copy_udp;
+	uint8_t 		crc_size;
+
+	uint32_t 		nxge_ncpus;
+	uint32_t 		nxge_cpumask;
+	uint32_t 		nxge_intrpkt;
+	uchar_t 		nxge_rxmode;
+	uint32_t	pci_cfg[64 >> 2];
+	/* These members are used for loopback tests */
+	p_lb_pkt_buf_t		lb_pkt_bufs_head;
+	p_lb_pkt_buf_t		lb_pkt_bufs_tail;
+	int			lb_rcv_pkt_cnt;
+};
+
+/*
+ * Driver state flags.
+ */
+#define	STATE_REGS_MAPPED	0x000000001	/* device registers mapped */
+#define	STATE_KSTATS_SETUP	0x000000002	/* kstats allocated	*/
+#define	STATE_NODE_CREATED	0x000000004	/* device node created	*/
+#define	STATE_HW_CONFIG_CREATED	0x000000008	/* hardware properties	*/
+#define	STATE_HW_INITIALIZED	0x000000010	/* hardware initialized	*/
+
+#define	STOP_POLL_THRESH 	9
+#define	START_POLL_THRESH	2
+
+/*
+ *
+ */
+
+#define DESC_NEXT_IDX(ring, cur)	(((cur) + 1) & ((ring)->wrap_mask))
+
+#define TX_RINGN(nep, n)	(nep)->tx_rings->rings[(n)]
+
+#define TX_DESC_NEXT_IDX(nep, r, c)	DESC_NEXT_IDX(TX_RINGN((nep), (r)), (c))
+
+#define TX_BUFF_COUNT(nep, r, x, y)    ((x) <= (y) ? ((y) - (x)) : \
+        (TX_RINGN((nep), (r))->ring_size - (x) + (y)))
+
+
+#define TX_BUFFS_AVAIL(nep, i)						\
+	(TX_RINGN((nep), (i))->ring_size - \
+	TX_RINGN((nep), (i))->descs_pending)
+
+/*
+ * Descriptor ring empty:
+ *		(1) head index is equal to tail index.
+ *		(2) wrapped around bits are the same.
+ * Descriptor ring full:
+ *		(1) head index is equal to tail index.
+ *		(2) wrapped around bits are different.
+ *
+ */
+#define TX_RING_EMPTY(head, head_wrap, tail, tail_wrap)	\
+	((head == tail && head_wrap == tail_wrap) ? B_TRUE : B_FALSE)
+
+#define TX_RING_FULL(head, head_wrap, tail, tail_wrap)	\
+	((head == tail && head_wrap != tail_wrap) ? B_TRUE : B_FALSE)
+
+
+
+
+
+#define RXC_RINGN(nep, n)	(nep)->rx_rcr_rings->rings[(n)]
+#define RXB_RINGN(nep, n)	(nep)->rx_rbr_rings->rings[(n)]
+
+#define RXC_DESC_NEXT_IDX(nep, r, c)	DESC_NEXT_IDX(RXC_RINGN((nep), (r)), (c))
+#define RXB_DESC_NEXT_IDX(nep, r, c)	DESC_NEXT_IDX(RXB_RINGN((nep), (r)), (c))
+
+#define RXC_BUFF_COUNT(nep, r, x, y)    ((x) <= (y) ? ((y) - (x)) : \
+        (RXC_RINGN((nep), (r))->ring_size - (x) + (y)))
+#define RXB_BUFF_COUNT(nep, r, x, y)    ((x) <= (y) ? ((y) - (x)) : \
+        (RXB_RINGN((nep), (r))->ring_size - (x) + (y)))
+
+
+#define RXC_BUFFS_AVAIL(nep, i)						\
+	  (RXC_RINGN((nep), (i))->out <= RXC_RINGN((nep), (i))->in ?	\
+	   RXC_RINGN((nep), (i))->out +					\
+	   (RXC_RINGN((nep), (i))->ring_size - 1) -			\
+	   RXC_RINGN((nep), (i))->in :					\
+	   RXC_RINGN((nep), (i))->out - RXC_RINGN((nep), (i))->in - 1)
+
+
+
+
+
+
+/* #define TXDMA_DESC_NEXT_INDEX(index, entries, wrap_mask) \ */
+/* 			((index + entries) & wrap_mask) */
+
+#define TX_DRR_WEIGHT_DEFAULT	0x001f
+
+#define NXGE_MC_EXACT_MATCH_SIZE       15
+#define NXGE_MC_HASH_SIZE              256
+#define NXGE_MC_HASH_MAX              (NXGE_MC_EXACT_MATCH_SIZE +	\
+				       NXGE_MC_HASH_SIZE)
+
+#endif /* __KERNEL_ */
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif
diff --git a/drivers/net/nxge/include/nxge_common.h b/drivers/net/nxge/include/nxge_common.h
new file mode 100644
index 0000000..58515f7
--- /dev/null
+++ b/drivers/net/nxge/include/nxge_common.h
@@ -0,0 +1,458 @@
+/*
+ * nxge_common.h	Neptune common interfaces (across OS)
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef	_SYS_NXGE_NXGE_COMMON_H
+#define	_SYS_NXGE_NXGE_COMMON_H
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#define	NXGE_DMA_START			B_TRUE
+#define	NXGE_DMA_STOP			B_FALSE
+
+/*
+ * Default DMA configurations.
+ */
+#define	NXGE_RDMA_PER_NIU_PORT		(NXGE_MAX_RDCS/NXGE_PORTS_NIU)
+#define	NXGE_TDMA_PER_NIU_PORT		(NXGE_MAX_TDCS_NIU/NXGE_PORTS_NIU)
+#define	NXGE_RDMA_PER_NEP_PORT		(NXGE_MAX_RDCS/NXGE_PORTS_NEPTUNE)
+#define	NXGE_TDMA_PER_NEP_PORT		(NXGE_MAX_TDCS/NXGE_PORTS_NEPTUNE)
+#define	NXGE_RDCGRP_PER_NIU_PORT	(NXGE_MAX_RDC_GROUPS/NXGE_PORTS_NIU)
+#define	NXGE_RDCGRP_PER_NEP_PORT	(NXGE_MAX_RDC_GROUPS/NXGE_PORTS_NEPTUNE)
+
+#ifdef  RX_INTR_BLANKING_LDG
+#define	NXGE_TIMER_RESO			600 /*recomended by Michael 11/06/2006 */
+#define	NXGE_TIMER_LDG			25  /*recomended by Michael 11/06/2006 */
+#else
+/* #define	NXGE_TIMER_RESO			2 */
+#define	NXGE_TIMER_RESO			600 /*recomended by Michael 11/06/2006 */
+#define	NXGE_TIMER_LDG			2
+#endif
+/*
+ * Receive and Transmit DMA definitions
+ */
+
+#define	NXGE_DMA_BLOCK		(64 * 64)
+
+#define	NXGE_RBR_RBB_MIN	(128)
+#define	NXGE_RBR_RBB_MAX	(64 * 128 -1)
+
+
+#define	NXGE_RBR_RBB_DEFAULT	(1024) /* x86 hello */
+#define	NXGE_RBR_SPARE		0
+
+
+#define	NXGE_RCR_MIN		(NXGE_RBR_RBB_MIN * 2)
+
+#define	NXGE_RCR_MAX		(65355) /* MAX hardware supported */
+
+#ifdef USE_RX_BIG_BUF
+#define	NXGE_RCR_DEFAULT	(NXGE_RBR_RBB_DEFAULT * 8)
+#else
+#define	NXGE_RCR_DEFAULT	(NXGE_RBR_RBB_DEFAULT * 4)
+#endif
+
+#define RBR_POST_BATCH 0x10
+#define RBR_POST_BATCH_MASK   RBR_POST_BATCH - 1
+
+#define	NXGE_TX_RING_DEFAULT	(4096)
+#define	NXGE_TX_RING_MAX	(64 * 128 - 1)
+
+#define	NXGE_TX_RECLAIM 	32
+
+/* per receive DMA channel configuration data structure */
+typedef struct  nxge_rdc_cfg {
+	uint32_t	flag;		/* 0: not configured, 1: configured */
+	struct nxge_hw_list *nxge_hw_p;
+	uint32_t	partition_id;
+	uint32_t	port;		/* function number */
+	uint32_t	rx_group_id;
+
+	/* Partitioning, DMC function zero. */
+	uint32_t	rx_log_page_vld_page0;	/* TRUE or FALSE */
+	uint32_t	rx_log_page_vld_page1;	/* TRUE or FALSE */
+	uint64_t	rx_log_mask1;
+	uint64_t	rx_log_value1;
+	uint64_t	rx_log_mask2;
+	uint64_t	rx_log_value2;
+	uint64_t	rx_log_page_relo1;
+	uint64_t	rx_log_page_relo2;
+	uint64_t	rx_log_page_hdl;
+
+	/* WRED parameters, DMC function zero */
+	uint32_t	red_enable;
+
+	uint32_t	thre_syn;
+	uint32_t	win_syn;
+	uint32_t	threshold;
+	uint32_t	win_non_syn;
+
+	/* RXDMA configuration, DMC */
+	char		*rdc_mbaddr_p;	/* mailbox address */
+	uint32_t	min_flag;	/* TRUE for 18 bytes header */
+
+	/* Software Reserved Packet Buffer Offset, DMC */
+	uint32_t	sw_offset;
+
+	/* RBR Configuration A */
+	uint64_t	rbr_staddr;	/* starting address of RBR */
+	uint32_t	rbr_nblks;	/* # of RBR entries */
+	uint32_t	rbr_len;	/* # of RBR entries in 64B lines */
+
+	/* RBR Configuration B */
+	uint32_t	bksize;		/* Block size is fixed. */
+#define	RBR_BKSIZE_4K			0
+#define	RBR_BKSIZE_8K			1
+#define	RBR_BKSIZE_16K			2
+#define	RBR_BKSIZE_32K			3
+
+	uint32_t	bufsz2;
+#define	RBR_BUFSZ2_2K			0
+#define	RBR_BUFSZ2_2K_BYTES		(2 * 1024)
+#define	RBR_BUFSZ2_4K			1
+#define	RBR_BUFSZ2_4K_BYTES		(4 * 1024)
+#define	RBR_BUFSZ2_8K			2
+#define	RBR_BUFSZ2_8K_BYTES		(8 * 1024)
+#define	RBR_BUFSZ2_16K			3
+#define	RBR_BUFSZ2_16K_BYTES		(16 * 1024)
+
+	uint32_t	bufsz1;
+#define	RBR_BUFSZ1_1K			0
+#define	RBR_BUFSZ1_1K_BYTES		1024
+#define	RBR_BUFSZ1_2K			1
+#define	RBR_BUFSZ1_2K_BYTES		(2 * 1024)
+#define	RBR_BUFSZ1_4K			2
+#define	RBR_BUFSZ1_4K_BYTES		(4 * 1024)
+#define	RBR_BUFSZ1_8K			3
+#define	RBR_BUFSZ1_8K_BYTES		(8 * 1024)
+
+	uint32_t	bufsz0;
+#define	RBR_BUFSZ0_256B			0
+#define	RBR_BUFSZ0_256_BYTES		256
+#define	RBR_BUFSZ0_512B			1
+#define	RBR_BUFSZ0_512B_BYTES		512
+#define	RBR_BUFSZ0_1K			2
+#define	RBR_BUFSZ0_1K_BYTES		(1024)
+#define	RBR_BUFSZ0_2K			3
+#define	RBR_BUFSZ0_2K_BYTES		(2 * 1024)
+
+	/* Receive buffers added by the software */
+	uint32_t	bkadd;		/* maximum size is 1 million */
+
+	/* Receive Completion Ring Configuration A */
+	uint32_t	rcr_len;	/* # of 64B blocks, each RCR is 8B */
+	uint64_t	rcr_staddr;
+
+	/* Receive Completion Ring Configuration B */
+	uint32_t	pthres;		/* packet threshold */
+	uint32_t	entout;		/* enable timeout */
+	uint32_t	timeout;	/* timeout value */
+
+	/* Logical Device Group Number */
+	uint16_t	rx_ldg;
+	uint16_t	rx_ld_state_flags;
+
+	/* Receive DMA Channel Event Mask */
+	uint64_t	rx_dma_ent_mask;
+
+	/* 32 bit (set to 1) or 64 bit (set to 0) addressing mode */
+	uint32_t	rx_addr_md;
+} nxge_rdc_cfg_t, *p_nxge_rdc_cfg_t;
+
+/*
+ * Per Transmit DMA Channel Configuration Data Structure (32 TDC)
+ */
+typedef struct  nxge_tdc_cfg {
+	uint32_t	flag;		/* 0: not configured 1: configured */
+	struct nxge_hw_list *nxge_hw_p;
+	uint32_t	partition_id;
+	uint32_t	port; 		/* function number */
+	/* partitioning, DMC function zero (All 0s for non-partitioning) */
+	uint32_t	tx_log_page_vld_page0;	/* TRUE or FALSE */
+	uint32_t	tx_log_page_vld_page1;	/* TRUE or FALSE */
+	uint64_t	tx_log_mask1;
+	uint64_t	tx_log_value1;
+	uint64_t	tx_log_mask2;
+	uint64_t	tx_log_value2;
+	uint64_t	tx_log_page_relo1;
+	uint64_t	tx_log_page_relo2;
+	uint64_t	tx_log_page_hdl;
+
+	/* Transmit Ring Configuration */
+	uint64_t	tx_staddr;
+	uint64_t	tx_rng_len;	/* in 64 B Blocks */
+#define	TX_MAX_BUF_SIZE			4096
+
+	/* TXDMA configuration, DMC */
+	char		*tdc_mbaddr_p;	/* mailbox address */
+
+	/* Logical Device Group Number */
+	uint16_t	tx_ldg;
+	uint16_t	tx_ld_state_flags;
+
+	/* TXDMA event flags */
+	uint64_t	tx_event_mask;
+
+	/* Transmit threshold before reclamation */
+	uint32_t	tx_rng_threshold;
+#define	TX_RING_THRESHOLD		(TX_DEFAULT_MAX_GPS/4)
+#define	TX_RING_JUMBO_THRESHOLD		(TX_DEFAULT_JUMBO_MAX_GPS/4)
+
+	/* For reclaim: a wrap-around counter (packets transmitted) */
+	uint32_t	tx_pkt_cnt;
+	/* last packet with the mark bit set */
+	uint32_t	tx_lastmark;
+} nxge_tdc_cfg_t, *p_nxge_tdc_cfg_t;
+
+#define	RDC_TABLE_ENTRY_METHOD_SEQ	0
+#define	RDC_TABLE_ENTRY_METHOD_REP	1
+
+/* per receive DMA channel table group data structure */
+typedef struct nxge_rdc_grp {
+	uint32_t	flag;		/* 0:not configured 1: configured */
+	uint8_t	port;
+	uint8_t	partition_id;
+	uint8_t	rx_group_id;
+	uint8_t	start_rdc;	/* assume assigned in sequence	*/
+	uint8_t	max_rdcs;
+	uint8_t	def_rdc;
+	uint8_t		rdc[NXGE_MAX_RDCS];
+	uint16_t	config_method;	/* debug: FIXME */
+} nxge_rdc_grp_t, *p_nxge_rdc_grp_t;
+
+/* Common RDC and TDC configuration of DMC */
+typedef struct _nxge_dma_common_cfg_t {
+	uint16_t	rdc_red_ran_init; /* RED initial seed value */
+
+	/* Transmit Ring */
+} nxge_dma_common_cfg_t, *p_nxge_dma_common_cfg_t;
+
+/*
+ * VLAN and MAC table configurations:
+ *  Each VLAN ID should belong to at most one RDC group.
+ *  Each port could own multiple RDC groups.
+ *  Each MAC should belong to one RDC group (FIXME, not sure)
+ */
+typedef struct nxge_mv_cfg {
+	uint8_t		flag;			/* 0:unconfigure 1:configured */
+	uint8_t		rdctbl;			/* RDC channel table group */
+	uint8_t		mpr_npr;		/* MAC and VLAN preference */
+	uint8_t		odd_parity;
+} nxge_mv_cfg_t, *p_nxge_mv_cfg_t;
+
+typedef struct nxge_param_map {
+#if defined(_BIG_ENDIAN)
+	uint32_t		rsrvd2:2;	/* [30:31] rsrvd */
+	uint32_t		remove:1;	/* [29] Remove */
+	uint32_t		pref:1;		/* [28] preference */
+	uint32_t		rsrv:4;		/* [27:24] preference */
+	uint32_t		map_to:8;	/* [23:16] map to resource */
+	uint32_t		param_id:16;	/* [15:0] Param ID */
+#else
+	uint32_t		param_id:16;	/* [15:0] Param ID */
+	uint32_t		map_to:8;	/* [23:16] map to resource */
+	uint32_t		rsrv:4;		/* [27:24] preference */
+	uint32_t		pref:1;		/* [28] preference */
+	uint32_t		remove:1;	/* [29] Remove */
+	uint32_t		rsrvd2:2;	/* [30:31] rsrvd */
+#endif
+} nxge_param_map_t, *p_nxge_param_map_t;
+
+typedef struct nxge_rcr_param {
+#if defined(_BIG_ENDIAN)
+	uint32_t		rsrvd2:2;	/* [30:31] rsrvd */
+	uint32_t		remove:1;	/* [29] Remove */
+	uint32_t		rsrv:5;		/* [28:24] preference */
+	uint32_t		rdc:8;		/* [23:16] rdc # */
+	uint32_t		cfg_val:16;	/* [15:0] interrupt parameter */
+#else
+	uint32_t		cfg_val:16;	/* [15:0] interrupt parameter */
+	uint32_t		rdc:8;		/* [23:16] rdc # */
+	uint32_t		rsrv:5;		/* [28:24] preference */
+	uint32_t		remove:1;	/* [29] Remove */
+	uint32_t		rsrvd2:2;	/* [30:31] rsrvd */
+#endif
+} nxge_rcr_param_t, *p_nxge_rcr_param_t;
+
+/* Needs to have entries in the ndd table */
+/*
+ * Hardware properties created by fcode.
+ * In order for those properties visible to the user
+ * command ndd, we need to add the following properties
+ * to the ndd defined parameter array and data structures.
+ *
+ * Use default static configuration for x86.
+ */
+typedef struct nxge_hw_pt_cfg {
+	uint32_t	partition_id;	 /* partition Id		*/
+	uint32_t	read_write_mode; /* read write permission mode	*/
+	uint32_t	function_number; /* function number		*/
+	uint32_t	start_tdc;	 /* start TDC (0 - 31)		*/
+	uint32_t	max_tdcs;	 /* max TDC in sequence		*/
+	uint32_t	start_rdc;	 /* start RDC (0 - 31)		*/
+	uint32_t	max_rdcs;	 /* max rdc in sequence		*/
+	uint32_t	ninterrupts;	/* obp interrupts(mac/mif/syserr) */
+	uint32_t	mac_ldvid;
+	uint32_t	mif_ldvid;
+	uint32_t	ser_ldvid;
+	uint32_t	def_rdc;	 /* default RDC			*/
+	uint32_t	drr_wt;		 /* port DRR weight		*/
+	uint32_t	rx_full_header;	 /* select the header flag	*/
+	uint32_t	start_grpid;	 /* starting group ID		*/
+	uint32_t	max_grpids;	 /* max group ID		*/
+	uint32_t	start_rdc_grpid; /* starting RDC group ID	*/
+	uint32_t	max_rdc_grpids;	 /* max RDC group ID		*/
+	uint32_t	start_ldg;	 /* starting logical group # 	*/
+	uint32_t	max_ldgs;	 /* max logical device group	*/
+	uint32_t	max_ldvs;	 /* max logical devices		*/
+	uint32_t	start_mac_entry; /* where to put the first mac	*/
+	uint32_t	max_macs;	 /* the max mac entry allowed	*/
+	uint32_t	mac_pref;	 /* preference over VLAN	*/
+	uint32_t	def_mac_rxdma_grpid; /* default RDC group ID	*/
+	uint32_t	start_vlan;	 /* starting VLAN ID		*/
+	uint32_t	max_vlans;	 /* max VLAN ID			*/
+	uint32_t	vlan_pref;	 /* preference over MAC		*/
+	uint32_t	def_vlan_rxdma_grpid; /* default RDC group Id	*/
+
+	/* Expand if we have more hardware or default configurations    */
+	uint16_t	ldg[NXGE_INT_MAX_LDG];
+	uint16_t	ldg_chn_start;
+} nxge_hw_pt_cfg_t, *p_nxge_hw_pt_cfg_t;
+
+
+/* per port configuration */
+typedef struct nxge_dma_pt_cfg {
+	uint8_t		mac_port;	/* MAC port (function)		*/
+	nxge_hw_pt_cfg_t hw_config;	/* hardware configuration 	*/
+
+	uint32_t alloc_buf_size;
+	uint32_t rbr_size;
+	uint32_t rcr_size;
+
+	/*
+	 * Configuration for hardware initialization based on the
+	 * hardware properties or the default properties.
+	 */
+	uint32_t	tx_dma_map;	/* Transmit DMA channel bit map */
+
+	/* Receive DMA channel */
+	nxge_rdc_grp_t	rdc_grps[NXGE_MAX_RDC_GROUPS];
+
+	uint16_t	rcr_timeout[NXGE_MAX_RDCS];
+	uint16_t	rcr_threshold[NXGE_MAX_RDCS];
+	uint8_t	rcr_full_header;
+	uint16_t	rx_drr_weight;
+
+	/* Add more stuff later */
+} nxge_dma_pt_cfg_t, *p_nxge_dma_pt_cfg_t;
+
+/* classification configuration */
+typedef struct nxge_class_pt_cfg {
+
+	/* MAC table */
+	nxge_mv_cfg_t	mac_host_info[NXGE_MAX_MACS];
+
+	/* VLAN table */
+	nxge_mv_cfg_t	vlan_tbl[NXGE_MAX_VLANS];
+	/* class config value */
+	uint32_t	init_h1;
+	uint16_t	init_h2;
+	uint8_t mcast_rdcgrp;
+	uint8_t mac_rdcgrp;
+	uint32_t	class_cfg[TCAM_CLASS_MAX];
+} nxge_class_pt_cfg_t, *p_nxge_class_pt_cfg_t;
+
+/* per Neptune sharable resources among ports */
+typedef struct nxge_common {
+	uint32_t		partition_id;
+	boolean_t		mode32;
+	/* DMA Channels: RDC and TDC */
+	nxge_rdc_cfg_t		rdc_config[NXGE_MAX_RDCS];
+	nxge_tdc_cfg_t		tdc_config[NXGE_MAX_TDCS];
+	nxge_dma_common_cfg_t	dma_common_config;
+
+	uint32_t		timer_res;
+	boolean_t		ld_sys_error_set;
+	uint8_t			sys_error_owner;
+
+	/* Layer 2/3/4 */
+	uint16_t		class2_etype;
+	uint16_t		class3_etype;
+
+	/* FCRAM (hashing) */
+	uint32_t		hash1_initval;
+	uint32_t		hash2_initval;
+} nxge_common_t, *p_nxge_common_t;
+
+/*
+ * Partition (logical domain) configuration per Neptune/NIU.
+ */
+typedef struct nxge_part_cfg {
+	uint32_t	rdc_grpbits;	/* RDC group bit masks */
+	uint32_t	tdc_bitmap;	/* bounded TDC */
+	nxge_dma_pt_cfg_t pt_config[NXGE_MAX_PORTS];
+
+	/* Flow Classification Partition (flow partition select register) */
+	uint8_t		hash_lookup;	/* external lookup is available */
+	uint8_t		base_mask;	/* select bits in base_h1 to replace */
+					/* bits [19:15} in Hash 1. */
+	uint8_t		base_h1;	/* value to replace Hash 1 [19:15]. */
+
+	/* Add more here */
+	uint32_t	attributes;	/* permission and attribute bits */
+#define	FZC_SERVICE_ENTITY		0x01
+#define	FZC_READ_WRITE			0x02
+#define	FZC_READ_ONLY			0x04
+} nxge_part_cfg_t, *p_nxge_part_cfg_t;
+
+typedef struct nxge_hw_list {
+	struct nxge_hw_list 	*next;
+	nxge_os_mutex_t		nxge_share_lock;
+	nxge_dev_info_t		*parent_devp;
+	nxge_intr_cookie_t 	intr_cookie;
+	nxge_common_t		nxge_common;
+	struct _nxge_t		*nxge_p[NXGE_MAX_PORTS];
+	uint32_t		ndevs;
+} nxge_hw_list_t, *p_nxge_hw_list_t;
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif	/* _SYS_NXGE_NXGE_COMMON_H */
diff --git a/drivers/net/nxge/include/nxge_common_impl.h b/drivers/net/nxge/include/nxge_common_impl.h
new file mode 100644
index 0000000..bdd2510
--- /dev/null
+++ b/drivers/net/nxge/include/nxge_common_impl.h
@@ -0,0 +1,419 @@
+/*
+ * nxge_common_impl.h	Neptune common implementation interfaces
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef	_SYS_NXGE_NXGE_COMMON_IMPL_H
+#define	_SYS_NXGE_NXGE_COMMON_IMPL_H
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#if defined(__i386) || defined(__x86_64)
+#define _LITTLE_ENDIAN
+#define _BIT_FIELDS_LTOH
+#endif
+
+#if defined(__sparc)
+#define _BIG_ENDIAN
+#define _BIT_FIELDS_HTOL
+#endif
+
+
+#define	NPI_REGH(npi_handle)		(npi_handle.regh)
+#define	NPI_REGP(npi_handle)		(npi_handle.regp)
+
+#if defined(NXGE_DEBUG_DMA) || defined(NXGE_DEBUG_TXC)
+#define	__NXGE_STATIC
+#define	__NXGE_INLINE
+#else
+#define	__NXGE_STATIC			static
+#define	__NXGE_INLINE			inline
+#endif
+
+#ifdef	AXIS_DEBUG
+#define	AXIS_WAIT			(100000)
+#define	AXIS_LONG_WAIT			(100000)
+#define	AXIS_WAIT_W			(80000)
+#define	AXIS_WAIT_R			(100000)
+#define	AXIS_WAIT_LOOP			(4000)
+#define	AXIS_WAIT_PER_LOOP		(AXIS_WAIT_R/AXIS_WAIT_LOOP)
+#endif
+
+
+
+#define		NO_DEBUG	0x0000000000000000ULL
+#define		MDT_CTL		0x0000000000000001ULL
+#define		RX_CTL		0x0000000000000002ULL
+#define		TX_CTL		0x0000000000000004ULL
+#define		OBP_CTL		0x0000000000000008ULL
+
+#define		VPD_CTL		0x0000000000000010ULL
+#define		DDI_CTL		0x0000000000000020ULL
+#define		MEM_CTL		0x0000000000000040ULL
+#define		SAP_CTL		0x0000000000000080ULL
+
+#define		IOC_CTL		0x0000000000000100ULL
+#define		MOD_CTL		0x0000000000000200ULL
+#define		DMA_CTL		0x0000000000000400ULL
+#define		STR_CTL		0x0000000000000800ULL
+
+#define		INT_CTL		0x0000000000001000ULL
+#define		SYSERR_CTL	0x0000000000002000ULL
+#define		KST_CTL		0x0000000000004000ULL
+#define		PCS_CTL		0x0000000000008000ULL
+
+#define		MII_CTL		0x0000000000010000ULL
+#define		MIF_CTL		0x0000000000020000ULL
+#define		FCRAM_CTL	0x0000000000040000ULL
+#define		MAC_CTL		0x0000000000080000ULL
+
+#define		IPP_CTL		0x0000000000100000ULL
+#define		DMA2_CTL	0x0000000000200000ULL
+#define		RX2_CTL		0x0000000000400000ULL
+#define		TX2_CTL		0x0000000000800000ULL
+
+#define		MEM2_CTL	0x0000000001000000ULL
+#define		MEM3_CTL	0x0000000002000000ULL
+#define		NEMO_CTL	0x0000000004000000ULL
+#define		NDD_CTL		0x0000000008000000ULL
+#define		NDD2_CTL	0x0000000010000000ULL
+
+#define		TCAM_CTL	0x0000000020000000ULL
+#define		CFG_CTL		0x0000000040000000ULL
+#define		CFG2_CTL	0x0000000080000000ULL
+
+#define		FFLP_CTL	TCAM_CTL | FCRAM_CTL
+
+#define		VIR_CTL		0x0000000100000000ULL
+#define		VIR2_CTL	0x0000000200000000ULL
+
+#define		NXGE_NOTE	0x0000001000000000ULL
+#define		NXGE_ERR_CTL	0x0000002000000000ULL
+
+#define		DUMP_ALWAYS	0x2000000000000000ULL
+
+/* NPI Debug and Error defines */
+#define		NPI_RDC_CTL	0x0000000000000001ULL
+#define		NPI_TDC_CTL	0x0000000000000002ULL
+#define		NPI_TXC_CTL	0x0000000000000004ULL
+#define		NPI_IPP_CTL	0x0000000000000008ULL
+
+#define		NPI_XPCS_CTL	0x0000000000000010ULL
+#define		NPI_PCS_CTL	0x0000000000000020ULL
+#define		NPI_ESR_CTL	0x0000000000000040ULL
+#define		NPI_BMAC_CTL	0x0000000000000080ULL
+#define		NPI_XMAC_CTL	0x0000000000000100ULL
+#define		NPI_MAC_CTL	NPI_BMAC_CTL | NPI_XMAC_CTL
+
+#define		NPI_ZCP_CTL	0x0000000000000200ULL
+#define		NPI_TCAM_CTL	0x0000000000000400ULL
+#define		NPI_FCRAM_CTL	0x0000000000000800ULL
+#define		NPI_FFLP_CTL	NPI_TCAM_CTL | NPI_FCRAM_CTL
+
+#define		NPI_VIR_CTL	0x0000000000001000ULL
+#define		NPI_PIO_CTL	0x0000000000002000ULL
+#define		NPI_VIO_CTL	0x0000000000004000ULL
+
+#define		NPI_REG_CTL	0x0000000040000000ULL
+#define		NPI_CTL		0x0000000080000000ULL
+#define		NPI_ERR_CTL	0x0000000080000000ULL
+
+#include <linux/autoconf.h>
+#include <linux/version.h>
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/compiler.h>
+#include <linux/slab.h>
+#include <linux/delay.h>
+#include <linux/init.h>
+#include <linux/ioport.h>
+#include <linux/pci.h>
+#include <linux/mm.h>
+#include <linux/highmem.h>
+#include <linux/list.h>
+
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/mii.h>
+#include <linux/skbuff.h>
+#include <linux/ethtool.h>
+#include <linux/crc32.h>
+#include <linux/random.h>
+
+#include <linux/if_vlan.h>
+#include <linux/llc.h>
+#include <linux/ip.h>
+#include <linux/ipv6.h>
+#include <linux/tcp.h>
+#include <linux/udp.h>
+
+#include <net/checksum.h>
+
+#include <asm/atomic.h>
+#include <asm/system.h>
+#include <asm/io.h>
+#include <asm/byteorder.h>
+#include <asm/uaccess.h>
+
+typedef unsigned char	uchar_t;
+typedef unsigned short	ushort_t;
+typedef unsigned int	uint_t;
+typedef unsigned long	ulong_t;
+
+#define	uintptr_t	unsigned long
+
+#define	ETHERADDRL ETH_ALEN
+/*
+ * Ethernet address - 6 octets
+ */
+struct	ether_addr {
+	uchar_t ether_addr_octet[ETHERADDRL];
+};
+
+typedef struct ether_addr ether_addr_st, *p_ether_addr_t;
+
+typedef enum {
+#undef B_FALSE
+	B_FALSE = 0,
+#undef B_TRUE
+	B_TRUE = 1
+} boolean_t;
+
+
+typedef enum  {
+	BKSIZE_4K,
+	BKSIZE_8K,
+	BKSIZE_16K,
+	BKSIZE_32K
+} nxge_rx_block_size_t;
+
+#define	IS_PORT_NUM_VALID(portn) (portn < 4)
+
+
+typedef spinlock_t			nxge_os_mutex_t;
+typedef	rwlock_t			nxge_os_rwlock_t;
+
+typedef	struct pci_dev			nxge_dev_info_t;
+typedef	void * 				nxge_intr_cookie_t;
+
+typedef void *				nxge_os_acc_handle_t;
+typedef	nxge_os_acc_handle_t		npi_reg_handle_t;
+typedef char				*npi_reg_ptr_t;
+
+typedef void *				nxge_os_dma_handle_t;
+typedef void				nxge_os_dma_common_t;
+typedef void				nxge_os_block_mv_t;
+typedef int				nxge_os_frtn_t;
+
+#define	MUTEX_INIT(lock, nm, tp, arg)	spin_lock_init((lock))
+#define	MUTEX_ENTER(lock)		spin_lock((lock))
+#define	MUTEX_TRY_ENTER(lock)		spin_trylock((lock))
+#define	MUTEX_EXIT(lock)		spin_unlock((lock))
+#define	MUTEX_ENTER_INT(lock, flags)	spin_lock_irqsave(lock, flags)
+#define	MUTEX_EXIT_INT(lock, flags)	spin_unlock_irqrestore(lock, flags)
+#define	MUTEX_DESTROY(lock)
+
+#define	RW_INIT(lock, nm, tp, arg)	rw_lock_init((lock))
+#define	RW_ENTER_WRITER(lock)		write_lock(lock)
+#define	RW_ENTER_READER(lock)		read_lock(lock)
+#define	RW_EXIT(lock)			write_unlock(lock)
+#define	RW_EXIT_READ(lock)		read_unlock(lock)
+#define	RW_DESTROY(lock)
+
+#define	NXGE_DELAY(usecs)	\
+(usecs < 20000 ?  udelay(usecs) : mdelay(usecs/1000))
+
+#define KMALLOC_ONE_PAGE_SIZE 4080
+
+static inline void * nxge_kzalloc(size_t size, int flag, int zero)
+{
+	void *ptr = 0;
+	unsigned long order = 0;
+	unsigned long paged_size = PAGE_SIZE;
+	if (size <= KMALLOC_ONE_PAGE_SIZE) {
+		ptr = kmalloc(size, flag);
+	} else {
+		while (paged_size < size) {
+			paged_size = PAGE_SIZE << order;
+			order++;
+		}
+		order = get_order(paged_size);
+		ptr = (void *) __get_free_pages(flag, order);
+	}
+	if ((zero == B_TRUE) && (ptr != NULL))
+		memset(ptr, 0, size);
+
+	return (ptr);
+}
+
+
+static inline void  nxge_kfree(void *ptr, size_t size)
+{
+	unsigned long order = 0;
+	int paged_size = PAGE_SIZE;
+	if (size <= KMALLOC_ONE_PAGE_SIZE) {
+		kfree(ptr);
+	} else {
+		while (paged_size < size) {
+			paged_size = PAGE_SIZE << order;
+			order++;
+		}
+		order = get_order(paged_size);
+		free_pages((unsigned long)ptr, order);
+	}
+}
+
+#define	KMEM_ALLOC(size, flag)		nxge_kzalloc(size, flag, B_FALSE)
+#define	KMEM_ZALLOC(size, flag)		nxge_kzalloc(size, flag, B_TRUE)
+#define	KMEM_FREE(buf, size)	nxge_kfree(buf, size)
+
+#ifndef readq
+static inline uint64_t readq(void *addr)
+{
+	uint32_t val32 = readl(addr);
+	uint64_t val64 = (uint64_t) readl(addr + 4);
+	return (val32 | (val64 << 32));
+}
+#endif
+
+#ifndef writeq
+static inline void writeq(uint64_t val64, void *addr)
+{
+	writel((uint32_t)(val64), addr);
+	writel((uint32_t)(val64 >> 32), (addr + 4));
+}
+#endif
+
+#define	NXGE_PIO_READ8(handle, devaddr, offset)		\
+	(readb((caddr_t)devaddr + offset))
+
+#define	NXGE_PIO_READ16(handle, devaddr, offset)	\
+	(readw((caddr_t)devaddr + offset))
+
+#define	NXGE_PIO_READ32(handle, devaddr, offset)	\
+	(readl((caddr_t)devaddr + offset))
+
+#define	NXGE_PIO_READ64(handle, devaddr, offset)	\
+	(readq((caddr_t)devaddr + offset))
+
+#define	NXGE_PIO_WRITE8(handle, devaddr, offset, data)	\
+	(writeb(data, ((caddr_t)devaddr + offset))
+
+#define	NXGE_PIO_WRITE16(handle, devaddr, offset, data)	\
+	(writew(data, ((caddr_t)devaddr + offset)))
+
+#define	NXGE_PIO_WRITE32(handle, devaddr, offset, data)	\
+	(writel(data, ((caddr_t)devaddr + offset)))
+
+#define	NXGE_PIO_WRITE64(handle, devaddr, offset, data)	\
+	(writeq(data, ((caddr_t)devaddr + offset)))
+
+#define	NXGE_NPI_PIO_READ8(npi_handle, offset)	\
+	(readb(NPI_REGP(npi_handle) + offset))
+
+#define	NXGE_NPI_PIO_READ16(npi_handle, offset)	\
+	(readw(NPI_REGP(npi_handle) + offset))
+
+#define	NXGE_NPI_PIO_READ32(npi_handle, offset)	\
+	(readl(NPI_REGP(npi_handle) + offset))
+
+
+#define	NXGE_NPI_PIO_READ64(npi_handle, offset)	\
+	(readq(NPI_REGP(npi_handle) + offset))
+
+#define	NXGE_NPI_PIO_WRITE8(npi_handle, offset, data)	\
+	(writeb(data, NPI_REGP(npi_handle) + offset))
+
+#define	NXGE_NPI_PIO_WRITE16(npi_handle, offset, data)	\
+	(writew(data, NPI_REGP(npi_handle) + offset))
+
+#define	NXGE_NPI_PIO_WRITE32(npi_handle, offset, data)	\
+	(writel(data, NPI_REGP(npi_handle) + offset))
+
+
+#define	NXGE_NPI_PIO_WRITE64(npi_handle, offset, data)	\
+	(writeq(data, NPI_REGP(npi_handle) + offset))
+
+#define	NXGE_MEM_PIO_READ8(npi_handle)	(*((uint8_t *)NPI_REGP(npi_handle)))
+
+#define	NXGE_MEM_PIO_READ16(npi_handle)	(*((uint16_t *)NPI_REGP(npi_handle)))
+
+#define	NXGE_MEM_PIO_READ32(npi_handle)	(*((uint32_t *)NPI_REGP(npi_handle)))
+
+#define	NXGE_MEM_PIO_READ64(npi_handle)	(*((uint64_t *)NPI_REGP(npi_handle)))
+
+#define	NXGE_MEM_PIO_WRITE8(npi_handle, data)	\
+	(*((uint8_t *)NPI_REGP(npi_handle))) = ((uint8_t)data)
+
+#define	NXGE_MEM_PIO_WRITE16(npi_handle, data)	\
+	(*((uint16_t *)NPI_REGP(npi_handle))) = ((uint16_t)data)
+
+#define	NXGE_MEM_PIO_WRITE32(npi_handle, data)	\
+	(*((uint32_t *)NPI_REGP(npi_handle))) = ((uint32_t)data)
+
+#define	NXGE_MEM_PIO_WRITE64(npi_handle, data)	\
+	(*((uint64_t *)NPI_REGP(npi_handle))) = ((uint64_t)data)
+
+
+#define	NXGE_REG_RD64(handle, offset, val_p) {\
+	*(val_p) = NXGE_NPI_PIO_READ64(handle, offset);\
+}
+
+/*
+ *	 In COSIM mode, we could loop for very long time when polling
+ *  for the completion of a Clause45 frame MDIO operations. Display
+ *  one rtrace line for each poll can result in messy screen.  Add
+ *  this MACRO for no rtrace show.
+ */
+#define	NXGE_REG_RD64_NO_SHOW(handle, offset, val_p) {\
+	*(val_p) = NXGE_NPI_PIO_READ64(handle, offset);\
+}
+
+#define	NXGE_REG_WR64(handle, offset, val) {\
+	NXGE_NPI_PIO_WRITE64(handle, (offset), (val));\
+}
+
+
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif	/* _SYS_NXGE_NXGE_COMMON_IMPL_H */
diff --git a/drivers/net/nxge/include/nxge_defs.h b/drivers/net/nxge/include/nxge_defs.h
new file mode 100644
index 0000000..a541234
--- /dev/null
+++ b/drivers/net/nxge/include/nxge_defs.h
@@ -0,0 +1,477 @@
+/*
+ * nxge_defs.h	Neptune HW block offsets
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef	_SYS_NXGE_NXGE_DEFS_H
+#define	_SYS_NXGE_NXGE_DEFS_H
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+/*
+ * Block Address Assignment (24-bit base address)
+ * (bits [23:20]: block	 [19]: set to 1 for FZC	)
+ */
+#define	PIO			0x000000
+#define	FZC_PIO			0x080000
+#define	RESERVED_1		0x100000
+#define	FZC_MAC			0x180000
+#define	RESERVED_2		0x200000
+#define	FZC_IPP			0x280000
+#define	FFLP			0x300000
+#define	FZC_FFLP		0x380000
+#define	PIO_VADDR		0x400000
+#define	RESERVED_3		0x480000
+#define	ZCP			0x500000
+#define	FZC_ZCP			0x580000
+#define	DMC			0x600000
+#define	FZC_DMC			0x680000
+#define	TXC			0x700000
+#define	FZC_TXC			0x780000
+#define	PIO_LDSV		0x800000
+#define	RESERVED_4		0x880000
+#define	PIO_LDGIM		0x900000
+#define	RESERVED_5		0x980000
+#define	PIO_IMASK0		0xa00000
+#define	RESERVED_6		0xa80000
+#define	PIO_IMASK1		0xb00000
+#define	RESERVED_7_START	0xb80000
+#define	RESERVED_7_END		0xc00000
+#define	FZC_PROM		0xc80000
+#define	RESERVED_8		0xd00000
+#define	FZC_PIM			0xd80000
+#define	RESERVED_9_START 	0xe00000
+#define	RESERVED_9_END 		0xf80000
+
+/* PIO		(0x000000) */
+
+
+/* FZC_PIO	(0x080000) */
+#define	LDGITMRES		(FZC_PIO + 0x00008)	/* timer resolution */
+#define	SID			(FZC_PIO + 0x10200)	/* 64 LDG, INT data */
+#define	LDG_NUM			(FZC_PIO + 0x20000)	/* 69 LDs */
+
+
+
+/* FZC_IPP 	(0x280000) */
+
+
+/* FFLP		(0x300000), Header Parser */
+
+/* PIO_VADDR	(0x400000), PIO Virtaul DMA Address */
+/* ?? how to access DMA via PIO_VADDR? */
+#define	VADDR			(PIO_VADDR + 0x00000) /* ?? not for driver */
+
+
+/* ZCP		(0x500000), Neptune Only */
+
+
+/* FZC_ZCP	(0x580000), Neptune Only */
+
+
+/* DMC 		(0x600000), register offset (32 DMA channels) */
+
+/* Transmit Ring Register Offset (32 Channels) */
+#define	TX_RNG_CFIG		(DMC + 0x40000)
+#define	TX_RING_HDH		(DMC + 0x40008)
+#define	TX_RING_HDL		(DMC + 0x40010)
+#define	TX_RING_KICK		(DMC + 0x40018)
+/* Transmit Operations (32 Channels) */
+#define	TX_ENT_MSK		(DMC + 0x40020)
+#define	TX_CS			(DMC + 0x40028)
+#define	TXDMA_MBH		(DMC + 0x40030)
+#define	TXDMA_MBL		(DMC + 0x40038)
+#define	TX_DMA_PRE_ST		(DMC + 0x40040)
+#define	TX_RNG_ERR_LOGH		(DMC + 0x40048)
+#define	TX_RNG_ERR_LOGL		(DMC + 0x40050)
+#ifdef OLD
+#define	SH_TX_RNG_ERR_LOGH	(DMC + 0x40058)
+#define	SH_TX_RNG_ERR_LOGL	(DMC + 0x40060)
+#endif
+
+/* FZC_DMC RED Initial Random Value register offset (global) */
+#define	RED_RAN_INIT		(FZC_DMC + 0x00068)
+
+#define	RX_ADDR_MD		(FZC_DMC + 0x00070)
+
+/* FZC_DMC Ethernet Timeout Countue register offset (global) */
+#define	EING_TIMEOUT		(FZC_DMC + 0x00078)
+
+/* RDC Table */
+#define	RDC_TBL			(FZC_DMC + 0x10000)	/* 256 * 8 */
+
+/* FZC_DMC partitioning support register offset (32 channels) */
+
+#define	TX_LOG_PAGE_VLD		(FZC_DMC + 0x40000)
+#define	TX_LOG_MASK1		(FZC_DMC + 0x40008)
+#define	TX_LOG_VAL1		(FZC_DMC + 0x40010)
+#define	TX_LOG_MASK2		(FZC_DMC + 0x40018)
+#define	TX_LOG_VAL2		(FZC_DMC + 0x40020)
+#define	TX_LOG_PAGE_RELO1	(FZC_DMC + 0x40028)
+#define	TX_LOG_PAGE_RELO2	(FZC_DMC + 0x40030)
+#define	TX_LOG_PAGE_HDL		(FZC_DMC + 0x40038)
+
+#define	TX_ADDR_MOD		(FZC_DMC + 0x41000) /* only one? */
+
+
+/* FZC_DMC RED Parameters register offset (32 channels) */
+#define	RDC_RED_PARA1		(FZC_DMC + 0x30000)
+#define	RDC_RED_PARA2		(FZC_DMC + 0x30008)
+/* FZC_DMC RED Discard Cound Register offset (32 channels) */
+#define	RED_DIS_CNT		(FZC_DMC + 0x30010)
+
+#ifdef OLD /* This has been moved to TXC */
+/* Transmit Ring Scheduler (per port) */
+#define	TX_DMA_MAP0		(FZC_DMC + 0x50000)
+#define	TX_DMA_MAP1		(FZC_DMC + 0x50008)
+#define	TX_DMA_MAP2		(FZC_DMC + 0x50010)
+#define	TX_DMA_MAP3		(FZC_DMC + 0x50018)
+#endif
+
+/* Transmit Ring Scheduler: DRR Weight (32 Channels) */
+#define	DRR_WT			(FZC_DMC + 0x51000)
+#ifdef OLD
+#define	TXRNG_USE		(FZC_DMC + 0x51008)
+#endif
+
+/* TXC		(0x700000)??	*/
+
+
+/* FZC_TXC	(0x780000)??	*/
+
+
+/*
+ * PIO_LDSV	(0x800000)
+ * Logical Device State Vector 0, 1, 2.
+ * (69 logical devices, 8192 apart, partitioning control)
+ */
+#define	LDSV0			(PIO_LDSV + 0x00000)	/* RO (64 - 69) */
+#define	LDSV1			(PIO_LDSV + 0x00008)	/* RO (32 - 63) */
+#define	LDSV2			(PIO_LDSV + 0x00010)	/* RO ( 0 - 31) */
+
+/*
+ * PIO_LDGIM	(0x900000)
+ * Logical Device Group Interrupt Management (64 groups).
+ * (count 64, step 8192)
+ */
+#define	LDGIMGN			(PIO_LDGIMGN + 0x00000)	/* RW */
+
+/*
+ * PIO_IMASK0	(0xA000000)
+ *
+ * Logical Device Masks 0, 1.
+ * (64 logical devices, 8192 apart, partitioning control)
+ */
+#define	LD_IM0			(PIO_IMASK0 + 0x00000)	/* RW ( 0 - 63) */
+
+/*
+ * PIO_IMASK0	(0xB000000)
+ *
+ * Logical Device Masks 0, 1.
+ * (5 logical devices, 8192 apart, partitioning control)
+ */
+#define	LD_IM1			(PIO_IMASK1 + 0x00000)	/* RW (64 - 69) */
+
+
+/* DMC/TMC CSR size */
+#define	DMA_CSR_SIZE		512
+#define	DMA_CSR_MIN_PAGE_SIZE	1024
+
+/*
+ * Define the Default RBR, RCR
+ */
+#define	RBR_DEFAULT_MAX_BLKS	4096	/* each entry (16 blockaddr/64B) */
+#define	RBR_NBLK_PER_LINE	16	/* 16 block addresses per 64 B line */
+#define	RBR_DEFAULT_MAX_LEN	(RBR_DEFAULT_MAX_BLKS)
+#define	RBR_DEFAULT_MIN_LEN	1
+
+#define	SW_OFFSET_NO_OFFSET		0
+#define	SW_OFFSET_64			1	/* 64 bytes */
+#define	SW_OFFSET_128			2	/* 128 bytes */
+#define	SW_OFFSET_INVALID		3
+
+/*
+ * RBR block descriptor is 32 bits (bits [43:12]
+ */
+#define	RBR_BKADDR_SHIFT	12
+
+
+#define	RCR_DEFAULT_MAX_BLKS	4096	/* each entry (8 blockaddr/64B) */
+#define	RCR_NBLK_PER_LINE	8	/* 8 block addresses per 64 B line */
+#define	RCR_DEFAULT_MAX_LEN	(RCR_DEFAULT_MAX_BLKS)
+#define	RCR_DEFAULT_MIN_LEN	1
+
+/*  DMA Channels.  */
+#define	NXGE_MAX_DMCS		(NXGE_MAX_RDCS + NXGE_MAX_TDCS)
+#define	NXGE_MAX_RDCS		16
+#define	NXGE_MAX_TDCS		24
+#define	NXGE_MAX_TDCS_NIU	16
+/*
+ * original mapping from Hypervisor
+ */
+#ifdef	ORIGINAL
+#define	NXGE_N2_RXDMA_START_LDG	0
+#define	NXGE_N2_TXDMA_START_LDG	16
+#define	NXGE_N2_MIF_LDG		32
+#define	NXGE_N2_MAC_0_LDG	33
+#define	NXGE_N2_MAC_1_LDG	34
+#define	NXGE_N2_SYS_ERROR_LDG	35
+#endif
+
+#define	NXGE_N2_RXDMA_START_LDG	19
+#define	NXGE_N2_TXDMA_START_LDG	27
+#define	NXGE_N2_MIF_LDG		17
+#define	NXGE_N2_MAC_0_LDG	16
+#define	NXGE_N2_MAC_1_LDG	35
+#define	NXGE_N2_SYS_ERROR_LDG	18
+#define	NXGE_N2_LDG_GAP		17
+
+#define	NXGE_MAX_RDC_GRPS	8
+
+/*
+ * Max. ports per Neptune and NIU
+ */
+#define	NXGE_MAX_PORTS			4
+#define	NXGE_PORTS_NEPTUNE		4
+#define	NXGE_PORTS_NIU			2
+
+/* Max. RDC table groups */
+#define	NXGE_MAX_RDC_GROUPS		8
+#define	NXGE_MAX_RDCS			16
+#define	NXGE_MAX_DMAS			32
+
+
+#define	NXGE_MAX_MACS_XMACS		16
+#define	NXGE_MAX_MACS_BMACS		8
+#define	NXGE_MAX_MACS			(NXGE_MAX_PORTS * NXGE_MAX_MACS_XMACS)
+
+#define	NXGE_MAX_VLANS			4096
+#define	VLAN_ETHERTYPE			(0x8100)
+
+
+/* Scaling factor for RBR (receive block ring) */
+#define	RBR_SCALE_1		0
+#define	RBR_SCALE_2		1
+#define	RBR_SCALE_3		2
+#define	RBR_SCALE_4		3
+#define	RBR_SCALE_5		4
+#define	RBR_SCALE_6		5
+#define	RBR_SCALE_7		6
+#define	RBR_SCALE_8		7
+
+
+#define	MAX_PORTS_PER_NXGE	4
+#define	MAX_MACS		32
+
+#define	TX_GATHER_POINTER_SZ	8
+#define	TX_GP_PER_BLOCK		8
+#define	TX_DEFAULT_MAX_GPS	1024	/* Max. # of gather pointers */
+#define	TX_DEFAULT_JUMBO_MAX_GPS 4096	/* Max. # of gather pointers */
+#define	TX_DEFAULT_MAX_LEN	(TX_DEFAULT_MAX_GPS/TX_GP_PER_BLOCK)
+#define	TX_DEFAULT_JUMBO_MAX_LEN (TX_DEFAULT_JUMBO_MAX_GPS/TX_GP_PER_BLOCK)
+
+#define	TX_RING_THRESHOLD		(TX_DEFAULT_MAX_GPS/4)
+#define	TX_RING_JUMBO_THRESHOLD		(TX_DEFAULT_JUMBO_MAX_GPS/4)
+
+#define	TRANSMIT_HEADER_SIZE		16	/* 16 B frame header */
+
+/* linux: use cpu_to_le64() before writing to the hardware register */
+#define	TX_DESC_SAD_SHIFT	0
+#define	TX_DESC_SAD_MASK	0x00000FFFFFFFFFFFULL	/* start address */
+#define	TX_DESC_TR_LEN_SHIFT	44
+#define	TX_DESC_TR_LEN_MASK	0x00FFF00000000000ULL	/* Transfer Length */
+#define	TX_DESC_NUM_PTR_SHIFT	58
+#define	TX_DESC_NUM_PTR_MASK	0x2C00000000000000ULL	/* gather pointers */
+#define	TX_DESC_MASK_SHIFT	62
+#define	TX_DESC_MASK_MASK	0x4000000000000000ULL	/* Mark bit */
+#define	TX_DESC_SOP_SHIF	63
+#define	TX_DESC_NUM_MASK	0x8000000000000000ULL	/* Start of packet */
+
+#define	TCAM_FLOW_KEY_MAX_CLASS		12
+#define	TCAM_L3_MAX_USER_CLASS		4
+#define	TCAM_NIU_TCAM_MAX_ENTRY		128
+#define	TCAM_NXGE_TCAM_MAX_ENTRY	256
+
+
+
+/* TCAM entry formats */
+#define	TCAM_IPV4_5TUPLE_FORMAT	0x00
+#define	TCAM_IPV6_5TUPLE_FORMAT	0x01
+#define	TCAM_ETHERTYPE_FORMAT	0x02
+
+
+/* TCAM */
+#define	TCAM_SELECT_IPV6	0x01
+#define	TCAM_LOOKUP		0x04
+#define	TCAM_DISCARD		0x08
+
+/* FLOW Key */
+#define	FLOW_L4_1_34_BYTES	0x10
+#define	FLOW_L4_1_78_BYTES	0x11
+#define	FLOW_L4_0_12_BYTES	(0x10 << 2)
+#define	FLOW_L4_0_56_BYTES	(0x11 << 2)
+#define	FLOW_PROTO_NEXT		0x10
+#define	FLOW_IPDA		0x20
+#define	FLOW_IPSA		0x40
+#define	FLOW_VLAN		0x80
+#define	FLOW_L2DA		0x100
+#define	FLOW_PORT		0x200
+
+/* TCAM */
+#define	MAX_EFRAME	11
+
+#define	TCAM_USE_L2RDC_FLOW_LOOKUP	0x00
+#define	TCAM_USE_OFFSET_DONE		0x01
+#define	TCAM_OVERRIDE_L2_FLOW_LOOKUP	0x02
+#define	TCAM_OVERRIDE_L2_USE_OFFSET	0x03
+
+/*
+ * FCRAM (Hashing):
+ *	1. IPv4 exact match
+ *	2. IPv6 exact match
+ *	3. IPv4 Optimistic match
+ *	4. IPv6 Optimistic match
+ *
+ */
+#define	FCRAM_IPV4_EXT_MATCH	0x00
+#define	FCRAM_IPV6_EXT_MATCH	0x01
+#define	FCRAM_IPV4_OPTI_MATCH	0x02
+#define	FCRAM_IPV6_OPTI_MATCH	0x03
+
+
+#define	NXGE_HASH_MAX_ENTRY	256
+
+
+#define	MAC_ADDR_LENGTH		6
+
+/* convert values */
+#define	NXGE_BASE(x, y)		(((y) << (x ## _SHIFT)) & (x ## _MASK))
+#define	NXGE_VAL(x, y)		(((y) & (x ## _MASK)) >> (x ## _SHIFT))
+
+/*
+ * Locate the DMA channel start offset (PIO_VADDR)
+ * (DMA virtual address space of the PIO block)
+ */
+#define	TDMC_PIOVADDR_OFFSET(channel)	(2 * DMA_CSR_SIZE * channel)
+#define	RDMC_PIOVADDR_OFFSET(channel)	(TDMC_OFFSET(channel) + DMA_CSR_SIZE)
+
+/*
+ * PIO access using the DMC block directly (DMC)
+ */
+#define	DMC_OFFSET(channel)	(DMA_CSR_SIZE * channel)
+#define	TDMC_OFFSET(channel)	(TX_RNG_CFIG + DMA_CSR_SIZE * channel)
+
+/*
+ * Number of logical pages.
+ */
+#define	NXGE_MAX_LOGICAL_PAGES		2
+
+#ifdef	SOLARIS
+#ifndef	i386
+#define	_BIT_FIELDS_BIG_ENDIAN		_BIT_FIELDS_HTOL
+#else
+#define	_BIT_FIELDS_LITTLE_ENDIAN	_BIT_FIELDS_LTOH
+#endif
+#else
+#define	_BIT_FIELDS_LITTLE_ENDIAN	_LITTLE_ENDIAN_BITFIELD
+#endif
+
+#ifdef COSIM
+#define	MAX_PIO_RETRIES		3200
+#else
+#define	MAX_PIO_RETRIES		32
+#endif
+
+#define	IS_PORT_NUM_VALID(portn)\
+	(portn < 4)
+
+/*
+ * The following macros expect unsigned input values.
+ */
+#define	TXDMA_CHANNEL_VALID(cn)		(cn < NXGE_MAX_TDCS)
+#define	TXDMA_PAGE_VALID(pn)		(pn < NXGE_MAX_LOGICAL_PAGES)
+#define	TXDMA_FUNC_VALID(fn)		(fn < MAX_PORTS_PER_NXGE)
+#define	FUNC_VALID(n)			(n < MAX_PORTS_PER_NXGE)
+
+/*
+ * DMA channel binding definitions.
+ */
+#define	VIR_PAGE_INDEX_MAX		8
+#define	VIR_SUB_REGIONS			2
+#define	VIR_DMA_BIND			1
+
+#define	SUBREGION_VALID(n)		(n < VIR_SUB_REGIONS)
+#define	VIR_PAGE_INDEX_VALID(n)		(n < VIR_PAGE_INDEX_MAX)
+#define	VRXDMA_CHANNEL_VALID(n)		(n < NXGE_MAX_RDCS)
+
+/*
+ * Logical device definitions.
+ */
+#define	NXGE_INT_MAX_LD		69
+#define	NXGE_INT_MAX_LDG	64
+
+#define	NXGE_RDMA_LD_START	 0
+#define	NXGE_TDMA_LD_START	32
+#define	NXGE_MIF_LD		63
+#define	NXGE_MAC_LD_PORT0	64
+#define	NXGE_MAC_LD_PORT1	65
+#define	NXGE_MAC_LD_PORT2	66
+#define	NXGE_MAC_LD_PORT3	67
+#define	NXGE_SYS_ERROR_LD	68
+
+#define	LDG_VALID(n)			(n < NXGE_INT_MAX_LDG)
+#define	LD_VALID(n)			(n < NXGE_INT_MAX_LD)
+#define	LD_RXDMA_LD_VALID(n)		(n < NXGE_MAX_RDCS)
+#define	LD_TXDMA_LD_VALID(n)		(n >= NXGE_MAX_RDCS && \
+					((n - NXGE_MAX_RDCS) < NXGE_MAX_TDCS)))
+#define	LD_MAC_VALID(n)			(IS_PORT_NUM_VALID(n))
+
+#define	LD_TIMER_MAX			0x3f
+#define	LD_INTTIMER_VALID(n)		(n <= LD_TIMER_MAX)
+
+/* System Interrupt Data */
+#define	SID_VECTOR_MAX			0x1f
+#define	SID_VECTOR_VALID(n)		(n <= SID_VECTOR_MAX)
+
+#define	NXGE_COMPILE_32
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif	/* _SYS_NXGE_NXGE_DEFS_H */
diff --git a/drivers/net/nxge/include/nxge_espc.h b/drivers/net/nxge/include/nxge_espc.h
new file mode 100644
index 0000000..948bdea
--- /dev/null
+++ b/drivers/net/nxge/include/nxge_espc.h
@@ -0,0 +1,247 @@
+/*
+ * nxge_espc.h	Neptune SPROM interface
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef	_SYS_NXGE_NXGE_ESPC_H
+#define	_SYS_NXGE_NXGE_ESPC_H
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#include <nxge_espc_hw.h>
+
+#define	ESPC_MAC_ADDR_0		ESPC_NCR_REGN(0)
+#define	ESPC_MAC_ADDR_1		ESPC_NCR_REGN(1)
+#define	ESPC_NUM_PORTS_MACS	ESPC_NCR_REGN(2)
+#define	ESPC_MOD_STR_LEN	ESPC_NCR_REGN(4)
+#define	ESPC_MOD_STR_1		ESPC_NCR_REGN(5)
+#define	ESPC_MOD_STR_2		ESPC_NCR_REGN(6)
+#define	ESPC_MOD_STR_3		ESPC_NCR_REGN(7)
+#define	ESPC_MOD_STR_4		ESPC_NCR_REGN(8)
+#define	ESPC_MOD_STR_5		ESPC_NCR_REGN(9)
+#define	ESPC_MOD_STR_6		ESPC_NCR_REGN(10)
+#define	ESPC_MOD_STR_7		ESPC_NCR_REGN(11)
+#define	ESPC_MOD_STR_8		ESPC_NCR_REGN(12)
+#define	ESPC_BD_MOD_STR_LEN	ESPC_NCR_REGN(13)
+#define	ESPC_BD_MOD_STR_1	ESPC_NCR_REGN(14)
+#define	ESPC_BD_MOD_STR_2	ESPC_NCR_REGN(15)
+#define	ESPC_BD_MOD_STR_3	ESPC_NCR_REGN(16)
+#define	ESPC_BD_MOD_STR_4	ESPC_NCR_REGN(17)
+#define	ESPC_PHY_TYPE		ESPC_NCR_REGN(18)
+#define	ESPC_MAX_FM_SZ		ESPC_NCR_REGN(19)
+#define	ESPC_INTR_NUM		ESPC_NCR_REGN(20)
+#define	ESPC_VER_IMGSZ		ESPC_NCR_REGN(21)
+#define	ESPC_CHKSUM		ESPC_NCR_REGN(22)
+
+#define	NUM_PORTS_MASK		0xff
+#define	NUM_MAC_ADDRS_MASK	0xff0000
+#define	NUM_MAC_ADDRS_SHIFT	16
+#define	MOD_STR_LEN_MASK	0xffff
+#define	BD_MOD_STR_LEN_MASK	0xffff
+#define	MAX_FM_SZ_MASK		0xffff
+#define	VER_NUM_MASK		0xffff
+#define	IMG_SZ_MASK		0xffff0000
+#define	IMG_SZ_SHIFT		16
+#define	CHKSUM_MASK		0xff
+
+/* 0 <= n < 8 */
+#define	ESPC_MOD_STR(n)		(ESPC_MOD_STR_1 + n*8)
+#define	MAX_MOD_STR_LEN		32
+
+/* 0 <= n < 4 */
+#define	ESPC_BD_MOD_STR(n)	(ESPC_BD_MOD_STR_1 + n*8)
+#define	MAX_BD_MOD_STR_LEN	16
+
+#define	ESC_PHY_10G_FIBER	0x0
+#define	ESC_PHY_10G_COPPER	0x1
+#define	ESC_PHY_1G_FIBER	0x2
+#define	ESC_PHY_1G_COPPER	0x3
+#define	ESC_PHY_NONE		0xf
+
+#define	ESC_IMG_CHKSUM_VAL	0xab
+
+typedef union _mac_addr_0_t {
+	uint64_t value;
+
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t msw;	/* Most significant word */
+		uint32_t lsw;	/* Least significant word */
+#elif defined(_LITTLE_ENDIAN)
+		uint32_t lsw;	/* Least significant word */
+		uint32_t msw;	/* Most significant word */
+#endif
+	} val;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t	w1;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t byte3		: 8;
+		uint32_t byte2		: 8;
+		uint32_t byte1		: 8;
+		uint32_t byte0		: 8;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t byte0		: 8;
+		uint32_t byte1		: 8;
+		uint32_t byte2		: 8;
+		uint32_t byte3		: 8;
+#endif
+		} w0;
+
+#if defined(_LITTLE_ENDIAN)
+		uint32_t	w1;
+#endif
+	} bits;
+} mac_addr_0_t;
+
+typedef union _mac_addr_1_t {
+	uint64_t value;
+
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t msw;	/* Most significant word */
+		uint32_t lsw;	/* Least significant word */
+#elif defined(_LITTLE_ENDIAN)
+		uint32_t lsw;	/* Least significant word */
+		uint32_t msw;	/* Most significant word */
+#endif
+	} val;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t	w1;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t res		: 16;
+		uint32_t byte5		: 8;
+		uint32_t byte4		: 8;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t byte4		: 8;
+		uint32_t byte5		: 8;
+		uint32_t res		: 16;
+#endif
+		} w0;
+
+#if defined(_LITTLE_ENDIAN)
+		uint32_t	w1;
+#endif
+	} bits;
+} mac_addr_1_t;
+
+
+typedef union _phy_type_t {
+	uint64_t value;
+
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t msw;	/* Most significant word */
+		uint32_t lsw;	/* Least significant word */
+#elif defined(_LITTLE_ENDIAN)
+		uint32_t lsw;	/* Least significant word */
+		uint32_t msw;	/* Most significant word */
+#endif
+	} val;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t	w1;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t pt0_phy_type	: 8;
+		uint32_t pt1_phy_type	: 8;
+		uint32_t pt2_phy_type	: 8;
+		uint32_t pt3_phy_type	: 8;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t pt3_phy_type	: 8;
+		uint32_t pt2_phy_type	: 8;
+		uint32_t pt1_phy_type	: 8;
+		uint32_t pt0_phy_type	: 8;
+#endif
+		} w0;
+
+#if defined(_LITTLE_ENDIAN)
+		uint32_t	w1;
+#endif
+	} bits;
+} phy_type_t;
+
+
+typedef union _intr_num_t {
+	uint64_t value;
+
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t msw;	/* Most significant word */
+		uint32_t lsw;	/* Least significant word */
+#elif defined(_LITTLE_ENDIAN)
+		uint32_t lsw;	/* Least significant word */
+		uint32_t msw;	/* Most significant word */
+#endif
+	} val;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t	w1;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t pt0_intr_num	: 8;
+		uint32_t pt1_intr_num	: 8;
+		uint32_t pt2_intr_num	: 8;
+		uint32_t pt3_intr_num	: 8;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t pt3_intr_num	: 8;
+		uint32_t pt2_intr_num	: 8;
+		uint32_t pt1_intr_num	: 8;
+		uint32_t pt0_intr_num	: 8;
+#endif
+		} w0;
+
+#if defined(_LITTLE_ENDIAN)
+		uint32_t	w1;
+#endif
+	} bits;
+} intr_num_t;
+
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif	/* _SYS_NXGE_NXGE_ESPC_H */
diff --git a/drivers/net/nxge/include/nxge_espc_hw.h b/drivers/net/nxge/include/nxge_espc_hw.h
new file mode 100644
index 0000000..261f514
--- /dev/null
+++ b/drivers/net/nxge/include/nxge_espc_hw.h
@@ -0,0 +1,75 @@
+/*
+ * nxge_espc_hw.h	Neptune SPROM HW offsets
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef	_SYS_NXGE_NXGE_ESPC_HW_H
+#define	_SYS_NXGE_NXGE_ESPC_HW_H
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#include <nxge_defs.h>
+
+/* EPC / SPC Registers offsets */
+#define	ESPC_PIO_EN_REG		0x040000
+#define	ESPC_PIO_EN_MASK	0x0000000000000001ULL
+#define	ESPC_PIO_STATUS_REG	0x040008
+
+/* EPC Status Register */
+#define	EPC_READ_INITIATE	(1ULL << 31)
+#define	EPC_READ_COMPLETE	(1 << 30)
+#define	EPC_WRITE_INITIATE	(1 << 29)
+#define	EPC_WRITE_COMPLETE	(1 << 28)
+#define	EPC_EEPROM_ADDR_BITS	0x3FFFF
+#define	EPC_EEPROM_ADDR_SHIFT	8
+#define	EPC_EEPROM_ADDR_MASK	(EPC_EEPROM_ADDR_BITS << EPC_EEPROM_ADDR_SHIFT)
+#define	EPC_EEPROM_DATA_MASK	0xFF
+
+#define	EPC_RW_WAIT		10	/* TBD */
+
+#define	ESPC_NCR_REG		0x040020   /* Count 128, step 8 */
+#define	ESPC_REG_ADDR(reg)	(FZC_PROM + (reg))
+
+#define	ESPC_NCR_REGN(n)	((ESPC_REG_ADDR(ESPC_NCR_REG)) + n*8)
+#define	ESPC_NCR_VAL_MASK	0x00000000FFFFFFFFULL
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif	/* _SYS_NXGE_NXGE_ESPC_HW_H */
diff --git a/drivers/net/nxge/include/nxge_fflp.h b/drivers/net/nxge/include/nxge_fflp.h
new file mode 100644
index 0000000..a414f02
--- /dev/null
+++ b/drivers/net/nxge/include/nxge_fflp.h
@@ -0,0 +1,244 @@
+/*
+ * nxge_fflp.h	Neptune FFLP Classifier interface
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef	_SYS_NXGE_NXGE_FFLP_H
+#define	_SYS_NXGE_NXGE_FFLP_H
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#include <npi_fflp.h>
+
+#define	MAX_PARTITION 8
+
+typedef	struct _fflp_errlog {
+	uint32_t		vlan;
+	uint32_t		tcam;
+	uint32_t		hash_pio[MAX_PARTITION];
+	uint32_t		hash_lookup1;
+	uint32_t		hash_lookup2;
+} fflp_errlog_t, *p_fflp_errlog_t;
+
+typedef struct _fflp_stats {
+	uint32_t 		tcam_entries;
+	uint32_t 		fcram_entries;
+	uint32_t 		tcam_parity_err;
+	uint32_t 		tcam_ecc_err;
+	uint32_t 		vlan_parity_err;
+	uint32_t 		hash_lookup_err;
+	uint32_t 		hash_pio_err[MAX_PARTITION];
+	fflp_errlog_t		errlog;
+} nxge_fflp_stats_t, *p_nxge_fflp_stats_t;
+
+/*
+ * The FCRAM (hash table) cosnists of 1 meg cells
+ * each 64 byte wide. Each cell can hold either of:
+ * 2 IPV4 Exact match entry (each 32 bytes)
+ * 1 IPV6 Exact match entry (each 56 bytes) and
+ *    1 Optimistic match entry (each 8 bytes)
+ * 8 Optimistic match entries (each 8 bytes)
+ * In the case IPV4 Exact match, half of the cell
+ * (the first or the second 32 bytes) could be used
+ * to hold 4 Optimistic matches
+ */
+
+#define	FCRAM_CELL_EMPTY	0x00
+#define	FCRAM_CELL_IPV4_IPV4	0x01
+#define	FCRAM_CELL_IPV4_OPT	0x02
+#define	FCRAM_CELL_OPT_IPV4	0x04
+#define	FCRAM_CELL_IPV6_OPT	0x08
+#define	FCRAM_CELL_OPT_OPT	0x10
+
+
+#define	FCRAM_SUBAREA0_OCCUPIED	0x01
+#define	FCRAM_SUBAREA1_OCCUPIED	0x02
+#define	FCRAM_SUBAREA2_OCCUPIED	0x04
+#define	FCRAM_SUBAREA3_OCCUPIED	0x08
+
+#define	FCRAM_SUBAREA4_OCCUPIED	0x10
+#define	FCRAM_SUBAREA5_OCCUPIED	0x20
+#define	FCRAM_SUBAREA6_OCCUPIED	0x40
+#define	FCRAM_SUBAREA7_OCCUPIED	0x20
+
+#define	FCRAM_IPV4_SUBAREA0_OCCUPIED \
+	(FCRAM_SUBAREA0_OCCUPIED | FCRAM_SUBAREA1_OCCUPIED | \
+	FCRAM_SUBAREA2_OCCUPIED | FCRAM_SUBAREA3_OCCUPIED)
+
+#define	FCRAM_IPV4_SUBAREA4_OCCUPIED \
+	(FCRAM_SUBAREA4_OCCUPIED | FCRAM_SUBAREA5_OCCUPIED | \
+	FCRAM_SUBAREA6_OCCUPIED | FCRAM_SUBAREA7_OCCUPIED)
+
+
+#define	FCRAM_IPV6_SUBAREA0_OCCUPIED \
+	(FCRAM_SUBAREA0_OCCUPIED | FCRAM_SUBAREA1_OCCUPIED | \
+	FCRAM_SUBAREA2_OCCUPIED | FCRAM_SUBAREA3_OCCUPIED | \
+	FCRAM_SUBAREA4_OCCUPIED | FCRAM_SUBAREA5_OCCUPIED | \
+	FCRAM_SUBAREA6_OCCUPIED)
+
+	/*
+	 * The current occupancy state of each FCRAM cell isy
+	 * described by the fcram_cell_t data structure.
+	 * The "type" field denotes the type of entry (or combination)
+	 * the cell holds (FCRAM_CELL_EMPTY ...... FCRAM_CELL_OPT_OPT)
+	 * The "occupied" field indicates if individual 8 bytes (subareas)
+	 * with in the cell are occupied
+	 */
+
+typedef struct _fcram_cell {
+	uint32_t 		type:8;
+	uint32_t 		occupied:8;
+	uint32_t 		shadow_loc:16;
+} fcram_cell_t, *p_fcram_cell_t;
+
+typedef struct _fcram_parition {
+	uint8_t 		id;
+	uint8_t 		base;
+	uint8_t 		mask;
+	uint8_t 		reloc;
+	uint32_t 		flags;
+#define	HASH_PARTITION_ENABLED 1
+	uint32_t 		offset;
+	uint32_t 		size;
+} fcram_parition_t, *p_fcram_partition_t;
+
+
+typedef struct _tcam_flow_spec {
+	tcam_entry_t tce;
+	uint64_t flags;
+	uint64_t user_info;
+} tcam_flow_spec_t, *p_tcam_flow_spec_t;
+
+
+/*
+ * Used for configuration.
+ * ndd as well nxge.conf use the following definitions
+ */
+
+#define	NXGE_CLASS_CONFIG_PARAMS	20
+/* Used for ip class flow key and tcam key config */
+
+#define	NXGE_CLASS_TCAM_LOOKUP		0x0001
+#define	NXGE_CLASS_TCAM_USE_SRC_ADDR	0x0002
+#define	NXGE_CLASS_FLOW_USE_PORTNUM	0x0010
+#define	NXGE_CLASS_FLOW_USE_L2DA	0x0020
+#define	NXGE_CLASS_FLOW_USE_VLAN	0x0040
+#define	NXGE_CLASS_FLOW_USE_PROTO	0x0080
+#define	NXGE_CLASS_FLOW_USE_IPSRC	0x0100
+#define	NXGE_CLASS_FLOW_USE_IPDST	0x0200
+#define	NXGE_CLASS_FLOW_USE_SRC_PORT	0x0400
+#define	NXGE_CLASS_FLOW_USE_DST_PORT	0x0800
+#define	NXGE_CLASS_DISCARD		0x80000000
+
+/* these are used for quick configs */
+#define	NXGE_CLASS_FLOW_WEB_SERVER	NXGE_CLASS_FLOW_USE_IPSRC | \
+					NXGE_CLASS_FLOW_USE_SRC_PORT
+
+#define	NXGE_CLASS_FLOW_GEN_SERVER	NXGE_CLASS_FLOW_USE_IPSRC | \
+					NXGE_CLASS_FLOW_USE_IPDST | \
+					NXGE_CLASS_FLOW_USE_SRC_PORT |	\
+					NXGE_CLASS_FLOW_USE_DST_PORT | \
+					NXGE_CLASS_FLOW_USE_PROTO | \
+					NXGE_CLASS_FLOW_USE_L2DA | \
+					NXGE_CLASS_FLOW_USE_VLAN
+
+/*
+ * used for use classes
+ */
+
+
+/* Ethernet Classes */
+#define	NXGE_CLASS_CFG_ETHER_TYPE_MASK		0x0000FFFF
+#define	NXGE_CLASS_CFG_ETHER_ENABLE_MASK	0x40000000
+
+/* IP Classes */
+#define	NXGE_CLASS_CFG_IP_TOS_MASK		0x000000FF
+#define	NXGE_CLASS_CFG_IP_TOS_SHIFT		0
+#define	NXGE_CLASS_CFG_IP_TOS_MASK_MASK		0x0000FF00
+#define	NXGE_CLASS_CFG_IP_TOS_MASK_SHIFT	8
+#define	NXGE_CLASS_CFG_IP_PROTO_MASK		0x00FFFF00
+#define	NXGE_CLASS_CFG_IP_PROTO_SHIFT		16
+
+#define	NXGE_CLASS_CFG_IP_IPV6_MASK		0x01000000
+#define	NXGE_CLASS_CFG_IP_PARAM_MASK	NXGE_CLASS_CFG_IP_TOS_MASK | \
+					NXGE_CLASS_CFG_IP_TOS_MASK_MASK | \
+					NXGE_CLASS_CFG_IP_PROTO_MASK | \
+					NXGE_CLASS_CFG_IP_IPV6_MASK
+
+#define	NXGE_CLASS_CFG_IP_ENABLE_MASK		0x40000000
+
+typedef struct _vlan_rdcgrp_map {
+	uint32_t		rsrvd:8;
+	uint32_t		vid:16;
+	uint32_t		rdc_grp:8;
+}	vlan_rdcgrp_map_t, *p_vlan_rdcgrp_map_t;
+
+#define	NXGE_INIT_VLAN_RDCG_TBL	32
+
+typedef struct _nxge_classify {
+	nxge_os_mutex_t 	tcam_lock;
+	nxge_os_mutex_t		fcram_lock;
+	nxge_os_mutex_t		hash_lock[MAX_PARTITION];
+	uint32_t 		tcam_size;
+	uint32_t 		state;
+#define	NXGE_FFLP_HW_RESET	0x1
+#define	NXGE_FFLP_HW_INIT	0x2
+#define	NXGE_FFLP_SW_INIT	0x4
+#define	NXGE_FFLP_FCRAM_PART	0x80000000
+	p_nxge_fflp_stats_t	fflp_stats;
+
+	tcam_flow_spec_t    *tcam_entries;
+	uint8_t		    tcam_location;
+#define	NXGE_FLOW_NO_SUPPORT  0x0
+#define	NXGE_FLOW_USE_TCAM    0x1
+#define	NXGE_FLOW_USE_FCRAM   0x2
+#define	NXGE_FLOW_USE_TCAM_FCRAM   0x3
+
+#define	NXGE_FLOW_COMPUTE_H1   0x10
+#define	NXGE_FLOW_COMPUTE_H2   0x20
+	uint8_t	fragment_bug;
+	uint8_t	fragment_bug_location;
+	fcram_cell_t		*hash_table; /* allocated for Neptune only */
+	fcram_parition_t    partition[MAX_PARTITION];
+} nxge_classify_t, *p_nxge_classify_t;
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif	/* _SYS_NXGE_NXGE_FFLP_H */
diff --git a/drivers/net/nxge/include/nxge_fflp_hash.h b/drivers/net/nxge/include/nxge_fflp_hash.h
new file mode 100644
index 0000000..a6ae15e
--- /dev/null
+++ b/drivers/net/nxge/include/nxge_fflp_hash.h
@@ -0,0 +1,72 @@
+/*
+ * nxge_fflp_hash.h	Neptune FFLP Classifier Hash interfaces
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef _SYS_NXGE_NXGE_CRC_H
+#define	_SYS_NXGE_NXGE_CRC_H
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+void nxge_crc32c_init(void);
+uint32_t nxge_crc32c(uint32_t, const uint8_t *, int);
+
+void nxge_crc_ccitt_init(void);
+uint16_t nxge_crc_ccitt(uint16_t, uint8_t *, int);
+
+uint32_t nxge_compute_h1_table1(uint32_t crcin,
+				uint32_t *flow, uint32_t length);
+uint32_t nxge_compute_h1_table4(uint32_t crcin,
+				uint32_t *flow, uint32_t length);
+uint32_t nxge_compute_h1_serial(uint32_t crcin,
+				uint32_t *flow, uint32_t length);
+
+#define	nxge_compute_h2(cin, flow, len)			\
+	nxge_crc_ccitt(cin, flow, len)
+
+void nxge_init_h1_table(void);
+
+#define	nxge_compute_h1(cin, flow, len)			\
+	nxge_compute_h1_table4(cin, flow, len)
+
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* _SYS_NXGE_NXGE_CRC_H */
diff --git a/drivers/net/nxge/include/nxge_fflp_hw.h b/drivers/net/nxge/include/nxge_fflp_hw.h
new file mode 100644
index 0000000..a135637
--- /dev/null
+++ b/drivers/net/nxge/include/nxge_fflp_hw.h
@@ -0,0 +1,1680 @@
+/*
+ * nxge_fflp_hw.h      Neptune FFLP Classifier HW offsets
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef	_SYS_NXGE_NXGE_FFLP_HW_H
+#define	_SYS_NXGE_NXGE_FFLP_HW_H
+
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#include <nxge_defs.h>
+
+
+/* FZC_FFLP Offsets */
+#define	    FFLP_ENET_VLAN_TBL_REG	(FZC_FFLP + 0x00000)
+
+	/* defines for FFLP_ENET_VLAN_TBL */
+
+#define	ENET_VLAN_TBL_VLANRDCTBLN0_MASK 	0x0000000000000003ULL
+#define	ENET_VLAN_TBL_VLANRDCTBLN0_SHIFT 	0
+#define	ENET_VLAN_TBL_VPR0_MASK			0x00000000000000008ULL
+#define	ENET_VLAN_TBL_VPR0_SHIFT		3
+
+#define	ENET_VLAN_TBL_VLANRDCTBLN1_MASK 	0x0000000000000030ULL
+#define	ENET_VLAN_TBL_VLANRDCTBLN1_SHIFT	4
+#define	ENET_VLAN_TBL_VPR1_MASK			0x00000000000000080ULL
+#define	ENET_VLAN_TBL_VPR1_SHIFT		7
+
+#define	ENET_VLAN_TBL_VLANRDCTBLN2_MASK 	0x0000000000000300ULL
+#define	ENET_VLAN_TBL_VLANRDCTBLN2_SHIFT 	8
+#define	ENET_VLAN_TBL_VPR2_MASK			0x00000000000000800ULL
+#define	ENET_VLAN_TBL_VPR2_SHIFT		11
+
+#define	ENET_VLAN_TBL_VLANRDCTBLN3_MASK 	0x0000000000003000ULL
+#define	ENET_VLAN_TBL_VLANRDCTBLN3_SHIFT 	12
+#define	ENET_VLAN_TBL_VPR3_MASK			0x0000000000008000ULL
+#define	ENET_VLAN_TBL_VPR3_SHIFT		15
+
+#define	ENET_VLAN_TBL_PARITY0_MASK		0x0000000000010000ULL
+#define	ENET_VLAN_TBL_PARITY0_SHIFT		16
+#define	ENET_VLAN_TBL_PARITY1_MASK		0x0000000000020000ULL
+#define	ENET_VLAN_TBL_PARITY1_SHIFT		17
+
+
+
+typedef union _fflp_enet_vlan_tbl_t {
+    uint64_t value;
+    struct {
+#if	defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#ifdef _BIT_FIELDS_HTOL
+			uint32_t rsrvd:14;
+			uint32_t parity1:1;
+			uint32_t parity0:1;
+			uint32_t vpr3:1;
+			uint32_t vlanrdctbln3:3;
+			uint32_t vpr2:1;
+			uint32_t vlanrdctbln2:3;
+			uint32_t vpr1:1;
+			uint32_t vlanrdctbln1:3;
+			uint32_t vpr0:1;
+			uint32_t vlanrdctbln0:3;
+#else
+			uint32_t vlanrdctbln0:3;
+			uint32_t vpr0:1;
+			uint32_t vlanrdctbln1:3;
+			uint32_t vpr1:1;
+			uint32_t vlanrdctbln2:3;
+			uint32_t vpr2:1;
+			uint32_t vlanrdctbln3:3;
+			uint32_t vpr3:1;
+			uint32_t parity0:1;
+			uint32_t parity1:1;
+			uint32_t rsrvd:14;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} fflp_enet_vlan_tbl_t, *p_fflp_enet_vlan_tbl_t;
+
+
+#define	FFLP_TCAM_CLS_BASE_OFFSET (FZC_FFLP + 0x20000)
+#define	FFLP_L2_CLS_ENET1_REG	  (FZC_FFLP + 0x20000)
+#define	FFLP_L2_CLS_ENET2_REG	  (FZC_FFLP + 0x20008)
+
+
+
+typedef union _tcam_class_prg_ether_t {
+#define	TCAM_ENET_USR_CLASS_ENABLE   0x1
+#define	TCAM_ENET_USR_CLASS_DISABLE  0x0
+
+    uint64_t value;
+    struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#ifdef _BIT_FIELDS_HTOL
+			uint32_t rsrvd:15;
+			uint32_t valid:1;
+			uint32_t etype:16;
+#else
+			uint32_t etype:16;
+			uint32_t valid:1;
+			uint32_t rsrvd:15;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} tcam_class_prg_ether_t, *p_tcam_class_prg_ether_t;
+
+
+#define		FFLP_L3_CLS_IP_U4_REG	(FZC_FFLP + 0x20010)
+#define		FFLP_L3_CLS_IP_U5_REG	(FZC_FFLP + 0x20018)
+#define		FFLP_L3_CLS_IP_U6_REG	(FZC_FFLP + 0x20020)
+#define		FFLP_L3_CLS_IP_U7_REG	(FZC_FFLP + 0x20028)
+
+typedef union _tcam_class_prg_ip_t {
+#define	TCAM_IP_USR_CLASS_ENABLE   0x1
+#define	TCAM_IP_USR_CLASS_DISABLE  0x0
+
+    uint64_t value;
+    struct {
+#if	defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#ifdef _BIT_FIELDS_HTOL
+			uint32_t rsrvd:6;
+			uint32_t valid:1;
+			uint32_t ipver:1;
+			uint32_t pid:8;
+			uint32_t tosmask:8;
+			uint32_t tos:8;
+#else
+			uint32_t tos:8;
+			uint32_t tosmask:8;
+			uint32_t pid:8;
+			uint32_t ipver:1;
+			uint32_t valid:1;
+			uint32_t rsrvd:6;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} tcam_class_prg_ip_t, *p_tcam_class_prg_ip_t;
+/* define the classes which use the above structure */
+
+typedef enum fflp_tcam_class {
+    TCAM_CLASS_INVALID = 0,
+    TCAM_CLASS_DUMMY = 1,
+    TCAM_CLASS_ETYPE_1 = 2,
+    TCAM_CLASS_ETYPE_2,
+    TCAM_CLASS_IP_USER_4,
+    TCAM_CLASS_IP_USER_5,
+    TCAM_CLASS_IP_USER_6,
+    TCAM_CLASS_IP_USER_7,
+    TCAM_CLASS_TCP_IPV4,
+    TCAM_CLASS_UDP_IPV4,
+    TCAM_CLASS_AH_ESP_IPV4,
+    TCAM_CLASS_SCTP_IPV4,
+    TCAM_CLASS_TCP_IPV6,
+    TCAM_CLASS_UDP_IPV6,
+    TCAM_CLASS_AH_ESP_IPV6,
+    TCAM_CLASS_SCTP_IPV6,
+    TCAM_CLASS_ARP,
+    TCAM_CLASS_RARP,
+    TCAM_CLASS_DUMMY_12,
+    TCAM_CLASS_DUMMY_13,
+    TCAM_CLASS_DUMMY_14,
+    TCAM_CLASS_DUMMY_15,
+    TCAM_CLASS_MAX
+} tcam_class_t;
+
+
+
+/*
+ * Specify how to build TCAM key for L3
+ * IP Classes. Both User configured and
+ * hardwired IP services are included.
+ * These are the supported 12 classes.
+ */
+
+#define		FFLP_TCAM_KEY_BASE_OFFSET	(FZC_FFLP + 0x20030)
+#define		FFLP_TCAM_KEY_IP_USR4_REG		(FZC_FFLP + 0x20030)
+#define		FFLP_TCAM_KEY_IP_USR5_REG		(FZC_FFLP + 0x20038)
+#define		FFLP_TCAM_KEY_IP_USR6_REG		(FZC_FFLP + 0x20040)
+#define		FFLP_TCAM_KEY_IP_USR7_REG		(FZC_FFLP + 0x20048)
+#define		FFLP_TCAM_KEY_IP4_TCP_REG		(FZC_FFLP + 0x20050)
+#define		FFLP_TCAM_KEY_IP4_UDP_REG		(FZC_FFLP + 0x20058)
+#define		FFLP_TCAM_KEY_IP4_AH_ESP_REG	(FZC_FFLP + 0x20060)
+#define		FFLP_TCAM_KEY_IP4_SCTP_REG		(FZC_FFLP + 0x20068)
+#define		FFLP_TCAM_KEY_IP6_TCP_REG		(FZC_FFLP + 0x20070)
+#define		FFLP_TCAM_KEY_IP6_UDP_REG		(FZC_FFLP + 0x20078)
+#define		FFLP_TCAM_KEY_IP6_AH_ESP_REG	(FZC_FFLP + 0x20080)
+#define		FFLP_TCAM_KEY_IP6_SCTP_REG		(FZC_FFLP + 0x20088)
+
+
+typedef union _tcam_class_key_ip_t {
+    uint64_t value;
+    struct {
+#if	defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#ifdef _BIT_FIELDS_HTOL
+			uint32_t rsrvd2:28;
+			uint32_t discard:1;
+			uint32_t tsel:1;
+			uint32_t rsrvd:1;
+			uint32_t ipaddr:1;
+#else
+			uint32_t ipaddr:1;
+			uint32_t rsrvd:1;
+			uint32_t tsel:1;
+			uint32_t discard:1;
+			uint32_t rsrvd2:28;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} tcam_class_key_ip_t, *p_tcam_class_key_ip_t;
+
+
+
+#define	FFLP_TCAM_KEY_0_REG			(FZC_FFLP + 0x20090)
+#define	FFLP_TCAM_KEY_1_REG		(FZC_FFLP + 0x20098)
+#define	FFLP_TCAM_KEY_2_REG		(FZC_FFLP + 0x200A0)
+#define	FFLP_TCAM_KEY_3_REG	(FZC_FFLP + 0x200A8)
+#define	FFLP_TCAM_MASK_0_REG	(FZC_FFLP + 0x200B0)
+#define	FFLP_TCAM_MASK_1_REG	(FZC_FFLP + 0x200B8)
+#define	FFLP_TCAM_MASK_2_REG	(FZC_FFLP + 0x200C0)
+#define	FFLP_TCAM_MASK_3_REG	(FZC_FFLP + 0x200C8)
+
+#define		FFLP_TCAM_CTL_REG		(FZC_FFLP + 0x200D0)
+
+/* bit defines for FFLP_TCAM_CTL register */
+#define	   TCAM_CTL_TCAM_WR		  0x0ULL
+#define	   TCAM_CTL_TCAM_RD		  0x040000ULL
+#define	   TCAM_CTL_TCAM_CMP		  0x080000ULL
+#define	   TCAM_CTL_RAM_WR		  0x100000ULL
+#define	   TCAM_CTL_RAM_RD		  0x140000ULL
+#define	   TCAM_CTL_RWC_STAT		  0x0020000ULL
+#define	   TCAM_CTL_RWC_MATCH		  0x0010000ULL
+
+
+typedef union _tcam_ctl_t {
+#define	TCAM_CTL_RWC_TCAM_WR	0x0
+#define	TCAM_CTL_RWC_TCAM_RD	0x1
+#define	TCAM_CTL_RWC_TCAM_CMP	0x2
+#define	TCAM_CTL_RWC_RAM_WR	0x4
+#define	TCAM_CTL_RWC_RAM_RD	0x5
+#define	TCAM_CTL_RWC_RWC_STAT	0x1
+#define	TCAM_CTL_RWC_RWC_MATCH	0x1
+
+	uint64_t value;
+	struct {
+#if	defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#ifdef _BIT_FIELDS_HTOL
+			uint32_t rsrvd2:11;
+			uint32_t rwc:3;
+			uint32_t stat:1;
+			uint32_t match:1;
+			uint32_t rsrvd:6;
+			uint32_t location:10;
+#else
+			uint32_t location:10;
+			uint32_t rsrvd:6;
+			uint32_t match:1;
+			uint32_t stat:1;
+			uint32_t rwc:3;
+			uint32_t rsrvd2:11;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} tcam_ctl_t, *p_tcam_ctl_t;
+
+
+
+/* Bit defines for TCAM ASC RAM */
+
+
+typedef union _tcam_res_t {
+	uint64_t value;
+	struct {
+#if	defined(_BIG_ENDIAN)
+		struct {
+			uint32_t rsrvd:22;
+			uint32_t syndrome:10;
+		} hdw;
+#endif
+		struct {
+#ifdef _BIT_FIELDS_HTOL
+			uint32_t syndrome:6;
+			uint32_t zfid:12;
+			uint32_t v4_ecc_ck:1;
+			uint32_t disc:1;
+			uint32_t tres:2;
+			uint32_t rdctbl:3;
+			uint32_t offset:5;
+			uint32_t zfld:1;
+			uint32_t age:1;
+#else
+			uint32_t age:1;
+			uint32_t zfld:1;
+			uint32_t offset:5;
+			uint32_t rdctbl:3;
+			uint32_t tres:2;
+			uint32_t disc:1;
+			uint32_t v4_ecc_ck:1;
+			uint32_t zfid:12;
+			uint32_t syndrome:6;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		struct {
+			uint32_t syndrome:10;
+			uint32_t rsrvd:22;
+		} hdw;
+#endif
+	} bits;
+} tcam_res_t, *p_tcam_res_t;
+
+
+
+#define	TCAM_ASC_DATA_AGE		0x0000000000000001ULL
+#define	TCAM_ASC_DATA_AGE_SHIFT		0x0
+#define	TCAM_ASC_DATA_ZFVLD		0x0000000000000002ULL
+#define	TCAM_ASC_DATA_ZFVLD_SHIFT	1
+
+#define	TCAM_ASC_DATA_OFFSET_MASK	0x000000000000007CULL
+#define	TCAM_ASC_DATA_OFFSET_SHIFT	2
+
+#define	TCAM_ASC_DATA_RDCTBL_MASK	0x0000000000000038ULL
+#define	TCAM_ASC_DATA_RDCTBL_SHIFT	7
+#define	TCAM_ASC_DATA_TRES_MASK		0x0000000000000C00ULL
+#define	TRES_CONT_USE_L2RDC		0x00
+#define	TRES_TERM_USE_OFFSET		0x01
+#define	TRES_CONT_OVRD_L2RDC		0x02
+#define	TRES_TERM_OVRD_L2RDC		0x03
+
+#define	TCAM_ASC_DATA_TRES_SHIFT	10
+#define	TCAM_TRES_CONT_USE_L2RDC	\
+		(0x0000000000000000ULL << TCAM_ASC_DATA_TRES_SHIFT)
+#define	TCAM_TRES_TERM_USE_OFFSET	\
+		(0x0000000000000001ULL << TCAM_ASC_DATA_TRES_SHIFT)
+#define	TCAM_TRES_CONT_OVRD_L2RDC	\
+		(0x0000000000000002ULL << TCAM_ASC_DATA_TRES_SHIFT)
+#define	TCAM_TRES_TERM_OVRD_L2RDC	\
+		(0x0000000000000003ULL << TCAM_ASC_DATA_TRES_SHIFT)
+
+#define	TCAM_ASC_DATA_DISC_MASK		0x0000000000001000ULL
+#define	TCAM_ASC_DATA_DISC_SHIFT	12
+#define	TCAM_ASC_DATA_V4_ECC_OK_MASK    0x0000000000002000ULL
+#define	TCAM_ASC_DATA_V4_ECC_OK_SHIFT	13
+#define	TCAM_ASC_DATA_V4_ECC_OK		\
+		(0x0000000000000001ULL << TCAM_ASC_DATA_V4_ECC_OK_MASK_SHIFT)
+
+#define	TCAM_ASC_DATA_ZFID_MASK		0x0000000003FF3000ULL
+#define	TCAM_ASC_DATA_ZFID_SHIFT	14
+#define	TCAM_ASC_DATA_ZFID(value)	\
+		((value & TCAM_ASC_DATA_ZFID_MASK) >> TCAM_ASC_DATA_ZFID_SHIFT)
+
+#define	TCAM_ASC_DATA_SYNDR_MASK	0x000003FFF3000000ULL
+#define	TCAM_ASC_DATA_SYNDR_SHIFT	26
+#define	TCAM_ASC_DATA_SYNDR(value)  \
+	((value & TCAM_ASC_DATA_SYNDR_MASK) >> TCAM_ASC_DATA_SYNDR_SHIFT)
+
+
+	/* error registers */
+
+#define	FFLP_VLAN_PAR_ERR_REG		(FZC_FFLP + 0x08000)
+
+typedef union _vlan_par_err_t {
+    uint64_t value;
+    struct {
+#if	defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#ifdef _BIT_FIELDS_HTOL
+			uint32_t err:1;
+			uint32_t m_err:1;
+			uint32_t addr:12;
+			uint32_t data:18;
+#else
+			uint32_t data:18;
+			uint32_t addr:12;
+			uint32_t m_err:1;
+			uint32_t err:1;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} vlan_par_err_t, *p_vlan_par_err_t;
+
+
+#define		FFLP_TCAM_ERR_REG		(FZC_FFLP + 0x200D8)
+
+typedef union _tcam_err_t {
+    uint64_t value;
+    struct {
+#if	defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#ifdef _BIT_FIELDS_HTOL
+			uint32_t err:1;
+			uint32_t p_ecc:1;
+			uint32_t mult:1;
+			uint32_t rsrvd:5;
+			uint32_t addr:8;
+			uint32_t syndrome:16;
+#else
+			uint32_t syndrome:16;
+			uint32_t addr:8;
+			uint32_t rsrvd:5;
+			uint32_t mult:1;
+			uint32_t p_ecc:1;
+			uint32_t err:1;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} tcam_err_t, *p_tcam_err_t;
+
+
+#define		TCAM_ERR_SYNDROME_MASK		0x000000000000FFFFULL
+#define		TCAM_ERR_MULT_SHIFT		29
+#define		TCAM_ERR_MULT			0x0000000020000000ULL
+#define		TCAM_ERR_P_ECC			0x0000000040000000ULL
+#define		TCAM_ERR_ERR			0x0000000080000000ULL
+
+#define		HASH_LKUP_ERR_LOG1_REG		(FZC_FFLP + 0x200E0)
+#define		HASH_LKUP_ERR_LOG2_REG		(FZC_FFLP + 0x200E8)
+
+
+
+typedef union _hash_lookup_err_log1_t {
+    uint64_t value;
+    struct {
+#if	defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#ifdef _BIT_FIELDS_HTOL
+			uint32_t rsrvd:28;
+			uint32_t ecc_err:1;
+			uint32_t mult_lk:1;
+			uint32_t cu:1;
+			uint32_t mult_bit:1;
+#else
+			uint32_t mult_bit:1;
+			uint32_t cu:1;
+			uint32_t mult_lk:1;
+			uint32_t ecc_err:1;
+			uint32_t rsrvd:28;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} hash_lookup_err_log1_t, *p_hash_lookup_err_log1_t;
+
+
+
+typedef union _hash_lookup_err_log2_t {
+    uint64_t value;
+    struct {
+#if	defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#ifdef _BIT_FIELDS_HTOL
+			uint32_t rsrvd:1;
+			uint32_t h1:20;
+			uint32_t subarea:3;
+			uint32_t syndrome:8;
+#else
+			uint32_t syndrome:8;
+			uint32_t subarea:3;
+			uint32_t h1:20;
+			uint32_t rsrvd:1;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} hash_lookup_err_log2_t, *p_hash_lookup_err_log2_t;
+
+
+
+#define		FFLP_FCRAM_ERR_TST0_REG	(FZC_FFLP + 0x20128)
+
+typedef union _fcram_err_tst0_t {
+    uint64_t value;
+    struct {
+#if	defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#ifdef _BIT_FIELDS_HTOL
+			uint32_t rsrvd:24;
+			uint32_t syndrome_mask:8;
+#else
+			uint32_t syndrome_mask:10;
+			uint32_t rsrvd:24;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} fcram_err_tst0_t, *p_fcram_err_tst0_t;
+
+
+#define		FFLP_FCRAM_ERR_TST1_REG	(FZC_FFLP + 0x20130)
+#define		FFLP_FCRAM_ERR_TST2_REG	(FZC_FFLP + 0x20138)
+
+typedef union _fcram_err_tst_t {
+    uint64_t value;
+    struct {
+#if	defined(_BIG_ENDIAN)
+		struct {
+			uint32_t dat;
+		} hdw;
+#endif
+		struct {
+			uint32_t dat;
+		} ldw;
+#ifndef _BIG_ENDIAN
+		struct {
+			uint32_t dat;
+		} hdw;
+#endif
+	} bits;
+} fcram_err_tst1_t, *p_fcram_err_tst1_t,
+	fcram_err_tst2_t, *p_fcram_err_tst2_t,
+	fcram_err_data_t, *p_fcram_err_data_t;
+
+
+
+#define		FFLP_ERR_MSK_REG	(FZC_FFLP + 0x20140)
+
+typedef union _fflp_err_mask_t {
+    uint64_t value;
+    struct {
+#if	defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#ifdef _BIT_FIELDS_HTOL
+			uint32_t rsrvd:21;
+			uint32_t hash_tbl_dat:8;
+			uint32_t hash_tbl_lkup:1;
+			uint32_t tcam:1;
+			uint32_t vlan:1;
+#else
+			uint32_t vlan:1;
+			uint32_t tcam:1;
+			uint32_t hash_tbl_lkup:1;
+			uint32_t hash_tbl_dat:8;
+			uint32_t rsrvd:21;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} fflp_err_mask_t, *p_fflp_err_mask_t;
+
+#define	FFLP_ERR_VLAN_MASK 0x00000001ULL
+#define	FFLP_ERR_VLAN 0x00000001ULL
+#define	FFLP_ERR_VLAN_SHIFT 0x0
+
+#define	FFLP_ERR_TCAM_MASK 0x00000002ULL
+#define	FFLP_ERR_TCAM 0x00000001ULL
+#define	FFLP_ERR_TCAM_SHIFT 0x1
+
+#define	FFLP_ERR_HASH_TBL_LKUP_MASK 0x00000004ULL
+#define	FFLP_ERR_HASH_TBL_LKUP 0x00000001ULL
+#define	FFLP_ERR_HASH_TBL_LKUP_SHIFT 0x2
+
+#define	FFLP_ERR_HASH_TBL_DAT_MASK 0x00000007F8ULL
+#define	FFLP_ERR_HASH_TBL_DAT 0x0000000FFULL
+#define	FFLP_ERR_HASH_TBL_DAT_SHIFT 0x3
+
+#define	FFLP_ERR_MASK_ALL (FFLP_ERR_VLAN_MASK | FFLP_ERR_TCAM_MASK | \
+			    FFLP_ERR_HASH_TBL_LKUP_MASK | \
+			    FFLP_ERR_HASH_TBL_DAT_MASK)
+
+
+#define		FFLP_CFG_1_REG	(FZC_FFLP + 0x20100)
+
+typedef union _fflp_cfg_1_t {
+    uint64_t value;
+    struct {
+#if	defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#ifdef _BIT_FIELDS_HTOL
+			uint32_t rsrvd:5;
+			uint32_t tcam_disable:1;
+			uint32_t pio_dbg_sel:3;
+			uint32_t pio_fio_rst:1;
+			uint32_t pio_fio_lat:2;
+			uint32_t camlatency:4;
+			uint32_t camratio:4;
+			uint32_t fcramratio:4;
+			uint32_t fcramoutdr:4;
+			uint32_t fcramqs:1;
+			uint32_t errordis:1;
+			uint32_t fflpinitdone:1;
+			uint32_t llcsnap:1;
+#else
+			uint32_t llcsnap:1;
+			uint32_t fflpinitdone:1;
+			uint32_t errordis:1;
+			uint32_t fcramqs:1;
+			uint32_t fcramoutdr:4;
+			uint32_t fcramratio:4;
+			uint32_t camratio:4;
+			uint32_t camlatency:4;
+			uint32_t pio_fio_lat:2;
+			uint32_t pio_fio_rst:1;
+			uint32_t pio_dbg_sel:3;
+			uint32_t tcam_disable:1;
+			uint32_t rsrvd:5;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} fflp_cfg_1_t, *p_fflp_cfg_1_t;
+
+
+typedef	enum fflp_fcram_output_drive {
+    FCRAM_OUTDR_NORMAL	= 0x0,
+    FCRAM_OUTDR_STRONG	= 0x5,
+    FCRAM_OUTDR_WEAK	= 0xa
+} fflp_fcram_output_drive_t;
+
+
+typedef	enum fflp_fcram_qs {
+    FCRAM_QS_MODE_QS	= 0x0,
+    FCRAM_QS_MODE_FREE	= 0x1
+} fflp_fcram_qs_t;
+
+#define		FCRAM_PIO_HIGH_PRI	0xf
+#define		FCRAM_PIO_MED_PRI	0xa
+#define		FCRAM_LOOKUP_HIGH_PRI	0x0
+#define		FCRAM_LOOKUP_HIGH_PRI	0x0
+#define		FCRAM_IO_DEFAULT_PRI	FCRAM_PIO_MED_PRI
+
+#define		TCAM_PIO_HIGH_PRI	0xf
+#define		TCAM_PIO_MED_PRI	0xa
+#define		TCAM_LOOKUP_HIGH_PRI	0x0
+#define		TCAM_LOOKUP_HIGH_PRI	0x0
+#define		TCAM_IO_DEFAULT_PRI	TCAM_PIO_MED_PRI
+
+#define		TCAM_DEFAULT_LATENCY	0x4
+
+
+#define		FFLP_DBG_TRAIN_VCT_REG	(FZC_FFLP + 0x20148)
+
+typedef union _fflp_dbg_train_vct_t {
+    uint64_t value;
+    struct {
+#if	defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#ifdef _BIT_FIELDS_HTOL
+			uint32_t vector;
+#else
+			uint32_t vector;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} fflp_dbg_train_vct_t, *p_fflp_dbg_train_vct_t;
+
+
+
+#define		FFLP_TCP_CFLAG_MSK_REG	(FZC_FFLP + 0x20108)
+
+typedef union _tcp_cflag_mask_t {
+    uint64_t value;
+    struct {
+#if	defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#ifdef _BIT_FIELDS_HTOL
+			uint32_t rsrvd:20;
+			uint32_t mask:12;
+#else
+			uint32_t mask:12;
+			uint32_t rsrvd:20;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} tcp_cflag_mask_t, *p_tcp_cflag_mask_t;
+
+
+
+#define		FFLP_FCRAM_REF_TMR_REG		(FZC_FFLP + 0x20110)
+
+
+typedef union _fcram_ref_tmr_t {
+#define		FCRAM_REFRESH_DEFAULT_MAX_TIME	0x200
+#define		FCRAM_REFRESH_DEFAULT_MIN_TIME	0x200
+#define		FCRAM_REFRESH_DEFAULT_SYS_TIME	0x200
+#define		FCRAM_REFRESH_MAX_TICK		39 /* usecs */
+#define		FCRAM_REFRESH_MIN_TICK		400 /* nsecs */
+
+    uint64_t value;
+    struct {
+#if	defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#ifdef _BIT_FIELDS_HTOL
+			uint32_t max:16;
+			uint32_t min:16;
+#else
+			uint32_t min:16;
+			uint32_t max:16;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} fcram_ref_tmr_t, *p_fcram_ref_tmr_t;
+
+
+
+
+#define		FFLP_FCRAM_FIO_ADDR_REG	(FZC_FFLP + 0x20118)
+
+typedef union _fcram_fio_addr_t {
+    uint64_t value;
+    struct {
+#if	defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#ifdef _BIT_FIELDS_HTOL
+			uint32_t rsrvd:22;
+			uint32_t addr:10;
+#else
+			uint32_t addr:10;
+			uint32_t rsrvd:22;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} fcram_fio_addr_t, *p_fcram_fio_addr_t;
+
+
+#define		FFLP_FCRAM_FIO_DAT_REG	(FZC_FFLP + 0x20120)
+
+typedef union _fcram_fio_dat_t {
+    uint64_t value;
+    struct {
+#if	defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#ifdef _BIT_FIELDS_HTOL
+			uint32_t rsrvd:22;
+			uint32_t addr:10;
+#else
+			uint32_t addr:10;
+			uint32_t rsrvd:22;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} fcram_fio_dat_t, *p_fcram_fio_dat_t;
+
+
+#define	FFLP_FCRAM_PHY_RD_LAT_REG	(FZC_FFLP + 0x20150)
+
+typedef union _fcram_phy_rd_lat_t {
+	uint64_t value;
+	struct {
+#if	defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#ifdef _BIT_FIELDS_HTOL
+			uint32_t rsrvd:24;
+			uint32_t lat:8;
+#else
+			uint32_t lat:8;
+			uint32_t rsrvd:24;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} fcram_phy_rd_lat_t, *p_fcram_phy_rd_lat_t;
+
+
+/*
+ * Specify how to build a flow key for IP
+ * classes, both programmable and hardwired
+ */
+#define		FFLP_FLOW_KEY_BASE_OFFSET		(FZC_FFLP + 0x40000)
+#define		FFLP_FLOW_KEY_IP_USR4_REG		(FZC_FFLP + 0x40000)
+#define		FFLP_FLOW_KEY_IP_USR5_REG		(FZC_FFLP + 0x40008)
+#define		FFLP_FLOW_KEY_IP_USR6_REG		(FZC_FFLP + 0x40010)
+#define		FFLP_FLOW_KEY_IP_USR7_REG		(FZC_FFLP + 0x40018)
+#define		FFLP_FLOW_KEY_IP4_TCP_REG		(FZC_FFLP + 0x40020)
+#define		FFLP_FLOW_KEY_IP4_UDP_REG		(FZC_FFLP + 0x40028)
+#define		FFLP_FLOW_KEY_IP4_AH_ESP_REG	(FZC_FFLP + 0x40030)
+#define		FFLP_FLOW_KEY_IP4_SCTP_REG		(FZC_FFLP + 0x40038)
+#define		FFLP_FLOW_KEY_IP6_TCP_REG		(FZC_FFLP + 0x40040)
+#define		FFLP_FLOW_KEY_IP6_UDP_REG		(FZC_FFLP + 0x40048)
+#define		FFLP_FLOW_KEY_IP6_AH_ESP_REG	(FZC_FFLP + 0x40050)
+#define		FFLP_FLOW_KEY_IP6_SCTP_REG		(FZC_FFLP + 0x40058)
+
+typedef union _flow_class_key_ip_t {
+    uint64_t value;
+    struct {
+#if	defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#ifdef _BIT_FIELDS_HTOL
+			uint32_t rsrvd2:22;
+			uint32_t port:1;
+			uint32_t l2da:1;
+			uint32_t vlan:1;
+			uint32_t ipsa:1;
+			uint32_t ipda:1;
+			uint32_t proto:1;
+			uint32_t l4_0:2;
+			uint32_t l4_1:2;
+#else
+			uint32_t l4_1:2;
+			uint32_t l4_0:2;
+			uint32_t proto:1;
+			uint32_t ipda:1;
+			uint32_t ipsa:1;
+			uint32_t vlan:1;
+			uint32_t l2da:1;
+			uint32_t port:1;
+			uint32_t rsrvd2:22;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} flow_class_key_ip_t, *p_flow_class_key_ip_t;
+
+
+#define		FFLP_H1POLY_REG		(FZC_FFLP + 0x40060)
+
+
+typedef union _hash_h1poly_t {
+    uint64_t value;
+    struct {
+#if	defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+			uint32_t init_value;
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} hash_h1poly_t, *p_hash_h1poly_t;
+
+#define		FFLP_H2POLY_REG		(FZC_FFLP + 0x40068)
+
+typedef union _hash_h2poly_t {
+    uint64_t value;
+    struct {
+#if	defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#ifdef _BIT_FIELDS_HTOL
+			uint32_t rsrvd:16;
+			uint32_t init_value:16;
+#else
+			uint32_t init_value:16;
+			uint32_t rsrvd:16;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} hash_h2poly_t, *p_hash_h2poly_t;
+
+#define		FFLP_FLW_PRT_SEL_REG		(FZC_FFLP + 0x40070)
+
+
+typedef union _flow_prt_sel_t {
+#define		FFLP_FCRAM_MAX_PARTITION	8
+    uint64_t value;
+    struct {
+#if	defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#ifdef _BIT_FIELDS_HTOL
+			uint32_t rsrvd3:15;
+			uint32_t ext:1;
+			uint32_t rsrvd2:3;
+			uint32_t mask:5;
+			uint32_t rsrvd:3;
+			uint32_t base:5;
+#else
+			uint32_t base:5;
+			uint32_t rsrvd:3;
+			uint32_t mask:5;
+			uint32_t rsrvd2:3;
+			uint32_t ext:1;
+			uint32_t rsrvd3:15;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} flow_prt_sel_t, *p_flow_prt_sel_t;
+
+
+
+/* FFLP Offsets */
+
+
+#define		FFLP_HASH_TBL_ADDR_REG		(FFLP + 0x00000)
+
+typedef union _hash_tbl_addr_t {
+    uint64_t value;
+    struct {
+#if	defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#ifdef _BIT_FIELDS_HTOL
+			uint32_t rsrvd:8;
+			uint32_t autoinc:1;
+			uint32_t addr:23;
+#else
+			uint32_t addr:23;
+			uint32_t autoinc:1;
+			uint32_t rsrvd:8;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} hash_tbl_addr_t, *p_hash_tbl_addr_t;
+
+
+#define		FFLP_HASH_TBL_DATA_REG		(FFLP + 0x00008)
+
+typedef union _hash_tbl_data_t {
+    uint64_t value;
+    struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+		uint32_t ldw;
+#else
+		uint32_t ldw;
+		uint32_t hdw;
+#endif
+	} bits;
+} hash_tbl_data_t, *p_hash_tbl_data_t;
+
+
+#define		FFLP_HASH_TBL_DATA_LOG_REG		(FFLP + 0x00010)
+
+
+typedef union _hash_tbl_data_log_t {
+    uint64_t value;
+    struct {
+#if	defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#ifdef _BIT_FIELDS_HTOL
+			uint32_t pio_err:1;
+			uint32_t fcram_addr:23;
+			uint32_t syndrome:8;
+#else
+			uint32_t syndrome:8;
+			uint32_t fcram_addr:23;
+			uint32_t pio_err:1;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} hash_tbl_data_log_t, *p_hash_tbl_data_log_t;
+
+
+
+#define	REG_PIO_WRITE64(handle, offset, value) \
+		NXGE_REG_WR64((handle), (offset), (value))
+#define	REG_PIO_READ64(handle, offset, val_p) \
+		NXGE_REG_RD64((handle), (offset), (val_p))
+
+
+#define	WRITE_TCAM_REG_CTL(handle, ctl) \
+		REG_PIO_WRITE64(handle, FFLP_TCAM_CTL_REG, ctl)
+
+#define	READ_TCAM_REG_CTL(handle, val_p) \
+		REG_PIO_READ64(handle, FFLP_TCAM_CTL_REG, val_p)
+
+
+#define	WRITE_TCAM_REG_KEY0(handle, key)	\
+		REG_PIO_WRITE64(handle,  FFLP_TCAM_KEY_0_REG, key)
+#define	WRITE_TCAM_REG_KEY1(handle, key) \
+		REG_PIO_WRITE64(handle,  FFLP_TCAM_KEY_1_REG, key)
+#define	WRITE_TCAM_REG_KEY2(handle, key) \
+		REG_PIO_WRITE64(handle,  FFLP_TCAM_KEY_2_REG, key)
+#define	WRITE_TCAM_REG_KEY3(handle, key) \
+		REG_PIO_WRITE64(handle,  FFLP_TCAM_KEY_3_REG, key)
+#define	WRITE_TCAM_REG_MASK0(handle, mask)   \
+		REG_PIO_WRITE64(handle,  FFLP_TCAM_MASK_0_REG, mask)
+#define	WRITE_TCAM_REG_MASK1(handle, mask)   \
+		REG_PIO_WRITE64(handle,  FFLP_TCAM_MASK_1_REG, mask)
+#define	WRITE_TCAM_REG_MASK2(handle, mask)   \
+		REG_PIO_WRITE64(handle,  FFLP_TCAM_MASK_2_REG, mask)
+#define	WRITE_TCAM_REG_MASK3(handle, mask)   \
+		REG_PIO_WRITE64(handle,  FFLP_TCAM_MASK_3_REG, mask)
+
+#define	READ_TCAM_REG_KEY0(handle, val_p)	\
+		REG_PIO_READ64(handle,  FFLP_TCAM_KEY_0_REG, val_p)
+#define	READ_TCAM_REG_KEY1(handle, val_p)	\
+		REG_PIO_READ64(handle,  FFLP_TCAM_KEY_1_REG, val_p)
+#define	READ_TCAM_REG_KEY2(handle, val_p)	\
+		REG_PIO_READ64(handle,  FFLP_TCAM_KEY_2_REG, val_p)
+#define	READ_TCAM_REG_KEY3(handle, val_p)	\
+		REG_PIO_READ64(handle,  FFLP_TCAM_KEY_3_REG, val_p)
+#define	READ_TCAM_REG_MASK0(handle, val_p)	\
+		REG_PIO_READ64(handle,  FFLP_TCAM_MASK_0_REG, val_p)
+#define	READ_TCAM_REG_MASK1(handle, val_p)	\
+		REG_PIO_READ64(handle,  FFLP_TCAM_MASK_1_REG, val_p)
+#define	READ_TCAM_REG_MASK2(handle, val_p)	\
+		REG_PIO_READ64(handle,  FFLP_TCAM_MASK_2_REG, val_p)
+#define	READ_TCAM_REG_MASK3(handle, val_p)	\
+		REG_PIO_READ64(handle,  FFLP_TCAM_MASK_3_REG, val_p)
+
+
+
+
+typedef struct tcam_ipv4 {
+#if defined(_BIG_ENDIAN)
+	uint32_t	reserved6;		/* 255 : 224 */
+	uint32_t	reserved5 : 24;		/* 223 : 200 */
+	uint32_t	cls_code : 5;		/* 199 : 195 */
+	uint32_t	reserved4 : 3;		/* 194 : 192 */
+	uint32_t	l2rd_tbl_num : 5;	/* 191: 187  */
+	uint32_t	noport : 1;		/* 186 */
+	uint32_t	reserved3 : 26;		/* 185: 160  */
+	uint32_t	reserved2;		/* 159: 128  */
+	uint32_t	reserved : 16;		/* 127 : 112 */
+	uint32_t	tos : 8;		/* 111 : 104 */
+	uint32_t	proto : 8;		/* 103 : 96  */
+	uint32_t	l4_port_spi;		/* 95 : 64   */
+	uint32_t	ip_src;			/* 63 : 32   */
+	uint32_t	ip_dest;		/* 31 : 0    */
+#else
+	uint32_t	ip_dest;		/* 31 : 0    */
+	uint32_t	ip_src;			/* 63 : 32   */
+	uint32_t	l4_port_spi;		/* 95 : 64   */
+	uint32_t	proto : 8;		/* 103 : 96  */
+	uint32_t	tos : 8;		/* 111 : 104 */
+	uint32_t	reserved : 16;		/* 127 : 112 */
+	uint32_t	reserved2;		/* 159: 128  */
+	uint32_t	reserved3 : 26;		/* 185: 160  */
+	uint32_t	noport : 1;		/* 186	*/
+	uint32_t	l2rd_tbl_num : 5;	/* 191: 187  */
+	uint32_t	reserved4 : 3;		/* 194 : 192 */
+	uint32_t	cls_code : 5;		/* 199 : 195 */
+	uint32_t	reserved5 : 24;		/* 223 : 200 */
+	uint32_t	reserved6;		/* 255 : 224 */
+#endif
+} tcam_ipv4_t;
+
+
+
+typedef struct tcam_reg {
+#if defined(_BIG_ENDIAN)
+    uint64_t		reg0;
+    uint64_t		reg1;
+    uint64_t		reg2;
+    uint64_t		reg3;
+#else
+    uint64_t		reg3;
+    uint64_t		reg2;
+    uint64_t		reg1;
+    uint64_t		reg0;
+#endif
+} tcam_reg_t;
+
+
+typedef struct tcam_ether {
+#if defined(_BIG_ENDIAN)
+	uint8_t		reserved3[7];		/* 255 : 200 */
+	uint8_t		cls_code : 5;		/* 199 : 195 */
+	uint8_t		reserved2 : 3;		/* 194 : 192 */
+	uint8_t		ethframe[11];		/* 191 : 104 */
+	uint8_t		reserved[13];		/* 103 : 0   */
+#else
+	uint8_t		reserved[13];		/* 103 : 0   */
+	uint8_t		ethframe[11];		/* 191 : 104 */
+	uint8_t		reserved2 : 3;		/* 194 : 192 */
+	uint8_t		cls_code : 5;		/* 199 : 195 */
+	uint8_t		reserved3[7];		/* 255 : 200 */
+#endif
+} tcam_ether_t;
+
+
+typedef struct tcam_ipv6 {
+#if defined(_BIG_ENDIAN)
+	uint32_t	reserved4;		/* 255 : 224 */
+	uint32_t	reserved3 : 24;		/* 223 : 200 */
+	uint32_t	cls_code : 5;		/* 199 : 195 */
+	uint32_t	reserved2 : 3;		/* 194 : 192 */
+	uint32_t	l2rd_tbl_num : 5;	/* 191: 187  */
+	uint32_t	noport : 1;		/* 186  */
+	uint32_t	reserved : 10;		/* 185 : 176 */
+	uint32_t	tos : 8;		/* 175 : 168 */
+	uint32_t	nxt_hdr : 8;		/* 167 : 160 */
+	uint32_t	l4_port_spi;		/* 159 : 128 */
+	uint32_t	ip_addr[4];		/* 127 : 0   */
+#else
+	uint32_t	ip_addr[4];		/* 127 : 0   */
+	uint32_t	l4_port_spi;		/* 159 : 128 */
+	uint32_t	nxt_hdr : 8;		/* 167 : 160 */
+	uint32_t	tos : 8;		/* 175 : 168 */
+	uint32_t	reserved : 10;		/* 185 : 176 */
+	uint32_t	noport : 1;		/* 186 */
+	uint32_t	l2rd_tbl_num : 5;	/* 191: 187  */
+	uint32_t	reserved2 : 3;		/* 194 : 192 */
+	uint32_t	cls_code : 5;		/* 199 : 195 */
+	uint32_t	reserved3 : 24;		/* 223 : 200 */
+	uint32_t	reserved4;		/* 255 : 224 */
+#endif
+} tcam_ipv6_t;
+
+
+typedef struct tcam_entry {
+    union  _tcam_entry {
+	tcam_reg_t	   regs_e;
+	tcam_ether_t	   ether_e;
+	tcam_ipv4_t	   ipv4_e;
+	tcam_ipv6_t	   ipv6_e;
+	} key, mask;
+	tcam_res_t	match_action;
+} tcam_entry_t;
+
+
+#define		key_reg0		key.regs_e.reg0
+#define		key_reg1		key.regs_e.reg1
+#define		key_reg2		key.regs_e.reg2
+#define		key_reg3		key.regs_e.reg3
+#define		mask_reg0		mask.regs_e.reg0
+#define		mask_reg1		mask.regs_e.reg1
+#define		mask_reg2		mask.regs_e.reg2
+#define		mask_reg3		mask.regs_e.reg3
+
+
+#define		key0			key.regs_e.reg0
+#define		key1			key.regs_e.reg1
+#define		key2			key.regs_e.reg2
+#define		key3			key.regs_e.reg3
+#define		mask0			mask.regs_e.reg0
+#define		mask1			mask.regs_e.reg1
+#define		mask2			mask.regs_e.reg2
+#define		mask3			mask.regs_e.reg3
+
+
+#define		ip4_src_key		key.ipv4_e.ip_src
+#define		ip4_dest_key		key.ipv4_e.ip_dest
+#define		ip4_proto_key		key.ipv4_e.proto
+#define		ip4_port_key		key.ipv4_e.l4_port_spi
+#define		ip4_tos_key		key.ipv4_e.tos
+#define		ip4_noport_key		key.ipv4_e.noport
+#define		ip4_nrdc_key		key.ipv4_e.l2rdc_tbl_num
+#define		ip4_class_key		key.ipv4_e.cls_code
+
+#define		ip4_src_mask		mask.ipv4_e.ip_src
+#define		ip4_dest_mask		mask.ipv4_e.ip_dest
+#define		ip4_proto_mask		mask.ipv4_e.proto
+#define		ip4_port_mask		mask.ipv4_e.l4_port_spi
+#define		ip4_tos_mask		mask.ipv4_e.tos
+#define		ip4_nrdc_mask		mask.ipv4_e.l2rdc_tbl_num
+#define		ip4_noport_mask		mask.ipv4_e.noport
+#define		ip4_class_mask		mask.ipv4_e.cls_code
+
+
+#define		ip6_ip_addr_key		key.ipv6_e.ip_addr
+#define		ip6_port_key		key.ipv6_e.l4_port_spi
+#define		ip6_nxt_hdr_key		key.ipv6_e.nxt_hdr
+#define		ip6_tos_key		key.ipv6_e.tos
+#define		ip6_nrdc_key		key.ipv6_e.l2rdc_tbl_num
+#define		ip6_noport_key		key.ipv6_e.noport
+#define		ip6_class_key		key.ipv6_e.cls_code
+
+
+#define		ip6_ip_addr_mask	mask.ipv6_e.ip_addr
+#define		ip6_port_mask		mask.ipv6_e.l4_port_spi
+#define		ip6_nxt_hdr_mask	mask.ipv6_e.nxt_hdr
+#define		ip6_tos_mask		mask.ipv6_e.tos
+#define		ip6_nrdc_mask		mask.ipv6_e.l2rdc_tbl_num
+#define		ip6_noport_mask		mask.ipv6_e.noport
+#define		ip6_class_mask		mask.ipv6_e.cls_code
+
+#define		ether_class_key		key.ether_e.cls_code
+#define		ether_ethframe_key	key.ether_e.ethframe
+#define		ether_class_mask	mask.ether_e.cls_code
+#define		ether_ethframe_mask	mask.ether_e.ethframe
+
+
+/*
+ * flow template structure
+ * The flow header is passed through the hash function
+ * which generates the H1 (and the H2 ) hash value.
+ * Hash computation is started at the 22 zeros.
+ *
+ * Since this structure uses the ip address fields,
+ * /usr/include/netinet/in.h has to be included
+ * before this header file.
+ * Need to move these includes to impl files ...
+ */
+
+#if defined(SOLARIS) || defined(COSIM)
+#include <netinet/in.h>
+#else
+#include <linux/in.h>
+#include <linux/in6.h>
+#endif
+
+
+typedef union flow_template {
+
+    struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t l4_0:16;  /* src port */
+		uint32_t l4_1:16;  /* dest Port */
+
+		uint32_t pid:8;
+		uint32_t port:2;
+		uint32_t zeros:22; /* 0 */
+
+		union {
+			struct {
+				struct in6_addr daddr;
+				struct in6_addr saddr;
+			} ip6_addr;
+
+			struct  {
+				uint32_t rsrvd1;
+				struct in_addr daddr;
+				uint32_t rsrvd2[3];
+				struct in_addr saddr;
+				uint32_t rsrvd5[2];
+			} ip4_addr;
+		} ipaddr;
+
+		union {
+			uint64_t l2_info;
+			struct {
+				uint32_t vlan_valid : 4;
+				uint32_t l2da_1 : 28;
+				uint32_t l2da_0 : 20;
+				uint32_t vlanid : 12;
+
+			}l2_bits;
+		}l2;
+#else
+
+		uint32_t l4_1:16;  /* dest Port */
+		uint32_t l4_0:16;  /* src port */
+
+		uint32_t zeros:22; /* 0 */
+		uint32_t port:2;
+		uint32_t pid:8;
+
+		union {
+			struct {
+				struct in6_addr daddr;
+				struct in6_addr saddr;
+			} ip6_addr;
+
+			struct  {
+				uint32_t rsrvd1;
+				struct in_addr daddr;
+				uint32_t rsrvd2[3];
+				struct in_addr saddr;
+				uint32_t rsrvd5[2];
+			} ip4_addr;
+		} ipaddr;
+
+		union {
+			uint64_t l2_info;
+			struct {
+
+				uint32_t l2da_1 : 28;
+				uint32_t vlan_valid : 4;
+
+				uint32_t vlanid : 12;
+				uint32_t l2da_0 : 20;
+			}l2_bits;
+		}l2;
+#endif
+	} bits;
+
+} flow_template_t;
+
+
+
+#define	ip4_saddr bits.ipaddr.ip4_addr.saddr.s_addr
+#define	ip4_daddr bits.ipaddr.ip4_addr.daddr.s_addr
+
+#define	ip_src_port  bits.l4_0
+#define	ip_dst_port  bits.l4_1
+#define	ip_proto  bits.pid
+
+#define	ip6_saddr bits.ipaddr.ip6_addr.saddr
+#define	ip6_daddr bits.ipaddr.ip6_addr.daddr
+
+
+
+
+typedef struct _flow_key_cfg_t {
+    uint32_t rsrvd:23;
+    uint32_t use_portnum:1;
+    uint32_t use_l2da:1;
+    uint32_t use_vlan:1;
+    uint32_t use_saddr:1;
+    uint32_t use_daddr:1;
+    uint32_t use_sport:1;
+    uint32_t use_dport:1;
+    uint32_t use_proto:1;
+    uint32_t ip_opts_exist:1;
+} flow_key_cfg_t;
+
+
+typedef struct _tcam_key_cfg_t {
+    uint32_t rsrvd:28;
+    uint32_t use_ip_daddr:1;
+    uint32_t use_ip_saddr:1;
+    uint32_t lookup_enable:1;
+    uint32_t discard:1;
+} tcam_key_cfg_t;
+
+
+
+/*
+ * FCRAM Entry Formats
+ *
+ * ip6 and ip4 entries, the first 64 bits layouts are identical
+ * optimistic entry has only 64 bit layout
+ * The first three bits, fmt, ext and valid are the same
+ * accoross all the entries
+ */
+
+typedef union hash_optim {
+    uint64_t value;
+    struct _bits {
+#if defined(_BIG_ENDIAN)
+		uint32_t	fmt : 1;	/* 63  set to zero */
+		uint32_t	ext : 1;	/* 62  set to zero */
+		uint32_t	valid : 1;	/* 61 */
+		uint32_t	rdc_offset : 5;	/* 60 : 56 */
+		uint32_t	h2 : 16;	/* 55 : 40 */
+		uint32_t	rsrvd : 8;	/* 32 : 32 */
+		uint32_t	usr_info;	/* 31 : 0   */
+#else
+		uint32_t	usr_info;	/* 31 : 0   */
+		uint32_t	rsrvd : 8;	/* 39 : 32  */
+		uint32_t	h2 : 16;	/* 55 : 40  */
+		uint32_t	rdc_offset : 5;	/* 60 : 56  */
+		uint32_t	valid : 1;	/* 61 */
+		uint32_t	ext : 1;	/* 62  set to zero */
+		uint32_t	fmt : 1;	/* 63  set to zero */
+#endif
+	} bits;
+} hash_optim_t;
+
+
+typedef    union _hash_hdr {
+    uint64_t value;
+    struct _exact_hdr {
+#if defined(_BIG_ENDIAN)
+		uint32_t	fmt : 1;	/* 63  1 for ipv6, 0 for ipv4 */
+		uint32_t	ext : 1;	/* 62  set to 1 */
+		uint32_t	valid : 1;	/* 61 */
+		uint32_t	rsrvd : 1;	/* 60 */
+		uint32_t	l2da_1 : 28;	/* 59 : 32 */
+		uint32_t	l2da_0 : 20;	/* 31 : 12 */
+		uint32_t	vlan : 12;	/* 12 : 0   */
+#else
+		uint32_t	vlan : 12;	/* 12 : 0   */
+		uint32_t	l2da_0 : 20;	/* 31 : 12 */
+		uint32_t	l2da_1 : 28;	/* 59 : 32 */
+		uint32_t	rsrvd : 1;	/* 60 */
+		uint32_t	valid : 1;	/* 61 */
+		uint32_t	ext : 1;	/* 62  set to 1 */
+		uint32_t	fmt : 1;	/* 63  1 for ipv6, 0 for ipv4 */
+#endif
+	} exact_hdr;
+    hash_optim_t optim_hdr;
+} hash_hdr_t;
+
+
+
+typedef    union _hash_ports {
+    uint64_t value;
+    struct _ports_bits {
+#if defined(_BIG_ENDIAN)
+		uint32_t	ip_dport : 16;	/* 63 : 48 */
+		uint32_t	ip_sport : 16;	/* 47 : 32 */
+		uint32_t	proto : 8;	/* 31 : 24 */
+		uint32_t	port : 2;	/* 23 : 22 */
+		uint32_t	rsrvd : 22;	/* 21 : 0   */
+#else
+		uint32_t	rsrvd : 22;	/* 21 : 0   */
+		uint32_t	port : 2;	/* 23 : 22 */
+		uint32_t	proto : 8;	/* 31 : 24 */
+		uint32_t	ip_sport : 16;	/* 47 : 32 */
+		uint32_t	ip_dport : 16;	/* 63 : 48 */
+#endif
+	} ports_bits;
+} hash_ports_t;
+
+
+
+typedef    union _hash_match_action {
+    uint64_t value;
+    struct _action_bits {
+#if defined(_BIG_ENDIAN)
+		uint32_t	rsrvd2 : 3;	/* 63 : 61  */
+		uint32_t	rdc_offset : 5;	/* 60 : 56 */
+		uint32_t	zfvld : 1;	/* 55 */
+		uint32_t	rsrvd : 3;	/* 54 : 52   */
+		uint32_t	zfid : 12;	/* 51 : 40 */
+		uint32_t	_rsrvd : 8;	/* 39 : 32 */
+		uint32_t	usr_info;	/* 31 : 0   */
+#else
+		uint32_t	usr_info;	/* 31 : 0   */
+		uint32_t	_rsrvd : 8;	/* 39 : 32  */
+		uint32_t	zfid : 12;	/* 51 : 40 */
+		uint32_t	rsrvd : 3;	/* 54 : 52   */
+		uint32_t	zfvld : 1;	/* 55 */
+		uint32_t	rdc_offset : 5;	/* 60 : 56 */
+		uint32_t	rsrvd2 : 1;	/* 63 : 61  */
+#endif
+	} action_bits;
+} hash_match_action_t;
+
+
+typedef    struct _ipaddr6 {
+    struct in6_addr	 saddr;
+    struct in6_addr	 daddr;
+} ip6_addr_t;
+
+
+typedef    struct   _ipaddr4   {
+#if defined(_BIG_ENDIAN)
+    struct in_addr	saddr;
+    struct in_addr	daddr;
+#else
+    struct in_addr	daddr;
+    struct in_addr	saddr;
+#endif
+} ip4_addr_t;
+
+
+	/* ipv4 has 32 byte layout */
+
+typedef struct hash_ipv4 {
+    hash_hdr_t		 hdr;
+    ip4_addr_t		 ip_addr;
+    hash_ports_t	 proto_ports;
+    hash_match_action_t	 action;
+} hash_ipv4_t;
+
+
+	/* ipv4 has 56 byte layout */
+typedef struct hash_ipv6 {
+	hash_hdr_t	hdr;
+    ip6_addr_t		  ip_addr;
+    hash_ports_t	  proto_ports;
+    hash_match_action_t	  action;
+} hash_ipv6_t;
+
+
+
+typedef union fcram_entry {
+    uint64_t		  value[8];
+    hash_tbl_data_t	  dreg[8];
+    hash_ipv6_t		  ipv6_entry;
+    hash_ipv4_t		  ipv4_entry;
+    hash_optim_t	  optim_entry;
+} fcram_entry_t;
+
+
+
+#define	hash_hdr_fmt	ipv4_entry.hdr.exact_hdr.fmt
+#define	hash_hdr_ext	ipv4_entry.hdr.exact_hdr.ext
+#define	hash_hdr_valid	ipv4_entry.hdr.exact_hdr.valid
+
+#define	HASH_ENTRY_EXACT(fc)	\
+	(fc->ipv4_entry.hdr.exact_hdr.ext == 1)
+#define	HASH_ENTRY_OPTIM(fc)	\
+	((fc->ipv4_entry.hdr.exact_hdr.ext == 0) && \
+	(fc->ipv6_entry.hdr.exact_hdr.fmt == 0))
+#define	HASH_ENTRY_EXACT_IP6(fc) \
+	((fc->ipv6_entry.hdr.exact_hdr.fmt == 1) && \
+	(fc->ipv4_entry.hdr.exact_hdr.ext == 1))
+
+#define	HASH_ENTRY_EXACT_IP4(fc) \
+	((fc->ipv6_entry.hdr.exact_hdr.fmt == 0) && \
+	(fc->ipv4_entry.hdr.exact_hdr.ext == 1))
+
+#define	HASH_ENTRY_TYPE(fc)	\
+	(fc->ipv4_entry.hdr.exact_hdr.ext | \
+	(fc->ipv4_entry.hdr.exact_hdr.fmt << 1))
+
+
+
+typedef enum fcram_entry_format {
+	FCRAM_ENTRY_OPTIM = 0x0,
+	FCRAM_ENTRY_EX_IP4 = 0x2,
+	FCRAM_ENTRY_EX_IP6 = 0x3,
+	FCRAM_ENTRY_UNKOWN = 0x1
+} fcram_entry_format_t;
+
+
+#define		HASH_ENTRY_TYPE_OPTIM		FCRAM_ENTRY_OPTIM
+#define		HASH_ENTRY_TYPE_OPTIM_IP4	FCRAM_ENTRY_OPTIM
+#define		HASH_ENTRY_TYPE_OPTIM_IP4	FCRAM_ENTRY_OPTIM
+#define		HASH_ENTRY_TYPE_EX_IP4		FCRAM_ENTRY_EX_IP4
+#define		HASH_ENTRY_TYPE_EX_IP6		FCRAM_ENTRY_EX_IP6
+
+
+
+
+	/* error xxx formats */
+
+
+typedef struct _hash_lookup_err_log {
+    uint32_t rsrvd:28;
+    uint32_t lookup_err:1;
+    uint32_t ecc_err:1;
+    uint32_t uncor_err:1;
+    uint32_t multi_lkup:1;
+    uint32_t multi_bit:1;
+    uint32_t subarea:3;
+    uint32_t syndrome:8;
+    uint32_t h1:20;
+} hash_lookup_err_log_t, *p_hash_lookup_err_log_t;
+
+
+
+typedef struct _hash_pio_err_log {
+    uint32_t rsrvd:32;
+    uint32_t pio_err:1;
+    uint32_t syndrome:8;
+    uint32_t addr:23;
+} hash_pio_err_log_t, *p_hash_pio_err_log_t;
+
+
+
+typedef struct _tcam_err_log {
+    uint32_t rsrvd:2;
+    uint32_t tcam_err:1;
+    uint32_t parity_err:1;
+    uint32_t ecc_err:1;
+    uint32_t multi_lkup:1;
+    uint32_t location:8;
+    uint32_t syndrome:16;
+} tcam_err_log_t, *p_tcam_err_log_t;
+
+
+typedef struct _vlan_tbl_err_log {
+    uint32_t rsrvd:32;
+    uint32_t err:1;
+    uint32_t multi:1;
+    uint32_t addr:12;
+    uint32_t data:18;
+} vlan_tbl_err_log_t, *p_vlan_tbl_err_log_t;
+
+
+#define		NEPTUNE_TCAM_SIZE		0x100
+#define		NIU_TCAM_SIZE			0x80
+#define		FCRAM_SIZE			0x100000
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif	/* _SYS_NXGE_NXGE_FFLP_HW_H */
diff --git a/drivers/net/nxge/include/nxge_flow.h b/drivers/net/nxge/include/nxge_flow.h
new file mode 100644
index 0000000..af9b16a
--- /dev/null
+++ b/drivers/net/nxge/include/nxge_flow.h
@@ -0,0 +1,212 @@
+/*
+ * nxge_flow.h	Neptune Flow Classification interface
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef	_SYS_NXGE_FLOW_H
+#define	_SYS_NXGE_FLOW_H
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#include <linux/types.h>
+#include <linux/ip.h>
+#if defined(__KERNEL_)
+#include <linux/in.h>
+#include <linux/in6.h>
+#define	 S6_addr32		s6_addr32
+#endif
+
+typedef uint32_t in_addr_t;
+typedef struct in6_addr in6_addr_t;
+
+typedef unsigned short in_port_t;
+
+typedef struct tcpip4_spec_s {
+	in_addr_t  ip4src;
+	in_addr_t  ip4dst;
+	in_port_t  psrc;
+	in_port_t  pdst;
+} tcpip4_spec_t;
+
+typedef struct udpip4_spec_s {
+	in_addr_t  ip4src;
+	in_addr_t  ip4dst;
+	in_port_t  psrc;
+	in_port_t  pdst;
+} udpip4_spec_t;
+
+typedef struct ahip4_spec_s {
+	in_addr_t  ip4src;
+	in_addr_t  ip4dst;
+	uint32_t   spi;
+} ahip4_spec_t;
+
+typedef ahip4_spec_t espip4_spec_t;
+
+typedef struct rawip4_spec_s {
+	struct in6_addr ip4src;
+	struct in6_addr ip4dst;
+	uint8_t    hdata[64];
+} rawip4_spec_t;
+
+
+#if 1
+typedef struct tcpip6_spec_s {
+	struct in6_addr ip6src;
+	struct in6_addr ip6dst;
+	in_port_t  psrc;
+	in_port_t  pdst;
+} tcpip6_spec_t;
+
+typedef struct udpip6_spec_s {
+	struct in6_addr ip6src;
+	struct in6_addr ip6dst;
+	in_port_t  psrc;
+	in_port_t  pdst;
+} udpip6_spec_t;
+
+typedef struct ahip6_spec_s {
+	struct in6_addr ip6src;
+	struct in6_addr ip6dst;
+	uint32_t   spi;
+} ahip6_spec_t;
+
+
+typedef ahip6_spec_t espip6_spec_t;
+
+typedef struct rawip6_spec_s {
+	struct in6_addr ip6src;
+	struct in6_addr ip6dst;
+	uint8_t    hdata[64];
+} rawip6_spec_t;
+
+#endif
+
+typedef struct ether_spec_s {
+	uint16_t   ether_type;
+	uint8_t    frame_size;
+	uint8_t    eframe[16];
+} ether_spec_t;
+
+
+typedef struct ip_user_spec_s {
+	uint8_t    id;
+	uint8_t    ip_ver;
+	uint8_t    proto;
+	uint8_t    tos_mask;
+	uint8_t    tos;
+} ip_user_spec_t;
+
+
+
+typedef ether_spec_t arpip_spec_t;
+typedef ether_spec_t ether_user_spec_t;
+
+
+typedef struct flow_spec_s {
+	uint32_t  flow_type;
+	union {
+		tcpip4_spec_t tcpip4spec;
+		udpip4_spec_t udpip4spec;
+		espip4_spec_t espip4spec;
+		ahip4_spec_t  ahip4spec;
+		rawip4_spec_t rawip4spec;
+#if 1
+		tcpip6_spec_t tcpip6spec;
+		udpip6_spec_t udpip6spec;
+		ahip6_spec_t  ahip6spec;
+		espip6_spec_t espip6spec;
+		rawip6_spec_t rawip6spec;
+#endif
+		ether_spec_t  etherspec;
+		arpip_spec_t  arpipspec;
+		ip_user_spec_t  ip_usr_spec;
+		uint8_t		hdata[64];
+	} uh, um; /* entry, mask */
+} flow_spec_t;
+
+#define	FSPEC_TCPIP4	0x1	/* TCP/IPv4 Flow */
+#define	FSPEC_TCPIP6	0x2	/* TCP/IPv6 */
+#define	FSPEC_UDPIP4	0x3	/* UDP/IPv4 */
+#define	FSPEC_UDPIP6	0x4	/* UDP/IPv6 */
+#define	FSPEC_ARPIP	0x5	/* ARP/IPv4 */
+#define	FSPEC_AHIP4	0x6	/* AH/IP4   */
+#define	FSPEC_AHIP6	0x7	/* AH/IP6   */
+#define	FSPEC_ESPIP4	0x8	/* ESP/IP4  */
+#define	FSPEC_ESPIP6	0x9	/* ESP/IP6  */
+#define	FSPEC_SCTPIP4	0xA	/* ESP/IP4  */
+#define	FSPEC_SCTPIP6	0xB	/* ESP/IP6  */
+#define	FSPEC_RAW4	0xC	/* RAW/IP4  */
+#define	FSPEC_RAW6	0xD	/* RAW/IP6  */
+#define	FSPEC_ETHER	0xE	/* ETHER Programmable  */
+#define	FSPEC_IP_USR	0xF	/* IP Programmable  */
+#define	FSPEC_HDATA	0x10	/* Pkt Headers eth-da,sa,etype,ip,tcp(Bitmap) */
+
+
+#define	TCAM_IPV6_ADDR(m32, ip6addr) {		\
+		m32[0] = ip6addr.S6_addr32[0]; \
+		m32[1] = ip6addr.S6_addr32[1]; \
+		m32[2] = ip6addr.S6_addr32[2]; \
+		m32[3] = ip6addr.S6_addr32[3]; \
+	}
+
+
+#define	TCAM_IPV4_ADDR(m32, ip4addr) (m32 = ip4addr)
+#define	TCAM_IP_PORTS(port32, dp, sp)	  (port32 = dp | (sp << 16))
+#define	TCAM_IP_CLASS(key, mask, class)	  {		\
+		key = class; \
+		mask = 0x1f; \
+	}
+
+#define	TCAM_IP_PROTO(key, mask, proto) {		\
+		key = proto; \
+		mask = 0xff; \
+	}
+
+
+typedef struct flow_resource_s {
+	uint64_t channel_cookie;
+	uint64_t flow_cookie;
+	flow_spec_t flow_spec;
+} flow_resource_t;
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif	/* _SYS_NXGE_NXGE_FLOW_H */
diff --git a/drivers/net/nxge/include/nxge_fzc.h b/drivers/net/nxge/include/nxge_fzc.h
new file mode 100644
index 0000000..baedcb6
--- /dev/null
+++ b/drivers/net/nxge/include/nxge_fzc.h
@@ -0,0 +1,109 @@
+/*
+ * nxge_fzc.h	Neptune Function Zero Control interface
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef	_SYS_NXGE_NXGE_FZC_H
+#define	_SYS_NXGE_NXGE_FZC_H
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#include <npi_vir.h>
+
+nxge_status_t nxge_fzc_intr_init(p_nxge_t nxgep);
+nxge_status_t nxge_fzc_intr_ldg_num_set(p_nxge_t nxgep);
+nxge_status_t nxge_fzc_intr_tmres_set(p_nxge_t nxgep);
+nxge_status_t nxge_fzc_intr_sid_set(p_nxge_t nxgep);
+
+nxge_status_t nxge_fzc_dmc_rx_log_page_vld(p_nxge_t nxgep, uint16_t rdc,
+	uint32_t page, boolean_t flag);
+nxge_status_t nxge_fzc_dmc_rx_log_page_mask(p_nxge_t nxgep, uint16_t rdc,
+	uint32_t page, uint32_t mask, uint32_t value);
+
+void nxge_init_fzc_txdma_channels(p_nxge_t nxgep);
+
+nxge_status_t nxge_init_fzc_txdma_channel(p_nxge_t nxgep, uint16_t channel,
+	p_tx_ring_t tx_ring_p, p_tx_mbox_t mbox_p);
+nxge_status_t nxge_init_fzc_txdma_port(p_nxge_t nxgep);
+
+nxge_status_t nxge_init_fzc_rxdma_channel(p_nxge_t nxgep, uint16_t channel,
+	p_rx_rbr_ring_t rbr_p, p_rx_rcr_ring_t rcr_p, p_rx_mbox_t mbox_p);
+
+nxge_status_t nxge_init_fzc_rdc_tbl(p_nxge_t nxgep);
+nxge_status_t nxge_init_fzc_rx_common(p_nxge_t nxgep);
+nxge_status_t nxge_init_fzc_rxdma_port(p_nxge_t nxgep);
+
+nxge_status_t nxge_init_fzc_rxdma_channel_pages(p_nxge_t nxgep,
+	uint16_t channel, p_rx_rbr_ring_t rbr_p);
+
+nxge_status_t nxge_init_fzc_rxdma_channel_red(p_nxge_t nxgep,
+	uint16_t channel, p_rx_rcr_ring_t rcr_p);
+
+nxge_status_t nxge_init_fzc_rxdma_channel_clrlog(p_nxge_t nxgep,
+	uint16_t channel, p_rx_rbr_ring_t rbr_p);
+
+nxge_status_t nxge_init_fzc_txdma_channel_pages(p_nxge_t nxgep,
+	uint16_t channel, p_tx_ring_t tx_ring_p);
+
+nxge_status_t nxge_init_fzc_txdma_channel_drr(p_nxge_t nxgep, uint16_t channel,
+	p_tx_ring_t tx_ring_p);
+
+nxge_status_t nxge_init_fzc_txdma_port(p_nxge_t nxgep);
+
+void nxge_init_fzc_ldg_num(p_nxge_t nxgep);
+void nxge_init_fzc_sys_int_data(p_nxge_t nxgep);
+void nxge_init_fzc_ldg_int_timer(p_nxge_t nxgep);
+
+void nxge_set_fzc_ldg_data(uint8_t ldg_id, nxge_status_t function,
+	int int_data);
+void nxge_set_fzc_ldg_num(uint8_t ldn, uint8_t ldg_id);
+
+nxge_status_t nxge_fzc_sys_err_mask_set(p_nxge_t nxgep, uint64_t mask);
+
+#if	defined(sun4v) && defined(NIU_LP_WORKAROUND)
+nxge_status_t nxge_init_hv_fzc_rxdma_channel_pages(p_nxge_t nxgep,
+	uint16_t channel, p_rx_rbr_ring_t rbr_p);
+nxge_status_t nxge_init_hv_fzc_txdma_channel_pages(p_nxge_t nxgep,
+	uint16_t channel, p_tx_ring_t tx_ring_p);
+#endif
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif	/* _SYS_NXGE_NXGE_FZC_H */
diff --git a/drivers/net/nxge/include/nxge_hw.h b/drivers/net/nxge/include/nxge_hw.h
new file mode 100644
index 0000000..a38666e
--- /dev/null
+++ b/drivers/net/nxge/include/nxge_hw.h
@@ -0,0 +1,1068 @@
+/*
+ * nxge_hw.h	Neptune HW register definitions
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef	_SYS_NXGE_NXGE_HW_H
+#define	_SYS_NXGE_NXGE_HW_H
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#if	!defined(_BIG_ENDIAN) && !defined(_LITTLE_ENDIAN) && \
+		!defined(__BIG_ENDIAN) && !defined(__LITTLE_ENDIAN)
+#error	Host endianness not defined
+#endif
+
+#if	!defined(_BIT_FIELDS_HTOL) && !defined(_BIT_FIELDS_LTOH) && \
+		!defined(__BIT_FIELDS_HTOL) && !defined(__BIT_FIELDS_LTOH)
+#error	Bit ordering not defined
+#endif
+
+#include <nxge_fflp_hw.h>
+#include <nxge_ipp_hw.h>
+#include <nxge_mac_hw.h>
+#include <nxge_rxdma_hw.h>
+#include <nxge_txc_hw.h>
+#include <nxge_txdma_hw.h>
+#include <nxge_zcp_hw.h>
+#include <nxge_espc_hw.h>
+#include <nxge_n2_esr_hw.h>
+#include <nxge_sr_hw.h>
+#include <nxge_phy_hw.h>
+
+
+/* Modes of NXGE core */
+typedef	enum nxge_mode_e {
+	NXGE_MODE_NE		= 1,
+	NXGE_MODE_N2		= 2
+} nxge_mode_t;
+
+/*
+ * Function control Register
+ * (bit 31 is reset to 0. Read back 0 then free to use it.
+ * (once done with it, bit 0:15 can be used to store SW status)
+ */
+#define	DEV_FUNC_SR_REG			(PIO + 0x10000)
+#define	DEV_FUNC_SR_SR_SHIFT		0
+#define	DEV_FUNC_SR_SR_MASK		0x000000000000FFFFULL
+#define	DEV_FUNC_SR_FUNCID_SHIFT	16
+#define	DEV_FUNC_SR_FUNCID_MASK		0x0000000000030000ULL
+#define	DEV_FUNC_SR_TAS_SHIFT		31
+#define	DEV_FUNC_SR_TAS_MASK		0x0000000080000000ULL
+
+typedef union _dev_func_sr_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t tas:1;
+			uint32_t res2:13;
+			uint32_t funcid:2;
+			uint32_t sr:16;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t sr:16;
+			uint32_t funcid:2;
+			uint32_t res2:13;
+			uint32_t tas:1;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} dev_func_sr_t, *p_dev_func_sr_t;
+
+
+/*
+ * Multi Parition Control Register (partitiion manager)
+ */
+#define	MULTI_PART_CTL_REG	(FZC_PIO + 0x00000)
+#define	MULTI_PART_CTL_MPC	0x0000000000000001ULL
+
+typedef union _multi_part_ctl_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res1:31;
+			uint32_t mpc:1;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t mpc:1;
+			uint32_t res1:31;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} multi_part_ctl_t, *p_multi_part_ctl_t;
+
+/*
+ * Virtual DMA CSR Address (partition manager)
+ */
+#define	VADDR_REG		(PIO_VADDR + 0x00000)
+
+/*
+ * DMA Channel Binding Register (partition manager)
+ */
+#define	DMA_BIND_REG		(FZC_PIO + 0x10000)
+#define	DMA_BIND_RX_SHIFT	0
+#define	DMA_BIND_RX_MASK	0x000000000000001FULL
+#define	DMA_BIND_RX_BIND_SHIFT	5
+#define	DMA_BIND_RX_BIND_SET	0x0000000000000020ULL
+#define	DMA_BIND_RX_BIND_MASK	0x0000000000000020ULL
+#define	DMA_BIND_TX_SHIFT	8
+#define	DMA_BIND_TX_MASK	0x0000000000001f00ULL
+#define	DMA_BIND_TX_BIND_SHIFT	13
+#define	DMA_BIND_TX_BIND_SET	0x0000000000002000ULL
+#define	DMA_BIND_TX_BIND_MASK	0x0000000000002000ULL
+
+typedef union _dma_bind_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res1_1:16;
+			uint32_t tx_bind:1;
+			uint32_t tx:5;
+			uint32_t res2:2;
+			uint32_t rx_bind:1;
+			uint32_t rx:5;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t rx:5;
+			uint32_t rx_bind:1;
+			uint32_t res2:2;
+			uint32_t tx:5;
+			uint32_t tx_bind:1;
+			uint32_t res1_1:16;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+}  dma_bind_t, *p_dma_bind_t;
+
+/*
+ * System interrupts:
+ *	Logical device and group definitions.
+ */
+#define	NXGE_INT_MAX_LDS		69
+#define	NXGE_INT_MAX_LDGS		64
+#define	NXGE_LDGRP_PER_NIU_PORT		(NXGE_INT_MAX_LDGS/2)
+#define	NXGE_LDGRP_PER_NEP_PORT		(NXGE_INT_MAX_LDGS/4)
+#define	NXGE_LDGRP_PER_2PORTS		(NXGE_INT_MAX_LDGS/2)
+#define	NXGE_LDGRP_PER_4PORTS		(NXGE_INT_MAX_LDGS/4)
+
+#define	NXGE_RDMA_LD_START		0
+#define	NXGE_TDMA_LD_START		32
+#define	NXGE_MIF_LD			63
+#define	NXGE_MAC_LD_START		64
+#define	NXGE_MAC_LD_PORT0		64
+#define	NXGE_MAC_LD_PORT1		65
+#define	NXGE_MAC_LD_PORT2		66
+#define	NXGE_MAC_LD_PORT3		67
+#define	NXGE_SYS_ERROR_LD		68
+
+/*
+ * Logical Device Group Number
+ */
+#define	LDG_NUM_REG		(FZC_PIO + 0x20000)
+#define	LDG_NUM_NUM_SHIFT	0
+#define	LDG_NUM_NUM_MASK	0x000000000000001FULL
+
+typedef union _ldg_num_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res1_1:26;
+			uint32_t num:6;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t num:6;
+			uint32_t res1_1:26;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} ldg_num_t, *p_ldg_num_t;
+
+/*
+ * Logical Device State Vector
+ */
+#define	LDSV0_REG		(PIO_LDSV + 0x00000)
+#define	LDSV0_LDF_SHIFT		0
+#define	LDSV0_LDF_MASK		0x00000000000003FFULL
+#define	LDG_NUM_NUM_MASK	0x000000000000001FULL
+#define	LDSV_MASK_ALL		0x0000000000000001ULL
+
+/*
+ * Logical Device State Vector 1
+ */
+#define	LDSV1_REG		(PIO_LDSV + 0x00008)
+
+/*
+ * Logical Device State Vector 2
+ */
+#define	LDSV2_REG		(PIO_LDSV + 0x00010)
+
+/* For Logical Device State Vector 0 and 1 */
+typedef union _ldsv_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		uint32_t ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} ldsv_t, *p_ldsv_t;
+
+#define	LDSV2_LDF0_SHIFT		0
+#define	LDSV2_LDF0_MASK			0x000000000000001FULL
+#define	LDSV2_LDF1_SHIFT		5
+#define	LDSV2_LDF1_MASK			0x00000000000001E0ULL
+
+typedef union _ldsv2_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res1_1:22;
+			uint32_t ldf1:5;
+			uint32_t ldf0:5;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t ldf0:5;
+			uint32_t ldf1:5;
+			uint32_t res1_1:22;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} ldsv2_t, *p_ldsv2_t;
+
+/*
+ * Logical Device Interrupt Mask 0
+ */
+#define	LD_IM0_REG		(PIO_IMASK0 + 0x00000)
+#define	LD_IM0_SHIFT		0
+#define	LD_IM0_MASK		0x0000000000000003ULL
+#define	LD_IM_MASK		0x0000000000000003ULL
+
+/*
+ * Logical Device Interrupt Mask 1
+ */
+#define	LD_IM1_REG		(PIO_IMASK1 + 0x00000)
+#define	LD_IM1_SHIFT		0
+#define	LD_IM1_MASK		0x0000000000000003ULL
+
+/* For Lofical Device Interrupt Mask 0 and 1 */
+typedef union _ld_im_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res1_1:30;
+			uint32_t ldf_mask:2;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t ldf_mask:2;
+			uint32_t res1_1:30;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} ld_im_t, *p_ld_im_t;
+
+/*
+ * Logical Device Group Interrupt Management
+ */
+#define	LDGIMGN_REG		(PIO_LDSV + 0x00018)
+#define	LDGIMGN_TIMER_SHIFT	0
+#define	LDGIMGM_TIMER_MASK	0x000000000000003FULL
+#define	LDGIMGN_ARM_SHIFT	31
+#define	LDGIMGM_ARM		0x0000000080000000ULL
+#define	LDGIMGM_ARM_MASK	0x0000000080000000ULL
+
+typedef union _ldgimgm_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t arm:1;
+		uint32_t res2:25;
+		uint32_t timer:6;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t timer:6;
+		uint32_t res2:25;
+		uint32_t arm:1;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} ldgimgm_t, *p_ldgimgm_t;
+
+/*
+ * Logical Device Group Interrupt Timer Resolution
+ */
+#define	LDGITMRES_REG		(FZC_PIO + 0x00008)
+#define	LDGTITMRES_RES_SHIFT	0			/* bits 19:0 */
+#define	LDGTITMRES_RES_MASK	0x00000000000FFFFFULL
+typedef union _ldgitmres_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t res1_1:12;
+		uint32_t res:20;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t res:20;
+		uint32_t res1_1:12;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} ldgitmres_t, *p_ldgitmres_t;
+
+/*
+ * System Interrupt Data
+ */
+#define	SID_REG			(FZC_PIO + 0x10200)
+#define	SID_DATA_SHIFT		0			/* bits 6:0 */
+#define	SID_DATA_MASK		0x000000000000007FULL
+#define	SID_DATA_INTNUM_SHIFT	0			/* bits 4:0 */
+#define	SID_DATA_INTNUM_MASK	0x000000000000001FULL
+#define	SID_DATA_FUNCNUM_SHIFT	5			/* bits 6:5 */
+#define	SID_DATA_FUNCNUM_MASK	0x0000000000000060ULL
+#define	SID_PCI_FUNCTION_SHIFT	(1 << 5)
+#define	SID_N2_INDEX		(1 << 6)
+
+#define	SID_DATA(f, v)		((f << SID_DATA_FUNCNUM_SHIFT) |	\
+				((v << SID_DATA_SHIFT) & SID_DATA_INTNUM_MASK))
+
+#define	SID_DATA_N2(v)		(v | SID_N2_INDEX)
+
+typedef union _sid_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t res1_1:25;
+		uint32_t data:7;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t data:7;
+		uint32_t res1_1:25;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} sid_t, *p_sid_t;
+
+/*
+ * Reset Control
+ */
+#define	RST_CTL_REG		(FZC_PIO + 0x00038)
+#define	RST_CTL_MAC_RST3	0x0000000000400000ULL
+#define	RST_CTL_MAC_RST3_SHIFT	22
+#define	RST_CTL_MAC_RST2	0x0000000000200000ULL
+#define	RST_CTL_MAC_RST2_SHIFT	21
+#define	RST_CTL_MAC_RST1	0x0000000000100000ULL
+#define	RST_CTL_MAC_RST1_SHIFT	20
+#define	RST_CTL_MAC_RST0	0x0000000000080000ULL
+#define	RST_CTL_MAC_RST0_SHIFT	19
+#define	RST_CTL_EN_ACK_TO	0x0000000000000800ULL
+#define	RST_CTL_EN_ACK_TO_SHIFT	11
+#define	RST_CTL_ACK_TO_MASK	0x00000000000007FEULL
+#define	RST_CTL_ACK_TO_SHIFT	1
+
+
+typedef union _rst_ctl_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t res1:9;
+		uint32_t mac_rst3:1;
+		uint32_t mac_rst2:1;
+		uint32_t mac_rst1:1;
+		uint32_t mac_rst0:1;
+		uint32_t res2:7;
+		uint32_t ack_to_en:1;
+		uint32_t ack_to_val:10;
+		uint32_t res3:1;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t res3:1;
+		uint32_t ack_to_val:10;
+		uint32_t ack_to_en:1;
+		uint32_t res2:7;
+		uint32_t mac_rst0:1;
+		uint32_t mac_rst1:1;
+		uint32_t mac_rst2:1;
+		uint32_t mac_rst3:1;
+		uint32_t res1:9;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} rst_ctl_t, *p_rst_ctl_t;
+
+/*
+ * System Error Mask
+ */
+#define	SYS_ERR_MASK_REG	(FZC_PIO + 0x00090)
+
+/*
+ * System Error Status
+ */
+#define	SYS_ERR_STAT_REG	(FZC_PIO + 0x00098)
+
+
+#define	SYS_ERR_META2_MASK	0x0000000000000400ULL
+#define	SYS_ERR_META2_SHIFT	10
+#define	SYS_ERR_META1_MASK	0x0000000000000200ULL
+#define	SYS_ERR_META1_SHIFT	9
+#define	SYS_ERR_PEU_MASK	0x0000000000000100ULL
+#define	SYS_ERR_PEU_SHIFT	8
+#define	SYS_ERR_TXC_MASK	0x0000000000000080ULL
+#define	SYS_ERR_TXC_SHIFT	7
+#define	SYS_ERR_RDMC_MASK	0x0000000000000040ULL
+#define	SYS_ERR_RDMC_SHIFT	6
+#define	SYS_ERR_TDMC_MASK	0x0000000000000020ULL
+#define	SYS_ERR_TDMC_SHIFT	5
+#define	SYS_ERR_ZCP_MASK	0x0000000000000010ULL
+#define	SYS_ERR_ZCP_SHIFT	4
+#define	SYS_ERR_FFLP_MASK	0x0000000000000008ULL
+#define	SYS_ERR_FFLP_SHIFT	3
+#define	SYS_ERR_IPP_MASK	0x0000000000000004ULL
+#define	SYS_ERR_IPP_SHIFT	2
+#define	SYS_ERR_MAC_MASK	0x0000000000000002ULL
+#define	SYS_ERR_MAC_SHIFT	1
+#define	SYS_ERR_SMX_MASK	0x0000000000000001ULL
+#define	SYS_ERR_SMX_SHIFT	0
+#define	SYS_ERR_MASK_ALL	(SYS_ERR_SMX_MASK | SYS_ERR_MAC_MASK | \
+				SYS_ERR_IPP_MASK | SYS_ERR_FFLP_MASK | \
+				SYS_ERR_ZCP_MASK | SYS_ERR_TDMC_MASK | \
+				SYS_ERR_RDMC_MASK | SYS_ERR_TXC_MASK | \
+				SYS_ERR_PEU_MASK | SYS_ERR_META1_MASK | \
+				SYS_ERR_META2_MASK)
+
+
+typedef union _sys_err_mask_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t res:21;
+		uint32_t meta2:1;
+		uint32_t meta1:1;
+		uint32_t peu:1;
+		uint32_t txc:1;
+		uint32_t rdmc:1;
+		uint32_t tdmc:1;
+		uint32_t zcp:1;
+		uint32_t fflp:1;
+		uint32_t ipp:1;
+		uint32_t mac:1;
+		uint32_t smx:1;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t smx:1;
+		uint32_t mac:1;
+		uint32_t ipp:1;
+		uint32_t fflp:1;
+		uint32_t zcp:1;
+		uint32_t tdmc:1;
+		uint32_t rdmc:1;
+		uint32_t txc:1;
+		uint32_t peu:1;
+		uint32_t meta1:1;
+		uint32_t meta2:1;
+		uint32_t res:21;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} sys_err_mask_t, sys_err_stat_t, *p_sys_err_mask_t, *p_sys_err_stat_t;
+
+
+/*
+ * Meta Arbiter Dirty Transaction ID Control
+ */
+
+#define	DIRTY_TID_CTL_REG		(FZC_PIO + 0x0010)
+#define	DIRTY_TID_CTL_WR_THRES_MASK	0x00000000003F0000ULL
+#define	DIRTY_TID_CTL_WR_THRES_SHIFT    16
+#define	DIRTY_TID_CTL_RD_THRES_MASK	0x00000000000003F0ULL
+#define	DIRTY_TID_CTL_RD_THRES_SHIFT	4
+#define	DIRTY_TID_CTL_DTID_CLR		0x0000000000000002ULL
+#define	DIRTY_TID_CTL_DTID_CLR_SHIFT	1
+#define	DIRTY_TID_CTL_DTID_EN		0x0000000000000001ULL
+#define	DIRTY_TID_CTL_DTID_EN_SHIFT	0
+
+typedef union _dty_tid_ctl_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t res1:10;
+		uint32_t np_wr_thres_val:6;
+		uint32_t res2:6;
+		uint32_t np_rd_thres_val:6;
+		uint32_t res3:2;
+		uint32_t dty_tid_clr:1;
+		uint32_t dty_tid_en:1;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t dty_tid_en:1;
+		uint32_t dty_tid_clr:1;
+		uint32_t res3:2;
+		uint32_t np_rd_thres_val:6;
+		uint32_t res2:6;
+		uint32_t np_wr_thres_val:6;
+		uint32_t res1:10;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} dty_tid_ctl_t, *p_dty_tid_ctl_t;
+
+
+/*
+ * Meta Arbiter Dirty Transaction ID Status
+ */
+#define	DIRTY_TID_STAT_REG			(FZC_PIO + 0x0018)
+#define	DIRTY_TID_STAT_WR_TID_DTY_CNT_MASK	0x0000000000003F00ULL
+#define	DIRTY_TID_STAT_WR_TID_DTY_CNT_SHIFT	8
+#define	DIRTY_TID_STAT_RD_TID_DTY_CNT_MASK	0x000000000000003FULL
+#define	DIRTY_TID_STAT_RD_TID_DTY_CNT_SHIFT	0
+
+typedef union _dty_tid_stat_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t res1:18;
+		uint32_t wr_tid_dirty_cnt:6;
+		uint32_t res2:2;
+		uint32_t rd_tid_dirty_cnt:6;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t rd_tid_dirty_cnt:6;
+		uint32_t res2:2;
+		uint32_t wr_tid_dirty_cnt:6;
+		uint32_t res1:18;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} dty_tid_stat_t, *p_dty_tid_stat_t;
+
+
+/*
+ * SMX Registers
+ */
+#define	SMX_CFIG_DAT_REG		(FZC_PIO + 0x00040)
+#define	SMX_CFIG_DAT_RAS_DET_EN_MASK	0x0000000080000000ULL
+#define	SMX_CFIG_DAT_RAS_DET_EN_SHIFT	31
+#define	SMX_CFIG_DAT_RAS_INJ_EN_MASK	0x0000000040000000ULL
+#define	SMX_CFIG_DAT_RAS_INJ_EN_SHIFT	30
+#define	SMX_CFIG_DAT_TRANS_TO_MASK	0x000000000FFFFFFFULL
+#define	SMX_CFIG_DAT_TRANS_TO_SHIFT	0
+
+typedef union _smx_cfg_dat_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t res_err_det:1;
+		uint32_t ras_err_inj_en:1;
+		uint32_t res:2;
+		uint32_t trans_to_val:28;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t trans_to_val:28;
+		uint32_t res:2;
+		uint32_t ras_err_inj_en:1;
+		uint32_t res_err_det:1;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} smx_cfg_dat_t, *p_smx_cfg_dat_t;
+
+
+#define	SMX_INT_STAT_REG	(FZC_PIO + 0x00048)
+#define	SMX_INT_STAT_SM_MASK	0x00000000FFFFFFC0ULL
+#define	SMX_INT_STAT_SM_SHIFT	6
+
+typedef union _smx_int_stat_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t st_mc_stat:26;
+		uint32_t res:6;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t res:6;
+		uint32_t st_mc_stat:26;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} smx_int_stat_t, *p_smx_int_stat_t;
+
+
+#define		SMX_CTL_REG	(FZC_PIO + 0x00050)
+
+typedef union _smx_ctl_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t res1:21;
+		uint32_t resp_err_inj:3;
+		uint32_t res2:1;
+		uint32_t xtb_err_inj:3;
+		uint32_t res3:1;
+		uint32_t dbg_sel:3;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t dbg_sel:3;
+		uint32_t res3:1;
+		uint32_t xtb_err_inj:3;
+		uint32_t res2:1;
+		uint32_t resp_err_inj:3;
+		uint32_t res1:21;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} smx_ctl_t, *p_smx_ctl_t;
+
+
+#define	SMX_DBG_VEC_REG	(FZC_PIO + 0x00058)
+
+typedef union _smx_dbg_vec_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+		uint32_t dbg_tng_vec;
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} smx_dbg_vec_t, *p_smx_dbg_vec_t;
+
+
+/*
+ * Debug registers
+ */
+
+#define	PIO_DBG_SEL_REG	(FZC_PIO + 0x00060)
+
+typedef union _pio_dbg_sel_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+		uint32_t sel;
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} pio_dbg_sel_t, *p_pio_dbg_sel_t;
+
+
+#define	PIO_TRAIN_VEC_REG	(FZC_PIO + 0x00068)
+
+typedef union _pio_tng_vec_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+		uint32_t training_vec;
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} pio_tng_vec_t, *p_pio_tng_vec_t;
+
+#define	PIO_ARB_CTL_REG	(FZC_PIO + 0x00070)
+
+typedef union _pio_arb_ctl_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+		uint32_t ctl;
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} pio_arb_ctl_t, *p_pio_arb_ctl_t;
+
+#define	PIO_ARB_DBG_VEC_REG	(FZC_PIO + 0x00078)
+
+typedef union _pio_arb_dbg_vec_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+		uint32_t dbg_vector;
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} pio_arb_dbg_vec_t, *p_pio_arb_dbg_vec_t;
+
+
+/*
+ * GPIO Registers
+ */
+
+#define	GPIO_EN_REG	(FZC_PIO + 0x00028)
+#define	GPIO_EN_ENABLE_MASK	 0x000000000000FFFFULL
+#define	GPIO_EN_ENABLE_SHIFT	 0
+typedef union _gpio_en_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t res:16;
+		uint32_t enable:16;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t enable:16;
+		uint32_t res:16;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} gpio_en_t, *p_gpio_en_t;
+
+#define	GPIO_DATA_IN_REG	(FZC_PIO + 0x00030)
+#define	GPIO_DATA_IN_MASK	0x000000000000FFFFULL
+#define	GPIO_DATA_IN_SHIFT	0
+typedef union _gpio_data_in_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t res:16;
+		uint32_t data_in:16;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t data_in:16;
+		uint32_t res:16;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} gpio_data_in_t, *p_gpio_data_in_t;
+
+
+/*
+ * PCI Express Interface Module (PIM) registers
+ */
+#define	PIM_CONTROL_REG	(FZC_PIM + 0x0)
+#define	PIM_CONTROL_DBG_SEL_MASK 0x000000000000000FULL
+#define	PIM_CONTROL_DBG_SEL_SHIFT	0
+typedef union _pim_ctl_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t res:28;
+		uint32_t dbg_sel:4;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t dbg_sel:4;
+		uint32_t res:28;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} pim_ctl_t, *p_pim_ctl_t;
+
+#define	PIM_DBG_TRAINING_VEC_REG	(FZC_PIM + 0x00008)
+#define	PIM_DBG_TRAINING_VEC_MASK	0x00000000FFFFFFFFULL
+
+#define	PIM_INTR_STATUS_REG		(FZC_PIM + 0x00010)
+#define	PIM_INTR_STATUS_MASK		0x00000000FFFFFFFFULL
+
+#define	PIM_INTERNAL_STATUS_REG		(FZC_PIM + 0x00018)
+#define	PIM_INTERNAL_STATUS_MASK	0x00000000FFFFFFFFULL
+
+#define	PIM_INTR_MASK_REG		(FZC_PIM + 0x00020)
+#define	PIM_INTR_MASK_MASK		0x00000000FFFFFFFFULL
+
+/*
+ * Partitioning Logical pages Definition registers.
+ * (used by both receive and transmit DMA channels)
+ */
+
+/* Logical page definitions */
+typedef union _log_page_vld_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res1_1:28;
+			uint32_t func:2;
+			uint32_t page1:1;
+			uint32_t page0:1;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t page0:1;
+			uint32_t page1:1;
+			uint32_t func:2;
+			uint32_t res1_1:28;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} log_page_vld_t, *p_log_page_vld_t;
+
+
+#define	DMA_LOG_PAGE_MASK_SHIFT		0
+#define	DMA_LOG_PAGE_MASK_MASK		0x00000000ffffffffULL
+
+/* Receive Logical Page Mask */
+typedef union _log_page_mask_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t mask:32;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t mask:32;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} log_page_mask_t, *p_log_page_mask_t;
+
+
+/* Receive Logical Page Value */
+#define	DMA_LOG_PAGE_VALUE_SHIFT	0
+#define	DMA_LOG_PAGE_VALUE_MASK		0x00000000ffffffffULL
+
+/* Receive Logical Page Value */
+typedef union _log_page_value_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t value:32;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t value:32;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} log_page_value_t, *p_log_page_value_t;
+
+/* Receive Logical Page Relocation */
+#define	DMA_LOG_PAGE_RELO_SHIFT		0			/* bits 31:0 */
+#define	DMA_LOG_PAGE_RELO_MASK		0x00000000ffffffffULL
+
+/* Receive Logical Page Relocation */
+typedef union _log_page_relo_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t relo:32;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t relo:32;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} log_page_relo_t, *p_log_page_relo_t;
+
+
+/* Receive Logical Page Handle */
+#define	DMA_LOG_PAGE_HANDLE_SHIFT	0			/* bits 19:0 */
+#define	DMA_LOG_PAGE_HANDLE_MASK	0x00000000ffffffffULL
+
+/* Receive Logical Page Handle */
+typedef union _log_page_hdl_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res1_1:12;
+			uint32_t handle:20;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t handle:20;
+			uint32_t res1_1:12;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} log_page_hdl_t, *p_log_page_hdl_t;
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif	/* _SYS_NXGE_NXGE_HW_H */
diff --git a/drivers/net/nxge/include/nxge_impl.h b/drivers/net/nxge/include/nxge_impl.h
new file mode 100644
index 0000000..8137984
--- /dev/null
+++ b/drivers/net/nxge/include/nxge_impl.h
@@ -0,0 +1,507 @@
+/*
+ * nxge_impl.h	Neptune function declarations
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef __NXGE_IMPL_H
+#define __NXGE_IMPL_H
+
+#include <nxge_common_impl.h>
+#include <nxge_defs.h>
+#include <nxge_hw.h>
+#include <nxge_mii.h>
+
+typedef struct _nxge_t nxge_t, *p_nxge_t;
+typedef uint32_t nxge_status_t;
+
+#include <nxge_txc.h>
+#include <nxge_fflp.h>
+#include <nxge_zcp.h>
+#include <nxge_espc.h>
+#include <nxge.h>
+
+#define	TXDMA_PORT_BITMAP(nxgep)		(nxgep->pt_config.tx_dma_map)
+
+/*
+ * NPI related macros
+ */
+#define	NXGE_DEV_NPI_HANDLE(nxgep)	(nxgep->npi_handle)
+
+#define NXGE_OK			0
+#define NXGE_ERROR		0x80000000
+#define NXGE_GET_PORT_NUM(n)	n
+
+/*
+ * Defaults
+ */
+
+#define	NXGE_RDC_RCR_THRESHOLD_MAX	256
+#define	NXGE_RDC_RCR_TIMEOUT_MAX	64
+#define	NXGE_RDC_RCR_THRESHOLD_MIN	1
+#define	NXGE_RDC_RCR_TIMEOUT_MIN	1
+#define	NXGE_RCR_FULL_HEADER		0
+
+#define	NXGE_TX_COPY_BUFFER_SIZE	0x100
+
+#define	NXGE_MAX_MMAC_ADDRS	32
+#define	NXGE_NUM_MMAC_ADDRS	8
+#define	NXGE_NUM_OF_PORTS_QUAD	4
+#define	NXGE_NUM_OF_PORTS_DUAL	2
+
+#define	NXGE_QGC_LP_BM_STR		"501-7606"
+#define	NXGE_2XGF_LP_BM_STR		"501-7283"
+#define	NXGE_QGC_PEM_BM_STR		"501-7765"
+#define	NXGE_2XGF_PEM_BM_STR	"501-7626"
+#define	NXGE_ALONSO_BM_STR		"373-0202-01"
+#define	NXGE_ALONSO_MODEL_STR	"SUNW,CP3220"
+#define	NXGE_EROM_LEN			1048576
+
+typedef enum {
+	CFG_DEFAULT = 0,	/* default cfg */
+	CFG_EQUAL,	/* Equal */
+	CFG_FAIR,	/* Equal */
+	CFG_CLASSIFY,
+	CFG_L2_CLASSIFY,
+	CFG_L3_CLASSIFY,
+	CFG_L3_DISTRIBUTE,
+	CFG_L3_WEB,
+	CFG_L3_TCAM,
+	CFG_NOT_SPECIFIED,
+	CFG_CUSTOM	/* Custom */
+} hw_cfg_type_t;
+
+typedef enum {
+	DEFAULT = 0,
+	EQUAL,
+	FAIR,
+	CUSTOM,
+	CLASSIFY,
+	L2_CLASSIFY,
+	L3_DISTRIBUTE,
+	L3_CLASSIFY,
+	L3_TCAM,
+	CONFIG_TOKEN_NONE
+} config_token_t;
+
+
+typedef enum {
+        NEPTUNE,        /* 4 ports */
+        NEPTUNE_2,      /* 2 ports */
+        N2_NIU          /* N2/NIU 2 ports */
+} niu_type_t;
+
+#if 1
+
+/************ Some FM error types to support error injection - take them out if error injection is not to be supported in Linux *********************/
+
+#define	EREPORT_FM_ID_SHIFT		16
+#define	EREPORT_FM_ID_MASK		0xFF
+#define	EREPORT_INDEX_MASK		0xFF
+#define	NXGE_FM_EREPORT_UNKNOWN		0
+
+#define	FM_SW_ID			0xFF
+#define	FM_PCS_ID			MAC_BLK_ID
+#define	FM_TXMAC_ID			TXMAC_BLK_ID
+#define	FM_RXMAC_ID			RXMAC_BLK_ID
+#define	FM_MIF_ID			MIF_BLK_ID
+#define	FM_IPP_ID			IPP_BLK_ID
+#define	FM_TXC_ID			TXC_BLK_ID
+#define	FM_TXDMA_ID			TXDMA_BLK_ID
+#define	FM_RXDMA_ID			RXDMA_BLK_ID
+#define	FM_ZCP_ID			ZCP_BLK_ID
+#define	FM_ESPC_ID			ESPC_BLK_ID
+#define	FM_FFLP_ID			FFLP_BLK_ID
+#define	FM_PCIE_ID			PCIE_BLK_ID
+#define	FM_ETHER_SERDES_ID		ETHER_SERDES_BLK_ID
+#define	FM_PCIE_SERDES_ID		PCIE_SERDES_BLK_ID
+#define	FM_VIR_ID			VIR_BLK_ID
+#endif
+
+#define	NXGE_ERROR_MSG(params) nxge_error_msg params
+
+#ifdef NXGE_DEBUG
+#define	NXGE_DEBUG_MSG(params) nxge_debug_msg params
+#define	NXGE_WARN_MSG(params) nxge_debug_msg params
+#else
+#define	NXGE_DEBUG_MSG(params)
+#define	NXGE_WARN_MSG(params)
+#endif
+
+#if 1
+/* General MAC ereports */
+typedef	enum {
+	NXGE_FM_EREPORT_XPCS_LINK_DOWN = (FM_PCS_ID << EREPORT_FM_ID_SHIFT),
+	NXGE_FM_EREPORT_XPCS_TX_LINK_FAULT,
+	NXGE_FM_EREPORT_XPCS_RX_LINK_FAULT,
+	NXGE_FM_EREPORT_PCS_LINK_DOWN,
+	NXGE_FM_EREPORT_PCS_REMOTE_FAULT
+} nxge_fm_ereport_pcs_t;
+
+/* MIF ereports */
+typedef	enum {
+	NXGE_FM_EREPORT_MIF_ACCESS_FAIL = (FM_MIF_ID << EREPORT_FM_ID_SHIFT)
+} nxge_fm_ereport_mif_t;
+
+/* FFLP ereports */
+typedef	enum {
+	NXGE_FM_EREPORT_FFLP_TCAM_ERR = (FM_FFLP_ID << EREPORT_FM_ID_SHIFT),
+	NXGE_FM_EREPORT_FFLP_VLAN_PAR_ERR,
+	NXGE_FM_EREPORT_FFLP_HASHT_DATA_ERR,
+	NXGE_FM_EREPORT_FFLP_HASHT_LOOKUP_ERR,
+	NXGE_FM_EREPORT_FFLP_ACCESS_FAIL
+} nxge_fm_ereport_fflp_t;
+
+/* IPP ereports */
+typedef	enum {
+	NXGE_FM_EREPORT_IPP_EOP_MISS = (FM_IPP_ID << EREPORT_FM_ID_SHIFT),
+	NXGE_FM_EREPORT_IPP_SOP_MISS,
+	NXGE_FM_EREPORT_IPP_DFIFO_UE,
+	NXGE_FM_EREPORT_IPP_DFIFO_CE,
+	NXGE_FM_EREPORT_IPP_PFIFO_PERR,
+	NXGE_FM_EREPORT_IPP_ECC_ERR_MAX,
+	NXGE_FM_EREPORT_IPP_PFIFO_OVER,
+	NXGE_FM_EREPORT_IPP_PFIFO_UND,
+	NXGE_FM_EREPORT_IPP_BAD_CS_MX,
+	NXGE_FM_EREPORT_IPP_PKT_DIS_MX,
+	NXGE_FM_EREPORT_IPP_RESET_FAIL
+} nxge_fm_ereport_ipp_t;
+
+/* RDMC ereports */
+typedef	enum {
+	NXGE_FM_EREPORT_RDMC_DCF_ERR = (FM_RXDMA_ID << EREPORT_FM_ID_SHIFT),
+	NXGE_FM_EREPORT_RDMC_RCR_ACK_ERR,
+	NXGE_FM_EREPORT_RDMC_DC_FIFO_ERR,
+	NXGE_FM_EREPORT_RDMC_RCR_SHA_PAR,
+	NXGE_FM_EREPORT_RDMC_RBR_PRE_PAR,
+	NXGE_FM_EREPORT_RDMC_RBR_TMOUT,
+	NXGE_FM_EREPORT_RDMC_RSP_CNT_ERR,
+	NXGE_FM_EREPORT_RDMC_BYTE_EN_BUS,
+	NXGE_FM_EREPORT_RDMC_RSP_DAT_ERR,
+	NXGE_FM_EREPORT_RDMC_ID_MISMATCH,
+	NXGE_FM_EREPORT_RDMC_ZCP_EOP_ERR,
+	NXGE_FM_EREPORT_RDMC_IPP_EOP_ERR,
+	NXGE_FM_EREPORT_RDMC_COMPLETION_ERR,
+	NXGE_FM_EREPORT_RDMC_CONFIG_ERR,
+	NXGE_FM_EREPORT_RDMC_RCRINCON,
+	NXGE_FM_EREPORT_RDMC_RCRFULL,
+	NXGE_FM_EREPORT_RDMC_RBRFULL,
+	NXGE_FM_EREPORT_RDMC_RBRLOGPAGE,
+	NXGE_FM_EREPORT_RDMC_CFIGLOGPAGE
+} nxge_fm_ereport_rdmc_t;
+
+/* ZCP ereports */
+typedef	enum {
+	NXGE_FM_EREPORT_ZCP_RRFIFO_UNDERRUN =
+					(FM_ZCP_ID << EREPORT_FM_ID_SHIFT),
+	NXGE_FM_EREPORT_ZCP_RSPFIFO_UNCORR_ERR,
+	NXGE_FM_EREPORT_ZCP_STAT_TBL_PERR,
+	NXGE_FM_EREPORT_ZCP_DYN_TBL_PERR,
+	NXGE_FM_EREPORT_ZCP_BUF_TBL_PERR,
+	NXGE_FM_EREPORT_ZCP_CFIFO_ECC,
+	NXGE_FM_EREPORT_ZCP_RRFIFO_OVERRUN,
+	NXGE_FM_EREPORT_ZCP_BUFFER_OVERFLOW,
+	NXGE_FM_EREPORT_ZCP_TT_PROGRAM_ERR,
+	NXGE_FM_EREPORT_ZCP_RSP_TT_INDEX_ERR,
+	NXGE_FM_EREPORT_ZCP_SLV_TT_INDEX_ERR,
+	NXGE_FM_EREPORT_ZCP_TT_INDEX_ERR,
+	NXGE_FM_EREPORT_ZCP_ACCESS_FAIL
+} nxge_fm_ereport_zcp_t;
+
+typedef enum {
+	NXGE_FM_EREPORT_RXMAC_UNDERFLOW = (FM_RXMAC_ID << EREPORT_FM_ID_SHIFT),
+	NXGE_FM_EREPORT_RXMAC_CRC_ERRCNT_EXP,
+	NXGE_FM_EREPORT_RXMAC_LENGTH_ERRCNT_EXP,
+	NXGE_FM_EREPORT_RXMAC_VIOL_ERRCNT_EXP,
+	NXGE_FM_EREPORT_RXMAC_RXFRAG_CNT_EXP,
+	NXGE_FM_EREPORT_RXMAC_ALIGN_ECNT_EXP,
+	NXGE_FM_EREPORT_RXMAC_LINKFAULT_CNT_EXP,
+	NXGE_FM_EREPORT_RXMAC_RESET_FAIL
+} nxge_fm_ereport_rxmac_t;
+
+typedef	enum {
+	NXGE_FM_EREPORT_TDMC_PREF_BUF_PAR_ERR =
+				(FM_TXDMA_ID << EREPORT_FM_ID_SHIFT),
+	NXGE_FM_EREPORT_TDMC_MBOX_ERR,
+	NXGE_FM_EREPORT_TDMC_NACK_PREF,
+	NXGE_FM_EREPORT_TDMC_NACK_PKT_RD,
+	NXGE_FM_EREPORT_TDMC_PKT_SIZE_ERR,
+	NXGE_FM_EREPORT_TDMC_TX_RING_OFLOW,
+	NXGE_FM_EREPORT_TDMC_CONF_PART_ERR,
+	NXGE_FM_EREPORT_TDMC_PKT_PRT_ERR,
+	NXGE_FM_EREPORT_TDMC_RESET_FAIL
+} nxge_fm_ereport_attr_tdmc_t;
+
+typedef	enum {
+	NXGE_FM_EREPORT_TXC_RO_CORRECT_ERR =
+				(FM_TXC_ID << EREPORT_FM_ID_SHIFT),
+	NXGE_FM_EREPORT_TXC_RO_UNCORRECT_ERR,
+	NXGE_FM_EREPORT_TXC_SF_CORRECT_ERR,
+	NXGE_FM_EREPORT_TXC_SF_UNCORRECT_ERR,
+	NXGE_FM_EREPORT_TXC_ASSY_DEAD,
+	NXGE_FM_EREPORT_TXC_REORDER_ERR
+} nxge_fm_ereport_attr_txc_t;
+
+typedef	enum {
+	NXGE_FM_EREPORT_TXMAC_UNDERFLOW =
+				(FM_TXMAC_ID << EREPORT_FM_ID_SHIFT),
+	NXGE_FM_EREPORT_TXMAC_OVERFLOW,
+	NXGE_FM_EREPORT_TXMAC_TXFIFO_XFR_ERR,
+	NXGE_FM_EREPORT_TXMAC_MAX_PKT_ERR,
+	NXGE_FM_EREPORT_TXMAC_RESET_FAIL
+} nxge_fm_ereport_attr_txmac_t;
+
+typedef	enum {
+	NXGE_FM_EREPORT_ESPC_ACCESS_FAIL = (FM_ESPC_ID << EREPORT_FM_ID_SHIFT)
+} nxge_fm_ereport_espc_t;
+
+typedef	enum {
+	NXGE_FM_EREPORT_SW_INVALID_PORT_NUM = (FM_SW_ID << EREPORT_FM_ID_SHIFT),
+	NXGE_FM_EREPORT_SW_INVALID_CHAN_NUM,
+	NXGE_FM_EREPORT_SW_INVALID_PARAM
+} nxge_fm_ereport_sw_t;
+
+#define	NXGE_FM_EREPORT_UNKNOWN			0
+#define	NXGE_FM_EREPORT_UNKNOWN_NAME		""
+#endif
+
+#define	RXDMA_MAX_ERR_SHOW	1
+
+/**********************************************************************/
+
+void nxge_debug_msg(p_nxge_t nep, debug_level_t level, char *fmt, ...);
+void nxge_error_msg(p_nxge_t nep, debug_level_t level, char *fmt, ...);
+nxge_status_t nxge_tx_port_fatal_err_recover(p_nxge_t nxgep);
+int nxge_rxdma_cfg_rdcgrp_default_rdc(p_nxge_t nxgep, uint8_t rdcgrp,
+									  uint8_t rdc);
+int nxge_rxdma_cfg_port_default_rdc(p_nxge_t nxgep,
+									uint8_t port, uint8_t rdc);
+int nxge_rxdma_cfg_rcr_threshold(p_nxge_t nxgep,
+								 uint8_t channel, uint16_t pkts);
+
+int nxge_rxdma_cfg_rcr_timeout(p_nxge_t nxgep, uint8_t channel,
+						   uint16_t tout, uint8_t enable);
+
+boolean_t nxge_check_rxdma_rdcgrp_member(p_nxge_t nxgep,
+										 uint8_t rdc_grp, uint8_t rdc);
+
+boolean_t nxge_check_rdcgrp_port_member(p_nxge_t nxgep, uint8_t rdc_grp);
+
+int nxge_dump_rxdma_channel(p_nxge_t, uint8_t);
+void nxge_txdma_regs_dump(p_nxge_t, int);
+/* nxge_classify.c */
+nxge_status_t nxge_classify_init(p_nxge_t);
+nxge_status_t nxge_set_hw_classify_config(p_nxge_t nxgep);
+
+/* nxge_fflp.c */
+nxge_status_t nxge_classify_init_hw(p_nxge_t);
+nxge_status_t nxge_classify_init_sw(p_nxge_t);
+nxge_status_t nxge_fflp_ip_class_config_all(p_nxge_t nxgep);
+nxge_status_t nxge_fflp_ip_class_config(p_nxge_t nxgep, tcam_class_t l3_class,
+				    uint32_t class_config);
+
+nxge_status_t nxge_fflp_ip_class_config_get(p_nxge_t nxgep, tcam_class_t l3_class,
+				    uint32_t *class_config);
+
+nxge_status_t nxge_cfg_ip_cls_flow_key(p_nxge_t nxgep, tcam_class_t l3_class,
+				    uint32_t class_config);
+
+nxge_status_t nxge_fflp_ip_usr_class_config(p_nxge_t nxgep, tcam_class_t class,
+				    uint32_t config);
+
+
+nxge_status_t nxge_fflp_config_tcam_enable(p_nxge_t nxgep);
+nxge_status_t nxge_fflp_config_tcam_disable(p_nxge_t nxgep);
+nxge_status_t nxge_fflp_config_hash_lookup_enable(p_nxge_t nxgep);
+nxge_status_t nxge_fflp_config_hash_lookup_disable(p_nxge_t nxgep);
+nxge_status_t nxge_fflp_config_llc_snap_enable(p_nxge_t nxgep);
+nxge_status_t nxge_fflp_config_llc_snap_disable(p_nxge_t nxgep);
+
+nxge_status_t nxge_logical_mac_assign_rdc_table(p_nxge_t nxgep, uint8_t mac);
+nxge_status_t nxge_fflp_config_vlan_table(p_nxge_t nxgep, uint16_t vlan_id);
+
+nxge_status_t nxge_fflp_set_hash1(p_nxge_t nxgep, uint32_t h1);
+
+nxge_status_t nxge_fflp_set_hash2(p_nxge_t nxgep, uint16_t h2);
+
+void nxge_handle_tcam_fragment_bug(p_nxge_t nxgep);
+nxge_status_t nxge_fflp_hw_reset(p_nxge_t nxgep);
+
+nxge_status_t nxge_fflp_handle_sys_errors(p_nxge_t nxgep);
+nxge_status_t nxge_zcp_handle_sys_errors(p_nxge_t nxgep);
+nxge_status_t nxge_classify_exit_sw(p_nxge_t nxgep);
+nxge_status_t nxge_fflp_init_hostinfo(p_nxge_t nxgep);
+void nxge_put_tcam(p_nxge_t nxgep, void *data);
+
+/* MAC functions */
+nxge_status_t nxge_mac_init(p_nxge_t);
+nxge_status_t nxge_link_init(p_nxge_t nxgep);
+nxge_status_t nxge_xif_init(p_nxge_t);
+nxge_status_t nxge_pcs_init(p_nxge_t);
+nxge_status_t nxge_serdes_init(p_nxge_t);
+nxge_status_t nxge_serdes_reset(p_nxge_t);
+nxge_status_t nxge_n2_serdes_init(p_nxge_t nxgep);
+nxge_status_t nxge_neptune_serdes_init(p_nxge_t nxgep);
+nxge_status_t nxge_xcvr_find(p_nxge_t nxgep);
+nxge_status_t nxge_xcvr_init(p_nxge_t);
+nxge_status_t nxge_tx_mac_init(p_nxge_t);
+nxge_status_t nxge_rx_mac_init(p_nxge_t);
+nxge_status_t nxge_tx_mac_enable(p_nxge_t);
+nxge_status_t nxge_tx_mac_disable(p_nxge_t);
+nxge_status_t nxge_rx_mac_enable(p_nxge_t);
+nxge_status_t nxge_rx_mac_disable(p_nxge_t);
+nxge_status_t nxge_tx_mac_reset(p_nxge_t);
+nxge_status_t nxge_rx_mac_reset(p_nxge_t);
+nxge_status_t nxge_link_intr(p_nxge_t nxgep, link_intr_enable_t enable);
+nxge_status_t nxge_link_monitor(p_nxge_t nxgep, link_mon_enable_t);
+nxge_status_t nxge_mii_xcvr_init(p_nxge_t nxgep);
+nxge_status_t nxge_mii_xcvr_fiber_init(p_nxge_t);
+nxge_status_t nxge_mii_read(p_nxge_t nxgep, uint8_t xcvr_portn, uint8_t xcvr_reg,
+			uint16_t *value);
+nxge_status_t nxge_mii_write(p_nxge_t nxgep, uint8_t xcvr_portn, uint8_t xcvr_reg,
+			uint16_t value);
+nxge_status_t nxge_mdio_read(p_nxge_t nxgep, uint8_t xcvr_portn, uint8_t device,
+			uint16_t xcvr_reg, uint16_t *value);
+nxge_status_t nxge_mdio_write(p_nxge_t nxgep, uint8_t xcvr_portn, uint8_t device,
+			uint16_t xcvr_reg, uint16_t value);
+nxge_status_t nxge_mii_check(p_nxge_t, mii_bmsr_t bmsr, mii_bmsr_t bmsr_ints);
+nxge_status_t nxge_pcs_check(p_nxge_t, uint8_t, nxge_link_state_t *);
+nxge_status_t nxge_check_mii_link(p_nxge_t nxgep);
+nxge_status_t nxge_check_10g_link(p_nxge_t nxgep);
+nxge_status_t nxge_check_serdes_link(p_nxge_t nxgep);
+void nxge_link_is_down(p_nxge_t);
+void nxge_link_is_up(p_nxge_t);
+uint32_t crc32_mchash(p_ether_addr_t);
+nxge_status_t nxge_mac_handle_sys_errors(p_nxge_t nxgep);
+nxge_status_t nxge_check_bcm8704_link(p_nxge_t, boolean_t *);
+nxge_status_t nxge_10g_link_led_on(p_nxge_t nxgep);
+nxge_status_t nxge_10g_link_led_off(p_nxge_t nxgep);
+void nxge_clear_mac_counters(p_nxge_t);
+nxge_status_t nxge_enable_address_filter(p_nxge_t, uint8_t, boolean_t);
+
+/* IPP prototypes */
+nxge_status_t nxge_ipp_init(p_nxge_t);
+nxge_status_t nxge_ipp_disable(p_nxge_t nxgep);
+nxge_status_t nxge_ipp_handle_sys_errors(p_nxge_t nxgep);
+
+/* FZC prototypes */
+nxge_status_t nxge_fzc_intr_init(p_nxge_t nxgep);
+nxge_status_t nxge_fzc_intr_ldg_num_set(p_nxge_t nxgep);
+nxge_status_t nxge_fzc_intr_tmres_set(p_nxge_t nxgep);
+nxge_status_t nxge_fzc_intr_sid_set(p_nxge_t nxgep);
+
+nxge_status_t nxge_test_and_set(p_nxge_t, uint8_t);
+nxge_status_t nxge_set_fzc_multi_part_ctl(p_nxge_t, boolean_t);
+nxge_status_t nxge_get_fzc_multi_part_ctl(p_nxge_t, boolean_t *);
+
+nxge_status_t nxge_init_fzc_rxdma_channel(p_nxge_t, uint16_t, p_rx_rbr_ring_t,
+				p_rx_rcr_ring_t, p_rx_mbox_t);
+nxge_status_t nxge_init_fzc_rxdma_channel_pages(p_nxge_t, uint16_t, p_rx_rbr_ring_t);
+nxge_status_t nxge_init_fzc_rxdma_channel_red(p_nxge_t, uint16_t, p_rx_rcr_ring_t);
+nxge_status_t nxge_init_fzc_rxdma_channel_clrlog(p_nxge_t, uint16_t, p_rx_rbr_ring_t);
+void nxge_init_fzc_rxdma_channels(p_nxge_t);
+
+nxge_status_t nxge_init_fzc_txdma_channel(p_nxge_t, uint16_t, p_tx_ring_t, p_tx_mbox_t);
+nxge_status_t nxge_init_fzc_txdma_channel_pages(p_nxge_t, uint16_t, p_tx_ring_t);
+nxge_status_t nxge_init_fzc_txdma_channel_drr(p_nxge_t, uint16_t, p_tx_ring_t);
+void nxge_init_fzc_txdma_channels(p_nxge_t);
+
+nxge_status_t nxge_init_fzc_common(p_nxge_t);
+nxge_status_t nxge_init_fzc_rx_common(p_nxge_t);
+nxge_status_t nxge_init_fzc_rdc_tbl(p_nxge_t);
+nxge_status_t nxge_init_fzc_rxdma_port(p_nxge_t);
+nxge_status_t nxge_init_fzc_mac_rdctbl(p_nxge_t);
+int nxge_init_fzc_vlan_rdctbl(p_nxge_t);
+void nxge_init_fzc_def_pt_rdc(p_nxge_t);
+nxge_status_t nxge_init_fzc_tx_common(p_nxge_t nxgep);
+nxge_status_t nxge_init_fzc_txdma_port(p_nxge_t nxgep);
+nxge_status_t nxge_fzc_dmc_rxdma_ck_div(p_nxge_t nxgep, uint16_t cnt, uint8_t timeout);
+nxge_status_t nxge_fzc_dmc_def_port_rdc(p_nxge_t nxgep, uint8_t port, uint16_t rdc);
+nxge_status_t nxge_fzc_dmc_rx_addr_md(p_nxge_t nxgep, boolean_t mode32);
+
+nxge_status_t nxge_fzc_dmc_rx_log_page_vld(p_nxge_t nxgep, uint16_t rdc,
+				 uint32_t page, boolean_t flag);
+nxge_status_t nxge_fzc_dmc_rx_log_page_mask(p_nxge_t nxgep, uint16_t rdc, uint32_t page,
+				  uint32_t mask, uint32_t value);
+nxge_status_t nxge_fzc_dmc_rx_log_page_relo(p_nxge_t nxgep, uint16_t rdc,
+				  uint32_t page, uint32_t relo);
+nxge_status_t nxge_fzc_dmc_rx_log_page_handle(p_nxge_t nxgep, uint16_t rdc,
+				    uint32_t page, uint32_t handle);
+nxge_status_t nxge_fzc_dmc_red_ran_init(p_nxge_t nxgep, uint16_t init_value);
+nxge_status_t nxge_fzc_dmc_rdc_red_para(p_nxge_t nxgep, uint16_t rdc, uint64_t param);
+nxge_status_t nxge_fzc_ipp_enet_cfifo(p_nxge_t nxgep, uint16_t port,
+			    uint16_t oflow, uint16_t count);
+nxge_status_t nxge_fzc_fflp_vlan_dis_cnt(p_nxge_t nxgep, uint16_t port,
+			       uint16_t oflow, uint16_t count);
+nxge_status_t nxge_fzc_ipp_eing_dfifo_disc(p_nxge_t nxgep, uint16_t port,
+				 uint16_t oflow, uint16_t count);
+
+nxge_status_t nxge_fzc_sys_err_mask_set(p_nxge_t nxgep, boolean_t mask);
+
+/* nxge_param.c */
+int nxge_set_tx_lb_policy(int);
+int nxge_get_tx_lb_policy(p_nxge_t);
+void nxge_init_tx_lb_policy(p_nxge_t);
+void nxge_get_param_int_arr(nxge_arr_param_index_t, int *, int **);
+int nxge_cfg_set_dma_cfg(p_nxge_t nxgep);
+int nxge_cfg_set_quick_config(p_nxge_t nxgep);
+void nxge_get_param_soft_properties(p_nxge_t);
+void nxge_copy_hw_default_to_param(p_nxge_t);
+void nxge_copy_param_hw_to_config(p_nxge_t);
+void nxge_setup_param(p_nxge_t);
+void nxge_init_param(p_nxge_t);
+void nxge_destroy_param(p_nxge_t);
+boolean_t nxge_check_rxdma_rdcgrp_member(p_nxge_t, uint8_t, uint8_t);
+boolean_t nxge_check_rxdma_port_member(p_nxge_t, uint8_t);
+boolean_t nxge_check_rdcrgp_port_member(p_nxge_t, uint8_t);
+boolean_t nxge_check_txdma_port_member(p_nxge_t, uint8_t);
+
+int nxge_param_get_generic(p_nxge_t, caddr_t, uint64_t *, uint8_t *);
+int nxge_param_set_generic(p_nxge_t, uint64_t, caddr_t);
+int nxge_set_public_param(p_nxge_t, char *, uint64_t);
+int nxge_get_public_param(p_nxge_t, char *, uint64_t *, uint8_t *);
+void nxge_get_public_param_list_len(p_nxge_t, uint64_t *);
+void nxge_get_public_param_list(p_nxge_t, uint8_t *);
+
+/* nxge_espc.c */
+nxge_status_t nxge_espc_verify_chksum(p_nxge_t nxgep);
+nxge_status_t nxge_espc_mac_addrs_get(p_nxge_t nxgep);
+nxge_status_t nxge_espc_num_ports_get(p_nxge_t nxgep);
+nxge_status_t nxge_espc_phy_type_get(p_nxge_t nxgep);
+nxge_status_t nxge_espc_max_frame_sz_get(p_nxge_t nxgep);
+void nxge_espc_get_next_mac_addr(uint8_t*, uint8_t, struct ether_addr*);
+nxge_status_t nxge_vpd_info_get(p_nxge_t);
+
+#endif	/* __NXGE_IMPL_H */
diff --git a/drivers/net/nxge/include/nxge_ipp.h b/drivers/net/nxge/include/nxge_ipp.h
new file mode 100644
index 0000000..fedfac6
--- /dev/null
+++ b/drivers/net/nxge/include/nxge_ipp.h
@@ -0,0 +1,94 @@
+/*
+ * nxge_ipp.h	Neptune Input Port Processing interface
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef _SYS_NXGE_NXGE_IPP_H
+#define	_SYS_NXGE_NXGE_IPP_H
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#include <nxge_ipp_hw.h>
+#include <npi_ipp.h>
+
+#define	IPP_MAX_PKT_SIZE	0x1FFFF
+#define	IPP_MAX_ERR_SHOW	10
+
+typedef	struct _ipp_errlog {
+	boolean_t		multiple_err;
+	uint16_t		dfifo_rd_ptr;
+	uint32_t		state_mach;
+	uint16_t		ecc_syndrome;
+} ipp_errlog_t, *p_ipp_errlog_t;
+
+typedef struct _nxge_ipp_stats {
+	uint32_t 		errors;
+	uint32_t 		inits;
+	uint32_t 		sop_miss;
+	uint32_t 		eop_miss;
+	uint32_t 		dfifo_ue;
+	uint32_t 		ecc_err_cnt;
+	uint32_t 		pfifo_perr;
+	uint32_t 		pfifo_over;
+	uint32_t 		pfifo_und;
+	uint32_t 		bad_cs_cnt;
+	uint32_t 		pkt_dis_cnt;
+	ipp_errlog_t		errlog;
+} nxge_ipp_stats_t, *p_nxge_ipp_stats_t;
+
+typedef	struct _nxge_ipp {
+	uint32_t		config;
+	uint32_t		iconfig;
+	ipp_status_t		status;
+	uint32_t		max_pkt_size;
+	nxge_ipp_stats_t	*stat;
+} nxge_ipp_t;
+
+/* IPP prototypes */
+nxge_status_t nxge_ipp_reset(p_nxge_t nxgep);
+nxge_status_t nxge_ipp_init(p_nxge_t nxgep);
+nxge_status_t nxge_ipp_disable(p_nxge_t nxgep);
+nxge_status_t nxge_ipp_handle_sys_errors(p_nxge_t nxgep);
+nxge_status_t nxge_ipp_fatal_err_recover(p_nxge_t nxgep);
+void nxge_ipp_inject_err(p_nxge_t nxgep, uint32_t err_id);
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif	/* _SYS_NXGE_NXGE_IPP_H */
diff --git a/drivers/net/nxge/include/nxge_ipp_hw.h b/drivers/net/nxge/include/nxge_ipp_hw.h
new file mode 100644
index 0000000..47d2585
--- /dev/null
+++ b/drivers/net/nxge/include/nxge_ipp_hw.h
@@ -0,0 +1,262 @@
+/*
+ * nxge_ipp_hw.h	Neptune IPP HW offsets and registers
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef _SYS_NXGE_NXGE_IPP_HW_H
+#define	_SYS_NXGE_NXGE_IPP_HW_H
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#include <nxge_defs.h>
+
+/* IPP Registers */
+#define	IPP_CONFIG_REG				0x000
+#define	IPP_DISCARD_PKT_CNT_REG			0x020
+#define	IPP_TCP_CKSUM_ERR_CNT_REG		0x028
+#define	IPP_ECC_ERR_COUNTER_REG			0x030
+#define	IPP_INT_STATUS_REG			0x040
+#define	IPP_INT_MASK_REG			0x048
+
+#define	IPP_PFIFO_RD_DATA0_REG			0x060
+#define	IPP_PFIFO_RD_DATA1_REG			0x068
+#define	IPP_PFIFO_RD_DATA2_REG			0x070
+#define	IPP_PFIFO_RD_DATA3_REG			0x078
+#define	IPP_PFIFO_RD_DATA4_REG			0x080
+#define	IPP_PFIFO_WR_DATA0_REG			0x088
+#define	IPP_PFIFO_WR_DATA1_REG			0x090
+#define	IPP_PFIFO_WR_DATA2_REG			0x098
+#define	IPP_PFIFO_WR_DATA3_REG			0x0a0
+#define	IPP_PFIFO_WR_DATA4_REG			0x0a8
+#define	IPP_PFIFO_RD_PTR_REG			0x0b0
+#define	IPP_PFIFO_WR_PTR_REG			0x0b8
+#define	IPP_DFIFO_RD_DATA0_REG			0x0c0
+#define	IPP_DFIFO_RD_DATA1_REG			0x0c8
+#define	IPP_DFIFO_RD_DATA2_REG			0x0d0
+#define	IPP_DFIFO_RD_DATA3_REG			0x0d8
+#define	IPP_DFIFO_RD_DATA4_REG			0x0e0
+#define	IPP_DFIFO_WR_DATA0_REG			0x0e8
+#define	IPP_DFIFO_WR_DATA1_REG			0x0f0
+#define	IPP_DFIFO_WR_DATA2_REG			0x0f8
+#define	IPP_DFIFO_WR_DATA3_REG			0x100
+#define	IPP_DFIFO_WR_DATA4_REG			0x108
+#define	IPP_DFIFO_RD_PTR_REG			0x110
+#define	IPP_DFIFO_WR_PTR_REG			0x118
+#define	IPP_STATE_MACHINE_REG			0x120
+#define	IPP_CKSUM_STATUS_REG			0x128
+#define	IPP_FFLP_CKSUM_INFO_REG			0x130
+#define	IPP_DEBUG_SELECT_REG			0x138
+#define	IPP_DFIFO_ECC_SYNDROME_REG		0x140
+#define	IPP_DFIFO_EOPM_RD_PTR_REG		0x148
+#define	IPP_ECC_CTRL_REG			0x150
+
+#define	IPP_PORT_OFFSET				0x4000
+#define	IPP_PORT0_OFFSET			0
+#define	IPP_PORT1_OFFSET			0x8000
+#define	IPP_PORT2_OFFSET			0x4000
+#define	IPP_PORT3_OFFSET			0xc000
+#define	IPP_REG_ADDR(port_num, reg)\
+	((port_num == 0) ? FZC_IPP + reg : \
+	FZC_IPP + reg + (((port_num % 2) * IPP_PORT_OFFSET) + \
+	((port_num / 3) * IPP_PORT_OFFSET) + IPP_PORT_OFFSET))
+#define	IPP_PORT_ADDR(port_num)\
+	((port_num == 0) ? FZC_IPP: \
+	FZC_IPP + (((port_num % 2) * IPP_PORT_OFFSET) + \
+	((port_num / 3) * IPP_PORT_OFFSET) + IPP_PORT_OFFSET))
+
+/* IPP Configuration Register */
+
+#define	IPP_SOFT_RESET				(1ULL << 31)
+#define	IPP_IP_MAX_PKT_BYTES_SHIFT		8
+#define	IPP_IP_MAX_PKT_BYTES_MASK		0x1FFFF
+#define	IPP_FFLP_CKSUM_INFO_PIO_WR_EN		(1 << 7)
+#define	IPP_PRE_FIFO_PIO_WR_EN			(1 << 6)
+#define	IPP_DFIFO_PIO_WR_EN			(1 << 5)
+#define	IPP_TCP_UDP_CKSUM_EN			(1 << 4)
+#define	IPP_DROP_BAD_CRC_EN			(1 << 3)
+#define	IPP_DFIFO_ECC_CORRECT_EN		(1 << 2)
+#define	IPP_EN					(1 << 0)
+
+/* IPP Interrupt Status Registers */
+
+#define	IPP_DFIFO_MISSED_SOP			(1ULL << 31)
+#define	IPP_DFIFO_MISSED_EOP			(1 << 30)
+#define	IPP_DFIFO_ECC_UNCORR_ERR_MASK		0x3
+#define	IPP_DFIFO_ECC_UNCORR_ERR_SHIFT		28
+#define	IPP_DFIFO_ECC_CORR_ERR_MASK		0x3
+#define	IPP_DFIFO_ECC_CORR_ERR_SHIFT		26
+#define	IPP_DFIFO_ECC_ERR_MASK			0x3
+#define	IPP_DFIFO_ECC_ERR_SHIFT			24
+#define	IPP_DFIFO_NO_ECC_ERR			(1 << 23)
+#define	IPP_DFIFO_ECC_ERR_ENTRY_INDEX_MASK	0x7FF
+#define	IPP_DFIFO_ECC_ERR_ENTRY_INDEX_SHIFT	12
+#define	IPP_PRE_FIFO_PERR			(1 << 11)
+#define	IPP_ECC_ERR_CNT_MAX			(1 << 10)
+#define	IPP_PRE_FIFO_PERR_ENTRY_INDEX_MASK	0x3F
+#define	IPP_PRE_FIFO_PERR_ENTRY_INDEX_SHIFT	4
+#define	IPP_PRE_FIFO_OVERRUN			(1 << 3)
+#define	IPP_PRE_FIFO_UNDERRUN			(1 << 2)
+#define	IPP_BAD_TCPIP_CHKSUM_CNT_MAX		(1 << 1)
+#define	IPP_PKT_DISCARD_CNT_MAX			(1 << 0)
+
+#define	IPP_P0_P1_DFIFO_ENTRIES			2048
+#define	IPP_P2_P3_DFIFO_ENTRIES			1024
+#define	IPP_NIU_DFIFO_ENTRIES			1024
+
+typedef	union _ipp_status {
+	uint64_t value;
+
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t	w1;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t dfifo_missed_sop	: 1;
+		uint32_t dfifo_missed_eop	: 1;
+		uint32_t dfifo_uncorr_ecc_err	: 2;
+		uint32_t dfifo_corr_ecc_err	: 2;
+		uint32_t dfifo_ecc_err		: 2;
+		uint32_t dfifo_no_ecc_err	: 1;
+		uint32_t dfifo_ecc_err_idx	: 11;
+		uint32_t pre_fifo_perr		: 1;
+		uint32_t ecc_err_cnt_ovfl	: 1;
+		uint32_t pre_fifo_perr_idx	: 6;
+		uint32_t pre_fifo_overrun	: 1;
+		uint32_t pre_fifo_underrun	: 1;
+		uint32_t bad_cksum_cnt_ovfl	: 1;
+		uint32_t pkt_discard_cnt_ovfl	: 1;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t pkt_discard_cnt_ovfl	: 1;
+		uint32_t bad_cksum_cnt_ovfl	: 1;
+		uint32_t pre_fifo_underrun	: 1;
+		uint32_t pre_fifo_overrun	: 1;
+		uint32_t pre_fifo_perr_idx	: 6;
+		uint32_t ecc_err_cnt_ovfl	: 1;
+		uint32_t pre_fifo_perr		: 1;
+		uint32_t dfifo_ecc_err_idx	: 11;
+		uint32_t dfifo_no_ecc_err	: 1;
+		uint32_t dfifo_ecc_err		: 2;
+		uint32_t dfifo_corr_ecc_err	: 2;
+		uint32_t dfifo_uncorr_ecc_err	: 2;
+		uint32_t dfifo_missed_eop	: 1;
+		uint32_t dfifo_missed_sop	: 1;
+#else
+#error	one of _BIT_FIELDS_HTOL or _BIT_FIELDS_LTOH must be defined
+#endif
+	} w0;
+
+#if !defined(_BIG_ENDIAN)
+		uint32_t	w1;
+#endif
+	} bits;
+} ipp_status_t;
+
+typedef	union _ipp_ecc_ctrl {
+	uint64_t value;
+
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t	w1;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t dis_dbl	: 1;
+		uint32_t res3		: 13;
+		uint32_t cor_dbl	: 1;
+		uint32_t cor_sng	: 1;
+		uint32_t rsvd		: 5;
+		uint32_t cor_all	: 1;
+		uint32_t res2		: 1;
+		uint32_t cor_1		: 1;
+		uint32_t res1		: 5;
+		uint32_t cor_lst	: 1;
+		uint32_t cor_snd	: 1;
+		uint32_t cor_fst	: 1;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t cor_fst	: 1;
+		uint32_t cor_snd	: 1;
+		uint32_t cor_lst	: 1;
+		uint32_t res1		: 5;
+		uint32_t cor_1		: 1;
+		uint32_t res2		: 1;
+		uint32_t cor_all	: 1;
+		uint32_t rsvd		: 5;
+		uint32_t cor_sng	: 1;
+		uint32_t cor_dbl	: 1;
+		uint32_t res3		: 13;
+		uint32_t dis_dbl	: 1;
+#else
+#error	one of _BIT_FIELDS_HTOL or _BIT_FIELDS_LTOH must be defined
+#endif
+	} w0;
+
+#if !defined(_BIG_ENDIAN)
+		uint32_t	w1;
+#endif
+	} bits;
+} ipp_ecc_ctrl_t;
+
+
+/* IPP Interrupt Mask Registers */
+
+#define	IPP_ECC_ERR_CNT_MAX_INTR_DIS		(1 << 7)
+#define	IPP_DFIFO_MISSING_EOP_SOP_INTR_DIS	(1 << 6)
+#define	IPP_DFIFO_ECC_UNCORR_ERR_INTR_DIS	(1 << 5)
+#define	IPP_PRE_FIFO_PERR_INTR_DIS		(1 << 4)
+#define	IPP_PRE_FIFO_OVERRUN_INTR_DIS		(1 << 3)
+#define	IPP_PRE_FIFO_UNDERRUN_INTR_DIS		(1 << 2)
+#define	IPP_BAD_TCPIP_CKSUM_CNT_INTR_DIS	(1 << 1)
+#define	IPP_PKT_DISCARD_CNT_INTR_DIS		(1 << 0)
+
+#define	IPP_RESET_WAIT				10
+
+/* DFIFO RD/WR pointers mask */
+
+#define	IPP_XMAC_DFIFO_PTR_MASK			0x7FF
+#define	IPP_BMAC_DFIFO_PTR_MASK			0x3FF
+
+#define	IPP_ECC_CNT_MASK			0xFF
+#define	IPP_BAD_CS_CNT_MASK			0x3FFF
+#define	IPP_PKT_DIS_CNT_MASK			0x3FFF
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif	/* _SYS_NXGE_NXGE_IPP_HW_H */
diff --git a/drivers/net/nxge/include/nxge_mac.h b/drivers/net/nxge/include/nxge_mac.h
new file mode 100644
index 0000000..7f1160f
--- /dev/null
+++ b/drivers/net/nxge/include/nxge_mac.h
@@ -0,0 +1,256 @@
+/*
+ * nxge_mac.h	Neptune MAC interface
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef	_SYS_NXGE_NXGE_MAC_H
+#define	_SYS_NXGE_NXGE_MAC_H
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#include <nxge_mac_hw.h>
+#include <npi_mac.h>
+
+#define	NXGE_XMAC_TX_INTRS	(ICFG_XMAC_TX_ALL & \
+					~(ICFG_XMAC_TX_FRAME_XMIT |\
+					ICFG_XMAC_TX_BYTE_CNT_EXP |\
+					ICFG_XMAC_TX_FRAME_CNT_EXP))
+
+#define	NXGE_XMAC_RX_INTRS	(ICFG_XMAC_RX_ALL & \
+					~(ICFG_XMAC_RX_FRAME_RCVD |\
+					ICFG_XMAC_RX_OCT_CNT_EXP |\
+					ICFG_XMAC_RX_HST_CNT1_EXP |\
+					ICFG_XMAC_RX_HST_CNT2_EXP |\
+					ICFG_XMAC_RX_HST_CNT3_EXP |\
+					ICFG_XMAC_RX_HST_CNT4_EXP |\
+					ICFG_XMAC_RX_HST_CNT5_EXP |\
+					ICFG_XMAC_RX_HST_CNT6_EXP |\
+					ICFG_XMAC_RX_BCAST_CNT_EXP |\
+					ICFG_XMAC_RX_MCAST_CNT_EXP |\
+ICFG_XMAC_RX_CRC_ERR_CNT_EXP | \
+					ICFG_XMAC_RX_HST_CNT7_EXP))
+
+#define	NXGE_BMAC_TX_INTRS	(ICFG_BMAC_TX_ALL & \
+					~(ICFG_BMAC_TX_FRAME_SENT |\
+					ICFG_BMAC_TX_BYTE_CNT_EXP |\
+					ICFG_BMAC_TX_FRAME_CNT_EXP))
+#define	NXGE_BMAC_RX_INTRS	(ICFG_BMAC_RX_ALL & \
+					~(ICFG_BMAC_RX_FRAME_RCVD |\
+					ICFG_BMAC_RX_FRAME_CNT_EXP |\
+					ICFG_BMAC_RX_BYTE_CNT_EXP))
+
+typedef enum  {
+	LINK_NO_CHANGE,
+	LINK_IS_UP,
+	LINK_IS_DOWN
+} nxge_link_state_t;
+
+
+/* Common MAC statistics */
+
+typedef	struct _nxge_mac_stats {
+	/*
+	 * MTU size
+	 */
+	uint32_t	mac_mtu;
+	uint16_t	rev_id;
+
+	/*
+	 * Transciever state informations.
+	 */
+	uint32_t	xcvr_inits;
+	xcvr_inuse_t	xcvr_inuse;
+/*	uint32_t	xcvr_portn; */
+	uint8_t	xcvr_portn;
+	uint32_t	xcvr_id;
+	uint32_t	serdes_inits;
+	uint32_t	serdes_portn;
+	uint32_t	cap_autoneg;
+	uint32_t	cap_10gfdx;
+	uint32_t	cap_10ghdx;
+	uint32_t	cap_1000fdx;
+	uint32_t	cap_1000hdx;
+	uint32_t	cap_100T4;
+	uint32_t	cap_100fdx;
+	uint32_t	cap_100hdx;
+	uint32_t	cap_10fdx;
+	uint32_t	cap_10hdx;
+	uint32_t	cap_asmpause;
+	uint32_t	cap_pause;
+
+	/*
+	 * Advertised capabilities.
+	 */
+	uint32_t	adv_cap_autoneg;
+	uint32_t	adv_cap_10gfdx;
+	uint32_t	adv_cap_10ghdx;
+	uint32_t	adv_cap_1000fdx;
+	uint32_t	adv_cap_1000hdx;
+	uint32_t	adv_cap_100T4;
+	uint32_t	adv_cap_100fdx;
+	uint32_t	adv_cap_100hdx;
+	uint32_t	adv_cap_10fdx;
+	uint32_t	adv_cap_10hdx;
+	uint32_t	adv_cap_asmpause;
+	uint32_t	adv_cap_pause;
+
+	/*
+	 * Link partner capabilities.
+	 */
+	uint32_t	lp_cap_autoneg;
+	uint32_t	lp_cap_10gfdx;
+	uint32_t	lp_cap_10ghdx;
+	uint32_t	lp_cap_1000fdx;
+	uint32_t	lp_cap_1000hdx;
+	uint32_t	lp_cap_100T4;
+	uint32_t	lp_cap_100fdx;
+	uint32_t	lp_cap_100hdx;
+	uint32_t	lp_cap_10fdx;
+	uint32_t	lp_cap_10hdx;
+	uint32_t	lp_cap_asmpause;
+	uint32_t	lp_cap_pause;
+
+	/*
+	 * Physical link statistics.
+	 */
+	uint32_t	link_T4;
+	uint32_t	link_speed;
+	uint32_t	link_duplex;
+	uint32_t	link_asmpause;
+	uint32_t	link_pause;
+	uint32_t	link_up;
+} nxge_mac_stats_t;
+
+/* XMAC Statistics */
+
+typedef	struct _nxge_xmac_stats {
+	uint32_t tx_frame_cnt;
+	uint32_t tx_underflow_err;
+	uint32_t tx_maxpktsize_err;
+	uint32_t tx_overflow_err;
+	uint32_t tx_fifo_xfr_err;
+	uint64_t tx_byte_cnt;
+	uint32_t rx_frame_cnt;
+	uint32_t rx_underflow_err;
+	uint32_t rx_overflow_err;
+	uint32_t rx_crc_err_cnt;
+	uint32_t rx_len_err_cnt;
+	uint32_t rx_viol_err_cnt;
+	uint64_t rx_byte_cnt;
+	uint64_t rx_hist1_cnt;
+	uint64_t rx_hist2_cnt;
+	uint64_t rx_hist3_cnt;
+	uint64_t rx_hist4_cnt;
+	uint64_t rx_hist5_cnt;
+	uint64_t rx_hist6_cnt;
+	uint64_t rx_hist7_cnt;
+	uint64_t rx_broadcast_cnt;
+	uint64_t rx_mult_cnt;
+	uint32_t rx_frag_cnt;
+	uint32_t rx_frame_align_err_cnt;
+	uint32_t rx_linkfault_err_cnt;
+	uint32_t rx_remotefault_err;
+	uint32_t rx_localfault_err;
+	uint32_t rx_pause_cnt;
+	uint32_t tx_pause_state;
+	uint32_t tx_nopause_state;
+	uint32_t xpcs_deskew_err_cnt;
+	uint32_t xpcs_ln0_symbol_err_cnt;
+	uint32_t xpcs_ln1_symbol_err_cnt;
+	uint32_t xpcs_ln2_symbol_err_cnt;
+	uint32_t xpcs_ln3_symbol_err_cnt;
+} nxge_xmac_stats_t, *p_nxge_xmac_stats_t;
+
+/* BMAC Statistics */
+
+typedef	struct _nxge_bmac_stats {
+	uint64_t tx_frame_cnt;
+	uint32_t tx_underrun_err;
+	uint32_t tx_max_pkt_err;
+	uint64_t tx_byte_cnt;
+	uint64_t rx_frame_cnt;
+	uint64_t rx_byte_cnt;
+	uint32_t rx_overflow_err;
+	uint32_t rx_align_err_cnt;
+	uint32_t rx_crc_err_cnt;
+	uint32_t rx_len_err_cnt;
+	uint32_t rx_viol_err_cnt;
+	uint32_t rx_pause_cnt;
+	uint32_t tx_pause_state;
+	uint32_t tx_nopause_state;
+} nxge_bmac_stats_t, *p_nxge_bmac_stats_t;
+
+typedef struct _hash_filter_t {
+	uint_t hash_ref_cnt;
+	uint16_t hash_filter_regs[NMCFILTER_REGS];
+	uint32_t hash_bit_ref_cnt[NMCFILTER_BITS];
+} hash_filter_t, *p_hash_filter_t;
+
+typedef	struct _nxge_mac {
+	uint8_t			portnum;
+	nxge_port_t		porttype;
+	nxge_port_mode_t	portmode;
+	nxge_linkchk_mode_t	linkchkmode;
+	boolean_t		is_jumbo;
+	uint32_t		tx_config;
+	uint32_t		rx_config;
+	uint32_t		xif_config;
+	uint32_t		tx_iconfig;
+	uint32_t		rx_iconfig;
+	uint32_t		ctl_iconfig;
+	uint16_t		minframesize;
+	uint16_t		maxframesize;
+	uint16_t		maxburstsize;
+	uint16_t		ctrltype;
+	uint16_t		pa_size;
+	uint8_t			ipg[3];
+	struct ether_addr	mac_addr;
+	struct ether_addr	alt_mac_addr[MAC_MAX_ALT_ADDR_ENTRY];
+	struct ether_addr	mac_addr_filter;
+	uint16_t		hashtab[MAC_MAX_HASH_ENTRY];
+	hostinfo_t		hostinfo[MAC_MAX_HOST_INFO_ENTRY];
+	nxge_mac_stats_t	*mac_stats;
+	nxge_xmac_stats_t	*xmac_stats;
+	nxge_bmac_stats_t	*bmac_stats;
+} nxge_mac_t;
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif	/* _SYS_NXGE_NXGE_MAC_H */
diff --git a/drivers/net/nxge/include/nxge_mac_hw.h b/drivers/net/nxge/include/nxge_mac_hw.h
new file mode 100644
index 0000000..b029957
--- /dev/null
+++ b/drivers/net/nxge/include/nxge_mac_hw.h
@@ -0,0 +1,2452 @@
+/*
+ * nxge_mac_hw.h	Neptune MAC HW offsets and registers
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef	_SYS_MAC_NXGE_MAC_HW_H
+#define	_SYS_MAC_NXGE_MAC_HW_H
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#include <nxge_defs.h>
+
+
+#define	NXGE_1GETHERMIN			255
+#define	NXGE_ETHERMIN			97
+#define	NXGE_MAX_HEADER			250
+
+/* Hardware reset */
+typedef enum  {
+	NXGE_TX_DISABLE,			/* Disable Tx side */
+	NXGE_RX_DISABLE,			/* Disable Rx side */
+	NXGE_CHIP_RESET				/* Full chip reset */
+} nxge_reset_t;
+
+#define	NXGE_DELAY_AFTER_TXRX		10000	/* 10ms after idling rx/tx */
+#define	NXGE_DELAY_AFTER_RESET		1000	/* 1ms after the reset */
+#define	NXGE_DELAY_AFTER_EE_RESET	10000	/* 10ms after EEPROM reset */
+#define	NXGE_DELAY_AFTER_LINK_RESET	13	/* 13 Us after link reset */
+#define	NXGE_LINK_RESETS		8	/* Max PHY resets to wait for */
+						/* linkup */
+
+#define	FILTER_M_CTL 			0xDCEF1
+#define	HASH_BITS			8
+#define	NMCFILTER_BITS			(1 << HASH_BITS)
+#define	HASH_REG_WIDTH			16
+#define	BROADCAST_HASH_WORD		0x0f
+#define	BROADCAST_HASH_BIT		0x8000
+#define	NMCFILTER_REGS			NMCFILTER_BITS / HASH_REG_WIDTH
+					/* Number of multicast filter regs */
+
+#define	XMAC_PORT_0			0
+#define	XMAC_PORT_1			1
+#define	BMAC_PORT_0			2
+#define	BMAC_PORT_1			3
+
+#define	MAC_RESET_WAIT			10	/* usecs */
+
+#define	MAC_ADDR_REG_MASK		0xFFFF
+
+/* Network Modes */
+
+typedef enum nxge_network_mode {
+	NET_2_10GE_FIBER = 1,
+	NET_2_10GE_COPPER,
+	NET_1_10GE_FIBER_3_1GE_COPPER,
+	NET_1_10GE_COPPER_3_1GE_COPPER,
+	NET_1_10GE_FIBER_3_1GE_FIBER,
+	NET_1_10GE_COPPER_3_1GE_FIBER,
+	NET_2_1GE_FIBER_2_1GE_COPPER,
+	NET_QGE_FIBER,
+	NET_QGE_COPPER
+} nxge_network_mode_t;
+
+typedef	enum nxge_port {
+	PORT_TYPE_XMAC = 1,
+	PORT_TYPE_BMAC
+} nxge_port_t;
+
+typedef	enum nxge_port_mode {
+	PORT_1G_COPPER = 1,
+	PORT_1G_FIBER,
+	PORT_10G_COPPER,
+	PORT_10G_FIBER,
+	PORT_10G_SERDES,
+	PORT_1G_SERDES,
+	PORT_1G_RGMII_FIBER
+} nxge_port_mode_t;
+
+typedef	enum nxge_linkchk_mode {
+	LINKCHK_INTR = 1,
+	LINKCHK_TIMER
+} nxge_linkchk_mode_t;
+
+typedef enum {
+	LINK_INTR_STOP,
+	LINK_INTR_START
+} link_intr_enable_t, *link_intr_enable_pt;
+
+typedef	enum {
+	LINK_MONITOR_STOP,
+	LINK_MONITOR_START
+} link_mon_enable_t, *link_mon_enable_pt;
+
+typedef enum {
+	NO_XCVR,
+	INT_MII_XCVR,
+	EXT_MII_XCVR,
+	PCS_XCVR,
+	XPCS_XCVR
+} xcvr_inuse_t;
+
+#define NXGE_ATCA_PORT0_1G 0x01
+#define NXGE_ATCA_PORT1_1G 0x02
+#define NXGE_ATCA_PORT0_10G 0x00
+#define NXGE_ATCA_PORT1_10G 0x00
+
+
+/* macros for port offset calculations */
+
+#define	PORT_1_OFFSET			0x6000
+#define	PORT_GT_1_OFFSET		0x4000
+
+/* XMAC address macros */
+
+#define	XMAC_ADDR_OFFSET_0		0
+#define	XMAC_ADDR_OFFSET_1		0x6000
+
+#define	XMAC_ADDR_OFFSET(port_num)\
+	(XMAC_ADDR_OFFSET_0 + ((port_num) * PORT_1_OFFSET))
+
+#define	XMAC_REG_ADDR(port_num, reg)\
+	(FZC_MAC + (XMAC_ADDR_OFFSET(port_num)) + (reg))
+
+#define	XMAC_PORT_ADDR(port_num)\
+	(FZC_MAC + XMAC_ADDR_OFFSET(port_num))
+
+/* BMAC address macros */
+
+#define	BMAC_ADDR_OFFSET_2		0x0C000
+#define	BMAC_ADDR_OFFSET_3		0x10000
+
+#define	BMAC_ADDR_OFFSET(port_num)\
+	(BMAC_ADDR_OFFSET_2 + (((port_num) - 2) * PORT_GT_1_OFFSET))
+
+#define	BMAC_REG_ADDR(port_num, reg)\
+	(FZC_MAC + (BMAC_ADDR_OFFSET(port_num)) + (reg))
+
+#define	BMAC_PORT_ADDR(port_num)\
+	(FZC_MAC + BMAC_ADDR_OFFSET(port_num))
+
+/* PCS address macros */
+
+#define	PCS_ADDR_OFFSET_0		0x04000
+#define	PCS_ADDR_OFFSET_1		0x0A000
+#define	PCS_ADDR_OFFSET_2		0x0E000
+#define	PCS_ADDR_OFFSET_3		0x12000
+
+#define	PCS_ADDR_OFFSET(port_num)\
+	((port_num <= 1) ? \
+	(PCS_ADDR_OFFSET_0 + (port_num) * PORT_1_OFFSET) : \
+	(PCS_ADDR_OFFSET_2 + (((port_num) - 2) * PORT_GT_1_OFFSET)))
+
+#define	PCS_REG_ADDR(port_num, reg)\
+	(FZC_MAC + (PCS_ADDR_OFFSET((port_num)) + (reg)))
+
+#define	PCS_PORT_ADDR(port_num)\
+	(FZC_MAC + (PCS_ADDR_OFFSET(port_num)))
+
+/* XPCS address macros */
+
+#define	XPCS_ADDR_OFFSET_0		0x02000
+#define	XPCS_ADDR_OFFSET_1		0x08000
+#define	XPCS_ADDR_OFFSET(port_num)\
+	(XPCS_ADDR_OFFSET_0 + ((port_num) * PORT_1_OFFSET))
+
+#define	XPCS_ADDR(port_num, reg)\
+	(FZC_MAC + (XPCS_ADDR_OFFSET((port_num)) + (reg)))
+
+#define	XPCS_PORT_ADDR(port_num)\
+	(FZC_MAC + (XPCS_ADDR_OFFSET(port_num)))
+
+/* ESR address macro */
+#define	ESR_ADDR_OFFSET		0x14000
+#define	ESR_ADDR(reg)\
+	(FZC_MAC + (ESR_ADDR_OFFSET) + (reg))
+
+/* MIF address macros */
+#define	MIF_ADDR_OFFSET		0x16000
+#define	MIF_ADDR(reg)\
+	(FZC_MAC + (MIF_ADDR_OFFSET) + (reg))
+
+/* BMAC registers offset */
+#define	BTXMAC_SW_RST_REG		0x000	/* TX MAC software reset */
+#define	BRXMAC_SW_RST_REG		0x008	/* RX MAC software reset */
+#define	MAC_SEND_PAUSE_REG		0x010	/* send pause command */
+#define	BTXMAC_STATUS_REG		0x020	/* TX MAC status */
+#define	BRXMAC_STATUS_REG		0x028	/* RX MAC status */
+#define	BMAC_CTRL_STAT_REG		0x030	/* MAC control status */
+#define	BTXMAC_STAT_MSK_REG		0x040	/* TX MAC mask */
+#define	BRXMAC_STAT_MSK_REG		0x048	/* RX MAC mask */
+#define	BMAC_C_S_MSK_REG		0x050	/* MAC control mask */
+#define	TXMAC_CONFIG_REG		0x060	/* TX MAC config */
+/* cfg register bitmap */
+
+typedef union _btxmac_config_t {
+	uint64_t value;
+
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t msw;	/* Most significant word */
+		uint32_t lsw;	/* Least significant word */
+#elif defined(_LITTLE_ENDIAN)
+		uint32_t lsw;	/* Least significant word */
+		uint32_t msw;	/* Most significant word */
+#endif
+	} val;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t	w1;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t rsrvd	: 22;
+			uint32_t hdx_ctrl2	: 1;
+			uint32_t no_fcs	: 1;
+			uint32_t hdx_ctrl	: 7;
+			uint32_t txmac_enable	: 1;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t txmac_enable	: 1;
+			uint32_t hdx_ctrl	: 7;
+			uint32_t no_fcs	: 1;
+			uint32_t hdx_ctrl2	: 1;
+			uint32_t rsrvd	: 22;
+#endif
+		} w0;
+
+#if defined(_LITTLE_ENDIAN)
+		uint32_t	w1;
+#endif
+	} bits;
+} btxmac_config_t, *p_btxmac_config_t;
+
+#define	RXMAC_CONFIG_REG		0x068	/* RX MAC config */
+
+typedef union _brxmac_config_t {
+	uint64_t value;
+
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t msw;	/* Most significant word */
+		uint32_t lsw;	/* Least significant word */
+#elif defined(_LITTLE_ENDIAN)
+		uint32_t lsw;	/* Least significant word */
+		uint32_t msw;	/* Most significant word */
+#endif
+	} val;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t	w1;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t rsrvd	: 20;
+			uint32_t mac_reg_sw_test : 2;
+			uint32_t mac2ipp_pkt_cnt_en : 1;
+			uint32_t rx_crs_extend_en : 1;
+			uint32_t error_chk_dis	: 1;
+			uint32_t addr_filter_en	: 1;
+			uint32_t hash_filter_en	: 1;
+			uint32_t promiscuous_group	: 1;
+			uint32_t promiscuous	: 1;
+			uint32_t strip_fcs	: 1;
+			uint32_t strip_pad	: 1;
+			uint32_t rxmac_enable	: 1;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t rxmac_enable	: 1;
+			uint32_t strip_pad	: 1;
+			uint32_t strip_fcs	: 1;
+			uint32_t promiscuous	: 1;
+			uint32_t promiscuous_group	: 1;
+			uint32_t hash_filter_en	: 1;
+			uint32_t addr_filter_en	: 1;
+			uint32_t error_chk_dis	: 1;
+			uint32_t rx_crs_extend_en : 1;
+			uint32_t mac2ipp_pkt_cnt_en : 1;
+			uint32_t mac_reg_sw_test : 2;
+			uint32_t rsrvd	: 20;
+#endif
+		} w0;
+
+#if defined(_LITTLE_ENDIAN)
+		uint32_t	w1;
+#endif
+	} bits;
+} brxmac_config_t, *p_brxmac_config_t;
+
+#define	MAC_CTRL_CONFIG_REG		0x070	/* MAC control config */
+#define	MAC_XIF_CONFIG_REG		0x078	/* XIF config */
+
+typedef union _bxif_config_t {
+	uint64_t value;
+
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t msw;	/* Most significant word */
+		uint32_t lsw;	/* Least significant word */
+#elif defined(_LITTLE_ENDIAN)
+		uint32_t lsw;	/* Least significant word */
+		uint32_t msw;	/* Most significant word */
+#endif
+	} val;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t	w1;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t rsrvd2		: 24;
+			uint32_t sel_clk_25mhz	: 1;
+			uint32_t led_polarity	: 1;
+			uint32_t force_led_on	: 1;
+			uint32_t used		: 1;
+			uint32_t gmii_mode	: 1;
+			uint32_t rsrvd		: 1;
+			uint32_t loopback	: 1;
+			uint32_t tx_output_en	: 1;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t tx_output_en	: 1;
+			uint32_t loopback	: 1;
+			uint32_t rsrvd		: 1;
+			uint32_t gmii_mode	: 1;
+			uint32_t used		: 1;
+			uint32_t force_led_on	: 1;
+			uint32_t led_polarity	: 1;
+			uint32_t sel_clk_25mhz	: 1;
+			uint32_t rsrvd2		: 24;
+#endif
+		} w0;
+
+#if defined(_LITTLE_ENDIAN)
+		uint32_t	w1;
+#endif
+	} bits;
+} bxif_config_t, *p_bxif_config_t;
+
+#define	BMAC_MIN_REG			0x0a0	/* min frame size */
+#define	BMAC_MAX_REG			0x0a8	/* max frame size reg */
+#define	MAC_PA_SIZE_REG			0x0b0	/* num of preamble bytes */
+#define	MAC_CTRL_TYPE_REG		0x0c8	/* type field of MAC ctrl */
+#define	BMAC_ADDR0_REG			0x100	/* MAC unique ad0 reg (HI 0) */
+#define	BMAC_ADDR1_REG			0x108	/* MAC unique ad1 reg */
+#define	BMAC_ADDR2_REG			0x110	/* MAC unique ad2 reg */
+#define	BMAC_ADDR3_REG			0x118	/* MAC alt ad0 reg (HI 1) */
+#define	BMAC_ADDR4_REG			0x120	/* MAC alt ad0 reg */
+#define	BMAC_ADDR5_REG			0x128	/* MAC alt ad0 reg */
+#define	BMAC_ADDR6_REG			0x130	/* MAC alt ad1 reg (HI 2) */
+#define	BMAC_ADDR7_REG			0x138	/* MAC alt ad1 reg */
+#define	BMAC_ADDR8_REG			0x140	/* MAC alt ad1 reg */
+#define	BMAC_ADDR9_REG			0x148	/* MAC alt ad2 reg (HI 3) */
+#define	BMAC_ADDR10_REG			0x150	/* MAC alt ad2 reg */
+#define	BMAC_ADDR11_REG			0x158	/* MAC alt ad2 reg */
+#define	BMAC_ADDR12_REG			0x160	/* MAC alt ad3 reg (HI 4) */
+#define	BMAC_ADDR13_REG			0x168	/* MAC alt ad3 reg */
+#define	BMAC_ADDR14_REG			0x170	/* MAC alt ad3 reg */
+#define	BMAC_ADDR15_REG			0x178	/* MAC alt ad4 reg (HI 5) */
+#define	BMAC_ADDR16_REG			0x180	/* MAC alt ad4 reg */
+#define	BMAC_ADDR17_REG			0x188	/* MAC alt ad4 reg */
+#define	BMAC_ADDR18_REG			0x190	/* MAC alt ad5 reg (HI 6) */
+#define	BMAC_ADDR19_REG			0x198	/* MAC alt ad5 reg */
+#define	BMAC_ADDR20_REG			0x1a0	/* MAC alt ad5 reg */
+#define	BMAC_ADDR21_REG			0x1a8	/* MAC alt ad6 reg (HI 7) */
+#define	BMAC_ADDR22_REG			0x1b0	/* MAC alt ad6 reg */
+#define	BMAC_ADDR23_REG			0x1b8	/* MAC alt ad6 reg */
+#define	MAC_FC_ADDR0_REG		0x268	/* FC frame addr0 (HI 0, p3) */
+#define	MAC_FC_ADDR1_REG		0x270	/* FC frame addr1 */
+#define	MAC_FC_ADDR2_REG		0x278	/* FC frame addr2 */
+#define	MAC_ADDR_FILT0_REG		0x298	/* bits [47:32] (HI 0, p2) */
+#define	MAC_ADDR_FILT1_REG		0x2a0	/* bits [31:16] */
+#define	MAC_ADDR_FILT2_REG		0x2a8	/* bits [15:0]  */
+#define	MAC_ADDR_FILT12_MASK_REG 	0x2b0	/* addr filter 2 & 1 mask */
+#define	MAC_ADDR_FILT00_MASK_REG	0x2b8	/* addr filter 0 mask */
+#define	MAC_HASH_TBL0_REG		0x2c0	/* hash table 0 reg */
+#define	MAC_HASH_TBL1_REG		0x2c8	/* hash table 1 reg */
+#define	MAC_HASH_TBL2_REG		0x2d0	/* hash table 2 reg */
+#define	MAC_HASH_TBL3_REG		0x2d8	/* hash table 3 reg */
+#define	MAC_HASH_TBL4_REG		0x2e0	/* hash table 4 reg */
+#define	MAC_HASH_TBL5_REG		0x2e8	/* hash table 5 reg */
+#define	MAC_HASH_TBL6_REG		0x2f0	/* hash table 6 reg */
+#define	MAC_HASH_TBL7_REG		0x2f8	/* hash table 7 reg */
+#define	MAC_HASH_TBL8_REG		0x300	/* hash table 8 reg */
+#define	MAC_HASH_TBL9_REG		0x308	/* hash table 9 reg */
+#define	MAC_HASH_TBL10_REG		0x310	/* hash table 10 reg */
+#define	MAC_HASH_TBL11_REG		0x318	/* hash table 11 reg */
+#define	MAC_HASH_TBL12_REG		0x320	/* hash table 12 reg */
+#define	MAC_HASH_TBL13_REG		0x328	/* hash table 13 reg */
+#define	MAC_HASH_TBL14_REG		0x330	/* hash table 14 reg */
+#define	MAC_HASH_TBL15_REG		0x338	/* hash table 15 reg */
+#define	RXMAC_FRM_CNT_REG		0x370	/* receive frame counter */
+#define	MAC_LEN_ER_CNT_REG		0x378	/* length error counter */
+#define	BMAC_AL_ER_CNT_REG		0x380	/* alignment error counter */
+#define	BMAC_CRC_ER_CNT_REG		0x388	/* FCS error counter */
+#define	BMAC_CD_VIO_CNT_REG		0x390	/* RX code violation err */
+#define	BMAC_SM_REG			0x3a0	/* (ro) state machine reg */
+#define	BMAC_ALTAD_CMPEN_REG		0x3f8	/* Alt addr compare enable */
+#define	BMAC_HOST_INF0_REG		0x400	/* Host info */
+						/* (own da, add filter, fc) */
+#define	BMAC_HOST_INF1_REG		0x408	/* Host info (alt ad 0) */
+#define	BMAC_HOST_INF2_REG		0x410	/* Host info (alt ad 1) */
+#define	BMAC_HOST_INF3_REG		0x418	/* Host info (alt ad 2) */
+#define	BMAC_HOST_INF4_REG		0x420	/* Host info (alt ad 3) */
+#define	BMAC_HOST_INF5_REG		0x428	/* Host info (alt ad 4) */
+#define	BMAC_HOST_INF6_REG		0x430	/* Host info (alt ad 5) */
+#define	BMAC_HOST_INF7_REG		0x438	/* Host info (alt ad 6) */
+#define	BMAC_HOST_INF8_REG		0x440	/* Host info (hash hit, miss) */
+#define	BTXMAC_BYTE_CNT_REG		0x448	/* Tx byte count */
+#define	BTXMAC_FRM_CNT_REG		0x450	/* frame count */
+#define	BRXMAC_BYTE_CNT_REG		0x458	/* Rx byte count */
+/* x ranges from 0 to 6 (BMAC_MAX_ALT_ADDR_ENTRY - 1) */
+#define	BMAC_ALT_ADDR0N_REG_ADDR(x)	(BMAC_ADDR3_REG + (x) * 24)
+#define	BMAC_ALT_ADDR1N_REG_ADDR(x)	(BMAC_ADDR3_REG + 8 + (x) * 24)
+#define	BMAC_ALT_ADDR2N_REG_ADDR(x)	(BMAC_ADDR3_REG + 0x10 + (x) * 24)
+#define	BMAC_HASH_TBLN_REG_ADDR(x)	(MAC_HASH_TBL0_REG + (x) * 8)
+#define	BMAC_HOST_INFN_REG_ADDR(x)	(BMAC_HOST_INF0_REG + (x) * 8)
+
+/* XMAC registers offset */
+#define	XTXMAC_SW_RST_REG		0x000	/* XTX MAC soft reset */
+#define	XRXMAC_SW_RST_REG		0x008	/* XRX MAC soft reset */
+#define	XTXMAC_STATUS_REG		0x020	/* XTX MAC status */
+#define	XRXMAC_STATUS_REG		0x028	/* XRX MAC status */
+#define	XMAC_CTRL_STAT_REG		0x030	/* Control / Status */
+#define	XTXMAC_STAT_MSK_REG		0x040	/* XTX MAC Status mask */
+#define	XRXMAC_STAT_MSK_REG		0x048	/* XRX MAC Status mask */
+#define	XMAC_C_S_MSK_REG		0x050	/* Control / Status mask */
+#define	XMAC_CONFIG_REG			0x060	/* Configuration */
+
+/* xmac config bit fields */
+typedef union _xmac_cfg_t {
+	uint64_t value;
+
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t msw;	/* Most significant word */
+		uint32_t lsw;	/* Least significant word */
+#elif defined(_LITTLE_ENDIAN)
+		uint32_t lsw;	/* Least significant word */
+		uint32_t msw;	/* Most significant word */
+#endif
+	} val;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t	w1;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t sel_clk_25mhz : 1;
+		uint32_t pcs_bypass	: 1;
+		uint32_t xpcs_bypass	: 1;
+		uint32_t mii_gmii_mode	: 2;
+		uint32_t lfs_disable	: 1;
+		uint32_t loopback	: 1;
+		uint32_t tx_output_en	: 1;
+		uint32_t sel_por_clk_src : 1;
+		uint32_t led_polarity	: 1;
+		uint32_t force_led_on	: 1;
+		uint32_t pass_fctl_frames : 1;
+		uint32_t recv_pause_en	: 1;
+		uint32_t mac2ipp_pkt_cnt_en : 1;
+		uint32_t strip_crc	: 1;
+		uint32_t addr_filter_en	: 1;
+		uint32_t hash_filter_en	: 1;
+		uint32_t code_viol_chk_dis	: 1;
+		uint32_t reserved_mcast	: 1;
+		uint32_t rx_crc_chk_dis	: 1;
+		uint32_t error_chk_dis	: 1;
+		uint32_t promisc_grp	: 1;
+		uint32_t promiscuous	: 1;
+		uint32_t rx_mac_enable	: 1;
+		uint32_t warning_msg_en	: 1;
+		uint32_t used		: 3;
+		uint32_t always_no_crc	: 1;
+		uint32_t var_min_ipg_en	: 1;
+		uint32_t strech_mode	: 1;
+		uint32_t tx_enable	: 1;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t tx_enable	: 1;
+		uint32_t strech_mode	: 1;
+		uint32_t var_min_ipg_en	: 1;
+		uint32_t always_no_crc	: 1;
+		uint32_t used		: 3;
+		uint32_t warning_msg_en	: 1;
+		uint32_t rx_mac_enable	: 1;
+		uint32_t promiscuous	: 1;
+		uint32_t promisc_grp	: 1;
+		uint32_t error_chk_dis	: 1;
+		uint32_t rx_crc_chk_dis	: 1;
+		uint32_t reserved_mcast	: 1;
+		uint32_t code_viol_chk_dis	: 1;
+		uint32_t hash_filter_en	: 1;
+		uint32_t addr_filter_en	: 1;
+		uint32_t strip_crc	: 1;
+		uint32_t mac2ipp_pkt_cnt_en : 1;
+		uint32_t recv_pause_en	: 1;
+		uint32_t pass_fctl_frames : 1;
+		uint32_t force_led_on	: 1;
+		uint32_t led_polarity	: 1;
+		uint32_t sel_por_clk_src : 1;
+		uint32_t tx_output_en	: 1;
+		uint32_t loopback	: 1;
+		uint32_t lfs_disable	: 1;
+		uint32_t mii_gmii_mode	: 2;
+		uint32_t xpcs_bypass	: 1;
+		uint32_t pcs_bypass	: 1;
+		uint32_t sel_clk_25mhz : 1;
+#endif
+		} w0;
+
+#if defined(_LITTLE_ENDIAN)
+		uint32_t	w1;
+#endif
+	} bits;
+} xmac_cfg_t, *p_xmac_cfg_t;
+
+#define	XMAC_IPG_REG			0x080	/* Inter-Packet-Gap */
+#define	XMAC_MIN_REG			0x088	/* min frame size register */
+#define	XMAC_MAX_REG			0x090	/* max frame/burst size */
+#define	XMAC_ADDR0_REG			0x0a0	/* [47:32] of MAC addr (HI17) */
+#define	XMAC_ADDR1_REG			0x0a8	/* [31:16] of MAC addr */
+#define	XMAC_ADDR2_REG			0x0b0	/* [15:0] of MAC addr */
+#define	XRXMAC_BT_CNT_REG		0x100	/* bytes received / 8 */
+#define	XRXMAC_BC_FRM_CNT_REG		0x108	/* good BC frames received */
+#define	XRXMAC_MC_FRM_CNT_REG		0x110	/* good MC frames received */
+#define	XRXMAC_FRAG_CNT_REG		0x118	/* frag frames rejected */
+#define	XRXMAC_HIST_CNT1_REG		0x120	/* 64 bytes frames */
+#define	XRXMAC_HIST_CNT2_REG		0x128	/* 65-127 bytes frames */
+#define	XRXMAC_HIST_CNT3_REG		0x130	/* 128-255 bytes frames */
+#define	XRXMAC_HIST_CNT4_REG		0x138	/* 256-511 bytes frames */
+#define	XRXMAC_HIST_CNT5_REG		0x140	/* 512-1023 bytes frames */
+#define	XRXMAC_HIST_CNT6_REG		0x148	/* 1024-1522 bytes frames */
+#define	XRXMAC_MPSZER_CNT_REG		0x150	/* frames > maxframesize */
+#define	XRXMAC_CRC_ER_CNT_REG		0x158	/* frames failed CRC */
+#define	XRXMAC_CD_VIO_CNT_REG		0x160	/* frames with code vio */
+#define	XRXMAC_AL_ER_CNT_REG		0x168	/* frames with align error */
+#define	XTXMAC_FRM_CNT_REG		0x170	/* tx frames */
+#define	XTXMAC_BYTE_CNT_REG		0x178	/* tx bytes / 8 */
+#define	XMAC_LINK_FLT_CNT_REG		0x180	/* link faults */
+#define	XRXMAC_HIST_CNT7_REG		0x188	/* MAC2IPP/>1523 bytes frames */
+#define	XMAC_SM_REG			0x1a8	/* State machine */
+#define	XMAC_INTERN1_REG		0x1b0	/* internal signals for diag */
+#define	XMAC_INTERN2_REG		0x1b8	/* internal signals for diag */
+#define	XMAC_ADDR_CMPEN_REG		0x208	/* alt MAC addr check */
+#define	XMAC_ADDR3_REG			0x218	/* alt MAC addr 0 (HI 0) */
+#define	XMAC_ADDR4_REG			0x220	/* alt MAC addr 0 */
+#define	XMAC_ADDR5_REG			0x228	/* alt MAC addr 0 */
+#define	XMAC_ADDR6_REG			0x230	/* alt MAC addr 1 (HI 1) */
+#define	XMAC_ADDR7_REG			0x238	/* alt MAC addr 1 */
+#define	XMAC_ADDR8_REG			0x240	/* alt MAC addr 1 */
+#define	XMAC_ADDR9_REG			0x248	/* alt MAC addr 2 (HI 2) */
+#define	XMAC_ADDR10_REG			0x250	/* alt MAC addr 2 */
+#define	XMAC_ADDR11_REG			0x258	/* alt MAC addr 2 */
+#define	XMAC_ADDR12_REG			0x260	/* alt MAC addr 3 (HI 3) */
+#define	XMAC_ADDR13_REG			0x268	/* alt MAC addr 3 */
+#define	XMAC_ADDR14_REG			0x270	/* alt MAC addr 3 */
+#define	XMAC_ADDR15_REG			0x278	/* alt MAC addr 4 (HI 4) */
+#define	XMAC_ADDR16_REG			0x280	/* alt MAC addr 4 */
+#define	XMAC_ADDR17_REG			0x288	/* alt MAC addr 4 */
+#define	XMAC_ADDR18_REG			0x290	/* alt MAC addr 5 (HI 5) */
+#define	XMAC_ADDR19_REG			0x298	/* alt MAC addr 5 */
+#define	XMAC_ADDR20_REG			0x2a0	/* alt MAC addr 5 */
+#define	XMAC_ADDR21_REG			0x2a8	/* alt MAC addr 6 (HI 6) */
+#define	XMAC_ADDR22_REG			0x2b0	/* alt MAC addr 6 */
+#define	XMAC_ADDR23_REG			0x2b8	/* alt MAC addr 6 */
+#define	XMAC_ADDR24_REG			0x2c0	/* alt MAC addr 7 (HI 7) */
+#define	XMAC_ADDR25_REG			0x2c8	/* alt MAC addr 7 */
+#define	XMAC_ADDR26_REG			0x2d0	/* alt MAC addr 7 */
+#define	XMAC_ADDR27_REG			0x2d8	/* alt MAC addr 8 (HI 8) */
+#define	XMAC_ADDR28_REG			0x2e0	/* alt MAC addr 8 */
+#define	XMAC_ADDR29_REG			0x2e8	/* alt MAC addr 8 */
+#define	XMAC_ADDR30_REG			0x2f0	/* alt MAC addr 9 (HI 9) */
+#define	XMAC_ADDR31_REG			0x2f8	/* alt MAC addr 9 */
+#define	XMAC_ADDR32_REG			0x300	/* alt MAC addr 9 */
+#define	XMAC_ADDR33_REG			0x308	/* alt MAC addr 10 (HI 10) */
+#define	XMAC_ADDR34_REG			0x310	/* alt MAC addr 10 */
+#define	XMAC_ADDR35_REG			0x318	/* alt MAC addr 10 */
+#define	XMAC_ADDR36_REG			0x320	/* alt MAC addr 11 (HI 11) */
+#define	XMAC_ADDR37_REG			0x328	/* alt MAC addr 11 */
+#define	XMAC_ADDR38_REG			0x330	/* alt MAC addr 11 */
+#define	XMAC_ADDR39_REG			0x338	/* alt MAC addr 12 (HI 12) */
+#define	XMAC_ADDR40_REG			0x340	/* alt MAC addr 12 */
+#define	XMAC_ADDR41_REG			0x348	/* alt MAC addr 12 */
+#define	XMAC_ADDR42_REG			0x350	/* alt MAC addr 13 (HI 13) */
+#define	XMAC_ADDR43_REG			0x358	/* alt MAC addr 13 */
+#define	XMAC_ADDR44_REG			0x360	/* alt MAC addr 13 */
+#define	XMAC_ADDR45_REG			0x368	/* alt MAC addr 14 (HI 14) */
+#define	XMAC_ADDR46_REG			0x370	/* alt MAC addr 14 */
+#define	XMAC_ADDR47_REG			0x378	/* alt MAC addr 14 */
+#define	XMAC_ADDR48_REG			0x380	/* alt MAC addr 15 (HI 15) */
+#define	XMAC_ADDR49_REG			0x388	/* alt MAC addr 15 */
+#define	XMAC_ADDR50_REG			0x390	/* alt MAC addr 15 */
+#define	XMAC_ADDR_FILT0_REG		0x818	/* [47:32] addr filter (HI18) */
+#define	XMAC_ADDR_FILT1_REG		0x820	/* [31:16] of addr filter */
+#define	XMAC_ADDR_FILT2_REG		0x828	/* [15:0] of addr filter */
+#define	XMAC_ADDR_FILT12_MASK_REG 	0x830	/* addr filter 2 & 1 mask */
+#define	XMAC_ADDR_FILT0_MASK_REG	0x838	/* addr filter 0 mask */
+#define	XMAC_HASH_TBL0_REG		0x840	/* hash table 0 reg */
+#define	XMAC_HASH_TBL1_REG		0x848	/* hash table 1 reg */
+#define	XMAC_HASH_TBL2_REG		0x850	/* hash table 2 reg */
+#define	XMAC_HASH_TBL3_REG		0x858	/* hash table 3 reg */
+#define	XMAC_HASH_TBL4_REG		0x860	/* hash table 4 reg */
+#define	XMAC_HASH_TBL5_REG		0x868	/* hash table 5 reg */
+#define	XMAC_HASH_TBL6_REG		0x870	/* hash table 6 reg */
+#define	XMAC_HASH_TBL7_REG		0x878	/* hash table 7 reg */
+#define	XMAC_HASH_TBL8_REG		0x880	/* hash table 8 reg */
+#define	XMAC_HASH_TBL9_REG		0x888	/* hash table 9 reg */
+#define	XMAC_HASH_TBL10_REG		0x890	/* hash table 10 reg */
+#define	XMAC_HASH_TBL11_REG		0x898	/* hash table 11 reg */
+#define	XMAC_HASH_TBL12_REG		0x8a0	/* hash table 12 reg */
+#define	XMAC_HASH_TBL13_REG		0x8a8	/* hash table 13 reg */
+#define	XMAC_HASH_TBL14_REG		0x8b0	/* hash table 14 reg */
+#define	XMAC_HASH_TBL15_REG		0x8b8	/* hash table 15 reg */
+#define	XMAC_HOST_INF0_REG		0x900	/* Host info 0 (alt ad 0) */
+#define	XMAC_HOST_INF1_REG		0x908	/* Host info 1 (alt ad 1) */
+#define	XMAC_HOST_INF2_REG		0x910	/* Host info 2 (alt ad 2) */
+#define	XMAC_HOST_INF3_REG		0x918	/* Host info 3 (alt ad 3) */
+#define	XMAC_HOST_INF4_REG		0x920	/* Host info 4 (alt ad 4) */
+#define	XMAC_HOST_INF5_REG		0x928	/* Host info 5 (alt ad 5) */
+#define	XMAC_HOST_INF6_REG		0x930	/* Host info 6 (alt ad 6) */
+#define	XMAC_HOST_INF7_REG		0x938	/* Host info 7 (alt ad 7) */
+#define	XMAC_HOST_INF8_REG		0x940	/* Host info 8 (alt ad 8) */
+#define	XMAC_HOST_INF9_REG		0x948	/* Host info 9 (alt ad 9) */
+#define	XMAC_HOST_INF10_REG		0x950	/* Host info 10 (alt ad 10) */
+#define	XMAC_HOST_INF11_REG		0x958	/* Host info 11 (alt ad 11) */
+#define	XMAC_HOST_INF12_REG		0x960	/* Host info 12 (alt ad 12) */
+#define	XMAC_HOST_INF13_REG		0x968	/* Host info 13 (alt ad 13) */
+#define	XMAC_HOST_INF14_REG		0x970	/* Host info 14 (alt ad 14) */
+#define	XMAC_HOST_INF15_REG		0x978	/* Host info 15 (alt ad 15) */
+#define	XMAC_HOST_INF16_REG		0x980	/* Host info 16 (hash hit) */
+#define	XMAC_HOST_INF17_REG		0x988	/* Host info 17 (own da) */
+#define	XMAC_HOST_INF18_REG		0x990	/* Host info 18 (filter hit) */
+#define	XMAC_HOST_INF19_REG		0x998	/* Host info 19 (fc hit) */
+#define	XMAC_PA_DATA0_REG		0xb80	/* preamble [31:0] */
+#define	XMAC_PA_DATA1_REG		0xb88	/* preamble [63:32] */
+#define	XMAC_DEBUG_SEL_REG		0xb90	/* debug select */
+#define	XMAC_TRAINING_VECT_REG		0xb98	/* training vector */
+/* x ranges from 0 to 15 (XMAC_MAX_ALT_ADDR_ENTRY - 1) */
+#define	XMAC_ALT_ADDR0N_REG_ADDR(x)	(XMAC_ADDR3_REG + (x) * 24)
+#define	XMAC_ALT_ADDR1N_REG_ADDR(x)	(XMAC_ADDR3_REG + 8 + (x) * 24)
+#define	XMAC_ALT_ADDR2N_REG_ADDR(x)	(XMAC_ADDR3_REG + 16 + (x) * 24)
+#define	XMAC_HASH_TBLN_REG_ADDR(x)	(XMAC_HASH_TBL0_REG + (x) * 8)
+#define	XMAC_HOST_INFN_REG_ADDR(x)	(XMAC_HOST_INF0_REG + (x) * 8)
+
+/* MIF registers offset */
+#define	MIF_BB_MDC_REG			0	   /* MIF bit-bang clock */
+#define	MIF_BB_MDO_REG			0x008	   /* MIF bit-bang data */
+#define	MIF_BB_MDO_EN_REG		0x010	   /* MIF bit-bang output en */
+#define	MIF_OUTPUT_FRAME_REG		0x018	   /* MIF frame/output reg */
+#define	MIF_CONFIG_REG			0x020	   /* MIF config reg */
+#define	MIF_POLL_STATUS_REG		0x028	   /* MIF poll status reg */
+#define	MIF_POLL_MASK_REG		0x030	   /* MIF poll mask reg */
+#define	MIF_STATE_MACHINE_REG		0x038	   /* MIF state machine reg */
+#define	MIF_STATUS_REG			0x040	   /* MIF status reg */
+#define	MIF_MASK_REG			0x048	   /* MIF mask reg */
+
+
+/* PCS registers offset */
+#define	PCS_MII_CTRL_REG		0	   /* PCS MII control reg */
+#define	PCS_MII_STATUS_REG		0x008	   /* PCS MII status reg */
+#define	PCS_MII_ADVERT_REG		0x010	   /* PCS MII advertisement */
+#define	PCS_MII_LPA_REG			0x018	   /* link partner ability */
+#define	PCS_CONFIG_REG			0x020	   /* PCS config reg */
+#define	PCS_STATE_MACHINE_REG		0x028	   /* PCS state machine */
+#define	PCS_INTR_STATUS_REG		0x030	/* PCS interrupt status */
+#define	PCS_DATAPATH_MODE_REG		0x0a0	   /* datapath mode reg */
+#define	PCS_PACKET_COUNT_REG		0x0c0	   /* PCS packet counter */
+
+#define	XPCS_CTRL_1_REG			0	/* Control */
+#define	XPCS_STATUS_1_REG		0x008
+#define	XPCS_DEV_ID_REG			0x010	/* 32bits IEEE manufacture ID */
+#define	XPCS_SPEED_ABILITY_REG		0x018
+#define	XPCS_DEV_IN_PKG_REG		0x020
+#define	XPCS_CTRL_2_REG			0x028
+#define	XPCS_STATUS_2_REG		0x030
+#define	XPCS_PKG_ID_REG			0x038	/* Package ID */
+#define	XPCS_STATUS_REG			0x040
+#define	XPCS_TEST_CTRL_REG		0x048
+#define	XPCS_CFG_VENDOR_1_REG		0x050
+#define	XPCS_DIAG_VENDOR_2_REG		0x058
+#define	XPCS_MASK_1_REG			0x060
+#define	XPCS_PKT_CNTR_REG		0x068
+#define	XPCS_TX_STATE_MC_REG		0x070
+#define	XPCS_DESKEW_ERR_CNTR_REG	0x078
+#define	XPCS_SYM_ERR_CNTR_L0_L1_REG	0x080
+#define	XPCS_SYM_ERR_CNTR_L2_L3_REG	0x088
+#define	XPCS_TRAINING_VECTOR_REG	0x090
+
+/* ESR registers offset */
+#define	ESR_RESET_REG			0
+#define	ESR_CONFIG_REG			0x008
+#define	ESR_0_PLL_CONFIG_REG		0x010
+#define	ESR_0_CONTROL_REG		0x018
+#define	ESR_0_TEST_CONFIG_REG		0x020
+#define	ESR_1_PLL_CONFIG_REG		0x028
+#define	ESR_1_CONTROL_REG		0x030
+#define	ESR_1_TEST_CONFIG_REG		0x038
+#define	ESR_ENET_RGMII_CFG_REG		0x040
+#define	ESR_INTERNAL_SIGNALS_REG	0x800
+#define	ESR_DEBUG_SEL_REG		0x808
+
+
+/* Reset Register */
+#define	MAC_SEND_PAUSE_TIME_MASK	0x0000FFFF /* value of pause time */
+#define	MAC_SEND_PAUSE_SEND		0x00010000 /* send pause flow ctrl */
+
+/* Tx MAC Status Register */
+#define	MAC_TX_FRAME_XMIT		0x00000001 /* successful tx frame */
+#define	MAC_TX_UNDERRUN			0x00000002 /* starvation in xmit */
+#define	MAC_TX_MAX_PACKET_ERR		0x00000004 /* TX frame exceeds max */
+#define	MAC_TX_BYTE_CNT_EXP		0x00000400 /* TX byte cnt overflow */
+#define	MAC_TX_FRAME_CNT_EXP		0x00000800 /* Tx frame cnt overflow */
+
+/* Rx MAC Status Register */
+#define	MAC_RX_FRAME_RECV		0x00000001 /* successful rx frame */
+#define	MAC_RX_OVERFLOW			0x00000002 /* RX FIFO overflow */
+#define	MAC_RX_FRAME_COUNT		0x00000004 /* rx frame cnt rollover */
+#define	MAC_RX_ALIGN_ERR		0x00000008 /* alignment err rollover */
+#define	MAC_RX_CRC_ERR			0x00000010 /* crc error cnt rollover */
+#define	MAC_RX_LEN_ERR			0x00000020 /* length err cnt rollover */
+#define	MAC_RX_VIOL_ERR			0x00000040 /* code vio err rollover */
+#define	MAC_RX_BYTE_CNT_EXP		0x00000080 /* RX MAC byte rollover */
+
+/* MAC Control Status Register */
+#define	MAC_CTRL_PAUSE_RECEIVED		0x00000001 /* successful pause frame */
+#define	MAC_CTRL_PAUSE_STATE		0x00000002 /* notpause-->pause */
+#define	MAC_CTRL_NOPAUSE_STATE		0x00000004 /* pause-->notpause */
+#define	MAC_CTRL_PAUSE_TIME_MASK	0xFFFF0000 /* value of pause time */
+#define	MAC_CTRL_PAUSE_TIME_SHIFT	16
+
+/* Tx MAC Configuration Register */
+#define	MAC_TX_CFG_TXMAC_ENABLE		0x00000001 /* enable TX MAC. */
+#define	MAC_TX_CFG_NO_FCS		0x00000100 /* TX not generate CRC */
+
+/* Rx MAC Configuration Register */
+#define	MAC_RX_CFG_RXMAC_ENABLE		0x00000001 /* enable RX MAC */
+#define	MAC_RX_CFG_STRIP_PAD		0x00000002 /* not supported, set to 0 */
+#define	MAC_RX_CFG_STRIP_FCS		0x00000004 /* strip last 4bytes (CRC) */
+#define	MAC_RX_CFG_PROMISC		0x00000008 /* promisc mode enable */
+#define	MAC_RX_CFG_PROMISC_GROUP  	0x00000010 /* accept all MC frames */
+#define	MAC_RX_CFG_HASH_FILTER_EN	0x00000020 /* use hash table */
+#define	MAC_RX_CFG_ADDR_FILTER_EN    	0x00000040 /* use address filter */
+#define	MAC_RX_CFG_DISABLE_DISCARD	0x00000080 /* do not set abort bit */
+#define	MAC_RX_MAC2IPP_PKT_CNT_EN	0x00000200 /* rx pkt cnt -> BMAC-IPP */
+#define	MAC_RX_MAC_REG_RW_TEST_MASK	0x00000c00 /* BMAC reg RW test */
+#define	MAC_RX_MAC_REG_RW_TEST_SHIFT	10
+
+/* MAC Control Configuration Register */
+#define	MAC_CTRL_CFG_SEND_PAUSE_EN	0x00000001 /* send pause flow ctrl */
+#define	MAC_CTRL_CFG_RECV_PAUSE_EN	0x00000002 /* receive pause flow ctrl */
+#define	MAC_CTRL_CFG_PASS_CTRL		0x00000004 /* accept MAC ctrl pkts */
+
+/* MAC XIF Configuration Register */
+#define	MAC_XIF_TX_OUTPUT_EN		0x00000001 /* enable Tx output driver */
+#define	MAC_XIF_MII_INT_LOOPBACK	0x00000002 /* loopback GMII xmit data */
+#define	MAC_XIF_GMII_MODE		0x00000008 /* operates with GMII clks */
+#define	MAC_XIF_LINK_LED		0x00000020 /* LINKLED# active (low) */
+#define	MAC_XIF_LED_POLARITY		0x00000040 /* LED polarity */
+#define	MAC_XIF_SEL_CLK_25MHZ		0x00000080 /* Select 10/100Mbps */
+
+/* MAC IPG Registers */
+#define	BMAC_MIN_FRAME_MASK		0x3FF	   /* 10-bit reg */
+
+/* MAC Max Frame Size Register */
+#define	BMAC_MAX_BURST_MASK    		0x3FFF0000 /* max burst size [30:16] */
+#define	BMAC_MAX_BURST_SHIFT   		16
+#define	BMAC_MAX_FRAME_MASK    		0x00007FFF /* max frame size [14:0] */
+#define	BMAC_MAX_FRAME_SHIFT   		0
+
+/* MAC Preamble size register */
+#define	BMAC_PA_SIZE_MASK		0x000003FF
+	/* # of preable bytes TxMAC sends at the beginning of each frame */
+
+/*
+ * mac address registers:
+ *	register	contains			comparison
+ *	--------	--------			----------
+ *	0		16 MSB of primary MAC addr	[47:32] of DA field
+ *	1		16 middle bits ""		[31:16] of DA field
+ *	2		16 LSB ""			[15:0] of DA field
+ *	3*x		16MSB of alt MAC addr 1-7	[47:32] of DA field
+ *	4*x		16 middle bits ""		[31:16]
+ *	5*x		16 LSB ""			[15:0]
+ *	42		16 MSB of MAC CTRL addr		[47:32] of DA.
+ *	43		16 middle bits ""		[31:16]
+ *	44		16 LSB ""			[15:0]
+ *	MAC CTRL addr must be the reserved multicast addr for MAC CTRL frames.
+ *	if there is a match, MAC will set the bit for alternative address
+ *	filter pass [15]
+ *
+ *	here is the map of registers given MAC address notation: a:b:c:d:e:f
+ *			ab		cd		ef
+ *	primary addr	reg 2		reg 1		reg 0
+ *	alt addr 1	reg 5		reg 4		reg 3
+ *	alt addr x	reg 5*x		reg 4*x		reg 3*x
+ *	|		|		|		|
+ *	|		|		|		|
+ *	alt addr 7	reg 23		reg 22		reg 21
+ *	ctrl addr	reg 44		reg 43		reg 42
+ */
+
+#define	BMAC_ALT_ADDR_BASE		0x118
+#define	BMAC_MAX_ALT_ADDR_ENTRY		7	   /* 7 alternate MAC addr */
+
+/* hash table registers */
+#define	MAC_MAX_HASH_ENTRY		16
+
+/* 27-bit register has the current state for key state machines in the MAC */
+#define	MAC_SM_RLM_MASK			0x07800000
+#define	MAC_SM_RLM_SHIFT		23
+#define	MAC_SM_RX_FC_MASK		0x00700000
+#define	MAC_SM_RX_FC_SHIFT		20
+#define	MAC_SM_TLM_MASK			0x000F0000
+#define	MAC_SM_TLM_SHIFT		16
+#define	MAC_SM_ENCAP_SM_MASK		0x0000F000
+#define	MAC_SM_ENCAP_SM_SHIFT		12
+#define	MAC_SM_TX_REQ_MASK		0x00000C00
+#define	MAC_SM_TX_REQ_SHIFT		10
+#define	MAC_SM_TX_FC_MASK		0x000003C0
+#define	MAC_SM_TX_FC_SHIFT		6
+#define	MAC_SM_FIFO_WRITE_SEL_MASK	0x00000038
+#define	MAC_SM_FIFO_WRITE_SEL_SHIFT	3
+#define	MAC_SM_TX_FIFO_EMPTY_MASK	0x00000007
+#define	MAC_SM_TX_FIFO_EMPTY_SHIFT	0
+
+#define	BMAC_ADDR0_CMPEN		0x00000001
+#define	BMAC_ADDRN_CMPEN(x)		(BMAC_ADDR0_CMP_EN << (x))
+
+/* MAC Host Info Table Registers */
+#define	BMAC_MAX_HOST_INFO_ENTRY	9 	/* 9 host entries */
+
+/*
+ * ********************* XMAC registers *********************************
+ */
+
+/* Reset Register */
+#define	XTXMAC_SOFT_RST			0x00000001 /* XTX MAC software reset */
+#define	XTXMAC_REG_RST			0x00000002 /* XTX MAC registers reset */
+#define	XRXMAC_SOFT_RST			0x00000001 /* XRX MAC software reset */
+#define	XRXMAC_REG_RST			0x00000002 /* XRX MAC registers reset */
+
+/* XTX MAC Status Register */
+#define	XMAC_TX_FRAME_XMIT		0x00000001 /* successful tx frame */
+#define	XMAC_TX_UNDERRUN		0x00000002 /* starvation in xmit */
+#define	XMAC_TX_MAX_PACKET_ERR		0x00000004 /* XTX frame exceeds max */
+#define	XMAC_TX_OVERFLOW		0x00000008 /* XTX byte cnt overflow */
+#define	XMAC_TX_FIFO_XFR_ERR		0x00000010 /* xtlm state mach error */
+#define	XMAC_TX_BYTE_CNT_EXP		0x00000400 /* XTX byte cnt overflow */
+#define	XMAC_TX_FRAME_CNT_EXP		0x00000800 /* XTX frame cnt overflow */
+
+/* XRX MAC Status Register */
+#define	XMAC_RX_FRAME_RCVD		0x00000001 /* successful rx frame */
+#define	XMAC_RX_OVERFLOW		0x00000002 /* RX FIFO overflow */
+#define	XMAC_RX_UNDERFLOW		0x00000004 /* RX FIFO underrun */
+#define	XMAC_RX_CRC_ERR_CNT_EXP		0x00000008 /* crc error cnt rollover */
+#define	XMAC_RX_LEN_ERR_CNT_EXP		0x00000010 /* length err cnt rollover */
+#define	XMAC_RX_VIOL_ERR_CNT_EXP	0x00000020 /* code vio err rollover */
+#define	XMAC_RX_OCT_CNT_EXP		0x00000040 /* XRX MAC byte rollover */
+#define	XMAC_RX_HST_CNT1_EXP		0x00000080 /* XRX MAC hist1 rollover */
+#define	XMAC_RX_HST_CNT2_EXP		0x00000100 /* XRX MAC hist2 rollover */
+#define	XMAC_RX_HST_CNT3_EXP		0x00000200 /* XRX MAC hist3 rollover */
+#define	XMAC_RX_HST_CNT4_EXP		0x00000400 /* XRX MAC hist4 rollover */
+#define	XMAC_RX_HST_CNT5_EXP		0x00000800 /* XRX MAC hist5 rollover */
+#define	XMAC_RX_HST_CNT6_EXP		0x00001000 /* XRX MAC hist6 rollover */
+#define	XMAC_RX_BCAST_CNT_EXP		0x00002000 /* XRX BC cnt rollover */
+#define	XMAC_RX_MCAST_CNT_EXP		0x00004000 /* XRX MC cnt rollover */
+#define	XMAC_RX_FRAG_CNT_EXP		0x00008000 /* fragment cnt rollover */
+#define	XMAC_RX_ALIGNERR_CNT_EXP	0x00010000 /* framealign err rollover */
+#define	XMAC_RX_LINK_FLT_CNT_EXP	0x00020000 /* link fault cnt rollover */
+#define	XMAC_RX_REMOTE_FLT_DET		0x00040000 /* Remote Fault detected */
+#define	XMAC_RX_LOCAL_FLT_DET		0x00080000 /* Local Fault detected */
+#define	XMAC_RX_HST_CNT7_EXP		0x00100000 /* XRX MAC hist7 rollover */
+
+
+#define	XMAC_CTRL_PAUSE_RCVD		0x00000001 /* successful pause frame */
+#define	XMAC_CTRL_PAUSE_STATE		0x00000002 /* notpause-->pause */
+#define	XMAC_CTRL_NOPAUSE_STATE		0x00000004 /* pause-->notpause */
+#define	XMAC_CTRL_PAUSE_TIME_MASK	0xFFFF0000 /* value of pause time */
+#define	XMAC_CTRL_PAUSE_TIME_SHIFT	16
+
+/* XMAC Configuration Register */
+#define	XMAC_CONFIG_TX_BIT_MASK		0x000000ff /* bits [7:0] */
+#define	XMAC_CONFIG_RX_BIT_MASK		0x001fff00 /* bits [20:8] */
+#define	XMAC_CONFIG_XIF_BIT_MASK	0xffe00000 /* bits [31:21] */
+
+/* XTX MAC config bits */
+#define	XMAC_TX_CFG_TX_ENABLE		0x00000001 /* enable XTX MAC */
+#define	XMAC_TX_CFG_STRETCH_MD		0x00000002 /* WAN application */
+#define	XMAC_TX_CFG_VAR_MIN_IPG_EN	0x00000004 /* Transmit pkts < minpsz */
+#define	XMAC_TX_CFG_ALWAYS_NO_CRC	0x00000008 /* No CRC generated */
+
+#define	XMAC_WARNING_MSG_ENABLE		0x00000080 /* Sim warning msg enable */
+
+/* XRX MAC config bits */
+#define	XMAC_RX_CFG_RX_ENABLE		0x00000100 /* enable XRX MAC */
+#define	XMAC_RX_CFG_PROMISC		0x00000200 /* promisc mode enable */
+#define	XMAC_RX_CFG_PROMISC_GROUP  	0x00000400 /* accept all MC frames */
+#define	XMAC_RX_CFG_ERR_CHK_DISABLE	0x00000800 /* do not set abort bit */
+#define	XMAC_RX_CFG_CRC_CHK_DISABLE	0x00001000 /* disable CRC logic */
+#define	XMAC_RX_CFG_RESERVED_MCAST	0x00002000 /* reserved MCaddr compare */
+#define	XMAC_RX_CFG_CD_VIO_CHK		0x00004000 /* rx code violation chk */
+#define	XMAC_RX_CFG_HASH_FILTER_EN	0x00008000 /* use hash table */
+#define	XMAC_RX_CFG_ADDR_FILTER_EN	0x00010000 /* use alt addr filter */
+#define	XMAC_RX_CFG_STRIP_CRC		0x00020000 /* strip last 4bytes (CRC) */
+#define	XMAC_RX_MAC2IPP_PKT_CNT_EN	0x00040000 /* histo_cntr7 cnt mode */
+#define	XMAC_RX_CFG_RX_PAUSE_EN		0x00080000 /* receive pause flow ctrl */
+#define	XMAC_RX_CFG_PASS_FLOW_CTRL	0x00100000 /* accept MAC ctrl pkts */
+
+
+/* MAC transceiver (XIF) configuration registers */
+
+#define	XMAC_XIF_FORCE_LED_ON		0x00200000 /* Force Link LED on */
+#define	XMAC_XIF_LED_POLARITY		0x00400000 /* LED polarity */
+#define	XMAC_XIF_SEL_POR_CLK_SRC	0x00800000 /* Select POR clk src */
+#define	XMAC_XIF_TX_OUTPUT_EN		0x01000000 /* enable MII/GMII modes */
+#define	XMAC_XIF_LOOPBACK		0x02000000 /* loopback xmac xgmii tx */
+#define	XMAC_XIF_LFS_DISABLE		0x04000000 /* disable link fault sig */
+#define	XMAC_XIF_MII_MODE_MASK		0x18000000 /* MII/GMII/XGMII mode */
+#define	XMAC_XIF_MII_MODE_SHIFT		27
+#define	XMAC_XIF_XGMII_MODE		0x00
+#define	XMAC_XIF_GMII_MODE		0x01
+#define	XMAC_XIF_MII_MODE		0x02
+#define	XMAC_XIF_ILLEGAL_MODE		0x03
+#define	XMAC_XIF_XPCS_BYPASS		0x20000000 /* use external xpcs */
+#define	XMAC_XIF_1G_PCS_BYPASS		0x40000000 /* use external pcs */
+#define	XMAC_XIF_SEL_CLK_25MHZ		0x80000000 /* 25Mhz clk for 100mbps */
+
+/* IPG register */
+#define	XMAC_IPG_VALUE_MASK		0x00000007 /* IPG in XGMII mode */
+#define	XMAC_IPG_VALUE_SHIFT		0
+#define	XMAC_IPG_VALUE1_MASK		0x0000ff00 /* IPG in GMII/MII mode */
+#define	XMAC_IPG_VALUE1_SHIFT		8
+#define	XMAC_IPG_STRETCH_RATIO_MASK	0x001f0000
+#define	XMAC_IPG_STRETCH_RATIO_SHIFT	16
+#define	XMAC_IPG_STRETCH_CONST_MASK	0x00e00000
+#define	XMAC_IPG_STRETCH_CONST_SHIFT	21
+
+#define	IPG_12_15_BYTE			3
+#define	IPG_16_19_BYTE			4
+#define	IPG_20_23_BYTE			5
+#define	IPG1_12_BYTES			10
+#define	IPG1_13_BYTES			11
+#define	IPG1_14_BYTES			12
+#define	IPG1_15_BYTES			13
+#define	IPG1_16_BYTES			14
+
+
+#define	XMAC_MIN_TX_FRM_SZ_MASK		0x3ff	   /* Min tx frame size */
+#define	XMAC_MIN_TX_FRM_SZ_SHIFT	0
+#define	XMAC_SLOT_TIME_MASK		0x0003fc00 /* slot time */
+#define	XMAC_SLOT_TIME_SHIFT		10
+#define	XMAC_MIN_RX_FRM_SZ_MASK		0x3ff00000 /* Min rx frame size */
+#define	XMAC_MIN_RX_FRM_SZ_SHIFT	20
+#define	XMAC_MAX_FRM_SZ_MASK		0x00003fff /* max tx frame size */
+
+/* State Machine Register */
+#define	XMAC_SM_TX_LNK_MGMT_MASK	0x00000007
+#define	XMAC_SM_TX_LNK_MGMT_SHIFT	0
+#define	XMAC_SM_SOP_DETECT		0x00000008
+#define	XMAC_SM_LNK_FLT_SIG_MASK	0x00000030
+#define	XMAC_SM_LNK_FLT_SIG_SHIFT	4
+#define	XMAC_SM_MII_GMII_MD_RX_LNK	0x00000040
+#define	XMAC_SM_XGMII_MD_RX_LNK		0x00000080
+#define	XMAC_SM_XGMII_ONLY_VAL_SIG	0x00000100
+#define	XMAC_SM_ALT_ADR_N_HSH_FN_SIG	0x00000200
+#define	XMAC_SM_RXMAC_IPP_STAT_MASK	0x00001c00
+#define	XMAC_SM_RXMAC_IPP_STAT_SHIFT	10
+#define	XMAC_SM_RXFIFO_WPTR_CLK_MASK	0x007c0000
+#define	XMAC_SM_RXFIFO_WPTR_CLK_SHIFT	18
+#define	XMAC_SM_RXFIFO_RPTR_CLK_MASK	0x0F800000
+#define	XMAC_SM_RXFIFO_RPTR_CLK_SHIFT	23
+#define	XMAC_SM_TXFIFO_FULL_CLK		0x10000000
+#define	XMAC_SM_TXFIFO_EMPTY_CLK	0x20000000
+#define	XMAC_SM_RXFIFO_FULL_CLK		0x40000000
+#define	XMAC_SM_RXFIFO_EMPTY_CLK	0x80000000
+
+/* Internal Signals 1 Register */
+#define	XMAC_IS1_OPP_TXMAC_STAT_MASK	0x0000000F
+#define	XMAC_IS1_OPP_TXMAC_STAT_SHIFT	0
+#define	XMAC_IS1_OPP_TXMAC_ABORT	0x00000010
+#define	XMAC_IS1_OPP_TXMAC_TAG 		0x00000020
+#define	XMAC_IS1_OPP_TXMAC_ACK		0x00000040
+#define	XMAC_IS1_TXMAC_OPP_REQ		0x00000080
+#define	XMAC_IS1_RXMAC_IPP_STAT_MASK	0x0FFFFF00
+#define	XMAC_IS1_RXMAC_IPP_STAT_SHIFT	8
+#define	XMAC_IS1_RXMAC_IPP_CTRL		0x10000000
+#define	XMAC_IS1_RXMAC_IPP_TAG		0x20000000
+#define	XMAC_IS1_IPP_RXMAC_REQ		0x40000000
+#define	XMAC_IS1_RXMAC_IPP_ACK		0x80000000
+
+/* Internal Signals 2 Register */
+#define	XMAC_IS2_TX_HB_TIMER_MASK	0x0000000F
+#define	XMAC_IS2_TX_HB_TIMER_SHIFT	0
+#define	XMAC_IS2_RX_HB_TIMER_MASK	0x000000F0
+#define	XMAC_IS2_RX_HB_TIMER_SHIFT	4
+#define	XMAC_IS2_XPCS_RXC_MASK		0x0000FF00
+#define	XMAC_IS2_XPCS_RXC_SHIFT		8
+#define	XMAC_IS2_XPCS_TXC_MASK		0x00FF0000
+#define	XMAC_IS2_XPCS_TXC_SHIFT		16
+#define	XMAC_IS2_LOCAL_FLT_OC_SYNC	0x01000000
+#define	XMAC_IS2_RMT_FLT_OC_SYNC	0x02000000
+
+/* Register size masking */
+
+#define	XTXMAC_FRM_CNT_MASK		0xFFFFFFFF
+#define	XTXMAC_BYTE_CNT_MASK		0xFFFFFFFF
+#define	XRXMAC_CRC_ER_CNT_MASK		0x000000FF
+#define	XRXMAC_MPSZER_CNT_MASK		0x000000FF
+#define	XRXMAC_CD_VIO_CNT_MASK		0x000000FF
+#define	XRXMAC_BT_CNT_MASK		0xFFFFFFFF
+#define	XRXMAC_HIST_CNT1_MASK		0x001FFFFF
+#define	XRXMAC_HIST_CNT2_MASK		0x001FFFFF
+#define	XRXMAC_HIST_CNT3_MASK		0x000FFFFF
+#define	XRXMAC_HIST_CNT4_MASK		0x0007FFFF
+#define	XRXMAC_HIST_CNT5_MASK		0x0003FFFF
+#define	XRXMAC_HIST_CNT6_MASK		0x0001FFFF
+#define	XRXMAC_BC_FRM_CNT_MASK		0x001FFFFF
+#define	XRXMAC_MC_FRM_CNT_MASK		0x001FFFFF
+#define	XRXMAC_FRAG_CNT_MASK		0x001FFFFF
+#define	XRXMAC_AL_ER_CNT_MASK		0x000000FF
+#define	XMAC_LINK_FLT_CNT_MASK		0x000000FF
+#define	BTXMAC_FRM_CNT_MASK		0x001FFFFF
+#define	BTXMAC_BYTE_CNT_MASK		0x07FFFFFF
+#define	RXMAC_FRM_CNT_MASK		0x0000FFFF
+#define	BRXMAC_BYTE_CNT_MASK		0x07FFFFFF
+#define	BMAC_AL_ER_CNT_MASK		0x0000FFFF
+#define	MAC_LEN_ER_CNT_MASK		0x0000FFFF
+#define	BMAC_CRC_ER_CNT_MASK		0x0000FFFF
+#define	BMAC_CD_VIO_CNT_MASK		0x0000FFFF
+#define	XMAC_XPCS_DESKEW_ERR_CNT_MASK	0x000000FF
+#define	XMAC_XPCS_SYM_ERR_CNT_L0_MASK	0x0000FFFF
+#define	XMAC_XPCS_SYM_ERR_CNT_L1_MASK	0xFFFF0000
+#define	XMAC_XPCS_SYM_ERR_CNT_L1_SHIFT	16
+#define	XMAC_XPCS_SYM_ERR_CNT_L2_MASK	0x0000FFFF
+#define	XMAC_XPCS_SYM_ERR_CNT_L3_MASK	0xFFFF0000
+#define	XMAC_XPCS_SYM_ERR_CNT_L3_SHIFT	16
+
+/* Alternate MAC address registers */
+#define	XMAC_MAX_ALT_ADDR_ENTRY		16	   /* 16 alternate MAC addrs */
+
+/* Max / Min parameters for Neptune MAC */
+
+#define	MAC_MAX_ALT_ADDR_ENTRY		XMAC_MAX_ALT_ADDR_ENTRY
+#define	MAC_MAX_HOST_INFO_ENTRY		XMAC_MAX_HOST_INFO_ENTRY
+
+/* HostInfo entry for the unique MAC address */
+#define	XMAC_UNIQUE_HOST_INFO_ENTRY	17
+#define	BMAC_UNIQUE_HOST_INFO_ENTRY	0
+
+/* HostInfo entry for the multicat address */
+#define	XMAC_MULTI_HOST_INFO_ENTRY	16
+#define	BMAC_MULTI_HOST_INFO_ENTRY	8
+
+/* XMAC Host Info Register */
+typedef union hostinfo {
+
+	uint64_t value;
+
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t msw;	/* Most significant word */
+		uint32_t lsw;	/* Least significant word */
+#elif defined(_LITTLE_ENDIAN)
+		uint32_t lsw;	/* Least significant word */
+		uint32_t msw;	/* Most significant word */
+#endif
+	} val;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t	w1;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t reserved2	: 23;
+		uint32_t mac_pref	: 1;
+		uint32_t reserved1	: 5;
+		uint32_t rdc_tbl_num	: 3;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t rdc_tbl_num	: 3;
+		uint32_t reserved1	: 5;
+		uint32_t mac_pref	: 1;
+		uint32_t reserved2	: 23;
+#endif
+		} w0;
+
+#if defined(_LITTLE_ENDIAN)
+		uint32_t	w1;
+#endif
+	} bits;
+
+} hostinfo_t;
+
+typedef union hostinfo *hostinfo_pt;
+
+#define	XMAC_HI_RDC_TBL_NUM_MASK	0x00000007
+#define	XMAC_HI_MAC_PREF		0x00000100
+
+#define	XMAC_MAX_HOST_INFO_ENTRY	20	   /* 20 host entries */
+
+/*
+ * ******************** MIF registers *********************************
+ */
+
+/*
+ * 32-bit register serves as an instruction register when the MIF is
+ * programmed in frame mode. load this register w/ a valid instruction
+ * (as per IEEE 802.3u MII spec). poll this register to check for instruction
+ * execution completion. during a read operation, this register will also
+ * contain the 16-bit data returned by the transceiver. unless specified
+ * otherwise, fields are considered "don't care" when polling for
+ * completion.
+ */
+
+#define	MIF_FRAME_START_MASK		0xC0000000 /* start of frame mask */
+#define	MIF_FRAME_ST_22			0x40000000 /* STart of frame, Cl 22 */
+#define	MIF_FRAME_ST_45			0x00000000 /* STart of frame, Cl 45 */
+#define	MIF_FRAME_OPCODE_MASK		0x30000000 /* opcode */
+#define	MIF_FRAME_OP_READ_22		0x20000000 /* read OPcode, Cl 22 */
+#define	MIF_FRAME_OP_WRITE_22		0x10000000 /* write OPcode, Cl 22 */
+#define	MIF_FRAME_OP_ADDR_45		0x00000000 /* addr of reg to access */
+#define	MIF_FRAME_OP_READ_45		0x30000000 /* read OPcode, Cl 45 */
+#define	MIF_FRAME_OP_WRITE_45		0x10000000 /* write OPcode, Cl 45 */
+#define	MIF_FRAME_OP_P_R_I_A_45		0x10000000 /* post-read-inc-addr */
+#define	MIF_FRAME_PHY_ADDR_MASK		0x0F800000 /* phy address mask */
+#define	MIF_FRAME_PHY_ADDR_SHIFT	23
+#define	MIF_FRAME_REG_ADDR_MASK		0x007C0000 /* reg addr in Cl 22 */
+						/* dev addr in Cl 45 */
+#define	MIF_FRAME_REG_ADDR_SHIFT	18
+#define	MIF_FRAME_TURN_AROUND_MSB	0x00020000 /* turn around, MSB. */
+#define	MIF_FRAME_TURN_AROUND_LSB	0x00010000 /* turn around, LSB. */
+#define	MIF_FRAME_DATA_MASK		0x0000FFFF /* instruction payload */
+
+/* Clause 45 frame field values */
+#define	FRAME45_ST		0
+#define	FRAME45_OP_ADDR		0
+#define	FRAME45_OP_WRITE	1
+#define	FRAME45_OP_READ_INC	2
+#define	FRAME45_OP_READ		3
+
+typedef union _mif_frame_t {
+
+	uint64_t value;
+
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t msw;	/* Most significant word */
+		uint32_t lsw;	/* Least significant word */
+#elif defined(_LITTLE_ENDIAN)
+		uint32_t lsw;	/* Least significant word */
+		uint32_t msw;	/* Most significant word */
+#endif
+	} val;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t	w1;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t st		: 2;
+		uint32_t op		: 2;
+		uint32_t phyad		: 5;
+		uint32_t regad		: 5;
+		uint32_t ta_msb		: 1;
+		uint32_t ta_lsb		: 1;
+		uint32_t data		: 16;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t data		: 16;
+		uint32_t ta_lsb		: 1;
+		uint32_t ta_msb		: 1;
+		uint32_t regad		: 5;
+		uint32_t phyad		: 5;
+		uint32_t op		: 2;
+		uint32_t st		: 2;
+#endif
+		} w0;
+
+#if defined(_LITTLE_ENDIAN)
+		uint32_t	w1;
+#endif
+	} bits;
+} mif_frame_t;
+
+#define	MIF_CFG_POLL_EN			0x00000008 /* enable polling */
+#define	MIF_CFG_BB_MODE			0x00000010 /* bit-bang mode */
+#define	MIF_CFG_POLL_REG_MASK		0x000003E0 /* reg addr to be polled */
+#define	MIF_CFG_POLL_REG_SHIFT		5
+#define	MIF_CFG_POLL_PHY_MASK		0x00007C00 /* XCVR addr to be polled */
+#define	MIF_CFG_POLL_PHY_SHIFT		10
+#define	MIF_CFG_INDIRECT_MODE		0x0000800
+					/* used to decide if Cl 22 */
+					/* or Cl 45 frame is */
+					/* constructed. */
+					/* 1 = Clause 45,ST = '00' */
+					/* 0 = Clause 22,ST = '01' */
+#define	MIF_CFG_ATCE_GE_EN	0x00010000 /* Enable ATCA gigabit mode */
+
+typedef union _mif_cfg_t {
+
+	uint64_t value;
+
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t msw;	/* Most significant word */
+		uint32_t lsw;	/* Least significant word */
+
+#elif defined(_LITTLE_ENDIAN)
+		uint32_t lsw;	/* Least significant word */
+		uint32_t msw;	/* Most significant word */
+#endif
+	} val;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t	w1;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t res2		: 15;
+		uint32_t atca_ge	: 1;
+		uint32_t indirect_md	: 1;
+		uint32_t phy_addr	: 5;
+		uint32_t reg_addr	: 5;
+		uint32_t bb_mode	: 1;
+		uint32_t poll_en	: 1;
+		uint32_t res1		: 2;
+		uint32_t res		: 1;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t res		: 1;
+		uint32_t res1		: 2;
+		uint32_t poll_en	: 1;
+		uint32_t bb_mode	: 1;
+		uint32_t reg_addr	: 5;
+		uint32_t phy_addr	: 5;
+		uint32_t indirect_md	: 1;
+		uint32_t atca_ge	: 1;
+		uint32_t res2		: 15;
+#endif
+		} w0;
+
+#if defined(_LITTLE_ENDIAN)
+		uint32_t	w1;
+#endif
+	} bits;
+
+} mif_cfg_t;
+
+#define	MIF_POLL_STATUS_DATA_MASK	0xffff0000
+#define	MIF_POLL_STATUS_STAT_MASK	0x0000ffff
+
+typedef union _mif_poll_stat_t {
+	uint64_t value;
+
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t msw;	/* Most significant word */
+		uint32_t lsw;	/* Least significant word */
+#elif defined(_LITTLE_ENDIAN)
+		uint32_t lsw;	/* Least significant word */
+		uint32_t msw;	/* Most significant word */
+#endif
+	} val;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t	w1;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t data;
+		uint16_t status;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint16_t status;
+		uint16_t data;
+#endif
+		} w0;
+
+#if defined(_LITTLE_ENDIAN)
+		uint32_t	w1;
+#endif
+	} bits;
+} mif_poll_stat_t;
+
+
+#define	MIF_POLL_MASK_MASK	0x0000ffff
+
+typedef union _mif_poll_mask_t {
+	uint64_t value;
+
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t msw;	/* Most significant word */
+		uint32_t lsw;	/* Least significant word */
+#elif defined(_LITTLE_ENDIAN)
+		uint32_t lsw;	/* Least significant word */
+		uint32_t msw;	/* Most significant word */
+#endif
+	} val;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t	w1;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t rsvd;
+		uint16_t mask;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint16_t mask;
+		uint16_t rsvd;
+#endif
+		} w0;
+
+#if defined(_LITTLE_ENDIAN)
+		uint32_t	w1;
+#endif
+	} bits;
+} mif_poll_mask_t;
+
+#define	MIF_STATUS_INIT_DONE_MASK	0x00000001
+#define	MIF_STATUS_XGE_ERR0_MASK	0x00000002
+#define	MIF_STATUS_XGE_ERR1_MASK	0x00000004
+#define	MIF_STATUS_PEU_ERR_MASK		0x00000008
+#define	MIF_STATUS_EXT_PHY_INTR0_MASK	0x00000010
+#define	MIF_STATUS_EXT_PHY_INTR1_MASK	0x00000020
+
+typedef union _mif_stat_t {
+	uint64_t value;
+
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t msw;	/* Most significant word */
+		uint32_t lsw;	/* Least significant word */
+#elif defined(_LITTLE_ENDIAN)
+		uint32_t lsw;	/* Least significant word */
+		uint32_t msw;	/* Most significant word */
+#endif
+	} val;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t	w1;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t rsvd:26;
+		uint32_t ext_phy_intr_flag1:1;
+		uint32_t ext_phy_intr_flag0:1;
+		uint32_t peu_err:1;
+		uint32_t xge_err1:1;
+		uint32_t xge_err0:1;
+		uint32_t mif_init_done_stat:1;
+
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t mif_init_done_stat:1;
+		uint32_t xge_err0:1;
+		uint32_t xge_err1:1;
+		uint32_t ext_phy_intr_flag0:1;
+		uint32_t ext_phy_intr_flag1:1;
+		uint32_t rsvd:26;
+#endif
+		} w0;
+
+#if defined(_LITTLE_ENDIAN)
+		uint32_t	w1;
+#endif
+	} bits;
+} mif_stat_t;
+
+/* MIF State Machine Register */
+
+#define	MIF_SM_EXECUTION_MASK		0x0000003f /* execution state */
+#define	MIF_SM_EXECUTION_SHIFT		0
+#define	MIF_SM_CONTROL_MASK		0x000001c0 /* control state */
+#define	MIF_SM_CONTROL_MASK_SHIFT	6
+#define	MIF_SM_MDI			0x00000200
+#define	MIF_SM_MDO			0x00000400
+#define	MIF_SM_MDO_EN			0x00000800
+#define	MIF_SM_MDC			0x00001000
+#define	MIF_SM_MDI_0			0x00002000
+#define	MIF_SM_MDI_1			0x00004000
+#define	MIF_SM_MDI_2			0x00008000
+#define	MIF_SM_PORT_ADDR_MASK		0x001f0000
+#define	MIF_SM_PORT_ADDR_SHIFT		16
+#define	MIF_SM_INT_SIG_MASK		0xffe00000
+#define	MIF_SM_INT_SIG_SHIFT		21
+
+
+/*
+ * ******************** PCS registers *********************************
+ */
+
+/* PCS Registers */
+#define	PCS_MII_CTRL_1000_SEL		0x0040	   /* reads 1. ignored on wr */
+#define	PCS_MII_CTRL_COLLISION_TEST	0x0080	   /* COL signal */
+#define	PCS_MII_CTRL_DUPLEX		0x0100	   /* forced 0x0. */
+#define	PCS_MII_RESTART_AUTONEG		0x0200	   /* self clearing. */
+#define	PCS_MII_ISOLATE			0x0400	   /* read 0. ignored on wr */
+#define	PCS_MII_POWER_DOWN		0x0800	   /* read 0. ignored on wr */
+#define	PCS_MII_AUTONEG_EN		0x1000	   /* autonegotiation */
+#define	PCS_MII_10_100_SEL		0x2000	   /* read 0. ignored on wr */
+#define	PCS_MII_RESET			0x8000	   /* reset PCS. */
+
+typedef union _pcs_ctrl_t {
+	uint64_t value;
+
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t msw;	/* Most significant word */
+		uint32_t lsw;	/* Least significant word */
+#elif defined(_LITTLE_ENDIAN)
+		uint32_t lsw;	/* Least significant word */
+		uint32_t msw;	/* Most significant word */
+#endif
+	} val;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t	w1;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res0		: 16;
+			uint32_t reset		: 1;
+			uint32_t res1		: 1;
+			uint32_t sel_10_100	: 1;
+			uint32_t an_enable	: 1;
+			uint32_t pwr_down	: 1;
+			uint32_t isolate	: 1;
+			uint32_t restart_an	: 1;
+			uint32_t duplex		: 1;
+			uint32_t col_test	: 1;
+			uint32_t sel_1000	: 1;
+			uint32_t res2		: 6;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t res2		: 6;
+			uint32_t sel_1000	: 1;
+			uint32_t col_test	: 1;
+			uint32_t duplex		: 1;
+			uint32_t restart_an	: 1;
+			uint32_t isolate	: 1;
+			uint32_t pwr_down	: 1;
+			uint32_t an_enable	: 1;
+			uint32_t sel_10_100	: 1;
+			uint32_t res1		: 1;
+			uint32_t reset		: 1;
+			uint32_t res0		: 16;
+#endif
+		} w0;
+
+#if defined(_LITTLE_ENDIAN)
+		uint32_t	w1;
+#endif
+	} bits;
+} pcs_ctrl_t;
+
+#define	PCS_MII_STATUS_EXTEND_CAP	0x0001	   /* reads 0 */
+#define	PCS_MII_STATUS_JABBER_DETECT	0x0002	   /* reads 0 */
+#define	PCS_MII_STATUS_LINK_STATUS	0x0004	   /* link status */
+#define	PCS_MII_STATUS_AUTONEG_ABLE	0x0008	   /* reads 1 */
+#define	PCS_MII_STATUS_REMOTE_FAULT	0x0010	   /* remote fault detected */
+#define	PCS_MII_STATUS_AUTONEG_COMP	0x0020	   /* auto-neg completed */
+#define	PCS_MII_STATUS_EXTEND_STATUS	0x0100	   /* 1000 Base-X PHY */
+
+typedef union _pcs_stat_t {
+	uint64_t value;
+
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t msw;	/* Most significant word */
+		uint32_t lsw;	/* Least significant word */
+#elif defined(_LITTLE_ENDIAN)
+		uint32_t lsw;	/* Least significant word */
+		uint32_t msw;	/* Most significant word */
+#endif
+	} val;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t	w1;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t res0		: 23;
+		uint32_t ext_stat	: 1;
+		uint32_t res1		: 2;
+		uint32_t an_complete	: 1;
+		uint32_t remote_fault	: 1;
+		uint32_t an_able	: 1;
+		uint32_t link_stat	: 1;
+		uint32_t jabber_detect	: 1;
+		uint32_t ext_cap	: 1;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t ext_cap	: 1;
+		uint32_t jabber_detect	: 1;
+		uint32_t link_stat	: 1;
+		uint32_t an_able	: 1;
+		uint32_t remote_fault	: 1;
+		uint32_t an_complete	: 1;
+		uint32_t res1		: 2;
+		uint32_t ext_stat	: 1;
+		uint32_t res0		: 23;
+#endif
+		} w0;
+
+#if defined(_LITTLE_ENDIAN)
+		uint32_t	w1;
+#endif
+	} bits;
+} pcs_stat_t;
+
+#define	PCS_MII_ADVERT_FD		0x0020	   /* advertise full duplex */
+#define	PCS_MII_ADVERT_HD		0x0040	   /* advertise half-duplex */
+#define	PCS_MII_ADVERT_SYM_PAUSE	0x0080	   /* advertise PAUSE sym */
+#define	PCS_MII_ADVERT_ASYM_PAUSE	0x0100	   /* advertises PAUSE asym */
+#define	PCS_MII_ADVERT_RF_MASK		0x3000	   /* remote fault */
+#define	PCS_MII_ADVERT_RF_SHIFT		12
+#define	PCS_MII_ADVERT_ACK		0x4000	   /* (ro) */
+#define	PCS_MII_ADVERT_NEXT_PAGE	0x8000	   /* (ro) forced 0x0 */
+
+#define	PCS_MII_LPA_FD			PCS_MII_ADVERT_FD
+#define	PCS_MII_LPA_HD			PCS_MII_ADVERT_HD
+#define	PCS_MII_LPA_SYM_PAUSE		PCS_MII_ADVERT_SYM_PAUSE
+#define	PCS_MII_LPA_ASYM_PAUSE		PCS_MII_ADVERT_ASYM_PAUSE
+#define	PCS_MII_LPA_RF_MASK		PCS_MII_ADVERT_RF_MASK
+#define	PCS_MII_LPA_RF_SHIFT		PCS_MII_ADVERT_RF_SHIFT
+#define	PCS_MII_LPA_ACK			PCS_MII_ADVERT_ACK
+#define	PCS_MII_LPA_NEXT_PAGE		PCS_MII_ADVERT_NEXT_PAGE
+
+typedef union _pcs_anar_t {
+	uint64_t value;
+
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t msw;	/* Most significant word */
+		uint32_t lsw;	/* Least significant word */
+#elif defined(_LITTLE_ENDIAN)
+		uint32_t lsw;	/* Least significant word */
+		uint32_t msw;	/* Most significant word */
+#endif
+	} val;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t	w1;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t res0		: 16;
+		uint32_t next_page	: 1;
+		uint32_t ack		: 1;
+		uint32_t remote_fault	: 2;
+		uint32_t res1		: 3;
+		uint32_t asm_pause	: 1;
+		uint32_t pause		: 1;
+		uint32_t half_duplex	: 1;
+		uint32_t full_duplex	: 1;
+		uint32_t res2		: 5;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t res2		: 5;
+		uint32_t full_duplex	: 1;
+		uint32_t half_duplex	: 1;
+		uint32_t pause		: 1;
+		uint32_t asm_pause	: 1;
+		uint32_t res1		: 3;
+		uint32_t remore_fault	: 2;
+		uint32_t ack		: 1;
+		uint32_t next_page	: 1;
+		uint32_t res0		: 16;
+#endif
+		} w0;
+
+#if defined(_LITTLE_ENDIAN)
+		uint32_t	w1;
+#endif
+	} bits;
+} pcs_anar_t, *p_pcs_anar_t;
+
+#define	PCS_CFG_EN			0x0001	   /* enable PCS. */
+#define	PCS_CFG_SD_OVERRIDE		0x0002
+#define	PCS_CFG_SD_ACTIVE_LOW		0x0004	   /* sig detect active low */
+#define	PCS_CFG_JITTER_STUDY_MASK	0x0018	   /* jitter measurements */
+#define	PCS_CFG_JITTER_STUDY_SHIFT	4
+#define	PCS_CFG_10MS_TIMER_OVERRIDE	0x0020	   /* shortens autoneg timer */
+#define	PCS_CFG_MASK			0x0040	   /* PCS global mask bit */
+
+typedef union _pcs_cfg_t {
+	uint64_t value;
+
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t msw;	/* Most significant word */
+		uint32_t lsw;	/* Least significant word */
+#elif defined(_LITTLE_ENDIAN)
+		uint32_t lsw;	/* Least significant word */
+		uint32_t msw;	/* Most significant word */
+#endif
+	} val;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t	w1;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t res0			: 25;
+		uint32_t mask			: 1;
+		uint32_t override_10ms_timer	: 1;
+		uint32_t jitter_study		: 2;
+		uint32_t sig_det_a_low		: 1;
+		uint32_t sig_det_override	: 1;
+		uint32_t enable			: 1;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t enable			: 1;
+		uint32_t sig_det_override	: 1;
+		uint32_t sig_det_a_low		: 1;
+		uint32_t jitter_study		: 2;
+		uint32_t override_10ms_timer	: 1;
+		uint32_t mask			: 1;
+		uint32_t res0			: 25;
+#endif
+		} w0;
+
+#if defined(_LITTLE_ENDIAN)
+		uint32_t	w1;
+#endif
+	} bits;
+} pcs_cfg_t, *p_pcs_cfg_t;
+
+
+/* used for diagnostic purposes. bits 20-22 autoclear on read */
+#define	PCS_SM_TX_STATE_MASK		0x0000000F /* Tx idle state mask */
+#define	PCS_SM_TX_STATE_SHIFT		0
+#define	PCS_SM_RX_STATE_MASK		0x000000F0 /* Rx idle state mask */
+#define	PCS_SM_RX_STATE_SHIFT		4
+#define	PCS_SM_WORD_SYNC_STATE_MASK	0x00000700 /* loss of sync state mask */
+#define	PCS_SM_WORD_SYNC_STATE_SHIFT	8
+#define	PCS_SM_SEQ_DETECT_STATE_MASK	0x00001800 /* sequence detect */
+#define	PCS_SM_SEQ_DETECT_STATE_SHIFT	11
+#define	PCS_SM_LINK_STATE_MASK		0x0001E000 /* link state */
+#define	PCS_SM_LINK_STATE_SHIFT		13
+#define	PCS_SM_LOSS_LINK_C		0x00100000 /* loss of link */
+#define	PCS_SM_LOSS_LINK_SYNC		0x00200000 /* loss of sync */
+#define	PCS_SM_LOSS_SIGNAL_DETECT	0x00400000 /* signal detect fail */
+#define	PCS_SM_NO_LINK_BREAKLINK	0x01000000 /* receipt of breaklink */
+#define	PCS_SM_NO_LINK_SERDES		0x02000000 /* serdes initializing */
+#define	PCS_SM_NO_LINK_C		0x04000000 /* C codes not stable */
+#define	PCS_SM_NO_LINK_SYNC		0x08000000 /* word sync not achieved */
+#define	PCS_SM_NO_LINK_WAIT_C		0x10000000 /* waiting for C codes */
+#define	PCS_SM_NO_LINK_NO_IDLE		0x20000000 /* linkpartner send C code */
+
+typedef union _pcs_stat_mc_t {
+	uint64_t value;
+
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t msw;	/* Most significant word */
+		uint32_t lsw;	/* Least significant word */
+#elif defined(_LITTLE_ENDIAN)
+		uint32_t lsw;	/* Least significant word */
+		uint32_t msw;	/* Most significant word */
+#endif
+	} val;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t	w1;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t res2		: 2;
+		uint32_t lnk_dwn_ni	: 1;
+		uint32_t lnk_dwn_wc	: 1;
+		uint32_t lnk_dwn_ls	: 1;
+		uint32_t lnk_dwn_nc	: 1;
+		uint32_t lnk_dwn_ser	: 1;
+		uint32_t lnk_loss_bc	: 1;
+		uint32_t res1		: 1;
+		uint32_t loss_sd	: 1;
+		uint32_t lnk_loss_sync	: 1;
+		uint32_t lnk_loss_c	: 1;
+		uint32_t res0		: 3;
+		uint32_t link_cfg_stat	: 4;
+		uint32_t seq_detc_stat	: 2;
+		uint32_t word_sync	: 3;
+		uint32_t rx_ctrl	: 4;
+		uint32_t tx_ctrl	: 4;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t tx_ctrl	: 4;
+		uint32_t rx_ctrl	: 4;
+		uint32_t word_sync	: 3;
+		uint32_t seq_detc_stat	: 2;
+		uint32_t link_cfg_stat	: 4;
+		uint32_t res0		: 3;
+		uint32_t lnk_loss_c	: 1;
+		uint32_t lnk_loss_sync	: 1;
+		uint32_t loss_sd	: 1;
+		uint32_t res1		: 1;
+		uint32_t lnk_loss_bc	: 1;
+		uint32_t lnk_dwn_ser	: 1;
+		uint32_t lnk_dwn_nc	: 1;
+		uint32_t lnk_dwn_ls	: 1;
+		uint32_t lnk_dwn_wc	: 1;
+		uint32_t lnk_dwn_ni	: 1;
+		uint32_t res2		: 2;
+#endif
+		} w0;
+
+#if defined(_LITTLE_ENDIAN)
+		uint32_t	w1;
+#endif
+	} bits;
+} pcs_stat_mc_t, *p_pcs_stat_mc_t;
+
+#define	PCS_INTR_STATUS_LINK_CHANGE	0x04	/* link status has changed */
+
+/*
+ * control which network interface is used. no more than one bit should
+ * be set.
+ */
+#define	PCS_DATAPATH_MODE_PCS		0	   /* Internal PCS is used */
+#define	PCS_DATAPATH_MODE_MII		0x00000002 /* GMII/RGMII is selected. */
+
+#define	PCS_PACKET_COUNT_TX_MASK	0x000007FF /* pkts xmitted by PCS */
+#define	PCS_PACKET_COUNT_RX_MASK	0x07FF0000 /* pkts recvd by PCS */
+#define	PCS_PACKET_COUNT_RX_SHIFT	16
+
+/*
+ * ******************** XPCS registers *********************************
+ */
+
+/* XPCS Base 10G Control1 Register */
+#define	XPCS_CTRL1_RST			0x8000 /* Self clearing reset. */
+#define	XPCS_CTRL1_LOOPBK		0x4000 /* xpcs Loopback */
+#define	XPCS_CTRL1_SPEED_SEL_3		0x2000 /* 1 indicates 10G speed */
+#define	XPCS_CTRL1_LOW_PWR		0x0800 /* low power mode. */
+#define	XPCS_CTRL1_SPEED_SEL_1		0x0040 /* 1 indicates 10G speed */
+#define	XPCS_CTRL1_SPEED_SEL_0_MASK	0x003c /* 0 indicates 10G speed. */
+#define	XPCS_CTRL1_SPEED_SEL_0_SHIFT	2
+
+
+
+typedef union _xpcs_ctrl1_t {
+	uint64_t value;
+
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t msw;	/* Most significant word */
+		uint32_t lsw;	/* Least significant word */
+#elif defined(_LITTLE_ENDIAN)
+		uint32_t lsw;	/* Least significant word */
+		uint32_t msw;	/* Most significant word */
+#endif
+	} val;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t	w1;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t res3		: 16;
+		uint32_t reset		: 1;
+		uint32_t csr_lb		: 1;
+		uint32_t csr_speed_sel3	: 1;
+		uint32_t res2		: 1;
+		uint32_t csr_low_pwr	: 1;
+		uint32_t res1		: 4;
+		uint32_t csr_speed_sel1	: 1;
+		uint32_t csr_speed_sel0	: 4;
+		uint32_t res0		: 2;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t res0		: 2;
+		uint32_t csr_speed_sel0	: 4;
+		uint32_t csr_speed_sel1	: 1;
+		uint32_t res1		: 4;
+		uint32_t csr_low_pwr	: 1;
+		uint32_t res2		: 1;
+		uint32_t csr_speed_sel3	: 1;
+		uint32_t csr_lb		: 1;
+		uint32_t reset		: 1;
+		uint32_t res3		: 16;
+#endif
+		} w0;
+
+#if defined(_LITTLE_ENDIAN)
+		uint32_t	w1;
+#endif
+	} bits;
+} xpcs_ctrl1_t;
+
+
+/* XPCS Base 10G Status1 Register (Read Only) */
+#define	XPCS_STATUS1_FAULT		0x0080
+#define	XPCS_STATUS1_RX_LINK_STATUS_UP	0x0004 /* Link status interrupt */
+#define	XPCS_STATUS1_LOW_POWER_ABILITY	0x0002 /* low power mode */
+#define	XPCS_STATUS_RX_LINK_STATUS_UP	0x1000 /* Link status interrupt */
+
+
+typedef	union _xpcs_stat1_t {
+	uint64_t value;
+
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t msw;	/* Most significant word */
+		uint32_t lsw;	/* Least significant word */
+#elif defined(_LITTLE_ENDIAN)
+		uint32_t lsw;	/* Least significant word */
+		uint32_t msw;	/* Most significant word */
+#endif
+	} val;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t	w1;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t res4			: 16;
+		uint32_t res3			: 8;
+		uint32_t csr_fault		: 1;
+		uint32_t res1			: 4;
+		uint32_t csr_rx_link_stat	: 1;
+		uint32_t csr_low_pwr_ability	: 1;
+		uint32_t res0			: 1;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t res0			: 1;
+		uint32_t csr_low_pwr_ability	: 1;
+		uint32_t csr_rx_link_stat	: 1;
+		uint32_t res1			: 4;
+		uint32_t csr_fault		: 1;
+		uint32_t res3			: 8;
+		uint32_t res4			: 16;
+#endif
+		} w0;
+
+#if defined(_LITTLE_ENDIAN)
+		uint32_t	w1;
+#endif
+	} bits;
+} xpcs_stat1_t;
+
+
+/* XPCS Base Speed Ability Register. Indicates 10G capability */
+#define	XPCS_SPEED_ABILITY_10_GIG	0x0001
+
+
+typedef	union _xpcs_speed_ab_t {
+	uint64_t value;
+
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t msw;	/* Most significant word */
+		uint32_t lsw;	/* Least significant word */
+#elif defined(_LITTLE_ENDIAN)
+		uint32_t lsw;	/* Least significant word */
+		uint32_t msw;	/* Most significant word */
+#endif
+	} val;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t	w1;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t res1		: 16;
+		uint32_t res0		: 15;
+		uint32_t csr_10gig	: 1;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t csr_10gig	: 1;
+		uint32_t res0		: 15;
+		uint32_t res1		: 16;
+#endif
+		} w0;
+
+#if defined(_LITTLE_ENDIAN)
+		uint32_t	w1;
+#endif
+	} bits;
+} xpcs_speed_ab_t;
+
+
+/* XPCS Base 10G Devices in Package Register */
+#define	XPCS_DEV_IN_PKG_CSR_VENDOR2	0x80000000
+#define	XPCS_DEV_IN_PKG_CSR_VENDOR1	0x40000000
+#define	XPCS_DEV_IN_PKG_DTE_XS		0x00000020
+#define	XPCS_DEV_IN_PKG_PHY_XS		0x00000010
+#define	XPCS_DEV_IN_PKG_PCS		0x00000008
+#define	XPCS_DEV_IN_PKG_WIS		0x00000004
+#define	XPCS_DEV_IN_PKG_PMD_PMA		0x00000002
+#define	XPCS_DEV_IN_PKG_CLS_22_REG	0x00000000
+
+
+
+typedef	union _xpcs_dev_in_pkg_t {
+	uint64_t value;
+
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t msw;	/* Most significant word */
+		uint32_t lsw;	/* Least significant word */
+#elif defined(_LITTLE_ENDIAN)
+		uint32_t lsw;	/* Least significant word */
+		uint32_t msw;	/* Most significant word */
+#endif
+	} val;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t	w1;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t csr_vendor2	: 1;
+		uint32_t csr_vendor1	: 1;
+		uint32_t res1		: 14;
+		uint32_t res0		: 10;
+		uint32_t dte_xs		: 1;
+		uint32_t phy_xs		: 1;
+		uint32_t pcs		: 1;
+		uint32_t wis		: 1;
+		uint32_t pmd_pma	: 1;
+		uint32_t clause_22_reg	: 1;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t clause_22_reg	: 1;
+		uint32_t pmd_pma	: 1;
+		uint32_t wis		: 1;
+		uint32_t pcs		: 1;
+		uint32_t phy_xs		: 1;
+		uint32_t dte_xs		: 1;
+		uint32_t res0		: 10;
+		uint32_t res1		: 14;
+		uint32_t csr_vendor1	: 1;
+		uint32_t csr_vendor2	: 1;
+#endif
+		} w0;
+
+#if defined(_LITTLE_ENDIAN)
+		uint32_t	w1;
+#endif
+	} bits;
+} xpcs_dev_in_pkg_t;
+
+
+/* XPCS Base 10G Control2 Register */
+#define	XPCS_PSC_SEL_MASK		0x0003
+#define	PSC_SEL_10G_BASE_X_PCS		0x0001
+
+
+typedef	union _xpcs_ctrl2_t {
+	uint64_t value;
+
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t msw;	/* Most significant word */
+		uint32_t lsw;	/* Least significant word */
+#elif defined(_LITTLE_ENDIAN)
+		uint32_t lsw;	/* Least significant word */
+		uint32_t msw;	/* Most significant word */
+#endif
+	} val;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t	w1;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t res1		: 16;
+		uint32_t res0		: 14;
+		uint32_t csr_psc_sel	: 2;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t csr_psc_sel	: 2;
+		uint32_t res0		: 14;
+		uint32_t res1		: 16;
+#endif
+		} w0;
+
+#if defined(_LITTLE_ENDIAN)
+		uint32_t	w1;
+#endif
+	} bits;
+} xpcs_ctrl2_t;
+
+
+/* XPCS Base10G Status2 Register */
+#define	XPCS_STATUS2_DEV_PRESENT_MASK	0xc000	/* ?????? */
+#define	XPCS_STATUS2_TX_FAULT		0x0800	/* Fault on tx path */
+#define	XPCS_STATUS2_RX_FAULT		0x0400	/* Fault on rx path */
+#define	XPCS_STATUS2_TEN_GBASE_W	0x0004	/* 10G-Base-W */
+#define	XPCS_STATUS2_TEN_GBASE_X	0x0002	/* 10G-Base-X */
+#define	XPCS_STATUS2_TEN_GBASE_R	0x0001	/* 10G-Base-R */
+
+typedef	union _xpcs_stat2_t {
+	uint64_t value;
+
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t msw;	/* Most significant word */
+		uint32_t lsw;	/* Least significant word */
+#elif defined(_LITTLE_ENDIAN)
+		uint32_t lsw;	/* Least significant word */
+		uint32_t msw;	/* Most significant word */
+#endif
+	} val;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t	w1;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t res2		: 16;
+		uint32_t csr_dev_pres	: 2;
+		uint32_t res1		: 2;
+		uint32_t csr_tx_fault	: 1;
+		uint32_t csr_rx_fault	: 1;
+		uint32_t res0		: 7;
+		uint32_t ten_gbase_w	: 1;
+		uint32_t ten_gbase_x	: 1;
+		uint32_t ten_gbase_r	: 1;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t ten_gbase_r	: 1;
+		uint32_t ten_gbase_x	: 1;
+		uint32_t ten_gbase_w	: 1;
+		uint32_t res0		: 7;
+		uint32_t csr_rx_fault	: 1;
+		uint32_t csr_tx_fault	: 1;
+		uint32_t res1		: 2;
+		uint32_t csr_dev_pres	: 2;
+		uint32_t res2		: 16;
+#endif
+		} w0;
+
+#if defined(_LITTLE_ENDIAN)
+		uint32_t	w1;
+#endif
+	} bits;
+} xpcs_stat2_t;
+
+
+
+/* XPCS Base10G Status Register */
+#define	XPCS_STATUS_LANE_ALIGN		0x1000 /* 10GBaseX PCS rx lanes align */
+#define	XPCS_STATUS_PATTERN_TEST_ABLE	0x0800 /* able to generate patterns. */
+#define	XPCS_STATUS_LANE3_SYNC		0x0008 /* Lane 3 is synchronized */
+#define	XPCS_STATUS_LANE2_SYNC		0x0004 /* Lane 2 is synchronized */
+#define	XPCS_STATUS_LANE1_SYNC		0x0002 /* Lane 1 is synchronized */
+#define	XPCS_STATUS_LANE0_SYNC		0x0001 /* Lane 0 is synchronized */
+
+typedef	union _xpcs_stat_t {
+	uint64_t value;
+
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t msw;	/* Most significant word */
+		uint32_t lsw;	/* Least significant word */
+#elif defined(_LITTLE_ENDIAN)
+		uint32_t lsw;	/* Least significant word */
+		uint32_t msw;	/* Most significant word */
+#endif
+	} val;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t	w1;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t res2			: 16;
+		uint32_t res1			: 3;
+		uint32_t csr_lane_align		: 1;
+		uint32_t csr_pattern_test_able	: 1;
+		uint32_t res0			: 7;
+		uint32_t csr_lane3_sync		: 1;
+		uint32_t csr_lane2_sync		: 1;
+		uint32_t csr_lane1_sync		: 1;
+		uint32_t csr_lane0_sync		: 1;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t csr_lane0_sync		: 1;
+		uint32_t csr_lane1_sync		: 1;
+		uint32_t csr_lane2_sync		: 1;
+		uint32_t csr_lane3_sync		: 1;
+		uint32_t res0			: 7;
+		uint32_t csr_pat_test_able	: 1;
+		uint32_t csr_lane_align		: 1;
+		uint32_t res1			: 3;
+		uint32_t res2			: 16;
+#endif
+		} w0;
+
+#if defined(_LITTLE_ENDIAN)
+		uint32_t	w1;
+#endif
+	} bits;
+} xpcs_stat_t;
+
+/* XPCS Base10G Test Control Register */
+#define	XPCS_TEST_CTRL_TX_TEST_ENABLE		0x0004
+#define	XPCS_TEST_CTRL_TEST_PATTERN_SEL_MASK	0x0003
+#define	TEST_PATTERN_HIGH_FREQ			0
+#define	TEST_PATTERN_LOW_FREQ			1
+#define	TEST_PATTERN_MIXED_FREQ			2
+
+typedef	union _xpcs_test_ctl_t {
+	uint64_t value;
+
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t msw;	/* Most significant word */
+		uint32_t lsw;	/* Least significant word */
+#elif defined(_LITTLE_ENDIAN)
+		uint32_t lsw;	/* Least significant word */
+		uint32_t msw;	/* Most significant word */
+#endif
+	} val;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t	w1;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t res1			: 16;
+		uint32_t res0			: 13;
+		uint32_t csr_tx_test_en		: 1;
+		uint32_t csr_test_pat_sel	: 2;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t csr_test_pat_sel	: 2;
+		uint32_t csr_tx_test_en		: 1;
+		uint32_t res0			: 13;
+		uint32_t res1			: 16;
+#endif
+		} w0;
+
+#if defined(_LITTLE_ENDIAN)
+		uint32_t	w1;
+#endif
+	} bits;
+} xpcs_test_ctl_t;
+
+/* XPCS Base10G Diagnostic Register */
+#define	XPCS_DIAG_EB_ALIGN_ERR3		0x40
+#define	XPCS_DIAG_EB_ALIGN_ERR2		0x20
+#define	XPCS_DIAG_EB_ALIGN_ERR1		0x10
+#define	XPCS_DIAG_EB_DESKEW_OK		0x08
+#define	XPCS_DIAG_EB_ALIGN_DET3		0x04
+#define	XPCS_DIAG_EB_ALIGN_DET2		0x02
+#define	XPCS_DIAG_EB_ALIGN_DET1		0x01
+#define	XPCS_DIAG_EB_DESKEW_LOSS	0
+
+#define	XPCS_DIAG_SYNC_3_INVALID	0x8
+#define	XPCS_DIAG_SYNC_2_INVALID	0x4
+#define	XPCS_DIAG_SYNC_1_INVALID	0x2
+#define	XPCS_DIAG_SYNC_IN_SYNC		0x1
+#define	XPCS_DIAG_SYNC_LOSS_SYNC	0
+
+#define	XPCS_RX_SM_RECEIVE_STATE	1
+#define	XPCS_RX_SM_FAULT_STATE		0
+
+typedef	union _xpcs_diag_t {
+	uint64_t value;
+
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t msw;	/* Most significant word */
+		uint32_t lsw;	/* Least significant word */
+#elif defined(_LITTLE_ENDIAN)
+		uint32_t lsw;	/* Least significant word */
+		uint32_t msw;	/* Most significant word */
+#endif
+	} val;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t	w1;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t res1			: 7;
+		uint32_t sync_sm_lane3		: 4;
+		uint32_t sync_sm_lane2		: 4;
+		uint32_t sync_sm_lane1		: 4;
+		uint32_t sync_sm_lane0		: 4;
+		uint32_t elastic_buffer_sm	: 8;
+		uint32_t receive_sm		: 1;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t receive_sm		: 1;
+		uint32_t elastic_buffer_sm	: 8;
+		uint32_t sync_sm_lane0		: 4;
+		uint32_t sync_sm_lane1		: 4;
+		uint32_t sync_sm_lane2		: 4;
+		uint32_t sync_sm_lane3		: 4;
+		uint32_t res1			: 7;
+#endif
+		} w0;
+
+#if defined(_LITTLE_ENDIAN)
+		uint32_t	w1;
+#endif
+	} bits;
+} xpcs_diag_t;
+
+/* XPCS Base10G Tx State Machine Register */
+#define	XPCS_TX_SM_SEND_UNDERRUN	0x9
+#define	XPCS_TX_SM_SEND_RANDOM_Q	0x8
+#define	XPCS_TX_SM_SEND_RANDOM_K	0x7
+#define	XPCS_TX_SM_SEND_RANDOM_A	0x6
+#define	XPCS_TX_SM_SEND_RANDOM_R	0x5
+#define	XPCS_TX_SM_SEND_Q		0x4
+#define	XPCS_TX_SM_SEND_K		0x3
+#define	XPCS_TX_SM_SEND_A		0x2
+#define	XPCS_TX_SM_SEND_SDP		0x1
+#define	XPCS_TX_SM_SEND_DATA		0
+
+/* XPCS Base10G Configuration Register */
+#define	XPCS_CFG_VENDOR_DBG_SEL_MASK	0x78
+#define	XPCS_CFG_VENDOR_DBG_SEL_SHIFT	3
+#define	XPCS_CFG_BYPASS_SIG_DETECT	0x0004
+#define	XPCS_CFG_ENABLE_TX_BUFFERS	0x0002
+#define	XPCS_CFG_XPCS_ENABLE		0x0001
+
+typedef	union _xpcs_config_t {
+	uint64_t value;
+
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t msw;	/* Most significant word */
+		uint32_t lsw;	/* Least significant word */
+#elif defined(_LITTLE_ENDIAN)
+		uint32_t lsw;	/* Least significant word */
+		uint32_t msw;	/* Most significant word */
+#endif
+	} val;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t	w1;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t res1			: 16;
+		uint32_t res0			: 9;
+		uint32_t csr_vendor_dbg_sel	: 4;
+		uint32_t csr_bypass_sig_detect	: 1;
+		uint32_t csr_en_tx_buf		: 1;
+		uint32_t csr_xpcs_en		: 1;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t csr_xpcs_en		: 1;
+		uint32_t csr_en_tx_buf		: 1;
+		uint32_t csr_bypass_sig_detect	: 1;
+		uint32_t csr_vendor_dbg_sel	: 4;
+		uint32_t res0			: 9;
+		uint32_t res1			: 16;
+#endif
+		} w0;
+
+#if defined(_LITTLE_ENDIAN)
+		uint32_t	w1;
+#endif
+	} bits;
+} xpcs_config_t;
+
+
+
+/* XPCS Base10G Mask1 Register */
+#define	XPCS_MASK1_FAULT_MASK		0x0080	/* mask fault interrupt. */
+#define	XPCS_MASK1_RX_LINK_STATUS_MASK	0x0040	/* mask linkstat interrupt */
+
+/* XPCS Base10G Packet Counter */
+#define	XPCS_PKT_CNTR_TX_PKT_CNT_MASK	0xffff0000
+#define	XPCS_PKT_CNTR_TX_PKT_CNT_SHIFT	16
+#define	XPCS_PKT_CNTR_RX_PKT_CNT_MASK	0x0000ffff
+#define	XPCS_PKT_CNTR_RX_PKT_CNT_SHIFT	0
+
+/* XPCS Base10G TX State Machine status register */
+#define	XPCS_TX_STATE_MC_TX_STATE_MASK	0x0f
+#define	XPCS_DESKEW_ERR_CNTR_MASK	0xff
+
+/* XPCS Base10G Lane symbol error counters */
+#define	XPCS_SYM_ERR_CNT_L1_MASK  0xffff0000
+#define	XPCS_SYM_ERR_CNT_L0_MASK  0x0000ffff
+#define	XPCS_SYM_ERR_CNT_L3_MASK  0xffff0000
+#define	XPCS_SYM_ERR_CNT_L2_MASK  0x0000ffff
+
+#define	XPCS_SYM_ERR_CNT_MULTIPLIER	16
+
+/* ESR Reset Register */
+#define	ESR_RESET_1			2
+#define	ESR_RESET_0			1
+
+/* ESR Configuration Register */
+#define	ESR_BLUNT_END_LOOPBACK		2
+#define	ESR_FORCE_SERDES_SERDES_RDY	1
+
+/* ESR Neptune Serdes PLL Configuration */
+#define	ESR_PLL_CFG_FBDIV_0		0x1
+#define	ESR_PLL_CFG_FBDIV_1		0x2
+#define	ESR_PLL_CFG_FBDIV_2		0x4
+#define	ESR_PLL_CFG_HALF_RATE_0		0x8
+#define	ESR_PLL_CFG_HALF_RATE_1		0x10
+#define	ESR_PLL_CFG_HALF_RATE_2		0x20
+#define	ESR_PLL_CFG_HALF_RATE_3		0x40
+#define	ESR_PLL_CFG_1G_SERDES		(ESR_PLL_CFG_FBDIV_0 |		\
+					ESR_PLL_CFG_HALF_RATE_0 |	\
+					ESR_PLL_CFG_HALF_RATE_1 |	\
+					ESR_PLL_CFG_HALF_RATE_2 |	\
+					ESR_PLL_CFG_HALF_RATE_3)
+
+#define	ESR_PLL_CFG_10G_SERDES		ESR_PLL_CFG_FBDIV_2
+
+/* ESR Neptune Serdes Control Register */
+#define	ESR_CTL_EN_SYNCDET_0		0x00000001
+#define	ESR_CTL_EN_SYNCDET_1		0x00000002
+#define	ESR_CTL_EN_SYNCDET_2		0x00000004
+#define	ESR_CTL_EN_SYNCDET_3		0x00000008
+#define	ESR_CTL_OUT_EMPH_0_MASK		0x00000070
+#define	ESR_CTL_OUT_EMPH_0_SHIFT	4
+#define	ESR_CTL_OUT_EMPH_1_MASK		0x00000380
+#define	ESR_CTL_OUT_EMPH_1_SHIFT	7
+#define	ESR_CTL_OUT_EMPH_2_MASK		0x00001c00
+#define	ESR_CTL_OUT_EMPH_2_SHIFT	10
+#define	ESR_CTL_OUT_EMPH_3_MASK		0x0000e000
+#define	ESR_CTL_OUT_EMPH_3_SHIFT	13
+#define	ESR_CTL_LOSADJ_0_MASK		0x00070000
+#define	ESR_CTL_LOSADJ_0_SHIFT		16
+#define	ESR_CTL_LOSADJ_1_MASK		0x00380000
+#define	ESR_CTL_LOSADJ_1_SHIFT		19
+#define	ESR_CTL_LOSADJ_2_MASK		0x01c00000
+#define	ESR_CTL_LOSADJ_2_SHIFT		22
+#define	ESR_CTL_LOSADJ_3_MASK		0x0e000000
+#define	ESR_CTL_LOSADJ_3_SHIFT		25
+#define	ESR_CTL_RXITERM_0		0x10000000
+#define	ESR_CTL_RXITERM_1		0x20000000
+#define	ESR_CTL_RXITERM_2		0x40000000
+#define	ESR_CTL_RXITERM_3		0x80000000
+#define	ESR_CTL_1G_SERDES		(ESR_CTL_EN_SYNCDET_0 |	\
+					ESR_CTL_EN_SYNCDET_1 |	\
+					ESR_CTL_EN_SYNCDET_2 |	\
+					ESR_CTL_EN_SYNCDET_3 |	\
+					(0x1 << ESR_CTL_OUT_EMPH_0_SHIFT) | \
+					(0x1 << ESR_CTL_OUT_EMPH_1_SHIFT) | \
+					(0x1 << ESR_CTL_OUT_EMPH_2_SHIFT) | \
+					(0x1 << ESR_CTL_OUT_EMPH_3_SHIFT) | \
+					(0x1 << ESR_CTL_OUT_EMPH_3_SHIFT) | \
+					(0x1 << ESR_CTL_LOSADJ_0_SHIFT) | \
+					(0x1 << ESR_CTL_LOSADJ_1_SHIFT) | \
+					(0x1 << ESR_CTL_LOSADJ_2_SHIFT) | \
+					(0x1 << ESR_CTL_LOSADJ_3_SHIFT))
+
+/* ESR Neptune Serdes Test Configuration Register */
+#define	ESR_TSTCFG_LBTEST_MD_0_MASK	0x00000003
+#define	ESR_TSTCFG_LBTEST_MD_0_SHIFT	0
+#define	ESR_TSTCFG_LBTEST_MD_1_MASK	0x0000000c
+#define	ESR_TSTCFG_LBTEST_MD_1_SHIFT	2
+#define	ESR_TSTCFG_LBTEST_MD_2_MASK	0x00000030
+#define	ESR_TSTCFG_LBTEST_MD_2_SHIFT	4
+#define	ESR_TSTCFG_LBTEST_MD_3_MASK	0x000000c0
+#define	ESR_TSTCFG_LBTEST_MD_3_SHIFT	6
+#define ESR_TSTCFG_LBTEST_PAD		(ESR_PAD_LOOPBACK_CH3 | \
+					ESR_PAD_LOOPBACK_CH2 | \
+					ESR_PAD_LOOPBACK_CH1 | \
+					ESR_PAD_LOOPBACK_CH0)
+
+/* ESR Neptune Ethernet RGMII Configuration Register */
+#define	ESR_RGMII_PT0_IN_USE		0x00000001
+#define	ESR_RGMII_PT1_IN_USE		0x00000002
+#define	ESR_RGMII_PT2_IN_USE		0x00000004
+#define	ESR_RGMII_PT3_IN_USE		0x00000008
+#define	ESR_RGMII_REG_RW_TEST		0x00000010
+
+/* ESR Internal Signals Observation Register */
+#define	ESR_SIG_MASK			0xFFFFFFFF
+#define	ESR_SIG_P0_BITS_MASK		0x33E0000F
+#define	ESR_SIG_P1_BITS_MASK		0x0C1F00F0
+#define	ESR_SIG_SERDES_RDY0_P0		0x20000000
+#define	ESR_SIG_DETECT0_P0		0x10000000
+#define	ESR_SIG_SERDES_RDY0_P1		0x08000000
+#define	ESR_SIG_DETECT0_P1		0x04000000
+#define	ESR_SIG_XSERDES_RDY_P0		0x02000000
+#define	ESR_SIG_XDETECT_P0_CH3		0x01000000
+#define	ESR_SIG_XDETECT_P0_CH2		0x00800000
+#define	ESR_SIG_XDETECT_P0_CH1		0x00400000
+#define	ESR_SIG_XDETECT_P0_CH0		0x00200000
+#define	ESR_SIG_XSERDES_RDY_P1		0x00100000
+#define	ESR_SIG_XDETECT_P1_CH3		0x00080000
+#define	ESR_SIG_XDETECT_P1_CH2		0x00040000
+#define	ESR_SIG_XDETECT_P1_CH1		0x00020000
+#define	ESR_SIG_XDETECT_P1_CH0		0x00010000
+#define	ESR_SIG_LOS_P1_CH3		0x00000080
+#define	ESR_SIG_LOS_P1_CH2		0x00000040
+#define	ESR_SIG_LOS_P1_CH1		0x00000020
+#define	ESR_SIG_LOS_P1_CH0		0x00000010
+#define	ESR_SIG_LOS_P0_CH3		0x00000008
+#define	ESR_SIG_LOS_P0_CH2		0x00000004
+#define	ESR_SIG_LOS_P0_CH1		0x00000002
+#define	ESR_SIG_LOS_P0_CH0		0x00000001
+#define	ESR_SIG_P0_BITS_MASK_1G		(ESR_SIG_SERDES_RDY0_P0 | ESR_SIG_DETECT0_P0)
+#define	ESR_SIG_P1_BITS_MASK_1G		(ESR_SIG_SERDES_RDY0_P1 | ESR_SIG_DETECT0_P1)
+
+/* ESR Debug Selection Register */
+#define	ESR_DEBUG_SEL_MASK		0x00000003f
+
+/* ESR Test Configuration Register */
+#define	ESR_NO_LOOPBACK_CH3		(0x0 << 6)
+#define	ESR_EWRAP_CH3			(0x1 << 6)
+#define	ESR_PAD_LOOPBACK_CH3		(0x2 << 6)
+#define	ESR_REVLOOPBACK_CH3		(0x3 << 6)
+#define	ESR_NO_LOOPBACK_CH2		(0x0 << 4)
+#define	ESR_EWRAP_CH2			(0x1 << 4)
+#define	ESR_PAD_LOOPBACK_CH2		(0x2 << 4)
+#define	ESR_REVLOOPBACK_CH2		(0x3 << 4)
+#define	ESR_NO_LOOPBACK_CH1		(0x0 << 2)
+#define	ESR_EWRAP_CH1			(0x1 << 2)
+#define	ESR_PAD_LOOPBACK_CH1		(0x2 << 2)
+#define	ESR_REVLOOPBACK_CH1		(0x3 << 2)
+#define	ESR_NO_LOOPBACK_CH0		0x0
+#define	ESR_EWRAP_CH0			0x1
+#define	ESR_PAD_LOOPBACK_CH0		0x2
+#define	ESR_REVLOOPBACK_CH0		0x3
+
+/* convert values */
+#define	NXGE_BASE(x, y)	\
+	(((y) << (x ## _SHIFT)) & (x ## _MASK))
+
+#define	NXGE_VAL_GET(fieldname, regval)		\
+	(((regval) & ((fieldname) ## _MASK)) >> ((fieldname) ## _SHIFT))
+
+#define	NXGE_VAL_SET(fieldname, regval, val)		\
+{							\
+	(regval) &= ~((fieldname) ## _MASK);		\
+	(regval) |= ((val) << (fieldname ## _SHIFT)); 	\
+}
+
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif	/* _SYS_MAC_NXGE_MAC_HW_H */
diff --git a/drivers/net/nxge/include/nxge_mii.h b/drivers/net/nxge/include/nxge_mii.h
new file mode 100644
index 0000000..a3a406b
--- /dev/null
+++ b/drivers/net/nxge/include/nxge_mii.h
@@ -0,0 +1,431 @@
+/*
+ * nxge_mii.h	Neptune MII interface
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef _SYS_NXGE_NXGE_MII_H_
+#define	_SYS_NXGE_NXGE_MII_H_
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/*
+ * Configuration Register space.
+ */
+
+#define	NXGE_MII_BMCR		0
+#define	NXGE_MII_BMSR		1
+#define	NXGE_MII_IDR1		2
+#define	NXGE_MII_IDR2		3
+#define	NXGE_MII_ANAR		4
+#define	NXGE_MII_ANLPAR		5
+#define	NXGE_MII_ANER		6
+#define	NXGE_MII_NPTXR		7
+#define	NXGE_MII_LPRXNPR	8
+#define	NXGE_MII_GCR		9
+#define	NXGE_MII_GSR		10
+#define	NXGE_MII_RES0		11
+#define	NXGE_MII_RES1		12
+#define	NXGE_MII_RES2		13
+#define	NXGE_MII_RES3		14
+#define	NXGE_MII_ESR		15
+#define	NXGE_MII_SHADOW		0x1c
+/* Shadow register definitions */
+#define NXGE_MII_MODE_CONTROL_REG	0x1f
+
+#define	NXGE_MAX_MII_REGS	32
+
+/*
+ * Configuration Register space.
+ */
+typedef struct _mii_regs {
+	uchar_t bmcr;		/* Basic mode control register */
+	uchar_t bmsr;		/* Basic mode status register */
+	uchar_t idr1;		/* Phy identifier register 1 */
+	uchar_t idr2;		/* Phy identifier register 2 */
+	uchar_t anar;		/* Auto-Negotiation advertisement register */
+	uchar_t anlpar;		/* Auto-Negotiation link Partner ability reg */
+	uchar_t aner;		/* Auto-Negotiation expansion register */
+	uchar_t nptxr;		/* Next page transmit register */
+	uchar_t lprxnpr;	/* Link partner received next page register */
+	uchar_t gcr;		/* Gigabit basic mode control register. */
+	uchar_t gsr;		/* Gigabit basic mode status register */
+	uchar_t mii_res1[4];	/* For future use by MII working group */
+	uchar_t esr;		/* Extended status register. */
+	uchar_t vendor_res[12];	/* For future use by Phy Vendors */
+	uchar_t shadow;
+	uchar_t vendor_res2[4];	/* For future use by Phy Vendors */
+} mii_regs_t, *p_mii_regs_t;
+
+/*
+ * MII Register 0: Basic mode control register.
+ */
+
+typedef union _mii_bmcr {
+	uint16_t value;
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t reset:1;
+		uint16_t loopback:1;
+		uint16_t speed_sel:1;
+		uint16_t enable_autoneg:1;
+		uint16_t power_down:1;
+		uint16_t isolate:1;
+		uint16_t restart_autoneg:1;
+		uint16_t duplex_mode:1;
+		uint16_t col_test:1;
+		uint16_t speed_1000_sel:1;
+		uint16_t res1:6;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint16_t res1:6;
+		uint16_t speed_1000_sel:1;
+		uint16_t col_test:1;
+		uint16_t duplex_mode:1;
+		uint16_t restart_autoneg:1;
+		uint16_t isolate:1;
+		uint16_t power_down:1;
+		uint16_t enable_autoneg:1;
+		uint16_t speed_sel:1;
+		uint16_t loopback:1;
+		uint16_t reset:1;
+#endif
+	} bits;
+} mii_bmcr_t, *p_mii_bmcr_t;
+
+/*
+ * MII Register 1:  Basic mode status register.
+ */
+
+typedef union _mii_bmsr {
+	uint16_t value;
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t link_100T4:1;
+		uint16_t link_100fdx:1;
+		uint16_t link_100hdx:1;
+		uint16_t link_10fdx:1;
+		uint16_t link_10hdx:1;
+		uint16_t res2:2;
+		uint16_t extend_status:1;
+		uint16_t res1:1;
+		uint16_t preamble_supress:1;
+		uint16_t auto_neg_complete:1;
+		uint16_t remote_fault:1;
+		uint16_t auto_neg_able:1;
+		uint16_t link_status:1;
+		uint16_t jabber_detect:1;
+		uint16_t ext_cap:1;
+#elif defined(_BIT_FIELDS_LTOH)
+		int16_t ext_cap:1;
+		uint16_t jabber_detect:1;
+		uint16_t link_status:1;
+		uint16_t auto_neg_able:1;
+		uint16_t remote_fault:1;
+		uint16_t auto_neg_complete:1;
+		uint16_t preamble_supress:1;
+		uint16_t res1:1;
+		uint16_t extend_status:1;
+		uint16_t res2:2;
+		uint16_t link_10hdx:1;
+		uint16_t link_10fdx:1;
+		uint16_t link_100hdx:1;
+		uint16_t link_100fdx:1;
+		uint16_t link_100T4:1;
+#endif
+	} bits;
+} mii_bmsr_t, *p_mii_bmsr_t;
+
+/*
+ * MII Register 2: Physical Identifier 1.
+ */
+/* contains BCM OUI bits [3:18] */
+typedef union _mii_idr1 {
+	uint16_t value;
+	struct {
+		uint16_t ieee_address:16;
+	} bits;
+} mii_idr1_t, *p_mii_idr1_t;
+
+/*
+ * MII Register 3: Physical Identifier 2.
+ */
+typedef union _mii_idr2 {
+	uint16_t value;
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t ieee_address:6;
+		uint16_t model_no:6;
+		uint16_t rev_no:4;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint16_t rev_no:4;
+		uint16_t model_no:6;
+		uint16_t ieee_address:6;
+#endif
+	} bits;
+} mii_idr2_t, *p_mii_idr2_t;
+
+/*
+ * MII Register 4: Auto-negotiation advertisement register.
+ */
+
+typedef union _mii_anar {
+	uint16_t value;
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t np_indication:1;
+		uint16_t acknowledge:1;
+		uint16_t remote_fault:1;
+		uint16_t res1:1;
+		uint16_t cap_asmpause:1;
+		uint16_t cap_pause:1;
+		uint16_t cap_100T4:1;
+		uint16_t cap_100fdx:1;
+		uint16_t cap_100hdx:1;
+		uint16_t cap_10fdx:1;
+		uint16_t cap_10hdx:1;
+		uint16_t selector:5;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint16_t selector:5;
+		uint16_t cap_10hdx:1;
+		uint16_t cap_10fdx:1;
+		uint16_t cap_100hdx:1;
+		uint16_t cap_100fdx:1;
+		uint16_t cap_100T4:1;
+		uint16_t cap_pause:1;
+		uint16_t cap_asmpause:1;
+		uint16_t res1:1;
+		uint16_t remote_fault:1;
+		uint16_t acknowledge:1;
+		uint16_t np_indication:1;
+#endif
+	} bits;
+} mii_anar_t, *p_mii_anar_t;
+
+/*
+ * MII Register 5: Auto-negotiation link partner ability register.
+ */
+
+typedef mii_anar_t mii_anlpar_t, *pmii_anlpar_t;
+
+/*
+ * MII Register 6: Auto-negotiation expansion register.
+ */
+
+typedef union _mii_aner {
+	uint16_t value;
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t res:11;
+		uint16_t mlf:1;
+		uint16_t lp_np_able:1;
+		uint16_t np_able:1;
+		uint16_t page_rx:1;
+		uint16_t lp_an_able:1;
+#else
+		uint16_t lp_an_able:1;
+		uint16_t page_rx:1;
+		uint16_t np_able:1;
+		uint16_t lp_np_able:1;
+		uint16_t mlf:1;
+		uint16_t res:11;
+#endif
+	} bits;
+} mii_aner_t, *p_mii_aner_t;
+
+/*
+ * MII Register 7: Next page transmit register.
+ */
+typedef	union _mii_nptxr {
+	uint16_t value;
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t np:1;
+		uint16_t res:1;
+		uint16_t msgp:1;
+		uint16_t ack2:1;
+		uint16_t toggle:1;
+		uint16_t res1:11;
+#else
+		uint16_t res1:11;
+		uint16_t toggle:1;
+		uint16_t ack2:1;
+		uint16_t msgp:1;
+		uint16_t res:1;
+		uint16_t np:1;
+#endif
+	} bits;
+} mii_nptxr_t, *p_mii_nptxr_t;
+
+/*
+ * MII Register 8: Link partner received next page register.
+ */
+typedef union _mii_lprxnpr {
+	uint16_t value;
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t np:1;
+			uint16_t ack:1;
+		uint16_t msgp:1;
+		uint16_t ack2:1;
+		uint16_t toggle:1;
+		uint16_t mcf:11;
+#else
+		uint16_t mcf:11;
+		uint16_t toggle:1;
+		uint16_t ack2:1;
+		uint16_t msgp:1;
+		uint16_t ack:1;
+		uint16_t np:1;
+#endif
+	} bits;
+} mii_lprxnpr_t, *p_mii_lprxnpr_t;
+
+/*
+ * MII Register 9: 1000BaseT control register.
+ */
+typedef union _mii_gcr {
+	uint16_t value;
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t test_mode:3;
+		uint16_t ms_mode_en:1;
+		uint16_t master:1;
+		uint16_t dte_or_repeater:1;
+		uint16_t link_1000fdx:1;
+		uint16_t link_1000hdx:1;
+		uint16_t res:8;
+#else
+		uint16_t res:8;
+		uint16_t link_1000hdx:1;
+		uint16_t link_1000fdx:1;
+		uint16_t dte_or_repeater:1;
+		uint16_t master:1;
+		uint16_t ms_mode_en:1;
+		uint16_t test_mode:3;
+#endif
+	} bits;
+} mii_gcr_t, *p_mii_gcr_t;
+
+/*
+ * MII Register 10: 1000BaseT status register.
+ */
+typedef union _mii_gsr {
+	uint16_t value;
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t ms_config_fault:1;
+		uint16_t ms_resolve:1;
+		uint16_t local_rx_status:1;
+		uint16_t remote_rx_status:1;
+		uint16_t link_1000fdx:1;
+		uint16_t link_1000hdx:1;
+		uint16_t res:2;
+		uint16_t idle_err_cnt:8;
+#else
+		uint16_t idle_err_cnt:8;
+		uint16_t res:2;
+		uint16_t link_1000hdx:1;
+		uint16_t link_1000fdx:1;
+		uint16_t remote_rx_status:1;
+		uint16_t local_rx_status:1;
+		uint16_t ms_resolve:1;
+		uint16_t ms_config_fault:1;
+#endif
+	} bits;
+} mii_gsr_t, *p_mii_gsr_t;
+
+/*
+ * MII Register 15: Extended status register.
+ */
+typedef union _mii_esr {
+	uint16_t value;
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t link_1000Xfdx:1;
+		uint16_t link_1000Xhdx:1;
+		uint16_t link_1000fdx:1;
+		uint16_t link_1000hdx:1;
+		uint16_t res:12;
+#else
+			uint16_t res:12;
+		uint16_t link_1000hdx:1;
+		uint16_t link_1000fdx:1;
+		uint16_t link_1000Xhdx:1;
+		uint16_t link_1000Xfdx:1;
+#endif
+	} bits;
+} mii_esr_t, *p_mii_esr_t;
+
+#define	NXGE_MODE_SELECT_FIBER	0x01
+
+/* Shadow regiser 0x11111 */
+typedef union _mii_mode_control_stat {
+	uint16_t value;
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t write_enable:1;
+		uint16_t shadow:5;
+		uint16_t rsv:1;
+		uint16_t change:1;
+		uint16_t copper:1;
+		uint16_t fiber:1;
+		uint16_t copper_energy:1;
+		uint16_t fiber_signal:1;
+		uint16_t rsv1:1;
+		uint16_t mode:2;
+		uint16_t enable:1;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint16_t enable:1;
+		uint16_t mode:2;
+		uint16_t rsv1:1;
+		uint16_t fiber_signal:1;
+		uint16_t copper_energy:1;
+		uint16_t fiber:1;
+		uint16_t copper:1;
+		uint16_t change:1;
+		uint16_t rsv:1;
+		uint16_t shadow:5;
+		uint16_t write_enable:1;
+#endif
+	} bits;
+} mii_mode_control_stat_t, *p_mode_control_stat_t;
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* _SYS_NXGE_NXGE_MII_H_ */
diff --git a/drivers/net/nxge/include/nxge_n2_esr_hw.h b/drivers/net/nxge/include/nxge_n2_esr_hw.h
new file mode 100644
index 0000000..09e0266
--- /dev/null
+++ b/drivers/net/nxge/include/nxge_n2_esr_hw.h
@@ -0,0 +1,374 @@
+/*
+ * nxge_n2_esr_hw.h	Neptune ESR HW register offsets
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef _SYS_NXGE_NXGE_N2_ESR_HW_H
+#define	_SYS_NXGE_NXGE_N2_ESR_HW_H
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#define	ESR_N2_DEV_ADDR		0x1E
+#define	ESR_N2_BASE		0x8000
+
+/*
+ * Definitions for TI WIZ6C2xxN2x0 Macro Family.
+ */
+
+/* Register Blocks base address */
+
+#define	ESR_N2_PLL_REG_OFFSET		0
+#define	ESR_N2_TEST_REG_OFFSET		0x004
+#define	ESR_N2_TX_REG_OFFSET		0x100
+#define	ESR_N2_TX_0_REG_OFFSET		0x100
+#define	ESR_N2_TX_1_REG_OFFSET		0x104
+#define	ESR_N2_TX_2_REG_OFFSET		0x108
+#define	ESR_N2_TX_3_REG_OFFSET		0x10c
+#define	ESR_N2_TX_4_REG_OFFSET		0x110
+#define	ESR_N2_TX_5_REG_OFFSET		0x114
+#define	ESR_N2_TX_6_REG_OFFSET		0x118
+#define	ESR_N2_TX_7_REG_OFFSET		0x11c
+#define	ESR_N2_RX_REG_OFFSET		0x120
+#define	ESR_N2_RX_0_REG_OFFSET		0x120
+#define	ESR_N2_RX_1_REG_OFFSET		0x124
+#define	ESR_N2_RX_2_REG_OFFSET		0x128
+#define	ESR_N2_RX_3_REG_OFFSET		0x12c
+#define	ESR_N2_RX_4_REG_OFFSET		0x130
+#define	ESR_N2_RX_5_REG_OFFSET		0x134
+#define	ESR_N2_RX_6_REG_OFFSET		0x138
+#define	ESR_N2_RX_7_REG_OFFSET		0x13c
+#define	ESR_N2_P1_REG_OFFSET		0x400
+
+/* Register address */
+
+#define	ESR_N2_PLL_CFG_REG		ESR_N2_BASE + ESR_N2_PLL_REG_OFFSET
+#define	ESR_N2_PLL_CFG_L_REG	ESR_N2_BASE + ESR_N2_PLL_REG_OFFSET
+#define	ESR_N2_PLL_CFG_H_REG	ESR_N2_BASE + ESR_N2_PLL_REG_OFFSET + 1
+#define	ESR_N2_PLL_STS_REG		ESR_N2_BASE + ESR_N2_PLL_REG_OFFSET + 2
+#define	ESR_N2_PLL_STS_L_REG	ESR_N2_BASE + ESR_N2_PLL_REG_OFFSET + 2
+#define	ESR_N2_PLL_STS_H_REG	ESR_N2_BASE + ESR_N2_PLL_REG_OFFSET + 3
+#define	ESR_N2_TEST_CFG_REG		ESR_N2_BASE + ESR_N2_TEST_REG_OFFSET
+#define	ESR_N2_TEST_CFG_L_REG	ESR_N2_BASE + ESR_N2_TEST_REG_OFFSET
+#define	ESR_N2_TEST_CFG_H_REG	ESR_N2_BASE + ESR_N2_TEST_REG_OFFSET + 1
+
+#define	ESR_N2_TX_CFG_REG_ADDR(chan)	(ESR_N2_BASE + ESR_N2_TX_REG_OFFSET +\
+					(chan * 4))
+#define	ESR_N2_TX_CFG_L_REG_ADDR(chan)	(ESR_N2_BASE + ESR_N2_TX_REG_OFFSET +\
+					(chan * 4))
+#define	ESR_N2_TX_CFG_H_REG_ADDR(chan)	(ESR_N2_BASE + ESR_N2_TX_REG_OFFSET +\
+					(chan * 4) + 1)
+#define	ESR_N2_TX_STS_REG_ADDR(chan)	(ESR_N2_BASE + ESR_N2_TX_REG_OFFSET +\
+					(chan * 4) + 2)
+#define	ESR_N2_TX_STS_L_REG_ADDR(chan)	(ESR_N2_BASE + ESR_N2_TX_REG_OFFSET +\
+					(chan * 4) + 2)
+#define	ESR_N2_TX_STS_H_REG_ADDR(chan)	(ESR_N2_BASE + ESR_N2_TX_REG_OFFSET +\
+					(chan * 4) + 3)
+#define	ESR_N2_RX_CFG_REG_ADDR(chan)	(ESR_N2_BASE + ESR_N2_RX_REG_OFFSET +\
+					(chan * 4))
+#define	ESR_N2_RX_CFG_L_REG_ADDR(chan)	(ESR_N2_BASE + ESR_N2_RX_REG_OFFSET +\
+					(chan * 4))
+#define	ESR_N2_RX_CFG_H_REG_ADDR(chan)	(ESR_N2_BASE + ESR_N2_RX_REG_OFFSET +\
+					(chan * 4) + 1)
+#define	ESR_N2_RX_STS_REG_ADDR(chan)	(ESR_N2_BASE + ESR_N2_RX_REG_OFFSET +\
+					(chan * 4) + 2)
+#define	ESR_N2_RX_STS_L_REG_ADDR(chan)	(ESR_N2_BASE + ESR_N2_RX_REG_OFFSET +\
+					(chan * 4) + 2)
+#define	ESR_N2_RX_STS_H_REG_ADDR(chan)	(ESR_N2_BASE + ESR_N2_RX_REG_OFFSET +\
+					(chan * 4) + 3)
+
+/* PLL Configuration Low 16-bit word */
+typedef	union _esr_ti_cfgpll_l {
+	uint16_t value;
+
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t res2		: 6;
+		uint16_t lb			: 2;
+		uint16_t res1		: 3;
+		uint16_t mpy		: 4;
+		uint16_t enpll		: 1;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint16_t enpll		: 1;
+		uint16_t mpy		: 4;
+		uint16_t res1		: 3;
+		uint16_t lb			: 2;
+		uint16_t res2		: 6;
+#endif
+	} bits;
+} esr_ti_cfgpll_l_t;
+
+/* PLL Configurations */
+#define	CFGPLL_LB_FREQ_DEP_BANDWIDTH	0
+#define	CFGPLL_LB_LOW_BANDWIDTH		0x2
+#define	CFGPLL_LB_HIGH_BANDWIDTH	0x3
+#define	CFGPLL_MPY_4X			0
+#define	CFGPLL_MPY_5X			0x1
+#define	CFGPLL_MPY_6X			0x2
+#define	CFGPLL_MPY_8X			0x4
+#define	CFGPLL_MPY_10X			0x5
+#define	CFGPLL_MPY_12X			0x6
+#define	CFGPLL_MPY_12P5X		0x7
+
+/* Rx Configuration Low 16-bit word */
+
+typedef	union _esr_ti_cfgrx_l {
+	uint16_t value;
+
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t los		: 2;
+		uint16_t align		: 2;
+		uint16_t res		: 1;
+		uint16_t term		: 3;
+		uint16_t invpair	: 1;
+		uint16_t rate		: 2;
+		uint16_t buswidth	: 3;
+		uint16_t entest		: 1;
+		uint16_t enrx		: 1;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint16_t enrx		: 1;
+		uint16_t entest		: 1;
+		uint16_t buswidth	: 3;
+		uint16_t rate		: 2;
+		uint16_t invpair	: 1;
+		uint16_t term		: 3;
+		uint16_t res		: 1;
+		uint16_t align		: 2;
+		uint16_t los		: 2;
+#endif
+	} bits;
+} esr_ti_cfgrx_l_t;
+
+/* Rx Configuration High 16-bit word */
+
+typedef	union _esr_ti_cfgrx_h {
+	uint16_t value;
+
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t res2		: 6;
+		uint16_t bsinrxn	: 1;
+		uint16_t bsinrxp	: 1;
+		uint16_t res1		: 1;
+		uint16_t eq		: 4;
+		uint16_t cdr		: 3;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint16_t cdr		: 3;
+		uint16_t eq		: 4;
+		uint16_t res1		: 1;
+		uint16_t bsinrxp	: 1;
+		uint16_t bsinrxn	: 1;
+		uint16_t res2		: 6;
+#endif
+	} bits;
+} esr_ti_cfgrx_h_t;
+
+/* Receive Configurations */
+#define	CFGRX_BUSWIDTH_10BIT			0
+#define	CFGRX_BUSWIDTH_8BIT			1
+#define	CFGRX_RATE_FULL				0
+#define	CFGRX_RATE_HALF				1
+#define	CFGRX_RATE_QUAD				2
+#define	CFGRX_TERM_VDDT				0
+#define	CFGRX_TERM_0P8VDDT			1
+#define	CFGRX_TERM_FLOAT			3
+#define	CFGRX_ALIGN_DIS				0
+#define	CFGRX_ALIGN_EN				1
+#define	CFGRX_ALIGN_JOG				2
+#define	CFGRX_LOS_DIS				0
+#define	CFGRX_LOS_HITHRES			1
+#define	CFGRX_LOS_LOTHRES			2
+#define	CFGRX_CDR_1ST_ORDER			0
+#define	CFGRX_CDR_2ND_ORDER_HP			1
+#define	CFGRX_CDR_2ND_ORDER_MP			2
+#define	CFGRX_CDR_2ND_ORDER_LP			3
+#define	CFGRX_CDR_1ST_ORDER_FAST_LOCK		4
+#define	CFGRX_CDR_2ND_ORDER_HP_FAST_LOCK	5
+#define	CFGRX_CDR_2ND_ORDER_MP_FAST_LOCK	6
+#define	CFGRX_CDR_2ND_ORDER_LP_FAST_LOCK	7
+#define	CFGRX_EQ_MAX_LF				0
+#define	CFGRX_EQ_ADAPTIVE_LP_ADAPTIVE_ZF	0x1
+#define	CFGRX_EQ_ADAPTIVE_LF_1084MHZ_ZF		0x8
+#define	CFGRX_EQ_ADAPTIVE_LF_805MHZ_ZF		0x9
+#define	CFGRX_EQ_ADAPTIVE_LP_573MHZ_ZF		0xA
+#define	CFGRX_EQ_ADAPTIVE_LP_402MHZ_ZF		0xB
+#define	CFGRX_EQ_ADAPTIVE_LP_304MHZ_ZF		0xC
+#define	CFGRX_EQ_ADAPTIVE_LP_216MHZ_ZF		0xD
+#define	CFGRX_EQ_ADAPTIVE_LP_156MHZ_ZF		0xE
+#define	CFGRX_EQ_ADAPTIVE_LP_135HZ_ZF		0xF
+
+/* Rx Status Low 16-bit word */
+
+typedef	union _esr_ti_stsrx_l {
+	uint16_t value;
+
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t res		: 10;
+		uint16_t bsrxn		: 1;
+		uint16_t bsrxp		: 1;
+		uint16_t losdtct	: 1;
+		uint16_t oddcg		: 1;
+		uint16_t sync		: 1;
+		uint16_t testfail	: 1;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint16_t testfail	: 1;
+		uint16_t sync		: 1;
+		uint16_t oddcg		: 1;
+		uint16_t losdtct	: 1;
+		uint16_t bsrxp		: 1;
+		uint16_t bsrxn		: 1;
+		uint16_t res		: 10;
+#endif
+	} bits;
+} esr_ti_stsrx_l_t;
+
+/* Tx Configuration Low 16-bit word */
+
+typedef	union _esr_ti_cfgtx_l {
+	uint16_t value;
+
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t de		: 4;
+		uint16_t swing		: 3;
+		uint16_t cm		: 1;
+		uint16_t invpair	: 1;
+		uint16_t rate		: 2;
+		uint16_t buswwidth	: 3;
+		uint16_t entest		: 1;
+		uint16_t entx		: 1;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint16_t entx		: 1;
+		uint16_t entest		: 1;
+		uint16_t buswwidth	: 3;
+		uint16_t rate		: 2;
+		uint16_t invpair	: 1;
+		uint16_t cm		: 1;
+		uint16_t swing		: 3;
+		uint16_t de		: 4;
+#endif
+	} bits;
+} esr_ti_cfgtx_l_t;
+
+/* Tx Configuration High 16-bit word */
+
+typedef	union _esr_ti_cfgtx_h {
+	uint16_t value;
+
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t res		: 14;
+		uint16_t bstx		: 1;
+		uint16_t enftp		: 1;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint16_t enftp		: 1;
+		uint16_t bstx		: 1;
+		uint16_t res		: 14;
+#endif
+	} bits;
+} esr_ti_cfgtx_h_t;
+
+/* Transmit Configurations */
+#define	CFGTX_BUSWIDTH_10BIT		0
+#define	CFGTX_BUSWIDTH_8BIT		1
+#define	CFGTX_RATE_FULL			0
+#define	CFGTX_RATE_HALF			1
+#define	CFGTX_RATE_QUAD			2
+#define	CFGTX_SWING_125MV		0
+#define	CFGTX_SWING_250MV		1
+#define	CFGTX_SWING_500MV		2
+#define	CFGTX_SWING_625MV		3
+#define	CFGTX_SWING_750MV		4
+#define	CFGTX_SWING_1000MV		5
+#define	CFGTX_SWING_1250MV		6
+#define	CFGTX_SWING_1375MV		7
+#define	CFGTX_DE_0			0
+#define	CFGTX_DE_4P76			1
+#define	CFGTX_DE_9P52			2
+#define	CFGTX_DE_14P28			3
+#define	CFGTX_DE_19P04			4
+#define	CFGTX_DE_23P8			5
+#define	CFGTX_DE_28P56			6
+#define	CFGTX_DE_33P32			7
+
+/* Test Configuration */
+
+typedef	union _esr_ti_testcfg {
+	uint16_t value;
+
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t res1		: 1;
+		uint16_t invpat		: 1;
+		uint16_t rate		: 2;
+		uint16_t res		: 1;
+		uint16_t enbspls	: 1;
+		uint16_t enbsrx		: 1;
+		uint16_t enbstx		: 1;
+		uint16_t loopback	: 2;
+		uint16_t clkbyp		: 2;
+		uint16_t enrxpatt	: 1;
+		uint16_t entxpatt	: 1;
+		uint16_t testpatt	: 2;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint16_t testpatt	: 2;
+		uint16_t entxpatt	: 1;
+		uint16_t enrxpatt	: 1;
+		uint16_t clkbyp		: 2;
+		uint16_t loopback	: 2;
+		uint16_t enbstx		: 1;
+		uint16_t enbsrx		: 1;
+		uint16_t enbspls	: 1;
+		uint16_t res		: 1;
+		uint16_t rate		: 2;
+		uint16_t invpat		: 1;
+		uint16_t res1		: 1;
+#endif
+	} bits;
+} esr_ti_testcfg_t;
+
+#define	TESTCFG_PAD_LOOPBACK		0x1
+#define	TESTCFG_INNER_CML_DIS_LOOPBACK	0x2
+#define	TESTCFG_INNER_CML_EN_LOOOPBACK	0x3
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif	/* _SYS_NXGE_NXGE_N2_ESR_HW_H */
diff --git a/drivers/net/nxge/include/nxge_phy_hw.h b/drivers/net/nxge/include/nxge_phy_hw.h
new file mode 100644
index 0000000..f5d7e61
--- /dev/null
+++ b/drivers/net/nxge/include/nxge_phy_hw.h
@@ -0,0 +1,642 @@
+/*
+ * nxge_phy_hw.h	Neptune PHY register offsets
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef	_SYS_NXGE_NXGE_PHY_HW_H
+#define	_SYS_NXGE_NXGE_PHY_HW_H
+
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#include <nxge_defs.h>
+
+#define	BCM5464_NEPTUNE_PORT_ADDR_BASE		10
+#define	BCM8704_NEPTUNE_PORT_ADDR_BASE		8
+#define	BCM8704_N2_PORT_ADDR_BASE		16
+#define	BCM8704_PMA_PMD_DEV_ADDR		1
+#define	BCM8704_PCS_DEV_ADDR			3
+#define	BCM8704_USER_DEV3_ADDR			3
+#define	BCM8704_PHYXS_ADDR			4
+#define	BCM8704_USER_DEV4_ADDR			4
+
+/* Definitions for BCM 5464R PHY chip */
+
+#define	BCM5464R_PHY_ECR	16
+#define	BCM5464R_PHY_ESR	17
+#define	BCM5464R_RXERR_CNT	18
+#define	BCM5464R_FALSECS_CNT	19
+#define	BCM5464R_RX_NOTOK_CNT	20
+#define	BCM5464R_ER_DATA	21
+#define	BCM5464R_RES		22
+#define	BCM5464R_ER_ACC		23
+#define	BCM5464R_AUX_CTL	24
+#define	BCM5464R_AUX_S		25
+#define	BCM5464R_INTR_S		26
+#define	BCM5464R_INTR_M		27
+#define	BCM5464R_MISC		28
+#define	BCM5464R_MISC1		29
+#define	BCM5464R_TESTR1		30
+
+#define	PHY_BCM_5464R_OUI	0x001018
+#define	PHY_BCM_5464R_MODEL	0x0B
+
+/*
+ * MII Register 16:  PHY Extended Control Register
+ */
+
+typedef	union _mii_phy_ecr_t {
+	uint16_t value;
+	struct {
+#ifdef _BIT_FIELDS_HTOL
+		uint16_t mac_phy_if_mode	: 1;
+		uint16_t dis_automdicross	: 1;
+		uint16_t tx_dis			: 1;
+		uint16_t intr_dis		: 1;
+		uint16_t force_intr		: 1;
+		uint16_t bypass_encdec		: 1;
+		uint16_t bypass_scrdes		: 1;
+		uint16_t bypass_mlt3		: 1;
+		uint16_t bypass_rx_sym		: 1;
+		uint16_t reset_scr		: 1;
+		uint16_t en_led_traffic		: 1;
+		uint16_t force_leds_on		: 1;
+		uint16_t force_leds_off		: 1;
+		uint16_t res			: 2;
+		uint16_t gmii_fifo_elas		: 1;
+#else
+		uint16_t gmii_fifo_elas		: 1;
+		uint16_t res			: 2;
+		uint16_t force_leds_off		: 1;
+		uint16_t force_leds_on		: 1;
+		uint16_t en_led_traffic		: 1;
+		uint16_t reset_scr		: 1;
+		uint16_t bypass_rx_sym		: 1;
+		uint16_t bypass_mlt3		: 1;
+		uint16_t bypass_scrdes		: 1;
+		uint16_t bypass_encdec		: 1;
+		uint16_t force_intr		: 1;
+		uint16_t intr_dis		: 1;
+		uint16_t tx_dis			: 1;
+		uint16_t dis_automdicross	: 1;
+		uint16_t mac_phy_if_mode	: 1;
+#endif
+	} bits;
+} mii_phy_ecr_t, *p_mii_phy_ecr_t;
+
+/*
+ * MII Register 17:  PHY Extended Status Register
+ */
+typedef	union _mii_phy_esr_t {
+	uint16_t value;
+	struct {
+#ifdef _BIT_FIELDS_HTOL
+		uint16_t anbpsfm		: 1;
+		uint16_t wsdwngr		: 1;
+		uint16_t mdi_crst		: 1;
+		uint16_t intr_s			: 1;
+		uint16_t rmt_rx_s		: 1;
+		uint16_t loc_rx_s		: 1;
+		uint16_t locked			: 1;
+		uint16_t link_s			: 1;
+		uint16_t crc_err		: 1;
+		uint16_t cext_err		: 1;
+		uint16_t bad_ssd		: 1;
+		uint16_t bad_esd		: 1;
+		uint16_t rx_err			: 1;
+		uint16_t tx_err			: 1;
+		uint16_t lock_err		: 1;
+		uint16_t mlt3_cerr		: 1;
+#else
+		uint16_t mlt3_cerr		: 1;
+		uint16_t lock_err		: 1;
+		uint16_t tx_err			: 1;
+		uint16_t rx_err			: 1;
+		uint16_t bad_esd		: 1;
+		uint16_t bad_ssd		: 1;
+		uint16_t cext_err		: 1;
+		uint16_t crc_err		: 1;
+		uint16_t link_s			: 1;
+		uint16_t locked			: 1;
+		uint16_t loc_rx_s		: 1;
+		uint16_t rmt_rx_s		: 1;
+		uint16_t intr_s			: 1;
+		uint16_t mdi_crst		: 1;
+		uint16_t wsdwngr		: 1;
+		uint16_t anbpsfm		: 1;
+#endif
+	} bits;
+} mii_phy_esr_t, *p_mii_phy_esr_t;
+
+/*
+ * MII Register 18:  Receive Error Counter Register
+ */
+typedef	union _mii_rxerr_cnt_t {
+	uint16_t value;
+	struct {
+		uint16_t rx_err_cnt		: 16;
+	} bits;
+} mii_rxerr_cnt_t, *p_mii_rxerr_cnt_t;
+
+/*
+ * MII Register 19:  False Carrier Sense Counter Register
+ */
+typedef	union _mii_falsecs_cnt_t {
+	uint16_t value;
+	struct {
+#ifdef _BIT_FIELDS_HTOL
+		uint16_t res			: 8;
+		uint16_t false_cs_cnt		: 8;
+#else
+		uint16_t false_cs_cnt		: 8;
+		uint16_t res			: 8;
+#endif
+	} bits;
+} mii_falsecs_cnt_t, *p_mii_falsecs_cnt_t;
+
+/*
+ * MII Register 20:  Receiver NOT_OK Counter Register
+ */
+typedef	union _mii_rx_notok_cnt_t {
+	uint16_t value;
+	struct {
+#ifdef _BIT_FIELDS_HTOL
+		uint16_t l_rx_notok_cnt		: 8;
+		uint16_t r_rx_notok_cnt		: 8;
+#else
+		uint16_t r_rx_notok_cnt		: 8;
+		uint16_t l_rx_notok_cnt		: 8;
+#endif
+	} bits;
+} mii_rx_notok_cnt_t, *p_mii_rx_notok_t;
+
+/*
+ * MII Register 21:  Expansion Register Data Register
+ */
+typedef	union _mii_er_data_t {
+	uint16_t value;
+	struct {
+		uint16_t reg_data;
+	} bits;
+} mii_er_data_t, *p_mii_er_data_t;
+
+/*
+ * MII Register 23:  Expansion Register Access Register
+ */
+typedef	union _mii_er_acc_t {
+	struct {
+#ifdef _BIT_FIELDS_HTOL
+		uint16_t res			: 4;
+		uint16_t er_sel			: 4;
+		uint16_t er_acc			: 8;
+#else
+		uint16_t er_acc			: 8;
+		uint16_t er_sel			: 4;
+		uint16_t res			: 4;
+#endif
+	} bits;
+} mii_er_acc_t, *p_mii_er_acc_t;
+
+#define	EXP_RXTX_PKT_CNT		0x0
+#define	EXP_INTR_STAT			0x1
+#define	MULTICOL_LED_SEL		0x4
+#define	MULTICOL_LED_FLASH_RATE_CTL	0x5
+#define	MULTICOL_LED_BLINK_CTL		0x6
+#define	CABLE_DIAG_CTL			0x10
+#define	CABLE_DIAG_RES			0x11
+#define	CABLE_DIAG_LEN_CH_2_1		0x12
+#define	CABLE_DIAG_LEN_CH_4_3		0x13
+
+/*
+ * MII Register 24:  Auxiliary Control Register
+ */
+typedef	union _mii_aux_ctl_t {
+	uint16_t value;
+	struct {
+#ifdef _BIT_FIELDS_HTOL
+		uint16_t ext_lb			: 1;
+		uint16_t ext_pkt_len		: 1;
+		uint16_t edge_rate_ctl_1000	: 2;
+		uint16_t res			: 1;
+		uint16_t write_1		: 1;
+		uint16_t res1			: 2;
+		uint16_t dis_partial_resp	: 1;
+		uint16_t res2			: 1;
+		uint16_t edge_rate_ctl_100	: 2;
+		uint16_t diag_mode		: 1;
+		uint16_t shadow_reg_sel		: 3;
+#else
+		uint16_t shadow_reg_sel		: 3;
+		uint16_t diag_mode		: 1;
+		uint16_t edge_rate_ctl_100	: 2;
+		uint16_t res2			: 1;
+		uint16_t dis_partial_resp	: 1;
+		uint16_t res1			: 2;
+		uint16_t write_1		: 1;
+		uint16_t res			: 1;
+		uint16_t edge_rate_ctl_1000	: 2;
+		uint16_t ext_pkt_len		: 1;
+		uint16_t ext_lb			: 1;
+#endif
+	} bits;
+} mii_aux_ctl_t, *p_mii_aux_ctl_t;
+
+#define	AUX_REG				0x0
+#define	AUX_10BASET			0x1
+#define	AUX_PWR_CTL			0x2
+#define	AUX_MISC_TEST			0x4
+#define	AUX_MISC_CTL			0x7
+
+/*
+ * MII Register 25:  Auxiliary Status Summary Register
+ */
+typedef	union _mii_aux_s_t {
+	uint16_t value;
+	struct {
+#ifdef _BIT_FIELDS_HTOL
+		uint16_t an_complete		: 1;
+		uint16_t an_complete_ack	: 1;
+		uint16_t an_ack_detect		: 1;
+		uint16_t an_ability_detect	: 1;
+		uint16_t an_np_wait		: 1;
+		uint16_t an_hcd			: 3;
+		uint16_t pd_fault		: 1;
+		uint16_t rmt_fault		: 1;
+		uint16_t an_page_rx		: 1;
+		uint16_t lp_an_ability		: 1;
+		uint16_t lp_np_ability		: 1;
+		uint16_t link_s			: 1;
+		uint16_t pause_res_rx_dir	: 1;
+		uint16_t pause_res_tx_dir	: 1;
+#else
+		uint16_t pause_res_tx_dir	: 1;
+		uint16_t pause_res_rx_dir	: 1;
+		uint16_t link_s			: 1;
+		uint16_t lp_np_ability		: 1;
+		uint16_t lp_an_ability		: 1;
+		uint16_t an_page_rx		: 1;
+		uint16_t rmt_fault		: 1;
+		uint16_t pd_fault		: 1;
+		uint16_t an_hcd			: 3;
+		uint16_t an_np_wait		: 1;
+		uint16_t an_ability_detect	: 1;
+		uint16_t an_ack_detect		: 1;
+		uint16_t an_complete_ack	: 1;
+		uint16_t an_complete		: 1;
+#endif
+	} bits;
+} mii_aux_s_t, *p_mii_aux_s_t;
+
+/*
+ * MII Register 26, 27:  Interrupt Status and Mask Registers
+ */
+typedef	union _mii_intr_t {
+	uint16_t value;
+	struct {
+#ifdef _BIT_FIELDS_HTOL
+		uint16_t res			: 1;
+		uint16_t illegal_pair_swap	: 1;
+		uint16_t mdix_status_change	: 1;
+		uint16_t exceed_hicnt_thres	: 1;
+		uint16_t exceed_locnt_thres	: 1;
+		uint16_t an_page_rx		: 1;
+		uint16_t hcd_nolink		: 1;
+		uint16_t no_hcd			: 1;
+		uint16_t neg_unsupported_hcd	: 1;
+		uint16_t scr_sync_err		: 1;
+		uint16_t rmt_rx_status_change	: 1;
+		uint16_t loc_rx_status_change	: 1;
+		uint16_t duplex_mode_change	: 1;
+		uint16_t link_speed_change	: 1;
+		uint16_t link_status_change	: 1;
+		uint16_t crc_err		: 1;
+#else
+		uint16_t crc_err		: 1;
+		uint16_t link_status_change	: 1;
+		uint16_t link_speed_change	: 1;
+		uint16_t duplex_mode_change	: 1;
+		uint16_t loc_rx_status_change	: 1;
+		uint16_t rmt_rx_status_change	: 1;
+		uint16_t scr_sync_err		: 1;
+		uint16_t neg_unsupported_hcd	: 1;
+		uint16_t no_hcd			: 1;
+		uint16_t hcd_nolink		: 1;
+		uint16_t an_page_rx		: 1;
+		uint16_t exceed_locnt_thres	: 1;
+		uint16_t exceed_hicnt_thres	: 1;
+		uint16_t mdix_status_change	: 1;
+		uint16_t illegal_pair_swap	: 1;
+		uint16_t res			: 1;
+#endif
+	} bits;
+} mii_intr_t, *p_mii_intr_t;
+
+/*
+ * MII Register 28:  Register 1C Access Register
+ */
+typedef	union _mii_misc_t {
+	uint16_t value;
+	struct {
+#ifdef _BIT_FIELDS_HTOL
+		uint16_t w_en			: 1;
+		uint16_t shadow_reg_sel		: 5;
+		uint16_t data			: 10;
+#else
+		uint16_t data			: 10;
+		uint16_t shadow_reg_sel		: 5;
+		uint16_t w_en			: 1;
+#endif
+	} bits;
+} mii_misc_t, *p_mii_misc_t;
+
+#define	LINK_LED_MODE			0x2
+#define	CLK_ALIGN_CTL			0x3
+#define	WIRE_SP_RETRY			0x4
+#define	CLK125				0x5
+#define	LED_STATUS			0x8
+#define	LED_CONTROL			0x9
+#define	AUTO_PWR_DOWN			0xA
+#define	LED_SEL1			0xD
+#define	LED_SEL2			0xE
+
+/*
+ * MII Register 29:  Master/Slave Seed / HCD Status Register
+ */
+
+typedef	union _mii_misc1_t {
+	uint16_t value;
+	struct {
+#ifdef _BIT_FIELDS_HTOL
+		uint16_t en_shadow_reg		: 1;
+		uint16_t data			: 15;
+#else
+		uint16_t data			: 15;
+		uint16_t en_shadow_reg		: 1;
+#endif
+	} bits;
+} mii_misc1_t, *p_mii_misc1_t;
+
+/*
+ * MII Register 30:  Test Register 1
+ */
+
+typedef	union _mii_test1_t {
+	uint16_t value;
+	struct {
+#ifdef _BIT_FIELDS_HTOL
+		uint16_t crc_err_cnt_sel	: 1;
+		uint16_t res			: 7;
+		uint16_t manual_swap_mdi_st	: 1;
+		uint16_t res1			: 7;
+#else
+		uint16_t res1			: 7;
+		uint16_t manual_swap_mdi_st	: 1;
+		uint16_t res			: 7;
+		uint16_t crc_err_cnt_sel	: 1;
+#endif
+	} bits;
+} mii_test1_t, *p_mii_test1_t;
+
+
+/* Definitions of BCM8704 */
+
+#define	BCM8704_PMD_CONTROL_REG			0
+#define	BCM8704_PMD_STATUS_REG			0x1
+#define	BCM8704_PMD_ID_0_REG			0x2
+#define	BCM8704_PMD_ID_1_REG			0x3
+#define	BCM8704_PMD_SPEED_ABIL_REG		0x4
+#define	BCM8704_PMD_DEV_IN_PKG1_REG		0x5
+#define	BCM8704_PMD_DEV_IN_PKG2_REG		0x6
+#define	BCM8704_PMD_CONTROL2_REG		0x7
+#define	BCM8704_PMD_STATUS2_REG			0x8
+#define	BCM8704_PMD_TRANSMIT_DIS_REG		0x9
+#define	BCM8704_PMD_RECEIVE_SIG_DETECT		0xa
+#define	BCM8704_PMD_ORG_UNIQUE_ID_0_REG		0xe
+#define	BCM8704_PMD_ORG_UNIQUE_ID_1_REG		0xf
+#define	BCM8704_PCS_CONTROL_REG			0
+#define	BCM8704_PCS_STATUS1_REG			0x1
+#define	BCM8704_PCS_ID_0_REG			0x2
+#define	BCM8704_PCS_ID_1_REG			0x3
+#define	BCM8704_PCS_SPEED_ABILITY_REG		0x4
+#define	BCM8704_PCS_DEV_IN_PKG1_REG		0x5
+#define	BCM8704_PCS_DEV_IN_PKG2_REG		0x6
+#define	BCM8704_PCS_CONTROL2_REG		0x7
+#define	BCM8704_PCS_STATUS2_REG			0x8
+#define	BCM8704_PCS_ORG_UNIQUE_ID_0_REG		0xe
+#define	BCM8704_PCS_ORG_UNIQUE_ID_1_REG		0xf
+#define	BCM8704_PCS_STATUS_REG			0x18
+#define	BCM8704_10GBASE_R_PCS_STATUS_REG	0x20
+#define	BCM8704_10GBASE_R_PCS_STATUS2_REG	0x21
+#define	BCM8704_PHYXS_CONTROL_REG		0
+#define	BCM8704_PHYXS_STATUS_REG		0x1
+#define	BCM8704_PHY_ID_0_REG			0x2
+#define	BCM8704_PHY_ID_1_REG			0x3
+#define	BCM8704_PHYXS_SPEED_ABILITY_REG		0x4
+#define	BCM8704_PHYXS_DEV_IN_PKG2_REG		0x5
+#define	BCM8704_PHYXS_DEV_IN_PKG1_REG		0x6
+#define	BCM8704_PHYXS_STATUS2_REG		0x8
+#define	BCM8704_PHYXS_ORG_UNIQUE_ID_0_REG	0xe
+#define	BCM8704_PHYXS_ORG_UNIQUE_ID_1_REG	0xf
+#define	BCM8704_PHYXS_XGXS_LANE_STATUS_REG	0x18
+#define	BCM8704_PHYXS_XGXS_TEST_CONTROL_REG	0x19
+#define	BCM8704_USER_CONTROL_REG		0xC800
+#define	BCM8704_USER_ANALOG_CLK_REG		0xC801
+#define	BCM8704_USER_PMD_RX_CONTROL_REG		0xC802
+#define	BCM8704_USER_PMD_TX_CONTROL_REG		0xC803
+#define	BCM8704_USER_OPTICS_DIGITAL_CTRL_REG	0xC808
+#define	BCM8704_USER_RX2_CONTROL1_REG		0x80C6
+#define	BCM8704_USER_RX1_CONTROL1_REG		0x80D6
+#define	BCM8704_USER_RX0_CONTROL1_REG		0x80E6
+
+/* Rx Channel Control1 Register bits */
+#define	BCM8704_RXPOL_FLIP			0x20
+
+typedef	union _phyxs_control {
+	uint16_t value;
+	struct {
+#ifdef _BIT_FIELDS_HTOL
+		uint16_t reset			: 1;
+		uint16_t loopback		: 1;
+		uint16_t speed_sel2		: 1;
+		uint16_t res2			: 1;
+		uint16_t low_power		: 1;
+		uint16_t res1			: 4;
+		uint16_t speed_sel1		: 1;
+		uint16_t speed_sel0		: 4;
+		uint16_t res0			: 2;
+#else
+		uint16_t res0			: 2;
+		uint16_t speed_sel0		: 4;
+		uint16_t speed_sel1		: 1;
+		uint16_t res1			: 4;
+		uint16_t low_power		: 1;
+		uint16_t res2			: 1;
+		uint16_t speed_sel2		: 1;
+		uint16_t loopback		: 1;
+		uint16_t reset			: 1;
+#endif
+	} bits;
+} phyxs_control_t, *p_phyxs_control_t, pcs_control_t, *p_pcs_control_t;
+
+
+/* PMD/Optics Digital Control Register (Dev=3 Addr=0xc800) */
+
+typedef	union _control {
+	uint16_t value;
+	struct {
+#ifdef _BIT_FIELDS_HTOL
+		uint16_t optxenb_lvl		: 1;
+		uint16_t optxrst_lvl		: 1;
+		uint16_t opbiasflt_lvl		: 1;
+		uint16_t obtmpflt_lvl		: 1;
+		uint16_t opprflt_lvl		: 1;
+		uint16_t optxflt_lvl		: 1;
+		uint16_t optrxlos_lvl		: 1;
+		uint16_t oprxflt_lvl		: 1;
+		uint16_t optxon_lvl		: 1;
+		uint16_t res1			: 7;
+#else
+		uint16_t res1			: 7;
+		uint16_t optxon_lvl		: 1;
+		uint16_t oprxflt_lvl		: 1;
+		uint16_t optrxlos_lvl		: 1;
+		uint16_t optxflt_lvl		: 1;
+		uint16_t opprflt_lvl		: 1;
+		uint16_t obtmpflt_lvl		: 1;
+		uint16_t opbiasflt_lvl		: 1;
+		uint16_t optxrst_lvl		: 1;
+		uint16_t optxenb_lvl		: 1;
+#endif
+	} bits;
+} control_t, *p_control_t;
+
+typedef	union _pmd_tx_control {
+	uint16_t value;
+	struct {
+#ifdef _BIT_FIELDS_HTOL
+		uint16_t res1			: 7;
+		uint16_t xfp_clken		: 1;
+		uint16_t tx_dac_txd		: 2;
+		uint16_t tx_dac_txck		: 2;
+		uint16_t tsd_lpwren		: 1;
+		uint16_t tsck_lpwren		: 1;
+		uint16_t cmu_lpwren		: 1;
+		uint16_t sfiforst		: 1;
+#else
+		uint16_t sfiforst		: 1;
+		uint16_t cmu_lpwren		: 1;
+		uint16_t tsck_lpwren		: 1;
+		uint16_t tsd_lpwren		: 1;
+		uint16_t tx_dac_txck		: 2;
+		uint16_t tx_dac_txd		: 2;
+		uint16_t xfp_clken		: 1;
+		uint16_t res1			: 7;
+#endif
+	} bits;
+} pmd_tx_control_t, *p_pmd_tx_control_t;
+
+
+/* PMD/Optics Digital Control Register (Dev=3 Addr=0xc808) */
+
+
+/* PMD/Optics Digital Control Register (Dev=3 Addr=0xc808) */
+
+typedef	union _optics_dcntr {
+	uint16_t value;
+	struct {
+#ifdef _BIT_FIELDS_HTOL
+		uint16_t fault_mode		: 1;
+		uint16_t tx_pwrdown		: 1;
+		uint16_t rx_pwrdown		: 1;
+		uint16_t ext_flt_en		: 1;
+		uint16_t opt_rst		: 1;
+		uint16_t pcs_tx_inv_b		: 1;
+		uint16_t pcs_rx_inv		: 1;
+		uint16_t res3			: 2;
+		uint16_t gpio_sel		: 2;
+		uint16_t res2			: 1;
+		uint16_t lpbk_err_dis		: 1;
+		uint16_t res1			: 2;
+		uint16_t txonoff_pwdwn_dis	: 1;
+#else
+		uint16_t txonoff_pwdwn_dis	: 1;
+		uint16_t res1			: 2;
+		uint16_t lpbk_err_dis		: 1;
+		uint16_t res2			: 1;
+		uint16_t gpio_sel		: 2;
+		uint16_t res3			: 2;
+		uint16_t pcs_rx_inv		: 1;
+		uint16_t pcs_tx_inv_b		: 1;
+		uint16_t opt_rst		: 1;
+		uint16_t ext_flt_en		: 1;
+		uint16_t rx_pwrdown		: 1;
+		uint16_t tx_pwrdown		: 1;
+		uint16_t fault_mode		: 1;
+#endif
+	} bits;
+} optics_dcntr_t, *p_optics_dcntr_t;
+
+/* PMD Receive Signal Detect Register (Dev = 1 Register Address = 0x000A) */
+
+#define	PMD_RX_SIG_DET3			0x10
+#define	PMD_RX_SIG_DET2			0x08
+#define	PMD_RX_SIG_DET1			0x04
+#define	PMD_RX_SIG_DET0			0x02
+#define	GLOB_PMD_RX_SIG_OK		0x01
+
+/* 10GBase-R PCS Status Register (Dev = 3, Register Address = 0x0020) */
+
+#define	PCS_10GBASE_RX_LINK_STATUS	0x1000
+#define	PCS_PRBS31_ABLE			0x0004
+#define	PCS_10GBASE_R_HI_BER		0x0002
+#define	PCS_10GBASE_R_PCS_BLK_LOCK	0x0001
+
+/* XGXS Lane Status Register (Dev = 4, Register Address = 0x0018) */
+
+#define	XGXS_LANE_ALIGN_STATUS		0x1000
+#define	XGXS_PATTERN_TEST_ABILITY	0x0800
+#define	XGXS_LANE3_SYNC			0x0008
+#define	XGXS_LANE2_SYNC			0x0004
+#define	XGXS_LANE1_SYNC			0x0002
+#define	XGXS_LANE0_SYNC			0x0001
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif	/* _SYS_NXGE_NXGE_PHY_HW_H */
diff --git a/drivers/net/nxge/include/nxge_rxdma_hw.h b/drivers/net/nxge/include/nxge_rxdma_hw.h
new file mode 100644
index 0000000..3897527
--- /dev/null
+++ b/drivers/net/nxge/include/nxge_rxdma_hw.h
@@ -0,0 +1,1963 @@
+/*
+ * nxge_rxdma_hw.h	Neptune RX DMA register offsets
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef	_SYS_NXGE_NXGE_RXDMA_HW_H
+#define	_SYS_NXGE_NXGE_RXDMA_HW_H
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#include <nxge_defs.h>
+  /*#include <nxge_hw.h> */
+
+/*
+ * NIU: Receive DMA Channels
+ */
+/* Receive DMA Clock Divider */
+#define	RX_DMA_CK_DIV_REG	(FZC_DMC + 0x00000)
+#define	RX_DMA_CK_DIV_SHIFT	0			/* bits 15:0 */
+#define	RX_DMA_CK_DIV_MASK	0x000000000000FFFFULL
+
+typedef union _rx_dma_ck_div_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res1_1:16;
+			uint32_t cnt:16;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t cnt:16;
+			uint32_t res1_1:16;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} rx_dma_ck_div_t, *p_rx_dma_ck_div_t;
+
+
+/*
+ * Default Port Receive DMA Channel (RDC)
+ */
+#define	DEF_PT_RDC_REG(port)	(FZC_DMC + 0x00008 * (port + 1))
+#define	DEF_PT0_RDC_REG		(FZC_DMC + 0x00008)
+#define	DEF_PT1_RDC_REG		(FZC_DMC + 0x00010)
+#define	DEF_PT2_RDC_REG		(FZC_DMC + 0x00018)
+#define	DEF_PT3_RDC_REG		(FZC_DMC + 0x00020)
+#define	DEF_PT_RDC_SHIFT	0			/* bits 4:0 */
+#define	DEF_PT_RDC_MASK		0x000000000000001FULL
+
+
+#define	RDC_TBL_REG		(FZC_ZCP + 0x10000)
+#define	RDC_TBL_SHIFT		0			/* bits 4:0 */
+#define	RDC_TBL_MASK		0x000000000000001FULL
+
+/* For the default port RDC and RDC table */
+typedef union _def_pt_rdc_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res1_1:27;
+			uint32_t rdc:5;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t rdc:5;
+			uint32_t res1_1:27;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} def_pt_rdc_t, *p_def_pt_rdc_t;
+
+typedef union _rdc_tbl_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res1_1:28;
+			uint32_t rdc:4;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t rdc:4;
+			uint32_t res1_1:28;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} rdc_tbl_t, *p_rdc_tbl_t;
+
+/*
+ * RDC: 32 bit Addressing mode
+ */
+#define	RX_ADDR_MD_REG		(FZC_DMC + 0x00070)
+#define	RX_ADDR_MD_SHIFT	0			/* bits 0:0 */
+#define	RX_ADDR_MD_SET_32	0x0000000000000001ULL	/* 1 to select 32 bit */
+#define	RX_ADDR_MD_MASK		0x0000000000000001ULL
+
+typedef union _rx_addr_md_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res1_1:28;
+			uint32_t dbg_pt_mux_sel:2;
+			uint32_t ram_acc:1;
+			uint32_t mode32:1;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t mode32:1;
+			uint32_t ram_acc:1;
+			uint32_t dbg_pt_mux_sel:2;
+			uint32_t res1_1:28;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} rx_addr_md_t, *p_rx_addr_md_t;
+
+/*
+ * RDC: Port Scheduler
+ */
+
+#define	PT_DRR_WT_REG(portnm)		((FZC_DMC + 0x00028) + (portnm * 8))
+#define	PT_DRR_WT0_REG		(FZC_DMC + 0x00028)
+#define	PT_DRR_WT1_REG		(FZC_DMC + 0x00030)
+#define	PT_DRR_WT2_REG		(FZC_DMC + 0x00038)
+#define	PT_DRR_WT3_REG		(FZC_DMC + 0x00040)
+#define	PT_DRR_WT_SHIFT		0
+#define	PT_DRR_WT_MASK		0x000000000000FFFFULL	/* bits 15:0 */
+#define	PT_DRR_WT_DEFAULT_10G	0x0400
+#define	PT_DRR_WT_DEFAULT_1G	0x0066
+
+typedef union _pt_drr_wt_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res1_1:16;
+			uint32_t wt:16;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t wt:16;
+			uint32_t res1_1:16;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} pt_drr_wt_t, *p_pt_drr_wt_t;
+
+#define	NXGE_RX_DRR_WT_10G	0x400
+#define	NXGE_RX_DRR_WT_1G	0x066
+
+/* Port FIFO Usage */
+#define	PT_USE_REG(portnum)		((FZC_DMC + 0x00048) + (portnum * 8))
+#define	PT_USE0_REG		(FZC_DMC + 0x00048)
+#define	PT_USE1_REG		(FZC_DMC + 0x00050)
+#define	PT_USE2_REG		(FZC_DMC + 0x00058)
+#define	PT_USE3_REG		(FZC_DMC + 0x00060)
+#define	PT_USE_SHIFT		0			/* bits 19:0 */
+#define	PT_USE_MASK		0x00000000000FFFFFULL
+
+typedef union _pt_use_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res1_1:12;
+			uint32_t cnt:20;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t cnt:20;
+			uint32_t res1_1:12;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} pt_use_t, *p_pt_use_t;
+
+/*
+ * RDC: Partitioning Support
+ *	(Each of the following registers is for each RDC)
+ * Please refer to nxge_hw.h for the common logical
+ * page configuration register definitions.
+ */
+#define	RX_LOG_REG_SIZE			0x40
+#define	RX_LOG_DMA_OFFSET(channel)	(channel * RX_LOG_REG_SIZE)
+
+#define	RX_LOG_PAGE_VLD_REG	(FZC_DMC + 0x20000)
+#define	RX_LOG_PAGE_MASK1_REG	(FZC_DMC + 0x20008)
+#define	RX_LOG_PAGE_VAL1_REG	(FZC_DMC + 0x20010)
+#define	RX_LOG_PAGE_MASK2_REG	(FZC_DMC + 0x20018)
+#define	RX_LOG_PAGE_VAL2_REG	(FZC_DMC + 0x20020)
+#define	RX_LOG_PAGE_RELO1_REG	(FZC_DMC + 0x20028)
+#define	RX_LOG_PAGE_RELO2_REG	(FZC_DMC + 0x20030)
+#define	RX_LOG_PAGE_HDL_REG	(FZC_DMC + 0x20038)
+
+/* RX and TX have the same definitions */
+#define	RX_LOG_PAGE1_VLD_SHIFT	1			/* bit 1 */
+#define	RX_LOG_PAGE0_VLD_SHIFT	0			/* bit 0 */
+#define	RX_LOG_PAGE1_VLD	0x0000000000000002ULL
+#define	RX_LOG_PAGE0_VLD	0x0000000000000001ULL
+#define	RX_LOG_PAGE1_VLD_MASK	0x0000000000000002ULL
+#define	RX_LOG_PAGE0_VLD_MASK	0x0000000000000001ULL
+#define	RX_LOG_FUNC_VLD_SHIFT	2			/* bit 3:2 */
+#define	RX_LOG_FUNC_VLD_MASK	0x000000000000000CULL
+
+#define	LOG_PAGE_ADDR_SHIFT	12	/* bits[43:12] --> bits[31:0] */
+
+/* RDC: Weighted Random Early Discard */
+#define	RED_RAN_INIT_REG	(FZC_DMC + 0x00068)
+
+#define	RED_RAN_INIT_SHIFT	0			/* bits 15:0 */
+#define	RED_RAN_INIT_MASK	0x000000000000ffffULL
+
+/* Weighted Random */
+typedef union _red_ran_init_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res1_1:15;
+			uint32_t enable:1;
+			uint32_t init:16;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t init:16;
+			uint32_t enable:1;
+			uint32_t res1_1:15;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} red_ran_init_t, *p_red_ran_init_t;
+
+/*
+ * Buffer block descriptor
+ */
+typedef struct _rx_desc_t {
+	uint32_t	block_addr;
+} rx_desc_t, *p_rx_desc_t;
+
+/*
+ * RDC: RED Parameter
+ *	(Each DMC has one RED register)
+ */
+#define	RDC_RED_CHANNEL_SIZE		(0x40)
+#define	RDC_RED_CHANNEL_OFFSET(channel)	(channel * RDC_RED_CHANNEL_SIZE)
+
+#define	RDC_RED_PARA_REG		(FZC_DMC + 0x30000)
+#define	RDC_RED_RDC_PARA_REG(rdc)	\
+	(RDC_RED_PARA_REG + (rdc * RDC_RED_CHANNEL_SIZE))
+
+/* the layout of this register is  rx_disc_cnt_t */
+#define	RDC_RED_DISC_CNT_REG		(FZC_DMC + 0x30008)
+#define	RDC_RED_RDC_DISC_REG(rdc)	\
+	(RDC_RED_DISC_CNT_REG + (rdc * RDC_RED_CHANNEL_SIZE))
+
+
+#define	RDC_RED_PARA1_RBR_SCL_SHIFT	0			/* bits 2:0 */
+#define	RDC_RED_PARA1_RBR_SCL_MASK	0x0000000000000007ULL
+#define	RDC_RED_PARA1_ENB_SHIFT		3			/* bit 3 */
+#define	RDC_RED_PARA1_ENB		0x0000000000000008ULL
+#define	RDC_RED_PARA1_ENB_MASK		0x0000000000000008ULL
+
+#define	RDC_RED_PARA_WIN_SHIFT		0			/* bits 3:0 */
+#define	RDC_RED_PARA_WIN_MASK		0x000000000000000fULL
+#define	RDC_RED_PARA_THRE_SHIFT	4			/* bits 15:4 */
+#define	RDC_RED_PARA_THRE_MASK		0x00000000000000f0ULL
+#define	RDC_RED_PARA_WIN_SYN_SHIFT	16			/* bits 19:16 */
+#define	RDC_RED_PARA_WIN_SYN_MASK	0x00000000000000f0ULL
+#define	RDC_RED_PARA_THRE_SYN_SHIFT	20			/* bits 31:20 */
+#define	RDC_RED_PARA_THRE_SYN_MASK	0x00000000000fff00ULL
+
+/* RDC:  RED parameters  */
+typedef union _rdc_red_para_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t thre_sync:12;
+		uint32_t win_syn:4;
+		uint32_t thre:12;
+		uint32_t win:4;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t win:4;
+		uint32_t thre:12;
+		uint32_t win_syn:4;
+		uint32_t thre_sync:12;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} rdc_red_para_t, *p_rdc_red_para_t;
+
+/*
+ * RDC: Receive DMA Datapath Configuration
+ *	The following register definitions are for
+ *	each DMA channel. Each DMA CSR is 512 bytes
+ *	(0x200).
+ */
+#define	RXDMA_CFIG1_REG			(DMC + 0x00000)
+#define	RXDMA_CFIG2_REG			(DMC + 0x00008)
+
+#define	RXDMA_CFIG1_MBADDR_H_SHIFT	0			/* bits 11:0 */
+#define	RXDMA_CFIG1_MBADDR_H_MASK	0x0000000000000fc0ULL
+#define	RXDMA_CFIG1_RST_SHIFT		30			/* bit 30 */
+#define	RXDMA_CFIG1_RST			0x0000000040000000ULL
+#define	RXDMA_CFIG1_RST_MASK		0x0000000040000000ULL
+#define	RXDMA_CFIG1_EN_SHIFT		31
+#define	RXDMA_CFIG1_EN			0x0000000080000000ULL
+#define	RXDMA_CFIG1_EN_MASK		0x0000000080000000ULL
+
+typedef union _rxdma_cfig1_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t en:1;
+			uint32_t rst:1;
+			uint32_t qst:1;
+			uint32_t res2:17;
+			uint32_t mbaddr_h:12;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t mbaddr_h:12;
+			uint32_t res2:17;
+			uint32_t qst:1;
+			uint32_t rst:1;
+			uint32_t en:1;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} rxdma_cfig1_t, *p_rxdma_cfig1_t;
+
+#define	RXDMA_HDR_SIZE_DEFAULT		2
+#define	RXDMA_HDR_SIZE_FULL		18
+
+#define	RXDMA_CFIG2_FULL_HDR_SHIFT	0			/* Set to 1 */
+#define	RXDMA_CFIG2_FULL_HDR		0x0000000000000001ULL
+#define	RXDMA_CFIG2_FULL_HDR_MASK	0x0000000000000001ULL
+#define	RXDMA_CFIG2_OFFSET_SHIFT		1		/* bit 3:1 */
+#define	RXDMA_CFIG2_OFFSET_MASK		0x000000004000000eULL
+#define	RXDMA_CFIG2_MBADDR_L_SHIFT	6			/* bit 31:6 */
+#define	RXDMA_CFIG2_MBADDR_L_MASK	0x00000000ffffffc0ULL
+
+typedef union _rxdma_cfig2_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t mbaddr:26;
+			uint32_t res2:3;
+			uint32_t offset:2;
+			uint32_t full_hdr:1;
+
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t full_hdr:1;
+			uint32_t offset:2;
+			uint32_t res2:3;
+			uint32_t mbaddr:26;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} rxdma_cfig2_t, *p_rxdma_cfig2_t;
+
+/*
+ * RDC: Receive Block Ring Configuration
+ *	The following register definitions are for
+ *	each DMA channel.
+ */
+#define	RBR_CFIG_A_REG			(DMC + 0x00010)
+#define	RBR_CFIG_B_REG			(DMC + 0x00018)
+#define	RBR_KICK_REG			(DMC + 0x00020)
+#define	RBR_STAT_REG			(DMC + 0x00028)
+#define	RBR_HDH_REG			(DMC + 0x00030)
+#define	RBR_HDL_REG			(DMC + 0x00038)
+
+#define	RBR_CFIG_A_STADDR_SHIFT		6			/* bits 17:6 */
+#define	RBR_CFIG_A_STDADDR_MASK		0x000000000003ffc0ULL
+#define	RBR_CFIG_A_STADDR_BASE_SHIFT	18			/* bits 43:18 */
+#define	RBR_CFIG_A_STDADDR_BASE_MASK	0x00000ffffffc0000ULL
+#define	RBR_CFIG_A_LEN_SHIFT		48			/* bits 63:48 */
+#define	RBR_CFIG_A_LEN_MASK		0xFFFF000000000000ULL
+
+typedef union _rbr_cfig_a_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t len:16;
+			uint32_t res1:4;
+			uint32_t staddr_base:12;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t staddr_base:12;
+			uint32_t res1:4;
+			uint32_t len:16;
+#endif
+		} hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t staddr_base:14;
+			uint32_t staddr:12;
+			uint32_t res2:6;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t res2:6;
+			uint32_t staddr:12;
+			uint32_t staddr_base:14;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t len:16;
+			uint32_t res1:4;
+			uint32_t staddr_base:12;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t staddr_base:12;
+			uint32_t res1:4;
+			uint32_t len:16;
+#endif
+		} hdw;
+#endif
+	} bits;
+} rbr_cfig_a_t, *p_rbr_cfig_a_t;
+
+
+#define	RBR_CFIG_B_BUFSZ0_SHIFT		0			/* bit 1:0 */
+#define	RBR_CFIG_B_BUFSZ0_MASK		0x0000000000000001ULL
+#define	RBR_CFIG_B_VLD0_SHIFT		7			/* bit 7 */
+#define	RBR_CFIG_B_VLD0			0x0000000000000008ULL
+#define	RBR_CFIG_B_VLD0_MASK		0x0000000000000008ULL
+#define	RBR_CFIG_B_BUFSZ1_SHIFT		8			/* bit 9:8 */
+#define	RBR_CFIG_B_BUFSZ1_MASK		0x0000000000000300ULL
+#define	RBR_CFIG_B_VLD1_SHIFT		15			/* bit 15 */
+#define	RBR_CFIG_B_VLD1			0x0000000000008000ULL
+#define	RBR_CFIG_B_VLD1_MASK		0x0000000000008000ULL
+#define	RBR_CFIG_B_BUFSZ2_SHIFT		16			/* bit 17:16 */
+#define	RBR_CFIG_B_BUFSZ2_MASK		0x0000000000030000ULL
+#define	RBR_CFIG_B_VLD2_SHIFT		23			/* bit 23 */
+#define	RBR_CFIG_B_VLD2			0x0000000000800000ULL
+#define	RBR_CFIG_B_BKSIZE_SHIFT		24			/* bit 25:24 */
+#define	RBR_CFIG_B_BKSIZE_MASK		0x0000000003000000ULL
+
+
+typedef union _rbr_cfig_b_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res1_1:6;
+			uint32_t bksize:2;
+			uint32_t vld2:1;
+			uint32_t res2:5;
+			uint32_t bufsz2:2;
+			uint32_t vld1:1;
+			uint32_t res3:5;
+			uint32_t bufsz1:2;
+			uint32_t vld0:1;
+			uint32_t res4:5;
+			uint32_t bufsz0:2;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t bufsz0:2;
+			uint32_t res4:5;
+			uint32_t vld0:1;
+			uint32_t bufsz1:2;
+			uint32_t res3:5;
+			uint32_t vld1:1;
+			uint32_t bufsz2:2;
+			uint32_t res2:5;
+			uint32_t vld2:1;
+			uint32_t bksize:2;
+			uint32_t res1_1:6;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} rbr_cfig_b_t, *p_rbr_cfig_b_t;
+
+
+#define	RBR_KICK_SHIFT			0			/* bit 15:0 */
+#define	RBR_KICK_MASK			0x00000000000ffff1ULL
+
+
+typedef union _rbr_kick_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res1_1:16;
+			uint32_t bkadd:16;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t bkadd:16;
+			uint32_t res1_1:16;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} rbr_kick_t, *p_rbr_kick_t;
+
+#define	RBR_STAT_QLEN_SHIFT		0		/* bit bit 15:0 */
+#define	RBR_STAT_QLEN_MASK		0x000000000000ffffULL
+#define	RBR_STAT_OFLOW_SHIFT		16		/* bit 16 */
+#define	RBR_STAT_OFLOW			0x0000000000010000ULL
+#define	RBR_STAT_OFLOW_MASK		0x0000000000010000ULL
+
+typedef union _rbr_stat_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res1_1:15;
+			uint32_t oflow:1;
+			uint32_t qlen:16;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t qlen:16;
+			uint32_t oflow:1;
+			uint32_t res1_1:15;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} rbr_stat_t, *p_rbr_stat_t;
+
+
+#define	RBR_HDH_HEAD_H_SHIFT		0			/* bit 11:0 */
+#define	RBR_HDH_HEAD_H_MASK		0x0000000000000fffULL
+typedef union _rbr_hdh_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res1_1:20;
+			uint32_t head_h:12;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t head_h:12;
+			uint32_t res1_1:20;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} rbr_hdh_t, *p_rbr_hdh_t;
+
+#define	RBR_HDL_HEAD_L_SHIFT		2			/* bit 31:2 */
+#define	RBR_HDL_HEAD_L_MASK		0x00000000FFFFFFFCULL
+
+typedef union _rbr_hdl_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t head_l:30;
+			uint32_t res2:2;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t res2:2;
+			uint32_t head_l:30;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} rbr_hdl_t, *p_rbr_hdl_t;
+
+#define NXGE_RXPKT_HDR_INPUT_PORT_MASK 0xC000
+#define NXGE_RXPKT_HDR_INPUT_PORT_SHIFT 14
+#define NXGE_RXPKT_HDR_MACCHECK_MASK 0x2000
+#define NXGE_RXPKT_HDR_MACCHECK_SHIFT 13
+#define NXGE_RXPKT_HDR_CLASS_MASK 0x1F00
+#define NXGE_RXPKT_HDR_CLASS_SHIFT 8
+#define NXGE_RXPKT_HDR_VLAN_MASK 0x0080
+#define NXGE_RXPKT_HDR_VLAN_SHIFT 7
+#define NXGE_RXPKT_HDR_LLCSNAP_MASK 0x0040
+#define NXGE_RXPKT_HDR_LLCSNAP_SHIFT 6
+#define NXGE_RXPKT_HDR_NOPORT_MASK 0x0020
+#define NXGE_RXPKT_HDR_NOPORT_SHIFT 5
+#define NXGE_RXPKT_HDR_BADIP_MASK 0x0010
+#define NXGE_RXPKT_HDR_BADIP_SHIFT 4
+#define NXGE_RXPKT_HDR_TCAMHIT_MASK 0x0008
+#define NXGE_RXPKT_HDR_TCAMHIT_SHIFT 3
+#define NXGE_RXPKT_HDR_TRES_MASK 0x0006
+#define NXGE_RXPKT_HDR_TRES_SHIFT 1
+#define NXGE_RXPKT_HDR_TZFVLD_MASK 0x0001
+#define NXGE_RXPKT_HDR_TZFVLD_SHIFT 0
+
+typedef union _rxpkt_short_hdr_t {
+	uint16_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint16_t inputport:2;
+		uint16_t maccheck:1;
+		uint16_t class:5;
+		uint16_t vlan:1;
+		uint16_t llcsnap:1;
+		uint16_t noport:1;
+		uint16_t badip:1;
+		uint16_t tcamhit:1;
+		uint16_t tres:2;
+		uint16_t tzfvld:1;
+#else
+		uint16_t class:5;
+		uint16_t maccheck:1;
+		uint16_t inputport:2;
+		uint16_t tzfvld:1;
+		uint16_t tres:2;
+		uint16_t tcamhit:1;
+		uint16_t badip:1;
+		uint16_t noport:1;
+		uint16_t llcsnap:1;
+		uint16_t vlan:1;
+
+#endif
+	} bits;
+} rxpkt_short_hdr_t, *p_rkpkt_short_hdr_t;
+
+/*
+ * Receive Completion Ring (RCR)
+ */
+#define	RCR_PKT_BUF_ADDR_SHIFT		0			/* bit 37:0 */
+#define	RCR_PKT_BUF_ADDR_SHIFT_FULL	6	/* fulll buffer address */
+#define	RCR_PKT_BUF_ADDR_MASK		0x0000003FFFFFFFFFULL
+#define	RCR_PKTBUFSZ_SHIFT		38			/* bit 39:38 */
+#define	RCR_PKTBUFSZ_MASK		0x000000C000000000ULL
+#define	RCR_L2_LEN_SHIFT		40			/* bit 39:38 */
+#define	RCR_L2_LEN_MASK			0x003fff0000000000ULL
+#define	RCR_DCF_ERROR_SHIFT		54			/* bit 54 */
+#define	RCR_DCF_ERROR_MASK		0x0040000000000000ULL
+#define	RCR_ERROR_SHIFT			55			/* bit 57:55 */
+#define	RCR_ERROR_MASK			0x0380000000000000ULL
+#define	RCR_PROMIS_SHIFT		58			/* bit 58 */
+#define	RCR_PROMIS_MASK			0x0400000000000000ULL
+#define	RCR_FRAG_SHIFT			59			/* bit 59 */
+#define	RCR_FRAG_MASK			0x0800000000000000ULL
+#define	RCR_ZERO_COPY_SHIFT		60			/* bit 60 */
+#define	RCR_ZERO_COPY_MASK		0x1000000000000000ULL
+#define	RCR_PKT_TYPE_SHIFT		61			/* bit 62:61 */
+#define	RCR_PKT_TYPE_MASK		0x6000000000000000ULL
+#define	RCR_MULTI_SHIFT			63			/* bit 63 */
+#define	RCR_MULTI_MASK			0x8000000000000000ULL
+
+#define	RCR_PKTBUFSZ_0			0x00
+#define	RCR_PKTBUFSZ_1			0x01
+#define	RCR_PKTBUFSZ_2			0x02
+#define	RCR_SINGLE_BLOCK		0x03
+
+#define	RCR_NO_ERROR			0x0
+#define	RCR_L2_ERROR			0x1
+#define	RCR_L4_CSUM_ERROR		0x3
+#define	RCR_FFLP_SOFT_ERROR		0x4
+#define	RCR_ZCP_SOFT_ERROR		0x5
+#define	RCR_ERROR_RESERVE		0x6
+#define	RCR_ERROR_RESERVE_END	0x7
+
+#define	RCR_PKT_TYPE_UDP		0x2
+#define	RCR_PKT_TYPE_TCP		0x1
+#define	RCR_PKT_TYPE_SCTP		0x3
+#define	RCR_PKT_TYPE_OTHERS		0x0
+#define	RCR_PKT_IS_TCP			0x2000000000000000ULL
+#define	RCR_PKT_IS_UDP			0x4000000000000000ULL
+#define	RCR_PKT_IS_SCTP			0x6000000000000000ULL
+
+
+typedef union _rcr_entry_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t multi:1;
+			uint32_t pkt_type:2;
+			uint32_t zero_copy:1;
+			uint32_t noport:1;
+			uint32_t promis:1;
+			uint32_t error:3;
+			uint32_t dcf_err:1;
+			uint32_t l2_len:14;
+			uint32_t pktbufsz:2;
+			uint32_t pkt_buf_addr:6;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t pkt_buf_addr:6;
+			uint32_t pktbufsz:2;
+			uint32_t l2_len:14;
+			uint32_t dcf_err:1;
+			uint32_t error:3;
+			uint32_t promis:1;
+			uint32_t noport:1;
+			uint32_t zero_copy:1;
+			uint32_t pkt_type:2;
+			uint32_t multi:1;
+#endif
+		} hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t pkt_buf_addr:32;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t pkt_buf_addr:32;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t multi:1;
+			uint32_t pkt_type:2;
+			uint32_t zero_copy:1;
+			uint32_t noport:1;
+			uint32_t promis:1;
+			uint32_t error:3;
+			uint32_t dcf_err:1;
+			uint32_t l2_len:14;
+			uint32_t pktbufsz:2;
+			uint32_t pkt_buf_addr:6;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t pkt_buf_addr:6;
+			uint32_t pktbufsz:2;
+			uint32_t l2_len:14;
+			uint32_t dcf_err:1;
+			uint32_t error:3;
+			uint32_t promis:1;
+			uint32_t noport:1;
+			uint32_t zero_copy:1;
+			uint32_t pkt_type:2;
+			uint32_t multi:1;
+#endif
+		} hdw;
+#endif
+	} bits;
+} rcr_entry_t, *p_rcr_entry_t;
+
+/*
+ * Receive Completion Ring Configuraiton.
+ * (for each DMA channel)
+ */
+#define	RCRCFIG_A_REG			(DMC + 0x00040)
+#define	RCRCFIG_B_REG			(DMC + 0x00048)
+#define	RCRSTAT_A_REG			(DMC + 0x00050)
+#define	RCRSTAT_B_REG			(DMC + 0x00058)
+#define	RCRSTAT_C_REG			(DMC + 0x00060)
+#define	RX_DMA_ENT_MSK_REG		(DMC + 0x00068)
+#define	RX_DMA_CTL_STAT_REG		(DMC + 0x00070)
+#define	RCR_FLSH_REG			(DMC + 0x00078)
+#ifdef OLD
+#define	RX_DMA_LOGA_REG			(DMC + 0x00080)
+#define	RX_DMA_LOGB_REG			(DMC + 0x00088)
+#endif
+#define	RX_DMA_CTL_STAT_DBG_REG		(DMC + 0x00098)
+
+/* (DMC + 0x00050) */
+#define	RCRCFIG_A_STADDR_SHIFT		6	/* bit 18:6 */
+#define	RCRCFIG_A_STADDR_MASK		0x000000000007FFC0ULL
+#define	RCRCFIG_A_STADDR_BASE_SHIF	19	/* bit 43:19 */
+#define	RCRCFIG_A_STADDR_BASE_MASK	0x00000FFFFFF80000ULL
+#define	RCRCFIG_A_LEN_SHIF		48	/* bit 63:48 */
+#define	RCRCFIG_A_LEN__MASK		0xFFFF000000000000ULL
+
+/* (DMC + 0x00058) */
+#define	RCRCFIG_B_TIMEOUT_SHIFT		0		/* bit 5:0 */
+#define	RCRCFIG_B_TIMEOUT_MASK		0x000000000000003FULL
+#define	RCRCFIG_B_ENTOUT_SHIFT		15		/* bit  15 */
+#define	RCRCFIG_B_TIMEOUT		0x0000000000008000ULL
+#define	RCRCFIG_B_PTHRES_SHIFT		16		/* bit 31:16 */
+#define	RCRCFIG_B_PTHRES_MASK		0x00000000FFFF0000ULL
+
+/* (DMC + 0x00060) */
+#define	RCRSTAT_A_QLEN_SHIFT		0		/* bit 15:0 */
+#define	RCRSTAT_A_QLEN_MASK		0x000000000000FFFFULL
+#define	RCRSTAT_A_PKT_OFL_SHIFT		16		/* bit 16 */
+#define	RCRSTAT_A_PKT_OFL_MASK		0x0000000000010000ULL
+#define	RCRSTAT_A_ENT_OFL_SHIFT		17		/* bit 17 */
+#define	RCRSTAT_A_ENT_QFL_MASK		0x0000000000020000ULL
+
+#define	RCRSTAT_C_TLPTR_H_SHIFT		0		/* bit 11:0 */
+#define	RCRSTAT_C_TLPTR_H_MASK		0x0000000000000FFFULL
+
+#define	RCRSTAT_D_TLPTR_L_SHIFT		3		/* bit 31:3 */
+#define	RCRSTAT_D_TLPTR_L_MASK		0x00000000FFFFFFF8ULL
+
+/* Receive DMA Interrupt Behavior: Event Mask  (DMC + 0x00068) */
+#define	RX_DMA_ENT_MSK_CFIGLOGPGE_SHIFT	0		/* bit 0: 0 to flag */
+#define	RX_DMA_ENT_MSK_CFIGLOGPGE_MASK	0x0000000000000001ULL
+#define	RX_DMA_ENT_MSK_RBRLOGPGE_SHIFT	1		/* bit 1: 0 to flag */
+#define	RX_DMA_ENT_MSK_RBRLOGPGE_MASK	0x0000000000000002ULL
+#define	RX_DMA_ENT_MSK_RBRFULL_SHIFT	2		/* bit 2: 0 to flag */
+#define	RX_DMA_ENT_MSK_RBRFULL_MASK	0x0000000000000004ULL
+#define	RX_DMA_ENT_MSK_RBREMPTY_SHIFT	3		/* bit 3: 0 to flag */
+#define	RX_DMA_ENT_MSK_RBREMPTY_MASK	0x0000000000000008ULL
+#define	RX_DMA_ENT_MSK_RCRFULL_SHIFT	4		/* bit 4: 0 to flag */
+#define	RX_DMA_ENT_MSK_RCRFULL_MASK	0x0000000000000010ULL
+#define	RX_DMA_ENT_MSK_RCRINCON_SHIFT	5		/* bit 5: 0 to flag */
+#define	RX_DMA_ENT_MSK_RCRINCON_MASK	0x0000000000000020ULL
+#define	RX_DMA_ENT_MSK_CONFIG_ERR_SHIFT	6		/* bit 6: 0 to flag */
+#define	RX_DMA_ENT_MSK_CONFIG_ERR_MASK	0x0000000000000040ULL
+#define	RX_DMA_ENT_MSK_RCRSH_FULL_SHIFT	7		/* bit 7: 0 to flag */
+#define	RX_DMA_ENT_MSK_RCRSH_FULL_MASK	0x0000000000000080ULL
+#define	RX_DMA_ENT_MSK_RBR_PRE_EMPTY_SHIFT	8	/* bit 8: 0 to flag */
+#define	RX_DMA_ENT_MSK_RBR_PRE_EMPTY_MASK	0x0000000000000100ULL
+#define	RX_DMA_ENT_MSK_WRED_DROP_SHIFT	9		/* bit 9: 0 to flag */
+#define	RX_DMA_ENT_MSK_WRED_DROP_MASK	0x0000000000000200ULL
+#define	RX_DMA_ENT_MSK_PTDROP_PKT_SHIFT	10		/* bit 10: 0 to flag */
+#define	RX_DMA_ENT_MSK_PTDROP_PKT_MASK	0x0000000000000400ULL
+#define	RX_DMA_ENT_MSK_RBR_PRE_PAR_SHIFT	11	/* bit 11: 0 to flag */
+#define	RX_DMA_ENT_MSK_RBR_PRE_PAR_MASK	0x0000000000000800ULL
+#define	RX_DMA_ENT_MSK_RCR_SHA_PAR_SHIFT	12	/* bit 12: 0 to flag */
+#define	RX_DMA_ENT_MSK_RCR_SHA_PAR_MASK	0x0000000000001000ULL
+#define	RX_DMA_ENT_MSK_RCRTO_SHIFT	13		/* bit 13: 0 to flag */
+#define	RX_DMA_ENT_MSK_RCRTO_MASK	0x0000000000002000ULL
+#define	RX_DMA_ENT_MSK_THRES_SHIFT	14		/* bit 14: 0 to flag */
+#define	RX_DMA_ENT_MSK_THRES_MASK	0x0000000000004000ULL
+#define	RX_DMA_ENT_MSK_DC_FIFO_ERR_SHIFT	16	/* bit 16: 0 to flag */
+#define	RX_DMA_ENT_MSK_DC_FIFO_ERR_MASK	0x0000000000010000ULL
+#define	RX_DMA_ENT_MSK_RCR_ACK_ERR_SHIFT	17	/* bit 17: 0 to flag */
+#define	RX_DMA_ENT_MSK_RCR_ACK_ERR_MASK	0x0000000000020000ULL
+#define	RX_DMA_ENT_MSK_RSP_DAT_ERR_SHIFT	18	/* bit 18: 0 to flag */
+#define	RX_DMA_ENT_MSK_RSP_DAT_ERR_MASK	0x0000000000040000ULL
+#define	RX_DMA_ENT_MSK_BYTE_EN_BUS_SHIFT	19	/* bit 19: 0 to flag */
+#define	RX_DMA_ENT_MSK_BYTE_EN_BUS_MASK	0x0000000000080000ULL
+#define	RX_DMA_ENT_MSK_RSP_CNT_ERR_SHIFT	20	/* bit 20: 0 to flag */
+#define	RX_DMA_ENT_MSK_RSP_CNT_ERR_MASK	0x0000000000100000ULL
+#define	RX_DMA_ENT_MSK_RBR_TMOUT_SHIFT	21		/* bit 21: 0 to flag */
+#define	RX_DMA_ENT_MSK_RBR_TMOUT_MASK	0x0000000000200000ULL
+#define	RX_DMA_ENT_MSK_ALL	(RX_DMA_ENT_MSK_CFIGLOGPGE_MASK |	\
+				RX_DMA_ENT_MSK_RBRLOGPGE_MASK |	\
+				RX_DMA_ENT_MSK_RBRFULL_MASK |		\
+				RX_DMA_ENT_MSK_RBREMPTY_MASK |		\
+				RX_DMA_ENT_MSK_RCRFULL_MASK |		\
+				RX_DMA_ENT_MSK_RCRINCON_MASK |		\
+				RX_DMA_ENT_MSK_CONFIG_ERR_MASK |	\
+				RX_DMA_ENT_MSK_RCRSH_FULL_MASK |	\
+				RX_DMA_ENT_MSK_RBR_PRE_EMPTY_MASK |	\
+				RX_DMA_ENT_MSK_WRED_DROP_MASK |	\
+				RX_DMA_ENT_MSK_PTDROP_PKT_MASK |	\
+				RX_DMA_ENT_MSK_RBR_PRE_PAR_MASK |	\
+				RX_DMA_ENT_MSK_RCR_SHA_PAR_MASK |	\
+				RX_DMA_ENT_MSK_RCRTO_MASK |		\
+				RX_DMA_ENT_MSK_THRES_MASK |		\
+				RX_DMA_ENT_MSK_DC_FIFO_ERR_MASK |	\
+				RX_DMA_ENT_MSK_RCR_ACK_ERR_MASK |	\
+				RX_DMA_ENT_MSK_RSP_DAT_ERR_MASK |	\
+				RX_DMA_ENT_MSK_BYTE_EN_BUS_MASK |	\
+				RX_DMA_ENT_MSK_RSP_CNT_ERR_MASK |	\
+				RX_DMA_ENT_MSK_RBR_TMOUT_MASK)
+
+/* Receive DMA Control and Status  (DMC + 0x00070) */
+#define	RX_DMA_CTL_STAT_PKTREAD_SHIFT	0	/* WO, bit 15:0 */
+#define	RX_DMA_CTL_STAT_PKTREAD_MASK	0x000000000000ffffULL
+#define	RX_DMA_CTL_STAT_PTRREAD_SHIFT	16	/* WO, bit 31:16 */
+#define	RX_DMA_CTL_STAT_PTRREAD_MASK	0x00000000FFFF0000ULL
+#define	RX_DMA_CTL_STAT_CFIGLOGPG_SHIFT 32	/* RO, bit 32 */
+#define	RX_DMA_CTL_STAT_CFIGLOGPG	0x0000000100000000ULL
+#define	RX_DMA_CTL_STAT_CFIGLOGPG_MASK	0x0000000100000000ULL
+#define	RX_DMA_CTL_STAT_RBRLOGPG_SHIFT	33	/* RO, bit 33 */
+#define	RX_DMA_CTL_STAT_RBRLOGPG	0x0000000200000000ULL
+#define	RX_DMA_CTL_STAT_RBRLOGPG_MASK	0x0000000200000000ULL
+#define	RX_DMA_CTL_STAT_RBRFULL_SHIFT	34	/* RO, bit 34 */
+#define	RX_DMA_CTL_STAT_RBRFULL		0x0000000400000000ULL
+#define	RX_DMA_CTL_STAT_RBRFULL_MASK	0x0000000400000000ULL
+#define	RX_DMA_CTL_STAT_RBREMPTY_SHIFT	35	/* RW1C, bit 35 */
+#define	RX_DMA_CTL_STAT_RBREMPTY	0x0000000800000000ULL
+#define	RX_DMA_CTL_STAT_RBREMPTY_MASK	0x0000000800000000ULL
+#define	RX_DMA_CTL_STAT_RCRFULL_SHIFT	36	/* RW1C, bit 36 */
+#define	RX_DMA_CTL_STAT_RCRFULL		0x0000001000000000ULL
+#define	RX_DMA_CTL_STAT_RCRFULL_MASK	0x0000001000000000ULL
+#define	RX_DMA_CTL_STAT_RCRINCON_SHIFT	37	/* RO, bit 37 */
+#define	RX_DMA_CTL_STAT_RCRINCON	0x0000002000000000ULL
+#define	RX_DMA_CTL_STAT_RCRINCON_MASK	0x0000002000000000ULL
+#define	RX_DMA_CTL_STAT_CONFIG_ERR_SHIFT 38	/* RO, bit 38 */
+#define	RX_DMA_CTL_STAT_CONFIG_ERR	0x0000004000000000ULL
+#define	RX_DMA_CTL_STAT_CONFIG_ERR_MASK	0x0000004000000000ULL
+#define	RX_DMA_CTL_STAT_RCR_SHDW_FULL_SHIFT 39	/* RO, bit 39 */
+#define	RX_DMA_CTL_STAT_RCR_SHDW_FULL 0x0000008000000000ULL
+#define	RX_DMA_CTL_STAT_RCR_SHDW_FULL_MASK 0x0000008000000000ULL
+#define	RX_DMA_CTL_STAT_RBR_PRE_EMTY_MASK  0x0000010000000000ULL
+#define	RX_DMA_CTL_STAT_RBR_PRE_EMTY_SHIFT 40	/* RO, bit 40 */
+#define	RX_DMA_CTL_STAT_RBR_PRE_EMTY 0x0000010000000000ULL
+#define	RX_DMA_CTL_STAT_RBR_PRE_EMTY_MASK  0x0000010000000000ULL
+#define	RX_DMA_CTL_STAT_WRED_DROP_SHIFT 41	/* RO, bit 41 */
+#define	RX_DMA_CTL_STAT_WRED_DROP 0x0000020000000000ULL
+#define	RX_DMA_CTL_STAT_WRED_DROP_MASK  0x0000020000000000ULL
+#define	RX_DMA_CTL_STAT_PORT_DROP_PKT_SHIFT 42	/* RO, bit 42 */
+#define	RX_DMA_CTL_STAT_PORT_DROP_PKT 0x0000040000000000ULL
+#define	RX_DMA_CTL_STAT_PORT_DROP_PKT_MASK  0x0000040000000000ULL
+#define	RX_DMA_CTL_STAT_RBR_PRE_PAR_SHIFT 43	/* RO, bit 43 */
+#define	RX_DMA_CTL_STAT_RBR_PRE_PAR 0x0000080000000000ULL
+#define	RX_DMA_CTL_STAT_RBR_PRE_PAR_MASK  0x0000080000000000ULL
+#define	RX_DMA_CTL_STAT_RCR_SHA_PAR_SHIFT 44	/* RO, bit 44 */
+#define	RX_DMA_CTL_STAT_RCR_SHA_PAR 0x0000100000000000ULL
+#define	RX_DMA_CTL_STAT_RCR_SHA_PAR_MASK  0x0000100000000000ULL
+#define	RX_DMA_CTL_STAT_RCRTO_SHIFT	45	/* RW1C, bit 45 */
+#define	RX_DMA_CTL_STAT_RCRTO		0x0000200000000000ULL
+#define	RX_DMA_CTL_STAT_RCRTO_MASK	0x0000200000000000ULL
+#define	RX_DMA_CTL_STAT_RCRTHRES_SHIFT	46	/* RO, bit 46 */
+#define	RX_DMA_CTL_STAT_RCRTHRES	0x0000400000000000ULL
+#define	RX_DMA_CTL_STAT_RCRTHRES_MASK	0x0000400000000000ULL
+#define	RX_DMA_CTL_STAT_MEX_SHIFT	47	/* RW, bit 47 */
+#define	RX_DMA_CTL_STAT_MEX		0x0000800000000000ULL
+#define	RX_DMA_CTL_STAT_MEX_MASK	0x0000800000000000ULL
+#define	RX_DMA_CTL_STAT_DC_FIFO_ERR_SHIFT	48	/* RW1C, bit 48 */
+#define	RX_DMA_CTL_STAT_DC_FIFO_ERR		0x0001000000000000ULL
+#define	RX_DMA_CTL_STAT_DC_FIFO_ERR_MASK	0x0001000000000000ULL
+#define	RX_DMA_CTL_STAT_RCR_ACK_ERR_SHIFT	49	/* RO, bit 49 */
+#define	RX_DMA_CTL_STAT_RCR_ACK_ERR		0x0002000000000000ULL
+#define	RX_DMA_CTL_STAT_RCR_ACK_ERR_MASK	0x0002000000000000ULL
+#define	RX_DMA_CTL_STAT_RSP_DAT_ERR_SHIFT	50	/* RO, bit 50 */
+#define	RX_DMA_CTL_STAT_RSP_DAT_ERR		0x0004000000000000ULL
+#define	RX_DMA_CTL_STAT_RSP_DAT_ERR_MASK	0x0004000000000000ULL
+
+#define	RX_DMA_CTL_STAT_BYTE_EN_BUS_SHIFT	51	/* RO, bit 51 */
+#define	RX_DMA_CTL_STAT_BYTE_EN_BUS		0x0008000000000000ULL
+#define	RX_DMA_CTL_STAT_BYTE_EN_BUS_MASK	0x0008000000000000ULL
+
+#define	RX_DMA_CTL_STAT_RSP_CNT_ERR_SHIFT	52	/* RO, bit 52 */
+#define	RX_DMA_CTL_STAT_RSP_CNT_ERR		0x0010000000000000ULL
+#define	RX_DMA_CTL_STAT_RSP_CNT_ERR_MASK	0x0010000000000000ULL
+
+#define	RX_DMA_CTL_STAT_RBR_TMOUT_SHIFT	53	/* RO, bit 53 */
+#define	RX_DMA_CTL_STAT_RBR_TMOUT		0x0020000000000000ULL
+#define	RX_DMA_CTL_STAT_RBR_TMOUT_MASK	0x0020000000000000ULL
+#define	RX_DMA_CTRL_STAT_ENT_MASK_SHIFT 32
+#define	RX_DMA_CTL_STAT_ERROR 			(RX_DMA_ENT_MSK_ALL << \
+						RX_DMA_CTRL_STAT_ENT_MASK_SHIFT)
+
+/* the following are write 1 to clear bits */
+#define	RX_DMA_CTL_STAT_WR1C	RX_DMA_CTL_STAT_RBREMPTY | \
+				RX_DMA_CTL_STAT_RCR_SHDW_FULL | \
+				RX_DMA_CTL_STAT_RBR_PRE_EMTY | \
+				RX_DMA_CTL_STAT_WRED_DROP | \
+				RX_DMA_CTL_STAT_PORT_DROP_PKT | \
+				RX_DMA_CTL_STAT_RCRTO | \
+				RX_DMA_CTL_STAT_RCRTHRES | \
+				RX_DMA_CTL_STAT_DC_FIFO_ERR
+
+/* Receive DMA Interrupt Behavior: Force an update to RCR  (DMC + 0x00078 */
+#define	RCR_FLSH_SHIFT			0	/* RW, bit 0:0 */
+#define	RCR_FLSH_SET			0x0000000000000001ULL
+#define	RCR_FLSH_MASK			0x0000000000000001ULL
+
+/* Receive DMA Interrupt Behavior: the first error log  (DMC + 0x00080 */
+#define	RX_DMA_LOGA_ADDR_SHIFT		0	/* RO, bit 11:0 */
+#define	RX_DMA_LOGA_ADDR		0x0000000000000FFFULL
+#define	RX_DMA_LOGA_ADDR_MASK		0x0000000000000FFFULL
+#define	RX_DMA_LOGA_TYPE_SHIFT		28	/* RO, bit 30:28 */
+#define	RX_DMA_LOGA_TYPE		0x0000000070000000ULL
+#define	RX_DMA_LOGA_TYPE_MASK		0x0000000070000FFFULL
+#define	RX_DMA_LOGA_MULTI_SHIFT		28	/* RO, bit 30:28 */
+#define	RX_DMA_LOGA_MULTI		0x0000000080000000ULL
+#define	RX_DMA_LOGA_MULTI_MASK		0x0000000080000FFFULL
+
+/* Receive DMA Interrupt Behavior: the first error log  (DMC + 0x00088 */
+#define	RX_DMA_LOGA_ADDR_L_SHIFT	0	/* RO, bit 31:0 */
+#define	RX_DMA_LOGA_ADDRL_L		0x00000000FFFFFFFFULL
+#define	RX_DMA_LOGA_ADDR_LMASK		0x00000000FFFFFFFFULL
+
+typedef union _rcrcfig_a_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t len:16;
+			uint32_t res1:4;
+			uint32_t staddr_base:12;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t staddr_base:12;
+			uint32_t res1:4;
+			uint32_t len:16;
+#endif
+		} hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t staddr_base:13;
+			uint32_t staddr:13;
+			uint32_t res2:6;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t res2:6;
+			uint32_t staddr:13;
+			uint32_t staddr_base:13;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t len:16;
+			uint32_t res1:4;
+			uint32_t staddr_base:12;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t staddr_base:12;
+			uint32_t res1:4;
+			uint32_t len:16;
+#endif
+		} hdw;
+#endif
+	} bits;
+} rcrcfig_a_t, *p_rcrcfig_a_t;
+
+
+typedef union _rcrcfig_b_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t pthres:16;
+			uint32_t entout:1;
+			uint32_t res1:9;
+			uint32_t timeout:6;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t timeout:6;
+			uint32_t res1:9;
+			uint32_t entout:1;
+			uint32_t pthres:16;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} rcrcfig_b_t, *p_rcrcfig_b_t;
+
+
+typedef union _rcrstat_a_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res1:16;
+			uint32_t qlen:16;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t qlen:16;
+			uint32_t res1:16;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} rcrstat_a_t, *p_rcrstat_a_t;
+
+
+typedef union _rcrstat_b_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res1:20;
+			uint32_t tlptr_h:12;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t tlptr_h:12;
+			uint32_t res1:20;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} rcrstat_b_t, *p_rcrstat_b_t;
+
+
+typedef union _rcrstat_c_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t tlptr_l:29;
+			uint32_t res1:3;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t res1:3;
+			uint32_t tlptr_l:29;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} rcrstat_c_t, *p_rcrstat_c_t;
+
+
+/* Receive DMA Event Mask */
+typedef union _rx_dma_ent_msk_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t rsrvd2:10;
+			uint32_t rbr_tmout:1;
+			uint32_t rsp_cnt_err:1;
+			uint32_t byte_en_bus:1;
+			uint32_t rsp_dat_err:1;
+			uint32_t rcr_ack_err:1;
+			uint32_t dc_fifo_err:1;
+			uint32_t rsrvd:1;
+			uint32_t rcrthres:1;
+			uint32_t rcrto:1;
+			uint32_t rcr_sha_par:1;
+			uint32_t rbr_pre_par:1;
+			uint32_t port_drop_pkt:1;
+			uint32_t wred_drop:1;
+			uint32_t rbr_pre_empty:1;
+			uint32_t rcr_shadow_full:1;
+			uint32_t config_err:1;
+			uint32_t rcrincon:1;
+			uint32_t rcrfull:1;
+			uint32_t rbr_empty:1;
+			uint32_t rbrfull:1;
+			uint32_t rbrlogpage:1;
+			uint32_t cfiglogpage:1;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t cfiglogpage:1;
+			uint32_t rbrlogpage:1;
+			uint32_t rbrfull:1;
+			uint32_t rbr_empty:1;
+			uint32_t rcrfull:1;
+			uint32_t rcrincon:1;
+			uint32_t config_err:1;
+			uint32_t rcr_shadow_full:1;
+			uint32_t rbr_pre_empty:1;
+			uint32_t wred_drop:1;
+			uint32_t port_drop_pkt:1;
+			uint32_t rbr_pre_par:1;
+			uint32_t rcr_sha_par:1;
+			uint32_t rcrto:1;
+			uint32_t rcrthres:1;
+			uint32_t rsrvd:1;
+			uint32_t dc_fifo_err:1;
+			uint32_t rcr_ack_err:1;
+			uint32_t rsp_dat_err:1;
+			uint32_t byte_en_bus:1;
+			uint32_t rsp_cnt_err:1;
+			uint32_t rbr_tmout:1;
+			uint32_t rsrvd2:10;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} rx_dma_ent_msk_t, *p_rx_dma_ent_msk_t;
+
+
+/* Receive DMA Control and Status */
+typedef union _rx_dma_ctl_stat_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t rsrvd:10;
+			uint32_t rbr_tmout:1;
+			uint32_t rsp_cnt_err:1;
+			uint32_t byte_en_bus:1;
+			uint32_t rsp_dat_err:1;
+			uint32_t rcr_ack_err:1;
+			uint32_t dc_fifo_err:1;
+			uint32_t mex:1;
+			uint32_t rcrthres:1;
+			uint32_t rcrto:1;
+			uint32_t rcr_sha_par:1;
+			uint32_t rbr_pre_par:1;
+			uint32_t port_drop_pkt:1;
+			uint32_t wred_drop:1;
+			uint32_t rbr_pre_empty:1;
+			uint32_t rcr_shadow_full:1;
+			uint32_t config_err:1;
+			uint32_t rcrincon:1;
+			uint32_t rcrfull:1;
+			uint32_t rbr_empty:1;
+			uint32_t rbrfull:1;
+			uint32_t rbrlogpage:1;
+			uint32_t cfiglogpage:1;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t cfiglogpage:1;
+			uint32_t rbrlogpage:1;
+			uint32_t rbrfull:1;
+			uint32_t rbr_empty:1;
+			uint32_t rcrfull:1;
+			uint32_t rcrincon:1;
+			uint32_t config_err:1;
+			uint32_t rcr_shadow_full:1;
+			uint32_t rbr_pre_empty:1;
+			uint32_t wred_drop:1;
+			uint32_t port_drop_pkt:1;
+			uint32_t rbr_pre_par:1;
+			uint32_t rcr_sha_par:1;
+			uint32_t rcrto:1;
+			uint32_t rcrthres:1;
+			uint32_t mex:1;
+			uint32_t dc_fifo_err:1;
+			uint32_t rcr_ack_err:1;
+			uint32_t rsp_dat_err:1;
+			uint32_t byte_en_bus:1;
+			uint32_t rsp_cnt_err:1;
+			uint32_t rbr_tmout:1;
+			uint32_t rsrvd:10;
+#endif
+		} hdw;
+
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t ptrread:16;
+			uint32_t pktread:16;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t pktread:16;
+			uint32_t ptrread:16;
+
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t rsrvd:10;
+			uint32_t rbr_tmout:1;
+			uint32_t rsp_cnt_err:1;
+			uint32_t byte_en_bus:1;
+			uint32_t rsp_dat_err:1;
+			uint32_t rcr_ack_err:1;
+			uint32_t dc_fifo_err:1;
+			uint32_t mex:1;
+			uint32_t rcrthres:1;
+			uint32_t rcrto:1;
+			uint32_t rcr_sha_par:1;
+			uint32_t rbr_pre_par:1;
+			uint32_t port_drop_pkt:1;
+			uint32_t wred_drop:1;
+			uint32_t rbr_pre_empty:1;
+			uint32_t rcr_shadow_full:1;
+			uint32_t config_err:1;
+			uint32_t rcrincon:1;
+			uint32_t rcrfull:1;
+			uint32_t rbr_empty:1;
+			uint32_t rbrfull:1;
+			uint32_t rbrlogpage:1;
+			uint32_t cfiglogpage:1;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t cfiglogpage:1;
+			uint32_t rbrlogpage:1;
+			uint32_t rbrfull:1;
+			uint32_t rbr_empty:1;
+			uint32_t rcrfull:1;
+			uint32_t rcrincon:1;
+			uint32_t config_err:1;
+			uint32_t rcr_shadow_full:1;
+			uint32_t rbr_pre_empty:1;
+			uint32_t wred_drop:1;
+			uint32_t port_drop_pkt:1;
+			uint32_t rbr_pre_par:1;
+			uint32_t rcr_sha_par:1;
+			uint32_t rcrto:1;
+			uint32_t rcrthres:1;
+			uint32_t mex:1;
+			uint32_t dc_fifo_err:1;
+			uint32_t rcr_ack_err:1;
+			uint32_t rsp_dat_err:1;
+			uint32_t byte_en_bus:1;
+			uint32_t rsp_cnt_err:1;
+			uint32_t rbr_tmout:1;
+			uint32_t rsrvd:10;
+#endif
+		} hdw;
+#endif
+	} bits;
+} rx_dma_ctl_stat_t, *p_rx_dma_ctl_stat_t;
+
+typedef union _rcr_flsh_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res1_1:31;
+			uint32_t flsh:1;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t flsh:1;
+			uint32_t res1_1:31;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} rcr_flsh_t, *p_rcr_flsh_t;
+
+
+typedef union _rx_dma_loga_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t multi:1;
+			uint32_t type:3;
+			uint32_t res1:16;
+			uint32_t addr:12;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t addr:12;
+			uint32_t res1:16;
+			uint32_t type:3;
+			uint32_t multi:1;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} rx_dma_loga_t, *p_rx_dma_loga_t;
+
+
+typedef union _rx_dma_logb_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t addr_l:32;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t addr_l:32;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} rx_dma_logb_t, *p_rx_dma_logb_t;
+
+
+#define	RX_DMA_MAILBOX_BYTE_LENGTH	64
+#define	RX_DMA_MBOX_UNUSED_1		8
+#define	RX_DMA_MBOX_UNUSED_2		16
+
+/* TODO: check the endianness */
+typedef struct _rxdma_mailbox_t {
+	rx_dma_ctl_stat_t	rxdma_ctl_stat;		/* 8 bytes */
+	rbr_stat_t		rbr_stat;		/* 8 bytes */
+	uint32_t		rbr_hdl;		/* 4 bytes (31:0) */
+	uint32_t		rbr_hdh;		/* 4 bytes (31:0) */
+	uint32_t		resv_1[RX_DMA_MBOX_UNUSED_1];
+	uint32_t		rcrstat_c;		/* 4 bytes (31:0) */
+	uint32_t		rcrstat_b;		/* 4 bytes (31:0) */
+	rcrstat_a_t		rcrstat_a;		/* 8 bytes */
+	uint32_t		resv_2[RX_DMA_MBOX_UNUSED_2];
+} rxdma_mailbox_t, *p_rxdma_mailbox_t;
+
+
+
+typedef union _rx_disc_cnt_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res_1:15;
+			uint32_t oflow:1;
+			uint32_t count:16;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t count:16;
+			uint32_t oflow:1;
+			uint32_t res_1:15;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} rx_disc_cnt_t, *p_rx_disc_cnt_t;
+
+#define	RXMISC_DISCARD_REG		(DMC + 0x00090)
+
+#ifdef OLD
+/*
+ * RBR Empty: If the RBR is empty or the prefetch buffer is empty,
+ * packets will be discarded (Each RBR has one).
+ * (16 channels, 0x200)
+ */
+#define	RDC_PRE_EMPTY_REG		(DMC + 0x000B0)
+#define	RDC_PRE_EMPTY_OFFSET(channel)	(RDC_PRE_EMPTY_REG + \
+						(DMC_OFFSET(channel))
+
+typedef union _rdc_pre_empty_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res_1:15;
+			uint32_t oflow:1;
+			uint32_t count:16;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t count:16;
+			uint32_t oflow:1;
+			uint32_t res_1:15;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} rdc_pre_empty_t, *p_rdc_pre_empty_t;
+#endif
+
+
+#define	FZC_DMC_REG_SIZE		0x20
+#define	FZC_DMC_OFFSET(channel)		(FZC_DMC_REG_SIZE * channel)
+
+/* WRED discard count register (16, 0x40) */
+#define	RED_DIS_CNT_REG			(FZC_DMC + 0x30008)
+#define	RED_DMC_OFFSET(channel)		(0x40 * channel)
+#define	RDC_DIS_CNT_OFFSET(rdc)	(RED_DIS_CNT_REG + RED_DMC_OFFSET(rdc))
+
+typedef union _red_disc_cnt_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res_1:15;
+			uint32_t oflow:1;
+			uint32_t count:16;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t count:16;
+			uint32_t oflow:1;
+			uint32_t res_1:15;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} red_disc_cnt_t, *p_red_disc_cnt_t;
+
+
+#define	RDMC_PRE_PAR_ERR_REG			(FZC_DMC + 0x00078)
+#define	RDMC_SHA_PAR_ERR_REG			(FZC_DMC + 0x00080)
+
+typedef union _rdmc_par_err_log {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res_1:16;
+			uint32_t err:1;
+			uint32_t merr:1;
+			uint32_t res:6;
+			uint32_t addr:8;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t addr:8;
+			uint32_t res:6;
+			uint32_t merr:1;
+			uint32_t err:1;
+			uint32_t res_1:16;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} rdmc_par_err_log_t, *p_rdmc_par_err_log_t;
+
+
+/* Used for accessing RDMC Memory */
+#define	RDMC_MEM_ADDR_REG			(FZC_DMC + 0x00088)
+
+
+typedef union _rdmc_mem_addr {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+
+#define	RDMC_MEM_ADDR_PREFETCH 0
+#define	RDMC_MEM_ADDR_SHADOW 1
+
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res_1:23;
+			uint32_t pre_shad:1;
+			uint32_t addr:8;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t addr:8;
+			uint32_t pre_shad:1;
+			uint32_t res_1:23;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} rdmc_mem_addr_t, *p_rdmc_mem_addr_t;
+
+
+#define	RDMC_MEM_DATA0_REG			(FZC_DMC + 0x00090)
+#define	RDMC_MEM_DATA1_REG			(FZC_DMC + 0x00098)
+#define	RDMC_MEM_DATA2_REG			(FZC_DMC + 0x000A0)
+#define	RDMC_MEM_DATA3_REG			(FZC_DMC + 0x000A8)
+#define	RDMC_MEM_DATA4_REG			(FZC_DMC + 0x000B0)
+
+typedef union _rdmc_mem_data {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t data;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t data;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} rdmc_mem_data_t, *p_rdmc_mem_data_t;
+
+
+typedef union _rdmc_mem_access {
+#define	RDMC_MEM_READ 1
+#define	RDMC_MEM_WRITE 2
+	uint32_t data[5];
+	uint8_t addr;
+	uint8_t location;
+} rdmc_mem_access_t, *p_rdmc_mem_access_t;
+
+
+#define	RX_CTL_DAT_FIFO_STAT_REG			(FZC_DMC + 0x000B8)
+#define	RX_CTL_DAT_FIFO_MASK_REG			(FZC_DMC + 0x000C0)
+#define	RX_CTL_DAT_FIFO_STAT_DBG_REG		(FZC_DMC + 0x000D0)
+
+typedef union _rx_ctl_dat_fifo {
+#define	FIFO_EOP_PORT0 0x1
+#define	FIFO_EOP_PORT1 0x2
+#define	FIFO_EOP_PORT2 0x4
+#define	FIFO_EOP_PORT3 0x8
+#define	FIFO_EOP_ALL 0xF
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res_1:23;
+			uint32_t id_mismatch:1;
+			uint32_t zcp_eop_err:4;
+			uint32_t ipp_eop_err:4;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t ipp_eop_err:4;
+			uint32_t zcp_eop_err:4;
+			uint32_t id_mismatch:1;
+			uint32_t res_1:23;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} rx_ctl_dat_fifo_mask_t, rx_ctl_dat_fifo_stat_t,
+	rx_ctl_dat_fifo_stat_dbg_t, *p_rx_ctl_dat_fifo_t;
+
+
+
+#define	RDMC_TRAINING_VECTOR_REG		(FZC_DMC + 0x000C8)
+
+typedef union _rx_training_vect {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+			uint32_t tv;
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} rx_training_vect_t, *p_rx_training_vect_t;
+
+#define	RXCTL_IPP_EOP_ERR_MASK	0x0000000FULL
+#define	RXCTL_IPP_EOP_ERR_SHIFT	0x0
+#define	RXCTL_ZCP_EOP_ERR_MASK	0x000000F0ULL
+#define	RXCTL_ZCP_EOP_ERR_SHIFT	0x4
+#define	RXCTL_ID_MISMATCH_MASK	0x00000100ULL
+#define	RXCTL_ID_MISMATCH_SHIFT	0x8
+
+
+/*
+ * Receive Packet Header Format
+ * Packet header before the packet.
+ * The minimum is 2 bytes and the max size is 18 bytes.
+ * TODO: not sure about the endianness
+ */
+/*
+ * Packet header format 0 (2 bytes).
+ */
+typedef union _rx_pkt_hdr0_t {
+	uint16_t value;
+	struct {
+#if	defined(_BIT_FIELDS_HTOL)
+		uint16_t inputport:2;
+		uint16_t maccheck:1;
+		uint16_t class:5;
+		uint16_t vlan:1;
+		uint16_t llcsnap:1;
+		uint16_t noport:1;
+		uint16_t badip:1;
+		uint16_t tcamhit:1;
+		uint16_t tres:2;
+		uint16_t tzfvld:1;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint16_t tzfvld:1;
+		uint16_t tres:2;
+		uint16_t tcamhit:1;
+		uint16_t badip:1;
+		uint16_t noport:1;
+		uint16_t llcsnap:1;
+		uint16_t vlan:1;
+		uint16_t class:5;
+		uint16_t maccheck:1;
+		uint16_t inputport:2;
+#endif
+	} bits;
+} rx_pkt_hdr0_t, *p_rx_pkt_hdr0_t;
+
+
+/*
+ * Packet header format 1.
+ */
+typedef union _rx_pkt_hdr1_b0_t {
+	uint8_t value;
+	struct  {
+#if	defined(_BIT_FIELDS_HTOL)
+		uint8_t hwrsvd:8;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint8_t hwrsvd:8;
+#endif
+	} bits;
+} rx_pkt_hdr1_b0_t, *p_rx_pkt_hdr1_b0_t;
+
+typedef union _rx_pkt_hdr1_b1_t {
+	uint8_t value;
+	struct  {
+#if	defined(_BIT_FIELDS_HTOL)
+		uint8_t tcammatch:8;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint8_t tcammatch:8;
+#endif
+	} bits;
+} rx_pkt_hdr1_b1_t, *p_rx_pkt_hdr1_b1_t;
+
+typedef union _rx_pkt_hdr1_b2_t {
+	uint8_t value;
+	struct  {
+#if	defined(_BIT_FIELDS_HTOL)
+		uint8_t resv:2;
+		uint8_t hashhit:1;
+		uint8_t exact:1;
+		uint8_t hzfvld:1;
+		uint8_t hashidx:3;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint8_t hashidx:3;
+		uint8_t hzfvld:1;
+		uint8_t exact:1;
+		uint8_t hashhit:1;
+		uint8_t resv:2;
+#endif
+	} bits;
+} rx_pkt_hdr1_b2_t, *p_rx_pkt_hdr1_b2_t;
+
+typedef union _rx_pkt_hdr1_b3_t {
+	uint8_t value;
+	struct  {
+#if	defined(_BIT_FIELDS_HTOL)
+		uint8_t zc_resv:8;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint8_t zc_resv:8;
+#endif
+	} bits;
+} rx_pkt_hdr1_b3_t, *p_rx_pkt_hdr1_b3_t;
+
+typedef union _rx_pkt_hdr1_b4_t {
+	uint8_t value;
+	struct  {
+#if	defined(_BIT_FIELDS_HTOL)
+		uint8_t resv:4;
+		uint8_t zflowid:4;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint8_t zflowid:4;
+		uint8_t resv:4;
+#endif
+	} bits;
+} rx_pkt_hdr1_b4_t, *p_rx_pkt_hdr1_b4_t;
+
+typedef union _rx_pkt_hdr1_b5_t {
+	uint8_t value;
+	struct  {
+#if	defined(_BIT_FIELDS_HTOL)
+		uint8_t zflowid:8;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint8_t zflowid:8;
+#endif
+	} bits;
+} rx_pkt_hdr1_b5_t, *p_rx_pkt_hdr1_b5_t;
+
+typedef union _rx_pkt_hdr1_b6_t {
+	uint8_t value;
+	struct  {
+#if	defined(_BIT_FIELDS_HTOL)
+		uint8_t hashval2:8;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint8_t hashval2:8;
+#endif
+	} bits;
+} rx_pkt_hdr1_b6_t, *p_rx_pkt_hdr1_b6_t;
+
+typedef union _rx_pkt_hdr1_b7_t {
+	uint8_t value;
+	struct  {
+#if	defined(_BIT_FIELDS_HTOL)
+		uint8_t hashval2:8;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint8_t hashval2:8;
+#endif
+	} bits;
+} rx_pkt_hdr1_b7_t, *p_rx_pkt_hdr1_b7_t;
+
+typedef union _rx_pkt_hdr1_b8_t {
+	uint8_t value;
+	struct  {
+#if defined(_BIT_FIELDS_HTOL)
+		uint8_t resv:4;
+		uint8_t h1:4;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint8_t h1:4;
+		uint8_t resv:4;
+#endif
+	} bits;
+} rx_pkt_hdr1_b8_t, *p_rx_pkt_hdr1_b8_t;
+
+typedef union _rx_pkt_hdr1_b9_t {
+	uint8_t value;
+	struct  {
+#if defined(_BIT_FIELDS_HTOL)
+		uint8_t h1:8;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint8_t h1:8;
+#endif
+	} bits;
+} rx_pkt_hdr1_b9_t, *p_rx_pkt_hdr1_b9_t;
+
+typedef union _rx_pkt_hdr1_b10_t {
+	uint8_t value;
+	struct  {
+#if defined(_BIT_FIELDS_HTOL)
+		uint8_t resv:4;
+		uint8_t h1:4;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint8_t h1:4;
+		uint8_t resv:4;
+#endif
+	} bits;
+} rx_pkt_hdr1_b10_t, *p_rx_pkt_hdr1_b10_t;
+
+typedef union _rx_pkt_hdr1_b11_b12_t {
+	uint16_t value;
+	struct {
+#if	defined(_BIT_FIELDS_HTOL)
+		uint16_t h1_1:8;
+		uint16_t h1_2:8;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint16_t h1_2:8;
+		uint16_t h1_1:8;
+#endif
+	} bits;
+} rx_pkt_hdr1_b11_b12_t, *p_rx_pkt_hdr1_b11_b12_t;
+
+typedef union _rx_pkt_hdr1_b13_t {
+	uint8_t value;
+	struct  {
+#if defined(_BIT_FIELDS_HTOL)
+		uint8_t usr_data:8;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint8_t usr_data:8;
+#endif
+	} bits;
+} rx_pkt_hdr1_b13_t, *p_rx_pkt_hdr1_b13_t;
+
+typedef union _rx_pkt_hdr1_b14_b17_t {
+	uint32_t value;
+	struct  {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t usr_data_1:8;
+		uint32_t usr_data_2:8;
+		uint32_t usr_data_3:8;
+		uint32_t usr_data_4:8;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t usr_data_4:8;
+		uint32_t usr_data_3:8;
+		uint32_t usr_data_2:8;
+		uint32_t usr_data_1:8;
+#endif
+	} bits;
+} rx_pkt_hdr1_b14_b17_t, *p_rx_pkt_hdr1_b14_b17_t;
+
+/* Receive packet header 1 format (18 bytes) */
+typedef struct _rx_pkt_hdr_t {
+	rx_pkt_hdr1_b0_t		rx_hdr1_b0;
+	rx_pkt_hdr1_b1_t		rx_hdr1_b1;
+	rx_pkt_hdr1_b2_t		rx_hdr1_b2;
+	rx_pkt_hdr1_b3_t		rx_hdr1_b3;
+	rx_pkt_hdr1_b4_t		rx_hdr1_b4;
+	rx_pkt_hdr1_b5_t		rx_hdr1_b5;
+	rx_pkt_hdr1_b6_t		rx_hdr1_b6;
+	rx_pkt_hdr1_b7_t		rx_hdr1_b7;
+	rx_pkt_hdr1_b8_t		rx_hdr1_b8;
+	rx_pkt_hdr1_b9_t		rx_hdr1_b9;
+	rx_pkt_hdr1_b10_t		rx_hdr1_b10;
+	rx_pkt_hdr1_b11_b12_t		rx_hdr1_b11_b12;
+	rx_pkt_hdr1_b13_t		rx_hdr1_b13;
+	rx_pkt_hdr1_b14_b17_t		rx_hdr1_b14_b17;
+} rx_pkt_hdr1_t, *p_rx_pkt_hdr1_t;
+
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif	/* _SYS_NXGE_NXGE_RXDMA_HW_H */
diff --git a/drivers/net/nxge/include/nxge_sr_hw.h b/drivers/net/nxge/include/nxge_sr_hw.h
new file mode 100644
index 0000000..259bba1
--- /dev/null
+++ b/drivers/net/nxge/include/nxge_sr_hw.h
@@ -0,0 +1,804 @@
+/*
+ * nxge_sr_hw.h		Neptune Serdes (PCI and Ethernet) HW offsets
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef	_SYS_NXGE_NXGE_SR_HW_H
+#define	_SYS_NXGE_NXGE_SR_HW_H
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#define	ESR_NEPTUNE_DEV_ADDR	0x1E
+#define	ESR_NEPTUNE_BASE	0
+#define	ESR_PORT_ADDR_BASE	0
+#define	PCISR_DEV_ADDR		0x1E
+#define	PCISR_BASE		0
+#define	PCISR_PORT_ADDR_BASE	2
+
+#define	PB	0
+
+#define	SR_RX_TX_COMMON_CONTROL	PB + 0x000
+#define	SR_RX_TX_RESET_CONTROL	PB + 0x004
+#define	SR_RX_POWER_CONTROL	PB + 0x008
+#define	SR_TX_POWER_CONTROL	PB + 0x00C
+#define	SR_MISC_POWER_CONTROL	PB + 0x010
+#define	SR_RX_TX_CONTROL_A	PB + 0x100
+#define	SR_RX_TX_TUNING_A	PB + 0x104
+#define	SR_RX_SYNCCHAR_A	PB + 0x108
+#define	SR_RX_TX_TEST_A		PB + 0x10C
+#define	SR_GLUE_CONTROL0_A	PB + 0x110
+#define	SR_GLUE_CONTROL1_A	PB + 0x114
+#define	SR_RX_TX_CONTROL_B	PB + 0x120
+#define	SR_RX_TX_TUNING_B	PB + 0x124
+#define	SR_RX_SYNCCHAR_B	PB + 0x128
+#define	SR_RX_TX_TEST_B		PB + 0x12C
+#define	SR_GLUE_CONTROL0_B	PB + 0x130
+#define	SR_GLUE_CONTROL1_B	PB + 0x134
+#define	SR_RX_TX_CONTROL_C	PB + 0x140
+#define	SR_RX_TX_TUNING_C	PB + 0x144
+#define	SR_RX_SYNCCHAR_C	PB + 0x148
+#define	SR_RX_TX_TEST_C		PB + 0x14C
+#define	SR_GLUE_CONTROL0_C	PB + 0x150
+#define	SR_GLUE_CONTROL1_C	PB + 0x154
+#define	SR_RX_TX_CONTROL_D	PB + 0x160
+#define	SR_RX_TX_TUNING_D	PB + 0x164
+#define	SR_RX_SYNCCHAR_D	PB + 0x168
+#define	SR_RX_TX_TEST_D		PB + 0x16C
+#define	SR_GLUE_CONTROL0_D	PB + 0x170
+#define	SR_GLUE_CONTROL1_D	PB + 0x174
+#define	SR_RX_TX_TUNING_1_A	PB + 0x184
+#define	SR_RX_TX_TUNING_1_B	PB + 0x1A4
+#define	SR_RX_TX_TUNING_1_C	PB + 0x1C4
+#define	SR_RX_TX_TUNING_1_D	PB + 0x1E4
+#define	SR_RX_TX_TUNING_2_A	PB + 0x204
+#define	SR_RX_TX_TUNING_2_B	PB + 0x224
+#define	SR_RX_TX_TUNING_2_C	PB + 0x244
+#define	SR_RX_TX_TUNING_2_D	PB + 0x264
+#define	SR_RX_TX_TUNING_3_A	PB + 0x284
+#define	SR_RX_TX_TUNING_3_B	PB + 0x2A4
+#define	SR_RX_TX_TUNING_3_C	PB + 0x2C4
+#define	SR_RX_TX_TUNING_3_D	PB + 0x2E4
+
+/*
+ * Shift right by 1 because the PRM requires that all the serdes register
+ * address be divided by 2
+ */
+#define	ESR_NEP_RX_TX_COMMON_CONTROL_L_ADDR()	(ESR_NEPTUNE_BASE +\
+						((SR_RX_TX_COMMON_CONTROL) >> 1))
+#define	ESR_NEP_RX_TX_COMMON_CONTROL_H_ADDR()	(ESR_NEPTUNE_BASE +\
+						((SR_RX_TX_COMMON_CONTROL) >> 1)\
+						+ 1)
+#define	ESR_NEP_RX_TX_RESET_CONTROL_L_ADDR()	(ESR_NEPTUNE_BASE +\
+						((SR_RX_TX_RESET_CONTROL) >> 1))
+#define	ESR_NEP_RX_TX_RESET_CONTROL_H_ADDR()	(ESR_NEPTUNE_BASE +\
+						((SR_RX_TX_RESET_CONTROL) >> 1)\
+						+ 1)
+#define	ESR_NEP_RX_POWER_CONTROL_L_ADDR()	(ESR_NEPTUNE_BASE +\
+						((SR_RX_POWER_CONTROL) >> 1))
+#define	ESR_NEP_RX_POWER_CONTROL_H_ADDR()	(ESR_NEPTUNE_BASE +\
+						((SR_RX_POWER_CONTROL) >> 1) + 1)
+#define	ESR_NEP_TX_POWER_CONTROL_L_ADDR()	(ESR_NEPTUNE_BASE +\
+						((SR_TX_POWER_CONTROL) >> 1))
+#define	ESR_NEP_TX_POWER_CONTROL_H_ADDR()	(ESR_NEPTUNE_BASE +\
+						((SR_TX_POWER_CONTROL) >> 1) + 1)
+#define	ESR_NEP_MISC_POWER_CONTROL_L_ADDR()	(ESR_NEPTUNE_BASE +\
+						((SR_MISC_POWER_CONTROL) >> 1))
+#define	ESR_NEP_MISC_POWER_CONTROL_H_ADDR()	(ESR_NEPTUNE_BASE +\
+						((SR_MISC_POWER_CONTROL) >> 1)\
+						+ 1)
+#define	ESR_NEP_RX_TX_CONTROL_L_ADDR(chan)	((ESR_NEPTUNE_BASE +\
+						SR_RX_TX_CONTROL_A +\
+						(chan * 0x20)) >> 1)
+#define	ESR_NEP_RX_TX_CONTROL_H_ADDR(chan)	((ESR_NEPTUNE_BASE +\
+						SR_RX_TX_CONTROL_A +\
+						(chan * 0x20)) >> 1) + 1
+#define	ESR_NEP_RX_TX_TUNING_L_ADDR(chan)	((ESR_NEPTUNE_BASE +\
+						SR_RX_TX_TUNING_A +\
+						(chan * 0x20)) >> 1)
+#define	ESR_NEP_RX_TX_TUNING_H_ADDR(chan)	((ESR_NEPTUNE_BASE +\
+						SR_RX_TX_TUNING_A +\
+						(chan * 0x20)) >> 1) + 1
+#define	ESR_NEP_RX_TX_SYNCCHAR_L_ADDR(chan)	((ESR_NEPTUNE_BASE +\
+						SR_RX_SYNCCHAR_A +\
+						(chan * 0x20)) >> 1)
+#define	ESR_NEP_RX_TX_SYNCCHAR_H_ADDR(chan)	((ESR_NEPTUNE_BASE +\
+						SR_RX_SYNCCHAR_A +\
+						(chan * 0x20)) >> 1) + 1
+#define	ESR_NEP_RX_TX_TEST_L_ADDR(chan)		((ESR_NEPTUNE_BASE +\
+						SR_RX_TX_TEST_A +\
+						(chan * 0x20)) >> 1)
+#define	ESR_NEP_RX_TX_TEST_H_ADDR(chan)		((ESR_NEPTUNE_BASE +\
+						SR_RX_TX_TEST_A +\
+						(chan * 0x20)) >> 1) + 1
+#define	ESR_NEP_GLUE_CONTROL0_L_ADDR(chan)	((ESR_NEPTUNE_BASE +\
+						SR_GLUE_CONTROL0_A +\
+						(chan * 0x20)) >> 1)
+#define	ESR_NEP_GLUE_CONTROL0_H_ADDR(chan)	((ESR_NEPTUNE_BASE +\
+						SR_GLUE_CONTROL0_A +\
+						(chan * 0x20)) >> 1) + 1
+#define	ESR_NEP_GLUE_CONTROL1_L_ADDR(chan)	((ESR_NEPTUNE_BASE +\
+						SR_GLUE_CONTROL1_A +\
+						(chan * 0x20)) >> 1)
+#define	ESR_NEP_GLUE_CONTROL1_H_ADDR(chan)	((ESR_NEPTUNE_BASE +\
+						SR_GLUE_CONTROL1_A +\
+						(chan * 0x20)) >> 1) + 1
+#define	ESR_NEP_RX_TX_TUNING_1_L_ADDR(chan)	((ESR_NEPTUNE_BASE +\
+						SR_RX_TX_TUNING_1_A +\
+						(chan * 0x20)) >> 1)
+#define	ESR_NEP_RX_TX_TUNING_1_H_ADDR(chan)	((ESR_NEPTUNE_BASE +\
+						SR_RX_TX_TUNING_1_A +\
+						(chan * 0x20)) >> 1) + 1
+#define	ESR_NEP_RX_TX_TUNING_2_L_ADDR(chan)	((ESR_NEPTUNE_BASE +\
+						SR_RX_TX_TUNING_2_A +\
+						(chan * 0x20)) >> 1)
+#define	ESR_NEP_RX_TX_TUNING_2_H_ADDR(chan)	((ESR_NEPTUNE_BASE +\
+						SR_RX_TX_TUNING_2_A +\
+						(chan * 0x20)) >> 1) + 1
+#define	ESR_NEP_RX_TX_TUNING_3_L_ADDR(chan)	((ESR_NEPTUNE_BASE +\
+						SR_RX_TX_TUNING_3_A +\
+						(chan * 0x20)) >> 1)
+#define	ESR_NEP_RX_TX_TUNING_3_H_ADDR(chan)	((ESR_NEPTUNE_BASE +\
+						SR_RX_TX_TUNING_3_A +\
+						(chan * 0x20)) >> 1) + 1
+
+typedef	union _sr_rx_tx_common_ctrl_l {
+	uint16_t value;
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t res3		: 3;
+		uint16_t refclkr_freq	: 5;
+		uint16_t res4		: 8;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint16_t res4		: 8;
+		uint16_t refclkr_freq	: 5;
+		uint16_t res3		: 3;
+#else
+#error one of _BIT_FIELDS_HTOL or _BIT_FIELDS_LTOH must be defined
+#endif
+	} bits;
+} sr_rx_tx_common_ctrl_l;
+
+typedef	union _sr_rx_tx_common_ctrl_h {
+	uint16_t value;
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t res1		: 5;
+		uint16_t tdmaster	: 3;
+		uint16_t tp		: 2;
+		uint16_t tz		: 2;
+		uint16_t res2		: 2;
+		uint16_t revlbrefsel	: 2;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint16_t revlbrefsel	: 2;
+		uint16_t res2		: 2;
+		uint16_t tz		: 2;
+		uint16_t tp		: 2;
+		uint16_t tdmaster	: 3;
+		uint16_t res1		: 5;
+#else
+#error one of _BIT_FIELDS_HTOL or _BIT_FIELDS_LTOH must be defined
+#endif
+	} bits;
+} sr_rx_tx_common_ctrl_h;
+
+
+/* RX TX Common Control Register field values */
+
+#define	TDMASTER_LANE_A		0
+#define	TDMASTER_LANE_B		1
+#define	TDMASTER_LANE_C		2
+#define	TDMASTER_LANE_D		3
+
+#define	REVLBREFSEL_GBT_RBC_A_O		0
+#define	REVLBREFSEL_GBT_RBC_B_O		1
+#define	REVLBREFSEL_GBT_RBC_C_O		2
+#define	REVLBREFSEL_GBT_RBC_D_O		3
+
+#define	REFCLKR_FREQ_SIM		0
+#define	REFCLKR_FREQ_53_125		0x1
+#define	REFCLKR_FREQ_62_5		0x3
+#define	REFCLKR_FREQ_70_83		0x4
+#define	REFCLKR_FREQ_75			0x5
+#define	REFCLKR_FREQ_78_125		0x6
+#define	REFCLKR_FREQ_79_6875		0x7
+#define	REFCLKR_FREQ_83_33		0x8
+#define	REFCLKR_FREQ_85			0x9
+#define	REFCLKR_FREQ_100		0xA
+#define	REFCLKR_FREQ_104_17		0xB
+#define	REFCLKR_FREQ_106_25		0xC
+#define	REFCLKR_FREQ_120		0xF
+#define	REFCLKR_FREQ_125		0x10
+#define	REFCLKR_FREQ_127_5		0x11
+#define	REFCLKR_FREQ_141_67		0x13
+#define	REFCLKR_FREQ_150		0x15
+#define	REFCLKR_FREQ_156_25		0x16
+#define	REFCLKR_FREQ_159_375		0x17
+#define	REFCLKR_FREQ_170		0x19
+#define	REFCLKR_FREQ_212_5		0x1E
+
+typedef	union _sr_rx_tx_reset_ctrl_l {
+	uint16_t value;
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t rxreset_0a	: 1;
+		uint16_t rxreset_0b	: 1;
+		uint16_t rxreset_0c	: 1;
+		uint16_t rxreset_0d	: 1;
+		uint16_t rxreset_1a	: 1;
+		uint16_t rxreset_1b	: 1;
+		uint16_t rxreset_1c	: 1;
+		uint16_t rxreset_1d	: 1;
+		uint16_t rxreset_2a	: 1;
+		uint16_t rxreset_2b	: 1;
+		uint16_t rxreset_2c	: 1;
+		uint16_t rxreset_2d	: 1;
+		uint16_t rxreset_3a	: 1;
+		uint16_t rxreset_3b	: 1;
+		uint16_t rxreset_3c	: 1;
+		uint16_t rxreset_3d	: 1;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint16_t rxreset_3d	: 1;
+		uint16_t rxreset_3c	: 1;
+		uint16_t rxreset_3b	: 1;
+		uint16_t rxreset_3a	: 1;
+		uint16_t rxreset_2d	: 1;
+		uint16_t rxreset_2c	: 1;
+		uint16_t rxreset_2b	: 1;
+		uint16_t rxreset_2a	: 1;
+		uint16_t rxreset_1d	: 1;
+		uint16_t rxreset_1c	: 1;
+		uint16_t rxreset_1b	: 1;
+		uint16_t rxreset_1a	: 1;
+		uint16_t rxreset_0d	: 1;
+		uint16_t rxreset_0c	: 1;
+		uint16_t rxreset_0b	: 1;
+		uint16_t rxreset_0a	: 1;
+#else
+#error one of _BIT_FIELDS_HTOL or _BIT_FIELDS_LTOH must be defined
+#endif
+	} bits;
+} sr_rx_tx_reset_ctrl_l;
+
+
+typedef	union _sr_rx_tx_reset_ctrl_h {
+	uint16_t value;
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t txreset_0a	: 1;
+		uint16_t txreset_0b	: 1;
+		uint16_t txreset_0c	: 1;
+		uint16_t txreset_0d	: 1;
+		uint16_t txreset_1a	: 1;
+		uint16_t txreset_1b	: 1;
+		uint16_t txreset_1c	: 1;
+		uint16_t txreset_1d	: 1;
+		uint16_t txreset_2a	: 1;
+		uint16_t txreset_2b	: 1;
+		uint16_t txreset_2c	: 1;
+		uint16_t txreset_2d	: 1;
+		uint16_t txreset_3a	: 1;
+		uint16_t txreset_3b	: 1;
+		uint16_t txreset_3c	: 1;
+		uint16_t txreset_3d	: 1;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint16_t txreset_3d	: 1;
+		uint16_t txreset_3c	: 1;
+		uint16_t txreset_3b	: 1;
+		uint16_t txreset_3a	: 1;
+		uint16_t txreset_2d	: 1;
+		uint16_t txreset_2c	: 1;
+		uint16_t txreset_2b	: 1;
+		uint16_t txreset_2a	: 1;
+		uint16_t txreset_1d	: 1;
+		uint16_t txreset_1c	: 1;
+		uint16_t txreset_1b	: 1;
+		uint16_t txreset_1a	: 1;
+		uint16_t txreset_0d	: 1;
+		uint16_t txreset_0c	: 1;
+		uint16_t txreset_0b	: 1;
+		uint16_t txreset_0a	: 1;
+#else
+#error one of _BIT_FIELDS_HTOL or _BIT_FIELDS_LTOH must be defined
+#endif
+	} bits;
+} sr_rx_tx_reset_ctrl_h;
+
+typedef	union _sr_rx_power_ctrl_l {
+	uint16_t value;
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t pdrxlos_0a	: 1;
+		uint16_t pdrxlos_0b	: 1;
+		uint16_t pdrxlos_0c	: 1;
+		uint16_t pdrxlos_0d	: 1;
+		uint16_t pdrxlos_1a	: 1;
+		uint16_t pdrxlos_1b	: 1;
+		uint16_t pdrxlos_1c	: 1;
+		uint16_t pdrxlos_1d	: 1;
+		uint16_t pdrxlos_2a	: 1;
+		uint16_t pdrxlos_2b	: 1;
+		uint16_t pdrxlos_2c	: 1;
+		uint16_t pdrxlos_2d	: 1;
+		uint16_t pdrxlos_3a	: 1;
+		uint16_t pdrxlos_3b	: 1;
+		uint16_t pdrxlos_3c	: 1;
+		uint16_t pdrxlos_3d	: 1;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint16_t pdrxlos_3d	: 1;
+		uint16_t pdrxlos_3c	: 1;
+		uint16_t pdrxlos_3b	: 1;
+		uint16_t pdrxlos_3a	: 1;
+		uint16_t pdrxlos_2d	: 1;
+		uint16_t pdrxlos_2c	: 1;
+		uint16_t pdrxlos_2b	: 1;
+		uint16_t pdrxlos_2a	: 1;
+		uint16_t pdrxlos_1d	: 1;
+		uint16_t pdrxlos_1c	: 1;
+		uint16_t pdrxlos_1b	: 1;
+		uint16_t pdrxlos_1a	: 1;
+		uint16_t pdrxlos_0d	: 1;
+		uint16_t pdrxlos_0c	: 1;
+		uint16_t pdrxlos_0b	: 1;
+		uint16_t pdrxlos_0a	: 1;
+#else
+#error one of _BIT_FIELDS_HTOL or _BIT_FIELDS_LTOH must be defined
+#endif
+	} bits;
+} sr_rx_power_ctrl_l_t;
+
+
+typedef	union _sr_rx_power_ctrl_h {
+	uint16_t value;
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t pdownr_0a	: 1;
+		uint16_t pdownr_0b	: 1;
+		uint16_t pdownr_0c	: 1;
+		uint16_t pdownr_0d	: 1;
+		uint16_t pdownr_1a	: 1;
+		uint16_t pdownr_1b	: 1;
+		uint16_t pdownr_1c	: 1;
+		uint16_t pdownr_1d	: 1;
+		uint16_t pdownr_2a	: 1;
+		uint16_t pdownr_2b	: 1;
+		uint16_t pdownr_2c	: 1;
+		uint16_t pdownr_2d	: 1;
+		uint16_t pdownr_3a	: 1;
+		uint16_t pdownr_3b	: 1;
+		uint16_t pdownr_3c	: 1;
+		uint16_t pdownr_3d	: 1;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint16_t pdownr_3d	: 1;
+		uint16_t pdownr_3c	: 1;
+		uint16_t pdownr_3b	: 1;
+		uint16_t pdownr_3a	: 1;
+		uint16_t pdownr_2d	: 1;
+		uint16_t pdownr_2c	: 1;
+		uint16_t pdownr_2b	: 1;
+		uint16_t pdownr_2a	: 1;
+		uint16_t pdownr_1d	: 1;
+		uint16_t pdownr_1c	: 1;
+		uint16_t pdownr_1b	: 1;
+		uint16_t pdownr_1a	: 1;
+		uint16_t pdownr_0d	: 1;
+		uint16_t pdownr_0c	: 1;
+		uint16_t pdownr_0b	: 1;
+		uint16_t pdownr_0a	: 1;
+#else
+#error one of _BIT_FIELDS_HTOL or _BIT_FIELDS_LTOH must be defined
+#endif
+	} bits;
+} sr_rx_power_ctrl_h_t;
+
+typedef	union _sr_tx_power_ctrl_l {
+	uint16_t value;
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t res1		: 8;
+		uint16_t pdownppll0	: 1;
+		uint16_t pdownppll1	: 1;
+		uint16_t pdownppll2	: 1;
+		uint16_t pdownppll3	: 1;
+		uint16_t res2		: 4;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint16_t res2		: 4;
+		uint16_t pdownppll3	: 1;
+		uint16_t pdownppll2	: 1;
+		uint16_t pdownppll1	: 1;
+		uint16_t pdownppll0	: 1;
+		uint16_t res1		: 8;
+#else
+#error one of _BIT_FIELDS_HTOL or _BIT_FIELDS_LTOH must be defined
+#endif
+	} bits;
+} sr_tx_power_ctrl_l_t;
+
+typedef	union _sr_tx_power_ctrl_h {
+	uint16_t value;
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t pdownt_0a	: 1;
+		uint16_t pdownt_0b	: 1;
+		uint16_t pdownt_0c	: 1;
+		uint16_t pdownt_0d	: 1;
+		uint16_t pdownt_1a	: 1;
+		uint16_t pdownt_1b	: 1;
+		uint16_t pdownt_1c	: 1;
+		uint16_t pdownt_1d	: 1;
+		uint16_t pdownt_2a	: 1;
+		uint16_t pdownt_2b	: 1;
+		uint16_t pdownt_2c	: 1;
+		uint16_t pdownt_2d	: 1;
+		uint16_t pdownt_3a	: 1;
+		uint16_t pdownt_3b	: 1;
+		uint16_t pdownt_3c	: 1;
+		uint16_t pdownt_3d	: 1;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint16_t pdownt_3d	: 1;
+		uint16_t pdownt_3c	: 1;
+		uint16_t pdownt_3b	: 1;
+		uint16_t pdownt_3a	: 1;
+		uint16_t pdownt_2d	: 1;
+		uint16_t pdownt_2c	: 1;
+		uint16_t pdownt_2b	: 1;
+		uint16_t pdownt_2a	: 1;
+		uint16_t pdownt_1d	: 1;
+		uint16_t pdownt_1c	: 1;
+		uint16_t pdownt_1b	: 1;
+		uint16_t pdownt_1a	: 1;
+		uint16_t pdownt_0d	: 1;
+		uint16_t pdownt_0c	: 1;
+		uint16_t pdownt_0b	: 1;
+		uint16_t pdownt_0a	: 1;
+#else
+#error one of _BIT_FIELDS_HTOL or _BIT_FIELDS_LTOH must be defined
+#endif
+	} bits;
+} sr_tx_power_ctrl_h_t;
+
+typedef	union _sr_misc_power_ctrl_l {
+	uint16_t value;
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t res1		: 3;
+		uint16_t pdrtrim	: 1;
+		uint16_t pdownpecl0	: 1;
+		uint16_t pdownpecl1	: 1;
+		uint16_t pdownpecl2	: 1;
+		uint16_t pdownpecl3	: 1;
+		uint16_t pdownppll0	: 1;
+		uint16_t pdownppll1	: 1;
+		uint16_t pdownppll2	: 1;
+		uint16_t pdownppll3	: 1;
+		uint16_t res2		: 4;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint16_t res2		: 4;
+		uint16_t pdownppll3	: 1;
+		uint16_t pdownppll2	: 1;
+		uint16_t pdownppll1	: 1;
+		uint16_t pdownppll0	: 1;
+		uint16_t pdownpecl3	: 1;
+		uint16_t pdownpecl2	: 1;
+		uint16_t pdownpecl1	: 1;
+		uint16_t pdownpecl0	: 1;
+		uint16_t pdrtrim	: 1;
+		uint16_t res1		: 3;
+#else
+#error one of _BIT_FIELDS_HTOL or _BIT_FIELDS_LTOH must be defined
+#endif
+	} bits;
+} sr_misc_power_ctrl_l_t;
+
+typedef	union _misc_power_ctrl_h {
+	uint16_t value;
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t pdclkout0	: 1;
+		uint16_t pdclkout1	: 1;
+		uint16_t pdclkout2	: 1;
+		uint16_t pdclkout3	: 1;
+		uint16_t res1		: 12;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint16_t res1		: 12;
+		uint16_t pdclkout3	: 1;
+		uint16_t pdclkout2	: 1;
+		uint16_t pdclkout1	: 1;
+		uint16_t pdclkout0	: 1;
+#else
+#error one of _BIT_FIELDS_HTOL or _BIT_FIELDS_LTOH must be defined
+#endif
+	} bits;
+} misc_power_ctrl_h_t;
+
+typedef	union _sr_rx_tx_ctrl_l {
+	uint16_t value;
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t res1		: 2;
+		uint16_t rxpreswin	: 2;
+		uint16_t res2		: 1;
+		uint16_t risefall	: 3;
+		uint16_t res3		: 7;
+		uint16_t enstretch	: 1;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint16_t enstretch	: 1;
+		uint16_t res3		: 7;
+		uint16_t risefall	: 3;
+		uint16_t res2		: 1;
+		uint16_t rxpreswin	: 2;
+		uint16_t res1		: 2;
+#else
+#error one of _BIT_FIELDS_HTOL or _BIT_FIELDS_LTOH must be defined
+#endif
+	} bits;
+} sr_rx_tx_ctrl_l_t;
+
+typedef	union _sr_rx_tx_ctrl_h {
+	uint16_t value;
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t biascntl	: 1;
+		uint16_t res1		: 5;
+		uint16_t tdenfifo	: 1;
+		uint16_t tdws20		: 1;
+		uint16_t vmuxlo		: 2;
+		uint16_t vpulselo	: 2;
+		uint16_t res2		: 4;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint16_t res2		: 4;
+		uint16_t vpulselo	: 2;
+		uint16_t vmuxlo		: 2;
+		uint16_t tdws20		: 1;
+		uint16_t tdenfifo	: 1;
+		uint16_t res1		: 5;
+		uint16_t biascntl	: 1;
+#else
+#error one of _BIT_FIELDS_HTOL or _BIT_FIELDS_LTOH must be defined
+#endif
+	} bits;
+} sr_rx_tx_ctrl_h_t;
+
+#define	RXPRESWIN_52US_300BITTIMES	0
+#define	RXPRESWIN_53US_300BITTIMES	1
+#define	RXPRESWIN_54US_300BITTIMES	2
+#define	RXPRESWIN_55US_300BITTIMES	3
+
+typedef	union _sr_rx_tx_tuning_l {
+	uint16_t value;
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t rxeq		: 4;
+		uint16_t res1		: 12;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint16_t res1		: 12;
+		uint16_t rxeq		: 4;
+#else
+#error one of _BIT_FIELDS_HTOL or _BIT_FIELDS_LTOH must be defined
+#endif
+	} bits;
+} sr_rx_tx_tuning_l_t;
+
+typedef	union _sr_rx_tx_tuning_h {
+	uint16_t value;
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t res1		: 8;
+		uint16_t rp		: 2;
+		uint16_t rz		: 2;
+		uint16_t vtxlo		: 4;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint16_t vtxlo		: 4;
+		uint16_t rz		: 2;
+		uint16_t rp		: 2;
+		uint16_t res1		: 8;
+#else
+#error one of _BIT_FIELDS_HTOL or _BIT_FIELDS_LTOH must be defined
+#endif
+	} bits;
+} sr_rx_tx_tuning_h_t;
+
+typedef	union _sr_rx_syncchar_l {
+	uint16_t value;
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t syncchar_0_3	: 4;
+		uint16_t res1		: 2;
+		uint16_t syncmask	: 10;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint16_t syncmask	: 10;
+		uint16_t res1		: 2;
+		uint16_t syncchar_0_3	: 4;
+#else
+#error one of _BIT_FIELDS_HTOL or _BIT_FIELDS_LTOH must be defined
+#endif
+	} bits;
+} sr_rx_syncchar_l_t;
+
+typedef	union _sr_rx_syncchar_h {
+	uint16_t value;
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t res1		: 1;
+		uint16_t syncpol	: 1;
+		uint16_t res2		: 8;
+		uint16_t syncchar_4_10	: 6;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint16_t syncchar_4_10	: 6;
+		uint16_t res2		: 8;
+		uint16_t syncpol	: 1;
+		uint16_t res1		: 1;
+#else
+#error one of _BIT_FIELDS_HTOL or _BIT_FIELDS_LTOH must be defined
+#endif
+	} bits;
+} sr_rx_syncchar_h_t;
+
+typedef	union _sr_rx_tx_test_l {
+	uint16_t value;
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t res1		: 15;
+		uint16_t ref50		: 1;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint16_t ref50		: 1;
+		uint16_t res1		: 15;
+#else
+#error one of _BIT_FIELDS_HTOL or _BIT_FIELDS_LTOH must be defined
+#endif
+	} bits;
+} sr_rx_tx_test_l_t;
+
+typedef	union _sr_rx_tx_test_h {
+	uint16_t value;
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t res1		: 5;
+		uint16_t selftest	: 3;
+		uint16_t res2		: 8;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint16_t res2		: 8;
+		uint16_t selftest	: 3;
+		uint16_t res1		: 5;
+#else
+#error one of _BIT_FIELDS_HTOL or _BIT_FIELDS_LTOH must be defined
+#endif
+	} bits;
+} sr_rx_tx_test_h_t;
+
+typedef	union _sr_glue_ctrl0_l {
+	uint16_t value;
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t rxlos_test	: 1;
+		uint16_t res1		: 1;
+		uint16_t rxlosenable	: 1;
+		uint16_t fastresync	: 1;
+		uint16_t samplerate	: 4;
+		uint16_t thresholdcount	: 8;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint16_t thresholdcount	: 8;
+		uint16_t samplerate	: 4;
+		uint16_t fastresync	: 1;
+		uint16_t rxlosenable	: 1;
+		uint16_t res1		: 1;
+		uint16_t rxlos_test	: 1;
+#else
+#error one of _BIT_FIELDS_HTOL or _BIT_FIELDS_LTOH must be defined
+#endif
+	} bits;
+} sr_glue_ctrl0_l_t;
+
+typedef	union _sr_glue_ctrl0_h {
+	uint16_t value;
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t res1		: 5;
+		uint16_t bitlocktime	: 3;
+		uint16_t res2		: 8;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint16_t res2		: 8;
+		uint16_t bitlocktime	: 3;
+		uint16_t res1		: 5;
+#else
+#error one of _BIT_FIELDS_HTOL or _BIT_FIELDS_LTOH must be defined
+#endif
+	} bits;
+} sr_glue_ctrl0_h_t;
+
+#define	BITLOCKTIME_64_CYCLES		0
+#define	BITLOCKTIME_128_CYCLES		1
+#define	BITLOCKTIME_256_CYCLES		2
+#define	BITLOCKTIME_300_CYCLES		3
+#define	BITLOCKTIME_384_CYCLES		4
+#define	BITLOCKTIME_512_CYCLES		5
+#define	BITLOCKTIME_1024_CYCLES		6
+#define	BITLOCKTIME_2048_CYCLES		7
+
+typedef	union _sr_glue_ctrl1_l {
+	uint16_t value;
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t res1		: 14;
+		uint16_t inittime	: 2;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint16_t inittime	: 2;
+		uint16_t res1		: 14;
+#else
+#error one of _BIT_FIELDS_HTOL or _BIT_FIELDS_LTOH must be defined
+#endif
+	} bits;
+} sr_glue_ctrl1_l_t;
+
+typedef	union glue_ctrl1_h {
+	uint16_t value;
+	struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint16_t termr_cfg	: 2;
+		uint16_t termt_cfg	: 2;
+		uint16_t rtrimen	: 2;
+		uint16_t res1		: 10;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint16_t res1		: 10;
+		uint16_t rtrimen	: 2;
+		uint16_t termt_cfg	: 2;
+		uint16_t termr_cfg	: 2;
+#else
+#error one of _BIT_FIELDS_HTOL or _BIT_FIELDS_LTOH must be defined
+#endif
+	} bits;
+} glue_ctrl1_h_t;
+
+#define	TERM_CFG_67OHM		0
+#define	TERM_CFG_72OHM		1
+#define	TERM_CFG_80OHM		2
+#define	TERM_CFG_87OHM		3
+#define	TERM_CFG_46OHM		4
+#define	TERM_CFG_48OHM		5
+#define	TERM_CFG_52OHM		6
+#define	TERM_CFG_55OHM		7
+
+#define	INITTIME_60US		0
+#define	INITTIME_120US		1
+#define	INITTIME_240US		2
+#define	INITTIME_480US		3
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif	/* _SYS_NXGE_NXGE_SR_HW_H */
diff --git a/drivers/net/nxge/include/nxge_txc.h b/drivers/net/nxge/include/nxge_txc.h
new file mode 100644
index 0000000..330c2a6
--- /dev/null
+++ b/drivers/net/nxge/include/nxge_txc.h
@@ -0,0 +1,94 @@
+/*
+ * nxge_txc.h	Neptune TX Controller interface
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef	_SYS_NXGE_NXGE_TXC_H
+#define	_SYS_NXGE_NXGE_TXC_H
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#include <nxge_txc_hw.h>
+#include <npi_txc.h>
+
+/* Suggested by hardware team 7/19/2006 */
+#define	TXC_DMA_MAX_BURST_DEFAULT	1000	/* Max burst used by DRR */
+
+typedef	struct _txc_errlog {
+	txc_ro_states_t		ro_st;
+	txc_sf_states_t		sf_st;
+} txc_errlog_t;
+
+typedef struct _nxge_txc_stats {
+	uint32_t		pkt_stuffed;
+	uint32_t		pkt_xmit;
+	uint32_t		ro_correct_err;
+	uint32_t		ro_uncorrect_err;
+	uint32_t		sf_correct_err;
+	uint32_t		sf_uncorrect_err;
+	uint32_t		address_failed;
+	uint32_t		dma_failed;
+	uint32_t		length_failed;
+	uint32_t		pkt_assy_dead;
+	uint32_t		reorder_err;
+	txc_errlog_t		errlog;
+} nxge_txc_stats_t, *p_nxge_txc_stats_t;
+
+typedef struct _nxge_txc {
+	uint32_t		dma_max_burst;
+	uint32_t		dma_length;
+	uint32_t		training;
+	uint8_t			debug_select;
+	uint64_t		control_status;
+	uint64_t		port_dma_list;
+	nxge_txc_stats_t	*txc_stats;
+} nxge_txc_t, *p_nxge_txc_t;
+
+/*
+ * Transmit Controller (TXC) prototypes.
+ */
+nxge_status_t nxge_txc_init(p_nxge_t nxgep);
+nxge_status_t nxge_txc_uninit(p_nxge_t nxgep);
+nxge_status_t nxge_txc_handle_sys_errors(p_nxge_t nxgep);
+void nxge_txc_inject_err(p_nxge_t nxgep, uint32_t err_id);
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif	/* _SYS_NXGE_NXGE_TXC_H */
diff --git a/drivers/net/nxge/include/nxge_txc_hw.h b/drivers/net/nxge/include/nxge_txc_hw.h
new file mode 100644
index 0000000..422c3f1
--- /dev/null
+++ b/drivers/net/nxge/include/nxge_txc_hw.h
@@ -0,0 +1,1281 @@
+/*
+ * nxge_txc_hw.h	Neptune TX Controller HW register offsets
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef	_SYS_NXGE_NXGE_TXC_HW_H
+#define	_SYS_NXGE_NXGE_TXC_HW_H
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#include <nxge_defs.h>
+
+/* Transmit Ring Scheduler Registers */
+#define	TXC_PORT_DMA_ENABLE_REG		(FZC_TXC + 0x20028)
+#define	TXC_PORT_DMA_LIST		0	/* RW bit 23:0 */
+#define	TXC_DMA_DMA_LIST_MASK		0x0000000000FFFFFFULL
+#define	TXC_DMA_DMA_LIST_MASK_N2	0x000000000000FFFFULL
+
+typedef union _txc_port_enable_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res:8;
+			uint32_t port_dma_list:24;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t port_dma_list:24;
+			uint32_t res:8;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_port_enable_t, *p_txc_port_enable_t;
+
+typedef union _txc_port_enable_n2_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res:16;
+			uint32_t port_dma_list:16;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t port_dma_list:16;
+			uint32_t res:16;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_port_enable_n2_t, *p_txc_port_enable_n2_t;
+
+/* Transmit Controller - Registers */
+#define	TXC_FZC_OFFSET			0x1000
+#define	TXC_FZC_PORT_OFFSET(port)	(port * TXC_FZC_OFFSET)
+#define	TXC_FZC_CHANNEL_OFFSET(channel)	(channel * TXC_FZC_OFFSET)
+#define	TXC_FZC_REG_CN_OFFSET(x, cn)	(x + TXC_FZC_CHANNEL_OFFSET(cn))
+
+#define	TXC_FZC_CONTROL_OFFSET		0x100
+#define	TXC_FZC_CNTL_PORT_OFFSET(port)	(port * TXC_FZC_CONTROL_OFFSET)
+#define	TXC_FZC_REG_PT_OFFSET(x, pt)	(x + TXC_FZC_CNTL_PORT_OFFSET(pt))
+
+#define	TXC_DMA_MAX_BURST_REG		(FZC_TXC + 0x00000)
+#define	TXC_DMA_MAX_BURST_SHIFT		0	/* RW bit 19:0 */
+#define	TXC_DMA_MAX_BURST_MASK		0x00000000000FFFFFULL
+
+#define	TXC_MAX_BURST_OFFSET(channel)	(TXC_DMA_MAX_BURST_REG + \
+					(channel * TXC_FZC_OFFSET))
+
+typedef union _txc_dma_max_burst_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res:12;
+			uint32_t dma_max_burst:20;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t dma_max_burst:20;
+			uint32_t res:12;
+
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_dma_max_burst_t, *p_txc_dma_max_burst_t;
+
+/* DRR Performance Monitoring Register */
+#define	TXC_DMA_MAX_LENGTH_REG		(FZC_TXC + 0x00008)
+#define	TXC_DMA_MAX_LENGTH_SHIFT	/* RW bit 27:0 */
+#define	TXC_DMA_MAX_LENGTH_MASK		0x000000000FFFFFFFULL
+
+#define	TXC_DMA_MAX_LEN_OFFSET(channel)	(TXC_DMA_MAX_LENGTH_REG + \
+					(channel * TXC_FZC_OFFSET))
+
+typedef union _txc_dma_max_length_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res:4;
+			uint32_t dma_length:28;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t dma_length:28;
+			uint32_t res:4;
+
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_dma_max_length_t, *p_txc_dma_max_length_t;
+
+
+#define	TXC_CONTROL_REG			(FZC_TXC + 0x20000)
+#define	TXC_DMA_LENGTH_SHIFT		0	/* RW bit 27:0 */
+#define	TXC_DMA_LENGTH_MASK		0x000000000FFFFFFFULL
+
+typedef union _txc_control_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res:27;
+			uint32_t txc_enabled:1;
+			uint32_t port3_enabled:1;
+			uint32_t port2_enabled:1;
+			uint32_t port1_enabled:1;
+			uint32_t port0_enabled:1;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t port0_enabled:1;
+			uint32_t port1_enabled:1;
+			uint32_t port2_enabled:1;
+			uint32_t port3_enabled:1;
+			uint32_t txc_enabled:1;
+			uint32_t res:27;
+
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_control_t, *p_txc_control_t;
+
+typedef union _txc_control_n2_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res:27;
+			uint32_t txc_enabled:1;
+			uint32_t res1:2;
+			uint32_t port1_enabled:1;
+			uint32_t port0_enabled:1;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t port0_enabled:1;
+			uint32_t port1_enabled:1;
+			uint32_t res1:2;
+			uint32_t txc_enabled:1;
+			uint32_t res:27;
+
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_control_n2_t, *p_txc_control_n2_t;
+
+
+#define	TXC_TRAINING_REG		(FZC_TXC + 0x20008)
+#define	TXC_TRAINING_VECTOR		0	/* RW bit 32:0 */
+#define	TXC_TRAINING_VECTOR_MASK	0x00000000FFFFFFFFULL
+
+typedef union _txc_training_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t txc_training_vector:32;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t txc_training_vector:32;
+
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_training_t, *p_txc_training_t;
+
+
+#define	TXC_DEBUG_SELECT_REG		(FZC_TXC + 0x20010)
+#define	TXC_DEBUG_SELECT_SHIFT		0	/* WO bit 5:0 */
+#define	TXC_DEBUG_SELECT_MASK		0x000000000000003FULL
+
+typedef union _txc_debug_select_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res:26;
+			uint32_t debug_select:6;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t debug_select:6;
+			uint32_t res:26;
+
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_debug_select_t, *p_txc_debug_select_t;
+
+
+#define	TXC_MAX_REORDER_REG		(FZC_TXC + 0x20018)
+#define	TXC_MAX_REORDER_MASK_2		(0xf)
+#define	TXC_MAX_REORDER_MASK_4		(0x7)
+#define	TXC_MAX_REORDER_SHIFT_BITS	8
+#define	TXC_MAX_REORDER_SHIFT(port)	(port * (TXC_MAX_REORDER_SHIFT_BITS))
+
+typedef union _txc_max_reorder_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t resv3:4;
+			uint32_t port3:4;
+			uint32_t resv2:4;
+			uint32_t port2:4;
+			uint32_t resv1:4;
+			uint32_t port1:4;
+			uint32_t resv0:4;
+			uint32_t port0:4;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t port0:4;
+			uint32_t resv0:4;
+			uint32_t port1:4;
+			uint32_t resv1:4;
+			uint32_t port2:4;
+			uint32_t resv2:4;
+			uint32_t port3:4;
+			uint32_t resv3:4;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_max_reorder_t, *p_txc_max_reorder_t;
+
+
+#define	TXC_PORT_CTL_REG		(FZC_TXC + 0x20020)	/* RO */
+#define	TXC_PORT_CTL_OFFSET(port)	(TXC_PORT_CTL_REG + \
+					(port * TXC_FZC_CONTROL_OFFSET))
+#define	TXC_PORT_CNTL_CLEAR		0x1
+
+typedef union _txc_port_ctl_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t rsvd:31;
+			uint32_t clr_all_stat:1;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t clr_all_stat:1;
+			uint32_t rsvd:31;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_port_ctl_t, *p_txc_port_ctl_t;
+
+#define	TXC_PKT_STUFFED_REG		(FZC_TXC + 0x20030)
+#define	TXC_PKT_STUFF_PKTASY_SHIFT	16	/* RW bit 16:0 */
+#define	TXC_PKT_STUFF_PKTASY_MASK	0x000000000000FFFFULL
+#define	TXC_PKT_STUFF_REORDER_SHIFT	0	/* RW bit 31:16 */
+#define	TXC_PKT_STUFF_REORDER_MASK	0x00000000FFFF0000ULL
+
+typedef union _txc_pkt_stuffed_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t pkt_pro_reorder:16;
+			uint32_t pkt_proc_pktasy:16;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t pkt_proc_pktasy:16;
+			uint32_t pkt_pro_reorder:16;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_pkt_stuffed_t, *p_txc_pkt_stuffed_t;
+
+
+#define	TXC_PKT_XMIT_REG		(FZC_TXC + 0x20038)
+#define	TXC_PKTS_XMIT_SHIFT		0	/* RW bit 15:0 */
+#define	TXC_PKTS_XMIT_MASK		0x000000000000FFFFULL
+#define	TXC_BYTES_XMIT_SHIFT		16	/* RW bit 31:16 */
+#define	TXC_BYTES_XMIT_MASK		0x00000000FFFF0000ULL
+
+typedef union _txc_pkt_xmit_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t bytes_transmitted:16;
+			uint32_t pkts_transmitted:16;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t pkts_transmitted:16;
+			uint32_t bytes_transmitted:16;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_pkt_xmit, *p_txc_pkt_xmit;
+
+
+/* count 4 step 0x00100 */
+#define	TXC_ROECC_CTL_REG		(FZC_TXC + 0x20040)
+#define	TXC_ROECC_CTL_OFFSET(port)	(TXC_ROECC_CTL_REG + \
+					(TXC_FZC_CNTL_PORT_OFFSET(port)))
+
+typedef union _txc_roecc_ctl_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t disable_ue_error:1;
+			uint32_t rsvd:13;
+			uint32_t double_bit_err:1;
+			uint32_t single_bit_err:1;
+			uint32_t rsvd_2:5;
+			uint32_t all_pkts:1;
+			uint32_t alternate_pkts:1;
+			uint32_t one_pkt:1;
+			uint32_t rsvd_3:5;
+			uint32_t last_line_pkt:1;
+			uint32_t second_line_pkt:1;
+			uint32_t firstd_line_pkt:1;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t firstd_line_pkt:1;
+			uint32_t second_line_pkt:1;
+			uint32_t last_line_pkt:1;
+			uint32_t rsvd_3:5;
+			uint32_t one_pkt:1;
+			uint32_t alternate_pkts:1;
+			uint32_t all_pkts:1;
+			uint32_t rsvd_2:5;
+			uint32_t single_bit_err:1;
+			uint32_t double_bit_err:1;
+			uint32_t rsvd:13;
+			uint32_t disable_ue_error:1;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_roecc_ctl_t, *p_txc_roecc_ctl_t;
+
+
+#define	TXC_ROECC_ST_REG		(FZC_TXC + 0x20048)
+
+#define	TXC_ROECC_ST_OFFSET(port)	(TXC_ROECC_ST_REG + \
+					(TXC_FZC_CNTL_PORT_OFFSET(port)))
+
+typedef union _txc_roecc_st_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t clr_st:1;
+			uint32_t res:13;
+			uint32_t correct_error:1;
+			uint32_t uncorrect_error:1;
+			uint32_t rsvd:6;
+			uint32_t ecc_address:10;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t ecc_address:10;
+			uint32_t rsvd:6;
+			uint32_t uncorrect_error:1;
+			uint32_t correct_error:1;
+			uint32_t res:13;
+			uint32_t clr_st:1;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_roecc_st_t, *p_txc_roecc_st_t;
+
+
+#define	TXC_RO_DATA0_REG		(FZC_TXC + 0x20050)
+#define	TXC_RO_DATA0_OFFSET(port)	(TXC_RO_DATA0_REG + \
+					(TXC_FZC_CNTL_PORT_OFFSET(port)))
+
+typedef union _txc_ro_data0_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t ro_ecc_data0:32;	/* ro_ecc_data[31:0] */
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t ro_ecc_data0:32;	/* ro_ecc_data[31:0] */
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_ro_data0_t, *p_txc_ro_data0_t;
+
+#define	TXC_RO_DATA1_REG		(FZC_TXC + 0x20058)
+#define	TXC_RO_DATA1_OFFSET(port)	(TXC_RO_DATA1_REG + \
+					(TXC_FZC_CNTL_PORT_OFFSET(port)))
+
+typedef union _txc_ro_data1_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t ro_ecc_data1:32;	/* ro_ecc_data[63:32] */
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t ro_ecc_data1:32;	/* ro_ecc_data[31:32] */
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_ro_data1_t, *p_txc_ro_data1_t;
+
+
+#define	TXC_RO_DATA2_REG		(FZC_TXC + 0x20060)
+
+#define	TXC_RO_DATA2_OFFSET(port)	(TXC_RO_DATA2_REG + \
+					(TXC_FZC_CNTL_PORT_OFFSET(port)))
+
+typedef union _txc_ro_data2_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t ro_ecc_data2:32;	/* ro_ecc_data[95:64] */
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t ro_ecc_data2:32;	/* ro_ecc_data[95:64] */
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_ro_data2_t, *p_txc_ro_data2_t;
+
+#define	TXC_RO_DATA3_REG		(FZC_TXC + 0x20068)
+#define	TXC_RO_DATA3_OFFSET(port)	(TXC_RO_DATA3_REG + \
+					(TXC_FZC_CNTL_PORT_OFFSET(port)))
+
+typedef union _txc_ro_data3_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t ro_ecc_data3:32; /* ro_ecc_data[127:96] */
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t ro_ecc_data3:32; /* ro_ecc_data[127:96] */
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_ro_data3_t, *p_txc_ro_data3_t;
+
+#define	TXC_RO_DATA4_REG		(FZC_TXC + 0x20070)
+#define	TXC_RO_DATA4_OFFSET(port)	(TXC_RO_DATA4_REG + \
+					(TXC_FZC_CNTL_PORT_OFFSET(port)))
+
+typedef union _txc_ro_data4_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t ro_ecc_data4:32; /* ro_ecc_data[151:128] */
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t ro_ecc_data4:32; /* ro_ecc_data[151:128] */
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_ro_data4_t, *p_txc_ro_data4_t;
+
+/* count 4 step 0x00100 */
+#define	TXC_SFECC_CTL_REG		(FZC_TXC + 0x20078)
+#define	TXC_SFECC_CTL_OFFSET(port)	(TXC_SFECC_CTL_REG + \
+					(TXC_FZC_CNTL_PORT_OFFSET(port)))
+
+typedef union _txc_sfecc_ctl_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t disable_ue_error:1;
+			uint32_t rsvd:13;
+			uint32_t double_bit_err:1;
+			uint32_t single_bit_err:1;
+			uint32_t rsvd_2:5;
+			uint32_t all_pkts:1;
+			uint32_t alternate_pkts:1;
+			uint32_t one_pkt:1;
+			uint32_t rsvd_3:5;
+			uint32_t last_line_pkt:1;
+			uint32_t second_line_pkt:1;
+			uint32_t firstd_line_pkt:1;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t firstd_line_pkt:1;
+			uint32_t second_line_pkt:1;
+			uint32_t last_line_pkt:1;
+			uint32_t rsvd_3:5;
+			uint32_t one_pkt:1;
+			uint32_t alternate_pkts:1;
+			uint32_t all_pkts:1;
+			uint32_t rsvd_2:5;
+			uint32_t single_bit_err:1;
+			uint32_t double_bit_err:1;
+			uint32_t rsvd:13;
+			uint32_t disable_ue_error:1;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_sfecc_ctl_t, *p_txc_sfecc_ctl_t;
+
+#define	TXC_SFECC_ST_REG		(FZC_TXC + 0x20080)
+#define	TXC_SFECC_ST_OFFSET(port)	(TXC_SFECC_ST_REG + \
+					(TXC_FZC_CNTL_PORT_OFFSET(port)))
+
+typedef union _txc_sfecc_st_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t clr_st:1;
+			uint32_t res:13;
+			uint32_t correct_error:1;
+			uint32_t uncorrect_error:1;
+			uint32_t rsvd:6;
+			uint32_t ecc_address:10;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t ecc_address:10;
+			uint32_t rsvd:6;
+			uint32_t uncorrect_error:1;
+			uint32_t correct_error:1;
+			uint32_t res:13;
+			uint32_t clr_st:1;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_sfecc_st_t, *p_txc_sfecc_st_t;
+
+#define	TXC_SF_DATA0_REG		(FZC_TXC + 0x20088)
+#define	TXC_SF_DATA0_OFFSET(port)	(TXC_SF_DATA0_REG + \
+					(TXC_FZC_CNTL_PORT_OFFSET(port)))
+
+typedef union _txc_sf_data0_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t sf_ecc_data0:32;	/* sf_ecc_data[31:0] */
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t sf_ecc_data0:32;	/* sf_ecc_data[31:0] */
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_sf_data0_t, *p_txc_sf_data0_t;
+
+#define	TXC_SF_DATA1_REG		(FZC_TXC + 0x20090)
+#define	TXC_SF_DATA1_OFFSET(port)	(TXC_SF_DATA1_REG + \
+					(TXC_FZC_CNTL_PORT_OFFSET(port)))
+
+typedef union _txc_sf_data1_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t sf_ecc_data1:32;	/* sf_ecc_data[63:32] */
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t sf_ecc_data1:32;	/* sf_ecc_data[31:32] */
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_sf_data1_t, *p_txc_sf_data1_t;
+
+
+#define	TXC_SF_DATA2_REG		(FZC_TXC + 0x20098)
+#define	TXC_SF_DATA2_OFFSET(port)	(TXC_SF_DATA2_REG + \
+					(TXC_FZC_CNTL_PORT_OFFSET(port)))
+
+typedef union _txc_sf_data2_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t sf_ecc_data2:32;	/* sf_ecc_data[95:64] */
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t sf_ecc_data2:32;	/* sf_ecc_data[95:64] */
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_sf_data2_t, *p_txc_sf_data2_t;
+
+#define	TXC_SF_DATA3_REG		(FZC_TXC + 0x200A0)
+#define	TXC_SF_DATA3_OFFSET(port)	(TXC_SF_DATA3_REG + \
+					(TXC_FZC_CNTL_PORT_OFFSET(port)))
+
+typedef union _txc_sf_data3_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t sf_ecc_data3:32; /* sf_ecc_data[127:96] */
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t sf_ecc_data3:32; /* sf_ecc_data[127:96] */
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_sf_data3_t, *p_txc_sf_data3_t;
+
+#define	TXC_SF_DATA4_REG		(FZC_TXC + 0x200A8)
+#define	TXC_SF_DATA4_OFFSET(port)	(TXC_SF_DATA4_REG + \
+					(TXC_FZC_CNTL_PORT_OFFSET(port)))
+
+typedef union _txc_sf_data4_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t sf_ecc_data4:32; /* sf_ecc_data[151:128] */
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t sf_ecc_data4:32; /* sf_ecc_data[151:128] */
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_sf_data4_t, *p_txc_sf_data4_t;
+
+#define	TXC_RO_TIDS_REG			(FZC_TXC + 0x200B0)
+#define	TXC_RO_TIDS_OFFSET(port)	(TXC_RO_TIDS_REG + \
+					(TXC_FZC_CNTL_PORT_OFFSET(port)))
+#define	TXC_RO_TIDS_MASK		0x00000000FFFFFFFFULL
+
+typedef union _txc_ro_tids_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t tids_in_use:32;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t tids_in_use:32;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_ro_tids_t, *p_txc_ro_tids_t;
+
+#define	TXC_RO_STATE0_REG		(FZC_TXC + 0x200B8)
+#define	TXC_RO_STATE0_OFFSET(port)	(TXC_STATE0_REG + \
+					(TXC_FZC_CNTL_PORT_OFFSET(port)))
+#define	TXC_RO_STATE0_MASK		0x00000000FFFFFFFFULL
+
+typedef union _txc_ro_state0_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t duplicate_tid:32;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t duplicate_tid:32;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_ro_state0_t, *p_txc_ro_state0_t;
+
+#define	TXC_RO_STATE1_REG		(FZC_TXC + 0x200C0)
+#define	TXC_RO_STATE1_OFFSET(port)	(TXC_STATE1_REG + \
+					(TXC_FZC_CNTL_PORT_OFFSET(port)))
+#define	TXC_RO_STATE1_MASK		0x00000000FFFFFFFFULL
+
+typedef union _txc_ro_state1_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t unused_tid:32;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t unused_tid:32;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_ro_state1_t, *p_txc_ro_state1_t;
+
+#define	TXC_RO_STATE2_REG		(FZC_TXC + 0x200C8)
+#define	TXC_RO_STATE2_OFFSET(port)	(TXC_STATE2_REG + \
+					(TXC_FZC_CNTL_PORT_OFFSET(port)))
+#define	TXC_RO_STATE2_MASK		0x00000000FFFFFFFFULL
+
+typedef union _txc_ro_state2_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t transaction_timeout:32;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t transaction_timeout:32;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_ro_state2_t, *p_txc_ro_state2_t;
+
+#define	TXC_RO_STATE3_REG		(FZC_TXC + 0x200D0)
+#define	TXC_RO_STATE3_OFFSET(port)	(TXC_RO_STATE3_REG + \
+					(TXC_FZC_CNTL_PORT_OFFSET(port)))
+
+typedef union _txc_ro_state3_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t enable_spacefilled_watermark:1;
+			uint32_t ro_spacefilled_watermask:10;
+			uint32_t ro_fifo_spaceavailable:10;
+			uint32_t rsv:2;
+			uint32_t enable_ro_watermark:1;
+			uint32_t highest_reorder_used:4;
+			uint32_t num_reorder_used:4;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t num_reorder_used:4;
+			uint32_t highest_reorder_used:4;
+			uint32_t enable_ro_watermark:1;
+			uint32_t rsv:2;
+			uint32_t ro_fifo_spaceavailable:10;
+			uint32_t ro_spacefilled_watermask:10;
+			uint32_t enable_spacefilled_watermark:1;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_ro_state3_t, *p_txc_ro_state3_t;
+
+#define	TXC_RO_CTL_REG			(FZC_TXC + 0x200D8)
+#define	TXC_RO_CTL_OFFSET(port)		(TXC_RO_CTL_REG + \
+					(TXC_FZC_CNTL_PORT_OFFSET(port)))
+
+typedef union _txc_ro_ctl_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t clr_fail_state:1;
+			uint32_t rsvd3:3;
+			uint32_t ro_addr1:4;
+			uint32_t rsvd2:1;
+			uint32_t address_failed:1;
+			uint32_t dma_failed:1;
+			uint32_t length_failed:1;
+			uint32_t rsv:1;
+			uint32_t capture_address_fail:1;
+			uint32_t capture_dma_fail:1;
+			uint32_t capture_length_fail:1;
+			uint32_t rsvd:8;
+			uint32_t ro_state_rd_done:1;
+			uint32_t ro_state_wr_done:1;
+			uint32_t ro_state_rd:1;
+			uint32_t ro_state_wr:1;
+			uint32_t ro_state_addr:4;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t ro_state_addr:4;
+			uint32_t ro_state_wr:1;
+			uint32_t ro_state_rd:1;
+			uint32_t ro_state_wr_done:1;
+			uint32_t ro_state_rd_done:1;
+			uint32_t rsvd:8;
+			uint32_t capture_length_fail:1;
+			uint32_t capture_dma_fail:1;
+			uint32_t capture_address_fail:1;
+			uint32_t rsv:1;
+			uint32_t length_failed:1;
+			uint32_t dma_failed:1;
+			uint32_t address_failed:1;
+			uint32_t rsvd2:1;
+			uint32_t ro_addr1:4;
+			uint32_t rsvd3:3;
+			uint32_t clr_fail_state:1;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_ro_ctl_t, *p_txc_ro_ctl_t;
+
+
+#define	TXC_RO_ST_DATA0_REG		(FZC_TXC + 0x200E0)
+#define	TXC_RO_ST_DATA0_OFFSET(port)	(TXC_RO_ST_DATA0_REG + \
+					(TXC_FZC_CNTL_PORT_OFFSET(port)))
+#define	TXC_RO_ST_DATA0_MASK		0x00000000FFFFFFFFULL
+
+typedef union _txc_ro_st_data0_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t ro_st_dat0:32;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t ro_st_dat0:32;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_ro_st_data0_t, *p_txc_ro_st_data0_t;
+
+
+#define	TXC_RO_ST_DATA1_REG		(FZC_TXC + 0x200E8)
+#define	TXC_RO_ST_DATA1_OFFSET(port)	(TXC_RO_ST_DATA1_REG + \
+					(TXC_FZC_CNTL_PORT_OFFSET(port)))
+#define	TXC_RO_ST_DATA1_MASK		0x00000000FFFFFFFFULL
+
+typedef union _txc_ro_st_data1_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t ro_st_dat1:32;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t ro_st_dat1:32;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_ro_st_data1_t, *p_txc_ro_st_data1_t;
+
+
+#define	TXC_RO_ST_DATA2_REG		(FZC_TXC + 0x200F0)
+#define	TXC_RO_ST_DATA2_OFFSET(port)	(TXC_RO_ST_DATA2_REG + \
+					(TXC_FZC_CNTL_PORT_OFFSET(port)))
+#define	TXC_RO_ST_DATA2_MASK		0x00000000FFFFFFFFULL
+
+typedef union _txc_ro_st_data2_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t ro_st_dat2:32;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t ro_st_dat2:32;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_ro_st_data2_t, *p_txc_ro_st_data2_t;
+
+#define	TXC_RO_ST_DATA3_REG		(FZC_TXC + 0x200F8)
+#define	TXC_RO_ST_DATA3_OFFSET(port)	(TXC_RO_ST_DATA3_REG + \
+					(TXC_FZC_CNTL_PORT_OFFSET(port)))
+#define	TXC_RO_ST_DATA3_MASK		0x00000000FFFFFFFFULL
+
+typedef union _txc_ro_st_data3_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t ro_st_dat3:32;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t ro_st_dat3:32;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_ro_st_data3_t, *p_txc_ro_st_data3_t;
+
+#define	TXC_PORT_PACKET_REQ_REG		(FZC_TXC + 0x20100)
+#define	TXC_PORT_PACKET_REQ_OFFSET(port) (TXC_PORT_PACKET_REQ_REG + \
+					(TXC_FZC_CNTL_PORT_OFFSET(port)))
+#define	TXC_PORT_PACKET_REQ_MASK	0x00000000FFFFFFFFULL
+
+typedef union _txc_port_packet_req_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t gather_req:4;
+			uint32_t packet_eq:12;
+			uint32_t pkterr_abort:16;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t pkterr_abort:16;
+			uint32_t packet_eq:12;
+			uint32_t gather_req:4;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_port_packet_req_t, *p_txc_port_packet_req_t;
+
+/* Reorder error bits in interrupt registers  */
+#define	TXC_INT_STAT_SF_CORR_ERR	0x01
+#define	TXC_INT_STAT_SF_UNCORR_ERR	0x02
+#define	TXC_INT_STAT_RO_CORR_ERR	0x04
+#define	TXC_INT_STAT_RO_UNCORR_ERR	0x08
+#define	TXC_INT_STAT_REORDER_ERR	0x10
+#define	TXC_INT_STAT_PKTASSYDEAD	0x20
+
+#define	TXC_INT_STAT_DBG_REG		(FZC_TXC + 0x20420)
+#define	TXC_INT_STAT_DBG_MASK		0x00000000FFFFFFFFULL
+
+typedef union _txc_int_stat_dbg_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t rsvd3:2;
+			uint32_t port3_int_status:6;
+			uint32_t rsvd2:2;
+			uint32_t port2_int_status:6;
+			uint32_t rsvd1:2;
+			uint32_t port1_int_status:6;
+			uint32_t rsvd:2;
+			uint32_t port0_int_status:6;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t port0_int_status:6;
+			uint32_t rsvd:2;
+			uint32_t port1_int_status:6;
+			uint32_t rsvd1:2;
+			uint32_t port2_int_status:6;
+			uint32_t rsvd2:2;
+			uint32_t port3_int_status:6;
+			uint32_t rsvd3:2;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_int_stat_dbg_t, *p_txc_int_stat_dbg_t;
+
+
+#define	TXC_INT_STAT_REG		(FZC_TXC + 0x20428)
+#define	TXC_INT_STAT_MASK		0x00000000FFFFFFFFULL
+
+typedef union _txc_int_stat_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t rsvd3:2;
+			uint32_t port3_int_status:6;
+			uint32_t rsvd2:2;
+			uint32_t port2_int_status:6;
+			uint32_t rsvd1:2;
+			uint32_t port1_int_status:6;
+			uint32_t rsvd:2;
+			uint32_t port0_int_status:6;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t port0_int_status:6;
+			uint32_t rsvd:2;
+			uint32_t port1_int_status:6;
+			uint32_t rsvd1:2;
+			uint32_t port2_int_status:6;
+			uint32_t rsvd2:2;
+			uint32_t port3_int_status:6;
+			uint32_t rsvd3:2;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_int_stat_t, *p_txc_int_stat_t;
+
+#define	TXC_INT_MASK_REG		(FZC_TXC + 0x20430)
+#define	TXC_INT_MASK_MASK		0x00000000FFFFFFFFULL
+
+typedef union _txc_int_mask_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t rsvd3:2;
+			uint32_t port3_int_mask:6;
+			uint32_t rsvd2:2;
+			uint32_t port2_int_mask:6;
+			uint32_t rsvd1:2;
+			uint32_t port1_int_mask:6;
+			uint32_t rsvd:2;
+			uint32_t port0_int_mask:6;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t port0_int_mask:6;
+			uint32_t rsvd:2;
+			uint32_t port1_int_mask:6;
+			uint32_t rsvd1:2;
+			uint32_t port2_int_mask:6;
+			uint32_t rsvd2:2;
+			uint32_t port3_int_mask:6;
+			uint32_t rsvd3:2;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_int_mask_t, *p_txc_int_mask_t;
+
+/* 2 ports */
+typedef union _txc_int_mask_n2_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t rsvd1:18;
+			uint32_t port1_int_mask:6;
+			uint32_t rsvd:2;
+			uint32_t port0_int_mask:6;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t port0_int_mask:6;
+			uint32_t rsvd:2;
+			uint32_t port1_int_mask:6;
+			uint32_t rsvd1:18;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txc_int_mask_n2_t, *p_txc_int_mask_n2_t;
+
+typedef	struct _txc_ro_states {
+	txc_roecc_st_t		roecc;
+	txc_ro_data0_t		d0;
+	txc_ro_data1_t		d1;
+	txc_ro_data2_t		d2;
+	txc_ro_data3_t		d3;
+	txc_ro_data4_t		d4;
+	txc_ro_tids_t		tids;
+	txc_ro_state0_t		st0;
+	txc_ro_state1_t		st1;
+	txc_ro_state2_t		st2;
+	txc_ro_state3_t		st3;
+	txc_ro_ctl_t		ctl;
+} txc_ro_states_t, *p_txc_ro_states_t;
+
+typedef	struct _txc_sf_states {
+	txc_sfecc_st_t		sfecc;
+	txc_sf_data0_t		d0;
+	txc_sf_data1_t		d1;
+	txc_sf_data2_t		d2;
+	txc_sf_data3_t		d3;
+	txc_sf_data4_t		d4;
+} txc_sf_states_t, *p_txc_sf_states_t;
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif	/* _SYS_NXGE_NXGE_TXC_HW_H */
diff --git a/drivers/net/nxge/include/nxge_txdma.h b/drivers/net/nxge/include/nxge_txdma.h
new file mode 100644
index 0000000..573bffd
--- /dev/null
+++ b/drivers/net/nxge/include/nxge_txdma.h
@@ -0,0 +1,311 @@
+/*
+ * nxge_txdma.h		Neptune TX interface
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef	_SYS_NXGE_NXGE_TXDMA_H
+#define	_SYS_NXGE_NXGE_TXDMA_H
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#include <sys/nxge/nxge_txdma_hw.h>
+#include <npi_txdma.h>
+
+#define	TXDMA_PORT_BITMAP(nxgep)		(nxgep->pt_config.tx_dma_map)
+
+#define	TXDMA_RECLAIM_PENDING_DEFAULT		64
+#define	TX_FULL_MARK				3
+
+/*
+ * Descriptor ring empty:
+ *		(1) head index is equal to tail index.
+ *		(2) wrapped around bits are the same.
+ * Descriptor ring full:
+ *		(1) head index is equal to tail index.
+ *		(2) wrapped around bits are different.
+ *
+ */
+#define	TXDMA_RING_EMPTY(head, head_wrap, tail, tail_wrap)	\
+	((head == tail && head_wrap == tail_wrap) ? B_TRUE : B_FALSE)
+
+#define	TXDMA_RING_FULL(head, head_wrap, tail, tail_wrap)	\
+	((head == tail && head_wrap != tail_wrap) ? B_TRUE : B_FALSE)
+
+#define	TXDMA_DESC_NEXT_INDEX(index, entries, wrap_mask) \
+			((index + entries) & wrap_mask)
+
+#define	TXDMA_DRR_WEIGHT_DEFAULT	0x001f
+
+typedef struct _tx_msg_t {
+	nxge_os_block_mv_t 	flags;		/* DMA, BCOPY, DVMA (?) */
+	nxge_os_dma_common_t	buf_dma;	/* premapped buffer blocks */
+	nxge_os_dma_handle_t	buf_dma_handle; /* premapped buffer handle */
+	nxge_os_dma_handle_t 	dma_handle;	/* DMA handle for normal send */
+	nxge_os_dma_handle_t 	dvma_handle;	/* Fast DVMA  handle */
+
+	p_mblk_t 		tx_message;
+	uint32_t 		tx_msg_size;
+	size_t			bytes_used;
+	int			head;
+	int			tail;
+} tx_msg_t, *p_tx_msg_t;
+
+/*
+ * TX  Statistics.
+ */
+typedef struct _nxge_tx_ring_stats_t {
+	uint64_t	opackets;
+	uint64_t	obytes;
+	uint64_t	oerrors;
+
+	uint32_t	tx_inits;
+	uint32_t	tx_no_buf;
+
+	uint32_t		mbox_err;
+	uint32_t		pkt_size_err;
+	uint32_t 		tx_ring_oflow;
+	uint32_t 		pre_buf_par_err;
+	uint32_t 		nack_pref;
+	uint32_t 		nack_pkt_rd;
+	uint32_t 		conf_part_err;
+	uint32_t 		pkt_part_err;
+	uint32_t		tx_starts;
+	uint32_t		tx_nocanput;
+	uint32_t		tx_msgdup_fail;
+	uint32_t		tx_allocb_fail;
+	uint32_t		tx_no_desc;
+	uint32_t		tx_dma_bind_fail;
+	uint32_t		tx_uflo;
+
+	uint32_t		tx_hdr_pkts;
+	uint32_t		tx_ddi_pkts;
+	uint32_t		tx_dvma_pkts;
+
+	uint32_t		tx_max_pend;
+	uint32_t		tx_jumbo_pkts;
+
+	txdma_ring_errlog_t	errlog;
+} nxge_tx_ring_stats_t, *p_nxge_tx_ring_stats_t;
+
+typedef struct _tx_ring_t {
+	nxge_os_dma_common_t	tdc_desc;
+	struct _nxge_t		*nxgep;
+	p_tx_msg_t 		tx_msg_ring;
+	uint32_t		tnblocks;
+	tx_rng_cfig_t		tx_ring_cfig;
+	tx_ring_hdl_t		tx_ring_hdl;
+	tx_ring_kick_t		tx_ring_kick;
+	tx_cs_t			tx_cs;
+	tx_dma_ent_msk_t	tx_evmask;
+	txdma_mbh_t		tx_mbox_mbh;
+	txdma_mbl_t		tx_mbox_mbl;
+	log_page_vld_t		page_valid;
+	log_page_mask_t		page_mask_1;
+	log_page_mask_t		page_mask_2;
+	log_page_value_t	page_value_1;
+	log_page_value_t	page_value_2;
+	log_page_relo_t		page_reloc_1;
+	log_page_relo_t		page_reloc_2;
+	log_page_hdl_t		page_hdl;
+	txc_dma_max_burst_t	max_burst;
+	boolean_t		cfg_set;
+	uint32_t		tx_ring_state;
+
+	nxge_os_mutex_t		lock;
+	uint16_t 		index;
+	uint16_t		tdc;
+	struct nxge_tdc_cfg	*tdc_p;
+	uint_t 			tx_ring_size;
+	uint32_t 		num_chunks;
+
+	uint_t 			tx_wrap_mask;
+	uint_t 			rd_index;
+	uint_t 			wr_index;
+	boolean_t		wr_index_wrap;
+	uint_t 			head_index;
+	boolean_t		head_wrap;
+	tx_ring_hdl_t		ring_head;
+	tx_ring_kick_t		ring_kick_tail;
+	txdma_mailbox_t		tx_mbox;
+
+	uint_t 			descs_pending;
+	boolean_t 		queueing;
+
+	nxge_os_mutex_t		sq_lock;
+
+	p_mblk_t 		head;
+	p_mblk_t 		tail;
+
+	uint16_t		ldg_group_id;
+	p_nxge_tx_ring_stats_t tdc_stats;
+
+	nxge_os_mutex_t 	dvma_lock;
+	uint_t 			dvma_wr_index;
+	uint_t 			dvma_rd_index;
+	uint_t 			dvma_pending;
+	uint_t 			dvma_available;
+	uint_t 			dvma_wrap_mask;
+
+	nxge_os_dma_handle_t 	*dvma_ring;
+
+#ifndef	NEW_NEMO
+	mac_resource_handle_t	tx_mac_resource_handle;
+#endif
+#if	defined(sun4v) && defined(NIU_LP_WORKAROUND)
+	uint64_t		hv_tx_buf_base_ioaddr_pp;
+	uint64_t		hv_tx_buf_ioaddr_size;
+	uint64_t		hv_tx_cntl_base_ioaddr_pp;
+	uint64_t		hv_tx_cntl_ioaddr_size;
+	boolean_t		hv_set;
+#endif
+} tx_ring_t, *p_tx_ring_t;
+
+
+/* Transmit Mailbox */
+typedef struct _tx_mbox_t {
+	nxge_os_mutex_t 	lock;
+	uint16_t		index;
+	struct _nxge_t		*nxgep;
+	uint16_t		tdc;
+	nxge_os_dma_common_t	tx_mbox;
+	txdma_mbl_t		tx_mbox_l;
+	txdma_mbh_t		tx_mbox_h;
+} tx_mbox_t, *p_tx_mbox_t;
+
+typedef struct _tx_rings_t {
+	p_tx_ring_t 		*rings;
+	boolean_t		txdesc_allocated;
+	uint32_t		ndmas;
+	nxge_os_dma_common_t	tdc_dma;
+	nxge_os_dma_common_t	tdc_mbox;
+} tx_rings_t, *p_tx_rings_t;
+
+
+#if defined(_KERNEL) || (defined(COSIM) && !defined(IODIAG))
+
+typedef struct _tx_buf_rings_t {
+	struct _tx_buf_ring_t 	*txbuf_rings;
+	boolean_t		txbuf_allocated;
+} tx_buf_rings_t, *p_tx_buf_rings_t;
+
+#endif
+
+typedef struct _tx_mbox_areas_t {
+	p_tx_mbox_t 		*txmbox_areas_p;
+	boolean_t		txmbox_allocated;
+} tx_mbox_areas_t, *p_tx_mbox_areas_t;
+
+typedef struct _tx_param_t {
+	nxge_logical_page_t tx_logical_pages[NXGE_MAX_LOGICAL_PAGES];
+} tx_param_t, *p_tx_param_t;
+
+typedef struct _tx_params {
+	struct _tx_param_t 	*tx_param_p;
+} tx_params_t, *p_tx_params_t;
+
+/*
+ * Global register definitions per chip and they are initialized
+ * using the function zero control registers.
+ * .
+ */
+typedef struct _txdma_globals {
+	boolean_t		mode32;
+} txdma_globals_t, *p_txdma_globals;
+
+
+#if	defined(SOLARIS) && (defined(_KERNEL) || \
+	(defined(COSIM) && !defined(IODIAG)))
+
+/*
+ * Transmit prototypes.
+ */
+nxge_status_t nxge_init_txdma_channels(p_nxge_t nxgep);
+void nxge_uninit_txdma_channels(p_nxge_t nxgep);
+void nxge_setup_dma_common(p_nxge_dma_common_t, p_nxge_dma_common_t,
+		uint32_t, uint32_t);
+nxge_status_t nxge_reset_txdma_channel(p_nxge_t nxgep, uint16_t channel,
+	uint64_t reg_data);
+nxge_status_t nxge_init_txdma_channel_event_mask(p_nxge_t nxgep,
+	uint16_t channel, p_tx_dma_ent_msk_t mask_p);
+nxge_status_t nxge_init_txdma_channel_cntl_stat(p_nxge_t nxgep,
+	uint16_t channel, uint64_t reg_data);
+nxge_status_t nxge_enable_txdma_channel(p_nxge_t nxgep, uint16_t channel,
+	p_tx_ring_t tx_desc_p, p_tx_mbox_t mbox_p);
+
+p_mblk_t nxge_tx_pkt_header_reserve(p_mblk_t mp, uint8_t *npads);
+int nxge_tx_pkt_nmblocks(p_mblk_t mp, int *tot_xfer_len_p);
+boolean_t nxge_txdma_reclaim(p_nxge_t nxgep, p_tx_ring_t tx_ring_p, int nmblks);
+
+void nxge_fill_tx_hdr(p_mblk_t mp, boolean_t fill_len, boolean_t l4_cksum,
+	int pkt_len, uint8_t npads, p_tx_pkt_hdr_all_t pkthdrp);
+
+nxge_status_t nxge_txdma_hw_mode(p_nxge_t nxgep, boolean_t enable);
+void nxge_hw_start_tx(p_nxge_t nxgep);
+void nxge_txdma_stop(p_nxge_t nxgep);
+void nxge_txdma_stop_start(p_nxge_t nxgep);
+void nxge_fixup_txdma_rings(p_nxge_t nxgep);
+void nxge_txdma_hw_kick(p_nxge_t nxgep);
+void nxge_txdma_fix_channel(p_nxge_t nxgep, uint16_t channel);
+void nxge_txdma_fixup_channel(p_nxge_t nxgep, p_tx_ring_t ring_p,
+	uint16_t channel);
+void nxge_txdma_hw_kick_channel(p_nxge_t nxgep, p_tx_ring_t ring_p,
+	uint16_t channel);
+
+void nxge_txdma_regs_dump(p_nxge_t nxgep, int channel);
+void nxge_txdma_regs_dump_channels(p_nxge_t nxgep);
+
+void nxge_check_tx_hang(p_nxge_t nxgep);
+void nxge_fixup_hung_txdma_rings(p_nxge_t nxgep);
+void nxge_txdma_fix_hung_channel(p_nxge_t nxgep, uint16_t channel);
+void nxge_txdma_fixup_hung_channel(p_nxge_t nxgep, p_tx_ring_t ring_p,
+	uint16_t channel);
+
+void nxge_reclaim_rings(p_nxge_t nxgep);
+int nxge_txdma_channel_hung(p_nxge_t nxgep,
+	p_tx_ring_t tx_ring_p, uint16_t channel);
+int nxge_txdma_hung(p_nxge_t nxgep);
+int nxge_txdma_stop_inj_err(p_nxge_t nxgep, int channel);
+void nxge_txdma_inject_err(p_nxge_t nxgep, uint32_t err_id, uint8_t chan);
+
+#endif
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif	/* _SYS_NXGE_NXGE_TXDMA_H */
diff --git a/drivers/net/nxge/include/nxge_txdma_hw.h b/drivers/net/nxge/include/nxge_txdma_hw.h
new file mode 100644
index 0000000..26db678
--- /dev/null
+++ b/drivers/net/nxge/include/nxge_txdma_hw.h
@@ -0,0 +1,942 @@
+/*
+ * nxge_txdma_hw.h	Neptune TX HW register offsets
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef	_SYS_NXGE_NXGE_TXDMA_HW_H
+#define	_SYS_NXGE_NXGE_TXDMA_HW_H
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#include <nxge_defs.h>
+  /*#include <nxge_hw.h> */
+
+#if !defined(_BIG_ENDIAN)
+#define	SWAP(X)	(X)
+#else
+#define	SWAP(X)   \
+	(((X >> 32) & 0x00000000ffffffff) | \
+	((X << 32) & 0xffffffff00000000))
+#endif
+
+/*
+ * Partitioning Suport: same as those defined for the RX
+ */
+/*
+ * TDC: Partitioning Support
+ *	(Each of the following registers is for each TDC)
+ */
+#define	TX_LOG_REG_SIZE			512
+#define	TX_LOG_DMA_OFFSET(channel)	(channel * TX_LOG_REG_SIZE)
+
+#define	TX_LOG_PAGE_VLD_REG		(FZC_DMC + 0x40000)
+#define	TX_LOG_PAGE_MASK1_REG		(FZC_DMC + 0x40008)
+#define	TX_LOG_PAGE_VAL1_REG		(FZC_DMC + 0x40010)
+#define	TX_LOG_PAGE_MASK2_REG		(FZC_DMC + 0x40018)
+#define	TX_LOG_PAGE_VAL2_REG		(FZC_DMC + 0x40020)
+#define	TX_LOG_PAGE_RELO1_REG		(FZC_DMC + 0x40028)
+#define	TX_LOG_PAGE_RELO2_REG		(FZC_DMC + 0x40030)
+#define	TX_LOG_PAGE_HDL_REG		(FZC_DMC + 0x40038)
+
+/* Transmit Addressing Mode: Set to 1 to select 32-bit addressing mode */
+#define	TX_ADDR_MD_REG			(FZC_DMC + 0x45000)
+
+#define	TX_ADDR_MD_SHIFT	0			/* bits 0:0 */
+#define	TX_ADDR_MD_SET_32	0x0000000000000001ULL	/* 1 to select 32 bit */
+#define	TX_ADDR_MD_MASK		0x0000000000000001ULL
+
+typedef union _tx_addr_md_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res1_1:31;
+			uint32_t mode32:1;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t mode32:1;
+			uint32_t res1_1:31;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} tx_addr_md_t, *p_tx_addr_md_t;
+
+/* Transmit Packet Descriptor Structure */
+#define	TX_PKT_DESC_SAD_SHIFT		0		/* bits 43:0 */
+#define	TX_PKT_DESC_SAD_MASK		0x00000FFFFFFFFFFFULL
+#define	TX_PKT_DESC_TR_LEN_SHIFT	44		/* bits 56:44 */
+#define	TX_PKT_DESC_TR_LEN_MASK		0x01FFF00000000000ULL
+#define	TX_PKT_DESC_NUM_PTR_SHIFT	58		/* bits 61:58 */
+#define	TX_PKT_DESC_NUM_PTR_MASK	0x3C00000000000000ULL
+#define	TX_PKT_DESC_MARK_SHIFT		62		/* bit 62 */
+#define	TX_PKT_DESC_MARK		0x4000000000000000ULL
+#define	TX_PKT_DESC_MARK_MASK		0x4000000000000000ULL
+#define	TX_PKT_DESC_SOP_SHIFT		63		/* bit 63 */
+#define	TX_PKT_DESC_SOP			0x8000000000000000ULL
+#define	TX_PKT_DESC_SOP_MASK		0x8000000000000000ULL
+
+typedef union _tx_desc_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t sop:1;
+			uint32_t mark:1;
+			uint32_t num_ptr:4;
+			uint32_t res1:1;
+			uint32_t tr_len:13;
+			uint32_t sad:12;
+
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t sad:12;
+			uint32_t tr_len:13;
+			uint32_t res1:1;
+			uint32_t num_ptr:4;
+			uint32_t mark:1;
+			uint32_t sop:1;
+
+#endif
+		} hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t sad:32;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t sad:32;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		struct {
+
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t sop:1;
+			uint32_t mark:1;
+			uint32_t num_ptr:4;
+			uint32_t res1:1;
+			uint32_t tr_len:13;
+			uint32_t sad:12;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t sad:12;
+			uint32_t tr_len:13;
+			uint32_t res1:1;
+			uint32_t num_ptr:4;
+			uint32_t mark:1;
+			uint32_t sop:1;
+#endif
+		} hdw;
+#endif
+	} bits;
+} tx_desc_t, *p_tx_desc_t;
+
+
+/* Transmit Ring Configuration (24 Channels) */
+#define	TX_RNG_CFIG_REG			(DMC + 0x40000)
+#define	TX_RING_HDL_REG			(DMC + 0x40010)
+#define	TX_RING_KICK_REG		(DMC + 0x40018)
+#define	TX_ENT_MSK_REG			(DMC + 0x40020)
+#define	TX_CS_REG			(DMC + 0x40028)
+#define	TXDMA_MBH_REG			(DMC + 0x40030)
+#define	TXDMA_MBL_REG			(DMC + 0x40038)
+#define	TX_DMA_PRE_ST_REG		(DMC + 0x40040)
+#define	TX_RNG_ERR_LOGH_REG		(DMC + 0x40048)
+#define	TX_RNG_ERR_LOGL_REG		(DMC + 0x40050)
+#define	TDMC_INTR_DBG_REG		(DMC + 0x40060)
+#define	TX_CS_DBG_REG			(DMC + 0x40068)
+
+/* Transmit Ring Configuration */
+#define	TX_RNG_CFIG_STADDR_SHIFT	6			/* bits 18:6 */
+#define	TX_RNG_CFIG_STADDR_MASK		0x000000000007FFC0ULL
+#define	TX_RNG_CFIG_ADDR_MASK		0x00000FFFFFFFFFC0ULL
+#define	TX_RNG_CFIG_STADDR_BASE_SHIFT	19			/* bits 43:19 */
+#define	TX_RNG_CFIG_STADDR_BASE_MASK	0x00000FFFFFF80000ULL
+#define	TX_RNG_CFIG_LEN_SHIFT		48			/* bits 60:48 */
+#define	TX_RNG_CFIG_LEN_MASK		0xFFF8000000000000ULL
+
+#define	TX_RNG_HEAD_TAIL_SHIFT		3
+#define	TX_RNG_HEAD_TAIL_WRAP_SHIFT	19
+
+typedef union _tx_rng_cfig_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res2:3;
+			uint32_t len:13;
+			uint32_t res1:4;
+			uint32_t staddr_base:12;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t staddr_base:12;
+			uint32_t res1:4;
+			uint32_t len:13;
+			uint32_t res2:3;
+#endif
+		} hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t staddr_base:13;
+			uint32_t staddr:13;
+			uint32_t res2:6;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t res2:6;
+			uint32_t staddr:13;
+			uint32_t staddr_base:13;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res2:3;
+			uint32_t len:13;
+			uint32_t res1:4;
+			uint32_t staddr_base:12;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t staddr_base:12;
+			uint32_t res1:4;
+			uint32_t len:13;
+			uint32_t res2:3;
+#endif
+		} hdw;
+#endif
+	} bits;
+} tx_rng_cfig_t, *p_tx_rng_cfig_t;
+
+/* Transmit Ring Head Low */
+#define	TX_RING_HDL_SHIFT		3			/* bit 31:3 */
+#define	TX_RING_HDL_MASK		0x00000000FFFFFFF8ULL
+
+typedef union _tx_ring_hdl_t {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res0:12;
+			uint32_t wrap:1;
+			uint32_t head:16;
+			uint32_t res2:3;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t res2:3;
+			uint32_t head:16;
+			uint32_t wrap:1;
+			uint32_t res0:12;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} tx_ring_hdl_t, *p_tx_ring_hdl_t;
+
+/* Transmit Ring Kick */
+#define	TX_RING_KICK_TAIL_SHIFT		3			/* bit 43:3 */
+#define	TX_RING_KICK_TAIL_MASK		0x000000FFFFFFFFFF8ULL
+
+typedef union _tx_ring_kick_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res0:12;
+			uint32_t wrap:1;
+			uint32_t tail:16;
+			uint32_t res2:3;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t res2:3;
+			uint32_t tail:16;
+			uint32_t wrap:1;
+			uint32_t res0:12;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} tx_ring_kick_t, *p_tx_ring_kick_t;
+
+/* Transmit Event Mask (DMC + 0x40020) */
+#define	TX_ENT_MSK_PKT_PRT_ERR_SHIFT		0	/* bit 0: 0 to flag */
+#define	TX_ENT_MSK_PKT_PRT_ERR_MASK		0x0000000000000001ULL
+#define	TX_ENT_MSK_CONF_PART_ERR_SHIFT		1	/* bit 1: 0 to flag */
+#define	TX_ENT_MSK_CONF_PART_ERR_MASK		0x0000000000000002ULL
+#define	TX_ENT_MSK_NACK_PKT_RD_SHIFT		2	/* bit 2: 0 to flag */
+#define	TX_ENT_MSK_NACK_PKT_RD_MASK		0x0000000000000004ULL
+#define	TX_ENT_MSK_NACK_PREF_SHIFT		3	/* bit 3: 0 to flag */
+#define	TX_ENT_MSK_NACK_PREF_MASK		0x0000000000000008ULL
+#define	TX_ENT_MSK_PREF_BUF_ECC_ERR_SHIFT	4	/* bit 4: 0 to flag */
+#define	TX_ENT_MSK_PREF_BUF_ECC_ERR_MASK	0x0000000000000010ULL
+#define	TX_ENT_MSK_TX_RING_OFLOW_SHIFT		5	/* bit 5: 0 to flag */
+#define	TX_ENT_MSK_TX_RING_OFLOW_MASK		0x0000000000000020ULL
+#define	TX_ENT_MSK_PKT_SIZE_ERR_SHIFT		6	/* bit 6: 0 to flag */
+#define	TX_ENT_MSK_PKT_SIZE_ERR_MASK		0x0000000000000040ULL
+#define	TX_ENT_MSK_MBOX_ERR_SHIFT		7	/* bit 7: 0 to flag */
+#define	TX_ENT_MSK_MBOX_ERR_MASK		0x0000000000000080ULL
+#define	TX_ENT_MSK_MK_SHIFT			15	/* bit 15: 0 to flag */
+#define	TX_ENT_MSK_MK_MASK			0x0000000000008000ULL
+#define	TX_ENT_MSK_MK_ALL		(TX_ENT_MSK_PKT_PRT_ERR_MASK | \
+					TX_ENT_MSK_CONF_PART_ERR_MASK |	\
+					TX_ENT_MSK_NACK_PKT_RD_MASK |	\
+					TX_ENT_MSK_NACK_PREF_MASK |	\
+					TX_ENT_MSK_PREF_BUF_ECC_ERR_MASK | \
+					TX_ENT_MSK_TX_RING_OFLOW_MASK |	\
+					TX_ENT_MSK_PKT_SIZE_ERR_MASK | \
+					TX_ENT_MSK_MBOX_ERR_MASK | \
+					TX_ENT_MSK_MK_MASK)
+
+
+typedef union _tx_dma_ent_msk_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res1_1:16;
+			uint32_t mk:1;
+			uint32_t res2:7;
+			uint32_t mbox_err:1;
+			uint32_t pkt_size_err:1;
+			uint32_t tx_ring_oflow:1;
+			uint32_t pref_buf_ecc_err:1;
+			uint32_t nack_pref:1;
+			uint32_t nack_pkt_rd:1;
+			uint32_t conf_part_err:1;
+			uint32_t pkt_prt_err:1;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t pkt_prt_err:1;
+			uint32_t conf_part_err:1;
+			uint32_t nack_pkt_rd:1;
+			uint32_t nack_pref:1;
+			uint32_t pref_buf_ecc_err:1;
+			uint32_t tx_ring_oflow:1;
+			uint32_t pkt_size_err:1;
+			uint32_t mbox_err:1;
+			uint32_t res2:7;
+			uint32_t mk:1;
+			uint32_t res1_1:16;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} tx_dma_ent_msk_t, *p_tx_dma_ent_msk_t;
+
+
+/* Transmit Control and Status  (DMC + 0x40028) */
+#define	TX_CS_PKT_PRT_ERR_SHIFT			0	/* RO, bit 0 */
+#define	TX_CS_PKT_PRT_ERR_MASK			0x0000000000000001ULL
+#define	TX_CS_CONF_PART_ERR_SHIF		1	/* RO, bit 1 */
+#define	TX_CS_CONF_PART_ERR_MASK		0x0000000000000002ULL
+#define	TX_CS_NACK_PKT_RD_SHIFT			2	/* RO, bit 2 */
+#define	TX_CS_NACK_PKT_RD_MASK			0x0000000000000004ULL
+#define	TX_CS_PREF_SHIFT			3	/* RO, bit 3 */
+#define	TX_CS_PREF_MASK				0x0000000000000008ULL
+#define	TX_CS_PREF_BUF_PAR_ERR_SHIFT		4	/* RO, bit 4 */
+#define	TX_CS_PREF_BUF_PAR_ERR_MASK		0x0000000000000010ULL
+#define	TX_CS_RING_OFLOW_SHIFT			5	/* RO, bit 5 */
+#define	TX_CS_RING_OFLOW_MASK			0x0000000000000020ULL
+#define	TX_CS_PKT_SIZE_ERR_SHIFT		6	/* RW, bit 6 */
+#define	TX_CS_PKT_SIZE_ERR_MASK			0x0000000000000040ULL
+#define	TX_CS_MMK_SHIFT				14	/* RC, bit 14 */
+#define	TX_CS_MMK_MASK				0x0000000000004000ULL
+#define	TX_CS_MK_SHIFT				15	/* RCW1C, bit 15 */
+#define	TX_CS_MK_MASK				0x0000000000008000ULL
+#define	TX_CS_SNG_SHIFT				27	/* RO, bit 27 */
+#define	TX_CS_SNG_MASK				0x0000000008000000ULL
+#define	TX_CS_STOP_N_GO_SHIFT			28	/* RW, bit 28 */
+#define	TX_CS_STOP_N_GO_MASK			0x0000000010000000ULL
+#define	TX_CS_MB_SHIFT				29	/* RO, bit 29 */
+#define	TX_CS_MB_MASK				0x0000000020000000ULL
+#define	TX_CS_RST_STATE_SHIFT			30	/* Rw, bit 30 */
+#define	TX_CS_RST_STATE_MASK			0x0000000040000000ULL
+#define	TX_CS_RST_SHIFT				31	/* Rw, bit 31 */
+#define	TX_CS_RST_MASK				0x0000000080000000ULL
+#define	TX_CS_LASTMASK_SHIFT			32	/* RW, bit 43:32 */
+#define	TX_CS_LASTMARK_MASK			0x00000FFF00000000ULL
+#define	TX_CS_PKT_CNT_SHIFT			48	/* RW, bit 59:48 */
+#define	TX_CS_PKT_CNT_MASK			0x0FFF000000000000ULL
+
+/* Trasnmit Control and Status */
+typedef union _tx_cs_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res1:4;
+			uint32_t pkt_cnt:12;
+			uint32_t res2:4;
+			uint32_t lastmark:12;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t lastmark:12;
+			uint32_t res2:4;
+			uint32_t pkt_cnt:12;
+			uint32_t res1:4;
+#endif
+		} hdw;
+
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t rst:1;
+			uint32_t rst_state:1;
+			uint32_t mb:1;
+			uint32_t stop_n_go:1;
+			uint32_t sng_state:1;
+			uint32_t res1:11;
+			uint32_t mk:1;
+			uint32_t mmk:1;
+			uint32_t res2:6;
+			uint32_t mbox_err:1;
+			uint32_t pkt_size_err:1;
+			uint32_t tx_ring_oflow:1;
+			uint32_t pref_buf_par_err:1;
+			uint32_t nack_pref:1;
+			uint32_t nack_pkt_rd:1;
+			uint32_t conf_part_err:1;
+			uint32_t pkt_prt_err:1;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t pkt_prt_err:1;
+			uint32_t conf_part_err:1;
+			uint32_t nack_pkt_rd:1;
+			uint32_t nack_pref:1;
+			uint32_t pref_buf_par_err:1;
+			uint32_t tx_ring_oflow:1;
+			uint32_t pkt_size_err:1;
+			uint32_t mbox_err:1;
+			uint32_t res2:6;
+			uint32_t mmk:1;
+			uint32_t mk:1;
+			uint32_t res1:11;
+			uint32_t sng_state:1;
+			uint32_t stop_n_go:1;
+			uint32_t mb:1;
+			uint32_t rst_state:1;
+			uint32_t rst:1;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res1:4;
+			uint32_t pkt_cnt:12;
+			uint32_t res2:4;
+			uint32_t lastmark:12;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t lastmark:12;
+			uint32_t res2:4;
+			uint32_t pkt_cnt:12;
+			uint32_t res1:4;
+#endif
+	} hdw;
+
+#endif
+	} bits;
+} tx_cs_t, *p_tx_cs_t;
+
+/* Trasnmit Mailbox High (DMC + 0x40030) */
+#define	TXDMA_MBH_SHIFT			0	/* bit 11:0 */
+#define	TXDMA_MBH_ADDR_SHIFT		32	/* bit 43:32 */
+#define	TXDMA_MBH_MASK			0x0000000000000FFFULL
+
+typedef union _txdma_mbh_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res1_1:20;
+			uint32_t mbaddr:12;
+
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t mbaddr:12;
+			uint32_t res1_1:20;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txdma_mbh_t, *p_txdma_mbh_t;
+
+
+/* Trasnmit Mailbox Low (DMC + 0x40038) */
+#define	TXDMA_MBL_SHIFT			6	/* bit 31:6 */
+#define	TXDMA_MBL_MASK			0x00000000FFFFFFC0ULL
+
+typedef union _txdma_mbl_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t mbaddr:26;
+			uint32_t res2:6;
+
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t res2:6;
+			uint32_t mbaddr:26;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} txdma_mbl_t, *p_txdma_mbl_t;
+
+/* Trasnmit Prefetch State High (DMC + 0x40040) */
+#define	TX_DMA_PREF_ST_SHIFT		0	/* bit 5:0 */
+#define	TX_DMA_PREF_ST_MASK		0x000000000000003FULL
+
+typedef union _tx_dma_pre_st_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res1_1:13;
+			uint32_t shadow_hd:19;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t shadow_hd:19;
+			uint32_t res1_1:13;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} tx_dma_pre_st_t, *p_tx_dma_pre_st_t;
+
+/* Trasnmit Ring Error Log High (DMC + 0x40048) */
+#define	TX_RNG_ERR_LOGH_ERR_ADDR_SHIFT		0	/* RO bit 11:0 */
+#define	TX_RNG_ERR_LOGH_ERR_ADDR_MASK		0x0000000000000FFFULL
+#define	TX_RNG_ERR_LOGH_ADDR_SHIFT		32
+#define	TX_RNG_ERR_LOGH_ERRCODE_SHIFT		26	/* RO bit 29:26 */
+#define	TX_RNG_ERR_LOGH_ERRCODE_MASK		0x000000003C000000ULL
+#define	TX_RNG_ERR_LOGH_MERR_SHIFT		30	/* RO bit 30 */
+#define	TX_RNG_ERR_LOGH_MERR_MASK		0x0000000040000000ULL
+#define	TX_RNG_ERR_LOGH_ERR_SHIFT		31	/* RO bit 31 */
+#define	TX_RNG_ERR_LOGH_ERR_MASK		0x0000000080000000ULL
+
+/* Transmit Ring Error codes */
+#define	TXDMA_RING_PKT_PRT_ERR			0
+#define	TXDMA_RING_CONF_PART_ERR		0x01
+#define	TXDMA_RING_NACK_PKT_ERR			0x02
+#define	TXDMA_RING_NACK_PREF_ERR		0x03
+#define	TXDMA_RING_PREF_BUF_PAR_ERR		0x04
+#define	TXDMA_RING_TX_RING_OFLOW_ERR		0x05
+#define	TXDMA_RING_PKT_SIZE_ERR			0x06
+
+typedef union _tx_rng_err_logh_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t err:1;
+			uint32_t merr:1;
+			uint32_t errcode:4;
+			uint32_t res2:14;
+			uint32_t err_addr:12;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t err_addr:12;
+			uint32_t res2:14;
+			uint32_t errcode:4;
+			uint32_t merr:1;
+			uint32_t err:1;
+
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} tx_rng_err_logh_t, *p_tx_rng_err_logh_t;
+
+
+/* Trasnmit Ring Error Log Log (DMC + 0x40050) */
+#define	TX_RNG_ERR_LOGL_ERR_ADDR_SHIFT		0	/* RO bit 31:0 */
+#define	TX_RNG_ERR_LOGL_ERR_ADDR_MASK		0x00000000FFFFFFFFULL
+
+typedef union _tx_rng_err_logl_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t err_addr:32;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t err_addr:32;
+
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} tx_rng_err_logl_t, *p_tx_rng_err_logl_t;
+
+/*
+ * TDMC_INTR_RBG_REG (DMC + 0x40060)
+ */
+typedef union _tdmc_intr_dbg_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res:16;
+			uint32_t mk:1;
+			uint32_t rsvd:7;
+			uint32_t mbox_err:1;
+			uint32_t pkt_size_err:1;
+			uint32_t tx_ring_oflow:1;
+			uint32_t pref_buf_par_err:1;
+			uint32_t nack_pref:1;
+			uint32_t nack_pkt_rd:1;
+			uint32_t conf_part_err:1;
+			uint32_t pkt_part_err:1;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t pkt_part_err:1;
+			uint32_t conf_part_err:1;
+			uint32_t nack_pkt_rd:1;
+			uint32_t nack_pref:1;
+			uint32_t pref_buf_par_err:1;
+			uint32_t tx_ring_oflow:1;
+			uint32_t pkt_size_err:1;
+			uint32_t mbox_err:1;
+			uint32_t rsvd:7;
+			uint32_t mk:1;
+			uint32_t res:16;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} tdmc_intr_dbg_t, *p_tdmc_intr_dbg_t;
+
+
+/*
+ * TX_CS_DBG (DMC + 0x40068)
+ */
+typedef union _tx_cs_dbg_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res1:4;
+			uint32_t pkt_cnt:12;
+			uint32_t res2:16;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t res2:16;
+			uint32_t pkt_cnt:12;
+			uint32_t res1:4;
+#endif
+		} hdw;
+
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t rsvd:32;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t rsvd:32;
+
+#endif
+		} ldw;
+
+#ifndef _BIG_ENDIAN
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t res1:4;
+			uint32_t pkt_cnt:12;
+			uint32_t res2:16;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t res2:16;
+			uint32_t pkt_cnt:12;
+			uint32_t res1:4;
+#endif
+	} hdw;
+
+#endif
+	} bits;
+} tx_cs_dbg_t, *p_tx_cs_dbg_t;
+
+#define	TXDMA_MAILBOX_BYTE_LENGTH		64
+#define	TXDMA_MAILBOX_UNUSED			24
+
+typedef struct _txdma_mailbox_t {
+	tx_cs_t			tx_cs;				/* 8 bytes */
+	tx_dma_pre_st_t		tx_dma_pre_st;			/* 8 bytes */
+	tx_ring_hdl_t		tx_ring_hdl;			/* 8 bytes */
+	tx_ring_kick_t		tx_ring_kick;			/* 8 bytes */
+	uint32_t		tx_rng_err_logh;		/* 4 bytes */
+	uint32_t		tx_rng_err_logl;		/* 4 bytes */
+	uint32_t		resv[TXDMA_MAILBOX_UNUSED];
+} txdma_mailbox_t, *p_txdma_mailbox_t;
+
+
+
+/*
+ * Internal Transmit Packet Format (16 bytes)
+ */
+#define	TX_PKT_HEADER_SIZE			16
+#define	TX_MAX_GATHER_POINTERS			15
+#define	TX_GATHER_POINTERS_THRESHOLD		8
+/*
+ * There is bugs in the hardware
+ * and max sfter len is changed from 4096 to 4076.
+ *
+ * Jumbo from 9500 to 9216
+ */
+#define	TX_MAX_TRANSFER_LENGTH			4076
+#define	TX_JUMBO_MTU				9216
+
+#define	TX_PKT_HEADER_PAD_SHIFT			0	/* bit 2:0 */
+#define	TX_PKT_HEADER_PAD_MASK			0x0000000000000007ULL
+#define	TX_PKT_HEADER_TOT_XFER_LEN_SHIFT	16	/* bit 16:29 */
+#define	TX_PKT_HEADER_TOT_XFER_LEN_MASK		0x000000000000FFF8ULL
+#define	TX_PKT_HEADER_L4STUFF_SHIFT		32	/* bit 37:32 */
+#define	TX_PKT_HEADER_L4STUFF_MASK		0x0000003F00000000ULL
+#define	TX_PKT_HEADER_L4START_SHIFT		40	/* bit 45:40 */
+#define	TX_PKT_HEADER_L4START_MASK		0x00003F0000000000ULL
+#define	TX_PKT_HEADER_L3START_SHIFT		48	/* bit 45:40 */
+#define	TX_PKT_HEADER_IHL_SHIFT			52	/* bit 52 */
+#define	TX_PKT_HEADER_VLAN__SHIFT		56	/* bit 56 */
+#define	TX_PKT_HEADER_TCP_UDP_CRC32C_SHIFT	57	/* bit 57 */
+#define	TX_PKT_HEADER_LLC_SHIFT			57	/* bit 57 */
+#define	TX_PKT_HEADER_TCP_UDP_CRC32C_SET	0x0200000000000000ULL
+#define	TX_PKT_HEADER_TCP_UDP_CRC32C_MASK	0x0200000000000000ULL
+#define	TX_PKT_HEADER_L4_PROTO_OP_SHIFT		2	/* bit 59:58 */
+#define	TX_PKT_HEADER_L4_PROTO_OP_MASK		0x0C00000000000000ULL
+#define	TX_PKT_HEADER_V4_HDR_CS_SHIFT		60	/* bit 60 */
+#define	TX_PKT_HEADER_V4_HDR_CS_SET		0x1000000000000000ULL
+#define	TX_PKT_HEADER_V4_HDR_CS_MASK		0x1000000000000000ULL
+#define	TX_PKT_HEADER_IP_VER_SHIFT		61	/* bit 61 */
+#define	TX_PKT_HEADER_IP_VER_MASK		0x2000000000000000ULL
+#define	TX_PKT_HEADER_PKT_TYPE_SHIFT		62	/* bit 62 */
+#define	TX_PKT_HEADER_PKT_TYPE_MASK		0xc000000000000000ULL
+
+/* L4 Prototol Operations */
+#define	TX_PKT_L4_PROTO_OP_NOP			0x00
+#define	TX_PKT_L4_PROTO_OP_FULL_L4_CSUM		0x01
+#define	TX_PKT_L4_PROTO_OP_L4_PAYLOAD_CSUM	0x02
+#define	TX_PKT_L4_PROTO_OP_SCTP_CRC32		0x04
+
+/* Transmit Packet Types */
+#define	TX_PKT_PKT_TYPE_NOP			0x00
+#define	TX_PKT_PKT_TYPE_TCP			0x01
+#define	TX_PKT_PKT_TYPE_UDP			0x02
+#define	TX_PKT_PKT_TYPE_SCTP			0x03
+
+typedef union _tx_pkt_header_t {
+	uint64_t value;
+	struct {
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t pad:3;
+			uint32_t resv2:13;
+			uint32_t tot_xfer_len:14;
+			uint32_t resv1:2;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t pad:3;
+			uint32_t resv2:13;
+			uint32_t tot_xfer_len:14;
+			uint32_t resv1:2;
+#endif
+		} ldw;
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t l4stuff:6;
+			uint32_t resv3:2;
+			uint32_t l4start:6;
+			uint32_t resv2:2;
+			uint32_t l3start:4;
+			uint32_t ihl:4;
+			uint32_t vlan:1;
+			uint32_t llc:1;
+			uint32_t res1:3;
+			uint32_t ip_ver:1;
+			uint32_t cksum_en_pkt_type:2;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t l4stuff:6;
+			uint32_t resv3:2;
+			uint32_t l4start:6;
+			uint32_t resv2:2;
+			uint32_t l3start:4;
+			uint32_t ihl:4;
+			uint32_t vlan:1;
+			uint32_t llc:1;
+			uint32_t res1:3;
+			uint32_t ip_ver:1;
+			uint32_t cksum_en_pkt_type:2;
+#endif
+		} hdw;
+	} bits;
+} tx_pkt_header_t, *p_tx_pkt_header_t;
+
+typedef struct _tx_pkt_hdr_all_t {
+	tx_pkt_header_t		pkthdr;
+	uint64_t		reserved;
+} tx_pkt_hdr_all_t, *p_tx_pkt_hdr_all_t;
+
+/* Debug only registers */
+#define	TDMC_INJ_PAR_ERR_REG		(FZC_DMC + 0x45040)
+#define	TDMC_INJ_PAR_ERR_MASK		0x0000000000FFFFFFULL
+#define	TDMC_INJ_PAR_ERR_MASK_N2	0x000000000000FFFFULL
+
+typedef union _tdmc_inj_par_err_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t rsvc:8;
+			uint32_t inject_parity_error:24;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t inject_parity_error:24;
+			uint32_t rsvc:8;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} tdmc_inj_par_err_t, *p_tdmc_inj_par_err_t;
+
+typedef union _tdmc_inj_par_err_n2_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t rsvc:16;
+			uint32_t inject_parity_error:16;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t inject_parity_error:16;
+			uint32_t rsvc:16;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} tdmc_inj_par_err_n2_t, *p_tdmc_inj_par_err_n2_t;
+
+#define	TDMC_DBG_SEL_REG		(FZC_DMC + 0x45080)
+#define	TDMC_DBG_SEL_MASK		0x000000000000003FULL
+
+typedef union _tdmc_dbg_sel_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t rsvc:26;
+			uint32_t dbg_sel:6;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t dbg_sel:6;
+			uint32_t rsvc:26;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} tdmc_dbg_sel_t, *p_tdmc_dbg_sel_t;
+
+#define	TDMC_TRAINING_REG		(FZC_DMC + 0x45088)
+#define	TDMC_TRAINING_MASK		0x00000000FFFFFFFFULL
+
+typedef union _tdmc_training_t {
+	uint64_t value;
+	struct {
+#ifdef	_BIG_ENDIAN
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t vec:32;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t vec:6;
+#endif
+		} ldw;
+#ifndef _BIG_ENDIAN
+		uint32_t hdw;
+#endif
+	} bits;
+} tdmc_training_t, *p_tdmc_training_t;
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif	/* _SYS_NXGE_NXGE_TXDMA_HW_H */
diff --git a/drivers/net/nxge/include/nxge_virtual.h b/drivers/net/nxge/include/nxge_virtual.h
new file mode 100644
index 0000000..fb2ae03
--- /dev/null
+++ b/drivers/net/nxge/include/nxge_virtual.h
@@ -0,0 +1,91 @@
+/*
+ * nxge_virtual.h	Neptune HW virtualization interface
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef	_SYS_NXGE_NXGE_VIRTUAL_H
+#define	_SYS_NXGE_NXGE_VIRTUAL_H
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+/*
+ * Neptune Virtualization Control Operations
+ */
+typedef enum {
+	NXGE_CTLOPS_NIUTYPE,
+	NXGE_CTLOPS_GET_ATTRIBUTES,
+	NXGE_CTLOPS_GET_HWPROPERTIES,
+	NXGE_CTLOPS_SET_HWPROPERTIES,
+	NXGE_CTLOPS_GET_SHARED_REG,
+	NXGE_CTLOPS_SET_SHARED_REG,
+	NXGE_CTLOPS_UPDATE_SHARED_REG,
+	NXGE_CTLOPS_GET_LOCK_BLOCK,
+	NXGE_CTLOPS_GET_LOCK_TRY,
+	NXGE_CTLOPS_FREE_LOCK,
+	NXGE_CTLOPS_SET_SHARED_REG_LOCK,
+	NXGE_CTLOPS_CLEAR_BIT_SHARED_REG,
+	NXGE_CTLOPS_CLEAR_BIT_SHARED_REG_UL,
+	NXGE_CTLOPS_END
+} nxge_ctl_enum_t;
+
+/* 12 bits are available */
+#define	COMMON_CFG_VALID	0x01
+#define	COMMON_CFG_BUSY	0x02
+#define	COMMON_INIT_START	0x04
+#define	COMMON_INIT_DONE	0x08
+#define	COMMON_TCAM_BUSY	0x10
+#define	COMMON_VLAN_BUSY	0x20
+
+#define	NXGE_SR_FUNC_BUSY_SHIFT	0x8
+#define	NXGE_SR_FUNC_BUSY_MASK	0xf00
+
+
+#define	COMMON_TXDMA_CFG	1
+#define	COMMON_RXDMA_CFG	2
+#define	COMMON_RXDMA_GRP_CFG	4
+#define	COMMON_CLASS_CFG	8
+#define	COMMON_QUICK_CFG	0x10
+
+nxge_status_t nxge_intr_mask_mgmt(p_nxge_t nxgep);
+void nxge_virint_regs_dump(p_nxge_t nxgep);
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif	/* _SYS_NXGE_NXGE_VIRTUAL_H */
diff --git a/drivers/net/nxge/include/nxge_zcp.h b/drivers/net/nxge/include/nxge_zcp.h
new file mode 100644
index 0000000..9bc3edc
--- /dev/null
+++ b/drivers/net/nxge/include/nxge_zcp.h
@@ -0,0 +1,86 @@
+/*
+ * nxge_zcp.h	Neptune Zero Copy interface
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef	_SYS_NXGE_NXGE_ZCP_H
+#define	_SYS_NXGE_NXGE_ZCP_H
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#include <nxge_zcp_hw.h>
+#include <npi_zcp.h>
+
+typedef	struct _zcp_errlog {
+	zcp_state_machine_t	state_mach;
+} zcp_errlog_t, *p_zcp_errlog_t;
+
+typedef struct _nxge_zcp_stats_t {
+	uint32_t 		errors;
+	uint32_t 		inits;
+	uint32_t 		rrfifo_underrun;
+	uint32_t 		rrfifo_overrun;
+	uint32_t 		rspfifo_uncorr_err;
+	uint32_t 		buffer_overflow;
+	uint32_t 		stat_tbl_perr;
+	uint32_t 		dyn_tbl_perr;
+	uint32_t 		buf_tbl_perr;
+	uint32_t 		tt_program_err;
+	uint32_t 		rsp_tt_index_err;
+	uint32_t 		slv_tt_index_err;
+	uint32_t 		zcp_tt_index_err;
+	uint32_t 		zcp_access_fail;
+	uint32_t 		cfifo_ecc;
+	zcp_errlog_t		errlog;
+} nxge_zcp_stats_t, *p_nxge_zcp_stats_t;
+
+typedef	struct _nxge_zcp {
+	uint32_t		config;
+	uint32_t		iconfig;
+	nxge_zcp_stats_t	*stat;
+} nxge_zcp_t;
+
+nxge_status_t nxge_zcp_init(p_nxge_t nxgep);
+void nxge_zcp_inject_err(p_nxge_t nxgep, uint32_t err_id);
+nxge_status_t nxge_zcp_fatal_err_recover(p_nxge_t nxgep);
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif	/* _SYS_NXGE_NXGE_ZCP_H */
diff --git a/drivers/net/nxge/include/nxge_zcp_hw.h b/drivers/net/nxge/include/nxge_zcp_hw.h
new file mode 100644
index 0000000..e1d22b6
--- /dev/null
+++ b/drivers/net/nxge/include/nxge_zcp_hw.h
@@ -0,0 +1,782 @@
+/*
+ * nxge_zcp_hw.h	Neptune Zero Copy HW register offsets
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef	_SYS_NXGE_NXGE_ZCP_HW_H
+#define	_SYS_NXGE_NXGE_ZCP_HW_H
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#include <nxge_defs.h>
+
+/*
+ * Neptune Zerocopy Hardware definitions
+ * Updated to reflect PRM-0.8.
+ */
+
+#define	ZCP_CONFIG_REG		(FZC_ZCP + 0x00000)
+#define	ZCP_INT_STAT_REG	(FZC_ZCP + 0x00008)
+#define	ZCP_INT_STAT_TEST_REG	(FZC_ZCP + 0x00108)
+#define	ZCP_INT_MASK_REG	(FZC_ZCP + 0x00010)
+
+#define	ZCP_BAM4_RE_CTL_REG 	(FZC_ZCP + 0x00018)
+#define	ZCP_BAM8_RE_CTL_REG 	(FZC_ZCP + 0x00020)
+#define	ZCP_BAM16_RE_CTL_REG 	(FZC_ZCP + 0x00028)
+#define	ZCP_BAM32_RE_CTL_REG 	(FZC_ZCP + 0x00030)
+
+#define	ZCP_DST4_RE_CTL_REG 	(FZC_ZCP + 0x00038)
+#define	ZCP_DST8_RE_CTL_REG 	(FZC_ZCP + 0x00040)
+#define	ZCP_DST16_RE_CTL_REG 	(FZC_ZCP + 0x00048)
+#define	ZCP_DST32_RE_CTL_REG 	(FZC_ZCP + 0x00050)
+
+#define	ZCP_RAM_DATA_REG	(FZC_ZCP + 0x00058)
+#define	ZCP_RAM_DATA0_REG	(FZC_ZCP + 0x00058)
+#define	ZCP_RAM_DATA1_REG	(FZC_ZCP + 0x00060)
+#define	ZCP_RAM_DATA2_REG	(FZC_ZCP + 0x00068)
+#define	ZCP_RAM_DATA3_REG	(FZC_ZCP + 0x00070)
+#define	ZCP_RAM_DATA4_REG	(FZC_ZCP + 0x00078)
+#define	ZCP_RAM_BE_REG		(FZC_ZCP + 0x00080)
+#define	ZCP_RAM_ACC_REG		(FZC_ZCP + 0x00088)
+
+#define	ZCP_TRAINING_VECTOR_REG	(FZC_ZCP + 0x000C0)
+#define	ZCP_STATE_MACHINE_REG	(FZC_ZCP + 0x000C8)
+#define	ZCP_CHK_BIT_DATA_REG	(FZC_ZCP + 0x00090)
+#define	ZCP_RESET_CFIFO_REG	(FZC_ZCP + 0x00098)
+#define	ZCP_RESET_CFIFO_MASK	0x0F
+
+#define	ZCP_CFIFIO_RESET_WAIT		10
+#define	ZCP_P0_P1_CFIFO_DEPTH		2048
+#define	ZCP_P2_P3_CFIFO_DEPTH		1024
+#define	ZCP_NIU_CFIFO_DEPTH		1024
+
+typedef union _zcp_reset_cfifo {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t rsrvd:28;
+			uint32_t reset_cfifo3:1;
+			uint32_t reset_cfifo2:1;
+			uint32_t reset_cfifo1:1;
+			uint32_t reset_cfifo0:1;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t reset_cfifo0:1;
+			uint32_t reset_cfifo1:1;
+			uint32_t reset_cfifo2:1;
+			uint32_t reset_cfifo3:1;
+			uint32_t rsrvd:28;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} zcp_reset_cfifo_t, *p_zcp_reset_cfifo_t;
+
+#define	ZCP_CFIFO_ECC_PORT0_REG	(FZC_ZCP + 0x000A0)
+#define	ZCP_CFIFO_ECC_PORT1_REG	(FZC_ZCP + 0x000A8)
+#define	ZCP_CFIFO_ECC_PORT2_REG	(FZC_ZCP + 0x000B0)
+#define	ZCP_CFIFO_ECC_PORT3_REG	(FZC_ZCP + 0x000B8)
+
+/* NOTE: Same as RX_LOG_PAGE_HDL */
+#define	ZCP_PAGE_HDL_REG	(FZC_DMC + 0x20038)
+
+/* Data Structures */
+
+typedef union zcp_config_reg_u {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t rsvd:7;
+			uint32_t mode_32_bit:1;
+			uint32_t debug_sel:8;
+			uint32_t rdma_th:11;
+			uint32_t ecc_chk_dis:1;
+			uint32_t par_chk_dis:1;
+			uint32_t dis_buf_rn:1;
+			uint32_t dis_buf_rq_if:1;
+			uint32_t zc_enable:1;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t zc_enable:1;
+			uint32_t dis_buf_rq_if:1;
+			uint32_t dis_buf_rn:1;
+			uint32_t par_chk_dis:1;
+			uint32_t ecc_chk_dis:1;
+			uint32_t rdma_th:11;
+			uint32_t debug_sel:8;
+			uint32_t mode_32_bit:1;
+			uint32_t rsvd:7;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} zcp_config_reg_t, *zcp_config_reg_pt;
+
+#define	ZCP_DEBUG_SEL_BITS	0xFF
+#define	ZCP_DEBUG_SEL_SHIFT	16
+#define	ZCP_DEBUG_SEL_MASK	(ZCP_DEBUG_SEL_BITS << ZCP_DEBUG_SEL_SHIFT)
+#define	RDMA_TH_BITS		0x7FF
+#define	RDMA_TH_SHIFT		5
+#define	RDMA_TH_MASK		(RDMA_TH_BITS << RDMA_TH_SHIFT)
+#define	ECC_CHK_DIS		(1 << 4)
+#define	PAR_CHK_DIS		(1 << 3)
+#define	DIS_BUFF_RN		(1 << 2)
+#define	DIS_BUFF_RQ_IF		(1 << 1)
+#define	ZC_ENABLE		(1 << 0)
+
+typedef union zcp_int_stat_reg_u {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t rsvd:16;
+			uint32_t rrfifo_urun:1;
+			uint32_t rrfifo_orun:1;
+			uint32_t rsvd1:1;
+			uint32_t rspfifo_uc_err:1;
+			uint32_t buf_overflow:1;
+			uint32_t stat_tbl_perr:1;
+			uint32_t dyn_tbl_perr:1;
+			uint32_t buf_tbl_perr:1;
+			uint32_t tt_tbl_perr:1;
+			uint32_t rsp_tt_index_err:1;
+			uint32_t slv_tt_index_err:1;
+			uint32_t zcp_tt_index_err:1;
+			uint32_t cfifo_ecc3:1;
+			uint32_t cfifo_ecc2:1;
+			uint32_t cfifo_ecc1:1;
+			uint32_t cfifo_ecc0:1;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t cfifo_ecc0:1;
+			uint32_t cfifo_ecc1:1;
+			uint32_t cfifo_ecc2:1;
+			uint32_t cfifo_ecc3:1;
+			uint32_t zcp_tt_index_err:1;
+			uint32_t slv_tt_index_err:1;
+			uint32_t rsp_tt_index_err:1;
+			uint32_t tt_tbl_perr:1;
+			uint32_t buf_tbl_perr:1;
+			uint32_t dyn_tbl_perr:1;
+			uint32_t stat_tbl_perr:1;
+			uint32_t buf_overflow:1;
+			uint32_t rspfifo_uc_err:1;
+			uint32_t rsvd1:1;
+			uint32_t rrfifo_orun:1;
+			uint32_t rrfifo_urun:1;
+			uint32_t rsvd:16;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} zcp_int_stat_reg_t, *zcp_int_stat_reg_pt, zcp_int_mask_reg_t,
+	*zcp_int_mask_reg_pt;
+
+#define	RRFIFO_UNDERRUN		(1 << 15)
+#define	RRFIFO_OVERRUN		(1 << 14)
+#define	RSPFIFO_UNCORR_ERR	(1 << 12)
+#define	BUFFER_OVERFLOW		(1 << 11)
+#define	STAT_TBL_PERR		(1 << 10)
+#define	BUF_DYN_TBL_PERR	(1 << 9)
+#define	BUF_TBL_PERR		(1 << 8)
+#define	TT_PROGRAM_ERR		(1 << 7)
+#define	RSP_TT_INDEX_ERR	(1 << 6)
+#define	SLV_TT_INDEX_ERR	(1 << 5)
+#define	ZCP_TT_INDEX_ERR	(1 << 4)
+#define	CFIFO_ECC3		(1 << 3)
+#define	CFIFO_ECC0		(1 << 0)
+#define	CFIFO_ECC2		(1 << 2)
+#define	CFIFO_ECC1		(1 << 1)
+
+typedef union zcp_bam_region_reg_u {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t loj:1;
+			uint32_t range_chk_en:1;
+			uint32_t last_zcfid:10;
+			uint32_t first_zcfid:10;
+			uint32_t offset:10;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t offset:10;
+			uint32_t first_zcfid:10;
+			uint32_t last_zcfid:10;
+			uint32_t range_chk_en:1;
+			uint32_t loj:1;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} zcp_bam_region_reg_t, *zcp_bam_region_reg_pt;
+
+typedef union zcp_dst_region_reg_u {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t rsvd:22;
+			uint32_t ds_offset:10;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t rsvd:22;
+			uint32_t ds_offset:10;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} zcp_dst_region_reg_t, *zcp_dst_region_reg_pt;
+
+typedef	enum tbuf_size_e {
+	TBUF_4K		= 0,
+	TBUF_8K,
+	TBUF_16K,
+	TBUF_32K,
+	TBUF_64K,
+	TBUF_128K,
+	TBUF_256K,
+	TBUF_512K,
+	TBUF_1M,
+	TBUF_2M,
+	TBUF_4M,
+	TBUF_8M
+} tbuf_size_t;
+
+typedef	enum tbuf_num_e {
+	TBUF_NUM_4	= 0,
+	TBUF_NUM_8,
+	TBUF_NUM_16,
+	TBUF_NUM_32
+} tbuf_num_t;
+
+typedef	enum tmode_e {
+	TMODE_BASIC		= 0,
+	TMODE_AUTO_UNMAP	= 1,
+	TMODE_AUTO_ADV		= 3
+} tmode_t;
+
+typedef	struct tte_sflow_attr_s {
+	union {
+		uint64_t value;
+		struct {
+#if defined(_BIG_ENDIAN)
+			uint32_t hdw;
+#endif
+			struct {
+#if defined(_BIT_FIELDS_HTOL)
+				uint32_t ulp_end:18;
+				uint32_t num_buf:2;
+				uint32_t buf_size:4;
+				uint32_t rdc_tbl_offset:8;
+#elif defined(_BIT_FIELDS_LTOH)
+				uint32_t rdc_tbl_offset:8;
+				uint32_t buf_size:4;
+				uint32_t num_buf:2;
+				uint32_t ulp_end:18;
+#endif
+			} ldw;
+#if !defined(_BIG_ENDIAN)
+			uint32_t hdw;
+#endif
+		} bits;
+	} qw0;
+
+	union {
+		uint64_t value;
+		struct {
+#if defined(_BIG_ENDIAN)
+			uint32_t hdw;
+#endif
+			struct {
+#if defined(_BIT_FIELDS_HTOL)
+				uint32_t ring_base:12;
+				uint32_t skip:1;
+				uint32_t rsvd:1;
+				uint32_t tmode:2;
+				uint32_t unmap_all_en:1;
+				uint32_t ulp_end_en:1;
+				uint32_t ulp_end:14;
+#elif defined(_BIT_FIELDS_LTOH)
+				uint32_t ulp_end:14;
+				uint32_t ulp_end_en:1;
+				uint32_t unmap_all_en:1;
+				uint32_t tmode:2;
+				uint32_t rsvd:1;
+				uint32_t skip:1;
+				uint32_t ring_base:12;
+#endif
+			} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		} bits;
+	} qw1;
+
+	union {
+		uint64_t value;
+		struct {
+#if defined(_BIG_ENDIAN)
+			uint32_t hdw;
+#endif
+			struct {
+#if defined(_BIT_FIELDS_HTOL)
+				uint32_t busy:1;
+				uint32_t ring_size:4;
+				uint32_t ring_base:27;
+#elif defined(_BIT_FIELDS_LTOH)
+				uint32_t ring_base:27;
+				uint32_t ring_size:4;
+				uint32_t busy:1;
+#endif
+			} ldw;
+#if !defined(_BIG_ENDIAN)
+			uint32_t hdw;
+#endif
+		} bits;
+	} qw2;
+
+	union {
+		uint64_t value;
+		struct {
+#if defined(_BIG_ENDIAN)
+			uint32_t hdw;
+#endif
+			struct {
+#if defined(_BIT_FIELDS_HTOL)
+				uint32_t rsvd:16;
+				uint32_t toq:16;
+#elif defined(_BIT_FIELDS_LTOH)
+				uint32_t toq:16;
+				uint32_t rsvd:16;
+#endif
+			} ldw;
+#if !defined(_BIG_ENDIAN)
+			uint32_t hdw;
+#endif
+		} bits;
+	} qw3;
+
+	union {
+		uint64_t value;
+		struct {
+#if defined(_BIG_ENDIAN)
+			uint32_t hdw;
+#endif
+			struct {
+#if defined(_BIT_FIELDS_HTOL)
+				uint32_t rsvd:28;
+				uint32_t dat4:4;
+#elif defined(_BIT_FIELDS_LTOH)
+				uint32_t dat4:4;
+				uint32_t rsvd:28;
+#endif
+			} ldw;
+#if !defined(_BIG_ENDIAN)
+			uint32_t hdw;
+#endif
+		} bits;
+	} qw4;
+
+} tte_sflow_attr_t, *tte_sflow_attr_pt;
+
+#define	TTE_RDC_TBL_SFLOW_BITS_EN	0x0001
+#define	TTE_BUF_SIZE_BITS_EN		0x0002
+#define	TTE_NUM_BUF_BITS_EN		0x0002
+#define	TTE_ULP_END_BITS_EN		0x003E
+#define	TTE_ULP_END_EN_BITS_EN		0x0020
+#define	TTE_UNMAP_ALL_BITS_EN		0x0020
+#define	TTE_TMODE_BITS_EN		0x0040
+#define	TTE_SKIP_BITS_EN		0x0040
+#define	TTE_RING_BASE_ADDR_BITS_EN	0x0FC0
+#define	TTE_RING_SIZE_BITS_EN		0x0800
+#define	TTE_BUSY_BITS_EN		0x0800
+#define	TTE_TOQ_BITS_EN			0x3000
+
+#define	TTE_MAPPED_IN_BITS_EN		0x0000F
+#define	TTE_ANCHOR_SEQ_BITS_EN		0x000F0
+#define	TTE_ANCHOR_OFFSET_BITS_EN	0x00700
+#define	TTE_ANCHOR_BUFFER_BITS_EN	0x00800
+#define	TTE_ANCHOR_BUF_FLAG_BITS_EN	0x00800
+#define	TTE_UNMAP_ON_LEFT_BITS_EN	0x00800
+#define	TTE_ULP_END_REACHED_BITS_EN	0x00800
+#define	TTE_ERR_STAT_BITS_EN		0x01000
+#define	TTE_WR_PTR_BITS_EN		0x01000
+#define	TTE_HOQ_BITS_EN			0x0E000
+#define	TTE_PREFETCH_ON_BITS_EN		0x08000
+
+typedef	enum tring_size_e {
+	TRING_SIZE_8		= 0,
+	TRING_SIZE_16,
+	TRING_SIZE_32,
+	TRING_SIZE_64,
+	TRING_SIZE_128,
+	TRING_SIZE_256,
+	TRING_SIZE_512,
+	TRING_SIZE_1K,
+	TRING_SIZE_2K,
+	TRING_SIZE_4K,
+	TRING_SIZE_8K,
+	TRING_SIZE_16K,
+	TRING_SIZE_32K
+} tring_size_t;
+
+typedef struct tte_dflow_attr_s {
+	union {
+		uint64_t value;
+		struct {
+#if defined(_BIG_ENDIAN)
+			uint32_t hdw;
+#endif
+			struct {
+#if defined(_BIT_FIELDS_HTOL)
+				uint32_t mapped_in;
+#elif defined(_BIT_FIELDS_LTOH)
+				uint32_t mapped_in;
+#endif
+			} ldw;
+#if !defined(_BIG_ENDIAN)
+			uint32_t hdw;
+#endif
+		} bits;
+	} qw0;
+
+	union {
+		uint64_t value;
+		struct {
+#if defined(_BIG_ENDIAN)
+			uint32_t hdw;
+#endif
+			struct {
+#if defined(_BIT_FIELDS_HTOL)
+				uint32_t anchor_seq;
+#elif defined(_BIT_FIELDS_LTOH)
+				uint32_t anchor_seq;
+#endif
+			} ldw;
+#if !defined(_BIG_ENDIAN)
+			uint32_t hdw;
+#endif
+		} bits;
+	} qw1;
+
+	union {
+		uint64_t value;
+		struct {
+#if defined(_BIG_ENDIAN)
+			uint32_t hdw;
+#endif
+			struct {
+#if defined(_BIT_FIELDS_HTOL)
+				uint32_t ulp_end_reached;
+				uint32_t unmap_on_left;
+				uint32_t anchor_buf_flag;
+				uint32_t anchor_buf:5;
+				uint32_t anchor_offset:24;
+#elif defined(_BIT_FIELDS_LTOH)
+				uint32_t anchor_offset:24;
+				uint32_t anchor_buf:5;
+				uint32_t anchor_buf_flag;
+				uint32_t unmap_on_left;
+				uint32_t ulp_end_reached;
+#endif
+			} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		} bits;
+	} qw2;
+
+	union {
+		uint64_t value;
+		struct {
+#if defined(_BIG_ENDIAN)
+			uint32_t hdw;
+#endif
+			struct {
+#if defined(_BIT_FIELDS_HTOL)
+				uint32_t rsvd1:1;
+				uint32_t prefetch_on:1;
+				uint32_t hoq:16;
+				uint32_t rsvd:6;
+				uint32_t wr_ptr:6;
+				uint32_t err_stat:2;
+#elif defined(_BIT_FIELDS_LTOH)
+				uint32_t err_stat:2;
+				uint32_t wr_ptr:6;
+				uint32_t rsvd:6;
+				uint32_t hoq:16;
+				uint32_t prefetch_on:1;
+				uint32_t rsvd1:1;
+#endif
+			} ldw;
+#if !defined(_BIG_ENDIAN)
+			uint32_t hdw;
+#endif
+		} bits;
+	} qw3;
+
+	union {
+		uint64_t value;
+		struct {
+#if defined(_BIG_ENDIAN)
+			uint32_t hdw;
+#endif
+			struct {
+#if defined(_BIT_FIELDS_HTOL)
+				uint32_t rsvd:28;
+				uint32_t dat4:4;
+#elif defined(_BIT_FIELDS_LTOH)
+				uint32_t dat4:4;
+				uint32_t rsvd:28;
+#endif
+			} ldw;
+#if !defined(_BIG_ENDIAN)
+			uint32_t hdw;
+#endif
+		} bits;
+	} qw4;
+
+} tte_dflow_attr_t, *tte_dflow_attr_pt;
+
+#define	MAX_BAM_BANKS	8
+
+typedef	struct zcp_ram_unit_s {
+	uint32_t	w0;
+	uint32_t	w1;
+	uint32_t	w2;
+	uint32_t	w3;
+	uint32_t	w4;
+} zcp_ram_unit_t;
+
+typedef	enum dmaw_type_e {
+	DMAW_NO_CROSS_BUF	= 0,
+	DMAW_IP_CROSS_BUF_2,
+	DMAW_IP_CROSS_BUF_3,
+	DMAW_IP_CROSS_BUF_4
+} dmaw_type_t;
+
+typedef union zcp_ram_data_u {
+	tte_sflow_attr_t sentry;
+	tte_dflow_attr_t dentry;
+} zcp_ram_data_t, *zcp_ram_data_pt;
+
+typedef union zcp_ram_access_u {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t busy:1;
+			uint32_t rdwr:1;
+			uint32_t rsvd:1;
+			uint32_t zcfid:12;
+			uint32_t ram_sel:5;
+			uint32_t cfifo:12;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t cfifo:12;
+			uint32_t ram_sel:5;
+			uint32_t zcfid:12;
+			uint32_t rsvd:1;
+			uint32_t rdwr:1;
+			uint32_t busy:1;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} zcp_ram_access_t, *zcp_ram_access_pt;
+
+#define	ZCP_RAM_WR		0
+#define	ZCP_RAM_RD		1
+#define	ZCP_RAM_SEL_BAM0	0
+#define	ZCP_RAM_SEL_BAM1	0x1
+#define	ZCP_RAM_SEL_BAM2	0x2
+#define	ZCP_RAM_SEL_BAM3	0x3
+#define	ZCP_RAM_SEL_BAM4	0x4
+#define	ZCP_RAM_SEL_BAM5	0x5
+#define	ZCP_RAM_SEL_BAM6	0x6
+#define	ZCP_RAM_SEL_BAM7	0x7
+#define	ZCP_RAM_SEL_TT_STATIC	0x8
+#define	ZCP_RAM_SEL_TT_DYNAMIC	0x9
+#define	ZCP_RAM_SEL_CFIFO0	0x10
+#define	ZCP_RAM_SEL_CFIFO1	0x11
+#define	ZCP_RAM_SEL_CFIFO2	0x12
+#define	ZCP_RAM_SEL_CFIFO3	0x13
+
+typedef union zcp_ram_benable_u {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t rsvd:15;
+			uint32_t be:17;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t be:17;
+			uint32_t rsvd:15;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} zcp_ram_benable_t, *zcp_ram_benable_pt;
+
+typedef union zcp_training_vector_u {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t train_vec;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t train_vec;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} zcp_training_vector_t, *zcp_training_vector_pt;
+
+typedef union zcp_state_machine_u {
+	uint64_t value;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+			uint32_t state;
+#elif defined(_BIT_FIELDS_LTOH)
+			uint32_t state;
+#endif
+		} ldw;
+#if !defined(_BIG_ENDIAN)
+		uint32_t hdw;
+#endif
+	} bits;
+} zcp_state_machine_t, *zcp_state_machine_pt;
+
+typedef	struct zcp_hdr_s {
+	uint16_t	zflowid;
+	uint16_t	tcp_hdr_len;
+	uint16_t	tcp_payld_len;
+	uint16_t	head_of_que;
+	uint32_t	first_b_offset;
+	boolean_t	reach_buf_end;
+	dmaw_type_t	dmaw_type;
+	uint8_t		win_buf_offset;
+} zcp_hdr_t;
+
+typedef	union _zcp_ecc_ctrl {
+	uint64_t value;
+
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t	w1;
+#endif
+		struct {
+#if defined(_BIT_FIELDS_HTOL)
+		uint32_t dis_dbl	: 1;
+		uint32_t res3		: 13;
+		uint32_t cor_dbl	: 1;
+		uint32_t cor_sng	: 1;
+		uint32_t res2		: 5;
+		uint32_t cor_all	: 1;
+		uint32_t res1		: 7;
+		uint32_t cor_lst	: 1;
+		uint32_t cor_snd	: 1;
+		uint32_t cor_fst	: 1;
+#elif defined(_BIT_FIELDS_LTOH)
+		uint32_t cor_fst	: 1;
+		uint32_t cor_snd	: 1;
+		uint32_t cor_lst	: 1;
+		uint32_t res1		: 7;
+		uint32_t cor_all	: 1;
+		uint32_t res2		: 5;
+		uint32_t cor_sng	: 1;
+		uint32_t cor_dbl	: 1;
+		uint32_t res3		: 13;
+		uint32_t dis_dbl	: 1;
+#else
+#error	one of _BIT_FIELDS_HTOL or _BIT_FIELDS_LTOH must be defined
+#endif
+	} w0;
+
+#if !defined(_BIG_ENDIAN)
+		uint32_t	w1;
+#endif
+	} bits;
+} zcp_ecc_ctrl_t;
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif	/* _SYS_NXGE_NXGE_ZCP_HW_H */
diff --git a/drivers/net/nxge/npi/npi.c b/drivers/net/nxge/npi/npi.c
new file mode 100644
index 0000000..ca88eea
--- /dev/null
+++ b/drivers/net/nxge/npi/npi.c
@@ -0,0 +1,104 @@
+/*
+ * npi.c	Neptune HW API functions
+ *
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#include <npi.h>
+
+uint64_t npi_debug_level;
+nxge_os_mutex_t npidebuglock;
+int npi_debug_init = 0;
+
+void
+npi_rtrace_buf_init(rtrace_t *rt)
+{
+	int i;
+
+	rt->next_idx = 0;
+	rt->last_idx = MAX_RTRACE_ENTRIES - 1;
+	rt->wrapped = B_FALSE;
+	for (i = 0; i < MAX_RTRACE_ENTRIES; i++) {
+		rt->buf[i].ctl_addr = TRACE_CTL_INVALID;
+		rt->buf[i].val_l32 = 0;
+		rt->buf[i].val_h32 = 0;
+	}
+}
+
+void
+npi_rtrace_update(npi_handle_t handle, boolean_t wr, rtrace_t *rt,
+		    uint32_t addr, uint64_t val)
+{
+	int idx;
+	idx = rt->next_idx;
+	if (wr == B_TRUE)
+		rt->buf[idx].ctl_addr = (addr & TRACE_ADDR_MASK)
+						| TRACE_CTL_WR;
+	else
+		rt->buf[idx].ctl_addr = (addr & TRACE_ADDR_MASK);
+	rt->buf[idx].ctl_addr |= (((handle.function.function
+				<< TRACE_FUNC_SHIFT) & TRACE_FUNC_MASK) |
+				((handle.function.instance
+				<< TRACE_INST_SHIFT) & TRACE_INST_MASK));
+	rt->buf[idx].val_l32 = val & 0xFFFFFFFF;
+	rt->buf[idx].val_h32 = (val >> 32) & 0xFFFFFFFF;
+	rt->next_idx++;
+	if (rt->next_idx > rt->last_idx) {
+		rt->next_idx = 0;
+		rt->wrapped = B_TRUE;
+	}
+}
+
+
+
+void
+npi_debug_msg(npi_handle_function_t function, uint64_t level, char *fmt, ...)
+{
+	va_list arglist;
+	static char msg_buf[MSG_BUF_SIZE];
+
+	if (npi_debug_init == 0) {
+		MUTEX_INIT(&npidebuglock, NULL, MUTEX_DRIVER, NULL);
+		npi_debug_init = 1;
+	}
+
+	MUTEX_ENTER(&npidebuglock);
+	va_start(arglist, fmt);
+	vsprintf(msg_buf, fmt, arglist);
+	va_end(arglist);
+	printk(KERN_ERR "npi[%d] %s \n", function.function, msg_buf);
+	MUTEX_EXIT(&npidebuglock);
+}
diff --git a/drivers/net/nxge/npi/npi.h b/drivers/net/nxge/npi/npi.h
new file mode 100644
index 0000000..7572224
--- /dev/null
+++ b/drivers/net/nxge/npi/npi.h
@@ -0,0 +1,277 @@
+/*
+ * npi.h	Neptune HW API functions
+ *
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef _NPI_H
+#define	_NPI_H
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#include <nxge_common_impl.h>
+
+typedef	uint32_t			npi_status_t;
+
+/* Common Block ID */
+
+#define	MAC_BLK_ID			0x1
+#define	TXMAC_BLK_ID			0x2
+#define	RXMAC_BLK_ID			0x3
+#define	MIF_BLK_ID			0x4
+#define	IPP_BLK_ID			0x5
+#define	TXC_BLK_ID			0x6
+#define	TXDMA_BLK_ID			0x7
+#define	RXDMA_BLK_ID			0x8
+#define	ZCP_BLK_ID			0x9
+#define	ESPC_BLK_ID			0xa
+#define	FFLP_BLK_ID			0xb
+#define	PHY_BLK_ID			0xc
+#define	ETHER_SERDES_BLK_ID		0xd
+#define	PCIE_SERDES_BLK_ID		0xe
+#define	VIR_BLK_ID			0xf
+
+/* Common HW error code */
+/* HW unable to exit from reset state. */
+#define	RESET_FAILED			0x81
+
+/* Write operation failed on indirect write. */
+#define	WRITE_FAILED			0x82
+/* Read operation failed on indirect read.	 */
+#define	READ_FAILED			0x83
+
+/* Error code boundary */
+
+#define	COMMON_SW_ERR_START		0x40
+#define	COMMON_SW_ERR_END		0x4f
+#define	BLK_SPEC_SW_ERR_START		0x50
+#define	BLK_SPEC_SW_ERR_END		0x7f
+#define	COMMON_HW_ERR_START		0x80
+#define	COMMON_HW_ERR_END		0x8f
+#define	BLK_SPEC_HW_ERR_START		0x90
+#define	BLK_SPEC_HW_ERR_END		0xbf
+
+#define	IS_PORT				0x00100000
+#define	IS_CHAN				0x00200000
+
+/* Common SW errors code */
+
+#define	PORT_INVALID			0x41	/* Invalid port number */
+#define	CHANNEL_INVALID			0x42	/* Invalid dma channel number */
+#define	OPCODE_INVALID			0x43	/* Invalid opcode */
+#define	REGISTER_INVALID		0x44	/* Invalid register number */
+#define	COUNTER_INVALID			0x45	/* Invalid counter number */
+#define	CONFIG_INVALID			0x46	/* Invalid config input */
+#define	LOGICAL_PAGE_INVALID		0x47	/* Invalid logical page # */
+#define	VLAN_INVALID			0x48	/* Invalid Vlan ID */
+#define	RDC_TAB_INVALID			0x49	/* Invalid RDC Group Number */
+#define	LOCATION_INVALID		0x4a	/* Invalid Entry Location */
+
+#define	NPI_SUCCESS			0		/* Operation succeed */
+#define	NPI_FAILURE			0x80000000	/* Operation failed */
+
+#define	NPI_CNT_CLR_VAL			0
+
+/*
+ * Block identifier starts at bit 8.
+ */
+#define	NPI_BLOCK_ID_SHIFT		8
+
+/*
+ * Port, channel and misc. information starts at bit 12.
+ */
+#define	NPI_PORT_CHAN_SHIFT			12
+
+/*
+ * Software Block specific error codes start at 0x50.
+ */
+#define	NPI_BK_ERROR_START		0x50
+
+/*
+ * Hardware block specific error codes start at 0x90.
+ */
+#define	NPI_BK_HW_ER_START		0x90
+
+/* Structures for register tracing */
+
+typedef struct _rt_buf {
+	uint32_t	ctl_addr;
+	uint32_t	val_l32;
+	uint32_t	val_h32;
+} rt_buf_t;
+
+/*
+ * Control Address field format
+ *
+ * Bit 0 - 23: Address
+ * Bit 24 - 25: Function Number
+ * Bit 26 - 29: Instance Number
+ * Bit 30: Read/Write Direction bit
+ * Bit 31: Invalid bit
+ */
+
+#define	MAX_RTRACE_ENTRIES	1024
+#define	MAX_RTRACE_IOC_ENTRIES	64
+#define	TRACE_ADDR_MASK		0x00FFFFFF
+#define	TRACE_FUNC_MASK		0x03000000
+#define	TRACE_INST_MASK		0x3C000000
+#define	TRACE_CTL_WR		0x40000000
+#define	TRACE_CTL_INVALID	0x80000000
+#define	TRACE_FUNC_SHIFT	24
+#define	TRACE_INST_SHIFT	26
+#define	MSG_BUF_SIZE		1024
+
+
+typedef struct _rtrace {
+	uint16_t	next_idx;
+	uint16_t	last_idx;
+	boolean_t	wrapped;
+	rt_buf_t	buf[MAX_RTRACE_ENTRIES];
+} rtrace_t;
+
+typedef struct _err_inject {
+	uint8_t		blk_id;
+	uint8_t		chan;
+	uint32_t	err_id;
+	uint32_t	control;
+} err_inject_t;
+
+/* Configuration options */
+typedef enum config_op {
+	DISABLE = 0,
+	ENABLE,
+	INIT
+} config_op_t;
+
+/* I/O options */
+typedef enum io_op {
+	OP_SET = 0,
+	OP_GET,
+	OP_UPDATE,
+	OP_CLEAR
+} io_op_t;
+
+/* Counter options */
+typedef enum counter_op {
+	SNAP_STICKY = 0,
+	SNAP_ACCUMULATE,
+	CLEAR
+} counter_op_t;
+
+/* NPI attribute */
+typedef struct _npi_attr_t {
+	uint32_t type;
+	uint32_t idata[16];
+	uint32_t odata[16];
+} npi_attr_t;
+
+/* NPI Handle */
+typedef	struct	_npi_handle_function {
+	uint16_t		instance;
+	uint16_t		function;
+} npi_handle_function_t;
+
+/* NPI Handle */
+typedef	struct	_npi_handle {
+	npi_reg_handle_t	regh;
+	npi_reg_ptr_t		regp;
+	boolean_t		is_vraddr; /* virtualization region address */
+	npi_handle_function_t	function;
+	void * nxgep;
+} npi_handle_t;
+
+/* NPI Counter */
+typedef struct _npi_counter_t {
+	uint32_t id;
+	char *name;
+	uint32_t val;
+} npi_counter_t;
+
+/*
+ * Commmon definitions for NPI RXDMA and TXDMA functions.
+ */
+typedef struct _dma_log_page {
+	uint64_t		mask;
+	uint64_t		value;
+	uint64_t		reloc;
+	uint8_t			page_num;
+	boolean_t		valid;
+	uint8_t			func_num;
+	uint8_t pad[5];
+} dma_log_page_t, *p_dma_log_page_t;
+
+/*
+ * VPD information.
+ */
+#define	NXGE_VPD_MOD_LEN	32
+#define	NXGE_VPD_BD_MOD_LEN	16
+#define	NXGE_VPD_PHY_LEN	5
+#define	NXGE_VPD_VER_LEN	60
+typedef struct _npi_vpd_info_t {
+	uint8_t		mac_addr[ETHERADDRL];
+	uint8_t		num_macs;
+	char		model[NXGE_VPD_MOD_LEN];
+	char		bd_model[NXGE_VPD_BD_MOD_LEN];
+	char		phy_type[NXGE_VPD_PHY_LEN];
+	char		ver[NXGE_VPD_VER_LEN];
+	boolean_t	ver_valid;
+} npi_vpd_info_t, *p_npi_vpd_info_t;
+
+extern	rtrace_t npi_rtracebuf;
+void npi_rtrace_buf_init(rtrace_t *rt);
+void npi_rtrace_update(npi_handle_t handle, boolean_t wr, rtrace_t *rt,
+			uint32_t addr, uint64_t val);
+void npi_rtrace_buf_init(rtrace_t *rt);
+
+void npi_debug_msg(npi_handle_function_t function, uint64_t level,
+	char *fmt, ...);
+
+#ifdef	NPI_DEBUG
+#define	NPI_DEBUG_MSG(params) npi_debug_msg params
+#else
+#define	NPI_DEBUG_MSG(params)
+#endif
+
+#define	NPI_ERROR_MSG(params) npi_debug_msg params
+#define	NPI_REG_DUMP_MSG(params) npi_debug_msg params
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif	/* _NPI_H */
diff --git a/drivers/net/nxge/npi/npi_espc.c b/drivers/net/nxge/npi/npi_espc.c
new file mode 100644
index 0000000..ca9b976
--- /dev/null
+++ b/drivers/net/nxge/npi/npi_espc.c
@@ -0,0 +1,651 @@
+/*
+ * npi_espc.c	Neptune  SPROM HW API functions
+ *
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#include <npi_espc.h>
+#include <nxge_espc.h>
+
+static int npi_vpd_read_prop(npi_handle_t handle, uint32_t ep,
+			     const char *prop, int len, char *val);
+
+npi_status_t
+npi_espc_pio_enable(npi_handle_t handle)
+{
+	NXGE_REG_WR64(handle, ESPC_REG_ADDR(ESPC_PIO_EN_REG), 0x1);
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_espc_pio_disable(npi_handle_t handle)
+{
+	NXGE_REG_WR64(handle, ESPC_PIO_EN_REG, 0);
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_espc_eeprom_entry(npi_handle_t handle, io_op_t op, uint32_t addr,
+			uint8_t *data)
+{
+	uint64_t val = 0;
+
+	if ((addr & ~EPC_EEPROM_ADDR_BITS) != 0) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_espc_eerprom_entry"
+			" Invalid input addr <0x%x>\n",
+			addr));
+		return (NPI_FAILURE | NPI_ESPC_EEPROM_ADDR_INVALID);
+	}
+
+	if (op == OP_SET) {
+		val = EPC_WRITE_INITIATE | (addr << EPC_EEPROM_ADDR_SHIFT) |
+			*data;
+		NXGE_REG_WR64(handle, ESPC_REG_ADDR(ESPC_PIO_STATUS_REG), val);
+		EPC_WAIT_RW_COMP(handle, &val, EPC_WRITE_COMPLETE);
+		if ((val & EPC_WRITE_COMPLETE) == 0) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				" npi_espc_eeprom_entry"
+				" HW Error: EEPROM_WR <0x%x>\n",
+				val));
+			return (NPI_FAILURE | NPI_ESPC_EEPROM_WRITE_FAILED);
+		}
+	} else if (op == OP_GET) {
+		val = EPC_READ_INITIATE | (addr << EPC_EEPROM_ADDR_SHIFT);
+		NXGE_REG_WR64(handle, ESPC_REG_ADDR(ESPC_PIO_STATUS_REG), val);
+		EPC_WAIT_RW_COMP(handle, &val, EPC_READ_COMPLETE);
+		if ((val & EPC_READ_COMPLETE) == 0) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				" npi_espc_eeprom_entry"
+				" HW Error: EEPROM_RD <0x%x>",
+				val));
+			return (NPI_FAILURE | NPI_ESPC_EEPROM_READ_FAILED);
+		}
+		NXGE_REG_RD64(handle, ESPC_REG_ADDR(ESPC_PIO_STATUS_REG), &val);
+		/* Workaround for synchronization issues - do a second PIO */
+		val = EPC_READ_INITIATE | (addr << EPC_EEPROM_ADDR_SHIFT);
+		NXGE_REG_WR64(handle, ESPC_REG_ADDR(ESPC_PIO_STATUS_REG), val);
+		EPC_WAIT_RW_COMP(handle, &val, EPC_READ_COMPLETE);
+		if ((val & EPC_READ_COMPLETE) == 0) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				" npi_espc_eeprom_entry"
+				" HW Error: EEPROM_RD <0x%x>",
+				val));
+			return (NPI_FAILURE | NPI_ESPC_EEPROM_READ_FAILED);
+		}
+		NXGE_REG_RD64(handle, ESPC_REG_ADDR(ESPC_PIO_STATUS_REG), &val);
+		*data = val & EPC_EEPROM_DATA_MASK;
+	} else {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_espc_eeprom_entry"
+				    " Invalid Input addr <0x%x>\n", addr));
+		return (NPI_FAILURE | NPI_ESPC_OPCODE_INVALID);
+	}
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_espc_mac_addr_get(npi_handle_t handle, uint8_t *data)
+{
+	mac_addr_0_t mac0;
+	mac_addr_1_t mac1;
+
+	NXGE_REG_RD64(handle, ESPC_MAC_ADDR_0, &mac0.value);
+	data[0] = mac0.bits.w0.byte0;
+	data[1] = mac0.bits.w0.byte1;
+	data[2] = mac0.bits.w0.byte2;
+	data[3] = mac0.bits.w0.byte3;
+
+	NXGE_REG_RD64(handle, ESPC_MAC_ADDR_1, &mac1.value);
+	data[4] = mac1.bits.w0.byte4;
+	data[5] = mac1.bits.w0.byte5;
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_espc_num_ports_get(npi_handle_t handle, uint8_t *data)
+{
+	uint64_t val = 0;
+
+	NXGE_REG_RD64(handle, ESPC_NUM_PORTS_MACS, &val);
+	val &= NUM_PORTS_MASK;
+	*data = (uint8_t)val;
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_espc_num_macs_get(npi_handle_t handle, uint8_t *data)
+{
+	uint64_t val = 0;
+
+	NXGE_REG_RD64(handle, ESPC_NUM_PORTS_MACS, &val);
+	val &= NUM_MAC_ADDRS_MASK;
+	val = (val >> NUM_MAC_ADDRS_SHIFT);
+	*data = (uint8_t)val;
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_espc_model_str_get(npi_handle_t handle, char *data)
+{
+	uint64_t val = 0;
+	uint16_t str_len;
+	int i, j;
+
+	NXGE_REG_RD64(handle, ESPC_MOD_STR_LEN, &val);
+	val &= MOD_STR_LEN_MASK;
+	str_len = (uint8_t)val;
+
+	if (str_len > MAX_MOD_STR_LEN) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				" npi_espc_model_str_get"
+				" Model string length %d exceeds max %d\n",
+				str_len, MAX_MOD_STR_LEN));
+		return (NPI_FAILURE | NPI_ESPC_STR_LEN_INVALID);
+	}
+
+	/*
+	 * Might have to reverse the order depending on how the string
+	 * is written.
+	 */
+	for (i = 0, j = 0; i < str_len; j++) {
+		NXGE_REG_RD64(handle, ESPC_MOD_STR(j), &val);
+		data[i++] = ((char *)&val)[3];
+		data[i++] = ((char *)&val)[2];
+		data[i++] = ((char *)&val)[1];
+		data[i++] = ((char *)&val)[0];
+	}
+
+	data[str_len] = '\0';
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_espc_bd_model_str_get(npi_handle_t handle, char *data)
+{
+	uint64_t val = 0;
+	uint16_t str_len;
+	int i, j;
+
+	NXGE_REG_RD64(handle, ESPC_BD_MOD_STR_LEN, &val);
+	val &= BD_MOD_STR_LEN_MASK;
+	str_len = (uint8_t)val;
+
+	if (str_len > MAX_BD_MOD_STR_LEN) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				" npi_espc_model_str_get"
+				" Board Model string length %d "
+				"exceeds max %d\n",
+				str_len, MAX_BD_MOD_STR_LEN));
+		return (NPI_FAILURE | NPI_ESPC_STR_LEN_INVALID);
+	}
+
+	/*
+	 * Might have to reverse the order depending on how the string
+	 * is written.
+	 */
+	for (i = 0, j = 0; i < str_len; j++) {
+		NXGE_REG_RD64(handle, ESPC_BD_MOD_STR(j), &val);
+		data[i++] = ((char *)&val)[3];
+		data[i++] = ((char *)&val)[2];
+		data[i++] = ((char *)&val)[1];
+		data[i++] = ((char *)&val)[0];
+	}
+
+	data[str_len] = '\0';
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_espc_phy_type_get(npi_handle_t handle, uint8_t *data)
+{
+	phy_type_t	phy;
+
+	NXGE_REG_RD64(handle, ESPC_PHY_TYPE, &phy.value);
+	data[0] = phy.bits.w0.pt0_phy_type;
+	data[1] = phy.bits.w0.pt1_phy_type;
+	data[2] = phy.bits.w0.pt2_phy_type;
+	data[3] = phy.bits.w0.pt3_phy_type;
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_espc_port_phy_type_get(npi_handle_t handle, uint8_t *data, uint8_t portn)
+{
+	phy_type_t	phy;
+
+	if (!IS_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_espc_port_phy_type_get"
+				    " Invalid Input: portn <%d>",
+				    portn));
+		return (NPI_FAILURE | NPI_ESPC_PORT_INVALID);
+	}
+
+	NXGE_REG_RD64(handle, ESPC_PHY_TYPE, &phy.value);
+	switch (portn) {
+	case 0:
+		*data = phy.bits.w0.pt0_phy_type;
+		break;
+	case 1:
+		*data = phy.bits.w0.pt1_phy_type;
+		break;
+	case 2:
+		*data = phy.bits.w0.pt2_phy_type;
+		break;
+	case 3:
+		*data = phy.bits.w0.pt3_phy_type;
+		break;
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				" npi_espc_port_phy_type_get"
+				" Invalid Input: portn <%d>",
+				portn));
+		return (NPI_FAILURE | NPI_ESPC_PORT_INVALID);
+	}
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_espc_max_frame_get(npi_handle_t handle, uint16_t *data)
+{
+	uint64_t val = 0;
+
+	NXGE_REG_RD64(handle, ESPC_MAX_FM_SZ, &val);
+	val &= MAX_FM_SZ_MASK;
+	*data = (uint8_t)val;
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_espc_version_get(npi_handle_t handle, uint16_t *data)
+{
+	uint64_t val = 0;
+
+	NXGE_REG_RD64(handle, ESPC_VER_IMGSZ, &val);
+	val &= VER_NUM_MASK;
+	*data = (uint8_t)val;
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_espc_img_sz_get(npi_handle_t handle, uint16_t *data)
+{
+	uint64_t val = 0;
+
+	NXGE_REG_RD64(handle, ESPC_VER_IMGSZ, &val);
+	val &= IMG_SZ_MASK;
+	val = val >> IMG_SZ_SHIFT;
+	*data = (uint8_t)val;
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_espc_chksum_get(npi_handle_t handle, uint8_t *data)
+{
+	uint64_t val = 0;
+
+	NXGE_REG_RD64(handle, ESPC_CHKSUM, &val);
+	val &= CHKSUM_MASK;
+	*data = (uint8_t)val;
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_espc_intr_num_get(npi_handle_t handle, uint8_t *data)
+{
+	intr_num_t	intr;
+
+	NXGE_REG_RD64(handle, ESPC_INTR_NUM, &intr.value);
+	data[0] = intr.bits.w0.pt0_intr_num;
+	data[1] = intr.bits.w0.pt1_intr_num;
+	data[2] = intr.bits.w0.pt2_intr_num;
+	data[3] = intr.bits.w0.pt3_intr_num;
+
+	return (NPI_SUCCESS);
+}
+
+void
+npi_espc_dump(npi_handle_t handle)
+{
+	int i;
+	uint64_t val = 0;
+
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+				    "Dumping SEEPROM registers directly:\n\n"));
+
+	for (i = 0; i < 23; i++) {
+		NXGE_REG_RD64(handle, ESPC_NCR_REGN(i), &val);
+		NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+					    "reg[%d]      0x%llx\n",
+					    i, val & 0xffffffff));
+	}
+
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL, "\n\n"));
+}
+
+uint32_t
+npi_espc_reg_get(npi_handle_t handle, int reg_idx)
+{
+	uint64_t val = 0;
+	uint32_t reg_val = 0;
+
+	NXGE_REG_RD64(handle, ESPC_NCR_REGN(reg_idx), &val);
+	reg_val = val & 0xffffffff;
+
+	return (reg_val);
+}
+
+static inline uint8_t vpd_rd(npi_handle_t handle, uint32_t addr)
+{
+	uint8_t data = 0;
+
+	if (npi_espc_eeprom_entry(handle, OP_GET, addr, &data) != NPI_SUCCESS)
+		data = 0;
+	return data;
+}
+
+npi_status_t
+npi_espc_vpd_info_get(npi_handle_t handle, p_npi_vpd_info_t vpdp,
+		      uint32_t rom_len)
+{
+#define EXPANSION_ROM_SIZE	65536
+#define FD_MODEL		0x01
+#define FD_BD_MODEL		0x02
+#define FD_MAC_ADDR		0x04
+#define FD_NUM_MACS		0x08
+#define FD_PHY_TYPE		0x10
+#define FD_FW_VERSION		0x20
+#define FD_ALL			0x3f
+	int		i, len;
+	uint32_t	base = 0, kstart = 0, ep, end;
+	uint8_t		fd_flags = 0;
+
+	/* Fill the vpd_info struct with invalid vals */
+	strcpy(vpdp->model, "\0");
+	strcpy (vpdp->bd_model, "\0");
+	strcpy (vpdp->phy_type, "\0");
+	strcpy (vpdp->ver, "\0");
+	vpdp->num_macs = 0;
+	for (i = 0; i < ETHERADDRL; i++) {
+		vpdp->mac_addr[i] = 0;
+	}
+
+	ep = 0;
+	end = ep + rom_len;
+
+	/* go through the images till OBP image type is found */
+	while (ep < end) {
+		base = ep;
+		/* check for expansion rom header signature */
+		if (vpd_rd(handle, ep) != 0x55 ||
+		    vpd_rd(handle, ep + 1) != 0xaa) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				       "npi_espc_vpd_info_get: "
+				       "expansion rom image not found, "
+				       "0x%x [0x%x 0x%x]", ep,
+				       vpd_rd(handle, ep),
+				       vpd_rd(handle, ep + 1)));
+			goto vpd_info_err;
+		}
+		/* go to the beginning of the PCI data struct of this image */
+		ep = ep + 23;
+		ep = base + ((vpd_rd(handle, ep) << 8) |
+			     (vpd_rd(handle, ep + 1)));
+		/* check for PCI data struct signature "PCIR" */
+		if ((vpd_rd(handle, ep) != 0x50) ||
+		    (vpd_rd(handle, ep + 1) != 0x43) ||
+		    (vpd_rd(handle, ep + 2) != 0x49) ||
+		    (vpd_rd(handle, ep + 3) != 0x52)) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				       "npi_espc_vpd_info_get: "
+				       "PCIR sig not found"));
+			goto vpd_info_err;
+		}
+		/* check for image type OBP */
+		if (vpd_rd(handle, ep + 20) != 0x01) {
+			/* go to the next image */
+			ep = base + ((vpd_rd(handle, base + 2)) * 512);
+			continue;
+		}
+		/* find the beginning of the VPD data */
+		base = base + (vpd_rd(handle, ep + 8) |
+			       (vpd_rd(handle, ep + 9) << 8));
+		break;
+	}
+
+	/* check first byte of identifier string tag */
+	if (!base || (vpd_rd(handle, base + 0) != 0x82)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			       "npi_espc_vpd_info_get: "
+			       "Could not find VPD!!"));
+		goto vpd_info_err;
+	}
+
+	/*
+	 * skip over the ID string descriptor to go to the read-only VPD
+	 * keywords list.
+	 */
+	i = (vpd_rd(handle, base + 1) |
+	     (vpd_rd(handle, base + 2) << 8)) + 3;
+
+	while (i < EXPANSION_ROM_SIZE) {
+		if (vpd_rd(handle, base + i) != 0x90) { /* no vpd found */
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				       "nxge_get_vpd_info: Could not find "
+				       "VPD ReadOnly list!! [0x%x] %d",
+				       vpd_rd(handle, base + i), i));
+			goto vpd_info_err;
+		}
+
+		/* found a vpd read-only list, get its length */
+		len = vpd_rd(handle, base + i + 1) |
+			(vpd_rd(handle, base + i + 2) << 8);
+
+		/* extract keywords */
+		kstart = base + i + 3;
+		ep = kstart;
+		/*
+		 * Each keyword field is as follows:
+		 * 2 bytes keyword in the form of "Zx" where x = 0,1,2....
+		 * 1 byte keyword data field length - klen
+		 * Now the actual keyword data field:
+		 * 	1 byte VPD property instance, 'M' / 'I'
+		 * 	2 bytes
+		 * 	1 byte VPD property data type, 'B' / 'S'
+		 * 	1 byte VPD property value length - n
+		 * 	Actual property string, length (klen - n - 5) bytes
+		 * 	Actual property value, length n bytes
+		 */
+		while ((ep - kstart) < len) {
+			int klen = vpd_rd(handle, ep + 2);
+			int dlen;
+			char type;
+
+			ep += 3;
+
+			/* look for the following properties:
+			 *
+			 * local-mac-address:
+			 * -- VPD Instance 'I'
+			 * -- VPD Type String 'B'
+			 * -- property string == local-mac-address
+			 *
+			 * model:
+			 * -- VPD Instance 'M'
+			 * -- VPD Type String 'S'
+			 * -- property string == model
+			 *
+			 * board-model:
+			 * -- VPD Instance 'M'
+			 * -- VPD Type String 'S'
+			 * -- property string == board-model
+			 *
+			 * num-mac-addresses:
+			 * -- VPD Instance 'I'
+			 * -- VPD Type String 'B'
+			 * -- property string == num-mac-addresses
+			 *
+			 * phy-type:
+			 * -- VPD Instance 'I'
+			 * -- VPD Type String 'S'
+			 * -- property string == phy-type
+			 *
+			 * version:
+			 * -- VPD Instance 'M'
+			 * -- VPD Type String 'S'
+			 * -- property string == version
+			 */
+			if ( vpd_rd(handle, ep) == 'M') {
+				type = vpd_rd(handle, ep + 3);
+				if (type == 'S') {
+					dlen = vpd_rd(handle, ep + 4);
+					if (npi_vpd_read_prop(handle, ep + 5,
+						      "model", dlen, vpdp->model)) {
+						fd_flags |= FD_MODEL;
+						goto next;
+					}
+					if (npi_vpd_read_prop(handle, ep + 5,
+							       "board-model",
+							       dlen,
+							       vpdp->bd_model)) {
+						fd_flags |= FD_BD_MODEL;
+						goto next;
+					}
+					if (npi_vpd_read_prop(handle, ep + 5,
+							       "version",
+							       dlen,
+							       vpdp->ver)) {
+						fd_flags |= FD_FW_VERSION;
+						goto next;
+					}
+				}
+				goto next;
+			} else if (vpd_rd(handle, ep) == 'I') {
+				type = vpd_rd(handle, ep + 3);
+				if (type == 'B') {
+					dlen = vpd_rd(handle, ep + 4);
+					if (npi_vpd_read_prop(handle, ep + 5,
+						"local-mac-address",
+						dlen,
+						(char *)(vpdp->mac_addr))) {
+						fd_flags |= FD_MAC_ADDR;
+						goto next;
+
+					}
+					if (npi_vpd_read_prop(handle, ep + 5,
+							       "num-mac-addresses",
+							       dlen,
+							       (char *)&(vpdp->num_macs))) {
+						fd_flags |= FD_NUM_MACS;
+					}
+				} else if (type == 'S') {
+					dlen = vpd_rd(handle, ep + 4);
+					if (npi_vpd_read_prop(handle, ep + 5,
+							       "phy-type",
+							       dlen,
+							       vpdp->phy_type)) {
+						fd_flags |= FD_PHY_TYPE;
+					}
+				}
+				goto next;
+			} else {
+				goto vpd_info_err;
+			}
+
+next:
+			if ((fd_flags & FD_ALL) == FD_ALL)
+				goto vpd_success;
+			ep += klen;
+		}
+		i += len + 3;
+	}
+
+vpd_success:
+#if 1
+	NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				   "Model[%s] BdModel[%s] Phy[%c%c%c] VPDver[%s]\n",
+				   vpdp->model, vpdp->bd_model, vpdp->phy_type[0],
+				   vpdp->phy_type[1],vpdp->phy_type[2], vpdp->ver));
+
+	NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			   "MAC addr [%2.2x:%2.2x:%2.2x:%2.2x:%2.2x:%2.2x]\n",
+		       vpdp->mac_addr[0],
+		       vpdp->mac_addr[1],
+		       vpdp->mac_addr[2],
+		       vpdp->mac_addr[3],
+		       vpdp->mac_addr[4],
+		       vpdp->mac_addr[5]));
+	NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				   "num_macs[%d]\n", vpdp->num_macs));
+#endif
+	return (NPI_SUCCESS);
+
+vpd_info_err:
+	return (NPI_FAILURE);
+}
+
+static int
+npi_vpd_read_prop(npi_handle_t handle, uint32_t ep, const char *prop, int len,
+		  char *val)
+{
+	int prop_len =  strlen(prop) + 1;
+	int i;
+
+	for (i = 0; i < prop_len; i++) {
+		if (vpd_rd(handle, ep + i) != prop[i])
+			return 0;
+	}
+
+	ep += prop_len;
+
+	for (i = 0; i < len; i++)
+		val[i] = vpd_rd(handle, ep + i);
+/* 	val[i] = '\0'; */
+	return 1;
+}
diff --git a/drivers/net/nxge/npi/npi_espc.h b/drivers/net/nxge/npi/npi_espc.h
new file mode 100644
index 0000000..4c42d43
--- /dev/null
+++ b/drivers/net/nxge/npi/npi_espc.h
@@ -0,0 +1,100 @@
+/*
+ * npi_espc.c	Neptune  SPROM HW API functions
+ *
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef _NPI_ESPC_H
+#define	_NPI_ESPC_H
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#include <npi.h>
+#include <nxge_espc_hw.h>
+
+#define	EPC_WAIT_RW_COMP(handle, val_p, comp_bit) {\
+	uint32_t cnt = MAX_PIO_RETRIES;\
+	do {\
+		NXGE_DELAY(EPC_RW_WAIT);\
+		NXGE_REG_RD64(handle, ESPC_REG_ADDR(ESPC_PIO_STATUS_REG),\
+				val_p); cnt--;\
+	} while (((val & comp_bit) == 0) && (cnt > 0));\
+}
+
+/* ESPC specific errors */
+
+#define	ESPC_EEPROM_ADDR_INVALID	0x51
+#define	ESPC_STR_LEN_INVALID		0x91
+
+/* ESPC error return macros */
+
+#define	NPI_ESPC_EEPROM_ADDR_INVALID	((ESPC_BLK_ID << 8) |\
+					ESPC_EEPROM_ADDR_INVALID)
+#define	NPI_ESPC_EEPROM_WRITE_FAILED	((ESPC_BLK_ID << 8) | WRITE_FAILED)
+#define	NPI_ESPC_EEPROM_READ_FAILED	((ESPC_BLK_ID << 8) | READ_FAILED)
+#define	NPI_ESPC_OPCODE_INVALID		((ESPC_BLK_ID << 8) | OPCODE_INVALID)
+#define	NPI_ESPC_STR_LEN_INVALID	((ESPC_BLK_ID << 8) |\
+					ESPC_STR_LEN_INVALID)
+#define	NPI_ESPC_PORT_INVALID		((ESPC_BLK_ID << 8) | PORT_INVALID)
+
+npi_status_t npi_espc_pio_enable(npi_handle_t);
+npi_status_t npi_espc_pio_disable(npi_handle_t);
+npi_status_t npi_espc_eeprom_entry(npi_handle_t, io_op_t,
+				uint32_t, uint8_t *);
+npi_status_t npi_espc_mac_addr_get(npi_handle_t, uint8_t *);
+npi_status_t npi_espc_num_ports_get(npi_handle_t, uint8_t *);
+	npi_status_t npi_espc_num_macs_get(npi_handle_t, uint8_t *);
+npi_status_t npi_espc_model_str_get(npi_handle_t, char *);
+npi_status_t npi_espc_bd_model_str_get(npi_handle_t, char *);
+npi_status_t npi_espc_phy_type_get(npi_handle_t, uint8_t *);
+npi_status_t npi_espc_port_phy_type_get(npi_handle_t, uint8_t *,
+				uint8_t);
+npi_status_t npi_espc_max_frame_get(npi_handle_t, uint16_t *);
+npi_status_t npi_espc_version_get(npi_handle_t, uint16_t *);
+	npi_status_t npi_espc_img_sz_get(npi_handle_t, uint16_t *);
+npi_status_t npi_espc_chksum_get(npi_handle_t, uint8_t *);
+npi_status_t npi_espc_intr_num_get(npi_handle_t, uint8_t *);
+uint32_t npi_espc_reg_get(npi_handle_t, int);
+void npi_espc_dump(npi_handle_t);
+npi_status_t npi_espc_vpd_info_get(npi_handle_t, p_npi_vpd_info_t, uint32_t);
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif	/* _NPI_ESPC_H */
diff --git a/drivers/net/nxge/npi/npi_fflp.c b/drivers/net/nxge/npi/npi_fflp.c
new file mode 100644
index 0000000..5d8d59e
--- /dev/null
+++ b/drivers/net/nxge/npi/npi_fflp.c
@@ -0,0 +1,2987 @@
+/*
+ * npi_fflp.c	Neptune  FFLP Classifier HW API functions
+ *
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#include <npi_fflp.h>
+#include <nxge_common.h>
+
+/* macros to compute calss configuration register offset */
+
+#define	  GET_TCAM_CLASS_OFFSET(cls) \
+	(FFLP_TCAM_CLS_BASE_OFFSET + (cls - 2) * 8)
+#define	  GET_TCAM_KEY_OFFSET(cls) \
+	(FFLP_TCAM_KEY_BASE_OFFSET + (cls - 4) * 8)
+#define	  GET_FLOW_KEY_OFFSET(cls) \
+	(FFLP_FLOW_KEY_BASE_OFFSET + (cls - 4) * 8)
+
+#define	  HASHTBL_PART_REG_STEP 8192
+#define	  HASHTBL_PART_REG_VIR_OFFSET 0x2100
+#define	  HASHTBL_PART_REG_VIR_STEP 0x4000
+#define	  GET_HASHTBL_PART_OFFSET_NVIR(partid, reg)	\
+	((partid  * HASHTBL_PART_REG_STEP) + reg)
+
+#define	  GET_HASHTBL_PART_OFFSET(handle, partid, reg)	\
+	    (handle.is_vraddr ?					\
+	    (((partid & 0x1) * HASHTBL_PART_REG_VIR_STEP) +	\
+	    (reg & 0x8) + (HASHTBL_PART_REG_VIR_OFFSET)) :	\
+	    (partid * HASHTBL_PART_REG_STEP) + reg)
+
+#define	 FFLP_PART_OFFSET(partid, reg) ((partid  * 8) + reg)
+#define	 FFLP_VLAN_OFFSET(vid, reg) ((vid  * 8) + reg)
+
+#define	 TCAM_COMPLETION_TRY_COUNT 10
+#define	 BIT_ENABLE	0x1
+#define	 BIT_DISABLE	0x0
+
+#define	 FCRAM_PARTITION_VALID(partid) \
+	((partid < NXGE_MAX_RDC_GRPS))
+#define	FFLP_VLAN_VALID(vid) \
+	((vid > 0) && (vid < NXGE_MAX_VLANS))
+#define	FFLP_PORT_VALID(port) \
+	((port < MAX_PORTS_PER_NXGE))
+#define	FFLP_RDC_TABLE_VALID(table) \
+	((table < NXGE_MAX_RDC_GRPS))
+#define	TCAM_L3_USR_CLASS_VALID(class) \
+	((class >= TCAM_CLASS_IP_USER_4) && (class <= TCAM_CLASS_IP_USER_7))
+#define	TCAM_L2_USR_CLASS_VALID(class) \
+	((class == TCAM_CLASS_ETYPE_1) || (class == TCAM_CLASS_ETYPE_2))
+#define	TCAM_L3_CLASS_VALID(class) \
+	((class >= TCAM_CLASS_IP_USER_4) && (class <= TCAM_CLASS_SCTP_IPV6))
+#define	TCAM_CLASS_VALID(class) \
+	((class >= TCAM_CLASS_ETYPE_1) && (class <= TCAM_CLASS_RARP))
+
+
+uint64_t fflp_fzc_offset[] = {
+	FFLP_ENET_VLAN_TBL_REG, FFLP_L2_CLS_ENET1_REG, FFLP_L2_CLS_ENET2_REG,
+	FFLP_TCAM_KEY_IP_USR4_REG, FFLP_TCAM_KEY_IP_USR5_REG,
+	FFLP_TCAM_KEY_IP_USR6_REG, FFLP_TCAM_KEY_IP_USR7_REG,
+	FFLP_TCAM_KEY_IP4_TCP_REG, FFLP_TCAM_KEY_IP4_UDP_REG,
+	FFLP_TCAM_KEY_IP4_AH_ESP_REG, FFLP_TCAM_KEY_IP4_SCTP_REG,
+	FFLP_TCAM_KEY_IP6_TCP_REG, FFLP_TCAM_KEY_IP6_UDP_REG,
+	FFLP_TCAM_KEY_IP6_AH_ESP_REG, FFLP_TCAM_KEY_IP6_SCTP_REG,
+	FFLP_TCAM_KEY_0_REG, FFLP_TCAM_KEY_1_REG, FFLP_TCAM_KEY_2_REG,
+	FFLP_TCAM_KEY_3_REG, FFLP_TCAM_MASK_0_REG, FFLP_TCAM_MASK_1_REG,
+	FFLP_TCAM_MASK_2_REG, FFLP_TCAM_MASK_3_REG, FFLP_TCAM_CTL_REG,
+	FFLP_VLAN_PAR_ERR_REG, FFLP_TCAM_ERR_REG, HASH_LKUP_ERR_LOG1_REG,
+	HASH_LKUP_ERR_LOG2_REG, FFLP_FCRAM_ERR_TST0_REG,
+	FFLP_FCRAM_ERR_TST1_REG, FFLP_FCRAM_ERR_TST2_REG, FFLP_ERR_MSK_REG,
+	FFLP_CFG_1_REG, FFLP_DBG_TRAIN_VCT_REG, FFLP_TCP_CFLAG_MSK_REG,
+	FFLP_FCRAM_REF_TMR_REG,  FFLP_FLOW_KEY_IP_USR4_REG,
+	FFLP_FLOW_KEY_IP_USR5_REG, FFLP_FLOW_KEY_IP_USR6_REG,
+	FFLP_FLOW_KEY_IP_USR7_REG, FFLP_FLOW_KEY_IP4_TCP_REG,
+	FFLP_FLOW_KEY_IP4_UDP_REG, FFLP_FLOW_KEY_IP4_AH_ESP_REG,
+	FFLP_FLOW_KEY_IP4_SCTP_REG, FFLP_FLOW_KEY_IP6_TCP_REG,
+	FFLP_FLOW_KEY_IP6_UDP_REG, FFLP_FLOW_KEY_IP6_AH_ESP_REG,
+	FFLP_FLOW_KEY_IP6_SCTP_REG, FFLP_H1POLY_REG, FFLP_H2POLY_REG,
+	FFLP_FLW_PRT_SEL_REG
+};
+
+const char *fflp_fzc_name[] = {
+	"FFLP_ENET_VLAN_TBL_REG", "FFLP_L2_CLS_ENET1_REG",
+	"FFLP_L2_CLS_ENET2_REG", "FFLP_TCAM_KEY_IP_USR4_REG",
+	"FFLP_TCAM_KEY_IP_USR5_REG", "FFLP_TCAM_KEY_IP_USR6_REG",
+	"FFLP_TCAM_KEY_IP_USR7_REG", "FFLP_TCAM_KEY_IP4_TCP_REG",
+	"FFLP_TCAM_KEY_IP4_UDP_REG", "FFLP_TCAM_KEY_IP4_AH_ESP_REG",
+	"FFLP_TCAM_KEY_IP4_SCTP_REG", "FFLP_TCAM_KEY_IP6_TCP_REG",
+	"FFLP_TCAM_KEY_IP6_UDP_REG", "FFLP_TCAM_KEY_IP6_AH_ESP_REG",
+	"FFLP_TCAM_KEY_IP6_SCTP_REG", "FFLP_TCAM_KEY_0_REG",
+	"FFLP_TCAM_KEY_1_REG", "FFLP_TCAM_KEY_2_REG", "FFLP_TCAM_KEY_3_REG",
+	"FFLP_TCAM_MASK_0_REG", "FFLP_TCAM_MASK_1_REG", "FFLP_TCAM_MASK_2_REG",
+	"FFLP_TCAM_MASK_3_REG", "FFLP_TCAM_CTL_REG", "FFLP_VLAN_PAR_ERR_REG",
+	"FFLP_TCAM_ERR_REG", "HASH_LKUP_ERR_LOG1_REG",
+	"HASH_LKUP_ERR_LOG2_REG", "FFLP_FCRAM_ERR_TST0_REG",
+	"FFLP_FCRAM_ERR_TST1_REG", "FFLP_FCRAM_ERR_TST2_REG",
+	"FFLP_ERR_MSK_REG", "FFLP_CFG_1_REG", "FFLP_DBG_TRAIN_VCT_REG",
+	"FFLP_TCP_CFLAG_MSK_REG", "FFLP_FCRAM_REF_TMR_REG",
+	"FFLP_FLOW_KEY_IP_USR4_REG", "FFLP_FLOW_KEY_IP_USR5_REG",
+	"FFLP_FLOW_KEY_IP_USR6_REG", "FFLP_FLOW_KEY_IP_USR7_REG",
+	"FFLP_FLOW_KEY_IP4_TCP_REG", "FFLP_FLOW_KEY_IP4_UDP_REG",
+	"FFLP_FLOW_KEY_IP4_AH_ESP_REG", "FFLP_FLOW_KEY_IP4_SCTP_REG",
+	"FFLP_FLOW_KEY_IP6_TCP_REG", "FFLP_FLOW_KEY_IP6_UDP_REG",
+	"FFLP_FLOW_KEY_IP6_AH_ESP_REG",
+	"FFLP_FLOW_KEY_IP6_SCTP_REG", "FFLP_H1POLY_REG", "FFLP_H2POLY_REG",
+	"FFLP_FLW_PRT_SEL_REG"
+};
+
+uint64_t fflp_reg_offset[] = {
+	FFLP_HASH_TBL_ADDR_REG, FFLP_HASH_TBL_DATA_REG,
+	FFLP_HASH_TBL_DATA_LOG_REG
+};
+
+const char *fflp_reg_name[] = {
+	"FFLP_HASH_TBL_ADDR_REG", "FFLP_HASH_TBL_DATA_REG",
+	"FFLP_HASH_TBL_DATA_LOG_REG"
+};
+
+
+
+
+npi_status_t
+npi_fflp_dump_regs(npi_handle_t handle)
+{
+
+	uint64_t value;
+	int num_regs, i;
+
+	num_regs = sizeof (fflp_fzc_offset) / sizeof (uint64_t);
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+		"\nFFLP_FZC Register Dump \n"));
+	for (i = 0; i < num_regs; i++) {
+		REG_PIO_READ64(handle, fflp_fzc_offset[i], &value);
+		NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+			" %8llx %s\t %8llx \n",
+			fflp_fzc_offset[i], fflp_fzc_name[i], value));
+
+	}
+
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+					    "\nFFLP Register Dump\n"));
+	num_regs = sizeof (fflp_reg_offset) / sizeof (uint64_t);
+
+	for (i = 0; i < num_regs; i++) {
+		REG_PIO_READ64(handle, fflp_reg_offset[i], &value);
+		NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+			" %8llx %s\t %8llx \n",
+			fflp_reg_offset[i], fflp_reg_name[i], value));
+
+	}
+
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+					    "\n FFLP Register Dump done\n"));
+
+	return (NPI_SUCCESS);
+}
+
+void
+npi_fflp_vlan_tbl_dump(npi_handle_t handle)
+{
+	uint64_t offset;
+	vlan_id_t vlan_id;
+	uint64_t value;
+	vlan_id_t start = 0, stop = NXGE_MAX_VLANS;
+
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+		"\nVlan Table Dump \n"));
+
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+		"VID\t Offset\t Value\n"));
+
+	for (vlan_id = start; vlan_id < stop; vlan_id++) {
+		offset = FFLP_VLAN_OFFSET(vlan_id, FFLP_ENET_VLAN_TBL_REG);
+		REG_PIO_READ64(handle, offset, &value);
+		NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+			    "%x\t %llx\t %llx\n", vlan_id, offset, value));
+	}
+
+}
+
+
+
+static uint64_t
+npi_fflp_tcam_check_completion(npi_handle_t handle, tcam_op_t op_type);
+
+
+/*
+ * npi_fflp_tcam_check_completion()
+ * Returns TCAM completion status.
+ *
+ * Input:
+ *           op_type :        Read, Write, Compare
+ *           handle  :        OS specific handle
+ *
+ * Output:
+ *        For Read and write operations:
+ *        0   Successful
+ *        -1  Fail/timeout
+ *
+ *       For Compare operations (debug only )
+ *        TCAM_REG_CTL read value    on success
+ *                     value contains match location
+ *        NPI_TCAM_COMP_NO_MATCH          no match
+ *
+ */
+
+
+static uint64_t
+npi_fflp_tcam_check_completion(npi_handle_t handle, tcam_op_t op_type)
+{
+
+	uint32_t try_counter, tcam_delay = 10;
+	tcam_ctl_t tctl;
+
+	try_counter = TCAM_COMPLETION_TRY_COUNT;
+
+	switch (op_type) {
+	case TCAM_RWC_STAT:
+
+		READ_TCAM_REG_CTL(handle, &tctl.value);
+		while ((try_counter) &&
+				(tctl.bits.ldw.stat != TCAM_CTL_RWC_RWC_STAT)) {
+			try_counter--;
+			NXGE_DELAY(tcam_delay);
+			READ_TCAM_REG_CTL(handle, &tctl.value);
+		}
+
+		if (!try_counter) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " TCAM RWC_STAT operation"
+					    " failed to complete \n"));
+			return (NPI_FFLP_TCAM_HW_ERROR);
+		}
+
+		tctl.value = 0;
+		break;
+
+	case TCAM_RWC_MATCH:
+		READ_TCAM_REG_CTL(handle, &tctl.value);
+
+		while ((try_counter) &&
+			(tctl.bits.ldw.match != TCAM_CTL_RWC_RWC_MATCH)) {
+			try_counter--;
+			NXGE_DELAY(tcam_delay);
+			READ_TCAM_REG_CTL(handle, &tctl.value);
+		}
+
+		if (!try_counter) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " TCAM Match operation"
+				    "failed to find match \n"));
+			tctl.value = NPI_TCAM_COMP_NO_MATCH;
+		}
+
+
+		break;
+
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+		" Invalid TCAM completion Request \n"));
+		return (NPI_FFLP_ERROR |
+		    NPI_TCAM_ERROR | OPCODE_INVALID);
+	}
+
+	return (tctl.value);
+}
+
+
+
+
+
+/*
+ * npi_fflp_tcam_entry_invalidate()
+ *
+ * invalidates entry at tcam location
+ *
+ * Input
+ * location
+ *
+ * Return
+ *   NPI_SUCCESS
+ *   NPI_SW_ERR
+ *   NPI_HW_ERR
+ *
+ */
+
+npi_status_t
+npi_fflp_tcam_entry_invalidate(npi_handle_t handle, tcam_location_t location)
+{
+
+	tcam_ctl_t tctl, tctl_stat;
+
+/*
+ * Need to write zero to class field.
+ * Class field is bits [195:191].
+ * This corresponds to TCAM key 0 register
+ *
+ */
+
+
+	WRITE_TCAM_REG_MASK0(handle, 0xffULL);
+	WRITE_TCAM_REG_KEY0(handle, 0x0ULL);
+	tctl.value = 0;
+	tctl.bits.ldw.location = location;
+	tctl.bits.ldw.rwc = TCAM_CTL_RWC_TCAM_WR;
+
+	WRITE_TCAM_REG_CTL(handle, tctl.value);
+
+	tctl_stat.value = npi_fflp_tcam_check_completion(handle, TCAM_RWC_STAT);
+
+	if (tctl_stat.value & NPI_FAILURE)
+		return (NPI_FFLP_TCAM_HW_ERROR);
+
+	return (NPI_SUCCESS);
+
+}
+
+
+
+
+/*
+ * npi_fflp_tcam_entry_match()
+ *
+ * lookup a tcam entry in the TCAM
+ *
+ * Input
+ *  tcam_ptr      TCAM entry ptr
+ *
+ * Return
+ *
+ *	 NPI_FAILURE | NPI_XX_ERROR:	     Operational Error (HW etc ...)
+ *	 NPI_TCAM_NO_MATCH:		     no match
+ *	 0 - TCAM_SIZE:			     matching entry location (if match)
+ */
+
+int
+npi_fflp_tcam_entry_match(npi_handle_t handle,  tcam_entry_t *tcam_ptr)
+{
+
+	uint64_t tcam_stat = 0;
+	tcam_ctl_t tctl, tctl_stat;
+
+	WRITE_TCAM_REG_MASK0(handle, tcam_ptr->mask0);
+	WRITE_TCAM_REG_MASK1(handle, tcam_ptr->mask1);
+	WRITE_TCAM_REG_MASK2(handle, tcam_ptr->mask2);
+	WRITE_TCAM_REG_MASK3(handle, tcam_ptr->mask3);
+
+	WRITE_TCAM_REG_KEY0(handle, tcam_ptr->key0);
+	WRITE_TCAM_REG_KEY1(handle, tcam_ptr->key1);
+	WRITE_TCAM_REG_KEY2(handle, tcam_ptr->key2);
+	WRITE_TCAM_REG_KEY3(handle, tcam_ptr->key3);
+
+	tctl.value = 0;
+	tctl.bits.ldw.rwc = TCAM_CTL_RWC_TCAM_CMP;
+
+	WRITE_TCAM_REG_CTL(handle, tctl.value);
+
+	tcam_stat = npi_fflp_tcam_check_completion(handle, TCAM_RWC_STAT);
+	if (tcam_stat & NPI_FAILURE) {
+		return ((uint32_t)tcam_stat);
+	}
+
+	tctl_stat.value = npi_fflp_tcam_check_completion(handle,
+				TCAM_RWC_MATCH);
+
+	if (tctl_stat.bits.ldw.match == TCAM_CTL_RWC_RWC_MATCH) {
+		return (uint32_t)(tctl_stat.bits.ldw.location);
+	}
+
+	return ((uint32_t)tctl_stat.value);
+
+}
+
+
+
+/*
+ * npi_fflp_tcam_entry_read ()
+ *
+ * Reads a tcam entry from the TCAM location, location
+ *
+ * Input:
+ * location
+ * tcam_ptr
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_HW_ERROR
+ * NPI_SW_ERROR
+ *
+ */
+
+
+
+npi_status_t
+npi_fflp_tcam_entry_read(npi_handle_t handle,
+						    tcam_location_t location,
+						    struct tcam_entry *tcam_ptr)
+{
+
+	uint64_t tcam_stat;
+	tcam_ctl_t tctl;
+
+	tctl.value = 0;
+	tctl.bits.ldw.location = location;
+	tctl.bits.ldw.rwc = TCAM_CTL_RWC_TCAM_RD;
+
+	WRITE_TCAM_REG_CTL(handle, tctl.value);
+
+	tcam_stat = npi_fflp_tcam_check_completion(handle, TCAM_RWC_STAT);
+
+	if (tcam_stat & NPI_FAILURE) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+		    "TCAM read failed loc %d \n", location));
+		return (NPI_FFLP_TCAM_RD_ERROR);
+	}
+
+	READ_TCAM_REG_MASK0(handle, &tcam_ptr->mask0);
+	READ_TCAM_REG_MASK1(handle, &tcam_ptr->mask1);
+	READ_TCAM_REG_MASK2(handle, &tcam_ptr->mask2);
+	READ_TCAM_REG_MASK3(handle, &tcam_ptr->mask3);
+
+	READ_TCAM_REG_KEY0(handle, &tcam_ptr->key0);
+	READ_TCAM_REG_KEY1(handle, &tcam_ptr->key1);
+	READ_TCAM_REG_KEY2(handle, &tcam_ptr->key2);
+	READ_TCAM_REG_KEY3(handle, &tcam_ptr->key3);
+
+	return (NPI_SUCCESS);
+}
+
+
+
+/*
+ * npi_fflp_tcam_entry_write()
+ *
+ * writes a tcam entry to the TCAM location, location
+ *
+ * Input:
+ * location
+ * tcam_ptr
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_HW_ERROR
+ * NPI_SW_ERROR
+ *
+ */
+
+npi_status_t
+npi_fflp_tcam_entry_write(npi_handle_t handle,
+			    tcam_location_t location,
+			    tcam_entry_t *tcam_ptr)
+{
+
+	uint64_t tcam_stat;
+
+	tcam_ctl_t tctl;
+
+	WRITE_TCAM_REG_MASK0(handle, tcam_ptr->mask0);
+	WRITE_TCAM_REG_MASK1(handle, tcam_ptr->mask1);
+	WRITE_TCAM_REG_MASK2(handle, tcam_ptr->mask2);
+	WRITE_TCAM_REG_MASK3(handle, tcam_ptr->mask3);
+
+	WRITE_TCAM_REG_KEY0(handle, tcam_ptr->key0);
+	WRITE_TCAM_REG_KEY1(handle, tcam_ptr->key1);
+	WRITE_TCAM_REG_KEY2(handle, tcam_ptr->key2);
+	WRITE_TCAM_REG_KEY3(handle, tcam_ptr->key3);
+
+	NPI_DEBUG_MSG((handle.function, NPI_FFLP_CTL,
+			    " tcam write: location %x\n"
+			    " key:  %llx %llx %llx %llx \n"
+			    " mask: %llx %llx %llx %llx \n",
+			    location, tcam_ptr->key0, tcam_ptr->key1,
+			    tcam_ptr->key2, tcam_ptr->key3,
+			    tcam_ptr->mask0, tcam_ptr->mask1,
+			    tcam_ptr->mask2, tcam_ptr->mask3));
+	tctl.value = 0;
+	tctl.bits.ldw.location = location;
+	tctl.bits.ldw.rwc = TCAM_CTL_RWC_TCAM_WR;
+	NPI_DEBUG_MSG((handle.function, NPI_FFLP_CTL,
+			    " tcam write: ctl value %llx \n", tctl.value));
+	WRITE_TCAM_REG_CTL(handle, tctl.value);
+
+	tcam_stat = npi_fflp_tcam_check_completion(handle, TCAM_RWC_STAT);
+
+	if (tcam_stat & NPI_FAILURE) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+		    "TCAM Write failed loc %d \n", location));
+		return (NPI_FFLP_TCAM_WR_ERROR);
+	}
+
+	return (NPI_SUCCESS);
+}
+
+
+
+/*
+ * npi_fflp_tcam_asc_ram_entry_write()
+ *
+ * writes a tcam associatedRAM at the TCAM location, location
+ *
+ * Input:
+ * location	tcam associatedRAM location
+ * ram_data	Value to write
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_HW_ERROR
+ * NPI_SW_ERROR
+ *
+ */
+
+
+npi_status_t
+npi_fflp_tcam_asc_ram_entry_write(npi_handle_t handle,
+				    tcam_location_t location,
+				    uint64_t ram_data)
+{
+
+	uint64_t tcam_stat = 0;
+	tcam_ctl_t tctl;
+
+
+	WRITE_TCAM_REG_KEY1(handle, ram_data);
+
+	tctl.value = 0;
+	tctl.bits.ldw.location = location;
+	tctl.bits.ldw.rwc = TCAM_CTL_RWC_RAM_WR;
+
+	NPI_DEBUG_MSG((handle.function, NPI_FFLP_CTL,
+		    " tcam ascr write: location %x data %llx ctl value %llx \n",
+		    location, ram_data, tctl.value));
+	WRITE_TCAM_REG_CTL(handle, tctl.value);
+	tcam_stat = npi_fflp_tcam_check_completion(handle, TCAM_RWC_STAT);
+
+	if (tcam_stat & NPI_FAILURE) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+		    "TCAM RAM write failed loc %d \n", location));
+		return (NPI_FFLP_ASC_RAM_WR_ERROR);
+	}
+
+	return (NPI_SUCCESS);
+}
+
+
+
+
+/*
+ * npi_fflp_tcam_asc_ram_entry_read()
+ *
+ * reads a tcam associatedRAM content at the TCAM location, location
+ *
+ * Input:
+ * location	tcam associatedRAM location
+ * ram_data	ptr to return contents
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_HW_ERROR
+ * NPI_SW_ERROR
+ *
+ */
+
+
+npi_status_t
+npi_fflp_tcam_asc_ram_entry_read(npi_handle_t handle,
+				    tcam_location_t location,
+				    uint64_t *ram_data)
+{
+
+	uint64_t tcam_stat;
+	tcam_ctl_t tctl;
+
+
+	tctl.value = 0;
+	tctl.bits.ldw.location = location;
+	tctl.bits.ldw.rwc = TCAM_CTL_RWC_RAM_RD;
+
+	WRITE_TCAM_REG_CTL(handle, tctl.value);
+
+	tcam_stat = npi_fflp_tcam_check_completion(handle, TCAM_RWC_STAT);
+
+	if (tcam_stat & NPI_FAILURE) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+		    "TCAM RAM read failed loc %d \n", location));
+		return (NPI_FFLP_ASC_RAM_RD_ERROR);
+	}
+
+	READ_TCAM_REG_KEY1(handle, ram_data);
+
+	return (NPI_SUCCESS);
+}
+
+
+
+/* FFLP FCRAM Related functions */
+/* The following are FCRAM datapath functions */
+
+/*
+ * npi_fflp_fcram_entry_write ()
+ * Populates an FCRAM entry
+ * Inputs:
+ *         handle:	opaque handle interpreted by the underlying OS
+ *	   partid:	Partition ID
+ *	   location:	Index to the FCRAM.
+ *			 Corresponds to last 20 bits of H1 value
+ *	   fcram_ptr:	Pointer to the FCRAM contents to be used for writing
+ *	   format:	Entry Format. Determines the size of the write.
+ *			      FCRAM_ENTRY_OPTIM:   8 bytes (a 64 bit write)
+ *			      FCRAM_ENTRY_EX_IP4:  32 bytes (4 X 64 bit write)
+ *			      FCRAM_ENTRY_EX_IP6:  56 bytes (7 X 64 bit write)
+ *
+ * Outputs:
+ *         NPI_SUCCESS:		        Successful
+ *	   NPI_FAILURE | NPI_XX_ERROR	failure and reason
+ */
+
+npi_status_t
+npi_fflp_fcram_entry_write(npi_handle_t handle, part_id_t partid,
+			    uint32_t location, fcram_entry_t *fcram_ptr,
+			    fcram_entry_format_t format)
+
+{
+
+	int num_subareas = 0;
+	uint64_t addr_reg, data_reg;
+	int subarea;
+	int autoinc;
+	hash_tbl_addr_t addr;
+	switch (format) {
+	case FCRAM_ENTRY_OPTIM:
+		if (location % 8) {
+		/* need to be 8 byte alligned */
+
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " FCRAM_ENTRY_OOPTIM Write:"
+				    " unaligned location %llx \n",
+				    location));
+
+			return (NPI_FFLP_FCRAM_LOC_INVALID);
+	}
+
+	num_subareas = 1;
+	autoinc = 0;
+	break;
+
+	case FCRAM_ENTRY_EX_IP4:
+		if (location % 32) {
+/* need to be 32 byte alligned */
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			    " FCRAM_ENTRY_EX_IP4 Write:"
+			    " unaligned location %llx \n",
+			    location));
+			return (NPI_FFLP_FCRAM_LOC_INVALID);
+	}
+
+	num_subareas = 4;
+	autoinc = 1;
+
+	break;
+	case FCRAM_ENTRY_EX_IP6:
+		if (location % 64) {
+				/* need to be 64 byte alligned */
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " FCRAM_ENTRY_EX_IP6 Write:"
+				    " unaligned location %llx \n",
+				    location));
+				return (NPI_FFLP_FCRAM_LOC_INVALID);
+
+		}
+		num_subareas = 7;
+		autoinc = 1;
+			break;
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			    " fcram_entry_write:"
+			    " unknown format param location %llx\n",
+			    location));
+		return (NPI_FFLP_ERROR | NPI_FCRAM_ERROR | OPCODE_INVALID);
+	}
+
+	addr.value = 0;
+	addr.bits.ldw.autoinc = autoinc;
+	addr.bits.ldw.addr = location;
+	addr_reg = GET_HASHTBL_PART_OFFSET(handle, partid,
+					    FFLP_HASH_TBL_ADDR_REG);
+	data_reg = GET_HASHTBL_PART_OFFSET(handle, partid,
+					    FFLP_HASH_TBL_DATA_REG);
+/* write to addr reg */
+	REG_PIO_WRITE64(handle, addr_reg, addr.value);
+/* write data to the data register */
+
+	for (subarea = 0; subarea < num_subareas; subarea++) {
+		REG_PIO_WRITE64(handle, data_reg, fcram_ptr->value[subarea]);
+	}
+
+	return (NPI_SUCCESS);
+}
+
+
+/*
+ * npi_fflp_fcram_read_read ()
+ * Reads an FCRAM entry
+ * Inputs:
+ *         handle:	opaque handle interpreted by the underlying OS
+ *	   partid:	Partition ID
+ *	   location:	Index to the FCRAM.
+ *                  Corresponds to last 20 bits of H1 value
+ *
+ *	   fcram_ptr:	Pointer to the FCRAM contents to be updated
+ *	   format:	Entry Format. Determines the size of the read.
+ *			      FCRAM_ENTRY_OPTIM:   8 bytes (a 64 bit read)
+ *			      FCRAM_ENTRY_EX_IP4:  32 bytes (4 X 64 bit read )
+ *			      FCRAM_ENTRY_EX_IP6:  56 bytes (7 X 64 bit read )
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+
+npi_status_t
+npi_fflp_fcram_entry_read(npi_handle_t handle,  part_id_t partid,
+			    uint32_t location, fcram_entry_t *fcram_ptr,
+			    fcram_entry_format_t format)
+{
+
+	int num_subareas = 0;
+	uint64_t addr_reg, data_reg;
+	int subarea, autoinc;
+	hash_tbl_addr_t addr;
+	switch (format) {
+		case FCRAM_ENTRY_OPTIM:
+			if (location % 8) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				" FCRAM_ENTRY_OOPTIM Read:"
+				" unaligned location %llx \n",
+				location));
+			/* need to be 8 byte alligned */
+				return (NPI_FFLP_FCRAM_LOC_INVALID);
+			}
+			num_subareas = 1;
+			autoinc = 0;
+			break;
+		case FCRAM_ENTRY_EX_IP4:
+			if (location % 32) {
+					/* need to be 32 byte alligned */
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					" FCRAM_ENTRY_EX_IP4 READ:"
+					" unaligned location %llx \n",
+					location));
+				return (NPI_FFLP_FCRAM_LOC_INVALID);
+			}
+			num_subareas = 4;
+			autoinc = 1;
+
+			break;
+		case FCRAM_ENTRY_EX_IP6:
+			if (location % 64) {
+					/* need to be 64 byte alligned */
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				" FCRAM_ENTRY_EX_IP6 READ:"
+				" unaligned location %llx \n",
+				location));
+
+				return (NPI_FFLP_FCRAM_LOC_INVALID);
+	}
+			num_subareas = 7;
+			autoinc = 1;
+
+			break;
+		default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" fcram_entry_read:"
+			" unknown format param location %llx\n",
+			location));
+		return (NPI_FFLP_SW_PARAM_ERROR);
+	}
+
+	addr.value = 0;
+	addr.bits.ldw.autoinc = autoinc;
+	addr.bits.ldw.addr = location;
+	addr_reg = GET_HASHTBL_PART_OFFSET(handle, partid,
+			FFLP_HASH_TBL_ADDR_REG);
+	data_reg = GET_HASHTBL_PART_OFFSET(handle, partid,
+			FFLP_HASH_TBL_DATA_REG);
+/* write to addr reg */
+	REG_PIO_WRITE64(handle, addr_reg, addr.value);
+/* read data from the data register */
+	for (subarea = 0; subarea < num_subareas; subarea++) {
+		REG_PIO_READ64(handle, data_reg, &fcram_ptr->value[subarea]);
+	}
+
+
+	return (NPI_SUCCESS);
+
+}
+
+
+
+/*
+ * npi_fflp_fcram_entry_invalidate ()
+ * Invalidate FCRAM entry at the given location
+ * Inputs:
+ *	handle:		opaque handle interpreted by the underlying OS
+ *	partid:		Partition ID
+ *	location:	location of the FCRAM/hash entry.
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+npi_status_t
+npi_fflp_fcram_entry_invalidate(npi_handle_t handle, part_id_t partid,
+				    uint32_t location)
+{
+
+	hash_tbl_addr_t addr;
+	uint64_t addr_reg, data_reg;
+	hash_hdr_t	   hdr;
+
+
+	if (location % 8) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" FCRAM_ENTRY_Invalidate:"
+			" unaligned location %llx \n",
+			location));
+			/* need to be 8 byte alligned */
+		return (NPI_FFLP_FCRAM_LOC_INVALID);
+	}
+
+	addr.value = 0;
+	addr.bits.ldw.addr = location;
+	addr_reg = GET_HASHTBL_PART_OFFSET(handle, partid,
+			FFLP_HASH_TBL_ADDR_REG);
+	data_reg = GET_HASHTBL_PART_OFFSET(handle, partid,
+			FFLP_HASH_TBL_DATA_REG);
+
+/* write to addr reg */
+	REG_PIO_WRITE64(handle, addr_reg, addr.value);
+
+	REG_PIO_READ64(handle, data_reg, &hdr.value);
+	hdr.exact_hdr.valid = 0;
+	REG_PIO_WRITE64(handle, data_reg, hdr.value);
+
+	return (NPI_SUCCESS);
+
+}
+
+/*
+ * npi_fflp_fcram_write_subarea ()
+ * Writes to FCRAM entry subarea i.e the 8 bytes within the 64 bytes
+ * pointed by the  last 20 bits of  H1. Effectively, this accesses
+ * specific 8 bytes within the hash table bucket.
+ *
+ *  H1-->  |-----------------|
+ *	   |	subarea 0    |
+ *	   |_________________|
+ *	   | Subarea 1	     |
+ *	   |_________________|
+ *	   | .......	     |
+ *	   |_________________|
+ *	   | Subarea 7       |
+ *	   |_________________|
+ *
+ * Inputs:
+ *         handle:	opaque handle interpreted by the underlying OS
+ *	   partid:	Partition ID
+ *	   location:	location of the subarea. It is derived from:
+ *			Bucket = [19:15][14:0]       (20 bits of H1)
+ *			location = (Bucket << 3 ) + subarea * 8
+ *				 = [22:18][17:3] || subarea * 8
+ *	   data:	Data
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+npi_status_t
+npi_fflp_fcram_subarea_write(npi_handle_t handle, part_id_t partid,
+			    uint32_t location, uint64_t data)
+{
+
+	hash_tbl_addr_t addr;
+	uint64_t addr_reg, data_reg;
+
+
+	if (location % 8) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" fcram_subarea_write:"
+			" unaligned location %llx \n",
+			location));
+			/* need to be 8 byte alligned */
+		return (NPI_FFLP_FCRAM_LOC_INVALID);
+	}
+
+	addr.value = 0;
+	addr.bits.ldw.addr = location;
+	addr_reg = GET_HASHTBL_PART_OFFSET(handle, partid,
+			FFLP_HASH_TBL_ADDR_REG);
+	data_reg = GET_HASHTBL_PART_OFFSET(handle, partid,
+			FFLP_HASH_TBL_DATA_REG);
+
+/* write to addr reg */
+	REG_PIO_WRITE64(handle, addr_reg, addr.value);
+	REG_PIO_WRITE64(handle, data_reg, data);
+
+	return (NPI_SUCCESS);
+
+}
+
+/*
+ * npi_fflp_fcram_subarea_read ()
+ * Reads an FCRAM entry subarea i.e the 8 bytes within the 64 bytes
+ * pointed by  the last 20 bits of  H1. Effectively, this accesses
+ * specific 8 bytes within the hash table bucket.
+ *
+ *  H1-->  |-----------------|
+ *	   |	subarea 0    |
+ *	   |_________________|
+ *	   | Subarea 1	     |
+ *	   |_________________|
+ *	   | .......	     |
+ *	   |_________________|
+ *	   | Subarea 7       |
+ *	   |_________________|
+ *
+ * Inputs:
+ *         handle:	opaque handle interpreted by the underlying OS
+ *	   partid:	Partition ID
+ *	   location:	location of the subarea. It is derived from:
+ *			Bucket = [19:15][14:0]       (20 bits of H1)
+ *			location = (Bucket << 3 ) + subarea * 8
+ *				 = [22:18][17:3] || subarea * 8
+ *	   data:	ptr do write subarea contents to.
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+npi_status_t
+npi_fflp_fcram_subarea_read(npi_handle_t handle, part_id_t partid,
+			    uint32_t location, uint64_t *data)
+
+{
+
+	hash_tbl_addr_t addr;
+	uint64_t addr_reg, data_reg;
+
+	if (location % 8) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " fcram_subarea_read:"
+				    " unaligned location %llx \n",
+				    location));
+			/* need to be 8 byte alligned */
+		return (NPI_FFLP_FCRAM_LOC_INVALID);
+	}
+
+	addr.value = 0;
+	addr.bits.ldw.addr = location;
+	addr_reg = GET_HASHTBL_PART_OFFSET(handle, partid,
+						    FFLP_HASH_TBL_ADDR_REG);
+	data_reg = GET_HASHTBL_PART_OFFSET(handle, partid,
+						    FFLP_HASH_TBL_DATA_REG);
+
+/* write to addr reg */
+	REG_PIO_WRITE64(handle, addr_reg, addr.value);
+	REG_PIO_READ64(handle, data_reg, data);
+
+	return (NPI_SUCCESS);
+
+}
+
+
+/*
+ * The following are zero function fflp configuration functions.
+ */
+
+/*
+ * npi_fflp_fcram_config_partition()
+ * Partitions and configures the FCRAM
+ *
+ * Input
+ *     partid			partition ID
+ *				Corresponds to the RDC table
+ *     part_size		Size of the partition
+ *
+ * Return
+ *      0			Successful
+ *      Non zero  error code    Partition failed, and reason.
+ *
+ */
+
+npi_status_t
+npi_fflp_cfg_fcram_partition(npi_handle_t handle, part_id_t partid,
+				    uint8_t base_mask, uint8_t base_reloc)
+
+{
+/*
+ * assumes that the base mask and relocation are computed somewhere
+ * and kept in the state data structure. Alternativiely, one can pass
+ * a partition size and a starting address and this routine can compute
+ * the mask and reloc vlaues.
+ */
+
+    flow_prt_sel_t sel;
+    uint64_t offset;
+	if (!FCRAM_PARTITION_VALID(partid)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_fflp_cfg_fcram_partition:"
+				    " Invalid Partition %d \n",
+				    partid));
+		return (NPI_FFLP_FCRAM_PART_INVALID);
+	}
+
+    offset = FFLP_PART_OFFSET(partid, FFLP_FLW_PRT_SEL_REG);
+    sel.value = 0;
+    sel.bits.ldw.mask = base_mask;
+    sel.bits.ldw.base = base_reloc;
+    sel.bits.ldw.ext = BIT_DISABLE; /* disable */
+    REG_PIO_WRITE64(handle, offset, sel.value);
+    return (NPI_SUCCESS);
+
+}
+
+
+
+/*
+ * npi_fflp_fcram_partition_enable
+ * Enable previously configured FCRAM partition
+ *
+ * Input
+ *     partid			partition ID
+ *				Corresponds to the RDC table
+ *
+ * Return
+ *      0			Successful
+ *      Non zero  error code    Enable failed, and reason.
+ *
+ */
+npi_status_t
+npi_fflp_cfg_fcram_partition_enable  (npi_handle_t handle, part_id_t partid)
+
+{
+
+    flow_prt_sel_t sel;
+    uint64_t offset;
+
+
+	if (!FCRAM_PARTITION_VALID(partid)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " fcram_partition enable:"
+				    " Invalid Partition %d \n",
+				    partid));
+		return (NPI_FFLP_FCRAM_PART_INVALID);
+	}
+
+	offset = FFLP_PART_OFFSET(partid, FFLP_FLW_PRT_SEL_REG);
+
+    REG_PIO_READ64(handle, offset, &sel.value);
+    sel.bits.ldw.ext = BIT_ENABLE; /* enable */
+    REG_PIO_WRITE64(handle, offset, sel.value);
+
+	return (NPI_SUCCESS);
+
+}
+
+
+
+/*
+ * npi_fflp_fcram_partition_disable
+ * Disabled previously configured FCRAM partition
+ *
+ * Input
+ *     partid			partition ID
+ *				Corresponds to the RDC table
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+npi_status_t
+npi_fflp_cfg_fcram_partition_disable(npi_handle_t handle, part_id_t partid)
+
+{
+
+	flow_prt_sel_t sel;
+	uint64_t offset;
+
+	if (!FCRAM_PARTITION_VALID(partid)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " fcram_partition disable:"
+				    " Invalid Partition %d \n",
+				    partid));
+		return (NPI_FFLP_FCRAM_PART_INVALID);
+	}
+	offset = FFLP_PART_OFFSET(partid, FFLP_FLW_PRT_SEL_REG);
+	REG_PIO_READ64(handle, offset, &sel.value);
+	sel.bits.ldw.ext = BIT_DISABLE; /* disable */
+	REG_PIO_WRITE64(handle, offset, sel.value);
+	return (NPI_SUCCESS);
+}
+
+
+/*
+ *  npi_fflp_cam_errorcheck_disable
+ *  Disables FCRAM and TCAM error checking
+ *
+ *  Input
+ *
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+npi_status_t
+npi_fflp_cfg_cam_errorcheck_disable(npi_handle_t handle)
+
+{
+
+	fflp_cfg_1_t fflp_cfg;
+	uint64_t offset;
+	offset = FFLP_CFG_1_REG;
+
+	REG_PIO_READ64(handle, offset, &fflp_cfg.value);
+
+	fflp_cfg.bits.ldw.errordis = BIT_ENABLE;
+	REG_PIO_WRITE64(handle, offset, fflp_cfg.value);
+
+	return (NPI_SUCCESS);
+
+}
+
+
+/*
+ *  npi_fflp_cam_errorcheck_enable
+ *  Enables FCRAM and TCAM error checking
+ *
+ *  Input
+ *
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+npi_status_t
+npi_fflp_cfg_cam_errorcheck_enable(npi_handle_t handle)
+
+{
+	fflp_cfg_1_t fflp_cfg;
+	uint64_t offset;
+	offset = FFLP_CFG_1_REG;
+
+	REG_PIO_READ64(handle, offset, &fflp_cfg.value);
+
+	fflp_cfg.bits.ldw.errordis = BIT_DISABLE;
+	REG_PIO_WRITE64(handle, offset, fflp_cfg.value);
+
+	return (NPI_SUCCESS);
+
+}
+
+
+
+
+/*
+ *  npi_fflp_cam_llcsnap_enable
+ *  Enables input parser llcsnap recognition
+ *
+ *  Input
+ *
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+npi_status_t
+npi_fflp_cfg_llcsnap_enable(npi_handle_t handle)
+
+{
+
+	fflp_cfg_1_t fflp_cfg;
+	uint64_t offset;
+	offset = FFLP_CFG_1_REG;
+
+	REG_PIO_READ64(handle, offset, &fflp_cfg.value);
+
+	fflp_cfg.bits.ldw.llcsnap = BIT_ENABLE;
+	REG_PIO_WRITE64(handle, offset, fflp_cfg.value);
+
+	return (NPI_SUCCESS);
+
+}
+
+
+
+/*
+ *  npi_fflp_cam_llcsnap_disable
+ *  Disables input parser llcsnap recognition
+ *
+ *  Input
+ *
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ *
+ */
+
+npi_status_t
+npi_fflp_cfg_llcsnap_disable(npi_handle_t handle)
+
+{
+
+
+	fflp_cfg_1_t fflp_cfg;
+	uint64_t offset;
+	offset = FFLP_CFG_1_REG;
+
+	REG_PIO_READ64(handle, offset, &fflp_cfg.value);
+
+	fflp_cfg.bits.ldw.llcsnap = BIT_DISABLE;
+	REG_PIO_WRITE64(handle, offset, fflp_cfg.value);
+
+	return (NPI_SUCCESS);
+
+}
+
+
+
+/*
+ * npi_fflp_config_fcram_refresh
+ * Set FCRAM min and max refresh time.
+ *
+ * Input
+ *	min_time		Minimum Refresh time count
+ *	max_time		maximum Refresh Time count
+ *	sys_time		System Clock rate
+ *
+ *	The counters are 16 bit counters. The maximum refresh time is
+ *      3.9us/clock cycle. The minimum is 400ns/clock cycle.
+ *	Clock cycle is the FCRAM clock cycle?????
+ *	If the cycle is FCRAM clock cycle, then sys_time parameter
+ *      is not needed as there wont be configuration variation due to
+ *      system clock cycle.
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+npi_status_t
+npi_fflp_cfg_fcram_refresh_time(npi_handle_t handle, uint32_t min_time,
+				    uint32_t max_time, uint32_t sys_time)
+
+{
+
+	uint64_t offset;
+	fcram_ref_tmr_t refresh_timer_reg;
+	uint16_t max, min;
+
+	offset = FFLP_FCRAM_REF_TMR_REG;
+/* need to figure out how to dervive the numbers */
+	max = max_time * sys_time;
+	min = min_time * sys_time;
+/* for now, just set with #def values */
+
+	max = FCRAM_REFRESH_DEFAULT_MAX_TIME;
+	min = FCRAM_REFRESH_DEFAULT_MIN_TIME;
+	REG_PIO_READ64(handle, offset, &refresh_timer_reg.value);
+	refresh_timer_reg.bits.ldw.min = min;
+	refresh_timer_reg.bits.ldw.max = max;
+	REG_PIO_WRITE64(handle, offset, refresh_timer_reg.value);
+	return (NPI_SUCCESS);
+}
+
+
+
+/*
+ *  npi_fflp_hash_lookup_err_report
+ *  Reports hash table (fcram) lookup errors
+ *
+ *  Input
+ *      err_stat			Pointer to return Error bits
+ *
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ *
+ */
+
+npi_status_t
+npi_fflp_fcram_get_lookup_err_log(npi_handle_t handle,
+				    hash_lookup_err_log_t *err_stat)
+
+{
+
+	hash_lookup_err_log1_t err_log1;
+	hash_lookup_err_log2_t err_log2;
+	uint64_t  err_log1_offset, err_log2_offset;
+	err_log1.value = 0;
+	err_log2.value = 0;
+
+	err_log1_offset = HASH_LKUP_ERR_LOG1_REG;
+	err_log2_offset = HASH_LKUP_ERR_LOG2_REG;
+
+	REG_PIO_READ64(handle, err_log1_offset, &err_log1.value);
+	REG_PIO_READ64(handle, err_log2_offset, &err_log2.value);
+
+	if (err_log1.value) {
+/* nonzero means there are some errors */
+		err_stat->lookup_err = BIT_ENABLE;
+		err_stat->syndrome = err_log2.bits.ldw.syndrome;
+		err_stat->subarea = err_log2.bits.ldw.subarea;
+		err_stat->h1 = err_log2.bits.ldw.h1;
+		err_stat->multi_bit = err_log1.bits.ldw.mult_bit;
+		err_stat->multi_lkup = err_log1.bits.ldw.mult_lk;
+		err_stat->ecc_err = err_log1.bits.ldw.ecc_err;
+		err_stat->uncor_err = err_log1.bits.ldw.cu;
+	} else {
+		err_stat->lookup_err = BIT_DISABLE;
+	}
+
+	return (NPI_SUCCESS);
+
+}
+
+
+/*
+ * npi_fflp_fcram_get_pio_err_log
+ * Reports hash table PIO read errors for the given partition.
+ * by default, it clears the error bit which was set by the HW.
+ *
+ * Input
+ *	partid:		partition ID
+ *      err_stat	Pointer to return Error bits
+ *
+ * Return
+ *	NPI_SUCCESS	Success
+ *
+ *
+ */
+
+npi_status_t
+npi_fflp_fcram_get_pio_err_log(npi_handle_t handle, part_id_t partid,
+				    hash_pio_err_log_t *err_stat)
+{
+
+	hash_tbl_data_log_t err_log;
+	uint64_t offset;
+
+	if (!FCRAM_PARTITION_VALID(partid)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" fcram_get_pio_err_log:"
+			" Invalid Partition %d \n",
+			partid));
+		return (NPI_FFLP_FCRAM_PART_INVALID);
+	}
+
+	offset = GET_HASHTBL_PART_OFFSET_NVIR(partid,
+			FFLP_HASH_TBL_DATA_LOG_REG);
+
+	REG_PIO_READ64(handle, offset, &err_log.value);
+
+	if (err_log.bits.ldw.pio_err == BIT_ENABLE) {
+/* nonzero means there are some errors */
+		err_stat->pio_err = BIT_ENABLE;
+		err_stat->syndrome = err_log.bits.ldw.syndrome;
+		err_stat->addr = err_log.bits.ldw.fcram_addr;
+		err_log.value = 0;
+		REG_PIO_WRITE64(handle, offset, err_log.value);
+	} else {
+		err_stat->pio_err = BIT_DISABLE;
+	}
+
+	return (NPI_SUCCESS);
+
+}
+
+
+
+
+/*
+ * npi_fflp_fcram_clr_pio_err_log
+ * Clears FCRAM PIO  error status for the partition.
+ * If there are TCAM errors as indicated by err bit set by HW,
+ *  then the SW will clear it by clearing the bit.
+ *
+ * Input
+ *	partid:		partition ID
+ *
+ *
+ * Return
+ *	NPI_SUCCESS	Success
+ *
+ *
+ */
+
+npi_status_t
+npi_fflp_fcram_clr_pio_err_log(npi_handle_t handle, part_id_t partid)
+{
+	uint64_t offset;
+
+	hash_tbl_data_log_t err_log;
+
+	if (!FCRAM_PARTITION_VALID(partid)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" fcram_clr_pio_err_log:"
+			" Invalid Partition %d \n",
+			partid));
+
+		return (NPI_FFLP_FCRAM_PART_INVALID);
+	}
+
+	offset = GET_HASHTBL_PART_OFFSET_NVIR(partid,
+			FFLP_HASH_TBL_DATA_LOG_REG);
+
+	err_log.value = 0;
+	REG_PIO_WRITE64(handle, offset, err_log.value);
+
+
+	return (NPI_SUCCESS);
+
+}
+
+
+
+
+/*
+ * npi_fflp_tcam_get_err_log
+ * Reports TCAM PIO read and lookup errors.
+ * If there are TCAM errors as indicated by err bit set by HW,
+ *  then the SW will clear it by clearing the bit.
+ *
+ * Input
+ *	err_stat:	 structure to report various TCAM errors.
+ *                       will be updated if there are TCAM errors.
+ *
+ *
+ * Return
+ *	NPI_SUCCESS	Success
+ *
+ *
+ */
+npi_status_t
+npi_fflp_tcam_get_err_log(npi_handle_t handle, tcam_err_log_t *err_stat)
+{
+	tcam_err_t err_log;
+	uint64_t offset;
+
+	offset = FFLP_TCAM_ERR_REG;
+	err_log.value = 0;
+
+	REG_PIO_READ64(handle, offset, &err_log.value);
+
+	if (err_log.bits.ldw.err == BIT_ENABLE) {
+/* non-zero means err */
+		err_stat->tcam_err = BIT_ENABLE;
+		if (err_log.bits.ldw.p_ecc) {
+			err_stat->parity_err = 0;
+			err_stat->ecc_err = 1;
+		} else {
+			err_stat->parity_err = 1;
+			err_stat->ecc_err = 0;
+
+		}
+		err_stat->syndrome = err_log.bits.ldw.syndrome;
+		err_stat->location = err_log.bits.ldw.addr;
+
+
+		err_stat->multi_lkup = err_log.bits.ldw.mult;
+			/* now clear the error */
+		err_log.value = 0;
+		REG_PIO_WRITE64(handle, offset, err_log.value);
+
+	} else {
+		err_stat->tcam_err = 0;
+	}
+	return (NPI_SUCCESS);
+
+}
+
+
+
+/*
+ * npi_fflp_tcam_clr_err_log
+ * Clears TCAM PIO read and lookup error status.
+ * If there are TCAM errors as indicated by err bit set by HW,
+ *  then the SW will clear it by clearing the bit.
+ *
+ * Input
+ *	err_stat:	 structure to report various TCAM errors.
+ *                       will be updated if there are TCAM errors.
+ *
+ *
+ * Return
+ *	NPI_SUCCESS	Success
+ *
+ *
+ */
+
+npi_status_t
+npi_fflp_tcam_clr_err_log(npi_handle_t handle)
+{
+	tcam_err_t err_log;
+	uint64_t offset;
+
+	offset = FFLP_TCAM_ERR_REG;
+	err_log.value = 0;
+	REG_PIO_WRITE64(handle, offset, err_log.value);
+
+	return (NPI_SUCCESS);
+
+}
+
+
+
+/*
+ * npi_fflp_fcram_err_synd_test
+ * Tests the FCRAM error detection logic.
+ * The error detection logic for the syndrome is tested.
+ * tst0->synd (8bits) are set to select the syndrome bits
+ * to be XOR'ed
+ *
+ * Input
+ *	syndrome_bits:	 Syndrome bits to select bits to be xor'ed
+ *
+ *
+ * Return
+ *	NPI_SUCCESS	Success
+ *
+ *
+ */
+npi_status_t
+npi_fflp_fcram_err_synd_test(npi_handle_t handle, uint8_t syndrome_bits)
+{
+
+	uint64_t t0_offset;
+	fcram_err_tst0_t tst0;
+	t0_offset = FFLP_FCRAM_ERR_TST0_REG;
+
+	tst0.value = 0;
+	tst0.bits.ldw.syndrome_mask = syndrome_bits;
+
+	REG_PIO_WRITE64(handle, t0_offset, tst0.value);
+
+	return (NPI_SUCCESS);
+
+}
+
+
+/*
+ * npi_fflp_fcram_err_data_test
+ * Tests the FCRAM error detection logic.
+ * The error detection logic for the datapath is tested.
+ * bits [63:0] are set to select the data bits to be xor'ed
+ *
+ * Input
+ *	data:	 data bits to select bits to be xor'ed
+ *
+ *
+ * Return
+ *	NPI_SUCCESS	Success
+ *
+ *
+ */
+npi_status_t
+npi_fflp_fcram_err_data_test(npi_handle_t handle, fcram_err_data_t *data)
+{
+
+	uint64_t t1_offset, t2_offset;
+	fcram_err_tst1_t tst1; /* for data bits [31:0] */
+	fcram_err_tst2_t tst2; /* for data bits [63:32] */
+
+	t1_offset = FFLP_FCRAM_ERR_TST1_REG;
+	t2_offset = FFLP_FCRAM_ERR_TST2_REG;
+	tst1.value = 0;
+	tst2.value = 0;
+	tst1.bits.ldw.dat = data->bits.ldw.dat;
+	tst2.bits.ldw.dat = data->bits.hdw.dat;
+
+	REG_PIO_WRITE64(handle, t1_offset, tst1.value);
+	REG_PIO_WRITE64(handle, t2_offset, tst2.value);
+
+	return (NPI_SUCCESS);
+
+}
+
+
+
+
+/*
+ * npi_fflp_cfg_enet_vlan_table_assoc
+ * associates port vlan id to rdc table.
+ *
+ * Input
+ *     mac_portn		port number
+ *     vlan_id			VLAN ID
+ *     rdc_table		RDC Table #
+ *
+ * Output
+ *
+ *	NPI_SUCCESS	Success
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+
+npi_status_t
+npi_fflp_cfg_enet_vlan_table_assoc(npi_handle_t handle, uint8_t mac_portn,
+				    vlan_id_t vlan_id, uint8_t rdc_table,
+				    uint8_t priority)
+{
+
+	fflp_enet_vlan_tbl_t cfg;
+	uint64_t offset;
+	uint8_t vlan_parity[8] = {0, 1, 1, 2, 1, 2, 2, 3};
+	uint8_t parity_bit;
+	if (!FFLP_VLAN_VALID(vlan_id)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" fflp_cfg_enet_vlan_table:"
+			" Invalid vlan ID %d \n",
+			vlan_id));
+		return (NPI_FFLP_VLAN_INVALID);
+	}
+
+	if (!FFLP_PORT_VALID(mac_portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" fflp_cfg_enet_vlan_table:"
+			" Invalid port num %d \n",
+			mac_portn));
+		return (NPI_FFLP_PORT_INVALID);
+	}
+
+
+	if (!FFLP_RDC_TABLE_VALID(rdc_table)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" fflp_cfg_enet_vlan_table:"
+			" Invalid RDC Table %d \n",
+			rdc_table));
+		return (NPI_FFLP_RDC_TABLE_INVALID);
+	}
+
+	offset = FFLP_VLAN_OFFSET(vlan_id, FFLP_ENET_VLAN_TBL_REG);
+	REG_PIO_READ64(handle, offset, &cfg.value);
+
+	switch (mac_portn) {
+		case 0:
+			cfg.bits.ldw.vlanrdctbln0 = rdc_table;
+			if (priority)
+				cfg.bits.ldw.vpr0 = BIT_ENABLE;
+			else
+				cfg.bits.ldw.vpr0 = BIT_DISABLE;
+				/* set the parity bits */
+			parity_bit = vlan_parity[cfg.bits.ldw.vlanrdctbln0] +
+				vlan_parity[cfg.bits.ldw.vlanrdctbln1] +
+				cfg.bits.ldw.vpr0 + cfg.bits.ldw.vpr1;
+			cfg.bits.ldw.parity0 = parity_bit & 0x1;
+			break;
+		case 1:
+			cfg.bits.ldw.vlanrdctbln1 = rdc_table;
+			if (priority)
+				cfg.bits.ldw.vpr1 = BIT_ENABLE;
+			else
+				cfg.bits.ldw.vpr1 = BIT_DISABLE;
+				/* set the parity bits */
+			parity_bit = vlan_parity[cfg.bits.ldw.vlanrdctbln0] +
+				vlan_parity[cfg.bits.ldw.vlanrdctbln1] +
+				cfg.bits.ldw.vpr0 + cfg.bits.ldw.vpr1;
+				cfg.bits.ldw.parity0 = parity_bit & 0x1;
+
+			break;
+		case 2:
+			cfg.bits.ldw.vlanrdctbln2 = rdc_table;
+			if (priority)
+				cfg.bits.ldw.vpr2 = BIT_ENABLE;
+			else
+				cfg.bits.ldw.vpr2 = BIT_DISABLE;
+				/* set the parity bits */
+			parity_bit = vlan_parity[cfg.bits.ldw.vlanrdctbln2] +
+				vlan_parity[cfg.bits.ldw.vlanrdctbln3] +
+				cfg.bits.ldw.vpr2 + cfg.bits.ldw.vpr3;
+			cfg.bits.ldw.parity1 = parity_bit & 0x1;
+
+			break;
+		case 3:
+			cfg.bits.ldw.vlanrdctbln3 = rdc_table;
+			if (priority)
+				cfg.bits.ldw.vpr3 = BIT_ENABLE;
+			else
+				cfg.bits.ldw.vpr3 = BIT_DISABLE;
+				/* set the parity bits */
+			parity_bit = vlan_parity[cfg.bits.ldw.vlanrdctbln2] +
+				vlan_parity[cfg.bits.ldw.vlanrdctbln3] +
+				cfg.bits.ldw.vpr2 + cfg.bits.ldw.vpr3;
+			cfg.bits.ldw.parity1 = parity_bit & 0x1;
+			break;
+		default:
+			return (NPI_FFLP_SW_PARAM_ERROR);
+	}
+
+	REG_PIO_WRITE64(handle, offset, cfg.value);
+	return (NPI_SUCCESS);
+}
+
+
+
+/*
+ * npi_fflp_cfg_enet_vlan_table_set_pri
+ * sets the  vlan based classification priority in respect to L2DA
+ * classification.
+ *
+ * Input
+ *     mac_portn	port number
+ *     vlan_id		VLAN ID
+ *     priority 	priority
+ *			1: vlan classification has higher priority
+ *			0: l2da classification has higher priority
+ *
+ * Output
+ *
+ *	NPI_SUCCESS			Successful
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+npi_status_t
+npi_fflp_cfg_enet_vlan_table_set_pri(npi_handle_t handle, uint8_t mac_portn,
+				    vlan_id_t vlan_id, uint8_t priority)
+{
+
+	fflp_enet_vlan_tbl_t cfg;
+	uint64_t offset;
+	uint64_t old_value;
+
+	if (!FFLP_VLAN_VALID(vlan_id)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" enet_vlan_table set pri:"
+			" Invalid vlan ID %d \n",
+			vlan_id));
+		return (NPI_FFLP_VLAN_INVALID);
+	}
+
+	if (!FFLP_PORT_VALID(mac_portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" enet_vlan_table set pri:"
+			" Invalid port num %d \n",
+			mac_portn));
+		return (NPI_FFLP_PORT_INVALID);
+	}
+
+
+	offset = FFLP_ENET_VLAN_TBL_REG + (vlan_id  << 3);
+	REG_PIO_READ64(handle, offset, &cfg.value);
+	old_value = cfg.value;
+	switch (mac_portn) {
+		case 0:
+			if (priority)
+				cfg.bits.ldw.vpr0 = BIT_ENABLE;
+			else
+				cfg.bits.ldw.vpr0 = BIT_DISABLE;
+			break;
+		case 1:
+			if (priority)
+				cfg.bits.ldw.vpr1 = BIT_ENABLE;
+			else
+				cfg.bits.ldw.vpr1 = BIT_DISABLE;
+			break;
+		case 2:
+			if (priority)
+				cfg.bits.ldw.vpr2 = BIT_ENABLE;
+			else
+				cfg.bits.ldw.vpr2 = BIT_DISABLE;
+			break;
+		case 3:
+			if (priority)
+				cfg.bits.ldw.vpr3 = BIT_ENABLE;
+			else
+				cfg.bits.ldw.vpr3 = BIT_DISABLE;
+			break;
+		default:
+			return (NPI_FFLP_SW_PARAM_ERROR);
+	}
+	if (old_value != cfg.value) {
+		if (mac_portn > 1)
+			cfg.bits.ldw.parity1++;
+		else
+			cfg.bits.ldw.parity0++;
+
+		REG_PIO_WRITE64(handle, offset, cfg.value);
+	}
+	return (NPI_SUCCESS);
+}
+
+
+
+/*
+ * npi_fflp_cfg_vlan_table_clear
+ * Clears the vlan RDC table
+ *
+ * Input
+ *     vlan_id		VLAN ID
+ *
+ * Output
+ *
+ *	NPI_SUCCESS			Successful
+ *
+ */
+
+npi_status_t
+npi_fflp_cfg_vlan_table_clear(npi_handle_t handle, vlan_id_t vlan_id)
+{
+
+	uint64_t offset;
+	uint64_t clear = 0ULL;
+	vlan_id_t start_vlan = 0;
+
+	if ((vlan_id < start_vlan) || (vlan_id >= NXGE_MAX_VLANS)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" enet_vlan_table clear:"
+			" Invalid vlan ID %d \n",
+			vlan_id));
+		return (NPI_FFLP_VLAN_INVALID);
+	}
+
+
+	offset = FFLP_VLAN_OFFSET(vlan_id, FFLP_ENET_VLAN_TBL_REG);
+
+	REG_PIO_WRITE64(handle, offset, clear);
+	return (NPI_SUCCESS);
+}
+
+
+
+/*
+ * npi_fflp_vlan_tbl_get_err_log
+ * Reports VLAN Table  errors.
+ * If there are VLAN Table errors as indicated by err bit set by HW,
+ *  then the SW will clear it by clearing the bit.
+ *
+ * Input
+ *	err_stat:	 structure to report various VLAN table errors.
+ *                       will be updated if there are errors.
+ *
+ *
+ * Return
+ *	NPI_SUCCESS	Success
+ *
+ *
+ */
+npi_status_t
+npi_fflp_vlan_tbl_get_err_log(npi_handle_t handle, vlan_tbl_err_log_t *err_stat)
+{
+	vlan_par_err_t err_log;
+	uint64_t offset;
+
+
+	offset = FFLP_VLAN_PAR_ERR_REG;
+	err_log.value = 0;
+
+	REG_PIO_READ64(handle, offset, &err_log.value);
+
+	if (err_log.bits.ldw.err == BIT_ENABLE) {
+/* non-zero means err */
+		err_stat->err = BIT_ENABLE;
+		err_stat->multi = err_log.bits.ldw.m_err;
+		err_stat->addr = err_log.bits.ldw.addr;
+		err_stat->data = err_log.bits.ldw.data;
+/* now clear the error */
+		err_log.value = 0;
+		REG_PIO_WRITE64(handle, offset, err_log.value);
+
+	} else {
+		err_stat->err = 0;
+	}
+
+	return (NPI_SUCCESS);
+}
+
+
+/*
+ * npi_fflp_vlan_tbl_clr_err_log
+ * Clears VLAN Table PIO  error status.
+ * If there are VLAN Table errors as indicated by err bit set by HW,
+ *  then the SW will clear it by clearing the bit.
+ *
+ * Input
+ *	err_stat:	 structure to report various VLAN Table errors.
+ *                       will be updated if there are  errors.
+ *
+ *
+ * Return
+ *	NPI_SUCCESS	Success
+ *
+ *
+ */
+
+npi_status_t
+npi_fflp_vlan_tbl_clr_err_log(npi_handle_t handle)
+{
+	vlan_par_err_t err_log;
+	uint64_t offset;
+
+	offset = FFLP_VLAN_PAR_ERR_REG;
+	err_log.value = 0;
+
+	REG_PIO_WRITE64(handle, offset, err_log.value);
+
+	return (NPI_SUCCESS);
+}
+
+
+
+
+/*
+ * npi_fflp_cfg_enet_usr_cls_set()
+ * Configures a user configurable ethernet class
+ *
+ * Input
+ *      class:       Ethernet Class  class
+ *		     (TCAM_CLASS_ETYPE or  TCAM_CLASS_ETYPE_2)
+ *      enet_type:   16 bit Ethernet Type value, corresponding ethernet bytes
+ *                        [13:14] in the frame.
+ *
+ *  by default, the class will be disabled until explicitly enabled.
+ *
+ * Return
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ *
+ *
+ */
+
+npi_status_t
+npi_fflp_cfg_enet_usr_cls_set(npi_handle_t handle,
+			    tcam_class_t class, uint16_t enet_type)
+{
+	uint64_t offset;
+	tcam_class_prg_ether_t cls_cfg;
+	cls_cfg.value = 0x0;
+
+/* check if etype is valid */
+
+	if (!TCAM_L2_USR_CLASS_VALID(class)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_fflp_cfg_enet_usr_cls_set:"
+			" Invalid class %d \n",
+			class));
+		return (NPI_FFLP_TCAM_CLASS_INVALID);
+	}
+	offset = GET_TCAM_CLASS_OFFSET(class);
+
+/*
+ * etype check code
+ *
+ * if (check_fail)
+ *  return (NPI_FAILURE | NPI_SW_ERROR);
+ */
+
+	cls_cfg.bits.ldw.etype = enet_type;
+	cls_cfg.bits.ldw.valid = BIT_DISABLE;
+	REG_PIO_WRITE64(handle, offset, cls_cfg.value);
+	return (NPI_SUCCESS);
+}
+
+
+
+/*
+ * npi_fflp_cfg_enet_usr_cls_enable()
+ * Enable previously configured TCAM user configurable Ethernet classes.
+ *
+ * Input
+ *      class:       Ethernet Class  class
+ *		     (TCAM_CLASS_ETYPE or  TCAM_CLASS_ETYPE_2)
+ *
+ * Return
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+npi_status_t
+npi_fflp_cfg_enet_usr_cls_enable(npi_handle_t handle, tcam_class_t class)
+{
+	uint64_t offset;
+	tcam_class_prg_ether_t cls_cfg;
+
+	if (!TCAM_L2_USR_CLASS_VALID(class)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_fflp_cfg_enet_usr_cls_enable:"
+			" Invalid class %d \n",
+			class));
+		return (NPI_FFLP_TCAM_CLASS_INVALID);
+	}
+
+	offset = GET_TCAM_CLASS_OFFSET(class);
+
+	REG_PIO_READ64(handle, offset, &cls_cfg.value);
+	cls_cfg.bits.ldw.valid = BIT_ENABLE;
+	REG_PIO_WRITE64(handle, offset, cls_cfg.value);
+	return (NPI_SUCCESS);
+}
+
+
+/*
+ * npi_fflp_cfg_enet_usr_cls_disable()
+ * Disables previously configured TCAM user configurable Ethernet classes.
+ *
+ * Input
+ *      class:       Ethernet Class  class
+ *		     (TCAM_CLASS_ETYPE or  TCAM_CLASS_ETYPE_2)
+ *
+ * Return
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERROR
+ *
+ */
+
+
+npi_status_t
+npi_fflp_cfg_enet_usr_cls_disable(npi_handle_t handle, tcam_class_t class)
+{
+	uint64_t offset;
+	tcam_class_prg_ether_t cls_cfg;
+
+	if (!TCAM_L2_USR_CLASS_VALID(class)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_fflp_cfg_enet_usr_cls_disable:"
+			" Invalid class %d \n",
+			class));
+		return (NPI_FFLP_TCAM_CLASS_INVALID);
+	}
+
+	offset = GET_TCAM_CLASS_OFFSET(class);
+
+	REG_PIO_READ64(handle, offset, &cls_cfg.value);
+	cls_cfg.bits.ldw.valid = BIT_DISABLE;
+
+	REG_PIO_WRITE64(handle, offset, cls_cfg.value);
+	return (NPI_SUCCESS);
+}
+
+
+
+/*
+ * npi_fflp_cfg_ip_usr_cls_set()
+ * Configures the TCAM user configurable IP classes.
+ *
+ * Input
+ *      class:       IP Class  class
+ *		     (TCAM_CLASS_IP_USER_4 <= class <= TCAM_CLASS_IP_USER_7)
+ *      tos:         IP TOS bits
+ *      tos_mask:    IP TOS bits mask. bits with mask bits set will be used
+ *      proto:       IP Proto
+ *      ver:         IP Version
+ * by default, will the class is disabled until explicitly enabled
+ *
+ * Return
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERROR
+ * NPI_SW_ERROR
+ *
+ */
+
+
+npi_status_t
+npi_fflp_cfg_ip_usr_cls_set(npi_handle_t handle, tcam_class_t class,
+			    uint8_t tos, uint8_t tos_mask,
+			    uint8_t proto, uint8_t ver)
+{
+	uint64_t offset;
+	tcam_class_prg_ip_t ip_cls_cfg;
+
+	if (!TCAM_L3_USR_CLASS_VALID(class)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_fflp_cfg_ip_usr_cls_set:"
+			" Invalid class %d \n",
+			class));
+		return (NPI_FFLP_TCAM_CLASS_INVALID);
+	}
+
+	offset = GET_TCAM_CLASS_OFFSET(class);
+
+	ip_cls_cfg.bits.ldw.pid = proto;
+	ip_cls_cfg.bits.ldw.ipver = ver;
+	ip_cls_cfg.bits.ldw.tos = tos;
+	ip_cls_cfg.bits.ldw.tosmask = tos_mask;
+	ip_cls_cfg.bits.ldw.valid = 0;
+	REG_PIO_WRITE64(handle, offset, ip_cls_cfg.value);
+	return (NPI_SUCCESS);
+
+}
+
+/*
+ * npi_fflp_cfg_ip_usr_cls_enable()
+ * Enable previously configured TCAM user configurable IP classes.
+ *
+ * Input
+ *      class:       IP Class  class
+ *		     (TCAM_CLASS_IP_USER_4 <= class <= TCAM_CLASS_IP_USER_7)
+ *
+ * Return
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERROR
+ * NPI_SW_ERROR
+ *
+ */
+
+npi_status_t
+npi_fflp_cfg_ip_usr_cls_enable(npi_handle_t handle, tcam_class_t class)
+{
+	uint64_t offset;
+	tcam_class_prg_ip_t ip_cls_cfg;
+
+	if (!TCAM_L3_USR_CLASS_VALID(class)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_fflp_cfg_ip_usr_cls_enable:"
+			" Invalid class %d \n",
+			class));
+		return (NPI_FFLP_TCAM_CLASS_INVALID);
+	}
+
+	offset = GET_TCAM_CLASS_OFFSET(class);
+	REG_PIO_READ64(handle, offset, &ip_cls_cfg.value);
+	ip_cls_cfg.bits.ldw.valid = 1;
+
+	REG_PIO_WRITE64(handle, offset, ip_cls_cfg.value);
+	return (NPI_SUCCESS);
+
+}
+
+
+
+/*
+ * npi_fflp_cfg_ip_usr_cls_disable()
+ * Disables previously configured TCAM user configurable IP classes.
+ *
+ * Input
+ *      class:       IP Class  class
+ *		     (TCAM_CLASS_IP_USER_4 <= class <= TCAM_CLASS_IP_USER_7)
+ *
+ * Return
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERROR
+ * NPI_SW_ERROR
+ *
+ */
+
+
+npi_status_t
+npi_fflp_cfg_ip_usr_cls_disable(npi_handle_t handle, tcam_class_t class)
+{
+	uint64_t offset;
+	tcam_class_prg_ip_t ip_cls_cfg;
+
+	if (!TCAM_L3_USR_CLASS_VALID(class)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_fflp_cfg_ip_usr_cls_disable:"
+			" Invalid class %d \n",
+			class));
+		return (NPI_FFLP_TCAM_CLASS_INVALID);
+	}
+
+	offset = GET_TCAM_CLASS_OFFSET(class);
+
+	REG_PIO_READ64(handle, offset, &ip_cls_cfg.value);
+	ip_cls_cfg.bits.ldw.valid = 0;
+
+	REG_PIO_WRITE64(handle, offset, ip_cls_cfg.value);
+	return (NPI_SUCCESS);
+
+}
+
+
+/*
+ * npi_fflp_cfg_ip_cls_tcam_key ()
+ *
+ * Configures the TCAM key generation for the IP classes
+ *
+ * Input
+ *      l3_class:        IP class to configure key generation
+ *      cfg:             Configuration bits:
+ *                   discard:      Discard all frames of this class
+ *                   use_ip_saddr: use ip src address (for ipv6)
+ *                   use_ip_daddr: use ip dest address (for ipv6)
+ *                   lookup_enable: Enable Lookup
+ *
+ *
+ * Return
+ * NPI_SUCCESS
+ * NPI_HW_ERROR
+ * NPI_SW_ERROR
+ *
+ */
+
+
+npi_status_t
+npi_fflp_cfg_ip_cls_tcam_key(npi_handle_t handle,
+			    tcam_class_t l3_class, tcam_key_cfg_t *cfg)
+{
+	uint64_t offset;
+	tcam_class_key_ip_t tcam_cls_cfg;
+
+	if (!(TCAM_L3_CLASS_VALID(l3_class))) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_fflp_cfg_ip_cls_tcam_key:"
+			" Invalid class %d \n",
+			l3_class));
+		return (NPI_FFLP_TCAM_CLASS_INVALID);
+	}
+
+	if ((cfg->use_ip_daddr) &&
+		(cfg->use_ip_saddr == cfg->use_ip_daddr)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			    " npi_fflp_cfg_ip_cls_tcam_key:"
+			    " Invalid configuration %x for class %d \n",
+			    *cfg, l3_class));
+		return (NPI_FFLP_SW_PARAM_ERROR);
+	}
+
+
+	offset = GET_TCAM_KEY_OFFSET(l3_class);
+	tcam_cls_cfg.value = 0;
+
+	if (cfg->discard) {
+		tcam_cls_cfg.bits.ldw.discard = 1;
+	}
+
+	if (cfg->use_ip_saddr) {
+		tcam_cls_cfg.bits.ldw.ipaddr = 1;
+	}
+
+	if (cfg->use_ip_daddr) {
+		tcam_cls_cfg.bits.ldw.ipaddr = 0;
+	}
+
+	if (cfg->lookup_enable) {
+		tcam_cls_cfg.bits.ldw.tsel = 1;
+	}
+
+	REG_PIO_WRITE64(handle, offset, tcam_cls_cfg.value);
+	return (NPI_SUCCESS);
+}
+
+
+
+/*
+ * npi_fflp_cfg_ip_cls_flow_key ()
+ *
+ * Configures the flow key generation for the IP classes
+ * Flow key is used to generate the H1 hash function value
+ * The fields used for the generation are configured using this
+ * NPI function.
+ *
+ * Input
+ *      l3_class:        IP class to configure flow key generation
+ *      cfg:             Configuration bits:
+ *                   use_proto:     Use IP proto field
+ *                   use_dport:     use l4 destination port
+ *                   use_sport:     use l4 source port
+ *                   ip_opts_exist: IP Options Present
+ *                   use_daddr:     use ip dest address
+ *                   use_saddr:     use ip source address
+ *                   use_vlan:      use VLAN ID
+ *                   use_l2da:      use L2 Dest MAC Address
+ *                   use_portnum:   use L2 virtual port number
+ *
+ *
+ * Return
+ * NPI_SUCCESS
+ * NPI_HW_ERROR
+ * NPI_SW_ERROR
+ *
+ */
+
+
+
+npi_status_t
+npi_fflp_cfg_ip_cls_flow_key(npi_handle_t handle, tcam_class_t l3_class,
+							    flow_key_cfg_t *cfg)
+{
+	uint64_t offset;
+	flow_class_key_ip_t flow_cfg_reg;
+
+
+	if (!(TCAM_L3_CLASS_VALID(l3_class))) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_fflp_cfg_ip_cls_flow_key:"
+			" Invalid class %d \n",
+			l3_class));
+		return (NPI_FFLP_TCAM_CLASS_INVALID);
+	}
+
+
+	offset = GET_FLOW_KEY_OFFSET(l3_class);
+	flow_cfg_reg.value = 0; /* default */
+
+	if (cfg->use_proto) {
+		flow_cfg_reg.bits.ldw.proto = 1;
+	}
+
+	if (cfg->use_dport) {
+		flow_cfg_reg.bits.ldw.l4_1 = 2;
+		if (cfg->ip_opts_exist)
+			flow_cfg_reg.bits.ldw.l4_1 = 3;
+	}
+
+	if (cfg->use_sport) {
+		flow_cfg_reg.bits.ldw.l4_0 = 2;
+		if (cfg->ip_opts_exist)
+			flow_cfg_reg.bits.ldw.l4_0 = 3;
+	}
+
+	if (cfg->use_daddr) {
+		flow_cfg_reg.bits.ldw.ipda = BIT_ENABLE;
+	}
+
+	if (cfg->use_saddr) {
+		flow_cfg_reg.bits.ldw.ipsa = BIT_ENABLE;
+	}
+
+	if (cfg->use_vlan) {
+		flow_cfg_reg.bits.ldw.vlan = BIT_ENABLE;
+	}
+
+	if (cfg->use_l2da) {
+		flow_cfg_reg.bits.ldw.l2da = BIT_ENABLE;
+	}
+
+	if (cfg->use_portnum) {
+		flow_cfg_reg.bits.ldw.port = BIT_ENABLE;
+	}
+
+	REG_PIO_WRITE64(handle, offset, flow_cfg_reg.value);
+	return (NPI_SUCCESS);
+
+}
+
+
+npi_status_t
+npi_fflp_cfg_ip_cls_flow_key_get(npi_handle_t handle,
+				    tcam_class_t l3_class,
+				    flow_key_cfg_t *cfg)
+{
+	uint64_t offset;
+	flow_class_key_ip_t flow_cfg_reg;
+
+
+	if (!(TCAM_L3_CLASS_VALID(l3_class))) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_fflp_cfg_ip_cls_flow_key:"
+				    " Invalid class %d \n",
+				    l3_class));
+		return (NPI_FFLP_TCAM_CLASS_INVALID);
+	}
+
+	offset = GET_FLOW_KEY_OFFSET(l3_class);
+
+	cfg->use_proto = 0;
+	cfg->use_dport = 0;
+	cfg->use_sport = 0;
+	cfg->ip_opts_exist = 0;
+	cfg->use_daddr = 0;
+	cfg->use_saddr = 0;
+	cfg->use_vlan = 0;
+	cfg->use_l2da = 0;
+	cfg->use_portnum  = 0;
+
+	REG_PIO_READ64(handle, offset, &flow_cfg_reg.value);
+
+	if (flow_cfg_reg.bits.ldw.proto) {
+		cfg->use_proto = 1;
+	}
+
+	if (flow_cfg_reg.bits.ldw.l4_1 == 2) {
+		cfg->use_dport = 1;
+	}
+
+	if (flow_cfg_reg.bits.ldw.l4_1 == 3) {
+		cfg->use_dport = 1;
+		cfg->ip_opts_exist = 1;
+	}
+
+	if (flow_cfg_reg.bits.ldw.l4_0 == 2) {
+		cfg->use_sport = 1;
+	}
+
+	if (flow_cfg_reg.bits.ldw.l4_0 == 3) {
+		cfg->use_sport = 1;
+		cfg->ip_opts_exist = 1;
+	}
+
+	if (flow_cfg_reg.bits.ldw.ipda) {
+		cfg->use_daddr = 1;
+	}
+
+	if (flow_cfg_reg.bits.ldw.ipsa) {
+		cfg->use_saddr = 1;
+	}
+
+	if (flow_cfg_reg.bits.ldw.vlan) {
+		cfg->use_vlan = 1;
+	}
+
+	if (flow_cfg_reg.bits.ldw.l2da) {
+		cfg->use_l2da = 1;
+	}
+
+	if (flow_cfg_reg.bits.ldw.port) {
+		cfg->use_portnum = 1;
+	}
+
+	NPI_DEBUG_MSG((handle.function, NPI_FFLP_CTL,
+			    " npi_fflp_cfg_ip_cls_flow_get %llx \n",
+			    flow_cfg_reg.value));
+
+	return (NPI_SUCCESS);
+
+}
+
+
+npi_status_t
+npi_fflp_cfg_ip_cls_tcam_key_get(npi_handle_t handle,
+			    tcam_class_t l3_class, tcam_key_cfg_t *cfg)
+{
+	uint64_t offset;
+	tcam_class_key_ip_t tcam_cls_cfg;
+
+	if (!(TCAM_L3_CLASS_VALID(l3_class))) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_fflp_cfg_ip_cls_tcam_key_get:"
+				    " Invalid class %d \n",
+				    l3_class));
+		return (NPI_FFLP_TCAM_CLASS_INVALID);
+	}
+
+
+	offset = GET_TCAM_KEY_OFFSET(l3_class);
+
+	REG_PIO_READ64(handle, offset, &tcam_cls_cfg.value);
+
+	cfg->discard = 0;
+	cfg->use_ip_saddr = 0;
+	cfg->use_ip_daddr = 1;
+	cfg->lookup_enable = 0;
+
+	if (tcam_cls_cfg.bits.ldw.discard)
+			cfg->discard = 1;
+
+	if (tcam_cls_cfg.bits.ldw.ipaddr) {
+		cfg->use_ip_saddr = 1;
+		cfg->use_ip_daddr = 0;
+	}
+
+	if (tcam_cls_cfg.bits.ldw.tsel) {
+		cfg->lookup_enable	= 1;
+	}
+
+	NPI_DEBUG_MSG((handle.function, NPI_CTL,
+				    " npi_fflp_cfg_ip_cls_tcam_key_get %llx \n",
+				    tcam_cls_cfg.value));
+	return (NPI_SUCCESS);
+}
+
+
+/*
+ * npi_fflp_cfg_fcram_access ()
+ *
+ * Sets the ratio between the FCRAM pio and lookup access
+ * Input:
+ * access_ratio: 0  Lookup has the highest priority
+ *		 15 PIO has maximum possible priority
+ *
+ * Return
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+npi_status_t
+npi_fflp_cfg_fcram_access(npi_handle_t handle, uint8_t access_ratio)
+{
+
+	fflp_cfg_1_t fflp_cfg;
+	uint64_t offset;
+	offset = FFLP_CFG_1_REG;
+	if (access_ratio > 0xf) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_fflp_cfg_fcram_access:"
+			" Invalid access ratio %d \n",
+			access_ratio));
+		return (NPI_FFLP_ERROR | NPI_FFLP_SW_PARAM_ERROR);
+	}
+
+	REG_PIO_READ64(handle, offset, &fflp_cfg.value);
+	fflp_cfg.bits.ldw.fflpinitdone = 0;
+	fflp_cfg.bits.ldw.fcramratio = access_ratio;
+	REG_PIO_WRITE64(handle, offset, fflp_cfg.value);
+	REG_PIO_READ64(handle, offset, &fflp_cfg.value);
+	fflp_cfg.bits.ldw.fflpinitdone = 1;
+	REG_PIO_WRITE64(handle, offset, fflp_cfg.value);
+	return (NPI_SUCCESS);
+
+}
+
+
+/*
+ * npi_fflp_cfg_tcam_access ()
+ *
+ * Sets the ratio between the TCAM pio and lookup access
+ * Input:
+ * access_ratio: 0  Lookup has the highest priority
+ *		 15 PIO has maximum possible priority
+ * Return
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+npi_status_t
+npi_fflp_cfg_tcam_access(npi_handle_t handle, uint8_t access_ratio)
+{
+
+	fflp_cfg_1_t fflp_cfg;
+	uint64_t offset;
+	offset = FFLP_CFG_1_REG;
+	if (access_ratio > 0xf) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_fflp_cfg_tcram_access:"
+			" Invalid access ratio %d \n",
+			access_ratio));
+		return (NPI_FFLP_ERROR | NPI_FFLP_SW_PARAM_ERROR);
+	}
+
+	REG_PIO_READ64(handle, offset, &fflp_cfg.value);
+	fflp_cfg.bits.ldw.fflpinitdone = 0;
+	fflp_cfg.bits.ldw.camratio = access_ratio;
+/* since the cam latency is fixed, we might set it here */
+	fflp_cfg.bits.ldw.camlatency = TCAM_DEFAULT_LATENCY;
+	REG_PIO_WRITE64(handle, offset, fflp_cfg.value);
+	REG_PIO_READ64(handle, offset, &fflp_cfg.value);
+	fflp_cfg.bits.ldw.fflpinitdone = 1;
+	REG_PIO_WRITE64(handle, offset, fflp_cfg.value);
+	return (NPI_SUCCESS);
+
+}
+
+
+
+
+
+/*
+ * npi_fflp_cfg_hash_h1poly()
+ * Initializes the H1 hash generation logic.
+ *
+ * Input
+ *      init_value:       The initial value (seed)
+ *
+ * Return
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+npi_status_t
+npi_fflp_cfg_hash_h1poly(npi_handle_t handle, uint32_t init_value)
+{
+
+
+	hash_h1poly_t h1_cfg;
+	uint64_t offset;
+	offset = FFLP_H1POLY_REG;
+
+	h1_cfg.value = 0;
+	h1_cfg.bits.ldw.init_value = init_value;
+
+	REG_PIO_WRITE64(handle, offset, h1_cfg.value);
+	return (NPI_SUCCESS);
+}
+
+
+
+/*
+ * npi_fflp_cfg_hash_h2poly()
+ * Initializes the H2 hash generation logic.
+ *
+ * Input
+ *      init_value:       The initial value (seed)
+ *
+ * Return
+ * NPI_SUCCESS
+ *
+ */
+
+npi_status_t
+npi_fflp_cfg_hash_h2poly(npi_handle_t handle, uint16_t init_value)
+{
+
+
+	hash_h2poly_t h2_cfg;
+	uint64_t offset;
+	offset = FFLP_H2POLY_REG;
+
+	h2_cfg.value = 0;
+	h2_cfg.bits.ldw.init_value = init_value;
+
+	REG_PIO_WRITE64(handle, offset, h2_cfg.value);
+	return (NPI_SUCCESS);
+
+
+}
+
+
+
+/*
+ *  npi_fflp_cfg_reset
+ *  Initializes the FCRAM reset sequence.
+ *
+ *  Input
+ *	strength:		FCRAM Drive strength
+ *				   strong, weak or normal
+ *				   HW recommended value:
+ *	qs:			FCRAM QS mode selection
+ *				   qs mode or free running
+ *				   HW recommended value is:
+ * type:       reset type:
+ *             FFLP_ONLY
+ *             FFLP_FCRAM
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+npi_status_t
+npi_fflp_cfg_fcram_reset(npi_handle_t handle,
+			    fflp_fcram_output_drive_t strength,
+			    fflp_fcram_qs_t qs)
+
+{
+
+	fflp_cfg_1_t fflp_cfg;
+	uint64_t offset;
+	offset = FFLP_CFG_1_REG;
+
+
+	/* These bits have to be configured before FCRAM reset is issued */
+	fflp_cfg.value = 0;
+	fflp_cfg.bits.ldw.pio_fio_rst = 1;
+	REG_PIO_WRITE64(handle, offset, fflp_cfg.value);
+
+	NXGE_DELAY(5); /* TODO: What is the correct delay? */
+
+	fflp_cfg.bits.ldw.pio_fio_rst = 0;
+	REG_PIO_WRITE64(handle, offset, fflp_cfg.value);
+	fflp_cfg.bits.ldw.fcramqs = qs;
+	fflp_cfg.bits.ldw.fcramoutdr = strength;
+	fflp_cfg.bits.ldw.fflpinitdone = 1;
+	REG_PIO_WRITE64(handle, offset, fflp_cfg.value);
+
+	return (NPI_SUCCESS);
+
+}
+
+
+
+
+npi_status_t
+npi_fflp_cfg_init_done(npi_handle_t handle)
+
+{
+
+	fflp_cfg_1_t fflp_cfg;
+	uint64_t offset;
+	offset = FFLP_CFG_1_REG;
+
+	REG_PIO_READ64(handle, offset, &fflp_cfg.value);
+	fflp_cfg.bits.ldw.fflpinitdone = 1;
+	REG_PIO_WRITE64(handle, offset, fflp_cfg.value);
+	return (NPI_SUCCESS);
+
+}
+
+
+
+
+npi_status_t
+npi_fflp_cfg_init_start(npi_handle_t handle)
+
+{
+
+	fflp_cfg_1_t fflp_cfg;
+	uint64_t offset;
+	offset = FFLP_CFG_1_REG;
+
+	REG_PIO_READ64(handle, offset, &fflp_cfg.value);
+	fflp_cfg.bits.ldw.fflpinitdone = 0;
+	REG_PIO_WRITE64(handle, offset, fflp_cfg.value);
+	return (NPI_SUCCESS);
+
+}
+
+
+
+/*
+ * Enables the TCAM search function.
+ *
+ */
+
+npi_status_t
+npi_fflp_cfg_tcam_enable(npi_handle_t handle)
+
+{
+
+	fflp_cfg_1_t fflp_cfg;
+	uint64_t offset;
+	offset = FFLP_CFG_1_REG;
+	REG_PIO_READ64(handle, offset, &fflp_cfg.value);
+	fflp_cfg.bits.ldw.tcam_disable = 0;
+	REG_PIO_WRITE64(handle, offset, fflp_cfg.value);
+	return (NPI_SUCCESS);
+
+}
+
+/*
+ * Disables the TCAM search function.
+ * While the TCAM is in disabled state, all TCAM matches would return NO_MATCH
+ *
+ */
+
+npi_status_t
+npi_fflp_cfg_tcam_disable(npi_handle_t handle)
+
+{
+
+	fflp_cfg_1_t fflp_cfg;
+	uint64_t offset;
+	offset = FFLP_CFG_1_REG;
+	REG_PIO_READ64(handle, offset, &fflp_cfg.value);
+	fflp_cfg.bits.ldw.tcam_disable = 1;
+	REG_PIO_WRITE64(handle, offset, fflp_cfg.value);
+	return (NPI_SUCCESS);
+
+}
+
+
+/*
+ * npi_rxdma_event_mask_config():
+ *	This function is called to operate on the event mask
+ *	register which is used for generating interrupts
+ *	and status register.
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	op_mode		- OP_GET: get hardware event mask
+ *			  OP_SET: set hardware interrupt event masks
+ *	channel		- hardware RXDMA channel from 0 to 23.
+ *	cfgp		- pointer to NPI defined event mask
+ *			  enum data type.
+ * Return:
+ *	NPI_SUCCESS		- If set is complete successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE		-
+ *	NPI_FFLP_ERROR | NPI_FFLP_SW_PARAM_ERROR
+ *
+ */
+npi_status_t
+npi_fflp_event_mask_config(npi_handle_t handle, io_op_t op_mode,
+		fflp_event_mask_cfg_t *mask_cfgp)
+{
+	int		status = NPI_SUCCESS;
+	fflp_err_mask_t mask_reg;
+
+	switch (op_mode) {
+	case OP_GET:
+
+		REG_PIO_READ64(handle, FFLP_ERR_MSK_REG, &mask_reg.value);
+		*mask_cfgp = mask_reg.value & FFLP_ERR_MASK_ALL;
+		break;
+
+	case OP_SET:
+		mask_reg.value = (~(*mask_cfgp) & FFLP_ERR_MASK_ALL);
+		REG_PIO_WRITE64(handle, FFLP_ERR_MSK_REG, mask_reg.value);
+		break;
+
+	case OP_UPDATE:
+		REG_PIO_READ64(handle, FFLP_ERR_MSK_REG, &mask_reg.value);
+		mask_reg.value |=  (~(*mask_cfgp) & FFLP_ERR_MASK_ALL);
+		REG_PIO_WRITE64(handle, FFLP_ERR_MSK_REG, mask_reg.value);
+		break;
+
+	case OP_CLEAR:
+		mask_reg.value = FFLP_ERR_MASK_ALL;
+		REG_PIO_WRITE64(handle, FFLP_ERR_MSK_REG, mask_reg.value);
+		break;
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+		    " npi_fflp_event_mask_config",
+		    " eventmask <0x%x>", op_mode));
+		return (NPI_FFLP_ERROR | NPI_FFLP_SW_PARAM_ERROR);
+	}
+
+	return (status);
+}
+
+/* Read vlan error bits */
+
+void
+npi_fflp_vlan_error_get(npi_handle_t handle, p_vlan_par_err_t p_err)
+{
+	REG_PIO_READ64(handle, FFLP_VLAN_PAR_ERR_REG, &p_err->value);
+}
+
+/* clear vlan error bits */
+void
+npi_fflp_vlan_error_clear(npi_handle_t handle)
+{
+	vlan_par_err_t p_err;
+	p_err.value  = 0;
+	p_err.bits.ldw.m_err = 0;
+	p_err.bits.ldw.err = 0;
+	REG_PIO_WRITE64(handle, FFLP_ERR_MSK_REG, p_err.value);
+
+}
+
+
+/* Read TCAM error bits */
+
+void
+npi_fflp_tcam_error_get(npi_handle_t handle, p_tcam_err_t p_err)
+{
+	REG_PIO_READ64(handle, FFLP_TCAM_ERR_REG, &p_err->value);
+}
+
+/* clear TCAM error bits */
+void
+npi_fflp_tcam_error_clear(npi_handle_t handle)
+{
+	tcam_err_t p_err;
+	p_err.value  = 0;
+	p_err.bits.ldw.p_ecc = 0;
+	p_err.bits.ldw.mult = 0;
+	p_err.bits.ldw.err = 0;
+	REG_PIO_WRITE64(handle, FFLP_TCAM_ERR_REG, p_err.value);
+
+}
+
+
+/* Read FCRAM error bits */
+
+void
+npi_fflp_fcram_error_get(npi_handle_t handle,
+		    p_hash_tbl_data_log_t p_err, uint8_t partition)
+{
+	uint64_t offset;
+	offset = FFLP_HASH_TBL_DATA_LOG_REG + partition * 8192;
+	REG_PIO_READ64(handle, offset, &p_err->value);
+}
+
+/* clear FCRAM error bits */
+void
+npi_fflp_fcram_error_clear(npi_handle_t handle, uint8_t partition)
+{
+	hash_tbl_data_log_t p_err;
+	uint64_t offset;
+	p_err.value  = 0;
+	p_err.bits.ldw.pio_err = 0;
+	offset = FFLP_HASH_TBL_DATA_LOG_REG + partition * 8192;
+
+	REG_PIO_WRITE64(handle, offset,
+			    p_err.value);
+
+}
+
+
+/* Read FCRAM lookup error log1 bits */
+
+void
+npi_fflp_fcram_error_log1_get(npi_handle_t handle,
+			    p_hash_lookup_err_log1_t log1)
+{
+	REG_PIO_READ64(handle, HASH_LKUP_ERR_LOG1_REG,
+				    &log1->value);
+}
+
+
+
+/* Read FCRAM lookup error log2 bits */
+
+void
+npi_fflp_fcram_error_log2_get(npi_handle_t handle,
+		    p_hash_lookup_err_log2_t log2)
+{
+	REG_PIO_READ64(handle, HASH_LKUP_ERR_LOG2_REG,
+			    &log2->value);
+}
diff --git a/drivers/net/nxge/npi/npi_fflp.h b/drivers/net/nxge/npi/npi_fflp.h
new file mode 100644
index 0000000..7584a36
--- /dev/null
+++ b/drivers/net/nxge/npi/npi_fflp.h
@@ -0,0 +1,1199 @@
+/*
+ * npi_fflp.h	Neptune  FFLP Classifier HW API functions
+ *
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef _NPI_FFLP_H
+#define	_NPI_FFLP_H
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+
+#include <npi.h>
+#include <nxge_fflp_hw.h>
+#include <nxge_fflp.h>
+
+
+typedef uint8_t part_id_t;
+typedef uint8_t tcam_location_t;
+typedef uint16_t vlan_id_t;
+
+typedef	enum _tcam_op {
+	TCAM_RWC_STAT	= 0x1,
+	TCAM_RWC_MATCH	= 0x2
+} tcam_op_t;
+
+
+#define	NPI_TCAM_COMP_NO_MATCH	0x8000000000000ULL
+
+/*
+ * NPI FFLP ERROR Codes
+ */
+
+#define	NPI_FFLP_BLK_CODE	FFLP_BLK_ID << 8
+#define	NPI_FFLP_ERROR		(NPI_FAILURE | NPI_FFLP_BLK_CODE)
+#define	NPI_TCAM_ERROR		0x10
+#define	NPI_FCRAM_ERROR		0x20
+#define	NPI_GEN_FFLP		0x30
+#define	NPI_FFLP_SW_PARAM_ERROR	0x40
+#define	NPI_FFLP_HW_ERROR	0x80
+
+
+#define	NPI_FFLP_RESET_ERROR	(NPI_FFLP_ERROR | NPI_GEN_FFLP | RESET_FAILED)
+#define	NPI_FFLP_RDC_TABLE_INVALID	(NPI_FFLP_ERROR | RDC_TAB_INVALID)
+#define	NPI_FFLP_VLAN_INVALID		(NPI_FFLP_ERROR | VLAN_INVALID)
+#define	NPI_FFLP_PORT_INVALID		(NPI_FFLP_ERROR | PORT_INVALID)
+#define	NPI_FFLP_TCAM_RD_ERROR		\
+	(NPI_FFLP_ERROR | NPI_TCAM_ERROR | READ_FAILED)
+#define	NPI_FFLP_TCAM_WR_ERROR		\
+	(NPI_FFLP_ERROR | NPI_TCAM_ERROR | WRITE_FAILED)
+#define	NPI_FFLP_TCAM_LOC_INVALID	\
+	(NPI_FFLP_ERROR | NPI_TCAM_ERROR | LOCATION_INVALID)
+#define	NPI_FFLP_ASC_RAM_RD_ERROR	\
+	(NPI_FFLP_ERROR | NPI_TCAM_ERROR | READ_FAILED)
+#define	NPI_FFLP_ASC_RAM_WR_ERROR	\
+	(NPI_FFLP_ERROR | NPI_TCAM_ERROR | WRITE_FAILED)
+#define	NPI_FFLP_FCRAM_READ_ERROR	\
+	(NPI_FFLP_ERROR | NPI_FCRAM_ERROR | READ_FAILED)
+#define	NPI_FFLP_FCRAM_WR_ERROR		\
+	(NPI_FFLP_ERROR | NPI_FCRAM_ERROR | WRITE_FAILED)
+#define	NPI_FFLP_FCRAM_PART_INVALID	\
+	(NPI_FFLP_ERROR | NPI_FCRAM_ERROR | RDC_TAB_INVALID)
+#define	NPI_FFLP_FCRAM_LOC_INVALID	\
+	(NPI_FFLP_ERROR | NPI_FCRAM_ERROR | LOCATION_INVALID)
+
+#define	TCAM_CLASS_INVALID		\
+	(NPI_FFLP_SW_PARAM_ERROR | 0xb)
+/* have only 0xc, 0xd, 0xe and 0xf left for sw error codes */
+#define	NPI_FFLP_TCAM_CLASS_INVALID	\
+	(NPI_FFLP_ERROR | NPI_TCAM_ERROR | TCAM_CLASS_INVALID)
+#define	NPI_FFLP_TCAM_HW_ERROR		\
+	(NPI_FFLP_ERROR | NPI_FFLP_HW_ERROR | NPI_TCAM_ERROR)
+#define	NPI_FFLP_FCRAM_HW_ERROR		\
+	(NPI_FFLP_ERROR | NPI_FFLP_HW_ERROR | NPI_FCRAM_ERROR)
+
+
+/*
+ * FFLP NPI defined event masks (mapped to the hardware defined masks).
+ */
+typedef	enum _fflp_event_mask_cfg_e {
+	CFG_FFLP_ENT_MSK_VLAN_MASK = FFLP_ERR_VLAN_MASK,
+	CFG_FFLP_ENT_MSK_TCAM_MASK = FFLP_ERR_TCAM_MASK,
+	CFG_FFLP_ENT_MSK_HASH_TBL_LKUP_MASK = FFLP_ERR_HASH_TBL_LKUP_MASK,
+	CFG_FFLP_ENT_MSK_HASH_TBL_DAT_MASK = FFLP_ERR_HASH_TBL_DAT_MASK,
+
+	CFG_FFLP_MASK_ALL	= (FFLP_ERR_VLAN_MASK | FFLP_ERR_TCAM_MASK |
+						FFLP_ERR_HASH_TBL_LKUP_MASK |
+						FFLP_ERR_HASH_TBL_DAT_MASK)
+} fflp_event_mask_cfg_t;
+
+
+/* FFLP FCRAM Related Functions */
+/* The following are FCRAM datapath functions */
+
+/*
+ * npi_fflp_fcram_entry_write ()
+ * Populates an FCRAM entry
+ * Inputs:
+ *         handle:	opaque handle interpreted by the underlying OS
+ *	   partid:	Partition ID
+ *	   location:	Index to the FCRAM.
+ *			Corresponds to last 20 bits of H1 value
+ *	   fcram_ptr:	Pointer to the FCRAM contents to be used for writing
+ *	   format:	Entry Format. Determines the size of the write.
+ *			      FCRAM_ENTRY_OPTIM:   8 bytes (a 64 bit write)
+ *			      FCRAM_ENTRY_EX_IP4:  32 bytes (4 X 64 bit write)
+ *			      FCRAM_ENTRY_EX_IP6:  56 bytes (7 X 64 bit write)
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+npi_status_t npi_fflp_fcram_entry_write(npi_handle_t, part_id_t,
+			    uint32_t, fcram_entry_t *,
+			    fcram_entry_format_t);
+
+/*
+ * npi_fflp_fcram_entry_read ()
+ * Reads an FCRAM entry
+ * Inputs:
+ *         handle:	opaque handle interpreted by the underlying OS
+ *	   partid:	Partition ID
+ *	   location:	Index to the FCRAM.
+ *			Corresponds to last 20 bits of H1 value
+ *	   fcram_ptr:	Pointer to the FCRAM contents to be updated
+ *	   format:	Entry Format. Determines the size of the read.
+ *			      FCRAM_ENTRY_OPTIM:   8 bytes (a 64 bit read)
+ *			      FCRAM_ENTRY_EX_IP4:  32 bytes (4 X 64 bit read )
+ *			      FCRAM_ENTRY_EX_IP6:  56 bytes (7 X 64 bit read )
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ */
+
+npi_status_t npi_fflp_fcram_entry_read(npi_handle_t,  part_id_t,
+				    uint32_t, fcram_entry_t *,
+				    fcram_entry_format_t);
+
+/*
+ * npi_fflp_fcram_entry_invalidate ()
+ * Invalidate FCRAM entry at the given location
+ * Inputs:
+ *	handle:		opaque handle interpreted by the underlying OS
+ *	partid:		Partition ID
+ *	location:	location of the FCRAM/hash entry.
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+npi_status_t
+npi_fflp_fcram_entry_invalidate(npi_handle_t, part_id_t,
+				    uint32_t);
+
+/*
+ * npi_fflp_fcram_subarea_write ()
+ * Writes to FCRAM entry subarea i.e the 8 bytes within the 64 bytes pointed by
+ * last 20 bits of  H1. Effectively, this accesses specific 8 bytes within the
+ * hash table bucket.
+ *
+ *    |-----------------| <-- H1
+ *	   |	subarea 0    |
+ *	   |_________________|
+ *	   | Subarea 1	     |
+ *	   |_________________|
+ *	   | .......	     |
+ *	   |_________________|
+ *	   | Subarea 7       |
+ *	   |_________________|
+ *
+ * Inputs:
+ *         handle:	opaque handle interpreted by the underlying OS
+ *	   partid:	Partition ID
+ *	   location:	location of the subarea. It is derived from:
+ *			Bucket = [19:15][14:0]       (20 bits of H1)
+ *			location = (Bucket << 3 ) + subarea * 8
+ *				 = [22:18][17:3] || subarea * 8
+ *	   data:	Data
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+
+npi_status_t npi_fflp_fcram_subarea_write(npi_handle_t, part_id_t,
+				    uint32_t, uint64_t);
+/*
+ * npi_fflp_fcram_subarea_read ()
+ * Reads an FCRAM entry subarea i.e the 8 bytes within the 64 bytes pointed by
+ * last 20 bits of  H1. Effectively, this accesses specific 8 bytes within the
+ * hash table bucket.
+ *
+ *  H1-->  |-----------------|
+ *	   |	subarea 0    |
+ *	   |_________________|
+ *	   | Subarea 1	     |
+ *	   |_________________|
+ *	   | .......	     |
+ *	   |_________________|
+ *	   | Subarea 7       |
+ *	   |_________________|
+ *
+ * Inputs:
+ *         handle:	opaque handle interpreted by the underlying OS
+ *	   partid:	Partition ID
+ *	   location:	location of the subarea. It is derived from:
+ *			Bucket = [19:15][14:0]       (20 bits of H1)
+ *			location = (Bucket << 3 ) + subarea * 8
+ *				 = [22:18][17:3] || subarea * 8
+ *	   data:	ptr do write subarea contents to.
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+npi_status_t npi_fflp_fcram_subarea_read  (npi_handle_t,
+			part_id_t, uint32_t, uint64_t *);
+
+
+/* The following are zero function fflp configuration functions */
+/*
+ * npi_fflp_fcram_config_partition()
+ * Partitions and configures the FCRAM
+ *
+ * Input
+ *     partid			partition ID
+ *				Corresponds to the RDC table
+ *     part_size		Size of the partition
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+npi_status_t npi_fflp_cfg_fcram_partition(npi_handle_t, part_id_t,
+				uint8_t, uint8_t);
+
+/*
+ * npi_fflp_fcram_partition_enable
+ * Enable previously configured FCRAM partition
+ *
+ * Input
+ *     partid			partition ID
+ *				Corresponds to the RDC table
+ *
+ * Return
+ *      0			Successful
+ *      Non zero  error code    Enable failed, and reason.
+ *
+ */
+npi_status_t npi_fflp_cfg_fcram_partition_enable(npi_handle_t,
+				part_id_t);
+
+/*
+ * npi_fflp_fcram_partition_disable
+ * Disable previously configured FCRAM partition
+ *
+ * Input
+ *     partid			partition ID
+ *				Corresponds to the RDC table
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+npi_status_t npi_fflp_cfg_fcram_partition_disable(npi_handle_t,
+				part_id_t);
+
+
+/*
+ *  npi_fflp_cfg_fcram_reset
+ *  Initializes the FCRAM reset sequence (including FFLP).
+ *
+ *  Input
+ *	strength:		FCRAM Drive strength
+ *				   strong, weak or normal
+ *				   HW recommended value:
+ *	qs:			FCRAM QS mode selection
+ *				   qs mode or free running
+ *				   HW recommended value is:
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+npi_status_t npi_fflp_cfg_fcram_reset(npi_handle_t,
+				    fflp_fcram_output_drive_t,
+				    fflp_fcram_qs_t);
+
+
+
+/*
+ *  npi_fflp_cfg_tcam_reset
+ *  Initializes the FFLP reset sequence
+ * Doesn't configure the FCRAM params.
+ *
+ *  Input
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+npi_status_t npi_fflp_cfg_tcam_reset(npi_handle_t);
+
+/*
+ *  npi_fflp_cfg_tcam_enable
+ *  Enables the TCAM function
+ *
+ *  Input
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+npi_status_t npi_fflp_cfg_tcam_enable(npi_handle_t);
+
+/*
+ *  npi_fflp_cfg_tcam_disable
+ *  Enables the TCAM function
+ *
+ *  Input
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+npi_status_t npi_fflp_cfg_tcam_disable(npi_handle_t);
+
+
+/*
+ *  npi_fflp_cfg_cam_errorcheck_disable
+ *  Disables FCRAM and TCAM error checking
+ *
+ *  Input
+ *
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+npi_status_t npi_fflp_cfg_cam_errorcheck_disable(npi_handle_t);
+
+/*
+ *  npi_fflp_cfg_cam_errorcheck_enable
+ *  Enables FCRAM and TCAM error checking
+ *
+ *  Input
+ *
+ *
+ *  Return
+ *      0			Successful
+ *      Non zero  error code    Enable failed, and reason.
+ *
+ */
+npi_status_t npi_fflp_cfg_cam_errorcheck_enable(npi_handle_t);
+
+
+/*
+ *  npi_fflp_cfg_llcsnap_enable
+ *  Enables input parser llcsnap recognition
+ *
+ *  Input
+ *
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ *
+ */
+npi_status_t npi_fflp_cfg_llcsnap_enable(npi_handle_t);
+
+/*
+ *  npi_fflp_cam_llcsnap_disable
+ *  Disables input parser llcsnap recognition
+ *
+ *  Input
+ *
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ *
+ */
+
+npi_status_t npi_fflp_cfg_llcsnap_disable(npi_handle_t);
+
+/*
+ * npi_fflp_config_fcram_refresh
+ * Set FCRAM min and max refresh time.
+ *
+ * Input
+ *	min_time		Minimum Refresh time count
+ *	max_time		maximum Refresh Time count
+ *	sys_time		System Clock rate
+ *
+ *	The counters are 16 bit counters. The maximum refresh time is
+ *      3.9us/clock cycle. The minimum is 400ns/clock cycle.
+ *	Clock cycle is the FCRAM clock cycle?????
+ *	If the cycle is FCRAM clock cycle, then sys_time parameter
+ *      is not needed as there wont be configuration variation due to
+ *      system clock cycle.
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+npi_status_t npi_fflp_cfg_fcram_refresh_time(npi_handle_t,
+		uint32_t, uint32_t, uint32_t);
+
+
+/*
+ * npi_fflp_cfg_fcram_access ()
+ *
+ * Sets the ratio between the FCRAM pio and lookup access
+ * Input:
+ * access_ratio: 0  Lookup has the highest priority
+ *		 15 PIO has maximum possible priority
+ *
+ */
+
+npi_status_t npi_fflp_cfg_fcram_access(npi_handle_t,
+					uint8_t);
+
+
+/*
+ * npi_fflp_cfg_tcam_access ()
+ *
+ * Sets the ratio between the TCAM pio and lookup access
+ * Input:
+ * access_ratio: 0  Lookup has the highest priority
+ *		 15 PIO has maximum possible priority
+ *
+ */
+
+npi_status_t npi_fflp_cfg_tcam_access(npi_handle_t, uint8_t);
+
+
+/*
+ *  npi_fflp_hash_lookup_err_report
+ *  Reports hash table (fcram) lookup errors
+ *
+ *  Input
+ *      status			Pointer to return Error bits
+ *
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+npi_status_t npi_fflp_fcram_get_lookup_err_log(npi_handle_t,
+				    hash_lookup_err_log_t *);
+
+
+
+/*
+ * npi_fflp_fcram_get_pio_err_log
+ * Reports hash table PIO read errors.
+ *
+ * Input
+ *	partid:		partition ID
+ *      err_stat	pointer to return Error bits
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+npi_status_t npi_fflp_fcram_get_pio_err_log(npi_handle_t,
+				part_id_t, hash_pio_err_log_t *);
+
+
+/*
+ * npi_fflp_fcram_clr_pio_err_log
+ * Clears FCRAM PIO  error status for the partition.
+ * If there are TCAM errors as indicated by err bit set by HW,
+ *  then the SW will clear it by clearing the bit.
+ *
+ * Input
+ *	partid:		partition ID
+ *
+ *
+ * Return
+ *	NPI_SUCCESS	Success
+ *
+ *
+ */
+
+npi_status_t npi_fflp_fcram_clr_pio_err_log(npi_handle_t,
+						part_id_t);
+
+
+
+/*
+ * npi_fflp_fcram_err_data_test
+ * Tests the FCRAM error detection logic.
+ * The error detection logic for the datapath is tested.
+ * bits [63:0] are set to select the data bits to be xored
+ *
+ * Input
+ *	data:	 data bits to select bits to be xored
+ *
+ *
+ * Return
+ *	NPI_SUCCESS	Success
+ *
+ *
+ */
+npi_status_t npi_fflp_fcram_err_data_test(npi_handle_t, fcram_err_data_t *);
+
+
+/*
+ * npi_fflp_fcram_err_synd_test
+ * Tests the FCRAM error detection logic.
+ * The error detection logic for the syndrome is tested.
+ * tst0->synd (8bits) are set to select the syndrome bits
+ * to be XOR'ed
+ *
+ * Input
+ *	syndrome_bits:	 Syndrome bits to select bits to be xor'ed
+ *
+ *
+ * Return
+ *	NPI_SUCCESS	Success
+ *
+ *
+ */
+npi_status_t npi_fflp_fcram_err_synd_test(npi_handle_t, uint8_t);
+
+
+/*
+ * npi_fflp_cfg_vlan_table_clear
+ * Clears the vlan RDC table
+ *
+ * Input
+ *     vlan_id		VLAN ID
+ *
+ * Output
+ *
+ *	NPI_SUCCESS			Successful
+ *
+ */
+
+npi_status_t npi_fflp_cfg_vlan_table_clear(npi_handle_t, vlan_id_t);
+
+/*
+ * npi_fflp_cfg_enet_vlan_table_assoc
+ * associates port vlan id to rdc table and sets the priority
+ * in respect to L2DA rdc table.
+ *
+ * Input
+ *     mac_portn		port number
+ *     vlan_id			VLAN ID
+ *     rdc_table		RDC Table #
+ *     priority			priority
+ *				1: vlan classification has higher priority
+ *				0: l2da classification has higher priority
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+npi_status_t npi_fflp_cfg_enet_vlan_table_assoc(npi_handle_t,
+				    uint8_t, vlan_id_t,
+				    uint8_t, uint8_t);
+
+
+/*
+ * npi_fflp_cfg_enet_vlan_table_set_pri
+ * sets the  vlan based classification priority in respect to
+ * L2DA classification.
+ *
+ * Input
+ *     mac_portn	port number
+ *     vlan_id		VLAN ID
+ *     priority 	priority
+ *			1: vlan classification has higher priority
+ *			0: l2da classification has higher priority
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+npi_status_t npi_fflp_cfg_enet_vlan_table_set_pri(npi_handle_t,
+				    uint8_t, vlan_id_t,
+				    uint8_t);
+
+/*
+ * npi_fflp_cfg_enet_usr_cls_set()
+ * Configures a user configurable ethernet class
+ *
+ * Input
+ *      class:       Ethernet Class
+ *		     class (TCAM_CLASS_ETYPE or  TCAM_CLASS_ETYPE_2)
+ *      enet_type:   16 bit Ethernet Type value, corresponding ethernet bytes
+ *                        [13:14] in the frame.
+ *
+ *  by default, the class will be disabled until explicitly enabled.
+ *
+ * Return
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ *
+ *
+ */
+
+npi_status_t npi_fflp_cfg_enet_usr_cls_set(npi_handle_t,
+				    tcam_class_t, uint16_t);
+
+/*
+ * npi_fflp_cfg_enet_usr_cls_enable()
+ * Enable previously configured TCAM user configurable Ethernet classes.
+ *
+ * Input
+ *      class:       Ethernet Class  class
+ *		     (TCAM_CLASS_ETYPE or  TCAM_CLASS_ETYPE_2)
+ *
+ * Return
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+npi_status_t npi_fflp_cfg_enet_usr_cls_enable(npi_handle_t, tcam_class_t);
+
+/*
+ * npi_fflp_cfg_enet_usr_cls_disable()
+ * Disables previously configured TCAM user configurable Ethernet classes.
+ *
+ * Input
+ *      class:       Ethernet Class
+ *		     class = (TCAM_CLASS_ETYPE or  TCAM_CLASS_ETYPE_2)
+ *
+ * Return
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+
+npi_status_t npi_fflp_cfg_enet_usr_cls_disable(npi_handle_t, tcam_class_t);
+
+
+/*
+ * npi_fflp_cfg_ip_usr_cls_set()
+ * Configures the TCAM user configurable IP classes.
+ *
+ * Input
+ *      class:       IP Class
+ *		     (TCAM_CLASS_IP_USER_4 <= class <= TCAM_CLASS_IP_USER_7)
+ *      tos:         IP TOS bits
+ *      tos_mask:    IP TOS bits mask. bits with mask bits set will be used
+ *      proto:       IP Proto
+ *      ver:         IP Version
+ * by default, will the class is disabled until explicitly enabled
+ *
+ * Return
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+npi_status_t npi_fflp_cfg_ip_usr_cls_set(npi_handle_t,
+					tcam_class_t,
+					uint8_t, uint8_t,
+					uint8_t, uint8_t);
+
+/*
+ * npi_fflp_cfg_ip_usr_cls_enable()
+ * Enable previously configured TCAM user configurable IP classes.
+ *
+ * Input
+ *      class:       IP Class
+ *		     (TCAM_CLASS_IP_USER_4 <= class <= TCAM_CLASS_IP_USER_7)
+ *
+ * Return
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+npi_status_t npi_fflp_cfg_ip_usr_cls_enable(npi_handle_t, tcam_class_t);
+
+/*
+ * npi_fflp_cfg_ip_usr_cls_disable()
+ * Disables previously configured TCAM user configurable IP classes.
+ *
+ * Input
+ *      class:       IP Class
+ *		     (TCAM_CLASS_IP_USER_4 <= class <= TCAM_CLASS_IP_USER_7)
+ *
+ * Return
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+
+npi_status_t npi_fflp_cfg_ip_usr_cls_disable(npi_handle_t, tcam_class_t);
+
+
+/*
+ * npi_fflp_cfg_ip_cls_tcam_key ()
+ *
+ * Configures the TCAM key generation for the IP classes
+ *
+ * Input
+ *      l3_class:        IP class to configure key generation
+ *      cfg:             Configuration bits:
+ *                   discard:      Discard all frames of this class
+ *                   use_ip_saddr: use ip src address (for ipv6)
+ *                   use_ip_daddr: use ip dest address (for ipv6)
+ *                   lookup_enable: Enable Lookup
+ *
+ *
+ * Return
+ * NPI_SUCCESS
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+
+npi_status_t npi_fflp_cfg_ip_cls_tcam_key(npi_handle_t,
+				    tcam_class_t, tcam_key_cfg_t *);
+
+/*
+ * npi_fflp_cfg_ip_cls_flow_key ()
+ *
+ * Configures the flow key generation for the IP classes
+ * Flow key is used to generate the H1 hash function value
+ * The fields used for the generation are configured using this
+ * NPI function.
+ *
+ * Input
+ *      l3_class:        IP class to configure flow key generation
+ *      cfg:             Configuration bits:
+ *                   use_proto:     Use IP proto field
+ *                   use_dport:     use l4 destination port
+ *                   use_sport:     use l4 source port
+ *                   ip_opts_exist: IP Options Present
+ *                   use_daddr:     use ip dest address
+ *                   use_saddr:     use ip source address
+ *                   use_vlan:      use VLAN ID
+ *                   use_l2da:      use L2 Dest MAC Address
+ *                   use_portnum:   use L2 virtual port number
+ *
+ *
+ * Return
+ * NPI_SUCCESS
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+npi_status_t npi_fflp_cfg_ip_cls_flow_key(npi_handle_t,
+			    tcam_class_t, flow_key_cfg_t *);
+
+
+
+npi_status_t npi_fflp_cfg_ip_cls_flow_key_get(npi_handle_t,
+				    tcam_class_t,
+				    flow_key_cfg_t *);
+
+
+npi_status_t npi_fflp_cfg_ip_cls_tcam_key_get(npi_handle_t,
+				    tcam_class_t, tcam_key_cfg_t *);
+/*
+ * npi_fflp_cfg_hash_h1poly()
+ * Initializes the H1 hash generation logic.
+ *
+ * Input
+ *      init_value:       The initial value (seed)
+ *
+ * Return
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+npi_status_t npi_fflp_cfg_hash_h1poly(npi_handle_t, uint32_t);
+
+
+
+/*
+ * npi_fflp_cfg_hash_h2poly()
+ * Initializes the H2 hash generation logic.
+ *
+ * Input
+ *      init_value:       The initial value (seed)
+ *
+ * Return
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+npi_status_t npi_fflp_cfg_hash_h2poly(npi_handle_t, uint16_t);
+
+
+/*
+ * Reset the fflp block (actually the FCRAM)
+ * Waits until reset is completed
+ *
+ * input
+ * strength	fcram output drive strength: weak, normal or strong
+ * qs		qs mode. Normal or free running
+ *
+ * return value
+ *	  NPI_SUCCESS
+ *	  NPI_SW_ERR
+ *	  NPI_HW_ERR
+ */
+
+npi_status_t npi_fflp_fcram_reset(npi_handle_t,
+			    fflp_fcram_output_drive_t,
+			    fflp_fcram_qs_t);
+
+
+/* FFLP TCAM Related Functions */
+
+
+/*
+ * npi_fflp_tcam_entry_match()
+ *
+ * Tests for TCAM match of the tcam entry
+ *
+ * Input
+ * tcam_ptr
+ *
+ * Return
+ *   NPI_SUCCESS
+ *   NPI_SW_ERR
+ *   NPI_HW_ERR
+ *
+ */
+
+int npi_fflp_tcam_entry_match(npi_handle_t, tcam_entry_t *);
+
+/*
+ * npi_fflp_tcam_entry_write()
+ *
+ * writes a tcam entry at the TCAM location, location
+ *
+ * Input
+ * location
+ * tcam_ptr
+ *
+ * Return
+ *   NPI_SUCCESS
+ *   NPI_SW_ERR
+ *   NPI_HW_ERR
+ *
+ */
+
+npi_status_t npi_fflp_tcam_entry_write(npi_handle_t,
+				tcam_location_t,
+				tcam_entry_t *);
+
+/*
+ * npi_fflp_tcam_entry_read ()
+ *
+ * Reads a tcam entry from the TCAM location, location
+ *
+ * Input:
+ * location
+ * tcam_ptr
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+
+npi_status_t npi_fflp_tcam_entry_read(npi_handle_t,
+					tcam_location_t,
+					tcam_entry_t *);
+
+/*
+ * npi_fflp_tcam_entry_invalidate()
+ *
+ * invalidates entry at tcam location
+ *
+ * Input
+ * location
+ *
+ * Return
+ *   NPI_SUCCESS
+ *   NPI_SW_ERR
+ *   NPI_HW_ERR
+ *
+ */
+
+npi_status_t npi_fflp_tcam_entry_invalidate(npi_handle_t,
+				    tcam_location_t);
+
+
+/*
+ * npi_fflp_tcam_asc_ram_entry_write()
+ *
+ * writes a tcam associatedRAM at the TCAM location, location
+ *
+ * Input:
+ * location	tcam associatedRAM location
+ * ram_data	Value to write
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+npi_status_t npi_fflp_tcam_asc_ram_entry_write(npi_handle_t,
+				    tcam_location_t,
+				    uint64_t);
+
+
+/*
+ * npi_fflp_tcam_asc_ram_entry_read()
+ *
+ * reads a tcam associatedRAM content at the TCAM location, location
+ *
+ * Input:
+ * location	tcam associatedRAM location
+ * ram_data	ptr to return contents
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+npi_status_t npi_fflp_tcam_asc_ram_entry_read(npi_handle_t,
+				    tcam_location_t,
+				    uint64_t *);
+
+/*
+ * npi_fflp_tcam_get_err_log
+ * Reports TCAM PIO read and lookup errors.
+ * If there are TCAM errors as indicated by err bit set by HW,
+ *  then the SW will clear it by clearing the bit.
+ *
+ * Input
+ *	err_stat:	 structure to report various TCAM errors.
+ *                       will be updated if there are TCAM errors.
+ *
+ *
+ * Return
+ *	NPI_SUCCESS	Success
+ *
+ *
+ */
+npi_status_t npi_fflp_tcam_get_err_log(npi_handle_t, tcam_err_log_t *);
+
+
+
+/*
+ * npi_fflp_tcam_clr_err_log
+ * Clears TCAM PIO read and lookup error status.
+ * If there are TCAM errors as indicated by err bit set by HW,
+ *  then the SW will clear it by clearing the bit.
+ *
+ * Input
+ *	err_stat:	 structure to report various TCAM errors.
+ *                       will be updated if there are TCAM errors.
+ *
+ *
+ * Return
+ *	NPI_SUCCESS	Success
+ *
+ *
+ */
+
+npi_status_t npi_fflp_tcam_clr_err_log(npi_handle_t);
+
+
+
+
+
+/*
+ * npi_fflp_vlan_tbl_clr_err_log
+ * Clears VLAN Table PIO  error status.
+ * If there are VLAN Table errors as indicated by err bit set by HW,
+ *  then the SW will clear it by clearing the bit.
+ *
+ * Input
+ *	err_stat:	 structure to report various VLAN Table errors.
+ *                       will be updated if there are  errors.
+ *
+ *
+ * Return
+ *	NPI_SUCCESS	Success
+ *
+ *
+ */
+
+npi_status_t npi_fflp_vlan_tbl_clr_err_log(npi_handle_t);
+
+
+/*
+ * npi_fflp_vlan_tbl_get_err_log
+ * Reports VLAN Table  errors.
+ * If there are VLAN Table errors as indicated by err bit set by HW,
+ *  then the SW will clear it by clearing the bit.
+ *
+ * Input
+ *	err_stat:	 structure to report various VLAN table errors.
+ *                       will be updated if there are errors.
+ *
+ *
+ * Return
+ *	NPI_SUCCESS	Success
+ *
+ *
+ */
+npi_status_t npi_fflp_vlan_tbl_get_err_log(npi_handle_t,
+				    vlan_tbl_err_log_t *);
+
+
+
+
+/*
+ * npi_rxdma_event_mask_config():
+ *	This function is called to operate on the event mask
+ *	register which is used for generating interrupts
+ *	and status register.
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	op_mode		- OP_GET: get hardware event mask
+ *			  OP_SET: set hardware interrupt event masks
+ *	channel		- hardware RXDMA channel from 0 to 23.
+ *	cfgp		- pointer to NPI defined event mask
+ *			  enum data type.
+ * Return:
+ *	NPI_SUCCESS		- If set is complete successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE		-
+ *	NPI_FFLP_ERROR | NPI_FFLP_SW_PARAM_ERROR
+ *
+ */
+npi_status_t
+npi_fflp_event_mask_config(npi_handle_t, io_op_t,
+			    fflp_event_mask_cfg_t *);
+
+npi_status_t npi_fflp_dump_regs(npi_handle_t);
+
+
+/* Error status read and clear functions */
+
+void	npi_fflp_vlan_error_get(npi_handle_t,
+				    p_vlan_par_err_t);
+void	npi_fflp_vlan_error_clear(npi_handle_t);
+void	npi_fflp_tcam_error_get(npi_handle_t,
+				    p_tcam_err_t);
+void	npi_fflp_tcam_error_clear(npi_handle_t);
+
+void	npi_fflp_fcram_error_get(npi_handle_t,
+				    p_hash_tbl_data_log_t,
+				    uint8_t);
+void npi_fflp_fcram_error_clear(npi_handle_t, uint8_t);
+
+void npi_fflp_fcram_error_log1_get(npi_handle_t,
+				    p_hash_lookup_err_log1_t);
+
+void npi_fflp_fcram_error_log2_get(npi_handle_t,
+			    p_hash_lookup_err_log2_t);
+
+void npi_fflp_vlan_tbl_dump(npi_handle_t);
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif	/* _NPI_FFLP_H */
diff --git a/drivers/net/nxge/npi/npi_ipp.c b/drivers/net/nxge/npi/npi_ipp.c
new file mode 100644
index 0000000..ba9ee2f
--- /dev/null
+++ b/drivers/net/nxge/npi/npi_ipp.c
@@ -0,0 +1,704 @@
+/*
+ * npi_ipp.c	Neptune  RX IPP HW API functions
+ *
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#include <npi_ipp.h>
+
+uint64_t ipp_fzc_offset[] = {
+		IPP_CONFIG_REG,
+		IPP_DISCARD_PKT_CNT_REG,
+		IPP_TCP_CKSUM_ERR_CNT_REG,
+		IPP_ECC_ERR_COUNTER_REG,
+		IPP_INT_STATUS_REG,
+		IPP_INT_MASK_REG,
+		IPP_PFIFO_RD_DATA0_REG,
+		IPP_PFIFO_RD_DATA1_REG,
+		IPP_PFIFO_RD_DATA2_REG,
+		IPP_PFIFO_RD_DATA3_REG,
+		IPP_PFIFO_RD_DATA4_REG,
+		IPP_PFIFO_WR_DATA0_REG,
+		IPP_PFIFO_WR_DATA1_REG,
+		IPP_PFIFO_WR_DATA2_REG,
+		IPP_PFIFO_WR_DATA3_REG,
+		IPP_PFIFO_WR_DATA4_REG,
+		IPP_PFIFO_RD_PTR_REG,
+		IPP_PFIFO_WR_PTR_REG,
+		IPP_DFIFO_RD_DATA0_REG,
+		IPP_DFIFO_RD_DATA1_REG,
+		IPP_DFIFO_RD_DATA2_REG,
+		IPP_DFIFO_RD_DATA3_REG,
+		IPP_DFIFO_RD_DATA4_REG,
+		IPP_DFIFO_WR_DATA0_REG,
+		IPP_DFIFO_WR_DATA1_REG,
+		IPP_DFIFO_WR_DATA2_REG,
+		IPP_DFIFO_WR_DATA3_REG,
+		IPP_DFIFO_WR_DATA4_REG,
+		IPP_DFIFO_RD_PTR_REG,
+		IPP_DFIFO_WR_PTR_REG,
+		IPP_STATE_MACHINE_REG,
+		IPP_CKSUM_STATUS_REG,
+		IPP_FFLP_CKSUM_INFO_REG,
+		IPP_DEBUG_SELECT_REG,
+		IPP_DFIFO_ECC_SYNDROME_REG,
+		IPP_DFIFO_EOPM_RD_PTR_REG,
+		IPP_ECC_CTRL_REG
+};
+
+const char *ipp_fzc_name[] = {
+		"IPP_CONFIG_REG",
+		"IPP_DISCARD_PKT_CNT_REG",
+		"IPP_TCP_CKSUM_ERR_CNT_REG",
+		"IPP_ECC_ERR_COUNTER_REG",
+		"IPP_INT_STATUS_REG",
+		"IPP_INT_MASK_REG",
+		"IPP_PFIFO_RD_DATA0_REG",
+		"IPP_PFIFO_RD_DATA1_REG",
+		"IPP_PFIFO_RD_DATA2_REG",
+		"IPP_PFIFO_RD_DATA3_REG",
+		"IPP_PFIFO_RD_DATA4_REG",
+		"IPP_PFIFO_WR_DATA0_REG",
+		"IPP_PFIFO_WR_DATA1_REG",
+		"IPP_PFIFO_WR_DATA2_REG",
+		"IPP_PFIFO_WR_DATA3_REG",
+		"IPP_PFIFO_WR_DATA4_REG",
+		"IPP_PFIFO_RD_PTR_REG",
+		"IPP_PFIFO_WR_PTR_REG",
+		"IPP_DFIFO_RD_DATA0_REG",
+		"IPP_DFIFO_RD_DATA1_REG",
+		"IPP_DFIFO_RD_DATA2_REG",
+		"IPP_DFIFO_RD_DATA3_REG",
+		"IPP_DFIFO_RD_DATA4_REG",
+		"IPP_DFIFO_WR_DATA0_REG",
+		"IPP_DFIFO_WR_DATA1_REG",
+		"IPP_DFIFO_WR_DATA2_REG",
+		"IPP_DFIFO_WR_DATA3_REG",
+		"IPP_DFIFO_WR_DATA4_REG",
+		"IPP_DFIFO_RD_PTR_REG",
+		"IPP_DFIFO_WR_PTR_REG",
+		"IPP_STATE_MACHINE_REG",
+		"IPP_CKSUM_STATUS_REG",
+		"IPP_FFLP_CKSUM_INFO_REG",
+		"IPP_DEBUG_SELECT_REG",
+		"IPP_DFIFO_ECC_SYNDROME_REG",
+		"IPP_DFIFO_EOPM_RD_PTR_REG",
+		"IPP_ECC_CTRL_REG",
+};
+
+npi_status_t
+npi_ipp_dump_regs(npi_handle_t handle, uint8_t port)
+{
+	uint64_t		value, offset;
+	int 			num_regs, i;
+
+	if (!IS_PORT_NUM_VALID(port)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_ipp_dump_regs"
+			" Invalid Input: port <%d>", port));
+		return (NPI_FAILURE);
+	}
+
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+		"\nIPP PORT Register Dump for port %d\n", port));
+
+	num_regs = sizeof (ipp_fzc_offset) / sizeof (uint64_t);
+	for (i = 0; i < num_regs; i++) {
+		offset = IPP_REG_ADDR(port, ipp_fzc_offset[i]);
+		NXGE_REG_RD64(handle, offset, &value);
+		NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL, "0x%08llx "
+			"%s\t 0x%08llx \n",
+			offset, ipp_fzc_name[i], value));
+	}
+
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+		"\n IPP FZC Register Dump for port %d done\n", port));
+
+	return (NPI_SUCCESS);
+}
+
+void
+npi_ipp_read_regs(npi_handle_t handle, uint8_t port)
+{
+	uint64_t		value, offset;
+	int 			num_regs, i;
+
+	if (!IS_PORT_NUM_VALID(port)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_ipp_dump_regs"
+			" Invalid Input: port <%d>", port));
+		return;
+	}
+
+	NPI_DEBUG_MSG((handle.function, NPI_IPP_CTL,
+		"\nIPP PORT Register read (to clear) for port %d\n", port));
+
+	num_regs = sizeof (ipp_fzc_offset) / sizeof (uint64_t);
+	for (i = 0; i < num_regs; i++) {
+		offset = IPP_REG_ADDR(port, ipp_fzc_offset[i]);
+		NXGE_REG_RD64(handle, offset, &value);
+	}
+
+}
+
+/* IPP Reset Routine */
+
+npi_status_t
+npi_ipp_reset(npi_handle_t handle, uint8_t portn)
+{
+	uint64_t val = 0;
+	uint32_t cnt = MAX_PIO_RETRIES;
+
+	if (!IS_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_ipp_reset"
+			" Invalid Input portn  <0x%x>",
+			portn));
+		return (NPI_FAILURE | NPI_IPP_PORT_INVALID(portn));
+	}
+
+	IPP_REG_RD(handle, portn, IPP_CONFIG_REG, &val);
+	val |= IPP_SOFT_RESET;
+	IPP_REG_WR(handle, portn, IPP_CONFIG_REG, val);
+
+	do {
+		NXGE_DELAY(IPP_RESET_WAIT);
+		IPP_REG_RD(handle, portn, IPP_CONFIG_REG, &val);
+		cnt--;
+	} while (((val & IPP_SOFT_RESET) != 0) && (cnt > 0));
+
+	if (cnt == 0) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_ipp_reset"
+				    " HW Error: IPP_RESET  <0x%x>", val));
+		return (NPI_FAILURE | NPI_IPP_RESET_FAILED(portn));
+	}
+
+	return (NPI_SUCCESS);
+}
+
+
+/* IPP Configuration Routine */
+
+npi_status_t
+npi_ipp_config(npi_handle_t handle, config_op_t op, uint8_t portn,
+		ipp_config_t config)
+{
+	uint64_t val = 0;
+
+	if (!IS_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_ipp_config"
+				    " Invalid Input portn <0x%x>", portn));
+		return (NPI_FAILURE | NPI_IPP_PORT_INVALID(portn));
+	}
+
+	switch (op) {
+
+	case ENABLE:
+	case DISABLE:
+		if ((config == 0) || ((config & ~CFG_IPP_ALL) != 0)) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				" npi_ipp_config",
+				" Invalid Input config <0x%x>",
+				config));
+			return (NPI_FAILURE | NPI_IPP_CONFIG_INVALID(portn));
+		}
+
+		IPP_REG_RD(handle, portn, IPP_CONFIG_REG, &val);
+
+		if (op == ENABLE)
+			val |= config;
+		else
+			val &= ~config;
+		break;
+
+	case INIT:
+		if ((config & ~CFG_IPP_ALL) != 0) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				" npi_ipp_config"
+				" Invalid Input config <0x%x>",
+				config));
+			return (NPI_FAILURE | NPI_IPP_CONFIG_INVALID(portn));
+		}
+		IPP_REG_RD(handle, portn, IPP_CONFIG_REG, &val);
+
+
+		val &= (IPP_IP_MAX_PKT_BYTES_MASK);
+		val |= config;
+		break;
+
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_ipp_config"
+				    " Invalid Input op <0x%x>", op));
+		return (NPI_FAILURE | NPI_IPP_OPCODE_INVALID(portn));
+	}
+
+	IPP_REG_WR(handle, portn, IPP_CONFIG_REG, val);
+	return (NPI_SUCCESS);
+}
+
+
+npi_status_t
+npi_ipp_set_max_pktsize(npi_handle_t handle, uint8_t portn, uint32_t bytes)
+{
+	uint64_t val = 0;
+
+	if (!IS_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_ipp_set_max_pktsize"
+			" Invalid Input portn <0x%x>",
+			portn));
+		return (NPI_FAILURE | NPI_IPP_PORT_INVALID(portn));
+	}
+
+	if (bytes > IPP_IP_MAX_PKT_BYTES_MASK) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_ipp_set_max_pktsize"
+			" Invalid Input Max bytes <0x%x>",
+			bytes));
+		return (NPI_FAILURE | NPI_IPP_MAX_PKT_BYTES_INVALID(portn));
+	}
+
+	IPP_REG_RD(handle, portn, IPP_CONFIG_REG, &val);
+	val &= ~(IPP_IP_MAX_PKT_BYTES_MASK << IPP_IP_MAX_PKT_BYTES_SHIFT);
+
+	val |= (bytes << IPP_IP_MAX_PKT_BYTES_SHIFT);
+	IPP_REG_WR(handle, portn, IPP_CONFIG_REG, val);
+
+	return (NPI_SUCCESS);
+}
+
+
+
+/* IPP Interrupt Configuration Routine */
+
+npi_status_t
+npi_ipp_iconfig(npi_handle_t handle, config_op_t op, uint8_t portn,
+		ipp_iconfig_t iconfig)
+{
+	uint64_t val = 0;
+
+	if (!IS_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_ipp_config"
+			" Invalid Input portn <0x%x>",
+			portn));
+		return (NPI_FAILURE | NPI_IPP_PORT_INVALID(portn));
+	}
+
+	switch (op) {
+	case ENABLE:
+	case DISABLE:
+
+		if ((iconfig == 0) || ((iconfig & ~ICFG_IPP_ALL) != 0)) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				" npi_ipp_iconfig"
+				" Invalid Input iconfig <0x%x>",
+				iconfig));
+			return (NPI_FAILURE | NPI_IPP_CONFIG_INVALID(portn));
+		}
+
+		IPP_REG_RD(handle, portn, IPP_INT_MASK_REG, &val);
+		if (op == ENABLE)
+			val &= ~iconfig;
+		else
+			val |= iconfig;
+		IPP_REG_WR(handle, portn, IPP_INT_MASK_REG, val);
+
+		break;
+	case INIT:
+
+		if ((iconfig & ~ICFG_IPP_ALL) != 0) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				" npi_ipp_iconfig"
+				" Invalid Input iconfig <0x%x>",
+				iconfig));
+			return (NPI_FAILURE | NPI_IPP_CONFIG_INVALID(portn));
+		}
+		IPP_REG_WR(handle, portn, IPP_INT_MASK_REG, ~iconfig);
+
+		break;
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_ipp_iconfig"
+			" Invalid Input iconfig <0x%x>",
+			iconfig));
+		return (NPI_FAILURE | NPI_IPP_OPCODE_INVALID(portn));
+	}
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_ipp_get_status(npi_handle_t handle, uint8_t portn, ipp_status_t *status)
+{
+	uint64_t val;
+
+	if (!IS_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_ipp_get_status"
+			" Invalid Input portn <0x%x>",
+			portn));
+		return (NPI_FAILURE | NPI_IPP_PORT_INVALID(portn));
+	}
+
+	IPP_REG_RD(handle, portn, IPP_INT_STATUS_REG, &val);
+
+	status->value = val;
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_ipp_get_pfifo_rd_ptr(npi_handle_t handle, uint8_t portn, uint16_t *rd_ptr)
+{
+	uint64_t value;
+	if (!IS_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_ipp_get_pfifo_rd_ptr"
+			" Invalid Input portn <0x%x>",
+			portn));
+		return (NPI_FAILURE | NPI_IPP_PORT_INVALID(portn));
+	}
+
+	IPP_REG_RD(handle, portn, IPP_PFIFO_RD_PTR_REG, &value);
+	*rd_ptr = value & 0xfff;
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_ipp_get_pfifo_wr_ptr(npi_handle_t handle, uint8_t portn, uint16_t *wr_ptr)
+{
+	uint64_t value;
+	if (!IS_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_ipp_get_pfifo_wr_ptr"
+			" Invalid Input portn <0x%x>",
+			portn));
+		return (NPI_FAILURE | NPI_IPP_PORT_INVALID(portn));
+	}
+
+	IPP_REG_RD(handle, portn, IPP_PFIFO_WR_PTR_REG, &value);
+	*wr_ptr = value & 0xfff;
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_ipp_get_dfifo_rd_ptr(npi_handle_t handle, uint8_t portn, uint16_t *rd_ptr)
+{
+	uint64_t value;
+	if (!IS_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_ipp_get_dfifo_rd_ptr"
+			" Invalid Input portn <0x%x>",
+			portn));
+		return (NPI_FAILURE | NPI_IPP_PORT_INVALID(portn));
+	}
+
+	IPP_REG_RD(handle, portn, IPP_DFIFO_RD_PTR_REG, &value);
+	*rd_ptr = (uint16_t)(value & ((portn < 2) ? IPP_XMAC_DFIFO_PTR_MASK :
+					IPP_BMAC_DFIFO_PTR_MASK));
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_ipp_get_dfifo_wr_ptr(npi_handle_t handle, uint8_t portn, uint16_t *wr_ptr)
+{
+	uint64_t value;
+	if (!IS_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_ipp_get_dfifo_wr_ptr"
+			" Invalid Input portn <0x%x>",
+			portn));
+		return (NPI_FAILURE | NPI_IPP_PORT_INVALID(portn));
+	}
+
+	IPP_REG_RD(handle, portn, IPP_DFIFO_WR_PTR_REG, &value);
+	*wr_ptr = (uint16_t)(value & ((portn < 2) ? IPP_XMAC_DFIFO_PTR_MASK :
+					IPP_BMAC_DFIFO_PTR_MASK));
+	return (NPI_SUCCESS);
+}
+
+
+npi_status_t
+npi_ipp_write_pfifo(npi_handle_t handle, uint8_t portn, uint8_t addr,
+		uint32_t d0, uint32_t d1, uint32_t d2, uint32_t d3, uint32_t d4)
+{
+	uint64_t val;
+
+	if (!IS_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_ipp_write_pfifo"
+			" Invalid Input portn <0x%x>",
+			portn));
+		return (NPI_FAILURE | NPI_IPP_PORT_INVALID(portn));
+	}
+
+	if (addr >= 64) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_ipp_write_pfifo"
+			" Invalid PFIFO address <0x%x>", addr));
+		return (NPI_FAILURE | NPI_IPP_FIFO_ADDR_INVALID(portn));
+	}
+
+	IPP_REG_RD(handle, portn, IPP_CONFIG_REG, &val);
+	val |= IPP_PRE_FIFO_PIO_WR_EN;
+	IPP_REG_WR(handle, portn, IPP_CONFIG_REG, val);
+
+	IPP_REG_WR(handle, portn, IPP_PFIFO_WR_PTR_REG, addr);
+	IPP_REG_WR(handle, portn, IPP_PFIFO_WR_DATA0_REG, d0);
+	IPP_REG_WR(handle, portn, IPP_PFIFO_WR_DATA1_REG, d1);
+	IPP_REG_WR(handle, portn, IPP_PFIFO_WR_DATA2_REG, d2);
+	IPP_REG_WR(handle, portn, IPP_PFIFO_WR_DATA3_REG, d3);
+	IPP_REG_WR(handle, portn, IPP_PFIFO_WR_DATA4_REG, d4);
+
+	val &= ~IPP_PRE_FIFO_PIO_WR_EN;
+	IPP_REG_WR(handle, portn, IPP_CONFIG_REG, val);
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_ipp_read_pfifo(npi_handle_t handle, uint8_t portn, uint8_t addr,
+		uint32_t *d0, uint32_t *d1, uint32_t *d2, uint32_t *d3,
+		uint32_t *d4)
+{
+	if (!IS_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_ipp_read_pfifo"
+			" Invalid Input portn <0x%x>",
+			portn));
+		return (NPI_FAILURE | NPI_IPP_PORT_INVALID(portn));
+	}
+
+	if (addr >= 64) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_ipp_read_pfifo"
+			" Invalid PFIFO address <0x%x>", addr));
+		return (NPI_FAILURE | NPI_IPP_FIFO_ADDR_INVALID(portn));
+	}
+
+	IPP_REG_WR(handle, portn, IPP_PFIFO_RD_PTR_REG, addr);
+	IPP_REG_RD(handle, portn, IPP_PFIFO_RD_DATA0_REG, d0);
+	IPP_REG_RD(handle, portn, IPP_PFIFO_RD_DATA1_REG, d1);
+	IPP_REG_RD(handle, portn, IPP_PFIFO_RD_DATA2_REG, d2);
+	IPP_REG_RD(handle, portn, IPP_PFIFO_RD_DATA3_REG, d3);
+	IPP_REG_RD(handle, portn, IPP_PFIFO_RD_DATA4_REG, d4);
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_ipp_write_dfifo(npi_handle_t handle, uint8_t portn, uint16_t addr,
+		uint32_t d0, uint32_t d1, uint32_t d2, uint32_t d3, uint32_t d4)
+{
+	uint64_t val;
+
+	if (!IS_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_ipp_write_dfifo"
+			" Invalid Input portn <0x%x>",
+			portn));
+		return (NPI_FAILURE | NPI_IPP_PORT_INVALID(portn));
+	}
+
+	if (addr >= 2048) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_ipp_write_dfifo"
+			" Invalid DFIFO address <0x%x>", addr));
+		return (NPI_FAILURE | NPI_IPP_FIFO_ADDR_INVALID(portn));
+	}
+
+	IPP_REG_RD(handle, portn, IPP_CONFIG_REG, &val);
+	val |= IPP_DFIFO_PIO_WR_EN;
+	IPP_REG_WR(handle, portn, IPP_CONFIG_REG, val);
+
+	IPP_REG_WR(handle, portn, IPP_DFIFO_WR_PTR_REG, addr);
+	IPP_REG_WR(handle, portn, IPP_DFIFO_WR_DATA0_REG, d0);
+	IPP_REG_WR(handle, portn, IPP_DFIFO_WR_DATA1_REG, d1);
+	IPP_REG_WR(handle, portn, IPP_DFIFO_WR_DATA2_REG, d2);
+	IPP_REG_WR(handle, portn, IPP_DFIFO_WR_DATA3_REG, d3);
+	IPP_REG_WR(handle, portn, IPP_DFIFO_WR_DATA4_REG, d4);
+
+	val &= ~IPP_DFIFO_PIO_WR_EN;
+	IPP_REG_WR(handle, portn, IPP_CONFIG_REG, val);
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_ipp_read_dfifo(npi_handle_t handle, uint8_t portn, uint16_t addr,
+		uint32_t *d0, uint32_t *d1, uint32_t *d2, uint32_t *d3,
+		uint32_t *d4)
+{
+	uint16_t max_addr;
+	if (!IS_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_ipp_read_dfifo"
+			" Invalid Input portn <0x%x>",
+			portn));
+		return (NPI_FAILURE | NPI_IPP_PORT_INVALID(portn));
+	}
+
+	if (portn < 2)
+		max_addr = IPP_P0_P1_DFIFO_ENTRIES;
+	else
+		max_addr = IPP_P2_P3_DFIFO_ENTRIES;
+
+	if (addr >= max_addr) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_ipp_read_dfifo"
+			" Invalid DFIFO address <0x%x>", addr));
+		return (NPI_FAILURE | NPI_IPP_FIFO_ADDR_INVALID(portn));
+	}
+
+	IPP_REG_WR(handle, portn, IPP_DFIFO_RD_PTR_REG, addr);
+	IPP_REG_RD(handle, portn, IPP_DFIFO_RD_DATA0_REG, d0);
+	IPP_REG_RD(handle, portn, IPP_DFIFO_RD_DATA1_REG, d1);
+	IPP_REG_RD(handle, portn, IPP_DFIFO_RD_DATA2_REG, d2);
+	IPP_REG_RD(handle, portn, IPP_DFIFO_RD_DATA3_REG, d3);
+	IPP_REG_RD(handle, portn, IPP_DFIFO_RD_DATA4_REG, d4);
+
+	return (NPI_SUCCESS);
+}
+
+
+npi_status_t
+npi_ipp_get_ecc_syndrome(npi_handle_t handle, uint8_t portn, uint16_t *syndrome)
+{
+	uint64_t val;
+
+	if (!IS_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_ipp_get_ecc_syndrome"
+			" Invalid Input portn <0x%x>",
+			portn));
+		return (NPI_FAILURE | NPI_IPP_PORT_INVALID(portn));
+	}
+
+	IPP_REG_RD(handle, portn, IPP_DFIFO_ECC_SYNDROME_REG, &val);
+
+	*syndrome = (uint16_t)val;
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_ipp_get_dfifo_eopm_rdptr(npi_handle_t handle, uint8_t portn,
+							uint16_t *rdptr)
+{
+	uint64_t val;
+
+	if (!IS_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_ipp_get_dfifo_rdptr"
+			" Invalid Input portn <0x%x>",
+			portn));
+		return (NPI_FAILURE | NPI_IPP_PORT_INVALID(portn));
+	}
+
+	IPP_REG_RD(handle, portn, IPP_DFIFO_EOPM_RD_PTR_REG, &val);
+
+	*rdptr = (uint16_t)val;
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_ipp_get_state_mach(npi_handle_t handle, uint8_t portn, uint32_t *sm)
+{
+	uint64_t val;
+
+	if (!IS_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_ipp_get_state_mach"
+			" Invalid Input portn <0x%x>",
+			portn));
+		return (NPI_FAILURE | NPI_IPP_PORT_INVALID(portn));
+	}
+
+	IPP_REG_RD(handle, portn, IPP_STATE_MACHINE_REG, &val);
+
+	*sm = (uint32_t)val;
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_ipp_get_ecc_err_count(npi_handle_t handle, uint8_t portn, uint8_t *err_cnt)
+{
+	if (!IS_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_ipp_get_ecc_err_count"
+			" Invalid Input portn <0x%x>",
+			portn));
+		return (NPI_FAILURE | NPI_IPP_PORT_INVALID(portn));
+	}
+
+	IPP_REG_RD(handle, portn, IPP_ECC_ERR_COUNTER_REG, err_cnt);
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_ipp_get_pkt_dis_count(npi_handle_t handle, uint8_t portn, uint16_t *dis_cnt)
+{
+	if (!IS_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_ipp_get_pkt_dis_count"
+			" Invalid Input portn <0x%x>",
+			portn));
+		return (NPI_FAILURE | NPI_IPP_PORT_INVALID(portn));
+	}
+
+	IPP_REG_RD(handle, portn, IPP_DISCARD_PKT_CNT_REG, dis_cnt);
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_ipp_get_cs_err_count(npi_handle_t handle, uint8_t portn, uint16_t *err_cnt)
+{
+	if (!IS_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_ipp_get_cs_err_count"
+			" Invalid Input portn <0x%x>",
+			portn));
+		return (NPI_FAILURE | NPI_IPP_PORT_INVALID(portn));
+	}
+
+	IPP_REG_RD(handle, portn, IPP_ECC_ERR_COUNTER_REG, err_cnt);
+
+	return (NPI_SUCCESS);
+}
diff --git a/drivers/net/nxge/npi/npi_ipp.h b/drivers/net/nxge/npi/npi_ipp.h
new file mode 100644
index 0000000..3ea0e8a
--- /dev/null
+++ b/drivers/net/nxge/npi/npi_ipp.h
@@ -0,0 +1,200 @@
+/*
+ * npi_ipp.h	Neptune  RX IPP HW API functions
+ *
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef _NPI_IPP_H
+#define	_NPI_IPP_H
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#include <npi.h>
+#include <nxge_ipp_hw.h>
+
+/* IBTP IPP Configuration */
+
+typedef enum ipp_config_e {
+	CFG_IPP =			IPP_EN,
+	CFG_IPP_DFIFO_ECC_CORRECT =	IPP_DFIFO_ECC_CORRECT_EN,
+	CFG_IPP_DROP_BAD_CRC =		IPP_DROP_BAD_CRC_EN,
+	CFG_IPP_TCP_UDP_CKSUM =		IPP_TCP_UDP_CKSUM_EN,
+	CFG_IPP_DFIFO_PIO_WR =		IPP_DFIFO_PIO_WR_EN,
+	CFG_IPP_PRE_FIFO_PIO_WR =	IPP_PRE_FIFO_PIO_WR_EN,
+	CFG_IPP_FFLP_CKSUM_INFO_PIO_WR = IPP_FFLP_CKSUM_INFO_PIO_WR_EN,
+	CFG_IPP_ALL =			(IPP_EN | IPP_DFIFO_ECC_CORRECT_EN |
+			IPP_DROP_BAD_CRC_EN | IPP_TCP_UDP_CKSUM_EN |
+			IPP_DFIFO_PIO_WR_EN | IPP_PRE_FIFO_PIO_WR_EN)
+} ipp_config_t;
+
+typedef enum ipp_iconfig_e {
+	ICFG_IPP_PKT_DISCARD_OVFL =	IPP_PKT_DISCARD_CNT_INTR_DIS,
+	ICFG_IPP_BAD_TCPIP_CKSUM_OVFL =	IPP_BAD_TCPIP_CKSUM_CNT_INTR_DIS,
+	ICFG_IPP_PRE_FIFO_UNDERRUN =	IPP_PRE_FIFO_UNDERRUN_INTR_DIS,
+	ICFG_IPP_PRE_FIFO_OVERRUN =	IPP_PRE_FIFO_OVERRUN_INTR_DIS,
+	ICFG_IPP_PRE_FIFO_PERR =	IPP_PRE_FIFO_PERR_INTR_DIS,
+	ICFG_IPP_DFIFO_ECC_UNCORR_ERR =	IPP_DFIFO_ECC_UNCORR_ERR_INTR_DIS,
+	ICFG_IPP_DFIFO_MISSING_EOP_SOP = IPP_DFIFO_MISSING_EOP_SOP_INTR_DIS,
+	ICFG_IPP_ECC_ERR_OVFL =		IPP_ECC_ERR_CNT_MAX_INTR_DIS,
+	ICFG_IPP_ALL =			(IPP_PKT_DISCARD_CNT_INTR_DIS |
+			IPP_BAD_TCPIP_CKSUM_CNT_INTR_DIS |
+			IPP_PRE_FIFO_UNDERRUN_INTR_DIS |
+			IPP_PRE_FIFO_OVERRUN_INTR_DIS |
+			IPP_PRE_FIFO_PERR_INTR_DIS |
+			IPP_DFIFO_ECC_UNCORR_ERR_INTR_DIS |
+			IPP_DFIFO_MISSING_EOP_SOP_INTR_DIS |
+			IPP_ECC_ERR_CNT_MAX_INTR_DIS)
+} ipp_iconfig_t;
+
+typedef enum ipp_counter_e {
+	CNT_IPP_DISCARD_PKT		= 0x00000001,
+	CNT_IPP_TCP_CKSUM_ERR		= 0x00000002,
+	CNT_IPP_ECC_ERR			= 0x00000004,
+	CNT_IPP_ALL			= 0x00000007
+} ipp_counter_t;
+
+
+typedef enum ipp_port_cnt_idx_e {
+	HWCI_IPP_PKT_DISCARD = 0,
+	HWCI_IPP_TCP_CKSUM_ERR,
+	HWCI_IPP_ECC_ERR,
+	CI_IPP_MISSING_EOP_SOP,
+	CI_IPP_UNCORR_ERR,
+	CI_IPP_PERR,
+	CI_IPP_FIFO_OVERRUN,
+	CI_IPP_FIFO_UNDERRUN,
+	CI_IPP_PORT_CNT_ARR_SIZE
+} ipp_port_cnt_idx_t;
+
+/* IPP specific errors */
+
+#define	IPP_MAX_PKT_BYTES_INVALID	0x50
+#define	IPP_FIFO_ADDR_INVALID		0x51
+
+/* IPP error return macros */
+
+#define	NPI_IPP_PORT_INVALID(portn)\
+		((IPP_BLK_ID << NPI_BLOCK_ID_SHIFT) | PORT_INVALID |\
+				IS_PORT | (portn << NPI_PORT_CHAN_SHIFT))
+#define	NPI_IPP_OPCODE_INVALID(portn)\
+		((IPP_BLK_ID << NPI_BLOCK_ID_SHIFT) | OPCODE_INVALID |\
+				IS_PORT | (portn << NPI_PORT_CHAN_SHIFT))
+#define	NPI_IPP_CONFIG_INVALID(portn)\
+		((IPP_BLK_ID << NPI_BLOCK_ID_SHIFT) | CONFIG_INVALID |\
+				IS_PORT | (portn << NPI_PORT_CHAN_SHIFT))
+#define	NPI_IPP_MAX_PKT_BYTES_INVALID(portn)\
+		((IPP_BLK_ID << NPI_BLOCK_ID_SHIFT) |\
+		IPP_MAX_PKT_BYTES_INVALID |\
+				IS_PORT | (portn << NPI_PORT_CHAN_SHIFT))
+#define	NPI_IPP_COUNTER_INVALID(portn)\
+		((IPP_BLK_ID << NPI_BLOCK_ID_SHIFT) | COUNTER_INVALID |\
+				IS_PORT | (portn << NPI_PORT_CHAN_SHIFT))
+#define	NPI_IPP_RESET_FAILED(portn)\
+		((IPP_BLK_ID << NPI_BLOCK_ID_SHIFT) | RESET_FAILED |\
+				IS_PORT | (portn << NPI_PORT_CHAN_SHIFT))
+#define	NPI_IPP_FIFO_ADDR_INVALID(portn)\
+		((IPP_BLK_ID << NPI_BLOCK_ID_SHIFT) |\
+		IPP_FIFO_ADDR_INVALID |\
+				IS_PORT | (portn << NPI_PORT_CHAN_SHIFT))
+
+#define	IPP_REG_RD(handle, portn, reg, val) {\
+	NXGE_REG_RD64(handle, IPP_REG_ADDR(portn, reg), val);\
+}
+
+#define	IPP_REG_WR(handle, portn, reg, val) {\
+	NXGE_REG_WR64(handle, IPP_REG_ADDR(portn, reg), val);\
+}
+
+/* IPP NPI function prototypes */
+npi_status_t npi_ipp_get_pfifo_rd_ptr(npi_handle_t, uint8_t,
+			    uint16_t *);
+
+npi_status_t npi_ipp_get_pfifo_wr_ptr(npi_handle_t, uint8_t,
+			    uint16_t *);
+
+npi_status_t npi_ipp_write_pfifo(npi_handle_t, uint8_t,
+			uint8_t, uint32_t, uint32_t, uint32_t,
+			uint32_t, uint32_t);
+
+npi_status_t npi_ipp_read_pfifo(npi_handle_t, uint8_t,
+			uint8_t, uint32_t *, uint32_t *, uint32_t *,
+			uint32_t *, uint32_t *);
+
+npi_status_t npi_ipp_write_dfifo(npi_handle_t, uint8_t,
+			uint16_t, uint32_t, uint32_t, uint32_t,
+			uint32_t, uint32_t);
+
+npi_status_t npi_ipp_read_dfifo(npi_handle_t, uint8_t,
+			uint16_t, uint32_t *, uint32_t *, uint32_t *,
+			uint32_t *, uint32_t *);
+
+npi_status_t npi_ipp_reset(npi_handle_t, uint8_t);
+npi_status_t npi_ipp_config(npi_handle_t, config_op_t, uint8_t,
+			ipp_config_t);
+npi_status_t npi_ipp_set_max_pktsize(npi_handle_t, uint8_t,
+			uint32_t);
+npi_status_t npi_ipp_iconfig(npi_handle_t, config_op_t, uint8_t,
+			ipp_iconfig_t);
+npi_status_t npi_ipp_get_status(npi_handle_t, uint8_t,
+			ipp_status_t *);
+npi_status_t npi_ipp_counters(npi_handle_t, counter_op_t,
+			ipp_counter_t, uint8_t, npi_counter_t *);
+npi_status_t npi_ipp_get_ecc_syndrome(npi_handle_t, uint8_t,
+			uint16_t *);
+npi_status_t npi_ipp_get_dfifo_eopm_rdptr(npi_handle_t, uint8_t,
+			uint16_t *);
+npi_status_t npi_ipp_get_state_mach(npi_handle_t, uint8_t,
+			uint32_t *);
+npi_status_t npi_ipp_get_dfifo_rd_ptr(npi_handle_t, uint8_t,
+			uint16_t *);
+npi_status_t npi_ipp_get_dfifo_wr_ptr(npi_handle_t, uint8_t,
+			uint16_t *);
+npi_status_t npi_ipp_get_ecc_err_count(npi_handle_t, uint8_t,
+			uint8_t *);
+npi_status_t npi_ipp_get_pkt_dis_count(npi_handle_t, uint8_t,
+			uint16_t *);
+npi_status_t npi_ipp_get_cs_err_count(npi_handle_t, uint8_t,
+			uint16_t *);
+npi_status_t npi_ipp_dump_regs(npi_handle_t, uint8_t);
+void npi_ipp_read_regs(npi_handle_t, uint8_t);
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif	/* _NPI_IPP_H */
diff --git a/drivers/net/nxge/npi/npi_mac.c b/drivers/net/nxge/npi/npi_mac.c
new file mode 100644
index 0000000..019a797
--- /dev/null
+++ b/drivers/net/nxge/npi/npi_mac.c
@@ -0,0 +1,3896 @@
+/*
+ * npi_mac.c	Neptune Ethernet MAC HW API functions
+ *
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#include <npi_mac.h>
+
+#define	XMAC_WAIT_REG(handle, portn, reg, val) {\
+	uint32_t cnt = MAX_PIO_RETRIES;\
+	do {\
+		NXGE_DELAY(MAC_RESET_WAIT);\
+		XMAC_REG_RD(handle, portn, reg, &val);\
+		cnt--;\
+	} while (((val & 0x3) != 0) && (cnt > 0));\
+}
+
+#define	BMAC_WAIT_REG(handle, portn, reg, val) {\
+	uint32_t cnt = MAX_PIO_RETRIES;\
+	do {\
+		NXGE_DELAY(MAC_RESET_WAIT);\
+		BMAC_REG_RD(handle, portn, reg, &val);\
+		cnt--;\
+	} while (((val & 0x3) != 0) && (cnt > 0));\
+}
+
+uint64_t xmac_offset[] = {
+	XTXMAC_SW_RST_REG,
+	XRXMAC_SW_RST_REG,
+	XTXMAC_STATUS_REG,
+	XRXMAC_STATUS_REG,
+	XMAC_CTRL_STAT_REG,
+	XTXMAC_STAT_MSK_REG,
+	XRXMAC_STAT_MSK_REG,
+	XMAC_C_S_MSK_REG,
+	XMAC_CONFIG_REG,
+	XMAC_IPG_REG,
+	XMAC_MIN_REG,
+	XMAC_MAX_REG,
+	XMAC_ADDR0_REG,
+	XMAC_ADDR1_REG,
+	XMAC_ADDR2_REG,
+	XRXMAC_BT_CNT_REG,
+	XRXMAC_BC_FRM_CNT_REG,
+	XRXMAC_MC_FRM_CNT_REG,
+	XRXMAC_FRAG_CNT_REG,
+	XRXMAC_HIST_CNT1_REG,
+	XRXMAC_HIST_CNT2_REG,
+	XRXMAC_HIST_CNT3_REG,
+	XRXMAC_HIST_CNT4_REG,
+	XRXMAC_HIST_CNT5_REG,
+	XRXMAC_HIST_CNT6_REG,
+	XRXMAC_MPSZER_CNT_REG,
+	XRXMAC_CRC_ER_CNT_REG,
+	XRXMAC_CD_VIO_CNT_REG,
+	XRXMAC_AL_ER_CNT_REG,
+	XTXMAC_FRM_CNT_REG,
+	XTXMAC_BYTE_CNT_REG,
+	XMAC_LINK_FLT_CNT_REG,
+	XRXMAC_HIST_CNT7_REG,
+	XMAC_SM_REG,
+	XMAC_INTERN1_REG,
+	XMAC_ADDR_CMPEN_REG,
+	XMAC_ADDR3_REG,
+	XMAC_ADDR4_REG,
+	XMAC_ADDR5_REG,
+	XMAC_ADDR6_REG,
+	XMAC_ADDR7_REG,
+	XMAC_ADDR8_REG,
+	XMAC_ADDR9_REG,
+	XMAC_ADDR10_REG,
+	XMAC_ADDR11_REG,
+	XMAC_ADDR12_REG,
+	XMAC_ADDR13_REG,
+	XMAC_ADDR14_REG,
+	XMAC_ADDR15_REG,
+	XMAC_ADDR16_REG,
+	XMAC_ADDR17_REG,
+	XMAC_ADDR18_REG,
+	XMAC_ADDR19_REG,
+	XMAC_ADDR20_REG,
+	XMAC_ADDR21_REG,
+	XMAC_ADDR22_REG,
+	XMAC_ADDR23_REG,
+	XMAC_ADDR24_REG,
+	XMAC_ADDR25_REG,
+	XMAC_ADDR26_REG,
+	XMAC_ADDR27_REG,
+	XMAC_ADDR28_REG,
+	XMAC_ADDR29_REG,
+	XMAC_ADDR30_REG,
+	XMAC_ADDR31_REG,
+	XMAC_ADDR32_REG,
+	XMAC_ADDR33_REG,
+	XMAC_ADDR34_REG,
+	XMAC_ADDR35_REG,
+	XMAC_ADDR36_REG,
+	XMAC_ADDR37_REG,
+	XMAC_ADDR38_REG,
+	XMAC_ADDR39_REG,
+	XMAC_ADDR40_REG,
+	XMAC_ADDR41_REG,
+	XMAC_ADDR42_REG,
+	XMAC_ADDR43_REG,
+	XMAC_ADDR44_REG,
+	XMAC_ADDR45_REG,
+	XMAC_ADDR46_REG,
+	XMAC_ADDR47_REG,
+	XMAC_ADDR48_REG,
+	XMAC_ADDR49_REG,
+	XMAC_ADDR50_REG,
+	XMAC_ADDR_FILT0_REG,
+	XMAC_ADDR_FILT1_REG,
+	XMAC_ADDR_FILT2_REG,
+	XMAC_ADDR_FILT12_MASK_REG,
+	XMAC_ADDR_FILT0_MASK_REG,
+	XMAC_HASH_TBL0_REG,
+	XMAC_HASH_TBL1_REG,
+	XMAC_HASH_TBL2_REG,
+	XMAC_HASH_TBL3_REG,
+	XMAC_HASH_TBL4_REG,
+	XMAC_HASH_TBL5_REG,
+	XMAC_HASH_TBL6_REG,
+	XMAC_HASH_TBL7_REG,
+	XMAC_HASH_TBL8_REG,
+	XMAC_HASH_TBL9_REG,
+	XMAC_HASH_TBL10_REG,
+	XMAC_HASH_TBL11_REG,
+	XMAC_HASH_TBL12_REG,
+	XMAC_HASH_TBL13_REG,
+	XMAC_HASH_TBL14_REG,
+	XMAC_HASH_TBL15_REG,
+	XMAC_HOST_INF0_REG,
+	XMAC_HOST_INF1_REG,
+	XMAC_HOST_INF2_REG,
+	XMAC_HOST_INF3_REG,
+	XMAC_HOST_INF4_REG,
+	XMAC_HOST_INF5_REG,
+	XMAC_HOST_INF6_REG,
+	XMAC_HOST_INF7_REG,
+	XMAC_HOST_INF8_REG,
+	XMAC_HOST_INF9_REG,
+	XMAC_HOST_INF10_REG,
+	XMAC_HOST_INF11_REG,
+	XMAC_HOST_INF12_REG,
+	XMAC_HOST_INF13_REG,
+	XMAC_HOST_INF14_REG,
+	XMAC_HOST_INF15_REG,
+	XMAC_HOST_INF16_REG,
+	XMAC_HOST_INF17_REG,
+	XMAC_HOST_INF18_REG,
+	XMAC_HOST_INF19_REG,
+	XMAC_PA_DATA0_REG,
+	XMAC_PA_DATA1_REG,
+	XMAC_DEBUG_SEL_REG,
+	XMAC_TRAINING_VECT_REG,
+};
+
+const char *xmac_name[] = {
+	"XTXMAC_SW_RST_REG",
+	"XRXMAC_SW_RST_REG",
+	"XTXMAC_STATUS_REG",
+	"XRXMAC_STATUS_REG",
+	"XMAC_CTRL_STAT_REG",
+	"XTXMAC_STAT_MSK_REG",
+	"XRXMAC_STAT_MSK_REG",
+	"XMAC_C_S_MSK_REG",
+	"XMAC_CONFIG_REG",
+	"XMAC_IPG_REG",
+	"XMAC_MIN_REG",
+	"XMAC_MAX_REG",
+	"XMAC_ADDR0_REG",
+	"XMAC_ADDR1_REG",
+	"XMAC_ADDR2_REG",
+	"XRXMAC_BT_CNT_REG",
+	"XRXMAC_BC_FRM_CNT_REG",
+	"XRXMAC_MC_FRM_CNT_REG",
+	"XRXMAC_FRAG_CNT_REG",
+	"XRXMAC_HIST_CNT1_REG",
+	"XRXMAC_HIST_CNT2_REG",
+	"XRXMAC_HIST_CNT3_REG",
+	"XRXMAC_HIST_CNT4_REG",
+	"XRXMAC_HIST_CNT5_REG",
+	"XRXMAC_HIST_CNT6_REG",
+	"XRXMAC_MPSZER_CNT_REG",
+	"XRXMAC_CRC_ER_CNT_REG",
+	"XRXMAC_CD_VIO_CNT_REG",
+	"XRXMAC_AL_ER_CNT_REG",
+	"XTXMAC_FRM_CNT_REG",
+	"XTXMAC_BYTE_CNT_REG",
+	"XMAC_LINK_FLT_CNT_REG",
+	"XRXMAC_HIST_CNT7_REG",
+	"XMAC_SM_REG",
+	"XMAC_INTERN1_REG",
+	"XMAC_ADDR_CMPEN_REG",
+	"XMAC_ADDR3_REG",
+	"XMAC_ADDR4_REG",
+	"XMAC_ADDR5_REG",
+	"XMAC_ADDR6_REG",
+	"XMAC_ADDR7_REG",
+	"XMAC_ADDR8_REG",
+	"XMAC_ADDR9_REG",
+	"XMAC_ADDR10_REG",
+	"XMAC_ADDR11_REG",
+	"XMAC_ADDR12_REG",
+	"XMAC_ADDR13_REG",
+	"XMAC_ADDR14_REG",
+	"XMAC_ADDR15_REG",
+	"XMAC_ADDR16_REG",
+	"XMAC_ADDR17_REG",
+	"XMAC_ADDR18_REG",
+	"XMAC_ADDR19_REG",
+	"XMAC_ADDR20_REG",
+	"XMAC_ADDR21_REG",
+	"XMAC_ADDR22_REG",
+	"XMAC_ADDR23_REG",
+	"XMAC_ADDR24_REG",
+	"XMAC_ADDR25_REG",
+	"XMAC_ADDR26_REG",
+	"XMAC_ADDR27_REG",
+	"XMAC_ADDR28_REG",
+	"XMAC_ADDR29_REG",
+	"XMAC_ADDR30_REG",
+	"XMAC_ADDR31_REG",
+	"XMAC_ADDR32_REG",
+	"XMAC_ADDR33_REG",
+	"XMAC_ADDR34_REG",
+	"XMAC_ADDR35_REG",
+	"XMAC_ADDR36_REG",
+	"XMAC_ADDR37_REG",
+	"XMAC_ADDR38_REG",
+	"XMAC_ADDR39_REG",
+	"XMAC_ADDR40_REG",
+	"XMAC_ADDR41_REG",
+	"XMAC_ADDR42_REG",
+	"XMAC_ADDR43_REG",
+	"XMAC_ADDR44_REG",
+	"XMAC_ADDR45_REG",
+	"XMAC_ADDR46_REG",
+	"XMAC_ADDR47_REG",
+	"XMAC_ADDR48_REG",
+	"XMAC_ADDR49_REG",
+	"XMAC_ADDR50_RE",
+	"XMAC_ADDR_FILT0_REG",
+	"XMAC_ADDR_FILT1_REG",
+	"XMAC_ADDR_FILT2_REG",
+	"XMAC_ADDR_FILT12_MASK_REG",
+	"XMAC_ADDR_FILT0_MASK_REG",
+	"XMAC_HASH_TBL0_REG",
+	"XMAC_HASH_TBL1_REG",
+	"XMAC_HASH_TBL2_REG",
+	"XMAC_HASH_TBL3_REG",
+	"XMAC_HASH_TBL4_REG",
+	"XMAC_HASH_TBL5_REG",
+	"XMAC_HASH_TBL6_REG",
+	"XMAC_HASH_TBL7_REG",
+	"XMAC_HASH_TBL8_REG",
+	"XMAC_HASH_TBL9_REG",
+	"XMAC_HASH_TBL10_REG",
+	"XMAC_HASH_TBL11_REG",
+	"XMAC_HASH_TBL12_REG",
+	"XMAC_HASH_TBL13_REG",
+	"XMAC_HASH_TBL14_REG",
+	"XMAC_HASH_TBL15_REG",
+	"XMAC_HOST_INF0_REG",
+	"XMAC_HOST_INF1_REG",
+	"XMAC_HOST_INF2_REG",
+	"XMAC_HOST_INF3_REG",
+	"XMAC_HOST_INF4_REG",
+	"XMAC_HOST_INF5_REG",
+	"XMAC_HOST_INF6_REG",
+	"XMAC_HOST_INF7_REG",
+	"XMAC_HOST_INF8_REG",
+	"XMAC_HOST_INF9_REG",
+	"XMAC_HOST_INF10_REG",
+	"XMAC_HOST_INF11_REG",
+	"XMAC_HOST_INF12_REG",
+	"XMAC_HOST_INF13_REG",
+	"XMAC_HOST_INF14_REG",
+	"XMAC_HOST_INF15_REG",
+	"XMAC_HOST_INF16_REG",
+	"XMAC_HOST_INF17_REG",
+	"XMAC_HOST_INF18_REG",
+	"XMAC_HOST_INF19_REG",
+	"XMAC_PA_DATA0_REG",
+	"XMAC_PA_DATA1_REG",
+	"XMAC_DEBUG_SEL_REG",
+	"XMAC_TRAINING_VECT_REG",
+};
+
+
+
+uint64_t pcs_offset[] = {
+	PCS_MII_CTRL_REG,
+	PCS_MII_STATUS_REG,
+	PCS_MII_ADVERT_REG,
+	PCS_MII_LPA_REG,
+	PCS_CONFIG_REG,
+	PCS_STATE_MACHINE_REG,
+	PCS_INTR_STATUS_REG,
+	PCS_DATAPATH_MODE_REG,
+	PCS_PACKET_COUNT_REG,
+};
+
+const char *pcs_name[] = {
+	"PCS_MII_CTRL_REG",
+	"PCS_MII_STATUS_REG",
+	"PCS_MII_ADVERT_REG",
+	"PCS_MII_LPA_REG",
+	"PCS_CONFIG_REG",
+	"PCS_STATE_MACHINE_REG",
+	"PCS_INTR_STATUS_REG",
+	"PCS_DATAPATH_MODE_REG",
+	"PCS_PACKET_COUNT_REG",
+};
+uint64_t bmac_offset[] = {
+	BTXMAC_SW_RST_REG,
+	BRXMAC_SW_RST_REG,
+	MAC_SEND_PAUSE_REG,
+	BTXMAC_STATUS_REG,
+	BRXMAC_STATUS_REG,
+	BMAC_CTRL_STAT_REG,
+	BTXMAC_STAT_MSK_REG,
+	BRXMAC_STAT_MSK_REG,
+	BMAC_C_S_MSK_REG,
+	TXMAC_CONFIG_REG,
+	RXMAC_CONFIG_REG,
+	MAC_CTRL_CONFIG_REG,
+	MAC_XIF_CONFIG_REG,
+	BMAC_MIN_REG,
+	BMAC_MAX_REG,
+	MAC_PA_SIZE_REG,
+	MAC_CTRL_TYPE_REG,
+	BMAC_ADDR0_REG,
+	BMAC_ADDR1_REG,
+	BMAC_ADDR2_REG,
+	BMAC_ADDR3_REG,
+	BMAC_ADDR4_REG,
+	BMAC_ADDR5_REG,
+	BMAC_ADDR6_REG,
+	BMAC_ADDR7_REG,
+	BMAC_ADDR8_REG,
+	BMAC_ADDR9_REG,
+	BMAC_ADDR10_REG,
+	BMAC_ADDR11_REG,
+	BMAC_ADDR12_REG,
+	BMAC_ADDR13_REG,
+	BMAC_ADDR14_REG,
+	BMAC_ADDR15_REG,
+	BMAC_ADDR16_REG,
+	BMAC_ADDR17_REG,
+	BMAC_ADDR18_REG,
+	BMAC_ADDR19_REG,
+	BMAC_ADDR20_REG,
+	BMAC_ADDR21_REG,
+	BMAC_ADDR22_REG,
+	BMAC_ADDR23_REG,
+	MAC_FC_ADDR0_REG,
+	MAC_FC_ADDR1_REG,
+	MAC_FC_ADDR2_REG,
+	MAC_ADDR_FILT0_REG,
+	MAC_ADDR_FILT1_REG,
+	MAC_ADDR_FILT2_REG,
+	MAC_ADDR_FILT12_MASK_REG,
+	MAC_ADDR_FILT00_MASK_REG,
+	MAC_HASH_TBL0_REG,
+	MAC_HASH_TBL1_REG,
+	MAC_HASH_TBL2_REG,
+	MAC_HASH_TBL3_REG,
+	MAC_HASH_TBL4_REG,
+	MAC_HASH_TBL5_REG,
+	MAC_HASH_TBL6_REG,
+	MAC_HASH_TBL7_REG,
+	MAC_HASH_TBL8_REG,
+	MAC_HASH_TBL9_REG,
+	MAC_HASH_TBL10_REG,
+	MAC_HASH_TBL11_REG,
+	MAC_HASH_TBL12_REG,
+	MAC_HASH_TBL13_REG,
+	MAC_HASH_TBL14_REG,
+	MAC_HASH_TBL15_REG,
+	RXMAC_FRM_CNT_REG,
+	MAC_LEN_ER_CNT_REG,
+	BMAC_AL_ER_CNT_REG,
+	BMAC_CRC_ER_CNT_REG,
+	BMAC_CD_VIO_CNT_REG,
+	BMAC_SM_REG,
+	BMAC_ALTAD_CMPEN_REG,
+	BMAC_HOST_INF0_REG,
+	BMAC_HOST_INF1_REG,
+	BMAC_HOST_INF2_REG,
+	BMAC_HOST_INF3_REG,
+	BMAC_HOST_INF4_REG,
+	BMAC_HOST_INF5_REG,
+	BMAC_HOST_INF6_REG,
+	BMAC_HOST_INF7_REG,
+	BMAC_HOST_INF8_REG,
+	BTXMAC_BYTE_CNT_REG,
+	BTXMAC_FRM_CNT_REG,
+	BRXMAC_BYTE_CNT_REG,
+};
+
+const char *bmac_name[] = {
+	"BTXMAC_SW_RST_REG",
+	"BRXMAC_SW_RST_REG",
+	"MAC_SEND_PAUSE_REG",
+	"BTXMAC_STATUS_REG",
+	"BRXMAC_STATUS_REG",
+	"BMAC_CTRL_STAT_REG",
+	"BTXMAC_STAT_MSK_REG",
+	"BRXMAC_STAT_MSK_REG",
+	"BMAC_C_S_MSK_REG",
+	"TXMAC_CONFIG_REG",
+	"RXMAC_CONFIG_REG",
+	"MAC_CTRL_CONFIG_REG",
+	"MAC_XIF_CONFIG_REG",
+	"BMAC_MIN_REG",
+	"BMAC_MAX_REG",
+	"MAC_PA_SIZE_REG",
+	"MAC_CTRL_TYPE_REG",
+	"BMAC_ADDR0_REG",
+	"BMAC_ADDR1_REG",
+	"BMAC_ADDR2_REG",
+	"BMAC_ADDR3_REG",
+	"BMAC_ADDR4_REG",
+	"BMAC_ADDR5_REG",
+	"BMAC_ADDR6_REG",
+	"BMAC_ADDR7_REG",
+	"BMAC_ADDR8_REG",
+	"BMAC_ADDR9_REG",
+	"BMAC_ADDR10_REG",
+	"BMAC_ADDR11_REG",
+	"BMAC_ADDR12_REG",
+	"BMAC_ADDR13_REG",
+	"BMAC_ADDR14_REG",
+	"BMAC_ADDR15_REG",
+	"BMAC_ADDR16_REG",
+	"BMAC_ADDR17_REG",
+	"BMAC_ADDR18_REG",
+	"BMAC_ADDR19_REG",
+	"BMAC_ADDR20_REG",
+	"BMAC_ADDR21_REG",
+	"BMAC_ADDR22_REG",
+	"BMAC_ADDR23_REG",
+	"MAC_FC_ADDR0_REG",
+	"MAC_FC_ADDR1_REG",
+	"MAC_FC_ADDR2_REG",
+	"MAC_ADDR_FILT0_REG",
+	"MAC_ADDR_FILT1_REG",
+	"MAC_ADDR_FILT2_REG",
+	"MAC_ADDR_FILT12_MASK_REG",
+	"MAC_ADDR_FILT00_MASK_REG",
+	"MAC_HASH_TBL0_REG",
+	"MAC_HASH_TBL1_REG",
+	"MAC_HASH_TBL2_REG",
+	"MAC_HASH_TBL3_REG",
+	"MAC_HASH_TBL4_REG",
+	"MAC_HASH_TBL5_REG",
+	"MAC_HASH_TBL6_REG",
+	"MAC_HASH_TBL7_REG",
+	"MAC_HASH_TBL8_REG",
+	"MAC_HASH_TBL9_REG",
+	"MAC_HASH_TBL10_REG",
+	"MAC_HASH_TBL11_REG",
+	"MAC_HASH_TBL12_REG",
+	"MAC_HASH_TBL13_REG",
+	"MAC_HASH_TBL14_REG",
+	"MAC_HASH_TBL15_REG",
+	"RXMAC_FRM_CNT_REG",
+	"MAC_LEN_ER_CNT_REG",
+	"BMAC_AL_ER_CNT_REG",
+	"BMAC_CRC_ER_CNT_REG",
+	"BMAC_CD_VIO_CNT_REG",
+	"BMAC_SM_REG",
+	"BMAC_ALTAD_CMPEN_REG",
+	"BMAC_HOST_INF0_REG",
+	"BMAC_HOST_INF1_REG",
+	"BMAC_HOST_INF2_REG",
+	"BMAC_HOST_INF3_REG",
+	"BMAC_HOST_INF4_REG",
+	"BMAC_HOST_INF5_REG",
+	"BMAC_HOST_INF6_REG",
+	"BMAC_HOST_INF7_REG",
+	"BMAC_HOST_INF8_REG",
+	"BTXMAC_BYTE_CNT_REG",
+	"BTXMAC_FRM_CNT_REG",
+	"BRXMAC_BYTE_CNT_REG",
+};
+
+
+static void
+npi_pcs_dump_regs(npi_handle_t handle, uint8_t port)
+{
+	uint64_t value;
+	int num_regs, i;
+
+	num_regs = sizeof (pcs_offset) / sizeof (uint64_t);
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+					  "\nPCS Register Dump for port %d\n",
+					  port));
+	for (i = 0; i < num_regs; i++) {
+		PCS_REG_RD(handle, port, pcs_offset[i], &value);
+		NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+						  "%08llx %s\t %08llx \n",
+						  (PCS_REG_ADDR((port), (pcs_offset[i]))),
+						  pcs_name[i], value));
+	}
+
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+					  "\n PCS Register Dump for port %d done\n",
+					  port));
+}
+
+
+npi_status_t
+npi_mac_dump_regs(npi_handle_t handle, uint8_t port)
+{
+
+	uint64_t value;
+	int num_regs, i;
+
+	if (!IS_PORT_NUM_VALID(port)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_mac_dump_regs"
+				    " Invalid Input: portn <%d>",
+				    port));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(port));
+	}
+
+	switch (port) {
+	case 0:
+	case 1:
+		num_regs = sizeof (xmac_offset) / sizeof (uint64_t);
+		NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+				    "\nXMAC Register Dump for port %d\n",
+				    port));
+		for (i = 0; i < num_regs; i++) {
+			XMAC_REG_RD(handle, port, xmac_offset[i], &value);
+			NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+				"%08llx %s\t %08llx \n",
+				(XMAC_REG_ADDR((port), (xmac_offset[i]))),
+				xmac_name[i], value));
+		}
+
+		NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+			    "\n XMAC Register Dump for port %d done\n",
+			    port));
+		break;
+
+	case 2:
+	case 3:
+		num_regs = sizeof (bmac_offset) / sizeof (uint64_t);
+		NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+				    "\nBMAC Register Dump for port %d\n",
+				    port));
+		for (i = 0; i < num_regs; i++) {
+			BMAC_REG_RD(handle, port, bmac_offset[i], &value);
+			NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+				"%08llx %s\t %08llx \n",
+				(BMAC_REG_ADDR((port), (bmac_offset[i]))),
+				bmac_name[i], value));
+		}
+
+		NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+			    "\n BMAC Register Dump for port %d done\n",
+			    port));
+		break;
+	}
+
+	return (NPI_SUCCESS);
+}
+
+
+
+npi_status_t
+npi_mac_pcs_link_intr_enable(npi_handle_t handle, uint8_t portn)
+{
+	pcs_cfg_t pcs_cfg;
+
+	if (!IS_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_mac_pcs_link_intr_enable"
+				    " Invalid Input: portn <%d>",
+				    portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	PCS_REG_RD(handle, portn, PCS_CONFIG_REG, &pcs_cfg.value);
+	pcs_cfg.bits.w0.mask = 0;
+	PCS_REG_WR(handle, portn, PCS_CONFIG_REG, pcs_cfg.value);
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_mac_pcs_link_intr_disable(npi_handle_t handle, uint8_t portn)
+{
+	pcs_cfg_t pcs_cfg;
+
+	if (!IS_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_mac_pcs_link_intr_disable"
+				    " Invalid Input: portn <%d>", portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	PCS_REG_RD(handle, portn, PCS_CONFIG_REG, &pcs_cfg.val.lsw);
+	pcs_cfg.bits.w0.mask = 1;
+	PCS_REG_WR(handle, portn, PCS_CONFIG_REG, pcs_cfg.val.lsw);
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_xmac_xpcs_link_intr_enable(npi_handle_t handle, uint8_t portn)
+{
+	xpcs_stat1_t xpcs_mask1;
+
+	if (!IS_XMAC_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_xmac_xpcs_link_intr_enable"
+				    " Invalid Input: portn <%d>", portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	XPCS_REG_RD(handle, portn, XPCS_MASK_1_REG, &xpcs_mask1.val.lsw);
+	xpcs_mask1.bits.w0.csr_rx_link_stat = 1;
+	XPCS_REG_WR(handle, portn, XPCS_MASK_1_REG, xpcs_mask1.val.lsw);
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_xmac_xpcs_link_intr_disable(npi_handle_t handle, uint8_t portn)
+{
+	xpcs_stat1_t xpcs_mask1;
+
+	if (!IS_XMAC_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_xmac_xpcs_link_intr_disable"
+				    " Invalid Input: portn <%d>", portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	XPCS_REG_RD(handle, portn, XPCS_MASK_1_REG, &xpcs_mask1.val.lsw);
+	xpcs_mask1.bits.w0.csr_rx_link_stat = 0;
+	XPCS_REG_WR(handle, portn, XPCS_MASK_1_REG, xpcs_mask1.val.lsw);
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_mac_mif_link_intr_disable(npi_handle_t handle, uint8_t portn)
+{
+	mif_cfg_t mif_cfg;
+
+	if (!IS_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_mac_mif_link_intr_disable"
+				    " Invalid Input: portn <%d>", portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	MIF_REG_RD(handle, MIF_CONFIG_REG, &mif_cfg.val.lsw);
+
+	mif_cfg.bits.w0.phy_addr = portn;
+	mif_cfg.bits.w0.poll_en = 0;
+
+	MIF_REG_WR(handle, MIF_CONFIG_REG, mif_cfg.val.lsw);
+
+	NXGE_DELAY(20);
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_mac_hashtab_entry(npi_handle_t handle, io_op_t op, uint8_t portn,
+			uint8_t entryn, uint16_t *data)
+{
+	uint64_t val;
+
+	if ((op != OP_SET) && (op != OP_GET)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_mac_hashtab_entry"
+				    " Invalid Input: op <0x%x>", op));
+		return (NPI_FAILURE | NPI_MAC_OPCODE_INVALID(portn));
+	}
+
+	if (!IS_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			    " npi_mac_hashtab_entry"
+			    " Invalid Input: portn <%d>", portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	if (entryn >= MAC_MAX_HASH_ENTRY) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_mac_hashtab_entry"
+				    " Invalid Input: entryn <0x%x>",
+				    entryn));
+		return (NPI_FAILURE | NPI_MAC_HASHTAB_ENTRY_INVALID(portn));
+	}
+
+	if (op == OP_SET) {
+		val = *data;
+		if ((portn == XMAC_PORT_0) || (portn == XMAC_PORT_1)) {
+			XMAC_REG_WR(handle, portn,
+					XMAC_HASH_TBLN_REG_ADDR(entryn), val);
+		} else {
+			BMAC_REG_WR(handle, portn,
+					BMAC_HASH_TBLN_REG_ADDR(entryn), val);
+		}
+	} else {
+		if ((portn == XMAC_PORT_0) || (portn == XMAC_PORT_1)) {
+			XMAC_REG_RD(handle, portn,
+					XMAC_HASH_TBLN_REG_ADDR(entryn), &val);
+		} else {
+			BMAC_REG_RD(handle, portn,
+					BMAC_HASH_TBLN_REG_ADDR(entryn), &val);
+		}
+		*data = val & 0xFFFF;
+	}
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_mac_hostinfo_entry(npi_handle_t handle, io_op_t op, uint8_t portn,
+				uint8_t entryn, hostinfo_t *hostinfo)
+{
+	if ((op != OP_SET) && (op != OP_GET)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_mac_hostinfo_entry"
+				    " Invalid Input: op <0x%x>", op));
+		return (NPI_FAILURE | NPI_MAC_OPCODE_INVALID(portn));
+	}
+
+	if (!IS_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_mac_hostinfo_entry"
+				    " Invalid Input: portn <%d>", portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	if ((portn == XMAC_PORT_0) || (portn == XMAC_PORT_1)) {
+		if (entryn >= XMAC_MAX_HOST_INFO_ENTRY) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_mac_hostinfo_entry"
+					    " Invalid Input: entryn <0x%x>",
+					    entryn));
+			return (NPI_FAILURE |
+				NPI_MAC_HOSTINFO_ENTRY_INVALID(portn));
+		}
+	} else {
+		if (entryn >= BMAC_MAX_HOST_INFO_ENTRY) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_mac_hostinfo_entry"
+					    " Invalid Input: entryn <0x%x>",
+					    entryn));
+			return (NPI_FAILURE |
+				NPI_MAC_HOSTINFO_ENTRY_INVALID(portn));
+		}
+	}
+
+	if (op == OP_SET) {
+		if ((portn == XMAC_PORT_0) || (portn == XMAC_PORT_1)) {
+			XMAC_REG_WR(handle, portn,
+					XMAC_HOST_INFN_REG_ADDR(entryn),
+					hostinfo->value);
+		} else {
+			BMAC_REG_WR(handle, portn,
+					BMAC_HOST_INFN_REG_ADDR(entryn),
+					hostinfo->value);
+		}
+	} else {
+		if ((portn == XMAC_PORT_0) || (portn == XMAC_PORT_1)) {
+			XMAC_REG_RD(handle, portn,
+					XMAC_HOST_INFN_REG_ADDR(entryn),
+					&hostinfo->value);
+		} else {
+			BMAC_REG_RD(handle, portn,
+					BMAC_HOST_INFN_REG_ADDR(entryn),
+					&hostinfo->value);
+		}
+	}
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_mac_altaddr_enable(npi_handle_t handle, uint8_t portn, uint8_t addrn)
+{
+	uint64_t val;
+
+	if (!IS_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_mac_altaddr_enable"
+				    " Invalid Input: portn <%d>", portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	if ((portn == XMAC_PORT_0) || (portn == XMAC_PORT_1)) {
+		if (addrn >= XMAC_MAX_ALT_ADDR_ENTRY) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_mac_altaddr_enable"
+					    " Invalid Input: addrn <0x%x>",
+					    addrn));
+			return (NPI_FAILURE |
+				NPI_MAC_ALT_ADDR_ENTRY_INVALID(portn));
+		}
+		XMAC_REG_RD(handle, portn, XMAC_ADDR_CMPEN_REG, &val);
+		val |= (1 << addrn);
+		XMAC_REG_WR(handle, portn, XMAC_ADDR_CMPEN_REG, val);
+	} else {
+		if (addrn > BMAC_MAX_ALT_ADDR_ENTRY) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_mac_altaddr_enable"
+					    " Invalid Input: addrn <0x%x>",
+					    addrn));
+			return (NPI_FAILURE |
+				NPI_MAC_ALT_ADDR_ENTRY_INVALID(portn));
+		}
+		BMAC_REG_RD(handle, portn, BMAC_ALTAD_CMPEN_REG, &val);
+		val |= (1 << addrn);
+		BMAC_REG_WR(handle, portn, BMAC_ALTAD_CMPEN_REG, val);
+	}
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * While all bits of XMAC_ADDR_CMPEN_REG are for alternate MAC addresses,
+ * bit0 of BMAC_ALTAD_CMPEN_REG is for unique MAC address.
+ */
+npi_status_t
+npi_mac_altaddr_disable(npi_handle_t handle, uint8_t portn, uint8_t addrn)
+{
+	uint64_t val;
+
+	if (!IS_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				" npi_mac_altaddr_disable"
+				" Invalid Input: portn <%d>", portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	if ((portn == XMAC_PORT_0) || (portn == XMAC_PORT_1)) {
+		if (addrn >= XMAC_MAX_ALT_ADDR_ENTRY) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					" npi_mac_altaddr_disable"
+					" Invalid Input: addrn <0x%x>",
+					addrn));
+			return (NPI_FAILURE |
+				NPI_MAC_ALT_ADDR_ENTRY_INVALID(portn));
+		}
+		XMAC_REG_RD(handle, portn, XMAC_ADDR_CMPEN_REG, &val);
+		val &= ~(1 << addrn);
+		XMAC_REG_WR(handle, portn, XMAC_ADDR_CMPEN_REG, val);
+	} else {
+		if (addrn > BMAC_MAX_ALT_ADDR_ENTRY) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					" npi_mac_altaddr_disable"
+					" Invalid Input: addrn <0x%x>",
+				    addrn));
+			return (NPI_FAILURE |
+				NPI_MAC_ALT_ADDR_ENTRY_INVALID(portn));
+		}
+		BMAC_REG_RD(handle, portn, BMAC_ALTAD_CMPEN_REG, &val);
+		val &= ~(1 << addrn);
+		BMAC_REG_WR(handle, portn, BMAC_ALTAD_CMPEN_REG, val);
+	}
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_mac_altaddr_entry(npi_handle_t handle, io_op_t op, uint8_t portn,
+			uint8_t entryn, npi_mac_addr_t *data)
+{
+	uint64_t val0, val1, val2;
+
+	if (!IS_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_mac_altaddr_entry",
+				    " Invalid Input: portn <%d>",
+				    portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	if ((op != OP_SET) && (op != OP_GET)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_mac_altaddr_entry"
+					    " Invalid Input: op <0x%x>", op));
+		return (NPI_FAILURE | NPI_MAC_OPCODE_INVALID(portn));
+	}
+
+	if ((portn == XMAC_PORT_0) || (portn == XMAC_PORT_1)) {
+		if (entryn >= XMAC_MAX_ALT_ADDR_ENTRY) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_mac_altaddr_entry"
+					    " Invalid Input: entryn <0x%x>",
+					    entryn));
+			return (NPI_FAILURE |
+				NPI_MAC_ALT_ADDR_ENTRY_INVALID(portn));
+		}
+		if (op == OP_SET) {
+			val0 = data->w0;
+			val1 = data->w1;
+			val2 = data->w2;
+			XMAC_REG_WR(handle, portn,
+				XMAC_ALT_ADDR0N_REG_ADDR(entryn), val0);
+			XMAC_REG_WR(handle, portn,
+				XMAC_ALT_ADDR1N_REG_ADDR(entryn), val1);
+			XMAC_REG_WR(handle, portn,
+				XMAC_ALT_ADDR2N_REG_ADDR(entryn), val2);
+		} else {
+			XMAC_REG_RD(handle, portn,
+				XMAC_ALT_ADDR0N_REG_ADDR(entryn), &val0);
+			XMAC_REG_RD(handle, portn,
+				XMAC_ALT_ADDR1N_REG_ADDR(entryn), &val1);
+			XMAC_REG_RD(handle, portn,
+				XMAC_ALT_ADDR2N_REG_ADDR(entryn), &val2);
+			data->w0 = val0 & 0xFFFF;
+			data->w1 = val1 & 0xFFFF;
+			data->w2 = val2 & 0xFFFF;
+		}
+	} else {
+		if (entryn >= BMAC_MAX_ALT_ADDR_ENTRY) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_mac_altaddr_entry"
+					    " Invalid Input: entryn <0x%x>",
+					    entryn));
+			return (NPI_FAILURE |
+				NPI_MAC_ALT_ADDR_ENTRY_INVALID(portn));
+		}
+		if (op == OP_SET) {
+			val0 = data->w0;
+			val1 = data->w1;
+			val2 = data->w2;
+			BMAC_REG_WR(handle, portn,
+				BMAC_ALT_ADDR0N_REG_ADDR(entryn), val0);
+			BMAC_REG_WR(handle, portn,
+				BMAC_ALT_ADDR1N_REG_ADDR(entryn), val1);
+			BMAC_REG_WR(handle, portn,
+				BMAC_ALT_ADDR2N_REG_ADDR(entryn), val2);
+		} else {
+			BMAC_REG_RD(handle, portn,
+				BMAC_ALT_ADDR0N_REG_ADDR(entryn), &val0);
+			BMAC_REG_RD(handle, portn,
+				BMAC_ALT_ADDR1N_REG_ADDR(entryn), &val1);
+			BMAC_REG_RD(handle, portn,
+				BMAC_ALT_ADDR2N_REG_ADDR(entryn), &val2);
+			data->w0 = val0 & 0xFFFF;
+			data->w1 = val1 & 0xFFFF;
+			data->w2 = val2 & 0xFFFF;
+		}
+	}
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_mac_port_attr(npi_handle_t handle, io_op_t op, uint8_t portn,
+			npi_attr_t *attrp)
+{
+	uint64_t val = 0;
+	uint32_t attr;
+
+	if (!IS_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_mac_port_attr"
+				    " Invalid Input: portn <%d>",
+				    portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	if ((op != OP_GET) && (op != OP_SET)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_mac_port_attr"
+				    " Invalid Input: op <0x%x>", op));
+		return (NPI_FAILURE | NPI_MAC_OPCODE_INVALID(portn));
+	}
+
+	switch (attrp->type) {
+	case MAC_PORT_MODE:
+		switch (portn) {
+		case XMAC_PORT_0:
+		case XMAC_PORT_1:
+			if (op == OP_SET) {
+				attr = attrp->idata[0];
+				if ((attr != MAC_MII_MODE) &&
+					(attr != MAC_GMII_MODE) &&
+					(attr != MAC_XGMII_MODE)) {
+					NPI_ERROR_MSG((handle.function,
+						    NPI_ERR_CTL,
+						    " npi_mac_port_attr"
+						    " Invalid Input:"
+						    " MAC_PORT_MODE <0x%x>",
+						    attr));
+					return (NPI_FAILURE |
+					NPI_MAC_PORT_ATTR_INVALID(portn));
+				}
+				XMAC_REG_RD(handle, portn, XMAC_CONFIG_REG,
+						&val);
+				val &= ~XMAC_XIF_MII_MODE_MASK;
+				switch (attr) {
+				case MAC_MII_MODE:
+					val |= (XMAC_XIF_MII_MODE <<
+						XMAC_XIF_MII_MODE_SHIFT);
+					break;
+				case MAC_GMII_MODE:
+					val |= (XMAC_XIF_GMII_MODE <<
+						XMAC_XIF_MII_MODE_SHIFT);
+					break;
+				case MAC_XGMII_MODE:
+					val |= (XMAC_XIF_XGMII_MODE <<
+						XMAC_XIF_MII_MODE_SHIFT);
+					break;
+				default:
+					return (NPI_FAILURE |
+					NPI_MAC_PORT_ATTR_INVALID(portn));
+				}
+				XMAC_REG_WR(handle, portn, XMAC_CONFIG_REG,
+						val);
+			} else {
+				XMAC_REG_RD(handle, portn, XMAC_CONFIG_REG,
+						&val);
+				val &= XMAC_XIF_MII_MODE_MASK;
+				attr = val >> XMAC_XIF_MII_MODE_SHIFT;
+				attrp->odata[0] = attr;
+			}
+			break;
+		case BMAC_PORT_0:
+		case BMAC_PORT_1:
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_mac_port_attr"
+					    " Invalid Input:"
+					    " MAC_PORT_MODE <0x%x>",
+					    attrp->type));
+			return (NPI_FAILURE |
+				NPI_MAC_PORT_ATTR_INVALID(portn));
+		default:
+			return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+		}
+		break;
+
+	case MAC_PORT_FRAME_SIZE: {
+		uint32_t min_fsize;
+		uint32_t max_fsize;
+
+		switch (portn) {
+		case XMAC_PORT_0:
+		case XMAC_PORT_1:
+			if (op == OP_SET) {
+				min_fsize = attrp->idata[0];
+				max_fsize = attrp->idata[1];
+				if ((min_fsize & ~XMAC_MIN_TX_FRM_SZ_MASK)
+						!= 0) {
+					NPI_ERROR_MSG((handle.function,
+						    NPI_ERR_CTL,
+						    " npi_mac_port_attr"
+						    " MAC_PORT_FRAME_SIZE:"
+						    " Invalid Input:"
+						    " xmac_min_fsize <0x%x>",
+						    min_fsize));
+					return (NPI_FAILURE |
+					NPI_MAC_PORT_ATTR_INVALID(portn));
+				}
+				if ((max_fsize & ~XMAC_MAX_FRM_SZ_MASK)
+						!= 0) {
+					NPI_ERROR_MSG((handle.function,
+						    NPI_ERR_CTL,
+						    " npi_mac_port_attr"
+						    " MAC_PORT_FRAME_SIZE:"
+						    " Invalid Input:"
+						    " xmac_max_fsize <0x%x>",
+						    max_fsize));
+					return (NPI_FAILURE |
+					NPI_MAC_PORT_ATTR_INVALID(portn));
+				}
+				XMAC_REG_RD(handle, portn, XMAC_MIN_REG, &val);
+				val &= ~(XMAC_MIN_TX_FRM_SZ_MASK |
+					XMAC_MIN_RX_FRM_SZ_MASK);
+				val |= (min_fsize << XMAC_MIN_TX_FRM_SZ_SHIFT);
+				val |= (min_fsize << XMAC_MIN_RX_FRM_SZ_SHIFT);
+				XMAC_REG_WR(handle, portn, XMAC_MIN_REG, val);
+				XMAC_REG_WR(handle, portn, XMAC_MAX_REG,
+						max_fsize);
+			} else {
+				XMAC_REG_RD(handle, portn, XMAC_MIN_REG, &val);
+				min_fsize = (val & XMAC_MIN_TX_FRM_SZ_MASK)
+						>> XMAC_MIN_TX_FRM_SZ_SHIFT;
+				XMAC_REG_RD(handle, portn, XMAC_MAX_REG, &val);
+				attrp->odata[0] = min_fsize;
+				attrp->odata[1] = val;
+			}
+			break;
+		case BMAC_PORT_0:
+		case BMAC_PORT_1:
+			if (op == OP_SET) {
+				min_fsize = attrp->idata[0];
+				max_fsize = attrp->idata[1];
+				if ((min_fsize & ~BMAC_MIN_FRAME_MASK)
+						!= 0) {
+					NPI_ERROR_MSG((handle.function,
+						    NPI_ERR_CTL,
+						    " npi_mac_port_attr"
+						    " MAC_FRAME_SIZE:"
+						    " Invalid Input:"
+						    " bmac_min_fsize <0x%x>",
+						    min_fsize));
+					return (NPI_FAILURE |
+					NPI_MAC_PORT_ATTR_INVALID(portn));
+				}
+				if ((max_fsize & ~BMAC_MAX_FRAME_MASK)
+						!= 0) {
+					NPI_ERROR_MSG((handle.function,
+						    NPI_ERR_CTL,
+						    " npi_mac_port_attr"
+						    " MAC_FRAME_SIZE:"
+						    " Invalid Input:"
+						    " bmac_max_fsize <0x%x>",
+						    max_fsize));
+					return (NPI_FAILURE |
+					NPI_MAC_PORT_ATTR_INVALID(portn));
+				}
+				BMAC_REG_RD(handle, portn, BMAC_MAX_REG, &val);
+				val &= ~BMAC_MAX_FRAME_MASK;
+				if (max_fsize <= 0x5EE)
+					val |= 0x5EE;
+				else if ((max_fsize > 0x5EE) &&
+					(max_fsize <= 0x5F6))
+					val |= 0x5F6;
+				else if ((max_fsize > 0x5F6) &&
+					(max_fsize <= 0x7D6))
+					val |= 0x7D6;
+				else if ((max_fsize > 0x7D6) &&
+					(max_fsize <= 0x232E))
+					val |= 0x232E;
+				else if ((max_fsize > 0x232E) &&
+					(max_fsize <= 0x2406))
+					val |= 0x2406;
+
+				BMAC_REG_WR(handle, portn, BMAC_MAX_REG, val);
+				BMAC_REG_WR(handle, portn, BMAC_MIN_REG,
+						min_fsize);
+			} else {
+				BMAC_REG_RD(handle, portn, BMAC_MIN_REG, &val);
+				min_fsize = val & BMAC_MIN_FRAME_MASK;
+				BMAC_REG_RD(handle, portn, BMAC_MAX_REG, &val);
+				max_fsize = val & BMAC_MAX_FRAME_MASK;
+				attrp->odata[0] = min_fsize;
+				attrp->odata[1] = max_fsize;
+			}
+			break;
+		default:
+			return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+		}
+	}	break;
+
+	case BMAC_PORT_MAX_BURST_SIZE: {
+		uint32_t burst_size;
+		switch (portn) {
+		case XMAC_PORT_0:
+		case XMAC_PORT_1:
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_mac_port_attr"
+					    " BMAC_PORT_MAX_BURST_SIZE:"
+					    " Invalid Input: portn <%d>",
+					    portn));
+			return (NPI_FAILURE | NPI_MAC_PORT_ATTR_INVALID(portn));
+		case BMAC_PORT_0:
+		case BMAC_PORT_1:
+			/* NOTE: Not used in Full duplex mode */
+			if (op == OP_SET) {
+				burst_size = attrp->idata[0];
+				if ((burst_size & ~0x7FFF) != 0) {
+					NPI_ERROR_MSG((handle.function,
+						    NPI_ERR_CTL,
+						    " npi_mac_port_attr"
+						    " BMAC_MAX_BURST_SIZE:"
+						    " Invalid Input:"
+						    " burst_size <0x%x>",
+						    burst_size));
+					return (NPI_FAILURE |
+					NPI_MAC_PORT_ATTR_INVALID(portn));
+				}
+				BMAC_REG_RD(handle, portn, BMAC_MAX_REG, &val);
+				val &= ~BMAC_MAX_BURST_MASK;
+				val |= (burst_size << BMAC_MAX_BURST_SHIFT);
+				BMAC_REG_WR(handle, portn, BMAC_MAX_REG, val);
+			} else {
+				BMAC_REG_RD(handle, portn, BMAC_MAX_REG, &val);
+				burst_size = (val & BMAC_MAX_BURST_MASK)
+						>> BMAC_MAX_BURST_SHIFT;
+				attrp->odata[0] = burst_size;
+			}
+			break;
+		default:
+			return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+		}
+	}	break;
+
+	case BMAC_PORT_PA_SIZE: {
+		uint32_t pa_size;
+		switch (portn) {
+		case XMAC_PORT_0:
+		case XMAC_PORT_1:
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_mac_port_attr"
+					    " BMAC_PORT_PA_SIZE:"
+					    " Invalid Input: portn <%d>",
+					    portn));
+			return (NPI_FAILURE | NPI_MAC_PORT_ATTR_INVALID(portn));
+		case BMAC_PORT_0:
+		case BMAC_PORT_1:
+			if (op == OP_SET) {
+				pa_size = attrp->idata[0];
+				if ((pa_size & ~0x3FF) != 0) {
+					NPI_ERROR_MSG((handle.function,
+					    NPI_ERR_CTL,
+					    " npi_mac_port_attr"
+					    " BMAC_PORT_PA_SIZE:"
+					    " Invalid Input: pa_size <0x%x>",
+					    pa_size));
+
+					return (NPI_FAILURE |
+					NPI_MAC_PORT_ATTR_INVALID(portn));
+				}
+				BMAC_REG_RD(handle, portn, MAC_PA_SIZE_REG,
+					    &val);
+				val &= ~BMAC_PA_SIZE_MASK;
+				val |= (pa_size << 0);
+				BMAC_REG_WR(handle, portn, MAC_PA_SIZE_REG,
+					    val);
+			} else {
+				BMAC_REG_RD(handle, portn, MAC_PA_SIZE_REG,
+					    &val);
+				pa_size = (val & BMAC_PA_SIZE_MASK) >> 0;
+				attrp->odata[0] = pa_size;
+			}
+			break;
+		default:
+			return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+		}
+	}	break;
+
+	case BMAC_PORT_CTRL_TYPE: {
+		uint32_t ctrl_type;
+		switch (portn) {
+		case XMAC_PORT_0:
+		case XMAC_PORT_1:
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_mac_port_attr"
+					    " BMAC_PORT_CTRL_TYPE:"
+					    " Invalid Input: portn <%d>",
+					    portn));
+			return (NPI_FAILURE | NPI_MAC_PORT_ATTR_INVALID(portn));
+		case BMAC_PORT_0:
+		case BMAC_PORT_1:
+			if (op == OP_SET) {
+				ctrl_type = attrp->idata[0];
+				if ((ctrl_type & ~0xFFFF) != 0) {
+					NPI_ERROR_MSG((handle.function,
+						    NPI_ERR_CTL,
+						    " npi_mac_port_attr"
+						    " BMAC_PORT_CTRL_TYPE:"
+						    " Invalid Input:"
+						    " ctrl_type <0x%x>",
+						    ctrl_type));
+					return (NPI_FAILURE |
+					NPI_MAC_PORT_ATTR_INVALID(portn));
+				}
+				BMAC_REG_WR(handle, portn, MAC_CTRL_TYPE_REG,
+						val);
+			} else {
+				BMAC_REG_RD(handle, portn, MAC_CTRL_TYPE_REG,
+						&val);
+				ctrl_type = (val & 0xFFFF);
+				attrp->odata[0] = ctrl_type;
+			}
+			break;
+		default:
+			return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+		}
+	}	break;
+
+	case XMAC_10G_PORT_IPG:
+		{
+		uint32_t	ipg0;
+
+		switch (portn) {
+		case XMAC_PORT_0:
+		case XMAC_PORT_1:
+			if (op == OP_SET) {
+				ipg0 = attrp->idata[0];
+
+				if ((ipg0 != XGMII_IPG_12_15) &&
+					(ipg0 != XGMII_IPG_16_19) &&
+					(ipg0 != XGMII_IPG_20_23)) {
+					NPI_ERROR_MSG((handle.function,
+						    NPI_ERR_CTL,
+						    " npi_mac_port_attr"
+						    " MAC_10G_PORT_IPG:"
+						    " Invalid Input:"
+						    " xgmii_ipg <0x%x>",
+						    ipg0));
+					return (NPI_FAILURE |
+					NPI_MAC_PORT_ATTR_INVALID(portn));
+				}
+
+				XMAC_REG_RD(handle, portn, XMAC_IPG_REG, &val);
+				val &= ~(XMAC_IPG_VALUE_MASK |
+					XMAC_IPG_VALUE1_MASK);
+
+				switch (ipg0) {
+				case XGMII_IPG_12_15:
+					val |= (IPG_12_15_BYTE <<
+						XMAC_IPG_VALUE_SHIFT);
+					break;
+				case XGMII_IPG_16_19:
+					val |= (IPG_16_19_BYTE <<
+						XMAC_IPG_VALUE_SHIFT);
+					break;
+				case XGMII_IPG_20_23:
+					val |= (IPG_20_23_BYTE <<
+						XMAC_IPG_VALUE_SHIFT);
+					break;
+				default:
+					return (NPI_FAILURE |
+					NPI_MAC_PORT_ATTR_INVALID(portn));
+				}
+				XMAC_REG_WR(handle, portn, XMAC_IPG_REG, val);
+			} else {
+				XMAC_REG_RD(handle, portn, XMAC_IPG_REG, &val);
+				ipg0 = (val & XMAC_IPG_VALUE_MASK) >>
+					XMAC_IPG_VALUE_SHIFT;
+				switch (ipg0) {
+				case IPG_12_15_BYTE:
+					attrp->odata[0] = XGMII_IPG_12_15;
+					break;
+				case IPG_16_19_BYTE:
+					attrp->odata[0] = XGMII_IPG_16_19;
+					break;
+				case IPG_20_23_BYTE:
+					attrp->odata[0] = XGMII_IPG_20_23;
+					break;
+				default:
+					return (NPI_FAILURE |
+					NPI_MAC_PORT_ATTR_INVALID(portn));
+				}
+			}
+			break;
+		case BMAC_PORT_0:
+		case BMAC_PORT_1:
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					" npi_mac_port_attr" "MAC_PORT_IPG:"
+					"  Invalid Input: portn <%d>",
+					portn));
+		default:
+			return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+		}
+		break;
+	}
+
+	case XMAC_PORT_IPG:
+		{
+		uint32_t	ipg1;
+		switch (portn) {
+		case XMAC_PORT_0:
+		case XMAC_PORT_1:
+			if (op == OP_SET) {
+				ipg1 = attrp->idata[0];
+				if ((ipg1 != MII_GMII_IPG_12) &&
+					(ipg1 != MII_GMII_IPG_13) &&
+					(ipg1 != MII_GMII_IPG_14) &&
+					(ipg1 != MII_GMII_IPG_15) &&
+					(ipg1 != MII_GMII_IPG_16)) {
+					NPI_ERROR_MSG((handle.function,
+						    NPI_ERR_CTL,
+						    " npi_mac_port_attr"
+						    " XMAC_PORT_IPG:"
+						    " Invalid Input:"
+						    " mii_gmii_ipg <0x%x>",
+						    ipg1));
+					return (NPI_FAILURE |
+					NPI_MAC_PORT_ATTR_INVALID(portn));
+				}
+
+				XMAC_REG_RD(handle, portn, XMAC_IPG_REG, &val);
+				val &= ~(XMAC_IPG_VALUE_MASK |
+					XMAC_IPG_VALUE1_MASK);
+
+				switch (ipg1) {
+				case MII_GMII_IPG_12:
+					val |= (IPG1_12_BYTES <<
+						XMAC_IPG_VALUE1_SHIFT);
+					break;
+				case MII_GMII_IPG_13:
+					val |= (IPG1_13_BYTES <<
+						XMAC_IPG_VALUE1_SHIFT);
+					break;
+				case MII_GMII_IPG_14:
+					val |= (IPG1_14_BYTES <<
+						XMAC_IPG_VALUE1_SHIFT);
+					break;
+				case MII_GMII_IPG_15:
+					val |= (IPG1_15_BYTES <<
+						XMAC_IPG_VALUE1_SHIFT);
+					break;
+				case MII_GMII_IPG_16:
+					val |= (IPG1_16_BYTES <<
+						XMAC_IPG_VALUE1_SHIFT);
+					break;
+				default:
+					return (NPI_FAILURE |
+					NPI_MAC_PORT_ATTR_INVALID(portn));
+				}
+				XMAC_REG_WR(handle, portn, XMAC_IPG_REG, val);
+			} else {
+				XMAC_REG_RD(handle, portn, XMAC_IPG_REG, &val);
+				ipg1 = (val & XMAC_IPG_VALUE1_MASK) >>
+					XMAC_IPG_VALUE1_SHIFT;
+				switch (ipg1) {
+				case IPG1_12_BYTES:
+					attrp->odata[1] = MII_GMII_IPG_12;
+					break;
+				case IPG1_13_BYTES:
+					attrp->odata[1] = MII_GMII_IPG_13;
+					break;
+				case IPG1_14_BYTES:
+					attrp->odata[1] = MII_GMII_IPG_14;
+					break;
+				case IPG1_15_BYTES:
+					attrp->odata[1] = MII_GMII_IPG_15;
+					break;
+				case IPG1_16_BYTES:
+					attrp->odata[1] = MII_GMII_IPG_16;
+					break;
+				default:
+					return (NPI_FAILURE |
+					NPI_MAC_PORT_ATTR_INVALID(portn));
+				}
+			}
+			break;
+		case BMAC_PORT_0:
+		case BMAC_PORT_1:
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_mac_port_attr"
+					    " MAC_PORT_IPG:"
+					    " Invalid Input: portn <%d>",
+					    portn));
+		default:
+			return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+		}
+		break;
+	}
+
+	case MAC_PORT_ADDR: {
+		uint32_t addr0;
+		uint32_t addr1;
+		uint32_t addr2;
+
+		switch (portn) {
+		case XMAC_PORT_0:
+		case XMAC_PORT_1:
+			if (op == OP_SET) {
+				addr0 = attrp->idata[0];
+				addr1 = attrp->idata[1];
+				addr2 = attrp->idata[2];
+				if ((addr0 & ~0xFFFF) != 0) {
+					NPI_ERROR_MSG((handle.function,
+						    NPI_ERR_CTL,
+						    " npi_mac_port_attr"
+						    " MAC_PORT_ADDR:"
+						    " Invalid Input:"
+						    " addr0 <0x%x>", addr0));
+
+					return (NPI_FAILURE |
+					NPI_MAC_PORT_ATTR_INVALID(portn));
+				}
+				if ((addr1 & ~0xFFFF) != 0) {
+					NPI_ERROR_MSG((handle.function,
+						    NPI_ERR_CTL,
+						    " npi_mac_port_attr"
+						    " MAC_PORT_ADDR:"
+						    " Invalid Input:"
+						    " addr1 <0x%x>", addr1));
+					return (NPI_FAILURE |
+					NPI_MAC_PORT_ATTR_INVALID(portn));
+				}
+				if ((addr2 & ~0xFFFF) != 0) {
+					NPI_ERROR_MSG((handle.function,
+						    NPI_ERR_CTL,
+						    " npi_mac_port_attr"
+						    " MAC_PORT_ADDR:"
+						    " Invalid Input:"
+						    " addr2 <0x%x.",
+						    addr2));
+
+					return (NPI_FAILURE |
+					NPI_MAC_PORT_ATTR_INVALID(portn));
+				}
+				XMAC_REG_WR(handle, portn, XMAC_ADDR0_REG,
+						addr0);
+				XMAC_REG_WR(handle, portn, XMAC_ADDR1_REG,
+						addr1);
+				XMAC_REG_WR(handle, portn, XMAC_ADDR2_REG,
+						addr2);
+			} else {
+				XMAC_REG_RD(handle, portn, XMAC_ADDR0_REG,
+						&addr0);
+				XMAC_REG_RD(handle, portn, XMAC_ADDR1_REG,
+						&addr1);
+				XMAC_REG_RD(handle, portn, XMAC_ADDR2_REG,
+						&addr2);
+				attrp->odata[0] = addr0 & MAC_ADDR_REG_MASK;
+				attrp->odata[1] = addr1 & MAC_ADDR_REG_MASK;
+				attrp->odata[2] = addr2 & MAC_ADDR_REG_MASK;
+			}
+			break;
+		case BMAC_PORT_0:
+		case BMAC_PORT_1:
+			if (op == OP_SET) {
+				addr0 = attrp->idata[0];
+				addr1 = attrp->idata[1];
+				addr2 = attrp->idata[2];
+				if ((addr0 & ~0xFFFF) != 0) {
+					NPI_ERROR_MSG((handle.function,
+						    NPI_ERR_CTL,
+						    " npi_mac_port_attr"
+						    " MAC_PORT_ADDR:"
+						    " Invalid Input:"
+						    " addr0 <0x%x>",
+						    addr0));
+					return (NPI_FAILURE |
+					NPI_MAC_PORT_ATTR_INVALID(portn));
+				}
+				if ((addr1 & ~0xFFFF) != 0) {
+					NPI_ERROR_MSG((handle.function,
+						    NPI_ERR_CTL,
+						    " npi_mac_port_attr"
+						    " MAC_PORT_ADDR:"
+						    " Invalid Input:"
+						    " addr1 <0x%x>",
+						    addr1));
+					return (NPI_FAILURE |
+					NPI_MAC_PORT_ATTR_INVALID(portn));
+				}
+				if ((addr2 & ~0xFFFF) != 0) {
+					NPI_ERROR_MSG((handle.function,
+						    NPI_ERR_CTL,
+						    " npi_mac_port_attr"
+						    " MAC_PORT_ADDR:"
+						    " Invalid Input:"
+						    " addr2 <0x%x>",
+						    addr2));
+					return (NPI_FAILURE |
+					NPI_MAC_PORT_ATTR_INVALID(portn));
+				}
+				BMAC_REG_WR(handle, portn, BMAC_ADDR0_REG,
+						addr0);
+				BMAC_REG_WR(handle, portn, BMAC_ADDR1_REG,
+						addr1);
+				BMAC_REG_WR(handle, portn, BMAC_ADDR2_REG,
+						addr2);
+			} else {
+				BMAC_REG_RD(handle, portn, BMAC_ADDR0_REG,
+						&addr0);
+				BMAC_REG_RD(handle, portn, BMAC_ADDR1_REG,
+						&addr1);
+				BMAC_REG_RD(handle, portn, BMAC_ADDR2_REG,
+						&addr2);
+				attrp->odata[0] = addr0 & MAC_ADDR_REG_MASK;
+				attrp->odata[1] = addr1 & MAC_ADDR_REG_MASK;
+				attrp->odata[2] = addr2 & MAC_ADDR_REG_MASK;
+			}
+			break;
+		default:
+			return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+		}
+	}	break;
+
+	case MAC_PORT_ADDR_FILTER: {
+		uint32_t addr0;
+		uint32_t addr1;
+		uint32_t addr2;
+
+		switch (portn) {
+		case XMAC_PORT_0:
+		case XMAC_PORT_1:
+			if (op == OP_SET) {
+				addr0 = attrp->idata[0];
+				addr1 = attrp->idata[1];
+				addr2 = attrp->idata[2];
+				if ((addr0 & ~0xFFFF) != 0) {
+					NPI_ERROR_MSG((handle.function,
+						    NPI_ERR_CTL,
+						    " npi_mac_port_attr"
+						    " MAC_PORT_ADDR_FILTER:"
+						    " Invalid Input:"
+						    " addr0 <0x%x>",
+						    addr0));
+					return (NPI_FAILURE |
+					NPI_MAC_PORT_ATTR_INVALID(portn));
+				}
+				if ((addr1 & ~0xFFFF) != 0) {
+					NPI_ERROR_MSG((handle.function,
+						    NPI_ERR_CTL,
+						    " npi_mac_port_attr"
+						    " MAC_PORT_ADDR_FILTER:"
+						    " Invalid Input:"
+						    " addr1 <0x%x>",
+						    addr1));
+					return (NPI_FAILURE |
+					NPI_MAC_PORT_ATTR_INVALID(portn));
+				}
+				if ((addr2 & ~0xFFFF) != 0) {
+					NPI_ERROR_MSG((handle.function,
+						    NPI_ERR_CTL,
+						    " npi_mac_port_attr"
+						    " MAC_PORT_ADDR_FILTER:"
+						    " Invalid Input:"
+						    " addr2 <0x%x>",
+						    addr2));
+					return (NPI_FAILURE |
+					NPI_MAC_PORT_ATTR_INVALID(portn));
+				}
+				XMAC_REG_WR(handle, portn,
+						XMAC_ADDR_FILT0_REG, addr0);
+				XMAC_REG_WR(handle, portn,
+						XMAC_ADDR_FILT1_REG, addr1);
+				XMAC_REG_WR(handle, portn,
+						XMAC_ADDR_FILT2_REG, addr2);
+			} else {
+				XMAC_REG_RD(handle, portn,
+						XMAC_ADDR_FILT0_REG, &addr0);
+				XMAC_REG_RD(handle, portn,
+						XMAC_ADDR_FILT1_REG, &addr1);
+				XMAC_REG_RD(handle, portn,
+						XMAC_ADDR_FILT2_REG, &addr2);
+				attrp->odata[0] = addr0 & MAC_ADDR_REG_MASK;
+				attrp->odata[1] = addr1 & MAC_ADDR_REG_MASK;
+				attrp->odata[2] = addr2 & MAC_ADDR_REG_MASK;
+			}
+			break;
+		case BMAC_PORT_0:
+		case BMAC_PORT_1:
+			if (op == OP_SET) {
+				addr0 = attrp->idata[0];
+				addr1 = attrp->idata[1];
+				addr2 = attrp->idata[2];
+				if ((addr0 & ~0xFFFF) != 0) {
+					NPI_ERROR_MSG((handle.function,
+						    NPI_ERR_CTL,
+						    " npi_mac_port_attr"
+						    " MAC_PORT_ADDR_FILTER:"
+						    " addr0",
+						    addr0));
+					return (NPI_FAILURE |
+					NPI_MAC_PORT_ATTR_INVALID(portn));
+				}
+				if ((addr1 & ~0xFFFF) != 0) {
+					NPI_ERROR_MSG((handle.function,
+						    NPI_ERR_CTL,
+						    " npi_mac_port_attr"
+						    " MAC_PORT_ADDR_FILTER:"
+						    " Invalid Input:"
+						    " addr1 <0x%x>",
+						    addr1));
+					return (NPI_FAILURE |
+					NPI_MAC_PORT_ATTR_INVALID(portn));
+				}
+				if ((addr2 & ~0xFFFF) != 0) {
+					NPI_ERROR_MSG((handle.function,
+						    NPI_ERR_CTL,
+						    " npi_mac_port_attr"
+						    " MAC_PORT_ADDR_FILTER:"
+						    " Invalid Input:"
+						    " addr2 <0x%x>",
+						    addr2));
+					return (NPI_FAILURE |
+					NPI_MAC_PORT_ATTR_INVALID(portn));
+				}
+				BMAC_REG_WR(handle, portn, MAC_ADDR_FILT0_REG,
+						addr0);
+				BMAC_REG_WR(handle, portn, MAC_ADDR_FILT1_REG,
+						addr1);
+				BMAC_REG_WR(handle, portn, MAC_ADDR_FILT2_REG,
+						addr2);
+			} else {
+				BMAC_REG_RD(handle, portn, MAC_ADDR_FILT0_REG,
+						&addr0);
+				BMAC_REG_RD(handle, portn, MAC_ADDR_FILT1_REG,
+						&addr1);
+				BMAC_REG_RD(handle, portn, MAC_ADDR_FILT2_REG,
+						&addr2);
+				attrp->odata[0] = addr0 & MAC_ADDR_REG_MASK;
+				attrp->odata[1] = addr1 & MAC_ADDR_REG_MASK;
+				attrp->odata[2] = addr2 & MAC_ADDR_REG_MASK;
+			}
+			break;
+		default:
+			return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+		}
+	}	break;
+
+	case MAC_PORT_ADDR_FILTER_MASK: {
+		uint32_t mask_1_2;
+		uint32_t mask_0;
+
+		switch (portn) {
+		case XMAC_PORT_0:
+		case XMAC_PORT_1:
+			if (op == OP_SET) {
+				mask_0 = attrp->idata[0];
+				mask_1_2 = attrp->idata[1];
+				if ((mask_0 & ~0xFFFF) != 0) {
+					NPI_ERROR_MSG((handle.function,
+						    NPI_ERR_CTL,
+						    " npi_mac_port_attr"
+						    " MAC_ADDR_FILTER_MASK:"
+						    " Invalid Input:"
+						    " mask_0 <0x%x>",
+						    mask_0));
+					return (NPI_FAILURE |
+					NPI_MAC_PORT_ATTR_INVALID(portn));
+				}
+				if ((mask_1_2 & ~0xFF) != 0) {
+					NPI_ERROR_MSG((handle.function,
+						    NPI_ERR_CTL,
+						    " npi_mac_port_attr"
+						    " MAC_ADDR_FILTER_MASK:"
+						    " Invalid Input:"
+						    " mask_1_2 <0x%x>",
+						    mask_1_2));
+					return (NPI_FAILURE |
+					NPI_MAC_PORT_ATTR_INVALID(portn));
+				}
+				XMAC_REG_WR(handle, portn,
+					XMAC_ADDR_FILT0_MASK_REG, mask_0);
+				XMAC_REG_WR(handle, portn,
+					XMAC_ADDR_FILT12_MASK_REG, mask_1_2);
+			} else {
+				XMAC_REG_RD(handle, portn,
+					XMAC_ADDR_FILT0_MASK_REG, &mask_0);
+				XMAC_REG_RD(handle, portn,
+					XMAC_ADDR_FILT12_MASK_REG, &mask_1_2);
+				attrp->odata[0] = mask_0 & 0xFFFF;
+				attrp->odata[1] = mask_1_2 & 0xFF;
+			}
+			break;
+		case BMAC_PORT_0:
+		case BMAC_PORT_1:
+			if (op == OP_SET) {
+				mask_0 = attrp->idata[0];
+				mask_1_2 = attrp->idata[1];
+				BMAC_REG_WR(handle, portn,
+					MAC_ADDR_FILT00_MASK_REG, mask_0);
+				BMAC_REG_WR(handle, portn,
+					MAC_ADDR_FILT12_MASK_REG, mask_1_2);
+			} else {
+				BMAC_REG_RD(handle, portn,
+					MAC_ADDR_FILT00_MASK_REG, &mask_0);
+				BMAC_REG_RD(handle, portn,
+					MAC_ADDR_FILT12_MASK_REG, &mask_1_2);
+				attrp->odata[0] = mask_0;
+				attrp->odata[1] = mask_1_2;
+			}
+			break;
+		default:
+			return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+		}
+	}	break;
+
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_mac_port_attr"
+				    " Invalid Input:"
+				    " attr <0x%x>", attrp->type));
+		return (NPI_FAILURE | NPI_MAC_PORT_ATTR_INVALID(portn));
+	}
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_xmac_reset(npi_handle_t handle, uint8_t portn, npi_mac_reset_t mode)
+{
+	uint64_t val;
+	boolean_t txmac = B_FALSE;
+
+	if (!IS_XMAC_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_mac_reset"
+				    " Invalid Input: portn <%d>", portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	switch (mode) {
+	case XTX_MAC_REG_RESET:
+		XMAC_REG_WR(handle, portn, XTXMAC_SW_RST_REG, XTXMAC_REG_RST);
+		XMAC_WAIT_REG(handle, portn, XTXMAC_SW_RST_REG, val);
+		txmac = B_TRUE;
+		break;
+	case XRX_MAC_REG_RESET:
+		XMAC_REG_WR(handle, portn, XRXMAC_SW_RST_REG, XRXMAC_REG_RST);
+		XMAC_WAIT_REG(handle, portn, XRXMAC_SW_RST_REG, val);
+		break;
+	case XTX_MAC_LOGIC_RESET:
+		XMAC_REG_WR(handle, portn, XTXMAC_SW_RST_REG, XTXMAC_SOFT_RST);
+		XMAC_WAIT_REG(handle, portn, XTXMAC_SW_RST_REG, val);
+		txmac = B_TRUE;
+		break;
+	case XRX_MAC_LOGIC_RESET:
+		XMAC_REG_WR(handle, portn, XRXMAC_SW_RST_REG, XRXMAC_SOFT_RST);
+		XMAC_WAIT_REG(handle, portn, XRXMAC_SW_RST_REG, val);
+		break;
+	case XTX_MAC_RESET_ALL:
+		XMAC_REG_WR(handle, portn, XTXMAC_SW_RST_REG,
+					XTXMAC_SOFT_RST | XTXMAC_REG_RST);
+		XMAC_WAIT_REG(handle, portn, XTXMAC_SW_RST_REG, val);
+		txmac = B_TRUE;
+		break;
+	case XRX_MAC_RESET_ALL:
+		XMAC_REG_WR(handle, portn, XRXMAC_SW_RST_REG,
+					XRXMAC_SOFT_RST | XRXMAC_REG_RST);
+		XMAC_WAIT_REG(handle, portn, XRXMAC_SW_RST_REG, val);
+		break;
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_mac_reset"
+				    " Invalid Input: mode <0x%x>",
+				    mode));
+		return (NPI_FAILURE | NPI_MAC_RESET_MODE_INVALID(portn));
+	}
+
+	if (val != 0) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_xmac_reset"
+				    " HW ERROR: MAC_RESET  failed <0x%x>",
+				    val));
+
+		if (txmac == B_TRUE)
+			return (NPI_FAILURE | NPI_TXMAC_RESET_FAILED(portn));
+		else
+			return (NPI_FAILURE | NPI_RXMAC_RESET_FAILED(portn));
+	}
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_xmac_xif_config(npi_handle_t handle, config_op_t op, uint8_t portn,
+			xmac_xif_config_t config)
+{
+	uint64_t val = 0;
+
+	if (!IS_XMAC_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_xmac_xif_config"
+				    " Invalid Input: portn <%d>",
+				    portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	switch (op) {
+	case ENABLE:
+	case DISABLE:
+		if ((config == 0) || (config & ~CFG_XMAC_XIF_ALL) != 0) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_xmac_xif_config"
+					    " Invalid Input:"
+					    " config <0x%x>", config));
+			return (NPI_FAILURE | NPI_MAC_OPCODE_INVALID(portn));
+		}
+		if (op == ENABLE) {
+			XMAC_REG_RD(handle, portn, XMAC_CONFIG_REG, &val);
+			if (config & CFG_XMAC_XIF_LED_FORCE)
+				val |= XMAC_XIF_FORCE_LED_ON;
+			if (config & CFG_XMAC_XIF_LED_POLARITY)
+				val |= XMAC_XIF_LED_POLARITY;
+			if (config & CFG_XMAC_XIF_SEL_POR_CLK_SRC)
+				val |= XMAC_XIF_SEL_POR_CLK_SRC;
+			if (config & CFG_XMAC_XIF_TX_OUTPUT)
+				val |= XMAC_XIF_TX_OUTPUT_EN;
+
+			if (config & CFG_XMAC_XIF_LOOPBACK) {
+				val &= ~XMAC_XIF_SEL_POR_CLK_SRC;
+				val |= XMAC_XIF_LOOPBACK;
+			}
+
+			if (config & CFG_XMAC_XIF_LFS)
+				val &= ~XMAC_XIF_LFS_DISABLE;
+			if (config & CFG_XMAC_XIF_XPCS_BYPASS)
+				val |= XMAC_XIF_XPCS_BYPASS;
+			if (config & CFG_XMAC_XIF_1G_PCS_BYPASS)
+				val |= XMAC_XIF_1G_PCS_BYPASS;
+			if (config & CFG_XMAC_XIF_SEL_CLK_25MHZ)
+				val |= XMAC_XIF_SEL_CLK_25MHZ;
+			XMAC_REG_WR(handle, portn, XMAC_CONFIG_REG, val);
+
+		} else {
+			XMAC_REG_RD(handle, portn, XMAC_CONFIG_REG, &val);
+			if (config & CFG_XMAC_XIF_LED_FORCE)
+				val &= ~XMAC_XIF_FORCE_LED_ON;
+			if (config & CFG_XMAC_XIF_LED_POLARITY)
+				val &= ~XMAC_XIF_LED_POLARITY;
+			if (config & CFG_XMAC_XIF_SEL_POR_CLK_SRC)
+				val &= ~XMAC_XIF_SEL_POR_CLK_SRC;
+			if (config & CFG_XMAC_XIF_TX_OUTPUT)
+				val &= ~XMAC_XIF_TX_OUTPUT_EN;
+			if (config & CFG_XMAC_XIF_LOOPBACK)
+				val &= ~XMAC_XIF_LOOPBACK;
+			if (config & CFG_XMAC_XIF_LFS)
+				val |= XMAC_XIF_LFS_DISABLE;
+			if (config & CFG_XMAC_XIF_XPCS_BYPASS)
+				val &= ~XMAC_XIF_XPCS_BYPASS;
+			if (config & CFG_XMAC_XIF_1G_PCS_BYPASS)
+				val &= ~XMAC_XIF_1G_PCS_BYPASS;
+			if (config & CFG_XMAC_XIF_SEL_CLK_25MHZ)
+				val &= ~XMAC_XIF_SEL_CLK_25MHZ;
+			XMAC_REG_WR(handle, portn, XMAC_CONFIG_REG, val);
+		}
+		break;
+	case INIT:
+		if ((config & ~CFG_XMAC_XIF_ALL) != 0) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_xmac_xif_config"
+					    " Invalid Input: config <0x%x>",
+					    config));
+			return (NPI_FAILURE | NPI_MAC_CONFIG_INVALID(portn));
+		}
+		XMAC_REG_RD(handle, portn, XMAC_CONFIG_REG, &val);
+
+		if (config & CFG_XMAC_XIF_LED_FORCE)
+			val |= XMAC_XIF_FORCE_LED_ON;
+		else
+			val &= ~XMAC_XIF_FORCE_LED_ON;
+		if (config & CFG_XMAC_XIF_LED_POLARITY)
+			val |= XMAC_XIF_LED_POLARITY;
+		else
+			val &= ~XMAC_XIF_LED_POLARITY;
+		if (config & CFG_XMAC_XIF_SEL_POR_CLK_SRC)
+			val |= XMAC_XIF_SEL_POR_CLK_SRC;
+		else
+			val &= ~XMAC_XIF_SEL_POR_CLK_SRC;
+		if (config & CFG_XMAC_XIF_TX_OUTPUT)
+			val |= XMAC_XIF_TX_OUTPUT_EN;
+		else
+			val &= ~XMAC_XIF_TX_OUTPUT_EN;
+
+		if (config & CFG_XMAC_XIF_LOOPBACK) {
+			val &= ~XMAC_XIF_SEL_POR_CLK_SRC;
+			val |= XMAC_XIF_LOOPBACK;
+
+		} else {
+			val &= ~XMAC_XIF_LOOPBACK;
+		}
+
+		if (config & CFG_XMAC_XIF_LFS)
+			val &= ~XMAC_XIF_LFS_DISABLE;
+		else
+			val |= XMAC_XIF_LFS_DISABLE;
+		if (config & CFG_XMAC_XIF_XPCS_BYPASS)
+			val |= XMAC_XIF_XPCS_BYPASS;
+		else
+			val &= ~XMAC_XIF_XPCS_BYPASS;
+		if (config & CFG_XMAC_XIF_1G_PCS_BYPASS)
+			val |= XMAC_XIF_1G_PCS_BYPASS;
+		else
+			val &= ~XMAC_XIF_1G_PCS_BYPASS;
+		if (config & CFG_XMAC_XIF_SEL_CLK_25MHZ)
+			val |= XMAC_XIF_SEL_CLK_25MHZ;
+		else
+			val &= ~XMAC_XIF_SEL_CLK_25MHZ;
+		XMAC_REG_WR(handle, portn, XMAC_CONFIG_REG, val);
+
+		break;
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_xmac_xif_config"
+				    " Invalid Input: op <0x%x>", op));
+		return (NPI_FAILURE | NPI_MAC_OPCODE_INVALID(portn));
+	}
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_xmac_tx_config(npi_handle_t handle, config_op_t op, uint8_t portn,
+			xmac_tx_config_t config)
+{
+	uint64_t val = 0;
+
+	if (!IS_XMAC_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_xmac_tx_config"
+				    " Invalid Input: portn <%d>",
+				    portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	switch (op) {
+	case ENABLE:
+	case DISABLE:
+		if ((config == 0) || (config & ~CFG_XMAC_TX_ALL) != 0) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_xmac_tx_config"
+				    " Invalid Input: config <0x%x>",
+				    config));
+			return (NPI_FAILURE | NPI_MAC_OPCODE_INVALID(portn));
+		}
+		if (op == ENABLE) {
+			XMAC_REG_RD(handle, portn, XMAC_CONFIG_REG, &val);
+			if (config & CFG_XMAC_TX)
+				val |= XMAC_TX_CFG_TX_ENABLE;
+			if (config & CFG_XMAC_TX_STRETCH_MODE)
+				val |= XMAC_TX_CFG_STRETCH_MD;
+			if (config & CFG_XMAC_VAR_IPG)
+				val |= XMAC_TX_CFG_VAR_MIN_IPG_EN;
+			if (config & CFG_XMAC_TX_CRC)
+				val &= ~XMAC_TX_CFG_ALWAYS_NO_CRC;
+			XMAC_REG_WR(handle, portn, XMAC_CONFIG_REG, val);
+		} else {
+			XMAC_REG_RD(handle, portn, XMAC_CONFIG_REG, &val);
+			if (config & CFG_XMAC_TX)
+				val &= ~XMAC_TX_CFG_TX_ENABLE;
+			if (config & CFG_XMAC_TX_STRETCH_MODE)
+				val &= ~XMAC_TX_CFG_STRETCH_MD;
+			if (config & CFG_XMAC_VAR_IPG)
+				val &= ~XMAC_TX_CFG_VAR_MIN_IPG_EN;
+			if (config & CFG_XMAC_TX_CRC)
+				val |= XMAC_TX_CFG_ALWAYS_NO_CRC;
+			XMAC_REG_WR(handle, portn, XMAC_CONFIG_REG, val);
+		}
+		break;
+	case INIT:
+		if ((config & ~CFG_XMAC_TX_ALL) != 0) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_xmac_tx_config"
+					    " Invalid Input: config <0x%x>",
+					    config));
+			return (NPI_FAILURE | NPI_MAC_CONFIG_INVALID(portn));
+		}
+		XMAC_REG_RD(handle, portn, XMAC_CONFIG_REG, &val);
+		if (config & CFG_XMAC_TX)
+			val |= XMAC_TX_CFG_TX_ENABLE;
+		else
+			val &= ~XMAC_TX_CFG_TX_ENABLE;
+		if (config & CFG_XMAC_TX_STRETCH_MODE)
+			val |= XMAC_TX_CFG_STRETCH_MD;
+		else
+			val &= ~XMAC_TX_CFG_STRETCH_MD;
+		if (config & CFG_XMAC_VAR_IPG)
+			val |= XMAC_TX_CFG_VAR_MIN_IPG_EN;
+		else
+			val &= ~XMAC_TX_CFG_VAR_MIN_IPG_EN;
+		if (config & CFG_XMAC_TX_CRC)
+			val &= ~XMAC_TX_CFG_ALWAYS_NO_CRC;
+		else
+			val |= XMAC_TX_CFG_ALWAYS_NO_CRC;
+
+		XMAC_REG_WR(handle, portn, XMAC_CONFIG_REG, val);
+		break;
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_xmac_tx_config"
+				    " Invalid Input: op <0x%x>",
+				    op));
+		return (NPI_FAILURE | NPI_MAC_OPCODE_INVALID(portn));
+	}
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_xmac_rx_config(npi_handle_t handle, config_op_t op, uint8_t portn,
+			xmac_rx_config_t config)
+{
+	uint64_t val = 0;
+
+	if (!IS_XMAC_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_xmac_rx_config"
+				    " Invalid Input: portn <%d>",
+				    portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	switch (op) {
+	case ENABLE:
+	case DISABLE:
+		if ((config == 0) || (config & ~CFG_XMAC_RX_ALL) != 0) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_xmac_rx_config"
+					    " Invalid Input: config <0x%x>",
+					    config));
+			return (NPI_FAILURE | NPI_MAC_CONFIG_INVALID(portn));
+		}
+		if (op == ENABLE) {
+			XMAC_REG_RD(handle, portn, XMAC_CONFIG_REG, &val);
+			if (config & CFG_XMAC_RX)
+				val |= XMAC_RX_CFG_RX_ENABLE;
+			if (config & CFG_XMAC_RX_PROMISCUOUS)
+				val |= XMAC_RX_CFG_PROMISC;
+			if (config & CFG_XMAC_RX_PROMISCUOUSGROUP)
+				val |= XMAC_RX_CFG_PROMISC_GROUP;
+			if (config & CFG_XMAC_RX_ERRCHK)
+				val &= ~XMAC_RX_CFG_ERR_CHK_DISABLE;
+			if (config & CFG_XMAC_RX_CRC_CHK)
+				val &= ~XMAC_RX_CFG_CRC_CHK_DISABLE;
+			if (config & CFG_XMAC_RX_RESV_MULTICAST)
+				val |= XMAC_RX_CFG_RESERVED_MCAST;
+			if (config & CFG_XMAC_RX_CODE_VIO_CHK)
+				val &= ~XMAC_RX_CFG_CD_VIO_CHK;
+			if (config & CFG_XMAC_RX_HASH_FILTER)
+				val |= XMAC_RX_CFG_HASH_FILTER_EN;
+			if (config & CFG_XMAC_RX_ADDR_FILTER)
+				val |= XMAC_RX_CFG_ADDR_FILTER_EN;
+			if (config & CFG_XMAC_RX_STRIP_CRC)
+				val |= XMAC_RX_CFG_STRIP_CRC;
+			if (config & CFG_XMAC_RX_PAUSE)
+				val |= XMAC_RX_CFG_RX_PAUSE_EN;
+			if (config & CFG_XMAC_RX_PASS_FC_FRAME)
+				val |= XMAC_RX_CFG_PASS_FLOW_CTRL;
+			XMAC_REG_WR(handle, portn, XMAC_CONFIG_REG, val);
+		} else {
+			XMAC_REG_RD(handle, portn, XMAC_CONFIG_REG, &val);
+			if (config & CFG_XMAC_RX)
+				val &= ~XMAC_RX_CFG_RX_ENABLE;
+			if (config & CFG_XMAC_RX_PROMISCUOUS)
+				val &= ~XMAC_RX_CFG_PROMISC;
+			if (config & CFG_XMAC_RX_PROMISCUOUSGROUP)
+				val &= ~XMAC_RX_CFG_PROMISC_GROUP;
+			if (config & CFG_XMAC_RX_ERRCHK)
+				val |= XMAC_RX_CFG_ERR_CHK_DISABLE;
+			if (config & CFG_XMAC_RX_CRC_CHK)
+				val |= XMAC_RX_CFG_CRC_CHK_DISABLE;
+			if (config & CFG_XMAC_RX_RESV_MULTICAST)
+				val &= ~XMAC_RX_CFG_RESERVED_MCAST;
+			if (config & CFG_XMAC_RX_CODE_VIO_CHK)
+				val |= XMAC_RX_CFG_CD_VIO_CHK;
+			if (config & CFG_XMAC_RX_HASH_FILTER)
+				val &= ~XMAC_RX_CFG_HASH_FILTER_EN;
+			if (config & CFG_XMAC_RX_ADDR_FILTER)
+				val &= ~XMAC_RX_CFG_ADDR_FILTER_EN;
+			if (config & CFG_XMAC_RX_STRIP_CRC)
+				val &= ~XMAC_RX_CFG_STRIP_CRC;
+			if (config & CFG_XMAC_RX_PAUSE)
+				val &= ~XMAC_RX_CFG_RX_PAUSE_EN;
+			if (config & CFG_XMAC_RX_PASS_FC_FRAME)
+				val &= ~XMAC_RX_CFG_PASS_FLOW_CTRL;
+			XMAC_REG_WR(handle, portn, XMAC_CONFIG_REG, val);
+		}
+		break;
+	case INIT:
+		if ((config & ~CFG_XMAC_RX_ALL) != 0) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_xmac_rx_config"
+					    " Invalid Input: config <0x%x>",
+					    config));
+			return (NPI_FAILURE | NPI_MAC_CONFIG_INVALID(portn));
+		}
+		XMAC_REG_RD(handle, portn, XMAC_CONFIG_REG, &val);
+		if (config & CFG_XMAC_RX)
+			val |= XMAC_RX_CFG_RX_ENABLE;
+		else
+			val &= ~XMAC_RX_CFG_RX_ENABLE;
+		if (config & CFG_XMAC_RX_PROMISCUOUS)
+			val |= XMAC_RX_CFG_PROMISC;
+		else
+			val &= ~XMAC_RX_CFG_PROMISC;
+		if (config & CFG_XMAC_RX_PROMISCUOUSGROUP)
+			val |= XMAC_RX_CFG_PROMISC_GROUP;
+		else
+			val &= ~XMAC_RX_CFG_PROMISC_GROUP;
+		if (config & CFG_XMAC_RX_ERRCHK)
+			val &= ~XMAC_RX_CFG_ERR_CHK_DISABLE;
+		else
+			val |= XMAC_RX_CFG_ERR_CHK_DISABLE;
+		if (config & CFG_XMAC_RX_CRC_CHK)
+			val &= ~XMAC_RX_CFG_CRC_CHK_DISABLE;
+		else
+			val |= XMAC_RX_CFG_CRC_CHK_DISABLE;
+		if (config & CFG_XMAC_RX_RESV_MULTICAST)
+			val |= XMAC_RX_CFG_RESERVED_MCAST;
+		else
+			val &= ~XMAC_RX_CFG_RESERVED_MCAST;
+		if (config & CFG_XMAC_RX_CODE_VIO_CHK)
+			val &= ~XMAC_RX_CFG_CD_VIO_CHK;
+		else
+			val |= XMAC_RX_CFG_CD_VIO_CHK;
+		if (config & CFG_XMAC_RX_HASH_FILTER)
+			val |= XMAC_RX_CFG_HASH_FILTER_EN;
+		else
+			val &= ~XMAC_RX_CFG_HASH_FILTER_EN;
+		if (config & CFG_XMAC_RX_ADDR_FILTER)
+			val |= XMAC_RX_CFG_ADDR_FILTER_EN;
+		else
+			val &= ~XMAC_RX_CFG_ADDR_FILTER_EN;
+		if (config & CFG_XMAC_RX_PAUSE)
+			val |= XMAC_RX_CFG_RX_PAUSE_EN;
+		else
+			val &= ~XMAC_RX_CFG_RX_PAUSE_EN;
+		if (config & CFG_XMAC_RX_STRIP_CRC)
+			val |= XMAC_RX_CFG_STRIP_CRC;
+		else
+			val &= ~XMAC_RX_CFG_STRIP_CRC;
+		if (config & CFG_XMAC_RX_PASS_FC_FRAME)
+			val |= XMAC_RX_CFG_PASS_FLOW_CTRL;
+		else
+			val &= ~XMAC_RX_CFG_PASS_FLOW_CTRL;
+
+		XMAC_REG_WR(handle, portn, XMAC_CONFIG_REG, val);
+		break;
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_xmac_rx_config"
+					    " Invalid Input: op <0x%x>", op));
+		return (NPI_FAILURE | NPI_MAC_OPCODE_INVALID(portn));
+	}
+
+	return (NPI_SUCCESS);
+}
+
+
+npi_status_t
+npi_mac_address_filter_enable(npi_handle_t handle,
+							  uint8_t portn, boolean_t enable)
+{
+	uint64_t val = 0;
+
+	if ((portn == XMAC_PORT_0) || (portn == XMAC_PORT_1)) {
+		XMAC_REG_RD(handle, portn, XMAC_CONFIG_REG, &val);
+		if (enable == B_TRUE) {
+			val |= XMAC_RX_CFG_ADDR_FILTER_EN;
+		} else {
+			val &= ~XMAC_RX_CFG_ADDR_FILTER_EN;
+		}
+		XMAC_REG_WR(handle, portn, XMAC_CONFIG_REG, val);
+	} else {
+		BMAC_REG_RD(handle, portn, RXMAC_CONFIG_REG, &val);
+		if (enable == B_TRUE) {
+			val |= MAC_RX_CFG_ADDR_FILTER_EN;
+		} else {
+			val &= ~MAC_RX_CFG_ADDR_FILTER_EN;
+		}
+		BMAC_REG_WR(handle, portn, RXMAC_CONFIG_REG, val);
+	}
+
+	return (NPI_SUCCESS);
+}
+
+
+npi_status_t
+npi_xmac_tx_iconfig(npi_handle_t handle, config_op_t op, uint8_t portn,
+		    xmac_tx_iconfig_t iconfig)
+{
+	uint64_t val = 0;
+
+	if (!IS_XMAC_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_xmac_tx_iconfig"
+				    " Invalid Input: portn <%d>",
+				    portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	switch (op) {
+	case ENABLE:
+	case DISABLE:
+
+		if ((iconfig == 0) || (iconfig & ~ICFG_XMAC_TX_ALL) != 0) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_xmac_tx_iconfig"
+				    " Invalid Input: iconfig <0x%x>",
+				    iconfig));
+			return (NPI_FAILURE | NPI_MAC_CONFIG_INVALID(portn));
+		}
+		XMAC_REG_RD(handle, portn, XTXMAC_STAT_MSK_REG, &val);
+		if (op == ENABLE)
+			val &= ~iconfig;
+		else
+			val |= iconfig;
+		XMAC_REG_WR(handle, portn, XTXMAC_STAT_MSK_REG, val);
+
+		break;
+	case INIT:
+
+		if ((iconfig & ~ICFG_XMAC_TX_ALL) != 0) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_xmac_tx_iconfig"
+				    " Invalid Input: iconfig <0x%x>",
+				    iconfig));
+			return (NPI_FAILURE | NPI_MAC_CONFIG_INVALID(portn));
+		}
+		XMAC_REG_WR(handle, portn, XTXMAC_STAT_MSK_REG, ~iconfig);
+
+		break;
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_xmac_tx_iconfig"
+				    " Invalid Input: iconfig <0x%x>",
+				    iconfig));
+		return (NPI_FAILURE | NPI_MAC_OPCODE_INVALID(portn));
+	}
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_xmac_rx_iconfig(npi_handle_t handle, config_op_t op, uint8_t portn,
+		    xmac_rx_iconfig_t iconfig)
+{
+	uint64_t val = 0;
+
+	if (!IS_XMAC_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_xmac_rx_iconfig"
+				    " Invalid Input: portn <%d>",
+				    portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	switch (op) {
+	case ENABLE:
+	case DISABLE:
+
+		if ((iconfig == 0) || (iconfig & ~ICFG_XMAC_RX_ALL) != 0) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_xmac_rx_iconfig"
+					    " Invalid Input: iconfig <0x%x>",
+					    iconfig));
+			return (NPI_FAILURE | NPI_MAC_CONFIG_INVALID(portn));
+		}
+		XMAC_REG_RD(handle, portn, XRXMAC_STAT_MSK_REG, &val);
+		if (op == ENABLE)
+			val &= ~iconfig;
+		else
+			val |= iconfig;
+		XMAC_REG_WR(handle, portn, XRXMAC_STAT_MSK_REG, val);
+
+		break;
+	case INIT:
+
+		if ((iconfig & ~ICFG_XMAC_RX_ALL) != 0) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_xmac_rx_iconfig"
+					    " Invalid Input: iconfig <0x%x>",
+					    iconfig));
+			return (NPI_FAILURE | NPI_MAC_CONFIG_INVALID(portn));
+		}
+		XMAC_REG_WR(handle, portn, XRXMAC_STAT_MSK_REG, ~iconfig);
+
+		break;
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_xmac_rx_iconfig"
+				    " Invalid Input: iconfig <0x%x>",
+				    iconfig));
+		return (NPI_FAILURE | NPI_MAC_OPCODE_INVALID(portn));
+	}
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_xmac_ctl_iconfig(npi_handle_t handle, config_op_t op, uint8_t portn,
+			xmac_ctl_iconfig_t iconfig)
+{
+	uint64_t val = 0;
+
+	if (!IS_XMAC_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_xmac_ctl_iconfig"
+				    " Invalid Input: portn <%d>",
+				    portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	switch (op) {
+	case ENABLE:
+	case DISABLE:
+
+		if ((iconfig == 0) || (iconfig & ~ICFG_XMAC_CTRL_ALL) != 0) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_xmac_ctl_iconfig"
+					    " Invalid Input: iconfig <0x%x>",
+					    iconfig));
+			return (NPI_FAILURE | NPI_MAC_CONFIG_INVALID(portn));
+		}
+		XMAC_REG_RD(handle, portn, XMAC_C_S_MSK_REG, &val);
+		if (op == ENABLE)
+			val &= ~iconfig;
+		else
+			val |= iconfig;
+		XMAC_REG_WR(handle, portn, XMAC_C_S_MSK_REG, val);
+
+		break;
+	case INIT:
+
+		if ((iconfig & ~ICFG_XMAC_CTRL_ALL) != 0) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_xmac_ctl_iconfig"
+					    " Invalid Input: iconfig <0x%x>",
+					    iconfig));
+			return (NPI_FAILURE | NPI_MAC_CONFIG_INVALID(portn));
+		}
+		XMAC_REG_WR(handle, portn, XMAC_C_S_MSK_REG, ~iconfig);
+
+		break;
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_xmac_ctl_iconfig"
+					    " Invalid Input: iconfig <0x%x>",
+					    iconfig));
+		return (NPI_FAILURE | NPI_MAC_OPCODE_INVALID(portn));
+	}
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_xmac_tx_get_istatus(npi_handle_t handle, uint8_t portn,
+			xmac_tx_iconfig_t *istatus)
+{
+	uint64_t val;
+
+	if (!IS_XMAC_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_xmac_tx_get_istatus"
+				    " Invalid Input: portn <%d>",
+				    portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	XMAC_REG_RD(handle, portn, XTXMAC_STATUS_REG, &val);
+	*istatus = (uint32_t)val;
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_xmac_rx_get_istatus(npi_handle_t handle, uint8_t portn,
+			xmac_rx_iconfig_t *istatus)
+{
+	uint64_t val;
+
+	if (!IS_XMAC_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_xmac_rx_get_istatus"
+				    " Invalid Input: portn <%d>",
+				    portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	XMAC_REG_RD(handle, portn, XRXMAC_STATUS_REG, &val);
+	*istatus = (uint32_t)val;
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_xmac_ctl_get_istatus(npi_handle_t handle, uint8_t portn,
+			xmac_ctl_iconfig_t *istatus)
+{
+	uint64_t val;
+
+	if (!IS_XMAC_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_xmac_ctl_get_istatus"
+				    " Invalid Input: portn <%d>",
+				    portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	XMAC_REG_RD(handle, portn, XMAC_CTRL_STAT_REG, &val);
+	*istatus = (uint32_t)val;
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_xmac_xpcs_reset(npi_handle_t handle, uint8_t portn)
+{
+	uint64_t val;
+	int delay = 100;
+	if (!IS_XMAC_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_xmac_xpcs_reset"
+				    " Invalid Input: portn <%d>",
+				    portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	XPCS_REG_RD(handle, portn, XPCS_CTRL_1_REG, &val);
+	val |= XPCS_CTRL1_RST;
+	XPCS_REG_WR(handle, portn, XPCS_CTRL_1_REG, val);
+
+	while ((--delay) && (val & XPCS_CTRL1_RST)) {
+		NXGE_DELAY(10);
+		XPCS_REG_RD(handle, portn, XPCS_CTRL_1_REG, &val);
+	}
+
+	if (delay == 0) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			    " npi_xmac_xpcs_reset"
+			    " portn <%d> failed",
+			    portn));
+		return (NPI_FAILURE);
+	}
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_xmac_xpcs_enable(npi_handle_t handle, uint8_t portn)
+{
+	uint64_t val;
+
+	if (!IS_XMAC_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_xmac_xpcs_enable"
+				    " Invalid Input: portn <%d>",
+				    portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	XPCS_REG_RD(handle, portn, XPCS_CFG_VENDOR_1_REG, &val);
+	val |= XPCS_CFG_XPCS_ENABLE;
+	XPCS_REG_WR(handle, portn, XPCS_CFG_VENDOR_1_REG, val);
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_xmac_xpcs_disable(npi_handle_t handle, uint8_t portn)
+{
+	uint64_t val;
+
+	if (!IS_XMAC_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_xmac_xpcs_disable"
+				    " Invalid Input: portn <%d>",
+				    portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	XPCS_REG_RD(handle, portn, XPCS_CFG_VENDOR_1_REG, &val);
+	val &= ~XPCS_CFG_XPCS_ENABLE;
+	XPCS_REG_WR(handle, portn, XPCS_CFG_VENDOR_1_REG, val);
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_xmac_xpcs_read(npi_handle_t handle, uint8_t portn, uint8_t xpcs_reg,
+			uint32_t *value)
+{
+	uint32_t reg;
+	uint64_t val;
+
+	if (!IS_XMAC_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_xmac_xpcs_read"
+				    " Invalid Input: portn <%d>",
+				    portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	switch (xpcs_reg) {
+	case XPCS_REG_CONTROL1:
+		reg = XPCS_CTRL_1_REG;
+		break;
+	case XPCS_REG_STATUS1:
+		reg = XPCS_STATUS_1_REG;
+		break;
+	case XPCS_REG_DEVICE_ID:
+		reg = XPCS_DEV_ID_REG;
+		break;
+	case XPCS_REG_SPEED_ABILITY:
+		reg = XPCS_SPEED_ABILITY_REG;
+		break;
+	case XPCS_REG_DEVICE_IN_PKG:
+		reg = XPCS_DEV_IN_PKG_REG;
+		break;
+	case XPCS_REG_CONTROL2:
+		reg = XPCS_CTRL_2_REG;
+		break;
+	case XPCS_REG_STATUS2:
+		reg = XPCS_STATUS_2_REG;
+		break;
+	case XPCS_REG_PKG_ID:
+		reg = XPCS_PKG_ID_REG;
+		break;
+	case XPCS_REG_STATUS:
+		reg = XPCS_STATUS_REG;
+		break;
+	case XPCS_REG_TEST_CONTROL:
+		reg = XPCS_TEST_CTRL_REG;
+		break;
+	case XPCS_REG_CONFIG_VENDOR1:
+		reg = XPCS_CFG_VENDOR_1_REG;
+		break;
+	case XPCS_REG_DIAG_VENDOR2:
+		reg = XPCS_DIAG_VENDOR_2_REG;
+		break;
+	case XPCS_REG_MASK1:
+		reg = XPCS_MASK_1_REG;
+		break;
+	case XPCS_REG_PACKET_COUNTER:
+		reg = XPCS_PKT_CNTR_REG;
+		break;
+	case XPCS_REG_TX_STATEMACHINE:
+		reg = XPCS_TX_STATE_MC_REG;
+		break;
+	case XPCS_REG_DESCWERR_COUNTER:
+		reg = XPCS_DESKEW_ERR_CNTR_REG;
+		break;
+	case XPCS_REG_SYMBOL_ERR_L0_1_COUNTER:
+		reg = XPCS_SYM_ERR_CNTR_L0_L1_REG;
+		break;
+	case XPCS_REG_SYMBOL_ERR_L2_3_COUNTER:
+		reg = XPCS_SYM_ERR_CNTR_L2_L3_REG;
+		break;
+	case XPCS_REG_TRAINING_VECTOR:
+		reg = XPCS_TRAINING_VECTOR_REG;
+		break;
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_xmac_xpcs_read"
+				    " Invalid Input: xpcs_reg <0x%x>",
+				    xpcs_reg));
+		return (NPI_FAILURE | NPI_MAC_REG_INVALID(portn));
+	}
+	XPCS_REG_RD(handle, portn, reg, &val);
+	*value = val & 0xFFFFFFFF;
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_xmac_xpcs_write(npi_handle_t handle, uint8_t portn, uint8_t xpcs_reg,
+			uint32_t value)
+{
+	uint32_t reg;
+	uint64_t val;
+
+	if (!IS_XMAC_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_xmac_xpcs_write"
+				    " Invalid Input: portn <%d>",
+				    portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	switch (xpcs_reg) {
+	case XPCS_REG_CONTROL1:
+		reg = XPCS_CTRL_1_REG;
+		break;
+	case XPCS_REG_TEST_CONTROL:
+		reg = XPCS_TEST_CTRL_REG;
+		break;
+	case XPCS_REG_CONFIG_VENDOR1:
+		reg = XPCS_CFG_VENDOR_1_REG;
+		break;
+	case XPCS_REG_DIAG_VENDOR2:
+		reg = XPCS_DIAG_VENDOR_2_REG;
+		break;
+	case XPCS_REG_MASK1:
+		reg = XPCS_MASK_1_REG;
+		break;
+	case XPCS_REG_PACKET_COUNTER:
+		reg = XPCS_PKT_CNTR_REG;
+		break;
+	case XPCS_REG_DESCWERR_COUNTER:
+		reg = XPCS_DESKEW_ERR_CNTR_REG;
+		break;
+	case XPCS_REG_TRAINING_VECTOR:
+		reg = XPCS_TRAINING_VECTOR_REG;
+		break;
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_xmac_xpcs_write"
+				    " Invalid Input: xpcs_reg <0x%x>",
+				    xpcs_reg));
+		return (NPI_FAILURE | NPI_MAC_PCS_REG_INVALID(portn));
+	}
+	val = value;
+
+	XPCS_REG_WR(handle, portn, reg, val);
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_bmac_reset(npi_handle_t handle, uint8_t portn, npi_mac_reset_t mode)
+{
+	uint64_t val = 0;
+	boolean_t txmac = B_FALSE;
+
+	if (!IS_BMAC_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_bmac_reset"
+				    " Invalid Input: portn <%d>",
+				    portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	switch (mode) {
+	case TX_MAC_RESET:
+		BMAC_REG_WR(handle, portn, BTXMAC_SW_RST_REG, 0x1);
+		BMAC_WAIT_REG(handle, portn, BTXMAC_SW_RST_REG, val);
+		txmac = B_TRUE;
+		break;
+	case RX_MAC_RESET:
+		BMAC_REG_WR(handle, portn, BRXMAC_SW_RST_REG, 0x1);
+		BMAC_WAIT_REG(handle, portn, BRXMAC_SW_RST_REG, val);
+		break;
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_bmac_reset"
+				    " Invalid Input: mode <0x%x>",
+				    mode));
+		return (NPI_FAILURE | NPI_MAC_RESET_MODE_INVALID(portn));
+	}
+
+	if (val != 0) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_bmac_reset"
+				    " BMAC_RESET HW Error: ret <0x%x>",
+				    val));
+		if (txmac == B_TRUE)
+			return (NPI_FAILURE | NPI_TXMAC_RESET_FAILED(portn));
+		else
+			return (NPI_FAILURE | NPI_RXMAC_RESET_FAILED(portn));
+	}
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_mac_pcs_reset(npi_handle_t handle, uint8_t portn)
+{
+	/* what to do here ? */
+	uint64_t val = 0;
+	int delay = 100;
+	if (!IS_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_bmac_pcs_reset"
+				    " Invalid Input: portn <%d>",
+				    portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	PCS_REG_RD(handle, portn, PCS_MII_CTRL_REG, &val);
+	val |= PCS_MII_RESET;
+	PCS_REG_WR(handle, portn, PCS_MII_CTRL_REG, val);
+	while ((delay) && (val & PCS_MII_RESET)) {
+		NXGE_DELAY(10);
+		PCS_REG_RD(handle, portn, PCS_MII_CTRL_REG, &val);
+		delay--;
+	}
+	if (delay == 0) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			    " npi_bmac_pcs_reset"
+			    " portn <%d> failed",
+			    portn));
+		return (NPI_FAILURE);
+	}
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_mac_get_link_status(npi_handle_t handle, uint8_t portn,
+			boolean_t *link_up)
+{
+	uint64_t val;
+
+	if (!IS_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_mac_get_link_status"
+				    " Invalid Input: portn <%d>",
+				    portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	PCS_REG_RD(handle, portn, PCS_MII_STATUS_REG, &val);
+
+	if (val & PCS_MII_STATUS_LINK_STATUS) {
+		*link_up = B_TRUE;
+	} else {
+		*link_up = B_FALSE;
+	}
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_bmac_tx_config(npi_handle_t handle, config_op_t op, uint8_t portn,
+			bmac_tx_config_t config)
+{
+	uint64_t val = 0;
+
+	if (!IS_BMAC_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_bmac_tx_config"
+				    " Invalid Input: portn <%d>",
+				    portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	switch (op) {
+	case ENABLE:
+	case DISABLE:
+		if ((config == 0) || (config & ~CFG_BMAC_TX_ALL) != 0) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_bmac_tx_config"
+					    " Invalid Input: config <0x%x>",
+					    config));
+			return (NPI_FAILURE | NPI_MAC_CONFIG_INVALID(portn));
+		}
+		if (op == ENABLE) {
+			BMAC_REG_RD(handle, portn, TXMAC_CONFIG_REG, &val);
+			if (config & CFG_BMAC_TX)
+				val |= MAC_TX_CFG_TXMAC_ENABLE;
+			if (config & CFG_BMAC_TX_CRC)
+				val &= ~MAC_TX_CFG_NO_FCS;
+			BMAC_REG_WR(handle, portn, TXMAC_CONFIG_REG, val);
+		} else {
+			BMAC_REG_RD(handle, portn, TXMAC_CONFIG_REG, &val);
+			if (config & CFG_BMAC_TX)
+				val &= ~MAC_TX_CFG_TXMAC_ENABLE;
+			if (config & CFG_BMAC_TX_CRC)
+				val |= MAC_TX_CFG_NO_FCS;
+			BMAC_REG_WR(handle, portn, TXMAC_CONFIG_REG, val);
+		}
+		break;
+	case INIT:
+		if ((config & ~CFG_BMAC_TX_ALL) != 0) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_bmac_tx_config"
+					    " Invalid Input: config <0x%x>",
+					    config));
+			return (NPI_FAILURE | NPI_MAC_CONFIG_INVALID(portn));
+		}
+		BMAC_REG_RD(handle, portn, TXMAC_CONFIG_REG, &val);
+		if (config & CFG_BMAC_TX)
+			val |= MAC_TX_CFG_TXMAC_ENABLE;
+		else
+			val &= ~MAC_TX_CFG_TXMAC_ENABLE;
+		if (config & CFG_BMAC_TX_CRC)
+			val &= ~MAC_TX_CFG_NO_FCS;
+		else
+			val |= MAC_TX_CFG_NO_FCS;
+		BMAC_REG_WR(handle, portn, TXMAC_CONFIG_REG, val);
+		break;
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_bmac_tx_config"
+				    " Invalid Input: op <0x%x>",
+				    op));
+		return (NPI_FAILURE | NPI_MAC_OPCODE_INVALID(portn));
+	}
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_bmac_rx_config(npi_handle_t handle, config_op_t op, uint8_t portn,
+			bmac_rx_config_t config)
+{
+	uint64_t val = 0;
+
+	if (!IS_BMAC_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_bmac_rx_config"
+					    " Invalid Input: portn <%d>",
+					    portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	switch (op) {
+	case ENABLE:
+	case DISABLE:
+		if ((config == 0) || (config & ~CFG_BMAC_RX_ALL) != 0) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_bmac_rx_config"
+					    " Invalid Input: config <0x%x>",
+					    config));
+			return (NPI_FAILURE | NPI_MAC_CONFIG_INVALID(portn));
+		}
+		if (op == ENABLE) {
+			BMAC_REG_RD(handle, portn, RXMAC_CONFIG_REG, &val);
+			if (config & CFG_BMAC_RX)
+				val |= MAC_RX_CFG_RXMAC_ENABLE;
+			if (config & CFG_BMAC_RX_STRIP_PAD)
+				val |= MAC_RX_CFG_STRIP_PAD;
+			if (config & CFG_BMAC_RX_STRIP_CRC)
+				val |= MAC_RX_CFG_STRIP_FCS;
+			if (config & CFG_BMAC_RX_PROMISCUOUS)
+				val |= MAC_RX_CFG_PROMISC;
+			if (config & CFG_BMAC_RX_PROMISCUOUSGROUP)
+				val |= MAC_RX_CFG_PROMISC_GROUP;
+			if (config & CFG_BMAC_RX_HASH_FILTER)
+				val |= MAC_RX_CFG_HASH_FILTER_EN;
+			if (config & CFG_BMAC_RX_ADDR_FILTER)
+				val |= MAC_RX_CFG_ADDR_FILTER_EN;
+			if (config & CFG_BMAC_RX_DISCARD_ON_ERR)
+				val &= ~MAC_RX_CFG_DISABLE_DISCARD;
+			BMAC_REG_WR(handle, portn, RXMAC_CONFIG_REG, val);
+		} else {
+			BMAC_REG_RD(handle, portn, RXMAC_CONFIG_REG, &val);
+			if (config & CFG_BMAC_RX)
+				val &= ~MAC_RX_CFG_RXMAC_ENABLE;
+			if (config & CFG_BMAC_RX_STRIP_PAD)
+				val &= ~MAC_RX_CFG_STRIP_PAD;
+			if (config & CFG_BMAC_RX_STRIP_CRC)
+				val &= ~MAC_RX_CFG_STRIP_FCS;
+			if (config & CFG_BMAC_RX_PROMISCUOUS)
+				val &= ~MAC_RX_CFG_PROMISC;
+			if (config & CFG_BMAC_RX_PROMISCUOUSGROUP)
+				val &= ~MAC_RX_CFG_PROMISC_GROUP;
+			if (config & CFG_BMAC_RX_HASH_FILTER)
+				val &= ~MAC_RX_CFG_HASH_FILTER_EN;
+			if (config & CFG_BMAC_RX_ADDR_FILTER)
+				val &= ~MAC_RX_CFG_ADDR_FILTER_EN;
+			if (config & CFG_BMAC_RX_DISCARD_ON_ERR)
+				val |= MAC_RX_CFG_DISABLE_DISCARD;
+			BMAC_REG_WR(handle, portn, RXMAC_CONFIG_REG, val);
+		}
+		break;
+	case INIT:
+		if ((config & ~CFG_BMAC_RX_ALL) != 0) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_bmac_rx_config"
+					    " Invalid Input: config <0x%x>",
+					    config));
+			return (NPI_FAILURE | NPI_MAC_CONFIG_INVALID(portn));
+		}
+		BMAC_REG_RD(handle, portn, RXMAC_CONFIG_REG, &val);
+		if (config & CFG_BMAC_RX)
+			val |= MAC_RX_CFG_RXMAC_ENABLE;
+		else
+			val &= ~MAC_RX_CFG_RXMAC_ENABLE;
+		if (config & CFG_BMAC_RX_STRIP_PAD)
+			val |= MAC_RX_CFG_STRIP_PAD;
+		else
+			val &= ~MAC_RX_CFG_STRIP_PAD;
+		if (config & CFG_BMAC_RX_STRIP_CRC)
+			val |= MAC_RX_CFG_STRIP_FCS;
+		else
+			val &= ~MAC_RX_CFG_STRIP_FCS;
+		if (config & CFG_BMAC_RX_PROMISCUOUS)
+			val |= MAC_RX_CFG_PROMISC;
+		else
+			val &= ~MAC_RX_CFG_PROMISC;
+		if (config & CFG_BMAC_RX_PROMISCUOUSGROUP)
+			val |= MAC_RX_CFG_PROMISC_GROUP;
+		else
+			val &= ~MAC_RX_CFG_PROMISC_GROUP;
+		if (config & CFG_BMAC_RX_HASH_FILTER)
+			val |= MAC_RX_CFG_HASH_FILTER_EN;
+		else
+			val &= ~MAC_RX_CFG_HASH_FILTER_EN;
+		if (config & CFG_BMAC_RX_ADDR_FILTER)
+			val |= MAC_RX_CFG_ADDR_FILTER_EN;
+		else
+			val &= ~MAC_RX_CFG_ADDR_FILTER_EN;
+		if (config & CFG_BMAC_RX_DISCARD_ON_ERR)
+			val &= ~MAC_RX_CFG_DISABLE_DISCARD;
+		else
+			val |= MAC_RX_CFG_DISABLE_DISCARD;
+
+		BMAC_REG_WR(handle, portn, RXMAC_CONFIG_REG, val);
+		break;
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_bmac_rx_config"
+					    " Invalid Input: op <0x%x>", op));
+		return (NPI_FAILURE | NPI_MAC_OPCODE_INVALID(portn));
+	}
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_bmac_rx_iconfig(npi_handle_t handle, config_op_t op, uint8_t portn,
+		    bmac_rx_iconfig_t iconfig)
+{
+	uint64_t val = 0;
+
+	if (!IS_BMAC_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_bmac_rx_iconfig"
+				    " Invalid Input: portn <%d>",
+				    portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	switch (op) {
+	case ENABLE:
+	case DISABLE:
+
+		if ((iconfig == 0) || (iconfig & ~ICFG_BMAC_RX_ALL) != 0) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_bmac_rx_iconfig"
+					    " Invalid Input: iconfig <0x%x>",
+					    iconfig));
+			return (NPI_FAILURE | NPI_MAC_CONFIG_INVALID(portn));
+		}
+		BMAC_REG_RD(handle, portn, BRXMAC_STAT_MSK_REG, &val);
+		if (op == ENABLE)
+			val &= ~iconfig;
+		else
+			val |= iconfig;
+		BMAC_REG_WR(handle, portn, BRXMAC_STAT_MSK_REG, val);
+
+		break;
+	case INIT:
+
+		if ((iconfig & ~ICFG_BMAC_RX_ALL) != 0) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_bmac_rx_iconfig"
+					    " Invalid Input: iconfig <0x%x>",
+					    iconfig));
+			return (NPI_FAILURE | NPI_MAC_CONFIG_INVALID(portn));
+		}
+		BMAC_REG_WR(handle, portn, BRXMAC_STAT_MSK_REG, ~iconfig);
+
+		break;
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_bmac_rx_iconfig"
+				    " Invalid Input: iconfig <0x%x>",
+				    iconfig));
+		return (NPI_FAILURE | NPI_MAC_OPCODE_INVALID(portn));
+	}
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_bmac_xif_config(npi_handle_t handle, config_op_t op, uint8_t portn,
+		    bmac_xif_config_t config)
+{
+	uint64_t val = 0;
+
+	if (!IS_BMAC_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_bmac_xif_config"
+				    " Invalid Input: portn <%d>",
+				    portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	switch (op) {
+	case ENABLE:
+	case DISABLE:
+		if ((config == 0) || (config & ~CFG_BMAC_XIF_ALL) != 0) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_bmac_xif_config"
+					    " Invalid Input: config <0x%x>",
+					    config));
+			return (NPI_FAILURE | NPI_MAC_CONFIG_INVALID(portn));
+		}
+		if (op == ENABLE) {
+			BMAC_REG_RD(handle, portn, MAC_XIF_CONFIG_REG, &val);
+			if (config & CFG_BMAC_XIF_TX_OUTPUT)
+				val |= MAC_XIF_TX_OUTPUT_EN;
+			if (config & CFG_BMAC_XIF_LOOPBACK)
+				val |= MAC_XIF_MII_INT_LOOPBACK;
+			if (config & CFG_BMAC_XIF_GMII_MODE)
+				val |= MAC_XIF_GMII_MODE;
+			if (config & CFG_BMAC_XIF_LINKLED)
+				val |= MAC_XIF_LINK_LED;
+			if (config & CFG_BMAC_XIF_LED_POLARITY)
+				val |= MAC_XIF_LED_POLARITY;
+			if (config & CFG_BMAC_XIF_SEL_CLK_25MHZ)
+				val |= MAC_XIF_SEL_CLK_25MHZ;
+			BMAC_REG_WR(handle, portn, MAC_XIF_CONFIG_REG, val);
+		} else {
+			BMAC_REG_RD(handle, portn, MAC_XIF_CONFIG_REG, &val);
+			if (config & CFG_BMAC_XIF_TX_OUTPUT)
+				val &= ~MAC_XIF_TX_OUTPUT_EN;
+			if (config & CFG_BMAC_XIF_LOOPBACK)
+				val &= ~MAC_XIF_MII_INT_LOOPBACK;
+			if (config & CFG_BMAC_XIF_GMII_MODE)
+				val &= ~MAC_XIF_GMII_MODE;
+			if (config & CFG_BMAC_XIF_LINKLED)
+				val &= ~MAC_XIF_LINK_LED;
+			if (config & CFG_BMAC_XIF_LED_POLARITY)
+				val &= ~MAC_XIF_LED_POLARITY;
+			if (config & CFG_BMAC_XIF_SEL_CLK_25MHZ)
+				val &= ~MAC_XIF_SEL_CLK_25MHZ;
+			BMAC_REG_WR(handle, portn, MAC_XIF_CONFIG_REG, val);
+		}
+		break;
+	case INIT:
+		if ((config & ~CFG_BMAC_XIF_ALL) != 0) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_bmac_xif_config"
+					    " Invalid Input: config <0x%x>",
+					    config));
+			return (NPI_FAILURE | NPI_MAC_CONFIG_INVALID(portn));
+		}
+		BMAC_REG_RD(handle, portn, MAC_XIF_CONFIG_REG, &val);
+		if (config & CFG_BMAC_XIF_TX_OUTPUT)
+			val |= MAC_XIF_TX_OUTPUT_EN;
+		else
+			val &= ~MAC_XIF_TX_OUTPUT_EN;
+		if (config & CFG_BMAC_XIF_LOOPBACK)
+			val |= MAC_XIF_MII_INT_LOOPBACK;
+		else
+			val &= ~MAC_XIF_MII_INT_LOOPBACK;
+		if (config & CFG_BMAC_XIF_GMII_MODE)
+			val |= MAC_XIF_GMII_MODE;
+		else
+			val &= ~MAC_XIF_GMII_MODE;
+		if (config & CFG_BMAC_XIF_LINKLED)
+			val |= MAC_XIF_LINK_LED;
+		else
+			val &= ~MAC_XIF_LINK_LED;
+		if (config & CFG_BMAC_XIF_LED_POLARITY)
+			val |= MAC_XIF_LED_POLARITY;
+		else
+			val &= ~MAC_XIF_LED_POLARITY;
+		if (config & CFG_BMAC_XIF_SEL_CLK_25MHZ)
+			val |= MAC_XIF_SEL_CLK_25MHZ;
+		else
+			val &= ~MAC_XIF_SEL_CLK_25MHZ;
+		BMAC_REG_WR(handle, portn, MAC_XIF_CONFIG_REG, val);
+		break;
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_bmac_xif_config"
+				    " Invalid Input: op <0x%x>",
+				    op));
+		return (NPI_FAILURE | NPI_MAC_OPCODE_INVALID(portn));
+	}
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_bmac_tx_iconfig(npi_handle_t handle, config_op_t op, uint8_t portn,
+		    bmac_tx_iconfig_t iconfig)
+{
+	uint64_t val = 0;
+
+	if (!IS_BMAC_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_bmac_tx_iconfig"
+				    " Invalid Input: portn <%d>",
+				    portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	switch (op) {
+	case ENABLE:
+	case DISABLE:
+
+		if ((iconfig == 0) || (iconfig & ~ICFG_XMAC_TX_ALL) != 0) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_bmac_tx_iconfig"
+					    " Invalid Input: iconfig <0x%x>",
+					    iconfig));
+			return (NPI_FAILURE | NPI_MAC_CONFIG_INVALID(portn));
+		}
+		BMAC_REG_RD(handle, portn, BTXMAC_STAT_MSK_REG, &val);
+		if (op == ENABLE)
+			val &= ~iconfig;
+		else
+			val |= iconfig;
+		BMAC_REG_WR(handle, portn, BTXMAC_STAT_MSK_REG, val);
+
+		break;
+	case INIT:
+
+		if ((iconfig & ~ICFG_XMAC_TX_ALL) != 0) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_bmac_tx_iconfig"
+					    " Invalid Input: iconfig <0x%x>",
+					    iconfig));
+			return (NPI_FAILURE | NPI_MAC_CONFIG_INVALID(portn));
+		}
+		BMAC_REG_WR(handle, portn, BTXMAC_STAT_MSK_REG, ~iconfig);
+
+		break;
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_bmac_tx_iconfig"
+				    " Invalid Input: iconfig <0x%x>",
+				    iconfig));
+		return (NPI_FAILURE | NPI_MAC_OPCODE_INVALID(portn));
+	}
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_bmac_ctl_iconfig(npi_handle_t handle, config_op_t op, uint8_t portn,
+			bmac_ctl_iconfig_t iconfig)
+{
+	uint64_t val = 0;
+
+	if (!IS_BMAC_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_bmac_ctl_iconfig"
+				    " Invalid Input: portn <%d>",
+				    portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	switch (op) {
+	case ENABLE:
+	case DISABLE:
+
+		if ((iconfig == 0) || (iconfig & ~ICFG_BMAC_CTL_ALL) != 0) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_bmac_ctl_iconfig"
+					    " Invalid Input: iconfig <0x%x>",
+					    iconfig));
+			return (NPI_FAILURE | NPI_MAC_CONFIG_INVALID(portn));
+		}
+		BMAC_REG_RD(handle, portn, BMAC_C_S_MSK_REG, &val);
+		if (op == ENABLE)
+			val &= ~iconfig;
+		else
+			val |= iconfig;
+		BMAC_REG_WR(handle, portn, BMAC_C_S_MSK_REG, val);
+
+		break;
+	case INIT:
+
+		if ((iconfig & ~ICFG_BMAC_RX_ALL) != 0) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_bmac_ctl_iconfig"
+					    " Invalid Input: iconfig <0x%x>",
+					    iconfig));
+			return (NPI_FAILURE | NPI_MAC_CONFIG_INVALID(portn));
+		}
+		BMAC_REG_WR(handle, portn, BMAC_C_S_MSK_REG, ~iconfig);
+
+		break;
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_bmac_ctl_iconfig"
+				    " Invalid Input: iconfig <0x%x>",
+				    iconfig));
+		return (NPI_FAILURE | NPI_MAC_OPCODE_INVALID(portn));
+	}
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_bmac_tx_get_istatus(npi_handle_t handle, uint8_t portn,
+			bmac_tx_iconfig_t *istatus)
+{
+	uint64_t val = 0;
+
+	if (!IS_BMAC_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_bmac_tx_get_istatus"
+				    " Invalid Input: portn <%d>",
+				    portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	BMAC_REG_RD(handle, portn, BTXMAC_STATUS_REG, &val);
+	*istatus = (uint32_t)val;
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_bmac_rx_get_istatus(npi_handle_t handle, uint8_t portn,
+			bmac_rx_iconfig_t *istatus)
+{
+	uint64_t val = 0;
+
+	if (!IS_BMAC_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_bmac_rx_get_istatus"
+				    " Invalid Input: portn <%d>",
+				    portn));
+
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	BMAC_REG_RD(handle, portn, BRXMAC_STATUS_REG, &val);
+	*istatus = (uint32_t)val;
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_bmac_ctl_get_istatus(npi_handle_t handle, uint8_t portn,
+				bmac_ctl_iconfig_t *istatus)
+{
+	uint64_t val = 0;
+
+	if (!IS_BMAC_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_bmac_ctl_get_istatus"
+				    " Invalid Input: portn <%d>",
+				    portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	BMAC_REG_RD(handle, portn, BMAC_CTRL_STAT_REG, &val);
+	*istatus = (uint32_t)val;
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_mac_mif_mdio_read(npi_handle_t handle, uint8_t portn, uint8_t device,
+			uint16_t xcvr_reg, uint16_t *value)
+{
+	mif_frame_t frame;
+	uint_t loop, loop_max = 1000;
+	uint32_t loop_delay = 100;
+	frame.value = 0;
+	frame.bits.w0.st = FRAME45_ST;		/* Clause 45	*/
+	frame.bits.w0.op = FRAME45_OP_ADDR;	/* Select address	*/
+	frame.bits.w0.phyad = portn;		/* Port number	*/
+	frame.bits.w0.regad = device;		/* Device number	*/
+	frame.bits.w0.ta_msb = 1;
+	frame.bits.w0.ta_lsb = 0;
+	frame.bits.w0.data = xcvr_reg;	/* register address */
+
+	NPI_DEBUG_MSG((handle.function, MIF_CTL,
+		"mdio read port %d addr val=0x%x\n", portn, frame.value));
+
+	MIF_REG_WR(handle, MIF_OUTPUT_FRAME_REG, frame.value);
+
+	loop = 0;
+	do {
+		NXGE_DELAY(loop_delay);
+		MIF_REG_RD(handle, MIF_OUTPUT_FRAME_REG, &frame.value)
+		loop++;
+	} while ((frame.bits.w0.ta_lsb == 0) && (loop < loop_max));
+
+	NPI_DEBUG_MSG((handle.function, MIF_CTL,
+		"mdio read port %d addr poll=0x%x\n", portn, frame.value));
+
+	if (loop == loop_max) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					"mdio read no response1\n"));
+	}
+
+	frame.bits.w0.st = FRAME45_ST; /* Clause 45 */
+	frame.bits.w0.op = FRAME45_OP_READ; /* Read */
+	frame.bits.w0.phyad = portn; /* Port Number */
+	frame.bits.w0.regad = device; /* Device Number */
+	frame.bits.w0.ta_msb = 1;
+	frame.bits.w0.ta_lsb = 0;
+
+	NPI_DEBUG_MSG((handle.function, MIF_CTL,
+		"mdio read port %d data frame=0x%x\n", portn, frame.value));
+
+	MIF_REG_WR(handle, MIF_OUTPUT_FRAME_REG, frame.value);
+
+	loop = 0;
+	do {
+		NXGE_DELAY(loop_delay);
+		MIF_REG_RD(handle, MIF_OUTPUT_FRAME_REG, &frame.value)
+		loop++;
+	} while ((frame.bits.w0.ta_lsb == 0) && (loop < loop_max));
+
+	NPI_DEBUG_MSG((handle.function, MIF_CTL,
+		"mdio read port %d data poll=0x%x\n", portn, frame.value));
+
+	*value = frame.bits.w0.data;
+	NPI_DEBUG_MSG((handle.function, MIF_CTL,
+		"mdio read port=%d val=0x%x\n", portn, *value));
+
+	if (loop == loop_max) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			"mdio read no response2\n"));
+	}
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_mac_mif_mii_read(npi_handle_t handle, uint8_t portn, uint8_t xcvr_reg,
+			uint16_t *value)
+{
+	mif_frame_t frame;
+	uint_t loop;
+
+	frame.bits.w0.st = 0x1; /* Clause 22 */
+	frame.bits.w0.op = 0x2;
+	frame.bits.w0.phyad = portn;
+	frame.bits.w0.regad = xcvr_reg;
+	frame.bits.w0.ta_msb = 1;
+	frame.bits.w0.ta_lsb = 0;
+	MIF_REG_WR(handle, MIF_OUTPUT_FRAME_REG, frame.value);
+
+	loop = 0;
+	do {
+		NXGE_DELAY(500);
+		MIF_REG_RD(handle, MIF_OUTPUT_FRAME_REG, &frame.value)
+		loop++;
+	} while ((frame.bits.w0.ta_lsb == 0) && (loop < MAX_PIO_RETRIES));
+
+	if (loop == MAX_PIO_RETRIES)
+		return (NPI_FAILURE | NPI_MAC_MII_READ_FAILED(portn));
+
+	*value = frame.bits.w0.data;
+	NPI_DEBUG_MSG((handle.function, MIF_CTL,
+			"mif mii read port %d reg=0x%x frame=0x%x\n", portn,
+			xcvr_reg, frame.bits.w0.data));
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_mac_mif_mdio_write(npi_handle_t handle, uint8_t portn, uint8_t device,
+			uint16_t xcvr_reg, uint16_t value)
+{
+	mif_frame_t frame;
+	uint_t loop, loop_max = 1000;
+	uint32_t loop_delay = 100;
+
+	frame.value = 0;
+	frame.bits.w0.st = FRAME45_ST; /* Clause 45 */
+	frame.bits.w0.op = FRAME45_OP_ADDR; /* Select Address */
+	frame.bits.w0.phyad = portn; /* Port Number */
+	frame.bits.w0.regad = device; /* Device Number */
+	frame.bits.w0.ta_msb = 1;
+	frame.bits.w0.ta_lsb = 0;
+	frame.bits.w0.data = xcvr_reg;	/* register address */
+
+	MIF_REG_WR(handle, MIF_OUTPUT_FRAME_REG, frame.value);
+
+	NPI_DEBUG_MSG((handle.function, MIF_CTL,
+		"mdio write port %d addr val=0x%x\n", portn, frame.value));
+
+	loop = 0;
+	do {
+		NXGE_DELAY(loop_delay);
+		MIF_REG_RD(handle, MIF_OUTPUT_FRAME_REG, &frame.value)
+		loop++;
+	} while ((frame.bits.w0.ta_lsb == 0) && loop < loop_max);
+
+	NPI_DEBUG_MSG((handle.function, MIF_CTL,
+		"mdio write port %d addr poll=0x%x\n", portn, frame.value));
+
+	if (loop == loop_max) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				"mdio write no response1\n"));
+	}
+
+	frame.bits.w0.st = FRAME45_ST; /* Clause 45 */
+	frame.bits.w0.op = FRAME45_OP_WRITE; /* Write */
+	frame.bits.w0.phyad = portn; /* Port number   */
+	frame.bits.w0.regad = device; /* Device number */
+	frame.bits.w0.ta_msb = 1;
+	frame.bits.w0.ta_lsb = 0;
+	frame.bits.w0.data = value;
+	MIF_REG_WR(handle, MIF_OUTPUT_FRAME_REG, frame.value);
+
+	NPI_DEBUG_MSG((handle.function, MIF_CTL,
+		"mdio write port %d data val=0x%x\n", portn, frame.value));
+
+	loop = 0;
+	do {
+		NXGE_DELAY(loop_delay);
+		MIF_REG_RD(handle, MIF_OUTPUT_FRAME_REG, &frame.value)
+		loop++;
+	} while ((frame.bits.w0.ta_lsb == 0) && loop < loop_max);
+
+	NPI_DEBUG_MSG((handle.function, MIF_CTL,
+		"mdio write port %d data poll=0x%x\n", portn, frame.value));
+
+	if (loop == loop_max) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				"mdio write no response2\n"));
+	}
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_mac_mif_mii_write(npi_handle_t handle, uint8_t portn, uint8_t xcvr_reg,
+			uint16_t value)
+{
+	mif_frame_t frame;
+	uint_t loop;
+
+	frame.bits.w0.st = 0x1; /* Clause 22 */
+	frame.bits.w0.op = 0x1;
+	frame.bits.w0.phyad = portn;
+	frame.bits.w0.regad = xcvr_reg;
+	frame.bits.w0.ta_msb = 1;
+	frame.bits.w0.ta_lsb = 0;
+	frame.bits.w0.data = value;
+	MIF_REG_WR(handle, MIF_OUTPUT_FRAME_REG, frame.value);
+
+	loop = 0;
+	do {
+		NXGE_DELAY(500);
+		MIF_REG_RD(handle, MIF_OUTPUT_FRAME_REG, &frame.value);
+		loop++;
+	} while ((frame.bits.w0.ta_lsb == 0) && loop < MAX_PIO_RETRIES);
+
+	NPI_DEBUG_MSG((handle.function, MIF_CTL,
+			"mif mii write port %d reg=0x%x frame=0x%x\n", portn,
+			xcvr_reg, frame.value));
+
+	if (loop == MAX_PIO_RETRIES)
+		return (NPI_FAILURE | NPI_MAC_MII_WRITE_FAILED(portn));
+
+	return (NPI_SUCCESS);
+}
+
+
+npi_status_t
+npi_mac_pcs_mii_read(npi_handle_t handle, uint8_t portn, uint8_t xcvr_reg,
+			uint16_t *value)
+{
+	pcs_anar_t pcs_anar;
+	pcs_anar_t pcs_anlpar;
+	pcs_stat_t pcs_stat;
+	pcs_stat_mc_t pcs_stat_mc;
+	mii_anar_t anar;
+	mii_anar_t anlpar;
+	mii_aner_t aner;
+	mii_esr_t esr;
+	mii_gsr_t gsr;
+	uint64_t val = 0;
+
+	if (!IS_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_mac_pcs_mii_read"
+				    " Invalid Input: portn <%d>",
+				    portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	switch (xcvr_reg) {
+	case NXGE_MII_BMCR:
+		PCS_REG_RD(handle, portn, PCS_MII_CTRL_REG, &val);
+		*value = (uint16_t)val;
+		break;
+	case NXGE_MII_BMSR:
+		PCS_REG_RD(handle, portn, PCS_MII_STATUS_REG, &val);
+		pcs_stat.value = val;
+		PCS_REG_RD(handle, portn, PCS_STATE_MACHINE_REG, &val);
+		pcs_stat_mc.value = val;
+		if ((pcs_stat_mc.bits.w0.link_cfg_stat == 0xB) &&
+			(pcs_stat_mc.bits.w0.word_sync != 0)) {
+			pcs_stat.bits.w0.link_stat = 1;
+		} else if (pcs_stat_mc.bits.w0.link_cfg_stat != 0xB) {
+			pcs_stat.bits.w0.link_stat = 0;
+		}
+		*value = (uint16_t)pcs_stat.value;
+		break;
+	case NXGE_MII_ESR:
+		PCS_REG_RD(handle, portn, PCS_MII_ADVERT_REG, &val);
+		pcs_anar.value = (uint16_t)val;
+		esr.value = 0;
+		esr.bits.link_1000fdx = pcs_anar.bits.w0.full_duplex;
+		esr.bits.link_1000hdx = pcs_anar.bits.w0.half_duplex;
+		*value = esr.value;
+		break;
+	case NXGE_MII_ANAR:
+		PCS_REG_RD(handle, portn, PCS_MII_ADVERT_REG, &val);
+		pcs_anar.value = (uint16_t)val;
+		anar.value = 0;
+		anar.bits.cap_pause = pcs_anar.bits.w0.pause;
+		anar.bits.cap_asmpause = pcs_anar.bits.w0.asm_pause;
+		*value = anar.value;
+		break;
+	case NXGE_MII_ANLPAR:
+		PCS_REG_RD(handle, portn, PCS_MII_LPA_REG, &val);
+		pcs_anlpar.value = (uint16_t)val;
+		anlpar.bits.cap_pause = pcs_anlpar.bits.w0.pause;
+		anlpar.bits.cap_asmpause = pcs_anlpar.bits.w0.asm_pause;
+		*value = anlpar.value;
+		break;
+	case NXGE_MII_ANER:
+		PCS_REG_RD(handle, portn, PCS_MII_ADVERT_REG, &val);
+		pcs_anar.value = (uint16_t)val;
+		aner.value = 0;
+		aner.bits.lp_an_able = pcs_anar.bits.w0.full_duplex |
+						pcs_anar.bits.w0.half_duplex;
+		*value = aner.value;
+		break;
+	case NXGE_MII_GSR:
+		PCS_REG_RD(handle, portn, PCS_MII_LPA_REG, &val);
+		pcs_anar.value = (uint16_t)val;
+		gsr.value = 0;
+		gsr.bits.link_1000fdx = pcs_anar.bits.w0.full_duplex;
+		gsr.bits.link_1000hdx = pcs_anar.bits.w0.half_duplex;
+		*value = gsr.value;
+		break;
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_mac_pcs_mii_read"
+				    " Invalid Input: xcvr_reg <0x%x>",
+				    xcvr_reg));
+		return (NPI_FAILURE | NPI_MAC_REG_INVALID(portn));
+	}
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_mac_pcs_mii_write(npi_handle_t handle, uint8_t portn, uint8_t xcvr_reg,
+			uint16_t value)
+{
+	pcs_anar_t pcs_anar;
+	mii_anar_t anar;
+	mii_gcr_t gcr;
+	uint64_t val;
+
+	if (!IS_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_mac_pcs_mii_write"
+				    " Invalid Input: portn <%d>",
+				    portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	switch (xcvr_reg) {
+	case NXGE_MII_BMCR:
+		val = (uint16_t)value;
+		PCS_REG_WR(handle, portn, PCS_MII_CTRL_REG, val);
+		break;
+	case NXGE_MII_ANAR:
+		PCS_REG_RD(handle, portn, PCS_MII_ADVERT_REG, &val);
+		pcs_anar.value = (uint16_t)val;
+		anar.value = value;
+		pcs_anar.bits.w0.asm_pause = anar.bits.cap_asmpause;
+		pcs_anar.bits.w0.pause = anar.bits.cap_pause;
+		val = pcs_anar.value;
+		PCS_REG_WR(handle, portn, PCS_MII_ADVERT_REG, val);
+		break;
+	case NXGE_MII_GCR:
+		PCS_REG_RD(handle, portn, PCS_MII_ADVERT_REG, &val);
+		pcs_anar.value = (uint16_t)val;
+		gcr.value = value;
+		pcs_anar.bits.w0.full_duplex = gcr.bits.link_1000fdx;
+		pcs_anar.bits.w0.half_duplex = gcr.bits.link_1000hdx;
+		val = pcs_anar.value;
+		PCS_REG_WR(handle, portn, PCS_MII_ADVERT_REG, val);
+		break;
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_mac_pcs_mii_write"
+				    " Invalid Input: xcvr_reg <0x%x>",
+				    xcvr_reg));
+		return (NPI_FAILURE | NPI_MAC_REG_INVALID(portn));
+	}
+
+	return (NPI_SUCCESS);
+}
+
+
+npi_status_t
+npi_mac_mif_link_intr_enable(npi_handle_t handle, uint8_t portn,
+				uint8_t xcvr_reg, uint16_t mask)
+{
+	mif_cfg_t mif_cfg;
+
+	if (!IS_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_mac_mif_link_intr_enable"
+				    " Invalid Input: portn <%d>",
+				    portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	if (xcvr_reg > NXGE_MAX_MII_REGS) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_mac_mif_link_intr_enable"
+				    " Invalid Input: xcvr_reg <0x%x>",
+				    xcvr_reg));
+		return (NPI_FAILURE | NPI_MAC_REG_INVALID(portn));
+	}
+
+	MIF_REG_RD(handle, MIF_CONFIG_REG, &mif_cfg.value);
+
+	mif_cfg.bits.w0.phy_addr = portn;		/* Port number */
+	mif_cfg.bits.w0.reg_addr = xcvr_reg;		/* Register address */
+	mif_cfg.bits.w0.indirect_md = 0; 		/* Clause 22 */
+	mif_cfg.bits.w0.poll_en = 1;
+
+	MIF_REG_WR(handle, MIF_MASK_REG, ~mask);
+	MIF_REG_WR(handle, MIF_CONFIG_REG, mif_cfg.value);
+
+	NXGE_DELAY(20);
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_mac_mif_mdio_link_intr_enable(npi_handle_t handle, uint8_t portn,
+			uint8_t device, uint16_t xcvr_reg, uint16_t mask)
+{
+	mif_cfg_t mif_cfg;
+	mif_frame_t frame;
+	uint_t loop;
+
+	if (!IS_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					"npi_mac_mif_mdio_poll_enable"
+					" Invalid Input: portn <%d>",
+					portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	frame.bits.w0.st = 0;		/* Clause 45 */
+	frame.bits.w0.op = 0;		/* Select address */
+	frame.bits.w0.phyad = portn;	/* Port number */
+	frame.bits.w0.regad = device;	/* Device number */
+	frame.bits.w0.ta_msb = 1;
+	frame.bits.w0.ta_lsb = 0;
+	frame.bits.w0.data = xcvr_reg;	/* register address */
+
+	MIF_REG_WR(handle, MIF_OUTPUT_FRAME_REG, frame.value);
+
+	loop = 0;
+	do {
+		NXGE_DELAY(500);
+		MIF_REG_RD(handle, MIF_OUTPUT_FRAME_REG, &frame.value)
+		loop++;
+	} while ((frame.bits.w0.ta_lsb == 0) && (loop < MAX_PIO_RETRIES));
+
+	if (loop == MAX_PIO_RETRIES)
+		return (NPI_FAILURE);
+
+	MIF_REG_RD(handle, MIF_CONFIG_REG, &mif_cfg.value);
+
+	mif_cfg.bits.w0.phy_addr = portn;		/* Port number */
+	mif_cfg.bits.w0.reg_addr = device;		/* Register address */
+	mif_cfg.bits.w0.indirect_md = 1; 		/* Clause 45 */
+	mif_cfg.bits.w0.poll_en = 1;
+
+	MIF_REG_WR(handle, MIF_MASK_REG, ~mask);
+	MIF_REG_WR(handle, MIF_CONFIG_REG, mif_cfg.value);
+
+	NXGE_DELAY(20);
+
+	return (NPI_SUCCESS);
+}
+
+void
+npi_mac_mif_set_indirect_mode(npi_handle_t handle, boolean_t on_off)
+{
+	mif_cfg_t mif_cfg;
+
+	MIF_REG_RD(handle, MIF_CONFIG_REG, &mif_cfg.value);
+	mif_cfg.bits.w0.indirect_md = on_off;
+	MIF_REG_WR(handle, MIF_CONFIG_REG, mif_cfg.value);
+}
+
+void
+npi_mac_mif_set_atca_mode(npi_handle_t handle, boolean_t on_off)
+{
+	mif_cfg_t mif_cfg;
+
+	MIF_REG_RD(handle, MIF_CONFIG_REG, &mif_cfg.value);
+	mif_cfg.bits.w0.atca_ge = on_off;
+	MIF_REG_WR(handle, MIF_CONFIG_REG, mif_cfg.value);
+}
+
+npi_status_t
+npi_bmac_send_pause(npi_handle_t handle, uint8_t portn, uint16_t pause_time)
+{
+	uint64_t val;
+
+	if (!IS_BMAC_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_bmac_send_pause"
+				    " Invalid Input: portn <%d>",
+				    portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	val = MAC_SEND_PAUSE_SEND | pause_time;
+	BMAC_REG_WR(handle, portn, MAC_SEND_PAUSE_REG, val);
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_xmac_xif_led(npi_handle_t handle, uint8_t portn, boolean_t on_off)
+{
+	uint64_t val = 0;
+
+	if (!IS_XMAC_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_xmac_led"
+				    " Invalid Input: portn <%d>", portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	XMAC_REG_RD(handle, portn, XMAC_CONFIG_REG, &val);
+
+	if (on_off == B_TRUE) {
+		val |= XMAC_XIF_LED_POLARITY;
+		val &= ~XMAC_XIF_FORCE_LED_ON;
+	} else {
+		val &= ~XMAC_XIF_LED_POLARITY;
+		val |= XMAC_XIF_FORCE_LED_ON;
+	}
+
+	XMAC_REG_WR(handle, portn, XMAC_CONFIG_REG, val);
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_xmac_zap_tx_counters(npi_handle_t handle, uint8_t portn)
+{
+	if (!IS_XMAC_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_xmac_zap_tx_counters"
+				    " Invalid Input: portn <%d>", portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	XMAC_REG_WR(handle, portn, XTXMAC_FRM_CNT_REG, 0);
+	XMAC_REG_WR(handle, portn, XTXMAC_BYTE_CNT_REG, 0);
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_xmac_zap_rx_counters(npi_handle_t handle, uint8_t portn)
+{
+	if (!IS_XMAC_PORT_NUM_VALID(portn)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_xmac_zap_rx_counters"
+				    " Invalid Input: portn <%d>", portn));
+		return (NPI_FAILURE | NPI_MAC_PORT_INVALID(portn));
+	}
+
+	XMAC_REG_WR(handle, portn, XRXMAC_BT_CNT_REG, 0);
+	XMAC_REG_WR(handle, portn, XRXMAC_BC_FRM_CNT_REG, 0);
+	XMAC_REG_WR(handle, portn, XRXMAC_MC_FRM_CNT_REG, 0);
+	XMAC_REG_WR(handle, portn, XRXMAC_FRAG_CNT_REG, 0);
+	XMAC_REG_WR(handle, portn, XRXMAC_HIST_CNT1_REG, 0);
+	XMAC_REG_WR(handle, portn, XRXMAC_HIST_CNT2_REG, 0);
+	XMAC_REG_WR(handle, portn, XRXMAC_HIST_CNT3_REG, 0);
+	XMAC_REG_WR(handle, portn, XRXMAC_HIST_CNT4_REG, 0);
+	XMAC_REG_WR(handle, portn, XRXMAC_HIST_CNT5_REG, 0);
+	XMAC_REG_WR(handle, portn, XRXMAC_HIST_CNT6_REG, 0);
+	XMAC_REG_WR(handle, portn, XRXMAC_MPSZER_CNT_REG, 0);
+	XMAC_REG_WR(handle, portn, XRXMAC_CRC_ER_CNT_REG, 0);
+	XMAC_REG_WR(handle, portn, XRXMAC_CD_VIO_CNT_REG, 0);
+	XMAC_REG_WR(handle, portn, XRXMAC_AL_ER_CNT_REG, 0);
+	XMAC_REG_WR(handle, portn, XMAC_LINK_FLT_CNT_REG, 0);
+
+	return (NPI_SUCCESS);
+}
diff --git a/drivers/net/nxge/npi/npi_mac.h b/drivers/net/nxge/npi/npi_mac.h
new file mode 100644
index 0000000..816c384
--- /dev/null
+++ b/drivers/net/nxge/npi/npi_mac.h
@@ -0,0 +1,588 @@
+/*
+ * npi_mac.h	Neptune Ethernet MAC HW API functions
+ *
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef _NPI_MAC_H
+#define	_NPI_MAC_H
+
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#include <npi.h>
+#include <nxge_mac_hw.h>
+#include <nxge_mii.h>
+
+typedef struct _npi_mac_addr {
+	uint16_t	w0;
+	uint16_t	w1;
+	uint16_t	w2;
+} npi_mac_addr_t;
+
+typedef enum npi_mac_attr {
+	MAC_PORT_MODE = 0,
+	MAC_PORT_FRAME_SIZE,
+	MAC_PORT_ADDR,
+	MAC_PORT_ADDR_FILTER,
+	MAC_PORT_ADDR_FILTER_MASK,
+	XMAC_PORT_IPG,
+	XMAC_10G_PORT_IPG,
+	BMAC_PORT_MAX_BURST_SIZE,
+	BMAC_PORT_PA_SIZE,
+	BMAC_PORT_CTRL_TYPE
+} npi_mac_attr_t;
+
+/* MAC Mode options */
+
+typedef enum npi_mac_mode_e {
+	MAC_MII_MODE = 0,
+	MAC_GMII_MODE,
+	MAC_XGMII_MODE
+} npi_mac_mode_t;
+
+typedef enum npi_mac_reset_e {
+	TX_MAC_RESET = 1,
+	RX_MAC_RESET,
+	XTX_MAC_REG_RESET,
+	XRX_MAC_REG_RESET,
+	XTX_MAC_LOGIC_RESET,
+	XRX_MAC_LOGIC_RESET,
+	XTX_MAC_RESET_ALL,
+	XRX_MAC_RESET_ALL,
+	BMAC_RESET_ALL,
+	XMAC_RESET_ALL
+} npi_mac_reset_t;
+
+typedef enum xmac_tx_iconfig_e {
+	ICFG_XMAC_TX_FRAME_XMIT 	= XMAC_TX_FRAME_XMIT,
+	ICFG_XMAC_TX_UNDERRUN		= XMAC_TX_UNDERRUN,
+	ICFG_XMAC_TX_MAX_PACKET_ERR	= XMAC_TX_MAX_PACKET_ERR,
+	ICFG_XMAC_TX_OVERFLOW		= XMAC_TX_OVERFLOW,
+	ICFG_XMAC_TX_FIFO_XFR_ERR	= XMAC_TX_FIFO_XFR_ERR,
+	ICFG_XMAC_TX_BYTE_CNT_EXP	= XMAC_TX_BYTE_CNT_EXP,
+	ICFG_XMAC_TX_FRAME_CNT_EXP	= XMAC_TX_FRAME_CNT_EXP,
+	ICFG_XMAC_TX_ALL = (XMAC_TX_FRAME_XMIT | XMAC_TX_UNDERRUN |
+				XMAC_TX_MAX_PACKET_ERR | XMAC_TX_OVERFLOW |
+				XMAC_TX_FIFO_XFR_ERR |  XMAC_TX_BYTE_CNT_EXP |
+				XMAC_TX_FRAME_CNT_EXP)
+} xmac_tx_iconfig_t;
+
+typedef enum xmac_rx_iconfig_e {
+	ICFG_XMAC_RX_FRAME_RCVD		= XMAC_RX_FRAME_RCVD,
+	ICFG_XMAC_RX_OVERFLOW		= XMAC_RX_OVERFLOW,
+	ICFG_XMAC_RX_UNDERFLOW		= XMAC_RX_UNDERFLOW,
+	ICFG_XMAC_RX_CRC_ERR_CNT_EXP	= XMAC_RX_CRC_ERR_CNT_EXP,
+	ICFG_XMAC_RX_LEN_ERR_CNT_EXP	= XMAC_RX_LEN_ERR_CNT_EXP,
+	ICFG_XMAC_RX_VIOL_ERR_CNT_EXP	= XMAC_RX_VIOL_ERR_CNT_EXP,
+	ICFG_XMAC_RX_OCT_CNT_EXP	= XMAC_RX_OCT_CNT_EXP,
+	ICFG_XMAC_RX_HST_CNT1_EXP	= XMAC_RX_HST_CNT1_EXP,
+	ICFG_XMAC_RX_HST_CNT2_EXP	= XMAC_RX_HST_CNT2_EXP,
+	ICFG_XMAC_RX_HST_CNT3_EXP	= XMAC_RX_HST_CNT3_EXP,
+	ICFG_XMAC_RX_HST_CNT4_EXP	= XMAC_RX_HST_CNT4_EXP,
+	ICFG_XMAC_RX_HST_CNT5_EXP	= XMAC_RX_HST_CNT5_EXP,
+	ICFG_XMAC_RX_HST_CNT6_EXP	= XMAC_RX_HST_CNT6_EXP,
+	ICFG_XMAC_RX_BCAST_CNT_EXP	= XMAC_RX_BCAST_CNT_EXP,
+	ICFG_XMAC_RX_MCAST_CNT_EXP	= XMAC_RX_MCAST_CNT_EXP,
+	ICFG_XMAC_RX_FRAG_CNT_EXP	= XMAC_RX_FRAG_CNT_EXP,
+	ICFG_XMAC_RX_ALIGNERR_CNT_EXP	= XMAC_RX_ALIGNERR_CNT_EXP,
+	ICFG_XMAC_RX_LINK_FLT_CNT_EXP	= XMAC_RX_LINK_FLT_CNT_EXP,
+	ICFG_XMAC_RX_HST_CNT7_EXP	= XMAC_RX_HST_CNT7_EXP,
+	ICFG_XMAC_RX_REMOTE_FLT_DET	= XMAC_RX_REMOTE_FLT_DET,
+	ICFG_XMAC_RX_LOCAL_FLT_DET	= XMAC_RX_LOCAL_FLT_DET,
+	ICFG_XMAC_RX_ALL = (XMAC_RX_FRAME_RCVD | XMAC_RX_OVERFLOW |
+				XMAC_RX_UNDERFLOW | XMAC_RX_CRC_ERR_CNT_EXP |
+				XMAC_RX_LEN_ERR_CNT_EXP |
+				XMAC_RX_VIOL_ERR_CNT_EXP |
+				XMAC_RX_OCT_CNT_EXP | XMAC_RX_HST_CNT1_EXP |
+				XMAC_RX_HST_CNT2_EXP | XMAC_RX_HST_CNT3_EXP |
+				XMAC_RX_HST_CNT4_EXP | XMAC_RX_HST_CNT5_EXP |
+				XMAC_RX_HST_CNT6_EXP | XMAC_RX_BCAST_CNT_EXP |
+				XMAC_RX_MCAST_CNT_EXP | XMAC_RX_FRAG_CNT_EXP |
+				XMAC_RX_ALIGNERR_CNT_EXP |
+				XMAC_RX_LINK_FLT_CNT_EXP |
+				XMAC_RX_HST_CNT7_EXP |
+				XMAC_RX_REMOTE_FLT_DET | XMAC_RX_LOCAL_FLT_DET)
+} xmac_rx_iconfig_t;
+
+typedef enum xmac_ctl_iconfig_e {
+	ICFG_XMAC_CTRL_PAUSE_RCVD	= XMAC_CTRL_PAUSE_RCVD,
+	ICFG_XMAC_CTRL_PAUSE_STATE	= XMAC_CTRL_PAUSE_STATE,
+	ICFG_XMAC_CTRL_NOPAUSE_STATE	= XMAC_CTRL_NOPAUSE_STATE,
+	ICFG_XMAC_CTRL_ALL = (XMAC_CTRL_PAUSE_RCVD | XMAC_CTRL_PAUSE_STATE |
+				XMAC_CTRL_NOPAUSE_STATE)
+} xmac_ctl_iconfig_t;
+
+
+typedef enum bmac_tx_iconfig_e {
+	ICFG_BMAC_TX_FRAME_SENT 	= MAC_TX_FRAME_XMIT,
+	ICFG_BMAC_TX_UNDERFLOW		= MAC_TX_UNDERRUN,
+	ICFG_BMAC_TX_MAXPKTSZ_ERR	= MAC_TX_MAX_PACKET_ERR,
+	ICFG_BMAC_TX_BYTE_CNT_EXP	= MAC_TX_BYTE_CNT_EXP,
+	ICFG_BMAC_TX_FRAME_CNT_EXP	= MAC_TX_FRAME_CNT_EXP,
+	ICFG_BMAC_TX_ALL = (MAC_TX_FRAME_XMIT | MAC_TX_UNDERRUN |
+				MAC_TX_MAX_PACKET_ERR | MAC_TX_BYTE_CNT_EXP |
+				MAC_TX_FRAME_CNT_EXP)
+} bmac_tx_iconfig_t;
+
+typedef enum bmac_rx_iconfig_e {
+	ICFG_BMAC_RX_FRAME_RCVD		= MAC_RX_FRAME_RECV,
+	ICFG_BMAC_RX_OVERFLOW		= MAC_RX_OVERFLOW,
+	ICFG_BMAC_RX_FRAME_CNT_EXP	= MAC_RX_FRAME_COUNT,
+	ICFG_BMAC_RX_CRC_ERR_CNT_EXP	= MAC_RX_ALIGN_ERR,
+	ICFG_BMAC_RX_LEN_ERR_CNT_EXP	= MAC_RX_CRC_ERR,
+	ICFG_BMAC_RX_VIOL_ERR_CNT_EXP	= MAC_RX_LEN_ERR,
+	ICFG_BMAC_RX_BYTE_CNT_EXP	= MAC_RX_VIOL_ERR,
+	ICFG_BMAC_RX_ALIGNERR_CNT_EXP	= MAC_RX_BYTE_CNT_EXP,
+	ICFG_BMAC_RX_ALL = (MAC_RX_FRAME_RECV | MAC_RX_OVERFLOW |
+				MAC_RX_FRAME_COUNT | MAC_RX_ALIGN_ERR |
+				MAC_RX_CRC_ERR | MAC_RX_LEN_ERR |
+				MAC_RX_VIOL_ERR | MAC_RX_BYTE_CNT_EXP)
+} bmac_rx_iconfig_t;
+
+typedef enum bmac_ctl_iconfig_e {
+	ICFG_BMAC_CTL_RCVPAUSE		= MAC_CTRL_PAUSE_RECEIVED,
+	ICFG_BMAC_CTL_INPAUSE_ST	= MAC_CTRL_PAUSE_STATE,
+	ICFG_BMAC_CTL_INNOTPAUSE_ST	= MAC_CTRL_NOPAUSE_STATE,
+	ICFG_BMAC_CTL_ALL = (MAC_CTRL_PAUSE_RECEIVED | MAC_CTRL_PAUSE_STATE |
+				MAC_CTRL_NOPAUSE_STATE)
+} bmac_ctl_iconfig_t;
+
+typedef	enum xmac_tx_config_e {
+	CFG_XMAC_TX			= 0x00000001,
+	CFG_XMAC_TX_STRETCH_MODE	= 0x00000002,
+	CFG_XMAC_VAR_IPG		= 0x00000004,
+	CFG_XMAC_TX_CRC			= 0x00000008,
+	CFG_XMAC_TX_ALL			= 0x0000000F
+} xmac_tx_config_t;
+
+typedef enum xmac_rx_config_e {
+	CFG_XMAC_RX			= 0x00000001,
+	CFG_XMAC_RX_PROMISCUOUS		= 0x00000002,
+	CFG_XMAC_RX_PROMISCUOUSGROUP	= 0x00000004,
+	CFG_XMAC_RX_ERRCHK		= 0x00000008,
+	CFG_XMAC_RX_CRC_CHK		= 0x00000010,
+	CFG_XMAC_RX_RESV_MULTICAST	= 0x00000020,
+	CFG_XMAC_RX_CODE_VIO_CHK	= 0x00000040,
+	CFG_XMAC_RX_HASH_FILTER		= 0x00000080,
+	CFG_XMAC_RX_ADDR_FILTER		= 0x00000100,
+	CFG_XMAC_RX_STRIP_CRC		= 0x00000200,
+	CFG_XMAC_RX_PAUSE		= 0x00000400,
+	CFG_XMAC_RX_PASS_FC_FRAME	= 0x00000800,
+	CFG_XMAC_RX_MAC2IPP_PKT_CNT	= 0x00001000,
+	CFG_XMAC_RX_ALL			= 0x00001FFF
+} xmac_rx_config_t;
+
+typedef	enum xmac_xif_config_e {
+	CFG_XMAC_XIF_LED_FORCE		= 0x00000001,
+	CFG_XMAC_XIF_LED_POLARITY	= 0x00000002,
+	CFG_XMAC_XIF_SEL_POR_CLK_SRC	= 0x00000004,
+	CFG_XMAC_XIF_TX_OUTPUT		= 0x00000008,
+	CFG_XMAC_XIF_LOOPBACK		= 0x00000010,
+	CFG_XMAC_XIF_LFS		= 0x00000020,
+	CFG_XMAC_XIF_XPCS_BYPASS	= 0x00000040,
+	CFG_XMAC_XIF_1G_PCS_BYPASS	= 0x00000080,
+	CFG_XMAC_XIF_SEL_CLK_25MHZ	= 0x00000100,
+	CFG_XMAC_XIF_ALL		= 0x000001FF
+} xmac_xif_config_t;
+
+typedef	enum bmac_tx_config_e {
+	CFG_BMAC_TX			= 0x00000001,
+	CFG_BMAC_TX_CRC			= 0x00000002,
+	CFG_BMAC_TX_ALL			= 0x00000003
+} bmac_tx_config_t;
+
+typedef enum bmac_rx_config_e {
+	CFG_BMAC_RX			= 0x00000001,
+	CFG_BMAC_RX_STRIP_PAD		= 0x00000002,
+	CFG_BMAC_RX_STRIP_CRC		= 0x00000004,
+	CFG_BMAC_RX_PROMISCUOUS		= 0x00000008,
+	CFG_BMAC_RX_PROMISCUOUSGROUP	= 0x00000010,
+	CFG_BMAC_RX_HASH_FILTER		= 0x00000020,
+	CFG_BMAC_RX_ADDR_FILTER		= 0x00000040,
+	CFG_BMAC_RX_DISCARD_ON_ERR	= 0x00000080,
+	CFG_BMAC_RX_ALL			= 0x000000FF
+} bmac_rx_config_t;
+
+typedef	enum bmac_xif_config_e {
+	CFG_BMAC_XIF_TX_OUTPUT		= 0x00000001,
+	CFG_BMAC_XIF_LOOPBACK		= 0x00000002,
+	CFG_BMAC_XIF_GMII_MODE		= 0x00000008,
+	CFG_BMAC_XIF_LINKLED		= 0x00000020,
+	CFG_BMAC_XIF_LED_POLARITY	= 0x00000040,
+	CFG_BMAC_XIF_SEL_CLK_25MHZ	= 0x00000080,
+	CFG_BMAC_XIF_ALL		= 0x000000FF
+} bmac_xif_config_t;
+
+
+typedef enum xmac_ipg_e {
+	XGMII_IPG_12_15 = 0,
+	XGMII_IPG_16_19,
+	XGMII_IPG_20_23,
+	MII_GMII_IPG_12,
+	MII_GMII_IPG_13,
+	MII_GMII_IPG_14,
+	MII_GMII_IPG_15,
+	MII_GMII_IPG_16
+} xmac_ipg_t;
+
+typedef	enum xpcs_reg_e {
+	XPCS_REG_CONTROL1,
+	XPCS_REG_STATUS1,
+	XPCS_REG_DEVICE_ID,
+	XPCS_REG_SPEED_ABILITY,
+	XPCS_REG_DEVICE_IN_PKG,
+	XPCS_REG_CONTROL2,
+	XPCS_REG_STATUS2,
+	XPCS_REG_PKG_ID,
+	XPCS_REG_STATUS,
+	XPCS_REG_TEST_CONTROL,
+	XPCS_REG_CONFIG_VENDOR1,
+	XPCS_REG_DIAG_VENDOR2,
+	XPCS_REG_MASK1,
+	XPCS_REG_PACKET_COUNTER,
+	XPCS_REG_TX_STATEMACHINE,
+	XPCS_REG_DESCWERR_COUNTER,
+	XPCS_REG_SYMBOL_ERR_L0_1_COUNTER,
+	XPCS_REG_SYMBOL_ERR_L2_3_COUNTER,
+	XPCS_REG_TRAINING_VECTOR
+} xpcs_reg_t;
+
+#define	IS_XMAC_PORT_NUM_VALID(portn)\
+	((portn == XMAC_PORT_0) || (portn == XMAC_PORT_1))
+
+#define	IS_BMAC_PORT_NUM_VALID(portn)\
+	((portn == BMAC_PORT_0) || (portn == BMAC_PORT_1))
+
+#define	XMAC_REG_WR(handle, portn, reg, val)\
+	NXGE_REG_WR64(handle, XMAC_REG_ADDR((portn), (reg)), (val))
+
+#define	XMAC_REG_RD(handle, portn, reg, val_p)\
+	NXGE_REG_RD64(handle, XMAC_REG_ADDR((portn), (reg)), (val_p))
+
+#define	BMAC_REG_WR(handle, portn, reg, val)\
+	NXGE_REG_WR64(handle, BMAC_REG_ADDR((portn), (reg)), (val))
+
+#define	BMAC_REG_RD(handle, portn, reg, val_p)\
+	NXGE_REG_RD64(handle, BMAC_REG_ADDR((portn), (reg)), (val_p))
+
+#define	PCS_REG_WR(handle, portn, reg, val)\
+	NXGE_REG_WR64(handle, PCS_REG_ADDR((portn), (reg)), (val))
+
+#define	PCS_REG_RD(handle, portn, reg, val_p)\
+	NXGE_REG_RD64(handle, PCS_REG_ADDR((portn), (reg)), (val_p))
+
+#define	XPCS_REG_WR(handle, portn, reg, val)\
+	NXGE_REG_WR64(handle, XPCS_ADDR((portn), (reg)), (val))
+
+#define	XPCS_REG_RD(handle, portn, reg, val_p)\
+	NXGE_REG_RD64(handle, XPCS_ADDR((portn), (reg)), (val_p))
+
+#define	MIF_REG_WR(handle, reg, val)\
+	NXGE_REG_WR64(handle, MIF_ADDR((reg)), (val))
+
+#define	MIF_REG_RD(handle, reg, val_p)\
+	NXGE_REG_RD64(handle, MIF_ADDR((reg)), (val_p))
+
+
+/*
+ * When MIF_REG_RD is called inside a poll loop and if the poll takes
+ * very long time to complete, then each poll will print a rt_show_reg
+ * result on the screen and the rtrace "register show" result may
+ * become too messy to read.  The solution is to call MIF_REG_RD_NO_SHOW
+ * instead of MIF_REG_RD in a polling loop. When COSIM or REG_SHOW is
+ * not defined, this macro is the same as MIF_REG_RD.  When both COSIM
+ * and REG_SHOW are defined, this macro calls NXGE_REG_RD64_NO_SHOW
+ * which does not call rt_show_reg.
+ */
+#if defined(COSIM) && defined(REG_SHOW)
+#define	MIF_REG_RD_NO_SHOW(handle, reg, val_p)\
+	NXGE_REG_RD64_NO_SHOW(handle, MIF_ADDR((reg)), (val_p))
+#else
+	/*	If not COSIM or REG_SHOW, still show */
+#define	MIF_REG_RD_NO_SHOW(handle, reg, val_p)\
+	NXGE_REG_RD64(handle, MIF_ADDR((reg)), (val_p))
+#endif
+
+#define	ESR_REG_WR(handle, reg, val)\
+	NXGE_REG_WR64(handle, ESR_ADDR((reg)), (val))
+
+#define	ESR_REG_RD(handle, reg, val_p)\
+	NXGE_REG_RD64(handle, ESR_ADDR((reg)), (val_p))
+
+/* Macros to read/modify MAC attributes */
+
+#define	SET_MAC_ATTR1(handle, p, portn, attr, val, stat) {\
+	p.type = attr;\
+	p.idata[0] = (uint32_t)val;\
+	stat = npi_mac_port_attr(handle, OP_SET, portn, (npi_attr_t *)&p);\
+}
+
+#define	SET_MAC_ATTR2(handle, p, portn, attr, val0, val1, stat) {\
+	p.type = attr;\
+	p.idata[0] = (uint32_t)val0;\
+	p.idata[1] = (uint32_t)val1;\
+	stat = npi_mac_port_attr(handle, OP_SET, portn, (npi_attr_t *)&p);\
+}
+
+#define	SET_MAC_ATTR3(handle, p, portn, attr, val0, val1, val2, stat) {\
+	p.type = attr;\
+	p.idata[0] = (uint32_t)val0;\
+	p.idata[1] = (uint32_t)val1;\
+	p.idata[2] = (uint32_t)val2;\
+	stat = npi_mac_port_attr(handle, OP_SET, portn, (npi_attr_t *)&p);\
+}
+
+#define	SET_MAC_ATTR4(handle, p, portn, attr, val0, val1, val2, val3, stat) {\
+	p.type = attr;\
+	p.idata[0] = (uint32_t)val0;\
+	p.idata[1] = (uint32_t)val1;\
+	p.idata[2] = (uint32_t)val2;\
+	p.idata[3] = (uint32_t)val3;\
+	stat = npi_mac_port_attr(handle, OP_SET, portn, (npi_attr_t *)&p);\
+}
+
+#define	GET_MAC_ATTR1(handle, p, portn, attr, val, stat) {\
+	p.type = attr;\
+	if ((stat = npi_mac_port_attr(handle, OP_GET, portn, \
+					(npi_attr_t *)&p)) == NPI_SUCCESS) {\
+		val = p.odata[0];\
+	}\
+}
+
+#define	GET_MAC_ATTR2(handle, p, portn, attr, val0, val1, stat) {\
+	p.type = attr;\
+	if ((stat = npi_mac_port_attr(handle, OP_GET, portn, \
+					(npi_attr_t *)&p)) == NPI_SUCCESS) {\
+		val0 = p.odata[0];\
+		val1 = p.odata[1];\
+	}\
+}
+
+#define	GET_MAC_ATTR3(handle, p, portn, attr, val0, val1, \
+			val2, stat) {\
+	p.type = attr;\
+	if ((stat = npi_mac_port_attr(handle, OP_GET, portn, \
+					(npi_attr_t *)&p)) == NPI_SUCCESS) {\
+		val0 = p.odata[0];\
+		val1 = p.odata[1];\
+		val2 = p.odata[2];\
+	}\
+}
+
+#define	GET_MAC_ATTR4(handle, p, portn, attr, val0, val1, \
+			val2, val3, stat) {\
+	p.type = attr;\
+	if ((stat = npi_mac_port_attr(handle, OP_GET, portn, \
+					(npi_attr_t *)&p)) == NPI_SUCCESS) {\
+		val0 = p.odata[0];\
+		val1 = p.odata[1];\
+		val2 = p.odata[2];\
+		val3 = p.odata[3];\
+	}\
+}
+
+/* MAC specific errors */
+
+#define	MAC_PORT_ATTR_INVALID		0x50
+#define	MAC_RESET_MODE_INVALID		0x51
+#define	MAC_HASHTAB_ENTRY_INVALID	0x52
+#define	MAC_HOSTINFO_ENTRY_INVALID	0x53
+#define	MAC_ALT_ADDR_ENTRY_INVALID	0x54
+
+/* MAC error return macros */
+
+#define	NPI_MAC_PORT_INVALID(portn)	((MAC_BLK_ID << NPI_BLOCK_ID_SHIFT) |\
+					PORT_INVALID | IS_PORT | (portn << 12))
+#define	NPI_MAC_OPCODE_INVALID(portn)	((MAC_BLK_ID << NPI_BLOCK_ID_SHIFT) |\
+					OPCODE_INVALID |\
+					IS_PORT | (portn << 12))
+#define	NPI_MAC_HASHTAB_ENTRY_INVALID(portn)\
+					((MAC_BLK_ID << NPI_BLOCK_ID_SHIFT) |\
+					MAC_HASHTAB_ENTRY_INVALID |\
+					IS_PORT | (portn << 12))
+#define	NPI_MAC_HOSTINFO_ENTRY_INVALID(portn)\
+					((MAC_BLK_ID << NPI_BLOCK_ID_SHIFT) |\
+					MAC_HOSTINFO_ENTRY_INVALID |\
+					IS_PORT | (portn << 12))
+#define	NPI_MAC_ALT_ADDR_ENTRY_INVALID(portn)\
+					((MAC_BLK_ID << NPI_BLOCK_ID_SHIFT) |\
+					MAC_ALT_ADDR_ENTRY_INVALID |\
+					IS_PORT | (portn << 12))
+#define	NPI_MAC_PORT_ATTR_INVALID(portn)\
+					((MAC_BLK_ID << NPI_BLOCK_ID_SHIFT) |\
+					MAC_PORT_ATTR_INVALID |\
+					IS_PORT | (portn << 12))
+#define	NPI_MAC_RESET_MODE_INVALID(portn)\
+					((MAC_BLK_ID << NPI_BLOCK_ID_SHIFT) |\
+					MAC_RESET_MODE_INVALID |\
+					IS_PORT | (portn << 12))
+#define	NPI_MAC_PCS_REG_INVALID(portn)	((MAC_BLK_ID << NPI_BLOCK_ID_SHIFT) |\
+					REGISTER_INVALID |\
+					IS_PORT | (portn << 12))
+#define	NPI_TXMAC_RESET_FAILED(portn)	((TXMAC_BLK_ID << NPI_BLOCK_ID_SHIFT) |\
+					RESET_FAILED | IS_PORT | (portn << 12))
+#define	NPI_RXMAC_RESET_FAILED(portn)	((RXMAC_BLK_ID << NPI_BLOCK_ID_SHIFT) |\
+					RESET_FAILED | IS_PORT | (portn << 12))
+#define	NPI_MAC_CONFIG_INVALID(portn)	((MAC_BLK_ID << NPI_BLOCK_ID_SHIFT) |\
+					CONFIG_INVALID |\
+					IS_PORT | (portn << 12))
+#define	NPI_MAC_REG_INVALID(portn)	((MAC_BLK_ID << NPI_BLOCK_ID_SHIFT) |\
+					REGISTER_INVALID |\
+					IS_PORT | (portn << 12))
+#define	NPI_MAC_MII_READ_FAILED(portn)	((MIF_BLK_ID << NPI_BLOCK_ID_SHIFT) |\
+					READ_FAILED | IS_PORT | (portn << 12))
+#define	NPI_MAC_MII_WRITE_FAILED(portn)	((MIF_BLK_ID << NPI_BLOCK_ID_SHIFT) |\
+					WRITE_FAILED | IS_PORT | (portn << 12))
+
+/* library functions prototypes */
+
+/* general mac functions */
+npi_status_t npi_mac_hashtab_entry(npi_handle_t, io_op_t,
+				uint8_t, uint8_t, uint16_t *);
+npi_status_t npi_mac_hostinfo_entry(npi_handle_t, io_op_t,
+				uint8_t, uint8_t,
+				hostinfo_t *);
+npi_status_t npi_mac_altaddr_enable(npi_handle_t, uint8_t,
+				uint8_t);
+npi_status_t npi_mac_altaddr_disble(npi_handle_t, uint8_t,
+				uint8_t);
+npi_status_t npi_mac_altaddr_entry(npi_handle_t, io_op_t,
+				uint8_t, uint8_t,
+				npi_mac_addr_t *);
+npi_status_t npi_mac_port_attr(npi_handle_t, io_op_t, uint8_t,
+				npi_attr_t *);
+npi_status_t npi_mac_get_link_status(npi_handle_t, uint8_t,
+				boolean_t *);
+npi_status_t npi_mac_get_10g_link_status(npi_handle_t, uint8_t,
+				boolean_t *);
+npi_status_t npi_mac_mif_mii_read(npi_handle_t, uint8_t,
+				uint8_t, uint16_t *);
+npi_status_t npi_mac_mif_mii_write(npi_handle_t, uint8_t,
+				uint8_t, uint16_t);
+npi_status_t npi_mac_mif_link_intr_enable(npi_handle_t, uint8_t,
+				uint8_t, uint16_t);
+npi_status_t npi_mac_mif_mdio_read(npi_handle_t, uint8_t,
+				uint8_t, uint16_t,
+				uint16_t *);
+npi_status_t npi_mac_mif_mdio_write(npi_handle_t, uint8_t,
+				uint8_t, uint16_t,
+				uint16_t);
+npi_status_t npi_mac_mif_mdio_link_intr_enable(npi_handle_t,
+				uint8_t, uint8_t,
+				uint16_t, uint16_t);
+npi_status_t npi_mac_mif_link_intr_disable(npi_handle_t, uint8_t);
+npi_status_t npi_mac_pcs_mii_read(npi_handle_t, uint8_t,
+				uint8_t, uint16_t *);
+npi_status_t npi_mac_pcs_mii_write(npi_handle_t, uint8_t,
+				uint8_t, uint16_t);
+npi_status_t npi_mac_pcs_link_intr_enable(npi_handle_t, uint8_t);
+npi_status_t npi_mac_pcs_link_intr_disable(npi_handle_t, uint8_t);
+npi_status_t npi_mac_pcs_reset(npi_handle_t, uint8_t);
+npi_status_t npi_mac_address_filter_enable(npi_handle_t,
+									   uint8_t, boolean_t);
+/* xmac functions */
+npi_status_t npi_xmac_reset(npi_handle_t, uint8_t,
+				npi_mac_reset_t);
+npi_status_t npi_xmac_xif_config(npi_handle_t, config_op_t,
+				uint8_t, xmac_xif_config_t);
+npi_status_t npi_xmac_tx_config(npi_handle_t, config_op_t,
+				uint8_t, xmac_tx_config_t);
+npi_status_t npi_xmac_rx_config(npi_handle_t, config_op_t,
+				uint8_t, xmac_rx_config_t);
+npi_status_t npi_xmac_tx_iconfig(npi_handle_t, config_op_t,
+				uint8_t, xmac_tx_iconfig_t);
+npi_status_t npi_xmac_rx_iconfig(npi_handle_t, config_op_t,
+				uint8_t, xmac_rx_iconfig_t);
+npi_status_t npi_xmac_ctl_iconfig(npi_handle_t, config_op_t,
+				uint8_t, xmac_ctl_iconfig_t);
+npi_status_t npi_xmac_tx_get_istatus(npi_handle_t, uint8_t,
+				xmac_tx_iconfig_t *);
+npi_status_t npi_xmac_rx_get_istatus(npi_handle_t, uint8_t,
+				xmac_rx_iconfig_t *);
+npi_status_t npi_xmac_ctl_get_istatus(npi_handle_t, uint8_t,
+				xmac_ctl_iconfig_t *);
+npi_status_t npi_xmac_xpcs_reset(npi_handle_t, uint8_t);
+npi_status_t npi_xmac_xpcs_enable(npi_handle_t, uint8_t);
+npi_status_t npi_xmac_xpcs_disable(npi_handle_t, uint8_t);
+npi_status_t npi_xmac_xpcs_read(npi_handle_t, uint8_t,
+				uint8_t, uint32_t *);
+npi_status_t npi_xmac_xpcs_write(npi_handle_t, uint8_t,
+				uint8_t, uint32_t);
+npi_status_t npi_xmac_xpcs_link_intr_enable(npi_handle_t, uint8_t);
+npi_status_t npi_xmac_xpcs_link_intr_disable(npi_handle_t,
+				uint8_t);
+npi_status_t npi_xmac_xif_led(npi_handle_t, uint8_t,
+				boolean_t);
+npi_status_t npi_xmac_zap_tx_counters(npi_handle_t, uint8_t);
+npi_status_t npi_xmac_zap_rx_counters(npi_handle_t, uint8_t);
+
+/* bmac functions */
+npi_status_t npi_bmac_reset(npi_handle_t, uint8_t,
+				npi_mac_reset_t mode);
+npi_status_t npi_bmac_tx_config(npi_handle_t, config_op_t,
+				uint8_t, bmac_tx_config_t);
+npi_status_t npi_bmac_rx_config(npi_handle_t, config_op_t,
+				uint8_t, bmac_rx_config_t);
+npi_status_t npi_bmac_rx_iconfig(npi_handle_t, config_op_t,
+				uint8_t, bmac_rx_iconfig_t);
+npi_status_t npi_bmac_xif_config(npi_handle_t, config_op_t,
+				uint8_t, bmac_xif_config_t);
+npi_status_t npi_bmac_tx_iconfig(npi_handle_t, config_op_t,
+				uint8_t, bmac_tx_iconfig_t);
+npi_status_t npi_bmac_ctl_iconfig(npi_handle_t, config_op_t,
+				uint8_t, bmac_ctl_iconfig_t);
+npi_status_t npi_bmac_tx_get_istatus(npi_handle_t, uint8_t,
+				bmac_tx_iconfig_t *);
+npi_status_t npi_bmac_rx_get_istatus(npi_handle_t, uint8_t,
+				bmac_rx_iconfig_t *);
+npi_status_t npi_bmac_ctl_get_istatus(npi_handle_t, uint8_t,
+				bmac_ctl_iconfig_t *);
+npi_status_t npi_bmac_send_pause(npi_handle_t, uint8_t,
+				uint16_t);
+npi_status_t npi_mac_dump_regs(npi_handle_t, uint8_t);
+
+/* MIF common functions */
+void npi_mac_mif_set_indirect_mode(npi_handle_t, boolean_t);
+void npi_mac_mif_set_atca_mode(npi_handle_t, boolean_t);
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif	/* _NPI_MAC_H */
diff --git a/drivers/net/nxge/npi/npi_rxdma.c b/drivers/net/nxge/npi/npi_rxdma.c
new file mode 100644
index 0000000..b6f189d
--- /dev/null
+++ b/drivers/net/nxge/npi/npi_rxdma.c
@@ -0,0 +1,2362 @@
+/*
+ * npi_rxdma.c	Neptune  RX DMA HW API functions
+ *
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#include <npi_rxdma.h>
+#include <nxge_common.h>
+
+#define	 RXDMA_RESET_TRY_COUNT	32
+#define	 RXDMA_RESET_DELAY	10
+
+#define	 RXDMA_OP_DISABLE	0
+#define	 RXDMA_OP_ENABLE	1
+#define	 RXDMA_OP_RESET	2
+
+#define	 RCR_TIMEOUT_ENABLE	1
+#define	 RCR_TIMEOUT_DISABLE	2
+#define	 RCR_THRESHOLD	4
+
+
+
+uint64_t rdc_dmc_offset[] = {
+	RXDMA_CFIG1_REG, RXDMA_CFIG2_REG, RBR_CFIG_A_REG, RBR_CFIG_B_REG,
+	RBR_KICK_REG, RBR_STAT_REG, RBR_HDH_REG, RBR_HDL_REG,
+	RCRCFIG_A_REG, RCRCFIG_B_REG, RCRSTAT_A_REG, RCRSTAT_B_REG,
+	RCRSTAT_C_REG, RX_DMA_ENT_MSK_REG, RX_DMA_CTL_STAT_REG, RCR_FLSH_REG,
+	RXMISC_DISCARD_REG
+};
+
+const char *rdc_dmc_name[] = {
+	"RXDMA_CFIG1", "RXDMA_CFIG2", "RBR_CFIG_A", "RBR_CFIG_B",
+	"RBR_KICK", "RBR_STAT", "RBR_HDH", "RBR_HDL",
+	"RCRCFIG_A", "RCRCFIG_B", "RCRSTAT_A", "RCRSTAT_B",
+	"RCRSTAT_C", "RX_DMA_ENT_MSK", "RX_DMA_CTL_STAT", "RCR_FLSH",
+	"RXMISC_DISCARD"
+};
+
+uint64_t rdc_fzc_offset [] = {
+	RX_LOG_PAGE_VLD_REG, RX_LOG_PAGE_MASK1_REG, RX_LOG_PAGE_VAL1_REG,
+	RX_LOG_PAGE_MASK2_REG, RX_LOG_PAGE_VAL2_REG, RX_LOG_PAGE_RELO1_REG,
+	RX_LOG_PAGE_RELO2_REG, RX_LOG_PAGE_HDL_REG, RDC_RED_PARA_REG,
+	RED_DIS_CNT_REG
+};
+
+
+const char *rdc_fzc_name [] = {
+	"RX_LOG_PAGE_VLD", "RX_LOG_PAGE_MASK1", "RX_LOG_PAGE_VAL1",
+	"RX_LOG_PAGE_MASK2", "RX_LOG_PAGE_VAL2", "RX_LOG_PAGE_RELO1",
+	"RX_LOG_PAGE_RELO2", "RX_LOG_PAGE_HDL", "RDC_RED_PARA", "RED_DIS_CNT"
+};
+
+
+/*
+ * Dump the MEM_ADD register first so all the data registers
+ * will have valid data buffer pointers.
+ */
+uint64_t rx_fzc_offset[] = {
+	RX_DMA_CK_DIV_REG, DEF_PT0_RDC_REG, DEF_PT1_RDC_REG, DEF_PT2_RDC_REG,
+	DEF_PT3_RDC_REG, RX_ADDR_MD_REG, PT_DRR_WT0_REG, PT_DRR_WT1_REG,
+	PT_DRR_WT2_REG, PT_DRR_WT3_REG, PT_USE0_REG, PT_USE1_REG,
+	PT_USE2_REG, PT_USE3_REG, RED_RAN_INIT_REG, RX_ADDR_MD_REG,
+	RDMC_PRE_PAR_ERR_REG, RDMC_SHA_PAR_ERR_REG,
+	RDMC_MEM_DATA4_REG, RDMC_MEM_DATA3_REG, RDMC_MEM_DATA2_REG,
+	RDMC_MEM_DATA1_REG, RDMC_MEM_DATA0_REG,
+	RDMC_MEM_ADDR_REG,
+	RX_CTL_DAT_FIFO_STAT_REG, RX_CTL_DAT_FIFO_MASK_REG,
+	RX_CTL_DAT_FIFO_STAT_DBG_REG,
+	RDMC_TRAINING_VECTOR_REG,
+};
+
+
+const char *rx_fzc_name[] = {
+	"RX_DMA_CK_DIV", "DEF_PT0_RDC", "DEF_PT1_RDC", "DEF_PT2_RDC",
+	"DEF_PT3_RDC", "RX_ADDR_MD", "PT_DRR_WT0", "PT_DRR_WT1",
+	"PT_DRR_WT2", "PT_DRR_WT3", "PT_USE0", "PT_USE1",
+	"PT_USE2", "PT_USE3", "RED_RAN_INIT", "RX_ADDR_MD",
+	"RDMC_PRE_PAR_ERR", "RDMC_SHA_PAR_ERR",
+	"RDMC_MEM_DATA4", "RDMC_MEM_DATA3", "RDMC_MEM_DATA2",
+	"RDMC_MEM_DATA1", "RDMC_MEM_DATA0",
+	"RDMC_MEM_ADDR",
+	"RX_CTL_DAT_FIFO_STAT", "RX_CTL_DAT_FIFO_MASK",
+	"RDMC_TRAINING_VECTOR_REG",
+	"RX_CTL_DAT_FIFO_STAT_DBG_REG"
+};
+
+
+npi_status_t
+npi_rxdma_cfg_rdc_ctl(npi_handle_t handle, uint8_t rdc, uint8_t op);
+npi_status_t
+npi_rxdma_cfg_rdc_rcr_ctl(npi_handle_t handle, uint8_t rdc, uint8_t op,
+				uint16_t param);
+
+
+/*
+ * npi_rxdma_dump_rdc_regs
+ * Dumps the contents of rdc csrs and fzc registers
+ *
+ * Input:
+ *         rdc:      RX DMA number
+ *
+ * return:
+ *     NPI_SUCCESS
+ *     NPI_FAILURE
+ *     NPI_RXDMA_RDC_INVALID
+ *
+ */
+
+npi_status_t
+npi_rxdma_dump_rdc_regs(npi_handle_t handle, uint8_t rdc)
+{
+
+	uint64_t value, offset;
+	int num_regs, i;
+#ifdef NPI_DEBUG
+	extern uint64_t npi_debug_level;
+	uint64_t old_npi_debug_level = npi_debug_level;
+#endif
+	if (!RXDMA_CHANNEL_VALID(rdc)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			    "npi_rxdma_dump_rdc_regs"
+			    " Illegal RDC number %d \n",
+			    rdc));
+		return (NPI_RXDMA_RDC_INVALID);
+	}
+#ifdef NPI_DEBUG
+	npi_debug_level |= DUMP_ALWAYS;
+#endif
+	num_regs = sizeof (rdc_dmc_offset) / sizeof (uint64_t);
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+			    "\nDMC Register Dump for Channel %d\n",
+			    rdc));
+	for (i = 0; i < num_regs; i++) {
+		RXDMA_REG_READ64(handle, rdc_dmc_offset[i], rdc, &value);
+		offset = NXGE_RXDMA_OFFSET(rdc_dmc_offset[i], handle.is_vraddr,
+				rdc);
+		NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+			"%08llx %s\t %08llx \n",
+			offset, rdc_dmc_name[i], value));
+	}
+
+	NPI_DEBUG_MSG((handle.function, DUMP_ALWAYS,
+			    "\nFZC_DMC Register Dump for Channel %d\n",
+			    rdc));
+	num_regs = sizeof (rdc_fzc_offset) / sizeof (uint64_t);
+
+	for (i = 0; i < num_regs; i++) {
+		offset = REG_FZC_RDC_OFFSET(rdc_fzc_offset[i], rdc);
+		NXGE_REG_RD64(handle, offset, &value);
+		NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+				    "%8llx %s\t %8llx \n",
+				    rdc_fzc_offset[i], rdc_fzc_name[i],
+				    value));
+
+	}
+
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+			    "\n Register Dump for Channel %d done\n",
+			    rdc));
+#ifdef NPI_DEBUG
+	npi_debug_level = old_npi_debug_level;
+#endif
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_rxdma_dump_fzc_regs
+ * Dumps the contents of rdc csrs and fzc registers
+ *
+ * Input:
+ *         rdc:      RX DMA number
+ *
+ * return:
+ *     NPI_SUCCESS
+ *     NPI_FAILURE
+ *     NPI_RXDMA_RDC_INVALID
+ *
+ */
+npi_status_t
+npi_rxdma_dump_fzc_regs(npi_handle_t handle)
+{
+
+	uint64_t value;
+	int num_regs, i;
+
+
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+			    "\nFZC_DMC Common Register Dump\n"));
+	num_regs = sizeof (rx_fzc_offset) / sizeof (uint64_t);
+
+	for (i = 0; i < num_regs; i++) {
+		NXGE_REG_RD64(handle, rx_fzc_offset[i], &value);
+		NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+			"0x%08llx %s\t 0x%08llx \n",
+			    rx_fzc_offset[i],
+			rx_fzc_name[i], value));
+	}
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+			    "\n FZC_DMC Register Dump Done \n"));
+
+	return (NPI_SUCCESS);
+}
+
+
+
+/* per rdc config functions */
+
+npi_status_t
+npi_rxdma_cfg_logical_page_disable(npi_handle_t handle, uint8_t rdc,
+				    uint8_t page_num)
+{
+	log_page_vld_t page_vld;
+	uint64_t valid_offset;
+
+	if (!RXDMA_CHANNEL_VALID(rdc)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    "rxdma_cfg_logical_page_disable"
+				    " Illegal RDC number %d \n",
+				    rdc));
+		return (NPI_RXDMA_RDC_INVALID);
+	}
+
+	if (!RXDMA_PAGE_VALID(page_num)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    "rxdma_cfg_logical_page_disable"
+				    " Illegal page number %d \n",
+				    page_num));
+		return (NPI_RXDMA_PAGE_INVALID);
+	}
+
+	valid_offset = REG_FZC_RDC_OFFSET(RX_LOG_PAGE_VLD_REG, rdc);
+	NXGE_REG_RD64(handle, valid_offset, &page_vld.value);
+
+	if (page_num == 0)
+		page_vld.bits.ldw.page0 = 0;
+
+	if (page_num == 1)
+		page_vld.bits.ldw.page1 = 0;
+
+	NXGE_REG_WR64(handle, valid_offset, page_vld.value);
+	return (NPI_SUCCESS);
+
+}
+
+
+npi_status_t
+npi_rxdma_cfg_logical_page(npi_handle_t handle, uint8_t rdc,
+			    dma_log_page_t *pg_cfg)
+{
+	log_page_vld_t page_vld;
+	log_page_mask_t page_mask;
+	log_page_value_t page_value;
+	log_page_relo_t page_reloc;
+	uint64_t value_offset = 0;
+	uint64_t reloc_offset = 0;
+	uint64_t mask_offset = 0;
+	uint64_t valid_offset = 0;
+
+
+	if (!RXDMA_CHANNEL_VALID(rdc)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " rxdma_cfg_logical_page"
+				    " Illegal RDC number %d \n",
+				    rdc));
+		return (NPI_RXDMA_RDC_INVALID);
+	}
+
+	if (!RXDMA_PAGE_VALID(pg_cfg->page_num)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " rxdma_cfg_logical_page"
+				    " Illegal page number %d \n",
+				    pg_cfg->page_num));
+		return (NPI_RXDMA_PAGE_INVALID);
+	}
+
+	valid_offset = REG_FZC_RDC_OFFSET(RX_LOG_PAGE_VLD_REG, rdc);
+	NXGE_REG_RD64(handle, valid_offset, &page_vld.value);
+
+	if (pg_cfg->valid == 0) {
+		if (pg_cfg->page_num == 0)
+			page_vld.bits.ldw.page0 = 0;
+
+		if (pg_cfg->page_num == 1)
+			page_vld.bits.ldw.page1 = 0;
+		NXGE_REG_WR64(handle, valid_offset, page_vld.value);
+		return (NPI_SUCCESS);
+	}
+
+	if (pg_cfg->page_num == 0) {
+		mask_offset = REG_FZC_RDC_OFFSET(RX_LOG_PAGE_MASK1_REG, rdc);
+		value_offset = REG_FZC_RDC_OFFSET(RX_LOG_PAGE_VAL1_REG, rdc);
+		reloc_offset = REG_FZC_RDC_OFFSET(RX_LOG_PAGE_RELO1_REG, rdc);
+		page_vld.bits.ldw.page0 = 1;
+	}
+
+	if (pg_cfg->page_num == 1) {
+		mask_offset = REG_FZC_RDC_OFFSET(RX_LOG_PAGE_MASK2_REG, rdc);
+		value_offset = REG_FZC_RDC_OFFSET(RX_LOG_PAGE_VAL2_REG, rdc);
+		reloc_offset = REG_FZC_RDC_OFFSET(RX_LOG_PAGE_RELO2_REG, rdc);
+		page_vld.bits.ldw.page1 = 1;
+	}
+
+
+	page_vld.bits.ldw.func = pg_cfg->func_num;
+
+	page_mask.value = 0;
+	page_value.value = 0;
+	page_reloc.value = 0;
+
+
+	page_mask.bits.ldw.mask = pg_cfg->mask >> LOG_PAGE_ADDR_SHIFT;
+	page_value.bits.ldw.value = pg_cfg->value >> LOG_PAGE_ADDR_SHIFT;
+	page_reloc.bits.ldw.relo = pg_cfg->reloc >> LOG_PAGE_ADDR_SHIFT;
+
+
+	NXGE_REG_WR64(handle, mask_offset, page_mask.value);
+	NXGE_REG_WR64(handle, value_offset, page_value.value);
+	NXGE_REG_WR64(handle, reloc_offset, page_reloc.value);
+
+
+/* enable the logical page */
+	NXGE_REG_WR64(handle, valid_offset, page_vld.value);
+	return (NPI_SUCCESS);
+}
+
+
+npi_status_t
+npi_rxdma_cfg_logical_page_handle(npi_handle_t handle, uint8_t rdc,
+				    uint64_t page_handle)
+{
+	uint64_t offset;
+	log_page_hdl_t page_hdl;
+
+	if (!RXDMA_CHANNEL_VALID(rdc)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+		    "rxdma_cfg_logical_page_handle"
+		    " Illegal RDC number %d \n", rdc));
+		return (NPI_RXDMA_RDC_INVALID);
+	}
+
+
+	page_hdl.value = 0;
+
+	page_hdl.bits.ldw.handle = (uint32_t)page_handle;
+	offset = REG_FZC_RDC_OFFSET(RX_LOG_PAGE_HDL_REG, rdc);
+	NXGE_REG_WR64(handle, offset, page_hdl.value);
+
+	return (NPI_SUCCESS);
+}
+
+
+
+/* RX DMA functions */
+
+npi_status_t
+npi_rxdma_cfg_rdc_ctl(npi_handle_t handle, uint8_t rdc, uint8_t op)
+{
+
+	rxdma_cfig1_t cfg;
+	uint32_t count = RXDMA_RESET_TRY_COUNT;
+	uint32_t delay_time = RXDMA_RESET_DELAY;
+	uint32_t error = NPI_RXDMA_ERROR_ENCODE(NPI_RXDMA_RESET_ERR, rdc);
+
+	if (!RXDMA_CHANNEL_VALID(rdc)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    "npi_rxdma_cfg_rdc_ctl"
+				    " Illegal RDC number %d \n", rdc));
+		return (NPI_RXDMA_RDC_INVALID);
+	}
+
+
+	switch (op) {
+		case RXDMA_OP_ENABLE:
+			RXDMA_REG_READ64(handle, RXDMA_CFIG1_REG, rdc,
+						&cfg.value);
+			cfg.bits.ldw.en = 1;
+			RXDMA_REG_WRITE64(handle, RXDMA_CFIG1_REG,
+					    rdc, cfg.value);
+
+			NXGE_DELAY(delay_time);
+			RXDMA_REG_READ64(handle, RXDMA_CFIG1_REG, rdc,
+						&cfg.value);
+			while ((count--) && (cfg.bits.ldw.qst == 0)) {
+				NXGE_DELAY(delay_time);
+				RXDMA_REG_READ64(handle, RXDMA_CFIG1_REG, rdc,
+						&cfg.value);
+			}
+
+			if (cfg.bits.ldw.qst == 0) {
+				NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_rxdma_cfg_rdc_ctl"
+				    " RXDMA_OP_ENABLE Failed for RDC %d \n",
+				    rdc));
+				return (error);
+			}
+
+			break;
+		case RXDMA_OP_DISABLE:
+			RXDMA_REG_READ64(handle, RXDMA_CFIG1_REG, rdc,
+						&cfg.value);
+			cfg.bits.ldw.en = 0;
+			RXDMA_REG_WRITE64(handle, RXDMA_CFIG1_REG,
+					    rdc, cfg.value);
+
+			NXGE_DELAY(delay_time);
+			RXDMA_REG_READ64(handle, RXDMA_CFIG1_REG, rdc,
+						&cfg.value);
+			while ((count--) && (cfg.bits.ldw.qst == 0)) {
+				NXGE_DELAY(delay_time);
+				RXDMA_REG_READ64(handle, RXDMA_CFIG1_REG, rdc,
+						&cfg.value);
+			}
+			if (cfg.bits.ldw.qst == 0) {
+				NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_rxdma_cfg_rdc_ctl"
+				    " RXDMA_OP_DISABLE Failed for RDC %d \n",
+				    rdc));
+				return (error);
+			}
+
+			break;
+		case RXDMA_OP_RESET:
+			cfg.value = 0;
+			cfg.bits.ldw.rst = 1;
+			RXDMA_REG_WRITE64(handle,
+					    RXDMA_CFIG1_REG,
+					    rdc, cfg.value);
+			NXGE_DELAY(delay_time);
+			RXDMA_REG_READ64(handle, RXDMA_CFIG1_REG, rdc,
+						&cfg.value);
+			while ((count--) && (cfg.bits.ldw.rst)) {
+				NXGE_DELAY(delay_time);
+				RXDMA_REG_READ64(handle, RXDMA_CFIG1_REG, rdc,
+						&cfg.value);
+			}
+			if (count == 0) {
+				NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_rxdma_cfg_rdc_ctl"
+					    " Reset Failed for RDC %d \n",
+					    rdc));
+				return (error);
+			}
+			break;
+		default:
+			return (NPI_RXDMA_SW_PARAM_ERROR);
+	}
+
+	return (NPI_SUCCESS);
+}
+
+
+
+npi_status_t
+npi_rxdma_cfg_rdc_enable(npi_handle_t handle, uint8_t rdc)
+{
+	return (npi_rxdma_cfg_rdc_ctl(handle, rdc, RXDMA_OP_ENABLE));
+}
+
+
+
+npi_status_t
+npi_rxdma_cfg_rdc_disable(npi_handle_t handle, uint8_t rdc)
+{
+	return (npi_rxdma_cfg_rdc_ctl(handle, rdc, RXDMA_OP_DISABLE));
+}
+
+npi_status_t
+npi_rxdma_cfg_rdc_reset(npi_handle_t handle, uint8_t rdc)
+{
+	return (npi_rxdma_cfg_rdc_ctl(handle, rdc, RXDMA_OP_RESET));
+}
+
+
+
+
+
+/*
+ * npi_rxdma_cfg_defualt_port_rdc()
+ * Set the default rdc for the port
+ *
+ * Inputs:
+ *	handle:		register handle interpreted by the underlying OS
+ *	portnm:		Physical Port Number
+ *	rdc:	RX DMA Channel number
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_RXDMA_RDC_INVALID
+ * NPI_RXDMA_PORT_INVALID
+ *
+ */
+
+npi_status_t npi_rxdma_cfg_default_port_rdc(npi_handle_t handle,
+				    uint8_t portnm, uint8_t rdc)
+{
+
+	uint64_t offset;
+	def_pt_rdc_t cfg;
+
+	if (!RXDMA_CHANNEL_VALID(rdc)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    "rxdma_cfg_default_port_rdc"
+				    " Illegal RDC number %d \n",
+				    rdc));
+		return (NPI_RXDMA_RDC_INVALID);
+	}
+
+	if (!RXDMA_PORT_VALID(portnm)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    "rxdma_cfg_default_port_rdc"
+				    " Illegal Port number %d \n",
+				    portnm));
+		return (NPI_RXDMA_PORT_INVALID);
+	}
+
+	offset = DEF_PT_RDC_REG(portnm);
+	cfg.value = 0;
+	cfg.bits.ldw.rdc = rdc;
+	NXGE_REG_WR64(handle, offset, cfg.value);
+	return (NPI_SUCCESS);
+}
+
+
+
+
+npi_status_t
+npi_rxdma_cfg_rdc_rcr_ctl(npi_handle_t handle, uint8_t rdc,
+			    uint8_t op, uint16_t param)
+{
+	rcrcfig_b_t rcr_cfgb;
+
+	if (!RXDMA_CHANNEL_VALID(rdc)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    "rxdma_cfg_rdc_rcr_ctl"
+				    " Illegal RDC number %d \n",
+				    rdc));
+		return (NPI_RXDMA_RDC_INVALID);
+	}
+
+
+	RXDMA_REG_READ64(handle, RCRCFIG_B_REG, rdc, &rcr_cfgb.value);
+
+	switch (op) {
+		case RCR_TIMEOUT_ENABLE:
+			rcr_cfgb.bits.ldw.timeout = (uint8_t)param;
+			rcr_cfgb.bits.ldw.entout = 1;
+			break;
+
+		case RCR_THRESHOLD:
+			rcr_cfgb.bits.ldw.pthres = param;
+			break;
+
+		case RCR_TIMEOUT_DISABLE:
+			rcr_cfgb.bits.ldw.entout = 0;
+			break;
+
+		default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    "rxdma_cfg_rdc_rcr_ctl"
+				    " Illegal opcode %x \n",
+				    op));
+		return (NPI_RXDMA_OPCODE_INVALID(rdc));
+	}
+
+	RXDMA_REG_WRITE64(handle, RCRCFIG_B_REG, rdc, rcr_cfgb.value);
+	return (NPI_SUCCESS);
+}
+
+
+npi_status_t
+npi_rxdma_cfg_rdc_rcr_timeout_disable(npi_handle_t handle, uint8_t rdc)
+{
+	return (npi_rxdma_cfg_rdc_rcr_ctl(handle, rdc,
+	    RCR_TIMEOUT_DISABLE, 0));
+}
+
+
+
+npi_status_t
+npi_rxdma_cfg_rdc_rcr_threshold(npi_handle_t handle, uint8_t rdc,
+				    uint16_t rcr_threshold)
+{
+	return (npi_rxdma_cfg_rdc_rcr_ctl(handle, rdc,
+	    RCR_THRESHOLD, rcr_threshold));
+
+}
+
+npi_status_t
+npi_rxdma_cfg_rdc_rcr_timeout(npi_handle_t handle, uint8_t rdc,
+			    uint8_t rcr_timeout)
+{
+	return (npi_rxdma_cfg_rdc_rcr_ctl(handle, rdc,
+	    RCR_TIMEOUT_ENABLE, rcr_timeout));
+
+}
+
+
+
+/*
+ * npi_rxdma_cfg_rdc_ring()
+ * Configure The RDC channel Rcv Buffer Ring
+ *
+ * Inputs:
+ *	rdc:		RX DMA Channel number
+ *	rdc_params:	RDC confiuration parameters
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_SW_ERR
+ * NPI_HW_ERR
+ *
+ */
+
+npi_status_t
+npi_rxdma_cfg_rdc_ring(npi_handle_t handle, uint8_t rdc,
+			    rdc_desc_cfg_t *rdc_desc_cfg)
+{
+	rbr_cfig_a_t cfga;
+	rbr_cfig_b_t cfgb;
+	rxdma_cfig1_t cfg1;
+	rxdma_cfig2_t cfg2;
+	rcrcfig_a_t rcr_cfga;
+	rcrcfig_b_t rcr_cfgb;
+
+	if (!RXDMA_CHANNEL_VALID(rdc)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    "rxdma_cfg_rdc_ring"
+				    " Illegal RDC number %d \n",
+				    rdc));
+		return (NPI_RXDMA_RDC_INVALID);
+	}
+
+
+	cfga.value = 0;
+	cfgb.value = 0;
+	cfg1.value = 0;
+	cfg2.value = 0;
+
+	if (rdc_desc_cfg->mbox_enable == 1) {
+		cfg1.bits.ldw.mbaddr_h =
+		    (rdc_desc_cfg->mbox_addr >> 32) & 0xfff;
+		cfg2.bits.ldw.mbaddr =
+		    ((rdc_desc_cfg->mbox_addr &
+			    RXDMA_CFIG2_MBADDR_L_MASK) >>
+			    RXDMA_CFIG2_MBADDR_L_SHIFT);
+
+
+		/*
+		 * Only after all the configurations are set, then
+		 * enable the RDC or else configuration fatal error
+		 * will be returned (especially if the Hypervisor
+		 * set up the logical pages with non-zero values.
+		 * This NPI function only sets up the configuration.
+		 * Call the enable function to enable the RDMC!
+		 */
+	}
+
+
+	if (rdc_desc_cfg->full_hdr == 1)
+		cfg2.bits.ldw.full_hdr = 1;
+
+	if (RXDMA_BUFF_OFFSET_VALID(rdc_desc_cfg->offset)) {
+		cfg2.bits.ldw.offset = rdc_desc_cfg->offset;
+	} else {
+		cfg2.bits.ldw.offset = SW_OFFSET_NO_OFFSET;
+	}
+
+		/* rbr config */
+
+	cfga.value = (rdc_desc_cfg->rbr_addr & (RBR_CFIG_A_STDADDR_MASK |
+					    RBR_CFIG_A_STDADDR_BASE_MASK));
+
+		/* TODO: check if the rbr queue length is valid */
+	if ((rdc_desc_cfg->rbr_len < RBR_DEFAULT_MIN_LEN) ||
+		    (rdc_desc_cfg->rbr_len > RBR_DEFAULT_MAX_LEN)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    "npi_rxdma_cfg_rdc_ring"
+				    " Illegal RBR Queue Length %d \n",
+				    rdc_desc_cfg->rbr_len));
+		return (NPI_RXDMA_ERROR_ENCODE(NPI_RXDMA_RBRSZIE_INVALID, rdc));
+	}
+
+
+	cfga.bits.hdw.len = rdc_desc_cfg->rbr_len;
+	NPI_DEBUG_MSG((handle.function, NPI_RDC_CTL,
+		"npi_rxdma_cfg_rdc_ring"
+		" CFGA 0x%llx hdw.len %d (RBR LEN %d)\n",
+		cfga.value, cfga.bits.hdw.len,
+		rdc_desc_cfg->rbr_len));
+
+	if (rdc_desc_cfg->page_size == SIZE_4KB)
+		cfgb.bits.ldw.bksize = RBR_BKSIZE_4K;
+	else if (rdc_desc_cfg->page_size == SIZE_8KB)
+		cfgb.bits.ldw.bksize = RBR_BKSIZE_8K;
+	else if (rdc_desc_cfg->page_size == SIZE_16KB)
+		cfgb.bits.ldw.bksize = RBR_BKSIZE_16K;
+	else if (rdc_desc_cfg->page_size == SIZE_32KB)
+		cfgb.bits.ldw.bksize = RBR_BKSIZE_32K;
+	else {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			    "rxdma_cfg_rdc_ring"
+			    " blksize: Illegal buffer size %d \n",
+			    rdc_desc_cfg->page_size));
+		return (NPI_RXDMA_BUFSZIE_INVALID);
+	}
+
+	if (rdc_desc_cfg->valid0) {
+
+		if (rdc_desc_cfg->size0 == SIZE_256B)
+			cfgb.bits.ldw.bufsz0 = RBR_BUFSZ0_256B;
+		else if (rdc_desc_cfg->size0 == SIZE_512B)
+			cfgb.bits.ldw.bufsz0 = RBR_BUFSZ0_512B;
+		else if (rdc_desc_cfg->size0 == SIZE_1KB)
+			cfgb.bits.ldw.bufsz0 = RBR_BUFSZ0_1K;
+		else if (rdc_desc_cfg->size0 == SIZE_2KB)
+			cfgb.bits.ldw.bufsz0 = RBR_BUFSZ0_2K;
+		else {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " rxdma_cfg_rdc_ring"
+				    " blksize0: Illegal buffer size %x \n",
+				    rdc_desc_cfg->size0));
+			return (NPI_RXDMA_BUFSZIE_INVALID);
+		}
+		cfgb.bits.ldw.vld0 = 1;
+	} else {
+		cfgb.bits.ldw.vld0 = 0;
+	}
+
+
+	if (rdc_desc_cfg->valid1) {
+		if (rdc_desc_cfg->size1 == SIZE_1KB)
+			cfgb.bits.ldw.bufsz1 = RBR_BUFSZ1_1K;
+		else if (rdc_desc_cfg->size1 == SIZE_2KB)
+			cfgb.bits.ldw.bufsz1 = RBR_BUFSZ1_2K;
+		else if (rdc_desc_cfg->size1 == SIZE_4KB)
+			cfgb.bits.ldw.bufsz1 = RBR_BUFSZ1_4K;
+		else if (rdc_desc_cfg->size1 == SIZE_8KB)
+			cfgb.bits.ldw.bufsz1 = RBR_BUFSZ1_8K;
+		else {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " rxdma_cfg_rdc_ring"
+				    " blksize1: Illegal buffer size %x \n",
+				    rdc_desc_cfg->size1));
+			return (NPI_RXDMA_BUFSZIE_INVALID);
+		}
+		cfgb.bits.ldw.vld1 = 1;
+	} else {
+		cfgb.bits.ldw.vld1 = 0;
+	}
+
+
+	if (rdc_desc_cfg->valid2) {
+		if (rdc_desc_cfg->size2 == SIZE_2KB)
+			cfgb.bits.ldw.bufsz2 = RBR_BUFSZ2_2K;
+		else if (rdc_desc_cfg->size2 == SIZE_4KB)
+			cfgb.bits.ldw.bufsz2 = RBR_BUFSZ2_4K;
+		else if (rdc_desc_cfg->size2 == SIZE_8KB)
+			cfgb.bits.ldw.bufsz2 = RBR_BUFSZ2_8K;
+		else if (rdc_desc_cfg->size2 == SIZE_16KB)
+			cfgb.bits.ldw.bufsz2 = RBR_BUFSZ2_16K;
+		else {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " rxdma_cfg_rdc_ring"
+				    " blksize2: Illegal buffer size %x \n",
+				    rdc_desc_cfg->size2));
+			return (NPI_RXDMA_BUFSZIE_INVALID);
+		}
+		cfgb.bits.ldw.vld2 = 1;
+	} else {
+		cfgb.bits.ldw.vld2 = 0;
+	}
+
+
+	rcr_cfga.value = (rdc_desc_cfg->rcr_addr &
+			    (RCRCFIG_A_STADDR_MASK |
+			    RCRCFIG_A_STADDR_BASE_MASK));
+
+
+	if ((rdc_desc_cfg->rcr_len < RCR_DEFAULT_MIN_LEN) ||
+		    (rdc_desc_cfg->rcr_len > NXGE_RCR_MAX)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			    " rxdma_cfg_rdc_ring"
+			    " Illegal RCR Queue Length %d \n",
+			    rdc_desc_cfg->rcr_len));
+		return (NPI_RXDMA_ERROR_ENCODE(NPI_RXDMA_RCRSZIE_INVALID, rdc));
+	}
+
+	rcr_cfga.bits.hdw.len = rdc_desc_cfg->rcr_len;
+
+
+	rcr_cfgb.value = 0;
+	if (rdc_desc_cfg->rcr_timeout_enable == 1) {
+		/* check if the rcr timeout value is valid */
+
+		if (RXDMA_RCR_TO_VALID(rdc_desc_cfg->rcr_timeout)) {
+			rcr_cfgb.bits.ldw.timeout = rdc_desc_cfg->rcr_timeout;
+			rcr_cfgb.bits.ldw.entout = 1;
+		} else {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " rxdma_cfg_rdc_ring"
+				    " Illegal RCR Timeout value %d \n",
+				    rdc_desc_cfg->rcr_timeout));
+			rcr_cfgb.bits.ldw.entout = 0;
+		}
+	} else {
+		rcr_cfgb.bits.ldw.entout = 0;
+	}
+
+		/* check if the rcr threshold value is valid */
+	if (RXDMA_RCR_THRESH_VALID(rdc_desc_cfg->rcr_threshold)) {
+		rcr_cfgb.bits.ldw.pthres = rdc_desc_cfg->rcr_threshold;
+	} else {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			    " rxdma_cfg_rdc_ring"
+			    " Illegal RCR Threshold value %d \n",
+			    rdc_desc_cfg->rcr_threshold));
+		rcr_cfgb.bits.ldw.pthres = 1;
+	}
+
+		/* now do the actual HW configuration */
+	RXDMA_REG_WRITE64(handle, RXDMA_CFIG1_REG, rdc, cfg1.value);
+	RXDMA_REG_WRITE64(handle, RXDMA_CFIG2_REG, rdc, cfg2.value);
+
+
+	RXDMA_REG_WRITE64(handle, RBR_CFIG_A_REG, rdc, cfga.value);
+	RXDMA_REG_WRITE64(handle, RBR_CFIG_B_REG, rdc, cfgb.value);
+
+	RXDMA_REG_WRITE64(handle, RCRCFIG_A_REG, rdc, rcr_cfga.value);
+	RXDMA_REG_WRITE64(handle, RCRCFIG_B_REG, rdc, rcr_cfgb.value);
+
+	return (NPI_SUCCESS);
+
+}
+
+
+
+/*
+ * npi_rxdma_red_discard_stat_get
+ * Gets the current discrad count due RED
+ * The counter overflow bit is cleared, if it has been set.
+ *
+ * Inputs:
+ *	rdc:		RX DMA Channel number
+ *	rx_disc_cnt_t	Structure to write current RDC discard stat
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_RXDMA_RDC_INVALID
+ *
+ */
+
+npi_status_t
+npi_rxdma_red_discard_stat_get(npi_handle_t handle, uint8_t rdc,
+				    rx_disc_cnt_t *cnt)
+{
+	uint64_t offset;
+
+	if (!RXDMA_CHANNEL_VALID(rdc)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_rxdma_red_discard_stat_get"
+				    " Illegal RDC Number %d \n",
+				    rdc));
+		return (NPI_RXDMA_RDC_INVALID);
+	}
+
+	offset = RDC_RED_RDC_DISC_REG(rdc);
+	NXGE_REG_RD64(handle, offset, &cnt->value);
+	if (cnt->bits.ldw.oflow) {
+		NPI_DEBUG_MSG((handle.function, NPI_ERR_CTL,
+			    " npi_rxdma_red_discard_stat_get"
+			    " Counter overflow for channel %d ",
+			    " ..... clearing \n",
+			    rdc));
+		cnt->bits.ldw.oflow = 0;
+		NXGE_REG_WR64(handle, offset, cnt->value);
+		cnt->bits.ldw.oflow = 1;
+	}
+
+	return (NPI_SUCCESS);
+}
+
+
+/*
+ * npi_rxdma_red_discard_oflow_clear
+ * Clear RED discard counter overflow bit
+ *
+ * Inputs:
+ *	rdc:		RX DMA Channel number
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_RXDMA_RDC_INVALID
+ *
+ */
+
+npi_status_t
+npi_rxdma_red_discard_oflow_clear(npi_handle_t handle, uint8_t rdc)
+
+{
+	uint64_t offset;
+	rx_disc_cnt_t cnt;
+
+	if (!RXDMA_CHANNEL_VALID(rdc)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			    " npi_rxdma_red_discard_oflow_clear"
+			    " Illegal RDC Number %d \n",
+			    rdc));
+		return (NPI_RXDMA_RDC_INVALID);
+	}
+
+	offset = RDC_RED_RDC_DISC_REG(rdc);
+	NXGE_REG_RD64(handle, offset, &cnt.value);
+	if (cnt.bits.ldw.oflow) {
+		NPI_DEBUG_MSG((handle.function, NPI_RDC_CTL,
+			    " npi_rxdma_red_discard_oflow_clear"
+			    " Counter overflow for channel %d ",
+			    " ..... clearing \n",
+			    rdc));
+		cnt.bits.ldw.oflow = 0;
+		NXGE_REG_WR64(handle, offset, cnt.value);
+	}
+	return (NPI_SUCCESS);
+}
+
+
+
+
+/*
+ * npi_rxdma_misc_discard_stat_get
+ * Gets the current discrad count for the rdc due to
+ * buffer pool empty
+ * The counter overflow bit is cleared, if it has been set.
+ *
+ * Inputs:
+ *	rdc:		RX DMA Channel number
+ *	rx_disc_cnt_t	Structure to write current RDC discard stat
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_RXDMA_RDC_INVALID
+ *
+ */
+
+
+npi_status_t
+npi_rxdma_misc_discard_stat_get(npi_handle_t handle, uint8_t rdc,
+				    rx_disc_cnt_t *cnt)
+{
+
+	if (!RXDMA_CHANNEL_VALID(rdc)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_rxdma_misc_discard_stat_get"
+				    " Illegal RDC Number %d \n",
+				    rdc));
+		return (NPI_RXDMA_RDC_INVALID);
+	}
+
+	RXDMA_REG_READ64(handle, RXMISC_DISCARD_REG, rdc, &cnt->value);
+	if (cnt->bits.ldw.oflow) {
+		NPI_DEBUG_MSG((handle.function, NPI_RDC_CTL,
+			    " npi_rxdma_misc_discard_stat_get"
+			    " Counter overflow for channel %d ",
+			    " ..... clearing \n",
+			    rdc));
+		cnt->bits.ldw.oflow = 0;
+		RXDMA_REG_WRITE64(handle, RXMISC_DISCARD_REG, rdc, cnt->value);
+		cnt->bits.ldw.oflow = 1;
+	}
+
+	return (NPI_SUCCESS);
+}
+
+
+
+/*
+ * npi_rxdma_red_discard_oflow_clear
+ * Clear RED discard counter overflow bit
+ * clear the overflow bit for  buffer pool empty discrad counter
+ * for the rdc
+ *
+ *
+ * Inputs:
+ *	rdc:		RX DMA Channel number
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_RXDMA_RDC_INVALID
+ *
+ */
+
+npi_status_t
+npi_rxdma_misc_discard_oflow_clear(npi_handle_t handle, uint8_t rdc)
+{
+	rx_disc_cnt_t cnt;
+
+	if (!RXDMA_CHANNEL_VALID(rdc)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			    " npi_rxdma_misc_discard_oflow_clear"
+			    " Illegal RDC Number %d \n",
+			    rdc));
+		return (NPI_RXDMA_RDC_INVALID);
+	}
+
+	RXDMA_REG_READ64(handle, RXMISC_DISCARD_REG, rdc, &cnt.value);
+	if (cnt.bits.ldw.oflow) {
+		NPI_DEBUG_MSG((handle.function, NPI_RDC_CTL,
+			    " npi_rxdma_misc_discard_oflow_clear"
+			    " Counter overflow for channel %d ",
+			    " ..... clearing \n",
+			    rdc));
+		cnt.bits.ldw.oflow = 0;
+		RXDMA_REG_WRITE64(handle, RXMISC_DISCARD_REG, rdc, cnt.value);
+	}
+
+	return (NPI_SUCCESS);
+}
+
+
+/*
+ * npi_rxdma_ring_perr_stat_get
+ * Gets the current RDC Memory parity error
+ * The counter overflow bit is cleared, if it has been set.
+ *
+ * Inputs:
+ * pre_log:	Structure to write current RDC Prefetch memory
+ *		Parity Error stat
+ * sha_log:	Structure to write current RDC Shadow memory
+ *		Parity Error stat
+ *
+ * Return:
+ * NPI_SUCCESS
+ *
+ */
+
+npi_status_t
+npi_rxdma_ring_perr_stat_get(npi_handle_t handle,
+			    rdmc_par_err_log_t *pre_log,
+			    rdmc_par_err_log_t *sha_log)
+{
+	uint64_t pre_offset, sha_offset;
+	rdmc_par_err_log_t clr;
+	int clr_bits = 0;
+
+	pre_offset = RDMC_PRE_PAR_ERR_REG;
+	sha_offset = RDMC_SHA_PAR_ERR_REG;
+	NXGE_REG_RD64(handle, pre_offset, &pre_log->value);
+	NXGE_REG_RD64(handle, sha_offset, &sha_log->value);
+
+	clr.value = pre_log->value;
+	if (pre_log->bits.ldw.err) {
+		NPI_DEBUG_MSG((handle.function, NPI_RDC_CTL,
+			    " npi_rxdma_ring_perr_stat_get"
+			    " PRE ERR Bit set ..... clearing \n"));
+		clr.bits.ldw.err = 0;
+		clr_bits++;
+	}
+
+	if (pre_log->bits.ldw.merr) {
+		NPI_DEBUG_MSG((handle.function, NPI_RDC_CTL,
+			    " npi_rxdma_ring_perr_stat_get"
+			    " PRE MERR Bit set ..... clearing \n"));
+		clr.bits.ldw.merr = 0;
+		clr_bits++;
+	}
+
+	if (clr_bits) {
+		NXGE_REG_WR64(handle, pre_offset, clr.value);
+	}
+
+	clr_bits = 0;
+	clr.value = sha_log->value;
+	if (sha_log->bits.ldw.err) {
+		NPI_DEBUG_MSG((handle.function, NPI_RDC_CTL,
+			    " npi_rxdma_ring_perr_stat_get"
+			    " SHA ERR Bit set ..... clearing \n"));
+		clr.bits.ldw.err = 0;
+		clr_bits++;
+	}
+
+	if (sha_log->bits.ldw.merr) {
+		NPI_DEBUG_MSG((handle.function, NPI_RDC_CTL,
+			    " npi_rxdma_ring_perr_stat_get"
+			    " SHA MERR Bit set ..... clearing \n"));
+		clr.bits.ldw.merr = 0;
+		clr_bits++;
+	}
+
+	if (clr_bits) {
+		NXGE_REG_WR64(handle, sha_offset, clr.value);
+	}
+
+	return (NPI_SUCCESS);
+}
+
+
+/*
+ * npi_rxdma_ring_perr_stat_clear
+ * Clear RDC Memory Parity Error counter overflow bits
+ *
+ * Inputs:
+ * Return:
+ * NPI_SUCCESS
+ *
+ */
+
+npi_status_t
+npi_rxdma_ring_perr_stat_clear(npi_handle_t handle)
+{
+	uint64_t pre_offset, sha_offset;
+	rdmc_par_err_log_t clr;
+	int clr_bits = 0;
+	pre_offset = RDMC_PRE_PAR_ERR_REG;
+	sha_offset = RDMC_SHA_PAR_ERR_REG;
+
+	NXGE_REG_RD64(handle, pre_offset, &clr.value);
+
+	if (clr.bits.ldw.err) {
+		NPI_DEBUG_MSG((handle.function, NPI_RDC_CTL,
+			    " npi_rxdma_ring_perr_stat_get"
+			    " PRE ERR Bit set ..... clearing \n"));
+		clr.bits.ldw.err = 0;
+		clr_bits++;
+	}
+
+	if (clr.bits.ldw.merr) {
+		NPI_DEBUG_MSG((handle.function, NPI_RDC_CTL,
+			    " npi_rxdma_ring_perr_stat_get"
+			    " PRE MERR Bit set ..... clearing \n"));
+		clr.bits.ldw.merr = 0;
+		clr_bits++;
+	}
+
+	if (clr_bits) {
+		NXGE_REG_WR64(handle, pre_offset, clr.value);
+	}
+
+	clr_bits = 0;
+	NXGE_REG_RD64(handle, sha_offset, &clr.value);
+	if (clr.bits.ldw.err) {
+		NPI_DEBUG_MSG((handle.function, NPI_RDC_CTL,
+			    " npi_rxdma_ring_perr_stat_get"
+			    " SHA ERR Bit set ..... clearing \n"));
+		clr.bits.ldw.err = 0;
+		clr_bits++;
+	}
+
+	if (clr.bits.ldw.merr) {
+		NPI_DEBUG_MSG((handle.function, NPI_RDC_CTL,
+			    " npi_rxdma_ring_perr_stat_get"
+			    " SHA MERR Bit set ..... clearing \n"));
+		clr.bits.ldw.merr = 0;
+		clr_bits++;
+	}
+
+	if (clr_bits) {
+		NXGE_REG_WR64(handle, sha_offset, clr.value);
+	}
+
+	return (NPI_SUCCESS);
+}
+
+
+/* Access the RDMC Memory: used for debugging */
+npi_status_t
+npi_rxdma_rdmc_memory_io(npi_handle_t handle,
+			    rdmc_mem_access_t *data, uint8_t op)
+{
+	uint64_t d0_offset, d1_offset, d2_offset, d3_offset, d4_offset;
+	uint64_t addr_offset;
+	rdmc_mem_addr_t addr;
+	rdmc_mem_data_t d0, d1, d2, d3, d4;
+	d0.value = 0;
+	d1.value = 0;
+	d2.value = 0;
+	d3.value = 0;
+	d4.value = 0;
+	addr.value = 0;
+
+
+	if ((data->location != RDMC_MEM_ADDR_PREFETCH) &&
+		    (data->location != RDMC_MEM_ADDR_SHADOW)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			    " npi_rxdma_rdmc_memory_io"
+			    " Illegal memory Type %x \n",
+			    data->location));
+		return (NPI_RXDMA_OPCODE_INVALID(0));
+	}
+
+	addr_offset = RDMC_MEM_ADDR_REG;
+	addr.bits.ldw.addr = data->addr;
+	addr.bits.ldw.pre_shad = data->location;
+
+	d0_offset = RDMC_MEM_DATA0_REG;
+	d1_offset = RDMC_MEM_DATA1_REG;
+	d2_offset = RDMC_MEM_DATA2_REG;
+	d3_offset = RDMC_MEM_DATA3_REG;
+	d4_offset = RDMC_MEM_DATA4_REG;
+
+
+	if (op == RDMC_MEM_WRITE) {
+		d0.bits.ldw.data = data->data[0];
+		d1.bits.ldw.data = data->data[1];
+		d2.bits.ldw.data = data->data[2];
+		d3.bits.ldw.data = data->data[3];
+		d4.bits.ldw.data = data->data[4];
+		NXGE_REG_WR64(handle, addr_offset, addr.value);
+		NXGE_REG_WR64(handle, d0_offset, d0.value);
+		NXGE_REG_WR64(handle, d1_offset, d1.value);
+		NXGE_REG_WR64(handle, d2_offset, d2.value);
+		NXGE_REG_WR64(handle, d3_offset, d3.value);
+		NXGE_REG_WR64(handle, d4_offset, d4.value);
+	}
+
+	if (op == RDMC_MEM_READ) {
+		NXGE_REG_WR64(handle, addr_offset, addr.value);
+		NXGE_REG_RD64(handle, d4_offset, &d4.value);
+		NXGE_REG_RD64(handle, d3_offset, &d3.value);
+		NXGE_REG_RD64(handle, d2_offset, &d2.value);
+		NXGE_REG_RD64(handle, d1_offset, &d1.value);
+		NXGE_REG_RD64(handle, d0_offset, &d0.value);
+
+		data->data[0] = d0.bits.ldw.data;
+		data->data[1] = d1.bits.ldw.data;
+		data->data[2] = d2.bits.ldw.data;
+		data->data[3] = d3.bits.ldw.data;
+		data->data[4] = d4.bits.ldw.data;
+	} else {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			    " npi_rxdma_rdmc_memory_io"
+			    " Illegal opcode %x \n",
+			    op));
+		return (NPI_RXDMA_OPCODE_INVALID(0));
+
+	}
+
+	return (NPI_SUCCESS);
+}
+
+/* system wide conf functions */
+
+npi_status_t
+npi_rxdma_cfg_clock_div_set(npi_handle_t handle, uint16_t count)
+{
+	uint64_t offset;
+	rx_dma_ck_div_t clk_div;
+
+	offset = RX_DMA_CK_DIV_REG;
+
+	clk_div.value = 0;
+	clk_div.bits.ldw.cnt = count;
+	NPI_DEBUG_MSG((handle.function, NPI_RDC_CTL,
+		    " npi_rxdma_cfg_clock_div_set: add 0x%llx "
+		    "handle 0x%llx value 0x%llx",
+		    handle.regp, handle.regh, clk_div.value));
+
+	NXGE_REG_WR64(handle, offset, clk_div.value);
+
+	return (NPI_SUCCESS);
+}
+
+
+
+
+npi_status_t
+npi_rxdma_cfg_red_rand_init(npi_handle_t handle, uint16_t init_value)
+{
+	uint64_t offset;
+	red_ran_init_t rand_reg;
+
+	offset = RED_RAN_INIT_REG;
+
+	rand_reg.value = 0;
+	rand_reg.bits.ldw.init = init_value;
+	rand_reg.bits.ldw.enable = 1;
+	NXGE_REG_WR64(handle, offset, rand_reg.value);
+
+	return (NPI_SUCCESS);
+
+}
+
+npi_status_t
+npi_rxdma_cfg_red_rand_disable(npi_handle_t handle)
+{
+	uint64_t offset;
+	red_ran_init_t rand_reg;
+
+	offset = RED_RAN_INIT_REG;
+
+	NXGE_REG_RD64(handle, offset, &rand_reg.value);
+	rand_reg.bits.ldw.enable = 0;
+	NXGE_REG_WR64(handle, offset, rand_reg.value);
+
+	return (NPI_SUCCESS);
+
+}
+
+
+
+npi_status_t
+npi_rxdma_cfg_32bitmode_enable(npi_handle_t handle)
+{
+	uint64_t offset;
+	rx_addr_md_t md_reg;
+	offset = RX_ADDR_MD_REG;
+	md_reg.value = 0;
+	md_reg.bits.ldw.mode32 = 1;
+
+	NXGE_REG_WR64(handle, offset, md_reg.value);
+	return (NPI_SUCCESS);
+
+}
+
+
+
+npi_status_t
+npi_rxdma_cfg_32bitmode_disable(npi_handle_t handle)
+{
+	uint64_t offset;
+	rx_addr_md_t md_reg;
+	offset = RX_ADDR_MD_REG;
+	md_reg.value = 0;
+
+	NXGE_REG_WR64(handle, offset, md_reg.value);
+	return (NPI_SUCCESS);
+
+}
+
+
+npi_status_t
+npi_rxdma_cfg_ram_access_enable(npi_handle_t handle)
+{
+	uint64_t offset;
+	rx_addr_md_t md_reg;
+	offset = RX_ADDR_MD_REG;
+	NXGE_REG_RD64(handle, offset, &md_reg.value);
+	md_reg.bits.ldw.ram_acc = 1;
+	NXGE_REG_WR64(handle, offset, md_reg.value);
+	return (NPI_SUCCESS);
+
+}
+
+
+
+npi_status_t
+npi_rxdma_cfg_ram_access_disable(npi_handle_t handle)
+{
+	uint64_t offset;
+	rx_addr_md_t md_reg;
+	offset = RX_ADDR_MD_REG;
+	NXGE_REG_RD64(handle, offset, &md_reg.value);
+	md_reg.bits.ldw.ram_acc = 0;
+	NXGE_REG_WR64(handle, offset, md_reg.value);
+	return (NPI_SUCCESS);
+
+}
+
+
+
+#define	WEIGHT_FACTOR 3/2
+/* assume weight is in byte frames unit */
+
+npi_status_t npi_rxdma_cfg_port_ddr_weight(npi_handle_t handle,
+				    uint8_t portnm, uint32_t weight)
+{
+
+	pt_drr_wt_t wt_reg;
+	uint64_t offset;
+	if (!RXDMA_PORT_VALID(portnm)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			    " rxdma_cfg_port_ddr_weight"
+			    " Illegal Port Number %d \n",
+			    portnm));
+		return (NPI_RXDMA_PORT_INVALID);
+	}
+
+	offset = PT_DRR_WT_REG(portnm);
+	wt_reg.value = 0;
+	wt_reg.bits.ldw.wt = weight;
+	NXGE_REG_WR64(handle, offset, wt_reg.value);
+	return (NPI_SUCCESS);
+}
+
+
+npi_status_t npi_rxdma_port_usage_get(npi_handle_t handle,
+				    uint8_t portnm, uint32_t *blocks)
+{
+
+	pt_use_t use_reg;
+	uint64_t offset;
+
+	if (!RXDMA_PORT_VALID(portnm)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			    " rxdma_port_usage_get"
+			    " Illegal Port Number %d \n",
+			    portnm));
+		return (NPI_RXDMA_PORT_INVALID);
+	}
+
+	offset = PT_USE_REG(portnm);
+	NXGE_REG_RD64(handle, offset, &use_reg.value);
+	*blocks = use_reg.bits.ldw.cnt;
+	return (NPI_SUCCESS);
+
+}
+
+
+
+npi_status_t npi_rxdma_cfg_wred_param(npi_handle_t handle, uint8_t rdc,
+				    rdc_red_para_t *wred_params)
+{
+	rdc_red_para_t wred_reg;
+	uint64_t offset;
+
+	if (!RXDMA_CHANNEL_VALID(rdc)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			    " rxdma_cfg_wred_param"
+			    " Illegal RDC Number %d \n",
+			    rdc));
+		return (NPI_RXDMA_RDC_INVALID);
+	}
+
+	/*
+	 * need to update RDC_RED_PARA_REG as well as bit defs in
+	 * the hw header file
+	 */
+	offset = RDC_RED_RDC_PARA_REG(rdc);
+
+	NPI_DEBUG_MSG((handle.function, NPI_RDC_CTL,
+		" npi_rxdma_cfg_wred_param: "
+		"set RED_PARA: passed value 0x%llx "
+		"win 0x%x thre 0x%x sync 0x%x thre_sync 0x%x",
+		wred_params->value,
+		wred_params->bits.ldw.win,
+		wred_params->bits.ldw.thre,
+		wred_params->bits.ldw.win_syn,
+		wred_params->bits.ldw.thre_sync));
+
+	wred_reg.value = 0;
+	wred_reg.bits.ldw.win = wred_params->bits.ldw.win;
+	wred_reg.bits.ldw.thre = wred_params->bits.ldw.thre;
+	wred_reg.bits.ldw.win_syn = wred_params->bits.ldw.win_syn;
+	wred_reg.bits.ldw.thre_sync = wred_params->bits.ldw.thre_sync;
+	NXGE_REG_WR64(handle, offset, wred_reg.value);
+
+	NPI_DEBUG_MSG((handle.function, NPI_RDC_CTL,
+		"set RED_PARA: value 0x%llx "
+		"win 0x%x thre 0x%x sync 0x%x thre_sync 0x%x",
+		wred_reg.value,
+		wred_reg.bits.ldw.win,
+		wred_reg.bits.ldw.thre,
+		wred_reg.bits.ldw.win_syn,
+		wred_reg.bits.ldw.thre_sync));
+
+	return (NPI_SUCCESS);
+}
+
+
+
+/*
+ * npi_rxdma_cfg_rdc_table()
+ * Configure/populate the RDC table
+ *
+ * Inputs:
+ *	handle:		register handle interpreted by the underlying OS
+ *	table:		RDC Group Number
+ *	rdc[]:	 Array of RX DMA Channels
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_RXDMA_TABLE_INVALID
+ *
+ */
+
+npi_status_t
+npi_rxdma_cfg_rdc_table(npi_handle_t handle,
+			    uint8_t table, uint8_t rdc[])
+{
+	uint64_t offset;
+	int tbl_offset;
+	rdc_tbl_t tbl_reg;
+	tbl_reg.value = 0;
+
+	if (!RXDMA_TABLE_VALID(table)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			    " npi_rxdma_cfg_rdc_table"
+			    " Illegal RDC Rable Number %d \n",
+			    rdc));
+		return (NPI_RXDMA_TABLE_INVALID);
+	}
+
+	offset = REG_RDC_TABLE_OFFSET(table);
+	for (tbl_offset = 0; tbl_offset < NXGE_MAX_RDCS; tbl_offset++) {
+		tbl_reg.bits.ldw.rdc = rdc[tbl_offset];
+		NXGE_REG_WR64(handle, offset, tbl_reg.value);
+		offset += 8;
+	}
+
+	return (NPI_SUCCESS);
+
+}
+
+npi_status_t
+npi_rxdma_cfg_rdc_table_default_rdc(npi_handle_t handle,
+			    uint8_t table, uint8_t rdc)
+{
+	uint64_t offset;
+	rdc_tbl_t tbl_reg;
+	tbl_reg.value = 0;
+
+	if (!RXDMA_TABLE_VALID(table)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			    " npi_rxdma_cfg_rdc_table"
+			    " Illegal RDC table Number %d \n",
+			    rdc));
+		return (NPI_RXDMA_TABLE_INVALID);
+	}
+
+	offset = REG_RDC_TABLE_OFFSET(table);
+	tbl_reg.bits.ldw.rdc = rdc;
+	NXGE_REG_WR64(handle, offset, tbl_reg.value);
+	return (NPI_SUCCESS);
+
+}
+
+npi_status_t
+npi_rxdma_dump_rdc_table(npi_handle_t handle,
+			    uint8_t table)
+{
+	uint64_t offset;
+	int tbl_offset;
+	uint64_t value;
+
+	if (!RXDMA_TABLE_VALID(table)) {
+		NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+			    " npi_rxdma_dump_rdc_table"
+			    " Illegal RDC Rable Number %d \n",
+			    table));
+		return (NPI_RXDMA_TABLE_INVALID);
+	}
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+			    "\n Register Dump for RDC Table %d \n",
+			    table));
+	offset = REG_RDC_TABLE_OFFSET(table);
+	for (tbl_offset = 0; tbl_offset < NXGE_MAX_RDCS; tbl_offset++) {
+		NXGE_REG_RD64(handle, offset, &value);
+		NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+					    " 0x%08llx 0x%08llx \n",
+					    offset, value));
+		offset += 8;
+	}
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+			    "\n Register Dump for RDC Table %d done\n",
+			    table));
+	return (NPI_SUCCESS);
+
+}
+
+
+npi_status_t
+npi_rxdma_rdc_rbr_stat_get(npi_handle_t handle, uint8_t rdc,
+			    rbr_stat_t *rbr_stat)
+{
+
+
+	if (!RXDMA_CHANNEL_VALID(rdc)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			    " rxdma_rdc_rbr_stat_get"
+			    " Illegal RDC Number %d \n",
+			    rdc));
+		return (NPI_RXDMA_RDC_INVALID);
+	}
+
+	RXDMA_REG_READ64(handle, RBR_STAT_REG, rdc, &rbr_stat->value);
+	return (NPI_SUCCESS);
+}
+
+
+
+
+/*
+ * npi_rxdma_rdc_rbr_head_get
+ * Gets the current rbr head pointer.
+ *
+ * Inputs:
+ *	rdc:		RX DMA Channel number
+ *	hdptr		ptr to write the rbr head value
+ *
+ * Return:
+ *
+ */
+
+npi_status_t	npi_rxdma_rdc_rbr_head_get(npi_handle_t handle,
+			    uint8_t rdc, addr44_t *hdptr)
+{
+	rbr_hdh_t hh_ptr;
+	rbr_hdl_t hl_ptr;
+
+
+	if (!RXDMA_CHANNEL_VALID(rdc)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			    " rxdma_rdc_rbr_head_get"
+			    " Illegal RDC Number %d \n",
+			    rdc));
+		return (NPI_RXDMA_RDC_INVALID);
+	}
+	hh_ptr.value = 0;
+	hl_ptr.value = 0;
+	RXDMA_REG_READ64(handle, RBR_HDH_REG, rdc, &hh_ptr.value);
+	RXDMA_REG_READ64(handle, RBR_HDL_REG, rdc, &hl_ptr.value);
+	hdptr->bits.ldw = hl_ptr.bits.ldw.head_l << 2;
+	hdptr->bits.hdw = hh_ptr.bits.ldw.head_h;
+	return (NPI_SUCCESS);
+
+}
+
+
+
+
+npi_status_t
+npi_rxdma_rdc_rcr_qlen_get(npi_handle_t handle, uint8_t rdc,
+			    uint16_t *rcr_qlen)
+{
+
+	rcrstat_a_t stats;
+	if (!RXDMA_CHANNEL_VALID(rdc)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			    " rxdma_rdc_rcr_qlen_get"
+			    " Illegal RDC Number %d \n",
+			    rdc));
+		return (NPI_RXDMA_RDC_INVALID);
+	}
+
+	RXDMA_REG_READ64(handle, RCRSTAT_A_REG, rdc, &stats.value);
+	*rcr_qlen =  stats.bits.ldw.qlen;
+	NPI_DEBUG_MSG((handle.function, NPI_RDC_CTL,
+		    " rxdma_rdc_rcr_qlen_get"
+		    " RDC %d qlen %x qlen %x\n",
+		    rdc, *rcr_qlen, stats.bits.ldw.qlen));
+	return (NPI_SUCCESS);
+}
+
+
+npi_status_t
+npi_rxdma_rdc_rcr_tail_get(npi_handle_t handle,
+			    uint8_t rdc, addr44_t *tail_addr)
+{
+
+	rcrstat_b_t th_ptr;
+	rcrstat_c_t tl_ptr;
+
+	if (!RXDMA_CHANNEL_VALID(rdc)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " rxdma_rdc_rcr_tail_get"
+				    " Illegal RDC Number %d \n",
+				    rdc));
+		return (NPI_RXDMA_RDC_INVALID);
+	}
+	th_ptr.value = 0;
+	tl_ptr.value = 0;
+	RXDMA_REG_READ64(handle, RCRSTAT_B_REG, rdc, &th_ptr.value);
+	RXDMA_REG_READ64(handle, RCRSTAT_C_REG, rdc, &tl_ptr.value);
+	tail_addr->bits.ldw = tl_ptr.bits.ldw.tlptr_l << 3;
+	tail_addr->bits.hdw = th_ptr.bits.ldw.tlptr_h;
+	NPI_DEBUG_MSG((handle.function, NPI_RDC_CTL,
+			    " rxdma_rdc_rcr_tail_get"
+			    " RDC %d rcr_tail %llx tl %x\n",
+			    rdc, tl_ptr.value,
+			    tl_ptr.bits.ldw.tlptr_l));
+
+	return (NPI_SUCCESS);
+
+
+}
+
+
+
+
+/*
+ * npi_rxdma_rxctl_fifo_error_intr_set
+ * Configure The RX ctrl fifo error interrupt generation
+ *
+ * Inputs:
+ *	mask:	rx_ctl_dat_fifo_mask_t specifying the errors
+ * valid fields in  rx_ctl_dat_fifo_mask_t structure are:
+ * zcp_eop_err, ipp_eop_err, id_mismatch. If a field is set
+ * to 1, we will enable interrupt generation for the
+ * corresponding error condition. In the hardware, the bit(s)
+ * have to be cleared to enable interrupt.
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ *
+ */
+npi_status_t
+npi_rxdma_rxctl_fifo_error_intr_set(npi_handle_t handle,
+				    rx_ctl_dat_fifo_mask_t *mask)
+{
+	uint64_t offset;
+	rx_ctl_dat_fifo_mask_t intr_mask;
+	offset = RX_CTL_DAT_FIFO_MASK_REG;
+	NXGE_REG_RD64(handle, offset, &intr_mask.value);
+
+	if (mask->bits.ldw.ipp_eop_err) {
+		intr_mask.bits.ldw.ipp_eop_err = 0;
+	}
+
+	if (mask->bits.ldw.zcp_eop_err) {
+		intr_mask.bits.ldw.zcp_eop_err = 0;
+	}
+
+	if (mask->bits.ldw.id_mismatch) {
+		intr_mask.bits.ldw.id_mismatch = 0;
+	}
+
+	NXGE_REG_WR64(handle, offset, intr_mask.value);
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_rxdma_rxctl_fifo_error_stat_get
+ * Read The RX ctrl fifo error Status
+ *
+ * Inputs:
+ *	stat:	rx_ctl_dat_fifo_stat_t to read the errors to
+ * valid fields in  rx_ctl_dat_fifo_stat_t structure are:
+ * zcp_eop_err, ipp_eop_err, id_mismatch.
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ *
+ */
+npi_status_t
+npi_rxdma_rxctl_fifo_error_intr_get(npi_handle_t handle,
+			    rx_ctl_dat_fifo_stat_t *stat)
+{
+	uint64_t offset = RX_CTL_DAT_FIFO_STAT_REG;
+	NXGE_REG_RD64(handle, offset, &stat->value);
+	return (NPI_SUCCESS);
+}
+
+
+
+npi_status_t
+npi_rxdma_rdc_rcr_pktread_update(npi_handle_t handle, uint8_t channel,
+				    uint16_t pkts_read)
+{
+
+	rx_dma_ctl_stat_t	cs;
+	uint16_t min_read = 0;
+	if (!RXDMA_CHANNEL_VALID(channel)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+		    " npi_rxdma_rdc_rcr_pktread_update ",
+		    " channel %d", channel));
+		return (NPI_FAILURE | NPI_RXDMA_CHANNEL_INVALID(channel));
+	}
+
+	if ((pkts_read < min_read) && (pkts_read > 512)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+		    " npi_rxdma_rdc_rcr_pktread_update ",
+		    " pkts %d out of bound", pkts_read));
+		return (NPI_RXDMA_OPCODE_INVALID(pkts_read));
+	}
+
+	RXDMA_REG_READ64(handle, RX_DMA_CTL_STAT_REG, channel,
+				&cs.value);
+	cs.bits.ldw.pktread = pkts_read;
+	RXDMA_REG_WRITE64(handle, RX_DMA_CTL_STAT_REG,
+				    channel, cs.value);
+
+	return (NPI_SUCCESS);
+}
+
+
+npi_status_t
+npi_rxdma_rdc_rcr_bufread_update(npi_handle_t handle, uint8_t channel,
+					    uint16_t bufs_read)
+{
+
+	rx_dma_ctl_stat_t	cs;
+	uint16_t min_read = 0;
+	if (!RXDMA_CHANNEL_VALID(channel)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+		    " npi_rxdma_rdc_rcr_bufread_update ",
+		    " channel %d", channel));
+		return (NPI_FAILURE | NPI_RXDMA_CHANNEL_INVALID(channel));
+	}
+
+	if ((bufs_read < min_read) && (bufs_read > 512)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+		    " npi_rxdma_rdc_rcr_bufread_update ",
+		    " bufs read %d out of bound", bufs_read));
+		return (NPI_RXDMA_OPCODE_INVALID(bufs_read));
+	}
+
+	RXDMA_REG_READ64(handle, RX_DMA_CTL_STAT_REG, channel,
+				&cs.value);
+	cs.bits.ldw.ptrread = bufs_read;
+	RXDMA_REG_WRITE64(handle, RX_DMA_CTL_STAT_REG,
+				    channel, cs.value);
+
+	return (NPI_SUCCESS);
+}
+
+
+
+npi_status_t
+npi_rxdma_rdc_rcr_read_update(npi_handle_t handle, uint8_t channel,
+				    uint16_t pkts_read, uint16_t bufs_read)
+{
+
+	rx_dma_ctl_stat_t	cs;
+
+	if (!RXDMA_CHANNEL_VALID(channel)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+		    " npi_rxdma_rdc_rcr_read_update ",
+		    " channel %d", channel));
+		return (NPI_FAILURE | NPI_RXDMA_CHANNEL_INVALID(channel));
+	}
+
+	NPI_DEBUG_MSG((handle.function, NPI_RDC_CTL,
+	    " npi_rxdma_rdc_rcr_read_update "
+	    " bufs read %d pkt read %d",
+		bufs_read, pkts_read));
+
+	RXDMA_REG_READ64(handle, RX_DMA_CTL_STAT_REG, channel,
+				&cs.value);
+
+	NPI_DEBUG_MSG((handle.function, NPI_RDC_CTL,
+		" npi_rxdma_rdc_rcr_read_update: "
+		" value: 0x%llx bufs read %d pkt read %d",
+		cs.value,
+		cs.bits.ldw.ptrread, cs.bits.ldw.pktread));
+
+	cs.bits.ldw.pktread = pkts_read;
+	cs.bits.ldw.ptrread = bufs_read;
+
+	RXDMA_REG_WRITE64(handle, RX_DMA_CTL_STAT_REG,
+				    channel, cs.value);
+
+	RXDMA_REG_READ64(handle, RX_DMA_CTL_STAT_REG, channel,
+				&cs.value);
+
+	NPI_DEBUG_MSG((handle.function, NPI_RDC_CTL,
+	    " npi_rxdma_rdc_rcr_read_update: read back after update "
+	    " value: 0x%llx bufs read %d pkt read %d",
+		cs.value,
+		cs.bits.ldw.ptrread, cs.bits.ldw.pktread));
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_rxdma_channel_mex_set():
+ *	This function is called to arm the DMA channel with
+ *	mailbox updating capability. Software needs to rearm
+ *	for each update by writing to the control and status register.
+ *
+ * Parameters:
+ *	handle		- NPI handle (virtualization flag must be defined).
+ *	channel		- logical RXDMA channel from 0 to 23.
+ *			  (If virtualization flag is not set, then
+ *			   logical channel is the same as the hardware
+ *			   channel number).
+ *
+ * Return:
+ *	NPI_SUCCESS		- If enable channel with mailbox update
+ *				  is complete successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE	-
+ *		NPI_RXDMA_CHANNEL_INVALID -
+ */
+npi_status_t
+npi_rxdma_channel_mex_set(npi_handle_t handle, uint8_t channel)
+{
+	return (npi_rxdma_channel_control(handle, RXDMA_MEX_SET, channel));
+}
+
+/*
+ * npi_rxdma_channel_rcrto_clear():
+ *	This function is called to reset RCRTO bit to 0.
+ *
+ * Parameters:
+ *	handle		- NPI handle (virtualization flag must be defined).
+ *	channel		- logical RXDMA channel from 0 to 23.
+ *			  (If virtualization flag is not set, then
+ *			   logical channel is the same as the hardware
+ *			   channel number).
+ * Return:
+ *	NPI_SUCCESS
+ *
+ *	Error:
+ *	NPI_FAILURE	-
+ *		NPI_RXDMA_CHANNEL_INVALID -
+ */
+npi_status_t
+npi_rxdma_channel_rcrto_clear(npi_handle_t handle, uint8_t channel)
+{
+	return (npi_rxdma_channel_control(handle, RXDMA_RCRTO_CLEAR, channel));
+}
+
+/*
+ * npi_rxdma_channel_pt_drop_pkt_clear():
+ *	This function is called to clear the port drop packet bit (debug).
+ *
+ * Parameters:
+ *	handle		- NPI handle (virtualization flag must be defined).
+ *	channel		- logical RXDMA channel from 0 to 23.
+ *			  (If virtualization flag is not set, then
+ *			   logical channel is the same as the hardware
+ *			   channel number).
+ * Return:
+ *	NPI_SUCCESS
+ *
+ *	Error:
+ *	NPI_FAILURE	-
+ *		NPI_RXDMA_CHANNEL_INVALID -
+ */
+npi_status_t
+npi_rxdma_channel_pt_drop_pkt_clear(npi_handle_t handle, uint8_t channel)
+{
+	return (npi_rxdma_channel_control(handle, RXDMA_PT_DROP_PKT_CLEAR,
+			channel));
+}
+
+/*
+ * npi_rxdma_channel_wred_drop_clear():
+ *	This function is called to wred drop bit (debug only).
+ *
+ * Parameters:
+ *	handle		- NPI handle (virtualization flag must be defined).
+ *	channel		- logical RXDMA channel from 0 to 23.
+ *			  (If virtualization flag is not set, then
+ *			   logical channel is the same as the hardware
+ *			   channel number).
+ * Return:
+ *	NPI_SUCCESS
+ *
+ *	Error:
+ *	NPI_FAILURE	-
+ *		NPI_RXDMA_CHANNEL_INVALID -
+ */
+npi_status_t
+npi_rxdma_channel_wred_dop_clear(npi_handle_t handle, uint8_t channel)
+{
+	return (npi_rxdma_channel_control(handle, RXDMA_WRED_DROP_CLEAR,
+			channel));
+}
+
+/*
+ * npi_rxdma_channel_rcr_shfull_clear():
+ *	This function is called to clear RCR shadow full bit.
+ *
+ * Parameters:
+ *	handle		- NPI handle (virtualization flag must be defined).
+ *	channel		- logical RXDMA channel from 0 to 23.
+ *			  (If virtualization flag is not set, then
+ *			   logical channel is the same as the hardware
+ *			   channel number).
+ * Return:
+ *	NPI_SUCCESS
+ *
+ *	Error:
+ *	NPI_FAILURE	-
+ *		NPI_RXDMA_CHANNEL_INVALID -
+ */
+npi_status_t
+npi_rxdma_channel_rcr_shfull_clear(npi_handle_t handle, uint8_t channel)
+{
+	return (npi_rxdma_channel_control(handle, RXDMA_RCR_SFULL_CLEAR,
+			channel));
+}
+
+/*
+ * npi_rxdma_channel_rcrfull_clear():
+ *	This function is called to clear RCR full bit.
+ *
+ * Parameters:
+ *	handle		- NPI handle (virtualization flag must be defined).
+ *	channel		- logical RXDMA channel from 0 to 23.
+ *			  (If virtualization flag is not set, then
+ *			   logical channel is the same as the hardware
+ *			   channel number).
+ * Return:
+ *	NPI_SUCCESS
+ *
+ *	Error:
+ *	NPI_FAILURE	-
+ *		NPI_RXDMA_CHANNEL_INVALID -
+ */
+npi_status_t
+npi_rxdma_channel_rcr_full_clear(npi_handle_t handle, uint8_t channel)
+{
+	return (npi_rxdma_channel_control(handle, RXDMA_RCR_FULL_CLEAR,
+			channel));
+}
+
+npi_status_t
+npi_rxdma_channel_rbr_empty_clear(npi_handle_t handle, uint8_t channel)
+{
+	return (npi_rxdma_channel_control(handle,
+		RXDMA_RBR_EMPTY_CLEAR, channel));
+}
+
+npi_status_t
+npi_rxdma_channel_cs_clear_all(npi_handle_t handle, uint8_t channel)
+{
+	return (npi_rxdma_channel_control(handle, RXDMA_CS_CLEAR_ALL, channel));
+}
+
+/*
+ * npi_rxdma_channel_control():
+ *	This function is called to control a receive DMA channel
+ *	for arming the channel with mailbox updates, resetting
+ *	various event status bits (control and status register).
+ *
+ * Parameters:
+ *	handle		- NPI handle (virtualization flag must be defined).
+ *	control		- NPI defined control type supported:
+ *				- RXDMA_MEX_SET
+ * 				- RXDMA_RCRTO_CLEAR
+ *				- RXDMA_PT_DROP_PKT_CLEAR
+ *				- RXDMA_WRED_DROP_CLEAR
+ *				- RXDMA_RCR_SFULL_CLEAR
+ *				- RXDMA_RCR_FULL_CLEAR
+ *				- RXDMA_RBR_PRE_EMPTY_CLEAR
+ *				- RXDMA_RBR_EMPTY_CLEAR
+ *	channel		- logical RXDMA channel from 0 to 23.
+ *			  (If virtualization flag is not set, then
+ *			   logical channel is the same as the hardware.
+ * Return:
+ *	NPI_SUCCESS
+ *
+ *	Error:
+ *	NPI_FAILURE		-
+ *		NPI_TXDMA_OPCODE_INVALID	-
+ *		NPI_TXDMA_CHANNEL_INVALID	-
+ */
+npi_status_t
+npi_rxdma_channel_control(npi_handle_t handle, rxdma_cs_cntl_t control,
+			uint8_t channel)
+{
+
+	rx_dma_ctl_stat_t	cs;
+
+	if (!RXDMA_CHANNEL_VALID(channel)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+		    " npi_rxdma_channel_control",
+		    " channel", channel));
+		return (NPI_FAILURE | NPI_RXDMA_CHANNEL_INVALID(channel));
+	}
+
+	switch (control) {
+	case RXDMA_MEX_SET:
+		RXDMA_REG_READ64(handle, RX_DMA_CTL_STAT_REG, channel,
+				&cs.value);
+		cs.bits.hdw.mex = 1;
+		RXDMA_REG_WRITE64(handle, RX_DMA_CTL_STAT_REG,
+				channel, cs.value);
+		break;
+
+	case RXDMA_RCRTO_CLEAR:
+		RXDMA_REG_READ64(handle, RX_DMA_CTL_STAT_REG, channel,
+				&cs.value);
+		cs.bits.hdw.rcrto = 0;
+		RXDMA_REG_WRITE64(handle, RX_DMA_CTL_STAT_REG, channel,
+				cs.value);
+		break;
+
+	case RXDMA_PT_DROP_PKT_CLEAR:
+		RXDMA_REG_READ64(handle, RX_DMA_CTL_STAT_REG, channel,
+				&cs.value);
+		cs.bits.hdw.port_drop_pkt = 0;
+		RXDMA_REG_WRITE64(handle, RX_DMA_CTL_STAT_REG, channel,
+				cs.value);
+		break;
+
+	case RXDMA_WRED_DROP_CLEAR:
+		RXDMA_REG_READ64(handle, RX_DMA_CTL_STAT_REG, channel,
+				&cs.value);
+		cs.bits.hdw.wred_drop = 0;
+		RXDMA_REG_WRITE64(handle, RX_DMA_CTL_STAT_REG, channel,
+				cs.value);
+		break;
+
+	case RXDMA_RCR_SFULL_CLEAR:
+		RXDMA_REG_READ64(handle, RX_DMA_CTL_STAT_REG, channel,
+				&cs.value);
+		cs.bits.hdw.rcr_shadow_full = 0;
+		RXDMA_REG_WRITE64(handle, RX_DMA_CTL_STAT_REG, channel,
+				cs.value);
+		break;
+
+	case RXDMA_RCR_FULL_CLEAR:
+		RXDMA_REG_READ64(handle, RX_DMA_CTL_STAT_REG, channel,
+				&cs.value);
+		cs.bits.hdw.rcrfull = 0;
+		RXDMA_REG_WRITE64(handle, RX_DMA_CTL_STAT_REG, channel,
+				cs.value);
+		break;
+
+	case RXDMA_RBR_PRE_EMPTY_CLEAR:
+		RXDMA_REG_READ64(handle, RX_DMA_CTL_STAT_REG, channel,
+				&cs.value);
+		cs.bits.hdw.rbr_pre_empty = 0;
+		RXDMA_REG_WRITE64(handle, RX_DMA_CTL_STAT_REG, channel,
+				cs.value);
+		break;
+
+	case RXDMA_RBR_EMPTY_CLEAR:
+		RXDMA_REG_READ64(handle, RX_DMA_CTL_STAT_REG, channel,
+				&cs.value);
+		cs.bits.hdw.rbr_empty = 1;
+		RXDMA_REG_WRITE64(handle, RX_DMA_CTL_STAT_REG, channel,
+				cs.value);
+		break;
+
+	case RXDMA_CS_CLEAR_ALL:
+		cs.value = 0;
+		RXDMA_REG_WRITE64(handle, RX_DMA_CTL_STAT_REG, channel,
+				cs.value);
+		break;
+
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    "npi_rxdma_channel_control",
+				    "control", control));
+		return (NPI_FAILURE | NPI_RXDMA_OPCODE_INVALID(channel));
+	}
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_rxdma_control_status():
+ *	This function is called to operate on the control
+ *	and status register.
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	op_mode		- OP_GET: get hardware control and status
+ *			  OP_SET: set hardware control and status
+ *			  OP_UPDATE: update hardware control and status.
+ *			  OP_CLEAR: clear control and status register to 0s.
+ *	channel		- hardware RXDMA channel from 0 to 23.
+ *	cs_p		- pointer to hardware defined control and status
+ *			  structure.
+ * Return:
+ *	NPI_SUCCESS
+ *
+ *	Error:
+ *	NPI_FAILURE		-
+ *		NPI_RXDMA_OPCODE_INVALID	-
+ *		NPI_RXDMA_CHANNEL_INVALID	-
+ */
+npi_status_t
+npi_rxdma_control_status(npi_handle_t handle, io_op_t op_mode,
+			uint8_t channel, p_rx_dma_ctl_stat_t cs_p)
+{
+	int			status = NPI_SUCCESS;
+	rx_dma_ctl_stat_t	cs;
+
+	if (!RXDMA_CHANNEL_VALID(channel)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+		    "npi_rxdma_control_status",
+		    "channel", channel));
+		return (NPI_FAILURE | NPI_RXDMA_CHANNEL_INVALID(channel));
+	}
+
+	switch (op_mode) {
+	case OP_GET:
+		RXDMA_REG_READ64(handle, RX_DMA_CTL_STAT_REG, channel,
+				&cs_p->value);
+		break;
+
+	case OP_SET:
+		RXDMA_REG_WRITE64(handle, RX_DMA_CTL_STAT_REG, channel,
+			cs_p->value);
+		break;
+
+	case OP_UPDATE:
+		RXDMA_REG_READ64(handle, RX_DMA_CTL_STAT_REG, channel,
+				&cs.value);
+		RXDMA_REG_WRITE64(handle, RX_DMA_CTL_STAT_REG, channel,
+			cs_p->value | cs.value);
+		break;
+
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+		    "npi_rxdma_control_status",
+		    "control", op_mode));
+		return (NPI_FAILURE | NPI_RXDMA_OPCODE_INVALID(channel));
+	}
+
+	return (status);
+}
+
+/*
+ * npi_rxdma_event_mask():
+ *	This function is called to operate on the event mask
+ *	register which is used for generating interrupts.
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	op_mode		- OP_GET: get hardware event mask
+ *			  OP_SET: set hardware interrupt event masks
+ *			  OP_CLEAR: clear control and status register to 0s.
+ *	channel		- hardware RXDMA channel from 0 to 23.
+ *	mask_p		- pointer to hardware defined event mask
+ *			  structure.
+ * Return:
+ *	NPI_SUCCESS		- If set is complete successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE		-
+ *		NPI_RXDMA_OPCODE_INVALID	-
+ *		NPI_RXDMA_CHANNEL_INVALID	-
+ */
+npi_status_t
+npi_rxdma_event_mask(npi_handle_t handle, io_op_t op_mode,
+		uint8_t channel, p_rx_dma_ent_msk_t mask_p)
+{
+	int			status = NPI_SUCCESS;
+	rx_dma_ent_msk_t	mask;
+
+	if (!RXDMA_CHANNEL_VALID(channel)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+		    "npi_rxdma_event_mask",
+		    "channel", channel));
+		return (NPI_FAILURE | NPI_RXDMA_CHANNEL_INVALID(channel));
+	}
+
+	switch (op_mode) {
+	case OP_GET:
+		RXDMA_REG_READ64(handle, RX_DMA_ENT_MSK_REG, channel,
+				&mask_p->value);
+		break;
+
+	case OP_SET:
+		RXDMA_REG_WRITE64(handle, RX_DMA_ENT_MSK_REG, channel,
+				mask_p->value);
+		break;
+
+	case OP_UPDATE:
+		RXDMA_REG_READ64(handle, RX_DMA_ENT_MSK_REG, channel,
+				&mask.value);
+		RXDMA_REG_WRITE64(handle, RX_DMA_ENT_MSK_REG, channel,
+			mask_p->value | mask.value);
+		break;
+
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+		    "npi_rxdma_event_mask",
+		    "eventmask", op_mode));
+		return (NPI_FAILURE | NPI_RXDMA_OPCODE_INVALID(channel));
+	}
+
+	return (status);
+}
+
+/*
+ * npi_rxdma_event_mask_config():
+ *	This function is called to operate on the event mask
+ *	register which is used for generating interrupts
+ *	and status register.
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	op_mode		- OP_GET: get hardware event mask
+ *			  OP_SET: set hardware interrupt event masks
+ *			  OP_CLEAR: clear control and status register to 0s.
+ *	channel		- hardware RXDMA channel from 0 to 23.
+ *	cfgp		- pointer to NPI defined event mask
+ *			  enum data type.
+ * Return:
+ *	NPI_SUCCESS		- If set is complete successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE		-
+ *		NPI_RXDMA_OPCODE_INVALID	-
+ *		NPI_RXDMA_CHANNEL_INVALID	-
+ */
+npi_status_t
+npi_rxdma_event_mask_config(npi_handle_t handle, io_op_t op_mode,
+		uint8_t channel, rxdma_ent_msk_cfg_t *mask_cfgp)
+{
+	int		status = NPI_SUCCESS;
+	uint64_t	value;
+
+	if (!RXDMA_CHANNEL_VALID(channel)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+		    "npi_rxdma_event_mask_config",
+		    "channel", channel));
+		return (NPI_FAILURE | NPI_RXDMA_CHANNEL_INVALID(channel));
+	}
+
+	switch (op_mode) {
+	case OP_GET:
+		RXDMA_REG_READ64(handle, RX_DMA_ENT_MSK_REG, channel,
+				mask_cfgp);
+		break;
+
+	case OP_SET:
+		RXDMA_REG_WRITE64(handle, RX_DMA_ENT_MSK_REG, channel,
+				*mask_cfgp);
+		break;
+
+	case OP_UPDATE:
+		RXDMA_REG_READ64(handle, RX_DMA_ENT_MSK_REG, channel, &value);
+		RXDMA_REG_WRITE64(handle, RX_DMA_ENT_MSK_REG, channel,
+			*mask_cfgp | value);
+		break;
+
+	case OP_CLEAR:
+		RXDMA_REG_WRITE64(handle, RX_DMA_ENT_MSK_REG, channel,
+			CFG_RXDMA_MASK_ALL);
+		break;
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+		    "npi_rxdma_event_mask_config",
+		    "eventmask", op_mode));
+		return (NPI_FAILURE | NPI_RXDMA_OPCODE_INVALID(channel));
+	}
+
+	return (status);
+}
diff --git a/drivers/net/nxge/npi/npi_rxdma.h b/drivers/net/nxge/npi/npi_rxdma.h
new file mode 100644
index 0000000..bdf4d66
--- /dev/null
+++ b/drivers/net/nxge/npi/npi_rxdma.h
@@ -0,0 +1,1348 @@
+/*
+ * npi_rxdma.h	Neptune  RX DMA HW API functions
+ *
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef _NPI_RXDMA_H
+#define	_NPI_RXDMA_H
+
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#include <npi.h>
+
+#include "nxge_defs.h"
+#include "nxge_hw.h"
+#include <nxge_rxdma_hw.h>
+
+/*
+ * Register offset (0x200 bytes for each channel) for receive ring registers.
+ */
+#define	NXGE_RXDMA_OFFSET(x, v, channel) (x + \
+		(!v ? DMC_OFFSET(channel) : \
+		    RDMC_PIOVADDR_OFFSET(channel)))
+
+
+#define	 REG_FZC_RDC_OFFSET(reg, rdc) (reg + RX_LOG_DMA_OFFSET(rdc))
+
+#define	 REG_RDC_TABLE_OFFSET(table) \
+	    (RDC_TBL_REG + table * (NXGE_MAX_RDCS * 8))
+
+#define	RXDMA_REG_READ64(handle, reg, channel, data_p) {\
+	NXGE_REG_RD64(handle, (NXGE_RXDMA_OFFSET(reg, handle.is_vraddr,\
+			channel)), (data_p))\
+}
+
+#define	RXDMA_REG_READ32(handle, reg, channel) \
+	NXGE_NPI_PIO_READ32(handle, (NXGE_RXDMA_OFFSET(reg, handle.is_vraddr,\
+			channel)))
+
+
+#define	RXDMA_REG_WRITE64(handle, reg, channel, data) {\
+	NXGE_REG_WR64(handle, (NXGE_RXDMA_OFFSET(reg, handle.is_vraddr,\
+			channel)), (data))\
+}
+
+/*
+ * RX NPI error codes
+ */
+#define	RXDMA_ER_ST			(RXDMA_BLK_ID << NPI_BLOCK_ID_SHIFT)
+#define	RXDMA_ID_SHIFT(n)		(n << NPI_PORT_CHAN_SHIFT)
+
+
+#define	NPI_RXDMA_ERROR			RXDMA_ER_ST
+
+#define	NPI_RXDMA_SW_PARAM_ERROR	(NPI_RXDMA_ERROR | 0x40)
+#define	NPI_RXDMA_HW_ERROR	(NPI_RXDMA_ERROR | 0x80)
+
+#define	NPI_RXDMA_RDC_INVALID		(NPI_RXDMA_ERROR | CHANNEL_INVALID)
+#define	NPI_RXDMA_PAGE_INVALID		(NPI_RXDMA_ERROR | LOGICAL_PAGE_INVALID)
+#define	NPI_RXDMA_RESET_ERR		(NPI_RXDMA_HW_ERROR | RESET_FAILED)
+#define	NPI_RXDMA_DISABLE_ERR		(NPI_RXDMA_HW_ERROR | 0x0000a)
+#define	NPI_RXDMA_ENABLE_ERR		(NPI_RXDMA_HW_ERROR | 0x0000b)
+#define	NPI_RXDMA_FUNC_INVALID		(NPI_RXDMA_SW_PARAM_ERROR | 0x0000a)
+#define	NPI_RXDMA_BUFSZIE_INVALID	(NPI_RXDMA_SW_PARAM_ERROR | 0x0000b)
+#define	NPI_RXDMA_RBRSZIE_INVALID	(NPI_RXDMA_SW_PARAM_ERROR | 0x0000c)
+#define	NPI_RXDMA_RCRSZIE_INVALID	(NPI_RXDMA_SW_PARAM_ERROR | 0x0000d)
+#define	NPI_RXDMA_PORT_INVALID		(NPI_RXDMA_ERROR | PORT_INVALID)
+#define	NPI_RXDMA_TABLE_INVALID		(NPI_RXDMA_ERROR | RDC_TAB_INVALID)
+
+#define	NPI_RXDMA_CHANNEL_INVALID(n)	(RXDMA_ID_SHIFT(n) |	\
+					NPI_RXDMA_ERROR | CHANNEL_INVALID)
+#define	NPI_RXDMA_OPCODE_INVALID(n)	(RXDMA_ID_SHIFT(n) |	\
+					NPI_RXDMA_ERROR | OPCODE_INVALID)
+
+
+#define	NPI_RXDMA_ERROR_ENCODE(err, rdc)	\
+	(RXDMA_ID_SHIFT(rdc) | RXDMA_ER_ST | err)
+
+
+#define	RXDMA_CHANNEL_VALID(rdc) \
+	((rdc < NXGE_MAX_RDCS))
+
+#define	RXDMA_PORT_VALID(port) \
+	((port < MAX_PORTS_PER_NXGE))
+
+#define	RXDMA_TABLE_VALID(table) \
+	((table < NXGE_MAX_RDC_GROUPS))
+
+
+#define	RXDMA_PAGE_VALID(page) \
+	((page == 0) || (page == 1))
+
+#define	RXDMA_BUFF_OFFSET_VALID(offset) \
+	((offset == SW_OFFSET_NO_OFFSET) || \
+	    (offset == SW_OFFSET_64) || \
+	    (offset == SW_OFFSET_128))
+
+
+#define	RXDMA_RCR_TO_VALID(tov) ((tov) && (tov < 64))
+#define	RXDMA_RCR_THRESH_VALID(thresh) ((thresh) && (thresh < 1024))
+
+
+/*
+ * RXDMA NPI defined control types.
+ */
+typedef	enum _rxdma_cs_cntl_e {
+	RXDMA_CS_CLEAR_ALL		= 0x1,
+	RXDMA_MEX_SET			= 0x2,
+	RXDMA_RCRTO_CLEAR		= 0x8,
+	RXDMA_PT_DROP_PKT_CLEAR		= 0x10,
+	RXDMA_WRED_DROP_CLEAR		= 0x20,
+	RXDMA_RCR_SFULL_CLEAR		= 0x40,
+	RXDMA_RCR_FULL_CLEAR		= 0x80,
+	RXDMA_RBR_PRE_EMPTY_CLEAR	= 0x100,
+	RXDMA_RBR_EMPTY_CLEAR		= 0x200
+} rxdma_cs_cntl_t;
+
+/*
+ * RXDMA NPI defined event masks (mapped to the hardware defined masks).
+ */
+typedef	enum _rxdma_ent_msk_cfg_e {
+	CFG_RXDMA_ENT_MSK_CFIGLOGPGE_MASK = RX_DMA_ENT_MSK_CFIGLOGPGE_MASK,
+	CFG_RXDMA_ENT_MSK_RBRLOGPGE_MASK  = RX_DMA_ENT_MSK_RBRLOGPGE_MASK,
+	CFG_RXDMA_ENT_MSK_RBRFULL_MASK	  = RX_DMA_ENT_MSK_RBRFULL_MASK,
+	CFG_RXDMA_ENT_MSK_RBREMPTY_MASK	  = RX_DMA_ENT_MSK_RBREMPTY_MASK,
+	CFG_RXDMA_ENT_MSK_RCRFULL_MASK	  = RX_DMA_ENT_MSK_RCRFULL_MASK,
+	CFG_RXDMA_ENT_MSK_RCRINCON_MASK	  = RX_DMA_ENT_MSK_RCRINCON_MASK,
+	CFG_RXDMA_ENT_MSK_CONFIG_ERR	  = RX_DMA_ENT_MSK_CONFIG_ERR_MASK,
+	CFG_RXDMA_ENT_MSK_RCR_SH_FULL_MASK = RX_DMA_ENT_MSK_RCRSH_FULL_MASK,
+	CFG_RXDMA_ENT_MSK_RBR_PRE_EMTY_MASK = RX_DMA_ENT_MSK_RBR_PRE_EMPTY_MASK,
+	CFG_RXDMA_ENT_MSK_WRED_DROP_MASK   = RX_DMA_ENT_MSK_WRED_DROP_MASK,
+	CFG_RXDMA_ENT_MSK_PT_DROP_PKT_MASK = RX_DMA_ENT_MSK_PTDROP_PKT_MASK,
+	CFG_RXDMA_ENT_MSK_RBR_PRE_PAR_MASK = RX_DMA_ENT_MSK_RBR_PRE_PAR_MASK,
+	CFG_RXDMA_ENT_MSK_RCR_SHA_PAR_MASK = RX_DMA_ENT_MSK_RCR_SHA_PAR_MASK,
+	CFG_RXDMA_ENT_MSK_RCRTO_MASK	  = RX_DMA_ENT_MSK_RCRTO_MASK,
+	CFG_RXDMA_ENT_MSK_THRES_MASK	  = RX_DMA_ENT_MSK_THRES_MASK,
+	CFG_RXDMA_ENT_MSK_DC_FIFO_ERR_MASK  = RX_DMA_ENT_MSK_DC_FIFO_ERR_MASK,
+	CFG_RXDMA_ENT_MSK_RCR_ACK_ERR_MASK  = RX_DMA_ENT_MSK_RCR_ACK_ERR_MASK,
+	CFG_RXDMA_ENT_MSK_RSP_DAT_ERR_MASK  = RX_DMA_ENT_MSK_RSP_DAT_ERR_MASK,
+	CFG_RXDMA_ENT_MSK_BYTE_EN_BUS_MASK  = RX_DMA_ENT_MSK_BYTE_EN_BUS_MASK,
+	CFG_RXDMA_ENT_MSK_RSP_CNT_ERR_MASK  = RX_DMA_ENT_MSK_RSP_CNT_ERR_MASK,
+	CFG_RXDMA_ENT_MSK_RBR_TMOUT_MASK  = RX_DMA_ENT_MSK_RBR_TMOUT_MASK,
+
+	CFG_RXDMA_MASK_ALL	  = (RX_DMA_ENT_MSK_CFIGLOGPGE_MASK |
+					RX_DMA_ENT_MSK_RBRLOGPGE_MASK |
+					RX_DMA_ENT_MSK_RBRFULL_MASK |
+					RX_DMA_ENT_MSK_RBREMPTY_MASK |
+					RX_DMA_ENT_MSK_RCRFULL_MASK |
+					RX_DMA_ENT_MSK_RCRINCON_MASK |
+					RX_DMA_ENT_MSK_CONFIG_ERR_MASK |
+					RX_DMA_ENT_MSK_RCRSH_FULL_MASK |
+					RX_DMA_ENT_MSK_RBR_PRE_EMPTY_MASK |
+					RX_DMA_ENT_MSK_WRED_DROP_MASK |
+					RX_DMA_ENT_MSK_PTDROP_PKT_MASK |
+					RX_DMA_ENT_MSK_RBR_PRE_PAR_MASK |
+					RX_DMA_ENT_MSK_RCR_SHA_PAR_MASK |
+					RX_DMA_ENT_MSK_RCRTO_MASK |
+					RX_DMA_ENT_MSK_THRES_MASK |
+					RX_DMA_ENT_MSK_DC_FIFO_ERR_MASK |
+					RX_DMA_ENT_MSK_RCR_ACK_ERR_MASK |
+					RX_DMA_ENT_MSK_RSP_DAT_ERR_MASK |
+					RX_DMA_ENT_MSK_BYTE_EN_BUS_MASK |
+					RX_DMA_ENT_MSK_RSP_CNT_ERR_MASK |
+					RX_DMA_ENT_MSK_RBR_TMOUT_MASK)
+} rxdma_ent_msk_cfg_t;
+
+
+
+typedef union _addr44 {
+	uint64_t	addr;
+	struct {
+#if defined(_BIG_ENDIAN)
+		uint32_t rsrvd:20;
+		uint32_t hdw:12;
+		uint32_t ldw;
+#else
+		uint32_t ldw;
+		uint32_t hdw:12;
+		uint32_t rsrvd:20;
+#endif
+	} bits;
+} addr44_t;
+
+
+/*
+ * npi_rxdma_cfg_default_port_rdc()
+ * Set the default rdc for the port
+ *
+ * Inputs:
+ *	handle:		register handle interpreted by the underlying OS
+ *	portnm:		Physical Port Number
+ *	rdc:	RX DMA Channel number
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_RXDMA_RDC_INVALID
+ * NPI_RXDMA_PORT_INVALID
+ *
+ */
+
+npi_status_t npi_rxdma_cfg_default_port_rdc(npi_handle_t,
+				    uint8_t, uint8_t);
+
+/*
+ * npi_rxdma_cfg_rdc_table()
+ * Configure/populate the RDC table
+ *
+ * Inputs:
+ *	handle:		register handle interpreted by the underlying OS
+ *	table:		RDC Group Number
+ *	rdc[]:	 Array of RX DMA Channels
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_RXDMA_TABLE_INVALID
+ *
+ */
+
+npi_status_t npi_rxdma_cfg_rdc_table(npi_handle_t,
+			    uint8_t, uint8_t []);
+
+npi_status_t npi_rxdma_cfg_rdc_table_default_rdc(npi_handle_t,
+					    uint8_t, uint8_t);
+npi_status_t npi_rxdma_cfg_rdc_rcr_timeout_disable(npi_handle_t,
+					    uint8_t);
+
+
+/*
+ * npi_rxdma_32bitmode_enable()
+ * Enable 32 bit mode
+ *
+ * Inputs:
+ *	handle:		register handle interpreted by the underlying OS
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ *
+ */
+
+npi_status_t npi_rxdma_cfg_32bitmode_enable(npi_handle_t);
+
+
+/*
+ * npi_rxdma_32bitmode_disable()
+ * disable 32 bit mode
+ *
+ * Inputs:
+ *	handle:		register handle interpreted by the underlying OS
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ *
+ */
+
+
+npi_status_t npi_rxdma_cfg_32bitmode_disable(npi_handle_t);
+
+/*
+ * npi_rxdma_cfg_ram_access_enable()
+ * Enable PIO access to shadow and prefetch memory.
+ * In the case of DMA errors, software may need to
+ * initialize the shadow and prefetch memories to
+ * sane value (may be clear it) before re-enabling
+ * the DMA channel.
+ *
+ * Inputs:
+ *	handle:		register handle interpreted by the underlying OS
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ *
+ */
+
+npi_status_t npi_rxdma_cfg_ram_access_enable(npi_handle_t);
+
+
+/*
+ * npi_rxdma_cfg_ram_access_disable()
+ * Disable PIO access to shadow and prefetch memory.
+ * This is the normal operation mode.
+ *
+ * Inputs:
+ *	handle:		register handle interpreted by the underlying OS
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ *
+ */
+
+npi_status_t npi_rxdma_cfg_ram_access_disable(npi_handle_t);
+
+
+/*
+ * npi_rxdma_cfg_clock_div_set()
+ * init the clock division, used for RX timers
+ * This determines the granularity of RX DMA countdown timers
+ * It depends on the system clock. For example if the system
+ * clock is 300 MHz, a value of 30000 will yield a granularity
+ * of 100usec.
+ *
+ * Inputs:
+ *	handle:		register handle interpreted by the underlying OS
+ *	count:		System clock divider
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_SW_ERR
+ * NPI_HW_ERR
+ *
+ */
+
+npi_status_t npi_rxdma_cfg_clock_div_set(npi_handle_t, uint16_t);
+
+/*
+ * npi_rxdma_cfg_red_rand_init()
+ * init the WRED Discard
+ * By default, it is enabled
+ *
+ * Inputs:
+ *	handle:		register handle interpreted by the underlying OS
+ *	init_value:	WRED init value
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_SW_ERR
+ * NPI_HW_ERR
+ *
+ */
+
+npi_status_t npi_rxdma_cfg_red_rand_init(npi_handle_t, uint16_t);
+
+/*
+ * npi_rxdma_cfg_wred_disable()
+ * init the WRED Discard
+ * By default, it is enabled
+ *
+ * Inputs:
+ *	handle:		register handle interpreted by the underlying OS
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_SW_ERR
+ * NPI_HW_ERR
+ *
+ */
+
+
+npi_status_t npi_rxdma_cfg_wred_disable(npi_handle_t);
+
+/*
+ * npi_rxdma_cfg_wred_param()
+ * COnfigure per rxdma channel WRED parameters
+ * By default, it is enabled
+ *
+ * Inputs:
+ *	handle:		register handle interpreted by the underlying OS
+ *	rdc:	RX DMA Channel number
+ *	wred_params:	WRED configuration parameters
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_SW_ERR
+ * NPI_HW_ERR
+ *
+ */
+
+
+
+npi_status_t npi_rxdma_cfg_wred_param(npi_handle_t, uint8_t,
+				    rdc_red_para_t *);
+
+
+/*
+ * npi_rxdma_port_ddr_weight
+ * Set the DDR weight for a port.
+ *
+ * Inputs:
+ *	handle:		register handle interpreted by the underlying OS
+ *	portnm:		Physical Port Number
+ *	weight:		Port relative weight (in approx. bytes)
+ *			Default values are:
+ *			0x400 (port 0 and 1) corresponding to 10 standard
+ *			      size (1500 bytes) Frames
+ *			0x66 (port 2 and 3) corresponding to 10% 10Gig ports
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+npi_status_t npi_rxdma_cfg_port_ddr_weight(npi_handle_t,
+				    uint8_t, uint32_t);
+
+
+/*
+ * npi_rxdma_port_usage_get()
+ * Gets the port usage, in terms of 16 byte blocks
+ *
+ * NOTE: The register count is cleared upon reading.
+ *
+ * Inputs:
+ *	handle:		register handle interpreted by the underlying OS
+ *	portnm:		Physical Port Number
+ *	blocks:		ptr to save current count.
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_HW_ERR
+ * NPI_SW_ERR
+ *
+ */
+
+npi_status_t npi_rxdma_port_usage_get(npi_handle_t,
+				    uint8_t, uint32_t *);
+
+
+/*
+ * npi_rxdma_cfg_logical_page()
+ * Configure per rxdma channel Logical page
+ *
+ * To disable the logical page, set valid = 0;
+ *
+ * Inputs:
+ *	handle:		register handle interpreted by the underlying OS
+ *	rdc:		RX DMA Channel number
+ *	page_params:	Logical Page configuration parameters
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_SW_ERR
+ * NPI_HW_ERR
+ *
+ */
+
+
+
+npi_status_t npi_rxdma_cfg_logical_page(npi_handle_t, uint8_t,
+				    dma_log_page_t *);
+
+
+/*
+ * npi_rxdma_cfg_logical_page_handle()
+ * Configure per rxdma channel Logical page handle
+ *
+ *
+ * Inputs:
+ *	handle:		register handle interpreted by the underlying OS
+ *	rdc:		RX DMA Channel number
+ *	pg_handle:	Logical Page handle
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_SW_ERR
+ * NPI_HW_ERR
+ *
+ */
+
+
+npi_status_t npi_rxdma_cfg_logical_page_handle(npi_handle_t, uint8_t,
+				    uint64_t);
+
+
+
+
+npi_status_t npi_rxdma_cfg_logical_page_disable(npi_handle_t,
+				    uint8_t, uint8_t);
+
+typedef enum _bsize {
+	SIZE_0B = 0x0,
+	SIZE_64B,
+	SIZE_128B,
+	SIZE_192B,
+	SIZE_256B,
+	SIZE_512B,
+	SIZE_1KB,
+	SIZE_2KB,
+	SIZE_4KB,
+	SIZE_8KB,
+	SIZE_16KB,
+	SIZE_32KB
+} bsize_t;
+
+
+
+/*
+ * npi_rxdma_cfg_rdc_ring()
+ * Configure The RDC channel Rcv Buffer Ring
+ *
+ * Inputs:
+ *	rdc:		RX DMA Channel number
+ *	rdc_params:	RDC configuration parameters
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_SW_ERR
+ * NPI_HW_ERR
+ *
+ */
+
+typedef struct _rdc_desc_cfg_t {
+	uint8_t mbox_enable;	/* Enable full (18b) header */
+	uint8_t full_hdr;	/* Enable full (18b) header */
+	uint8_t offset;	/* 64 byte offsets */
+	uint8_t valid2;	/* size 2 is valid */
+	bsize_t size2;	/* Size 2 length */
+	uint8_t valid1;	/* size 1 is valid */
+	bsize_t size1;	/* Size 1 length */
+	uint8_t valid0;	/* size 0 is valid */
+	bsize_t size0;	/* Size 1 length */
+	bsize_t page_size;   /* Page or buffer Size */
+    uint8_t	rcr_timeout_enable;
+    uint8_t	rcr_timeout;
+    uint16_t	rcr_threshold;
+	uint16_t rcr_len;	   /* RBR Descriptor size (entries) */
+	uint16_t rbr_len;	   /* RBR Descriptor size (entries) */
+	uint64_t mbox_addr;	   /* Mailbox Address */
+	uint64_t rcr_addr;	   /* RCR Address */
+	uint64_t rbr_addr;	   /* RBB Address */
+} rdc_desc_cfg_t;
+
+
+
+npi_status_t npi_rxdma_cfg_rdc_ring(npi_handle_t, uint8_t,
+				    rdc_desc_cfg_t *);
+
+
+
+
+/*
+ * npi_rxdma_rdc_rcr_flush
+ * Forces RX completion ring update
+ *
+ * Inputs:
+ *	rdc:		RX DMA Channel number
+ *
+ * Return:
+ *
+ */
+
+#define	npi_rxdma_rdc_rcr_flush(handle, rdc) \
+	RXDMA_REG_WRITE64(handle, RCR_FLSH_REG, rdc, \
+		    (RCR_FLSH_SET << RCR_FLSH_SHIFT))
+
+
+
+/*
+ * npi_rxdma_rdc_rcr_read_update
+ * Update the number of rcr packets and buffers processed
+ *
+ * Inputs:
+ *	channel:	RX DMA Channel number
+ *	num_pkts:	Number of pkts processed by SW.
+ *			    A packet could constitute multiple
+ *			    buffers, in case jumbo packets.
+ *	num_bufs:	Number of buffer processed by SW.
+ *
+ * Return:
+ *	NPI_FAILURE		-
+ *		NPI_RXDMA_OPCODE_INVALID	-
+ *		NPI_RXDMA_CHANNEL_INVALID	-
+ *
+ */
+
+npi_status_t npi_rxdma_rdc_rcr_read_update(npi_handle_t, uint8_t,
+				    uint16_t, uint16_t);
+/*
+ * npi_rxdma_rdc_rcr_pktread_update
+ * Update the number of packets processed
+ *
+ * Inputs:
+ *	channel:	RX DMA Channel number
+ *	num_pkts:	Number ofpkts processed by SW.
+ *			A packet could constitute multiple
+ *			buffers, in case jumbo packets.
+ *
+ * Return:
+ *	NPI_FAILURE		-
+ *		NPI_RXDMA_OPCODE_INVALID	-
+ *		NPI_RXDMA_CHANNEL_INVALID	-
+ *
+ */
+
+npi_status_t npi_rxdma_rdc_rcr_pktread_update(npi_handle_t,
+					uint8_t, uint16_t);
+
+
+
+/*
+ * npi_rxdma_rdc_rcr_bufread_update
+ * Update the number of buffers processed
+ *
+ * Inputs:
+ *	channel:		RX DMA Channel number
+ *	num_bufs:	Number of buffer processed by SW. Multiple buffers
+ *   could be part of a single packet.
+ *
+ * Return:
+ *	NPI_FAILURE		-
+ *		NPI_RXDMA_OPCODE_INVALID	-
+ *		NPI_RXDMA_CHANNEL_INVALID	-
+ *
+ */
+
+npi_status_t npi_rxdma_rdc_rcr_bufread_update(npi_handle_t,
+					uint8_t, uint16_t);
+
+
+
+/*
+ * npi_rxdma_rdc_rbr_kick
+ * Kick RDC RBR
+ *
+ * Inputs:
+ *	rdc:		RX DMA Channel number
+ *	num_buffers:	Number of Buffers posted to the RBR
+ *
+ * Return:
+ *
+ */
+
+#define	npi_rxdma_rdc_rbr_kick(handle, rdc, num_buffers) \
+	RXDMA_REG_WRITE64(handle, RBR_KICK_REG, rdc, num_buffers)
+
+
+/*
+ * npi_rxdma_rdc_rbr_head_get
+ * Gets the current rbr head pointer.
+ *
+ * Inputs:
+ *	rdc:		RX DMA Channel number
+ *	hdptr		ptr to write the rbr head value
+ *
+ * Return:
+ *
+ */
+
+npi_status_t npi_rxdma_rdc_rbr_head_get(npi_handle_t,
+				    uint8_t, addr44_t  *);
+
+
+
+/*
+ * npi_rxdma_rdc_rbr_stat_get
+ * Returns the RBR stat. The stat consists of the
+ * RX buffers in the ring. It also indicates if there
+ * has been an overflow.
+ *
+ * Inputs:
+ *	rdc:		RX DMA Channel number
+ *	rbr_stat_t:	Structure to update stat
+ *
+ * Return:
+ *
+ */
+
+npi_status_t npi_rxdma_rdc_rbr_stat_get(npi_handle_t, uint8_t,
+				    rbr_stat_t *);
+
+
+
+/*
+ * npi_rxdma_cfg_rdc_reset
+ * Resets the RDC channel
+ *
+ * Inputs:
+ *	rdc:		RX DMA Channel number
+ *
+ * Return:
+ *
+ */
+
+npi_status_t npi_rxdma_cfg_rdc_reset(npi_handle_t, uint8_t);
+
+
+/*
+ * npi_rxdma_rdc_enable
+ * Enables the RDC channel
+ *
+ * Inputs:
+ *	rdc:		RX DMA Channel number
+ *
+ * Return:
+ *
+ */
+
+npi_status_t npi_rxdma_cfg_rdc_enable(npi_handle_t, uint8_t);
+
+/*
+ * npi_rxdma_rdc_disable
+ * Disables the RDC channel
+ *
+ * Inputs:
+ *	rdc:		RX DMA Channel number
+ *
+ * Return:
+ *
+ */
+
+npi_status_t npi_rxdma_cfg_rdc_disable(npi_handle_t, uint8_t);
+
+
+/*
+ * npi_rxdma_cfg_rdc_rcr_timeout()
+ * Configure The RDC channel completion ring timeout.
+ * If a frame has been received, an event would be
+ * generated atleast at the expiration of the timeout.
+ *
+ * Enables timeout by default.
+ *
+ * Inputs:
+ *	rdc:		RX DMA Channel number
+ *	rcr_timeout:	Completion Ring timeout value
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_SW_ERR
+ * NPI_HW_ERR
+ *
+ */
+
+npi_status_t npi_rxdma_cfg_rdc_rcr_timeout(npi_handle_t, uint8_t,
+				    uint8_t);
+
+
+/*
+ * npi_rxdma_cfg_rdc_rcr_threshold()
+ * Configure The RDC channel completion ring threshold.
+ * An event would be If the number of frame received,
+ * surpasses the threshold value
+ *
+ * Inputs:
+ *	rdc:		RX DMA Channel number
+ *	rcr_threshold:	Completion Ring Threshold count
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ * NPI_SW_ERR
+ * NPI_HW_ERR
+ *
+ */
+
+npi_status_t npi_rxdma_cfg_rdc_rcr_threshold(npi_handle_t, uint8_t,
+				    uint16_t);
+
+
+npi_status_t npi_rxdma_cfg_rdc_rcr_timeout_disable(npi_handle_t, uint8_t);
+
+typedef struct _rdc_error_stat_t {
+	uint8_t fault:1;
+    uint8_t	multi_fault:1;
+    uint8_t	rbr_fault:1;
+    uint8_t	buff_fault:1;
+    uint8_t	rcr_fault:1;
+	addr44_t fault_addr;
+} rdc_error_stat_t;
+
+#ifdef OLD
+/*
+ * npi_rxdma_rdc_error_stat_get
+ * Gets the current Error stat for the RDC.
+ *
+ * Inputs:
+ *	rdc:		RX DMA Channel number
+ *	error_stat	Structure to write current RDC Error stat
+ *
+ * Return:
+ *
+ */
+
+npi_status_t npi_rxdma_rdc_error_stat_get(npi_handle_t,
+				    uint8_t, rdc_error_stat_t *);
+
+#endif
+
+/*
+ * npi_rxdma_rdc_rcr_tail_get
+ * Gets the current RCR tail address for the RDC.
+ *
+ * Inputs:
+ *	rdc:		RX DMA Channel number
+ *	tail_addr	Structure to write current RDC RCR tail address
+ *
+ * Return:
+ *
+ */
+
+npi_status_t npi_rxdma_rdc_rcr_tail_get(npi_handle_t,
+				    uint8_t, addr44_t *);
+
+
+npi_status_t npi_rxdma_rdc_rcr_qlen_get(npi_handle_t,
+				    uint8_t, uint16_t *);
+
+
+
+typedef struct _rdc_discard_stat_t {
+    uint8_t	nobuf_ovflow;
+    uint8_t	red_ovflow;
+    uint32_t	nobuf_discard;
+    uint32_t	red_discard;
+} rdc_discard_stat_t;
+
+
+/*
+ * npi_rxdma_rdc_discard_stat_get
+ * Gets the current discrad stats for the RDC.
+ *
+ * Inputs:
+ *	rdc:		RX DMA Channel number
+ *	rcr_stat	Structure to write current RDC discard stat
+ *
+ * Return:
+ *
+ */
+
+npi_status_t npi_rxdma_rdc_discard_stat_get(npi_handle_t,
+				    uint8_t, rdc_discard_stat_t);
+
+
+/*
+ * npi_rx_port_discard_stat_get
+ * Gets the current input (IPP) discrad stats for the rx port.
+ *
+ * Inputs:
+ *	rdc:		RX DMA Channel number
+ *	rx_disc_cnt_t	Structure to write current RDC discard stat
+ *
+ * Return:
+ *
+ */
+
+npi_status_t npi_rx_port_discard_stat_get(npi_handle_t,
+				    uint8_t,
+				    rx_disc_cnt_t *);
+
+
+/*
+ * npi_rxdma_red_discard_stat_get
+ * Gets the current discrad count due RED
+ * The counter overflow bit is cleared, if it has been set.
+ *
+ * Inputs:
+ *	rdc:		RX DMA Channel number
+ *	rx_disc_cnt_t	Structure to write current RDC discard stat
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_RXDMA_RDC_INVALID
+ *
+ */
+
+npi_status_t npi_rxdma_red_discard_stat_get(npi_handle_t, uint8_t,
+				    rx_disc_cnt_t *);
+
+
+
+/*
+ * npi_rxdma_red_discard_oflow_clear
+ * Clear RED discard counter overflow bit
+ *
+ * Inputs:
+ *	rdc:		RX DMA Channel number
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_RXDMA_RDC_INVALID
+ *
+ */
+
+npi_status_t npi_rxdma_red_discard_oflow_clear(npi_handle_t,
+					uint8_t);
+
+
+
+
+/*
+ * npi_rxdma_misc_discard_stat_get
+ * Gets the current discrad count for the rdc due to
+ * buffer pool empty
+ * The counter overflow bit is cleared, if it has been set.
+ *
+ * Inputs:
+ *	rdc:		RX DMA Channel number
+ *	rx_disc_cnt_t	Structure to write current RDC discard stat
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_RXDMA_RDC_INVALID
+ *
+ */
+
+npi_status_t npi_rxdma_misc_discard_stat_get(npi_handle_t, uint8_t,
+				    rx_disc_cnt_t *);
+
+
+
+/*
+ * npi_rxdma_red_discard_oflow_clear
+ * Clear RED discard counter overflow bit
+ * clear the overflow bit for  buffer pool empty discrad counter
+ * for the rdc
+ *
+ *
+ * Inputs:
+ *	rdc:		RX DMA Channel number
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_RXDMA_RDC_INVALID
+ *
+ */
+
+npi_status_t npi_rxdma_misc_discard_oflow_clear(npi_handle_t,
+					uint8_t);
+
+
+
+/*
+ * npi_rxdma_ring_perr_stat_get
+ * Gets the current RDC Memory parity error
+ * The counter overflow bit is cleared, if it has been set.
+ *
+ * Inputs:
+ * pre_cnt:	Structure to write current RDC Prefetch memory
+ *		Parity Error stat
+ * sha_cnt:	Structure to write current RDC Shadow memory
+ *		Parity Error stat
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_RXDMA_RDC_INVALID
+ *
+ */
+
+npi_status_t npi_rxdma_ring_perr_stat_get(npi_handle_t,
+				    rdmc_par_err_log_t *,
+				    rdmc_par_err_log_t *);
+
+
+/*
+ * npi_rxdma_ring_perr_stat_get
+ * Clear RDC Memory Parity Error counter overflow bits
+ *
+ * Inputs:
+ * Return:
+ * NPI_SUCCESS
+ *
+ */
+
+npi_status_t npi_rxdma_ring_perr_stat_clear(npi_handle_t);
+
+
+/* Access the RDMC Memory: used for debugging */
+
+npi_status_t npi_rxdma_rdmc_memory_io(npi_handle_t,
+			    rdmc_mem_access_t *, uint8_t);
+
+
+
+/*
+ * npi_rxdma_rxctl_fifo_error_intr_set
+ * Configure The RX ctrl fifo error interrupt generation
+ *
+ * Inputs:
+ *	mask:	rx_ctl_dat_fifo_mask_t specifying the errors
+ *
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ *
+ */
+
+npi_status_t npi_rxdma_rxctl_fifo_error_intr_set(npi_handle_t,
+				    rx_ctl_dat_fifo_mask_t *);
+
+/*
+ * npi_rxdma_rxctl_fifo_error_status_get
+ * Read The RX ctrl fifo error Status
+ *
+ * Inputs:
+ *	stat:	rx_ctl_dat_fifo_stat_t to read the errors to
+ * valid fields in  rx_ctl_dat_fifo_stat_t structure are:
+ * zcp_eop_err, ipp_eop_err, id_mismatch.
+ * Return:
+ * NPI_SUCCESS
+ * NPI_FAILURE
+ *
+ */
+
+npi_status_t npi_rxdma_rxctl_fifo_error_status_get(npi_handle_t,
+				    rx_ctl_dat_fifo_stat_t *);
+
+
+/*
+ * npi_rxdma_channel_mex_set():
+ *	This function is called to arm the DMA channel with
+ *	mailbox updating capability. Software needs to rearm
+ *	for each update by writing to the control and status register.
+ *
+ * Parameters:
+ *	handle		- NPI handle (virtualization flag must be defined).
+ *	channel		- logical RXDMA channel from 0 to 23.
+ *			  (If virtualization flag is not set, then
+ *			   logical channel is the same as the hardware
+ *			   channel number).
+ *
+ * Return:
+ *	NPI_SUCCESS		- If enable channel with mailbox update
+ *				  is complete successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE	-
+ *		NPI_RXDMA_CHANNEL_INVALID -
+ */
+npi_status_t npi_rxdma_channel_mex_set(npi_handle_t, uint8_t);
+
+/*
+ * npi_rxdma_channel_rcrto_clear():
+ *	This function is called to reset RCRTO bit to 0.
+ *
+ * Parameters:
+ *	handle		- NPI handle (virtualization flag must be defined).
+ *	channel		- logical RXDMA channel from 0 to 23.
+ *			  (If virtualization flag is not set, then
+ *			   logical channel is the same as the hardware
+ *			   channel number).
+ * Return:
+ *	NPI_SUCCESS
+ *
+ *	Error:
+ *	NPI_FAILURE	-
+ *		NPI_RXDMA_CHANNEL_INVALID -
+ */
+npi_status_t npi_rxdma_channel_rcrto_clear(npi_handle_t, uint8_t);
+
+/*
+ * npi_rxdma_channel_pt_drop_pkt_clear():
+ *	This function is called to clear the port drop packet bit (debug).
+ *
+ * Parameters:
+ *	handle		- NPI handle (virtualization flag must be defined).
+ *	channel		- logical RXDMA channel from 0 to 23.
+ *			  (If virtualization flag is not set, then
+ *			   logical channel is the same as the hardware
+ *			   channel number).
+ * Return:
+ *	NPI_SUCCESS
+ *
+ *	Error:
+ *	NPI_FAILURE	-
+ *		NPI_RXDMA_CHANNEL_INVALID -
+ */
+npi_status_t npi_rxdma_channel_pt_drop_pkt_clear(npi_handle_t, uint8_t);
+
+/*
+ * npi_rxdma_channel_wred_drop_clear():
+ *	This function is called to wred drop bit (debug only).
+ *
+ * Parameters:
+ *	handle		- NPI handle (virtualization flag must be defined).
+ *	channel		- logical RXDMA channel from 0 to 23.
+ *			  (If virtualization flag is not set, then
+ *			   logical channel is the same as the hardware
+ *			   channel number).
+ * Return:
+ *	NPI_SUCCESS
+ *
+ *	Error:
+ *	NPI_FAILURE	-
+ *		NPI_RXDMA_CHANNEL_INVALID -
+ */
+npi_status_t npi_rxdma_channel_wred_drop_clear(npi_handle_t, uint8_t);
+
+/*
+ * npi_rxdma_channel_rcr_shfull_clear():
+ *	This function is called to clear RCR shadow full bit.
+ *
+ * Parameters:
+ *	handle		- NPI handle (virtualization flag must be defined).
+ *	channel		- logical RXDMA channel from 0 to 23.
+ *			  (If virtualization flag is not set, then
+ *			   logical channel is the same as the hardware
+ *			   channel number).
+ * Return:
+ *	NPI_SUCCESS
+ *
+ *	Error:
+ *	NPI_FAILURE	-
+ *		NPI_RXDMA_CHANNEL_INVALID -
+ */
+npi_status_t npi_rxdma_channel_rcr_shfull_clear(npi_handle_t, uint8_t);
+
+/*
+ * npi_rxdma_channel_rcrfull_clear():
+ *	This function is called to clear RCR full bit.
+ *
+ * Parameters:
+ *	handle		- NPI handle (virtualization flag must be defined).
+ *	channel		- logical RXDMA channel from 0 to 23.
+ *			  (If virtualization flag is not set, then
+ *			   logical channel is the same as the hardware
+ *			   channel number).
+ * Return:
+ *	NPI_SUCCESS
+ *
+ *	Error:
+ *	NPI_FAILURE	-
+ *		NPI_RXDMA_CHANNEL_INVALID -
+ */
+npi_status_t npi_rxdma_channel_rcrfull_clear(npi_handle_t, uint8_t);
+
+/*
+ * npi_rxdma_rbr_pre_empty_clear():
+ *	This function is called to control a receive DMA channel
+ *	for arming the channel with mailbox updates, resetting
+ *	various event status bits (control and status register).
+ *
+ * Parameters:
+ *	handle		- NPI handle (virtualization flag must be defined).
+ *	control		- NPI defined control type supported:
+ *				- RXDMA_MEX_SET
+ * 				- RXDMA_RCRTO_CLEAR
+ *				- RXDMA_PT_DROP_PKT_CLEAR
+ *				- RXDMA_WRED_DROP_CLEAR
+ *				- RXDMA_RCR_SFULL_CLEAR
+ *				- RXDMA_RCR_FULL_CLEAR
+ *				- RXDMA_RBR_PRE_EMPTY_CLEAR
+ *	channel		- logical RXDMA channel from 0 to 23.
+ *			  (If virtualization flag is not set, then
+ *			   logical channel is the same as the hardware.
+ * Return:
+ *	NPI_SUCCESS
+ *
+ *	Error:
+ *	NPI_FAILURE		-
+ *		NPI_RXDMA_CHANNEL_INVALID -
+ */
+npi_status_t npi_rxdma_channel_rbr_pre_empty_clear(npi_handle_t, uint8_t);
+
+/*
+ * npi_rxdma_channel_control():
+ *	This function is called to control a receive DMA channel
+ *	for arming the channel with mailbox updates, resetting
+ *	various event status bits (control and status register).
+ *
+ * Parameters:
+ *	handle		- NPI handle (virtualization flag must be defined).
+ *	control		- NPI defined control type supported:
+ *				- RXDMA_MEX_SET
+ * 				- RXDMA_RCRTO_CLEAR
+ *				- RXDMA_PT_DROP_PKT_CLEAR
+ *				- RXDMA_WRED_DROP_CLEAR
+ *				- RXDMA_RCR_SFULL_CLEAR
+ *				- RXDMA_RCR_FULL_CLEAR
+ *				- RXDMA_RBR_PRE_EMPTY_CLEAR
+ *	channel		- logical RXDMA channel from 0 to 23.
+ *			  (If virtualization flag is not set, then
+ *			   logical channel is the same as the hardware.
+ * Return:
+ *	NPI_SUCCESS
+ *
+ *	Error:
+ *	NPI_FAILURE		-
+ *		NPI_TXDMA_OPCODE_INVALID	-
+ *		NPI_TXDMA_CHANNEL_INVALID	-
+ */
+npi_status_t npi_rxdma_channel_control(npi_handle_t,
+				rxdma_cs_cntl_t, uint8_t);
+
+/*
+ * npi_rxdma_control_status():
+ *	This function is called to operate on the control
+ *	and status register.
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	op_mode		- OP_GET: get hardware control and status
+ *			  OP_SET: set hardware control and status
+ *			  OP_UPDATE: update hardware control and status.
+ *			  OP_CLEAR: clear control and status register to 0s.
+ *	channel		- hardware RXDMA channel from 0 to 23.
+ *	cs_p		- pointer to hardware defined control and status
+ *			  structure.
+ * Return:
+ *	NPI_SUCCESS
+ *
+ *	Error:
+ *	NPI_FAILURE		-
+ *		NPI_RXDMA_OPCODE_INVALID	-
+ *		NPI_RXDMA_CHANNEL_INVALID	-
+ */
+npi_status_t npi_rxdma_control_status(npi_handle_t, io_op_t,
+			uint8_t, p_rx_dma_ctl_stat_t);
+
+/*
+ * npi_rxdma_event_mask():
+ *	This function is called to operate on the event mask
+ *	register which is used for generating interrupts.
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	op_mode		- OP_GET: get hardware event mask
+ *			  OP_SET: set hardware interrupt event masks
+ *			  OP_CLEAR: clear control and status register to 0s.
+ *	channel		- hardware RXDMA channel from 0 to 23.
+ *	mask_p		- pointer to hardware defined event mask
+ *			  structure.
+ * Return:
+ *	NPI_SUCCESS		- If set is complete successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE		-
+ *		NPI_RXDMA_OPCODE_INVALID	-
+ *		NPI_RXDMA_CHANNEL_INVALID	-
+ */
+npi_status_t npi_rxdma_event_mask(npi_handle_t, io_op_t,
+		uint8_t, p_rx_dma_ent_msk_t);
+
+/*
+ * npi_rxdma_event_mask_config():
+ *	This function is called to operate on the event mask
+ *	register which is used for generating interrupts
+ *	and status register.
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	op_mode		- OP_GET: get hardware event mask
+ *			  OP_SET: set hardware interrupt event masks
+ *			  OP_CLEAR: clear control and status register to 0s.
+ *	channel		- hardware RXDMA channel from 0 to 23.
+ *	cfgp		- pointer to NPI defined event mask
+ *			  enum data type.
+ * Return:
+ *	NPI_SUCCESS		- If set is complete successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE		-
+ *		NPI_RXDMA_OPCODE_INVALID	-
+ *		NPI_RXDMA_CHANNEL_INVALID	-
+ */
+npi_status_t npi_rxdma_event_mask_config(npi_handle_t, io_op_t,
+		uint8_t, rxdma_ent_msk_cfg_t *);
+
+
+/*
+ * npi_rxdma_dump_rdc_regs
+ * Dumps the contents of rdc csrs and fzc registers
+ *
+ * Input:
+ *         rdc:      RX DMA number
+ *
+ * return:
+ *     NPI_SUCCESS
+ *     NPI_FAILURE
+ *     NPI_RXDMA_RDC_INVALID
+ *
+ */
+
+npi_status_t npi_rxdma_dump_rdc_regs(npi_handle_t, uint8_t);
+
+
+/*
+ * npi_rxdma_dump_fzc_regs
+ * Dumps the contents of rdc csrs and fzc registers
+ *
+ * Input:
+ *         rdc:      RX DMA number
+ *
+ * return:
+ *     NPI_SUCCESS
+ *     NPI_FAILURE
+ *     NPI_RXDMA_RDC_INVALID
+ *
+ */
+
+npi_status_t npi_rxdma_dump_fzc_regs(npi_handle_t);
+
+npi_status_t npi_rxdma_channel_rbr_empty_clear(npi_handle_t,
+							uint8_t);
+npi_status_t npi_rxdma_rxctl_fifo_error_intr_get(npi_handle_t,
+				rx_ctl_dat_fifo_stat_t *);
+
+npi_status_t npi_rxdma_rxctl_fifo_error_intr_set(npi_handle_t,
+				rx_ctl_dat_fifo_mask_t *);
+
+npi_status_t npi_rxdma_dump_rdc_table(npi_handle_t, uint8_t);
+#ifdef	__cplusplus
+}
+#endif
+
+#endif	/* _NPI_RXDMA_H */
diff --git a/drivers/net/nxge/npi/npi_txc.c b/drivers/net/nxge/npi/npi_txc.c
new file mode 100644
index 0000000..47dadc5
--- /dev/null
+++ b/drivers/net/nxge/npi/npi_txc.c
@@ -0,0 +1,1157 @@
+/*
+ * npi_txc.c	Neptune TX Controller HW API functions
+ *
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+
+#include <npi_txc.h>
+
+/*
+ * Transmit Controller (TXC) Functions.
+ */
+
+uint64_t txc_fzc_dmc_offset[] = {
+	TXC_DMA_MAX_BURST_REG,
+	TXC_DMA_MAX_LENGTH_REG
+};
+
+const char *txc_fzc_dmc_name[] = {
+	"TXC_DMA_MAX_BURST_REG",
+	"TXC_DMA_MAX_LENGTH_REG"
+};
+
+uint64_t txc_fzc_offset [] = {
+	TXC_CONTROL_REG,
+	TXC_TRAINING_REG,
+	TXC_DEBUG_SELECT_REG,
+	TXC_MAX_REORDER_REG,
+	TXC_INT_STAT_DBG_REG,
+	TXC_INT_STAT_REG,
+	TXC_INT_MASK_REG
+};
+
+const char *txc_fzc_name [] = {
+	"TXC_CONTROL_REG",
+	"TXC_TRAINING_REG",
+	"TXC_DEBUG_SELECT_REG",
+	"TXC_MAX_REORDER_REG",
+	"TXC_INT_STAT_DBG_REG",
+	"TXC_INT_STAT_REG",
+	"TXC_INT_MASK_REG"
+};
+
+uint64_t txc_fzc_port_offset[] = {
+	TXC_PORT_CTL_REG,
+	TXC_PORT_DMA_ENABLE_REG,
+	TXC_PKT_STUFFED_REG,
+	TXC_PKT_XMIT_REG,
+	TXC_ROECC_CTL_REG,
+	TXC_ROECC_ST_REG,
+	TXC_RO_DATA0_REG,
+	TXC_RO_DATA1_REG,
+	TXC_RO_DATA2_REG,
+	TXC_RO_DATA3_REG,
+	TXC_RO_DATA4_REG,
+	TXC_SFECC_CTL_REG,
+	TXC_SFECC_ST_REG,
+	TXC_SF_DATA0_REG,
+	TXC_SF_DATA1_REG,
+	TXC_SF_DATA2_REG,
+	TXC_SF_DATA3_REG,
+	TXC_SF_DATA4_REG,
+	TXC_RO_TIDS_REG,
+	TXC_RO_STATE0_REG,
+	TXC_RO_STATE1_REG,
+	TXC_RO_STATE2_REG,
+	TXC_RO_STATE3_REG,
+	TXC_RO_CTL_REG,
+	TXC_RO_ST_DATA0_REG,
+	TXC_RO_ST_DATA1_REG,
+	TXC_RO_ST_DATA2_REG,
+	TXC_RO_ST_DATA3_REG,
+	TXC_PORT_PACKET_REQ_REG
+};
+
+const char *txc_fzc_port_name[] = {
+	"TXC_PORT_CTL_REG",
+	"TXC_PORT_DMA_ENABLE_REG",
+	"TXC_PKT_STUFFED_REG",
+	"TXC_PKT_XMIT_REG",
+	"TXC_ROECC_CTL_REG",
+	"TXC_ROECC_ST_REG",
+	"TXC_RO_DATA0_REG",
+	"TXC_RO_DATA1_REG",
+	"TXC_RO_DATA2_REG",
+	"TXC_RO_DATA3_REG",
+	"TXC_RO_DATA4_REG",
+	"TXC_SFECC_CTL_REG",
+	"TXC_SFECC_ST_REG",
+	"TXC_SF_DATA0_REG",
+	"TXC_SF_DATA1_REG",
+	"TXC_SF_DATA2_REG",
+	"TXC_SF_DATA3_REG",
+	"TXC_SF_DATA4_REG",
+	"TXC_RO_TIDS_REG",
+	"TXC_RO_STATE0_REG",
+	"TXC_RO_STATE1_REG",
+	"TXC_RO_STATE2_REG",
+	"TXC_RO_STATE3_REG",
+	"TXC_RO_CTL_REG",
+	"TXC_RO_ST_DATA0_REG",
+	"TXC_RO_ST_DATA1_REG",
+	"TXC_RO_ST_DATA2_REG",
+	"TXC_RO_ST_DATA3_REG",
+	"TXC_PORT_PACKET_REQ_REG"
+};
+
+/*
+ * npi_txc_dump_tdc_fzc_regs
+ * Dumps the contents of TXC csrs and fzc registers
+ *
+ * Input:
+ *         tdc:      TX DMA number
+ *
+ * return:
+ *     NPI_SUCCESS
+ *     NPI_FAILURE
+ *     NPI_TXC_CHANNEL_INVALID
+ *
+ */
+npi_status_t
+npi_txc_dump_tdc_fzc_regs(npi_handle_t handle, uint8_t tdc)
+{
+	uint64_t		value, offset;
+	int 			num_regs, i;
+
+	if (!TXDMA_CHANNEL_VALID(tdc)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			"npi_txc_dump_tdc_fzc_regs"
+			" Invalid TDC number %d \n",
+			tdc));
+		return (NPI_FAILURE | NPI_TXC_CHANNEL_INVALID(tdc));
+	}
+
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+		    "\nTXC FZC DMC Register Dump for Channel %d\n",
+			    tdc));
+
+	num_regs = sizeof (txc_fzc_dmc_offset) / sizeof (uint64_t);
+	for (i = 0; i < num_regs; i++) {
+		offset = TXC_FZC_REG_CN_OFFSET(txc_fzc_dmc_offset[i], tdc);
+		NXGE_REG_RD64(handle, offset, &value);
+		NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL, "0x%08llx "
+			"%s\t 0x%08llx \n",
+			offset, txc_fzc_dmc_name[i], value));
+	}
+
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+		"\n TXC FZC Register Dump for Channel %d done\n", tdc));
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_txc_dump_fzc_regs
+ * Dumps the contents of txc csrs and fzc registers
+ *
+ *
+ * return:
+ *     NPI_SUCCESS
+ *     NPI_FAILURE
+ *
+ */
+npi_status_t
+npi_txc_dump_fzc_regs(npi_handle_t handle)
+{
+
+	uint64_t value;
+	int num_regs, i;
+
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+		"\nTXC FZC Common Register Dump\n"));
+
+	num_regs = sizeof (txc_fzc_offset) / sizeof (uint64_t);
+	for (i = 0; i < num_regs; i++) {
+		NXGE_REG_RD64(handle, txc_fzc_offset[i], &value);
+		NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL, "0x%08llx "
+			"%s\t 0x%08llx \n",
+			txc_fzc_offset[i], txc_fzc_name[i], value));
+	}
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+		"\n TXC FZC Common Register Dump Done \n"));
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_txc_dump_port_fzc_regs
+ * Dumps the contents of TXC csrs and fzc registers
+ *
+ * Input:
+ *         port:      port number
+ *
+ * return:
+ *     NPI_SUCCESS
+ *     NPI_FAILURE
+ *
+ */
+npi_status_t
+npi_txc_dump_port_fzc_regs(npi_handle_t handle, uint8_t port)
+{
+	uint64_t		value, offset;
+	int 			num_regs, i;
+
+	if (!IS_PORT_NUM_VALID(port)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_txc_dump_port_fzc"
+			" Invalid Input: port <%d>", port));
+		return (NPI_FAILURE | NPI_TXC_PORT_INVALID(port));
+	}
+
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+		"\nTXC FZC PORT Register Dump for port %d\n", port));
+
+	num_regs = sizeof (txc_fzc_port_offset) / sizeof (uint64_t);
+	for (i = 0; i < num_regs; i++) {
+		offset = TXC_FZC_REG_PT_OFFSET(txc_fzc_port_offset[i], port);
+		NXGE_REG_RD64(handle, offset, &value);
+		NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL, "0x%08llx "
+			"%s\t 0x%08llx \n",
+			offset, txc_fzc_port_name[i], value));
+	}
+
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+		"\n TXC FZC Register Dump for port %d done\n", port));
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_txc_dma_max_burst():
+ *	This function is called to configure the max burst bytes.
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	op_mode		- OP_GET: get max burst value
+ *			- OP_SET: set max burst value
+ *	channel		- channel number (0 - 23)
+ *	dma_max_burst_p - pointer to store or used for max burst value.
+ * Return:
+ *	NPI_SUCCESS	- If operation is complete successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE	-
+ *		NPI_TXC_OPCODE_INVALID
+ *		NPI_TXC_CHANNEL_INVALID
+ */
+npi_status_t
+npi_txc_dma_max_burst(npi_handle_t handle, io_op_t op_mode, uint8_t channel,
+		uint32_t *dma_max_burst_p)
+{
+	uint64_t val;
+
+	if (!TXDMA_CHANNEL_VALID(channel)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txc_dma_max_burst"
+				    " Invalid Input: channel <0x%x>",
+				    channel));
+		return (NPI_FAILURE | NPI_TXC_CHANNEL_INVALID(channel));
+	}
+
+	switch (op_mode) {
+	case OP_GET:
+		TXC_FZC_REG_READ64(handle, TXC_DMA_MAX_BURST_REG, channel,
+					&val);
+		*dma_max_burst_p = (uint32_t)val;
+		break;
+
+	case OP_SET:
+		TXC_FZC_REG_WRITE64(handle,
+			TXC_DMA_MAX_BURST_REG, channel, *dma_max_burst_p);
+		break;
+
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txc_dma_max_burst"
+				    " Invalid Input: burst <0x%x>",
+				    op_mode));
+		return (NPI_FAILURE | NPI_TXC_OPCODE_INVALID(channel));
+	}
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_txc_dma_max_burst_set():
+ *	This function is called to set the max burst bytes.
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	channel		- channel number (0 - 23)
+ *	max_burst 	- max burst to set
+ * Return:
+ *	NPI_SUCCESS	- If operation is complete successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE	-
+ */
+npi_status_t
+npi_txc_dma_max_burst_set(npi_handle_t handle, uint8_t channel,
+		uint32_t max_burst)
+{
+	if (!TXDMA_CHANNEL_VALID(channel)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txc_dma_max_burst_set"
+				    " Invalid Input: channel <0x%x>",
+				    channel));
+		return (NPI_FAILURE | NPI_TXC_CHANNEL_INVALID(channel));
+	}
+
+	TXC_FZC_REG_WRITE64(handle, TXC_DMA_MAX_BURST_REG,
+		channel, (uint64_t)max_burst);
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_txc_dma_bytes_transmitted():
+ *	This function is called to get # of bytes transmitted by
+ *	DMA (hardware register is cleared on read).
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	channel		- channel number (0 - 23)
+ *	dma_bytes_p 	- pointer to store bytes transmitted.
+ * Return:
+ *	NPI_SUCCESS	- If get is complete successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE	-
+ *		NPI_TXC_PORT_INVALID
+ */
+npi_status_t
+npi_txc_dma_bytes_transmitted(npi_handle_t handle, uint8_t channel,
+		uint32_t *dma_bytes_p)
+{
+	uint64_t val;
+
+	if (!TXDMA_CHANNEL_VALID(channel)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txc_dma_bytes_transmitted"
+				    " Invalid Input: channel %d",
+				    channel));
+		return (NPI_FAILURE | NPI_TXC_CHANNEL_INVALID(channel));
+	}
+
+	TXC_FZC_REG_READ64(handle, TXC_DMA_MAX_LENGTH_REG, channel, &val);
+	*dma_bytes_p = (uint32_t)val;
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_txc_control():
+ *	This function is called to get or set the control register.
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	op_mode		- OP_GET: get control register value
+ *			  OP_SET: set control register value
+ *	txc_control_p	- pointer to hardware defined data structure.
+ * Return:
+ *	NPI_SUCCESS	- If operation is complete successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE	-
+ *		NPI_TXC_OPCODE_INVALID
+ *		NPI_TXC_PORT_INVALID
+ */
+npi_status_t
+npi_txc_control(npi_handle_t handle, io_op_t op_mode,
+		p_txc_control_t txc_control_p)
+{
+	switch (op_mode) {
+	case OP_GET:
+		NXGE_REG_RD64(handle, TXC_CONTROL_REG, &txc_control_p->value);
+		break;
+
+	case OP_SET:
+		NXGE_REG_WR64(handle, TXC_CONTROL_REG,
+			txc_control_p->value);
+		break;
+
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txc_control"
+				    " Invalid Input:  control 0x%x",
+				    op_mode));
+		return (NPI_FAILURE | NPI_TXC_OPCODE_INVALID(op_mode));
+	}
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_txc_global_enable():
+ *	This function is called to globally enable TXC.
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ * Return:
+ *	NPI_SUCCESS	- If enable is complete successfully.
+ *
+ *	Error:
+ */
+npi_status_t
+npi_txc_global_enable(npi_handle_t handle)
+{
+	txc_control_t	cntl;
+	uint64_t	val;
+
+	cntl.value = 0;
+	cntl.bits.ldw.txc_enabled = 1;
+
+	NXGE_REG_RD64(handle, TXC_CONTROL_REG, &val);
+	NXGE_REG_WR64(handle, TXC_CONTROL_REG, val | cntl.value);
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_txc_global_disable():
+ *	This function is called to globally disable TXC.
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ * Return:
+ *	NPI_SUCCESS	- If disable is complete successfully.
+ *
+ *	Error:
+ */
+npi_status_t
+npi_txc_global_disable(npi_handle_t handle)
+{
+	txc_control_t	cntl;
+	uint64_t	val;
+
+
+	cntl.value = 0;
+	cntl.bits.ldw.txc_enabled = 0;
+
+	NXGE_REG_RD64(handle, TXC_CONTROL_REG, &val);
+	NXGE_REG_WR64(handle, TXC_CONTROL_REG, val | cntl.value);
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_txc_control_clear():
+ *	This function is called to clear all bits.
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ * Return:
+ *	NPI_SUCCESS	- If reset all bits to 0s is complete successfully.
+ *
+ *	Error:
+ */
+npi_status_t
+npi_txc_control_clear(npi_handle_t handle, uint8_t port)
+{
+	if (!IS_PORT_NUM_VALID(port)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			    " npi_txc_port_control_clear"
+			    " Invalid Input: port <%d>", port));
+		return (NPI_FAILURE | NPI_TXC_PORT_INVALID(port));
+	}
+
+	NXGE_REG_WR64(handle, TXC_PORT_CTL_REG, TXC_PORT_CNTL_CLEAR);
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_txc_training_set():
+ *	This function is called to set the debug training vector.
+ *
+ * Parameters:
+ *	handle			- NPI handle
+ *	vector			- training vector to set.
+ * Return:
+ *	NPI_SUCCESS
+ *
+ *	Error:
+ *	NPI_FAILURE		-
+ */
+npi_status_t
+npi_txc_training_set(npi_handle_t handle, uint32_t vector)
+{
+	NXGE_REG_WR64(handle, TXC_TRAINING_REG, (uint64_t)vector);
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_txc_training_get():
+ *	This function is called to get the debug training vector.
+ *
+ * Parameters:
+ *	handle			- NPI handle
+ *	vector_p		- pointer to store training vector.
+ * Return:
+ *	NPI_SUCCESS
+ *
+ *	Error:
+ *	NPI_FAILURE		-
+ */
+npi_status_t
+npi_txc_training_get(npi_handle_t handle, uint32_t *vector_p)
+{
+	uint64_t val;
+
+	NXGE_REG_RD64(handle, (TXC_TRAINING_REG & TXC_TRAINING_VECTOR_MASK),
+			&val);
+	*vector_p = (uint32_t)val;
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_txc_port_enable():
+ *	This function is called to enable a particular port.
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	port		- port number (0 - 3)
+ * Return:
+ *	NPI_SUCCESS	- If port is enabled successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE	-
+ *		NPI_TXC_PORT_INVALID
+ */
+npi_status_t
+npi_txc_port_enable(npi_handle_t handle, uint8_t port)
+{
+	uint64_t val;
+
+	if (!IS_PORT_NUM_VALID(port)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txc_port_enable:",
+				    " Invalid Input port <%d>", port));
+		return (NPI_FAILURE | NPI_TXC_PORT_INVALID(port));
+	}
+
+	NXGE_REG_RD64(handle, TXC_CONTROL_REG, &val);
+	NXGE_REG_WR64(handle, TXC_CONTROL_REG, val | (1 << port));
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_txc_port_disable():
+ *	This function is called to disable a particular port.
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	port		- port number (0 - 3)
+ * Return:
+ *	NPI_SUCCESS	- If port is disabled successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE	-
+ *		NPI_TXC_PORT_INVALID
+ */
+npi_status_t
+npi_txc_port_disable(npi_handle_t handle, uint8_t port)
+{
+	uint64_t val;
+
+	if (!IS_PORT_NUM_VALID(port)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txc_port_disable",
+				    " Invalid Input: port <%d>", port));
+		return (NPI_FAILURE | NPI_TXC_PORT_INVALID(port));
+	}
+
+	NXGE_REG_RD64(handle, TXC_CONTROL_REG, &val);
+	NXGE_REG_WR64(handle, TXC_CONTROL_REG, (val & ~(1 << port)));
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_txc_port_dma_enable():
+ *	This function is called to bind DMA channels (bitmap) to a port.
+ *
+ * Parameters:
+ *	handle			- NPI handle
+ *	port			- port number (0 - 3)
+ *	port_dma_list_bitmap	- channels bitmap
+ *				(1 to bind, 0 - 23 bits one bit/channel)
+ * Return:
+ *	NPI_SUCCESS		- If channels are bound successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE	-
+ *		NPI_TXC_PORT_INVALID
+ */
+npi_status_t
+npi_txc_port_dma_enable(npi_handle_t handle, uint8_t port,
+		uint32_t port_dma_list_bitmap)
+{
+
+	if (!IS_PORT_NUM_VALID(port)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txc_port_dma_enable",
+				    " Invalid Input: port", port));
+		return (NPI_FAILURE | NPI_TXC_PORT_INVALID(port));
+	}
+
+	TXC_FZC_CNTL_REG_WRITE64(handle, TXC_PORT_DMA_ENABLE_REG, port,
+		port_dma_list_bitmap);
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_txc_port_dma_list_get(npi_handle_t handle, uint8_t port,
+		uint32_t *port_dma_list_bitmap)
+{
+	uint64_t val;
+
+	if (!IS_PORT_NUM_VALID(port)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txc_port_dma_list_get"
+				    " Invalid Input: port <%d>", port));
+		return (NPI_FAILURE | NPI_TXC_PORT_INVALID(port));
+	}
+
+	TXC_FZC_CNTL_REG_READ64(handle, TXC_PORT_DMA_ENABLE_REG, port, &val);
+	*port_dma_list_bitmap = (uint32_t)(val & TXC_DMA_DMA_LIST_MASK);
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_txc_port_dma_channel_enable():
+ *	This function is called to bind a channel to a port.
+ *
+ * Parameters:
+ *	handle			- NPI handle
+ *	port			- port number (0 - 3)
+ *	channel			- channel number (0 - 23)
+ * Return:
+ *	NPI_SUCCESS		- If channel is bound successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE		-
+ *		NPI_TXC_PORT_INVALID	-
+ */
+npi_status_t
+npi_txc_port_dma_channel_enable(npi_handle_t handle, uint8_t port,
+		uint8_t channel)
+{
+	uint64_t val;
+
+	if (!IS_PORT_NUM_VALID(port)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txc_port_dma_enable"
+				    " Invalid Input: port <%d>", port));
+		return (NPI_FAILURE | NPI_TXC_PORT_INVALID(port));
+	}
+	if (!TXDMA_CHANNEL_VALID(channel)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txc_port_dma_channel_enable"
+				    " Invalid Input: channel <0x%x>", channel));
+		return (NPI_FAILURE | NPI_TXC_CHANNEL_INVALID(channel));
+	}
+
+	TXC_FZC_CNTL_REG_READ64(handle, TXC_PORT_DMA_ENABLE_REG, port, &val);
+	TXC_FZC_CNTL_REG_WRITE64(handle, TXC_PORT_DMA_ENABLE_REG, port,
+				(val | (1 << channel)));
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_txc_port_dma_channel_disable():
+ *	This function is called to unbind a channel to a port.
+ *
+ * Parameters:
+ *	handle			- NPI handle
+ *	port			- port number (0 - 3)
+ *	channel			- channel number (0 - 23)
+ * Return:
+ *	NPI_SUCCESS		- If channel is unbound successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE		-
+ *		NPI_TXC_PORT_INVALID	-
+ */
+npi_status_t
+npi_txc_port_dma_channel_disable(npi_handle_t handle, uint8_t port,
+		uint8_t channel)
+{
+	uint64_t val;
+
+	if (!IS_PORT_NUM_VALID(port)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txc_port_dma_disable"
+				    " Invalid Input: port <%d>", port));
+		return (NPI_FAILURE | NPI_TXC_PORT_INVALID(port));
+	}
+
+	if (!TXDMA_CHANNEL_VALID(channel)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txc_port_dma_channel_disable"
+				    " Invalid Input: channel <0x%x>", channel));
+		return (NPI_FAILURE | NPI_TXC_CHANNEL_INVALID(channel));
+	}
+
+	TXC_FZC_CNTL_REG_READ64(handle, TXC_PORT_DMA_ENABLE_REG, port, &val)
+	TXC_FZC_CNTL_REG_WRITE64(handle, TXC_PORT_DMA_ENABLE_REG, port,
+				val & ~(1 << channel));
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_txc_max_reorder_set():
+ *	This function is called to set the per port reorder resources
+ *
+ * Parameters:
+ *	handle			- NPI handle
+ *	port			- port to set
+ *	reorder			- reorder resources (4 bits)
+ * Return:
+ *	NPI_SUCCESS
+ *
+ *	Error:
+ *	NPI_FAILURE		-
+ */
+npi_status_t
+npi_txc_reorder_set(npi_handle_t handle, uint8_t port, uint8_t *reorder)
+{
+	uint64_t val;
+
+	if (!IS_PORT_NUM_VALID(port)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			    " npi_txc_reorder_set"
+			    " Invalid Input: port <%d>", port));
+		return (NPI_FAILURE | NPI_TXC_PORT_INVALID(port));
+	}
+
+	/* XXXX sanity check */
+	NXGE_REG_RD64(handle, TXC_MAX_REORDER_REG, &val);
+
+	val |= (*reorder << TXC_MAX_REORDER_SHIFT(port));
+
+	NXGE_REG_WR64(handle, TXC_MAX_REORDER_REG, val);
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_txc_reorder_get():
+ *	This function is called to get the txc reorder resources.
+ *
+ * Parameters:
+ *	handle			- NPI handle
+ *	port			- port to get
+ *	reorder			- data to be stored at
+ * Return:
+ *	NPI_SUCCESS
+ *
+ *	Error:
+ *	NPI_FAILURE		-
+ */
+npi_status_t
+npi_txc_reorder_get(npi_handle_t handle, uint8_t port, uint32_t *reorder)
+{
+	uint64_t val;
+
+	if (!IS_PORT_NUM_VALID(port)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			    " npi_txc_reorder_get"
+			    " Invalid Input: port <%d>", port));
+		return (NPI_FAILURE | NPI_TXC_PORT_INVALID(port));
+	}
+
+	/* XXXX error check on reorder */
+
+
+	NXGE_REG_RD64(handle, TXC_MAX_REORDER_REG, &val);
+
+	*reorder = (uint8_t)(val >> TXC_MAX_REORDER_SHIFT(port));
+
+	return (NPI_SUCCESS);
+}
+
+
+/*
+ * npi_txc_pkt_stuffed_get():
+ *	This function is called to get total # of packets processed
+ *	by reorder engine and packetAssy engine.
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	port		- port number (0 - 3)
+ *	pkt_assy_p 	- packets processed by Assy engine.
+ *	pkt_reorder_p	- packets processed by reorder engine.
+ *
+ * Return:
+ *	NPI_SUCCESS	- If get is complete successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE	-
+ *		NPI_TXC_PORT_INVALID
+ */
+npi_status_t
+npi_txc_pkt_stuffed_get(npi_handle_t handle, uint8_t port,
+		uint32_t *pkt_assy_p, uint32_t *pkt_reorder_p)
+{
+	uint64_t		value;
+
+	if (!IS_PORT_NUM_VALID(port)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txc_pkt_stuffed_get"
+				    " Invalid Input: port <%d>", port));
+		return (NPI_FAILURE | NPI_TXC_PORT_INVALID(port));
+	}
+
+	TXC_FZC_CNTL_REG_READ64(handle, TXC_PKT_STUFFED_REG, port, &value);
+	*pkt_assy_p = ((uint32_t)((value & TXC_PKT_STUFF_PKTASY_MASK) >>
+		TXC_PKT_STUFF_PKTASY_SHIFT));
+	*pkt_reorder_p = ((uint32_t)((value & TXC_PKT_STUFF_REORDER_MASK) >>
+		TXC_PKT_STUFF_REORDER_SHIFT));
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_txc_pkt_xmt_to_mac_get():
+ *	This function is called to get total # of packets transmitted
+ *	to the MAC.
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	port		- port number (0 - 3)
+ *	mac_bytes_p 	- bytes transmitted to the MAC.
+ *	mac_pkts_p	- packets transmitted to the MAC.
+ *
+ * Return:
+ *	NPI_SUCCESS	- If get is complete successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE	-
+ *	NPI_TXC_PORT_INVALID
+ */
+npi_status_t
+npi_txc_pkt_xmt_to_mac_get(npi_handle_t handle, uint8_t port,
+		uint32_t *mac_bytes_p, uint32_t *mac_pkts_p)
+{
+	uint64_t		value;
+
+	if (!IS_PORT_NUM_VALID(port)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txc_xmt_to_mac_get"
+				    " Invalid Input: port <%d>", port));
+		return (NPI_FAILURE | NPI_TXC_PORT_INVALID(port));
+	}
+
+	TXC_FZC_CNTL_REG_READ64(handle, TXC_PKT_XMIT_REG, port, &value);
+	*mac_pkts_p = ((uint32_t)((value & TXC_PKTS_XMIT_MASK) >>
+		TXC_PKTS_XMIT_SHIFT));
+	*mac_bytes_p = ((uint32_t)((value & TXC_BYTES_XMIT_MASK) >>
+		TXC_BYTES_XMIT_SHIFT));
+
+	return (NPI_SUCCESS);
+}
+
+
+/*
+ * npi_txc_get_ro_states():
+ *	This function is called to get TXC's reorder state-machine states.
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	port		- port number
+ *	*states		- TXC Re-order states.
+ *
+ * Return:
+ *	NPI_SUCCESS	- If get is complete successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE	-
+ *	NPI_TXC_PORT_INVALID
+ */
+npi_status_t
+npi_txc_ro_states_get(npi_handle_t handle, uint8_t port,
+				txc_ro_states_t *states)
+{
+	txc_ro_ctl_t	ctl;
+	txc_ro_tids_t	tids;
+	txc_ro_state0_t	s0;
+	txc_ro_state1_t	s1;
+	txc_ro_state2_t	s2;
+	txc_ro_state3_t	s3;
+	txc_roecc_st_t	ecc;
+	txc_ro_data0_t	d0;
+	txc_ro_data1_t	d1;
+	txc_ro_data2_t	d2;
+	txc_ro_data3_t	d3;
+	txc_ro_data4_t	d4;
+
+	if (!IS_PORT_NUM_VALID(port)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txc_ro_states_get"
+				    " Invalid Input: port <%d>", port));
+		return (NPI_FAILURE | NPI_TXC_PORT_INVALID(port));
+	}
+
+	TXC_FZC_CNTL_REG_READ64(handle, TXC_ROECC_ST_REG, port, &ecc.value);
+	if ((ecc.bits.ldw.correct_error) || (ecc.bits.ldw.uncorrect_error)) {
+		TXC_FZC_CNTL_REG_READ64(handle, TXC_RO_DATA0_REG, port,
+								&d0.value);
+		TXC_FZC_CNTL_REG_READ64(handle, TXC_RO_DATA1_REG, port,
+								&d1.value);
+		TXC_FZC_CNTL_REG_READ64(handle, TXC_RO_DATA2_REG, port,
+								&d2.value);
+		TXC_FZC_CNTL_REG_READ64(handle, TXC_RO_DATA3_REG, port,
+								&d3.value);
+		TXC_FZC_CNTL_REG_READ64(handle, TXC_RO_DATA4_REG, port,
+								&d4.value);
+		states->d0.value = d0.value;
+		states->d1.value = d1.value;
+		states->d2.value = d2.value;
+		states->d3.value = d3.value;
+		states->d4.value = d4.value;
+
+		ecc.bits.ldw.ecc_address = 0;
+		ecc.bits.ldw.correct_error = 0;
+		ecc.bits.ldw.uncorrect_error = 0;
+		ecc.bits.ldw.clr_st = 1;
+		TXC_FZC_CNTL_REG_WRITE64(handle, TXC_ROECC_ST_REG, port,
+						ecc.value);
+	}
+
+	TXC_FZC_CNTL_REG_READ64(handle, TXC_RO_CTL_REG, port, &ctl.value);
+	TXC_FZC_CNTL_REG_READ64(handle, TXC_RO_STATE0_REG, port, &s0.value);
+	TXC_FZC_CNTL_REG_READ64(handle, TXC_RO_STATE1_REG, port, &s1.value);
+	TXC_FZC_CNTL_REG_READ64(handle, TXC_RO_STATE2_REG, port, &s2.value);
+	TXC_FZC_CNTL_REG_READ64(handle, TXC_RO_STATE3_REG, port, &s3.value);
+	TXC_FZC_CNTL_REG_READ64(handle, TXC_RO_TIDS_REG, port, &tids.value);
+
+	states->roecc.value = ctl.value;
+	states->st0.value = s0.value;
+	states->st1.value = s1.value;
+	states->st2.value = s2.value;
+	states->st3.value = s3.value;
+	states->ctl.value = ctl.value;
+	states->tids.value = tids.value;
+
+	ctl.bits.ldw.clr_fail_state = 1;
+	TXC_FZC_CNTL_REG_WRITE64(handle, TXC_RO_CTL_REG, port, ctl.value);
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_txc_ro_ecc_state_clr(npi_handle_t handle, uint8_t port)
+{
+	if (!IS_PORT_NUM_VALID(port)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txc_ro_ecc_state_clr"
+				    " Invalid Input: port <%d>", port));
+		return (NPI_FAILURE | NPI_TXC_PORT_INVALID(port));
+	}
+
+	TXC_FZC_CNTL_REG_WRITE64(handle, TXC_ROECC_ST_REG, port, 0);
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_txc_sf_states_get():
+ *	This function is called to get TXC's store-forward state-machine states.
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	port		- port number
+ *	states		- TXC Store-forward states
+ *
+ * Return:
+ *	NPI_SUCCESS	- If get is complete successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE	-
+ *	NPI_TXC_PORT_INVALID
+ */
+
+#ifdef lint
+/*ARGSUSED*/
+#endif
+
+npi_status_t
+npi_txc_sf_states_get(npi_handle_t handle, uint8_t port,
+				txc_sf_states_t *states)
+{
+	txc_sfecc_st_t	ecc;
+	txc_sf_data0_t	d0;
+	txc_sf_data1_t	d1;
+	txc_sf_data2_t	d2;
+	txc_sf_data3_t	d3;
+	txc_sf_data4_t	d4;
+
+	if (!IS_PORT_NUM_VALID(port)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txc_sf_states_get"
+				    " Invalid Input: port <%d>", port));
+		return (NPI_FAILURE | NPI_TXC_PORT_INVALID(port));
+	}
+
+	TXC_FZC_CNTL_REG_READ64(handle, TXC_SFECC_ST_REG, port, &ecc.value);
+	if ((ecc.bits.ldw.correct_error) || (ecc.bits.ldw.uncorrect_error)) {
+		TXC_FZC_CNTL_REG_READ64(handle, TXC_SF_DATA0_REG, port,
+								&d0.value);
+		TXC_FZC_CNTL_REG_READ64(handle, TXC_SF_DATA1_REG, port,
+								&d1.value);
+		TXC_FZC_CNTL_REG_READ64(handle, TXC_SF_DATA2_REG, port,
+								&d2.value);
+		TXC_FZC_CNTL_REG_READ64(handle, TXC_SF_DATA3_REG, port,
+								&d3.value);
+		TXC_FZC_CNTL_REG_READ64(handle, TXC_SF_DATA4_REG, port,
+								&d4.value);
+		ecc.bits.ldw.ecc_address = 0;
+		ecc.bits.ldw.correct_error = 0;
+		ecc.bits.ldw.uncorrect_error = 0;
+		ecc.bits.ldw.clr_st = 1;
+		TXC_FZC_CNTL_REG_WRITE64(handle, TXC_SFECC_ST_REG, port,
+						ecc.value);
+	}
+
+	states->sfecc.value = ecc.value;
+	states->d0.value = d0.value;
+	states->d1.value = d1.value;
+	states->d2.value = d2.value;
+	states->d3.value = d3.value;
+	states->d4.value = d4.value;
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_txc_sf_ecc_state_clr(npi_handle_t handle, uint8_t port)
+{
+	if (!IS_PORT_NUM_VALID(port)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txc_sf_ecc_state_clr"
+				    " Invalid Input: port <%d>", port));
+		return (NPI_FAILURE | NPI_TXC_PORT_INVALID(port));
+	}
+
+	TXC_FZC_CNTL_REG_WRITE64(handle, TXC_SFECC_ST_REG, port, 0);
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_txc_global_istatus_get():
+ *	This function is called to get TXC's global interrupt status.
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	istatus		- TXC global interrupt status
+ *
+ * Return:
+ */
+
+void
+npi_txc_global_istatus_get(npi_handle_t handle, txc_int_stat_t *istatus)
+{
+	txc_int_stat_t	status;
+
+	NXGE_REG_RD64(handle, TXC_INT_STAT_REG, &status.value);
+
+	istatus->value = status.value;
+}
+
+/*
+ * npi_txc_global_istatus_clear():
+ *	This function is called to clear TXC's global interrupt status.
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	istatus		- TXC global interrupt status
+ *
+ * Return:
+ */
+
+void
+npi_txc_global_istatus_clear(npi_handle_t handle, uint64_t istatus)
+{
+	NXGE_REG_WR64(handle, TXC_INT_STAT_REG, istatus);
+}
+
+void
+npi_txc_global_imask_set(npi_handle_t handle, uint8_t portn, uint8_t istatus)
+{
+	uint64_t val;
+
+	NXGE_REG_RD64(handle, TXC_INT_MASK_REG, &val);
+	switch (portn) {
+	case 0:
+		val &= 0xFFFFFF00;
+		val |= istatus & 0x3F;
+		break;
+	case 1:
+		val &= 0xFFFF00FF;
+		val |= (istatus << 8) & 0x3F00;
+		break;
+	case 2:
+		val &= 0xFF00FFFF;
+		val |= (istatus << 16) & 0x3F0000;
+		break;
+	case 3:
+		val &= 0x00FFFFFF;
+		val |= (istatus << 24) & 0x3F000000;
+		break;
+	default:
+		;
+	}
+	NXGE_REG_WR64(handle, TXC_INT_MASK_REG, val);
+}
diff --git a/drivers/net/nxge/npi/npi_txc.h b/drivers/net/nxge/npi/npi_txc.h
new file mode 100644
index 0000000..a50ec7b
--- /dev/null
+++ b/drivers/net/nxge/npi/npi_txc.h
@@ -0,0 +1,151 @@
+/*
+ * npi_txc.h	Neptune TX Controller HW API functions
+ *
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef _NPI_TXC_H
+#define	_NPI_TXC_H
+
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#include <npi.h>
+#include <nxge_txc_hw.h>
+
+/*
+ * Transmit Controller (TXC) NPI error codes
+ */
+#define	TXC_ER_ST			(TXC_BLK_ID << NPI_BLOCK_ID_SHIFT)
+#define	TXC_ID_SHIFT(n)			(n << NPI_PORT_CHAN_SHIFT)
+
+#define	NPI_TXC_PORT_INVALID(n)		(TXC_ID_SHIFT(n) | IS_PORT |\
+					TXC_ER_ST | PORT_INVALID)
+
+#define	NPI_TXC_CHANNEL_INVALID(n)	(TXC_ID_SHIFT(n) | IS_PORT |\
+					TXC_ER_ST | CHANNEL_INVALID)
+
+#define	NPI_TXC_OPCODE_INVALID(n)	(TXC_ID_SHIFT(n) | IS_PORT |\
+					TXC_ER_ST | OPCODE_INVALID)
+
+/*
+ * Register offset (0x1000 bytes for each channel) for TXC registers.
+ */
+#define	NXGE_TXC_FZC_OFFSET(x, cn)	(x + TXC_FZC_CHANNEL_OFFSET(cn))
+
+/*
+ * Register offset (0x100 bytes for each port) for TXC Function zero
+ * control registers.
+ */
+#define	NXGE_TXC_FZC_CNTL_OFFSET(x, port) (x + \
+			TXC_FZC_CNTL_PORT_OFFSET(port))
+/*
+ * PIO macros to read and write the transmit control registers.
+ */
+#define	TXC_FZC_REG_READ64(handle, reg, cn, val_p)	\
+		NXGE_REG_RD64(handle, \
+		(NXGE_TXC_FZC_OFFSET(reg, cn)), val_p)
+
+#define	TXC_FZC_REG_WRITE64(handle, reg, cn, data)	\
+		NXGE_REG_WR64(handle, \
+		(NXGE_TXC_FZC_OFFSET(reg, cn)), data)
+
+#define	TXC_FZC_CNTL_REG_READ64(handle, reg, port, val_p)	\
+		NXGE_REG_RD64(handle, \
+		(NXGE_TXC_FZC_CNTL_OFFSET(reg, port)), val_p)
+
+#define	TXC_FZC_CNTL_REG_WRITE64(handle, reg, port, data)	\
+		NXGE_REG_WR64(handle, \
+		(NXGE_TXC_FZC_CNTL_OFFSET(reg, port)), data)
+
+/*
+ * TXC (Transmit Controller) prototypes.
+ */
+npi_status_t npi_txc_dma_max_burst(npi_handle_t, io_op_t,
+		uint8_t, uint32_t *);
+npi_status_t npi_txc_dma_max_burst_set(npi_handle_t, uint8_t,
+		uint32_t);
+npi_status_t npi_txc_dma_bytes_transmitted(npi_handle_t,
+		uint8_t, uint32_t *);
+npi_status_t npi_txc_control(npi_handle_t, io_op_t,
+		p_txc_control_t);
+npi_status_t npi_txc_global_enable(npi_handle_t);
+npi_status_t npi_txc_global_disable(npi_handle_t);
+npi_status_t npi_txc_control_clear(npi_handle_t, uint8_t);
+npi_status_t npi_txc_training_set(npi_handle_t, uint32_t);
+npi_status_t npi_txc_training_get(npi_handle_t, uint32_t *);
+npi_status_t npi_txc_port_control_get(npi_handle_t, uint8_t,
+		uint32_t *);
+npi_status_t npi_txc_port_enable(npi_handle_t, uint8_t);
+npi_status_t npi_txc_port_disable(npi_handle_t, uint8_t);
+npi_status_t npi_txc_dma_max_burst(npi_handle_t, io_op_t,
+		uint8_t, uint32_t *);
+npi_status_t npi_txc_port_dma_enable(npi_handle_t, uint8_t,
+		uint32_t);
+npi_status_t npi_txc_port_dma_list_get(npi_handle_t, uint8_t,
+		uint32_t *);
+npi_status_t npi_txc_port_dma_channel_enable(npi_handle_t, uint8_t,
+		uint8_t);
+npi_status_t npi_txc_port_dma_channel_disable(npi_handle_t, uint8_t,
+		uint8_t);
+
+npi_status_t npi_txc_pkt_stuffed_get(npi_handle_t, uint8_t,
+		uint32_t *, uint32_t *);
+npi_status_t npi_txc_pkt_xmt_to_mac_get(npi_handle_t, uint8_t,
+		uint32_t *, uint32_t *);
+npi_status_t npi_txc_reorder_get(npi_handle_t, uint8_t,
+		uint32_t *);
+npi_status_t npi_txc_dump_tdc_fzc_regs(npi_handle_t, uint8_t);
+npi_status_t npi_txc_dump_fzc_regs(npi_handle_t);
+npi_status_t npi_txc_dump_port_fzc_regs(npi_handle_t, uint8_t);
+npi_status_t npi_txc_ro_states_get(npi_handle_t, uint8_t,
+		txc_ro_states_t *);
+npi_status_t npi_txc_ro_ecc_state_clr(npi_handle_t, uint8_t);
+npi_status_t npi_txc_sf_states_get(npi_handle_t, uint8_t,
+		txc_sf_states_t *);
+npi_status_t npi_txc_sf_ecc_state_clr(npi_handle_t, uint8_t);
+void npi_txc_global_istatus_get(npi_handle_t, txc_int_stat_t *);
+void npi_txc_global_istatus_clear(npi_handle_t, uint64_t);
+void npi_txc_global_imask_set(npi_handle_t, uint8_t,
+		uint8_t);
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif	/* _NPI_TXC_H */
diff --git a/drivers/net/nxge/npi/npi_txdma.c b/drivers/net/nxge/npi/npi_txdma.c
new file mode 100644
index 0000000..6a0bd59
--- /dev/null
+++ b/drivers/net/nxge/npi/npi_txdma.c
@@ -0,0 +1,2067 @@
+/*
+ * npi_txdma.c	Neptune  TX DMA HW API functions
+ *
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#include	<npi_txdma.h>
+
+#define	TXDMA_WAIT_LOOP		10000
+#define	TXDMA_WAIT_MSEC		5
+
+static npi_status_t npi_txdma_control_reset_wait(npi_handle_t handle,
+							uint8_t channel);
+static npi_status_t npi_txdma_control_stop_wait(npi_handle_t handle,
+							uint8_t channel);
+static npi_status_t npi_txdma_control_resume_wait(npi_handle_t handle,
+							uint8_t channel);
+
+uint64_t tdc_dmc_offset[] = {
+	TX_RNG_CFIG_REG,
+	TX_RING_HDL_REG,
+	TX_RING_KICK_REG,
+	TX_ENT_MSK_REG,
+	TX_CS_REG,
+	TXDMA_MBH_REG,
+	TXDMA_MBL_REG,
+	TX_DMA_PRE_ST_REG,
+	TX_RNG_ERR_LOGH_REG,
+	TX_RNG_ERR_LOGL_REG,
+	TDMC_INTR_DBG_REG,
+	TX_CS_DBG_REG
+};
+
+const char *tdc_dmc_name[] = {
+	"TX_RNG_CFIG_REG",
+	"TX_RING_HDL_REG",
+	"TX_RING_KICK_REG",
+	"TX_ENT_MSK_REG",
+	"TX_CS_REG",
+	"TXDMA_MBH_REG",
+	"TXDMA_MBL_REG",
+	"TX_DMA_PRE_ST_REG",
+	"TX_RNG_ERR_LOGH_REG",
+	"TX_RNG_ERR_LOGL_REG",
+	"TDMC_INTR_DBG_REG",
+	"TX_CS_DBG_REG"
+};
+
+uint64_t tdc_fzc_offset [] = {
+	TX_LOG_PAGE_VLD_REG,
+	TX_LOG_PAGE_MASK1_REG,
+	TX_LOG_PAGE_VAL1_REG,
+	TX_LOG_PAGE_MASK2_REG,
+	TX_LOG_PAGE_VAL2_REG,
+	TX_LOG_PAGE_RELO1_REG,
+	TX_LOG_PAGE_RELO2_REG,
+	TX_LOG_PAGE_HDL_REG
+};
+
+const char *tdc_fzc_name [] = {
+	"TX_LOG_PAGE_VLD_REG",
+	"TX_LOG_PAGE_MASK1_REG",
+	"TX_LOG_PAGE_VAL1_REG",
+	"TX_LOG_PAGE_MASK2_REG",
+	"TX_LOG_PAGE_VAL2_REG",
+	"TX_LOG_PAGE_RELO1_REG",
+	"TX_LOG_PAGE_RELO2_REG",
+	"TX_LOG_PAGE_HDL_REG"
+};
+
+uint64_t tx_fzc_offset[] = {
+	TX_ADDR_MD_REG,
+	TDMC_INJ_PAR_ERR_REG,
+	TDMC_DBG_SEL_REG,
+	TDMC_TRAINING_REG
+};
+
+const char *tx_fzc_name[] = {
+	"TX_ADDR_MD_REG",
+	"TDMC_INJ_PAR_ERR_REG",
+	"TDMC_DBG_SEL_REG",
+	"TDMC_TRAINING_REG"
+};
+
+/*
+ * npi_txdma_dump_tdc_regs
+ * Dumps the contents of tdc csrs and fzc registers
+ *
+ * Input:
+ *         tdc:      TX DMA number
+ *
+ * return:
+ *     NPI_SUCCESS
+ *     NPI_FAILURE
+ *     NPI_TXDMA_CHANNEL_INVALID
+ *
+ */
+npi_status_t
+npi_txdma_dump_tdc_regs(npi_handle_t handle, uint8_t tdc)
+{
+
+	uint64_t		value, offset;
+	int 			num_regs, i;
+
+	if (!TXDMA_CHANNEL_VALID(tdc)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			"npi_txdma_dump_tdc_regs"
+			" Invalid TDC number %d \n",
+			tdc));
+
+		return (NPI_FAILURE | NPI_TXDMA_CHANNEL_INVALID(tdc));
+	}
+
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+		    "\nTXDMA DMC Register Dump for Channel %d\n",
+			    tdc));
+
+	num_regs = sizeof (tdc_dmc_offset) / sizeof (uint64_t);
+	for (i = 0; i < num_regs; i++) {
+		TXDMA_REG_READ64(handle, tdc_dmc_offset[i], tdc, &value);
+		offset = NXGE_TXDMA_OFFSET(tdc_dmc_offset[i], handle.is_vraddr,
+				tdc);
+		NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL, "0x%08llx "
+			"%s\t 0x%016llx \n",
+			offset, tdc_dmc_name[i],
+			value));
+	}
+
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+		"\nTXDMA FZC_DMC Register Dump for Channel %d\n",
+		tdc));
+
+	num_regs = sizeof (tdc_fzc_offset) / sizeof (uint64_t);
+	for (i = 0; i < num_regs; i++) {
+		offset = NXGE_TXLOG_OFFSET(tdc_fzc_offset[i], tdc);
+		NXGE_REG_RD64(handle, offset, &value);
+		NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL, "0x%08llx "
+			"%s\t %016llx \n",
+			offset, tdc_fzc_name[i],
+			value));
+	}
+
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+		"\n TXDMA Register Dump for Channel %d done\n", tdc));
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_txdma_dump_fzc_regs
+ * Dumps the contents of tdc csrs and fzc registers
+ *
+ * Input:
+ *         tdc:      TX DMA number
+ *
+ * return:
+ *     NPI_SUCCESS
+ *     NPI_FAILURE
+ *     NPI_TXDMA_CHANNEL_INVALID
+ *
+ */
+npi_status_t
+npi_txdma_dump_fzc_regs(npi_handle_t handle)
+{
+
+	uint64_t value;
+	int num_regs, i;
+
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+		"\nFZC_DMC Common Register Dump\n"));
+
+	num_regs = sizeof (tx_fzc_offset) / sizeof (uint64_t);
+	for (i = 0; i < num_regs; i++) {
+		NXGE_REG_RD64(handle, tx_fzc_offset[i], &value);
+		NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL, "0x%08llx "
+			"%s\t 0x%08llx \n",
+			tx_fzc_offset[i],
+			tx_fzc_name[i], value));
+	}
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+		"\n TXDMA FZC_DMC Register Dump Done \n"));
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_txdma_tdc_regs_zero(npi_handle_t handle, uint8_t tdc)
+{
+	uint64_t		value;
+	int 			num_regs, i;
+
+	if (!TXDMA_CHANNEL_VALID(tdc)) {
+		NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+			"npi_txdma_tdc_regs_zero"
+			" InvaliInvalid TDC number %d \n",
+			tdc));
+		return (NPI_FAILURE | NPI_TXDMA_CHANNEL_INVALID(tdc));
+	}
+
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+		    "\nTXDMA DMC Register (zero) for Channel %d\n",
+			    tdc));
+
+	num_regs = sizeof (tdc_dmc_offset) / sizeof (uint64_t);
+	value = 0;
+	for (i = 0; i < num_regs; i++) {
+		TXDMA_REG_WRITE64(handle, tdc_dmc_offset[i], tdc,
+			value);
+	}
+
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+		"\nTXDMA FZC_DMC Register clear for Channel %d\n",
+		tdc));
+
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+		"\n TXDMA Register Clear to 0s for Channel %d done\n", tdc));
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_txdma_address_mode32_set():
+ *	This function is called to only support 32 bit addressing.
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	mode_enable	- B_TRUE  (enable 32 bit mode)
+ *			  B_FALSE (disable 32 bit mode)
+ *
+ * Return:
+ *	NPI_SUCCESS		- If set is complete successfully.
+ *
+ *	Error:
+ *	NONE
+ */
+npi_status_t
+npi_txdma_mode32_set(npi_handle_t handle, boolean_t mode_enable)
+{
+	tx_addr_md_t		mode32;
+
+	mode32.value = 0;
+	if (mode_enable) {
+		mode32.bits.ldw.mode32 = 1;
+	} else {
+		mode32.bits.ldw.mode32 = 0;
+	}
+	NXGE_REG_WR64(handle, TX_ADDR_MD_REG, mode32.value);
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_txdma_log_page_set():
+ *	This function is called to configure a logical page
+ *	(valid bit, mask, value, relocation).
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	cfgp		- pointer to NPI defined data structure:
+ *				- page valid
+ * 				- mask
+ *				- value
+ *				- relocation
+ *	channel		- hardware TXDMA channel from 0 to 23.
+ *
+ * Return:
+ *	NPI_SUCCESS		- If configurations are set successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE -
+ *		NPI_TXDMA_CHANNEL_INVALID	-
+ *		NPI_TXDMA_FUNC_INVALID	-
+ *		NPI_TXDMA_PAGE_INVALID	-
+ */
+npi_status_t
+npi_txdma_log_page_set(npi_handle_t handle, uint8_t channel,
+		p_dma_log_page_t cfgp)
+{
+	log_page_vld_t		vld;
+	int			status;
+	uint64_t		val;
+	dma_log_page_t		cfg;
+
+	DMA_LOG_PAGE_FN_VALIDATE(channel, cfgp->page_num, cfgp->func_num,
+		status);
+	if (status) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txdma_log_page_set"
+				    " npi_status <0x%x>", status));
+		return (status);
+	}
+
+	TX_LOG_REG_WRITE64(handle, TX_LOG_PAGE_VLD_REG, channel, 0);
+	TX_LOG_REG_READ64(handle, TX_LOG_PAGE_VLD_REG, channel, &val);
+
+	NPI_DEBUG_MSG((handle.function, NPI_TDC_CTL,
+			    "\n==> npi_txdma_log_page_set: WRITE 0 and "
+			    " READ back 0x%llx\n ", val));
+
+	vld.value = 0;
+	TX_LOG_REG_READ64(handle, TX_LOG_PAGE_VLD_REG, channel, &val);
+
+	val &= 0x3;
+	vld.value |= val;
+
+	vld.value = 0;
+	vld.bits.ldw.func = cfgp->func_num;
+
+	if (!cfgp->page_num) {
+		TX_LOG_REG_WRITE64(handle, TX_LOG_PAGE_MASK1_REG,
+			channel, (cfgp->mask & DMA_LOG_PAGE_MASK_MASK));
+		TX_LOG_REG_WRITE64(handle, TX_LOG_PAGE_VAL1_REG,
+			channel, (cfgp->value & DMA_LOG_PAGE_VALUE_MASK));
+		TX_LOG_REG_WRITE64(handle, TX_LOG_PAGE_RELO1_REG,
+			channel, (cfgp->reloc & DMA_LOG_PAGE_RELO_MASK));
+	} else {
+		TX_LOG_REG_WRITE64(handle, TX_LOG_PAGE_MASK2_REG,
+			channel, (cfgp->mask & DMA_LOG_PAGE_MASK_MASK));
+		TX_LOG_REG_WRITE64(handle, TX_LOG_PAGE_VAL2_REG,
+			channel, (cfgp->value & DMA_LOG_PAGE_VALUE_MASK));
+		TX_LOG_REG_WRITE64(handle, TX_LOG_PAGE_RELO2_REG,
+			channel, (cfgp->reloc & DMA_LOG_PAGE_RELO_MASK));
+	}
+
+	TX_LOG_REG_WRITE64(handle, TX_LOG_PAGE_VLD_REG, channel,
+		vld.value | (cfgp->valid << cfgp->page_num));
+
+	NPI_DEBUG_MSG((handle.function, NPI_REG_CTL,
+				    "\n==> npi_txdma_log_page_set: vld value "
+				    " 0x%llx function %d page_valid01 0x%x\n",
+				    vld.value,
+				    vld.bits.ldw.func,
+		(cfgp->valid << cfgp->page_num)));
+
+
+	cfg.page_num = 0;
+	cfg.func_num = 0;
+	(void) npi_txdma_log_page_get(handle, channel, &cfg);
+	cfg.page_num = 1;
+	(void) npi_txdma_log_page_get(handle, channel, &cfg);
+
+	return (status);
+}
+
+/*
+ * npi_txdma_log_page_get():
+ *	This function is called to get a logical page
+ *	(valid bit, mask, value, relocation).
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	cfgp		- Get the following values (NPI defined structure):
+ *				- page valid
+ * 				- mask
+ *				- value
+ *				- relocation
+ *	channel		- hardware TXDMA channel from 0 to 23.
+ *
+ * Return:
+ *	NPI_SUCCESS		- If configurations are read successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE -
+ *		NPI_TXDMA_CHANNEL_INVALID	-
+ *		NPI_TXDMA_FUNC_INVALID	-
+ *		NPI_TXDMA_PAGE_INVALID	-
+ */
+npi_status_t
+npi_txdma_log_page_get(npi_handle_t handle, uint8_t channel,
+		p_dma_log_page_t cfgp)
+{
+	log_page_vld_t		vld;
+	int			status;
+	uint64_t		val;
+
+	DMA_LOG_PAGE_VALIDATE(channel, cfgp->page_num, status);
+	if (status) {
+		NPI_ERROR_MSG((handle.function, NPI_REG_CTL,
+					    " npi_txdma_log_page_get"
+					    " npi_status <0x%x>", status));
+		return (status);
+	}
+
+	vld.value = 0;
+	vld.bits.ldw.func = cfgp->func_num;
+	TX_LOG_REG_READ64(handle, TX_LOG_PAGE_VLD_REG, channel, &val);
+
+	NPI_DEBUG_MSG((handle.function, NPI_TDC_CTL,
+				    "\n==> npi_txdma_log_page_get: read value "
+				    " function %d  value 0x%llx\n",
+				    cfgp->func_num, val));
+
+	vld.value |= val;
+	cfgp->func_num = vld.bits.ldw.func;
+
+	if (!cfgp->page_num) {
+		TX_LOG_REG_READ64(handle, TX_LOG_PAGE_MASK1_REG, channel, &val);
+		cfgp->mask = val & DMA_LOG_PAGE_MASK_MASK;
+		TX_LOG_REG_READ64(handle, TX_LOG_PAGE_VAL1_REG, channel, &val);
+		cfgp->value = val & DMA_LOG_PAGE_VALUE_MASK;
+		TX_LOG_REG_READ64(handle, TX_LOG_PAGE_RELO1_REG, channel, &val);
+		cfgp->reloc = val & DMA_LOG_PAGE_RELO_MASK;
+		cfgp->valid = vld.bits.ldw.page0;
+	} else {
+		TX_LOG_REG_READ64(handle, TX_LOG_PAGE_MASK2_REG, channel, &val);
+		cfgp->mask = val & DMA_LOG_PAGE_MASK_MASK;
+		TX_LOG_REG_READ64(handle, TX_LOG_PAGE_VAL2_REG, channel, &val);
+		cfgp->value = val & DMA_LOG_PAGE_VALUE_MASK;
+		TX_LOG_REG_READ64(handle, TX_LOG_PAGE_RELO2_REG, channel, &val);
+		cfgp->reloc = val & DMA_LOG_PAGE_RELO_MASK;
+		cfgp->valid = vld.bits.ldw.page1;
+	}
+
+	return (status);
+}
+
+/*
+ * npi_txdma_log_page_handle_set():
+ *	This function is called to program a page handle
+ *	(bits [63:44] of a 64-bit address to generate
+ *	a 64 bit address)
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	hdl_p		- pointer to a logical page handle
+ *			  hardware data structure (log_page_hdl_t).
+ *	channel		- hardware TXDMA channel from 0 to 23.
+ *
+ * Return:
+ *	NPI_SUCCESS		- If configurations are set successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE -
+ *		NPI_TXDMA_CHANNEL_INVALID	-
+ *		NPI_TXDMA_FUNC_INVALID	-
+ *		NPI_TXDMA_PAGE_INVALID	-
+ */
+npi_status_t
+npi_txdma_log_page_handle_set(npi_handle_t handle, uint8_t channel,
+		p_log_page_hdl_t hdl_p)
+{
+	int			status = NPI_SUCCESS;
+
+	if (!TXDMA_CHANNEL_VALID(channel)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txdma_log_page_handle_set"
+				    " Invalid Input: channel <0x%x>",
+				    channel));
+		return (NPI_FAILURE | NPI_TXDMA_CHANNEL_INVALID(channel));
+	}
+
+	TX_LOG_REG_WRITE64(handle, TX_LOG_PAGE_HDL_REG,
+		channel, hdl_p->value);
+
+	return (status);
+}
+
+/*
+ * npi_txdma_log_page_config():
+ *	This function is called to IO operations on
+ *	 a logical page to set, get, clear
+ *	valid bit, mask, value, relocation).
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	op_mode		- OP_GET, OP_SET, OP_CLEAR
+ *	type		- NPI specific config type
+ *			   TXDMA_LOG_PAGE_MASK
+ *			   TXDMA_LOG_PAGE_VALUE
+ *			   TXDMA_LOG_PAGE_RELOC
+ *			   TXDMA_LOG_PAGE_VALID
+ *			   TXDMA_LOG_PAGE_ALL
+ *	channel		- hardware TXDMA channel from 0 to 23.
+ *	cfgp		- pointer to the NPI config structure.
+ * Return:
+ *	NPI_SUCCESS		- If configurations are read successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE		-
+ *		NPI_TXDMA_OPCODE_INVALID	-
+ *		NPI_TXDMA_CHANNEL_INVALID	-
+ *		NPI_TXDMA_FUNC_INVALID	-
+ *		NPI_TXDMA_PAGE_INVALID	-
+ */
+npi_status_t
+npi_txdma_log_page_config(npi_handle_t handle, io_op_t op_mode,
+		txdma_log_cfg_t type, uint8_t channel,
+		p_dma_log_page_t cfgp)
+{
+	int			status = NPI_SUCCESS;
+	uint64_t		val;
+
+	if (!TXDMA_CHANNEL_VALID(channel)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txdma_log_page_config"
+				    " Invalid Input: channel <0x%x>",
+				    channel));
+		return (NPI_FAILURE | NPI_TXDMA_CHANNEL_INVALID(channel));
+	}
+
+	switch (op_mode) {
+	case OP_GET:
+		switch (type) {
+		case TXDMA_LOG_PAGE_ALL:
+			return (npi_txdma_log_page_get(handle, channel,
+					cfgp));
+		case TXDMA_LOG_PAGE_MASK:
+			if (!cfgp->page_num) {
+				TX_LOG_REG_READ64(handle, TX_LOG_PAGE_MASK1_REG,
+						channel, &val);
+				cfgp->mask = val & DMA_LOG_PAGE_MASK_MASK;
+			} else {
+				TX_LOG_REG_READ64(handle, TX_LOG_PAGE_MASK2_REG,
+						channel, &val);
+				cfgp->mask = val & DMA_LOG_PAGE_MASK_MASK;
+			}
+			break;
+
+		case TXDMA_LOG_PAGE_VALUE:
+			if (!cfgp->page_num) {
+				TX_LOG_REG_READ64(handle, TX_LOG_PAGE_VAL1_REG,
+						channel, &val);
+				cfgp->value = val & DMA_LOG_PAGE_VALUE_MASK;
+			} else {
+				TX_LOG_REG_READ64(handle, TX_LOG_PAGE_VAL2_REG,
+						channel, &val);
+				cfgp->value = val & DMA_LOG_PAGE_VALUE_MASK;
+			}
+			break;
+
+		case TXDMA_LOG_PAGE_RELOC:
+			if (!cfgp->page_num) {
+				TX_LOG_REG_READ64(handle, TX_LOG_PAGE_RELO1_REG,
+						channel, &val);
+				cfgp->reloc = val & DMA_LOG_PAGE_RELO_MASK;
+			} else {
+				TX_LOG_REG_READ64(handle, TX_LOG_PAGE_VAL2_REG,
+						channel, &val);
+				cfgp->reloc = val & DMA_LOG_PAGE_RELO_MASK;
+			}
+			break;
+
+		default:
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_txdma_log_page_config"
+					    " Invalid Input: pageconfig <0x%x>",
+					    type));
+			return (NPI_FAILURE |
+				NPI_TXDMA_OPCODE_INVALID(channel));
+		}
+
+		break;
+
+	case OP_SET:
+	case OP_CLEAR:
+		if (op_mode == OP_CLEAR) {
+			cfgp->valid = 0;
+			cfgp->mask = cfgp->func_num = 0;
+			cfgp->value = cfgp->reloc = 0;
+		}
+		switch (type) {
+		case TXDMA_LOG_PAGE_ALL:
+			return (npi_txdma_log_page_set(handle, channel,
+					cfgp));
+		case TXDMA_LOG_PAGE_MASK:
+			if (!cfgp->page_num) {
+				TX_LOG_REG_WRITE64(handle,
+				TX_LOG_PAGE_MASK1_REG, channel,
+				(cfgp->mask & DMA_LOG_PAGE_MASK_MASK));
+			} else {
+				TX_LOG_REG_WRITE64(handle,
+				TX_LOG_PAGE_MASK2_REG,
+				channel, (cfgp->mask & DMA_LOG_PAGE_MASK_MASK));
+			}
+			break;
+
+		case TXDMA_LOG_PAGE_VALUE:
+			if (!cfgp->page_num) {
+				TX_LOG_REG_WRITE64(handle,
+				TX_LOG_PAGE_VAL1_REG, channel,
+				(cfgp->value & DMA_LOG_PAGE_VALUE_MASK));
+			} else {
+				TX_LOG_REG_WRITE64(handle,
+				TX_LOG_PAGE_VAL2_REG, channel,
+				(cfgp->value & DMA_LOG_PAGE_VALUE_MASK));
+			}
+			break;
+
+		case TXDMA_LOG_PAGE_RELOC:
+			if (!cfgp->page_num) {
+				TX_LOG_REG_WRITE64(handle,
+				TX_LOG_PAGE_RELO1_REG, channel,
+				(cfgp->reloc & DMA_LOG_PAGE_RELO_MASK));
+			} else {
+				TX_LOG_REG_WRITE64(handle,
+				TX_LOG_PAGE_RELO2_REG, channel,
+				(cfgp->reloc & DMA_LOG_PAGE_RELO_MASK));
+			}
+			break;
+
+		default:
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_txdma_log_page_config"
+					    " Invalid Input: pageconfig <0x%x>",
+					    type));
+			return (NPI_FAILURE |
+				NPI_TXDMA_OPCODE_INVALID(channel));
+		}
+
+		break;
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_txdma_log_page_config"
+					    " Invalid Input: op <0x%x>",
+					    op_mode));
+		return (NPI_FAILURE | NPI_TXDMA_OPCODE_INVALID(channel));
+	}
+
+	return (status);
+}
+
+/*
+ * npi_txdma_log_page_vld_config():
+ *	This function is called to configure the logical
+ *	page valid register.
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	op_mode		- OP_GET: get valid page configuration
+ *			  OP_SET: set valid page configuration
+ *			  OP_UPDATE: update valid page configuration
+ *			  OP_CLEAR: reset both valid pages to
+ *			  not defined (0).
+ *	channel		- hardware TXDMA channel from 0 to 23.
+ *	vld_p		- pointer to hardware defined log page valid register.
+ * Return:
+ *	NPI_SUCCESS		- If set is complete successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE -
+ *		NPI_TXDMA_CHANNEL_INVALID -
+ *		NPI_TXDMA_OPCODE_INVALID -
+ */
+npi_status_t
+npi_txdma_log_page_vld_config(npi_handle_t handle, io_op_t op_mode,
+		uint8_t channel, p_log_page_vld_t vld_p)
+{
+	int			status = NPI_SUCCESS;
+	log_page_vld_t		vld;
+
+	if (!TXDMA_CHANNEL_VALID(channel)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txdma_log_page_vld_config"
+				    " Invalid Input: channel <0x%x>",
+				    channel));
+		return (NPI_FAILURE | NPI_TXDMA_CHANNEL_INVALID(channel));
+	}
+
+	switch (op_mode) {
+	case OP_GET:
+		TX_LOG_REG_READ64(handle, TX_LOG_PAGE_VLD_REG, channel,
+					&vld_p->value);
+		break;
+
+	case OP_SET:
+		TX_LOG_REG_WRITE64(handle, TX_LOG_PAGE_VLD_REG,
+					channel, vld_p->value);
+		break;
+
+	case OP_UPDATE:
+		TX_LOG_REG_READ64(handle, TX_LOG_PAGE_VLD_REG, channel,
+					&vld.value);
+		TX_LOG_REG_WRITE64(handle, TX_LOG_PAGE_VLD_REG,
+					channel, vld.value | vld_p->value);
+		break;
+
+	case OP_CLEAR:
+		TX_LOG_REG_WRITE64(handle, TX_LOG_PAGE_VLD_REG,
+					channel, 0);
+		break;
+
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txdma_log_pag_vld_cofig"
+				    " Invalid Input: pagevld <0x%x>",
+				    op_mode));
+		return (NPI_FAILURE | NPI_TXDMA_OPCODE_INVALID(channel));
+	}
+
+	return (status);
+}
+
+/*
+ * npi_txdma_channel_reset():
+ *	This function is called to reset a transmit DMA channel.
+ *	(This function is used to reset a channel and reinitialize
+ *	 all other bits except RST_STATE).
+ *
+ * Parameters:
+ *	handle		- NPI handle (virtualization flag must be defined).
+ *	channel		- logical TXDMA channel from 0 to 23.
+ *			  (If virtualization flag is not set, then
+ *			   logical channel is the same as the hardware
+ *			   channel number).
+ *
+ * Return:
+ *	NPI_SUCCESS		- If reset is complete successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE	-
+ *		NPI_TXDMA_CHANNEL_INVALID -
+ *		NPI_TXDMA_RESET_FAILED -
+ */
+npi_status_t
+npi_txdma_channel_reset(npi_handle_t handle, uint8_t channel)
+{
+	NPI_DEBUG_MSG((handle.function, NPI_TDC_CTL,
+			    " npi_txdma_channel_reset"
+			    " RESETTING",
+			    channel));
+	return (npi_txdma_channel_control(handle, TXDMA_RESET, channel));
+}
+
+/*
+ * npi_txdma_channel_init_enable():
+ *	This function is called to start a transmit DMA channel after reset.
+ *
+ * Parameters:
+ *	handle		- NPI handle (virtualization flag must be defined).
+ *	channel		- logical TXDMA channel from 0 to 23.
+ *			  (If virtualization flag is not set, then
+ *			   logical channel is the same as the hardware
+ *			   channel number).
+ * Return:
+ *	NPI_SUCCESS		- If DMA channel is started successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE	-
+ *		NPI_TXDMA_CHANNEL_INVALID -
+ */
+npi_status_t
+npi_txdma_channel_init_enable(npi_handle_t handle, uint8_t channel)
+{
+	return (npi_txdma_channel_control(handle, TXDMA_INIT_START, channel));
+}
+
+/*
+ * npi_txdma_channel_enable():
+ *	This function is called to start a transmit DMA channel.
+ *
+ * Parameters:
+ *	handle		- NPI handle (virtualization flag must be defined).
+ *	channel		- logical TXDMA channel from 0 to 23.
+ *			  (If virtualization flag is not set, then
+ *			   logical channel is the same as the hardware
+ *			   channel number).
+ * Return:
+ *	NPI_SUCCESS		- If DMA channel is stopped successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE	-
+ *		NPI_TXDMA_CHANNEL_INVALID -
+ */
+
+npi_status_t
+npi_txdma_channel_enable(npi_handle_t handle, uint8_t channel)
+{
+	return (npi_txdma_channel_control(handle, TXDMA_START, channel));
+}
+
+/*
+ * npi_txdma_channel_disable():
+ *	This function is called to stop a transmit DMA channel.
+ *
+ * Parameters:
+ *	handle		- NPI handle (virtualization flag must be defined).
+ *	channel		- logical TXDMA channel from 0 to 23.
+ *			  (If virtualization flag is not set, then
+ *			   logical channel is the same as the hardware
+ *			   channel number).
+ * Return:
+ *	NPI_SUCCESS		- If DMA channel is stopped successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE	-
+ *		NPI_TXDMA_CHANNEL_INVALID -
+ *		NPI_TXDMA_STOP_FAILED -
+ */
+npi_status_t
+npi_txdma_channel_disable(npi_handle_t handle, uint8_t channel)
+{
+	return (npi_txdma_channel_control(handle, TXDMA_STOP, channel));
+}
+
+/*
+ * npi_txdma_channel_resume():
+ *	This function is called to restart a transmit DMA channel.
+ *
+ * Parameters:
+ *	handle		- NPI handle (virtualization flag must be defined).
+ *	channel		- logical TXDMA channel from 0 to 23.
+ *			  (If virtualization flag is not set, then
+ *			   logical channel is the same as the hardware
+ *			   channel number).
+ * Return:
+ *	NPI_SUCCESS		- If DMA channel is stopped successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE	-
+ *		NPI_TXDMA_CHANNEL_INVALID -
+ *		NPI_TXDMA_RESUME_FAILED -
+ */
+npi_status_t
+npi_txdma_channel_resume(npi_handle_t handle, uint8_t channel)
+{
+	return (npi_txdma_channel_control(handle, TXDMA_RESUME, channel));
+}
+
+/*
+ * npi_txdma_channel_mmk_clear():
+ *	This function is called to clear MMK bit.
+ *
+ * Parameters:
+ *	handle		- NPI handle (virtualization flag must be defined).
+ *	channel		- logical TXDMA channel from 0 to 23.
+ *			  (If virtualization flag is not set, then
+ *			   logical channel is the same as the hardware
+ *			   channel number).
+ * Return:
+ *	NPI_SUCCESS		- If MMK is reset successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE	-
+ *		NPI_TXDMA_CHANNEL_INVALID -
+ */
+npi_status_t
+npi_txdma_channel_mmk_clear(npi_handle_t handle, uint8_t channel)
+{
+	return (npi_txdma_channel_control(handle, TXDMA_CLEAR_MMK, channel));
+}
+
+/*
+ * npi_txdma_channel_mbox_enable():
+ *	This function is called to enable the mailbox update.
+ *
+ * Parameters:
+ *	handle		- NPI handle (virtualization flag must be defined).
+ *	channel		- logical TXDMA channel from 0 to 23.
+ *			  (If virtualization flag is not set, then
+ *			   logical channel is the same as the hardware
+ *			   channel number).
+ * Return:
+ *	NPI_SUCCESS		- If mailbox is enabled successfully.
+ *
+ *	Error:
+ *	NPI_HW_ERROR		-
+ *	NPI_FAILURE	-
+ *		NPI_TXDMA_CHANNEL_INVALID -
+ */
+npi_status_t
+npi_txdma_channel_mbox_enable(npi_handle_t handle, uint8_t channel)
+{
+	return (npi_txdma_channel_control(handle, TXDMA_MBOX_ENABLE, channel));
+}
+
+/*
+ * npi_txdma_channel_control():
+ *	This function is called to control a transmit DMA channel
+ *	for reset, start or stop.
+ *
+ * Parameters:
+ *	handle		- NPI handle (virtualization flag must be defined).
+ *	control		- NPI defined control type supported
+ *				- TXDMA_INIT_RESET
+ * 				- TXDMA_INIT_START
+ *				- TXDMA_RESET
+ *				- TXDMA_START
+ *				- TXDMA_STOP
+ *	channel		- logical TXDMA channel from 0 to 23.
+ *			  (If virtualization flag is not set, then
+ *			   logical channel is the same as the hardware
+ *
+ * Return:
+ *	NPI_SUCCESS		- If reset is complete successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE		-
+ *		NPI_TXDMA_OPCODE_INVALID	-
+ *		NPI_TXDMA_CHANNEL_INVALID	-
+ *		NPI_TXDMA_RESET_FAILED	-
+ *		NPI_TXDMA_STOP_FAILED	-
+ *		NPI_TXDMA_RESUME_FAILED	-
+ */
+npi_status_t
+npi_txdma_channel_control(npi_handle_t handle, txdma_cs_cntl_t control,
+		uint8_t channel)
+{
+	int		status = NPI_SUCCESS;
+	tx_cs_t		cs;
+
+	if (!TXDMA_CHANNEL_VALID(channel)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txdma_channel_control"
+				    " Invalid Input: channel <0x%x>",
+				    channel));
+		return (NPI_FAILURE | NPI_TXDMA_CHANNEL_INVALID(channel));
+	}
+
+	switch (control) {
+	case TXDMA_INIT_RESET:
+		cs.value = 0;
+		TXDMA_REG_READ64(handle, TX_CS_REG, channel, &cs.value);
+		cs.bits.ldw.rst = 1;
+		TXDMA_REG_WRITE64(handle, TX_CS_REG, channel, cs.value);
+		return (npi_txdma_control_reset_wait(handle, channel));
+
+	case TXDMA_INIT_START:
+		cs.value = 0;
+		TXDMA_REG_WRITE64(handle, TX_CS_REG, channel, cs.value);
+		break;
+
+	case TXDMA_RESET:
+		/*
+		 * Sets reset bit only (Hardware will reset all
+		 * the RW bits but leave the RO bits alone.
+		 */
+		cs.value = 0;
+		cs.bits.ldw.rst = 1;
+		TXDMA_REG_WRITE64(handle, TX_CS_REG, channel, cs.value);
+		return (npi_txdma_control_reset_wait(handle, channel));
+
+	case TXDMA_START:
+		/* Enable the DMA channel */
+		TXDMA_REG_READ64(handle, TX_CS_REG, channel, &cs.value);
+		cs.bits.ldw.stop_n_go = 0;
+		TXDMA_REG_WRITE64(handle, TX_CS_REG, channel, cs.value);
+		break;
+
+	case TXDMA_STOP:
+		/* Disable the DMA channel */
+		TXDMA_REG_READ64(handle, TX_CS_REG, channel, &cs.value);
+		cs.bits.ldw.stop_n_go = 1;
+		TXDMA_REG_WRITE64(handle, TX_CS_REG, channel, cs.value);
+		status = npi_txdma_control_stop_wait(handle, channel);
+		if (status) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    "Cannot stop channel %d (TXC hung!)",
+				    channel));
+		}
+		break;
+
+	case TXDMA_RESUME:
+		/* Resume the packet transmission after stopping */
+		TXDMA_REG_READ64(handle, TX_CS_REG, channel, &cs.value);
+		cs.value |= ~TX_CS_STOP_N_GO_MASK;
+		TXDMA_REG_WRITE64(handle, TX_CS_REG, channel, cs.value);
+		return (npi_txdma_control_resume_wait(handle, channel));
+
+	case TXDMA_CLEAR_MMK:
+		/* Write 1 to MK bit to clear the MMK bit */
+		TXDMA_REG_READ64(handle, TX_CS_REG, channel, &cs.value);
+		cs.bits.ldw.mk = 1;
+		TXDMA_REG_WRITE64(handle, TX_CS_REG, channel, cs.value);
+		break;
+
+	case TXDMA_MBOX_ENABLE:
+		/*
+		 * Write 1 to MB bit to enable mailbox update
+		 * (cleared to 0 by hardware after update).
+		 */
+		TXDMA_REG_READ64(handle, TX_CS_REG, channel, &cs.value);
+		cs.bits.ldw.mb = 1;
+		TXDMA_REG_WRITE64(handle, TX_CS_REG, channel, cs.value);
+		break;
+
+	default:
+		status =  (NPI_FAILURE | NPI_TXDMA_OPCODE_INVALID(channel));
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txdma_channel_control"
+				    " Invalid Input: control <0x%x>",
+				    control));
+	}
+
+	return (status);
+}
+
+/*
+ * npi_txdma_control_status():
+ *	This function is called to operate on the control
+ *	and status register.
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	op_mode		- OP_GET: get hardware control and status
+ *			  OP_SET: set hardware control and status
+ *			  OP_UPDATE: update hardware control and status.
+ *			  OP_CLEAR: clear control and status register to 0s.
+ *	channel		- hardware TXDMA channel from 0 to 23.
+ *	cs_p		- pointer to hardware defined control and status
+ *			  structure.
+ * Return:
+ *	NPI_SUCCESS		- If set is complete successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE		-
+ *		NPI_TXDMA_OPCODE_INVALID	-
+ *		NPI_TXDMA_CHANNEL_INVALID	-
+ *		NPI_TXDMA_FUNC_INVALID	-
+ */
+npi_status_t
+npi_txdma_control_status(npi_handle_t handle, io_op_t op_mode,
+		uint8_t channel, p_tx_cs_t cs_p)
+{
+	int		status = NPI_SUCCESS;
+	tx_cs_t		txcs;
+
+	if (!TXDMA_CHANNEL_VALID(channel)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txdma_control_status"
+				    " Invalid Input: channel <0x%x>",
+				    channel));
+		return (NPI_FAILURE | NPI_TXDMA_CHANNEL_INVALID(channel));
+	}
+
+	switch (op_mode) {
+	case OP_GET:
+		TXDMA_REG_READ64(handle, TX_CS_REG, channel, &cs_p->value);
+		break;
+
+	case OP_SET:
+		TXDMA_REG_WRITE64(handle, TX_CS_REG, channel, cs_p->value);
+		break;
+
+	case OP_UPDATE:
+		TXDMA_REG_READ64(handle, TX_CS_REG, channel, &txcs.value);
+		TXDMA_REG_WRITE64(handle, TX_CS_REG, channel,
+			cs_p->value | txcs.value);
+		break;
+
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txdma_control_status"
+				    " Invalid Input: control <0x%x>",
+				    op_mode));
+		return (NPI_FAILURE | NPI_TXDMA_OPCODE_INVALID(channel));
+	}
+
+	return (status);
+
+}
+
+/*
+ * npi_txdma_event_mask():
+ *	This function is called to operate on the event mask
+ *	register which is used for generating interrupts..
+ *	and status register.
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	op_mode		- OP_GET: get hardware event mask
+ *			  OP_SET: set hardware interrupt event masks
+ *			  OP_CLEAR: clear control and status register to 0s.
+ *	channel		- hardware TXDMA channel from 0 to 23.
+ *	mask_p		- pointer to hardware defined event mask
+ *			  structure.
+ * Return:
+ *	NPI_SUCCESS		- If set is complete successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE		-
+ *		NPI_TXDMA_OPCODE_INVALID	-
+ *		NPI_TXDMA_CHANNEL_INVALID	-
+ */
+npi_status_t
+npi_txdma_event_mask(npi_handle_t handle, io_op_t op_mode,
+		uint8_t channel, p_tx_dma_ent_msk_t mask_p)
+{
+	int			status = NPI_SUCCESS;
+	tx_dma_ent_msk_t	mask;
+
+	if (!TXDMA_CHANNEL_VALID(channel)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_txdma_event_mask"
+					    " Invalid Input: channel <0x%x>",
+					    channel));
+		return (NPI_FAILURE | NPI_TXDMA_CHANNEL_INVALID(channel));
+	}
+
+	switch (op_mode) {
+	case OP_GET:
+		TXDMA_REG_READ64(handle, TX_ENT_MSK_REG, channel,
+				&mask_p->value);
+		break;
+
+	case OP_SET:
+		TXDMA_REG_WRITE64(handle, TX_ENT_MSK_REG, channel,
+				mask_p->value);
+		break;
+
+	case OP_UPDATE:
+		TXDMA_REG_READ64(handle, TX_ENT_MSK_REG, channel, &mask.value);
+		TXDMA_REG_WRITE64(handle, TX_ENT_MSK_REG, channel,
+			mask_p->value | mask.value);
+		break;
+
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txdma_event_mask"
+				    " Invalid Input: eventmask <0x%x>",
+				    op_mode));
+		return (NPI_FAILURE | NPI_TXDMA_OPCODE_INVALID(channel));
+	}
+
+	return (status);
+}
+
+/*
+ * npi_txdma_event_mask_config():
+ *	This function is called to operate on the event mask
+ *	register which is used for generating interrupts..
+ *	and status register.
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	op_mode		- OP_GET: get hardware event mask
+ *			  OP_SET: set hardware interrupt event masks
+ *			  OP_CLEAR: clear control and status register to 0s.
+ *	channel		- hardware TXDMA channel from 0 to 23.
+ *	cfgp		- pointer to NPI defined event mask
+ *			  enum data type.
+ * Return:
+ *	NPI_SUCCESS		- If set is complete successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE		-
+ *		NPI_TXDMA_OPCODE_INVALID	-
+ *		NPI_TXDMA_CHANNEL_INVALID	-
+ */
+npi_status_t
+npi_txdma_event_mask_config(npi_handle_t handle, io_op_t op_mode,
+		uint8_t channel, txdma_ent_msk_cfg_t *mask_cfgp)
+{
+	int		status = NPI_SUCCESS;
+	uint64_t	value;
+
+	if (!TXDMA_CHANNEL_VALID(channel)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txdma_event_mask_config"
+				    " Invalid Input: channel <0x%x>",
+				    channel));
+
+		return (NPI_FAILURE | NPI_TXDMA_CHANNEL_INVALID(channel));
+	}
+
+	switch (op_mode) {
+	case OP_GET:
+		TXDMA_REG_READ64(handle, TX_ENT_MSK_REG, channel, mask_cfgp);
+		break;
+
+	case OP_SET:
+		TXDMA_REG_WRITE64(handle, TX_ENT_MSK_REG, channel,
+				*mask_cfgp);
+		break;
+
+	case OP_UPDATE:
+		TXDMA_REG_READ64(handle, TX_ENT_MSK_REG, channel, &value);
+		TXDMA_REG_WRITE64(handle, TX_ENT_MSK_REG, channel,
+			*mask_cfgp | value);
+		break;
+
+	case OP_CLEAR:
+		TXDMA_REG_WRITE64(handle, TX_ENT_MSK_REG, channel,
+			CFG_TXDMA_MASK_ALL);
+		break;
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txdma_event_mask_config"
+				    " Invalid Input: eventmask <0x%x>",
+				    op_mode));
+		return (NPI_FAILURE | NPI_TXDMA_OPCODE_INVALID(channel));
+	}
+
+	return (status);
+}
+
+/*
+ * npi_txdma_event_mask_mk_out():
+ *	This function is called to mask out the packet transmit marked event.
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	channel		- hardware TXDMA channel from 0 to 23.
+ *			  enum data type.
+ * Return:
+ *	NPI_SUCCESS		- If set is complete successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE		-
+ *		NPI_TXDMA_CHANNEL_INVALID	-
+ */
+npi_status_t
+npi_txdma_event_mask_mk_out(npi_handle_t handle, uint8_t channel)
+{
+	txdma_ent_msk_cfg_t event_mask;
+	int		status = NPI_SUCCESS;
+
+	if (!TXDMA_CHANNEL_VALID(channel)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txdma_event_mask_mk_out"
+				    " Invalid Input: channel <0x%x>",
+				    channel));
+		return (NPI_FAILURE | NPI_TXDMA_CHANNEL_INVALID(channel));
+	}
+
+	TXDMA_REG_READ64(handle, TX_ENT_MSK_REG, channel, &event_mask);
+	TXDMA_REG_WRITE64(handle, TX_ENT_MSK_REG, channel,
+		event_mask & (~TX_ENT_MSK_MK_MASK));
+
+	return (status);
+}
+
+/*
+ * npi_txdma_event_mask_mk_in():
+ *	This function is called to set the mask for the the packet marked event.
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	channel		- hardware TXDMA channel from 0 to 23.
+ *			  enum data type.
+ * Return:
+ *	NPI_SUCCESS		- If set is complete successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE		-
+ *		NPI_TXDMA_CHANNEL_INVALID	-
+ */
+npi_status_t
+npi_txdma_event_mask_mk_in(npi_handle_t handle, uint8_t channel)
+{
+	txdma_ent_msk_cfg_t event_mask;
+	int		status = NPI_SUCCESS;
+
+	if (!TXDMA_CHANNEL_VALID(channel)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txdma_event_mask_mk_in"
+				    " Invalid Input: channel <0x%x>",
+				    channel));
+		return (NPI_FAILURE | NPI_TXDMA_CHANNEL_INVALID(channel));
+	}
+
+	TXDMA_REG_READ64(handle, TX_ENT_MSK_REG, channel, &event_mask);
+	TXDMA_REG_WRITE64(handle, TX_ENT_MSK_REG, channel,
+		event_mask | TX_ENT_MSK_MK_MASK);
+
+	return (status);
+}
+
+/*
+ * npi_txdma_ring_addr_set():
+ *	This function is called to configure the transmit descriptor
+ *	ring address and its size.
+ *
+ * Parameters:
+ *	handle		- NPI handle (virtualization flag must be defined
+ *			  if its register pointer is from the virtual region).
+ *	channel		- logical TXDMA channel from 0 to 23.
+ *			  (If virtualization flag is not set, then
+ *			   logical channel is the same as the hardware
+ *			   channel number).
+ *	start_addr	- starting address of the descriptor
+ *	len		- maximum length of the descriptor
+ *			  (in number of 64 bytes block).
+ * Return:
+ *	NPI_SUCCESS		- If set is complete successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE		-
+ *		NPI_TXDMA_OPCODE_INVALID	-
+ *		NPI_TXDMA_CHANNEL_INVALID	-
+ */
+npi_status_t
+npi_txdma_ring_addr_set(npi_handle_t handle, uint8_t channel,
+		uint64_t start_addr, uint32_t len)
+{
+	int		status = NPI_SUCCESS;
+	tx_rng_cfig_t	cfg;
+
+	if (!TXDMA_CHANNEL_VALID(channel)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txdma_ring_addr_set"
+				    " Invalid Input: channel <0x%x>",
+				    channel));
+		return (NPI_FAILURE | NPI_TXDMA_CHANNEL_INVALID(channel));
+	}
+
+	cfg.value = ((start_addr & TX_RNG_CFIG_ADDR_MASK) |
+			(((uint64_t)len) << TX_RNG_CFIG_LEN_SHIFT));
+	TXDMA_REG_WRITE64(handle, TX_RNG_CFIG_REG, channel, cfg.value);
+
+	return (status);
+}
+
+/*
+ * npi_txdma_ring_config():
+ *	This function is called to config a descriptor ring
+ *	by using the hardware defined data.
+ *
+ * Parameters:
+ *	handle		- NPI handle (virtualization flag must be defined
+ *			  if its register pointer is from the virtual region).
+ *	channel		- logical TXDMA channel from 0 to 23.
+ *			  (If virtualization flag is not set, then
+ *			   logical channel is the same as the hardware
+ *			   channel number).
+ *	op_mode		- OP_GET: get transmit ring configuration
+ *			  OP_SET: set transmit ring configuration
+ *	reg_data	- pointer to hardware defined transmit ring
+ *			  configuration data structure.
+ * Return:
+ *	NPI_SUCCESS		- If set/get is complete successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE		-
+ *		NPI_TXDMA_CHANNEL_INVALID	-
+ */
+npi_status_t
+npi_txdma_ring_config(npi_handle_t handle, io_op_t op_mode,
+		uint8_t channel, uint64_t *reg_data)
+{
+	int		status = NPI_SUCCESS;
+
+	if (!TXDMA_CHANNEL_VALID(channel)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txdma_ring_config"
+				    " Invalid Input: channel <0x%x>",
+				    channel));
+		return (NPI_FAILURE | NPI_TXDMA_CHANNEL_INVALID(channel));
+	}
+
+	switch (op_mode) {
+	case OP_GET:
+		TXDMA_REG_READ64(handle, TX_RNG_CFIG_REG, channel, reg_data);
+		break;
+
+	case OP_SET:
+		TXDMA_REG_WRITE64(handle, TX_RNG_CFIG_REG, channel,
+			*reg_data);
+		break;
+
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txdma_ring_config"
+				    " Invalid Input: ring_config <0x%x>",
+				    op_mode));
+		return (NPI_FAILURE | NPI_TXDMA_OPCODE_INVALID(channel));
+	}
+
+	return (status);
+}
+
+/*
+ * npi_txdma_mbox_config():
+ *	This function is called to config the mailbox address
+ *
+ * Parameters:
+ *	handle		- NPI handle (virtualization flag must be defined
+ *			  if its register pointer is from the virtual region).
+ *	channel		- logical TXDMA channel from 0 to 23.
+ *			  (If virtualization flag is not set, then
+ *			   logical channel is the same as the hardware
+ *			   channel number).
+ *	op_mode		- OP_GET: get the mailbox address
+ *			  OP_SET: set the mailbox address
+ *	reg_data	- pointer to the mailbox address.
+ * Return:
+ *	NPI_SUCCESS		- If set is complete successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE		-
+ *		NPI_TXDMA_OPCODE_INVALID	-
+ *		NPI_TXDMA_CHANNEL_INVALID	-
+ */
+npi_status_t
+npi_txdma_mbox_config(npi_handle_t handle, io_op_t op_mode,
+		uint8_t channel, uint64_t *mbox_addr)
+{
+	int		status = NPI_SUCCESS;
+	txdma_mbh_t	mh;
+	txdma_mbl_t	ml;
+
+	if (!TXDMA_CHANNEL_VALID(channel)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txdma_mbox_config"
+				    " Invalid Input: channel <0x%x>",
+				    channel));
+		return (NPI_FAILURE | NPI_TXDMA_CHANNEL_INVALID(channel));
+	}
+
+	mh.value = ml.value = 0;
+
+	switch (op_mode) {
+	case OP_GET:
+		TXDMA_REG_READ64(handle, TXDMA_MBH_REG, channel, &mh.value);
+		TXDMA_REG_READ64(handle, TXDMA_MBL_REG, channel, &ml.value);
+		*mbox_addr = ml.value;
+		*mbox_addr |= (mh.value << TXDMA_MBH_ADDR_SHIFT);
+
+		break;
+
+	case OP_SET:
+		ml.bits.ldw.mbaddr = ((*mbox_addr & TXDMA_MBL_MASK) >>
+			TXDMA_MBL_SHIFT);
+		TXDMA_REG_WRITE64(handle, TXDMA_MBL_REG, channel, ml.value);
+		mh.bits.ldw.mbaddr = ((*mbox_addr >> TXDMA_MBH_ADDR_SHIFT) &
+			TXDMA_MBH_MASK);
+		TXDMA_REG_WRITE64(handle, TXDMA_MBH_REG, channel, mh.value);
+
+		break;
+
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txdma_mbox_config"
+				    " Invalid Input: mbox <0x%x>",
+				    op_mode));
+		return (NPI_FAILURE | NPI_TXDMA_OPCODE_INVALID(channel));
+	}
+
+	return (status);
+
+}
+
+/*
+ * npi_txdma_desc_gather_set():
+ *	This function is called to set up a transmit descriptor entry.
+ *
+ * Parameters:
+ *	handle		- NPI handle (register pointer is the
+ *			  descriptor address in memory).
+ *	desc_p		- pointer to a descriptor
+ *	gather_index	- which entry (starts from index 0 to 15)
+ *	mark		- mark bit (only valid if it is the first gather).
+ *	ngathers	- number of gather pointers to set to the first gather.
+ *	dma_ioaddr	- starting dma address of an IO buffer to write.
+ *			  (SAD)
+ *	transfer_len	- transfer len.
+ * Return:
+ *	NPI_SUCCESS		- If set is complete successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE		-
+ *		NPI_TXDMA_OPCODE_INVALID	-
+ *		NPI_TXDMA_CHANNEL_INVALID	-
+ *		NPI_TXDMA_XFER_LEN_INVALID	-
+ */
+npi_status_t
+npi_txdma_desc_gather_set(npi_handle_t handle,
+		p_tx_desc_t desc_p, uint8_t gather_index,
+		boolean_t mark, uint8_t ngathers,
+		uint64_t dma_ioaddr, uint32_t transfer_len)
+{
+	int		status;
+
+	status = NPI_TXDMA_GATHER_INDEX(gather_index);
+	if (status) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txdma_desc_gather_set"
+				    " Invalid Input: gather_index <0x%x>",
+				    gather_index));
+		return (status);
+	}
+
+	if (transfer_len > TX_MAX_TRANSFER_LENGTH) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txdma_desc_gather_set"
+				    " Invalid Input: tr_len <0x%x>",
+				    transfer_len));
+		return (NPI_FAILURE | NPI_TXDMA_XFER_LEN_INVALID);
+	}
+
+	if (gather_index == 0) {
+		desc_p->bits.hdw.sop = 1;
+		desc_p->bits.hdw.mark = mark;
+		desc_p->bits.hdw.num_ptr = ngathers;
+		NPI_DEBUG_MSG((handle.function, NPI_TDC_CTL,
+			"npi_txdma_gather_set: SOP len %d (%d)",
+			desc_p->bits.hdw.tr_len, transfer_len));
+	}
+
+	desc_p->bits.hdw.tr_len = transfer_len;
+	desc_p->bits.hdw.sad = dma_ioaddr >> 32;
+	desc_p->bits.ldw.sad = dma_ioaddr & 0xffffffff;
+
+	NPI_DEBUG_MSG((handle.function, NPI_TDC_CTL,
+		"npi_txdma_gather_set: xfer len %d to set (%d)",
+		desc_p->bits.hdw.tr_len, transfer_len));
+
+	NXGE_MEM_PIO_WRITE64(handle, desc_p->value);
+
+	return (status);
+}
+
+/*
+ * npi_txdma_desc_sop_set():
+ *	This function is called to set up the first gather entry.
+ *
+ * Parameters:
+ *	handle		- NPI handle (register pointer is the
+ *			  descriptor address in memory).
+ *	desc_p		- pointer to a descriptor
+ *	mark		- mark bit (only valid if it is the first gather).
+ *	ngathers	- number of gather pointers to set to the first gather.
+ * Return:
+ *	NPI_SUCCESS		- If set is complete successfully.
+ *
+ *	Error:
+ */
+npi_status_t
+npi_txdma_desc_gather_sop_set(npi_handle_t handle,
+		p_tx_desc_t desc_p,
+		boolean_t mark_mode,
+		uint8_t ngathers)
+{
+	int		status = NPI_SUCCESS;
+
+	desc_p->bits.hdw.sop = 1;
+	desc_p->bits.hdw.mark = mark_mode;
+	desc_p->bits.hdw.num_ptr = ngathers;
+
+	NXGE_MEM_PIO_WRITE64(handle, desc_p->value);
+
+	return (status);
+}
+npi_status_t
+npi_txdma_desc_gather_sop_set_1(npi_handle_t handle,
+		p_tx_desc_t desc_p,
+		boolean_t mark_mode,
+		uint8_t ngathers,
+		uint32_t extra)
+{
+	int		status = NPI_SUCCESS;
+
+	desc_p->bits.hdw.sop = 1;
+	desc_p->bits.hdw.mark = mark_mode;
+	desc_p->bits.hdw.num_ptr = ngathers;
+	desc_p->bits.hdw.tr_len += extra;
+
+	NXGE_MEM_PIO_WRITE64(handle, desc_p->value);
+
+	return (status);
+}
+
+npi_status_t
+npi_txdma_desc_set_xfer_len(npi_handle_t handle,
+		p_tx_desc_t desc_p,
+		uint32_t transfer_len)
+{
+	int		status = NPI_SUCCESS;
+
+	desc_p->bits.hdw.tr_len = transfer_len;
+
+	NPI_DEBUG_MSG((handle.function, NPI_TDC_CTL,
+		"npi_set_xfer_len: len %d (%d)",
+		desc_p->bits.hdw.tr_len, transfer_len));
+
+	NXGE_MEM_PIO_WRITE64(handle, desc_p->value);
+
+	return (status);
+}
+
+npi_status_t
+npi_txdma_desc_set_zero(npi_handle_t handle, uint16_t entries)
+{
+	uint32_t	offset;
+	int		i;
+
+	/*
+	 * Assume no wrapped around.
+	 */
+	offset = 0;
+	for (i = 0; i < entries; i++) {
+		NXGE_REG_WR64(handle, offset, 0);
+		offset += (i * TXDMA_DESC_SIZE);
+	}
+
+	return (NPI_SUCCESS);
+}
+
+
+npi_status_t
+npi_txdma_desc_mem_get(npi_handle_t handle, uint16_t index,
+		p_tx_desc_t desc_p)
+{
+	int		status = NPI_SUCCESS;
+
+	npi_txdma_dump_desc_one(handle, desc_p, index);
+
+	return (status);
+
+}
+
+/*
+ * npi_txdma_desc_kick_reg_set():
+ *	This function is called to kick the transmit  to start transmission.
+ *
+ * Parameters:
+ *	handle		- NPI handle (virtualization flag must be defined).
+ *	channel		- logical TXDMA channel from 0 to 23.
+ *			  (If virtualization flag is not set, then
+ *			   logical channel is the same as the hardware
+ *			   channel number).
+ *	tail_index	- index into the transmit descriptor
+ *	wrap		- toggle bit to indicate if the tail index is
+ *			  wrapped around.
+ *
+ * Return:
+ *	NPI_SUCCESS		- If set is complete successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE		-
+ *		NPI_TXDMA_CHANNEL_INVALID	-
+ */
+npi_status_t
+npi_txdma_desc_kick_reg_set(npi_handle_t handle, uint8_t channel,
+		uint16_t tail_index, boolean_t wrap)
+{
+	int			status = NPI_SUCCESS;
+	tx_ring_kick_t		kick;
+
+	if (!TXDMA_CHANNEL_VALID(channel)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txdma_desc_kick_reg_set"
+				    " Invalid Input: channel <0x%x>",
+				    channel));
+		return (NPI_FAILURE | NPI_TXDMA_CHANNEL_INVALID(channel));
+	}
+
+	NPI_DEBUG_MSG((handle.function, NPI_TDC_CTL,
+		" npi_txdma_desc_kick_reg_set: "
+		" KICKING channel %d",
+		channel));
+
+	/* Toggle the wrap around bit */
+	kick.value = 0;
+	kick.bits.ldw.wrap = wrap;
+	kick.bits.ldw.tail = tail_index;
+
+	/* Kick start the Transmit kick register */
+	TXDMA_REG_WRITE64(handle, TX_RING_KICK_REG, channel, kick.value);
+
+	return (status);
+}
+
+/*
+ * npi_txdma_desc_kick_reg_get():
+ *	This function is called to kick the transmit  to start transmission.
+ *
+ * Parameters:
+ *	handle		- NPI handle (virtualization flag must be defined).
+ *	channel		- logical TXDMA channel from 0 to 23.
+ *			  (If virtualization flag is not set, then
+ *			   logical channel is the same as the hardware
+ *			   channel number).
+ *	tail_index	- index into the transmit descriptor
+ *	wrap		- toggle bit to indicate if the tail index is
+ *			  wrapped around.
+ *
+ * Return:
+ *	NPI_SUCCESS		- If get is complete successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE		-
+ *		NPI_TXDMA_CHANNEL_INVALID	-
+ */
+npi_status_t
+npi_txdma_desc_kick_reg_get(npi_handle_t handle, uint8_t channel,
+		p_tx_ring_kick_t kick_p)
+{
+	int		status = NPI_SUCCESS;
+
+	if (!TXDMA_CHANNEL_VALID(channel)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txdma_desc_kick_reg_get"
+				    " Invalid Input: channel <0x%x>",
+				    channel));
+		return (NPI_FAILURE | NPI_TXDMA_CHANNEL_INVALID(channel));
+	}
+
+	TXDMA_REG_READ64(handle, TX_RING_KICK_REG, channel, &kick_p->value);
+
+	return (status);
+}
+
+/*
+ * npi_txdma_ring_head_get():
+ *	This function is called to get the transmit ring head index.
+ *
+ * Parameters:
+ *	handle		- NPI handle (virtualization flag must be defined).
+ *	channel		- logical TXDMA channel from 0 to 23.
+ *			  (If virtualization flag is not set, then
+ *			   logical channel is the same as the hardware
+ *			   channel number).
+ *	hdl_p		- pointer to the hardware defined transmit
+ *			  ring header data (head index and wrap bit).
+ *
+ * Return:
+ *	NPI_SUCCESS		- If get is complete successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE		-
+ *		NPI_TXDMA_CHANNEL_INVALID	-
+ */
+npi_status_t
+npi_txdma_ring_head_get(npi_handle_t handle, uint8_t channel,
+		p_tx_ring_hdl_t hdl_p)
+{
+	int		status = NPI_SUCCESS;
+
+	if (!TXDMA_CHANNEL_VALID(channel)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txdma_ring_head_get"
+				    " Invalid Input: channel <0x%x>",
+				    channel));
+		return (NPI_FAILURE | NPI_TXDMA_CHANNEL_INVALID(channel));
+	}
+
+	TXDMA_REG_READ64(handle, TX_RING_HDL_REG, channel, &hdl_p->value);
+
+	return (status);
+}
+
+/*ARGSUSED*/
+npi_status_t
+npi_txdma_channel_mbox_get(npi_handle_t handle, uint8_t channel,
+		p_txdma_mailbox_t mbox_p)
+{
+	int		status = NPI_SUCCESS;
+
+	return (status);
+
+}
+
+npi_status_t
+npi_txdma_channel_pre_state_get(npi_handle_t handle, uint8_t channel,
+		p_tx_dma_pre_st_t prep)
+{
+	int		status = NPI_SUCCESS;
+
+	if (!TXDMA_CHANNEL_VALID(channel)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txdma_channel_pre_state_get"
+				    " Invalid Input: channel <0x%x>",
+				    channel));
+		return (NPI_FAILURE | NPI_TXDMA_CHANNEL_INVALID(channel));
+	}
+
+	TXDMA_REG_READ64(handle, TX_DMA_PRE_ST_REG, channel, &prep->value);
+
+	return (status);
+}
+
+npi_status_t
+npi_txdma_ring_error_get(npi_handle_t handle, uint8_t channel,
+		p_txdma_ring_errlog_t ring_errlog_p)
+{
+	tx_rng_err_logh_t	logh;
+	tx_rng_err_logl_t	logl;
+	int			status = NPI_SUCCESS;
+
+	if (!TXDMA_CHANNEL_VALID(channel)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_txdma_ring_error_get"
+				    " Invalid Input: channel <0x%x>",
+				    channel));
+		return (NPI_FAILURE | NPI_TXDMA_CHANNEL_INVALID(channel));
+	}
+
+	logh.value = 0;
+	TXDMA_REG_READ64(handle, TX_RNG_ERR_LOGH_REG, channel, &logh.value);
+	TXDMA_REG_READ64(handle, TX_RNG_ERR_LOGL_REG, channel, &logl.value);
+	ring_errlog_p->logh.bits.ldw.err = logh.bits.ldw.err;
+	ring_errlog_p->logh.bits.ldw.merr = logh.bits.ldw.merr;
+	ring_errlog_p->logh.bits.ldw.errcode = logh.bits.ldw.errcode;
+	ring_errlog_p->logh.bits.ldw.err_addr = logh.bits.ldw.err_addr;
+	ring_errlog_p->logl.bits.ldw.err_addr = logl.bits.ldw.err_addr;
+
+	return (status);
+}
+
+npi_status_t
+npi_txdma_inj_par_error_clear(npi_handle_t handle)
+{
+	NXGE_REG_WR64(handle, TDMC_INJ_PAR_ERR_REG, 0);
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_txdma_inj_par_error_set(npi_handle_t handle, uint32_t err_bits)
+{
+	tdmc_inj_par_err_t	inj;
+
+	inj.value = 0;
+	inj.bits.ldw.inject_parity_error = (err_bits & TDMC_INJ_PAR_ERR_MASK);
+	NXGE_REG_WR64(handle, TDMC_INJ_PAR_ERR_REG, inj.value);
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_txdma_inj_par_error_update(npi_handle_t handle, uint32_t err_bits)
+{
+	tdmc_inj_par_err_t	inj;
+
+	inj.value = 0;
+	NXGE_REG_RD64(handle, TDMC_INJ_PAR_ERR_REG, &inj.value);
+	inj.value |= (err_bits & TDMC_INJ_PAR_ERR_MASK);
+	NXGE_REG_WR64(handle, TDMC_INJ_PAR_ERR_REG, inj.value);
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_txdma_inj_par_error_get(npi_handle_t handle, uint32_t *err_bits)
+{
+	tdmc_inj_par_err_t	inj;
+
+	inj.value = 0;
+	NXGE_REG_RD64(handle, TDMC_INJ_PAR_ERR_REG, &inj.value);
+	*err_bits = (inj.value & TDMC_INJ_PAR_ERR_MASK);
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_txdma_dbg_sel_set(npi_handle_t handle, uint8_t dbg_sel)
+{
+	tdmc_dbg_sel_t		dbg;
+
+	dbg.value = 0;
+	dbg.bits.ldw.dbg_sel = (dbg_sel & TDMC_DBG_SEL_MASK);
+
+	NXGE_REG_WR64(handle, TDMC_DBG_SEL_REG, dbg.value);
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_txdma_training_vector_set(npi_handle_t handle, uint32_t training_vector)
+{
+	tdmc_training_t		vec;
+
+	vec.value = 0;
+	vec.bits.ldw.vec = training_vector;
+
+	NXGE_REG_WR64(handle, TDMC_TRAINING_REG, vec.value);
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_txdma_dump_desc_one(npi_handle_t handle, p_tx_desc_t desc_p,
+ *	int desc_index)
+ *
+ *	Dumps the contents of transmit descriptors.
+ *
+ * Parameters:
+ *	handle		- NPI handle (register pointer is the
+ *			  descriptor address in memory).
+ *	desc_p		- pointer to place the descriptor contents
+ *	desc_index	- descriptor index
+ *
+ */
+/*ARGSUSED*/
+void
+npi_txdma_dump_desc_one(npi_handle_t handle, p_tx_desc_t desc_p, int desc_index)
+{
+
+	tx_desc_t 		desc, *desp;
+#ifdef NXGE_DEBUG
+	uint64_t		sad;
+	int			xfer_len;
+#endif
+
+	NPI_DEBUG_MSG((handle.function, NPI_TDC_CTL,
+		"\n==> npi_txdma_dump_desc_one: dump "
+		" desc_p $%p descriptor entry %d\n",
+		desc_p, desc_index));
+	desc.value = 0;
+	desp = ((desc_p != NULL) ? desc_p : (p_tx_desc_t)&desc);
+	desp->value = NXGE_MEM_PIO_READ64(handle);
+#ifdef NXGE_DEBUG
+	sad = (desp->value & TX_PKT_DESC_SAD_MASK);
+	xfer_len = ((desp->value & TX_PKT_DESC_TR_LEN_MASK) >>
+			TX_PKT_DESC_TR_LEN_SHIFT);
+#endif
+	NPI_DEBUG_MSG((handle.function, NPI_TDC_CTL, "\n\t: value 0x%llx\n"
+		"\t\tsad $%p\ttr_len %d len %d\tnptrs %d\tmark %d sop %d\n",
+		desp->value,
+		sad,
+		desp->bits.hdw.tr_len,
+		xfer_len,
+		desp->bits.hdw.num_ptr,
+		desp->bits.hdw.mark,
+		desp->bits.hdw.sop));
+
+	NPI_DEBUG_MSG((handle.function, NPI_TDC_CTL,
+			    "\n<== npi_txdma_dump_desc_one: Done \n"));
+
+}
+
+/*ARGSUSED*/
+void
+npi_txdma_dump_hdr(npi_handle_t handle, p_tx_pkt_header_t hdrp)
+{
+	NPI_DEBUG_MSG((handle.function, NPI_TDC_CTL,
+				    "\n==> npi_txdma_dump_hdr: dump\n"));
+	NPI_DEBUG_MSG((handle.function, NPI_TDC_CTL,
+				    "\n\t: value 0x%llx\n"
+		"\t\tpkttype 0x%x\tip_ver %d\tllc %d\tvlan %d \tihl %d\n"
+		"\t\tl3start %d\tl4start %d\tl4stuff %d\n"
+		"\t\txferlen %d\tpad %d\n",
+		hdrp->value,
+		hdrp->bits.hdw.cksum_en_pkt_type,
+		hdrp->bits.hdw.ip_ver,
+		hdrp->bits.hdw.llc,
+		hdrp->bits.hdw.vlan,
+		hdrp->bits.hdw.ihl,
+		hdrp->bits.hdw.l3start,
+		hdrp->bits.hdw.l4start,
+		hdrp->bits.hdw.l4stuff,
+		hdrp->bits.ldw.tot_xfer_len,
+		hdrp->bits.ldw.pad));
+
+	NPI_DEBUG_MSG((handle.function, NPI_TDC_CTL,
+			    "\n<== npi_txdma_dump_hdr: Done \n"));
+}
+
+npi_status_t
+npi_txdma_inj_int_error_set(npi_handle_t handle, uint8_t channel,
+	p_tdmc_intr_dbg_t erp)
+{
+	int		status = NPI_SUCCESS;
+
+	if (!TXDMA_CHANNEL_VALID(channel)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_txdma_inj_int_error_set"
+			" Invalid Input: channel <0x%x>",
+					    channel));
+		return (NPI_FAILURE | NPI_TXDMA_CHANNEL_INVALID(channel));
+	}
+
+	TXDMA_REG_WRITE64(handle, TDMC_INTR_DBG_REG, channel, erp->value);
+
+	return (status);
+}
+
+/*
+ * Static functions start here.
+ */
+static npi_status_t
+npi_txdma_control_reset_wait(npi_handle_t handle, uint8_t channel)
+{
+
+	tx_cs_t		txcs;
+	int		loop = 0;
+
+	do {
+		NXGE_DELAY(TXDMA_WAIT_MSEC);
+		TXDMA_REG_READ64(handle, TX_CS_REG, channel, &txcs.value);
+		if (!txcs.bits.ldw.rst) {
+			return (NPI_SUCCESS);
+		}
+		loop++;
+	} while (loop < TXDMA_WAIT_LOOP);
+
+	if (loop == TXDMA_WAIT_LOOP) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			    "npi_txdma_control_reset_wait: RST bit not "
+			    "cleared to 0 txcs.bits 0x%llx", txcs.value));
+		return (NPI_FAILURE | NPI_TXDMA_RESET_FAILED);
+	}
+	return (NPI_SUCCESS);
+}
+
+static npi_status_t
+npi_txdma_control_stop_wait(npi_handle_t handle, uint8_t channel)
+{
+	tx_cs_t		txcs;
+	int		loop = 0;
+
+	do {
+		NXGE_DELAY(TXDMA_WAIT_MSEC);
+		TXDMA_REG_READ64(handle, TX_CS_REG, channel, &txcs.value);
+		if (txcs.bits.ldw.sng_state) {
+			return (NPI_SUCCESS);
+		}
+		loop++;
+	} while (loop < TXDMA_WAIT_LOOP);
+
+	if (loop == TXDMA_WAIT_LOOP) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			    "npi_txdma_control_stop_wait: SNG_STATE not "
+			    "set to 1 txcs.bits 0x%llx", txcs.value));
+		return (NPI_FAILURE | NPI_TXDMA_STOP_FAILED);
+	}
+
+	return (NPI_SUCCESS);
+}
+
+static npi_status_t
+npi_txdma_control_resume_wait(npi_handle_t handle, uint8_t channel)
+{
+	tx_cs_t		txcs;
+	int		loop = 0;
+
+	do {
+		NXGE_DELAY(TXDMA_WAIT_MSEC);
+		TXDMA_REG_READ64(handle, TX_CS_REG, channel, &txcs.value);
+		if (!txcs.bits.ldw.sng_state) {
+			return (NPI_SUCCESS);
+		}
+		loop++;
+	} while (loop < TXDMA_WAIT_LOOP);
+
+	if (loop == TXDMA_WAIT_LOOP) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			    "npi_txdma_control_resume_wait: sng_state not "
+			    "set to 0 txcs.bits 0x%llx", txcs.value));
+		return (NPI_FAILURE | NPI_TXDMA_RESUME_FAILED);
+	}
+
+	return (NPI_SUCCESS);
+}
diff --git a/drivers/net/nxge/npi/npi_txdma.h b/drivers/net/nxge/npi/npi_txdma.h
new file mode 100644
index 0000000..381f320
--- /dev/null
+++ b/drivers/net/nxge/npi/npi_txdma.h
@@ -0,0 +1,303 @@
+/*
+ * npi_txdma.h	Neptune  TX DMA HW API functions
+ *
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef _NPI_TXDMA_H
+#define	_NPI_TXDMA_H
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#include <npi.h>
+#include <nxge_hw.h>
+#include <nxge_txdma_hw.h>
+
+#define	DMA_LOG_PAGE_FN_VALIDATE(cn, pn, fn, status)	\
+{									\
+	status = NPI_SUCCESS;						\
+	if (!TXDMA_CHANNEL_VALID(channel)) {				\
+		status = (NPI_FAILURE | NPI_TXDMA_CHANNEL_INVALID(cn));	\
+	} else if (!TXDMA_PAGE_VALID(pn)) {			\
+		status =  (NPI_FAILURE | NPI_TXDMA_PAGE_INVALID(pn));	\
+	} else if (!TXDMA_FUNC_VALID(fn)) {			\
+		status =  (NPI_FAILURE | NPI_TXDMA_FUNC_INVALID(fn));	\
+	} \
+}
+
+#define	DMA_LOG_PAGE_VALIDATE(cn, pn, status)	\
+{									\
+	status = NPI_SUCCESS;						\
+	if (!TXDMA_CHANNEL_VALID(channel)) {				\
+		status = (NPI_FAILURE | NPI_TXDMA_CHANNEL_INVALID(cn));	\
+	} else if (!TXDMA_PAGE_VALID(pn)) {			\
+		status =  (NPI_FAILURE | NPI_TXDMA_PAGE_INVALID(pn));	\
+	} \
+}
+
+typedef	enum _txdma_cs_cntl_e {
+	TXDMA_INIT_RESET	= 0x1,
+	TXDMA_INIT_START	= 0x2,
+	TXDMA_START		= 0x3,
+	TXDMA_RESET		= 0x4,
+	TXDMA_STOP		= 0x5,
+	TXDMA_RESUME		= 0x6,
+	TXDMA_CLEAR_MMK		= 0x7,
+	TXDMA_MBOX_ENABLE	= 0x8
+} txdma_cs_cntl_t;
+
+typedef	enum _txdma_log_cfg_e {
+	TXDMA_LOG_PAGE_MASK	= 0x01,
+	TXDMA_LOG_PAGE_VALUE	= 0x02,
+	TXDMA_LOG_PAGE_RELOC	= 0x04,
+	TXDMA_LOG_PAGE_VALID	= 0x08,
+	TXDMA_LOG_PAGE_ALL	= (TXDMA_LOG_PAGE_MASK | TXDMA_LOG_PAGE_VALUE |
+				TXDMA_LOG_PAGE_RELOC | TXDMA_LOG_PAGE_VALID)
+} txdma_log_cfg_t;
+
+typedef	enum _txdma_ent_msk_cfg_e {
+	CFG_TXDMA_PKT_PRT_MASK		= TX_ENT_MSK_PKT_PRT_ERR_MASK,
+	CFG_TXDMA_CONF_PART_MASK	= TX_ENT_MSK_CONF_PART_ERR_MASK,
+	CFG_TXDMA_NACK_PKT_RD_MASK	= TX_ENT_MSK_NACK_PKT_RD_MASK,
+	CFG_TXDMA_NACK_PREF_MASK	= TX_ENT_MSK_NACK_PREF_MASK,
+	CFG_TXDMA_PREF_BUF_ECC_ERR_MASK	= TX_ENT_MSK_PREF_BUF_ECC_ERR_MASK,
+	CFG_TXDMA_TX_RING_OFLOW_MASK	= TX_ENT_MSK_TX_RING_OFLOW_MASK,
+	CFG_TXDMA_PKT_SIZE_ERR_MASK	= TX_ENT_MSK_PKT_SIZE_ERR_MASK,
+	CFG_TXDMA_MBOX_ERR_MASK		= TX_ENT_MSK_MBOX_ERR_MASK,
+	CFG_TXDMA_MK_MASK		= TX_ENT_MSK_MK_MASK,
+	CFG_TXDMA_MASK_ALL		= (TX_ENT_MSK_PKT_PRT_ERR_MASK |
+					TX_ENT_MSK_CONF_PART_ERR_MASK |
+					TX_ENT_MSK_NACK_PKT_RD_MASK |
+					TX_ENT_MSK_NACK_PREF_MASK |
+					TX_ENT_MSK_PREF_BUF_ECC_ERR_MASK |
+					TX_ENT_MSK_TX_RING_OFLOW_MASK |
+					TX_ENT_MSK_PKT_SIZE_ERR_MASK |
+					TX_ENT_MSK_MBOX_ERR_MASK |
+					TX_ENT_MSK_MK_MASK)
+} txdma_ent_msk_cfg_t;
+
+
+typedef	struct _txdma_ring_errlog {
+	tx_rng_err_logl_t	logl;
+	tx_rng_err_logh_t	logh;
+} txdma_ring_errlog_t, *p_txdma_ring_errlog_t;
+
+/*
+ * Register offset (0x200 bytes for each channel) for logical pages registers.
+ */
+#define	NXGE_TXLOG_OFFSET(x, channel) (x + TX_LOG_DMA_OFFSET(channel))
+
+/*
+ * Register offset (0x200 bytes for each channel) for transmit ring registers.
+ * (Ring configuration, kick register, event mask, control and status,
+ *  mailbox, prefetch, ring errors).
+ */
+#define	NXGE_TXDMA_OFFSET(x, v, channel) (x + \
+		(!v ? DMC_OFFSET(channel) : TDMC_PIOVADDR_OFFSET(channel)))
+/*
+ * Register offset (0x8 bytes for each port) for transmit mapping registers.
+ */
+#define	NXGE_TXDMA_MAP_OFFSET(x, port) (x + TX_DMA_MAP_PORT_OFFSET(port))
+
+/*
+ * Register offset (0x10 bytes for each channel) for transmit DRR and ring
+ * usage registers.
+ */
+#define	NXGE_TXDMA_DRR_OFFSET(x, channel) (x + \
+			TXDMA_DRR_RNG_USE_OFFSET(channel))
+
+/*
+ * PIO macros to read and write the transmit registers.
+ */
+#define	TX_LOG_REG_READ64(handle, reg, channel, val_p)	\
+	NXGE_REG_RD64(handle, NXGE_TXLOG_OFFSET(reg, channel), val_p)
+
+#define	TX_LOG_REG_WRITE64(handle, reg, channel, data)	\
+	NXGE_REG_WR64(handle, NXGE_TXLOG_OFFSET(reg, channel), data)
+
+#define	TXDMA_REG_READ64(handle, reg, channel, val_p)	\
+		NXGE_REG_RD64(handle, \
+		(NXGE_TXDMA_OFFSET(reg, handle.is_vraddr, channel)), val_p)
+
+#define	TXDMA_REG_WRITE64(handle, reg, channel, data)	\
+		NXGE_REG_WR64(handle, \
+		NXGE_TXDMA_OFFSET(reg, handle.is_vraddr, channel), data)
+
+#define	TX_DRR_RNGUSE_REG_READ64(handle, reg, channel, val_p)	\
+	NXGE_REG_RD64(handle, (NXGE_TXDMA_DRR_OFFSET(reg, channel)), val_p)
+
+#define	TX_DRR_RNGUSE_REG_WRITE64(handle, reg, channel, data)	\
+	NXGE_REG_WR64(handle, NXGE_TXDMA_DRR_OFFSET(reg, channel), data)
+
+/*
+ * Transmit Descriptor Definitions.
+ */
+#define	TXDMA_DESC_SIZE			(sizeof (tx_desc_t))
+
+#define	NPI_TXDMA_GATHER_INDEX(index)	\
+	((index <= TX_MAX_GATHER_POINTERS)) ? NPI_SUCCESS : \
+				(NPI_TXDMA_GATHER_INVALID)
+
+/*
+ * Transmit NPI error codes
+ */
+#define	TXDMA_ER_ST			(TXDMA_BLK_ID << NPI_BLOCK_ID_SHIFT)
+#define	TXDMA_ID_SHIFT(n)		(n << NPI_PORT_CHAN_SHIFT)
+
+#define	TXDMA_HW_STOP_FAILED		(NPI_BK_HW_ER_START | 0x1)
+#define	TXDMA_HW_RESUME_FAILED		(NPI_BK_HW_ER_START | 0x2)
+
+#define	TXDMA_GATHER_INVALID		(NPI_BK_ERROR_START | 0x1)
+#define	TXDMA_XFER_LEN_INVALID		(NPI_BK_ERROR_START | 0x2)
+
+#define	NPI_TXDMA_OPCODE_INVALID(n)	(TXDMA_ID_SHIFT(n) |	\
+					TXDMA_ER_ST | OPCODE_INVALID)
+
+#define	NPI_TXDMA_FUNC_INVALID(n)	(TXDMA_ID_SHIFT(n) |	\
+					TXDMA_ER_ST | PORT_INVALID)
+#define	NPI_TXDMA_CHANNEL_INVALID(n)	(TXDMA_ID_SHIFT(n) |	\
+					TXDMA_ER_ST | CHANNEL_INVALID)
+
+#define	NPI_TXDMA_PAGE_INVALID(n)	(TXDMA_ID_SHIFT(n) |	\
+					TXDMA_ER_ST | LOGICAL_PAGE_INVALID)
+
+#define	NPI_TXDMA_REGISTER_INVALID	(TXDMA_ER_ST | REGISTER_INVALID)
+#define	NPI_TXDMA_COUNTER_INVALID	(TXDMA_ER_ST | COUNTER_INVALID)
+#define	NPI_TXDMA_CONFIG_INVALID	(TXDMA_ER_ST | CONFIG_INVALID)
+
+
+#define	NPI_TXDMA_GATHER_INVALID	(TXDMA_ER_ST | TXDMA_GATHER_INVALID)
+#define	NPI_TXDMA_XFER_LEN_INVALID	(TXDMA_ER_ST | TXDMA_XFER_LEN_INVALID)
+
+#define	NPI_TXDMA_RESET_FAILED		(TXDMA_ER_ST | RESET_FAILED)
+#define	NPI_TXDMA_STOP_FAILED		(TXDMA_ER_ST | TXDMA_HW_STOP_FAILED)
+#define	NPI_TXDMA_RESUME_FAILED		(TXDMA_ER_ST | TXDMA_HW_RESUME_FAILED)
+
+/*
+ * Transmit DMA Channel NPI Prototypes.
+ */
+npi_status_t npi_txdma_mode32_set(npi_handle_t, boolean_t);
+npi_status_t npi_txdma_log_page_set(npi_handle_t, uint8_t,
+		p_dma_log_page_t);
+npi_status_t npi_txdma_log_page_get(npi_handle_t, uint8_t,
+		p_dma_log_page_t);
+npi_status_t npi_txdma_log_page_handle_set(npi_handle_t, uint8_t,
+		p_log_page_hdl_t);
+npi_status_t npi_txdma_log_page_config(npi_handle_t, io_op_t,
+		txdma_log_cfg_t, uint8_t, p_dma_log_page_t);
+npi_status_t npi_txdma_log_page_vld_config(npi_handle_t, io_op_t,
+		uint8_t, p_log_page_vld_t);
+npi_status_t npi_txdma_drr_weight_set(npi_handle_t, uint8_t,
+		uint32_t);
+npi_status_t npi_txdma_channel_reset(npi_handle_t, uint8_t);
+npi_status_t npi_txdma_channel_init_enable(npi_handle_t,
+		uint8_t);
+npi_status_t npi_txdma_channel_enable(npi_handle_t, uint8_t);
+npi_status_t npi_txdma_channel_disable(npi_handle_t, uint8_t);
+npi_status_t npi_txdma_channel_resume(npi_handle_t, uint8_t);
+npi_status_t npi_txdma_channel_mmk_clear(npi_handle_t, uint8_t);
+npi_status_t npi_txdma_channel_mbox_enable(npi_handle_t, uint8_t);
+npi_status_t npi_txdma_channel_control(npi_handle_t,
+		txdma_cs_cntl_t, uint8_t);
+npi_status_t npi_txdma_control_status(npi_handle_t, io_op_t,
+		uint8_t, p_tx_cs_t);
+
+npi_status_t npi_txdma_event_mask(npi_handle_t, io_op_t,
+		uint8_t, p_tx_dma_ent_msk_t);
+npi_status_t npi_txdma_event_mask_config(npi_handle_t, io_op_t,
+		uint8_t, txdma_ent_msk_cfg_t *);
+npi_status_t npi_txdma_event_mask_mk_out(npi_handle_t, uint8_t);
+npi_status_t npi_txdma_event_mask_mk_in(npi_handle_t, uint8_t);
+
+npi_status_t npi_txdma_ring_addr_set(npi_handle_t, uint8_t,
+		uint64_t, uint32_t);
+npi_status_t npi_txdma_ring_config(npi_handle_t, io_op_t,
+		uint8_t, uint64_t *);
+npi_status_t npi_txdma_mbox_config(npi_handle_t, io_op_t,
+		uint8_t, uint64_t *);
+npi_status_t npi_txdma_desc_gather_set(npi_handle_t,
+		p_tx_desc_t, uint8_t,
+		boolean_t, uint8_t,
+		uint64_t, uint32_t);
+
+npi_status_t npi_txdma_desc_gather_sop_set(npi_handle_t,
+		p_tx_desc_t, boolean_t, uint8_t);
+
+npi_status_t npi_txdma_desc_gather_sop_set_1(npi_handle_t,
+		p_tx_desc_t, boolean_t, uint8_t,
+		uint32_t);
+
+npi_status_t npi_txdma_desc_set_xfer_len(npi_handle_t,
+		p_tx_desc_t, uint32_t);
+
+npi_status_t npi_txdma_desc_set_zero(npi_handle_t, uint16_t);
+npi_status_t npi_txdma_desc_mem_get(npi_handle_t, uint16_t,
+		p_tx_desc_t);
+npi_status_t npi_txdma_desc_kick_reg_set(npi_handle_t, uint8_t,
+		uint16_t, boolean_t);
+npi_status_t npi_txdma_desc_kick_reg_get(npi_handle_t, uint8_t,
+		p_tx_ring_kick_t);
+npi_status_t npi_txdma_ring_head_get(npi_handle_t, uint8_t,
+		p_tx_ring_hdl_t);
+npi_status_t npi_txdma_channel_mbox_get(npi_handle_t, uint8_t,
+		p_txdma_mailbox_t);
+npi_status_t npi_txdma_channel_pre_state_get(npi_handle_t,
+		uint8_t, p_tx_dma_pre_st_t);
+npi_status_t npi_txdma_ring_error_get(npi_handle_t,
+		uint8_t, p_txdma_ring_errlog_t);
+npi_status_t npi_txdma_inj_par_error_clear(npi_handle_t);
+npi_status_t npi_txdma_inj_par_error_set(npi_handle_t,
+		uint32_t);
+npi_status_t npi_txdma_inj_par_error_update(npi_handle_t,
+		uint32_t);
+npi_status_t npi_txdma_inj_par_error_get(npi_handle_t,
+		uint32_t *);
+npi_status_t npi_txdma_dbg_sel_set(npi_handle_t, uint8_t);
+npi_status_t npi_txdma_training_vector_set(npi_handle_t,
+		uint32_t);
+void npi_txdma_dump_desc_one(npi_handle_t, p_tx_desc_t,
+	int);
+npi_status_t npi_txdma_dump_tdc_regs(npi_handle_t, uint8_t);
+npi_status_t npi_txdma_dump_fzc_regs(npi_handle_t);
+npi_status_t npi_txdma_inj_int_error_set(npi_handle_t, uint8_t,
+	p_tdmc_intr_dbg_t);
+#ifdef	__cplusplus
+}
+#endif
+
+#endif	/* _NPI_TXDMA_H */
diff --git a/drivers/net/nxge/npi/npi_vir.c b/drivers/net/nxge/npi/npi_vir.c
new file mode 100644
index 0000000..72da0d6
--- /dev/null
+++ b/drivers/net/nxge/npi/npi_vir.c
@@ -0,0 +1,1499 @@
+/*
+ * npi_vir.c	Neptune HW virtualization HW API functions
+ *
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#include	<npi_vir.h>
+
+/* One register only */
+uint64_t pio_offset[] = {
+	DEV_FUNC_SR_REG
+};
+
+const char *pio_name[] = {
+	"DEV_FUNC_SR_REG",
+};
+
+/* One register only */
+uint64_t fzc_pio_offset[] = {
+	MULTI_PART_CTL_REG,
+	LDGITMRES_REG
+};
+
+const char *fzc_pio_name[] = {
+	"MULTI_PART_CTL_REG",
+	"LDGITMRES_REG"
+};
+
+/* 64 sets */
+uint64_t fzc_pio_dma_bind_offset[] = {
+	DMA_BIND_REG
+};
+
+const char *fzc_pio_dma_bind_name[] = {
+	"DMA_BIND_REG",
+};
+
+/* 69 logical devices */
+uint64_t fzc_pio_ldgnum_offset[] = {
+	LDG_NUM_REG
+};
+
+const char *fzc_pio_ldgnum_name[] = {
+	"LDG_NUM_REG",
+};
+
+/* PIO_LDSV, 64 sets by 8192 bytes */
+uint64_t pio_ldsv_offset[] = {
+	LDSV0_REG,
+	LDSV1_REG,
+	LDSV2_REG,
+	LDGIMGN_REG
+};
+const char *pio_ldsv_name[] = {
+	"LDSV0_REG",
+	"LDSV1_REG",
+	"LDSV2_REG",
+	"LDGIMGN_REG"
+};
+
+/* PIO_IMASK0: 64 by 8192 */
+uint64_t pio_imask0_offset[] = {
+	LD_IM0_REG,
+};
+
+const char *pio_imask0_name[] = {
+	"LD_IM0_REG",
+};
+
+/* PIO_IMASK1: 5 by 8192 */
+uint64_t pio_imask1_offset[] = {
+	LD_IM1_REG
+};
+
+const char *pio_imask1_name[] = {
+	"LD_IM1_REG"
+};
+
+/* SID: 64 by 8 */
+uint64_t fzc_pio_sid_offset[] = {
+	SID_REG
+};
+
+const char *fzc_pio_sid_name[] = {
+	"SID_REG"
+};
+
+npi_status_t
+npi_vir_dump_pio_fzc_regs_one(npi_handle_t handle)
+{
+	uint64_t value;
+	int num_regs, i;
+
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+		"\nPIO FZC Common Register Dump\n"));
+
+	num_regs = sizeof (pio_offset) / sizeof (uint64_t);
+	for (i = 0; i < num_regs; i++) {
+		value = 0;
+		NXGE_REG_RD64(handle, pio_offset[i], &value);
+		NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL, "0x%08llx "
+			"%s\t 0x%08llx \n",
+			pio_offset[i],
+			pio_name[i], value));
+	}
+
+	num_regs = sizeof (fzc_pio_offset) / sizeof (uint64_t);
+	for (i = 0; i < num_regs; i++) {
+		NXGE_REG_RD64(handle, fzc_pio_offset[i], &value);
+		NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL, "0x%08llx "
+			"%s\t 0x%08llx \n",
+			fzc_pio_offset[i],
+			fzc_pio_name[i], value));
+	}
+
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+		"\n PIO FZC Register Dump Done \n"));
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_vir_dump_ldgnum(npi_handle_t handle)
+{
+	uint64_t value = 0, offset = 0;
+	int num_regs, i, ldv;
+
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+		"\nFZC PIO LDG Number Register Dump\n"));
+
+	num_regs = sizeof (fzc_pio_ldgnum_offset) / sizeof (uint64_t);
+	for (ldv = 0; ldv < NXGE_INT_MAX_LDS; ldv++) {
+		for (i = 0; i < num_regs; i++) {
+			value = 0;
+			offset = fzc_pio_ldgnum_offset[i] + 8 * ldv;
+			NXGE_REG_RD64(handle, offset, &value);
+			NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+				"Logical Device %d: 0x%08llx "
+				"%s\t 0x%08llx \n",
+				ldv, offset,
+				fzc_pio_ldgnum_name[i], value));
+		}
+	}
+
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+		"\n FZC PIO LDG Register Dump Done \n"));
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_vir_dump_ldsv(npi_handle_t handle)
+{
+	uint64_t value, offset;
+	int num_regs, i, ldg;
+
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+		"\nLD Device State Vector Register Dump\n"));
+
+	num_regs = sizeof (pio_ldsv_offset) / sizeof (uint64_t);
+	for (ldg = 0; ldg < NXGE_INT_MAX_LDGS; ldg++) {
+		for (i = 0; i < num_regs; i++) {
+			value = 0;
+			offset = pio_ldsv_offset[i] + 8192 * ldg;
+			NXGE_REG_RD64(handle, offset, &value);
+			NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+				    "LDG State: group %d: 0x%08llx "
+				    "%s\t 0x%08llx \n",
+				ldg, offset,
+				pio_ldsv_name[i], value));
+		}
+	}
+
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+		"\n FZC PIO LDG Register Dump Done \n"));
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_vir_dump_imask0(npi_handle_t handle)
+{
+	uint64_t value, offset;
+	int num_regs, i, ldv;
+
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+		"\nLD Interrupt Mask Register Dump\n"));
+
+	num_regs = sizeof (pio_imask0_offset) / sizeof (uint64_t);
+	for (ldv = 0; ldv < 64; ldv++) {
+		for (i = 0; i < num_regs; i++) {
+			value = 0;
+			offset = pio_imask0_offset[i] + 8192 * ldv;
+			NXGE_REG_RD64(handle, offset,
+				&value);
+			NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+				"LD Interrupt Mask %d: 0x%08llx "
+				"%s\t 0x%08llx \n",
+				ldv, offset,
+				pio_imask0_name[i], value));
+		}
+	}
+	num_regs = sizeof (pio_imask1_offset) / sizeof (uint64_t);
+	for (ldv = 64; ldv < 69; ldv++) {
+		for (i = 0; i < num_regs; i++) {
+			value = 0;
+			offset = pio_imask1_offset[i] + 8192 * (ldv - 64);
+			NXGE_REG_RD64(handle, offset,
+				&value);
+			NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+				"LD Interrupt Mask %d: 0x%08llx "
+				"%s\t 0x%08llx \n",
+				ldv, offset,
+				pio_imask1_name[i], value));
+		}
+	}
+
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+		"\n FZC PIO Logical Device Group Register Dump Done \n"));
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_vir_dump_sid(npi_handle_t handle)
+{
+	uint64_t value, offset;
+	int num_regs, i, ldg;
+
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+		"\nSystem Interrupt Data Register Dump\n"));
+
+	num_regs = sizeof (fzc_pio_sid_offset) / sizeof (uint64_t);
+	for (ldg = 0; ldg < NXGE_INT_MAX_LDGS; ldg++) {
+		for (i = 0; i < num_regs; i++) {
+			value = 0;
+			offset = fzc_pio_sid_offset[i] + 8 * ldg;
+			NXGE_REG_RD64(handle, offset,
+				&value);
+			NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+				"SID for group %d: 0x%08llx "
+				"%s\t 0x%08llx \n",
+				ldg, offset,
+				fzc_pio_sid_name[i], value));
+		}
+	}
+
+	NPI_REG_DUMP_MSG((handle.function, NPI_REG_CTL,
+		"\n FZC PIO SID Register Dump Done \n"));
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_dev_func_sr_init():
+ *	This function is called to initialize the device function
+ *	shared register (set the software implementation lock
+ *	state to FREE).
+ * Parameters:
+ *	handle		- NPI handle
+ * Return:
+ *	NPI_SUCCESS	- If initialization is complete successfully.
+ *			  (set sr bits to free).
+ *	Error:
+ *	NPI_FAILURE
+ *		VIR_TAS_BUSY
+ */
+npi_status_t
+npi_dev_func_sr_init(npi_handle_t handle)
+{
+	dev_func_sr_t		sr;
+	int			status = NPI_SUCCESS;
+
+	NXGE_REG_RD64(handle, DEV_FUNC_SR_REG, &sr.value);
+	if (!sr.bits.ldw.tas) {
+		/*
+		 * After read, this bit is set to 1 by hardware.
+		 * We own it if tas bit read as 0.
+		 * Set the lock state to free if it is in reset state.
+		 */
+		if (!sr.bits.ldw.sr) {
+			/* reset state */
+			sr.bits.ldw.sr |= NPI_DEV_SR_LOCK_ST_FREE;
+			NXGE_REG_WR64(handle, DEV_FUNC_SR_REG, sr.value);
+			sr.bits.ldw.tas = 0;
+			NXGE_REG_WR64(handle, DEV_FUNC_SR_REG, sr.value);
+		}
+
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			" npi_dev_func_sr_init"
+			" sr <0x%x>",
+			sr.bits.ldw.sr));
+	} else {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_dev_func_sr_init"
+				    " tas busy <0x%x>",
+				    sr.bits.ldw));
+		status = NPI_VIR_TAS_BUSY(sr.bits.ldw.funcid);
+	}
+
+	return (status);
+}
+
+/*
+ * npi_dev_func_sr_lock_enter():
+ *	This function is called to lock the function shared register
+ *	by setting the lock state to busy.
+ * Parameters:
+ *	handle		- NPI handle
+ * Return:
+ *	NPI_SUCCESS	- If the function id can own the lock.
+ *
+ *	Error:
+ *	NPI_FAILURE
+ *		VIR_SR_RESET
+ *		VIR_SR_BUSY
+ *		VIR_SR_INVALID
+ *		VIR_TAS_BUSY
+ */
+npi_status_t
+npi_dev_func_sr_lock_enter(npi_handle_t handle)
+{
+	dev_func_sr_t		sr;
+	int			status = NPI_SUCCESS;
+	uint32_t		state;
+
+	NXGE_REG_RD64(handle, DEV_FUNC_SR_REG, &sr.value);
+	if (!sr.bits.ldw.tas) {
+		/*
+		 * tas bit will be set to 1 by hardware.
+		 * reset tas bit when we unlock the sr.
+		 */
+		state = sr.bits.ldw.sr & NPI_DEV_SR_LOCK_ST_MASK;
+		switch (state) {
+		case NPI_DEV_SR_LOCK_ST_FREE:
+			/*
+			 * set it to busy and our function id.
+			 */
+			sr.bits.ldw.sr |= (NPI_DEV_SR_LOCK_ST_BUSY |
+						(sr.bits.ldw.funcid <<
+						NPI_DEV_SR_LOCK_FID_SHIFT));
+			NXGE_REG_WR64(handle, DEV_FUNC_SR_REG, sr.value);
+			break;
+
+		case NPI_DEV_SR_LOCK_ST_RESET:
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_dev_func_sr_lock_enter"
+					    " reset state <0x%x>",
+					    sr.bits.ldw.sr));
+			status = NPI_VIR_SR_RESET(sr.bits.ldw.funcid);
+			break;
+
+		case NPI_DEV_SR_LOCK_ST_BUSY:
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_dev_func_sr_lock_enter"
+					    " busy <0x%x>",
+					    sr.bits.ldw.sr));
+			status = NPI_VIR_SR_BUSY(sr.bits.ldw.funcid);
+			break;
+
+		default:
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_dev_func_sr_lock_enter",
+					    " invalid state",
+					    sr.bits.ldw.sr));
+			status = NPI_VIR_SR_INVALID(sr.bits.ldw.funcid);
+			break;
+		}
+	} else {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_dev_func_sr_lock_enter",
+				    " tas busy", sr.bits.ldw));
+		status = NPI_VIR_TAS_BUSY(sr.bits.ldw.funcid);
+	}
+
+	return (status);
+}
+
+/*
+ * npi_dev_func_sr_lock_free():
+ *	This function is called to free the function shared register
+ *	by setting the lock state to free.
+ * Parameters:
+ *	handle		- NPI handle
+ * Return:
+ *	NPI_SUCCESS	- If the function id can free the lock.
+ *
+ *	Error:
+ *	NPI_FAILURE
+ *		VIR_SR_NOTOWNER
+ *		VIR_TAS_NOTREAD
+ */
+npi_status_t
+npi_dev_func_sr_lock_free(npi_handle_t handle)
+{
+	dev_func_sr_t		sr;
+	int			status = NPI_SUCCESS;
+
+	NXGE_REG_RD64(handle, DEV_FUNC_SR_REG, &sr.value);
+	if (sr.bits.ldw.tas) {
+		if (sr.bits.ldw.funcid == NPI_GET_LOCK_OWNER(sr.bits.ldw.sr)) {
+			sr.bits.ldw.sr &= NPI_DEV_SR_IMPL_ST_MASK;
+			sr.bits.ldw.sr |= NPI_DEV_SR_LOCK_ST_FREE;
+			sr.bits.ldw.tas = 0;
+			NXGE_REG_WR64(handle, DEV_FUNC_SR_REG, sr.value);
+		} else {
+			NPI_DEBUG_MSG((handle.function, NPI_VIR_CTL,
+					    " npi_dev_func_sr_lock_free"
+					    " not owner <0x%x>",
+					    sr.bits.ldw.sr));
+			status = NPI_VIR_SR_NOTOWNER(sr.bits.ldw.funcid);
+		}
+	} else {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_dev_func_sr_lock_free",
+				    " invalid tas state <0x%x>",
+				    sr.bits.ldw.tas));
+		status = NPI_VIR_TAS_NOTREAD(sr.bits.ldw.funcid);
+	}
+
+	return (status);
+}
+
+/*
+ * npi_dev_func_sr_funcid_get():
+ *	This function is called to get the caller's function ID.
+ *	(based on address bits [25:26] on read access.
+ *	(After read, the TAS bit is always set to 1. Software needs
+ *	to write 0 to clear.) This function will write 0 to clear
+ *	the TAS bit if we own it.
+ * Parameters:
+ *	handle		- NPI handle
+ *	funcid_p	- pointer to store the function id.
+ * Return:
+ *	NPI_SUCCESS	- If get function id is complete successfully.
+ *
+ *	Error:
+ */
+npi_status_t
+npi_dev_func_sr_funcid_get(npi_handle_t handle, uint8_t *funcid_p)
+{
+	dev_func_sr_t		sr;
+
+	NXGE_REG_RD64(handle, DEV_FUNC_SR_REG, &sr.value);
+	*funcid_p = NXGE_VAL(DEV_FUNC_SR_FUNCID, sr.value);
+	if (!sr.bits.ldw.tas) {
+		/*
+		 * After read, this bit is set to 1 by hardware.
+		 * We own it if tas bit read as 0.
+		 */
+		sr.bits.ldw.tas = 0;
+		NXGE_REG_WR64(handle, DEV_FUNC_SR_REG, sr.value);
+	}
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_dev_func_sr_sr_get():
+ *	This function is called to get the shared register value.
+ *	(After read, the TAS bit is always set to 1. Software needs
+ *	to write 0 to clear if we own it.)
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	sr_p		- pointer to store the shared value of this register.
+ *
+ * Return:
+ *	NPI_SUCCESS		- If shared value get is complete successfully.
+ *
+ *	Error:
+ */
+npi_status_t
+npi_dev_func_sr_sr_raw_get(npi_handle_t handle, uint16_t *sr_p)
+{
+	dev_func_sr_t		sr;
+
+	NXGE_REG_RD64(handle, DEV_FUNC_SR_REG, &sr.value);
+	*sr_p = NXGE_VAL(DEV_FUNC_SR_FUNCID, sr.value);
+	if (!sr.bits.ldw.tas) {
+		/*
+		 * After read, this bit is set to 1 by hardware.
+		 * We own it if tas bit read as 0.
+		 */
+		sr.bits.ldw.tas = 0;
+		NXGE_REG_WR64(handle, DEV_FUNC_SR_REG, sr.value);
+	}
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_dev_func_sr_sr_get():
+ *	This function is called to get the shared register value.
+ *	(After read, the TAS bit is always set to 1. Software needs
+ *	to write 0 to clear if we own it.)
+ *
+ * Parameters:
+ *	handle	- NPI handle
+ *	sr_p	- pointer to store the shared value of this register.
+ *		. this will get only non-lock, non-function id portion
+ *              . of the register
+ *
+ *
+ * Return:
+ *	NPI_SUCCESS		- If shared value get is complete successfully.
+ *
+ *	Error:
+ */
+
+npi_status_t
+npi_dev_func_sr_sr_get(npi_handle_t handle, uint16_t *sr_p)
+{
+	dev_func_sr_t		sr;
+	uint16_t sr_impl = 0;
+
+	NXGE_REG_RD64(handle, DEV_FUNC_SR_REG, &sr.value);
+	sr_impl = NXGE_VAL(DEV_FUNC_SR_FUNCID, sr.value);
+	*sr_p =  (sr_impl << NPI_DEV_SR_IMPL_ST_SHIFT);
+	if (!sr.bits.ldw.tas) {
+		/*
+		 * After read, this bit is set to 1 by hardware.
+		 * We own it if tas bit read as 0.
+		 */
+		sr.bits.ldw.tas = 0;
+		NXGE_REG_WR64(handle, DEV_FUNC_SR_REG, sr.value);
+	}
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_dev_func_sr_sr_get_set_clear():
+ *	This function is called to set the shared register value.
+ *	(Shared register must be read first. If tas bit is 0, then
+ *	it implies that the software can proceed to set). After
+ *	setting, tas bit will be cleared.
+ * Parameters:
+ *	handle		- NPI handle
+ *	impl_sr		- shared value to set (only the 8 bit
+ *			  implementation specific state info).
+ *
+ * Return:
+ *	NPI_SUCCESS		- If shared value is set successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE
+ *		VIR_TAS_BUSY
+ */
+npi_status_t
+npi_dev_func_sr_sr_get_set_clear(npi_handle_t handle, uint16_t impl_sr)
+{
+	dev_func_sr_t		sr;
+	int			status;
+
+	status = npi_dev_func_sr_lock_enter(handle);
+	if (status != NPI_SUCCESS) {
+		NPI_DEBUG_MSG((handle.function, NPI_VIR_CTL,
+				    " npi_dev_func_sr_src_get_set_clear"
+				    " unable to acquire lock:"
+				    " status <0x%x>", status));
+		return (status);
+	}
+
+	NXGE_REG_RD64(handle, DEV_FUNC_SR_REG, &sr.value);
+	sr.bits.ldw.sr |= (impl_sr << NPI_DEV_SR_IMPL_ST_SHIFT);
+	NXGE_REG_WR64(handle, DEV_FUNC_SR_REG, sr.value);
+
+	return (npi_dev_func_sr_lock_free(handle));
+}
+
+/*
+ * npi_dev_func_sr_sr_set_only():
+ *	This function is called to only set the shared register value.
+ * Parameters:
+ *	handle		- NPI handle
+ *	impl_sr		- shared value to set.
+ *
+ * Return:
+ *	NPI_SUCCESS		- If shared value is set successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE
+ *		VIR_TAS_BUSY
+ */
+npi_status_t
+npi_dev_func_sr_sr_set_only(npi_handle_t handle, uint16_t impl_sr)
+{
+	int		status = NPI_SUCCESS;
+	dev_func_sr_t	sr;
+
+	NXGE_REG_RD64(handle, DEV_FUNC_SR_REG, &sr.value);
+	/* must be the owner */
+	if (sr.bits.ldw.funcid == NPI_GET_LOCK_OWNER(sr.bits.ldw.sr)) {
+		sr.bits.ldw.sr |= (impl_sr << NPI_DEV_SR_IMPL_ST_SHIFT);
+		NXGE_REG_WR64(handle, DEV_FUNC_SR_REG, sr.value);
+	} else {
+		NPI_DEBUG_MSG((handle.function, NPI_VIR_CTL,
+				    " npi_dev_func_sr_sr_set_only"
+				    " not owner <0x%x>",
+				    sr.bits.ldw.sr));
+		status = NPI_VIR_SR_NOTOWNER(sr.bits.ldw.funcid);
+	}
+
+	return (status);
+}
+
+/*
+ * npi_dev_func_sr_busy():
+ *	This function is called to see if we can own the device.
+ *	It will not reset the tas bit.
+ * Parameters:
+ *	handle		- NPI handle
+ *	busy_p		- pointer to store busy flag.
+ *				(B_TRUE: device is in use, B_FALSE: free).
+ * Return:
+ *	NPI_SUCCESS		- If tas bit is read successfully.
+ *	Error:
+ */
+npi_status_t
+npi_dev_func_sr_busy(npi_handle_t handle, boolean_t *busy_p)
+{
+	dev_func_sr_t	sr;
+
+	NXGE_REG_RD64(handle, DEV_FUNC_SR_REG, &sr.value);
+	if (!sr.bits.ldw.tas) {
+		sr.bits.ldw.tas = 0;
+		NXGE_REG_WR64(handle, DEV_FUNC_SR_REG, sr.value);
+		*busy_p = B_FALSE;
+	} else {
+		/* Other function already owns it */
+		*busy_p = B_TRUE;
+	}
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_dev_func_sr_tas_get():
+ *	This function is called to get the tas bit
+ *	(after read, this bit is always set to 1, software write 0
+ *	 to clear it).
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	tas_p		- pointer to store the tas value
+ *
+ * Return:
+ *	NPI_SUCCESS		- If tas value get is complete successfully.
+ *	Error:
+ */
+npi_status_t
+npi_dev_func_sr_tas_get(npi_handle_t handle, uint8_t *tas_p)
+{
+	dev_func_sr_t		sr;
+
+	NXGE_REG_RD64(handle, DEV_FUNC_SR_REG, &sr.value);
+	*tas_p = sr.bits.ldw.tas;
+	if (!sr.bits.ldw.tas) {
+		sr.bits.ldw.tas = 0;
+		NXGE_REG_WR64(handle, DEV_FUNC_SR_REG, sr.value);
+
+	}
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_fzc_mpc_set():
+ *	This function is called to enable the write access
+ *	to FZC region to function zero.
+ * Parameters:
+ *	handle		- NPI handle
+ * Return:
+ *	NPI_SUCCESS	-
+ *	Error:
+ */
+npi_status_t
+npi_fzc_mpc_set(npi_handle_t handle, boolean_t mpc)
+{
+	multi_part_ctl_t	mp;
+
+	mp.value = 0;
+	if (mpc) {
+		mp.bits.ldw.mpc = 1;
+	}
+	NXGE_REG_WR64(handle, MULTI_PART_CTL_REG, mp.value);
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_fzc_mpc_get():
+ *	This function is called to get the access mode.
+ * Parameters:
+ *	handle		- NPI handle
+ * Return:
+ *	NPI_SUCCESS	-
+ *
+ */
+npi_status_t
+npi_fzc_mpc_get(npi_handle_t handle, boolean_t *mpc_p)
+{
+	multi_part_ctl_t	mpc;
+
+	mpc.value = 0;
+	NXGE_REG_RD64(handle, MULTI_PART_CTL_REG, &mpc.value);
+	*mpc_p = mpc.bits.ldw.mpc;
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_fzc_dma_bind_set():
+ *	This function is called to set DMA binding register.
+ * Parameters:
+ *	handle		- NPI handle
+ *	dma_bind	- NPI defined data structure that
+ *			  contains the tx/rx channel binding info.
+ *			  to set.
+ * Return:
+ *	NPI_SUCCESS	-
+ *	Error:
+ *	NPI_FAILURE
+ *
+ */
+npi_status_t
+npi_fzc_dma_bind_set(npi_handle_t handle, fzc_dma_bind_t dma_bind)
+{
+	dma_bind_t	bind;
+	int		status;
+	uint8_t		fn, region, id, tn, rn;
+
+	fn = dma_bind.function_id;
+	region = dma_bind.sub_vir_region;
+	id = dma_bind.vir_index;
+	tn = dma_bind.tx_channel;
+	rn = dma_bind.rx_channel;
+
+	DMA_BIND_VADDR_VALIDATE(fn, region, id, status);
+	if (status) {
+		return (status);
+	}
+
+	if (dma_bind.tx_bind) {
+		DMA_BIND_TX_VALIDATE(tn, status);
+		if (status) {
+			return (status);
+		}
+	}
+
+	if (dma_bind.rx_bind) {
+		DMA_BIND_RX_VALIDATE(rn, status);
+		if (status) {
+			return (status);
+		}
+	}
+
+	bind.value = 0;
+	if (dma_bind.tx_bind) {
+		bind.bits.ldw.tx_bind = 1;
+		bind.bits.ldw.tx = tn;
+	}
+	if (dma_bind.rx_bind) {
+		bind.bits.ldw.rx_bind = 1;
+		bind.bits.ldw.rx = rn;
+	}
+
+	NXGE_REG_WR64(handle, DMA_BIND_REG +
+		DMA_BIND_REG_OFFSET(fn, rn, id), bind.value);
+
+	return (status);
+}
+
+/*
+ * npi_fzc_ldg_num_set():
+ *	This function is called to set up a logical group number that
+ *	a logical device belongs to.
+ * Parameters:
+ *	handle		- NPI handle
+ *	ld		- logical device number (0 - 68)
+ *	ldg		- logical device group number (0 - 63)
+ * Return:
+ *	NPI_SUCCESS	-
+ *	Error:
+ *	NPI_FAILURE
+ *
+ */
+npi_status_t
+npi_fzc_ldg_num_set(npi_handle_t handle, uint8_t ld, uint8_t ldg)
+{
+	ldg_num_t	gnum;
+
+	if (!LD_VALID(ld)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_fzc_ldg_num_set"
+				    "ld <0x%x>", ld));
+		return (NPI_FAILURE | NPI_VIR_LD_INVALID(ld));
+	}
+	if (!LDG_VALID(ldg)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_fzc_ldg_num_set"
+				    " ldg <0x%x>", ldg));
+		return (NPI_FAILURE | NPI_VIR_LDG_INVALID(ld));
+	}
+
+	gnum.value = 0;
+	gnum.bits.ldw.num = ldg;
+
+	NXGE_REG_WR64(handle, LDG_NUM_REG + LD_NUM_OFFSET(ld),
+		gnum.value);
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_fzc_ldg_num_get():
+ *	This function is called to get the logical device group that
+ *	a logical device belongs to.
+ * Parameters:
+ *	handle		- NPI handle
+ *	ld		- logical device number (0 - 68)
+ *	*ldg_p		- pointer to store its group number.
+ * Return:
+ *	NPI_SUCCESS	-
+ *	Error:
+ *	NPI_FAILURE
+ */
+npi_status_t
+npi_fzc_ldg_num_get(npi_handle_t handle, uint8_t ld, uint8_t *ldg_p)
+{
+	uint64_t val;
+
+	if (!LD_VALID(ld)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_fzc_ldg_num_get"
+				    " Invalid Input:",
+				    " ld <0x%x>", ld));
+		return (NPI_FAILURE | NPI_VIR_LD_INVALID(ld));
+	}
+
+	NXGE_REG_RD64(handle, LDG_NUM_REG + LD_NUM_OFFSET(ld), &val);
+
+	*ldg_p = (uint8_t)(val & LDG_NUM_NUM_MASK);
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_ldsv_ldfs_get():
+ *	This function is called to get device state vectors.
+ * Parameters:
+ *	handle		- NPI handle
+ *	ldg		- logical device group (0 - 63)
+ *	*ldf_p		- pointer to store ldf0 and ldf1 flag bits.
+ * Return:
+ *	NPI_SUCCESS	-
+ *	Error:
+ *	NPI_FAILURE
+ */
+npi_status_t
+npi_ldsv_ldfs_get(npi_handle_t handle, uint8_t ldg, uint64_t *vector0_p,
+		uint64_t *vector1_p, uint64_t *vector2_p)
+{
+	int	status;
+
+	if ((status = npi_ldsv_get(handle, ldg, VECTOR0, vector0_p))) {
+		return (status);
+	}
+	if ((status = npi_ldsv_get(handle, ldg, VECTOR1, vector1_p))) {
+		return (status);
+	}
+	if ((status = npi_ldsv_get(handle, ldg, VECTOR2, vector2_p))) {
+		return (status);
+	}
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_ldsv_get():
+ *	This function is called to get device state vectors.
+ * Parameters:
+ *	handle		- NPI handle
+ *	ldg		- logical device group (0 - 63)
+ *	ldf_type	- either LDF0 (0) or LDF1 (1)
+ *	vector		- vector type (0, 1 or 2)
+ *	*ldf_p		- pointer to store its flag bits.
+ * Return:
+ *	NPI_SUCCESS	-
+ *	Error:
+ *	NPI_FAILURE
+ */
+npi_status_t
+npi_ldsv_get(npi_handle_t handle, uint8_t ldg, ldsv_type_t vector,
+		uint64_t *ldf_p)
+{
+	uint64_t		offset;
+
+	if (!LDG_VALID(ldg)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_ldsv_get"
+				    " Invalid Input "
+				    " ldg <0x%x>", ldg));
+		return (NPI_FAILURE | NPI_VIR_LDG_INVALID(ldg));
+	}
+
+	switch (vector) {
+	case VECTOR0:
+		offset = LDSV0_REG + LDSV_OFFSET(ldg);
+		break;
+
+	case VECTOR1:
+		offset = LDSV1_REG + LDSV_OFFSET(ldg);
+		break;
+
+	case VECTOR2:
+		offset = LDSV2_REG + LDSV_OFFSET(ldg);
+		break;
+
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_ldsv_get"
+				    " Invalid Input: "
+				    " ldsv type <0x%x>", vector));
+		return (NPI_FAILURE | NPI_VIR_LDSV_INVALID(vector));
+	}
+
+	NXGE_REG_RD64(handle, offset, ldf_p);
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_ldsv_ld_get():
+ *	This function is called to get the flag bit value of a device.
+ * Parameters:
+ *	handle		- NPI handle
+ *	ldg		- logical device group (0 - 63)
+ *	ld		- logical device (0 - 68)
+ *	ldf_type	- either LDF0 (0) or LDF1 (1)
+ *	vector		- vector type (0, 1 or 2)
+ *	*ldf_p		- pointer to store its flag bits.
+ * Return:
+ *	NPI_SUCCESS	-
+ *	Error:
+ *	NPI_FAILURE
+ */
+npi_status_t
+npi_ldsv_ld_get(npi_handle_t handle, uint8_t ldg, uint8_t ld,
+		ldsv_type_t vector, ldf_type_t ldf_type, boolean_t *flag_p)
+{
+	uint64_t		sv;
+	uint64_t		offset;
+
+	if (!LDG_VALID(ldg)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_ldsv_ld_get"
+				    " Invalid Input: "
+				    " ldg <0x%x>", ldg));
+		return (NPI_FAILURE | NPI_VIR_LDG_INVALID(ldg));
+	}
+	if (!LD_VALID(ld)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_ldsv_ld_get Invalid Input: "
+				    " ld <9x%x>", ld));
+		return (NPI_FAILURE | NPI_VIR_LD_INVALID(ld));
+	} else if (vector == VECTOR2 && ld < NXGE_MAC_LD_START) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_ldsv_ld_get Invalid Input:"
+				    " ld-vector2 <0x%x>", ld));
+		return (NPI_FAILURE | NPI_VIR_LD_INVALID(ld));
+	}
+
+	switch (vector) {
+	case VECTOR0:
+		offset = LDSV0_REG + LDSV_OFFSET(ldg);
+		break;
+
+	case VECTOR1:
+		offset = LDSV1_REG + LDSV_OFFSET(ldg);
+		break;
+
+	case VECTOR2:
+		offset = LDSV2_REG + LDSV_OFFSET(ldg);
+
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL, "npi_ldsv_get"
+			"ldsv", vector));
+		return (NPI_FAILURE | NPI_VIR_LDSV_INVALID(vector));
+	}
+
+	NXGE_REG_RD64(handle, offset, &sv);
+	if (vector != VECTOR2) {
+		*flag_p = ((sv >> ld) & LDSV_MASK_ALL);
+	} else {
+		if (ldf_type) {
+			*flag_p = (((sv >> LDSV2_LDF1_SHIFT) >>
+				(ld - NXGE_MAC_LD_START)) & LDSV_MASK_ALL);
+		} else {
+			*flag_p = (((sv >> LDSV2_LDF0_SHIFT) >>
+				(ld - NXGE_MAC_LD_START)) & LDSV_MASK_ALL);
+		}
+	}
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_ldsv_ld_ldf0_get():
+ *	This function is called to get the ldf0 bit value of a device.
+ * Parameters:
+ *	handle		- NPI handle
+ *	ldg		- logical device group (0 - 63)
+ *	ld		- logical device (0 - 68)
+ *	*ldf_p		- pointer to store its flag bits.
+ * Return:
+ *	NPI_SUCCESS	-
+ *	Error:
+ *	NPI_FAILURE
+ */
+npi_status_t
+npi_ldsv_ld_ldf0_get(npi_handle_t handle, uint8_t ldg, uint8_t ld,
+		boolean_t *flag_p)
+{
+	ldsv_type_t vector = 0;
+
+	if (ld >= NXGE_MAC_LD_START) {
+		vector = VECTOR2;
+	}
+
+	return (npi_ldsv_ld_get(handle, ldg, ld, vector, LDF0, flag_p));
+}
+
+/*
+ * npi_ldsv_ld_ldf1_get():
+ *	This function is called to get the ldf1 bit value of a device.
+ * Parameters:
+ *	handle		- NPI handle
+ *	ldg		- logical device group (0 - 63)
+ *	ld		- logical device (0 - 68)
+ *	*ldf_p		- pointer to store its flag bits.
+ * Return:
+ *	NPI_SUCCESS	-
+ *	Error:
+ *	NPI_FAILURE
+ */
+npi_status_t
+npi_ldsv_ld_ldf1_get(npi_handle_t handle, uint8_t ldg, uint8_t ld,
+		boolean_t *flag_p)
+{
+	ldsv_type_t vector = 0;
+
+	if (ld >= NXGE_MAC_LD_START) {
+		vector = VECTOR2;
+	}
+
+	return (npi_ldsv_ld_get(handle, ldg, ld, vector, LDF1, flag_p));
+}
+
+/*
+ * npi_intr_mask_set():
+ *	This function is called to select the mask bits for both ldf0 and ldf1.
+ * Parameters:
+ *	handle		- NPI handle
+ *	ld		- logical device (0 - 68)
+ *	ldf_mask	- mask value to set (both ldf0 and ldf1).
+ * Return:
+ *	NPI_SUCCESS	-
+ *	Error:
+ *	NPI_FAILURE
+ */
+npi_status_t
+npi_intr_mask_set(npi_handle_t handle, uint8_t ld, uint8_t ldf_mask)
+{
+	uint64_t		offset;
+
+	if (!LD_VALID(ld)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			    " npi_intr_mask_set ld", ld));
+		return (NPI_FAILURE | NPI_VIR_LD_INVALID(ld));
+	}
+
+	ldf_mask &= LD_IM0_MASK;
+	offset = LDSV_OFFSET_MASK(ld);
+
+	NPI_DEBUG_MSG((handle.function, NPI_VIR_CTL,
+		"npi_intr_mask_set: ld %d "
+		" offset 0x%0llx "
+		" mask 0x%x",
+		ld, offset, ldf_mask));
+
+	NXGE_REG_WR64(handle, offset, (uint64_t)ldf_mask);
+
+	return (NPI_SUCCESS);
+
+}
+
+/*
+ * npi_intr_mask_get():
+ *	This function is called to get the mask bits.
+ * Parameters:
+ *	handle		- NPI handle
+ *	ld		- logical device (0 - 68)
+ *	ldf_mask	- pointer to store mask bits info.
+ * Return:
+ *	NPI_SUCCESS	-
+ *	Error:
+ *	NPI_FAILURE
+ */
+npi_status_t
+npi_intr_mask_get(npi_handle_t handle, uint8_t ld, uint8_t *ldf_mask_p)
+{
+	uint64_t		offset;
+	uint64_t		val;
+
+	if (!LD_VALID(ld)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+			    " npi_intr_mask_get ld", ld));
+		return (NPI_FAILURE | NPI_VIR_LD_INVALID(ld));
+	}
+
+	offset = LDSV_OFFSET_MASK(ld);
+
+	NXGE_REG_RD64(handle, offset, &val);
+
+	*ldf_mask_p = (uint8_t)(val & LD_IM_MASK);
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_intr_ldg_mgmt_set():
+ *	This function is called to set interrupt timer and arm bit.
+ * Parameters:
+ *	handle		- NPI handle
+ *	ldg		- logical device group (0 - 63)
+ *	arm		- B_TRUE (arm) B_FALSE (disable)
+ * Return:
+ *	NPI_SUCCESS	-
+ *	Error:
+ *	NPI_FAILURE
+ */
+npi_status_t
+npi_intr_ldg_mgmt_set(npi_handle_t handle, uint8_t ldg, boolean_t arm,
+			uint8_t timer)
+{
+	ldgimgm_t		mgm;
+	uint64_t		val;
+
+	if (!LDG_VALID(ldg)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_intr_ldg_mgmt_set"
+				    " Invalid Input: "
+				    " ldg <0x%x>", ldg));
+		return (NPI_FAILURE | NPI_VIR_LDG_INVALID(ldg));
+	}
+	if (!LD_INTTIMER_VALID(timer)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_intr_ldg_mgmt_set Invalid Input"
+				    " timer <0x%x>", timer));
+		return (NPI_FAILURE | NPI_VIR_INTM_TM_INVALID(ldg));
+	}
+
+	if (arm) {
+		mgm.bits.ldw.arm = 1;
+	} else {
+		NXGE_REG_RD64(handle, LDGIMGN_REG + LDSV_OFFSET(ldg), &val);
+		mgm.value = val & LDGIMGM_ARM_MASK;
+	}
+
+	mgm.bits.ldw.timer = timer;
+	NXGE_REG_WR64(handle, LDGIMGN_REG + LDSV_OFFSET(ldg),
+		mgm.value);
+
+	NPI_DEBUG_MSG((handle.function, NPI_VIR_CTL,
+		" npi_intr_ldg_mgmt_set: ldg %d"
+		" reg offset 0x%x",
+		ldg, LDGIMGN_REG + LDSV_OFFSET(ldg)));
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_intr_ldg_mgmt_timer_get():
+ *	This function is called to get the timer counter
+ * Parameters:
+ *	handle		- NPI handle
+ *	ldg		- logical device group (0 - 63)
+ *	timer_p		- pointer to store the timer counter.
+ * Return:
+ *	NPI_SUCCESS	-
+ *	Error:
+ *	NPI_FAILURE
+ */
+npi_status_t
+npi_intr_ldg_mgmt_timer_get(npi_handle_t handle, uint8_t ldg, uint8_t *timer_p)
+{
+	uint64_t val;
+
+	if (!LDG_VALID(ldg)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_intr_ldg_mgmt_timer_get"
+				    " Invalid Input: ldg <0x%x>", ldg));
+		return (NPI_FAILURE | NPI_VIR_LDG_INVALID(ldg));
+	}
+
+	NXGE_REG_RD64(handle, LDGIMGN_REG + LDSV_OFFSET(ldg), &val);
+
+	*timer_p = (uint8_t)(val & LDGIMGM_TIMER_MASK);
+
+	NPI_DEBUG_MSG((handle.function, NPI_VIR_CTL,
+		" npi_intr_ldg_mgmt_timer_get: ldg %d"
+		" reg offset 0x%x",
+		ldg, LDGIMGN_REG + LDSV_OFFSET(ldg)));
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_intr_ldg_mgmt_arm():
+ *	This function is called to arm the group.
+ * Parameters:
+ *	handle		- NPI handle
+ *	ldg		- logical device group (0 - 63)
+ * Return:
+ *	NPI_SUCCESS	-
+ *	Error:
+ *	NPI_FAILURE
+ */
+npi_status_t
+npi_intr_ldg_mgmt_arm(npi_handle_t handle, uint8_t ldg)
+{
+	ldgimgm_t		mgm;
+
+	if (!LDG_VALID(ldg)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_intr_ldg_mgmt_arm"
+				    " Invalid Input: ldg <0x%x>",
+				    ldg));
+		return (NPI_FAILURE | NPI_VIR_LDG_INVALID(ldg));
+	}
+
+	NXGE_REG_RD64(handle, (LDGIMGN_REG + LDSV_OFFSET(ldg)), &mgm.value);
+	mgm.bits.ldw.arm = 1;
+
+	NXGE_REG_WR64(handle, LDGIMGN_REG + LDSV_OFFSET(ldg),
+			mgm.value);
+	NPI_DEBUG_MSG((handle.function, NPI_VIR_CTL,
+		" npi_intr_ldg_mgmt_arm: ldg %d"
+		" reg offset 0x%x",
+		ldg, LDGIMGN_REG + LDSV_OFFSET(ldg)));
+
+	return (NPI_SUCCESS);
+}
+
+
+/*
+ * npi_fzc_ldg_timer_res_set():
+ *	This function is called to set the timer resolution.
+ * Parameters:
+ *	handle		- NPI handle
+ *	res		- timer resolution (# of system clocks)
+ * Return:
+ *	NPI_SUCCESS	-
+ *	Error:
+ *	NPI_FAILURE
+ */
+npi_status_t
+npi_fzc_ldg_timer_res_set(npi_handle_t handle, uint32_t res)
+{
+	if (res > LDGTITMRES_RES_MASK) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_fzc_ldg_timer_res_set"
+				    " Invalid Input: res <0x%x>",
+				    res));
+		return (NPI_FAILURE | NPI_VIR_TM_RES_INVALID);
+	}
+
+	NXGE_REG_WR64(handle, LDGITMRES_REG, (res & LDGTITMRES_RES_MASK));
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_fzc_ldg_timer_res_get():
+ *	This function is called to get the timer resolution.
+ * Parameters:
+ *	handle		- NPI handle
+ *	res_p		- pointer to store the timer resolution.
+ * Return:
+ *	NPI_SUCCESS	-
+ *	Error:
+ *	NPI_FAILURE
+ */
+npi_status_t
+npi_fzc_ldg_timer_res_get(npi_handle_t handle, uint8_t *res_p)
+{
+	uint64_t val;
+
+	NXGE_REG_RD64(handle, LDGITMRES_REG, &val);
+
+	*res_p = (uint8_t)(val & LDGIMGM_TIMER_MASK);
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_fzc_sid_set():
+ *	This function is called to set the system interrupt data.
+ * Parameters:
+ *	handle		- NPI handle
+ *	ldg		- logical group (0 - 63)
+ *	sid		- NPI defined data to set
+ * Return:
+ *	NPI_SUCCESS	-
+ *	Error:
+ *	NPI_FAILURE
+ */
+npi_status_t
+npi_fzc_sid_set(npi_handle_t handle, fzc_sid_t sid)
+{
+	sid_t		sd;
+
+	if (!LDG_VALID(sid.ldg)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_fzc_sid_set"
+				    " Invalid Input: ldg <0x%x>",
+				    sid.ldg));
+		return (NPI_FAILURE | NPI_VIR_LDG_INVALID(sid.ldg));
+	}
+	if (!sid.niu) {
+		if (!FUNC_VALID(sid.func)) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_fzc_sid_set"
+					    " Invalid Input: func <0x%x>",
+					    sid.func));
+			return (NPI_FAILURE | NPI_VIR_FUNC_INVALID(sid.func));
+		}
+
+		if (!SID_VECTOR_VALID(sid.vector)) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_fzc_sid_set"
+					    " Invalid Input: vector <0x%x>",
+					    sid.vector));
+			return (NPI_FAILURE |
+				NPI_VIR_SID_VEC_INVALID(sid.vector));
+		}
+	}
+	sd.value = 0;
+	if (!sid.niu) {
+		sd.bits.ldw.data = ((sid.func << SID_DATA_FUNCNUM_SHIFT) |
+				(sid.vector & SID_DATA_INTNUM_MASK));
+	}
+
+	NXGE_REG_WR64(handle,  SID_REG + LDG_SID_OFFSET(sid.ldg), sd.value);
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_fzc_sid_get():
+ *	This function is called to get the system interrupt data.
+ * Parameters:
+ *	handle		- NPI handle
+ *	ldg		- logical group (0 - 63)
+ *	sid_p		- NPI defined data to get
+ * Return:
+ *	NPI_SUCCESS	-
+ *	Error:
+ *	NPI_FAILURE
+ */
+npi_status_t
+npi_fzc_sid_get(npi_handle_t handle, p_fzc_sid_t sid_p)
+{
+	sid_t		sd;
+
+	if (!LDG_VALID(sid_p->ldg)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_fzc_sid_get"
+				    " Invalid Input: ldg <0x%x>",
+				    sid_p->ldg));
+		return (NPI_FAILURE | NPI_VIR_LDG_INVALID(sid_p->ldg));
+	}
+	NXGE_REG_RD64(handle, (SID_REG + LDG_SID_OFFSET(sid_p->ldg)),
+		&sd.value);
+	if (!sid_p->niu) {
+		sid_p->func = ((sd.bits.ldw.data & SID_DATA_FUNCNUM_MASK) >>
+			SID_DATA_FUNCNUM_SHIFT);
+		sid_p->vector = ((sd.bits.ldw.data & SID_DATA_INTNUM_MASK) >>
+			SID_DATA_INTNUM_SHIFT);
+	} else {
+		sid_p->vector = (sd.value & SID_DATA_MASK);
+	}
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_fzc_sys_err_mask_set():
+ *	This function is called to mask/unmask the device error mask bits.
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	mask		- set bit mapped mask
+ * Return:
+ *	NPI_SUCCESS	-
+ *	Error:
+ *	NPI_FAILURE
+ */
+npi_status_t
+npi_fzc_sys_err_mask_set(npi_handle_t handle, uint64_t mask)
+{
+	NXGE_REG_WR64(handle,  SYS_ERR_MASK_REG, mask);
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_fzc_sys_err_stat_get():
+ *	This function is called to get the system error stats.
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	err_stat	- sys_err_stat structure to hold stats.
+ * Return:
+ *	NPI_SUCCESS	-
+ *	Error:
+ *	NPI_FAILURE
+ */
+npi_status_t
+npi_fzc_sys_err_stat_get(npi_handle_t handle, p_sys_err_stat_t statp)
+{
+	NXGE_REG_RD64(handle,  SYS_ERR_STAT_REG, &statp->value);
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_fzc_rst_ctl_get(npi_handle_t handle, p_rst_ctl_t rstp)
+{
+	NXGE_REG_RD64(handle, RST_CTL_REG, &rstp->value);
+
+	return (NPI_SUCCESS);
+}
+
+/*
+ * npi_fzc_mpc_get():
+ *	This function is called to get the access mode.
+ * Parameters:
+ *	handle		- NPI handle
+ * Return:
+ *	NPI_SUCCESS	-
+ *
+ */
+npi_status_t
+npi_fzc_rst_ctl_reset_mac(npi_handle_t handle, uint8_t port)
+{
+	rst_ctl_t 		rst;
+
+	rst.value = 0;
+	NXGE_REG_RD64(handle, RST_CTL_REG, &rst.value);
+	rst.value |= (1 << (RST_CTL_MAC_RST0_SHIFT + port));
+	NXGE_REG_WR64(handle, RST_CTL_REG, rst.value);
+
+	return (NPI_SUCCESS);
+}
diff --git a/drivers/net/nxge/npi/npi_vir.h b/drivers/net/nxge/npi/npi_vir.h
new file mode 100644
index 0000000..1982185
--- /dev/null
+++ b/drivers/net/nxge/npi/npi_vir.h
@@ -0,0 +1,702 @@
+/*
+ * npi_vir.c	Neptune  HW virtualization HW API functions
+ *
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#ifndef _NPI_VIR_H
+#define	_NPI_VIR_H
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#include <npi.h>
+#include <nxge_hw.h>
+
+/*
+ * Virtualization and Logical devices NPI error codes
+ */
+#define	FUNCID_INVALID		PORT_INVALID
+#define	VIR_ERR_ST		(VIR_BLK_ID << NPI_BLOCK_ID_SHIFT)
+#define	VIR_ID_SHIFT(n)		(n << NPI_PORT_CHAN_SHIFT)
+
+#define	VIR_HW_BUSY		(NPI_BK_HW_ERROR_START | 0x1)
+
+#define	VIR_TAS_BUSY		(NPI_BK_ERROR_START | 0x1)
+#define	VIR_TAS_NOTREAD	(NPI_BK_ERROR_START | 0x2)
+
+#define	VIR_SR_RESET		(NPI_BK_ERROR_START | 0x3)
+#define	VIR_SR_FREE		(NPI_BK_ERROR_START | 0x4)
+#define	VIR_SR_BUSY		(NPI_BK_ERROR_START | 0x5)
+#define	VIR_SR_INVALID		(NPI_BK_ERROR_START | 0x6)
+#define	VIR_SR_NOTOWNER	(NPI_BK_ERROR_START | 0x7)
+#define	VIR_SR_INITIALIZED	(NPI_BK_ERROR_START | 0x8)
+
+#define	VIR_MPC_DENY		(NPI_BK_ERROR_START | 0x10)
+
+#define	VIR_BD_FUNC_INVALID	(NPI_BK_ERROR_START | 0x20)
+#define	VIR_BD_REG_INVALID	(NPI_BK_ERROR_START | 0x21)
+#define	VIR_BD_ID_INVALID	(NPI_BK_ERROR_START | 0x22)
+#define	VIR_BD_TXDMA_INVALID	(NPI_BK_ERROR_START | 0x23)
+#define	VIR_BD_RXDMA_INVALID	(NPI_BK_ERROR_START | 0x24)
+
+#define	VIR_LD_INVALID		(NPI_BK_ERROR_START | 0x30)
+#define	VIR_LDG_INVALID		(NPI_BK_ERROR_START | 0x31)
+#define	VIR_LDSV_INVALID	(NPI_BK_ERROR_START | 0x32)
+
+#define	VIR_INTM_TM_INVALID	(NPI_BK_ERROR_START | 0x33)
+#define	VIR_TM_RES_INVALID	(NPI_BK_ERROR_START | 0x34)
+#define	VIR_SID_VEC_INVALID	(NPI_BK_ERROR_START | 0x35)
+
+#define	NPI_VIR_OCODE_INVALID(n) (VIR_ID_SHIFT(n) | VIR_ERR_ST | OPCODE_INVALID)
+#define	NPI_VIR_FUNC_INVALID(n)	 (VIR_ID_SHIFT(n) | VIR_ERR_ST | FUNCID_INVALID)
+#define	NPI_VIR_CN_INVALID(n)	(VIR_ID_SHIFT(n) | VIR_ERR_ST | CHANNEL_INVALID)
+
+/*
+ * Errors codes of shared register functions.
+ */
+#define	NPI_VIR_TAS_BUSY(n)	(VIR_ID_SHIFT(n) | VIR_ERR_ST | VIR_TAS_BUSY)
+#define	NPI_VIR_TAS_NOTREAD(n)	(VIR_ID_SHIFT(n) | VIR_ERR_ST | VIR_TAS_NOTREAD)
+#define	NPI_VIR_SR_RESET(n)	(VIR_ID_SHIFT(n) | VIR_ERR_ST | VIR_SR_RESET)
+#define	NPI_VIR_SR_FREE(n)	(VIR_ID_SHIFT(n) | VIR_ERR_ST | VIR_SR_FREE)
+#define	NPI_VIR_SR_BUSY(n)	(VIR_ID_SHIFT(n) | VIR_ERR_ST | VIR_SR_BUSY)
+#define	NPI_VIR_SR_INVALID(n)	(VIR_ID_SHIFT(n) | VIR_ERR_ST | VIR_SR_INVALID)
+#define	NPI_VIR_SR_NOTOWNER(n)	(VIR_ID_SHIFT(n) | VIR_ERR_ST | VIR_SR_NOTOWNER)
+#define	NPI_VIR_SR_INITIALIZED(n) (VIR_ID_SHIFT(n) | \
+					VIR_ERR_ST | VIR_SR_INITIALIZED)
+
+/*
+ * Error codes of muti-partition control register functions.
+ */
+#define	NPI_VIR_MPC_DENY	(VIR_ERR_ST | VIR_MPU_DENY)
+
+/*
+ * Error codes of DMA binding functions.
+ */
+#define	NPI_VIR_BD_FUNC_INVALID(n)	(VIR_ID_SHIFT(n) | \
+					VIR_ERR_ST | VIR_BD_FUNC_INVALID)
+#define	NPI_VIR_BD_REG_INVALID(n)	(VIR_ID_SHIFT(n) | \
+					VIR_ERR_ST | VIR_BD_REG_INVALID)
+#define	NPI_VIR_BD_ID_INVALID(n)	(VIR_ID_SHIFT(n) | \
+					VIR_ERR_ST | VIR_BD_ID_INVALID)
+#define	NPI_VIR_BD_TXDMA_INVALID(n)	(VIR_ID_SHIFT(n) | \
+					VIR_ERR_ST | VIR_BD_TXDMA_INVALID)
+#define	NPI_VIR_BD_RXDMA_INVALID(n)	(VIR_ID_SHIFT(n) | \
+					VIR_ERR_ST | VIR_BD_RXDMA_INVALID)
+
+/*
+ * Error codes of logical devices and groups functions.
+ */
+#define	NPI_VIR_LD_INVALID(n) 	(VIR_ID_SHIFT(n) | VIR_ERR_ST | VIR_LD_INVALID)
+#define	NPI_VIR_LDG_INVALID(n)	(VIR_ID_SHIFT(n) | VIR_ERR_ST | VIR_LDG_INVALID)
+#define	NPI_VIR_LDSV_INVALID(n) (VIR_ID_SHIFT(n) | \
+					VIR_ERR_ST | VIR_LDSV_INVALID)
+#define	NPI_VIR_INTM_TM_INVALID(n)	(VIR_ID_SHIFT(n) | \
+					VIR_ERR_ST | VIR_INTM_TM_INVALID)
+#define	NPI_VIR_TM_RES_INVALID		(VIR_ERR_ST | VIR_TM_RES_INVALID)
+#define	NPI_VIR_SID_VEC_INVALID(n)	(VIR_ID_SHIFT(n) | \
+						VIR_ERR_ST | VIR_TM_RES_INVALID)
+
+/*
+ * Bit definition ([15:0] of the shared register
+ * used by the driver as locking mechanism.
+ *	[1:0]		lock state (RESET, FREE, BUSY)
+ *	[3:2]		function ID (owner)
+ *	[11:4]		Implementation specific states
+ *	[15:12]  	Individual function state
+ */
+#define	NPI_DEV_SR_LOCK_ST_RESET	0
+#define	NPI_DEV_SR_LOCK_ST_FREE		1
+#define	NPI_DEV_SR_LOCK_ST_BUSY		2
+
+#define	NPI_DEV_SR_LOCK_ST_SHIFT	0
+#define	NPI_DEV_SR_LOCK_ST_MASK		0x03
+#define	NPI_DEV_SR_LOCK_FID_SHIFT	2
+#define	NPI_DEV_SR_LOCK_FID_MASK	0x0C
+
+#define	NPI_DEV_SR_IMPL_ST_SHIFT	4
+#define	NPI_DEV_SR_IMPL_ST_MASK	0xfff0
+
+#define	NPI_GET_LOCK_OWNER(sr)		((sr & NPI_DEV_SR_LOCK_FID_MASK) \
+						>> NPI_DEV_SR_LOCK_FID_SHIFT)
+#define	NPI_GET_LOCK_ST(sr)		(sr & NPI_DEV_SR_LOCK_ST_MASK)
+#define	NPI_GET_LOCK_IMPL_ST(sr)	((sr & NPI_DEV_SR_IMPL_ST_MASK) \
+						>> NPI_DEV_SR_IMPL_ST_SHIFT)
+
+/*
+ * DMA channel binding definitions.
+ */
+#define	DMA_BIND_VADDR_VALIDATE(fn, rn, id, status)			\
+{									\
+	status = NPI_SUCCESS;						\
+	if (!TXDMA_FUNC_VALID(fn)) {					\
+		status = (NPI_FAILURE | NPI_VIR_BD_FUNC_INVALID(fn));	\
+	} else if (!SUBREGION_VALID(rn)) {				\
+		status = (NPI_FAILURE | NPI_VIR_BD_REG_INVALID(rn));	\
+	} else if (!VIR_PAGE_INDEX_VALID(id)) {				\
+		status = (NPI_FAILURE | NPI_VIR_BD_ID_INVALID(id));	\
+	}								\
+}
+
+#define	DMA_BIND_TX_VALIDATE(n, status)					\
+{									\
+	status = NPI_SUCCESS;						\
+	if (!TXDMA_CHANNEL_VALID(n)) {					\
+		status = (NPI_FAILURE | NPI_VIR_BD_TXDMA_INVALID(n));	\
+	}								\
+}
+
+#define	DMA_BIND_RX_VALIDATE(n, status)					\
+{									\
+	status = NPI_SUCCESS;						\
+	if (!VRXDMA_CHANNEL_VALID(n)) {					\
+		status = (NPI_FAILURE | NPI_VIR_BD_RXDMA_INVALID(n));	\
+	}								\
+}
+
+#define	DMA_BIND_STEP			8
+#define	DMA_BIND_REG_OFFSET(fn, rn, id)	(DMA_BIND_STEP * \
+					(fn * 2 * VIR_PAGE_INDEX_MAX + \
+					rn * VIR_PAGE_INDEX_MAX) + id)
+
+/*
+ * NPI defined data structure to program the DMA binding register.
+ */
+typedef struct _fzc_dma_bind {
+	uint8_t		function_id;	/* 0 to 3 */
+	uint8_t		sub_vir_region;	/* 0 or 1 */
+	uint8_t		vir_index;	/* 0 to 7 */
+	boolean_t	tx_bind;	/* set 1 to bind */
+	uint8_t		tx_channel;	/* hardware channel number (0 - 23) */
+	boolean_t	rx_bind;	/* set 1 to bind */
+	uint8_t		rx_channel;	/* hardware channel number (0 - 15) */
+} fzc_dma_bind_t, *p_fzc_dma_bind;
+
+/*
+ * Logical device definitions.
+ */
+#define	LD_NUM_STEP		8
+#define	LD_NUM_OFFSET(ld)	(ld * LDG_NUM_STEP)
+#define	LDG_NUM_STEP		8
+#define	LDG_NUM_OFFSET(ldg)	(ldg * LDG_NUM_STEP)
+#define	LDGNUM_OFFSET(ldg)	(ldg * LDG_NUM_STEP)
+#define	LDSV_STEP		8192
+#define	LDSVG_OFFSET(ldg)	(ldg * LDSV_STEP)
+#define	LDSV_OFFSET(ldv)	(ldv * LDSV_STEP)
+
+#define	LDSV_OFFSET_MASK(ld)			\
+	(((ld < NXGE_MAC_LD_START) ?		\
+	(LD_IM0_REG + LDSV_OFFSET(ld)) :	\
+	(LD_IM1_REG + LDSV_OFFSET((ld - NXGE_MAC_LD_START))))); \
+
+#define	LDG_SID_STEP		8
+#define	LDG_SID_OFFSET(ldg)	(ldg * LDG_SID_STEP)
+
+typedef enum {
+	LDF0,
+	LDF1
+} ldf_type_t;
+
+typedef enum {
+	VECTOR0,
+	VECTOR1,
+	VECTOR2
+} ldsv_type_t;
+
+/*
+ * Definitions for the system interrupt data.
+ */
+typedef struct _fzc_sid {
+	boolean_t	niu;
+	uint8_t		ldg;
+	uint8_t		func;
+	uint8_t		vector;
+} fzc_sid_t, *p_fzc_sid_t;
+
+/*
+ * Virtualization and Interrupt Prototypes.
+ */
+/*
+ * npi_dev_func_sr_init():
+ *	This function is called to initialize the device function
+ *	shared register (set the software implementation lock
+ *	state to FREE).
+ * Parameters:
+ *	handle		- NPI handle
+ * Return:
+ *	NPI_SUCCESS	- If initialization is complete successfully.
+ *			  (set sr bits to free).
+ *	Error:
+ *	NPI_FAILURE
+ *		VIR_TAS_BUSY
+ */
+npi_status_t npi_dev_func_sr_init(npi_handle_t);
+
+/*
+ * npi_dev_func_sr_lock_enter():
+ *	This function is called to lock the function shared register
+ *	by setting the lock state to busy.
+ * Parameters:
+ *	handle		- NPI handle
+ * Return:
+ *	NPI_SUCCESS	- If the function id can own the lock.
+ *
+ *	Error:
+ *	NPI_FAILURE
+ *		VIR_SR_RESET
+ *		VIR_SR_BUSY
+ *		VIR_SR_INVALID
+ *		VIR_TAS_BUSY
+ */
+npi_status_t npi_dev_func_sr_lock_enter(npi_handle_t);
+
+/*
+ * npi_dev_func_sr_lock_free():
+ *	This function is called to free the function shared register
+ *	by setting the lock state to free.
+ * Parameters:
+ *	handle		- NPI handle
+ * Return:
+ *	NPI_SUCCESS	- If the function id can free the lock.
+ *
+ *	Error:
+ *	NPI_FAILURE
+ *		VIR_SR_NOTOWNER
+ *		VIR_TAS_NOTREAD
+ */
+npi_status_t npi_dev_func_sr_lock_free(npi_handle_t);
+
+/*
+ * npi_dev_func_sr_funcid_get():
+ *	This function is called to get the caller's function ID.
+ *	(based on address bits [25:26] on read access.
+ *	(After read, the TAS bit is always set to 1. Software needs
+ *	to write 0 to clear.) This function will write 0 to clear
+ *	the TAS bit if we own it.
+ * Parameters:
+ *	handle		- NPI handle
+ *	funcid_p	- pointer to store the function id.
+ * Return:
+ *	NPI_SUCCESS	- If get function id is complete successfully.
+ *
+ *	Error:
+ */
+npi_status_t npi_dev_func_sr_funcid_get(npi_handle_t, uint8_t *);
+
+/*
+ * npi_dev_func_sr_sr_raw_get():
+ *	This function is called to get the shared register value.
+ *	(After read, the TAS bit is always set to 1. Software needs
+ *	to write 0 to clear if we own it.)
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	sr_p		- pointer to store the shared value of this register.
+ *
+ * Return:
+ *	NPI_SUCCESS		- If shared value get is complete successfully.
+ *
+ *	Error:
+ */
+npi_status_t npi_dev_func_sr_sr_raw_get(npi_handle_t, uint16_t *);
+
+/*
+ * npi_dev_func_sr_sr_get():
+ *	This function is called to get the shared register value.
+ *	(After read, the TAS bit is always set to 1. Software needs
+ *	to write 0 to clear if we own it.)
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	sr_p		- pointer to store the shared value of this register.
+ *		    . this will get only non-lock, non-function id portion
+ *              . of the register
+ *
+ *
+ * Return:
+ *	NPI_SUCCESS		- If shared value get is complete successfully.
+ *
+ *	Error:
+ */
+
+npi_status_t npi_dev_func_sr_sr_get(npi_handle_t, uint16_t *);
+
+/*
+ * npi_dev_func_sr_sr_get_set_clear():
+ *	This function is called to set the shared register value.
+ *	(Shared register must be read first. If tas bit is 0, then
+ *	it implies that the software can proceed to set). After
+ *	setting, tas bit will be cleared.
+ * Parameters:
+ *	handle		- NPI handle
+ *	impl_sr		- shared value to set (only the 8 bit
+ *			  implementation specific state info).
+ *
+ * Return:
+ *	NPI_SUCCESS		- If shared value is set successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE
+ *		VIR_TAS_BUSY
+ */
+npi_status_t npi_dev_func_sr_sr_get_set_clear(npi_handle_t,
+					    uint16_t);
+
+/*
+ * npi_dev_func_sr_sr_set_only():
+ *	This function is called to only set the shared register value.
+ * Parameters:
+ *	handle		- NPI handle
+ *	impl_sr		- shared value to set.
+ *
+ * Return:
+ *	NPI_SUCCESS		- If shared value is set successfully.
+ *
+ *	Error:
+ *	NPI_FAILURE
+ *		VIR_TAS_BUSY
+ */
+npi_status_t npi_dev_func_sr_sr_set_only(npi_handle_t, uint16_t);
+
+/*
+ * npi_dev_func_sr_busy():
+ *	This function is called to see if we can own the device.
+ *	It will not reset the tas bit.
+ * Parameters:
+ *	handle		- NPI handle
+ *	busy_p		- pointer to store busy flag.
+ *				(B_TRUE: device is in use, B_FALSE: free).
+ * Return:
+ *	NPI_SUCCESS		- If tas bit is read successfully.
+ *	Error:
+ */
+npi_status_t npi_dev_func_sr_busy(npi_handle_t, boolean_t *);
+
+/*
+ * npi_dev_func_sr_tas_get():
+ *	This function is called to get the tas bit
+ *	(after read, this bit is always set to 1, software write 0
+ *	 to clear it).
+ *
+ * Parameters:
+ *	handle		- NPI handle
+ *	tas_p		- pointer to store the tas value
+ *
+ * Return:
+ *	NPI_SUCCESS		- If tas value get is complete successfully.
+ *	Error:
+ */
+npi_status_t npi_dev_func_sr_tas_get(npi_handle_t, uint8_t *);
+
+/*
+ * npi_fzc_mpc_set():
+ *	This function is called to enable the write access
+ *	to FZC region to function zero.
+ * Parameters:
+ *	handle		- NPI handle
+ * Return:
+ *	NPI_SUCCESS	-
+ *	Error:
+ */
+npi_status_t npi_fzc_mpc_set(npi_handle_t, boolean_t);
+
+/*
+ * npi_fzc_mpc_get():
+ *	This function is called to get the access mode.
+ * Parameters:
+ *	handle		- NPI handle
+ * Return:
+ *	NPI_SUCCESS	-
+ *
+ */
+npi_status_t npi_fzc_mpc_get(npi_handle_t, boolean_t *);
+
+/*
+ * npi_fzc_dma_bind_set():
+ *	This function is called to set DMA binding register.
+ * Parameters:
+ *	handle		- NPI handle
+ *	dma_bind	- NPI defined data structure that
+ *			  contains the tx/rx channel binding info.
+ *			  to set.
+ * Return:
+ *	NPI_SUCCESS	-
+ *	Error:
+ *	NPI_FAILURE
+ *
+ */
+npi_status_t npi_fzc_dma_bind_set(npi_handle_t, fzc_dma_bind_t);
+
+/*
+ * npi_fzc_ldg_num_set():
+ *	This function is called to set up a logical group number that
+ *	a logical device belongs to.
+ * Parameters:
+ *	handle		- NPI handle
+ *	ld		- logical device number (0 - 68)
+ *	ldg		- logical device group number (0 - 63)
+ * Return:
+ *	NPI_SUCCESS	-
+ *	Error:
+ *	NPI_FAILURE
+ *
+ */
+npi_status_t npi_fzc_ldg_num_set(npi_handle_t, uint8_t, uint8_t);
+
+/*
+ * npi_fzc_ldg_num_get():
+ *	This function is called to get the logical device group that
+ *	a logical device belongs to.
+ * Parameters:
+ *	handle		- NPI handle
+ *	ld		- logical device number (0 - 68)
+ *	*ldg_p		- pointer to store its group number.
+ * Return:
+ *	NPI_SUCCESS	-
+ *	Error:
+ *	NPI_FAILURE
+ */
+npi_status_t npi_fzc_ldg_num_get(npi_handle_t, uint8_t,
+		uint8_t *);
+
+npi_status_t npi_ldsv_ldfs_get(npi_handle_t, uint8_t,
+		uint64_t *, uint64_t *, uint64_t *);
+/*
+ * npi_ldsv_get():
+ *	This function is called to get device state vectors.
+ * Parameters:
+ *	handle		- NPI handle
+ *	ldg		- logical device group (0 - 63)
+ *	ldf_type	- either LDF0 (0) or LDF1 (1)
+ *	vector		- vector type (0, 1 or 2)
+ *	*ldf_p		- pointer to store its flag bits.
+ * Return:
+ *	NPI_SUCCESS	-
+ *	Error:
+ *	NPI_FAILURE
+ */
+npi_status_t npi_ldsv_get(npi_handle_t, uint8_t, ldsv_type_t,
+		uint64_t *);
+
+/*
+ * npi_ldsv_ld_get():
+ *	This function is called to get the flag bit value of a device.
+ * Parameters:
+ *	handle		- NPI handle
+ *	ldg		- logical device group (0 - 63)
+ *	ld		- logical device (0 - 68)
+ *	ldf_type	- either LDF0 (0) or LDF1 (1)
+ *	vector		- vector type (0, 1 or 2)
+ *	*ldf_p		- pointer to store its flag bits.
+ * Return:
+ *	NPI_SUCCESS	-
+ *	Error:
+ *	NPI_FAILURE
+ */
+npi_status_t npi_ldsv_ld_get(npi_handle_t, uint8_t, uint8_t,
+		ldsv_type_t, ldf_type_t, boolean_t *);
+/*
+ * npi_ldsv_ld_ldf0_get():
+ *	This function is called to get the ldf0 bit value of a device.
+ * Parameters:
+ *	handle		- NPI handle
+ *	ldg		- logical device group (0 - 63)
+ *	ld		- logical device (0 - 68)
+ *	*ldf_p		- pointer to store its flag bits.
+ * Return:
+ *	NPI_SUCCESS	-
+ *	Error:
+ *	NPI_FAILURE
+ */
+npi_status_t npi_ldsv_ld_ldf0_get(npi_handle_t, uint8_t, uint8_t,
+		boolean_t *);
+
+/*
+ * npi_ldsv_ld_ldf1_get():
+ *	This function is called to get the ldf1 bit value of a device.
+ * Parameters:
+ *	handle		- NPI handle
+ *	ldg		- logical device group (0 - 63)
+ *	ld		- logical device (0 - 68)
+ *	*ldf_p		- pointer to store its flag bits.
+ * Return:
+ *	NPI_SUCCESS	-
+ *	Error:
+ *	NPI_FAILURE
+ */
+npi_status_t npi_ldsv_ld_ldf1_get(npi_handle_t, uint8_t, uint8_t,
+		boolean_t *);
+/*
+ * npi_intr_mask_set():
+ *	This function is called to select the mask bits for both ldf0 and ldf1.
+ * Parameters:
+ *	handle		- NPI handle
+ *	ld		- logical device (0 - 68)
+ *	ldf_mask	- mask value to set (both ldf0 and ldf1).
+ * Return:
+ *	NPI_SUCCESS	-
+ *	Error:
+ *	NPI_FAILURE
+ */
+npi_status_t npi_intr_mask_set(npi_handle_t, uint8_t,
+			uint8_t);
+
+/*
+ * npi_intr_mask_get():
+ *	This function is called to get the mask bits.
+ * Parameters:
+ *	handle		- NPI handle
+ *	ld		- logical device (0 - 68)
+ *	ldf_mask	- pointer to store mask bits info.
+ * Return:
+ *	NPI_SUCCESS	-
+ *	Error:
+ *	NPI_FAILURE
+ */
+npi_status_t npi_intr_mask_get(npi_handle_t, uint8_t,
+			uint8_t *);
+
+/*
+ * npi_intr_ldg_mgmt_set():
+ *	This function is called to set interrupt timer and arm bit.
+ * Parameters:
+ *	handle		- NPI handle
+ *	ldg		- logical device group (0 - 63)
+ *	arm		- B_TRUE (arm) B_FALSE (disable)
+ * Return:
+ *	NPI_SUCCESS	-
+ *	Error:
+ *	NPI_FAILURE
+ */
+npi_status_t npi_intr_ldg_mgmt_set(npi_handle_t, uint8_t,
+			boolean_t, uint8_t);
+
+
+/*
+ * npi_intr_ldg_mgmt_timer_get():
+ *	This function is called to get the timer counter
+ * Parameters:
+ *	handle		- NPI handle
+ *	ldg		- logical device group (0 - 63)
+ *	timer_p		- pointer to store the timer counter.
+ * Return:
+ *	NPI_SUCCESS	-
+ *	Error:
+ *	NPI_FAILURE
+ */
+npi_status_t npi_intr_ldg_mgmt_timer_get(npi_handle_t, uint8_t,
+		uint8_t *);
+
+/*
+ * npi_intr_ldg_mgmt_arm():
+ *	This function is called to arm the group.
+ * Parameters:
+ *	handle		- NPI handle
+ *	ldg		- logical device group (0 - 63)
+ * Return:
+ *	NPI_SUCCESS	-
+ *	Error:
+ *	NPI_FAILURE
+ */
+npi_status_t npi_intr_ldg_mgmt_arm(npi_handle_t, uint8_t);
+
+/*
+ * npi_fzc_ldg_timer_res_set():
+ *	This function is called to set the timer resolution.
+ * Parameters:
+ *	handle		- NPI handle
+ *	res		- timer resolution (# of system clocks)
+ * Return:
+ *	NPI_SUCCESS	-
+ *	Error:
+ *	NPI_FAILURE
+ */
+npi_status_t npi_fzc_ldg_timer_res_set(npi_handle_t, uint32_t);
+
+/*
+ * npi_fzc_ldg_timer_res_get():
+ *	This function is called to get the timer resolution.
+ * Parameters:
+ *	handle		- NPI handle
+ *	res_p		- pointer to store the timer resolution.
+ * Return:
+ *	NPI_SUCCESS	-
+ *	Error:
+ *	NPI_FAILURE
+ */
+npi_status_t npi_fzc_ldg_timer_res_get(npi_handle_t, uint8_t *);
+
+/*
+ * npi_fzc_sid_set():
+ *	This function is called to set the system interrupt data.
+ * Parameters:
+ *	handle		- NPI handle
+ *	ldg		- logical group (0 - 63)
+ *	sid		- NPI defined data to set
+ * Return:
+ *	NPI_SUCCESS	-
+ *	Error:
+ *	NPI_FAILURE
+ */
+npi_status_t npi_fzc_sid_set(npi_handle_t, fzc_sid_t);
+
+/*
+ * npi_fzc_sid_get():
+ *	This function is called to get the system interrupt data.
+ * Parameters:
+ *	handle		- NPI handle
+ *	ldg		- logical group (0 - 63)
+ *	sid_p		- NPI defined data to get
+ * Return:
+ *	NPI_SUCCESS	-
+ *	Error:
+ *	NPI_FAILURE
+ */
+npi_status_t npi_fzc_sid_get(npi_handle_t, p_fzc_sid_t);
+npi_status_t npi_fzc_sys_err_mask_set(npi_handle_t, uint64_t);
+npi_status_t npi_fzc_sys_err_stat_get(npi_handle_t,
+						p_sys_err_stat_t);
+npi_status_t npi_vir_dump_pio_fzc_regs_one(npi_handle_t);
+npi_status_t npi_vir_dump_ldgnum(npi_handle_t);
+npi_status_t npi_vir_dump_ldsv(npi_handle_t);
+npi_status_t npi_vir_dump_imask0(npi_handle_t);
+npi_status_t npi_vir_dump_sid(npi_handle_t);
+#ifdef	__cplusplus
+}
+#endif
+
+#endif	/* _NPI_VIR_H */
diff --git a/drivers/net/nxge/npi/npi_zcp.c b/drivers/net/nxge/npi/npi_zcp.c
new file mode 100644
index 0000000..3fb479f
--- /dev/null
+++ b/drivers/net/nxge/npi/npi_zcp.c
@@ -0,0 +1,771 @@
+/*
+ * npi_zcp.c	Neptune ZCP HW API functions
+ *
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+
+#include <npi_zcp.h>
+
+static int zcp_mem_read(npi_handle_t, uint16_t, uint8_t,
+			uint16_t, zcp_ram_unit_t *);
+static int zcp_mem_write(npi_handle_t, uint16_t, uint8_t,
+			uint32_t, uint16_t,
+			zcp_ram_unit_t *);
+
+npi_status_t
+npi_zcp_config(npi_handle_t handle, config_op_t op, zcp_config_t config)
+{
+	uint64_t val = 0;
+
+	switch (op) {
+	case ENABLE:
+	case DISABLE:
+		if ((config == 0) || (config & ~CFG_ZCP_ALL) != 0) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_zcp_config"
+					    " Invalid Input: config <0x%x>",
+					    config));
+			return (NPI_FAILURE | NPI_ZCP_CONFIG_INVALID);
+		}
+
+		NXGE_REG_RD64(handle, ZCP_CONFIG_REG, &val);
+		if (op == ENABLE) {
+			if (config & CFG_ZCP)
+				val |= ZC_ENABLE;
+			if (config & CFG_ZCP_ECC_CHK)
+				val &= ~ECC_CHK_DIS;
+			if (config & CFG_ZCP_PAR_CHK)
+				val &= ~PAR_CHK_DIS;
+			if (config & CFG_ZCP_BUF_RESP)
+				val &= ~DIS_BUFF_RN;
+			if (config & CFG_ZCP_BUF_REQ)
+				val &= ~DIS_BUFF_RQ_IF;
+		} else {
+			if (config & CFG_ZCP)
+				val &= ~ZC_ENABLE;
+			if (config & CFG_ZCP_ECC_CHK)
+				val |= ECC_CHK_DIS;
+			if (config & CFG_ZCP_PAR_CHK)
+				val |= PAR_CHK_DIS;
+			if (config & CFG_ZCP_BUF_RESP)
+				val |= DIS_BUFF_RN;
+			if (config & CFG_ZCP_BUF_REQ)
+				val |= DIS_BUFF_RQ_IF;
+		}
+		NXGE_REG_WR64(handle, ZCP_CONFIG_REG, val);
+
+		break;
+	case INIT:
+		NXGE_REG_RD64(handle, ZCP_CONFIG_REG, &val);
+		val &= ((ZCP_DEBUG_SEL_MASK) | (RDMA_TH_MASK));
+		if (config & CFG_ZCP)
+			val |= ZC_ENABLE;
+		else
+			val &= ~ZC_ENABLE;
+		if (config & CFG_ZCP_ECC_CHK)
+			val &= ~ECC_CHK_DIS;
+		else
+			val |= ECC_CHK_DIS;
+		if (config & CFG_ZCP_PAR_CHK)
+			val &= ~PAR_CHK_DIS;
+		else
+			val |= PAR_CHK_DIS;
+		if (config & CFG_ZCP_BUF_RESP)
+			val &= ~DIS_BUFF_RN;
+		else
+			val |= DIS_BUFF_RN;
+		if (config & CFG_ZCP_BUF_REQ)
+			val &= DIS_BUFF_RQ_IF;
+		else
+			val |= DIS_BUFF_RQ_IF;
+		NXGE_REG_WR64(handle, ZCP_CONFIG_REG, val);
+
+		break;
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_zcp_config"
+					    " Invalid Input: config <0x%x>",
+					    config));
+		return (NPI_FAILURE | NPI_ZCP_OPCODE_INVALID);
+	}
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_zcp_iconfig(npi_handle_t handle, config_op_t op, zcp_iconfig_t iconfig)
+{
+	uint64_t val = 0;
+
+	switch (op) {
+	case ENABLE:
+	case DISABLE:
+		if ((iconfig == 0) || (iconfig & ~ICFG_ZCP_ALL) != 0) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_zcp_iconfig"
+					    " Invalid Input: iconfig <0x%x>",
+					    iconfig));
+			return (NPI_FAILURE | NPI_ZCP_CONFIG_INVALID);
+		}
+
+		NXGE_REG_RD64(handle, ZCP_INT_MASK_REG, &val);
+		if (op == ENABLE)
+			val |= iconfig;
+		else
+			val &= ~iconfig;
+		NXGE_REG_WR64(handle, ZCP_INT_MASK_REG, val);
+
+		break;
+
+	case INIT:
+		if ((iconfig & ~ICFG_ZCP_ALL) != 0) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_zcp_iconfig"
+					    " Invalid Input: iconfig <0x%x>",
+					    iconfig));
+			return (NPI_FAILURE | NPI_ZCP_CONFIG_INVALID);
+		}
+		val = (uint64_t)iconfig;
+		NXGE_REG_WR64(handle, ZCP_INT_MASK_REG, val);
+
+		break;
+	default:
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_zcp_iconfig"
+				    " Invalid Input: iconfig <0x%x>",
+				    iconfig));
+		return (NPI_FAILURE | NPI_ZCP_OPCODE_INVALID);
+	}
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_zcp_get_istatus(npi_handle_t handle, zcp_iconfig_t *istatus)
+{
+	uint64_t val;
+
+	NXGE_REG_RD64(handle, ZCP_INT_STAT_REG, &val);
+	*istatus = (uint32_t)val;
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_zcp_clear_istatus(npi_handle_t handle)
+{
+	uint64_t val;
+
+	val = (uint64_t)0xffffULL;
+	NXGE_REG_WR64(handle, ZCP_INT_STAT_REG, val);
+	return (NPI_SUCCESS);
+}
+
+
+npi_status_t
+npi_zcp_set_dma_thresh(npi_handle_t handle, uint16_t dma_thres)
+{
+	uint64_t val = 0;
+
+	if ((dma_thres & ~RDMA_TH_BITS) != 0) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_zcp_set_dma_thresh"
+				    " Invalid Input: dma_thres <0x%x>",
+				    dma_thres));
+		return (NPI_FAILURE | NPI_ZCP_DMA_THRES_INVALID);
+	}
+
+	NXGE_REG_RD64(handle, ZCP_CONFIG_REG, &val);
+
+	val &= ~RDMA_TH_MASK;
+	val |= (dma_thres << RDMA_TH_SHIFT);
+
+	NXGE_REG_WR64(handle, ZCP_CONFIG_REG, val);
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_zcp_set_bam_region(npi_handle_t handle, zcp_buf_region_t region,
+			zcp_bam_region_reg_t *region_attr)
+{
+
+	if (!IS_VALID_BAM_REGION(region)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_zcp_set_bam_region"
+				    " Invalid Input: region <0x%x>",
+				    region));
+		return (NPI_FAILURE | ZCP_BAM_REGION_INVALID);
+	}
+
+	switch (region) {
+	case BAM_4BUF:
+		NXGE_REG_WR64(handle, ZCP_BAM4_RE_CTL_REG, region_attr->value);
+		break;
+	case BAM_8BUF:
+		NXGE_REG_WR64(handle, ZCP_BAM8_RE_CTL_REG, region_attr->value);
+		break;
+	case BAM_16BUF:
+		NXGE_REG_WR64(handle, ZCP_BAM16_RE_CTL_REG, region_attr->value);
+		break;
+	case BAM_32BUF:
+		NXGE_REG_WR64(handle, ZCP_BAM32_RE_CTL_REG, region_attr->value);
+		break;
+	}
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_zcp_set_dst_region(npi_handle_t handle, zcp_buf_region_t region,
+				uint16_t row_idx)
+{
+	uint64_t val = 0;
+
+	if (!IS_VALID_BAM_REGION(region)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_zcp_set_dst_region"
+				    " Invalid Input: region <0x%x>",
+				    region));
+		return (NPI_FAILURE | NPI_ZCP_BAM_REGION_INVALID);
+	}
+
+	if ((row_idx & ~0x3FF) != 0) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_zcp_set_dst_region"
+				    " Invalid Input: row_idx", row_idx));
+		return (NPI_FAILURE | NPI_ZCP_ROW_INDEX_INVALID);
+	}
+
+	val = (uint64_t)row_idx;
+
+	switch (region) {
+	case BAM_4BUF:
+		NXGE_REG_WR64(handle, ZCP_DST4_RE_CTL_REG, val);
+		break;
+	case BAM_8BUF:
+		NXGE_REG_WR64(handle, ZCP_DST8_RE_CTL_REG, val);
+		break;
+	case BAM_16BUF:
+		NXGE_REG_WR64(handle, ZCP_DST16_RE_CTL_REG, val);
+		break;
+	case BAM_32BUF:
+		NXGE_REG_WR64(handle, ZCP_DST32_RE_CTL_REG, val);
+		break;
+	}
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_zcp_tt_static_entry(npi_handle_t handle, io_op_t op, uint16_t flow_id,
+			tte_sflow_attr_mask_t mask, tte_sflow_attr_t *sflow)
+{
+	uint32_t		byte_en = 0;
+	tte_sflow_attr_t	val;
+
+	if ((op != OP_SET) && (op != OP_GET)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_zcp_tt_static_entry"
+				    " Invalid Input: op <0x%x>",
+				    op));
+		return (NPI_FAILURE | NPI_ZCP_OPCODE_INVALID);
+	}
+
+	if ((mask & TTE_SFLOW_ATTR_ALL) == 0) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_zcp_tt_static_entry"
+				    " Invalid Input: mask <0x%x>",
+				    mask));
+		return (NPI_FAILURE | NPI_ZCP_SFLOW_ATTR_INVALID);
+	}
+
+	if ((flow_id & ~0x0FFF) != 0) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_zcp_tt_static_entry"
+				    " Invalid Input: flow_id<0x%x>",
+				    flow_id));
+		return (NPI_FAILURE | NPI_ZCP_FLOW_ID_INVALID);
+	}
+
+	if (zcp_mem_read(handle, flow_id, ZCP_RAM_SEL_TT_STATIC, 0,
+			(zcp_ram_unit_t *)&val) != 0) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_zcp_tt_static_entry"
+				    " HW Error: ZCP_RAM_ACC <0x%x>",
+				    NULL));
+		return (NPI_FAILURE | NPI_ZCP_MEM_READ_FAILED);
+	}
+
+	if (op == OP_SET) {
+		if (mask & TTE_RDC_TBL_OFF) {
+			val.qw0.bits.ldw.rdc_tbl_offset =
+					sflow->qw0.bits.ldw.rdc_tbl_offset;
+			byte_en |= TTE_RDC_TBL_SFLOW_BITS_EN;
+		}
+		if (mask & TTE_BUF_SIZE) {
+			val.qw0.bits.ldw.buf_size =
+					sflow->qw0.bits.ldw.buf_size;
+			byte_en |= TTE_BUF_SIZE_BITS_EN;
+		}
+		if (mask & TTE_NUM_BUF) {
+			val.qw0.bits.ldw.num_buf = sflow->qw0.bits.ldw.num_buf;
+			byte_en |= TTE_NUM_BUF_BITS_EN;
+		}
+		if (mask & TTE_ULP_END) {
+			val.qw0.bits.ldw.ulp_end = sflow->qw0.bits.ldw.ulp_end;
+			byte_en |=  TTE_ULP_END_BITS_EN;
+		}
+		if (mask & TTE_ULP_END) {
+			val.qw1.bits.ldw.ulp_end = sflow->qw1.bits.ldw.ulp_end;
+			byte_en |= TTE_ULP_END_BITS_EN;
+		}
+		if (mask & TTE_ULP_END_EN) {
+			val.qw1.bits.ldw.ulp_end_en =
+				sflow->qw1.bits.ldw.ulp_end_en;
+			byte_en |= TTE_ULP_END_EN_BITS_EN;
+		}
+		if (mask & TTE_UNMAP_ALL_EN) {
+			val.qw1.bits.ldw.unmap_all_en =
+					sflow->qw1.bits.ldw.unmap_all_en;
+			byte_en |= TTE_UNMAP_ALL_EN;
+		}
+		if (mask & TTE_TMODE) {
+			val.qw1.bits.ldw.tmode = sflow->qw1.bits.ldw.tmode;
+			byte_en |= TTE_TMODE_BITS_EN;
+		}
+		if (mask & TTE_SKIP) {
+			val.qw1.bits.ldw.skip = sflow->qw1.bits.ldw.skip;
+			byte_en |= TTE_SKIP_BITS_EN;
+		}
+		if (mask & TTE_HBM_RING_BASE_ADDR) {
+			val.qw1.bits.ldw.ring_base =
+					sflow->qw1.bits.ldw.ring_base;
+			byte_en |= TTE_RING_BASE_ADDR_BITS_EN;
+		}
+		if (mask & TTE_HBM_RING_BASE_ADDR) {
+			val.qw2.bits.ldw.ring_base =
+					sflow->qw2.bits.ldw.ring_base;
+			byte_en |= TTE_RING_BASE_ADDR_BITS_EN;
+		}
+		if (mask & TTE_HBM_RING_SIZE) {
+			val.qw2.bits.ldw.ring_size =
+					sflow->qw2.bits.ldw.ring_size;
+			byte_en |= TTE_RING_SIZE_BITS_EN;
+		}
+		if (mask & TTE_HBM_BUSY) {
+			val.qw2.bits.ldw.busy = sflow->qw2.bits.ldw.busy;
+			byte_en |= TTE_BUSY_BITS_EN;
+		}
+		if (mask & TTE_HBM_TOQ) {
+			val.qw3.bits.ldw.toq = sflow->qw3.bits.ldw.toq;
+			byte_en |= TTE_TOQ_BITS_EN;
+		}
+
+		if (zcp_mem_write(handle, flow_id, ZCP_RAM_SEL_TT_STATIC,
+					byte_en, 0,
+					(zcp_ram_unit_t *)&val) != 0) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_zcp_tt_static_entry"
+					    " HW Error: ZCP_RAM_ACC <0x%x>",
+					    NULL));
+			return (NPI_FAILURE | NPI_ZCP_MEM_WRITE_FAILED);
+		}
+	} else {
+		sflow->qw0.value = val.qw0.value;
+		sflow->qw1.value = val.qw1.value;
+		sflow->qw2.value = val.qw2.value;
+		sflow->qw3.value = val.qw3.value;
+		sflow->qw4.value = val.qw4.value;
+	}
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_zcp_tt_dynamic_entry(npi_handle_t handle, io_op_t op, uint16_t flow_id,
+			tte_dflow_attr_mask_t mask, tte_dflow_attr_t *dflow)
+{
+	uint32_t		byte_en = 0;
+	tte_dflow_attr_t	val;
+
+	if ((op != OP_SET) && (op != OP_GET)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_zcp_tt_dynamic_entry"
+				    " Invalid Input: op <0x%x>", op));
+		return (NPI_FAILURE | NPI_ZCP_OPCODE_INVALID);
+	}
+
+	if ((mask & TTE_DFLOW_ATTR_ALL) == 0) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_zcp_tt_dynamic_entry"
+				    " Invalid Input: mask <0x%x>",
+				    mask));
+		return (NPI_FAILURE | NPI_ZCP_DFLOW_ATTR_INVALID);
+	}
+
+	if ((flow_id & ~0x0FFF) != 0) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_zcp_tt_dynamic_entry"
+				    " Invalid Input: flow_id <0x%x>",
+				    flow_id));
+		return (NPI_FAILURE | NPI_ZCP_FLOW_ID_INVALID);
+	}
+
+	if (zcp_mem_read(handle, flow_id, ZCP_RAM_SEL_TT_DYNAMIC, 0,
+			(zcp_ram_unit_t *)&val) != 0) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_zcp_tt_dynamic_entry"
+				    " HW Error: ZCP_RAM_ACC <0x%x>",
+				    NULL));
+		return (NPI_FAILURE | NPI_ZCP_MEM_READ_FAILED);
+	}
+
+	if (op == OP_SET) {
+
+		/* Get data read */
+		if (mask & TTE_MAPPED_IN) {
+			val.qw0.bits.ldw.mapped_in =
+					dflow->qw0.bits.ldw.mapped_in;
+			byte_en |= TTE_MAPPED_IN_BITS_EN;
+		}
+		if (mask & TTE_ANCHOR_SEQ) {
+			val.qw1.bits.ldw.anchor_seq =
+					dflow->qw1.bits.ldw.anchor_seq;
+			byte_en |= TTE_ANCHOR_SEQ_BITS_EN;
+		}
+		if (mask & TTE_ANCHOR_OFFSET) {
+			val.qw2.bits.ldw.anchor_offset =
+					dflow->qw2.bits.ldw.anchor_offset;
+			byte_en |= TTE_ANCHOR_OFFSET_BITS_EN;
+		}
+		if (mask & TTE_ANCHOR_BUFFER) {
+			val.qw2.bits.ldw.anchor_buf =
+					dflow->qw2.bits.ldw.anchor_buf;
+			byte_en |= TTE_ANCHOR_BUFFER_BITS_EN;
+		}
+		if (mask & TTE_ANCHOR_BUF_FLAG) {
+			val.qw2.bits.ldw.anchor_buf_flag =
+					dflow->qw2.bits.ldw.anchor_buf_flag;
+			byte_en |= TTE_ANCHOR_BUF_FLAG_BITS_EN;
+		}
+		if (mask & TTE_UNMAP_ON_LEFT) {
+			val.qw2.bits.ldw.unmap_on_left =
+					dflow->qw2.bits.ldw.unmap_on_left;
+			byte_en |= TTE_UNMAP_ON_LEFT_BITS_EN;
+		}
+		if (mask & TTE_ULP_END_REACHED) {
+			val.qw2.bits.ldw.ulp_end_reached =
+					dflow->qw2.bits.ldw.ulp_end_reached;
+			byte_en |= TTE_ULP_END_REACHED_BITS_EN;
+		}
+		if (mask & TTE_ERR_STAT) {
+			val.qw3.bits.ldw.err_stat =
+					dflow->qw3.bits.ldw.err_stat;
+			byte_en |= TTE_ERR_STAT_BITS_EN;
+		}
+		if (mask & TTE_HBM_WR_PTR) {
+			val.qw3.bits.ldw.wr_ptr = dflow->qw3.bits.ldw.wr_ptr;
+			byte_en |= TTE_WR_PTR_BITS_EN;
+		}
+		if (mask & TTE_HBM_HOQ) {
+			val.qw3.bits.ldw.hoq = dflow->qw3.bits.ldw.hoq;
+			byte_en |= TTE_HOQ_BITS_EN;
+		}
+		if (mask & TTE_HBM_PREFETCH_ON) {
+			val.qw3.bits.ldw.prefetch_on =
+					dflow->qw3.bits.ldw.prefetch_on;
+			byte_en |= TTE_PREFETCH_ON_BITS_EN;
+		}
+
+		if (zcp_mem_write(handle, flow_id, ZCP_RAM_SEL_TT_DYNAMIC,
+					byte_en, 0,
+					(zcp_ram_unit_t *)&val) != 0) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_zcp_tt_dynamic_entry"
+					    " HW Error: ZCP_RAM_ACC <0x%x>",
+					    NULL));
+			return (NPI_FAILURE | NPI_ZCP_MEM_WRITE_FAILED);
+		}
+	} else {
+		dflow->qw0.value = val.qw0.value;
+		dflow->qw1.value = val.qw1.value;
+		dflow->qw2.value = val.qw2.value;
+		dflow->qw3.value = val.qw3.value;
+		dflow->qw4.value = val.qw4.value;
+	}
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_zcp_tt_bam_entry(npi_handle_t handle, io_op_t op, uint16_t flow_id,
+			uint8_t bankn, uint8_t word_en, zcp_ram_unit_t *data)
+{
+	zcp_ram_unit_t val;
+
+	if ((op != OP_SET) && (op != OP_GET)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_zcp_tt_bam_entry"
+				    " Invalid Input: op <0x%x>", op));
+		return (NPI_FAILURE | NPI_ZCP_OPCODE_INVALID);
+	}
+
+	if ((flow_id & ~0x0FFF) != 0) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_zcp_tt_dynamic_entry"
+				    " Invalid Input: flow_id <0x%x>",
+				    flow_id));
+		return (NPI_FAILURE | NPI_ZCP_FLOW_ID_INVALID);
+	}
+
+	if (bankn >= MAX_BAM_BANKS) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_zcp_tt_bam_entry"
+				    " Invalid Input: bankn <0x%x>",
+				    bankn));
+		return (NPI_FAILURE | NPI_ZCP_BAM_BANK_INVALID);
+	}
+
+	if ((word_en & ~0xF) != 0) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_zcp_tt_bam_entry"
+				    " Invalid Input: word_en <0x%x>",
+				    word_en));
+		return (NPI_FAILURE | NPI_ZCP_BAM_WORD_EN_INVALID);
+	}
+
+	if (zcp_mem_read(handle, flow_id, ZCP_RAM_SEL_BAM0 + bankn, 0,
+				(zcp_ram_unit_t *)&val) != 0) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_zcp_tt_bam_entry"
+				    " HW Error: ZCP_RAM_ACC <0x%x>",
+				    NULL));
+		return (NPI_FAILURE | NPI_ZCP_MEM_READ_FAILED);
+	}
+
+	if (op == OP_SET) {
+		if (zcp_mem_write(handle, flow_id, ZCP_RAM_SEL_BAM0 + bankn,
+					word_en, 0,
+					(zcp_ram_unit_t *)&val) != 0) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_zcp_tt_bam_entry"
+					    " HW Error: ZCP_RAM_ACC <0x%x>",
+					    NULL));
+			return (NPI_FAILURE | NPI_ZCP_MEM_WRITE_FAILED);
+		}
+	} else {
+		data->w0 = val.w0;
+		data->w1 = val.w1;
+		data->w2 = val.w2;
+		data->w3 = val.w3;
+	}
+
+	return (NPI_SUCCESS);
+}
+
+npi_status_t
+npi_zcp_tt_cfifo_entry(npi_handle_t handle, io_op_t op, uint8_t portn,
+			uint16_t entryn, zcp_ram_unit_t *data)
+{
+	if ((op != OP_SET) && (op != OP_GET)) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_zcp_tt_cfifo_entry"
+				    " Invalid Input: op <0x%x>", op));
+		return (NPI_FAILURE | NPI_ZCP_OPCODE_INVALID);
+	}
+
+	if (portn > 3) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_zcp_tt_cfifo_entry"
+				    " Invalid Input: portn <%d>", portn));
+		return (NPI_FAILURE | NPI_ZCP_PORT_INVALID(portn));
+	}
+
+	if (op == OP_SET) {
+		if (zcp_mem_write(handle, 0, ZCP_RAM_SEL_CFIFO0 + portn,
+					0x1ffff, entryn, data) != 0) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_zcp_tt_cfifo_entry"
+					    " HW Error: ZCP_RAM_ACC <0x%x>",
+					    NULL));
+			return (NPI_FAILURE | NPI_ZCP_MEM_WRITE_FAILED);
+		}
+	} else {
+		if (zcp_mem_read(handle, 0, ZCP_RAM_SEL_CFIFO0 + portn,
+					entryn, data) != 0) {
+			NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+					    " npi_zcp_tt_cfifo_entry"
+					    " HW Error: ZCP_RAM_ACC  <0x%x>",
+					NULL));
+			return (NPI_FAILURE | NPI_ZCP_MEM_READ_FAILED);
+		}
+	}
+
+	return (NPI_SUCCESS);
+}
+
+
+npi_status_t
+npi_zcp_rest_cfifo_port(npi_handle_t handle, uint8_t port)
+{
+	uint64_t offset = ZCP_RESET_CFIFO_REG;
+	zcp_reset_cfifo_t cfifo_reg;
+	NXGE_REG_RD64(handle, offset, &cfifo_reg.value);
+	cfifo_reg.value &= ZCP_RESET_CFIFO_MASK;
+
+	switch (port) {
+		case 0:
+			cfifo_reg.bits.ldw.reset_cfifo0 = 1;
+			NXGE_REG_WR64(handle, offset, cfifo_reg.value);
+			cfifo_reg.bits.ldw.reset_cfifo0 = 0;
+
+			break;
+		case 1:
+			cfifo_reg.bits.ldw.reset_cfifo1 = 1;
+			NXGE_REG_WR64(handle, offset, cfifo_reg.value);
+			cfifo_reg.bits.ldw.reset_cfifo1 = 0;
+			break;
+		case 2:
+			cfifo_reg.bits.ldw.reset_cfifo2 = 1;
+			NXGE_REG_WR64(handle, offset, cfifo_reg.value);
+			cfifo_reg.bits.ldw.reset_cfifo2 = 0;
+			break;
+		case 3:
+			cfifo_reg.bits.ldw.reset_cfifo3 = 1;
+			NXGE_REG_WR64(handle, offset, cfifo_reg.value);
+			cfifo_reg.bits.ldw.reset_cfifo3 = 0;
+			break;
+		default:
+			break;
+	}
+
+	NXGE_DELAY(ZCP_CFIFIO_RESET_WAIT);
+	NXGE_REG_WR64(handle, offset, cfifo_reg.value);
+
+	return (NPI_SUCCESS);
+}
+
+
+npi_status_t
+npi_zcp_rest_cfifo_all(npi_handle_t handle)
+{
+	uint64_t offset = ZCP_RESET_CFIFO_REG;
+	zcp_reset_cfifo_t cfifo_reg;
+
+	cfifo_reg.value = ZCP_RESET_CFIFO_MASK;
+	NXGE_REG_WR64(handle, offset, cfifo_reg.value);
+	cfifo_reg.value = 0;
+	NXGE_DELAY(ZCP_CFIFIO_RESET_WAIT);
+	NXGE_REG_WR64(handle, offset, cfifo_reg.value);
+	return (NPI_SUCCESS);
+}
+
+static int
+zcp_mem_read(npi_handle_t handle, uint16_t flow_id, uint8_t ram_sel,
+		uint16_t cfifo_entryn, zcp_ram_unit_t *val)
+{
+	zcp_ram_access_t ram_ctl;
+
+	ram_ctl.value = 0;
+	ram_ctl.bits.ldw.ram_sel = ram_sel;
+	ram_ctl.bits.ldw.zcfid = flow_id;
+	ram_ctl.bits.ldw.rdwr = ZCP_RAM_RD;
+	ram_ctl.bits.ldw.cfifo = cfifo_entryn;
+
+	/* Wait for RAM ready to be read */
+	ZCP_WAIT_RAM_READY(handle, ram_ctl.value);
+	if (ram_ctl.bits.ldw.busy != 0) {
+		NPI_ERROR_MSG((handle.function, NPI_ERR_CTL,
+				    " npi_zcp_tt_static_entry"
+				    " HW Error: ZCP_RAM_ACC <0x%x>",
+				    ram_ctl.value));
+		return (-1);
+	}
+
+	/* Read from RAM */
+	NXGE_REG_WR64(handle, ZCP_RAM_ACC_REG, ram_ctl.value);
+
+	/* Wait for RAM read done */
+	ZCP_WAIT_RAM_READY(handle, ram_ctl.value);
+	if (ram_ctl.bits.ldw.busy != 0)
+		return (-1);
+
+	/* Get data */
+	NXGE_REG_RD64(handle, ZCP_RAM_DATA0_REG, &val->w0);
+	NXGE_REG_RD64(handle, ZCP_RAM_DATA1_REG, &val->w1);
+	NXGE_REG_RD64(handle, ZCP_RAM_DATA2_REG, &val->w2);
+	NXGE_REG_RD64(handle, ZCP_RAM_DATA3_REG, &val->w3);
+	NXGE_REG_RD64(handle, ZCP_RAM_DATA4_REG, &val->w4);
+
+	return (0);
+}
+
+static int
+zcp_mem_write(npi_handle_t handle, uint16_t flow_id, uint8_t ram_sel,
+		uint32_t byte_en, uint16_t cfifo_entryn, zcp_ram_unit_t *val)
+{
+	zcp_ram_access_t	ram_ctl;
+	zcp_ram_benable_t	ram_en;
+
+	ram_ctl.value = 0;
+	ram_ctl.bits.ldw.ram_sel = ram_sel;
+	ram_ctl.bits.ldw.zcfid = flow_id;
+	ram_ctl.bits.ldw.rdwr = ZCP_RAM_WR;
+	ram_en.bits.ldw.be = byte_en;
+	ram_ctl.bits.ldw.cfifo = cfifo_entryn;
+
+	/* Setup data */
+	NXGE_REG_WR64(handle, ZCP_RAM_DATA0_REG, val->w0);
+	NXGE_REG_WR64(handle, ZCP_RAM_DATA1_REG, val->w1);
+	NXGE_REG_WR64(handle, ZCP_RAM_DATA2_REG, val->w2);
+	NXGE_REG_WR64(handle, ZCP_RAM_DATA3_REG, val->w3);
+	NXGE_REG_WR64(handle, ZCP_RAM_DATA4_REG, val->w4);
+
+	/* Set byte mask */
+	NXGE_REG_WR64(handle, ZCP_RAM_BE_REG, ram_en.value);
+
+	/* Write to RAM */
+	NXGE_REG_WR64(handle, ZCP_RAM_ACC_REG, ram_ctl.value);
+
+	/* Wait for RAM write complete */
+	ZCP_WAIT_RAM_READY(handle, ram_ctl.value);
+	if (ram_ctl.bits.ldw.busy != 0)
+		return (-1);
+
+	return (0);
+}
diff --git a/drivers/net/nxge/npi/npi_zcp.h b/drivers/net/nxge/npi/npi_zcp.h
new file mode 100644
index 0000000..f9048bb
--- /dev/null
+++ b/drivers/net/nxge/npi/npi_zcp.h
@@ -0,0 +1,199 @@
+/*
+ * npi_zcp.h	Neptune ZCP HW API definitions
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+
+#ifndef _NPI_ZCP_H
+#define	_NPI_ZCP_H
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+#include <npi.h>
+#include <nxge_zcp_hw.h>
+
+typedef	enum zcp_buf_region_e {
+	BAM_4BUF			= 1,
+	BAM_8BUF			= 2,
+	BAM_16BUF			= 3,
+	BAM_32BUF			= 4
+} zcp_buf_region_t;
+
+typedef enum zcp_config_e {
+	CFG_ZCP				= 0x01,
+	CFG_ZCP_ECC_CHK			= 0x02,
+	CFG_ZCP_PAR_CHK			= 0x04,
+	CFG_ZCP_BUF_RESP		= 0x08,
+	CFG_ZCP_BUF_REQ			= 0x10,
+	CFG_ZCP_ALL			= 0x1F
+} zcp_config_t;
+
+typedef enum zcp_iconfig_e {
+	ICFG_ZCP_RRFIFO_UNDERRUN	= RRFIFO_UNDERRUN,
+	ICFG_ZCP_RRFIFO_OVERRUN		= RRFIFO_OVERRUN,
+	ICFG_ZCP_RSPFIFO_UNCORR_ERR	= RSPFIFO_UNCORR_ERR,
+	ICFG_ZCP_BUFFER_OVERFLOW	= BUFFER_OVERFLOW,
+	ICFG_ZCP_STAT_TBL_PERR		= STAT_TBL_PERR,
+	ICFG_ZCP_DYN_TBL_PERR		= BUF_DYN_TBL_PERR,
+	ICFG_ZCP_BUF_TBL_PERR		= BUF_TBL_PERR,
+	ICFG_ZCP_TT_PROGRAM_ERR		= TT_PROGRAM_ERR,
+	ICFG_ZCP_RSP_TT_INDEX_ERR	= RSP_TT_INDEX_ERR,
+	ICFG_ZCP_SLV_TT_INDEX_ERR	= SLV_TT_INDEX_ERR,
+	ICFG_ZCP_TT_INDEX_ERR		= ZCP_TT_INDEX_ERR,
+	ICFG_ZCP_CFIFO_ECC3		= CFIFO_ECC3,
+	ICFG_ZCP_CFIFO_ECC2		= CFIFO_ECC2,
+	ICFG_ZCP_CFIFO_ECC1		= CFIFO_ECC1,
+	ICFG_ZCP_CFIFO_ECC0		= CFIFO_ECC0,
+	ICFG_ZCP_ALL			= (RRFIFO_UNDERRUN | RRFIFO_OVERRUN |
+				RSPFIFO_UNCORR_ERR | STAT_TBL_PERR |
+				BUF_DYN_TBL_PERR | BUF_TBL_PERR |
+				TT_PROGRAM_ERR | RSP_TT_INDEX_ERR |
+				SLV_TT_INDEX_ERR | ZCP_TT_INDEX_ERR |
+				CFIFO_ECC3 | CFIFO_ECC2 |  CFIFO_ECC1 |
+				CFIFO_ECC0 | BUFFER_OVERFLOW)
+} zcp_iconfig_t;
+
+typedef enum tte_sflow_attr_mask_e {
+	TTE_RDC_TBL_OFF			= 0x0001,
+	TTE_BUF_SIZE			= 0x0002,
+	TTE_NUM_BUF			= 0x0004,
+	TTE_ULP_END			= 0x0008,
+	TTE_ULP_END_EN			= 0x0010,
+	TTE_UNMAP_ALL_EN		= 0x0020,
+	TTE_TMODE			= 0x0040,
+	TTE_SKIP			= 0x0080,
+	TTE_HBM_RING_BASE_ADDR		= 0x0100,
+	TTE_HBM_RING_SIZE		= 0x0200,
+	TTE_HBM_BUSY			= 0x0400,
+	TTE_HBM_TOQ			= 0x0800,
+	TTE_SFLOW_ATTR_ALL		= 0x0FFF
+} tte_sflow_attr_mask_t;
+
+typedef	enum tte_dflow_attr_mask_e {
+	TTE_MAPPED_IN			= 0x0001,
+	TTE_ANCHOR_SEQ			= 0x0002,
+	TTE_ANCHOR_OFFSET		= 0x0004,
+	TTE_ANCHOR_BUFFER		= 0x0008,
+	TTE_ANCHOR_BUF_FLAG		= 0x0010,
+	TTE_UNMAP_ON_LEFT		= 0x0020,
+	TTE_ULP_END_REACHED		= 0x0040,
+	TTE_ERR_STAT			= 0x0080,
+	TTE_HBM_WR_PTR			= 0x0100,
+	TTE_HBM_HOQ			= 0x0200,
+	TTE_HBM_PREFETCH_ON		= 0x0400,
+	TTE_DFLOW_ATTR_ALL		= 0x07FF
+} tte_dflow_attr_mask_t;
+
+#define	IS_VALID_BAM_REGION(region)\
+		((region == BAM_4BUF) || (region == BAM_8BUF) ||\
+		(region == BAM_16BUF) || (region == BAM_32BUF))
+
+#define	ZCP_WAIT_RAM_READY(handle, val) {\
+	uint32_t cnt = MAX_PIO_RETRIES;\
+	do {\
+		NXGE_REG_RD64(handle, ZCP_RAM_ACC_REG, &val);\
+		cnt--;\
+	} while ((ram_ctl.bits.ldw.busy != 0) && (cnt > 0));\
+}
+
+#define	ZCP_DMA_THRES_INVALID		0x10
+#define	ZCP_BAM_REGION_INVALID		0x11
+#define	ZCP_ROW_INDEX_INVALID		0x12
+#define	ZCP_SFLOW_ATTR_INVALID		0x13
+#define	ZCP_DFLOW_ATTR_INVALID		0x14
+#define	ZCP_FLOW_ID_INVALID		0x15
+#define	ZCP_BAM_BANK_INVALID		0x16
+#define	ZCP_BAM_WORD_EN_INVALID		0x17
+
+#define	NPI_ZCP_OPCODE_INVALID		((ZCP_BLK_ID << 8) | OPCODE_INVALID)
+#define	NPI_ZCP_CONFIG_INVALID		((ZCP_BLK_ID << 8) | CONFIG_INVALID)
+#define	NPI_ZCP_DMA_THRES_INVALID	((ZCP_BLK_ID << 8) |\
+					ZCP_DMA_THRES_INVALID)
+#define	NPI_ZCP_BAM_REGION_INVALID	((ZCP_BLK_ID << 8) |\
+					ZCP_BAM_REGION_INVALID)
+#define	NPI_ZCP_ROW_INDEX_INVALID	((ZCP_BLK_ID << 8) |\
+					ZCP_ROW_INDEX_INVALID)
+#define	NPI_ZCP_SFLOW_ATTR_INVALID	((ZCP_BLK_ID << 8) |\
+					ZCP_SFLOW_ATTR_INVALID)
+#define	NPI_ZCP_DFLOW_ATTR_INVALID	((ZCP_BLK_ID << 8) |\
+					ZCP_DFLOW_ATTR_INVALID)
+#define	NPI_ZCP_FLOW_ID_INVALID		((ZCP_BLK_ID << 8) |\
+					ZCP_FLOW_ID_INVALID)
+#define	NPI_ZCP_MEM_WRITE_FAILED	((ZCP_BLK_ID << 8) | WRITE_FAILED)
+#define	NPI_ZCP_MEM_READ_FAILED		((ZCP_BLK_ID << 8) | READ_FAILED)
+#define	NPI_ZCP_BAM_BANK_INVALID	((ZCP_BLK_ID << 8) |\
+					(ZCP_BAM_BANK_INVALID))
+#define	NPI_ZCP_BAM_WORD_EN_INVALID	((ZCP_BLK_ID << 8) |\
+					(ZCP_BAM_WORD_EN_INVALID))
+#define	NPI_ZCP_PORT_INVALID(portn)	((ZCP_BLK_ID << 8) | PORT_INVALID |\
+					(portn << 12))
+
+/* ZCP HW NPI Prototypes */
+npi_status_t npi_zcp_config(npi_handle_t, config_op_t,
+				zcp_config_t);
+npi_status_t npi_zcp_iconfig(npi_handle_t, config_op_t,
+				zcp_iconfig_t);
+npi_status_t npi_zcp_get_istatus(npi_handle_t, zcp_iconfig_t *);
+npi_status_t npi_zcp_clear_istatus(npi_handle_t);
+npi_status_t npi_zcp_set_dma_thresh(npi_handle_t, uint16_t);
+npi_status_t npi_zcp_set_bam_region(npi_handle_t,
+				zcp_buf_region_t,
+				zcp_bam_region_reg_t *);
+npi_status_t npi_zcp_set_sdt_region(npi_handle_t,
+				zcp_buf_region_t, uint16_t);
+npi_status_t npi_zcp_tt_static_entry(npi_handle_t, io_op_t,
+				uint16_t, tte_sflow_attr_mask_t,
+				tte_sflow_attr_t *);
+npi_status_t npi_zcp_tt_dynamic_entry(npi_handle_t, io_op_t,
+				uint16_t, tte_dflow_attr_mask_t,
+				tte_dflow_attr_t *);
+npi_status_t npi_zcp_tt_bam_entry(npi_handle_t, io_op_t,
+				uint16_t, uint8_t,
+				uint8_t, zcp_ram_unit_t *);
+npi_status_t npi_zcp_tt_cfifo_entry(npi_handle_t, io_op_t,
+				uint8_t, uint16_t,
+				zcp_ram_unit_t *);
+
+npi_status_t npi_zcp_rest_cfifo_port(npi_handle_t, uint8_t);
+npi_status_t npi_zcp_rest_cfifo_all(npi_handle_t);
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif	/* _NPI_ZCP_H */
diff --git a/drivers/net/nxge/nxge_espc.c b/drivers/net/nxge/nxge_espc.c
new file mode 100644
index 0000000..30e1290
--- /dev/null
+++ b/drivers/net/nxge/nxge_espc.c
@@ -0,0 +1,399 @@
+/*
+ * nxge_espc.c	Neptune  SPROM Interface functions
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#include <nxge_impl.h>
+#include <nxge_mac.h>
+#include <npi_espc.h>
+#include <nxge_espc.h>
+
+extern nxge_os_mutex_t nxge_eeprom_lock;
+static nxge_status_t nxge_check_vpd_version(p_nxge_t nxgep);
+
+void
+nxge_espc_get_next_mac_addr(uint8_t *st_mac, uint8_t nxt_cnt,
+			    struct ether_addr *final_mac)
+{
+	uint64_t	mac[ETHERADDRL];
+	uint64_t	mac_addr = 0;
+	int		i, j;
+
+	for (i = ETHERADDRL - 1, j = 0; j < ETHERADDRL; i--, j++) {
+		mac[j] = st_mac[i];
+		mac_addr |= (mac[j] << (j*8));
+	}
+
+	mac_addr += nxt_cnt;
+
+	final_mac->ether_addr_octet[0] = (mac_addr & 0xff0000000000ULL) >> 40;
+	final_mac->ether_addr_octet[1] = (mac_addr & 0xff00000000ULL) >> 32;
+	final_mac->ether_addr_octet[2] = (mac_addr & 0xff000000ULL) >> 24;
+	final_mac->ether_addr_octet[3] = (mac_addr & 0xff0000ULL) >> 16;
+	final_mac->ether_addr_octet[4] = (mac_addr & 0xff00) >> 8;
+	final_mac->ether_addr_octet[5] = (mac_addr & 0xff);
+}
+
+nxge_status_t
+nxge_espc_mac_addrs_get(p_nxge_t nxgep)
+{
+	nxge_status_t	status = NXGE_OK;
+	npi_status_t	npi_status = NPI_SUCCESS;
+	uint8_t		port_num = nxgep->mac.portnum;
+	npi_handle_t	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	uint8_t		mac_addr[ETHERADDRL];
+	int		i;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_espc_mac_addr_get, port[%d]",
+			port_num));
+
+	npi_status = npi_espc_mac_addr_get(handle, mac_addr);
+	if (npi_status != NPI_SUCCESS) {
+		status = (NXGE_ERROR | npi_status);
+		goto exit;
+	}
+
+	if (!is_valid_ether_addr(mac_addr)) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"Invalid MAC address in SEEPROM: "));
+		for (i = 0; i < 6; i++)
+			printk("%2.2x%c", mac_addr[i],
+			       i == 5 ? ' ' : ':');
+		printk("\n");
+		status = NXGE_ERROR;
+		goto exit;
+	}
+
+	nxge_espc_get_next_mac_addr(mac_addr, port_num, &nxgep->mac.mac_addr);
+
+	/* TODO - get num of addrs and fill up alt mac addrs with algo here */
+
+exit:
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_espc_mac_addr_get, "
+			"status [0x%x]", status));
+
+	return (status);
+}
+
+nxge_status_t
+nxge_espc_num_ports_get(p_nxge_t nxgep)
+{
+	nxge_status_t	status = NXGE_OK;
+	npi_status_t	npi_status = NPI_SUCCESS;
+	npi_handle_t	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_espc_num_ports_get"));
+
+	npi_status = npi_espc_num_ports_get(handle, &nxgep->nports);
+	if (npi_status != NPI_SUCCESS) {
+		status = (NXGE_ERROR | npi_status);
+	}
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_espc_num_ports_get, "
+			"status [0x%x]", status));
+
+	return (status);
+}
+
+nxge_status_t
+nxge_espc_phy_type_get(p_nxge_t nxgep)
+{
+	nxge_status_t	status = NXGE_OK;
+	npi_status_t	npi_status = NPI_SUCCESS;
+	npi_handle_t	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	uint8_t		port_num = nxgep->mac.portnum;
+	uint8_t		phy_type;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_espc_phy_type_get, port[%d]",
+			port_num));
+
+	npi_status = npi_espc_port_phy_type_get(handle, &phy_type,
+						port_num);
+	if (npi_status != NPI_SUCCESS) {
+		status = (NXGE_ERROR | npi_status);
+		goto exit;
+	}
+
+	switch (phy_type) {
+	case ESC_PHY_10G_FIBER:
+		nxgep->mac.portmode = PORT_10G_FIBER;
+		nxgep->statsp->mac_stats.xcvr_inuse = XPCS_XCVR;
+		nxgep->niu_type = NEPTUNE_2;
+		break;
+	case ESC_PHY_10G_COPPER:
+		nxgep->mac.portmode = PORT_10G_COPPER;
+		nxgep->statsp->mac_stats.xcvr_inuse = XPCS_XCVR;
+		nxgep->niu_type = NEPTUNE_2;
+		break;
+	case ESC_PHY_1G_FIBER:
+		nxgep->mac.portmode = PORT_1G_FIBER;
+		nxgep->statsp->mac_stats.xcvr_inuse = PCS_XCVR;
+		nxgep->niu_type = NEPTUNE;
+		break;
+	case ESC_PHY_1G_COPPER:
+		nxgep->mac.portmode = PORT_1G_COPPER;
+		nxgep->statsp->mac_stats.xcvr_inuse = INT_MII_XCVR;
+		nxgep->niu_type = NEPTUNE;
+		break;
+	case ESC_PHY_NONE:
+		status = NXGE_ERROR;
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL, "nxge_espc_phy_type_get:"
+				"No phy type set"));
+		break;
+	default:
+		status = NXGE_ERROR;
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL, "nxge_espc_phy_type_get: "
+				"Unknown phy type [%d]", phy_type));
+		break;
+	}
+
+exit:
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_espc_phy_type_get, "
+			"status [0x%x]", status));
+
+	return (status);
+}
+
+nxge_status_t
+nxge_espc_max_frame_sz_get(p_nxge_t nxgep)
+{
+	nxge_status_t	status = NXGE_OK;
+	npi_status_t	npi_status = NPI_SUCCESS;
+	npi_handle_t	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_espc_max_frame_sz_get"));
+
+	npi_status = npi_espc_max_frame_get(handle, &nxgep->mac.maxframesize);
+	if (npi_status != NPI_SUCCESS) {
+		status = (NXGE_ERROR | npi_status);
+	}
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_espc_max_frame_sz_get, "
+			       "status [0x%x]", status));
+
+	return (status);
+}
+
+nxge_status_t
+nxge_espc_verify_chksum(p_nxge_t nxgep)
+{
+	uint16_t	img_size;
+	uint64_t	sum = 0;
+	int		reg_cnt, i;
+	uint64_t	val = 0;
+	nxge_status_t	status = NXGE_OK;
+	npi_status_t	npi_status = NPI_SUCCESS;
+	npi_handle_t	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_espc_verify_chksum"));
+
+	npi_status = npi_espc_img_sz_get(handle, &img_size);
+	if (npi_status != NPI_SUCCESS) {
+		status = (NXGE_ERROR | npi_status);
+		goto fail;
+	}
+
+	nxgep->seeprom_len = img_size;
+
+	reg_cnt = img_size/4;
+
+	for (i = 0; i < reg_cnt; i++) {
+		NXGE_REG_RD64(handle, ESPC_NCR_REGN(i), &val);
+		sum += ((val & 0xff) + ((val & 0xff00) >> 8) +
+			((val & 0xff0000) >> 16) +
+			((val & 0xff000000) >> 24));
+	}
+
+	if ((sum & 0xff) != ESC_IMG_CHKSUM_VAL) {
+		printk(KERN_ERR "SEEPROM checksum 0x%llx INVALID!!\n",
+		       (unsigned long long)(sum & 0xff));
+		status = NXGE_ERROR;
+	}
+
+  fail:
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_espc_verify_chksum, "
+			       "status [0x%x]", status));
+
+	return (status);
+}
+
+nxge_status_t
+nxge_vpd_info_get(p_nxge_t nxgep)
+{
+	npi_status_t	status;
+	npi_handle_t	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+
+	MUTEX_ENTER(&nxge_eeprom_lock);
+	npi_espc_pio_enable(handle);
+	status = npi_espc_vpd_info_get(handle, &nxgep->vpd_info,
+				       NXGE_EROM_LEN);
+	npi_espc_pio_disable(handle);
+	MUTEX_EXIT(&nxge_eeprom_lock);
+
+	nxgep->vpd_info.ver_valid = B_FALSE;
+	if (status == NPI_SUCCESS) {
+		(void) nxge_check_vpd_version(nxgep);
+		return (NXGE_OK);
+	} else {
+		return (NXGE_ERROR);
+	}
+}
+
+#define	NXGE_FCODE_ID_STR	"FCode "
+#define	NXGE_PXE_ID_STR		"PXE"
+#define	NXGE_FCODE_VER_STR_LEN	5
+#define	NXGE_VPD_VALID_VER_W	3
+#define	NXGE_VPD_VALID_VER_F	4
+
+static nxge_status_t
+nxge_check_vpd_version(p_nxge_t nxgep)
+{
+	int		i, j;
+	nxge_status_t	status = NXGE_OK;
+	const char	*fcode_str = NXGE_FCODE_ID_STR;
+	int		fcode_str_len = strlen(fcode_str);
+	char		ver_num_str[NXGE_VPD_VER_LEN];
+	char		*ver_num_w = NULL;
+	char		*ver_num_f = NULL;
+	int		ver_num_w_len = 0;
+	int		ver_num_f_len = 0;
+	int		ver_w = 0;
+	int		ver_f = 0;
+	boolean_t	copy = B_TRUE;
+	boolean_t	mod_copied = B_FALSE;
+	const char	*pxe_str = NXGE_PXE_ID_STR;
+	int		pxe_str_len = strlen(pxe_str);
+	int		fw_ver_len = 0;
+
+	nxgep->vpd_info.ver_valid = B_FALSE;
+	ver_num_str[0] = '\0';
+
+	/*
+	 * First pick out the necessary info from the version string.
+	 * We need to do this to fit in all the important info such as the
+	 * board type, the PXE version and the FCode version within 31
+	 * characters allowed in the fw_version field of the ethtool_drvinfo
+	 * structure.
+	 */
+	NXGE_DEBUG_MSG((nxgep, NXGE_CFG_CTL,
+			"nxge_check_vpd_version: original fw_version "
+			"string [%s]", nxgep->vpd_info.ver));
+	memcpy(ver_num_str, nxgep->vpd_info.ver, NXGE_VPD_VER_LEN);
+	for (i = 0, j = 0; i < NXGE_VPD_VER_LEN; i++) {
+		if (ver_num_str[i] == pxe_str[0]) {
+			if (strncmp(&ver_num_str[i], pxe_str,
+			    pxe_str_len) == 0)
+				copy = B_TRUE;
+		}
+		if (ver_num_str[i] == fcode_str[0]) {
+			if (strncmp(&ver_num_str[i], fcode_str,
+			    fcode_str_len) == 0)
+				copy = B_TRUE;
+		}
+		if (copy) {
+			nxgep->vpd_info.ver[j] = ver_num_str[i];
+			j++;
+			if (ver_num_str[i] == ' ' && !mod_copied) {
+				copy = B_FALSE;
+				mod_copied = B_TRUE;
+			}
+		}
+	}
+	nxgep->vpd_info.ver[j] = '\0';
+	fw_ver_len = strlen(nxgep->vpd_info.ver);
+	if (fw_ver_len > 31) {
+		/* take out the last word which is the date string */
+		for (i = fw_ver_len - 1; i >= 0; i--) {
+			if (nxgep->vpd_info.ver[i] == ' ') {
+				nxgep->vpd_info.ver[i] = '\0';
+				break;
+			}
+		}
+	}
+	NXGE_DEBUG_MSG((nxgep, NXGE_CFG_CTL,
+			"nxge_check_vpd_version: truncated fw_version "
+			"string [%s]", nxgep->vpd_info.ver));
+
+	for (i = 0; i < NXGE_VPD_VER_LEN; i++) {
+		if (nxgep->vpd_info.ver[i] == fcode_str[0]) {
+			if ((i + fcode_str_len + NXGE_FCODE_VER_STR_LEN) >
+			    NXGE_VPD_VER_LEN)
+				break;
+			for (j = 0; j < fcode_str_len; j++, i++) {
+				if (nxgep->vpd_info.ver[i] != fcode_str[j])
+					break;
+			}
+			if (j < fcode_str_len)
+				continue;
+
+			/* found the Fcode version string */
+			for (j = 0; j < NXGE_FCODE_VER_STR_LEN; j++, i++) {
+				ver_num_str[j] = nxgep->vpd_info.ver[i];
+				if (ver_num_str[j] == ' ')
+					break;
+			}
+			ver_num_str[j] = '\0';
+			break;
+		}
+	}
+
+	ver_num_w = ver_num_str;
+	for (i = 0; i < strlen(ver_num_str); i++) {
+		if (ver_num_str[i] == '.') {
+			ver_num_f = &ver_num_str[i + 1];
+			ver_num_w_len = i;
+			ver_num_f_len = strlen(ver_num_str) - (i + 1);
+			break;
+		}
+	}
+
+	for (i = 0; i < ver_num_w_len; i++) {
+		ver_w = (ver_w * 10) + (ver_num_w[i] - '0');
+	}
+
+	for (i = 0; i < ver_num_f_len; i++) {
+		ver_f = (ver_f * 10) + (ver_num_f[i] - '0');
+	}
+
+	if ((ver_w > NXGE_VPD_VALID_VER_W) ||
+	    (ver_w == NXGE_VPD_VALID_VER_W && ver_f >= NXGE_VPD_VALID_VER_F))
+		nxgep->vpd_info.ver_valid = B_TRUE;
+
+	return (status);
+}
+
+
diff --git a/drivers/net/nxge/nxge_ethtool.c b/drivers/net/nxge/nxge_ethtool.c
new file mode 100644
index 0000000..6bcdd50
--- /dev/null
+++ b/drivers/net/nxge/nxge_ethtool.c
@@ -0,0 +1,2062 @@
+/*
+ * nxge_ethtool.c	Neptune Ethertool interface functions
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+
+#include <nxge_impl.h>
+
+static struct {
+	const uint64_t offsets;
+} ethtool_bmac_reg_table[] = {
+	{BTXMAC_SW_RST_REG},
+	{BRXMAC_SW_RST_REG},
+	{MAC_SEND_PAUSE_REG},
+	{BTXMAC_STATUS_REG},
+	{BRXMAC_STATUS_REG},
+	{BMAC_CTRL_STAT_REG},
+	{BTXMAC_STAT_MSK_REG},
+	{BRXMAC_STAT_MSK_REG},
+	{BMAC_C_S_MSK_REG},
+	{TXMAC_CONFIG_REG},
+	{RXMAC_CONFIG_REG},
+	{MAC_CTRL_CONFIG_REG},
+	{MAC_XIF_CONFIG_REG},
+	{BMAC_MIN_REG},
+	{BMAC_MAX_REG},
+	{MAC_PA_SIZE_REG},
+	{MAC_CTRL_TYPE_REG},
+	{BMAC_ADDR0_REG},
+	{BMAC_ADDR1_REG},
+	{BMAC_ADDR2_REG},
+	{BMAC_ADDR3_REG},
+	{BMAC_ADDR4_REG},
+	{BMAC_ADDR5_REG},
+	{BMAC_ADDR6_REG},
+	{BMAC_ADDR7_REG},
+	{BMAC_ADDR8_REG},
+	{BMAC_ADDR9_REG},
+	{BMAC_ADDR10_REG},
+	{BMAC_ADDR11_REG},
+	{BMAC_ADDR12_REG},
+	{BMAC_ADDR13_REG},
+	{BMAC_ADDR14_REG},
+	{BMAC_ADDR15_REG},
+	{BMAC_ADDR16_REG},
+	{BMAC_ADDR17_REG},
+	{BMAC_ADDR18_REG},
+	{BMAC_ADDR19_REG},
+	{BMAC_ADDR20_REG},
+	{BMAC_ADDR21_REG},
+	{BMAC_ADDR22_REG},
+	{BMAC_ADDR23_REG},
+	{MAC_FC_ADDR0_REG},
+	{MAC_FC_ADDR1_REG},
+	{MAC_FC_ADDR2_REG},
+	{MAC_ADDR_FILT0_REG},
+	{MAC_ADDR_FILT1_REG},
+	{MAC_ADDR_FILT2_REG},
+	{MAC_ADDR_FILT12_MASK_REG},
+	{MAC_ADDR_FILT00_MASK_REG},
+	{MAC_HASH_TBL0_REG},
+	{MAC_HASH_TBL1_REG},
+	{MAC_HASH_TBL2_REG},
+	{MAC_HASH_TBL3_REG},
+	{MAC_HASH_TBL4_REG},
+	{MAC_HASH_TBL5_REG},
+	{MAC_HASH_TBL6_REG},
+	{MAC_HASH_TBL7_REG},
+	{MAC_HASH_TBL8_REG},
+	{MAC_HASH_TBL9_REG},
+	{MAC_HASH_TBL10_REG},
+	{MAC_HASH_TBL11_REG},
+	{MAC_HASH_TBL12_REG},
+	{MAC_HASH_TBL13_REG},
+	{MAC_HASH_TBL14_REG},
+	{MAC_HASH_TBL15_REG},
+	{RXMAC_FRM_CNT_REG},
+	{MAC_LEN_ER_CNT_REG},
+	{BMAC_AL_ER_CNT_REG},
+	{BMAC_CRC_ER_CNT_REG},
+	{BMAC_CD_VIO_CNT_REG},
+	{BMAC_SM_REG},
+	{BMAC_ALTAD_CMPEN_REG},
+	{BMAC_HOST_INF0_REG},
+	{BMAC_HOST_INF1_REG},
+	{BMAC_HOST_INF2_REG},
+	{BMAC_HOST_INF3_REG},
+	{BMAC_HOST_INF4_REG},
+	{BMAC_HOST_INF5_REG},
+	{BMAC_HOST_INF6_REG},
+	{BMAC_HOST_INF7_REG},
+	{BMAC_HOST_INF8_REG},
+	{BTXMAC_BYTE_CNT_REG},
+	{BTXMAC_FRM_CNT_REG},
+	{BRXMAC_BYTE_CNT_REG}
+};
+#define NXGE_BMAC_REG_LEN	(sizeof(ethtool_bmac_reg_table)/sizeof(uint64_t))
+
+static struct {
+	const uint64_t offsets;
+} ethtool_xmac_reg_table[] = {
+  {XTXMAC_SW_RST_REG},
+  {XRXMAC_SW_RST_REG},
+  {XTXMAC_STATUS_REG},
+  {XRXMAC_STATUS_REG},
+  {XMAC_CTRL_STAT_REG},
+  {XTXMAC_STAT_MSK_REG},
+  {XRXMAC_STAT_MSK_REG},
+  {XMAC_C_S_MSK_REG},
+  {XMAC_CONFIG_REG},
+  {XMAC_IPG_REG},
+  {XMAC_MIN_REG},
+  {XMAC_MAX_REG},
+  {XMAC_ADDR0_REG},
+  {XMAC_ADDR1_REG},
+  {XMAC_ADDR2_REG},
+  {XRXMAC_BT_CNT_REG},
+  {XRXMAC_BC_FRM_CNT_REG},
+  {XRXMAC_MC_FRM_CNT_REG},
+  {XRXMAC_FRAG_CNT_REG},
+  {XRXMAC_HIST_CNT1_REG},
+  {XRXMAC_HIST_CNT2_REG},
+  {XRXMAC_HIST_CNT3_REG},
+  {XRXMAC_HIST_CNT4_REG},
+  {XRXMAC_HIST_CNT5_REG},
+  {XRXMAC_HIST_CNT6_REG},
+  {XRXMAC_MPSZER_CNT_REG},
+  {XRXMAC_CRC_ER_CNT_REG},
+  {XRXMAC_CD_VIO_CNT_REG},
+  {XRXMAC_AL_ER_CNT_REG},
+  {XTXMAC_FRM_CNT_REG},
+  {XTXMAC_BYTE_CNT_REG},
+  {XMAC_LINK_FLT_CNT_REG},
+  {XRXMAC_HIST_CNT7_REG},
+  {XMAC_SM_REG},
+  {XMAC_INTERN1_REG},
+  {XMAC_ADDR_CMPEN_REG},
+  {XMAC_ADDR3_REG},
+  {XMAC_ADDR4_REG},
+  {XMAC_ADDR5_REG},
+  {XMAC_ADDR6_REG},
+  {XMAC_ADDR7_REG},
+  {XMAC_ADDR8_REG},
+  {XMAC_ADDR9_REG},
+  {XMAC_ADDR10_REG},
+  {XMAC_ADDR11_REG},
+  {XMAC_ADDR12_REG},
+  {XMAC_ADDR13_REG},
+  {XMAC_ADDR14_REG},
+  {XMAC_ADDR15_REG},
+  {XMAC_ADDR16_REG},
+  {XMAC_ADDR17_REG},
+  {XMAC_ADDR18_REG},
+  {XMAC_ADDR19_REG},
+  {XMAC_ADDR20_REG},
+  {XMAC_ADDR21_REG},
+  {XMAC_ADDR22_REG},
+  {XMAC_ADDR23_REG},
+  {XMAC_ADDR24_REG},
+  {XMAC_ADDR25_REG},
+  {XMAC_ADDR26_REG},
+  {XMAC_ADDR27_REG},
+  {XMAC_ADDR28_REG},
+  {XMAC_ADDR29_REG},
+  {XMAC_ADDR30_REG},
+  {XMAC_ADDR31_REG},
+  {XMAC_ADDR32_REG},
+  {XMAC_ADDR33_REG},
+  {XMAC_ADDR34_REG},
+  {XMAC_ADDR35_REG},
+  {XMAC_ADDR36_REG},
+  {XMAC_ADDR37_REG},
+  {XMAC_ADDR38_REG},
+  {XMAC_ADDR39_REG},
+  {XMAC_ADDR40_REG},
+  {XMAC_ADDR41_REG},
+  {XMAC_ADDR42_REG},
+  {XMAC_ADDR43_REG},
+  {XMAC_ADDR44_REG},
+  {XMAC_ADDR45_REG},
+  {XMAC_ADDR46_REG},
+  {XMAC_ADDR47_REG},
+  {XMAC_ADDR48_REG},
+  {XMAC_ADDR49_REG},
+  {XMAC_ADDR50_REG},
+  {XMAC_ADDR_FILT0_REG},
+  {XMAC_ADDR_FILT1_REG},
+  {XMAC_ADDR_FILT2_REG},
+  {XMAC_ADDR_FILT12_MASK_REG},
+  {XMAC_ADDR_FILT0_MASK_REG},
+  {XMAC_HASH_TBL0_REG},
+  {XMAC_HASH_TBL1_REG},
+  {XMAC_HASH_TBL2_REG},
+  {XMAC_HASH_TBL3_REG},
+  {XMAC_HASH_TBL4_REG},
+  {XMAC_HASH_TBL5_REG},
+  {XMAC_HASH_TBL6_REG},
+  {XMAC_HASH_TBL7_REG},
+  {XMAC_HASH_TBL8_REG},
+  {XMAC_HASH_TBL9_REG},
+  {XMAC_HASH_TBL10_REG},
+  {XMAC_HASH_TBL11_REG},
+  {XMAC_HASH_TBL12_REG},
+  {XMAC_HASH_TBL13_REG},
+  {XMAC_HASH_TBL14_REG},
+  {XMAC_HASH_TBL15_REG},
+  {XMAC_HOST_INF0_REG},
+  {XMAC_HOST_INF1_REG},
+  {XMAC_HOST_INF2_REG},
+  {XMAC_HOST_INF3_REG},
+  {XMAC_HOST_INF4_REG},
+  {XMAC_HOST_INF5_REG},
+  {XMAC_HOST_INF6_REG},
+  {XMAC_HOST_INF7_REG},
+  {XMAC_HOST_INF8_REG},
+  {XMAC_HOST_INF9_REG},
+  {XMAC_HOST_INF10_REG},
+  {XMAC_HOST_INF11_REG},
+  {XMAC_HOST_INF12_REG},
+  {XMAC_HOST_INF13_REG},
+  {XMAC_HOST_INF14_REG},
+  {XMAC_HOST_INF15_REG},
+  {XMAC_HOST_INF16_REG},
+  {XMAC_HOST_INF17_REG},
+  {XMAC_HOST_INF18_REG},
+  {XMAC_HOST_INF19_REG},
+  {XMAC_PA_DATA0_REG},
+  {XMAC_PA_DATA1_REG},
+  {XMAC_DEBUG_SEL_REG},
+  {XMAC_TRAINING_VECT_REG}
+};
+#define NXGE_XMAC_REG_LEN	(sizeof(ethtool_xmac_reg_table)/sizeof(uint64_t))
+
+static struct {
+	const uint64_t offsets;
+} ethtool_mif_reg_table[] = {
+  {MIF_STATUS_REG},
+  {MIF_STATE_MACHINE_REG}
+};
+#define NXGE_MIF_REG_LEN	(sizeof(ethtool_mif_reg_table)/sizeof(uint64_t))
+
+static struct {
+	const uint64_t offsets;
+} ethtool_pcs_reg_table[] = {
+  {PCS_MII_CTRL_REG},
+  {PCS_MII_STATUS_REG},
+  {PCS_MII_ADVERT_REG},
+  {PCS_MII_LPA_REG},
+  {PCS_CONFIG_REG},
+  {PCS_STATE_MACHINE_REG},
+  {PCS_INTR_STATUS_REG},
+  {PCS_DATAPATH_MODE_REG},
+  {PCS_PACKET_COUNT_REG}
+};
+#define NXGE_PCS_REG_LEN	(sizeof(ethtool_pcs_reg_table)/sizeof(uint64_t))
+
+static struct {
+	const uint64_t offsets;
+} ethtool_xpcs_reg_table[] = {
+  {XPCS_CTRL_1_REG},
+  {XPCS_STATUS_1_REG},
+  {XPCS_DEV_ID_REG},
+  {XPCS_SPEED_ABILITY_REG},
+  {XPCS_DEV_IN_PKG_REG},
+  {XPCS_CTRL_2_REG},
+  {XPCS_STATUS_2_REG},
+  {XPCS_PKG_ID_REG},
+  {XPCS_STATUS_REG},
+  {XPCS_TEST_CTRL_REG},
+  {XPCS_CFG_VENDOR_1_REG},
+  {XPCS_DIAG_VENDOR_2_REG},
+  {XPCS_MASK_1_REG},
+  {XPCS_PKT_CNTR_REG},
+  {XPCS_TX_STATE_MC_REG},
+  {XPCS_DESKEW_ERR_CNTR_REG},
+  {XPCS_SYM_ERR_CNTR_L0_L1_REG},
+  {XPCS_SYM_ERR_CNTR_L2_L3_REG},
+  {XPCS_TRAINING_VECTOR_REG}
+};
+#define NXGE_XPCS_REG_LEN	(sizeof(ethtool_xpcs_reg_table)/sizeof(uint64_t))
+
+typedef struct ethtool_ipp_reg_s {
+	const uint64_t offsets;
+} ethtool_ipp_reg_t;
+
+static ethtool_ipp_reg_t ethtool_ipp_reg_table[] = {
+  {IPP_CONFIG_REG},
+  {IPP_DISCARD_PKT_CNT_REG},
+  {IPP_TCP_CKSUM_ERR_CNT_REG},
+  {IPP_ECC_ERR_COUNTER_REG},
+  {IPP_INT_STATUS_REG},
+  {IPP_INT_MASK_REG},
+  {IPP_PFIFO_RD_DATA0_REG},
+  {IPP_PFIFO_RD_DATA1_REG},
+  {IPP_PFIFO_RD_DATA2_REG},
+  {IPP_PFIFO_RD_DATA3_REG},
+  {IPP_PFIFO_RD_DATA4_REG},
+  {IPP_PFIFO_WR_DATA0_REG},
+  {IPP_PFIFO_WR_DATA1_REG},
+  {IPP_PFIFO_WR_DATA2_REG},
+  {IPP_PFIFO_WR_DATA3_REG},
+  {IPP_PFIFO_WR_DATA4_REG},
+  {IPP_PFIFO_RD_PTR_REG},
+  {IPP_PFIFO_WR_PTR_REG},
+  {IPP_DFIFO_RD_DATA0_REG},
+  {IPP_DFIFO_RD_DATA1_REG},
+  {IPP_DFIFO_RD_DATA2_REG},
+  {IPP_DFIFO_RD_DATA3_REG},
+  {IPP_DFIFO_RD_DATA4_REG},
+  {IPP_DFIFO_WR_DATA0_REG},
+  {IPP_DFIFO_WR_DATA1_REG},
+  {IPP_DFIFO_WR_DATA2_REG},
+  {IPP_DFIFO_WR_DATA3_REG},
+  {IPP_DFIFO_WR_DATA4_REG},
+  {IPP_DFIFO_RD_PTR_REG},
+  {IPP_DFIFO_WR_PTR_REG},
+  {IPP_STATE_MACHINE_REG},
+  {IPP_CKSUM_STATUS_REG},
+  {IPP_FFLP_CKSUM_INFO_REG},
+  {IPP_DEBUG_SELECT_REG},
+  {IPP_DFIFO_ECC_SYNDROME_REG},
+  {IPP_DFIFO_EOPM_RD_PTR_REG},
+  {IPP_ECC_CTRL_REG}
+};
+#define NXGE_IPP_REG_LEN	(sizeof(ethtool_ipp_reg_table)/sizeof(uint64_t))
+
+typedef struct ethtool_rxdma_reg_s {
+	const uint64_t offsets;
+	const boolean_t multi;
+	const uint32_t	cnt;
+	const uint32_t	step;
+} ethtool_rxdma_reg_t;
+
+static ethtool_rxdma_reg_t ethtool_rxdma_reg_table[] = {
+
+  /* 128 cnt, step 0x8 */
+  {RDC_TBL_REG, B_TRUE, NXGE_MAX_RDC_GROUPS * NXGE_MAX_RDCS, 0x8},
+
+  /* 16 cnt, step 0x200 */
+  {RXDMA_CFIG1_REG, B_TRUE, NXGE_MAX_RDCS, 0x200},
+  {RXDMA_CFIG2_REG, B_TRUE, NXGE_MAX_RDCS, 0x200},
+  {RBR_CFIG_A_REG, B_TRUE, NXGE_MAX_RDCS, 0x200},
+  {RBR_CFIG_B_REG, B_TRUE, NXGE_MAX_RDCS, 0x200},
+  {RBR_KICK_REG, B_TRUE, NXGE_MAX_RDCS, 0x200},
+  {RBR_STAT_REG, B_TRUE, NXGE_MAX_RDCS, 0x200},
+  {RBR_HDH_REG, B_TRUE, NXGE_MAX_RDCS, 0x200},
+  {RBR_HDL_REG, B_TRUE, NXGE_MAX_RDCS, 0x200},
+  {RCRCFIG_A_REG, B_TRUE, NXGE_MAX_RDCS, 0x200},
+  {RCRCFIG_B_REG, B_TRUE, NXGE_MAX_RDCS, 0x200},
+  {RCRSTAT_A_REG, B_TRUE, NXGE_MAX_RDCS, 0x200},
+  {RCRSTAT_B_REG, B_TRUE, NXGE_MAX_RDCS, 0x200},
+  {RCRSTAT_C_REG, B_TRUE, NXGE_MAX_RDCS, 0x200},
+  {RX_DMA_ENT_MSK_REG, B_TRUE, NXGE_MAX_RDCS, 0x200},
+  {RX_DMA_CTL_STAT_REG, B_TRUE, NXGE_MAX_RDCS, 0x200},
+  {RCR_FLSH_REG, B_TRUE, NXGE_MAX_RDCS, 0x200},
+/*   {RX_DMA_LOGA_REG, B_TRUE, NXGE_MAX_RDCS, 0x200}, */
+/*   {RX_DMA_LOGB_REG, B_TRUE, NXGE_MAX_RDCS, 0x200}, */
+/*   {RDC_PRE_EMPTY_REG, B_TRUE, NXGE_MAX_RDCS, 0x200},	 */
+  {RXMISC_DISCARD_REG, B_TRUE, NXGE_MAX_RDCS, 0x200},
+  {RX_DMA_CTL_STAT_DBG_REG, B_TRUE, NXGE_MAX_RDCS, 0x200},
+
+  {RX_DMA_CK_DIV_REG, B_FALSE, 0, 0},
+  {DEF_PT0_RDC_REG, B_FALSE, 0, 0},
+  {DEF_PT1_RDC_REG, B_FALSE, 0, 0},
+  {DEF_PT2_RDC_REG, B_FALSE, 0, 0},
+  {DEF_PT3_RDC_REG, B_FALSE, 0, 0},
+  {PT_DRR_WT0_REG, B_FALSE, 0, 0},
+  {PT_DRR_WT1_REG, B_FALSE, 0, 0},
+  {PT_DRR_WT2_REG, B_FALSE, 0, 0},
+  {PT_DRR_WT3_REG, B_FALSE, 0, 0},
+  {PT_USE0_REG, B_FALSE, 0, 0},
+  {PT_USE1_REG, B_FALSE, 0, 0},
+  {PT_USE2_REG, B_FALSE, 0, 0},
+  {PT_USE3_REG, B_FALSE, 0, 0},
+  {RX_ADDR_MD_REG, B_FALSE, 0, 0},
+  {RED_RAN_INIT_REG, B_FALSE, 0, 0},
+
+  {RDMC_PRE_PAR_ERR_REG, B_FALSE, 0, 0},
+  {RDMC_SHA_PAR_ERR_REG, B_FALSE, 0, 0},
+  {RDMC_MEM_ADDR_REG, B_FALSE, 0, 0},
+  {RDMC_MEM_DATA0_REG, B_FALSE, 0, 0},
+  {RDMC_MEM_DATA1_REG, B_FALSE, 0, 0},
+  {RDMC_MEM_DATA2_REG, B_FALSE, 0, 0},
+  {RDMC_MEM_DATA3_REG, B_FALSE, 0, 0},
+  {RDMC_MEM_DATA4_REG, B_FALSE, 0, 0},
+  {RX_CTL_DAT_FIFO_STAT_REG, B_FALSE, 0, 0},
+  {RX_CTL_DAT_FIFO_MASK_REG, B_FALSE, 0, 0},
+  {RDMC_TRAINING_VECTOR_REG, B_FALSE, 0, 0},
+  {RX_CTL_DAT_FIFO_STAT_DBG_REG, B_FALSE, 0, 0},
+
+  /* 16 cnt, step 0x40 */
+  {RX_LOG_PAGE_VLD_REG, B_TRUE, NXGE_MAX_RDCS, 0x40},
+  {RX_LOG_PAGE_MASK1_REG, B_TRUE, NXGE_MAX_RDCS, 0x40},
+  {RX_LOG_PAGE_VAL1_REG, B_TRUE, NXGE_MAX_RDCS, 0x40},
+  {RX_LOG_PAGE_MASK2_REG, B_TRUE, NXGE_MAX_RDCS, 0x40},
+  {RX_LOG_PAGE_VAL2_REG, B_TRUE, NXGE_MAX_RDCS, 0x40},
+  {RX_LOG_PAGE_RELO1_REG, B_TRUE, NXGE_MAX_RDCS, 0x40},
+  {RX_LOG_PAGE_RELO2_REG, B_TRUE, NXGE_MAX_RDCS, 0x40},
+  {RX_LOG_PAGE_HDL_REG, B_TRUE, NXGE_MAX_RDCS, 0x40},
+
+  /* 16 cnt, step 0x40 */
+  {RED_DIS_CNT_REG, B_TRUE, NXGE_MAX_RDCS, 0x40},
+  {RDC_RED_PARA_REG, B_TRUE, NXGE_MAX_RDCS, 0x40}
+};
+#define NXGE_RXDMA_REG_LEN	(sizeof(ethtool_rxdma_reg_table)/sizeof(ethtool_rxdma_reg_t))
+
+typedef struct ethtool_fflp_reg_s {
+	const uint64_t offsets;
+	const boolean_t multi;
+	const uint32_t	cnt;
+	const uint32_t	step;
+} ethtool_fflp_reg_t;
+
+static ethtool_fflp_reg_t ethtool_fflp_reg_table[] = {
+
+  {FFLP_HASH_TBL_ADDR_REG, B_TRUE, 8, 8192},
+  {FFLP_HASH_TBL_DATA_REG, B_TRUE, 8, 8192},
+  {FFLP_HASH_TBL_DATA_LOG_REG, B_TRUE, 8, 8192},
+
+  {FFLP_ENET_VLAN_TBL_REG, B_TRUE, 4096, 8},
+
+  {FFLP_VLAN_PAR_ERR_REG, B_FALSE, 0, 0},
+  {FFLP_L2_CLS_ENET1_REG, B_FALSE, 0, 0},
+  {FFLP_L2_CLS_ENET2_REG, B_FALSE, 0, 0},
+  {FFLP_L3_CLS_IP_U4_REG, B_FALSE, 0, 0},
+  {FFLP_L3_CLS_IP_U5_REG, B_FALSE, 0, 0},
+  {FFLP_L3_CLS_IP_U6_REG, B_FALSE, 0, 0},
+  {FFLP_L3_CLS_IP_U7_REG, B_FALSE, 0, 0},
+  {FFLP_TCAM_KEY_IP_USR4_REG, B_FALSE, 0, 0},
+  {FFLP_TCAM_KEY_IP_USR5_REG, B_FALSE, 0, 0},
+  {FFLP_TCAM_KEY_IP_USR6_REG, B_FALSE, 0, 0},
+  {FFLP_TCAM_KEY_IP_USR7_REG, B_FALSE, 0, 0},
+  {FFLP_TCAM_KEY_IP4_TCP_REG, B_FALSE, 0, 0},
+  {FFLP_TCAM_KEY_IP4_UDP_REG, B_FALSE, 0, 0},
+  {FFLP_TCAM_KEY_IP4_AH_ESP_REG, B_FALSE, 0, 0},
+  {FFLP_TCAM_KEY_IP4_SCTP_REG, B_FALSE, 0, 0},
+  {FFLP_TCAM_KEY_IP6_TCP_REG, B_FALSE, 0, 0},
+  {FFLP_TCAM_KEY_IP6_UDP_REG, B_FALSE, 0, 0},
+  {FFLP_TCAM_KEY_IP6_AH_ESP_REG, B_FALSE, 0, 0},
+  {FFLP_TCAM_KEY_IP6_SCTP_REG, B_FALSE, 0, 0},
+  {FFLP_TCAM_KEY_0_REG, B_FALSE, 0, 0},
+  {FFLP_TCAM_KEY_1_REG, B_FALSE, 0, 0},
+  {FFLP_TCAM_KEY_2_REG, B_FALSE, 0, 0},
+  {FFLP_TCAM_KEY_3_REG, B_FALSE, 0, 0},
+  {FFLP_TCAM_MASK_0_REG, B_FALSE, 0, 0},
+  {FFLP_TCAM_MASK_1_REG, B_FALSE, 0, 0},
+  {FFLP_TCAM_MASK_2_REG, B_FALSE, 0, 0},
+  {FFLP_TCAM_MASK_3_REG, B_FALSE, 0, 0},
+  {FFLP_TCAM_CTL_REG, B_FALSE, 0, 0},
+  {FFLP_TCAM_ERR_REG, B_FALSE, 0, 0},
+  {HASH_LKUP_ERR_LOG1_REG, B_FALSE, 0, 0},
+  {HASH_LKUP_ERR_LOG2_REG, B_FALSE, 0, 0},
+  {FFLP_CFG_1_REG, B_FALSE, 0, 0},
+  {FFLP_TCP_CFLAG_MSK_REG, B_FALSE, 0, 0},
+  {FFLP_FCRAM_REF_TMR_REG, B_FALSE, 0, 0},
+  {FFLP_FCRAM_FIO_ADDR_REG, B_FALSE, 0, 0},
+  {FFLP_FCRAM_FIO_DAT_REG, B_FALSE, 0, 0},
+  {FFLP_FCRAM_ERR_TST0_REG, B_FALSE, 0, 0},
+  {FFLP_FCRAM_ERR_TST1_REG, B_FALSE, 0, 0},
+  {FFLP_FCRAM_ERR_TST2_REG, B_FALSE, 0, 0},
+  {FFLP_ERR_MSK_REG, B_FALSE, 0, 0},
+  {FFLP_DBG_TRAIN_VCT_REG, B_FALSE, 0, 0},
+  {FFLP_FCRAM_PHY_RD_LAT_REG, B_FALSE, 0, 0},
+  {FFLP_FLOW_KEY_IP_USR4_REG, B_FALSE, 0, 0},
+  {FFLP_FLOW_KEY_IP_USR5_REG, B_FALSE, 0, 0},
+  {FFLP_FLOW_KEY_IP_USR6_REG, B_FALSE, 0, 0},
+  {FFLP_FLOW_KEY_IP_USR7_REG, B_FALSE, 0, 0},
+  {FFLP_FLOW_KEY_IP4_TCP_REG, B_FALSE, 0, 0},
+  {FFLP_FLOW_KEY_IP4_UDP_REG, B_FALSE, 0, 0},
+  {FFLP_FLOW_KEY_IP4_AH_ESP_REG, B_FALSE, 0, 0},
+  {FFLP_FLOW_KEY_IP4_SCTP_REG, B_FALSE, 0, 0},
+  {FFLP_FLOW_KEY_IP6_TCP_REG, B_FALSE, 0, 0},
+  {FFLP_FLOW_KEY_IP6_UDP_REG, B_FALSE, 0, 0},
+  {FFLP_FLOW_KEY_IP6_AH_ESP_REG, B_FALSE, 0, 0},
+  {FFLP_FLOW_KEY_IP6_SCTP_REG, B_FALSE, 0, 0},
+  {FFLP_H1POLY_REG, B_FALSE, 0, 0},
+  {FFLP_H2POLY_REG, B_FALSE, 0, 0},
+  {FFLP_FLW_PRT_SEL_REG, B_TRUE, 8, 8},
+
+};
+#define NXGE_FFLP_REG_LEN	(sizeof(ethtool_fflp_reg_table)/sizeof(ethtool_fflp_reg_t))
+
+typedef struct ethtool_espc_reg_s {
+	const uint64_t offsets;
+	const boolean_t multi;
+	const uint32_t	cnt;
+	const uint32_t	step;
+} ethtool_espc_reg_t;
+
+static ethtool_espc_reg_t ethtool_espc_reg_table[] = {
+  {ESPC_PIO_EN_REG, B_FALSE, 0, 0},
+  {ESPC_PIO_STATUS_REG, B_FALSE, 0, 0},
+  {ESPC_NCR_REG, B_TRUE, 128, 8}
+};
+#define NXGE_ESPC_REG_LEN	(sizeof(ethtool_espc_reg_table)/sizeof(uint64_t))
+
+typedef struct ethtool_txc_reg_s {
+	const uint64_t offsets;
+	const boolean_t multi;
+	const boolean_t pport;
+	const uint32_t	cnt;
+	const uint32_t	step;
+} ethtool_txc_reg_t;
+
+#define PORT_CNT	NXGE_PORTS_NEPTUNE
+
+static ethtool_txc_reg_t ethtool_txc_reg_table[] = {
+  {TXC_DMA_MAX_BURST_REG, B_TRUE, B_FALSE, NXGE_MAX_TDCS, 0x1000},
+  {TXC_DMA_MAX_LENGTH_REG, B_TRUE, B_FALSE, NXGE_MAX_TDCS, 0x1000},
+
+  {TXC_CONTROL_REG, B_FALSE, B_FALSE, 0, 0},
+  {TXC_TRAINING_REG, B_FALSE, B_FALSE, 0, 0},
+  {TXC_DEBUG_SELECT_REG, B_FALSE, B_FALSE, 0, 0},
+  {TXC_MAX_REORDER_REG, B_FALSE, B_FALSE, 0, 0},
+
+  {TXC_PORT_CTL_REG, B_TRUE, B_TRUE, PORT_CNT, 0x100},
+  {TXC_PORT_DMA_ENABLE_REG, B_TRUE, B_TRUE, PORT_CNT, 0x100},
+  {TXC_PKT_STUFFED_REG, B_TRUE, B_TRUE, PORT_CNT, 0x100},
+  {TXC_PKT_XMIT_REG, B_TRUE, B_TRUE, PORT_CNT, 0x100},
+  {TXC_ROECC_CTL_REG, B_TRUE, B_TRUE, PORT_CNT, 0x100},
+  {TXC_ROECC_ST_REG, B_TRUE, B_TRUE, PORT_CNT, 0x100},
+  {TXC_RO_DATA0_REG, B_TRUE, B_TRUE, PORT_CNT, 0x100},
+  {TXC_RO_DATA1_REG, B_TRUE, B_TRUE, PORT_CNT, 0x100},
+  {TXC_RO_DATA2_REG, B_TRUE, B_TRUE, PORT_CNT, 0x100},
+  {TXC_RO_DATA3_REG, B_TRUE, B_TRUE, PORT_CNT, 0x100},
+  {TXC_RO_DATA4_REG, B_TRUE, B_TRUE, PORT_CNT, 0x100},
+  {TXC_SFECC_CTL_REG, B_TRUE, B_TRUE, PORT_CNT, 0x100},
+  {TXC_SFECC_ST_REG, B_TRUE, B_TRUE, PORT_CNT, 0x100},
+  {TXC_SF_DATA0_REG, B_TRUE, B_TRUE, PORT_CNT, 0x100},
+  {TXC_SF_DATA1_REG, B_TRUE, B_TRUE, PORT_CNT, 0x100},
+  {TXC_SF_DATA2_REG, B_TRUE, B_TRUE, PORT_CNT, 0x100},
+  {TXC_SF_DATA3_REG, B_TRUE, B_TRUE, PORT_CNT, 0x100},
+  {TXC_SF_DATA4_REG, B_TRUE, B_TRUE, PORT_CNT, 0x100},
+  {TXC_RO_TIDS_REG, B_TRUE, B_TRUE, PORT_CNT, 0x100},
+  {TXC_RO_STATE0_REG, B_TRUE, B_TRUE, PORT_CNT, 0x100},
+  {TXC_RO_STATE1_REG, B_TRUE, B_TRUE, PORT_CNT, 0x100},
+  {TXC_RO_STATE2_REG, B_TRUE, B_TRUE, PORT_CNT, 0x100},
+  {TXC_RO_STATE3_REG, B_TRUE, B_TRUE, PORT_CNT, 0x100},
+  {TXC_RO_CTL_REG, B_TRUE, B_TRUE, PORT_CNT, 0x100},
+  {TXC_RO_ST_DATA0_REG, B_TRUE, B_TRUE, PORT_CNT, 0x100},
+  {TXC_RO_ST_DATA1_REG, B_TRUE, B_TRUE, PORT_CNT, 0x100},
+  {TXC_RO_ST_DATA2_REG, B_TRUE, B_TRUE, PORT_CNT, 0x100},
+  {TXC_RO_ST_DATA3_REG, B_TRUE, B_TRUE, PORT_CNT, 0x100},
+
+  {TXC_INT_STAT_DBG_REG, B_FALSE, B_FALSE, 0, 0},
+  {TXC_INT_STAT_REG, B_FALSE, B_FALSE, 0, 0},
+  {TXC_INT_MASK_REG, B_FALSE, B_FALSE, 0, 0}
+};
+#define NXGE_TXC_REG_LEN	(sizeof(ethtool_txc_reg_table)/sizeof(ethtool_txc_reg_t))
+
+typedef struct ethtool_txdma_reg_s {
+	const uint64_t	offsets;
+	const boolean_t multi;
+	const uint32_t	cnt;
+	const uint32_t	step;
+} ethtool_txdma_reg_t;
+
+static ethtool_txdma_reg_t ethtool_txdma_reg_table[] = {
+
+  {TX_RNG_CFIG_REG, B_TRUE, NXGE_MAX_TDCS, 0x200},
+/*   {TX_RING_HDH_REG, B_TRUE, NXGE_MAX_TDCS, 0x200}, */
+  {TX_RING_HDL_REG, B_TRUE, NXGE_MAX_TDCS, 0x200},
+  {TX_RING_KICK_REG, B_TRUE, NXGE_MAX_TDCS, 0x200},
+  {TX_ENT_MSK_REG, B_TRUE, NXGE_MAX_TDCS, 0x200},
+  {TX_CS_REG, B_TRUE, NXGE_MAX_TDCS, 0x200},
+  {TXDMA_MBH_REG, B_TRUE, NXGE_MAX_TDCS, 0x200},
+  {TXDMA_MBL_REG, B_TRUE, NXGE_MAX_TDCS, 0x200},
+  {TX_DMA_PRE_ST_REG, B_TRUE, NXGE_MAX_TDCS, 0x200},
+  {TX_RNG_ERR_LOGH_REG, B_TRUE, NXGE_MAX_TDCS, 0x200},
+  {TX_RNG_ERR_LOGL_REG, B_TRUE, NXGE_MAX_TDCS, 0x200},
+  {TDMC_INTR_DBG_REG, B_TRUE, NXGE_MAX_TDCS, 0x200},
+  {TX_CS_DBG_REG, B_TRUE, NXGE_MAX_TDCS, 0x200},
+
+  {TX_LOG_PAGE_VLD_REG, B_TRUE, NXGE_MAX_TDCS, 0x200},
+  {TX_LOG_PAGE_MASK1_REG, B_TRUE, NXGE_MAX_TDCS, 0x200},
+  {TX_LOG_PAGE_VAL1_REG, B_TRUE, NXGE_MAX_TDCS, 0x200},
+  {TX_LOG_PAGE_MASK2_REG, B_TRUE, NXGE_MAX_TDCS, 0x200},
+  {TX_LOG_PAGE_VAL2_REG, B_TRUE, NXGE_MAX_TDCS, 0x200},
+  {TX_LOG_PAGE_RELO1_REG, B_TRUE, NXGE_MAX_TDCS, 0x200},
+  {TX_LOG_PAGE_RELO2_REG, B_TRUE, NXGE_MAX_TDCS, 0x200},
+  {TX_LOG_PAGE_HDL_REG, B_TRUE, NXGE_MAX_TDCS, 0x200},
+
+  {TX_ADDR_MD_REG, B_FALSE, 0, 0},
+  {TDMC_INJ_PAR_ERR_REG, B_FALSE, 0, 0},
+  {TDMC_DBG_SEL_REG, B_FALSE, 0, 0},
+  {TDMC_TRAINING_REG, B_FALSE, 0, 0}
+
+};
+#define NXGE_TXDMA_REG_LEN	(sizeof(ethtool_txdma_reg_table)/sizeof(ethtool_txdma_reg_t))
+
+static struct {
+	const uint64_t offsets;
+} ethtool_zcp_reg_table[] = {
+  {ZCP_CONFIG_REG},
+  {ZCP_INT_STAT_REG},
+  {ZCP_INT_STAT_TEST_REG},
+  {ZCP_INT_MASK_REG},
+  {ZCP_BAM4_RE_CTL_REG},
+  {ZCP_BAM8_RE_CTL_REG},
+  {ZCP_BAM16_RE_CTL_REG},
+  {ZCP_BAM32_RE_CTL_REG},
+  {ZCP_DST4_RE_CTL_REG},
+  {ZCP_DST8_RE_CTL_REG},
+  {ZCP_DST16_RE_CTL_REG},
+  {ZCP_DST32_RE_CTL_REG},
+  {ZCP_RAM_DATA_REG},
+  {ZCP_RAM_DATA0_REG},
+  {ZCP_RAM_DATA1_REG},
+  {ZCP_RAM_DATA2_REG},
+  {ZCP_RAM_DATA3_REG},
+  {ZCP_RAM_DATA4_REG},
+  {ZCP_RAM_BE_REG},
+  {ZCP_RAM_ACC_REG},
+  {ZCP_TRAINING_VECTOR_REG},
+  {ZCP_STATE_MACHINE_REG},
+  {ZCP_CHK_BIT_DATA_REG},
+  {ZCP_RESET_CFIFO_REG},
+  {ZCP_CFIFO_ECC_PORT0_REG},
+  {ZCP_CFIFO_ECC_PORT1_REG},
+  {ZCP_CFIFO_ECC_PORT2_REG},
+  {ZCP_CFIFO_ECC_PORT3_REG}
+};
+#define NXGE_ZCP_REG_LEN	(sizeof(ethtool_zcp_reg_table)/sizeof(uint64_t))
+
+
+#define NXGE_STAT_COUNTER(cntr) \
+offsetof(nxge_stats_t, cntr), sizeof(((p_nxge_stats_t)0)->cntr),
+
+
+typedef struct ethtool_nxge_stats_s {
+	const char string[ETH_GSTRING_LEN];
+	uint16_t offset;
+	uint8_t width;
+} ethtool_nxge_stats_t;
+
+static ethtool_nxge_stats_t ethtool_nxge_stats [] = {
+	{"rx_packets", NXGE_STAT_COUNTER(ipackets)},
+	{"rx_errors", NXGE_STAT_COUNTER(ierrors)},
+	{"rx_octets", NXGE_STAT_COUNTER(rbytes)},
+	{"rx_mcast", NXGE_STAT_COUNTER(multircv)},
+	{"rx_bcast", NXGE_STAT_COUNTER(brdcstrcv)},
+	{"rx_jumbo", NXGE_STAT_COUNTER(rx_jumbo_pkts)},
+	{"tx_packets", NXGE_STAT_COUNTER(opackets)},
+	{"tx_errors", NXGE_STAT_COUNTER(oerrors)},
+	{"tx_octets", NXGE_STAT_COUNTER(obytes)},
+	{"tx_mcast", NXGE_STAT_COUNTER(multixmt)},
+	{"tx_bcast", NXGE_STAT_COUNTER(brdcstxmt)},
+	{"tx_jumbo", NXGE_STAT_COUNTER(tx_jumbo_pkts)},
+	{"tx_tso", NXGE_STAT_COUNTER(tx_tso)},
+	{"mac_mtu", NXGE_STAT_COUNTER(mac_stats.mac_mtu)},
+	{"xcvr_inits", NXGE_STAT_COUNTER(mac_stats.xcvr_inits)},
+	{"xcvr_portn", NXGE_STAT_COUNTER(mac_stats.xcvr_portn)},
+	{"xcvr_id", NXGE_STAT_COUNTER(mac_stats.xcvr_id)},
+	{"serdes_inits", NXGE_STAT_COUNTER(mac_stats.serdes_inits)},
+	{"serdes_portn", NXGE_STAT_COUNTER(mac_stats.serdes_portn)},
+	{"cap_autoneg", NXGE_STAT_COUNTER(mac_stats.cap_autoneg)},
+	{"cap_10gfdx", NXGE_STAT_COUNTER(mac_stats.cap_10gfdx)},
+	{"cap_10ghdx", NXGE_STAT_COUNTER(mac_stats.cap_10ghdx)},
+	{"cap_1000fdx", NXGE_STAT_COUNTER(mac_stats.cap_1000fdx)},
+	{"cap_1000hdx", NXGE_STAT_COUNTER(mac_stats.cap_1000hdx)},
+	{"cap_100T4", NXGE_STAT_COUNTER(mac_stats.cap_100T4)},
+	{"cap_100fdx", NXGE_STAT_COUNTER(mac_stats.cap_100fdx)},
+	{"cap_100hdx", NXGE_STAT_COUNTER(mac_stats.cap_100hdx)},
+	{"cap_10fdx", NXGE_STAT_COUNTER(mac_stats.cap_10fdx)},
+	{"cap_10hdx", NXGE_STAT_COUNTER(mac_stats.cap_10hdx)},
+	{"cap_asmpause", NXGE_STAT_COUNTER(mac_stats.cap_asmpause)},
+	{"cap_pause", NXGE_STAT_COUNTER(mac_stats.cap_pause)},
+	{"adv_cap_autoneg", NXGE_STAT_COUNTER(mac_stats.adv_cap_autoneg)},
+	{"adv_cap_10gfdx", NXGE_STAT_COUNTER(mac_stats.adv_cap_10gfdx)},
+	{"adv_cap_10ghdx", NXGE_STAT_COUNTER(mac_stats.adv_cap_10ghdx)},
+	{"adv_cap_1000fdx", NXGE_STAT_COUNTER(mac_stats.adv_cap_1000fdx)},
+	{"adv_cap_1000hdx", NXGE_STAT_COUNTER(mac_stats.adv_cap_1000hdx)},
+	{"adv_cap_100T4", NXGE_STAT_COUNTER(mac_stats.adv_cap_100T4)},
+	{"adv_cap_100fdx", NXGE_STAT_COUNTER(mac_stats.adv_cap_100fdx)},
+	{"adv_cap_100hdx", NXGE_STAT_COUNTER(mac_stats.adv_cap_100hdx)},
+	{"adv_cap_10fdx", NXGE_STAT_COUNTER(mac_stats.adv_cap_10fdx)},
+	{"adv_cap_10hdx", NXGE_STAT_COUNTER(mac_stats.adv_cap_10hdx)},
+	{"adv_cap_asmpause", NXGE_STAT_COUNTER(mac_stats.adv_cap_asmpause)},
+	{"adv_cap_pause", NXGE_STAT_COUNTER(mac_stats.adv_cap_pause)},
+	{"lp_cap_autoneg", NXGE_STAT_COUNTER(mac_stats.lp_cap_autoneg)},
+	{"lp_cap_10gfdx", NXGE_STAT_COUNTER(mac_stats.lp_cap_10gfdx)},
+	{"lp_cap_10ghdx", NXGE_STAT_COUNTER(mac_stats.lp_cap_10ghdx)},
+	{"lp_cap_1000fdx", NXGE_STAT_COUNTER(mac_stats.lp_cap_1000fdx)},
+	{"lp_cap_1000hdx", NXGE_STAT_COUNTER(mac_stats.lp_cap_1000hdx)},
+	{"lp_cap_100T4", NXGE_STAT_COUNTER(mac_stats.lp_cap_100T4)},
+	{"lp_cap_100fdx", NXGE_STAT_COUNTER(mac_stats.lp_cap_100fdx)},
+	{"lp_cap_100hdx", NXGE_STAT_COUNTER(mac_stats.lp_cap_100hdx)},
+	{"lp_cap_10fdx", NXGE_STAT_COUNTER(mac_stats.lp_cap_10fdx)},
+	{"lp_cap_10hdx", NXGE_STAT_COUNTER(mac_stats.lp_cap_10hdx)},
+	{"lp_cap_asmpause", NXGE_STAT_COUNTER(mac_stats.lp_cap_asmpause)},
+	{"lp_cap_pause", NXGE_STAT_COUNTER(mac_stats.lp_cap_pause)},
+	{"link_T4", NXGE_STAT_COUNTER(mac_stats.link_T4)},
+	{"link_speed", NXGE_STAT_COUNTER(mac_stats.link_speed)},
+	{"link_duplex", NXGE_STAT_COUNTER(mac_stats.link_duplex)},
+	{"link_asmpause", NXGE_STAT_COUNTER(mac_stats.link_asmpause)},
+	{"link_pause", NXGE_STAT_COUNTER(mac_stats.link_pause)},
+	{"link_up", NXGE_STAT_COUNTER(mac_stats.link_up)},
+
+	{"norcvbuf", NXGE_STAT_COUNTER(norcvbuf)},
+	{"noxmtbuf", NXGE_STAT_COUNTER(noxmtbuf)},
+	{"lb_mode", NXGE_STAT_COUNTER(lb_mode)},
+	{"poll_mode", NXGE_STAT_COUNTER(poll_mode)},
+	{"tx_inits", NXGE_STAT_COUNTER(tx_inits)},
+	{"tx_starts", NXGE_STAT_COUNTER(tx_starts)},
+	{"tx_no_desc", NXGE_STAT_COUNTER(tx_no_desc)},
+	{"tx_hdr_pkts", NXGE_STAT_COUNTER(tx_hdr_pkts)},
+	{"tx_skb_errs", NXGE_STAT_COUNTER(tx_skb_errs)},
+	{"rx_inits", NXGE_STAT_COUNTER(rx_inits)},
+	{"rx_hdr_pkts", NXGE_STAT_COUNTER(rx_hdr_pkts)},
+	{"rx_mtu_pkts", NXGE_STAT_COUNTER(rx_mtu_pkts)},
+	{"rx_split_pkts", NXGE_STAT_COUNTER(rx_split_pkts)},
+	{"rx_no_buf", NXGE_STAT_COUNTER(rx_no_buf)},
+	{"rx_ov_flow", NXGE_STAT_COUNTER(rx_ov_flow)},
+	{"rx_len_mm", NXGE_STAT_COUNTER(rx_len_mm)},
+	{"rx_tag_err", NXGE_STAT_COUNTER(rx_tag_err)},
+	{"rx_pkts_drop", NXGE_STAT_COUNTER(rx_pkts_dropped)},
+
+/* ipp stats */
+	{"ipp_errors", NXGE_STAT_COUNTER(ipp_stats.errors)},
+	{"ipp_inits", NXGE_STAT_COUNTER(ipp_stats.inits)},
+	{"ipp_discards", NXGE_STAT_COUNTER(ipp_stats.pkt_dis_cnt)},
+	{"ipp_sop_miss", NXGE_STAT_COUNTER(ipp_stats.sop_miss)},
+	{"ipp_eop_miss", NXGE_STAT_COUNTER(ipp_stats.eop_miss)},
+	{"ipp_dfifo_ue", NXGE_STAT_COUNTER(ipp_stats.dfifo_ue)},
+	{"ipp_pfifo_perr", NXGE_STAT_COUNTER(ipp_stats.pfifo_perr)},
+	{"ipp_ecc_err_max", NXGE_STAT_COUNTER(ipp_stats.ecc_err_cnt)},
+	{"ipp_pfifo_over", NXGE_STAT_COUNTER(ipp_stats.pfifo_over)},
+	{"ipp_pfifo_und", NXGE_STAT_COUNTER(ipp_stats.pfifo_und)},
+	{"ipp_bad_cs_cnt", NXGE_STAT_COUNTER(ipp_stats.bad_cs_cnt)},
+
+/* zcp stats */
+	{"zcp_errors", NXGE_STAT_COUNTER(zcp_stats.errors)},
+	{"zcp_inits", NXGE_STAT_COUNTER(zcp_stats.inits)},
+	{"zcp_rfifo_underrun", NXGE_STAT_COUNTER(zcp_stats.rrfifo_underrun)},
+	{"zcp_rfifo_overrun", NXGE_STAT_COUNTER(zcp_stats.rrfifo_overrun)},
+	{"zcp_rspfifo_ue", NXGE_STAT_COUNTER(zcp_stats.rspfifo_uncorr_err)},
+	{"zcp_cfifo_ecc", NXGE_STAT_COUNTER(zcp_stats.cfifo_ecc)},
+
+
+	{"rdc_sys_pre_par", NXGE_STAT_COUNTER(rdc_sys_stats.pre_par)},
+	{"rdc_sys_sha_par", NXGE_STAT_COUNTER(rdc_sys_stats.sha_par)},
+	{"rdc_sys_id_mismatch", NXGE_STAT_COUNTER(rdc_sys_stats.id_mismatch)},
+	{"rdc_sys_ipp_eop_err", NXGE_STAT_COUNTER(rdc_sys_stats.ipp_eop_err)},
+	{"rdc_sys_zcp_eop_err", NXGE_STAT_COUNTER(rdc_sys_stats.zcp_eop_err)},
+
+	{"txc_pkt_stuffed", NXGE_STAT_COUNTER(txc_stats.pkt_stuffed)},
+	{"txc_pkt_xmit", NXGE_STAT_COUNTER(txc_stats.pkt_xmit)},
+	{"txc_ro_correct_err", NXGE_STAT_COUNTER(txc_stats.ro_correct_err)},
+	{"txc_ro_uncorrect_err", NXGE_STAT_COUNTER(txc_stats.ro_uncorrect_err)},
+	{"txc_sf_correct_err", NXGE_STAT_COUNTER(txc_stats.sf_correct_err)},
+	{"txc_sf_uncorrect_err", NXGE_STAT_COUNTER(txc_stats.sf_uncorrect_err)},
+	{"txc_address_failed", NXGE_STAT_COUNTER(txc_stats.address_failed)},
+	{"txc_dma_failed", NXGE_STAT_COUNTER(txc_stats.dma_failed)},
+	{"txc_length_failed", NXGE_STAT_COUNTER(txc_stats.length_failed)},
+	{"txc_pkt_assy_dead", NXGE_STAT_COUNTER(txc_stats.pkt_assy_dead)},
+	{"txc_reorder_err", NXGE_STAT_COUNTER(txc_stats.reorder_err)},
+
+	{"fflp_tcam_entries", NXGE_STAT_COUNTER(fflp_stats.tcam_entries)},
+	{"fflp_fcram_entries", NXGE_STAT_COUNTER(fflp_stats.fcram_entries)},
+	{"fflp_tcam_parity_err", NXGE_STAT_COUNTER(fflp_stats.tcam_parity_err)},
+	{"fflp_tcam_ecc_err", NXGE_STAT_COUNTER(fflp_stats.tcam_ecc_err)},
+	{"fflp_vlan_parity_err", NXGE_STAT_COUNTER(fflp_stats.vlan_parity_err)},
+	{"fflp_hash_lookup_err", NXGE_STAT_COUNTER(fflp_stats.hash_lookup_err)}
+};
+
+#define NXGE_STAT_NAMES_CNT (sizeof(ethtool_nxge_stats)/sizeof(ethtool_nxge_stats_t))
+
+static ethtool_nxge_stats_t ethtool_nxge_xmac_stats [] = {
+	{"tx_frame_cnt", NXGE_STAT_COUNTER(xmac_stats.tx_frame_cnt)},
+	{"tx_byte_cnt", NXGE_STAT_COUNTER(xmac_stats.tx_byte_cnt)},
+	{"tx_underflow_err", NXGE_STAT_COUNTER(xmac_stats.tx_underflow_err)},
+	{"tx_maxpktsize_err", NXGE_STAT_COUNTER(xmac_stats.tx_maxpktsize_err)},
+	{"tx_overflow_err", NXGE_STAT_COUNTER(xmac_stats.tx_overflow_err)},
+	{"tx_fifo_xfr_err", NXGE_STAT_COUNTER(xmac_stats.tx_fifo_xfr_err)},
+	{"rx_frame_cnt", NXGE_STAT_COUNTER(xmac_stats.rx_frame_cnt)},
+	{"rx_byte_cnt", NXGE_STAT_COUNTER(xmac_stats.rx_byte_cnt)},
+	{"rx_hist1_cnt", NXGE_STAT_COUNTER(xmac_stats.rx_hist1_cnt)},
+	{"rx_hist2_cnt", NXGE_STAT_COUNTER(xmac_stats.rx_hist2_cnt)},
+	{"rx_hist3_cnt", NXGE_STAT_COUNTER(xmac_stats.rx_hist3_cnt)},
+	{"rx_hist4_cnt", NXGE_STAT_COUNTER(xmac_stats.rx_hist4_cnt)},
+	{"rx_hist5_cnt", NXGE_STAT_COUNTER(xmac_stats.rx_hist5_cnt)},
+	{"rx_hist6_cnt", NXGE_STAT_COUNTER(xmac_stats.rx_hist6_cnt)},
+	{"rx_hist7_cnt", NXGE_STAT_COUNTER(xmac_stats.rx_hist7_cnt)},
+	{"rx_broadcast_cnt", NXGE_STAT_COUNTER(xmac_stats.rx_broadcast_cnt)},
+	{"rx_mult_cnt", NXGE_STAT_COUNTER(xmac_stats.rx_mult_cnt)},
+	{"rx_underflow_err", NXGE_STAT_COUNTER(xmac_stats.rx_underflow_err)},
+	{"rx_overflow_err", NXGE_STAT_COUNTER(xmac_stats.rx_overflow_err)},
+	{"rx_crc_err_cnt", NXGE_STAT_COUNTER(xmac_stats.rx_crc_err_cnt)},
+	{"rx_len_err_cnt", NXGE_STAT_COUNTER(xmac_stats.rx_len_err_cnt)},
+	{"rx_viol_err_cnt", NXGE_STAT_COUNTER(xmac_stats.rx_viol_err_cnt)},
+	{"rx_frag_cnt", NXGE_STAT_COUNTER(xmac_stats.rx_frag_cnt)},
+	{"rx_frame_align_err_cnt", NXGE_STAT_COUNTER(xmac_stats.rx_frame_align_err_cnt)},
+	{"rx_linkfault_err_cnt", NXGE_STAT_COUNTER(xmac_stats.rx_linkfault_err_cnt)},
+	{"rx_remotefault_err", NXGE_STAT_COUNTER(xmac_stats.rx_remotefault_err)},
+	{"rx_localfault_err", NXGE_STAT_COUNTER(xmac_stats.rx_localfault_err)},
+	{"rx_pause_cnt", NXGE_STAT_COUNTER(xmac_stats.rx_pause_cnt)},
+	{"tx_pause_state", NXGE_STAT_COUNTER(xmac_stats.tx_pause_state)},
+	{"tx_nopause_state", NXGE_STAT_COUNTER(xmac_stats.tx_nopause_state)}
+};
+
+#define NXGE_XMAC_STAT_NAMES_CNT (sizeof(ethtool_nxge_xmac_stats)/sizeof(ethtool_nxge_stats_t))
+
+static ethtool_nxge_stats_t ethtool_nxge_bmac_stats [] = {
+	{"tx_frame_cnt", NXGE_STAT_COUNTER(bmac_stats.tx_frame_cnt)},
+	{"tx_byte_cnt", NXGE_STAT_COUNTER(bmac_stats.tx_byte_cnt)},
+	{"tx_underrun_err", NXGE_STAT_COUNTER(bmac_stats.tx_underrun_err)},
+	{"tx_max_pkt_err", NXGE_STAT_COUNTER(bmac_stats.tx_max_pkt_err)},
+	{"rx_frame_cnt", NXGE_STAT_COUNTER(bmac_stats.rx_frame_cnt)},
+	{"rx_byte_cnt", NXGE_STAT_COUNTER(bmac_stats.rx_byte_cnt)},
+	{"rx_overflow_err", NXGE_STAT_COUNTER(bmac_stats.rx_overflow_err)},
+	{"rx_align_err_cnt", NXGE_STAT_COUNTER(bmac_stats.rx_align_err_cnt)},
+	{"rx_crc_err_cnt", NXGE_STAT_COUNTER(bmac_stats.rx_crc_err_cnt)},
+	{"rx_len_err_cnt", NXGE_STAT_COUNTER(bmac_stats.rx_len_err_cnt)},
+	{"rx_viol_err_cnt", NXGE_STAT_COUNTER(bmac_stats.rx_viol_err_cnt)},
+	{"rx_pause_cnt", NXGE_STAT_COUNTER(bmac_stats.rx_pause_cnt)},
+	{"tx_pause_state", NXGE_STAT_COUNTER(bmac_stats.tx_pause_state)},
+	{"tx_nopause_state", NXGE_STAT_COUNTER(bmac_stats.tx_nopause_state)}
+};
+
+#define NXGE_BMAC_STAT_NAMES_CNT (sizeof(ethtool_nxge_bmac_stats)/sizeof(ethtool_nxge_stats_t))
+
+#define NXGE_RDC_STAT(rxst) \
+offsetof(nxge_rx_ring_stats_t, rxst), sizeof(((p_nxge_rx_ring_stats_t)0)->rxst)
+
+static ethtool_nxge_stats_t ethtool_nxge_rdc_stats [] = {
+	{"\n\nRX_Channel#", NXGE_RDC_STAT(channel_num)},
+	{"rx_packets", NXGE_RDC_STAT(ipackets)},
+	{"rx_bytes", NXGE_RDC_STAT(ibytes)},
+	{"rx_errors", NXGE_RDC_STAT(ierrors)},
+	{"rx_mcast", NXGE_RDC_STAT(multircv)},
+	{"rx_bcast", NXGE_RDC_STAT(brdcstrcv)},
+	{"rx_no_buf", NXGE_RDC_STAT(rx_no_buf)},
+	{"rx_jumbo", NXGE_RDC_STAT(rx_jumbo_pkts)},
+	{"rx_bufsz0", NXGE_RDC_STAT(rx_rcvd_bufsz[0])},
+	{"rx_bufsz1", NXGE_RDC_STAT(rx_rcvd_bufsz[1])},
+	{"rx_bufsz2", NXGE_RDC_STAT(rx_rcvd_bufsz[2])},
+	{"rx_sgl_blk", NXGE_RDC_STAT(rx_rcvd_bufsz[3])},
+	{"rx_rbr_tmout", NXGE_RDC_STAT(rx_rbr_tmout)},
+	{"rx_l2_err", NXGE_RDC_STAT(l2_err)},
+	{"rx_l4_cksum_err", NXGE_RDC_STAT(l4_cksum_err)},
+	{"rx_fflp_soft_err", NXGE_RDC_STAT(fflp_soft_err)},
+	{"rx_zcp_soft_err", NXGE_RDC_STAT(zcp_soft_err)},
+	{"rx_dcf_err", NXGE_RDC_STAT(dcf_err)},
+	{"rx_rbr_tmout", NXGE_RDC_STAT(rbr_tmout)},
+	{"rx_rsp_cnt_err", NXGE_RDC_STAT(rsp_cnt_err)},
+	{"rx_byte_en_err", NXGE_RDC_STAT(byte_en_err)},
+	{"rx_byte_en_bus", NXGE_RDC_STAT(byte_en_bus)},
+	{"rx_rsp_dat_err", NXGE_RDC_STAT(rsp_dat_err)},
+	{"rx_rcr_ack_err", NXGE_RDC_STAT(rcr_ack_err)},
+	{"rx_dc_fifo_err", NXGE_RDC_STAT(dc_fifo_err)},
+	{"rx_rcr_sha_par", NXGE_RDC_STAT(rcr_sha_par)},
+	{"rx_rbr_pre_par", NXGE_RDC_STAT(rbr_pre_par)},
+	{"rx_port_drop_pkt", NXGE_RDC_STAT(port_drop_pkt)},
+	{"rx_wred_drop", NXGE_RDC_STAT(wred_drop)},
+	{"rx_rbr_pre_empty", NXGE_RDC_STAT(rbr_pre_empty)},
+	{"rx_rcr_shadow_full", NXGE_RDC_STAT(rcr_shadow_full)},
+	{"rx_config_err", NXGE_RDC_STAT(config_err)},
+	{"rx_rcrincon", NXGE_RDC_STAT(rcrincon)},
+	{"rx_rcrfull", NXGE_RDC_STAT(rcrfull)},
+	{"rx_rbr_empty", NXGE_RDC_STAT(rbr_empty)},
+	{"rx_rbrfull", NXGE_RDC_STAT(rbrfull)},
+	{"rx_rbrlogpage", NXGE_RDC_STAT(rbrlogpage)},
+	{"rx_cfiglogpage", NXGE_RDC_STAT(cfiglogpage)}
+};
+
+#define NXGE_RDC_STAT_NAMES_CNT (sizeof(ethtool_nxge_rdc_stats)/sizeof(ethtool_nxge_stats_t))
+
+#define NXGE_TDC_STAT(txst) \
+offsetof(nxge_tx_ring_stats_t, txst), offsetof(nxge_tx_ring_stats_t, txst)
+
+static ethtool_nxge_stats_t ethtool_nxge_tdc_stats [] = {
+	{"\n\nTX_Channel#", NXGE_TDC_STAT(channel_num)},
+	{"tx_ring_size", NXGE_TDC_STAT(ring_size)},
+	{"tx_packets", NXGE_TDC_STAT(opackets)},
+	{"tx_bytes", NXGE_TDC_STAT(obytes)},
+	{"tx_errors", NXGE_TDC_STAT(oerrors)},
+	{"tx_mbox_err", NXGE_TDC_STAT(mbox_err)},
+	{"tx_pkt_size_err", NXGE_TDC_STAT(pkt_size_err)},
+	{"tx_ring_oflow", NXGE_TDC_STAT(tx_ring_oflow)},
+	{"tx_no_buf", NXGE_TDC_STAT(tx_no_buf)},
+	{"tx_no_desc", NXGE_TDC_STAT(tx_no_desc)},
+	{"tx_pre_buf_par_err", NXGE_TDC_STAT(pre_buf_par_err)},
+	{"tx_nack_pref", NXGE_TDC_STAT(nack_pref)},
+	{"tx_nack_pkt_rd", NXGE_TDC_STAT(nack_pkt_rd)},
+	{"tx_conf_part_err", NXGE_TDC_STAT(conf_part_err)},
+	{"tx_pkt_part_err", NXGE_TDC_STAT(pkt_part_err)},
+	{"tx_tx_intr", NXGE_TDC_STAT(tx_intr)},
+	{"tx_tx_blocked", NXGE_TDC_STAT(tx_blocked)},
+	{"tx_unknown_err", NXGE_TDC_STAT(tx_unknown_fail)},
+	{"tx_descs_pending", NXGE_TDC_STAT(tx_descs_pending)},
+	{"tx_tso_packets", NXGE_TDC_STAT(tx_packets_tso)},
+	{"tx_tso_no_buff", NXGE_TDC_STAT(tx_tso_nobuff)},
+	{"tx_tso_no_desc", NXGE_TDC_STAT(tx_tso_no_desc)},
+	{"tx_jumbo", NXGE_TDC_STAT(jumbo)},
+
+#ifdef NXGE_DEBUG_INFO
+	{"tx_stk_tx_pkts", NXGE_TDC_STAT(stk_tx_pkts)},
+	{"tx_frag0", NXGE_TDC_STAT(frag0)},
+	{"tx_frag1", NXGE_TDC_STAT(frag1)},
+	{"tx_frag2", NXGE_TDC_STAT(frag2)},
+	{"tx_frag3", NXGE_TDC_STAT(frag3)},
+	{"tx_frag4", NXGE_TDC_STAT(frag4)},
+	{"tx_frag5", NXGE_TDC_STAT(frag5)},
+	{"tx_frag_many", NXGE_TDC_STAT(frag_many)},
+	{"tx_cksum", NXGE_TDC_STAT(cksum)},
+	{"tx_descs_kicked", NXGE_TDC_STAT(descs_kicked)}
+#endif
+};
+
+#define NXGE_TDC_STAT_NAMES_CNT (sizeof(ethtool_nxge_tdc_stats)/sizeof(ethtool_nxge_stats_t))
+
+
+
+static int
+nxge_get_settings(struct net_device *dev, struct ethtool_cmd *ecmd)
+{
+	p_nxge_t	nep = netdev_priv(dev);
+	unsigned long	flags;
+	p_nxge_stats_t	statsp = nep->statsp;
+	nxge_status_t	status = NXGE_OK;
+
+	ecmd->transceiver = XCVR_INTERNAL;
+	ecmd->phy_address = nep->statsp->mac_stats.xcvr_portn;
+	ecmd->advertising = 0;
+	ecmd->supported = 0;
+	if (statsp->mac_stats.cap_10gfdx) {
+		ecmd->supported |= SUPPORTED_10000baseT_Full;
+		ecmd->advertising |= ADVERTISED_10000baseT_Full;
+	}
+
+	/* Record PHY settings if HW is on. */
+	spin_lock_irqsave(&nep->lock, flags);
+
+
+	switch (nep->mac.portmode) {
+	case PORT_10G_FIBER:
+		ecmd->port = PORT_FIBRE;
+		ecmd->supported   |= SUPPORTED_FIBRE;
+		ecmd->advertising |= ADVERTISED_FIBRE;
+		status = nxge_check_10g_link(nep);
+		break;
+	case PORT_1G_COPPER:
+
+		ecmd->port = PORT_MII;
+		ecmd->advertising |= (ADVERTISED_TP | ADVERTISED_MII |
+				     ADVERTISED_10baseT_Full |
+				     ADVERTISED_100baseT_Full |
+				     ADVERTISED_1000baseT_Full |
+				      ADVERTISED_Autoneg);
+
+		ecmd->supported |= (SUPPORTED_TP | SUPPORTED_MII |
+				   SUPPORTED_10baseT_Full |
+				   SUPPORTED_100baseT_Full |
+				   SUPPORTED_1000baseT_Full |
+				    SUPPORTED_Autoneg);
+
+		status = nxge_check_mii_link(nep);
+		break;
+
+	case PORT_1G_FIBER:
+	default:
+		break;
+	}
+
+	spin_unlock_irqrestore(&nep->lock, flags);
+
+	if (statsp->mac_stats.adv_cap_autoneg) {
+		ecmd->advertising |= ADVERTISED_Autoneg;
+	}
+
+	if (statsp->mac_stats.cap_autoneg) {
+		ecmd->autoneg = AUTONEG_ENABLE;
+	} else {
+		ecmd->autoneg = AUTONEG_DISABLE;
+	}
+
+	switch (nep->statsp->mac_stats.link_speed) {
+	case 10000:
+		ecmd->speed = SPEED_10000;
+		break;
+	case 1000:
+		ecmd->speed = SPEED_1000;
+		break;
+	case 100:
+		ecmd->speed = SPEED_100;
+		break;
+	case 10:
+		ecmd->speed = SPEED_10;
+		break;
+	default:
+		ecmd->speed = 0;
+		break;
+	}
+
+	switch (nep->statsp->mac_stats.link_duplex) {
+	case 1:
+		ecmd->duplex = DUPLEX_HALF;
+		break;
+	case 2:
+		ecmd->duplex = DUPLEX_FULL;
+		break;
+	default:
+		ecmd->duplex = 0xff;
+		break;
+	}
+
+	return 0;
+}
+
+
+static int
+nxge_set_settings(struct net_device *dev, struct ethtool_cmd *ecmd)
+{
+	p_nxge_t	nep = netdev_priv(dev);
+	unsigned long	flags;
+	nxge_status_t status = NXGE_OK;
+	p_nxge_param_t	param_arr = nep->param_arr;
+	boolean_t changed = 0;
+
+	if (!capable(CAP_NET_ADMIN))
+		return -EPERM;
+
+	if (ecmd->duplex != DUPLEX_FULL)
+		return -EINVAL;
+
+	if (nep->mac.portmode == PORT_1G_COPPER) {
+		/* Verify the settings we care about. */
+		if (ecmd->autoneg != AUTONEG_ENABLE &&
+		    ecmd->autoneg != AUTONEG_DISABLE)
+			return -EINVAL;
+
+		if (ecmd->autoneg == AUTONEG_DISABLE &&
+		    ((ecmd->speed != SPEED_1000) &&
+		     (ecmd->speed != SPEED_100) &&
+		     (ecmd->speed != SPEED_10)))
+			return -EINVAL;
+	} else {
+		if (ecmd->autoneg == AUTONEG_ENABLE)
+			return -EINVAL;
+		if (nep->mac.portmode == PORT_10G_FIBER) {
+			if (ecmd->speed != SPEED_10000)
+				return -EINVAL;
+		} else if (nep->mac.portmode == PORT_1G_FIBER) {
+			if (ecmd->speed != SPEED_1000)
+				return -EINVAL;
+		}
+
+		return 0;
+	}
+
+	/* Apply settings and restart link process. */
+	spin_lock_irqsave(&nep->lock, flags);
+	nep->ep = ecmd;
+	if (nep->mac.portmode == PORT_1G_COPPER){
+
+		param_arr[param_anar_100T4].old_value =
+			param_arr[param_anar_100T4].value;
+
+		param_arr[param_autoneg].old_value =
+			param_arr[param_autoneg].value;
+
+		if (ecmd->autoneg == AUTONEG_ENABLE) {
+			param_arr[param_autoneg].value = 1;
+			if (param_arr[param_autoneg].value !=
+				param_arr[param_autoneg].old_value)
+				changed++;
+			param_arr[param_anar_1000fdx].old_value =
+				param_arr[param_anar_100fdx].value;
+
+			param_arr[param_anar_1000fdx].old_value =
+				param_arr[param_anar_100fdx].value;
+
+			param_arr[param_anar_10fdx].old_value =
+				param_arr[param_anar_10fdx].value;
+			param_arr[param_anar_pause].old_value =
+				param_arr[param_anar_pause].value;
+			param_arr[param_anar_asmpause].value =
+				param_arr[param_anar_asmpause].value;
+
+			param_arr[param_anar_1000fdx].value = 1;
+			param_arr[param_anar_100fdx].value = 1;
+			param_arr[param_anar_10fdx].value = 1;
+
+		}
+
+		if (ecmd->autoneg == AUTONEG_DISABLE) {
+			param_arr[param_autoneg].value = 0;
+			if (param_arr[param_autoneg].value !=
+				param_arr[param_autoneg].old_value)
+				changed++;
+
+			param_arr[param_anar_1000fdx].old_value =
+				param_arr[param_anar_100fdx].value;
+
+			param_arr[param_anar_1000fdx].old_value =
+				param_arr[param_anar_100fdx].value;
+
+			param_arr[param_anar_10fdx].old_value =
+				param_arr[param_anar_10fdx].value;
+			param_arr[param_anar_pause].old_value =
+				param_arr[param_anar_pause].value;
+			param_arr[param_anar_asmpause].value =
+				param_arr[param_anar_asmpause].value;
+
+			param_arr[param_anar_1000fdx].value = 0;
+			param_arr[param_anar_100fdx].value = 0;
+			param_arr[param_anar_10fdx].value = 0;
+
+			if (ecmd->speed == SPEED_1000) {
+				param_arr[param_anar_1000fdx].value = 1;
+				if (param_arr[param_anar_1000fdx].value !=
+					param_arr[param_anar_1000fdx].old_value)
+					changed++;
+			}
+			if (ecmd->speed == SPEED_100) {
+				param_arr[param_anar_100fdx].value = 1;
+				if (param_arr[param_anar_100fdx].value !=
+					param_arr[param_anar_100fdx].old_value)
+					changed++;
+			}
+			if (ecmd->speed == SPEED_10) {
+				param_arr[param_anar_10fdx].value = 1;
+				if (param_arr[param_anar_10fdx].value !=
+					param_arr[param_anar_10fdx].old_value)
+					changed++;
+			}
+		}
+	}
+	if (changed)
+	status = nxge_link_init(nep);
+	if (status != NXGE_OK) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,"Ethertool - failed to "
+				"initialize link"));
+		goto exit;
+	}
+
+	status = nxge_mac_init(nep);
+	if (status != NXGE_OK) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "Ethertool - failed to "
+				"initialize MAC"));
+		goto exit;
+	}
+
+  exit:
+	nep->ep = NULL;
+	spin_unlock_irqrestore(&nep->lock, flags);
+	return 0;
+}
+
+static void
+nxge_get_drvinfo(struct net_device *dev, struct ethtool_drvinfo *info)
+{
+	p_nxge_t	nep = netdev_priv(dev);
+
+	strcpy(info->driver,  DRV_MODULE_NAME);
+	strcpy(info->version, DRV_MODULE_VERSION);
+	strcpy(info->fw_version, nep->vpd_info.ver);
+	strcpy(info->bus_info, pci_name(nep->pdev));
+}
+
+/* NOTE: Both get_regs_len and get_regs need to be supported together */
+static int
+nxge_get_regs_len(struct net_device *dev)
+{
+	unsigned long	flags;
+	uint64_t	reg_cnt = 0;
+	p_nxge_t	nep = netdev_priv(dev);
+	int		i;
+	nxge_port_t	portt;
+	uint8_t		portn;
+	int		reg_len = 0;
+
+	spin_lock_irqsave(&nep->lock, flags);
+
+	portt = nep->mac.porttype;
+	portn = nep->mac.portnum;
+
+	if (portt == PORT_TYPE_XMAC) {
+		/* XMAC Registers */
+		reg_cnt += NXGE_XMAC_REG_LEN;
+	} else {
+		/* BMAC Registers */
+		reg_cnt += NXGE_BMAC_REG_LEN;
+	}
+
+	if (nep->statsp->mac_stats.xcvr_inuse == INT_MII_XCVR ||
+	    nep->statsp->mac_stats.xcvr_inuse == EXT_MII_XCVR) {
+		/* MIF Registers */
+		reg_cnt += NXGE_MIF_REG_LEN;
+	} else if (nep->statsp->mac_stats.xcvr_inuse == PCS_XCVR) {
+		if (portn == 2 || portn == 3) {
+			/* PCS Registers */
+			reg_cnt += NXGE_PCS_REG_LEN;
+		} else {
+			/* XPCS Registers */
+			reg_cnt += NXGE_XPCS_REG_LEN;
+		}
+	}
+
+	spin_unlock_irqrestore(&nep->lock, flags);
+
+	/* IPP Registers */
+	reg_cnt += NXGE_IPP_REG_LEN;
+
+	/* RxDMA Registers */
+	for (i = 0; i < NXGE_RXDMA_REG_LEN; i++) {
+		if (ethtool_rxdma_reg_table[i].multi) {
+			reg_cnt += ethtool_rxdma_reg_table[i].cnt;
+		} else {
+			reg_cnt++;
+		}
+	}
+#if 0
+	/* FFLP Registers */
+	for (i = 0; i < NXGE_FFLP_REG_LEN; i++) {
+		if (ethtool_fflp_reg_table[i].multi) {
+			reg_cnt += ethtool_fflp_reg_table[i].cnt;
+		} else {
+			reg_cnt++;
+		}
+	}
+
+	/* ESPC Registers */
+	for (i = 0; i < NXGE_ESPC_REG_LEN; i++) {
+		if (ethtool_espc_reg_table[i].multi) {
+			reg_cnt += ethtool_espc_reg_table[i].cnt;
+		} else {
+			reg_cnt++;
+		}
+	}
+#endif
+	/* TXC Registers */
+	for (i = 0; i < NXGE_TXC_REG_LEN; i++) {
+		if (ethtool_txc_reg_table[i].multi &&
+		    !ethtool_txc_reg_table[i].pport) {
+			reg_cnt += ethtool_txc_reg_table[i].cnt;
+		} else {
+			reg_cnt++;
+		}
+	}
+
+	/* TxDMA Registers */
+	for (i = 0; i < NXGE_TXDMA_REG_LEN; i++) {
+		if (ethtool_txdma_reg_table[i].multi) {
+			reg_cnt += ethtool_txdma_reg_table[i].cnt;
+		} else {
+			reg_cnt++;
+		}
+	}
+
+	/* ZeroCopy Registers */
+	reg_cnt += NXGE_ZCP_REG_LEN;
+
+	reg_len = reg_cnt * sizeof (uint64_t);
+
+	return (reg_len);
+}
+
+static void
+nxge_get_regs(struct net_device *dev, struct ethtool_regs *edata, void *p)
+{
+	unsigned long	flags;
+	p_nxge_t	nep = netdev_priv(dev);
+	uint64_t	*reg = (uint64_t *) p;
+	uint64_t	*reg_start = reg;
+	int		i;
+	nxge_port_t	portt;
+	uint8_t		portn;
+	npi_handle_t	handle;
+
+	edata->version = 0;
+
+	spin_lock_irqsave(&nep->lock, flags);
+
+	portt = nep->mac.porttype;
+	portn = nep->mac.portnum;
+	handle = nep->npi_handle;
+
+	if (portt == PORT_TYPE_XMAC) {
+		/* XMAC Registers */
+		for (i = 0; i < NXGE_XMAC_REG_LEN; i++, reg++) {
+			XMAC_REG_RD(handle, portn,
+				    ethtool_xmac_reg_table[i].offsets, reg);
+		}
+	} else {
+		/* BMAC Registers */
+		for (i = 0; i < NXGE_BMAC_REG_LEN; i++, reg++) {
+			BMAC_REG_RD(handle, portn,
+				    ethtool_bmac_reg_table[i].offsets, reg);
+		}
+	}
+
+	if (nep->statsp->mac_stats.xcvr_inuse == INT_MII_XCVR ||
+	    nep->statsp->mac_stats.xcvr_inuse == EXT_MII_XCVR) {
+		/* MIF Registers */
+		for (i = 0; i < NXGE_MIF_REG_LEN; i++, reg++) {
+			MIF_REG_RD(handle, ethtool_mif_reg_table[i].offsets,
+				   reg);
+		}
+	} else if (nep->statsp->mac_stats.xcvr_inuse == PCS_XCVR) {
+		if (portn == 2 || portn == 3) {
+			/* PCS Registers */
+			for (i = 0; i < NXGE_PCS_REG_LEN; i++, reg++) {
+				PCS_REG_RD(handle, portn,
+					   ethtool_pcs_reg_table[i].offsets,
+					   reg);
+			}
+		} else {
+/* 	} else if (nep->statsp->mac_stats.xcvr_inuse == XPCS_XCVR) { */
+		/* XPCS Registers */
+			for (i = 0; i < NXGE_XPCS_REG_LEN; i++, reg++) {
+				XPCS_REG_RD(handle, portn,
+					    ethtool_xpcs_reg_table[i].offsets,
+					    reg);
+			}
+		}
+	}
+
+	/* IPP Registers */
+	for (i = 0; i < NXGE_IPP_REG_LEN; i++, reg++) {
+		NXGE_REG_RD64(handle,
+		       IPP_REG_ADDR(portn, ethtool_ipp_reg_table[i].offsets),
+			      reg);
+	}
+
+	/* RxDMA Registers */
+	for (i = 0; i < NXGE_RXDMA_REG_LEN; i++) {
+
+		if (ethtool_rxdma_reg_table[i].multi) {
+			int j = 0;
+			uint32_t step = ethtool_rxdma_reg_table[i].step;
+			uint32_t cnt = ethtool_rxdma_reg_table[i].cnt;
+			uint64_t b_addr = ethtool_rxdma_reg_table[i].offsets;
+			for (j = 0; j < cnt; j++) {
+				NXGE_REG_RD64(handle, b_addr + (j * step),
+					      reg);
+				reg++;
+			}
+		} else {
+			NXGE_REG_RD64(handle,
+				      ethtool_rxdma_reg_table[i].offsets,
+				      reg);
+			reg++;
+		}
+	}
+#if 0
+	/* FFLP Registers */
+	for (i = 0; i < NXGE_FFLP_REG_LEN; i++) {
+
+		if (ethtool_fflp_reg_table[i].multi) {
+			int j = 0;
+			uint32_t step = ethtool_fflp_reg_table[i].step;
+			uint32_t cnt = ethtool_fflp_reg_table[i].cnt;
+			uint64_t b_addr = ethtool_fflp_reg_table[i].offsets;
+			for (j = 0; j < cnt; j++) {
+				NXGE_REG_RD64(handle, b_addr + (j * step),
+					      reg);
+				reg++;
+			}
+		} else {
+			NXGE_REG_RD64(handle,
+				      ethtool_fflp_reg_table[i].offsets,
+				      reg);
+			reg++;
+		}
+	}
+#endif
+	/* ZeroCopy Registers */
+	for (i = 0; i < NXGE_ZCP_REG_LEN; i++, reg++) {
+		NXGE_REG_RD64(handle, ethtool_zcp_reg_table[i].offsets, reg);
+	}
+
+	/* TXC Registers */
+	for (i = 0; i < NXGE_TXC_REG_LEN; i++) {
+		if (ethtool_txc_reg_table[i].multi) {
+			uint64_t offset = ethtool_txc_reg_table[i].offsets;
+			uint64_t step = ethtool_txc_reg_table[i].step;
+			uint32_t cnt = ethtool_txc_reg_table[i].cnt;
+			if (ethtool_txc_reg_table[i].pport) {
+				NXGE_REG_RD64(handle, offset + (portn * step),
+					      reg);
+				reg++;
+			} else {
+				int j = 0;
+				for (j = 0; j < cnt; j++) {
+					NXGE_REG_RD64(handle,
+						      offset + (j * step),
+						      reg);
+					reg++;
+				}
+			}
+		} else {
+			NXGE_REG_RD64(handle, ethtool_txc_reg_table[i].offsets,
+				      reg);
+			reg++;
+		}
+	}
+
+	/* TxDMA Registers */
+	for (i = 0; i < NXGE_TXDMA_REG_LEN; i++) {
+		if (ethtool_txdma_reg_table[i].multi) {
+			int j = 0;
+			uint32_t step = ethtool_txdma_reg_table[i].step;
+			uint64_t b_addr = ethtool_txdma_reg_table[i].offsets;
+			for (j = 0; j < ethtool_txdma_reg_table[i].cnt; j++) {
+				NXGE_REG_RD64(handle, b_addr + (j * step),
+					      reg);
+				reg++;
+			}
+		} else {
+			NXGE_REG_RD64(handle,
+				      ethtool_txdma_reg_table[i].offsets,
+				      reg);
+			reg++;
+		}
+	}
+#if 0
+	/* ESPC Registers */
+	for (i = 0; i < NXGE_ESPC_REG_LEN; i++) {
+		if (ethtool_espc_reg_table[i].multi) {
+			int j = 0;
+			uint32_t step = ethtool_espc_reg_table[i].step;
+			uint64_t b_addr =
+			  ESPC_REG_ADDR(ethtool_espc_reg_table[i].offsets);
+			for (j = 0; j < ethtool_espc_reg_table[i].cnt; j++) {
+				NXGE_REG_RD64(handle, b_addr + (j * step),
+					      reg);
+				reg++;
+			}
+		} else {
+			NXGE_REG_RD64(handle,
+				      ESPC_REG_ADDR(ethtool_espc_reg_table[i].offsets),
+				      reg);
+			reg++;
+		}
+	}
+#endif
+	spin_unlock_irqrestore(&nep->lock, flags);
+
+	edata->len = (reg - reg_start) * sizeof(uint64_t);
+
+	printk(KERN_INFO "nxge_get_regs: edata len[%d]\n", edata->len);
+}
+
+static int
+nxge_nway_reset(struct net_device *dev)
+{
+	return 0;
+}
+
+
+static uint32_t
+nxge_get_msg_level(struct net_device *dev)
+{
+	p_nxge_t	nep = netdev_priv(dev);
+	return (nep->msg_enable);
+}
+
+static void
+nxge_set_msg_level(struct net_device *dev, uint32_t data)
+{
+	p_nxge_t	nep = netdev_priv(dev);
+	nep->msg_enable = data;
+}
+
+
+static int
+nxge_set_eeprom(struct net_device *dev, struct ethtool_eeprom *eeprom,
+		uint8_t *bytes)
+{
+	return 0;
+}
+
+static int
+nxge_get_eeprom_len(struct net_device *dev)
+{
+	p_nxge_t	nep = netdev_priv(dev);
+	return (nep->seeprom_len);
+}
+
+static int
+nxge_get_eeprom(struct net_device *dev, struct ethtool_eeprom *eeprom,
+		uint8_t *bytes)
+{
+	p_nxge_t	nep = netdev_priv(dev);
+	npi_handle_t	handle = NXGE_DEV_NPI_HANDLE(nep);
+	int		st_reg_idx, end_reg_idx;
+	int		i, ret = 0;
+	uint32_t	*buf;
+
+	if(eeprom->len == 0) {
+		ret = -EINVAL;
+		goto exit;
+	}
+
+	if (eeprom->offset > eeprom->offset + eeprom->len) {
+		ret = -EINVAL;
+		goto exit;
+	}
+
+	if ((eeprom->offset + eeprom->len) > (nep->seeprom_len))
+		eeprom->len = nep->seeprom_len - eeprom->offset;
+
+	eeprom->magic = nep->pdev->vendor | (nep->pdev->device << 16);
+
+	st_reg_idx = eeprom->offset >> 2;
+	end_reg_idx = (eeprom->offset + eeprom->len) >> 2;
+
+	buf = kmalloc(sizeof(uint32_t) * (end_reg_idx - st_reg_idx + 1),
+		      GFP_KERNEL);
+
+	if (buf == NULL)
+		return -ENOMEM;
+
+	for (i = 0; i <= (end_reg_idx - st_reg_idx); i++) {
+		buf[i] = npi_espc_reg_get(handle, st_reg_idx + i);
+	}
+
+	memcpy(bytes, (uint8_t *)buf + (eeprom->offset & 3),
+	       eeprom->len);
+
+	kfree(buf);
+
+  exit:
+	return ret;
+}
+
+
+static void
+nxge_get_ring_config(struct net_device *dev, struct ethtool_ringparam *ring)
+{
+	return;
+}
+
+static int
+nxge_set_ring_config(struct net_device *dev, struct ethtool_ringparam *ring)
+{
+	return 0;
+}
+
+
+static void
+nxge_get_pauseparam(struct net_device *dev,
+	    struct ethtool_pauseparam *pause)
+{
+
+}
+
+static int
+nxge_set_pauseparam(struct net_device *dev,
+		    struct ethtool_pauseparam *pause)
+{
+	return 0;
+}
+
+static uint32_t
+nxge_get_rx_cksum(struct net_device *dev)
+{
+	p_nxge_t	nep = netdev_priv(dev);
+
+	return (nep->flags & NXGE_FLAG_RX_HW_CSUM);
+}
+
+static int
+nxge_set_rx_cksum(struct net_device *dev, uint32_t data)
+{
+	uint64_t	val = 0;
+	p_nxge_t	nep = netdev_priv(dev);
+	uint8_t		portn = NXGE_GET_PORT_NUM(nep->function_num);
+
+	if (data) {
+		if (!(nep->flags & NXGE_FLAG_RX_HW_CSUM)) {
+			nep->flags |= NXGE_FLAG_RX_HW_CSUM;
+			IPP_REG_RD(nep->npi_handle, portn, IPP_CONFIG_REG,
+				   &val);
+			val |= CFG_IPP_TCP_UDP_CKSUM;
+			IPP_REG_WR(nep->npi_handle, portn, IPP_CONFIG_REG,
+				   val);
+		}
+	} else {
+		if (nep->flags & NXGE_FLAG_RX_HW_CSUM) {
+			nep->flags &= ~NXGE_FLAG_RX_HW_CSUM;
+			IPP_REG_RD(nep->npi_handle, portn, IPP_CONFIG_REG,
+				   &val);
+			val &= ~CFG_IPP_TCP_UDP_CKSUM;
+			IPP_REG_WR(nep->npi_handle, portn, IPP_CONFIG_REG,
+				   val);
+		}
+	}
+
+	return 0;
+}
+
+static uint32_t
+nxge_get_tx_cksum(struct net_device *dev)
+{
+	return ((dev->features & NETIF_F_IP_CSUM) != 0);
+}
+
+static int
+nxge_set_tx_cksum(struct net_device *dev, uint32_t data)
+{
+	p_nxge_t	nep = netdev_priv(dev);
+
+	if (data) {
+		nep->flags |= NXGE_FLAG_TX_HW_CSUM;
+		dev->features |= NETIF_F_IP_CSUM;
+	} else {
+		nep->flags  &= ~NXGE_FLAG_TX_HW_CSUM;
+		dev->features &= ~NETIF_F_IP_CSUM;
+	}
+
+	return 0;
+}
+
+
+static int
+nxge_set_tso(struct net_device *dev, uint32_t data)
+{
+	p_nxge_t	nep = netdev_priv(dev);
+	if (data) {
+		nep->flags |= NXGE_FLAG_TSO_ENABLED;
+		dev->features |= NETIF_F_TSO;
+
+	} else {
+		dev->features &= ~NETIF_F_TSO;
+		nep->flags &= ~NXGE_FLAG_TSO_ENABLED;
+
+	}
+	return 0;
+}
+
+static int
+nxge_get_intr_blanking(struct net_device *dev,
+					   struct  ethtool_coalesce *intr_param)
+{
+	p_nxge_t	nep = netdev_priv(dev);
+
+	intr_param->rx_coalesce_usecs =  	nep->intr_timeout & ~0x8000;
+	intr_param->rx_max_coalesced_frames = nep->intr_thresh;
+	intr_param->rx_max_coalesced_frames_irq = nep->max_receive_pkts;
+	return 0;
+}
+
+static int
+nxge_set_intr_blanking(struct net_device *dev,
+					   struct  ethtool_coalesce *intr_param)
+{
+	p_nxge_t	nep = netdev_priv(dev);
+
+	if ((intr_param->rx_max_coalesced_frames == 0) &&
+		(intr_param->rx_coalesce_usecs == 0)) {
+		return (1);
+	}
+	if (intr_param->rx_max_coalesced_frames_irq < 8)
+		return (1);
+	if (intr_param->rx_coalesce_usecs) {
+		nep->intr_timeout = RCRCFIG_B_TIMEOUT |
+			intr_param->rx_coalesce_usecs;
+	} else {
+		nep->intr_timeout = 0;
+	}
+	nep->intr_thresh =  intr_param->rx_max_coalesced_frames;
+	nep->max_receive_pkts =  intr_param->rx_max_coalesced_frames_irq;
+
+	return 0;
+
+}
+
+static void
+nxge_get_strings(struct net_device *dev, uint32_t stringset, uint8_t *data)
+{
+	int 		i, j, k;
+	p_nxge_t	nep = netdev_priv(dev);
+
+	switch (stringset) {
+	case ETH_SS_STATS:
+                for (i = 0; i < NXGE_STAT_NAMES_CNT; i++) {
+			memcpy(data + i * ETH_GSTRING_LEN,
+			       ethtool_nxge_stats[i].string,
+			       ETH_GSTRING_LEN);
+		}
+
+		if (nep->mac.porttype == PORT_TYPE_XMAC) {
+			for (j = 0; j < NXGE_XMAC_STAT_NAMES_CNT; j++, i++) {
+				memcpy(data + i * ETH_GSTRING_LEN,
+				       ethtool_nxge_xmac_stats[j].string,
+				       ETH_GSTRING_LEN);
+			}
+		} else {
+			for (j = 0; j < NXGE_BMAC_STAT_NAMES_CNT; j++, i++) {
+				memcpy(data + i * ETH_GSTRING_LEN,
+				       ethtool_nxge_bmac_stats[j].string,
+				       ETH_GSTRING_LEN);
+			}
+		}
+
+		for (j = 0; j < nep->max_rdcs; j++) {
+			for(k = 0; k < NXGE_RDC_STAT_NAMES_CNT; k++, i++) {
+				memcpy(data + i * ETH_GSTRING_LEN,
+				       ethtool_nxge_rdc_stats[k].string,
+				       ETH_GSTRING_LEN);
+			}
+		}
+		for (j = 0; j < nep->max_tdcs; j++) {
+			for(k = 0; k < NXGE_TDC_STAT_NAMES_CNT; k++, i++) {
+				memcpy(data + i * ETH_GSTRING_LEN,
+				       ethtool_nxge_tdc_stats[k].string,
+				       ETH_GSTRING_LEN);
+			}
+		}
+
+/* 		memcpy(data, &ethtool_nxge_stats_names, */
+/* 		       sizeof(ethtool_nxge_stats_names)); */
+		break;
+	default:
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+				"nxge_get_strings: Unknown stringset type %d",
+				stringset));
+		break;
+	}
+}
+
+static int
+nxge_get_stats_count(struct net_device *dev)
+{
+	p_nxge_t	nep = netdev_priv(dev);
+	int		stats_cnt = (NXGE_STAT_NAMES_CNT) +
+		((NXGE_RDC_STAT_NAMES_CNT) * nep->max_rdcs) +
+		((NXGE_TDC_STAT_NAMES_CNT) * nep->max_tdcs) +
+		((nep->mac.porttype == PORT_TYPE_XMAC) ?
+		    NXGE_XMAC_STAT_NAMES_CNT : NXGE_BMAC_STAT_NAMES_CNT);
+	return (stats_cnt);
+}
+
+static p_nxge_stats_t
+nxge_get_stats(p_nxge_t nep)
+{
+	p_nxge_stats_t statsp = nep->statsp;
+	nxge_port_t portt;
+	uint8_t portn;
+	npi_handle_t handle;
+	uint64_t reg_val = 0;
+	unsigned long flags;
+	int dma_count, dma_index;
+	p_nxge_rx_ring_stats_t	rdc_stats;
+	p_nxge_tx_ring_stats_t tdc_stats;
+	uint64_t packets, bytes, jumbo, tso;
+	uint32_t perrors, dropped;
+
+	if (!nep->hw_running)
+/* 		return old_statsp; */
+		return (statsp);
+
+	spin_lock_irqsave(&nep->stats_lock, flags);
+
+	portt = nep->mac.porttype;
+	portn = nep->mac.portnum;
+	handle = nep->npi_handle;
+	dma_count = nep->nrdc;
+	bytes = packets = jumbo = tso = 0;
+	perrors = dropped = 0;
+	for (dma_index = 0; dma_index < dma_count; dma_index++) {
+		rdc_stats = &nep->statsp->rdc_stats[dma_index];
+		packets += rdc_stats->ipackets;
+		bytes += rdc_stats->ibytes;
+		jumbo += rdc_stats->rx_jumbo_pkts;
+/*		statsp->norcvbuf += rdc_stats->rx_no_buf; */
+	}
+
+	statsp->ipackets = packets;
+	statsp->rbytes = bytes;
+	statsp->rx_jumbo_pkts = jumbo;
+	bytes = packets = jumbo = 0;
+	perrors = dropped = 0;
+	dma_count = nep->ntdc;
+	for (dma_index = 0; dma_index < dma_count; dma_index++) {
+		tdc_stats = &nep->statsp->tdc_stats[dma_index];
+		packets += tdc_stats->opackets;
+		bytes += tdc_stats->obytes;
+		jumbo += tdc_stats->jumbo;
+		tso += tdc_stats->tx_packets_tso;
+/*		statsp->noxmtbuf += tdc_stats->tx_no_buf; */
+	}
+	statsp->opackets = packets;
+	statsp->obytes = bytes;
+	statsp->tx_jumbo_pkts = jumbo;
+	statsp->tx_tso = tso;
+
+/* 	statsp->ipackets = old_statsp->ipackets; */
+/* 	statsp->rbytes = old_statsp->rbytes; */
+/* 	statsp->norcvbuf = old_statsp->norcvbuf; */
+/* 	statsp->opackets = old_statsp->opackets; */
+/* 	statsp->obytes = old_statsp->obytes; */
+/* 	statsp->noxmtbuf = old_statsp->noxmtbuf; */
+
+	if (portt == PORT_TYPE_XMAC) {
+		/* RX */
+		XMAC_REG_RD(handle, portn, XRXMAC_HIST_CNT1_REG, &reg_val);
+		statsp->xmac_stats.rx_hist1_cnt += (reg_val & 0x1fffff);
+		statsp->xmac_stats.rx_frame_cnt += (reg_val & 0x1fffff);
+
+		XMAC_REG_RD(handle, portn, XRXMAC_HIST_CNT2_REG, &reg_val);
+		statsp->xmac_stats.rx_hist2_cnt += (reg_val & 0x1fffff);
+		statsp->xmac_stats.rx_frame_cnt += (reg_val & 0x1fffff);
+
+		XMAC_REG_RD(handle, portn, XRXMAC_HIST_CNT3_REG, &reg_val);
+		statsp->xmac_stats.rx_hist3_cnt += (reg_val & 0xfffff);
+		statsp->xmac_stats.rx_frame_cnt += (reg_val & 0xfffff);
+
+		XMAC_REG_RD(handle, portn, XRXMAC_HIST_CNT4_REG, &reg_val);
+		statsp->xmac_stats.rx_hist4_cnt += (reg_val & 0x7ffff);
+		statsp->xmac_stats.rx_frame_cnt += (reg_val & 0x7ffff);
+
+		XMAC_REG_RD(handle, portn, XRXMAC_HIST_CNT5_REG, &reg_val);
+		statsp->xmac_stats.rx_hist5_cnt += (reg_val & 0x3ffff);
+		statsp->xmac_stats.rx_frame_cnt += (reg_val & 0x3ffff);
+
+		XMAC_REG_RD(handle, portn, XRXMAC_HIST_CNT6_REG, &reg_val);
+		statsp->xmac_stats.rx_hist6_cnt += (reg_val & 0x1ffff);
+		statsp->xmac_stats.rx_frame_cnt += (reg_val & 0x1ffff);
+
+		XMAC_REG_RD(handle, portn, XRXMAC_HIST_CNT7_REG, &reg_val);
+		statsp->xmac_stats.rx_hist7_cnt += (reg_val & 0x7ffffff);
+		statsp->xmac_stats.rx_frame_cnt += (reg_val & 0x7ffffff);
+
+		XMAC_REG_RD(handle, portn, XRXMAC_BC_FRM_CNT_REG, &reg_val);
+		statsp->xmac_stats.rx_broadcast_cnt += (reg_val & 0x1fffff);
+
+		XMAC_REG_RD(handle, portn, XRXMAC_MC_FRM_CNT_REG, &reg_val);
+		statsp->xmac_stats.rx_mult_cnt += (reg_val & 0x1fffff);
+
+		XMAC_REG_RD(handle, portn, XRXMAC_BT_CNT_REG, &reg_val);
+		statsp->xmac_stats.rx_byte_cnt += (reg_val & 0xffffffff);
+
+		XMAC_REG_RD(handle, portn, XRXMAC_FRAG_CNT_REG, &reg_val);
+		statsp->xmac_stats.rx_frag_cnt += (reg_val & 0x1fffff);
+
+		XMAC_REG_RD(handle, portn, XRXMAC_MPSZER_CNT_REG, &reg_val);
+		statsp->xmac_stats.rx_len_err_cnt += (reg_val & 0xff);
+		statsp->ierrors += (reg_val & 0xff);
+
+		XMAC_REG_RD(handle, portn, XRXMAC_CRC_ER_CNT_REG, &reg_val);
+		statsp->xmac_stats.rx_crc_err_cnt += (reg_val & 0xff);
+		statsp->ierrors += (reg_val & 0xff);
+
+		XMAC_REG_RD(handle, portn, XRXMAC_CD_VIO_CNT_REG, &reg_val);
+		statsp->xmac_stats.rx_viol_err_cnt += (reg_val & 0xff);
+		statsp->ierrors += (reg_val & 0xff);
+
+		XMAC_REG_RD(handle, portn, XRXMAC_AL_ER_CNT_REG, &reg_val);
+		statsp->xmac_stats.rx_frame_align_err_cnt += (reg_val & 0xff);
+		statsp->ierrors += (reg_val & 0xff);
+
+		XMAC_REG_RD(handle, portn, XMAC_LINK_FLT_CNT_REG, &reg_val);
+		statsp->xmac_stats.rx_linkfault_err_cnt += (reg_val & 0xff);
+
+		/* TX */
+		XMAC_REG_RD(handle, portn, XTXMAC_FRM_CNT_REG, &reg_val);
+		statsp->xmac_stats.tx_frame_cnt += reg_val;
+
+		XMAC_REG_RD(handle, portn, XTXMAC_BYTE_CNT_REG, &reg_val);
+		statsp->xmac_stats.tx_byte_cnt += (reg_val & 0xffffffff);
+
+	} else {
+		BMAC_REG_RD(handle, portn, BTXMAC_FRM_CNT_REG, &reg_val);
+		statsp->bmac_stats.tx_frame_cnt += reg_val;
+		/* Clear register as it is not auto clear on read */
+		BMAC_REG_WR(handle, portn, BTXMAC_FRM_CNT_REG, 0);
+
+		BMAC_REG_RD(handle, portn, BTXMAC_BYTE_CNT_REG, &reg_val);
+		statsp->bmac_stats.tx_byte_cnt += reg_val;
+		/* Clear register as it is not auto clear on read */
+		BMAC_REG_WR(handle, portn, BTXMAC_BYTE_CNT_REG, 0);
+
+		BMAC_REG_RD(handle, portn, RXMAC_FRM_CNT_REG, &reg_val);
+		statsp->bmac_stats.rx_frame_cnt += reg_val;
+		/* Clear register as it is not auto clear on read */
+		BMAC_REG_WR(handle, portn, RXMAC_FRM_CNT_REG, 0);
+
+		BMAC_REG_RD(handle, portn, BRXMAC_BYTE_CNT_REG, &reg_val);
+		statsp->bmac_stats.rx_byte_cnt += reg_val;
+		/* Clear register as it is not auto clear on read */
+		BMAC_REG_WR(handle, portn, BRXMAC_BYTE_CNT_REG, 0);
+
+		BMAC_REG_RD(handle, portn, MAC_LEN_ER_CNT_REG, &reg_val);
+		statsp->bmac_stats.rx_len_err_cnt += reg_val;
+		statsp->ierrors += reg_val;
+		/* Clear register as it is not auto clear on read */
+		BMAC_REG_WR(handle, portn, MAC_LEN_ER_CNT_REG, 0);
+
+		BMAC_REG_RD(handle, portn, BMAC_AL_ER_CNT_REG, &reg_val);
+		statsp->bmac_stats.rx_align_err_cnt += reg_val;
+		statsp->ierrors += reg_val;
+		/* Clear register as it is not auto clear on read */
+		BMAC_REG_WR(handle, portn, BMAC_AL_ER_CNT_REG, 0);
+
+		BMAC_REG_RD(handle, portn, BMAC_CRC_ER_CNT_REG, &reg_val);
+		statsp->bmac_stats.rx_crc_err_cnt += reg_val;
+		statsp->ierrors += reg_val;
+		/* Clear register as it is not auto clear on read */
+		BMAC_REG_WR(handle, portn, BMAC_CRC_ER_CNT_REG, 0);
+
+		BMAC_REG_RD(handle, portn, BMAC_CD_VIO_CNT_REG, &reg_val);
+		statsp->bmac_stats.rx_viol_err_cnt += reg_val;
+		statsp->ierrors += reg_val;
+		/* Clear register as it is not auto clear on read */
+		BMAC_REG_WR(handle, portn, BMAC_CD_VIO_CNT_REG, 0);
+
+	}
+
+	IPP_REG_RD(handle, portn,
+			   IPP_ECC_ERR_COUNTER_REG, &reg_val);
+	statsp->ipp_stats.ecc_err_cnt +=	reg_val;
+
+	IPP_REG_RD(handle, portn,
+				   IPP_TCP_CKSUM_ERR_CNT_REG, &reg_val);
+
+	statsp->ipp_stats.bad_cs_cnt += reg_val;
+
+	IPP_REG_RD(handle, portn,
+				   IPP_DISCARD_PKT_CNT_REG, &reg_val);
+
+	statsp->ipp_stats.pkt_dis_cnt += reg_val;
+	spin_unlock_irqrestore(&nep->stats_lock, flags);
+
+	return (statsp);
+}
+
+static void
+nxge_get_ethtool_stats(struct net_device *dev, struct ethtool_stats *stats,
+			    uint64_t *data)
+{
+	p_nxge_t nep = netdev_priv(dev);
+	p_nxge_stats_t statsp = nxge_get_stats(nep);
+	int i, j, k;
+	uint8_t counter64 = sizeof (uint64_t);
+
+	for (i = 0; i < NXGE_STAT_NAMES_CNT; i++) {
+		char *p = (char *)(((char *)statsp) +
+			    ethtool_nxge_stats[i].offset);
+		if (ethtool_nxge_stats[i].width == counter64)
+			data[i] = *(uint64_t *)p;
+		else
+			data[i] = *(uint32_t *)p;
+	}
+
+	if (nep->mac.porttype == PORT_TYPE_XMAC) {
+		for (j = 0; j < NXGE_XMAC_STAT_NAMES_CNT; j++, i++) {
+			char *p = (char *)(((char *)statsp) +
+				    ethtool_nxge_xmac_stats[j].offset);
+			if (ethtool_nxge_stats[i].width == counter64)
+				data[i] = *(uint64_t *)p;
+			else
+				data[i] = *(uint32_t *)p;
+
+		}
+	} else {
+		for (j = 0; j < NXGE_BMAC_STAT_NAMES_CNT; j++, i++) {
+			char *p = (char *)(((char *)statsp) +
+				    ethtool_nxge_bmac_stats[j].offset);
+			if (ethtool_nxge_stats[i].width == counter64)
+				data[i] = *(uint64_t *)p;
+			else
+				data[i] = *(uint32_t *)p;
+		}
+	}
+
+	for (j = 0; j < nep->max_rdcs; j++) {
+
+		for (k = 0; k < NXGE_RDC_STAT_NAMES_CNT; k++, i++) {
+			char *p = (char *)(((char *)(&statsp->rdc_stats[j])) +
+				    ethtool_nxge_rdc_stats[k].offset);
+			if (ethtool_nxge_stats[i].width == counter64)
+				data[i] = *(uint64_t *)p;
+			else
+				data[i] = *(uint32_t *)p;
+		}
+	}
+
+	for (j = 0; j < nep->max_tdcs; j++) {
+
+		for (k = 0; k < NXGE_TDC_STAT_NAMES_CNT; k++, i++) {
+			char *p = (char *)(((char *)(&statsp->tdc_stats[j])) +
+				    ethtool_nxge_tdc_stats[k].offset);
+			if (ethtool_nxge_stats[i].width == counter64)
+				data[i] = *(uint64_t *)p;
+			else
+				data[i] = *(uint32_t *)p;
+		}
+	}
+}
+
+
+struct ethtool_ops nxge_ethtool_ops = {
+	.get_settings		= nxge_get_settings,
+	.set_settings		= nxge_set_settings,
+	.get_pauseparam		= nxge_get_pauseparam,
+	.set_pauseparam		= nxge_set_pauseparam,
+	.get_drvinfo		= nxge_get_drvinfo,
+	.get_regs_len		= nxge_get_regs_len,
+	.get_regs		= nxge_get_regs,
+	.get_msglevel		= nxge_get_msg_level,
+	.set_msglevel		= nxge_set_msg_level,
+	.nway_reset		= nxge_nway_reset,
+	.get_link		= ethtool_op_get_link,
+	.get_eeprom_len		= nxge_get_eeprom_len,
+	.get_eeprom		= nxge_get_eeprom,
+#if 0
+	.set_eeprom		= nxge_set_eeprom,
+#endif
+	.get_ringparam		= nxge_get_ring_config,
+	.set_ringparam		= nxge_set_ring_config,
+	.get_rx_csum		= nxge_get_rx_cksum,
+	.set_rx_csum		= nxge_set_rx_cksum,
+	.get_tx_csum		= nxge_get_tx_cksum,
+	.set_tx_csum		= nxge_set_tx_cksum,
+	.get_sg		= ethtool_op_get_sg,
+	.set_sg		= ethtool_op_set_sg,
+	.get_tso		= ethtool_op_get_tso,
+	.set_tso		= nxge_set_tso,
+
+	.get_coalesce		= nxge_get_intr_blanking,
+	.set_coalesce		= nxge_set_intr_blanking,
+
+	.get_strings		= nxge_get_strings,
+	.get_stats_count	= nxge_get_stats_count,
+	.get_ethtool_stats	= nxge_get_ethtool_stats,
+};
diff --git a/drivers/net/nxge/nxge_fflp.c b/drivers/net/nxge/nxge_fflp.c
new file mode 100644
index 0000000..33f3298
--- /dev/null
+++ b/drivers/net/nxge/nxge_fflp.c
@@ -0,0 +1,2719 @@
+/*
+ * nxge_fflp.c	Neptune FFLP Classifier interface functions
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#include <npi_fflp.h>
+#include <npi_mac.h>
+#include <nxge_defs.h>
+#include <nxge_flow.h>
+#include <nxge_fflp.h>
+#include <nxge_impl.h>
+#include <nxge_impl.h>
+#include <linux/types.h>
+#include <nxge_fflp_hash.h>
+#include <nxge_common.h>
+
+
+int 	nxge_tcam_class_enable = 0;
+int 	nxge_tcam_lookup_enable = 0;
+
+int 	nxge_flow_dist_enable = NXGE_CLASS_FLOW_USE_DST_PORT |
+	NXGE_CLASS_FLOW_USE_SRC_PORT | NXGE_CLASS_FLOW_USE_IPDST |
+	NXGE_CLASS_FLOW_USE_IPSRC | NXGE_CLASS_FLOW_USE_PROTO |
+	NXGE_CLASS_FLOW_USE_PORTNUM;
+
+/*
+ * Bit mapped
+ * 0x80000000:      Drop
+ * 0x0000:      NO TCAM Lookup Needed
+ * 0x0001:      TCAM Lookup Needed with Dest Addr (IPv6)
+ * 0x0003:      TCAM Lookup Needed with SRC Addr (IPv6)
+ * 0x0010:      use MAC Port
+ * 0x0020:      use L2DA
+ * 0x0040:      use VLAN
+ * 0x0080:      use proto
+ * 0x0100:      use IP src addr
+ * 0x0200:      use IP dest addr
+ * 0x0400:      use Src Port
+ * 0x0800:      use Dest Port
+ * 0x0fff:      enable all options for IPv6 (with src addr)
+ * 0x0ffd:      enable all options for IPv6 (with dest addr)
+ * 0x0fff:      enable all options for IPv4
+ * 0x0ffd:      enable all options for IPv4
+ *
+ */
+
+/*
+ * the default is to distribute as function of:
+ * protocol
+ * ip src address
+ * ip dest address
+ * src port
+ * dest port
+ *
+ * 0x0f80
+ *
+ */
+
+int 	nxge_tcp4_class = NXGE_CLASS_FLOW_USE_DST_PORT |
+	NXGE_CLASS_FLOW_USE_SRC_PORT | NXGE_CLASS_FLOW_USE_IPDST |
+	NXGE_CLASS_FLOW_USE_IPSRC | NXGE_CLASS_FLOW_USE_PROTO |
+	NXGE_CLASS_FLOW_USE_PORTNUM;
+
+int 	nxge_udp4_class = NXGE_CLASS_FLOW_USE_DST_PORT |
+	NXGE_CLASS_FLOW_USE_SRC_PORT | NXGE_CLASS_FLOW_USE_IPDST |
+	NXGE_CLASS_FLOW_USE_IPSRC | NXGE_CLASS_FLOW_USE_PROTO |
+	NXGE_CLASS_FLOW_USE_PORTNUM;
+
+int 	nxge_ah4_class = 0x0f90;
+int 	nxge_sctp4_class = 0x0f90;
+int 	nxge_tcp6_class = 0x0f90;
+int 	nxge_udp6_class = 0x0f90;
+int 	nxge_ah6_class = 0x0f90;
+int 	nxge_sctp6_class = 0xf90;
+uint32_t	nxge_fflp_init_h1 = 0xffffffff;
+uint32_t	nxge_fflp_init_h2 = 0xffff;
+
+uint64_t class_quick_config_distribute [NXGE_CLASS_CONFIG_PARAMS] = {
+	0xffffffffULL,			/* h1_init */
+	0xffffULL,			/* h2_init */
+	0x0,				/* cfg_ether_usr1 */
+	0x0,				/* cfg_ether_usr2 */
+	0x0,				/* cfg_ip_usr4 */
+	0x0,				/* cfg_ip_usr5 */
+	0x0,				/* cfg_ip_usr6 */
+	0x0,				/* cfg_ip_usr7 */
+	0x0,				/* opt_ip_usr4 */
+	0x0,				/* opt_ip_usr5 */
+	0x0,				/* opt_ip_usr6 */
+	0x0,				/* opt_ip_usr7 */
+	NXGE_CLASS_FLOW_GEN_SERVER,	/* opt_ipv4_tcp */
+	NXGE_CLASS_FLOW_GEN_SERVER,	/* opt_ipv4_udp */
+	NXGE_CLASS_FLOW_GEN_SERVER,	/* opt_ipv4_ah */
+	NXGE_CLASS_FLOW_GEN_SERVER,	/* opt_ipv4_sctp */
+	NXGE_CLASS_FLOW_GEN_SERVER,	/* opt_ipv6_tcp */
+	NXGE_CLASS_FLOW_GEN_SERVER,	/* opt_ipv6_udp */
+	NXGE_CLASS_FLOW_GEN_SERVER,	/* opt_ipv6_ah */
+	NXGE_CLASS_FLOW_GEN_SERVER	/* opt_ipv6_sctp */
+};
+
+uint64_t class_quick_config_web_server [NXGE_CLASS_CONFIG_PARAMS] = {
+	0xffffffffULL,			/* h1_init */
+	0xffffULL,			/* h2_init */
+	0x0,				/* cfg_ether_usr1 */
+	0x0,				/* cfg_ether_usr2 */
+	0x0,				/* cfg_ip_usr4 */
+	0x0,				/* cfg_ip_usr5 */
+	0x0,				/* cfg_ip_usr6 */
+	0x0,				/* cfg_ip_usr7 */
+	0x0,				/* opt_ip_usr4 */
+	0x0,				/* opt_ip_usr5 */
+	0x0,				/* opt_ip_usr6 */
+	0x0,				/* opt_ip_usr7 */
+	NXGE_CLASS_FLOW_WEB_SERVER,	/* opt_ipv4_tcp */
+	NXGE_CLASS_FLOW_GEN_SERVER,	/* opt_ipv4_udp */
+	NXGE_CLASS_FLOW_GEN_SERVER,	/* opt_ipv4_ah */
+	NXGE_CLASS_FLOW_GEN_SERVER,	/* opt_ipv4_sctp */
+	NXGE_CLASS_FLOW_GEN_SERVER,	/* opt_ipv6_tcp */
+	NXGE_CLASS_FLOW_GEN_SERVER,	/* opt_ipv6_udp */
+	NXGE_CLASS_FLOW_GEN_SERVER,	/* opt_ipv6_ah */
+	NXGE_CLASS_FLOW_GEN_SERVER	/* opt_ipv6_sctp */
+};
+
+
+/* function prototypes  */
+static nxge_status_t nxge_fflp_vlan_tbl_clear_all(p_nxge_t nxgep);
+static nxge_status_t nxge_fflp_tcam_invalidate_all(p_nxge_t nxgep);
+static nxge_status_t nxge_fflp_tcam_init(p_nxge_t nxgep);
+static nxge_status_t nxge_fflp_fcram_init(p_nxge_t nxgep);
+
+static void nxge_fill_tcam_entry_tcp(p_nxge_t nxgep,
+				    flow_spec_t *flow_spec,
+				    tcam_entry_t *tcam_ptr);
+static void nxge_fill_tcam_entry_udp(p_nxge_t nxgep,
+					    flow_spec_t *flow_spec,
+					    tcam_entry_t *tcam_ptr);
+static void nxge_fill_tcam_entry_sctp(p_nxge_t nxgep,
+					    flow_spec_t *flow_spec,
+					    tcam_entry_t *tcam_ptr);
+
+static void nxge_fill_tcam_entry_tcp_ipv6(p_nxge_t nxgep,
+					    flow_spec_t *flow_spec,
+					    tcam_entry_t *tcam_ptr);
+
+static void nxge_fill_tcam_entry_udp_ipv6(p_nxge_t nxgep,
+					    flow_spec_t *flow_spec,
+					    tcam_entry_t *tcam_ptr);
+
+static void nxge_fill_tcam_entry_sctp_ipv6(p_nxge_t nxgep,
+					    flow_spec_t *flow_spec,
+					    tcam_entry_t *tcam_ptr);
+
+static uint8_t nxge_get_rdc_offset(p_nxge_t nxgep, uint8_t class,
+				    uint32_t channel_cookie);
+static uint8_t nxge_get_rdc_group(p_nxge_t nxgep, uint8_t class,
+				    uint32_t channel_cookie);
+
+static tcam_location_t nxge_get_tcam_location(p_nxge_t nxgep,
+						    uint8_t class);
+
+
+/* functions used outside this file */
+nxge_status_t nxge_fflp_config_vlan_table(p_nxge_t nxgep, uint16_t vlan_id);
+nxge_status_t nxge_fflp_ip_class_config_all(p_nxge_t nxgep);
+nxge_status_t nxge_add_flow(p_nxge_t nxgep, flow_resource_t *flow_res);
+nxge_status_t nxge_handle_tcam_fragment(p_nxge_t nxgep);
+nxge_status_t nxge_add_tcam_entry(p_nxge_t nxgep, flow_resource_t *flow_res);
+nxge_status_t nxge_add_fcram_entry(p_nxge_t nxgep, flow_resource_t *flow_res);
+nxge_status_t nxge_flow_get_hash(p_nxge_t nxgep, flow_resource_t *flow_res,
+			    uint32_t *H1, uint16_t *H2);
+
+nxge_status_t nxge_classify_exit_sw(p_nxge_t nxgep);
+
+/*
+ * The crc32c algorithms are taken from sctp_crc32 implementation
+ * common/inet/sctp_crc32.{c,h}
+ *
+ */
+
+/*
+ * SCTP uses reflected/reverse polynomial CRC32 with generating
+ * polynomial 0x1EDC6F41L
+ */
+#define	SCTP_POLY 0x1EDC6F41L
+
+/* CRC-CCITT Polynomial */
+#define	CRC_CCITT_POLY 0x1021
+
+/* The four CRC32c tables. */
+static uint32_t crc32c_tab[4][256];
+
+/* The four CRC-CCITT tables. */
+static uint16_t crc_ccitt_tab[4][256];
+
+/* the four tables for H1 Computation */
+static uint32_t h1table[4][256];
+
+#define	CRC_32C_POLY 0x1EDC6F41L
+
+#define	COMPUTE_H1_BYTE(crc, data) \
+	(crc = (crc<<8)^h1table[0][((crc >> 24) ^data) & 0xff])
+
+
+static uint32_t
+reflect_32(uint32_t b)
+{
+	int i;
+	uint32_t rw = 0;
+
+	for (i = 0; i < 32; i++) {
+		if (b & 1) {
+			rw |= 1 << (31 - i);
+		}
+		b >>= 1;
+	}
+	return (rw);
+}
+
+#ifdef	LATER
+static uint16_t
+reflect_16(uint16_t b)
+{
+	int i;
+	uint16_t rw = 0;
+
+	for (i = 0; i < 16; i++) {
+		if (b & 1) {
+			rw |= 1 << (15 - i);
+		}
+		b >>= 1;
+	}
+	return (rw);
+}
+#endif
+
+static uint32_t
+flip32(uint32_t w)
+{
+	return (((w >> 24) | ((w >> 8) & 0xff00) | ((w << 8) & 0xff0000) |
+		(w << 24)));
+}
+
+#ifdef	LATER
+static uint16_t
+flip16(uint16_t w)
+{
+	return (((w >> 8) & 0x00ff) | ((w << 8) & 0xff00));
+}
+#endif
+
+
+/*
+ * Initialize the crc32c tables.
+ */
+
+void
+nxge_crc32c_init(void)
+{
+	uint32_t index, bit, byte, crc;
+
+	for (index = 0; index < 256; index++) {
+		crc = reflect_32(index);
+		for (byte = 0; byte < 4; byte++) {
+			for (bit = 0; bit < 8; bit++) {
+				crc = (crc & 0x80000000) ?
+				    (crc << 1) ^ SCTP_POLY : crc << 1;
+			}
+#ifdef _BIG_ENDIAN
+			crc32c_tab[3 - byte][index] = flip32(reflect_32(crc));
+#else
+			crc32c_tab[byte][index] = reflect_32(crc);
+#endif
+		}
+	}
+}
+
+
+/*
+ * Initialize the crc-ccitt tables.
+ */
+
+void
+nxge_crc_ccitt_init(void)
+{
+
+	uint16_t crc;
+	uint16_t index, bit, byte;
+
+	for (index = 0; index < 256; index++) {
+		crc = index << 8;
+		for (byte = 0; byte < 4; byte++) {
+			for (bit = 0; bit < 8; bit++) {
+				crc = (crc & 0x8000) ?
+				    (crc << 1) ^ CRC_CCITT_POLY : crc << 1;
+			}
+
+#ifdef _BIG_ENDIAN
+			crc_ccitt_tab[3 - byte][index] = crc;
+#else
+			crc_ccitt_tab[byte][index] = crc;
+#endif
+		}
+	}
+
+}
+
+
+/*
+ * Lookup  the crc32c for a byte stream
+ */
+
+static void
+nxge_crc32c_byte(uint32_t *crcptr, const uint8_t *buf, int len)
+{
+	uint32_t crc;
+	int i;
+
+	crc = *crcptr;
+	for (i = 0; i < len; i++) {
+#ifdef _BIG_ENDIAN
+		crc = (crc << 8) ^ crc32c_tab[3][buf[i] ^ (crc >> 24)];
+#else
+		crc = (crc >> 8) ^ crc32c_tab[0][buf[i] ^ (crc & 0xff)];
+#endif
+	}
+	*crcptr = crc;
+}
+
+
+
+/*
+ * Lookup  the crc-ccitt for a byte stream
+ */
+
+static void
+nxge_crc_ccitt_byte(uint16_t *crcptr, uint8_t *buf, int len)
+{
+	uint16_t crc;
+	int i;
+
+	crc = *crcptr;
+	for (i = 0; i < len; i++) {
+
+#ifdef _BIG_ENDIAN
+		crc = (crc << 8) ^ crc_ccitt_tab[3][buf[i] ^ (crc >> 8)];
+#else
+		crc = (crc << 8) ^ crc_ccitt_tab[0][buf[i] ^ (crc >> 8)];
+#endif
+	}
+	*crcptr = crc;
+}
+
+
+
+
+/*
+ * Lookup  the crc32c for a 32 bit word stream
+ * Lookup is done fro the 4 bytes in parallel
+ * from the tables computed earlier
+ *
+ */
+
+static void
+nxge_crc32c_word(uint32_t *crcptr, const uint32_t *buf, int len)
+{
+	uint32_t w, crc;
+	int i;
+
+	crc = *crcptr;
+	for (i = 0; i < len; i++) {
+		w = crc ^ buf[i];
+		crc = crc32c_tab[0][w >> 24] ^ crc32c_tab[1][(w >> 16) & 0xff] ^
+		    crc32c_tab[2][(w >> 8) & 0xff] ^ crc32c_tab[3][w & 0xff];
+	}
+	*crcptr = crc;
+}
+
+
+
+
+/*
+ * Lookup  the crc-ccitt for a stream of bytes
+ *
+ * Since the parallel lookup version doesn't work yet,
+ * use the byte stream version (lookup crc for a byte
+ * at a time
+ *
+ */
+uint16_t
+nxge_crc_ccitt(uint16_t crc16, uint8_t *buf, int len)
+{
+#ifdef	LATER
+	int rem;
+#endif
+
+#ifndef LATER
+	nxge_crc_ccitt_byte(&crc16, buf, len);
+#else
+	rem = 4 - ((uintptr_t)buf) & 3;
+	if (rem != 0) {
+		if (len < rem) {
+			rem = len;
+		}
+		nxge_crc_ccitt_byte(&crc16, buf, rem);
+		buf = buf + rem;
+		len = len - rem;
+	}
+
+	if (len > 3) {
+		nxge_crc_ccitt_word(&crc16, (const uint32_t *)buf, len / 4);
+	}
+
+	rem = len & 3;
+	if (rem != 0) {
+		nxge_crc_ccitt_byte(&crc16, buf + len - rem, rem);
+	}
+#endif
+
+	return (crc16);
+}
+
+
+
+/*
+ * Lookup  the crc32c for a stream of bytes
+ *
+ * Tries to lookup the CRC on 4 byte words
+ * If the buffer is not 4 byte aligned, first compute
+ * with byte lookup until aligned. Then compute crc
+ * for each 4 bytes. If there are bytes left at the end of
+ * the buffer, then perform a byte lookup for the remaining bytes
+ *
+ *
+ */
+
+uint32_t
+nxge_crc32c(uint32_t crc32, const uint8_t *buf, int len)
+{
+	int rem;
+
+	rem = 4 - (((uint64_t)buf) & 3);
+	if (rem != 0) {
+		if (len < rem) {
+			rem = len;
+		}
+		nxge_crc32c_byte(&crc32, buf, rem);
+		buf = buf + rem;
+		len = len - rem;
+	}
+
+	if (len > 3) {
+		nxge_crc32c_word(&crc32, (const uint32_t *)buf, len / 4);
+	}
+
+	rem = len & 3;
+	if (rem != 0) {
+		nxge_crc32c_byte(&crc32, buf + len - rem, rem);
+	}
+	return (crc32);
+}
+
+
+
+
+void
+nxge_init_h1_table()
+{
+	uint32_t crc, bit, byte, index;
+
+	for (index = 0; index < 256; index ++) {
+		crc = index << 24;
+		for (byte = 0; byte < 4; byte++) {
+			for (bit = 0; bit < 8; bit++) {
+				crc = ((crc  & 0x80000000)) ?
+					(crc << 1) ^ CRC_32C_POLY : crc << 1;
+			}
+			h1table[byte][index] = crc;
+		}
+	}
+}
+
+
+/*
+ * Reference Neptune H1 computation function
+ *
+ * It is a slightly modified implementation of
+ * CRC-32C implementation
+ */
+
+uint32_t
+nxge_compute_h1_serial(uint32_t init_value,
+					    uint32_t *flow, uint32_t len)
+{
+	int bit, byte;
+	uint32_t crc_h1 = init_value;
+	uint8_t *buf;
+	buf = (uint8_t *)flow;
+	for (byte = 0; byte < len; byte++) {
+		for (bit = 0; bit < 8; bit++) {
+			crc_h1 = (((crc_h1 >> 24) & 0x80) ^
+					    ((buf[byte] << bit) & 0x80)) ?
+				(crc_h1 << 1) ^ CRC_32C_POLY : crc_h1 << 1;
+		}
+	}
+
+	return (crc_h1);
+}
+
+
+
+/*
+ * table based implementation
+ * uses 4 four tables in parallel
+ * 1 for each byte of a 32 bit word
+ *
+ * This is the default h1 computing function
+ *
+ */
+
+uint32_t
+nxge_compute_h1_table4(uint32_t crcin,
+					    uint32_t *flow, uint32_t length)
+{
+
+	uint32_t w, fw, i, crch1 = crcin;
+	uint32_t *buf;
+	buf = (uint32_t *)flow;
+
+	for (i = 0; i < length / 4; i++) {
+#ifdef _BIG_ENDIAN
+		fw = buf[i];
+#else
+		fw = flip32(buf[i]);
+		fw = buf[i];
+#endif
+		w = crch1 ^ fw;
+		crch1 = h1table[3][w >> 24] ^ h1table[2][(w >> 16) & 0xff] ^
+		    h1table[1][(w >> 8) & 0xff] ^ h1table[0][w & 0xff];
+	}
+	return (crch1);
+}
+
+
+
+/*
+ * table based implementation
+ * uses a single table and computes h1 for a byte
+ * at a time.
+ *
+ */
+
+uint32_t
+nxge_compute_h1_table1(uint32_t crcin,
+					    uint32_t *flow, uint32_t length)
+{
+
+	uint32_t i, crch1 = 0, tmp = crcin;
+	uint8_t *buf;
+	buf = (uint8_t *)flow;
+
+	tmp = crcin;
+	for (i = 0; i < length; i++) {
+		crch1 = COMPUTE_H1_BYTE(tmp, buf[i]);
+		tmp = crch1;
+	}
+
+	return (crch1);
+}
+
+
+nxge_status_t
+nxge_tcam_dump_entry(p_nxge_t nxgep, uint32_t location)
+{
+	tcam_entry_t tcam_rdptr;
+	uint64_t asc_ram = 0;
+	npi_handle_t handle;
+	npi_status_t status;
+
+	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+
+	memset((char *)&tcam_rdptr, 0, sizeof (struct tcam_entry));
+	status = npi_fflp_tcam_entry_read(handle, (tcam_location_t)location,
+				    (struct tcam_entry *)&tcam_rdptr);
+	if (status & NPI_FAILURE) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				    " nxge_tcam_dump_entry:"
+				    "  tcam read failed at location %d ",
+				    location));
+		return (NXGE_ERROR);
+	}
+
+	status = npi_fflp_tcam_asc_ram_entry_read(handle,
+				    (tcam_location_t)location, &asc_ram);
+
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL, "location %x\n"
+		    " key:  %llx %llx %llx %llx \n"
+		    " mask: %llx %llx %llx %llx \n"
+		    " ASC RAM %llx \n", location,
+		    tcam_rdptr.key0, tcam_rdptr.key1,
+		    tcam_rdptr.key2, tcam_rdptr.key3,
+		    tcam_rdptr.mask0, tcam_rdptr.mask1,
+		    tcam_rdptr.mask2, tcam_rdptr.mask3,
+		    asc_ram));
+	return (NXGE_OK);
+}
+
+void
+nxge_get_tcam(p_nxge_t nxgep, void *mp)
+{
+
+	tcam_location_t tcam_loc;
+	uint32_t *lptr;
+	int location;
+
+	int start_location = 0, stop_location = nxgep->classifier.tcam_size;
+
+	lptr = (uint32_t *)mp;
+	location = *lptr;
+
+	if ((location >= nxgep->classifier.tcam_size) || (location < -1)) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			    "nxge_tcam_dump: Invalid location %d \n",
+			    location));
+		return;
+	}
+	if (location == -1) {
+		start_location = 0;
+		stop_location = nxgep->classifier.tcam_size;
+	} else {
+		start_location = location;
+		stop_location = location +1;
+	}
+	for (tcam_loc = start_location; tcam_loc < stop_location; tcam_loc++)
+		(void) nxge_tcam_dump_entry(nxgep, tcam_loc);
+
+}
+
+
+/*
+ * nxge_fflp_vlan_table_invalidate_all
+ * invalidates the vlan RDC table entries.
+ * INPUT
+ * nxge    soft state data structure
+ * Return
+ *      NXGE_OK
+ *      NXGE_ERROR
+ *
+ */
+static nxge_status_t
+nxge_fflp_vlan_tbl_clear_all(p_nxge_t nxgep)
+{
+	vlan_id_t vlan_id;
+	npi_handle_t handle;
+	npi_status_t rs = NPI_SUCCESS;
+	vlan_id_t start = 0, stop = NXGE_MAX_VLANS;
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, "==> nxge_fflp_vlan_tbl_clear_all "));
+	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	for (vlan_id = start; vlan_id < stop; vlan_id++) {
+		rs = npi_fflp_cfg_vlan_table_clear(handle, vlan_id);
+		if (rs != NPI_SUCCESS) {
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			    "VLAN Table invalidate failed for vlan id %d ",
+			    vlan_id));
+			return (NXGE_ERROR | rs);
+		}
+	}
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, "<== nxge_fflp_vlan_tbl_clear_all "));
+
+	return (NXGE_OK);
+}
+
+
+
+/*
+ * The following functions are used by other modules to init
+ * the fflp module.
+ * these functions are the basic API used to init
+ * the fflp modules (tcam, fcram etc ......)
+ *
+ * The TCAM search future would be disabled  by default.
+ */
+
+static nxge_status_t
+nxge_fflp_tcam_init(p_nxge_t nxgep)
+{
+
+	uint8_t access_ratio;
+	tcam_class_t class;
+	npi_status_t rs = NPI_SUCCESS;
+	npi_handle_t handle;
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, "==> nxge_fflp_tcam_init"));
+	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+
+	rs = npi_fflp_cfg_tcam_disable(handle);
+	if (rs != NPI_SUCCESS) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL, "failed TCAM Disable\n"));
+		return (NXGE_ERROR | rs);
+	}
+
+	access_ratio = nxgep->param_arr[param_tcam_access_ratio].value;
+	rs = npi_fflp_cfg_tcam_access(handle, access_ratio);
+	if (rs != NPI_SUCCESS) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			    "failed TCAM Access cfg\n"));
+		return (NXGE_ERROR | rs);
+	}
+
+/* disable configurable classes */
+/* disable the configurable ethernet classes; */
+	for (class = TCAM_CLASS_ETYPE_1;
+		class <= TCAM_CLASS_ETYPE_2; class++) {
+		rs = npi_fflp_cfg_enet_usr_cls_disable(handle, class);
+		if (rs != NPI_SUCCESS) {
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+							"TCAM USR Ether Class"
+							"config failed."));
+			return (NXGE_ERROR | rs);
+		}
+	}
+
+		/* disable the configurable ip classes; */
+	for (class = TCAM_CLASS_IP_USER_4;
+		class <= TCAM_CLASS_IP_USER_7; class++) {
+		rs = npi_fflp_cfg_ip_usr_cls_disable(handle, class);
+		if (rs != NPI_SUCCESS) {
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+					"TCAM USR IP Class"
+					"cnfg failed."));
+			return (NXGE_ERROR | rs);
+		}
+	}
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, "<== nxge_fflp_tcam_init"));
+
+	return (NXGE_OK);
+}
+
+
+/*
+ * nxge_fflp_tcam_invalidate_all
+ * invalidates all the tcam entries.
+ * INPUT
+ * nxge    soft state data structure
+ * Return
+ *      NXGE_OK
+ *      NXGE_ERROR
+ *
+ */
+static nxge_status_t
+nxge_fflp_tcam_invalidate_all(p_nxge_t nxgep)
+{
+	uint16_t location;
+	npi_status_t rs = NPI_SUCCESS;
+	npi_handle_t handle;
+	int start = 0, stop = nxgep->classifier.tcam_size;
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL,
+			    "==> nxge_fflp_tcam_invalidate_all"));
+	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	MUTEX_ENTER(nxgep->tcam_lock);
+	for (location = start; location < stop; location++) {
+		rs = npi_fflp_tcam_entry_invalidate(handle, location);
+		if (rs != NPI_SUCCESS) {
+			MUTEX_EXIT(nxgep->tcam_lock);
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				    "TCAM invalidate failed at loc %d ",
+				    location));
+			return (NXGE_ERROR | rs);
+		}
+	}
+	MUTEX_EXIT(nxgep->tcam_lock);
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL,
+			    "<== nxge_fflp_tcam_invalidate_all"));
+	return (NXGE_OK);
+}
+
+
+static nxge_status_t
+nxge_fflp_fcram_init(p_nxge_t nxgep)
+
+{
+	fflp_fcram_output_drive_t strength;
+	fflp_fcram_qs_t qs;
+	npi_status_t rs = NPI_SUCCESS;
+	uint8_t access_ratio;
+    int partition;
+	npi_handle_t handle;
+	uint32_t min_time,  max_time,  sys_time;
+
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, "==> nxge_fflp_fcram_init"));
+
+/*
+ * for these we need to get the recommended values from Michael
+ */
+	min_time = FCRAM_REFRESH_DEFAULT_MIN_TIME;
+	max_time = FCRAM_REFRESH_DEFAULT_MAX_TIME;
+	sys_time = FCRAM_REFRESH_DEFAULT_SYS_TIME;
+	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	strength = FCRAM_OUTDR_NORMAL;
+	qs = FCRAM_QS_MODE_QS;
+	rs = npi_fflp_cfg_fcram_reset(handle, strength, qs);
+	if (rs != NPI_SUCCESS) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL, "failed FCRAM Reset. "));
+		return (NXGE_ERROR | rs);
+	}
+
+    access_ratio = 0xa;
+    rs = npi_fflp_cfg_fcram_access(handle, access_ratio);
+    if (rs != NPI_SUCCESS) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL, "failed FCRAM Access ratio"
+						"configuration \n"));
+		return (NXGE_ERROR | rs);
+	}
+
+	rs = npi_fflp_cfg_fcram_refresh_time(handle, min_time,
+						    max_time, sys_time);
+	if (rs != NPI_SUCCESS) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+						"failed FCRAM refresh cfg"));
+		return (NXGE_ERROR);
+	}
+
+	/* disable all the partitions until explicitly enabled */
+	for (partition = 0; partition < FFLP_FCRAM_MAX_PARTITION; partition++) {
+		rs = npi_fflp_cfg_fcram_partition_disable(handle,
+							    partition);
+		if (rs != NPI_SUCCESS) {
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+					"failed FCRAM partition"
+					" enable for partition %d ",
+					partition));
+			return (NXGE_ERROR | rs);
+		}
+	}
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, "<== nxge_fflp_fcram_init"));
+
+    return (NXGE_OK);
+
+}
+
+
+nxge_status_t
+nxge_logical_mac_assign_rdc_table(p_nxge_t nxgep, uint8_t alt_mac)
+{
+
+	npi_status_t rs = NPI_SUCCESS;
+	hostinfo_t mac_rdc;
+	npi_handle_t handle;
+	p_nxge_class_pt_cfg_t 	p_class_cfgp;
+	p_class_cfgp = (p_nxge_class_pt_cfg_t)&nxgep->class_config;
+
+	if (p_class_cfgp->mac_host_info[alt_mac].flag == 0) {
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				    " nxge_logical_mac_assign_rdc_table"
+				    " unconfigured alt MAC addr %d ",
+				    alt_mac));
+		return (NXGE_ERROR);
+	}
+
+	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	mac_rdc.value = 0;
+	mac_rdc.bits.w0.rdc_tbl_num =
+		    p_class_cfgp->mac_host_info[alt_mac].rdctbl;
+	mac_rdc.bits.w0.mac_pref = p_class_cfgp->mac_host_info[alt_mac].mpr_npr;
+
+	rs = npi_mac_hostinfo_entry(handle, OP_SET,
+				    nxgep->function_num, alt_mac, &mac_rdc);
+
+	if (rs != NPI_SUCCESS) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				    "failed Assign RDC table"));
+		return (NXGE_ERROR | rs);
+	}
+	return (NXGE_OK);
+}
+
+nxge_status_t
+nxge_main_mac_assign_rdc_table(p_nxge_t nxgep)
+{
+
+	npi_status_t rs = NPI_SUCCESS;
+	hostinfo_t mac_rdc;
+	npi_handle_t handle;
+
+	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	mac_rdc.value = 0;
+	mac_rdc.bits.w0.rdc_tbl_num = nxgep->class_config.mac_rdcgrp;
+	mac_rdc.bits.w0.mac_pref = 1;
+	switch (nxgep->function_num) {
+		case 0:
+		case 1:
+			rs = npi_mac_hostinfo_entry(handle, OP_SET,
+				nxgep->function_num,
+				XMAC_UNIQUE_HOST_INFO_ENTRY, &mac_rdc);
+			break;
+		case 2:
+		case 3:
+			rs = npi_mac_hostinfo_entry(handle, OP_SET,
+				nxgep->function_num,
+				BMAC_UNIQUE_HOST_INFO_ENTRY, &mac_rdc);
+			break;
+		default:
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"failed Assign RDC table (invalid funcion #)"));
+			return (NXGE_ERROR);
+	}
+
+	if (rs != NPI_SUCCESS) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+						"failed Assign RDC table"));
+		return (NXGE_ERROR | rs);
+	}
+	return (NXGE_OK);
+}
+
+
+nxge_status_t
+nxge_multicast_mac_assign_rdc_table(p_nxge_t nxgep)
+{
+
+	npi_status_t rs = NPI_SUCCESS;
+	hostinfo_t mac_rdc;
+	npi_handle_t handle;
+
+	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	mac_rdc.value = 0;
+	mac_rdc.bits.w0.rdc_tbl_num = nxgep->class_config.mcast_rdcgrp;
+	mac_rdc.bits.w0.mac_pref = 1;
+	switch (nxgep->function_num) {
+		case 0:
+		case 1:
+			rs = npi_mac_hostinfo_entry(handle, OP_SET,
+				nxgep->function_num,
+				XMAC_MULTI_HOST_INFO_ENTRY, &mac_rdc);
+			break;
+		case 2:
+		case 3:
+			rs = npi_mac_hostinfo_entry(handle, OP_SET,
+				nxgep->function_num,
+				BMAC_MULTI_HOST_INFO_ENTRY, &mac_rdc);
+			break;
+		default:
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"failed Assign RDC table (invalid funcion #)"));
+			return (NXGE_ERROR);
+	}
+
+	if (rs != NPI_SUCCESS) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+						"failed Assign RDC table"));
+		return (NXGE_ERROR | rs);
+	}
+	return (NXGE_OK);
+}
+
+nxge_status_t
+nxge_fflp_init_hostinfo(p_nxge_t nxgep)
+{
+	nxge_status_t status = NXGE_OK;
+	status = nxge_multicast_mac_assign_rdc_table(nxgep);
+	status = nxge_main_mac_assign_rdc_table(nxgep);
+	return (status);
+}
+
+
+nxge_status_t
+nxge_fflp_hw_reset(p_nxge_t nxgep)
+{
+	npi_handle_t handle;
+	npi_status_t rs = NPI_SUCCESS;
+	nxge_status_t status = NXGE_OK;
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, " ==> nxge_fflp_hw_reset"));
+
+
+	if (nxgep->niu_type == NEPTUNE) {
+		status = nxge_fflp_fcram_init(nxgep);
+		if (status  != NXGE_OK) {
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+					    " failed FCRAM init. "));
+			return (status);
+		}
+	}
+
+
+	status = nxge_fflp_tcam_init(nxgep);
+	if (status != NXGE_OK) {
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+					    "failed TCAM init."));
+			return (status);
+	}
+
+	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	rs = npi_fflp_cfg_llcsnap_enable(handle);
+	if (rs != NPI_SUCCESS) {
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+					"failed LLCSNAP enable. "));
+			return (NXGE_ERROR | rs);
+	}
+
+	rs = npi_fflp_cfg_cam_errorcheck_disable(handle);
+	if (rs != NPI_SUCCESS) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"failed CAM Error Check enable. "));
+		return (NXGE_ERROR | rs);
+	}
+
+/* init the hash generators */
+
+	rs = npi_fflp_cfg_hash_h1poly(handle, 0);
+	if (rs != NPI_SUCCESS) {
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+					"failed H1 Poly Init. "));
+			return (NXGE_ERROR | rs);
+	}
+
+	rs = npi_fflp_cfg_hash_h2poly(handle, 0);
+	if (rs != NPI_SUCCESS) {
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+					"failed H2 Poly Init. "));
+			return (NXGE_ERROR | rs);
+	}
+
+
+/* invalidate TCAM entries */
+	status = nxge_fflp_tcam_invalidate_all(nxgep);
+	if (status != NXGE_OK) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"failed TCAM Entry Invalidate. "));
+		return (status);
+	}
+
+/* invalidate VLAN RDC tables */
+
+	status = nxge_fflp_vlan_tbl_clear_all(nxgep);
+	if (status != NXGE_OK) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"failed VLAN Table Invalidate. "));
+		return (status);
+	}
+
+	nxgep->classifier.state |= NXGE_FFLP_HW_RESET;
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, "<== nxge_fflp_hw_reset"));
+
+	return (NXGE_OK);
+
+}
+
+
+nxge_status_t
+nxge_cfg_ip_cls_flow_key(p_nxge_t nxgep, tcam_class_t l3_class,
+			    uint32_t class_config)
+{
+	flow_key_cfg_t fcfg;
+	npi_handle_t handle;
+	npi_status_t rs = NPI_SUCCESS;
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, " ==> nxge_cfg_ip_cls_flow_key"));
+	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	memset(&fcfg, 0, sizeof (flow_key_cfg_t));
+
+	if (class_config &  NXGE_CLASS_FLOW_USE_PROTO)
+		fcfg.use_proto = 1;
+	if (class_config & NXGE_CLASS_FLOW_USE_DST_PORT)
+		fcfg.use_dport = 1;
+	if (class_config & NXGE_CLASS_FLOW_USE_SRC_PORT)
+		fcfg.use_sport = 1;
+	if (class_config & NXGE_CLASS_FLOW_USE_IPDST)
+		fcfg.use_daddr = 1;
+	if (class_config & NXGE_CLASS_FLOW_USE_IPSRC)
+		fcfg.use_saddr = 1;
+	if (class_config & NXGE_CLASS_FLOW_USE_VLAN)
+		fcfg.use_vlan = 1;
+
+	if (class_config & NXGE_CLASS_FLOW_USE_L2DA)
+		fcfg.use_l2da = 1;
+
+	if (class_config & NXGE_CLASS_FLOW_USE_PORTNUM)
+		fcfg.use_portnum = 1;
+
+	fcfg.ip_opts_exist = 0;
+
+	rs = npi_fflp_cfg_ip_cls_flow_key(handle, l3_class, &fcfg);
+	if (rs & NPI_FFLP_ERROR) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL, " nxge_cfg_ip_cls_flow_key"
+				    " opt %x for class %d failed ",
+				    class_config, l3_class));
+		return (NXGE_ERROR | rs);
+	}
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, " <== nxge_cfg_ip_cls_flow_key"));
+	return (NXGE_OK);
+}
+
+
+nxge_status_t
+nxge_cfg_ip_cls_flow_key_get(p_nxge_t nxgep, tcam_class_t l3_class,
+			    uint32_t *class_config)
+{
+	flow_key_cfg_t fcfg;
+	npi_handle_t handle;
+	npi_status_t rs = NPI_SUCCESS;
+	uint32_t ccfg = 0;
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, " ==> nxge_cfg_ip_cls_flow_key_get"));
+	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	memset(&fcfg, 0, sizeof (flow_key_cfg_t));
+
+	rs = npi_fflp_cfg_ip_cls_flow_key_get(handle, l3_class, &fcfg);
+	if (rs & NPI_FFLP_ERROR) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL, " nxge_cfg_ip_cls_flow_key"
+				    " opt %x for class %d failed ",
+				    class_config, l3_class));
+		return (NXGE_ERROR | rs);
+	}
+
+	if (fcfg.use_proto)
+		ccfg |= NXGE_CLASS_FLOW_USE_PROTO;
+
+	if (fcfg.use_dport)
+		ccfg |= NXGE_CLASS_FLOW_USE_DST_PORT;
+
+	if (fcfg.use_sport)
+		ccfg |= NXGE_CLASS_FLOW_USE_SRC_PORT;
+
+	if (fcfg.use_daddr)
+		ccfg |= NXGE_CLASS_FLOW_USE_IPDST;
+
+	if (fcfg.use_saddr)
+		ccfg |= NXGE_CLASS_FLOW_USE_IPSRC;
+
+	if (fcfg.use_vlan)
+		ccfg |= NXGE_CLASS_FLOW_USE_VLAN;
+
+	if (fcfg.use_l2da)
+		ccfg |= NXGE_CLASS_FLOW_USE_L2DA;
+
+	if (fcfg.use_portnum)
+		ccfg |=  NXGE_CLASS_FLOW_USE_PORTNUM;
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL,
+			    " nxge_cfg_ip_cls_flow_key_get %x", ccfg));
+	*class_config = ccfg;
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL,
+			    " <== nxge_cfg_ip_cls_flow_key_get"));
+	return (NXGE_OK);
+}
+
+
+static nxge_status_t
+nxge_cfg_tcam_ip_class_get(p_nxge_t nxgep, tcam_class_t class,
+			    uint32_t  *class_config)
+{
+	npi_status_t rs = NPI_SUCCESS;
+	tcam_key_cfg_t cfg;
+	npi_handle_t handle;
+	uint32_t ccfg = 0;
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, "==> nxge_cfg_tcam_ip_class"));
+
+	memset(&cfg, 0, sizeof (tcam_key_cfg_t));
+	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+
+	rs = npi_fflp_cfg_ip_cls_tcam_key_get(handle, class, &cfg);
+	if (rs & NPI_FFLP_ERROR) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL, " nxge_cfg_tcam_ip_class"
+				    " opt %x for class %d failed ",
+				    class_config, class));
+		return (NXGE_ERROR | rs);
+	}
+	if (cfg.discard)
+		ccfg |=  NXGE_CLASS_DISCARD;
+
+	if (cfg.lookup_enable)
+		ccfg |= NXGE_CLASS_TCAM_LOOKUP;
+
+	if (cfg.use_ip_daddr)
+		ccfg |= 	NXGE_CLASS_TCAM_USE_SRC_ADDR;
+
+	*class_config = ccfg;
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL,
+			    " ==> nxge_cfg_tcam_ip_class %x", ccfg));
+	return (NXGE_OK);
+}
+
+
+static nxge_status_t
+nxge_cfg_tcam_ip_class(p_nxge_t nxgep, tcam_class_t class,
+			    uint32_t  class_config)
+{
+	npi_status_t rs = NPI_SUCCESS;
+	tcam_key_cfg_t cfg;
+	npi_handle_t handle;
+	p_nxge_class_pt_cfg_t 	p_class_cfgp;
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, "==> nxge_cfg_tcam_ip_class"));
+	p_class_cfgp = (p_nxge_class_pt_cfg_t)&nxgep->class_config;
+	p_class_cfgp->class_cfg[class] = class_config;
+
+	memset(&cfg, 0, sizeof (tcam_key_cfg_t));
+	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	cfg.discard = 0;
+	cfg.lookup_enable = 0;
+	cfg.use_ip_daddr = 0;
+	if (class_config & NXGE_CLASS_DISCARD)
+		cfg.discard = 1;
+	if (class_config & NXGE_CLASS_TCAM_LOOKUP)
+		cfg.lookup_enable = 1;
+	if (class_config & 	NXGE_CLASS_TCAM_USE_SRC_ADDR)
+		cfg.use_ip_daddr = 1;
+
+	rs = npi_fflp_cfg_ip_cls_tcam_key(handle, class, &cfg);
+	if (rs & NPI_FFLP_ERROR) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL, " nxge_cfg_tcam_ip_class"
+				    " opt %x for class %d failed ",
+				    class_config, class));
+		return (NXGE_ERROR | rs);
+	}
+	return (NXGE_OK);
+}
+
+
+nxge_status_t
+nxge_fflp_set_hash1(p_nxge_t nxgep, uint32_t h1)
+{
+	npi_status_t rs = NPI_SUCCESS;
+	npi_handle_t handle;
+	p_nxge_class_pt_cfg_t 	p_class_cfgp;
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, " ==> nxge_fflp_init_h1"));
+	p_class_cfgp = (p_nxge_class_pt_cfg_t)&nxgep->class_config;
+	p_class_cfgp->init_h1 = h1;
+	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	rs = npi_fflp_cfg_hash_h1poly(handle, h1);
+	if (rs & NPI_FFLP_ERROR) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL, " nxge_fflp_init_h1"
+				    "  %x failed ",
+				    h1));
+		return (NXGE_ERROR | rs);
+	}
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, " <== nxge_fflp_init_h1"));
+	return (NXGE_OK);
+}
+
+nxge_status_t
+nxge_fflp_set_hash2(p_nxge_t nxgep, uint16_t h2)
+{
+	npi_status_t rs = NPI_SUCCESS;
+	npi_handle_t handle;
+	p_nxge_class_pt_cfg_t 	p_class_cfgp;
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, " ==> nxge_fflp_init_h2"));
+	p_class_cfgp = (p_nxge_class_pt_cfg_t)&nxgep->class_config;
+	p_class_cfgp->init_h2 = h2;
+
+	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	rs = npi_fflp_cfg_hash_h2poly(handle, h2);
+	if (rs & NPI_FFLP_ERROR) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL, " nxge_fflp_init_h2"
+				    "  %x failed ",
+				    h2));
+		return (NXGE_ERROR | rs);
+	}
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, " <== nxge_fflp_init_h2"));
+	return (NXGE_OK);
+}
+
+
+nxge_status_t
+nxge_classify_init_sw(p_nxge_t nxgep)
+{
+	int alloc_size;
+	nxge_classify_t *classify_ptr;
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, "==> nxge_classify_init_sw"));
+	classify_ptr = &nxgep->classifier;
+
+	if (classify_ptr->state & NXGE_FFLP_SW_INIT) {
+		return (NXGE_OK);
+	}
+
+		/* Init SW structures */
+
+	classify_ptr->tcam_size = TCAM_NIU_TCAM_MAX_ENTRY;
+		/* init data structures, based on HW type */
+	if (nxgep->niu_type == NEPTUNE) {
+		classify_ptr->tcam_size = TCAM_NXGE_TCAM_MAX_ENTRY;
+			/*
+			 * check if fcram based classification is required
+			 * and init the flow storage
+			 */
+	}
+
+	alloc_size = sizeof (tcam_flow_spec_t) * classify_ptr->tcam_size;
+	classify_ptr->tcam_entries = KMEM_ZALLOC(alloc_size, GFP_KERNEL);
+
+		/* Init defaults */
+		/*
+		 * add hacks required for HW shortcomings
+		 * for example, code to handle fragmented packets
+		 */
+
+	nxge_init_h1_table();
+	nxge_crc_ccitt_init();
+	nxgep->classifier.tcam_location = nxgep->function_num;
+	nxgep->classifier.fragment_bug = 1;
+	classify_ptr->state |= NXGE_FFLP_SW_INIT;
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, "<== nxge_classify_init_sw"));
+
+	return (NXGE_OK);
+
+}
+
+
+
+nxge_status_t
+nxge_classify_exit_sw(p_nxge_t nxgep)
+{
+	int alloc_size;
+	nxge_classify_t *classify_ptr;
+	int fsize;
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, "==> nxge_classify_exit_sw"));
+	classify_ptr = &nxgep->classifier;
+
+	fsize = sizeof (tcam_flow_spec_t);
+	if (classify_ptr->tcam_entries) {
+		alloc_size =  fsize * classify_ptr->tcam_size;
+		KMEM_FREE((void*)classify_ptr->tcam_entries, alloc_size);
+	}
+
+	nxgep->classifier.state = 0;
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, "<== nxge_classify_exit_sw"));
+
+	return (NXGE_OK);
+
+}
+
+
+
+/*
+ * Figures out the location where the TCAM entry is
+ * to be inserted.
+ *
+ * The current implementation is just a place holder and it
+ * returns the next tcam location.
+ * The real location determining algorithm would consider
+ * the priority, partition etc ... before deciding which
+ * location to insert.
+ *
+ */
+
+#ifdef lint
+/* ARGSUSED */
+#endif
+static tcam_location_t
+nxge_get_tcam_location(p_nxge_t nxgep, uint8_t class)
+{
+	tcam_location_t location;
+	location = nxgep->classifier.tcam_location;
+	nxgep->classifier.tcam_location = (location + nxgep->nports) %
+					    nxgep->classifier.tcam_size;
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			    "nxge_get_tcam_location: location %d next %d \n",
+			    location, nxgep->classifier.tcam_location));
+	return (location);
+}
+
+
+/*
+ * Figures out the RDC Group for the entry
+ *
+ * The current implementation is just a place holder and it
+ * returns 0.
+ * The real location determining algorithm would consider
+ * the partition etc ... before deciding w
+ *
+ */
+
+static uint8_t
+nxge_get_rdc_group(p_nxge_t nxgep, uint8_t class, uint32_t cookie)
+{
+	int use_port_rdc_grp = 0;
+	uint8_t rdc_grp = 0;
+	p_nxge_dma_pt_cfg_t	p_dma_cfgp;
+	p_nxge_hw_pt_cfg_t	p_cfgp;
+	p_nxge_rdc_grp_t	rdc_grp_p;
+	p_dma_cfgp = (p_nxge_dma_pt_cfg_t)&nxgep->pt_config;
+	p_cfgp = (p_nxge_hw_pt_cfg_t)&p_dma_cfgp->hw_config;
+	rdc_grp_p = &p_dma_cfgp->rdc_grps[use_port_rdc_grp];
+	rdc_grp = p_cfgp->start_rdc_grpid;
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+		    "nxge_get_rdc_group: grp 0x%x real_grp %x grpp $%p\n",
+		    cookie, rdc_grp, rdc_grp_p));
+	return (rdc_grp);
+}
+
+
+static uint8_t
+nxge_get_rdc_offset(p_nxge_t nxgep, uint8_t class, uint32_t cookie)
+{
+	return ((uint8_t)cookie);
+}
+
+
+static void
+nxge_fill_tcam_entry_udp(p_nxge_t nxgep, flow_spec_t *flow_spec,
+			    tcam_entry_t *tcam_ptr)
+{
+	udpip4_spec_t *fspec_key;
+	udpip4_spec_t *fspec_mask;
+
+	fspec_key = (udpip4_spec_t *)&flow_spec->uh.udpip4spec;
+	fspec_mask = (udpip4_spec_t *)&flow_spec->um.udpip4spec;
+
+	TCAM_IPV4_ADDR(tcam_ptr->ip4_dest_key, fspec_key->ip4dst);
+	TCAM_IPV4_ADDR(tcam_ptr->ip4_dest_mask, fspec_mask->ip4dst);
+	TCAM_IPV4_ADDR(tcam_ptr->ip4_src_key, fspec_key->ip4src);
+	TCAM_IPV4_ADDR(tcam_ptr->ip4_src_mask, fspec_mask->ip4src);
+
+	TCAM_IP_PORTS(tcam_ptr->ip4_port_key,
+					fspec_key->pdst, fspec_key->psrc);
+	TCAM_IP_PORTS(tcam_ptr->ip4_port_mask,
+					fspec_mask->pdst, fspec_mask->psrc);
+
+	TCAM_IP_CLASS(tcam_ptr->ip4_class_key,
+				    tcam_ptr->ip4_class_mask,
+				    TCAM_CLASS_UDP_IPV4);
+
+	TCAM_IP_PROTO(tcam_ptr->ip4_proto_key,
+				    tcam_ptr->ip4_proto_mask,
+				    IPPROTO_UDP);
+
+}
+
+static void
+nxge_fill_tcam_entry_udp_ipv6(p_nxge_t nxgep, flow_spec_t *flow_spec,
+			    tcam_entry_t *tcam_ptr)
+{
+	udpip6_spec_t *fspec_key;
+	udpip6_spec_t *fspec_mask;
+	p_nxge_class_pt_cfg_t 	p_class_cfgp;
+
+	fspec_key = (udpip6_spec_t *)&flow_spec->uh.udpip6spec;
+	fspec_mask = (udpip6_spec_t *)&flow_spec->um.udpip6spec;
+	p_class_cfgp = (p_nxge_class_pt_cfg_t)&nxgep->class_config;
+	if (p_class_cfgp->class_cfg[TCAM_CLASS_UDP_IPV6] &
+		NXGE_CLASS_TCAM_USE_SRC_ADDR) {
+		TCAM_IPV6_ADDR(tcam_ptr->ip6_ip_addr_key, fspec_key->ip6src);
+		TCAM_IPV6_ADDR(tcam_ptr->ip6_ip_addr_mask, fspec_mask->ip6src);
+	} else {
+		TCAM_IPV6_ADDR(tcam_ptr->ip6_ip_addr_key, fspec_key->ip6dst);
+		TCAM_IPV6_ADDR(tcam_ptr->ip6_ip_addr_mask, fspec_mask->ip6dst);
+	}
+
+	TCAM_IP_CLASS(tcam_ptr->ip6_class_key,
+		    tcam_ptr->ip6_class_mask, TCAM_CLASS_UDP_IPV6);
+
+
+	TCAM_IP_PROTO(tcam_ptr->ip6_nxt_hdr_key,
+		    tcam_ptr->ip6_nxt_hdr_mask, IPPROTO_UDP);
+
+	TCAM_IP_PORTS(tcam_ptr->ip6_port_key,
+			fspec_key->pdst, fspec_key->psrc);
+	TCAM_IP_PORTS(tcam_ptr->ip6_port_mask,
+			fspec_mask->pdst, fspec_mask->psrc);
+
+}
+
+static void
+nxge_fill_tcam_entry_tcp(p_nxge_t nxgep, flow_spec_t *flow_spec,
+							tcam_entry_t *tcam_ptr)
+{
+
+	tcpip4_spec_t *fspec_key;
+	tcpip4_spec_t *fspec_mask;
+
+	fspec_key = (tcpip4_spec_t *)&flow_spec->uh.tcpip4spec;
+	fspec_mask = (tcpip4_spec_t *)&flow_spec->um.tcpip4spec;
+
+
+	TCAM_IPV4_ADDR(tcam_ptr->ip4_dest_key, fspec_key->ip4dst);
+	TCAM_IPV4_ADDR(tcam_ptr->ip4_dest_mask, fspec_mask->ip4dst);
+	TCAM_IPV4_ADDR(tcam_ptr->ip4_src_key, fspec_key->ip4src);
+	TCAM_IPV4_ADDR(tcam_ptr->ip4_src_mask, fspec_mask->ip4src);
+
+	TCAM_IP_PORTS(tcam_ptr->ip4_port_key,
+				fspec_key->pdst, fspec_key->psrc);
+	TCAM_IP_PORTS(tcam_ptr->ip4_port_mask,
+				fspec_mask->pdst, fspec_mask->psrc);
+
+	TCAM_IP_CLASS(tcam_ptr->ip4_class_key,
+		    tcam_ptr->ip4_class_mask, TCAM_CLASS_TCP_IPV4);
+
+	TCAM_IP_PROTO(tcam_ptr->ip4_proto_key,
+			    tcam_ptr->ip4_proto_mask, IPPROTO_TCP);
+}
+
+static void
+nxge_fill_tcam_entry_sctp(p_nxge_t nxgep, flow_spec_t *flow_spec,
+							tcam_entry_t *tcam_ptr)
+{
+
+	tcpip4_spec_t *fspec_key;
+	tcpip4_spec_t *fspec_mask;
+
+	fspec_key = (tcpip4_spec_t *)&flow_spec->uh.tcpip4spec;
+	fspec_mask = (tcpip4_spec_t *)&flow_spec->um.tcpip4spec;
+
+	TCAM_IPV4_ADDR(tcam_ptr->ip4_dest_key, fspec_key->ip4dst);
+	TCAM_IPV4_ADDR(tcam_ptr->ip4_dest_mask, fspec_mask->ip4dst);
+	TCAM_IPV4_ADDR(tcam_ptr->ip4_src_key, fspec_key->ip4src);
+	TCAM_IPV4_ADDR(tcam_ptr->ip4_src_mask, fspec_mask->ip4src);
+
+	TCAM_IP_CLASS(tcam_ptr->ip4_class_key,
+		    tcam_ptr->ip4_class_mask, TCAM_CLASS_SCTP_IPV4);
+
+	TCAM_IP_PROTO(tcam_ptr->ip4_proto_key,
+			    tcam_ptr->ip4_proto_mask, IPPROTO_SCTP);
+
+	TCAM_IP_PORTS(tcam_ptr->ip4_port_key,
+			    fspec_key->pdst, fspec_key->psrc);
+	TCAM_IP_PORTS(tcam_ptr->ip4_port_mask,
+			    fspec_mask->pdst, fspec_mask->psrc);
+}
+
+
+
+static void
+nxge_fill_tcam_entry_tcp_ipv6(p_nxge_t nxgep, flow_spec_t *flow_spec,
+							tcam_entry_t *tcam_ptr)
+{
+
+	tcpip6_spec_t *fspec_key;
+	tcpip6_spec_t *fspec_mask;
+	p_nxge_class_pt_cfg_t 	p_class_cfgp;
+	fspec_key = (tcpip6_spec_t *)&flow_spec->uh.tcpip6spec;
+	fspec_mask = (tcpip6_spec_t *)&flow_spec->um.tcpip6spec;
+
+	p_class_cfgp = (p_nxge_class_pt_cfg_t)&nxgep->class_config;
+	if (p_class_cfgp->class_cfg[TCAM_CLASS_UDP_IPV6] &
+		NXGE_CLASS_TCAM_USE_SRC_ADDR) {
+		TCAM_IPV6_ADDR(tcam_ptr->ip6_ip_addr_key, fspec_key->ip6src);
+		TCAM_IPV6_ADDR(tcam_ptr->ip6_ip_addr_mask, fspec_mask->ip6src);
+
+	} else {
+		TCAM_IPV6_ADDR(tcam_ptr->ip6_ip_addr_key, fspec_key->ip6dst);
+		TCAM_IPV6_ADDR(tcam_ptr->ip6_ip_addr_mask, fspec_mask->ip6dst);
+	}
+
+	TCAM_IP_CLASS(tcam_ptr->ip6_class_key,
+			    tcam_ptr->ip6_class_mask, TCAM_CLASS_TCP_IPV6);
+
+
+	TCAM_IP_PROTO(tcam_ptr->ip6_nxt_hdr_key,
+			    tcam_ptr->ip6_nxt_hdr_mask, IPPROTO_TCP);
+
+	TCAM_IP_PORTS(tcam_ptr->ip6_port_key,
+			    fspec_key->pdst, fspec_key->psrc);
+	TCAM_IP_PORTS(tcam_ptr->ip6_port_mask,
+			    fspec_mask->pdst, fspec_mask->psrc);
+
+}
+
+
+static void
+nxge_fill_tcam_entry_sctp_ipv6(p_nxge_t nxgep, flow_spec_t *flow_spec,
+							tcam_entry_t *tcam_ptr)
+{
+
+	tcpip6_spec_t *fspec_key;
+	tcpip6_spec_t *fspec_mask;
+	p_nxge_class_pt_cfg_t 	p_class_cfgp;
+
+	fspec_key = (tcpip6_spec_t *)&flow_spec->uh.tcpip6spec;
+	fspec_mask = (tcpip6_spec_t *)&flow_spec->um.tcpip6spec;
+	p_class_cfgp = (p_nxge_class_pt_cfg_t)&nxgep->class_config;
+
+	if (p_class_cfgp->class_cfg[TCAM_CLASS_UDP_IPV6] &
+		NXGE_CLASS_TCAM_USE_SRC_ADDR) {
+		TCAM_IPV6_ADDR(tcam_ptr->ip6_ip_addr_key, fspec_key->ip6src);
+		TCAM_IPV6_ADDR(tcam_ptr->ip6_ip_addr_mask, fspec_mask->ip6src);
+
+	} else {
+		TCAM_IPV6_ADDR(tcam_ptr->ip6_ip_addr_key, fspec_key->ip6dst);
+		TCAM_IPV6_ADDR(tcam_ptr->ip6_ip_addr_mask, fspec_mask->ip6dst);
+	}
+
+	TCAM_IP_CLASS(tcam_ptr->ip6_class_key,
+			    tcam_ptr->ip6_class_mask, TCAM_CLASS_SCTP_IPV6);
+
+
+	TCAM_IP_PROTO(tcam_ptr->ip6_nxt_hdr_key,
+			    tcam_ptr->ip6_nxt_hdr_mask, IPPROTO_SCTP);
+
+
+	TCAM_IP_PORTS(tcam_ptr->ip6_port_key,
+			    fspec_key->pdst, fspec_key->psrc);
+	TCAM_IP_PORTS(tcam_ptr->ip6_port_mask,
+			    fspec_mask->pdst, fspec_mask->psrc);
+
+
+}
+
+
+nxge_status_t
+nxge_flow_get_hash(p_nxge_t nxgep, flow_resource_t *flow_res,
+			    uint32_t *H1, uint16_t *H2)
+{
+
+
+	uint32_t channel_cookie;
+	uint32_t flow_cookie;
+	flow_spec_t *flow_spec;
+	uint32_t class_cfg;
+	flow_template_t ft;
+	p_nxge_class_pt_cfg_t 	p_class_cfgp;
+
+	int ft_size = sizeof (flow_template_t);
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, "==> nxge_flow_get_hash"));
+
+	flow_spec = (flow_spec_t *)&flow_res->flow_spec;
+	flow_cookie = flow_res->flow_cookie;
+	channel_cookie = flow_res->channel_cookie;
+#ifdef lint
+	flow_cookie = flow_cookie;
+	channel_cookie = channel_cookie;
+#endif
+	memset((char *)&ft, 0, ft_size);
+	p_class_cfgp = (p_nxge_class_pt_cfg_t)&nxgep->class_config;
+	switch (flow_spec->flow_type) {
+		case FSPEC_TCPIP4:
+			class_cfg =
+			    p_class_cfgp->class_cfg[TCAM_CLASS_TCP_IPV4];
+			if (class_cfg & NXGE_CLASS_FLOW_USE_PROTO)
+				ft.ip_proto = IPPROTO_TCP;
+			if (class_cfg & NXGE_CLASS_FLOW_USE_IPSRC)
+			ft.ip4_saddr =
+			(uint32_t)flow_res->flow_spec.uh.tcpip4spec.ip4src;
+
+			if (class_cfg & NXGE_CLASS_FLOW_USE_IPDST)
+			ft.ip4_daddr =
+			(uint32_t)flow_res->flow_spec.uh.tcpip4spec.ip4dst;
+
+			if (class_cfg & NXGE_CLASS_FLOW_USE_SRC_PORT)
+			ft.ip_src_port =
+			(uint16_t)flow_res->flow_spec.uh.tcpip4spec.psrc;
+
+			if (class_cfg & NXGE_CLASS_FLOW_USE_DST_PORT)
+			ft.ip_dst_port =
+			(uint16_t)flow_res->flow_spec.uh.tcpip4spec.pdst;
+
+			break;
+
+		case FSPEC_UDPIP4:
+			class_cfg =
+			    p_class_cfgp->class_cfg[TCAM_CLASS_UDP_IPV4];
+			if (class_cfg & NXGE_CLASS_FLOW_USE_PROTO)
+				ft.ip_proto = IPPROTO_UDP;
+
+			if (class_cfg & NXGE_CLASS_FLOW_USE_IPSRC)
+			ft.ip4_saddr =
+			(uint32_t)flow_res->flow_spec.uh.udpip4spec.ip4src;
+
+			if (class_cfg & NXGE_CLASS_FLOW_USE_IPDST)
+				ft.ip4_daddr =
+			    (uint32_t)flow_res->flow_spec.uh.udpip4spec.ip4dst;
+
+			if (class_cfg & NXGE_CLASS_FLOW_USE_SRC_PORT)
+				ft.ip_src_port =
+				    flow_res->flow_spec.uh.udpip4spec.psrc;
+
+			if (class_cfg & NXGE_CLASS_FLOW_USE_DST_PORT)
+				ft.ip_dst_port =
+				    flow_res->flow_spec.uh.udpip4spec.pdst;
+
+			break;
+
+		default:
+			return (NXGE_ERROR);
+	}
+
+	*H1 = nxge_compute_h1(p_class_cfgp->init_h1,
+	    (uint32_t *)&ft, ft_size) & 0xfffff;
+	*H2 = nxge_compute_h2(p_class_cfgp->init_h2,
+	    (uint8_t *)&ft, ft_size);
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, "<== nxge_flow_get_hash"));
+	return (NXGE_OK);
+}
+
+/* ARGSUSED */
+nxge_status_t
+nxge_add_fcram_entry(p_nxge_t nxgep, flow_resource_t *flow_res)
+{
+
+	uint32_t H1;
+	uint16_t H2;
+	nxge_status_t status = NXGE_OK;
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, "==> nxge_add_fcram_entry"));
+	status = nxge_flow_get_hash(nxgep, flow_res, &H1, &H2);
+	if (status != NXGE_OK) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			    " nxge_add_fcram_entry failed "));
+		return (status);
+	}
+
+		/* add code to find rdc grp */
+		/* add code to determine the action */
+		/* Determine type of match (exact, optimistic etc ...) */
+		/* Compose hash entry */
+		/* add hash entry */
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, "<== nxge_add_fcram_entry"));
+	return (NXGE_OK);
+}
+
+
+/*
+ * Already decided this flow goes into the tcam
+ */
+
+nxge_status_t
+nxge_add_tcam_entry(p_nxge_t nxgep, flow_resource_t *flow_res)
+{
+
+	npi_handle_t handle;
+	uint32_t channel_cookie;
+	uint32_t flow_cookie;
+	flow_spec_t *flow_spec;
+	npi_status_t rs = NPI_SUCCESS;
+	tcam_entry_t tcam_ptr;
+	tcam_location_t location = 0;
+	uint8_t offset, rdc_grp;
+
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, "==> nxge_add_tcam_entry"));
+	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+
+	memset((void*)&tcam_ptr, 0, sizeof (tcam_entry_t));
+	flow_spec = (flow_spec_t *)&flow_res->flow_spec;
+	flow_cookie = flow_res->flow_cookie;
+	channel_cookie = flow_res->channel_cookie;
+
+	switch (flow_spec->flow_type) {
+		case FSPEC_TCPIP4:
+
+			nxge_fill_tcam_entry_tcp(nxgep, flow_spec, &tcam_ptr);
+			location = nxge_get_tcam_location(nxgep,
+						    TCAM_CLASS_TCP_IPV4);
+			rdc_grp = nxge_get_rdc_group(nxgep, TCAM_CLASS_TCP_IPV4,
+							    flow_cookie);
+			offset = nxge_get_rdc_offset(nxgep, TCAM_CLASS_TCP_IPV4,
+							    channel_cookie);
+
+			break;
+
+
+		case FSPEC_UDPIP4:
+
+			nxge_fill_tcam_entry_udp(nxgep, flow_spec, &tcam_ptr);
+			location = nxge_get_tcam_location(nxgep,
+						    TCAM_CLASS_UDP_IPV4);
+			rdc_grp = nxge_get_rdc_group(nxgep,
+					    TCAM_CLASS_UDP_IPV4,
+					    flow_cookie);
+			offset = nxge_get_rdc_offset(nxgep,
+					    TCAM_CLASS_UDP_IPV4,
+					    channel_cookie);
+
+			break;
+
+
+		case FSPEC_TCPIP6:
+
+			nxge_fill_tcam_entry_tcp_ipv6(nxgep,
+						    flow_spec, &tcam_ptr);
+			location = nxge_get_tcam_location(nxgep,
+						    TCAM_CLASS_TCP_IPV6);
+			rdc_grp = nxge_get_rdc_group(nxgep, TCAM_CLASS_TCP_IPV6,
+							    flow_cookie);
+			offset = nxge_get_rdc_offset(nxgep, TCAM_CLASS_TCP_IPV6,
+							    channel_cookie);
+
+			break;
+
+		case FSPEC_UDPIP6:
+
+			nxge_fill_tcam_entry_udp_ipv6(nxgep,
+						    flow_spec, &tcam_ptr);
+
+			location = nxge_get_tcam_location(nxgep,
+						    TCAM_CLASS_UDP_IPV6);
+			rdc_grp = nxge_get_rdc_group(nxgep,
+					    TCAM_CLASS_UDP_IPV6,
+					    channel_cookie);
+			offset = nxge_get_rdc_offset(nxgep,
+					    TCAM_CLASS_UDP_IPV6,
+					    flow_cookie);
+
+			break;
+
+		case FSPEC_SCTPIP4:
+
+			nxge_fill_tcam_entry_sctp(nxgep, flow_spec, &tcam_ptr);
+			location = nxge_get_tcam_location(nxgep,
+						    TCAM_CLASS_SCTP_IPV4);
+			rdc_grp = nxge_get_rdc_group(nxgep,
+						    TCAM_CLASS_SCTP_IPV4,
+						    channel_cookie);
+			offset = nxge_get_rdc_offset(nxgep,
+						    TCAM_CLASS_SCTP_IPV4,
+						    flow_cookie);
+
+			break;
+
+		case FSPEC_SCTPIP6:
+
+			nxge_fill_tcam_entry_sctp_ipv6(nxgep,
+						    flow_spec, &tcam_ptr);
+			location = nxge_get_tcam_location(nxgep,
+						    TCAM_CLASS_SCTP_IPV4);
+			rdc_grp = nxge_get_rdc_group(nxgep,
+						    TCAM_CLASS_SCTP_IPV6,
+						    channel_cookie);
+			offset = nxge_get_rdc_offset(nxgep,
+						    TCAM_CLASS_SCTP_IPV6,
+						    flow_cookie);
+
+			break;
+
+
+		default:
+			return (NXGE_OK);
+	}
+
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			    " nxge_add_tcam_entry write"
+			    " for location %d offset %d", location, offset));
+	MUTEX_ENTER(nxgep->tcam_lock);
+	rs = npi_fflp_tcam_entry_write(handle,
+					    location, &tcam_ptr);
+
+	if (rs & NPI_FFLP_ERROR) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				    " nxge_add_tcam_entry write"
+				    " failed for location %d",
+				    location));
+		MUTEX_EXIT(nxgep->tcam_lock);
+		return (NXGE_ERROR | rs);
+	}
+	tcam_ptr.match_action.value = 0; /* TODO need to do more ECC etc .... */
+	tcam_ptr.match_action.bits.ldw.rdctbl = rdc_grp;
+	tcam_ptr.match_action.bits.ldw.offset = offset;
+	tcam_ptr.match_action.bits.ldw.tres =
+		TRES_TERM_OVRD_L2RDC;
+	if (channel_cookie == 0xff) {
+		tcam_ptr.match_action.bits.ldw.disc = 1;
+	}
+	rs = npi_fflp_tcam_asc_ram_entry_write(handle,
+					    location,
+					    tcam_ptr.match_action.value);
+
+	if (rs & NPI_FFLP_ERROR) {
+		MUTEX_EXIT(nxgep->tcam_lock);
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				    " nxge_add_tcam_entry write"
+				    " failed for ASC RAM location %d",
+				    location));
+
+		return (NXGE_ERROR | rs);
+	}
+
+	memcpy((void *)&nxgep->classifier.tcam_entries[location].tce,
+		    (void *)&tcam_ptr,
+		    sizeof (tcam_entry_t));
+	MUTEX_EXIT(nxgep->tcam_lock);
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, "<== nxge_add_tcam_entry"));
+	return (NXGE_OK);
+}
+
+
+nxge_status_t
+nxge_handle_tcam_fragment(p_nxge_t nxgep)
+{
+	tcam_entry_t tcam_ptr;
+	tcam_location_t location;
+	uint32_t class_config;
+	tcam_class_t class;
+	npi_handle_t handle;
+	npi_status_t rs = NPI_SUCCESS;
+	nxge_status_t status = NXGE_OK;
+
+	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	class = 0;
+	memset((void*)&tcam_ptr, 0, sizeof (tcam_entry_t));
+	tcam_ptr.ip4_noport_key = 1;
+	tcam_ptr.ip4_noport_mask = 1;
+	location = nxgep->function_num;
+
+	nxgep->classifier.fragment_bug_location = location;
+
+	MUTEX_ENTER(nxgep->tcam_lock);
+
+	rs = npi_fflp_tcam_entry_write(handle,
+					    location, &tcam_ptr);
+
+	if (rs & NPI_FFLP_ERROR) {
+		MUTEX_EXIT(nxgep->tcam_lock);
+
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				    " nxge_handle_tcam_fragment_bug "
+				    " tcam_entry write"
+				    " failed for location %d",
+				    location));
+		return (NXGE_ERROR);
+	}
+
+	tcam_ptr.match_action.bits.ldw.rdctbl = nxgep->class_config.mac_rdcgrp;
+	tcam_ptr.match_action.bits.ldw.offset = 0; /* use the default */
+	tcam_ptr.match_action.bits.ldw.tres =
+		TRES_TERM_USE_OFFSET;
+
+	rs = npi_fflp_tcam_asc_ram_entry_write(handle,
+					    location,
+					    tcam_ptr.match_action.value);
+
+	if (rs & NPI_FFLP_ERROR) {
+		MUTEX_EXIT(nxgep->tcam_lock);
+
+		NXGE_DEBUG_MSG((nxgep,
+				    FFLP_CTL,
+				    " nxge_handle_tcam_fragment_bug "
+				    " tcam_entry write"
+				    " failed for ASC RAM location %d",
+				    location));
+		return (NXGE_ERROR);
+	}
+
+	memcpy((void *)&nxgep->classifier.tcam_entries[location].tce,
+			(void *)&tcam_ptr,
+		    sizeof (tcam_entry_t));
+
+	for (class = TCAM_CLASS_TCP_IPV4;
+		    class <= TCAM_CLASS_SCTP_IPV6; class++) {
+		class_config = nxgep->class_config.class_cfg[class];
+		class_config |= NXGE_CLASS_TCAM_LOOKUP;
+		status = nxge_fflp_ip_class_config(nxgep, class, class_config);
+
+		if (status & NPI_FFLP_ERROR) {
+			MUTEX_EXIT(nxgep->tcam_lock);
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+						"nxge_fflp_ip_class_config failed "
+						" class %d config %x ",
+						class, class_config));
+			return (NXGE_ERROR);
+		}
+	}
+
+	rs = npi_fflp_cfg_tcam_enable(handle);
+	if (rs & NPI_FFLP_ERROR) {
+		MUTEX_EXIT(nxgep->tcam_lock);
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			    " nxge_fflp_config_tcam_enable failed"));
+		return (NXGE_ERROR);
+	}
+	nxgep->classifier.tcam_location = (location + nxgep->nports) %
+					    nxgep->classifier.tcam_size;
+	MUTEX_EXIT(nxgep->tcam_lock);
+	return (status);
+}
+
+nxge_status_t
+nxge_add_flow(p_nxge_t nxgep, flow_resource_t *flow_res)
+{
+
+	nxge_status_t status = NXGE_OK;
+	status = nxge_add_tcam_entry(nxgep, flow_res);
+
+	return (status);
+}
+
+void
+nxge_put_tcam(p_nxge_t nxgep, void *data)
+{
+
+	flow_resource_t *fs;
+	fs = (flow_resource_t *)data;
+
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			    "nxge_put_tcam addr fs $%p  type %x offset %x",
+			    fs, fs->flow_spec.flow_type, fs->channel_cookie));
+
+	(void) nxge_add_tcam_entry(nxgep, fs);
+}
+
+
+nxge_status_t
+nxge_fflp_config_tcam_enable(p_nxge_t nxgep)
+{
+	npi_handle_t handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	npi_status_t rs = NPI_SUCCESS;
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, " ==> nxge_fflp_config_tcam_enable"));
+	rs = npi_fflp_cfg_tcam_enable(handle);
+	if (rs & NPI_FFLP_ERROR) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			    " nxge_fflp_config_tcam_enable failed"));
+		return (NXGE_ERROR | rs);
+	}
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, " <== nxge_fflp_config_tcam_enable"));
+	return (NXGE_OK);
+}
+
+nxge_status_t
+nxge_fflp_config_tcam_disable(p_nxge_t nxgep)
+{
+	npi_handle_t handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	npi_status_t rs = NPI_SUCCESS;
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL,
+			    " ==> nxge_fflp_config_tcam_disable"));
+	rs = npi_fflp_cfg_tcam_disable(handle);
+	if (rs & NPI_FFLP_ERROR) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			    " nxge_fflp_config_tcam_disable failed"));
+		return (NXGE_ERROR | rs);
+	}
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL,
+			    " <== nxge_fflp_config_tcam_disable"));
+	return (NXGE_OK);
+}
+
+
+
+nxge_status_t
+nxge_fflp_config_hash_lookup_enable(p_nxge_t nxgep)
+{
+	npi_handle_t handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	npi_status_t rs = NPI_SUCCESS;
+	p_nxge_dma_pt_cfg_t	p_dma_cfgp;
+	p_nxge_hw_pt_cfg_t	p_cfgp;
+	uint8_t		partition;
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL,
+			    " ==> nxge_fflp_config_hash_lookup_enable"));
+	p_dma_cfgp = (p_nxge_dma_pt_cfg_t)&nxgep->pt_config;
+	p_cfgp = (p_nxge_hw_pt_cfg_t)&p_dma_cfgp->hw_config;
+
+
+	for (partition = p_cfgp->start_rdc_grpid;
+		partition < p_cfgp->max_rdc_grpids; partition++) {
+		rs = npi_fflp_cfg_fcram_partition_enable(handle,
+							    partition);
+		if (rs != NPI_SUCCESS) {
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+					" nxge_fflp_config_hash_lookup_enable"
+					"failed FCRAM partition"
+					" enable for partition %d ",
+					partition));
+			return (NXGE_ERROR | rs);
+		}
+	}
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL,
+			    " <== nxge_fflp_config_hash_lookup_enable"));
+	return (NXGE_OK);
+}
+
+nxge_status_t
+nxge_fflp_config_hash_lookup_disable(p_nxge_t nxgep)
+{
+	npi_handle_t handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	npi_status_t rs = NPI_SUCCESS;
+	p_nxge_dma_pt_cfg_t	p_dma_cfgp;
+	p_nxge_hw_pt_cfg_t	p_cfgp;
+	uint8_t		partition;
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL,
+			    " ==> nxge_fflp_config_hash_lookup_disable"));
+	p_dma_cfgp = (p_nxge_dma_pt_cfg_t)&nxgep->pt_config;
+	p_cfgp = (p_nxge_hw_pt_cfg_t)&p_dma_cfgp->hw_config;
+
+
+	for (partition = p_cfgp->start_rdc_grpid;
+		partition < p_cfgp->max_rdc_grpids; partition++) {
+		rs = npi_fflp_cfg_fcram_partition_disable(handle,
+							    partition);
+		if (rs != NPI_SUCCESS) {
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+					" nxge_fflp_config_hash_lookup_disable"
+					" failed FCRAM partition"
+					" disable for partition %d ",
+					partition));
+			return (NXGE_ERROR | rs);
+		}
+	}
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL,
+			    " <== nxge_fflp_config_hash_lookup_disable"));
+	return (NXGE_OK);
+}
+
+
+nxge_status_t
+nxge_fflp_config_llc_snap_enable(p_nxge_t nxgep)
+{
+	npi_handle_t handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	npi_status_t rs = NPI_SUCCESS;
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL,
+			    " ==> nxge_fflp_config_llc_snap_enable"));
+	rs = npi_fflp_cfg_llcsnap_enable(handle);
+	if (rs & NPI_FFLP_ERROR) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			    " nxge_fflp_config_llc_snap_enable failed"));
+		return (NXGE_ERROR | rs);
+	}
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL,
+			    " <== nxge_fflp_config_llc_snap_enable"));
+	return (NXGE_OK);
+}
+
+nxge_status_t
+nxge_fflp_config_llc_snap_disable(p_nxge_t nxgep)
+{
+	npi_handle_t handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	npi_status_t rs = NPI_SUCCESS;
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL,
+			    " ==> nxge_fflp_config_llc_snap_disable"));
+	rs = npi_fflp_cfg_llcsnap_disable(handle);
+	if (rs & NPI_FFLP_ERROR) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			    " nxge_fflp_config_llc_snap_disable failed"));
+		return (NXGE_ERROR | rs);
+	}
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL,
+			    " <== nxge_fflp_config_llc_snap_disable"));
+	return (NXGE_OK);
+}
+
+
+
+nxge_status_t
+nxge_fflp_ip_usr_class_config(p_nxge_t nxgep, tcam_class_t class,
+			    uint32_t config)
+{
+	npi_status_t rs = NPI_SUCCESS;
+	npi_handle_t handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	uint8_t tos, tos_mask, proto, ver = 0;
+	uint8_t class_enable = 0;
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, "==> nxge_fflp_ip_usr_class_config"));
+
+	tos = (config & NXGE_CLASS_CFG_IP_TOS_MASK) >>
+			    NXGE_CLASS_CFG_IP_TOS_SHIFT;
+	tos_mask = (config & NXGE_CLASS_CFG_IP_TOS_MASK_MASK) >>
+		NXGE_CLASS_CFG_IP_TOS_MASK_SHIFT;
+	proto = (config & NXGE_CLASS_CFG_IP_PROTO_MASK) >>
+		NXGE_CLASS_CFG_IP_PROTO_SHIFT;
+
+	if (config & NXGE_CLASS_CFG_IP_IPV6_MASK)
+		ver = 1;
+	if (config & NXGE_CLASS_CFG_IP_ENABLE_MASK)
+		class_enable = 1;
+
+	rs = npi_fflp_cfg_ip_usr_cls_set(handle, class, tos, tos_mask,
+		    proto, ver);
+	if (rs & NPI_FFLP_ERROR) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			" nxge_fflp_ip_usr_class_config"
+			" for class %d failed ", class));
+		return (NXGE_ERROR | rs);
+	}
+
+	if (class_enable)
+		rs = npi_fflp_cfg_ip_usr_cls_enable(handle, class);
+	else
+		rs = npi_fflp_cfg_ip_usr_cls_disable(handle, class);
+
+	if (rs & NPI_FFLP_ERROR) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			    " nxge_fflp_ip_usr_class_config"
+			    " TCAM enable/disable for class %d failed ",
+			    class));
+		return (NXGE_ERROR | rs);
+	}
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, "<== nxge_fflp_ip_usr_class_config"));
+	return (NXGE_OK);
+}
+
+
+
+nxge_status_t
+nxge_fflp_ip_class_config(p_nxge_t nxgep, tcam_class_t class,
+				    uint32_t config)
+{
+
+	uint32_t class_config;
+	nxge_status_t t_status = NXGE_OK;
+	nxge_status_t f_status = NXGE_OK;
+	p_nxge_class_pt_cfg_t 	p_class_cfgp;
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, " ==> nxge_fflp_ip_class_config"));
+	p_class_cfgp = (p_nxge_class_pt_cfg_t)&nxgep->class_config;
+	class_config = p_class_cfgp->class_cfg[class];
+
+	if (class_config != config) {
+		p_class_cfgp->class_cfg[class] = config;
+		class_config = config;
+	}
+
+	t_status = nxge_cfg_tcam_ip_class(nxgep, class, class_config);
+	f_status = nxge_cfg_ip_cls_flow_key(nxgep, class, class_config);
+
+	if (t_status & NPI_FFLP_ERROR) {
+		NXGE_DEBUG_MSG((nxgep, FFLP_CTL,
+				    " nxge_fflp_ip_class_config %x"
+				    " for class %d tcam failed", config,
+					class));
+		return (t_status);
+	}
+
+	if (f_status & NPI_FFLP_ERROR) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				    " nxge_fflp_ip_class_config %x"
+				    " for class %d flow key failed", config,
+					class));
+		return (f_status);
+	}
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, "<== nxge_fflp_ip_class_config"));
+	return (NXGE_OK);
+}
+
+
+nxge_status_t
+nxge_fflp_ip_class_config_get(p_nxge_t nxgep, tcam_class_t class,
+				    uint32_t *config)
+{
+
+	uint32_t t_class_config, f_class_config;
+	int t_status = NXGE_OK;
+	int f_status = NXGE_OK;
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, " ==> nxge_fflp_ip_class_config"));
+	t_class_config = f_class_config = 0;
+	t_status = nxge_cfg_tcam_ip_class_get(nxgep, class, &t_class_config);
+	f_status = nxge_cfg_ip_cls_flow_key_get(nxgep, class, &f_class_config);
+
+
+	if (t_status & NPI_FFLP_ERROR) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				    " nxge_fflp_ip_class_config_get  "
+				    " for class %d tcam failed", class));
+		return (t_status);
+	}
+
+	if (f_status & NPI_FFLP_ERROR) {
+		NXGE_DEBUG_MSG((nxgep, FFLP_CTL,
+				    " nxge_fflp_ip_class_config_get  "
+				    " for class %d flow key failed", class));
+		return (f_status);
+	}
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL,
+			    " nxge_fflp_ip_class_config tcam %x flow %x",
+			    t_class_config, f_class_config));
+
+	*config = t_class_config | f_class_config;
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, "<== nxge_fflp_ip_class_config_get"));
+	return (NXGE_OK);
+}
+
+
+nxge_status_t
+nxge_fflp_ip_class_config_all(p_nxge_t nxgep)
+{
+
+	uint32_t class_config;
+	tcam_class_t class;
+
+#ifdef	NXGE_DEBUG
+	int status = NXGE_OK;
+#endif
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, "==> nxge_fflp_ip_class_config"));
+	for (class = TCAM_CLASS_TCP_IPV4;
+		    class <= TCAM_CLASS_SCTP_IPV6; class++) {
+		class_config = nxgep->class_config.class_cfg[class];
+#ifndef	NXGE_DEBUG
+		(void) nxge_fflp_ip_class_config(nxgep, class, class_config);
+#else
+		status = nxge_fflp_ip_class_config(nxgep, class, class_config);
+		if (status & NPI_FFLP_ERROR) {
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+					"nxge_fflp_ip_class_config failed "
+					" class %d config %x ",
+					class, class_config));
+		}
+#endif
+	}
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, "<== nxge_fflp_ip_class_config"));
+	return (NXGE_OK);
+}
+
+
+
+nxge_status_t
+nxge_fflp_config_vlan_table(p_nxge_t nxgep, uint16_t vlan_id)
+{
+	uint8_t port, rdc_grp;
+	npi_handle_t handle;
+	npi_status_t rs = NPI_SUCCESS;
+	uint8_t priority = 1;
+	p_nxge_mv_cfg_t vlan_table;
+	p_nxge_class_pt_cfg_t 	p_class_cfgp;
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, "==> nxge_fflp_config_vlan_table"));
+	p_class_cfgp = (p_nxge_class_pt_cfg_t)&nxgep->class_config;
+	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	vlan_table = p_class_cfgp->vlan_tbl;
+	port = nxgep->function_num;
+
+	if (vlan_table[vlan_id].flag == 0) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				    " nxge_fflp_config_vlan_table"
+				    " vlan id is not configured %d", vlan_id));
+		return (NXGE_ERROR);
+	}
+
+	rdc_grp = vlan_table[vlan_id].rdctbl;
+	MUTEX_ENTER(nxgep->vlan_lock);
+	rs = npi_fflp_cfg_enet_vlan_table_assoc(handle,
+						    port, vlan_id,
+						    rdc_grp, priority);
+	MUTEX_EXIT(nxgep->vlan_lock);
+
+	if (rs & NPI_FFLP_ERROR) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+					"nxge_fflp_config_vlan_table failed "
+					" Port %d vlan_id %d rdc_grp %d",
+					port, vlan_id, rdc_grp));
+		return (NXGE_ERROR | rs);
+	}
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, "<== nxge_fflp_config_vlan_table"));
+	return (NXGE_OK);
+}
+
+nxge_status_t
+nxge_fflp_update_hw(p_nxge_t nxgep)
+{
+
+	nxge_status_t status = NXGE_OK;
+	p_nxge_param_t pa;
+	uint64_t cfgd_vlans;
+	uint64_t *val_ptr;
+	int i;
+	int num_macs;
+	uint8_t alt_mac;
+	nxge_param_map_t *p_map;
+
+	p_nxge_mv_cfg_t vlan_table;
+	p_nxge_class_pt_cfg_t 	p_class_cfgp;
+	p_nxge_dma_pt_cfg_t	p_all_cfgp;
+	p_nxge_hw_pt_cfg_t	p_cfgp;
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, "==> nxge_fflp_update_hw"));
+	p_class_cfgp = (p_nxge_class_pt_cfg_t)&nxgep->class_config;
+	p_all_cfgp = (p_nxge_dma_pt_cfg_t)&nxgep->pt_config;
+	p_cfgp = (p_nxge_hw_pt_cfg_t)&p_all_cfgp->hw_config;
+
+	status = nxge_fflp_set_hash1(nxgep, p_class_cfgp->init_h1);
+	if (status != NXGE_OK) {
+		NXGE_DEBUG_MSG((nxgep, FFLP_CTL,
+		    "nxge_fflp_set_hash1 Failed"));
+		return (NXGE_ERROR);
+	}
+
+	status = nxge_fflp_set_hash2(nxgep, p_class_cfgp->init_h2);
+	if (status != NXGE_OK) {
+		NXGE_DEBUG_MSG((nxgep, FFLP_CTL,
+		    "nxge_fflp_set_hash2 Failed"));
+		return (NXGE_ERROR);
+	}
+
+	vlan_table = p_class_cfgp->vlan_tbl;
+
+/* configure vlan tables */
+	pa = (p_nxge_param_t)&nxgep->param_arr[param_vlan_2rdc_grp];
+	val_ptr = (uint64_t *)((uint64_t)(pa->value));
+	cfgd_vlans = ((pa->type &  NXGE_PARAM_ARRAY_CNT_MASK) >>
+			    NXGE_PARAM_ARRAY_CNT_SHIFT);
+
+	for (i = 0; i < cfgd_vlans; i++) {
+		p_map = (nxge_param_map_t *)&val_ptr[i];
+		if (vlan_table[p_map->param_id].flag) {
+			status = nxge_fflp_config_vlan_table(nxgep,
+							    p_map->param_id);
+			if (status != NXGE_OK) {
+				NXGE_DEBUG_MSG((nxgep, FFLP_CTL,
+				    "nxge_fflp_config_vlan_table Failed"));
+				return (NXGE_ERROR);
+			}
+		}
+	}
+		/* config MAC addresses */
+	num_macs = p_cfgp->max_macs;
+	pa = (p_nxge_param_t)&nxgep->param_arr[param_mac_2rdc_grp];
+	val_ptr = (uint64_t *)((uint64_t)(pa->value));
+
+	for (alt_mac = 0; alt_mac < num_macs; alt_mac++) {
+		if (p_class_cfgp->mac_host_info[alt_mac].flag) {
+			status = nxge_logical_mac_assign_rdc_table(nxgep,
+								    alt_mac);
+			if (status != NXGE_OK) {
+				NXGE_DEBUG_MSG((nxgep, FFLP_CTL,
+				"nxge_logical_mac_assign_rdc_table Failed"));
+				return (NXGE_ERROR);
+			}
+		}
+	}
+		/* Config Hash values */
+		/* config classess */
+	status = nxge_fflp_ip_class_config_all(nxgep);
+	if (status != NXGE_OK) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+		    "nxge_fflp_ip_class_config_all Failed"));
+		return (NXGE_ERROR);
+	}
+
+	return (NXGE_OK);
+}
+
+
+nxge_status_t
+nxge_classify_init_hw(p_nxge_t nxgep)
+{
+	nxge_status_t status = NXGE_OK;
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, "==> nxge_classify_init_hw"));
+
+	if (nxgep->classifier.state & NXGE_FFLP_HW_INIT) {
+		return (NXGE_OK);
+	}
+
+
+/* Now do a real configuration */
+	status = nxge_fflp_update_hw(nxgep);
+	if (status != NXGE_OK) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+		    "nxge_fflp_update_hw failed"));
+		return (NXGE_ERROR);
+	}
+/* Init RDC tables? ? who should do that? rxdma or fflp ? */
+/* attach rdc table to the MAC port. */
+	status = nxge_main_mac_assign_rdc_table(nxgep);
+	if (status != NXGE_OK) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+		    "nxge_main_mac_assign_rdc_table failed"));
+		return (NXGE_ERROR);
+	}
+	status = nxge_multicast_mac_assign_rdc_table(nxgep);
+	if (status != NXGE_OK) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+		    "nxge_multicast_mac_assign_rdc_table failed"));
+		return (NXGE_ERROR);
+	}
+	status = nxge_handle_tcam_fragment(nxgep);
+	if (status != NXGE_OK) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+						"nxge_handle_tcam_fragment failed"));
+		return (NXGE_ERROR);
+	}
+
+	nxgep->classifier.state |= NXGE_FFLP_HW_INIT;
+
+	NXGE_DEBUG_MSG((nxgep, FFLP_CTL, "<== nxge_classify_init_hw"));
+
+	return (NXGE_OK);
+
+}
+
+nxge_status_t
+nxge_fflp_handle_sys_errors(p_nxge_t nxgep)
+{
+	npi_handle_t		handle;
+	p_nxge_fflp_stats_t	statsp;
+	uint8_t			portn, rdc_grp;
+	p_nxge_dma_pt_cfg_t	p_dma_cfgp;
+	p_nxge_hw_pt_cfg_t	p_cfgp;
+	vlan_par_err_t		vlan_err;
+	tcam_err_t		tcam_err;
+	hash_lookup_err_log1_t	fcram1_err;
+	hash_lookup_err_log2_t	fcram2_err;
+	hash_tbl_data_log_t	fcram_err;
+	boolean_t this_port_err = B_FALSE;
+	handle = nxgep->npi_handle;
+	statsp = (p_nxge_fflp_stats_t)&nxgep->statsp->fflp_stats;
+	portn = nxgep->mac.portnum;
+
+	/*
+	 * need to read the fflp error registers to figure out
+	 * what the error is
+	 */
+	npi_fflp_vlan_error_get(handle, &vlan_err);
+	npi_fflp_tcam_error_get(handle, &tcam_err);
+
+	if (vlan_err.bits.ldw.m_err || vlan_err.bits.ldw.err) {
+		NXGE_ERROR_MSG((nxgep, FFLP_CTL,
+				    " vlan table parity error on port %d"
+				    " addr: 0x%x data: 0x%x",
+				    portn, vlan_err.bits.ldw.addr,
+				    vlan_err.bits.ldw.data));
+		statsp->vlan_parity_err++;
+		this_port_err = B_TRUE;
+		if (vlan_err.bits.ldw.m_err) {
+			NXGE_ERROR_MSG((nxgep, FFLP_CTL,
+				    " vlan table multiple errors on port %d",
+				    portn));
+		}
+
+		statsp->errlog.vlan = (uint32_t)vlan_err.value;
+		npi_fflp_vlan_error_clear(handle);
+	}
+
+	if (tcam_err.bits.ldw.err) {
+		this_port_err = B_TRUE;
+		if (tcam_err.bits.ldw.p_ecc != 0) {
+			NXGE_ERROR_MSG((nxgep, FFLP_CTL,
+				    " TCAM ECC error on port %d"
+				    " TCAM entry: 0x%x syndrome: 0x%x",
+				    portn, tcam_err.bits.ldw.addr,
+				    tcam_err.bits.ldw.syndrome));
+			statsp->tcam_ecc_err++;
+		} else {
+			NXGE_ERROR_MSG((nxgep, FFLP_CTL,
+				    " TCAM Parity error on port %d"
+				    " addr: 0x%x parity value: 0x%x",
+				    portn, tcam_err.bits.ldw.addr,
+				    tcam_err.bits.ldw.syndrome));
+			statsp->tcam_parity_err++;
+		}
+
+		if (tcam_err.bits.ldw.mult) {
+			NXGE_ERROR_MSG((nxgep, FFLP_CTL,
+				    " TCAM Multiple errors on port %d",
+				    portn));
+		} else {
+			NXGE_ERROR_MSG((nxgep, FFLP_CTL,
+					    " TCAM PIO error on port %d",
+					    portn));
+		}
+
+		statsp->errlog.tcam = (uint32_t)tcam_err.value;
+		npi_fflp_tcam_error_clear(handle);
+	}
+
+	p_dma_cfgp = (p_nxge_dma_pt_cfg_t)&nxgep->pt_config;
+	p_cfgp = (p_nxge_hw_pt_cfg_t)&p_dma_cfgp->hw_config;
+
+	for (rdc_grp = p_cfgp->start_rdc_grpid;
+		    rdc_grp < p_cfgp->max_rdc_grpids; rdc_grp++) {
+		npi_fflp_fcram_error_get(handle, &fcram_err, rdc_grp);
+		if (fcram_err.bits.ldw.pio_err) {
+			this_port_err = B_TRUE;
+			NXGE_ERROR_MSG((nxgep, FFLP_CTL,
+				    " FCRAM PIO ECC error on port %d"
+				    " rdc group: %d Hash Table addr: 0x%x"
+				    " syndrome: 0x%x",
+				    portn, rdc_grp,
+				    fcram_err.bits.ldw.fcram_addr,
+				    fcram_err.bits.ldw.syndrome));
+			statsp->hash_pio_err[rdc_grp]++;
+			statsp->errlog.hash_pio[rdc_grp] =
+						(uint32_t)fcram_err.value;
+
+			npi_fflp_fcram_error_clear(handle, rdc_grp);
+		}
+	}
+
+	npi_fflp_fcram_error_log1_get(handle, &fcram1_err);
+	if (fcram1_err.bits.ldw.ecc_err) {
+		char *multi_str = "";
+		char *multi_bit_str = "";
+		this_port_err = B_TRUE;
+		npi_fflp_fcram_error_log2_get(handle, &fcram2_err);
+		if (fcram1_err.bits.ldw.mult_lk) {
+			multi_str = "multiple";
+		}
+		if (fcram1_err.bits.ldw.mult_bit) {
+			multi_bit_str = "multiple bits";
+		}
+		NXGE_ERROR_MSG((nxgep, FFLP_CTL,
+				    " FCRAM %s lookup %s ECC error on port %d"
+				    " H1: 0x%x Subarea: 0x%x Syndrome: 0x%x",
+				    multi_str, multi_bit_str, portn,
+				    fcram2_err.bits.ldw.h1,
+				    fcram2_err.bits.ldw.subarea,
+				    fcram2_err.bits.ldw.syndrome));
+	}
+	statsp->errlog.hash_lookup1 = (uint32_t)fcram1_err.value;
+	statsp->errlog.hash_lookup2 = (uint32_t)fcram2_err.value;
+	if (this_port_err == B_TRUE) {
+		MUTEX_ENTER(nxgep->syserr_lock);
+		*(nxgep->syserr) &= ~NXGE_SYSERR_FFLP;
+		MUTEX_EXIT(nxgep->syserr_lock);
+
+	}
+
+	return (NXGE_OK);
+}
+
+
+
+
+nxge_status_t
+nxge_classify_init(p_nxge_t nxgep)
+{
+	nxge_status_t	status = NXGE_OK;
+
+	status = nxge_classify_init_sw(nxgep);
+	if (status != NXGE_OK)
+		return (status);
+	status = nxge_set_hw_classify_config(nxgep);
+	if (status != NXGE_OK)
+		return (status);
+
+	status = nxge_classify_init_hw(nxgep);
+	if (status != NXGE_OK)
+		return (status);
+
+	return (NXGE_OK);
+}
+
+/* ARGSUSED */
+uint64_t
+nxge_classify_get_cfg_value(p_nxge_t nxgep, uint8_t cfg_type,
+				    uint8_t cfg_param)
+{
+	uint64_t cfg_value;
+	if (cfg_param >= NXGE_CLASS_CONFIG_PARAMS)
+		return (-1);
+	switch (cfg_type) {
+		case CFG_L3_WEB:
+			cfg_value = class_quick_config_web_server[cfg_param];
+			break;
+		case CFG_L3_DISTRIBUTE:
+		default:
+			cfg_value = class_quick_config_distribute[cfg_param];
+			break;
+	}
+	return (cfg_value);
+
+}
+
+
+nxge_status_t
+nxge_set_hw_classify_config(p_nxge_t nxgep)
+{
+	p_nxge_dma_pt_cfg_t	p_all_cfgp;
+	p_nxge_hw_pt_cfg_t	p_cfgp;
+
+	NXGE_DEBUG_MSG((nxgep, MOD_CTL, "==> nxge_set_hw_classify_config"));
+
+	/* Get mac rdc table info from HW/Prom/.conf etc ...... */
+	/* for now, get it from dma configs */
+	p_all_cfgp = (p_nxge_dma_pt_cfg_t)&nxgep->pt_config;
+	p_cfgp = (p_nxge_hw_pt_cfg_t)&p_all_cfgp->hw_config;
+
+	/*
+	 * classify_init needs to call first.
+	 */
+	nxgep->class_config.mac_rdcgrp = p_cfgp->def_mac_rxdma_grpid;
+	nxgep->class_config.mcast_rdcgrp = p_cfgp->def_mac_rxdma_grpid;
+	NXGE_DEBUG_MSG((nxgep, MOD_CTL, "<== nxge_set_hw_classify_config"));
+
+	return (NXGE_OK);
+}
diff --git a/drivers/net/nxge/nxge_fzc.c b/drivers/net/nxge/nxge_fzc.c
new file mode 100644
index 0000000..a83d740
--- /dev/null
+++ b/drivers/net/nxge/nxge_fzc.c
@@ -0,0 +1,733 @@
+/*
+ * nxge_fzc.c	Neptune Function Zero Control interface functions
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#include	<nxge_impl.h>
+#include	<npi_mac.h>
+#include	<npi_vir.h>
+#include	<npi_rxdma.h>
+
+/*
+ * The following interfaces are controlled by the
+ * fucntion control registers. Some global registers
+ * are to be initialized by only one of the 2/4 functions.
+ * Use the test and set register.
+ */
+
+nxge_status_t
+nxge_test_and_set(p_nxge_t nep, uint8_t tas)
+{
+	npi_handle_t		handle;
+	npi_status_t		rs = NPI_SUCCESS;
+
+	handle = nep->npi_handle;
+	if ((rs = npi_dev_func_sr_sr_get_set_clear(handle, tas))
+	    != NPI_SUCCESS) {
+		return (NXGE_ERROR | rs);
+	}
+
+	return (NXGE_OK);
+
+}
+
+nxge_status_t
+nxge_set_fzc_multi_part_ctl(p_nxge_t nep, boolean_t mpc)
+{
+	npi_handle_t		handle;
+	npi_status_t		rs = NPI_SUCCESS;
+
+	NXGE_DEBUG_MSG((nep, DMA_CTL, "==> nxge_set_fzc_multi_part_ctl"));
+
+	/*
+	 * In multi-partitioning, the partition manager
+	 * who owns function zero should set this multi-partition
+	 * control bit.
+	 */
+#if 0
+	if (nep->use_partition && nep->function_num) {
+		return (DDI_FAILURE);
+	}
+#endif
+
+	handle = nep->npi_handle;
+	if ((rs = npi_fzc_mpc_set(handle, mpc)) != NPI_SUCCESS) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+			"<== nxge_set_fzc_multi_part_ctl"));
+		return (NXGE_ERROR | rs);
+	}
+
+	NXGE_DEBUG_MSG((nep, DMA_CTL, "<== nxge_set_fzc_multi_part_ctl"));
+
+	return (NXGE_OK);
+
+}
+
+nxge_status_t
+nxge_get_fzc_multi_part_ctl(p_nxge_t nep, boolean_t *mpc_p)
+{
+	npi_handle_t		handle;
+	npi_status_t		rs = NPI_SUCCESS;
+
+	NXGE_DEBUG_MSG((nep, DMA_CTL, "<== nxge_get_fzc_multi_part_ctl"));
+
+	handle = nep->npi_handle;
+	if ((rs = npi_fzc_mpc_get(handle, mpc_p)) != NPI_SUCCESS) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+			"<== nxge_set_fzc_multi_part_ctl"));
+		return (NXGE_ERROR | rs);
+	}
+	NXGE_DEBUG_MSG((nep, DMA_CTL, "<== nxge_get_fzc_multi_part_ctl"));
+
+	return (NXGE_OK);
+}
+
+/*
+ * System interrupt registers that are under function zero
+ * management.
+ */
+nxge_status_t
+nxge_fzc_intr_init(p_nxge_t nxgep)
+{
+	nxge_status_t	status = NXGE_OK;
+
+	NXGE_DEBUG_MSG((nxgep, INT_CTL, "==> nxge_fzc_intr_init"));
+
+	/* Configure the initial timer resolution */
+	if ((status = nxge_fzc_intr_tmres_set(nxgep)) != NXGE_OK) {
+		return (status);
+	}
+
+	/*
+	 * Set up the logical device group's logical devices that
+	 * the group owns.
+	 */
+	if ((status = nxge_fzc_intr_ldg_num_set(nxgep)) != NXGE_OK) {
+		return (status);
+	}
+
+	/* Configure the system interrupt data */
+	if ((status = nxge_fzc_intr_sid_set(nxgep)) != NXGE_OK) {
+		return (status);
+	}
+
+	NXGE_DEBUG_MSG((nxgep, INT_CTL, "<== nxge_fzc_intr_init"));
+
+	return (status);
+}
+
+nxge_status_t
+nxge_fzc_intr_ldg_num_set(p_nxge_t nxgep)
+{
+	p_nxge_ldg_t	ldgp;
+	p_nxge_ldv_t	ldvp;
+	npi_handle_t	handle;
+	int		i, j;
+	npi_status_t	rs = NPI_SUCCESS;
+
+	NXGE_DEBUG_MSG((nxgep, INT_CTL, "==> nxge_fzc_intr_ldg_num_set"));
+
+	if (nxgep->ldgvp == NULL) {
+		return (NXGE_ERROR);
+	}
+
+	handle = nxgep->npi_handle;
+	ldgp = nxgep->ldgvp->ldgp;
+	ldvp = nxgep->ldgvp->ldvp;
+
+	if (ldgp == NULL || ldvp == NULL) {
+		return (NXGE_ERROR);
+	}
+
+	for (i = 0; i < nxgep->ldgvp->ldg_intrs; i++, ldgp++) {
+		NXGE_DEBUG_MSG((nxgep, INT_CTL,
+			"==> nxge_fzc_intr_ldg_num_set "
+			"<== nxge_f(Neptune): # ldv %d "
+			"in group %d", ldgp->nldvs, ldgp->ldg));
+
+		for (j = 0; j < ldgp->nldvs; j++, ldvp++) {
+			rs = npi_fzc_ldg_num_set(handle, ldvp->ldv,
+				ldvp->ldg_assigned);
+			if (rs != NPI_SUCCESS) {
+				NXGE_DEBUG_MSG((nxgep, INT_CTL,
+					"<== nxge_fzc_intr_ldg_num_set failed "
+					" rs 0x%x ldv %d ldg %d",
+					rs, ldvp->ldv, ldvp->ldg_assigned));
+				return (NXGE_ERROR | rs);
+			}
+			NXGE_DEBUG_MSG((nxgep, INT_CTL,
+				"<== nxge_fzc_intr_ldg_num_set OK "
+				" ldv %d ldg %d",
+				ldvp->ldv, ldvp->ldg_assigned));
+		}
+	}
+
+	NXGE_DEBUG_MSG((nxgep, INT_CTL, "<== nxge_fzc_intr_ldg_num_set"));
+
+	return (NXGE_OK);
+}
+
+nxge_status_t
+nxge_fzc_intr_tmres_set(p_nxge_t nxgep)
+{
+	npi_handle_t	handle;
+	npi_status_t	rs = NPI_SUCCESS;
+
+	NXGE_DEBUG_MSG((nxgep, INT_CTL, "==> nxge_fzc_intr_tmres_set"));
+	/*
+	 * XXX: TODO only set once by owner of function zero.
+	 */
+	if (nxgep->ldgvp == NULL) {
+		return (NXGE_ERROR);
+	}
+
+	handle = nxgep->npi_handle;
+	if ((rs = npi_fzc_ldg_timer_res_set(handle, nxgep->ldgvp->tmres))) {
+		return (NXGE_ERROR | rs);
+	}
+	NXGE_DEBUG_MSG((nxgep, INT_CTL, "<== nxge_fzc_intr_tmres_set"));
+
+	return (NXGE_OK);
+}
+
+nxge_status_t
+nxge_fzc_intr_sid_set(p_nxge_t nxgep)
+{
+	npi_handle_t	handle;
+	p_nxge_ldg_t	ldgp;
+	fzc_sid_t	sid;
+	int		i;
+	npi_status_t	rs = NPI_SUCCESS;
+
+	NXGE_DEBUG_MSG((nxgep, INT_CTL, "==> nxge_fzc_intr_sid_set"));
+	/*
+	 * XXX: TODO only set once by owner of function zero.
+	 */
+	if (nxgep->ldgvp == NULL) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"<== nxge_fzc_intr_sid_set: no ldg"));
+		return (NXGE_ERROR);
+	}
+
+	handle = nxgep->npi_handle;
+	ldgp = nxgep->ldgvp->ldgp;
+	NXGE_DEBUG_MSG((nxgep, INT_CTL,
+		"==> nxge_fzc_intr_sid_set: #int %d", nxgep->ldgvp->ldg_intrs));
+	sid.niu = B_FALSE;
+	for (i = 0; i < nxgep->ldgvp->ldg_intrs; i++, ldgp++) {
+		sid.ldg = ldgp->ldg;
+		sid.func = ldgp->func;
+		sid.vector = ldgp->vector;
+		NXGE_DEBUG_MSG((nxgep, INT_CTL,
+			"==> nxge_fzc_intr_sid_set(%d): func %d group %d "
+			"vector %d",
+			i, sid.func, sid.ldg, sid.vector));
+		rs = npi_fzc_sid_set(handle, sid);
+		if (rs != NPI_SUCCESS) {
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"<== nxge_fzc_intr_sid_set:failed 0x%x",
+				rs));
+			return (NXGE_ERROR | rs);
+		}
+	}
+
+	NXGE_DEBUG_MSG((nxgep, INT_CTL, "<== nxge_fzc_intr_sid_set"));
+
+	return (NXGE_OK);
+
+}
+
+/*
+ * Receive DMA registers that are under function zero
+ * management.
+ */
+
+nxge_status_t
+nxge_init_fzc_rxdma_channel(p_nxge_t nxgep, uint16_t channel,
+			    p_rx_rbr_ring_t rbr_p, p_rx_rcr_ring_t rcr_p,
+			    p_rx_mbox_t mbox_p)
+{
+	nxge_status_t	status = NXGE_OK;
+
+	NXGE_DEBUG_MSG((nxgep, RX_CTL, "==> nxge_init_fzc_rxdma_channel"));
+
+	/* Initialize the RXDMA logical pages */
+	status = nxge_init_fzc_rxdma_channel_pages(nxgep, channel, rbr_p);
+	if (status != NXGE_OK) {
+		return (status);
+	}
+
+	/* Configure RED parameters */
+	status = nxge_init_fzc_rxdma_channel_red(nxgep, channel, rcr_p);
+	if (status != NXGE_OK) {
+		return (status);
+	}
+
+	NXGE_DEBUG_MSG((nxgep, RX_CTL, "<== nxge_init_fzc_rxdma_channel"));
+
+	return (status);
+}
+
+nxge_status_t
+nxge_init_fzc_rxdma_channel_pages(p_nxge_t nxgep, uint16_t channel,
+				  p_rx_rbr_ring_t rbrp)
+{
+	npi_handle_t		handle;
+	dma_log_page_t		cfg;
+	npi_status_t		rs = NPI_SUCCESS;
+
+	NXGE_DEBUG_MSG((nxgep, DMA_CTL,
+		"==> nxge_init_fzc_rxdma_channel_pages"));
+
+	handle = nxgep->npi_handle;
+	/*
+	 * Initialize logical page 1.
+	 */
+	cfg.func_num = nxgep->function_num;
+	cfg.page_num = 0;
+	cfg.valid = rbrp->page_valid.bits.ldw.page0;
+	cfg.value = rbrp->page_value_1.value;
+	cfg.mask = rbrp->page_mask_1.value;
+	cfg.reloc = rbrp->page_reloc_1.value;
+	rs = npi_rxdma_cfg_logical_page(handle, channel,
+					    (p_dma_log_page_t)&cfg);
+	if (rs != NPI_SUCCESS) {
+		return (NXGE_ERROR | rs);
+	}
+
+	/*
+	 * Initialize logical page 2.
+	 */
+	cfg.page_num = 1;
+	cfg.valid = rbrp->page_valid.bits.ldw.page1;
+	cfg.value = rbrp->page_value_2.value;
+	cfg.mask = rbrp->page_mask_2.value;
+	cfg.reloc = rbrp->page_reloc_2.value;
+
+	rs = npi_rxdma_cfg_logical_page(handle, channel, &cfg);
+	if (rs != NPI_SUCCESS) {
+		return (NXGE_ERROR | rs);
+	}
+
+	/* Initialize the page handle */
+	rs = npi_rxdma_cfg_logical_page_handle(handle, channel,
+						   rbrp->page_hdl.bits.ldw.handle);
+
+	if (rs != NPI_SUCCESS) {
+		return (NXGE_ERROR | rs);
+	}
+
+	NXGE_DEBUG_MSG((nxgep, DMA_CTL,
+		"<== nxge_init_fzc_rxdma_channel_pages"));
+
+	return (NXGE_OK);
+}
+
+nxge_status_t
+nxge_init_fzc_rxdma_channel_red(p_nxge_t nxgep, uint16_t channel,
+				p_rx_rcr_ring_t rcr_p)
+{
+	npi_handle_t		handle;
+	rdc_red_para_t		red;
+	npi_status_t		rs = NPI_SUCCESS;
+
+	NXGE_DEBUG_MSG((nxgep, DMA_CTL, "==> nxge_init_fzc_rxdma_channel_red"));
+
+	handle = nxgep->npi_handle;
+	red.value = 0;
+	red.bits.ldw.win = RXDMA_RED_WINDOW_DEFAULT;
+	red.bits.ldw.thre = (rcr_p->ring_size - RXDMA_RED_LESS_ENTRIES);
+	red.bits.ldw.win_syn = RXDMA_RED_WINDOW_DEFAULT;
+	red.bits.ldw.thre_sync = (rcr_p->ring_size - RXDMA_RED_LESS_ENTRIES);
+
+	NXGE_DEBUG_MSG((nxgep, DMA_CTL,
+		"==> nxge_init_fzc_rxdma_channel_red(thre_sync %d)",
+		red.bits.ldw.thre_sync));
+	rs = npi_rxdma_cfg_wred_param(handle, channel, &red);
+	if (rs != NPI_SUCCESS) {
+		return (NXGE_ERROR | rs);
+	}
+
+	NXGE_DEBUG_MSG((nxgep, DMA_CTL,
+		"<== nxge_init_fzc_rxdma_channel_red"));
+
+	return (NXGE_OK);
+}
+
+void
+nxge_init_fzc_rxdma_channels(p_nxge_t nxgep)
+{
+	/*
+	 * For each DMA channel, initialize each DMA specific
+	 * configurations.
+	 */
+
+#if 0
+	for (i = 0; i < ndmas; i++) {
+		nxge_init_fzc_rxdma_channel(p_nxge_t, channel);
+	}
+#endif
+
+}
+
+nxge_status_t
+nxge_init_fzc_txdma_channel(p_nxge_t nxgep, uint16_t channel,
+			    p_tx_ring_t tx_ring_p, p_tx_mbox_t mbox_p)
+{
+	nxge_status_t	status = NXGE_OK;
+
+	NXGE_DEBUG_MSG((nxgep, DMA_CTL,
+			"==> nxge_init_fzc_txdma_channel"));
+
+	/* Initialize the TXDMA logical pages */
+	nxge_init_fzc_txdma_channel_pages(nxgep, channel,
+						 tx_ring_p);
+	/*
+	 * Configure Transmit DRR Weight parameters
+	 * (It actually programs the TXC max burst register).
+	 */
+	nxge_init_fzc_txdma_channel_drr(nxgep, channel, tx_ring_p);
+
+	NXGE_DEBUG_MSG((nxgep, DMA_CTL,
+		"<== nxge_init_fzc_txdma_channel"));
+	return (status);
+}
+
+nxge_status_t
+nxge_init_fzc_common(p_nxge_t nxgep)
+{
+	nxge_status_t	status = NXGE_OK;
+
+
+	nxge_init_fzc_rx_common(nxgep);
+	nxge_init_fzc_tx_common(nxgep);
+
+	return (status);
+}
+
+nxge_status_t
+nxge_init_fzc_rx_common(p_nxge_t nxgep)
+{
+	npi_handle_t	handle;
+	npi_status_t	rs = NPI_SUCCESS;
+	nxge_status_t	status = NXGE_OK;
+	uint32_t	lbolt;
+
+	NXGE_DEBUG_MSG((nxgep, DMA_CTL, "==> nxge_init_fzc_rx_common"));
+	handle = nxgep->npi_handle;
+
+	if (!handle.regp) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"nxge_init_fzc_rx_common null ptr"));
+		return (NXGE_ERROR);
+	}
+
+	/*
+	 * TODO: Need to acquire the FZC lock.
+	 */
+
+	/*
+	 * Configure 32 bit mode.
+	 */
+
+	/*
+	 * Configure the rxdma clock divider
+	 * This is the granularity counter based on
+	 * the hardware system clock (i.e. 300 Mhz) and
+	 * it is running around 3 nanoseconds.
+	 * So, set the clock divider counter to 1000 to get
+	 * microsecond granularity.
+	 * For example, for a 3 microsecond timeout, the timeout
+	 * will be set to 1.
+	 */
+	rs = npi_rxdma_cfg_clock_div_set(handle, RXDMA_CK_DIV_DEFAULT);
+	if (rs != NPI_SUCCESS)
+		return (NXGE_ERROR | rs);
+
+#if defined(__i386)
+	rs = npi_rxdma_cfg_32bitmode_enable(handle);
+	if (rs != NPI_SUCCESS)
+		return (NXGE_ERROR | rs);
+	rs = npi_txdma_mode32_set(handle, B_TRUE);
+	if (rs != NPI_SUCCESS)
+		return (NXGE_ERROR | rs);
+#endif
+
+
+	/*
+	 * TODO:
+	 *	Initialize the shadow memory.
+	 */
+
+	/*
+	 * Enable WRED and program an initial value.
+	 * Use time to set the initial random number.
+	 */
+#ifndef COSIM
+	lbolt = jiffies;
+#else
+	lbolt = 0x3456;
+#endif
+	rs = npi_rxdma_cfg_red_rand_init(handle, (uint16_t)lbolt);
+	if (rs != NPI_SUCCESS)
+		return (NXGE_ERROR | rs);
+
+	/* Initialize the RDC tables for each group */
+	status = nxge_init_fzc_rdc_tbl(nxgep);
+
+
+	/* Ethernet Timeout Counter (?) */
+
+	NXGE_DEBUG_MSG((nxgep, DMA_CTL,
+		"<== nxge_init_fzc_rx_common:status 0x%08x", status));
+	return (status);
+}
+
+nxge_status_t
+nxge_init_fzc_rdc_tbl(p_nxge_t nxgep)
+{
+	npi_handle_t		handle;
+	p_nxge_dma_pt_cfg_t	p_all_cfgp;
+	p_nxge_hw_pt_cfg_t	p_cfgp;
+	p_nxge_rdc_grp_t	rdc_grp_p;
+	uint8_t 		grp_tbl_id;
+	int			ngrps;
+	int			i;
+	npi_status_t		rs = NPI_SUCCESS;
+	nxge_status_t		status = NXGE_OK;
+
+
+	NXGE_DEBUG_MSG((nxgep, DMA_CTL, "==> nxge_init_fzc_rdc_tbl"));
+
+	handle = nxgep->npi_handle;
+	p_all_cfgp = (p_nxge_dma_pt_cfg_t)&nxgep->pt_config;
+	p_cfgp = (p_nxge_hw_pt_cfg_t)&p_all_cfgp->hw_config;
+
+	grp_tbl_id = p_cfgp->start_rdc_grpid;
+	rdc_grp_p = &p_all_cfgp->rdc_grps[0];
+	ngrps = p_cfgp->max_rdc_grpids;
+	for (i = 0; i < ngrps; i++, rdc_grp_p++) {
+		rs = npi_rxdma_cfg_rdc_table(handle, grp_tbl_id++,
+			rdc_grp_p->rdc);
+		if (rs != NPI_SUCCESS) {
+			status = NXGE_ERROR | rs;
+			break;
+		}
+	}
+
+	NXGE_DEBUG_MSG((nxgep, DMA_CTL, "<== nxge_init_fzc_rdc_tbl"));
+	return (status);
+}
+
+nxge_status_t
+nxge_init_fzc_rxdma_port(p_nxge_t nxgep)
+{
+	npi_handle_t		handle;
+	p_nxge_dma_pt_cfg_t	p_all_cfgp;
+	p_nxge_hw_pt_cfg_t	p_cfgp;
+	hostinfo_t 		hostinfo;
+	uint32_t ddr_weight;
+	int			i;
+	npi_status_t		rs = NPI_SUCCESS;
+	p_nxge_class_pt_cfg_t 	p_class_cfgp;
+
+	/* TODO: acquire FZC lock */
+	NXGE_DEBUG_MSG((nxgep, DMA_CTL, "==> nxge_init_fzc_rxdma_port"));
+	p_all_cfgp = (p_nxge_dma_pt_cfg_t)&nxgep->pt_config;
+	p_cfgp = (p_nxge_hw_pt_cfg_t)&p_all_cfgp->hw_config;
+	handle = nxgep->npi_handle;
+	/*
+	 * Initialize the port scheduler DRR weight.
+	 * npi_rxdma_cfg_port_ddr_weight();
+	 */
+
+	/* Ethernet FIFO Discard counter */
+
+	/* VLAN Discard counter */
+
+	/* Ethernet DATA FIFO Discard counter */
+
+	/* Program the default RDC of a port */
+	rs = npi_rxdma_cfg_default_port_rdc(handle, nxgep->function_num,
+						p_cfgp->def_rdc);
+	if (rs != NPI_SUCCESS) {
+		return (NXGE_ERROR | rs);
+	}
+
+	ddr_weight = (nxgep->nports == 4) ?
+		PT_DRR_WT_DEFAULT_1G : PT_DRR_WT_DEFAULT_10G;
+	rs = npi_rxdma_cfg_port_ddr_weight(handle, nxgep->function_num,
+						ddr_weight);
+	if (rs != NPI_SUCCESS) {
+		return (NXGE_ERROR | rs);
+	}
+
+
+	/*
+	 * Configure the MAC host info table with RDC tables
+	 */
+	/* TODO: for now set it to 1 entry */
+	hostinfo.value = 0;
+	p_class_cfgp = (p_nxge_class_pt_cfg_t)&nxgep->class_config;
+	for (i = 0; i < p_cfgp->max_macs; i++) {
+		hostinfo.bits.w0.rdc_tbl_num = p_cfgp->start_rdc_grpid;
+		hostinfo.bits.w0.mac_pref = p_cfgp->mac_pref;
+		if (p_class_cfgp->mac_host_info[i].flag) {
+			hostinfo.bits.w0.rdc_tbl_num =
+				p_class_cfgp->mac_host_info[i].rdctbl;
+			hostinfo.bits.w0.mac_pref =
+				p_class_cfgp->mac_host_info[i].mpr_npr;
+		}
+		rs = npi_mac_hostinfo_entry(handle, OP_SET,
+						nxgep->function_num, i,
+						&hostinfo);
+		if (rs != NPI_SUCCESS)
+			return (NXGE_ERROR | rs);
+	}
+
+	NXGE_DEBUG_MSG((nxgep, DMA_CTL,
+		"<== nxge_init_fzc_rxdma_port"));
+
+	return (NXGE_OK);
+
+}
+
+
+nxge_status_t
+nxge_init_fzc_tx_common(p_nxge_t nxgep)
+{
+	nxge_status_t	status = NXGE_OK;
+
+
+	return (status);
+}
+
+nxge_status_t
+nxge_fzc_dmc_def_port_rdc(p_nxge_t nxgep, uint8_t port, uint16_t rdc)
+{
+	npi_status_t rs = NPI_SUCCESS;
+	rs = npi_rxdma_cfg_default_port_rdc(nxgep->npi_handle,
+				    port, rdc);
+	if (rs & NPI_FAILURE)
+		return (NXGE_ERROR | rs);
+	return (NXGE_OK);
+}
+
+nxge_status_t
+nxge_init_fzc_txdma_channel_pages(p_nxge_t nxgep, uint16_t channel,
+	p_tx_ring_t tx_ring_p)
+{
+	npi_handle_t		handle;
+	dma_log_page_t		cfg;
+	npi_status_t		rs = NPI_SUCCESS;
+
+	NXGE_DEBUG_MSG((nxgep, DMA_CTL,
+		"<== nxge_init_fzc_txdma_channel_pages"));
+
+	/*
+	 * Initialize logical page 1.
+	 */
+	handle = nxgep->npi_handle;
+	cfg.func_num = nxgep->function_num;
+	cfg.page_num = 0;
+	cfg.valid = tx_ring_p->page_valid.bits.ldw.page0;
+	cfg.value = tx_ring_p->page_value_1.value;
+	cfg.mask = tx_ring_p->page_mask_1.value;
+	cfg.reloc = tx_ring_p->page_reloc_1.value;
+
+	rs = npi_txdma_log_page_set(handle, channel,
+		(p_dma_log_page_t)&cfg);
+	if (rs != NPI_SUCCESS) {
+		return (NXGE_ERROR | rs);
+	}
+
+	/*
+	 * Initialize logical page 2.
+	 */
+	cfg.page_num = 1;
+	cfg.valid = tx_ring_p->page_valid.bits.ldw.page1;
+	cfg.value = tx_ring_p->page_value_2.value;
+	cfg.mask = tx_ring_p->page_mask_2.value;
+	cfg.reloc = tx_ring_p->page_reloc_2.value;
+
+	rs = npi_txdma_log_page_set(handle, channel, &cfg);
+	if (rs != NPI_SUCCESS) {
+		return (NXGE_ERROR | rs);
+	}
+
+	/* Initialize the page handle */
+	rs = npi_txdma_log_page_handle_set(handle, channel,
+			&tx_ring_p->page_hdl);
+
+	if (rs == NPI_SUCCESS) {
+		return (NXGE_OK);
+	} else {
+		return (NXGE_ERROR | rs);
+	}
+}
+
+
+nxge_status_t
+nxge_init_fzc_txdma_channel_drr(p_nxge_t nxgep, uint16_t channel,
+	p_tx_ring_t tx_ring_p)
+{
+	npi_status_t	rs = NPI_SUCCESS;
+	npi_handle_t	handle;
+
+	handle = nxgep->npi_handle;
+	rs = npi_txc_dma_max_burst_set(handle, channel,
+			tx_ring_p->max_burst.value);
+	if (rs == NPI_SUCCESS) {
+		return (NXGE_OK);
+	} else {
+		return (NXGE_ERROR | rs);
+	}
+}
+
+nxge_status_t
+nxge_fzc_sys_err_mask_set(p_nxge_t nxgep, boolean_t mask)
+{
+	npi_status_t	rs = NPI_SUCCESS;
+	npi_handle_t	handle;
+
+	handle = nxgep->npi_handle;
+	rs = npi_fzc_sys_err_mask_set(handle, mask);
+	if (rs == NPI_SUCCESS) {
+		return (NXGE_OK);
+	} else {
+		return (NXGE_ERROR | rs);
+	}
+}
diff --git a/drivers/net/nxge/nxge_mac.c b/drivers/net/nxge/nxge_mac.c
new file mode 100644
index 0000000..3973895
--- /dev/null
+++ b/drivers/net/nxge/nxge_mac.c
@@ -0,0 +1,3718 @@
+/*
+ * nxge_mac.c	Neptune MAC interface functions
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modifiation, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#include <nxge_impl.h>
+#include <nxge_mac.h>
+
+extern uint32_t nxge_no_link_notify;
+extern uint32_t nxge_no_msg;
+extern uint32_t nxge_lb_dbg;
+extern boolean_t nxge_jumbo_enable;
+extern uint32_t	nxge_jumbo_mtu;
+extern uint32_t	nxge_dont_strip_crc;
+
+
+nxge_status_t nxge_mac_init(p_nxge_t nxgep);
+static void nxge_mii_dump(p_nxge_t);
+static nxge_status_t nxge_mii_get_link_mode(p_nxge_t);
+
+static nxge_status_t nxge_1G_serdes_init(p_nxge_t nxgep);
+static nxge_status_t nxge_neptune_10G_serdes_init(p_nxge_t nxgep);
+
+/* Initialize the entire MAC and physical layer */
+
+nxge_status_t
+nxge_mac_init(p_nxge_t nxgep)
+{
+	uint8_t			portn;
+	nxge_status_t		status = NXGE_OK;
+	uint64_t val;
+
+	portn = NXGE_GET_PORT_NUM(nxgep->function_num);
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "==> nxge_mac_init: port<%d>", portn));
+
+	nxgep->mac.portnum = portn;
+	nxgep->mac.porttype = PORT_TYPE_XMAC;
+
+	if ((portn == BMAC_PORT_0) || (portn == BMAC_PORT_1))
+		nxgep->mac.porttype = PORT_TYPE_BMAC;
+
+	/* Initialize XIF to configure a network mode */
+	if ((status = nxge_xif_init(nxgep)) != NXGE_OK) {
+		goto fail;
+	}
+
+	if ((status = nxge_pcs_init(nxgep)) != NXGE_OK) {
+		goto fail;
+	}
+
+	if (nxgep->dev->mtu > 1500) {
+		nxgep->mac.is_jumbo = B_TRUE;
+	} else {
+		nxgep->mac.is_jumbo = B_FALSE;
+	}
+
+	/* Initialize TX and RX MACs */
+	/*
+	 * Always perform XIF init first, before TX and RX MAC init
+	 */
+	if ((status = nxge_tx_mac_reset(nxgep)) != NXGE_OK)
+		goto fail;
+
+	if ((status = nxge_tx_mac_init(nxgep)) != NXGE_OK)
+		goto fail;
+
+	if ((status = nxge_rx_mac_reset(nxgep)) != NXGE_OK)
+		goto fail;
+
+	if ((status = nxge_rx_mac_init(nxgep)) != NXGE_OK)
+		goto fail;
+
+	if ((status = nxge_tx_mac_enable(nxgep)) != NXGE_OK)
+		goto fail;
+
+	if ((status = nxge_rx_mac_enable(nxgep)) != NXGE_OK)
+		goto fail;
+
+	nxgep->statsp->mac_stats.mac_mtu = nxgep->mac.maxframesize;
+
+	/* The Neptune Serdes needs to reinitialized again */
+	if ((nxgep->niu_type == NEPTUNE) &&
+	    ((nxgep->mac.portmode == PORT_1G_SERDES) ||
+	    (nxgep->mac.portmode == PORT_1G_FIBER)) &&
+	    ((portn == 0) || (portn == 1))) {
+		NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+		"nxge_mac_init: reinit Neptune 1G Serdes "));
+		if ((status = nxge_1G_serdes_init(nxgep)) != NXGE_OK) {
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+							"nxge_mac_init"
+							" nxge_neptune_serdes_init failed"
+							" port %d mode %d ",
+							portn, nxgep->mac.portmode));
+			goto fail;
+		}
+	}
+
+	XMAC_REG_RD(nxgep->npi_handle, portn, XMAC_CONFIG_REG, &val);
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "<== nxge_mac_init: port<%d>", portn));
+
+	return (NXGE_OK);
+fail:
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+			"nxge_mac_init: failed to initialize MAC port<%d>",
+			portn));
+	return (status);
+}
+
+/* Initialize the Ethernet Link */
+
+nxge_status_t
+nxge_link_init(p_nxge_t nxgep)
+{
+	nxge_status_t		status = NXGE_OK;
+	nxge_port_mode_t	portmode;
+
+	uint8_t			portn;
+
+	portn = nxgep->mac.portnum;
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "==> nxge_link_init: port<%d>", portn));
+
+	portmode = nxgep->mac.portmode;
+	if (nxgep->niu_type == N2_NIU && (portmode != PORT_10G_SERDES) &&
+	    (portmode != PORT_1G_SERDES)) {
+		/* Workaround to get link up in both NIU ports */
+		if ((status = nxge_xcvr_init(nxgep)) != NXGE_OK) {
+			goto fail;
+		}
+	}
+
+	/* Initialize internal serdes */
+	if ((status = nxge_serdes_init(nxgep)) != NXGE_OK)
+		goto fail;
+	NXGE_DELAY(200000);
+	if ((status = nxge_xcvr_init(nxgep)) != NXGE_OK)
+		goto fail;
+
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "<== nxge_link_init: port<%d>", portn));
+
+
+	return (NXGE_OK);
+
+fail:
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+		"nxge_link_init: ",
+		"failed to initialize Ethernet link on port<%d>",
+		portn));
+
+	return (status);
+}
+
+/* Enable/Disable Address filtering feature */
+
+nxge_status_t
+nxge_enable_address_filter(p_nxge_t nxgep, uint8_t portn, boolean_t set)
+{
+	nxge_port_t		portt;
+	nxge_port_mode_t	portmode;
+	p_nxge_stats_t		statsp;
+	npi_status_t		rs = NPI_SUCCESS;
+	npi_handle_t		handle;
+
+	handle = nxgep->npi_handle;
+	portmode = nxgep->mac.portmode;
+	portt = nxgep->mac.porttype;
+	statsp = nxgep->statsp;
+
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL, "Address filter port %d with %x",
+					portn, set));
+
+	rs = npi_mac_address_filter_enable(handle, portn, set);
+	if (rs != NPI_SUCCESS)
+		goto fail;
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "<== nxge_enable_address_filter: port<%d>", portn));
+	return (NXGE_OK);
+fail:
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+			"nxge_enable_address_filter: Failed to configure port<%d>",
+			portn));
+	return (NXGE_ERROR | rs);
+}
+
+
+/* Initialize the XIF sub-block within the MAC */
+
+nxge_status_t
+nxge_xif_init(p_nxge_t nxgep)
+{
+	uint32_t		xif_cfg = 0;
+	npi_attr_t		ap;
+	uint8_t			portn;
+	nxge_port_t		portt;
+	nxge_port_mode_t	portmode;
+	p_nxge_stats_t		statsp;
+	npi_status_t		rs = NPI_SUCCESS;
+	npi_handle_t		handle;
+
+	portn = NXGE_GET_PORT_NUM(nxgep->function_num);
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "==> nxge_xif_init: port<%d>", portn));
+
+	handle = nxgep->npi_handle;
+	portmode = nxgep->mac.portmode;
+	portt = nxgep->mac.porttype;
+	statsp = nxgep->statsp;
+
+
+	if ((nxgep->niu_type == NEPTUNE) &&
+	    ((nxgep->mac.portmode == PORT_1G_SERDES) ||
+	    (nxgep->mac.portmode == PORT_1G_FIBER)) &&
+	    ((portn == 0) || (portn == 1))) {
+		NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+		    "nxge_xcvr_init: set ATCA mode"));
+		npi_mac_mif_set_atca_mode(nxgep->npi_handle, B_TRUE);
+	}
+
+	if (portt == PORT_TYPE_XMAC) {
+
+		/* Setup XIF Configuration for XMAC */
+
+		if ((portmode == PORT_10G_FIBER) ||
+			(portmode == PORT_10G_COPPER) ||
+			(portmode == PORT_10G_SERDES))
+			xif_cfg |= CFG_XMAC_XIF_LFS;
+
+		if (portmode == PORT_1G_COPPER) {
+			xif_cfg |= CFG_XMAC_XIF_1G_PCS_BYPASS;
+		}
+
+		/* Set MAC Internal Loopback if necessary */
+		if (statsp->lb_mode == nxge_lb_mac1000)
+			xif_cfg |= CFG_XMAC_XIF_LOOPBACK;
+
+		if (statsp->mac_stats.link_speed == 100)
+			xif_cfg |= CFG_XMAC_XIF_SEL_CLK_25MHZ;
+
+		xif_cfg |= CFG_XMAC_XIF_TX_OUTPUT;
+
+		if ((portmode == PORT_10G_FIBER) || (portmode == PORT_10G_SERDES)) {
+			if (statsp->mac_stats.link_up) {
+				xif_cfg |= CFG_XMAC_XIF_LED_POLARITY;
+			} else {
+				xif_cfg |= CFG_XMAC_XIF_LED_FORCE;
+			}
+		}
+
+		rs = npi_xmac_xif_config(handle, INIT, portn, xif_cfg);
+		if (rs != NPI_SUCCESS)
+			goto fail;
+
+		nxgep->mac.xif_config = xif_cfg;
+
+		/* Set Port Mode */
+		if ((portmode == PORT_10G_FIBER) ||
+			(portmode == PORT_10G_SERDES) ||
+				(portmode == PORT_10G_COPPER)) {
+			SET_MAC_ATTR1(handle, ap, portn, MAC_PORT_MODE,
+						MAC_XGMII_MODE, rs);
+			if (rs != NPI_SUCCESS)
+				goto fail;
+			if (statsp->mac_stats.link_up) {
+				if (nxge_10g_link_led_on(nxgep) != NXGE_OK)
+					goto fail;
+			} else {
+				if (nxge_10g_link_led_off(nxgep) != NXGE_OK)
+					goto fail;
+			}
+		} else if ((portmode == PORT_1G_FIBER) ||
+					(portmode == PORT_1G_COPPER) ||
+				   (portmode == PORT_1G_SERDES) ||
+				   (portmode == PORT_1G_RGMII_FIBER)) {
+			statsp->mac_stats.link_speed = 1000;
+			if (statsp->mac_stats.link_speed == 1000) {
+				SET_MAC_ATTR1(handle, ap, portn, MAC_PORT_MODE,
+							MAC_GMII_MODE, rs);
+			} else {
+				SET_MAC_ATTR1(handle, ap, portn, MAC_PORT_MODE,
+							MAC_MII_MODE, rs);
+			}
+			if (rs != NPI_SUCCESS)
+				goto fail;
+		} else {
+			NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+					"nxge_xif_init: Unknown port mode (%d)"
+					" for port<%d>", portmode, portn));
+			goto fail;
+		}
+
+	} else if (portt == PORT_TYPE_BMAC) {
+
+		/* Setup XIF Configuration for BMAC */
+
+		if ((portmode == PORT_1G_COPPER) ||
+			(portmode == PORT_1G_RGMII_FIBER)) {
+			if (statsp->mac_stats.link_speed == 100)
+				xif_cfg |= CFG_BMAC_XIF_SEL_CLK_25MHZ;
+		}
+
+		if (statsp->lb_mode == nxge_lb_mac1000)
+			xif_cfg |= CFG_BMAC_XIF_LOOPBACK;
+
+		if (statsp->mac_stats.link_speed == 1000)
+			xif_cfg |= CFG_BMAC_XIF_GMII_MODE;
+
+		xif_cfg |= CFG_BMAC_XIF_TX_OUTPUT;
+
+		rs = npi_bmac_xif_config(handle, INIT, portn, xif_cfg);
+		if (rs != NPI_SUCCESS)
+			goto fail;
+		nxgep->mac.xif_config = xif_cfg;
+	}
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "<== nxge_xif_init: port<%d>", portn));
+	return (NXGE_OK);
+fail:
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+			"nxge_xif_init: Failed to initialize XIF port<%d>",
+			portn));
+	return (NXGE_ERROR | rs);
+}
+
+/* Initialize the PCS sub-block in the MAC */
+
+nxge_status_t
+nxge_pcs_init(p_nxge_t nxgep)
+{
+	pcs_cfg_t		pcs_cfg;
+	uint32_t		val;
+	uint8_t			portn;
+	nxge_port_mode_t	portmode;
+	npi_handle_t		handle;
+	p_nxge_stats_t		statsp;
+	npi_status_t		rs = NPI_SUCCESS;
+	handle = nxgep->npi_handle;
+	portmode = nxgep->mac.portmode;
+	portn = nxgep->mac.portnum;
+	statsp = nxgep->statsp;
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "==> nxge_pcs_init: port<%d>", portn));
+
+	if (portmode == PORT_1G_FIBER || (portmode == PORT_1G_SERDES)) {
+		if ((rs = npi_mac_pcs_reset(handle, portn)) != NPI_SUCCESS)
+			goto fail;
+
+		/* Initialize port's PCS */
+		pcs_cfg.value = 0;
+		pcs_cfg.bits.w0.enable = 1;
+		pcs_cfg.bits.w0.mask = 1;
+		PCS_REG_WR(handle, portn, PCS_CONFIG_REG, pcs_cfg.value);
+		PCS_REG_WR(handle, portn, PCS_DATAPATH_MODE_REG, 0);
+
+		NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+		    "==> nxge_pcs_init: (1G) port<%d> write config 0x%llx",
+		    portn, pcs_cfg.value));
+	} else if ((portmode == PORT_10G_FIBER) ||
+		    (portmode == PORT_10G_COPPER) ||
+		    (portmode == PORT_10G_SERDES)) {
+		/* Use internal XPCS, bypass 1G PCS */
+		XMAC_REG_RD(handle, portn, XMAC_CONFIG_REG, &val);
+		val &= ~XMAC_XIF_XPCS_BYPASS;
+		XMAC_REG_WR(handle, portn, XMAC_CONFIG_REG, val);
+
+		if ((rs = npi_xmac_xpcs_reset(handle, portn)) != NPI_SUCCESS)
+			goto fail;
+
+		/* Set XPCS Internal Loopback if necessary */
+		if ((rs = npi_xmac_xpcs_read(handle, portn,
+						XPCS_REG_CONTROL1, &val))
+						!= NPI_SUCCESS)
+			goto fail;
+		if ((statsp->lb_mode == nxge_lb_mac10g) ||
+			(statsp->lb_mode == nxge_lb_mac1000))
+			val |= XPCS_CTRL1_LOOPBK;
+		else
+			val &= ~XPCS_CTRL1_LOOPBK;
+		if ((rs = npi_xmac_xpcs_write(handle, portn,
+						XPCS_REG_CONTROL1, val))
+						!= NPI_SUCCESS)
+			goto fail;
+
+		/* Clear descw errors */
+		if ((rs = npi_xmac_xpcs_write(handle, portn,
+						XPCS_REG_DESCWERR_COUNTER, 0))
+						!= NPI_SUCCESS)
+			goto fail;
+		/* Clear symbol errors */
+		if ((rs = npi_xmac_xpcs_read(handle, portn,
+					XPCS_REG_SYMBOL_ERR_L0_1_COUNTER, &val))
+					!= NPI_SUCCESS)
+			goto fail;
+		if ((rs = npi_xmac_xpcs_read(handle, portn,
+					XPCS_REG_SYMBOL_ERR_L2_3_COUNTER, &val))
+					!= NPI_SUCCESS)
+			goto fail;
+
+	} else if (portmode == PORT_1G_COPPER ||
+		    (portmode == PORT_1G_RGMII_FIBER)) {
+		NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+			"==> nxge_pcs_init: (1G) copper port<%d>",
+			portn));
+		if (portn < 4) {
+			PCS_REG_WR(handle, portn, PCS_DATAPATH_MODE_REG,
+					PCS_DATAPATH_MODE_MII);
+		}
+		if ((rs = npi_mac_pcs_reset(handle, portn)) != NPI_SUCCESS)
+			goto fail;
+
+	} else {
+		goto fail;
+	}
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "<== nxge_pcs_init: port<%d> ", portn));
+	return (NXGE_OK);
+fail:
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_pcs_init: Failed to initialize PCS port<%d>",
+			portn));
+	return (NXGE_ERROR | rs);
+}
+
+
+/* Initialize the Internal Serdes */
+
+nxge_status_t
+nxge_serdes_init(p_nxge_t nxgep)
+{
+	p_nxge_stats_t		statsp;
+
+	uint8_t			portn;
+	nxge_status_t		status = NXGE_OK;
+
+
+	portn = nxgep->mac.portnum;
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+		"==> nxge_serdes_init port<%d>", portn));
+
+
+	statsp = nxgep->statsp;
+
+	switch (nxgep->niu_type) {
+		case N2_NIU:
+			status = nxge_n2_serdes_init(nxgep);
+			if (status != NXGE_OK) {
+				NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL, "nxge_serdes_init:"
+								" call to nxge_n2_serdes_init failed"
+								" port  %d mode %d type %x",
+								portn, nxgep->mac.portmode, nxgep->niu_type));
+				goto fail;
+			}
+			break;
+
+		case NEPTUNE:
+		case NEPTUNE_2:
+			status = nxge_neptune_serdes_init(nxgep);
+			if (status != NXGE_OK) {
+				NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL, "nxge_serdes_init:"
+								" call to nxge_neptune_serdes_init failed"
+								" port  %d mode %d type %x",
+								portn, nxgep->mac.portmode, nxgep->niu_type));
+				goto fail;
+			}
+			break;
+		default:
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL, "nxge_serdes_init unkown NIU type"
+						" port  %d mode %d type %x",
+						portn, nxgep->mac.portmode, nxgep->niu_type));
+		break;
+	}
+	statsp->mac_stats.serdes_inits++;
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "<== nxge_serdes_init port<%d>",
+			portn));
+
+	return (NXGE_OK);
+
+fail:
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+		"nxge_serdes_init: Failed to initialize serdes for port<%d>",
+			portn));
+
+	return (status);
+}
+
+/* Initialize the TI Hedwig Internal Serdes (N2-NIU only) */
+
+nxge_status_t
+nxge_n2_serdes_init(p_nxge_t nxgep)
+{
+	uint8_t portn;
+	int chan;
+	esr_ti_cfgpll_l_t pll_cfg_l;
+	esr_ti_cfgrx_l_t rx_cfg_l;
+	esr_ti_cfgrx_h_t rx_cfg_h;
+	esr_ti_cfgtx_l_t tx_cfg_l;
+	esr_ti_cfgtx_h_t tx_cfg_h;
+	esr_ti_testcfg_t test_cfg;
+	nxge_status_t status = NXGE_OK;
+
+	portn = nxgep->mac.portnum;
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "==> nxge_n2_serdes_init port<%d>",
+			portn));
+
+	tx_cfg_l.value = 0;
+	tx_cfg_h.value = 0;
+	rx_cfg_l.value = 0;
+	rx_cfg_h.value = 0;
+	pll_cfg_l.value = 0;
+	test_cfg.value = 0;
+
+	if (nxgep->mac.portmode == PORT_10G_FIBER ||
+		(nxgep->mac.portmode == PORT_10G_SERDES)) {
+		/* 0x0E01 */
+		tx_cfg_l.bits.entx = 1;
+		tx_cfg_l.bits.swing = CFGTX_SWING_1375MV;
+
+		/* 0x9101 */
+		rx_cfg_l.bits.enrx = 1;
+		rx_cfg_l.bits.term = CFGRX_TERM_0P8VDDT;
+		rx_cfg_l.bits.align = CFGRX_ALIGN_EN;
+		rx_cfg_l.bits.los = CFGRX_LOS_LOTHRES;
+
+		/* 0x0008 */
+		rx_cfg_h.bits.eq = CFGRX_EQ_ADAPTIVE_LP_ADAPTIVE_ZF;
+
+		/* Set loopback mode if necessary */
+		if (nxgep->statsp->lb_mode == nxge_lb_serdes10g) {
+			tx_cfg_l.bits.entest = 1;
+			rx_cfg_l.bits.entest = 1;
+			test_cfg.bits.loopback = TESTCFG_INNER_CML_DIS_LOOPBACK;
+			if ((status = nxge_mdio_write(nxgep, portn,
+				ESR_N2_DEV_ADDR,
+				ESR_N2_TEST_CFG_REG, test_cfg.value))
+				!= NXGE_OK)
+			goto fail;
+		}
+
+		/* Use default PLL value */
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"N2 10G FIBER/SERDES "));
+
+	} else if ((nxgep->mac.portmode == PORT_1G_FIBER) ||
+		(nxgep->mac.portmode == PORT_1G_SERDES)) {
+
+		/* 0x0E21 */
+		tx_cfg_l.bits.entx = 1;
+		tx_cfg_l.bits.rate = CFGTX_RATE_HALF;
+		tx_cfg_l.bits.swing = CFGTX_SWING_1375MV;
+
+		/* 0x9121 */
+		rx_cfg_l.bits.enrx = 1;
+		rx_cfg_l.bits.rate = CFGRX_RATE_HALF;
+		rx_cfg_l.bits.term = CFGRX_TERM_0P8VDDT;
+		rx_cfg_l.bits.align = CFGRX_ALIGN_EN;
+		rx_cfg_l.bits.los = CFGRX_LOS_LOTHRES;
+
+		/* 0x8 */
+		rx_cfg_h.bits.eq = CFGRX_EQ_ADAPTIVE_LP_ADAPTIVE_ZF;
+
+		/* MPY = 0x100 */
+		pll_cfg_l.bits.mpy = CFGPLL_MPY_8X;
+
+		/* Set PLL */
+		pll_cfg_l.bits.enpll = 1;
+		if ((status = nxge_mdio_write(nxgep, portn, ESR_N2_DEV_ADDR,
+				ESR_N2_PLL_CFG_L_REG, pll_cfg_l.value))
+				!= NXGE_OK)
+			goto fail;
+	} else {
+		goto fail;
+	}
+
+	/*   MIF_REG_WR(handle, MIF_MASK_REG, ~mask); */
+
+	NXGE_DELAY(20);
+
+	/* init TX channels */
+	for (chan = 0; chan < 4; chan++) {
+		if ((status = nxge_mdio_write(nxgep, portn, ESR_N2_DEV_ADDR,
+				ESR_N2_TX_CFG_L_REG_ADDR(chan), tx_cfg_l.value))
+				!= NXGE_OK)
+			goto fail;
+
+		if ((status = nxge_mdio_write(nxgep, portn, ESR_N2_DEV_ADDR,
+				ESR_N2_TX_CFG_H_REG_ADDR(chan), tx_cfg_h.value))
+				!= NXGE_OK)
+			goto fail;
+	}
+
+	/* init RX channels */
+	for (chan = 0; chan < 4; chan++) {
+		if ((status = nxge_mdio_write(nxgep, portn, ESR_N2_DEV_ADDR,
+				ESR_N2_RX_CFG_L_REG_ADDR(chan), rx_cfg_l.value))
+				!= NXGE_OK)
+			goto fail;
+
+		if ((status = nxge_mdio_write(nxgep, portn, ESR_N2_DEV_ADDR,
+				ESR_N2_RX_CFG_H_REG_ADDR(chan), rx_cfg_h.value))
+				!= NXGE_OK)
+			goto fail;
+	}
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "<== nxge_n2_serdes_init port<%d>",
+			portn));
+
+	return (NXGE_OK);
+fail:
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+	"nxge_n2_serdes_init: Failed to initialize N2 serdes for port<%d>",
+				portn));
+
+	return (status);
+}
+
+
+
+/* Initialize Neptune Internal Serdes for 1G (Neptune only) */
+
+static nxge_status_t
+nxge_1G_serdes_init(p_nxge_t nxgep)
+{
+	npi_handle_t		handle;
+	uint8_t			portn;
+	int			chan;
+	sr_rx_tx_ctrl_l_t	rx_tx_ctrl_l;
+	sr_rx_tx_ctrl_h_t	rx_tx_ctrl_h;
+	sr_glue_ctrl0_l_t	glue_ctrl0_l;
+	sr_glue_ctrl0_h_t	glue_ctrl0_h;
+	uint64_t		val;
+	uint16_t		val16l;
+	uint16_t		val16h;
+	nxge_status_t		status = NXGE_OK;
+
+	portn = nxgep->mac.portnum;
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+	    "==> nxge_1G_serdes_init port<%d>", portn));
+
+	handle = nxgep->npi_handle;
+
+	switch (portn) {
+	case 0:
+		/* Assert the reset register */
+		ESR_REG_RD(handle, ESR_RESET_REG, &val);
+		val |= ESR_RESET_0;
+		ESR_REG_WR(handle, ESR_RESET_REG, val);
+
+		/* Set the PLL register to 0x79 */
+		ESR_REG_WR(handle, ESR_0_PLL_CONFIG_REG,
+		    ESR_PLL_CFG_1G_SERDES);
+
+		/* Set the control register to 0x249249f */
+		ESR_REG_WR(handle, ESR_0_CONTROL_REG, ESR_CTL_1G_SERDES);
+
+		/* Set Serdes0 Internal Loopback if necessary */
+		if (nxgep->statsp->lb_mode == nxge_lb_serdes1000) {
+			/* Set pad loopback modes 0xaa */
+			ESR_REG_WR(handle, ESR_0_TEST_CONFIG_REG,
+			    ESR_TSTCFG_LBTEST_PAD);
+		} else {
+			ESR_REG_WR(handle, ESR_0_TEST_CONFIG_REG, 0);
+		}
+
+		/* Deassert the reset register */
+		ESR_REG_RD(handle, ESR_RESET_REG, &val);
+		val &= ~ESR_RESET_0;
+		ESR_REG_WR(handle, ESR_RESET_REG, val);
+		break;
+
+	case 1:
+		/* Assert the reset register */
+		ESR_REG_RD(handle, ESR_RESET_REG, &val);
+		val |= ESR_RESET_1;
+		ESR_REG_WR(handle, ESR_RESET_REG, val);
+
+		/* Set PLL register to 0x79 */
+		ESR_REG_WR(handle, ESR_1_PLL_CONFIG_REG,
+		    ESR_PLL_CFG_1G_SERDES);
+
+		/* Set the control register to 0x249249f */
+		ESR_REG_WR(handle, ESR_1_CONTROL_REG, ESR_CTL_1G_SERDES);
+
+		/* Set Serdes1 Internal Loopback if necessary */
+		if (nxgep->statsp->lb_mode == nxge_lb_serdes1000) {
+			/* Set pad loopback mode 0xaa */
+			ESR_REG_WR(handle, ESR_1_TEST_CONFIG_REG,
+			    ESR_TSTCFG_LBTEST_PAD);
+		} else {
+			ESR_REG_WR(handle, ESR_1_TEST_CONFIG_REG, 0);
+		}
+
+		/* Deassert the reset register */
+		ESR_REG_RD(handle, ESR_RESET_REG, &val);
+		val &= ~ESR_RESET_1;
+		ESR_REG_WR(handle, ESR_RESET_REG, val);
+		break;
+
+	default:
+		/* Nothing to do here */
+		goto done;
+	}
+
+	/* init TX RX channels */
+	for (chan = 0; chan < 4; chan++) {
+		if ((status = nxge_mdio_read(nxgep, portn,
+		    ESR_NEPTUNE_DEV_ADDR, ESR_NEP_RX_TX_CONTROL_L_ADDR(chan),
+		    &rx_tx_ctrl_l.value)) != NXGE_OK) {
+			goto fail;
+		}
+		if ((status = nxge_mdio_read(nxgep, portn,
+		    ESR_NEPTUNE_DEV_ADDR, ESR_NEP_RX_TX_CONTROL_H_ADDR(chan),
+		    &rx_tx_ctrl_h.value)) != NXGE_OK) {
+			goto fail;
+		}
+		if ((status = nxge_mdio_read(nxgep, portn,
+		    ESR_NEPTUNE_DEV_ADDR, ESR_NEP_GLUE_CONTROL0_L_ADDR(chan),
+		    &glue_ctrl0_l.value)) != NXGE_OK) {
+			goto fail;
+		}
+		if ((status = nxge_mdio_read(nxgep, portn,
+		    ESR_NEPTUNE_DEV_ADDR, ESR_NEP_GLUE_CONTROL0_H_ADDR(chan),
+		    &glue_ctrl0_h.value)) != NXGE_OK) {
+			goto fail;
+		}
+
+		rx_tx_ctrl_l.bits.enstretch = 1;
+		rx_tx_ctrl_h.bits.vmuxlo = 2;
+		rx_tx_ctrl_h.bits.vpulselo = 2;
+		glue_ctrl0_l.bits.rxlosenable = 1;
+		glue_ctrl0_l.bits.samplerate = 0xF;
+		glue_ctrl0_l.bits.thresholdcount = 0xFF;
+		glue_ctrl0_h.bits.bitlocktime = BITLOCKTIME_300_CYCLES;
+		if ((status = nxge_mdio_write(nxgep, portn,
+		    ESR_NEPTUNE_DEV_ADDR, ESR_NEP_RX_TX_CONTROL_L_ADDR(chan),
+		    rx_tx_ctrl_l.value)) != NXGE_OK) {
+			goto fail;
+		}
+		if ((status = nxge_mdio_write(nxgep, portn,
+		    ESR_NEPTUNE_DEV_ADDR, ESR_NEP_RX_TX_CONTROL_H_ADDR(chan),
+		    rx_tx_ctrl_h.value)) != NXGE_OK) {
+			goto fail;
+		}
+		if ((status = nxge_mdio_write(nxgep, portn,
+		    ESR_NEPTUNE_DEV_ADDR, ESR_NEP_GLUE_CONTROL0_L_ADDR(chan),
+		    glue_ctrl0_l.value)) != NXGE_OK) {
+			goto fail;
+		}
+		if ((status = nxge_mdio_write(nxgep, portn,
+		    ESR_NEPTUNE_DEV_ADDR, ESR_NEP_GLUE_CONTROL0_H_ADDR(chan),
+		    glue_ctrl0_h.value)) != NXGE_OK) {
+			goto fail;
+		}
+	}
+
+	if ((status = nxge_mdio_write(nxgep, portn, ESR_NEPTUNE_DEV_ADDR,
+	    ESR_NEP_RX_POWER_CONTROL_L_ADDR(), 0xfff)) != NXGE_OK) {
+		goto fail;
+	}
+	if ((status = nxge_mdio_write(nxgep, portn, ESR_NEPTUNE_DEV_ADDR,
+	    ESR_NEP_RX_POWER_CONTROL_H_ADDR(), 0xfff)) != NXGE_OK) {
+		goto fail;
+	}
+	if ((status = nxge_mdio_write(nxgep, portn, ESR_NEPTUNE_DEV_ADDR,
+	    ESR_NEP_TX_POWER_CONTROL_L_ADDR(), 0x70)) != NXGE_OK) {
+		goto fail;
+	}
+	if ((status = nxge_mdio_write(nxgep, portn, ESR_NEPTUNE_DEV_ADDR,
+	    ESR_NEP_TX_POWER_CONTROL_H_ADDR(), 0xfff)) != NXGE_OK) {
+		goto fail;
+	}
+
+	/* Apply Tx core reset */
+	if ((status = nxge_mdio_write(nxgep, portn, ESR_NEPTUNE_DEV_ADDR,
+	    ESR_NEP_RX_TX_RESET_CONTROL_L_ADDR(), (uint16_t)0)) != NXGE_OK) {
+		goto fail;
+	}
+
+	if ((status = nxge_mdio_write(nxgep, portn, ESR_NEPTUNE_DEV_ADDR,
+	    ESR_NEP_RX_TX_RESET_CONTROL_H_ADDR(), (uint16_t)0xffff)) !=
+	    NXGE_OK) {
+		goto fail;
+	}
+
+	NXGE_DELAY(200);
+
+	/* Apply Rx core reset */
+	if ((status = nxge_mdio_write(nxgep, portn, ESR_NEPTUNE_DEV_ADDR,
+	    ESR_NEP_RX_TX_RESET_CONTROL_L_ADDR(), (uint16_t)0xffff)) !=
+	    NXGE_OK) {
+		goto fail;
+	}
+
+	NXGE_DELAY(200);
+	if ((status = nxge_mdio_write(nxgep, portn, ESR_NEPTUNE_DEV_ADDR,
+	    ESR_NEP_RX_TX_RESET_CONTROL_H_ADDR(), (uint16_t)0)) != NXGE_OK) {
+		goto fail;
+	}
+
+	NXGE_DELAY(200);
+	if ((status = nxge_mdio_read(nxgep, portn, ESR_NEPTUNE_DEV_ADDR,
+	    ESR_NEP_RX_TX_RESET_CONTROL_L_ADDR(), &val16l)) != NXGE_OK) {
+		goto fail;
+	}
+	if ((status = nxge_mdio_read(nxgep, portn, ESR_NEPTUNE_DEV_ADDR,
+	    ESR_NEP_RX_TX_RESET_CONTROL_H_ADDR(), &val16h)) != NXGE_OK) {
+		goto fail;
+	}
+	if ((val16l != 0) || (val16h != 0)) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+		    "Failed to reset port<%d> XAUI Serdes "
+		    "(val16l 0x%x val16h 0x%x)", portn, val16l, val16h));
+		status = NXGE_ERROR;
+		goto fail;
+	}
+
+	NXGE_DELAY(200);
+	ESR_REG_RD(handle, ESR_INTERNAL_SIGNALS_REG, &val);
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+	    "nxge_1G_serdes_init: read internal signal reg port<%d> "
+	    "val 0x%x", portn, val));
+	if (portn == 0) {
+		if ((val & ESR_SIG_P0_BITS_MASK_1G) !=
+		    (ESR_SIG_SERDES_RDY0_P0 | ESR_SIG_DETECT0_P0)) {
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			    "nxge_1G_serdes_init: "
+			    "Failed to get Serdes up for port<%d> val 0x%x",
+			    portn, (val & ESR_SIG_P0_BITS_MASK)));
+			status = NXGE_ERROR;
+			goto fail;
+		}
+	} else if (portn == 1) {
+		if ((val & ESR_SIG_P1_BITS_MASK_1G) !=
+		    (ESR_SIG_SERDES_RDY0_P1 | ESR_SIG_DETECT0_P1)) {
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			    "nxge_neptune_serdes_init: "
+			    "Failed to get Serdes up for port<%d> val 0x%x",
+			    portn, (val & ESR_SIG_P1_BITS_MASK)));
+			status = NXGE_ERROR;
+			goto fail;
+		}
+	}
+done:
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+	    "<== nxge_1G_serdes_init port<%d>", portn));
+	return (NXGE_OK);
+fail:
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+	    "nxge_1G_serdes_init: "
+	    "Failed to initialize Neptune serdes for port<%d>",
+	    portn));
+
+	return (status);
+}
+
+
+static nxge_status_t
+nxge_neptune_10G_serdes_init(p_nxge_t nxgep)
+{
+	npi_handle_t		handle;
+	uint8_t			portn;
+	int			chan;
+	sr_rx_tx_ctrl_l_t	rx_tx_ctrl_l;
+	sr_rx_tx_ctrl_h_t	rx_tx_ctrl_h;
+	sr_glue_ctrl0_l_t	glue_ctrl0_l;
+	sr_glue_ctrl0_h_t	glue_ctrl0_h;
+	uint64_t		val;
+	uint16_t		val16l;
+	uint16_t		val16h;
+	nxge_status_t		status = NXGE_OK;
+
+	portn = nxgep->mac.portnum;
+
+	if ((portn != 0) && (portn != 1))
+		return (NXGE_OK);
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+	    "==> nxge_neptune_10G_serdes_init port<%d>", portn));
+
+	handle = nxgep->npi_handle;
+	switch (portn) {
+	case 0:
+
+		ESR_REG_WR(handle, ESR_RESET_REG, ESR_RESET_0);
+		NXGE_DELAY(20);
+		ESR_REG_WR(handle, ESR_RESET_REG, 0x0);
+		NXGE_DELAY(2000);
+
+		ESR_REG_WR(handle, ESR_0_PLL_CONFIG_REG,
+		    ESR_PLL_CFG_10G_SERDES);
+
+		ESR_REG_WR(handle, ESR_0_CONTROL_REG,
+		    ESR_CTL_EN_SYNCDET_0 | ESR_CTL_EN_SYNCDET_1 |
+		    ESR_CTL_EN_SYNCDET_2 | ESR_CTL_EN_SYNCDET_3 |
+		    (0x5 << ESR_CTL_OUT_EMPH_0_SHIFT) |
+		    (0x5 << ESR_CTL_OUT_EMPH_1_SHIFT) |
+		    (0x5 << ESR_CTL_OUT_EMPH_2_SHIFT) |
+		    (0x5 << ESR_CTL_OUT_EMPH_3_SHIFT) |
+		    (0x5 << ESR_CTL_OUT_EMPH_3_SHIFT) |
+		    (0x1 << ESR_CTL_LOSADJ_0_SHIFT) |
+		    (0x1 << ESR_CTL_LOSADJ_1_SHIFT) |
+		    (0x1 << ESR_CTL_LOSADJ_2_SHIFT) |
+		    (0x1 << ESR_CTL_LOSADJ_3_SHIFT));
+
+		/* Set Serdes0 Internal Loopback if necessary */
+		if (nxgep->statsp->lb_mode == nxge_lb_serdes10g) {
+			ESR_REG_WR(handle,
+			    ESR_0_TEST_CONFIG_REG,
+			    ESR_PAD_LOOPBACK_CH3 |
+			    ESR_PAD_LOOPBACK_CH2 |
+			    ESR_PAD_LOOPBACK_CH1 |
+			    ESR_PAD_LOOPBACK_CH0);
+		} else {
+			ESR_REG_WR(handle, ESR_0_TEST_CONFIG_REG, 0);
+		}
+		break;
+	case 1:
+
+
+		ESR_REG_WR(handle, ESR_RESET_REG, ESR_RESET_1);
+		NXGE_DELAY(20);
+		ESR_REG_WR(handle, ESR_RESET_REG, 0x0);
+		NXGE_DELAY(2000);
+
+		ESR_REG_WR(handle, ESR_1_PLL_CONFIG_REG,
+		    ESR_PLL_CFG_10G_SERDES);
+
+		ESR_REG_WR(handle, ESR_1_CONTROL_REG,
+		    ESR_CTL_EN_SYNCDET_0 | ESR_CTL_EN_SYNCDET_1 |
+		    ESR_CTL_EN_SYNCDET_2 | ESR_CTL_EN_SYNCDET_3 |
+		    (0x5 << ESR_CTL_OUT_EMPH_0_SHIFT) |
+		    (0x5 << ESR_CTL_OUT_EMPH_1_SHIFT) |
+		    (0x5 << ESR_CTL_OUT_EMPH_2_SHIFT) |
+		    (0x5 << ESR_CTL_OUT_EMPH_3_SHIFT) |
+		    (0x5 << ESR_CTL_OUT_EMPH_3_SHIFT) |
+		    (0x1 << ESR_CTL_LOSADJ_0_SHIFT) |
+		    (0x1 << ESR_CTL_LOSADJ_1_SHIFT) |
+		    (0x1 << ESR_CTL_LOSADJ_2_SHIFT) |
+		    (0x1 << ESR_CTL_LOSADJ_3_SHIFT));
+
+		/* Set Serdes1 Internal Loopback if necessary */
+		if (nxgep->statsp->lb_mode == nxge_lb_serdes10g) {
+			ESR_REG_WR(handle, ESR_1_TEST_CONFIG_REG,
+			    ESR_PAD_LOOPBACK_CH3 | ESR_PAD_LOOPBACK_CH2 |
+			    ESR_PAD_LOOPBACK_CH1 | ESR_PAD_LOOPBACK_CH0);
+		} else {
+			ESR_REG_WR(handle, ESR_1_TEST_CONFIG_REG, 0);
+		}
+		break;
+	default:
+		/* Nothing to do here */
+		goto done;
+	}
+
+	/* init TX RX channels */
+	for (chan = 0; chan < 4; chan++) {
+		if ((status = nxge_mdio_read(nxgep, portn,
+		    ESR_NEPTUNE_DEV_ADDR, ESR_NEP_RX_TX_CONTROL_L_ADDR(chan),
+					 &rx_tx_ctrl_l.value)) != NXGE_OK) {
+			NXGE_DEBUG_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_neptune_10G_serdes_init Failed MDIO read"
+				" ESR_NEP_RX_TX_CONTROL_L_ADDR Channel %d",
+				chan));
+			goto fail;
+		}
+		if ((status = nxge_mdio_read(nxgep, portn,
+		    ESR_NEPTUNE_DEV_ADDR, ESR_NEP_RX_TX_CONTROL_H_ADDR(chan),
+					 &rx_tx_ctrl_h.value)) != NXGE_OK) {
+				NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+					"nxge_neptune_10G_serdes_init Failed"
+					" MDIO read "
+					" ESR_NEP_RX_TX_CONTROL_H_ADDR"
+					" channel %d",
+					chan));
+			goto fail;
+		}
+		if ((status = nxge_mdio_read(nxgep, portn,
+		    ESR_NEPTUNE_DEV_ADDR, ESR_NEP_GLUE_CONTROL0_L_ADDR(chan),
+					 &glue_ctrl0_l.value)) != NXGE_OK) {
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"nxge_neptune_10G_serdes_init Failed"
+				" MDIO read"
+				" ESR_NEP_GLUE_CONTROL0_L_ADDR channel %d",
+				chan));
+			goto fail;
+		}
+		if ((status = nxge_mdio_read(nxgep, portn,
+		    ESR_NEPTUNE_DEV_ADDR, ESR_NEP_GLUE_CONTROL0_H_ADDR(chan),
+				 &glue_ctrl0_h.value)) != NXGE_OK) {
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_neptune_10G_serdes_init Failed MDIO read"
+			" ESR_NEP_GLUE_CONTROL0_H_ADDR channel %d",
+				chan));
+			goto fail;
+		}
+		rx_tx_ctrl_l.bits.enstretch = 1;
+		rx_tx_ctrl_h.bits.vmuxlo = 2;
+		rx_tx_ctrl_h.bits.vpulselo = 2;
+		glue_ctrl0_l.bits.rxlosenable = 1;
+		glue_ctrl0_l.bits.samplerate = 0xF;
+		glue_ctrl0_l.bits.thresholdcount = 0xFF;
+		glue_ctrl0_h.bits.bitlocktime = BITLOCKTIME_300_CYCLES;
+		if ((status = nxge_mdio_write(nxgep, portn,
+		    ESR_NEPTUNE_DEV_ADDR, ESR_NEP_RX_TX_CONTROL_L_ADDR(chan),
+				  rx_tx_ctrl_l.value)) != NXGE_OK) {
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_neptune_10G_serdes_init Failed MDIO write"
+			" ESR_NEP_RX_TX_CONTROL_L_ADDR channel %d",
+							chan));
+			goto fail;
+		}
+		if ((status = nxge_mdio_write(nxgep, portn,
+		    ESR_NEPTUNE_DEV_ADDR, ESR_NEP_RX_TX_CONTROL_H_ADDR(chan),
+					  rx_tx_ctrl_h.value)) != NXGE_OK) {
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_neptune_10G_serdes_init Failed MDIO write"
+			" ESR_NEP_RX_TX_CONTROL_H_ADDR %d",
+					chan));
+			goto fail;
+		}
+		if ((status = nxge_mdio_write(nxgep, portn,
+		    ESR_NEPTUNE_DEV_ADDR, ESR_NEP_GLUE_CONTROL0_L_ADDR(chan),
+			  glue_ctrl0_l.value)) != NXGE_OK) {
+			  NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_neptune_10G_serdes_init Failed MDIO write"
+			" ESR_NEP_GLUE_CONTROL0_L_ADDR %d",
+						chan));
+
+			goto fail;
+		}
+		if ((status = nxge_mdio_write(nxgep, portn,
+		    ESR_NEPTUNE_DEV_ADDR, ESR_NEP_GLUE_CONTROL0_H_ADDR(chan),
+					  glue_ctrl0_h.value)) != NXGE_OK) {
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_neptune_10G_serdes_init Failed MDIO write"
+				" ESR_NEP_GLUE_CONTROL0_H_ADDR %d",
+				chan));
+			goto fail;
+		}
+	}
+
+	/* Apply Tx core reset */
+	if ((status = nxge_mdio_write(nxgep, portn,
+	    ESR_NEPTUNE_DEV_ADDR, ESR_NEP_RX_TX_RESET_CONTROL_L_ADDR(),
+					  (uint16_t)0)) != NXGE_OK) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_neptune_10G_serdes_init Failed MDIO write"
+			" tx reset ESR_NEP_RX_TX_RESET_CONTROL_L_ADDR"));
+		goto fail;
+	}
+
+	if ((status = nxge_mdio_write(nxgep, portn,
+		     ESR_NEPTUNE_DEV_ADDR,
+		     ESR_NEP_RX_TX_RESET_CONTROL_H_ADDR(),
+		      (uint16_t)0xffff)) != NXGE_OK) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_neptune_10G_serdes_init Failed MDIO write"
+			" tx reset ESR_NEP_RX_TX_RESET_CONTROL_H_ADDR"));
+		goto fail;
+	}
+
+	NXGE_DELAY(200);
+
+	/* Apply Rx core reset */
+	if ((status = nxge_mdio_write(nxgep, portn, ESR_NEPTUNE_DEV_ADDR,
+			    ESR_NEP_RX_TX_RESET_CONTROL_L_ADDR(),
+			    (uint16_t)0xffff)) != NXGE_OK) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_neptune_10G_serdes_init Failed MDIO write"
+			" rx reset ESR_NEP_RX_TX_RESET_CONTROL_L_ADDR"));
+		goto fail;
+	}
+
+	NXGE_DELAY(200);
+	if ((status = nxge_mdio_write(nxgep, portn, ESR_NEPTUNE_DEV_ADDR,
+			  ESR_NEP_RX_TX_RESET_CONTROL_H_ADDR(),
+					  (uint16_t)0)) != NXGE_OK) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_neptune_10G_serdes_init Failed MDIO write"
+			" rx reset ESR_NEP_RX_TX_RESET_CONTROL_H_ADDR"));
+
+		goto fail;
+	}
+
+	NXGE_DELAY(200);
+	if ((status = nxge_mdio_read(nxgep, portn,
+		    ESR_NEPTUNE_DEV_ADDR,
+		    ESR_NEP_RX_TX_RESET_CONTROL_L_ADDR(),
+				 &val16l)) != NXGE_OK) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_neptune_10G_serdes_init Failed MDIO read"
+			" ESR_NEP_RX_TX_RESET_CONTROL_L_ADDR"));
+
+		goto fail;
+	}
+
+	if ((status = nxge_mdio_read(nxgep, portn, ESR_NEPTUNE_DEV_ADDR,
+				 ESR_NEP_RX_TX_RESET_CONTROL_H_ADDR(),
+						 &val16h)) != NXGE_OK) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_neptune_10G_serdes_init Failed MDIO read"
+			" ESR_NEP_RX_TX_RESET_CONTROL_H_ADDR"));
+		goto fail;
+	}
+	if ((val16l != 0) || (val16h != 0)) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+		    "Failed to reset port<%d> XAUI Serdes "
+		    "(val16l 0x%x val16h 0x%x)",
+		    portn, val16l, val16h));
+	}
+
+	ESR_REG_RD(handle, ESR_INTERNAL_SIGNALS_REG, &val);
+
+	if (portn == 0) {
+		if ((val & ESR_SIG_P0_BITS_MASK) !=
+				(ESR_SIG_SERDES_RDY0_P0 |
+				 ESR_SIG_DETECT0_P0 |
+				ESR_SIG_XSERDES_RDY_P0 |
+				ESR_SIG_XDETECT_P0_CH3 |
+				ESR_SIG_XDETECT_P0_CH2 |
+				ESR_SIG_XDETECT_P0_CH1 |
+				ESR_SIG_XDETECT_P0_CH0)) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"nxge_neptune_10G_serdes_init port 0 val %x"
+				" not expected", val));
+
+			goto fail;
+		}
+	} else if (portn == 1) {
+		if ((val & ESR_SIG_P1_BITS_MASK) !=
+				(ESR_SIG_SERDES_RDY0_P1 |
+				 ESR_SIG_DETECT0_P1 |
+				ESR_SIG_XSERDES_RDY_P1 |
+				ESR_SIG_XDETECT_P1_CH3 |
+				ESR_SIG_XDETECT_P1_CH2 |
+				ESR_SIG_XDETECT_P1_CH1 |
+				ESR_SIG_XDETECT_P1_CH0)) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_neptune_10G_serdes_init port 1 val %x"
+				" not expected", val));
+				goto fail;
+		}
+	}
+
+done:
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+	    "<== nxge_neptune_10G_serdes_init port<%d>", portn));
+
+	return (NXGE_OK);
+fail:
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+	    "nxge_neptune_10G_serdes_init: "
+	    "Failed to initialize Neptune serdes for port<%d>", portn));
+
+	return (status);
+}
+
+/* Initialize Neptune Internal Serdes (Neptune only) */
+nxge_status_t
+nxge_neptune_serdes_init(p_nxge_t nxgep)
+{
+
+	uint8_t			portn;
+	nxge_port_mode_t	portmode;
+	nxge_status_t		status = NXGE_OK;
+
+	portn = nxgep->mac.portnum;
+	portmode = nxgep->mac.portmode;
+
+	switch (portmode) {
+	case PORT_10G_FIBER:
+	case PORT_10G_COPPER:
+	case PORT_10G_SERDES:
+		status = nxge_neptune_10G_serdes_init(nxgep);
+		break;
+	case PORT_1G_FIBER:
+	case PORT_1G_SERDES:
+		status = nxge_1G_serdes_init(nxgep);
+		break;
+	default:
+		break;
+	}
+	return (NXGE_OK);
+}
+
+/* Look for transceiver type */
+
+nxge_status_t
+nxge_xcvr_find(p_nxge_t nxgep)
+{
+	uint8_t		portn;
+
+	portn = nxgep->mac.portnum;
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "==> nxge_xcvr_find: port<%d>", portn));
+
+	nxgep->mac.linkchkmode = LINKCHK_TIMER;
+	if ((nxgep->mac.portmode == PORT_10G_FIBER) ||
+		(nxgep->mac.portmode == PORT_10G_SERDES)){
+		nxgep->statsp->mac_stats.xcvr_inuse = PCS_XCVR;
+		if ((nxgep->niu_type == NEPTUNE) ||
+			(nxgep->niu_type == NEPTUNE_2)) {
+			nxgep->statsp->mac_stats.xcvr_portn =
+					BCM8704_NEPTUNE_PORT_ADDR_BASE + portn;
+		} else if (nxgep->niu_type == N2_NIU) {
+			nxgep->statsp->mac_stats.xcvr_portn =
+					BCM8704_N2_PORT_ADDR_BASE + portn;
+		} else
+			return (NXGE_ERROR);
+	} else if ((nxgep->mac.portmode == PORT_1G_COPPER) ||
+		(nxgep->mac.portmode == PORT_1G_RGMII_FIBER)){
+		nxgep->statsp->mac_stats.xcvr_inuse = INT_MII_XCVR;
+		/*
+		 * For Altas, Xcvr port numbers are swapped with ethernet
+		 * port number. This is designed for better signal
+		 * integrity in routing.
+		 */
+
+		switch (portn) {
+		case 0:
+			nxgep->statsp->mac_stats.xcvr_portn =
+					BCM5464_NEPTUNE_PORT_ADDR_BASE + 3;
+			break;
+		case 1:
+			nxgep->statsp->mac_stats.xcvr_portn =
+					BCM5464_NEPTUNE_PORT_ADDR_BASE + 2;
+			break;
+		case 2:
+			/*
+			 * The Alonso/Neptune, xcvr port numbers for
+			 *  port 2 and 3 are not swapped.
+			 */
+			if (strncmp(nxgep->vpd_info.bd_model,
+			    NXGE_ALONSO_BM_STR,
+			    strlen(NXGE_ALONSO_BM_STR)) != 0) {
+				nxgep->statsp->mac_stats.xcvr_portn =
+				    BCM5464_NEPTUNE_PORT_ADDR_BASE + 1;
+			} else {
+				nxgep->statsp->mac_stats.xcvr_portn =
+				    BCM5464_NEPTUNE_PORT_ADDR_BASE;
+			}
+			break;
+		case 3:
+			if (strncmp(nxgep->vpd_info.bd_model,
+			    NXGE_ALONSO_BM_STR,
+			    strlen(NXGE_ALONSO_BM_STR)) != 0) {
+				nxgep->statsp->mac_stats.xcvr_portn =
+				    BCM5464_NEPTUNE_PORT_ADDR_BASE;
+			} else {
+				nxgep->statsp->mac_stats.xcvr_portn =
+				    BCM5464_NEPTUNE_PORT_ADDR_BASE + 1;
+			}
+			break;
+
+		default:
+			return (NXGE_ERROR);
+		}
+	} else if ((nxgep->mac.portmode == PORT_1G_FIBER) ||
+		(nxgep->mac.portmode == PORT_1G_SERDES)) {
+		nxgep->statsp->mac_stats.xcvr_inuse = PCS_XCVR;
+		nxgep->statsp->mac_stats.xcvr_portn = portn;
+	} else {
+		return (NXGE_ERROR);
+	}
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+		        "<== nxge_xcvr_find: xcvr_inuse = %d",
+			nxgep->statsp->mac_stats.xcvr_inuse));
+	return (NXGE_OK);
+}
+
+/* Initialize transceiver */
+
+nxge_status_t
+nxge_xcvr_init(p_nxge_t nxgep)
+{
+	p_nxge_param_t		param_arr;
+	p_nxge_stats_t		statsp;
+	uint8_t			portn;
+	uint16_t		val;
+	uint8_t			phy_port_addr;
+	pmd_tx_control_t	tx_ctl;
+	control_t		ctl;
+	phyxs_control_t		phyxs_ctl;
+	pcs_control_t		pcs_ctl;
+	uint32_t		delay = 0;
+	optics_dcntr_t		op_ctr;
+	nxge_status_t		status = NXGE_OK;
+
+	portn = nxgep->mac.portnum;
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "==> nxge_xcvr_init: port<%d>", portn));
+
+	param_arr = nxgep->param_arr;
+	statsp = nxgep->statsp;
+
+	/*
+	 * Initialize parameter array.
+	 */
+
+	/*
+	 * Initialise the xcvr statistics.
+	 */
+	statsp->mac_stats.cap_autoneg = 0;
+	statsp->mac_stats.cap_100T4 = 0;
+	statsp->mac_stats.cap_100fdx = 0;
+	statsp->mac_stats.cap_100hdx = 0;
+	statsp->mac_stats.cap_10fdx = 0;
+	statsp->mac_stats.cap_10hdx = 0;
+	statsp->mac_stats.cap_asmpause = 0;
+	statsp->mac_stats.cap_pause = 0;
+	statsp->mac_stats.cap_1000fdx = 0;
+	statsp->mac_stats.cap_1000hdx = 0;
+	statsp->mac_stats.cap_10gfdx = 0;
+	statsp->mac_stats.cap_10ghdx = 0;
+
+	/*
+	 * Initialize the link statistics.
+	 */
+	statsp->mac_stats.link_T4 = 0;
+	statsp->mac_stats.link_asmpause = 0;
+	statsp->mac_stats.link_pause = 0;
+
+	phy_port_addr = nxgep->statsp->mac_stats.xcvr_portn;
+
+	switch (nxgep->mac.portmode) {
+	case PORT_10G_FIBER:
+		/* Disable Link LEDs */
+		if (nxge_10g_link_led_off(nxgep) != NXGE_OK)
+			goto fail;
+
+		/* Set Clause 45 */
+		npi_mac_mif_set_indirect_mode(nxgep->npi_handle, B_TRUE);
+
+		/* Reset the transceiver */
+		if ((status = nxge_mdio_read(nxgep,
+				phy_port_addr,
+				BCM8704_PHYXS_ADDR,
+				BCM8704_PHYXS_CONTROL_REG,
+				&phyxs_ctl.value)) != NXGE_OK)
+			goto fail;
+
+		phyxs_ctl.bits.reset = 1;
+		if ((status = nxge_mdio_write(nxgep,
+				phy_port_addr,
+				BCM8704_PHYXS_ADDR,
+				BCM8704_PHYXS_CONTROL_REG,
+				phyxs_ctl.value)) != NXGE_OK)
+			goto fail;
+
+		do {
+			NXGE_DELAY(500);
+			if ((status = nxge_mdio_read(nxgep,
+					phy_port_addr,
+					BCM8704_PHYXS_ADDR,
+					BCM8704_PHYXS_CONTROL_REG,
+					&phyxs_ctl.value)) != NXGE_OK)
+				goto fail;
+			delay++;
+		} while ((phyxs_ctl.bits.reset) && (delay < 100));
+		if (delay == 100) {
+			NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+				"nxge_xcvr_init: "
+				"failed to reset Transceiver on port<%d>",
+				portn));
+			status = NXGE_ERROR;
+			goto fail;
+		}
+
+		/* Set to 0x7FBF */
+		ctl.value = 0;
+		ctl.bits.res1 = 0x3F;
+		ctl.bits.optxon_lvl = 1;
+		ctl.bits.oprxflt_lvl = 1;
+		ctl.bits.optrxlos_lvl = 1;
+		ctl.bits.optxflt_lvl = 1;
+		ctl.bits.opprflt_lvl = 1;
+		ctl.bits.obtmpflt_lvl = 1;
+		ctl.bits.opbiasflt_lvl = 1;
+		ctl.bits.optxrst_lvl = 1;
+		if ((status = nxge_mdio_write(nxgep,
+				phy_port_addr,
+				BCM8704_USER_DEV3_ADDR,
+				BCM8704_USER_CONTROL_REG, ctl.value))
+				!= NXGE_OK)
+			goto fail;
+
+		/* Set to 0x164 */
+		tx_ctl.value = 0;
+		tx_ctl.bits.tsck_lpwren = 1;
+		tx_ctl.bits.tx_dac_txck = 0x2;
+		tx_ctl.bits.tx_dac_txd = 0x1;
+		tx_ctl.bits.xfp_clken = 1;
+		if ((status = nxge_mdio_write(nxgep,
+				phy_port_addr,
+				BCM8704_USER_DEV3_ADDR,
+				BCM8704_USER_PMD_TX_CONTROL_REG, tx_ctl.value))
+				!= NXGE_OK)
+			goto fail;
+		/*
+		 * According to Broadcom's instruction, SW needs to read
+		 * back these registers twice after written.
+		 */
+		if ((status = nxge_mdio_read(nxgep,
+				phy_port_addr,
+				BCM8704_USER_DEV3_ADDR,
+				BCM8704_USER_CONTROL_REG, &val))
+				!= NXGE_OK)
+			goto fail;
+
+		if ((status = nxge_mdio_read(nxgep,
+				phy_port_addr,
+				BCM8704_USER_DEV3_ADDR,
+				BCM8704_USER_CONTROL_REG, &val))
+				!= NXGE_OK)
+			goto fail;
+
+		if ((status = nxge_mdio_read(nxgep,
+				phy_port_addr,
+				BCM8704_USER_DEV3_ADDR,
+				BCM8704_USER_PMD_TX_CONTROL_REG, &val))
+				!= NXGE_OK)
+			goto fail;
+
+		if ((status = nxge_mdio_read(nxgep,
+				phy_port_addr,
+				BCM8704_USER_DEV3_ADDR,
+				BCM8704_USER_PMD_TX_CONTROL_REG, &val))
+				!= NXGE_OK)
+			goto fail;
+
+
+		/* Enable Tx and Rx LEDs to be driven by traffic */
+		if ((status = nxge_mdio_read(nxgep,
+					phy_port_addr,
+					BCM8704_USER_DEV3_ADDR,
+					BCM8704_USER_OPTICS_DIGITAL_CTRL_REG,
+					&op_ctr.value)) != NXGE_OK)
+			goto fail;
+		op_ctr.bits.gpio_sel = 0x3;
+		if ((status = nxge_mdio_write(nxgep,
+					phy_port_addr,
+					BCM8704_USER_DEV3_ADDR,
+					BCM8704_USER_OPTICS_DIGITAL_CTRL_REG,
+					op_ctr.value)) != NXGE_OK)
+			goto fail;
+
+		NXGE_DELAY(1000000);
+
+		/* Set BCM8704 Internal Loopback mode if necessary */
+		if ((status = nxge_mdio_read(nxgep,
+					phy_port_addr,
+					BCM8704_PCS_DEV_ADDR,
+					BCM8704_PCS_CONTROL_REG,
+					&pcs_ctl.value)) != NXGE_OK)
+			goto fail;
+		if (nxgep->statsp->lb_mode == nxge_lb_phy10g)
+			pcs_ctl.bits.loopback = 1;
+		else
+			pcs_ctl.bits.loopback = 0;
+		if ((status = nxge_mdio_write(nxgep,
+					phy_port_addr,
+					BCM8704_PCS_DEV_ADDR,
+					BCM8704_PCS_CONTROL_REG,
+					pcs_ctl.value)) != NXGE_OK)
+			goto fail;
+
+		status = nxge_mdio_read(nxgep, phy_port_addr,
+				0x1, 0xA, &val);
+		if (status != NXGE_OK)
+			goto fail;
+		NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+				"BCM8704 port<%d> Dev 1 Reg 0xA = 0x%x\n",
+				portn, val));
+		status = nxge_mdio_read(nxgep, phy_port_addr, 0x3, 0x20, &val);
+		if (status != NXGE_OK)
+			goto fail;
+		NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+				"BCM8704 port<%d> Dev 3 Reg 0x20 = 0x%x\n",
+				portn, val));
+		status = nxge_mdio_read(nxgep, phy_port_addr, 0x4, 0x18, &val);
+		if (status != NXGE_OK)
+			goto fail;
+		NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+				"BCM8704 port<%d> Dev 4 Reg 0x18 = 0x%x\n",
+				portn, val));
+
+	case PORT_10G_SERDES:
+		statsp->mac_stats.cap_10gfdx = 1;
+		statsp->mac_stats.lp_cap_10gfdx = 1;
+		break;
+	case PORT_10G_COPPER:
+		break;
+	case PORT_1G_FIBER:
+	case PORT_1G_COPPER:
+		/* Set Clause 22 */
+		npi_mac_mif_set_indirect_mode(nxgep->npi_handle, B_FALSE);
+
+		/* Set capability flags */
+		statsp->mac_stats.adv_cap_autoneg =
+				param_arr[param_autoneg].value;
+		statsp->mac_stats.cap_1000fdx =
+				param_arr[param_anar_1000fdx].value;
+		statsp->mac_stats.cap_100fdx =
+				param_arr[param_anar_100fdx].value;
+		statsp->mac_stats.cap_10fdx = param_arr[param_anar_10fdx].value;
+
+		if ((status = nxge_mii_xcvr_init(nxgep)) != NXGE_OK)
+			goto fail;
+		break;
+
+	case PORT_1G_RGMII_FIBER:
+		/* Set Clause 22 */
+		npi_mac_mif_set_indirect_mode(nxgep->npi_handle, B_FALSE);
+		/* Set capability flags */
+		statsp->mac_stats.cap_1000fdx =
+		    param_arr[param_anar_1000fdx].value;
+		if ((status = nxge_mii_xcvr_fiber_init(nxgep)) != NXGE_OK) {
+			goto fail;
+		}
+		break;
+
+	case PORT_1G_SERDES:
+		statsp->mac_stats.cap_1000fdx =
+		    param_arr[param_anar_1000fdx].value;
+		break;
+	default:
+		goto fail;
+	}
+
+	statsp->mac_stats.xcvr_inits++;
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "==> nxge_xcvr_init: port<%d>", portn));
+	return (NXGE_OK);
+
+fail:
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+		"nxge_xcvr_init: failed to initialize transceiver for port<%d>",
+		portn));
+	return (status);
+}
+
+
+/* Initialize the TxMAC sub-block */
+
+nxge_status_t
+nxge_tx_mac_init(p_nxge_t nxgep)
+{
+	npi_attr_t		ap;
+	uint8_t			portn;
+	nxge_port_mode_t	portmode;
+	nxge_port_t		portt;
+	npi_handle_t		handle;
+	npi_status_t		rs = NPI_SUCCESS;
+
+	portn = NXGE_GET_PORT_NUM(nxgep->function_num);
+	portt    = nxgep->mac.porttype;
+	handle   = nxgep->npi_handle;
+	portmode = nxgep->mac.portmode;
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "==> nxge_tx_mac_init: port<%d>",
+			portn));
+
+	/* Set Max and Min Frame Size */
+	if (nxgep->mac.is_jumbo == B_TRUE) {
+		SET_MAC_ATTR2(handle, ap, portn,
+		    MAC_PORT_FRAME_SIZE, 64, nxge_jumbo_mtu, rs);
+	} else {
+		SET_MAC_ATTR2(handle, ap, portn,
+		    MAC_PORT_FRAME_SIZE, 64, 0x5EE + 4, rs);
+	}
+
+	if (rs != NPI_SUCCESS)
+		goto fail;
+	if (nxgep->mac.is_jumbo == B_TRUE)
+		nxgep->mac.maxframesize = nxge_jumbo_mtu;
+	else
+		nxgep->mac.maxframesize = 0x5EE + 4;
+	nxgep->mac.minframesize = 64;
+
+	if (portt == PORT_TYPE_XMAC) {
+		if ((rs = npi_xmac_tx_iconfig(handle, INIT, portn,
+				0)) != NPI_SUCCESS)
+			goto fail;
+		nxgep->mac.tx_iconfig = NXGE_XMAC_TX_INTRS;
+		if ((portmode == PORT_10G_FIBER) ||
+			(portmode == PORT_10G_SERDES) ||
+					(portmode == PORT_10G_COPPER)) {
+			SET_MAC_ATTR1(handle, ap, portn, XMAC_10G_PORT_IPG,
+					XGMII_IPG_12_15, rs);
+			if (rs != NPI_SUCCESS)
+				goto fail;
+			nxgep->mac.ipg[0] = XGMII_IPG_12_15;
+		} else {
+			SET_MAC_ATTR1(handle, ap, portn, XMAC_PORT_IPG,
+					MII_GMII_IPG_12, rs);
+			if (rs != NPI_SUCCESS)
+				goto fail;
+			nxgep->mac.ipg[0] = MII_GMII_IPG_12;
+		}
+		if ((rs = npi_xmac_tx_config(handle, INIT, portn,
+				CFG_XMAC_TX_CRC | CFG_XMAC_TX)) != NPI_SUCCESS)
+			goto fail;
+		nxgep->mac.tx_config = CFG_XMAC_TX_CRC | CFG_XMAC_TX;
+		nxgep->mac.maxburstsize = 0;	/* not programmable */
+		nxgep->mac.ctrltype = 0;	/* not programmable */
+		nxgep->mac.pa_size = 0;		/* not programmable */
+
+		if ((rs = npi_xmac_zap_tx_counters(handle, portn))
+							!= NPI_SUCCESS)
+			goto fail;
+
+	} else {
+		if ((rs = npi_bmac_tx_iconfig(handle, INIT, portn,
+				0)) != NPI_SUCCESS)
+			goto fail;
+		nxgep->mac.tx_iconfig = NXGE_BMAC_TX_INTRS;
+
+		SET_MAC_ATTR1(handle, ap, portn, BMAC_PORT_CTRL_TYPE, 0x8808,
+				rs);
+		if (rs != NPI_SUCCESS)
+			goto fail;
+		nxgep->mac.ctrltype = 0x8808;
+
+		SET_MAC_ATTR1(handle, ap, portn, BMAC_PORT_PA_SIZE, 0x7, rs);
+		if (rs != NPI_SUCCESS)
+			goto fail;
+		nxgep->mac.pa_size = 0x7;
+
+		if ((rs = npi_bmac_tx_config(handle, INIT, portn,
+				CFG_BMAC_TX_CRC | CFG_BMAC_TX)) != NPI_SUCCESS)
+			goto fail;
+		nxgep->mac.tx_config = CFG_BMAC_TX_CRC | CFG_BMAC_TX;
+	}
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "<== nxge_tx_mac_init: port<%d>",
+			portn));
+
+	return (NXGE_OK);
+fail:
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+		"nxge_tx_mac_init: failed to initialize port<%d> TXMAC",
+					portn));
+
+	return (NXGE_ERROR | rs);
+}
+
+/* Initialize the RxMAC sub-block */
+
+nxge_status_t
+nxge_rx_mac_init(p_nxge_t nxgep)
+{
+	npi_attr_t		ap;
+	uint32_t		i;
+	npi_mac_addr_t		altmac_e;
+	nxge_port_t		portt;
+	uint8_t			portn;
+	npi_handle_t		handle;
+	npi_status_t		rs = NPI_SUCCESS;
+	uint16_t 		*addr16p;
+	uint16_t 		addr0, addr1, addr2;
+	xmac_rx_config_t	xconfig;
+	bmac_rx_config_t	bconfig;
+
+	portn = NXGE_GET_PORT_NUM(nxgep->function_num);
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "==> nxge_rx_mac_init: port<%d>\n",
+			portn));
+	handle = nxgep->npi_handle;
+	portt = nxgep->mac.porttype;
+
+	addr16p = (uint16_t *)nxgep->mac.mac_addr.ether_addr_octet;
+	addr0 = ntohs(addr16p[2]);
+	addr1 = ntohs(addr16p[1]);
+	addr2 = ntohs(addr16p[0]);
+	SET_MAC_ATTR3(handle, ap, portn, MAC_PORT_ADDR, addr0, addr1, addr2,
+		rs);
+
+	if (rs != NPI_SUCCESS)
+		goto fail;
+	SET_MAC_ATTR3(handle, ap, portn, MAC_PORT_ADDR_FILTER, 0, 0, 0, rs);
+	if (rs != NPI_SUCCESS)
+		goto fail;
+	SET_MAC_ATTR2(handle, ap, portn, MAC_PORT_ADDR_FILTER_MASK, 0, 0, rs);
+	if (rs != NPI_SUCCESS)
+		goto fail;
+
+
+	/*
+	* Load the multicast hash filter bits.
+	*/
+	for (i = 0; i < MAC_MAX_HASH_ENTRY; i++) {
+		NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+			    "==> nxge_rx_mac_init: port%d"
+			    " hash[%d]=0x%x",
+			    portn, i, nxgep->hash_table[i]));
+
+		rs = npi_mac_hashtab_entry(handle, OP_SET, portn, i,
+			    (uint16_t *)&(nxgep->hash_table[i]));
+		if (rs != NPI_SUCCESS)
+		    goto fail;
+	}
+
+	if (portt == PORT_TYPE_XMAC) {
+		if ((rs = npi_xmac_rx_iconfig(handle, INIT, portn,
+				0)) != NPI_SUCCESS)
+			goto fail;
+		nxgep->mac.rx_iconfig = NXGE_XMAC_RX_INTRS;
+
+		altmac_e.w0 = 0;
+		altmac_e.w1 = 0;
+		altmac_e.w2 = 0;
+		for (i = 0; i < XMAC_MAX_ALT_ADDR_ENTRY; i++) {
+			if ((rs = npi_mac_altaddr_entry(handle, OP_SET, portn,
+				i, (npi_mac_addr_t *)&altmac_e)) != NPI_SUCCESS)
+				goto fail;
+		}
+
+		(void) nxge_fflp_init_hostinfo(nxgep);
+
+		xconfig = CFG_XMAC_RX_ERRCHK | CFG_XMAC_RX_CRC_CHK |
+			CFG_XMAC_RX | CFG_XMAC_RX_CODE_VIO_CHK;
+
+		if (nxge_dont_strip_crc == B_FALSE)
+			xconfig |= CFG_XMAC_RX_STRIP_CRC;
+
+		if (nxgep->if_flags & PROMISC_F)
+			xconfig |= CFG_XMAC_RX_PROMISCUOUS;
+		if (nxgep->if_flags & ALLMULTI_F)
+			xconfig |= CFG_XMAC_RX_PROMISCUOUSGROUP;
+		if (nxgep->if_flags & HASHMATCH_F)
+			xconfig |= CFG_XMAC_RX_HASH_FILTER;
+
+		if ((rs = npi_xmac_rx_config(handle, INIT, portn,
+					xconfig)) != NPI_SUCCESS)
+			goto fail;
+		nxgep->mac.rx_config = xconfig;
+
+		/* Comparison of mac unique address is always enabled on XMAC */
+
+		if ((rs = npi_xmac_zap_rx_counters(handle, portn))
+							!= NPI_SUCCESS)
+			goto fail;
+	} else {
+		altmac_e.w0 = 0;
+		altmac_e.w1 = 0;
+		altmac_e.w2 = 0;
+		for (i = 0; i < BMAC_MAX_ALT_ADDR_ENTRY; i++) {
+			if ((rs = npi_mac_altaddr_entry(handle, OP_SET, portn,
+				i, (npi_mac_addr_t *)&altmac_e)) != NPI_SUCCESS)
+				goto fail;
+		}
+
+		(void) nxge_fflp_init_hostinfo(nxgep);
+
+		if (npi_bmac_rx_iconfig(nxgep->npi_handle, INIT, portn,
+					0) != NPI_SUCCESS)
+			goto fail;
+		nxgep->mac.rx_iconfig = NXGE_BMAC_RX_INTRS;
+
+		bconfig = CFG_BMAC_RX_DISCARD_ON_ERR | CFG_BMAC_RX;
+
+		if (nxge_dont_strip_crc == B_FALSE)
+			bconfig |= CFG_BMAC_RX_STRIP_CRC;
+
+		if (nxgep->if_flags & PROMISC_F)
+			bconfig |= CFG_BMAC_RX_PROMISCUOUS;
+
+		if (nxgep->if_flags & ALLMULTI_F)
+			bconfig |= CFG_BMAC_RX_PROMISCUOUSGROUP;
+		if (nxgep->if_flags & HASHMATCH_F)
+			bconfig |= CFG_BMAC_RX_HASH_FILTER;
+		if ((rs = npi_bmac_rx_config(handle, INIT, portn,
+					bconfig)) != NPI_SUCCESS)
+			goto fail;
+		nxgep->mac.rx_config = bconfig;
+
+		/* Always enable comparison of mac unique address */
+		if ((rs = npi_mac_altaddr_enable(handle, portn, 0))
+					!= NPI_SUCCESS)
+			goto fail;
+	}
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "<== nxge_rx_mac_init: port<%d>\n",
+			portn));
+
+	return (NXGE_OK);
+
+fail:
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+		"nxge_rx_mac_init: Failed to Initialize port<%d> RxMAC",
+				portn));
+
+	return (NXGE_ERROR | rs);
+}
+
+/* Enable TXMAC */
+
+nxge_status_t
+nxge_tx_mac_enable(p_nxge_t nxgep)
+{
+	npi_handle_t	handle;
+	npi_status_t	rs = NPI_SUCCESS;
+	nxge_status_t	status = NXGE_OK;
+
+	handle = nxgep->npi_handle;
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "==> nxge_tx_mac_enable: port<%d>",
+			nxgep->mac.portnum));
+
+	if ((status = nxge_tx_mac_init(nxgep)) != NXGE_OK)
+		goto fail;
+
+
+	if (nxgep->mac.porttype == PORT_TYPE_XMAC) {
+		if ((rs = npi_xmac_tx_config(handle, ENABLE, nxgep->mac.portnum,
+						CFG_XMAC_TX)) != NPI_SUCCESS)
+			goto fail;
+	} else {
+		if ((rs = npi_bmac_tx_config(handle, ENABLE, nxgep->mac.portnum,
+						CFG_BMAC_TX)) != NPI_SUCCESS)
+			goto fail;
+	}
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "<== nxge_tx_mac_enable: port<%d>",
+			nxgep->mac.portnum));
+
+	return (NXGE_OK);
+fail:
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxgep_tx_mac_enable: Failed to enable port<%d> TxMAC",
+			nxgep->mac.portnum));
+	if (rs != NPI_SUCCESS)
+		return (NXGE_ERROR | rs);
+	else
+		return (status);
+}
+
+/* Disable TXMAC */
+
+nxge_status_t
+nxge_tx_mac_disable(p_nxge_t nxgep)
+{
+	npi_handle_t	handle;
+	npi_status_t	rs = NPI_SUCCESS;
+
+	handle = nxgep->npi_handle;
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "==> nxge_tx_mac_disable: port<%d>",
+			nxgep->mac.portnum));
+
+	if (nxgep->mac.porttype == PORT_TYPE_XMAC) {
+		if ((rs = npi_xmac_tx_config(handle, DISABLE,
+			nxgep->mac.portnum, CFG_XMAC_TX)) != NPI_SUCCESS)
+			goto fail;
+	} else {
+		if ((rs = npi_bmac_tx_config(handle, DISABLE,
+			nxgep->mac.portnum, CFG_BMAC_TX)) != NPI_SUCCESS)
+			goto fail;
+	}
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "<== nxge_tx_mac_disable: port<%d>",
+			nxgep->mac.portnum));
+	return (NXGE_OK);
+fail:
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_tx_mac_disable: Failed to disable port<%d> TxMAC",
+			nxgep->mac.portnum));
+	return (NXGE_ERROR | rs);
+}
+
+/* Enable RXMAC */
+
+nxge_status_t
+nxge_rx_mac_enable(p_nxge_t nxgep)
+{
+	npi_handle_t	handle;
+	uint8_t 	portn;
+	npi_status_t	rs = NPI_SUCCESS;
+	nxge_status_t	status = NXGE_OK;
+
+	handle = nxgep->npi_handle;
+	portn = nxgep->mac.portnum;
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "==> nxge_rx_mac_enable: port<%d>",
+			portn));
+
+	if ((status = nxge_rx_mac_init(nxgep)) != NXGE_OK)
+		goto fail;
+
+	if (nxgep->mac.porttype == PORT_TYPE_XMAC) {
+		if ((rs = npi_xmac_rx_config(handle, ENABLE, portn,
+						CFG_XMAC_RX)) != NPI_SUCCESS)
+			goto fail;
+	} else {
+		if ((rs = npi_bmac_rx_config(handle, ENABLE, portn,
+						CFG_BMAC_RX)) != NPI_SUCCESS)
+			goto fail;
+	}
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "<== nxge_rx_mac_enable: port<%d>",
+			portn));
+
+	return (NXGE_OK);
+fail:
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxgep_rx_mac_enable: Failed to enable port<%d> RxMAC",
+			portn));
+
+	if (rs != NPI_SUCCESS)
+		return (NXGE_ERROR | rs);
+	else
+		return (status);
+}
+
+/* Disable RXMAC */
+
+nxge_status_t
+nxge_rx_mac_disable(p_nxge_t nxgep)
+{
+	npi_handle_t	handle;
+	uint8_t		portn;
+	npi_status_t	rs = NPI_SUCCESS;
+
+	handle = nxgep->npi_handle;
+	portn = nxgep->mac.portnum;
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "==> nxge_rx_mac_disable: port<%d>",
+			portn));
+
+	if (nxgep->mac.porttype == PORT_TYPE_XMAC) {
+		if ((rs = npi_xmac_rx_config(handle, DISABLE, portn,
+						CFG_XMAC_RX)) != NPI_SUCCESS)
+			goto fail;
+	} else {
+		if ((rs = npi_bmac_rx_config(handle, DISABLE, portn,
+						CFG_BMAC_RX)) != NPI_SUCCESS)
+			goto fail;
+	}
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "<== nxge_rx_mac_disable: port<%d>",
+			portn));
+	return (NXGE_OK);
+fail:
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxgep_rx_mac_disable: ",
+			"Failed to disable port<%d> RxMAC",
+			portn));
+
+	return (NXGE_ERROR | rs);
+}
+
+/* Reset TXMAC */
+
+nxge_status_t
+nxge_tx_mac_reset(p_nxge_t nxgep)
+{
+	npi_handle_t	handle;
+	uint8_t		portn;
+	npi_status_t	rs = NPI_SUCCESS;
+
+	handle = nxgep->npi_handle;
+	portn = nxgep->mac.portnum;
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "==> nxge_tx_mac_reset: port<%d>",
+			portn));
+
+	if (nxgep->mac.porttype == PORT_TYPE_XMAC) {
+		if ((rs = npi_xmac_reset(handle, portn, XTX_MAC_RESET_ALL))
+		    != NPI_SUCCESS)
+			goto fail;
+	} else {
+		if ((rs = npi_bmac_reset(handle, portn, TX_MAC_RESET))
+					!= NPI_SUCCESS)
+			goto fail;
+	}
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "<== nxge_tx_mac_reset: port<%d>",
+			portn));
+
+	return (NXGE_OK);
+fail:
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_tx_mac_reset: Failed to Reset TxMAC port<%d>",
+			portn));
+
+	return (NXGE_ERROR | rs);
+}
+
+/* Reset RXMAC */
+
+nxge_status_t
+nxge_rx_mac_reset(p_nxge_t nxgep)
+{
+	npi_handle_t	handle;
+	uint8_t		portn;
+	npi_status_t	rs = NPI_SUCCESS;
+
+	handle = nxgep->npi_handle;
+	portn = nxgep->mac.portnum;
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "==> nxge_rx_mac_reset: port<%d>",
+			portn));
+
+	if (nxgep->mac.porttype == PORT_TYPE_XMAC) {
+		if ((rs = npi_xmac_reset(handle, portn, XRX_MAC_RESET_ALL))
+		    != NPI_SUCCESS)
+		goto fail;
+	} else {
+		if ((rs = npi_bmac_reset(handle, portn, RX_MAC_RESET))
+					!= NPI_SUCCESS)
+		goto fail;
+	}
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "<== nxge_rx_mac_reset: port<%d>",
+			portn));
+
+	return (NXGE_OK);
+fail:
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_rx_mac_reset: Failed to Reset RxMAC port<%d>",
+			portn));
+	return (NXGE_ERROR | rs);
+}
+
+
+/* Enable/Disable MII Link Status change interrupt */
+
+nxge_status_t
+nxge_link_intr(p_nxge_t nxgep, link_intr_enable_t enable)
+{
+	uint8_t			portn;
+	nxge_port_mode_t	portmode;
+	npi_status_t		rs = NPI_SUCCESS;
+
+	portn = nxgep->mac.portnum;
+	portmode = nxgep->mac.portmode;
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "==> nxge_link_intr: port<%d>", portn));
+
+	if (enable == LINK_INTR_START) {
+		if ((portmode == PORT_10G_FIBER) ||
+			(portmode == PORT_10G_SERDES)) {
+			if ((rs = npi_xmac_xpcs_link_intr_enable(
+						nxgep->npi_handle,
+						portn)) != NPI_SUCCESS)
+				goto fail;
+		} else if ((portmode == PORT_1G_FIBER) ||
+				   (portmode == PORT_1G_SERDES)) {
+			if ((rs = npi_mac_pcs_link_intr_enable(
+						nxgep->npi_handle,
+						portn)) != NPI_SUCCESS)
+				goto fail;
+		} else if ((portmode == PORT_1G_COPPER) ||
+			(portmode == PORT_1G_RGMII_FIBER)) {
+			if ((rs = npi_mac_mif_link_intr_enable(
+				nxgep->npi_handle,
+				portn, NXGE_MII_BMSR, BMSR_LSTATUS)) != NPI_SUCCESS)
+				goto fail;
+		} else
+			goto fail;
+	} else if (enable == LINK_INTR_STOP) {
+		if ((portmode == PORT_10G_FIBER) ||
+			(portmode == PORT_10G_SERDES)) {
+			if ((rs = npi_xmac_xpcs_link_intr_disable(
+						nxgep->npi_handle,
+						portn)) != NPI_SUCCESS)
+				goto fail;
+		} else  if ((portmode == PORT_1G_FIBER) ||
+			(portmode == PORT_1G_SERDES)) {
+			if ((rs = npi_mac_pcs_link_intr_disable(
+						nxgep->npi_handle,
+						portn)) != NPI_SUCCESS)
+				goto fail;
+		} else if ((portmode == PORT_1G_COPPER) ||
+			(portmode == PORT_1G_RGMII_FIBER)) {
+			if ((rs = npi_mac_mif_link_intr_disable(
+						nxgep->npi_handle,
+						portn)) != NPI_SUCCESS)
+				goto fail;
+		} else
+			goto fail;
+	}
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "<== nxge_link_intr: port<%d>", portn));
+
+	return (NXGE_OK);
+fail:
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_link_intr: Failed to set port<%d> mif intr mode",
+			portn));
+
+	return (NXGE_ERROR | rs);
+}
+
+/* Initialize 1G Fiber / Copper transceiver using Clause 22 */
+
+nxge_status_t
+nxge_mii_xcvr_init(p_nxge_t nxgep)
+{
+	p_nxge_param_t	param_arr;
+	p_nxge_stats_t	statsp;
+	uint8_t		xcvr_portn;
+	p_mii_regs_t	mii_regs;
+	mii_bmcr_t	bmcr;
+	mii_bmsr_t	bmsr;
+	mii_anar_t	anar;
+	mii_gcr_t	gcr;
+	mii_esr_t	esr;
+	mii_aux_ctl_t	bcm5464r_aux;
+	int		status = NXGE_OK;
+
+	uint_t delay;
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "==> nxge_mii_xcvr_init"));
+
+	param_arr = nxgep->param_arr;
+	statsp = nxgep->statsp;
+	xcvr_portn = statsp->mac_stats.xcvr_portn;
+
+	mii_regs = NULL;
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+		"nxge_param_autoneg = 0x%02x", param_arr[param_autoneg].value));
+
+	/*
+	 * The mif phy mode may be connected to either a copper link
+	 * or fiber link. Read the mode control register to get the fiber
+	 * configuration if it is hard-wired to fiber link.
+	 */
+	(void) nxge_mii_get_link_mode(nxgep);
+	if (nxgep->mac.portmode == PORT_1G_RGMII_FIBER) {
+		return (nxge_mii_xcvr_fiber_init(nxgep));
+	}
+
+	/*
+	 * Reset the transceiver.
+	 */
+	delay = 0;
+	bmcr.value = 0;
+	bmcr.bits.reset = 1;
+	if ((status = nxge_mii_write(nxgep, xcvr_portn,
+		(uint8_t)(uint64_t)&mii_regs->bmcr, bmcr.value)) != NXGE_OK)
+		goto fail;
+	do {
+		NXGE_DELAY(500);
+		if ((status = nxge_mii_read(nxgep, xcvr_portn,
+			(uint8_t)(uint64_t)&mii_regs->bmcr, &bmcr.value))
+				!= NXGE_OK)
+			goto fail;
+		delay++;
+	} while ((bmcr.bits.reset) && (delay < 1000));
+	if (delay == 1000) {
+		NXGE_DEBUG_MSG((nxgep, MAC_CTL, "Xcvr reset failed."));
+		goto fail;
+	}
+
+	if ((status = nxge_mii_read(nxgep, xcvr_portn,
+			(uint8_t)(uint64_t)(&mii_regs->bmsr),
+			&bmsr.value)) != NXGE_OK)
+		goto fail;
+
+
+	param_arr[param_autoneg].value &= bmsr.bits.auto_neg_able;
+	param_arr[param_anar_100T4].value &= bmsr.bits.link_100T4;
+	param_arr[param_anar_100fdx].value &= bmsr.bits.link_100fdx;
+	param_arr[param_anar_100hdx].value = 0;
+	param_arr[param_anar_10fdx].value &= bmsr.bits.link_10fdx;
+	param_arr[param_anar_10hdx].value = 0;
+
+	/*
+	 * Initialise the xcvr statistics.
+	 */
+	statsp->mac_stats.cap_autoneg = bmsr.bits.auto_neg_able;
+	statsp->mac_stats.cap_100T4 = bmsr.bits.link_100T4;
+	statsp->mac_stats.cap_100fdx = bmsr.bits.link_100fdx;
+	statsp->mac_stats.cap_100hdx = 0;
+	statsp->mac_stats.cap_10fdx = bmsr.bits.link_10fdx;
+	statsp->mac_stats.cap_10hdx = 0;
+	statsp->mac_stats.cap_asmpause = param_arr[param_anar_asmpause].value;
+	statsp->mac_stats.cap_pause = param_arr[param_anar_pause].value;
+
+	/*
+	 * Initialise the xcvr advertised capability statistics.
+	 */
+	statsp->mac_stats.adv_cap_autoneg = param_arr[param_autoneg].value;
+	statsp->mac_stats.adv_cap_1000fdx = param_arr[param_anar_1000fdx].value;
+	statsp->mac_stats.adv_cap_1000hdx = param_arr[param_anar_1000hdx].value;
+	statsp->mac_stats.adv_cap_100T4 = param_arr[param_anar_100T4].value;
+	statsp->mac_stats.adv_cap_100fdx = param_arr[param_anar_100fdx].value;
+	statsp->mac_stats.adv_cap_100hdx = param_arr[param_anar_100hdx].value;
+	statsp->mac_stats.adv_cap_10fdx = param_arr[param_anar_10fdx].value;
+	statsp->mac_stats.adv_cap_10hdx = param_arr[param_anar_10hdx].value;
+	statsp->mac_stats.adv_cap_asmpause =
+					param_arr[param_anar_asmpause].value;
+	statsp->mac_stats.adv_cap_pause = param_arr[param_anar_pause].value;
+
+
+	/*
+	 * Check for extended status just in case we're
+	 * running a Gigibit phy.
+	 */
+	if (bmsr.bits.extend_status) {
+		if ((status = nxge_mii_read(nxgep, xcvr_portn,
+			(uint8_t)(uint64_t)(&mii_regs->esr), &esr.value))
+				!= NXGE_OK)
+			goto fail;
+		param_arr[param_anar_1000fdx].value &=
+					esr.bits.link_1000fdx;
+		param_arr[param_anar_1000hdx].value = 0;
+
+		statsp->mac_stats.cap_1000fdx =
+			(esr.bits.link_1000Xfdx ||
+				esr.bits.link_1000fdx);
+		statsp->mac_stats.cap_1000hdx = 0;
+	} else {
+		param_arr[param_anar_1000fdx].value = 0;
+		param_arr[param_anar_1000hdx].value = 0;
+	}
+
+	/*
+	 * Initialize 1G Statistics once the capability is established.
+	 */
+	statsp->mac_stats.adv_cap_1000fdx = param_arr[param_anar_1000fdx].value;
+	statsp->mac_stats.adv_cap_1000hdx = param_arr[param_anar_1000hdx].value;
+
+	/*
+	 * Initialise the link statistics.
+	 */
+	statsp->mac_stats.link_T4 = 0;
+	statsp->mac_stats.link_asmpause = 0;
+	statsp->mac_stats.link_pause = 0;
+	statsp->mac_stats.link_speed = 0;
+	statsp->mac_stats.link_duplex = 0;
+	statsp->mac_stats.link_up = 0;
+
+	/*
+	 * Switch off Auto-negotiation, 100M and full duplex.
+	 */
+	bmcr.value = 0;
+	if ((status = nxge_mii_write(nxgep, xcvr_portn,
+		(uint8_t)(uint64_t)(&mii_regs->bmcr), bmcr.value)) != NXGE_OK)
+		goto fail;
+
+	if ((statsp->lb_mode == nxge_lb_phy) ||
+			(statsp->lb_mode == nxge_lb_phy1000)) {
+		bmcr.bits.loopback = 1;
+		bmcr.bits.enable_autoneg = 0;
+		if (statsp->lb_mode == nxge_lb_phy1000)
+			bmcr.bits.speed_1000_sel = 1;
+		bmcr.bits.duplex_mode = 1;
+		param_arr[param_autoneg].value = 0;
+	} else {
+		bmcr.bits.loopback = 0;
+	}
+
+	if ((statsp->lb_mode == nxge_lb_ext1000) ||
+		(statsp->lb_mode == nxge_lb_ext100) ||
+		(statsp->lb_mode == nxge_lb_ext10)) {
+		param_arr[param_autoneg].value = 0;
+		bcm5464r_aux.value = 0;
+		bcm5464r_aux.bits.ext_lb = 1;
+		bcm5464r_aux.bits.write_1 = 1;
+		if ((status = nxge_mii_write(nxgep, xcvr_portn,
+				BCM5464R_AUX_CTL, bcm5464r_aux.value))
+				!= NXGE_OK)
+			goto fail;
+	}
+
+	if (param_arr[param_autoneg].value) {
+		NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+				"Restarting Auto-negotiation."));
+		/*
+		 * Setup our Auto-negotiation advertisement register.
+		 */
+		anar.value = 0;
+		anar.bits.selector = 1;
+		anar.bits.cap_100T4 = param_arr[param_anar_100T4].value;
+		anar.bits.cap_100fdx = param_arr[param_anar_100fdx].value;
+		anar.bits.cap_100hdx = param_arr[param_anar_100hdx].value;
+		anar.bits.cap_10fdx = param_arr[param_anar_10fdx].value;
+		anar.bits.cap_10hdx = param_arr[param_anar_10hdx].value;
+		anar.bits.cap_asmpause = 0;
+		anar.bits.cap_pause = 0;
+		if (param_arr[param_anar_1000fdx].value ||
+			param_arr[param_anar_100fdx].value ||
+			param_arr[param_anar_10fdx].value) {
+			anar.bits.cap_asmpause = statsp->mac_stats.cap_asmpause;
+			anar.bits.cap_pause = statsp->mac_stats.cap_pause;
+		}
+
+		if ((status = nxge_mii_write(nxgep, xcvr_portn,
+			(uint8_t)(uint64_t)(&mii_regs->anar), anar.value))
+				!= NXGE_OK)
+			goto fail;
+		if (bmsr.bits.extend_status) {
+			gcr.value = 0;
+			gcr.bits.ms_mode_en =
+				param_arr[param_master_cfg_enable].value;
+			gcr.bits.master =
+				param_arr[param_master_cfg_value].value;
+			gcr.bits.link_1000fdx =
+				param_arr[param_anar_1000fdx].value;
+			gcr.bits.link_1000hdx =
+				param_arr[param_anar_1000hdx].value;
+			if ((status = nxge_mii_write(nxgep, xcvr_portn,
+				(uint8_t)(uint64_t)(&mii_regs->gcr), gcr.value))
+				!= NXGE_OK)
+				goto fail;
+		}
+
+		bmcr.bits.enable_autoneg = 1;
+		bmcr.bits.restart_autoneg = 1;
+
+	} else {
+		NXGE_DEBUG_MSG((nxgep, MAC_CTL, "Going into forced mode."));
+		bmcr.bits.speed_1000_sel =
+			param_arr[param_anar_1000fdx].value |
+				param_arr[param_anar_1000hdx].value;
+		bmcr.bits.speed_sel = (~bmcr.bits.speed_1000_sel) &
+			(param_arr[param_anar_100fdx].value |
+				param_arr[param_anar_100hdx].value);
+		if (bmcr.bits.speed_1000_sel) {
+			statsp->mac_stats.link_speed = 1000;
+			gcr.value = 0;
+			gcr.bits.ms_mode_en =
+				param_arr[param_master_cfg_enable].value;
+			gcr.bits.master =
+				param_arr[param_master_cfg_value].value;
+			if ((status = nxge_mii_write(nxgep, xcvr_portn,
+				(uint8_t)(uint64_t)(&mii_regs->gcr),
+				gcr.value))
+				!= NXGE_OK)
+				goto fail;
+			if (param_arr[param_anar_1000fdx].value) {
+				bmcr.bits.duplex_mode = 1;
+				statsp->mac_stats.link_duplex = 2;
+			} else
+				statsp->mac_stats.link_duplex = 1;
+		} else if (bmcr.bits.speed_sel) {
+			statsp->mac_stats.link_speed = 100;
+			if (param_arr[param_anar_100fdx].value) {
+				bmcr.bits.duplex_mode = 1;
+				statsp->mac_stats.link_duplex = 2;
+			} else
+				statsp->mac_stats.link_duplex = 1;
+		} else {
+			statsp->mac_stats.link_speed = 10;
+			if (param_arr[param_anar_10fdx].value) {
+				bmcr.bits.duplex_mode = 1;
+				statsp->mac_stats.link_duplex = 2;
+			} else
+				statsp->mac_stats.link_duplex = 1;
+		}
+		if (statsp->mac_stats.link_duplex != 1) {
+			statsp->mac_stats.link_asmpause =
+						statsp->mac_stats.cap_asmpause;
+			statsp->mac_stats.link_pause =
+						statsp->mac_stats.cap_pause;
+		}
+
+		if ((statsp->lb_mode == nxge_lb_ext1000) ||
+			(statsp->lb_mode == nxge_lb_ext100) ||
+			(statsp->lb_mode == nxge_lb_ext10)) {
+			if (statsp->lb_mode == nxge_lb_ext1000) {
+				/* BCM5464R 1000mbps external loopback mode */
+				gcr.value = 0;
+				gcr.bits.ms_mode_en = 1;
+				gcr.bits.master = 1;
+				if ((status = nxge_mii_write(nxgep, xcvr_portn,
+					(uint8_t)(uint64_t)(&mii_regs->gcr),
+					gcr.value))
+					!= NXGE_OK)
+					goto fail;
+				bmcr.value = 0;
+				bmcr.bits.speed_1000_sel = 1;
+				statsp->mac_stats.link_speed = 1000;
+			} else if (statsp->lb_mode
+			    == nxge_lb_ext100) {
+				/* BCM5464R 100mbps external loopback mode */
+				bmcr.value = 0;
+				bmcr.bits.speed_sel = 1;
+				bmcr.bits.duplex_mode = 1;
+				statsp->mac_stats.link_speed = 100;
+			} else if (statsp->lb_mode
+			    == nxge_lb_ext10) {
+				/* BCM5464R 10mbps external loopback mode */
+				bmcr.value = 0;
+				bmcr.bits.duplex_mode = 1;
+				statsp->mac_stats.link_speed = 10;
+			}
+		}
+	}
+
+	if ((status = nxge_mii_write(nxgep, xcvr_portn,
+			(uint8_t)(uint64_t)(&mii_regs->bmcr),
+			bmcr.value)) != NXGE_OK)
+		goto fail;
+
+	if ((status = nxge_mii_read(nxgep, xcvr_portn,
+		(uint8_t)(uint64_t)(&mii_regs->bmcr), &bmcr.value)) != NXGE_OK)
+		goto fail;
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "bmcr = 0x%04X", bmcr.value));
+
+	/*
+	 * Initialize the xcvr status kept in the context structure.
+	 */
+	nxgep->soft_bmsr.value = 0;
+
+	if ((status = nxge_mii_read(nxgep, xcvr_portn,
+		(uint8_t)(uint64_t)(&mii_regs->bmsr),
+			&nxgep->bmsr.value)) != NXGE_OK)
+		goto fail;
+
+	statsp->mac_stats.xcvr_inits++;
+	nxgep->bmsr.value = 0;
+
+fail:
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+			"<== nxge_mii_xcvr_init status 0x%x", status));
+	return (status);
+}
+
+
+nxge_status_t
+nxge_mii_xcvr_fiber_init(p_nxge_t nxgep)
+{
+	p_nxge_param_t	param_arr;
+	p_nxge_stats_t	statsp;
+	uint8_t		xcvr_portn;
+	p_mii_regs_t	mii_regs;
+	mii_bmcr_t	bmcr;
+	mii_bmsr_t	bmsr;
+	mii_gcr_t	gcr;
+	mii_esr_t	esr;
+	mii_aux_ctl_t	bcm5464r_aux;
+	int		status = NXGE_OK;
+
+	uint_t delay;
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "==> nxge_mii_xcvr_fiber_init"));
+
+	param_arr = nxgep->param_arr;
+	statsp = nxgep->statsp;
+	xcvr_portn = statsp->mac_stats.xcvr_portn;
+
+	mii_regs = NULL;
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+	    "nxge_mii_xcvr_fiber_init: "
+	    "nxge_param_autoneg = 0x%02x", param_arr[param_autoneg].value));
+
+	/*
+	 * Reset the transceiver.
+	 */
+	delay = 0;
+	bmcr.value = 0;
+	bmcr.bits.reset = 1;
+	if ((status = nxge_mii_write(nxgep, xcvr_portn,
+						 (uint8_t)(uint64_t)&mii_regs->bmcr,
+						 bmcr.value)) != NXGE_OK) {
+		goto fail;
+	}
+	do {
+		NXGE_DELAY(500);
+		if ((status = nxge_mii_read(nxgep, xcvr_portn,
+			(uint8_t)(uint64_t)&mii_regs->bmcr, &bmcr.value))
+			!= NXGE_OK) {
+			goto fail;
+		}
+		delay++;
+	} while ((bmcr.bits.reset) && (delay < 1000));
+	if (delay == 1000) {
+		NXGE_DEBUG_MSG((nxgep, MAC_CTL, "Xcvr reset failed."));
+		goto fail;
+	}
+
+	if ((status = nxge_mii_read(nxgep, xcvr_portn,
+			(uint8_t)(uint64_t)(&mii_regs->bmsr),
+								&bmsr.value)) != NXGE_OK) {
+		goto fail;
+	}
+
+	param_arr[param_autoneg].value &= bmsr.bits.auto_neg_able;
+	param_arr[param_anar_100T4].value = 0;
+	param_arr[param_anar_100fdx].value = 0;
+	param_arr[param_anar_100hdx].value = 0;
+	param_arr[param_anar_10fdx].value = 0;
+	param_arr[param_anar_10hdx].value = 0;
+
+	/*
+	 * Initialize the xcvr statistics.
+	 */
+	statsp->mac_stats.cap_autoneg = bmsr.bits.auto_neg_able;
+	statsp->mac_stats.cap_100T4 = 0;
+	statsp->mac_stats.cap_100fdx = 0;
+	statsp->mac_stats.cap_100hdx = 0;
+	statsp->mac_stats.cap_10fdx = 0;
+	statsp->mac_stats.cap_10hdx = 0;
+	statsp->mac_stats.cap_asmpause = param_arr[param_anar_asmpause].value;
+	statsp->mac_stats.cap_pause = param_arr[param_anar_pause].value;
+
+	/*
+	 * Initialize the xcvr advertised capability statistics.
+	 */
+	statsp->mac_stats.adv_cap_autoneg = param_arr[param_autoneg].value;
+	statsp->mac_stats.adv_cap_1000fdx = param_arr[param_anar_1000fdx].value;
+	statsp->mac_stats.adv_cap_1000hdx = param_arr[param_anar_1000hdx].value;
+	statsp->mac_stats.adv_cap_100T4 = param_arr[param_anar_100T4].value;
+	statsp->mac_stats.adv_cap_100fdx = param_arr[param_anar_100fdx].value;
+	statsp->mac_stats.adv_cap_100hdx = param_arr[param_anar_100hdx].value;
+	statsp->mac_stats.adv_cap_10fdx = param_arr[param_anar_10fdx].value;
+	statsp->mac_stats.adv_cap_10hdx = param_arr[param_anar_10hdx].value;
+	statsp->mac_stats.adv_cap_asmpause =
+					param_arr[param_anar_asmpause].value;
+	statsp->mac_stats.adv_cap_pause = param_arr[param_anar_pause].value;
+
+	/*
+	 * Check for extended status just in case we're
+	 * running a Gigibit phy.
+	 */
+	if (bmsr.bits.extend_status) {
+		if ((status = nxge_mii_read(nxgep, xcvr_portn,
+			(uint8_t)(uint64_t)(&mii_regs->esr), &esr.value))
+			!= NXGE_OK) {
+			goto fail;
+		}
+		param_arr[param_anar_1000fdx].value &=
+					esr.bits.link_1000fdx;
+		param_arr[param_anar_1000hdx].value = 0;
+
+		statsp->mac_stats.cap_1000fdx =
+			(esr.bits.link_1000Xfdx ||
+				esr.bits.link_1000fdx);
+		statsp->mac_stats.cap_1000hdx = 0;
+	} else {
+		param_arr[param_anar_1000fdx].value = 0;
+		param_arr[param_anar_1000hdx].value = 0;
+	}
+
+	/*
+	 * Initialize 1G Statistics once the capability is established.
+	 */
+	statsp->mac_stats.adv_cap_1000fdx = param_arr[param_anar_1000fdx].value;
+	statsp->mac_stats.adv_cap_1000hdx = param_arr[param_anar_1000hdx].value;
+
+	/*
+	 * Initialize the link statistics.
+	 */
+	statsp->mac_stats.link_T4 = 0;
+	statsp->mac_stats.link_asmpause = 0;
+	statsp->mac_stats.link_pause = 0;
+	statsp->mac_stats.link_speed = 0;
+	statsp->mac_stats.link_duplex = 0;
+	statsp->mac_stats.link_up = 0;
+
+	/*
+	 * Switch off Auto-negotiation, 100M and full duplex.
+	 */
+	bmcr.value = 0;
+	if ((status = nxge_mii_write(nxgep, xcvr_portn,
+							 (uint8_t)(uint64_t)(&mii_regs->bmcr),
+							 bmcr.value)) != NXGE_OK) {
+		goto fail;
+	}
+
+	if ((statsp->lb_mode == nxge_lb_phy) ||
+			(statsp->lb_mode == nxge_lb_phy1000)) {
+		bmcr.bits.loopback = 1;
+		bmcr.bits.enable_autoneg = 0;
+		if (statsp->lb_mode == nxge_lb_phy1000)
+			bmcr.bits.speed_1000_sel = 1;
+		bmcr.bits.duplex_mode = 1;
+		param_arr[param_autoneg].value = 0;
+	} else {
+		bmcr.bits.loopback = 0;
+	}
+
+	if (statsp->lb_mode == nxge_lb_ext1000) {
+		param_arr[param_autoneg].value = 0;
+		bcm5464r_aux.value = 0;
+		bcm5464r_aux.bits.ext_lb = 1;
+		bcm5464r_aux.bits.write_1 = 1;
+		if ((status = nxge_mii_write(nxgep, xcvr_portn,
+				BCM5464R_AUX_CTL, bcm5464r_aux.value))
+			!= NXGE_OK) {
+			goto fail;
+		}
+	}
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "Going into forced mode."));
+	bmcr.bits.speed_1000_sel = 1;
+	bmcr.bits.speed_sel = 0;
+	bmcr.bits.duplex_mode = 1;
+	statsp->mac_stats.link_speed = 1000;
+	statsp->mac_stats.link_duplex = 2;
+
+	if ((statsp->lb_mode == nxge_lb_ext1000)) {
+		/* BCM5464R 1000mbps external loopback mode */
+		gcr.value = 0;
+		gcr.bits.ms_mode_en = 1;
+		gcr.bits.master = 1;
+		if ((status = nxge_mii_write(nxgep, xcvr_portn,
+			(uint8_t)(uint64_t)(&mii_regs->gcr),
+			gcr.value))
+			!= NXGE_OK) {
+			goto fail;
+
+		}
+		bmcr.value = 0;
+		bmcr.bits.speed_1000_sel = 1;
+		statsp->mac_stats.link_speed = 1000;
+	}
+
+	if ((status = nxge_mii_write(nxgep, xcvr_portn,
+			(uint8_t)(uint64_t)(&mii_regs->bmcr),
+								 bmcr.value)) != NXGE_OK) {
+		goto fail;
+	}
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+	    "nxge_mii_xcvr_fiber_init: value wrote bmcr = 0x%x",
+	    bmcr.value));
+
+	if ((status = nxge_mii_read(nxgep, xcvr_portn,
+								(uint8_t)(uint64_t)(&mii_regs->bmcr), &bmcr.value)) != NXGE_OK) {
+		goto fail;
+	}
+
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+	    "nxge_mii_xcvr_fiber_init: read bmcr = 0x%04X", bmcr.value));
+
+	/*
+	 * Initialize the xcvr status kept in the context structure.
+	 */
+	nxgep->soft_bmsr.value = 0;
+	if ((status = nxge_mii_read(nxgep, xcvr_portn,
+		(uint8_t)(uint64_t)(&mii_regs->bmsr),
+								&nxgep->bmsr.value)) != NXGE_OK) {
+		goto fail;
+	}
+
+	statsp->mac_stats.xcvr_inits++;
+	nxgep->bmsr.value = 0;
+	return (status);
+
+fail:
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+	    "<== nxge_mii_xcvr_fiber_init Failed: status 0x%x", status));
+	return (status);
+}
+
+
+/* Read from a MII compliant register */
+
+nxge_status_t
+nxge_mii_read(p_nxge_t nxgep, uint8_t xcvr_portn, uint8_t xcvr_reg,
+		uint16_t *value)
+{
+	npi_status_t rs = NPI_SUCCESS;
+
+	NXGE_DEBUG_MSG((nxgep, MIF_CTL, "==> nxge_mii_read: xcvr_port<%d>"
+			"xcvr_reg<%d>", xcvr_portn, xcvr_reg));
+
+	MUTEX_ENTER(nxgep->link_lock);
+
+	if ((nxgep->mac.portmode == PORT_1G_COPPER) ||
+		(nxgep->mac.portmode == PORT_1G_RGMII_FIBER)) {
+		if ((rs = npi_mac_mif_mii_read(nxgep->npi_handle,
+				xcvr_portn, xcvr_reg, value)) != NPI_SUCCESS)
+			goto fail;
+	} else if ((nxgep->mac.portmode == PORT_1G_FIBER) ||
+		(nxgep->mac.portmode == PORT_1G_SERDES)) {
+		if ((rs = npi_mac_pcs_mii_read(nxgep->npi_handle,
+				xcvr_portn, xcvr_reg, value)) != NPI_SUCCESS)
+			goto fail;
+	} else
+		goto fail;
+
+	MUTEX_EXIT(nxgep->link_lock);
+
+	NXGE_DEBUG_MSG((nxgep, MIF_CTL, "<== nxge_mii_read: xcvr_port<%d>"
+			"xcvr_reg<%d> value=0x%x",
+			xcvr_portn, xcvr_reg, *value));
+	return (NXGE_OK);
+fail:
+	MUTEX_EXIT(nxgep->link_lock);
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_mii_read: Failed to read mii on xcvr %d",
+			xcvr_portn));
+
+	return (NXGE_ERROR | rs);
+}
+
+/* Write to a MII cimpliant Register */
+
+nxge_status_t
+nxge_mii_write(p_nxge_t nxgep, uint8_t xcvr_portn, uint8_t xcvr_reg,
+		uint16_t value)
+{
+	npi_status_t rs = NPI_SUCCESS;
+
+	NXGE_DEBUG_MSG((nxgep, MIF_CTL, "==> nxge_mii_write: xcvr_port<%d>"
+			"xcvr_reg<%d> value=0x%x", xcvr_portn, xcvr_reg,
+			value));
+
+	MUTEX_ENTER(nxgep->link_lock);
+
+	if ((nxgep->mac.portmode == PORT_1G_COPPER) ||
+		(nxgep->mac.portmode == PORT_1G_RGMII_FIBER)) {
+		if ((rs = npi_mac_mif_mii_write(nxgep->npi_handle,
+				xcvr_portn, xcvr_reg, value)) != NPI_SUCCESS)
+			goto fail;
+	} else if ((nxgep->mac.portmode == PORT_1G_FIBER) ||
+		(nxgep->mac.portmode == PORT_1G_SERDES)){
+		if ((rs = npi_mac_pcs_mii_write(nxgep->npi_handle,
+				xcvr_portn, xcvr_reg, value)) != NPI_SUCCESS)
+			goto fail;
+	} else
+		goto fail;
+
+	MUTEX_EXIT(nxgep->link_lock);
+
+	NXGE_DEBUG_MSG((nxgep, MIF_CTL, "<== nxge_mii_write: xcvr_port<%d>"
+			"xcvr_reg<%d>", xcvr_portn, xcvr_reg));
+	return (NXGE_OK);
+fail:
+	MUTEX_EXIT(nxgep->link_lock);
+
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_mii_write: Failed to write mii on xcvr %d",
+			xcvr_portn));
+
+	return (NXGE_ERROR | rs);
+}
+
+/* Perform read from Clause45 serdes / transceiver device */
+
+nxge_status_t
+nxge_mdio_read(p_nxge_t nxgep, uint8_t xcvr_portn, uint8_t device,
+		uint16_t xcvr_reg, uint16_t *value)
+{
+	npi_status_t rs = NPI_SUCCESS;
+
+	NXGE_DEBUG_MSG((nxgep, MIF_CTL, "==> nxge_mdio_read: xcvr_port<%d>",
+			xcvr_portn));
+
+	MUTEX_ENTER(nxgep->link_lock);
+
+	if ((rs = npi_mac_mif_mdio_read(nxgep->npi_handle,
+			xcvr_portn, device, xcvr_reg, value)) != NPI_SUCCESS)
+		goto fail;
+
+	MUTEX_EXIT(nxgep->link_lock);
+
+	NXGE_DEBUG_MSG((nxgep, MIF_CTL, "<== nxge_mdio_read: xcvr_port<%d>",
+			xcvr_portn));
+	return (NXGE_OK);
+fail:
+	MUTEX_EXIT(nxgep->link_lock);
+
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_mdio_read: Failed to read mdio on xcvr %d",
+			xcvr_portn));
+
+	return (NXGE_ERROR | rs);
+}
+
+/* Perform write to Clause45 serdes / transceiver device */
+
+nxge_status_t
+nxge_mdio_write(p_nxge_t nxgep, uint8_t xcvr_portn, uint8_t device,
+		uint16_t xcvr_reg, uint16_t value)
+{
+	npi_status_t rs = NPI_SUCCESS;
+
+	NXGE_DEBUG_MSG((nxgep, MIF_CTL, "==> nxge_mdio_write: xcvr_port<%d>",
+			xcvr_portn));
+
+	MUTEX_ENTER(nxgep->link_lock);
+
+	if ((rs = npi_mac_mif_mdio_write(nxgep->npi_handle,
+			xcvr_portn, device, xcvr_reg, value)) != NPI_SUCCESS)
+		goto fail;
+
+	MUTEX_EXIT(nxgep->link_lock);
+
+	NXGE_DEBUG_MSG((nxgep, MIF_CTL, "<== nxge_mdio_write: xcvr_port<%d>",
+			xcvr_portn));
+	return (NXGE_OK);
+fail:
+	MUTEX_EXIT(nxgep->link_lock);
+
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_mdio_write: Failed to write mdio on xcvr %d",
+			xcvr_portn));
+
+	return (NXGE_ERROR | rs);
+}
+
+
+/* Check MII to see if there is any link status change */
+
+nxge_status_t
+nxge_mii_check(p_nxge_t nxgep, mii_bmsr_t bmsr, mii_bmsr_t bmsr_ints)
+{
+	p_nxge_param_t	param_arr;
+	p_nxge_stats_t	statsp;
+	p_mii_regs_t	mii_regs;
+	p_mii_bmsr_t	soft_bmsr;
+	mii_anar_t	anar;
+	mii_anlpar_t	anlpar;
+	mii_anar_t	an_common;
+	mii_aner_t	aner;
+	mii_gsr_t	gsr;
+	nxge_status_t	status = NXGE_OK;
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "==> nxge_mii_check"));
+
+	mii_regs = NULL;
+	param_arr = nxgep->param_arr;
+	statsp = nxgep->statsp;
+	soft_bmsr = &nxgep->soft_bmsr;
+
+	if (bmsr_ints.bits.link_status) {
+		if (bmsr.bits.link_status) {
+			soft_bmsr->bits.link_status = 1;
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+					"Link UP"));
+		} else {
+			statsp->mac_stats.link_up = 0;
+			soft_bmsr->bits.link_status = 0;
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+					"Link DOWN...cable problem"));
+			nxge_link_is_down(nxgep);
+		}
+	}
+
+	if (nxgep->mac.portmode == PORT_1G_COPPER &&
+	    param_arr[param_autoneg].value) {
+		if (bmsr_ints.bits.auto_neg_complete) {
+			if (bmsr.bits.auto_neg_complete)
+				soft_bmsr->bits.auto_neg_complete = 1;
+			else
+				soft_bmsr->bits.auto_neg_complete = 0;
+		}
+		if (soft_bmsr->bits.link_status == 0) {
+			statsp->mac_stats.link_T4 = 0;
+			statsp->mac_stats.link_speed = 0;
+			statsp->mac_stats.link_duplex = 0;
+			statsp->mac_stats.link_asmpause = 0;
+			statsp->mac_stats.link_pause = 0;
+			statsp->mac_stats.lp_cap_autoneg = 0;
+			statsp->mac_stats.lp_cap_100T4 = 0;
+			statsp->mac_stats.lp_cap_1000fdx = 0;
+			statsp->mac_stats.lp_cap_1000hdx = 0;
+			statsp->mac_stats.lp_cap_100fdx = 0;
+			statsp->mac_stats.lp_cap_100hdx = 0;
+			statsp->mac_stats.lp_cap_10fdx = 0;
+			statsp->mac_stats.lp_cap_10hdx = 0;
+			statsp->mac_stats.lp_cap_10gfdx = 0;
+			statsp->mac_stats.lp_cap_10ghdx = 0;
+			statsp->mac_stats.lp_cap_asmpause = 0;
+			statsp->mac_stats.lp_cap_pause = 0;
+		}
+	} else
+		soft_bmsr->bits.auto_neg_complete = 1;
+
+	if ((bmsr_ints.bits.link_status ||
+		bmsr_ints.bits.auto_neg_complete) &&
+		soft_bmsr->bits.link_status &&
+		soft_bmsr->bits.auto_neg_complete) {
+		statsp->mac_stats.link_up = 1;
+		if (nxgep->mac.portmode == PORT_1G_COPPER &&
+		    param_arr[param_autoneg].value) {
+			if ((status = nxge_mii_read(nxgep,
+				statsp->mac_stats.xcvr_portn,
+				(uint8_t)(uint64_t)(&mii_regs->anar),
+					&anar.value)) != NXGE_OK)
+				goto fail;
+			if ((status = nxge_mii_read(nxgep,
+				statsp->mac_stats.xcvr_portn,
+				(uint8_t)(uint64_t)(&mii_regs->anlpar),
+					&anlpar.value)) != NXGE_OK)
+				goto fail;
+			if ((status = nxge_mii_read(nxgep,
+				statsp->mac_stats.xcvr_portn,
+				(uint8_t)(uint64_t)(&mii_regs->aner),
+					&aner.value)) != NXGE_OK)
+				goto fail;
+			statsp->mac_stats.lp_cap_autoneg = aner.bits.lp_an_able;
+			statsp->mac_stats.lp_cap_100T4 = anlpar.bits.cap_100T4;
+			statsp->mac_stats.lp_cap_100fdx =
+							anlpar.bits.cap_100fdx;
+			statsp->mac_stats.lp_cap_100hdx =
+							anlpar.bits.cap_100hdx;
+			statsp->mac_stats.lp_cap_10fdx = anlpar.bits.cap_10fdx;
+			statsp->mac_stats.lp_cap_10hdx = anlpar.bits.cap_10hdx;
+			statsp->mac_stats.lp_cap_asmpause =
+						anlpar.bits.cap_asmpause;
+			statsp->mac_stats.lp_cap_pause = anlpar.bits.cap_pause;
+			an_common.value = anar.value & anlpar.value;
+			if (param_arr[param_anar_1000fdx].value ||
+				param_arr[param_anar_1000hdx].value) {
+				if ((status = nxge_mii_read(nxgep,
+					statsp->mac_stats.xcvr_portn,
+					(uint8_t)(uint64_t)(&mii_regs->gsr),
+						&gsr.value))
+						!= NXGE_OK)
+					goto fail;
+				statsp->mac_stats.lp_cap_1000fdx =
+					gsr.bits.link_1000fdx;
+				statsp->mac_stats.lp_cap_1000hdx =
+					gsr.bits.link_1000hdx;
+				if (param_arr[param_anar_1000fdx].value &&
+					gsr.bits.link_1000fdx) {
+					statsp->mac_stats.link_speed = 1000;
+					statsp->mac_stats.link_duplex = 2;
+				} else if (
+					param_arr[param_anar_1000hdx].value &&
+						gsr.bits.link_1000hdx) {
+					statsp->mac_stats.link_speed = 1000;
+					statsp->mac_stats.link_duplex = 1;
+				}
+			}
+			if ((an_common.value != 0) &&
+					!(statsp->mac_stats.link_speed)) {
+				if (an_common.bits.cap_100T4) {
+					statsp->mac_stats.link_T4 = 1;
+					statsp->mac_stats.link_speed = 100;
+					statsp->mac_stats.link_duplex = 1;
+				} else if (an_common.bits.cap_100fdx) {
+					statsp->mac_stats.link_speed = 100;
+					statsp->mac_stats.link_duplex = 2;
+				} else if (an_common.bits.cap_100hdx) {
+					statsp->mac_stats.link_speed = 100;
+					statsp->mac_stats.link_duplex = 1;
+				} else if (an_common.bits.cap_10fdx) {
+					statsp->mac_stats.link_speed = 10;
+					statsp->mac_stats.link_duplex = 2;
+				} else if (an_common.bits.cap_10hdx) {
+					statsp->mac_stats.link_speed = 10;
+					statsp->mac_stats.link_duplex = 1;
+				} else {
+					goto fail;
+				}
+			}
+			if (statsp->mac_stats.link_duplex != 1) {
+				statsp->mac_stats.link_asmpause =
+					an_common.bits.cap_asmpause;
+				if (statsp->mac_stats.link_asmpause)
+				if ((statsp->mac_stats.cap_pause == 0) &&
+						(statsp->mac_stats.lp_cap_pause
+						== 1))
+						statsp->mac_stats.link_pause
+						= 0;
+					else
+						statsp->mac_stats.link_pause
+						= 1;
+				else
+					statsp->mac_stats.link_pause =
+						an_common.bits.cap_pause;
+			}
+		} else if (nxgep->mac.portmode == PORT_1G_RGMII_FIBER) {
+			statsp->mac_stats.link_speed = 1000;
+			statsp->mac_stats.link_duplex = 2;
+		}
+		nxge_link_is_up(nxgep);
+	}
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "<== nxge_mii_check"));
+	return (NXGE_OK);
+fail:
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_mii_check: Unable to check MII"));
+	return (status);
+}
+
+nxge_status_t
+nxge_pcs_check(p_nxge_t nxgep, uint8_t portn, nxge_link_state_t *link_up)
+{
+	p_nxge_param_t	param_arr;
+	p_nxge_stats_t	statsp;
+	boolean_t	linkup;
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "==> nxge_pcs_check"));
+
+	param_arr = nxgep->param_arr;
+	statsp = nxgep->statsp;
+	*link_up = LINK_NO_CHANGE;
+
+	(void)npi_mac_get_link_status(nxgep->npi_handle, portn, &linkup);
+	if (linkup) {
+		if (nxgep->statsp->mac_stats.link_up == 0) {
+			statsp->mac_stats.link_up = 1;
+			statsp->mac_stats.link_speed = 1000;
+			statsp->mac_stats.link_duplex = 2;
+			*link_up = LINK_IS_UP;
+			nxge_link_is_up(nxgep);
+		}
+	} else {
+		if (nxgep->statsp->mac_stats.link_up == 1) {
+			statsp->mac_stats.link_up = 0;
+			statsp->mac_stats.link_speed = 0;
+			statsp->mac_stats.link_duplex = 0;
+			*link_up = LINK_IS_DOWN;
+			nxge_link_is_down(nxgep);
+		}
+	}
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "<== nxge_pcs_check"));
+	return (NXGE_OK);
+}
+
+/* Check status of MII (MIF or PCS) link */
+nxge_status_t
+nxge_check_mii_link(p_nxge_t nxgep)
+{
+	mii_bmsr_t bmsr_ints, bmsr_data;
+	mii_anlpar_t anlpar;
+	mii_gsr_t gsr;
+	p_mii_regs_t mii_regs;
+	nxge_status_t status = NXGE_OK;
+	uint8_t portn;
+	nxge_link_state_t link_up;
+	p_nxge_stats_t	statsp;
+	uint8_t		xcvr_portn;
+
+	portn = nxgep->mac.portnum;
+	statsp = nxgep->statsp;
+	xcvr_portn = statsp->mac_stats.xcvr_portn;
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "==> nxge_check_mii_link port<%d>",
+				portn));
+
+	mii_regs = NULL;
+
+
+
+	switch (nxgep->mac.portmode) {
+
+	case PORT_1G_SERDES:
+		NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+			"==> nxge_check_mii_link port<%d> (SERDES)",
+			portn));
+		if ((status = nxge_pcs_check(nxgep, portn, &link_up))
+				!= NXGE_OK) {
+			goto fail;
+		}
+		break;
+
+	default:
+		bmsr_data.value = 0;
+
+		if ((status = nxge_mii_read(nxgep,
+				nxgep->statsp->mac_stats.xcvr_portn,
+				(uint8_t)(uint64_t)(&mii_regs->bmsr),
+				&bmsr_data.value)) != NXGE_OK) {
+			goto fail;
+		}
+
+		NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+			"==> nxge_check_mii_link port<0x%x> "
+			"RIGHT AFTER READ bmsr_data 0x%x (nxgep->bmsr 0x%x ",
+			xcvr_portn, bmsr_data.value, nxgep->bmsr.value));
+
+
+
+		if (nxgep->param_arr[param_autoneg].value) {
+			if ((status = nxge_mii_read(nxgep,
+				nxgep->statsp->mac_stats.xcvr_portn,
+				(uint8_t)(uint64_t)(&mii_regs->gsr),
+				&gsr.value)) != NXGE_OK)
+				goto fail;
+			if ((status = nxge_mii_read(nxgep,
+				nxgep->statsp->mac_stats.xcvr_portn,
+				(uint8_t)(uint64_t)(&mii_regs->anlpar),
+				&anlpar.value)) != NXGE_OK)
+				goto fail;
+		}
+
+		/* Workaround for link down issue */
+		if (bmsr_data.value == 0) {
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+							"!LINK DEBUG: Read zero bmsr"));
+			goto nxge_check_mii_link_exit;
+		}
+
+
+		NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+			"==> nxge_check_mii_link port<0x%x> :"
+			"BEFORE BMSR ^ nxgep->bmsr 0x%x bmsr_data 0x%x",
+			xcvr_portn, nxgep->bmsr.value,
+			bmsr_data.value));
+
+		bmsr_ints.value = nxgep->bmsr.value ^ bmsr_data.value;
+		nxgep->bmsr.value = bmsr_data.value;
+
+		NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+			"==> nxge_check_mii_link port<0x%x> CALLING "
+			"bmsr_data 0x%x bmsr_ints.value 0x%x",
+			xcvr_portn, bmsr_data.value, bmsr_ints.value));
+
+		if ((status = nxge_mii_check(nxgep, bmsr_data, bmsr_ints)) != NXGE_OK)
+			goto fail; {
+		}
+		break;
+
+	}
+
+nxge_check_mii_link_exit:
+
+	(void) nxge_link_monitor(nxgep, LINK_MONITOR_START);
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "<== nxge_check_mii_link port<%d>",
+				portn));
+	return (NXGE_OK);
+
+fail:
+
+	(void) nxge_link_monitor(nxgep, LINK_MONITOR_START);
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_check_mii_link: Failed to check link port<%d>",
+			portn));
+	return (status);
+}
+
+
+
+
+/*ARGSUSED*/
+nxge_status_t
+nxge_check_10g_link(p_nxge_t nxgep)
+{
+	uint8_t		portn;
+
+	nxge_status_t	status = NXGE_OK;
+	boolean_t	link_up;
+	npi_status_t	rs;
+	uint32_t	val, val2;
+	boolean_t	xpcs_up, xmac_up;
+	portn = nxgep->mac.portnum;
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "==> nxge_check_10g_link port<%d>",
+				portn));
+
+	link_up = B_FALSE;
+
+	switch (nxgep->mac.portmode) {
+
+	case PORT_10G_SERDES:
+
+		rs = npi_xmac_xpcs_read(nxgep->npi_handle, nxgep->mac.portnum,
+				XPCS_REG_STATUS, &val);
+
+		if (rs != 0)
+			goto fail;
+
+
+		xmac_up = B_FALSE;
+		xpcs_up = B_FALSE;
+
+		if (val & XPCS_STATUS_RX_LINK_STATUS_UP) {
+			xpcs_up = B_TRUE;
+		}
+
+		/*
+		 * Read the xMAC internal signal 2 register.
+		 * This register should be the superset of the XPCS when wanting
+		 * to get the link status. If this register read is proved to be
+		 * reliable, there is no need to read the XPCS register.
+		 */
+		xmac_up = B_TRUE;
+		XMAC_REG_RD(nxgep->npi_handle, portn, XMAC_INTERN2_REG, &val2);
+		if (val2 & XMAC_IS2_LOCAL_FLT_OC_SYNC) { /* link is down */
+			xmac_up = B_FALSE;
+		}
+
+		if (xpcs_up && xmac_up) {
+			link_up = B_TRUE;
+		}
+
+		break;
+	default:
+		status = nxge_check_bcm8704_link(nxgep, &link_up);
+
+		if (status != NXGE_OK) {
+			goto fail;
+		}
+
+		break;
+
+	}
+
+	if (link_up) {
+		if (nxgep->statsp->mac_stats.link_up == 0) {
+			if (nxge_10g_link_led_on(nxgep) != NXGE_OK)
+				goto fail;
+			nxgep->statsp->mac_stats.link_up = 1;
+			nxgep->statsp->mac_stats.link_speed = 10000;
+			nxgep->statsp->mac_stats.link_duplex = 2;
+
+			nxge_link_is_up(nxgep);
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"Link UP"));
+		}
+	} else {
+		if (nxgep->statsp->mac_stats.link_up == 1) {
+			if (nxge_10g_link_led_off(nxgep) != NXGE_OK)
+				goto fail;
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+					"Link DOWN...cable problem"));
+			nxgep->statsp->mac_stats.link_up = 0;
+			nxgep->statsp->mac_stats.link_speed = 0;
+			nxgep->statsp->mac_stats.link_duplex = 0;
+
+			nxge_link_is_down(nxgep);
+
+		}
+	}
+
+	(void) nxge_link_monitor(nxgep, LINK_MONITOR_START);
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "<== nxge_check_10g_link port<%d>",
+				portn));
+	return (NXGE_OK);
+
+fail:
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_check_10g_link: Failed to check link port<%d>",
+			portn));
+	return (status);
+}
+
+
+/* Declare link down */
+
+void
+nxge_link_is_down(p_nxge_t nxgep)
+{
+	p_nxge_stats_t statsp;
+	char link_stat_msg[64];
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "==> nxge_link_is_down"));
+
+	statsp = nxgep->statsp;
+	(void) sprintf(link_stat_msg, "xcvr addr:0x%02x - link down",
+			statsp->mac_stats.xcvr_portn);
+
+
+	/* TODO - do we call netif_carrier_off here ? */
+	netif_carrier_off(nxgep->dev);
+	netif_stop_queue(nxgep->dev);
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "<== nxge_link_is_down"));
+}
+
+/* Declare link up */
+
+void
+nxge_link_is_up(p_nxge_t nxgep)
+{
+	p_nxge_stats_t statsp;
+	char link_stat_msg[64];
+	uint32_t val;
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "==> nxge_link_is_up"));
+
+	statsp = nxgep->statsp;
+	(void) sprintf(link_stat_msg, "xcvr addr:0x%02x - link up %d Mbps ",
+				statsp->mac_stats.xcvr_portn,
+				statsp->mac_stats.link_speed);
+
+	if (statsp->mac_stats.link_T4)
+		(void) strcat(link_stat_msg, "T4");
+	else if (statsp->mac_stats.link_duplex == 2)
+		(void) strcat(link_stat_msg, "full duplex");
+	else
+		(void) strcat(link_stat_msg, "half duplex");
+	(void) nxge_xif_init(nxgep);
+	/* Clean up symbol errors incurred during link transition */
+	if ((nxgep->mac.portmode == PORT_10G_FIBER) ||
+		(nxgep->mac.portmode == PORT_10G_SERDES)) {
+		(void) npi_xmac_xpcs_read(nxgep->npi_handle, nxgep->mac.portnum,
+					XPCS_REG_SYMBOL_ERR_L0_1_COUNTER, &val);
+		(void) npi_xmac_xpcs_read(nxgep->npi_handle, nxgep->mac.portnum,
+					XPCS_REG_SYMBOL_ERR_L2_3_COUNTER, &val);
+	}
+	/* TODO - do we call netif_carrier_on here ? */
+	netif_carrier_on(nxgep->dev);
+	netif_start_queue(nxgep->dev);
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "<== nxge_link_is_up"));
+}
+
+/*
+ * Calculate the bit in the multicast address filter
+ * that selects the given * address.
+ * Note: For GEM, the last 8-bits are used.
+ */
+uint32_t
+crc32_mchash(p_ether_addr_t addr)
+{
+	uint8_t *cp;
+	uint32_t crc;
+	uint32_t c;
+	int byte;
+	int bit;
+
+	cp = (uint8_t *)addr;
+	crc = (uint32_t)0xffffffff;
+	for (byte = 0; byte < 6; byte++) {
+		c = (uint32_t)cp[byte];
+		for (bit = 0; bit < 8; bit++) {
+			if ((c & 0x1) ^ (crc & 0x1))
+				crc = (crc >> 1)^0xedb88320;
+			else
+				crc = (crc >> 1);
+			c >>= 1;
+		}
+	}
+	return ((~crc) >> (32 - HASH_BITS));
+}
+
+/* Reset serdes */
+
+nxge_status_t
+nxge_serdes_reset(p_nxge_t nxgep)
+{
+	npi_handle_t		handle;
+
+	handle = nxgep->npi_handle;
+
+	ESR_REG_WR(handle, ESR_RESET_REG, ESR_RESET_0 | ESR_RESET_1);
+	NXGE_DELAY(500);
+	ESR_REG_WR(handle, ESR_CONFIG_REG, 0);
+
+	return (NXGE_OK);
+}
+
+/* Monitor link status using interupt or polling */
+
+nxge_status_t
+nxge_link_monitor(p_nxge_t nxgep, link_mon_enable_t enable)
+{
+	nxge_status_t status = NXGE_OK;
+
+	/*
+	 * Make sure that we don't check the link if this happen to
+	 * be not port0 or 1 and it is not BMAC port.
+	 */
+	if (nxgep->mac.portnum > 1) {
+		if ((nxgep->mac.portmode == PORT_10G_FIBER) ||
+			(nxgep->mac.portmode == PORT_10G_SERDES))
+			return (NXGE_OK);
+	}
+
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+			    "nxge_link_monitor: enable[%d] "
+			    "link_chk_intr [%d]", enable,
+			    (nxgep->mac.linkchkmode == LINKCHK_INTR) ? 1 : 0));
+
+	if (enable == LINK_MONITOR_START) {
+		NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+				"nxge_link_monitor: ENABLED"));
+		if (nxgep->mac.linkchkmode == LINKCHK_INTR) {
+			NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+				"nxge_link_monitor: link chk intr start"));
+			if ((status = nxge_link_intr(nxgep, LINK_INTR_START))
+							!= NXGE_OK)
+				goto fail;
+		} else {
+			NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+			    "nxge_link_monitor: link_timer_stopped [%d]",
+			    nxgep->link_timer_stopped));
+
+			if (nxgep->link_timer_stopped == B_TRUE)
+				nxgep->link_timer_stopped = B_FALSE;
+			mod_timer(&nxgep->wd_timer,
+				    jiffies + NXGE_LINK_TIMEOUT);
+
+		}
+	} else if (enable == LINK_MONITOR_STOP) {
+		NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+				"nxge_link_monitor: DISABLED"));
+		if (nxgep->mac.linkchkmode == LINKCHK_INTR) {
+			NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+				"nxge_link_monitor: link chk intr stop"));
+			if ((status = nxge_link_intr(nxgep, LINK_INTR_STOP))
+							!= NXGE_OK)
+				goto fail;
+		} else {
+
+			nxgep->link_timer_stopped = B_TRUE;
+			NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+					"nxge_link_monitor: stopping"
+					" watchdog timer"));
+		}
+	}
+	return (NXGE_OK);
+fail:
+	return (status);
+}
+
+nxge_status_t
+nxge_check_bcm8704_link(p_nxge_t nxgep, boolean_t *link_up)
+{
+	uint8_t		phy_port_addr;
+	nxge_status_t	status = NXGE_OK;
+	boolean_t	rx_sig_ok;
+	boolean_t	pcs_blk_lock;
+	boolean_t	link_align;
+	uint16_t	val1, val2, val3;
+#ifdef	NXGE_DEBUG_SYMBOL_ERR
+	uint16_t	val_debug;
+	uint16_t	val;
+#endif
+
+	phy_port_addr = nxgep->statsp->mac_stats.xcvr_portn;
+
+#ifdef	NXGE_DEBUG_SYMBOL_ERR
+	/* Check Device 3 Register Device 3 0xC809 */
+	(void) nxge_mdio_read(nxgep, phy_port_addr, 0x3, 0xC809, &val_debug);
+	if ((val_debug & ~0x200) != 0) {
+		cmn_err(CE_NOTE, "Port%d BCM8704 Dev3 Reg 0xc809 = 0x%x\n",
+				nxgep->mac.portnum, val_debug);
+		(void) nxge_mdio_read(nxgep, phy_port_addr, 0x4, 0x18,
+				&val_debug);
+		cmn_err(CE_NOTE, "Port%d BCM8704 Dev4 Reg 0x18 = 0x%x\n",
+				nxgep->mac.portnum, val_debug);
+	}
+
+	(void) npi_xmac_xpcs_read(nxgep->npi_handle, nxgep->mac.portnum,
+					XPCS_REG_DESCWERR_COUNTER, &val);
+	if (val != 0)
+		cmn_err(CE_NOTE, "XPCS DESCWERR = 0x%x\n", val);
+
+	(void) npi_xmac_xpcs_read(nxgep->npi_handle, nxgep->mac.portnum,
+					XPCS_REG_SYMBOL_ERR_L0_1_COUNTER, &val);
+	if (val != 0)
+		cmn_err(CE_NOTE, "XPCS SYMBOL_ERR_L0_1 = 0x%x\n", val);
+
+	(void) npi_xmac_xpcs_read(nxgep->npi_handle, nxgep->mac.portnum,
+					XPCS_REG_SYMBOL_ERR_L2_3_COUNTER, &val);
+	if (val != 0)
+		cmn_err(CE_NOTE, "XPCS SYMBOL_ERR_L2_3 = 0x%x\n", val);
+#endif
+
+	/* Check from BCM8704 if 10G link is up or down */
+
+	/* Check Device 1 Register 0xA bit0 */
+	status = nxge_mdio_read(nxgep, phy_port_addr,
+			BCM8704_PMA_PMD_DEV_ADDR,
+			BCM8704_PMD_RECEIVE_SIG_DETECT,
+			&val1);
+	if (status != NXGE_OK)
+		goto fail;
+	rx_sig_ok = ((val1 & GLOB_PMD_RX_SIG_OK) ? B_TRUE : B_FALSE);
+
+	/* Check Device 3 Register 0x20 bit0 */
+	if ((status = nxge_mdio_read(nxgep, phy_port_addr,
+			BCM8704_PCS_DEV_ADDR,
+			BCM8704_10GBASE_R_PCS_STATUS_REG,
+			&val2)) != NPI_SUCCESS)
+		goto fail;
+	pcs_blk_lock = ((val2 & PCS_10GBASE_R_PCS_BLK_LOCK) ? B_TRUE : B_FALSE);
+
+	/* Check Device 4 Register 0x18 bit12 */
+	status = nxge_mdio_read(nxgep, phy_port_addr,
+			BCM8704_PHYXS_ADDR,
+			BCM8704_PHYXS_XGXS_LANE_STATUS_REG,
+			&val3);
+	if (status != NXGE_OK)
+		goto fail;
+	link_align = (val3 == (XGXS_LANE_ALIGN_STATUS | XGXS_LANE3_SYNC |
+				XGXS_LANE2_SYNC | XGXS_LANE1_SYNC |
+				XGXS_LANE0_SYNC | 0x400)) ? B_TRUE : B_FALSE;
+
+#ifdef	NXGE_DEBUG_ALIGN_ERR
+	/* Temp workaround for link down issue */
+	if (pcs_blk_lock == B_FALSE) {
+		if (val2 != 0x4) {
+			pcs_blk_lock = B_TRUE;
+			cmn_err(CE_NOTE,
+				"LINK DEBUG: port%d PHY Dev3 Reg 0x20 = 0x%x\n",
+				nxgep->mac.portnum, val2);
+		}
+	}
+
+	if (link_align == B_FALSE) {
+		if (val3 != 0x140f) {
+			link_align = B_TRUE;
+			cmn_err(CE_NOTE,
+				"LINK DEBUG: port%d PHY Dev4 Reg 0x18 = 0x%x\n",
+				nxgep->mac.portnum, val3);
+		}
+	}
+
+	if (rx_sig_ok == B_FALSE) {
+		if ((val2 == 0) || (val3 == 0)) {
+			rx_sig_ok = B_TRUE;
+			cmn_err(CE_NOTE,
+				"LINK DEBUG: port %d Dev3 or Dev4 read zero\n",
+				nxgep->mac.portnum);
+		}
+	}
+#endif
+
+	*link_up = ((rx_sig_ok == B_TRUE) && (pcs_blk_lock == B_TRUE) &&
+			(link_align == B_TRUE)) ? B_TRUE : B_FALSE;
+
+	return (NXGE_OK);
+fail:
+	return (status);
+}
+
+
+nxge_status_t
+nxge_10g_link_led_on(p_nxge_t nxgep)
+{
+	if (npi_xmac_xif_led(nxgep->npi_handle, nxgep->mac.portnum, B_TRUE)
+							!= NPI_SUCCESS)
+		return (NXGE_ERROR);
+	else
+		return (NXGE_OK);
+}
+
+nxge_status_t
+nxge_10g_link_led_off(p_nxge_t nxgep)
+{
+	if (npi_xmac_xif_led(nxgep->npi_handle, nxgep->mac.portnum, B_FALSE)
+							!= NPI_SUCCESS)
+		return (NXGE_ERROR);
+	else
+		return (NXGE_OK);
+}
+
+void
+nxge_clear_mac_counters(p_nxge_t nep)
+{
+	uint64_t reg_val = 0;
+	npi_handle_t handle = nep->npi_handle;
+	uint8_t portn = nep->mac.portnum;
+	nxge_port_t portt = nep->mac.porttype;
+
+	if (portt != PORT_TYPE_XMAC)
+		return;
+
+	XMAC_REG_RD(handle, portn, XRXMAC_HIST_CNT1_REG, &reg_val);
+	XMAC_REG_RD(handle, portn, XRXMAC_HIST_CNT2_REG, &reg_val);
+	XMAC_REG_RD(handle, portn, XRXMAC_HIST_CNT3_REG, &reg_val);
+	XMAC_REG_RD(handle, portn, XRXMAC_HIST_CNT4_REG, &reg_val);
+	XMAC_REG_RD(handle, portn, XRXMAC_HIST_CNT5_REG, &reg_val);
+	XMAC_REG_RD(handle, portn, XRXMAC_HIST_CNT6_REG, &reg_val);
+	XMAC_REG_RD(handle, portn, XRXMAC_HIST_CNT7_REG, &reg_val);
+	XMAC_REG_RD(handle, portn, XRXMAC_BC_FRM_CNT_REG, &reg_val);
+	XMAC_REG_RD(handle, portn, XRXMAC_MC_FRM_CNT_REG, &reg_val);
+	XMAC_REG_RD(handle, portn, XRXMAC_BT_CNT_REG, &reg_val);
+	XMAC_REG_RD(handle, portn, XRXMAC_FRAG_CNT_REG, &reg_val);
+	XMAC_REG_RD(handle, portn, XRXMAC_MPSZER_CNT_REG, &reg_val);
+	XMAC_REG_RD(handle, portn, XRXMAC_CRC_ER_CNT_REG, &reg_val);
+	XMAC_REG_RD(handle, portn, XRXMAC_CD_VIO_CNT_REG, &reg_val);
+	XMAC_REG_RD(handle, portn, XRXMAC_AL_ER_CNT_REG, &reg_val);
+	XMAC_REG_RD(handle, portn, XMAC_LINK_FLT_CNT_REG, &reg_val);
+	XMAC_REG_RD(handle, portn, XTXMAC_FRM_CNT_REG, &reg_val);
+	XMAC_REG_RD(handle, portn, XTXMAC_BYTE_CNT_REG, &reg_val);
+}
+
+
+static nxge_status_t
+nxge_mii_get_link_mode(p_nxge_t nxgep)
+{
+	p_nxge_stats_t	statsp;
+	uint8_t		xcvr_portn;
+	p_mii_regs_t	mii_regs;
+	mii_mode_control_stat_t	mode;
+	int		status = NXGE_OK;
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL, "==> nxge_mii_get_link_mode"));
+
+	statsp = nxgep->statsp;
+	xcvr_portn = statsp->mac_stats.xcvr_portn;
+	mii_regs = NULL;
+	mode.value = 0;
+	mode.bits.shadow = NXGE_MII_MODE_CONTROL_REG;
+	if ((status = nxge_mii_write(nxgep, xcvr_portn,
+	    (uint8_t)(uint64_t)(&mii_regs->shadow),
+	    mode.value)) != NXGE_OK) {
+		goto fail;
+	}
+	if ((status = nxge_mii_read(nxgep,
+			xcvr_portn,
+			(uint8_t)(uint64_t)(&mii_regs->shadow),
+			&mode.value)) != NXGE_OK) {
+		goto fail;
+	}
+
+	if (mode.bits.mode == NXGE_MODE_SELECT_FIBER) {
+		nxgep->mac.portmode = PORT_1G_RGMII_FIBER;
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+		    "nxge_mii_get_link_mode: fiber mode"));
+	}
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+	    "nxge_mii_get_link_mode: "
+	    "(address 0x%x) port 0x%x mode value 0x%x link mode 0x%x",
+	    MII_MODE_CONTROL_REG, xcvr_portn,
+	    mode.value, nxgep->mac.portmode));
+
+	NXGE_DEBUG_MSG((nxgep, MAC_CTL,
+	    "<== nxge_mii_get_link_mode"));
+	return (status);
+fail:
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+	    "<== nxge_mii_get_link_mode (failed)"));
+	return (NXGE_ERROR);
+}
+
+static void
+nxge_mii_dump(p_nxge_t nxgep)
+{
+	p_nxge_param_t	param_arr;
+	p_nxge_stats_t	statsp;
+	uint8_t		xcvr_portn;
+	p_mii_regs_t	mii_regs;
+	mii_bmcr_t	bmcr;
+	mii_bmsr_t	bmsr;
+	mii_idr1_t	idr1;
+	mii_idr2_t	idr2;
+	mii_mode_control_stat_t	mode;
+
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL, "==> nxge_mii_dump"));
+
+	param_arr = nxgep->param_arr;
+	statsp = nxgep->statsp;
+	xcvr_portn = statsp->mac_stats.xcvr_portn;
+
+	mii_regs = NULL;
+
+	(void) nxge_mii_read(nxgep,
+	    nxgep->statsp->mac_stats.xcvr_portn,
+	    (uint8_t)(uint64_t)(&mii_regs->bmcr),
+	    &bmcr.value);
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+	    "nxge_mii_dump: bmcr (0) xcvr 0x%x value 0x%x",
+	    xcvr_portn, bmcr.value));
+
+	(void) nxge_mii_read(nxgep,
+	    nxgep->statsp->mac_stats.xcvr_portn,
+	    (uint8_t)(uint64_t)(&mii_regs->bmsr),
+	    &bmsr.value);
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+	    "nxge_mii_dump: bmsr (1) xcvr 0x%x value 0x%x",
+	    xcvr_portn, bmsr.value));
+
+	(void) nxge_mii_read(nxgep,
+	    nxgep->statsp->mac_stats.xcvr_portn,
+	    (uint8_t)(uint64_t)(&mii_regs->idr1),
+	    &idr1.value);
+
+	(void) nxge_mii_read(nxgep,
+	    nxgep->statsp->mac_stats.xcvr_portn,
+	    (uint8_t)(uint64_t)(&mii_regs->idr2),
+	    &idr2.value);
+
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+	   "nxge_mii_dump: idr1 (2) xcvr 0x%x value 0x%x",
+	   xcvr_portn, idr1.value));
+
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+	    "nxge_mii_dump: idr2 (3) xcvr 0x%x value 0x%x",
+	    xcvr_portn, idr2.value));
+
+	mode.value = 0;
+	mode.bits.shadow = NXGE_MII_MODE_CONTROL_REG;
+	(void) nxge_mii_write(nxgep, xcvr_portn,
+	    (uint8_t)(uint64_t)(&mii_regs->shadow), mode.value);
+	(void) nxge_mii_read(nxgep, xcvr_portn,
+	    (uint8_t)(uint64_t)(&mii_regs->shadow),
+	    &mode.value);
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+		"nxge_mii_dump: mode control xcvr 0x%x value 0x%x",
+		xcvr_portn, mode.value));
+}
diff --git a/drivers/net/nxge/nxge_main.c b/drivers/net/nxge/nxge_main.c
new file mode 100644
index 0000000..9a76a65
--- /dev/null
+++ b/drivers/net/nxge/nxge_main.c
@@ -0,0 +1,14023 @@
+/*
+ * nxge_main.c	Neptune main function - rx, tx, interrupts, ioctls
+ * and driver entry points.
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#include <nxge_impl.h>
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,9)
+#define PCI_SAVE_STATE(pdev, buf)	pci_save_state((pdev))
+#define PCI_RESTORE_STATE(pdev, buf)	pci_restore_state((pdev))
+#else
+#define PCI_SAVE_STATE(pdev, buf)	pci_save_state((pdev), (buf))
+#define PCI_RESTORE_STATE(pdev, buf)	pci_restore_state((pdev), (buf))
+#endif
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,9)
+#define NETDEV_TX_OK 0
+#define NETDEV_TX_BUSY -1
+#define NETDEV_TX_LOCKED 1
+#define NET_IP_ALIGN 16
+#endif
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,16)
+#define NXGE_ADD_SKB_TRUESIZE(skb, len) (skb->truesize += len)
+#else
+#define NXGE_ADD_SKB_TRUESIZE(skb, len)
+#endif
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 17)
+#define USE_GSO_API
+#define NXGE_SKB_LINEARIZE(skb, gfp) skb_linearize(skb)
+#else
+#define NXGE_SKB_LINEARIZE(skb, gfp) skb_linearize(skb, gfp)
+#endif
+
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,18)
+#define NXGE_RX_CHECKSUM_OK(skb, flag) \
+{\
+skb->ip_summed = (flag ? CHECKSUM_UNNECESSARY : CHECKSUM_NONE); \
+}
+#define NXGE_TX_CHECKSUM_NEEDED CHECKSUM_PARTIAL
+#else
+#define NXGE_RX_CHECKSUM_OK(skb, flag) \
+{\
+skb->ip_summed = (flag ? CHECKSUM_UNNECESSARY : CHECKSUM_NONE); \
+}
+#define NXGE_TX_CHECKSUM_NEEDED CHECKSUM_HW
+#endif
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 19)
+#define PCI_DRIVER_INIT(driver) pci_register_driver(driver)
+#else
+#define PCI_DRIVER_INIT(driver) pci_module_init(driver)
+#endif
+
+
+
+
+#if (LINUX_VERSION_CODE == KERNEL_VERSION(2, 6, 16))
+#if defined(SLE_VERSION_CODE)
+#define USE_GSO_API
+#undef NXGE_SKB_LINEARIZE
+#define NXGE_SKB_LINEARIZE(skb, gfp) skb_linearize(skb)
+#endif
+#endif
+
+#ifndef skb_header_cloned
+#define skb_header_cloned(x) 0
+#endif
+
+#ifdef USE_GSO_API
+#define SKB_IS_GSO(skb) skb_shinfo(skb)->gso_size
+#define SKB_GSO_SEGS(skb) skb_shinfo(skb)->gso_segs
+#else
+#define SKB_IS_GSO(skb) skb_shinfo(skb)->tso_size
+#define SKB_GSO_SEGS(skb) skb_shinfo(skb)->tso_segs
+#endif
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 19)
+#define PCI_DRIVER_INIT(driver) pci_register_driver(driver)
+#define SKB_CKSUM_OFFSET(skb) skb->csum_offset
+#else
+#define PCI_DRIVER_INIT(driver) pci_module_init(driver)
+#define SKB_CKSUM_OFFSET(skb) skb->csum
+#endif
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 23)
+#define SET_MODULE_OWNER(dev)
+#endif
+
+
+#define NXGE_USE_NXGE_GSO
+
+#define NXGE_FILL_TX_HDR(hdr_buffer, skb, pkt_len, padbytes) \
+{ \
+	p_tx_pkt_header_t	hdrp; \
+	p_tx_pkt_hdr_all_t	pkthdrp; \
+	p_skb_hdr_info_t p_hdr; \
+	pkthdrp = (p_tx_pkt_hdr_all_t) hdr_buffer; \
+	hdrp = (p_tx_pkt_header_t) &pkthdrp->pkthdr; \
+	pkthdrp->reserved = 0; \
+	p_hdr = (skb_hdr_info_t *) skb->cb; \
+	hdrp->value = 0; \
+	if (skb->ip_summed == NXGE_TX_CHECKSUM_NEEDED) \
+	hdrp->value  = p_hdr->ctl_hdrp.value; \
+	hdrp->bits.ldw.tot_xfer_len = pkt_len - TX_PKT_HEADER_SIZE; \
+	hdrp->bits.ldw.pad = (padbytes >> 1); \
+}
+
+#define NXGE_FILL_TX_HDR_GSO(hdr_buffer, csum_value, real_len, pads) \
+{ \
+	p_tx_pkt_header_t	hdrp; \
+	p_tx_pkt_hdr_all_t	pkthdrp; \
+	pkthdrp = (p_tx_pkt_hdr_all_t) hdr_buffer; \
+	hdrp = (p_tx_pkt_header_t) &pkthdrp->pkthdr; \
+	pkthdrp->reserved = 0; \
+	hdrp->value  = csum_value; \
+	hdrp->bits.ldw.tot_xfer_len = real_len; \
+	hdrp->bits.ldw.pad = pads; \
+}
+
+#define PCI_EXTENDED_BASE 0x80
+
+#define NXGE_MAX_NEPTUNE_SYSTEM 10
+
+#define NXGE_NEPTUNE_MAGIC 0x4E584745UL
+
+
+nxge_neptune_t neptune[NXGE_MAX_NEPTUNE_SYSTEM];
+
+#define	NXGE_ACTUAL_RDCGRP(nxgep, rdcgrp)	\
+	(rdcgrp + nxgep->pt_config.hw_config.start_rdc_grpid)
+#define	NXGE_ACTUAL_RDC(nxgep, rdc)	\
+	(rdc + nxgep->pt_config.hw_config.start_rdc)
+
+
+#define  nxge_post_buf_blk(nep, rx_rbr_p, rx_msg_p) \
+{ \
+	rx_msg_p->free = B_FALSE; \
+	rx_msg_p->cur_usage_cnt = 0; \
+	rx_msg_p->max_usage_cnt = 0; \
+	rx_msg_p->pkt_buf_size = 0; \
+	rx_rbr_p->tmp_buffers[rx_rbr_p->tmp_count] = rx_msg_p->shifted_addr; \
+	rx_rbr_p->tmp_count++; \
+	if (rx_rbr_p->tmp_count == RBR_POST_BATCH) { \
+	int i; \
+	for (i = 0; i < RBR_POST_BATCH; i++) { \
+	rx_rbr_p->rbr_wr_index = ((rx_rbr_p->rbr_wr_index + 1) & rx_rbr_p->wrap_mask); \
+	rx_rbr_p->rbr_desc_vp[rx_rbr_p->rbr_wr_index] = rx_rbr_p->tmp_buffers[i]; \
+	} \
+RXDMA_REG_WRITE64(nep->npi_handle, RBR_KICK_REG, rx_rbr_p->rdc, RBR_POST_BATCH); \
+	   rx_rbr_p->tmp_count = 0; \
+	 } \
+}
+
+#define USE_TXHDR_BUFFER
+#ifdef USE_TXHDR_BUFFER
+#define USE_STREAMING_TXHDR_BUFFER
+#endif
+
+#define USE_TX_WAKEUP_INTERRUPT
+
+
+#define NXGE_MAX_ADDRESS_BITS 0x00000FFFFFFFFFFFULL
+
+/* RX Datapath */
+/*
+ * if USE_DYNAMIC_ALLOC is defined, then either
+ * USE_RING_HASH or USE_VA_INDEX
+ */
+
+#define USE_DYNAMIC_ALLOC
+
+/*
+ * could define only one buffer address search mechanism
+ *
+ * USE_RING_HASH
+ *    Uses hash table to find dma_addr --> va_addr;
+ * USE_BTREE
+ *    Assumes the buffer pool doesn't change. If it changes
+ *    resorting of the information array is required. Hence
+ *    if USE_BTREE is defined, USE_DYNAMIC_ALLOC should be
+ *    undefined.
+ * USE_VA_INDEX
+ *    assume __va function works. Possibly only on 64 bit systems.
+ *    The buffer index is embedded at base ptr - 4
+ *
+ * USE_DYNAMIC_ALLOC
+ *    Uses skb fragments to attach data buffers. The main skb
+ *    contains only the header (L2 and maybe L3+L4 headers)
+ *
+ *
+ *
+ */
+
+#ifdef USE_DYNAMIC_ALLOC
+
+#ifdef CONFIG_XEN
+#define USE_RING_HASH
+#define RING_HASH_FUNC 0x1ff
+#define NXGE_RX_RING_HASH_NULL_KEY 0xffffffff
+#else
+#define USE_VA_INDEX
+#endif /* CONFIG_XEN */
+#else
+#define USE_BTREE
+#endif /* USE_DYNAMIC_ALLOC */
+
+
+#define NXGE_MAP_KERN_PAGE(pg)      kmap_atomic((pg), KM_SKB_DATA_SOFTIRQ)
+#define NXGE_UNMAP_KERN_PAGE(pg)    kunmap_atomic((pg), KM_SKB_DATA_SOFTIRQ)
+
+/*#define	USE_DYNAMIC_BLANKING */
+
+#ifdef	USE_DYNAMIC_BLANKING
+uint16_t	nxge_rx_intr_timeout = RCRCFIG_B_TIMEOUT | RXDMA_RCR_TO_DEFAULT;
+#endif
+
+
+#define NXGE_TX_DESC_SIZE_SHIFT 3 /* 8 bytes */
+#define NXGE_RCR_DESC_SIZE_SHIFT 3 /* 8 bytes */
+
+#ifdef CONFIG_PCI_MSI
+uint32_t	nxge_msi_enable = 1;		/* debug: turn msi on/off */
+
+#if defined(CONFIG_NXGE_NAPI) && defined(HAVE_NETDEV_POLL)
+/* If NAPI enabled, disable MSI-X */
+uint32_t	nxge_msix_enable = 0;		/* turn msix on/off */
+#else
+uint32_t	nxge_msix_enable = 1;		/* debug: turn msix on/off */
+#endif
+
+uint32_t	nxge_legacy_api = 0;		/* debug: old apis */
+#else
+uint32_t	nxge_legacy_api = 1;		/* debug: old apis */
+#endif
+
+extern int nxge_atca_port_mode;
+uint32_t 	nxge_rbr_size = NXGE_RBR_RBB_DEFAULT;	/* 512 		*/
+uint32_t 	nxge_rbr_spare_size = 0;
+
+uint32_t 	nxge_rcr_size = NXGE_RCR_DEFAULT; 	/* 4096 	*/
+
+#define NO_HINT 0xffff
+
+uint32_t 	nxge_reclaim_pending = NXGE_TX_RECLAIM;
+uint32_t 	nxge_tx_ring_size = NXGE_TX_RING_DEFAULT;	/* 2048 */
+uint32_t 	nxge_tx_min_free = TX_MIN_FREE;
+uint32_t	nxge_tx_max_gathers = TX_MAX_GATHER_POINTERS;
+
+
+uint32_t	nxge_tx_intr_thres = 0;
+
+uint32_t	nxge_jumbo_mtu	= TX_JUMBO_MTU;
+uint32_t	nxge_jumbo_enable = B_TRUE;
+/* either 0 or 4 */
+uint32_t	nxge_dont_strip_crc = 4;
+/*
+ * Add tunable to reduce the amount of time spent in the
+ * ISR doing Rx Processing.
+ */
+uint32_t nxge_max_rx_pkts = RXDMA_RCR_MAX_PKTS_INTR;
+uint32_t	nxge_rcr_timeout = RXDMA_RCR_TO_DEFAULT | RCRCFIG_B_TIMEOUT;
+uint32_t	nxge_rcr_threshold = RXDMA_RCR_PTHRES_DEFAULT;
+
+#ifdef RX_INTR_BLANKING_LDG
+uint32_t	nxge_ldg_timeout = NXGE_TIMER_LDG;
+#endif
+
+uint32_t nxge_tx_bcopy_threshold = NXGE_TX_COPY_BUFFER_SIZE;
+uint32_t nxge_txdesc_max_xfer_len =  TX_MAX_TRANSFER_LENGTH;
+static int nxge_debug = -1;
+
+boolean_t nxge_use_partition = B_FALSE;
+
+nxge_os_mutex_t nxge_eeprom_lock;
+static atomic_t nxge_eeprom_lock_init = ATOMIC_INIT(0);
+
+nxge_os_mutex_t nxge_common_lock;
+static atomic_t nxge_common_lock_init = ATOMIC_INIT(0);
+static atomic_t nxge_common_lock_init_done = ATOMIC_INIT(0);
+
+
+#define NXGE_DEF_MSG_ENABLE	  \
+	(NETIF_MSG_DRV		| \
+	 NETIF_MSG_PROBE	| \
+	 NETIF_MSG_LINK		| \
+	 NETIF_MSG_TIMER	| \
+	 NETIF_MSG_IFDOWN	| \
+	 NETIF_MSG_IFUP		| \
+	 NETIF_MSG_RX_ERR	| \
+	 NETIF_MSG_TX_ERR)
+
+#if 0
+static char version[] __devinitdata =
+	DRV_MODULE_NAME ".c:v" DRV_MODULE_VERSION " (" DRV_MODULE_RELDATE ")\n";
+#endif
+
+MODULE_AUTHOR("Sun Microsystems Inc. (neptune_SW_dev@sun.com)");
+MODULE_DESCRIPTION("Sun Multithreaded 10G Ethernet Driver");
+MODULE_LICENSE("Dual BSD/GPL");
+
+static struct pci_device_id nxge_pci_tbl[] __devinitdata = {
+	{ PCI_VENDOR_ID_SUN, PCI_DEVICE_ID_SUN_NEPTUNE,
+	  PCI_ANY_ID, PCI_ANY_ID, 0, 0, 0UL },
+	{ 0, }
+};
+
+MODULE_DEVICE_TABLE(pci, nxge_pci_tbl);
+
+
+#define nxge_pci_func(x)	PCI_FUNC(x)
+
+/*
+ * Static partition configuration of each Neptune/NIU.
+ * (Its config should be from the partition manager)
+ * This list contains the per Neptune/NIU data strucures
+ * and most of its configuration data are configured
+ * by the service paritition.
+ */
+spinlock_t 		nxge_list_lock;
+
+rtrace_t npi_rtracebuf;
+
+/*
+ * This list contains the instance structures for the Neptune
+ * devices present in the system. The lock exists to guarantee
+ * mutually exclusive access to the list.
+ */
+void 		*nxge_fn_list;
+
+extern struct ethtool_ops nxge_ethtool_ops;
+
+spinlock_t nxge_tx_ring_lb_lock;
+
+/*
+ * dma chunk sizes.
+ *
+ * Try to allocate the largest possible size
+ * so that fewer number of dma chunks would be managed
+ */
+size_t alloc_sizes [] = {0x1000, 0x2000, 0x4000, 0x8000, 0x10000, 0x20000,
+			 0x40000, 0x80000, 0x100000, 0x200000, 0x400000,
+			 0x800000, 0x1000000};
+
+
+void nxge_virint_regs_dump(p_nxge_t);
+void nxge_int_regs_dump(p_nxge_t);
+void nxge_txdma_regs_dump_channels(p_nxge_t);
+void nxge_rxdma_regs_dump_channels(p_nxge_t);
+void nxge_rxdma_regs_dump(p_nxge_t nep, int rdc);
+
+/********************************************************************/
+
+#if defined(CONFIG_NXGE_NAPI) && defined(HAVE_NETDEV_POLL)
+#define USE_NAPI
+#define nxge_skb_release(nep, skb) \
+nep->dev->last_rx = jiffies; \
+netif_receive_skb(skb);
+#else
+#define nxge_skb_release(nep, skb)  \
+nep->dev->last_rx = jiffies; \
+netif_rx(skb);
+#endif
+
+
+/*************************************************************************
+ *                 Static function declarations
+ *
+ *************************************************************************/
+
+static nxge_status_t nxge_hw_reset(p_nxge_t);
+static void nxge_hw_stop(p_nxge_t);
+static void nxge_shutdown(p_nxge_t);
+static int nxge_setup_multicast(p_nxge_t);
+
+#ifdef CONFIG_NET_POLL_CONTROLLER
+static void nxge_netpoll(struct net_device *);
+#endif
+
+static void nxge_watchdog_timer(unsigned long);
+static void nxge_tx_timeout(struct net_device *netdev);
+static int nxge_change_mtu(struct net_device *, int);
+static struct net_device_stats *nxge_get_netdev_stats(struct net_device *);
+static void nxge_set_multicast(struct net_device *);
+static int nxge_set_mac_address(struct net_device *, void *);
+static void nxge_tx_softintr(unsigned long);
+
+#ifdef NXGE_USE_SOFT_RX
+static void nxge_rx_isr(unsigned long);
+#endif
+static int nxge_ethtool_ioctl(struct net_device *, void *);
+static int nxge_rtrace_ioctl(p_nxge_t, rtrace_t *);
+static int nxge_ioctl(struct net_device *, struct ifreq *, int);
+static void nxge_pci_cfg_space_dump(p_nxge_t);
+
+
+static void nxge_get_default_sprom_config(p_nxge_t nep);
+static int nxge_get_sprom_config(p_nxge_t nxgep);
+static int nxge_get_eeprom_config(p_nxge_t nxgep);
+static void nxge_setup_hw_pciconfig(p_nxge_t nxgep);
+static void nxge_setup_hw_vpd_rom_mac(p_nxge_t nxgep);
+static void nxge_set_hw_class_config(p_nxge_t nxgep);
+static void nxge_use_cfg_class_config(p_nxge_t nxgep);
+static void nxge_set_hw_mac_class_config(p_nxge_t nxgep);
+static void nxge_use_cfg_mac_class_config(p_nxge_t nxgep);
+static void nxge_set_hw_vlan_class_config(p_nxge_t nxgep);
+static void nxge_use_cfg_vlan_class_config(p_nxge_t nxgep);
+static void nxge_set_rdc_intr_property(p_nxge_t nxgep);
+static void nxge_set_hw_dma_config(p_nxge_t nxgep);
+static void nxge_use_cfg_dma_config(p_nxge_t nxgep);
+static void nxge_use_cfg_neptune_properties(p_nxge_t nxgep);
+static int nxge_get_config_properties(p_nxge_t nxgep);
+
+static void nxge_txdma_stop(p_nxge_t);
+static void nxge_txdma_stop_start(p_nxge_t);
+static int nxge_txdma_stop_inj_err(p_nxge_t, int);
+static nxge_status_t nxge_init_txdma_channel_event_mask(p_nxge_t nxgep,
+							uint16_t channel,
+							p_tx_dma_ent_msk_t mask_p);
+static void nxge_hw_start_tx(p_nxge_t);
+static p_tx_ring_t nxge_txdma_get_ring(p_nxge_t nxgep, uint16_t channel);
+static p_tx_mbox_t nxge_txdma_get_mbox(p_nxge_t nxgep, uint16_t channel);
+static nxge_status_t nxge_tx_err_evnts(p_nxge_t, uint_t, p_nxge_ldv_t,
+				       tx_cs_t);
+static void nxge_fixup_txdma_rings(p_nxge_t nep);
+static void nxge_txdma_fix_channel(p_nxge_t nep, uint16_t channel);
+static void nxge_txdma_fixup_channel(p_nxge_t, p_tx_ring_t, uint16_t);
+static void nxge_txdma_hw_kick(p_nxge_t);
+static void nxge_txdma_kick_channel(p_nxge_t nep, uint16_t channel);
+static void nxge_txdma_hw_kick_channel(p_nxge_t, p_tx_ring_t, uint16_t);
+static void nxge_check_tx_hang(p_nxge_t nxgep);
+static int nxge_txdma_hung(p_nxge_t nxgep);
+static boolean_t nxge_txdma_channel_hung(p_nxge_t, p_tx_ring_t, uint16_t);
+static void nxge_fixup_hung_txdma_rings(p_nxge_t nxgep);
+static void nxge_txdma_fix_hung_channel(p_nxge_t nxgep, uint16_t channel);
+static void nxge_txdma_fixup_hung_channel(p_nxge_t, p_tx_ring_t, uint16_t);
+static void nxge_uninit_tx_cntl_ring(p_nxge_t, uint16_t, int);
+static int nxge_init_tx_cntl_ring(p_nxge_t, uint16_t, int);
+static void nxge_txdma_disable_txdma_channel(p_nxge_t, uint16_t);
+static int nxge_enable_txdma_channel(p_nxge_t, uint16_t, p_tx_ring_t,
+				     p_tx_mbox_t);
+static int nxge_reset_txdma_channel(p_nxge_t, uint16_t, uint64_t);
+static void nxge_unmap_txdma(p_nxge_t);
+static int nxge_map_txdma(p_nxge_t);
+static int nxge_txdma_hw_mode(p_nxge_t, boolean_t);
+static void nxge_txdma_hw_stop_common(p_nxge_t);
+static int nxge_txdma_hw_start_common(p_nxge_t);
+static int nxge_txdma_start_channel(p_nxge_t, uint16_t, p_tx_ring_t,
+				    p_tx_mbox_t);
+static void nxge_txdma_stop_channel(p_nxge_t, uint16_t,
+				    p_tx_ring_t, p_tx_mbox_t);
+static int nxge_txdma_hw_start(p_nxge_t);
+static void nxge_txdma_hw_stop(p_nxge_t);
+static void nxge_uninit_txdma(p_nxge_t);
+static int nxge_init_txdma(p_nxge_t);
+static int nxge_free_tx_cntl_pool(p_nxge_t);
+static int nxge_alloc_tx_cntl_pool(p_nxge_t);
+static int nxge_clean_tx(p_nxge_t);
+int nxge_tx_reclaim(p_nxge_t, p_tx_ring_t, int);
+
+int nxge_tx_ring(p_nxge_t, int, struct sk_buff *);
+int nxge_tx_lb(p_nxge_t nep, struct sk_buff *skb);
+int nxge_start_xmit(struct sk_buff *, struct net_device *);
+
+
+static nxge_status_t nxge_rxdma_handle_port_errors(p_nxge_t, uint32_t, uint32_t);
+static nxge_status_t nxge_rxdma_fatal_err_recover(p_nxge_t, uint16_t);
+#ifdef USE_BTREE
+static int nxge_sort_compare(const void *, const void *);
+static int nxge_rxbuf_index_info_init(p_nxge_t, p_rx_rbr_ring_t);
+#endif
+static void nxge_hw_start_rx(p_nxge_t);
+static void nxge_rxdma_fix_channel(p_nxge_t nxgep, uint16_t channel);
+static void nxge_rxdma_fixup_channel(p_nxge_t, uint16_t, int);
+static void nxge_fixup_rxdma_rings(p_nxge_t);
+static int nxge_rxdma_get_ring_index(p_nxge_t nxgep, uint16_t channel);
+static p_rx_rbr_ring_t nxge_rxdma_get_rbr_ring(p_nxge_t, uint16_t);
+static p_rx_rcr_ring_t nxge_rxdma_get_rcr_ring(p_nxge_t, uint16_t);
+static int nxge_init_rxdma_channels(p_nxge_t nxgep);
+static void nxge_uninit_rxdma_channels(p_nxge_t nxgep);
+static int nxge_disable_rxdma_channel(p_nxge_t, uint16_t);
+static int nxge_enable_rxdma_channel(p_nxge_t, uint16_t, p_rx_rbr_ring_t,
+				     p_rx_rcr_ring_t, p_rx_mbox_t);
+static void nxge_free_rx_buf_pool(p_nxge_t, int);
+static int nxge_alloc_rx_buf_pool(p_nxge_t, int, size_t, uint16_t);
+static void nxge_uninit_rx_buf_ring(p_nxge_t, uint16_t, int);
+static int nxge_init_rx_buf_ring(p_nxge_t, uint16_t, int);
+static void nxge_uninit_rx_cntl_ring(p_nxge_t, uint16_t, int);
+static int nxge_init_rx_cntl_ring(p_nxge_t, uint16_t, int);
+static void nxge_unmap_rxdma(p_nxge_t);
+static int nxge_map_rxdma(p_nxge_t);
+static int nxge_rxdma_hw_mode(p_nxge_t, boolean_t);
+static int nxge_init_rxdma_channel_cntl_stat(p_nxge_t, uint16_t,
+					     p_rx_dma_ctl_stat_t);
+static int nxge_rxdma_start_channel(p_nxge_t, uint16_t, p_rx_rbr_ring_t,
+				    p_rx_rcr_ring_t, p_rx_mbox_t);
+static int nxge_rxdma_stop_channel(p_nxge_t, uint16_t);
+static int nxge_rxdma_hw_start_common(p_nxge_t);
+static void nxge_rxdma_hw_stop_common(p_nxge_t);
+static int nxge_rxdma_hw_start(p_nxge_t);
+static void nxge_rxdma_hw_stop(p_nxge_t);
+static void nxge_uninit_rxdma(p_nxge_t);
+static int nxge_init_rxdma(p_nxge_t);
+static int nxge_free_rx_cntl_pool(p_nxge_t);
+static int nxge_alloc_rx_cntl_pool(p_nxge_t);
+static int nxge_rxbuf_pp_to_vp(p_nxge_t, p_rx_rbr_ring_t, uint8_t, dma_addr_t,
+			       caddr_t *, uint32_t *, uint32_t *);
+
+static void nxge_receive_packet(p_nxge_t,
+				p_rx_rcr_ring_t, uint64_t, boolean_t *,
+				struct sk_buff **, int *, int);
+static nxge_status_t nxge_rx_err_evnts(p_nxge_t, uint_t, p_nxge_ldv_t,
+				       rx_dma_ctl_stat_t);
+static int nxge_rx_pkts_ring(p_nxge_t, uint_t, int, int *,
+ rx_dma_ctl_stat_t);
+
+#ifdef USE_NAPI
+static int nxge_poll(struct net_device *, int *);
+static int nxge_rx_intr_set(p_nxge_t, boolean_t);
+#endif
+
+
+static int nxge_free_cntl_pool(p_nxge_t);
+static int nxge_alloc_cntl_pool(p_nxge_t);
+static void nxge_setup_system_dma_pages(p_nxge_t);
+
+static int nxge_intr_mask_mgmt_set(p_nxge_t, boolean_t);
+
+static void nxge_intr_hw_disable(p_nxge_t);
+static inline boolean_t nxge_is_ldf_set(uint8_t, uint64_t, uint64_t, uint64_t);
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,18)
+static irqreturn_t nxge_intr(int, void *);
+static irqreturn_t nxge_intr_legacy(int, void *);
+static irqreturn_t nxge_mif_intr(int, void *);
+static irqreturn_t nxge_mac_intr(int, void *);
+static irqreturn_t nxge_syserr_intr(int, void *);
+static irqreturn_t nxge_tx_intr(int, void *);
+static irqreturn_t nxge_rx_intr(int, void *);
+#define NXGE_SET_IRQ_TYPE(irq, type) set_irq_type(irq, type);
+#else
+
+static irqreturn_t nxge_intr(int, void *, struct pt_regs *);
+static irqreturn_t nxge_intr_legacy(int, void *, struct pt_regs *);
+static irqreturn_t nxge_mif_intr(int, void *, struct pt_regs *);
+static irqreturn_t nxge_mac_intr(int, void *, struct pt_regs *);
+static irqreturn_t nxge_syserr_intr(int, void *, struct pt_regs *);
+static irqreturn_t nxge_tx_intr(int, void *, struct pt_regs *);
+static irqreturn_t nxge_rx_intr(int, void *, struct pt_regs *);
+#define NXGE_SET_IRQ_TYPE(irq, type)
+
+#endif
+
+static int nxge_ldv_cnt(p_nxge_t);
+static void nxge_ldgv_setup(p_nxge_ldg_t *, p_nxge_ldv_t *,
+			    uint8_t, uint8_t, uint8_t *);
+static int nxge_ldgv_init(p_nxge_t, int, int *);
+static int nxge_ldgv_uninit(p_nxge_t);
+static int nxge_intr_ldgv_init(p_nxge_t);
+static int nxge_intr_mask_mgmt(p_nxge_t);
+static int nxge_add_intrs_legacy(p_nxge_t);
+static int nxge_add_intrs_msi(p_nxge_t);
+static int nxge_add_intrs_msix(p_nxge_t);
+static int nxge_add_intrs(p_nxge_t);
+static void nxge_remove_intrs(p_nxge_t);
+
+static int nxge_start(struct net_device *);
+static int nxge_stop(struct net_device *);
+static int nxge_open(struct net_device *);
+static int nxge_close(struct net_device *);
+static int nxge_init_device(p_nxge_t);
+static void nxge_uninit_device(p_nxge_t);
+static void nxge_unmap_regs(p_nxge_t);
+static int nxge_map_regs(p_nxge_t);
+static int __devinit nxge_init_one(struct pci_dev *,
+				   const struct pci_device_id *);
+static void __devexit nxge_remove_one(struct pci_dev *);
+static int __init nxge_init(void);
+static void __exit nxge_cleanup(void);
+#ifdef CONFIG_PM
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 10)
+static int nxge_suspend(struct pci_dev *, pm_message_t);
+#else
+static int nxge_suspend(struct pci_dev *, uint32_t);
+#endif
+static int nxge_resume(struct pci_dev *);
+#endif
+
+
+/*ARGSUSED*/
+/*VARARGS*/
+#ifdef NXGE_DEBUG
+static debug_level_t nxge_debug_level = NXGE_ERR_CTL | MOD_CTL | TX_CTL | RX_CTL | MEM_CTL | MAC_CTL | MII_CTL | INT_CTL | DMA_CTL | SYSERR_CTL | PCS_CTL | MIF_CTL | IOC_CTL | CFG_CTL | FFLP_CTL | NXGE_ERR_CTL;
+#endif
+
+nxge_os_mutex_t nxgedebuglock;
+int nxge_debug_init = 0;
+
+
+void
+nxge_init_stats(p_nxge_t nxgep)
+{
+
+	size_t nxge_stats_sz;
+
+	NXGE_DEBUG_MSG((nxgep, MOD_CTL, "==> nxge_init_stats"));
+
+	nxge_stats_sz = sizeof (nxge_stats_t) - sizeof (uint32_t);
+	nxgep->statsp = KMEM_ZALLOC(nxge_stats_sz, GFP_KERNEL);
+	nxgep->old_statsp = KMEM_ZALLOC(nxge_stats_sz, GFP_KERNEL);
+	nxgep->statsp->stats_size = nxge_stats_sz;
+	NXGE_DEBUG_MSG((nxgep, MOD_CTL, " <== nxge_init_stats"));
+
+}
+
+/* TODO: neptune: add neptune specific */
+void
+nxge_setup_stats(p_nxge_t nxgep)
+{
+#if 0
+	size_t nxge_stats_sz;
+
+	NXGE_DEBUG_MSG((nxgep, MOD_CTL, "==> nxge_setup_stats"));
+
+	nxge_stats_sz = sizeof (nxge_stats_t) - sizeof (uint32_t);
+	nxgep->statsp = KMEM_ZALLOC(nxge_stats_sz, GFP_KERNEL);
+	nxgep->old_statsp = KMEM_ZALLOC(nxge_stats_sz, GFP_KERNEL);
+	nxgep->statsp->stats_size = nxge_stats_sz;
+
+	NXGE_DEBUG_MSG((nxgep, MOD_CTL, "<== nxge_setup_stats"));
+#endif
+}
+
+void
+nxge_destroy_stats(p_nxge_t nxgep)
+{
+	NXGE_DEBUG_MSG((nxgep, MOD_CTL, "==> nxge_destroy_stats"));
+
+	if (nxgep->statsp) {
+		KMEM_FREE(nxgep->statsp, nxgep->statsp->stats_size);
+		KMEM_FREE(nxgep->old_statsp, nxgep->statsp->stats_size);
+	}
+	NXGE_DEBUG_MSG((nxgep, MOD_CTL, "<== nxge_destroy_stats"));
+}
+
+#ifdef NXGE_DEBUG
+void
+nxge_debug_msg(p_nxge_t nep, debug_level_t level, char *fmt, ...)
+{
+	va_list args;
+	char msg_buffer[2048];
+
+	if (nxge_debug_init == 0) {
+		MUTEX_INIT(&nxgedebuglock, NULL, MUTEX_DRIVER, NULL);
+		nxge_debug_init = 1;
+	}
+
+	MUTEX_ENTER(&nxgedebuglock);
+
+	va_start(args, fmt);
+	vsprintf(msg_buffer, fmt, args);
+	va_end(args);
+
+	if ((level & nxge_debug_level) || (level & NXGE_ERR_CTL)) {
+		if (nep == NULL) {
+	    printk(KERN_INFO "nxge: %s\n", msg_buffer);
+		} else {
+			printk(KERN_INFO "%s[nxge%d]: %s\n",
+			       nep->dev->name, nep->function_num, msg_buffer);
+		}
+	}
+
+	MUTEX_EXIT(&nxgedebuglock);
+}
+#endif
+
+void
+nxge_error_msg(p_nxge_t nep, debug_level_t level, char *fmt, ...)
+{
+	va_list args;
+	char msg_buffer[2048];
+
+	va_start(args, fmt);
+	vsprintf(msg_buffer, fmt, args);
+	va_end(args);
+
+	if (nep == NULL) {
+	    printk(KERN_INFO "nxge: %s\n", msg_buffer);
+	} else {
+		printk(KERN_INFO "%s[nxge%d]: %s\n",
+			   nep->dev->name, nep->function_num, msg_buffer);
+	}
+}
+
+
+static nxge_status_t nxge_hw_reset(p_nxge_t nep)
+{
+	nxge_status_t 	status = NXGE_OK;
+
+	NXGE_DEBUG_MSG((nep, MOD_CTL, "==> nxge_hw_reset"));
+
+	NXGE_DEBUG_MSG((nep, MOD_CTL, "<== nxge_hw_reset"));
+
+	return (status);
+
+}
+
+
+static void nxge_hw_init_niu_common(p_nxge_t nxgep)
+{
+
+	NXGE_DEBUG_MSG((nxgep, MOD_CTL, "==> nxge_hw_init_niu_common"));
+	MUTEX_ENTER(nxgep->cfg_lock);
+
+	/*
+	 * Set up the shared hw portions.
+	 * The first device to attach will do this config.
+	 * The others will wait until the config is complete.
+	 */
+	if (neptune[nxgep->neptune_index].flags & COMMON_INIT_DONE) {
+		NXGE_DEBUG_MSG((nxgep, MOD_CTL, "nxge_hw_init_niu_common"
+					" already done for device id %x function %d exiting",
+					neptune[nxgep->neptune_index].neptune_id,
+					nxgep->function_num));
+		MUTEX_EXIT(nxgep->cfg_lock);
+		return;
+	}
+
+	neptune[nxgep->neptune_index].flags = COMMON_INIT_START;
+
+	NXGE_DEBUG_MSG((nxgep, MOD_CTL,
+			        "nxge_hw_init_niu_common"
+				" Started for device id %x with function %d",
+				neptune[nxgep->neptune_index].neptune_id,
+				nxgep->function_num));
+
+	nxge_fflp_hw_reset(nxgep); /* per neptune common block init */
+
+	neptune[nxgep->neptune_index].flags = COMMON_INIT_DONE;
+
+	NXGE_DEBUG_MSG((nxgep, MOD_CTL,
+		        "nxge_hw_init_niu_common"
+			" Done for device id %x with function %d",
+			neptune[nxgep->neptune_index].neptune_id,
+			nxgep->function_num));
+
+	MUTEX_EXIT(nxgep->cfg_lock);
+	NXGE_DEBUG_MSG((nxgep, DDI_CTL, "<== nxge_hw_init_niu_common"));
+}
+
+static void nxge_hw_stop(p_nxge_t nep)
+{
+	NXGE_DEBUG_MSG((nep, MOD_CTL, "==> nxge_hw_stop"));
+
+	nxge_tx_mac_disable(nep);
+	nxge_rx_mac_disable(nep);
+	nxge_txdma_hw_mode(nep, NXGE_DMA_STOP);
+	nxge_rxdma_hw_mode(nep, NXGE_DMA_STOP);
+
+	NXGE_DEBUG_MSG((nep, MOD_CTL, "<== nxge_hw_stop"));
+}
+
+
+static void nxge_shutdown(p_nxge_t nep)
+{
+
+}
+
+static int nxge_setup_multicast(p_nxge_t nep)
+{
+	int		i;
+	uint16_t	data;
+	uint32_t crc;
+	struct dev_mc_list *dmi = nep->dev->mc_list;
+
+	NXGE_DEBUG_MSG((nep, MAC_CTL, "==> nxge_setup_multicast"));
+	nep->if_flags &= ~(PROMISC_F | ALLMULTI_F | HASHMATCH_F);
+
+	if (nep->dev->flags & IFF_PROMISC) {
+		NXGE_DEBUG_MSG((nep, MAC_CTL,
+				"nxge_setup_multicast: Promisc flag set"));
+		nep->if_flags |= PROMISC_F;
+	}
+	if (nep->dev->flags & IFF_ALLMULTI) {
+		NXGE_DEBUG_MSG((nep, MAC_CTL,
+				"nxge_setup_multicast: Allmulti flag set"));
+		nep->if_flags |= ALLMULTI_F;
+		data = 0xFFFF;
+		for (i = 0; i < MAC_MAX_HASH_ENTRY; i++) {
+			nep->hash_table[i] = data;
+		}
+	}
+	{
+		/* use hw hash table for the next series of
+		   multicast addresses */
+		memset(nep->hash_table, 0, sizeof(nep->hash_table));
+		while (dmi) {
+ 			crc = ether_crc_le(ETH_ALEN, dmi->dmi_addr);
+			crc >>= 24;
+			nep->hash_table[crc >> 4] |= 1 << (15 - (crc & 0xf));
+			dmi = dmi->next;
+		}
+		NXGE_DEBUG_MSG((nep, MAC_CTL, "nxge_setup_multicast: "
+				"Hashmatch flag set"));
+		nep->if_flags |= HASHMATCH_F;
+	}
+
+	NXGE_DEBUG_MSG((nep, MAC_CTL, "<== nxge_setup_multicast"));
+
+	return 0;
+}
+
+
+#ifdef CONFIG_NET_POLL_CONTROLLER
+static void nxge_netpoll(struct net_device *dev)
+{
+}
+#endif
+
+
+
+#define NXGE_TXC_DRAIN_MAX_ITER	50
+#define NXGE_TXC_DRAIN_CHK_INT	1
+
+static int nxge_change_mtu(struct net_device *dev, int new_mtu)
+{
+	p_nxge_t nep = netdev_priv(dev);
+	int ring;
+	uint64_t burst;
+	int i;
+	uint32_t cnt1, cnt2, oldcnt1, oldcnt2;
+	boolean_t was_jumbo = nep->mac.is_jumbo;
+
+	NXGE_DEBUG_MSG((nep, MAC_CTL, "==> nxge_change_mtu"));
+
+	dev->mtu = new_mtu;
+	if (!netif_running(dev) || !netif_device_present(dev)) {
+	  goto nxge_change_mtu_exit;
+	}
+
+	if (new_mtu < NXGE_MIN_MTU || new_mtu > NXGE_MAX_MTU)
+		return -EINVAL;
+	if ((new_mtu > 1500) && (nxge_jumbo_enable  == B_FALSE))
+		return -EINVAL;
+	if (new_mtu > 1500)
+		nep->mac.is_jumbo = B_TRUE;
+	else
+		nep->mac.is_jumbo = B_FALSE;
+
+
+	if (was_jumbo && !nep->mac.is_jumbo) {
+		/* stop the tx queue, drain the tx descriptors */
+		netif_stop_queue(dev);
+		npi_txc_pkt_xmt_to_mac_get(nep->npi_handle,
+					   nep->function_num,
+					   &cnt1, &cnt2);
+		oldcnt1 = cnt1;
+		oldcnt2 = cnt2;
+		for (i = 0; i < NXGE_TXC_DRAIN_MAX_ITER; i++) {
+			mdelay(NXGE_TXC_DRAIN_CHK_INT);
+			npi_txc_pkt_xmt_to_mac_get(nep->npi_handle,
+						   nep->function_num,
+						   &cnt1, &cnt2);
+			if (cnt1 == oldcnt1 && cnt2 == oldcnt2)
+				break;
+			oldcnt1 = cnt1;
+			oldcnt2 = cnt2;
+		}
+	}
+
+	nep->tx_copy_udp = B_TRUE;
+
+	if ((nep->mac.is_jumbo == B_FALSE) &&
+		((nep->mac.portmode == PORT_10G_FIBER) ||
+		 (nep->mac.portmode == PORT_10G_COPPER))) {
+		nep->tx_copy_udp = B_FALSE;
+	}
+
+	if (new_mtu > 1500)
+		burst = 4096;
+	else
+		burst = new_mtu + 0x20; /* per Arvind */
+
+	for (ring = 0; ring < nep->ntdc; ring++) {
+		TXC_FZC_REG_WRITE64(nep->npi_handle,
+			TXC_DMA_MAX_BURST_REG, nep->tdc[ring], burst);
+	}
+
+	nxge_mac_init(nep);
+
+	if (netif_queue_stopped(dev)) {
+		netif_wake_queue(dev);
+	}
+
+nxge_change_mtu_exit:
+
+	NXGE_DEBUG_MSG((nep, MAC_CTL, "<== nxge_change_mtu"));
+	return 0;
+}
+
+/****************************************************************************
+ *
+ * CONFIG related functions.
+ *
+ ***************************************************************************/
+
+
+boolean_t
+nxge_check_txdma_port_member(p_nxge_t nxgep, uint8_t tdc)
+{
+
+	p_nxge_dma_pt_cfg_t	p_dma_cfgp;
+	p_nxge_hw_pt_cfg_t	p_cfgp;
+	int status = B_FALSE;
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_check_rxdma_port_member"));
+
+	p_dma_cfgp = (p_nxge_dma_pt_cfg_t)&nxgep->pt_config;
+	p_cfgp = (p_nxge_hw_pt_cfg_t)&p_dma_cfgp->hw_config;
+
+
+	if (tdc < p_cfgp->max_tdcs)
+		status = B_TRUE;
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, " <== nxge_check_rxdma_port_member"));
+		return (status);
+
+}
+
+
+/*ARGSUSED*/
+boolean_t
+nxge_check_rxdma_rdcgrp_member(p_nxge_t nxgep, uint8_t rdc_grp, uint8_t rdc)
+{
+	p_nxge_dma_pt_cfg_t	p_dma_cfgp;
+	int status = B_TRUE;
+	p_nxge_rdc_grp_t	rdc_grp_p;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL,
+			    " ==> nxge_check_rxdma_rdcgrp_member"));
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "  nxge_check_rxdma_rdcgrp_member"
+			    " rdc  %d group %d",
+			    rdc, rdc_grp));
+	p_dma_cfgp = (p_nxge_dma_pt_cfg_t)&nxgep->pt_config;
+
+	rdc_grp_p = &p_dma_cfgp->rdc_grps[rdc_grp];
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "  max  %d ",
+			    rdc_grp_p->max_rdcs));
+	if (rdc >= rdc_grp_p->max_rdcs) {
+		status = B_FALSE;
+	}
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL,
+			    " <== nxge_check_rxdma_rdcgrp_member"));
+	return (status);
+}
+
+
+static void nxge_get_default_sprom_config(p_nxge_t nep)
+{
+	int portn = nep->mac.portnum;
+
+/* set the mac structure */
+
+	if (portn == 0 || portn == 1) {
+		nep->mac.porttype = PORT_TYPE_XMAC;
+		nep->mac.portmode = PORT_10G_FIBER;
+		nep->statsp->mac_stats.xcvr_inuse = PCS_XCVR;
+	} else {
+		nep->mac.porttype = PORT_TYPE_BMAC;
+		nep->mac.portmode = PORT_1G_COPPER;
+		nep->statsp->mac_stats.xcvr_inuse = INT_MII_XCVR;
+	}
+	nep->dev->dev_addr[0] = 0x00;
+	nep->dev->dev_addr[1] = 0x09;
+	nep->dev->dev_addr[2] = 0x3d;
+	get_random_bytes(&nep->dev->dev_addr[3], 3);
+
+	memcpy(nep->mac.mac_addr.ether_addr_octet, nep->dev->dev_addr,
+	       ETH_ALEN);
+}
+
+static void
+nxge_dump_config_info(p_nxge_t nxgep)
+{
+	int i;
+
+	if (nxge_debug_init == 0) {
+		MUTEX_INIT(&nxgedebuglock, NULL, MUTEX_DRIVER, NULL);
+		nxge_debug_init = 1;
+	}
+
+	MUTEX_ENTER(&nxgedebuglock);
+
+	printk(KERN_INFO "DUMPING Config info\n\n");
+	printk(KERN_INFO "Port number [%d] Num ports [%d]\n",
+	       nxgep->mac.portnum, nxgep->nports);
+
+	switch (nxgep->mac.porttype) {
+	case PORT_TYPE_XMAC:
+		printk(KERN_INFO "Port type XMAC\n");
+		break;
+	case PORT_TYPE_BMAC:
+		printk(KERN_INFO "Port type BMAC\n");
+		break;
+	default:
+		printk(KERN_INFO "Unknown Port type [%d]\n",
+		       nxgep->mac.porttype);
+		break;
+	}
+
+	switch (nxgep->mac.portmode) {
+	case PORT_10G_FIBER:
+		printk(KERN_INFO "Port mode 10G_FIBER\n");
+		break;
+	case PORT_10G_COPPER:
+		printk(KERN_INFO "Port mode 10G_COPPER\n");
+		break;
+	case PORT_10G_SERDES:
+		printk(KERN_INFO "Port mode 10G_SERDES\n");
+		break;
+	case PORT_1G_FIBER:
+		printk(KERN_INFO "Port mode 1G_FIBER\n");
+		break;
+	case PORT_1G_SERDES:
+		printk(KERN_INFO "Port mode 1G_SERDES\n");
+		break;
+	case PORT_1G_COPPER:
+		printk(KERN_INFO "Port mode 1G_COPPER\n");
+		break;
+	case PORT_1G_RGMII_FIBER:
+		printk(KERN_INFO "Port mode 1G_RGMII_FIBER\n");
+		break;
+	default:
+		printk(KERN_INFO "Unknown Port mode [%d]\n",
+		       nxgep->mac.portmode);
+		break;
+	}
+
+	switch (nxgep->statsp->mac_stats.xcvr_inuse) {
+	case PCS_XCVR:
+		printk(KERN_INFO "Xcvr type PCS\n");
+		break;
+	case XPCS_XCVR:
+		printk(KERN_INFO "Xcvr type XPCS\n");
+		break;
+	case INT_MII_XCVR:
+		printk(KERN_INFO "Xcvr type Int_MII\n");
+		break;
+	default:
+		printk(KERN_INFO "unknown Xcvr type [%d]\n",
+		       nxgep->statsp->mac_stats.xcvr_inuse);
+		break;
+	}
+
+	printk(KERN_INFO "MAC address: ");
+	for (i = 0; i < 6; i++)
+		printk("%2.2x%c",
+		       nxgep->mac.mac_addr.ether_addr_octet[i],
+		       i == 5 ? ' ' : ':');
+	printk("\n");
+
+	printk(KERN_INFO "Model [%s]\n", nxgep->model);
+	printk(KERN_INFO "Board Model [%s]\n", nxgep->bd_model);
+
+	MUTEX_EXIT(&nxgedebuglock);
+
+}
+
+
+static int
+nxge_get_sprom_config(p_nxge_t nxgep)
+{
+	int status = NXGE_OK;
+	npi_handle_t	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	npi_status_t	npi_stat = NPI_SUCCESS;
+
+	/*
+	 * Verify the SEEPROM checksum
+	 */
+	if (nxge_espc_verify_chksum(nxgep) != NXGE_OK) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"Invalid SEEPROM checksum"));
+		status = NXGE_ERROR;
+		goto exit;
+	}
+
+	/* this call sets mac.portmode and mac_stats.xcvr_inuse */
+	status = nxge_espc_phy_type_get(nxgep);
+	if (status != NXGE_OK) {
+		goto exit;
+	}
+
+	status = nxge_espc_mac_addrs_get(nxgep);
+	if (status != NXGE_OK) {
+		goto exit;
+	}
+
+	memcpy(nxgep->dev->dev_addr, nxgep->mac.mac_addr.ether_addr_octet,
+	       ETH_ALEN);
+
+	npi_espc_num_ports_get(handle, &nxgep->nports);
+
+	npi_stat = npi_espc_model_str_get(handle, nxgep->model);
+	if (npi_stat != NPI_SUCCESS) {
+		status = NXGE_ERROR;
+		goto exit;
+	}
+
+	npi_stat = npi_espc_bd_model_str_get(handle, nxgep->bd_model);
+	if (npi_stat != NPI_SUCCESS) {
+		status = NXGE_ERROR;
+		goto exit;
+	}
+#ifdef NXGE_DEBUG
+	/* For debug purpose only */
+	nxge_dump_config_info(nxgep);
+#endif
+  exit:
+	return (status);
+}
+
+static int
+nxge_get_eeprom_config(p_nxge_t nxgep)
+{
+	int i;
+	char *phy_type;
+
+	if (nxge_vpd_info_get(nxgep) != NXGE_OK) {
+		return (NXGE_ERROR);
+	}
+
+	if (!nxgep->vpd_info.ver_valid)
+		return (NXGE_ERROR);
+
+	if(!is_valid_ether_addr(nxgep->vpd_info.mac_addr)) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"Invalid MAC address in EEPROM: "));
+		for (i = 0; i < ETHERADDRL; i++)
+			printk("%2.2x%c", nxgep->vpd_info.mac_addr[i],
+			       i == 5 ? ' ' : ':');
+		printk("\n");
+		return (NXGE_ERROR);
+	}
+	nxge_espc_get_next_mac_addr(nxgep->vpd_info.mac_addr,
+				    nxgep->mac.portnum,
+				    &nxgep->mac.mac_addr);
+
+	memcpy(nxgep->dev->dev_addr, nxgep->mac.mac_addr.ether_addr_octet,
+	       ETH_ALEN);
+
+
+	phy_type = nxgep->vpd_info.phy_type;
+	if (strncmp(nxgep->vpd_info.bd_model, NXGE_ALONSO_BM_STR,
+		    strlen(NXGE_ALONSO_BM_STR)) == 0) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+						"Adjusting phy type for Alonso"));
+		switch (nxgep->mac.porttype) {
+			case PORT_TYPE_BMAC:
+				phy_type[0] = 'm';
+				phy_type[1] = 'i';
+				phy_type[2] = 'f';
+				phy_type[3] = '\0';
+				break;
+			case PORT_TYPE_XMAC:
+				if ((nxgep->mac.portnum == 1) &&
+					(nxge_atca_port_mode & NXGE_ATCA_PORT1_1G)) {
+					phy_type[0] = 'g';
+					phy_type[1] = 's';
+					phy_type[2] = 'd';
+					phy_type[3] = '\0';
+				}
+				if ((nxgep->mac.portnum == 0) &&
+					(nxge_atca_port_mode & NXGE_ATCA_PORT0_1G)) {
+					phy_type[0] = 'g';
+					phy_type[1] = 's';
+					phy_type[2] = 'd';
+					phy_type[3] = '\0';
+				}
+
+				break;
+		}
+	}
+
+	if (phy_type[0] == 'm' && phy_type[1] == 'i' && phy_type[2] == 'f') {
+		if (strncmp(nxgep->vpd_info.bd_model, NXGE_ALONSO_BM_STR,
+		    strlen(NXGE_ALONSO_BM_STR)) == 0) {
+			nxgep->mac.portmode = PORT_1G_RGMII_FIBER;
+		} else {
+			nxgep->mac.portmode = PORT_1G_COPPER;
+		}
+		nxgep->statsp->mac_stats.xcvr_inuse = INT_MII_XCVR;
+
+	} else if (phy_type[0] == 'x' && phy_type[1] == 'g' &&
+		   phy_type[2] == 'f') {
+		nxgep->mac.portmode = PORT_10G_FIBER;
+		nxgep->statsp->mac_stats.xcvr_inuse = XPCS_XCVR;
+	} else if (phy_type[0] == 'p' && phy_type[1] == 'c' &&
+		   phy_type[2] == 's') {
+		nxgep->mac.portmode = PORT_1G_FIBER;
+		nxgep->statsp->mac_stats.xcvr_inuse = PCS_XCVR;
+	} else if (phy_type[0] == 'x' && phy_type[1] == 'g' &&
+		   phy_type[2] == 'c') {
+		nxgep->mac.portmode = PORT_10G_COPPER;
+		nxgep->statsp->mac_stats.xcvr_inuse = XPCS_XCVR;
+	} else if (phy_type[0] == 'x' && phy_type[1] == 'g' &&
+		   phy_type[2] == 's' && phy_type[3] == 'd') {
+		nxgep->mac.portmode = PORT_10G_SERDES;
+		nxgep->statsp->mac_stats.xcvr_inuse = XPCS_XCVR;
+	} else if (phy_type[0] == 'g' && phy_type[1] == 's' &&
+		   phy_type[2] == 'd') {
+		nxgep->mac.portmode = PORT_1G_SERDES;
+		nxgep->statsp->mac_stats.xcvr_inuse = PCS_XCVR;
+	}else {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"nxge_get_eeprom_config: "
+				"Unknown phy type [%c%c%c] in EEPROM",
+				phy_type[0], phy_type[1], phy_type[2]));
+		return (NXGE_ERROR);
+	}
+
+	if ((strncmp(nxgep->vpd_info.bd_model, NXGE_QGC_LP_BM_STR,
+		     strlen(NXGE_QGC_LP_BM_STR)) == 0) ||
+	    (strncmp(nxgep->vpd_info.bd_model, NXGE_QGC_PEM_BM_STR,
+		     strlen(NXGE_QGC_PEM_BM_STR)) == 0) ||
+	    (strncmp(nxgep->vpd_info.bd_model, NXGE_ALONSO_BM_STR,
+		    strlen(NXGE_ALONSO_BM_STR)) == 0)) {
+		nxgep->nports = NXGE_NUM_OF_PORTS_QUAD;
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+		    "nxge_get_eeprom_config: model %s bd_modle %s",
+		    nxgep->vpd_info.model, nxgep->vpd_info.bd_model ));
+	} else if ((strncmp(nxgep->vpd_info.bd_model, NXGE_2XGF_LP_BM_STR,
+			    strlen(NXGE_2XGF_LP_BM_STR)) == 0) ||
+		   (strncmp(nxgep->vpd_info.bd_model, NXGE_2XGF_PEM_BM_STR,
+			    strlen(NXGE_2XGF_PEM_BM_STR)) == 0)) {
+		nxgep->nports = NXGE_NUM_OF_PORTS_DUAL;
+	} else {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"nxge_get_eeprom_config: port num not set "
+				"in EEPROM"));
+		return (NXGE_ERROR);
+	}
+
+	if (nxgep->nports == NXGE_NUM_OF_PORTS_DUAL)
+		nxgep->niu_type = NEPTUNE_2;
+	else if (nxgep->nports == NXGE_NUM_OF_PORTS_QUAD)
+		nxgep->niu_type = NEPTUNE;
+
+	strcpy(nxgep->model, nxgep->vpd_info.model);
+	strcpy(nxgep->bd_model, nxgep->vpd_info.bd_model);
+
+	nxge_dump_config_info(nxgep);
+
+	return (NXGE_OK);
+}
+
+static void
+nxge_setup_hw_pciconfig(p_nxge_t nxgep)
+{
+
+}
+
+static void
+nxge_setup_hw_vpd_rom_mac(p_nxge_t nxgep)
+{
+
+}
+
+
+static void
+nxge_set_hw_class_config(p_nxge_t nxgep)
+{
+	int			i;
+	p_nxge_param_t param_arr;
+	uint32_t cfg_value;
+	p_nxge_class_pt_cfg_t 	p_class_cfgp;
+	int start_prop, end_prop;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, " ==> nxge_set_hw_class_config"));
+
+	p_class_cfgp = (p_nxge_class_pt_cfg_t)&nxgep->class_config;
+
+	param_arr = nxgep->param_arr;
+
+	start_prop =  param_class_opt_ip_usr4;
+	end_prop = param_class_opt_ipv6_sctp;
+
+	for (i = start_prop; i <= end_prop; i++) {
+		cfg_value = (uint32_t)param_arr[i].value;
+		p_class_cfgp->class_cfg[i - start_prop] = cfg_value;
+	}
+
+	if (NXGE_IS_PARAM_CONFIG(nxgep, param_h1_init_value)) {
+		cfg_value = (uint32_t)param_arr[param_h1_init_value].conf_value;
+	} else {
+		cfg_value = (uint32_t)param_arr[param_h1_init_value].value;
+	}
+
+	p_class_cfgp->init_h1 = (uint32_t)cfg_value;
+
+	if (NXGE_IS_PARAM_CONFIG(nxgep, param_h2_init_value)) {
+		cfg_value = (uint32_t)param_arr[param_h2_init_value].conf_value;
+	} else {
+		cfg_value = (uint32_t)param_arr[param_h2_init_value].value;
+	}
+
+	p_class_cfgp->init_h2 = (uint16_t)cfg_value;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, " <== nxge_set_hw_class_config"));
+}
+
+static void
+nxge_use_cfg_class_config(p_nxge_t nxgep)
+{
+	nxge_set_hw_class_config(nxgep);
+}
+
+static void
+nxge_set_hw_mac_class_config(p_nxge_t nxgep)
+{
+	int			i;
+	p_nxge_dma_pt_cfg_t	p_dma_cfgp;
+	p_nxge_hw_pt_cfg_t	p_cfgp;
+	p_nxge_param_t param_arr;
+	uint_t mac_cnt;
+	int *mac_cfg_val;
+	nxge_param_map_t *mac_map;
+	p_nxge_class_pt_cfg_t 	p_class_cfgp;
+	int good_count = 0;
+	int good_cfg[NXGE_MAX_MACS];
+	nxge_mv_cfg_t	*mac_host_info;
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_set_hw_mac_config"));
+
+	p_dma_cfgp = (p_nxge_dma_pt_cfg_t)&nxgep->pt_config;
+	p_cfgp = (p_nxge_hw_pt_cfg_t)&p_dma_cfgp->hw_config;
+	p_class_cfgp = (p_nxge_class_pt_cfg_t)&nxgep->class_config;
+	mac_host_info = (nxge_mv_cfg_t	*)&p_class_cfgp->mac_host_info[0];
+
+	param_arr = nxgep->param_arr;
+
+	for (i = 0; i < NXGE_MAX_MACS; i++) {
+		p_class_cfgp->mac_host_info[i].flag = 0;
+	}
+
+	nxge_get_param_int_arr(param_mac_2rdc_grp, &mac_cnt, &mac_cfg_val);
+	for (i = 0; i < mac_cnt; i++) {
+		mac_map = (nxge_param_map_t *)&mac_cfg_val[i];
+		if ((mac_map->param_id < p_cfgp->max_macs) &&
+		    (mac_map->map_to < p_cfgp->max_rdc_grpids)) {
+			NXGE_DEBUG_MSG((nxgep, CFG_CTL,
+					" nxge_mac_config mapping"
+					" id %d grp %d",
+					mac_map->param_id,
+					mac_map->map_to));
+
+			mac_host_info[mac_map->param_id].mpr_npr =
+				mac_map->pref;
+			mac_host_info[mac_map->param_id].rdctbl =
+				mac_map->map_to +
+				p_cfgp->start_rdc_grpid;
+			good_cfg[good_count] = mac_cfg_val[i];
+			if (mac_host_info[mac_map->param_id].flag == 0)
+				good_count++;
+			mac_host_info[mac_map->param_id].flag = 1;
+		}
+	}
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, " <== nxge_set_hw_mac_config"));
+}
+
+static void
+nxge_use_cfg_mac_class_config(p_nxge_t nxgep)
+{
+	p_nxge_dma_pt_cfg_t	p_dma_cfgp;
+	p_nxge_hw_pt_cfg_t	p_cfgp;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_use_default_mac_config"));
+	p_dma_cfgp = (p_nxge_dma_pt_cfg_t)&nxgep->pt_config;
+	p_cfgp = (p_nxge_hw_pt_cfg_t)&p_dma_cfgp->hw_config;
+	p_cfgp->start_mac_entry = 0;
+
+	switch (nxgep->function_num) {
+	case 0:
+	case 1:
+		/* 10G ports */
+		p_cfgp->max_macs = NXGE_MAX_MACS_XMACS;
+		break;
+	case 2:
+	case 3:
+		/* 1G ports */
+	default:
+		p_cfgp->max_macs = NXGE_MAX_MACS_BMACS;
+		break;
+	}
+
+	p_cfgp->mac_pref = 1;
+	p_cfgp->def_mac_rxdma_grpid = p_cfgp->start_rdc_grpid;
+
+	nxge_set_hw_mac_class_config(nxgep);
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, " <== nxge_use_default_mac_config"));
+}
+
+static void
+nxge_set_hw_vlan_class_config(p_nxge_t nxgep)
+{
+	int			i;
+	p_nxge_dma_pt_cfg_t	p_dma_cfgp;
+	p_nxge_hw_pt_cfg_t	p_cfgp;
+	p_nxge_param_t param_arr;
+	uint_t vlan_cnt;
+	int *vlan_cfg_val;
+	nxge_param_map_t *vmap;
+	p_nxge_class_pt_cfg_t 	p_class_cfgp;
+	uint32_t good_cfg[32];
+	int good_count = 0;
+	nxge_mv_cfg_t	*vlan_tbl;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, " ==> nxge_set_hw_vlan_config"));
+	p_dma_cfgp = (p_nxge_dma_pt_cfg_t)&nxgep->pt_config;
+	p_cfgp = (p_nxge_hw_pt_cfg_t)&p_dma_cfgp->hw_config;
+	p_class_cfgp = (p_nxge_class_pt_cfg_t)&nxgep->class_config;
+
+	param_arr = nxgep->param_arr;
+
+		/*
+		 * By default, VLAN to RDC group mapping is disabled
+		 * Need to read HW or .conf properties to find out
+		 * if mapping is required
+		 *
+		 * Format
+		 *
+		 * uint32_t array, each array entry specifying the
+		 * VLAN id and the mapping
+		 *
+		 * bit[30] = add
+		 * bit[29] = remove
+		 * bit[28]  = preference
+		 * bits[23-16] = rdcgrp
+		 * bits[15-0] = VLAN ID ( )
+		 */
+	for (i = 0; i < NXGE_MAX_VLANS; i++) {
+		p_class_cfgp->vlan_tbl[i].flag = 0;
+	}
+
+	vlan_tbl = (nxge_mv_cfg_t *)&p_class_cfgp->vlan_tbl[0];
+
+	nxge_get_param_int_arr(param_vlan_2rdc_grp, &vlan_cnt, &vlan_cfg_val);
+	for (i = 0; i < vlan_cnt; i++) {
+		vmap = (nxge_param_map_t *)&vlan_cfg_val[i];
+		if ((vmap->param_id) &&
+		    (vmap->param_id < NXGE_MAX_VLANS) &&
+		    (vmap->map_to < p_cfgp->max_rdc_grpids)) {
+			NXGE_DEBUG_MSG((nxgep, CFG_CTL,
+					" nxge_vlan_config mapping"
+					" id %d grp %d",
+					vmap->param_id, vmap->map_to));
+
+			good_cfg[good_count] = vlan_cfg_val[i];
+			if (vlan_tbl[vmap->param_id].flag == 0)
+				good_count++;
+			vlan_tbl[vmap->param_id].flag = 1;
+			vlan_tbl[vmap->param_id].rdctbl =
+				vmap->map_to + p_cfgp->start_rdc_grpid;
+			vlan_tbl[vmap->param_id].mpr_npr = vmap->pref;
+
+		}
+	}
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, " <== nxge_set_hw_vlan_config"));
+}
+
+static void
+nxge_use_cfg_vlan_class_config(p_nxge_t nxgep)
+{
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, " ==> nxge_use_cfg_vlan_config"));
+	nxge_set_hw_vlan_class_config(nxgep);
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, " <== nxge_use_cfg_vlan_config"));
+
+}
+
+static void
+nxge_set_rdc_intr_property(p_nxge_t nxgep)
+{
+	int			i;
+	p_nxge_dma_pt_cfg_t	p_dma_cfgp;
+	p_nxge_hw_pt_cfg_t	p_cfgp;
+	p_nxge_param_t param_arr;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, " => nxge_set_rdc_intr_property"));
+	p_dma_cfgp = (p_nxge_dma_pt_cfg_t)&nxgep->pt_config;
+
+	for (i = 0; i < NXGE_MAX_RDCS; i++) {
+		p_dma_cfgp->rcr_timeout[i] = nxge_rcr_timeout;
+		p_dma_cfgp->rcr_threshold[i] = nxge_rcr_threshold;
+	}
+
+	p_cfgp = (p_nxge_hw_pt_cfg_t)&p_dma_cfgp->hw_config;
+	param_arr = nxgep->param_arr;
+
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, " <== nxge_set_rdc_intr_property"));
+}
+
+static void
+nxge_set_hw_dma_config(p_nxge_t nxgep)
+{
+	int			i, j, rdc, ndmas, ngrps, bitmap, end, st_rdc;
+	int32_t			rdcs_per_grp;
+	p_nxge_dma_pt_cfg_t	p_dma_cfgp;
+	p_nxge_hw_pt_cfg_t	p_cfgp;
+	p_nxge_rdc_grp_t	rdc_grp_p;
+	int 			rdcgrp_cfg = CFG_NOT_SPECIFIED;
+	int			rx_quick_cfg;
+	p_nxge_param_t 		param_arr;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_set_hw_dma_config"));
+
+	p_dma_cfgp = (p_nxge_dma_pt_cfg_t)&nxgep->pt_config;
+	p_cfgp = (p_nxge_hw_pt_cfg_t)&p_dma_cfgp->hw_config;
+
+	rdc_grp_p = p_dma_cfgp->rdc_grps;
+
+	/* Transmit DMA Channels */
+	bitmap = 0;
+	end = p_cfgp->start_tdc + p_cfgp->max_tdcs;
+	nxgep->ntdc = p_cfgp->max_tdcs;
+	p_dma_cfgp->tx_dma_map = 0;
+	for (i = p_cfgp->start_tdc; i < end; i++) {
+		bitmap |= (1 << i);
+		nxgep->tdc[i - p_cfgp->start_tdc] = i;
+	}
+
+	p_dma_cfgp->tx_dma_map = bitmap;
+
+	param_arr = nxgep->param_arr;
+
+	rx_quick_cfg = param_arr[param_rx_qcfg_type].value;
+	switch (rx_quick_cfg) {
+		case CFG_NOT_SPECIFIED:
+			rdcgrp_cfg = CFG_L3_DISTRIBUTE;
+			break;
+		case CFG_L3_WEB:
+		case CFG_L3_DISTRIBUTE:
+		case CFG_L2_CLASSIFY:
+		case CFG_L3_TCAM:
+			rdcgrp_cfg = rx_quick_cfg;
+			break;
+		default:
+			rdcgrp_cfg = CFG_L3_DISTRIBUTE;
+			break;
+	}
+
+	/* Receive DMA Channels */
+	st_rdc = p_cfgp->start_rdc;
+	nxgep->nrdc = p_cfgp->max_rdcs;
+	for (i = 0; i < p_cfgp->max_rdcs; i++)
+		nxgep->rdc[i] = i + p_cfgp->start_rdc;
+
+	switch (rdcgrp_cfg) {
+		case CFG_L3_DISTRIBUTE:
+		case CFG_L3_WEB:
+		case CFG_L3_TCAM:
+			ndmas = p_cfgp->max_rdcs;
+			ngrps = 1;
+			rdcs_per_grp = ndmas/ngrps;
+			break;
+		case CFG_L2_CLASSIFY:
+			ndmas = p_cfgp->max_rdcs / 2;
+			if (p_cfgp->max_rdcs < 2)
+				ndmas = 1;
+			ngrps = 1;
+			rdcs_per_grp = ndmas/ngrps;
+			break;
+		default:
+			ngrps = p_cfgp->max_rdc_grpids;
+			ndmas = p_cfgp->max_rdcs;
+			rdcs_per_grp = ndmas/ngrps;
+			break;
+	}
+
+	/* Assume RDCs are evenly distributed */
+	for (i = 0; i < ngrps; i++) {
+		rdc_grp_p = &p_dma_cfgp->rdc_grps[i];
+		rdc_grp_p->start_rdc = st_rdc + i * rdcs_per_grp;
+		rdc_grp_p->max_rdcs = rdcs_per_grp;
+
+		/* default to: 0, 1, 2, 3, ...., 0, 1, 2, 3.... */
+		rdc_grp_p->config_method = RDC_TABLE_ENTRY_METHOD_SEQ;
+		rdc = rdc_grp_p->start_rdc;
+		for (j = 0; j < NXGE_MAX_RDCS; j++) {
+			rdc_grp_p->rdc[j] = rdc++;
+			if (rdc == (rdc_grp_p->start_rdc + rdcs_per_grp)) {
+				rdc = rdc_grp_p->start_rdc;
+			}
+		}
+		rdc_grp_p->def_rdc = rdc_grp_p->rdc[0];
+		rdc_grp_p->flag = 1;		/* configured */
+/* 		rdc_grp_p++; */
+	}
+
+	/* default RDC */
+/* 	p_cfgp->def_rdc = p_all_cfgp->rdc_grps[grp_tbl_id].start_rdc; */
+	p_cfgp->def_rdc = p_cfgp->start_rdc;
+
+	nxgep->def_rdc = p_cfgp->start_rdc;
+
+	/* full 18 byte header ?  2 byte enabled by default */
+	if (NXGE_IS_PARAM_CONFIG(nxgep, param_rx_full_header))
+		p_dma_cfgp->rcr_full_header = param_arr[param_rx_full_header].conf_value;
+	else
+		p_dma_cfgp->rcr_full_header = NXGE_RCR_FULL_HEADER;
+
+
+	if (NXGE_IS_PARAM_CONFIG(nxgep, param_rx_drr_weight)) {
+		p_dma_cfgp->rx_drr_weight = param_arr[param_rx_drr_weight].conf_value;
+	} else {
+		p_dma_cfgp->rx_drr_weight = PT_DRR_WT_DEFAULT_10G;
+		if (nxgep->mac.portnum == 4)
+			p_dma_cfgp->rx_drr_weight = PT_DRR_WT_DEFAULT_1G;
+
+	}
+
+	if (NXGE_IS_PARAM_CONFIG(nxgep, param_rx_rbr_size)) {
+		p_dma_cfgp->rbr_size = param_arr[param_rx_rbr_size].conf_value;
+	} else {
+		p_dma_cfgp->rbr_size = nxge_rbr_size;
+	}
+
+	if (NXGE_IS_PARAM_CONFIG(nxgep, param_rx_rcr_size)) {
+		p_dma_cfgp->rcr_size = param_arr[param_rx_rcr_size].conf_value;
+	} else {
+		p_dma_cfgp->rcr_size = nxge_rcr_size;
+	}
+
+	nxge_set_rdc_intr_property(nxgep);
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, " <== nxge_set_hw_dma_config"));
+
+}
+
+
+static void
+nxge_use_cfg_dma_config(p_nxge_t nxgep)
+{
+
+	nxge_cfg_set_dma_cfg(nxgep);
+
+	nxge_set_hw_dma_config(nxgep);
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_use_cfg_dma_config"));
+}
+
+
+static void
+nxge_use_cfg_neptune_properties(p_nxge_t nxgep)
+{
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL,
+			"==> nxge_use_cfg_neptune_properties"));
+
+	nxge_use_cfg_dma_config(nxgep);
+	nxge_use_cfg_vlan_class_config(nxgep);
+	nxge_use_cfg_mac_class_config(nxgep);
+	nxge_use_cfg_class_config(nxgep);
+
+
+	/* Setup the PCI related configuration */
+	nxge_setup_hw_pciconfig(nxgep);
+
+	/* Setup the VPD, expansion ROM, or MAC addresses configuration */
+	nxge_setup_hw_vpd_rom_mac(nxgep);
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL,
+			"<== nxge_use_cfg_neptune_properties"));
+}
+
+
+static int
+nxge_get_config_properties(p_nxge_t nxgep)
+{
+	int 			status = NXGE_OK;
+	int portn = NXGE_GET_PORT_NUM(nxgep->function_num);
+
+	/*
+	 * Assume function 0 (port 0) and uses group 0 and 1.
+	 */
+	NXGE_DEBUG_MSG((nxgep, MOD_CTL, " ==> nxge_get_config_properties"));
+
+	nxgep->mac.portnum = portn;
+	nxgep->flags = 0;
+
+	if (portn == 0 || portn == 1) {
+		nxgep->mac.porttype = PORT_TYPE_XMAC;
+	} else {
+		nxgep->mac.porttype = PORT_TYPE_BMAC;
+	}
+
+	/*
+	 * First get the hw configuration from the EEPROM.
+	 */
+	if (nxge_get_eeprom_config(nxgep) != NXGE_OK) {
+		/*
+		 * Get the configurations stored in the
+		 * SEEPROM via the serial prom controller (SPC).
+		 * Hardware properties such as MAC addresses,
+		 * phy type and # of ports.
+		 */
+		if (nxge_get_sprom_config(nxgep) != NXGE_OK) {
+				NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+						"Please update the EEPROM "
+						"on this card"));
+			return (NXGE_ERROR);
+		}
+	}
+
+
+	nxgep->classifier.tcam_size = TCAM_NXGE_TCAM_MAX_ENTRY;
+
+	status = nxge_cfg_set_quick_config(nxgep);
+	nxge_use_cfg_neptune_properties(nxgep);
+
+	NXGE_DEBUG_MSG((nxgep, MOD_CTL, " <== nxge_get_config_properties"));
+	return (status);
+}
+
+
+/***************************************************************************/
+static struct net_device_stats *nxge_get_netdev_stats(struct net_device *dev)
+{
+	p_nxge_t nep = netdev_priv(dev);
+	unsigned long flags;
+	struct net_device_stats *statsp = &nep->net_stats;
+	nxge_port_t portt;
+	int dma_count, dma_index;
+	p_nxge_rx_ring_stats_t	rdc_stats;
+	p_nxge_tx_ring_stats_t tdc_stats;
+	uint64_t packets, bytes;
+	uint32_t perrors, dropped;
+
+	NXGE_DEBUG_MSG((nep, MOD_CTL, "==> nxge_get_netdev_stats, "
+			"refcnt=%d", dev->refcnt));
+
+	if (nep->statsp == NULL || !nep->hw_running)
+		return (statsp);
+
+	portt = nep->mac.porttype;
+
+	MUTEX_ENTER_INT(&nep->stats_lock, flags);
+	dma_count = nep->nrdc;
+	bytes = packets = 0;
+	perrors = dropped = 0;
+	for (dma_index = 0; dma_index < dma_count; dma_index++) {
+		rdc_stats = &nep->statsp->rdc_stats[dma_index];
+		packets += rdc_stats->ipackets;
+		bytes += rdc_stats->ibytes;
+		perrors = rdc_stats->ierrors;
+		dropped += rdc_stats->port_drop_pkt;
+	}
+	statsp->rx_packets = packets;
+	statsp->rx_bytes = bytes;
+	statsp->rx_errors = perrors;
+	statsp->rx_dropped = dropped;
+
+	bytes = packets = 0;
+	perrors = dropped = 0;
+	dma_count = nep->ntdc;
+	for (dma_index = 0; dma_index < dma_count; dma_index++) {
+		tdc_stats = &nep->statsp->tdc_stats[dma_index];
+		packets += tdc_stats->opackets;
+		bytes += tdc_stats->obytes;
+		perrors += tdc_stats->oerrors;
+	}
+	statsp->tx_packets = packets;
+	statsp->tx_bytes = bytes;
+	statsp->tx_errors = perrors;
+
+	statsp->multicast = nep->statsp->multircv;
+	statsp->collisions = 0;
+
+/* 	statsp->rx_over_errors = nep->statsp->; */ /* rcvr ring buff overflow */
+	if (portt == PORT_TYPE_XMAC) {
+		statsp->rx_length_errors =
+			nep->statsp->xmac_stats.rx_len_err_cnt;
+		/* recv'r fifo overrun */
+		statsp->rx_fifo_errors =
+			nep->statsp->xmac_stats.rx_overflow_err;
+		/* rcvd pkt with crc error */
+		statsp->rx_crc_errors = nep->statsp->xmac_stats.rx_crc_err_cnt;
+		/* recv'd frame alignment error */
+		statsp->rx_frame_errors =
+			nep->statsp->xmac_stats.rx_frame_align_err_cnt;
+		statsp->tx_fifo_errors =
+			nep->statsp->xmac_stats.tx_overflow_err;
+	} else {
+		statsp->rx_length_errors =
+			nep->statsp->bmac_stats.rx_len_err_cnt;
+		/* recv'r fifo overrun */
+		statsp->rx_fifo_errors =
+			nep->statsp->bmac_stats.rx_overflow_err;
+		/* rcvd pkt with crc error */
+		statsp->rx_crc_errors = nep->statsp->bmac_stats.rx_crc_err_cnt;
+		/* recv'd frame alignment error */
+		statsp->rx_frame_errors =
+			nep->statsp->bmac_stats.rx_align_err_cnt;
+/* 		statsp->tx_fifo_errors = */
+/* 			nep->statsp->xmac_stats.tx_overflow_err; */
+	}
+
+
+/* 	statsp->rx_missed_errors = nep->statsp->; */ /* receiver missed packet       */
+/* 	statsp->tx_aborted_errors = nep->statsp->; */
+/* 	statsp->tx_carrier_errors = nep->statsp->; */
+
+/* 	statsp->tx_heartbeat_errors = nep->statsp->; */
+/* 	statsp->tx_window_errors = nep->statsp->; */
+/* 	statsp-> = nep->statsp->; */
+
+	MUTEX_EXIT_INT(&nep->stats_lock, flags);
+	return (statsp);
+}
+
+/*
+ * This is the entry point that is called whenever the multicast address
+ * list or the network interface flags are updated.  This routine is
+ * responsible for configuring the hardware for proper multicast,
+ * promiscuous mode, and all-multi behavior.
+ */
+
+static void nxge_set_multicast(struct net_device *dev)
+{
+	p_nxge_t	nep = netdev_priv(dev);
+	unsigned long	flags;
+	uint32_t	rs;
+	NXGE_DEBUG_MSG((nep, MAC_CTL, "==> nxge_set_multicast"));
+
+	if (!nep->hw_running)
+		return;
+	MUTEX_ENTER_INT(&nep->lock, flags);
+
+	if ((rs = nxge_rx_mac_disable(nep)) != NXGE_OK)
+		goto nxge_set_multicast_fail;
+	/* program hash filters */
+	nxge_setup_multicast(nep);
+
+nxge_set_multicast_fail:
+	nxge_rx_mac_enable(nep);
+	MUTEX_EXIT_INT(&nep->lock, flags);
+	NXGE_DEBUG_MSG((nep, MAC_CTL, "<== nxge_set_multicast"));
+}
+
+static int nxge_set_mac_address(struct net_device *dev, void *p)
+{
+	p_nxge_t		nep = netdev_priv(dev);
+	struct sockaddr		*addr = (struct sockaddr *)p;
+
+	NXGE_DEBUG_MSG((nep, MAC_CTL, "==> nxge_set_mac_address"));
+
+	if(!is_valid_ether_addr(addr->sa_data))
+		return -EADDRNOTAVAIL;
+
+	MUTEX_ENTER(&nep->lock);
+
+	memcpy(dev->dev_addr, addr->sa_data, ETH_ALEN);
+	memcpy(nep->mac.mac_addr.ether_addr_octet, addr->sa_data, ETH_ALEN);
+
+	if (nxge_rx_mac_disable(nep) != NXGE_OK)
+		goto fail;
+	if (nxge_rx_mac_enable(nep) != NXGE_OK)
+		goto fail;
+
+	MUTEX_EXIT(&nep->lock);
+	NXGE_DEBUG_MSG((nep, MAC_CTL, "<== nxge_set_mac_address"));
+	return 0;
+  fail:
+	MUTEX_EXIT(&nep->lock);
+	NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "<== nxge_set_mac_address failed"));
+	return -EADDRNOTAVAIL;
+
+}
+
+
+static int nxge_ethtool_ioctl(struct net_device *dev, void *ep_user)
+{
+
+	return 0;
+
+}
+
+
+static int nxge_rtrace_ioctl(p_nxge_t nep, rtrace_t *rtp)
+{
+	uint32_t	i, j;
+	uint32_t	start_blk;
+	uint32_t	base_entry;
+	uint32_t	num_entries;
+	int             rc = 0;
+
+	NXGE_DEBUG_MSG((nep, MOD_CTL, "==> nxge_rtrace_ioctl"));
+
+	if (rtp == NULL) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+				"nxge_rtrace_ioctl: Buffer not allocated"));
+		rc = EINVAL;
+		return (rc);
+	}
+
+	start_blk = rtp->next_idx;
+	num_entries = rtp->last_idx;
+	base_entry = start_blk * MAX_RTRACE_IOC_ENTRIES;
+
+	NXGE_DEBUG_MSG((nep, MOD_CTL, "start_blk = %d\n", start_blk));
+	NXGE_DEBUG_MSG((nep, MOD_CTL, "num_entries = %d\n", num_entries));
+	NXGE_DEBUG_MSG((nep, MOD_CTL, "base_entry = %d\n", base_entry));
+
+	rtp->next_idx = npi_rtracebuf.next_idx;
+	rtp->last_idx = npi_rtracebuf.last_idx;
+	rtp->wrapped = npi_rtracebuf.wrapped;
+	for (i = 0, j = base_entry; i < num_entries; i++, j++) {
+		rtp->buf[i].ctl_addr = npi_rtracebuf.buf[j].ctl_addr;
+		rtp->buf[i].val_l32 = npi_rtracebuf.buf[j].val_l32;
+		rtp->buf[i].val_h32 = npi_rtracebuf.buf[j].val_h32;
+	}
+
+	NXGE_DEBUG_MSG((nep, MOD_CTL, "<== nxge_rtrace_ioctl"));
+	return (rc);
+}
+
+
+static int nxge_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)
+{
+	p_nxge_t nep = netdev_priv(dev);
+	int rc = -EOPNOTSUPP;
+	uint64_t addr, val;
+	int i;
+
+	rtrace_t *rtp = (rtrace_t *)ifr->ifr_data;
+	nxge_dbg_reg_t *reg_info_p = (nxge_dbg_reg_t *)ifr->ifr_data;
+	nxge_cfg_cmd_t *cfg_cmd_p = (nxge_cfg_cmd_t *)ifr->ifr_data;
+
+
+	switch (cmd) {
+	case NXGE_RTRACE:
+	  printk(KERN_INFO "************ In NXGE_RTRACE ioctl ********\n");
+
+/* 	  rc = 0; */
+/* 	  copy_to_user(iocp, cp->dev->dev_addr, sizeof (struct cas_test_ioc)); */
+	  rc = nxge_rtrace_ioctl(nep, rtp);
+	  break;
+
+	  /********************** Some Debug ioctls *************************/
+
+	case NXGE_DEBUG_IOC:
+
+	  printk(KERN_INFO "****** In NXGE_DEBUG_IOC ioctl ******\n");
+	  rc = 0;
+	  switch (reg_info_p->opcode) {
+	  case DUMP_TX_REGS:
+		  nxge_txdma_regs_dump_channels(nep);
+		  break;
+
+	  case DUMP_RX_REGS:
+		  nxge_rxdma_regs_dump_channels(nep);
+		  break;
+
+	  case DUMP_INTR_REGS:
+		  nxge_int_regs_dump(nep);
+		  break;
+
+	  case DUMP_MAC_REGS:
+		  npi_mac_dump_regs(nep->npi_handle, nep->function_num);
+		  break;
+
+	  case DUMP_IPP_REGS:
+		  npi_ipp_dump_regs(nep->npi_handle, nep->function_num);
+		  break;
+
+	  case DUMP_VLAN_TABLE:
+		  (void) npi_fflp_vlan_tbl_dump(nep->npi_handle);
+		  break;
+
+	  case DUMP_FFLP_REGS:
+		  npi_fflp_dump_regs(nep->npi_handle);
+		  for (i = 0; i < 8; i ++)
+			  npi_rxdma_dump_rdc_table(nep->npi_handle, i);
+		  break;
+
+	  case DUMP_PCI_REGS:
+		 nxge_pci_cfg_space_dump(nep);
+		 break;
+
+	  case SET_REG_VAL:
+		  addr = reg_info_p->addr;
+		  val = reg_info_p->val;
+		  printk(KERN_INFO "ADDR[0x%llx] VAL[0x%llx] \n", addr, val);
+		  writeq(val, nep->npi_handle.regp + addr);
+		  val = readq(nep->npi_handle.regp + addr);
+		  printk(KERN_INFO "Read Back: ADDR[0x%llx] VAL[0x%llx] \n",
+			 addr, val);
+		  break;
+
+	  case GET_REG_VAL:
+		  addr = reg_info_p->addr;
+		  val = readq(nep->npi_handle.regp + addr);
+		  printk(KERN_INFO "ADDR[0x%llx] VAL[0x%llx]", addr, val);
+		  reg_info_p->val = val;
+		  break;
+
+	  case SET_TX_LB_POLICY:
+		  nep->tx_lb_policy = reg_info_p->tx_lb_policy;
+		  printk(KERN_INFO "Setting TX LB policy to %d\n",
+			 reg_info_p->tx_lb_policy);
+		  break;
+
+	  case GET_TX_LB_POLICY:
+		  reg_info_p->tx_lb_policy = nep->tx_lb_policy;
+		  printk(KERN_INFO "TX LB policy is %d\n",
+			 reg_info_p->tx_lb_policy);
+		  break;
+	  default:
+		  printk(KERN_INFO "Unknown opcode %d for NXGE_DEBUG ioctl\n",
+			reg_info_p->opcode);
+		  rc = -EOPNOTSUPP;
+		  break;
+	  }
+	  break;
+
+	case NXGE_TCAM_ADD_IOC:
+	  printk(KERN_INFO "******* In NXGE_TCAM_ADD_IOC ioctl ******\n");
+	  nxge_put_tcam(nep, (void *)ifr->ifr_data);
+	  rc = 0;
+	  break;
+
+	case NXGE_TCAM_COMPARE_IOC: /* read and compare use same ioc. */
+	  printk(KERN_INFO "******* In NXGE_TCAM_COMPARE_IOC ioctl ******\n");
+	  rc = 0;
+	  break;
+
+	case NXGE_CFGTOOL_IOC:
+	  printk(KERN_INFO "****** In NXGE_CFGTOOL_IOC ioctl ******\n");
+	  switch (cfg_cmd_p->cmd) {
+	  case CFG_GET_SIZE_CMD:
+		  nxge_get_public_param_list_len(nep, &cfg_cmd_p->value);
+		  rc = 0;
+		  break;
+	  case CFG_GET_ALL_CMD:
+		  nxge_get_public_param_list(nep, cfg_cmd_p->param_list);
+		  rc = 0;
+		  break;
+	  case CFG_GET_ONE_CMD:
+		  rc = nxge_get_public_param(nep, cfg_cmd_p->param_name,
+					     &cfg_cmd_p->value,
+					     cfg_cmd_p->param_list);
+		  break;
+	  case CFG_SET_CMD:
+		  rc = nxge_set_public_param(nep, cfg_cmd_p->param_name,
+					     cfg_cmd_p->value);
+		  break;
+	  default:
+		  printk(KERN_INFO "Unknown cmd %u for NXGE_DEBUG ioctl\n",
+			 (unsigned int)cfg_cmd_p->cmd);
+		  break;
+	  }
+	  break;
+
+#if 0
+	case SIOCETHTOOL:
+		rc = nxge_ethtool_ioctl(dev, ifr->ifr_data);
+		break;
+
+
+	case SIOCGMIIPHY:		/* Get address of MII PHY in use. */
+		data->phy_id = nep->phy_addr;
+
+	case SIOCGMIIREG:		/* Read MII PHY register. */
+		spin_lock_irqsave(&nep->lock, flags);
+		nxge_mif_poll(cp, 0);
+		data->val_out = nxge_phy_read(nep, data->reg_num & 0x1f);
+		nxge_mif_poll(cp, 1);
+		spin_unlock_irqrestore(&nep->lock, flags);
+		rc = 0;
+		break;
+
+	case SIOCSMIIREG:		/* Write MII PHY register. */
+		if (!capable(CAP_NET_ADMIN)) {
+			rc = -EPERM;
+			break;
+		}
+		spin_lock_irqsave(&nep->lock, flags);
+		nxge_mif_poll(cp, 0);
+		rc = nxge_phy_write(nep, data->reg_num & 0x1f, data->val_in);
+		nxge_mif_poll(cp, 1);
+		spin_unlock_irqrestore(&nep->lock, flags);
+		break;
+#endif
+	default:
+		break;
+	};
+
+	return rc;
+}
+
+static void nxge_pci_cfg_space_dump(p_nxge_t nep)
+{
+	int i, j;
+	uint32_t pci_word;
+
+	printk(KERN_INFO "PCI config space for interface %s[nxge%d]:\n",
+	       nep->dev->name, nep->function_num);
+	for (i = 0, j = 1; i < 512; j++) {
+		if (j == 0 || (((j - 1) % 4) == 0))
+			printk(KERN_INFO "%04x:\t", i);
+		pci_read_config_dword(nep->pdev, i, &pci_word);
+		printk(KERN_INFO "0x%08x\t", pci_word);
+		i += 4;
+		if ((j % 4) == 0)
+			printk(KERN_INFO "\n");
+	}
+}
+
+/***********************************************************************
+ *                 TX Functions
+ *
+ **********************************************************************/
+void
+nxge_txdma_regs_dump(p_nxge_t nep, int channel)
+{
+	npi_handle_t		handle;
+	tx_ring_hdl_t 		hdl;
+	tx_ring_kick_t 		kick;
+	tx_cs_t 		cs;
+	txc_control_t		control;
+	uint32_t		bitmap = 0;
+	uint32_t		burst = 0;
+	uint32_t		bytes = 0;
+	dma_log_page_t		cfg;
+
+	printk(KERN_INFO "\n\tfunc # %d tdc %d\n ",
+	       nep->function_num, channel);
+
+	cfg.page_num = 0;
+	handle = NXGE_DEV_NPI_HANDLE(nep);
+	npi_txdma_log_page_get(handle, channel, &cfg);
+
+	printk(KERN_INFO "\n\tlog page func %d valid page 0 %d\n",
+	       cfg.func_num, cfg.valid);
+
+	cfg.page_num = 1;
+	npi_txdma_log_page_get(handle, channel, &cfg);
+
+	printk(KERN_INFO "\n\tlog page func %d valid page 1 %d\n",
+	       cfg.func_num, cfg.valid);
+
+	npi_txdma_ring_head_get(handle, channel, &hdl);
+	npi_txdma_desc_kick_reg_get(handle, channel, &kick);
+
+	printk(KERN_INFO "\n\thead value is 0x%0llx",
+	       (long long)hdl.value);
+	printk(KERN_INFO "\n\thead index %d", hdl.bits.ldw.head);
+	printk(KERN_INFO "\n\tkick value is 0x%0llx",
+	       (long long)kick.value);
+	printk(KERN_INFO "\n\ttail index %d\n", kick.bits.ldw.tail);
+
+	npi_txdma_control_status(handle, OP_GET, channel, &cs);
+
+	printk(KERN_INFO "\n\tControl statue is 0x%0llx",
+	       (long long)cs.value);
+	printk(KERN_INFO "\n\tControl status RST state %d\n", cs.bits.ldw.rst);
+
+	npi_txc_control(handle, OP_GET, &control);
+	npi_txc_port_dma_list_get(handle, nep->function_num, &bitmap);
+	npi_txc_dma_max_burst(handle, OP_GET, channel, &burst);
+	npi_txc_dma_bytes_transmitted(handle, channel, &bytes);
+
+	printk(KERN_INFO "\n\tTXC port control 0x%0llx",
+	       (long long)control.value);
+	printk(KERN_INFO "\n\tTXC port bitmap 0x%x", bitmap);
+	printk(KERN_INFO "\n\tTXC max burst %d", burst);
+	printk(KERN_INFO "\n\tTXC bytes xmt %d\n", bytes);
+
+	{
+		ipp_status_t status;
+
+		npi_ipp_get_status(handle, nep->function_num, &status);
+
+		printk(KERN_INFO "\n\tIPP status 0x%llx\n\n", status.value);
+	}
+}
+
+void
+nxge_txdma_dump_channel_stats(p_nxge_t nep, int i, uint8_t channel)
+{
+	nxge_tx_ring_stats_t	stats = nep->statsp->tdc_stats[i];
+
+	printk(KERN_INFO "\n\nTX Stats for channel [%d]\n", channel);
+
+	printk(KERN_INFO "\n\topackets\t\t0x%llx", stats.opackets);
+	printk(KERN_INFO "\n\toerrors\t\t0x%llx", stats.oerrors);
+	printk(KERN_INFO "\n\tobytes\t\t0x%llx", stats.obytes);
+#ifdef NXGE_DEBUG_INFO
+	printk(KERN_INFO "\n\tstk_tx_pkts\t\t0x%llx", stats.stk_tx_pkts);
+#endif
+	printk(KERN_INFO "\n\tdescs_kicked\t\t0x%llx", stats.descs_kicked);
+
+	printk(KERN_INFO "\n\n");
+}
+
+void
+nxge_txdma_regs_dump_channels(p_nxge_t nep)
+{
+	int			index, ndmas;
+	uint16_t		channel;
+	p_tx_rings_t 		tx_rings;
+	npi_handle_t		handle;
+
+	printk(KERN_INFO "==> nxge_txdma_regs_dump_channels\n");
+
+	handle = NXGE_DEV_NPI_HANDLE(nep);
+	(void) npi_txdma_dump_fzc_regs(handle);
+
+	tx_rings = nep->tx_rings;
+	if (tx_rings == NULL) {
+		printk(KERN_INFO "<== nxge_txdma_regs_dump_channels: NULL ring\n");
+		return;
+	}
+
+	ndmas = nep->max_tdcs;
+	if (!ndmas) {
+		printk(KERN_INFO "<== nxge_txdma_regs_dump_channels: "
+			"no channel allocated\n");
+		return;
+	}
+
+	if (tx_rings->rings == NULL) {
+		printk(KERN_INFO "<== nxge_txdma_regs_dump_channels: NULL rings\n");
+	return;
+	}
+
+	printk(KERN_INFO "==> nxge_txdma_regs_dump_channels: "
+		"tx_rings $%p tx_desc_rings $%p ndmas %d\n",
+		tx_rings, tx_rings->rings, ndmas);
+
+	for (index = 0; index < ndmas; index++) {
+		channel = tx_rings->rings[index]->tdc;
+		printk(KERN_INFO "==> nxge_txdma_regs_dump_channels: channel %d\n",
+			channel);
+		nxge_txdma_regs_dump(nep, channel);
+		npi_txdma_dump_tdc_regs(handle, channel);
+	}
+
+	/* Dump TXC registers */
+	npi_txc_dump_fzc_regs(handle);
+	npi_txc_dump_port_fzc_regs(handle, nep->function_num);
+
+	for (index = 0; index < ndmas; index++) {
+		channel = tx_rings->rings[index]->tdc;
+		printk(KERN_INFO "==> nxge_txdma_regs_dump_channels: channel %d\n",
+			channel);
+		npi_txc_dump_tdc_fzc_regs(handle, channel);
+	}
+
+	for (index = 0; index < ndmas; index++) {
+		channel = tx_rings->rings[index]->tdc;
+		printk(KERN_INFO "==> nxge_txdma_regs_dump_channels: channel %d\n",
+			channel);
+		nxge_txdma_regs_dump(nep, channel);
+	}
+
+	for (index = 0; index < ndmas; index++) {
+		channel = tx_rings->rings[index]->tdc;
+		nxge_txdma_dump_channel_stats(nep, index, channel);
+	}
+
+	printk(KERN_INFO "<== nxge_txdma_regs_dump\n");
+}
+
+
+static nxge_status_t
+nxge_txdma_fatal_err_recover(p_nxge_t nxgep, uint16_t channel,
+			     p_tx_ring_t tx_ring_p)
+{
+	npi_handle_t	handle;
+	npi_status_t	rs = NPI_SUCCESS;
+	p_tx_mbox_t	tx_mbox_p;
+	nxge_status_t	status = NXGE_OK;
+
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "<== nxge_txdma_fatal_err_recover"));
+
+	/*
+	 * Stop the dma channel waits for the stop done.
+	 * If the stop done bit is not set, then create
+	 * an error.
+	 */
+
+	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "nxge_txdma_fatal_err_recover: "
+									 "Tx stop..."));
+	MUTEX_ENTER(&tx_ring_p->lock);
+	rs = npi_txdma_channel_control(handle, TXDMA_STOP, channel);
+	if (rs != NPI_SUCCESS) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"==> nxge_txdma_fatal_err_recover (channel %d): "
+			"stop failed ", channel));
+		goto fail;
+	}
+
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "nxge_txdma_fatal_err_recover: "
+					" Tx reclaim..."));
+	nxge_tx_reclaim(nxgep, tx_ring_p, 0);
+
+	/*
+	 * Reset TXDMA channel
+	 */
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "nxge_txdma_fatal_err_recover: "
+									 "Tx reset..."));
+	if ((rs = npi_txdma_channel_control(handle, TXDMA_RESET, channel)) !=
+						NPI_SUCCESS) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"==> nxge_txdma_fatal_err_recover (channel %d)"
+			" reset channel failed 0x%x", channel, rs));
+		goto fail;
+	}
+	/*
+	 * Reset the tail (kick) register to 0.
+	 * (Hardware will not reset it. Tx overflow fatal
+	 * error if tail is not set to 0 after reset!
+	 */
+	TXDMA_REG_WRITE64(handle, TX_RING_KICK_REG, channel, 0);
+
+
+
+	/*
+	 * Reset the tail (kick) register to 0.
+	 * (Hardware will not reset it. Tx overflow fatal
+	 * error if tail is not set to 0 after reset!
+	 */
+	TXDMA_REG_WRITE64(handle, TX_RING_KICK_REG, channel, 0);
+
+	/* Restart TXDMA channel */
+
+	/*
+	 * Initialize the TXDMA channel specific FZC control
+	 * configurations. These FZC registers are pertaining
+	 * to each TX channel (i.e. logical pages).
+	 */
+	tx_mbox_p = nxge_txdma_get_mbox(nxgep, channel);
+	if (tx_mbox_p == NULL)
+		goto fail;
+
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "nxge_txdma_fatal_err_recover: "
+			"Tx restart..."));
+
+	status = nxge_init_fzc_txdma_channel(nxgep, channel,
+					     tx_ring_p, tx_mbox_p);
+	if (status != NXGE_OK)
+		goto fail;
+
+	/*
+	 * Initialize the event masks.
+	 */
+	tx_ring_p->tx_evmask.value = 0;
+	status = nxge_init_txdma_channel_event_mask(nxgep, channel,
+						    &tx_ring_p->tx_evmask);
+	if (status != NXGE_OK)
+		goto fail;
+
+	tx_ring_p->in_wrap = B_FALSE;
+	tx_ring_p->in = 0;
+	tx_ring_p->out = 0;
+
+	/*
+	 * Load TXDMA descriptors, buffers, mailbox,
+	 * initialise the DMA channels and
+	 * enable each DMA channel.
+	 */
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "nxge_txdma_fatal_err_recover: "
+					"Tx enable..."));
+	status = nxge_enable_txdma_channel(nxgep, channel,
+					   tx_ring_p, tx_mbox_p);
+	if (status != NXGE_OK)
+		goto fail;
+
+	MUTEX_EXIT(&tx_ring_p->lock);
+
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "==> nxge_txdma_fatal_err_recover"));
+
+	return (NXGE_OK);
+
+fail:
+	MUTEX_EXIT(&tx_ring_p->lock);
+#if 0
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+		"nxge_txdma_fatal_err_recover (channel %u): "
+		"failed to recover this txdma channel", channel));
+#else
+	printk(KERN_INFO "nxge_txdma_fatal_err_recover (channel %u): "
+		"failed to recover this txdma channel", channel);
+#endif
+
+	return (status);
+}
+
+nxge_status_t
+nxge_tx_port_fatal_err_recover(p_nxge_t nxgep)
+{
+	npi_handle_t	handle;
+	npi_status_t	rs = NPI_SUCCESS;
+	nxge_status_t	status = NXGE_OK;
+	p_tx_ring_t 	*tx_desc_rings;
+	p_tx_rings_t	tx_rings;
+	p_tx_ring_t	tx_ring_p;
+	p_tx_mbox_t	tx_mbox_p;
+	int		i, ndmas;
+	uint16_t	channel;
+
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "<== nxge_tx_port_fatal_err_recover"));
+
+	/*
+	 * Stop the dma channel waits for the stop done.
+	 * If the stop done bit is not set, then create
+	 * an error.
+	 */
+
+	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "nxge_tx_port_fatal_err_recover: "
+					"Tx stop all channels..."));
+
+	tx_rings = nxgep->tx_rings;
+	tx_desc_rings = tx_rings->rings;
+	ndmas = nxgep->max_tdcs;
+
+	for (i = 0; i < ndmas; i++) {
+		if (tx_desc_rings[i] == NULL) {
+			continue;
+		}
+		channel = tx_desc_rings[i]->tdc;
+		tx_ring_p = tx_rings->rings[i];
+		MUTEX_ENTER(&tx_ring_p->lock);
+		rs = npi_txdma_channel_control(handle, TXDMA_STOP, channel);
+		MUTEX_EXIT(&tx_ring_p->lock);
+		if (rs != NPI_SUCCESS) {
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"==> nxge_txdma_fatal_err_recover (channel %d): "
+			"stop failed ", channel));
+			goto fail;
+		}
+	}
+
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "nxge_tx_port_fatal_err_recover: "
+					"Tx reclaim all channels ..."));
+
+	for (i = 0; i < ndmas; i++) {
+		if (tx_desc_rings[i] == NULL) {
+			continue;
+		}
+		tx_ring_p = tx_rings->rings[i];
+		MUTEX_ENTER(&tx_ring_p->lock);
+		nxge_tx_reclaim(nxgep, tx_ring_p, 0);
+		MUTEX_EXIT(&tx_ring_p->lock);
+	}
+
+	/*
+	 * Reset TXDMA channel
+	 */
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "nxge_tx_port_fatal_err_recover: "
+					"Tx reset all channels..."));
+
+	for (i = 0; i < ndmas; i++) {
+		if (tx_desc_rings[i] == NULL) {
+			continue;
+		}
+		channel = tx_desc_rings[i]->tdc;
+		tx_ring_p = tx_rings->rings[i];
+		MUTEX_ENTER(&tx_ring_p->lock);
+		if ((rs = npi_txdma_channel_control(handle, TXDMA_RESET,
+				channel)) != NPI_SUCCESS) {
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"==> nxge_txdma_fatal_err_recover (channel %d)"
+				" reset channel failed 0x%x", channel, rs));
+			MUTEX_EXIT(&tx_ring_p->lock);
+			goto fail;
+		}
+		MUTEX_EXIT(&tx_ring_p->lock);
+	}
+
+	/* Restart TXDMA channel */
+
+	/*
+	 * Initialize the TXDMA channel specific FZC control
+	 * configurations. These FZC registers are pertaining
+	 * to each TX channel (i.e. logical pages).
+	 */
+
+
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "nxge_tx_port_fatal_err_recover: "
+							   "Tx restart all channels "));
+
+	for (i = 0; i < ndmas; i++) {
+		if (tx_desc_rings[i] == NULL) {
+			continue;
+		}
+		channel = tx_desc_rings[i]->tdc;
+		tx_ring_p = tx_rings->rings[i];
+		tx_mbox_p = nxge_txdma_get_mbox(nxgep, channel);
+		MUTEX_ENTER(&tx_ring_p->lock);
+		status = nxge_init_fzc_txdma_channel(nxgep, channel,
+						tx_ring_p, tx_mbox_p);
+		tx_ring_p->tx_evmask.value = 0;
+		/*
+		 * Initialize the event masks.
+		 */
+		status = nxge_init_txdma_channel_event_mask(nxgep, channel,
+							&tx_ring_p->tx_evmask);
+		if (status != NXGE_OK)
+			goto fail;
+		MUTEX_EXIT(&tx_ring_p->lock);
+		if (status != NXGE_OK)
+			goto fail;
+	}
+
+	/*
+	 * Load TXDMA descriptors, buffers, mailbox,
+	 * initialise the DMA channels and
+	 * enable each DMA channel.
+	 */
+
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "==> nxge_tx_port_fatal_err_recover"
+				"Tx enable all channels ..."));
+
+	for (i = 0; i < ndmas; i++) {
+		if (tx_desc_rings[i] == NULL) {
+			continue;
+		}
+		channel = tx_desc_rings[i]->tdc;
+		tx_ring_p = tx_rings->rings[i];
+		tx_mbox_p = nxge_txdma_get_mbox(nxgep, channel);
+		MUTEX_ENTER(&tx_ring_p->lock);
+		status = nxge_enable_txdma_channel(nxgep, channel,
+						tx_ring_p, tx_mbox_p);
+		MUTEX_EXIT(&tx_ring_p->lock);
+		if (status != NXGE_OK)
+			goto fail;
+	}
+
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "==> nxge_tx_port_fatal_err_recover"));
+
+	return (NXGE_OK);
+
+fail:
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+		"nxge_txdma_fatal_err_recover (channel %d): "
+		"failed to recover this txdma channel"));
+
+	return (status);
+}
+
+
+void
+nxge_txdma_inject_err(p_nxge_t nxgep, uint32_t err_id, uint8_t chan)
+{
+	tdmc_intr_dbg_t		tdi;
+
+	switch (err_id) {
+
+	case NXGE_FM_EREPORT_TDMC_PREF_BUF_PAR_ERR:
+	case NXGE_FM_EREPORT_TDMC_MBOX_ERR:
+	case NXGE_FM_EREPORT_TDMC_NACK_PREF:
+	case NXGE_FM_EREPORT_TDMC_NACK_PKT_RD:
+	case NXGE_FM_EREPORT_TDMC_PKT_SIZE_ERR:
+	case NXGE_FM_EREPORT_TDMC_TX_RING_OFLOW:
+	case NXGE_FM_EREPORT_TDMC_CONF_PART_ERR:
+	case NXGE_FM_EREPORT_TDMC_PKT_PRT_ERR:
+		TXDMA_REG_READ64(nxgep->npi_handle, TDMC_INTR_DBG_REG,
+			chan, &tdi.value);
+		if (err_id == NXGE_FM_EREPORT_TDMC_PREF_BUF_PAR_ERR)
+			tdi.bits.ldw.pref_buf_par_err = 1;
+		else if (err_id == NXGE_FM_EREPORT_TDMC_MBOX_ERR)
+			tdi.bits.ldw.mbox_err = 1;
+		else if (err_id == NXGE_FM_EREPORT_TDMC_NACK_PREF)
+			tdi.bits.ldw.nack_pref = 1;
+		else if (err_id == NXGE_FM_EREPORT_TDMC_NACK_PKT_RD)
+			tdi.bits.ldw.nack_pkt_rd = 1;
+		else if (err_id == NXGE_FM_EREPORT_TDMC_PKT_SIZE_ERR)
+			tdi.bits.ldw.pkt_size_err = 1;
+		else if (err_id == NXGE_FM_EREPORT_TDMC_TX_RING_OFLOW)
+			tdi.bits.ldw.tx_ring_oflow = 1;
+		else if (err_id == NXGE_FM_EREPORT_TDMC_CONF_PART_ERR)
+			tdi.bits.ldw.conf_part_err = 1;
+		else if (err_id == NXGE_FM_EREPORT_TDMC_PKT_PRT_ERR)
+			tdi.bits.ldw.pkt_part_err = 1;
+		printk(KERN_INFO "Write 0x%llx to TDMC_INTR_DBG_REG\n",
+				tdi.value);
+		TXDMA_REG_WRITE64(nxgep->npi_handle, TDMC_INTR_DBG_REG,
+			chan, tdi.value);
+	}
+}
+
+
+
+static void nxge_txdma_stop(p_nxge_t nep)
+{
+	NXGE_DEBUG_MSG((nep, TX_CTL, "==> nxge_txdma_stop"));
+
+	nxge_link_monitor(nep, LINK_MONITOR_STOP);
+	nxge_tx_mac_disable(nep);
+	nxge_txdma_hw_mode(nep, NXGE_DMA_STOP);
+
+	nxge_hw_reset(nep);
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "<== nxge_txdma_stop"));
+}
+
+static void nxge_txdma_stop_start(p_nxge_t nep)
+{
+	NXGE_DEBUG_MSG((nep, TX_CTL, "==> nxge_txdma_stop_start"));
+
+	nxge_txdma_stop(nep);
+	nxge_xcvr_init(nep);
+	nxge_link_monitor(nep, LINK_MONITOR_START);
+	nxge_fixup_txdma_rings(nep);
+	nxge_txdma_hw_mode(nep, NXGE_DMA_START);
+	nxge_tx_mac_enable(nep);
+	nxge_txdma_hw_kick(nep);
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "<== nxge_txdma_stop_start"));
+}
+
+static int
+nxge_txdma_stop_inj_err(p_nxge_t nxgep, int channel)
+{
+	npi_handle_t		handle;
+	tdmc_intr_dbg_t		intr_dbg;
+	int			status;
+	npi_status_t		rs = NPI_SUCCESS;
+
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "==> nxge_txdma_stop_inj_err"));
+	/*
+	 * Stop the dma channel waits for the stop done.
+	 * If the stop done bit is not set, then create
+	 * an error.
+	 */
+	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	rs = npi_txdma_channel_disable(handle, channel);
+	status = ((rs == NPI_SUCCESS) ? NXGE_OK : NXGE_ERROR | rs);
+	if (status == NXGE_OK) {
+		NXGE_DEBUG_MSG((nxgep, TX_CTL,
+			"<== nxge_txdma_stop_inj_err (channel %d): "
+			"stopped OK", channel));
+		return (status);
+	}
+
+	NXGE_DEBUG_MSG((nxgep, NXGE_ERR_CTL,
+		"==> nxge_txdma_stop_inj_err (channel %d): stop failed (0x%x) "
+		"injecting error", channel, rs));
+	/* Inject any error */
+	intr_dbg.value = 0;
+	intr_dbg.bits.ldw.nack_pref = 1;
+	(void) npi_txdma_inj_int_error_set(handle, channel, &intr_dbg);
+
+	/* Stop done bit will be set as a result of error injection */
+	rs = npi_txdma_channel_disable(handle, channel);
+	status = ((rs == NPI_SUCCESS) ? NXGE_OK : NXGE_ERROR | rs);
+	if (!(rs & NPI_TXDMA_STOP_FAILED)) {
+		NXGE_DEBUG_MSG((nxgep, TX_CTL,
+			"<== nxge_txdma_stop_inj_err (channel %d): "
+			"stopped OK ", channel));
+		return (status);
+	}
+
+	NXGE_DEBUG_MSG((nxgep, NXGE_ERR_CTL,
+		"==> nxge_txdma_stop_inj_err (channel): stop failed (0x%x) "
+		" (injected error but still not stopped)", channel, rs));
+
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "<== nxge_txdma_stop_inj_err"));
+	return (status);
+}
+
+static void nxge_hw_start_tx(p_nxge_t nep)
+{
+	NXGE_DEBUG_MSG((nep, TX_CTL, "==> nxge_hw_start_tx"));
+
+	nxge_txdma_hw_start(nep);
+	nxge_tx_mac_enable(nep);
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "<== nxge_hw_start_tx"));
+}
+
+static p_tx_ring_t
+nxge_txdma_get_ring(p_nxge_t nep, uint16_t channel)
+{
+	int			index, ndmas;
+	uint16_t		tdc;
+	p_tx_rings_t 		tx_rings;
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "==> nxge_txdma_get_ring"));
+
+	tx_rings = nep->tx_rings;
+	if (tx_rings == NULL) {
+		NXGE_DEBUG_MSG((nep, TX_CTL,
+			"<== nxge_txdma_get_ring: NULL ring pointer"));
+		return (NULL);
+	}
+
+	ndmas = nep->max_tdcs;
+	if (!ndmas) {
+		NXGE_DEBUG_MSG((nep, TX_CTL,
+			"<== nxge_txdma_get_ring: no channel allocated"));
+		return (NULL);
+	}
+
+	if (tx_rings->rings == NULL) {
+		NXGE_DEBUG_MSG((nep, TX_CTL,
+			"<== nxge_txdma_get_ring: NULL rings pointer"));
+		return (NULL);
+	}
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "nxge_txdma_get_ring: "
+		"tx_rings $%p tx_desc_rings $%p ndmas %d",
+		tx_rings, tx_rings, ndmas));
+
+	for (index = 0; index < ndmas; index++) {
+		tdc = tx_rings->rings[index]->tdc;
+		NXGE_DEBUG_MSG((nep, TX_CTL,
+			"nxge_txdma_get_ring: channel %d", tdc));
+		if (channel == tdc) {
+			NXGE_DEBUG_MSG((nep, TX_CTL,
+				"<== nxge_txdma_get_ring: tdc %d "
+				"ring $%p",
+				tdc, tx_rings->rings[index]));
+			return (p_tx_ring_t)(tx_rings->rings[index]);
+		}
+	}
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "<== nxge_txdma_get_ring"));
+	return (NULL);
+}
+
+static p_tx_mbox_t
+nxge_txdma_get_mbox(p_nxge_t nxgep, uint16_t channel)
+{
+	int			index, tdc, ndmas;
+	p_tx_rings_t 		tx_rings;
+	p_tx_mbox_areas_t 	tx_mbox_areas_p;
+	p_tx_mbox_t		*tx_mbox_p;
+
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "==> nxge_txdma_get_mbox"));
+
+	tx_rings = nxgep->tx_rings;
+	if (tx_rings == NULL) {
+		NXGE_DEBUG_MSG((nxgep, TX_CTL,
+			"<== nxge_txdma_get_mbox: NULL ring pointer"));
+		return (NULL);
+	}
+
+	tx_mbox_areas_p = nxgep->tx_mbox_areas_p;
+	if (tx_mbox_areas_p == NULL) {
+		NXGE_DEBUG_MSG((nxgep, TX_CTL,
+			"<== nxge_txdma_get_mbox: NULL mbox pointer"));
+		return (NULL);
+	}
+
+	tx_mbox_p = tx_mbox_areas_p->txmbox_areas;
+
+	ndmas = nxgep->max_tdcs;
+	if (!ndmas) {
+		NXGE_DEBUG_MSG((nxgep, TX_CTL,
+			"<== nxge_txdma_get_mbox: no channel allocated"));
+		return (NULL);
+	}
+
+	if (tx_rings->rings == NULL) {
+		NXGE_DEBUG_MSG((nxgep, TX_CTL,
+			"<== nxge_txdma_get_mbox: NULL rings pointer"));
+		return (NULL);
+	}
+
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "==> nxge_txdma_get_mbox: "
+		"tx_rings $%p tx_desc_rings $%p ndmas %d",
+		tx_rings, tx_rings, ndmas));
+
+	for (index = 0; index < ndmas; index++) {
+		tdc = tx_rings->rings[index]->tdc;
+		NXGE_DEBUG_MSG((nxgep, TX_CTL,
+			"==> nxge_txdma_get_mbox: channel %d", tdc));
+		if (channel == tdc) {
+			NXGE_DEBUG_MSG((nxgep, TX_CTL,
+				"<== nxge_txdma_get_mbox: tdc %d "
+				"ring $%p",
+				tdc, tx_rings->rings[index]));
+			return (p_tx_mbox_t)(tx_mbox_p[index]);
+		}
+	}
+
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "<== nxge_txdma_get_mbox"));
+	return (NULL);
+}
+
+static nxge_status_t
+nxge_tx_err_evnts(p_nxge_t nxgep, uint_t index,
+				  p_nxge_ldv_t ldvp, tx_cs_t cs)
+{
+	npi_handle_t		handle;
+	npi_status_t		rs;
+	uint8_t			channel;
+	p_tx_ring_t 		*tx_rings;
+	p_tx_ring_t 		tx_ring_p;
+	p_nxge_tx_ring_stats_t	tdc_stats;
+	boolean_t		txchan_fatal = B_FALSE;
+	nxge_status_t		status = NXGE_OK;
+	tdmc_inj_par_err_t	par_err;
+
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "==> nxge_tx_err_evnts"));
+
+	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	channel = ldvp->channel;
+
+	tx_rings = nxgep->tx_rings->rings;
+	tx_ring_p = tx_rings[index];
+	tdc_stats = tx_ring_p->tdc_stats;
+	if ((cs.bits.ldw.pkt_size_err) || (cs.bits.ldw.pref_buf_par_err) ||
+		(cs.bits.ldw.nack_pref) || (cs.bits.ldw.nack_pkt_rd) ||
+		(cs.bits.ldw.conf_part_err) || (cs.bits.ldw.pkt_prt_err)) {
+		if ((rs = npi_txdma_ring_error_get(handle, channel,
+					&tdc_stats->errlog)) != NPI_SUCCESS)
+			return (NXGE_ERROR | rs);
+	}
+
+	if (cs.bits.ldw.mbox_err) {
+		tdc_stats->mbox_err++;
+/* 		NXGE_FM_REPORT_ERROR(nxgep, nxgep->mac.portnum, channel, */
+/* 					NXGE_FM_EREPORT_TDMC_MBOX_ERR); */
+		printk(KERN_INFO "==> nxge_tx_err_evnts(channel %d): "
+		       "fatal error: mailbox\n", channel);
+		txchan_fatal = B_TRUE;
+	}
+	if (cs.bits.ldw.pkt_size_err) {
+		tdc_stats->pkt_size_err++;
+		printk(KERN_INFO "==> nxge_tx_err_evnts(channel %d): "
+		       "fatal error: pkt_size_err\n", channel);
+		txchan_fatal = B_TRUE;
+	}
+	if (cs.bits.ldw.tx_ring_oflow) {
+		tdc_stats->tx_ring_oflow++;
+		printk(KERN_INFO "==> nxge_tx_err_evnts(channel %d): "
+		       "fatal error: tx_ring_oflow\n", channel);
+		txchan_fatal = B_TRUE;
+	}
+	if (cs.bits.ldw.pref_buf_par_err) {
+		uint64_t value;
+		tdc_stats->pre_buf_par_err++;
+		printk(KERN_INFO "==> nxge_tx_err_evnts(channel %d): "
+		       "fatal error: pre_buf_par_err\n", channel);
+		/* Clear error injection source for parity error */
+		(void) npi_txdma_inj_par_error_get(handle, (uint32_t *)&value);
+		par_err.value = value;
+		par_err.bits.ldw.inject_parity_error &= ~(1 << channel);
+		(void) npi_txdma_inj_par_error_set(handle, par_err.value);
+		txchan_fatal = B_TRUE;
+	}
+	if (cs.bits.ldw.nack_pref) {
+		tdc_stats->nack_pref++;
+		printk(KERN_INFO "==> nxge_tx_err_evnts(channel %d): "
+		       "fatal error: nack_pref\n", channel);
+		txchan_fatal = B_TRUE;
+	}
+	if (cs.bits.ldw.nack_pkt_rd) {
+		tdc_stats->nack_pkt_rd++;
+		printk(KERN_INFO "==> nxge_tx_err_evnts(channel %d): "
+		       "fatal error: nack_pkt_rd\n", channel);
+		txchan_fatal = B_TRUE;
+	}
+	if (cs.bits.ldw.conf_part_err) {
+		tdc_stats->conf_part_err++;
+		printk(KERN_INFO "==> nxge_tx_err_evnts(channel %d): "
+		       "fatal error: config_partition_err\n", channel);
+		txchan_fatal = B_TRUE;
+	}
+	if (cs.bits.ldw.pkt_prt_err) {
+		tdc_stats->pkt_part_err++;
+		printk(KERN_INFO "==> nxge_tx_err_evnts(channel %d): "
+		       "fatal error: pkt_prt_err\n", channel);
+		txchan_fatal = B_TRUE;
+	}
+
+	/* Clear error injection source in case this is an injected error */
+	TXDMA_REG_WRITE64(nxgep->npi_handle, TDMC_INTR_DBG_REG, channel, 0);
+
+	if (txchan_fatal) {
+		printk(KERN_INFO "%s[nxge%d]: nxge_tx_err_evnts: "
+		       " fatal error on channel %d cs 0x%llx\n",
+		       nxgep->dev->name, nxgep->function_num,
+		       channel, cs.value);
+#if 0
+		nxge_txdma_regs_dump_channels(nxgep);
+#endif
+		/* XXX: Need to reset txdma channel here */
+		status = nxge_txdma_fatal_err_recover(nxgep, channel,
+						      tx_ring_p);
+	}
+
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "<== nxge_tx_err_evnts"));
+
+	return (status);
+}
+
+static void
+nxge_fixup_txdma_rings(p_nxge_t nep)
+{
+	int			index, ndmas;
+	uint16_t		channel;
+	p_tx_rings_t 		tx_rings;
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "==> nxge_fixup_txdma_rings"));
+
+	/* XXXX: define what needs to be done here (FMA) */
+	/*
+	 * For each transmit channel, reclaim each descriptor and
+	 * free buffers.
+	 */
+	tx_rings = nep->tx_rings;
+	if (tx_rings == NULL) {
+		NXGE_DEBUG_MSG((nep, TX_CTL,
+			"<== nxge_fixup_txdma_rings: NULL ring pointer"));
+		return;
+	}
+
+	ndmas = nep->max_tdcs;
+	if (!ndmas) {
+		NXGE_DEBUG_MSG((nep, TX_CTL,
+			"<== nxge_fixup_txdma_rings: no channel allocated"));
+		return;
+	}
+
+	if (tx_rings->rings == NULL) {
+		NXGE_DEBUG_MSG((nep, TX_CTL,
+			"<== nxge_fixup_txdma_rings: NULL rings pointer"));
+		return;
+	}
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "==> nxge_fixup_txdma_rings: "
+		"tx_rings $%p tx_desc_rings $%p ndmas %d",
+		tx_rings, tx_rings->rings, ndmas));
+
+	for (index = 0; index < ndmas; index++) {
+		channel = tx_rings->rings[index]->tdc;
+		NXGE_DEBUG_MSG((nep, TX_CTL,
+			"==> nxge_fixup_txdma_rings: channel %d", channel));
+
+		nxge_txdma_fixup_channel(nep, tx_rings->rings[index],
+			channel);
+	}
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "<== nxge_fixup_txdma_rings"));
+}
+
+static void
+nxge_txdma_fix_channel(p_nxge_t nep, uint16_t channel)
+{
+	p_tx_ring_t	ring_p;
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "==> nxge_txdma_fix_channel"));
+	ring_p = nxge_txdma_get_ring(nep, channel);
+	if (ring_p == NULL) {
+		NXGE_DEBUG_MSG((nep, TX_CTL, "<== nxge_txdma_fix_channel"));
+		return;
+	}
+
+	if (ring_p->tdc != channel) {
+		NXGE_DEBUG_MSG((nep, TX_CTL,
+			"<== nxge_txdma_fix_channel: channel not matched "
+			"ring tdc %d passed channel",
+			ring_p->tdc, channel));
+		return;
+	}
+
+	nxge_txdma_fixup_channel(nep, ring_p, channel);
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "<== nxge_txdma_fix_channel"));
+}
+
+static void
+nxge_txdma_fixup_channel(p_nxge_t nep, p_tx_ring_t ring_p, uint16_t channel)
+{
+	NXGE_DEBUG_MSG((nep, TX_CTL, "==> nxge_txdma_fixup_channel"));
+
+	if (ring_p == NULL) {
+		NXGE_DEBUG_MSG((nep, TX_CTL,
+			"<== nxge_txdma_fixup_channel: NULL ring pointer"));
+		return;
+	}
+
+	if (ring_p->tdc != channel) {
+		NXGE_DEBUG_MSG((nep, TX_CTL,
+			"<== nxge_txdma_fixup_channel: channel not matched "
+			"ring tdc %d passed channel",
+			ring_p->tdc, channel));
+		return;
+	}
+
+	MUTEX_ENTER(&ring_p->lock);
+	nxge_tx_reclaim(nep, ring_p, 0);
+
+	/* XXX: should we free up all the remaining buffers? */
+	ring_p->in = 0;
+	ring_p->out = 0;
+	ring_p->ring_head.value = 0;
+	ring_p->ring_kick_tail.value = 0;
+	ring_p->descs_pending = 0;
+	MUTEX_EXIT(&ring_p->lock);
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "<== nxge_txdma_fixup_channel"));
+}
+
+static void
+nxge_txdma_hw_kick(p_nxge_t nep)
+{
+	int			index, ndmas;
+	uint16_t		channel;
+	p_tx_rings_t 		tx_rings;
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "==> nxge_txdma_hw_kick"));
+
+	tx_rings = nep->tx_rings;
+	if (tx_rings == NULL) {
+		NXGE_DEBUG_MSG((nep, TX_CTL,
+			"<== nxge_txdma_hw_kick: NULL ring pointer"));
+		return;
+	}
+
+	ndmas = nep->max_tdcs;
+	if (!ndmas) {
+		NXGE_DEBUG_MSG((nep, TX_CTL,
+			"<== nxge_txdma_hw_kick: no channel allocated"));
+		return;
+	}
+
+	if (tx_rings->rings == NULL) {
+		NXGE_DEBUG_MSG((nep, TX_CTL,
+			"<== nxge_txdma_hw_kick: NULL rings pointer"));
+		return;
+	}
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "nxge_txdma_hw_kick: "
+		"tx_rings $%p tx_desc_rings $%p ndmas %d",
+		tx_rings, tx_rings->rings, ndmas));
+
+	for (index = 0; index < ndmas; index++) {
+		channel = tx_rings->rings[index]->tdc;
+		NXGE_DEBUG_MSG((nep, TX_CTL,
+			"nxge_txdma_hw_kick: channel %d", channel));
+		nxge_txdma_hw_kick_channel(nep, tx_rings->rings[index],
+			channel);
+	}
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "<== nxge_txdma_hw_kick"));
+}
+
+static void
+nxge_txdma_kick_channel(p_nxge_t nep, uint16_t channel)
+{
+	p_tx_ring_t	ring_p;
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "==> nxge_txdma_kick_channel"));
+
+	ring_p = nxge_txdma_get_ring(nep, channel);
+	if (ring_p == NULL) {
+		NXGE_DEBUG_MSG((nep, TX_CTL, "<== nxge_txdma_kick_channel"));
+		return;
+	}
+
+	if (ring_p->tdc != channel) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+			"<== nxge_txdma_kick_channel: channel not matched "
+			"ring tdc %d passed channel",
+			ring_p->tdc, channel));
+		return;
+	}
+
+	nxge_txdma_hw_kick_channel(nep, ring_p, channel);
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "<== nxge_txdma_kick_channel"));
+}
+
+static void
+nxge_txdma_hw_kick_channel(p_nxge_t nep, p_tx_ring_t ring_p, uint16_t channel)
+{
+	npi_handle_t		handle;
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "==> nxge_txdma_hw_kick_channel"));
+
+	if (ring_p == NULL) {
+		NXGE_DEBUG_MSG((nep, TX_CTL,
+			"<== nxge_txdma_hw_kick_channel: NULL ring pointer"));
+		return;
+	}
+
+	handle = NXGE_DEV_NPI_HANDLE(nep);
+#if 0
+	if (ring_p->descs_pending) {
+		(void) npi_txdma_desc_kick_reg_set(handle,
+						   (uint8_t)channel,
+						   (uint16_t) ring_p->in,
+						   ring_p->in_wrap);
+	}
+#endif
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "<== nxge_txdma_hw_kick_channel"));
+}
+
+
+static void
+nxge_check_tx_hang(p_nxge_t nxgep)
+{
+
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "==> nxge_check_tx_hang"));
+
+	/*
+	 * Needs inputs from hardware for regs:
+	 *	head index had not moved since last timeout.
+	 *	packets not transmitted or stuffed registers.
+	 */
+
+	if (nxge_txdma_hung(nxgep)) {
+		nxge_fixup_hung_txdma_rings(nxgep);
+	}
+
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "<== nxge_check_tx_hang"));
+}
+
+static int
+nxge_txdma_hung(p_nxge_t nxgep)
+{
+	int			index, ndmas;
+	uint16_t		channel;
+	p_tx_rings_t 		tx_rings;
+	p_tx_ring_t 		tx_ring_p;
+
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "==> nxge_txdma_hung"));
+	tx_rings = nxgep->tx_rings;
+	if (tx_rings == NULL) {
+		NXGE_DEBUG_MSG((nxgep, TX_CTL,
+			"<== nxge_txdma_hung: NULL ring pointer"));
+		return (B_FALSE);
+	}
+
+	ndmas = nxgep->max_tdcs;
+	if (!ndmas) {
+		NXGE_DEBUG_MSG((nxgep, TX_CTL,
+			"<== nxge_txdma_hung: no channel "
+			"allocated"));
+		return (B_FALSE);
+	}
+
+	if (tx_rings->rings == NULL) {
+		NXGE_DEBUG_MSG((nxgep, TX_CTL,
+			"<== nxge_txdma_hung: NULL rings pointer"));
+		return (B_FALSE);
+	}
+
+	for (index = 0; index < ndmas; index++) {
+		channel = tx_rings->rings[index]->tdc;
+		tx_ring_p = tx_rings->rings[index];
+		NXGE_DEBUG_MSG((nxgep, TX_CTL,
+			"==> nxge_txdma_hung: channel %d", channel));
+		if (nxge_txdma_channel_hung(nxgep, tx_ring_p, channel)) {
+			return (B_TRUE);
+		}
+	}
+
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "<== nxge_txdma_hung"));
+
+	return (B_FALSE);
+}
+
+static boolean_t
+nxge_txdma_channel_hung(p_nxge_t nxgep, p_tx_ring_t tx_ring_p, uint16_t channel)
+{
+	uint16_t		head_index, tail_index;
+	boolean_t		head_wrap, tail_wrap;
+	npi_handle_t		handle;
+	tx_ring_hdl_t		tx_head;
+	uint_t			tx_rd_index;
+
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "==> nxge_txdma_channel_hung"));
+
+	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	NXGE_DEBUG_MSG((nxgep, TX_CTL,
+		"==> nxge_txdma_channel_hung: channel %d", channel));
+	MUTEX_ENTER(&tx_ring_p->lock);
+	nxge_tx_reclaim(nxgep, tx_ring_p, 0);
+
+	tail_index = tx_ring_p->in;
+	tail_wrap = tx_ring_p->in_wrap;
+	tx_rd_index = tx_ring_p->out;
+	MUTEX_EXIT(&tx_ring_p->lock);
+
+	NXGE_DEBUG_MSG((nxgep, TX_CTL,
+		"==> nxge_txdma_channel_hung: tdc %d tx_rd_index %d "
+		"tail_index %d tail_wrap %d ",
+		channel, tx_rd_index, tail_index, tail_wrap));
+	/*
+	 * Read the hardware maintained transmit head
+	 * and wrap around bit.
+	 */
+	(void) npi_txdma_ring_head_get(handle, channel, &tx_head);
+	head_index =  tx_head.bits.ldw.head;
+	head_wrap = tx_head.bits.ldw.wrap;
+	NXGE_DEBUG_MSG((nxgep, TX_CTL,
+		"==> nxge_txdma_channel_hung: "
+		"tx_rd_index %d tail %d tail_wrap %d "
+		"head %d wrap %d",
+		tx_rd_index, tail_index, tail_wrap,
+		head_index, head_wrap));
+
+	if (TX_RING_EMPTY(head_index, head_wrap,
+			tail_index, tail_wrap) &&
+			(head_index == tx_rd_index)) {
+		NXGE_DEBUG_MSG((nxgep, TX_CTL,
+			"==> nxge_txdma_channel_hung: EMPTY"));
+		return (B_FALSE);
+	}
+
+	NXGE_DEBUG_MSG((nxgep, TX_CTL,
+		"==> nxge_txdma_channel_hung: Checking if ring full"));
+	if (TX_RING_FULL(head_index, head_wrap, tail_index,
+			tail_wrap)) {
+		NXGE_DEBUG_MSG((nxgep, TX_CTL,
+			"==> nxge_txdma_channel_hung: full"));
+		return (B_TRUE);
+	}
+
+	/* XXXX: If not full, check with hardware to see if it is hung */
+
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "<== nxge_txdma_channel_hung"));
+
+	return (B_FALSE);
+}
+
+
+static void
+nxge_fixup_hung_txdma_rings(p_nxge_t nxgep)
+{
+	int			index, ndmas;
+	uint16_t		channel;
+	p_tx_rings_t 		tx_rings;
+
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "==> nxge_fixup_hung_txdma_rings"));
+	tx_rings = nxgep->tx_rings;
+	if (tx_rings == NULL) {
+		NXGE_DEBUG_MSG((nxgep, TX_CTL,
+			"<== nxge_fixup_hung_txdma_rings: NULL ring pointer"));
+		return;
+	}
+
+	ndmas = nxgep->max_tdcs;
+	if (!ndmas) {
+		NXGE_DEBUG_MSG((nxgep, TX_CTL,
+			"<== nxge_fixup_hung_txdma_rings: no channel "
+			"allocated"));
+		return;
+	}
+
+	if (tx_rings->rings == NULL) {
+		NXGE_DEBUG_MSG((nxgep, TX_CTL,
+			"<== nxge_fixup_hung_txdma_rings: NULL rings pointer"));
+		return;
+	}
+
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "==> nxge_fixup_hung_txdma_rings: "
+		"tx_rings $%p tx_desc_rings $%p ndmas %d",
+		tx_rings, tx_rings->rings, ndmas));
+
+	for (index = 0; index < ndmas; index++) {
+		channel = tx_rings->rings[index]->tdc;
+		NXGE_DEBUG_MSG((nxgep, TX_CTL,
+			"==> nxge_fixup_hung_txdma_rings: channel %d",
+			channel));
+
+		nxge_txdma_fixup_hung_channel(nxgep, tx_rings->rings[index],
+			channel);
+	}
+
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "<== nxge_fixup_hung_txdma_rings"));
+}
+
+
+static void
+nxge_txdma_fix_hung_channel(p_nxge_t nxgep, uint16_t channel)
+{
+	p_tx_ring_t	ring_p;
+
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "==> nxge_txdma_fix_hung_channel"));
+	ring_p = nxge_txdma_get_ring(nxgep, channel);
+	if (ring_p == NULL) {
+		NXGE_DEBUG_MSG((nxgep, TX_CTL,
+			"<== nxge_txdma_fix_hung_channel"));
+		return;
+	}
+
+	if (ring_p->tdc != channel) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"<== nxge_txdma_fix_hung_channel: channel not matched "
+			"ring tdc %d passed channel",
+			ring_p->tdc, channel));
+		return;
+	}
+
+	nxge_txdma_fixup_channel(nxgep, ring_p, channel);
+
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "<== nxge_txdma_fix_hung_channel"));
+}
+
+
+static void
+nxge_txdma_fixup_hung_channel(p_nxge_t nxgep, p_tx_ring_t ring_p,
+	uint16_t channel)
+{
+	npi_handle_t		handle;
+	tdmc_intr_dbg_t		intr_dbg;
+	int			status;
+
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "==> nxge_txdma_fixup_hung_channel"));
+
+	if (ring_p == NULL) {
+		NXGE_DEBUG_MSG((nxgep, TX_CTL,
+			"<== nxge_txdma_fixup_channel: NULL ring pointer"));
+		return;
+	}
+
+	if (ring_p->tdc != channel) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"<== nxge_txdma_fixup_hung_channel: channel "
+			"not matched "
+			"ring tdc %d passed channel",
+			ring_p->tdc, channel));
+		return;
+	}
+
+	MUTEX_ENTER(&ring_p->lock);
+	/* Reclaim descriptors */
+	nxge_tx_reclaim(nxgep, ring_p, 0);
+	MUTEX_EXIT(&ring_p->lock);
+
+	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	/*
+	 * Stop the dma channel waits for the stop done.
+	 * If the stop done bit is not set, then create
+	 * an error.
+	 */
+	status = npi_txdma_channel_disable(handle, channel);
+	if (!(status & NPI_TXDMA_STOP_FAILED)) {
+		NXGE_DEBUG_MSG((nxgep, TX_CTL,
+			"<== nxge_txdma_fixup_hung_channel: not stopped error "
+			"ring tdc %d passed channel",
+			ring_p->tdc, channel));
+		return;
+	}
+
+	/* Inject any error */
+	intr_dbg.value = 0;
+	intr_dbg.bits.ldw.nack_pref = 1;
+	(void) npi_txdma_inj_int_error_set(handle, channel, &intr_dbg);
+
+	/* Stop done bit will be set as a result of error injection */
+	status = npi_txdma_channel_disable(handle, channel);
+	if (!(status & NPI_TXDMA_STOP_FAILED)) {
+		NXGE_DEBUG_MSG((nxgep, TX_CTL,
+			"<== nxge_txdma_fixup_hung_channel: not stopped error "
+			"ring tdc %d passed channel",
+			ring_p->tdc, channel));
+		return;
+	}
+
+	NXGE_DEBUG_MSG((nxgep, TX_CTL,
+		"<== nxge_txdma_fixup_hung_channel: stop done still not set!! "
+		"ring tdc %d passed channel",
+		ring_p->tdc, channel));
+
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "<== nxge_txdma_fixup_hung_channel"));
+}
+
+
+static void nxge_uninit_tx_cntl_ring(p_nxge_t nep, uint16_t channel, int ring)
+{
+	p_tx_ring_t 		tx_ring_p;
+	p_tx_mbox_t 		mboxp;
+	p_tx_msg_t 		tx_msg_ring;
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "==> nxge_uninit_tx_cntl_ring"));
+
+	mboxp = nep->tx_mbox_areas_p->txmbox_areas[ring];
+	tx_ring_p = nep->tx_rings->rings[ring];
+	tx_msg_ring = tx_ring_p->tx_msg_ring;
+
+	MUTEX_DESTROY(&tx_ring_p->stats_lock);
+	MUTEX_DESTROY(&tx_ring_p->lock);
+	MUTEX_DESTROY(&mboxp->lock);
+
+	KMEM_FREE(tx_msg_ring, tx_ring_p->ring_size * sizeof(tx_msg_t));
+	KMEM_FREE(mboxp, sizeof (tx_mbox_t));
+	KMEM_FREE(tx_ring_p, sizeof (tx_ring_t));
+
+	nep->tx_rings->rings[ring] = NULL;
+	nep->tx_mbox_areas_p->txmbox_areas[ring] = NULL;
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "<== nxge_uninit_tx_cntl_ring"));
+
+	return;
+}
+
+#if defined(USE_TXHDR_BUFFER)
+static void nxge_uninit_tx_buf_ring(p_nxge_t nep, uint16_t channel, int ring)
+{
+	p_tx_ring_t 		tx_ring_p;
+	int chunk_cnt, i;
+	p_nxge_dma_buf_t	dma_buf_p;
+
+	tx_ring_p = nep->tx_rings->rings[ring];
+	NXGE_DEBUG_MSG((nep, TX_CTL, "==> nxge_uninit_tx_buf_ring"));
+
+
+#ifdef USE_CONSISTENT_TXHDR_BUFFER
+	pci_free_consistent(nep->pdev,
+						tx_ring_p->ring_size * NXGE_TX_COPY_BUFFER_SIZE,
+					    tx_ring_p->cp_cpu_addr, tx_ring_p->cp_dma_addr);
+#endif
+#ifdef USE_STREAMING_TXHDR_BUFFER
+	chunk_cnt = tx_ring_p->chunk_cnt;
+	dma_buf_p = tx_ring_p->dma_buf_p;
+	/* Unmap and free the page chunks */
+	for (i = 0; i < chunk_cnt; i++) {
+		pci_unmap_page(nep->pdev, dma_buf_p[i].dma_addr,
+			       nep->page_size, PCI_DMA_FROMDEVICE);
+		__free_pages(dma_buf_p[i].page_p, nep->page_order);
+	}
+
+	/* Clear message ring addresses */
+	for (i = 0; i < tx_ring_p->ring_size; i++) {
+		tx_ring_p->tx_msg_ring[i].cp_cpu_addr = 0;
+		tx_ring_p->tx_msg_ring[i].cp_dma_addr = 0;
+	}
+
+	KMEM_FREE(dma_buf_p, sizeof (nxge_dma_buf_t) * tx_ring_p->chunk_cnt);
+#endif
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "<== nxge_uninit_tx_buff_ring"));
+
+	return;
+}
+#endif
+
+static int nxge_init_tx_cntl_ring(p_nxge_t nep, uint16_t channel, int ring)
+{
+	p_nxge_dma_common_t 	dma_cntl_p;
+	p_tx_ring_t 		tx_ring_p;
+	p_tx_mbox_t 		mboxp;
+	p_tx_msg_t 		tx_msg_ring;
+	p_tx_rng_cfig_t		tx_ring_cfig_p;
+	p_tx_ring_kick_t	tx_ring_kick_p;
+	p_tx_cs_t		tx_cs_p;
+	p_tx_dma_ent_msk_t	tx_evmask_p;
+	p_txdma_mbh_t		mboxh_p;
+	p_txdma_mbl_t		mboxl_p;
+	int			total_tx_desc_size;
+	int			status = NXGE_OK;
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "==> nxge_init_tx_cntl_ring"));
+
+	dma_cntl_p = &(nep->tx_cntl_pool_p->dma_buf_pool_p[ring]);
+
+	/* Map in the transmit descriptor ring */
+	tx_ring_p = (p_tx_ring_t) KMEM_ZALLOC(sizeof (tx_ring_t), GFP_KERNEL);
+	tx_ring_p->nep = nep;
+	tx_ring_p->tdc = channel;
+	tx_ring_p->ring = ring;
+	MUTEX_INIT(&tx_ring_p->lock, 0, 0, 0);
+	MUTEX_INIT(&tx_ring_p->stats_lock, 0, 0, 0);
+
+	/* TODO: use config value */
+	tx_ring_p->ring_size = nxge_tx_ring_size;
+	tx_ring_p->wrap_mask = nxge_tx_ring_size - 1;
+	if (!nxge_tx_intr_thres) {
+		nxge_tx_intr_thres = nxge_tx_ring_size/4;
+	}
+
+	total_tx_desc_size = sizeof (tx_desc_t) * tx_ring_p->ring_size;
+	tx_ring_p->tdc_desc = *dma_cntl_p;
+	tx_ring_p->tdc_desc.alloc_len = total_tx_desc_size;
+	memset((void *)(tx_ring_p->tdc_desc.cpu_addr), 0x55, total_tx_desc_size);
+	NXGE_DEBUG_MSG((nep, TX_CTL, "nxge_init_tx_cntl_ring, ch[%d] zeroed"
+			" out addr st[0x%llx], len[%d]",
+			channel, tx_ring_p->tdc_desc.cpu_addr,
+			total_tx_desc_size));
+
+
+	tx_ring_cfig_p = &(tx_ring_p->tx_ring_cfig);
+	tx_ring_kick_p = &(tx_ring_p->tx_ring_kick);
+	tx_cs_p = &(tx_ring_p->tx_cs);
+	tx_evmask_p = &(tx_ring_p->tx_evmask);
+	tx_ring_cfig_p->value = 0;
+	tx_ring_kick_p->value = 0;
+	tx_cs_p->value = 0;
+	tx_evmask_p->value = 0;
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "nxge_init_tx_cntl_ring: des 0x%lx sz %d",
+		       tx_ring_p->tdc_desc.dma_addr, tx_ring_p->ring_size));
+	tx_ring_cfig_p->value = 0;
+	tx_ring_cfig_p->value =
+		(tx_ring_p->tdc_desc.dma_addr & TX_RNG_CFIG_ADDR_MASK) |
+		(((uint64_t)(tx_ring_p->ring_size >> 3)) << TX_RNG_CFIG_LEN_SHIFT);
+
+	tx_cs_p->bits.ldw.rst = 1;
+
+	/* Map in mailbox */
+	mboxp = (p_tx_mbox_t) KMEM_ZALLOC(sizeof (tx_mbox_t), GFP_KERNEL);
+	mboxp->nep = nep;
+	MUTEX_INIT(&mboxp->lock, 0, 0, 0);
+
+	mboxp->tx_mbox = *dma_cntl_p;
+	mboxp->tx_mbox.cpu_addr += total_tx_desc_size;
+	mboxp->tx_mbox.dma_addr += total_tx_desc_size;
+	mboxp->tx_mbox.alloc_len = sizeof (txdma_mailbox_t);
+	memset((void *)(mboxp->tx_mbox.cpu_addr), 0, sizeof (txdma_mailbox_t));
+
+	mboxh_p = (p_txdma_mbh_t) &tx_ring_p->tx_mbox_mbh;
+	mboxl_p = (p_txdma_mbl_t) &tx_ring_p->tx_mbox_mbl;
+	mboxh_p->value = mboxl_p->value = 0;
+	NXGE_DEBUG_MSG((nep, TX_CTL, "nxge_init_tx_cntl_ring: mbox 0x%lx",
+		       mboxp->tx_mbox.dma_addr));
+
+	mboxh_p->bits.ldw.mbaddr = ((mboxp->tx_mbox.dma_addr >>
+				     TXDMA_MBH_ADDR_SHIFT) & TXDMA_MBH_MASK);
+
+	mboxl_p->bits.ldw.mbaddr = ((mboxp->tx_mbox.dma_addr & TXDMA_MBL_MASK)
+				    >> TXDMA_MBL_SHIFT);
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "nxge_init_tx_cntl_ring: mbox 0x%lx",
+		       mboxp->tx_mbox.dma_addr));
+	NXGE_DEBUG_MSG((nep, TX_CTL, "nxge_init_tx_cntl_ring: "
+		       "hmbox 0x%x lmbox 0x%x",
+		       mboxh_p->bits.ldw.mbaddr, mboxl_p->bits.ldw.mbaddr));
+
+	/*
+	 * Allocate transmit message rings.
+	 */
+	tx_msg_ring = (p_tx_msg_t)KMEM_ZALLOC(tx_ring_p->ring_size * sizeof (tx_msg_t),
+				  GFP_KERNEL);
+
+	tx_ring_p->page_valid.value = 0;
+	tx_ring_p->page_mask_1.value = tx_ring_p->page_mask_2.value = 0;
+	tx_ring_p->page_value_1.value = tx_ring_p->page_value_2.value = 0;
+	tx_ring_p->page_reloc_1.value = tx_ring_p->page_reloc_2.value = 0;
+	tx_ring_p->page_hdl.value = 0;
+
+	tx_ring_p->page_valid.bits.ldw.page0 = 1;
+	tx_ring_p->page_valid.bits.ldw.page1 = 1;
+
+	tx_ring_p->max_burst.value = 0;
+	tx_ring_p->max_burst.bits.ldw.dma_max_burst = TXC_DMA_MAX_BURST_DEFAULT;
+
+	tx_ring_p->tdc_stats = &nep->statsp->tdc_stats[ring];
+	tx_ring_p->tdc_stats->channel_num = channel;
+	tx_ring_p->tdc_stats->ring_size = tx_ring_p->ring_size;
+	tx_ring_p->descs_pending = 0;
+	nep->tx_rings->rings[ring] = tx_ring_p;
+	tx_ring_p->tx_msg_ring = tx_msg_ring;
+
+	nep->tx_mbox_areas_p->txmbox_areas[ring] = mboxp;
+
+	goto nxge_init_tx_cntl_ring;
+
+nxge_init_tx_cntl_ring:
+	NXGE_DEBUG_MSG((nep, TX_CTL, "<== nxge_init_tx_cntl_ring status 0x%x",
+		       status));
+
+	return (status);
+}
+
+static void
+nxge_txdma_disable_txdma_channel(p_nxge_t nep, uint16_t channel)
+{
+
+	NXGE_DEBUG_MSG((nep, TX_CTL,
+		"==> nxge_txdma_disable_txdma_channel: channel %d", channel));
+
+	/* stop the transmit dma channels */
+	npi_txdma_channel_disable(nep->npi_handle, channel);
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "<== nxge_txdma_disable_txdma_channel"));
+}
+
+static int
+nxge_enable_txdma_channel(p_nxge_t nep, uint16_t channel,
+			  p_tx_ring_t tx_desc_p, p_tx_mbox_t mbox_p)
+{
+	int			ret = NXGE_OK;
+	int			status;
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "==> nxge_enable_txdma_channel"));
+
+	/*
+	 * Use configuration data composed at init time.
+	 * Write to hardware the transmit ring configurations.
+	 */
+	status = npi_txdma_ring_config(nep->npi_handle, OP_SET, channel,
+				       &(tx_desc_p->tx_ring_cfig.value));
+
+	if (status != NPI_SUCCESS) {
+	  goto nxge_enable_txdma_channel_fail;
+	}
+
+	/* Write to hardware the mailbox */
+	status = npi_txdma_mbox_config(nep->npi_handle, OP_SET, channel,
+				       (uint64_t*)&(mbox_p->tx_mbox.dma_addr));
+
+	if (status != NPI_SUCCESS) {
+	  goto nxge_enable_txdma_channel_fail;
+	}
+
+	/* Start the DMA engine. */
+	status = npi_txdma_channel_init_enable(nep->npi_handle, channel);
+
+	if (status != NPI_SUCCESS) {
+	  goto nxge_enable_txdma_channel_fail;
+	}
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "<== nxge_enable_txdma_channel, "
+			"status 0x%x", status));
+
+	return (ret);
+
+nxge_enable_txdma_channel_fail:
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "<== nxge_enable_txdma_channel, "
+			"status 0x%x", status));
+
+	return (NXGE_ERROR);
+
+}
+
+static int
+nxge_reset_txdma_channel(p_nxge_t nep, uint16_t channel, uint64_t data)
+{
+	npi_status_t		rs = NPI_SUCCESS;
+	int			ret = NXGE_OK;
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "==> nxge_reset_txdma_channel"));
+
+	if ((data & TX_CS_RST_MASK) == TX_CS_RST_MASK) {
+		rs = npi_txdma_channel_reset(nep->npi_handle, channel);
+	} else {
+		rs = npi_txdma_channel_control(nep->npi_handle,
+					       TXDMA_RESET, channel);
+	}
+
+	if (rs != NPI_SUCCESS) {
+		ret = NXGE_ERROR | rs;
+	}
+
+	/*
+	 * Reset the tail (kick) register to 0.
+	 * (Hardware will not reset it. Tx overflow fatal
+	 * error if tail is not set to 0 after reset!
+	 */
+	TXDMA_REG_WRITE64(nep->npi_handle, TX_RING_KICK_REG, channel, 0);
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "<== nxge_reset_txdma_channel "
+			"status 0x%x", ret));
+	return (ret);
+}
+
+static void nxge_unmap_txdma(p_nxge_t nep)
+{
+	int			i, ndmas;
+	uint16_t		channel;
+	p_tx_rings_t 		tx_rings;
+	p_tx_ring_t 		*tx_desc_rings;
+	p_tx_mbox_areas_t 	tx_mbox_areas_p;
+	p_tx_mbox_t		*tx_mbox_p;
+	p_nxge_dma_pool_t	dma_cntl_poolp;
+	p_nxge_dma_common_t	dma_cntl_p;
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "==> nxge_unmap_txdma"));
+
+	ndmas = nep->max_tdcs;
+	if (!ndmas) {
+		NXGE_DEBUG_MSG((nep, TX_CTL, "nxge_unmap_txdma: "
+				"No DMAs allocated"));
+		return;
+	}
+
+	dma_cntl_poolp = nep->tx_cntl_pool_p;
+	if (!dma_cntl_poolp->buf_allocated) {
+		NXGE_DEBUG_MSG((nep, TX_CTL, "nxge_unmap_txdma: "
+				"No Cntl pool buffers allocated"));
+		return;
+	}
+
+	dma_cntl_p = dma_cntl_poolp->dma_buf_pool_p;
+	tx_rings = nep->tx_rings;
+	tx_desc_rings = tx_rings->rings;
+	tx_mbox_areas_p = nep->tx_mbox_areas_p;
+	tx_mbox_p = tx_mbox_areas_p->txmbox_areas;
+
+	for (i = 0; i < ndmas; i++) {
+		/*
+		 * Set up and prepare descriptors and mailbox.
+		 */
+		channel = dma_cntl_p[i].dma_channel;
+#if defined(USE_TXHDR_BUFFER)
+		nxge_uninit_tx_buf_ring(nep, channel, i);
+#endif
+		nxge_uninit_tx_cntl_ring(nep, channel, i);
+	}
+
+	KMEM_FREE(tx_mbox_p, sizeof (p_tx_mbox_t) * ndmas);
+	KMEM_FREE(tx_mbox_areas_p, sizeof (tx_mbox_areas_t));
+	KMEM_FREE(tx_desc_rings, sizeof (p_tx_ring_t) * ndmas);
+	KMEM_FREE(tx_rings, sizeof (tx_rings_t));
+
+	nep->tx_rings = NULL;
+	nep->tx_mbox_areas_p = NULL;
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "<== nxge_unmap_txdma"));
+	return;
+}
+
+#ifdef USE_TXHDR_BUFFER
+
+static int nxge_alloc_tx_buf_pool(p_nxge_t nep, int ring,  uint16_t channel, uint32_t buffer_size)
+{
+	int			status = NXGE_OK;
+	int			i, j, k;
+	p_tx_ring_t 		tx_ring_p;
+	p_nxge_dma_buf_t	dma_buf_p;
+	size_t			total_ring_buf_size;
+	int			chunk_cnt;
+	int			blks_in_chunk;
+	uint32_t block_size;
+#ifdef USE_CONSISTENT_TXHDR_BUFFER
+	uint64_t dma_addr;
+	uint64_t cpu_addr;
+#endif
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "==> nxge_alloc_rx_buf_pool"));
+
+	tx_ring_p = nep->tx_rings->rings[ring];
+	total_ring_buf_size = buffer_size * tx_ring_p->ring_size;
+
+#ifdef USE_CONSISTENT_TXHDR_BUFFER
+	tx_ring_p->cp_cpu_addr = pci_alloc_consistent(nep->pdev,
+										  total_ring_buf_size,
+										  &tx_ring_p->cp_dma_addr);
+
+	dma_addr = (uint64_t ) tx_ring_p->cp_dma_addr;
+	cpu_addr = (uint64_t ) tx_ring_p->cp_cpu_addr;
+
+	for (i = 0; i < tx_ring_p->ring_size; i++) {
+		tx_ring_p->tx_msg_ring[i].cp_cpu_addr = (caddr_t) cpu_addr;
+		tx_ring_p->tx_msg_ring[i].cp_dma_addr = (dma_addr_t) dma_addr;
+		dma_addr += buffer_size;
+		cpu_addr += buffer_size;
+	}
+
+#endif
+#ifdef USE_STREAMING_TXHDR_BUFFER
+
+	chunk_cnt = total_ring_buf_size / nep->page_size;
+
+	dma_buf_p = (p_nxge_dma_buf_t) KMEM_ZALLOC(sizeof (nxge_dma_buf_t) *
+						   chunk_cnt, GFP_KERNEL);
+
+	if (dma_buf_p == NULL) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+				"<== nxge_alloc_rx_buf_pool: "
+				"kmalloc failed for dma_buf"));
+		return (NXGE_ERROR);
+	}
+
+
+	tx_ring_p->chunk_cnt = chunk_cnt;
+	block_size = nep->page_size;
+	/* allocate the page chunks. each chunk is a bunch of contiguous pages */
+	for (i = 0; i < chunk_cnt; i++) {
+		dma_buf_p[i].page_p = alloc_pages(GFP_ATOMIC, nep->page_order);
+		if (dma_buf_p[i].page_p == NULL) {
+			NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+					"<== nxge_alloc_tx_buf_pool: "
+					"alloc_pages failed for order %d, "
+					"chunk %d",
+					nep->page_order, i));
+			goto nxge_alloc_tx_buf_pool_err;
+		}
+		dma_buf_p[i].dma_addr = pci_map_page(nep->pdev,
+						     dma_buf_p[i].page_p,
+						     0, nep->page_size,
+						     PCI_DMA_TODEVICE);
+		dma_buf_p[i].dma_channel = channel;
+		dma_buf_p[i].alloc_len = nep->page_size;
+		dma_buf_p[i].block_size = block_size;
+		dma_buf_p[i].dma_chunk_index = i;
+
+	}
+
+	tx_ring_p->dma_buf_p = dma_buf_p;
+
+	blks_in_chunk = nep->page_size / buffer_size;
+
+	/* break up the page chunks to blocks */
+
+	i = 0;
+	for (j = 0; j < chunk_cnt; j++) {
+		for (k = 0; k < blks_in_chunk; k++, i++) {
+			tx_ring_p->tx_msg_ring[i].cp_cpu_addr =
+				((caddr_t)page_address(dma_buf_p[j].page_p)) +
+				(k * buffer_size);
+			tx_ring_p->tx_msg_ring[i].cp_dma_addr = dma_buf_p[j].dma_addr +
+				(k * buffer_size);
+		}
+	}
+
+
+	goto nxge_alloc_tx_buf_pool_exit;
+
+nxge_alloc_tx_buf_pool_err:
+
+	status = NXGE_ERROR;
+	for (i--; i >= 0; i--) {
+		pci_unmap_page(nep->pdev, dma_buf_p[i].dma_addr,
+			       nep->page_size, PCI_DMA_FROMDEVICE);
+		__free_pages(dma_buf_p[i].page_p, nep->page_order);
+
+	}
+	KMEM_FREE(dma_buf_p, sizeof (nxge_dma_buf_t) *
+						   chunk_cnt);
+
+#endif
+
+nxge_alloc_tx_buf_pool_exit:
+	NXGE_DEBUG_MSG((nep, RX_CTL, "<== nxge_alloc_tx_buf_pool status 0x%x",
+			status));
+	return (status);
+
+}
+#endif
+
+static int nxge_map_txdma(p_nxge_t nep)
+{
+	int			i, ndmas;
+	uint16_t		channel;
+	p_tx_rings_t 		tx_rings;
+	p_tx_ring_t 		*tx_desc_rings;
+	p_tx_mbox_areas_t 	tx_mbox_areas_p;
+	p_tx_mbox_t		*tx_mbox_p;
+	p_nxge_dma_pool_t	dma_cntl_poolp;
+	p_nxge_dma_common_t	dma_cntl_p;
+	int			status = NXGE_OK;
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "==> nxge_map_txdma"));
+
+	ndmas = nep->max_tdcs;
+	if (!ndmas) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_map_txdma: "
+				"No DMAs allocated"));
+		return (NXGE_ERROR);
+	}
+
+	dma_cntl_poolp = nep->tx_cntl_pool_p;
+	if (!dma_cntl_poolp->buf_allocated) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_map_txdma: "
+				"No Cntl pool buffers allocated"));
+		return (NXGE_ERROR);
+	}
+
+	dma_cntl_p = dma_cntl_poolp->dma_buf_pool_p;
+
+	tx_rings = (p_tx_rings_t) KMEM_ZALLOC(sizeof (tx_rings_t), GFP_KERNEL);
+	if (tx_rings == NULL)
+		goto fail1;
+	tx_desc_rings = (p_tx_ring_t *) KMEM_ZALLOC(sizeof (p_tx_ring_t) * ndmas, GFP_KERNEL);
+	if (tx_desc_rings == NULL)
+		goto fail2;
+
+	tx_mbox_areas_p = (p_tx_mbox_areas_t)
+		KMEM_ZALLOC(sizeof (tx_mbox_areas_t), GFP_KERNEL);
+	if (tx_mbox_areas_p == NULL)
+		goto fail3;
+	tx_mbox_p = (p_tx_mbox_t *) KMEM_ZALLOC(sizeof (p_tx_mbox_t) * ndmas,
+						GFP_KERNEL);
+	if (tx_mbox_p == NULL)
+		goto fail4;
+
+	tx_rings->rings = tx_desc_rings;
+	nep->tx_rings = tx_rings;
+
+	tx_mbox_areas_p->txmbox_areas = tx_mbox_p;
+	nep->tx_mbox_areas_p = tx_mbox_areas_p;
+
+	for (i = 0; i < ndmas; i++) {
+		/*
+		 * Set up and prepare descriptors and mailbox.
+		 */
+		channel = dma_cntl_p[i].dma_channel;
+
+		status = nxge_init_tx_cntl_ring(nep, channel, i);
+
+		if (status != NXGE_OK) {
+			NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_map_txdma: "
+					"nxge_init_tx_cntl_ring failed"
+					" for channel %d i %d",
+					channel, i));
+			goto nxge_init_txdma_fail1;
+		}
+#ifdef USE_TXHDR_BUFFER
+		status = nxge_alloc_tx_buf_pool(nep, i, channel,
+										NXGE_TX_COPY_BUFFER_SIZE);
+#endif
+
+	}
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "<== nxge_map_txdma"));
+	return (status);
+
+nxge_init_txdma_fail1:
+
+	i--;
+	for (; i >= 0; i--) {
+		channel = dma_cntl_p[i].dma_channel;
+		nxge_uninit_tx_cntl_ring(nep, channel, i);
+	}
+	KMEM_FREE(tx_mbox_p, sizeof (p_tx_mbox_t) * ndmas);
+fail4:
+	KMEM_FREE(tx_mbox_areas_p, sizeof (tx_mbox_areas_t));
+fail3:
+	KMEM_FREE(tx_desc_rings, sizeof (p_tx_ring_t) * ndmas);
+fail2:
+	KMEM_FREE(tx_rings, sizeof (tx_rings_t));
+fail1:
+	nep->tx_rings = NULL;
+	nep->tx_mbox_areas_p = NULL;
+
+	NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "<== nxge_map_txdma, err status %d",
+			status));
+	return (status);
+
+}
+
+static int
+nxge_txdma_hw_mode(p_nxge_t nep, boolean_t enable)
+{
+	int			i, ndmas;
+	int			status = NXGE_OK;
+	uint16_t		channel;
+	p_tx_rings_t 		tx_rings;
+	p_tx_ring_t 		*tx_desc_rings;
+	npi_handle_t		handle;
+
+	NXGE_DEBUG_MSG((nep, TX_CTL,
+		"==> nxge_txdma_hw_mode: enable mode %d", enable));
+
+	/* XXXX: lock */
+	if (!nep->drv_state) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+			"<== nxge_txdma_mode: not initialized"));
+		return (NXGE_ERROR);
+	}
+
+	tx_rings = nep->tx_rings;
+	if (tx_rings == NULL) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+			"<== nxge_txdma_hw_mode: NULL ring pointer"));
+		return (NXGE_ERROR);
+	}
+
+	tx_desc_rings = tx_rings->rings;
+	if (tx_desc_rings == NULL) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+			"<== nxge_txdma_hw_mode: NULL rings pointer"));
+		return (NXGE_ERROR);
+	}
+
+	ndmas = nep->max_tdcs;
+	if (!ndmas) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+			"<== nxge_txdma_hw_mode: no dma channel allocated"));
+		return (NXGE_ERROR);
+	}
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "==> nxge_txdma_hw_mode: "
+		"tx_rings $%p tx_desc_rings $%p ndmas %d",
+		tx_rings, tx_desc_rings, ndmas));
+
+	handle = nep->npi_handle;
+	for (i = 0; i < ndmas; i++) {
+		if (nep->tx_cntl_pool_p->dma_buf_pool_p == NULL ||
+		    nep->tx_cntl_pool_p->buf_allocated == B_FALSE)
+			continue;
+		channel = nep->tx_cntl_pool_p->dma_buf_pool_p[i].dma_channel;
+		NXGE_DEBUG_MSG((nep, TX_CTL,
+			"nxge_txdma_hw_mode: channel %d", channel));
+		if (enable) {
+			NXGE_DEBUG_MSG((nep, TX_CTL,
+				"nxge_txdma_hw_mode: channel %d (enable)",
+				channel));
+			status = npi_txdma_channel_enable(handle, channel);
+		} else {
+			/*
+			 * Stop the dma channel and waits for the stop done.
+			 * If the stop done bit is not set, then force
+			 * an error so TXC will stop.
+			 * All channels bound to this port need to be stopped
+			 * and reset after injecting an interrupt error.
+			 */
+			NXGE_DEBUG_MSG((nep, TX_CTL,
+				"nxge_txdma_hw_mode: channel %d (disable)",
+				channel));
+			status = npi_txdma_channel_disable(handle, channel);
+			{
+				tdmc_intr_dbg_t		intr_dbg;
+
+				if (status != NPI_SUCCESS) {
+					/* Inject any error */
+					intr_dbg.value = 0;
+					intr_dbg.bits.ldw.nack_pref = 1;
+					NXGE_DEBUG_MSG((nep, TX_CTL,
+						"==> nxge_txdma_hw_mode: "
+						"channel %d (stop failed 0x%x) "
+						"(inject err)", channel,
+							status));
+					(void) npi_txdma_inj_int_error_set(
+						handle, channel, &intr_dbg);
+					status = npi_txdma_channel_disable(handle,
+						channel);
+					NXGE_DEBUG_MSG((nep, TX_CTL,
+						"==> nxge_txdma_hw_mode: "
+						"channel %d (stop again 0x%x) "
+						"(after inject err)",
+						channel, status));
+				}
+			}
+		}
+	}
+
+	status = ((status == NPI_SUCCESS) ? NXGE_OK : NXGE_ERROR);
+
+	NXGE_DEBUG_MSG((nep, TX_CTL,
+		"<== nxge_txdma_hw_mode: status 0x%x", status));
+
+	return (status);
+}
+
+static void
+nxge_txdma_hw_stop_common(p_nxge_t nep)
+{
+	NXGE_DEBUG_MSG((nep, TX_CTL, "==> nxge_txdma_hw_stop_common"));
+
+	/*
+	 * Load the sharable parameters by writing to the
+	 * function zero control registers. These FZC registers
+	 * should be initialized only once for the entire chip.
+	 */
+#if 0
+	nxge_init_fzc_tx_common(nep);
+#endif
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "<== nxge_txdma_hw_stop_common"));
+}
+
+static int
+nxge_txdma_hw_start_common(p_nxge_t nep)
+{
+	int		status = NXGE_OK;
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "==> nxge_txdma_hw_start_common"));
+
+	/*
+	 * Load the sharable parameters by writing to the
+	 * function zero control registers. These FZC registers
+	 * should be initialized only once for the entire chip.
+	 */
+	nxge_init_fzc_tx_common(nep);
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "<== nxge_txdma_hw_start_common"));
+
+	return (status);
+}
+
+static int
+nxge_txdma_start_channel(p_nxge_t nep, uint16_t channel,
+    p_tx_ring_t tx_ring_p, p_tx_mbox_t tx_mbox_p)
+
+{
+	int			status = NXGE_OK;
+
+	NXGE_DEBUG_MSG((nep, TX_CTL,
+		"==> nxge_txdma_start_channel (channel %d)", channel));
+
+	/*
+	 * TXDMA/TXC must be in stopped state.
+	 */
+	(void) nxge_txdma_stop_inj_err(nep, channel);
+	/*
+	 * Reset TXDMA channel
+	 */
+	tx_ring_p->tx_cs.value = 0;
+	tx_ring_p->tx_cs.bits.ldw.rst = 1;
+	status = nxge_reset_txdma_channel(nep, channel,
+					  tx_ring_p->tx_cs.value);
+
+	if (status != NXGE_OK) {
+		goto nxge_txdma_start_channel_exit;
+	}
+	/*
+	 * Initialize the TXDMA channel specific FZC control
+	 * configurations. These FZC registers are pertaining
+	 * to each TX channel (i.e. logical pages).
+	 */
+	status = nxge_init_fzc_txdma_channel(nep, channel,
+					     tx_ring_p, tx_mbox_p);
+	if (status != NXGE_OK) {
+		goto nxge_txdma_start_channel_exit;
+	}
+
+	/*
+	 * Initialize the event masks.
+	 */
+	tx_ring_p->tx_evmask.value = 0;
+	/* Set up the interrupt event masks. */
+	status = npi_txdma_event_mask(nep->npi_handle, OP_SET, channel,
+				      &tx_ring_p->tx_evmask);
+	if (status != NPI_SUCCESS) {
+		goto nxge_txdma_start_channel_exit;
+	}
+
+
+	/*
+	 * Load TXDMA descriptors, buffers, mailbox,
+	 * initialise the DMA channels and
+	 * enable each DMA channel.
+	 */
+	status = nxge_enable_txdma_channel(nep, channel,
+					   tx_ring_p, tx_mbox_p);
+	if (status != NXGE_OK) {
+		goto nxge_txdma_start_channel_exit;
+	}
+
+nxge_txdma_start_channel_exit:
+	NXGE_DEBUG_MSG((nep, TX_CTL, "<== nxge_txdma_start_channel, status"
+			       "[0x%x]", status));
+
+	return (status);
+}
+
+/*ARGSUSED*/
+static void
+nxge_txdma_stop_channel(p_nxge_t nep, uint16_t channel,
+			p_tx_ring_t tx_ring_p, p_tx_mbox_t tx_mbox_p)
+{
+	int		status = NXGE_OK;
+
+	NXGE_DEBUG_MSG((nep, TX_CTL,
+		"==> nxge_txdma_stop_channel: channel %d", channel));
+
+
+	/*
+	 * Stop (disable) TXDMA and TXC (if stop bit is set
+	 * and STOP_N_GO bit not set, the TXDMA reset state will
+	 * not be set if reset TXDMA.
+	 */
+	nxge_txdma_stop_inj_err(nep, channel);
+
+	/*
+	 * Reset TXDMA channel
+	 */
+	tx_ring_p->tx_cs.value = 0;
+	tx_ring_p->tx_cs.bits.ldw.rst = 1;
+	status = nxge_reset_txdma_channel(nep, channel,
+					  tx_ring_p->tx_cs.value);
+	if (status != NXGE_OK) {
+		goto nxge_txdma_stop_channel_exit;
+	}
+
+#ifdef HARDWARE_REQUIRED
+	/* Set up the interrupt event masks. */
+	tx_ring_p->tx_evmask.value = 0;
+	status = npi_txdma_event_mask(nep->npi_handle, OP_SET, channel,
+				      &tx_ring_p->tx_evmask);
+	if (status != NXGE_OK) {
+		goto nxge_txdma_stop_channel_exit;
+	}
+
+	/* Initialize the DMA control and status register */
+	tx_ring_p->tx_cs.value = TX_ENT_MSK_MK_ALL;
+	status = npi_txdma_control_status(nep->npi_handle, OP_SET,
+					  channel, &tx_ring_p->tx_cs);
+	if (status != NXGE_OK) {
+		goto nxge_txdma_stop_channel_exit;
+	}
+
+	/* Disable channel */
+	status = nxge_disable_txdma_channel(nep, channel,
+					    tx_ring_p, tx_mbox_p);
+	if (status != NXGE_OK) {
+		goto nxge_txdma_start_channel_exit;
+	}
+
+	NXGE_DEBUG_MSG((nep, TX_CTL,
+			"nxge_txdma_stop_channel: event done"));
+
+#endif
+
+nxge_txdma_stop_channel_exit:
+	NXGE_DEBUG_MSG((nep, TX_CTL, "<== nxge_txdma_stop_channel"));
+}
+
+
+static int
+nxge_txdma_hw_start(p_nxge_t nep)
+{
+	int			i, ndmas;
+	uint16_t		channel;
+	p_tx_rings_t 		tx_rings;
+	p_tx_ring_t 		*tx_desc_rings;
+	p_tx_mbox_areas_t 	tx_mbox_areas_p;
+	p_tx_mbox_t		*tx_mbox_p;
+	int			status = NXGE_OK;
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "==> nxge_txdma_hw_start"));
+
+	tx_rings = nep->tx_rings;
+	if (tx_rings == NULL) {
+		NXGE_DEBUG_MSG((nep, TX_CTL,
+			"<== nxge_txdma_hw_start: NULL ring pointer"));
+		return (NXGE_ERROR);
+	}
+	tx_desc_rings = tx_rings->rings;
+	if (tx_desc_rings == NULL) {
+		NXGE_DEBUG_MSG((nep, TX_CTL,
+			"<== nxge_txdma_hw_start: NULL ring pointers"));
+		return (NXGE_ERROR);
+	}
+
+	ndmas = nep->max_tdcs;
+	if (!ndmas) {
+		NXGE_DEBUG_MSG((nep, TX_CTL,
+			"<== nxge_txdma_hw_start: no dma channel allocated"));
+		return (NXGE_ERROR);
+	}
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "nxge_txdma_hw_start: "
+		"tx_rings $%p tx_desc_rings $%p ndmas %d",
+		tx_rings, tx_desc_rings, ndmas));
+
+	tx_mbox_areas_p = nep->tx_mbox_areas_p;
+	tx_mbox_p = tx_mbox_areas_p->txmbox_areas;
+
+	for (i = 0; i < ndmas; i++) {
+		channel = nep->tx_cntl_pool_p->dma_buf_pool_p[i].dma_channel;
+		status = nxge_txdma_start_channel(nep, channel,
+						  (p_tx_ring_t)tx_desc_rings[i],
+						  (p_tx_mbox_t)tx_mbox_p[i]);
+		if (status != NXGE_OK) {
+			goto nxge_txdma_hw_start_fail1;
+		}
+	}
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "nxge_txdma_hw_start: "
+		"tx_rings $%p rings $%p",
+		nep->tx_rings, nep->tx_rings->rings));
+	NXGE_DEBUG_MSG((nep, TX_CTL, "nxge_txdma_hw_start: "
+		"tx_rings $%p tx_desc_rings $%p",
+		nep->tx_rings, tx_desc_rings));
+
+	goto nxge_txdma_hw_start_exit;
+
+nxge_txdma_hw_start_fail1:
+	NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+		"nxge_txdma_hw_start: disable "
+		"(status 0x%x channel %d i %d)", status, channel, i));
+	i--;
+	for (; i >= 0; i--) {
+		channel = nep->tx_cntl_pool_p->dma_buf_pool_p[i].dma_channel;
+		nxge_txdma_stop_channel(nep, channel,
+			(p_tx_ring_t)tx_desc_rings[i],
+			(p_tx_mbox_t)tx_mbox_p[i]);
+	}
+
+nxge_txdma_hw_start_exit:
+	NXGE_DEBUG_MSG((nep, TX_CTL,
+		"<== nxge_txdma_hw_start: (status 0x%x)", status));
+
+	return (status);
+}
+
+static void
+nxge_txdma_hw_stop(p_nxge_t nep)
+{
+	int			i, ndmas;
+	uint16_t		channel;
+	p_tx_rings_t 		tx_rings;
+	p_tx_ring_t 		*tx_desc_rings;
+	p_tx_mbox_areas_t 	tx_mbox_areas_p;
+	p_tx_mbox_t		*tx_mbox_p;
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "==> nxge_txdma_hw_stop"));
+
+	tx_rings = nep->tx_rings;
+	if (tx_rings == NULL) {
+		NXGE_DEBUG_MSG((nep, TX_CTL,
+			"<== nxge_txdma_hw_stop: NULL ring pointer"));
+		return;
+	}
+	tx_desc_rings = tx_rings->rings;
+	if (tx_desc_rings == NULL) {
+		NXGE_DEBUG_MSG((nep, TX_CTL,
+			"<== nxge_txdma_hw_stop: NULL ring pointers"));
+		return;
+	}
+
+	ndmas = nep->max_tdcs;
+	if (!ndmas) {
+		NXGE_DEBUG_MSG((nep, TX_CTL,
+			"<== nxge_txdma_hw_stop: no dma channel allocated"));
+		return;
+	}
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "nxge_txdma_hw_stop: "
+		"tx_rings $%p tx_desc_rings $%p",
+		tx_rings, tx_desc_rings));
+
+	tx_mbox_areas_p = nep->tx_mbox_areas_p;
+	tx_mbox_p = tx_mbox_areas_p->txmbox_areas;
+
+	for (i = 0; i < ndmas; i++) {
+		channel = nep->tx_cntl_pool_p->dma_buf_pool_p[i].dma_channel;
+		nxge_txdma_stop_channel(nep, channel,
+					(p_tx_ring_t)tx_desc_rings[i],
+					(p_tx_mbox_t)tx_mbox_p[i]);
+	}
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "==> nxge_txdma_hw_stop: "
+		"tx_rings $%p tx_desc_rings $%p",
+		tx_rings, tx_desc_rings));
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "<== nxge_txdma_hw_stop"));
+}
+
+static void
+nxge_uninit_txdma(p_nxge_t nep)
+{
+	NXGE_DEBUG_MSG((nep, TX_CTL, "==> nxge_uninit_txdma"));
+
+	nxge_txdma_hw_stop(nep);
+	nxge_txdma_hw_stop_common(nep);
+	nxge_unmap_txdma(nep);
+
+	NXGE_DEBUG_MSG((nep, TX_CTL,
+		"<== nxge_uinit_txdma"));
+}
+
+static int nxge_init_txdma(p_nxge_t nep)
+{
+
+	int			status = NXGE_OK;
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "==> nxge_init_txdma"));
+
+	status = nxge_map_txdma(nep);
+	if (status != NXGE_OK) {
+		goto nxge_init_txdma_exit;
+	}
+
+	status = nxge_txdma_hw_start_common(nep);
+	if (status != NXGE_OK) {
+		nxge_unmap_txdma(nep);
+		goto nxge_init_txdma_exit;
+	}
+
+	status = nxge_txdma_hw_start(nep);
+	if (status != NXGE_OK) {
+		nxge_unmap_txdma(nep);
+		goto nxge_init_txdma_exit;
+	}
+
+  nxge_init_txdma_exit:
+
+	NXGE_DEBUG_MSG((nep, TX_CTL,
+		"<== nxge_init_txdma: status 0x%x", status));
+
+	return (status);
+}
+
+
+static int nxge_free_tx_cntl_pool(p_nxge_t nep)
+{
+	int			status = NXGE_OK;
+	int			i, ndmas;
+	struct pci_dev		*pdev;
+	p_nxge_dma_pool_t	dma_cntl_poolp;
+	p_nxge_dma_common_t	dma_cntl_p;
+	size_t			tx_cntl_alloc_size;
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "==> nxge_free_tx_cntl_pool"));
+
+	pdev = nep->pdev;
+	dma_cntl_poolp = nep->tx_cntl_pool_p;
+	dma_cntl_p = dma_cntl_poolp->dma_buf_pool_p;
+	ndmas = dma_cntl_poolp->ndmas;
+
+	if (dma_cntl_poolp == NULL || dma_cntl_poolp->buf_allocated == B_FALSE
+	    || dma_cntl_p == NULL) {
+		goto  nxge_free_tx_cntl_exit;
+	}
+
+	tx_cntl_alloc_size = nxge_tx_ring_size;
+	tx_cntl_alloc_size *= (sizeof (tx_desc_t));
+	tx_cntl_alloc_size += sizeof (txdma_mailbox_t);
+
+	for (i = 0; i < ndmas; i++) {
+		tx_cntl_alloc_size = dma_cntl_p[i].alloc_len;
+		NXGE_DEBUG_MSG((nep, TX_CTL, "nxge_free_tx_cntl_pool: "
+				"Channel[%d], CPU addr[0x%x], DMA addr[0x%x],"
+				" Size[0x%x]", dma_cntl_p[i].dma_channel,
+				dma_cntl_p[i].cpu_addr,
+				dma_cntl_p[i].dma_addr,
+				tx_cntl_alloc_size));
+		NXGE_DEBUG_MSG((nep, TX_CTL, "nxge_free_tx_cntl_pool: "
+				" Size[0x%x]", tx_cntl_alloc_size));
+
+		pci_free_consistent(pdev, tx_cntl_alloc_size,
+				    (void *)(dma_cntl_p[i].cpu_addr),
+				    dma_cntl_p[i].dma_addr);
+	}
+
+nxge_free_tx_cntl_exit:
+	if (dma_cntl_p != NULL)
+		KMEM_FREE(dma_cntl_p, sizeof (nxge_dma_common_t) * ndmas);
+
+	if (dma_cntl_poolp != NULL)
+		KMEM_FREE(dma_cntl_poolp, sizeof (nxge_dma_pool_t));
+
+	nep->tx_cntl_pool_p = NULL;
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "<== nxge_free_tx_cntl_pool: "
+		       "status 0x%x", status));
+
+	return (status);
+}
+
+static int nxge_alloc_tx_cntl_pool(p_nxge_t nep)
+{
+	int			status = NXGE_OK;
+	int			i, ndmas, st_tdc;
+	struct pci_dev		*pdev;
+	p_nxge_dma_pool_t	dma_cntl_poolp;
+	p_nxge_dma_common_t	dma_cntl_p;
+	size_t			tx_cntl_alloc_size;
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "==> nxge_alloc_tx_cntl_pool"));
+
+	pdev = nep->pdev;
+	st_tdc = nep->start_tdc;
+	ndmas = nep->max_tdcs;
+
+	/*
+	 * Allocate memory for each transmit DMA channel.
+	 */
+	dma_cntl_poolp = (p_nxge_dma_pool_t)
+		KMEM_ZALLOC(sizeof (nxge_dma_pool_t), GFP_KERNEL);
+	if (dma_cntl_poolp == NULL) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_alloc_tx_cntl_pool: "
+			       "alloc failed for cntl pool"));
+		goto nxge_alloc_tx_cntl_fail1;
+	}
+
+	dma_cntl_p = (p_nxge_dma_common_t)
+		KMEM_ZALLOC(sizeof (nxge_dma_common_t) * ndmas, GFP_KERNEL);
+	if (dma_cntl_p == NULL) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_alloc_tx_cntl_pool: "
+			       "alloc failed for cntl pool blocks"));
+		goto nxge_alloc_tx_cntl_fail2;
+	}
+
+	/*
+	 * Addresses of transmit descriptor ring and the
+	 * mailbox must be all cache-aligned (64 bytes).
+	 */
+	tx_cntl_alloc_size = nxge_tx_ring_size;
+	tx_cntl_alloc_size *= (sizeof (tx_desc_t));
+	tx_cntl_alloc_size += sizeof (txdma_mailbox_t);
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "nxge_alloc_tx_cntl_pool: "
+			"Cntl pool size [0x%x] [%d]", tx_cntl_alloc_size,
+			tx_cntl_alloc_size));
+
+	/*
+	 * Allocate memory for descriptor rings and mailbox.
+	 */
+	for (i = 0; i < ndmas; i++) {
+		void	*addr;
+
+		dma_cntl_p[i].cpu_addr = NULL;
+
+		addr =  pci_alloc_consistent(pdev, tx_cntl_alloc_size,
+					     &dma_cntl_p[i].dma_addr);
+
+		if(addr == NULL) {
+			goto nxge_alloc_tx_cntl_fail3;
+		}
+
+		dma_cntl_p[i].cpu_addr = (caddr_t)addr;
+
+		dma_cntl_p[i].dma_channel = st_tdc;
+		dma_cntl_p[i].alloc_len = tx_cntl_alloc_size;
+		NXGE_DEBUG_MSG((nep, TX_CTL, "nxge_alloc_tx_cntl_pool: "
+				"Cntl pool size [0x%x]",
+				dma_cntl_p[i].alloc_len));
+
+		NXGE_DEBUG_MSG((nep, TX_CTL, "nxge_alloc_tx_cntl_pool: "
+				"Channel[%d], CPU addr[0x%llx], "
+				"DMA addr[0x%llx], Bus addr [0x%llx]"
+				" Size[0x%x]", st_tdc,
+				dma_cntl_p[i].cpu_addr,
+				virt_to_bus(dma_cntl_p[i].cpu_addr),
+				dma_cntl_p[i].dma_addr,
+				tx_cntl_alloc_size));
+
+		NXGE_DEBUG_MSG((nep, TX_CTL, "nxge_alloc_tx_cntl_pool: "
+				"Cntl pool size [0x%x]", tx_cntl_alloc_size));
+		st_tdc++;
+	}
+
+	dma_cntl_poolp->ndmas = ndmas;
+	dma_cntl_poolp->buf_allocated = B_TRUE;
+	nep->tx_cntl_pool_p = dma_cntl_poolp;
+	dma_cntl_poolp->dma_buf_pool_p = dma_cntl_p;
+
+	goto nxge_alloc_tx_cntl_exit;
+
+nxge_alloc_tx_cntl_fail3:
+	/* Free control buffers */
+	i--;
+	for (; i >= 0; i--) {
+		pci_free_consistent(pdev, tx_cntl_alloc_size,
+				    (void *)(dma_cntl_p[i].cpu_addr),
+				    dma_cntl_p[i].dma_addr);
+	}
+
+	KMEM_FREE(dma_cntl_p, sizeof (nxge_dma_common_t) * ndmas);
+
+nxge_alloc_tx_cntl_fail2:
+	KMEM_FREE(dma_cntl_poolp, sizeof (nxge_dma_pool_t));
+
+nxge_alloc_tx_cntl_fail1:
+	status = NXGE_ERROR;
+
+nxge_alloc_tx_cntl_exit:
+	NXGE_DEBUG_MSG((nep, TX_CTL, "<== nxge_alloc_tx_cntl_pool: "
+		       "status 0x%x", status));
+
+	return (status);
+}
+
+
+static int nxge_clean_tx(p_nxge_t nep)
+{
+	int status = NXGE_OK;
+	int i, j;
+	p_tx_ring_t tx_ring_p;
+	p_tx_msg_t tx_msg_p;
+	unsigned long flags;
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "==> nxge_clean_tx"));
+
+	for (i = 0; i < nep->max_tdcs; i++) {
+		tx_ring_p = nep->tx_rings->rings[i];
+		MUTEX_ENTER_INT(&tx_ring_p->lock, flags);
+		for (j = 0; j < tx_ring_p->ring_size; j++) {
+			tx_msg_p = &(tx_ring_p->tx_msg_ring[j]);
+			if (tx_msg_p == NULL)
+				continue;
+			if (tx_msg_p->addr != 0) {
+				pci_unmap_page(nep->pdev, tx_msg_p->addr,
+				       tx_msg_p->length, PCI_DMA_TODEVICE);
+
+				NXGE_DEBUG_MSG((nep, TX_CTL, "nxge_clean_tx"
+						       ": freeing skb [%p]",
+						tx_msg_p->buf));
+			}
+			if (tx_msg_p->buf != NULL) {
+				p_skb_hdr_info_t p_hdr = (skb_hdr_info_t *) tx_msg_p->buf->cb;
+				if (p_hdr->mappings_len) {
+					int mapping;
+					p_skb_page_mapping_info_t skmap_dpage_info;
+					p_skb_mapping_info_t skmap_info;
+					skmap_info = (p_skb_mapping_info_t) p_hdr->map_info;
+					skmap_dpage_info =
+						(p_skb_page_mapping_info_t)skmap_info->data_buffer_mapping_info;
+					for (mapping = 0; mapping  < skmap_info->mappings; mapping++)
+						pci_unmap_page(nep->pdev, skmap_dpage_info[mapping].addr,
+									   skmap_dpage_info[mapping].length,
+									   PCI_DMA_TODEVICE);
+
+					pci_unmap_single(nep->pdev,
+									 skmap_info->hdr_buffer_dma_addr,
+									 skmap_info->hdr_buffer_length,
+									 PCI_DMA_TODEVICE);
+
+					KMEM_FREE(p_hdr->map_info, p_hdr->mappings_len);
+					p_hdr->mappings_len = 0;
+				}
+				dev_kfree_skb_any(tx_msg_p->buf);
+				tx_msg_p->buf = NULL;
+			}
+		}
+
+		memset(tx_ring_p->tx_msg_ring, 0,
+		       tx_ring_p->ring_size * sizeof (tx_msg_t));
+		MUTEX_EXIT_INT(&tx_ring_p->lock, flags);
+	}
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "<== nxge_clean_tx"));
+	return (status);
+}
+
+int nxge_tx_reclaim(p_nxge_t nep, p_tx_ring_t tx_ring_p,
+				 int nbufs)
+{
+	uint8_t		status = NXGE_TX_RECLAIM_OK;
+	tx_ring_hdl_t		tx_head;
+	uint_t			tx_out_index;
+	uint16_t		head_index, tail_index;
+	boolean_t		head_wrap, tail_wrap;
+	uint64_t descs_pending;
+
+	p_nxge_tx_ring_stats_t	tdc_stats = tx_ring_p->tdc_stats;
+
+	p_tx_msg_t 		tx_msg_p;
+
+	tail_index = tx_ring_p->in;
+	tail_wrap = tx_ring_p->in_wrap;
+	tx_out_index = tx_ring_p->out;
+		/*
+		 * Read the hardware maintained transmit head
+		 * and wrap around bit.
+		 */
+
+	TXDMA_REG_READ64(nep->npi_handle,
+					 TX_RING_HDL_REG, tx_ring_p->tdc, &tx_head.value);
+	head_index =  tx_head.bits.ldw.head;
+	head_wrap = tx_head.bits.ldw.wrap;
+
+	NXGE_DEBUG_MSG((nep, TX_CTL,
+					"tail_index %d, tail_wrap %d, "
+					"tx_out_index %d, head_index %d, "
+					"head_wrap %d",
+					tail_index, tail_wrap, tx_out_index,
+					head_index, head_wrap));
+
+
+	if (head_index == tail_index) {
+		if (TX_RING_EMPTY(head_index, head_wrap, tail_index,
+						  tail_wrap) &&
+			(head_index == tx_out_index)) {
+			NXGE_DEBUG_MSG((nep, NXGE_ERR_CTL,
+						"nxge_tx_reclaim: EMPTY"));
+			return (NXGE_TX_RECLAIM_OK);
+		}
+
+		if (TX_RING_FULL(head_index, head_wrap, tail_index,
+						 tail_wrap)) {
+			NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+							"nxge_tx_reclaim: FULL"));
+			return (NXGE_TX_RECLAIM_FAIL);
+		}
+	}
+
+	descs_pending = tx_ring_p->descs_pending;
+	while ((tx_out_index != head_index) && descs_pending) {
+
+		tx_msg_p = &(tx_ring_p->tx_msg_ring[tx_out_index]);
+		if (tx_msg_p->addr) {
+			pci_unmap_page(nep->pdev, tx_msg_p->addr,
+						   tx_msg_p->length, PCI_DMA_TODEVICE);
+			tx_msg_p->addr = 0;
+		}
+
+		if (tx_msg_p->buf != NULL) {
+			p_skb_hdr_info_t p_hdr = (skb_hdr_info_t *) tx_msg_p->buf->cb;
+			if (p_hdr->mappings_len) {
+				int mapping;
+				p_skb_page_mapping_info_t skmap_dpage_info;
+				p_skb_mapping_info_t skmap_info;
+				skmap_info = (p_skb_mapping_info_t) p_hdr->map_info;
+				skmap_dpage_info =
+					(p_skb_page_mapping_info_t)skmap_info->data_buffer_mapping_info;
+				for (mapping = 0; mapping  < skmap_info->mappings; mapping++)
+					pci_unmap_page(nep->pdev, skmap_dpage_info[mapping].addr,
+								   skmap_dpage_info[mapping].length,
+								   PCI_DMA_TODEVICE);
+
+				pci_unmap_single(nep->pdev,
+							   skmap_info->hdr_buffer_dma_addr,
+							   skmap_info->hdr_buffer_length,
+							   PCI_DMA_TODEVICE);
+
+				KMEM_FREE(p_hdr->map_info, p_hdr->mappings_len);
+				p_hdr->mappings_len = 0;
+			}
+			dev_kfree_skb_any(tx_msg_p->buf);
+			tx_msg_p->buf = NULL;
+		}
+		tx_out_index = DESC_NEXT_IDX(tx_ring_p, tx_out_index);
+		descs_pending--;
+	}
+
+	tx_ring_p->descs_pending = descs_pending;
+	tx_ring_p->out = tx_out_index;
+	tdc_stats->tx_descs_pending = tx_ring_p->descs_pending;
+	status = (nbufs <= (tx_ring_p->ring_size -
+						(tx_ring_p->descs_pending + TX_FULL_MARK))) ?
+
+		NXGE_TX_RECLAIM_OK : NXGE_TX_RECLAIM_FAIL;
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "<== nxge_tx_reclaim status = 0x%x, "
+			"TX ring size = %d, pending %d, TX_FULL_MARK %d "
+			"tx_out_index = %d",
+			status, tx_ring_p->ring_size,
+			tx_ring_p->descs_pending, TX_FULL_MARK,
+			tx_ring_p->out));
+
+	return (status);
+}
+
+static void nxge_tx_softintr(unsigned long neptr)
+{
+	p_nxge_t nep = (p_nxge_t)neptr;
+	if (nep->tx_softintr_sched == B_TRUE) {
+		if (netif_queue_stopped(nep->dev)) {
+			netif_wake_queue(nep->dev);
+		}
+		nep->tx_softintr_sched = B_FALSE;
+	}
+}
+
+#ifdef  NXGE_USE_NXGE_GSO
+
+#define NXGE_GSO_TEMPLATE_SIZE 128
+#define NXGE_GSO_TEMPLATE_ALLOC_SIZE(x) (x << 2)
+#define MAPPING_INFO_HDR NXGE_TX_COPY_BUFFER_SIZE
+#define GSO_HDR_BUFFER_LEN NXGE_TX_COPY_BUFFER_SIZE
+#define NXGE_ETHER_IP_OFFSET 2
+
+
+int nxge_tx_ring_gso(p_nxge_t nep, int ring,
+				struct sk_buff *skb)
+{
+	struct net_device	*dev = nep->dev;
+	p_tx_ring_t		tx_ring_p = TX_RINGN(nep, ring);
+	p_tx_msg_t		tx_msg_p;
+	unsigned long		flags;
+	int			entry, nr_frags, i, sop_index, last_entry;
+	uint32_t		len, hdr_data_len = 0;
+	uint64_t		pkt_len, buf_len;
+	uint32_t		offset = 0;
+	int			status = NETDEV_TX_OK;
+	uint64_t		pkthdr_len = 0;
+	uint64_t 		mark_mode = 0x0ULL;
+	uint64_t		ngathers = 0;
+	npi_handle_t		handle;
+	p_tx_desc_t 		tx_desc_ptr;
+	tx_ring_kick_t		kick;
+	int padbytes = 0;
+	int pads;
+	int last_len = 0;
+	uint8_t reclaim_status = B_TRUE;
+	void			*hdr_ptr = NULL;
+	uint64_t		sop_data_len;
+	uint64_t hdr_csum_value;
+	int min_descs;
+
+	int info_alloc_size;
+	uint64_t *desc_value;
+	void *nxge_gdesc = NULL;
+
+	uint8_t *template;
+
+	uint8_t *tptr, *tptr_first, *tptr_last;
+	p_skb_hdr_info_t p_hdr;
+	dma_addr_t sop_addr;
+	int p_hdr_len = 0;
+	int l3_hdr_len = 0;
+	int l4_hdr_len = 0;
+	int l3l4_hdr_len = 0;
+	int mss;
+	int l4_payload_len;
+	int l4_len;
+	int desc_index;
+	int data_descs = 0;
+	int frame_payload_remain;
+	int header_buffers = 0;
+	int payload_length;
+
+	uint32_t sequence;
+	uint16_t ip_id;
+	uint32_t saddr;
+	uint32_t daddr;
+	uint8_t ihl;
+	struct iphdr	*iphdrp = NULL;
+	struct iphdr	*iphdrp_first = NULL;
+	struct iphdr	*iphdrp_last = NULL;
+	struct tcphdr   *tcphdrp = NULL;
+	struct tcphdr   *tcphdrp_first = NULL;
+	struct tcphdr   *tcphdrp_last = NULL;
+	struct iphdr	*iph = NULL;
+	struct tcphdr   *tcph = NULL;
+	uint8_t *eptr;
+	uint8_t *hdr_cpu_addr;
+	uint8_t *hdr_dma_addr;
+	int skb_hdr_len;
+	int segmented_bytes = 0;
+	int frame_count = 0;
+	int mapping_info_alloc_length;
+	int mapped_skb_pages;
+	int frame_index = 0;
+	int max_frame_index = 1;
+	int start_desc = 0;
+	uint16_t real_xfer_len;
+	p_skb_page_mapping_info_t skmap_dpages_info;
+	p_skb_mapping_info_t  skmap_info;
+
+
+	p_hdr = (skb_hdr_info_t *) skb->cb;
+	hdr_csum_value = p_hdr->ctl_hdrp.value;
+
+	if (tx_ring_p->descs_pending >= nep->tx_reclaim_pending) {
+		MUTEX_ENTER_INT(&tx_ring_p->lock, flags);
+		reclaim_status = nxge_tx_reclaim(nep, tx_ring_p, nxge_tx_min_free);
+		MUTEX_EXIT_INT(&tx_ring_p->lock, flags);
+		if (reclaim_status == NXGE_TX_RECLAIM_FAIL) {
+			MUTEX_ENTER_INT(&tx_ring_p->stats_lock, flags);
+			nep->statsp->tdc_stats[ring].oerrors++;
+			nep->statsp->tdc_stats[ring].tx_tso_no_desc++;
+			MUTEX_EXIT_INT(&tx_ring_p->stats_lock, flags);
+			if ((atomic_read(&nep->tx_intr_set)) &&
+				(!netif_queue_stopped(dev))) {
+				netif_stop_queue(dev);
+				NXGE_DEBUG_MSG((nep, TX_CTL,
+								"nxge_tx_ring_gso stopped after reclaim "));
+
+			}
+			return (NETDEV_TX_BUSY);
+		}
+
+	}
+
+	if (skb_header_cloned(skb)) {
+		status = pskb_expand_head(skb, 0, 0, GFP_ATOMIC);
+		if (status)
+			return status;
+	}
+
+	mss = SKB_IS_GSO(skb);
+	pkthdr_len = TX_PKT_HEADER_SIZE;
+
+	nr_frags = skb_shinfo(skb)->nr_frags;
+
+	skb_hdr_len = skb_headlen(skb);
+	l4_hdr_len = (p_hdr->data_offset - p_hdr->l4_offset);
+	l3_hdr_len = (p_hdr->l4_offset - p_hdr->l3_offset);
+
+	l3l4_hdr_len = l4_hdr_len + l3_hdr_len;
+	p_hdr_len = p_hdr->l3_offset + /* l2 len */
+		 l3l4_hdr_len; /* l3l4 hdr len */
+
+	l4_payload_len =  skb->len - p_hdr_len;
+	frame_count = SKB_GSO_SEGS(skb);
+	info_alloc_size = (frame_count << 5) /* uint64_t * 4 per frame */ +
+		NXGE_GSO_TEMPLATE_ALLOC_SIZE(NXGE_GSO_TEMPLATE_SIZE);
+
+	nxge_gdesc = KMEM_ALLOC(info_alloc_size, GFP_ATOMIC);
+
+	if (nxge_gdesc == NULL) {
+		MUTEX_ENTER_INT(&tx_ring_p->stats_lock, flags);
+		nep->statsp->tdc_stats[ring].oerrors++;
+		nep->statsp->tdc_stats[ring].tx_tso_nobuff++;
+		MUTEX_EXIT_INT(&tx_ring_p->stats_lock, flags);
+		return (NETDEV_TX_BUSY);
+
+	}
+	template = (uint8_t *) nxge_gdesc;
+	desc_value = (uint64_t *) ((uint64_t) template +
+								NXGE_GSO_TEMPLATE_ALLOC_SIZE(NXGE_GSO_TEMPLATE_SIZE));
+
+
+	tptr = (uint8_t *) template;
+	tptr += NXGE_ETHER_IP_OFFSET;
+	tptr_first = tptr + NXGE_GSO_TEMPLATE_SIZE;
+	tptr_last = tptr_first + NXGE_GSO_TEMPLATE_SIZE;
+
+	memcpy((void *)(tptr), (void *)skb->data, p_hdr_len);
+	memcpy((void *)(tptr_first), (void *)tptr, p_hdr_len);
+	memcpy((void *)(tptr_last),  (void *)tptr, p_hdr_len);
+
+	iphdrp = (struct iphdr *) (tptr + p_hdr->l3_offset);
+	tcphdrp = (struct tcphdr *)  (tptr + p_hdr->l4_offset);
+
+		/* do header for all */
+		/* do header adjustment for first */
+		/* do header adjustment for last */
+
+
+	iphdrp_first = (struct iphdr *) (tptr_first + p_hdr->l3_offset);
+	tcphdrp_first = (struct tcphdr *)  (tptr_first + p_hdr->l4_offset);
+	iphdrp_last = (struct iphdr *) (tptr_last + p_hdr->l3_offset);
+	tcphdrp_last = (struct tcphdr *)  (tptr_last + p_hdr->l4_offset);
+
+		/* adjust feilds */
+	tcphdrp->urg = tcphdrp_last->urg = 0; /* urg 1st keeps from large */
+	tcphdrp->psh = tcphdrp_first->psh = 0; /* psh last keeps from large */
+	tcphdrp->rst = tcphdrp_first->rst = 0; /* RST last keeps from large */
+	tcphdrp->fin = tcphdrp_first->fin = 0; /* fin last keeps from large */
+
+		/* ack number: all keep from large pkt */
+		/* ack bit: all keep from large pkt */
+		/* TCP Sequence # is increment by  payload in each frame */
+		/* Psuedo Checksum is recomputed for each header. The HW does
+		   the actual checksum based on the psuedo
+		*/
+/* for IP */
+		/*
+		 * Total length field is updated for each frame
+		 * it is the mss for all but the last frame.
+		 */
+		/* Frame header checksum is recomputed for each frame */
+		/* ID is increment for each frame */
+
+	data_descs = 0;
+	header_buffers = 0;
+	frame_payload_remain = mss;
+	hdr_data_len = (skb_hdr_len - p_hdr_len);
+	if (hdr_data_len > 0)
+		header_buffers = 1;
+
+
+	mapping_info_alloc_length = MAPPING_INFO_HDR;
+	mapping_info_alloc_length += frame_count * GSO_HDR_BUFFER_LEN;
+	mapped_skb_pages = nr_frags + header_buffers;
+	mapping_info_alloc_length += mapped_skb_pages *
+		sizeof(skb_page_mapping_info_t);
+	p_hdr->map_info =
+		(void *) KMEM_ALLOC(mapping_info_alloc_length,
+						 GFP_ATOMIC);
+	if (!p_hdr->map_info) {
+		KMEM_FREE(nxge_gdesc, info_alloc_size);
+		MUTEX_ENTER_INT(&tx_ring_p->stats_lock, flags);
+		nep->statsp->tdc_stats[ring].oerrors++;
+		nep->statsp->tdc_stats[ring].tx_tso_nobuff++;
+		MUTEX_EXIT_INT(&tx_ring_p->stats_lock, flags);
+		return (NETDEV_TX_BUSY);
+
+	}
+	skmap_info = (p_skb_mapping_info_t) p_hdr->map_info;
+	p_hdr->mappings_len = mapping_info_alloc_length;
+	skmap_info->mappings =  nr_frags + header_buffers;
+	skmap_info->hdr_buffer_length = frame_count * GSO_HDR_BUFFER_LEN;
+	skmap_info->hdr_buffer_cpu_addr = (uint64_t)p_hdr->map_info +
+		MAPPING_INFO_HDR;
+	skmap_info->data_buffer_mapping_info =
+		(p_skb_page_mapping_info_t)((uint64_t)skmap_info->hdr_buffer_cpu_addr +
+								skmap_info->hdr_buffer_length);
+
+	skmap_dpages_info = (p_skb_page_mapping_info_t)
+		skmap_info->data_buffer_mapping_info;
+
+	skmap_info->hdr_buffer_dma_addr =
+		pci_map_single(nep->pdev,
+					   (void *)skmap_info->hdr_buffer_cpu_addr,
+					   skmap_info->hdr_buffer_length,
+					   PCI_DMA_TODEVICE);
+
+
+	hdr_cpu_addr = (uint8_t *)skmap_info->hdr_buffer_cpu_addr;
+	hdr_dma_addr = (uint8_t *)skmap_info->hdr_buffer_dma_addr;
+	eptr = tptr_first;
+	iph = iphdrp_first;
+	tcph = tcphdrp_first;
+	ngathers = 1ULL;
+	payload_length = mss;
+	padbytes = NXGE_ETHER_IP_OFFSET;
+	pkthdr_len += padbytes;
+	pads = (padbytes >> 1);
+
+	sop_data_len = pkthdr_len + p_hdr_len;
+	iph->tot_len = htons(payload_length + l3l4_hdr_len); /*first */
+	iphdrp->tot_len = htons(payload_length + l3l4_hdr_len); /*middle */
+	pkt_len = sop_data_len + payload_length;
+	real_xfer_len = pkt_len - TX_PKT_HEADER_SIZE;
+	saddr = iph->saddr;
+	daddr = iph->daddr;
+	ihl = iph->ihl;
+	ip_id = ntohs(iphdrp->id);
+	sequence = ntohl(tcphdrp->seq);
+	l4_len = payload_length + l4_hdr_len;
+	frame_index = 0;
+	max_frame_index = frame_count  - 1;
+
+	pci_dma_sync_single_for_cpu(nep->pdev, skmap_info->hdr_buffer_dma_addr,
+								skmap_info->hdr_buffer_length,
+								   PCI_DMA_TODEVICE);
+
+	if (header_buffers) {
+			/* skb->data contains data as well */
+
+		skmap_dpages_info[0].length = skb_hdr_len;
+		skmap_dpages_info[0].addr = pci_map_page(nep->pdev,
+					      virt_to_page(skb->data),
+					      offset_in_page(skb->data),
+					      skb_hdr_len, PCI_DMA_TODEVICE);
+
+		len = hdr_data_len;
+		offset = p_hdr_len;
+
+		while (len) {
+
+			if (frame_payload_remain == mss) {
+				start_desc = data_descs;
+				data_descs++;
+			}
+			ngathers++;
+			if ((len >= frame_payload_remain) &&
+				(frame_payload_remain <= TX_MAX_TRANSFER_LENGTH)){
+				buf_len = frame_payload_remain;
+				desc_value[data_descs] =
+					(buf_len << TX_PKT_DESC_TR_LEN_SHIFT) |
+					(dma_addr_t) ((uint64_t)skmap_dpages_info[0].addr + offset);
+
+				frame_payload_remain = mss;
+				segmented_bytes += mss;
+
+				tcph->seq = htonl(sequence);
+			tcph->check = ~csum_tcpudp_magic(saddr, daddr,l4_len,
+							 IPPROTO_TCP, 0);
+
+			iph->id = htons(ip_id++);
+			iph->check  = 0;
+			iph->check = ip_fast_csum((uint8_t *)iph, ihl);
+			hdr_ptr = (void *) hdr_cpu_addr;
+			sop_addr = (uint64_t) hdr_dma_addr;
+			memcpy((void *)((uint8_t *)hdr_ptr + pkthdr_len),
+				   (void *)eptr, p_hdr_len);
+
+			desc_value[start_desc] =
+				((sop_data_len << TX_PKT_DESC_TR_LEN_SHIFT) |
+						  (ngathers << TX_PKT_DESC_NUM_PTR_SHIFT) |
+						  TX_PKT_DESC_SOP | (uint64_t)sop_addr);
+
+			NXGE_FILL_TX_HDR_GSO(hdr_ptr, hdr_csum_value, real_xfer_len, pads);
+			hdr_cpu_addr += GSO_HDR_BUFFER_LEN;
+			hdr_dma_addr += GSO_HDR_BUFFER_LEN;
+			sequence += payload_length;
+			eptr = tptr;
+			iph = iphdrp;
+			tcph = tcphdrp;
+			frame_index++;
+			ngathers = 1ULL;
+
+			} else {
+				buf_len = len < TX_MAX_TRANSFER_LENGTH ?
+					len : TX_MAX_TRANSFER_LENGTH;
+				desc_value[data_descs] =
+					(buf_len << TX_PKT_DESC_TR_LEN_SHIFT) |
+					(dma_addr_t)((uint64_t) skmap_dpages_info[0].addr + offset);
+
+				frame_payload_remain = (frame_payload_remain - buf_len);
+			}
+
+			len -= buf_len;
+			offset += buf_len;
+			data_descs++;
+		}
+	}
+
+	for (i = 0; i < nr_frags; i++) {
+		skb_frag_t *fragp = &skb_shinfo(skb)->frags[i];
+		len = fragp->size;
+		offset = 0;
+		skmap_dpages_info[i + header_buffers].length = len;
+		skmap_dpages_info[i + header_buffers].addr =
+			pci_map_page(nep->pdev, fragp->page,
+					 fragp->page_offset,
+					 len, PCI_DMA_TODEVICE);
+
+		while (len) {
+			if (frame_payload_remain == mss) {
+				start_desc = data_descs;
+				data_descs++;
+			}
+			ngathers++;
+
+			if ((len >= frame_payload_remain) &&
+				(frame_payload_remain <= TX_MAX_TRANSFER_LENGTH)){
+				buf_len = frame_payload_remain;
+				desc_value[data_descs] =
+					(buf_len << TX_PKT_DESC_TR_LEN_SHIFT) |
+					(dma_addr_t)((uint64_t) skmap_dpages_info[i + header_buffers].addr + offset);
+				frame_payload_remain = mss;
+
+				if (frame_index < max_frame_index) {
+					segmented_bytes += mss;
+					tcph->seq = htonl(sequence);
+					tcph->check = ~csum_tcpudp_magic(saddr, daddr,l4_len,
+											 IPPROTO_TCP, 0);
+
+					iph->id = htons(ip_id++);
+					iph->check  = 0;
+					iph->check = ip_fast_csum((uint8_t *)iph, ihl);
+					hdr_ptr = (void *) hdr_cpu_addr;
+					sop_addr = (uint64_t) hdr_dma_addr;
+					memcpy((void *)((uint8_t *)hdr_ptr + pkthdr_len),
+						   (void *)eptr, p_hdr_len);
+					NXGE_FILL_TX_HDR_GSO(hdr_ptr, hdr_csum_value, real_xfer_len, pads);
+					desc_value[start_desc] =
+						((sop_data_len << TX_PKT_DESC_TR_LEN_SHIFT) |
+						  (ngathers << TX_PKT_DESC_NUM_PTR_SHIFT) |
+						  TX_PKT_DESC_SOP | (uint64_t)sop_addr);
+
+					hdr_cpu_addr += GSO_HDR_BUFFER_LEN;
+					hdr_dma_addr += GSO_HDR_BUFFER_LEN;
+					sequence += payload_length;
+					eptr = tptr;
+					iph = iphdrp;
+					tcph = tcphdrp;
+					frame_index++;
+					ngathers = 1ULL;
+				}
+
+			} else {
+				buf_len = len < TX_MAX_TRANSFER_LENGTH ?
+					len : TX_MAX_TRANSFER_LENGTH;
+				desc_value[data_descs] =
+					(buf_len << TX_PKT_DESC_TR_LEN_SHIFT) |
+					(dma_addr_t) ((uint64_t) skmap_dpages_info[i + header_buffers].addr + offset);
+
+				frame_payload_remain = (frame_payload_remain - buf_len);
+			}
+
+			len -= buf_len;
+			offset += buf_len;
+			data_descs++;
+		}
+	}
+
+	i = data_descs - 1;
+
+	ngathers = (data_descs - start_desc);
+
+	last_len = (l4_payload_len - segmented_bytes);
+
+	iph = iphdrp_last;
+	tcph = tcphdrp_last;
+	eptr = tptr_last;
+	payload_length = last_len;
+	iph->tot_len = htons(payload_length + l3l4_hdr_len);
+	pkt_len = sop_data_len + payload_length;
+	real_xfer_len = pkt_len - TX_PKT_HEADER_SIZE;
+	tcph->seq = htonl(sequence);
+	tcph->check = ~csum_tcpudp_magic(saddr, daddr,
+								 payload_length + l4_hdr_len,
+								 IPPROTO_TCP, 0);
+
+	iph->id = htons(ip_id);
+	iph->check  = 0;
+	iph->check = ip_fast_csum((uint8_t *)iph, ihl);
+	hdr_ptr = (void *) hdr_cpu_addr;
+	sop_addr = (uint64_t) hdr_dma_addr;
+	memcpy((void *)((uint8_t *)hdr_ptr + pkthdr_len),
+				   (void *)eptr, p_hdr_len);
+
+	NXGE_FILL_TX_HDR_GSO(hdr_ptr, hdr_csum_value, real_xfer_len, pads);
+
+	desc_value[start_desc] =
+				((sop_data_len << TX_PKT_DESC_TR_LEN_SHIFT) |
+						  (ngathers << TX_PKT_DESC_NUM_PTR_SHIFT) |
+						  TX_PKT_DESC_SOP | (uint64_t)sop_addr);
+
+
+	sop_data_len = pkthdr_len + p_hdr_len;
+
+	min_descs = data_descs + frame_count + nxge_tx_min_free;
+
+	MUTEX_ENTER_INT(&tx_ring_p->lock, flags);
+/* check if we have enough descriptors to post all*/
+
+	if (TX_BUFFS_AVAIL(nep, ring) < min_descs) {
+		int mapping;
+		MUTEX_EXIT_INT(&tx_ring_p->lock, flags);
+			/* free up the resources */
+		for (mapping = 0; mapping  < skmap_info->mappings; mapping++)
+			pci_unmap_page(nep->pdev, skmap_dpages_info[mapping].addr,
+						   skmap_dpages_info[mapping].length,
+						   PCI_DMA_TODEVICE);
+		pci_unmap_single(nep->pdev,
+							   skmap_info->hdr_buffer_dma_addr,
+							   skmap_info->hdr_buffer_length,
+							   PCI_DMA_TODEVICE);
+
+		KMEM_FREE(p_hdr->map_info, p_hdr->mappings_len);
+
+		KMEM_FREE(nxge_gdesc, info_alloc_size);
+		p_hdr->mappings_len = 0;
+		MUTEX_ENTER_INT(&tx_ring_p->stats_lock, flags);
+		nep->statsp->tdc_stats[ring].oerrors++;
+		nep->statsp->tdc_stats[ring].tx_tso_no_desc++;
+		MUTEX_EXIT_INT(&tx_ring_p->stats_lock, flags);
+		return (NETDEV_TX_BUSY);
+	}
+
+	if (tx_ring_p->intr_marked == B_FALSE) {
+		if (tx_ring_p->descs_pending &&
+			(TX_BUFFS_AVAIL(nep, ring)	< TX_MARK_GSO_FREE)) {
+			mark_mode = 1ULL;
+				/* mark the last packet */
+			desc_value[start_desc] |=
+				(mark_mode << TX_PKT_DESC_MARK_SHIFT);
+			tx_ring_p->intr_marked = B_TRUE;
+			atomic_set(&nep->tx_intr_set, 1);
+		}
+	}
+
+	handle.regh = NULL;
+	handle.is_vraddr = B_FALSE;
+	entry = last_entry = sop_index = tx_ring_p->in;
+	tx_desc_ptr = (p_tx_desc_t) (tx_ring_p->tdc_desc.cpu_addr);
+	prefetch(tx_desc_ptr + sop_index);
+	prefetch(desc_value);
+
+	for (desc_index = 0; desc_index < data_descs; desc_index++) {
+
+		handle.regp = (npi_reg_ptr_t )(tx_desc_ptr + entry);
+
+		NXGE_MEM_PIO_WRITE64(handle, desc_value[desc_index]);
+		last_entry = entry;
+		entry = TX_DESC_NEXT_IDX(nep, ring, entry);
+	}
+
+	pci_dma_sync_single_for_device(nep->pdev, skmap_info->hdr_buffer_dma_addr,
+								skmap_info->hdr_buffer_length,
+								   PCI_DMA_TODEVICE);
+
+	nep->statsp->tdc_stats[ring].tx_packets_tso++;
+
+	tx_msg_p = &(tx_ring_p->tx_msg_ring[last_entry]);
+	tx_msg_p->buf = skb;
+
+	if (entry <= tx_ring_p->in) {
+		tx_ring_p->in_wrap = (tx_ring_p->in_wrap == B_TRUE) ?
+			B_FALSE : B_TRUE;
+	}
+
+	tx_ring_p->in = entry;
+	tx_ring_p->descs_pending += data_descs;
+	nep->statsp->tdc_stats[ring].tx_descs_pending = tx_ring_p->descs_pending;
+
+	kick.value = 0;
+	kick.bits.ldw.wrap = tx_ring_p->in_wrap;
+	kick.bits.ldw.tail = (uint16_t)tx_ring_p->in;
+
+
+		/* Kick start the Transmit kick register */
+	TXDMA_REG_WRITE64(nep->npi_handle, TX_RING_KICK_REG,
+					  (uint8_t)tx_ring_p->tdc, kick.value);
+
+	MUTEX_EXIT_INT(&tx_ring_p->lock, flags);
+	MUTEX_ENTER_INT(&tx_ring_p->stats_lock, flags);
+	nep->statsp->tdc_stats[ring].descs_kicked += data_descs;
+	nep->statsp->tdc_stats[ring].obytes += skb->len;
+	nep->statsp->tdc_stats[ring].opackets += frame_count;
+	MUTEX_EXIT_INT(&tx_ring_p->stats_lock, flags);
+
+	KMEM_FREE(nxge_gdesc, info_alloc_size);
+	NXGE_DEBUG_MSG((nep, TX_CTL, "<== nxge_tx_ring"));
+	return (NETDEV_TX_OK);
+
+}
+
+#else
+
+
+int nxge_tx_ring_gso(p_nxge_t nep, int ring,
+				struct sk_buff *tx_skb)
+{
+	struct net_device	*dev = nep->dev;
+	p_tx_ring_t		tx_ring_p = TX_RINGN(nep, ring);
+	unsigned long		flags;
+	int			entry, nr_frags, i, sop_index, last_entry;
+	uint32_t		len, buf_len;
+	uint64_t		pkt_len;
+	uint32_t		offset = 0;
+	int			status = NETDEV_TX_OK;
+	uint64_t		pkthdr_len = 0;
+	uint64_t 		mark_mode = B_FALSE;
+	unsigned char		*pkt_ptr = NULL;
+	uint64_t		ngathers = 0;
+	int padbytes = 0;
+	p_skb_hdr_info_t p_hdr, p_hdr_orig;
+	skb_hdr_info_t skb_hdr;
+	struct sk_buff *skb, *seg_skbs;
+
+	int p_hdr_len = 0;
+	int l3_hdr_len = 0;
+	int l4_hdr_len = 0;
+	int l3l4_hdr_len = 0;
+	int mss;
+	int l4_payload_len;
+	int frame_payload_remain;
+	int header_buffers = 0;
+	int payload_length;
+	int ether_frames;
+	uint32_t saddr;
+	uint32_t daddr;
+	uint8_t ihl;
+
+	uint32_t sequence;
+	uint16_t ip_id;
+	struct iphdr	*iphdrp = NULL;
+	struct iphdr	*iphdrp_first = NULL;
+	struct iphdr	*iphdrp_last = NULL;
+	struct tcphdr   *tcphdrp = NULL;
+	struct tcphdr   *tcphdrp_first = NULL;
+	struct tcphdr   *tcphdrp_last = NULL;
+	struct iphdr	*iph = NULL;
+	struct tcphdr   *tcph = NULL;
+	int last_len;
+	int running_len;
+	int skb_hdr_len;
+	uint8_t *tptr;
+
+	p_hdr_orig = (skb_hdr_info_t *) tx_skb->cb;
+	p_hdr = (skb_hdr_info_t *) &skb_hdr;
+	memcpy(p_hdr, tx_skb->cb, sizeof(skb_hdr_info_t));
+
+	mss = SKB_IS_GSO(tx_skb);
+
+	l4_hdr_len = tx_skb->h.th->doff << 2;
+	l3_hdr_len = (p_hdr->l4_offset - p_hdr->l3_offset);
+
+	l3l4_hdr_len = l4_hdr_len + l3_hdr_len;
+	p_hdr_len = p_hdr->l3_offset + /* l2 len */
+		 l3l4_hdr_len; /* l3l4 hdr len */
+
+	payload_length = tx_skb->len - (p_hdr_len);
+
+	tptr = (uint8_t *)tx_skb->data;
+	iphdrp = (struct iphdr *) (tptr + p_hdr->l3_offset);
+	tcphdrp = (struct tcphdr *)  (tptr + p_hdr->l4_offset);
+
+	sequence = ntohl(tcphdrp->seq);
+	ip_id = ntohs(iphdrp->id);
+
+	seg_skbs = skb_gso_segment(tx_skb, nep->dev->features & ~NETIF_F_TSO);
+	skb = seg_skbs; /* first segment */
+	seg_skbs = seg_skbs->next;
+	memcpy(skb->cb, p_hdr, sizeof(skb_hdr_info_t));
+	skb->next = NULL;
+	tptr = (uint8_t *)skb->data;
+	iph = (struct iphdr *) (tptr + p_hdr->l3_offset);
+	tcph = (struct tcphdr *)  (tptr + p_hdr->l4_offset);
+	saddr = iph->saddr;
+	daddr = iph->daddr;
+	ihl = iph->ihl;
+
+	tcph->psh = 0; /* psh last keeps from large */
+	tcph->rst = 0; /* RST last keeps from large */
+	tcph->fin = 0; /* fin last keeps from large */
+
+	tcph->seq = htonl(sequence);
+	tcph->check = 0;
+	tcph->check = ~csum_tcpudp_magic(saddr,
+									 daddr,
+									 mss + l4_hdr_len, /* len */
+									 IPPROTO_TCP, 0);
+
+	iph->id = htons(ip_id++);
+	iph->check  = 0;
+	iph->tot_len = htons(mss + l3l4_hdr_len);
+	iph->check = ip_fast_csum((uint8_t *)iph, ihl);
+
+	status = nxge_tx_ring(nep, ring, skb);
+	if (status != NETDEV_TX_OK) {
+		dev_kfree_skb_any(tx_skb);
+		return (NETDEV_TX_OK);
+	}
+
+	sequence += mss;
+	running_len = mss;
+	while (seg_skbs->next) {
+		skb = seg_skbs; /* first segment */
+		memcpy(skb->cb, p_hdr, sizeof(skb_hdr_info_t));
+
+		seg_skbs = seg_skbs->next;
+		skb->next = NULL;
+			/* main loop */
+		tptr = (uint8_t *)skb->data;
+		iph = (struct iphdr *) (tptr + p_hdr->l3_offset);
+		tcph = (struct tcphdr *)  (tptr + p_hdr->l4_offset);
+		tcph->psh = 0; /* psh last keeps from large */
+		tcph->rst = 0; /* RST last keeps from large */
+		tcph->fin = 0; /* fin last keeps from large */
+		tcph->urg = 0; /* urg 1st keeps from large */
+
+		tcph->seq = htonl(sequence);
+		tcph->check = 0;
+		tcph->check = ~csum_tcpudp_magic(saddr,
+										 daddr,
+										 mss + l4_hdr_len, /* len */
+										 IPPROTO_TCP, 0);
+
+		iph->id = htons(ip_id++);
+		iph->check  = 0;
+		iph->tot_len = htons(mss + l3l4_hdr_len);
+		iph->check = ip_fast_csum((uint8_t *)iph, ihl);
+
+		status = nxge_tx_ring(nep, ring, skb);
+		if (status != NETDEV_TX_OK) {
+			dev_kfree_skb_any(tx_skb);
+			return (NETDEV_TX_OK);
+		}
+		sequence += mss;
+		running_len += mss;
+	}
+
+	skb = seg_skbs; /* last one */
+	memcpy(skb->cb, p_hdr, sizeof(skb_hdr_info_t));
+	skb->next = NULL;
+	last_len = payload_length - running_len;
+	tptr = (uint8_t *)skb->data;
+	iph = (struct iphdr *) (tptr + p_hdr->l3_offset);
+	tcph = (struct tcphdr *)  (tptr + p_hdr->l4_offset);
+	tcph->urg = 0;
+
+	tcph->seq = htonl(sequence);
+	tcph->check = 0;
+	tcph->check = ~csum_tcpudp_magic(saddr,
+									 daddr,
+									 last_len + l4_hdr_len,
+									 IPPROTO_TCP, 0);
+
+	iph->id = htons(ip_id++);
+	iph->check  = 0;
+	iph->tot_len = htons(last_len + l3l4_hdr_len);
+	iph->check = ip_fast_csum((uint8_t *)iph, ihl);
+
+	status = nxge_tx_ring(nep, ring, skb);
+	dev_kfree_skb_any(tx_skb);
+	nep->statsp->tdc_stats[ring].tx_packets_tso++;
+	return (NETDEV_TX_OK);
+}
+
+#endif
+
+
+int nxge_tx_ring(p_nxge_t nep, int ring,
+				struct sk_buff *tx_skb)
+{
+	struct net_device	*dev = nep->dev;
+	p_tx_ring_t		tx_ring_p = TX_RINGN(nep, ring);
+	p_tx_msg_t		tx_msg_p;
+	unsigned long		flags;
+	int			entry, nr_frags, i, sop_index, last_entry;
+	uint32_t		len, buf_len;
+	uint64_t		pkt_len;
+	uint32_t		offset = 0;
+	int			status = NETDEV_TX_OK;
+	uint64_t		pkthdr_len = 0;
+	uint64_t 		mark_mode = B_FALSE;
+	unsigned char		*pkt_ptr = NULL;
+	uint64_t		ngathers = 0;
+	npi_handle_t		handle;
+	tx_ring_kick_t		kick;
+	int padbytes = 0;
+	uint8_t reclaim_status = B_TRUE;
+	boolean_t copy_2hdr_buffer = B_FALSE;
+	void			*hdr_ptr = NULL;
+	uint64_t		sop_data_len;
+	uint64_t		desc_data_len[TX_MAX_GATHER_POINTERS];
+	uint64_t		desc_dma_addr[TX_MAX_GATHER_POINTERS];
+	p_skb_hdr_info_t p_hdr;
+	struct sk_buff *skb;
+	dma_addr_t sop_addr;
+	int headroom = 0;
+	int map_index = 1;
+	int frame_bytes;
+	boolean_t use_skb_hdr = B_FALSE;
+	unsigned long allign;
+	skb = tx_skb;
+	p_hdr = (skb_hdr_info_t *) skb->cb;
+
+#ifdef NXGE_DEBUG_INFO
+	/* for debug purposes only */
+	nep->statsp->tdc_stats[ring].stk_tx_pkts++;
+#endif
+
+	if (tx_ring_p->descs_pending >= nep->tx_reclaim_pending) {
+		MUTEX_ENTER_INT(&tx_ring_p->lock, flags);
+		reclaim_status = nxge_tx_reclaim(nep, tx_ring_p, nxge_tx_min_free);
+		MUTEX_EXIT_INT(&tx_ring_p->lock, flags);
+
+		if (reclaim_status == NXGE_TX_RECLAIM_FAIL) {
+			MUTEX_ENTER_INT(&tx_ring_p->stats_lock, flags);
+			nep->statsp->tdc_stats[ring].oerrors++;
+			nep->statsp->tdc_stats[ring].tx_no_desc++;
+			MUTEX_EXIT_INT(&tx_ring_p->stats_lock, flags);
+			if ((!netif_queue_stopped(dev)) &&
+				atomic_read(&nep->tx_intr_set)) {
+				netif_stop_queue(dev);
+				NXGE_DEBUG_MSG((nep, TX_CTL,
+								"nxge_tx_ring stopped queue after reclaim "));
+			}
+			return (NETDEV_TX_BUSY);
+		}
+	}
+
+		/* check if we can post */
+	if (TX_BUFFS_AVAIL(nep, ring) < nxge_tx_max_gathers) {
+		MUTEX_ENTER_INT(&tx_ring_p->stats_lock, flags);
+		nep->statsp->tdc_stats[ring].oerrors++;
+		nep->statsp->tdc_stats[ring].tx_no_desc++;
+		MUTEX_EXIT_INT(&tx_ring_p->stats_lock, flags);
+
+		if ((!netif_queue_stopped(dev)) &&
+			atomic_read(&nep->tx_intr_set)) {
+			netif_stop_queue(dev);
+				NXGE_DEBUG_MSG((nep, TX_CTL,
+								"nxge_tx_ring stopped queue too few descs "));
+
+		}
+		return (NETDEV_TX_BUSY);
+	}
+
+
+	if ((p_hdr->skb_process_info & NXGE_TX_SKB_COPY) &&
+		(skb->len > (NXGE_TX_COPY_BUFFER_SIZE - TX_PKT_HEADER_SIZE))){
+		skb = skb_copy(tx_skb, GFP_ATOMIC);
+		if (skb == NULL) {
+			MUTEX_ENTER_INT(&tx_ring_p->stats_lock, flags);
+			nep->statsp->tdc_stats[ring].tx_no_buf++;
+			MUTEX_EXIT_INT(&tx_ring_p->stats_lock, flags);
+			return (NETDEV_TX_BUSY);
+		} else {
+			p_hdr = (skb_hdr_info_t *) skb->cb;
+			memcpy(skb->cb, tx_skb->cb, sizeof(skb_hdr_info_t));
+			dev_kfree_skb_any(tx_skb);
+		}
+	}
+
+
+	pkthdr_len = TX_PKT_HEADER_SIZE;
+	nr_frags = skb_shinfo(skb)->nr_frags;
+	if ((p_hdr->skb_process_info & NXGE_TX_SKB_TINY) && nr_frags) {
+		if (NXGE_SKB_LINEARIZE(tx_skb, GFP_ATOMIC)) {
+			MUTEX_ENTER_INT(&tx_ring_p->stats_lock, flags);
+			nep->statsp->tdc_stats[ring].tx_no_buf++;
+			MUTEX_EXIT_INT(&tx_ring_p->stats_lock, flags);
+			return (NETDEV_TX_BUSY);
+		}
+		nr_frags = skb_shinfo(skb)->nr_frags;
+	}
+
+	if ((nr_frags == 0) &&
+		(skb->len < (NXGE_TX_COPY_BUFFER_SIZE - TX_PKT_HEADER_SIZE))) {
+		copy_2hdr_buffer = B_TRUE;
+	}
+
+	sop_data_len = pkthdr_len;
+	len = skb_headlen(skb);
+	ngathers = 1;
+	frame_bytes = skb->len;
+
+	if (copy_2hdr_buffer == B_TRUE) {
+		sop_data_len += len;
+		pkt_len = sop_data_len;
+		goto post_sop_desc;
+	}
+
+	pkt_len = pkthdr_len + skb->len;
+	pkt_ptr = skb->data;
+	if (skb->len > 1518)
+		nep->statsp->tdc_stats[ring].jumbo++;
+
+
+	headroom = skb_headroom(skb);
+	allign = ((unsigned long)skb->data) & 0x0fUL;
+	hdr_ptr = (void *)pkt_ptr;
+
+	if (headroom > (pkthdr_len + allign)) {
+		padbytes = allign;
+		use_skb_hdr = B_TRUE;
+		ngathers = 0;
+		map_index = 0;
+		pkt_ptr -= (pkthdr_len + padbytes);
+		hdr_ptr = (void *)pkt_ptr;
+		len += (pkthdr_len + padbytes);
+		pkt_len += padbytes;
+		memset(hdr_ptr, 0, (pkthdr_len + padbytes));
+	}
+
+
+	while (len) {
+		/*
+		 * Hardware limits the transfer length to 4K.
+		 * If len is more than 4K, we need to break
+		 * it up into 4K blocks.
+		 */
+
+		buf_len = len < TX_MAX_TRANSFER_LENGTH ?
+			len : TX_MAX_TRANSFER_LENGTH;
+		desc_data_len[ngathers] = buf_len;
+		desc_dma_addr[ngathers] = pci_map_page(nep->pdev,
+					      virt_to_page(pkt_ptr + offset),
+					      offset_in_page(pkt_ptr + offset),
+					      buf_len, PCI_DMA_TODEVICE);
+		ngathers++;
+		len -= buf_len;
+		offset += buf_len;
+	}
+
+	for (i = 0; i < nr_frags; i++) {
+		skb_frag_t *fragp = &skb_shinfo(skb)->frags[i];
+		len = fragp->size;
+		offset = 0;
+		while (len) {
+			buf_len = len < TX_MAX_TRANSFER_LENGTH ?
+				len : TX_MAX_TRANSFER_LENGTH;
+			desc_data_len[ngathers] = buf_len;
+			desc_dma_addr[ngathers] =
+				pci_map_page(nep->pdev, fragp->page,
+					     fragp->page_offset + offset,
+					     buf_len, PCI_DMA_TODEVICE);
+			ngathers++;
+			len -= buf_len;
+			offset += buf_len;
+		}
+	}
+
+	if (ngathers  >	nxge_tx_max_gathers) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_tx_ring: "
+						"Exceeded max gather cnt, "
+						"aborting tx"));
+		MUTEX_ENTER_INT(&tx_ring_p->stats_lock, flags);
+		nep->statsp->tdc_stats[ring].oerrors++;
+		nep->statsp->tdc_stats[ring].tx_no_desc++;
+		MUTEX_EXIT_INT(&tx_ring_p->stats_lock, flags);
+	/* Unmap all the mapped pages */
+		for (i = map_index; i < ngathers; i++) {
+			pci_unmap_page(nep->pdev, desc_dma_addr[i], desc_data_len[i],
+						   PCI_DMA_TODEVICE);
+		}
+		return (NETDEV_TX_BUSY);
+	}
+
+
+ post_sop_desc:
+	MUTEX_ENTER_INT(&tx_ring_p->lock, flags);
+
+
+	if (tx_ring_p->intr_marked == B_FALSE) {
+		if (tx_ring_p->descs_pending &&
+			(TX_BUFFS_AVAIL(nep, ring)	< TX_MARK_FREE)) {
+			mark_mode = 1ULL;
+			tx_ring_p->intr_marked = B_TRUE;
+			atomic_set(&nep->tx_intr_set, 1);
+		}
+	}
+
+	entry = last_entry = sop_index = tx_ring_p->in;
+	handle.regh = NULL;
+	handle.is_vraddr = B_FALSE;
+
+	handle.regp = tx_ring_p->tdc_desc.cpu_addr +
+		(sop_index << NXGE_TX_DESC_SIZE_SHIFT);
+
+	tx_msg_p = &(tx_ring_p->tx_msg_ring[sop_index]);
+
+	if (use_skb_hdr == B_FALSE) {
+		hdr_ptr = (void *) tx_msg_p->cp_cpu_addr;
+		sop_addr =  tx_msg_p->cp_dma_addr;
+#ifdef USE_STREAMING_TXHDR_BUFFER
+		pci_dma_sync_single_for_cpu(nep->pdev,
+									sop_addr,
+									NXGE_TX_COPY_BUFFER_SIZE,
+									PCI_DMA_TODEVICE);
+#endif
+
+		tx_msg_p->addr = 0;
+		tx_msg_p->buf = NULL;
+		if (copy_2hdr_buffer == B_TRUE) {
+
+			/* this is a small packet fitting into the hdr copy buffer */
+			/* copy the data buffer as well */
+			memcpy((void *)(tx_msg_p->cp_cpu_addr + pkthdr_len),
+				   (void *)skb->data, skb->len);
+			if (p_hdr->skb_process_info & NXGE_TX_SKB_TINY) {
+				int min_len = nep->msg_min + pkthdr_len;
+				memset((void *)(tx_msg_p->cp_cpu_addr + sop_data_len), 0,
+					   min_len - sop_data_len);
+				sop_data_len  = min_len;
+				pkt_len = min_len;
+			}
+		}
+
+	} else {
+		sop_addr = desc_dma_addr[0];
+		sop_data_len = desc_data_len[0];
+	}
+
+	NXGE_MEM_PIO_WRITE64(handle,
+			 ((sop_data_len << TX_PKT_DESC_TR_LEN_SHIFT) |
+			  (ngathers << TX_PKT_DESC_NUM_PTR_SHIFT) |
+			  (mark_mode << TX_PKT_DESC_MARK_SHIFT) |
+			  TX_PKT_DESC_SOP |
+			  (uint64_t)sop_addr));
+
+	last_entry = entry;
+	entry = TX_DESC_NEXT_IDX(nep, ring, sop_index);
+	tx_msg_p = &(tx_ring_p->tx_msg_ring[entry]);
+
+	for (i = 1; i < ngathers; i++) {
+		handle.regp = tx_ring_p->tdc_desc.cpu_addr +
+			(entry << NXGE_TX_DESC_SIZE_SHIFT);
+
+		tx_msg_p->buf = NULL;
+		tx_msg_p->length = desc_data_len[i];
+		tx_msg_p->addr = desc_dma_addr[i];
+		NXGE_MEM_PIO_WRITE64(handle,
+				 ((desc_data_len[i] << TX_PKT_DESC_TR_LEN_SHIFT) |
+				  desc_dma_addr[i]));
+
+		last_entry = entry;
+		entry = TX_DESC_NEXT_IDX(nep, ring, entry);
+		tx_msg_p = &(tx_ring_p->tx_msg_ring[entry]);
+	}
+
+	NXGE_FILL_TX_HDR(hdr_ptr, skb, pkt_len, padbytes);
+
+	 /* set skb for the last buf of the pkt */
+
+	tx_msg_p = &(tx_ring_p->tx_msg_ring[last_entry]);
+
+	if (copy_2hdr_buffer == B_TRUE) {
+		dev_kfree_skb_any(skb);
+	} else {
+		tx_msg_p->buf = skb;
+	}
+
+	if (entry <= tx_ring_p->in) {
+		tx_ring_p->in_wrap = (tx_ring_p->in_wrap == B_TRUE) ?
+			B_FALSE : B_TRUE;
+	}
+
+	tx_ring_p->in = entry;
+	tx_ring_p->descs_pending += ngathers;
+	nep->statsp->tdc_stats[ring].tx_descs_pending = tx_ring_p->descs_pending;
+
+#ifdef USE_STREAMING_TXHDR_BUFFER
+	if (use_skb_hdr == B_FALSE) {
+		pci_dma_sync_single_for_device(nep->pdev, sop_addr,
+								   NXGE_TX_COPY_BUFFER_SIZE,
+								   PCI_DMA_TODEVICE);
+	}
+#endif
+
+	kick.value = 0;
+	kick.bits.ldw.wrap = tx_ring_p->in_wrap;
+	kick.bits.ldw.tail = (uint16_t)tx_ring_p->in;
+
+		/* Kick start the Transmit kick register */
+	TXDMA_REG_WRITE64(nep->npi_handle,
+					  TX_RING_KICK_REG,
+					  (uint8_t)tx_ring_p->tdc,
+					  kick.value);
+
+	MUTEX_EXIT_INT(&tx_ring_p->lock, flags);
+	MUTEX_ENTER_INT(&tx_ring_p->stats_lock, flags);
+	nep->statsp->tdc_stats[ring].obytes += frame_bytes;
+	nep->statsp->tdc_stats[ring].opackets++;
+#ifdef NXGE_DEBUG_INFO
+	nep->statsp->tdc_stats[ring].descs_kicked += ngathers;
+#endif
+	MUTEX_EXIT_INT(&tx_ring_p->stats_lock, flags);
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "<== nxge_tx_ring"));
+	return (NETDEV_TX_OK);
+
+}
+
+
+int nxge_tx_lb(p_nxge_t nep, struct sk_buff *skb)
+{
+	int		ring = 0;
+	uint32_t	maxtdcs = nep->max_tdcs;
+	uint32_t	cpu_id;
+	uint16_t	eth_type;
+	struct iphdr	*iphdrp = NULL;
+	struct ipv6hdr	*ipv6hdrp = NULL;
+	uint8_t		ipproto = 0;
+	struct tcphdr   *tcphdrp = NULL;
+	struct udphdr   *udphdrp = NULL;
+	uint8_t		mac_byte;
+	int		tx_lb_policy;
+	uint16_t vlan_id = 0;
+	p_skb_hdr_info_t p_hdr;
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "==> nxge_tx_lb"));
+	p_hdr = (skb_hdr_info_t *) skb->cb;
+
+	tx_lb_policy = nxge_get_tx_lb_policy(nep);
+	eth_type = ntohs(skb->protocol);
+	p_hdr->l3_offset = ETH_HLEN;
+
+	if ((eth_type == ETH_P_8021Q) || (eth_type < ETH_DATA_LEN)) {
+		if (eth_type == ETH_P_8021Q) {
+			/* header with VLAN */
+			eth_type =
+				ntohs(((struct vlan_ethhdr *)skb->data)->h_vlan_encapsulated_proto);
+			if (eth_type == ETH_P_IP ||
+				eth_type == ETH_P_IPV6) {
+				p_hdr->l3_offset = VLAN_ETH_HLEN;
+				p_hdr->ctl_hdrp.bits.hdw.vlan = 1;
+			}
+			vlan_id = vlan_tx_tag_get(skb);
+		} else  if (*(uint16_t*)(skb->data + ETH_HLEN) == LLC_SAP_SNAP) {
+					/* LLC-SNAP header */
+				eth_type = ntohs(*((uint16_t*)(skb->data +
+											   ETH_HLEN + 6)));
+				if (eth_type == ETH_P_IP ||
+					eth_type == ETH_P_IPV6) {
+					p_hdr->ctl_hdrp.bits.hdw.llc = 1;
+					p_hdr->l3_offset = ETH_HLEN + 8;
+				} else {
+					goto nxge_tx_lb_exit;
+				}
+		}
+
+	}
+
+	switch(eth_type) {
+		case ETH_P_IP:
+				/* IPv4 */
+			iphdrp = (struct iphdr *)(skb->data + p_hdr->l3_offset);
+			ipproto = iphdrp->protocol;
+
+			if (ipproto == IPPROTO_ICMP) {
+				ring = 0;
+				p_hdr->skb_process_info |= NXGE_TX_SKB_COPY;
+
+				goto nxge_tx_lb_exit;
+			}
+
+			p_hdr->l3_type = NXGE_LB_L3_IPV4;
+			p_hdr->l4_offset = p_hdr->l3_offset + (iphdrp->ihl << 2);
+			if (!(iphdrp->frag_off & 0xff3f)) {
+				p_hdr->ctl_hdrp.bits.hdw.ip_ver = 0;
+				p_hdr->ctl_hdrp.bits.hdw.ihl = iphdrp->ihl;
+				p_hdr->ctl_hdrp.bits.hdw.l3start = ((unsigned long)iphdrp -
+					  (unsigned long)skb->data) >> 1;
+			} else {
+				goto nxge_tx_lb_exit;
+			}
+			break;
+
+		case ETH_P_IPV6:
+			/* IPv6 */
+
+			ipv6hdrp = (struct ipv6hdr *)(skb->data + p_hdr->l3_offset);
+			ipproto = ipv6hdrp->nexthdr;
+			if ((ipproto == IPPROTO_ICMP) ||
+				(ipproto == IPPROTO_ICMPV6)) {
+				ring = 0;
+				p_hdr->skb_process_info |= NXGE_TX_SKB_COPY;
+				goto nxge_tx_lb_exit;
+			}
+			p_hdr->l3_type = NXGE_LB_L3_IPV6;
+			p_hdr->l4_offset = p_hdr->l3_offset + 40;
+			p_hdr->ctl_hdrp.bits.hdw.ip_ver = 1;
+			p_hdr->ctl_hdrp.bits.hdw.ihl = (40 >> 2);
+			p_hdr->ctl_hdrp.bits.hdw.l3start = ((unsigned long)ipv6hdrp -
+					  (unsigned long)skb->data) >> 1;
+
+			break;
+		default:
+			goto nxge_tx_lb_exit;
+			break;
+	}
+
+
+	switch (ipproto) {
+		case IPPROTO_TCP:
+				/* TCP */
+			tcphdrp =  (struct tcphdr *) (skb->data + p_hdr->l4_offset);
+			p_hdr->l4_type = NXGE_LB_L4_TCP;
+			p_hdr->data_offset = p_hdr->l4_offset +
+				(tcphdrp->doff << 2);
+
+			p_hdr->ctl_hdrp.bits.hdw.l4start = p_hdr->l4_offset >> 1;
+			p_hdr->ctl_hdrp.bits.hdw.l4stuff =
+				p_hdr->ctl_hdrp.bits.hdw.l4start +
+				(SKB_CKSUM_OFFSET(skb) >> 1);
+			p_hdr->ctl_hdrp.bits.hdw.cksum_en_pkt_type = 1;
+
+			break;
+
+		case IPPROTO_UDP:
+			/* UDP */
+
+			udphdrp = (struct udphdr *) (skb->data + p_hdr->l4_offset);
+			p_hdr->l4_type = NXGE_LB_L4_UDP;
+			p_hdr->data_offset = p_hdr->l4_offset + sizeof (struct udphdr);
+
+			if (nep->tx_copy_udp == B_TRUE)
+				p_hdr->skb_process_info |= NXGE_TX_SKB_COPY;
+			p_hdr->ctl_hdrp.bits.hdw.l4start =  p_hdr->l4_offset >> 1;
+			p_hdr->ctl_hdrp.bits.hdw.l4stuff =
+				p_hdr->ctl_hdrp.bits.hdw.l4start +
+				(SKB_CKSUM_OFFSET(skb) >> 1);
+			p_hdr->ctl_hdrp.bits.hdw.cksum_en_pkt_type = 2;
+			break;
+		default:
+			if (eth_type == ETH_P_IP) {
+				ring = (iphdrp->daddr % maxtdcs) & 0xff;
+				goto nxge_tx_lb_exit;
+			} else if (eth_type == ETH_P_IPV6) {
+				ring = (ipv6hdrp->daddr.s6_addr[0] % maxtdcs)
+					& 0xff; /* check this for endianness */
+				goto nxge_tx_lb_exit;
+			} else {
+				/* should not come here !!! */
+				goto nxge_tx_lb_exit;
+			}
+	}
+
+
+	switch (tx_lb_policy) {
+		case NXGE_TX_LB_L4_PORT: /* default L4 Port */
+			if (p_hdr->l4_type == NXGE_LB_L4_TCP) {
+				ring = ((ntohs(tcphdrp->source ^ tcphdrp->dest))  % maxtdcs);
+			}
+			else if (p_hdr->l4_type == NXGE_LB_L4_UDP)
+				ring = ((ntohs(udphdrp->source ^ udphdrp->dest)) % maxtdcs);
+			else
+				ring = 0;
+
+			break;
+
+		case NXGE_TX_LB_L4_SRC: /* default L4 Port */
+			if (p_hdr->l4_type == NXGE_LB_L4_TCP) {
+				ring = (ntohs(tcphdrp->source)) % maxtdcs;
+			}
+			else if (p_hdr->l4_type == NXGE_LB_L4_UDP)
+				ring = (ntohs(udphdrp->source)) % maxtdcs;
+			else
+				ring = 0;
+			break;
+
+		case NXGE_TX_LB_L4_DEST: /*  L4 Dest Port */
+			if (p_hdr->l4_type == NXGE_LB_L4_TCP) {
+				ring = (ntohs(tcphdrp->dest)) % maxtdcs;
+			}
+			else if (p_hdr->l4_type == NXGE_LB_L4_UDP)
+				ring = (ntohs(udphdrp->dest)) % maxtdcs;
+			else
+				ring = 0;
+			break;
+
+
+		case NXGE_TX_LB_IP_DEST:
+/* add a case for l3 src, dest addresses */
+			if (p_hdr->l3_type == NXGE_LB_L3_IPV4)
+				ring = (ntohl(iphdrp->daddr) % maxtdcs);
+			else if (p_hdr->l3_type == NXGE_LB_L3_IPV6)
+				ring = (ntohl(ipv6hdrp->daddr.s6_addr[0]) % maxtdcs);
+			else
+				ring = 0;
+			break;
+		case NXGE_TX_LB_IP_SRC:
+			if (p_hdr->l3_type == NXGE_LB_L3_IPV4)
+				ring = (ntohl(iphdrp->saddr) % maxtdcs);
+			else if (p_hdr->l3_type == NXGE_LB_L3_IPV6)
+				ring = (ntohl(ipv6hdrp->saddr.s6_addr[0]) % maxtdcs);
+			else
+				ring = 0;
+			break;
+/* add a case for ip 5 tuple */
+		case NXGE_TX_LB_IP_ADDR:
+			if (p_hdr->l3_type == NXGE_LB_L3_IPV4)
+				ring = (ntohl(iphdrp->daddr ^ iphdrp->saddr) % maxtdcs);
+			else if (p_hdr->l3_type == NXGE_LB_L3_IPV6)
+				ring = (ntohl(ipv6hdrp->daddr.s6_addr[0] ^
+							  ipv6hdrp->saddr.s6_addr[0]) % maxtdcs);
+			else
+				ring = 0;
+			break;
+
+		case NXGE_TX_LB_DEST_MAC:
+				/* last byte of dest MAC */
+			mac_byte = *(uint8_t *)(skb->data + 5);
+			ring = mac_byte % maxtdcs;
+			break;
+
+		case NXGE_TX_LB_VLAN:
+			ring = vlan_id % maxtdcs;
+			break;
+
+		case NXGE_TX_LB_NONE:
+			ring = 0;
+			break;
+
+		case NXGE_TX_LB_CPU_ID:
+		default:
+		cpu_id = smp_processor_id();
+		ring = cpu_id % maxtdcs;
+		break;
+	}
+nxge_tx_lb_exit:
+	ring = ring % maxtdcs;
+	NXGE_DEBUG_MSG((nep, TX_CTL, "<== nxge_tx_lb: ring %d", ring));
+	return (ring);
+
+}
+
+
+
+int nxge_start_xmit(struct sk_buff *skb, struct net_device *dev)
+{
+	int		status = 0;
+	p_nxge_t	nep = netdev_priv(dev);
+
+
+	/* this is only used as a load-balancing hint, so it doesn't
+	 * need to be SMP safe */
+	int	ring = 0;
+	int		skblen;
+	p_skb_hdr_info_t p_hdr;
+
+	skblen = skb->len;
+
+	if(skblen <= 0) {
+		dev_kfree_skb_any(skb);
+		nep->statsp->tx_skb_errs++;
+		return (NETDEV_TX_OK);
+	}
+
+	p_hdr = (skb_hdr_info_t *) skb->cb;
+	memset(p_hdr, 0x0, sizeof(skb_hdr_info_t));
+
+	if (skblen >= nep->msg_min) {
+		ring = nxge_tx_lb(nep, skb);
+	} else {
+		ring = nxge_tx_lb(nep, skb);
+		p_hdr->skb_process_info |= NXGE_TX_SKB_TINY;
+	}
+
+	if (SKB_IS_GSO(skb)) {
+		status = nxge_tx_ring_gso(nep, ring, skb);
+	} else {
+		status = nxge_tx_ring(nep, ring, skb);
+	}
+
+	dev->trans_start = jiffies;
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "<== nxge_start_xmit"));
+	return (status);
+}
+
+
+/***********************************************************************
+ *                 RX Functions
+ *
+ **********************************************************************/
+
+nxge_status_t
+nxge_rxdma_handle_sys_errors(p_nxge_t nxgep)
+{
+	npi_handle_t		handle;
+	p_nxge_rdc_sys_stats_t	statsp;
+	rx_ctl_dat_fifo_stat_t	stat;
+	uint32_t		zcp_err_status;
+	uint32_t		ipp_err_status;
+	nxge_status_t		status = NXGE_OK;
+	npi_status_t		rs = NPI_SUCCESS;
+	boolean_t		my_err = B_FALSE;
+
+	handle = nxgep->npi_handle;
+	statsp = (p_nxge_rdc_sys_stats_t)&nxgep->statsp->rdc_sys_stats;
+
+	rs = npi_rxdma_rxctl_fifo_error_intr_get(handle, &stat);
+
+	if (rs != NPI_SUCCESS)
+		return (NXGE_ERROR | rs);
+
+	if (stat.bits.ldw.id_mismatch) {
+		statsp->id_mismatch++;
+	}
+
+	if ((stat.bits.ldw.zcp_eop_err) || (stat.bits.ldw.ipp_eop_err)) {
+		switch (nxgep->mac.portnum) {
+		case 0:
+			if ((stat.bits.ldw.zcp_eop_err & FIFO_EOP_PORT0) ||
+				(stat.bits.ldw.ipp_eop_err & FIFO_EOP_PORT0)) {
+				my_err = B_TRUE;
+				zcp_err_status = stat.bits.ldw.zcp_eop_err;
+				ipp_err_status = stat.bits.ldw.ipp_eop_err;
+			}
+			break;
+		case 1:
+			if ((stat.bits.ldw.zcp_eop_err & FIFO_EOP_PORT1) ||
+				(stat.bits.ldw.ipp_eop_err & FIFO_EOP_PORT1)) {
+				my_err = B_TRUE;
+				zcp_err_status = stat.bits.ldw.zcp_eop_err;
+				ipp_err_status = stat.bits.ldw.ipp_eop_err;
+			}
+			break;
+		case 2:
+			if ((stat.bits.ldw.zcp_eop_err & FIFO_EOP_PORT2) ||
+				(stat.bits.ldw.ipp_eop_err & FIFO_EOP_PORT2)) {
+				my_err = B_TRUE;
+				zcp_err_status = stat.bits.ldw.zcp_eop_err;
+				ipp_err_status = stat.bits.ldw.ipp_eop_err;
+			}
+			break;
+		case 3:
+			if ((stat.bits.ldw.zcp_eop_err & FIFO_EOP_PORT3) ||
+				(stat.bits.ldw.ipp_eop_err & FIFO_EOP_PORT3)) {
+				my_err = B_TRUE;
+				zcp_err_status = stat.bits.ldw.zcp_eop_err;
+				ipp_err_status = stat.bits.ldw.ipp_eop_err;
+			}
+			break;
+		default:
+			return (NXGE_ERROR);
+		}
+	}
+
+	if (my_err) {
+		status = nxge_rxdma_handle_port_errors(nxgep, ipp_err_status,
+							zcp_err_status);
+		if (status != NXGE_OK)
+			return (status);
+	}
+
+	return (NXGE_OK);
+}
+
+static nxge_status_t
+nxge_rxdma_handle_port_errors(p_nxge_t nxgep, uint32_t ipp_status,
+			      uint32_t zcp_status)
+{
+	boolean_t		rxport_fatal = B_FALSE;
+	p_nxge_rdc_sys_stats_t	statsp;
+	nxge_status_t		status = NXGE_OK;
+	uint8_t			portn;
+
+	NXGE_DEBUG_MSG((nxgep, RX_CTL, "==> nxge_rxdma_handle_port_errors"));
+
+	portn = nxgep->mac.portnum;
+
+	statsp = (p_nxge_rdc_sys_stats_t)&nxgep->statsp->rdc_sys_stats;
+
+	if (ipp_status & (0x1 << portn)) {
+		statsp->ipp_eop_err++;
+		rxport_fatal = B_TRUE;
+	}
+
+	if (zcp_status & (0x1 << portn)) {
+		statsp->zcp_eop_err++;
+		rxport_fatal = B_TRUE;
+	}
+
+	if (rxport_fatal) {
+		NXGE_ERROR_MSG((nxgep, RX_CTL,
+			    " nxge_rxdma_handle_port_error: "
+			    " fatal error on Port #%d\n",
+				portn));
+	}
+
+	NXGE_DEBUG_MSG((nxgep, RX_CTL, "<== nxge_rxdma_handle_port_errors"));
+
+	return (status);
+}
+
+static nxge_status_t
+nxge_rxdma_fatal_err_recover(p_nxge_t nxgep, uint16_t channel)
+{
+	npi_handle_t		handle;
+	npi_status_t		rs = NPI_SUCCESS;
+	nxge_status_t		status = NXGE_OK;
+	p_rx_rbr_ring_t		rbrp;
+	p_rx_rcr_ring_t		rcrp;
+	p_rx_mbox_t		mboxp;
+	rx_dma_ent_msk_t	ent_mask;
+	int			ring_idx;
+	p_rx_msg_t		rx_msg_p;
+	int			i;
+
+	NXGE_DEBUG_MSG((nxgep, RX_CTL, "<== nxge_rxdma_fatal_err_recover"));
+	/*
+	 * Stop the dma channel waits for the stop done.
+	 * If the stop done bit is not set, then create
+	 * an error.
+	 */
+
+	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	printk(KERN_INFO "Rx stop...\n");
+
+	ring_idx = nxge_rxdma_get_ring_index(nxgep, channel);
+	rbrp = (p_rx_rbr_ring_t)nxgep->rx_rbr_rings->rings[ring_idx];
+	rcrp = (p_rx_rcr_ring_t)nxgep->rx_rcr_rings->rings[ring_idx];
+
+	MUTEX_ENTER(&rcrp->lock);
+	MUTEX_ENTER(&rbrp->lock);
+
+	rs = npi_rxdma_cfg_rdc_disable(handle, channel);
+	if (rs != NPI_SUCCESS) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_disable_rxdma_channel:failed"));
+		goto fail;
+	}
+
+	/* Disable interrupt */
+	ent_mask.value = RX_DMA_ENT_MSK_ALL &
+		~(RX_DMA_ENT_MSK_WRED_DROP_MASK | RX_DMA_ENT_MSK_PTDROP_PKT_MASK);
+	rs = npi_rxdma_event_mask(handle, OP_SET, channel, &ent_mask);
+	if (rs != NPI_SUCCESS) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"nxge_rxdma_stop_channel: "
+				"set rxdma event masks failed (channel %d)",
+				channel));
+	}
+
+	printk(KERN_INFO "Rx reset...\n");
+
+	/* Reset RXDMA channel */
+	rs = npi_rxdma_cfg_rdc_reset(handle, channel);
+	if (rs != NPI_SUCCESS) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_rxdma_fatal_err_recover: "
+				" reset rxdma failed (channel %d)", channel));
+		goto fail;
+	}
+
+	mboxp =
+	(p_rx_mbox_t)nxgep->rx_mbox_areas_p->rxmbox_areas[ring_idx];
+
+	rbrp->rbr_wr_index = (rbrp->ring_size - 1);
+
+	rcrp->out = 0;
+	rbrp->tmp_count = 0;
+
+	memset((void *)(rcrp->rcr_desc.cpu_addr), 0, rcrp->rcr_desc.alloc_len);
+
+	printk(KERN_INFO "rbr entries = %d\n", rbrp->ring_size);
+
+	for (i = 0; i < rbrp->ring_size; i++) {
+		rx_msg_p = &rbrp->rx_msg_ring[i];
+		/* Buffer can be re-posted */
+		rx_msg_p->free = B_TRUE;
+		rx_msg_p->cur_usage_cnt = 0;
+		nxge_post_buf_blk(nxgep, rbrp, rx_msg_p);
+	}
+
+	printk(KERN_INFO "Rx start...\n");
+	status = nxge_rxdma_start_channel(nxgep, channel, rbrp, rcrp, mboxp);
+	if (status != NXGE_OK) {
+		goto fail;
+	}
+
+	MUTEX_EXIT(&rbrp->lock);
+	MUTEX_EXIT(&rcrp->lock);
+
+	NXGE_DEBUG_MSG((nxgep, RX_CTL, "==> nxge_rxdma_fatal_err_recover"));
+
+	return (NXGE_OK);
+fail:
+	MUTEX_EXIT(&rbrp->lock);
+	MUTEX_EXIT(&rcrp->lock);
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+		"nxge_rxdma_fatal_err_recover (channel %d): "
+		"failed to recover this rxdma channel"));
+
+	return (NXGE_ERROR | rs);
+}
+
+nxge_status_t
+nxge_rx_port_fatal_err_recover(p_nxge_t nxgep)
+{
+	nxge_status_t		status = NXGE_OK;
+	uint16_t		channel;
+	int			ndmas;
+	int			i;
+
+	NXGE_DEBUG_MSG((nxgep, RX_CTL, "<== nxge_rx_port_fatal_err_recover"));
+
+	if (nxge_rx_mac_disable(nxgep) != NXGE_OK)
+		goto fail;
+
+	NXGE_DELAY(1000);
+
+	ndmas = nxgep->max_rdcs;
+
+	printk(KERN_INFO "Stop all rxdma channels...\n");
+	for (i = 0; i < ndmas; i++) {
+		channel = nxgep->rx_rcr_rings->rings[i]->rdc;
+		if (nxge_rxdma_fatal_err_recover(nxgep, channel) != NXGE_OK) {
+			printk(KERN_INFO "Could not recover channel %d\n",
+					channel);
+		}
+	}
+
+	if (nxge_ipp_reset(nxgep) != NXGE_OK) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"nxge_rx_port_fatal_err_recover: "
+				"Failed to reset IPP"));
+		goto fail;
+	}
+
+	if (nxge_rx_mac_reset(nxgep) != NXGE_OK) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"nxge_rx_port_fatal_err_recover: "
+				"Failed to reset RXMAC"));
+		goto fail;
+	}
+
+	if (nxge_ipp_init(nxgep) != NXGE_OK) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"nxge_rx_port_fatal_err_recover: "
+				"Failed to init IPP"));
+		goto fail;
+	}
+
+	if ((status = nxge_rx_mac_init(nxgep)) != NXGE_OK) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"nxge_rx_port_fatal_err_recover: "
+				"Failed to reset RxMAC"));
+		goto fail;
+	}
+
+	if ((status = nxge_rx_mac_enable(nxgep)) != NXGE_OK) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"nxge_rx_port_fatal_err_recover: "
+				"Failed to enable RxMAC"));
+		goto fail;
+	}
+
+	return (NXGE_OK);
+fail:
+	return (status);
+}
+
+void
+nxge_rxdma_inject_err(p_nxge_t nxgep, uint32_t err_id, uint8_t chan)
+{
+	rx_dma_ctl_stat_t	cs;
+	rx_ctl_dat_fifo_stat_t	cdfs;
+
+	switch (err_id) {
+	case NXGE_FM_EREPORT_RDMC_RCR_ACK_ERR:
+	case NXGE_FM_EREPORT_RDMC_DC_FIFO_ERR:
+	case NXGE_FM_EREPORT_RDMC_RCR_SHA_PAR:
+	case NXGE_FM_EREPORT_RDMC_RBR_PRE_PAR:
+	case NXGE_FM_EREPORT_RDMC_RBR_TMOUT:
+	case NXGE_FM_EREPORT_RDMC_RSP_CNT_ERR:
+	case NXGE_FM_EREPORT_RDMC_BYTE_EN_BUS:
+	case NXGE_FM_EREPORT_RDMC_RSP_DAT_ERR:
+	case NXGE_FM_EREPORT_RDMC_RCRINCON:
+	case NXGE_FM_EREPORT_RDMC_RCRFULL:
+	case NXGE_FM_EREPORT_RDMC_RBRFULL:
+	case NXGE_FM_EREPORT_RDMC_RBRLOGPAGE:
+	case NXGE_FM_EREPORT_RDMC_CFIGLOGPAGE:
+		RXDMA_REG_READ64(nxgep->npi_handle, RX_DMA_CTL_STAT_DBG_REG,
+			chan, &cs.value);
+		if (err_id == NXGE_FM_EREPORT_RDMC_RCR_ACK_ERR)
+			cs.bits.hdw.rcr_ack_err = 1;
+		else if (err_id == NXGE_FM_EREPORT_RDMC_DC_FIFO_ERR)
+			cs.bits.hdw.dc_fifo_err = 1;
+		else if (err_id == NXGE_FM_EREPORT_RDMC_RCR_SHA_PAR)
+			cs.bits.hdw.rcr_sha_par = 1;
+		else if (err_id == NXGE_FM_EREPORT_RDMC_RBR_PRE_PAR)
+			cs.bits.hdw.rbr_pre_empty = 1;
+		else if (err_id == NXGE_FM_EREPORT_RDMC_RBR_TMOUT)
+			cs.bits.hdw.rbr_tmout = 1;
+		else if (err_id == NXGE_FM_EREPORT_RDMC_RSP_CNT_ERR)
+			cs.bits.hdw.rsp_cnt_err = 1;
+		else if (err_id == NXGE_FM_EREPORT_RDMC_BYTE_EN_BUS)
+			cs.bits.hdw.byte_en_bus = 1;
+		else if (err_id == NXGE_FM_EREPORT_RDMC_RSP_DAT_ERR)
+			cs.bits.hdw.rsp_dat_err = 1;
+		else if (err_id == NXGE_FM_EREPORT_RDMC_CONFIG_ERR)
+			cs.bits.hdw.config_err = 1;
+		else if (err_id == NXGE_FM_EREPORT_RDMC_RCRINCON)
+			cs.bits.hdw.rcrincon = 1;
+		else if (err_id == NXGE_FM_EREPORT_RDMC_RCRFULL)
+			cs.bits.hdw.rcrfull = 1;
+		else if (err_id == NXGE_FM_EREPORT_RDMC_RBRFULL)
+			cs.bits.hdw.rbrfull = 1;
+		else if (err_id == NXGE_FM_EREPORT_RDMC_RBRLOGPAGE)
+			cs.bits.hdw.rbrlogpage = 1;
+		else if (err_id == NXGE_FM_EREPORT_RDMC_CFIGLOGPAGE)
+			cs.bits.hdw.cfiglogpage = 1;
+		printk(KERN_INFO "Write 0x%llx to RX_DMA_CTL_STAT_DBG_REG\n",
+				cs.value);
+		RXDMA_REG_WRITE64(nxgep->npi_handle, RX_DMA_CTL_STAT_DBG_REG,
+			chan, cs.value);
+		break;
+	case NXGE_FM_EREPORT_RDMC_ID_MISMATCH:
+	case NXGE_FM_EREPORT_RDMC_ZCP_EOP_ERR:
+	case NXGE_FM_EREPORT_RDMC_IPP_EOP_ERR:
+		RXDMA_REG_READ64(nxgep->npi_handle,
+			RX_CTL_DAT_FIFO_STAT_DBG_REG, chan, &cdfs.value);
+		if (err_id ==  NXGE_FM_EREPORT_RDMC_ID_MISMATCH)
+			cdfs.bits.ldw.id_mismatch = 1;
+		else if (err_id == NXGE_FM_EREPORT_RDMC_ZCP_EOP_ERR)
+			cdfs.bits.ldw.zcp_eop_err = 1;
+		else if (err_id == NXGE_FM_EREPORT_RDMC_IPP_EOP_ERR)
+			cdfs.bits.ldw.ipp_eop_err = 1;
+		printk(KERN_INFO "Write 0x%llx to RX_CTL_DAT_FIFO_STAT_DBG_REG\n", cs.value);
+		RXDMA_REG_WRITE64(nxgep->npi_handle,
+			RX_CTL_DAT_FIFO_STAT_DBG_REG, chan, cdfs.value);
+		break;
+	case NXGE_FM_EREPORT_RDMC_DCF_ERR:
+		break;
+	case NXGE_FM_EREPORT_RDMC_COMPLETION_ERR:
+		break;
+	}
+}
+
+int
+nxge_dump_rxdma_channel(p_nxge_t nep, uint8_t channel)
+{
+	int			status = NXGE_OK;
+	npi_handle_t		handle;
+
+	printk(KERN_INFO "==> nxge_dump_rxdma_channel\n");
+
+	handle = nep->npi_handle;
+	status = npi_rxdma_dump_rdc_regs(handle, channel);
+
+	if (status != NPI_SUCCESS) {
+		status = NXGE_ERROR;
+	}
+
+	printk(KERN_INFO "<== nxge_dump_rxdma_channel\n");
+
+	return (status);
+}
+
+void
+nxge_rxdma_dump_channel_stats(p_nxge_t nep, int i, uint8_t channel)
+{
+	nxge_rx_ring_stats_t	stats = nep->statsp->rdc_stats[i];
+
+	printk(KERN_INFO "\n\nStats for channel [%d]\n", channel);
+
+	printk(KERN_INFO "\n\tipackets\t\t0x%llx", stats.ipackets);
+	printk(KERN_INFO "\n\tibytes\t\t0x%llx", stats.ibytes);
+	printk(KERN_INFO "\n\trx_bufsz0_rcvd\t\t0x%llx", stats.rx_bufsz0_rcvd);
+	printk(KERN_INFO "\n\trx_bufsz1_rcvd\t\t0x%llx", stats.rx_bufsz1_rcvd);
+	printk(KERN_INFO "\n\trx_bufsz2_rcvd\t\t0x%llx", stats.rx_bufsz2_rcvd);
+	printk(KERN_INFO "\n\trx_sgl_blk_rcvd\t\t0x%llx",
+	       stats.rx_sgl_blk_rcvd);
+	printk(KERN_INFO "\n\n");
+}
+
+void
+nxge_rxdma_regs_dump_channels(p_nxge_t nep)
+{
+	int			i, ndmas;
+	uint16_t		channel;
+	p_rx_rbr_rings_t 	rx_rbr_rings;
+	p_rx_rbr_ring_t		*rbr_rings;
+	npi_handle_t		handle;
+
+	printk(KERN_INFO "==> nxge_rxdma_regs_dump_channels\n");
+
+	handle = NXGE_DEV_NPI_HANDLE(nep);
+	(void) npi_rxdma_dump_fzc_regs(handle);
+
+	rx_rbr_rings = nep->rx_rbr_rings;
+	if (rx_rbr_rings == NULL) {
+		printk(KERN_INFO "<== nxge_rxdma_regs_dump_channels: "
+			"NULL ring pointer\n");
+		return;
+	}
+
+	ndmas = nep->max_rdcs;
+	if (!ndmas) {
+		printk(KERN_INFO "<== nxge_rxdma_regs_dump_channels: no channel\n");
+		return;
+	}
+
+	printk(KERN_INFO "==> nxge_rxdma_regs_dump_channels (ndmas %d)\n", ndmas);
+
+	rbr_rings = rx_rbr_rings->rings;
+	for (i = 0; i < ndmas; i++) {
+		if (rbr_rings == NULL || rbr_rings[i] == NULL) {
+			continue;
+		}
+		channel = rbr_rings[i]->rdc;
+#if 1
+		nxge_rxdma_regs_dump(nep, channel);
+#endif
+		nxge_dump_rxdma_channel(nep, channel);
+		nxge_rxdma_dump_channel_stats(nep, i, channel);
+	}
+
+	printk(KERN_INFO "<== nxge_rxdma_regs_dump_channels\n");
+}
+
+void
+nxge_dump_rcr_entry(p_nxge_t nep, p_rcr_entry_t entry_p)
+{
+
+	uint32_t bptr;
+	uint64_t pp;
+	char *ptr;
+
+	uint64_t		rcr_entry;
+
+	bptr = entry_p->bits.hdw.pkt_buf_addr;
+
+	rcr_entry = *((uint64_t *)entry_p);
+	printk(KERN_INFO "\trcr entry $%p "
+		"\trcr entry 0x%llx "
+/* 		"\trcr entry 0x%08x " */
+/* 		"\trcr entry 0x%08x " */
+		"\tvalue 0x%llx\n"
+		"\tmulti = %d\n"
+		"\tpkt_type = 0x%x\n"
+		"\tzero_copy = %d\n"
+		"\tnoport = %d\n"
+		"\tpromis = %d\n"
+		"\terror = 0x%04x\n"
+		"\tdcf_err = 0x%01x\n"
+		"\tl2_len = %d\n"
+		"\tpktbufsize = %d\n"
+		"\tpkt_buf_addr (<< 6) = 0x%lx\n",
+		entry_p,
+		*(int64_t *)entry_p,
+/* 		*(int32_t *)entry_p, */
+/* 		*(int32_t *)((char *)entry_p + 32), */
+		entry_p->value,
+		entry_p->bits.hdw.multi,
+		entry_p->bits.hdw.pkt_type,
+		entry_p->bits.hdw.zero_copy,
+		entry_p->bits.hdw.noport,
+		entry_p->bits.hdw.promis,
+		entry_p->bits.hdw.error,
+		entry_p->bits.hdw.dcf_err,
+		entry_p->bits.hdw.l2_len,
+		entry_p->bits.hdw.pktbufsz,
+		(unsigned long)(entry_p->bits.ldw.pkt_buf_addr));
+
+/* 	       (rcr_entry & RCR_MULTI_MASK) >> RCR_MULTI_SHIFT, */
+/* 	       (rcr_entry & RCR_PKT_TYPE_MASK) >> RCR_PKT_TYPE_SHIFT, */
+/* 	       (rcr_entry & RCR_ZERO_COPY_MASK) >> RCR_ZERO_COPY_SHIFT, */
+/* 	       (rcr_entry & RCR_FRAG_MASK) >> RCR_FRAG_SHIFT, */
+/* 	       (rcr_entry & RCR_PROMIS_MASK) >> RCR_PROMIS_SHIFT, */
+/* 	       (rcr_entry & RCR_ERROR_MASK) >> RCR_ERROR_SHIFT, */
+/* 	       (rcr_entry & RCR_DCF_ERROR_MASK) >> RCR_DCF_ERROR_SHIFT, */
+/* 	       (rcr_entry & RCR_L2_LEN_MASK) >> RCR_L2_LEN_SHIFT, */
+/* 	       (rcr_entry & RCR_PKTBUFSZ_MASK) >> RCR_PKTBUFSZ_SHIFT, */
+/* 	       (rcr_entry & RCR_PKT_BUF_ADDR_MASK) >> RCR_PKT_BUF_ADDR_SHIFT); */
+
+	ptr = (char *)entry_p;
+
+	pp = (entry_p->value & RCR_PKT_BUF_ADDR_MASK) <<
+		RCR_PKT_BUF_ADDR_SHIFT_FULL;
+
+	printk(KERN_INFO "rcr pp 0x%llx \n\n", pp);
+}
+
+void
+nxge_rxdma_regs_dump(p_nxge_t nep, int rdc)
+{
+	npi_handle_t		handle;
+	rbr_stat_t 		rbr_stat;
+	addr44_t 		hd_addr;
+	addr44_t 		tail_addr;
+	uint16_t 		qlen;
+
+	printk(KERN_INFO "==> nxge_rxdma_regs_dump: rdc channel %d\n", rdc);
+
+	handle = nep->npi_handle;
+
+	/* RBR head */
+	hd_addr.addr = 0;
+	npi_rxdma_rdc_rbr_head_get(handle, rdc, &hd_addr);
+
+	printk(KERN_INFO "nxge_rxdma_regs_dump: got hdptr 0x%llx \n",
+	       hd_addr.addr);
+
+	/* RBR stats */
+	npi_rxdma_rdc_rbr_stat_get(handle, rdc, &rbr_stat);
+
+	printk(KERN_INFO "nxge_rxdma_regs_dump: rbr len %d \n",
+	       rbr_stat.bits.ldw.qlen);
+
+	/* RCR tail */
+	tail_addr.addr = 0;
+	npi_rxdma_rdc_rcr_tail_get(handle, rdc, &tail_addr);
+
+	printk(KERN_INFO "nxge_rxdma_regs_dump: got tail ptr 0x%llx \n",
+	       tail_addr.addr);
+
+	/* RCR qlen */
+	npi_rxdma_rdc_rcr_qlen_get(handle, rdc, &qlen);
+
+	printk(KERN_INFO "nxge_rxdma_regs_dump: rcr len %x \n", qlen);
+
+	printk(KERN_INFO "<== nxge_rxdma_regs_dump: rdc %d\n", rdc);
+}
+
+int
+nxge_rxdma_channel_rcrflush(p_nxge_t nep, uint8_t channel)
+{
+	npi_handle_t		handle;
+	int			status = NXGE_OK;
+
+	handle = nep->npi_handle;
+	npi_rxdma_rdc_rcr_flush(handle, channel);
+
+	return (status);
+
+}
+
+boolean_t
+nxge_check_rdcgrp_port_member(p_nxge_t nxgep, uint8_t rdc_grp)
+{
+	p_nxge_dma_pt_cfg_t	p_all_cfgp;
+	p_nxge_hw_pt_cfg_t	p_cfgp;
+
+	NXGE_DEBUG_MSG((nxgep, MOD_CTL, "==> nxge_check_rdcgrp_port_member"));
+
+	p_all_cfgp = (p_nxge_dma_pt_cfg_t)&nxgep->pt_config;
+	p_cfgp = (p_nxge_hw_pt_cfg_t)&p_all_cfgp->hw_config;
+
+	if ((rdc_grp >= p_cfgp->start_grpid) &&
+	    (rdc_grp < (p_cfgp->max_grpids + p_cfgp->start_grpid)))
+		return (B_TRUE);
+	else
+		return (B_FALSE);
+}
+
+int
+nxge_rxdma_cfg_rdcgrp_default_rdc(p_nxge_t nxgep, uint8_t rdcgrp,
+				    uint8_t rdc)
+{
+	int status = NXGE_OK;
+	npi_handle_t		handle;
+	p_nxge_dma_pt_cfg_t	p_all_cfgp;
+	p_nxge_rdc_grp_t	rdc_grp_p;
+	uint8_t actual_rdcgrp, actual_rdc;
+
+	NXGE_DEBUG_MSG((nxgep, RX_CTL,
+			    " ==> nxge_rxdma_cfg_rdcgrp_defualt_rdc"));
+	p_all_cfgp = (p_nxge_dma_pt_cfg_t)&nxgep->pt_config;
+	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+
+	rdc_grp_p = &p_all_cfgp->rdc_grps[rdcgrp];
+	rdc_grp_p->rdc[0] = rdc;
+
+	actual_rdcgrp = NXGE_ACTUAL_RDCGRP(nxgep, rdcgrp);
+	actual_rdc = NXGE_ACTUAL_RDC(nxgep, rdc);
+
+	status = npi_rxdma_cfg_rdc_table_default_rdc(handle, actual_rdcgrp,
+						     actual_rdc);
+
+	if (status != NPI_SUCCESS) {
+		return (NXGE_ERROR);
+	}
+	NXGE_DEBUG_MSG((nxgep, RX_CTL,
+			    " <== nxge_rxdma_cfg_rdcgrp_default_rdc"));
+	return (NXGE_OK);
+}
+
+
+int
+nxge_rxdma_cfg_port_default_rdc(p_nxge_t nxgep, uint8_t port, uint8_t rdc)
+{
+	int status = NXGE_OK;
+	npi_handle_t		handle;
+	uint8_t actual_rdc;
+
+	NXGE_DEBUG_MSG((nxgep, RX_CTL,
+			    " ==> nxge_rxdma_cfg_port_default_rdc"));
+
+	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	actual_rdc = NXGE_ACTUAL_RDC(nxgep, rdc);
+	status = npi_rxdma_cfg_default_port_rdc(handle,	port, actual_rdc);
+
+	if (status != NPI_SUCCESS) {
+		return (NXGE_ERROR);
+	}
+	NXGE_DEBUG_MSG((nxgep, RX_CTL,
+			    " <== nxge_rxdma_cfg_port_default_rdc"));
+
+	return (NXGE_OK);
+}
+
+
+int
+nxge_rxdma_cfg_rcr_threshold(p_nxge_t nxgep, uint8_t channel, uint16_t pkts)
+{
+	int status = NXGE_OK;
+	npi_handle_t	handle;
+	NXGE_DEBUG_MSG((nxgep, RX_CTL,
+			    " ==> nxge_rxdma_cfg_rcr_threshold"));
+	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+
+	status = npi_rxdma_cfg_rdc_rcr_threshold(handle, channel,
+						    pkts);
+
+	if (status != NPI_SUCCESS) {
+		return (NXGE_ERROR);
+	}
+	NXGE_DEBUG_MSG((nxgep, RX_CTL, " <== nxge_rxdma_cfg_rcr_threshold"));
+	return (NXGE_OK);
+}
+
+int
+nxge_rxdma_cfg_rcr_timeout(p_nxge_t nxgep, uint8_t channel,
+			    uint16_t tout, uint8_t enable)
+{
+	int status = NXGE_OK;
+	npi_handle_t	handle;
+	NXGE_DEBUG_MSG((nxgep, RX_CTL, " ==> nxge_rxdma_cfg_rcr_timeout"));
+	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	if (enable == 0) {
+		status = npi_rxdma_cfg_rdc_rcr_timeout_disable(handle, channel);
+	} else {
+		status = npi_rxdma_cfg_rdc_rcr_timeout(handle, channel,
+							    tout);
+	}
+
+	if (status != NPI_SUCCESS) {
+		return (NXGE_ERROR);
+	}
+	NXGE_DEBUG_MSG((nxgep, RX_CTL, " <== nxge_rxdma_cfg_rcr_timeout"));
+	return (NXGE_OK);
+}
+
+void
+nxge_rxdma_stop(p_nxge_t nxgep)
+{
+	NXGE_DEBUG_MSG((nxgep, RX_CTL, "==> nxge_rxdma_stop"));
+
+	(void) nxge_link_monitor(nxgep, LINK_MONITOR_STOP);
+	(void) nxge_rx_mac_disable(nxgep);
+	(void) nxge_rxdma_hw_mode(nxgep, NXGE_DMA_STOP);
+	NXGE_DEBUG_MSG((nxgep, RX_CTL, "<== nxge_rxdma_stop"));
+}
+
+void
+nxge_rxdma_stop_reinit(p_nxge_t nxgep)
+{
+	NXGE_DEBUG_MSG((nxgep, RX_CTL, "==> nxge_rxdma_stop_reinit"));
+
+	(void) nxge_rxdma_stop(nxgep);
+	(void) nxge_uninit_rxdma_channels(nxgep);
+	(void) nxge_init_rxdma_channels(nxgep);
+
+	(void) nxge_xcvr_init(nxgep);
+	(void) nxge_link_monitor(nxgep, LINK_MONITOR_START);
+	(void) nxge_rx_mac_enable(nxgep);
+
+	NXGE_DEBUG_MSG((nxgep, RX_CTL, "<== nxge_rxdma_stop_reinit"));
+}
+
+#ifdef USE_BTREE
+/*
+ * used by quick sort (qsort) function
+ * to perform comparision
+ */
+static int nxge_sort_compare(const void *p1, const void *p2)
+{
+
+	rxbuf_index_info_t *a, *b;
+
+	a = (rxbuf_index_info_t *)p1;
+	b = (rxbuf_index_info_t *)p2;
+
+	if (a->dma_addr > b->dma_addr)
+		return (1);
+	if (a->dma_addr < b->dma_addr)
+		return (-1);
+	return (0);
+}
+
+
+
+/*
+ * grabbed this sort implementation from common/syscall/avl.c
+ *
+ */
+/*
+ * Generic shellsort, from K&R (1st ed, p 58.), somewhat modified.
+ * v = Ptr to array/vector of objs
+ * n = # objs in the array
+ * s = size of each obj (must be multiples of a word size)
+ * f = ptr to function to compare two objs
+ *	returns (-1 = less than, 0 = equal, 1 = greater than
+ */
+void
+nxge_ksort(caddr_t v, int n, int s, int (*f)())
+{
+	int g, i, j, ii;
+	unsigned int *p1, *p2;
+	unsigned int tmp;
+
+	/* No work to do */
+	if (v == NULL || n <= 1)
+		return;
+	/* Sanity check on arguments */
+	if (!(((uintptr_t)v & 0x3) == 0 && (s & 0x3) == 0)) {
+	    NXGE_DEBUG_MSG((NULL, MOD_CTL, " nxge_ksort: Incorrect arguments"
+			    " v = 0x%x, s = %d", v, s));
+	}
+	if (!(s > 0)) {
+	  NXGE_DEBUG_MSG((NULL, MOD_CTL, " nxge_ksort: Incorrect arguments"
+			  " s[%d] <= 0", s));
+	}
+
+	for (g = n / 2; g > 0; g /= 2) {
+		for (i = g; i < n; i++) {
+			for (j = i - g; j >= 0 &&
+				(*f)(v + j * s, v + (j + g) * s) == 1;
+					j -= g) {
+				p1 = (unsigned *)(v + j * s);
+				p2 = (unsigned *)(v + (j + g) * s);
+				for (ii = 0; ii < s / 4; ii++) {
+					tmp = *p1;
+					*p1++ = *p2;
+					*p2++ = tmp;
+				}
+			}
+		}
+	}
+}
+
+
+
+/*
+ * Initialize data structures required for rxdma
+ * buffer dvma->vmem address lookup
+ */
+
+static int
+nxge_rxbuf_index_info_init(p_nxge_t nep, p_rx_rbr_ring_t rbrp)
+{
+
+	int index;
+	rxring_info_t *ring_info;
+	int max_iteration = 0, max_index = 0;
+
+	NXGE_DEBUG_MSG((nep, DMA_CTL, "==> nxge_rxbuf_index_info_init"));
+
+	ring_info = rbrp->ring_info;
+	ring_info->search_hint[0] = NO_HINT;
+	ring_info->search_hint[1] = NO_HINT;
+	ring_info->search_hint[2] = NO_HINT;
+	ring_info->search_hint[3] = NO_HINT;
+	max_index = rbrp->chunk_cnt;
+
+		/* read the DVMA address information and sort it */
+		/* do init of the information array */
+
+
+	NXGE_DEBUG_MSG((nep, DMA_CTL, "nxge_rxbuf_index_info_init Sort ptrs"));
+
+		/* sort the array */
+	nxge_ksort((void *)ring_info->buffer_arr, max_index,
+		   sizeof (rxbuf_index_info_t), nxge_sort_compare);
+
+
+	max_iteration = 0;
+	while (max_index >= (1ULL << max_iteration))
+		max_iteration++;
+	ring_info->max_iterations = max_iteration + 1;
+	NXGE_DEBUG_MSG((nep, DMA_CTL,
+			"nxge_rxbuf_index_info_init Find max iter %d",
+			ring_info->max_iterations));
+
+	NXGE_DEBUG_MSG((nep, DMA_CTL, "<== nxge_rxbuf_index_info_init"));
+	return (NXGE_OK);
+}
+
+#endif
+
+static void nxge_hw_start_rx(p_nxge_t nep)
+{
+	NXGE_DEBUG_MSG((nep, RX_CTL, "==> nxge_hw_start_rx"));
+
+	nxge_rxdma_hw_mode(nep, NXGE_DMA_START);
+	nxge_rx_mac_enable(nep);
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "<== nxge_hw_start_rx"));
+}
+
+static void
+nxge_rxdma_fix_channel(p_nxge_t nxgep, uint16_t channel)
+{
+	int		i;
+
+	NXGE_DEBUG_MSG((nxgep, RX_CTL, "==> nxge_rxdma_fix_channel"));
+	i = nxge_rxdma_get_ring_index(nxgep, channel);
+	if (i < 0) {
+		NXGE_DEBUG_MSG((nxgep, RX_CTL,
+			"<== nxge_rxdma_fix_channel: no entry found"));
+		return;
+	}
+
+	nxge_rxdma_fixup_channel(nxgep, channel, i);
+
+	NXGE_DEBUG_MSG((nxgep, RX_CTL, "<== nxge_txdma_fix_channel"));
+}
+
+
+static void
+nxge_rxdma_fixup_channel(p_nxge_t nep, uint16_t channel, int entry)
+{
+	int			ndmas;
+	p_nxge_dma_pool_t	dma_cntl_poolp;
+	p_rx_rbr_rings_t 	rx_rbr_rings;
+	p_rx_rbr_ring_t		*rbr_rings = NULL;
+	p_rx_rcr_rings_t 	rx_rcr_rings;
+	p_rx_rcr_ring_t		*rcr_rings = NULL;
+	p_rx_mbox_areas_t 	rx_mbox_areas_p;
+	p_rx_mbox_t		*rx_mbox_p = NULL;
+	p_rx_rbr_ring_t 	rbrp;
+	p_rx_rcr_ring_t 	rcrp;
+	p_rx_mbox_t 		mboxp;
+
+	int			status = NXGE_OK;
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "==> nxge_rxdma_fixup_channel"));
+
+	(void) nxge_rxdma_stop_channel(nep, channel);
+
+	ndmas = nep->max_rdcs;
+	if (!ndmas) {
+		NXGE_DEBUG_MSG((nep, RX_CTL, "nxge_rxdma_fixup_channel: "
+				"No DMAs allocated"));
+		return;
+	}
+
+	dma_cntl_poolp = nep->rx_cntl_pool_p;
+	if (!dma_cntl_poolp->buf_allocated) {
+		NXGE_DEBUG_MSG((nep, RX_CTL, "nxge_rxdma_fixup_channel: "
+				"No cntl buffers allocated"));
+		return;
+	}
+
+	rx_rbr_rings = nep->rx_rbr_rings;
+	rx_rcr_rings = nep->rx_rcr_rings;
+	rbr_rings = rx_rbr_rings->rings;
+	rcr_rings = rx_rcr_rings->rings;
+	rx_mbox_areas_p = nep->rx_mbox_areas_p;
+	if (rx_mbox_areas_p) {
+		rx_mbox_p = rx_mbox_areas_p->rxmbox_areas;
+	}
+
+
+	/* Reinitialize the receive block and completion rings */
+	rbrp = (p_rx_rbr_ring_t)rbr_rings[entry],
+	rcrp = (p_rx_rcr_ring_t)rcr_rings[entry],
+	mboxp = (p_rx_mbox_t)rx_mbox_p[entry];
+
+	rbrp->rbr_wr_index = rbrp->ring_size - 1;
+	rcrp->out = 0;
+	rbrp->tmp_count = 0;
+
+	memset((void *)(rcrp->rcr_desc.cpu_addr), 0, rcrp->rcr_desc.alloc_len);
+
+
+	status = nxge_rxdma_start_channel(nep, channel, rbrp, rcrp, mboxp);
+	if (status != NXGE_OK) {
+		goto nxge_rxdma_fixup_channel_fail;
+	}
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "<== nxge_rxdma_fixup_channel"));
+
+nxge_rxdma_fixup_channel_fail:
+	NXGE_DEBUG_MSG((nep, RX_CTL,
+		"==> nxge_rxdma_fixup_channel: failed (0x%08x)", status));
+}
+
+
+static void nxge_fixup_rxdma_rings(p_nxge_t nxgep)
+{
+	int			i, ndmas;
+	uint16_t		rdc;
+	p_rx_rbr_rings_t 	rx_rbr_rings;
+	p_rx_rbr_ring_t		*rbr_rings;
+	p_rx_rcr_rings_t 	rx_rcr_rings;
+
+	NXGE_DEBUG_MSG((nxgep, RX_CTL, "==> nxge_fixup_rxdma_rings"));
+
+	/* XXXX: define what needs to be done here (FMA) */
+
+	rx_rbr_rings = nxgep->rx_rbr_rings;
+	if (rx_rbr_rings == NULL) {
+		NXGE_DEBUG_MSG((nxgep, RX_CTL,
+			"<== nxge_fixup_rxdma_rings: NULL ring pointer"));
+		return;
+	}
+	ndmas = nxgep->max_rdcs;
+	if (!ndmas) {
+		NXGE_DEBUG_MSG((nxgep, RX_CTL,
+			"<== nxge_fixup_rxdma_rings: no channel"));
+		return;
+	}
+
+	rx_rcr_rings = nxgep->rx_rcr_rings;
+	if (rx_rcr_rings == NULL) {
+		NXGE_DEBUG_MSG((nxgep, RX_CTL,
+				"<== nxge_fixup_rxdma_rings: NULL ring pointer"));
+		return;
+	}
+	NXGE_DEBUG_MSG((nxgep, RX_CTL,
+		"==> nxge_fixup_rxdma_rings (ndmas %d)", ndmas));
+
+	nxge_rxdma_hw_stop(nxgep);
+
+	rbr_rings = rx_rbr_rings->rings;
+	for (i = 0; i < ndmas; i++) {
+		rdc = rbr_rings[i]->rdc;
+		NXGE_DEBUG_MSG((nxgep, RX_CTL,
+			"==> nxge_fixup_rxdma_rings: channel %d "
+			"ring $%p", rdc, rbr_rings[i]));
+		/* XXXX: buffers still owned by the protocol stack */
+		(void) nxge_rxdma_fixup_channel(nxgep, rdc, i);
+	}
+
+	NXGE_DEBUG_MSG((nxgep, RX_CTL, "<== nxge_fixup_rxdma_rings"));
+}
+
+static int
+nxge_rxdma_get_ring_index(p_nxge_t nxgep, uint16_t channel)
+{
+	int			i, ndmas;
+	uint16_t		rdc;
+	p_rx_rbr_rings_t 	rx_rbr_rings;
+
+	NXGE_DEBUG_MSG((nxgep, RX_CTL,
+		"==> nxge_rxdma_get_ring_index: channel %d", channel));
+
+	rx_rbr_rings = nxgep->rx_rbr_rings;
+	if (rx_rbr_rings == NULL) {
+		NXGE_DEBUG_MSG((nxgep, RX_CTL,
+			"<== nxge_rxdma_get_ring_index: NULL ring pointer"));
+		return (-1);
+	}
+	ndmas = nxgep->max_rdcs;
+	if (!ndmas) {
+		NXGE_DEBUG_MSG((nxgep, RX_CTL,
+			"<== nxge_rxdma_get_ring_index: no channel"));
+		return (-1);
+	}
+
+	NXGE_DEBUG_MSG((nxgep, RX_CTL,
+		"==> nxge_rxdma_get_ring_index (ndmas %d)", ndmas));
+
+	if (rx_rbr_rings->rings == NULL) {
+		NXGE_DEBUG_MSG((nxgep, RX_CTL,
+			"<== nxge_rxdma_get_ring_index: NULL rings pointer"));
+		return (-1);
+	}
+
+	for (i = 0; i < ndmas; i++) {
+		rdc = rx_rbr_rings->rings[i]->rdc;
+		if (channel == rdc) {
+			NXGE_DEBUG_MSG((nxgep, RX_CTL,
+				"==> nxge_rxdma_get_rbr_ring: "
+				"channel %d (index %d) "
+				"ring $%p", channel, i,
+				rx_rbr_rings->rings[i]));
+			return (i);
+		}
+	}
+
+	NXGE_DEBUG_MSG((nxgep, RX_CTL,
+		"<== nxge_rxdma_get_rbr_ring_index: not found"));
+
+	return (-1);
+}
+
+static p_rx_rbr_ring_t
+nxge_rxdma_get_rbr_ring(p_nxge_t nxgep, uint16_t channel)
+{
+	int			i, ndmas;
+	uint16_t		rdc;
+	p_rx_rbr_rings_t 	rx_rbr_rings;
+	p_rx_rbr_ring_t		*rbr_rings;
+
+	NXGE_DEBUG_MSG((nxgep, RX_CTL,
+		"==> nxge_rxdma_get_rbr_ring: channel %d", channel));
+
+	rx_rbr_rings = nxgep->rx_rbr_rings;
+	if (rx_rbr_rings == NULL) {
+		NXGE_DEBUG_MSG((nxgep, RX_CTL,
+			"<== nxge_rxdma_get_rbr_ring: NULL ring pointer"));
+		return (NULL);
+	}
+	ndmas = nxgep->max_rdcs;
+	if (!ndmas) {
+		NXGE_DEBUG_MSG((nxgep, RX_CTL,
+			"<== nxge_rxdma_get_rbr_ring: no channel"));
+		return (NULL);
+	}
+
+	NXGE_DEBUG_MSG((nxgep, RX_CTL,
+		"==> nxge_rxdma_get_ring (ndmas %d)", ndmas));
+
+	rbr_rings = rx_rbr_rings->rings;
+	for (i = 0; i < ndmas; i++) {
+		rdc = rbr_rings[i]->rdc;
+		if (channel == rdc) {
+			NXGE_DEBUG_MSG((nxgep, RX_CTL,
+				"==> nxge_rxdma_get_rbr_ring: channel %d "
+				"ring $%p", channel, rbr_rings[i]));
+			return (rbr_rings[i]);
+		}
+	}
+
+	NXGE_DEBUG_MSG((nxgep, RX_CTL,
+		"<== nxge_rxdma_get_rbr_ring: not found"));
+
+	return (NULL);
+}
+
+static p_rx_rcr_ring_t
+nxge_rxdma_get_rcr_ring(p_nxge_t nxgep, uint16_t channel)
+{
+	int			i, ndmas;
+	uint16_t		rdc;
+	p_rx_rcr_rings_t 	rx_rcr_rings;
+	p_rx_rcr_ring_t		*rcr_rings;
+
+	NXGE_DEBUG_MSG((nxgep, RX_CTL,
+		"==> nxge_rxdma_get_rcr_ring: channel %d", channel));
+
+	rx_rcr_rings = nxgep->rx_rcr_rings;
+	if (rx_rcr_rings == NULL) {
+		NXGE_DEBUG_MSG((nxgep, RX_CTL,
+			"<== nxge_rxdma_get_rcr_ring: NULL ring pointer"));
+		return (NULL);
+	}
+	ndmas = nxgep->max_rdcs;
+	if (!ndmas) {
+		NXGE_DEBUG_MSG((nxgep, RX_CTL,
+			"<== nxge_rxdma_get_rcr_ring: no channel"));
+		return (NULL);
+	}
+
+	NXGE_DEBUG_MSG((nxgep, RX_CTL,
+		"==> nxge_rxdma_get_rcr_ring (ndmas %d)", ndmas));
+
+	rcr_rings = rx_rcr_rings->rings;
+	for (i = 0; i < ndmas; i++) {
+		rdc = rcr_rings[i]->rdc;
+		if (channel == rdc) {
+			NXGE_DEBUG_MSG((nxgep, RX_CTL,
+				"==> nxge_rxdma_get_rcr_ring: channel %d "
+				"ring $%p", channel, rcr_rings[i]));
+			return (rcr_rings[i]);
+		}
+	}
+
+	NXGE_DEBUG_MSG((nxgep, RX_CTL,
+		"<== nxge_rxdma_get_rcr_ring: not found"));
+
+	return (NULL);
+}
+
+static int
+nxge_init_rxdma_channels(p_nxge_t nxgep)
+{
+	int			status = NXGE_OK;
+
+	NXGE_DEBUG_MSG((nxgep, RX_CTL, "==> nxge_init_rxdma_channels"));
+
+	status = nxge_map_rxdma(nxgep);
+	if (status) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"<== nxge_init_rxdma: status 0x%x", status));
+		return (NXGE_ERROR);
+	}
+
+	status = nxge_rxdma_hw_start_common(nxgep);
+	if (status) {
+		nxge_unmap_rxdma(nxgep);
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"<== nxge_init_rxdma_channels: status 0x%x",
+				status));
+	}
+
+	status = nxge_rxdma_hw_start(nxgep);
+	if (status) {
+		nxge_unmap_rxdma(nxgep);
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"<== nxge_init_rxdma_channels: status 0x%x",
+				status));
+	}
+
+	NXGE_DEBUG_MSG((nxgep, RX_CTL,
+		"<== nxge_init_rxdma_channels: status 0x%x", status));
+
+	return (status);
+}
+
+static void
+nxge_uninit_rxdma_channels(p_nxge_t nxgep)
+{
+	NXGE_DEBUG_MSG((nxgep, RX_CTL, "==> nxge_uninit_rxdma_channels"));
+
+	nxge_rxdma_hw_stop(nxgep);
+	nxge_rxdma_hw_stop_common(nxgep);
+	nxge_unmap_rxdma(nxgep);
+
+	NXGE_DEBUG_MSG((nxgep, RX_CTL,
+		"<== nxge_uinit_rxdma_channels"));
+}
+
+static int
+nxge_disable_rxdma_channel(p_nxge_t nep, uint16_t channel)
+{
+	npi_handle_t		handle;
+	int			status = NXGE_OK;
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "==> nxge_disable_rxdma_channel"));
+	handle = nep->npi_handle;
+
+	/* disable the DMA */
+	status = npi_rxdma_cfg_rdc_disable(handle, channel);
+	if (status != NPI_SUCCESS) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+			"<== nxge_disable_rxdma_channel:failed (0x%x)",
+			status));
+		return (NXGE_ERROR);
+	}
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "<== nxge_disable_rxdma_channel"));
+	return (status);
+}
+
+static int
+nxge_enable_rxdma_channel(p_nxge_t nep, uint16_t channel,
+			  p_rx_rbr_ring_t rbr_p, p_rx_rcr_ring_t rcr_p,
+			  p_rx_mbox_t mbox_p)
+{
+	npi_handle_t		handle;
+	rdc_desc_cfg_t 		rdc_desc;
+	p_rcrcfig_b_t		cfgb_p;
+	int			status = NXGE_OK;
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "==> nxge_enable_rxdma_channel"));
+	handle = nep->npi_handle;
+	/*
+	 * Use configuration data composed at init time.
+	 * Write to hardware the receive ring configurations.
+	 */
+	memset((void *)&rdc_desc, 0, sizeof (rdc_desc_cfg_t));
+#if 1
+	rdc_desc.mbox_enable = 1;
+#endif
+	rdc_desc.mbox_addr = mbox_p->mbox_addr;
+	NXGE_DEBUG_MSG((nep, RX_CTL,
+		"==> nxge_enable_rxdma_channel: mboxp $%p($%p)",
+		 mbox_p->mbox_addr, rdc_desc.mbox_addr));
+
+	rdc_desc.rbr_len = rbr_p->ring_size;
+	rdc_desc.rbr_addr = rbr_p->rbr_addr;
+
+	switch (nep->rx_bksize_code) {
+	case RBR_BKSIZE_4K:
+		rdc_desc.page_size = SIZE_4KB;
+		break;
+	case RBR_BKSIZE_8K:
+		rdc_desc.page_size = SIZE_8KB;
+		break;
+	case RBR_BKSIZE_16K:
+		rdc_desc.page_size = SIZE_16KB;
+		break;
+	case RBR_BKSIZE_32K:
+		rdc_desc.page_size = SIZE_32KB;
+		break;
+	}
+
+	rdc_desc.size0 = rbr_p->npi_pkt_buf_size0;
+	rdc_desc.valid0 = 1;
+
+	rdc_desc.size1 = rbr_p->npi_pkt_buf_size1;
+	rdc_desc.valid1 = 1;
+
+	rdc_desc.size2 = rbr_p->npi_pkt_buf_size2;
+	rdc_desc.valid2 = 1;
+
+	rdc_desc.full_hdr = rcr_p->full_hdr_flag;
+	rdc_desc.offset = rcr_p->sw_priv_hdr_len;
+
+	rdc_desc.rcr_len = rcr_p->ring_size;
+	rdc_desc.rcr_addr = rcr_p->rcr_addr;
+
+	cfgb_p = &(rcr_p->rcr_cfgb);
+	rdc_desc.rcr_threshold = cfgb_p->bits.ldw.pthres;
+	rdc_desc.rcr_timeout = cfgb_p->bits.ldw.timeout;
+	rdc_desc.rcr_timeout_enable = cfgb_p->bits.ldw.entout;
+
+	status = npi_rxdma_cfg_rdc_ring(handle, rbr_p->rdc, &rdc_desc);
+	if (status != NPI_SUCCESS) {
+		return (NXGE_ERROR);
+	}
+
+	/*
+	 * Enable the timeout and threshold.
+	 */
+	status = npi_rxdma_cfg_rdc_rcr_threshold(handle, channel,
+						 rdc_desc.rcr_threshold);
+	if (status != NPI_SUCCESS) {
+		return (NXGE_ERROR);
+	}
+
+	status = npi_rxdma_cfg_rdc_rcr_timeout(handle, channel,
+					       rdc_desc.rcr_timeout);
+	if (status != NPI_SUCCESS) {
+		return (NXGE_ERROR);
+	}
+
+	/* Enable the DMA */
+	status = npi_rxdma_cfg_rdc_enable(handle, channel);
+	if (status != NPI_SUCCESS) {
+		return (NXGE_ERROR);
+	}
+
+	/* TODO: other initialization functions. */
+	/* Kick the DMA engine. */
+	npi_rxdma_rdc_rbr_kick(handle, channel, rbr_p->rbb_max);
+
+	/* Clear the rbr empty bit */
+	npi_rxdma_channel_rbr_empty_clear(handle, channel);
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "<== nxge_enable_rxdma_channel"));
+
+	return (status);
+}
+
+static void nxge_free_rx_buf_pool(p_nxge_t nep, int ring)
+{
+	int			i;
+	p_rx_rbr_ring_t		rx_rbr_ring_p;
+	p_nxge_dma_buf_t	dma_buf_p;
+
+#ifdef USE_BTREE
+	p_rxbuf_index_info_t    rxbuf_idx_info_p;
+#endif
+
+	int			chunk_cnt;
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "==> nxge_free_rx_buf_pool"));
+
+	if (!nep->rx_rbr_rings || !nep->rx_rbr_rings->rings) {
+		NXGE_DEBUG_MSG((nep, RX_CTL, "<== nxge_free_rx_buf_pool"
+				       " Buffers/rings not allocated"));
+		return;
+	}
+
+#ifdef USE_BTREE
+	if (!nep->rx_rbr_rings->rings[ring]->ring_info) {
+		NXGE_DEBUG_MSG((nep, RX_CTL, "<== nxge_free_rx_buf_pool"
+				       " Buffers/rings not allocated"));
+		return;
+	}
+#endif
+
+	rx_rbr_ring_p = nep->rx_rbr_rings->rings[ring];
+	chunk_cnt = rx_rbr_ring_p->chunk_cnt;
+	dma_buf_p = rx_rbr_ring_p->dma_buf_p;
+
+#ifdef USE_BTREE
+	rxbuf_idx_info_p = rx_rbr_ring_p->ring_info->buffer_arr;
+#endif
+
+	/* Unmap and free the page chunks */
+	for (i = 0; i < chunk_cnt; i++) {
+		pci_unmap_page(nep->pdev, dma_buf_p[i].dma_addr,
+			       nep->page_size, PCI_DMA_FROMDEVICE);
+		__free_pages(dma_buf_p[i].page_p, nep->page_order);
+	}
+
+	/* Clear message ring addresses */
+	for (i = 0; i < rx_rbr_ring_p->rbb_max; i++) {
+		rx_rbr_ring_p->rx_msg_ring[i].cpu_addr = 0;
+		rx_rbr_ring_p->rx_msg_ring[i].dma_addr = 0;
+	}
+
+	KMEM_FREE(dma_buf_p, rx_rbr_ring_p->chunk_cnt * sizeof (nxge_dma_buf_t));
+#ifdef USE_BTREE
+	KMEM_FREE(rxbuf_idx_info_p, sizeof (rxbuf_index_info_t) * rx_rbr_ring_p->chunk_cnt);
+#endif
+
+	nep->rx_rbr_rings->rxbuf_allocated = B_FALSE;
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "<== nxge_free_rx_buf_pool"));
+}
+
+static int nxge_alloc_rx_buf_pool(p_nxge_t nep, int ring, size_t block_size,
+				  uint16_t channel)
+{
+	int			status = NXGE_OK;
+	int			i, j, k;
+	p_rx_rbr_ring_t		rx_rbr_ring_p;
+	p_nxge_dma_buf_t	dma_buf_p;
+#ifdef USE_BTREE
+	p_rxbuf_index_info_t    rxbuf_idx_info_p;
+#endif
+	p_rx_msg_t		msg_p;
+	size_t			total_ring_buf_size;
+	int			chunk_cnt;
+	int			blks_in_chunk;
+	NXGE_DEBUG_MSG((nep, RX_CTL, "==> nxge_alloc_rx_buf_pool"));
+
+	rx_rbr_ring_p = nep->rx_rbr_rings->rings[ring];
+
+	total_ring_buf_size = block_size * rx_rbr_ring_p->rbb_max;
+
+	chunk_cnt = total_ring_buf_size / nep->page_size;
+	chunk_cnt += (((nep->page_size * chunk_cnt) <= total_ring_buf_size) ? 0 : 1);
+	dma_buf_p = (p_nxge_dma_buf_t) KMEM_ZALLOC(sizeof (nxge_dma_buf_t) *
+						   chunk_cnt, GFP_KERNEL);
+
+	NXGE_DEBUG_MSG((nep, RX2_CTL,
+					"nxge_alloc_rx_buf_pool: ring %d total size 0x%x chunk count 0x%x\n"
+					" Block size 0x%x rbb_max 0x%x ",
+					ring, total_ring_buf_size, chunk_cnt, block_size, rx_rbr_ring_p->rbb_max));
+	if (dma_buf_p == NULL) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+				"<== nxge_alloc_rx_buf_pool: "
+				"kmalloc failed for dma_buf"));
+		return (NXGE_ERROR);
+	}
+#ifdef USE_BTREE
+	rxbuf_idx_info_p = (p_rxbuf_index_info_t)
+	  KMEM_ZALLOC(sizeof (rxbuf_index_info_t) * chunk_cnt, GFP_KERNEL);
+
+	if (rxbuf_idx_info_p == NULL) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+				"<== nxge_alloc_rx_buf_pool: "
+				"kmalloc failed for rxbuf_idx_info"));
+		KMEM_FREE(dma_buf_p, sizeof (nxge_dma_buf_t) * chunk_cnt);
+		return (NXGE_ERROR);
+	}
+#endif
+
+	rx_rbr_ring_p->chunk_cnt = chunk_cnt;
+
+	/* allocate the page chunks. each chunk is a bunch of contiguous pages */
+	for (i = 0; i < chunk_cnt; i++) {
+		dma_buf_p[i].page_p = alloc_pages(GFP_ATOMIC, nep->page_order);
+		if (dma_buf_p[i].page_p == NULL) {
+			NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+					"<== nxge_alloc_rx_buf_pool: "
+					"alloc_pages failed for order %d, "
+					"chunk %d",
+					nep->page_order, i));
+			goto nxge_alloc_rx_buf_pool_err;
+		}
+		dma_buf_p[i].dma_addr = pci_map_page(nep->pdev,
+						     dma_buf_p[i].page_p,
+						     0, nep->page_size,
+						     PCI_DMA_FROMDEVICE);
+		dma_buf_p[i].dma_channel = channel;
+		dma_buf_p[i].alloc_len = nep->page_size;
+		dma_buf_p[i].block_size = block_size;
+		dma_buf_p[i].dma_chunk_index = i;
+#ifdef USE_BTREE
+		rxbuf_idx_info_p[i].page_p = dma_buf_p[i].page_p;
+		rxbuf_idx_info_p[i].dma_addr = dma_buf_p[i].dma_addr;
+		rxbuf_idx_info_p[i].block_index = i;
+		rxbuf_idx_info_p[i].block_size = dma_buf_p[i].alloc_len;
+#endif
+	}
+	rx_rbr_ring_p->dma_buf_p = dma_buf_p;
+
+	blks_in_chunk = nep->page_size / block_size;
+	rx_rbr_ring_p->blks_per_chunk = blks_in_chunk;
+
+	/* break up the page chunks to blocks */
+
+	i = 0;
+	for (j = 0; j < chunk_cnt && i < rx_rbr_ring_p->rbb_max; j++) {
+#ifdef USE_BTREE
+	  rxbuf_idx_info_p[j].start_buf_index = i;
+#endif
+		for (k = 0; k < blks_in_chunk && i < rx_rbr_ring_p->rbb_max;
+		     k++, i++) {
+			msg_p = &rx_rbr_ring_p->rx_msg_ring[i];
+			msg_p->free = B_FALSE;
+			msg_p->cur_usage_cnt = 0;
+			msg_p->cpu_addr =
+				((caddr_t)page_address(dma_buf_p[j].page_p)) +
+				(k * block_size);
+			msg_p->dma_addr = dma_buf_p[j].dma_addr +
+				(k * block_size);
+
+			msg_p->block_size = block_size;
+			msg_p->block_index = i;
+		}
+	}
+
+#ifdef USE_BTREE
+	nep->rx_rbr_rings->rings[ring]->ring_info->buffer_arr =
+	  rxbuf_idx_info_p;
+	nep->rx_rbr_rings->rings[ring]->ring_info->block_size_mask =
+	  block_size - 1;
+#endif
+
+	nep->rx_rbr_rings->rxbuf_allocated = B_TRUE;
+	goto nxge_alloc_rx_buf_pool_exit;
+
+nxge_alloc_rx_buf_pool_err:
+#ifdef USE_BTREE
+	KMEM_FREE(rxbuf_idx_info_p, sizeof (rxbuf_index_info_t) * chunk_cnt);
+#endif
+	status = NXGE_ERROR;
+	for (i--; i >= 0; i--) {
+		pci_unmap_page(nep->pdev, dma_buf_p[i].dma_addr,
+			       nep->page_size, PCI_DMA_FROMDEVICE);
+		__free_pages(dma_buf_p[i].page_p, nep->page_order);
+	}
+	KMEM_FREE(dma_buf_p, sizeof (nxge_dma_buf_t) * chunk_cnt);
+
+
+nxge_alloc_rx_buf_pool_exit:
+	NXGE_DEBUG_MSG((nep, RX_CTL, "<== nxge_alloc_rx_buf_pool status 0x%x",
+			status));
+	return (status);
+
+}
+
+static void nxge_uninit_rx_buf_ring(p_nxge_t nep, uint16_t channel, int ring)
+{
+	p_rx_rbr_ring_t 	rbrp;
+	p_rx_msg_t 		rx_msg_ring;
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "==> nxge_uninit_rx_buf_ring"));
+
+	rbrp = nep->rx_rbr_rings->rings[ring];
+	if (rbrp == NULL) {
+		NXGE_DEBUG_MSG((nep, RX_CTL, "<== nxge_uninit_rx_buf_ring"
+				       " NULL rbrp"));
+		return;
+	}
+
+	rx_msg_ring = rbrp->rx_msg_ring;
+
+	if (rx_msg_ring == NULL) {
+		NXGE_DEBUG_MSG((nep, RX_CTL, "<== nxge_uninit_rx_buf_ring"
+				       " NULL rx_msg_ring"));
+		return;
+	}
+
+#ifdef USE_BTREE
+	if (rbrp->ring_info == NULL) {
+		NXGE_DEBUG_MSG((nep, RX_CTL, "<== nxge_uninit_rx_buf_ring"
+				       " NULL ring_info"));
+		return;
+	}
+#endif
+
+	nxge_free_rx_buf_pool(nep, ring);
+
+	MUTEX_DESTROY(&rbrp->lock);
+	KMEM_FREE(rx_msg_ring, rbrp->ring_size * sizeof (rx_msg_t));
+#ifdef USE_RING_HASH
+	KMEM_FREE(rbrp->ring_hash, sizeof(rxring_info_hash_t) * RING_HASH_BUCKETS * RING_HASH_SIZE);
+#endif
+
+#ifdef USE_BTREE
+	KMEM_FREE(rbrp->ring_info, sizeof(rxring_info_t));
+#endif
+	KMEM_FREE(rbrp, sizeof (rx_rbr_ring_t));
+	nep->rx_rbr_rings->rings[ring] = NULL;
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "<== nxge_uninit_rx_buf_ring"));
+}
+
+
+
+static int nxge_init_rx_buf_ring(p_nxge_t nep, uint16_t channel, int ring)
+{
+	p_rx_rbr_ring_t 	rbrp;
+	p_rx_msg_t 		rx_msg_ring;
+#ifdef USE_BTREE
+	rxring_info_t           *ring_info = NULL;
+#endif
+#ifdef USE_RING_HASH
+	int index, bucket;
+	p_rxring_info_hash_t rh;
+	size_t alloc_size;
+#endif
+	p_nxge_dma_pt_cfg_t	p_all_cfgp;
+	uint32_t		nxge_port_rbr_spare_size;
+
+	int			status = NXGE_OK;
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "==> nxge_init_rx_buf_ring"));
+
+	p_all_cfgp = (p_nxge_dma_pt_cfg_t)&nep->pt_config;
+	nxge_port_rbr_spare_size = nxge_rbr_spare_size;
+
+
+	rbrp = (p_rx_rbr_ring_t)
+			KMEM_ZALLOC(sizeof (rx_rbr_ring_t), GFP_KERNEL);
+	rbrp->nep = nep;
+	rbrp->rdc = channel;
+	rbrp->rbb_max = p_all_cfgp->rbr_size;
+	rbrp->ring_size = p_all_cfgp->rbr_size + nxge_port_rbr_spare_size;
+	rbrp->wrap_mask = rbrp->ring_size - 1;
+	MUTEX_INIT(&rbrp->lock, 0, 0, 0);
+
+	/* TODO: */
+	rbrp->pkt_buf_size0 = RBR_BUFSZ0_256B;
+	rbrp->pkt_buf_size0_bytes = RBR_BUFSZ0_256_BYTES;
+	rbrp->npi_pkt_buf_size0 = SIZE_256B;
+	rbrp->max_usage[RCR_PKTBUFSZ_0] = nep->rx_default_block_size / rbrp->pkt_buf_size0_bytes;
+	rbrp->bsize[RCR_PKTBUFSZ_0] = rbrp->pkt_buf_size0_bytes;
+
+
+	rbrp->pkt_buf_size1 = RBR_BUFSZ0_1K;
+	rbrp->pkt_buf_size1_bytes = RBR_BUFSZ1_1K_BYTES;
+	rbrp->npi_pkt_buf_size1 = SIZE_1KB;
+	rbrp->max_usage[RCR_PKTBUFSZ_1] = nep->rx_default_block_size / rbrp->pkt_buf_size1_bytes;
+
+	rbrp->block_size = nep->rx_default_block_size;
+	rbrp->bsize[RCR_PKTBUFSZ_1] = rbrp->pkt_buf_size1_bytes;
+
+
+#if defined(USE_DYNAMIC_ALLOC)
+	rbrp->pkt_buf_size2 = RBR_BUFSZ2_4K;
+	rbrp->pkt_buf_size2_bytes = RBR_BUFSZ2_4K_BYTES;
+	rbrp->npi_pkt_buf_size2 = SIZE_4KB;
+	rbrp->max_usage[RCR_PKTBUFSZ_2] = nep->rx_default_block_size / rbrp->pkt_buf_size2_bytes;
+	rbrp->bsize[RCR_PKTBUFSZ_2] = rbrp->pkt_buf_size2_bytes;
+
+#else
+	rbrp->pkt_buf_size2 = RBR_BUFSZ2_2K;
+	rbrp->pkt_buf_size2_bytes = RBR_BUFSZ2_2K_BYTES;
+	rbrp->npi_pkt_buf_size2 = SIZE_2KB;
+	rbrp->max_usage[RCR_PKTBUFSZ_2] = nep->rx_default_block_size / rbrp->pkt_buf_size2_bytes;
+	rbrp->bsize[RCR_PKTBUFSZ_2] = rbrp->pkt_buf_size2_bytes;
+
+#endif
+
+	rbrp->max_usage[RCR_SINGLE_BLOCK] = 1;
+	rbrp->bsize[RCR_SINGLE_BLOCK] = rbrp->block_size;
+
+	rx_msg_ring = (p_rx_msg_t)
+		KMEM_ZALLOC(rbrp->ring_size * sizeof (rx_msg_t), GFP_KERNEL);
+	if (rx_msg_ring == NULL) {
+		goto nxge_init_rx_buf_ring_fail1;
+	}
+	rbrp->rx_msg_ring = rx_msg_ring;
+#ifdef USE_BTREE
+	ring_info = (rxring_info_t *) KMEM_ZALLOC(sizeof(rxring_info_t),
+						  GFP_KERNEL);
+	if (ring_info == NULL) {
+		goto nxge_init_rx_buf_ring_fail1;
+	}
+#endif
+
+#ifdef USE_RING_HASH
+	alloc_size = RING_HASH_SIZE * RING_HASH_BUCKETS * sizeof (rxring_info_hash_t);
+	rbrp->ring_hash = (p_rxring_info_hash_t) KMEM_ZALLOC(alloc_size, GFP_KERNEL);
+	if (rbrp->ring_hash == NULL) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "ring %d alloc failed for 0x%x bytes", ring,
+						RING_HASH_SIZE * RING_HASH_BUCKETS * sizeof (rxring_info_hash_t)));
+		goto nxge_init_rx_buf_ring_fail1;
+	}
+	for (index =0; index < RING_HASH_SIZE; index++) {
+		rh = (p_rxring_info_hash_t) rbrp->ring_hash;
+		rh += RING_HASH_BUCKETS * index;
+		for(bucket = 0; bucket < RING_HASH_BUCKETS; bucket++) {
+			rh[bucket].key	= NXGE_RX_RING_HASH_NULL_KEY;
+			rh[bucket].index = -1;
+		}
+	}
+	rbrp->search_hint[0] = NO_HINT;
+	rbrp->search_hint[1] = NO_HINT;
+	rbrp->search_hint[2] = NO_HINT;
+	rbrp->search_hint[3] = NO_HINT;
+#endif
+#ifdef USE_BTREE
+	rbrp->ring_info = ring_info;
+#endif
+	rbrp->index = ring;
+	nep->rx_rbr_rings->rings[ring] = rbrp;
+
+	/*
+	 * Map in pages for the packet buffer pool.
+	 */
+	status = nxge_alloc_rx_buf_pool(nep, ring, nep->rx_default_block_size,
+					channel);
+
+	if (status == NXGE_ERROR) {
+	  goto nxge_init_rx_buf_ring_fail2;
+	}
+#ifdef USE_BTREE
+	status = nxge_rxbuf_index_info_init(nep, rbrp);
+
+	if (status == NXGE_ERROR) {
+	  nxge_free_rx_buf_pool(nep, ring);
+	  goto nxge_init_rx_buf_ring_fail2;
+	}
+#endif
+
+	goto nxge_init_rx_buf_ring_exit;
+
+nxge_init_rx_buf_ring_fail2:
+#ifdef USE_RING_HASH
+	if (rbrp->ring_hash)
+		KMEM_FREE(rbrp->ring_hash,
+				  RING_HASH_SIZE * RING_HASH_BUCKETS * sizeof (rxring_info_hash_t));
+#endif
+#ifdef USE_BTREE
+	KMEM_FREE(ring_info, sizeof(rxring_info_t));
+#endif
+
+nxge_init_rx_buf_ring_fail1:
+	if (rx_msg_ring)
+		KMEM_FREE(rx_msg_ring, rbrp->ring_size * sizeof (rx_msg_t));
+
+	MUTEX_DESTROY(&rbrp->lock);
+
+	if (rbrp)
+		KMEM_FREE(rbrp, sizeof (rx_rbr_ring_t));
+	nep->rx_rbr_rings->rings[ring] = NULL;
+
+	status = NXGE_ERROR;
+	NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "<== nxge_init_rx_buf_ring FAILED"
+			" status 0x%x", status));
+
+nxge_init_rx_buf_ring_exit:
+	NXGE_DEBUG_MSG((nep, RX_CTL, "<== nxge_init_rx_buf_ring status 0x%x",
+		       status));
+
+	return (status);
+}
+
+static void nxge_uninit_rx_cntl_ring(p_nxge_t nep, uint16_t dma_channel,
+				     int ring)
+{
+	p_rx_rcr_ring_t 	rcrp;
+	p_rx_mbox_t 		mboxp;
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "==> nxge_uninit_rx_cntl_ring"));
+
+	rcrp = nep->rx_rcr_rings->rings[ring];
+	if (rcrp != NULL) {
+		MUTEX_DESTROY(&rcrp->lock);
+		KMEM_FREE(rcrp, sizeof (rx_rcr_ring_t));
+	}
+
+	mboxp = nep->rx_mbox_areas_p->rxmbox_areas[ring];
+	if (mboxp != NULL) {
+		MUTEX_DESTROY(&mboxp->lock);
+		KMEM_FREE(mboxp, sizeof (rx_mbox_t));
+	}
+
+	nep->rx_mbox_areas_p->rxmbox_areas[ring] = NULL;
+	nep->rx_rcr_rings->rings[ring] = NULL;
+	nep->rx_rbr_rings->rings[ring]->rx_rcr_p = NULL;
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "<== nxge_uninit_rx_cntl_ring"));
+}
+
+static int nxge_init_rx_cntl_ring(p_nxge_t nep, uint16_t dma_channel,
+				  int ring)
+{
+	p_rx_rbr_ring_t 	rbrp;
+	p_rx_rcr_ring_t 	rcrp;
+	p_rx_mbox_t 		mboxp;
+	p_nxge_dma_common_t 	dma_cntl_p;
+	int			rb_ring_size, rc_ring_size;
+	p_rx_msg_t 		rx_msg_ring;
+	p_rbr_cfig_a_t		rcfga_p;
+	p_rbr_cfig_b_t		rcfgb_p;
+	p_rcrcfig_a_t		cfga_p;
+	p_rcrcfig_b_t		cfgb_p;
+	p_rxdma_cfig1_t		cfig1_p;
+	p_rxdma_cfig2_t		cfig2_p;
+	p_rbr_kick_t		kick_p;
+	uint32_t		dmaaddrp;
+	uint32_t		*rbr_vaddrp;
+	uint32_t		bkaddr;
+	int			i;
+#ifdef USE_RING_HASH
+	int index, bucket;
+	p_rxring_info_hash_t rh;
+#endif
+#ifdef USE_VA_INDEX
+	uint32_t *iptr;
+#endif
+
+	int			status = NXGE_OK;
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "==> nxge_init_rx_cntl_ring"));
+
+	dma_cntl_p = &(nep->rx_cntl_pool_p->dma_buf_pool_p[ring]);
+
+	/* Map in the receive block ring */
+	rbrp = nep->rx_rbr_rings->rings[ring];
+	rb_ring_size = sizeof (rx_desc_t) * rbrp->ring_size;
+	rbrp->rbr_desc = *dma_cntl_p;
+	rbrp->rbr_desc.alloc_len = rb_ring_size;
+	memset((void *)(rbrp->rbr_desc.cpu_addr), 0, rb_ring_size);
+
+	rcfga_p = &(rbrp->rbr_cfga);
+	rcfgb_p = &(rbrp->rbr_cfgb);
+	kick_p = &(rbrp->rbr_kick);
+	rcfga_p->value = 0;
+	rcfgb_p->value = 0;
+	kick_p->value = 0;
+/* 	dmaaddrp = (uint32_t) (rbrp->rbr_desc.dma_addr & 0xffffffff); */
+/* 	rcfga_p->bits.hdw.len = rbrp->ring_size; */
+/* 	rcfga_p->bits.ldw.staddr = (dmaaddrp >> 6) & 0x3ffc0; */
+/* 	rcfga_p->bits.ldw.staddr_base = (dmaaddrp >> 18); */
+/* 	dmaaddrp = (uint32_t) (rbrp->rbr_desc.dma_addr >> 32 & 0xffffffff); */
+/* 	rcfga_p->bits.hdw.staddr_base = (dmaaddrp & 0x3fff); */
+	rbrp->rbr_addr = rbrp->rbr_desc.dma_addr;
+	rcfga_p->value = (rbrp->rbr_addr &
+				(RBR_CFIG_A_STDADDR_MASK |
+				RBR_CFIG_A_STDADDR_BASE_MASK));
+	rcfga_p->value |= ((uint64_t)rbrp->ring_size <<
+				RBR_CFIG_A_LEN_SHIFT);
+
+	/* TODO: how to choose packet buffer sizes */
+	rcfgb_p->bits.ldw.bufsz0 = rbrp->pkt_buf_size0;
+
+	rcfgb_p->bits.ldw.vld0 = 1;
+	rcfgb_p->bits.ldw.bufsz1 = rbrp->pkt_buf_size1;
+
+	rcfgb_p->bits.ldw.vld1 = 1;
+	rcfgb_p->bits.ldw.bufsz2 = rbrp->pkt_buf_size2;
+	rcfgb_p->bits.ldw.vld2 = 1;
+	rcfgb_p->bits.ldw.bksize = nep->rx_bksize_code;
+
+	/*
+	 * For each buffer block (page), enter the address to the ring.
+	 */
+	rbr_vaddrp = (uint32_t *) rbrp->rbr_desc.cpu_addr;
+	rbrp->rbr_desc_vp = (uint32_t *) rbrp->rbr_desc.cpu_addr;
+	rx_msg_ring = rbrp->rx_msg_ring;
+	for (i = 0; i < rbrp->ring_size; i++) {
+		rx_msg_ring[i].nxgep = nep;
+		rx_msg_ring[i].rx_rbr_p = rbrp;
+		bkaddr = (uint32_t) (rx_msg_ring[i].dma_addr
+				     >> RBR_BKADDR_SHIFT);
+		rx_msg_ring[i].shifted_addr = bkaddr;
+#ifdef USE_VA_INDEX
+		iptr = (uint32_t *) (rx_msg_ring[i].cpu_addr + 60);
+		*iptr = i;
+#endif
+#ifdef USE_RING_HASH
+			index = (bkaddr) & RING_HASH_FUNC;
+			rh = (p_rxring_info_hash_t) rbrp->ring_hash;
+			rh += RING_HASH_BUCKETS * index;
+			for (bucket = 0; bucket < RING_HASH_BUCKETS; bucket++) {
+				if (rh[bucket].key == NXGE_RX_RING_HASH_NULL_KEY) {
+					rh[bucket].key = bkaddr;
+					rh[bucket].index = i;
+					break;
+				}
+			}
+			if (bucket == RING_HASH_BUCKETS)
+				NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+								"Ring %d dma_addr 0x%llx bkaddr %x index %x:"
+								" base 0x%llx index_base 0x%llx bucket too big",
+								ring, rx_msg_ring[i].dma_addr, bkaddr,
+								index, rbrp->ring_hash, rh));
+#endif
+			*rbr_vaddrp++ = bkaddr;
+	}
+
+	kick_p->bits.ldw.bkadd = rbrp->ring_size;
+	rbrp->rbr_wr_index = rbrp->ring_size - 1; /***** RIGHT? **********/
+	rbrp->tmp_count = 0;
+
+	/*
+	 * TODO: for now hard coded to set page valid and no mask
+	 */
+	rbrp->page_valid.value = 0;
+	rbrp->page_mask_1.value = rbrp->page_mask_2.value = 0;
+	rbrp->page_value_1.value = rbrp->page_value_2.value = 0;
+	rbrp->page_reloc_1.value = rbrp->page_reloc_2.value = 0;
+	rbrp->page_hdl.value = 0;
+
+	rbrp->page_valid.bits.ldw.page0 = 1;
+	rbrp->page_valid.bits.ldw.page1 = 1;
+
+
+	/* Map in the receive completion ring */
+	rcrp = (p_rx_rcr_ring_t) KMEM_ZALLOC(sizeof (rx_rcr_ring_t),
+					     GFP_KERNEL);
+	rcrp->nep = nep;
+	rcrp->rdc = dma_channel;
+	MUTEX_INIT(&rcrp->lock, 0, 0, 0);
+
+	rcrp->ring_size = nxge_rcr_size;
+	rcrp->wrap_mask = nxge_rcr_size - 1;
+
+	rcrp->rcr_desc = *dma_cntl_p;
+	rcrp->rcr_desc.cpu_addr += rb_ring_size;
+	rcrp->rcr_desc.dma_addr += rb_ring_size;
+	rc_ring_size = sizeof (rcr_entry_t) * rcrp->ring_size;
+	rcrp->rcr_desc.alloc_len = rc_ring_size;
+	memset((void *)(rcrp->rcr_desc.cpu_addr), 0, rc_ring_size);
+
+	NXGE_DEBUG_MSG((nep, RX_CTL,
+			"nxge_init_rx_cntl_ring: RCR "
+			"channel %d cpu_addr $%p damaddr $%p ",
+			dma_channel, rcrp->rcr_desc.cpu_addr,
+			rcrp->rcr_desc.dma_addr));
+
+	rcrp->out = 0;
+	rcrp->rcr_head_pp = rcrp->rcr_tail_pp =
+		(uint64_t)(rcrp->rcr_desc.dma_addr);
+
+
+	/* TODO */
+	rcrp->full_hdr_flag = B_FALSE;
+	rcrp->sw_priv_hdr_len = SW_OFFSET_64;
+	switch (rcrp->sw_priv_hdr_len) {
+		case SW_OFFSET_NO_OFFSET:
+			rcrp->sw_offset_bytes = 0;
+			break;
+		case SW_OFFSET_64:
+			rcrp->sw_offset_bytes = 64;
+			break;
+		case SW_OFFSET_128:
+			rcrp->sw_offset_bytes = 128;
+			break;
+		default:
+			rcrp->sw_priv_hdr_len = SW_OFFSET_NO_OFFSET;
+			rcrp->sw_offset_bytes = 0;
+			break;
+	}
+	cfga_p = &(rcrp->rcr_cfga);
+	cfgb_p = &(rcrp->rcr_cfgb);
+	cfga_p->value = 0;
+	cfgb_p->value = 0;
+	rcrp->rcr_addr = rcrp->rcr_desc.dma_addr;
+	rcrp->cpu_addr = rcrp->rcr_desc.cpu_addr;
+	/* TODO: verify that rcr is 64 byte aligned */
+	/* TODO: check the endianess */
+
+	cfga_p->value = (rcrp->rcr_addr &
+			    (RCRCFIG_A_STADDR_MASK |
+			    RCRCFIG_A_STADDR_BASE_MASK));
+	cfga_p->value |= ((uint64_t)rcrp->ring_size <<
+			  RCRCFIG_A_LEN_SHIF);
+
+
+#ifdef USE_NAPI
+	cfgb_p->bits.ldw.pthres = RXDMA_RCR_PTHRES_DEFAULT;
+	cfgb_p->bits.ldw.timeout = RXDMA_RCR_TO_DEFAULT;
+	rcrp->intr_thresh =  nep->intr_thresh;
+	rcrp->max_receive_pkts = nep->max_receive_pkts;
+#else
+
+#ifdef RX_INTR_BLANKING_LDG
+	rcrp->intr_timeout = RXDMA_RCR_TO_DEFAULT | RCRCFIG_B_TIMEOUT;
+#else
+	rcrp->intr_timeout = nep->intr_timeout;
+#endif
+
+	rcrp->intr_thresh =  nep->intr_thresh;
+	rcrp->max_receive_pkts = nep->max_receive_pkts;
+	cfgb_p->bits.ldw.pthres = rcrp->intr_thresh;
+	cfgb_p->bits.ldw.timeout = rcrp->intr_timeout;
+#endif
+
+	cfgb_p->bits.ldw.entout = 1;
+
+	/* Map in the mailbox */
+	mboxp = (p_rx_mbox_t) KMEM_ZALLOC(sizeof (rx_mbox_t), GFP_KERNEL);
+	mboxp->nep = nep;
+	MUTEX_INIT(&mboxp->lock, 0, 0, 0);
+
+	mboxp->rx_mbox = *dma_cntl_p;
+	mboxp->rx_mbox.cpu_addr += (rb_ring_size + rc_ring_size);
+	mboxp->rx_mbox.dma_addr += (rb_ring_size + rc_ring_size);
+	mboxp->rx_mbox.alloc_len = sizeof (rxdma_mailbox_t);
+	memset((void *)(mboxp->rx_mbox.cpu_addr), 0, sizeof (rxdma_mailbox_t));
+
+	cfig1_p = (p_rxdma_cfig1_t) &mboxp->rx_cfg1;
+	cfig2_p = (p_rxdma_cfig2_t) &mboxp->rx_cfg2;
+	cfig1_p->value = cfig2_p->value = 0;
+
+	mboxp->mbox_addr = mboxp->rx_mbox.dma_addr;
+	dmaaddrp = (uint32_t) (mboxp->rx_mbox.dma_addr >> 32 & 0xfff);
+	cfig1_p->bits.ldw.mbaddr_h = dmaaddrp;
+
+	dmaaddrp = (uint32_t) (mboxp->rx_mbox.dma_addr & 0xffffffff);
+	dmaaddrp = (uint32_t) (mboxp->rx_mbox.dma_addr &
+			       RXDMA_CFIG2_MBADDR_L_MASK);
+	cfig2_p->bits.ldw.mbaddr = (dmaaddrp >> RXDMA_CFIG2_MBADDR_L_SHIFT);
+
+	/* TODO: offset and full header */
+	cfig2_p->bits.ldw.full_hdr = rcrp->full_hdr_flag;
+	cfig2_p->bits.ldw.offset = rcrp->sw_priv_hdr_len;
+
+	NXGE_DEBUG_MSG((nep, RX_CTL,
+			"nxge_init_rx_cntl_ring: Mailbox "
+			"channel %d damaddrp %x "
+			"cfg1 0x%016llx cfig2 0x%016llx",
+			dma_channel, dmaaddrp,
+			cfig1_p->value, cfig2_p->value));
+
+	rcrp->rdc_stats = &nep->statsp->rdc_stats[ring];
+	rcrp->rdc_stats->channel_num = dma_channel;
+	nep->rx_rcr_rings->rings[ring] = rcrp;
+	nep->rx_mbox_areas_p->rxmbox_areas[ring] = mboxp;
+	rbrp->rx_rcr_p = rcrp;
+	rcrp->rx_rbr_p = rbrp;
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "<== nxge_init_rx_cntl_ring"));
+
+	return (status);
+}
+
+static void nxge_unmap_rxdma(p_nxge_t nep)
+{
+	int			i, ndmas;
+	uint16_t		channel;
+	p_rx_rbr_rings_t 	rx_rbr_rings;
+	p_rx_rbr_ring_t		*rbr_rings;
+	p_rx_rcr_rings_t 	rx_rcr_rings;
+	p_rx_rcr_ring_t		*rcr_rings;
+	p_rx_mbox_areas_t 	rx_mbox_areas_p;
+	p_rx_mbox_t		*rx_mbox_p;
+	p_nxge_dma_pool_t	dma_cntl_poolp;
+	p_nxge_dma_common_t	dma_cntl_p;
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "==> nxge_unmap_rxdma"));
+
+	ndmas = nep->max_rdcs;
+	if (!ndmas) {
+		NXGE_DEBUG_MSG((nep, RX_CTL, "nxge_unmap_rxdma: "
+				"No DMAs allocated"));
+		return;
+	}
+
+	dma_cntl_poolp = nep->rx_cntl_pool_p;
+	if (!dma_cntl_poolp->buf_allocated) {
+		NXGE_DEBUG_MSG((nep, RX_CTL, "nxge_unmap_rxdma: "
+				"No cntl buffers allocated"));
+		return;
+	}
+
+	dma_cntl_p = dma_cntl_poolp->dma_buf_pool_p;
+
+	for (i = 0; i < ndmas; i++) {
+
+		channel = dma_cntl_p[i].dma_channel;
+		nxge_uninit_rx_cntl_ring(nep, channel, i);
+	}
+
+	for (i = 0; i < ndmas; i++) {
+
+		channel = dma_cntl_p[i].dma_channel;
+		nxge_uninit_rx_buf_ring(nep, channel, i);
+	}
+
+	rx_rbr_rings = nep->rx_rbr_rings;
+	rbr_rings = rx_rbr_rings->rings;
+	rx_rcr_rings = nep->rx_rcr_rings;
+	rcr_rings = rx_rcr_rings->rings;
+
+	rx_mbox_areas_p = nep->rx_mbox_areas_p;
+	rx_mbox_p = rx_mbox_areas_p->rxmbox_areas;
+
+	KMEM_FREE(rx_mbox_p, sizeof (p_rx_mbox_t) * ndmas);
+	KMEM_FREE(rx_mbox_areas_p, sizeof (rx_mbox_areas_t));
+	KMEM_FREE(rcr_rings, sizeof (p_rx_rcr_ring_t) * ndmas);
+	KMEM_FREE(rx_rcr_rings, sizeof (rx_rcr_rings_t));
+	KMEM_FREE(rbr_rings, sizeof (p_rx_rbr_ring_t) * ndmas);
+	KMEM_FREE(rx_rbr_rings, sizeof (rx_rbr_rings_t));
+
+	nep->rx_rbr_rings = NULL;
+	nep->rx_rcr_rings = NULL;
+	nep->rx_mbox_areas_p = NULL;
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "<== nxge_unmap_rxdma"));
+
+	return;
+}
+
+static int nxge_map_rxdma(p_nxge_t nep)
+{
+
+	int			i, ndmas;
+	uint16_t		channel = 0;
+	p_rx_rbr_rings_t 	rx_rbr_rings;
+	p_rx_rbr_ring_t		*rbr_rings;
+	p_rx_rcr_rings_t 	rx_rcr_rings;
+	p_rx_rcr_ring_t		*rcr_rings;
+	p_rx_mbox_areas_t 	rx_mbox_areas_p;
+	p_rx_mbox_t		*rx_mbox_p;
+	p_nxge_dma_pool_t	dma_cntl_poolp;
+	p_nxge_dma_common_t	dma_cntl_p;
+	int			status = NXGE_OK;
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "==> nxge_map_rxdma"));
+
+	ndmas = nep->max_rdcs;
+	if (!ndmas) {
+		NXGE_DEBUG_MSG((nep, RX_CTL, "nxge_map_rxdma: "
+				"No DMAs allocated"));
+		return (NXGE_ERROR);
+	}
+
+	dma_cntl_poolp = nep->rx_cntl_pool_p;
+	if (!dma_cntl_poolp->buf_allocated) {
+		NXGE_DEBUG_MSG((nep, RX_CTL, "nxge_map_rxdma: "
+				"No cntl buffers allocated"));
+		return (NXGE_ERROR);
+	}
+
+	dma_cntl_p = dma_cntl_poolp->dma_buf_pool_p;
+
+	rx_rbr_rings = (p_rx_rbr_rings_t)
+		KMEM_ZALLOC(sizeof (rx_rbr_rings_t), GFP_KERNEL);
+	if (rx_rbr_rings == NULL)
+		goto fail1;
+
+	rbr_rings = (p_rx_rbr_ring_t *)
+		KMEM_ZALLOC(sizeof (p_rx_rbr_ring_t) * ndmas, GFP_KERNEL);
+	if (rbr_rings == NULL)
+		goto fail2;
+
+	rx_rcr_rings = (p_rx_rcr_rings_t)
+		KMEM_ZALLOC(sizeof (rx_rcr_rings_t), GFP_KERNEL);
+	if (rx_rcr_rings == NULL)
+		goto fail3;
+
+	rcr_rings = (p_rx_rcr_ring_t *)
+		KMEM_ZALLOC(sizeof (p_rx_rcr_ring_t) * ndmas, GFP_KERNEL);
+	if (rcr_rings == NULL)
+		goto fail4;
+
+	rx_mbox_areas_p = (p_rx_mbox_areas_t)
+		KMEM_ZALLOC(sizeof (rx_mbox_areas_t), GFP_KERNEL);
+	if (rx_mbox_areas_p == NULL)
+		goto fail5;
+
+	rx_mbox_p = (p_rx_mbox_t *)
+		KMEM_ZALLOC(sizeof (p_rx_mbox_t) * ndmas, GFP_KERNEL);
+	if (rx_mbox_p == NULL)
+		goto fail6;
+
+	rx_rbr_rings->rings = rbr_rings;
+	nep->rx_rbr_rings = rx_rbr_rings;
+
+	rx_rcr_rings->rings = rcr_rings;
+	nep->rx_rcr_rings = rx_rcr_rings;
+
+	rx_mbox_areas_p->rxmbox_areas = rx_mbox_p;
+	nep->rx_mbox_areas_p = rx_mbox_areas_p;
+
+	/*
+	 * Initialize and enable each receive DMA channel.
+	 */
+	for (i = 0; i < ndmas; i++) {
+		/*
+		 * Set up and prepare buffer blocks, descriptors
+		 * and mailbox.
+		 */
+		channel = dma_cntl_p[i].dma_channel;
+
+		/*
+		 * Receive buffer blocks
+		 */
+		status = nxge_init_rx_buf_ring(nep, channel, i);
+		if (status != NXGE_OK) {
+			goto nxge_map_rxdma_fail1;
+		}
+	}
+
+	for (i = 0; i < ndmas; i++) {
+
+		channel = dma_cntl_p[i].dma_channel;
+		/*
+		 * Receive block ring, completion ring and mailbox.
+		 */
+		status = nxge_init_rx_cntl_ring(nep, channel, i);
+		if (status != NXGE_OK) {
+			goto nxge_map_rxdma_fail2;
+		}
+	}
+
+	goto nxge_map_rxdma_exit;
+
+  nxge_map_rxdma_fail2:
+	i--;
+	for (; i >= 0; i--) {
+		channel = dma_cntl_p[i].dma_channel;
+		nxge_uninit_rx_cntl_ring(nep, channel, i);
+	}
+	i = ndmas;
+
+  nxge_map_rxdma_fail1:
+	i--;
+	for (; i >= 0; i--) {
+		channel = dma_cntl_p[i].dma_channel;
+		nxge_uninit_rx_buf_ring(nep, channel, i);
+	}
+
+	KMEM_FREE(rx_mbox_p, sizeof (p_rx_mbox_t) * ndmas);
+  fail6:
+	KMEM_FREE(rx_mbox_areas_p, sizeof (rx_mbox_areas_t));
+  fail5:
+	KMEM_FREE(rcr_rings, sizeof (p_rx_rcr_ring_t) * ndmas);
+  fail4:
+	KMEM_FREE(rx_rcr_rings, sizeof (rx_rcr_rings_t));
+  fail3:
+	KMEM_FREE(rbr_rings, sizeof (p_rx_rbr_ring_t) * ndmas);
+  fail2:
+	KMEM_FREE(rx_rbr_rings, sizeof (rx_rbr_rings_t));
+  fail1:
+	nep->rx_rbr_rings = NULL;
+	nep->rx_rcr_rings = NULL;
+	nep->rx_mbox_areas_p = NULL;
+
+  nxge_map_rxdma_exit:
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "<== nxge_map_rxdma: "
+		"(status 0x%x channel %d)", status, channel));
+
+	return (status);
+}
+
+static int
+nxge_rxdma_hw_mode(p_nxge_t nep, boolean_t enable)
+{
+	int			i, ndmas;
+	int			status = NXGE_OK;
+	uint16_t		channel;
+	p_rx_rbr_rings_t 	rx_rbr_rings;
+	p_rx_rbr_ring_t		*rbr_rings;
+	npi_handle_t		handle;
+
+	NXGE_DEBUG_MSG((nep, RX_CTL,
+		"==> nxge_rxdma_hw_mode: mode %d", enable));
+
+	if (!nep->drv_state) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+			"<== nxge_rxdma_mode: not initialized"));
+		return (NXGE_ERROR);
+	}
+
+	rx_rbr_rings = nep->rx_rbr_rings;
+	if (rx_rbr_rings == NULL) {
+		NXGE_DEBUG_MSG((nep, RX_CTL,
+			"<== nxge_rxdma_mode: NULL ring pointer"));
+		return (NXGE_ERROR);
+	}
+	ndmas = nep->max_rdcs;
+	if (!ndmas) {
+		NXGE_DEBUG_MSG((nep, RX_CTL,
+			"<== nxge_rxdma_mode: no channel"));
+		return (NXGE_ERROR);
+	}
+
+	NXGE_DEBUG_MSG((nep, RX_CTL,
+		"==> nxge_rxdma_mode (ndmas %d)", ndmas));
+
+	rbr_rings = rx_rbr_rings->rings;
+
+	handle = nep->npi_handle;
+	for (i = 0; i < ndmas; i++) {
+		if (nep->rx_cntl_pool_p->dma_buf_pool_p == NULL ||
+		    nep->rx_cntl_pool_p->buf_allocated == B_FALSE)
+			continue;
+		channel = nep->rx_cntl_pool_p->dma_buf_pool_p[i].dma_channel;
+		if (enable) {
+			NXGE_DEBUG_MSG((nep, RX_CTL,
+				"==> nxge_rxdma_hw_mode: channel %d (enable)",
+				channel));
+			status = npi_rxdma_cfg_rdc_enable(handle, channel);
+		} else {
+			NXGE_DEBUG_MSG((nep, RX_CTL,
+				"==> nxge_rxdma_hw_mode: channel %d (disable)",
+				channel));
+			status = npi_rxdma_cfg_rdc_disable(handle, channel);
+		}
+	}
+
+	status = ((status == NPI_SUCCESS) ? NXGE_OK : NXGE_ERROR);
+
+	NXGE_DEBUG_MSG((nep, RX_CTL,
+		"<== nxge_rxdma_hw_mode: status 0x%x", status));
+
+	return (status);
+}
+
+static nxge_status_t
+nxge_init_txdma_channel_event_mask(p_nxge_t nxgep, uint16_t channel,
+				   p_tx_dma_ent_msk_t mask_p)
+{
+	npi_handle_t		handle;
+	npi_status_t		rs = NPI_SUCCESS;
+	nxge_status_t		status = NXGE_OK;
+
+	NXGE_DEBUG_MSG((nxgep, TX_CTL,
+		"==> nxge_init_txdma_channel_event_mask"));
+
+	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	rs = npi_txdma_event_mask(handle, OP_SET, channel, mask_p);
+	if (rs != NPI_SUCCESS) {
+		status = NXGE_ERROR | rs;
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"<== nxge_init_txdma_channel_event_mask"
+				       " status 0x%x", status));
+	}
+
+	return (status);
+}
+
+static int
+nxge_init_rxdma_channel_cntl_stat(p_nxge_t nxgep, uint16_t channel,
+    p_rx_dma_ctl_stat_t cs_p)
+{
+	npi_handle_t		handle;
+	int			status = NXGE_OK;
+
+	NXGE_DEBUG_MSG((nxgep, RX_CTL,
+		"==> nxge_init_rxdma_channel_cntl_stat"));
+
+	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	status = npi_rxdma_control_status(handle, OP_SET, channel,
+			cs_p);
+
+	if (status != NPI_SUCCESS) {
+		status = NXGE_ERROR;
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"<== nxge_init_rxdma_channel_cntl_stat "
+				       "status 0x%x", status));
+	}
+
+	return (status);
+}
+
+static int
+nxge_rxdma_start_channel(p_nxge_t nep, uint16_t channel,
+			 p_rx_rbr_ring_t rbr_p, p_rx_rcr_ring_t rcr_p,
+			 p_rx_mbox_t mbox_p)
+
+{
+	npi_handle_t		handle;
+	rx_dma_ctl_stat_t	cs;
+	rx_dma_ent_msk_t	ent_mask;
+	int			status;
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "==> nxge_rxdma_start_channel"));
+
+	handle = nep->npi_handle;
+
+	/* Reset RXDMA channel */
+	status = npi_rxdma_cfg_rdc_reset(handle, channel);
+	if (status != NPI_SUCCESS) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+			"==> nxge_rxdma_start_channel: "
+			"reset rxdma failed (0x%08x channel %d)",
+			status, channel));
+		return (NXGE_ERROR);
+	}
+
+	NXGE_DEBUG_MSG((nep, RX_CTL,
+		"==> nxge_rxdma_start_channel: reset done"));
+
+	/*
+	 * Initialize the RXDMA channel specific FZC control
+	 * configurations. These FZC registers are pertaining
+	 * to each RX channel (logical pages).
+	 */
+	status = nxge_init_fzc_rxdma_channel(nep,
+			channel, rbr_p, rcr_p, mbox_p);
+	if (status != NXGE_OK) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+			"==> nxge_rxdma_start_channel: "
+			"init fzc rxdma failed (0x%08x channel %d)",
+			status, channel));
+		return (NXGE_ERROR);
+	}
+
+	NXGE_DEBUG_MSG((nep, RX_CTL,
+		"==> nxge_rxdma_start_channel: fzc done"));
+
+	/*
+	 * Zero out the shadow  and prefetch ram.
+	 */
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "==> nxge_rxdma_start_channel: "
+		"ram done"));
+
+	/* Set up the interrupt event masks. */
+	ent_mask.value = ~RX_DMA_ENT_MSK_ALL;;
+	ent_mask.value |= RX_DMA_ENT_MSK_RBREMPTY_MASK;
+	status = npi_rxdma_event_mask(handle, OP_SET, channel,
+			&ent_mask);
+	if (status != NPI_SUCCESS) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+				"==> nxge_rxdma_start_channel: "
+				"init rxdma event masks failed "
+				"(0x%08x channel %d)",
+				status, channel));
+		return (NXGE_ERROR);
+	}
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "==> nxge_rxdma_start_channel: "
+		"event done"));
+
+	/* Initialize the receive DMA control and status register */
+	cs.value = 0;
+	cs.bits.hdw.mex = 1;
+	cs.bits.hdw.rcrthres = 1;
+	cs.bits.hdw.rcrto = 1;
+	cs.bits.hdw.rbr_empty = 1;
+	status = nxge_init_rxdma_channel_cntl_stat(nep, channel, &cs);
+	NXGE_DEBUG_MSG((nep, RX_CTL, "==> nxge_rxdma_start_channel: "
+		"channel %d rx_dma_cntl_stat 0x%0016llx", channel, cs.value));
+	if (status != NXGE_OK) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+			"==> nxge_rxdma_start_channel: "
+			"init rxdma control register failed (0x%08x channel %d",
+			status, channel));
+		return (NXGE_ERROR);
+	}
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "==> nxge_rxdma_start_channel: "
+		"control done - channel %d cs 0x%016llx", channel, cs.value));
+
+	/*
+	 * Load RXDMA descriptors, buffers, mailbox,
+	 * initialise the receive DMA channels and
+	 * enable each DMA channel.
+	 */
+	status = nxge_enable_rxdma_channel(nep,
+					   channel, rbr_p, rcr_p, mbox_p);
+
+	if (status != NXGE_OK) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+				"==> nxge_rxdma_start_channel: "
+				"init enable rxdma failed (0x%08x channel %d)",
+				status, channel));
+		return (NXGE_ERROR);
+	}
+
+	ent_mask.value = ~RX_DMA_ENT_MSK_ALL;
+	ent_mask.value |= (RX_DMA_ENT_MSK_WRED_DROP_MASK |
+					   RX_DMA_ENT_MSK_PTDROP_PKT_MASK);
+
+	status = npi_rxdma_event_mask(handle, OP_SET, channel,
+			&ent_mask);
+	if (status != NXGE_OK) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+				"==> nxge_rxdma_start_channel: "
+				"init rxdma event masks failed "
+				"(0x%08x channel %d)",
+				status, channel));
+		return (NXGE_ERROR);
+	}
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "==> nxge_rxdma_start_channel: "
+		"control done - channel %d cs 0x%016llx", channel, cs.value));
+
+	NXGE_DEBUG_MSG((nep, RX_CTL,
+		"==> nxge_rxdma_start_channel: enable done"));
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "<== nxge_rxdma_start_channel"));
+
+	return (NXGE_OK);
+}
+
+static int
+nxge_rxdma_stop_channel(p_nxge_t nep, uint16_t channel)
+{
+	npi_handle_t		handle;
+	rx_dma_ctl_stat_t	cs;
+	rx_dma_ent_msk_t	ent_mask;
+	int			status;
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "==> nxge_rxdma_stop_channel"));
+
+	handle = nep->npi_handle;
+
+	/* Reset RXDMA channel */
+	status = npi_rxdma_cfg_rdc_reset(handle, channel);
+	if (status != NPI_SUCCESS) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+				"==> nxge_rxdma_stop_channel: "
+				"reset rxdma failed (0x%08x channel %d)",
+				status, channel));
+		return (NXGE_ERROR);
+	}
+
+	NXGE_DEBUG_MSG((nep, RX_CTL,
+		"==> nxge_rxdma_stop_channel: reset done"));
+
+	/* Set up the interrupt event masks. */
+	ent_mask.value = RX_DMA_ENT_MSK_ALL;
+	status = npi_rxdma_event_mask(handle, OP_SET, channel,
+			&ent_mask);
+	if (status != NPI_SUCCESS) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+				"==> nxge_rxdma_stop_channel: "
+				"set rxdma event masks failed "
+				"(0x%08x channel %d)",
+				status, channel));
+		return (NXGE_ERROR);
+	}
+
+	NXGE_DEBUG_MSG((nep, RX_CTL,
+		"==> nxge_rxdma_stop_channel: event done"));
+
+	/* Initialize the receive DMA control and status register */
+	cs.value = 0;
+	status = npi_rxdma_control_status(handle, OP_SET, channel, &cs);
+	NXGE_DEBUG_MSG((nep, RX_CTL, "==> nxge_rxdma_stop_channel: control "
+		" to default (all 0s) 0x%08x", cs.value));
+	if (status != NPI_SUCCESS) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+				"==> nxge_rxdma_stop_channel: "
+				"init rxdma control register failed "
+				"(0x%08x channel %d",
+				status, channel));
+		return (NXGE_ERROR);
+	}
+
+	NXGE_DEBUG_MSG((nep, RX_CTL,
+		"==> nxge_rxdma_stop_channel: control done"));
+
+	/* disable dma channel */
+	status = nxge_disable_rxdma_channel(nep, channel);
+
+	if (status != NXGE_OK) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+				"==> nxge_rxdma_stop_channel: "
+				"init enable rxdma failed (0x%08x channel %d)",
+				status, channel));
+		return (NXGE_ERROR);
+	}
+
+	NXGE_DEBUG_MSG((nep, RX_CTL,
+			"==> nxge_rxdma_stop_channel: disable done"));
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "<== nxge_rxdma_stop_channel"));
+
+	return (NXGE_OK);
+}
+
+static int
+nxge_rxdma_hw_start_common(p_nxge_t nep)
+{
+	int			status = NXGE_OK;
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "==> nxge_rxdma_hw_start_common"));
+
+	/*
+	 * Load the sharable parameters by writing to the
+	 * function zero control registers. These FZC registers
+	 * should be initialized only once for the entire chip.
+	 */
+	(void) nxge_init_fzc_rx_common(nep);
+
+	/*
+	 * Initialize the RXDMA port specific FZC control configurations.
+	 * These FZC registers are pertaining to each port.
+	 */
+	(void) nxge_init_fzc_rxdma_port(nep);
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "<== nxge_rxdma_hw_start_common"));
+
+	return (status);
+}
+
+/*ARGSUSED*/
+static void
+nxge_rxdma_hw_stop_common(p_nxge_t nep)
+{
+	NXGE_DEBUG_MSG((nep, RX_CTL, "==> nxge_rxdma_hw_stop_common"));
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "<== nxge_rxdma_hw_stop_common"));
+}
+
+static int
+nxge_rxdma_hw_start(p_nxge_t nep)
+{
+	int			i, ndmas;
+	uint16_t		channel;
+	p_rx_rbr_rings_t 	rx_rbr_rings;
+	p_rx_rbr_ring_t		*rbr_rings;
+	p_rx_rcr_rings_t 	rx_rcr_rings;
+	p_rx_rcr_ring_t		*rcr_rings;
+	p_rx_mbox_areas_t 	rx_mbox_areas_p;
+	p_rx_mbox_t		*rx_mbox_p = NULL;
+	p_nxge_dma_common_t	dma_cntl_p;
+	int			status = NXGE_OK;
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "==> nxge_rxdma_hw_start"));
+		/* interrupt related parameters */
+
+	rx_rbr_rings = nep->rx_rbr_rings;
+	rx_rcr_rings = nep->rx_rcr_rings;
+	if (rx_rbr_rings == NULL || rx_rcr_rings == NULL) {
+		NXGE_DEBUG_MSG((nep, RX_CTL,
+			"<== nxge_rxdma_hw_start: NULL ring pointers"));
+		return (NXGE_ERROR);
+	}
+	ndmas = nep->max_rdcs;
+	if (ndmas == 0) {
+		NXGE_DEBUG_MSG((nep, RX_CTL,
+			"<== nxge_rxdma_hw_start: no dma channel allocated"));
+		return (NXGE_ERROR);
+	}
+
+	NXGE_DEBUG_MSG((nep, RX_CTL,
+		"==> nxge_rxdma_hw_start (ndmas %d)", ndmas));
+
+	rbr_rings = rx_rbr_rings->rings;
+	rcr_rings = rx_rcr_rings->rings;
+	rx_mbox_areas_p = nep->rx_mbox_areas_p;
+	if (rx_mbox_areas_p) {
+		rx_mbox_p = rx_mbox_areas_p->rxmbox_areas;
+	}
+
+	dma_cntl_p = nep->rx_cntl_pool_p->dma_buf_pool_p;
+
+	for (i = 0; i < ndmas; i++) {
+		channel = dma_cntl_p[i].dma_channel;
+		NXGE_DEBUG_MSG((nep, RX_CTL,
+			"==> nxge_rxdma_hw_start (ndmas %d) channel %d",
+				ndmas, channel));
+		status = nxge_rxdma_start_channel(nep, channel,
+				(p_rx_rbr_ring_t)(rbr_rings[i]),
+				(p_rx_rcr_ring_t)(rcr_rings[i]),
+				(p_rx_mbox_t)(rx_mbox_p[i]));
+		if (status != NXGE_OK) {
+			goto nxge_rxdma_hw_start_fail1;
+		}
+	}
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "==> nxge_rxdma_hw_start: "
+		"rx_rbr_rings $%p rings $%p",
+		rx_rbr_rings, rx_rcr_rings));
+
+	goto nxge_rxdma_hw_start_exit;
+
+nxge_rxdma_hw_start_fail1:
+	NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+		"==> nxge_rxdma_hw_start: disable "
+		"(status 0x%x channel %d i %d)", status, channel, i));
+	i--;
+	for (; i >= 0; i--) {
+		channel = dma_cntl_p[i].dma_channel;
+		nxge_rxdma_stop_channel(nep, channel);
+	}
+
+nxge_rxdma_hw_start_exit:
+	NXGE_DEBUG_MSG((nep, RX_CTL,
+		"==> nxge_rxdma_hw_start: (status 0x%x)", status));
+
+	return (status);
+}
+
+static void
+nxge_rxdma_hw_stop(p_nxge_t nep)
+{
+	int			i, ndmas;
+	uint16_t		channel;
+	p_rx_rbr_rings_t 	rx_rbr_rings;
+	p_rx_rbr_ring_t		*rbr_rings;
+	p_rx_rcr_rings_t 	rx_rcr_rings;
+	p_nxge_dma_common_t	dma_cntl_p;
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "==> nxge_rxdma_hw_stop"));
+
+	rx_rbr_rings = nep->rx_rbr_rings;
+	rx_rcr_rings = nep->rx_rcr_rings;
+	if (rx_rbr_rings == NULL || rx_rcr_rings == NULL) {
+		NXGE_DEBUG_MSG((nep, RX_CTL,
+			"<== nxge_rxdma_hw_stop: NULL ring pointers"));
+		return;
+	}
+	ndmas = nep->max_rdcs;
+	if (!ndmas) {
+		NXGE_DEBUG_MSG((nep, RX_CTL,
+			"<== nxge_rxdma_hw_stop: no dma channel allocated"));
+		return;
+	}
+
+	NXGE_DEBUG_MSG((nep, RX_CTL,
+		"==> nxge_rxdma_hw_stop (ndmas %d)", ndmas));
+
+	rbr_rings = rx_rbr_rings->rings;
+	dma_cntl_p = nep->rx_cntl_pool_p->dma_buf_pool_p;
+
+	for (i = 0; i < ndmas; i++) {
+		channel = dma_cntl_p[i].dma_channel;
+		NXGE_DEBUG_MSG((nep, RX_CTL,
+			"==> nxge_rxdma_hw_stop (ndmas %d) channel %d",
+				ndmas, channel));
+		nxge_rxdma_stop_channel(nep, channel);
+	}
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "==> nxge_rxdma_hw_stop: "
+		"rx_rbr_rings $%p rings $%p",
+		rx_rbr_rings, rx_rcr_rings));
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "<== nxge_rxdma_hw_stop"));
+}
+
+
+
+
+static void nxge_uninit_rxdma(p_nxge_t nep)
+{
+	NXGE_DEBUG_MSG((nep, RX_CTL, "==> nxge_uninit_rxdma"));
+
+	nxge_rxdma_hw_stop(nep);
+	nxge_rxdma_hw_stop_common(nep);
+	nxge_unmap_rxdma(nep);
+
+	NXGE_DEBUG_MSG((nep, RX_CTL,
+		"<== nxge_uninit_rxdma"));
+}
+
+static int nxge_init_rxdma(p_nxge_t nep)
+{
+	int			status = NXGE_OK;
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "==> nxge_init_rxdma"));
+
+	status = nxge_map_rxdma(nep);
+	if (status) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+			"<== nxge_init_rxdma: status 0x%x", status));
+		return (NXGE_ERROR);
+	}
+
+	status = nxge_rxdma_hw_start_common(nep);
+	if (status) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+				"<== nxge_init_rxdma_channels: status 0x%x",
+				status));
+		nxge_unmap_rxdma(nep);
+	}
+
+	status = nxge_rxdma_hw_start(nep);
+	if (status) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+				"<== nxge_init_rxdma_channels: status 0x%x",
+				status));
+		nxge_unmap_rxdma(nep);
+	}
+
+	NXGE_DEBUG_MSG((nep, RX_CTL,
+		"<== nxge_init_rxdma_channels: status 0x%x", status));
+
+	return (status);
+}
+
+
+
+static int nxge_free_rx_cntl_pool(p_nxge_t nep)
+{
+	int			status = NXGE_OK;
+	int			i, ndmas;
+	struct pci_dev		*pdev;
+	p_nxge_dma_pool_t	dma_cntl_poolp;
+	p_nxge_dma_common_t	dma_cntl_p;
+	size_t			rx_cntl_alloc_size;
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "==> nxge_free_rx_cntl_pool"));
+
+	pdev = nep->pdev;
+	dma_cntl_poolp = nep->rx_cntl_pool_p;
+	dma_cntl_p = dma_cntl_poolp->dma_buf_pool_p;
+	ndmas = dma_cntl_poolp->ndmas;
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "==> nxge_free_rx_cntl_pool: "
+			"NDMAs[%d]", ndmas));
+
+	if (dma_cntl_poolp == NULL || dma_cntl_poolp->buf_allocated == B_FALSE
+	    || dma_cntl_p == NULL) {
+		goto  nxge_free_rx_cntl_exit;
+	}
+
+	for (i = 0; i < ndmas; i++) {
+		rx_cntl_alloc_size = dma_cntl_p[i].alloc_len;
+		NXGE_DEBUG_MSG((nep, RX_CTL, "nxge_free_rx_cntl_pool: "
+				"Channel[%d], CPU addr[0x%x], DMA addr[0x%x],"
+				" Size[0x%x]", dma_cntl_p[i].dma_channel,
+				dma_cntl_p[i].cpu_addr,
+				dma_cntl_p[i].dma_addr,
+				rx_cntl_alloc_size));
+		NXGE_DEBUG_MSG((nep, RX_CTL, "nxge_free_rx_cntl_pool: "
+				" Size[0x%x]", rx_cntl_alloc_size));
+		pci_free_consistent(pdev, rx_cntl_alloc_size,
+				    (void *)(dma_cntl_p[i].cpu_addr),
+				    dma_cntl_p[i].dma_addr);
+	}
+
+nxge_free_rx_cntl_exit:
+	if (dma_cntl_p != NULL)
+		KMEM_FREE(dma_cntl_p, sizeof (nxge_dma_common_t) * ndmas);
+
+	if (dma_cntl_poolp != NULL)
+		KMEM_FREE(dma_cntl_poolp, sizeof (nxge_dma_pool_t));
+
+	nep->rx_cntl_pool_p = NULL;
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "<== nxge_free_rx_cntl_pool: "
+		       "status 0x%x", status));
+
+	return (status);
+}
+
+static int nxge_alloc_rx_cntl_pool(p_nxge_t nep)
+{
+	int			status = NXGE_OK;
+	int			i, ndmas, st_rdc;
+	struct pci_dev 		*pdev;
+	p_nxge_dma_pool_t	dma_cntl_poolp;
+	p_nxge_dma_common_t	dma_cntl_p;
+	size_t			rx_cntl_alloc_size;
+
+	uint32_t nxge_port_rbr_size;
+	uint32_t nxge_port_rbr_spare_size;
+	uint32_t nxge_port_rcr_size;
+
+	p_nxge_dma_pt_cfg_t	p_all_cfgp;
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "==> nxge_alloc_rx_cntl_pool"));
+
+
+	p_all_cfgp = (p_nxge_dma_pt_cfg_t)&nep->pt_config;
+
+	pdev = nep->pdev;
+	st_rdc = nep->start_rdc;
+	ndmas = nep->max_rdcs;
+
+	nep->intr_timeout = nxge_rcr_timeout | RCRCFIG_B_TIMEOUT;
+	nep->intr_thresh =  nxge_rcr_threshold;
+	nep->max_receive_pkts = nxge_max_rx_pkts;
+
+	/*
+	 * Allocate memory for each receive DMA channel.
+	 */
+	dma_cntl_poolp = (p_nxge_dma_pool_t)
+		KMEM_ZALLOC(sizeof (nxge_dma_pool_t), GFP_KERNEL);
+	if (dma_cntl_poolp == NULL) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_alloc_rx_cntl_pool: "
+			       "alloc failed for cntl pool"));
+		goto nxge_alloc_rx_cntl_fail1;
+	}
+
+	dma_cntl_p = (p_nxge_dma_common_t)
+		KMEM_ZALLOC(sizeof (nxge_dma_common_t) * ndmas, GFP_KERNEL);
+	if (dma_cntl_p == NULL) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_alloc_rx_cntl_pool: "
+			       "alloc failed for cntl pool blocks"));
+		goto nxge_alloc_rx_cntl_fail2;
+	}
+
+	/*
+	 * Assume that each DMA channel will be configured with default
+	 * block size.
+	 * rbr block counts are mod of batch count (16).
+	 */
+	nxge_port_rbr_size = p_all_cfgp->rbr_size;
+	nxge_port_rcr_size = p_all_cfgp->rcr_size;
+	if (!nxge_port_rbr_size) {
+		nxge_port_rbr_size = NXGE_RBR_RBB_DEFAULT;
+	}
+
+	p_all_cfgp->rbr_size = nxge_port_rbr_size;
+	nxge_port_rbr_spare_size = nxge_rbr_spare_size;
+
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "nxge_alloc_rx_cntl_pool: "
+                        "RBR sz[%d] RBR spare sz[%d] RCR sz[%d] MBOX sz[%d]"
+			" RXdesc sz[%d] RXctl sz[%d]", nxge_port_rbr_size, nxge_port_rbr_spare_size, nxge_port_rcr_size, sizeof (rxdma_mailbox_t), sizeof (rx_desc_t), sizeof (rcr_entry_t)));
+
+	/*
+	 * Addresses of receive block ring, receive completion ring and the
+	 * mailbox must be all cache-aligned (64 bytes).
+	 */
+	rx_cntl_alloc_size = nxge_port_rbr_size + nxge_port_rbr_spare_size;
+	rx_cntl_alloc_size *= (sizeof (rx_desc_t));
+	rx_cntl_alloc_size += (sizeof (rcr_entry_t) * nxge_port_rcr_size);
+	rx_cntl_alloc_size += sizeof (rxdma_mailbox_t);
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "nxge_alloc_rx_cntl_pool: "
+			"Cntl pool size [0x%x] [%d]", rx_cntl_alloc_size,
+			rx_cntl_alloc_size));
+
+	/*
+	 * Allocate memory for descriptor rings and mailbox.
+	 */
+	for (i = 0; i < ndmas; i++) {
+
+		void	*addr;
+
+		dma_cntl_p[i].cpu_addr = NULL;
+		addr =  pci_alloc_consistent(pdev, rx_cntl_alloc_size,
+					     &dma_cntl_p[i].dma_addr);
+		if(addr == NULL) {
+		  NXGE_DEBUG_MSG((nep, RX_CTL, "nxge_alloc_rx_cntl_pool: "
+				  "pci_alloc_consistent failed for dma "
+				  "channel[%d]", st_rdc));
+			goto nxge_alloc_rx_cntl_fail3;
+		}
+
+		dma_cntl_p[i].cpu_addr = (caddr_t)addr;
+
+		dma_cntl_p[i].dma_channel = st_rdc;
+		dma_cntl_p[i].alloc_len = rx_cntl_alloc_size;
+
+		NXGE_DEBUG_MSG((nep, RX_CTL, "nxge_alloc_rx_cntl_pool: "
+				"Cntl pool size [0x%x]",
+				dma_cntl_p[i].alloc_len));
+
+		NXGE_DEBUG_MSG((nep, RX_CTL, "nxge_alloc_rx_cntl_pool: "
+				"Channel[%d], CPU addr[0x%x], DMA addr[0x%x],"
+				" Size[0x%x]", st_rdc,
+				dma_cntl_p[i].cpu_addr,
+				dma_cntl_p[i].dma_addr,
+				rx_cntl_alloc_size));
+
+
+		NXGE_DEBUG_MSG((nep, RX_CTL, "nxge_alloc_rx_cntl_pool: "
+				"Cntl pool size [0x%x]", rx_cntl_alloc_size));
+		st_rdc++;
+	}
+
+	dma_cntl_poolp->ndmas = ndmas;
+	NXGE_DEBUG_MSG((nep, RX_CTL, "nxge_alloc_rx_cntl_pool: "
+			"NDMAs[%d]", ndmas));
+	dma_cntl_poolp->buf_allocated = B_TRUE;
+	nep->rx_cntl_pool_p = dma_cntl_poolp;
+	dma_cntl_poolp->dma_buf_pool_p = dma_cntl_p;
+
+	/*
+	 * Set up each descriptor ring using the control block pages.
+	 */
+
+	goto nxge_alloc_rx_cntl_exit;
+
+nxge_alloc_rx_cntl_fail3:
+	/* Free control buffers */
+	i--;
+	for (; i >= 0; i--) {
+		pci_free_consistent(pdev, rx_cntl_alloc_size,
+				    (void *)(dma_cntl_p[i].cpu_addr),
+				    dma_cntl_p[i].dma_addr);
+	}
+
+	KMEM_FREE(dma_cntl_p, sizeof (nxge_dma_common_t) * ndmas);
+
+nxge_alloc_rx_cntl_fail2:
+	KMEM_FREE(dma_cntl_poolp, sizeof (nxge_dma_pool_t));
+
+nxge_alloc_rx_cntl_fail1:
+	status = NXGE_ERROR;
+
+nxge_alloc_rx_cntl_exit:
+	NXGE_DEBUG_MSG((nep, RX_CTL, "<== nxge_alloc_rx_cntl_pool: "
+		       "status 0x%x", status));
+
+	return (status);
+}
+
+
+
+#ifdef USE_VA_INDEX
+static int
+nxge_rxbuf_pp_to_vp(p_nxge_t nep, p_rx_rbr_ring_t rbr_p,
+		uint8_t pktbufsz_type,
+		dma_addr_t pkt_buf_addr_pp,
+		caddr_t *pkt_buf_addr_p,
+		uint32_t *bufoffset,
+		uint32_t *bufoffset_index)
+{
+	uint64_t		pktbuf_pp;
+	uint32_t		offset;
+	p_rx_msg_t 		rx_msg_ring;
+	uint64_t page_alligned;
+	uint64_t cpu_addr;
+	uint32_t *iptr;
+	int index;
+
+
+	pktbuf_pp = (uint64_t)pkt_buf_addr_pp;
+	rx_msg_ring = rbr_p->rx_msg_ring;
+	offset = pktbuf_pp & ~ PAGE_MASK;
+	page_alligned = pktbuf_pp & PAGE_MASK;
+	cpu_addr = (uint64_t)(__va((dma64_addr_t)page_alligned));
+
+	iptr = (uint32_t *) (cpu_addr + 60);
+	index = *iptr;
+	if ((rx_msg_ring[index].cpu_addr == (caddr_t)(cpu_addr))) {
+		*pkt_buf_addr_p = (caddr_t)(cpu_addr + offset);
+		*bufoffset_index = index;
+		*bufoffset =  offset;
+	} else {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "RX Address mismatch"
+						" index %x expected %llx found %llx",
+						rx_msg_ring[index].cpu_addr, cpu_addr));
+		return (NXGE_ERROR);
+
+	}
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "<== nxge_rxbuf_pp_to_vp"));
+	return (NXGE_OK);
+
+}
+#endif
+
+#ifdef USE_RING_HASH
+
+static int
+nxge_rxbuf_pp_to_vp(p_nxge_t nep, p_rx_rbr_ring_t rbr_p,
+		uint8_t pktbufsz_type,
+		dma_addr_t pkt_buf_addr_pp,
+		caddr_t *pkt_buf_addr_p,
+		uint32_t *bufoffset,
+		uint32_t *bufoffset_index)
+{
+	int			bufsize;
+	uint64_t		pktbuf_pp;
+	uint64_t		dma_addr;
+	rxring_info_t		*ring_info;
+	uint32_t		offset, chunk_size, block_size, page_size_mask;
+	uint32_t		chunk_index, block_index, total_index;
+	int 			max_iterations, iteration;
+	rxbuf_index_info_t 	*buf_info;
+
+	p_rx_msg_t 		rx_msg_ring;
+	int index, bucket, hash_found = -1;
+	uint32_t bkaddr;
+	uint64_t page_alligned;
+	p_rxring_info_hash_t rh;
+	uint64_t cpu_addr = 0;
+	pktbuf_pp = (uint64_t)pkt_buf_addr_pp;
+
+	switch (pktbufsz_type) {
+	case 0:
+		bufsize = rbr_p->pkt_buf_size0;
+		break;
+	case 1:
+		bufsize = rbr_p->pkt_buf_size1;
+		break;
+	case 2:
+		bufsize = rbr_p->pkt_buf_size2;
+		break;
+	case RCR_SINGLE_BLOCK:
+		bufsize = rbr_p->block_size;
+		break;
+	default:
+		return (-1);
+	}
+
+
+	page_alligned = pktbuf_pp & PAGE_MASK;
+	offset = pktbuf_pp & ~ PAGE_MASK;
+	bkaddr = page_alligned >> RBR_BKADDR_SHIFT;
+	rx_msg_ring = rbr_p->rx_msg_ring;
+
+
+	if (rbr_p->search_hint[pktbufsz_type] != NO_HINT) {
+		hash_found = rbr_p->search_hint[pktbufsz_type];
+		if (rx_msg_ring[hash_found].shifted_addr == bkaddr) {
+			cpu_addr = (uint64_t)rx_msg_ring[hash_found].cpu_addr + offset;
+			goto found_info;
+		}
+	}
+
+	index = (bkaddr) & RING_HASH_FUNC;
+	bucket = 0;
+	rh = (p_rxring_info_hash_t) rbr_p->ring_hash;
+	rh += RING_HASH_BUCKETS * index;
+	for (bucket = 0; bucket < RING_HASH_BUCKETS; bucket++) {
+		if (rh[bucket].key == bkaddr) {
+			hash_found = rh[bucket].index;
+			cpu_addr = (uint64_t)rx_msg_ring[hash_found].cpu_addr + offset;
+			break;
+		}
+	}
+	if (bucket == RING_HASH_BUCKETS) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+						"search: failed dma_addr 0x%llx bkaddr %x",
+						pktbuf_pp, bkaddr));
+		return (NXGE_ERROR);
+	}
+  found_info:
+
+	if (offset == 0) {
+			/* first buffer of the block: set hint */
+		rbr_p->search_hint[pktbufsz_type] = hash_found;
+	}
+
+	if ((offset + bufsize) == rbr_p->block_size) {
+			/* first buffer of the block: set hint */
+		rbr_p->search_hint[pktbufsz_type] = NO_HINT;
+	}
+
+	*pkt_buf_addr_p = (caddr_t)cpu_addr;
+	total_index = hash_found;
+
+	NXGE_DEBUG_MSG((nep, DMA_CTL, "nxge_rxbuf_pp_to_vp: get msg index"));
+
+	*bufoffset_index = total_index;
+	*bufoffset =  offset;
+
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "<== nxge_rxbuf_pp_to_vp"));
+
+	return (NXGE_OK);
+}
+#endif
+
+#ifdef USE_BTREE
+
+#define MID_INDEX(l, r) ((r + l + 1) >> 1)
+
+#define TO_LEFT -1
+#define TO_RIGHT 1
+#define BOTH_RIGHT (TO_RIGHT + TO_RIGHT)
+#define BOTH_LEFT (TO_LEFT + TO_LEFT)
+#define IN_MIDDLE (TO_RIGHT + TO_LEFT)
+
+static int
+nxge_rxbuf_pp_to_vp(p_nxge_t nep, p_rx_rbr_ring_t rbr_p,
+		uint8_t pktbufsz_type,
+		dma_addr_t pkt_buf_addr_pp,
+		caddr_t *pkt_buf_addr_p,
+		uint32_t *bufoffset,
+		uint32_t *bufoffset_index)
+{
+	int			bufsize;
+	uint64_t		pktbuf_pp;
+	uint64_t		dma_addr;
+	rxring_info_t		*ring_info;
+	int			base_side, end_side;
+	int			r_index, l_index, anchor_index = 0;
+	int			found, search_done;
+	uint32_t		offset, chunk_size, block_size, page_size_mask;
+	uint32_t		chunk_index, block_index, total_index;
+	int 			max_iterations, iteration;
+	rxbuf_index_info_t 	*buf_info;
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "==> nxge_rxbuf_pp_to_vp"));
+
+	NXGE_DEBUG_MSG((nep, RX_CTL,
+			"nxge_rxbuf_pp_to_vp: buf_pp $%p btype %d",
+			pkt_buf_addr_pp,
+			pktbufsz_type));
+
+	pktbuf_pp = (uint64_t)pkt_buf_addr_pp;
+
+	switch (pktbufsz_type) {
+	case 0:
+		bufsize = rbr_p->pkt_buf_size0;
+		break;
+	case 1:
+		bufsize = rbr_p->pkt_buf_size1;
+		break;
+	case 2:
+		bufsize = rbr_p->pkt_buf_size2;
+		break;
+	case RCR_SINGLE_BLOCK:
+		bufsize = rbr_p->block_size;
+		anchor_index = 0;
+		break;
+	default:
+		return (-1);
+	}
+
+	if (rbr_p->chunk_cnt == 1) {
+		anchor_index = 0;
+		ring_info = rbr_p->ring_info;
+		buf_info = (rxbuf_index_info_t *) ring_info->buffer_arr;
+		NXGE_DEBUG_MSG((nep, RX_CTL,
+				"nxge_rxbuf_pp_to_vp: (found, 1 block) "
+				"buf_pp $%p btype %d "
+				"anchor_index %d buf_info $%p",
+				pkt_buf_addr_pp,
+				pktbufsz_type,
+				anchor_index,
+				buf_info));
+
+		goto found_index;
+	}
+
+	NXGE_DEBUG_MSG((nep, RX_CTL,
+			"nxge_rxbuf_pp_to_vp: "
+			"buf_pp $%p btype %d anchor_index %d",
+			pkt_buf_addr_pp,
+			pktbufsz_type,
+			anchor_index));
+
+	ring_info = rbr_p->ring_info;
+	found = B_FALSE;
+	buf_info = (rxbuf_index_info_t *) ring_info->buffer_arr;
+	iteration = 0;
+	max_iterations = ring_info->max_iterations;
+		/*
+		 * First check if this block have been seen
+		 * recently. This is indicated by a hint which
+		 * is initialized when the first buffer of the block
+		 * is seen. The hint is reset when the last buffer of
+		 * the block has been processed.
+		 * As three block sizes are supported, three hints
+		 * are kept. The idea behind the hints is that once
+		 * the hardware  uses a block for a buffer  of that
+		 * size, it will use it exclusively for that size
+		 * and will use it until it is exhausted. It is assumed
+		 * that there would a single block being used for the same
+		 * buffer sizes at any given time.
+		 */
+	if (ring_info->search_hint[pktbufsz_type] != NO_HINT) {
+		anchor_index = ring_info->search_hint[pktbufsz_type];
+		dma_addr =  (uint64_t)(buf_info[anchor_index].dma_addr);
+		chunk_size = buf_info[anchor_index].block_size;
+		if ((pktbuf_pp >= dma_addr) &&
+			(pktbuf_pp < (dma_addr + chunk_size))) {
+			found = B_TRUE;
+				/*
+				 * check if this is the last buffer in the
+				 * block
+				 * If so, then reset the hint for the size;
+				 */
+
+			if ((pktbuf_pp + bufsize) >= (dma_addr + chunk_size))
+				ring_info->search_hint[pktbufsz_type] = NO_HINT;
+		}
+	}
+
+	if (found == B_FALSE) {
+		NXGE_DEBUG_MSG((nep, RX_CTL,
+				"nxge_rxbuf_pp_to_vp: (!found)"
+				"buf_pp $%p btype %d "
+				"anchor_index %d",
+				pkt_buf_addr_pp,
+				pktbufsz_type,
+				anchor_index));
+
+			/*
+			 * This is the first buffer of the block of this
+			 * size. Need to search the whole information
+			 * array.
+			 * the search algorithm uses a binary tree search
+			 * algorithm. It assumes that the information is
+			 * already sorted with increasing order
+			 * info[0] < info[1] < info[2]  .... < info[n-1]
+			 * where n is the size of the information array
+			 */
+		r_index = rbr_p->chunk_cnt - 1; /* ring_info->num_blocks - 1; */
+		l_index = 0;
+		search_done = B_FALSE;
+		anchor_index = MID_INDEX(r_index, l_index);
+		while (search_done == B_FALSE) {
+			if ((r_index == l_index) || (iteration >= max_iterations))
+				search_done = B_TRUE;
+			end_side = TO_RIGHT; /* to the right */
+			base_side = TO_LEFT; /* to the left */
+			/* read the DVMA address information and sort it */
+			dma_addr =  (uint64_t)(buf_info[anchor_index].dma_addr);
+			chunk_size = buf_info[anchor_index].block_size;
+			NXGE_DEBUG_MSG((nep, RX_CTL,
+					"nxge_rxbuf_pp_to_vp: (searching)"
+					"buf_pp $%p btype %d "
+					"anchor_index %d chunk_size %d "
+					"dmaaddr $%p",
+					pkt_buf_addr_pp,
+					pktbufsz_type,
+					anchor_index,
+					chunk_size,
+					dma_addr));
+
+			if (pktbuf_pp >= dma_addr)
+				base_side = TO_RIGHT; /* to the right */
+			if (pktbuf_pp < (dma_addr + chunk_size))
+				end_side = TO_LEFT; /* to the left */
+
+			switch (base_side + end_side) {
+				case IN_MIDDLE:
+						/* found */
+					found = B_TRUE;
+					search_done = B_TRUE;
+					if ((pktbuf_pp + bufsize) <
+						(dma_addr + chunk_size))
+						ring_info->search_hint[pktbufsz_type] =
+						buf_info[anchor_index].block_index;
+					break;
+				case BOTH_RIGHT:
+						/* not found: go to the right */
+					l_index = anchor_index + 1;
+					anchor_index = MID_INDEX(r_index, l_index);
+					break;
+
+				case  BOTH_LEFT:
+						/* not found: go to the left */
+					r_index = anchor_index - 1;
+					anchor_index = MID_INDEX(r_index, l_index);
+					break;
+				default: /* should't come here */
+					return (NXGE_OK);
+			}
+			iteration++;
+		}
+
+		NXGE_DEBUG_MSG((nep, RX_CTL,
+				"nxge_rxbuf_pp_to_vp: (search done)"
+				"buf_pp $%p btype %d "
+				"anchor_index %d",
+				pkt_buf_addr_pp,
+				pktbufsz_type,
+				anchor_index));
+	}
+
+	if (found == B_FALSE) {
+		NXGE_DEBUG_MSG((nep, RX_CTL,
+				"nxge_rxbuf_pp_to_vp: (search failed)"
+				"buf_pp $%p btype %d "
+				"anchor_index %d",
+				pkt_buf_addr_pp,
+				pktbufsz_type,
+				anchor_index));
+		return (NXGE_OK);
+	}
+
+  found_index:
+	NXGE_DEBUG_MSG((nep, RX_CTL,
+			"nxge_rxbuf_pp_to_vp: (FOUND1)"
+			"buf_pp $%p btype %d bufsize %d anchor_index %d",
+			pkt_buf_addr_pp,
+			pktbufsz_type,
+			bufsize,
+			anchor_index));
+
+	/* index of the first block in this chunk */
+	chunk_index = buf_info[anchor_index].start_buf_index;
+	dma_addr =  (uint64_t)(buf_info[anchor_index].dma_addr);
+	page_size_mask = ring_info->block_size_mask;
+
+	NXGE_DEBUG_MSG((nep, RX_CTL,
+			"nxge_rxbuf_pp_to_vp: (FOUND3), get chunk)"
+			"buf_pp $%p btype %d bufsize %d "
+			"anchor_index %d chunk_index %d dma $%p",
+			pkt_buf_addr_pp,
+			pktbufsz_type,
+			bufsize,
+			anchor_index,
+			chunk_index,
+			dma_addr));
+
+	offset = pktbuf_pp - dma_addr; /* offset within the chunk */
+	block_size = rbr_p->block_size; /* System  block(page) size */
+
+	NXGE_DEBUG_MSG((nep, RX_CTL,
+			"nxge_rxbuf_pp_to_vp: (FOUND4), get chunk)"
+			"buf_pp $%p btype %d bufsize %d "
+			"anchor_index %d chunk_index %d dma $%p "
+			"offset %d block_size %d",
+			pkt_buf_addr_pp,
+			pktbufsz_type,
+			bufsize,
+			anchor_index,
+			chunk_index,
+			dma_addr,
+			offset,
+			block_size));
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "==> getting total index"));
+
+	block_index = (offset / block_size); /* index within chunk */
+	total_index = chunk_index + block_index;
+
+
+	NXGE_DEBUG_MSG((nep, RX_CTL,
+			"nxge_rxbuf_pp_to_vp: "
+			"total_index %d dma_addr $%p "
+			"offset %d block_size %d "
+			"block_index %d ",
+			total_index, dma_addr,
+			offset, block_size,
+			block_index));
+
+	*pkt_buf_addr_p = (caddr_t)(page_address(buf_info[anchor_index].page_p)) + (offset);
+
+	NXGE_DEBUG_MSG((nep, RX_CTL,
+			"nxge_rxbuf_pp_to_vp: "
+			"total_index %d dma_addr $%p "
+			"offset %d block_size %d "
+			"block_index %d "
+			"*pkt_buf_addr_p $%p",
+			total_index, dma_addr,
+			offset, block_size,
+			block_index,
+			*pkt_buf_addr_p));
+
+
+	NXGE_DEBUG_MSG((nep, DMA_CTL, "nxge_rxbuf_pp_to_vp: get msg index"));
+#if 0
+	pci_dma_sync_single_for_cpu(nep->pdev, pkt_buf_addr_pp, bufsize,
+				    PCI_DMA_FROMDEVICE);
+#endif
+	*bufoffset_index = total_index;
+	*bufoffset =  (offset & page_size_mask);
+
+	NXGE_DEBUG_MSG((nep, RX_CTL,
+			"nxge_rxbuf_pp_to_vp: get msg index: "
+			"bufoffset_index %d, bufoffset %d",
+			*bufoffset_index, *bufoffset));
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "<== nxge_rxbuf_pp_to_vp"));
+
+	return (NXGE_OK);
+}
+#endif
+
+
+#ifdef USE_VA_INDEX
+static
+struct sk_buff *
+nxge_rx_replace_page(p_nxge_t nep, uint8_t channel,
+					 p_rx_msg_t		rx_msg_p,
+					 p_rx_rbr_ring_t rx_rbr_p,
+					 int index, struct sk_buff *old_skb)
+{
+	struct sk_buff		*skb;
+	p_nxge_dma_buf_t 	dma_buf_p;
+	uint32_t *iptr;
+	struct page	*page, *old_page;
+	skb_frag_t *fragp;
+	int frag_ptr = 0;
+
+	dma_buf_p = rx_rbr_p->dma_buf_p;
+
+	if (old_skb) {
+		skb = old_skb;
+		frag_ptr = skb_shinfo(skb)->nr_frags;
+	} else {
+		skb = dev_alloc_skb(128 + NET_IP_ALIGN);
+		if (skb == NULL) {
+			return (skb);
+		}
+	}
+
+	old_page = dma_buf_p[index].page_p;
+	page = alloc_pages(GFP_ATOMIC, nep->page_order);
+	if (page == NULL) {
+		if (old_skb == NULL) {
+			dev_kfree_skb_any(skb);
+		}
+		return (NULL);
+	}
+	pci_unmap_page(nep->pdev,
+				   dma_buf_p[index].dma_addr,
+				   nep->page_size, PCI_DMA_FROMDEVICE);
+	dma_buf_p[index].page_p = page;
+	fragp = &skb_shinfo(skb)->frags[frag_ptr];
+	fragp->page = old_page;
+
+	dma_buf_p[index].dma_addr = pci_map_page(nep->pdev,
+											 dma_buf_p[index].page_p,
+											 0, nep->page_size,
+											 PCI_DMA_FROMDEVICE);
+	dma_buf_p[index].dma_channel = channel;
+	dma_buf_p[index].dma_chunk_index = index;
+
+	rx_msg_p->free = B_TRUE;
+	rx_msg_p->cur_usage_cnt = 0;
+
+	rx_msg_p->cpu_addr = ((caddr_t)page_address(dma_buf_p[index].page_p));
+	iptr = (uint32_t *) (rx_msg_p->cpu_addr + 60);
+	*iptr = index;
+	rx_msg_p->dma_addr = dma_buf_p[index].dma_addr;
+	rx_msg_p->block_index = index;
+	rx_msg_p->shifted_addr = (uint32_t) (rx_msg_p->dma_addr >> RBR_BKADDR_SHIFT);
+	nxge_post_buf_blk(nep, rx_rbr_p, rx_msg_p);
+	return (skb);
+}
+#endif
+
+#ifdef USE_RING_HASH
+
+static
+struct sk_buff *
+nxge_rx_replace_page(p_nxge_t nep, uint8_t channel,
+					 p_rx_msg_t		rx_msg_p,
+					 p_rx_rbr_ring_t rbr_p,
+					 int index, struct sk_buff *old_skb)
+{
+	struct sk_buff		*skb, *new_skb;
+	p_nxge_dma_buf_t 	dma_buf_p;
+	uint32_t h_index, bucket;
+	p_rxring_info_hash_t rh;
+	struct page	*page, *old_page;
+	skb_frag_t *fragp;
+	int frag_ptr = 0;
+
+	dma_buf_p = rbr_p->dma_buf_p;
+
+	if (old_skb) {
+		skb = old_skb;
+		frag_ptr  = skb_shinfo(skb)->nr_frags;
+	} else {
+		skb = dev_alloc_skb(128 + NET_IP_ALIGN);
+		if (skb == NULL) {
+			return (skb);
+		}
+	}
+	old_page = dma_buf_p[index].page_p;
+	page = alloc_pages(GFP_ATOMIC, nep->page_order);
+	if (page == NULL) {
+		if (old_skb == NULL) {
+			dev_kfree_skb_any(skb);
+		}
+		return (NULL);
+	}
+	pci_unmap_page(nep->pdev,
+				   dma_buf_p[index].dma_addr,
+				   nep->page_size, PCI_DMA_FROMDEVICE);
+	dma_buf_p[index].page_p = page;
+	fragp = &skb_shinfo(skb)->frags[frag_ptr];
+	fragp->page = old_page;
+
+	h_index = (rx_msg_p->shifted_addr) & RING_HASH_FUNC;
+	bucket = 0;
+
+	rh = (p_rxring_info_hash_t) rbr_p->ring_hash;
+	rh += RING_HASH_BUCKETS * h_index;
+	for (bucket = 0; bucket < RING_HASH_BUCKETS; bucket++) {
+		if (rh[bucket].key == rx_msg_p->shifted_addr) {
+			rh[bucket].key = NXGE_RX_RING_HASH_NULL_KEY;
+			rh[bucket].index = -1;
+			break;
+		}
+	}
+	if (bucket == RING_HASH_BUCKETS)
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+						"search to delete: failed dma_addr bkaddr %x",
+						rx_msg_p->shifted_addr));
+
+	dma_buf_p[index].dma_addr = pci_map_page(nep->pdev,
+											 dma_buf_p[index].page_p,
+											 0, nep->page_size,
+											 PCI_DMA_FROMDEVICE);
+	dma_buf_p[index].dma_channel = channel;
+	dma_buf_p[index].dma_chunk_index = index;
+
+	rx_msg_p->free = B_TRUE;
+	rx_msg_p->cur_usage_cnt = 0;
+	rx_msg_p->cpu_addr = ((caddr_t)page_address(dma_buf_p[index].page_p));
+
+	rx_msg_p->dma_addr = dma_buf_p[index].dma_addr;
+	rx_msg_p->block_index = index;
+	rx_msg_p->shifted_addr = (uint32_t) (rx_msg_p->dma_addr >> RBR_BKADDR_SHIFT);
+	h_index = (rx_msg_p->shifted_addr) & RING_HASH_FUNC;
+	rh = (p_rxring_info_hash_t) rbr_p->ring_hash;
+	rh += RING_HASH_BUCKETS * h_index;
+	for (bucket = 0; bucket < RING_HASH_BUCKETS; bucket++) {
+		if (rh[bucket].key == NXGE_RX_RING_HASH_NULL_KEY) {
+			rh[bucket].key = rx_msg_p->shifted_addr;
+			rh[bucket].index = index;
+			break;
+		}
+	}
+	if (bucket == RING_HASH_BUCKETS)
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+						"search: to add failed dma_addr bkaddr %x",
+						rx_msg_p->shifted_addr));
+
+	nxge_post_buf_blk(nep, rbr_p, rx_msg_p);
+	return (skb);
+}
+#endif
+
+#define NXGE_RX_SINGLE_PKT 0
+#define NXGE_RX_MULTI_PKT_FIRST 1
+#define NXGE_RX_MULTI_PKT_OTHER 2
+#define NXGE_RX_MULTI_PKT_LAST 4
+
+static void nxge_receive_packet(p_nxge_t nep,
+				p_rx_rcr_ring_t rcr_p,
+				uint64_t rcr_entry,
+				boolean_t *multi_p,
+				struct sk_buff **first_skbp,
+				int *multi_bytes_remain, int rcr_index)
+{
+	boolean_t		multi = B_FALSE;
+	uint8_t			pkt_type;
+	boolean_t		first_entry = B_FALSE;
+	boolean_t		last_entry = B_FALSE;
+	boolean_t		one_buf_pkt = B_FALSE;
+	boolean_t		hw_cksum = B_FALSE;
+	uint8_t			error_type;
+	uint8_t channel;
+	uint8_t			noport;
+	uint8_t multi_pkt_type = NXGE_RX_SINGLE_PKT;
+
+	uint16_t		l2_len, l2_len_nocrcstrip;
+	uint8_t			pktbufsz_type;
+	dma_addr_t		pkt_buf_addr_pp;
+	caddr_t			pkt_buf_addr_p, pkt_addr_p, pkt_data_addr_p;
+	uint32_t		bufoffset_index, buf_offset;
+	uint32_t		bsize, pkt_data_size;
+	uint32_t sw_offset_bytes;
+	uint32_t		msg_index;
+	p_rx_rbr_ring_t		rx_rbr_p;
+	p_rx_msg_t 		rx_msg_ring_p;
+	p_rx_msg_t		rx_msg_p;
+	uint16_t		hdr_size = 0;
+	struct sk_buff		*skb;
+	boolean_t		status = B_TRUE;
+	p_nxge_rx_ring_stats_t	rdc_stats;
+	boolean_t		dcf_err = B_FALSE;
+	boolean_t	release_skb = B_FALSE;
+	rxpkt_short_hdr_t *pkt_hdr;
+	boolean_t bfree = B_FALSE;
+	channel = rcr_p->rdc;
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "==> nxge_receive_packet"));
+
+	multi = ((((rcr_entry & RCR_MULTI_MASK) >> RCR_MULTI_SHIFT)) ?
+			B_TRUE : B_FALSE);
+	dcf_err = ((((rcr_entry & RCR_DCF_ERROR_MASK) >>
+		     RCR_DCF_ERROR_SHIFT)) ? B_TRUE : B_FALSE);
+	pkt_type = ((rcr_entry & RCR_PKT_TYPE_MASK) >> RCR_PKT_TYPE_SHIFT);
+	error_type = ((rcr_entry & RCR_ERROR_MASK) >> RCR_ERROR_SHIFT) & 0x07;
+	noport = ((rcr_entry & RCR_FRAG_MASK) >> RCR_FRAG_SHIFT);
+	l2_len = ((rcr_entry & RCR_L2_LEN_MASK) >> RCR_L2_LEN_SHIFT);
+	pktbufsz_type = ((rcr_entry & RCR_PKTBUFSZ_MASK) >> RCR_PKTBUFSZ_SHIFT);
+
+
+	pkt_buf_addr_pp =
+		(rcr_entry & RCR_PKT_BUF_ADDR_MASK) >> RCR_PKT_BUF_ADDR_SHIFT;
+
+	/* get the stats ptr */
+	rdc_stats = rcr_p->rdc_stats;
+
+	l2_len_nocrcstrip = l2_len;
+
+	/* shift 6 bits to get the full io address */
+	pkt_buf_addr_pp = ((uint64_t)pkt_buf_addr_pp <<
+			   RCR_PKT_BUF_ADDR_SHIFT_FULL);
+
+	if ((l2_len == 0) ||
+		(pkt_buf_addr_pp == 0)) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_receive_packet failed: "
+		       "Bad RCR Entry? pkt_buf_addr_pp %llx l2_len %d rcr_entry %llx",
+						pkt_buf_addr_pp, l2_len, rcr_entry));
+		rdc_stats->ierrors++;
+		return;
+	}
+
+	l2_len  -= 4;
+
+	rx_rbr_p = rcr_p->rx_rbr_p;
+	rx_msg_ring_p = rx_rbr_p->rx_msg_ring;
+
+	if (multi == B_FALSE) {
+		if (*multi_p == B_FALSE) {
+			multi_pkt_type = NXGE_RX_SINGLE_PKT;
+			one_buf_pkt = B_TRUE;
+			first_entry = B_TRUE;
+			last_entry = B_TRUE;
+		} else {
+			multi_pkt_type = NXGE_RX_MULTI_PKT_LAST;
+			one_buf_pkt = B_FALSE;
+			first_entry = B_FALSE;
+			last_entry = B_TRUE;
+		}
+	} else {
+		one_buf_pkt = B_FALSE;
+		last_entry = B_FALSE;
+		if (*multi_p == B_FALSE) {
+			multi_pkt_type = NXGE_RX_MULTI_PKT_FIRST;
+			first_entry = B_TRUE;
+		} else {
+			multi_pkt_type = NXGE_RX_MULTI_PKT_OTHER;
+			first_entry = B_FALSE;
+		}
+	}
+
+	sw_offset_bytes = rcr_p->sw_offset_bytes;
+	if (first_entry) {
+		hdr_size = (rcr_p->full_hdr_flag ? RXDMA_HDR_SIZE_FULL :
+			RXDMA_HDR_SIZE_DEFAULT);
+
+			/*
+			 * ERROR, FRAG and PKT_TYPE are only reported
+			 * in the first entry.
+			 * If a packet is not fragmented and no error bit is
+			 * set, then L4 checksum is OK.
+			 */
+		if ((!noport) && (!error_type) &&
+			(pkt_type == RCR_PKT_TYPE_UDP ||
+			 pkt_type == RCR_PKT_TYPE_TCP)) {
+			hw_cksum = B_TRUE;
+		}
+
+	}
+
+	*multi_p = multi;
+
+	NXGE_DEBUG_MSG((nep, RX_CTL,
+		"(rbr 1) nxge_receive_packet: entry 0x%0llx "
+		"full pkt_buf_addr_pp $%p l2_len %d",
+		rcr_entry, pkt_buf_addr_pp, l2_len));
+
+	/*
+	 * Packet buffer address in the completion entry points
+	 * to the starting buffer address (offset 0).
+	 * Use the starting buffer address to locate the corresponding
+	 * kernel address.
+	 */
+	status = nxge_rxbuf_pp_to_vp(nep, rx_rbr_p,
+				     pktbufsz_type, pkt_buf_addr_pp,
+				     &pkt_buf_addr_p, &buf_offset,
+				     &bufoffset_index);
+
+	if (status != NXGE_OK) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+				"nxge_receive_packet: find vaddr failed"
+						" index %d rcr_entry %llx hwaddr %llx ",
+				rcr_index, rcr_entry, pkt_buf_addr_pp));
+		return;
+	}
+
+#if 1
+	switch (pktbufsz_type) {
+	case RCR_PKTBUFSZ_0:
+		bsize = rx_rbr_p->pkt_buf_size0_bytes;
+/*		rdc_stats->rx_bufsz0_rcvd++; */
+		NXGE_DEBUG_MSG((nep, RX_CTL,
+				"nxge_receive_packet: 0 buf %d", bsize));
+
+		break;
+	case RCR_PKTBUFSZ_1:
+		bsize = rx_rbr_p->pkt_buf_size1_bytes;
+/*		rdc_stats->rx_bufsz1_rcvd++; */
+		NXGE_DEBUG_MSG((nep, RX_CTL,
+				"nxge_receive_packet: 1 buf %d", bsize));
+
+		break;
+	case RCR_PKTBUFSZ_2:
+		bsize = rx_rbr_p->pkt_buf_size2_bytes;
+/*		rdc_stats->rx_bufsz2_rcvd++; */
+		release_skb = B_TRUE;
+
+		break;
+	case RCR_SINGLE_BLOCK:
+		bsize = rx_rbr_p->block_size;
+/*		rdc_stats->rx_sgl_blk_rcvd++; */
+		NXGE_DEBUG_MSG((nep, RX_CTL,
+				"nxge_receive_packet: single %d", bsize));
+		release_skb = B_TRUE;
+		break;
+	default:
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_receive_packet: "
+			       "invalid pkt buf size type %d\n",
+			       pktbufsz_type));
+		bsize = 0;
+		return;
+	}
+#else
+	bsize = rx_rbr_p->bsize[pktbufsz_type];
+#endif
+
+	rdc_stats->rx_rcvd_bufsz[pktbufsz_type]++;
+
+
+	if (release_skb == B_FALSE)
+		pci_dma_sync_single_for_cpu(nep->pdev, pkt_buf_addr_pp, bsize,
+				    PCI_DMA_FROMDEVICE);
+
+	msg_index = bufoffset_index;
+	rx_msg_p = &rx_msg_ring_p[msg_index];
+	pkt_addr_p = pkt_buf_addr_p + sw_offset_bytes;
+
+#if 0
+	if (rx_msg_p->cur_usage_cnt == 0) {
+		rx_msg_p->pkt_buf_size = bsize;
+		rx_msg_p->cur_usage_cnt = 1;
+		if (pktbufsz_type == RCR_SINGLE_BLOCK) {
+			/*
+			 * Buffer can be reused once the free function
+			 * is called.
+			 */
+			rx_msg_p->max_usage_cnt = 1;
+			bfree = B_TRUE;
+		} else {
+			rx_msg_p->max_usage_cnt = rx_rbr_p->max_usage[pktbufsz_type];
+			if (rx_msg_p->max_usage_cnt == 1) {
+				bfree = B_TRUE;
+			}
+		}
+	} else {
+		rx_msg_p->cur_usage_cnt++;
+		if (rx_msg_p->cur_usage_cnt == rx_msg_p->max_usage_cnt) {
+			bfree = B_TRUE;
+		}
+	}
+#else
+	rx_msg_p->cur_usage_cnt++;
+	if (rx_msg_p->cur_usage_cnt == rx_rbr_p->max_usage[pktbufsz_type])
+		bfree = B_TRUE;
+#endif
+
+/*
+ * workaround  for fragment bug
+ * Fragment packets are marked as having checksum error.
+ * Since this could be a false alarm, we just let these frames
+ * go upto the stack
+ *
+ */
+	if ((error_type) || (dcf_err == B_TRUE)) {
+		boolean_t drop_packet = B_FALSE;
+
+		rdc_stats->ierrors++;
+		if (dcf_err == B_TRUE) {
+			NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_receive_packet: "
+				"dcf_err"));
+			rdc_stats->dcf_err++;
+		/* Fatal error, need to reset rxport */
+		} else {
+
+		/* Update error stats */
+			rdc_stats->errlog.compl_err_type = error_type;
+			switch (error_type) {
+				case RCR_L2_ERROR:
+					rdc_stats->l2_err++;
+					break;
+
+				case RCR_L4_CSUM_ERROR:
+					rdc_stats->l4_cksum_err++;
+					break;
+
+				case RCR_FFLP_SOFT_ERROR:
+					rdc_stats->fflp_soft_err++;
+					break;
+
+				case RCR_ZCP_SOFT_ERROR:
+					rdc_stats->zcp_soft_err++;
+					break;
+
+				default:
+					NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+							" nxge_receive_packet:"
+							" RCR error 0x%x",
+							error_type));
+					break;
+
+			}
+
+		}
+
+		/*
+		 * Update and repost buffer block if max usage
+		 * count is reached.
+		 */
+		if (drop_packet == B_TRUE) {
+			if (bfree == B_TRUE) {
+				pci_dma_sync_single_for_device(nep->pdev,
+										   pkt_buf_addr_pp,
+										   bsize, PCI_DMA_FROMDEVICE);
+				nxge_post_buf_blk(nep, rx_rbr_p, rx_msg_p);
+			}
+			return;
+		}
+	}
+
+
+	pkt_data_addr_p = pkt_addr_p + hdr_size;
+
+	pkt_data_size = bsize - (hdr_size + sw_offset_bytes);
+	pkt_hdr = (rxpkt_short_hdr_t *)pkt_addr_p;
+
+	if (multi_pkt_type == NXGE_RX_SINGLE_PKT) {
+		skb_frag_t *fragp;
+#if 0
+		int copy_hdr_len = ETH_HLEN;
+		if (pkt_hdr->bits.vlan || pkt_hdr->bits.llcsnap)
+			copy_hdr_len += 4;
+#else
+		int copy_hdr_len = 64;
+
+#endif
+
+		if (release_skb == B_TRUE) {
+			skb = nxge_rx_replace_page(nep, channel,
+					   rx_msg_p, rx_rbr_p, msg_index, *first_skbp);
+			if (skb == NULL) {
+				rdc_stats->rx_no_buf++;
+				NXGE_DEBUG_MSG((nep, RX_CTL,
+						"nxge_receive_packet: "
+						"skb alloc failed S1"));
+				if (bfree == B_TRUE) {
+					pci_dma_sync_single_for_device(nep->pdev,
+							   pkt_buf_addr_pp,
+							   bsize,
+							   PCI_DMA_FROMDEVICE);
+					nxge_post_buf_blk(nep, rx_rbr_p, rx_msg_p);
+				}
+				return;
+			}
+
+			skb->dev = nep->dev;
+			skb_reserve(skb, NET_IP_ALIGN);
+			fragp = &skb_shinfo(skb)->frags[0];
+			NXGE_MAP_KERN_PAGE(fragp->page);
+			memcpy(skb_put(skb, copy_hdr_len),
+				   pkt_data_addr_p, copy_hdr_len);
+			NXGE_UNMAP_KERN_PAGE(fragp->page);
+			skb->len = l2_len;
+			skb->data_len = fragp->size = l2_len - copy_hdr_len;
+			fragp->page_offset = (hdr_size + sw_offset_bytes + copy_hdr_len);
+			skb_shinfo(skb)->nr_frags = 1;
+		} else {
+			skb = dev_alloc_skb(l2_len + NET_IP_ALIGN);
+			if (skb == NULL) {
+				rdc_stats->rx_no_buf++;
+				NXGE_DEBUG_MSG((nep, RX_CTL,
+						"nxge_receive_packet: "
+						"skb alloc failed SS"));
+				if (bfree == B_TRUE) {
+					pci_dma_sync_single_for_device(nep->pdev,
+						    pkt_buf_addr_pp,
+						    bsize,
+						    PCI_DMA_FROMDEVICE);
+					nxge_post_buf_blk(nep, rx_rbr_p, rx_msg_p);
+				}
+				return;
+			}
+			skb->dev = nep->dev;
+			skb_reserve(skb, NET_IP_ALIGN);
+			skb_put(skb, l2_len);
+			memcpy(skb->data, pkt_data_addr_p, l2_len);
+
+				/*
+				 * Update and repost buffer block if max usage
+				 * count is reached.
+				 */
+			if ((bfree == B_TRUE) &&
+				(release_skb == B_FALSE)) {
+				pci_dma_sync_single_for_device(nep->pdev,
+											   pkt_buf_addr_pp,
+											   bsize, PCI_DMA_FROMDEVICE);
+				nxge_post_buf_blk(nep, rx_rbr_p, rx_msg_p);
+			}
+
+		}
+
+		NXGE_ADD_SKB_TRUESIZE(skb, l2_len);
+
+		skb->ip_summed =
+			(hw_cksum ? CHECKSUM_UNNECESSARY : CHECKSUM_NONE);
+		skb->csum = (hw_cksum ? 0 : 0xffff);
+		skb->protocol = eth_type_trans(skb, nep->dev);
+		nxge_skb_release(nep, skb);
+		rdc_stats->ipackets++;
+		rdc_stats->ibytes += l2_len;
+
+	} else if (multi_pkt_type == NXGE_RX_MULTI_PKT_FIRST) {
+		skb_frag_t *fragp;
+#if 0
+		int hdr_len = ETH_HLEN;
+		if (pkt_hdr->bits.vlan || pkt_hdr->bits.llcsnap)
+			hdr_len += 4;
+#else
+		int hdr_len = 64;
+#endif
+
+		skb = nxge_rx_replace_page(nep, channel,
+								   rx_msg_p, rx_rbr_p,
+								   msg_index, *first_skbp);
+		if (skb == NULL) {
+			rdc_stats->rx_no_buf++;
+			NXGE_DEBUG_MSG((nep, RX_CTL,
+							"nxge_receive_packet: "
+							"skb alloc failed M1"));
+			if (bfree == B_TRUE) {
+				pci_dma_sync_single_for_device(nep->pdev,
+											   pkt_buf_addr_pp,
+											   bsize,
+											   PCI_DMA_FROMDEVICE);
+				nxge_post_buf_blk(nep, rx_rbr_p, rx_msg_p);
+			}
+			return;
+		}
+
+		skb->dev = nep->dev;
+
+		skb_reserve(skb, NET_IP_ALIGN);
+		fragp = &skb_shinfo(skb)->frags[0];
+		NXGE_MAP_KERN_PAGE(fragp->page);
+		memcpy(skb_put(skb, hdr_len), pkt_data_addr_p, hdr_len);
+		NXGE_UNMAP_KERN_PAGE(fragp->page);
+		NXGE_RX_CHECKSUM_OK(skb, hw_cksum);
+		skb->len = l2_len;
+		NXGE_ADD_SKB_TRUESIZE(skb, l2_len);
+		skb->data_len = l2_len - hdr_len;
+		fragp->page_offset = (hdr_size + sw_offset_bytes + hdr_len);
+		fragp->size = pkt_data_size - hdr_len;
+		skb_shinfo(skb)->nr_frags = 1;
+
+		*first_skbp = skb; /* the main skb */
+		*multi_bytes_remain = l2_len - pkt_data_size;
+
+	} else if (multi_pkt_type == NXGE_RX_MULTI_PKT_OTHER) {
+		/* add skb data to tail of first-skb */
+
+		skb_frag_t *fragp;
+		if (*first_skbp == NULL) {
+			NXGE_DEBUG_MSG((nep, RX_CTL,
+							"nxge_receive_packet: "
+							"skb alloc failed M1_1"));
+			pci_dma_sync_single_for_device(nep->pdev,
+										   pkt_buf_addr_pp,
+										   bsize,
+										   PCI_DMA_FROMDEVICE);
+			nxge_post_buf_blk(nep, rx_rbr_p, rx_msg_p);
+			return;
+		}
+
+		skb = nxge_rx_replace_page(nep, channel,
+					   rx_msg_p, rx_rbr_p, msg_index, *first_skbp);
+		if (skb == NULL) {
+			rdc_stats->rx_no_buf++;
+			NXGE_DEBUG_MSG((nep, RX_CTL,
+							"nxge_receive_packet: "
+							"skb alloc failed MO_1"));
+
+			if (bfree == B_TRUE) {
+				pci_dma_sync_single_for_device(nep->pdev,
+											   pkt_buf_addr_pp,
+											   bsize,
+											   PCI_DMA_FROMDEVICE);
+				nxge_post_buf_blk(nep, rx_rbr_p, rx_msg_p);
+			}
+
+			if (*first_skbp != NULL) {
+					dev_kfree_skb_any(*first_skbp);
+					*first_skbp = NULL;
+			}
+			return;
+		}
+
+		fragp = &skb_shinfo(skb)->frags[skb_shinfo(skb)->nr_frags];
+		fragp->page_offset = (hdr_size + sw_offset_bytes);
+		fragp->size = pkt_data_size;
+		skb_shinfo(skb)->nr_frags++;
+		*multi_bytes_remain -= pkt_data_size;
+	}  else if (multi_pkt_type == NXGE_RX_MULTI_PKT_LAST) {
+		skb_frag_t *fragp;
+		if (*first_skbp == NULL) {
+			NXGE_DEBUG_MSG((nep, RX_CTL,
+							"nxge_receive_packet: "
+							"skb alloc failed ML_1"));
+
+			pci_dma_sync_single_for_device(nep->pdev,
+										   pkt_buf_addr_pp,
+										   bsize,
+										   PCI_DMA_FROMDEVICE);
+			nxge_post_buf_blk(nep, rx_rbr_p, rx_msg_p);
+			return;
+		}
+
+		skb = nxge_rx_replace_page(nep, channel,
+					   rx_msg_p, rx_rbr_p, msg_index, *first_skbp);
+		if (skb == NULL) {
+			rdc_stats->rx_no_buf++;
+			NXGE_DEBUG_MSG((nep, RX_CTL,
+							"nxge_receive_packet: "
+							"skb alloc failed ML_0"));
+
+			if (bfree == B_TRUE) {
+				pci_dma_sync_single_for_device(nep->pdev,
+											   pkt_buf_addr_pp,
+											   bsize,
+											   PCI_DMA_FROMDEVICE);
+				nxge_post_buf_blk(nep, rx_rbr_p, rx_msg_p);
+			}
+
+			if (*first_skbp != NULL) {
+					dev_kfree_skb_any(*first_skbp);
+					*first_skbp = NULL;
+			}
+			return;
+		}
+
+		fragp = &skb_shinfo(skb)->frags[skb_shinfo(skb)->nr_frags];
+		pkt_data_size = *multi_bytes_remain;
+		if (pkt_data_size > 0) {
+			fragp->page_offset = (hdr_size + sw_offset_bytes);
+			fragp->size = pkt_data_size;
+			skb_shinfo(skb)->nr_frags++;
+		} else {
+			__free_pages(fragp->page, nep->page_order);
+			fragp->page = NULL;
+		}
+		skb->protocol = eth_type_trans(skb, nep->dev);
+
+		if (l2_len_nocrcstrip >= 0x5EE)
+			rdc_stats->rx_jumbo_pkts++;
+
+		if (l2_len_nocrcstrip > nep->mac.maxframesize) {
+			dev_kfree_skb_any(skb);
+			rdc_stats->l2_err++;
+		} else {
+			nxge_skb_release(nep, skb);
+			rdc_stats->ipackets++;
+			rdc_stats->ibytes += l2_len;
+		}
+		*multi_bytes_remain = 0;
+	}
+
+	*multi_p = multi;
+	NXGE_DEBUG_MSG((nep, RX_CTL, "<== nxge_receive_packet"));
+}
+
+
+static nxge_status_t
+nxge_rx_err_evnts(p_nxge_t nxgep, uint_t index, p_nxge_ldv_t ldvp,
+		  rx_dma_ctl_stat_t cs)
+{
+	p_nxge_rx_ring_stats_t	rdc_stats;
+	npi_handle_t		handle;
+	npi_status_t		rs;
+	boolean_t		rxchan_fatal = B_FALSE;
+	boolean_t		rxport_fatal = B_FALSE;
+	uint8_t			channel;
+	uint8_t			portn;
+	nxge_status_t		status = NXGE_OK;
+
+	int			rxchan_fe_num = 0;
+
+	NXGE_DEBUG_MSG((nxgep, RX_CTL, "==> nxge_rx_err_evnts"));
+
+	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	channel = ldvp->channel;
+	portn = nxgep->mac.portnum;
+	rdc_stats = &nxgep->statsp->rdc_stats[index];
+
+	if (cs.bits.hdw.rbr_tmout) {
+		rdc_stats->rx_rbr_tmout++;
+		if (rdc_stats->rx_rbr_tmout < RXDMA_MAX_ERR_SHOW )
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+					"nxge_rx_err_evnts: ch[%d] "
+					"RDMC_RBR_TMOUT ########## [%d]",
+					channel, rdc_stats->rx_rbr_tmout));
+		rxchan_fatal = B_TRUE;
+		rxchan_fe_num = 1;
+	}
+	if (cs.bits.hdw.rsp_cnt_err) {
+		rdc_stats->rsp_cnt_err++;
+		if (rdc_stats->rsp_cnt_err < RXDMA_MAX_ERR_SHOW)
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+					"nxge_rx_err_evnts: ch[%d] "
+					"RDMC_RSP_CNT_ERR ########## [%d]",
+					channel, rdc_stats->rsp_cnt_err));
+
+		rxchan_fatal = B_TRUE;
+		rxchan_fe_num = 2;
+	}
+	if (cs.bits.hdw.byte_en_bus) {
+		rdc_stats->byte_en_bus++;
+		if (rdc_stats->byte_en_bus < RXDMA_MAX_ERR_SHOW)
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+					"nxge_rx_err_evnts: ch[%d] "
+					"RDMC_BYTE_EN_BUS ########## [%d]",
+					channel, rdc_stats->byte_en_bus));
+		rxchan_fatal = B_TRUE;
+		rxchan_fe_num = 3;
+	}
+	if (cs.bits.hdw.rsp_dat_err) {
+		rdc_stats->rsp_dat_err++;
+		if (rdc_stats->rsp_dat_err < RXDMA_MAX_ERR_SHOW)
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+					"nxge_rx_err_evnts: ch[%d] "
+					"RDMC_RSP_DAT_ERR ########## [%d]",
+					channel, rdc_stats->rsp_dat_err));
+		rxchan_fatal = B_TRUE;
+		rxchan_fe_num = 4;
+	}
+	if (cs.bits.hdw.rcr_ack_err) {
+		rdc_stats->rcr_ack_err++;
+		if (rdc_stats->rcr_ack_err < RXDMA_MAX_ERR_SHOW)
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+					"nxge_rx_err_evnts: ch[%d] "
+					"RDMC_RCR_ACK_ERR ########## [%d]",
+					channel, rdc_stats->rcr_ack_err));
+		rxchan_fatal = B_TRUE;
+		rxchan_fe_num = 5;
+	}
+	if (cs.bits.hdw.dc_fifo_err) {
+		rdc_stats->dc_fifo_err++;
+		if (rdc_stats->dc_fifo_err < RXDMA_MAX_ERR_SHOW)
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+					"WARNING: nxge_rx_err_evnts: ch[%d] "
+					"RDMC_DC_FIFO_ERR ########## [%d]",
+					channel, rdc_stats->dc_fifo_err));
+	}
+	if ((cs.bits.hdw.rcr_sha_par) || (cs.bits.hdw.rbr_pre_par)) {
+		if ((rs = npi_rxdma_ring_perr_stat_get(handle,
+				&rdc_stats->errlog.pre_par,
+				&rdc_stats->errlog.sha_par))
+				!= NPI_SUCCESS) {
+			return (NXGE_ERROR | rs);
+		}
+		if (cs.bits.hdw.rcr_sha_par) {
+			rdc_stats->rcr_sha_par++;
+			if (rdc_stats->rcr_sha_par < RXDMA_MAX_ERR_SHOW)
+				NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+						"nxge_rx_err_evnts: ch[%d] "
+						"RDMC_RCR_SHA_PAR ########## [%d]",
+						channel,
+						rdc_stats->rcr_sha_par));
+			rxchan_fatal = B_TRUE;
+			rxchan_fe_num = 6;
+		}
+		if (cs.bits.hdw.rbr_pre_par) {
+			rdc_stats->rbr_pre_par++;
+			if (rdc_stats->rbr_pre_par < RXDMA_MAX_ERR_SHOW)
+				NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+						"nxge_rx_err_evnts: ch[%d] "
+						"RDMC_RBR_PRE_PAR ########## [%d]",
+						channel,
+						rdc_stats->rbr_pre_par));
+			rxchan_fatal = B_TRUE;
+			rxchan_fe_num = 7;
+		}
+	}
+	if (cs.bits.hdw.port_drop_pkt) {
+		uint64_t value = 1;
+		RXDMA_REG_READ64(handle, RXMISC_DISCARD_REG, channel, &value);
+		RXDMA_REG_WRITE64(handle, RXMISC_DISCARD_REG, channel, 0x0ull);
+		rdc_stats->port_drop_pkt += value;
+		if (rdc_stats->port_drop_pkt < RXDMA_MAX_ERR_SHOW)
+			NXGE_DEBUG_MSG((nxgep, NXGE_ERR_CTL,
+					"WARNING: nxge_rx_err_evnts: "
+					"ch[%d] RDMC_PORT_DROP_PKT ########## [%d]",
+					channel, rdc_stats->port_drop_pkt));
+	}
+	if (cs.bits.hdw.wred_drop) {
+		uint64_t value = 1;
+		RXDMA_REG_READ64(handle, RED_DIS_CNT_REG, channel, &value);
+		RXDMA_REG_WRITE64(handle, RED_DIS_CNT_REG, channel, 0x0ull);
+		rdc_stats->wred_drop += value;
+		if (rdc_stats->wred_drop < RXDMA_MAX_ERR_SHOW)
+			NXGE_DEBUG_MSG((nxgep, NXGE_ERR_CTL,
+					"WARNING: nxge_rx_err_evnts: "
+					"ch[%d] RDMC_WRED_DROP ########## [%d]",
+					channel, rdc_stats->wred_drop));
+	}
+	if (cs.bits.hdw.rbr_pre_empty) {
+		rdc_stats->rbr_pre_empty++;
+		if (rdc_stats->rbr_pre_empty < RXDMA_MAX_ERR_SHOW)
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+					"WARNING: nxge_rx_err_evnts: "
+					"ch[%d] RDMC_RBR_PRE_EMPTY ########## [%d]",
+					channel, rdc_stats->rbr_pre_empty));
+	}
+	if (cs.bits.hdw.rcr_shadow_full) {
+		rdc_stats->rcr_shadow_full++;
+		if (rdc_stats->rcr_shadow_full < RXDMA_MAX_ERR_SHOW)
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+					"WARNING: nxge_rx_err_evnts: "
+					"ch[%d] RDMC_RCR_SHADOW_FULL ########## [%d]",
+					channel, rdc_stats->rcr_shadow_full));
+	}
+	if (cs.bits.hdw.config_err) {
+		rdc_stats->config_err++;
+		if (rdc_stats->config_err < RXDMA_MAX_ERR_SHOW)
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+					"nxge_rx_err_evnts: ch[%d] "
+					"RDMC_CFG_ERR ########## [%d]",
+					channel, rdc_stats->config_err));
+		rxchan_fatal = B_TRUE;
+		rxchan_fe_num = 8;
+	}
+	if (cs.bits.hdw.rcrincon) {
+		rdc_stats->rcrincon++;
+		if (rdc_stats->rcrincon < RXDMA_MAX_ERR_SHOW)
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+					"nxge_rx_err_evnts: "
+					"ch[%d] RDMC_RCRINCON ########## [%d]",
+					channel, rdc_stats->rcrincon));
+		rxchan_fatal = B_TRUE;
+		rxchan_fe_num = 9;
+	}
+	if (cs.bits.hdw.rcrfull) {
+		rdc_stats->rcrfull++;
+		if (rdc_stats->rcrfull < RXDMA_MAX_ERR_SHOW)
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+					"WARNING: nxge_rx_err_evnts: "
+					"ch[%d] RDMC_RCRFULL ########## [%d]",
+					channel, rdc_stats->rcrfull));
+	}
+	if (cs.bits.hdw.rbr_empty) {
+		rdc_stats->rbr_empty++;
+		if (rdc_stats->rbr_empty < RXDMA_MAX_ERR_SHOW)
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+					"WARNING: nxge_rx_err_evnts: "
+					"ch[%d] RDMC_RBREMPTY ########## [%d] ",
+					channel, rdc_stats->rbr_empty));
+	}
+	if (cs.bits.hdw.rbrfull) {
+		rdc_stats->rbrfull++;
+		if (rdc_stats->rbrfull < RXDMA_MAX_ERR_SHOW)
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+					"nxge_rx_err_evnts: "
+					"ch[%d] RDMC_RBRFULL ########## [%d]",
+					channel, rdc_stats->rbrfull));
+		rxchan_fatal = B_TRUE;
+		rxchan_fe_num = 11;
+	}
+	if (cs.bits.hdw.rbrlogpage) {
+		rdc_stats->rbrlogpage++;
+		if (rdc_stats->rbrlogpage < RXDMA_MAX_ERR_SHOW)
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+					"nxge_rx_err_evnts: "
+					"ch[%d] RDMC_RBRLOGPAGE ########## [%d]",
+					channel, rdc_stats->rbrlogpage));
+		rxchan_fatal = B_TRUE;
+		rxchan_fe_num = 12;
+	}
+	if (cs.bits.hdw.cfiglogpage) {
+		rdc_stats->cfiglogpage++;
+		if (rdc_stats->cfiglogpage < RXDMA_MAX_ERR_SHOW)
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+					"nxge_rx_err_evnts: "
+					"ch[%d] RDMC_CFGLOGPAGE ########## [%d]",
+					channel, rdc_stats->cfiglogpage));
+		rxchan_fatal = B_TRUE;
+		rxchan_fe_num = 13;
+	}
+
+	if (rxport_fatal)  {
+		/* XXX: Need to reset rx port here */
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				" nxge_rx_err_evnts: "
+				" fatal error on Port #%d ",
+				portn));
+
+	} else if (rxchan_fatal) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				" nxge_rx_err_evnts: "
+				" fatal error on Channel #%d err #%d\n",
+				channel, rxchan_fe_num));
+
+		/* XXX: Need to reset rxdma channel here */
+		status = nxge_rxdma_fatal_err_recover(nxgep, channel);
+
+	}
+
+	NXGE_DEBUG_MSG((nxgep, RX_CTL, "<== nxge_rx_err_evnts"));
+
+
+	return (status);
+
+}
+
+static int nxge_rx_pkts_ring(p_nxge_t nep, uint_t vindex, int budget,
+							 int *npkts_p, rx_dma_ctl_stat_t cs)
+{
+	npi_handle_t		handle;
+	uint8_t			channel;
+	p_rx_rcr_rings_t	rx_rcr_rings;
+	p_rx_rcr_ring_t		rcr_p;
+	uint32_t		comp_rd_index;
+	uint16_t		qlen,  nrcr_read, npkt_read;
+	uint32_t qlen_hw;
+	struct sk_buff		*first_skb = NULL;
+	int multi_bytes_remain = 0;
+	boolean_t		multi;
+	int			status = NXGE_OK;
+	uint64_t rcr_cfg_b = 0x0ull;
+	uint64_t *rcr_addr;
+	uint64_t *rcr_entry;
+
+	handle = nep->npi_handle;
+	rx_rcr_rings = nep->rx_rcr_rings;
+	rcr_p = rx_rcr_rings->rings[vindex];
+
+
+	channel = rcr_p->rdc;
+	qlen = RXDMA_REG_READ32(handle, RCRSTAT_A_REG, channel) & 0xffff;
+
+	if ((qlen == 0) || (qlen > rcr_p->ring_size)) {
+		*npkts_p = 0;
+		NXGE_DEBUG_MSG((nep, RX_CTL, "rdc %d qlen out of bound %d",
+						channel, qlen));
+
+		return (NXGE_ERROR);
+	}
+
+
+
+#ifndef USE_NAPI
+	MUTEX_ENTER(&rcr_p->lock);
+#endif
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "nxge_rx_pkts_ring: START: "
+			"rcr channel %d rcrdesc->cpuaddr $%p  "
+			"rcrdesc->dma_addr $%p ",
+			channel, rcr_p->rcr_desc.cpu_addr,
+			rcr_p->rcr_desc.dma_addr));
+
+
+	nrcr_read = npkt_read = 0;
+	comp_rd_index = rcr_p->out;
+
+	rcr_addr = (uint64_t *)rcr_p->cpu_addr;
+	rcr_entry = rcr_addr + comp_rd_index;
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "==> nxge_rx_pkts_ring: rcr channel %d "
+		"qlen %d head_pp $%p tail $%p",
+		channel, qlen,
+		rcr_desc_rd_pp,
+		rcr_desc_wt_p));
+
+	/*
+	 * Number of packets queued
+	 * (The jumbo or multi packet will be counted as only one
+	 *  packet and it may take up more than one completion entry).
+	 */
+
+#ifndef USE_NAPI
+	qlen_hw = (qlen < rcr_p->max_receive_pkts) ?
+		qlen : rcr_p->max_receive_pkts;
+#else
+	qlen_hw = (qlen < budget) ? qlen : budget;
+#endif
+
+#ifdef USE_DYNAMIC_BLANKING
+	rcr_cfg_b = (qlen_hw << 16) | nxge_rx_intr_timeout;
+#endif
+
+	multi = B_FALSE;
+	first_skb = NULL;
+
+	while (qlen_hw) {
+		/*
+		 * Process one completion ring entry.
+		 */
+		nxge_receive_packet(nep, rcr_p, *rcr_entry,
+					&multi, &first_skb, &multi_bytes_remain, comp_rd_index);
+
+		if (multi == B_FALSE) {
+			qlen_hw--;
+			npkt_read++;
+			first_skb = NULL;
+			multi_bytes_remain = 0;
+		}
+
+		/*
+		 * Update the next read entry.
+		 */
+		comp_rd_index = DESC_NEXT_IDX(rcr_p, comp_rd_index);
+
+		rcr_entry = rcr_addr + comp_rd_index;
+		nrcr_read++;
+	}
+
+	*npkts_p = npkt_read;
+
+	/*
+	 * Update RCR buffer pointer read and number of packets
+	 * read.
+	 */
+	rcr_p->out = comp_rd_index;
+#ifdef USE_DYNAMIC_BLANKING
+	RXDMA_REG_WRITE64(handle, RCRCFIG_B_REG,
+			    channel, rcr_cfg_b);
+#else
+
+#ifdef RX_INTR_BLANKING_LDG
+	if (nep->intr_thresh != rcr_p->intr_thresh) {
+		rcr_p->intr_thresh = nep->intr_thresh;
+		rcr_cfg_b =	RCRCFIG_B_TIMEOUT | RXDMA_RCR_TO_DEFAULT |
+			(rcr_p->intr_thresh << 16);
+#else
+	if ((nep->intr_timeout != rcr_p->intr_timeout) ||
+		(nep->intr_thresh != rcr_p->intr_thresh)) {
+/*
+ * Interrupt parameters have changed.
+ * Update the HW with the new values.
+ */
+		rcr_p->intr_thresh = nep->intr_thresh;
+		rcr_p->intr_timeout = nep->intr_timeout;
+		rcr_cfg_b = rcr_p->intr_timeout | RCRCFIG_B_TIMEOUT |
+			(rcr_p->intr_thresh << 16);
+
+#endif
+		RXDMA_REG_WRITE64(handle, RCRCFIG_B_REG,
+						  channel, rcr_cfg_b);
+	}
+#endif
+
+	cs.bits.ldw.pktread = npkt_read;
+	cs.bits.ldw.ptrread = nrcr_read;
+	RXDMA_REG_WRITE64(handle, RX_DMA_CTL_STAT_REG,
+			    channel, cs.value);
+
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "nxge_rx_pkts_ring: EXIT: "
+			"rcr channel %d head_pp $%p  index 0x%llx pkts_read (%d)",
+			channel, rcr_desc_rd_pp, comp_rd_index, npkt_read));
+#ifndef USE_NAPI
+	MUTEX_EXIT(&rcr_p->lock);
+#endif
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "<== nxge_rx_pkts_ring"));
+	return (status);
+}
+
+
+#ifdef USE_NAPI
+
+static
+int nxge_poll(struct net_device *dev, int *budget)
+{
+
+	p_nxge_t		nep;
+	p_nxge_ldv_t		ldvp;
+	uint8_t			ldv;
+	npi_handle_t		handle;
+	p_nxge_ldgv_t		ldgvp = NULL;
+	p_nxge_ldg_t		ldgp = NULL;
+	p_nxge_ldg_t		t_ldgp = NULL;
+	p_nxge_ldv_t		t_ldvp = NULL;
+	uint64_t		vector0 = 0, vector1 = 0, vector2 = 0;
+	int			i, j, nldvs, nintrs = 1;
+	irqreturn_t 		serviced1;
+	npi_status_t		rs = NPI_SUCCESS;
+	int debit = 0;
+	int rdc_budget = 1;
+	uint8_t channel;
+	int npkts_read = 0;
+	ldgimgm_t		mgm;
+	boolean_t credit = B_TRUE;
+	boolean_t more_work = B_FALSE;
+	rx_dma_ctl_stat_t	cs;
+	nxge_ldv_t		i_ldvp;
+
+	nep = netdev_priv(dev);
+
+	if (nep == NULL) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+				"<== nxge_intr: not initialized - nxge struct not set "));
+
+		return (0);
+	}
+
+
+	if (nep->ldgvp == NULL) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+				"<== nxge_intr: Not ready - nep->ldgvp NULL"));
+		return (0);
+	}
+
+	ldgvp = nep->ldgvp;
+	t_ldvp = ldvp = ldgvp->ldvp;
+
+	if (ldvp) {
+		ldgp = t_ldgp = ldvp->ldgp;
+	}
+
+
+
+	if (*budget > nep->nrdc)
+		rdc_budget = *budget / nep->nrdc;
+
+	handle = nep->npi_handle;
+
+	nldvs = ldgp->nldvs;
+	t_ldgp = ldgp;
+	t_ldvp = ldgp->ldvp;
+	nldvs = t_ldgp->nldvs;
+	for (j = 0; j < nldvs; j++, t_ldvp++) {
+		ldv = t_ldvp->ldv;
+		i_ldvp.intr_hdlr_arg.dev = nep->dev;
+		i_ldvp.intr_hdlr_arg.ldvp = t_ldvp;
+		if (t_ldvp->is_rxdma) {
+			if (credit == B_TRUE) {
+				channel = t_ldvp->channel;
+				npkts_read = 0;
+				t_ldvp->budget = rdc_budget;
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,18)
+				serviced1 = (t_ldvp->ldv_intr_handler)(0,
+							   (void *) &i_ldvp.intr_hdlr_arg);
+#else
+				serviced1 = (t_ldvp->ldv_intr_handler)(0,
+									   (void *) &i_ldvp.intr_hdlr_arg,
+						       0);
+#endif
+				debit += t_ldvp->budget;
+				if (debit >= *budget) {
+					credit = B_FALSE;
+					more_work = B_TRUE;
+				}
+			}
+		} else {
+			if (nxge_is_ldf_set(ldv, vector0, vector1, vector2)) {
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,18)
+			serviced1 = (t_ldvp->ldv_intr_handler)(0,
+							   (void *) &i_ldvp.intr_hdlr_arg);
+#else
+			serviced1 = (t_ldvp->ldv_intr_handler)(0,
+							   (void *) &i_ldvp.intr_hdlr_arg,
+						       0);
+#endif
+			}
+		}
+
+	}
+
+
+	*budget    -= debit;
+	dev->quota -= debit;
+	if (more_work == B_TRUE) {
+		return (1);
+	}
+
+	netif_rx_complete(dev);
+
+		/* rearm group interrupts */
+	mgm.value = 0x0ull;
+	mgm.bits.ldw.arm = 1;
+	mgm.bits.ldw.timer = NXGE_TIMER_LDG;
+	NXGE_REG_WR64(handle, LDGIMGN_REG +
+					  LDSV_OFFSET(ldgp->ldg), mgm.value);
+
+	NXGE_DEBUG_MSG((nep, INT_CTL,  "<== nxge_intr"));
+	return (0);
+}
+
+#endif
+
+
+/***********************************************************************
+ *                 General DMA Functions
+ *
+ **********************************************************************/
+
+static int nxge_free_cntl_pool(p_nxge_t nep)
+{
+	int	status;
+
+	NXGE_DEBUG_MSG((nep, DMA_CTL, "==> nxge_free_cntl_pool"));
+
+	status = nxge_free_rx_cntl_pool(nep);
+	if (status != NXGE_OK) {
+		return (status);
+	}
+
+	status = nxge_free_tx_cntl_pool(nep);
+
+	NXGE_DEBUG_MSG((nep, DMA_CTL, "<== nxge_free_cntl_pool"));
+	return (status);
+}
+
+static int nxge_alloc_cntl_pool(p_nxge_t nep)
+{
+	int	status;
+
+	NXGE_DEBUG_MSG((nep, DMA_CTL, "==> nxge_alloc_cntl_pool"));
+
+	status = nxge_alloc_rx_cntl_pool(nep);
+	if (status != NXGE_OK) {
+		return (status);
+	}
+
+	status = nxge_alloc_tx_cntl_pool(nep);
+	if (status != NXGE_OK) {
+		nxge_free_rx_cntl_pool(nep);
+	}
+
+	NXGE_DEBUG_MSG((nep, DMA_CTL, "<== nxge_alloc_cntl_pool"));
+
+	return (status);
+}
+
+static void nxge_setup_system_dma_pages(p_nxge_t nep)
+{
+#ifdef USE_BTREE
+	struct page	*page;
+	uint32_t	order;
+#endif
+
+	NXGE_DEBUG_MSG((nep, DMA_CTL, "==> nxge_setup_system_dma_pages"));
+
+	/* get page size for rx buffers. */
+	nep->page_order = 0;
+
+
+#ifdef USE_BTREE
+	if (PAGE_SHIFT < NXGE_MAX_CONT_BYTES_SHIFT) {
+		/* see if we can allocate larger pages */
+		order = NXGE_MAX_CONT_BYTES_SHIFT - PAGE_SHIFT;
+		page = alloc_pages(GFP_ATOMIC, order);
+		if (page) {
+			__free_pages(page, order);
+			nep->page_order = order;
+		}
+	}
+#endif
+
+	nep->page_size = (PAGE_SIZE << nep->page_order);
+
+	switch (nep->page_size) {
+
+	case 0x1000:
+		nep->rx_default_block_size = 0x1000;
+		nep->rx_bksize_code = RBR_BKSIZE_4K;
+		break;
+	case 0x2000:
+		nep->rx_default_block_size = 0x2000;
+		nep->rx_bksize_code = RBR_BKSIZE_8K;
+		break;
+	case 0x4000:
+		nep->rx_default_block_size = 0x4000;
+		nep->rx_bksize_code = RBR_BKSIZE_16K;
+		break;
+	case 0x8000:
+		nep->rx_default_block_size = 0x8000;
+		nep->rx_bksize_code = RBR_BKSIZE_32K;
+		break;
+	default:
+	  /* should we set page size to 4k and page order accordingly */
+		if (nep->page_size > 0x8000) {
+			nep->rx_default_block_size = 0x8000;
+			nep->rx_bksize_code = RBR_BKSIZE_32K;
+		} else {
+			nep->rx_default_block_size = 0x1000;
+			nep->rx_bksize_code = RBR_BKSIZE_4K;
+		}
+		break;
+	}
+
+	NXGE_DEBUG_MSG((nep, DMA_CTL, "nxge_setup_system_dma_pages: "
+			"Page order %d, page size 0x%x, "
+			"Rx default blk size 0x%x", nep->page_order,
+			nep->page_size, nep->rx_default_block_size));
+
+	NXGE_DEBUG_MSG((nep, DMA_CTL, "<== nxge_setup_system_dma_pages"));
+	return;
+}
+
+
+/***********************************************************************
+ *                 Interrupt related Functions
+ *
+ **********************************************************************/
+
+
+static int
+nxge_intr_mask_mgmt_set(p_nxge_t nxgep, boolean_t on)
+{
+	p_nxge_ldgv_t	ldgvp;
+	p_nxge_ldg_t	ldgp;
+	p_nxge_ldv_t	ldvp;
+	npi_handle_t	handle;
+	int		i, j;
+	int		status = NXGE_OK;
+
+	NXGE_DEBUG_MSG((nxgep, INT_CTL,
+		"==> nxge_intr_mask_mgmt_set (%d)", on));
+
+	if ((ldgvp = nxgep->ldgvp) == NULL) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"<== nxge_intr_mask_mgmt_set: Null ldgvp"));
+		return (NXGE_ERROR);
+	}
+	handle = nxgep->npi_handle;
+	ldgp = ldgvp->ldgp;
+	ldvp = ldgvp->ldvp;
+	if (ldgp == NULL || ldvp == NULL) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"<== nxge_intr_mask_mgmt_set: Null ldgp or ldvp"));
+		return (NXGE_ERROR);
+	}
+
+	/* set masks. */
+	for (i = 0; i < ldgvp->ldg_intrs; i++, ldgp++) {
+		NXGE_DEBUG_MSG((nxgep, INT_CTL,
+			"nxge_intr_mask_mgmt_set: flag %d ldg %d"
+			"set mask nldvs %d", on, ldgp->ldg, ldgp->nldvs));
+		for (j = 0; j < ldgp->nldvs; j++, ldvp++) {
+			NXGE_DEBUG_MSG((nxgep, INT_CTL,
+				"nxge_intr_mask_mgmt_set: "
+				"for %d %d flag %d", i, j, on));
+			if (on == B_TRUE) {
+				ldvp->ldv_ldf_masks = 0;
+			} else {
+				ldvp->ldv_ldf_masks =
+					(uint8_t)LD_IM1_MASK;
+				NXGE_DEBUG_MSG((nxgep, INT_CTL,
+					"nxge_intr_mask_mgmt_set:mask on"));
+			}
+			status = npi_intr_mask_set(handle, ldvp->ldv,
+					ldvp->ldv_ldf_masks);
+			if (status) {
+				NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"<== nxge_intr_mask_mgmt_set: "
+				"set mask failed "
+				" status 0x%x ldv %d mask 0x%x",
+				status, ldvp->ldv, ldvp->ldv_ldf_masks));
+				return (NXGE_ERROR);
+			}
+			NXGE_DEBUG_MSG((nxgep, INT_CTL,
+				"nxge_intr_mask_mgmt_set: flag %d"
+				" set mask OK "
+				" ldv %d mask 0x%x",
+				on, ldvp->ldv, ldvp->ldv_ldf_masks));
+		}
+	}
+
+	ldgp = ldgvp->ldgp;
+	/* set the arm bit */
+	for (i = 0; i < nxgep->ldgvp->ldg_intrs; i++, ldgp++) {
+		if (on == B_TRUE) {
+			ldgp->arm = B_TRUE;
+		} else {
+			ldgp->arm = B_FALSE;
+		}
+		status = npi_intr_ldg_mgmt_set(handle, ldgp->ldg,
+				ldgp->arm, ldgp->ldg_timer);
+		if (status) {
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"<== nxge_intr_mask_mgmt_set: "
+				"set timer failed "
+				" status 0x%x ldg %d timer 0x%x arm %d",
+				status, ldgp->ldg, ldgp->ldg_timer,
+				ldgp->arm));
+			return (NXGE_ERROR);
+		}
+		NXGE_DEBUG_MSG((nxgep, INT_CTL,
+			"nxge_intr_mask_mgmt_set: OK (flag %d) "
+			"set timer "
+			" ldg %d timer 0x%x arm %d",
+			on, ldgp->ldg, ldgp->ldg_timer, ldgp->arm));
+	}
+
+	NXGE_DEBUG_MSG((nxgep, INT_CTL, "<== nxge_intr_mask_mgmt_set"));
+	return (status);
+}
+
+
+static void
+nxge_intr_hw_disable(p_nxge_t nep)
+{
+	NXGE_DEBUG_MSG((nep, INT_CTL, "==> nxge_intr_hw_disable"));
+
+	(void) nxge_intr_mask_mgmt_set(nep, B_FALSE);
+
+	NXGE_DEBUG_MSG((nep, INT_CTL, "<== nxge_intr_hw_disable"));
+}
+
+
+/* static irqreturn_t nxge_ld_grp_intr(int irq, void *dev_id, */
+/* 				    struct pt_regs *regs) */
+/* { */
+/* 	p_nxge_sys_intr_arg_t	argp = (p_nxge_sys_intr_arg_t)dev_id; */
+/* 	struct net_device	*dev = argp->dev; */
+/* 	p_nxge_t		nep = netdev_priv(dev); */
+/* 	p_nxge_ldg_t		ldgp = argp->ldgp; */
+
+
+/* 	return IRQ_HANDLED; */
+/* } */
+
+#define     LDV_ON(ldv, vector)     ((vector >> ldv) & 0x1)
+#define	LDV2_ON_1(ldv, vector)	((vector >> (ldv - 64)) & 0x1)
+#define	LDV2_ON_2(ldv, vector)	(((vector >> 5) >> (ldv - 64)) & 0x1)
+
+static inline boolean_t
+nxge_is_ldf_set(uint8_t ldv, uint64_t v0, uint64_t v1, uint64_t v2)
+{
+#if 0
+	if (ldv < NXGE_MAC_LD_START) {
+		return (((1 << ldv) & v0) || ((1 << ldv) & v1));
+	} else {
+		return (((1 << (ldv - NXGE_MAC_LD_START)) & v2) ||
+			((1 << ((ldv - NXGE_MAC_LD_START) +
+				(NXGE_INT_MAX_LDS - NXGE_MAC_LD_START))) & v2));
+	}
+#else
+#if 0
+	return (((ldv < NXGE_MAC_LD_START) &&
+				(LDV_ON(ldv, v0) |
+					(LDV_ON(ldv, v1)))) ||
+				(LDV_ON(ldv, v2)));
+#endif
+	return ((((ldv < NXGE_MAC_LD_START) &&
+				(LDV_ON(ldv, v0) |
+					(LDV_ON(ldv, v1)))) ||
+				(ldv >= NXGE_MAC_LD_START &&
+				((LDV2_ON_1(ldv, v2)) ||
+				 (LDV2_ON_2(ldv, v2))))));
+#endif
+}
+
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,18)
+static irqreturn_t nxge_intr(int irq, void *info)
+#else
+static irqreturn_t nxge_intr(int irq, void *info, struct pt_regs *regs)
+#endif
+{
+
+	struct net_device	*dev;
+	p_nxge_t		nep;
+	p_nxge_ldv_t		ldvp;
+	irqreturn_t 		serviced = IRQ_NONE;
+	irqreturn_t 		serviced1 = IRQ_NONE;
+	uint8_t			ldv;
+	npi_handle_t		handle;
+	p_nxge_ldgv_t		ldgvp = NULL;
+	p_nxge_ldg_t		ldgp = NULL;
+	p_nxge_ldg_t		t_ldgp = NULL;
+	p_nxge_ldv_t		t_ldvp = NULL;
+	uint64_t		vector0 = 0, vector1 = 0, vector2 = 0;
+	int			i, j, nldvs, nintrs = 1;
+	npi_status_t		rs = NPI_SUCCESS;
+
+	NXGE_DEBUG_MSG((NULL, INT_CTL, "==> nxge_intr"));
+
+	dev = ((p_nxge_intr_arg_t)info)->dev;
+	if (dev == NULL) {
+		NXGE_ERROR_MSG((NULL, NXGE_ERR_CTL,
+			"<== nxge_intr: not initialized - netdev not set 0x%x",
+			serviced));
+		return (IRQ_NONE);
+	}
+	nep = netdev_priv(dev);
+	if (nep == NULL) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+			"<== nxge_intr: not initialized - nxge struct not set 0x%x",
+			serviced));
+		return (IRQ_NONE);
+	}
+
+
+	ldvp = ((p_nxge_intr_arg_t)info)->ldvp;
+
+	if (!(nep->drv_state & STATE_HW_INITIALIZED)) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+			"<== nxge_intr: not initialized 0x%x",
+			serviced));
+		return (IRQ_NONE);
+	}
+
+	if (nep->ldgvp == NULL) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+				"<== nxge_intr: Not ready - nep->ldgvp NULL"));
+		return (IRQ_NONE);
+	}
+
+	ldgvp = nep->ldgvp;
+
+	if (ldvp == NULL && ldgvp) {
+		t_ldvp = ldvp = ldgvp->ldvp;
+	}
+	if (ldvp) {
+		ldgp = t_ldgp = ldvp->ldgp;
+	}
+
+	if (ldvp == NULL || ldgp == NULL) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "==> nxge_intr: "
+			"ldgvp $%p ldvp $%p ldgp $%p",
+			ldgvp, ldvp, ldgp));
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+				"<== nxge_intr: not ready"));
+		return (IRQ_NONE);
+	}
+
+	/*
+	 * This interrupt handler will have to go through
+	 * all the logical devices to find out which
+	 * logical device interrupts us and then call
+	 * its handler to process the events.
+	 */
+	handle = nep->npi_handle;
+	nldvs = ldgp->nldvs;
+	t_ldgp = ldgp;
+	t_ldvp = ldgp->ldvp;
+
+	NXGE_DEBUG_MSG((nep, INT_CTL, "nxge_intr: #ldvs %d #intrs %d",
+	       nldvs, ldgvp->ldg_intrs));
+
+	for (i = 0; i < nintrs; i++, t_ldgp++) {
+		NXGE_DEBUG_MSG((nep, INT_CTL, "==> nxge_intr(%d): #ldvs %d "
+			" #intrs %d", i, nldvs, nintrs));
+
+		/*
+		 * Get this group's flag bits.
+		 */
+		t_ldgp->interrupted = B_FALSE;
+		rs = npi_ldsv_ldfs_get(handle, t_ldgp->ldg,
+				&vector0, &vector1, &vector2);
+
+		if (rs) {
+			NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+							" Vector read failed %x", rs));
+			continue;
+		}
+
+		if (!vector0 && !vector1 && !vector2) {
+			NXGE_DEBUG_MSG((nep, INT_CTL, "nxge_intr: "
+				       "no interrupts on group %d",
+				       t_ldgp->ldg));
+			continue;
+		}
+
+		NXGE_DEBUG_MSG((nep, INT_CTL, "==> nxge_intr: "
+				"vector0 0x%llx vector1 0x%llx vector2 0x%llx",
+				vector0, vector1, vector2));
+
+
+		t_ldgp->interrupted = B_TRUE;
+		serviced = IRQ_HANDLED;
+		nldvs = t_ldgp->nldvs;
+		for (j = 0; j < nldvs; j++, t_ldvp++) {
+			/*
+			 * Call device's handler if flag bits are on.
+			 */
+			ldv = t_ldvp->ldv;
+			if (nxge_is_ldf_set(ldv, vector0, vector1, vector2)) {
+				NXGE_DEBUG_MSG((nep, INT_CTL,
+					"nxge_intr: "
+					"calling device %d #ldvs %d #intrs %d",
+					j, nldvs, nintrs));
+				((p_nxge_intr_arg_t)info)->ldvp = t_ldvp;
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,18)
+				serviced1 = (t_ldvp->ldv_intr_handler)(irq, info);
+#else
+				serviced1 = (t_ldvp->ldv_intr_handler)(irq,
+								       info,
+								       regs);
+#endif
+
+				if (serviced1 != IRQ_HANDLED) {
+					NXGE_DEBUG_MSG((nep, INT_CTL,
+						       "nxge_intr: "
+						       "Intr not handled for"
+						       " ldv %d", ldv));
+				}
+			}
+		}
+	}
+
+	t_ldgp = ldgp;
+#if 1
+	for (i = 0; i < nintrs; i++, t_ldgp++) {
+		/* rearm group interrupts */
+		if (t_ldgp->interrupted) {
+			NXGE_DEBUG_MSG((nep, INT_CTL,  "nxge_intr: "
+				       "arm %d group %d", t_ldgp->arm,
+					t_ldgp->ldg));
+			npi_intr_ldg_mgmt_set(handle, t_ldgp->ldg,
+				t_ldgp->arm, t_ldgp->ldg_timer);
+		}
+	}
+#endif
+
+	NXGE_DEBUG_MSG((nep, INT_CTL,  "<== nxge_intr"));
+	return (serviced);
+}
+
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,18)
+static irqreturn_t nxge_intr_legacy(int irq, void *info)
+#else
+static irqreturn_t nxge_intr_legacy(int irq, void *info,
+								 struct pt_regs *regs)
+#endif
+{
+	struct net_device	*dev;
+	p_nxge_t		nep;
+	p_nxge_ldv_t		ldvp;
+	irqreturn_t 		serviced = IRQ_NONE;
+	irqreturn_t 		serviced1 = IRQ_NONE;
+	uint8_t			ldv;
+	npi_handle_t		handle;
+	p_nxge_ldgv_t		ldgvp = NULL;
+	p_nxge_ldg_t		ldgp = NULL;
+	p_nxge_ldg_t		t_ldgp = NULL;
+	p_nxge_ldv_t		t_ldvp = NULL;
+	uint64_t		vector0 = 0, vector1 = 0, vector2 = 0;
+	int			j, nldvs;
+	npi_status_t		rs = NPI_SUCCESS;
+	ldgimgm_t		mgm;
+	nxge_ldv_t		i_ldvp;
+
+	NXGE_DEBUG_MSG((NULL, INT_CTL, "==> nxge_intr"));
+
+	dev = (struct net_device *)info;
+	if (dev == NULL) {
+		NXGE_ERROR_MSG((NULL, NXGE_ERR_CTL,
+			"<== nxge_intr: not initialized - netdev not set 0x%x",
+			serviced));
+		return (IRQ_NONE);
+	}
+	nep = netdev_priv(dev);
+	if (nep == NULL) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+			"<== nxge_intr: not initialized - nxge struct not set 0x%x",
+			serviced));
+		return (IRQ_NONE);
+	}
+
+
+	if (!(nep->drv_state & STATE_HW_INITIALIZED)) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+			"<== nxge_intr: not initialized 0x%x",
+			serviced));
+		return (IRQ_NONE);
+	}
+
+	if (nep->ldgvp == NULL) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+				"<== nxge_intr: Not ready - nep->ldgvp NULL"));
+		return (IRQ_NONE);
+	}
+
+	ldgvp = nep->ldgvp;
+
+	t_ldvp = ldvp = ldgvp->ldvp;
+
+	if (ldvp) {
+		ldgp = t_ldgp = ldvp->ldgp;
+	}
+
+	if (ldvp == NULL || ldgp == NULL) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "==> nxge_intr: "
+			"ldgvp $%p ldvp $%p ldgp $%p",
+			ldgvp, ldvp, ldgp));
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+				"<== nxge_intr: not ready"));
+		return (IRQ_NONE);
+	}
+
+	/*
+	 * This interrupt handler will have to go through
+	 * all the logical devices to find out which
+	 * logical device interrupts us and then call
+	 * its handler to process the events.
+	 */
+	handle = nep->npi_handle;
+	nldvs = ldgp->nldvs;
+	t_ldgp = ldgp;
+	t_ldvp = ldgp->ldvp;
+
+	mgm.value = 0;
+	NXGE_REG_RD64(handle, LDGIMGN_REG +
+				  LDSV_OFFSET(ldgp->ldg), &mgm.value);
+
+	if (mgm.value) {
+		return (IRQ_NONE);
+	}
+
+	rs = npi_ldsv_ldfs_get(handle, t_ldgp->ldg,
+						   &vector0, &vector1, &vector2);
+
+	if (!vector0 && !vector1 && !vector2) {
+		NXGE_DEBUG_MSG((nep, INT_CTL, "nxge_intr: "
+						"no interrupts on group %d",
+						t_ldgp->ldg));
+		return (IRQ_NONE);
+	}
+
+#ifdef USE_NAPI
+	netif_rx_schedule(dev);
+	return (IRQ_HANDLED);
+#endif
+
+	serviced = IRQ_HANDLED;
+	nldvs = t_ldgp->nldvs;
+	for (j = 0; j < nldvs; j++, t_ldvp++) {
+			/*
+			 * Call device's handler if flag bits are on.
+			 */
+		ldv = t_ldvp->ldv;
+		if (nxge_is_ldf_set(ldv, vector0, vector1, vector2)) {
+			i_ldvp.intr_hdlr_arg.dev = nep->dev;
+			i_ldvp.intr_hdlr_arg.ldvp = t_ldvp;
+			((p_nxge_intr_arg_t)info)->ldvp = t_ldvp;
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,18)
+			serviced1 = (t_ldvp->ldv_intr_handler)(irq,
+							   (void *) &i_ldvp.intr_hdlr_arg);
+#else
+			serviced1 = (t_ldvp->ldv_intr_handler)(irq,
+							   (void *) &i_ldvp.intr_hdlr_arg,
+						       regs);
+#endif
+
+			if (serviced1 != IRQ_HANDLED) {
+				NXGE_DEBUG_MSG((nep, INT_CTL,
+								"nxge_intr: "
+								"Intr not handled for"
+								" ldv %d", ldv));
+			}
+		}
+	}
+
+
+	mgm.value = 0x0ull;
+	mgm.bits.ldw.arm = 1;
+	mgm.bits.ldw.timer = NXGE_TIMER_LDG;
+	NXGE_REG_WR64(handle, LDGIMGN_REG +
+					  LDSV_OFFSET(ldgp->ldg), mgm.value);
+
+	NXGE_DEBUG_MSG((nep, INT_CTL,  "<== nxge_intr"));
+	return (serviced);
+}
+
+
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,18)
+static irqreturn_t nxge_mif_intr(int irq, void *info)
+#else
+static irqreturn_t nxge_mif_intr(int irq, void *info,
+								 struct pt_regs *regs)
+#endif
+{
+	irqreturn_t 		serviced = IRQ_NONE;
+
+	NXGE_DEBUG_MSG((nep, INT_CTL, "==> nxge_mif_intr"));
+
+
+	NXGE_DEBUG_MSG((nep, INT_CTL, "<== nxge_mif_intr"));
+	return (serviced);
+}
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,18)
+static irqreturn_t nxge_mac_intr(int irq, void *info)
+#else
+static irqreturn_t nxge_mac_intr(int irq, void *info, struct pt_regs *regs)
+#endif
+{
+	struct net_device	*dev = ((p_nxge_intr_arg_t)info)->dev;
+	p_nxge_t		nep = netdev_priv(dev);
+	p_nxge_ldv_t		ldvp = ((p_nxge_intr_arg_t)info)->ldvp;
+	p_nxge_ldg_t		ldgp;
+	irqreturn_t 		serviced = IRQ_NONE;
+	npi_handle_t		handle = nep->npi_handle;
+	p_nxge_stats_t		statsp;
+	uint8_t			portn;
+	uint32_t		status;
+
+	ldgp = ldvp->ldgp;
+
+	NXGE_DEBUG_MSG((nep, MAC_CTL, "==> nxge_mac_intr"
+		"group %d", ldgp->ldg));
+
+	/*
+	 * This interrupt handler is for a specific
+	 * mac port.
+	 */
+	statsp = (p_nxge_stats_t)nep->statsp;
+	portn = nep->mac.portnum;
+
+	if (nep->mac.porttype == PORT_TYPE_XMAC) {
+		if (npi_xmac_tx_get_istatus(handle, portn,
+				(xmac_tx_iconfig_t *)&status) == NPI_SUCCESS) {
+/* 			printk(KERN_ERR "nxge_mac_intr: txistatus[0x%x]\n",
+			status); */
+				if (status & ICFG_XMAC_TX_UNDERRUN)
+					statsp->xmac_stats.tx_underflow_err++;
+				if (status & ICFG_XMAC_TX_MAX_PACKET_ERR)
+					statsp->xmac_stats.tx_maxpktsize_err++;
+				if (status & ICFG_XMAC_TX_OVERFLOW)
+					statsp->xmac_stats.tx_overflow_err++;
+				if (status & ICFG_XMAC_TX_FIFO_XFR_ERR)
+					statsp->xmac_stats.tx_fifo_xfr_err++;
+				if (status & ICFG_XMAC_TX_BYTE_CNT_EXP)
+					statsp->xmac_stats.tx_byte_cnt +=
+							XTXMAC_BYTE_CNT_MASK;
+				if (status & ICFG_XMAC_TX_FRAME_CNT_EXP)
+					statsp->xmac_stats.tx_frame_cnt +=
+							XTXMAC_FRM_CNT_MASK;
+		} else
+			goto nxge_mac_intr_fail;
+
+		if (npi_xmac_rx_get_istatus(handle, portn,
+				(xmac_rx_iconfig_t *)&status) == NPI_SUCCESS) {
+/* 			printk(KERN_ERR "nxge_mac_intr: rxistatus[0x%x]\n",
+			status);  */
+				if (status & ICFG_XMAC_RX_OVERFLOW)
+					statsp->xmac_stats.rx_overflow_err++;
+				if (status & ICFG_XMAC_RX_UNDERFLOW)
+					statsp->xmac_stats.rx_underflow_err++;
+				if (status & ICFG_XMAC_RX_CRC_ERR_CNT_EXP)
+					statsp->xmac_stats.rx_crc_err_cnt +=
+							XRXMAC_CRC_ER_CNT_MASK;
+				if (status & ICFG_XMAC_RX_LEN_ERR_CNT_EXP)
+					statsp->xmac_stats.rx_len_err_cnt +=
+							MAC_LEN_ER_CNT_MASK;
+				if (status & ICFG_XMAC_RX_VIOL_ERR_CNT_EXP)
+					statsp->xmac_stats.rx_viol_err_cnt +=
+							XRXMAC_CD_VIO_CNT_MASK;
+				if (status & ICFG_XMAC_RX_OCT_CNT_EXP)
+					statsp->xmac_stats.rx_byte_cnt +=
+							XRXMAC_BT_CNT_MASK;
+				if (status & ICFG_XMAC_RX_HST_CNT1_EXP)
+					statsp->xmac_stats.rx_hist1_cnt +=
+							XRXMAC_HIST_CNT1_MASK;
+				if (status & ICFG_XMAC_RX_HST_CNT2_EXP)
+					statsp->xmac_stats.rx_hist2_cnt +=
+							XRXMAC_HIST_CNT2_MASK;
+				if (status & ICFG_XMAC_RX_HST_CNT3_EXP)
+					statsp->xmac_stats.rx_hist3_cnt +=
+							XRXMAC_HIST_CNT3_MASK;
+				if (status & ICFG_XMAC_RX_HST_CNT4_EXP)
+					statsp->xmac_stats.rx_hist4_cnt +=
+							XRXMAC_HIST_CNT4_MASK;
+				if (status & ICFG_XMAC_RX_HST_CNT5_EXP)
+					statsp->xmac_stats.rx_hist5_cnt +=
+							XRXMAC_HIST_CNT5_MASK;
+				if (status & ICFG_XMAC_RX_HST_CNT6_EXP)
+					statsp->xmac_stats.rx_hist6_cnt +=
+							XRXMAC_HIST_CNT6_MASK;
+				if (status & ICFG_XMAC_RX_BCAST_CNT_EXP)
+					statsp->xmac_stats.rx_broadcast_cnt +=
+							XRXMAC_BC_FRM_CNT_MASK;
+				if (status & ICFG_XMAC_RX_MCAST_CNT_EXP)
+					statsp->xmac_stats.rx_mult_cnt +=
+							XRXMAC_MC_FRM_CNT_MASK;
+				if (status & ICFG_XMAC_RX_FRAG_CNT_EXP)
+					statsp->xmac_stats.rx_frag_cnt +=
+							XRXMAC_FRAG_CNT_MASK;
+				if (status & ICFG_XMAC_RX_ALIGNERR_CNT_EXP)
+					statsp->xmac_stats.rx_frame_align_err_cnt
+						+= XRXMAC_AL_ER_CNT_MASK;
+				if (status & ICFG_XMAC_RX_LINK_FLT_CNT_EXP)
+					statsp->xmac_stats.rx_linkfault_err_cnt
+						+= XMAC_LINK_FLT_CNT_MASK;
+				if (status & ICFG_XMAC_RX_REMOTE_FLT_DET)
+					statsp->xmac_stats.rx_remotefault_err++;
+				if (status & ICFG_XMAC_RX_LOCAL_FLT_DET)
+					statsp->xmac_stats.rx_localfault_err++;
+		} else
+			goto nxge_mac_intr_fail;
+
+		if (npi_xmac_ctl_get_istatus(handle, portn,
+				(xmac_ctl_iconfig_t *)&status) == NPI_SUCCESS) {
+/* 			printk(KERN_ERR "nxge_mac_intr: ctlistatus[0x%x]\n",
+			status); */
+				if (status & ICFG_XMAC_CTRL_PAUSE_RCVD)
+					statsp->xmac_stats.rx_pause_cnt++;
+				if (status & ICFG_XMAC_CTRL_PAUSE_STATE)
+					statsp->xmac_stats.tx_pause_state++;
+				if (status & ICFG_XMAC_CTRL_NOPAUSE_STATE)
+					statsp->xmac_stats.tx_nopause_state++;
+		} else
+			goto nxge_mac_intr_fail;
+
+	} else if (nep->mac.porttype == PORT_TYPE_BMAC) {
+		if (npi_bmac_tx_get_istatus(handle, portn,
+				(bmac_tx_iconfig_t *)&status) == NPI_SUCCESS) {
+			if (status & ICFG_BMAC_TX_ALL) {
+				if (status & ICFG_BMAC_TX_UNDERFLOW)
+					statsp->bmac_stats.tx_underrun_err++;
+				if (status & ICFG_BMAC_TX_MAXPKTSZ_ERR)
+					statsp->bmac_stats.tx_max_pkt_err++;
+				if (status & ICFG_BMAC_TX_BYTE_CNT_EXP)
+					statsp->bmac_stats.tx_byte_cnt +=
+							BTXMAC_BYTE_CNT_MASK;
+				if (status & ICFG_BMAC_TX_FRAME_CNT_EXP)
+					statsp->bmac_stats.tx_frame_cnt +=
+							BTXMAC_FRM_CNT_MASK;
+			}
+		} else
+			goto nxge_mac_intr_fail;
+
+		if (npi_bmac_rx_get_istatus(handle, portn,
+				(bmac_rx_iconfig_t *)&status) == NPI_SUCCESS) {
+			if (status & ICFG_BMAC_RX_ALL) {
+				if (status & ICFG_BMAC_RX_OVERFLOW)
+					statsp->bmac_stats.rx_overflow_err++;
+				if (status & ICFG_BMAC_RX_FRAME_CNT_EXP)
+					statsp->bmac_stats.rx_frame_cnt +=
+							RXMAC_FRM_CNT_MASK;
+				if (status & ICFG_BMAC_RX_CRC_ERR_CNT_EXP)
+					statsp->bmac_stats.rx_crc_err_cnt +=
+							BMAC_CRC_ER_CNT_MASK;
+				if (status & ICFG_BMAC_RX_LEN_ERR_CNT_EXP)
+					statsp->bmac_stats.rx_len_err_cnt +=
+							MAC_LEN_ER_CNT_MASK;
+				if (status & ICFG_BMAC_RX_VIOL_ERR_CNT_EXP)
+					statsp->bmac_stats.rx_viol_err_cnt +=
+							BMAC_CD_VIO_CNT_MASK;
+				if (status & ICFG_BMAC_RX_BYTE_CNT_EXP)
+					statsp->bmac_stats.rx_byte_cnt +=
+							BRXMAC_BYTE_CNT_MASK;
+				if (status & ICFG_BMAC_RX_ALIGNERR_CNT_EXP)
+					statsp->bmac_stats.rx_align_err_cnt +=
+							BMAC_AL_ER_CNT_MASK;
+			}
+		} else
+			goto nxge_mac_intr_fail;
+
+		if (npi_bmac_ctl_get_istatus(handle, portn,
+				(bmac_ctl_iconfig_t *)&status) == NPI_SUCCESS) {
+			if (status & ICFG_BMAC_CTL_ALL) {
+				if (status & ICFG_BMAC_CTL_RCVPAUSE)
+					statsp->bmac_stats.rx_pause_cnt++;
+				if (status & ICFG_BMAC_CTL_INPAUSE_ST)
+					statsp->bmac_stats.tx_pause_state++;
+				if (status & ICFG_BMAC_CTL_INNOTPAUSE_ST)
+					statsp->bmac_stats.tx_nopause_state++;
+			}
+		} else
+			goto nxge_mac_intr_fail;
+	}
+
+	if (ldgp->nldvs == 1) {
+		(void) npi_intr_ldg_mgmt_set(handle, ldgp->ldg,
+			B_TRUE, ldgp->ldg_timer);
+	}
+	serviced = IRQ_HANDLED;
+
+  nxge_mac_intr_fail:
+
+	NXGE_DEBUG_MSG((nep, MAC_CTL, "<== nxge_mac_intr"));
+	return (serviced);
+}
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,18)
+static irqreturn_t nxge_syserr_intr(int irq, void *info)
+#else
+static irqreturn_t nxge_syserr_intr(int irq, void *info, struct pt_regs *regs)
+#endif
+{
+	struct net_device	*dev = ((p_nxge_intr_arg_t)info)->dev;
+	p_nxge_t		nep = netdev_priv(dev);
+	p_nxge_ldv_t		ldvp = ((p_nxge_intr_arg_t)info)->ldvp;
+	p_nxge_ldg_t		ldgp = NULL;
+	npi_handle_t		handle;
+	sys_err_stat_t		estat;
+	irqreturn_t 		serviced = IRQ_NONE;
+	uint32_t		syserr = 0;
+	NXGE_DEBUG_MSG((NULL, INT_CTL, "==> nxge_syserr_intr"));
+
+	NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "==> nxge_syserr_intr"));
+
+	if (ldvp == NULL || nep == NULL) {
+		NXGE_DEBUG_MSG((nep, INT_CTL, "nxge_syserr_intr: nxgep "
+				"$%p ldvp $%p", nep, ldvp ));
+		return (serviced);
+	}
+
+	if (ldvp != NULL && ldvp->use_timer == B_FALSE) {
+		ldgp = ldvp->ldgp;
+		if (ldgp == NULL) {
+			NXGE_DEBUG_MSG((nep, INT_CTL,
+					"<== nxge_syserr_intr(no "
+					"logical group): "
+					"nxgep $%p ldvp $%p",
+					nep, ldvp));
+			return (serviced);
+		}
+
+		/*
+		 * Get the logical device state if the function uses interrupt.
+		 */
+	}
+
+	/* This interrupt handler is for system error interrupts.  */
+	handle = NXGE_DEV_NPI_HANDLE(nep);
+	estat.value = 0;
+	npi_fzc_sys_err_stat_get(handle, &estat);
+	NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+		"==> nxge_syserr_intr: device error 0x%016llx",
+		estat.value));
+
+	if (estat.bits.ldw.smx) {
+		/* SMX */
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+			"==> nxge_syserr_intr: device error - SMX"));
+			syserr |= NXGE_SYSERR_SMX;
+	} else if (estat.bits.ldw.mac) {
+		/* MAC */
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+			"==> nxge_syserr_intr: device error - MAC"));
+		/*
+		 * There is nothing to be done here. All MAC errors
+		 * go to per MAC port interrupt. MIF interrupt is
+		 * the only MAC sub-block that can generate status
+		 * here. MIF status reported will be ignored here.
+		 * It is checked by per port timer instead.
+		 */
+		syserr |= NXGE_SYSERR_MAC;
+
+	} else if (estat.bits.ldw.ipp) {
+		syserr |= NXGE_SYSERR_IPP;
+
+		/* IPP */
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+			"==> nxge_syserr_intr: device error - IPP"));
+		nxge_ipp_handle_sys_errors(nep);
+	} else if (estat.bits.ldw.zcp) {
+		/* ZCP */
+		syserr |= NXGE_SYSERR_ZCP;
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+			"==> nxge_syserr_intr: device error - ZCP"));
+		nxge_zcp_handle_sys_errors(nep);
+	} else if (estat.bits.ldw.tdmc) {
+		/* TDMC */
+		syserr |= NXGE_SYSERR_TDMC;
+
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+			"==> nxge_syserr_intr: device error - TDMC"));
+		/*
+		 * There is no TDMC system errors defined in the PRM.
+		 * All TDMC channel specific errors are reported on
+		 * a per channel basis.
+		 */
+	} else if (estat.bits.ldw.rdmc) {
+		/* RDMC */
+		syserr |= NXGE_SYSERR_RDMC;
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+			"==> nxge_syserr_intr: device error - RDMC"));
+		nxge_rxdma_handle_sys_errors(nep);
+	} else if (estat.bits.ldw.txc) {
+		/* TXC */
+		syserr |= NXGE_SYSERR_TXC;
+
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+			"==> nxge_syserr_intr: device error - TXC"));
+		nxge_txc_handle_sys_errors(nep);
+	} else if (estat.bits.ldw.peu) {
+		/* PCI-E */
+		syserr |= NXGE_SYSERR_PEU;
+
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+			"==> nxge_syserr_intr: device error - PCI-E"));
+	} else if (estat.bits.ldw.meta1) {
+		/* META1 */
+		syserr |= NXGE_SYSERR_META1;
+
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+			"==> nxge_syserr_intr: device error - META1"));
+	} else if (estat.bits.ldw.meta2) {
+		/* META2 */
+		syserr |= NXGE_SYSERR_META2;
+
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+			"==> nxge_syserr_intr: device error - META2"));
+	} else if (estat.bits.ldw.fflp) {
+		/* FFLP */
+		syserr |= NXGE_SYSERR_FFLP;
+
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+			"==> nxge_syserr_intr: device error - FFLP"));
+		nxge_fflp_handle_sys_errors(nep);
+	}
+
+	serviced = IRQ_HANDLED;
+
+	MUTEX_ENTER(nep->syserr_lock);
+	*(nep->syserr) = syserr;
+	MUTEX_EXIT(nep->syserr_lock);
+
+	if (ldgp != NULL && ldvp != NULL && ldgp->nldvs == 1 &&
+	    !ldvp->use_timer) {
+		npi_intr_ldg_mgmt_set(handle, ldgp->ldg,
+				      B_TRUE, ldgp->ldg_timer);
+	}
+
+	NXGE_DEBUG_MSG((nep, INT_CTL, "<== nxge_syserr_intr"));
+
+	return (serviced);
+}
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,18)
+static irqreturn_t nxge_tx_intr(int irq, void *info)
+#else
+static irqreturn_t nxge_tx_intr(int irq, void *info, struct pt_regs *regs)
+#endif
+
+{
+	struct net_device	*dev = ((p_nxge_intr_arg_t)info)->dev;
+	p_nxge_t		nep = netdev_priv(dev);
+	p_nxge_ldv_t		ldvp = ((p_nxge_intr_arg_t)info)->ldvp;
+	p_nxge_ldg_t		ldgp;
+	uint8_t			channel;
+	npi_handle_t		handle;
+	tx_cs_t			cs;
+	p_tx_ring_t 		tx_ring_p;
+	irqreturn_t 		serviced = IRQ_NONE;
+	int			status;
+	uint32_t		vindex;
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "==> nxge_tx_intr"));
+
+	if (ldvp == NULL || nep == NULL) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_tx_intr: nxgep "
+				"$%p ldvp $%p", nep, ldvp ));
+		return (serviced);
+	}
+	/*
+	 * This interrupt handler is for a specific
+	 * tx dma channel.
+	 */
+	handle = nep->npi_handle;
+	/* Get the control and status for this channel. */
+	channel = ldvp->channel;
+
+	NXGE_DEBUG_MSG((nep, INT_CTL,
+			"nxge_tx_intr: ldvp $%p channel %d",
+			ldvp, channel));
+
+	ldgp = ldvp->ldgp;
+
+	status = npi_txdma_control_status(handle, OP_GET, channel, &cs);
+	vindex = ldvp->vdma_index;
+	NXGE_DEBUG_MSG((nep, INT_CTL,
+			"nxge_tx_intr:channel %d ring index %d status 0x%08x",
+			channel, vindex, status));
+
+	if (!status && cs.bits.ldw.mk) {
+		NXGE_DEBUG_MSG((nep, INT_CTL,
+				"nxge_tx_intr:channel %d ring index %d "
+				"status 0x%08x (mk bit set)",
+				channel, vindex, status));
+
+		tx_ring_p = nep->tx_rings->rings[vindex];
+		NXGE_DEBUG_MSG((nep, INT_CTL,
+				"nxge_tx_intr:channel %d ring index %d "
+				"status 0x%08x (mk bit set, calling reclaim)",
+				channel, vindex, status));
+		nep->statsp->tdc_stats[vindex].tx_intr++;
+		MUTEX_ENTER(&tx_ring_p->lock);
+		nxge_tx_reclaim(nep, tx_ring_p, 0);
+		if (tx_ring_p->intr_marked == B_TRUE)
+			tx_ring_p->intr_marked = B_FALSE;
+		MUTEX_EXIT(&tx_ring_p->lock);
+	}
+
+
+	/*
+	 * Process other transmit control and status.
+	 * Check the ldv state.
+	 */
+	status = nxge_tx_err_evnts(nep, ldvp->vdma_index, ldvp, cs);
+
+	/*
+	 * Rearm this logical group if this is a single device
+	 * group.
+	 */
+	if (ldgp->nldvs == 1) {
+		if (status == NXGE_OK) {
+			npi_intr_ldg_mgmt_set(handle, ldgp->ldg,
+					      B_TRUE, ldgp->ldg_timer);
+		}
+	}
+		/* reenbale queue */
+
+	if (netif_queue_stopped(nep->dev)) {
+		netif_wake_queue(nep->dev);
+		atomic_set(&nep->tx_intr_set, 0);
+	}
+
+	serviced = IRQ_HANDLED;
+
+	NXGE_DEBUG_MSG((nep, TX_CTL, "<== nxge_tx_intr"));
+	return (serviced);
+}
+
+#ifdef NXGE_USE_SOFT_RX
+static void
+nxge_rx_isr(unsigned long info)
+{
+
+	p_nxge_ldg_t		ldgp;
+	p_nxge_ldv_t		ldvp = (p_nxge_ldv_t) info;
+	p_nxge_t		nep = ldvp->nxgep;
+	struct net_device	*dev;
+
+	uint8_t			ldg, channel;
+	uint32_t		vindex;
+	npi_handle_t		handle;
+	rx_dma_ctl_stat_t	cs;
+	int			status = 0;
+	nxge_status_t status1;
+	int			npkts_read;
+	dev = nep->dev;
+
+	handle = nep->npi_handle;
+
+	/*
+	 * Get the control and status for this channel.
+	 */
+	ldgp = ldvp->ldgp;
+	channel = ldvp->channel;
+	cs.value = ldvp->csr;
+
+#ifdef USE_NAPI
+	netif_rx_schedule(dev);
+#endif
+
+	nxge_rx_pkts_ring(nep, ldvp->vdma_index,
+					  0, &npkts_read, cs);
+
+
+	/* Process receive packets */
+
+
+
+	if (cs.value & RX_DMA_CTL_STAT_ERROR) {
+		status1 = nxge_rx_err_evnts(nep, ldvp->vdma_index, ldvp, cs);
+		NXGE_DEBUG_MSG((nep, INT_CTL,
+				"nxge_rx_intr: err event stat 0x%x "
+				"cs stat 0x%llx",
+				status1, cs.value));
+	}
+
+
+	/*
+	 * Enable the mailbox update interrupt if we want
+	 * to use mailbox. We probably don't need to use
+	 * mailbox as it only saves us one pio read.
+	 * Also write 1 to rcrthres and rcrto to clear
+	 * these two edge triggered bits.
+	 */
+	cs.value &= RX_DMA_CTL_STAT_WR1C;
+	cs.bits.hdw.mex = 1;
+	RXDMA_REG_WRITE64(handle, RX_DMA_CTL_STAT_REG, channel,
+			cs.value);
+
+	/*
+	 * Rearm this logical group if this is a single device
+	 * group.
+	 */
+	if (ldgp->nldvs == 1) {
+			ldgimgm_t		mgm;
+#ifdef RX_INTR_BLANKING_LDG
+			ldgp->ldg_timer = nep->intr_timeout;
+#endif
+			mgm.value = 0;
+			mgm.bits.ldw.arm = 1;
+			mgm.bits.ldw.timer = ldgp->ldg_timer;
+			NXGE_REG_WR64(handle,
+				    LDGIMGN_REG + LDSV_OFFSET(ldgp->ldg),
+				    mgm.value);
+
+	}
+}
+
+
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,18)
+static irqreturn_t nxge_rx_intr(int irq, void *info)
+#else
+static irqreturn_t nxge_rx_intr(int irq, void *info, struct pt_regs *regs)
+#endif
+{
+	struct net_device	*dev = ((p_nxge_intr_arg_t)info)->dev;
+	p_nxge_t		nep = netdev_priv(dev);
+	p_nxge_ldv_t		ldvp = ((p_nxge_intr_arg_t)info)->ldvp;
+	irqreturn_t 		serviced = IRQ_HANDLED;
+	p_nxge_ldg_t		ldgp;
+	uint8_t			ldg, channel;
+	uint32_t		vindex;
+	npi_handle_t		handle;
+	rx_dma_ctl_stat_t	cs;
+	int			npkts_read;
+	int			status = 0;
+	nxge_status_t status1;
+
+	if (ldvp == NULL || ldvp->ldgp == NULL) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_rx_intr: "
+			       "Not ready, ldvp/ldgp not set"));
+		return (serviced);
+	}
+
+	handle = nep->npi_handle;
+
+	channel = ldvp->channel;
+
+	RXDMA_REG_READ64(handle, RX_DMA_CTL_STAT_REG, channel, &cs.value);
+	ldvp->csr = cs.value;
+
+	tasklet_schedule(&ldvp->soft_intr);
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "<== nxge_rx_intr"));
+	return (serviced);
+}
+
+#else
+
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,18)
+static irqreturn_t nxge_rx_intr(int irq, void *info)
+#else
+static irqreturn_t nxge_rx_intr(int irq, void *info, struct pt_regs *regs)
+#endif
+{
+	struct net_device	*dev = ((p_nxge_intr_arg_t)info)->dev;
+	p_nxge_t		nep = netdev_priv(dev);
+	p_nxge_ldv_t		ldvp = ((p_nxge_intr_arg_t)info)->ldvp;
+	irqreturn_t 		serviced = IRQ_NONE;
+	p_nxge_ldg_t		ldgp;
+	uint8_t			channel;
+	npi_handle_t		handle;
+	rx_dma_ctl_stat_t	cs;
+	int			npkts_read = 0;
+	int budget = 0;
+	nxge_status_t status1;
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "==> nxge_rx_intr"));
+
+	/*
+	 * This interrupt handler is for specific receive dma channel.
+	 */
+	if (ldvp == NULL || ldvp->ldgp == NULL) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_rx_intr: "
+			       "Not ready, ldvp/ldgp not set"));
+		return (serviced);
+	}
+
+	ldgp = ldvp->ldgp;
+	handle = nep->npi_handle;
+
+	/*
+	 * Get the control and status for this channel.
+	 */
+	channel = ldvp->channel;
+
+	/* Process receive packets */
+#ifdef USE_NAPI
+	budget = ldvp->budget;
+#endif
+
+	RXDMA_REG_READ64(handle, RX_DMA_CTL_STAT_REG, channel, &cs.value);
+	ldvp->csr = cs.value;
+	nxge_rx_pkts_ring(nep, ldvp->vdma_index,
+						  budget, &npkts_read, cs);
+
+	serviced = IRQ_HANDLED;
+
+	/* error events. */
+	/*
+	 * XXXX: bit 40 and not bit 35 should be checked against rbr empty.
+	 */
+	if (cs.value & RX_DMA_CTL_STAT_ERROR) {
+		status1 = nxge_rx_err_evnts(nep, ldvp->vdma_index, ldvp, cs);
+		NXGE_DEBUG_MSG((nep, INT_CTL,
+				"nxge_rx_intr: err event stat 0x%x "
+				"cs stat 0x%llx",
+				status1, cs.value));
+	}
+
+
+#ifdef USE_NAPI
+	ldvp->budget = npkts_read;
+#endif
+	/*
+	 * Enable the mailbox update interrupt if we want
+	 * to use mailbox. We probably don't need to use
+	 * mailbox as it only saves us one pio read.
+	 * Also write 1 to rcrthres and rcrto to clear
+	 * these two edge triggered bits.
+	 */
+	cs.value &= RX_DMA_CTL_STAT_WR1C;
+	cs.bits.hdw.mex = 1;
+	RXDMA_REG_WRITE64(handle, RX_DMA_CTL_STAT_REG, channel,
+			cs.value);
+
+	/*
+	 * Rearm this logical group if this is a single device
+	 * group.
+	 */
+	if (ldgp->nldvs == 1) {
+			ldgimgm_t		mgm;
+#ifdef RX_INTR_BLANKING_LDG
+			ldgp->ldg_timer = nep->intr_timeout;
+#endif
+			mgm.value = 0;
+			mgm.bits.ldw.arm = 1;
+			mgm.bits.ldw.timer = ldgp->ldg_timer;
+			NXGE_REG_WR64(handle,
+				    LDGIMGN_REG + LDSV_OFFSET(ldgp->ldg),
+				    mgm.value);
+
+	}
+
+	NXGE_DEBUG_MSG((nep, RX_CTL, "<== nxge_rx_intr"));
+	return (serviced);
+}
+
+#endif
+
+static int nxge_ldv_cnt(p_nxge_t nep)
+{
+	p_nxge_dma_pt_cfg_t	p_all_cfgp;
+	p_nxge_hw_pt_cfg_t	p_cfgp;
+	int			nldvs;
+
+	NXGE_DEBUG_MSG((nep, INT_CTL, "==> nxge_ldv_cnt"));
+
+	p_all_cfgp = (p_nxge_dma_pt_cfg_t)&nep->pt_config;
+	p_cfgp = (p_nxge_hw_pt_cfg_t)&p_all_cfgp->hw_config;
+	nldvs = p_cfgp->max_tdcs + p_cfgp->max_rdcs;
+	/*
+	 * If function zero instance, it needs to handle the
+	 * system error interrupts.
+	 */
+	if (nep->function_num == 0) {
+		nldvs++;
+	} else {
+		/* use timer for other functions */
+		nldvs++;
+	}
+
+	/*
+	 * XXX: Assume single partition, each function owns mac.
+	 */
+	nldvs++;
+
+	/* One ldv for MIF for function 0 and 1 each for timers for others */
+	nldvs++;
+
+	NXGE_DEBUG_MSG((nep, INT_CTL, "<== nxge_ldv_cnt"));
+
+	return (nldvs);
+}
+
+static void
+nxge_ldgv_setup(p_nxge_ldg_t *ldgp, p_nxge_ldv_t *ldvp, uint8_t ldv,
+                uint8_t endldg, uint8_t *ngrps)
+{
+
+        NXGE_DEBUG_MSG((NULL, INT_CTL, "==> nxge_ldgv_setup"));
+        /* Assign the group number for each device. */
+        (*ldvp)->ldg_assigned = (*ldgp)->ldg;
+        (*ldvp)->ldgp = *ldgp;
+        (*ldvp)->ldv = ldv;
+
+        NXGE_DEBUG_MSG((NULL, INT_CTL, "==> nxge_ldgv_setup: "
+                "ldv %d endldg %d ldg %d, ldvp $%p",
+                ldv, endldg, (*ldgp)->ldg, (*ldgp)->ldvp));
+
+        (*ldgp)->nldvs++;
+        if ((*ldgp)->ldg == (endldg - 1)) {
+                if ((*ldgp)->ldvp == NULL) {
+                        (*ldgp)->ldvp = *ldvp;
+                        *ngrps += 1;
+
+                        NXGE_DEBUG_MSG((NULL, INT_CTL,
+                                "==> nxge_ldgv_setup: ngrps %d", *ngrps));
+                }
+
+		NXGE_DEBUG_MSG((NULL, INT_CTL, "nxge_ldgv_setup: "
+				"ldvp $%p",
+				*ldvp));
+                ++*ldvp;
+        } else {
+                (*ldgp)->ldvp = *ldvp;
+                *ngrps += 1;
+
+                NXGE_DEBUG_MSG((NULL, INT_CTL, "==> nxge_ldgv_setup(done): "
+                        "ldv %d endldg %d ldg %d, ldvp $%p",
+                        ldv, endldg, (*ldgp)->ldg, (*ldgp)->ldvp));
+                (*ldvp) = ++*ldvp;
+                (*ldgp) = ++*ldgp;
+
+                NXGE_DEBUG_MSG((NULL, INT_CTL,
+                        "==> nxge_ldgv_setup: new ngrps %d", *ngrps));
+
+        }
+
+        NXGE_DEBUG_MSG((NULL, INT_CTL, "==> nxge_ldgv_setup: "
+                "ldv %d endldg %d ngrps %d", ldv, endldg, *ngrps));
+
+        NXGE_DEBUG_MSG((NULL, INT_CTL, "<== nxge_ldgv_setup"));
+
+}
+
+static int nxge_ldgv_init(p_nxge_t nep, int intr_avail, int *intr_used)
+{
+	int			i, maxldvs, maxldgs, start, end, nldvs;
+	int			ldv, ldg, endldg, ngrps;
+	p_nxge_dma_pt_cfg_t	p_all_cfgp;
+	p_nxge_hw_pt_cfg_t	p_cfgp;
+	p_nxge_ldgv_t		ldgvp;
+	p_nxge_ldg_t		ldgp, ptr;
+	p_nxge_ldv_t		ldvp;
+	uint8_t			channel;
+	int			status = NXGE_OK;
+
+	NXGE_DEBUG_MSG((nep, INT_CTL, "==> nxge_ldgv_init"));
+	if (!intr_avail) {
+		*intr_used = 0;
+		NXGE_DEBUG_MSG((nep, INT_CTL, "nxge_ldgv_init:no avail"));
+		return (NXGE_ERROR);
+	}
+
+	p_all_cfgp = (p_nxge_dma_pt_cfg_t)&nep->pt_config;
+	p_cfgp = (p_nxge_hw_pt_cfg_t)&p_all_cfgp->hw_config;
+
+	maxldvs = nxge_ldv_cnt(nep);
+	maxldgs = p_cfgp->max_ldgs;
+	if (!maxldvs || !maxldgs) {
+		/* No devices configured. */
+		NXGE_DEBUG_MSG((nep, INT_CTL, "nxge_ldgv_init: "
+		       "no logical devices or groups configured."));
+		return (NXGE_ERROR);
+	}
+	ldgvp = nep->ldgvp;
+	if (ldgvp == NULL) {
+		ldgvp = KMEM_ZALLOC(sizeof (nxge_ldgv_t), GFP_KERNEL);
+		nep->ldgvp = ldgvp;
+		ldgvp->maxldgs = maxldgs;
+		ldgvp->maxldvs = maxldvs;
+		ldgp = ldgvp->ldgp = KMEM_ZALLOC(sizeof (nxge_ldg_t) * maxldgs,
+						 GFP_KERNEL);
+		ldvp = ldgvp->ldvp = KMEM_ZALLOC(sizeof (nxge_ldv_t) * maxldvs,
+						 GFP_KERNEL);
+	} else {
+		ldgp = ldgvp->ldgp;
+		ldvp = ldgvp->ldvp;
+	}
+
+	ldgvp->ndma_ldvs = p_cfgp->max_tdcs + p_cfgp->max_rdcs;
+	ldgvp->tmres = NXGE_TIMER_RESO;
+
+	NXGE_DEBUG_MSG((nep, INT_CTL, "nxge_ldgv_init: maxldvs %d maxldgs %d",
+	       maxldvs, maxldgs));
+	ldg = p_cfgp->start_ldg;
+	ptr = ldgp;
+	for (i = 0; i < maxldgs; i++) {
+		ptr->func = nep->function_num;
+		ptr->arm = B_TRUE;
+		ptr->vldg_index = i;
+		ptr->ldg_timer = NXGE_TIMER_LDG;
+		ptr->ldg = ldg++;
+		ptr->sys_intr_handler = nxge_intr;
+		ptr->nldvs = 0;
+		ptr->nxgep = nep;
+		ptr++;
+	}
+
+	ldg = p_cfgp->start_ldg;
+	if (maxldgs > intr_avail) {
+		ngrps = intr_avail;
+	} else {
+		ngrps = maxldgs;
+	}
+	endldg = ldg + ngrps;
+
+	/*
+	 * Receive DMA channels.
+	 */
+	channel = p_cfgp->start_rdc;
+	start = p_cfgp->start_rdc + NXGE_RDMA_LD_START;
+	end = start + p_cfgp->max_rdcs;
+	nldvs = 0;
+	ldgvp->nldvs = 0;
+	ldgp->ldvp = NULL;
+	*intr_used = 0;
+	/*
+	 * Start with RDC to configure logical devices for each group.
+	 */
+	for (i = 0, ldv = start; ldv < end; i++, ldv++) {
+		ldvp->is_rxdma = B_TRUE;
+		ldgp->has_rxdma = B_TRUE;
+		ldvp->ldv = ldv;
+		/* TODO: if non-seq needs to change the following code */
+		ldvp->channel = channel++;
+		ldvp->vdma_index = i;
+		ldvp->ldv_intr_handler = nxge_rx_intr;
+		ldvp->ldv_ldf_masks = 0;
+		ldvp->use_timer = B_FALSE;
+		ldvp->nxgep = nep;
+#ifdef NXGE_USE_SOFT_RX
+			tasklet_init(&ldvp->soft_intr,
+						 nxge_rx_isr, (unsigned long)ldvp);
+#endif
+		nxge_ldgv_setup(&ldgp, &ldvp, ldv, endldg,
+				(uint8_t *)intr_used);
+		nldvs++;
+	}
+
+	/*
+	 * Transmit DMA channels.
+	 */
+	channel = p_cfgp->start_tdc;
+	start = p_cfgp->start_tdc + NXGE_TDMA_LD_START;
+	end = start + p_cfgp->max_tdcs;
+	for (i = 0, ldv = start; ldv < end; i++, ldv++) {
+		ldvp->is_txdma = B_TRUE;
+		ldgp->has_txdma = B_TRUE;
+		ldvp->ldv = ldv;
+		ldvp->channel = channel++;
+		ldvp->vdma_index = i;
+		ldvp->ldv_intr_handler = nxge_tx_intr;
+		ldvp->ldv_ldf_masks = 0;
+		ldvp->use_timer = B_FALSE;
+		ldvp->nxgep = nep;
+		nxge_ldgv_setup(&ldgp, &ldvp, ldv, endldg,
+				(uint8_t *)intr_used);
+		nldvs++;
+	}
+
+	/*
+	 * MIF
+	 */
+	if ((!nxge_use_partition) && (nep->function_num == 0)) {
+		ldv = NXGE_MIF_LD;
+		ldvp->ldv = ldv;
+		ldvp->is_mif = B_TRUE;
+		ldvp->ldv_intr_handler = nxge_mif_intr;
+		ldvp->ldv_ldf_masks = 0;
+		ldvp->use_timer = B_FALSE;
+		ldvp->nxgep = nep;
+		nxge_ldgv_setup(&ldgp, &ldvp, ldv, endldg,
+				(uint8_t *)intr_used);
+		nldvs++;
+
+	}
+
+	/*
+	 * XXX: MAC port (TODO: function zero control)
+	 */
+	if (!nxge_use_partition) {
+		ldvp->is_mac = B_TRUE;
+		ldvp->ldv_intr_handler = nxge_mac_intr;
+		ldvp->ldv_ldf_masks = 0;
+		ldv = nep->function_num + NXGE_MAC_LD_START;
+		ldvp->ldv = ldv;
+		ldvp->use_timer = B_FALSE;
+		ldvp->nxgep = nep;
+		nxge_ldgv_setup(&ldgp, &ldvp, ldv, endldg,
+				(uint8_t *)intr_used);
+		nldvs++;
+	}
+
+	/*
+	 * XXX: Function 0 owns system error interrupts.
+	 */
+	if (nep->function_num == 0) {
+		ldv = NXGE_SYS_ERROR_LD;
+		ldvp->ldv = ldv;
+		ldvp->is_syserr = B_TRUE;
+		ldvp->ldv_intr_handler = nxge_syserr_intr;
+		ldvp->ldv_ldf_masks = 0;
+#ifdef	NXGE_SYSERR_USE_INT
+		ldvp->use_timer = B_FALSE;
+#else
+		ldvp->use_timer = B_TRUE;
+#endif
+		ldvp->nxgep = nep;
+		ldgvp->ldvp_syserr = ldvp;
+		/*
+		 * Unmask the system interrupt states.
+		 */
+		nxge_fzc_sys_err_mask_set(nep, SYS_ERR_SMX_MASK |
+				SYS_ERR_IPP_MASK | SYS_ERR_TXC_MASK |
+				SYS_ERR_ZCP_MASK);
+
+		nxge_ldgv_setup(&ldgp, &ldvp, ldv, endldg,
+				(uint8_t *)intr_used);
+		nldvs++;
+	}
+
+	ldgvp->ldg_intrs = *intr_used;
+	ldgvp->nldvs = nldvs;
+
+	NXGE_DEBUG_MSG((nep, INT_CTL, "nxge_ldgv_init: "
+			"func %d nldvs=%d navail=%d used=%d",
+			nep->function_num, nldvs, intr_avail, *intr_used));
+
+	NXGE_DEBUG_MSG((nep, INT_CTL, "<== nxge_ldgv_init"));
+
+	return (status);
+}
+
+static int nxge_ldgv_uninit(p_nxge_t nep)
+{
+	p_nxge_ldgv_t		ldgvp;
+	int			status = NXGE_OK;
+
+	NXGE_DEBUG_MSG((nep, INT_CTL, "==> nxge_ldgv_uninit"));
+	ldgvp = nep->ldgvp;
+	if (ldgvp == NULL) {
+		NXGE_DEBUG_MSG((nep, INT_CTL, "nxge_ldgv_uninit: "
+				"no logical group configured."));
+		return (status);
+	}
+
+	if (ldgvp->ldgp) {
+#ifdef NXGE_USE_SOFT_RX
+		int		i, nldvs;
+		p_nxge_ldv_t ldvp;
+		ldvp = ldgvp->ldgp->ldvp;
+		nldvs = ldgvp->ldgp->nldvs;
+		for (i=0; i < nldvs; i++, ldvp++) {
+			if (ldvp->is_rxdma == B_TRUE)
+				tasklet_kill(&ldvp->soft_intr);
+		}
+
+#endif
+		KMEM_FREE(ldgvp->ldgp, ldgvp->maxldgs * sizeof (nxge_ldg_t));
+	}
+
+	if (ldgvp->ldvp) {
+		KMEM_FREE(ldgvp->ldvp, ldgvp->maxldvs * sizeof (nxge_ldv_t));
+	}
+
+	KMEM_FREE(ldgvp, sizeof (nxge_ldgv_t));
+	nep->ldgvp = NULL;
+
+	NXGE_DEBUG_MSG((nep, INT_CTL, "<== nxge_ldgv_uninit"));
+
+	return (status);
+}
+
+static int
+nxge_intr_ldgv_init(p_nxge_t nep)
+{
+	int		status = NXGE_OK;
+
+	NXGE_DEBUG_MSG((nep, INT_CTL, "==> nxge_intr_ldgv_init"));
+	/*
+	 * Configure the logical device group numbers, state vectors
+	 * and interrupt masks for each logical device.
+	 */
+	status = nxge_fzc_intr_init(nep);
+
+	/*
+	 * Configure logical device masks and timers.
+	 */
+	status = nxge_intr_mask_mgmt(nep);
+
+	NXGE_DEBUG_MSG((nep, INT_CTL, "<== nxge_intr_ldgv_init"));
+	return (status);
+}
+
+static int
+nxge_intr_mask_mgmt(p_nxge_t nep)
+{
+	p_nxge_ldgv_t	ldgvp;
+	p_nxge_ldg_t	ldgp;
+	p_nxge_ldv_t	ldvp;
+	npi_handle_t	handle;
+	int		i, j;
+	npi_status_t	rs = NPI_SUCCESS;
+
+	NXGE_DEBUG_MSG((nep, INT_CTL, "==> nxge_intr_mask_mgmt"));
+
+	if ((ldgvp = nep->ldgvp) == NULL) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+			"<== nxge_intr_mask_mgmt: Null ldgvp"));
+		return (NXGE_ERROR);
+	}
+	handle = nep->npi_handle;
+	ldgp = ldgvp->ldgp;
+	ldvp = ldgvp->ldvp;
+	if (ldgp == NULL || ldvp == NULL) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+			"<== nxge_intr_mask_mgmt: Null ldgp or ldvp"));
+		return (NXGE_ERROR);
+	}
+
+	NXGE_DEBUG_MSG((nep, INT_CTL,
+		"==> nxge_intr_mask_mgmt: # of intrs %d ", ldgvp->ldg_intrs));
+
+	/* Initialize masks. */
+	for (i = 0; i < ldgvp->ldg_intrs; i++, ldgp++) {
+		NXGE_DEBUG_MSG((nep, INT_CTL,
+				"nxge_intr_mask_mgmt(Neptune): # ldv %d "
+				"in group %d", ldgp->nldvs, ldgp->ldg));
+		for (j = 0; j < ldgp->nldvs; j++, ldvp++) {
+			NXGE_DEBUG_MSG((nep, INT_CTL,
+					"nxge_intr_mask_mgmt: set ldv # %d "
+					"for ldg %d", ldvp->ldv, ldgp->ldg));
+
+			rs = npi_intr_mask_set(handle, ldvp->ldv,
+						   ldvp->ldv_ldf_masks);
+			if (rs != NPI_SUCCESS) {
+				NXGE_DEBUG_MSG((nep, INT_CTL,
+					       "nxge_intr_mask_mgmt: "
+					       "set mask failed status 0x%x "
+					       "ldv %d mask 0x%x",
+					       rs, ldvp->ldv,
+					       ldvp->ldv_ldf_masks));
+				return (NXGE_ERROR | rs);
+			}
+			NXGE_DEBUG_MSG((nep, INT_CTL,
+					"nxge_intr_mask_mgmt: "
+					"set mask OK "
+					" status 0x%x ldv %d mask 0x%x",
+					rs, ldvp->ldv,
+					ldvp->ldv_ldf_masks));
+		}
+	}
+
+	ldgp = ldgvp->ldgp;
+	/* Configure timer and arm bit */
+	for (i = 0; i < nep->ldgvp->ldg_intrs; i++, ldgp++) {
+#if 1
+		rs = npi_intr_ldg_mgmt_set(handle, ldgp->ldg,
+					       ldgp->arm, ldgp->ldg_timer);
+#else
+		rs = npi_intr_ldg_mgmt_set(handle, ldgp->ldg,
+					       0, ldgp->ldg_timer);
+#endif
+		if (rs != NPI_SUCCESS) {
+			NXGE_DEBUG_MSG((nep, INT_CTL, "nxge_intr_mask_mgmt: "
+				       "set timer failed. "
+				       " status 0x%x ldg %d timer 0x%x arm %d",
+				       rs, ldgp->ldg, ldgp->ldg_timer,
+					ldgp->arm));
+			return (NXGE_ERROR | rs);
+		}
+		NXGE_DEBUG_MSG((nep, INT_CTL,
+			"nxge_intr_mask_mgmt: "
+			"set timer OK "
+			" status 0x%x ldg %d timer 0x%x arm %d",
+			rs, ldgp->ldg, ldgp->ldg_timer, ldgp->arm));
+	}
+
+	NXGE_DEBUG_MSG((nep, INT_CTL, "<== nxge_intr_mask_mgmt"));
+	return ((int)rs);
+}
+
+static int nxge_add_intrs_legacy(p_nxge_t nep)
+{
+	int		status = NXGE_OK;
+	p_nxge_msix_t	intrp;
+	p_nxge_ldg_t	ldgp;
+	int		nactual, navail, nrequired;
+
+	/* Use only one interrupt */
+	NXGE_DEBUG_MSG((nep, INT_CTL, "==> nxge_add_intrs_legacy\n"));
+	intrp = (p_nxge_msix_t)&nep->nxge_intr_msix;
+	intrp->start_inum = 0;
+	nactual = navail = 1;
+	intrp->msi_intx_cnt = nactual;
+	intrp->intr_added = nactual;
+	nrequired = 0;
+	status = nxge_ldgv_init(nep, navail, &nrequired);
+	if (status != NXGE_OK) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_add_intrs_legacy: "
+			       "nxge_ldgv_init failed: err %d", status));
+		return (status);
+	}
+
+	NXGE_DEBUG_MSG((nep, INT_CTL, "nxge_add_intrs_legacy: "
+		       "nxge_ldgv_init:avail %d used %d", navail, nrequired));
+
+	ldgp = nep->ldgvp->ldgp;
+/*	ldgp->vector = nep->pdev->irq; */
+	ldgp->vector = 0;
+	ldgp->intdata = SID_DATA(ldgp->func, ldgp->vector);
+	status = request_irq(nep->pdev->irq, &nxge_intr_legacy,
+			     NXGE_IRQ_SHARED, nep->dev->name,
+			     (void *) nep->dev);
+	if (status != 0) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_add_intrs_legacy: "
+			       "request_irq failed. err %d", status));
+		nxge_ldgv_uninit(nep);
+		return (status);
+	}
+
+	intrp->intr_added = nrequired;
+	status = nxge_intr_ldgv_init(nep);
+	NXGE_DEBUG_MSG((nep, INT_CTL, "<== nxge_add_intrs_legacy\n"));
+
+	return (status);
+}
+
+#ifdef CONFIG_PCI_MSI
+
+static int nxge_add_intrs_msi(p_nxge_t nep)
+{
+	int		status = NXGE_OK;
+	p_nxge_msix_t	intrp;
+	p_nxge_ldg_t	ldgp;
+	int		nactual, navail, nrequired;
+
+	/* Use only one interrupt */
+	NXGE_DEBUG_MSG((nep, INT_CTL, "==> nxge_add_intrs_msi\n"));
+	intrp = (p_nxge_msix_t)&nep->nxge_intr_msix;
+	intrp->start_inum = 0;
+	nactual = navail = 1;
+	intrp->msi_intx_cnt = nactual;
+	nrequired = 0;
+	status = nxge_ldgv_init(nep, navail, &nrequired);
+	if (status != NXGE_OK) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_add_intrs_msi: "
+			       "nxge_ldgv_init failed: err %d", status));
+		return (status);
+	}
+
+	NXGE_DEBUG_MSG((nep, INT_CTL, "nxge_add_intrs_msi: "
+		       "nxge_ldgv_init:avail %d used %d", navail, nrequired));
+
+	status = pci_enable_msi(nep->pdev);
+	if (status != 0) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_add_intrs_msi: "
+			       "pci_enable_msifailed. err %d", status));
+		nxge_ldgv_uninit(nep);
+		return (NXGE_ERROR);
+	}
+
+	ldgp = nep->ldgvp->ldgp;
+	/*ldgp->vector = nep->pdev->irq; */
+	ldgp->vector = 0;
+	ldgp->intdata = SID_DATA(ldgp->func, ldgp->vector);
+	status = request_irq(nep->pdev->irq, &nxge_intr_legacy,
+			     NXGE_IRQ_MSI, nep->dev->name,
+			     (void *) nep->dev);
+	if (status != 0) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_add_intrs_msi: "
+			       "request_irq failed. err %d", status));
+		nxge_ldgv_uninit(nep);
+		return (status);
+	}
+
+	intrp->intr_added = nrequired;
+	status = nxge_intr_ldgv_init(nep);
+
+	NXGE_DEBUG_MSG((nep, INT_CTL, "<== nxge_add_intrs_msi"));
+
+	return (status);
+
+}
+
+
+static int nxge_add_intrs_msix(p_nxge_t nep)
+{
+	int			status = NXGE_OK;
+	int			intr_reqd, intr_avail, intr_used;
+	int			nldvs, nldgs, i, stat;
+	p_nxge_msix_t		intrp;
+	p_nxge_ldg_t		ldgp;
+
+	p_nxge_dma_pt_cfg_t	p_all_cfgp;
+	p_nxge_hw_pt_cfg_t	p_cfgp;
+	struct msix_entry	*msix_vector_p;
+
+	NXGE_DEBUG_MSG((nep, INT_CTL, "==> nxge_add_intrs_msix"));
+	intrp = (p_nxge_msix_t)&nep->nxge_intr_msix;
+	intrp->start_inum = 0;
+
+	p_all_cfgp = (p_nxge_dma_pt_cfg_t)&nep->pt_config;
+	p_cfgp = (p_nxge_hw_pt_cfg_t)&p_all_cfgp->hw_config;
+	nldvs = nxge_ldv_cnt(nep);
+	nldgs = p_cfgp->max_ldgs;
+
+	if (nldgs < nldvs) {
+		intr_reqd = nldgs;
+	} else {
+		intr_reqd = nldvs;
+	}
+	if (!nldvs || !nldgs) {
+		/* No devices configured. */
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_add_intrs_msix: "
+			       "no logical devices or groups configured."));
+		return (NXGE_ERROR);
+	}
+
+	msix_vector_p = KMEM_ZALLOC(sizeof(struct msix_entry) * intr_reqd,
+				    GFP_KERNEL);
+
+	if (msix_vector_p == NULL) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_add_intrs_msix: "
+			       "Failed to alloc memory for msix table"));
+		return (NXGE_ERROR);
+	}
+
+	for (i = 0; i < intr_reqd; i++) {
+		(msix_vector_p + i)->entry = i;
+	}
+
+	intrp->msix_alloced = intr_reqd;
+
+	stat = pci_enable_msix(nep->pdev, msix_vector_p, intr_reqd);
+	if (stat < 0) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_add_intrs_msix: "
+			       "pci_enable_msix failed for %d vectors. "
+				"err %d\n", intr_reqd, stat));
+		KMEM_FREE(msix_vector_p, sizeof(struct msix_entry) * intr_reqd);
+		return (NXGE_ERROR);
+	}
+
+	if (stat > 0) {
+		intr_avail = stat;
+		NXGE_DEBUG_MSG((nep, NXGE_ERR_CTL, "nxge_add_intrs_msix: "
+			       "intrs requested=%d available=%d ",
+			       intr_reqd, intr_avail));
+		stat = pci_enable_msix(nep->pdev, msix_vector_p, intr_avail);
+		if (stat != 0) {
+			NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+					"nxge_add_intrs_msix: "
+					"pci_enable_msix failed to get "
+					"promised %d intr. err %d ",
+					intr_avail, stat));
+			KMEM_FREE(msix_vector_p, sizeof(struct msix_entry) * intr_reqd);
+			return (NXGE_ERROR);
+		}
+
+	} else {
+		intr_avail = intr_reqd;
+	}
+
+	intrp->msix_vector = msix_vector_p;
+	intr_used = 0;
+
+	status = nxge_ldgv_init(nep, intr_avail, &intr_used);
+	if (status != NXGE_OK) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_add_intrs_msix: "
+			       "nxge_ldgv_init failed. err %d ", status));
+		pci_disable_msix(nep->pdev);
+		KMEM_FREE(msix_vector_p, sizeof(struct msix_entry) * intr_reqd);
+		intrp->msix_vector = NULL;
+		return (status);
+	}
+
+	ldgp = nep->ldgvp->ldgp;
+	for (i = 0; i < intr_used; i++, ldgp++) {
+		struct msix_entry	*entryp = (intrp->msix_vector + i);
+
+/*		ldgp->vector = entryp->vector; */
+		ldgp->vector = i;
+		ldgp->intdata = SID_DATA(ldgp->func, ldgp->vector);
+		if (ldgp->nldvs == 1) {
+			sprintf(intrp->irq_name[i].name,"%s_i%x", nep->dev->name, i);
+			ldgp->ldvp->intr_hdlr_arg.dev = nep->dev;
+			ldgp->ldvp->intr_hdlr_arg.ldvp = ldgp->ldvp;
+
+
+			status = request_irq(entryp->vector,
+					     ldgp->ldvp->ldv_intr_handler,
+								 0,
+								 intrp->irq_name[i].name,
+					     (void *) &(ldgp->ldvp->intr_hdlr_arg));
+			NXGE_SET_IRQ_TYPE(entryp->vector, IRQF_PERCPU | IRQF_TRIGGER_HIGH);
+			if (status != 0) {
+				NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+					       "nxge_add_intrs_msix: "
+					       "request_irq failed. err %d",
+					       status));
+				goto nxge_add_intrs_msix_fail1;
+			}
+			NXGE_DEBUG_MSG((nep, INT_CTL,
+				       "nxge_add_intrs_msix: "
+					"request_irq succeeded for intr %d "
+					"with ldv_intr_handler",
+					i));
+
+		} else if (ldgp->nldvs > 1) {
+			sprintf(intrp->irq_name[i].name,"%s_s%x", nep->dev->name, i);
+			ldgp->ldvp->intr_hdlr_arg.dev = nep->dev;
+			ldgp->ldvp->intr_hdlr_arg.ldvp = ldgp->ldvp;
+			status = request_irq(entryp->vector,
+					     ldgp->sys_intr_handler,
+								 0, intrp->irq_name[i].name,
+					     (void *) &ldgp->ldvp->intr_hdlr_arg);
+			NXGE_SET_IRQ_TYPE(entryp->vector, IRQF_TRIGGER_HIGH);
+			if (status != 0) {
+				NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+					       "nxge_add_intrs_msix: "
+					       "request_irq failed. err %d",
+					       status));
+				goto nxge_add_intrs_msix_fail1;
+			}
+			NXGE_DEBUG_MSG((nep, INT_CTL,
+				       "nxge_add_intrs_msix: "
+				       "request_irq succeeded for intr %d "
+					"with sys_intr_handler",
+					i));
+		}
+	}
+
+	intrp->intr_added = intr_used;
+	status = nxge_intr_ldgv_init(nep);
+
+	NXGE_DEBUG_MSG((nep, INT_CTL, "<== nxge_add_intrs_msix"));
+
+	return (status);
+
+nxge_add_intrs_msix_fail1:
+	/* TODO - nxge_ldgv_uninit */
+	nxge_ldgv_uninit(nep);
+	pci_disable_msix(nep->pdev);
+	KMEM_FREE(msix_vector_p, sizeof(struct msix_entry) * intr_reqd);
+	intrp->msix_vector = NULL;
+
+	NXGE_DEBUG_MSG((nep, INT_CTL, "<== nxge_add_intrs_msix"));
+	return (status);
+}
+#endif
+
+static int nxge_add_intrs(p_nxge_t nep)
+{
+	int		status = NXGE_OK;
+	int		pos;
+
+	NXGE_DEBUG_MSG((nep, INT_CTL, "==> nxge_add_intrs"));
+
+	nep->nxge_intr_msix.intr_registered = B_FALSE;
+	nep->nxge_intr_msix.intr_enabled = B_FALSE;
+	nep->nxge_intr_msix.msi_intx_cnt = 0;
+	nep->nxge_intr_msix.intr_added = 0;
+	nep->nxge_intr_msix.niu_msi_enable = B_FALSE;
+	nep->nxge_intr_msix.intr_type = 0;
+
+#ifdef CONFIG_PCI_MSI
+	if (nxge_msi_enable || nxge_msix_enable) {
+		nep->nxge_intr_msix.niu_msi_enable = B_TRUE;
+	}
+
+	if (nxge_msix_enable) {
+		pos = pci_find_capability(nep->pdev, PCI_CAP_ID_MSIX);
+		if (pos) {
+			nep->nxge_intr_msix.intr_type = INTR_TYPE_MSIX;
+			if ((status = nxge_add_intrs_msix(nep)) != NXGE_OK) {
+				NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+						"nxge_add_intrs: "
+						"nxge_add_intrs_msix failed: "
+						"status 0x%x", status));
+			} else {
+				nep->nxge_intr_msix.intr_registered = B_TRUE;
+			}
+			return (status);
+		}
+	}
+
+	if (nxge_msi_enable) {
+		pos = pci_find_capability(nep->pdev, PCI_CAP_ID_MSI);
+		if (pos) {
+			nep->nxge_intr_msix.intr_type = INTR_TYPE_MSI;
+			if ((status = nxge_add_intrs_msi(nep)) != NXGE_OK) {
+				NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_add_intrs: "
+								"nxge_add_intrs_msi failed: "
+								"status 0x%x", status));
+			} else {
+				nep->nxge_intr_msix.intr_registered = B_TRUE;
+			}
+			return (status);
+		}
+	}
+#endif
+
+	if ((status = nxge_add_intrs_legacy(nep)) != NXGE_OK) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_add_intrs: "
+			       "nxge_add_intrs_legacy failed: status 0x%x",
+			       status));
+		return (status);
+	} else {
+		nep->nxge_intr_msix.intr_type = INTR_TYPE_INTX;
+		nep->nxge_intr_msix.intr_registered = B_TRUE;
+		return (status);
+	}
+
+
+	if (nep->nxge_intr_msix.intr_registered == B_FALSE) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_add_intrs: "
+			       "failed to register interrupts"));
+		status = NXGE_ERROR;
+	}
+
+	NXGE_DEBUG_MSG((nep, INT_CTL, "<== nxge_add_intrs"));
+	return (status);
+}
+
+static void nxge_remove_intrs(p_nxge_t nep)
+{
+	int		i;
+	p_nxge_msix_t	intrp;
+	p_nxge_ldg_t	ldgp;
+
+	NXGE_DEBUG_MSG((nep, INT_CTL, "==> nxge_remove_intrs"));
+
+	intrp = (p_nxge_msix_t)&nep->nxge_intr_msix;
+	if (!intrp->intr_registered) {
+		NXGE_DEBUG_MSG((nep, INT_CTL, "==> nxge_remove_intrs: "
+				       "interrupts not registered"));
+		return;
+	}
+
+	ldgp = nep->ldgvp->ldgp;
+	switch (intrp->intr_type) {
+#ifdef CONFIG_PCI_MSI
+	case INTR_TYPE_MSIX:
+		nxge_ldgv_uninit(nep);
+		for (i = 0; i < intrp->intr_added; i++, ldgp++) {
+			struct msix_entry *entryp = (intrp->msix_vector + i);
+			free_irq(entryp->vector,
+				 (void *) &(ldgp->ldvp->intr_hdlr_arg));
+		}
+
+		pci_disable_msix(nep->pdev);
+		KMEM_FREE(intrp->msix_vector,
+				  intrp->msix_alloced * sizeof(struct msix_entry));
+		break;
+	case INTR_TYPE_MSI:
+		nxge_ldgv_uninit(nep);
+		free_irq(nep->pdev->irq, (void *) nep->dev);
+		pci_disable_msi(nep->pdev);
+		break;
+#endif
+
+	case INTR_TYPE_INTX:
+		nxge_ldgv_uninit(nep);
+		free_irq(nep->pdev->irq, (void *) nep->dev);
+		break;
+	case INTR_TYPE_INVALID:
+	default:
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_remove_intrs: "
+			       "invalid intr type %d",
+			       intrp->intr_type));
+		break;
+	}
+
+	intrp->intr_registered = B_FALSE;
+	intrp->intr_enabled = B_FALSE;
+	NXGE_DEBUG_MSG((nep, INT_CTL, "<== nxge_remove_intrs"));
+}
+
+void
+nxge_check_hw_state(p_nxge_t nxgep, boolean_t start_timer)
+{
+	NXGE_DEBUG_MSG((nxgep, MOD_CTL, "==> nxge_check_hw_state"));
+	if (start_timer) {
+		if (!timer_pending(&nxgep->wd_timer)) {
+			nxgep->hw_st_timer_stopped = B_FALSE;
+			NXGE_DEBUG_MSG((nxgep, MOD_CTL,
+					"nxge_check_hw_state: starting"
+					" watchdog timer"));
+			mod_timer(&nxgep->wd_timer, jiffies);
+		} else if (nxgep->hw_st_timer_stopped == B_TRUE) {
+				nxgep->hw_st_timer_stopped = B_FALSE;
+				NXGE_DEBUG_MSG((nxgep, MOD_CTL,
+						"nxge_check_hw_state: timer"
+						" already pending, resetting"
+						" stop flag"));
+		}
+	} else {
+/* 		if (timer_pending(&nxgep->wd_timer)) { */
+				nxgep->hw_st_timer_stopped = B_TRUE;
+/* 		} */
+	}
+	NXGE_DEBUG_MSG((nxgep, MOD_CTL, "<== nxge_check_hw_state"));
+	return;
+}
+
+void
+nxge_virint_regs_dump(p_nxge_t nxgep)
+{
+	npi_handle_t	handle;
+
+	NXGE_DEBUG_MSG((nxgep, INT_CTL, "==> nxge_virint_regs_dump"));
+	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	(void) npi_vir_dump_pio_fzc_regs_one(handle);
+	(void) npi_vir_dump_ldgnum(handle);
+	(void) npi_vir_dump_ldsv(handle);
+	(void) npi_vir_dump_imask0(handle);
+	(void) npi_vir_dump_sid(handle);
+	(void) npi_mac_dump_regs(handle, nxgep->function_num);
+	(void) npi_ipp_dump_regs(handle, nxgep->function_num);
+	(void) npi_fflp_dump_regs(handle);
+	NXGE_DEBUG_MSG((nxgep, INT_CTL, "<== nxge_virint_regs_dump"));
+}
+
+void
+nxge_int_regs_dump(p_nxge_t nxgep)
+{
+	npi_handle_t	handle;
+
+	NXGE_DEBUG_MSG((nxgep, IOC_CTL, "==> nxge_int_regs_dump"));
+	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+	(void) npi_vir_dump_pio_fzc_regs_one(handle);
+
+	(void) npi_vir_dump_ldgnum(handle);
+	(void) npi_vir_dump_ldsv(handle);
+	(void) npi_vir_dump_imask0(handle);
+	(void) npi_vir_dump_sid(handle);
+
+	NXGE_DEBUG_MSG((nxgep, IOC_CTL, "<== nxge_int_regs_dump"));
+}
+
+/***********************************************************************
+ *                 Driver Functions
+ *
+ **********************************************************************/
+
+
+nxge_status_t
+nxge_tx_port_timeout(p_nxge_t nep)
+{
+
+
+	if (netif_queue_stopped(nep->dev)) {
+		netif_wake_queue(nep->dev);
+	}
+
+	return (NXGE_OK);
+
+}
+
+static void nxge_tx_timeout(struct net_device *netdev)
+{
+	p_nxge_t nep = netdev_priv(netdev);
+
+	NXGE_DEBUG_MSG((nep, MOD_CTL, "==> nxge_tx_timeout"));
+
+	atomic_inc(&nep->rst_pend_cnt);
+	nxge_tx_port_timeout(nep);
+
+	NXGE_DEBUG_MSG((nep, MOD_CTL, "<== nxge_tx_timeout"));
+}
+
+
+static void nxge_watchdog_timer(unsigned long data)
+{
+	p_nxge_t nep = (p_nxge_t) data;
+	nxge_status_t status = NXGE_OK;
+
+	NXGE_DEBUG_MSG((nep, MOD_CTL, "==>nxge_watchdog_timer: "
+			"link_timer_stopped[%d] hw_st_timer_stopped[%d]",
+			nep->link_timer_stopped, nep->hw_st_timer_stopped));
+
+	if (nep->link_timer_stopped == B_FALSE) {
+		switch (nep->mac.portmode) {
+		case PORT_10G_FIBER:
+		case PORT_10G_SERDES:
+			status = nxge_check_10g_link(nep);
+			NXGE_DEBUG_MSG((nep, MOD_CTL, "nxge_watchdog_timer: "
+					"nxge_check_10g_link returned status"
+					" 0x%x", status));
+			break;
+		case PORT_1G_COPPER:
+		case PORT_1G_RGMII_FIBER:
+		case PORT_1G_FIBER:
+		case PORT_1G_SERDES:
+			status = nxge_check_mii_link(nep);
+			NXGE_DEBUG_MSG((nep, MOD_CTL, "nxge_watchdog_timer: "
+					"nxge_check_mii_link returned status"
+					" 0x%x",
+					status));
+			break;
+		default:
+			break;
+		}
+	}
+
+
+
+	if (nep->hw_st_timer_stopped == B_FALSE) {
+		if ((++nep->timer_ticks & 0x3)) {
+			nxge_ipp_handle_sys_errors(nep);
+			nxge_zcp_handle_sys_errors(nep);
+			nxge_fflp_handle_sys_errors(nep);
+		}
+	}
+
+	if ((nep->hw_st_timer_stopped == B_FALSE) ||
+		(nep->link_timer_stopped == B_FALSE))
+		mod_timer(&nep->wd_timer, jiffies + NXGE_LINK_TIMEOUT);
+	NXGE_DEBUG_MSG((nep, MOD_CTL, "<== nxge_watchdog_timer: "));
+}
+
+static int nxge_start(struct net_device *dev)
+{
+	p_nxge_t	nep = netdev_priv(dev);
+	int		hw_was_up, err;
+	int		status;
+
+	NXGE_DEBUG_MSG((nep, MOD_CTL, "==> nxge_start, hw running[%d]",
+			nep->hw_running));
+
+	hw_was_up = nep->hw_running;
+	if (nep->opened) {
+		return (0);
+	}
+
+	nxge_link_monitor(nep, LINK_MONITOR_START);
+	nep->hw_st_timer_stopped = B_FALSE;
+
+	/*
+	 * Allocate system memory for the receive/transmit descriptor rings.
+	 */
+	status = nxge_alloc_cntl_pool(nep);
+	if (status != NXGE_OK) {
+		err = -ENOMEM;
+		goto nxge_start_fail;
+	}
+
+	/*
+	 * Initialize and enable TXC registers
+	 * (Globally enable TX controller,
+	 *  enable a port, configure dma channel bitmap,
+	 *  configure the max burst size)
+	 */
+	status = nxge_txc_init(nep);
+	if (status != NXGE_OK) {
+		err = -ENODEV;
+		goto nxge_start_fail1;
+	}
+
+	/*
+	 * Initialize and enable TXDMA channels.
+	 */
+	status = nxge_init_txdma(nep);
+	if (status != NXGE_OK) {
+		err = -ENOMEM;
+		goto nxge_start_fail2;
+	}
+
+	nxge_init_tx_lb_policy(nep);
+	/*
+	 * Initialize and enable RXDMA channels.
+	 */
+	status = nxge_init_rxdma(nep);
+	if (status != NXGE_OK) {
+		err = -ENOMEM;
+		goto nxge_start_fail3;
+	}
+
+       /*
+	* Initialize TCAM and FCRAM (Neptune).
+	*/
+	status = nxge_classify_init(nep);
+	if (status != NXGE_OK) {
+		err = -ENOMEM;
+		goto nxge_start_fail4;
+	}
+
+	/* Initialize IPP. */
+	status = nxge_ipp_init(nep);
+	if (status != NXGE_OK) {
+		err = -ENODEV;
+		goto nxge_start_fail5;
+	}
+
+	/*
+	 * Initialize ZCP.
+	 */
+	status = nxge_zcp_init(nep);
+	if (status != NXGE_OK) {
+		err = -ENODEV;
+		goto nxge_start_fail6;
+	}
+
+	/*
+	 * Initialize and enable the MAC.
+	 */
+	status = nxge_mac_init(nep);
+	if (status != NXGE_OK) {
+		err = -ENODEV;
+		goto nxge_start_fail7;
+	}
+
+	if (nep->msg_min < NXGE_MSG_MIN) {
+		nep->msg_min = NXGE_MSG_MIN;
+		NXGE_DEBUG_MSG((nep, TX_CTL,
+					"set min length to nep->msg_min "));
+	} else {
+		NXGE_DEBUG_MSG((nep, TX_CTL,
+				"set min length:already set to nep->msg_min "));
+	}
+
+	/* add interrupts */
+	if (nxge_add_intrs(nep) != NXGE_OK) {
+		err = -EAGAIN;
+		goto err_out_intrs;
+	}
+
+
+	tasklet_init(&nep->tx_task, nxge_tx_softintr, (unsigned long)nep);
+	nep->tx_softintr_sched = B_FALSE;
+
+#ifdef USE_NAPI
+	skb_queue_head_init(&nep->rx_pktq);
+#endif
+
+	nep->opened = 1;
+	nep->drv_state |= STATE_HW_INITIALIZED;
+
+
+	/* Start timer to check hw state */
+	nxge_check_hw_state(nep, B_TRUE);
+
+	NXGE_DEBUG_MSG((nep, MOD_CTL, "<== nxge_start"));
+	return 0;
+
+err_out_intrs:
+	printk(KERN_INFO "nxge_start: err_out_intrs\n");
+	nxge_remove_intrs(nep);
+
+nxge_start_fail7:
+	NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_start: nxge_start_fail7\n"));
+nxge_start_fail6:
+	NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_start: nxge_start_fail6\n"));
+nxge_start_fail5:
+	NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_start: nxge_start_fail5\n"));
+nxge_start_fail4:
+	NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_start: nxge_start_fail4\n"));
+	nxge_uninit_rxdma(nep);
+nxge_start_fail3:
+	NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_start: nxge_start_fail3\n"));
+	nxge_uninit_txdma(nep);
+nxge_start_fail2:
+	NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_start: nxge_start_fail2\n"));
+	nxge_txc_uninit(nep);
+nxge_start_fail1:
+	NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_start: nxge_start_fail1\n"));
+	nxge_free_cntl_pool(nep);
+nxge_start_fail:
+	NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "<== nxge_start, error=%d\n", err));
+	return err;
+}
+
+
+static int nxge_open(struct net_device *dev)
+{
+	p_nxge_t	nep = netdev_priv(dev);
+	int		status;
+
+	NXGE_DEBUG_MSG((nep, MOD_CTL, "==> nxge_open, hw running[%d]",
+			nep->hw_running));
+
+	status = nxge_start(dev);
+	if (status != 0) {
+		goto nxge_open_fail;
+	}
+
+	if (nep->statsp->mac_stats.link_up == 1) {
+		netif_carrier_on(dev);
+		netif_start_queue(dev);
+	}
+
+	NXGE_DEBUG_MSG((nep, MOD_CTL, "<== nxge_open"));
+	return status;
+
+nxge_open_fail:
+
+	NXGE_DEBUG_MSG((nep, MOD_CTL, "<== nxge_open, error=#####[%d]#####",
+			status));
+	return status;
+}
+
+
+static int nxge_stop(struct net_device *dev)
+{
+	unsigned long flags;
+	p_nxge_t nep = netdev_priv(dev);
+
+	MUTEX_ENTER_INT(&nep->lock, flags);
+	NXGE_DEBUG_MSG((nep, MOD_CTL, "==> nxge_stop"));
+
+	if (nep->opened == 0) {
+		MUTEX_EXIT_INT(&nep->lock, flags);
+		NXGE_DEBUG_MSG((nep, MOD_CTL, "<== nxge_stop: device already"
+				" stopped"));
+		return (0);
+	}
+
+
+	nep->opened = 0;
+
+	nxge_link_monitor(nep, LINK_MONITOR_STOP);
+	nep->hw_st_timer_stopped = B_TRUE;
+
+	nxge_intr_hw_disable(nep);
+
+	/*
+	 * Reset the receive MAC side.
+	 */
+	nxge_rx_mac_disable(nep);
+
+	/* Disable and soft reset the IPP */
+	nxge_ipp_disable(nep);
+
+	nxge_classify_exit_sw(nep);
+
+	nxge_clean_tx(nep);
+
+	/*
+	 * Reset the transmit/receive DMA side.
+	 */
+	nxge_txdma_hw_mode(nep, NXGE_DMA_STOP);
+	nxge_rxdma_hw_mode(nep, NXGE_DMA_STOP);
+
+	nxge_uninit_txdma(nep);
+	nxge_uninit_rxdma(nep);
+
+#ifdef USE_NAPI
+	skb_queue_purge(&nep->rx_pktq);
+#endif
+
+	/*
+	 * Reset the transmit MAC side.
+	 */
+	nxge_tx_mac_disable(nep);
+
+	nxge_free_cntl_pool(nep);
+
+	nxge_hw_reset(nep);
+
+	nep->drv_state = 0;
+
+	tasklet_kill(&nep->tx_task);
+
+	nxge_remove_intrs(nep);
+
+	MUTEX_EXIT_INT(&nep->lock, flags);
+
+	NXGE_DEBUG_MSG((NULL, MOD_CTL, "<== nxge_stop"));
+	return 0;
+}
+
+
+static int nxge_close(struct net_device *dev)
+{
+
+
+	NXGE_DEBUG_MSG((NULL, MOD_CTL, "==> nxge_close"));
+
+	netif_carrier_off(dev);
+	netif_stop_queue(dev);
+
+	nxge_stop(dev);
+
+	NXGE_DEBUG_MSG((NULL, MOD_CTL, "<== nxge_close"));
+	return 0;
+}
+
+static int nxge_init_device(p_nxge_t nep)
+{
+	int nept_index = 0;
+	NXGE_DEBUG_MSG((NULL, MOD_CTL, "==> nxge_init_device"));
+
+	atomic_inc(&nxge_common_lock_init);
+	if (atomic_read(&nxge_common_lock_init) == 1) {
+		MUTEX_INIT(&nxge_common_lock, 0, 0, 0);
+		MUTEX_ENTER(&nxge_common_lock);
+		NXGE_DEBUG_MSG((nep, MOD_CTL,
+						" Very First Neptune:"
+						" First neptune Device: Function %d neptune id %x",
+						nep->function_num,
+						nep->neptune_id));
+		for (nept_index = 0; nept_index < NXGE_MAX_NEPTUNE_SYSTEM;
+			 nept_index++) {
+			memset((void *)&neptune[nept_index],
+				   0, sizeof(nxge_neptune_t));
+		}
+		atomic_inc(&nxge_common_lock_init_done);
+		MUTEX_EXIT(&nxge_common_lock);
+	}
+
+
+	MUTEX_ENTER(&nxge_common_lock);
+	for (nept_index = 0; nept_index < NXGE_MAX_NEPTUNE_SYSTEM;
+		 nept_index++) {
+		if (neptune[nept_index].magic == 0) {
+				/* this is the first time for this neptune*/
+			neptune[nept_index].magic = NXGE_NEPTUNE_MAGIC;
+			neptune[nept_index].neptune_id = nep->neptune_id;
+			MUTEX_INIT(&neptune[nept_index].nxge_cfg_lock, 0, 0, 0);
+			MUTEX_INIT(&neptune[nept_index].nxge_mdio_lock, 0, 0, 0);
+			MUTEX_INIT(&neptune[nept_index].nxge_syserr_lock, 0, 0, 0);
+			MUTEX_INIT(&neptune[nept_index].nxge_tcam_lock, 0, 0, 0);
+			MUTEX_INIT(&neptune[nept_index].nxge_vlan_lock, 0, 0, 0);
+			nep->neptune_index = nept_index;
+			nep->cfg_lock = &neptune[nept_index].nxge_cfg_lock;
+			nep->link_lock = &neptune[nept_index].nxge_mdio_lock;
+			nep->syserr_lock = &neptune[nept_index].nxge_syserr_lock;
+			nep->tcam_lock = &neptune[nept_index].nxge_tcam_lock;
+			nep->vlan_lock = &neptune[nept_index].nxge_vlan_lock;
+			nep->syserr = &neptune[nept_index].syserr;
+			neptune[nept_index].active.func[nep->function_num] = 1;
+			NXGE_DEBUG_MSG((nep, MOD_CTL,
+						" nxge_init_device: NEW: neptune Index %d"
+						" First neptune Device: Function %d on neptune id %x",
+							nep->neptune_index, nep->function_num,
+							neptune[nept_index].neptune_id));
+			break;
+		} else if ((neptune[nept_index].magic == NXGE_NEPTUNE_MAGIC) &&
+				   (neptune[nept_index].neptune_id == nep->neptune_id)) {
+			nep->neptune_index = nept_index;
+			nep->cfg_lock = &neptune[nept_index].nxge_cfg_lock;
+			nep->link_lock = &neptune[nept_index].nxge_mdio_lock;
+			nep->syserr_lock = &neptune[nept_index].nxge_syserr_lock;
+			nep->tcam_lock = &neptune[nept_index].nxge_tcam_lock;
+			nep->vlan_lock = &neptune[nept_index].nxge_vlan_lock;
+			nep->syserr = &neptune[nept_index].syserr;
+			neptune[nept_index].active.func[nep->function_num] = 1;
+			NXGE_DEBUG_MSG((nep, MOD_CTL,
+							" nxge_init_device: existing neptune Index %d"
+							" Function %d on already inited neptune id %x",
+							nep->neptune_index, nep->function_num,
+							neptune[nept_index].neptune_id));
+			break;
+		} else {
+
+			NXGE_DEBUG_MSG((nep, MOD_CTL,
+							" nxge_init_device:"
+							" searching for neptune on Index %d"
+							" Function %d: Din't find neptune id %x",
+							nept_index, nep->function_num,
+							nep->neptune_id));
+		}
+	}
+
+	if (nept_index == NXGE_MAX_NEPTUNE_SYSTEM) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+						"nxge_init_device: Too many Neptunes"));
+		MUTEX_EXIT(&nxge_common_lock);
+		return (-1);
+	}
+	MUTEX_EXIT(&nxge_common_lock);
+	NXGE_DEBUG_MSG((NULL, MOD_CTL, "<== nxge_init_device"));
+	return 0;
+}
+
+
+static void nxge_uninit_device(p_nxge_t nep)
+{
+	int nept_index = 0;
+
+	NXGE_DEBUG_MSG((NULL, MOD_CTL, "==> nxge_uninit_device"));
+
+	if (atomic_read(&nxge_common_lock_init) == 0) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_uninit_device: "
+			       "nxge_common block not initialized  !!!!!!!!"));
+		return;
+	}
+
+	MUTEX_ENTER(&nxge_common_lock);
+
+	for (nept_index = 0; nept_index < NXGE_MAX_NEPTUNE_SYSTEM;
+		 nept_index++) {
+		if ((neptune[nept_index].magic == NXGE_NEPTUNE_MAGIC) &&
+			(neptune[nept_index].neptune_id == nep->neptune_id)) {
+				/* this is the my neptune*/
+			NXGE_DEBUG_MSG((nep, MOD_CTL,
+						" nxge_uninit_device: neptune Index %d"
+						" Function %d on uninit neptune id %x",
+							nep->neptune_index, nep->function_num,
+							neptune[nept_index].neptune_id));
+			neptune[nept_index].active.func[nep->function_num] = 0;
+			if (neptune[nept_index].active.all == 0) {
+				NXGE_DEBUG_MSG((nep, MOD_CTL,
+						" nxge_uninit_device: neptune Index %d"
+						" last neptune Device: Function %d on neptune id %x",
+							nep->neptune_index, nep->function_num,
+							neptune[nept_index].neptune_id));
+				/* this is the last function for this neptune*/
+				neptune[nept_index].magic = 0;
+				neptune[nept_index].neptune_id = 0;
+
+				MUTEX_DESTROY(&neptune[nept_index].nxge_vlan_lock);
+				MUTEX_DESTROY(&neptune[nept_index].nxge_tcam_lock);
+				MUTEX_DESTROY(&neptune[nept_index].nxge_mdio_lock);
+				MUTEX_DESTROY(&neptune[nept_index].nxge_syserr_lock);
+				MUTEX_DESTROY(&neptune[nept_index].nxge_cfg_lock);
+				memset((void *)&neptune[nept_index],
+				   0, sizeof(nxge_neptune_t));
+				nep->neptune_index = 0;
+			}
+			break;
+		} else {
+			NXGE_DEBUG_MSG((nep, MOD_CTL,
+				"nxge_uninit_device:"
+				" neptune function %d id %d not found on index",
+				nep->function_num, nep->neptune_id, nept_index));
+		}
+	}
+
+	if (nept_index == NXGE_MAX_NEPTUNE_SYSTEM) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+					"nxge_uninit_device:"
+					" No Neptune is found for this device"));
+	}
+
+	MUTEX_EXIT(&nxge_common_lock);
+	atomic_dec(&nxge_common_lock_init);
+	if (atomic_read(&nxge_common_lock_init) == 0) {
+		MUTEX_DESTROY(&nxge_common_lock);
+		NXGE_DEBUG_MSG((nep, MOD_CTL,
+						" Very Last Neptune:"
+						" First neptune Device: Function %d neptune id %x",
+						nep->function_num,
+						nep->neptune_id));
+	}
+
+	NXGE_DEBUG_MSG((NULL, MOD_CTL, "<== nxge_uninit_device"));
+	return;
+}
+
+
+static void nxge_unmap_regs(p_nxge_t nep)
+{
+	if (nep->dev_regs.nxge_regp != 0UL) {
+		iounmap((void *) nep->dev_regs.nxge_regp);
+		nep->dev_regs.nxge_regp = 0UL;
+	}
+
+	if (nep->dev_regs.nxge_msix_regp != 0UL) {
+		iounmap((void *) nep->dev_regs.nxge_msix_regp);
+		nep->dev_regs.nxge_msix_regp = 0UL;
+	}
+}
+
+static int nxge_map_regs(p_nxge_t nep)
+{
+	unsigned long nxge_reg_base, nxge_reg_len;
+	struct pci_dev *pdev = nep->pdev;
+	int status = 0;
+
+	NXGE_DEBUG_MSG((nep, MOD_CTL, "==> nxge_map_regs"));
+
+	nxge_reg_base = pci_resource_start(pdev, 0);
+	nxge_reg_len = pci_resource_len(pdev, 0);
+
+	NXGE_DEBUG_MSG((nep, MOD_CTL, "nxge_map_regs: "
+			"reg_base[0x%x], reg_len[%d]\n",
+			nxge_reg_base, nxge_reg_len));
+
+	NXGE_DEBUG_MSG((nep, MOD_CTL, "nxge_map_regs: "
+			"pdev->resource, start[0x%x], end[0x%x]\n",
+			pdev->resource[0].start,
+			pdev->resource[0].end));
+
+	nep->dev_regs.nxge_regp = (caddr_t) ioremap(nxge_reg_base,
+						    nxge_reg_len);
+	if (nep->dev_regs.nxge_regp == 0UL) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_map_regs: "
+				"Cannot map device registers, aborting.\n"));
+		status = -1;
+		goto nxge_map_regs_fail1;
+	}
+	nep->dev_regs.dev_reg_len = nxge_reg_len;
+	nep->npi_handle.regh = NULL;
+	nep->npi_handle.regp = nep->dev_regs.nxge_regp;
+	nep->npi_handle.function.function = nep->function_num;
+	nep->npi_handle.function.instance = 0;
+	nep->npi_handle.nxgep = (void *) nep;
+
+	NXGE_DEBUG_MSG((nep, MOD_CTL, "nxge_map_regs: reg addr 0x%x ",
+			nep->dev_regs.nxge_regp));
+
+	nep->dev_regs.nxge_msix_regp = 0UL;
+
+
+	status = 0;
+
+	goto nxge_map_regs_exit;
+
+nxge_map_regs_fail1:
+	nxge_unmap_regs(nep);
+	goto nxge_map_regs_exit;
+
+nxge_map_regs_exit:
+	NXGE_DEBUG_MSG((nep, MOD_CTL, "<== nxge_map_regs"));
+	return (status);
+}
+
+
+static boolean_t
+nxge_check_dev_function_valid(struct pci_dev *pdev, int function)
+{
+	boolean_t valid = B_TRUE;
+	unsigned long tmp_reg_base, tmp_reg_len;
+	uint64_t num_ports = 0;
+	caddr_t maddr;
+	npi_handle_t handle;
+	npi_vpd_info_t vpd_info;
+	npi_status_t status;
+		/* functions 0 & 1 always valid */
+	if (function < 2)
+		return B_TRUE;
+
+	tmp_reg_base = pci_resource_start(pdev, 0);
+	tmp_reg_len = pci_resource_len(pdev, 0);
+	if (!(request_mem_region(tmp_reg_base, tmp_reg_len, "nxge"))) {
+		NXGE_ERROR_MSG((NULL, NXGE_ERR_CTL,
+						"nxge_init_one: request_mem_region "
+						"failed for bar 0"));
+		return B_FALSE;
+	}
+	maddr = (caddr_t) ioremap(tmp_reg_base, tmp_reg_len);
+
+	handle.regh = NULL;
+	handle.regp = maddr;
+	handle.function.function = function;
+	handle.function.instance = 0;
+	handle.nxgep = (void *) NULL;
+
+
+	MUTEX_ENTER(&nxge_eeprom_lock);
+	npi_espc_pio_enable(handle);
+	status = npi_espc_vpd_info_get(handle, &vpd_info,
+				       NXGE_EROM_LEN);
+	npi_espc_pio_disable(handle);
+	MUTEX_EXIT(&nxge_eeprom_lock);
+
+	if (status == NPI_SUCCESS) {
+		if ((strncmp(vpd_info.bd_model, NXGE_2XGF_LP_BM_STR,
+			    strlen(NXGE_2XGF_LP_BM_STR)) == 0) ||
+		   (strncmp(vpd_info.bd_model, NXGE_2XGF_PEM_BM_STR,
+			    strlen(NXGE_2XGF_PEM_BM_STR)) == 0)) {
+			valid = B_FALSE;
+		}
+	} else {
+		NXGE_ERROR_MSG((NULL, MOD_CTL, "nxge_init_one: nxge%d: "
+				"VPD read failed...Trying to read SEEPROM",
+				function));
+		num_ports = readq(maddr + ESPC_NUM_PORTS_MACS);
+		num_ports &= NUM_PORTS_MASK;
+		if (num_ports == 0) {
+			NXGE_ERROR_MSG((NULL, MOD_CTL, "nxge_init_one: "
+					"nxge%d: SEEPROM read failed..."
+					"Detaching function %d",
+					function));
+			iounmap((void *) maddr);
+			release_mem_region(tmp_reg_base, tmp_reg_len);
+			return (B_FALSE);
+		}
+		if (num_ports != 4)
+			valid = B_FALSE;
+	}
+
+	iounmap((void *) maddr);
+	release_mem_region(tmp_reg_base, tmp_reg_len);
+
+	return valid;
+}
+
+static int __devinit nxge_init_one(struct pci_dev *pdev,
+				   const struct pci_device_id *ent)
+{
+	struct net_device *dev;
+	p_nxge_t nep = NULL;
+	int error;
+	int use_dac = 0;
+	nxge_status_t status = NXGE_OK;
+	uint16_t pcie_dev_ctl, pcie_reg_offset;
+
+	NXGE_DEBUG_MSG((NULL, MOD_CTL, "==> nxge_init_one"));
+
+	error = pci_enable_device(pdev);
+
+	if (error) {
+		NXGE_ERROR_MSG((NULL, NXGE_ERR_CTL,
+				"Cannot enable PCI device, aborting.\n"));
+		return error;
+	}
+
+	if (!(pci_resource_flags(pdev, 0) & IORESOURCE_MEM)) {
+		NXGE_ERROR_MSG((NULL, NXGE_ERR_CTL, "nxge_init_one: "
+			       "Cannot find BAR0 PCI device base "
+			       "address, aborting.\n"));
+		error = -ENODEV;
+		goto err_out_disable_pdev;
+	}
+
+	if (!(pci_resource_flags(pdev, 2) & IORESOURCE_MEM)) {
+		NXGE_ERROR_MSG((NULL, NXGE_ERR_CTL, "nxge_init_one: "
+			       "Cannot find BAR2 base "
+			       "address, aborting.\n"));
+		error = -ENODEV;
+		goto err_out_disable_pdev;
+	}
+
+	atomic_inc(&nxge_eeprom_lock_init);
+	if (atomic_read(&nxge_eeprom_lock_init) == 1) {
+		MUTEX_INIT(&nxge_eeprom_lock, 0, 0, 0);
+	}
+
+/*
+ * check if this nxge function is usable
+ * In 10G cards, only the first two functions are
+ * valid and usable
+ */
+	if (nxge_check_dev_function_valid(pdev, pdev->devfn) == B_FALSE) {
+		error = ENODEV;
+		NXGE_DEBUG_MSG((NULL, MOD_CTL, "nxge_init_one:  "
+			       " Invalid device function[%d], aborting.",
+					pdev->devfn));
+		atomic_dec(&nxge_eeprom_lock_init);
+		if (atomic_read(&nxge_eeprom_lock_init) == 0)
+			MUTEX_DESTROY(&nxge_eeprom_lock);
+
+		goto err_out_disable_pdev;
+	}
+
+
+	dev = alloc_etherdev(sizeof(nxge_t));
+	if (!dev) {
+		NXGE_ERROR_MSG((NULL, NXGE_ERR_CTL, "nxge_init_one:  "
+			       "Etherdev alloc failed, aborting."));
+		error = -ENOMEM;
+		goto err_out_free_res;
+	}
+	SET_MODULE_OWNER(dev);
+	SET_NETDEV_DEV(dev, &pdev->dev);
+
+	nep = netdev_priv(dev);
+	nep->pdev = pdev;
+	nep->dev = dev;
+
+	nep->neptune_id = pdev->bus->number;
+	nep->function_num = nxge_pci_func(pdev->devfn);
+
+	/* Do per Neptune allocs and initializations if first instance */
+	nxge_init_device(nep);
+#ifdef CONFIG_PCI_MSI
+	if (nxge_msix_enable) {
+			/* fix this to read from props to see if MSIX is supported */
+		if (!(request_mem_region(pci_resource_start(pdev, 0),
+                         pci_resource_len(pdev, 0), dev->name))) {
+                        NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+					"nxge_init_one: request_mem_region "
+					"failed for bar 0"));
+                        error = ENODEV;
+                }
+                if (!(request_mem_region(pci_resource_start(pdev, 4),
+                         pci_resource_len(pdev, 4), dev->name))) {
+                        NXGE_ERROR_MSG((nep, NXGE_ERR_CTL,
+					"nxge_init_one: request_mem_region "
+					"failed for bar 4"));
+                        error = ENODEV;
+                        release_mem_region(pci_resource_start(pdev, 0),
+                                   pci_resource_len(pdev, 0));
+                }
+
+	} else {
+		error = pci_request_regions(pdev, dev->name);
+	}
+#else
+		error = pci_request_regions(pdev, dev->name);
+#endif
+
+	if (error) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_init_one: "
+				"PCI pci_request_regions failed, aborting.\n"));
+		goto err_out_free_netdev;
+	}
+
+	pci_set_master(pdev);
+
+	pcie_reg_offset = PCI_EXTENDED_BASE + PCI_EXP_DEVCTL;
+		/*
+		 * some AMD systems doesn't configure the nVidia chip
+		 * and the config space of the pcie device to matching
+		 * read size.
+		 * the optimal  MAX_READ_SIZE value for the nVidia
+		 * CK08-04 controller is 0 ( corresponding to 128 byte)
+		 * Also, some controllers need no snoop bit cleared
+		 * to function properly.
+		 *
+		 */
+	pcie_dev_ctl  = 0x0;
+	pci_read_config_word(pdev, pcie_reg_offset, &pcie_dev_ctl);
+	pcie_dev_ctl  &= ~PCI_EXP_DEVCTL_NOSNOOP_EN;
+	pcie_dev_ctl  |= PCI_EXP_DEVCTL_RELAX_EN;
+	pcie_dev_ctl  |= (PCI_EXP_DEVCTL_CERE | PCI_EXP_DEVCTL_NFERE |
+					  PCI_EXP_DEVCTL_FERE | PCI_EXP_DEVCTL_URRE);
+
+	pci_write_config_word(pdev, pcie_reg_offset, pcie_dev_ctl);
+
+	/* Configure DMA attributes. */
+
+	if (!pci_set_dma_mask(pdev, NXGE_MAX_ADDRESS_BITS)) {
+		use_dac = 1;
+		error = pci_set_consistent_dma_mask(pdev, NXGE_MAX_ADDRESS_BITS);
+		if (error < 0) {
+			NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_init_one: "
+							"64-bit DMA  unavailable for consistent mode"));
+			goto err_out_free_res;
+		}
+
+	} else {
+		error = pci_set_dma_mask(pdev, 0xffffffffULL);
+		if (error) {
+			NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_init_one: "
+							"No DMA available: abort attaching "));
+			goto err_out_free_res;
+		}
+	}
+
+
+	nep->drv_state = 0;
+
+	NXGE_DEBUG_MSG((nep, MOD_CTL, "nxge_init_one: "
+			"Function Number [%d] Device[0x%x]",
+			nep->function_num, pdev->device));
+	NXGE_DEBUG_MSG((nep, MOD_CTL, "nxge_init_one: "
+			"*******PCI_SLOT [%d] PCI_FUNC [%d]*********",
+			PCI_SLOT(pdev->devfn), PCI_FUNC(pdev->devfn)));
+
+	if (nxge_map_regs(nep) < 0) {
+		goto err_out_iounmap;
+	}
+
+	nep->msg_enable = (nxge_debug < 0) ? NXGE_DEF_MSG_ENABLE : nxge_debug;
+
+	memset(nep->hash_table, 0, sizeof (nep->hash_table));
+	memset(nep->alt_addr_arr, 0, sizeof (nep->alt_addr_arr));
+	nep->if_flags = 0;
+	nep->ep = NULL;
+
+	MUTEX_INIT(&nep->lock, 0, 0, 0);
+	MUTEX_INIT(&nep->stats_lock, 0, 0, 0);
+
+	nep->link_timer_stopped = B_TRUE;
+	nep->hw_st_timer_stopped = B_TRUE;
+	init_timer(&nep->wd_timer);
+	nep->wd_timer.function = nxge_watchdog_timer;
+	nep->wd_timer.data = (unsigned long) nep;
+	atomic_set(&nep->rst_pend_cnt, 0);
+	atomic_set(&nep->tx_intr_set, 0);
+
+
+	netif_carrier_off(nep->dev);
+	netif_stop_queue(nep->dev);
+	nep->timer_ticks = 0;
+
+	/**********************************/
+
+	PCI_SAVE_STATE(pdev, nep->pci_cfg);
+
+		/* Setup the parameters for the this instance. */
+	nxge_init_param(nep);
+
+	/* Setup the stats for the driver. */
+	nxge_init_stats(nep);
+
+	/*
+	 * Read the hw config properties from seeprom and config
+	 * file/ default, dma, vlan, mac and classification props.
+	 * The flags property is set after this.
+	 */
+	if (nxge_get_config_properties(nep) != NXGE_OK)
+		goto err_out_1;
+
+
+	nxge_setup_param(nep);
+
+	nxge_setup_system_dma_pages(nep);
+	nxge_hw_reset(nep);
+
+	nxge_hw_init_niu_common(nep);
+	nxge_fflp_hw_reset(nep);
+
+	status = nxge_xcvr_find(nep);
+	if (status != NXGE_OK) {
+		NXGE_DEBUG_MSG((nep, MOD_CTL,
+			    " nxge_init_one status "
+			    " (xcvr find 0x%08x)", status));
+		goto err_out_1;
+	}
+
+
+	status = nxge_link_init(nep);
+	if (status != NXGE_OK) {
+		NXGE_DEBUG_MSG((nep, MOD_CTL,
+			    " nxge_init_one status "
+			    "(xcvr init 0x%08x)", status));
+		goto err_out_1;
+	}
+
+	dev->open = nxge_open;
+	dev->stop = nxge_close;
+	dev->hard_start_xmit = nxge_start_xmit;
+	dev->get_stats = nxge_get_netdev_stats;
+	dev->set_multicast_list = nxge_set_multicast;
+	dev->set_mac_address = nxge_set_mac_address;
+	dev->do_ioctl = nxge_ioctl;
+	dev->ethtool_ops = &nxge_ethtool_ops;
+
+	dev->tx_timeout = nxge_tx_timeout;
+	dev->watchdog_timeo = NXGE_TX_TIMEOUT;
+	dev->change_mtu = nxge_change_mtu;
+
+#ifdef USE_NAPI
+	dev->poll = nxge_poll;
+	dev->weight = 16;
+	if (nep->mac.portmode == PORT_10G_FIBER ||
+	    nep->mac.portmode == PORT_10G_COPPER) {
+		dev->weight = 64;
+	}
+#endif
+
+
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	dev->poll_controller = nxge_netpoll;
+#endif
+
+	dev->dma = 0;
+
+	/* Neptune features. */
+	nep->flags |=  (NXGE_FLAG_RX_HW_CSUM |
+					NXGE_FLAG_TX_HW_CSUM |
+					NXGE_FLAG_TSO_ENABLED);
+
+	dev->features |= NETIF_F_IP_CSUM;
+	dev->features |= NETIF_F_LLTX;
+	dev->features |= NETIF_F_SG;
+	dev->features |= NETIF_F_TSO;
+
+	if (use_dac) {
+		dev->features |= NETIF_F_HIGHDMA;
+		NXGE_DEBUG_MSG((nep, MOD_CTL, "nxge_init_one: "
+				"Setting Highmem DMA support.\n"));
+	}
+
+	if (register_netdev(dev)) {
+		NXGE_ERROR_MSG((nep, NXGE_ERR_CTL, "nxge_init_one: "
+				"Cannot register netdevice, aborting."));
+		goto err_out_iounmap;
+	}
+
+
+	pci_set_drvdata(pdev, dev);
+	nep->hw_running = 1;
+
+	nxge_clear_mac_counters(nep);
+
+	if (nep->mac.portmode == PORT_10G_FIBER ||
+		nep->mac.portmode == PORT_10G_COPPER) {
+		nep->tx_reclaim_pending = NXGE_TX_RECLAIM;
+		nep->tx_copy_udp = B_FALSE;
+
+	} else {
+		nep->tx_reclaim_pending = NXGE_TX_RECLAIM * 2;
+		nep->tx_copy_udp = B_TRUE;
+
+	}
+
+	/* Enable the Link monitoring */
+	nxge_link_monitor(nep, LINK_MONITOR_START);
+	nep->hw_st_timer_stopped = B_FALSE;
+	NXGE_DEBUG_MSG((NULL, MOD_CTL, "<== nxge_init_one"));
+
+	return 0;
+
+err_out_1:
+	nep->link_timer_stopped = B_TRUE;
+		/* leave hw in know state */
+	nxge_zcp_init(nep);
+	nxge_ipp_init(nep);
+
+	/* Free param resources. */
+	nxge_destroy_param(nep);
+
+	/* Free stats resources. */
+	nxge_destroy_stats(nep);
+
+	/* Destroy all mutexes. */
+
+	atomic_dec(&nxge_eeprom_lock_init);
+	if (atomic_read(&nxge_eeprom_lock_init) == 0)
+		MUTEX_DESTROY(&nxge_eeprom_lock);
+
+	MUTEX_DESTROY(&nep->stats_lock);
+
+	MUTEX_DESTROY(&nep->lock);
+
+	flush_scheduled_work();
+
+	del_timer_sync(&nep->wd_timer);
+
+err_out_iounmap:
+	NXGE_DEBUG_MSG((NULL, MOD_CTL, "nxge_init_one: err_out_iounmap"));
+	if (nep->dev_regs.nxge_regp != 0UL ||
+	    nep->dev_regs.nxge_msix_regp != 0UL) {
+		if (nep->hw_running)
+			nxge_shutdown(nep);
+	}
+
+	nxge_unmap_regs(nep);
+
+err_out_free_res:
+	NXGE_DEBUG_MSG((NULL, MOD_CTL, "nxge_init_one: err_out_free_res"));
+#ifdef CONFIG_PCI_MSI
+	if (nxge_msix_enable == 1) {
+		release_mem_region(pci_resource_start(pdev, 0),
+                        pci_resource_len(pdev, 0));
+        release_mem_region(pci_resource_start(pdev, 4),
+                       pci_resource_len(pdev, 4));
+
+	} else
+#endif
+		pci_release_regions(pdev);
+
+err_out_free_netdev:
+	NXGE_DEBUG_MSG((NULL, MOD_CTL, "nxge_init_one: err_out_free_netdev"));
+	nxge_uninit_device(nep);
+	free_netdev(dev);
+
+err_out_disable_pdev:
+	NXGE_DEBUG_MSG((NULL, MOD_CTL, "nxge_init_one: err_out_disable_pdev"));
+	pci_disable_device(pdev);
+	pci_set_drvdata(pdev, NULL);
+	NXGE_DEBUG_MSG((NULL, MOD_CTL, "<== nxge_init_one"));
+
+	return -ENODEV;
+}
+
+static void __devexit nxge_remove_one(struct pci_dev *pdev)
+{
+	struct net_device *dev = pci_get_drvdata(pdev);
+	p_nxge_t nep;
+
+	NXGE_DEBUG_MSG((NULL, MOD_CTL, "==> nxge_remove_one"));
+	if (!dev) {
+		NXGE_ERROR_MSG((NULL, NXGE_ERR_CTL, "<== nxge_remove_one: NULL"
+				" net_device"));
+		return;
+	}
+
+	nep = netdev_priv(dev);
+
+	/*
+	 * Stop the device and free resources.
+	 */
+
+	nxge_link_monitor(nep, LINK_MONITOR_STOP);
+	nep->hw_st_timer_stopped = B_TRUE;
+
+
+	nxge_hw_stop(nep);
+
+	/* Free param resources. */
+	nxge_destroy_param(nep);
+
+	/* Free stats resources. */
+	nxge_destroy_stats(nep);
+
+	/*  Destroy all mutexes. */
+
+	MUTEX_DESTROY(&nep->stats_lock);
+	MUTEX_DESTROY(&nep->lock);
+
+	NXGE_DEBUG_MSG((nep, MOD_CTL, "nxge_remove_one b4 unregister_netdev, "
+			"refcnt=%d", dev->refcnt));
+
+	unregister_netdev(dev);
+
+	NXGE_DEBUG_MSG((nep, MOD_CTL, "nxge_remove_one after unregister_netdev, "
+			"refcnt=%d", dev->refcnt));
+	flush_scheduled_work();
+
+	del_timer_sync(&nep->wd_timer);
+
+
+	nxge_unmap_regs(nep);
+	nxge_uninit_device(nep);
+
+	atomic_dec(&nxge_eeprom_lock_init);
+	if (atomic_read(&nxge_eeprom_lock_init) == 0)
+		MUTEX_DESTROY(&nxge_eeprom_lock);
+
+	free_netdev(dev);
+#ifdef CONFIG_PCI_MSI
+	if (nxge_msix_enable == 1) {
+		release_mem_region(pci_resource_start(pdev, 0),
+		pci_resource_len(pdev, 0));
+		release_mem_region(pci_resource_start(pdev, 4),
+		pci_resource_len(pdev, 4));
+
+	} else
+#endif
+		pci_release_regions(pdev);
+	pci_disable_device(pdev);
+	pci_set_drvdata(pdev, NULL);
+	NXGE_DEBUG_MSG((NULL, MOD_CTL, "<== nxge_remove_one"));
+}
+
+#ifdef CONFIG_PM
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 10)
+static int nxge_suspend(struct pci_dev *pdev, pm_message_t state)
+#else
+static int nxge_suspend(struct pci_dev *pdev, uint32_t state)
+#endif
+{
+	struct net_device *dev = pci_get_drvdata(pdev);
+	p_nxge_t nep = netdev_priv(dev);
+
+	NXGE_DEBUG_MSG((NULL, MOD_CTL, "==> nxge_suspend"));
+
+	if (!netif_running(dev))
+	return (0);
+
+	if (nep->opened) {
+		netif_device_detach(dev);
+
+	}
+
+	if (nep->hw_running) {
+		nxge_shutdown(nep);
+	}
+
+	PCI_SAVE_STATE(pdev, nep->pci_cfg);
+	pci_set_power_state(pdev, 3);
+
+	NXGE_DEBUG_MSG((NULL, MOD_CTL, "<== nxge_suspend"));
+
+	return (0);
+}
+
+static int nxge_resume(struct pci_dev *pdev)
+{
+#if 0
+	struct net_device *dev = pci_get_drvdata(pdev);
+
+	NXGE_ERROR_MSG((NULL, NXGE_ERR_CTL, "==> nxge_resume"));
+
+	if (!netif_running(dev))
+	   return (0);
+
+	pci_set_power_state(pdev, 0);
+	PCI_RESTORE_STATE(pdev);
+	pci_enable_device(dev);
+	netif_device_attach(dev);
+	nxge_hw_start(dev);
+
+#endif
+	NXGE_DEBUG_MSG((NULL, MOD_CTL, "<== nxge_resume"));
+	return (0);
+}
+#endif /* CONFIG_PM */
+
+static struct pci_driver nxge_driver = {
+	.name		= DRV_MODULE_NAME,
+	.id_table	= nxge_pci_tbl,
+	.probe		= nxge_init_one,
+	.remove		= __devexit_p(nxge_remove_one),
+#ifdef CONFIG_PM
+	.suspend	= nxge_suspend,
+	.resume		= nxge_resume
+#endif
+};
+
+static int __init nxge_init(void)
+{
+	int status = 0;
+
+	NXGE_DEBUG_MSG((NULL, MOD_CTL, "==> nxge_init"));
+	NXGE_DEBUG_MSG((NULL, MOD_CTL, "%s", version));
+
+	status = PCI_DRIVER_INIT(&nxge_driver);
+
+	if (status < 0) {
+		NXGE_ERROR_MSG((NULL, NXGE_ERR_CTL,
+				    "pci_module_init failed with err[%d]",
+				    status));
+	}
+	NXGE_DEBUG_MSG((NULL, MOD_CTL, "<== nxge_init"));
+	return (status);
+}
+
+static void __exit nxge_cleanup(void)
+{
+	NXGE_DEBUG_MSG((NULL, MOD_CTL, "==> nxge_cleanup"));
+	pci_unregister_driver(&nxge_driver);
+	NXGE_DEBUG_MSG((NULL, MOD_CTL, "<== nxge_cleanup"));
+}
+
+module_init(nxge_init);
+module_exit(nxge_cleanup);
+
diff --git a/drivers/net/nxge/nxge_param.c b/drivers/net/nxge/nxge_param.c
new file mode 100644
index 0000000..8d8af1a
--- /dev/null
+++ b/drivers/net/nxge/nxge_param.c
@@ -0,0 +1,2967 @@
+/*
+ * nxge_param.c	Neptune Configuration interface functions
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#include <nxge_impl.h>
+#include <linux/moduleparam.h>
+
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 10)
+#define	NXGE_USE_MODULE_PARAM_NEW_NAMED
+#endif
+
+extern uint64_t npi_debug_level;
+extern uint32_t	nxge_msix_enable;
+
+#define	NXGE_PARAM_MAC_RW	NXGE_PARAM_RW | NXGE_PARAM_MAC | \
+	NXGE_PARAM_NDD_WR_OK | NXGE_PARAM_READ_PROP
+
+#define	NXGE_PARAM_MAC_DONT_SHOW	NXGE_PARAM_RW | NXGE_PARAM_MAC | \
+	    NXGE_PARAM_DONT_SHOW
+
+
+#define	NXGE_PARAM_RXDMA_RW	NXGE_PARAM_RW | NXGE_PARAM_RXDMA | \
+	NXGE_PARAM_NDD_WR_OK | NXGE_PARAM_READ_PROP
+
+#define	NXGE_PARAM_RXDMA_RWC	NXGE_PARAM_RWP | NXGE_PARAM_RXDMA | \
+	NXGE_PARAM_INIT_ONLY | NXGE_PARAM_READ_PROP
+
+
+#define	NXGE_PARAM_L2CLASS_CFG	NXGE_PARAM_RW | NXGE_PARAM_PROP_ARR32 | \
+	NXGE_PARAM_READ_PROP | NXGE_PARAM_NDD_WR_OK
+
+
+#define	NXGE_PARAM_CLASS_RWS	NXGE_PARAM_RWS |  \
+	NXGE_PARAM_READ_PROP
+
+#define	NXGE_PARAM_ARRAY_INIT_SIZE	0x20ULL
+
+#define	SET_RX_INTR_TIME_DISABLE 0
+#define	SET_RX_INTR_TIME_ENABLE 1
+#define	SET_RX_INTR_PKTS 2
+
+#define	BASE_ANY	0
+#define	BASE_BINARY	2
+#define	BASE_HEX	16
+#define	BASE_DECIMAL	10
+#define	ALL_FF_64	0xFFFFFFFFFFFFFFFFULL
+#define	ALL_FF_32	0xFFFFFFFFUL
+
+#define	NXGE_NDD_INFODUMP_BUFF_SIZE	2048 /* is 2k enough? */
+
+#define	PARAM_OUTOF_RANGE(vptr, eptr, rval, pa)	\
+	((vptr == eptr) || (rval < pa->minimum) || (rval > pa->maximum))
+
+
+static uint8_t p2_tx_fair[2] = {12, 12};
+static uint8_t p2_tx_equal[2] = {12, 12};
+static uint8_t p4_tx_fair[4] = {8, 8, 4, 4};
+static uint8_t p4_tx_equal[4] = {6, 6, 6, 6};
+
+static uint8_t p2_rx_fair[2] = {8, 8};
+static uint8_t p2_rx_equal[2] = {8, 8};
+static uint8_t p4_rx_fair[4] = {6, 6, 2, 2};
+static uint8_t p4_rx_equal[4] = {4, 4, 4, 4};
+
+static uint8_t p2_rdcgrp_fair[2] = {4, 4};
+static uint8_t p2_rdcgrp_equal[2] = {4, 4};
+static uint8_t p4_rdcgrp_fair[4] = {2, 2, 1, 1};
+static uint8_t p4_rdcgrp_equal[4] = {2, 2, 2, 2};
+
+static int nxge_param_set_mac(p_nxge_t, uint64_t value, caddr_t);
+static int nxge_param_set_port_rdc(p_nxge_t, uint64_t value, caddr_t);
+
+static int nxge_param_set_grp_rdc(p_nxge_t, uint64_t value, caddr_t);
+
+static int nxge_param_set_ether_usr(p_nxge_t, uint64_t value, caddr_t);
+static int nxge_param_set_ip_usr(p_nxge_t, uint64_t value, caddr_t);
+static int nxge_param_set_ip_opt(p_nxge_t, uint64_t value, caddr_t);
+static int nxge_param_set_tx_lb_policy(p_nxge_t, uint64_t value, caddr_t);
+static int nxge_param_set_rx_allow_all(p_nxge_t, uint64_t value, caddr_t);
+static int nxge_param_set_vlan_rdcgrp(p_nxge_t, uint64_t value, caddr_t);
+static int nxge_param_set_mac_rdcgrp(p_nxge_t, uint64_t value, caddr_t);
+static int nxge_param_fflp_hash_init(p_nxge_t, uint64_t value, caddr_t);
+
+static int nxge_param_llc_snap_enable(p_nxge_t, uint64_t value, caddr_t);
+static int nxge_param_hash_lookup_enable(p_nxge_t, uint64_t value, caddr_t);
+
+static int nxge_param_tcam_enable(p_nxge_t nxgep, uint64_t value, caddr_t);
+
+static int nxge_param_get_rxdma_info(p_nxge_t nxgep, caddr_t cp,
+				     uint64_t *value, uint8_t *dump_str);
+static int nxge_param_get_txdma_info(p_nxge_t nxgep, caddr_t cp,
+				     uint64_t *value, uint8_t *dump_str);
+
+static int nxge_param_get_vlan_rdcgrp(p_nxge_t nxgep, caddr_t cp,
+				     uint64_t *value, uint8_t *dump_str);
+static int nxge_param_get_mac_rdcgrp(p_nxge_t nxgep, caddr_t cp,
+				     uint64_t *value, uint8_t *dump_str);
+static int nxge_param_get_rxdma_rdcgrp_info(p_nxge_t nxgep, caddr_t cp,
+					    uint64_t *value,
+					    uint8_t *dump_str);
+
+static int nxge_param_get_ip_opt(p_nxge_t nxgep, caddr_t cp,
+				 uint64_t *value, uint8_t *dump_str);
+
+static int
+nxge_param_get_mac(p_nxge_t nxgep, caddr_t cp,
+		   uint64_t *value, uint8_t *dump_str);
+
+static int
+nxge_param_get_llc_snap_enable(p_nxge_t nxgep, caddr_t cp,
+			       uint64_t *value, uint8_t *dump_str);
+static int
+nxge_param_get_debug_flag(p_nxge_t nxgep, caddr_t cp,
+			  uint64_t *value, uint8_t *dump_str);
+
+static int nxge_param_set_nxge_debug_flag(p_nxge_t, uint64_t, caddr_t);
+static int nxge_param_set_npi_debug_flag(p_nxge_t, uint64_t, caddr_t);
+
+static int nxge_param_dump_rdc(p_nxge_t nxgep, caddr_t cp,
+			       uint64_t *value, uint8_t *dump_str);
+static int nxge_param_dump_tdc(p_nxge_t nxgep, caddr_t cp, uint64_t *value,
+			       uint8_t *dump_str);
+static int nxge_param_dump_fflp_regs(p_nxge_t nxgep, caddr_t cp,
+				     uint64_t *value, uint8_t *dump_str);
+static int nxge_param_dump_mac_regs(p_nxge_t nxgep, caddr_t cp,
+				    uint64_t *value, uint8_t *dump_str);
+static int nxge_param_dump_ipp_regs(p_nxge_t nxgep, caddr_t cp,
+				    uint64_t *value, uint8_t *dump_str);
+static int nxge_param_dump_vlan_table(p_nxge_t nxgep, caddr_t cp,
+				      uint64_t *value, uint8_t *dump_str);
+static int nxge_param_dump_rdc_table(p_nxge_t nxgep, caddr_t cp,
+				     uint64_t *value, uint8_t *dump_str);
+static int nxge_param_dump_ptrs(p_nxge_t nxgep, caddr_t cp, uint64_t *value,
+				uint8_t *dump_str);
+
+static boolean_t nxge_param_link_update(p_nxge_t nxgep);
+
+static void nxge_lookup_param(p_nxge_t nxgep, char *param_name,
+			      p_nxge_param_t *param_p);
+
+
+/*
+ * TODO: add read/write modes: read only, read and write.
+ *	 use the first character to indicate its mode.
+ * TODO: add the global configurations for an entire Neptune (i.e
+ *	 (the RDC table)
+ */
+/*
+ * Global array of Neptune changable parameters.
+ * This array is initialized to correspond to the default
+ * Neptune 4 port configuration. This array would be copied
+ * into each port's parameter structure and modifed per
+ * nxge.conf configuration. Later, the parameters are
+ * exported to ndd to display and run-time configuration (at least
+ * some of them).
+ *
+ */
+
+static	nxge_param_t	nxge_param_arr[] = {
+/* getf    setf    min    max	 value    old    conf-val    conf-name	*/
+{  nxge_param_get_generic,	NULL,
+	NXGE_PARAM_READ | NXGE_PARAM_DONT_SHOW,
+	0, 3,	0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(function_number) },
+/* Partition Id */
+{  nxge_param_get_generic,	NULL,
+	NXGE_PARAM_READ | NXGE_PARAM_DONT_SHOW,
+	0, 8,	0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(partition_id) },
+/* Read Write Permission Mode */
+{  nxge_param_get_generic,	NULL,
+	NXGE_PARAM_READ | NXGE_PARAM_DONT_SHOW,
+	0, 2,	0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(read_write_mode) },
+/* hw cfg types */
+/* control the DMA config of Neptune/NIU */
+{  nxge_param_get_generic,	NULL,
+	NXGE_PARAM_READ | NXGE_PARAM_DONT_SHOW,
+	CFG_DEFAULT, CFG_CUSTOM, CFG_DEFAULT, CFG_DEFAULT, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(niu_cfg_type) },
+/* control the TXDMA config of the Port controlled by tx-quick-cfg */
+{  nxge_param_get_generic,	NULL,
+	NXGE_PARAM_READ | NXGE_PARAM_DONT_SHOW,
+	CFG_DEFAULT, CFG_CUSTOM, CFG_NOT_SPECIFIED, CFG_DEFAULT,
+	NXGE_PARAM_NOT_CONF,
+	NXGE_STR(tx_qcfg_type) },
+/* control the RXDMA config of the Port controlled by rx-quick-cfg */
+{  nxge_param_get_generic,	NULL,
+	NXGE_PARAM_READ | NXGE_PARAM_DONT_SHOW,
+	CFG_DEFAULT, CFG_CUSTOM, CFG_NOT_SPECIFIED, CFG_DEFAULT,
+	    NXGE_PARAM_NOT_CONF,
+	NXGE_STR(rx_qcfg_type) },
+
+{  nxge_param_get_mac,	nxge_param_set_mac,
+	NXGE_PARAM_RW  | NXGE_PARAM_DONT_SHOW,
+	0, 1,	0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(master_cfg_enable) },
+{  nxge_param_get_mac,	nxge_param_set_mac,
+	NXGE_PARAM_RW | NXGE_PARAM_DONT_SHOW,
+	0, 1,	0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(master_cfg_value) },
+
+{  nxge_param_get_mac,	nxge_param_set_mac,
+	NXGE_PARAM_MAC_RW | NXGE_PARAM_DONT_SHOW,
+	0, 1,	1,	1, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(adv_autoneg_cap) },
+{  nxge_param_get_mac,	nxge_param_set_mac,
+	NXGE_PARAM_MAC_RW | NXGE_PARAM_DONT_SHOW,
+	0, 1,	1,	1, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(adv_10gfdx_cap) },
+{  nxge_param_get_mac,	nxge_param_set_mac,
+	NXGE_PARAM_MAC_DONT_SHOW,
+	0, 1,	0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(adv_10ghdx_cap) },
+
+{  nxge_param_get_mac,	nxge_param_set_mac,
+	NXGE_PARAM_MAC_RW | NXGE_PARAM_DONT_SHOW,
+	0, 1,	1,	1, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(adv_1000fdx_cap) },
+{  nxge_param_get_mac,	nxge_param_set_mac,
+	NXGE_PARAM_MAC_DONT_SHOW,
+	0, 1,	0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(adv_1000hdx_cap) },
+{  nxge_param_get_mac,	nxge_param_set_mac,
+	NXGE_PARAM_MAC_DONT_SHOW,
+	0, 1,	1,	1, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(adv_100T4_cap) },
+{  nxge_param_get_mac,	nxge_param_set_mac,
+	NXGE_PARAM_MAC_RW | NXGE_PARAM_DONT_SHOW,
+	0, 1,	1,	1, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(adv_100fdx_cap) },
+{  nxge_param_get_mac,	nxge_param_set_mac,
+	NXGE_PARAM_MAC_DONT_SHOW,
+	0, 1,	0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(adv_100hdx_cap) },
+{  nxge_param_get_mac,	nxge_param_set_mac,
+	NXGE_PARAM_MAC_RW | NXGE_PARAM_DONT_SHOW,
+	0, 1,	1,	1, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(adv_10fdx_cap) },
+{  nxge_param_get_mac,	nxge_param_set_mac,
+	NXGE_PARAM_MAC_DONT_SHOW,
+	0, 1,	0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(adv_10hdx_cap) },
+{  nxge_param_get_mac,	nxge_param_set_mac,
+	NXGE_PARAM_MAC_RW | NXGE_PARAM_DONT_SHOW,
+	0, 1,	0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(adv_asmpause_cap) },
+{  nxge_param_get_mac,	nxge_param_set_mac,
+	NXGE_PARAM_MAC_RW | NXGE_PARAM_DONT_SHOW,
+	0, 1,	0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(adv_pause_cap) },
+{  nxge_param_get_mac,	nxge_param_set_mac,
+	NXGE_PARAM_MAC_RW | NXGE_PARAM_DONT_SHOW,
+	0, 1,	0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(use_int_xcvr) },
+{  nxge_param_get_mac,	nxge_param_set_mac,
+	NXGE_PARAM_MAC_RW | NXGE_PARAM_DONT_SHOW,
+	0, 1,	1,	1, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(enable_ipg0) },
+{  nxge_param_get_mac,	nxge_param_set_mac,
+	NXGE_PARAM_MAC_RW | NXGE_PARAM_DONT_SHOW,
+	0, 255,	8,	8, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(ipg0) },
+{  nxge_param_get_mac,	nxge_param_set_mac,
+	NXGE_PARAM_MAC_RW | NXGE_PARAM_DONT_SHOW,
+	0, 255,	8,	8, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(ipg1) },
+{  nxge_param_get_mac,	nxge_param_set_mac,
+	NXGE_PARAM_MAC_RW | NXGE_PARAM_DONT_SHOW,
+	0, 255,	4,	4, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(ipg2) },
+{  nxge_param_get_mac,	nxge_param_set_mac,
+	NXGE_PARAM_MAC_RW | NXGE_PARAM_DONT_SHOW,
+	0, 1,	0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(accept_jumbo) },
+
+/* Transmit DMA channels */
+{  nxge_param_get_generic,	NULL,
+	NXGE_PARAM_READ | NXGE_PARAM_READ_PROP | NXGE_PARAM_DONT_SHOW,
+	0, 3,	0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(tx_dma_weight) },
+{  nxge_param_get_generic,	NULL,
+	NXGE_PARAM_READ | NXGE_PARAM_READ_PROP | NXGE_PARAM_DONT_SHOW,
+	0, 31,	0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(tx_dma_channels_begin) },
+
+{  nxge_param_get_generic,	NULL,
+	NXGE_PARAM_READ | NXGE_PARAM_READ_PROP | NXGE_PARAM_DONT_SHOW,
+	0, 32,	0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(tx_dma_channels) },
+{  nxge_param_get_txdma_info,	NULL,
+	NXGE_PARAM_READ | NXGE_PARAM_READ_PROP | NXGE_PARAM_DUMP_STR | NXGE_PARAM_DONT_SHOW,
+	0, 32,	0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(tx_dma_info) },
+/* Receive DMA channels */
+{  nxge_param_get_generic,	NULL,
+	NXGE_PARAM_READ | NXGE_PARAM_READ_PROP | NXGE_PARAM_DONT_SHOW,
+	0, 31,	0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(rx_dma_channels_begin) },
+{  nxge_param_get_generic,	NULL,
+	NXGE_PARAM_READ | NXGE_PARAM_READ_PROP | NXGE_PARAM_DONT_SHOW,
+	0, 32,	0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(rx_dma_channels) },
+{  nxge_param_get_generic,	NULL,
+	NXGE_PARAM_READ | NXGE_PARAM_READ_PROP | NXGE_PARAM_DONT_SHOW,
+	0, 65535,	PT_DRR_WT_DEFAULT_10G,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(rx_drr_weight) },
+{  nxge_param_get_generic,	NULL,
+	NXGE_PARAM_READ | NXGE_PARAM_READ_PROP | NXGE_PARAM_DONT_SHOW,
+	0, 1,	NXGE_RCR_FULL_HEADER,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(rx_full_header) },
+{  nxge_param_get_rxdma_info,	NULL,
+	NXGE_PARAM_READ | NXGE_PARAM_DUMP_STR | NXGE_PARAM_DONT_SHOW,
+	0, 32,	0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(rx_dma_info) },
+
+{  nxge_param_get_rxdma_info,	NULL,
+	NXGE_PARAM_READ | NXGE_PARAM_DONT_SHOW | NXGE_PARAM_DUMP_STR,
+	NXGE_RBR_RBB_MIN, NXGE_RBR_RBB_MAX, NXGE_RBR_RBB_DEFAULT, 0,
+	    NXGE_PARAM_NOT_CONF,
+	NXGE_STR(rx_rbr_size) },
+
+{  nxge_param_get_rxdma_info,	NULL,
+	NXGE_PARAM_READ | NXGE_PARAM_DONT_SHOW | NXGE_PARAM_DUMP_STR,
+	NXGE_RCR_MIN, NXGE_RCR_MAX, NXGE_RCR_DEFAULT, 0,
+	   NXGE_PARAM_NOT_CONF,
+	NXGE_STR(rx_rcr_size) },
+
+{  nxge_param_get_generic,	nxge_param_set_port_rdc,
+	NXGE_PARAM_RXDMA_RW | NXGE_PARAM_DONT_SHOW,
+	0, 15,	0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(default_port_rdc) },
+
+{  nxge_param_get_generic,	NULL, NXGE_PARAM_READ_PROP | NXGE_PARAM_DONT_SHOW,
+	0, 8,	0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(rx_rdc_grps_begin) },
+{  nxge_param_get_generic,	NULL,
+	NXGE_PARAM_READ_PROP | NXGE_PARAM_DONT_SHOW,
+	0, 8,	0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(rx_rdc_grps) },
+
+{  nxge_param_get_generic,	nxge_param_set_grp_rdc,
+	NXGE_PARAM_RXDMA_RW | NXGE_PARAM_PER_PORT_CHK | NXGE_PARAM_DONT_SHOW,
+	0, 15,	0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(default_grp0_rdc) },
+{  nxge_param_get_generic,	nxge_param_set_grp_rdc,
+	NXGE_PARAM_RXDMA_RW | NXGE_PARAM_PER_PORT_CHK | NXGE_PARAM_DONT_SHOW,
+	0, 15,	2,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(default_grp1_rdc) },
+{  nxge_param_get_generic,	nxge_param_set_grp_rdc,
+	NXGE_PARAM_RXDMA_RW | NXGE_PARAM_PER_PORT_CHK | NXGE_PARAM_DONT_SHOW,
+	0, 15,	4,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(default_grp2_rdc) },
+{  nxge_param_get_generic,	nxge_param_set_grp_rdc,
+	NXGE_PARAM_RXDMA_RW | NXGE_PARAM_PER_PORT_CHK | NXGE_PARAM_DONT_SHOW,
+	0, 15,	6,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(default_grp3_rdc) },
+{  nxge_param_get_generic,	nxge_param_set_grp_rdc,
+	NXGE_PARAM_RXDMA_RW | NXGE_PARAM_PER_PORT_CHK | NXGE_PARAM_DONT_SHOW,
+	0, 15,	8,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(default_grp4_rdc) },
+{  nxge_param_get_generic,	nxge_param_set_grp_rdc,
+	NXGE_PARAM_RXDMA_RW | NXGE_PARAM_PER_PORT_CHK | NXGE_PARAM_DONT_SHOW,
+	0, 15,	10,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(default_grp5_rdc) },
+{  nxge_param_get_generic,	nxge_param_set_grp_rdc,
+	NXGE_PARAM_RXDMA_RW | NXGE_PARAM_PER_PORT_CHK | NXGE_PARAM_DONT_SHOW,
+	0, 15,	12,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(default_grp6_rdc) },
+{  nxge_param_get_generic,	nxge_param_set_grp_rdc,
+	NXGE_PARAM_RXDMA_RW | NXGE_PARAM_PER_PORT_CHK | NXGE_PARAM_DONT_SHOW,
+	0, 15,	14,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(default_grp7_rdc) },
+
+{  nxge_param_get_rxdma_rdcgrp_info, NULL,
+	NXGE_PARAM_READ | NXGE_PARAM_CMPLX | NXGE_PARAM_DUMP_STR | NXGE_PARAM_DONT_SHOW,
+	0, 8,	0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(rdc_groups_info) },
+/* Logical device groups */
+{  nxge_param_get_generic,	NULL,
+	NXGE_PARAM_READ | NXGE_PARAM_DONT_SHOW,
+	0, 63,	0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(start_ldg) },
+{  nxge_param_get_generic,	NULL,
+	NXGE_PARAM_READ | NXGE_PARAM_DONT_SHOW,
+	0, 64,	0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(max_ldg) },
+
+/* MAC table information */
+{  nxge_param_get_mac_rdcgrp,	nxge_param_set_mac_rdcgrp,
+	NXGE_PARAM_L2CLASS_CFG | NXGE_PARAM_DUMP_STR | NXGE_PARAM_DONT_SHOW,
+	0, 31,	0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(mac_2rdc_grp) },
+/* VLAN table information */
+
+{  nxge_param_get_vlan_rdcgrp,	nxge_param_set_vlan_rdcgrp,
+	NXGE_PARAM_L2CLASS_CFG | NXGE_PARAM_DUMP_STR | NXGE_PARAM_DONT_SHOW,
+	0, 31,	0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(vlan_2rdc_grp) },
+
+{  nxge_param_get_generic,	NULL,
+	NXGE_PARAM_READ_PROP | NXGE_PARAM_READ | NXGE_PARAM_PROP_ARR32 | NXGE_PARAM_DONT_SHOW,
+	0, 0x0ffff, 0x0ffff, 0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(fcram_part_cfg) },
+
+{  nxge_param_get_generic,	NULL,
+	NXGE_PARAM_CLASS_RWS | NXGE_PARAM_DONT_SHOW,
+	0, 0x10,	0xa,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(fcram_access_ratio) },
+
+{  nxge_param_get_generic, NULL,
+	NXGE_PARAM_CLASS_RWS | NXGE_PARAM_DONT_SHOW,
+	0, 0x10,	0xa,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(tcam_access_ratio) },
+
+
+{  nxge_param_get_generic,	nxge_param_tcam_enable,
+	NXGE_PARAM_CLASS_RWS | NXGE_PARAM_DONT_SHOW,
+	0,	0x1,	0x0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(tcam_enable) },
+
+{  nxge_param_get_generic,	nxge_param_hash_lookup_enable,
+	NXGE_PARAM_CLASS_RWS | NXGE_PARAM_DONT_SHOW,
+	0,	0x1,	0x0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(hash_lookup_enable) },
+
+{  nxge_param_get_llc_snap_enable,	nxge_param_llc_snap_enable,
+	NXGE_PARAM_CLASS_RWS | NXGE_PARAM_DONT_SHOW,
+	0, 0x01, 0x01, 0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(llc_snap_enable) },
+
+{  nxge_param_get_generic,	nxge_param_fflp_hash_init,
+	NXGE_PARAM_CLASS_RWS | NXGE_PARAM_DONT_SHOW,
+	0, ALL_FF_32,	ALL_FF_32, 0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(h1_init_value) },
+{  nxge_param_get_generic,	nxge_param_fflp_hash_init,
+	NXGE_PARAM_CLASS_RWS | NXGE_PARAM_DONT_SHOW,
+	0, 0x0ffff,	0x0ffff, 0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(h2_init_value) },
+
+{  nxge_param_get_generic,	nxge_param_set_ether_usr,
+	NXGE_PARAM_CLASS_RWS | NXGE_PARAM_DONT_SHOW,
+	0, ALL_FF_32,	0x0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(class_cfg_ether_usr1) },
+
+{  nxge_param_get_generic,	nxge_param_set_ether_usr,
+	NXGE_PARAM_CLASS_RWS | NXGE_PARAM_DONT_SHOW,
+	0, ALL_FF_32,	0x0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(class_cfg_ether_usr2) },
+{  nxge_param_get_generic,	nxge_param_set_ip_usr,
+	NXGE_PARAM_CLASS_RWS | NXGE_PARAM_DONT_SHOW,
+	0, ALL_FF_32,	0x0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(class_cfg_ip_usr4) },
+{  nxge_param_get_generic,	nxge_param_set_ip_usr,
+	NXGE_PARAM_CLASS_RWS | NXGE_PARAM_DONT_SHOW,
+	0, ALL_FF_32,	0x0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(class_cfg_ip_usr5) },
+{  nxge_param_get_generic,	nxge_param_set_ip_usr,
+	NXGE_PARAM_CLASS_RWS | NXGE_PARAM_DONT_SHOW,
+	0, ALL_FF_32,	0x0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(class_cfg_ip_usr6) },
+{  nxge_param_get_generic,	nxge_param_set_ip_usr,
+	NXGE_PARAM_CLASS_RWS | NXGE_PARAM_DONT_SHOW,
+	0, ALL_FF_32,	0x0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(class_cfg_ip_usr7) },
+{  nxge_param_get_ip_opt,	nxge_param_set_ip_opt,
+	NXGE_PARAM_CLASS_RWS | NXGE_PARAM_DONT_SHOW,
+	0, ALL_FF_32,	0x0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(class_opt_ip_usr4) },
+{  nxge_param_get_ip_opt,	nxge_param_set_ip_opt,
+	NXGE_PARAM_CLASS_RWS | NXGE_PARAM_DONT_SHOW,
+	0, ALL_FF_32,	0x0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(class_opt_ip_usr5) },
+{  nxge_param_get_ip_opt,	nxge_param_set_ip_opt,
+	NXGE_PARAM_CLASS_RWS | NXGE_PARAM_DONT_SHOW,
+	0, ALL_FF_32,	0x0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(class_opt_ip_usr6) },
+{  nxge_param_get_ip_opt, nxge_param_set_ip_opt,
+	NXGE_PARAM_CLASS_RWS | NXGE_PARAM_DONT_SHOW,
+	0, ALL_FF_32,	0x0,	0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(class_opt_ip_usr7) },
+
+{  nxge_param_get_ip_opt,	nxge_param_set_ip_opt,
+	NXGE_PARAM_CLASS_RWS,
+	0, ALL_FF_32, NXGE_CLASS_FLOW_GEN_SERVER, 0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(class_opt_ipv4_tcp) },
+{  nxge_param_get_ip_opt,	nxge_param_set_ip_opt,
+	NXGE_PARAM_CLASS_RWS,
+	0, ALL_FF_32, NXGE_CLASS_FLOW_GEN_SERVER, 0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(class_opt_ipv4_udp) },
+{  nxge_param_get_ip_opt,	nxge_param_set_ip_opt,
+	NXGE_PARAM_CLASS_RWS,
+	0, ALL_FF_32, NXGE_CLASS_FLOW_GEN_SERVER, 0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(class_opt_ipv4_ah) },
+{  nxge_param_get_ip_opt,	nxge_param_set_ip_opt,
+	NXGE_PARAM_CLASS_RWS,
+	0, ALL_FF_32, NXGE_CLASS_FLOW_GEN_SERVER, 0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(class_opt_ipv4_sctp) },
+{  nxge_param_get_ip_opt,	nxge_param_set_ip_opt,
+	NXGE_PARAM_CLASS_RWS,
+	0, ALL_FF_32, NXGE_CLASS_FLOW_GEN_SERVER, 0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(class_opt_ipv6_tcp) },
+{  nxge_param_get_ip_opt,	nxge_param_set_ip_opt,
+	NXGE_PARAM_CLASS_RWS,
+	0, ALL_FF_32, NXGE_CLASS_FLOW_GEN_SERVER, 0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(class_opt_ipv6_udp) },
+{  nxge_param_get_ip_opt,	nxge_param_set_ip_opt,
+	NXGE_PARAM_CLASS_RWS,
+	0, ALL_FF_32, NXGE_CLASS_FLOW_GEN_SERVER, 0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(class_opt_ipv6_ah) },
+{  nxge_param_get_ip_opt,	nxge_param_set_ip_opt,
+	NXGE_PARAM_CLASS_RWS,
+	0, ALL_FF_32, NXGE_CLASS_FLOW_GEN_SERVER, 0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(class_opt_ipv6_sctp) },
+
+{  nxge_param_get_generic,	nxge_param_set_tx_lb_policy,
+	NXGE_PARAM_CLASS_RWS,
+	0, ALL_FF_32, NXGE_TX_LB_TCP_PORT, 0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(tx_lb_policy) },
+
+{  nxge_param_get_generic,	nxge_param_set_rx_allow_all,
+	NXGE_PARAM_CLASS_RWS,
+	0, ALL_FF_32, NXGE_TX_LB_TCP_PORT, 0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(rx_allow_all) },
+
+{  nxge_param_get_debug_flag, nxge_param_set_nxge_debug_flag,
+	NXGE_PARAM_RW | NXGE_PARAM_DONT_SHOW,
+	0, ALL_FF_32, 0, 0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(nxge_debug_flag) },
+{  nxge_param_get_debug_flag, nxge_param_set_npi_debug_flag,
+	NXGE_PARAM_RW | NXGE_PARAM_DONT_SHOW,
+	0, ALL_FF_32, 0, 0, NXGE_PARAM_NOT_CONF,
+	NXGE_STR(npi_debug_flag) },
+
+{  nxge_param_dump_rdc,	NULL, NXGE_PARAM_READ | NXGE_PARAM_DUMP_STR
+	   | NXGE_PARAM_DONT_SHOW,
+	   0, 0x0fffffff,	0x0fffffff, 0,
+	   NXGE_PARAM_NOT_CONF,
+	   NXGE_STR(dump_rdc) },
+
+{  nxge_param_dump_tdc,	NULL, NXGE_PARAM_READ | NXGE_PARAM_DUMP_STR
+	   | NXGE_PARAM_DONT_SHOW,
+	0, 0x0fffffff,	0x0fffffff,	0,
+	   NXGE_PARAM_NOT_CONF,
+	   NXGE_STR(dump_tdc)},
+
+{  nxge_param_dump_mac_regs,	NULL, NXGE_PARAM_READ | NXGE_PARAM_DUMP_STR
+	   | NXGE_PARAM_DONT_SHOW,
+	0, 0x0fffffff,	0x0fffffff,	0,
+	   NXGE_PARAM_NOT_CONF,
+	   NXGE_STR(dump_mac_regs)},
+
+{  nxge_param_dump_ipp_regs,	NULL, NXGE_PARAM_READ | NXGE_PARAM_DUMP_STR
+	   | NXGE_PARAM_DONT_SHOW,
+	0, 0x0fffffff,	0x0fffffff,	0,
+	   NXGE_PARAM_NOT_CONF,
+	   NXGE_STR(dump_ipp_regs)},
+
+{  nxge_param_dump_fflp_regs,	NULL, NXGE_PARAM_READ | NXGE_PARAM_DUMP_STR
+	   | NXGE_PARAM_DONT_SHOW,
+	0, 0x0fffffff,	0x0fffffff,	0,
+	   NXGE_PARAM_NOT_CONF,
+	   NXGE_STR(dump_fflp_regs)},
+
+{  nxge_param_dump_vlan_table,	NULL, NXGE_PARAM_READ | NXGE_PARAM_DUMP_STR
+	   | NXGE_PARAM_DONT_SHOW,
+	0, 0x0fffffff,	0x0fffffff,	0,
+	   NXGE_PARAM_NOT_CONF,
+	   NXGE_STR(dump_vlan_table)},
+
+{  nxge_param_dump_rdc_table,	NULL, NXGE_PARAM_READ | NXGE_PARAM_DUMP_STR
+	   | NXGE_PARAM_DONT_SHOW,
+	0, 0x0fffffff,	0x0fffffff,	0,
+	   NXGE_PARAM_NOT_CONF,
+	   NXGE_STR(dump_rdc_table)},
+
+{  nxge_param_dump_ptrs,	NULL, NXGE_PARAM_READ | NXGE_PARAM_DUMP_STR
+	   | NXGE_PARAM_DONT_SHOW,
+	0, 0x0fffffff,	0x0fffffff,	0,
+	   NXGE_PARAM_NOT_CONF,
+	   NXGE_STR(dump_ptrs)},
+
+{  NULL,	NULL, NXGE_PARAM_READ | NXGE_PARAM_DONT_SHOW,
+	0, 0x0fffffff,	0x0fffffff,	0, NXGE_PARAM_NOT_CONF,	NXGE_STR(end) },
+};
+
+#define	MPN(a, b, c, d) module_param_named(a, b, c, d)
+#define	PARAM_VALUE(index) nxge_param_arr[index].conf_value
+
+MPN(function_number, PARAM_VALUE(param_function_number), int, 0);
+
+MPN(partition_id, PARAM_VALUE(param_partition_id), int, 0);
+MPN(read_write_mode, PARAM_VALUE(param_read_write_mode), int, 0);
+MPN(niu_cfg_type, PARAM_VALUE(param_niu_cfg_type), int, 0);
+MPN(tx_qcfg_type, PARAM_VALUE(param_tx_qcfg_type), int, 0);
+MPN(rx_qcfg_type, PARAM_VALUE(param_rx_qcfg_type), int, 0);
+MPN(master_cfg_enable, PARAM_VALUE(param_master_cfg_enable), int, 0);
+MPN(master_cfg_value, PARAM_VALUE(param_master_cfg_value), int, 0);
+MPN(adv_autoneg_cap, PARAM_VALUE(param_autoneg), int, 0);
+
+MPN(adv_10gfdx_cap, PARAM_VALUE(param_anar_10gfdx), int, 0);
+MPN(adv_10ghdx_cap, PARAM_VALUE(param_anar_10ghdx), int, 0);
+MPN(adv_1000fdx_cap, PARAM_VALUE(param_anar_1000fdx), int, 0);
+MPN(adv_1000hdx_cap, PARAM_VALUE(param_anar_1000hdx), int, 0);
+MPN(adv_100T4_cap, PARAM_VALUE(param_anar_100T4), int, 0);
+MPN(adv_100fdx_cap, PARAM_VALUE(param_anar_100fdx), int, 0);
+MPN(adv_100hdx_cap, PARAM_VALUE(param_anar_100hdx), int, 0);
+MPN(adv_10fdx_cap, PARAM_VALUE(param_anar_10fdx), int, 0);
+MPN(adv_10hdx_cap, PARAM_VALUE(param_anar_10hdx), int, 0);
+MPN(adv_asmpause_cap, PARAM_VALUE(param_anar_asmpause), int, 0);
+
+MPN(adv_pause_cap, PARAM_VALUE(param_anar_pause), int, 0);
+MPN(use_int_xcvr, PARAM_VALUE(param_use_int_xcvr), int, 0);
+MPN(enable_ipg0, PARAM_VALUE(param_enable_ipg0), int, 0);
+MPN(ipg0, PARAM_VALUE(param_ipg0), int, 0);
+MPN(ipg1, PARAM_VALUE(param_ipg1), int, 0);
+MPN(ipg2, PARAM_VALUE(param_ipg2), int, 0);
+MPN(accept_jumbo, PARAM_VALUE(param_accept_jumbo), int, 0);
+MPN(txdma_weight, PARAM_VALUE(param_txdma_weight), int, 0);
+MPN(tx_dma_channels_begin, PARAM_VALUE(param_tx_dma_channels_begin), int, 0);
+MPN(tx_dma_channels, PARAM_VALUE(param_tx_dma_channels), int, 0);
+
+MPN(tx_dma_info, PARAM_VALUE(param_tx_dma_info), int, 0);
+MPN(rx_dma_channels_begin, PARAM_VALUE(param_rx_dma_channels_begin), int, 0);
+MPN(rx_dma_channels, PARAM_VALUE(param_rx_dma_channels), int, 0);
+MPN(rx_drr_weight, PARAM_VALUE(param_rx_drr_weight), int, 0);
+MPN(rx_full_header, PARAM_VALUE(param_rx_full_header), int, 0);
+MPN(rx_dma_info, PARAM_VALUE(param_rx_dma_info), int, 0);
+MPN(rx_rbr_size, PARAM_VALUE(param_rx_rbr_size), int, 0);
+MPN(rx_rcr_size, PARAM_VALUE(param_rx_rcr_size), int, 0);
+MPN(default_port_rdc, PARAM_VALUE(param_default_port_rdc), int, 0);
+
+MPN(rx_rdc_grps_begin, PARAM_VALUE(param_rx_rdc_grps_begin), int, 0);
+
+MPN(rx_rdc_grps, PARAM_VALUE(param_rx_rdc_grps), int, 0);
+MPN(default_grp0_rdc, PARAM_VALUE(param_default_grp0_rdc), int, 0);
+MPN(default_grp1_rdc, PARAM_VALUE(param_default_grp1_rdc), int, 0);
+MPN(default_grp2_rdc, PARAM_VALUE(param_default_grp2_rdc), int, 0);
+MPN(default_grp3_rdc, PARAM_VALUE(param_default_grp3_rdc), int, 0);
+MPN(default_grp4_rdc, PARAM_VALUE(param_default_grp4_rdc), int, 0);
+MPN(default_grp5_rdc, PARAM_VALUE(param_default_grp5_rdc), int, 0);
+MPN(default_grp6_rdc, PARAM_VALUE(param_default_grp6_rdc), int, 0);
+MPN(default_grp7_rdc, PARAM_VALUE(param_default_grp7_rdc), int, 0);
+
+
+MPN(rdc_groups_info, PARAM_VALUE(param_rdc_groups_info), int, 0);
+MPN(start_ldg, PARAM_VALUE(param_start_ldg), int, 0);
+MPN(max_ldg, PARAM_VALUE(param_max_ldg), int, 0);
+/* MPN(mac_2rdc_grp, PARAM_VALUE(param_mac_2rdc_grp), int, 0); */
+/* MPN(vlan_2rdc_grp, PARAM_VALUE(param_vlan_2rdc_grp), int, 0); */
+MPN(fcram_part_cfg, PARAM_VALUE(param_fcram_part_cfg), int, 0);
+MPN(fcram_access_ratio, PARAM_VALUE(param_fcram_access_ratio), int, 0);
+MPN(tcam_access_ratio, PARAM_VALUE(param_tcam_access_ratio), int, 0);
+MPN(tcam_enable, PARAM_VALUE(param_tcam_enable), int, 0);
+MPN(hash_lookup_enable, PARAM_VALUE(param_hash_lookup_enable), int, 0);
+MPN(llc_snap_enable, PARAM_VALUE(param_llc_snap_enable), int, 0);
+MPN(h1_init_value, PARAM_VALUE(param_h1_init_value), int, 0);
+MPN(h2_init_value, PARAM_VALUE(param_h2_init_value), int, 0);
+MPN(class_cfg_ether_usr1, PARAM_VALUE(param_class_cfg_ether_usr1), int, 0);
+MPN(class_cfg_ether_usr2, PARAM_VALUE(param_class_cfg_ether_usr2), int, 0);
+MPN(class_cfg_ip_usr4, PARAM_VALUE(param_class_cfg_ip_usr4), int, 0);
+MPN(class_cfg_ip_usr5, PARAM_VALUE(param_class_cfg_ip_usr5), int, 0);
+MPN(class_cfg_ip_usr6, PARAM_VALUE(param_class_cfg_ip_usr6), int, 0);
+MPN(class_cfg_ip_usr7, PARAM_VALUE(param_class_cfg_ip_usr7), int, 0);
+MPN(class_opt_ip_usr4, PARAM_VALUE(param_class_opt_ip_usr4), int, 0);
+MPN(class_opt_ip_usr5, PARAM_VALUE(param_class_opt_ip_usr5), int, 0);
+MPN(class_opt_ip_usr6, PARAM_VALUE(param_class_opt_ip_usr6), int, 0);
+MPN(class_opt_ip_usr7, PARAM_VALUE(param_class_opt_ip_usr7), int, 0);
+MPN(class_opt_ipv4_tcp, PARAM_VALUE(param_class_opt_ipv4_tcp), int, 0);
+MPN(class_opt_ipv4_udp, PARAM_VALUE(param_class_opt_ipv4_udp), int, 0);
+MPN(class_opt_ipv4_ah, PARAM_VALUE(param_class_opt_ipv4_ah), int, 0);
+MPN(class_opt_ipv4_sctp, PARAM_VALUE(param_class_opt_ipv4_sctp), int, 0);
+MPN(class_opt_ipv6_tcp, PARAM_VALUE(param_class_opt_ipv6_tcp), int, 0);
+MPN(class_opt_ipv6_udp, PARAM_VALUE(param_class_opt_ipv6_udp), int, 0);
+MPN(class_opt_ipv6_ah, PARAM_VALUE(param_class_opt_ipv6_ah), int, 0);
+MPN(class_opt_ipv6_sctp, PARAM_VALUE(param_class_opt_ipv6_sctp), int, 0);
+MPN(tx_lb_policy, PARAM_VALUE(param_tx_lb_policy), int, 0);
+MPN(rx_allow_all, PARAM_VALUE(param_rx_allow_all), int, 0);
+MPN(nxge_debug_flag, PARAM_VALUE(param_nxge_debug_flag), int, 0);
+MPN(npi_debug_flag, PARAM_VALUE(param_npi_debug_flag), int, 0);
+
+MPN(dump_rdc, PARAM_VALUE(param_dump_rdc), int, 0);
+MPN(dump_tdc, PARAM_VALUE(param_dump_tdc), int, 0);
+MPN(dump_mac_regs, PARAM_VALUE(param_dump_mac_regs), int, 0);
+MPN(dump_ipp_regs, PARAM_VALUE(param_dump_ipp_regs), int, 0);
+MPN(dump_fflp_regs, PARAM_VALUE(param_dump_fflp_regs), int, 0);
+MPN(dump_vlan_table, PARAM_VALUE(param_dump_vlan_table), int, 0);
+MPN(dump_rdc_table, PARAM_VALUE(param_dump_rdc_table), int, 0);
+MPN(dump_ptrs, PARAM_VALUE(param_dump_ptrs), int, 0);
+
+MPN(end, PARAM_VALUE(param_end), int, 0);
+
+/* ************************************************************** */
+
+
+static int nxge_tx_lb_policy = NXGE_TX_LB_TCP_PORT;
+int nxge_atca_port_mode = NXGE_ATCA_PORT0_1G | NXGE_ATCA_PORT1_1G;
+module_param(nxge_atca_port_mode, int, 0);
+
+module_param(nxge_tx_lb_policy, int, S_IRUGO | S_IWUSR);
+
+#define	NXGE_WEB_SVR_CFG_STR	"web-server"
+#define	NXGE_GEN_SVR_CFG_STR	"generic-server"
+static char *nxge_rx_quick_cfg_str = NXGE_GEN_SVR_CFG_STR;
+module_param(nxge_rx_quick_cfg_str, charp, 0);
+
+#define	NXGE_DMA_EQUAL_CFG_STR	"equal"
+#define	NXGE_DMA_FAIR_CFG_STR	"fair"
+#define	NXGE_DMA_CUSTOM_CFG_STR	"custom"
+static char *nxge_dma_cfg_str = NXGE_DMA_EQUAL_CFG_STR;
+module_param(nxge_dma_cfg_str, charp, 0);
+
+static int num_txdmas_pport = 0;
+static int nxge_txdmas_pport[NXGE_MAX_PORTS] =
+	    { [0 ... (NXGE_MAX_PORTS - 1)] = -1 };
+
+static int num_st_txdma_pport = 0;
+static int nxge_st_txdma_pport[NXGE_MAX_PORTS] =
+	    { [0 ... (NXGE_MAX_PORTS - 1)] = -1 };
+
+static int num_rxdmas_pport = 0;
+static int nxge_rxdmas_pport[NXGE_MAX_PORTS] =
+	    { [0 ... (NXGE_MAX_PORTS - 1)] = -1 };
+
+static int num_st_rxdma_pport = 0;
+static int nxge_st_rxdma_pport[NXGE_MAX_PORTS] =
+	    { [0 ... (NXGE_MAX_PORTS - 1)] = -1 };
+
+static int num_rdcgrps_pport = 0;
+static int nxge_rdcgrps_pport[NXGE_MAX_PORTS] =
+	    { [0 ... (NXGE_MAX_PORTS - 1)] = -1 };
+
+static int num_st_rdcgrp_pport = 0;
+static int nxge_st_rdcgrp_pport[NXGE_MAX_PORTS] =
+	    { [0 ... (NXGE_MAX_PORTS - 1)] = -1 };
+
+
+static int num_rxdma_intr_time = 0;
+static int nxge_rxdma_intr_time[NXGE_MAX_RDCS] =
+	    { [0 ... (NXGE_MAX_RDCS - 1)] = -1 };
+
+static int num_rxdma_intr_pkts = 0;
+static int nxge_rxdma_intr_pkts[NXGE_MAX_RDCS] =
+	    { [0 ... (NXGE_MAX_RDCS - 1)] = -1 };
+
+static int num_mac_2rdc_grp = 0;
+static int nxge_mac_2rdc_grp[NXGE_MAX_MACS] =
+	    { [0 ... (NXGE_MAX_MACS - 1)] = -1 };
+
+static int num_vlan_2rdc_grp = 0;
+static int nxge_vlan_2rdc_grp[NXGE_MAX_VLANS] =
+	    { [0 ... (NXGE_MAX_VLANS - 1)] = -1 };
+
+#define	   MPAN(a, b, c, d, e) \
+module_param_array_named(a, b, c, d, e)
+
+#ifdef NXGE_USE_MODULE_PARAM_NEW_NAMED
+MPAN(txdmas_pport, nxge_txdmas_pport, int, &num_txdmas_pport, 0);
+MPAN(st_txdma_pport, nxge_st_txdma_pport, int, &num_st_txdma_pport, 0);
+MPAN(rxdmas_pport, nxge_rxdmas_pport, int, &num_rxdmas_pport, 0);
+MPAN(st_rxdma_pport, nxge_st_rxdma_pport, int, &num_st_rxdma_pport, 0);
+MPAN(rdcgrps_pport, nxge_rdcgrps_pport, int, &num_rdcgrps_pport, 0);
+MPAN(st_rdcgrp_pport, nxge_st_rdcgrp_pport, int, &num_st_rdcgrp_pport, 0);
+MPAN(rxdma_intr_time, nxge_rxdma_intr_time, int, &num_rxdma_intr_time, 0);
+MPAN(rxdma_intr_pkts, nxge_rxdma_intr_pkts, int, &num_rxdma_intr_pkts, 0);
+MPAN(mac_2rdc_grp, nxge_mac_2rdc_grp, int, &num_mac_2rdc_grp, 0);
+MPAN(vlan_2rdc_grp, nxge_vlan_2rdc_grp, int, &num_vlan_2rdc_grp, 0);
+#else
+MPAN(txdmas_pport, nxge_txdmas_pport, int, num_txdmas_pport, 0);
+MPAN(st_txdma_pport, nxge_st_txdma_pport, int, num_st_txdma_pport, 0);
+MPAN(rxdmas_pport, nxge_rxdmas_pport, int, num_rxdmas_pport, 0);
+MPAN(st_rxdma_pport, nxge_st_rxdma_pport, int, num_st_rxdma_pport, 0);
+MPAN(rdcgrps_pport, nxge_rdcgrps_pport, int, num_rdcgrps_pport, 0);
+MPAN(st_rdcgrp_pport, nxge_st_rdcgrp_pport, int, num_st_rdcgrp_pport, 0);
+MPAN(rxdma_intr_time, nxge_rxdma_intr_time, int, num_rxdma_intr_time, 0);
+MPAN(rxdma_intr_pkts, nxge_rxdma_intr_pkts, int, num_rxdma_intr_pkts, 0);
+MPAN(mac_2rdc_grp, nxge_mac_2rdc_grp, int, num_mac_2rdc_grp, 0);
+MPAN(vlan_2rdc_grp, nxge_vlan_2rdc_grp, int, num_vlan_2rdc_grp, 0);
+#endif
+
+
+typedef struct _nxge_parr_t {
+	int	*size;
+	int	*arr;
+} nxge_parr_t;
+
+
+/* ************************************************************** */
+
+extern void 		*nxge_list;
+
+int
+nxge_set_tx_lb_policy(int policy)
+{
+
+	if (policy != NXGE_TX_LB_NONE &&
+		policy != NXGE_TX_LB_TCP_PORT &&
+	    policy != NXGE_TX_LB_CPU_ID &&
+	    policy != NXGE_TX_LB_L4_PORT &&
+	    policy != NXGE_TX_LB_L4_SRC &&
+	    policy != NXGE_TX_LB_L4_DEST &&
+	    policy != NXGE_TX_LB_VLAN &&
+	    policy != NXGE_TX_LB_IP_ADDR &&
+	    policy != NXGE_TX_LB_IP_SRC &&
+	    policy != NXGE_TX_LB_IP_DEST &&
+	    policy != NXGE_TX_LB_DEST_MAC) {
+		NXGE_ERROR_MSG((NULL, NXGE_ERR_CTL,
+				"Invalid TX LB policy value %d",
+				policy));
+		return (-1);
+	}
+	nxge_tx_lb_policy = policy;
+	return (0);
+}
+
+int
+nxge_get_tx_lb_policy(p_nxge_t nep)
+{
+	return (nep->tx_lb_policy);
+}
+
+void
+nxge_init_tx_lb_policy(p_nxge_t nep)
+{
+	nep->tx_lb_policy = nxge_tx_lb_policy;
+}
+
+void
+nxge_get_param_int_arr(nxge_arr_param_index_t param, int *size, int **arr)
+{
+#if 0
+	*size = *(nxge_arr_param_arr[param].size);
+	*arr =  nxge_arr_param_arr[param].arr;
+#else
+	*size = 0;
+#endif
+
+}
+
+int
+nxge_cfg_set_dma_cfg(p_nxge_t nxgep)
+{
+	int			i, j, bits;
+	int			ntxdmas, nrxdmas, nrxgrps;
+	int			ntdcs, nrdcs, nrdcgrps;
+	int			start_tdc, start_rdc, start_rdc_grp;
+	uint8_t 		func;
+	p_nxge_dma_pt_cfg_t	p_all_cfgp;
+	p_nxge_hw_pt_cfg_t	p_cfgp;
+	boolean_t		good_custom_config = B_TRUE;
+	int			num_ports;
+	config_token_t		cfg_type;
+	uint8_t			tx_port_arr[NXGE_MAX_PORTS];
+	uint8_t			rx_port_arr[NXGE_MAX_PORTS];
+	uint8_t			rxgrp_port_arr[NXGE_MAX_PORTS];
+	uint8_t			*tmp_tx_port_arr;
+	uint8_t			*tmp_rx_port_arr;
+	uint8_t			*tmp_rxgrp_port_arr;
+	uint8_t			custom_start_txdma[NXGE_MAX_PORTS];
+	uint8_t			custom_start_rxdma[NXGE_MAX_PORTS];
+	uint8_t			custom_start_rdcgrp[NXGE_MAX_PORTS];
+	uint32_t		tdc_bitmap[NXGE_MAX_PORTS];
+	uint32_t		rdc_bitmap[NXGE_MAX_PORTS];
+	uint32_t		rdcgrp_bitmap[NXGE_MAX_PORTS];
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_cfg_set_dma_cfg"));
+
+	num_ports = nxgep->nports;
+	func = nxgep->function_num;
+	cfg_type = EQUAL;
+
+	if (strcmp(NXGE_DMA_EQUAL_CFG_STR, nxge_dma_cfg_str) == 0) {
+		cfg_type = EQUAL;
+		NXGE_DEBUG_MSG((nxgep, CFG_CTL, "DMA config: equal "));
+	} else if (strcmp(NXGE_DMA_FAIR_CFG_STR, nxge_dma_cfg_str) == 0) {
+		cfg_type = FAIR;
+		NXGE_DEBUG_MSG((nxgep, CFG_CTL, "DMA config: fair "));
+	} else if (strcmp(NXGE_DMA_CUSTOM_CFG_STR, nxge_dma_cfg_str) == 0) {
+		cfg_type = CUSTOM;
+		NXGE_DEBUG_MSG((nxgep, CFG_CTL, "DMA config: custom "));
+	} else {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL, "nxge_cfg_set_dma_cfg: "
+				"Unknown dma cfg %s", nxge_dma_cfg_str));
+	}
+
+	p_all_cfgp = (p_nxge_dma_pt_cfg_t)&nxgep->pt_config;
+	p_cfgp = (p_nxge_hw_pt_cfg_t)&p_all_cfgp->hw_config;
+	p_cfgp->function_number = func;
+
+	start_tdc = start_rdc = start_rdc_grp = 0;
+
+	switch (cfg_type) {
+	case CUSTOM:
+		ntdcs = nrdcs = nrdcgrps = 0;
+
+		/* TXDMA config */
+		if ((num_txdmas_pport < num_ports) ||
+		    (num_st_txdma_pport < num_ports)) {
+
+			good_custom_config = B_FALSE;
+		} else {
+			for (i = 0; i < num_ports; i++) {
+
+				tx_port_arr[i] = nxge_txdmas_pport[i];
+				custom_start_txdma[i] = nxge_st_txdma_pport[i];
+
+				if (custom_start_txdma[i] >= NXGE_MAX_TDCS) {
+					good_custom_config = B_FALSE;
+					break;
+				}
+
+				if ((tx_port_arr[i] > NXGE_MAX_TDCS) ||
+				    ((tx_port_arr[i] + custom_start_txdma[i])
+				> NXGE_MAX_TDCS)) {
+					good_custom_config = B_FALSE;
+					break;
+				}
+
+				ntdcs += tx_port_arr[i];
+				if (ntdcs > NXGE_MAX_TDCS) {
+					good_custom_config = B_FALSE;
+					break;
+				}
+
+				tdc_bitmap[i] = 0;
+				for (bits = 0;
+				    bits < tx_port_arr[i]; bits++) {
+					    tdc_bitmap[i] |=
+					    (1 <<
+					    (bits + custom_start_txdma[i]));
+				}
+			}
+			if (good_custom_config == B_TRUE) {
+				/* check for overlap */
+				for (i = 0; i < num_ports - 1; i++) {
+					for (j = i + 1; j < num_ports; j++) {
+						if (tdc_bitmap[i] &
+						    tdc_bitmap[j]) {
+							NXGE_ERROR_MSG((nxgep,
+							    NXGE_ERR_CTL,
+							    " txdma-cfg"
+							    " property custom"
+							    " bit overlap"
+							    " %d %d ",
+							    i, j));
+							good_custom_config =
+								B_FALSE;
+							break;
+						}
+					}
+					if (good_custom_config == B_FALSE)
+						break;
+				}
+			}
+
+		}
+		if (good_custom_config == B_FALSE) {
+			tmp_tx_port_arr = (num_ports == 4) ?
+				p4_tx_equal : p2_tx_equal;
+			for (i = 0; i < num_ports; i++) {
+				tx_port_arr[i] = tmp_tx_port_arr[i];
+			}
+			for (i = 0; i < func; i++) {
+				start_tdc += tx_port_arr[i];
+			}
+		} else {
+			start_tdc = custom_start_txdma[func];
+		}
+
+		/* RXDMA config */
+		if ((num_rxdmas_pport < num_ports) ||
+		    (num_st_rxdma_pport < num_ports)) {
+
+			good_custom_config = B_FALSE;
+		} else {
+			for (i = 0; i < num_ports; i++) {
+
+				rx_port_arr[i] = nxge_rxdmas_pport[i];
+				custom_start_rxdma[i] = nxge_st_rxdma_pport[i];
+
+				if (custom_start_rxdma[i] >= NXGE_MAX_RDCS) {
+					good_custom_config = B_FALSE;
+					break;
+				}
+
+				if ((rx_port_arr[i] > NXGE_MAX_RDCS) ||
+				    ((rx_port_arr[i] + custom_start_rxdma[i])
+				    > NXGE_MAX_RDCS)) {
+					good_custom_config = B_FALSE;
+					break;
+				}
+
+				nrdcs += rx_port_arr[i];
+				if (nrdcs > NXGE_MAX_RDCS) {
+					good_custom_config = B_FALSE;
+					break;
+				}
+
+				rdc_bitmap[i] = 0;
+				for (bits = 0;
+				    bits < rx_port_arr[i]; bits++) {
+					    rdc_bitmap[i] |=
+					    (1 <<
+					    (bits + custom_start_rxdma[i]));
+				}
+			}
+			if (good_custom_config == B_TRUE) {
+				/* check for overlap */
+				for (i = 0; i < num_ports - 1; i++) {
+					for (j = i + 1; j < num_ports; j++) {
+						if (rdc_bitmap[i] &
+						    rdc_bitmap[j]) {
+							NXGE_ERROR_MSG((nxgep,
+							    NXGE_ERR_CTL,
+							    " rxdma-cfg"
+							    " property custom"
+							    " bit overlap"
+							    " %d %d ",
+							    i, j));
+							good_custom_config =
+								B_FALSE;
+							break;
+						}
+					}
+					if (good_custom_config == B_FALSE)
+						break;
+				}
+			}
+		}
+		if (good_custom_config == B_FALSE) {
+			tmp_rx_port_arr = (num_ports == 4) ?
+				p4_rx_equal : p2_rx_equal;
+			for (i = 0; i < num_ports; i++) {
+				rx_port_arr[i] = tmp_rx_port_arr[i];
+			}
+			for (i = 0; i < func; i++) {
+				start_rdc += rx_port_arr[i];
+			}
+		} else {
+			start_rdc = custom_start_rxdma[func];
+		}
+
+		/* RDC GRP config */
+		if ((num_rdcgrps_pport < num_ports) ||
+		    (num_st_rdcgrp_pport < num_ports)) {
+
+			good_custom_config = B_FALSE;
+		} else {
+			for (i = 0; i < num_ports; i++) {
+
+				rxgrp_port_arr[i] = nxge_rdcgrps_pport[i];
+				custom_start_rdcgrp[i] =
+					    nxge_st_rdcgrp_pport[i];
+
+				if (custom_start_rdcgrp[i] >=
+				    NXGE_MAX_RDC_GRPS) {
+					good_custom_config = B_FALSE;
+					break;
+				}
+
+				if ((rxgrp_port_arr[i] > NXGE_MAX_RDC_GRPS)
+				    || ((rxgrp_port_arr[i] +
+					custom_start_rdcgrp[i]) >
+					NXGE_MAX_RDC_GRPS)) {
+					good_custom_config = B_FALSE;
+					break;
+				}
+
+				nrdcgrps += rxgrp_port_arr[i];
+				if (nrdcgrps > NXGE_MAX_TDCS) {
+					good_custom_config = B_FALSE;
+					break;
+				}
+
+				rdcgrp_bitmap[i] = 0;
+				for (bits = 0;
+				    bits < rxgrp_port_arr[i]; bits++) {
+					    rdcgrp_bitmap[i] |=
+					    (1 <<
+					    (bits + custom_start_rdcgrp[i]));
+				}
+			}
+			if (good_custom_config == B_TRUE) {
+				/* check for overlap */
+				for (i = 0; i < num_ports - 1; i++) {
+					for (j = i + 1; j < num_ports; j++) {
+						if (rdcgrp_bitmap[i] &
+						    rdcgrp_bitmap[j]) {
+							NXGE_ERROR_MSG((nxgep,
+							    NXGE_ERR_CTL,
+							    " rxGrp-cfg"
+							    " property custom"
+							    " bit overlap"
+							    " %d %d ",
+							    i, j));
+							good_custom_config =
+								B_FALSE;
+							break;
+						}
+					}
+					if (good_custom_config == B_FALSE)
+						break;
+				}
+			}
+		}
+
+		if (good_custom_config == B_FALSE) {
+			tmp_rxgrp_port_arr = (num_ports == 4) ?
+				p4_rdcgrp_equal : p2_rdcgrp_equal;
+			for (i = 0; i < num_ports; i++) {
+				rxgrp_port_arr[i] = tmp_rxgrp_port_arr[i];
+			}
+			for (i = 0; i < func; i++) {
+				start_rdc_grp += rxgrp_port_arr[i];
+			}
+		} else {
+			start_rdc_grp = custom_start_rdcgrp[func];
+		}
+
+		break;
+	case FAIR:
+		tmp_tx_port_arr = (num_ports == 4) ? p4_tx_fair : p2_tx_fair;
+		tmp_rx_port_arr = (num_ports == 4) ? p4_rx_fair : p2_rx_fair;
+		tmp_rxgrp_port_arr = (num_ports == 4) ?
+			p4_rdcgrp_fair : p2_rdcgrp_fair;
+
+		for (i = 0; i < num_ports; i++) {
+			tx_port_arr[i] = tmp_tx_port_arr[i];
+			rx_port_arr[i] = tmp_rx_port_arr[i];
+			rxgrp_port_arr[i] = tmp_rxgrp_port_arr[i];
+		}
+
+		for (i = 0; i < func; i++) {
+			start_tdc += tx_port_arr[i];
+			start_rdc += rx_port_arr[i];
+			start_rdc_grp += rxgrp_port_arr[i];
+		}
+
+		break;
+	case EQUAL:
+	default:
+		tmp_tx_port_arr = (num_ports == 4) ?
+			p4_tx_equal : p2_tx_equal;
+		tmp_rx_port_arr = (num_ports == 4) ?
+			p4_rx_equal : p2_rx_equal;
+		tmp_rxgrp_port_arr = (num_ports == 4) ?
+			p4_rdcgrp_equal : p2_rdcgrp_equal;
+
+		for (i = 0; i < num_ports; i++) {
+			tx_port_arr[i] = tmp_tx_port_arr[i];
+			rx_port_arr[i] = tmp_rx_port_arr[i];
+			rxgrp_port_arr[i] = tmp_rxgrp_port_arr[i];
+		}
+
+		for (i = 0; i < func; i++) {
+			start_tdc += tx_port_arr[i];
+			start_rdc += rx_port_arr[i];
+			start_rdc_grp += rxgrp_port_arr[i];
+		}
+
+		break;
+	}
+
+	/* Transmit DMA */
+	ntxdmas = tx_port_arr[func];
+
+#ifdef CONFIG_PCI_MSI
+	if (nxge_msix_enable) {
+			/* Receive DMA */
+		nrxdmas = rx_port_arr[func];
+			/* Receive DMA Group */
+		nrxgrps = rxgrp_port_arr[func];
+	} else {
+		nrxdmas = 1;
+		nrxgrps = 1;
+	}
+#else
+/* INTX case */
+	nrxdmas = 1;
+	nrxgrps = 1;
+#endif
+
+
+	p_cfgp->start_tdc = nxgep->start_tdc = start_tdc;
+	p_cfgp->max_tdcs =  nxgep->max_tdcs = ntxdmas;
+	for (i = 0; i < ntxdmas; i++) {
+		nxgep->statsp->tdc_stats[i].channel_num = start_tdc + i;
+	}
+
+	p_cfgp->start_rdc = nxgep->start_rdc = start_rdc;
+	p_cfgp->max_rdcs = nxgep->max_rdcs = nrxdmas;
+	for (i = 0; i < nrxdmas; i++) {
+		nxgep->statsp->rdc_stats[i].channel_num = start_rdc + i;
+	}
+
+	p_cfgp->max_rdc_grpids = nrxgrps;
+	p_cfgp->start_rdc_grpid = start_rdc_grp;
+
+	/*
+	 * 2/4 ports have the same hard-wired logical
+	 * groups assigned.
+	 */
+	p_cfgp->start_ldg = func * NXGE_LDGRP_PER_4PORTS;
+	p_cfgp->max_ldgs = (p_cfgp->max_rdcs < NXGE_LDGRP_PER_4PORTS) ?
+		(p_cfgp->max_rdcs + 1) : NXGE_LDGRP_PER_4PORTS;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_cfg_set_dma_cfg"));
+
+	return (0);
+}
+
+int
+nxge_cfg_set_quick_config(p_nxge_t nxgep)
+{
+
+	int status = NXGE_OK;
+	uint32_t cfg_value = CFG_NOT_SPECIFIED;
+	p_nxge_param_t param_arr = nxgep->param_arr;
+
+	/*
+	 * good value are
+	 *
+	 * "web-server"
+	 * "generic-server"
+	 * "l3-classify"
+	 * "flow-classify"
+	 */
+
+	if (strcmp(NXGE_WEB_SVR_CFG_STR, nxge_rx_quick_cfg_str) == 0) {
+		cfg_value = CFG_L3_WEB;
+		NXGE_DEBUG_MSG((nxgep, CFG_CTL,
+				    "Rx quick config: web server "));
+	}
+
+	if (strcmp(NXGE_GEN_SVR_CFG_STR, nxge_rx_quick_cfg_str) == 0) {
+		cfg_value = CFG_L3_DISTRIBUTE;
+		NXGE_DEBUG_MSG((nxgep, CFG_CTL,
+				    "Rx quick config: distribute "));
+	}
+		/* more */
+
+	param_arr[param_rx_qcfg_type].value =
+		param_arr[param_rx_qcfg_type].conf_value = cfg_value;
+		/* now handle specified cases: */
+	return (status);
+}
+
+static int
+nxge_private_param_register(p_nxge_t nxgep, p_nxge_param_t param_arr)
+{
+
+	int status = B_TRUE;
+	int channel;
+	uint8_t grp;
+	char *prop_name;
+	char *end;
+	uint32_t name_chars;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL,
+			    " nxge_private_param_register %s",
+			    param_arr->name));
+
+	if ((param_arr->type & NXGE_PARAM_PRIV) != NXGE_PARAM_PRIV)
+		return (B_TRUE);
+	prop_name =  param_arr->name;
+	if (param_arr->type & NXGE_PARAM_RXDMA) {
+		if (strncmp("rxdma_intr", prop_name, 10) == 0)
+			return (B_TRUE);
+		name_chars = strlen("default_grp");
+		if (strncmp("default_grp", prop_name, name_chars) == 0) {
+			prop_name += name_chars;
+			grp = simple_strtol(prop_name, &end, 10);
+				/* now check if this rdcgrp is in config */
+			return (nxge_check_rdcgrp_port_member(nxgep, grp));
+		}
+		name_chars = strlen(prop_name);
+		if (strncmp("default_port_rdc", prop_name, name_chars) == 0) {
+			return (B_TRUE);
+		}
+
+		return (B_FALSE);
+	}
+
+	if (param_arr->type & NXGE_PARAM_TXDMA) {
+		name_chars = strlen("txdma");
+		if (strncmp("txdma", prop_name, name_chars) == 0) {
+			prop_name += name_chars;
+			channel = simple_strtol(prop_name, &end, 10);
+				/* now check if this rdc is in config */
+			NXGE_DEBUG_MSG((nxgep, CFG_CTL,
+					    " nxge_private_param_register: %d",
+					    channel));
+			return (nxge_check_txdma_port_member(nxgep, channel));
+		}
+		return (B_FALSE);
+	}
+
+	status = B_FALSE;
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_private_param_register"));
+
+	return (status);
+}
+
+void
+nxge_get_public_param_list_len(p_nxge_t nxgep, uint64_t *len_p)
+{
+	p_nxge_param_t param_arr;
+	int i, j, param_size;
+	p_nxge_dma_pt_cfg_t	p_all_cfgp;
+	p_nxge_hw_pt_cfg_t	p_cfgp;
+	int num_rdc_grps = 0;
+	boolean_t grp_exported = B_FALSE;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_get_public_param_list_len"));
+	param_size = 0;
+	param_arr = nxgep->param_arr;
+	param_arr[param_function_number].value = nxgep->function_num;
+	nxgep->tx_lb_policy = nxge_get_tx_lb_policy(nxgep);
+
+	p_all_cfgp = (p_nxge_dma_pt_cfg_t)&nxgep->pt_config;
+	p_cfgp = (p_nxge_hw_pt_cfg_t)&p_all_cfgp->hw_config;
+	num_rdc_grps = p_cfgp->max_rdc_grpids;
+
+	for (i = 0; i < nxgep->param_count; i++) {
+		if ((param_arr[i].type & NXGE_PARAM_PRIV) ||
+		    (param_arr[i].type & NXGE_PARAM_CMPLX) ||
+		    (param_arr[i].type & NXGE_PARAM_DONT_SHOW) ||
+		    (param_arr[i].type & NXGE_PARAM_INIT_ONLY)) {
+			continue;
+		}
+
+		if (param_arr[i].type & NXGE_PARAM_PER_PORT_CHK) {
+			if (strncmp(param_arr[i].name, "default_grp", 11) == 0
+			    && strncmp(param_arr[i].name + 12, "_rdc", 4) == 0) {
+				if (grp_exported)
+					continue;
+				for (j = 0; j < num_rdc_grps; j++) {
+					param_size += sizeof (nxge_cfg_attr_t);
+				}
+				grp_exported = B_TRUE;
+				continue;
+			}
+
+		}
+		param_size += sizeof (nxge_cfg_attr_t);
+	}
+
+	*len_p = param_size;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_get_public_param_list_len"));
+}
+
+void
+nxge_get_public_param_list(p_nxge_t nxgep, uint8_t *param_list)
+{
+	p_nxge_param_t param_arr;
+	int i, j;
+	nxge_cfg_attr_t *param_p;
+	p_nxge_dma_pt_cfg_t	p_all_cfgp;
+	p_nxge_hw_pt_cfg_t	p_cfgp;
+	int num_rdc_grps = 0;
+	char	grp_param[20];
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_get_public_param_list_len"));
+
+	param_arr = nxgep->param_arr;
+	param_arr[param_function_number].value = nxgep->function_num;
+
+	p_all_cfgp = (p_nxge_dma_pt_cfg_t)&nxgep->pt_config;
+	p_cfgp = (p_nxge_hw_pt_cfg_t)&p_all_cfgp->hw_config;
+	num_rdc_grps = p_cfgp->max_rdc_grpids;
+
+	for (i = 0; i < nxgep->param_count; i++) {
+		if ((param_arr[i].type & NXGE_PARAM_PRIV) ||
+		    (param_arr[i].type & NXGE_PARAM_CMPLX) ||
+		    (param_arr[i].type & NXGE_PARAM_DONT_SHOW) ||
+		    (param_arr[i].type & NXGE_PARAM_INIT_ONLY)) {
+			continue;
+		}
+
+		if (param_arr[i].type & NXGE_PARAM_PER_PORT_CHK) {
+			if (strncmp(param_arr[i].name, "default_grp", 11) == 0
+			    && strncmp(param_arr[i].name + 12, "_rdc", 4) == 0) {
+				for (j = 0; j < num_rdc_grps; j++) {
+					sprintf(grp_param,
+						"default_grp%1d_rdc", j);
+					if (strcmp(grp_param,
+						   param_arr[i].name) == 0) {
+						param_p =
+							(nxge_cfg_attr_t *)param_list;
+						strcpy(param_p->param_name,
+						       param_arr[i].name);
+						if (param_arr[i].type &
+						    NXGE_PARAM_READ) {
+							if (param_arr[i].type
+							    & NXGE_PARAM_WRITE) {
+								param_p->cfg_type = rw;
+							} else {
+								param_p->cfg_type = ro;
+							}
+						}
+						if (param_arr[i].type &
+						    NXGE_PARAM_DUMP_STR) {
+							param_p->is_dump_str = 1;
+						} else {
+							param_p->is_dump_str = 0;
+						}
+						param_list +=
+							(sizeof (nxge_cfg_attr_t));
+					}
+				}
+				continue;
+			}
+
+		}
+
+		param_p = (nxge_cfg_attr_t *)param_list;
+		strcpy(param_p->param_name, param_arr[i].name);
+		if (param_arr[i].type & NXGE_PARAM_READ) {
+		    if (param_arr[i].type & NXGE_PARAM_WRITE) {
+			param_p->cfg_type = rw;
+		    } else {
+			    param_p->cfg_type = ro;
+		    }
+		}
+		if (param_arr[i].type & NXGE_PARAM_DUMP_STR) {
+			param_p->is_dump_str = 1;
+		} else {
+			param_p->is_dump_str = 0;
+		}
+                param_list += (sizeof (nxge_cfg_attr_t));
+	}
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_get_public_param_list"));
+}
+
+void
+nxge_setup_param(p_nxge_t nxgep)
+{
+	p_nxge_param_t param_arr;
+	int i;
+/* 	pfi_t set_pfi; */
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_setup_param"));
+
+	param_arr = nxgep->param_arr;
+	param_arr[param_function_number].value = nxgep->function_num;
+	nxgep->tx_lb_policy = nxge_get_tx_lb_policy(nxgep);
+	for (i = 0; i < nxgep->param_count; i++) {
+		if ((param_arr[i].type & NXGE_PARAM_PRIV) &&
+			(nxge_private_param_register(nxgep,
+						    &param_arr[i]) ==
+			    B_FALSE)) {
+			param_arr[i].setf = NULL;
+			param_arr[i].getf = NULL;
+		}
+
+		if (param_arr[i].type & NXGE_PARAM_CMPLX)
+			param_arr[i].setf = NULL;
+
+		if (param_arr[i].type & NXGE_PARAM_DONT_SHOW) {
+			param_arr[i].setf = NULL;
+			param_arr[i].getf = NULL;
+		}
+
+#ifdef TODO
+		set_pfi = (pfi_t)param_arr[i].setf;
+
+		if ((set_pfi) &&
+			(param_arr[i].type & NXGE_PARAM_INIT_ONLY)) {
+			set_pfi = NULL;
+		}
+
+
+		if (!nxge_nd_load(&nxgep->param_list,
+				param_arr[i].name,
+				(pfi_t)param_arr[i].getf,
+				set_pfi,
+				(caddr_t)&param_arr[i])) {
+			(void) nxge_nd_free(&nxgep->param_list);
+			break;
+		}
+#endif
+	}
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_setup_param"));
+}
+
+
+void
+nxge_init_param(p_nxge_t nxgep)
+{
+	p_nxge_param_t param_arr;
+	int i;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_init_param"));
+
+	param_arr = nxgep->param_arr;
+	if (param_arr == NULL) {
+		param_arr = (p_nxge_param_t)KMEM_ZALLOC(
+				sizeof (nxge_param_arr), GFP_ATOMIC);
+	}
+
+	memcpy(param_arr, nxge_param_arr, sizeof (nxge_param_arr));
+	nxgep->param_arr = param_arr;
+	nxgep->param_count = sizeof (nxge_param_arr)/sizeof (nxge_param_t);
+	for (i = 0; i < nxgep->param_count; i++) {
+		if (param_arr[i].conf_value != NXGE_PARAM_NOT_CONF) {
+			param_arr[i].setf(nxgep, param_arr[i].conf_value,
+					  (caddr_t)(&param_arr[i]));
+		}
+	}
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_init_param: count %d",
+			nxgep->param_count));
+}
+
+void
+nxge_destroy_param(p_nxge_t nxgep)
+{
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_destroy_param"));
+	if (nxgep->param_arr)
+                KMEM_FREE(nxgep->param_arr, sizeof (nxge_param_arr));
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_destroy_param"));
+}
+
+int
+nxge_get_public_param(p_nxge_t nxgep, char *param_name, uint64_t *value,
+		      uint8_t *dump_str)
+{
+	nxge_param_t	*param_p = NULL;
+
+	nxge_lookup_param(nxgep, param_name, &param_p);
+	if (param_p == NULL) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL, "nxge_get_public_param: "
+				"Unknown param %s", param_name));
+		return (-1);
+	}
+
+	param_p->getf(nxgep, (caddr_t)param_p, value, dump_str);
+
+	return (0);
+}
+
+
+/*
+ * Extracts the value from the 'nxge' parameter array and prints the
+ * parameter value. cp points to the required parameter.
+ */
+/* ARGSUSED */
+int
+
+nxge_param_get_generic(p_nxge_t nxgep, caddr_t cp, uint64_t *value,
+		       uint8_t *dump_str)
+{
+	p_nxge_param_t pa = (p_nxge_param_t)cp;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, " ==> nxge_param_get_generic"));
+	*value = pa->value;
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_param_get_generic"));
+	return (0);
+}
+
+
+static int
+nxge_param_get_mac(p_nxge_t nxgep, caddr_t cp, uint64_t *value,
+		   uint8_t *dump_str)
+{
+	p_nxge_param_t pa = (p_nxge_param_t)cp;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_param_get_mac"));
+	*value = pa->value;
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_param_get_mac"));
+	return (0);
+}
+
+static int
+nxge_param_get_llc_snap_enable(p_nxge_t nxgep, caddr_t cp,
+			       uint64_t *value, uint8_t *dump_str)
+{
+
+	fflp_cfg_1_t fflp_cfg;
+	uint64_t offset;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_param_get_llc_snap_enable"));
+
+	offset = FFLP_CFG_1_REG;
+	REG_PIO_READ64(nxgep->npi_handle, offset, &fflp_cfg.value);
+
+	*value = (fflp_cfg.bits.ldw.llcsnap ? 1 : 0);
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_param_get_llc_snap_enable"));
+	return (0);
+}
+
+
+/* ARGSUSED */
+int
+nxge_param_get_txdma_info(p_nxge_t nxgep, caddr_t cp, uint64_t *value,
+			  uint8_t *dump_str)
+{
+	int tdc;
+	char *str_ptr = (char *)dump_str;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_param_get_txdma_info"));
+
+	sprintf(str_ptr, "TXDMA Information for Port\t %d \n",
+		nxgep->function_num);
+
+	str_ptr += strlen(str_ptr);
+
+	sprintf(str_ptr, "Total TDCs\t %d\n", nxgep->ntdc);
+	str_ptr += strlen(str_ptr);
+	sprintf(str_ptr, "TDC\t HW TDC\t\n");
+	str_ptr += strlen(str_ptr);
+	for (tdc = 0; tdc < nxgep->ntdc; tdc++) {
+		sprintf(str_ptr, "%d\t %d\n",
+			tdc, nxgep->tdc[tdc]);
+		str_ptr += strlen(str_ptr);
+	}
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_param_get_txdma_info"));
+	return (0);
+}
+
+
+/* ARGSUSED */
+int
+nxge_param_get_rxdma_info(p_nxge_t nxgep, caddr_t cp, uint64_t *value,
+			  uint8_t *dump_str)
+{
+	int rdc;
+	char *str_ptr = (char *)dump_str;
+
+	p_nxge_dma_pt_cfg_t	p_dma_cfgp;
+	p_nxge_hw_pt_cfg_t	p_cfgp;
+
+	p_rx_rcr_rings_t 	rx_rcr_rings;
+	p_rx_rcr_ring_t		*rcr_rings = NULL;
+	p_rx_rbr_rings_t 	rx_rbr_rings;
+	p_rx_rbr_ring_t		*rbr_rings = NULL;
+
+	int print_ring_ptrs = 0;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_param_get_rxdma_info"));
+
+	sprintf(str_ptr, "RXDMA Information for Port\t %d \n",
+		nxgep->function_num);
+
+	str_ptr += strlen(str_ptr);
+
+	p_dma_cfgp = (p_nxge_dma_pt_cfg_t)&nxgep->pt_config;
+	p_cfgp = (p_nxge_hw_pt_cfg_t)&p_dma_cfgp->hw_config;
+
+	rx_rcr_rings = nxgep->rx_rcr_rings;
+	rx_rbr_rings = nxgep->rx_rbr_rings;
+	if (rx_rcr_rings != NULL && rx_rbr_rings != NULL) {
+		rcr_rings = rx_rcr_rings->rings;
+		rbr_rings = rx_rbr_rings->rings;
+		print_ring_ptrs = 1;
+	}
+
+	sprintf(str_ptr, "Total RDCs\t %d\n",
+				    p_cfgp->max_rdcs);
+	str_ptr += strlen(str_ptr);
+
+	sprintf(str_ptr, "RDC\t HW RDC\t Timeout\t Threshold");
+	str_ptr += strlen(str_ptr);
+	if (print_ring_ptrs) {
+		sprintf(str_ptr, "\t  RBR ptr \t\tRCR ptr\n");
+	} else {
+		sprintf(str_ptr, "\n");
+	}
+	str_ptr += strlen(str_ptr);
+
+	for (rdc = 0; rdc < p_cfgp->max_rdcs; rdc++) {
+		sprintf(str_ptr, " %d\t  %d\t   %x\t\t %x",
+			    rdc, nxgep->rdc[rdc],
+			    p_dma_cfgp->rcr_timeout[rdc],
+			    p_dma_cfgp->rcr_threshold[rdc]);
+		str_ptr += strlen(str_ptr);
+		if (print_ring_ptrs) {
+			sprintf(str_ptr, "\t\t $%p\t $%p\n",
+			    rbr_rings[rdc],
+			    rcr_rings[rdc]);
+		} else {
+			sprintf(str_ptr, "\n");
+		}
+		str_ptr += strlen(str_ptr);
+	}
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_param_get_rxdma_info"));
+	return (0);
+}
+
+
+/* ARGSUSED */
+int
+nxge_param_get_rxdma_rdcgrp_info(p_nxge_t nxgep, caddr_t cp, uint64_t *value,
+				 uint8_t *dump_str)
+{
+	char *str_ptr = (char *)dump_str;
+	int offset, rdc;
+	int i, rdc_grp;
+	p_nxge_rdc_grp_t	rdc_grp_p;
+	p_nxge_dma_pt_cfg_t	p_dma_cfgp;
+	p_nxge_hw_pt_cfg_t	p_cfgp;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL,
+			    "==> nxge_param_get_rxdma_rdcgrp_info"));
+
+	p_dma_cfgp = (p_nxge_dma_pt_cfg_t)&nxgep->pt_config;
+	p_cfgp = (p_nxge_hw_pt_cfg_t)&p_dma_cfgp->hw_config;
+
+	sprintf(str_ptr, "RXDMA RDC Group Information for Port\t %d \n",
+		nxgep->function_num);
+	str_ptr += strlen(str_ptr);
+
+	rdc_grp = p_cfgp->start_rdc_grpid;
+
+	sprintf(str_ptr, "Total RDC Groups\t %d \n"
+		"start RDC group\t %d\n",
+		p_cfgp->max_rdc_grpids,
+		p_cfgp->start_rdc_grpid);
+	str_ptr += strlen(str_ptr);
+
+	for (i = 0, rdc_grp = p_cfgp->start_rdc_grpid;
+	    rdc_grp < (p_cfgp->max_rdc_grpids + p_cfgp->start_rdc_grpid);
+	    rdc_grp++, i++) {
+		rdc_grp_p = &p_dma_cfgp->rdc_grps[i];
+		sprintf(str_ptr, "\nRDC Group Info for Group [%d] %d\n"
+			"RDC Count %d\tstart RDC %d\n"
+				    "RDC Group Population Information"
+			" (offsets 0 - 15)\n",
+			i, rdc_grp, rdc_grp_p->max_rdcs,
+			rdc_grp_p->start_rdc);
+		str_ptr += strlen(str_ptr);
+		sprintf(str_ptr, "\n");
+		str_ptr += strlen(str_ptr);
+
+		for (rdc = 0; rdc < rdc_grp_p->max_rdcs; rdc++) {
+			sprintf(str_ptr, "[%d]=%d ", rdc,
+				rdc_grp_p->start_rdc + rdc);
+			str_ptr += strlen(str_ptr);
+		}
+		sprintf(str_ptr, "\n");
+		str_ptr += strlen(str_ptr);
+
+		for (offset = 0; offset < 16; offset++) {
+			sprintf(str_ptr, " %2d ", rdc_grp_p->rdc[offset]);
+			str_ptr += strlen(str_ptr);
+		}
+		sprintf(str_ptr, "\n");
+		str_ptr += strlen(str_ptr);
+	}
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL,
+			"<== nxge_param_get_rxdma_rdcgrp_info"));
+	return (0);
+}
+
+
+int
+nxge_set_public_param(p_nxge_t nxgep, char *param_name, uint64_t value)
+{
+	nxge_param_t	*param_p = NULL;
+
+	nxge_lookup_param(nxgep, param_name, &param_p);
+	if (param_p == NULL) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL, "nxge_set_public_param: "
+				"Unknown param %s", param_name));
+		return (-1);
+	}
+
+	if (!(param_p->type & NXGE_PARAM_WRITE)) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL, "nxge_set_public_param: "
+				"Param %s not writable", param_name));
+		return (-1);
+	}
+
+	param_p->setf(nxgep, value, (caddr_t)param_p);
+
+	return (0);
+}
+
+/*
+ * Sets the ge parameter to the value in the nxge_param_register using
+ * nxge_nd_load().
+ */
+/* ARGSUSED */
+int
+nxge_param_set_generic(p_nxge_t nxgep, uint64_t value, caddr_t cp)
+{
+	uint32_t new_value;
+	p_nxge_param_t pa = (p_nxge_param_t)cp;
+
+	NXGE_DEBUG_MSG((nxgep, IOC_CTL, " ==> nxge_param_set_generic"));
+
+	new_value = value;
+	if (new_value < pa->minimum || new_value > pa->maximum) {
+			return (EINVAL);
+	}
+	pa->value = new_value;
+
+	NXGE_DEBUG_MSG((nxgep, IOC_CTL, " <== nxge_param_set_generic"));
+	return (0);
+}
+
+
+/*
+ * Sets the ge parameter to the value in the nxge_param_register using
+ * nxge_nd_load().
+ */
+/* ARGSUSED */
+
+int
+nxge_param_set_mac(p_nxge_t nxgep, uint64_t value, caddr_t cp)
+{
+	uint32_t new_value;
+	int status = 0;
+	p_nxge_param_t pa = (p_nxge_param_t)cp;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_param_set_mac"));
+	new_value = value;
+	if (new_value < pa->minimum || new_value > pa->maximum) {
+			return (EINVAL);
+	}
+	if (pa->value != new_value) {
+		pa->old_value = pa->value;
+		pa->value = new_value;
+	}
+
+	if (!nxge_param_link_update(nxgep)) {
+		NXGE_DEBUG_MSG((nxgep, CFG_CTL,
+				    " false ret from nxge_param_link_update"));
+		status = EINVAL;
+	}
+
+
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_param_set_mac"));
+	return (status);
+}
+
+static void
+nxge_lookup_param(p_nxge_t nxgep, char *param_name, p_nxge_param_t *param_p)
+{
+	int i;
+	p_nxge_param_t param_arr = nxgep->param_arr;
+
+	for (i = 0; i < nxgep->param_count; i++) {
+		if (strcmp(param_arr[i].name, param_name) == 0) {
+			*param_p = &param_arr[i];
+			break;
+		}
+	}
+}
+
+
+/* ARGSUSED */
+static int
+nxge_param_set_mac_rdcgrp(p_nxge_t nxgep, uint64_t value, caddr_t cp)
+{
+	uint32_t status = 0, cfg_value;
+	p_nxge_param_t pa = (p_nxge_param_t)cp;
+	uint32_t cfg_it = B_FALSE;
+	p_nxge_dma_pt_cfg_t	p_dma_cfgp;
+	p_nxge_hw_pt_cfg_t	p_cfgp;
+	uint32_t *val_ptr, *old_val_ptr;
+	nxge_param_map_t *mac_map;
+	p_nxge_class_pt_cfg_t 	p_class_cfgp;
+	nxge_mv_cfg_t	*mac_host_info;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_param_set_mac_rdcgrp "));
+
+	p_dma_cfgp = (p_nxge_dma_pt_cfg_t)&nxgep->pt_config;
+	p_cfgp = (p_nxge_hw_pt_cfg_t)&p_dma_cfgp->hw_config;
+	p_class_cfgp = (p_nxge_class_pt_cfg_t)&nxgep->class_config;
+	mac_host_info = (nxge_mv_cfg_t	*)&p_class_cfgp->mac_host_info[0];
+
+	cfg_value = value;
+		/* now do decoding */
+		/* */
+	mac_map = (nxge_param_map_t *)&cfg_value;
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL,
+			    " cfg_value %x id %x map_to %x",
+			    cfg_value, mac_map->param_id,
+			    mac_map->map_to));
+
+	if ((mac_map->param_id < p_cfgp->max_macs) &&
+		(mac_map->map_to <
+		    (p_cfgp->max_rdc_grpids + p_cfgp->start_rdc_grpid)) &&
+		    (mac_map->map_to >= p_cfgp->start_rdc_grpid)) {
+		NXGE_DEBUG_MSG((nxgep, CFG_CTL,
+				    " nxge_param_set_mac_rdcgrp mapping"
+				    " id %d grp %d",
+				    mac_map->param_id, mac_map->map_to));
+		val_ptr = (uint32_t *)pa->value;
+		old_val_ptr = (uint32_t *)pa->old_value;
+		if (val_ptr[mac_map->param_id] != cfg_value) {
+			old_val_ptr[mac_map->param_id] =
+				    val_ptr[mac_map->param_id];
+			val_ptr[mac_map->param_id] = cfg_value;
+			mac_host_info[mac_map->param_id].mpr_npr =
+				    mac_map->pref;
+			mac_host_info[mac_map->param_id].flag = 1;
+			mac_host_info[mac_map->param_id].rdctbl =
+				    mac_map->map_to;
+			cfg_it = B_TRUE;
+		}
+
+	} else {
+		return (EINVAL);
+	}
+
+	if (cfg_it == B_TRUE) {
+		status = nxge_logical_mac_assign_rdc_table(nxgep,
+						    (uint8_t)mac_map->param_id);
+		if (status != NXGE_OK)
+			return (EINVAL);
+	}
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_param_set_mac_rdcgrp"));
+	return (0);
+}
+
+/* ARGSUSED */
+static int
+nxge_param_set_vlan_rdcgrp(p_nxge_t nxgep, uint64_t value, caddr_t cp)
+{
+	uint32_t status = 0, cfg_value;
+	p_nxge_param_t pa = (p_nxge_param_t)cp;
+	uint32_t cfg_it = B_FALSE;
+	p_nxge_dma_pt_cfg_t	p_dma_cfgp;
+	p_nxge_hw_pt_cfg_t	p_cfgp;
+	uint32_t *val_ptr, *old_val_ptr;
+	nxge_param_map_t *vmap, *old_map;
+	p_nxge_class_pt_cfg_t 	p_class_cfgp;
+	uint64_t cfgd_vlans;
+	int i, inc = 0, cfg_position = 0;
+	nxge_mv_cfg_t	*vlan_tbl;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_param_set_vlan_rdcgrp "));
+
+	p_dma_cfgp = (p_nxge_dma_pt_cfg_t)&nxgep->pt_config;
+	p_cfgp = (p_nxge_hw_pt_cfg_t)&p_dma_cfgp->hw_config;
+	p_class_cfgp = (p_nxge_class_pt_cfg_t)&nxgep->class_config;
+	vlan_tbl = (nxge_mv_cfg_t *)&p_class_cfgp->vlan_tbl[0];
+
+	cfg_value = value;
+		/* now do decoding */
+		/* */
+	cfgd_vlans = ((pa->type &  NXGE_PARAM_ARRAY_CNT_MASK) >>
+			    NXGE_PARAM_ARRAY_CNT_SHIFT);
+
+	if (cfgd_vlans == NXGE_PARAM_ARRAY_INIT_SIZE) {
+		/*
+		 * for now, we process only upto max
+		 * NXGE_PARAM_ARRAY_INIT_SIZE parameters
+		 * In the future, we may want to expand
+		 * the storage array and continue
+		 */
+		return (EINVAL);
+	}
+	vmap = (nxge_param_map_t *)&cfg_value;
+	if ((vmap->param_id) &&
+		(vmap->param_id < NXGE_MAX_VLANS) &&
+		(vmap->map_to < p_cfgp->max_rdc_grpids)) {
+		NXGE_DEBUG_MSG((nxgep, CFG_CTL,
+				    " nxge_param_set_vlan_rdcgrp mapping"
+				    " id %d grp %d",
+				    vmap->param_id, vmap->map_to));
+		val_ptr = (uint32_t *)pa->value;
+		old_val_ptr = (uint32_t *)pa->old_value;
+
+		/* search to see if this vlan id is already configured */
+		for (i = 0; i < cfgd_vlans; i++) {
+			old_map = (nxge_param_map_t *)&val_ptr[i];
+			if ((old_map->param_id == 0) ||
+				(vmap->param_id == old_map->param_id) ||
+				(vlan_tbl[vmap->param_id].flag)) {
+				cfg_position = i;
+				break;
+			}
+		}
+
+		if (cfgd_vlans == 0) {
+			cfg_position = 0;
+			inc++;
+		}
+
+		if (i == cfgd_vlans) {
+			cfg_position = i;
+			inc++;
+		}
+
+		NXGE_DEBUG_MSG((nxgep, CFG_CTL,
+				    " set_vlan_rdcgrp mapping"
+				    " i %d cfgd_vlans %llx position %d ",
+				    i, cfgd_vlans, cfg_position));
+		if (val_ptr[cfg_position] != cfg_value) {
+			old_val_ptr[cfg_position] = val_ptr[cfg_position];
+			val_ptr[cfg_position] = cfg_value;
+			vlan_tbl[vmap->param_id].mpr_npr = vmap->pref;
+			vlan_tbl[vmap->param_id].flag = 1;
+			vlan_tbl[vmap->param_id].rdctbl =
+			    vmap->map_to + p_cfgp->start_rdc_grpid;
+			cfg_it = B_TRUE;
+			if (inc) {
+				cfgd_vlans++;
+				pa->type &= ~NXGE_PARAM_ARRAY_CNT_MASK;
+				pa->type |= (cfgd_vlans <<
+						    NXGE_PARAM_ARRAY_CNT_SHIFT);
+
+			}
+			NXGE_DEBUG_MSG((nxgep, CFG_CTL,
+					    " after: param_set_vlan_rdcgrp "
+					    " cfg_vlans %llx position %d \n",
+					    cfgd_vlans, cfg_position));
+		}
+
+	} else {
+		return (EINVAL);
+	}
+
+	if (cfg_it == B_TRUE) {
+		status = nxge_fflp_config_vlan_table(nxgep,
+						    (uint16_t)vmap->param_id);
+		if (status != NXGE_OK)
+			return (EINVAL);
+	}
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_param_set_vlan_rdcgrp"));
+	return (0);
+}
+
+
+
+
+
+/* ARGSUSED */
+static int
+nxge_param_get_vlan_rdcgrp(p_nxge_t nxgep, caddr_t cp, uint64_t *value,
+			   uint8_t *dump_str)
+{
+	int i;
+	p_nxge_param_t pa = (p_nxge_param_t)cp;
+	char *str_ptr = (char *)dump_str;
+
+	uint32_t *val_ptr;
+	nxge_param_map_t *vmap;
+	p_nxge_class_pt_cfg_t 	p_class_cfgp;
+	p_nxge_dma_pt_cfg_t	p_dma_cfgp;
+	p_nxge_hw_pt_cfg_t	p_cfgp;
+	uint64_t cfgd_vlans = 0;
+	nxge_mv_cfg_t	*vlan_tbl;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_param_set_vlan_rdcgrp "));
+	sprintf(str_ptr, "VLAN RDC Mapping Information for Port\t %d \n",
+		nxgep->function_num);
+
+	p_dma_cfgp = (p_nxge_dma_pt_cfg_t)&nxgep->pt_config;
+	p_cfgp = (p_nxge_hw_pt_cfg_t)&p_dma_cfgp->hw_config;
+
+	cfgd_vlans = (pa->type &  NXGE_PARAM_ARRAY_CNT_MASK) >>
+		NXGE_PARAM_ARRAY_CNT_SHIFT;
+
+	i = (int)cfgd_vlans;
+	p_class_cfgp = (p_nxge_class_pt_cfg_t)&nxgep->class_config;
+	vlan_tbl = (nxge_mv_cfg_t *)&p_class_cfgp->vlan_tbl[0];
+	sprintf(str_ptr, "Configured VLANs %d\n"
+		"VLAN ID\t RDC GRP (Actual/Port)\t"
+		" Preference\n", i);
+	str_ptr += strlen(str_ptr);
+
+	val_ptr = (uint32_t *)pa->value;
+
+	for (i = 0; i < cfgd_vlans; i++) {
+		vmap = (nxge_param_map_t *)&val_ptr[i];
+		if (p_class_cfgp->vlan_tbl[vmap->param_id].flag) {
+			sprintf(str_ptr, "  %d\t\t %d/%d\t\t %d\n",
+				vmap->param_id,
+				vlan_tbl[vmap->param_id].rdctbl,
+				vlan_tbl[vmap->param_id].rdctbl -
+				p_cfgp->start_rdc_grpid,
+				vlan_tbl[vmap->param_id].mpr_npr);
+			str_ptr += strlen(str_ptr);
+		}
+	}
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_param_get_vlan_rdcgrp"));
+	return (0);
+}
+
+
+
+/* ARGSUSED */
+static int
+nxge_param_get_mac_rdcgrp(p_nxge_t nxgep, caddr_t cp, uint64_t *value,
+			  uint8_t *dump_str)
+{
+	int i;
+	char *str_ptr = (char *)dump_str;
+
+	p_nxge_class_pt_cfg_t 	p_class_cfgp;
+	p_nxge_dma_pt_cfg_t	p_dma_cfgp;
+	p_nxge_hw_pt_cfg_t	p_cfgp;
+	nxge_mv_cfg_t	*mac_host_info;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_param_get_mac_rdcgrp "));
+	sprintf(str_ptr, " MAC ADDR RDC Mapping Information for Port\t %d\n",
+		nxgep->function_num);
+	str_ptr += strlen(str_ptr);
+
+	p_class_cfgp = (p_nxge_class_pt_cfg_t)&nxgep->class_config;
+	p_dma_cfgp = (p_nxge_dma_pt_cfg_t)&nxgep->pt_config;
+	p_cfgp = (p_nxge_hw_pt_cfg_t)&p_dma_cfgp->hw_config;
+	mac_host_info = (nxge_mv_cfg_t	*)&p_class_cfgp->mac_host_info[0];
+	sprintf(str_ptr, "MAC ID\t RDC GRP (Actual/Port)\t Prefernce\n");
+	str_ptr += strlen(str_ptr);
+	for (i = 0; i < p_cfgp->max_macs; i++) {
+		if (mac_host_info[i].flag) {
+			sprintf(str_ptr, "   %d\t  %d/%d\t\t %d\n",
+				i,
+				mac_host_info[i].rdctbl,
+				mac_host_info[i].rdctbl -
+				p_cfgp->start_rdc_grpid,
+				mac_host_info[i].mpr_npr);
+			str_ptr += strlen(str_ptr);
+		}
+	}
+	sprintf(str_ptr, "Done Info Dumping \n");
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_param_get_macrdcgrp"));
+
+	return (0);
+}
+
+
+
+/* ARGSUSED */
+static int
+nxge_param_tcam_enable(p_nxge_t nxgep, uint64_t value, caddr_t cp)
+{
+	uint32_t status = 0, cfg_value;
+	p_nxge_param_t pa = (p_nxge_param_t)cp;
+	uint32_t cfg_it = B_FALSE;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_param_tcam_enable"));
+
+	cfg_value = value;
+	if (pa->value != cfg_value) {
+		pa->old_value = pa->value;
+		pa->value = cfg_value;
+		cfg_it = B_TRUE;
+	}
+
+	if (cfg_it == B_TRUE) {
+		if (pa->value)
+			status = nxge_fflp_config_tcam_enable(nxgep);
+		else
+			status = nxge_fflp_config_tcam_disable(nxgep);
+		if (status != NXGE_OK)
+			return (EINVAL);
+	}
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, " <== nxge_param_tcam_enable"));
+	return (0);
+}
+
+/* ARGSUSED */
+static int
+nxge_param_set_rx_allow_all(p_nxge_t nxgep, uint64_t value, caddr_t cp)
+{
+	uint32_t status = 0, cfg_value;
+	p_nxge_param_t pa = (p_nxge_param_t)cp;
+	uint32_t cfg_it = B_FALSE;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_param_rx_allow_all"));
+
+	cfg_value = value;
+	if (pa->value != cfg_value) {
+		pa->old_value = pa->value;
+		pa->value = cfg_value;
+		cfg_it = B_TRUE;
+	}
+
+	if (cfg_it == B_TRUE) {
+		if (pa->value) {
+			status = nxge_enable_address_filter(nxgep,
+											nxgep->mac.portnum, B_TRUE);
+		} else {
+			status = nxge_enable_address_filter(nxgep,
+											nxgep->mac.portnum, B_FALSE);
+		}
+		if (status != NXGE_OK)
+			return (EINVAL);
+	}
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, " <== nxge_param_tcam_enable"));
+	return (0);
+}
+
+static int
+nxge_param_hash_lookup_enable(p_nxge_t nxgep, uint64_t value, caddr_t cp)
+{
+	uint32_t status = 0, cfg_value;
+	p_nxge_param_t pa = (p_nxge_param_t)cp;
+	uint32_t cfg_it = B_FALSE;
+
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_param_hash_lookup_enable"));
+
+	cfg_value = value;
+	if (pa->value != cfg_value) {
+		pa->old_value = pa->value;
+		pa->value = cfg_value;
+		cfg_it = B_TRUE;
+	}
+
+	if (cfg_it == B_TRUE) {
+		if (pa->value)
+			status = nxge_fflp_config_hash_lookup_enable(nxgep);
+		else
+			status = nxge_fflp_config_hash_lookup_disable(nxgep);
+		if (status != NXGE_OK)
+			return (EINVAL);
+	}
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, " <== nxge_param_hash_lookup_enable"));
+	return (0);
+}
+
+/* ARGSUSED */
+static int
+nxge_param_llc_snap_enable(p_nxge_t nxgep, uint64_t value, caddr_t cp)
+{
+	uint32_t status = 0, cfg_value;
+	p_nxge_param_t pa = (p_nxge_param_t)cp;
+	uint32_t cfg_it = B_FALSE;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_param_llc_snap_enable"));
+
+	cfg_value = value;
+	if (pa->value != cfg_value) {
+		pa->old_value = pa->value;
+		pa->value = cfg_value;
+		cfg_it = B_TRUE;
+	}
+
+	if (cfg_it == B_TRUE) {
+		if (pa->value)
+			status = nxge_fflp_config_llc_snap_enable(nxgep);
+		else
+			status = nxge_fflp_config_llc_snap_disable(nxgep);
+
+		if (status != NXGE_OK)
+			return (EINVAL);
+	}
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, " <== nxge_param_llc_snap_enable"));
+	return (0);
+}
+
+
+/* ARGSUSED */
+
+static int
+nxge_param_set_ether_usr(p_nxge_t nxgep, uint64_t value, caddr_t cp)
+{
+	char *end;
+	uint32_t status = 0, cfg_value;
+	uint8_t ether_class;
+	p_nxge_param_t pa = (p_nxge_param_t)cp;
+	uint8_t cfg_it = B_FALSE;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_param_set_ether_usr"));
+
+	cfg_value = value;
+	if (cfg_value < pa->minimum || cfg_value > pa->maximum) {
+			return (EINVAL);
+	}
+	if (pa->value != cfg_value) {
+		pa->old_value = pa->value;
+		pa->value = cfg_value;
+		cfg_it = B_TRUE;
+	}
+
+	/* do the actual hw setup  */
+	if (cfg_it == B_TRUE) {
+		ether_class = simple_strtol(pa->name, &end, 10);
+		NXGE_DEBUG_MSG((nxgep, CFG_CTL, " nxge_param_set_ether_usr"));
+	}
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_param_set_ether_usr"));
+	return (status);
+}
+
+/* ARGSUSED */
+static int
+nxge_param_set_ip_usr(p_nxge_t nxgep, uint64_t value, caddr_t cp)
+{
+	char *end;
+	tcam_class_t class;
+	uint32_t status = 0, cfg_value;
+	p_nxge_param_t pa = (p_nxge_param_t)cp;
+	uint32_t cfg_it = B_FALSE;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_param_set_ip_usr"));
+
+	cfg_value = value;
+	if (cfg_value < pa->minimum || cfg_value > pa->maximum) {
+			return (EINVAL);
+	}
+
+	if (pa->value != cfg_value) {
+		pa->old_value = pa->value;
+		pa->value = cfg_value;
+		cfg_it = B_TRUE;
+	}
+
+	/* do the actual hw setup with cfg_value. */
+	if (cfg_it == B_TRUE) {
+		class = simple_strtol(pa->name, &end, 10);
+		status = nxge_fflp_ip_usr_class_config(nxgep, class, pa->value);
+	}
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_param_set_ip_usr"));
+	return (status);
+}
+
+static int
+nxge_class_name_2value(p_nxge_t nxgep, char *name)
+{
+	int i;
+	int class_instance = param_class_opt_ip_usr4;
+	p_nxge_param_t param_arr;
+	param_arr = nxgep->param_arr;
+	for (i = TCAM_CLASS_IP_USER_4; i <= TCAM_CLASS_SCTP_IPV6; i++) {
+		if (strcmp(param_arr[class_instance].name, name) == 0)
+			return (i);
+		class_instance++;
+	}
+	return (-1);
+}
+
+/* ARGSUSED */
+static int
+nxge_param_set_tx_lb_policy(p_nxge_t nxgep, uint64_t value, caddr_t cp)
+{
+	uint32_t cfg_value, policy;
+	p_nxge_param_t pa = (p_nxge_param_t)cp;
+	uint32_t cfg_it = B_FALSE;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_param_set_ip_opt"));
+
+
+	cfg_value = value & 0xfffffff;
+	if (cfg_value < pa->minimum || cfg_value > pa->maximum) {
+			return (EINVAL);
+	}
+	policy = cfg_value;
+	if (policy != NXGE_TX_LB_NONE &&
+		policy != NXGE_TX_LB_TCP_PORT &&
+	    policy != NXGE_TX_LB_CPU_ID &&
+	    policy != NXGE_TX_LB_L4_PORT &&
+	    policy != NXGE_TX_LB_L4_SRC &&
+	    policy != NXGE_TX_LB_L4_DEST &&
+	    policy != NXGE_TX_LB_VLAN &&
+	    policy != NXGE_TX_LB_IP_ADDR &&
+	    policy != NXGE_TX_LB_IP_SRC &&
+	    policy != NXGE_TX_LB_IP_DEST &&
+	    policy != NXGE_TX_LB_DEST_MAC) {
+		NXGE_ERROR_MSG((NULL, NXGE_ERR_CTL,
+				"Invalid TX LB policy value 0x%x",
+				policy));
+		return (EINVAL);
+	}
+
+	if (pa->value != cfg_value) {
+		pa->old_value = pa->value;
+		pa->value = cfg_value;
+		cfg_it = B_TRUE;
+	}
+
+	if (cfg_it == B_TRUE) {
+		nxgep->tx_lb_policy = policy;
+	}
+
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_param_set_tx_lb_policy"));
+	return (0);
+}
+
+
+static int
+nxge_param_set_ip_opt(p_nxge_t nxgep, uint64_t value, caddr_t cp)
+{
+	uint32_t status, cfg_value;
+	p_nxge_param_t pa = (p_nxge_param_t)cp;
+	tcam_class_t class;
+	uint32_t cfg_it = B_FALSE;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_param_set_ip_opt"));
+
+
+	cfg_value = value & 0xffffff0;
+	if (cfg_value < pa->minimum || cfg_value > pa->maximum) {
+			return (EINVAL);
+	}
+
+	if (pa->value != cfg_value) {
+		pa->old_value = pa->value;
+		pa->value = cfg_value;
+		cfg_it = B_TRUE;
+	}
+
+	if (cfg_it == B_TRUE) {
+			/* do the actual hw setup  */
+		class = nxge_class_name_2value(nxgep, pa->name);
+		if (class == -1)
+			return (EINVAL);
+
+		status = nxge_fflp_ip_class_config(nxgep, class, pa->value);
+		if (status != NXGE_OK)
+			return (EINVAL);
+	}
+
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_param_set_ip_opt"));
+	return (0);
+}
+
+/* ARGSUSED */
+static int
+nxge_param_get_ip_opt(p_nxge_t nxgep, caddr_t cp, uint64_t *value,
+		      uint8_t *dump_str)
+{
+	uint32_t status, cfg_value;
+	p_nxge_param_t pa = (p_nxge_param_t)cp;
+	tcam_class_t class;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_param_get_ip_opt"));
+
+		/* do the actual hw setup  */
+	class = nxge_class_name_2value(nxgep, pa->name);
+	if (class == -1)
+		return (EINVAL);
+	cfg_value = 0;
+	status = nxge_fflp_ip_class_config_get(nxgep, class, &cfg_value);
+	if (status != NXGE_OK)
+		return (EINVAL);
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL,
+			    "nxge_param_get_ip_opt_get %x ", cfg_value));
+	pa->value = cfg_value;
+	*value = cfg_value & 0xfffffff0;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_param_get_ip_opt status "));
+	return (0);
+}
+
+
+/* ARGSUSED */
+static int
+nxge_param_fflp_hash_init(p_nxge_t nxgep, uint64_t value, caddr_t cp)
+{
+	char *end;
+	uint32_t status, cfg_value;
+	p_nxge_param_t pa = (p_nxge_param_t)cp;
+	tcam_class_t class;
+	uint32_t cfg_it = B_FALSE;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_param_fflp_hash_init"));
+
+	cfg_value = value;
+	if (cfg_value < pa->minimum || cfg_value > pa->maximum) {
+			return (EINVAL);
+	}
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL,
+			    " nxge_param_fflp_hash_init value %x",
+			    cfg_value));
+	if (pa->value != cfg_value) {
+		pa->old_value = pa->value;
+		pa->value = cfg_value;
+		cfg_it = B_TRUE;
+	}
+
+
+	if (cfg_it == B_TRUE) {
+		char *h_name;
+		/* do the actual hw setup */
+		h_name = pa->name;
+		h_name++;
+		class = simple_strtol(h_name, &end, 10);
+		switch (class) {
+			case 1:
+				status = nxge_fflp_set_hash1(nxgep,
+						    (uint32_t)pa->value);
+				break;
+
+			case 2:
+				status = nxge_fflp_set_hash2(nxgep,
+						    (uint16_t)pa->value);
+				break;
+
+			default:
+			NXGE_DEBUG_MSG((nxgep, CFG_CTL,
+					    " nxge_param_fflp_hash_init"
+					    " %s Wrong hash var %d",
+					    pa->name, class));
+			return (EINVAL);
+		}
+		if (status != NXGE_OK)
+			return (EINVAL);
+	}
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, " <== nxge_param_fflp_hash_init"));
+	return (0);
+}
+
+
+/* ARGSUSED */
+
+static int
+nxge_param_set_grp_rdc(p_nxge_t nxgep, uint64_t value, caddr_t cp)
+{
+	char *end;
+	uint32_t status = 0, cfg_value;
+	p_nxge_param_t pa = (p_nxge_param_t)cp;
+	uint32_t cfg_it = B_FALSE;
+	int rdc_grp;
+	uint8_t real_rdc;
+	p_nxge_dma_pt_cfg_t	p_dma_cfgp;
+	p_nxge_hw_pt_cfg_t	p_cfgp;
+	p_nxge_rdc_grp_t	rdc_grp_p;
+
+	p_dma_cfgp = (p_nxge_dma_pt_cfg_t)&nxgep->pt_config;
+	p_cfgp = (p_nxge_hw_pt_cfg_t)&p_dma_cfgp->hw_config;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_param_set_grp_rdc"));
+
+	cfg_value = value;
+	if (cfg_value < pa->minimum || cfg_value > pa->maximum) {
+			return (EINVAL);
+	}
+	if (cfg_value >= p_cfgp->max_rdcs) {
+		return (EINVAL);
+	}
+	if (pa->value != cfg_value) {
+		pa->old_value = pa->value;
+		pa->value = cfg_value;
+		cfg_it = B_TRUE;
+	}
+
+	if (cfg_it == B_TRUE) {
+		char *grp_name;
+		grp_name = pa->name;
+		grp_name += strlen("default-grp");
+		rdc_grp = simple_strtol(grp_name, &end, 10);
+		rdc_grp_p = &p_dma_cfgp->rdc_grps[rdc_grp];
+		real_rdc = rdc_grp_p->start_rdc + cfg_value;
+		if (nxge_check_rxdma_rdcgrp_member(nxgep, rdc_grp,
+						    cfg_value) == B_FALSE) {
+			pa->value = pa->old_value;
+			NXGE_DEBUG_MSG((nxgep, CFG_CTL,
+					    " nxge_param_set_grp_rdc"
+					    " %d read %d actual %d outof range",
+					    rdc_grp, cfg_value, real_rdc));
+			return (EINVAL);
+		}
+		status = nxge_rxdma_cfg_rdcgrp_default_rdc(nxgep, rdc_grp,
+							    real_rdc);
+		if (status != NXGE_OK)
+			return (EINVAL);
+	}
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_param_set_grp_rdc"));
+	return (0);
+}
+
+/* ARGSUSED */
+static int
+nxge_param_set_port_rdc(p_nxge_t nxgep, uint64_t value, caddr_t cp)
+{
+	uint32_t status = B_TRUE, cfg_value;
+	p_nxge_param_t pa = (p_nxge_param_t)cp;
+	uint32_t cfg_it = B_FALSE;
+
+	p_nxge_dma_pt_cfg_t	p_dma_cfgp;
+	p_nxge_hw_pt_cfg_t	p_cfgp;
+
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_param_set_port_rdc"));
+	p_dma_cfgp = (p_nxge_dma_pt_cfg_t)&nxgep->pt_config;
+	p_cfgp = (p_nxge_hw_pt_cfg_t)&p_dma_cfgp->hw_config;
+
+	cfg_value = value;
+	if (cfg_value < pa->minimum || cfg_value > pa->maximum) {
+			return (EINVAL);
+	}
+
+	if (pa->value != cfg_value) {
+		if (cfg_value >= p_cfgp->max_rdcs)
+			return (EINVAL);
+		pa->old_value = pa->value;
+		pa->value = cfg_value;
+		cfg_it = B_TRUE;
+	}
+
+	if (cfg_it == B_TRUE) {
+		status = nxge_rxdma_cfg_port_default_rdc(nxgep,
+						    nxgep->function_num,
+						    nxgep->rdc[cfg_value]);
+		if (status != NXGE_OK)
+			return (EINVAL);
+	}
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_param_set_port_rdc"));
+	return (0);
+}
+
+
+/* ARGSUSED */
+
+static int
+nxge_param_set_nxge_debug_flag(p_nxge_t nxgep, uint64_t value, caddr_t cp)
+{
+	uint64_t cfg_value = 0;
+	p_nxge_param_t pa = (p_nxge_param_t)cp;
+	uint32_t cfg_it = B_FALSE;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_param_set_nxge_debug_flag"));
+	cfg_value = value;
+
+	if (cfg_value < pa->minimum || cfg_value > pa->maximum) {
+		NXGE_DEBUG_MSG((nxgep, CFG_CTL,
+				    " nxge_param_set_nxge_debug_flag"
+				    " outof range %llx",
+				    cfg_value));
+		return (EINVAL);
+	}
+	if (pa->value != cfg_value) {
+		pa->old_value = pa->value;
+		pa->value = cfg_value;
+		cfg_it = B_TRUE;
+	}
+
+	if (cfg_it == B_TRUE) {
+		nxgep->nxge_debug_level = pa->value;
+	}
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_param_set_nxge_debug_flag"));
+	return (0);
+}
+
+static int
+nxge_param_set_npi_debug_flag(p_nxge_t nxgep, uint64_t value, caddr_t cp)
+{
+	uint64_t cfg_value = 0;
+	p_nxge_param_t pa = (p_nxge_param_t)cp;
+	uint32_t cfg_it = B_FALSE;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_param_set_npi_debug_flag"));
+	cfg_value = value;
+
+		if (cfg_value < pa->minimum || cfg_value > pa->maximum) {
+		NXGE_DEBUG_MSG((nxgep, CFG_CTL, " nxge_param_set_npi_debug_flag"
+				    " outof range %llx", cfg_value));
+		return (EINVAL);
+	}
+	if (pa->value != cfg_value) {
+		pa->old_value = pa->value;
+		pa->value = cfg_value;
+		cfg_it = B_TRUE;
+	}
+
+	if (cfg_it == B_TRUE) {
+		npi_debug_level = pa->value;
+	}
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_param_set_debug_flag"));
+	return (0);
+}
+
+
+/* ARGSUSED */
+int
+
+nxge_param_get_debug_flag(p_nxge_t nxgep, caddr_t cp, uint64_t *value,
+			  uint8_t *dump_str)
+{
+	p_nxge_param_t pa = (p_nxge_param_t)cp;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_param_get_debug_flag"));
+
+	*value = pa->value;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_param_get_debug_flag"));
+
+	return (0);
+}
+
+
+/* ARGSUSED */
+int
+nxge_param_dump_rdc(p_nxge_t nxgep, caddr_t cp, uint64_t *value,
+		    uint8_t *dump_str)
+{
+	uint_t	rdc;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_param_dump_rdc"));
+
+	(void) npi_rxdma_dump_fzc_regs(NXGE_DEV_NPI_HANDLE(nxgep));
+
+	for (rdc = 0; rdc < nxgep->nrdc; rdc++)
+		(void) nxge_dump_rxdma_channel(nxgep, nxgep->rdc[rdc]);
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_param_dump_rdc"));
+	return (0);
+}
+
+static int
+nxge_param_dump_tdc(p_nxge_t nxgep, caddr_t cp, uint64_t *value,
+		    uint8_t *dump_str)
+{
+
+	uint_t	tdc;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_param_dump_tdc"));
+
+	for (tdc = 0; tdc < nxgep->ntdc; tdc++)
+		(void) nxge_txdma_regs_dump(nxgep, nxgep->tdc[tdc]);
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_param_dump_tdc"));
+	return (0);
+}
+
+
+/* ARGSUSED */
+static int
+nxge_param_dump_fflp_regs(p_nxge_t nxgep, caddr_t cp, uint64_t *value,
+			  uint8_t *dump_str)
+{
+
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_param_dump_fflp_regs"));
+
+	(void) npi_fflp_dump_regs(NXGE_DEV_NPI_HANDLE(nxgep));
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_param_dump_fflp_regs"));
+	return (0);
+}
+
+/* ARGSUSED */
+static int
+nxge_param_dump_mac_regs(p_nxge_t nxgep, caddr_t cp, uint64_t *value,
+			 uint8_t *dump_str)
+{
+
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_param_dump_mac_regs"));
+
+	(void) npi_mac_dump_regs(NXGE_DEV_NPI_HANDLE(nxgep),
+				    nxgep->function_num);
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_param_dump_mac_regs"));
+	return (0);
+}
+
+/* ARGSUSED */
+static int
+nxge_param_dump_ipp_regs(p_nxge_t nxgep, caddr_t cp, uint64_t *value,
+			 uint8_t *dump_str)
+{
+
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_param_dump_ipp_regs"));
+
+	(void) 	(void) npi_ipp_dump_regs(NXGE_DEV_NPI_HANDLE(nxgep),
+					    nxgep->function_num);
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_param_dump_ipp_regs"));
+	return (0);
+}
+
+/* ARGSUSED */
+static int
+nxge_param_dump_vlan_table(p_nxge_t nxgep, caddr_t cp, uint64_t *value,
+			   uint8_t *dump_str)
+{
+
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_param_dump_vlan_table"));
+
+	(void) npi_fflp_vlan_tbl_dump(NXGE_DEV_NPI_HANDLE(nxgep));
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_param_dump_vlan_table"));
+	return (0);
+}
+
+/* ARGSUSED */
+static int
+nxge_param_dump_rdc_table(p_nxge_t nxgep, caddr_t cp, uint64_t *value,
+			  uint8_t *dump_str)
+{
+
+	uint8_t table;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_param_dump_rdc_table"));
+	for (table = 0; table < NXGE_MAX_RDC_GROUPS; table++) {
+		(void) npi_rxdma_dump_rdc_table(NXGE_DEV_NPI_HANDLE(nxgep),
+					    table);
+	}
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_param_dump_rdc_table"));
+	return (0);
+}
+
+
+typedef struct block_info {
+	char		*name;
+	uint32_t	offset;
+} block_info_t;
+
+block_info_t reg_block[] = {
+	{"PIO",		PIO},
+	{"FZC_PIO",	FZC_PIO},
+	{"FZC_XMAC",	FZC_MAC},
+	{"FZC_IPP",	FZC_IPP},
+	{"FFLP",	FFLP},
+	{"FZC_FFLP",	FZC_FFLP},
+	{"PIO_VADDR",	PIO_VADDR},
+	{"ZCP",	ZCP},
+	{"FZC_ZCP",	FZC_ZCP},
+	{"DMC",	DMC},
+	{"FZC_DMC",	FZC_DMC},
+	{"TXC",	TXC},
+	{"FZC_TXC",	FZC_TXC},
+	{"PIO_LDSV",	PIO_LDSV},
+	{"PIO_LDGIM",	PIO_LDGIM},
+	{"PIO_IMASK0",	PIO_IMASK0},
+	{"PIO_IMASK1",	PIO_IMASK1},
+	{"FZC_PROM",	FZC_PROM},
+	{"END",	ALL_FF_32},
+};
+
+/* ARGSUSED */
+static int
+nxge_param_dump_ptrs(p_nxge_t nxgep, caddr_t cp, uint64_t *value,
+		     uint8_t *dump_str)
+{
+	int rdc, tdc, block;
+	uint64_t base;
+	p_nxge_dma_pt_cfg_t	p_dma_cfgp;
+	p_nxge_hw_pt_cfg_t	p_cfgp;
+	p_tx_ring_t 		*tx_rings;
+	p_rx_rcr_rings_t 	rx_rcr_rings;
+	p_rx_rcr_ring_t		*rcr_rings;
+	p_rx_rbr_rings_t 	rx_rbr_rings;
+	p_rx_rbr_ring_t		*rbr_rings;
+
+	char *str_ptr = (char *)dump_str;
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "==> nxge_param_dump_ptrs"));
+
+	sprintf(str_ptr, "ptr information for Port\t %d \n",
+		nxgep->function_num);
+	str_ptr += strlen(str_ptr);
+
+	p_dma_cfgp = (p_nxge_dma_pt_cfg_t)&nxgep->pt_config;
+	p_cfgp = (p_nxge_hw_pt_cfg_t)&p_dma_cfgp->hw_config;
+
+	rx_rcr_rings = nxgep->rx_rcr_rings;
+	rcr_rings = rx_rcr_rings->rings;
+	rx_rbr_rings = nxgep->rx_rbr_rings;
+	rbr_rings = rx_rbr_rings->rings;
+	sprintf(str_ptr, "nxgep (nxge_t) $%p\n"
+		"dev_regs (dev_regs_t) $%p\n",
+		nxgep, &nxgep->dev_regs);
+	str_ptr += strlen(str_ptr);
+
+		/* do register pointers */
+	sprintf(str_ptr, "reg base (npi_reg_ptr_t) $%p\n",
+		nxgep->dev_regs.nxge_regp);
+	str_ptr += strlen(str_ptr);
+
+	sprintf(str_ptr, "\nBlock \t Offset \n");
+	str_ptr += strlen(str_ptr);
+
+	block = 0;
+	base = (uint64_t)nxgep->dev_regs.nxge_regp;
+	while (reg_block[block].offset != ALL_FF_32) {
+		sprintf(str_ptr, "%9s\t 0x%llx\n",
+			reg_block[block].name,
+			(unsigned long long)(reg_block[block].offset + base));
+		str_ptr += strlen(str_ptr);
+		block++;
+	}
+
+
+	sprintf(str_ptr, "\nRDC\t rcrp (rx_rcr_ring_t)\t "
+		"rbrp (rx_rbr_ring_t)\n");
+	str_ptr += strlen(str_ptr);
+
+	for (rdc = 0; rdc < p_cfgp->max_rdcs; rdc++) {
+		sprintf(str_ptr, " %d\t  $%p\t\t   $%p\n",
+			rdc, rcr_rings[rdc], rbr_rings[rdc]);
+		str_ptr += strlen(str_ptr);
+	}
+
+	sprintf(str_ptr, "\nTDC\t tdcp (tx_ring_t)\n");
+
+	tx_rings = nxgep->tx_rings->rings;
+	for (tdc = 0; tdc < p_cfgp->max_tdcs; tdc++) {
+		sprintf(str_ptr, " %d\t  $%p\n",
+			tdc, tx_rings[tdc]);
+		str_ptr += strlen(str_ptr);
+	}
+
+	sprintf(str_ptr, "\n\n");
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL, "<== nxge_param_dump_ptrs"));
+	return (0);
+}
+
+
+/* ARGSUSED */
+static boolean_t
+nxge_param_link_update(p_nxge_t nxgep)
+{
+
+	NXGE_DEBUG_MSG((nxgep, CFG_CTL,
+			"<== nxge_param_link_update"));
+	return (0);
+}
+
diff --git a/drivers/net/nxge/nxge_rxport.c b/drivers/net/nxge/nxge_rxport.c
new file mode 100644
index 0000000..be60423
--- /dev/null
+++ b/drivers/net/nxge/nxge_rxport.c
@@ -0,0 +1,1168 @@
+/*
+ * nxge_rxport.c	Neptune Input Port interface functions
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#include <nxge_impl.h>
+#include <nxge_ipp.h>
+#include <nxge_zcp.h>
+
+#define NXGE_IPP_FIFO_SYNC_TRY_COUNT 100
+
+static nxge_status_t nxge_ipp_eccue_valid_check(p_nxge_t nxgep,
+											boolean_t *valid);
+
+nxge_status_t
+nxge_ipp_init(p_nxge_t nxgep)
+{
+	uint8_t		portn;
+	uint32_t	config;
+	npi_handle_t	handle;
+	uint32_t	pkt_size;
+	ipp_status_t	istatus;
+	npi_status_t	rs = NPI_SUCCESS;
+	uint32_t	d0, d1, d2, d3, d4;
+	int		i;
+	uint32_t	dfifo_entries;
+
+	handle = nxgep->npi_handle;
+	portn = NXGE_GET_PORT_NUM(nxgep->function_num);
+
+	NXGE_DEBUG_MSG((nxgep, IPP_CTL, "==> nxge_ipp_init: port%d", portn));
+
+	/* Initialize ECC and parity in SRAM of DFIFO and PFIFO */
+	if (portn < 2)
+		dfifo_entries = IPP_P0_P1_DFIFO_ENTRIES;
+	else
+		dfifo_entries = IPP_P2_P3_DFIFO_ENTRIES;
+
+	for (i = 0; i < dfifo_entries; i++) {
+		if ((rs = npi_ipp_write_dfifo(handle, portn, i, 0, 0, 0, 0, 0))
+								!= NPI_SUCCESS)
+			goto fail;
+		if ((rs = npi_ipp_read_dfifo(handle, portn, i, &d0, &d1, &d2,
+						&d3, &d4)) != NPI_SUCCESS)
+			goto fail;
+	}
+
+	/* Clear PFIFO DFIFO status bits */
+
+	if ((rs = npi_ipp_get_status(handle, portn, &istatus)) != NPI_SUCCESS)
+		goto fail;
+	if ((rs = npi_ipp_get_status(handle, portn, &istatus)) != NPI_SUCCESS)
+		goto fail;
+
+	/*
+	 * Soft reset to make sure we bring the FIFO pointers back to the
+	 * original initial position.
+	 */
+	if ((rs = npi_ipp_reset(handle, portn)) != NPI_SUCCESS)
+		goto fail;
+
+	/* Clean up counters */
+
+	npi_ipp_read_regs(nxgep->npi_handle, portn);
+
+	if ((rs = npi_ipp_get_status(handle, portn, &istatus)) != NPI_SUCCESS)
+		goto fail;
+
+	/* Configure IPP port */
+
+	nxgep->ipp.iconfig = ICFG_IPP_ALL &
+		~(ICFG_IPP_PKT_DISCARD_OVFL |
+		  ICFG_IPP_BAD_TCPIP_CKSUM_OVFL |
+		  ICFG_IPP_ECC_ERR_OVFL);
+	if ((rs = npi_ipp_iconfig(handle, INIT, portn,
+							  nxgep->ipp.iconfig))
+							!= NPI_SUCCESS)
+		goto fail;
+
+
+	config = CFG_IPP | CFG_IPP_DFIFO_ECC_CORRECT | CFG_IPP_DROP_BAD_CRC |
+			CFG_IPP_TCP_UDP_CKSUM;
+	if ((rs = npi_ipp_config(handle, INIT, portn, config)) != NPI_SUCCESS)
+		goto fail;
+	nxgep->ipp.config = config;
+
+	/* Set max packet size */
+	pkt_size = IPP_MAX_PKT_SIZE;
+	if ((rs = npi_ipp_set_max_pktsize(handle, portn, IPP_MAX_PKT_SIZE)) !=
+						NPI_SUCCESS)
+		goto fail;
+	nxgep->ipp.max_pkt_size = pkt_size;
+
+	NXGE_DEBUG_MSG((nxgep, IPP_CTL, "<== nxge_ipp_init: port%d", portn));
+
+	return (NXGE_OK);
+fail:
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_ipp_init: Fail to initialize IPP Port #%d ",
+			portn));
+	return (NXGE_ERROR | rs);
+}
+
+
+nxge_status_t
+nxge_ipp_disable(p_nxge_t nxgep)
+{
+	uint8_t		portn;
+	uint32_t	config;
+	npi_handle_t	handle;
+	npi_status_t	rs = NPI_SUCCESS;
+	uint16_t wr_ptr, rd_ptr;
+	uint32_t try_count;
+	boolean_t init_values = B_FALSE;
+	handle = nxgep->npi_handle;
+	portn = NXGE_GET_PORT_NUM(nxgep->function_num);
+
+	NXGE_DEBUG_MSG((nxgep, IPP_CTL, "==> nxge_ipp_disable: port%d", portn));
+	nxge_rx_mac_disable(nxgep);
+
+	/*
+	 * Wait until ip read and write fifo pointers
+	 * are equal
+	 */
+	(void) npi_ipp_get_dfifo_rd_ptr(handle, portn, &rd_ptr);
+	(void) npi_ipp_get_dfifo_wr_ptr(handle, portn, &wr_ptr);
+
+	if ((rd_ptr == 0) && (wr_ptr == 1))
+		init_values = B_TRUE;
+
+	if (init_values == B_FALSE) {
+		try_count = NXGE_IPP_FIFO_SYNC_TRY_COUNT;
+
+		while ((try_count > 0) && (rd_ptr != wr_ptr)) {
+			(void) npi_ipp_get_dfifo_rd_ptr(handle, portn, &rd_ptr);
+			(void) npi_ipp_get_dfifo_wr_ptr(handle, portn, &wr_ptr);
+			try_count--;
+		}
+
+		if (try_count == 0) {
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+							" nxge_ipp_disable: port%d failed"
+							" rd_fifo != wr_fifo", portn));
+			goto fail;
+		}
+	}
+
+	/* disable the IPP */
+	config = nxgep->ipp.config;
+	if ((rs = npi_ipp_config(handle, DISABLE, portn, config))
+							!= NPI_SUCCESS) {
+		goto fail;
+	}
+
+/* add code to reset control FIFO */
+	/* IPP soft reset */
+	if ((rs = npi_ipp_reset(handle, portn)) != NPI_SUCCESS) {
+		goto fail;
+	}
+
+	NXGE_DEBUG_MSG((nxgep, IPP_CTL, "<== nxge_ipp_disable: port%d", portn));
+
+	return (NXGE_OK);
+fail:
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+		"nxge_ipp_disable: Fail to disable IPP Port #%d ",
+		portn));
+	return (NXGE_ERROR | rs);
+}
+
+nxge_status_t
+nxge_ipp_reset(p_nxge_t nxgep)
+{
+	uint8_t		portn;
+	uint32_t	config;
+	npi_handle_t	handle;
+	npi_status_t	rs = NPI_SUCCESS;
+	uint16_t wr_ptr, rd_ptr;
+	uint32_t try_count;
+	boolean_t init_values = B_FALSE;
+	handle = nxgep->npi_handle;
+	portn = NXGE_GET_PORT_NUM(nxgep->function_num);
+
+	NXGE_DEBUG_MSG((nxgep, IPP_CTL, "==> nxge_ipp_reset: port%d", portn));
+
+
+	/*
+	 * Wait until ip read and write fifo pointers
+	 * are equal
+	 */
+	(void) npi_ipp_get_dfifo_rd_ptr(handle, portn, &rd_ptr);
+	(void) npi_ipp_get_dfifo_wr_ptr(handle, portn, &wr_ptr);
+	if ((rd_ptr == 0) && (wr_ptr == 1))
+		init_values = B_TRUE;
+
+	if (init_values == B_FALSE) {
+		try_count = NXGE_IPP_FIFO_SYNC_TRY_COUNT;
+
+		while ((try_count > 0) && (rd_ptr != wr_ptr)) {
+			(void) npi_ipp_get_dfifo_rd_ptr(handle, portn, &rd_ptr);
+			(void) npi_ipp_get_dfifo_wr_ptr(handle, portn, &wr_ptr);
+			try_count--;
+		}
+
+		if (try_count == 0) {
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+							" nxge_ipp_reset: port%d failed"
+							" rd_fifo != wr_fifo", portn));
+			goto fail;
+		}
+	}
+
+	/* disable the IPP */
+	config = nxgep->ipp.config;
+	if ((rs = npi_ipp_config(handle, DISABLE, portn, config))
+							!= NPI_SUCCESS) {
+		goto fail;
+	}
+
+	/* IPP soft reset */
+	if ((rs = npi_ipp_reset(handle, portn)) != NPI_SUCCESS) {
+		goto fail;
+	}
+
+	/* to reset control FIFO */
+	if ((rs = npi_zcp_rest_cfifo_port(handle, portn)) != NPI_SUCCESS) {
+		goto fail;
+	}
+
+	/*
+	 * Making sure that error source is cleared if this is an
+	 * injected error.
+	 */
+	IPP_REG_WR(handle, portn, IPP_ECC_CTRL_REG, 0);
+
+	NXGE_DEBUG_MSG((nxgep, IPP_CTL, "<== nxge_ipp_reset: port%d", portn));
+
+	return (NXGE_OK);
+fail:
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_ipp_init: Fail to Reset IPP Port #%d ",
+			portn));
+	return (NXGE_ERROR | rs);
+}
+
+
+
+nxge_status_t
+nxge_ipp_enable(p_nxge_t nxgep)
+{
+	uint8_t		portn;
+	uint32_t	config;
+	npi_handle_t	handle;
+	uint32_t	pkt_size;
+	npi_status_t	rs = NPI_SUCCESS;
+
+	handle = nxgep->npi_handle;
+	portn = NXGE_GET_PORT_NUM(nxgep->function_num);
+
+	NXGE_DEBUG_MSG((nxgep, IPP_CTL, "==> nxge_ipp_enable: port%d", portn));
+
+	config = CFG_IPP | CFG_IPP_DFIFO_ECC_CORRECT | CFG_IPP_DROP_BAD_CRC |
+			CFG_IPP_TCP_UDP_CKSUM;
+	if ((rs = npi_ipp_config(handle, INIT, portn, config)) != NPI_SUCCESS)
+		goto fail;
+	nxgep->ipp.config = config;
+
+	/* Set max packet size */
+	pkt_size = IPP_MAX_PKT_SIZE;
+	if ((rs = npi_ipp_set_max_pktsize(handle, portn, IPP_MAX_PKT_SIZE)) !=
+						NPI_SUCCESS)
+		goto fail;
+	nxgep->ipp.max_pkt_size = pkt_size;
+
+	NXGE_DEBUG_MSG((nxgep, IPP_CTL, "<== nxge_ipp_enable: port%d", portn));
+
+	return (NXGE_OK);
+fail:
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_ipp_init: Fail to Enable IPP Port #%d ",
+			portn));
+	return (NXGE_ERROR | rs);
+}
+
+
+nxge_status_t
+nxge_ipp_handle_sys_errors(p_nxge_t nxgep)
+{
+	npi_handle_t		handle;
+	npi_status_t		rs = NPI_SUCCESS;
+	p_nxge_ipp_stats_t	statsp;
+	ipp_status_t		istatus;
+	uint8_t			portn;
+	p_ipp_errlog_t		errlogp;
+	nxge_status_t status;
+	uint64_t val;
+
+	boolean_t		rxport_fatal = B_FALSE;
+
+	handle = nxgep->npi_handle;
+	statsp = (p_nxge_ipp_stats_t)&nxgep->statsp->ipp_stats;
+	portn = nxgep->mac.portnum;
+
+	errlogp = (p_ipp_errlog_t)&statsp->errlog;
+
+	if ((rs = npi_ipp_get_status(handle, portn, &istatus)) != NPI_SUCCESS)
+		return (NXGE_ERROR | rs);
+
+	if (istatus.value == 0)
+		/*
+		 * The error is not initiated from this port, so just exit.
+		 */
+		return (NXGE_OK);
+
+	if (istatus.bits.w0.dfifo_missed_sop) {
+		statsp->sop_miss++;
+		if ((rs = npi_ipp_get_dfifo_eopm_rdptr(handle, portn,
+					&errlogp->dfifo_rd_ptr)) != NPI_SUCCESS)
+			return (NXGE_ERROR | rs);
+		if ((rs = npi_ipp_get_state_mach(handle, portn,
+					&errlogp->state_mach)) != NPI_SUCCESS)
+			return (NXGE_ERROR | rs);
+
+		if (statsp->sop_miss < IPP_MAX_ERR_SHOW)
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"nxge_ipp_err_evnts: fatal error: sop_miss "));
+		rxport_fatal = B_TRUE;
+	}
+	if (istatus.bits.w0.dfifo_missed_eop) {
+		statsp->eop_miss++;
+		if ((rs = npi_ipp_get_dfifo_eopm_rdptr(handle, portn,
+					&errlogp->dfifo_rd_ptr)) != NPI_SUCCESS)
+			return (NXGE_ERROR | rs);
+		if ((rs = npi_ipp_get_state_mach(handle, portn,
+					&errlogp->state_mach)) != NPI_SUCCESS)
+			return (NXGE_ERROR | rs);
+
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"nxge_ipp_err_evnts: fatal error: eop_miss "));
+		rxport_fatal = B_TRUE;
+	}
+	if (istatus.bits.w0.dfifo_uncorr_ecc_err) {
+		boolean_t ue_ecc_valid;
+
+		if ((status = nxge_ipp_eccue_valid_check(nxgep, &ue_ecc_valid))
+								!= NXGE_OK)
+
+			return (status);
+
+		if (ue_ecc_valid) {
+			statsp->dfifo_ue++;
+			if ((rs = npi_ipp_get_ecc_syndrome(handle, portn,
+					&errlogp->ecc_syndrome)) != NPI_SUCCESS)
+				return (NXGE_ERROR | rs);
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"nxge_ipp_err_evnts: fatal error: dfifo_ue\n"));
+			rxport_fatal = B_TRUE;
+		} else {
+			NXGE_DEBUG_MSG((nxgep, IPP_CTL, "Fake UE detected\n"));
+		}
+	}
+#ifdef IPP_ECC_CORR_ERR
+	if (istatus.bits.w0.dfifo_corr_ecc_err) {
+		/*
+		 * Do nothing here. ECC errors are collected from the
+		 * ECC counter.
+		 */
+		;
+	}
+#endif
+	if (istatus.bits.w0.pre_fifo_perr) {
+		statsp->pfifo_perr++;
+		if (statsp->pfifo_perr < IPP_MAX_ERR_SHOW)
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_ipp_err_evnts: fatal error: pre_pifo_perr "));
+		rxport_fatal = B_TRUE;
+	}
+	if (istatus.bits.w0.ecc_err_cnt_ovfl) {
+		IPP_REG_RD(handle, portn,
+				   IPP_ECC_ERR_COUNTER_REG, &val);
+		statsp->ecc_err_cnt += val;
+		if (statsp->ecc_err_cnt < (IPP_MAX_ERR_SHOW * IPP_ECC_CNT_MASK))
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"WARNING: nxge_ipp_err_evnts: ecc_err_max "));
+	}
+	if (istatus.bits.w0.pre_fifo_overrun) {
+		statsp->pfifo_over++;
+
+		if (statsp->pfifo_over < IPP_MAX_ERR_SHOW)
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_ipp_err_evnts: fatal error: pfifo_over "));
+		rxport_fatal = B_TRUE;
+	}
+	if (istatus.bits.w0.pre_fifo_underrun) {
+		statsp->pfifo_und++;
+
+		if (statsp->pfifo_und < IPP_MAX_ERR_SHOW)
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_ipp_err_evnts: fatal error: pfifo_und "));
+		rxport_fatal = B_TRUE;
+	}
+	if (istatus.bits.w0.bad_cksum_cnt_ovfl) {
+		IPP_REG_RD(handle, portn,
+				   IPP_TCP_CKSUM_ERR_CNT_REG, &val);
+		statsp->bad_cs_cnt += val;
+		if (statsp->bad_cs_cnt <
+				(IPP_MAX_ERR_SHOW * IPP_BAD_CS_CNT_MASK))
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"WARNING: nxge_ipp_err_evnts: bad_cs_max "));
+	}
+	if (istatus.bits.w0.pkt_discard_cnt_ovfl) {
+		IPP_REG_RD(handle, portn,
+				   IPP_DISCARD_PKT_CNT_REG, &val);
+		statsp->pkt_dis_cnt += val;
+		if (statsp->pkt_dis_cnt <
+				(IPP_MAX_ERR_SHOW * IPP_PKT_DIS_CNT_MASK))
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"WARNING: nxge_ipp_err_evnts: pkt_dis_max "));
+	}
+	/*
+	 * Making sure that error source is cleared if this is an
+	 * injected error.
+	 */
+	IPP_REG_WR(handle, portn, IPP_ECC_CTRL_REG, 0);
+
+	if (rxport_fatal) {
+		NXGE_DEBUG_MSG((nxgep, IPP_CTL,
+			    " nxge_ipp_handle_sys_errors:"
+			    " fatal Error on  Port #%d ",
+			    portn));
+		if (nxge_ipp_fatal_err_recover(nxgep) == NXGE_OK) {
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"nxge_ipp_err_evnts: IPP error recovered"));
+		}
+	}
+
+#if 0
+	MUTEX_ENTER(nxgep->syserr_lock);
+	*(nxgep->syserr) &= ~NXGE_SYSERR_IPP;
+	MUTEX_EXIT(nxgep->syserr_lock);
+#endif
+
+	return (NXGE_OK);
+}
+
+void
+nxge_ipp_inject_err(p_nxge_t nxgep, uint32_t err_id)
+{
+	ipp_status_t	ipps;
+	ipp_ecc_ctrl_t	ecc_ctrl;
+	uint8_t		portn = nxgep->mac.portnum;
+
+	switch (err_id) {
+	case NXGE_FM_EREPORT_IPP_DFIFO_UE:
+		ecc_ctrl.value = 0;
+		ecc_ctrl.bits.w0.cor_dbl = 1;
+		ecc_ctrl.bits.w0.cor_1 = 1;
+		ecc_ctrl.bits.w0.cor_lst = 1;
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				    "Write 0x%llx to IPP_ECC_CTRL_REG ",
+				    ecc_ctrl.value));
+		IPP_REG_WR(nxgep->npi_handle, portn, IPP_ECC_CTRL_REG,
+				ecc_ctrl.value);
+		break;
+	case NXGE_FM_EREPORT_IPP_DFIFO_CE:
+		ecc_ctrl.value = 0;
+		ecc_ctrl.bits.w0.cor_sng = 1;
+		ecc_ctrl.bits.w0.cor_1 = 1;
+		ecc_ctrl.bits.w0.cor_snd = 1;
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				    "Write 0x%llx to IPP_ECC_CTRL_REG ",
+				    ecc_ctrl.value));
+
+		IPP_REG_WR(nxgep->npi_handle, portn, IPP_ECC_CTRL_REG,
+				ecc_ctrl.value);
+		break;
+	case NXGE_FM_EREPORT_IPP_EOP_MISS:
+	case NXGE_FM_EREPORT_IPP_SOP_MISS:
+	case NXGE_FM_EREPORT_IPP_PFIFO_PERR:
+	case NXGE_FM_EREPORT_IPP_ECC_ERR_MAX:
+	case NXGE_FM_EREPORT_IPP_PFIFO_OVER:
+	case NXGE_FM_EREPORT_IPP_PFIFO_UND:
+	case NXGE_FM_EREPORT_IPP_BAD_CS_MX:
+	case NXGE_FM_EREPORT_IPP_PKT_DIS_MX:
+	case NXGE_FM_EREPORT_IPP_RESET_FAIL:
+		IPP_REG_RD(nxgep->npi_handle, portn, IPP_INT_STATUS_REG,
+			&ipps.value);
+		if (err_id == NXGE_FM_EREPORT_IPP_EOP_MISS)
+			ipps.bits.w0.dfifo_missed_eop = 1;
+		else if (err_id == NXGE_FM_EREPORT_IPP_SOP_MISS)
+			ipps.bits.w0.dfifo_missed_sop = 1;
+		else if (err_id == NXGE_FM_EREPORT_IPP_DFIFO_UE)
+			ipps.bits.w0.dfifo_uncorr_ecc_err = 1;
+		else if (err_id == NXGE_FM_EREPORT_IPP_DFIFO_CE)
+			ipps.bits.w0.dfifo_corr_ecc_err = 1;
+		else if (err_id == NXGE_FM_EREPORT_IPP_PFIFO_PERR)
+			ipps.bits.w0.pre_fifo_perr = 1;
+		else if (err_id == NXGE_FM_EREPORT_IPP_ECC_ERR_MAX)
+			ipps.bits.w0.ecc_err_cnt_ovfl = 1;
+		else if (err_id == NXGE_FM_EREPORT_IPP_PFIFO_OVER)
+			ipps.bits.w0.pre_fifo_overrun = 1;
+		else if (err_id == NXGE_FM_EREPORT_IPP_PFIFO_UND)
+			ipps.bits.w0.pre_fifo_underrun = 1;
+		else if (err_id == NXGE_FM_EREPORT_IPP_BAD_CS_MX)
+			ipps.bits.w0.bad_cksum_cnt_ovfl = 1;
+		else if (err_id == NXGE_FM_EREPORT_IPP_PKT_DIS_MX)
+			ipps.bits.w0.pkt_discard_cnt_ovfl = 1;
+		printk(KERN_INFO "Write 0x%llx to IPP_INT_STATUS_REG ",
+				ipps.value);
+		IPP_REG_WR(nxgep->npi_handle, portn, IPP_INT_STATUS_REG,
+				ipps.value);
+		break;
+	}
+}
+
+nxge_status_t
+nxge_ipp_fatal_err_recover(p_nxge_t nxgep)
+{
+	npi_handle_t		handle;
+	npi_status_t		rs = NPI_SUCCESS;
+	nxge_status_t		status = NXGE_OK;
+	uint8_t			portn;
+	uint16_t		wr_ptr;
+	uint16_t		rd_ptr;
+	uint32_t		try_count;
+	uint32_t		dfifo_entries;
+	ipp_status_t		istatus;
+	uint32_t		d0, d1, d2, d3, d4;
+	int			i;
+
+	NXGE_DEBUG_MSG((nxgep, RX_CTL, "<== nxge_ipp_fatal_err_recover"));
+
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"Recovering from RxPort error..."));
+
+	handle = nxgep->npi_handle;
+	portn = nxgep->mac.portnum;
+
+	/*
+	 * Making sure that error source is cleared if this is an
+	 * injected error.
+	 */
+	IPP_REG_WR(handle, portn, IPP_ECC_CTRL_REG, 0);
+
+	/* Disable RxMAC */
+
+	if (nxge_rx_mac_disable(nxgep) != NXGE_OK)
+		goto fail;
+
+	/* When recovering from IPP, RxDMA channel resets are not necessary */
+	/* Reset ZCP CFIFO */
+	NXGE_DEBUG_MSG((nxgep, IPP_CTL, "port%d Reset ZCP CFIFO...", portn));
+	if ((rs = npi_zcp_rest_cfifo_port(handle, portn)) != NPI_SUCCESS)
+		goto fail;
+
+	/*
+	 * Wait until ip read and write fifo pointers
+	 * are equal
+	 */
+	(void) npi_ipp_get_dfifo_rd_ptr(handle, portn, &rd_ptr);
+	(void) npi_ipp_get_dfifo_wr_ptr(handle, portn, &wr_ptr);
+	try_count = 512;
+
+	while ((try_count > 0) && (rd_ptr != wr_ptr)) {
+		(void) npi_ipp_get_dfifo_rd_ptr(handle, portn, &rd_ptr);
+		(void) npi_ipp_get_dfifo_wr_ptr(handle, portn, &wr_ptr);
+		try_count--;
+	}
+
+	if (try_count == 0) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				    " nxge_ipp_reset: port%d IPP stalled..."
+				    " rd_fifo_ptr = 0x%x wr_fifo_ptr = 0x%x",
+				    portn, rd_ptr, wr_ptr));
+		/*
+		 * This means the fatal error occurred on the first line
+		 * of the fifo. In this case, just reset the IPP without
+		 * draining the PFIFO.
+		 */
+	}
+
+
+	if (portn < 2)
+		dfifo_entries = IPP_P0_P1_DFIFO_ENTRIES;
+	else
+		dfifo_entries = IPP_P2_P3_DFIFO_ENTRIES;
+
+
+	/* Clean up DFIFO SRAM entries */
+	for (i = 0; i < dfifo_entries; i++) {
+		if ((rs = npi_ipp_write_dfifo(handle, portn, i, 0, 0, 0, 0, 0))
+								!= NPI_SUCCESS)
+			goto fail;
+		if ((rs = npi_ipp_read_dfifo(handle, portn, i, &d0, &d1, &d2,
+						&d3, &d4)) != NPI_SUCCESS)
+			goto fail;
+	}
+
+	/* Clear PFIFO DFIFO status bits */
+
+	if ((rs = npi_ipp_get_status(handle, portn, &istatus)) != NPI_SUCCESS)
+		goto fail;
+	if ((rs = npi_ipp_get_status(handle, portn, &istatus)) != NPI_SUCCESS)
+		goto fail;
+
+	/* Reset IPP */
+	NXGE_DEBUG_MSG((nxgep, IPP_CTL, "port%d Reset IPP...", portn));
+	if ((rs = npi_ipp_reset(handle, portn)) != NPI_SUCCESS)
+		goto fail;
+
+	NXGE_DEBUG_MSG((nxgep, IPP_CTL, "port%d Reset RxMAC...", portn));
+	if (nxge_rx_mac_reset(nxgep) != NXGE_OK)
+		goto fail;
+
+	NXGE_DEBUG_MSG((nxgep, IPP_CTL, "port%d Initialize RxMAC...", portn));
+
+	if ((status = nxge_rx_mac_init(nxgep)) != NXGE_OK)
+		goto fail;
+
+	NXGE_DEBUG_MSG((nxgep, IPP_CTL, "port%d Enable RxMAC...", portn));
+
+	if (nxge_rx_mac_enable(nxgep) != NXGE_OK)
+		goto fail;
+
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"Recovery Sucessful, RxPort Restored"));
+	NXGE_DEBUG_MSG((nxgep, RX_CTL, "==> nxge_ipp_fatal_err_recover"));
+
+	return (NXGE_OK);
+fail:
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL, "Recovery failed"));
+	return (status | rs);
+}
+
+
+nxge_status_t
+nxge_zcp_init(p_nxge_t nxgep)
+{
+	uint8_t		portn;
+	npi_handle_t	handle;
+	zcp_iconfig_t	istatus;
+	npi_status_t	rs = NPI_SUCCESS;
+	int		i;
+	zcp_ram_unit_t	w_data;
+	zcp_ram_unit_t	r_data;
+	uint32_t	cfifo_depth;
+
+	handle = nxgep->npi_handle;
+	portn = NXGE_GET_PORT_NUM(nxgep->function_num);
+
+	if (portn < 2)
+		cfifo_depth = ZCP_P0_P1_CFIFO_DEPTH;
+	else
+		cfifo_depth = ZCP_P2_P3_CFIFO_DEPTH;
+
+	/* Clean up CFIFO */
+
+	w_data.w0 = 0;
+	w_data.w1 = 0;
+	w_data.w2 = 0;
+	w_data.w3 = 0;
+	w_data.w4 = 0;
+	for (i = 0; i < cfifo_depth; i++) {
+		if (npi_zcp_tt_cfifo_entry(handle, OP_SET, portn, i, &w_data)
+							!= NPI_SUCCESS)
+			goto fail;
+		if (npi_zcp_tt_cfifo_entry(handle, OP_GET, portn, i, &r_data)
+							!= NPI_SUCCESS)
+			goto fail;
+	}
+
+	if (npi_zcp_rest_cfifo_port(handle, portn) != NPI_SUCCESS)
+			goto fail;
+	/*
+	 * Making sure that error source is cleared if this is an
+	 * injected error.
+	 */
+	switch (portn) {
+	case 0:
+		NXGE_REG_WR64(handle, ZCP_CFIFO_ECC_PORT0_REG, 0);
+		break;
+	case 1:
+		NXGE_REG_WR64(handle, ZCP_CFIFO_ECC_PORT1_REG, 0);
+		break;
+	case 2:
+		NXGE_REG_WR64(handle, ZCP_CFIFO_ECC_PORT2_REG, 0);
+		break;
+	case 3:
+		NXGE_REG_WR64(handle, ZCP_CFIFO_ECC_PORT3_REG, 0);
+		break;
+	}
+
+	if ((rs = npi_zcp_clear_istatus(handle)) != NPI_SUCCESS)
+		return (NXGE_ERROR | rs);
+
+
+	if ((rs = npi_zcp_get_istatus(handle, &istatus)) != NPI_SUCCESS)
+		return (NXGE_ERROR | rs);
+
+
+	if (istatus != 0) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+						"ZCP istatus cannot be cleared %llx",
+						istatus));
+	}
+
+	if ((rs = npi_zcp_iconfig(handle, INIT, ICFG_ZCP_ALL)) != NPI_SUCCESS)
+		goto fail;
+
+	NXGE_DEBUG_MSG((nxgep, IPP_CTL, "==> nxge_ipp_init: port%d", portn));
+
+	return (NXGE_OK);
+
+fail:
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_zcp_init: Fail to initialize ZCP Port #%d ",
+			portn));
+	return (NXGE_ERROR | rs);
+}
+
+nxge_status_t
+nxge_zcp_handle_sys_errors(p_nxge_t nxgep)
+{
+	npi_handle_t		handle;
+	npi_status_t		rs = NPI_SUCCESS;
+	p_nxge_zcp_stats_t	statsp;
+	uint8_t			portn;
+	zcp_iconfig_t		istatus;
+	boolean_t		rxport_fatal = B_FALSE;
+	boolean_t this_port_err = B_FALSE;
+	nxge_status_t status;
+
+
+	handle = nxgep->npi_handle;
+	statsp = (p_nxge_zcp_stats_t)&nxgep->statsp->zcp_stats;
+	portn = nxgep->mac.portnum;
+
+	if ((rs = npi_zcp_get_istatus(handle, &istatus)) != NPI_SUCCESS)
+		return (NXGE_ERROR | rs);
+
+
+	if (istatus == 0) {
+			/* Error is from somebody else: Just return */
+
+		return (NXGE_OK);
+	}
+
+
+	if (istatus & ICFG_ZCP_RRFIFO_UNDERRUN) {
+		this_port_err = B_TRUE;
+		statsp->rrfifo_underrun++;
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_zcp_err_evnts: rrfifo_underrun"));
+	}
+	if (istatus & ICFG_ZCP_RRFIFO_OVERRUN) {
+		this_port_err = B_TRUE;
+		statsp->rrfifo_overrun++;
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_zcp_err_evnts: buf_rrfifo_overrun"));
+	}
+	if (istatus & ICFG_ZCP_RSPFIFO_UNCORR_ERR) {
+		this_port_err = B_TRUE;
+		statsp->rspfifo_uncorr_err++;
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_zcp_err_evnts: rspfifo_uncorr_err"));
+	}
+	if (istatus & ICFG_ZCP_BUFFER_OVERFLOW) {
+		this_port_err = B_TRUE;
+		statsp->buffer_overflow++;
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_zcp_err_evnts: buffer_overflow"));
+		rxport_fatal = B_TRUE;
+	}
+	if (istatus & ICFG_ZCP_STAT_TBL_PERR) {
+		this_port_err = B_TRUE;
+		statsp->stat_tbl_perr++;
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_zcp_err_evnts: stat_tbl_perr"));
+	}
+	if (istatus & ICFG_ZCP_DYN_TBL_PERR) {
+		this_port_err = B_TRUE;
+		statsp->dyn_tbl_perr++;
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_zcp_err_evnts: dyn_tbl_perr"));
+	}
+	if (istatus & ICFG_ZCP_BUF_TBL_PERR) {
+		this_port_err = B_TRUE;
+		statsp->buf_tbl_perr++;
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_zcp_err_evnts: buf_tbl_perr"));
+	}
+	if (istatus & ICFG_ZCP_TT_PROGRAM_ERR) {
+		this_port_err = B_TRUE;
+		statsp->tt_program_err++;
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_zcp_err_evnts: tt_program_err"));
+	}
+	if (istatus & ICFG_ZCP_RSP_TT_INDEX_ERR) {
+		this_port_err = B_TRUE;
+		statsp->rsp_tt_index_err++;
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_zcp_err_evnts: rsp_tt_index_err"));
+	}
+	if (istatus & ICFG_ZCP_SLV_TT_INDEX_ERR) {
+		this_port_err = B_TRUE;
+		statsp->slv_tt_index_err++;
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_zcp_err_evnts: slv_tt_index_err"));
+	}
+	if (istatus & ICFG_ZCP_TT_INDEX_ERR) {
+		this_port_err = B_TRUE;
+		statsp->zcp_tt_index_err++;
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_zcp_err_evnts: tt_index_err"));
+	}
+	if (((portn == 0) && (istatus & ICFG_ZCP_CFIFO_ECC0)) ||
+		((portn == 1) && (istatus & ICFG_ZCP_CFIFO_ECC1)) ||
+		((portn == 2) && (istatus & ICFG_ZCP_CFIFO_ECC2)) ||
+		((portn == 3) && (istatus & ICFG_ZCP_CFIFO_ECC3))) {
+		boolean_t ue_ecc_valid;
+
+		if ((status = nxge_ipp_eccue_valid_check(nxgep, &ue_ecc_valid))
+								!= NXGE_OK)
+
+			return (status);
+		if (ue_ecc_valid) {
+			statsp->cfifo_ecc++;
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_zcp_err_evnts: port%d buf_cfifo_ecc",
+					portn));
+			rxport_fatal = B_TRUE;
+		}
+	}
+
+	(void) npi_zcp_clear_istatus(handle);
+
+	if ((rs = npi_zcp_get_istatus(handle, &istatus)) != NPI_SUCCESS)
+		return (NXGE_ERROR | rs);
+
+	if (rxport_fatal) {
+		NXGE_DEBUG_MSG((nxgep, IPP_CTL,
+			    " nxge_zcp_handle_sys_errors:"
+			    " fatal Error on  Port #%d ",
+			    portn));
+		if (nxge_zcp_fatal_err_recover(nxgep) == NXGE_OK) {
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				    " nxge_zcp_handle_sys_errors:"
+				    " fatal Error on  Port #%d: Recovered ",
+				    portn));
+		}
+	}
+
+	if ((rs = npi_zcp_get_istatus(handle, &istatus)) != NPI_SUCCESS)
+		return (NXGE_ERROR | rs);
+#if 0
+	if (this_port_err == B_TRUE) {
+		MUTEX_ENTER(nxgep->syserr_lock);
+		*(nxgep->syserr) &= ~NXGE_SYSERR_ZCP;
+		MUTEX_EXIT(nxgep->syserr_lock);
+	}
+#endif
+
+	return (NXGE_OK);
+}
+
+void
+nxge_zcp_inject_err(p_nxge_t nxgep, uint32_t err_id)
+{
+	zcp_int_stat_reg_t	zcps;
+	uint8_t			portn = nxgep->mac.portnum;
+	zcp_ecc_ctrl_t		ecc_ctrl;
+
+	switch (err_id) {
+	case NXGE_FM_EREPORT_ZCP_CFIFO_ECC:
+		ecc_ctrl.value = 0;
+		ecc_ctrl.bits.w0.cor_dbl = 1;
+		ecc_ctrl.bits.w0.cor_lst = 1;
+		ecc_ctrl.bits.w0.cor_all = 0;
+		switch (portn) {
+		case 0:
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			    "Write 0x%llx to port%d ZCP_CFIFO_ECC_PORT ",
+			    ecc_ctrl.value, portn));
+			NXGE_REG_WR64(nxgep->npi_handle,
+					ZCP_CFIFO_ECC_PORT0_REG,
+					ecc_ctrl.value);
+			break;
+		case 1:
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				    "Write 0x%llx to port%d ZCP_CFIFO_ECC_PORT",
+				    ecc_ctrl.value, portn));
+			NXGE_REG_WR64(nxgep->npi_handle,
+					ZCP_CFIFO_ECC_PORT1_REG,
+					ecc_ctrl.value);
+			break;
+		case 2:
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				    "Write 0x%llx to port%d ZCP_CFIFO_ECC_PORT",
+				ecc_ctrl.value, portn));
+			NXGE_REG_WR64(nxgep->npi_handle,
+					ZCP_CFIFO_ECC_PORT2_REG,
+					ecc_ctrl.value);
+			break;
+		case 3:
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				    "Write 0x%llx to port%d"
+				    "  ZCP_CFIFO_ECC_PORT ",
+				    ecc_ctrl.value, portn));
+			NXGE_REG_WR64(nxgep->npi_handle,
+					ZCP_CFIFO_ECC_PORT3_REG,
+					ecc_ctrl.value);
+			break;
+		}
+		break;
+	case NXGE_FM_EREPORT_ZCP_RRFIFO_UNDERRUN:
+	case NXGE_FM_EREPORT_ZCP_RSPFIFO_UNCORR_ERR:
+	case NXGE_FM_EREPORT_ZCP_STAT_TBL_PERR:
+	case NXGE_FM_EREPORT_ZCP_DYN_TBL_PERR:
+	case NXGE_FM_EREPORT_ZCP_BUF_TBL_PERR:
+	case NXGE_FM_EREPORT_ZCP_RRFIFO_OVERRUN:
+	case NXGE_FM_EREPORT_ZCP_BUFFER_OVERFLOW:
+	case NXGE_FM_EREPORT_ZCP_TT_PROGRAM_ERR:
+	case NXGE_FM_EREPORT_ZCP_RSP_TT_INDEX_ERR:
+	case NXGE_FM_EREPORT_ZCP_SLV_TT_INDEX_ERR:
+	case NXGE_FM_EREPORT_ZCP_TT_INDEX_ERR:
+		NXGE_REG_RD64(nxgep->npi_handle, ZCP_INT_STAT_TEST_REG,
+					&zcps.value);
+		if (err_id == NXGE_FM_EREPORT_ZCP_RRFIFO_UNDERRUN)
+			zcps.bits.ldw.rrfifo_urun = 1;
+		if (err_id == NXGE_FM_EREPORT_ZCP_RSPFIFO_UNCORR_ERR)
+			zcps.bits.ldw.rspfifo_uc_err = 1;
+		if (err_id == NXGE_FM_EREPORT_ZCP_STAT_TBL_PERR)
+			zcps.bits.ldw.stat_tbl_perr = 1;
+		if (err_id == NXGE_FM_EREPORT_ZCP_DYN_TBL_PERR)
+			zcps.bits.ldw.dyn_tbl_perr = 1;
+		if (err_id == NXGE_FM_EREPORT_ZCP_BUF_TBL_PERR)
+			zcps.bits.ldw.buf_tbl_perr = 1;
+		if (err_id == NXGE_FM_EREPORT_ZCP_CFIFO_ECC) {
+			switch (portn) {
+			case 0:
+				zcps.bits.ldw.cfifo_ecc0 = 1;
+				break;
+			case 1:
+				zcps.bits.ldw.cfifo_ecc1 = 1;
+				break;
+			case 2:
+				zcps.bits.ldw.cfifo_ecc2 = 1;
+				break;
+			case 3:
+				zcps.bits.ldw.cfifo_ecc3 = 1;
+				break;
+			}
+			default:
+				;
+		}
+		if (err_id == NXGE_FM_EREPORT_ZCP_RRFIFO_OVERRUN)
+			zcps.bits.ldw.rrfifo_orun = 1;
+		if (err_id == NXGE_FM_EREPORT_ZCP_BUFFER_OVERFLOW)
+			zcps.bits.ldw.buf_overflow = 1;
+		if (err_id == NXGE_FM_EREPORT_ZCP_TT_PROGRAM_ERR)
+			zcps.bits.ldw.tt_tbl_perr = 1;
+		if (err_id == NXGE_FM_EREPORT_ZCP_RSP_TT_INDEX_ERR)
+			zcps.bits.ldw.rsp_tt_index_err = 1;
+		if (err_id == NXGE_FM_EREPORT_ZCP_SLV_TT_INDEX_ERR)
+			zcps.bits.ldw.slv_tt_index_err = 1;
+		if (err_id == NXGE_FM_EREPORT_ZCP_TT_INDEX_ERR)
+			zcps.bits.ldw.zcp_tt_index_err = 1;
+		NXGE_DEBUG_MSG((nxgep, RX_CTL,
+			    "Write 0x%llx to ZCP_INT_STAT_TEST_REG ",
+			    zcps.value));
+		NXGE_REG_WR64(nxgep->npi_handle, ZCP_INT_STAT_TEST_REG,
+					zcps.value);
+		break;
+	}
+}
+
+nxge_status_t
+nxge_zcp_fatal_err_recover(p_nxge_t nxgep)
+{
+	npi_handle_t		handle;
+	npi_status_t		rs = NPI_SUCCESS;
+	nxge_status_t		status = NXGE_OK;
+	uint8_t			portn;
+	zcp_ram_unit_t		w_data;
+	zcp_ram_unit_t		r_data;
+	uint32_t		cfifo_depth;
+	int			i;
+
+	NXGE_DEBUG_MSG((nxgep, RX_CTL, "==> nxge_zcp_fatal_err_recover"));
+
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"Recovering from RxPort error..."));
+
+	handle = nxgep->npi_handle;
+	portn = nxgep->mac.portnum;
+
+	/* Disable RxMAC */
+	if (nxge_rx_mac_disable(nxgep) != NXGE_OK)
+		goto fail;
+
+	/* Make sure source is clear if this is an injected error */
+	switch (portn) {
+	case 0:
+		NXGE_REG_WR64(handle, ZCP_CFIFO_ECC_PORT0_REG, 0);
+		break;
+	case 1:
+		NXGE_REG_WR64(handle, ZCP_CFIFO_ECC_PORT1_REG, 0);
+		break;
+	case 2:
+		NXGE_REG_WR64(handle, ZCP_CFIFO_ECC_PORT2_REG, 0);
+		break;
+	case 3:
+		NXGE_REG_WR64(handle, ZCP_CFIFO_ECC_PORT3_REG, 0);
+		break;
+	}
+
+	/* Clear up CFIFO */
+	cfifo_depth = ZCP_P0_P1_CFIFO_DEPTH;
+	if (portn > 1)
+		cfifo_depth = ZCP_P2_P3_CFIFO_DEPTH;
+	w_data.w0 = 0;
+	w_data.w1 = 0;
+	w_data.w2 = 0;
+	w_data.w3 = 0;
+	w_data.w4 = 0;
+	for (i = 0; i < cfifo_depth; i++) {
+		if (npi_zcp_tt_cfifo_entry(handle, OP_SET, portn, i, &w_data)
+							!= NPI_SUCCESS)
+			goto fail;
+		if (npi_zcp_tt_cfifo_entry(handle, OP_GET, portn, i, &r_data)
+							!= NPI_SUCCESS)
+			goto fail;
+	}
+
+	/* When recovering from ZCP, RxDMA channel resets are not necessary */
+	/* Reset ZCP CFIFO */
+	NXGE_DEBUG_MSG((nxgep, RX_CTL, "port%d Reset ZCP CFIFO...", portn));
+	if ((rs = npi_zcp_rest_cfifo_port(handle, portn)) != NPI_SUCCESS)
+		goto fail;
+
+	/* Reset IPP */
+	NXGE_DEBUG_MSG((nxgep, RX_CTL, "port%d Reset IPP...", portn));
+	if ((rs = npi_ipp_reset(handle, portn)) != NPI_SUCCESS)
+		goto fail;
+
+	NXGE_DEBUG_MSG((nxgep, RX_CTL, "port%d Reset RxMAC...", portn));
+	if (nxge_rx_mac_reset(nxgep) != NXGE_OK)
+		goto fail;
+
+	NXGE_DEBUG_MSG((nxgep, RX_CTL, "port%d Initialize RxMAC...", portn));
+
+	if ((status = nxge_rx_mac_init(nxgep)) != NXGE_OK)
+		goto fail;
+
+	NXGE_DEBUG_MSG((nxgep, RX_CTL, "port%d Enable RxMAC...", portn));
+
+	if (nxge_rx_mac_enable(nxgep) != NXGE_OK)
+		goto fail;
+
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"Recovery Sucessful, RxPort Restored"));
+	NXGE_DEBUG_MSG((nxgep, RX_CTL, "==> nxge_zcp_fatal_err_recover"));
+
+	return (NXGE_OK);
+fail:
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL, "Recovery failed"));
+	return (status | rs);
+}
+
+static nxge_status_t
+nxge_ipp_eccue_valid_check(p_nxge_t nxgep, boolean_t *valid)
+{
+	npi_handle_t handle;
+	npi_status_t rs = NPI_SUCCESS;
+	uint8_t portn;
+	uint16_t rd_ptr;
+	uint16_t wr_ptr;
+	uint16_t curr_rd_ptr;
+	uint16_t curr_wr_ptr;
+	uint32_t stall_cnt;
+	uint32_t d0, d1, d2, d3, d4;
+	uint16_t dfifo_entries;
+	handle = nxgep->npi_handle;
+	portn = nxgep->mac.portnum;
+	*valid = B_TRUE;
+
+	if (portn < 2)
+		dfifo_entries = IPP_P0_P1_DFIFO_ENTRIES;
+	else
+		dfifo_entries = IPP_P2_P3_DFIFO_ENTRIES;
+
+	if ((rs = npi_ipp_get_dfifo_rd_ptr(handle, portn, &rd_ptr))
+							!= NPI_SUCCESS)
+		goto fail;
+	if ((rs = npi_ipp_get_dfifo_wr_ptr(handle, portn, &wr_ptr))
+							!= NPI_SUCCESS)
+		goto fail;
+
+
+	if ((rd_ptr == wr_ptr) ||
+		((rd_ptr == 0) && (wr_ptr == 1))) {
+		*valid = B_FALSE;	/* IPP not stuck */
+	} else {
+		stall_cnt = 0;
+		while (stall_cnt < 16) {
+			if ((rs = npi_ipp_get_dfifo_rd_ptr(handle, portn,
+								&curr_rd_ptr))
+							!= NPI_SUCCESS)
+				goto fail;
+			if ((rs = npi_ipp_get_dfifo_wr_ptr(handle, portn,
+								&curr_wr_ptr))
+							!= NPI_SUCCESS)
+				goto fail;
+
+			if ((rd_ptr == curr_rd_ptr) && (wr_ptr == curr_wr_ptr))
+				stall_cnt++;
+			else {
+				*valid = B_FALSE;
+				break;
+			}
+		}
+
+		if (valid) {	/* futher check to see if ECC UE is valid */
+			if ((rs = npi_ipp_read_dfifo(handle, portn, rd_ptr, &d0,
+					&d1, &d2, &d3, &d4)) != NPI_SUCCESS)
+				goto fail;
+			if ((d4 & 0x1) == 0)	/* Not the 1st line */
+				*valid = B_FALSE;
+		}
+	}
+
+	return (NXGE_OK);
+fail:
+	return (NXGE_ERROR | rs);
+}
diff --git a/drivers/net/nxge/nxge_txc.c b/drivers/net/nxge/nxge_txc.c
new file mode 100644
index 0000000..22a04ab
--- /dev/null
+++ b/drivers/net/nxge/nxge_txc.c
@@ -0,0 +1,397 @@
+/*
+ * nxge_ethtool.c	Neptune TX Controller interface functions
+ *
+ * Copyright (C) 2007 Sun Microsystems, Inc.  All rights reserved.
+ * Use is subject to license terms.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * - Redistribution of source code must retain the above copyright notice,
+ * this list of conditions and the following disclaimer.
+ *
+ * - Redistribution in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ *  Neither the name of Sun Microsystems, Inc. or the names of contributors
+ * may be used to endorse or promote products derived from this software
+ * without specific prior written permission.
+ *
+ *  This software is provided "AS IS," without a warranty of any kind.
+ * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING
+ * ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR
+ * NON-INFRINGEMENT, ARE HEREBY EXCLUDED. SUN MICROSYSTEMS, INC. ("SUN") AND
+ * ITS LICENSORS SHALL NOT BE LIABLE FOR ANY DAMAGES SUFFERED BY LICENSEE AS A
+ * RESULT OF USING, MODIFYING OR DISTRIBUTING THIS SOFTWARE OR ITS DERIVATIVES.
+ * IN NO EVENT WILL SUN OR ITS LICENSORS BE LIABLE FOR ANY LOST REVENUE, PROFIT
+ * OR DATA, OR FOR DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL OR
+ * PUNITIVE DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY,
+ * ARISING OUT OF THE USE OF OR INABILITY TO USE THIS SOFTWARE, EVEN IF SUN
+ * HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.
+ *
+ *   You acknowledge that this software is not designed, licensed or intended
+ * for use in the design, construction, operation or maintenance of any
+ * nuclear facility.
+ */
+
+#include <nxge_impl.h>
+#include <nxge_txc.h>
+
+
+static nxge_status_t
+nxge_txc_handle_port_errors(p_nxge_t nxgep, uint32_t err_status);
+
+static void
+nxge_txc_inject_port_err(uint8_t portn, txc_int_stat_dbg_t *txcs,
+			    uint8_t istats);
+
+nxge_status_t
+nxge_txc_init(p_nxge_t nxgep)
+{
+	uint8_t			port;
+	npi_handle_t		handle;
+	npi_status_t		rs = NPI_SUCCESS;
+
+	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+
+	port = nxgep->mac.portnum;
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "==> nxge_txc_init: portn %d", port));
+
+	(void) npi_txc_control_clear(handle, port);
+	/*
+	 * Enable the TXC controller.
+	 */
+	if ((rs = npi_txc_global_enable(handle)) != NPI_SUCCESS) {
+		goto fail;
+	}
+
+	/* Enable this port within the TXC. */
+	if ((rs = npi_txc_port_enable(handle, port)) != NPI_SUCCESS) {
+		goto fail;
+	}
+
+	/* Clear Counters for this port port within the TXC. */
+	if ((rs = npi_txc_control_clear(handle, port)) != NPI_SUCCESS) {
+		goto fail;
+	}
+
+	/* Bind DMA channels to this port. */
+	if ((rs = npi_txc_port_dma_enable(handle, port,
+			TXDMA_PORT_BITMAP(nxgep))) != NPI_SUCCESS) {
+		goto fail;
+	}
+
+	/* Unmask all TXC interrupts */
+	npi_txc_global_imask_set(handle, port, 0x3F);
+
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "<== nxge_txc_init: portn %d", port));
+
+	return (NXGE_OK);
+fail:
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_txc_init: Failed to initialize txc on port %d",
+			port));
+	return (NXGE_ERROR | rs);
+}
+
+nxge_status_t
+nxge_txc_uninit(p_nxge_t nxgep)
+{
+	uint8_t			port;
+	npi_handle_t		handle;
+	npi_status_t		rs = NPI_SUCCESS;
+
+	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+
+	port = nxgep->mac.portnum;
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "==> nxge_txc_uninit: portn %d", port));
+
+	/*
+	 * disable the TXC controller.
+	 */
+	if ((rs = npi_txc_global_disable(handle)) != NPI_SUCCESS) {
+		goto fail;
+	}
+
+	/* disable this port within the TXC. */
+	if ((rs = npi_txc_port_disable(handle, port)) != NPI_SUCCESS) {
+		goto fail;
+	}
+
+	/* unbind DMA channels to this port. */
+	if ((rs = npi_txc_port_dma_enable(handle, port, 0)) != NPI_SUCCESS) {
+		goto fail;
+	}
+
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "<== nxge_txc_uninit: portn %d", port));
+
+	return (NXGE_OK);
+fail:
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			"nxge_txc_init: Failed to uninitialize txc on port %d",
+			port));
+
+	return (NXGE_ERROR | rs);
+}
+
+void
+nxge_txc_regs_dump(p_nxge_t nxgep)
+{
+	uint32_t		cnt1, cnt2;
+	npi_handle_t		handle;
+	txc_control_t		control;
+	uint32_t		bitmap = 0;
+
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "\nTXC dump: func # %d:\n",
+		nxgep->function_num));
+
+	handle = NXGE_DEV_NPI_HANDLE(nxgep);
+
+	npi_txc_control(handle, OP_GET, &control);
+	npi_txc_port_dma_list_get(handle, nxgep->function_num, &bitmap);
+
+	NXGE_DEBUG_MSG((nxgep, TX_CTL, "\n\tTXC port control 0x%0llx",
+			    (long long)control.value));
+	NXGE_DEBUG_MSG((nxgep, TX_CTL,
+			    "\n\tTXC port bitmap 0x%x", bitmap));
+
+	npi_txc_pkt_xmt_to_mac_get(handle, nxgep->function_num, &cnt1, &cnt2);
+	NXGE_DEBUG_MSG((nxgep, TX_CTL,
+			    "\n\tTXC bytes to MAC %d packets to MAC %d",
+			    cnt1, cnt2));
+
+	npi_txc_pkt_stuffed_get(handle, nxgep->function_num, &cnt1, &cnt2);
+	NXGE_DEBUG_MSG((nxgep, TX_CTL,
+			    "\n\tTXC ass packets %d reorder packets %d",
+			    cnt1 & 0xffff, cnt2 & 0xffff));
+
+	npi_txc_reorder_get(handle, nxgep->function_num, &cnt1);
+	NXGE_DEBUG_MSG((nxgep, TX_CTL,
+		    "\n\tTXC reorder resource %d", cnt1 & 0xff));
+}
+
+nxge_status_t
+nxge_txc_handle_sys_errors(p_nxge_t nxgep)
+{
+	npi_handle_t		handle;
+	txc_int_stat_t		istatus;
+	uint32_t		err_status = 0;
+	uint8_t			err_portn = 0;
+	boolean_t		my_err = B_FALSE;
+	nxge_status_t		status = NXGE_OK;
+
+	handle = nxgep->npi_handle;
+
+	npi_txc_global_istatus_get(handle, (txc_int_stat_t *)&istatus.value);
+
+	switch (nxgep->mac.portnum) {
+	case 0:
+		if (istatus.bits.ldw.port0_int_status) {
+			my_err = B_TRUE;
+			err_portn = 0;
+			err_status = istatus.bits.ldw.port0_int_status;
+		}
+		break;
+	case 1:
+		if (istatus.bits.ldw.port1_int_status) {
+			my_err = B_TRUE;
+			err_portn = 1;
+			err_status = istatus.bits.ldw.port1_int_status;
+		}
+		break;
+	case 2:
+		if (istatus.bits.ldw.port2_int_status) {
+			my_err = B_TRUE;
+			err_portn = 2;
+			err_status = istatus.bits.ldw.port2_int_status;
+		}
+		break;
+	case 3:
+		if (istatus.bits.ldw.port3_int_status) {
+			my_err = B_TRUE;
+			err_portn = 3;
+			err_status = istatus.bits.ldw.port3_int_status;
+		}
+		break;
+	default:
+		return (NXGE_ERROR);
+	}
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+			    " nxge_txc_handle_sys_erors: errored port %d",
+			    err_portn));
+	NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL, "nxge_txc_handle_sys_erors: "
+			"my_err = 0x%x", err_status));
+	if (my_err) {
+		status = nxge_txc_handle_port_errors(nxgep, err_status);
+	}
+	return (NXGE_OK);
+}
+
+static nxge_status_t
+nxge_txc_handle_port_errors(p_nxge_t nxgep, uint32_t err_status)
+{
+	npi_handle_t		handle;
+	npi_status_t		rs = NPI_SUCCESS;
+	p_nxge_txc_stats_t	statsp;
+	txc_int_stat_t		istatus;
+	boolean_t		txport_fatal = B_FALSE;
+	uint8_t			portn;
+	nxge_status_t		status = NXGE_OK;
+
+	handle = nxgep->npi_handle;
+	statsp = (p_nxge_txc_stats_t)&nxgep->statsp->txc_stats;
+	portn = nxgep->mac.portnum;
+	istatus.value = 0;
+
+	if ((err_status & TXC_INT_STAT_RO_CORR_ERR) ||
+			(err_status & TXC_INT_STAT_RO_CORR_ERR) ||
+			(err_status & TXC_INT_STAT_RO_UNCORR_ERR) ||
+			(err_status & TXC_INT_STAT_REORDER_ERR)) {
+		if ((rs = npi_txc_ro_states_get(handle, portn,
+				&statsp->errlog.ro_st)) != NPI_SUCCESS) {
+			return (NXGE_ERROR | rs);
+		}
+		if (err_status & TXC_INT_STAT_RO_CORR_ERR) {
+			statsp->ro_correct_err++;
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+					"nxge_txc_err_evnts: "
+					"RO FIFO correctable error"));
+		}
+		if (err_status & TXC_INT_STAT_RO_UNCORR_ERR) {
+			statsp->ro_uncorrect_err++;
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+					"nxge_txc_err_evnts: "
+					"RO FIFO uncorrectable error"));
+		}
+		if (err_status & TXC_INT_STAT_REORDER_ERR) {
+			statsp->reorder_err++;
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+					"nxge_txc_err_evnts: "
+					"FATAL: Reorder error"));
+			txport_fatal = B_TRUE;
+		}
+	}
+
+	if ((err_status & TXC_INT_STAT_SF_CORR_ERR) ||
+			(err_status & TXC_INT_STAT_SF_UNCORR_ERR)) {
+		if ((rs = npi_txc_sf_states_get(handle, portn,
+				&statsp->errlog.sf_st)) != NPI_SUCCESS) {
+			return (NXGE_ERROR | rs);
+		}
+		if (err_status & TXC_INT_STAT_SF_CORR_ERR) {
+			statsp->sf_correct_err++;
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+					"nxge_txc_err_evnts: "
+					"SF FIFO correctable error"));
+		}
+		if (err_status & TXC_INT_STAT_SF_UNCORR_ERR) {
+			statsp->sf_uncorrect_err++;
+			NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+					"nxge_txc_err_evnts: "
+					"SF FIFO uncorrectable error"));
+		}
+	}
+
+	if (err_status & TXC_INT_STAT_PKTASSYDEAD) {
+		statsp->pkt_assy_dead++;
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				"nxge_txc_err_evnts: Assembly dead"));
+	}
+
+	/* Clear corresponding errors */
+	switch (portn) {
+	case 0:
+		istatus.bits.ldw.port0_int_status = err_status;
+		break;
+	case 1:
+		istatus.bits.ldw.port1_int_status = err_status;
+		break;
+	case 2:
+		istatus.bits.ldw.port2_int_status = err_status;
+		break;
+	case 3:
+		istatus.bits.ldw.port3_int_status = err_status;
+		break;
+	default:
+		return (NXGE_ERROR);
+	}
+
+	npi_txc_global_istatus_clear(handle, istatus.value);
+
+	if (txport_fatal) {
+		NXGE_ERROR_MSG((nxgep, NXGE_ERR_CTL,
+				" nxge_txc_handle_sys_errors:"
+				" fatal Error on Port#%d\n",
+				portn));
+		status = nxge_tx_port_fatal_err_recover(nxgep);
+	}
+
+	return (status);
+}
+
+void
+nxge_txc_inject_err(p_nxge_t nxgep, uint32_t err_id)
+{
+	txc_int_stat_dbg_t	txcs;
+	uint8_t			portn = nxgep->mac.portnum;
+
+	printk(KERN_INFO "TXC error Inject\n");
+	switch (err_id) {
+	case NXGE_FM_EREPORT_TXC_RO_CORRECT_ERR:
+	case NXGE_FM_EREPORT_TXC_RO_UNCORRECT_ERR:
+	case NXGE_FM_EREPORT_TXC_SF_CORRECT_ERR:
+	case NXGE_FM_EREPORT_TXC_SF_UNCORRECT_ERR:
+	case NXGE_FM_EREPORT_TXC_ASSY_DEAD:
+	case NXGE_FM_EREPORT_TXC_REORDER_ERR:
+		NXGE_REG_RD64(nxgep->npi_handle, TXC_INT_STAT_DBG_REG,
+					&txcs.value);
+		if (err_id == NXGE_FM_EREPORT_TXC_RO_CORRECT_ERR) {
+			nxge_txc_inject_port_err(portn, &txcs,
+						TXC_INT_STAT_RO_CORR_ERR);
+		} else if (err_id == NXGE_FM_EREPORT_TXC_RO_UNCORRECT_ERR) {
+			nxge_txc_inject_port_err(portn, &txcs,
+						TXC_INT_STAT_RO_UNCORR_ERR);
+		} else if (err_id == NXGE_FM_EREPORT_TXC_SF_CORRECT_ERR) {
+			nxge_txc_inject_port_err(portn,	&txcs,
+						TXC_INT_STAT_SF_CORR_ERR);
+		} else if (err_id == NXGE_FM_EREPORT_TXC_SF_UNCORRECT_ERR) {
+			nxge_txc_inject_port_err(portn, &txcs,
+						TXC_INT_STAT_SF_UNCORR_ERR);
+		} else if (err_id == NXGE_FM_EREPORT_TXC_ASSY_DEAD) {
+			nxge_txc_inject_port_err(portn, &txcs,
+						TXC_INT_STAT_PKTASSYDEAD);
+		} else if (err_id == NXGE_FM_EREPORT_TXC_REORDER_ERR) {
+			nxge_txc_inject_port_err(portn, &txcs,
+						TXC_INT_STAT_REORDER_ERR);
+		}
+		NXGE_DEBUG_MSG((nxgep, TX_CTL,
+				    "Write 0x%llx to TXC_INT_STAT_DBG_REG\n",
+				    txcs.value));
+		NXGE_REG_WR64(nxgep->npi_handle, TXC_INT_STAT_DBG_REG,
+					txcs.value);
+	}
+}
+
+static void
+nxge_txc_inject_port_err(uint8_t portn, txc_int_stat_dbg_t *txcs,
+				uint8_t istats)
+{
+	switch (portn) {
+	case 0:
+		txcs->bits.ldw.port0_int_status |= istats;
+		break;
+	case 1:
+		txcs->bits.ldw.port1_int_status |= istats;
+		break;
+	case 2:
+		txcs->bits.ldw.port2_int_status |= istats;
+		break;
+	case 3:
+		txcs->bits.ldw.port3_int_status |= istats;
+		break;
+	default:
+		;
+	}
+}
-- 
1.5.5.4

