From 85bc911e88acfc15cfc8f78b396584f5f33640df Mon Sep 17 00:00:00 2001
From: Hong H. Pham <hong.pham@windriver.com>
Date: Mon, 22 Dec 2008 13:57:49 -0500
Subject: [PATCH] cmt load balance

The scheduler's load balancer tries to place tasks on CPUs within the same
scheduler domain, as tasks which are "close" to a processor have less memory
access latency.  This strategy is not optimal for multicore CMT (Chip Multi
Threaded) processors such as the Niagara T1 and T2.

CMT processors increase parallelism by having a core support multiple
strands.  A strand appears as a single (virtual) processor to the OS.
Unlike SMP, a core allows only one of its strand to run at a time.  The
other strands are stalled waiting for some CPU resource, such as
memory bandwidth or an integer/floating point unit.  Placing tasks within
the same core (which is handled at the SD_LVL_CPU scheduling domain)
increases resource contention, which can result in a performance degradation.

This patch changes the load balancing bias towards scheduler domains on other
cores that are within the same NUMA node when CONFIG_SCHED_CMT is enabled.
Benchmarks have shown that for a lightly loaded system, i.e. less than 30%
loaded, a performance gain of up to ~43% is realized.  Diminishing to no
performance gains are seen with high thread counts (eg. over 24 threads)
as the system becomes saturated.

Signed-off-by: Hong H. Pham <hong.pham@windriver.com>
diff --git a/kernel/sched.c b/kernel/sched.c
index 431d2e1..96663f3 100644
--- a/kernel/sched.c
+++ b/kernel/sched.c
@@ -2118,48 +2118,77 @@ static unsigned long target_load(int cpu, int type)
 	unsigned long total = weighted_cpuload(cpu);

 	if (type == 0 || !sched_feat(LB_BIAS))
 		return total;

 	return max(rq->cpu_load[type-1], total);
 }

+#ifdef CONFIG_SCHED_CMT
+/*
+ * For Chip Multi Threaded (CMT) processors, bias balancing towards
+ * scheduler domains on other cores that are within the same NUMA node.
+ */
+static inline int sched_cmt_bias(struct sched_domain *sd)
+{
+	switch(sd->level) {
+	case SD_LV_CPU:
+	case SD_LV_MC:
+	case SD_LV_SIBLING:
+		return 1;
+	default:
+		return 0;
+	}
+}
+#else
+static inline int sched_cmt_bias(struct sched_domain *sd)
+{
+	return 0;
+}
+#endif
+
 /*
  * find_idlest_group finds and returns the least busy CPU group within the
  * domain.
  */
 static struct sched_group *
 find_idlest_group(struct sched_domain *sd, struct task_struct *p, int this_cpu)
 {
 	struct sched_group *idlest = NULL, *this = NULL, *group = sd->groups;
 	unsigned long min_load = ULONG_MAX, this_load = 0;
 	int load_idx = sd->forkexec_idx;
 	int imbalance = 100 + (sd->imbalance_pct-100)/2;

 	do {
 		unsigned long load, avg_load;
 		int local_group;
+		int cmt_bias;
 		int i;

 		/* Skip over this group if it has no CPUs allowed */
 		if (!cpus_intersects(group->cpumask, p->cpus_allowed))
 			continue;

 		local_group = cpu_isset(this_cpu, group->cpumask);

 		/* Tally up the load of all CPUs in the group */
 		avg_load = 0;
+		cmt_bias = sched_cmt_bias(sd);

 		for_each_cpu_mask_nr(i, group->cpumask) {
 			/* Bias balancing toward cpus of our domain */
 			if (local_group)
-				load = source_load(i, load_idx);
+				load = cmt_bias
+				       ? target_load(i, load_idx)
+				       : source_load(i, load_idx);
 			else
-				load = target_load(i, load_idx);
+				load = cmt_bias
+				       ? source_load(i, load_idx)
+				       : target_load(i, load_idx);

 			avg_load += load;
 		}

 		/* Adjust by relative CPU power of the group */
 		avg_load = sg_div_cpu_power(group,
 				avg_load * SCHED_LOAD_SCALE);

@@ -3197,23 +3226,25 @@ find_busiest_group(struct sched_domain *sd, int this_cpu,
 	else if (idle == CPU_NEWLY_IDLE)
 		load_idx = sd->newidle_idx;
 	else
 		load_idx = sd->idle_idx;

 	do {
 		unsigned long load, group_capacity, max_cpu_load, min_cpu_load;
 		int local_group;
+		int cmt_bias;
 		int i;
 		int __group_imb = 0;
 		unsigned int balance_cpu = -1, first_idle_cpu = 0;
 		unsigned long sum_nr_running, sum_weighted_load;
 		unsigned long sum_avg_load_per_task;
 		unsigned long avg_load_per_task;

+		cmt_bias = sched_cmt_bias(sd);
 		local_group = cpu_isset(this_cpu, group->cpumask);

 		if (local_group)
 			balance_cpu = first_cpu(group->cpumask);

 		/* Tally up the load of all CPUs in the group */
 		sum_weighted_load = sum_nr_running = avg_load = 0;
 		sum_avg_load_per_task = avg_load_per_task = 0;
@@ -3234,19 +3265,23 @@ find_busiest_group(struct sched_domain *sd, int this_cpu,

 			/* Bias balancing toward cpus of our domain */
 			if (local_group) {
 				if (idle_cpu(i) && !first_idle_cpu) {
 					first_idle_cpu = 1;
 					balance_cpu = i;
 				}

-				load = target_load(i, load_idx);
+				load = cmt_bias
+				       ? target_load(i, load_idx)
+				       : source_load(i, load_idx);
 			} else {
-				load = source_load(i, load_idx);
+				load = cmt_bias
+				       ? source_load(i, load_idx)
+				       : target_load(i, load_idx);
 				if (load > max_cpu_load)
 					max_cpu_load = load;
 				if (min_cpu_load > load)
 					min_cpu_load = load;
 			}

 			avg_load += load;
 			sum_nr_running += rq->nr_running;
