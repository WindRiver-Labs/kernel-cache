From 19287bab45a2ae79af62a64a68460f51440359b4 Mon Sep 17 00:00:00 2001
From: Kumar Gala <galak@kernel.crashing.org>
Date: Thu, 18 Dec 2008 17:27:11 +0800
Subject: [PATCH] powerpc/85xx: Add SMP support for e500/85xx based platforms

git: git://git.kernel.org/pub/scm/linux/kernel/git/galak/powerpc.git 8572-smp
commit id: cf56718f99eb4ca19de08fb8c7c34a6f3874c44a

Add SMP support for e500/85xx based platforms.

Signed-off-by: Kevin Hao <kexin.hao@windriver.com>
---
 arch/powerpc/include/asm/pgalloc-32.h  |    7 +
 arch/powerpc/kernel/head_fsl_booke.S   |  546 ++++++++++++++++++++++++--------
 arch/powerpc/kernel/misc_32.S          |   44 +++-
 arch/powerpc/mm/pgtable_32.c           |   99 ++++++-
 arch/powerpc/platforms/85xx/Makefile   |    2 +
 arch/powerpc/platforms/85xx/smp.c      |  103 ++++++
 arch/powerpc/platforms/Kconfig.cputype |    2 +-
 7 files changed, 663 insertions(+), 140 deletions(-)
 create mode 100644 arch/powerpc/platforms/85xx/smp.c

diff --git a/arch/powerpc/include/asm/pgalloc-32.h b/arch/powerpc/include/asm/pgalloc-32.h
index 58c0714..1cb9245 100644
--- a/arch/powerpc/include/asm/pgalloc-32.h
+++ b/arch/powerpc/include/asm/pgalloc-32.h
@@ -36,7 +36,14 @@ extern pgtable_t pte_alloc_one(struct mm_struct *mm, unsigned long addr);
 extern void pte_free_kernel(struct mm_struct *mm, pte_t *pte);
 extern void pte_free(struct mm_struct *mm, pgtable_t pte);
 
+#ifdef CONFIG_SMP
+extern void pgtable_free_tlb(struct mmu_gather *tlb, struct page *pte);
+
+#define __pte_free_tlb(tlb, pte)	pgtable_free_tlb(tlb, pte)
+
+#else
 #define __pte_free_tlb(tlb, pte)	pte_free((tlb)->mm, (pte))
+#endif	/* CONFIG_SMP */
 
 #define check_pgt_cache()	do { } while (0)
 
diff --git a/arch/powerpc/kernel/head_fsl_booke.S b/arch/powerpc/kernel/head_fsl_booke.S
index 78f8505..d697d9c 100644
--- a/arch/powerpc/kernel/head_fsl_booke.S
+++ b/arch/powerpc/kernel/head_fsl_booke.S
@@ -92,6 +92,7 @@ _ENTRY(_start);
  * if needed
  */
 
+_ENTRY(__early_start)
 /* 1. Find the index of the entry we're executing in */
 	bl	invstr				/* Find our address */
 invstr:	mflr	r6				/* Make it accessible */
@@ -344,6 +345,15 @@ skpinv:	addi	r6,r6,1				/* Increment */
 	mtspr	SPRN_DBSR,r2
 #endif
 
+#ifdef CONFIG_SMP
+	/* Check to see if we're the second processor, and jump
+	 * to the secondary_start code if so
+	 */
+	mfspr	r24,SPRN_PIR
+	cmpwi	r24,0
+	bne	__secondary_start
+#endif
+
 	/*
 	 * This is where the main kernel code starts.
 	 */
@@ -411,34 +421,64 @@ skpinv:	addi	r6,r6,1				/* Increment */
 
 /* Macros to hide the PTE size differences
  *
- * FIND_PTE -- walks the page tables given EA & pgdir pointer
+ * FIND_PTE_ADDR -- walks the page tables given EA & pgdir pointer
  *   r10 -- EA of fault
  *   r11 -- PGDIR pointer
  *   r12 -- free
  *   label 2: is the bailout case
  *
  * if we find the pte (fall through):
- *   r11 is low pte word
  *   r12 is pointer to the pte
  */
+
 #ifdef CONFIG_PTE_64BIT
-#define PTE_FLAGS_OFFSET	4
-#define FIND_PTE	\
+#  define FIND_PTE_ADDR \
 	rlwinm	r12, r10, 13, 19, 29;	/* Compute pgdir/pmd offset */	\
 	lwzx	r11, r12, r11;		/* Get pgd/pmd entry */		\
 	rlwinm.	r12, r11, 0, 0, 20;	/* Extract pt base address */	\
-	beq	2f;			/* Bail if no table */		\
-	rlwimi	r12, r10, 23, 20, 28;	/* Compute pte address */	\
-	lwz	r11, 4(r12);		/* Get pte entry */
+	rlwimi	r12, r10, 23, 20, 28;	/* Compute pte address */
+
+#    define LOAD_PTE \
+	lwz	r11, 4(r12);
+
+#  ifdef CONFIG_SMP
+#    define LWARX_PTE \
+	li	r11, 4;							\
+	lwarx	r11, r11, r12;		/* lwarx pte */
+
+#    define STWCX_PTE \
+	addi	r12, r12, 4;	\
+	stwcx.	r11, 0, r12;	\
+	addi	r12, r12, -4;	\
 #else
-#define PTE_FLAGS_OFFSET	0
-#define FIND_PTE	\
-	rlwimi	r11, r10, 12, 20, 29;	/* Create L1 (pgdir/pmd) address */	\
-	lwz	r11, 0(r11);		/* Get L1 entry */			\
-	rlwinm.	r12, r11, 0, 0, 19;	/* Extract L2 (pte) base address */	\
-	beq	2f;			/* Bail if no table */			\
-	rlwimi	r12, r10, 22, 20, 29;	/* Compute PTE address */		\
-	lwz	r11, 0(r12);		/* Get Linux PTE */
+#    define LWARX_PTE \
+	lwz	r11, 4(r12)
+
+#    define STWCX_PTE \
+	stw	r11, 4(r12)
+#endif /* CONFIG_SMP */
+
+#else /* 32-bit PTEs */
+#  define FIND_PTE_ADDR	\
+	rlwinm	r12, r10, 12, 20, 29;	/* Compute pgdir/pmd offset */	\
+	lwzx	r11, r12, r11;		/* Get L1 entry */		\
+	rlwinm.	r12, r11, 0, 0, 19;	/* Extract pte base address */	\
+	rlwimi	r12, r10, 22, 20, 29;	/* Compute PTE address */	\
+
+#    define LOAD_PTE \
+	lwz	r11, 0(r12);
+
+#  ifdef CONFIG_SMP
+#    define LWARX_PTE \
+	lwarx	r11, 0, r12;		/* lwarx pte */
+#    define STWCX_PTE \
+	stwcx.	r11, 0, r12;
+#  else
+#    define LWARX_PTE \
+	lwz	r11, 0(r12);
+#    define STWCX_PTE \
+	stw	r11, 0(r12);
+#  endif
 #endif
 
 /*
@@ -503,38 +543,76 @@ interrupt_base:
 	mfspr	r11,SPRN_SPRG3
 	lwz	r11,PGDIR(r11)
 4:
-	FIND_PTE
+	FIND_PTE_ADDR
+
+	beq	2f	/* Bail if there's no entry */
+
+5:
+	LOAD_PTE
 
 	/* Are _PAGE_USER & _PAGE_RW set & _PAGE_HWWRITE not? */
 	andi.	r13, r11, _PAGE_RW|_PAGE_USER|_PAGE_HWWRITE
 	cmpwi	0, r13, _PAGE_RW|_PAGE_USER
 	bne	2f			/* Bail if not */
 
-	/* Update 'changed'. */
-	ori	r11, r11, _PAGE_DIRTY|_PAGE_ACCESSED|_PAGE_HWWRITE
-	stw	r11, PTE_FLAGS_OFFSET(r12) /* Update Linux page table */
-
-	/* MAS2 not updated as the entry does exist in the tlb, this
-	   fault taken to detect state transition (eg: COW -> DIRTY)
-	 */
-	andi.	r11, r11, _PAGE_HWEXEC
-	rlwimi	r11, r11, 31, 27, 27	/* SX <- _PAGE_HWEXEC */
-	ori	r11, r11, (MAS3_UW|MAS3_SW|MAS3_UR|MAS3_SR)@l /* set static perms */
-
 	/* update search PID in MAS6, AS = 0 */
-	mfspr	r12, SPRN_PID0
-	slwi	r12, r12, 16
-	mtspr	SPRN_MAS6, r12
+	mfspr	r13, SPRN_PID0
+	slwi	r13, r13, 16
+	mtspr	SPRN_MAS6, r13
 
-	/* find the TLB index that caused the fault.  It has to be here. */
+	/* find the TLB index that caused the fault. */
 	tlbsx	0, r10
 
+#ifdef CONFIG_SMP
+	/*
+	 * It's possible another processor kicked out the entry
+	 * before we did our tlbsx, so check if we hit
+	 */
+	mfspr	r13, SPRN_MAS1
+	rlwinm.	r13,r13, 0, 0, 0;	/* Check the Valid bit */
+	beq	2f
+#endif /* CONFIG_SMP */
+
+	/*
+	 * MAS2 not updated as the entry does exist in the tlb, this
+	 * fault taken to detect state transition (eg: COW -> DIRTY)
+	 */
+	andi.	r13, r11, _PAGE_HWEXEC
+	rlwimi	r13, r13, 31, 27, 27	/* SX <- _PAGE_HWEXEC */
+	ori	r13, r13, (MAS3_UW|MAS3_SW|MAS3_UR|MAS3_SR)@l /* set static perms */
+
 	/* only update the perm bits, assume the RPN is fine */
-	mfspr	r12, SPRN_MAS3
-	rlwimi	r12, r11, 0, 20, 31
-	mtspr	SPRN_MAS3,r12
+	mfspr	r10, SPRN_MAS3
+	rlwimi	r10, r13, 0, 20, 31
+	mtspr	SPRN_MAS3,r10
 	tlbwe
 
+#ifdef CONFIG_SMP
+	mr	r13, r11
+	LWARX_PTE
+	cmpw	r13, r11
+	bne-	7f
+#endif
+
+	/* Update 'changed'. */
+	ori	r11, r11, _PAGE_DIRTY|_PAGE_ACCESSED|_PAGE_HWWRITE
+	STWCX_PTE	/* r11 and r12 must be PTE and &PTE */
+
+#ifdef CONFIG_SMP
+	/*
+	 * If the stwcx. failed, we invalidate the entry we just wrote,
+	 * and start over
+	 */
+	beq+	6f
+7:	mfspr	r13, SPRN_MAS1
+	rlwinm	r13, r13, 0, 1, 31	/* Clear Valid bit */
+	mtspr	SPRN_MAS1, r13
+	tlbwe
+
+	b	5b		/* Try again */
+#endif	/* CONFIG_SMP */
+6:
+
 	/* Done...restore registers and get out of here.  */
 	mfspr	r11, SPRN_SPRG7R
 	mtcr	r11
@@ -545,6 +623,13 @@ interrupt_base:
 	rfi			/* Force context change */
 
 2:
+#ifdef CONFIG_SMP
+	/* Clear the reservation */
+	lis	r11, dummy_stwcx@h
+	ori	r11,r11, dummy_stwcx@l
+	stwcx.	r11, 0, r11
+#endif
+
 	/*
 	 * The bailout.  Restore registers to pre-exception conditions
 	 * and call the heavyweights to help us out.
@@ -634,18 +719,123 @@ interrupt_base:
 	lwz	r11,PGDIR(r11)
 
 4:
-	FIND_PTE
+	FIND_PTE_ADDR
+
+	beq	2f	/* Bail if no entry */
+
+5:
+	LOAD_PTE
+
 	andi.	r13, r11, _PAGE_PRESENT	/* Is the page present? */
 	beq	2f			/* Bail if not present */
 
+	/*
+	 * We set execute, because we don't have the granularity to
+	 * properly set this at the page level (Linux problem).
+	 * Many of these bits are software only.  Bits we don't set
+	 * here we (properly should) assume have the appropriate value.
+	 */
+
+	mfspr	r10, SPRN_MAS2
 #ifdef CONFIG_PTE_64BIT
-	lwz	r13, 0(r12)
+	rlwimi	r10, r11, 26, 24, 31	/* extract ...WIMGE from pte */
+#else
+	rlwimi	r10, r11, 26, 27, 31	/* extract WIMGE from pte */
+#endif
+#ifdef CONFIG_SMP
+	ori	r10, r10, (MAS2_M)@l
+#endif
+	mtspr	SPRN_MAS2, r10
+
+	bge	5, 11f
+
+	/* is user addr */
+	andi.	r13, r11, (_PAGE_USER | _PAGE_HWWRITE | _PAGE_HWEXEC)
+	andi.	r10, r11, _PAGE_USER	/* Test for _PAGE_USER */
+	srwi	r10, r13, 1
+	or	r10, r13, r10	/* Copy user perms into supervisor */
+	iseleq	r10, 0, r10
+	b	22f
+
+	/* is kernel addr */
+11:	rlwinm	r10, r11, 31, 29, 29	/* Extract _PAGE_HWWRITE into SW */
+	ori	r10, r10, (MAS3_SX | MAS3_SR)
+
+#ifdef CONFIG_PTE_64BIT
+22:	lwz	r13, 0(r12)
+	rlwimi	r10, r13, 24, 0, 7	/* grab RPN[32:39] */
+	rlwimi	r10, r11, 24, 8, 19	/* grab RPN[40:51] */
+	mtspr	SPRN_MAS3, r10
+BEGIN_FTR_SECTION
+	srwi	r10, r13, 8		/* grab RPN[8:31] */
+	mtspr	SPRN_MAS7, r10
+END_FTR_SECTION_IFSET(CPU_FTR_BIG_PHYS)
+#else
+22:	rlwimi	r10, r11, 0, 0, 19	/* Extract RPN from PTE and
+					 * merge with perms */
+	mtspr	SPRN_MAS3, r10
+#endif
+#ifdef CONFIG_E200
+	/* Round robin TLB1 entries assignment */
+	mfspr	r13, SPRN_MAS0
+
+	/* Extract TLB1CFG(NENTRY) */
+	mfspr	r10, SPRN_TLB1CFG
+	andi.	r10, r10, 0xfff
+
+	/* Extract MAS0(NV) */
+	andi.	r13, r13, 0xfff
+	addi	r13, r13, 1
+	cmpw	0, r13, r10
+
+	/* check if we need to wrap */
+	blt	77f
+
+	/* wrap back to first free tlbcam entry */
+	lis	r13, tlbcam_index@ha
+	lwz	r13, tlbcam_index@l(r13)
+77:
+	mfspr	r10, SPRN_MAS0
+	rlwimi	r10, r13, 0, 20, 31
+	mtspr	SPRN_MAS0,r10
+#endif /* CONFIG_E200 */
+
+	tlbwe
+
+#ifdef CONFIG_SMP
+	mr	r13, r11
+	LWARX_PTE
+	cmpw	r13, r11
+	bne-	7f
 #endif
+
 	ori	r11, r11, _PAGE_ACCESSED
-	stw	r11, PTE_FLAGS_OFFSET(r12)
+	STWCX_PTE
+
+#ifdef CONFIG_SMP
+	/*
+	 * If the stwcx. failed, we need to invalidate
+	 * the TLB entry, and re-lwarx the pte
+	 */
+	beq	88f
+7:	mfspr	r13, SPRN_MAS1
+	rlwinm	r13, r13, 0, 1, 31	/* Clear Valid bit */
+	mtspr	SPRN_MAS1, r13
+	tlbwe
+
+	b	5b
+#endif	/* CONFIG_SMP */
+
+88:
+	/* Done...restore registers and get out of here.  */
+	mfspr	r11, SPRN_SPRG7R
+	mtcr	r11
+	mfspr	r13, SPRN_SPRG5R
+	mfspr	r12, SPRN_SPRG4R
+	mfspr	r11, SPRN_SPRG1
+	mfspr	r10, SPRN_SPRG0
+	rfi					/* Force context change */
 
-	 /* Jump to common tlb load */
-	b	finish_tlb_load
 2:
 	/* The bailout.  Restore registers to pre-exception conditions
 	 * and call the heavyweights to help us out.
@@ -694,18 +884,122 @@ interrupt_base:
 	lwz	r11,PGDIR(r11)
 
 4:
-	FIND_PTE
+	FIND_PTE_ADDR
+
+	beq	2f	/* Bail if no entry */
+
+5:
+	LOAD_PTE
+
 	andi.	r13, r11, _PAGE_PRESENT	/* Is the page present? */
 	beq	2f			/* Bail if not present */
 
+	/*
+	 * We set execute, because we don't have the granularity to
+	 * properly set this at the page level (Linux problem).
+	 * Many of these bits are software only.  Bits we don't set
+	 * here we (properly should) assume have the appropriate value.
+	 */
+
+	mfspr	r10, SPRN_MAS2
+#ifdef CONFIG_PTE_64BIT
+	rlwimi	r10, r11, 26, 24, 31	/* extract ...WIMGE from pte */
+#else
+	rlwimi	r10, r11, 26, 27, 31	/* extract WIMGE from pte */
+#endif
+#ifdef CONFIG_SMP
+	ori	r10, r10, (MAS2_M)@l
+#endif
+	mtspr	SPRN_MAS2, r10
+
+	bge	5, 11f
+
+	/* is user addr */
+	andi.	r13, r11, (_PAGE_USER | _PAGE_HWWRITE | _PAGE_HWEXEC)
+	andi.	r10, r11, _PAGE_USER	/* Test for _PAGE_USER */
+	srwi	r10, r13, 1
+	or	r10, r13, r10	/* Copy user perms into supervisor */
+	iseleq	r10, 0, r10
+	b	22f
+
+	/* is kernel addr */
+11:	rlwinm	r10, r11, 31, 29, 29	/* Extract _PAGE_HWWRITE into SW */
+	ori	r10, r10, (MAS3_SX | MAS3_SR)
+
 #ifdef CONFIG_PTE_64BIT
-	lwz	r13, 0(r12)
+22:	lwz	r13, 0(r12)
+	rlwimi	r10, r13, 24, 0, 7	/* grab RPN[32:39] */
+	rlwimi	r10, r11, 24, 8, 19	/* grab RPN[40:51] */
+	mtspr	SPRN_MAS3, r10
+BEGIN_FTR_SECTION
+	srwi	r10, r13, 8		/* grab RPN[8:31] */
+	mtspr	SPRN_MAS7, r10
+END_FTR_SECTION_IFSET(CPU_FTR_BIG_PHYS)
+#else
+22:	rlwimi	r10, r11, 0, 0, 19	/* Extract RPN from PTE and
+					 * merge with perms */
+	mtspr	SPRN_MAS3, r10
+#endif
+#ifdef CONFIG_E200
+	/* Round robin TLB1 entries assignment */
+	mfspr	r13, SPRN_MAS0
+
+	/* Extract TLB1CFG(NENTRY) */
+	mfspr	r10, SPRN_TLB1CFG
+	andi.	r10, r10, 0xfff
+
+	/* Extract MAS0(NV) */
+	andi.	r13, r13, 0xfff
+	addi	r13, r13, 1
+	cmpw	0, r13, r10
+
+	/* check if we need to wrap */
+	blt	77f
+
+	/* wrap back to first free tlbcam entry */
+	lis	r13, tlbcam_index@ha
+	lwz	r13, tlbcam_index@l(r13)
+77:
+	mfspr	r10, SPRN_MAS0
+	rlwimi	r10, r13, 0, 20, 31
+	mtspr	SPRN_MAS0,r10
+#endif /* CONFIG_E200 */
+
+	tlbwe
+
+#ifdef CONFIG_SMP
+	mr	r13, r11
+	LWARX_PTE
+	cmpw	r13, r11
+	bne-	7f
 #endif
+
 	ori	r11, r11, _PAGE_ACCESSED
-	stw	r11, PTE_FLAGS_OFFSET(r12)
+	STWCX_PTE
 
-	/* Jump to common TLB load point */
-	b	finish_tlb_load
+#ifdef CONFIG_SMP
+	/*
+	 * If the stwcx. failed, we need to invalidate
+	 * the TLB entry, and re-lwarx the pte
+	 */
+	beq	88f
+7:	mfspr	r13, SPRN_MAS1
+	rlwinm	r13, r13, 0, 1, 31	/* Clear Valid bit */
+	mtspr	SPRN_MAS1, r13
+	tlbwe
+
+	b	5b
+#endif	/* CONFIG_SMP */
+
+88:
+	/* Done...restore registers and get out of here.  */
+	mfspr	r11, SPRN_SPRG7R
+	mtcr	r11
+	mfspr	r13, SPRN_SPRG5R
+	mfspr	r12, SPRN_SPRG4R
+	mfspr	r11, SPRN_SPRG1
+	mfspr	r10, SPRN_SPRG0
+	rfi					/* Force context change */
 
 2:
 	/* The bailout.  Restore registers to pre-exception conditions
@@ -773,96 +1067,6 @@ data_access:
 	addi	r3,r1,STACK_FRAME_OVERHEAD
 	EXC_XFER_EE_LITE(0x0300, CacheLockingException)
 
-/*
-
- * Both the instruction and data TLB miss get to this
- * point to load the TLB.
- *	r10 - EA of fault
- *	r11 - TLB (info from Linux PTE)
- *	r12, r13 - available to use
- *	CR5 - results of addr >= PAGE_OFFSET
- *	MAS0, MAS1 - loaded with proper value when we get here
- *	MAS2, MAS3 - will need additional info from Linux PTE
- *	Upon exit, we reload everything and RFI.
- */
-finish_tlb_load:
-	/*
-	 * We set execute, because we don't have the granularity to
-	 * properly set this at the page level (Linux problem).
-	 * Many of these bits are software only.  Bits we don't set
-	 * here we (properly should) assume have the appropriate value.
-	 */
-
-	mfspr	r12, SPRN_MAS2
-#ifdef CONFIG_PTE_64BIT
-	rlwimi	r12, r11, 26, 24, 31	/* extract ...WIMGE from pte */
-#else
-	rlwimi	r12, r11, 26, 27, 31	/* extract WIMGE from pte */
-#endif
-	mtspr	SPRN_MAS2, r12
-
-	bge	5, 1f
-
-	/* is user addr */
-	andi.	r12, r11, (_PAGE_USER | _PAGE_HWWRITE | _PAGE_HWEXEC)
-	andi.	r10, r11, _PAGE_USER	/* Test for _PAGE_USER */
-	srwi	r10, r12, 1
-	or	r12, r12, r10	/* Copy user perms into supervisor */
-	iseleq	r12, 0, r12
-	b	2f
-
-	/* is kernel addr */
-1:	rlwinm	r12, r11, 31, 29, 29	/* Extract _PAGE_HWWRITE into SW */
-	ori	r12, r12, (MAS3_SX | MAS3_SR)
-
-#ifdef CONFIG_PTE_64BIT
-2:	rlwimi	r12, r13, 24, 0, 7	/* grab RPN[32:39] */
-	rlwimi	r12, r11, 24, 8, 19	/* grab RPN[40:51] */
-	mtspr	SPRN_MAS3, r12
-BEGIN_FTR_SECTION
-	srwi	r10, r13, 8		/* grab RPN[8:31] */
-	mtspr	SPRN_MAS7, r10
-END_FTR_SECTION_IFSET(CPU_FTR_BIG_PHYS)
-#else
-2:	rlwimi	r11, r12, 0, 20, 31	/* Extract RPN from PTE and merge with perms */
-	mtspr	SPRN_MAS3, r11
-#endif
-#ifdef CONFIG_E200
-	/* Round robin TLB1 entries assignment */
-	mfspr	r12, SPRN_MAS0
-
-	/* Extract TLB1CFG(NENTRY) */
-	mfspr	r11, SPRN_TLB1CFG
-	andi.	r11, r11, 0xfff
-
-	/* Extract MAS0(NV) */
-	andi.	r13, r12, 0xfff
-	addi	r13, r13, 1
-	cmpw	0, r13, r11
-	addi	r12, r12, 1
-
-	/* check if we need to wrap */
-	blt	7f
-
-	/* wrap back to first free tlbcam entry */
-	lis	r13, tlbcam_index@ha
-	lwz	r13, tlbcam_index@l(r13)
-	rlwimi	r12, r13, 0, 20, 31
-7:
-	mtspr	SPRN_MAS0,r12
-#endif /* CONFIG_E200 */
-
-	tlbwe
-
-	/* Done...restore registers and get out of here.  */
-	mfspr	r11, SPRN_SPRG7R
-	mtcr	r11
-	mfspr	r13, SPRN_SPRG5R
-	mfspr	r12, SPRN_SPRG4R
-	mfspr	r11, SPRN_SPRG1
-	mfspr	r10, SPRN_SPRG0
-	rfi					/* Force context change */
-
 #ifdef CONFIG_SPE
 /* Note that the SPE support is closely modeled after the AltiVec
  * support.  Changes to one are likely to be applicable to the
@@ -1109,6 +1313,73 @@ _GLOBAL(flush_dcache_L1)
 
 	blr
 
+#ifdef CONFIG_SMP
+/* When we get here, r24 needs to hold the CPU # */
+	.globl __secondary_start
+__secondary_start:
+	lis	r3,__secondary_hold_acknowledge@h
+	ori	r3,r3,__secondary_hold_acknowledge@l
+	stw	r24,0(r3)
+
+	li	r3,0
+	mr	r4,r24		/* Why? */
+	bl	call_setup_cpu
+
+	/* r26 should be safe, here */
+	lis	r26, tlbcam_index@ha
+	lwz	r26, tlbcam_index@l(r26)
+
+	/* Load CAM 0 */
+	li	r3,0
+	bl	loadcam_entry
+
+	/* Load CAM 1, if it's set */
+	li	r3,1
+	cmpw	r3,r26
+	bgt	1f
+	bl	loadcam_entry
+1:
+	/* Load CAM 2, if it's set */
+	li	r3,2
+	cmpw	r3,r26
+	bgt	2f
+	bl	loadcam_entry
+2:
+
+	/* get current_thread_info and current */
+	lis	r1,secondary_ti@ha
+	lwz	r1,secondary_ti@l(r1)
+	lwz	r2,TI_TASK(r1)
+
+	/* stack */
+	addi	r1,r1,THREAD_SIZE-STACK_FRAME_OVERHEAD
+	li	r0,0
+	stw	r0,0(r1)
+
+	/* ptr to current thread */
+	addi	r4,r2,THREAD	/* address of our thread_struct */
+	mtspr	SPRN_SPRG3,r4
+
+	/* Setup the defaults for TLB entries */
+	li	r4,(MAS4_TSIZED(BOOKE_PAGESZ_4K))@l
+	mtspr	SPRN_MAS4,r4
+
+	/* Jump to start_secondary */
+	lis	r4,MSR_KERNEL@h
+	ori	r4,r4,MSR_KERNEL@l
+	lis	r3,start_secondary@h
+	ori	r3,r3,start_secondary@l
+	mtspr	SPRN_SRR0,r3
+	mtspr	SPRN_SRR1,r4
+	sync
+	rfi
+	sync
+
+	.globl __secondary_hold_acknowledge
+__secondary_hold_acknowledge:
+	.long	-1
+#endif
+
 /*
  * We put a few things here that have to be page-aligned. This stuff
  * goes at the beginning of the data segment, which is page-aligned.
@@ -1123,6 +1394,13 @@ empty_zero_page:
 	.globl	swapper_pg_dir
 swapper_pg_dir:
 	.space	PGD_TABLE_SIZE
+/*
+ * We need a place to stwcx. to when we want to clear a reservation
+ * without knowing where the original was.
+ */
+	.globl	dummy_stwcx
+dummy_stwcx:
+	.space	4
 
 /*
  * Room for two PTE pointers, usually the kernel and current user pointers
diff --git a/arch/powerpc/kernel/misc_32.S b/arch/powerpc/kernel/misc_32.S
index 9eb7efd..8b177b5 100644
--- a/arch/powerpc/kernel/misc_32.S
+++ b/arch/powerpc/kernel/misc_32.S
@@ -271,6 +271,12 @@ _GLOBAL(real_writeb)
 
 #endif /* CONFIG_40x */
 
+#ifdef CONFIG_SMP
+	.globl	ivax_lock
+ivax_lock:
+	.space	4
+#endif	/* CONFIG_SMP */
+
 /*
  * Flush MMU TLB
  */
@@ -294,6 +300,20 @@ _GLOBAL(_tlbia)
 
 	isync
 #elif defined(CONFIG_FSL_BOOKE)
+#ifdef CONFIG_SMP
+	/* For mutual exclusion for invalidate */
+	mfmsr	r11
+	wrteei	0
+	lis	r8, ivax_lock@h
+	ori	r8, r8, ivax_lock@l
+11:	lwarx	r5, 0, r8
+	cmpwi	r5, 0
+	bne	11b
+	ori	r5,r5,1
+	stwcx.	r5, 0, r8
+	bne	11b
+#endif	/* CONFIG_SMP */
+
 	/* Invalidate all entries in TLB0 */
 	li	r3, 0x04
 	tlbivax	0,3
@@ -301,8 +321,11 @@ _GLOBAL(_tlbia)
 	li	r3, 0x0c
 	tlbivax	0,3
 	msync
+	TLBSYNC
 #ifdef CONFIG_SMP
-	tlbsync
+	li	r0, 0
+	stw	r0, 0(r8)
+	wrtee	r11
 #endif /* CONFIG_SMP */
 #else /* !(CONFIG_40x || CONFIG_44x || CONFIG_FSL_BOOKE) */
 #if defined(CONFIG_SMP)
@@ -392,13 +415,30 @@ _GLOBAL(_tlbie)
 	isync
 10:
 #elif defined(CONFIG_FSL_BOOKE)
+#ifdef CONFIG_SMP
+	/* For mutual exclusion for invalidate */
+	mfmsr	r11
+	wrteei	0
+	lis	r8, ivax_lock@h
+	ori	r8, r8, ivax_lock@l
+11:	lwarx	r5, 0, r8
+	cmpwi	r5, 0
+	bne	11b
+	ori	r5,r5,1
+	stwcx.	r5, 0, r8
+	bne	11b
+#endif	/* CONFIG_SMP */
+
 	rlwinm	r4, r3, 0, 0, 19
 	ori	r5, r4, 0x08	/* TLBSEL = 1 */
 	tlbivax	0, r4
 	tlbivax	0, r5
 	msync
+	TLBSYNC
 #if defined(CONFIG_SMP)
-	tlbsync
+	li	r0, 0
+	stw	r0, 0(r8)
+	wrtee	r11
 #endif /* CONFIG_SMP */
 #else /* !(CONFIG_40x || CONFIG_44x || CONFIG_FSL_BOOKE) */
 #if defined(CONFIG_SMP)
diff --git a/arch/powerpc/mm/pgtable_32.c b/arch/powerpc/mm/pgtable_32.c
index 3e69b0d..9eef287 100644
--- a/arch/powerpc/mm/pgtable_32.c
+++ b/arch/powerpc/mm/pgtable_32.c
@@ -26,7 +26,13 @@
 #include <linux/vmalloc.h>
 #include <linux/init.h>
 #include <linux/highmem.h>
+#include <linux/sched.h>
 
+#ifdef CONFIG_SMP
+#include <linux/rcupdate.h>
+#endif
+
+#include <asm/tlb.h>
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
 #include <asm/fixmap.h>
@@ -48,7 +54,7 @@ EXPORT_SYMBOL(ioremap_bot);	/* aka VMALLOC_END */
 
 extern char etext[], _stext[];
 
-#ifdef CONFIG_SMP
+#if defined(CONFIG_SMP) && !defined(CONFIG_FSL_BOOKE)
 extern void hash_page_sync(void);
 #endif
 
@@ -79,6 +85,93 @@ extern unsigned long p_mapped_by_tlbcam(unsigned long pa);
 #define PGDIR_ORDER	0
 #endif
 
+#ifdef CONFIG_SMP
+struct pte_freelist_batch
+{
+	struct rcu_head	rcu;
+	unsigned int	index;
+	struct page *	tables[0];
+};
+
+#define PTE_FREELIST_SIZE \
+	((PAGE_SIZE - sizeof(struct pte_freelist_batch)) \
+	  / sizeof(struct page *))
+
+DEFINE_PER_CPU(struct pte_freelist_batch *, pte_freelist_cur);
+
+static inline void pgtable_free(struct page *pte)
+{
+	unsigned long addr = (unsigned long)page_address(pte);
+
+	pgtable_page_dtor(pte);
+
+	if (!addr) {
+		__free_page(pte);
+	}
+}
+
+static void pte_free_smp_sync(void *arg)
+{
+	/* Do nothing, just ensure we sync with all CPUs */
+}
+
+/* This is only called when we are critically out of memory
+ * (and fail to get a page in pte_free_tlb).
+ */
+static void pgtable_free_now(struct page *pte)
+{
+	smp_call_function(pte_free_smp_sync, NULL, 1);
+
+	pgtable_free(pte);
+}
+
+static void pte_free_rcu_callback(struct rcu_head *head)
+{
+	struct pte_freelist_batch *batch =
+		container_of(head, struct pte_freelist_batch, rcu);
+	unsigned int i;
+
+	for (i = 0; i < batch->index; i++)
+		pgtable_free(batch->tables[i]);
+
+	free_page((unsigned long)batch);
+}
+
+static void pte_free_submit(struct pte_freelist_batch *batch)
+{
+	INIT_RCU_HEAD(&batch->rcu);
+	call_rcu(&batch->rcu, pte_free_rcu_callback);
+}
+
+void pgtable_free_tlb(struct mmu_gather *tlb, struct page *pte)
+{
+	/* This is safe since tlb_gather_mmu has disabled preemption */
+        cpumask_t local_cpumask = cpumask_of_cpu(smp_processor_id());
+	struct pte_freelist_batch **batchp = &__get_cpu_var(pte_freelist_cur);
+
+	if (atomic_read(&tlb->mm->mm_users) < 2 ||
+	    cpus_equal(tlb->mm->cpu_vm_mask, local_cpumask)) {
+		pgtable_free(pte);
+		return;
+	}
+
+	if (*batchp == NULL) {
+		*batchp = (struct pte_freelist_batch *)__get_free_page(GFP_ATOMIC);
+		if (*batchp == NULL) {
+			pgtable_free_now(pte);
+			return;
+		}
+		(*batchp)->index = 0;
+	}
+	(*batchp)->tables[(*batchp)->index++] = pte;
+	if ((*batchp)->index == PTE_FREELIST_SIZE) {
+		pte_free_submit(*batchp);
+		*batchp = NULL;
+	}
+}
+
+#endif	/* CONFIG_SMP */
+
 pgd_t *pgd_alloc(struct mm_struct *mm)
 {
 	pgd_t *ret;
@@ -127,7 +220,7 @@ pgtable_t pte_alloc_one(struct mm_struct *mm, unsigned long address)
 
 void pte_free_kernel(struct mm_struct *mm, pte_t *pte)
 {
-#ifdef CONFIG_SMP
+#if defined(CONFIG_SMP) && !defined(CONFIG_FSL_BOOKE)
 	hash_page_sync();
 #endif
 	free_page((unsigned long)pte);
@@ -135,7 +228,7 @@ void pte_free_kernel(struct mm_struct *mm, pte_t *pte)
 
 void pte_free(struct mm_struct *mm, pgtable_t ptepage)
 {
-#ifdef CONFIG_SMP
+#if defined(CONFIG_SMP) && !defined(CONFIG_FSL_BOOKE)
 	hash_page_sync();
 #endif
 	pgtable_page_dtor(ptepage);
diff --git a/arch/powerpc/platforms/85xx/Makefile b/arch/powerpc/platforms/85xx/Makefile
index cb3054e..f0798c0 100644
--- a/arch/powerpc/platforms/85xx/Makefile
+++ b/arch/powerpc/platforms/85xx/Makefile
@@ -1,6 +1,8 @@
 #
 # Makefile for the PowerPC 85xx linux kernel.
 #
+obj-$(CONFIG_SMP) += smp.o
+
 obj-$(CONFIG_MPC8540_ADS) += mpc85xx_ads.o
 obj-$(CONFIG_MPC8560_ADS) += mpc85xx_ads.o
 obj-$(CONFIG_MPC85xx_CDS) += mpc85xx_cds.o
diff --git a/arch/powerpc/platforms/85xx/smp.c b/arch/powerpc/platforms/85xx/smp.c
new file mode 100644
index 0000000..c91576a
--- /dev/null
+++ b/arch/powerpc/platforms/85xx/smp.c
@@ -0,0 +1,103 @@
+/*
+ * Author: Andy Fleming <afleming@freescale.com>
+ * 	   Kumar Gala <galak@kernel.crashing.org>
+ *
+ * Copyright 2006-2008 Freescale Semiconductor Inc.
+ *
+ * This program is free software; you can redistribute  it and/or modify it
+ * under  the terms of  the GNU General  Public License as published by the
+ * Free Software Foundation;  either version 2 of the  License, or (at your
+ * option) any later version.
+ */
+
+#include <linux/stddef.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/delay.h>
+
+#include <asm/pgtable.h>
+#include <asm/page.h>
+#include <asm/pci-bridge.h>
+#include <asm/mpic.h>
+#include <asm/cacheflush.h>
+
+#include <sysdev/fsl_soc.h>
+
+extern volatile unsigned long __secondary_hold_acknowledge;
+extern void __early_start(void);
+
+#define BOOT_ENTRY_ADDR_UPPER	0
+#define BOOT_ENTRY_ADDR_LOWER	1
+#define BOOT_ENTRY_R3_UPPER	2
+#define BOOT_ENTRY_R3_LOWER	3
+#define BOOT_ENTRY_RESV		4
+#define BOOT_ENTRY_PIR		5
+#define BOOT_ENTRY_R6_UPPER	6
+#define BOOT_ENTRY_R6_LOWER	7
+#define NUM_BOOT_ENTRY		8
+#define SIZE_BOOT_ENTRY		(NUM_BOOT_ENTRY * sizeof(u32))
+
+static void __init
+smp_85xx_kick_cpu(int nr)
+{
+	unsigned long flags;
+	const u64 *cpu_rel_addr;
+	__iomem u32 *bptr_vaddr;
+	struct device_node *np;
+	int n = 0;
+
+	WARN_ON (nr < 0 || nr >= NR_CPUS);
+
+	pr_debug("smp_85xx_kick_cpu: kick CPU #%d\n", nr);
+
+	local_irq_save(flags);
+
+	np = of_get_cpu_node(nr, NULL);
+	cpu_rel_addr = of_get_property(np, "cpu-release-addr", NULL);
+
+	if (cpu_rel_addr == NULL) {
+		printk(KERN_ERR "No cpu-release-addr for cpu %d\n", nr);
+		return;
+	}
+
+	/* Map the spin table */
+	bptr_vaddr = ioremap(*cpu_rel_addr, SIZE_BOOT_ENTRY);
+
+	out_be32(bptr_vaddr + BOOT_ENTRY_PIR, nr);
+	out_be32(bptr_vaddr + BOOT_ENTRY_ADDR_LOWER, __pa(__early_start));
+
+	/* Wait a bit for the CPU to ack. */
+	while ((__secondary_hold_acknowledge != nr) && (++n < 1000))
+		mdelay(1);
+
+	iounmap(bptr_vaddr);
+
+	local_irq_restore(flags);
+
+	pr_debug("waited %d msecs for CPU #%d.\n", nr, n);
+}
+
+static void __init
+smp_85xx_setup_cpu(int cpu_nr)
+{
+	mpic_setup_this_cpu();
+
+	/* Clear any pending timer interrupts */
+	mtspr(SPRN_TSR, TSR_ENW | TSR_WIS | TSR_DIS | TSR_FIS);
+
+	/* Enable decrementer interrupt */
+	mtspr(SPRN_TCR, TCR_DIE);
+}
+
+struct smp_ops_t smp_85xx_ops = {
+	.message_pass = smp_mpic_message_pass,
+	.probe = smp_mpic_probe,
+	.kick_cpu = smp_85xx_kick_cpu,
+	.setup_cpu = smp_85xx_setup_cpu,
+};
+
+void __init
+mpc85xx_smp_init(void)
+{
+	smp_ops = &smp_85xx_ops;
+}
diff --git a/arch/powerpc/platforms/Kconfig.cputype b/arch/powerpc/platforms/Kconfig.cputype
index 7f65127..cfb214c 100644
--- a/arch/powerpc/platforms/Kconfig.cputype
+++ b/arch/powerpc/platforms/Kconfig.cputype
@@ -221,7 +221,7 @@ config VIRT_CPU_ACCOUNTING
 	  If in doubt, say Y here.
 
 config SMP
-	depends on PPC_STD_MMU
+	depends on PPC_STD_MMU || FSL_BOOKE
 	bool "Symmetric multi-processing support"
 	---help---
 	  This enables support for systems with more than one CPU. If you have
-- 
1.6.0.3

