From 4a75bbda56d87d2a8e953325bb401f3b3260b28d Mon Sep 17 00:00:00 2001
From: Phil Staub <Phil.Staub@windriver.com>
Date: Thu, 24 Sep 2009 17:35:03 -0700
Subject: [PATCH] MIPS: Enable RI/XI support on Cavium

Hooks in MM to complete support of read inhibit/execute inhibit
capabilities of some Cavium processors.

The current state of these files reflects an ongoing development
process, and as such implies an extensive modification history that is
not cited in detail here. It is worth noting, however, that this
collection represents the work of the following people who were
involved in that history:

Tomaso Paoletti <tpaoletti@caviumnetworks.com>
David Daney <ddaney@caviumnetworks.com>
Paul Gortmaker <Paul.Gortmaker@windriver.com>

Signed-off-by: Phil Staub <Phil.Staub@windriver.com>
---
 arch/mips/mm/cache.c            |   19 +++++++++++++++
 arch/mips/mm/init.c             |   22 ++++++++++--------
 include/asm-mips/pgtable-64.h   |   18 +++++++++++++++
 include/asm-mips/pgtable-bits.h |   47 ++++++++++++++++++++++++++++++++++++++-
 4 files changed, 95 insertions(+), 11 deletions(-)

diff --git a/arch/mips/mm/cache.c b/arch/mips/mm/cache.c
index 789c97a..16630cf 100644
--- a/arch/mips/mm/cache.c
+++ b/arch/mips/mm/cache.c
@@ -137,6 +137,16 @@ EXPORT_SYMBOL_GPL(_page_cachable_default);
 
 static inline void setup_protection_map(void)
 {
+#ifdef _PAGE_NO_EXEC
+	/*
+	 * It was statically initialized with everything but the
+	 * _page_cachable_default bits.
+	 */
+	int i;
+	for (i = 0; i < 16; i++)
+		protection_map[i] = __pgprot(pgprot_val(protection_map[i]) |
+					_page_cachable_default);
+#else
 	protection_map[0] = PAGE_NONE;
 	protection_map[1] = PAGE_READONLY;
 	protection_map[2] = PAGE_COPY;
@@ -153,6 +163,7 @@ static inline void setup_protection_map(void)
 	protection_map[13] = PAGE_READONLY;
 	protection_map[14] = PAGE_SHARED;
 	protection_map[15] = PAGE_SHARED;
+#endif
 }
 
 void __devinit cpu_cache_init(void)
@@ -183,6 +194,14 @@ void __devinit cpu_cache_init(void)
 		tx39_cache_init();
 	}
 
+#ifdef CONFIG_CPU_CAVIUM_OCTEON
+	if (cpu_has_octeon_cache) {
+		extern void __weak octeon_cache_init(void);
+
+		octeon_cache_init();
+	}
+#endif
+
 	setup_protection_map();
 }
 
diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 137c14b..13e73fc 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -146,7 +146,7 @@ void *kmap_coherent(struct page *page, unsigned long addr)
 #if defined(CONFIG_64BIT_PHYS_ADDR) && defined(CONFIG_CPU_MIPS32)
 	entrylo = pte.pte_high;
 #else
-	entrylo = pte_val(pte) >> 6;
+	entrylo = pte_to_entrylo(pte_val(pte));
 #endif
 
 	ENTER_CRITICAL(flags);
@@ -457,6 +457,8 @@ void free_init_pages(const char *what, unsigned long begin, unsigned long end)
 {
 	unsigned long pfn;
 
+	return; /* disabled while we work on section mismatches */
+
 	for (pfn = PFN_UP(begin); pfn < PFN_DOWN(end); pfn++) {
 		struct page *page = pfn_to_page(pfn);
 		void *addr = phys_to_virt(PFN_PHYS(pfn));
@@ -492,18 +494,18 @@ unsigned long pgd_current[NR_CPUS];
  * On 64-bit we've got three-level pagetables with a slightly
  * different layout ...
  */
-#define __page_aligned(order) __attribute__((__aligned__(PAGE_SIZE<<order)))
-
 /*
- * gcc 3.3 and older have trouble determining that PTRS_PER_PGD and PGD_ORDER
- * are constants.  So we use the variants from asm-offset.h until that gcc
- * will officially be retired.
+ * These tables are now all aligned in the linker script.
  */
-pgd_t swapper_pg_dir[_PTRS_PER_PGD] __page_aligned(_PGD_ORDER);
+pgd_t swapper_pg_dir[_PTRS_PER_PGD] \
+	__attribute__((__section__(".data.swapper_pg_dir")));
 #ifdef CONFIG_64BIT
 #ifdef MODULE_START
-pgd_t module_pg_dir[PTRS_PER_PGD] __page_aligned(PGD_ORDER);
+pgd_t module_pg_dir[PTRS_PER_PGD] \
+	__attribute__((__section__(".data.module_pg_dir")));
 #endif
-pmd_t invalid_pmd_table[PTRS_PER_PMD] __page_aligned(PMD_ORDER);
+pmd_t invalid_pmd_table[PTRS_PER_PMD]  \
+	__attribute__((__section__(".data.invalid_pmd_table")));
 #endif
-pte_t invalid_pte_table[PTRS_PER_PTE] __page_aligned(PTE_ORDER);
+pte_t invalid_pte_table[PTRS_PER_PTE]  \
+	__attribute__((__section__(".data.invalid_pte_table")));
diff --git a/include/asm-mips/pgtable-64.h b/include/asm-mips/pgtable-64.h
index 943515f..6c25f84 100644
--- a/include/asm-mips/pgtable-64.h
+++ b/include/asm-mips/pgtable-64.h
@@ -17,6 +17,7 @@
 #include <asm/fixmap.h>
 
 #include <asm-generic/pgtable-nopud.h>
+#include <asm/pgtable-bits.h>
 
 /*
  * Each address space has 2 4K pages as its page directory, giving 1024
@@ -83,6 +84,12 @@
 #define PMD_ORDER		0
 #define PTE_ORDER		0
 #endif
+#ifdef CONFIG_PAGE_SIZE_32KB
+#define PGD_ORDER		0
+#define PUD_ORDER		aieeee_attempt_to_allocate_pud
+#define PMD_ORDER		0
+#define PTE_ORDER		0
+#endif
 #ifdef CONFIG_PAGE_SIZE_64KB
 #define PGD_ORDER		0
 #define PUD_ORDER		aieeee_attempt_to_allocate_pud
@@ -129,7 +136,12 @@ extern pmd_t empty_bad_pmd_table[PTRS_PER_PMD];
  */
 static inline int pmd_none(pmd_t pmd)
 {
+#ifdef CONFIG_CPU_CAVIUM_OCTEON
+	return (pmd_val(pmd) == (unsigned long) invalid_pte_table) ||
+				(!pmd_val(pmd));
+#else
 	return pmd_val(pmd) == (unsigned long) invalid_pte_table;
+#endif
 }
 
 #define pmd_bad(pmd)		(pmd_val(pmd) & ~PAGE_MASK)
@@ -173,9 +185,15 @@ static inline void pud_clear(pud_t *pudp)
 #define pte_pfn(x)		((unsigned long)((x).pte >> (PAGE_SHIFT + 2)))
 #define pfn_pte(pfn, prot)	__pte(((pfn) << (PAGE_SHIFT + 2)) | pgprot_val(prot))
 #else
+#ifdef _PAGE_NO_EXEC
+/* The NO_READ and NO_EXEC added an extra two bits */
+#define pte_pfn(x)		((unsigned long)((x).pte >> (PAGE_SHIFT+2)))
+#define pfn_pte(pfn, prot)	__pte(((pfn) << (PAGE_SHIFT+2)) | pgprot_val(prot))
+#else
 #define pte_pfn(x)		((unsigned long)((x).pte >> PAGE_SHIFT))
 #define pfn_pte(pfn, prot)	__pte(((pfn) << PAGE_SHIFT) | pgprot_val(prot))
 #endif
+#endif
 
 #define __pgd_offset(address)	pgd_index(address)
 #define __pud_offset(address)	(((address) >> PUD_SHIFT) & (PTRS_PER_PUD-1))
diff --git a/include/asm-mips/pgtable-bits.h b/include/asm-mips/pgtable-bits.h
index 1073e6d..4f02284 100644
--- a/include/asm-mips/pgtable-bits.h
+++ b/include/asm-mips/pgtable-bits.h
@@ -50,7 +50,41 @@
 #define _CACHE_SHIFT                3
 #define _CACHE_MASK                 (7<<3)
 
-#else
+#elif defined(CONFIG_USE_RI_XI_PAGE_BITS)
+
+/* When using the RI/XI bit support, we have 14 bits of flags below the physical
+	address. The RI/XI bits are places such that a SRL 8 can strip off the software
+	bits, then a ROTR 2 can move the RI/XI into bits [63:62]. This also limits
+	physical address to 56 bits, which is more than we need right now. Octeon
+	CSRs use 48 bits */
+#define _PAGE_PRESENT               (1<<0)  /* implemented in software */
+#define _PAGE_WRITE                 (1<<2)  /* implemented in software */
+#define _PAGE_ACCESSED              (1<<3)  /* implemented in software */
+#define _PAGE_MODIFIED              (1<<4)  /* implemented in software */
+#define _PAGE_FILE                  (1<<4)  /* set:pagecache unset:swap */
+#define _PAGE_HUGE                  (1<<5)  /* huge tlb page */
+#define _PAGE_NO_EXEC               (1<<6)  /* Page cannot be executed */
+#define _PAGE_NO_READ               (1<<7)  /* Page cannot be read */
+#define _PAGE_GLOBAL                (1<<8)
+#define _PAGE_VALID                 (1<<9)
+#define _PAGE_SILENT_READ           (1<<9)  /* synonym                 */
+#define _PAGE_DIRTY                 (1<<10) /* The MIPS dirty bit      */
+#define _PAGE_SILENT_WRITE          (1<<10)
+#define _CACHE_SHIFT                11
+#define _CACHE_MASK                 (7<<_CACHE_SHIFT)
+#ifndef __ASSEMBLY__
+/* pte_to_entrylo converts a page table entry (PTE) into a Mips entrylo0/1
+   value. This is an inline function instead of a macro since the kernel
+   code tends to pass expressions with ++ in them */
+static inline uint64_t pte_to_entrylo(unsigned long pte_val)
+{
+	/* C has no way to express that this is a DSRL 8 followed by a ROTR 2.
+		Luckily in the fast path this is done in assembly */
+	return (pte_val>>8)|((pte_val&(_PAGE_NO_EXEC|_PAGE_NO_READ))<<56);
+}
+#endif
+
+#else /* CONFIG_USE_RI_XI_PAGE_BITS */
 
 #define _PAGE_PRESENT               (1<<0)  /* implemented in software */
 #define _PAGE_READ                  (1<<1)  /* implemented in software */
@@ -58,6 +92,13 @@
 #define _PAGE_ACCESSED              (1<<3)  /* implemented in software */
 #define _PAGE_MODIFIED              (1<<4)  /* implemented in software */
 #define _PAGE_FILE                  (1<<4)  /* set:pagecache unset:swap */
+#ifndef __ASSEMBLY__
+/* pte_to_entrylo converts a page table entry (PTE) into a Mips entrylo0/1 value */
+static inline uint64_t pte_to_entrylo(unsigned long pte_val)
+{
+	return pte_val>>6;
+}
+#endif
 
 #if defined(CONFIG_CPU_R3000) || defined(CONFIG_CPU_TX39XX)
 
@@ -130,7 +171,11 @@
 
 #endif
 
+#ifdef _PAGE_NO_READ
+#define __READABLE	(_PAGE_SILENT_READ | _PAGE_ACCESSED)
+#else
 #define __READABLE	(_PAGE_READ | _PAGE_SILENT_READ | _PAGE_ACCESSED)
+#endif
 #define __WRITEABLE	(_PAGE_WRITE | _PAGE_SILENT_WRITE | _PAGE_MODIFIED)
 
 #define _PAGE_CHG_MASK  (PAGE_MASK | _PAGE_ACCESSED | _PAGE_MODIFIED | _CACHE_MASK)
-- 
1.5.5.1

