From eced7d6b59b57f9923e2ed76338921c7283d0345 Mon Sep 17 00:00:00 2001
From: Phil Staub <Phil.Staub@windriver.com>
Date: Thu, 24 Sep 2009 14:12:23 -0700
Subject: [PATCH] Cavium: Flash support files for Cavium Octeon family BSP

These files provide flash device support in Cavium's SDK 1.9 BSP,
including compact flash (CF), PATA, NAND and MTD support. The current
state of these files reflects an ongoing development process, and as
such implies an extensive modification history that is not cited in
detail here. It is worth noting, however, that this collection
represents the work of the following people who were involved in that
history:

David Daney <ddaney@caviumnetworks.com>
Paul Gortmaker <Paul.Gortmaker@windriver.com>
Phil Staub <Phil.Staub@windriver.com>

Signed-off-by: Phil Staub <Phil.Staub@windriver.com>
---
 arch/mips/cavium-octeon/ebt3000_cf.c  |  731 +++++++++++++++++++++++++
 arch/mips/cavium-octeon/octeon-nand.c |  515 +++++++++++++++++
 drivers/ata/pata_octeon_cf.c          |  970 +++++++++++++++++++++++++++++++++
 drivers/mtd/maps/Kconfig              |    7 +
 drivers/mtd/maps/Makefile             |    1 +
 drivers/mtd/maps/cavium_flash.c       |   80 +++
 include/linux/mtd/nand.h              |    5 +
 7 files changed, 2309 insertions(+), 0 deletions(-)
 create mode 100644 arch/mips/cavium-octeon/ebt3000_cf.c
 create mode 100644 arch/mips/cavium-octeon/octeon-nand.c
 create mode 100644 drivers/ata/pata_octeon_cf.c
 create mode 100644 drivers/mtd/maps/cavium_flash.c

diff --git a/arch/mips/cavium-octeon/ebt3000_cf.c b/arch/mips/cavium-octeon/ebt3000_cf.c
new file mode 100644
index 0000000..623ee51
--- /dev/null
+++ b/arch/mips/cavium-octeon/ebt3000_cf.c
@@ -0,0 +1,731 @@
+/*
+ * Extra-simple block driver for the Octeon bootbus compact flash. We
+ * are unable to use the normal Linux ATA subsystem since the interrupt
+ * line INTRQ is not hooked up on most Octeon evaluation boards. This
+ * driver is based on the excellent article and example code from LWN.
+ * http://lwn.net/Articles/58719/
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2005-2008 Cavium Networks
+ */
+
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/init.h>
+
+#include <linux/kernel.h>
+#include <linux/fs.h>
+#include <linux/errno.h>
+#include <linux/types.h>
+#include <linux/vmalloc.h>
+#include <linux/genhd.h>
+#include <linux/blkdev.h>
+#include <linux/hdreg.h>
+#include <linux/ide.h>
+#include <linux/delay.h>
+
+#include "cvmx-app-init.h"
+#include "cvmx-sysinfo.h"
+#include "cvmx-compactflash.h"
+#include "hal.h"
+
+#define VERSION "2.0"
+#define DEVICE_NAME "cf"
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Cavium Networks <support@caviumnetworks.com>");
+MODULE_DESCRIPTION("Cavium Networks Octeon Bootbus IDE driver");
+
+static int nodma = 1;
+module_param(nodma, int, 0644);
+MODULE_PARM_DESC(nodma, "Disable DMA access to compact flash.");
+
+#define OCTEON_IDE_DMA_CHANNEL 0
+#define OCTEON_IDE_TIMEOUT (5*HZ)
+
+/* Timing multiple used for configuring the boot bus DMA engine */
+#define CF_DMA_TIMING_MULT	4
+
+/* The internal representation of our device */
+struct cf_device {
+	void *base_ptr;		/* This is a pointer to the external IDE
+				   registers */
+	unsigned long num_sectors;	/* Total number of sectors this disk
+					   has */
+	unsigned long sector_size;	/* Size of each sector in bytes */
+	struct gendisk *gd;	/* Generic disk structure required by the
+				   kernel */
+	struct hd_geometry geo;	/* Geometry object used to answer size ioctls */
+
+	spinlock_t lock;	/* Lock used to control access to the queue */
+	struct request_queue *queue;	/* Linux request queue for pending
+					   operations */
+
+	int region;		/* Octeon bootbus region to use */
+	int pio_mode;		/* CF pio mode */
+	int is16bit;		/* Device is wired using 16bits instead of
+				   8bits */
+	int is_true_ide;	/* Device is wired to support IDE mode. is16bit
+				   must also be set */
+	int use_dma;		/* Non zero if we should use DMA */
+	int dma_channel;	/* Which DMA channel we should use */
+	int dma_mode;		/* CF mwdma mode */
+
+	struct request *current_req;	/* The request currently being
+					   processed */
+	int current_num_sectors;	/* Number of sectors left in the
+					   current request */
+	unsigned long current_start_sector;	/* The start sector for the
+						   current request */
+	void *current_buffer;	/* Memory buffer for the current request */
+
+	struct tasklet_struct poll_tasklet;
+	struct timer_list timeout_timer;
+};
+
+/* We try very hard in this driver to not use globals incase a future board has
+   multiple devices. The funny name is to make it easy to find where single
+   device assumptions were made */
+static struct cf_device STATIC_DEVICE;
+static void octeon_ide_polled_io(unsigned long data);
+
+
+/**
+ * Read an IDE register
+ *
+ * @param cf     Our device state
+ * @param reg    Register to read defined in linux/ata.h
+ *
+ * @return Result of the read
+ */
+static inline uint8_t octeon_ide_read_reg(struct cf_device *cf, int reg)
+{
+	if (cf->is_true_ide)
+		return __raw_readw(cf->base_ptr + reg * 2) & 0xff;
+	else if (cf->is16bit)
+		return __raw_readb(cf->base_ptr + (reg ^ 1));
+	else
+		return __raw_readb(cf->base_ptr + reg);
+}
+
+
+/**
+ * Send a command to the IDE device. The caller must insure the
+ * device is idle.
+ *
+ * @param cf       Our device state
+ * @param sectors  Number of sectors to transfer
+ * @param lba      Starting logical block address
+ * @param command  IDE command to execute
+ * @param features IDE features usd for DMA
+ */
+static void octeon_ide_command(struct cf_device *cf, int sectors,
+			       unsigned long lba, int command, int features)
+{
+	if (cf->is_true_ide) {
+		if (features)	/* Feature 0 is not valid */
+			__raw_writew(features & 0xff,
+				     cf->base_ptr + ATA_REG_FEATURE * 2);
+		__raw_writew(sectors, cf->base_ptr + ATA_REG_NSECT * 2);
+		__raw_writew(lba & 0xff, cf->base_ptr + ATA_REG_LBAL * 2);
+		__raw_writew((lba >> 8) & 0xff,
+			     cf->base_ptr + ATA_REG_LBAM * 2);
+		__raw_writew((lba >> 16) & 0xff,
+			     cf->base_ptr + ATA_REG_LBAH * 2);
+		__raw_writew(((lba >> 24) & 0x0f) | 0xe0,
+			     cf->base_ptr + ATA_REG_DEVICE * 2);
+		__raw_writew(command, cf->base_ptr + ATA_REG_CMD * 2);
+	} else if (cf->is16bit) {
+		__raw_writew(sectors | ((lba & 0xff) << 8), cf->base_ptr + 2);
+		__raw_writew(lba >> 8, cf->base_ptr + 4);
+		__raw_writew(((lba >> 24) & 0x0f) | 0xe0 | command << 8,
+			     cf->base_ptr + 6);
+	} else {
+		__raw_writeb(sectors, cf->base_ptr + ATA_REG_NSECT);
+		__raw_writeb(lba & 0xff, cf->base_ptr + ATA_REG_LBAL);
+		__raw_writeb((lba >> 8) & 0xff, cf->base_ptr + ATA_REG_LBAM);
+		__raw_writeb((lba >> 16) & 0xff, cf->base_ptr + ATA_REG_LBAH);
+		__raw_writeb(((lba >> 24) & 0x0f) | 0xe0,
+			     cf->base_ptr + ATA_REG_DEVICE);
+		__raw_writeb(command, cf->base_ptr + ATA_REG_CMD);
+	}
+}
+
+
+/**
+ * Start a read or write transfer. This function does not block.
+ * If a transfer can't be started it will return failure and
+ * should be called again.
+ *
+ * @param cf     Our device state
+ *
+ * @return Zero on success, negative on failure
+ */
+static void octeon_ide_start_transfer(unsigned long data)
+{
+	struct cf_device *cf = (struct cf_device *) data;
+	int ide_command;
+	int number_sectors = cf->current_num_sectors;
+	int is_write = rq_data_dir(cf->current_req);
+
+	/* Try again later if the device is busy. We have to poll since we
+	   don't have the INTRQ interrupt */
+	if (octeon_ide_read_reg(cf, ATA_REG_STATUS) & ATA_BUSY) {
+		cf->poll_tasklet.func = octeon_ide_start_transfer;
+		tasklet_schedule(&cf->poll_tasklet);
+		return;
+	}
+
+	/* We can only do block transfer of at most 256 sectors. If we need
+	   more, then we will be called again after 256 have completed */
+	if (number_sectors > 256)
+		number_sectors = 256;
+
+	mb();
+	if (cf->use_dma && !nodma) {
+		/* Setup DMA engine */
+		cvmx_mio_boot_dma_cfgx_t dma_cfg;
+		dma_cfg.u64 = 0;
+		dma_cfg.s.en = 1;
+		dma_cfg.s.rw = is_write;
+		dma_cfg.s.swap8 = 1;
+		dma_cfg.s.adr = virt_to_phys(cf->current_buffer);
+		dma_cfg.s.size = number_sectors * (512 / 2) - 1;
+		cvmx_write_csr(CVMX_MIO_BOOT_DMA_CFGX(cf->dma_channel),
+			       dma_cfg.u64);
+		ide_command = (is_write) ? WIN_WRITEDMA : WIN_READDMA;
+	} else {
+		/* We need a polling tasklet since we don't have DMA */
+		cf->poll_tasklet.func = octeon_ide_polled_io;
+		tasklet_schedule(&cf->poll_tasklet);
+		ide_command = (is_write) ? WIN_WRITE : WIN_READ;
+	}
+	mb();
+	octeon_ide_command(cf, number_sectors & 0xff, cf->current_start_sector,
+			   ide_command, 0);
+	mod_timer(&cf->timeout_timer, jiffies + OCTEON_IDE_TIMEOUT);
+}
+
+
+/**
+ * Submit the next pending request to the IDE. If there is a
+ * request currently processing, then nothing is done.
+ *
+ * @param cf     Our device state
+ */
+static void octeon_ide_handle_next_request(struct cf_device *cf)
+{
+	/* We really only submit one transaction, but we may loop through
+	   multiple in error conditions */
+	cf->current_req = elv_next_request(cf->queue);
+	while (cf->current_req && !blk_fs_request(cf->current_req)) {
+		printk(KERN_INFO "%s: Skip non-CMD request\n",
+		       cf->current_req->rq_disk->disk_name);
+		end_request(cf->current_req, 0);
+		cf->current_req = elv_next_request(cf->queue);
+	}
+
+	if (cf->current_req) {
+		cf->current_buffer = cf->current_req->buffer;
+		cf->current_num_sectors = cf->current_req->current_nr_sectors;
+		cf->current_start_sector = cf->current_req->sector;
+		octeon_ide_start_transfer((unsigned long) cf);
+	} else if (cf->is_true_ide && !nodma) {
+		octeon_ide_command(cf, 0, 0, WIN_FLUSH_CACHE, 0);
+	}
+}
+
+
+/**
+ * Called as a tasklet when we don't have DMA. This function polls
+ * the IDE for read/write data. If it would need to stall for any
+ * reason it resubmits itself as a tasklet and returns.
+ *
+ * @param data   Our device state
+ */
+static void octeon_ide_polled_io(unsigned long data)
+{
+	struct cf_device *cf = (struct cf_device *) data;
+	int is_write = rq_data_dir(cf->current_req);
+	int number_sectors = cf->current_num_sectors;
+	int count;
+	unsigned long flags;
+
+	/* The sector read/write commands can move a max of 256 cycles in a
+	   single command */
+	if (number_sectors > 256)
+		number_sectors = 256;
+
+	while (number_sectors--) {
+		int status = octeon_ide_read_reg(cf, ATA_REG_STATUS);
+
+		/* Reschedule ourselves if the device is still busy */
+		if (status & ATA_BUSY) {
+			tasklet_schedule(&cf->poll_tasklet);
+			return;
+		}
+
+		/* There should be data available. If there isn't then we have
+		   an IO error */
+		if (!(status & ATA_DRQ)) {
+			printk(KERN_ERR "%s: IO error on %s\n",
+			       cf->current_req->rq_disk->disk_name,
+			       (is_write) ? "write" : "read");
+			goto done;
+		}
+
+		/* Reads and writes are performed differently in 8 bit or 16
+		   bit mode */
+		if (cf->is16bit) {
+			uint16_t *ptr = (uint16_t *) cf->current_buffer;
+			if (is_write) {
+				for (count = 0; count < cf->sector_size / 2;
+				     count++) {
+					writew(*ptr++, cf->base_ptr);
+					/* Every 16 writes do a read so the
+					   bootbus FIFO doesn't fill up */
+					if (count && ((count & 0xf) == 0))
+						octeon_ide_read_reg(cf,
+							ATA_REG_STATUS);
+				}
+			} else {
+				for (count = 0; count < cf->sector_size / 2;
+				     count++)
+					*ptr++ = readw(cf->base_ptr);
+			}
+		} else {
+			uint8_t *ptr = (uint8_t *) cf->current_buffer;
+			if (is_write) {
+				for (count = 0; count < cf->sector_size;
+				     count++) {
+					writeb(*ptr++, cf->base_ptr);
+					/* Every 16 writes do a read so the
+					   bootbus FIFO doesn't fill up */
+					if (count && ((count & 0xf) == 0))
+						octeon_ide_read_reg(cf,
+							ATA_REG_STATUS);
+				}
+			} else {
+				for (count = 0; count < cf->sector_size;
+				     count++)
+					*ptr++ = readb(cf->base_ptr);
+			}
+		}
+		/* We completed a sector. Update our pointers */
+		cf->current_buffer += cf->sector_size;
+		cf->current_start_sector++;
+		cf->current_num_sectors--;
+	}
+
+	/* If we still have more to go start a new transfer */
+	if (cf->current_num_sectors) {
+		octeon_ide_start_transfer((unsigned long) cf);
+		return;
+	}
+
+	/* We don't complete the transaction until the IDE is idle again */
+	if (octeon_ide_read_reg(cf, ATA_REG_STATUS) & ATA_BUSY) {
+		tasklet_schedule(&cf->poll_tasklet);
+		return;
+	}
+
+done:
+	spin_lock_irqsave(&cf->lock, flags);
+	del_timer(&cf->timeout_timer);
+	end_request(cf->current_req, (cf->current_num_sectors) ? -EIO : 1);
+	octeon_ide_handle_next_request(cf);
+	spin_unlock_irqrestore(&cf->lock, flags);
+}
+
+
+/**
+ * Handle a DMA completion interrupt from the hardware
+ *
+ * @param irq    IRQ number
+ * @param state  Device state
+ *
+ * @return IRG_HANDLED or IRQ_NONE
+ */
+static irqreturn_t octeon_ide_dma_interrupt(int irq, void *state)
+{
+	cvmx_mio_boot_dma_intx_t mio_boot_dma_int;
+	unsigned long flags;
+	struct cf_device *cf = state;
+	int number_sectors = cf->current_num_sectors;
+	cvmx_mio_boot_dma_cfgx_t mio_boot_dma_cfg;
+
+	/* Return if this interrupt isn't for us */
+	mio_boot_dma_int.u64 =
+		cvmx_read_csr(CVMX_MIO_BOOT_DMA_INTX(cf->dma_channel));
+	if (!mio_boot_dma_int.s.done)
+		return IRQ_NONE;
+
+	/* Clear the interrupt */
+	cvmx_write_csr(CVMX_MIO_BOOT_DMA_INTX(cf->dma_channel),
+		       mio_boot_dma_int.u64);
+
+	/* The sector read/write commands can move a max of 256 cycles in a
+	   single command */
+	if (number_sectors > 256)
+		number_sectors = 256;
+
+	/* Check to make sure the DMA completed */
+	mio_boot_dma_cfg.u64 =
+		cvmx_read_csr(CVMX_MIO_BOOT_DMA_CFGX(cf->dma_channel));
+	cvmx_write_csr(CVMX_MIO_BOOT_DMA_CFGX(cf->dma_channel), 0);
+	if (mio_boot_dma_cfg.s.size != 0xfffff) {
+		printk(KERN_ERR "%s: IO error on dma %s\n",
+		       cf->current_req->rq_disk->disk_name,
+		       (mio_boot_dma_cfg.s.rw) ? "write" : "read");
+		goto done;
+	}
+
+	/* Update the sector counts and buffer pointer to be after the transfer
+	   that just completed */
+	cf->current_num_sectors -= number_sectors;
+	cf->current_buffer += number_sectors * cf->sector_size;
+	cf->current_start_sector += number_sectors;
+
+	/* If we still have more to go start a new transfer */
+	if (cf->current_num_sectors) {
+		octeon_ide_start_transfer((unsigned long) cf);
+		return IRQ_HANDLED;
+	}
+
+done:
+	spin_lock_irqsave(&cf->lock, flags);
+	del_timer(&cf->timeout_timer);
+	end_request(cf->current_req, (cf->current_num_sectors) ? -EIO : 1);
+	octeon_ide_handle_next_request(cf);
+	spin_unlock_irqrestore(&cf->lock, flags);
+	return IRQ_HANDLED;
+}
+
+
+/**
+ * Called when a request doesn't complete in time and a timeout
+ * occurs.
+ *
+ * @param data   Our device state
+ */
+static void octeon_ide_timeout(unsigned long data)
+{
+	struct cf_device *cf = (struct cf_device *)data;
+	unsigned long flags;
+
+	printk(KERN_ERR "%s: Timeout\n", cf->current_req->rq_disk->disk_name);
+	spin_lock_irqsave(&cf->lock, flags);
+	if (cf->use_dma && !nodma)
+		cvmx_write_csr(CVMX_MIO_BOOT_DMA_CFGX(cf->dma_channel), 0);
+	end_request(cf->current_req, -ETIME);
+	octeon_ide_handle_next_request(cf);
+	spin_unlock_irqrestore(&cf->lock, flags);
+}
+
+
+/**
+ * Spin waiting for the IDE to become idle
+ *
+ * @param cf     Our device state
+ *
+ * @return Zero on success, negative on timeout
+ */
+static inline int octeon_ide_wait_idle(struct cf_device *cf)
+{
+	unsigned long timeout = jiffies + HZ;
+	while (octeon_ide_read_reg(cf, ATA_REG_STATUS) & ATA_BUSY) {
+		if (unlikely(time_after(jiffies, timeout)))
+			return -1;
+		udelay(5);
+	}
+	return 0;
+}
+
+
+/**
+ * Perform an IDE identify
+ *
+ * @param cf         Our device state
+ * @param drive_info Identify info to fill in
+ *
+ * @return Zero on success, negative on failure
+ */
+static int octeon_ide_identify(struct cf_device *cf,
+			       struct hd_driveid *drive_info)
+{
+	int count;
+
+	if (octeon_ide_wait_idle(cf))
+		return -1;
+
+	octeon_ide_command(cf, 0, 0, WIN_IDENTIFY, 0);
+
+	if (octeon_ide_wait_idle(cf))
+		return -1;
+
+	if (cf->is16bit) {
+		uint16_t *ptr = (uint16_t *) drive_info;
+		for (count = 0; count < sizeof(*drive_info); count += 2)
+			*ptr++ = readw(cf->base_ptr);	/* Swaps internally */
+	} else {
+		unsigned char *ptr = (unsigned char *) drive_info;
+		for (count = 0; count < sizeof(*drive_info); count++)
+			*ptr++ = readb(cf->base_ptr);
+	}
+	ide_fix_driveid(drive_info);
+	ide_fixstring(drive_info->model, sizeof(drive_info->model), 0);
+	ide_fixstring(drive_info->fw_rev, sizeof(drive_info->fw_rev), 0);
+	ide_fixstring(drive_info->serial_no, sizeof(drive_info->serial_no), 0);
+	return 0;
+}
+
+
+/**
+ * Identify a compact flash disk
+ *
+ * @param cf     Device to check and update
+ * @return Zero on success. Failure will result in a device of zero
+ *         size and a -1 return code.
+ */
+static int octeon_ide_init(struct cf_device *cf)
+{
+	cvmx_mio_boot_dma_int_enx_t dma_int_en;
+	struct hd_driveid drive_info;
+	int result;
+
+	memset(&drive_info, 0, sizeof(drive_info));
+
+	/* Default to PIO mode 0 for initial identify */
+	if (cf->is_true_ide)
+		cvmx_compactflash_set_piomode(cf->region, cf->region+1, 0);
+
+	result = octeon_ide_identify(cf, &drive_info);
+	if (result == 0) {
+		/* Sandisk 1G reports the wrong sector size */
+		drive_info.sector_bytes = 512;
+		printk(KERN_INFO
+		       "%s: %s Serial %s (%u sectors, %u bytes/sector)\n",
+		       DEVICE_NAME, drive_info.model, drive_info.serial_no,
+		       drive_info.lba_capacity, (int) drive_info.sector_bytes);
+		cf->num_sectors = drive_info.lba_capacity;
+		cf->sector_size = drive_info.sector_bytes;
+		cf->geo.cylinders = drive_info.cyls;
+		cf->geo.heads = drive_info.heads;
+		cf->geo.sectors = drive_info.sectors;
+		cf->geo.start = 0;
+	} else {
+		cf->num_sectors = 0;
+		cf->sector_size = 512;
+	}
+
+	/* Change to PIO mode */
+	if (cf->is_true_ide) {
+		cf->pio_mode = 4;
+		printk(KERN_INFO "%s: using PIO%d\n", DEVICE_NAME,
+			cf->pio_mode);
+		cvmx_compactflash_set_piomode(cf->region, cf->region+1,
+			cf->pio_mode);
+	}
+
+	if (cf->is_true_ide && !nodma) {
+		uint64_t dma_tim =
+			cvmx_compactflash_generate_dma_tim(CF_DMA_TIMING_MULT,
+							   (void *) &drive_info,
+							   &(cf->dma_mode));
+		if (dma_tim) {
+			cf->dma_channel = OCTEON_IDE_DMA_CHANNEL;
+			cf->use_dma = 1;
+
+			printk(KERN_NOTICE
+			       "%s: using MWDMA mode %d, dma channel %d\n",
+			       DEVICE_NAME, cf->dma_mode, cf->dma_channel);
+			printk(KERN_NOTICE "%s: DMA can be enabled/disabled "
+			       "with %s.nodma=0/1\n", DEVICE_NAME,
+			       "ebt3000_cf");
+			cvmx_write_csr(CVMX_MIO_BOOT_DMA_TIMX(cf->dma_channel),
+				       dma_tim);
+
+			/* Select the DMA mode that we want to use (this does
+			   not seem to be needed or help) */
+			octeon_ide_command(cf, (0x4 << 3) | cf->dma_mode, 0,
+					   WIN_SETFEATURES, 0x3);
+
+			/* Clear the interrupt */
+			cvmx_write_csr(CVMX_MIO_BOOT_DMA_INTX(cf->dma_channel),
+				cvmx_read_csr(CVMX_MIO_BOOT_DMA_INTX(
+					cf->dma_channel)));
+
+			/* Request the interrupt */
+			if (request_irq(OCTEON_IRQ_BOOTDMA,
+					octeon_ide_dma_interrupt, IRQF_SHARED,
+					"MIO_DMA", cf))
+				panic(DEVICE_NAME "couldn't obtain irq  %u",
+				      OCTEON_IRQ_BOOTDMA);
+
+			/* Enable interrupts for DMA completion */
+			dma_int_en.u64 =
+				cvmx_read_csr(CVMX_MIO_BOOT_DMA_INT_ENX
+					      (cf->dma_channel));
+			dma_int_en.s.done = 1;
+			cvmx_write_csr(CVMX_MIO_BOOT_DMA_INT_ENX
+				       (cf->dma_channel), dma_int_en.u64);
+		} else
+			printk(KERN_NOTICE "%s: MWDMA not supported.\n",
+			       DEVICE_NAME);
+	}
+
+	return result;
+}
+
+
+/**
+ * Handle queued IO requests
+ * We're called with the cf->lock
+ *
+ * @param q      queue of requests
+ */
+static void octeon_ide_request(struct request_queue *q)
+{
+	struct cf_device *cf = q->queuedata;
+	if (!cf->current_req)
+		octeon_ide_handle_next_request(cf);
+}
+
+
+/**
+ * Ioctl.
+ *
+ * @param inode
+ * @param filp
+ * @param cmd
+ * @param arg
+ * @return
+ */
+static int octeon_ide_ioctl(struct inode *inode, struct file *filp,
+			    unsigned int cmd, unsigned long arg)
+{
+	struct block_device *bdev = inode->i_bdev;
+	struct gendisk *disk = bdev->bd_disk;
+	struct cf_device *cf = disk->private_data;
+	struct hd_geometry geo;
+	switch (cmd) {
+	case HDIO_GETGEO:
+		if (!arg)
+			return -EINVAL;
+		memcpy(&geo, &cf->geo, sizeof(geo));
+		geo.start = get_start_sect(bdev);
+		if (copy_to_user
+		    ((struct hd_geometry __user *) arg, &geo, sizeof(geo)))
+			return -EFAULT;
+		return 0;
+	}
+	return -ENOTTY;		/* unknown command */
+}
+
+
+/*
+ * The device operations structure.
+ */
+static struct block_device_operations octeon_ide_ops = {
+	.owner = THIS_MODULE,
+	.ioctl = octeon_ide_ioctl,
+};
+
+
+/**
+ * Initialization
+ *
+ * @return
+ */
+static int __init octeon_ide_module_init(void)
+{
+	extern cvmx_bootinfo_t *octeon_bootinfo;
+	struct cf_device *cf = &STATIC_DEVICE;
+	int major_num;
+	int region;
+
+	printk(KERN_NOTICE DEVICE_NAME
+	       ": Octeon bootbus compact flash driver version %s\n", VERSION);
+	if (!octeon_bootinfo->compact_flash_common_base_addr) {
+		printk(KERN_NOTICE DEVICE_NAME
+		       ": Compact flash interface not present.\n");
+		return -ENOMEM;
+	}
+
+	memset(cf, 0, sizeof(*cf));
+	cf->base_ptr =
+		cvmx_phys_to_ptr(octeon_bootinfo->
+				 compact_flash_common_base_addr);
+	tasklet_init(&cf->poll_tasklet, octeon_ide_polled_io,
+		     (unsigned long) cf);
+	setup_timer(&cf->timeout_timer, octeon_ide_timeout, (unsigned long)cf);
+
+	/* Determine from base address is in true IDE mode or not */
+	if (!(octeon_bootinfo->compact_flash_common_base_addr & 0xffff))
+		cf->is_true_ide = 1;
+
+	/* Find the bootbus region for the CF to determine 16 or 8 bit */
+	for (region = 0; region < 8; region++) {
+		cvmx_mio_boot_reg_cfgx_t cfg;
+		cfg.u64 = cvmx_read_csr(CVMX_MIO_BOOT_REG_CFGX(region));
+		if (cfg.s.base ==
+		    octeon_bootinfo->compact_flash_common_base_addr >> 16) {
+			cf->is16bit = cfg.s.width;
+			cf->region = region;
+			printk(KERN_NOTICE DEVICE_NAME ": Compact flash found "
+			       "in bootbus region %d (%d bit%s).\n",
+			       region, (cf->is16bit) ? 16 : 8,
+			       (cf->is_true_ide) ? ", ide" : "");
+			break;
+		}
+	}
+
+	spin_lock_init(&cf->lock);
+
+	/* Probe the disk and see how big it is */
+	octeon_ide_init(cf);
+	if (cf->num_sectors == 0) {
+		printk(KERN_NOTICE DEVICE_NAME
+		       ": Compact flash not detected.\n");
+		return 0;
+	}
+
+	/* Get a request queue. */
+	cf->queue = blk_init_queue(octeon_ide_request, &cf->lock);
+	if (cf->queue == NULL) {
+		printk(DEVICE_NAME
+		       ": unable to allocate block request queue\n");
+		return -ENOMEM;
+	}
+	cf->queue->queuedata = cf;
+	blk_queue_hardsect_size(cf->queue, cf->sector_size);
+
+	/* Get registered. */
+	major_num = register_blkdev(0, DEVICE_NAME);
+	if (major_num <= 0) {
+		printk(DEVICE_NAME ": unable to get major number\n");
+		return -ENOMEM;
+	}
+
+	/* And the gendisk structure. */
+	cf->gd = alloc_disk(64);
+	if (cf->gd == NULL) {
+		printk(DEVICE_NAME ": unable to allocate disk\n");
+		unregister_blkdev(major_num, DEVICE_NAME);
+		return -ENOMEM;
+	}
+
+	cf->gd->major = major_num;
+	cf->gd->first_minor = 0;
+	cf->gd->fops = &octeon_ide_ops;
+	cf->gd->private_data = cf;
+	cf->gd->queue = cf->queue;
+	strcpy(cf->gd->disk_name, DEVICE_NAME "a");
+	set_capacity(cf->gd, cf->num_sectors);
+	add_disk(cf->gd);
+	return 0;
+}
+
+late_initcall(octeon_ide_module_init);
diff --git a/arch/mips/cavium-octeon/octeon-nand.c b/arch/mips/cavium-octeon/octeon-nand.c
new file mode 100644
index 0000000..4c9755b
--- /dev/null
+++ b/arch/mips/cavium-octeon/octeon-nand.c
@@ -0,0 +1,515 @@
+/**
+ * Driver for the Octeon NAND flash controller introduced in CN52XX pass 2.
+ *
+ * LICENSE:
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2008 Cavium Networks
+ */
+
+#include <linux/platform_device.h>
+#include <linux/mtd/nand.h>
+#include <linux/mtd/partitions.h>
+#include <asm/irq.h>
+
+#include "cvmx.h"
+#include "cvmx-nand.h"
+
+#define DRIVER_NAME "octeon-nand"
+
+#define DEBUG_INIT		(1<<0)
+#define DEBUG_READ		(1<<1)
+#define DEBUG_READ_BUFFER	(1<<2)
+#define DEBUG_WRITE		(1<<3)
+#define DEBUG_WRITE_BUFFER	(1<<4)
+#define DEBUG_CONTROL		(1<<5)
+#define DEBUG_SELECT		(1<<6)
+#define DEBUG_ALL		-1
+
+#define MAX_NAND_NAME_LEN       20
+
+#ifdef CONFIG_MTD_PARTITIONS
+static const char *part_probes[] = { "cmdlinepart", NULL };
+static int nr_parts;
+static struct mtd_partition *mtd_parts;
+#endif
+
+#define DEV_DBG(level, dev, format, arg...)			\
+	if (unlikely(debug & level))				\
+		dev_info(dev , "%s " format , __func__, ## arg)
+
+static int debug;
+module_param(debug, int, 0644);
+MODULE_PARM_DESC(debug, "Debug bit field. -1 will turn on all debugging.");
+
+static struct platform_device *octeon_nand_pdev;
+static struct mtd_info *octeon_nand_open_mtd[8];
+
+struct octeon_nand_priv {
+	uint8_t data[4096];	/* Temporary location to store read data. This
+					must be 64bit aligned */
+	int data_len;		/* Number of byte in the data buffer */
+	int data_index;		/* Current read index. Equal to data_len when
+					all data has been read */
+	int selected_chip;	/* Currently selected NAND chip */
+	int selected_page;	/* Last page chosen by SEQIN for PROGRAM */
+	struct platform_device *pdev;	/* Pointer to the platform device */
+};
+
+
+/**
+ * Read a single byte from the temporary buffer. Used after READID
+ * to get the NAND information.
+ *
+ * @param mtd
+ *
+ * @return
+ */
+static uint8_t octeon_nand_read_byte(struct mtd_info *mtd)
+{
+	struct nand_chip *nand = mtd->priv;
+	struct octeon_nand_priv *priv = nand->priv;
+
+	if (priv->data_index < priv->data_len) {
+		DEV_DBG(DEBUG_READ, &priv->pdev->dev, "read of 0x%02x\n",
+			0xff & priv->data[priv->data_index]);
+		return priv->data[priv->data_index++];
+	} else {
+		dev_err(&priv->pdev->dev, "No data to read\n");
+		return 0xff;
+	}
+}
+
+
+/**
+ * Read two bytes from the temporary buffer. Used after READID to
+ * get the NAND information on 16 bit devices.
+ *
+ * @param mtd
+ *
+ * @return
+ */
+static uint16_t octeon_nand_read_word(struct mtd_info *mtd)
+{
+	struct nand_chip *nand = mtd->priv;
+	struct octeon_nand_priv *priv = nand->priv;
+
+	if (priv->data_index + 1 < priv->data_len) {
+		uint16_t result = le16_to_cpup((uint16_t *)(priv->data +
+			priv->data_index));
+		priv->data_index += 2;
+		DEV_DBG(DEBUG_READ, &priv->pdev->dev, "read of 0x%04x\n",
+			0xffff & result);
+		return result;
+	} else {
+		dev_err(&priv->pdev->dev, "No data to read\n");
+		return 0xff;
+	}
+	return 0;
+}
+
+
+/**
+ * Since we have a write page, I don't think this can ever be
+ * called.
+ *
+ * @param mtd
+ * @param buf
+ * @param len
+ */
+static void octeon_nand_write_buf(struct mtd_info *mtd, const uint8_t *buf,
+				int len)
+{
+	struct nand_chip *nand = mtd->priv;
+	struct octeon_nand_priv *priv = nand->priv;
+
+	DEV_DBG(DEBUG_WRITE_BUFFER, &priv->pdev->dev, "len=%d\n", len);
+
+	memcpy(priv->data + priv->data_index, buf, len);
+	priv->data_index += len;
+	priv->data_len += len;
+	/* Linux sometimes thinks there is less OOB data than the chip really
+		has. Make sure all OOB is set to 0xff */
+	memset(priv->data + priv->data_index, 0xff,
+		sizeof(priv->data) - priv->data_index);
+}
+
+
+/**
+ * Read a number of pending bytes from the temporary buffer. Used
+ * to get page and OOB data.
+ *
+ * @param mtd
+ * @param buf
+ * @param len
+ */
+static void octeon_nand_read_buf(struct mtd_info *mtd, uint8_t *buf, int len)
+{
+	struct nand_chip *nand = mtd->priv;
+	struct octeon_nand_priv *priv = nand->priv;
+
+	DEV_DBG(DEBUG_READ_BUFFER, &priv->pdev->dev, "len=%d\n", len);
+
+	if (len <= priv->data_len - priv->data_index) {
+		memcpy(buf, priv->data + priv->data_index, len);
+		priv->data_index += len;
+	} else {
+		dev_err(&priv->pdev->dev,
+			"Not enough data for read of %d bytes\n", len);
+		priv->data_len = 0;
+	}
+}
+
+
+/**
+ * Verify the supplied buffer matches the data we last read
+ *
+ * @param mtd
+ * @param buf
+ * @param len
+ *
+ * @return
+ */
+static int octeon_nand_verify_buf(struct mtd_info *mtd, const uint8_t *buf,
+				int len)
+{
+	struct nand_chip *nand = mtd->priv;
+	struct octeon_nand_priv *priv = nand->priv;
+	if (memcmp(buf, priv->data, len)) {
+		dev_err(&priv->pdev->dev, "Write verify failed\n");
+		return -EFAULT;
+	} else
+		return 0;
+}
+
+
+/**
+ * Select which NAND chip we are working on. A chip of -1
+ * represents that no chip should be selected.
+ *
+ * @param mtd
+ * @param chip
+ */
+static void octeon_nand_select_chip(struct mtd_info *mtd, int chip)
+{
+	/* We don't need to do anything here */
+}
+
+
+/**
+ * Issue a NAND command to the chip. Almost all work is done here.
+ *
+ * @param mtd
+ * @param command
+ * @param column
+ * @param page_addr
+ */
+static void octeon_nand_cmdfunc(struct mtd_info *mtd, unsigned command,
+				int column, int page_addr)
+{
+	int status;
+	struct nand_chip *nand = mtd->priv;
+	struct octeon_nand_priv *priv = nand->priv;
+
+	switch (command) {
+	case NAND_CMD_READID:
+		DEV_DBG(DEBUG_CONTROL, &priv->pdev->dev, "READID\n");
+		priv->data_index = 0;
+		priv->data_len = cvmx_nand_read_id(priv->selected_chip, 0,
+						virt_to_phys(priv->data), 4);
+		if (priv->data_len < 4) {
+			dev_err(&priv->pdev->dev, "READID failed with %d\n",
+				priv->data_len);
+			priv->data_len = 0;
+		}
+		break;
+
+	case NAND_CMD_READOOB:
+		DEV_DBG(DEBUG_CONTROL, &priv->pdev->dev,
+			"READOOB page_addr=0x%x\n", page_addr);
+		priv->data_index = 1;
+		priv->data_len = cvmx_nand_page_read(priv->selected_chip,
+					(page_addr << nand->page_shift) +
+					(1<<nand->page_shift) - 1,
+					virt_to_phys(priv->data),
+					mtd->oobsize + 1);
+		if (priv->data_len < mtd->oobsize + 1) {
+			dev_err(&priv->pdev->dev, "READOOB failed with %d\n",
+				priv->data_len);
+			priv->data_len = 0;
+		}
+		break;
+
+	case NAND_CMD_READ0:
+		DEV_DBG(DEBUG_CONTROL, &priv->pdev->dev,
+			"READ0 page_addr=0x%x\n", page_addr);
+		priv->data_index = 0;
+		priv->data_len = cvmx_nand_page_read(priv->selected_chip,
+					column +
+					(page_addr << nand->page_shift),
+					virt_to_phys(priv->data),
+					(1 << nand->page_shift) +
+					mtd->oobsize);
+		if (priv->data_len < (1 << nand->page_shift) + mtd->oobsize) {
+			dev_err(&priv->pdev->dev, "READ0 failed with %d\n",
+				priv->data_len);
+			priv->data_len = 0;
+		}
+		break;
+
+	case NAND_CMD_ERASE1:
+		DEV_DBG(DEBUG_CONTROL, &priv->pdev->dev,
+			"ERASE1 page_addr=0x%x\n", page_addr);
+		if (cvmx_nand_block_erase(priv->selected_chip,
+			page_addr << nand->page_shift)) {
+			dev_err(&priv->pdev->dev, "ERASE1 failed\n");
+		}
+		break;
+
+	case NAND_CMD_ERASE2:
+		/* We do all erase processing in the first command, so ignore
+			this one */
+		break;
+
+	case NAND_CMD_STATUS:
+		DEV_DBG(DEBUG_CONTROL, &priv->pdev->dev, "STATUS\n");
+		priv->data_index = 0;
+		priv->data_len = 2;
+		priv->data[0] = cvmx_nand_get_status(priv->selected_chip);
+		priv->data[1] = priv->data[0];
+		break;
+
+	case NAND_CMD_SEQIN:
+		DEV_DBG(DEBUG_CONTROL, &priv->pdev->dev,
+			"SEQIN column=%d page_addr=0x%x\n", column, page_addr);
+		/* If we don't seem to be doing sequential writes then erase
+			all data assuming it is old */
+		if (priv->data_index != column)
+			memset(priv->data, 0xff, sizeof(priv->data));
+		priv->data_index = column;
+		priv->data_len = column;
+		priv->selected_page = page_addr;
+		break;
+
+	case NAND_CMD_PAGEPROG:
+		DEV_DBG(DEBUG_CONTROL, &priv->pdev->dev, "PAGEPROG\n");
+		status = cvmx_nand_page_write(priv->selected_chip,
+			priv->selected_page << nand->page_shift,
+			virt_to_phys(priv->data));
+		if (status)
+			dev_err(&priv->pdev->dev, "PAGEPROG failed with %d\n",
+				status);
+		break;
+
+	case NAND_CMD_RESET:
+		DEV_DBG(DEBUG_CONTROL, &priv->pdev->dev, "RESET\n");
+		priv->data_index = 0;
+		priv->data_len = 0;
+		memset(priv->data, 0xff, sizeof(priv->data));
+		status = cvmx_nand_reset(priv->selected_chip);
+		if (status)
+			dev_err(&priv->pdev->dev, "RESET failed with %d\n",
+				status);
+		break;
+
+	default:
+		dev_err(&priv->pdev->dev, "Unsupported command 0x%x\n",
+			command);
+		break;
+	}
+}
+
+
+/**
+ * Determine what NAND devices are available
+ *
+ * @param pdev
+ *
+ * @return
+ */
+static int octeon_nand_probe(struct platform_device *pdev)
+{
+	struct mtd_info *mtd;
+	struct nand_chip *nand;
+	struct octeon_nand_priv *priv;
+	int chip;
+	int active_chips;
+	char *name;
+	int chip_num = 0; /* Count of detected chips, used for device naming */
+
+	DEV_DBG(DEBUG_INIT, &pdev->dev, "called\n");
+
+	cvmx_nand_initialize(0 /* | CVMX_NAND_INITIALIZE_FLAGS_DEBUG */, 0xff);
+
+	active_chips = cvmx_nand_get_active_chips();
+	for (chip = 0; chip < 8; chip++) {
+		/* Skip chip selects that don't have NAND */
+		if ((active_chips & (1<<chip)) == 0)
+			continue;
+
+		/* Allocate and initialize mtd_info, nand_chip and private
+			structures */
+		mtd = kzalloc(sizeof(struct mtd_info) +
+				sizeof(struct nand_chip) +
+				sizeof(struct octeon_nand_priv), GFP_KERNEL);
+		if (!mtd) {
+			dev_err(&pdev->dev, "Unable to allocate structures\n");
+			return -ENOMEM;
+		}
+		name = kmalloc(MAX_NAND_NAME_LEN, GFP_KERNEL);
+		if (!name) {
+			kfree(mtd);
+			dev_err(&pdev->dev, "Unable to allocate structures\n");
+			return -ENOMEM;
+		}
+
+		nand = (struct nand_chip *) (mtd + 1);
+		mtd->priv = (void *) nand;
+		mtd->owner = THIS_MODULE;
+		priv = (struct octeon_nand_priv *) (nand + 1);
+		nand->priv = (void *) priv;
+		memset(priv->data, 0xff, sizeof(priv->data));
+		priv->pdev = pdev;
+		priv->selected_chip = chip;
+
+		nand->ecc.mode = NAND_ECC_SOFT;
+		/* nand->options |= NAND_BUSWIDTH_16; */
+
+		nand->read_byte = octeon_nand_read_byte;
+		nand->read_word = octeon_nand_read_word;
+		nand->write_buf = octeon_nand_write_buf;
+		nand->read_buf = octeon_nand_read_buf;
+		nand->verify_buf = octeon_nand_verify_buf;
+		nand->select_chip = octeon_nand_select_chip;
+		nand->cmdfunc = octeon_nand_cmdfunc;
+
+		if (nand_scan(mtd, 1) != 0) {
+			dev_err(&pdev->dev, "NAND scan failed\n");
+			kfree(mtd);
+			return -ENXIO;
+		}
+
+		/* We need to override the name, as the default names
+		** have spaces in them, and this prevents the passing
+		** of partitioning information on the kernel command line.
+		*/
+		snprintf(name, MAX_NAND_NAME_LEN, "octeon_nand%d", chip_num);
+		mtd->name = name;
+
+
+#ifdef CONFIG_MTD_PARTITIONS
+		nr_parts =
+		parse_mtd_partitions(mtd, part_probes,
+				     &mtd_parts, 0);
+		if (nr_parts > 0)
+			add_mtd_partitions(mtd, mtd_parts, nr_parts);
+		else {
+			if (add_mtd_device(mtd)) {
+				dev_err(&pdev->dev, "Failed to register MTD\n");
+				kfree(mtd);
+				return -ENOMEM;
+			}
+		}
+#else
+		if (add_mtd_device(mtd)) {
+			dev_err(&pdev->dev, "Failed to register MTD\n");
+			kfree(mtd);
+			return -ENOMEM;
+		}
+#endif
+		octeon_nand_open_mtd[chip] = mtd;
+		chip_num++;
+	}
+	return 0;
+}
+
+
+/**
+ * Called when the driver is unloaded. It must clean up all
+ * created devices.
+ *
+ * @param pdev
+ *
+ * @return
+ */
+static int octeon_nand_remove(struct platform_device *pdev)
+{
+	int chip;
+
+	DEV_DBG(DEBUG_INIT, &pdev->dev, "called\n");
+	for (chip = 0; chip < 8; chip++) {
+		if (octeon_nand_open_mtd[chip]) {
+			del_mtd_device(octeon_nand_open_mtd[chip]);
+			kfree(octeon_nand_open_mtd[chip]);
+			octeon_nand_open_mtd[chip] = NULL;
+		}
+	}
+	return 0;
+}
+
+static struct platform_driver octeon_nand_driver = {
+	.probe = octeon_nand_probe,
+	.remove = octeon_nand_remove,
+	.driver = {
+		.owner = THIS_MODULE,
+		.name = DRIVER_NAME,
+	},
+};
+
+/**
+ * Module init
+ *
+ * @return Zero on success, negative on failure
+ */
+static int __init octeon_nand_init(void)
+{
+	struct resource irq_resource;
+
+	if (!OCTEON_IS_MODEL(OCTEON_CN52XX) ||
+		OCTEON_IS_MODEL(OCTEON_CN52XX_PASS1_X))
+		return -ENODEV;
+
+	if (platform_driver_register(&octeon_nand_driver)) {
+		printk(KERN_ERR "%s: Failed to register driver\n", DRIVER_NAME);
+		return -ENOMEM;
+	}
+
+	memset(&irq_resource, 0, sizeof(irq_resource));
+	irq_resource.start = OCTEON_IRQ_BOOTDMA;
+	irq_resource.end = irq_resource.start;
+	irq_resource.flags = IORESOURCE_IRQ;
+
+	octeon_nand_pdev = platform_device_register_simple(DRIVER_NAME, 0,
+		&irq_resource, 1);
+	if (!octeon_nand_pdev) {
+		printk(KERN_ERR "%s: Failed to allocate platform device\n",
+			DRIVER_NAME);
+		platform_driver_unregister(&octeon_nand_driver);
+		return -ENOMEM;
+	}
+	return 0;
+}
+
+
+/**
+ * Module unload
+ *
+ * @return
+ */
+static void __exit octeon_nand_cleanup(void)
+{
+	platform_device_unregister(octeon_nand_pdev);
+	platform_driver_unregister(&octeon_nand_driver);
+}
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Cavium Networks <support@caviumnetworks.com>");
+MODULE_DESCRIPTION("Cavium Networks Octeon NAND driver.");
+/* We need to call octeon_nand_init late enough that the MTD command line
+ * parser is already registered.  If built into the kernel , use a late
+ * initcall. */
+late_initcall(octeon_nand_init);  /* This works for built-in and modular
+				     builds */
+module_exit(octeon_nand_cleanup);
diff --git a/drivers/ata/pata_octeon_cf.c b/drivers/ata/pata_octeon_cf.c
new file mode 100644
index 0000000..a3cf774
--- /dev/null
+++ b/drivers/ata/pata_octeon_cf.c
@@ -0,0 +1,970 @@
+/*
+ * Driver for the Octeon bootbus compact flash.
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2005 - 2009 Cavium Networks
+ * Copyright (C) 2008 Wind River Systems
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/libata.h>
+#include <linux/irq.h>
+#include <linux/platform_device.h>
+#include <linux/workqueue.h>
+#include <scsi/scsi_host.h>
+
+#include "cvmx-app-init.h"
+#include "cvmx-sysinfo.h"
+#include "cvmx-compactflash.h"
+#include "hal.h"
+
+/*
+ * The Octeon bootbus compact flash interface is connected in at least
+ * 3 different configurations on various evaluation boards:
+ *
+ * -- 8  bits no irq, no DMA
+ * -- 16 bits no irq, no DMA
+ * -- 16 bits True IDE mode with DMA, but no irq.
+ *
+ * In the last case the DMA engine can generate an interrupt when the
+ * transfer is complete.  For the first two cases only PIO is supported.
+ *
+ */
+
+#define DRV_NAME	"pata_octeon_cf"
+#define DRV_VERSION	"2.1"
+
+
+struct octeon_cf_port {
+	struct workqueue_struct *wq;
+	struct delayed_work delayed_finish;
+	struct ata_port *ap;
+	int dma_finished;
+};
+
+static struct scsi_host_template octeon_cf_sht = {
+	ATA_PIO_SHT(DRV_NAME),
+};
+
+/**
+ * Convert nanosecond based time to setting used in the
+ * boot bus timing register, based on timing multiple
+ */
+static unsigned int ns_to_tim_reg(unsigned int tim_mult, unsigned int nsecs)
+{
+	unsigned int val;
+
+	/*
+	 * Compute # of eclock periods to get desired duration in
+	 * nanoseconds.
+	 */
+	val = DIV_ROUND_UP(nsecs * (octeon_get_clock_rate() / 1000000),
+			  1000 * tim_mult);
+
+	return val;
+}
+
+static void octeon_cf_set_boot_reg_cfg(int cs)
+{
+	cvmx_mio_boot_reg_cfgx_t reg_cfg;
+	reg_cfg.u64 = cvmx_read_csr(CVMX_MIO_BOOT_REG_CFGX(cs));
+	reg_cfg.s.dmack = 0;	/* Don't assert DMACK on access */
+	reg_cfg.s.tim_mult = 2;	/* Timing mutiplier 2x */
+	reg_cfg.s.rd_dly = 0;	/* Sample on falling edge of BOOT_OE */
+	reg_cfg.s.sam = 0;	/* Don't combine write and output enable */
+	reg_cfg.s.we_ext = 0;	/* No write enable extension */
+	reg_cfg.s.oe_ext = 0;	/* No read enable extension */
+	reg_cfg.s.en = 1;	/* Enable this region */
+	reg_cfg.s.orbit = 0;	/* Don't combine with previous region */
+	reg_cfg.s.ale = 0;	/* Don't do address multiplexing */
+	cvmx_write_csr(CVMX_MIO_BOOT_REG_CFGX(cs), reg_cfg.u64);
+}
+
+/**
+ * Called after libata determines the needed PIO mode. This
+ * function programs the Octeon bootbus regions to support the
+ * timing requirements of the PIO mode.
+ *
+ * @ap:     ATA port information
+ * @dev:    ATA device
+ */
+static void octeon_cf_set_piomode(struct ata_port *ap, struct ata_device *dev)
+{
+	struct octeon_cf_data *ocd = ap->dev->platform_data;
+	cvmx_mio_boot_reg_timx_t reg_tim;
+	int cs = ocd->base_region;
+	int T;
+	struct ata_timing timing;
+
+	int use_iordy;
+	int trh;
+	int pause;
+	/* These names are timing parameters from the ATA spec */
+	int t1;
+	int t2;
+	int t2i;
+
+	T = (int)(2000000000000LL / octeon_get_clock_rate());
+
+	if (ata_timing_compute(dev, dev->pio_mode, &timing, T, T))
+		BUG();
+
+	t1 = timing.setup;
+	if (t1)
+		t1--;
+	t2 = timing.active;
+	if (t2)
+		t2--;
+	t2i = timing.act8b;
+	if (t2i)
+		t2i--;
+
+	trh = ns_to_tim_reg(2, 20);
+	if (trh)
+		trh--;
+
+	pause = timing.cycle - timing.active - timing.setup - trh;
+	if (pause)
+		pause--;
+
+	octeon_cf_set_boot_reg_cfg(cs);
+	if (ocd->dma_engine >= 0)
+		/* True IDE mode, program both chip selects.  */
+		octeon_cf_set_boot_reg_cfg(cs + 1);
+
+
+	use_iordy = ata_pio_need_iordy(dev);
+
+	reg_tim.u64 = cvmx_read_csr(CVMX_MIO_BOOT_REG_TIMX(cs));
+	/* Disable page mode */
+	reg_tim.s.pagem = 0;
+	/* Enable dynamic timing */
+	reg_tim.s.waitm = use_iordy;
+	/* Pages are disabled */
+	reg_tim.s.pages = 0;
+	/* We don't use multiplexed address mode */
+	reg_tim.s.ale = 0;
+	/* Not used */
+	reg_tim.s.page = 0;
+	/* Time after IORDY to coninue to assert the data */
+	reg_tim.s.wait = 0;
+	/* Time to wait to complete the cycle. */
+	reg_tim.s.pause = pause;
+	/* How long to hold after a write to de-assert CE. */
+	reg_tim.s.wr_hld = trh;
+	/* How long to wait after a read to de-assert CE. */
+	reg_tim.s.rd_hld = trh;
+	/* How long write enable is asserted */
+	reg_tim.s.we = t2;
+	/* How long read enable is asserted */
+	reg_tim.s.oe = t2;
+	/* Time after CE that read/write starts */
+	reg_tim.s.ce = ns_to_tim_reg(2, 5);
+	/* Time before CE that address is valid */
+	reg_tim.s.adr = 0;
+
+	/* Program the bootbus region timing for the data port chip select. */
+	cvmx_write_csr(CVMX_MIO_BOOT_REG_TIMX(cs), reg_tim.u64);
+	if (ocd->dma_engine >= 0)
+		/* True IDE mode, program both chip selects.  */
+		cvmx_write_csr(CVMX_MIO_BOOT_REG_TIMX(cs + 1), reg_tim.u64);
+}
+
+static void octeon_cf_set_dmamode(struct ata_port *ap, struct ata_device *dev)
+{
+	struct octeon_cf_data *ocd = dev->link->ap->dev->platform_data;
+	cvmx_mio_boot_dma_timx_t dma_tim;
+	unsigned int oe_a;
+	unsigned int oe_n;
+	unsigned int dma_ackh;
+	unsigned int dma_arq;
+	unsigned int pause;
+	unsigned int T0, Tkr, Td;
+	unsigned int tim_mult;
+
+	const struct ata_timing *timing;
+
+	timing = ata_timing_find_mode(dev->dma_mode);
+	T0	= timing->cycle;
+	Td	= timing->active;
+	Tkr	= timing->recover;
+	dma_ackh = timing->dmack_hold;
+
+	dma_tim.u64 = 0;
+	/* dma_tim.s.tim_mult = 0 --> 4x */
+	tim_mult = 4;
+
+	/* not spec'ed, value in eclocks, not affected by tim_mult */
+	dma_arq = 8;
+	pause = 25 - dma_arq * 1000 /
+		(octeon_get_clock_rate() / 1000000); /* Tz */
+
+	oe_a = Td;
+	/* Tkr from cf spec, lengthened to meet T0 */
+	oe_n = max(T0 - oe_a, Tkr);
+
+	dma_tim.s.dmack_pi = 1;
+
+	dma_tim.s.oe_n = ns_to_tim_reg(tim_mult, oe_n);
+	dma_tim.s.oe_a = ns_to_tim_reg(tim_mult, oe_a);
+
+	/*
+	 * This is tI, C.F. spec. says 0, but Sony CF card requires
+	 * more, we use 20 nS.
+	 */
+	dma_tim.s.dmack_s = ns_to_tim_reg(tim_mult, 20);;
+	dma_tim.s.dmack_h = ns_to_tim_reg(tim_mult, dma_ackh);
+
+	dma_tim.s.dmarq = dma_arq;
+	dma_tim.s.pause = ns_to_tim_reg(tim_mult, pause);
+
+	dma_tim.s.rd_dly = 0;	/* Sample right on edge */
+
+	/*  writes only */
+	dma_tim.s.we_n = ns_to_tim_reg(tim_mult, oe_n);
+	dma_tim.s.we_a = ns_to_tim_reg(tim_mult, oe_a);
+
+	pr_debug("ns to ticks (mult %d) of %d is: %d\n", tim_mult, 60,
+		 ns_to_tim_reg(tim_mult, 60));
+	pr_debug("oe_n: %d, oe_a: %d, dmack_s: %d, dmack_h: "
+		 "%d, dmarq: %d, pause: %d\n",
+		 dma_tim.s.oe_n, dma_tim.s.oe_a, dma_tim.s.dmack_s,
+		 dma_tim.s.dmack_h, dma_tim.s.dmarq, dma_tim.s.pause);
+
+	cvmx_write_csr(CVMX_MIO_BOOT_DMA_TIMX(ocd->dma_engine),
+		       dma_tim.u64);
+
+}
+
+/**
+ * Handle an 8 bit I/O request.
+ *
+ * @dev:        Device to access
+ * @buffer:     Data buffer
+ * @buflen:     Length of the buffer.
+ * @rw:         True to write.
+ */
+static unsigned int octeon_cf_data_xfer8(struct ata_device *dev,
+					 unsigned char *buffer,
+					 unsigned int buflen,
+					 int rw)
+{
+	struct ata_port *ap		= dev->link->ap;
+	void __iomem *data_addr		= ap->ioaddr.data_addr;
+	unsigned long words;
+	int count;
+
+	words = buflen;
+	if (rw) {
+		count = 16;
+		while (words--) {
+			iowrite8(*buffer, data_addr);
+			buffer++;
+			/*
+			 * Every 16 writes do a read so the bootbus
+			 * FIFO doesn't fill up.
+			 */
+			if (--count == 0) {
+				ioread8(ap->ioaddr.altstatus_addr);
+				count = 16;
+			}
+		}
+	} else {
+		ioread8_rep(data_addr, buffer, words);
+	}
+	return buflen;
+}
+
+/**
+ * Handle a 16 bit I/O request.
+ *
+ * @dev:        Device to access
+ * @buffer:     Data buffer
+ * @buflen:     Length of the buffer.
+ * @rw:         True to write.
+ */
+static unsigned int octeon_cf_data_xfer16(struct ata_device *dev,
+					  unsigned char *buffer,
+					  unsigned int buflen,
+					  int rw)
+{
+	struct ata_port *ap		= dev->link->ap;
+	void __iomem *data_addr		= ap->ioaddr.data_addr;
+	unsigned long words;
+	int count;
+
+	words = buflen / 2;
+	if (rw) {
+		count = 16;
+		while (words--) {
+			iowrite16(*(uint16_t *)buffer, data_addr);
+			buffer += sizeof(uint16_t);
+			/*
+			 * Every 16 writes do a read so the bootbus
+			 * FIFO doesn't fill up.
+			 */
+			if (--count == 0) {
+				ioread8(ap->ioaddr.altstatus_addr);
+				count = 16;
+			}
+		}
+	} else {
+		while (words--) {
+			*(uint16_t *)buffer = ioread16(data_addr);
+			buffer += sizeof(uint16_t);
+		}
+	}
+	/* Transfer trailing 1 byte, if any. */
+	if (unlikely(buflen & 0x01)) {
+		__le16 align_buf[1] = { 0 };
+
+		if (rw == READ) {
+			align_buf[0] = cpu_to_le16(ioread16(data_addr));
+			memcpy(buffer, align_buf, 1);
+		} else {
+			memcpy(align_buf, buffer, 1);
+			iowrite16(le16_to_cpu(align_buf[0]), data_addr);
+		}
+		words++;
+	}
+	return buflen;
+}
+
+/**
+ * Read the taskfile for 16bit non-True IDE only.
+ */
+static void octeon_cf_tf_read16(struct ata_port *ap, struct ata_taskfile *tf)
+{
+	u16 blob;
+	/* The base of the registers is at ioaddr.data_addr. */
+	void __iomem *base = ap->ioaddr.data_addr;
+
+	blob = __raw_readw(base + 0xc);
+	tf->feature = blob >> 8;
+
+	blob = __raw_readw(base + 2);
+	tf->nsect = blob & 0xff;
+	tf->lbal = blob >> 8;
+
+	blob = __raw_readw(base + 4);
+	tf->lbam = blob & 0xff;
+	tf->lbah = blob >> 8;
+
+	blob = __raw_readw(base + 6);
+	tf->device = blob & 0xff;
+	tf->command = blob >> 8;
+
+	if (tf->flags & ATA_TFLAG_LBA48) {
+		if (likely(ap->ioaddr.ctl_addr)) {
+			iowrite8(tf->ctl | ATA_HOB, ap->ioaddr.ctl_addr);
+
+			blob = __raw_readw(base + 0xc);
+			tf->hob_feature = blob >> 8;
+
+			blob = __raw_readw(base + 2);
+			tf->hob_nsect = blob & 0xff;
+			tf->hob_lbal = blob >> 8;
+
+			blob = __raw_readw(base + 4);
+			tf->hob_lbam = blob & 0xff;
+			tf->hob_lbah = blob >> 8;
+
+			iowrite8(tf->ctl, ap->ioaddr.ctl_addr);
+			ap->last_ctl = tf->ctl;
+		} else {
+			WARN_ON(1);
+		}
+	}
+}
+
+static u8 octeon_cf_check_status16(struct ata_port *ap)
+{
+	u16 blob;
+	void __iomem *base = ap->ioaddr.data_addr;
+
+	blob = __raw_readw(base + 6);
+	return blob >> 8;
+}
+
+static int octeon_cf_softreset16(struct ata_link *link, unsigned int *classes,
+				 unsigned long deadline)
+{
+	struct ata_port *ap = link->ap;
+	void __iomem *base = ap->ioaddr.data_addr;
+	int rc;
+	u8 err;
+
+	DPRINTK("about to softreset\n");
+	__raw_writew(ap->ctl, base + 0xe);
+	udelay(20);
+	__raw_writew(ap->ctl | ATA_SRST, base + 0xe);
+	udelay(20);
+	__raw_writew(ap->ctl, base + 0xe);
+
+	rc = ata_sff_wait_after_reset(link, 1, deadline);
+	if (rc) {
+		ata_link_printk(link, KERN_ERR, "SRST failed (errno=%d)\n", rc);
+		return rc;
+	}
+
+	/* determine by signature whether we have ATA or ATAPI devices */
+	classes[0] = ata_sff_dev_classify(&link->device[0], 1, &err);
+	DPRINTK("EXIT, classes[0]=%u [1]=%u\n", classes[0], classes[1]);
+	return 0;
+}
+
+/**
+ * Load the taskfile for 16bit non-True IDE only.  The device_addr is
+ * not loaded, we do this as part of octeon_cf_exec_command16.
+ */
+static void octeon_cf_tf_load16(struct ata_port *ap,
+				const struct ata_taskfile *tf)
+{
+	unsigned int is_addr = tf->flags & ATA_TFLAG_ISADDR;
+	/* The base of the registers is at ioaddr.data_addr. */
+	void __iomem *base = ap->ioaddr.data_addr;
+
+	if (tf->ctl != ap->last_ctl) {
+		iowrite8(tf->ctl, ap->ioaddr.ctl_addr);
+		ap->last_ctl = tf->ctl;
+		ata_wait_idle(ap);
+	}
+	if (is_addr && (tf->flags & ATA_TFLAG_LBA48)) {
+		__raw_writew(tf->hob_feature << 8, base + 0xc);
+		__raw_writew(tf->hob_nsect | tf->hob_lbal << 8, base + 2);
+		__raw_writew(tf->hob_lbam | tf->hob_lbah << 8, base + 4);
+		VPRINTK("hob: feat 0x%X nsect 0x%X, lba 0x%X 0x%X 0x%X\n",
+			tf->hob_feature,
+			tf->hob_nsect,
+			tf->hob_lbal,
+			tf->hob_lbam,
+			tf->hob_lbah);
+	}
+	if (is_addr) {
+		__raw_writew(tf->feature << 8, base + 0xc);
+		__raw_writew(tf->nsect | tf->lbal << 8, base + 2);
+		__raw_writew(tf->lbam | tf->lbah << 8, base + 4);
+		VPRINTK("feat 0x%X nsect 0x%X, lba 0x%X 0x%X 0x%X\n",
+			tf->feature,
+			tf->nsect,
+			tf->lbal,
+			tf->lbam,
+			tf->lbah);
+	}
+	ata_wait_idle(ap);
+}
+
+
+static void octeon_cf_dev_select(struct ata_port *ap, unsigned int device)
+{
+/*  There is only one device, do nothing. */
+	return;
+}
+
+/*
+ * Issue ATA command to host controller.  The device_addr is also sent
+ * as it must be written in a combined write with the command.
+ */
+static void octeon_cf_exec_command16(struct ata_port *ap,
+				const struct ata_taskfile *tf)
+{
+	/* The base of the registers is at ioaddr.data_addr. */
+	void __iomem *base = ap->ioaddr.data_addr;
+	u16 blob;
+
+	if (tf->flags & ATA_TFLAG_DEVICE) {
+		VPRINTK("device 0x%X\n", tf->device);
+		blob = tf->device;
+	} else {
+		blob = 0;
+	}
+
+	DPRINTK("ata%u: cmd 0x%X\n", ap->print_id, tf->command);
+	blob |= (tf->command << 8);
+	__raw_writew(blob, base + 6);
+
+
+	ata_wait_idle(ap);
+}
+
+static u8 octeon_cf_irq_on(struct ata_port *ap)
+{
+	return 0;
+}
+
+static void octeon_cf_irq_clear(struct ata_port *ap)
+{
+	return;
+}
+
+static void octeon_cf_dma_setup(struct ata_queued_cmd *qc)
+{
+	struct ata_port *ap = qc->ap;
+	struct octeon_cf_port *cf_port;
+
+	cf_port = (struct octeon_cf_port *)ap->private_data;
+	DPRINTK("ENTER\n");
+	/* issue r/w command */
+	qc->cursg = qc->sg;
+	cf_port->dma_finished = 0;
+	ap->ops->sff_exec_command(ap, &qc->tf);
+	DPRINTK("EXIT\n");
+}
+
+/**
+ * Start a DMA transfer that was already setup
+ *
+ * @qc:     Information about the DMA
+ */
+static void octeon_cf_dma_start(struct ata_queued_cmd *qc)
+{
+	struct octeon_cf_data *ocd = qc->ap->dev->platform_data;
+	cvmx_mio_boot_dma_cfgx_t mio_boot_dma_cfg;
+	cvmx_mio_boot_dma_intx_t mio_boot_dma_int;
+	struct scatterlist *sg;
+
+	VPRINTK("%d scatterlists\n", qc->n_elem);
+
+	/* Get the scatter list entry we need to DMA into */
+	sg = qc->cursg;
+	BUG_ON(!sg);
+
+	/*
+	 * Clear the DMA complete status.
+	 */
+	mio_boot_dma_int.u64 = 0;
+	mio_boot_dma_int.s.done = 1;
+	cvmx_write_csr(CVMX_MIO_BOOT_DMA_INTX(ocd->dma_engine),
+		       mio_boot_dma_int.u64);
+
+	/* Enable the interrupt.  */
+	cvmx_write_csr(CVMX_MIO_BOOT_DMA_INT_ENX(ocd->dma_engine),
+		       mio_boot_dma_int.u64);
+
+	/* Set the direction of the DMA */
+	mio_boot_dma_cfg.u64 = 0;
+	mio_boot_dma_cfg.s.en = 1;
+	mio_boot_dma_cfg.s.rw = ((qc->tf.flags & ATA_TFLAG_WRITE) != 0);
+
+	/*
+	 * Don't stop the DMA if the device deasserts DMARQ. Many
+	 * compact flashes deassert DMARQ for a short time between
+	 * sectors. Instead of stopping and restarting the DMA, we'll
+	 * let the hardware do it. If the DMA is really stopped early
+	 * due to an error condition, a later timeout will force us to
+	 * stop.
+	 */
+	mio_boot_dma_cfg.s.clr = 0;
+
+	/* Size is specified in 16bit words and minus one notation */
+	mio_boot_dma_cfg.s.size = sg_dma_len(sg) / 2 - 1;
+
+	/* We need to swap the high and low bytes of every 16 bits */
+	mio_boot_dma_cfg.s.swap8 = 1;
+
+	mio_boot_dma_cfg.s.adr = sg_dma_address(sg);
+
+	VPRINTK("%s %d bytes address=%p\n",
+		(mio_boot_dma_cfg.s.rw) ? "write" : "read", sg->length,
+		(void *)(unsigned long)mio_boot_dma_cfg.s.adr);
+
+	cvmx_write_csr(CVMX_MIO_BOOT_DMA_CFGX(ocd->dma_engine),
+		       mio_boot_dma_cfg.u64);
+}
+
+/**
+ *
+ *	LOCKING:
+ *	spin_lock_irqsave(host lock)
+ *
+ */
+static unsigned int octeon_cf_dma_finished(struct ata_port *ap,
+					struct ata_queued_cmd *qc)
+{
+	struct ata_eh_info *ehi = &ap->link.eh_info;
+	struct octeon_cf_data *ocd = ap->dev->platform_data;
+	cvmx_mio_boot_dma_cfgx_t dma_cfg;
+	cvmx_mio_boot_dma_intx_t dma_int;
+	struct octeon_cf_port *cf_port;
+	u8 status;
+
+	VPRINTK("ata%u: protocol %d task_state %d\n",
+		ap->print_id, qc->tf.protocol, ap->hsm_task_state);
+
+
+	if (ap->hsm_task_state != HSM_ST_LAST)
+		return 0;
+
+	cf_port = (struct octeon_cf_port *)ap->private_data;
+
+	dma_cfg.u64 = cvmx_read_csr(CVMX_MIO_BOOT_DMA_CFGX(ocd->dma_engine));
+	if (dma_cfg.s.size != 0xfffff) {
+		/* Error, the transfer was not complete.  */
+		qc->err_mask |= AC_ERR_HOST_BUS;
+		ap->hsm_task_state = HSM_ST_ERR;
+	}
+
+	/* Stop and clear the dma engine.  */
+	dma_cfg.u64 = 0;
+	dma_cfg.s.size = -1;
+	cvmx_write_csr(CVMX_MIO_BOOT_DMA_CFGX(ocd->dma_engine), dma_cfg.u64);
+
+	/* Disable the interrupt.  */
+	dma_int.u64 = 0;
+	cvmx_write_csr(CVMX_MIO_BOOT_DMA_INT_ENX(ocd->dma_engine), dma_int.u64);
+
+	/* Clear the DMA complete status */
+	dma_int.s.done = 1;
+	cvmx_write_csr(CVMX_MIO_BOOT_DMA_INTX(ocd->dma_engine), dma_int.u64);
+
+	status = ap->ops->sff_check_status(ap);
+
+	ata_sff_hsm_move(ap, qc, status, 0);
+
+	if (unlikely(qc->err_mask) && (qc->tf.protocol == ATA_PROT_DMA))
+		ata_ehi_push_desc(ehi, "DMA stat 0x%x", status);
+
+	return 1;
+}
+
+/*
+ * Check if any queued commands have more DMAs, if so start the next
+ * transfer, else do end of transfer handling.
+ */
+static irqreturn_t octeon_cf_interrupt(int irq, void *dev_instance)
+{
+	struct ata_host *host = dev_instance;
+	struct octeon_cf_port *cf_port;
+	int i;
+	unsigned int handled = 0;
+	unsigned long flags;
+
+	spin_lock_irqsave(&host->lock, flags);
+
+	DPRINTK("ENTER\n");
+	for (i = 0; i < host->n_ports; i++) {
+		u8 status;
+		struct ata_port *ap;
+		struct ata_queued_cmd *qc;
+		cvmx_mio_boot_dma_intx_t dma_int;
+		cvmx_mio_boot_dma_cfgx_t dma_cfg;
+		struct octeon_cf_data *ocd;
+
+		ap = host->ports[i];
+		ocd = ap->dev->platform_data;
+		if (!ap || (ap->flags & ATA_FLAG_DISABLED))
+			continue;
+
+		ocd = ap->dev->platform_data;
+		cf_port = (struct octeon_cf_port *)ap->private_data;
+		dma_int.u64 =
+			cvmx_read_csr(CVMX_MIO_BOOT_DMA_INTX(ocd->dma_engine));
+		dma_cfg.u64 =
+			cvmx_read_csr(CVMX_MIO_BOOT_DMA_CFGX(ocd->dma_engine));
+
+		qc = ata_qc_from_tag(ap, ap->link.active_tag);
+
+		if (qc && (!(qc->tf.flags & ATA_TFLAG_POLLING)) &&
+		    (qc->flags & ATA_QCFLAG_ACTIVE)) {
+			if (dma_int.s.done && !dma_cfg.s.en) {
+				if (!sg_is_last(qc->cursg)) {
+					qc->cursg = sg_next(qc->cursg);
+					handled = 1;
+					octeon_cf_dma_start(qc);
+					continue;
+				} else {
+					cf_port->dma_finished = 1;
+				}
+			}
+			if (!cf_port->dma_finished)
+				continue;
+			status = ioread8(ap->ioaddr.altstatus_addr);
+			if (status & (ATA_BUSY | ATA_DRQ)) {
+				/*
+				 * We are busy, try to handle it
+				 * later.  This is the DMA finished
+				 * interrupt, and it could take a
+				 * little while for the card to be
+				 * ready for more commands.
+				 */
+				/* Clear DMA irq. */
+				dma_int.u64 = 0;
+				dma_int.s.done = 1;
+				cvmx_write_csr(
+					CVMX_MIO_BOOT_DMA_INTX(ocd->dma_engine),
+					dma_int.u64);
+
+				queue_delayed_work(cf_port->wq,
+						   &cf_port->delayed_finish, 1);
+				handled = 1;
+			} else {
+				handled |= octeon_cf_dma_finished(ap, qc);
+			}
+		}
+	}
+	spin_unlock_irqrestore(&host->lock, flags);
+	DPRINTK("EXIT\n");
+	return IRQ_RETVAL(handled);
+}
+
+static void octeon_cf_delayed_finish(struct work_struct *work)
+{
+	struct octeon_cf_port *cf_port = container_of(work,
+						      struct octeon_cf_port,
+						      delayed_finish.work);
+	struct ata_port *ap = cf_port->ap;
+	struct ata_host *host = ap->host;
+	struct ata_queued_cmd *qc;
+	unsigned long flags;
+	u8 status;
+
+	spin_lock_irqsave(&host->lock, flags);
+
+	/*
+	 * If the port is not waiting for completion, it must have
+	 * handled it previously.  The hsm_task_state is
+	 * protected by host->lock.
+	 */
+	if (ap->hsm_task_state != HSM_ST_LAST || !cf_port->dma_finished)
+		goto out;
+
+	status = ioread8(ap->ioaddr.altstatus_addr);
+	if (status & (ATA_BUSY | ATA_DRQ)) {
+		/* Still busy, try again. */
+		queue_delayed_work(cf_port->wq,
+				   &cf_port->delayed_finish, 1);
+		goto out;
+	}
+	qc = ata_qc_from_tag(ap, ap->link.active_tag);
+	if (qc && (!(qc->tf.flags & ATA_TFLAG_POLLING)) &&
+	    (qc->flags & ATA_QCFLAG_ACTIVE))
+		octeon_cf_dma_finished(ap, qc);
+out:
+	spin_unlock_irqrestore(&host->lock, flags);
+}
+
+static void octeon_cf_dev_config(struct ata_device *dev)
+{
+	/*
+	 * A maximum of 2^20 - 1 16 bit transfers are possible with
+	 * the bootbus DMA.  So we need to throttle max_sectors to
+	 * (2^12 - 1 == 4095) to assure that this can never happen.
+	 */
+	dev->max_sectors = min(dev->max_sectors, 4095U);
+}
+
+/*
+ * Trap if driver tries to do standard bmdma commands.  They are not
+ * supported.
+ */
+static void unreachable_qc(struct ata_queued_cmd *qc)
+{
+	BUG();
+}
+
+static u8 unreachable_port(struct ata_port *ap)
+{
+	BUG();
+	return 0;
+}
+
+/*
+ * We don't do ATAPI DMA so return 0.
+ */
+static int octeon_cf_check_atapi_dma(struct ata_queued_cmd *qc)
+{
+	return 0;
+}
+
+static unsigned int octeon_cf_qc_issue(struct ata_queued_cmd *qc)
+{
+	struct ata_port *ap = qc->ap;
+
+	switch (qc->tf.protocol) {
+	case ATA_PROT_DMA:
+		WARN_ON(qc->tf.flags & ATA_TFLAG_POLLING);
+
+		ap->ops->sff_tf_load(ap, &qc->tf);  /* load tf registers */
+		octeon_cf_dma_setup(qc);	    /* set up dma */
+		octeon_cf_dma_start(qc);	    /* initiate dma */
+		ap->hsm_task_state = HSM_ST_LAST;
+		break;
+
+	case ATAPI_PROT_DMA:
+		dev_err(ap->dev, "Error, ATAPI not supported\n");
+		BUG();
+
+	default:
+		return ata_sff_qc_issue(qc);
+	}
+
+	return 0;
+}
+
+static struct ata_port_operations octeon_cf_ops = {
+	.inherits		= &ata_sff_port_ops,
+	.check_atapi_dma	= octeon_cf_check_atapi_dma,
+	.qc_prep		= ata_noop_qc_prep,
+	.qc_issue		= octeon_cf_qc_issue,
+	.sff_dev_select		= octeon_cf_dev_select,
+	.sff_irq_on		= octeon_cf_irq_on,
+	.sff_irq_clear		= octeon_cf_irq_clear,
+	.bmdma_setup		= unreachable_qc,
+	.bmdma_start		= unreachable_qc,
+	.bmdma_stop		= unreachable_qc,
+	.bmdma_status		= unreachable_port,
+	.cable_detect		= ata_cable_40wire,
+	.set_piomode		= octeon_cf_set_piomode,
+	.set_dmamode		= octeon_cf_set_dmamode,
+	.dev_config		= octeon_cf_dev_config,
+};
+
+static int __devinit octeon_cf_probe(struct platform_device *pdev)
+{
+	struct resource *res_cs0, *res_cs1;
+
+	void __iomem *cs0;
+	void __iomem *cs1 = NULL;
+	struct ata_host *host;
+	struct ata_port *ap;
+	struct octeon_cf_data *ocd;
+	int irq = 0;
+	irq_handler_t irq_handler = NULL;
+	void __iomem *base;
+	struct octeon_cf_port *cf_port;
+
+	res_cs0 = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+
+	if (!res_cs0)
+		return -EINVAL;
+
+	ocd = pdev->dev.platform_data;
+
+	cs0 = devm_ioremap_nocache(&pdev->dev, res_cs0->start,
+				   res_cs0->end - res_cs0->start + 1);
+
+	if (!cs0)
+		return -ENOMEM;
+
+	/* Determine from availability of DMA if True IDE mode or not */
+	if (ocd->dma_engine >= 0) {
+		res_cs1 = platform_get_resource(pdev, IORESOURCE_MEM, 1);
+		if (!res_cs1)
+			return -EINVAL;
+
+		cs1 = devm_ioremap_nocache(&pdev->dev, res_cs1->start,
+					   res_cs0->end - res_cs1->start + 1);
+
+		if (!cs1)
+			return -ENOMEM;
+	}
+
+	cf_port = kzalloc(sizeof(*cf_port), GFP_KERNEL);
+	if (!cf_port)
+		return -ENOMEM;
+
+	/* allocate host */
+	host = ata_host_alloc(&pdev->dev, 1);
+	if (!host)
+		goto free_cf_port;
+
+	ap = host->ports[0];
+	ap->private_data = cf_port;
+	cf_port->ap = ap;
+	ap->ops = &octeon_cf_ops;
+	ap->pio_mask = 0x7f; /* Support PIO 0-6 */
+	ap->flags |= ATA_FLAG_MMIO | ATA_FLAG_NO_LEGACY
+		  | ATA_FLAG_NO_ATAPI | ATA_FLAG_PIO_POLLING;
+
+	base = cs0 + ocd->base_region_bias;
+	if (!ocd->is16bit) {
+		ap->ioaddr.cmd_addr	= base;
+		ata_sff_std_ports(&ap->ioaddr);
+
+		ap->ioaddr.altstatus_addr = base + 0xe;
+		ap->ioaddr.ctl_addr	= base + 0xe;
+		octeon_cf_ops.sff_data_xfer = octeon_cf_data_xfer8;
+	} else if (cs1) {
+		/* Presence of cs1 indicates True IDE mode.  */
+		ap->ioaddr.cmd_addr	= base + (ATA_REG_CMD << 1) + 1;
+		ap->ioaddr.data_addr	= base + (ATA_REG_DATA << 1);
+		ap->ioaddr.error_addr	= base + (ATA_REG_ERR << 1) + 1;
+		ap->ioaddr.feature_addr	= base + (ATA_REG_FEATURE << 1) + 1;
+		ap->ioaddr.nsect_addr	= base + (ATA_REG_NSECT << 1) + 1;
+		ap->ioaddr.lbal_addr	= base + (ATA_REG_LBAL << 1) + 1;
+		ap->ioaddr.lbam_addr	= base + (ATA_REG_LBAM << 1) + 1;
+		ap->ioaddr.lbah_addr	= base + (ATA_REG_LBAH << 1) + 1;
+		ap->ioaddr.device_addr	= base + (ATA_REG_DEVICE << 1) + 1;
+		ap->ioaddr.status_addr	= base + (ATA_REG_STATUS << 1) + 1;
+		ap->ioaddr.command_addr	= base + (ATA_REG_CMD << 1) + 1;
+		ap->ioaddr.altstatus_addr = cs1 + (6 << 1) + 1;
+		ap->ioaddr.ctl_addr	= cs1 + (6 << 1) + 1;
+		octeon_cf_ops.sff_data_xfer = octeon_cf_data_xfer16;
+
+		ap->mwdma_mask	= 0x1f; /* Support MWDMA 0-4 */
+		irq = platform_get_irq(pdev, 0);
+		irq_handler = octeon_cf_interrupt;
+
+		/* True IDE mode needs delayed work to poll for not-busy.  */
+		cf_port->wq = create_singlethread_workqueue(DRV_NAME);
+		if (!cf_port->wq)
+			goto free_cf_port;
+		INIT_DELAYED_WORK(&cf_port->delayed_finish,
+				  octeon_cf_delayed_finish);
+
+	} else {
+		/* 16 bit but not True IDE */
+		octeon_cf_ops.sff_data_xfer	= octeon_cf_data_xfer16;
+		octeon_cf_ops.softreset		= octeon_cf_softreset16;
+		octeon_cf_ops.sff_check_status	= octeon_cf_check_status16;
+		octeon_cf_ops.sff_tf_read	= octeon_cf_tf_read16;
+		octeon_cf_ops.sff_tf_load	= octeon_cf_tf_load16;
+		octeon_cf_ops.sff_exec_command	= octeon_cf_exec_command16;
+
+		ap->ioaddr.data_addr	= base + ATA_REG_DATA;
+		ap->ioaddr.nsect_addr	= base + ATA_REG_NSECT;
+		ap->ioaddr.lbal_addr	= base + ATA_REG_LBAL;
+		ap->ioaddr.ctl_addr	= base + 0xe;
+		ap->ioaddr.altstatus_addr = base + 0xe;
+	}
+
+	ata_port_desc(ap, "cmd %p ctl %p", base, ap->ioaddr.ctl_addr);
+
+
+	dev_info(&pdev->dev, "version " DRV_VERSION" %d bit%s.\n",
+		 (ocd->is16bit) ? 16 : 8,
+		 (cs1) ? ", True IDE" : "");
+
+
+	return ata_host_activate(host, irq, irq_handler, 0, &octeon_cf_sht);
+
+free_cf_port:
+	kfree(cf_port);
+	return -ENOMEM;
+}
+
+static struct platform_driver octeon_cf_driver = {
+	.probe		= octeon_cf_probe,
+	.driver		= {
+		.name	= DRV_NAME,
+		.owner	= THIS_MODULE,
+	},
+};
+
+static int __init octeon_cf_init(void)
+{
+	return platform_driver_register(&octeon_cf_driver);
+}
+
+
+MODULE_AUTHOR("David Daney <ddaney@caviumnetworks.com>");
+MODULE_DESCRIPTION("low-level driver for Cavium OCTEON Compact Flash PATA");
+MODULE_LICENSE("GPL");
+MODULE_VERSION(DRV_VERSION);
+MODULE_ALIAS("platform:" DRV_NAME);
+
+module_init(octeon_cf_init);
diff --git a/drivers/mtd/maps/Kconfig b/drivers/mtd/maps/Kconfig
index 5c79fc2..a64c293 100644
--- a/drivers/mtd/maps/Kconfig
+++ b/drivers/mtd/maps/Kconfig
@@ -580,5 +580,12 @@ config MTD_ATMEL49XX_FLASH
           flash is split into 2 partitions which are accessed as separate
           MTD devices.
 
+config MTD_CAVIUM_FLASH
+	tristate "Flash chip on Cavium OCTEON Reference Boards"
+	depends on  CAVIUM_OCTEON_REFERENCE_BOARD && MTD
+	help
+           This provides a driver for the on-board boot flash on
+           Cavium OCTEON Reference Boards.
+
 endmenu
 
diff --git a/drivers/mtd/maps/Makefile b/drivers/mtd/maps/Makefile
index 9e17291..4980af8 100644
--- a/drivers/mtd/maps/Makefile
+++ b/drivers/mtd/maps/Makefile
@@ -65,4 +65,5 @@ obj-$(CONFIG_MTD_PLATRAM)	+= plat-ram.o
 obj-$(CONFIG_MTD_OMAP_NOR)	+= omap_nor.o
 obj-$(CONFIG_MTD_INTEL_VR_NOR)	+= intel_vr_nor.o
 obj-$(CONFIG_MTD_BFIN_ASYNC)	+= bfin-async-flash.o
+obj-$(CONFIG_MTD_CAVIUM_FLASH)	+= cavium_flash.o
 obj-$(CONFIG_MTD_ATMEL49XX_FLASH)   += atmel49xx_flash.o atmel49xx_uflash.o
diff --git a/drivers/mtd/maps/cavium_flash.c b/drivers/mtd/maps/cavium_flash.c
new file mode 100644
index 0000000..a588122
--- /dev/null
+++ b/drivers/mtd/maps/cavium_flash.c
@@ -0,0 +1,80 @@
+/*
+ *   Octeon Bootbus flash setup
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2007 Cavium Networks
+ */
+#include <linux/kernel.h>
+#include <linux/mtd/mtd.h>
+#include <linux/mtd/map.h>
+#include <linux/mtd/partitions.h>
+
+#include "octeon-hal-read-write.h"
+
+static struct map_info flash_map;
+static struct mtd_info *mymtd;
+#ifdef CONFIG_MTD_PARTITIONS
+static int nr_parts;
+static struct mtd_partition *parts;
+static const char *part_probe_types[] = {
+#ifdef CONFIG_MTD_CMDLINE_PARTS
+	"cmdlinepart",
+#endif
+#ifdef CONFIG_MTD_REDBOOT_PARTS
+	"RedBoot",
+#endif
+	NULL
+};
+#endif
+
+/**
+ * Module/ driver initialization.
+ *
+ * @return Zero on success
+ */
+static int __init flash_init(void)
+{
+	/* Read the bootbus region 0 setup to determine where the base of flash
+	   is set for */
+	cvmx_mio_boot_reg_cfgx_t region_cfg;
+	region_cfg.u64 = cvmx_read_csr(CVMX_MIO_BOOT_REG_CFG0);
+	if (region_cfg.s.en) {
+		/* The bootloader always takes the flash and sets its address
+		   so the entire flash fits below 0x1fc00000. This way the
+		   flash aliases to 0x1fc00000 for booting. Software can access
+		   the full flash at the true address, while core boot can
+		   access 4MB */
+		flash_map.name = "phys_mapped_flash";	/* Use this name so old
+							   part lines work */
+		flash_map.phys = region_cfg.s.base << 16;
+		flash_map.size = 0x1fc00000 - flash_map.phys;
+		flash_map.bankwidth = 1;
+		flash_map.virt = ioremap(flash_map.phys, flash_map.size);
+		pr_notice("Bootbus flash: Setting flash for %luMB flash at "
+			  "0x%08lx\n", flash_map.size >> 20, flash_map.phys);
+		simple_map_init(&flash_map);
+		mymtd = do_map_probe("cfi_probe", &flash_map);
+		if (mymtd) {
+			mymtd->owner = THIS_MODULE;
+
+#ifdef CONFIG_MTD_PARTITIONS
+			nr_parts =
+				parse_mtd_partitions(mymtd, part_probe_types,
+						     &parts, 0);
+			if (nr_parts > 0)
+				add_mtd_partitions(mymtd, parts, nr_parts);
+			else
+				add_mtd_device(mymtd);
+#else
+			add_mtd_device(mymtd);
+#endif
+		} else
+			pr_err("Failed to register MTD device for flash\n");
+	}
+	return 0;
+}
+
+late_initcall(flash_init);
diff --git a/include/linux/mtd/nand.h b/include/linux/mtd/nand.h
index 81774e5..a54a7c9 100644
--- a/include/linux/mtd/nand.h
+++ b/include/linux/mtd/nand.h
@@ -178,8 +178,13 @@ typedef enum {
 #define NAND_HAS_CACHEPROG(chip) ((chip->options & NAND_CACHEPRG))
 #define NAND_HAS_COPYBACK(chip) ((chip->options & NAND_COPYBACK))
 /* Large page NAND with SOFT_ECC should support subpage reads */
+#ifdef CONFIG_CPU_CAVIUM_OCTEON
+/* Subpage is not supported in NAND flash used by Octeon. */
+#define NAND_SUBPAGE_READ(chip) 0
+#else
 #define NAND_SUBPAGE_READ(chip) ((chip->ecc.mode == NAND_ECC_SOFT) \
 					&& (chip->page_shift > 9))
+#endif
 
 /* Mask to zero out the chip options, which come from the id table */
 #define NAND_CHIPOPTIONS_MSK	(0x0000ffff & ~NAND_NO_AUTOINCR)
-- 
1.5.5.1

