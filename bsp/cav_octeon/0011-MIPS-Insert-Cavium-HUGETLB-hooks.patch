From c7a30c1811bf4361c5e95e0595dea740f8f9ec78 Mon Sep 17 00:00:00 2001
From: Phil Staub <Phil.Staub@windriver.com>
Date: Thu, 24 Sep 2009 17:35:03 -0700
Subject: [PATCH] MIPS: Insert Cavium HUGETLB hooks

Add support for HUGETLBFS.

The current state of these files reflects an ongoing development
process, and as such implies an extensive modification history that is
not cited in detail here. It is worth noting, however, that this
collection represents the work of the following people who were
involved in that history:

Tomaso Paoletti <tpaoletti@caviumnetworks.com>
David Daney <ddaney@caviumnetworks.com>
Paul Gortmaker <Paul.Gortmaker@windriver.com>

Signed-off-by: Phil Staub <Phil.Staub@windriver.com>
---
 arch/mips/Kconfig               |   14 +-
 arch/mips/mm/Makefile           |    1 +
 arch/mips/mm/hugetlbpage.c      |  119 ++++++
 arch/mips/mm/tlb-r4k.c          |   56 +++-
 arch/mips/mm/tlbex.c            |  764 +++++++++++++++++++++++++++++++++++++--
 arch/mips/mm/uasm.h             |   34 ++
 fs/Kconfig                      |    2 +-
 include/asm-mips/hugetlb.h      |  107 ++++++
 include/asm-mips/mipsregs.h     |   70 ++++
 include/asm-mips/page.h         |   14 +
 include/asm-mips/pgtable-bits.h |    1 +
 include/asm-mips/pgtable.h      |   48 +++-
 include/asm-mips/sparsemem.h    |    5 +
 include/asm-mips/thread_info.h  |    9 +
 14 files changed, 1198 insertions(+), 46 deletions(-)
 create mode 100644 arch/mips/mm/hugetlbpage.c
 create mode 100644 include/asm-mips/hugetlb.h

diff --git a/arch/mips/Kconfig b/arch/mips/Kconfig
index f1d3aea..c7b8071 100644
--- a/arch/mips/Kconfig
+++ b/arch/mips/Kconfig
@@ -1420,11 +1420,11 @@ config PAGE_SIZE_4KB
 
 config PAGE_SIZE_8KB
 	bool "8kB"
-	depends on EXPERIMENTAL && CPU_R8000
+	depends on EXPERIMENTAL && (CPU_R8000 || CPU_CAVIUM_OCTEON)
 	help
 	  Using 8kB page size will result in higher performance kernel at
 	  the price of higher memory consumption.  This option is available
-	  only on the R8000 processor.  Not that at the time of this writing
+	  only on the R8000 & Octeon processor.  Not that at the time of this writing
 	  this option is still high experimental; there are also issues with
 	  compatibility of user applications.
 
@@ -1437,6 +1437,16 @@ config PAGE_SIZE_16KB
 	  all non-R3000 family processors.  Note that you will need a suitable
 	  Linux distribution to support this.
 
+config PAGE_SIZE_32KB
+	bool "32kB"
+	depends on EXPERIMENTAL && CPU_CAVIUM_OCTEON
+	help
+	  Using 32kB page size will result in higher performance kernel at
+	  the price of higher memory consumption.  This option is available on
+	  all Octeon processors.  Not that at the time of this writing this
+	  option is still high experimental; there are also issues with
+	  compatibility of user applications.
+
 config PAGE_SIZE_64KB
 	bool "64kB"
 	depends on EXPERIMENTAL && !CPU_R3000 && !CPU_TX39XX
diff --git a/arch/mips/mm/Makefile b/arch/mips/mm/Makefile
index 15b54da..8a8116b 100644
--- a/arch/mips/mm/Makefile
+++ b/arch/mips/mm/Makefile
@@ -24,6 +24,7 @@ endif
 obj-$(CONFIG_32BIT)		+= ioremap.o pgtable-32.o
 obj-$(CONFIG_64BIT)		+= pgtable-64.o
 obj-$(CONFIG_HIGHMEM)		+= highmem.o
+obj-$(CONFIG_HUGETLB_PAGE)	+= hugetlbpage.o
 
 obj-$(CONFIG_CPU_LOONGSON2)	+= c-r4k.o cex-gen.o tlb-r4k.o
 obj-$(CONFIG_CPU_MIPS32)	+= c-r4k.o cex-gen.o tlb-r4k.o
diff --git a/arch/mips/mm/hugetlbpage.c b/arch/mips/mm/hugetlbpage.c
new file mode 100644
index 0000000..6544d15
--- /dev/null
+++ b/arch/mips/mm/hugetlbpage.c
@@ -0,0 +1,119 @@
+/*
+ * MIPS Huge TLB Page Support for Kernel.
+ *
+ * Copyright (C) 2002, Rohit Seth <rohit.seth@intel.com>
+ * Copyright 2005, Embedded Alley Solutions, Inc.
+ * Matt Porter <mporter@embeddedalley.com>
+ */
+
+#include <linux/init.h>
+#include <linux/fs.h>
+#include <linux/mm.h>
+#include <linux/hugetlb.h>
+#include <linux/pagemap.h>
+#include <linux/smp_lock.h>
+#include <linux/slab.h>
+#include <linux/err.h>
+#include <linux/sysctl.h>
+#include <asm/mman.h>
+#include <asm/tlb.h>
+#include <asm/tlbflush.h>
+
+pte_t *huge_pte_alloc(struct mm_struct *mm, unsigned long addr,
+		      unsigned long sz)
+{
+	pgd_t *pgd;
+	pud_t *pud;
+	pte_t *pte = NULL;
+
+
+	pgd = pgd_offset(mm, addr);
+	pud = pud_alloc(mm, pgd, addr);
+	if (pud)
+		pte = (pte_t *)pmd_alloc(mm, pud, addr);
+
+	return pte;
+}
+
+pte_t *huge_pte_offset(struct mm_struct *mm, unsigned long addr)
+{
+	pgd_t *pgd;
+	pud_t *pud;
+	pmd_t *pmd = NULL;
+
+	pgd = pgd_offset(mm, addr);
+	if (pgd_present(*pgd)) {
+		pud = pud_offset(pgd, addr);
+		if (pud_present(*pud))
+			pmd = pmd_offset(pud, addr);
+	}
+	return (pte_t *) pmd;
+}
+
+/*
+ * unmap huge page backed by shared pte.
+ *
+ * Hugetlb pte page is ref counted at the time of mapping.  If pte is shared
+ * indicated by page_count > 1, unmap is achieved by clearing pud and
+ * decrementing the ref count. If count == 1, the pte page is not shared.
+ *
+ * called with vma->vm_mm->page_table_lock held.
+ *
+ * returns: 1 successfully unmapped a shared pte page
+ *	    0 the underlying pte page is not shared, or it is the last user
+ */
+int huge_pmd_unshare(struct mm_struct *mm, unsigned long *addr, pte_t *ptep)
+{
+	pgd_t *pgd = pgd_offset(mm, *addr);
+	pud_t *pud = pud_offset(pgd, *addr);
+
+	BUG_ON(page_count(virt_to_page(ptep)) == 0);
+	if (page_count(virt_to_page(ptep)) == 1)
+		return 0;
+
+	pud_clear(pud);
+	put_page(virt_to_page(ptep));
+	*addr = ALIGN(*addr, HPAGE_SIZE * PTRS_PER_PTE) - HPAGE_SIZE;
+	return 1;
+}
+
+/*
+ * This function checks for proper alignment of input addr and len parameters.
+ */
+int is_aligned_hugepage_range(unsigned long addr, unsigned long len)
+{
+	if (len & ~HPAGE_MASK)
+		return -EINVAL;
+	if (addr & ~HPAGE_MASK)
+		return -EINVAL;
+	return 0;
+}
+
+struct page *
+follow_huge_addr(struct mm_struct *mm, unsigned long address, int write)
+{
+	return ERR_PTR(-EINVAL);
+}
+
+int pmd_huge(pmd_t pmd)
+{
+	return (pmd_val(pmd) & _PAGE_HUGE) != 0;
+}
+
+int pud_huge(pud_t pud)
+{
+	return (pud_val(pud) & _PAGE_HUGE) != 0;
+}
+
+struct page *
+follow_huge_pmd(struct mm_struct *mm, unsigned long address,
+		pmd_t *pmd, int write)
+{
+	struct page *page;
+
+	page = pte_page(*(pte_t *)pmd);
+	if (page)
+		page += ((address & ~HPAGE_MASK) >> PAGE_SHIFT);
+	return page;
+}
+
diff --git a/arch/mips/mm/tlb-r4k.c b/arch/mips/mm/tlb-r4k.c
index 892be42..9128d69 100644
--- a/arch/mips/mm/tlb-r4k.c
+++ b/arch/mips/mm/tlb-r4k.c
@@ -12,6 +12,10 @@
 #include <linux/sched.h>
 #include <linux/mm.h>
 
+#ifdef CONFIG_HUGETLB_PAGE
+#include <linux/hugetlb.h>
+#endif
+
 #include <asm/cpu.h>
 #include <asm/bootinfo.h>
 #include <asm/mmu_context.h>
@@ -295,21 +299,43 @@ void __update_tlb(struct vm_area_struct * vma, unsigned long address, pte_t pte)
 	pudp = pud_offset(pgdp, address);
 	pmdp = pmd_offset(pudp, address);
 	idx = read_c0_index();
-	ptep = pte_offset_map(pmdp, address);
+
+#ifdef CONFIG_HUGETLB_PAGE
+	/* this could be a huge page  */
+	if (pmd_huge(*pmdp)) {
+		uint64_t lo;
+		write_c0_pagemask(PM_HUGE_MASK);
+		ptep = (pte_t *)pmdp;
+		lo = pte_to_entrylo(pte_val(*ptep));
+		write_c0_entrylo0(lo);
+		write_c0_entrylo1(lo + (HPAGE_SIZE >> 7));
+
+		mtc0_tlbw_hazard();
+		if (idx < 0)
+			tlb_write_random();
+		else
+			tlb_write_indexed();
+		write_c0_pagemask(PM_DEFAULT_MASK);
+	} else
+#endif
+	{
+		ptep = pte_offset_map(pmdp, address);
 
 #if defined(CONFIG_64BIT_PHYS_ADDR) && defined(CONFIG_CPU_MIPS32)
-	write_c0_entrylo0(ptep->pte_high);
-	ptep++;
-	write_c0_entrylo1(ptep->pte_high);
+		write_c0_entrylo0(ptep->pte_high);
+		ptep++;
+		write_c0_entrylo1(ptep->pte_high);
 #else
-	write_c0_entrylo0(pte_val(*ptep++) >> 6);
-	write_c0_entrylo1(pte_val(*ptep) >> 6);
+		write_c0_entrylo0(pte_to_entrylo(pte_val(*ptep++)));
+		write_c0_entrylo1(pte_to_entrylo(pte_val(*ptep)));
 #endif
-	mtc0_tlbw_hazard();
-	if (idx < 0)
-		tlb_write_random();
-	else
-		tlb_write_indexed();
+		mtc0_tlbw_hazard();
+		if (idx < 0)
+			tlb_write_random();
+		else
+			tlb_write_indexed();
+	}
+
 	tlbw_use_hazard();
 	FLUSH_ITLB_VM(vma);
 	EXIT_CRITICAL(flags);
@@ -337,8 +363,8 @@ static void r4k_update_mmu_cache_hwbug(struct vm_area_struct * vma,
 	pmdp = pmd_offset(pgdp, address);
 	idx = read_c0_index();
 	ptep = pte_offset_map(pmdp, address);
-	write_c0_entrylo0(pte_val(*ptep++) >> 6);
-	write_c0_entrylo1(pte_val(*ptep) >> 6);
+	write_c0_entrylo0(pte_to_entrylo(pte_val(*ptep++)));
+	write_c0_entrylo1(pte_to_entrylo(pte_val(*ptep)));
 	mtc0_tlbw_hazard();
 	if (idx < 0)
 		tlb_write_random();
@@ -480,6 +506,10 @@ void __cpuinit tlb_init(void)
 	    current_cpu_type() == CPU_R12000 ||
 	    current_cpu_type() == CPU_R14000)
 		write_c0_framemask(0);
+#ifdef CONFIG_USE_RI_XI_PAGE_BITS
+	/* Enable the no read, no exec bits, and enable large virtual address */
+	write_c0_pagegrain(7<<29);
+#endif
 	temp_tlb_entry = current_cpu_data.tlbsize - 1;
 
         /* From this point on the ARC firmware is dead.  */
diff --git a/arch/mips/mm/tlbex.c b/arch/mips/mm/tlbex.c
index 51a8116..4340c12 100644
--- a/arch/mips/mm/tlbex.c
+++ b/arch/mips/mm/tlbex.c
@@ -29,6 +29,35 @@
 
 #include "uasm.h"
 
+#if defined(CONFIG_CPU_CAVIUM_OCTEON) && defined(CONFIG_TEMPORARY_SCRATCHPAD_FOR_KERNEL) && defined(CONFIG_SMP) && defined(CONFIG_64BIT)
+#if !defined(CONFIG_MIPS_MT_SMTC) && CONFIG_TEMPORARY_SCRATCHPAD_FOR_KERNEL
+#define OCTEON_FAST_PATH_TLB_REFILL_HANDLER
+#endif
+#endif
+
+#ifdef OCTEON_FAST_PATH_TLB_REFILL_HANDLER
+#define USER_REGISTER_TO_USE 1 // $1 saved, used, restored during refill handler
+#if (defined(MODULE_START) && (MODULE_START & 0x000000FFFFFFFFFFULL)) || (VMALLOC_START & 0x000000FFFFFFFFFFULL)
+#define OCTEON_FAST_PATH_TLB_REFILL_HANDLER_SAVEK0
+#endif
+#endif
+
+// stick this here for now
+#define PGDIR_BITS 10 // PTRS_PER_PGD = 1 << PGDIR_BITS
+#if defined(CONFIG_CPU_CAVIUM_OCTEON)
+#define TLB_SEGBITS 49
+#else
+#define TLB_SEGBITS (PGDIR_SHIFT+PGDIR_BITS) // really is just 40
+#endif
+#if TLB_SEGBITS > (PGDIR_SHIFT+PGDIR_BITS)
+#define CHECK_FOR_HIGH_SEGBITS_ADDRESSES
+#endif
+
+extern void tlb_do_page_fault_0(void);
+extern void tlb_do_page_fault_1(void);
+
+#define insn_is_eret(inst) (((inst) & 0xFE00003F) == 0x42000018)
+
 static inline int r45k_bvahwbug(void)
 {
 	/* XXX: We should probe for the presence of this bug, but we don't. */
@@ -77,11 +106,19 @@ enum label_id {
 	label_vmalloc_done,
 	label_tlbw_hazard,
 	label_split,
+	label_tlbl_goaround1,
+	label_tlbl_goaround2,
 	label_nopage_tlbl,
 	label_nopage_tlbs,
 	label_nopage_tlbm,
 	label_smp_pgtable_change,
 	label_r3000_write_probe_fail,
+#ifdef CONFIG_HUGETLB_PAGE
+	label_tlb_huge_update,
+#endif
+#ifdef CHECK_FOR_HIGH_SEGBITS_ADDRESSES
+	label_large_segbits_fault,
+#endif
 };
 
 UASM_L_LA(_second_part)
@@ -94,10 +131,18 @@ UASM_L_LA(_vmalloc_done)
 UASM_L_LA(_tlbw_hazard)
 UASM_L_LA(_split)
 UASM_L_LA(_nopage_tlbl)
+UASM_L_LA(_tlbl_goaround1)
+UASM_L_LA(_tlbl_goaround2)
 UASM_L_LA(_nopage_tlbs)
 UASM_L_LA(_nopage_tlbm)
 UASM_L_LA(_smp_pgtable_change)
 UASM_L_LA(_r3000_write_probe_fail)
+#ifdef CONFIG_HUGETLB_PAGE
+UASM_L_LA(_tlb_huge_update)
+#endif
+#ifdef CHECK_FOR_HIGH_SEGBITS_ADDRESSES
+UASM_L_LA(_large_segbits_fault)
+#endif
 
 /*
  * For debug purposes.
@@ -125,6 +170,7 @@ static inline void dump_handler(const u32 *handler, int count)
 #define C0_TCBIND	2, 2
 #define C0_ENTRYLO1	3, 0
 #define C0_CONTEXT	4, 0
+#define C0_PAGEMASK	5, 0
 #define C0_BADVADDR	8, 0
 #define C0_ENTRYHI	10, 0
 #define C0_EPC		14, 0
@@ -275,7 +321,7 @@ static void __cpuinit build_tlb_write_entry(u32 **p, struct uasm_label **l,
 	case tlb_indexed: tlbw = uasm_i_tlbwi; break;
 	}
 
-	if (cpu_has_mips_r2) {
+	if (cpu_has_mips_r2 && current_cpu_type() != CPU_CAVIUM_OCTEON) {
 		uasm_i_ehb(p);
 		tlbw(p);
 		return;
@@ -336,6 +382,7 @@ static void __cpuinit build_tlb_write_entry(u32 **p, struct uasm_label **l,
 	case CPU_BCM4710:
 	case CPU_LOONGSON2:
 	case CPU_R5500:
+	case CPU_CAVIUM_OCTEON:
 		if (m4kc_tlbp_war())
 			uasm_i_nop(p);
 		tlbw(p);
@@ -405,6 +452,173 @@ static void __cpuinit build_tlb_write_entry(u32 **p, struct uasm_label **l,
 	}
 }
 
+#ifdef CONFIG_HUGETLB_PAGE
+/* Restore the page mask and unconditionally branch to a label*/
+static __cpuinit void build_restore_pagemask(u32 **p, struct uasm_reloc **r,
+					     unsigned int tmp,
+					     enum label_id lid)
+{
+	/* Reset default page size */
+	if (PM_DEFAULT_MASK>>16) {
+		uasm_i_lui(p, tmp, PM_DEFAULT_MASK >> 16);
+		uasm_i_ori(p, tmp, tmp, PM_DEFAULT_MASK & 0xffff);
+		uasm_il_b(p, r, lid);
+		uasm_i_mtc0(p, tmp, C0_PAGEMASK);
+	} else if (PM_DEFAULT_MASK) {
+		uasm_i_ori(p, tmp, 0, PM_DEFAULT_MASK);
+		uasm_il_b(p, r, lid);
+		uasm_i_mtc0(p, tmp, C0_PAGEMASK);
+	} else {
+		uasm_il_b(p, r, lid);
+		uasm_i_mtc0(p, 0, C0_PAGEMASK);
+	}
+}
+
+static __cpuinit void build_huge_tlb_write_entry(u32 **p, struct uasm_label **l,
+					 struct uasm_reloc **r,
+					 unsigned int tmp,
+					 enum tlb_write_entry wmode)
+{
+	void(*tlbw)(u32 **) = NULL;
+
+	switch (wmode) {
+	case tlb_random:
+		tlbw = uasm_i_tlbwr; break;
+	case tlb_indexed:
+		tlbw = uasm_i_tlbwi; break;
+	}
+
+	/* Set huge page tlb entry size */
+	uasm_i_lui(p, tmp, PM_HUGE_MASK >> 16);
+	uasm_i_ori(p, tmp, tmp, PM_HUGE_MASK & 0xffff);
+	uasm_i_mtc0(p, tmp, C0_PAGEMASK);
+
+	switch (current_cpu_data.cputype) {
+	case CPU_R4000PC:
+	case CPU_R4000SC:
+	case CPU_R4000MC:
+	case CPU_R4400PC:
+	case CPU_R4400SC:
+	case CPU_R4400MC:
+		/*
+		 * This branch uses up a mtc0 hazard nop slot and saves
+		 * two nops after the tlbw instruction.
+		 */
+		uasm_il_bgezl(p, r, 0, label_tlbw_hazard);
+		tlbw(p);
+		uasm_l_tlbw_hazard(l, *p);
+		uasm_i_nop(p);
+		break;
+
+	case CPU_R4600:
+	case CPU_R4700:
+	case CPU_R5000:
+	case CPU_R5000A:
+		uasm_i_nop(p);
+		tlbw(p);
+		uasm_i_nop(p);
+		break;
+
+	case CPU_R4300:
+	case CPU_5KC:
+	case CPU_TX49XX:
+	case CPU_AU1000:
+	case CPU_AU1100:
+	case CPU_AU1500:
+	case CPU_AU1550:
+	case CPU_AU1200:
+	case CPU_PR4450:
+		uasm_i_nop(p);
+		tlbw(p);
+		break;
+
+	case CPU_R10000:
+	case CPU_R12000:
+	case CPU_4KC:
+	case CPU_SB1:
+	case CPU_4KSC:
+	case CPU_20KC:
+	case CPU_25KF:
+	case CPU_CAVIUM_OCTEON:
+		tlbw(p);
+		break;
+
+	case CPU_NEVADA:
+		uasm_i_nop(p); /* QED specifies 2 nops hazard */
+		/*
+		 * This branch uses up a mtc0 hazard nop slot and saves
+		 * a nop after the tlbw instruction.
+		 */
+		uasm_il_bgezl(p, r, 0, label_tlbw_hazard);
+		tlbw(p);
+		uasm_l_tlbw_hazard(l, *p);
+		break;
+
+	case CPU_RM7000:
+		uasm_i_nop(p);
+		uasm_i_nop(p);
+		uasm_i_nop(p);
+		uasm_i_nop(p);
+		tlbw(p);
+		break;
+
+	case CPU_4KEC:
+	case CPU_24K:
+	case CPU_34K:
+		uasm_i_ehb(p);
+		tlbw(p);
+		break;
+
+	case CPU_RM9000:
+		/*
+		 * When the JTLB is updated by tlbwi or tlbwr, a subsequent
+		 * use of the JTLB for instructions should not occur for 4
+		 * cpu cycles and use for data translations should not occur
+		 * for 3 cpu cycles.
+		 */
+		uasm_i_ssnop(p);
+		uasm_i_ssnop(p);
+		uasm_i_ssnop(p);
+		uasm_i_ssnop(p);
+		tlbw(p);
+		uasm_i_ssnop(p);
+		uasm_i_ssnop(p);
+		uasm_i_ssnop(p);
+		uasm_i_ssnop(p);
+		break;
+
+	case CPU_VR4111:
+	case CPU_VR4121:
+	case CPU_VR4122:
+	case CPU_VR4181:
+	case CPU_VR4181A:
+		uasm_i_nop(p);
+		uasm_i_nop(p);
+		tlbw(p);
+		uasm_i_nop(p);
+		uasm_i_nop(p);
+		break;
+
+	case CPU_VR4131:
+	case CPU_VR4133:
+	case CPU_R5432:
+		uasm_i_nop(p);
+		uasm_i_nop(p);
+		tlbw(p);
+		break;
+
+	default:
+		panic("No TLB refill handler yet (CPU type: %d)",
+		      current_cpu_data.cputype);
+		break;
+	}
+
+	/* Reset default page size */
+	build_restore_pagemask(p, r, tmp, label_leave);
+}
+
+#endif
+
 #ifdef CONFIG_64BIT
 /*
  * TMP and PTR are scratch.
@@ -420,11 +634,29 @@ build_get_pmde64(u32 **p, struct uasm_label **l, struct uasm_reloc **r,
 	 * The vmalloc handling is not in the hotpath.
 	 */
 	uasm_i_dmfc0(p, tmp, C0_BADVADDR);
+
+#ifdef CHECK_FOR_HIGH_SEGBITS_ADDRESSES
+        //
+        // The kernel currently implicitely assumes that the MIPS SEGBITS
+        // parameter for the processor is (PGDIR_SHIFT+PGDIR_BITS) or less, and will never allocate virtual
+        // addresses outside the maximum range for SEGBITS = (PGDIR_SHIFT+PGDIR_BITS). But that
+        // doesn't prevent user code from accessing the higher xuseg addresses.
+        // Here, we make sure that everything but the lower xuseg addresses
+        // goes down the module_alloc/vmalloc path.
+        //
+        uasm_i_dsrl32(p, ptr, tmp, PGDIR_SHIFT+PGDIR_BITS-32);
+#ifdef MODULE_START
+        uasm_il_bnez(p, r, ptr, label_module_alloc);
+#else
+        uasm_il_bnez(p, r, ptr, label_vmalloc);
+#endif
+#else // CHECK_FOR_HIGH_SEGBITS_ADDRESSES
 #ifdef MODULE_START
 	uasm_il_bltz(p, r, tmp, label_module_alloc);
 #else
 	uasm_il_bltz(p, r, tmp, label_vmalloc);
 #endif
+#endif // CHECK_FOR_HIGH_SEGBITS_ADDRESSES
 	/* No uasm_i_nop needed here, since the next insn doesn't touch TMP. */
 
 #ifdef CONFIG_SMP
@@ -473,7 +705,7 @@ build_get_pmde64(u32 **p, struct uasm_label **l, struct uasm_reloc **r,
  */
 static void __cpuinit
 build_get_pgd_vmalloc64(u32 **p, struct uasm_label **l, struct uasm_reloc **r,
-			unsigned int bvaddr, unsigned int ptr)
+			unsigned int bvaddr, unsigned int ptr, unsigned int is_refill)
 {
 	long swpd = (long)swapper_pg_dir;
 
@@ -481,6 +713,18 @@ build_get_pgd_vmalloc64(u32 **p, struct uasm_label **l, struct uasm_reloc **r,
 	long modd = (long)module_pg_dir;
 
 	uasm_l_module_alloc(l, *p);
+#else
+	uasm_l_vmalloc(l, *p);
+#endif
+
+        if(is_refill) {
+#ifdef CHECK_FOR_HIGH_SEGBITS_ADDRESSES
+           // branch to large_segbits_fault if xuseg or xsseg address
+           uasm_il_bgez(p, r, bvaddr, label_large_segbits_fault);
+#endif
+        }
+
+#ifdef MODULE_START
 	/*
 	 * Assumption:
 	 * VMALLOC_START >= 0xc000000000000000UL
@@ -519,7 +763,6 @@ build_get_pgd_vmalloc64(u32 **p, struct uasm_label **l, struct uasm_reloc **r,
 	else
 		UASM_i_LA(p, ptr, VMALLOC_START);
 #else
-	uasm_l_vmalloc(l, *p);
 	UASM_i_LA(p, ptr, VMALLOC_START);
 #endif
 	uasm_i_dsubu(p, bvaddr, bvaddr, ptr);
@@ -535,6 +778,23 @@ build_get_pgd_vmalloc64(u32 **p, struct uasm_label **l, struct uasm_reloc **r,
 		else
 			uasm_i_daddiu(p, ptr, ptr, uasm_rel_lo(swpd));
 	}
+#ifdef CHECK_FOR_HIGH_SEGBITS_ADDRESSES
+        if(is_refill) {
+           uasm_l_large_segbits_fault(l, *p);
+           //
+           // We get here if we are an xsseg address, or if we are
+           // an xuseg address above (PGDIR_SHIFT+PGDIR_BITS) boundary.
+           //
+           // Ignoring xsseg (assume disabled so would generate address
+           // errors?), the only remaining possibility is the upper xuseg addresses.
+           // On processors with TLB_SEGBITS <= PGDIR_SHIFT+PGDIR_BITS, these addresses
+           // would have taken an address error. We try to mimic that
+           // here by taking a load/istream page fault.
+           //
+           uasm_i_j(p, (unsigned long)tlb_do_page_fault_0 & 0x0fffffff);
+           uasm_i_nop(p);
+        }
+#endif
 }
 
 #else /* !CONFIG_64BIT */
@@ -604,7 +864,7 @@ static void __cpuinit build_adjust_context(u32 **p, unsigned int ctx)
 	uasm_i_andi(p, ctx, ctx, mask);
 }
 
-static void __cpuinit build_get_ptep(u32 **p, unsigned int tmp, unsigned int ptr)
+static void __cpuinit __maybe_unused build_get_ptep(u32 **p, unsigned int tmp, unsigned int ptr)
 {
 	/*
 	 * Bug workaround for the Nevada. It seems as if under certain
@@ -629,6 +889,35 @@ static void __cpuinit build_get_ptep(u32 **p, unsigned int tmp, unsigned int ptr
 	UASM_i_ADDU(p, ptr, ptr, tmp); /* add in offset */
 }
 
+static __cpuinit __maybe_unused void
+	build_convert_pte_to_entrylo(u32 **p, unsigned int reg)
+{
+#if defined(CONFIG_USE_RI_XI_PAGE_BITS)
+	uasm_i_dsrl(p, reg, reg, ilog2(_PAGE_NO_EXEC));
+	uasm_i_drotr(p, reg, reg, ilog2(_PAGE_GLOBAL) - ilog2(_PAGE_NO_EXEC));
+#elif defined(CONFIG_64BIT_PHYS_ADDR)
+	uasm_i_dsrl(p, reg, reg, 6);
+#else
+	uasm_i_SRL(p, reg, reg, 6);
+#endif
+}
+
+static __cpuinit void build_convert_pte_to_entrylo2(u32 **p, unsigned int e0, unsigned int e1)
+{
+#if defined(CONFIG_USE_RI_XI_PAGE_BITS)
+	uasm_i_dsrl(p, e0, e0, ilog2(_PAGE_NO_EXEC));
+	uasm_i_dsrl(p, e1, e1, ilog2(_PAGE_NO_EXEC));
+	uasm_i_drotr(p, e0, e0, ilog2(_PAGE_GLOBAL) - ilog2(_PAGE_NO_EXEC));
+	uasm_i_drotr(p, e1, e1, ilog2(_PAGE_GLOBAL) - ilog2(_PAGE_NO_EXEC));
+#elif defined(CONFIG_64BIT_PHYS_ADDR)
+	uasm_i_dsrl(p, e0, e0, 6);
+	uasm_i_dsrl(p, e1, e1, 6);
+#else
+	uasm_i_SRL(p, e0, e0, 6);
+	uasm_i_SRL(p, e1, e1, 6);
+#endif
+}
+
 static void __cpuinit build_update_entries(u32 **p, unsigned int tmp,
 					unsigned int ptep)
 {
@@ -640,9 +929,8 @@ static void __cpuinit build_update_entries(u32 **p, unsigned int tmp,
 	if (cpu_has_64bits) {
 		uasm_i_ld(p, tmp, 0, ptep); /* get even pte */
 		uasm_i_ld(p, ptep, sizeof(pte_t), ptep); /* get odd pte */
-		uasm_i_dsrl(p, tmp, tmp, 6); /* convert to entrylo0 */
+		build_convert_pte_to_entrylo2(p, tmp, ptep);
 		uasm_i_mtc0(p, tmp, C0_ENTRYLO0); /* load it */
-		uasm_i_dsrl(p, ptep, ptep, 6); /* convert to entrylo1 */
 		uasm_i_mtc0(p, ptep, C0_ENTRYLO1); /* load it */
 	} else {
 		int pte_off_even = sizeof(pte_t) / 2;
@@ -659,11 +947,10 @@ static void __cpuinit build_update_entries(u32 **p, unsigned int tmp,
 	UASM_i_LW(p, ptep, sizeof(pte_t), ptep); /* get odd pte */
 	if (r45k_bvahwbug())
 		build_tlb_probe_entry(p);
-	UASM_i_SRL(p, tmp, tmp, 6); /* convert to entrylo0 */
+	build_convert_pte_to_entrylo2(p, tmp, ptep);
 	if (r4k_250MHZhwbug())
 		uasm_i_mtc0(p, 0, C0_ENTRYLO0);
 	uasm_i_mtc0(p, tmp, C0_ENTRYLO0); /* load it */
-	UASM_i_SRL(p, ptep, ptep, 6); /* convert to entrylo1 */
 	if (r45k_bvahwbug())
 		uasm_i_mfc0(p, tmp, C0_INDEX);
 	if (r4k_250MHZhwbug())
@@ -672,6 +959,208 @@ static void __cpuinit build_update_entries(u32 **p, unsigned int tmp,
 #endif
 }
 
+#ifdef CONFIG_HUGETLB_PAGE
+
+/*
+ * Check if Huge PTE is present, if so then jump to LABEL.
+ */
+static void __cpuinit
+build_is_huge_pte(u32 **p, struct uasm_reloc **r, unsigned int tmp,
+		unsigned int pmd, int lid)
+{
+	UASM_i_LW(p, tmp, 0, pmd);
+#ifdef CONFIG_CPU_CAVIUM_OCTEON
+	uasm_il_bbit1(p, r, tmp, ilog2(_PAGE_HUGE), lid);
+#else
+	uasm_i_andi(p, tmp, tmp, _PAGE_HUGE);
+	uasm_il_bnez(p, r, tmp, lid);
+#endif
+}
+
+static __cpuinit void build_huge_update_entries(u32 **p, struct uasm_label **l,
+		unsigned int tmp, unsigned int pmd)
+{
+	int small_sequence;
+	uasm_l_tlb_huge_update(l, *p);
+
+	/*
+	 * A huge PTE describes an area the size of the
+	 * configured huge page size. This is twice the
+	 * of the large TLB entry size we intend to use.
+	 * A TLB entry half the size of the configured
+	 * huge page size is configured into entrylo0
+	 * and entrylo1 to cover the contiguous huge PTE
+	 * address space.
+	 */
+#ifndef OCTEON_FAST_PATH_TLB_REFILL_HANDLER
+	/*
+	 * For OCTEON_FAST_PATH_TLB_REFILL_HANDLER, tmp was not
+	 * clobbered before branching here.  No need to restore it.
+	 */
+	UASM_i_LW(p, tmp, 0, pmd);
+#endif
+	small_sequence = (HPAGE_SIZE >> 7) < 0x10000;
+
+	/* We can clobber pmd.  It isn't used after this.*/
+	if (!small_sequence)
+		UASM_i_ADDIU(p, pmd, 0, 1); /* load constant 1 to pmd.  */
+
+	build_convert_pte_to_entrylo(p, tmp);
+	uasm_i_mtc0(p, tmp, C0_ENTRYLO0); /* load it */
+	if (small_sequence)
+		/* convert to entrylo1 */
+		UASM_i_ADDIU(p, tmp, tmp, HPAGE_SIZE >> 7);
+	else
+		uasm_i_dins(p, tmp, pmd, HPAGE_SHIFT-7, 1);
+
+	uasm_i_mtc0(p, tmp, C0_ENTRYLO1); /* load it */
+}
+
+#endif
+
+#ifdef OCTEON_FAST_PATH_TLB_REFILL_HANDLER
+
+static void __cpuinit build_octeon_fast_tlb_refill_handler (u32 **p,
+                 struct uasm_label **l,
+                 struct uasm_reloc **r,
+		 unsigned int tmp, // K0
+                 unsigned int ptr // K1
+                 )
+{
+//
+// This function is an optimized refill handler for OCTEON. It will work for
+// the case CONFIG_SMP && CONFIG_64BIT && !CONFIG_MIPS_MT_SMTC.
+// It gets part of it's speedup from moving
+// the instructions around. But most of its speedup comes from
+// the use of a CVMSEG LM scratch register
+// (i.e. CONFIG_TEMPORARY_SCRATCHPAD_FOR_KERNEL > 0). The speedup is
+// about 18 cycles over the unoptimized version.
+//
+// this function is a replacement for these functions:
+//	build_get_pmde64(&p, &l, &r, K0, K1); /* get pmd in K1 */
+//#ifdef CONFIG_HUGETLB_PAGE
+//	build_is_huge_pte(&p, &r, K0, K1, label_tlb_huge_update);
+//#endif
+//	build_get_ptep(&p, K0, K1);
+//	build_update_entries(&p, K0, K1);
+//	build_tlb_write_entry(&p, &l, &r, tlb_random);
+//	uasm_l_leave(&l, p);
+//#ifdef CONFIG_FAST_ACCESS_TO_THREAD_POINTER
+//	UASM_i_LW(&p, K0, FAST_ACCESS_THREAD_OFFSET, 0);  /* K0 = thread pointer */
+//#endif
+//	uasm_i_eret(&p); /* return from trap */
+//
+
+        u32 *initial_p = *p;
+	long pgdc = (long)pgd_current;                                                         // build_get_pmde64
+
+#ifdef CHECK_FOR_HIGH_SEGBITS_ADDRESSES
+
+	uasm_i_dmfc0(p, tmp, C0_BADVADDR);                                                          // build_get_pmde64
+
+	uasm_i_dmfc0(p, ptr, C0_CONTEXT);                                                           // build_get_pmde64
+
+	UASM_i_SW(p, USER_REGISTER_TO_USE, TEMPORARY_SCRATCHPAD_FOR_KERNEL_OFFSET(0), 0);           // NEW
+
+	uasm_i_dsrl32(p, USER_REGISTER_TO_USE, tmp, PGDIR_SHIFT+PGDIR_BITS-32);                     // build_get_pmde64
+
+#ifdef MODULE_START
+	uasm_il_bnez(p, r, USER_REGISTER_TO_USE, label_module_alloc);                               // build_get_pmde64
+#else
+	uasm_il_bnez(p, r, USER_REGISTER_TO_USE, label_vmalloc);                                    // build_get_pmde64
+#endif
+	UASM_i_LA_mostly(p, USER_REGISTER_TO_USE, pgdc);                                            // build_get_pmde64
+
+	uasm_i_dsrl(p, ptr, ptr, 23);                                                               // build_get_pmde64
+
+#else // CHECK_FOR_HIGH_SEGBITS_ADDRESSES
+
+	uasm_i_dmfc0(p, ptr, C0_CONTEXT);                                                           // build_get_pmde64
+
+	uasm_i_dmfc0(p, tmp, C0_BADVADDR);                                                          // build_get_pmde64
+
+	UASM_i_SW(p, USER_REGISTER_TO_USE, TEMPORARY_SCRATCHPAD_FOR_KERNEL_OFFSET(0), 0);           // NEW
+	UASM_i_LA_mostly(p, USER_REGISTER_TO_USE, pgdc);                                            // build_get_pmde64
+
+	uasm_i_dsrl(p, ptr, ptr, 23);                                                               // build_get_pmde64
+
+#ifdef MODULE_START
+	uasm_il_bltz(p, r, tmp, label_module_alloc);                                                // build_get_pmde64
+#else
+	uasm_il_bltz(p, r, tmp, label_vmalloc);                                                     // build_get_pmde64
+#endif
+
+#endif // CHECK_FOR_HIGH_SEGBITS_ADDRESSES
+
+	uasm_i_daddu(p, ptr, ptr, USER_REGISTER_TO_USE);                                            // build_get_pmde64
+
+	// uasm_i_dmfc0(p, tmp, C0_BADVADDR);                                                       // build_get_pmde64
+	uasm_i_ld(p, ptr, uasm_rel_lo(pgdc), ptr);                                                       // build_get_pmde64
+
+#ifdef OCTEON_FAST_PATH_TLB_REFILL_HANDLER_SAVEK0
+        // the module_alloc and/or vmalloc routines may mangle K0/tmp, so need to copy K0 to USER_REGISTER_TO_USE
+        uasm_i_move(p, USER_REGISTER_TO_USE, tmp);                                                  // NEW
+#define LOC_REG_TMP USER_REGISTER_TO_USE
+#define LOC_REG_USER tmp
+#else
+#define LOC_REG_TMP tmp
+#define LOC_REG_USER USER_REGISTER_TO_USE
+#endif
+
+	uasm_l_vmalloc_done(l, *p);                                                                 // build_get_pmde64
+
+        // fall-through case =   K0 has badvaddr                K1 has *pgd_current       USER may have badvaddr
+        // vmalloc case      =   K0 has badvaddr-VMALLOC_START  K1 has swapper_pg_dir     USER may have badvaddr
+        // module case       =   K0 has badvaddr-MODULE_START   K1 has module_pg_dir      USER may have badvaddr
+
+	if (PGDIR_SHIFT - 3 < 32)		/* get pgd offset in bytes */                  // build_get_pmde64
+		uasm_i_dsrl(p, LOC_REG_USER, tmp, PGDIR_SHIFT-3);                                   // build_get_pmde64
+	else                                                                                   // build_get_pmde64
+		uasm_i_dsrl32(p, LOC_REG_USER, tmp, PGDIR_SHIFT - 3 - 32);                          // build_get_pmde64
+
+	uasm_i_andi(p, LOC_REG_USER, LOC_REG_USER, (PTRS_PER_PGD - 1)<<3);                          // build_get_pmde64
+
+	uasm_i_daddu(p, ptr, ptr, LOC_REG_USER); /* add in pgd offset */                            // build_get_pmde64
+	// uasm_i_dmfc0(p, LOC_REG_TMP, C0_BADVADDR); /* get faulting address */                    // build_get_pmde64
+	uasm_i_dsrl(p, LOC_REG_USER, LOC_REG_TMP, PMD_SHIFT-3); /* get pmd offset in bytes */       // build_get_pmde64
+
+	uasm_i_ld(p, ptr, 0, ptr); /* get pmd pointer */                                            // build_get_pmde64
+	uasm_i_andi(p, LOC_REG_USER, LOC_REG_USER, (PTRS_PER_PMD - 1)<<3);                          // build_get_pmde64
+
+	GET_CONTEXT(p, LOC_REG_TMP); /* get context reg */                                     // build_get_ptep
+
+	uasm_i_daddu(p, ptr, ptr, LOC_REG_USER); /* add in pmd offset */                            // build_get_pmde64
+
+#ifdef CONFIG_HUGETLB_PAGE
+	build_is_huge_pte(p, r, LOC_REG_USER, ptr, label_tlb_huge_update);
+	// UASM_i_LW(p, ptr, 0, ptr);                                                               // build_get_ptep
+#else // CONFIG_HUGETLB_PAGE
+	UASM_i_LW(p, LOC_REG_USER, 0, ptr);                                                         // build_get_ptep
+#endif // CONFIG_HUGETLB_PAGE
+
+        build_adjust_context(p, LOC_REG_TMP);                                                  // build_get_ptep
+
+	UASM_i_ADDU(p, ptr, LOC_REG_USER, LOC_REG_TMP); /* add in offset */                         // build_get_ptep
+
+	build_update_entries(p, LOC_REG_TMP, ptr);
+	build_tlb_write_entry(p, l, r, tlb_random);
+	uasm_l_leave(l, *p);
+
+	UASM_i_LW(p, USER_REGISTER_TO_USE, TEMPORARY_SCRATCHPAD_FOR_KERNEL_OFFSET(0), 0);           // NEW
+
+#ifdef CONFIG_FAST_ACCESS_TO_THREAD_POINTER
+	UASM_i_LW(p, K0, FAST_ACCESS_THREAD_OFFSET, 0);  /* K0 = thread pointer */
+#endif
+
+	uasm_i_eret(p); /* return from trap */
+
+        while(*p - initial_p < 31) uasm_i_nop(p); // make the total instruction count be 31 or more
+}
+#undef LOC_REG_TMP
+#undef LOC_REG_USER
+
+#endif // OCTEON_FAST_PATH_TLB_REFILL_HANDLER
+
 static void __cpuinit build_r4000_tlb_refill_handler(void)
 {
 	u32 *p = tlb_handler;
@@ -688,6 +1177,10 @@ static void __cpuinit build_r4000_tlb_refill_handler(void)
 	/*
 	 * create the plain linear handler
 	 */
+#ifdef OCTEON_FAST_PATH_TLB_REFILL_HANDLER
+	build_octeon_fast_tlb_refill_handler(&p, &l, &r, K0, K1);
+#else // OCTEON_FAST_PATH_TLB_REFILL_HANDLER
+
 	if (bcm1250_m3_war())
 		build_bcm1250_m3_war(&p, &r);
 
@@ -697,14 +1190,28 @@ static void __cpuinit build_r4000_tlb_refill_handler(void)
 	build_get_pgde32(&p, K0, K1); /* get pgd in K1 */
 #endif
 
+#ifdef CONFIG_HUGETLB_PAGE
+	build_is_huge_pte(&p, &r, K0, K1, label_tlb_huge_update);
+#endif
+
 	build_get_ptep(&p, K0, K1);
 	build_update_entries(&p, K0, K1);
 	build_tlb_write_entry(&p, &l, &r, tlb_random);
 	uasm_l_leave(&l, p);
+#ifdef CONFIG_FAST_ACCESS_TO_THREAD_POINTER
+	UASM_i_LW(&p, K0, FAST_ACCESS_THREAD_OFFSET, 0);  /* K0 = thread ptr */
+#endif
 	uasm_i_eret(&p); /* return from trap */
 
+#endif // OCTEON_FAST_PATH_TLB_REFILL_HANDLER
+
+#ifdef CONFIG_HUGETLB_PAGE
+	build_huge_update_entries(&p, &l, K0, K1);
+	build_huge_tlb_write_entry(&p, &l, &r, K0, tlb_random);
+#endif
+
 #ifdef CONFIG_64BIT
-	build_get_pgd_vmalloc64(&p, &l, &r, K0, K1);
+	build_get_pgd_vmalloc64(&p, &l, &r, K0, K1, 1 /*is_refill*/);
 #endif
 
 	/*
@@ -727,8 +1234,10 @@ static void __cpuinit build_r4000_tlb_refill_handler(void)
 
 	/*
 	 * Now fold the handler in the TLB refill handler space.
+	 * Note: We always do this on Octeon since IO addresses are at
+	 * 64bit addresses. CP0_STATUS[ST0_KX] is always set.
 	 */
-#if defined(CONFIG_32BIT) || defined(CONFIG_CPU_LOONGSON2)
+#if (defined(CONFIG_32BIT) || defined(CONFIG_CPU_LOONGSON2)) && !defined(CONFIG_CPU_CAVIUM_OCTEON)
 	f = final_handler;
 	/* Simplest case, just copy the handler. */
 	uasm_copy_handler(relocs, labels, tlb_handler, p, f);
@@ -742,26 +1251,55 @@ static void __cpuinit build_r4000_tlb_refill_handler(void)
 	} else {
 		u32 *split = tlb_handler + 30;
 
+		int split_on_eret = insn_is_eret(*split);
+		int next_is_eret = 0;
 		/*
 		 * Find the split point.
 		 */
-		if (uasm_insn_has_bdelay(relocs, split - 1))
+		if(split_on_eret) {
+			// the original split choice pointed to an eret
+			// no need to insert a branch in this case, code does not flow past an eret
+			// for cleanliness, we will add a NOP instruction below, which will be the 32nd instruction
+			split++;
+		}
+		else {
+			if (insn_is_eret(split[1])) {
+				// the instruction following the original split choice is an eret
+				// no need to insert a branch in this case, code does not flow past an eret
+				// existing eret instruction will be the 32nd
+				split_on_eret = 1;
+				next_is_eret = 1;
+				split += 2;
+			}
+			else {
+				// the normal case - not an eret
+				if (uasm_insn_has_bdelay(relocs, split - 1))
 			split--;
+			}
+		}
 
 		/* Copy first part of the handler. */
 		uasm_copy_handler(relocs, labels, tlb_handler, split, f);
 		f += split - tlb_handler;
 
-		/* Insert branch. */
-		uasm_l_split(&l, final_handler);
-		uasm_il_b(&f, &r, label_split);
-		if (uasm_insn_has_bdelay(relocs, split))
-			uasm_i_nop(&f);
+		if(split_on_eret) {
+			if(!next_is_eret) {
+				// probably not necessary, but seems cleanly to insert a NOP as the 32nd instruction
+				uasm_i_nop(&f);
+			}
+		}
 		else {
-			uasm_copy_handler(relocs, labels, split, split + 1, f);
-			uasm_move_labels(labels, f, f + 1, -1);
-			f++;
-			split++;
+			/* Insert branch. */
+			uasm_l_split(&l, final_handler);
+			uasm_il_b(&f, &r, label_split);
+			if (uasm_insn_has_bdelay(relocs, split))
+				uasm_i_nop(&f);
+			else {
+				uasm_copy_handler(relocs, labels, split, split + 1, f);
+				uasm_move_labels(labels, f, f + 1, -1);
+				f++;
+				split++;
+			}
 		}
 
 		/* Copy the rest of the handler. */
@@ -785,9 +1323,6 @@ static void __cpuinit build_r4000_tlb_refill_handler(void)
  * Only the fastpath gets synthesized at runtime, the slowpath for
  * do_page_fault remains normal asm.
  */
-extern void tlb_do_page_fault_0(void);
-extern void tlb_do_page_fault_1(void);
-
 /*
  * 128 instructions for the fastpath handler is generous and should
  * never be exceeded.
@@ -799,7 +1334,7 @@ u32 handle_tlbs[FASTPATH_SIZE] __cacheline_aligned;
 u32 handle_tlbm[FASTPATH_SIZE] __cacheline_aligned;
 
 static void __cpuinit
-iPTE_LW(u32 **p, struct uasm_label **l, unsigned int pte, unsigned int ptr)
+uasm_iPTE_LW(u32 **p, unsigned int pte, unsigned int ptr)
 {
 #ifdef CONFIG_SMP
 # ifdef CONFIG_64BIT_PHYS_ADDR
@@ -882,10 +1417,15 @@ static void __cpuinit
 build_pte_present(u32 **p, struct uasm_label **l, struct uasm_reloc **r,
 		  unsigned int pte, unsigned int ptr, enum label_id lid)
 {
+#if defined(_PAGE_NO_READ) || defined(_PAGE_NO_EXEC)
+	uasm_il_bbit0(p, r, pte, ilog2(_PAGE_PRESENT), lid);
+	uasm_i_nop(p);
+#else
 	uasm_i_andi(p, pte, pte, _PAGE_PRESENT | _PAGE_READ);
 	uasm_i_xori(p, pte, pte, _PAGE_PRESENT | _PAGE_READ);
 	uasm_il_bnez(p, r, pte, lid);
-	iPTE_LW(p, l, pte, ptr);
+	uasm_iPTE_LW(p, pte, ptr);
+#endif
 }
 
 /* Make PTE valid, store result in PTR. */
@@ -909,7 +1449,7 @@ build_pte_writable(u32 **p, struct uasm_label **l, struct uasm_reloc **r,
 	uasm_i_andi(p, pte, pte, _PAGE_PRESENT | _PAGE_WRITE);
 	uasm_i_xori(p, pte, pte, _PAGE_PRESENT | _PAGE_WRITE);
 	uasm_il_bnez(p, r, pte, lid);
-	iPTE_LW(p, l, pte, ptr);
+	uasm_iPTE_LW(p, pte, ptr);
 }
 
 /* Make PTE writable, update software status bits as well, then store
@@ -933,9 +1473,14 @@ static void __cpuinit
 build_pte_modifiable(u32 **p, struct uasm_label **l, struct uasm_reloc **r,
 		     unsigned int pte, unsigned int ptr, enum label_id lid)
 {
+#ifdef CONFIG_CPU_CAVIUM_OCTEON
+	uasm_il_bbit0(p, r, pte, ilog2(_PAGE_WRITE), lid);
+	uasm_i_nop(p);
+#else
 	uasm_i_andi(p, pte, pte, _PAGE_WRITE);
 	uasm_il_beqz(p, r, pte, lid);
-	iPTE_LW(p, l, pte, ptr);
+	uasm_iPTE_LW(p, pte, ptr);
+#endif
 }
 
 /*
@@ -1104,6 +1649,15 @@ build_r4000_tlbchange_handler_head(u32 **p, struct uasm_label **l,
 	build_get_pgde32(p, pte, ptr); /* get pgd in ptr */
 #endif
 
+#ifdef CONFIG_HUGETLB_PAGE
+	/* For huge tlb entries, pmd doesn't contain an address but
+		instead contains the tlb pte. Check the PAGE_HUGE bit
+		and see if we need to jump to huge tlb processing */
+	UASM_i_LW(p, pte, 0, ptr);
+	uasm_il_bbit1(p, r, pte, ilog2(_PAGE_HUGE), label_tlb_huge_update);
+	uasm_i_nop(p); /* delay slot */
+#endif
+
 	UASM_i_MFC0(p, pte, C0_BADVADDR);
 	UASM_i_LW(p, ptr, 0, ptr);
 	UASM_i_SRL(p, pte, pte, PAGE_SHIFT + PTE_ORDER - PTE_T_LOG2);
@@ -1113,7 +1667,7 @@ build_r4000_tlbchange_handler_head(u32 **p, struct uasm_label **l,
 #ifdef CONFIG_SMP
 	uasm_l_smp_pgtable_change(l, *p);
 #endif
-	iPTE_LW(p, l, pte, ptr); /* get even pte */
+	uasm_iPTE_LW(p, pte, ptr); /* get even pte */
 	if (!m4kc_tlbp_war())
 		build_tlb_probe_entry(p);
 }
@@ -1128,15 +1682,23 @@ build_r4000_tlbchange_handler_tail(u32 **p, struct uasm_label **l,
 	build_update_entries(p, tmp, ptr);
 	build_tlb_write_entry(p, l, r, tlb_indexed);
 	uasm_l_leave(l, *p);
+#ifdef CONFIG_FAST_ACCESS_TO_THREAD_POINTER
+	UASM_i_LW(p, K0, FAST_ACCESS_THREAD_OFFSET, 0);  /* K0 = thread ptr */
+#endif
 	uasm_i_eret(p); /* return from trap */
 
 #ifdef CONFIG_64BIT
-	build_get_pgd_vmalloc64(p, l, r, tmp, ptr);
+	build_get_pgd_vmalloc64(p, l, r, tmp, ptr, 0 /*is_refill*/);
 #endif
 }
 
 static void __cpuinit build_r4000_tlb_load_handler(void)
 {
+#if defined(CONFIG_HUGETLB_PAGE) || defined(_PAGE_NO_READ) || \
+	defined(_PAGE_NO_EXEC)
+	const int PTE = K0;
+	const int PTRC = K1;
+#endif
 	u32 *p = handle_tlbl;
 	struct uasm_label *l = labels;
 	struct uasm_reloc *r = relocs;
@@ -1152,9 +1714,86 @@ static void __cpuinit build_r4000_tlb_load_handler(void)
 	build_pte_present(&p, &l, &r, K0, K1, label_nopage_tlbl);
 	if (m4kc_tlbp_war())
 		build_tlb_probe_entry(&p);
+#if defined(_PAGE_NO_READ) || defined(_PAGE_NO_EXEC)
+	/*
+	 * If the page is not _PAGE_VALID,  RI or XI could not have triggered
+	 * it. Skip the expensive test..
+	 */
+	uasm_il_bbit0(&p, &r, PTE, ilog2(_PAGE_VALID), label_tlbl_goaround1);
+	uasm_i_nop(&p);
+	uasm_i_tlbr(&p);
+	/* Examine  entrylo 0 or 1 based on ptr. */
+	uasm_i_bbit0(&p, PTRC, ilog2(sizeof(pte_t)), 8);
+	uasm_i_mfc0(&p, PTE, C0_ENTRYLO0); /* load it in the delay slot*/
+	uasm_i_mfc0(&p, PTE, C0_ENTRYLO1); /* load it if ptr is odd */
+	/* If the entryLo (now in PTE) is valid (bit 1), RI or XI must have
+	   triggered it. */
+	uasm_il_bbit1(&p, &r, PTE, 1, label_nopage_tlbl);
+	/* Reload the PTE value */
+	uasm_iPTE_LW(&p, PTE, PTRC);
+
+	uasm_l_tlbl_goaround1(&l, p);
+#endif
 	build_make_valid(&p, &r, K0, K1);
 	build_r4000_tlbchange_handler_tail(&p, &l, &r, K0, K1);
 
+#ifdef CONFIG_HUGETLB_PAGE
+	/* This is the entry point when build_r4000_tlbchange_handler_head
+		spots a huge page */
+	uasm_l_tlb_huge_update(&l, p);
+	uasm_iPTE_LW(&p, PTE, PTRC);
+	uasm_il_bbit0(&p, &r, PTE, ilog2(_PAGE_PRESENT), label_nopage_tlbl);
+	build_tlb_probe_entry(&p);
+#if defined(_PAGE_NO_READ) || defined(_PAGE_NO_EXEC)
+	/*
+	 * If the page is not _PAGE_VALID,  RI or XI could not have triggered
+	 * it. Skip the expensive test..
+	 */
+	uasm_il_bbit0(&p, &r, PTE, ilog2(_PAGE_VALID), label_tlbl_goaround2);
+	uasm_i_nop(&p);
+	uasm_i_tlbr(&p);
+	/* Examine  entrylo 0 or 1 based on ptr. */
+	uasm_i_bbit0(&p, PTRC, ilog2(sizeof(pte_t)), 8);
+	uasm_i_mfc0(&p, PTE, C0_ENTRYLO0); /* load it in the delay slot*/
+	uasm_i_mfc0(&p, PTE, C0_ENTRYLO1); /* load it if ptr is odd */
+	/* If the entryLo (now in PTE) is valid (bit 1), RI or XI must have
+	   triggered it. */
+	uasm_il_bbit0(&p, &r, PTE, 1, label_tlbl_goaround2);
+	/* Reload the PTE value */
+	uasm_iPTE_LW(&p, PTE, PTRC);
+
+	/*
+	 * We clobbered C0_PAGEMASK, restore it.  On the other branch
+	 * it is restored in build_huge_tlb_write_entry.
+	 */
+	build_restore_pagemask(&p, &r, PTE, label_nopage_tlbl);
+
+	uasm_l_tlbl_goaround2(&l, p);
+#else
+	uasm_il_bbit0(&p, &r, PTE, ilog2(_PAGE_READ), label_nopage_tlbl);
+#endif
+	uasm_i_ori(&p, PTE, PTE, (_PAGE_ACCESSED | _PAGE_VALID));
+#ifdef CONFIG_SMP
+	UASM_i_SC(&p, PTE, 0, PTRC);
+	uasm_il_beqz(&p, &r, PTE, label_tlb_huge_update);
+	UASM_i_LW(&p, PTE, 0, PTRC); /* Needed because SC killed our PTE */
+#else
+	UASM_i_SW(&p, PTE, 0, PTRC);
+#endif
+	build_convert_pte_to_entrylo(&p, PTE);
+	uasm_i_mtc0(&p, PTE, C0_ENTRYLO0); /* load it */
+	if (HPAGE_SIZE>>7 < 0x10000)
+		uasm_i_addiu(&p, PTE, PTE, HPAGE_SIZE>>7); /* convert to entrylo1 */
+	else
+	{
+		uasm_i_nor(&p, PTE, PTE, 0);
+		uasm_i_dins(&p, PTE, 0, HPAGE_SHIFT-7, 1);
+		uasm_i_nor(&p, PTE, PTE, 0);
+	}
+	uasm_i_mtc0(&p, PTE, C0_ENTRYLO1); /* load it */
+	build_huge_tlb_write_entry(&p, &l, &r, PTE, tlb_indexed);
+#endif
+
 	uasm_l_nopage_tlbl(&l, p);
 	uasm_i_j(&p, (unsigned long)tlb_do_page_fault_0 & 0x0fffffff);
 	uasm_i_nop(&p);
@@ -1171,6 +1810,10 @@ static void __cpuinit build_r4000_tlb_load_handler(void)
 
 static void __cpuinit build_r4000_tlb_store_handler(void)
 {
+#ifdef CONFIG_HUGETLB_PAGE
+	const int PTE = K0;
+	const int PTRC = K1;
+#endif
 	u32 *p = handle_tlbs;
 	struct uasm_label *l = labels;
 	struct uasm_reloc *r = relocs;
@@ -1186,6 +1829,36 @@ static void __cpuinit build_r4000_tlb_store_handler(void)
 	build_make_write(&p, &r, K0, K1);
 	build_r4000_tlbchange_handler_tail(&p, &l, &r, K0, K1);
 
+#ifdef CONFIG_HUGETLB_PAGE
+	/* This is the entry point when build_r4000_tlbchange_handler_head
+		spots a huge page */
+	uasm_l_tlb_huge_update(&l, p);
+	uasm_iPTE_LW(&p, PTE, PTRC);
+	uasm_il_bbit0(&p, &r, PTE, ilog2(_PAGE_PRESENT), label_nopage_tlbs);
+	build_tlb_probe_entry(&p);
+	uasm_il_bbit0(&p, &r, PTE, ilog2(_PAGE_WRITE), label_nopage_tlbs);
+	uasm_i_ori(&p, PTE, PTE, (_PAGE_ACCESSED | _PAGE_MODIFIED | _PAGE_VALID | _PAGE_DIRTY));
+#ifdef CONFIG_SMP
+	UASM_i_SC(&p, PTE, 0, PTRC);
+	uasm_il_beqz(&p, &r, PTE, label_tlb_huge_update);
+	UASM_i_LW(&p, PTE, 0, PTRC); /* Needed because SC killed our PTE */
+#else
+	UASM_i_SW(&p, PTE, 0, PTRC);
+#endif
+	build_convert_pte_to_entrylo(&p, PTE);
+	uasm_i_mtc0(&p, PTE, C0_ENTRYLO0); /* load it */
+	if (HPAGE_SIZE>>7 < 0x10000)
+		uasm_i_addiu(&p, PTE, PTE, HPAGE_SIZE>>7); /* convert to entrylo1 */
+	else
+	{
+		uasm_i_nor(&p, PTE, PTE, 0);
+		uasm_i_dins(&p, PTE, 0, HPAGE_SHIFT-7, 1);
+		uasm_i_nor(&p, PTE, PTE, 0);
+	}
+	uasm_i_mtc0(&p, PTE, C0_ENTRYLO1); /* load it */
+	build_huge_tlb_write_entry(&p, &l, &r, PTE, tlb_indexed);
+#endif
+
 	uasm_l_nopage_tlbs(&l, p);
 	uasm_i_j(&p, (unsigned long)tlb_do_page_fault_1 & 0x0fffffff);
 	uasm_i_nop(&p);
@@ -1202,6 +1875,10 @@ static void __cpuinit build_r4000_tlb_store_handler(void)
 
 static void __cpuinit build_r4000_tlb_modify_handler(void)
 {
+#ifdef CONFIG_HUGETLB_PAGE
+	const int PTE = K0;
+	const int PTRC = K1;
+#endif
 	u32 *p = handle_tlbm;
 	struct uasm_label *l = labels;
 	struct uasm_reloc *r = relocs;
@@ -1218,6 +1895,35 @@ static void __cpuinit build_r4000_tlb_modify_handler(void)
 	build_make_write(&p, &r, K0, K1);
 	build_r4000_tlbchange_handler_tail(&p, &l, &r, K0, K1);
 
+#ifdef CONFIG_HUGETLB_PAGE
+	/* This is the entry point when build_r4000_tlbchange_handler_head
+		spots a huge page */
+	uasm_l_tlb_huge_update(&l, p);
+	uasm_iPTE_LW(&p, PTE, PTRC);
+	uasm_il_bbit0(&p, &r, PTE, ilog2(_PAGE_WRITE), label_nopage_tlbm);
+	build_tlb_probe_entry(&p);
+	uasm_i_ori(&p, PTE, PTE, (_PAGE_ACCESSED | _PAGE_MODIFIED | _PAGE_VALID | _PAGE_DIRTY));
+#ifdef CONFIG_SMP
+	UASM_i_SC(&p, PTE, 0, PTRC);
+	uasm_il_beqz(&p, &r, PTE, label_tlb_huge_update);
+	UASM_i_LW(&p, PTE, 0, PTRC); /* Needed because SC killed our PTE */
+#else
+	UASM_i_SW(&p, PTE, 0, PTRC);
+#endif
+	build_convert_pte_to_entrylo(&p, PTE);
+	uasm_i_mtc0(&p, PTE, C0_ENTRYLO0); /* load it */
+	if (HPAGE_SIZE>>7 < 0x10000)
+		uasm_i_addiu(&p, PTE, PTE, HPAGE_SIZE>>7); /* convert to entrylo1 */
+	else
+	{
+		uasm_i_nor(&p, PTE, PTE, 0);
+		uasm_i_dins(&p, PTE, 0, HPAGE_SHIFT-7, 1);
+		uasm_i_nor(&p, PTE, PTE, 0);
+	}
+	uasm_i_mtc0(&p, PTE, C0_ENTRYLO1); /* load it */
+	build_huge_tlb_write_entry(&p, &l, &r, PTE, tlb_indexed);
+#endif
+
 	uasm_l_nopage_tlbm(&l, p);
 	uasm_i_j(&p, (unsigned long)tlb_do_page_fault_1 & 0x0fffffff);
 	uasm_i_nop(&p);
diff --git a/arch/mips/mm/uasm.h b/arch/mips/mm/uasm.h
index 67e54e4..04156fa 100644
--- a/arch/mips/mm/uasm.h
+++ b/arch/mips/mm/uasm.h
@@ -44,6 +44,19 @@ void __cpuinit uasm_i##op(u32 **buf, unsigned int a, signed int b)
 
 #define Ip_0(op) void __cpuinit uasm_i##op(u32 **buf)
 
+/* FIXME: These prototypes for dins/dinsu don't share like the above.
+	They should probably use Ip_u2u1u4u3(op) and do the u4/u3 arg
+	blending in the callee and not the build_insn() itself. */
+#ifdef CONFIG_CPU_CAVIUM_OCTEON
+#define Ip_u2u1msbu3(op)						\
+void __init uasm_i##op(u32 **buf, unsigned int a, unsigned int b,	\
+			unsigned int c, unsigned int d)
+
+#define Ip_u2u1msb33u3(op)						\
+void __init uasm_i##op(u32 **buf, unsigned int rt, unsigned int rs,	\
+			unsigned int pos, unsigned int size)
+#endif
+
 Ip_u2u1s3(_addiu);
 Ip_u3u1u2(_addu);
 Ip_u2u1u3(_andi);
@@ -64,6 +77,7 @@ Ip_u2u1u3(_dsll);
 Ip_u2u1u3(_dsll32);
 Ip_u2u1u3(_dsra);
 Ip_u2u1u3(_dsrl);
+Ip_u2u1u3(_drotr);
 Ip_u2u1u3(_dsrl32);
 Ip_u3u1u2(_dsubu);
 Ip_0(_eret);
@@ -90,10 +104,20 @@ Ip_u2u1u3(_srl);
 Ip_u3u1u2(_subu);
 Ip_u2s3u1(_sw);
 Ip_0(_tlbp);
+Ip_0(_tlbr);
 Ip_0(_tlbwi);
 Ip_0(_tlbwr);
 Ip_u3u1u2(_xor);
 Ip_u2u1u3(_xori);
+#ifdef CONFIG_CPU_CAVIUM_OCTEON
+Ip_u3u1u2(_nor);
+Ip_u1u2s3(_bbit1);
+Ip_u1u2s3(_bbit132);
+Ip_u1u2s3(_bbit0);
+Ip_u1u2s3(_bbit032);
+Ip_u2u1msbu3(_dins);
+Ip_u2u1msb33u3(_dinsu);
+#endif
 
 /* Handle labels. */
 struct uasm_label {
@@ -183,3 +207,13 @@ void uasm_il_bne(u32 **p, struct uasm_reloc **r, unsigned int reg1,
 void uasm_il_bnez(u32 **p, struct uasm_reloc **r, unsigned int reg, int lid);
 void uasm_il_bgezl(u32 **p, struct uasm_reloc **r, unsigned int reg, int lid);
 void uasm_il_bgez(u32 **p, struct uasm_reloc **r, unsigned int reg, int lid);
+#ifdef CONFIG_CPU_CAVIUM_OCTEON
+void uasm_il_bbit1(u32 **p, struct uasm_reloc **r, unsigned int reg,
+		unsigned int bit, int lid);
+void uasm_il_bbit132(u32 **p, struct uasm_reloc **r, unsigned int reg,
+		unsigned int bit, int lid);
+void uasm_il_bbit032(u32 **p, struct uasm_reloc **r, unsigned int reg,
+		unsigned int bit, int lid);
+void uasm_il_bbit0(u32 **p, struct uasm_reloc **r, unsigned int reg,
+		unsigned int bit, int lid);
+#endif
diff --git a/fs/Kconfig b/fs/Kconfig
index 62c447d..949c1f3 100644
--- a/fs/Kconfig
+++ b/fs/Kconfig
@@ -964,7 +964,7 @@ config TMPFS_POSIX_ACL
 config HUGETLBFS
 	bool "HugeTLB file system support"
 	depends on X86 || IA64 || PPC64 || SPARC64 || (SUPERH && MMU) || \
-		   (S390 && 64BIT) || BROKEN
+		   (S390 && 64BIT) || (MIPS && 64BIT) || BROKEN
 	help
 	  hugetlbfs is a filesystem backing for HugeTLB pages, based on
 	  ramfs. For architectures that support it, say Y here and read
diff --git a/include/asm-mips/hugetlb.h b/include/asm-mips/hugetlb.h
new file mode 100644
index 0000000..8873fa1
--- /dev/null
+++ b/include/asm-mips/hugetlb.h
@@ -0,0 +1,107 @@
+#ifndef _ASM_MIPS_HUGETLB_H
+#define _ASM_MIPS_HUGETLB_H
+
+#include <asm/page.h>
+
+
+static inline int is_hugepage_only_range(struct mm_struct *mm,
+					 unsigned long addr,
+					 unsigned long len)
+{
+	return 0;
+}
+
+/*
+ * If the arch doesn't supply something else, assume that hugepage
+ * size aligned regions are ok without further preparation.
+ */
+static inline int prepare_hugepage_range(struct file *file,
+			unsigned long addr, unsigned long len)
+{
+	unsigned long task_size = STACK_TOP;
+	struct hstate *h = hstate_file(file);
+
+	if (len & ~huge_page_mask(h))
+		return -EINVAL;
+	if (addr & ~huge_page_mask(h))
+		return -EINVAL;
+	if (len > task_size)
+		return -ENOMEM;
+	if (task_size - len < addr)
+		return -EINVAL;
+	return 0;
+}
+
+static inline void hugetlb_prefault_arch_hook(struct mm_struct *mm)
+{
+}
+
+static inline void hugetlb_free_pgd_range(struct mmu_gather *tlb,
+					  unsigned long addr, unsigned long end,
+					  unsigned long floor,
+					  unsigned long ceiling)
+{
+	free_pgd_range(tlb, addr, end, floor, ceiling);
+}
+
+static inline void set_huge_pte_at(struct mm_struct *mm, unsigned long addr,
+				   pte_t *ptep, pte_t pte)
+{
+	set_pte_at(mm, addr, ptep, pte);
+}
+
+static inline pte_t huge_ptep_get_and_clear(struct mm_struct *mm,
+					    unsigned long addr, pte_t *ptep)
+{
+	pte_t clear;
+	pte_t pte = *ptep;
+
+	pte_val(clear) = (unsigned long)invalid_pte_table;
+	set_pte_at(mm, addr, ptep, clear);
+	return pte;
+}
+
+static inline void huge_ptep_clear_flush(struct vm_area_struct *vma,
+					 unsigned long addr, pte_t *ptep)
+{
+}
+
+static inline int huge_pte_none(pte_t pte)
+{
+	unsigned long val = pte_val(pte) & ~_PAGE_GLOBAL;
+	return !val || (val == (unsigned long)invalid_pte_table);
+}
+
+static inline pte_t huge_pte_wrprotect(pte_t pte)
+{
+	return pte_wrprotect(pte);
+}
+
+static inline void huge_ptep_set_wrprotect(struct mm_struct *mm,
+					   unsigned long addr, pte_t *ptep)
+{
+	ptep_set_wrprotect(mm, addr, ptep);
+}
+
+static inline int huge_ptep_set_access_flags(struct vm_area_struct *vma,
+					     unsigned long addr, pte_t *ptep,
+					     pte_t pte, int dirty)
+{
+	return ptep_set_access_flags(vma, addr, ptep, pte, dirty);
+}
+
+static inline pte_t huge_ptep_get(pte_t *ptep)
+{
+	return *ptep;
+}
+
+static inline int arch_prepare_hugepage(struct page *page)
+{
+	return 0;
+}
+
+static inline void arch_release_hugepage(struct page *page)
+{
+}
+
+#endif /* _ASM_MIPS_HUGETLB_H */
diff --git a/include/asm-mips/mipsregs.h b/include/asm-mips/mipsregs.h
index c9c820e..b636950 100644
--- a/include/asm-mips/mipsregs.h
+++ b/include/asm-mips/mipsregs.h
@@ -184,7 +184,9 @@
 #else
 
 #define PM_4K		0x00000000
+#define PM_8K		0x00002000  /* Used for Cavium Octeon */
 #define PM_16K		0x00006000
+#define PM_32K		0x0000e000  /* Used for Cavium Octeon */
 #define PM_64K		0x0001e000
 #define PM_256K		0x0007e000
 #define PM_1M		0x001fe000
@@ -200,14 +202,36 @@
  */
 #ifdef CONFIG_PAGE_SIZE_4KB
 #define PM_DEFAULT_MASK	PM_4K
+#elif defined(CONFIG_PAGE_SIZE_8KB)
+#define PM_DEFAULT_MASK	PM_8K
 #elif defined(CONFIG_PAGE_SIZE_16KB)
 #define PM_DEFAULT_MASK	PM_16K
+#elif defined(CONFIG_PAGE_SIZE_32KB)
+#define PM_DEFAULT_MASK	PM_32K
 #elif defined(CONFIG_PAGE_SIZE_64KB)
 #define PM_DEFAULT_MASK	PM_64K
 #else
 #error Bad page size configuration!
 #endif
 
+/*
+ * Default huge tlb size for a given kernel configuration
+ */
+#ifdef CONFIG_HUGETLB_PAGE
+#ifdef CONFIG_PAGE_SIZE_4KB
+#define PM_HUGE_MASK	PM_1M
+#elif defined(CONFIG_PAGE_SIZE_8KB)
+#define PM_HUGE_MASK	PM_4M
+#elif defined(CONFIG_PAGE_SIZE_16KB)
+#define PM_HUGE_MASK	PM_16M
+#elif defined(CONFIG_PAGE_SIZE_32KB)
+#define PM_HUGE_MASK	PM_64M
+#elif defined(CONFIG_PAGE_SIZE_64KB)
+#define PM_HUGE_MASK	PM_256M
+#else
+#error Bad page size configuration (only 4KB, 8KB, 16KB, 32KB, and 64KB supported for hugetlbfs)!
+#endif
+#endif
 
 /*
  * Values used for computation of new tlb entries
@@ -553,6 +577,33 @@
 #define MIPS_FPIR_L		(_ULCAST_(1) << 21)
 #define MIPS_FPIR_F64		(_ULCAST_(1) << 22)
 
+/*
+ * These defines are used on Octeon to implement fast access to the thread pointer
+ * from userspace. Octeon uses a 64bit location in CVMSEG to store the thread pointer
+ * for quick access.
+ */
+#define TOP_OF_CVMSEG_STORE        (CONFIG_CAVIUM_OCTEON_CVMSEG_SIZE*128-32768)
+#ifdef CONFIG_FAST_ACCESS_TO_THREAD_POINTER
+#define FAST_ACCESS_THREAD_OFFSET       (TOP_OF_CVMSEG_STORE-8)
+#define FAST_ACCESS_THREAD_REGISTER     *(unsigned long *)(FAST_ACCESS_THREAD_OFFSET)
+#else
+#define FAST_ACCESS_THREAD_OFFSET       TOP_OF_CVMSEG_STORE
+#endif
+
+/*
+ * These defines are used on Octeon to give the kernel access to CVMSEG locations to
+ * improve performance. The kernel may use this space much like it normally uses
+ * K0 and K1 in the main register file - as a temporary store.
+ *
+ * Put it as high as possible in CVMSEG space, but below THREAD_POINTER
+ */
+#ifdef CONFIG_TEMPORARY_SCRATCHPAD_FOR_KERNEL
+#define TEMPORARY_SCRATCHPAD_FOR_KERNEL_OFFSET(i)    (FAST_ACCESS_THREAD_OFFSET-(8*(CONFIG_TEMPORARY_SCRATCHPAD_FOR_KERNEL-(i))))
+#define TEMPORARY_SCRATCHPAD_FOR_KERNEL_REGISTER(i)  *(unsigned long)(TEMPORARY_SCRATCHPAD_FOR_KERNEL_OFFSET(i))
+#else
+#define TEMPORARY_SCRATCHPAD_FOR_KERNEL_OFFSET(i)    FAST_ACCESS_THREAD_OFFSET
+#endif
+
 #ifndef __ASSEMBLY__
 
 /*
@@ -787,6 +838,9 @@ do {									\
 #define read_c0_pagemask()	__read_32bit_c0_register($5, 0)
 #define write_c0_pagemask(val)	__write_32bit_c0_register($5, 0, val)
 
+#define read_c0_pagegrain()	__read_32bit_c0_register($5, 1)
+#define write_c0_pagegrain(val)	__write_32bit_c0_register($5, 1, val)
+
 #define read_c0_wired()		__read_32bit_c0_register($6, 0)
 #define write_c0_wired(val)	__write_32bit_c0_register($6, 0, val)
 
@@ -808,7 +862,14 @@ do {									\
 #define write_c0_count3(val)	__write_32bit_c0_register($9, 7, val)
 
 #define read_c0_entryhi()	__read_ulong_c0_register($10, 0)
+#ifdef CONFIG_CPU_CAVIUM_OCTEON
+/* The Cavium Octeon simulator validates that unused hardware bits are zero.
+    The kernel by default allows the ASID to overflow the actual hardware
+    space. Most mips hardware doesn't care, but the simulator does. */
+#define write_c0_entryhi(val)	__write_ulong_c0_register($10, 0, (val) & ~0x1f00ul)
+#else
 #define write_c0_entryhi(val)	__write_ulong_c0_register($10, 0, val)
+#endif
 
 #define read_c0_compare()	__read_32bit_c0_register($11, 0)
 #define write_c0_compare(val)	__write_32bit_c0_register($11, 0, val)
@@ -967,7 +1028,12 @@ do {									\
 #define read_c0_derraddr0()	__read_ulong_c0_register($26, 1)
 #define write_c0_derraddr0(val)	__write_ulong_c0_register($26, 1, val)
 
+#ifdef CONFIG_CPU_CAVIUM_OCTEON
+#define read_c0_cacheerr()	__read_64bit_c0_register($27, 0)
+#define write_c0_cacheerr(val)	__write_64bit_c0_register($27, 0, val)
+#else
 #define read_c0_cacheerr()	__read_32bit_c0_register($27, 0)
+#endif
 
 #define read_c0_derraddr1()	__read_ulong_c0_register($27, 1)
 #define write_c0_derraddr1(val)	__write_ulong_c0_register($27, 1, val)
@@ -1000,6 +1066,10 @@ do {									\
 #define read_c0_ebase()		__read_32bit_c0_register($15, 1)
 #define write_c0_ebase(val)	__write_32bit_c0_register($15, 1, val)
 
+/* Cavium OCTEON (cnMIPS) */
+#define read_c0_cvmcount()	__read_ulong_c0_register($9, 6)
+#define write_c0_cvmcount(val)	__write_ulong_c0_register($9, 6, val)
+
 /*
  * Macros to access the floating point coprocessor control registers
  */
diff --git a/include/asm-mips/page.h b/include/asm-mips/page.h
index 9ebb0d4..6fa2b16 100644
--- a/include/asm-mips/page.h
+++ b/include/asm-mips/page.h
@@ -23,12 +23,22 @@
 #ifdef CONFIG_PAGE_SIZE_16KB
 #define PAGE_SHIFT	14
 #endif
+#ifdef CONFIG_PAGE_SIZE_32KB
+#define PAGE_SHIFT	15
+#endif
 #ifdef CONFIG_PAGE_SIZE_64KB
 #define PAGE_SHIFT	16
 #endif
 #define PAGE_SIZE	(1UL << PAGE_SHIFT)
 #define PAGE_MASK       (~((1 << PAGE_SHIFT) - 1))
 
+#ifdef CONFIG_HUGETLB_PAGE
+#define HPAGE_SHIFT	(PAGE_SHIFT + PAGE_SHIFT - 3)
+#define HPAGE_SIZE	((1UL) << HPAGE_SHIFT)
+#define HPAGE_MASK	(~(HPAGE_SIZE - 1))
+#define HUGETLB_PAGE_ORDER	(HPAGE_SHIFT - PAGE_SHIFT)
+#endif
+
 #ifndef __ASSEMBLY__
 
 #include <linux/pfn.h>
@@ -151,7 +161,11 @@ typedef struct { unsigned long pgprot; } pgprot_t;
     ((unsigned long)(x) - PAGE_OFFSET + PHYS_OFFSET)
 #endif
 #define __va(x)		((void *)((unsigned long)(x) + PAGE_OFFSET - PHYS_OFFSET))
+#if (defined (CONFIG_64BIT) && defined(CONFIG_CPU_CAVIUM_OCTEON))
+#define __pa_symbol(x)	CPHYSADDR((unsigned long)(x))
+#else
 #define __pa_symbol(x)	__pa(RELOC_HIDE((unsigned long)(x), 0))
+#endif
 
 #define pfn_to_kaddr(pfn)	__va((pfn) << PAGE_SHIFT)
 
diff --git a/include/asm-mips/pgtable-bits.h b/include/asm-mips/pgtable-bits.h
index 51b34a4..1073e6d 100644
--- a/include/asm-mips/pgtable-bits.h
+++ b/include/asm-mips/pgtable-bits.h
@@ -72,6 +72,7 @@
 #else
 
 #define _PAGE_R4KBUG                (1<<5)  /* workaround for r4k bug  */
+#define _PAGE_HUGE                  (1<<5)  /* huge tlb page */
 #define _PAGE_GLOBAL                (1<<6)
 #define _PAGE_VALID                 (1<<7)
 #define _PAGE_SILENT_READ           (1<<7)  /* synonym                 */
diff --git a/include/asm-mips/pgtable.h b/include/asm-mips/pgtable.h
index 6a0edf7..3ac0c45 100644
--- a/include/asm-mips/pgtable.h
+++ b/include/asm-mips/pgtable.h
@@ -21,6 +21,33 @@
 struct mm_struct;
 struct vm_area_struct;
 
+#ifdef _PAGE_NO_EXEC
+#define PAGE_BASE_FLAGS (_PAGE_PRESENT | _page_cachable_default)
+#define PAGE_NONE	__pgprot(_PAGE_PRESENT | _CACHE_CACHABLE_NONCOHERENT)
+#define PAGE_SHARED	__pgprot(PAGE_BASE_FLAGS | _PAGE_WRITE)
+#define PAGE_COPY __pgprot(PAGE_BASE_FLAGS | _PAGE_NO_EXEC)
+#define PAGE_READONLY __pgprot(PAGE_BASE_FLAGS)
+#define PAGE_KERNEL __pgprot(PAGE_BASE_FLAGS | __READABLE | __WRITEABLE | \
+			     _PAGE_GLOBAL)
+#define __P000	__pgprot(_PAGE_PRESENT | _PAGE_NO_EXEC | _PAGE_NO_READ)
+#define __P001	__pgprot(_PAGE_PRESENT | _PAGE_NO_EXEC)
+#define __P010	__pgprot(_PAGE_PRESENT | _PAGE_NO_EXEC | _PAGE_NO_READ)
+#define __P011	__pgprot(_PAGE_PRESENT | _PAGE_NO_EXEC)
+#define __P100	__pgprot(_PAGE_PRESENT | _PAGE_NO_READ)
+#define __P101	__pgprot(_PAGE_PRESENT)
+#define __P110	__pgprot(_PAGE_PRESENT | _PAGE_NO_READ)
+#define __P111	__pgprot(_PAGE_PRESENT)
+
+#define __S000	__pgprot(_PAGE_PRESENT | _PAGE_NO_EXEC | _PAGE_NO_READ)
+#define __S001	__pgprot(_PAGE_PRESENT | _PAGE_NO_EXEC)
+#define __S010	__pgprot(_PAGE_PRESENT | _PAGE_NO_EXEC | _PAGE_WRITE | \
+			 _PAGE_NO_READ)
+#define __S011	__pgprot(_PAGE_PRESENT | _PAGE_NO_EXEC | _PAGE_WRITE)
+#define __S100	__pgprot(_PAGE_PRESENT | _PAGE_NO_READ)
+#define __S101	__pgprot(_PAGE_PRESENT)
+#define __S110	__pgprot(_PAGE_PRESENT | _PAGE_WRITE  | _PAGE_NO_READ)
+#define __S111	__pgprot(_PAGE_PRESENT | _PAGE_WRITE)
+#else
 #define PAGE_NONE	__pgprot(_PAGE_PRESENT | _CACHE_CACHABLE_NONCOHERENT)
 #define PAGE_SHARED	__pgprot(_PAGE_PRESENT | _PAGE_READ | _PAGE_WRITE | \
 				 _page_cachable_default)
@@ -34,7 +61,6 @@ struct vm_area_struct;
 				 _page_cachable_default)
 #define PAGE_KERNEL_UNCACHED __pgprot(_PAGE_PRESENT | __READABLE | \
 			__WRITEABLE | _PAGE_GLOBAL | _CACHE_UNCACHED)
-
 /*
  * MIPS can't do page protection for execute, and considers that the same like
  * read. Also, write permissions imply read permissions. This is the closest
@@ -63,6 +89,8 @@ struct vm_area_struct;
 #define __S110 __pgprot(0)
 #define __S111 __pgprot(0)
 
+#endif
+
 extern unsigned long _page_cachable_default;
 
 /*
@@ -250,6 +278,9 @@ static inline int pte_write(pte_t pte)	{ return pte_val(pte) & _PAGE_WRITE; }
 static inline int pte_dirty(pte_t pte)	{ return pte_val(pte) & _PAGE_MODIFIED; }
 static inline int pte_young(pte_t pte)	{ return pte_val(pte) & _PAGE_ACCESSED; }
 static inline int pte_file(pte_t pte)	{ return pte_val(pte) & _PAGE_FILE; }
+#ifdef CONFIG_HUGETLB_PAGE
+static inline int pte_huge(pte_t pte)	{ return pte_val(pte) & _PAGE_HUGE; }
+#endif
 
 static inline pte_t pte_wrprotect(pte_t pte)
 {
@@ -287,12 +318,27 @@ static inline pte_t pte_mkdirty(pte_t pte)
 
 static inline pte_t pte_mkyoung(pte_t pte)
 {
+#ifdef _PAGE_NO_READ
+	pte_val(pte) |= _PAGE_ACCESSED;
+	if (!(pte_val(pte) & _PAGE_NO_READ))
+		pte_val(pte) |= _PAGE_SILENT_READ;
+#else
 	pte_val(pte) |= _PAGE_ACCESSED;
 	if (pte_val(pte) & _PAGE_READ)
 		pte_val(pte) |= _PAGE_SILENT_READ;
+#endif
+	return pte;
+}
+#endif
+
+#ifdef CONFIG_HUGETLB_PAGE
+static inline pte_t pte_mkhuge(pte_t pte)
+{
+	pte_val(pte) |= _PAGE_HUGE;
 	return pte;
 }
 #endif
+
 static inline int pte_special(pte_t pte)	{ return 0; }
 static inline pte_t pte_mkspecial(pte_t pte)	{ return pte; }
 
diff --git a/include/asm-mips/sparsemem.h b/include/asm-mips/sparsemem.h
index 795ac6c..849b74c 100644
--- a/include/asm-mips/sparsemem.h
+++ b/include/asm-mips/sparsemem.h
@@ -6,7 +6,12 @@
  * SECTION_SIZE_BITS		2^N: how big each section will be
  * MAX_PHYSMEM_BITS		2^N: how much memory we can have in that space
  */
+#if defined(CONFIG_PAGE_SIZE_64KB) && defined(CONFIG_HUGETLB_PAGE)
+/* We need bigger sections with 64KB in order to support 512MB hugetlb */
+#define SECTION_SIZE_BITS       29
+#else
 #define SECTION_SIZE_BITS       28
+#endif
 #define MAX_PHYSMEM_BITS        35
 
 #endif /* CONFIG_SPARSEMEM */
diff --git a/include/asm-mips/thread_info.h b/include/asm-mips/thread_info.h
index 9966728..4503ab8 100644
--- a/include/asm-mips/thread_info.h
+++ b/include/asm-mips/thread_info.h
@@ -75,6 +75,9 @@ register struct thread_info *__current_thread_info __asm__("$28");
 #ifdef CONFIG_PAGE_SIZE_16KB
 #define THREAD_SIZE_ORDER (0)
 #endif
+#ifdef CONFIG_PAGE_SIZE_32KB
+#define THREAD_SIZE_ORDER (0)
+#endif
 #ifdef CONFIG_PAGE_SIZE_64KB
 #define THREAD_SIZE_ORDER (0)
 #endif
@@ -124,6 +127,9 @@ register struct thread_info *__current_thread_info __asm__("$28");
 #define TIF_32BIT_REGS		22	/* also implies 16/32 fprs */
 #define TIF_32BIT_ADDR		23	/* 32-bit address space (o32/n32) */
 #define TIF_FPUBOUND		24	/* thread bound to FPU-full CPU set */
+#define TIF_LOAD_WATCH		25	/* If set, load watch registers */
+#define TIF_XKPHYS_MEM_EN	26
+#define TIF_XKPHYS_IO_EN	27
 #define TIF_KERNEL_TRACE	30	/* kernel trace active */
 #define TIF_SYSCALL_TRACE	31	/* syscall trace active */
 
@@ -142,12 +148,15 @@ register struct thread_info *__current_thread_info __asm__("$28");
 #define _TIF_RESTORE_SIGMASK	(1<<TIF_RESTORE_SIGMASK)
 #define _TIF_USEDFPU		(1<<TIF_USEDFPU)
 #define _TIF_POLLING_NRFLAG	(1<<TIF_POLLING_NRFLAG)
+#define _TIF_XKPHYS_MEM_EN	(1<<TIF_XKPHYS_MEM_EN)
+#define _TIF_XKPHYS_IO_EN	(1<<TIF_XKPHYS_IO_EN)
 #define _TIF_FREEZE		(1<<TIF_FREEZE)
 #define _TIF_FIXADE		(1<<TIF_FIXADE)
 #define _TIF_LOGADE		(1<<TIF_LOGADE)
 #define _TIF_32BIT_REGS		(1<<TIF_32BIT_REGS)
 #define _TIF_32BIT_ADDR		(1<<TIF_32BIT_ADDR)
 #define _TIF_FPUBOUND		(1<<TIF_FPUBOUND)
+#define _TIF_LOAD_WATCH		(1<<TIF_LOAD_WATCH)
 
 /* work to do on interrupt/exception return */
 #define _TIF_WORK_MASK		(0x0000ffef & ~_TIF_SECCOMP)
-- 
1.7.0

