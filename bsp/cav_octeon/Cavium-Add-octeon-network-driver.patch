From f1a068988c593727b757f1a0c6f1184bf6032083 Mon Sep 17 00:00:00 2001
From: ltian <le.tian@windriver.com>
Date: Thu, 11 Nov 2010 15:51:07 +0800
Subject: [PATCH 017/132] Cavium: Add octeon network driver.

Source: SDK 2.0.0-366

Add files needed for Octeon's networking needs.

Signed-off-by: ltian <le.tian@windriver.com>
---
 drivers/net/octeon/Kconfig            |   36 ++
 drivers/net/octeon/Makefile           |   28 +
 drivers/net/octeon/ethernet-defines.h |  126 ++++
 drivers/net/octeon/ethernet-mdio.c    |  410 +++++++++++++
 drivers/net/octeon/ethernet-mdio.h    |   45 ++
 drivers/net/octeon/ethernet-mem.c     |  198 +++++++
 drivers/net/octeon/ethernet-mem.h     |   29 +
 drivers/net/octeon/ethernet-proc.c    |  247 ++++++++
 drivers/net/octeon/ethernet-proc.h    |   40 ++
 drivers/net/octeon/ethernet-rgmii.c   |  333 +++++++++++
 drivers/net/octeon/ethernet-rx.c      |  680 ++++++++++++++++++++++
 drivers/net/octeon/ethernet-rx.h      |   52 ++
 drivers/net/octeon/ethernet-sgmii.c   |  104 ++++
 drivers/net/octeon/ethernet-spi.c     |  290 ++++++++++
 drivers/net/octeon/ethernet-srio.c    |  209 +++++++
 drivers/net/octeon/ethernet-tx.c      |  342 +++++++++++
 drivers/net/octeon/ethernet-tx.h      |   31 +
 drivers/net/octeon/ethernet-util.h    |   80 +++
 drivers/net/octeon/ethernet-xaui.c    |  102 ++++
 drivers/net/octeon/ethernet-xmit.c    |  419 ++++++++++++++
 drivers/net/octeon/ethernet.c         | 1026 +++++++++++++++++++++++++++++++++
 drivers/net/octeon/octeon-ethernet.h  |  137 +++++
 22 files changed, 4964 insertions(+), 0 deletions(-)
 create mode 100644 drivers/net/octeon/ethernet-defines.h
 create mode 100644 drivers/net/octeon/ethernet-mdio.c
 create mode 100644 drivers/net/octeon/ethernet-mdio.h
 create mode 100644 drivers/net/octeon/ethernet-mem.c
 create mode 100644 drivers/net/octeon/ethernet-mem.h
 create mode 100644 drivers/net/octeon/ethernet-proc.c
 create mode 100644 drivers/net/octeon/ethernet-proc.h
 create mode 100644 drivers/net/octeon/ethernet-rgmii.c
 create mode 100644 drivers/net/octeon/ethernet-rx.c
 create mode 100644 drivers/net/octeon/ethernet-rx.h
 create mode 100644 drivers/net/octeon/ethernet-sgmii.c
 create mode 100644 drivers/net/octeon/ethernet-spi.c
 create mode 100644 drivers/net/octeon/ethernet-srio.c
 create mode 100644 drivers/net/octeon/ethernet-tx.c
 create mode 100644 drivers/net/octeon/ethernet-tx.h
 create mode 100644 drivers/net/octeon/ethernet-util.h
 create mode 100644 drivers/net/octeon/ethernet-xaui.c
 create mode 100644 drivers/net/octeon/ethernet-xmit.c
 create mode 100644 drivers/net/octeon/ethernet.c
 create mode 100644 drivers/net/octeon/octeon-ethernet.h

diff --git a/drivers/net/octeon/Kconfig b/drivers/net/octeon/Kconfig
index 1e56bbf..bc17953 100644
--- a/drivers/net/octeon/Kconfig
+++ b/drivers/net/octeon/Kconfig
@@ -8,3 +8,39 @@ config OCTEON_MGMT_ETHERNET
 	  This option enables the ethernet driver for the management
 	  port on Cavium Networks' Octeon CN57XX, CN56XX, CN55XX,
 	  CN54XX, CN52XX, and CN6XXX chips.
+
+config OCTEON_NUM_PACKET_BUFFERS
+	int "Number of packet buffers (and work queue entries) for the Ethernet driver"
+	range 0 8192
+	depends on OCTEON_ETHERNET
+	default "1024"
+	help
+	  Number of packet buffers (and work queue entries) to allocate for
+	  the ethernet driver. Zero is treated as 1024.
+
+config OCTEON_ETHERNET
+	tristate "Cavium Networks Octeon Ethernet support"
+	depends on CPU_CAVIUM_OCTEON
+	select PHYLIB
+	select MDIO_OCTEON
+	help
+	  This driver supports the builtin ethernet ports on Cavium
+	  Networks' products in the Octeon family. This driver supports the
+	  CN3XXX, CN5XXX and CN6XXX Octeon processors.
+
+	  To compile this driver as a module, choose M here.  The module
+	  will be called octeon-ethernet.
+
+
+choice
+	prompt "Lockless operations in ethernet driver"
+	default OCTEON_ETHERNET_LOCKLESS_IF_SUPPORTED
+	depends on OCTEON_ETHERNET
+
+config OCTEON_ETHERNET_LOCKLESS_IF_SUPPORTED
+	bool "Lockless if supported"
+
+config OCTEON_ETHERNET_LOCKED
+	bool "Locked operation"
+
+endchoice
diff --git a/drivers/net/octeon/Makefile b/drivers/net/octeon/Makefile
index 906edec..d3566db 100644
--- a/drivers/net/octeon/Makefile
+++ b/drivers/net/octeon/Makefile
@@ -1,2 +1,30 @@
+#
+# Makefile for Octeon specific Ethernet drivers.
+#
+# This file is subject to the terms and conditions of the GNU General Public
+# License.  See the file "COPYING" in the main directory of this archive
+# for more details.
+#
+# Copyright (C) 2010 Cavium Networks
+#
+# Rules to build modules for Octeon ethernet driver, management port and
+# POW module.
+#
 
 obj-$(CONFIG_OCTEON_MGMT_ETHERNET)	+= octeon_mgmt.o
+
+obj-${CONFIG_OCTEON_ETHERNET} +=  octeon-ethernet.o
+
+octeon-ethernet-objs := ethernet.o
+octeon-ethernet-objs += ethernet-mdio.o
+octeon-ethernet-objs += ethernet-mem.o
+octeon-ethernet-objs += ethernet-rgmii.o
+octeon-ethernet-objs += ethernet-rx.o
+octeon-ethernet-objs += ethernet-sgmii.o
+octeon-ethernet-objs += ethernet-spi.o
+octeon-ethernet-objs += ethernet-tx.o
+octeon-ethernet-objs += ethernet-xaui.o
+octeon-ethernet-objs += ethernet-proc.o
+ifdef CONFIG_RAPIDIO
+octeon-ethernet-objs += ethernet-srio.o
+endif
diff --git a/drivers/net/octeon/ethernet-defines.h b/drivers/net/octeon/ethernet-defines.h
new file mode 100644
index 0000000..43d92ed
--- /dev/null
+++ b/drivers/net/octeon/ethernet-defines.h
@@ -0,0 +1,126 @@
+/**********************************************************************
+ * Author: Cavium Networks
+ *
+ * Contact: support@caviumnetworks.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2007 Cavium Networks
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium Networks for more information
+**********************************************************************/
+
+/*
+ * A few defines are used to control the operation of this driver:
+ *  CONFIG_CAVIUM_RESERVE32
+ *      This kernel config options controls the amount of memory configured
+ *      in a wired TLB entry for all processes to share. If this is set, the
+ *      driver will use this memory instead of kernel memory for pools. This
+ *      allows 32bit userspace application to access the buffers, but also
+ *      requires all received packets to be copied.
+ *  CONFIG_OCTEON_NUM_PACKET_BUFFERS
+ *      This kernel config option allows the user to control the number of
+ *      packet and work queue buffers allocated by the driver. If this is zero,
+ *      the driver uses the default from below.
+ *  USE_SKBUFFS_IN_HW
+ *      Tells the driver to populate the packet buffers with kernel skbuffs.
+ *      This allows the driver to receive packets without copying them. It also
+ *      means that 32bit userspace can't access the packet buffers.
+ *  USE_32BIT_SHARED
+ *      This define tells the driver to allocate memory for buffers from the
+ *      32bit sahred region instead of the kernel memory space.
+ *  USE_HW_TCPUDP_CHECKSUM
+ *      Controls if the Octeon TCP/UDP checksum engine is used for packet
+ *      output. If this is zero, the kernel will perform the checksum in
+ *      software.
+ *  USE_ASYNC_IOBDMA
+ *      Use asynchronous IO access to hardware. This uses Octeon's asynchronous
+ *      IOBDMAs to issue IO accesses without stalling. Set this to zero
+ *      to disable this. Note that IOBDMAs require CVMSEG.
+ *  REUSE_SKBUFFS_WITHOUT_FREE
+ *      Allows the TX path to free an skbuff into the FPA hardware pool. This
+ *      can significantly improve performance for forwarding and bridging, but
+ *      may be somewhat dangerous. Checks are made, but if any buffer is reused
+ *      without the proper Linux cleanup, the networking stack may have very
+ *      bizarre bugs.
+ */
+#ifndef __ETHERNET_DEFINES_H__
+#define __ETHERNET_DEFINES_H__
+
+#include <asm/octeon/cvmx-config.h>
+
+
+#define OCTEON_ETHERNET_VERSION "2.0"
+
+#ifndef CONFIG_CAVIUM_RESERVE32
+#define CONFIG_CAVIUM_RESERVE32 0
+#endif
+
+#if CONFIG_CAVIUM_RESERVE32
+#define USE_32BIT_SHARED            1
+#define USE_SKBUFFS_IN_HW           0
+#define REUSE_SKBUFFS_WITHOUT_FREE  0
+#else /* !CONFIG_CAVIUM_RESERVE32 */
+#define USE_32BIT_SHARED            0
+#define USE_SKBUFFS_IN_HW           1
+#ifdef CONFIG_NETFILTER
+#define REUSE_SKBUFFS_WITHOUT_FREE  0
+#else
+#define REUSE_SKBUFFS_WITHOUT_FREE  1
+#endif /* CONFIG_NETFILTER */
+#endif /* CONFIG_CAVIUM_RESERVE32 */
+
+#define USE_HW_TCPUDP_CHECKSUM      1
+
+/* Enable Random Early Dropping under load */
+#define USE_RED                     1
+#define USE_ASYNC_IOBDMA            (CONFIG_CAVIUM_OCTEON_CVMSEG_SIZE > 0)
+
+/*
+ * Allow SW based preamble removal at 10Mbps to workaround PHYs giving
+ * us bad preambles.
+ */
+#define USE_10MBPS_PREAMBLE_WORKAROUND 1
+
+/**
+ * Allow software to configure hardware timestamping even if it
+ * isn't supported by the hardware. This can be used to allow
+ * debugging timestamp code on Octeons that don't support
+ * hardware timestamps. The generated software timestamps will
+ * generally be more accurate than Linux's default implementation,
+ * so this can be useful even in production systems.
+ */
+#define ALLOW_TIMESTAMPS_WITHOUT_HARDWARE 1
+
+/*
+ * Use this to have all FPA frees also tell the L2 not to write data
+ * to memory.
+ */
+#define DONT_WRITEBACK(x)           (x)
+/* Use this to not have FPA frees control L2 */
+/*#define DONT_WRITEBACK(x)         0   */
+
+/* Maximum number of SKBs to try to free per xmit packet. */
+#define MAX_OUT_QUEUE_DEPTH 1000
+
+#define FAU_NUM_PACKET_BUFFERS_TO_FREE (CVMX_FAU_REG_END - sizeof(uint32_t))
+
+#define TOTAL_NUMBER_OF_PORTS       (CVMX_PIP_NUM_INPUT_PORTS+1)
+
+
+#endif /* __ETHERNET_DEFINES_H__ */
diff --git a/drivers/net/octeon/ethernet-mdio.c b/drivers/net/octeon/ethernet-mdio.c
new file mode 100644
index 0000000..f779efb
--- /dev/null
+++ b/drivers/net/octeon/ethernet-mdio.c
@@ -0,0 +1,410 @@
+/**********************************************************************
+ * Author: Cavium Networks
+ *
+ * Contact: support@caviumnetworks.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2007 Cavium Networks
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium Networks for more information
+**********************************************************************/
+#include <linux/kernel.h>
+#include <linux/ethtool.h>
+#include <linux/phy.h>
+#include <linux/net_tstamp.h>
+
+#include <net/dst.h>
+
+#include <asm/octeon/octeon.h>
+#include <asm/octeon/cvmx-helper.h>
+#include <asm/octeon/cvmx-helper-board.h>
+#include <asm/octeon/cvmx-srio.h>
+
+#include <asm/octeon/cvmx-smix-defs.h>
+#include <asm/octeon/cvmx-pko-defs.h>
+#include <asm/octeon/cvmx-gmxx-defs.h>
+
+#include "ethernet-defines.h"
+#include "octeon-ethernet.h"
+#include "ethernet-mdio.h"
+#include "ethernet-util.h"
+
+
+static void cvm_oct_get_drvinfo(struct net_device *dev,
+				struct ethtool_drvinfo *info)
+{
+	strcpy(info->driver, "octeon-ethernet");
+	strcpy(info->version, OCTEON_ETHERNET_VERSION);
+	strcpy(info->bus_info, "Builtin");
+}
+
+static int cvm_oct_get_settings(struct net_device *dev, struct ethtool_cmd *cmd)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	if (priv->phydev)
+		return phy_ethtool_gset(priv->phydev, cmd);
+
+	return -EINVAL;
+}
+
+static int cvm_oct_set_settings(struct net_device *dev, struct ethtool_cmd *cmd)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	if (!capable(CAP_NET_ADMIN))
+		return -EPERM;
+
+	if (priv->phydev)
+		return phy_ethtool_sset(priv->phydev, cmd);
+
+	return -EINVAL;
+}
+
+static int cvm_oct_nway_reset(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	if (!capable(CAP_NET_ADMIN))
+		return -EPERM;
+
+	if (priv->phydev)
+		return phy_start_aneg(priv->phydev);
+
+	return -EINVAL;
+}
+
+const struct ethtool_ops cvm_oct_ethtool_ops = {
+	.get_drvinfo = cvm_oct_get_drvinfo,
+	.get_settings = cvm_oct_get_settings,
+	.set_settings = cvm_oct_set_settings,
+	.nway_reset = cvm_oct_nway_reset,
+	.get_link = ethtool_op_get_link,
+	.get_sg = ethtool_op_get_sg,
+	.get_tx_csum = ethtool_op_get_tx_csum,
+};
+
+/**
+ * cvm_oct_ioctl_hwtstamp - IOCTL support for timestamping
+ * @dev:    Device to change
+ * @rq:     the request
+ * @cmd:    the command
+ *
+ * Returns Zero on success
+ */
+static int cvm_oct_ioctl_hwtstamp(struct net_device *dev,
+	struct ifreq *rq, int cmd)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	struct hwtstamp_config config;
+	union cvmx_mio_ptp_clock_cfg ptp;
+	int have_hw_timestamps = 0;
+
+	if (copy_from_user(&config, rq->ifr_data, sizeof(config)))
+		return -EFAULT;
+
+	if (config.flags) /* reserved for future extensions */
+		return -EINVAL;
+
+	/* Check the status of hardware for tiemstamps */
+	if (OCTEON_IS_MODEL(OCTEON_CN6XXX)) {
+		/* Write TX timestamp into word 4 */
+		cvmx_write_csr(CVMX_PKO_REG_TIMESTAMP, 4);
+		/* Get the current state of the PTP clock */
+		ptp.u64 = cvmx_read_csr(CVMX_MIO_PTP_CLOCK_CFG);
+		if (!ptp.s.ext_clk_en) {
+			/*
+			 * The clock has not been configured to use an
+			 * external source.  Program it to use the main clock
+			 * reference.
+			 */
+			unsigned long long clock_comp = (NSEC_PER_SEC << 32) /
+				octeon_get_io_clock_rate();
+			cvmx_write_csr(CVMX_MIO_PTP_CLOCK_COMP, clock_comp);
+			pr_info("PTP Clock: Using sclk reference at %lld Hz\n",
+				(NSEC_PER_SEC << 32) / clock_comp);
+		} else {
+			/* The clock is already programmed to use a GPIO */
+			unsigned long long clock_comp = cvmx_read_csr(
+				CVMX_MIO_PTP_CLOCK_COMP);
+			pr_info("PTP Clock: Using GPIO %d at %lld Hz\n",
+				ptp.s.ext_clk_in,
+				(NSEC_PER_SEC << 32) / clock_comp);
+		}
+
+		/* Enable the clock if it wasn't done already */
+		if (!ptp.s.ptp_en) {
+			ptp.s.ptp_en = 1;
+			cvmx_write_csr(CVMX_MIO_PTP_CLOCK_CFG, ptp.u64);
+		}
+		have_hw_timestamps = 1;
+		/* Only the first two interfaces support hardware timestamps */
+		if (priv->port >= 32)
+			have_hw_timestamps = 0;
+	}
+
+	/* Require hardware if ALLOW_TIMESTAMPS_WITHOUT_HARDWARE=0 */
+	if (!ALLOW_TIMESTAMPS_WITHOUT_HARDWARE && !have_hw_timestamps)
+		return -EINVAL;
+
+	switch (config.tx_type) {
+	case HWTSTAMP_TX_OFF:
+		priv->flags &= ~(OCTEON_ETHERNET_FLAG_TX_TIMESTAMP_SW |
+				 OCTEON_ETHERNET_FLAG_TX_TIMESTAMP_HW);
+		break;
+	case HWTSTAMP_TX_ON:
+		priv->flags |= (have_hw_timestamps) ?
+			OCTEON_ETHERNET_FLAG_TX_TIMESTAMP_HW :
+			OCTEON_ETHERNET_FLAG_TX_TIMESTAMP_SW;
+		break;
+	default:
+		return -ERANGE;
+	}
+
+	switch (config.rx_filter) {
+	case HWTSTAMP_FILTER_NONE:
+		priv->flags &= ~(OCTEON_ETHERNET_FLAG_RX_TIMESTAMP_HW |
+				 OCTEON_ETHERNET_FLAG_RX_TIMESTAMP_SW);
+		if (have_hw_timestamps) {
+			int interface = INTERFACE(priv->port);
+			int index = INDEX(priv->port);
+			union cvmx_gmxx_rxx_frm_ctl gmxx_rxx_frm_ctl;
+			union cvmx_pip_prt_cfgx pip_prt_cfgx;
+
+			gmxx_rxx_frm_ctl.u64 = cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface));
+			gmxx_rxx_frm_ctl.s.ptp_mode = 0;
+			cvmx_write_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface), gmxx_rxx_frm_ctl.u64);
+
+			pip_prt_cfgx.u64 = cvmx_read_csr(CVMX_PIP_PRT_CFGX(priv->port));
+			pip_prt_cfgx.s.skip = 0;
+			cvmx_write_csr(CVMX_PIP_PRT_CFGX(priv->port), pip_prt_cfgx.u64);
+		}
+		break;
+	case HWTSTAMP_FILTER_ALL:
+	case HWTSTAMP_FILTER_SOME:
+	case HWTSTAMP_FILTER_PTP_V1_L4_EVENT:
+	case HWTSTAMP_FILTER_PTP_V1_L4_SYNC:
+	case HWTSTAMP_FILTER_PTP_V1_L4_DELAY_REQ:
+	case HWTSTAMP_FILTER_PTP_V2_L4_EVENT:
+	case HWTSTAMP_FILTER_PTP_V2_L4_SYNC:
+	case HWTSTAMP_FILTER_PTP_V2_L4_DELAY_REQ:
+	case HWTSTAMP_FILTER_PTP_V2_L2_EVENT:
+	case HWTSTAMP_FILTER_PTP_V2_L2_SYNC:
+	case HWTSTAMP_FILTER_PTP_V2_L2_DELAY_REQ:
+	case HWTSTAMP_FILTER_PTP_V2_EVENT:
+	case HWTSTAMP_FILTER_PTP_V2_SYNC:
+	case HWTSTAMP_FILTER_PTP_V2_DELAY_REQ:
+		priv->flags |= (have_hw_timestamps) ?
+			OCTEON_ETHERNET_FLAG_RX_TIMESTAMP_HW :
+			OCTEON_ETHERNET_FLAG_RX_TIMESTAMP_SW;
+		config.rx_filter = HWTSTAMP_FILTER_ALL;
+		if (have_hw_timestamps) {
+			int interface = INTERFACE(priv->port);
+			int index = INDEX(priv->port);
+			union cvmx_gmxx_rxx_frm_ctl gmxx_rxx_frm_ctl;
+			union cvmx_pip_prt_cfgx pip_prt_cfgx;
+
+			gmxx_rxx_frm_ctl.u64 = cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface));
+			gmxx_rxx_frm_ctl.s.ptp_mode = 1;
+			cvmx_write_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface), gmxx_rxx_frm_ctl.u64);
+
+			pip_prt_cfgx.u64 = cvmx_read_csr(CVMX_PIP_PRT_CFGX(priv->port));
+			pip_prt_cfgx.s.skip = 8;
+			cvmx_write_csr(CVMX_PIP_PRT_CFGX(priv->port), pip_prt_cfgx.u64);
+		}
+		break;
+	default:
+		return -ERANGE;
+	}
+
+	if (copy_to_user(rq->ifr_data, &config, sizeof(config)))
+		return -EFAULT;
+
+	return 0;
+}
+
+/**
+ * cvm_oct_ioctl - IOCTL support for PHY control
+ * @dev:    Device to change
+ * @rq:     the request
+ * @cmd:    the command
+ *
+ * Returns Zero on success
+ */
+int cvm_oct_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	cvmx_srio_tx_message_header_t tx_header;
+	int ivalue;
+
+	switch (cmd) {
+	case CAVIUM_NET_IOCTL_SETPRIO:
+		ivalue = rq->ifr_ifru.ifru_ivalue;
+		if ((ivalue >= 0) && (ivalue < 4)) {
+			tx_header.u64 = priv->srio_tx_header;
+			tx_header.s.prio = ivalue;
+			priv->srio_tx_header = tx_header.u64;
+			return 0;
+		}
+		return -EINVAL;
+
+	case CAVIUM_NET_IOCTL_GETPRIO:
+		tx_header.u64 = priv->srio_tx_header;
+		rq->ifr_ifru.ifru_ivalue = tx_header.s.prio;
+		return 0;
+
+	case CAVIUM_NET_IOCTL_SETIDSIZE:
+		ivalue = rq->ifr_ifru.ifru_ivalue;
+		if ((ivalue >= 0) && (ivalue < 2)) {
+			tx_header.u64 = priv->srio_tx_header;
+			tx_header.s.tt = ivalue;
+			priv->srio_tx_header = tx_header.u64;
+			return 0;
+		}
+		return -EINVAL;
+
+	case CAVIUM_NET_IOCTL_GETIDSIZE:
+		tx_header.u64 = priv->srio_tx_header;
+		rq->ifr_ifru.ifru_ivalue = tx_header.s.tt;
+		return 0;
+
+	case CAVIUM_NET_IOCTL_SETSRCID:
+		ivalue = rq->ifr_ifru.ifru_ivalue;
+		if ((ivalue >= 0) && (ivalue < 2)) {
+			tx_header.u64 = priv->srio_tx_header;
+			tx_header.s.sis = ivalue;
+			priv->srio_tx_header = tx_header.u64;
+			return 0;
+		}
+		return -EINVAL;
+
+	case CAVIUM_NET_IOCTL_GETSRCID:
+		tx_header.u64 = priv->srio_tx_header;
+		rq->ifr_ifru.ifru_ivalue = tx_header.s.sis;
+		return 0;
+
+	case CAVIUM_NET_IOCTL_SETLETTER:
+		ivalue = rq->ifr_ifru.ifru_ivalue;
+		if ((ivalue >= -1) && (ivalue < 4)) {
+			tx_header.u64 = priv->srio_tx_header;
+			tx_header.s.lns = (ivalue == -1);
+			if (tx_header.s.lns)
+				tx_header.s.letter = 0;
+			else
+				tx_header.s.letter = ivalue;
+			priv->srio_tx_header = tx_header.u64;
+			return 0;
+		}
+		return -EINVAL;
+
+	case CAVIUM_NET_IOCTL_GETLETTER:
+		tx_header.u64 = priv->srio_tx_header;
+		if (tx_header.s.lns)
+			rq->ifr_ifru.ifru_ivalue = -1;
+		else
+			rq->ifr_ifru.ifru_ivalue = tx_header.s.letter;
+		return 0;
+
+	case SIOCSHWTSTAMP:
+		return cvm_oct_ioctl_hwtstamp(dev, rq, cmd);
+
+	default:
+		if (priv->phydev)
+			return phy_mii_ioctl(priv->phydev, if_mii(rq), cmd);
+		return -EINVAL;
+	}
+}
+
+/**
+ * cvm_oct_set_carrier - common wrapper of netif_carrier_{on,off}
+ *
+ * @priv: Device struct.
+ * @link_info: Current state.
+ */
+void cvm_oct_set_carrier(struct octeon_ethernet *priv, cvmx_helper_link_info_t link_info)
+{
+	if (link_info.s.link_up) {
+		if (!netif_carrier_ok(priv->netdev))
+			netif_carrier_on(priv->netdev);
+		if (priv->num_tx_queues)
+			DEBUGPRINT("%s: %u Mbps %s duplex, port %2d, queue %2d\n",
+				   priv->netdev->name, link_info.s.speed,
+				   (link_info.s.full_duplex) ? "Full" : "Half",
+				   priv->port, priv->tx_queue[0].queue);
+		else
+			DEBUGPRINT("%s: %u Mbps %s duplex, port %2d, POW\n",
+				   priv->netdev->name, link_info.s.speed,
+				   (link_info.s.full_duplex) ? "Full" : "Half",
+				   priv->port);
+	} else {
+		if (netif_carrier_ok(priv->netdev))
+			netif_carrier_off(priv->netdev);
+		DEBUGPRINT("%s: Link down\n", priv->netdev->name);
+	}
+}
+
+static void cvm_oct_adjust_link(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	cvmx_helper_link_info_t link_info;
+
+	if (priv->last_link != priv->phydev->link) {
+		priv->last_link = priv->phydev->link;
+		link_info.u64 = 0;
+		link_info.s.link_up = priv->last_link ? 1 : 0;
+		link_info.s.full_duplex = priv->phydev->duplex ? 1 : 0;
+		link_info.s.speed = priv->phydev->speed;
+		cvmx_helper_link_set(priv->port, link_info);
+		cvm_oct_set_carrier(priv, link_info);
+	}
+}
+
+
+/**
+ * cvm_oct_phy_setup_device - setup the PHY
+ *
+ * @dev:    Device to setup
+ *
+ * Returns Zero on success, negative on failure
+ */
+int cvm_oct_phy_setup_device(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	int phy_addr = cvmx_helper_board_get_mii_address(priv->port);
+	if (phy_addr != -1) {
+		char phy_id[20];
+
+		if (phy_addr & 0x100)
+			snprintf(phy_id, sizeof(phy_id), PHY_ID_FMT, "1", phy_addr & 0xff);
+		else
+			snprintf(phy_id, sizeof(phy_id), PHY_ID_FMT, "0", phy_addr);
+
+		priv->phydev = phy_connect(dev, phy_id, cvm_oct_adjust_link, 0,
+					   PHY_INTERFACE_MODE_GMII);
+
+		if (IS_ERR(priv->phydev)) {
+			priv->phydev = NULL;
+			return -1;
+		}
+		priv->last_link = 0;
+		phy_start_aneg(priv->phydev);
+	}
+	return 0;
+}
diff --git a/drivers/net/octeon/ethernet-mdio.h b/drivers/net/octeon/ethernet-mdio.h
new file mode 100644
index 0000000..a417d4f
--- /dev/null
+++ b/drivers/net/octeon/ethernet-mdio.h
@@ -0,0 +1,45 @@
+/*********************************************************************
+ * Author: Cavium Networks
+ *
+ * Contact: support@caviumnetworks.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2007 Cavium Networks
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium Networks for more information
+*********************************************************************/
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/init.h>
+#include <linux/etherdevice.h>
+#include <linux/ip.h>
+#include <linux/string.h>
+#include <linux/ethtool.h>
+#include <linux/seq_file.h>
+#include <linux/proc_fs.h>
+#include <net/dst.h>
+#ifdef CONFIG_XFRM
+#include <linux/xfrm.h>
+#include <net/xfrm.h>
+#endif /* CONFIG_XFRM */
+
+extern const struct ethtool_ops cvm_oct_ethtool_ops;
+int cvm_oct_ioctl(struct net_device *dev, struct ifreq *rq, int cmd);
+int cvm_oct_phy_setup_device(struct net_device *dev);
diff --git a/drivers/net/octeon/ethernet-mem.c b/drivers/net/octeon/ethernet-mem.c
new file mode 100644
index 0000000..0073a24
--- /dev/null
+++ b/drivers/net/octeon/ethernet-mem.c
@@ -0,0 +1,198 @@
+/**********************************************************************
+ * Author: Cavium Networks
+ *
+ * Contact: support@caviumnetworks.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2010 Cavium Networks
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium Networks for more information
+**********************************************************************/
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+
+#include <asm/octeon/octeon.h>
+#include <asm/octeon/cvmx-fpa.h>
+
+#include "ethernet-defines.h"
+
+#if USE_32BIT_SHARED
+#include <asm/octeon/cvmx-bootmem.h>
+#endif
+
+/**
+ * cvm_oct_fill_hw_skbuff - fill the supplied hardware pool with skbuffs
+ * @pool:     Pool to allocate an skbuff for
+ * @size:     Size of the buffer needed for the pool
+ * @elements: Number of buffers to allocate
+ *
+ * Returns the actual number of buffers allocated.
+ */
+static int cvm_oct_fill_hw_skbuff(int pool, int size, int elements)
+{
+	int freed = elements;
+	while (freed) {
+
+		struct sk_buff *skb = dev_alloc_skb(size + 256);
+		if (unlikely(skb == NULL)) {
+			pr_warning("Failed to allocate skb for hardware pool %d\n", pool);
+			break;
+		}
+
+		skb_reserve(skb, 256 - (((unsigned long)skb->data) & 0x7f));
+		*(struct sk_buff **)(skb->data - sizeof(void *)) = skb;
+		cvmx_fpa_free(skb->data, pool, DONT_WRITEBACK(size / 128));
+		freed--;
+	}
+	return elements - freed;
+}
+
+/**
+ * cvm_oct_free_hw_skbuff- free hardware pool skbuffs
+ * @pool:     Pool to allocate an skbuff for
+ * @size:     Size of the buffer needed for the pool
+ * @elements: Number of buffers to allocate
+ */
+static void cvm_oct_free_hw_skbuff(int pool, int size, int elements)
+{
+	char *memory;
+
+	do {
+		memory = cvmx_fpa_alloc(pool);
+		if (memory) {
+			struct sk_buff *skb =
+			    *(struct sk_buff **)(memory - sizeof(void *));
+			elements--;
+			dev_kfree_skb(skb);
+		}
+	} while (memory);
+
+	if (elements < 0)
+		pr_warning("Freeing of pool %u had too many skbuffs (%d)\n", pool, elements);
+	else if (elements > 0)
+		pr_warning("Freeing of pool %u is missing %d skbuffs\n", pool, elements);
+}
+
+/**
+ * cvm_oct_fill_hw_memory - fill a hardware pool with memory.
+ * @pool:     Pool to populate
+ * @size:     Size of each buffer in the pool
+ * @elements: Number of buffers to allocate
+ *
+ * Returns the actual number of buffers allocated.
+ */
+static int cvm_oct_fill_hw_memory(int pool, int size, int elements)
+{
+	char *memory;
+	char *fpa;
+	int freed = elements;
+
+	if (USE_32BIT_SHARED) {
+		extern uint64_t octeon_reserve32_memory;
+		memory = cvmx_bootmem_alloc_range(elements*size, 128, octeon_reserve32_memory,
+			    octeon_reserve32_memory + (CONFIG_CAVIUM_RESERVE32<<20) - 1);
+
+		if (memory == NULL)
+			panic("Unable to allocate %u bytes for FPA pool %d\n", elements*size, pool);
+
+		printk(KERN_INFO "Memory range %p - %p reserved for hardware\n",
+			memory, memory + elements*size - 1);
+
+		while (freed) {
+			cvmx_fpa_free(memory, pool, 0);
+			memory += size;
+			freed--;
+		}
+		return elements - freed;
+	}
+	while (freed) {
+		/*
+		 * FPA memory must be 128 byte aligned.  Since we are
+		 * aligning we need to save the original pointer so we
+		 * can feed it to kfree when the memory is returned to
+		 * the kernel.
+		 *
+		 * We allocate an extra 256 bytes to allow for
+		 * alignment and space for the original pointer saved
+		 * just before the block.
+		 */
+		memory = kmalloc(size + 256, GFP_ATOMIC);
+		if (unlikely(memory == NULL)) {
+			pr_warning("Unable to allocate %u bytes for FPA pool %d\n",
+				   elements * size, pool);
+			break;
+		}
+		fpa = (char *)(((unsigned long)memory + 256) & ~0x7fUL);
+		*((char **)fpa - 1) = memory;
+		cvmx_fpa_free(fpa, pool, 0);
+		freed--;
+	}
+	return elements - freed;
+}
+
+/**
+ * cvm_oct_free_hw_memory - Free memory allocated by cvm_oct_fill_hw_memory
+ * @pool:     FPA pool to free
+ * @size:     Size of each buffer in the pool
+ * @elements: Number of buffers that should be in the pool
+ */
+static void cvm_oct_free_hw_memory(int pool, int size, int elements)
+{
+	char *memory;
+	char *fpa;
+	if (USE_32BIT_SHARED) {
+		printk(KERN_WARNING "Warning: 32 shared memory is not freeable\n");
+		return;
+	}
+
+	do {
+		fpa = cvmx_fpa_alloc(pool);
+		if (fpa) {
+			elements--;
+			fpa = (char *)phys_to_virt(cvmx_ptr_to_phys(fpa));
+			memory = *((char **)fpa - 1);
+			kfree(memory);
+		}
+	} while (fpa);
+
+	if (elements < 0)
+		pr_warning("Freeing of pool %u had too many buffers (%d)\n",
+			pool, elements);
+	else if (elements > 0)
+		pr_warning("Warning: Freeing of pool %u is missing %d buffers\n",
+			pool, elements);
+}
+
+int cvm_oct_mem_fill_fpa(int pool, int size, int elements)
+{
+	int freed;
+	if (USE_SKBUFFS_IN_HW && pool == CVMX_FPA_PACKET_POOL)
+		freed = cvm_oct_fill_hw_skbuff(pool, size, elements);
+	else
+		freed = cvm_oct_fill_hw_memory(pool, size, elements);
+	return freed;
+}
+
+void cvm_oct_mem_empty_fpa(int pool, int size, int elements)
+{
+	if (USE_SKBUFFS_IN_HW && pool == CVMX_FPA_PACKET_POOL)
+		cvm_oct_free_hw_skbuff(pool, size, elements);
+	else
+		cvm_oct_free_hw_memory(pool, size, elements);
+}
diff --git a/drivers/net/octeon/ethernet-mem.h b/drivers/net/octeon/ethernet-mem.h
new file mode 100644
index 0000000..713f2ed
--- /dev/null
+++ b/drivers/net/octeon/ethernet-mem.h
@@ -0,0 +1,29 @@
+/*********************************************************************
+ * Author: Cavium Networks
+ *
+ * Contact: support@caviumnetworks.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2007 Cavium Networks
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium Networks for more information
+********************************************************************/
+
+int cvm_oct_mem_fill_fpa(int pool, int size, int elements);
+void cvm_oct_mem_empty_fpa(int pool, int size, int elements);
diff --git a/drivers/net/octeon/ethernet-proc.c b/drivers/net/octeon/ethernet-proc.c
new file mode 100644
index 0000000..812842f
--- /dev/null
+++ b/drivers/net/octeon/ethernet-proc.c
@@ -0,0 +1,247 @@
+/*************************************************************************
+* Cavium Octeon Ethernet Driver
+*
+* Author: Cavium Networks info@caviumnetworks.com
+*
+* Copyright (c) 2003-2010  Cavium Networks (support@cavium.com). All rights
+* reserved.
+*
+* Redistribution and use in source and binary forms, with or without
+* modification, are permitted provided that the following conditions are
+* met:
+*
+*     * Redistributions of source code must retain the above copyright
+*       notice, this list of conditions and the following disclaimer.
+*
+*     * Redistributions in binary form must reproduce the above
+*       copyright notice, this list of conditions and the following
+*       disclaimer in the documentation and/or other materials provided
+*       with the distribution.
+*
+*     * Neither the name of Cavium Networks nor the names of
+*       its contributors may be used to endorse or promote products
+*       derived from this software without specific prior written
+*       permission.
+*
+ TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+* AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+* OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+* RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+* REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+* DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+* OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+* PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+* POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+* OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+*************************************************************************/
+#include <linux/kernel.h>
+#include <linux/phy.h>
+#include <linux/seq_file.h>
+#include <linux/proc_fs.h>
+#include <net/dst.h>
+
+#include "octeon-ethernet.h"
+#include "ethernet-proc.h"
+
+static unsigned long long cvm_oct_stats_read_switch(struct mii_bus *mii_bus, int phy_id, int offset)
+{
+	mii_bus->write(mii_bus, phy_id, 0x1d, 0xcc00 | offset);
+	return ((uint64_t)mii_bus->read(mii_bus, phy_id, 0x1e)<<16) | (uint64_t)mii_bus->read(mii_bus, phy_id, 0x1f);
+}
+
+static int cvm_oct_stats_switch_show(struct octeon_ethernet *priv, struct seq_file *m, void *v)
+{
+	static const int ports[] = {0, 1, 2, 3, 9, -1};
+	int index = 0;
+
+	while (ports[index] != -1) {
+
+		/* Latch port */
+		struct mii_bus *mii_bus = priv->mii_bus;
+		if (mii_bus == NULL)
+			break;
+		mutex_lock(&mii_bus->mdio_lock);
+
+		priv->mii_bus->write(mii_bus, 0x1b, 0x1d, 0xdc00 | ports[index]);
+		seq_printf(m, "\nSwitch Port %d\n", ports[index]);
+		seq_printf(m, "InGoodOctets:   %12llu\t"
+			   "OutOctets:      %12llu\t"
+			   "64 Octets:      %12llu\n",
+			   cvm_oct_stats_read_switch(mii_bus, 0x1b, 0x00) | (cvm_oct_stats_read_switch(mii_bus, 0x1b, 0x01)<<32),
+			   cvm_oct_stats_read_switch(mii_bus, 0x1b, 0x0E) | (cvm_oct_stats_read_switch(mii_bus, 0x1b, 0x0F)<<32),
+			   cvm_oct_stats_read_switch(mii_bus, 0x1b, 0x08));
+
+		seq_printf(m, "InBadOctets:    %12llu\t"
+			   "OutUnicast:     %12llu\t"
+			   "65-127 Octets:  %12llu\n",
+			   cvm_oct_stats_read_switch(mii_bus, 0x1b, 0x02),
+			   cvm_oct_stats_read_switch(mii_bus, 0x1b, 0x10),
+			   cvm_oct_stats_read_switch(mii_bus, 0x1b, 0x09));
+
+		seq_printf(m, "InUnicast:      %12llu\t"
+			   "OutBroadcasts:  %12llu\t"
+			   "128-255 Octets: %12llu\n",
+			   cvm_oct_stats_read_switch(mii_bus, 0x1b, 0x04),
+			   cvm_oct_stats_read_switch(mii_bus, 0x1b, 0x13),
+			   cvm_oct_stats_read_switch(mii_bus, 0x1b, 0x0A));
+
+		seq_printf(m, "InBroadcasts:   %12llu\t"
+			   "OutMulticasts:  %12llu\t"
+			   "256-511 Octets: %12llu\n",
+			   cvm_oct_stats_read_switch(mii_bus, 0x1b, 0x06),
+			   cvm_oct_stats_read_switch(mii_bus, 0x1b, 0x12),
+			   cvm_oct_stats_read_switch(mii_bus, 0x1b, 0x0B));
+
+		seq_printf(m, "InMulticasts:   %12llu\t"
+			   "OutPause:       %12llu\t"
+			   "512-1023 Octets:%12llu\n",
+			   cvm_oct_stats_read_switch(mii_bus, 0x1b, 0x07),
+			   cvm_oct_stats_read_switch(mii_bus, 0x1b, 0x15),
+			   cvm_oct_stats_read_switch(mii_bus, 0x1b, 0x0C));
+
+		seq_printf(m, "InPause:        %12llu\t"
+			   "Excessive:      %12llu\t"
+			   "1024-Max Octets:%12llu\n",
+			   cvm_oct_stats_read_switch(mii_bus, 0x1b, 0x16),
+			   cvm_oct_stats_read_switch(mii_bus, 0x1b, 0x11),
+			   cvm_oct_stats_read_switch(mii_bus, 0x1b, 0x0D));
+
+		seq_printf(m, "InUndersize:    %12llu\t"
+			   "Collisions:     %12llu\n",
+			   cvm_oct_stats_read_switch(mii_bus, 0x1b, 0x18),
+			   cvm_oct_stats_read_switch(mii_bus, 0x1b, 0x1E));
+
+		seq_printf(m, "InFragments:    %12llu\t"
+			   "Deferred:       %12llu\n",
+			   cvm_oct_stats_read_switch(mii_bus, 0x1b, 0x19),
+			   cvm_oct_stats_read_switch(mii_bus, 0x1b, 0x05));
+
+		seq_printf(m, "InOversize:     %12llu\t"
+			   "Single:         %12llu\n",
+			   cvm_oct_stats_read_switch(mii_bus, 0x1b, 0x1A),
+			   cvm_oct_stats_read_switch(mii_bus, 0x1b, 0x14));
+
+		seq_printf(m, "InJabber:       %12llu\t"
+			   "Multiple:       %12llu\n",
+			   cvm_oct_stats_read_switch(mii_bus, 0x1b, 0x1B),
+			   cvm_oct_stats_read_switch(mii_bus, 0x1b, 0x17));
+
+		seq_printf(m, "In RxErr:       %12llu\t"
+			   "OutFCSErr:      %12llu\n",
+			   cvm_oct_stats_read_switch(mii_bus, 0x1b, 0x1C),
+			   cvm_oct_stats_read_switch(mii_bus, 0x1b, 0x03));
+
+		seq_printf(m, "InFCSErr:       %12llu\t"
+			   "Late:           %12llu\n",
+			   cvm_oct_stats_read_switch(mii_bus, 0x1b, 0x1D),
+			   cvm_oct_stats_read_switch(mii_bus, 0x1b, 0x1F));
+		index++;
+		mutex_unlock(&mii_bus->mdio_lock);
+	}
+	return 0;
+}
+
+/**
+ * User is reading /proc/octeon_ethernet_stats
+ *
+ * @param m
+ * @param v
+ * @return
+ */
+static int cvm_oct_stats_show(struct seq_file *m, void *v)
+{
+	struct octeon_ethernet *priv;
+	list_for_each_entry(priv, &cvm_oct_list, list) {
+		seq_printf(m, "\nOcteon Port %d (%s)\n", priv->port, priv->netdev->name);
+		seq_printf(m,
+			   "rx_packets:             %12lu\t"
+			   "tx_packets:             %12lu\n",
+			   priv->netdev->stats.rx_packets,
+			   priv->netdev->stats.tx_packets);
+		seq_printf(m,
+			   "rx_bytes:               %12lu\t"
+			   "tx_bytes:               %12lu\n",
+			   priv->netdev->stats.rx_bytes,
+			   priv->netdev->stats.tx_bytes);
+		seq_printf(m,
+			   "rx_errors:              %12lu\t"
+			   "tx_errors:              %12lu\n",
+			   priv->netdev->stats.rx_errors,
+			   priv->netdev->stats.tx_errors);
+		seq_printf(m,
+			   "rx_dropped:             %12lu\t"
+			   "tx_dropped:             %12lu\n",
+			   priv->netdev->stats.rx_dropped,
+			   priv->netdev->stats.tx_dropped);
+		seq_printf(m,
+			   "rx_length_errors:       %12lu\t"
+			   "tx_aborted_errors:      %12lu\n",
+			   priv->netdev->stats.rx_length_errors,
+			   priv->netdev->stats.tx_aborted_errors);
+		seq_printf(m,
+			   "rx_over_errors:         %12lu\t"
+			   "tx_carrier_errors:      %12lu\n",
+			   priv->netdev->stats.rx_over_errors,
+			   priv->netdev->stats.tx_carrier_errors);
+		seq_printf(m,
+			   "rx_crc_errors:          %12lu\t"
+			   "tx_fifo_errors:         %12lu\n",
+			   priv->netdev->stats.rx_crc_errors,
+			   priv->netdev->stats.tx_fifo_errors);
+		seq_printf(m,
+			   "rx_frame_errors:        %12lu\t"
+			   "tx_heartbeat_errors:    %12lu\n",
+			   priv->netdev->stats.rx_frame_errors,
+			   priv->netdev->stats.tx_heartbeat_errors);
+		seq_printf(m,
+			   "rx_fifo_errors:         %12lu\t"
+			   "tx_window_errors:       %12lu\n",
+			   priv->netdev->stats.rx_fifo_errors,
+			   priv->netdev->stats.tx_window_errors);
+		seq_printf(m,
+			   "rx_missed_errors:       %12lu\t"
+			   "multicast:              %12lu\n",
+			   priv->netdev->stats.rx_missed_errors,
+			   priv->netdev->stats.multicast);
+	}
+
+	priv = list_first_entry(&cvm_oct_list, struct octeon_ethernet, list);
+	if ((priv->port == 0) && (priv->imode == CVMX_HELPER_INTERFACE_MODE_GMII))
+		cvm_oct_stats_switch_show(priv, m, v);
+	return 0;
+}
+
+
+/**
+ * /proc/octeon_ethernet_stats was openned. Use the single_open iterator
+ *
+ * @param inode
+ * @param file
+ * @return
+ */
+static int cvm_oct_stats_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, cvm_oct_stats_show, NULL);
+}
+
+
+static const struct file_operations cvm_oct_stats_operations = {
+	.open		= cvm_oct_stats_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+
+void cvm_oct_proc_initialize(void)
+{
+	struct proc_dir_entry *entry = create_proc_entry("octeon_ethernet_stats", 0, NULL);
+	if (entry)
+		entry->proc_fops = &cvm_oct_stats_operations;
+}
+
+void cvm_oct_proc_shutdown(void)
+{
+	remove_proc_entry("octeon_ethernet_stats", NULL);
+}
+
diff --git a/drivers/net/octeon/ethernet-proc.h b/drivers/net/octeon/ethernet-proc.h
new file mode 100644
index 0000000..7fa9372
--- /dev/null
+++ b/drivers/net/octeon/ethernet-proc.h
@@ -0,0 +1,40 @@
+/*************************************************************************
+* Cavium Octeon Ethernet Driver
+*
+* Author: Cavium Networks info@caviumnetworks.com
+*
+* Copyright (c) 2003-2010  Cavium Networks (support@cavium.com). All rights
+* reserved.
+*
+* Redistribution and use in source and binary forms, with or without
+* modification, are permitted provided that the following conditions are
+* met:
+*
+*     * Redistributions of source code must retain the above copyright
+*       notice, this list of conditions and the following disclaimer.
+*
+*     * Redistributions in binary form must reproduce the above
+*       copyright notice, this list of conditions and the following
+*       disclaimer in the documentation and/or other materials provided
+*       with the distribution.
+*
+*     * Neither the name of Cavium Networks nor the names of
+*       its contributors may be used to endorse or promote products
+*       derived from this software without specific prior written
+*       permission.
+*
+* TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+* AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+* OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+* RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+* REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+* DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+* OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+* PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+* POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+* OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+*************************************************************************/
+
+void cvm_oct_proc_initialize(void);
+void cvm_oct_proc_shutdown(void);
+
diff --git a/drivers/net/octeon/ethernet-rgmii.c b/drivers/net/octeon/ethernet-rgmii.c
new file mode 100644
index 0000000..6eccaf0
--- /dev/null
+++ b/drivers/net/octeon/ethernet-rgmii.c
@@ -0,0 +1,333 @@
+/*********************************************************************
+ * Author: Cavium Networks
+ *
+ * Contact: support@caviumnetworks.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2007 Cavium Networks
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium Networks for more information
+**********************************************************************/
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/phy.h>
+#include <net/dst.h>
+
+#include <asm/octeon/octeon.h>
+#include <asm/octeon/cvmx-helper.h>
+
+#include <asm/octeon/cvmx-ipd-defs.h>
+#include <asm/octeon/cvmx-npi-defs.h>
+#include <asm/octeon/cvmx-gmxx-defs.h>
+
+#include "ethernet-defines.h"
+#include "octeon-ethernet.h"
+#include "ethernet-util.h"
+
+static int number_rgmii_ports;
+
+static void cvm_oct_rgmii_poll(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	cvmx_helper_link_info_t link_info;
+
+	link_info = cvmx_helper_link_get(priv->port);
+	if (link_info.u64 == priv->link_info) {
+
+		/*
+		 * If the 10Mbps preamble workaround is supported and we're
+		 * at 10Mbps we may need to do some special checking.
+		 */
+		if (USE_10MBPS_PREAMBLE_WORKAROUND && (link_info.s.speed == 10)) {
+
+			/*
+			 * Read the GMXX_RXX_INT_REG[PCTERR] bit and
+			 * see if we are getting preamble errors.
+			 */
+			int interface = INTERFACE(priv->port);
+			int index = INDEX(priv->port);
+			union cvmx_gmxx_rxx_int_reg gmxx_rxx_int_reg;
+			gmxx_rxx_int_reg.u64 = cvmx_read_csr(CVMX_GMXX_RXX_INT_REG(index, interface));
+			if (gmxx_rxx_int_reg.s.pcterr) {
+				/*
+				 * We are getting preamble errors at
+				 * 10Mbps.  Most likely the PHY is
+				 * giving us packets with mis aligned
+				 * preambles. In order to get these
+				 * packets we need to disable preamble
+				 * checking and do it in software.
+				 */
+				union cvmx_gmxx_rxx_frm_ctl gmxx_rxx_frm_ctl;
+				union cvmx_ipd_sub_port_fcs ipd_sub_port_fcs;
+
+				/* Disable preamble checking */
+				gmxx_rxx_frm_ctl.u64 = cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface));
+				gmxx_rxx_frm_ctl.s.pre_chk = 0;
+				cvmx_write_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface), gmxx_rxx_frm_ctl.u64);
+
+				/* Disable FCS stripping */
+				ipd_sub_port_fcs.u64 = cvmx_read_csr(CVMX_IPD_SUB_PORT_FCS);
+				ipd_sub_port_fcs.s.port_bit &= 0xffffffffull ^ (1ull << priv->port);
+				cvmx_write_csr(CVMX_IPD_SUB_PORT_FCS, ipd_sub_port_fcs.u64);
+
+				/* Clear any error bits */
+				cvmx_write_csr(CVMX_GMXX_RXX_INT_REG(index, interface), gmxx_rxx_int_reg.u64);
+				DEBUGPRINT("%s: Using 10Mbps with software preamble removal\n",
+				     dev->name);
+			}
+		}
+		return;
+	}
+
+	/* If the 10Mbps preamble workaround is allowed we need to on
+	   preamble checking, FCS stripping, and clear error bits on
+	   every speed change. If errors occur during 10Mbps operation
+	   the above code will change this stuff */
+	if (USE_10MBPS_PREAMBLE_WORKAROUND) {
+
+		union cvmx_gmxx_rxx_frm_ctl gmxx_rxx_frm_ctl;
+		union cvmx_ipd_sub_port_fcs ipd_sub_port_fcs;
+		union cvmx_gmxx_rxx_int_reg gmxx_rxx_int_reg;
+		int interface = INTERFACE(priv->port);
+		int index = INDEX(priv->port);
+
+		/* Enable preamble checking */
+		gmxx_rxx_frm_ctl.u64 = cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface));
+		gmxx_rxx_frm_ctl.s.pre_chk = 1;
+		cvmx_write_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface), gmxx_rxx_frm_ctl.u64);
+		/* Enable FCS stripping */
+		ipd_sub_port_fcs.u64 = cvmx_read_csr(CVMX_IPD_SUB_PORT_FCS);
+		ipd_sub_port_fcs.s.port_bit |= 1ull << priv->port;
+		cvmx_write_csr(CVMX_IPD_SUB_PORT_FCS, ipd_sub_port_fcs.u64);
+		/* Clear any error bits */
+		gmxx_rxx_int_reg.u64 = cvmx_read_csr(CVMX_GMXX_RXX_INT_REG(index, interface));
+		cvmx_write_csr(CVMX_GMXX_RXX_INT_REG(index, interface), gmxx_rxx_int_reg.u64);
+	}
+	if (priv->phydev == NULL) {
+		link_info = cvmx_helper_link_autoconf(priv->port);
+		priv->link_info = link_info.u64;
+	}
+
+	if (priv->phydev == NULL)
+		cvm_oct_set_carrier(priv, link_info);
+}
+
+static irqreturn_t cvm_oct_rgmii_rml_interrupt(int cpl, void *dev_id)
+{
+	union cvmx_npi_rsl_int_blocks rsl_int_blocks;
+	int index;
+	irqreturn_t return_status = IRQ_NONE;
+
+	rsl_int_blocks.u64 = cvmx_read_csr(CVMX_NPI_RSL_INT_BLOCKS);
+
+	/* Check and see if this interrupt was caused by the GMX0 block */
+	if (rsl_int_blocks.s.gmx0) {
+
+		int interface = 0;
+		/* Loop through every port of this interface */
+		for (index = 0;
+		     index < cvmx_helper_ports_on_interface(interface);
+		     index++) {
+
+			/* Read the GMX interrupt status bits */
+			union cvmx_gmxx_rxx_int_reg gmx_rx_int_reg;
+			gmx_rx_int_reg.u64 = cvmx_read_csr(CVMX_GMXX_RXX_INT_REG(index, interface));
+			gmx_rx_int_reg.u64 &= cvmx_read_csr(CVMX_GMXX_RXX_INT_EN(index, interface));
+			/* Poll the port if inband status changed */
+			if (gmx_rx_int_reg.s.phy_dupx
+			    || gmx_rx_int_reg.s.phy_link
+			    || gmx_rx_int_reg.s.phy_spd) {
+				struct octeon_ethernet *priv = cvm_oct_by_port[cvmx_helper_get_ipd_port(interface, index)];
+
+				if (priv && !atomic_read(&cvm_oct_poll_queue_stopping))
+					queue_work(cvm_oct_poll_queue, &priv->port_work);
+
+				gmx_rx_int_reg.u64 = 0;
+				gmx_rx_int_reg.s.phy_dupx = 1;
+				gmx_rx_int_reg.s.phy_link = 1;
+				gmx_rx_int_reg.s.phy_spd = 1;
+				cvmx_write_csr(CVMX_GMXX_RXX_INT_REG(index, interface), gmx_rx_int_reg.u64);
+				return_status = IRQ_HANDLED;
+			}
+		}
+	}
+
+	/* Check and see if this interrupt was caused by the GMX1 block */
+	if (rsl_int_blocks.s.gmx1) {
+
+		int interface = 1;
+		/* Loop through every port of this interface */
+		for (index = 0;
+		     index < cvmx_helper_ports_on_interface(interface);
+		     index++) {
+
+			/* Read the GMX interrupt status bits */
+			union cvmx_gmxx_rxx_int_reg gmx_rx_int_reg;
+			gmx_rx_int_reg.u64 = cvmx_read_csr(CVMX_GMXX_RXX_INT_REG(index, interface));
+			gmx_rx_int_reg.u64 &= cvmx_read_csr(CVMX_GMXX_RXX_INT_EN(index, interface));
+			/* Poll the port if inband status changed */
+			if (gmx_rx_int_reg.s.phy_dupx
+			    || gmx_rx_int_reg.s.phy_link
+			    || gmx_rx_int_reg.s.phy_spd) {
+				struct octeon_ethernet *priv = cvm_oct_by_port[cvmx_helper_get_ipd_port(interface, index)];
+
+				if (priv && !atomic_read(&cvm_oct_poll_queue_stopping))
+					queue_work(cvm_oct_poll_queue, &priv->port_work);
+
+				gmx_rx_int_reg.u64 = 0;
+				gmx_rx_int_reg.s.phy_dupx = 1;
+				gmx_rx_int_reg.s.phy_link = 1;
+				gmx_rx_int_reg.s.phy_spd = 1;
+				cvmx_write_csr(CVMX_GMXX_RXX_INT_REG(index, interface), gmx_rx_int_reg.u64);
+				return_status = IRQ_HANDLED;
+			}
+		}
+	}
+	return return_status;
+}
+
+int cvm_oct_rgmii_open(struct net_device *dev)
+{
+	union cvmx_gmxx_prtx_cfg gmx_cfg;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	int interface = INTERFACE(priv->port);
+	int index = INDEX(priv->port);
+	cvmx_helper_link_info_t link_info;
+
+	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
+	gmx_cfg.s.en = 1;
+	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmx_cfg.u64);
+
+	if (!octeon_is_simulation()) {
+		link_info = cvmx_helper_link_get(priv->port);
+		if (!link_info.s.link_up)
+			netif_carrier_off(dev);
+	}
+
+	return 0;
+}
+
+int cvm_oct_rgmii_stop(struct net_device *dev)
+{
+	union cvmx_gmxx_prtx_cfg gmx_cfg;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	int interface = INTERFACE(priv->port);
+	int index = INDEX(priv->port);
+
+	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
+	gmx_cfg.s.en = 0;
+	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmx_cfg.u64);
+	return 0;
+}
+
+static void cvm_oct_rgmii_immediate_poll(struct work_struct *work)
+{
+	struct octeon_ethernet *priv = container_of(work, struct octeon_ethernet, port_work);
+	cvm_oct_rgmii_poll(priv->netdev);
+}
+
+int cvm_oct_rgmii_init(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	int r;
+
+	cvm_oct_common_init(dev);
+	dev->netdev_ops->ndo_stop(dev);
+	INIT_WORK(&priv->port_work, cvm_oct_rgmii_immediate_poll);
+	/*
+	 * Due to GMX errata in CN3XXX series chips, it is necessary
+	 * to take the link down immediately when the PHY changes
+	 * state. In order to do this we call the poll function every
+	 * time the RGMII inband status changes.  This may cause
+	 * problems if the PHY doesn't implement inband status
+	 * properly.
+	 */
+	if (number_rgmii_ports == 0) {
+		r = request_irq(OCTEON_IRQ_RML, cvm_oct_rgmii_rml_interrupt,
+				IRQF_SHARED, "RGMII", &number_rgmii_ports);
+		if (r != 0)
+			return r;
+	}
+	number_rgmii_ports++;
+
+	/*
+	 * Only true RGMII ports need to be polled. In GMII mode, port
+	 * 0 is really a RGMII port.
+	 */
+	if (((priv->imode == CVMX_HELPER_INTERFACE_MODE_GMII) && (priv->port == 0))
+	    || (priv->imode == CVMX_HELPER_INTERFACE_MODE_RGMII)) {
+
+		if (!octeon_is_simulation()) {
+
+			union cvmx_gmxx_rxx_int_en gmx_rx_int_en;
+			int interface = INTERFACE(priv->port);
+			int index = INDEX(priv->port);
+
+			/*
+			 * Enable interrupts on inband status changes
+			 * for this port.
+			 */
+			gmx_rx_int_en.u64 = cvmx_read_csr(CVMX_GMXX_RXX_INT_EN(index, interface));
+			gmx_rx_int_en.s.phy_dupx = 1;
+			gmx_rx_int_en.s.phy_link = 1;
+			gmx_rx_int_en.s.phy_spd = 1;
+			cvmx_write_csr(CVMX_GMXX_RXX_INT_EN(index, interface), gmx_rx_int_en.u64);
+			priv->poll = cvm_oct_rgmii_poll;
+		}
+	}
+
+	return 0;
+}
+
+void cvm_oct_rgmii_uninit(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	cvm_oct_common_uninit(dev);
+
+	/*
+	 * Only true RGMII ports need to be polled. In GMII mode, port
+	 * 0 is really a RGMII port.
+	 */
+	if (((priv->imode == CVMX_HELPER_INTERFACE_MODE_GMII) && (priv->port == 0))
+	    || (priv->imode == CVMX_HELPER_INTERFACE_MODE_RGMII)) {
+
+		if (!octeon_is_simulation()) {
+
+			union cvmx_gmxx_rxx_int_en gmx_rx_int_en;
+			int interface = INTERFACE(priv->port);
+			int index = INDEX(priv->port);
+
+			/*
+			 * Disable interrupts on inband status changes
+			 * for this port.
+			 */
+			gmx_rx_int_en.u64 = cvmx_read_csr(CVMX_GMXX_RXX_INT_EN(index, interface));
+			gmx_rx_int_en.s.phy_dupx = 0;
+			gmx_rx_int_en.s.phy_link = 0;
+			gmx_rx_int_en.s.phy_spd = 0;
+			cvmx_write_csr(CVMX_GMXX_RXX_INT_EN(index, interface), gmx_rx_int_en.u64);
+		}
+	}
+
+	/* Remove the interrupt handler when the last port is removed. */
+	number_rgmii_ports--;
+	if (number_rgmii_ports == 0)
+		free_irq(OCTEON_IRQ_RML, &number_rgmii_ports);
+	cancel_work_sync(&priv->port_work);
+}
diff --git a/drivers/net/octeon/ethernet-rx.c b/drivers/net/octeon/ethernet-rx.c
new file mode 100644
index 0000000..161f3e4
--- /dev/null
+++ b/drivers/net/octeon/ethernet-rx.c
@@ -0,0 +1,680 @@
+/**********************************************************************
+ * Author: Cavium Networks
+ *
+ * Contact: support@caviumnetworks.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2010 Cavium Networks
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium Networks for more information
+**********************************************************************/
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/cache.h>
+#include <linux/cpumask.h>
+#include <linux/netdevice.h>
+#include <linux/init.h>
+#include <linux/etherdevice.h>
+#include <linux/ip.h>
+#include <linux/string.h>
+#include <linux/prefetch.h>
+#include <linux/smp.h>
+#include <linux/ktime.h>
+#include <net/dst.h>
+#ifdef CONFIG_XFRM
+#include <linux/xfrm.h>
+#include <net/xfrm.h>
+#endif /* CONFIG_XFRM */
+
+#include <asm/atomic.h>
+
+#include <asm/octeon/octeon.h>
+#include <asm/octeon/cvmx-helper.h>
+#include <asm/octeon/cvmx-wqe.h>
+#include <asm/octeon/cvmx-fau.h>
+#include <asm/octeon/cvmx-pow.h>
+#include <asm/octeon/cvmx-pip.h>
+#include <asm/octeon/cvmx-srio.h>
+#include <asm/octeon/cvmx-scratch.h>
+
+#include <asm/octeon/cvmx-gmxx-defs.h>
+
+#include "ethernet-defines.h"
+#include "ethernet-mem.h"
+#include "ethernet-rx.h"
+#include "octeon-ethernet.h"
+#include "ethernet-util.h"
+
+
+struct cvm_napi_wrapper {
+	struct napi_struct napi;
+	atomic_t available;
+} ____cacheline_aligned_in_smp;
+
+static struct cvm_napi_wrapper cvm_oct_napi[NR_CPUS] __cacheline_aligned_in_smp;
+
+struct cvm_oct_core_state {
+	int baseline_cores;
+	atomic_t active_cores;
+} ____cacheline_aligned_in_smp;
+
+static struct cvm_oct_core_state core_state __cacheline_aligned_in_smp;
+
+static void cvm_oct_enable_napi(void *arg)
+{
+	struct napi_struct *napi = arg;
+	napi_schedule(napi);
+}
+
+static void cvm_oct_enable_one_cpu(void)
+{
+	int v;
+	int cpu;
+
+	/* ... if a CPU is available, Turn on NAPI polling for that CPU.  */
+	for_each_online_cpu(cpu) {
+		if (atomic_sub_if_positive(1, &cvm_oct_napi[cpu].available) >= 0) {
+			atomic_inc(&core_state.active_cores);
+			v = smp_call_function_single(cpu, cvm_oct_enable_napi,
+						     &cvm_oct_napi[cpu].napi, 0);
+			if (v)
+				panic("Can't enable NAPI.");
+			break;
+		}
+	}
+}
+
+static void cvm_oct_no_more_work(struct napi_struct *napi)
+{
+	struct cvm_napi_wrapper *nr = container_of(napi, struct cvm_napi_wrapper, napi);
+	int current_active, napi_state;
+
+	current_active = atomic_dec_return(&core_state.active_cores);
+
+	napi_state = atomic_inc_return(&nr->available);
+
+	BUG_ON(napi_state != 1);
+
+	if (current_active == 0) {
+		/*
+		 * No more CPUs doing processing, enable interrupts so
+		 * we can start processing again when there is
+		 * something to do.
+		 */
+		enable_irq(OCTEON_IRQ_WORKQ0 + pow_receive_group);
+	}
+}
+
+/**
+ * cvm_oct_do_interrupt - interrupt handler.
+ *
+ * The interrupt occurs whenever the POW has packets in our group.
+ *
+ */
+static irqreturn_t cvm_oct_do_interrupt(int cpl, void *dev_id)
+{
+	int cpu = smp_processor_id();
+
+	/* Disable the IRQ and start napi_poll. */
+	disable_irq_nosync(OCTEON_IRQ_WORKQ0 + pow_receive_group);
+
+	/* ... and NAPI better not be running on this CPU.  */
+	if (atomic_sub_if_positive(1, &cvm_oct_napi[cpu].available) < 0)
+		BUG();
+
+	/* There better be cores available...  */
+	if (atomic_inc_return(&core_state.active_cores) > core_state.baseline_cores)
+		BUG();
+
+	cvm_oct_enable_napi(&cvm_oct_napi[cpu].napi);
+
+	return IRQ_HANDLED;
+}
+
+/**
+ * cvm_oct_check_rcv_error - process receive errors
+ * @work: Work queue entry pointing to the packet.
+ *
+ * Returns Non-zero if the packet can be dropped, zero otherwise.
+ */
+static inline int cvm_oct_check_rcv_error(cvmx_wqe_t *work)
+{
+	if ((work->word2.snoip.err_code == 10) && (work->len <= 64)) {
+		/*
+		 * Ignore length errors on min size packets. Some
+		 * equipment incorrectly pads packets to 64+4FCS
+		 * instead of 60+4FCS.  Note these packets still get
+		 * counted as frame errors.
+		 */
+	} else
+	    if (USE_10MBPS_PREAMBLE_WORKAROUND
+		&& ((work->word2.snoip.err_code == 5)
+		    || (work->word2.snoip.err_code == 7))) {
+
+		/*
+		 * We received a packet with either an alignment error
+		 * or a FCS error. This may be signalling that we are
+		 * running 10Mbps with GMXX_RXX_FRM_CTL[PRE_CHK}
+		 * off. If this is the case we need to parse the
+		 * packet to determine if we can remove a non spec
+		 * preamble and generate a correct packet.
+		 */
+		int interface = cvmx_helper_get_interface_num(work->ipprt);
+		int index = cvmx_helper_get_interface_index_num(work->ipprt);
+		union cvmx_gmxx_rxx_frm_ctl gmxx_rxx_frm_ctl;
+		gmxx_rxx_frm_ctl.u64 = cvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface));
+		if (gmxx_rxx_frm_ctl.s.pre_chk == 0) {
+
+			uint8_t *ptr = cvmx_phys_to_ptr(work->packet_ptr.s.addr);
+			int i = 0;
+
+			while (i < work->len - 1) {
+				if (*ptr != 0x55)
+					break;
+				ptr++;
+				i++;
+			}
+
+			if (*ptr == 0xd5) {
+				/*
+				   DEBUGPRINT("Port %d received 0xd5 preamble\n", work->ipprt);
+				 */
+				work->packet_ptr.s.addr += i + 1;
+				work->len -= i + 5;
+			} else if ((*ptr & 0xf) == 0xd) {
+				/*
+				   DEBUGPRINT("Port %d received 0x?d preamble\n", work->ipprt);
+				 */
+				work->packet_ptr.s.addr += i;
+				work->len -= i + 4;
+				for (i = 0; i < work->len; i++) {
+					*ptr = ((*ptr & 0xf0) >> 4) | ((*(ptr + 1) & 0xf) << 4);
+					ptr++;
+				}
+			} else {
+				DEBUGPRINT("Port %d unknown preamble, packet dropped\n",
+				     work->ipprt);
+				/* cvmx_helper_dump_packet(work); */
+				cvm_oct_free_work(work);
+				return 1;
+			}
+		}
+	} else {
+		DEBUGPRINT("Port %d receive error code %d, packet dropped\n",
+			   work->ipprt, work->word2.snoip.err_code);
+		cvm_oct_free_work(work);
+		return 1;
+	}
+
+	return 0;
+}
+
+/**
+ * Convert a hardware PTP timestamp into a kernel timestamp.
+ *
+ * @param ptptime 64 bit PTP timestamp, normally in nanoseconds
+ *
+ * @return ktime_t
+ */
+static ktime_t cvm_oct_ptp_to_ktime(uint64_t ptptime)
+{
+	ktime_t ktimebase;
+	uint64_t ptpbase;
+	unsigned long flags;
+
+	local_irq_save(flags);
+	ktimebase = ktime_get_real();
+	/* FIXME: Needed for CN63XX errata */
+	cvmx_read_csr(CVMX_MIO_PTP_CLOCK_HI);
+	ptpbase = cvmx_read_csr(CVMX_MIO_PTP_CLOCK_HI);
+	local_irq_restore(flags);
+
+	return ktime_sub_ns(ktimebase, ptpbase - ptptime);
+}
+
+/**
+ * cvm_oct_napi_poll - the NAPI poll function.
+ * @napi: The NAPI instance, or null if called from cvm_oct_poll_controller
+ * @budget: Maximum number of packets to receive.
+ *
+ * Returns the number of packets processed.
+ */
+static int cvm_oct_napi_poll(struct napi_struct *napi, int budget)
+{
+	const int	coreid = cvmx_get_core_num();
+	uint64_t	old_group_mask;
+	uint64_t	old_scratch;
+	int		rx_count = 0;
+	int		did_work_request = 0;
+	int		packet_not_copied;
+
+	char		*p = (char *)cvm_oct_by_port;
+	/* Prefetch cvm_oct_device since we know we need it soon */
+	prefetch(&p[0]);
+	prefetch(&p[SMP_CACHE_BYTES]);
+	prefetch(&p[2 * SMP_CACHE_BYTES]);
+
+	if (USE_ASYNC_IOBDMA) {
+		/* Save scratch in case userspace is using it */
+		CVMX_SYNCIOBDMA;
+		old_scratch = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
+	}
+
+	/* Only allow work for our group (and preserve priorities) */
+	old_group_mask = cvmx_read_csr(CVMX_POW_PP_GRP_MSKX(coreid));
+	cvmx_write_csr(CVMX_POW_PP_GRP_MSKX(coreid),
+		       (old_group_mask & ~0xFFFFull) | 1 << pow_receive_group);
+
+	if (USE_ASYNC_IOBDMA) {
+		cvmx_pow_work_request_async(CVMX_SCR_SCRATCH, CVMX_POW_NO_WAIT);
+		did_work_request = 1;
+	}
+
+	while (rx_count < budget) {
+		struct sk_buff *skb = NULL;
+		struct sk_buff **pskb = NULL;
+		enum cvm_oct_callback_result callback_result;
+		int skb_in_hw;
+		cvmx_wqe_t *work;
+
+		if (USE_ASYNC_IOBDMA && did_work_request)
+			work = cvmx_pow_work_response_async(CVMX_SCR_SCRATCH);
+		else
+			work = cvmx_pow_work_request_sync(CVMX_POW_NO_WAIT);
+
+		prefetch(work);
+		did_work_request = 0;
+		if (work == NULL) {
+			union cvmx_pow_wq_int wq_int;
+			wq_int.u64 = 0;
+			wq_int.s.iq_dis = 1 << pow_receive_group;
+			wq_int.s.wq_int = 1 << pow_receive_group;
+			cvmx_write_csr(CVMX_POW_WQ_INT, wq_int.u64);
+			break;
+		}
+		pskb = (struct sk_buff **)(cvm_oct_get_buffer_ptr(work->packet_ptr) - sizeof(void *));
+		prefetch(pskb);
+
+		if (USE_ASYNC_IOBDMA && rx_count < (budget - 1)) {
+			cvmx_pow_work_request_async_nocheck(CVMX_SCR_SCRATCH, CVMX_POW_NO_WAIT);
+			did_work_request = 1;
+		}
+
+		if (rx_count == 0) {
+			/*
+			 * First time through, see if there is enough
+			 * work waiting to merit waking another
+			 * CPU.
+			 */
+			union cvmx_pow_wq_int_cntx counts;
+			int backlog;
+			int cores_in_use = atomic_read(&core_state.active_cores);
+			counts.u64 = cvmx_read_csr(CVMX_POW_WQ_INT_CNTX(pow_receive_group));
+			backlog = counts.s.iq_cnt + counts.s.ds_cnt;
+			if (backlog > budget * cores_in_use &&
+			    napi != NULL &&
+			    cores_in_use < core_state.baseline_cores)
+				cvm_oct_enable_one_cpu();
+		}
+
+		/*
+		 * If WORD2[SOFTWARE] then this WQE is a complete for
+		 * a TX packet.
+		 */
+		if (work->word2.s.software) {
+			struct octeon_ethernet *priv;
+			int packet_qos = work->unused;
+			skb = (struct sk_buff *)work->packet_ptr.u64;
+			priv = netdev_priv(skb->dev);
+			if (!netif_running(skb->dev))
+				netif_wake_queue(skb->dev);
+			if (priv->flags & OCTEON_ETHERNET_FLAG_TX_TIMESTAMP_HW) {
+				uint64_t ns = *(uint64_t *)work->packet_data;
+				struct skb_shared_hwtstamps ts;
+				ts.syststamp = cvm_oct_ptp_to_ktime(ns);
+				ts.hwtstamp = ns_to_ktime(ns);
+				skb_tstamp_tx(skb, &ts);
+			}
+			if (priv->flags & OCTEON_ETHERNET_FLAG_TX_TIMESTAMP_SW) {
+				uint64_t ns = *(uint64_t *)work->packet_data;
+				struct skb_shared_hwtstamps ts;
+				ts.syststamp = ns_to_ktime(ns);
+				ts.hwtstamp = ns_to_ktime(0);
+				skb_tstamp_tx(skb, &ts);
+			}
+
+			dev_kfree_skb_any(skb);
+
+			cvmx_fpa_free(work, CVMX_FPA_TX_WQE_POOL, DONT_WRITEBACK(1));
+
+			/*
+			 * We are done with this one, adjust the queue
+			 * depth.
+			 */
+			cvmx_fau_atomic_add32(priv->tx_queue[packet_qos].fau, -1);
+			continue;
+		}
+
+		skb_in_hw = USE_SKBUFFS_IN_HW && work->word2.s.bufs == 1;
+		if (likely(skb_in_hw)) {
+			skb = *pskb;
+			prefetch(&skb->head);
+			prefetch(&skb->len);
+		}
+		prefetch(cvm_oct_by_port[work->ipprt]);
+
+		/* Immediately throw away all packets with receive errors */
+		if (unlikely(work->word2.snoip.rcv_error)) {
+			if (cvm_oct_check_rcv_error(work))
+				continue;
+		}
+
+		/*
+		 * We can only use the zero copy path if skbuffs are
+		 * in the FPA pool and the packet fits in a single
+		 * buffer.
+		 */
+		if (likely(skb_in_hw)) {
+			skb->data = skb->head + work->packet_ptr.s.addr - cvmx_ptr_to_phys(skb->head);
+			prefetch(skb->data);
+			skb->len = work->len;
+			skb_set_tail_pointer(skb, skb->len);
+			packet_not_copied = 1;
+		} else {
+			/*
+			 * We have to copy the packet. First allocate
+			 * an skbuff for it.
+			 */
+			skb = dev_alloc_skb(work->len);
+			if (!skb) {
+				DEBUGPRINT("Port %d failed to allocate skbuff, packet dropped\n",
+					   work->ipprt);
+				cvm_oct_free_work(work);
+				continue;
+			}
+
+			/*
+			 * Check if we've received a packet that was
+			 * entirely stored in the work entry.
+			 */
+			if (unlikely(work->word2.s.bufs == 0)) {
+				uint8_t *ptr = work->packet_data;
+
+				if (likely(!work->word2.s.not_IP)) {
+					/*
+					 * The beginning of the packet
+					 * moves for IP packets.
+					 */
+					if (work->word2.s.is_v6)
+						ptr += 2;
+					else
+						ptr += 6;
+				}
+				memcpy(skb_put(skb, work->len), ptr, work->len);
+				/* No packet buffers to free */
+			} else {
+				int segments = work->word2.s.bufs;
+				union cvmx_buf_ptr segment_ptr = work->packet_ptr;
+				int len = work->len;
+
+				while (segments--) {
+					union cvmx_buf_ptr  next_ptr;
+					int segment_size;
+
+					next_ptr = *(union cvmx_buf_ptr *)cvmx_phys_to_ptr(segment_ptr.s.addr - 8);
+
+			/*
+			 * Octeon Errata PKI-100: The segment size is
+			 * wrong. Until it is fixed, calculate the
+			 * segment size based on the packet pool
+			 * buffer size. When it is fixed, the
+			 * following line should be replaced with this
+			 * one: int segment_size =
+			 * segment_ptr.s.size;
+			 */
+					segment_size = CVMX_FPA_PACKET_POOL_SIZE -
+						(segment_ptr.s.addr - (((segment_ptr.s.addr >> 7) - segment_ptr.s.back) << 7));
+					/*
+					 * Don't copy more than what
+					 * is left in the packet.
+					 */
+					if (segment_size > len)
+						segment_size = len;
+					/* Copy the data into the packet */
+					memcpy(skb_put(skb, segment_size),
+					       cvmx_phys_to_ptr(segment_ptr.s.addr),
+					       segment_size);
+					len -= segment_size;
+					segment_ptr = next_ptr;
+				}
+			}
+			packet_not_copied = 0;
+		}
+
+		if (likely((work->ipprt < TOTAL_NUMBER_OF_PORTS) && cvm_oct_by_port[work->ipprt])) {
+			struct octeon_ethernet *priv = cvm_oct_by_port[work->ipprt];
+#ifdef CONFIG_RAPIDIO
+			if (unlikely(priv->imode == CVMX_HELPER_INTERFACE_MODE_SRIO)) {
+				const cvmx_srio_rx_message_header_t *rx_header = (const cvmx_srio_rx_message_header_t *)__skb_pull(skb, sizeof(cvmx_srio_rx_message_header_t));
+				priv = cvm_oct_by_srio_mbox[(work->ipprt - 40) >> 1][rx_header->word0.s.mbox];
+
+				atomic64_add(1, (atomic64_t *)&priv->netdev->stats.rx_packets);
+				atomic64_add(skb->len, (atomic64_t *)&priv->netdev->stats.rx_bytes);
+			}
+#endif
+			/*
+			 * Only accept packets for devices that are
+			 * currently up.
+			 */
+			if (likely(priv->netdev->flags & IFF_UP)) {
+				if (priv->flags & OCTEON_ETHERNET_FLAG_RX_TIMESTAMP_SW) {
+					struct skb_shared_hwtstamps *ts;
+					ts = skb_hwtstamps(skb);
+					ts->syststamp = ktime_get_real();
+					ts->hwtstamp = ns_to_ktime(0);
+				}
+				if (priv->flags & OCTEON_ETHERNET_FLAG_RX_TIMESTAMP_HW) {
+					/* The first 8 bytes are the timestamp */
+					uint64_t ns = *(uint64_t *)skb->data;
+					struct skb_shared_hwtstamps *ts;
+					ts = skb_hwtstamps(skb);
+					ts->hwtstamp = ns_to_ktime(ns);
+					ts->syststamp = cvm_oct_ptp_to_ktime(ns);
+					__skb_pull(skb, 8);
+				}
+				skb->protocol = eth_type_trans(skb, priv->netdev);
+				skb->dev = priv->netdev;
+
+				if (unlikely(work->word2.s.not_IP || work->word2.s.IP_exc || work->word2.s.L4_error))
+					skb->ip_summed = CHECKSUM_NONE;
+				else
+					skb->ip_summed = CHECKSUM_UNNECESSARY;
+
+				/* Increment RX stats for virtual ports */
+				if (work->ipprt >= CVMX_PIP_NUM_INPUT_PORTS) {
+					atomic64_add(1, (atomic64_t *)&priv->netdev->stats.rx_packets);
+					atomic64_add(skb->len, (atomic64_t *)&priv->netdev->stats.rx_bytes);
+				}
+				if (priv->intercept_cb) {
+					callback_result = priv->intercept_cb(priv->netdev, work, skb);
+					switch (callback_result) {
+					case CVM_OCT_PASS:
+						netif_receive_skb(skb);
+						rx_count++;
+						break;
+					case CVM_OCT_DROP:
+						dev_kfree_skb_irq(skb);
+						atomic64_add(1, (atomic64_t *)&priv->netdev->stats.rx_dropped);
+						break;
+					case CVM_OCT_TAKE_OWNERSHIP_WORK:
+						/*
+						 * Interceptor took
+						 * our work, but we
+						 * need to free the
+						 * skbuff
+						 */
+						if (USE_SKBUFFS_IN_HW && likely(packet_not_copied)) {
+							/*
+							 * We can't free the skbuff since its data is
+							 * the same as the work. In this case we don't
+							 * do anything
+							 */
+						} else {
+							dev_kfree_skb_irq(skb);
+						}
+						break;
+					case CVM_OCT_TAKE_OWNERSHIP_SKB:
+						/* Interceptor took our packet */
+						break;
+					}
+				} else {
+					netif_receive_skb(skb);
+					callback_result = CVM_OCT_PASS;
+					rx_count++;
+				}
+			} else {
+				/* Drop any packet received for a device that isn't up */
+				/*
+				DEBUGPRINT("%s: Device not up, packet dropped\n",
+					   dev->name);
+				*/
+				atomic64_add(1, (atomic64_t *)&priv->netdev->stats.rx_dropped);
+				dev_kfree_skb_irq(skb);
+				callback_result = CVM_OCT_DROP;
+			}
+		} else {
+			/*
+			 * Drop any packet received for a device that
+			 * doesn't exist.
+			 */
+			DEBUGPRINT("Port %d not controlled by Linux, packet dropped\n",
+				   work->ipprt);
+			dev_kfree_skb_irq(skb);
+			callback_result = CVM_OCT_DROP;
+		}
+		/* We only need to free the work if the interceptor didn't
+		   take over ownership of it */
+		if (callback_result != CVM_OCT_TAKE_OWNERSHIP_WORK) {
+			/*
+			 * Check to see if the skbuff and work share
+			 * the same packet buffer.
+			 */
+			if (USE_SKBUFFS_IN_HW && likely(packet_not_copied)) {
+				/*
+				 * This buffer needs to be replaced,
+				 * increment the number of buffers we
+				 * need to free by one.
+				 */
+				cvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, 1);
+				cvmx_fpa_free(work, CVMX_FPA_WQE_POOL, DONT_WRITEBACK(1));
+			} else {
+				cvm_oct_free_work(work);
+			}
+		}
+	}
+	/* Restore the original POW group mask */
+	cvmx_write_csr(CVMX_POW_PP_GRP_MSKX(coreid), old_group_mask);
+	if (USE_ASYNC_IOBDMA) {
+		/* Restore the scratch area */
+		cvmx_scratch_write64(CVMX_SCR_SCRATCH, old_scratch);
+	}
+	cvm_oct_rx_refill_pool(0);
+
+	if (rx_count < budget && napi != NULL) {
+		/* No more work */
+		napi_complete(napi);
+		cvm_oct_no_more_work(napi);
+	}
+	return rx_count;
+}
+
+#ifdef CONFIG_NET_POLL_CONTROLLER
+/**
+ * cvm_oct_poll_controller - poll for receive packets
+ * device.
+ *
+ * @dev:    Device to poll. Unused
+ */
+void cvm_oct_poll_controller(struct net_device *dev)
+{
+	cvm_oct_napi_poll(NULL, 16);
+}
+#endif
+
+void cvm_oct_rx_initialize(void)
+{
+	int i;
+	struct net_device *dev_for_napi = NULL;
+	union cvmx_pow_wq_int_thrx int_thr;
+	union cvmx_pow_wq_int_pc int_pc;
+
+	if (list_empty(&cvm_oct_list))
+		panic("No net_devices were allocated.");
+
+	dev_for_napi = list_first_entry(&cvm_oct_list,
+					struct octeon_ethernet,
+					list)->netdev;
+
+	if (max_rx_cpus > 1  && max_rx_cpus < num_online_cpus())
+		core_state.baseline_cores = max_rx_cpus;
+	else
+		core_state.baseline_cores = num_online_cpus();
+
+	for_each_possible_cpu(i) {
+		atomic_set(&cvm_oct_napi[i].available, 1);
+		netif_napi_add(dev_for_napi, &cvm_oct_napi[i].napi,
+			       cvm_oct_napi_poll, rx_napi_weight);
+		napi_enable(&cvm_oct_napi[i].napi);
+	}
+	/* Register an IRQ hander for to receive POW interrupts */
+	i = request_irq(OCTEON_IRQ_WORKQ0 + pow_receive_group,
+			cvm_oct_do_interrupt, 0, dev_for_napi->name, &cvm_oct_list);
+
+	if (i)
+		panic("Could not acquire Ethernet IRQ %d\n",
+		      OCTEON_IRQ_WORKQ0 + pow_receive_group);
+
+	disable_irq_nosync(OCTEON_IRQ_WORKQ0 + pow_receive_group);
+
+	int_thr.u64 = 0;
+	int_thr.s.tc_en = 1;
+	int_thr.s.tc_thr = 1;
+	/* Enable POW interrupt when our port has at least one packet */
+	cvmx_write_csr(CVMX_POW_WQ_INT_THRX(pow_receive_group), int_thr.u64);
+
+	int_pc.u64 = 0;
+	int_pc.s.pc_thr = 5;
+	cvmx_write_csr(CVMX_POW_WQ_INT_PC, int_pc.u64);
+
+
+	/* Scheduld NAPI now.  This will indirectly enable interrupts. */
+	cvm_oct_enable_one_cpu();
+}
+
+void cvm_oct_rx_shutdown(void)
+{
+	int i;
+	/* Shutdown all of the NAPIs */
+	for_each_possible_cpu(i)
+		netif_napi_del(&cvm_oct_napi[i].napi);
+
+	/* Free the interrupt handler */
+	free_irq(OCTEON_IRQ_WORKQ0 + pow_receive_group, &cvm_oct_list);
+
+}
diff --git a/drivers/net/octeon/ethernet-rx.h b/drivers/net/octeon/ethernet-rx.h
new file mode 100644
index 0000000..9240c85
--- /dev/null
+++ b/drivers/net/octeon/ethernet-rx.h
@@ -0,0 +1,52 @@
+/*********************************************************************
+ * Author: Cavium Networks
+ *
+ * Contact: support@caviumnetworks.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2007 Cavium Networks
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium Networks for more information
+*********************************************************************/
+#include <asm/octeon/cvmx-fau.h>
+
+void cvm_oct_poll_controller(struct net_device *dev);
+void cvm_oct_rx_initialize(void);
+void cvm_oct_rx_shutdown(void);
+
+static inline void cvm_oct_rx_refill_pool(int fill_threshold)
+{
+	int number_to_free;
+	int num_freed;
+	/* Refill the packet buffer pool */
+	number_to_free =
+		cvmx_fau_fetch_and_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, 0);
+
+	if (number_to_free > fill_threshold) {
+		cvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE,
+				      -number_to_free);
+		num_freed = cvm_oct_mem_fill_fpa(CVMX_FPA_PACKET_POOL,
+						 CVMX_FPA_PACKET_POOL_SIZE,
+						 number_to_free);
+		if (num_freed != number_to_free) {
+			cvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE,
+					number_to_free - num_freed);
+		}
+	}
+}
diff --git a/drivers/net/octeon/ethernet-sgmii.c b/drivers/net/octeon/ethernet-sgmii.c
new file mode 100644
index 0000000..dcff1e5
--- /dev/null
+++ b/drivers/net/octeon/ethernet-sgmii.c
@@ -0,0 +1,104 @@
+/**********************************************************************
+ * Author: Cavium Networks
+ *
+ * Contact: support@caviumnetworks.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2007 Cavium Networks
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium Networks for more information
+**********************************************************************/
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <net/dst.h>
+
+#include <asm/octeon/octeon.h>
+#include <asm/octeon/cvmx-helper.h>
+
+#include <asm/octeon/cvmx-gmxx-defs.h>
+
+#include "ethernet-defines.h"
+#include "octeon-ethernet.h"
+#include "ethernet-util.h"
+
+
+int cvm_oct_sgmii_open(struct net_device *dev)
+{
+	union cvmx_gmxx_prtx_cfg gmx_cfg;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	int interface = INTERFACE(priv->port);
+	int index = INDEX(priv->port);
+	cvmx_helper_link_info_t link_info;
+
+	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
+	gmx_cfg.s.en = 1;
+	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmx_cfg.u64);
+
+	if (!octeon_is_simulation()) {
+		link_info = cvmx_helper_link_get(priv->port);
+		if (!link_info.s.link_up)
+			netif_carrier_off(dev);
+	}
+
+	return 0;
+}
+
+int cvm_oct_sgmii_stop(struct net_device *dev)
+{
+	union cvmx_gmxx_prtx_cfg gmx_cfg;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	int interface = INTERFACE(priv->port);
+	int index = INDEX(priv->port);
+
+	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
+	gmx_cfg.s.en = 0;
+	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmx_cfg.u64);
+	return 0;
+}
+
+static void cvm_oct_sgmii_poll(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	cvmx_helper_link_info_t link_info;
+
+	link_info = cvmx_helper_link_get(priv->port);
+	if (link_info.u64 == priv->link_info)
+		return;
+
+	link_info = cvmx_helper_link_autoconf(priv->port);
+	priv->link_info = link_info.u64;
+
+	/* Tell the core */
+	cvm_oct_set_carrier(priv, link_info);
+}
+
+int cvm_oct_sgmii_init(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	cvm_oct_common_init(dev);
+	dev->netdev_ops->ndo_stop(dev);
+	if (!octeon_is_simulation() && priv->phydev == NULL)
+		priv->poll = cvm_oct_sgmii_poll;
+	return 0;
+}
+
+void cvm_oct_sgmii_uninit(struct net_device *dev)
+{
+	cvm_oct_common_uninit(dev);
+}
diff --git a/drivers/net/octeon/ethernet-spi.c b/drivers/net/octeon/ethernet-spi.c
new file mode 100644
index 0000000..66d43f8
--- /dev/null
+++ b/drivers/net/octeon/ethernet-spi.c
@@ -0,0 +1,290 @@
+/**********************************************************************
+ * Author: Cavium Networks
+ *
+ * Contact: support@caviumnetworks.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2007 Cavium Networks
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium Networks for more information
+**********************************************************************/
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <net/dst.h>
+
+#include <asm/octeon/octeon.h>
+#include <asm/octeon/cvmx-spi.h>
+#include <asm/octeon/cvmx-npi-defs.h>
+#include <asm/octeon/cvmx-spxx-defs.h>
+#include <asm/octeon/cvmx-stxx-defs.h>
+
+#include "ethernet-defines.h"
+#include "octeon-ethernet.h"
+#include "ethernet-util.h"
+
+static int number_spi_ports;
+static int need_retrain[2] = { 0, 0 };
+
+static irqreturn_t cvm_oct_spi_rml_interrupt(int cpl, void *dev_id)
+{
+	irqreturn_t return_status = IRQ_NONE;
+	union cvmx_npi_rsl_int_blocks rsl_int_blocks;
+
+	/* Check and see if this interrupt was caused by the GMX block */
+	rsl_int_blocks.u64 = cvmx_read_csr(CVMX_NPI_RSL_INT_BLOCKS);
+	if (rsl_int_blocks.s.spx1) {	/* 19 - SPX1_INT_REG & STX1_INT_REG */
+
+		union cvmx_spxx_int_reg spx_int_reg;
+		union cvmx_stxx_int_reg stx_int_reg;
+
+		spx_int_reg.u64 = cvmx_read_csr(CVMX_SPXX_INT_REG(1));
+		cvmx_write_csr(CVMX_SPXX_INT_REG(1), spx_int_reg.u64);
+		if (!need_retrain[1]) {
+
+			spx_int_reg.u64 &= cvmx_read_csr(CVMX_SPXX_INT_MSK(1));
+			if (spx_int_reg.s.spf)
+				pr_err("SPI1: SRX Spi4 interface down\n");
+			if (spx_int_reg.s.calerr)
+				pr_err("SPI1: SRX Spi4 Calendar table parity error\n");
+			if (spx_int_reg.s.syncerr)
+				pr_err("SPI1: SRX Consecutive Spi4 DIP4 errors have exceeded SPX_ERR_CTL[ERRCNT]\n");
+			if (spx_int_reg.s.diperr)
+				pr_err("SPI1: SRX Spi4 DIP4 error\n");
+			if (spx_int_reg.s.tpaovr)
+				pr_err("SPI1: SRX Selected port has hit TPA overflow\n");
+			if (spx_int_reg.s.rsverr)
+				pr_err("SPI1: SRX Spi4 reserved control word detected\n");
+			if (spx_int_reg.s.drwnng)
+				pr_err("SPI1: SRX Spi4 receive FIFO drowning/overflow\n");
+			if (spx_int_reg.s.clserr)
+				pr_err("SPI1: SRX Spi4 packet closed on non-16B alignment without EOP\n");
+			if (spx_int_reg.s.spiovr)
+				pr_err("SPI1: SRX Spi4 async FIFO overflow\n");
+			if (spx_int_reg.s.abnorm)
+				pr_err("SPI1: SRX Abnormal packet termination (ERR bit)\n");
+			if (spx_int_reg.s.prtnxa)
+				pr_err("SPI1: SRX Port out of range\n");
+		}
+
+		stx_int_reg.u64 = cvmx_read_csr(CVMX_STXX_INT_REG(1));
+		cvmx_write_csr(CVMX_STXX_INT_REG(1), stx_int_reg.u64);
+		if (!need_retrain[1]) {
+
+			stx_int_reg.u64 &= cvmx_read_csr(CVMX_STXX_INT_MSK(1));
+			if (stx_int_reg.s.syncerr)
+				pr_err("SPI1: STX Interface encountered a fatal error\n");
+			if (stx_int_reg.s.frmerr)
+				pr_err("SPI1: STX FRMCNT has exceeded STX_DIP_CNT[MAXFRM]\n");
+			if (stx_int_reg.s.unxfrm)
+				pr_err("SPI1: STX Unexpected framing sequence\n");
+			if (stx_int_reg.s.nosync)
+				pr_err("SPI1: STX ERRCNT has exceeded STX_DIP_CNT[MAXDIP]\n");
+			if (stx_int_reg.s.diperr)
+				pr_err("SPI1: STX DIP2 error on the Spi4 Status channel\n");
+			if (stx_int_reg.s.datovr)
+				pr_err("SPI1: STX Spi4 FIFO overflow error\n");
+			if (stx_int_reg.s.ovrbst)
+				pr_err("SPI1: STX Transmit packet burst too big\n");
+			if (stx_int_reg.s.calpar1)
+				pr_err("SPI1: STX Calendar Table Parity Error Bank1\n");
+			if (stx_int_reg.s.calpar0)
+				pr_err("SPI1: STX Calendar Table Parity Error Bank0\n");
+		}
+
+		cvmx_write_csr(CVMX_SPXX_INT_MSK(1), 0);
+		cvmx_write_csr(CVMX_STXX_INT_MSK(1), 0);
+		need_retrain[1] = 1;
+		return_status = IRQ_HANDLED;
+	}
+
+	if (rsl_int_blocks.s.spx0) {	/* 18 - SPX0_INT_REG & STX0_INT_REG */
+		union cvmx_spxx_int_reg spx_int_reg;
+		union cvmx_stxx_int_reg stx_int_reg;
+
+		spx_int_reg.u64 = cvmx_read_csr(CVMX_SPXX_INT_REG(0));
+		cvmx_write_csr(CVMX_SPXX_INT_REG(0), spx_int_reg.u64);
+		if (!need_retrain[0]) {
+
+			spx_int_reg.u64 &= cvmx_read_csr(CVMX_SPXX_INT_MSK(0));
+			if (spx_int_reg.s.spf)
+				pr_err("SPI0: SRX Spi4 interface down\n");
+			if (spx_int_reg.s.calerr)
+				pr_err("SPI0: SRX Spi4 Calendar table parity error\n");
+			if (spx_int_reg.s.syncerr)
+				pr_err("SPI0: SRX Consecutive Spi4 DIP4 errors have exceeded SPX_ERR_CTL[ERRCNT]\n");
+			if (spx_int_reg.s.diperr)
+				pr_err("SPI0: SRX Spi4 DIP4 error\n");
+			if (spx_int_reg.s.tpaovr)
+				pr_err("SPI0: SRX Selected port has hit TPA overflow\n");
+			if (spx_int_reg.s.rsverr)
+				pr_err("SPI0: SRX Spi4 reserved control word detected\n");
+			if (spx_int_reg.s.drwnng)
+				pr_err("SPI0: SRX Spi4 receive FIFO drowning/overflow\n");
+			if (spx_int_reg.s.clserr)
+				pr_err("SPI0: SRX Spi4 packet closed on non-16B alignment without EOP\n");
+			if (spx_int_reg.s.spiovr)
+				pr_err("SPI0: SRX Spi4 async FIFO overflow\n");
+			if (spx_int_reg.s.abnorm)
+				pr_err("SPI0: SRX Abnormal packet termination (ERR bit)\n");
+			if (spx_int_reg.s.prtnxa)
+				pr_err("SPI0: SRX Port out of range\n");
+		}
+
+		stx_int_reg.u64 = cvmx_read_csr(CVMX_STXX_INT_REG(0));
+		cvmx_write_csr(CVMX_STXX_INT_REG(0), stx_int_reg.u64);
+		if (!need_retrain[0]) {
+
+			stx_int_reg.u64 &= cvmx_read_csr(CVMX_STXX_INT_MSK(0));
+			if (stx_int_reg.s.syncerr)
+				pr_err("SPI0: STX Interface encountered a fatal error\n");
+			if (stx_int_reg.s.frmerr)
+				pr_err("SPI0: STX FRMCNT has exceeded STX_DIP_CNT[MAXFRM]\n");
+			if (stx_int_reg.s.unxfrm)
+				pr_err("SPI0: STX Unexpected framing sequence\n");
+			if (stx_int_reg.s.nosync)
+				pr_err("SPI0: STX ERRCNT has exceeded STX_DIP_CNT[MAXDIP]\n");
+			if (stx_int_reg.s.diperr)
+				pr_err("SPI0: STX DIP2 error on the Spi4 Status channel\n");
+			if (stx_int_reg.s.datovr)
+				pr_err("SPI0: STX Spi4 FIFO overflow error\n");
+			if (stx_int_reg.s.ovrbst)
+				pr_err("SPI0: STX Transmit packet burst too big\n");
+			if (stx_int_reg.s.calpar1)
+				pr_err("SPI0: STX Calendar Table Parity Error Bank1\n");
+			if (stx_int_reg.s.calpar0)
+				pr_err("SPI0: STX Calendar Table Parity Error Bank0\n");
+		}
+
+		cvmx_write_csr(CVMX_SPXX_INT_MSK(0), 0);
+		cvmx_write_csr(CVMX_STXX_INT_MSK(0), 0);
+		need_retrain[0] = 1;
+		return_status = IRQ_HANDLED;
+	}
+
+	return return_status;
+}
+
+static void cvm_oct_spi_enable_error_reporting(int interface)
+{
+	union cvmx_spxx_int_msk spxx_int_msk;
+	union cvmx_stxx_int_msk stxx_int_msk;
+
+	spxx_int_msk.u64 = cvmx_read_csr(CVMX_SPXX_INT_MSK(interface));
+	spxx_int_msk.s.calerr = 1;
+	spxx_int_msk.s.syncerr = 1;
+	spxx_int_msk.s.diperr = 1;
+	spxx_int_msk.s.tpaovr = 1;
+	spxx_int_msk.s.rsverr = 1;
+	spxx_int_msk.s.drwnng = 1;
+	spxx_int_msk.s.clserr = 1;
+	spxx_int_msk.s.spiovr = 1;
+	spxx_int_msk.s.abnorm = 1;
+	spxx_int_msk.s.prtnxa = 1;
+	cvmx_write_csr(CVMX_SPXX_INT_MSK(interface), spxx_int_msk.u64);
+
+	stxx_int_msk.u64 = cvmx_read_csr(CVMX_STXX_INT_MSK(interface));
+	stxx_int_msk.s.frmerr = 1;
+	stxx_int_msk.s.unxfrm = 1;
+	stxx_int_msk.s.nosync = 1;
+	stxx_int_msk.s.diperr = 1;
+	stxx_int_msk.s.datovr = 1;
+	stxx_int_msk.s.ovrbst = 1;
+	stxx_int_msk.s.calpar1 = 1;
+	stxx_int_msk.s.calpar0 = 1;
+	cvmx_write_csr(CVMX_STXX_INT_MSK(interface), stxx_int_msk.u64);
+}
+
+static void cvm_oct_spi_poll(struct net_device *dev)
+{
+	static int spi4000_port;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	int interface;
+
+	for (interface = 0; interface < 2; interface++) {
+
+		if ((priv->port == interface * 16) && need_retrain[interface]) {
+
+			if (cvmx_spi_restart_interface(interface, CVMX_SPI_MODE_DUPLEX, 10) == 0) {
+				need_retrain[interface] = 0;
+				cvm_oct_spi_enable_error_reporting(interface);
+			}
+		}
+
+		/*
+		 * The SPI4000 TWSI interface is very slow. In order
+		 * not to bring the system to a crawl, we only poll a
+		 * single port every second. This means negotiation
+		 * speed changes take up to 10 seconds, but at least
+		 * we don't waste absurd amounts of time waiting for
+		 * TWSI.
+		 */
+		if (priv->port == spi4000_port) {
+			/*
+			 * This function does nothing if it is called on an
+			 * interface without a SPI4000.
+			 */
+			cvmx_spi4000_check_speed(interface, priv->port);
+			/*
+			 * Normal ordering increments. By decrementing
+			 * we only match once per iteration.
+			 */
+			spi4000_port--;
+			if (spi4000_port < 0)
+				spi4000_port = 10;
+		}
+	}
+}
+
+int cvm_oct_spi_init(struct net_device *dev)
+{
+	int r;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	if (number_spi_ports == 0) {
+		r = request_irq(OCTEON_IRQ_RML, cvm_oct_spi_rml_interrupt,
+				IRQF_SHARED, "SPI", &number_spi_ports);
+		if (r != 0) {
+			pr_err("%s: request_irq(%d) failed\n", dev->name, OCTEON_IRQ_RML);
+			return r;
+		}
+	}
+	number_spi_ports++;
+
+	if ((priv->port == 0) || (priv->port == 16)) {
+		cvm_oct_spi_enable_error_reporting(INTERFACE(priv->port));
+		priv->poll = cvm_oct_spi_poll;
+	}
+	cvm_oct_common_init(dev);
+	return 0;
+}
+
+void cvm_oct_spi_uninit(struct net_device *dev)
+{
+	int interface;
+
+	cvm_oct_common_uninit(dev);
+	number_spi_ports--;
+	if (number_spi_ports == 0) {
+		for (interface = 0; interface < 2; interface++) {
+			cvmx_write_csr(CVMX_SPXX_INT_MSK(interface), 0);
+			cvmx_write_csr(CVMX_STXX_INT_MSK(interface), 0);
+		}
+		free_irq(OCTEON_IRQ_RML, &number_spi_ports);
+	}
+}
diff --git a/drivers/net/octeon/ethernet-srio.c b/drivers/net/octeon/ethernet-srio.c
new file mode 100644
index 0000000..222690f
--- /dev/null
+++ b/drivers/net/octeon/ethernet-srio.c
@@ -0,0 +1,209 @@
+/*************************************************************************
+* Cavium Octeon Ethernet Driver
+*
+* Author: Cavium Networks info@caviumnetworks.com
+*
+* Copyright (c) 2010  Cavium Networks (support@cavium.com). All rights
+* reserved.
+*
+* Redistribution and use in source and binary forms, with or without
+* modification, are permitted provided that the following conditions are
+* met:
+*
+*     * Redistributions of source code must retain the above copyright
+*       notice, this list of conditions and the following disclaimer.
+*
+*     * Redistributions in binary form must reproduce the above
+*       copyright notice, this list of conditions and the following
+*       disclaimer in the documentation and/or other materials provided
+*       with the distribution.
+*
+*     * Neither the name of Cavium Networks nor the names of
+*       its contributors may be used to endorse or promote products
+*       derived from this software without specific prior written
+*       permission.
+*
+* TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+* AND WITH ALL FAULTS AND CAVIUM NETWORKS MAKES NO PROMISES, REPRESENTATIONS
+* OR WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH
+* RESPECT TO THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY
+* REPRESENTATION OR DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT
+* DEFECTS, AND CAVIUM SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES
+* OF TITLE, MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR
+* PURPOSE, LACK OF VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET
+* POSSESSION OR CORRESPONDENCE TO DESCRIPTION.  THE ENTIRE RISK ARISING OUT
+* OF USE OR PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+*************************************************************************/
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <net/dst.h>
+#include <net/sock.h>
+#include <linux/rio.h>
+#include "../../rapidio/rio.h"
+
+#include <asm/octeon/octeon.h>
+#include <asm/octeon/cvmx-srio.h>
+
+#include "octeon-ethernet.h"
+#include "ethernet-tx.h"
+#include "ethernet-mdio.h"
+#include "ethernet-util.h"
+
+/**
+ * Get ethernet counters for port/mailbox
+ *
+ * @param dev
+ *
+ * @return
+ */
+struct net_device_stats *cvm_oct_srio_get_stats(struct net_device *dev)
+{
+	return &dev->stats;
+}
+
+/**
+ * Change the MAC address
+ *
+ * @param dev
+ * @param addr
+ *
+ * @return
+ */
+int cvm_oct_srio_set_mac_address(struct net_device *dev, void *addr)
+{
+	/* FIXME: Should this be allowed? Should it change our device ID? */
+	memcpy(dev->dev_addr, addr + 2, 6);
+	return 0;
+}
+
+/**
+ * Change the MTU
+ *
+ * @param dev
+ * @param new_mtu
+ *
+ * @return
+ */
+int cvm_oct_srio_change_mtu(struct net_device *dev, int new_mtu)
+{
+	/*
+	 * Limit the MTU to make sure the ethernet packets are between
+	 * 1 byte and 4096-14 bytes.
+	 */
+	if ((new_mtu < 1) || (new_mtu > 4096 - 14)) {
+		printk(KERN_WARNING "%s: MTU must be between %d and %d.\n", dev->name, 1, 4096-14);
+		return -EINVAL;
+	}
+	dev->mtu = new_mtu;
+	return 0;
+}
+
+/**
+ * Packet transmit for a SRIO port
+ *
+ * @param skb    Packet to send
+ * @param dev    Device info structure
+ *
+ * @return Always returns zero
+ */
+int cvm_oct_xmit_srio(struct sk_buff *skb, struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	cvmx_srio_tx_message_header_t tx_header;
+	uint64_t dest_mac;
+
+	if (unlikely(skb->len > 4096)) {
+		dev_kfree_skb(skb);
+		DEBUGPRINT("%s: TX packet larger than 4096 bytes. Dropped.\n", dev->name);
+		return 0;
+	}
+
+	tx_header.u64 = priv->srio_tx_header;
+	/* Use the socket priority if it is available */
+	if (skb->sk) {
+		if (skb->sk->sk_priority < 0)
+			tx_header.s.prio = 0;
+		else if (skb->sk->sk_priority > 3)
+			tx_header.s.prio = 3;
+		else
+			tx_header.s.prio = skb->sk->sk_priority;
+	}
+
+	/* Extract the destination MAC address from the packet */
+	dest_mac = *(uint64_t *)skb->data >> 16;
+
+	/* If this is a broadcast we must manually send to everyone */
+	if (dest_mac == 0xffffffffffffull) {
+		struct rio_dev *rdev;
+		struct sk_buff *new_skb;
+
+		/* The RIO device list must be protected by a global lock */
+		spin_lock(&rio_global_list_lock);
+		list_for_each_entry(rdev, &rio_devices, global_list) {
+			/* Skip devices not on my rio port */
+			if (rdev->net->hport->id != (priv->port&1))
+				continue;
+			/* Create a new SKB since each packet will have different data */
+			new_skb = skb_copy(skb, GFP_ATOMIC);
+			if (new_skb) {
+				tx_header.s.did = rdev->destid;
+				*(uint64_t *)__skb_push(new_skb, 8) = tx_header.u64;
+				cvm_oct_xmit(new_skb, dev);
+			} else {
+				DEBUGPRINT("%s: SKB allocation failed\n", dev->name);
+				break;
+			}
+		}
+		spin_unlock(&rio_global_list_lock);
+
+		dev->stats.tx_packets++;
+		dev->stats.tx_bytes += skb->len;
+		dev_kfree_skb(skb);
+		return NETDEV_TX_OK;
+	} else {
+		/* Use the low two bytes of the destination MAC as the SRIO destination */
+		tx_header.s.did = *(uint16_t *)(skb->data + 4);
+		if (unlikely(skb_headroom(skb) < 8)) {
+			struct sk_buff *new_skb = skb_copy(skb, GFP_ATOMIC);
+			dev_kfree_skb(skb);
+			if (!new_skb) {
+				DEBUGPRINT("%s: SKB didn't have room for SRIO header and allocation failed\n", dev->name);
+				return NETDEV_TX_OK;
+			}
+			skb = new_skb;
+		}
+
+		dev->stats.tx_packets++;
+		dev->stats.tx_bytes += skb->len;
+		*(uint64_t *)__skb_push(skb, 8) = tx_header.u64;
+		return cvm_oct_xmit(skb, dev);
+	}
+}
+
+/**
+ * Initialize SRIO
+ *
+ * @param dev
+ *
+ * @return
+ */
+int cvm_oct_srio_init(struct net_device *dev)
+{
+	struct sockaddr sa;
+
+	dev->features |= NETIF_F_LLTX; /* We do our own locking, Linux doesn't need to */
+
+	SET_ETHTOOL_OPS(dev, &cvm_oct_ethtool_ops);
+
+	sa.sa_data[0] = 0;
+	sa.sa_data[1] = 0;
+	sa.sa_data[2] = 0;
+	sa.sa_data[3] = 0;
+	sa.sa_data[4] = 0; /* FIXME Device ID */
+	sa.sa_data[5] = priv->port;
+
+	dev->netdev_ops->ndo_set_mac_address(dev, &sa);
+	dev->netdev_ops->ndo_change_mtu(dev, dev->mtu);
+
+	return 0;
+}
diff --git a/drivers/net/octeon/ethernet-tx.c b/drivers/net/octeon/ethernet-tx.c
new file mode 100644
index 0000000..65c4131
--- /dev/null
+++ b/drivers/net/octeon/ethernet-tx.c
@@ -0,0 +1,342 @@
+/*********************************************************************
+ * Author: Cavium Networks
+ *
+ * Contact: support@caviumnetworks.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2010 Cavium Networks
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium Networks for more information
+*********************************************************************/
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/init.h>
+#include <linux/etherdevice.h>
+#include <linux/ip.h>
+#include <linux/string.h>
+#include <net/dst.h>
+#ifdef CONFIG_XFRM
+#include <linux/xfrm.h>
+#include <net/xfrm.h>
+#endif /* CONFIG_XFRM */
+
+#include <asm/octeon/octeon.h>
+#include <asm/octeon/cvmx-wqe.h>
+#include <asm/octeon/cvmx-fau.h>
+#include <asm/octeon/cvmx-pip.h>
+#include <asm/octeon/cvmx-pko.h>
+#include <asm/octeon/cvmx-helper.h>
+
+#include <asm/octeon/cvmx-gmxx-defs.h>
+
+#include "ethernet-defines.h"
+#include "octeon-ethernet.h"
+#include "ethernet-tx.h"
+#include "ethernet-util.h"
+
+
+#define CVM_OCT_SKB_CB(skb)	((u64 *)((skb)->cb))
+
+/*
+ * You can define GET_SKBUFF_QOS() to override how the skbuff output
+ * function determines which output queue is used. The default
+ * implementation always uses the base queue for the port. If, for
+ * example, you wanted to use the skb->priority fieid, define
+ * GET_SKBUFF_QOS as: #define GET_SKBUFF_QOS(skb) ((skb)->priority)
+ */
+#ifndef GET_SKBUFF_QOS
+#define GET_SKBUFF_QOS(skb) 0
+#else
+
+#ifdef CONFIG_OCTEON_ETHERNET_LOCKLESS_IF_SUPPORTED
+#error "GET_SKBUFF_QOS doesn't work with LOCKLESS"
+#endif
+
+#endif
+
+#ifdef CONFIG_OCTEON_ETHERNET_LOCKLESS_IF_SUPPORTED
+#define CVM_OCT_LOCKLESS 1
+#include "ethernet-xmit.c"
+#endif
+
+#undef CVM_OCT_LOCKLESS
+#include "ethernet-xmit.c"
+
+/**
+ * cvm_oct_xmit_pow - transmit a packet to the POW
+ * @skb:    Packet to send
+ * @dev:    Device info structure
+
+ * Returns Always returns zero
+ */
+int cvm_oct_xmit_pow(struct sk_buff *skb, struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	void               *packet_buffer;
+	void               *copy_location;
+	cvmx_buf_ptr_t     *buffer_ptr;
+	int remaining_bytes = skb->len;
+	int offset          = 0;
+	int length;
+
+	/* Get a work queue entry */
+	cvmx_wqe_t *work = cvmx_fpa_alloc(CVMX_FPA_WQE_POOL);
+	if (unlikely(work == NULL)) {
+		DEBUGPRINT("%s: Failed to allocate a work queue entry\n", dev->name);
+		dev->stats.tx_dropped++;
+		dev_kfree_skb(skb);
+		return 0;
+	}
+
+	/* Fill in some of the work queue fields. We may need to add more
+	   if the software at the other end needs them */
+	work->hw_chksum     = skb->csum;
+	work->len           = skb->len;
+	work->ipprt         = priv->port;
+	work->qos           = priv->port & 0x7;
+	work->grp           = pow_send_group;
+	work->tag_type      = CVMX_HELPER_INPUT_TAG_TYPE;
+	work->tag           = pow_send_group; /* FIXME */
+	work->word2.u64     = 0;    /* Default to zero. Sets of zero later are commented out */
+	work->word2.s.bufs  = 0;
+	work->packet_ptr.u64 = 0;
+
+	buffer_ptr = &work->packet_ptr;
+
+	/* Get a packet buffer */
+	packet_buffer = cvmx_fpa_alloc(CVMX_FPA_PACKET_POOL);
+	if (unlikely(packet_buffer == NULL)) {
+		DEBUGPRINT("%s: Failed to allocate a packet buffer\n",
+			   dev->name);
+		cvmx_fpa_free(work, CVMX_FPA_WQE_POOL, DONT_WRITEBACK(1));
+		dev->stats.tx_dropped++;
+		dev_kfree_skb(skb);
+		return 0;
+	}
+
+	/* Calculate where we need to copy the data to. We need to leave 8 bytes
+	   for a next pointer. */
+	copy_location = packet_buffer + sizeof(cvmx_buf_ptr_t);
+
+	/* We also need to include any configure skip. Then we need to align
+	   the IP packet src and dest into the same 64bit word. The below
+	   calculation may add a little extra, but that doesn't hurt */
+	copy_location += ((CVMX_HELPER_FIRST_MBUFF_SKIP+7)&0xfff8) + 6;
+
+	do {
+		length = CVMX_FPA_PACKET_POOL_SIZE;
+		length -= copy_location - packet_buffer;
+
+		/* Copy the remaining bytes in the last buffer. */
+		if (remaining_bytes < length)
+			length = remaining_bytes;
+
+		/* We have to copy the packet since whoever processes this packet
+		   will free it to a hardware pool. We can't use the trick of
+		   counting outstanding packets like in cvm_oct_xmit */
+		memcpy(copy_location, skb->data + offset, length);
+
+		buffer_ptr->s.addr = cvmx_ptr_to_phys(copy_location);
+		buffer_ptr->s.pool = CVMX_FPA_PACKET_POOL;
+		buffer_ptr->s.size = length;
+		buffer_ptr->s.back = (copy_location - packet_buffer) >> 7;
+
+		/* Increment the number of packet pointers used. */
+		work->word2.s.bufs++;
+
+		offset += length;
+		remaining_bytes -= length;
+
+		/* Create new packet buffer to fill the remaining data. */
+		if (remaining_bytes) {
+			packet_buffer = cvmx_fpa_alloc(CVMX_FPA_PACKET_POOL);
+			if (unlikely(packet_buffer == NULL)) {
+				DEBUGPRINT("%s: Failed to allocate a packet buffer\n",
+					   dev->name);
+				cvm_oct_free_work(work);
+				dev->stats.tx_dropped++;
+				dev_kfree_skb(skb);
+				return 0;
+			}
+			buffer_ptr = copy_location - sizeof(cvmx_buf_ptr_t);
+			copy_location = packet_buffer + sizeof(cvmx_buf_ptr_t);
+		}
+	} while (remaining_bytes);
+
+	if (skb->protocol == htons(ETH_P_IP)) {
+		work->word2.s.ip_offset     = 14;
+#if 0
+		work->word2.s.vlan_valid  = 0; /* FIXME */
+		work->word2.s.vlan_cfi    = 0; /* FIXME */
+		work->word2.s.vlan_id     = 0; /* FIXME */
+		work->word2.s.dec_ipcomp  = 0; /* FIXME */
+#endif
+		work->word2.s.tcp_or_udp    = (ip_hdr(skb)->protocol == IPPROTO_TCP) || (ip_hdr(skb)->protocol == IPPROTO_UDP);
+#if 0
+		work->word2.s.dec_ipsec   = 0; /* FIXME */
+		work->word2.s.is_v6       = 0; /* We only support IPv4 right now */
+		work->word2.s.software    = 0; /* Hardware would set to zero */
+		work->word2.s.L4_error    = 0; /* No error, packet is internal */
+#endif
+		work->word2.s.is_frag       = !((ip_hdr(skb)->frag_off == 0) || (ip_hdr(skb)->frag_off == 1<<14));
+#if 0
+		work->word2.s.IP_exc      = 0;  /* Assume Linux is sending a good packet */
+#endif
+		work->word2.s.is_bcast      = (skb->pkt_type == PACKET_BROADCAST);
+		work->word2.s.is_mcast      = (skb->pkt_type == PACKET_MULTICAST);
+#if 0
+		work->word2.s.not_IP      = 0; /* This is an IP packet */
+		work->word2.s.rcv_error   = 0; /* No error, packet is internal */
+		work->word2.s.err_code    = 0; /* No error, packet is internal */
+#endif
+
+		/* When copying the data, include 4 bytes of the ethernet header to
+		   align the same way hardware does */
+		memcpy(work->packet_data, skb->data + 10, sizeof(work->packet_data));
+	} else {
+#if 0
+		work->word2.snoip.vlan_valid  = 0; /* FIXME */
+		work->word2.snoip.vlan_cfi    = 0; /* FIXME */
+		work->word2.snoip.vlan_id     = 0; /* FIXME */
+		work->word2.snoip.software    = 0; /* Hardware would set to zero */
+#endif
+		work->word2.snoip.is_rarp       = skb->protocol == htons(ETH_P_RARP);
+		work->word2.snoip.is_arp        = skb->protocol == htons(ETH_P_ARP);
+		work->word2.snoip.is_bcast      = (skb->pkt_type == PACKET_BROADCAST);
+		work->word2.snoip.is_mcast      = (skb->pkt_type == PACKET_MULTICAST);
+		work->word2.snoip.not_IP        = 1; /* IP was done up above */
+#if 0
+		work->word2.snoip.rcv_error   = 0; /* No error, packet is internal */
+		work->word2.snoip.err_code    = 0; /* No error, packet is internal */
+#endif
+		memcpy(work->packet_data, skb->data, sizeof(work->packet_data));
+	}
+
+	/* Submit the packet to the POW */
+	cvmx_pow_work_submit(work, work->tag, work->tag_type, work->qos, work->grp);
+	dev->stats.tx_packets++;
+	dev->stats.tx_bytes += skb->len;
+	dev_kfree_skb(skb);
+	return 0;
+}
+/**
+ * cvm_oct_transmit_qos - transmit a work queue entry out of the ethernet port.
+ *
+ * Both the work queue entry and the packet data can optionally be
+ * freed. The work will be freed on error as well.
+ *
+ * @dev: Device to transmit out.
+ * @work_queue_entry: Work queue entry to send
+ * @do_free: True if the work queue entry and packet data should be
+ *           freed. If false, neither will be freed.
+ * @qos: Index into the queues for this port to transmit on. This is
+ *       used to implement QoS if their are multiple queues per
+ *       port. This parameter must be between 0 and the number of
+ *       queues per port minus 1. Values outside of this range will be
+ *       change to zero.
+ *
+ * Returns Zero on success, negative on failure.
+ */
+int cvm_oct_transmit_qos(struct net_device *dev,
+			 void *work_queue_entry,
+			 int do_free,
+			 int qos)
+{
+	unsigned long			flags;
+	cvmx_buf_ptr_t			hw_buffer;
+	cvmx_pko_command_word0_t	pko_command;
+	int				dropped;
+	struct octeon_ethernet		*priv = netdev_priv(dev);
+	cvmx_wqe_t			*work = work_queue_entry;
+	cvmx_pko_lock_t lock_type;
+
+	if (!(dev->flags & IFF_UP)) {
+		DEBUGPRINT("%s: Device not up\n", dev->name);
+		if (do_free)
+			cvm_oct_free_work(work);
+		return -1;
+	}
+
+	if (priv->flags & OCTEON_ETHERNET_FLAG_TX_LOCKLESS) {
+		qos = cvmx_get_core_num();
+		lock_type = CVMX_PKO_LOCK_NONE;
+	} else {
+		/*
+		 * The check on CVMX_PKO_QUEUES_PER_PORT_* is designed to
+		 * completely remove "qos" in the event neither interface
+		 * supports multiple queues per port
+		 */
+		if ((CVMX_PKO_QUEUES_PER_PORT_INTERFACE0 > 1) ||
+			(CVMX_PKO_QUEUES_PER_PORT_INTERFACE1 > 1)) {
+			if (qos <= 0)
+				qos = 0;
+			else if (qos >= priv->num_tx_queues)
+				qos = 0;
+		} else
+			qos = 0;
+		lock_type = CVMX_PKO_LOCK_CMD_QUEUE;
+	}
+
+	/* Start off assuming no drop */
+	dropped = 0;
+
+	local_irq_save(flags);
+
+	cvmx_pko_send_packet_prepare(priv->port, priv->tx_queue[qos].queue, lock_type);
+
+	/* Build the PKO buffer pointer */
+	hw_buffer.u64 = 0;
+	hw_buffer.s.addr = work->packet_ptr.s.addr;
+	hw_buffer.s.pool = CVMX_FPA_PACKET_POOL;
+	hw_buffer.s.size = CVMX_FPA_PACKET_POOL_SIZE;
+	hw_buffer.s.back = work->packet_ptr.s.back;
+
+	/* Build the PKO command */
+	pko_command.u64 = 0;
+	pko_command.s.n2 = 1; /* Don't pollute L2 with the outgoing packet */
+	pko_command.s.dontfree = !do_free;
+	pko_command.s.segs = work->word2.s.bufs;
+	pko_command.s.total_bytes = work->len;
+
+	/* Check if we can use the hardware checksumming */
+	if (unlikely(work->word2.s.not_IP || work->word2.s.IP_exc))
+		pko_command.s.ipoffp1 = 0;
+	else
+		pko_command.s.ipoffp1 = sizeof(struct ethhdr) + 1;
+
+	/* Send the packet to the output queue */
+	if (unlikely(cvmx_pko_send_packet_finish(priv->port, priv->tx_queue[qos].queue, pko_command, hw_buffer, lock_type))) {
+		DEBUGPRINT("%s: Failed to send the packet\n", dev->name);
+		dropped = -1;
+	}
+	local_irq_restore(flags);
+
+	if (unlikely(dropped)) {
+		if (do_free)
+			cvm_oct_free_work(work);
+		dev->stats.tx_dropped++;
+	} else
+	if (do_free)
+		cvmx_fpa_free(work, CVMX_FPA_WQE_POOL, DONT_WRITEBACK(1));
+
+	return dropped;
+}
+EXPORT_SYMBOL(cvm_oct_transmit_qos);
+
diff --git a/drivers/net/octeon/ethernet-tx.h b/drivers/net/octeon/ethernet-tx.h
new file mode 100644
index 0000000..68dfa0b
--- /dev/null
+++ b/drivers/net/octeon/ethernet-tx.h
@@ -0,0 +1,31 @@
+/*********************************************************************
+ * Author: Cavium Networks
+ *
+ * Contact: support@caviumnetworks.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2007 Cavium Networks
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium Networks for more information
+*********************************************************************/
+
+int cvm_oct_xmit(struct sk_buff *skb, struct net_device *dev);
+int cvm_oct_xmit_lockless(struct sk_buff *skb, struct net_device *dev);
+int cvm_oct_xmit_pow(struct sk_buff *skb, struct net_device *dev);
+
diff --git a/drivers/net/octeon/ethernet-util.h b/drivers/net/octeon/ethernet-util.h
new file mode 100644
index 0000000..5b1a5f6
--- /dev/null
+++ b/drivers/net/octeon/ethernet-util.h
@@ -0,0 +1,80 @@
+/**********************************************************************
+ * Author: Cavium Networks
+ *
+ * Contact: support@caviumnetworks.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2007 Cavium Networks
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium Networks for more information
+*********************************************************************/
+
+#include <asm/octeon/cvmx-pip.h>
+
+#define DEBUGPRINT(format, ...) do { if (printk_ratelimit())		\
+					printk(format, ##__VA_ARGS__);	\
+				} while (0)
+
+/**
+ * cvm_oct_get_buffer_ptr - convert packet data address to pointer
+ * @packet_ptr: Packet data hardware address
+ *
+ * Returns Packet buffer pointer
+ */
+static inline void *cvm_oct_get_buffer_ptr(union cvmx_buf_ptr packet_ptr)
+{
+	return cvmx_phys_to_ptr(((packet_ptr.s.addr >> 7) - packet_ptr.s.back)
+				<< 7);
+}
+
+/**
+ * INTERFACE - convert IPD port to locgical interface
+ * @ipd_port: Port to check
+ *
+ * Returns Logical interface
+ */
+static inline int INTERFACE(int ipd_port)
+{
+	int interface;
+
+	/* Non existant interface for POW0 */
+	if (ipd_port == CVMX_PIP_NUM_INPUT_PORTS)
+		return cvmx_helper_get_interface_num(ipd_port-1) + 1;
+
+	interface = cvmx_helper_get_interface_num(ipd_port);
+
+	if (interface == -1)
+		panic("Illegal ipd_port %d passed to INTERFACE\n", ipd_port);
+
+	return interface;
+}
+
+/**
+ * INDEX - convert IPD/PKO port number to the port's interface index
+ * @ipd_port: Port to check
+ *
+ * Returns Index into interface port list
+ */
+static inline int INDEX(int ipd_port)
+{
+	if (ipd_port == CVMX_PIP_NUM_INPUT_PORTS)
+		return 0;
+
+	return cvmx_helper_get_interface_index_num(ipd_port);
+}
diff --git a/drivers/net/octeon/ethernet-xaui.c b/drivers/net/octeon/ethernet-xaui.c
new file mode 100644
index 0000000..9004469
--- /dev/null
+++ b/drivers/net/octeon/ethernet-xaui.c
@@ -0,0 +1,102 @@
+/**********************************************************************
+ * Author: Cavium Networks
+ *
+ * Contact: support@caviumnetworks.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2007 Cavium Networks
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium Networks for more information
+**********************************************************************/
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <net/dst.h>
+
+#include <asm/octeon/octeon.h>
+#include <asm/octeon/cvmx-helper.h>
+#include <asm/octeon/cvmx-gmxx-defs.h>
+
+#include "ethernet-defines.h"
+#include "octeon-ethernet.h"
+#include "ethernet-util.h"
+
+int cvm_oct_xaui_open(struct net_device *dev)
+{
+	union cvmx_gmxx_prtx_cfg gmx_cfg;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	int interface = INTERFACE(priv->port);
+	int index = INDEX(priv->port);
+	cvmx_helper_link_info_t link_info;
+
+	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
+	gmx_cfg.s.en = 1;
+	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmx_cfg.u64);
+
+	if (!octeon_is_simulation()) {
+		link_info = cvmx_helper_link_get(priv->port);
+		if (!link_info.s.link_up)
+			netif_carrier_off(dev);
+	}
+	return 0;
+}
+
+int cvm_oct_xaui_stop(struct net_device *dev)
+{
+	union cvmx_gmxx_prtx_cfg gmx_cfg;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	int interface = INTERFACE(priv->port);
+	int index = INDEX(priv->port);
+
+	gmx_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
+	gmx_cfg.s.en = 0;
+	cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmx_cfg.u64);
+	return 0;
+}
+
+static void cvm_oct_xaui_poll(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	cvmx_helper_link_info_t link_info;
+
+	link_info = cvmx_helper_link_get(priv->port);
+	if (link_info.u64 == priv->link_info)
+		return;
+
+	link_info = cvmx_helper_link_autoconf(priv->port);
+	priv->link_info = link_info.u64;
+
+	/* Tell the core. */
+	cvm_oct_set_carrier(priv, link_info);
+}
+
+int cvm_oct_xaui_init(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	cvm_oct_common_init(dev);
+	dev->netdev_ops->ndo_stop(dev);
+	if (!octeon_is_simulation() && priv->phydev == NULL)
+		priv->poll = cvm_oct_xaui_poll;
+
+	return 0;
+}
+
+void cvm_oct_xaui_uninit(struct net_device *dev)
+{
+	cvm_oct_common_uninit(dev);
+}
diff --git a/drivers/net/octeon/ethernet-xmit.c b/drivers/net/octeon/ethernet-xmit.c
new file mode 100644
index 0000000..2f9e775
--- /dev/null
+++ b/drivers/net/octeon/ethernet-xmit.c
@@ -0,0 +1,419 @@
+/*********************************************************************
+ * Author: Cavium Networks
+ *
+ * Contact: support@caviumnetworks.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2010 Cavium Networks
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium Networks for more information
+*********************************************************************/
+
+#undef CVM_OCT_XMIT
+#undef CVM_OCT_PKO_LOCK_TYPE
+
+#ifdef CVM_OCT_LOCKLESS
+#define CVM_OCT_XMIT cvm_oct_xmit_lockless
+#define CVM_OCT_PKO_LOCK_TYPE CVMX_PKO_LOCK_NONE
+#else
+#define CVM_OCT_XMIT cvm_oct_xmit
+#define CVM_OCT_PKO_LOCK_TYPE CVMX_PKO_LOCK_CMD_QUEUE
+#endif
+
+/**
+ * cvm_oct_xmit - transmit a packet
+ * @skb:    Packet to send
+ * @dev:    Device info structure
+ *
+ * Returns Always returns NETDEV_TX_OK
+ */
+int
+CVM_OCT_XMIT
+(struct sk_buff *skb, struct net_device *dev)
+{
+	cvmx_pko_command_word0_t pko_command;
+	union cvmx_buf_ptr hw_buffer;
+	uint64_t old_scratch;
+	uint64_t old_scratch2;
+	int qos;
+	int i;
+	enum {QUEUE_HW, QUEUE_WQE, QUEUE_DROP} queue_type;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	int32_t queue_depth;
+	int32_t buffers_to_free;
+	unsigned long flags;
+	cvmx_wqe_t *work = NULL;
+#if REUSE_SKBUFFS_WITHOUT_FREE
+	unsigned char *fpa_head;
+#endif
+
+	/*
+	 * Prefetch the private data structure.  It is larger than one
+	 * cache line.
+	 */
+	prefetch(priv);
+
+	if (USE_ASYNC_IOBDMA) {
+		/* Save scratch in case userspace is using it */
+		CVMX_SYNCIOBDMA;
+		old_scratch = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
+		old_scratch2 = cvmx_scratch_read64(CVMX_SCR_SCRATCH + 8);
+
+		/*
+		 * Fetch and increment the number of packets to be
+		 * freed.
+		 */
+		cvmx_fau_async_fetch_and_add32(CVMX_SCR_SCRATCH + 8,
+					       FAU_NUM_PACKET_BUFFERS_TO_FREE,
+					       0);
+
+	}
+
+
+#ifdef CVM_OCT_LOCKLESS
+	qos = cvmx_get_core_num();
+#else
+	/*
+	 * The check on CVMX_PKO_QUEUES_PER_PORT_* is designed to
+	 * completely remove "qos" in the event neither interface
+	 * supports multiple queues per port.
+	 */
+	if ((CVMX_PKO_QUEUES_PER_PORT_INTERFACE0 > 1) ||
+		(CVMX_PKO_QUEUES_PER_PORT_INTERFACE1 > 1)) {
+		qos = GET_SKBUFF_QOS(skb);
+		if (qos <= 0)
+			qos = 0;
+		else if (qos >= priv->num_tx_queues)
+			qos = 0;
+	} else
+		qos = 0;
+#endif
+
+	if (USE_ASYNC_IOBDMA) {
+		cvmx_fau_async_fetch_and_add32(CVMX_SCR_SCRATCH,
+					       priv->tx_queue[qos].fau, 1);
+	}
+
+	/*
+	 * We have space for 6 segment pointers, If there will be more
+	 * than that, we must linearize.
+	 */
+	if (unlikely(skb_shinfo(skb)->nr_frags > 5)) {
+		if (unlikely(__skb_linearize(skb))) {
+			queue_type = QUEUE_DROP;
+			goto skip_xmit;
+		}
+	}
+
+#ifndef CVM_OCT_LOCKLESS
+	/*
+	 * The CN3XXX series of parts has an errata (GMX-401) which
+	 * causes the GMX block to hang if a collision occurs towards
+	 * the end of a <68 byte packet. As a workaround for this, we
+	 * pad packets to be 68 bytes whenever we are in half duplex
+	 * mode. We don't handle the case of having a small packet but
+	 * no room to add the padding.  The kernel should always give
+	 * us at least a cache line
+	 */
+	if ((skb->len < 64) && OCTEON_IS_MODEL(OCTEON_CN3XXX)) {
+		union cvmx_gmxx_prtx_cfg gmx_prt_cfg;
+		int interface = INTERFACE(priv->port);
+		int index = INDEX(priv->port);
+
+		if (interface < 2) {
+			/* We only need to pad packet in half duplex mode */
+			gmx_prt_cfg.u64 = cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
+			if (gmx_prt_cfg.s.duplex == 0) {
+				int add_bytes = 64 - skb->len;
+				if ((skb_tail_pointer(skb) + add_bytes) <= skb_end_pointer(skb))
+					memset(__skb_put(skb, add_bytes), 0, add_bytes);
+			}
+		}
+	}
+#endif
+	/* Build the PKO command */
+	pko_command.u64 = 0;
+	pko_command.s.n2 = 1;	/* Don't pollute L2 with the outgoing packet */
+	pko_command.s.segs = 1;
+	pko_command.s.total_bytes = skb->len;
+	/* Use fau0 to decrement the number of packets queued */
+	pko_command.s.size0 = CVMX_FAU_OP_SIZE_32;
+	pko_command.s.subone0 = 1;
+	pko_command.s.reg0 = priv->tx_queue[qos].fau;
+	pko_command.s.dontfree = 1;
+
+	/* Build the PKO buffer pointer */
+	hw_buffer.u64 = 0;
+	if (skb_shinfo(skb)->nr_frags == 0) {
+		hw_buffer.s.addr = XKPHYS_TO_PHYS((u64)skb->data);
+		hw_buffer.s.pool = 0;
+		hw_buffer.s.size = skb->len;
+	} else {
+		hw_buffer.s.addr = XKPHYS_TO_PHYS((u64)skb->data);
+		hw_buffer.s.pool = 0;
+		hw_buffer.s.size = skb_headlen(skb);
+		CVM_OCT_SKB_CB(skb)[0] = hw_buffer.u64;
+		for (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {
+			struct skb_frag_struct *fs = skb_shinfo(skb)->frags + i;
+			hw_buffer.s.addr = XKPHYS_TO_PHYS((u64)(page_address(fs->page) + fs->page_offset));
+			hw_buffer.s.size = fs->size;
+			CVM_OCT_SKB_CB(skb)[i + 1] = hw_buffer.u64;
+		}
+		hw_buffer.s.addr = XKPHYS_TO_PHYS((u64)CVM_OCT_SKB_CB(skb));
+		hw_buffer.s.size = skb_shinfo(skb)->nr_frags + 1;
+		pko_command.s.segs = skb_shinfo(skb)->nr_frags + 1;
+		pko_command.s.gather = 1;
+		goto dont_put_skbuff_in_hw;
+	}
+
+	/*
+	 * See if we can put this skb in the FPA pool. Any strange
+	 * behavior from the Linux networking stack will most likely
+	 * be caused by a bug in the following code. If some field is
+	 * in use by the network stack and get carried over when a
+	 * buffer is reused, bad thing may happen.  If in doubt and
+	 * you dont need the absolute best performance, disable the
+	 * define REUSE_SKBUFFS_WITHOUT_FREE. The reuse of buffers has
+	 * shown a 25% increase in performance under some loads.
+	 */
+#if REUSE_SKBUFFS_WITHOUT_FREE
+	/* Timestamps are returned in the WQE, so we can't reuse the buffer */
+	if (unlikely(priv->flags & (OCTEON_ETHERNET_FLAG_TX_TIMESTAMP_SW |
+				    OCTEON_ETHERNET_FLAG_TX_TIMESTAMP_HW)))
+		goto dont_put_skbuff_in_hw;
+
+	fpa_head = skb->head + 256 - ((unsigned long)skb->head & 0x7f);
+	if (unlikely(skb->data < fpa_head)) {
+		/*
+		 * printk("TX buffer beginning can't meet FPA
+		 * alignment constraints\n");
+		 */
+		goto dont_put_skbuff_in_hw;
+	}
+	if (unlikely((skb_end_pointer(skb) - fpa_head) < CVMX_FPA_PACKET_POOL_SIZE)) {
+		/*
+		   printk("TX buffer isn't large enough for the FPA\n");
+		 */
+		goto dont_put_skbuff_in_hw;
+	}
+	if (unlikely(skb_shared(skb))) {
+		/*
+		   printk("TX buffer sharing data with someone else\n");
+		 */
+		goto dont_put_skbuff_in_hw;
+	}
+	if (unlikely(skb_cloned(skb))) {
+		/*
+		   printk("TX buffer has been cloned\n");
+		 */
+		goto dont_put_skbuff_in_hw;
+	}
+	if (unlikely(skb_header_cloned(skb))) {
+		/*
+		   printk("TX buffer header has been cloned\n");
+		 */
+		goto dont_put_skbuff_in_hw;
+	}
+	if (unlikely(skb->destructor)) {
+		/*
+		   printk("TX buffer has a destructor\n");
+		 */
+		goto dont_put_skbuff_in_hw;
+	}
+	if (unlikely(skb_shinfo(skb)->nr_frags)) {
+		/*
+		   printk("TX buffer has fragments\n");
+		 */
+		goto dont_put_skbuff_in_hw;
+	}
+	if (unlikely(skb->truesize != sizeof(*skb) + skb_end_pointer(skb) - skb->head)) {
+		/*
+		   printk("TX buffer truesize has been changed\n");
+		 */
+		goto dont_put_skbuff_in_hw;
+	}
+
+	/*
+	 * We can use this buffer in the FPA.  We don't need the FAU
+	 * update anymore
+	 */
+	pko_command.s.dontfree = 0;
+
+	hw_buffer.s.back = ((unsigned long)skb->data >> 7) - ((unsigned long)fpa_head >> 7);
+	*(struct sk_buff **)(fpa_head - sizeof(void *)) = skb;
+
+	/*
+	 * The skbuff will be reused without ever being freed. We must
+	 * cleanup a bunch of core things.
+	 */
+	dst_release(skb_dst(skb));
+	skb_dst_set(skb, NULL);
+#ifdef CONFIG_XFRM
+	secpath_put(skb->sp);
+	skb->sp = NULL;
+#endif
+	nf_reset(skb);
+
+#ifdef CONFIG_NET_SCHED
+	skb->tc_index = 0;
+#ifdef CONFIG_NET_CLS_ACT
+	skb->tc_verd = 0;
+#endif /* CONFIG_NET_CLS_ACT */
+#endif /* CONFIG_NET_SCHED */
+#endif /* REUSE_SKBUFFS_WITHOUT_FREE */
+
+dont_put_skbuff_in_hw:
+
+	/* Check if we can use the hardware checksumming */
+	if (USE_HW_TCPUDP_CHECKSUM && (skb->protocol == htons(ETH_P_IP)) &&
+	    (ip_hdr(skb)->version == 4) && (ip_hdr(skb)->ihl == 5) &&
+	    ((ip_hdr(skb)->frag_off == 0) || (ip_hdr(skb)->frag_off == 1 << 14))
+	    && ((ip_hdr(skb)->protocol == IPPROTO_TCP) || (ip_hdr(skb)->protocol == IPPROTO_UDP))) {
+		/* Use hardware checksum calc */
+		pko_command.s.ipoffp1 = sizeof(struct ethhdr) + 1;
+	}
+
+	if (USE_ASYNC_IOBDMA) {
+		/* Get the number of skbuffs in use by the hardware */
+		CVMX_SYNCIOBDMA;
+		queue_depth = cvmx_scratch_read64(CVMX_SCR_SCRATCH);
+		buffers_to_free = cvmx_scratch_read64(CVMX_SCR_SCRATCH + 8);
+	} else {
+		/* Get the number of skbuffs in use by the hardware */
+		queue_depth = cvmx_fau_fetch_and_add32(priv->tx_queue[qos].fau, 1);
+		buffers_to_free = cvmx_fau_fetch_and_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, 0);
+	}
+
+	/*
+	 * If we're sending faster than the receive can free them then
+	 * don't do the HW free.
+	 */
+	if (unlikely(buffers_to_free < -100))
+		pko_command.s.dontfree = 1;
+
+	/* Drop this packet if we have too many already queued to the HW */
+	if (unlikely(queue_depth >= MAX_OUT_QUEUE_DEPTH)) {
+		if (dev->tx_queue_len != 0) {
+			netif_stop_queue(dev);
+		} else {
+			/* If not using normal queueing.  */
+			queue_type = QUEUE_DROP;
+			goto skip_xmit;
+		}
+	}
+
+	if (pko_command.s.dontfree)
+		queue_type = QUEUE_WQE;
+	else
+		queue_type = QUEUE_HW;
+
+	if (queue_type == QUEUE_WQE) {
+		work = cvmx_fpa_alloc(CVMX_FPA_TX_WQE_POOL);
+		if (unlikely(!work)) {
+			DEBUGPRINT("%s: Failed WQE allocate\n", dev->name);
+			queue_type = QUEUE_DROP;
+			goto skip_xmit;
+		}
+
+		pko_command.s.rsp = 1;
+		pko_command.s.wqp = 1;
+		work->hw_chksum = 0;
+		/*
+		 * work->unused will carry the qos for this packet,
+		 * this allows us to find the proper FAU when freeing
+		 * the packet.  We decrement the FAU when the WQE is
+		 * replaced in the pool.
+		 */
+		pko_command.s.reg0 = 0;
+		work->unused = (uint8_t)qos;
+
+		work->next_ptr = 0;
+		work->len = 0;
+		work->tag_type = CVMX_POW_TAG_TYPE_NULL;
+		work->tag = 0;
+		work->word2.u64 = 0;
+		work->word2.s.software = 1;
+		work->ipprt = priv->port;
+		/* Use a different queue for return packet than rx so a
+		    high load on one port is spread across multiple
+		    queues */
+		work->qos = (~priv->port) & 7;
+		work->grp = pow_receive_group;
+		work->packet_ptr.u64 = (unsigned long)skb;
+	}
+
+	local_irq_save(flags);
+
+	cvmx_pko_send_packet_prepare(priv->port, priv->tx_queue[qos].queue, CVM_OCT_PKO_LOCK_TYPE);
+
+	/* Send the packet to the output queue */
+	if (queue_type == QUEUE_WQE) {
+		uint64_t word2 = cvmx_ptr_to_phys(work);
+		if (priv->flags & OCTEON_ETHERNET_FLAG_TX_TIMESTAMP_SW) {
+			/* The first 8 bytes work->packet_data will get the timestamp */
+			*(uint64_t *)work->packet_data = ktime_to_ns(ktime_get_real());
+		}
+		if (priv->flags & OCTEON_ETHERNET_FLAG_TX_TIMESTAMP_HW) {
+			/* The first 8 bytes work->packet_data will get the timestamp */
+			*(uint64_t *)work->packet_data = 0;
+			word2 |= 1ull<<40; /* Bit 40 controls timestamps */
+		}
+		if (unlikely(cvmx_pko_send_packet_finish3(priv->port,
+							  priv->tx_queue[qos].queue, pko_command, hw_buffer,
+							  word2, CVM_OCT_PKO_LOCK_TYPE))) {
+				queue_type = QUEUE_DROP;
+				cvmx_fpa_free(work, CVMX_FPA_TX_WQE_POOL, 0);
+				DEBUGPRINT("%s: Failed to send the packet with wqe\n", dev->name);
+		}
+	} else {
+		if (unlikely(cvmx_pko_send_packet_finish(priv->port,
+							 priv->tx_queue[qos].queue,
+							 pko_command, hw_buffer,
+							 CVM_OCT_PKO_LOCK_TYPE))) {
+			DEBUGPRINT("%s: Failed to send the packet\n", dev->name);
+			queue_type = QUEUE_DROP;
+		}
+	}
+	local_irq_restore(flags);
+
+skip_xmit:
+	switch (queue_type) {
+	case QUEUE_DROP:
+		cvmx_fau_atomic_add32(priv->tx_queue[qos].fau, -1);
+		dev_kfree_skb_any(skb);
+		dev->stats.tx_dropped++;
+		break;
+	case QUEUE_HW:
+		cvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE, -1);
+		break;
+	case QUEUE_WQE:
+		/* Cleanup is done on the RX path when the WQE returns */
+		break;
+	default:
+		BUG();
+	}
+
+	if (USE_ASYNC_IOBDMA) {
+		CVMX_SYNCIOBDMA;
+		/* Restore the scratch area */
+		cvmx_scratch_write64(CVMX_SCR_SCRATCH, old_scratch);
+		cvmx_scratch_write64(CVMX_SCR_SCRATCH + 8, old_scratch2);
+	}
+	return NETDEV_TX_OK;
+}
diff --git a/drivers/net/octeon/ethernet.c b/drivers/net/octeon/ethernet.c
new file mode 100644
index 0000000..95eec18
--- /dev/null
+++ b/drivers/net/octeon/ethernet.c
@@ -0,0 +1,1026 @@
+/**********************************************************************
+ * Author: Cavium Networks
+ *
+ * Contact: support@caviumnetworks.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2007 Cavium Networks
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium Networks for more information
+**********************************************************************/
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/phy.h>
+
+#include <net/dst.h>
+
+#include <asm/octeon/octeon.h>
+#include <asm/octeon/cvmx-pip.h>
+#include <asm/octeon/cvmx-pko.h>
+#include <asm/octeon/cvmx-fau.h>
+#include <asm/octeon/cvmx-ipd.h>
+#include <asm/octeon/cvmx-srio.h>
+#include <asm/octeon/cvmx-helper.h>
+
+#include <asm/octeon/cvmx-gmxx-defs.h>
+#include <asm/octeon/cvmx-smix-defs.h>
+
+#include "ethernet-defines.h"
+#include "octeon-ethernet.h"
+#include "ethernet-proc.h"
+#include "ethernet-mem.h"
+#include "ethernet-rx.h"
+#include "ethernet-tx.h"
+#include "ethernet-mdio.h"
+#include "ethernet-util.h"
+
+
+#if defined(CONFIG_OCTEON_NUM_PACKET_BUFFERS) \
+	&& CONFIG_OCTEON_NUM_PACKET_BUFFERS
+int num_packet_buffers = CONFIG_OCTEON_NUM_PACKET_BUFFERS;
+#else
+int num_packet_buffers = 1024;
+#endif
+module_param(num_packet_buffers, int, 0444);
+MODULE_PARM_DESC(num_packet_buffers, "\n"
+	"\tNumber of packet buffers to allocate and store in the\n"
+	"\tFPA. By default, 1024 packet buffers are used unless\n"
+	"\tCONFIG_OCTEON_NUM_PACKET_BUFFERS is defined.");
+
+int pow_receive_group = 15;
+module_param(pow_receive_group, int, 0444);
+MODULE_PARM_DESC(pow_receive_group, "\n"
+	"\tPOW group to receive packets from. All ethernet hardware\n"
+	"\twill be configured to send incomming packets to this POW\n"
+	"\tgroup. Also any other software can submit packets to this\n"
+	"\tgroup for the kernel to process.");
+
+int pow_send_group = -1;
+module_param(pow_send_group, int, 0644);
+MODULE_PARM_DESC(pow_send_group, "\n"
+	"\tPOW group to send packets to other software on. This\n"
+	"\tcontrols the creation of the virtual device pow0.\n"
+	"\talways_use_pow also depends on this value.");
+
+int always_use_pow;
+module_param(always_use_pow, int, 0444);
+MODULE_PARM_DESC(always_use_pow, "\n"
+	"\tWhen set, always send to the pow group. This will cause\n"
+	"\tpackets sent to real ethernet devices to be sent to the\n"
+	"\tPOW group instead of the hardware. Unless some other\n"
+	"\tapplication changes the config, packets will still be\n"
+	"\treceived from the low level hardware. Use this option\n"
+	"\tto allow a CVMX app to intercept all packets from the\n"
+	"\tlinux kernel. You must specify pow_send_group along with\n"
+	"\tthis option.");
+
+char pow_send_list[128] = "";
+module_param_string(pow_send_list, pow_send_list, sizeof(pow_send_list), 0444);
+MODULE_PARM_DESC(pow_send_list, "\n"
+	"\tComma separated list of ethernet devices that should use the\n"
+	"\tPOW for transmit instead of the actual ethernet hardware. This\n"
+	"\tis a per port version of always_use_pow. always_use_pow takes\n"
+	"\tprecedence over this list. For example, setting this to\n"
+	"\t\"eth2,spi3,spi7\" would cause these three devices to transmit\n"
+	"\tusing the pow_send_group.");
+
+static int disable_core_queueing = 1;
+module_param(disable_core_queueing, int, 0444);
+MODULE_PARM_DESC(disable_core_queueing, "\n"
+		"\t\tWhen set the networking core's tx_queue_len is set to zero.  This\n"
+		"\t\tallows packets to be sent without lock contention in the packet scheduler\n"
+		"\t\tresulting in some cases in improved throughput.");
+
+int max_rx_cpus = -1;
+module_param(max_rx_cpus, int, 0444);
+MODULE_PARM_DESC(max_rx_cpus, "\n"
+	"\t\tThe maximum number of CPUs to use for packet reception.\n"
+	"\t\tUse -1 to use all available CPUs.");
+
+int rx_napi_weight = 32;
+module_param(rx_napi_weight, int, 0444);
+MODULE_PARM_DESC(rx_napi_weight, "The NAPI WEIGHT parameter.");
+
+/*
+ * The offset from mac_addr_base that should be used for the next port
+ * that is configured.  By convention, if any mgmt ports exist on the
+ * chip, they get the first mac addresses, The ports controlled by
+ * this driver are numbered sequencially following any mgmt addresses
+ * that may exist.
+ */
+static unsigned int cvm_oct_mac_addr_offset;
+
+/**
+ * cvm_oct_poll_queue - Workqueue for polling operations.
+ */
+struct workqueue_struct *cvm_oct_poll_queue;
+
+/**
+ * cvm_oct_poll_queue_stopping - flag to indicate polling should stop.
+ *
+ * Set to one right before cvm_oct_poll_queue is destroyed.
+ */
+atomic_t cvm_oct_poll_queue_stopping = ATOMIC_INIT(0);
+
+/*
+ * cvm_oct_by_port is an array of every ethernet device owned by this driver indexed by
+ * the ipd input port number.
+ */
+struct octeon_ethernet *cvm_oct_by_port[TOTAL_NUMBER_OF_PORTS] __cacheline_aligned;
+
+/*
+ * cvm_oct_by_srio_mbox is indexed by the SRIO mailbox.
+ */
+struct octeon_ethernet *cvm_oct_by_srio_mbox[2][4];
+
+/*
+ * cvm_oct_list is a list of all cvm_oct_private_t created by this driver.
+ */
+LIST_HEAD(cvm_oct_list);
+
+static void cvm_oct_rx_refill_worker(struct work_struct *work);
+static DECLARE_DELAYED_WORK(cvm_oct_rx_refill_work, cvm_oct_rx_refill_worker);
+
+static void cvm_oct_rx_refill_worker(struct work_struct *work)
+{
+	/*
+	 * FPA 0 may have been drained, try to refill it if we need
+	 * more than num_packet_buffers / 2, otherwise normal receive
+	 * processing will refill it.  If it were drained, no packets
+	 * could be received so cvm_oct_napi_poll would never be
+	 * invoked to do the refill.
+	 */
+	cvm_oct_rx_refill_pool(num_packet_buffers / 2);
+
+	if (!atomic_read(&cvm_oct_poll_queue_stopping))
+		queue_delayed_work(cvm_oct_poll_queue,
+				   &cvm_oct_rx_refill_work, HZ);
+}
+
+static void cvm_oct_periodic_worker(struct work_struct *work)
+{
+	struct octeon_ethernet *priv = container_of(work,
+						    struct octeon_ethernet,
+						    port_periodic_work.work);
+
+	if (priv->poll)
+		priv->poll(priv->netdev);
+
+	priv->netdev->netdev_ops->ndo_get_stats(priv->netdev);
+
+	if (!atomic_read(&cvm_oct_poll_queue_stopping))
+		queue_delayed_work(cvm_oct_poll_queue, &priv->port_periodic_work, HZ);
+ }
+
+static int cvm_oct_num_output_buffers;
+
+static __init void cvm_oct_configure_common_hw(void)
+{
+	/* Setup the FPA */
+	cvmx_fpa_enable();
+	cvm_oct_mem_fill_fpa(CVMX_FPA_PACKET_POOL, CVMX_FPA_PACKET_POOL_SIZE,
+			     num_packet_buffers);
+	cvm_oct_mem_fill_fpa(CVMX_FPA_WQE_POOL, CVMX_FPA_WQE_POOL_SIZE,
+			     num_packet_buffers);
+	if (CVMX_FPA_OUTPUT_BUFFER_POOL != CVMX_FPA_PACKET_POOL) {
+		cvm_oct_num_output_buffers = 4 * octeon_pko_get_total_queues();
+		cvm_oct_mem_fill_fpa(CVMX_FPA_OUTPUT_BUFFER_POOL,
+				     CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE, cvm_oct_num_output_buffers);
+	}
+
+	if (USE_RED)
+		cvmx_helper_setup_red(num_packet_buffers / 4,
+				      num_packet_buffers / 8);
+
+}
+
+/**
+ * cvm_oct_register_callback -  Register a intercept callback for the named device.
+ *
+ * It returns the net_device structure for the ethernet port. Usign a
+ * callback of NULL will remove the callback. Note that this callback
+ * must not disturb scratch. It will be called with SYNCIOBDMAs in
+ * progress and userspace may be using scratch. It also must not
+ * disturb the group mask.
+ *
+ * @device_name: Device name to register for. (Example: "eth0")
+ * @callback: Intercept callback to set.
+ *
+ * Returns the net_device structure for the ethernet port or NULL on failure.
+ */
+struct net_device *cvm_oct_register_callback(const char *device_name, cvm_oct_callback_t callback)
+{
+	struct octeon_ethernet *priv;
+
+	list_for_each_entry(priv, &cvm_oct_list, list) {
+		if (strcmp(device_name, priv->netdev->name) == 0) {
+			priv->intercept_cb = callback;
+			wmb();
+			return priv->netdev;
+		}
+	}
+	return NULL;
+}
+EXPORT_SYMBOL(cvm_oct_register_callback);
+
+/**
+ * cvm_oct_free_work- Free a work queue entry
+ *
+ * @work_queue_entry: Work queue entry to free
+ *
+ * Returns Zero on success, Negative on failure.
+ */
+int cvm_oct_free_work(void *work_queue_entry)
+{
+	cvmx_wqe_t *work = work_queue_entry;
+
+	int segments = work->word2.s.bufs;
+	union cvmx_buf_ptr segment_ptr = work->packet_ptr;
+
+	while (segments--) {
+		union cvmx_buf_ptr next_ptr = *(union cvmx_buf_ptr *)cvmx_phys_to_ptr(segment_ptr.s.addr - 8);
+		if (unlikely(!segment_ptr.s.i))
+			cvmx_fpa_free(cvm_oct_get_buffer_ptr(segment_ptr),
+				      segment_ptr.s.pool,
+				      DONT_WRITEBACK(CVMX_FPA_PACKET_POOL_SIZE / 128));
+		segment_ptr = next_ptr;
+	}
+	cvmx_fpa_free(work, CVMX_FPA_WQE_POOL, DONT_WRITEBACK(1));
+
+	return 0;
+}
+EXPORT_SYMBOL(cvm_oct_free_work);
+
+/* Lock to protect racy cvmx_pko_get_port_status() */
+static DEFINE_SPINLOCK(cvm_oct_tx_stat_lock);
+
+/**
+ * cvm_oct_common_get_stats - get the low level ethernet statistics
+ * @dev:    Device to get the statistics from
+ *
+ * Returns Pointer to the statistics
+ */
+static struct net_device_stats *cvm_oct_common_get_stats(struct net_device *dev)
+{
+	unsigned long flags;
+	cvmx_pip_port_status_t rx_status;
+	cvmx_pko_port_status_t tx_status;
+	u64 current_tx_octets;
+	u32 current_tx_packets;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	if (priv->port < CVMX_PIP_NUM_INPUT_PORTS) {
+		if (octeon_is_simulation()) {
+			/* The simulator doesn't support statistics */
+			memset(&rx_status, 0, sizeof(rx_status));
+			memset(&tx_status, 0, sizeof(tx_status));
+		} else {
+			cvmx_pip_get_port_status(priv->port, 1, &rx_status);
+
+			spin_lock_irqsave(&cvm_oct_tx_stat_lock, flags);
+			cvmx_pko_get_port_status(priv->port, 0, &tx_status);
+			current_tx_packets = tx_status.packets;
+			current_tx_octets = tx_status.octets;
+			/*
+			 * The tx_packets counter is 32-bits as are
+			 * all these variables.  No truncation
+			 * necessary.
+			 */
+			tx_status.packets = current_tx_packets - priv->last_tx_packets;
+			/*
+			 * The tx_octets counter is only 48-bits, so
+			 * we need to truncate in case there was a
+			 * wrap-around
+			 */
+			tx_status.octets = (current_tx_octets - priv->last_tx_octets) & 0xffffffffffffull;
+			priv->last_tx_packets = current_tx_packets;
+			priv->last_tx_octets = current_tx_octets;
+			spin_unlock_irqrestore(&cvm_oct_tx_stat_lock, flags);
+		}
+
+		dev->stats.rx_packets += rx_status.inb_packets;
+		dev->stats.tx_packets += tx_status.packets;
+		dev->stats.rx_bytes += rx_status.inb_octets;
+		dev->stats.tx_bytes += tx_status.octets;
+		dev->stats.multicast += rx_status.multicast_packets;
+		dev->stats.rx_crc_errors += rx_status.inb_errors;
+		dev->stats.rx_frame_errors += rx_status.fcs_align_err_packets;
+
+		/*
+		 * The drop counter must be incremented atomically
+		 * since the RX tasklet also increments it.
+		 */
+		atomic64_add(rx_status.dropped_packets,
+			     (atomic64_t *)&dev->stats.rx_dropped);
+	}
+
+	return &dev->stats;
+}
+
+/**
+ * cvm_oct_common_change_mtu - change the link MTU
+ * @dev:     Device to change
+ * @new_mtu: The new MTU
+ *
+ * Returns Zero on success
+ */
+static int cvm_oct_common_change_mtu(struct net_device *dev, int new_mtu)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	int interface = INTERFACE(priv->port);
+	int index = INDEX(priv->port);
+#if defined(CONFIG_VLAN_8021Q) || defined(CONFIG_VLAN_8021Q_MODULE)
+	int vlan_bytes = 4;
+#else
+	int vlan_bytes = 0;
+#endif
+
+	/*
+	 * Limit the MTU to make sure the ethernet packets are between
+	 * 64 bytes and 65535 bytes.
+	 */
+	if ((new_mtu + 14 + 4 + vlan_bytes < 64)
+	    || (new_mtu + 14 + 4 + vlan_bytes > 65392)) {
+		pr_err("MTU must be between %d and %d.\n",
+		       64 - 14 - 4 - vlan_bytes, 65392 - 14 - 4 - vlan_bytes);
+		return -EINVAL;
+	}
+	dev->mtu = new_mtu;
+
+	if ((interface < 2)
+	    && (cvmx_helper_interface_get_mode(interface) !=
+		CVMX_HELPER_INTERFACE_MODE_SPI)) {
+		/* Add ethernet header and FCS, and VLAN if configured. */
+		int max_packet = new_mtu + 14 + 4 + vlan_bytes;
+
+		if (OCTEON_IS_MODEL(OCTEON_CN3XXX)
+		    || OCTEON_IS_MODEL(OCTEON_CN58XX)) {
+			/* Signal errors on packets larger than the MTU */
+			cvmx_write_csr(CVMX_GMXX_RXX_FRM_MAX(index, interface),
+				       max_packet);
+		} else {
+			/*
+			 * Set the hardware to truncate packets larger
+			 * than the MTU and smaller the 64 bytes.
+			 */
+			union cvmx_pip_frm_len_chkx frm_len_chk;
+			frm_len_chk.u64 = 0;
+			frm_len_chk.s.minlen = 64;
+			frm_len_chk.s.maxlen = max_packet;
+			cvmx_write_csr(CVMX_PIP_FRM_LEN_CHKX(interface), frm_len_chk.u64);
+		}
+		/*
+		 * Set the hardware to truncate packets larger than
+		 * the MTU. The jabber register must be set to a
+		 * multiple of 8 bytes, so round up.
+		 */
+		cvmx_write_csr(CVMX_GMXX_RXX_JABBER(index, interface),
+			       (max_packet + 7) & ~7u);
+	}
+	return 0;
+}
+
+/**
+ * cvm_oct_common_set_multicast_list - set the multicast list
+ * @dev:    Device to work on
+ */
+static void cvm_oct_common_set_multicast_list(struct net_device *dev)
+{
+	union cvmx_gmxx_prtx_cfg gmx_cfg;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	int interface = INTERFACE(priv->port);
+	int index = INDEX(priv->port);
+
+	if ((interface < 2)
+	    && (cvmx_helper_interface_get_mode(interface) !=
+		CVMX_HELPER_INTERFACE_MODE_SPI)) {
+		union cvmx_gmxx_rxx_adr_ctl control;
+		control.u64 = 0;
+		control.s.bcst = 1;	/* Allow broadcast MAC addresses */
+
+		if (dev->mc_list || (dev->flags & IFF_ALLMULTI) ||
+		    (dev->flags & IFF_PROMISC))
+			/* Force accept multicast packets */
+			control.s.mcst = 2;
+		else
+			/* Force reject multicat packets */
+			control.s.mcst = 1;
+
+		if (dev->flags & IFF_PROMISC)
+			/*
+			 * Reject matches if promisc. Since CAM is
+			 * shut off, should accept everything.
+			 */
+			control.s.cam_mode = 0;
+		else
+			/* Filter packets based on the CAM */
+			control.s.cam_mode = 1;
+
+		gmx_cfg.u64 =
+		    cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
+		cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface),
+			       gmx_cfg.u64 & ~1ull);
+
+		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CTL(index, interface),
+			       control.u64);
+		if (dev->flags & IFF_PROMISC)
+			cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM_EN
+				       (index, interface), 0);
+		else
+			cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM_EN
+				       (index, interface), 1);
+
+		cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface),
+			       gmx_cfg.u64);
+	}
+}
+
+/**
+ * cvm_oct_common_set_mac_address - set the hardware MAC address for a device
+ * @dev:    The device in question.
+ * @addr:   Address structure to change it too.
+
+ * Returns Zero on success
+ */
+static int cvm_oct_common_set_mac_address(struct net_device *dev, void *addr)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	union cvmx_gmxx_prtx_cfg gmx_cfg;
+	int interface = INTERFACE(priv->port);
+	int index = INDEX(priv->port);
+
+	memcpy(dev->dev_addr, addr + 2, 6);
+
+	if ((interface < 2)
+	    && (cvmx_helper_interface_get_mode(interface) !=
+		CVMX_HELPER_INTERFACE_MODE_SPI)) {
+		int i;
+		uint8_t *ptr = addr;
+		uint64_t mac = 0;
+		for (i = 0; i < 6; i++)
+			mac = (mac << 8) | (uint64_t) (ptr[i + 2]);
+
+		gmx_cfg.u64 =
+		    cvmx_read_csr(CVMX_GMXX_PRTX_CFG(index, interface));
+		cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface),
+			       gmx_cfg.u64 & ~1ull);
+
+		cvmx_write_csr(CVMX_GMXX_SMACX(index, interface), mac);
+		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM0(index, interface), ptr[2]);
+		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM1(index, interface), ptr[3]);
+		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM2(index, interface), ptr[4]);
+		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM3(index, interface), ptr[5]);
+		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM4(index, interface), ptr[6]);
+		cvmx_write_csr(CVMX_GMXX_RXX_ADR_CAM5(index, interface), ptr[7]);
+		cvm_oct_common_set_multicast_list(dev);
+		cvmx_write_csr(CVMX_GMXX_PRTX_CFG(index, interface), gmx_cfg.u64);
+	}
+	return 0;
+}
+
+/**
+ * cvm_oct_common_init - per network device initialization
+ * @dev:    Device to initialize
+ *
+ * Returns Zero on success
+ */
+int cvm_oct_common_init(struct net_device *dev)
+{
+	unsigned long flags;
+	cvmx_pko_port_status_t tx_status;
+	struct octeon_ethernet *priv = netdev_priv(dev);
+	struct sockaddr sa;
+	u64 mac = ((u64)(octeon_bootinfo->mac_addr_base[0] & 0xff) << 40) |
+		((u64)(octeon_bootinfo->mac_addr_base[1] & 0xff) << 32) |
+		((u64)(octeon_bootinfo->mac_addr_base[2] & 0xff) << 24) |
+		((u64)(octeon_bootinfo->mac_addr_base[3] & 0xff) << 16) |
+		((u64)(octeon_bootinfo->mac_addr_base[4] & 0xff) << 8) |
+		(u64)(octeon_bootinfo->mac_addr_base[5] & 0xff);
+
+	mac += cvm_oct_mac_addr_offset;
+	sa.sa_data[0] = (mac >> 40) & 0xff;
+	sa.sa_data[1] = (mac >> 32) & 0xff;
+	sa.sa_data[2] = (mac >> 24) & 0xff;
+	sa.sa_data[3] = (mac >> 16) & 0xff;
+	sa.sa_data[4] = (mac >> 8) & 0xff;
+	sa.sa_data[5] = mac & 0xff;
+
+	if (cvm_oct_mac_addr_offset >= octeon_bootinfo->mac_addr_count)
+		printk(KERN_DEBUG "%s: Using MAC outside of the assigned range:"
+			" %02x:%02x:%02x:%02x:%02x:%02x\n", dev->name,
+			sa.sa_data[0] & 0xff, sa.sa_data[1] & 0xff,
+			sa.sa_data[2] & 0xff, sa.sa_data[3] & 0xff,
+			sa.sa_data[4] & 0xff, sa.sa_data[5] & 0xff);
+	cvm_oct_mac_addr_offset++;
+
+	if (priv->num_tx_queues) {
+		dev->features |= NETIF_F_SG;
+		if (USE_HW_TCPUDP_CHECKSUM)
+			dev->features |= NETIF_F_IP_CSUM;
+	}
+
+	/* We do our own locking, Linux doesn't need to */
+	dev->features |= NETIF_F_LLTX;
+	SET_ETHTOOL_OPS(dev, &cvm_oct_ethtool_ops);
+
+	cvm_oct_phy_setup_device(dev);
+	dev->netdev_ops->ndo_set_mac_address(dev, &sa);
+	dev->netdev_ops->ndo_change_mtu(dev, dev->mtu);
+
+	spin_lock_irqsave(&cvm_oct_tx_stat_lock, flags);
+	cvmx_pko_get_port_status(priv->port, 0, &tx_status);
+	priv->last_tx_packets = tx_status.packets;
+	priv->last_tx_octets = tx_status.octets;
+	/*
+	 * Zero out stats for port so we won't mistakenly show
+	 * counters from the bootloader.
+	 */
+	memset(&dev->stats, 0, sizeof(struct net_device_stats));
+	spin_unlock_irqrestore(&cvm_oct_tx_stat_lock, flags);
+
+	return 0;
+}
+
+void cvm_oct_common_uninit(struct net_device *dev)
+{
+	struct octeon_ethernet *priv = netdev_priv(dev);
+
+	if (priv->phydev)
+		phy_disconnect(priv->phydev);
+}
+
+static const struct net_device_ops cvm_oct_npi_netdev_ops = {
+	.ndo_init		= cvm_oct_common_init,
+	.ndo_uninit		= cvm_oct_common_uninit,
+	.ndo_start_xmit		= cvm_oct_xmit,
+	.ndo_set_multicast_list	= cvm_oct_common_set_multicast_list,
+	.ndo_set_mac_address	= cvm_oct_common_set_mac_address,
+	.ndo_do_ioctl		= cvm_oct_ioctl,
+	.ndo_change_mtu		= cvm_oct_common_change_mtu,
+	.ndo_get_stats		= cvm_oct_common_get_stats,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= cvm_oct_poll_controller,
+#endif
+};
+
+static const struct net_device_ops cvm_oct_xaui_netdev_ops = {
+	.ndo_init		= cvm_oct_xaui_init,
+	.ndo_uninit		= cvm_oct_xaui_uninit,
+	.ndo_open		= cvm_oct_xaui_open,
+	.ndo_stop		= cvm_oct_xaui_stop,
+	.ndo_start_xmit		= cvm_oct_xmit,
+	.ndo_set_multicast_list	= cvm_oct_common_set_multicast_list,
+	.ndo_set_mac_address	= cvm_oct_common_set_mac_address,
+	.ndo_do_ioctl		= cvm_oct_ioctl,
+	.ndo_change_mtu		= cvm_oct_common_change_mtu,
+	.ndo_get_stats		= cvm_oct_common_get_stats,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= cvm_oct_poll_controller,
+#endif
+};
+static const struct net_device_ops cvm_oct_sgmii_netdev_ops = {
+	.ndo_init		= cvm_oct_sgmii_init,
+	.ndo_uninit		= cvm_oct_sgmii_uninit,
+	.ndo_open		= cvm_oct_sgmii_open,
+	.ndo_stop		= cvm_oct_sgmii_stop,
+	.ndo_start_xmit		= cvm_oct_xmit,
+	.ndo_set_multicast_list	= cvm_oct_common_set_multicast_list,
+	.ndo_set_mac_address	= cvm_oct_common_set_mac_address,
+	.ndo_do_ioctl		= cvm_oct_ioctl,
+	.ndo_change_mtu		= cvm_oct_common_change_mtu,
+	.ndo_get_stats		= cvm_oct_common_get_stats,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= cvm_oct_poll_controller,
+#endif
+};
+static const struct net_device_ops cvm_oct_spi_netdev_ops = {
+	.ndo_init		= cvm_oct_spi_init,
+	.ndo_uninit		= cvm_oct_spi_uninit,
+	.ndo_start_xmit		= cvm_oct_xmit,
+	.ndo_set_multicast_list	= cvm_oct_common_set_multicast_list,
+	.ndo_set_mac_address	= cvm_oct_common_set_mac_address,
+	.ndo_do_ioctl		= cvm_oct_ioctl,
+	.ndo_change_mtu		= cvm_oct_common_change_mtu,
+	.ndo_get_stats		= cvm_oct_common_get_stats,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= cvm_oct_poll_controller,
+#endif
+};
+static const struct net_device_ops cvm_oct_rgmii_netdev_ops = {
+	.ndo_init		= cvm_oct_rgmii_init,
+	.ndo_uninit		= cvm_oct_rgmii_uninit,
+	.ndo_open		= cvm_oct_rgmii_open,
+	.ndo_stop		= cvm_oct_rgmii_stop,
+	.ndo_start_xmit		= cvm_oct_xmit,
+	.ndo_set_multicast_list	= cvm_oct_common_set_multicast_list,
+	.ndo_set_mac_address	= cvm_oct_common_set_mac_address,
+	.ndo_do_ioctl		= cvm_oct_ioctl,
+	.ndo_change_mtu		= cvm_oct_common_change_mtu,
+	.ndo_get_stats		= cvm_oct_common_get_stats,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= cvm_oct_poll_controller,
+#endif
+};
+#ifdef CONFIG_OCTEON_ETHERNET_LOCKLESS_IF_SUPPORTED
+static const struct net_device_ops cvm_oct_xaui_lockless_netdev_ops = {
+	.ndo_init		= cvm_oct_xaui_init,
+	.ndo_uninit		= cvm_oct_xaui_uninit,
+	.ndo_open		= cvm_oct_xaui_open,
+	.ndo_stop		= cvm_oct_xaui_stop,
+	.ndo_start_xmit		= cvm_oct_xmit_lockless,
+	.ndo_set_multicast_list	= cvm_oct_common_set_multicast_list,
+	.ndo_set_mac_address	= cvm_oct_common_set_mac_address,
+	.ndo_do_ioctl		= cvm_oct_ioctl,
+	.ndo_change_mtu		= cvm_oct_common_change_mtu,
+	.ndo_get_stats		= cvm_oct_common_get_stats,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= cvm_oct_poll_controller,
+#endif
+};
+static const struct net_device_ops cvm_oct_sgmii_lockless_netdev_ops = {
+	.ndo_init		= cvm_oct_sgmii_init,
+	.ndo_uninit		= cvm_oct_sgmii_uninit,
+	.ndo_open		= cvm_oct_sgmii_open,
+	.ndo_stop		= cvm_oct_sgmii_stop,
+	.ndo_start_xmit		= cvm_oct_xmit_lockless,
+	.ndo_set_multicast_list	= cvm_oct_common_set_multicast_list,
+	.ndo_set_mac_address	= cvm_oct_common_set_mac_address,
+	.ndo_do_ioctl		= cvm_oct_ioctl,
+	.ndo_change_mtu		= cvm_oct_common_change_mtu,
+	.ndo_get_stats		= cvm_oct_common_get_stats,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= cvm_oct_poll_controller,
+#endif
+};
+static const struct net_device_ops cvm_oct_spi_lockless_netdev_ops = {
+	.ndo_init		= cvm_oct_spi_init,
+	.ndo_uninit		= cvm_oct_spi_uninit,
+	.ndo_start_xmit		= cvm_oct_xmit_lockless,
+	.ndo_set_multicast_list	= cvm_oct_common_set_multicast_list,
+	.ndo_set_mac_address	= cvm_oct_common_set_mac_address,
+	.ndo_do_ioctl		= cvm_oct_ioctl,
+	.ndo_change_mtu		= cvm_oct_common_change_mtu,
+	.ndo_get_stats		= cvm_oct_common_get_stats,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= cvm_oct_poll_controller,
+#endif
+};
+static const struct net_device_ops cvm_oct_rgmii_lockless_netdev_ops = {
+	.ndo_init		= cvm_oct_rgmii_init,
+	.ndo_uninit		= cvm_oct_rgmii_uninit,
+	.ndo_open		= cvm_oct_rgmii_open,
+	.ndo_stop		= cvm_oct_rgmii_stop,
+	.ndo_start_xmit		= cvm_oct_xmit_lockless,
+	.ndo_set_multicast_list	= cvm_oct_common_set_multicast_list,
+	.ndo_set_mac_address	= cvm_oct_common_set_mac_address,
+	.ndo_do_ioctl		= cvm_oct_ioctl,
+	.ndo_change_mtu		= cvm_oct_common_change_mtu,
+	.ndo_get_stats		= cvm_oct_common_get_stats,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= cvm_oct_poll_controller,
+#endif
+};
+#endif
+#ifdef CONFIG_RAPIDIO
+static const struct net_device_ops cvm_oct_srio_netdev_ops = {
+	.ndo_init		= cvm_oct_srio_init,
+	.ndo_start_xmit		= cvm_oct_xmit_srio,
+	.ndo_set_mac_address	= cvm_oct_srio_set_mac_address,
+	.ndo_do_ioctl		= cvm_oct_ioctl,
+	.ndo_change_mtu		= cvm_oct_srio_change_mtu,
+	.ndo_get_stats		= cvm_oct_srio_get_stats,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= cvm_oct_poll_controller,
+#endif
+};
+#endif
+static const struct net_device_ops cvm_oct_pow_netdev_ops = {
+	.ndo_init		= cvm_oct_common_init,
+	.ndo_start_xmit		= cvm_oct_xmit_pow,
+	.ndo_set_multicast_list	= cvm_oct_common_set_multicast_list,
+	.ndo_set_mac_address	= cvm_oct_common_set_mac_address,
+	.ndo_do_ioctl		= cvm_oct_ioctl,
+	.ndo_change_mtu		= cvm_oct_common_change_mtu,
+	.ndo_get_stats		= cvm_oct_common_get_stats,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= cvm_oct_poll_controller,
+#endif
+};
+
+extern void octeon_mdiobus_force_mod_depencency(void);
+
+static int num_devices_extra_wqe;
+#define PER_DEVICE_EXTRA_WQE (MAX_OUT_QUEUE_DEPTH)
+
+static void cvm_oct_override_pko_queue_priority(int pko_port, uint64_t priorities[16])
+{
+	int i;
+
+	if (octeon_pko_lockless()) {
+		for (i = 0; i < 16; i++)
+			priorities[i] = 8;
+	}
+}
+
+
+static int __init cvm_oct_init_module(void)
+{
+	int num_interfaces;
+	int interface;
+	int fau = FAU_NUM_PACKET_BUFFERS_TO_FREE;
+	int qos;
+
+	octeon_mdiobus_force_mod_depencency();
+	pr_notice("octeon-ethernet %s\n", OCTEON_ETHERNET_VERSION);
+
+	cvmx_override_pko_queue_priority = cvm_oct_override_pko_queue_priority;
+
+	if (OCTEON_IS_MODEL(OCTEON_CN52XX) || OCTEON_IS_MODEL(OCTEON_CN63XX))
+		cvm_oct_mac_addr_offset = 2; /* First two are the mgmt ports. */
+	else if (OCTEON_IS_MODEL(OCTEON_CN56XX))
+		cvm_oct_mac_addr_offset = 1; /* First one is the mgmt port. */
+	else
+		cvm_oct_mac_addr_offset = 0;
+
+	cvm_oct_poll_queue = create_singlethread_workqueue("octeon-ethernet");
+	if (cvm_oct_poll_queue == NULL) {
+		pr_err("octeon-ethernet: Cannot create workqueue");
+		return -ENOMEM;
+	}
+
+	cvm_oct_proc_initialize();
+	cvm_oct_configure_common_hw();
+
+	cvmx_helper_initialize_packet_io_global();
+
+	/* Change the input group for all ports before input is enabled */
+	num_interfaces = cvmx_helper_get_number_of_interfaces();
+	for (interface = 0; interface < num_interfaces; interface++) {
+		int num_ports = cvmx_helper_ports_on_interface(interface);
+		int port;
+
+		for (port = cvmx_helper_get_ipd_port(interface, 0);
+		     port < cvmx_helper_get_ipd_port(interface, num_ports);
+		     port++) {
+			union cvmx_pip_prt_tagx pip_prt_tagx;
+			pip_prt_tagx.u64 = cvmx_read_csr(CVMX_PIP_PRT_TAGX(port));
+			pip_prt_tagx.s.grp = pow_receive_group;
+			cvmx_write_csr(CVMX_PIP_PRT_TAGX(port), pip_prt_tagx.u64);
+		}
+	}
+
+	cvmx_helper_ipd_and_packet_input_enable();
+
+	/*
+	 * Initialize the FAU used for counting packet buffers that
+	 * need to be freed.
+	 */
+	cvmx_fau_atomic_write32(FAU_NUM_PACKET_BUFFERS_TO_FREE, 0);
+
+	if ((pow_send_group != -1)) {
+		struct net_device *dev;
+		pr_info("\tConfiguring device for POW only access\n");
+		dev = alloc_etherdev(sizeof(struct octeon_ethernet));
+		if (dev) {
+			/* Initialize the device private structure. */
+			struct octeon_ethernet *priv = netdev_priv(dev);
+
+			priv->netdev = dev;
+			dev->netdev_ops = &cvm_oct_pow_netdev_ops;
+			priv->num_tx_queues = 0;
+			priv->imode = CVMX_HELPER_INTERFACE_MODE_DISABLED;
+			priv->port = CVMX_PIP_NUM_INPUT_PORTS;
+			strcpy(dev->name, "pow%d");
+
+			if (register_netdev(dev) < 0) {
+				pr_err("Failed to register ethernet device for POW\n");
+				kfree(dev);
+			} else {
+				list_add_tail(&priv->list, &cvm_oct_list);
+				cvm_oct_by_port[CVMX_PIP_NUM_INPUT_PORTS] = priv;
+				pr_info("%s: POW send group %d, receive group %d\n",
+					dev->name, pow_send_group,
+					pow_receive_group);
+			}
+		} else {
+			pr_err("Failed to allocate ethernet device for POW\n");
+		}
+	}
+
+	for (interface = 0; interface < num_interfaces; interface++) {
+		cvmx_helper_interface_mode_t imode = cvmx_helper_interface_get_mode(interface);
+		int num_ports = cvmx_helper_ports_on_interface(interface);
+		int port;
+
+		if (imode == CVMX_HELPER_INTERFACE_MODE_SRIO)
+			num_ports = 4;
+
+		for (port = cvmx_helper_get_ipd_port(interface, 0);
+		     port < cvmx_helper_get_ipd_port(interface, num_ports);
+		     port++) {
+			struct octeon_ethernet *priv;
+			int base_queue;
+			struct net_device *dev = alloc_etherdev(sizeof(struct octeon_ethernet));
+			if (!dev) {
+				pr_err("Failed to allocate ethernet device for port %d\n", port);
+				continue;
+			}
+
+			if (disable_core_queueing)
+				dev->tx_queue_len = 0;
+
+			/* Initialize the device private structure. */
+			priv = netdev_priv(dev);
+			priv->netdev = dev;
+
+			INIT_DELAYED_WORK(&priv->port_periodic_work,
+					  cvm_oct_periodic_worker);
+			priv->imode = imode;
+
+			if (imode == CVMX_HELPER_INTERFACE_MODE_SRIO) {
+				cvmx_srio_tx_message_header_t tx_header;
+				tx_header.u64 = 0;
+				tx_header.s.tt = 1;
+				tx_header.s.ssize = 0xe;
+				tx_header.s.mbox = port;
+				tx_header.s.lns = 1;
+				priv->srio_tx_header = tx_header.u64;
+				priv->port = cvmx_helper_get_ipd_port(interface, port >> 1);
+				base_queue = cvmx_pko_get_base_queue(priv->port) + (port & 1);
+				priv->num_tx_queues = 1;
+				cvm_oct_by_srio_mbox[interface - 4][port] = priv;
+			} else {
+				priv->port = port;
+				base_queue = cvmx_pko_get_base_queue(priv->port);
+				priv->num_tx_queues = cvmx_pko_get_num_queues(priv->port);
+			}
+
+			for (qos = 0; qos < priv->num_tx_queues; qos++) {
+				priv->tx_queue[qos].queue = base_queue + qos;
+				fau = fau - sizeof(uint32_t);
+				priv->tx_queue[qos].fau = fau;
+				cvmx_fau_atomic_write32(priv->tx_queue[qos].fau, 0);
+			}
+
+			switch (priv->imode) {
+
+			/* These types don't support ports to IPD/PKO */
+			case CVMX_HELPER_INTERFACE_MODE_DISABLED:
+			case CVMX_HELPER_INTERFACE_MODE_PCIE:
+			case CVMX_HELPER_INTERFACE_MODE_PICMG:
+				break;
+
+			case CVMX_HELPER_INTERFACE_MODE_NPI:
+				dev->netdev_ops = &cvm_oct_npi_netdev_ops;
+				strcpy(dev->name, "npi%d");
+				break;
+
+			case CVMX_HELPER_INTERFACE_MODE_XAUI:
+#ifdef CONFIG_OCTEON_ETHERNET_LOCKLESS_IF_SUPPORTED
+				if (octeon_pko_lockless()) {
+					dev->netdev_ops = &cvm_oct_xaui_lockless_netdev_ops;
+					priv->flags |= OCTEON_ETHERNET_FLAG_TX_LOCKLESS;
+				} else
+#endif
+					dev->netdev_ops = &cvm_oct_xaui_netdev_ops;
+				strcpy(dev->name, "xaui%d");
+				break;
+
+			case CVMX_HELPER_INTERFACE_MODE_LOOP:
+				dev->netdev_ops = &cvm_oct_npi_netdev_ops;
+				strcpy(dev->name, "loop%d");
+				break;
+
+			case CVMX_HELPER_INTERFACE_MODE_SGMII:
+#ifdef CONFIG_OCTEON_ETHERNET_LOCKLESS_IF_SUPPORTED
+				if (octeon_pko_lockless()) {
+					dev->netdev_ops = &cvm_oct_sgmii_lockless_netdev_ops;
+					priv->flags |= OCTEON_ETHERNET_FLAG_TX_LOCKLESS;
+				} else
+#endif
+					dev->netdev_ops = &cvm_oct_sgmii_netdev_ops;
+				strcpy(dev->name, "eth%d");
+				break;
+
+			case CVMX_HELPER_INTERFACE_MODE_SPI:
+#ifdef CONFIG_OCTEON_ETHERNET_LOCKLESS_IF_SUPPORTED
+				if (octeon_pko_lockless()) {
+					dev->netdev_ops = &cvm_oct_spi_lockless_netdev_ops;
+					priv->flags |= OCTEON_ETHERNET_FLAG_TX_LOCKLESS;
+				} else
+#endif
+					dev->netdev_ops = &cvm_oct_spi_netdev_ops;
+				strcpy(dev->name, "spi%d");
+				break;
+
+			case CVMX_HELPER_INTERFACE_MODE_RGMII:
+			case CVMX_HELPER_INTERFACE_MODE_GMII:
+#ifdef CONFIG_OCTEON_ETHERNET_LOCKLESS_IF_SUPPORTED
+				if (octeon_pko_lockless()) {
+					dev->netdev_ops = &cvm_oct_rgmii_lockless_netdev_ops;
+					priv->flags |= OCTEON_ETHERNET_FLAG_TX_LOCKLESS;
+				} else
+#endif
+					dev->netdev_ops = &cvm_oct_rgmii_netdev_ops;
+				strcpy(dev->name, "eth%d");
+				break;
+#ifdef CONFIG_RAPIDIO
+			case CVMX_HELPER_INTERFACE_MODE_SRIO:
+				dev->netdev_ops = &cvm_oct_srio_netdev_ops;
+				strcpy(dev->name, "rio%d");
+				break;
+#endif
+			}
+
+			if (!dev->netdev_ops) {
+				kfree(dev);
+			} else if (register_netdev(dev) < 0) {
+				pr_err("Failed to register ethernet device for interface %d, port %d\n",
+					 interface, priv->port);
+				kfree(dev);
+			} else {
+				list_add_tail(&priv->list, &cvm_oct_list);
+				cvm_oct_by_port[priv->port] = priv;
+				/*
+				 * Each transmit queue will need its
+				 * own MAX_OUT_QUEUE_DEPTH worth of
+				 * WQE to track the transmit skbs.
+				 */
+				cvm_oct_mem_fill_fpa(CVMX_FPA_TX_WQE_POOL, CVMX_FPA_TX_WQE_POOL_SIZE, PER_DEVICE_EXTRA_WQE);
+				num_devices_extra_wqe++;
+
+				queue_delayed_work(cvm_oct_poll_queue, &priv->port_periodic_work, HZ);
+			}
+		}
+	}
+
+	cvm_oct_rx_initialize();
+
+	queue_delayed_work(cvm_oct_poll_queue, &cvm_oct_rx_refill_work, HZ);
+
+	return 0;
+}
+
+static void __exit cvm_oct_cleanup_module(void)
+{
+	struct octeon_ethernet *priv;
+	struct octeon_ethernet *tmp;
+
+	/* Disable POW interrupt */
+	cvmx_write_csr(CVMX_POW_WQ_INT_THRX(pow_receive_group), 0);
+
+	cvmx_ipd_disable();
+
+	atomic_inc_return(&cvm_oct_poll_queue_stopping);
+	cancel_delayed_work_sync(&cvm_oct_rx_refill_work);
+
+	cvm_oct_rx_shutdown();
+
+	/* Free the ethernet devices */
+	list_for_each_entry_safe_reverse(priv, tmp, &cvm_oct_list, list) {
+		list_del(&priv->list);
+		cancel_delayed_work_sync(&priv->port_periodic_work);
+		unregister_netdev(priv->netdev);
+		cvm_oct_by_port[priv->port] = NULL;
+		kfree(priv->netdev);
+	}
+
+	destroy_workqueue(cvm_oct_poll_queue);
+
+	cvm_oct_proc_shutdown();
+
+	cvmx_helper_shutdown_packet_io_global();
+
+	/* Free the HW pools */
+	cvm_oct_mem_empty_fpa(CVMX_FPA_PACKET_POOL, CVMX_FPA_PACKET_POOL_SIZE, num_packet_buffers);
+	cvm_oct_mem_empty_fpa(CVMX_FPA_WQE_POOL, CVMX_FPA_WQE_POOL_SIZE, num_packet_buffers);
+	cvm_oct_mem_empty_fpa(CVMX_FPA_TX_WQE_POOL, CVMX_FPA_TX_WQE_POOL_SIZE, num_devices_extra_wqe * PER_DEVICE_EXTRA_WQE);
+
+	if (CVMX_FPA_OUTPUT_BUFFER_POOL != CVMX_FPA_PACKET_POOL)
+		cvm_oct_mem_empty_fpa(CVMX_FPA_OUTPUT_BUFFER_POOL, CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE, cvm_oct_num_output_buffers);
+}
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Cavium Networks <support@caviumnetworks.com>");
+MODULE_DESCRIPTION("Cavium Networks Octeon ethernet driver.");
+module_init(cvm_oct_init_module);
+module_exit(cvm_oct_cleanup_module);
diff --git a/drivers/net/octeon/octeon-ethernet.h b/drivers/net/octeon/octeon-ethernet.h
new file mode 100644
index 0000000..4dfa0fb
--- /dev/null
+++ b/drivers/net/octeon/octeon-ethernet.h
@@ -0,0 +1,137 @@
+/**********************************************************************
+ * Author: Cavium Networks
+ *
+ * Contact: support@caviumnetworks.com
+ * This file is part of the OCTEON SDK
+ *
+ * Copyright (c) 2003-2010 Cavium Networks
+ *
+ * This file is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, Version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This file is distributed in the hope that it will be useful, but
+ * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or
+ * NONINFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this file; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
+ * or visit http://www.gnu.org/licenses/.
+ *
+ * This file may also be available under a different license from Cavium.
+ * Contact Cavium Networks for more information
+**********************************************************************/
+
+/*
+ * External interface for the Cavium Octeon ethernet driver.
+ */
+#ifndef OCTEON_ETHERNET_H
+#define OCTEON_ETHERNET_H
+
+#include <asm/octeon/cvmx-helper.h>
+
+#include <asm/octeon/octeon-ethernet-user.h>
+
+/**
+ * This is the definition of the Ethernet driver's private
+ * driver state stored in netdev_priv(dev).
+ */
+struct octeon_ethernet {
+	/* PKO hardware output port */
+	int port;
+	/* My netdev. */
+	struct net_device *netdev;
+	/* My location in the cvm_oct_list */
+	struct list_head list;
+	/*
+	 * Type of port. This is one of the enums in
+	 * cvmx_helper_interface_mode_t
+	 */
+	int imode;
+
+	/* Flags controlling aspects of the device */
+	int flags;
+#define OCTEON_ETHERNET_FLAG_TX_TIMESTAMP_SW (1<<0)
+#define OCTEON_ETHERNET_FLAG_TX_TIMESTAMP_HW (1<<1)
+#define OCTEON_ETHERNET_FLAG_RX_TIMESTAMP_SW (1<<2)
+#define OCTEON_ETHERNET_FLAG_RX_TIMESTAMP_HW (1<<3)
+#define OCTEON_ETHERNET_FLAG_TX_LOCKLESS     (1<<4)
+
+	/* Optional intecept callback defined above */
+	cvm_oct_callback_t      intercept_cb;
+
+	/* Number of elements in tx_queue below */
+	int                     num_tx_queues;
+
+	/* SRIO ports add this header for the SRIO block */
+	uint64_t                srio_tx_header;
+
+	struct {
+		/* PKO hardware queue for the port */
+		int	queue;
+		/* Hardware fetch and add to count outstanding tx buffers */
+		int	fau;
+	} tx_queue[16];
+
+	struct phy_device *phydev;
+	/* MII bus the PHY is connected to */
+	struct mii_bus *mii_bus;
+	unsigned int last_link;
+	/* Last negotiated link state */
+	uint64_t link_info;
+	/* Called periodically to check link status */
+	void (*poll) (struct net_device *dev);
+	struct delayed_work	port_periodic_work;
+	struct work_struct	port_work;	/* may be unused. */
+	u64 last_tx_octets;
+	u32 last_tx_packets;
+};
+
+int cvm_oct_free_work(void *work_queue_entry);
+
+extern int cvm_oct_rgmii_init(struct net_device *dev);
+extern void cvm_oct_rgmii_uninit(struct net_device *dev);
+extern int cvm_oct_rgmii_open(struct net_device *dev);
+extern int cvm_oct_rgmii_stop(struct net_device *dev);
+
+extern int cvm_oct_sgmii_init(struct net_device *dev);
+extern void cvm_oct_sgmii_uninit(struct net_device *dev);
+extern int cvm_oct_sgmii_open(struct net_device *dev);
+extern int cvm_oct_sgmii_stop(struct net_device *dev);
+
+extern int cvm_oct_spi_init(struct net_device *dev);
+extern void cvm_oct_spi_uninit(struct net_device *dev);
+extern int cvm_oct_xaui_init(struct net_device *dev);
+extern void cvm_oct_xaui_uninit(struct net_device *dev);
+extern int cvm_oct_xaui_open(struct net_device *dev);
+extern int cvm_oct_xaui_stop(struct net_device *dev);
+
+extern int cvm_oct_srio_init(struct net_device *dev);
+extern int cvm_oct_xmit_srio(struct sk_buff *skb, struct net_device *dev);
+extern int cvm_oct_srio_set_mac_address(struct net_device *dev, void *addr);
+extern int cvm_oct_srio_change_mtu(struct net_device *dev, int new_mtu);
+extern struct net_device_stats *cvm_oct_srio_get_stats(struct net_device *dev);
+
+extern int cvm_oct_common_init(struct net_device *dev);
+extern void cvm_oct_common_uninit(struct net_device *dev);
+
+extern void cvm_oct_set_carrier(struct octeon_ethernet *priv, cvmx_helper_link_info_t link_info);
+
+extern int always_use_pow;
+extern int pow_send_group;
+extern int pow_receive_group;
+extern char pow_send_list[];
+extern struct list_head cvm_oct_list;
+extern struct octeon_ethernet *cvm_oct_by_port[];
+extern struct octeon_ethernet *cvm_oct_by_srio_mbox[2][4];
+
+extern struct workqueue_struct *cvm_oct_poll_queue;
+extern atomic_t cvm_oct_poll_queue_stopping;
+
+extern int max_rx_cpus;
+extern int rx_napi_weight;
+
+#endif
-- 
1.6.5.2

