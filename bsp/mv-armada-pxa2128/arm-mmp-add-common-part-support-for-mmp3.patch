From 9b8db3c805338490f71719521c51813aa7e16827 Mon Sep 17 00:00:00 2001
From: Zumeng Chen <zumeng.chen@windriver.com>
Date: Wed, 4 Sep 2013 13:56:07 +0800
Subject: [PATCH 04/60] arm: mmp: add common part support for mmp3

Original patches come from M5.3.12.5.4_sources, which has totally
more than 6000 thousand patches including many boards with MMPs.

Signed-off-by: Zumeng Chen <zumeng.chen@windriver.com>
---
 arch/arm/Kconfig                           |   90 +++++++++++++++++-
 arch/arm/common/Kconfig                    |    2 +-
 arch/arm/include/asm/assembler.h           |    9 ++
 arch/arm/include/asm/atomic.h              |  125 ++++++++++++++++++++++---
 arch/arm/include/asm/barrier.h             |    6 +
 arch/arm/include/asm/futex.h               |   64 +++++++++++++
 arch/arm/include/asm/hardware/cache-l2x0.h |    4 +
 arch/arm/include/asm/locks.h               |  137 ++++++++++++++++++++++++++++
 arch/arm/include/asm/pj4b-errata-6011.h    |   68 ++++++++++++++
 arch/arm/include/asm/setup.h               |   12 +++
 arch/arm/include/asm/spinlock.h            |   99 +++++++++++++++++++-
 arch/arm/kernel/entry-armv.S               |   42 ++++++++-
 arch/arm/kernel/iwmmxt.S                   |    2 +-
 arch/arm/kernel/swp_emulate.c              |   35 +++++++
 arch/arm/lib/bitops.h                      |   11 ++
 arch/arm/mm/Kconfig                        |  103 +++++++++++++++++++++
 arch/arm/mm/cache-l2x0.c                   |   49 ++++++++++
 arch/arm/mm/cache-v7.S                     |   19 ++++
 arch/arm/mm/proc-v7.S                      |   87 +++++++++++++++++-
 include/linux/spi/pxa2xx_spi.h             |    2 +-
 20 files changed, 937 insertions(+), 29 deletions(-)
 create mode 100644 arch/arm/include/asm/pj4b-errata-6011.h

diff --git a/arch/arm/Kconfig b/arch/arm/Kconfig
index 6b7931a..660b372 100644
--- a/arch/arm/Kconfig
+++ b/arch/arm/Kconfig
@@ -643,18 +643,19 @@ config ARCH_ORION5X
 	  Orion-2 (5281), Orion-1-90 (6183).
 
 config ARCH_MMP
-	bool "Marvell PXA168/910/MMP2"
+	bool "Marvell PXA168/910/MMP2/MMP3"
 	depends on MMU
 	select ARCH_REQUIRE_GPIOLIB
+	select HAVE_SMP
 	select CLKDEV_LOOKUP
 	select GENERIC_CLOCKEVENTS
-	select GPIO_PXA
 	select TICK_ONESHOT
 	select PLAT_PXA
-	select SPARSE_IRQ
+	select ARCH_HAS_CPUFREQ
 	select GENERIC_ALLOCATOR
+	select NEED_MACH_MEMORY_H
 	help
-	  Support for Marvell's PXA168/PXA910(MMP) and MMP2 processor line.
+	  Support for Marvell's PXA168/PXA910(MMP), MMP2 and MMP3 processor line.
 
 config ARCH_KS8695
 	bool "Micrel/Kendin KS8695"
@@ -1437,6 +1438,78 @@ config ARM_ERRATA_775420
 	 to deadlock. This workaround puts DSB before executing ISB if
 	 an abort may occur on cache maintenance.
 
+config PJ4B_ERRATA_5807
+	bool "PJ4B errata 5807"
+	depends on CPU_PJ4B && SMP
+	help
+	  CP15 loses TLB invalidate request, when it received request before
+	  previous request completed. This option uses invalidate TLB by ASID
+	  to replace invalidate TLB entry by MVA. (lightweight version)
+
+config PJ4B_ERRATA_6026
+	bool "PJ4B errata 6026"
+	depends on CPU_PJ4B
+	help
+	  This option replaces all broadcast clean operations with broadcast
+	  clean and invalidate operations.
+
+config PJ4B_ERRATA_6315
+	bool "PJ4B errata 6315"
+	depends on CPU_PJ4B
+	help
+	  Replace DMB w. DSB when its necessary to enforce an ordering between an
+	  explicit load/store op and a set/way op.
+
+config PJ4B_ERRATA_6133
+	bool "PJ4B errata 6133"
+	depends on CPU_PJ4B
+	help
+	  This option uses TLB invalidate all instead of a loop of TLB invalidate by MVA operations.
+
+config PJ4B_ERRATA_6011
+	bool "PJ4B errata 6011"
+	depends on CPU_PJ4B
+	help
+	  Invalidating snoop between LDREX/STREX pair causes STREX to incorrectly succeed
+	  without updated store data. For cacheable, coherent memory, disable interrupt and
+	  replace every occurrence of an LDREX with 5 equivalent LDREX instructions.
+
+config PJ4B_ERRATA_6075
+	bool "PJ4B errata 6075"
+	depends on CPU_PJ4B
+	select ARCH_HAS_BARRIERS
+	help
+	  Barriers fail to enforce ordering among pended loads to non-cacheable
+	  memory. This option replaces a DMB with a DSB to avoid this bug.
+
+config PJ4B_ERRATA_6107
+	bool "PJ4B errata 6107"
+	depends on CPU_PJ4B
+	help
+	  Matching L0 linefill can cause SFB entry to be marked for delayed clearing when it
+	  isn't necessary. This option disables L0 to avoid the bug.
+
+config PJ4B_ERRATA_6359
+	bool "PJ4B errata 6359"
+	depends on CPU_PJ4B && !PJ4B_ERRATA_6359_LIGHTWEIGHT
+	help
+	  Boradcast DMB operations do not fully synchronize local cache hits. This option
+	  replaces all occurences of DMB with DSB.
+
+config PJ4B_ERRATA_6359_LIGHTWEIGHT
+	bool "PJ4B errata 6359 lightweight"
+	depends on CPU_PJ4B
+	help
+	  Boradcast DMB operations do not fully synchronize local cache hits. This option
+	  add 8 NOP after DMB.
+
+config PJ4B_ERRATA_6409
+	bool "PJ4B errata 6409"
+	depends on CPU_PJ4B
+	help
+	  IFU predicts-taken a branch, but tells ROB not-taken. This option disable static
+	  branch prediction
+
 endmenu
 
 source "arch/arm/common/Kconfig"
@@ -1665,6 +1738,7 @@ config HZ
 	default OMAP_32K_TIMER_HZ if ARCH_OMAP && OMAP_32K_TIMER
 	default AT91_TIMER_HZ if ARCH_AT91
 	default SHMOBILE_TIMER_HZ if ARCH_SHMOBILE
+	default 128 if (ARCH_PXA || ARCH_MMP) && PXA_32KTIMER
 	default 100
 
 config THUMB2_KERNEL
@@ -1801,6 +1875,7 @@ config FORCE_MAX_ZONEORDER
 	range 11 64 if ARCH_SHMOBILE
 	default "9" if SA1111
 	default "14" if ARCH_MX6Q
+	default "16" if (ARCH_MMP || ARCH_PXA)
 	default "11"
 	help
 	  The kernel memory allocator divides physically contiguous memory
@@ -2225,6 +2300,13 @@ config CPU_FREQ_S3C24XX_IODEBUG
 	help
 	  Enable s3c_freq_iodbg for the Samsung S3C CPUfreq core
 
+config CPU_FREQ_MMP3
+	bool "MMP3 CPUFREQ support"
+	depends on CPU_FREQ && CPU_MMP3
+	default y
+	help
+	  Say 'Y' here if you want CPUFREQ on MMP3 processor
+
 config CPU_FREQ_S3C24XX_DEBUGFS
 	bool "Export debugfs for CPUFreq"
 	depends on CPU_FREQ_S3C24XX && DEBUG_FS
diff --git a/arch/arm/common/Kconfig b/arch/arm/common/Kconfig
index 283fa1d..213ab17 100644
--- a/arch/arm/common/Kconfig
+++ b/arch/arm/common/Kconfig
@@ -1,6 +1,6 @@
 config ARM_GIC
 	select IRQ_DOMAIN
-	select MULTI_IRQ_HANDLER
+	select MULTI_IRQ_HANDLER if !ARCH_MMP
 	bool
 
 config GIC_NON_BANKED
diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 5c8b3bf4..2311e67 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -211,7 +211,16 @@
 #ifdef CONFIG_SMP
 #if __LINUX_ARM_ARCH__ >= 7
 	.ifeqs "\mode","arm"
+#if defined(CONFIG_SHEEVA_ERRATA_ARM_CPU_6075) || defined(CONFIG_PJ4B_ERRATA_6359)
+	ALT_SMP(dsb)
+#elif defined(CONFIG_PJ4B_ERRATA_6359_LIGHTWEIGHT)
 	ALT_SMP(dmb)
+	.rept	8
+	ALT_SMP(nop)
+	.endr
+#else
+	ALT_SMP(dmb)
+#endif
 	.else
 	ALT_SMP(W(dmb))
 	.endif
diff --git a/arch/arm/include/asm/atomic.h b/arch/arm/include/asm/atomic.h
index 68374ba..cb156dc 100644
--- a/arch/arm/include/asm/atomic.h
+++ b/arch/arm/include/asm/atomic.h
@@ -17,6 +17,10 @@
 #include <asm/barrier.h>
 #include <asm/cmpxchg.h>
 
+#ifdef CONFIG_PJ4B_ERRATA_6011
+#include <asm/pj4b-errata-6011.h>
+#endif
+
 #define ATOMIC_INIT(i)	{ (i) }
 
 #ifdef __KERNEL__
@@ -42,7 +46,12 @@ static inline void atomic_add(int i, atomic_t *v)
 	int result;
 
 	__asm__ __volatile__("@ atomic_add\n"
-"1:	ldrex	%0, [%3]\n"
+"1:		\n"
+#ifdef CONFIG_PJ4B_ERRATA_6011
+	pj4b_6011_ldrex(%0, %3, %1)
+#else
+"	ldrex	%0, [%3]\n"
+#endif
 "	add	%0, %0, %4\n"
 "	strex	%1, %0, [%3]\n"
 "	teq	%1, #0\n"
@@ -60,7 +69,12 @@ static inline int atomic_add_return(int i, atomic_t *v)
 	smp_mb();
 
 	__asm__ __volatile__("@ atomic_add_return\n"
-"1:	ldrex	%0, [%3]\n"
+"1:		\n"
+#ifdef CONFIG_PJ4B_ERRATA_6011
+	pj4b_6011_ldrex(%0, %3, %1)
+#else
+"	ldrex	%0, [%3]\n"
+#endif
 "	add	%0, %0, %4\n"
 "	strex	%1, %0, [%3]\n"
 "	teq	%1, #0\n"
@@ -80,7 +94,12 @@ static inline void atomic_sub(int i, atomic_t *v)
 	int result;
 
 	__asm__ __volatile__("@ atomic_sub\n"
-"1:	ldrex	%0, [%3]\n"
+"1:		\n"
+#ifdef CONFIG_PJ4B_ERRATA_6011
+	pj4b_6011_ldrex(%0, %3, %1)
+#else
+"	ldrex	%0, [%3]\n"
+#endif
 "	sub	%0, %0, %4\n"
 "	strex	%1, %0, [%3]\n"
 "	teq	%1, #0\n"
@@ -98,7 +117,12 @@ static inline int atomic_sub_return(int i, atomic_t *v)
 	smp_mb();
 
 	__asm__ __volatile__("@ atomic_sub_return\n"
-"1:	ldrex	%0, [%3]\n"
+"1:		\n"
+#ifdef CONFIG_PJ4B_ERRATA_6011
+	pj4b_6011_ldrex(%0, %3, %1)
+#else
+"	ldrex	%0, [%3]\n"
+#endif
 "	sub	%0, %0, %4\n"
 "	strex	%1, %0, [%3]\n"
 "	teq	%1, #0\n"
@@ -120,7 +144,11 @@ static inline int atomic_cmpxchg(atomic_t *ptr, int old, int new)
 
 	do {
 		__asm__ __volatile__("@ atomic_cmpxchg\n"
+#ifdef CONFIG_PJ4B_ERRATA_6011
+		pj4b_6011_ldrex(%1, %3, %0)
+#else
 		"ldrex	%1, [%3]\n"
+#endif
 		"mov	%0, #0\n"
 		"teq	%1, %4\n"
 		"strexeq %0, %5, [%3]\n"
@@ -139,7 +167,12 @@ static inline void atomic_clear_mask(unsigned long mask, unsigned long *addr)
 	unsigned long tmp, tmp2;
 
 	__asm__ __volatile__("@ atomic_clear_mask\n"
-"1:	ldrex	%0, [%3]\n"
+"1:     \n"
+#ifdef CONFIG_PJ4B_ERRATA_6011
+	pj4b_6011_ldrex(%0, %3, %1)
+#else
+"	ldrex   %0, [%3]\n"
+#endif
 "	bic	%0, %0, %4\n"
 "	strex	%1, %0, [%3]\n"
 "	teq	%1, #0\n"
@@ -245,19 +278,43 @@ typedef struct {
 
 static inline u64 atomic64_read(atomic64_t *v)
 {
+#ifdef CONFIG_PJ4B_ERRATA_6011
 	u64 result;
+	unsigned long tmp;
 
 	__asm__ __volatile__("@ atomic64_read\n"
-"	ldrexd	%0, %H0, [%1]"
-	: "=&r" (result)
+	pj4b_6011_ldrexd(%0, %H0, %2, %1)
+	: "=&r" (result), "=&r" (tmp)
 	: "r" (&v->counter), "Qo" (v->counter)
 	);
+#else
+	u64 result;
 
+	__asm__ __volatile__("@ atomic64_read\n"
+"	ldrexd  %0, %H0, [%1]"
+	: "=&r" (result)
+	: "r" (&v->counter), "Qo" (v->counter)
+	);
+#endif
 	return result;
 }
 
 static inline void atomic64_set(atomic64_t *v, u64 i)
 {
+#ifdef CONFIG_PJ4B_ERRATA_6011
+	u64 tmp;
+	unsigned long tmp1;
+
+	__asm__ __volatile__("@ atomic64_set\n"
+"1:	\n"
+	pj4b_6011_ldrexd(%0, %H0, %3, %1)
+"	strexd	%0, %4, %H4, [%3]\n"
+"	teq	%0, #0\n"
+"	bne	1b"
+	: "=&r" (tmp), "=&r" (tmp1), "=Qo" (v->counter)
+	: "r" (&v->counter), "r" (i)
+	: "cc");
+#else
 	u64 tmp;
 
 	__asm__ __volatile__("@ atomic64_set\n"
@@ -268,6 +325,7 @@ static inline void atomic64_set(atomic64_t *v, u64 i)
 	: "=&r" (tmp), "=Qo" (v->counter)
 	: "r" (&v->counter), "r" (i)
 	: "cc");
+#endif
 }
 
 static inline void atomic64_add(u64 i, atomic64_t *v)
@@ -276,7 +334,12 @@ static inline void atomic64_add(u64 i, atomic64_t *v)
 	unsigned long tmp;
 
 	__asm__ __volatile__("@ atomic64_add\n"
-"1:	ldrexd	%0, %H0, [%3]\n"
+"1:	\n"
+#ifdef CONFIG_PJ4B_ERRATA_6011
+	pj4b_6011_ldrexd(%0, %H0, %3, %1)
+#else
+"	ldrexd	%0, %H0, [%3]\n"
+#endif
 "	adds	%0, %0, %4\n"
 "	adc	%H0, %H0, %H4\n"
 "	strexd	%1, %0, %H0, [%3]\n"
@@ -295,7 +358,12 @@ static inline u64 atomic64_add_return(u64 i, atomic64_t *v)
 	smp_mb();
 
 	__asm__ __volatile__("@ atomic64_add_return\n"
-"1:	ldrexd	%0, %H0, [%3]\n"
+"1:     \n"
+#ifdef CONFIG_PJ4B_ERRATA_6011
+        pj4b_6011_ldrexd(%0, %H0, %3, %1)
+#else
+"	ldrexd	%0, %H0, [%3]\n"
+#endif
 "	adds	%0, %0, %4\n"
 "	adc	%H0, %H0, %H4\n"
 "	strexd	%1, %0, %H0, [%3]\n"
@@ -316,7 +384,12 @@ static inline void atomic64_sub(u64 i, atomic64_t *v)
 	unsigned long tmp;
 
 	__asm__ __volatile__("@ atomic64_sub\n"
-"1:	ldrexd	%0, %H0, [%3]\n"
+"1:     \n"
+#ifdef CONFIG_PJ4B_ERRATA_6011
+        pj4b_6011_ldrexd(%0, %H0, %3, %1)
+#else
+"	ldrexd	%0, %H0, [%3]\n"
+#endif
 "	subs	%0, %0, %4\n"
 "	sbc	%H0, %H0, %H4\n"
 "	strexd	%1, %0, %H0, [%3]\n"
@@ -335,7 +408,12 @@ static inline u64 atomic64_sub_return(u64 i, atomic64_t *v)
 	smp_mb();
 
 	__asm__ __volatile__("@ atomic64_sub_return\n"
-"1:	ldrexd	%0, %H0, [%3]\n"
+"1:     \n"
+#ifdef CONFIG_PJ4B_ERRATA_6011
+        pj4b_6011_ldrexd(%0, %H0, %3, %1)
+#else
+"	ldrexd	%0, %H0, [%3]\n"
+#endif
 "	subs	%0, %0, %4\n"
 "	sbc	%H0, %H0, %H4\n"
 "	strexd	%1, %0, %H0, [%3]\n"
@@ -359,7 +437,11 @@ static inline u64 atomic64_cmpxchg(atomic64_t *ptr, u64 old, u64 new)
 
 	do {
 		__asm__ __volatile__("@ atomic64_cmpxchg\n"
+#ifdef CONFIG_PJ4B_ERRATA_6011
+		pj4b_6011_ldrexd(%1, %H1, %3, %0)
+#else
 		"ldrexd		%1, %H1, [%3]\n"
+#endif
 		"mov		%0, #0\n"
 		"teq		%1, %4\n"
 		"teqeq		%H1, %H4\n"
@@ -382,7 +464,12 @@ static inline u64 atomic64_xchg(atomic64_t *ptr, u64 new)
 	smp_mb();
 
 	__asm__ __volatile__("@ atomic64_xchg\n"
-"1:	ldrexd	%0, %H0, [%3]\n"
+"1:     \n"
+#ifdef CONFIG_PJ4B_ERRATA_6011
+	pj4b_6011_ldrexd(%0, %H0, %3, %1)
+#else
+"	ldrexd	%0, %H0, [%3]\n"
+#endif
 "	strexd	%1, %4, %H4, [%3]\n"
 "	teq	%1, #0\n"
 "	bne	1b"
@@ -403,7 +490,12 @@ static inline u64 atomic64_dec_if_positive(atomic64_t *v)
 	smp_mb();
 
 	__asm__ __volatile__("@ atomic64_dec_if_positive\n"
-"1:	ldrexd	%0, %H0, [%3]\n"
+"1:     \n"
+#ifdef CONFIG_PJ4B_ERRATA_6011
+	pj4b_6011_ldrexd(%0, %H0, %3, %1)
+#else
+"	ldrexd	%0, %H0, [%3]\n"
+#endif
 "	subs	%0, %0, #1\n"
 "	sbc	%H0, %H0, #0\n"
 "	teq	%H0, #0\n"
@@ -430,7 +522,12 @@ static inline int atomic64_add_unless(atomic64_t *v, u64 a, u64 u)
 	smp_mb();
 
 	__asm__ __volatile__("@ atomic64_add_unless\n"
-"1:	ldrexd	%0, %H0, [%4]\n"
+"1:     \n"
+#ifdef CONFIG_PJ4B_ERRATA_6011
+	pj4b_6011_ldrexd(%0, %H0, %4, %2)
+#else
+"	ldrexd	%0, %H0, [%4]\n"
+#endif
 "	teq	%0, %5\n"
 "	teqeq	%H0, %H5\n"
 "	moveq	%1, #0\n"
diff --git a/arch/arm/include/asm/barrier.h b/arch/arm/include/asm/barrier.h
index 0511238..c9b5c6e 100644
--- a/arch/arm/include/asm/barrier.h
+++ b/arch/arm/include/asm/barrier.h
@@ -16,7 +16,13 @@
 #if __LINUX_ARM_ARCH__ >= 7
 #define isb() __asm__ __volatile__ ("isb" : : : "memory")
 #define dsb() __asm__ __volatile__ ("dsb" : : : "memory")
+#if defined(CONFIG_PJ4B_ERRATA_6359)
+#define dmb() __asm__ __volatile__ ("dsb" : : : "memory")
+#elif defined(CONFIG_PJ4B_ERRATA_6359_LIGHTWEIGHT)
+#define dmb() __asm__ __volatile__ ("dmb\nnop\nnop\nnop\nnop\nnop\nnop\nnop\nnop\n" : : : "memory")
+#else
 #define dmb() __asm__ __volatile__ ("dmb" : : : "memory")
+#endif
 #elif defined(CONFIG_CPU_XSC3) || __LINUX_ARM_ARCH__ == 6
 #define isb() __asm__ __volatile__ ("mcr p15, 0, %0, c7, c5, 4" \
 				    : : "r" (0) : "memory")
diff --git a/arch/arm/include/asm/futex.h b/arch/arm/include/asm/futex.h
index 7be5469..d0cefd3 100644
--- a/arch/arm/include/asm/futex.h
+++ b/arch/arm/include/asm/futex.h
@@ -25,6 +25,69 @@
 
 #ifdef CONFIG_SMP
 
+#ifdef CONFIG_PJ4B_ERRATA_6011
+#include <asm/pj4b-errata-6011.h>
+#define __futex_atomic_op(insn, ret, oldval, tmp, uaddr, oparg)	\
+	smp_mb();						\
+	{							\
+		__asm__ __volatile__(				\
+		"	mrs     %2, cpsr\n"			\
+		"	cpsid	i\n"				\
+		"1:	ldrex	%1, [%3]\n"			\
+		"	ldrex	%1, [%3]\n"			\
+		"	ldrex	%1, [%3]\n"			\
+		"	ldrex	%1, [%3]\n"			\
+		"	ldrex	%1, [%3]\n"			\
+		"	msr    cpsr_c, %2\n"			\
+		"	" insn "\n"				\
+		"2:	strex	%2, %0, [%3]\n"			\
+		"	teq	%2, #0\n"			\
+		"	bne	1b\n"				\
+		"	mov	%0, #0\n"			\
+		__futex_atomic_ex_table("%5")			\
+		: "=&r" (ret), "=&r" (oldval), "=&r" (tmp)	\
+		: "r" (uaddr), "r" (oparg), "Ir" (-EFAULT)	\
+		: "cc", "memory");				\
+	}
+
+static inline int
+futex_atomic_cmpxchg_inatomic(u32 *uval, u32 __user *uaddr,
+			      u32 oldval, u32 newval)
+{
+	int ret;
+	u32 val;
+	u32 tmp;
+
+	if (!access_ok(VERIFY_WRITE, uaddr, sizeof(u32)))
+		return -EFAULT;
+
+	smp_mb();
+	__asm__ __volatile__("@futex_atomic_cmpxchg_inatomic\n"
+	"	mrs     %2, cpsr\n"
+	"	cpsid	i\n"
+	"1:	ldrex	%1, [%5]\n"
+	"	ldrex	%1, [%5]\n"
+	"	ldrex	%1, [%5]\n"
+	"	ldrex	%1, [%5]\n"
+	"	ldrex	%1, [%5]\n"
+	"	msr    cpsr_c, %2\n"
+	"	teq	%1, %3\n"
+	"	ite	eq	@ explicit IT needed for the 2b label\n"
+	"2:	strexeq	%0, %4, [%5]\n"
+	"	movne	%0, #0\n"
+	"	teq	%0, #0\n"
+	"	bne	1b\n"
+	__futex_atomic_ex_table("%6")
+	: "=&r" (ret), "=&r" (val), "=&r" (tmp)
+	: "r" (oldval), "r" (newval), "r" (uaddr), "Ir" (-EFAULT)
+	: "cc", "memory");
+	smp_mb();
+
+	*uval = val;
+	return ret;
+}
+#else /* !CONFIG_PJ4B_ERRATA_6011 */
+
 #define __futex_atomic_op(insn, ret, oldval, tmp, uaddr, oparg)	\
 	smp_mb();						\
 	__asm__ __volatile__(					\
@@ -67,6 +130,7 @@ futex_atomic_cmpxchg_inatomic(u32 *uval, u32 __user *uaddr,
 	*uval = val;
 	return ret;
 }
+#endif /* !CONFIG_PJ4B_ERRATA_6011 */
 
 #else /* !SMP, we can work around lack of atomic ops by disabling preemption */
 
diff --git a/arch/arm/include/asm/hardware/cache-l2x0.h b/arch/arm/include/asm/hardware/cache-l2x0.h
index c4c87bc..ee967ef 100644
--- a/arch/arm/include/asm/hardware/cache-l2x0.h
+++ b/arch/arm/include/asm/hardware/cache-l2x0.h
@@ -47,6 +47,10 @@
 #define L2X0_CLEAN_INV_LINE_PA		0x7F0
 #define L2X0_CLEAN_INV_LINE_IDX		0x7F8
 #define L2X0_CLEAN_INV_WAY		0x7FC
+
+/* MMP3 TAUROS3 SL2 AUX2 Register */
+#define TAUROS3_SL2_AUX2		0x820
+
 /*
  * The lockdown registers repeat 8 times for L310, the L210 has only one
  * D and one I lockdown register at 0x0900 and 0x0904.
diff --git a/arch/arm/include/asm/locks.h b/arch/arm/include/asm/locks.h
index ef4c897..bb21644 100644
--- a/arch/arm/include/asm/locks.h
+++ b/arch/arm/include/asm/locks.h
@@ -14,6 +14,141 @@
 
 #if __LINUX_ARM_ARCH__ >= 6
 
+#ifdef CONFIG_PJ4B_ERRATA_6011
+
+#include <asm/pj4b-errata-6011.h>
+
+#define __down_op(ptr,fail)			\
+	({					\
+	__asm__ __volatile__(			\
+	"@ down_op\n"				\
+"1:	\n"					\
+	pj4b_6011_ldrex(lr, %0, ip)		\
+"	sub	lr, lr, %1\n"			\
+"	strex	ip, lr, [%0]\n"			\
+"	teq	ip, #0\n"			\
+"	bne	1b\n"				\
+"	teq	lr, #0\n"			\
+"	movmi	ip, %0\n"			\
+"	blmi	" #fail				\
+	:					\
+	: "r" (ptr), "I" (1)			\
+	: "ip", "lr", "cc");			\
+	smp_mb();				\
+	})
+
+#define __down_op_ret(ptr,fail)			\
+	({					\
+		unsigned int ret;		\
+	__asm__ __volatile__(			\
+	"@ down_op_ret\n"			\
+"1:	\n"					\
+	pj4b_6011_ldrex(lr, %1, ip)		\
+"	sub	lr, lr, %2\n"			\
+"	strex	ip, lr, [%1]\n"			\
+"	teq	ip, #0\n"			\
+"	bne	1b\n"				\
+"	teq	lr, #0\n"			\
+"	movmi	ip, %1\n"			\
+"	movpl	ip, #0\n"			\
+"	blmi	" #fail "\n"			\
+"	mov	%0, ip"				\
+	: "=&r" (ret)				\
+	: "r" (ptr), "I" (1)			\
+	: "ip", "lr", "cc");			\
+	smp_mb();				\
+	ret;					\
+	})
+
+#define __up_op(ptr,wake)			\
+	({					\
+	smp_mb();				\
+	__asm__ __volatile__(			\
+	"@ up_op\n"				\
+"1:	\n"					\
+	pj4b_6011_ldrex(lr, %0, ip)		\
+"	add	lr, lr, %1\n"			\
+"	strex	ip, lr, [%0]\n"			\
+"	teq	ip, #0\n"			\
+"	bne	1b\n"				\
+"	cmp	lr, #0\n"			\
+"	movle	ip, %0\n"			\
+"	blle	" #wake				\
+	:					\
+	: "r" (ptr), "I" (1)			\
+	: "ip", "lr", "cc");			\
+	})
+
+/*
+ * The value 0x01000000 supports up to 128 processors and
+ * lots of processes.  BIAS must be chosen such that sub'ing
+ * BIAS once per CPU will result in the long remaining
+ * negative.
+ */
+#define RW_LOCK_BIAS      0x01000000
+#define RW_LOCK_BIAS_STR "0x01000000"
+
+#define __down_op_write(ptr,fail)		\
+	({					\
+	__asm__ __volatile__(			\
+	"@ down_op_write\n"			\
+"1:	\n"					\
+	pj4b_6011_ldrex(lr, %0, ip)		\
+"	sub	lr, lr, %1\n"			\
+"	strex	ip, lr, [%0]\n"			\
+"	teq	ip, #0\n"			\
+"	bne	1b\n"				\
+"	teq	lr, #0\n"			\
+"	movne	ip, %0\n"			\
+"	blne	" #fail				\
+	:					\
+	: "r" (ptr), "I" (RW_LOCK_BIAS)		\
+	: "ip", "lr", "cc");			\
+	smp_mb();				\
+	})
+
+#define __up_op_write(ptr,wake)			\
+	({					\
+	smp_mb();				\
+	__asm__ __volatile__(			\
+	"@ up_op_write\n"			\
+"1:	\n"					\
+	pj4b_6011_ldrex(lr, %0, ip)		\
+"	adds	lr, lr, %1\n"			\
+"	strex	ip, lr, [%0]\n"			\
+"	teq	ip, #0\n"			\
+"	bne	1b\n"				\
+"	movcs	ip, %0\n"			\
+"	blcs	" #wake				\
+	:					\
+	: "r" (ptr), "I" (RW_LOCK_BIAS)		\
+	: "ip", "lr", "cc");			\
+	})
+
+#define __down_op_read(ptr,fail)		\
+	__down_op(ptr, fail)
+
+#define __up_op_read(ptr,wake)			\
+	({					\
+	smp_mb();				\
+	__asm__ __volatile__(			\
+	"@ up_op_read\n"			\
+"1:	\n"					\
+	pj4b_6011_ldrex(lr, %0, ip)		\
+"	add	lr, lr, %1\n"			\
+"	strex	ip, lr, [%0]\n"			\
+"	teq	ip, #0\n"			\
+"	bne	1b\n"				\
+"	teq	lr, #0\n"			\
+"	moveq	ip, %0\n"			\
+"	bleq	" #wake				\
+	:					\
+	: "r" (ptr), "I" (1)			\
+	: "ip", "lr", "cc");			\
+	})
+
+#else
+
 #define __down_op(ptr,fail)			\
 	({					\
 	__asm__ __volatile__(			\
@@ -137,6 +272,8 @@
 	: "ip", "lr", "cc");			\
 	})
 
+#endif
+
 #else
 
 #define __down_op(ptr,fail)			\
diff --git a/arch/arm/include/asm/pj4b-errata-6011.h b/arch/arm/include/asm/pj4b-errata-6011.h
new file mode 100644
index 0000000..4e72a7d
--- /dev/null
+++ b/arch/arm/include/asm/pj4b-errata-6011.h
@@ -0,0 +1,68 @@
+#ifndef __ASMARM_PJ4B_ERRATA_6011_H
+#define __ASMARM_PJ4B_ERRATA_6011_H
+
+#ifdef __ASSEMBLY__
+	.macro  pj4b_6011_ldrex, p0, p1, p2
+	mrs	\p2, cpsr
+	cpsid	i
+	ldrex  \p0, [\p1]
+	ldrex  \p0, [\p1]
+	ldrex  \p0, [\p1]
+	ldrex  \p0, [\p1]
+	ldrex  \p0, [\p1]
+	msr    cpsr_c, \p2
+	.endm
+
+	.macro  pj4b_6011_ldrexb, p0, p1, p2
+	mrs	\p2, cpsr
+	cpsid	i
+	ldrexb  \p0, [\p1]
+	ldrexb  \p0, [\p1]
+	ldrexb  \p0, [\p1]
+	ldrexb  \p0, [\p1]
+	ldrexb  \p0, [\p1]
+	msr    cpsr_c, \p2
+	.endm
+#endif
+
+#define pj4b_6011_ldrexb(p0, p1, p2) \
+	"mrs    "#p2", cpsr\n" \
+	"cpsid  i\n" \
+	"ldrexb "#p0", ["#p1"]\n" \
+	"ldrexb "#p0", ["#p1"]\n" \
+	"ldrexb "#p0", ["#p1"]\n" \
+	"ldrexb "#p0", ["#p1"]\n" \
+	"ldrexb "#p0", ["#p1"]\n" \
+	"msr    cpsr_c, "#p2"\n" \
+
+#define pj4b_6011_ldrexh(p0, p1, p2) \
+	"mrs    "#p2", cpsr\n" \
+	"cpsid  i\n" \
+	"ldrexh "#p0", ["#p1"]\n" \
+	"ldrexh "#p0", ["#p1"]\n" \
+	"ldrexh "#p0", ["#p1"]\n" \
+	"ldrexh "#p0", ["#p1"]\n" \
+	"ldrexh "#p0", ["#p1"]\n" \
+	"msr    cpsr_c, "#p2"\n" \
+
+#define pj4b_6011_ldrex(p0, p1, p2) \
+	"mrs    "#p2", cpsr\n" \
+	"cpsid	i\n" \
+	"ldrex	"#p0", ["#p1"]\n" \
+	"ldrex	"#p0", ["#p1"]\n" \
+	"ldrex	"#p0", ["#p1"]\n" \
+	"ldrex	"#p0", ["#p1"]\n" \
+	"ldrex	"#p0", ["#p1"]\n" \
+	"msr    cpsr_c, "#p2"\n" \
+
+#define pj4b_6011_ldrexd(p0, p1, p2, p3)  \
+	"mrs    "#p3", cpsr\n" \
+	"cpsid	i\n" \
+	"ldrexd	"#p0", "#p1", ["#p2"]\n" \
+	"ldrexd	"#p0", "#p1", ["#p2"]\n" \
+	"ldrexd	"#p0", "#p1", ["#p2"]\n" \
+	"ldrexd	"#p0", "#p1", ["#p2"]\n" \
+	"ldrexd "#p0", "#p1", ["#p2"]\n" \
+	"msr    cpsr_c, "#p3"\n" \
+
+#endif
diff --git a/arch/arm/include/asm/setup.h b/arch/arm/include/asm/setup.h
index 23ebc0c..62d10fd 100644
--- a/arch/arm/include/asm/setup.h
+++ b/arch/arm/include/asm/setup.h
@@ -137,6 +137,15 @@ struct tag_acorn {
 };
 
 /* footbridge memory clock, see arch/arm/mach-footbridge/arch.c */
+
+#ifdef CONFIG_CPU_MMP3
+#define ATAG_PROFILE    0x41000403
+struct tag_mv_profile {
+	u32	soc_prof;
+	u32	soc_stepping;
+};
+#endif
+
 #define ATAG_MEMCLK	0x41000402
 
 struct tag_memclk {
@@ -165,6 +174,9 @@ struct tag {
 		 * DC21285 specific
 		 */
 		struct tag_memclk	memclk;
+#ifdef CONFIG_CPU_MMP3
+		struct tag_mv_profile   mv_prof;
+#endif
 	} u;
 };
 
diff --git a/arch/arm/include/asm/spinlock.h b/arch/arm/include/asm/spinlock.h
index 65fa3c8..e9ad569 100644
--- a/arch/arm/include/asm/spinlock.h
+++ b/arch/arm/include/asm/spinlock.h
@@ -42,6 +42,10 @@
 #define WFE(cond)	ALT_SMP("wfe" cond, "nop")
 #endif
 
+#ifdef CONFIG_PJ4B_ERRATA_6011
+#include <asm/pj4b-errata-6011.h>
+#endif
+
 static inline void dsb_sev(void)
 {
 #if __LINUX_ARM_ARCH__ >= 7
@@ -78,6 +82,23 @@ static inline void dsb_sev(void)
 
 static inline void arch_spin_lock(arch_spinlock_t *lock)
 {
+#ifdef CONFIG_PJ4B_ERRATA_6011
+	unsigned long tmp, tmp1;
+
+	__asm__ __volatile__(
+"1:		\n"
+	pj4b_6011_ldrex(%0, %2, %1)
+"	teq	%0, #0\n"
+	WFE("ne")
+"	strexeq	%0, %3, [%2]\n"
+"	teqeq	%0, #0\n"
+"	bne	1b"
+	: "=&r" (tmp), "=&r" (tmp1)
+	: "r" (&lock->lock), "r" (1)
+	: "cc");
+
+	smp_mb();
+#else
 	unsigned long tmp;
 
 	__asm__ __volatile__(
@@ -92,10 +113,29 @@ static inline void arch_spin_lock(arch_spinlock_t *lock)
 	: "cc");
 
 	smp_mb();
+#endif
 }
 
 static inline int arch_spin_trylock(arch_spinlock_t *lock)
 {
+#ifdef CONFIG_PJ4B_ERRATA_6011
+	unsigned long tmp, tmp1;
+
+	__asm__ __volatile__(
+	pj4b_6011_ldrex(%0, %2, %1)
+"	teq	%0, #0\n"
+"	strexeq	%0, %3, [%2]"
+	: "=&r" (tmp), "=&r" (tmp1)
+	: "r" (&lock->lock), "r" (1)
+	: "cc");
+
+	if (tmp == 0) {
+		smp_mb();
+		return 1;
+	} else {
+		return 0;
+	}
+#else
 	unsigned long tmp;
 
 	__asm__ __volatile__(
@@ -112,6 +152,7 @@ static inline int arch_spin_trylock(arch_spinlock_t *lock)
 	} else {
 		return 0;
 	}
+#endif
 }
 
 static inline void arch_spin_unlock(arch_spinlock_t *lock)
@@ -137,6 +178,23 @@ static inline void arch_spin_unlock(arch_spinlock_t *lock)
 
 static inline void arch_write_lock(arch_rwlock_t *rw)
 {
+#ifdef CONFIG_PJ4B_ERRATA_6011
+	unsigned long tmp, tmp1;
+
+	__asm__ __volatile__(
+"1:		\n"
+	pj4b_6011_ldrex(%0, %2, %1)
+"	teq	%0, #0\n"
+	WFE("ne")
+"	strexeq	%0, %3, [%2]\n"
+"	teq	%0, #0\n"
+"	bne	1b"
+	: "=&r" (tmp), "=&r" (tmp1)
+	: "r" (&rw->lock), "r" (0x80000000)
+	: "cc");
+
+	smp_mb();
+#else
 	unsigned long tmp;
 
 	__asm__ __volatile__(
@@ -151,10 +209,29 @@ static inline void arch_write_lock(arch_rwlock_t *rw)
 	: "cc");
 
 	smp_mb();
+#endif
 }
 
 static inline int arch_write_trylock(arch_rwlock_t *rw)
 {
+#ifdef CONFIG_PJ4B_ERRATA_6011
+	unsigned long tmp, tmp1;
+
+	__asm__ __volatile__(
+	pj4b_6011_ldrex(%0, %2, %1)
+"	teq	%0, #0\n"
+"	strexeq	%0, %3, [%2]"
+	: "=&r" (tmp), "=&r" (tmp1)
+	: "r" (&rw->lock), "r" (0x80000000)
+	: "cc");
+
+	if (tmp == 0) {
+		smp_mb();
+		return 1;
+	} else {
+		return 0;
+	}
+#else
 	unsigned long tmp;
 
 	__asm__ __volatile__(
@@ -171,6 +248,7 @@ static inline int arch_write_trylock(arch_rwlock_t *rw)
 	} else {
 		return 0;
 	}
+#endif
 }
 
 static inline void arch_write_unlock(arch_rwlock_t *rw)
@@ -206,7 +284,12 @@ static inline void arch_read_lock(arch_rwlock_t *rw)
 	unsigned long tmp, tmp2;
 
 	__asm__ __volatile__(
-"1:	ldrex	%0, [%2]\n"
+"1:		\n"
+#ifdef CONFIG_PJ4B_ERRATA_6011
+	pj4b_6011_ldrex(%0, %2, %1)
+#else
+"	ldrex	%0, [%2]\n"
+#endif
 "	adds	%0, %0, #1\n"
 "	strexpl	%1, %0, [%2]\n"
 	WFE("mi")
@@ -226,7 +309,12 @@ static inline void arch_read_unlock(arch_rwlock_t *rw)
 	smp_mb();
 
 	__asm__ __volatile__(
-"1:	ldrex	%0, [%2]\n"
+"1:		\n"
+#ifdef CONFIG_PJ4B_ERRATA_6011
+	pj4b_6011_ldrex(%0, %2, %1)
+#else
+"	ldrex	%0, [%2]\n"
+#endif
 "	sub	%0, %0, #1\n"
 "	strex	%1, %0, [%2]\n"
 "	teq	%1, #0\n"
@@ -244,7 +332,12 @@ static inline int arch_read_trylock(arch_rwlock_t *rw)
 	unsigned long tmp, tmp2 = 1;
 
 	__asm__ __volatile__(
-"1:	ldrex	%0, [%2]\n"
+"1:		\n"
+#ifdef CONFIG_PJ4B_ERRATA_6011
+	pj4b_6011_ldrex(%0, %2, %1)
+#else
+"	ldrex	%0, [%2]\n"
+#endif
 "	adds	%0, %0, #1\n"
 "	strexpl	%1, %0, [%2]\n"
 	: "=&r" (tmp), "+r" (tmp2)
diff --git a/arch/arm/kernel/entry-armv.S b/arch/arm/kernel/entry-armv.S
index 8f29865..8430cb4 100644
--- a/arch/arm/kernel/entry-armv.S
+++ b/arch/arm/kernel/entry-armv.S
@@ -32,6 +32,10 @@
 #include "entry-header.S"
 #include <asm/entry-macro-multi.S>
 
+#ifdef CONFIG_PJ4B_ERRATA_6011
+#include <asm/pj4b-errata-6011.h>
+#endif
+
 /*
  * Interrupt handling.
  */
@@ -791,6 +795,33 @@ ENDPROC(__switch_to)
 	.globl	__kuser_helper_start
 __kuser_helper_start:
 
+#ifdef CONFIG_PJ4B_ERRATA_6011
+__kuser_cmpxchg_pj4b_errata_6011:
+	smp_dmb	arm
+1:
+	stmfd	sp!, {r0}
+	pj4b_6011_ldrex r3, r2, r0
+	ldmfd	sp!, {r0}
+	subs	r3, r3, r0
+	strexeq	r3, r1, [r2]
+	teqeq	r3, #1
+	beq	1b
+	rsbs	r0, r3, #0
+	/* beware -- each __kuser slot must be 8 instructions max */
+	ALT_SMP(b	__kuser_memory_barrier)
+	ALT_UP(usr_ret	lr)
+
+	.align	5
+#endif
+
+#if defined(CONFIG_PJ4B_ERRATA_6359_LIGHTWEIGHT)
+__kuser_memory_barrier_errata_6359_lightweight:
+	smp_dmb	arm
+	usr_ret	lr
+
+	.align	5
+#endif
+
 /*
  * Due to the length of some sequences, __kuser_cmpxchg64 spans 2 regular
  * kuser "slots", therefore 0xffff0f80 is not used as a valid entry point.
@@ -885,8 +916,16 @@ kuser_cmpxchg64_fixup:
 	.align	5
 
 __kuser_memory_barrier:				@ 0xffff0fa0
+#if defined(CONFIG_PJ4B_ERRATA_6359_LIGHTWEIGHT)
+	/*
+	 * smp_dmb is now defined as "DMB + 8 NOPs", which exceeds the 8 words
+	 * limitation of kuser function. We jump to another place for workaround.
+	 */
+	b	__kuser_memory_barrier_errata_6359_lightweight
+#else
 	smp_dmb	arm
 	usr_ret	lr
+#endif
 
 	.align	5
 
@@ -946,8 +985,9 @@ kuser_cmpxchg32_fixup:
 	usr_ret	lr
 #endif
 
+#elif defined(CONFIG_PJ4B_ERRATA_6011)
+	b       __kuser_cmpxchg_pj4b_errata_6011
 #else
-
 	smp_dmb	arm
 1:	ldrex	r3, [r2]
 	subs	r3, r3, r0
diff --git a/arch/arm/kernel/iwmmxt.S b/arch/arm/kernel/iwmmxt.S
index a087838..0a1b0c4 100644
--- a/arch/arm/kernel/iwmmxt.S
+++ b/arch/arm/kernel/iwmmxt.S
@@ -19,7 +19,7 @@
 #include <asm/thread_info.h>
 #include <asm/asm-offsets.h>
 
-#if defined(CONFIG_CPU_PJ4)
+#if defined(CONFIG_CPU_PJ4) || defined(CONFIG_CPU_PJ4B)
 #define PJ4(code...)		code
 #define XSC(code...)
 #else
diff --git a/arch/arm/kernel/swp_emulate.c b/arch/arm/kernel/swp_emulate.c
index ab1017b..ada402d 100644
--- a/arch/arm/kernel/swp_emulate.c
+++ b/arch/arm/kernel/swp_emulate.c
@@ -32,6 +32,40 @@
 /*
  * Error-checking SWP macros implemented using ldrex{b}/strex{b}
  */
+#ifdef CONFIG_PJ4B_ERRATA_6011
+#include <asm/pj4b-errata-6011.h>
+#define __user_swpX_asm(data, addr, res, temp, B)			\
+	do {								\
+		unsigned long temp1;					\
+		__asm__ __volatile__(					\
+		"	mov		%2, %1\n"			\
+		"	mrs    %3, cpsr\n"				\
+		"	cpsid  i\n"					\
+		"0:	ldrex"B" %1, [%4]\n"				\
+		"	ldrex"B" %1, [%4]\n"				\
+		"	ldrex"B" %1, [%4]\n"				\
+		"	ldrex"B" %1, [%4]\n"				\
+		"	ldrex"B" %1, [%4]\n"				\
+		"	msr    cpsr_c, %3\n"				\
+		"1:	strex"B"	%0, %2, [%4]\n"			\
+		"	cmp		%0, #0\n"			\
+		"	movne		%0, %5\n"			\
+		"2:\n"							\
+		"	.section	 .fixup,\"ax\"\n"		\
+		"	.align		2\n"				\
+		"3:	mov		%0, %6\n"			\
+		"	b		2b\n"				\
+		"	.previous\n"					\
+		"	.section	 __ex_table,\"a\"\n"		\
+		"	.align		3\n"				\
+		"	.long		0b, 3b\n"			\
+		"	.long		1b, 3b\n"			\
+		"	.previous"					\
+		: "=&r" (res), "+r" (data), "=&r" (temp), "=&r" (temp1)	\
+		: "r" (addr), "i" (-EAGAIN), "i" (-EFAULT)		\
+		: "cc", "memory");					\
+	} while (0)
+#else /* !CONFIG_PJ4B_ERRATA_6011 */
 #define __user_swpX_asm(data, addr, res, temp, B)		\
 	__asm__ __volatile__(					\
 	"	mov		%2, %1\n"			\
@@ -53,6 +87,7 @@
 	: "=&r" (res), "+r" (data), "=&r" (temp)		\
 	: "r" (addr), "i" (-EAGAIN), "i" (-EFAULT)		\
 	: "cc", "memory")
+#endif /* !CONFIG_PJ4B_ERRATA_6011 */
 
 #define __user_swp_asm(data, addr, res, temp) \
 	__user_swpX_asm(data, addr, res, temp, "")
diff --git a/arch/arm/lib/bitops.h b/arch/arm/lib/bitops.h
index d6408d1..5ac99ca 100644
--- a/arch/arm/lib/bitops.h
+++ b/arch/arm/lib/bitops.h
@@ -1,6 +1,9 @@
 #include <asm/unwind.h>
 
 #if __LINUX_ARM_ARCH__ >= 6
+#ifdef CONFIG_PJ4B_ERRATA_6011
+#include <asm/pj4b-errata-6011.h>
+#endif
 	.macro	bitop, name, instr
 ENTRY(	\name		)
 UNWIND(	.fnstart	)
@@ -11,7 +14,11 @@ UNWIND(	.fnstart	)
 	mov	r0, r0, lsr #5
 	add	r1, r1, r0, lsl #2	@ Get word offset
 	mov	r3, r2, lsl r3
+#ifdef CONFIG_PJ4B_ERRATA_6011
+1:	pj4b_6011_ldrex r2, r1, r0
+#else
 1:	ldrex	r2, [r1]
+#endif
 	\instr	r2, r2, r3
 	strex	r0, r2, [r1]
 	cmp	r0, #0
@@ -32,7 +39,11 @@ UNWIND(	.fnstart	)
 	add	r1, r1, r0, lsl #2	@ Get word offset
 	mov	r3, r2, lsl r3		@ create mask
 	smp_dmb
+#ifdef CONFIG_PJ4B_ERRATA_6011
+1:	pj4b_6011_ldrex r2, r1, ip
+#else
 1:	ldrex	r2, [r1]
+#endif
 	ands	r0, r2, r3		@ save old value of bit
 	\instr	r2, r2, r3		@ toggle bit
 	strex	ip, r2, [r1]
diff --git a/arch/arm/mm/Kconfig b/arch/arm/mm/Kconfig
index 7c8a7d8..402f51a 100644
--- a/arch/arm/mm/Kconfig
+++ b/arch/arm/mm/Kconfig
@@ -388,6 +388,12 @@ config CPU_PJ4
 	select CPU_V7
 	select ARM_THUMBEE
 
+# Marvell PJ4B
+config CPU_PJ4B
+	bool
+	select CPU_V7
+	select ARM_THUMBEE
+
 # ARMv6
 config CPU_V6
 	bool "Support ARM V6 processor" if ARCH_INTEGRATOR || MACH_REALVIEW_EB || MACH_REALVIEW_PBX
@@ -856,6 +862,11 @@ config CACHE_L2X0
 	help
 	  This option enables the L2x0 PrimeCell.
 
+config CACHE_L2X0_PREFETCH
+	bool "Enable prefetch modes for L2x0 cache"
+	depends on CACHE_L2X0
+	default n
+
 config CACHE_PL310
 	bool
 	depends on CACHE_L2X0
@@ -864,6 +875,17 @@ config CACHE_PL310
 	  This option enables optimisations for the PL310 cache
 	  controller.
 
+config CACHE_TAUROS3_PREFETCH_OFFSET
+	int "Prefetch offset value (0-31)"
+	range 0 31
+	depends on CACHE_L2X0_PREFETCH && CACHE_PL310
+	default "0"
+	help
+	  This value will be set in the L2CC prefetch control register[4:0],
+	  which indicates that the (value+1)th cacheline will be prefetched.
+	  According to the SPEC the valid values can be 0-7, 15, 23 and 31,
+	  other values are not supported by PL310
+
 config CACHE_TAUROS2
 	bool "Enable the Tauros2 L2 cache controller"
 	depends on (ARCH_DOVE || ARCH_MMP || CPU_PJ4)
@@ -873,6 +895,27 @@ config CACHE_TAUROS2
 	  This option enables the Tauros2 L2 cache controller (as
 	  found on PJ1/PJ4).
 
+config CACHE_TAUROS2_PREFETCH_OFF
+	bool "Disable prefetch mode for L2 cache"
+	depends on CACHE_TAUROS2
+	default y
+	help
+	  Say Y if you want to disable L2 cache prefetch feature
+
+config CACHE_TAUROS2_LINEFILL_BURST8
+	bool "Enable L2 Fill Burst Length 8 of Tauros2 L2 controller"
+	depends on CACHE_TAUROS2 && !CACHE_TAUROS2_PREFETCH_OFF
+	default n
+	help
+	  Say Y if you want to enable L2 line fill burst length to 8
+
+config CACHE_TAUROS2_WRITEBUFFER_COALESCING_OFF
+	bool "Disable write buffer coalescing for L2 cache"
+	depends on CACHE_TAUROS2
+	default n
+	help
+	  Say Y if you want to enable L2 cache write buffer coalescing feature
+
 config CACHE_XSC3L2
 	bool "Enable the L2 cache on XScale3"
 	depends on CPU_XSC3
@@ -918,3 +961,63 @@ config ARCH_HAS_BARRIERS
 	help
 	  This option allows the use of custom mandatory barriers
 	  included via the mach/barriers.h file.
+
+config PJ4_RETURNSTACK_ISSUE
+	bool "PJ4 core issue: enable return stack would cause system unstable"
+	depends on CPU_PJ4 || CPU_PJ4B
+	default y
+	help
+		This option enables the workaround for Marvell PJ4 to disable return
+		stack which might cause system unstable.
+
+config DISABLE_CWF
+	bool "Disable critical word first sequencing"
+	depends on CPU_PJ4B && SMP
+	default y
+	help
+	  This option disables critical word first sequencing.
+
+config DISABLE_CACHE_TLB_OP_BCAST
+	bool "Disable cache and TLB maintenance broadcast"
+	depends on CPU_PJ4B && SMP
+	default n
+	help
+	  This option disables cache and TLB maintenance broadcast.
+
+config FORCE_SCU_SINGLE_ENTRY
+	bool "Force SCU single entry"
+	depends on CPU_PJ4B && SMP
+	help
+	  This option forces SCU snoop queue to single entry.
+
+config DISABLE_SCU_PIPELINE
+	bool "Disable SCU pipeline"
+	depends on CPU_PJ4B
+	help
+	  This option disables SCU pipeline.
+
+config DISABLE_CLEAN_INTERVENTION
+	bool "Disable clean intervention"
+	depends on CPU_PJ4B
+	help
+	  This option disables clean intervention.
+
+config CACHE_TAUROS3_WRITETHROUGH
+	bool "Force Tauros3 L2 cache write through"
+	depends on CACHE_L2X0
+	help
+	  Say Y here to use the Tauros3 L2 cache in writethrough mode.
+
+config CACHE_TAUROS3_DISABLE_MEMORY_MAPPED_FUNCTIONS
+	bool "Disable Tauros3 memory mapped cache maintenance functions"
+	depends on CACHE_L2X0
+	help
+	  Say Y here to disable Tauros3 memory mapped cache maintenance functions.
+	  In Tauros3, when "Point of Coherence" cache maintenance operations are
+	  applied on a given address, both the L1 and L2 are automatically taken care of.
+
+config CACHE_TAUROS3_ENABLE_FULL_WRITE_LINE
+	bool "Enable Tauros3 full write line"
+	depends on CACHE_L2X0 && CPU_MMP3
+	help
+	  This option enables Tauros3 L2 cache full write line.
diff --git a/arch/arm/mm/cache-l2x0.c b/arch/arm/mm/cache-l2x0.c
index 2a8e380..e5e35e0 100644
--- a/arch/arm/mm/cache-l2x0.c
+++ b/arch/arm/mm/cache-l2x0.c
@@ -289,6 +289,7 @@ static void l2x0_disable(void)
 
 static void l2x0_unlock(u32 cache_id)
 {
+#ifndef CONFIG_CPU_MMP3
 	int lockregs;
 	int i;
 
@@ -304,6 +305,7 @@ static void l2x0_unlock(u32 cache_id)
 		writel_relaxed(0x0, l2x0_base + L2X0_LOCKDOWN_WAY_I_BASE +
 			       i * L2X0_LOCKDOWN_STRIDE);
 	}
+#endif
 }
 
 void __init l2x0_init(void __iomem *base, u32 aux_val, u32 aux_mask)
@@ -313,6 +315,11 @@ void __init l2x0_init(void __iomem *base, u32 aux_val, u32 aux_mask)
 	u32 way_size = 0;
 	int ways;
 	const char *type;
+	u32 debug_ctrl;
+	__u32 prefetch_ctrl = 0;
+#ifdef CONFIG_CPU_MMP3
+	u32 aux2;
+#endif
 
 	l2x0_base = base;
 
@@ -349,6 +356,42 @@ void __init l2x0_init(void __iomem *base, u32 aux_val, u32 aux_mask)
 
 	l2x0_way_mask = (1 << ways) - 1;
 
+#ifdef CONFIG_CACHE_L2X0_PREFETCH
+	/* Configure double line fill and prefetch */
+
+	prefetch_ctrl = 0;
+	prefetch_ctrl |= (1u << 30); /* DLF (double linefill) enabled*/
+	prefetch_ctrl |= (1u << 29); /* instruction prefetch enabled */
+	prefetch_ctrl |= (1u << 28); /* data prefetch enabled */
+	prefetch_ctrl |= (0u << 27); /* DLF on WRAP read enabled*/
+	prefetch_ctrl |= (0u << 24); /* no discard prefetch reads*/
+	prefetch_ctrl |= (0u << 23); /* no 8x64-bit burst on read miss*/
+	prefetch_ctrl |= (0u << 21); /* use same AXI ID on exclusive seq */
+	if (CONFIG_CACHE_TAUROS3_PREFETCH_OFFSET <= 7)
+		prefetch_ctrl |= CONFIG_CACHE_TAUROS3_PREFETCH_OFFSET;
+	else if (CONFIG_CACHE_TAUROS3_PREFETCH_OFFSET <= 15)
+		prefetch_ctrl |= 15;
+	else if (CONFIG_CACHE_TAUROS3_PREFETCH_OFFSET <= 23)
+		prefetch_ctrl |= 23;
+	else
+		prefetch_ctrl |= 31;
+	writel_relaxed(prefetch_ctrl, l2x0_base + L2X0_PREFETCH_CTRL);
+#endif
+
+#ifdef CONFIG_CACHE_TAUROS3_WRITETHROUGH
+	debug_ctrl = readl_relaxed(l2x0_base + L2X0_DEBUG_CTRL);
+	debug_ctrl |= (1 << 1);
+	writel_relaxed(debug_ctrl, l2x0_base + L2X0_DEBUG_CTRL);
+#endif
+
+#ifdef CONFIG_CPU_MMP3
+	aux2 = readl_relaxed(l2x0_base + TAUROS3_SL2_AUX2);
+#ifdef CONFIG_CACHE_TAUROS3_ENABLE_FULL_WRITE_LINE
+	aux2 |= (1 << 13);
+	writel_relaxed(aux2, l2x0_base + TAUROS3_SL2_AUX2);
+	aux2 = readl_relaxed(l2x0_base + TAUROS3_SL2_AUX2);
+#endif
+#endif
 	/*
 	 * L2 cache Size =  Way size * Number of ways
 	 */
@@ -376,6 +419,7 @@ void __init l2x0_init(void __iomem *base, u32 aux_val, u32 aux_mask)
 		writel_relaxed(1, l2x0_base + L2X0_CTRL);
 	}
 
+#ifndef CONFIG_CACHE_TAUROS3_DISABLE_MEMORY_MAPPED_FUNCTIONS
 	outer_cache.inv_range = l2x0_inv_range;
 	outer_cache.clean_range = l2x0_clean_range;
 	outer_cache.flush_range = l2x0_flush_range;
@@ -383,10 +427,15 @@ void __init l2x0_init(void __iomem *base, u32 aux_val, u32 aux_mask)
 	outer_cache.flush_all = l2x0_flush_all;
 	outer_cache.inv_all = l2x0_inv_all;
 	outer_cache.disable = l2x0_disable;
+#endif
 
 	printk(KERN_INFO "%s cache controller enabled\n", type);
 	printk(KERN_INFO "l2x0: %d ways, CACHE_ID 0x%08x, AUX_CTRL 0x%08x, Cache size: %d B\n",
 			ways, cache_id, aux, l2x0_size);
+	printk(KERN_INFO "l2x0: PREFETCH_CTRL 0x%08x\n", prefetch_ctrl);
+#ifdef CONFIG_CPU_MMP3
+	printk(KERN_INFO "tauros3: SL2_AUX2 0x%08x\n", aux2);
+#endif
 }
 
 #ifdef CONFIG_OF
diff --git a/arch/arm/mm/cache-v7.S b/arch/arm/mm/cache-v7.S
index 82ab2c5..bc102cc 100644
--- a/arch/arm/mm/cache-v7.S
+++ b/arch/arm/mm/cache-v7.S
@@ -42,7 +42,17 @@ ENDPROC(v7_flush_icache_all)
  *	- mm    - mm_struct describing address space
  */
 ENTRY(v7_flush_dcache_all)
+#if defined(CONFIG_PJ4B_ERRATA_6359)
+	dsb
+#elif defined(CONFIG_PJ4B_ERRATA_6359_LIGHTWEIGHT)
+	dmb
+	.rept 8
+	nop
+	.endr
+#else
 	dmb					@ ensure ordering with previous memory accesses
+#endif
+
 	mrc	p15, 1, r0, c0, c0, 1		@ read clidr
 	ands	r3, r0, #0x7000000		@ extract loc from clidr
 	mov	r3, r3, lsr #23			@ left align loc bit field
@@ -50,6 +60,9 @@ ENTRY(v7_flush_dcache_all)
 	mov	r10, #0				@ start clean at cache level 0
 loop1:
 	add	r2, r10, r10, lsr #1		@ work out 3x current cache level
+#ifdef CONFIG_PJ4B_ERRATA_6315
+	mrc     p15, 1, r0, c0, c0, 1           @ read clidr
+#endif
 	mov	r1, r0, lsr r2			@ extract cache type bits from clidr
 	and	r1, r1, #7			@ mask of the bits for current cache only
 	cmp	r1, #2				@ see what cache we have at this level
@@ -79,6 +92,12 @@ loop3:
  ARM(	orr	r11, r11, r7, lsl r2	)	@ factor index number into r11
  THUMB(	lsl	r6, r7, r2		)
  THUMB(	orr	r11, r11, r6		)	@ factor index number into r11
+#ifdef CONFIG_PJ4B_ERRATA_6315
+	dsb					@ replace dmb w. dsb to ensure ordering with previous memory accesses
+	mov	r0, #24				@ require approximately a 24 cycle delay to guarantee all outstanding
+1:	subs	r0, r0, #1			@ request to complete
+	bne	1b
+#endif
 	mcr	p15, 0, r11, c7, c14, 2		@ clean & invalidate by set/way
 	subs	r9, r9, #1			@ decrement the way
 	bge	loop3
diff --git a/arch/arm/mm/proc-v7.S b/arch/arm/mm/proc-v7.S
index c2e2b66..59bcc57 100644
--- a/arch/arm/mm/proc-v7.S
+++ b/arch/arm/mm/proc-v7.S
@@ -69,7 +69,9 @@ ENDPROC(cpu_v7_reset)
  *	IRQs are already disabled.
  */
 ENTRY(cpu_v7_do_idle)
+#ifndef CONFIG_CPU_PJ4B
 	dsb					@ WFI may enter a low-power mode
+#endif
 	wfi
 	mov	pc, lr
 ENDPROC(cpu_v7_do_idle)
@@ -161,15 +163,92 @@ __v7_ca7mp_setup:
 __v7_ca15mp_setup:
 	mov	r10, #0
 1:
+__v7_setup:
+#if defined(CONFIG_CPU_PJ4B) && defined(CONFIG_NEON)
+	mrc	p15, 1, r0, c15, c1, 1
+	orr	r0, r0, #(1 << 11)		@ PJ4B NEON enable bit
+	mcr	p15, 1, r0, c15, c1, 1
+#endif
+#if defined(CONFIG_CPU_MMP3) && defined(CONFIG_NEON)
+	mrc	p15, 0, r5, c1, c0, 2		@ Read coprocessor access
+	mov	r0, r5
+	orr	r0, r0, #0x00f00000		@ Allow access to cp10 and cp11
+	mcr	p15, 0, r0, c1, c0, 2		@ Enable CP10, CP11 coprocessor access
+	mov	r0, #0
+	mcr	p15, 0, r0, c7, c5, 4		@ Flush prefetch buffer
+	mrc	p10, 7, r0, c8, c0, 0
+	orr	r0, r0, #0x40000000
+	mcr	p10, 7, r0, c8, c0, 0		@ Enable FPEXC.EN bit
+	isb
+	mov	r0, #1
+	mcr	p10, 7, r0, c9, c0, 0		@ Enable dual-issue feature in FPCFG.bit0
+	/* Continuous 6 VMOV instructions to initialize the unused second pipe */
+	.word	0xeeb00b00			@ VMOV.F64 d0, #2.00000000
+	.word	0xeeb00b00
+	.word	0xeeb00b00
+	.word	0xeeb00b00
+	.word	0xeeb00b00
+	.word	0xeeb00b00
+	mov	r0, #0
+	mcr	p10, 7, r0, c9, c0, 0		@ Disable dual-issue feature in FPCFG.bit0
+	isb
+	mrc	p10, 7, r0, c8, c0, 0
+	bic	r0, r0, #0x40000000
+	mcr	p10, 7, r0, c8, c0, 0		@ Disable FPEXC.EN bit
+	mcr	p15, 0, r5, c1, c0, 2		@ Disable CP10, CP11 coprocessor access
+	mov	r0, #0
+	mcr	p15, 0, r0, c7, c5, 4		@ Flush prefetch buffer
+#endif
 #ifdef CONFIG_SMP
 	ALT_SMP(mrc	p15, 0, r0, c1, c0, 1)
 	ALT_UP(mov	r0, #(1 << 6))		@ fake it for UP
 	tst	r0, #(1 << 6)			@ SMP/nAMP mode enabled?
-	orreq	r0, r0, #(1 << 6)		@ Enable SMP/nAMP mode
-	orreq	r0, r0, r10			@ Enable CPU-specific SMP bits
-	mcreq	p15, 0, r0, c1, c0, 1
+#ifndef CONFIG_DISABLE_CACHE_TLB_OP_BCAST
+	orreq	r0, r0, #(1 << 6) | (1 << 0)	@ Enable SMP/nAMP mode and
+#else
+	biceq	r0, r0, #(1 << 0)		@ clear FW
+	orreq	r0, r0, #(1 << 6)		@ set SMP/nAMP
+#endif
+	mcreq	p15, 0, r0, c1, c0, 1		@ TLB ops broadcasting
+#endif
+#ifdef CONFIG_PJ4_RETURNSTACK_ISSUE
+	mrc	p15, 1, r0, c15, c1, 1
+	bic	r0, r0, #0x1			@disable return stack
+	mcr	p15, 1, r0, c15, c1, 1
+#endif
+#ifdef CONFIG_DISABLE_CWF
+	mrc	p15, 1, r0, c15, c1, 2
+	orr	r0, r0, #(1 << 27)
+	mcr	p15, 1, r0, c15, c1, 2
+#endif
+#ifdef CONFIG_FORCE_SCU_SINGLE_ENTRY
+	mrc	p15, 1, r0, c15, c1, 1
+	orr	r0, r0, #(1 << 15)
+	mcr	p15, 1, r0, c15, c1, 1
+#endif
+#ifdef CONFIG_DISABLE_SCU_PIPELINE
+	mrc	p15, 1, r0, c15, c1, 1
+	orr	r0, r0, #(1 << 4)
+	mcr	p15, 1, r0, c15, c1, 1
+#endif
+
+#ifdef CONFIG_DISABLE_CLEAN_INTERVENTION
+	mrc	p15, 1, r0, c15, c1, 1
+	orr	r0, r0, #(1 << 16)
+	mcr	p15, 1, r0, c15, c1, 1
+#endif
+
+#ifdef CONFIG_PJ4B_ERRATA_6409
+	mrc	p15, 1, r0, c15, c1, 1
+	bic	r0, r0, #0x4			@ Disable Static BP
+	mcr	p15, 1, r0, c15, c1, 1
+#endif
+
+#ifdef CONFIG_PJ4B_ERRATA_6107
+	mrc	p15, 1, r0, c15, c1, 0
+	orr	r0, r0, #(1 << 0)		@ disable L0
+	mcr	p15, 1, r0, c15, c1, 0
 #endif
-__v7_setup:
 	adr	r12, __v7_setup_stack		@ the local stack
 	stmia	r12, {r0-r5, r7, r9, r11, lr}
 	bl	v7_flush_dcache_all
diff --git a/include/linux/spi/pxa2xx_spi.h b/include/linux/spi/pxa2xx_spi.h
index d3e1075..c73d144 100644
--- a/include/linux/spi/pxa2xx_spi.h
+++ b/include/linux/spi/pxa2xx_spi.h
@@ -43,7 +43,7 @@ struct pxa2xx_spi_chip {
 	void (*cs_control)(u32 command);
 };
 
-#ifdef CONFIG_ARCH_PXA
+#if defined(CONFIG_ARCH_PXA) || defined(CONFIG_ARCH_MMP)
 
 #include <linux/clk.h>
 #include <mach/dma.h>
-- 
1.7.5.4

