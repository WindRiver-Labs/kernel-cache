From c1819ac271d6df020cbd16d845450f5b32666c53 Mon Sep 17 00:00:00 2001
From: Stanley.Miao <stanley.miao@windriver.com>
Date: Tue, 19 Oct 2010 17:35:20 +0800
Subject: [PATCH 12/26] Comcerto: M83xxx: Comcerto Ethernet driver

Original codes came from Mindspeed's vendor drop sdk-comcerto-openwrt-6.0.

The M83xxx includes two ethernet ports: WAN (ETH0) and LAN (ETH2). Both
Ethernet interfaces support MII, RMII, GMII, RGMII.

Integrated-by: Stanley.Miao <stanley.miao@windriver.com>
---
 drivers/net/Kconfig              |    2 +
 drivers/net/Makefile             |    2 +-
 drivers/net/comcerto/Kconfig     |   11 +
 drivers/net/comcerto/Makefile    |   10 +
 drivers/net/comcerto/c1000_eth.c | 1480 ++++++++++++++++++++++++++++++++++++++
 drivers/net/comcerto/c1000_eth.h |  226 ++++++
 6 files changed, 1730 insertions(+), 1 deletions(-)
 create mode 100644 drivers/net/comcerto/Kconfig
 create mode 100644 drivers/net/comcerto/Makefile
 create mode 100644 drivers/net/comcerto/c1000_eth.c
 create mode 100644 drivers/net/comcerto/c1000_eth.h

diff --git a/drivers/net/Kconfig b/drivers/net/Kconfig
index fcc9321..e5295b0 100644
--- a/drivers/net/Kconfig
+++ b/drivers/net/Kconfig
@@ -25,6 +25,8 @@ menuconfig NETDEVICES
 # that for each of the symbols.
 if NETDEVICES
 
+source "drivers/net/comcerto/Kconfig"
+
 config IFB
 	tristate "Intermediate Functional Block support"
 	depends on NET_CLS_ACT
diff --git a/drivers/net/Makefile b/drivers/net/Makefile
index c61363a..f269e75 100644
--- a/drivers/net/Makefile
+++ b/drivers/net/Makefile
@@ -49,7 +49,7 @@ obj-$(CONFIG_FSL_PQ_MDIO) += fsl_pq_mdio.o
 obj-$(CONFIG_PLIP) += plip.o
 
 obj-$(CONFIG_ROADRUNNER) += rrunner.o
-
+obj-$(CONFIG_NET_COMCERTO) += comcerto/
 obj-$(CONFIG_HAPPYMEAL) += sunhme.o
 obj-$(CONFIG_SUNLANCE) += sunlance.o
 obj-$(CONFIG_SUNQE) += sunqe.o
diff --git a/drivers/net/comcerto/Kconfig b/drivers/net/comcerto/Kconfig
new file mode 100644
index 0000000..8309b41
--- /dev/null
+++ b/drivers/net/comcerto/Kconfig
@@ -0,0 +1,11 @@
+config NET_COMCERTO
+	bool "Mindspeed Comcerto Network infrastructure"
+	depends on (ARCH_M83XXX || ARCH_M821XX)
+	help
+
+config COMCERTO_ETH
+	tristate "Mindspeed's Comcerto Ethernet Driver"
+	select PHYLIB
+	depends on NET_COMCERTO
+	default y
+
diff --git a/drivers/net/comcerto/Makefile b/drivers/net/comcerto/Makefile
new file mode 100644
index 0000000..4f9850b
--- /dev/null
+++ b/drivers/net/comcerto/Makefile
@@ -0,0 +1,10 @@
+
+ifeq ($(CONFIG_ARCH_M821XX),y)
+	obj-$(CONFIG_COMCERTO_ETH) += comcerto_eth.o comcerto_gemac.o comcerto_gem_AL.o \
+                        comcerto_mii.o comcerto_ethtool.o comcerto_sysfs.o
+endif
+ifeq ($(CONFIG_ARCH_M83XXX),y)
+	obj-$(CONFIG_COMCERTO_ETH) += c1000_fbpool.o c1000_eth.o c1000_gemac.o comcerto_gem_AL.o \
+                        comcerto_mii.o c1000_ethtool.o
+endif
+
diff --git a/drivers/net/comcerto/c1000_eth.c b/drivers/net/comcerto/c1000_eth.c
new file mode 100644
index 0000000..4abaa9b
--- /dev/null
+++ b/drivers/net/comcerto/c1000_eth.c
@@ -0,0 +1,1480 @@
+ /*
+  *  linux/drivers/net/comcerto/c1000_eth.c
+  *
+  *  Copyright (C) 2006 Mindspeed Technologies, Inc.
+  *
+  * This program is free software; you can redistribute it and/or modify
+  * it under the terms of the GNU General Public License as published by
+  * the Free Software Foundation; either version 2 of the License, or
+  * (at your option) any later version.
+  *
+  * This program is distributed in the hope that it will be useful,
+  * but WITHOUT ANY WARRANTY; without even the implied warranty of
+  * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+  * GNU General Public License for more details.
+  *
+  * You should have received a copy of the GNU General Public License
+  * along with this program; if not, write to the Free Software
+  * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
+  */
+
+#include <linux/kernel.h>
+#include <linux/interrupt.h>
+#include <linux/dma-mapping.h>
+#include <linux/dmapool.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/ethtool.h>
+#include <linux/mii.h>
+#include <linux/phy.h>
+#include <linux/platform_device.h>
+#include <linux/slab.h>
+#include <net/ip.h>
+#include <net/sock.h>
+#include <linux/io.h>
+#include <linux/irq.h>
+#include <linux/delay.h>
+#include <mach/hardware.h>
+#include <mach/memory.h>
+#include <mach/irq.h>
+
+#if defined(CONFIG_NF_CONNTRACK_MARK)
+#include <net/netfilter/nf_conntrack.h>
+#endif
+
+#if defined(CONFIG_INET_IPSEC_OFFLOAD) || defined(CONFIG_INET6_IPSEC_OFFLOAD)
+#include <net/xfrm.h>
+#endif
+
+#include "c1000_eth.h"
+#include "c1000_fbpool.h"
+#include "comcerto_mii.h"
+
+const char comcerto_eth_driver_version[] = "1.0";
+
+extern struct comcerto_eth_platform_data comcerto_gem0_pdata;
+extern struct comcerto_eth_platform_data comcerto_gem1_pdata;
+extern struct ethtool_ops c1k_ethtool_ops;
+extern  struct xfrm_state *xfrm_state_lookup_byhandle(struct net *net, u16 handle);
+
+#define PKT_HEADROOM	(64 + 2)
+#define PKT_BUF_SZ	1536
+
+
+static void c1k_eth_release_buffers(struct net_device *dev);
+static int c1k_eth_init_buffers(struct net_device *dev);
+irqreturn_t c1k_eth_rx_interrupt(int irq, void *dev_id);
+static void c1k_adjust_link(struct net_device *dev);
+int c1k_set_tx_csum(struct net_device *dev, uint32_t data);
+
+extern int c1k_emac_checksum(struct eth_c1k_priv *priv, u32 status, u8 *ip_summed);
+extern irqreturn_t c1k_gemac_interrupt(int irq, void *dev_id);
+extern void c1k_gemac_setduplex(struct net_device *dev, int duplex);
+extern void c1k_gemac_setspeed(struct net_device *dev, int speed);
+
+extern int c1k_gemac_init(struct net_device *dev);
+
+static void c1k_eth_poolB_cb(struct c1k_fbpool *fbpool)
+{
+	void *ret_ptr;
+	struct sk_buff *skb;
+	struct net_device *dev;
+	struct eth_c1k_priv *priv;
+	unsigned long flags;
+	int free[2] = {0, 0};
+	struct net_device *device[2];
+	int i;
+
+	while ((ret_ptr = c1k_fbpool_get(fbpool))) {
+		skb = (struct sk_buff *)(*(unsigned long*)ret_ptr);
+		dev = skb->dev;
+		priv = netdev_priv(dev);
+
+		free[priv->id]++;
+
+		device[priv->id] = dev;
+
+		dev_kfree_skb_any(skb);
+	}
+
+	for (i = 0; i < 2; i++) {
+		if (!free[i])
+			continue;
+
+		dev = device[i];
+		priv = netdev_priv(dev);
+
+		spin_lock_irqsave(&priv->txlock, flags);
+
+		priv->Txavail += free[i];
+
+		if (netif_msg_intr(priv))
+			printk(KERN_DEBUG "%s %s: txavail %d\n", dev->name, __func__, priv->Txavail);
+
+		if (unlikely(netif_queue_stopped(dev) && (priv->Txavail > (priv->TxRingSize >> 1))))
+			netif_wake_queue(dev);
+
+		spin_unlock_irqrestore(&priv->txlock, flags);
+	}
+}
+
+
+static phy_interface_t c1k_get_interface(struct net_device *dev)
+{
+	struct eth_c1k_priv *priv = netdev_priv(dev);
+	u32 mii_mode = priv->einfo->mii_config;
+
+	if (priv->einfo->gemac_mode & (GEMAC_SW_CONF)) {
+		switch (mii_mode) {
+		case CONFIG_COMCERTO_USE_GMII:
+			return PHY_INTERFACE_MODE_GMII;
+		case CONFIG_COMCERTO_USE_RGMII:
+			return PHY_INTERFACE_MODE_RGMII;
+		case CONFIG_COMCERTO_USE_RMII:
+			return PHY_INTERFACE_MODE_RMII;
+		default:
+		case CONFIG_COMCERTO_USE_MII:
+			return PHY_INTERFACE_MODE_MII;
+		}
+	} else {
+		/* Bootstrap config read from controller */
+		BUG();
+		return 0;
+	}
+}
+
+/* Initializes driver's PHY state, and attaches to the PHY.
+ * Returns 0 on success.
+ */
+static int c1k_phy_start(struct net_device *dev)
+{
+	struct eth_c1k_priv *priv = netdev_priv(dev);
+	struct phy_device *phydev;
+	char phy_id[MII_BUS_ID_SIZE + 3];
+	char bus_id[4];
+	phy_interface_t interface;
+
+	priv->oldlink = 0;
+	priv->oldspeed = 0;
+	priv->oldduplex = -1;
+
+	sprintf(bus_id, "%d", priv->einfo->bus_id);
+	snprintf(phy_id, MII_BUS_ID_SIZE + 3, PHY_ID_FMT, bus_id, priv->einfo->phy_id);
+
+	printk(KERN_INFO "PHY start: %s\n", phy_id);
+
+	interface = c1k_get_interface(dev);
+
+	priv->oldlink = 0;
+	priv->oldspeed = 0;
+	priv->oldduplex = -1;
+
+	phydev = phy_connect(dev, phy_id, &c1k_adjust_link, 0, interface);
+
+	if (IS_ERR(phydev)) {
+		printk(KERN_ERR "%s: Could not attach to PHY\n", dev->name);
+		return PTR_ERR(phydev);
+	}
+
+	priv->phydev = phydev;
+
+	return 0;
+
+}
+
+static void c1k_phy_stop(struct net_device *dev)
+{
+	struct eth_c1k_priv *priv = netdev_priv(dev);
+
+	phy_disconnect(priv->phydev);
+	priv->phydev = NULL;
+}
+
+
+static int c1k_fast_path_set(struct net_device *dev, unsigned short state)
+{
+	return 0;
+}
+
+/* Bring the controller up and running */
+int c1k_eth_start(struct net_device *dev)
+{
+	struct eth_c1k_priv *priv = netdev_priv(dev);
+
+	if (netif_msg_drv(priv))
+		printk (KERN_INFO "%s: %s\n", __func__, dev->name);
+
+	if (c1k_eth_init_buffers(dev)) {
+		printk(KERN_ERR "%s: Could not allocate buffer descriptors!\n", dev->name);
+		return -ENOMEM;
+	}
+
+	if (priv->phydev)
+		phy_start(priv->phydev);
+
+	/* Enable scheduler block for tx */
+	writel(0x3, priv->baseaddr + GEM_SCH_BLOCK + SCH_CONTROL);
+
+	if (priv->fast_path_enabled) {
+		writel(readl(priv->fpp_smi_baseaddr + FPP_SMI_CTRL) | GEM_RX_EN | GEM_TX_EN, priv->fpp_smi_baseaddr + FPP_SMI_CTRL);
+	} else {
+
+		/* clean up queue */
+		while (readl(priv->baseaddr + GEM_ADM_BLOCK + ADM_QUEUEDEPTH))
+			writel(0, priv->baseaddr + GEM_ADM_BLOCK + ADM_PKTDQ);
+
+		/* Start the controller */
+		gem_set_rx_offset(&priv->gemdev, PKT_HEADROOM);
+
+		writel(priv->RxRingSize - 2, priv->baseaddr + GEM_ADM_BLOCK + ADM_QFULLTHR);
+
+		/* setup Rx coalescing */
+		writel(priv->rx_coal_time * 125, priv->baseaddr + GEM_ADM_BLOCK + ADM_BATCHINTRTIMERINIT);
+		writel(priv->rx_coal_count, priv->baseaddr + GEM_ADM_BLOCK + ADM_BATCHINTRPKTTHRES);
+
+		writel(0x84210030, priv->baseaddr + GEM_ADM_BLOCK + ADM_CNFG);
+		writel(0x000000aa, priv->baseaddr + GEM_ADM_BLOCK + ADM_DECAYTIMER);
+
+		writel(0, priv->baseaddr + GEM_ADM_BLOCK + ADM_BATCHINTRPKTCNT);
+		writel(0x7F, priv->baseaddr + GEM_ADM_BLOCK + ADM_CONTROL);
+
+		gem_enable_rx(&priv->gemdev);
+		gem_enable_tx(&priv->gemdev);
+	}
+
+	return 0;
+}
+
+void c1k_eth_stop(struct net_device *dev)
+{
+	struct eth_c1k_priv *priv = netdev_priv(dev);
+	unsigned long flags;
+	unsigned long tmp;
+
+	if (netif_msg_drv(priv))
+		printk (KERN_INFO "%s: %s\n", __func__, dev->name);
+
+	/* Lock it down */
+	spin_lock_irqsave(&priv->txlock, flags);
+
+	if (priv->fast_path_enabled) {
+		tmp = readl(priv->fpp_smi_baseaddr + FPP_SMI_CTRL);
+		tmp &= ~(GEM_RX_EN | GEM_TX_EN);
+		writel(tmp, priv->fpp_smi_baseaddr + FPP_SMI_CTRL);
+	} else {
+		gem_abort_tx(&priv->gemdev);
+		gem_disable_rx(&priv->gemdev);
+	}
+
+	spin_unlock_irqrestore(&priv->txlock, flags);
+
+	if (priv->phydev)
+		phy_stop(priv->phydev);
+
+	c1k_eth_release_buffers(dev);
+}
+
+
+int c1k_eth_open(struct net_device *dev)
+{
+	struct eth_c1k_priv *priv = netdev_priv(dev);
+	u16 tmp;
+	int rc;
+
+	if (netif_msg_ifup(priv))
+		printk (KERN_INFO "%s: %s\n", __func__, dev->name);
+
+	if (priv->fast_path_enabled) {
+		dev->irq = priv->virt_rx_coal_int;
+
+		priv->RxRingSize = SLOW_PATH_DESC_NT;
+		priv->TxRingSize = SLOW_PATH_DESC_NT;
+		priv->rx_coal_count = DEFAULT_RX_COAL_PKTS;
+		priv->rx_coal_time = DEFAULT_RX_COAL_TIME;
+
+	} else {
+		dev->irq = priv->phys_rx_coal_int;
+
+		priv->RxRingSize = DEFAULT_RX_DESC_NT;
+		priv->TxRingSize = DEFAULT_TX_DESC_NT;
+		priv->rx_coal_count = DEFAULT_RX_COAL_PKTS;
+		priv->rx_coal_time = DEFAULT_RX_COAL_TIME;
+	}
+
+	priv->poolB = c1k_fbpool_open(1, c1k_eth_poolB_cb);
+	if (!priv->poolB) {
+		printk(KERN_ERR "%s %s: failed to open free buffer pool B\n", dev->name, __func__);
+		rc = -EINVAL;
+		goto err1;
+	}
+
+	c1k_gemac_init(dev);
+
+	if (!is_valid_ether_addr(dev->dev_addr)) {
+		printk(KERN_ERR "%s %s: invalid MAC address\n", dev->name, __func__);
+		rc = -EADDRNOTAVAIL;
+		goto err2;
+	}
+
+	gem_add_arc_entry(&priv->gemdev, dev->dev_addr);
+
+	if (!(priv->einfo->phy_flags & GEMAC_NO_PHY)) {
+		rc = c1k_phy_start(dev);
+		if (rc) {
+			printk(KERN_ERR "%s %s: failed to get phy\n", dev->name, __func__);
+			goto err2;
+		}
+
+#if defined(CONFIG_EVM_C1KMFCN_EVM)
+		/* sw reset */
+		phy_write(priv->phydev, MII_BMCR, BMCR_RESET);
+		while (phy_read(priv->phydev, MII_BMCR) & BMCR_RESET);
+#endif
+
+		if (priv->einfo->phy_flags & GEMAC_PHY_RGMII_ADD_DELAY) {
+			tmp = phy_read(priv->phydev, 20);
+			/* enable additional delay on Rx_clk and TX_clk */
+			tmp |= 0x0082;
+			phy_write(priv->phydev, 20, tmp);
+			/* sw reset */
+			phy_write(priv->phydev, MII_BMCR, BMCR_RESET);
+			while (phy_read(priv->phydev, MII_BMCR) & BMCR_RESET);
+		}
+	}
+
+	napi_enable(&priv->napi);
+
+	rc = c1k_eth_start(dev);
+	if (rc)
+		goto err3;
+
+	if (priv->fast_path_enabled)
+		c1k_fast_path_set(dev, 1);
+
+	netif_wake_queue(dev);
+
+	rc = request_irq(dev->irq, c1k_eth_rx_interrupt, IRQF_DISABLED, "c1k_gemRx", dev);
+	if (rc) {
+		printk(KERN_ERR "%s %s: failed to get the Rx IRQ = %d\n", dev->name, __func__, dev->irq);
+		goto err4;
+	}
+
+	if (!priv->fast_path_enabled) {
+		rc = request_irq(priv->gem_int, c1k_gemac_interrupt, IRQF_DISABLED, "c1k_gemac", dev);
+		if (rc) {
+			printk(KERN_ERR "%s %s: failed to get the GemIP IRQ = %d\n", dev->name, __func__, priv->gem_int);
+			goto err5;
+		}
+	}
+
+	return rc;
+
+err5:
+	free_irq(dev->irq, dev);
+
+err4:
+	netif_stop_queue(dev);
+
+	if (priv->fast_path_enabled)
+		c1k_fast_path_set(dev, 0);
+
+	c1k_eth_stop(dev);
+
+err3:
+	napi_disable(&priv->napi);
+
+	/* Disconnect from the PHY */
+	if (!(priv->einfo->phy_flags & GEMAC_NO_PHY))
+		c1k_phy_stop(dev);
+
+err2:
+	c1k_fbpool_close(priv->poolB);
+
+err1:
+	return rc;
+}
+
+
+int c1k_eth_close(struct net_device *dev)
+{
+	struct eth_c1k_priv *priv = netdev_priv(dev);
+
+	if (netif_msg_ifdown(priv))
+		printk(KERN_INFO "%s: %s\n", dev->name, __func__);
+
+	/* Free the IRQs */
+	if (!priv->fast_path_enabled)
+		free_irq(priv->gem_int, dev);
+
+	free_irq(dev->irq, dev);
+
+	netif_stop_queue(dev);
+
+	if (priv->fast_path_enabled)
+		c1k_fast_path_set(dev, 0);
+
+	c1k_eth_stop(dev);
+	napi_disable(&priv->napi);
+
+	if (!(priv->einfo->phy_flags & GEMAC_NO_PHY))
+		c1k_phy_stop(dev);
+
+	c1k_fbpool_close(priv->poolB);
+
+	return 0;
+}
+
+static int c1k_set_mac_address(struct net_device *dev, void *addr)
+{
+	struct eth_c1k_priv *priv = netdev_priv(dev);
+	struct sockaddr *sa = addr;
+
+	if (!is_valid_ether_addr(sa->sa_data))
+		return -EADDRNOTAVAIL;
+
+	memcpy(dev->dev_addr, sa->sa_data, ETH_ALEN);
+
+	gem_add_arc_entry(&priv->gemdev, dev->dev_addr);
+
+	return 0;
+}
+
+static void c1k_eth_set_multi(struct net_device *dev)
+{
+	struct eth_c1k_priv *priv = netdev_priv(dev);
+	struct dev_mc_list  *mcptr;
+	GEM_DEVICE *gemdev = &priv->gemdev;
+	u8          *addr_start = NULL; /* start of address */
+	MAC_ADDR    hash_addr;          /* hash register structure */
+	u8          temp1, temp2, temp3, temp4, temp5, temp6, temp7, temp8;
+	int         i, result;          /* index into hash register to set.. */
+
+
+	if (dev->flags & IFF_PROMISC) {
+		if (netif_msg_drv(priv))
+			printk(KERN_INFO "%s : Entering promiscuous mode.\n", dev->name);
+
+		gem_enable_copy_all(gemdev);
+	} else {
+		gem_disable_copy_all(gemdev);
+	}
+
+	/* Enable broadcast frame reception if required. */
+	if (dev->flags & IFF_BROADCAST) {
+		gem_allow_broadcast(gemdev);
+	} else {
+		if (netif_msg_drv(priv))
+			printk(KERN_INFO "%s : disabling broadcast frame reception.\n", dev->name);
+
+		gem_no_broadcast(gemdev);
+	}
+
+	if (dev->flags & IFF_ALLMULTI) {
+		/* Set the hash to rx all multicast frames */
+		hash_addr.bottom = 0xFFFFFFFF;
+		hash_addr.top = 0xFFFFFFFF;
+		gem_set_hash(gemdev, &hash_addr);
+		gem_enable_multicast(gemdev);
+
+	} else if (dev->mc_count > 0) {
+		hash_addr.bottom = 0;
+		hash_addr.top = 0;
+
+		mcptr = dev->mc_list;
+
+		for (i = 0; i < dev->mc_count; i++) {
+			addr_start = (unsigned char *)&mcptr->dmi_addr;
+
+			if (netif_msg_drv(priv))
+				printk(KERN_DEBUG "%s: adding multicast address %X:%X:%X:%X:%X:%X to gem filter\n",
+						dev->name,
+						addr_start[0], addr_start[1], addr_start[2],
+						addr_start[3], addr_start[4], addr_start[5]);
+
+			temp1 = addr_start[0] & 0x3F;
+			temp2 = ((addr_start[0] & 0xC0)  >> 6) | ((addr_start[1] & 0x0F) << 2);
+			temp3 = ((addr_start[1] & 0xF0) >> 4) | ((addr_start[2] & 0x03) << 4);
+			temp4 = (addr_start[2] & 0xFC) >> 2;
+			temp5 = addr_start[3] & 0x3F;
+			temp6 = ((addr_start[3] & 0xC0) >> 6) | ((addr_start[4] & 0x0F) << 2);
+			temp7 = ((addr_start[4] & 0xF0) >> 4) | ((addr_start[5] & 0x03) << 4);
+			temp8 = ((addr_start[5] & 0xFC) >> 2);
+
+			result = temp1 ^ temp2 ^ temp3 ^ temp4 ^ temp5 ^ temp6 ^ temp7 ^ temp8;
+
+			if (result >= GEM_HASH_REG_BITS) {
+				break;
+			} else {
+				if (result < 32)
+					hash_addr.bottom |= (1 << result);
+				else
+					hash_addr.top |= (1 << (result - 32));
+			}
+
+			mcptr = mcptr->next;
+		}
+
+		gem_set_hash(gemdev, &hash_addr);
+		gem_enable_multicast(gemdev);
+	} else
+		gem_disable_multicast(gemdev);
+
+	if (dev->flags & IFF_LOOPBACK)
+		gem_set_loop(gemdev, LB_LOCAL);
+
+	return;
+}
+
+static struct net_device_stats *c1k_eth_get_stats(struct net_device *dev)
+{
+	struct eth_c1k_priv *priv = netdev_priv(dev);
+
+	return &priv->stats;
+}
+
+static int c1k_eth_change_mtu(struct net_device *dev, int new_mtu)
+{
+
+	struct eth_c1k_priv *priv = netdev_priv(dev);
+	GEM_DEVICE *gemdev = &priv->gemdev;
+	int oldsize = dev->mtu ;
+	int frame_size = new_mtu + ETH_HLEN + 4;
+
+#if !defined(JUMBO_FRAME_SIZE)
+#	define JUMBO_FRAME_SIZE 0x2048
+#endif
+	if ((frame_size < 64) || (frame_size > JUMBO_FRAME_SIZE)) {
+		if (netif_msg_drv(priv))
+			printk(KERN_ERR "%s: Invalid MTU setting\n",
+					dev->name);
+		return -EINVAL;
+	}
+	if (frame_size > 1536) {
+		/* Can not support larger packets.
+		 * Would need to:
+		 * change ip_network config
+		 * rx buffer size in dma config
+		 * rx staggering in buffer pool
+		 */
+		if (netif_msg_drv(priv))
+			printk(KERN_ERR "%s: Invalid MTU setting - Jumbo frames not supported\n",
+					dev->name);
+		return -EINVAL;
+	}
+	/* Only stop and start the controller if it isn't already
+	 * stopped, and we changed something */
+	if ((oldsize != new_mtu) && ((dev->flags & IFF_UP) || (priv->fast_path_enabled))) {
+		printk(KERN_ERR "%s: Can not change MTU - fast_path must be disabled and ifconfig down must be issued first\n",
+				dev->name);
+		return -EINVAL;
+	}
+
+	dev->mtu = new_mtu;
+
+	/* If the mtu is larger than the max size for standard
+	 * ethernet frames (ie, a jumbo frame), setup Jumbo frames */
+	if (frame_size <= 1536) {
+		if (frame_size > 1500 + ETH_HLEN+4)
+			gem_enable_1536_rx(gemdev);
+		else
+			gem_disable_1536_rx(gemdev);
+	}
+
+	return 0;
+}
+
+static void c1k_eth_release_buffers(struct net_device *dev)
+{
+	struct eth_c1k_priv *priv = netdev_priv(dev);
+	struct tRXdesc *ThisRXdesc;
+	void *data;
+	int offset;
+	int length;
+	int i;
+
+	if (netif_msg_drv(priv))
+		printk (KERN_INFO "%s: %s\n", __func__, dev->name);
+
+	for (i = 0; i < priv->RxRingSize; i++) {
+		ThisRXdesc = priv->RxBase + i;
+
+		data = phys_to_virt(ThisRXdesc->rx_data);
+		offset = PKT_HEADROOM;
+
+		length = (ThisRXdesc->rx_status & RX_STA_LEN_MASK) >> RX_STA_LEN_POS;
+
+
+		if (!priv->fast_path_enabled)
+			if (ThisRXdesc->rx_extstatus & GEMRX_OWN)
+				writel(length, priv->baseaddr + GEM_ADM_BLOCK + ADM_PKTDQ);
+
+		/* preserve stagger offset */
+		data -= offset;
+		__dma_single_cpu_to_dev(data, PKT_BUF_SZ + PKT_HEADROOM, DMA_FROM_DEVICE);
+		kfree((void *)((unsigned long)data & ~(FPP_SKB_SIZE - 1)));
+
+		ThisRXdesc->rx_extstatus = 0;
+
+
+	}
+}
+
+
+static int c1k_eth_init_buffers(struct net_device *dev)
+{
+	struct eth_c1k_priv *priv = netdev_priv(dev);
+	struct tRXdesc *ThisRXdesc;
+	struct tTXdesc *ThisTXdesc;
+	int i;
+	u8 *data;
+
+	if (netif_msg_drv(priv))
+		printk (KERN_INFO "%s: %s\n", __func__, dev->name);
+
+	priv->RxBase = priv->ARAM_baseaddr_v;
+
+	memset(priv->RxBase, 0, priv->RxRingSize * sizeof(struct tRXdesc));
+	if (!priv->fast_path_enabled) {
+		ThisRXdesc = priv->RxBase;
+		for (i = 0; i < priv->RxRingSize; i++) {
+			data = kmalloc(FPP_SKB_SIZE, GFP_ATOMIC);
+			if (likely(data)) {
+				ThisRXdesc->rx_data = virt_to_phys(data) + PKT_HEADROOM;
+			} else {
+				if (netif_msg_rx_err(priv))
+					printk(KERN_ERR "%s %s: low on mem\n", dev->name, __func__);
+
+				goto err;
+			}
+			ThisRXdesc++;
+		}
+	}
+	ThisRXdesc = priv->RxBase + priv->RxRingSize - 1;
+	ThisRXdesc->rx_status |= GEMRX_WRAP;
+
+	writel(virt_to_aram(priv->RxBase), priv->baseaddr + GEM_IP + GEM_RX_QPTR);
+
+	/* init Tx ring */
+	priv->TxBase = (struct tTXdesc *) (priv->RxBase + priv->RxRingSize);
+
+	memset(priv->TxBase, 0, priv->TxRingSize * sizeof(struct tTXdesc));
+	ThisTXdesc = priv->TxBase;
+
+	for (i = 0; i < priv->TxRingSize; i++) {
+		ThisTXdesc->txdata = 0;
+		ThisTXdesc->txctl = GEMTX_USED_MASK;
+		ThisTXdesc++;
+	}
+
+	if (!priv->fast_path_enabled)
+		writel(virt_to_aram(priv->TxBase), priv->baseaddr + GEM_IP + GEM_QUEUE_BASE0);
+
+	priv->RxtocleanIndex = 0;
+	priv->RxtofillIndex = 0;
+	priv->Txtosend = 0;
+	priv->Txdone = 0;
+	priv->Txavail = priv->TxRingSize - 1;
+
+	return 0;
+
+err:
+	c1k_eth_release_buffers(dev);
+
+	return -ENOMEM;
+}
+
+	static int
+c1k_hardware_send_packet(struct net_device *dev, struct sk_buff *skb)
+{
+	struct eth_c1k_priv *priv = netdev_priv(dev);
+	struct tTXdesc *ThisTXdesc;
+	int err = priv->Txavail;
+	u32 txctl;
+	int Txavail = priv->Txavail;
+
+	if (netif_msg_intr(priv))
+		printk(KERN_DEBUG "%s %s: txavail %d txtosend %d\n", dev->name, __func__, priv->Txavail, priv->Txtosend);
+
+	if (unlikely(Txavail <= 0)) {
+		err = -ENOMEM;
+		goto err_unlock;
+	}
+
+	ThisTXdesc = priv->TxBase + priv->Txtosend;
+
+	if ((skb_headroom(skb) < FPP_TX_SKB_HEADROOM) || (skb_tailroom(skb) < FPP_TX_SKB_TAILROOM)) {
+		if (pskb_expand_head(skb, FPP_TX_SKB_HEADROOM, FPP_TX_SKB_TAILROOM, GFP_ATOMIC)) {
+			kfree_skb(skb);
+			priv->stats.tx_dropped++;
+			goto err_unlock;
+		}
+	} else if ((skb = skb_unshare(skb, GFP_ATOMIC)) == NULL) {
+		priv->stats.tx_dropped++;
+		goto err_unlock;
+	}
+
+	/* save skb pointer at a fixed offset in data payload
+	 * for PoolB return and skb release
+	 */
+	*(unsigned long *)(skb->data - FPP_TX_SKB_HEADROOM) = (unsigned long)skb;
+
+	ThisTXdesc->txdata = virt_to_phys(skb->data);
+
+	txctl = skb->len | ((FPP_TX_SKB_HEADROOM << GEMTX_OFFSET_SHIFT) | GEMTX_LAST);
+
+	if (!priv->fast_path_enabled) {
+		txctl |= GEMTX_BUFRET | GEMTX_FCS | GEMTX_POOLB;
+		if ((priv->flags & TX_CSUM_OFFLOAD_ENABLED) && (skb->ip_summed == CHECKSUM_PARTIAL))
+			txctl |= GEMTX_L4_CSUM | GEMTX_L3_CSUM;
+	}
+
+	if (priv->Txtosend == (priv->TxRingSize - 1))
+		txctl |= GEMTX_WRAP;
+
+	/*__dma_single_cpu_to_dev*/
+	__dma_single_cpu_to_dev(skb->data, skb->len, DMA_TO_DEVICE);
+
+#if defined(CONFIG_INET_IPSEC_OFFLOAD) || defined(CONFIG_INET6_IPSEC_OFFLOAD)
+	if (skb->ipsec_offload) {
+		if (skb->sp) {
+			int i;
+			struct tTXextdesc *ThisTXextdesc = priv->TxextBase + priv->Txtosend;
+			volatile u16 *sah = ThisTXextdesc->SAhandle;
+
+			*(u64 *)ThisTXextdesc = 0;
+			for (i = skb->sp->len - 1; i >= 0; i--) {
+				struct xfrm_state *x = skb->sp->xvec[i];
+				*sah++ = x->handle;
+			}
+
+			txctl |= GEMTX_EXPT_IPSEC_OUT;
+		} else
+			txctl |= GEMTX_EXPT_IPSEC_IN;
+	}
+#endif
+
+	ThisTXdesc->txctl = txctl;
+
+	priv->Txtosend = (priv->Txtosend + 1) & (priv->TxRingSize - 1);
+	priv->Txavail = Txavail - 1;
+
+	err = priv->Txavail;
+
+	if (unlikely(!priv->Txavail))
+		err = -ENOSPC;
+
+	if (priv->fast_path_enabled)
+		comcerto_softirq_set(priv->virt_tx_int);
+	else
+		writel(skb->len, priv->baseaddr + GEM_SCH_BLOCK + SCH_PACKET_QUEUED);
+
+	priv->stats.tx_packets++;
+	priv->stats.tx_bytes += skb->len;
+
+err_unlock:
+	return err;
+}
+
+
+static int c1k_eth_send_packet(struct sk_buff *skb, struct net_device *dev)
+{
+	struct eth_c1k_priv *priv = netdev_priv(dev);
+	unsigned long flags;
+	int result;
+
+	if (!spin_trylock_irqsave(&priv->txlock, flags)) {
+		/* Collision - tell upper layer to requeue */
+		return NETDEV_TX_LOCKED;
+	}
+
+	result = c1k_hardware_send_packet(dev, skb);
+
+	switch (result) {
+	case -ENOSPC:
+		/* We queued the skb, but now we're out of space. */
+		if (netif_msg_intr(priv))
+			printk(KERN_DEBUG "%s %s: no space for tx\n", dev->name, __func__);
+
+		netif_stop_queue(dev);
+
+		break;
+
+	case -ENOMEM:
+		/* This is a hard error - log it. */
+		if (netif_msg_tx_err(priv))
+			printk(KERN_ERR "%s %s: out of tx resources, returning skb\n", dev->name, __func__);
+
+		netif_stop_queue(dev);
+		spin_unlock_irqrestore(&priv->txlock, flags);
+
+		return NETDEV_TX_BUSY;
+
+	default:
+		break;
+	}
+
+	dev->trans_start = jiffies;
+	spin_unlock_irqrestore(&priv->txlock, flags);
+
+	if ((priv->TxRingSize - result) >= (priv->TxRingSize >> 2))
+		c1k_fbpool_flush(priv->poolB);
+
+	return NETDEV_TX_OK;
+}
+
+#define inc_rx_idx(idxname) (idxname = (idxname + 1) & (priv->RxRingSize - 1))
+
+static int c1k_eth_rx_refill(struct net_device *dev)
+{
+	struct eth_c1k_priv *priv = netdev_priv(dev);
+	u8 *data;
+	int rtf, rtc;
+	int ret = 0;
+	int offset;
+	struct tRXdesc *ThisRXdesc;
+
+	if (priv->RxtofillIndex != priv->RxtocleanIndex) {
+		rtf = priv->RxtofillIndex;
+		rtc = priv->RxtocleanIndex;
+
+		while (rtc != rtf) {
+			ThisRXdesc = priv->RxBase + rtf;
+			if (priv->fast_path_enabled)
+				offset = (ThisRXdesc->rx_extstatus & RX_STA_L3OFF_MASK) >> RX_STA_L3OFF_POS;
+			else
+				offset = PKT_HEADROOM;
+
+			data = kmalloc(FPP_SKB_SIZE, GFP_ATOMIC);
+			if (likely(data)) {
+				/* preserve stagger offset */
+				data += ((ThisRXdesc->rx_data - offset) & (FPP_SKB_SIZE - 1));
+				__dma_single_cpu_to_dev(data, PKT_BUF_SZ + PKT_HEADROOM, DMA_FROM_DEVICE);
+				ThisRXdesc->rx_data = virt_to_phys(data) + PKT_HEADROOM;
+				/* unlock descriptor */
+				*(volatile u32 *) &ThisRXdesc->rx_extstatus = 0;
+				if (netif_msg_drv(priv))
+					printk(KERN_INFO "%s %s refill poolA refill %p\n", dev->name, __func__, data);
+			} else {
+				if (netif_msg_rx_err(priv))
+					printk(KERN_ERR "%s %s: low on mem\n", dev->name, __func__);
+
+				ret = -EAGAIN;
+				break;
+			}
+
+			inc_rx_idx(rtf);
+		}
+
+		priv->RxtofillIndex = rtf;
+	}
+
+	return ret;
+}
+
+static int c1k_eth_rx_packet(struct net_device *dev, unsigned int *work_done,
+		unsigned int work_to_do)
+{
+	struct eth_c1k_priv *priv = netdev_priv(dev);
+	struct tRXdesc *ThisRXdesc;
+	int rtc;
+	u32 rx_data;
+	u32 rx_status;
+	u32 rx_extstatus;
+	void *data_addr;
+	void *buff_addr;
+	int offset;
+	int length;
+	struct sk_buff *skb;
+	int ret = 0;
+	u8 ip_summed;
+	u8 *data;
+	int rtf;
+
+	if (netif_msg_intr(priv))
+		printk(KERN_DEBUG "%s %s\n", dev->name, __func__);
+
+	rtc = priv->RxtocleanIndex;
+	rtf = priv->RxtofillIndex;
+
+	while (1) {
+		/* loop thru rx FDescs */
+		ThisRXdesc = priv->RxBase + rtc;
+		rx_extstatus = ThisRXdesc->rx_extstatus;
+
+		if (unlikely(*work_done >= work_to_do)) {
+			ret = -EAGAIN;
+
+			if (netif_msg_intr(priv)) {
+				printk (KERN_DEBUG "%s %s: FDesc %#lx, RxtocleanIndex %d\n",
+						dev->name, __func__, (unsigned long)ThisRXdesc, rtc);
+				printk (KERN_DEBUG "%s %s: work_done %d, work_to_do %d\n",
+						dev->name, __func__, *work_done, work_to_do);
+			}
+
+			goto done_exit;
+		}
+
+		if (!(rx_extstatus & GEMRX_OWN)) {
+			if (netif_msg_intr(priv))
+				printk(KERN_DEBUG "%s %s: done, FDesc %#lx, RxtocleanIndex %d\n",
+						dev->name, __func__, (unsigned long)ThisRXdesc, rtc);
+			/* here you are pointing to a buffer that has not been
+			 * filled by the emac, ie there is nothing to pass to
+			 * upper layer
+			 */
+			break;
+		}
+
+		rx_data = ThisRXdesc->rx_data;
+		rx_status = ThisRXdesc->rx_status;
+
+		data_addr = phys_to_virt(rx_data);
+		length = (rx_status & RX_STA_LEN_MASK) >> RX_STA_LEN_POS;
+
+		buff_addr = (void *)((unsigned long)data_addr & ~(FPP_SKB_SIZE - 1));
+		offset = data_addr - buff_addr;
+
+		if (!priv->fast_path_enabled)
+			writel(length, priv->baseaddr + GEM_ADM_BLOCK + ADM_PKTDQ);
+
+		if (netif_msg_pktdata(priv)) {
+			u32 tmp;
+			int i;
+			char *pdbg = (char *)data_addr, *pchar;
+			char buf[256];
+			pchar = buf;
+			printk(KERN_DEBUG "%s %s: done, FDesc %#lx, offset %x status %x extstatus %x Length %x\n",
+					dev->name, __func__, (unsigned long)ThisRXdesc,
+					offset, rx_status, rx_extstatus, length);
+			tmp = (length > 96) ? 96 : length;
+			for (i = 0; i < tmp; i++) {
+				pchar += sprintf(pchar, "%02x", (int)*pdbg);
+				pdbg++;
+				if ((i&31) == 31) {
+					*pchar++ = '\n';
+				}
+				if ((i&3) == 3)
+					*pchar++ = ' ';
+			}
+			*pchar++ = '\0';
+
+			printk(KERN_DEBUG "%s %s: FDesc %#lx data @%p:\n %s\n",
+					dev->name, __func__,
+					(unsigned long)ThisRXdesc,
+					data_addr, buf);
+		}
+		if (!(rx_status & RX_CHECK_ERROR) && !c1k_emac_checksum(priv, rx_extstatus, &ip_summed)) {
+			skb = alloc_skb_header(FPP_SKB_SIZE, buff_addr, GFP_ATOMIC);
+			if (unlikely(!skb)) {
+				goto pkt_drop;
+			}
+
+			skb_reserve(skb, offset);
+
+#if defined(CONFIG_INET_IPSEC_OFFLOAD) || defined(CONFIG_INET6_IPSEC_OFFLOAD)
+			if (rx_status & RX_IPSEC_IN) {
+				struct sec_path *sp;
+				struct xfrm_state *x;
+				u16 *sah = (u16 *)&ThisRXdesc->pad;
+				int i = 0;
+
+				sp = secpath_dup(skb->sp);
+
+				if (!sp) {
+					kfree_skb(skb);
+					goto pkt_drop;
+				}
+
+				skb->sp = sp;
+				while ((*sah)) {
+					if ((i > 3) || ((x = xfrm_state_lookup_byhandle(dev_net(dev), *sah++)) == NULL)) {
+						kfree_skb(skb);
+						goto pkt_drop;
+					}
+
+					sp->xvec[i++] = x;
+					if (!x->curlft.use_time)
+						x->curlft.use_time = get_seconds();
+				}
+
+				sp->len = i;
+			}
+#endif
+
+			/* Packet will be processed */
+			skb->dev = dev;
+			skb_put(skb, length);
+			skb->protocol = eth_type_trans(skb, dev);
+
+			skb->ip_summed = ip_summed;
+
+			netif_receive_skb(skb);
+			priv->stats.rx_packets++;
+			priv->stats.rx_bytes += length;
+			dev->last_rx = jiffies;
+
+			(*work_done)++;
+		} else {
+pkt_drop:
+			kfree(buff_addr);
+			priv->stats.rx_errors++;
+		}
+
+		inc_rx_idx(rtc);
+		{
+			if (priv->fast_path_enabled)
+				offset = (rx_extstatus & RX_STA_L3OFF_MASK) >> RX_STA_L3OFF_POS;
+			else
+				offset = PKT_HEADROOM;
+
+			data = kmalloc(FPP_SKB_SIZE, GFP_ATOMIC);
+			if (likely(data)) {
+				/* preserve stagger offset */
+				data += ((rx_data - offset) & (FPP_SKB_SIZE - 1));
+				__dma_single_cpu_to_dev(data, PKT_BUF_SZ + PKT_HEADROOM, DMA_FROM_DEVICE);
+				ThisRXdesc->rx_data = virt_to_phys(data) + PKT_HEADROOM;
+
+				/* unlock descriptor */
+				*(volatile u32 *)&ThisRXdesc->rx_extstatus = 0;
+				if (netif_msg_intr(priv))
+					printk(KERN_INFO "%s %s refill poolArx %p offset %x/%x\n",
+							dev->name, __func__,
+							data, offset,
+							(rx_data - offset) & (FPP_SKB_SIZE - 1));
+			} else {
+				if (netif_msg_rx_err(priv))
+					printk(KERN_ERR "%s %s: low on mem\n", dev->name, __func__);
+
+				ret = -EAGAIN;
+				goto done_exit;
+			}
+			inc_rx_idx(rtf);
+		}
+	}
+
+done_exit:
+	priv->RxtocleanIndex = rtc;
+	priv->RxtofillIndex = rtf;
+	return ret;
+}
+
+static int c1k_eth_poll(struct napi_struct *napi, int budget)
+{
+	struct eth_c1k_priv *priv = container_of(napi, struct eth_c1k_priv, napi);
+	struct net_device *dev = priv->dev;
+	unsigned int work_done = 0;
+	int rc;
+
+	c1k_eth_rx_packet(dev, &work_done, budget);
+	rc = c1k_eth_rx_refill(dev);
+
+	if (netif_msg_intr(priv))
+		printk(KERN_DEBUG "%s %s: work_done %d\n", dev->name, __func__, work_done);
+
+	/* If no Rx receive nor cleanup work was done, exit polling mode. */
+	if (((work_done < budget) && !rc) || !netif_running(dev)) {
+		napi_complete(napi);
+
+		if (netif_msg_intr(priv))
+			printk(KERN_DEBUG "%s %s: re-enable irqs\n", dev->name, __func__);
+
+		if (!priv->fast_path_enabled)
+			/* clear batch counter and timer interrupt status */
+			writel(0x3, priv->baseaddr + GEM_ADM_BLOCK + ADM_BATCHINTRSTAT);
+
+		/* re-enable irq for this port(exit polling) */
+		comcerto_irq_ack(dev->irq);
+		if (irq_to_desc(dev->irq)->depth)
+			enable_irq(dev->irq);
+	}
+
+	return work_done;
+}
+
+irqreturn_t c1k_eth_rx_interrupt(int irq, void *dev_id)
+{
+	struct net_device *dev = (struct net_device *)dev_id;
+	struct eth_c1k_priv *priv = netdev_priv(dev);
+
+	if (netif_msg_intr(priv))
+		printk(KERN_DEBUG "%s %s: irq %d\n", dev->name, __func__, irq);
+
+	if (napi_schedule_prep(&priv->napi)) {
+		if (netif_msg_intr(priv))
+			printk(KERN_DEBUG "%s %s: schedule poll\n", dev->name, __func__);
+
+		disable_irq_nosync(irq);
+		__napi_schedule(&priv->napi);
+	} else if (netif_running(dev)) {
+		printk(KERN_ERR "%s %s: bug! interrupt while in poll\n", dev->name, __func__);
+
+		disable_irq_nosync(irq);
+	} else {
+		printk(KERN_ERR "%s %s: interrupt while stopped, disabling irq\n", dev->name, __func__);
+		disable_irq_nosync(irq);
+	}
+
+	return IRQ_HANDLED;
+}
+
+static const struct net_device_ops c1k_netdev_ops = {
+	.ndo_open = c1k_eth_open,
+	.ndo_stop = c1k_eth_close,
+	.ndo_start_xmit = c1k_eth_send_packet,
+	.ndo_get_stats = c1k_eth_get_stats,
+	.ndo_change_mtu = c1k_eth_change_mtu,
+	.ndo_set_mac_address = c1k_set_mac_address,
+	.ndo_set_multicast_list = c1k_eth_set_multi,
+	.ndo_validate_addr = eth_validate_addr,
+};
+
+static int c1k_eth_probe(struct platform_device *pdev)
+{
+	struct net_device *dev = NULL;
+	struct eth_c1k_priv *priv = NULL;
+	GEM_DEVICE *gemdev;
+	struct comcerto_eth_platform_data *einfo;
+	struct resource *r;
+	int idx;
+	int err;
+	dma_addr_t desc_addr;
+	dma_addr_t desc_addr_physbase;
+
+	printk(KERN_INFO "%s: gemac %d\n", __func__, pdev->id);
+
+	einfo = (struct comcerto_eth_platform_data *) pdev->dev.platform_data;
+	if (!einfo) {
+		printk(KERN_ERR "%s: gemac %d missing additional platform data\n", __func__, pdev->id);
+		err = -ENODEV;
+		goto err0;
+	}
+
+	/* Create an ethernet device instance */
+	dev = alloc_etherdev(sizeof (*priv));
+	if (!dev) {
+		printk(KERN_ERR "%s: gemac %d device allocation failed\n", __func__, pdev->id);
+		err = -ENOMEM;
+		goto err0;
+	}
+
+	priv = netdev_priv(dev);
+	gemdev = &priv->gemdev;
+	priv->dev = dev;
+	priv->id = pdev->id;
+
+	/* Set the info in the priv to the current info */
+	priv->einfo = einfo;
+
+	/* fill out IRQ fields */
+	priv->phys_rx_coal_int = platform_get_irq_byname(pdev, "rx");
+	priv->virt_rx_coal_int = platform_get_irq_byname(pdev, "fpp_rx");
+	priv->virt_tx_int = platform_get_irq_byname(pdev, "fpp_tx");
+	priv->gem_int = platform_get_irq_byname(pdev, "gemac");
+
+	if (priv->phys_rx_coal_int < 0 || priv->virt_rx_coal_int < 0 || priv->virt_tx_int < 0 || priv->gem_int < 0) {
+		printk(KERN_ERR "%s: gemac %d missing resource information\n", __func__, pdev->id);
+		err = -EINVAL;
+		goto err1;
+	}
+
+	/* get a pointer to the gemac register  */
+	r = platform_get_resource_byname(pdev, IORESOURCE_MEM, "gemac");
+	if (!r) {
+		printk(KERN_ERR "%s: gemac %d missing resource information\n", __func__, pdev->id);
+		err = -EINVAL;
+		goto err1;
+	}
+
+#if defined(USE_IOREMAP)
+	priv->baseaddr = ioremap(r->start, r->end - r->start + 1);
+#else
+	priv->baseaddr = (void *)APB_VADDR(r->start);
+#endif
+	/* Kernel may need to access registers before device is fully opened */
+	gemdev->gemac_baseaddr = priv->baseaddr;
+	gemdev->registers = priv->baseaddr + GEM_IP;
+
+	/* get a pointer to the internal memory (if any) used for descriptors*/
+	r = platform_get_resource_byname(pdev, IORESOURCE_MEM, "aram");
+	if (!r) {
+		printk(KERN_ERR "%s: gemac %d missing resource information\n", __func__, pdev->id);
+		err = -EINVAL;
+		goto err2;
+	}
+
+#if defined(USE_IOREMAP)
+	priv->ARAM_baseaddr_v = ioremap(r->start, r->end - r->start + 1);
+#else
+	priv->ARAM_baseaddr_v = aram_to_virt(r->start);
+#endif
+
+	/* use DDR for descriptors for "slow path" */
+	desc_addr = (dma_addr_t)dma_alloc_coherent(NULL,
+			SLOW_PATH_DESC_NT * sizeof(struct tRXdesc) + \
+			SLOW_PATH_DESC_NT * (sizeof(struct tTXdesc) + sizeof(struct tTXextdesc)),
+			&desc_addr_physbase,
+			GFP_KERNEL);
+	if (!desc_addr) {
+		if (netif_msg_ifup(priv))
+			printk(KERN_ERR "%s: Could not allocate buffer descriptors!\n",
+					dev->name);
+
+		err = -ENOMEM;
+		goto err3;
+	}
+
+	priv->expt_baseaddr_v = (void *)desc_addr;
+	priv->expt_baseaddr_p = desc_addr_physbase;
+
+	/* get a pointer to the internal memory used for
+	 * virtual path Shared Memory Interface (virtual registers)*/
+	r = platform_get_resource_byname(pdev, IORESOURCE_MEM, "fpp_smi");
+	if (!r) {
+		printk(KERN_ERR "%s: gemac %d missing resource information\n", __func__, pdev->id);
+		err = -EINVAL;
+		goto err4;
+	}
+
+#if defined(USE_IOREMAP)
+	priv->fpp_smi_baseaddr = ioremap(r->start, r->end - r->start + 1);
+#else
+	priv->fpp_smi_baseaddr = aram_to_virt(r->start);
+#endif
+
+	spin_lock_init(&priv->txlock);
+
+	platform_set_drvdata(pdev, dev);
+
+	/* Copy the station address into the dev structure, */
+	memcpy(dev->dev_addr, einfo->mac_addr, MAC_ADDR_LEN);
+
+	err = dev_alloc_name(dev, einfo->name);
+	if (err < 0) {
+		printk(KERN_ERR "%s: cannot allocate net device name %s, aborting.\n", __func__, einfo->name);
+		err = -EINVAL;
+		goto err5;
+	}
+
+	SET_NETDEV_DEV(dev, &pdev->dev);
+
+	netif_napi_add(dev, &priv->napi, c1k_eth_poll, COMCERTO_DEV_WEIGHT);
+	dev->mtu = 1500;
+	dev->features |= NETIF_F_IP_CSUM;
+	dev->netdev_ops = &c1k_netdev_ops;
+	dev->ethtool_ops = &c1k_ethtool_ops;
+
+	priv->flags = RX_CSUM_OFFLOAD_ENABLED | TX_CSUM_OFFLOAD_ENABLED;
+
+	/* Enable basic messages by default */
+	priv->msg_enable = NETIF_MSG_IFUP | NETIF_MSG_IFDOWN | NETIF_MSG_LINK | NETIF_MSG_PROBE;
+
+	err = register_netdev(dev);
+	if (err) {
+		printk(KERN_ERR "%s: cannot register net device, aborting.\n", dev->name);
+		goto err5;
+	}
+
+	priv->default_priority = DEFAULT_PRIORITY;
+
+	/* Default Admittance block configuration */
+	priv->RxRingSize = DEFAULT_RX_DESC_NT;
+
+	/* account for average queue depth lag relative to the instant queue depth
+	 * this is, in the worst case, given by the average window weigth, set to 8 bellow
+	 * keep 6 entries for reserved traffic */
+	/* Most of these are overwriten by qosapp/FPP later on */
+	writel(priv->RxRingSize - 2, priv->baseaddr + GEM_ADM_BLOCK + ADM_QFULLTHR);
+	writel((priv->RxRingSize - 8 - 6) << 8, priv->baseaddr + GEM_ADM_BLOCK + ADM_QDROPMAXTHR);
+	writel((priv->RxRingSize - 8 - 6 - 6) << 8, priv->baseaddr + GEM_ADM_BLOCK + ADM_QDROPMINTHR);
+
+	/* Print out the device info */
+	printk(KERN_INFO DEVICE_NAME " ", dev->name);
+	for (idx = 0; idx < 6; idx++)
+		printk("%2.2x%c", dev->dev_addr[idx], idx == 5 ? ' ' : ':');
+	printk("\n");
+
+	return 0;
+
+err5:
+	platform_set_drvdata(pdev, NULL);
+
+#if defined (USE_IOREMAP)
+	iounmap(priv->fpp_smi_baseaddr);
+#endif
+
+err4:
+	dma_free_coherent(NULL, DEFAULT_RX_DESC_NT * sizeof(struct tRXdesc) + \
+			DEFAULT_TX_DESC_NT * (sizeof(struct tTXdesc) + sizeof(struct tTXextdesc)),
+			priv->expt_baseaddr_v, priv->expt_baseaddr_p);
+
+err3:
+
+#if defined (USE_IOREMAP)
+	iounmap(priv->ARAM_baseaddr_v);
+#endif
+err2:
+#if defined (USE_IOREMAP)
+	iounmap(priv->baseaddr);
+#endif
+err1:
+	free_netdev(dev);
+err0:
+	return err;
+}
+
+static void c1k_adjust_link(struct net_device *dev)
+{
+	struct eth_c1k_priv *priv = netdev_priv(dev);
+	unsigned long flags;
+	struct phy_device *phydev = priv->phydev;
+	int new_state = 0;
+
+	if (netif_msg_drv(priv) && net_ratelimit())
+		printk(KERN_DEBUG "%s c1k_adjust_link\n", dev->name);
+
+	spin_lock_irqsave(&priv->txlock, flags);
+	if (phydev->link) {
+		/* Now we make sure that we can be in full duplex mode.
+		 * If not, we operate in half-duplex mode. */
+		if (phydev->duplex != priv->oldduplex) {
+			new_state = 1;
+			c1k_gemac_setduplex(dev, phydev->duplex);
+			priv->oldduplex = phydev->duplex;
+
+		}
+
+		if (phydev->speed != priv->oldspeed) {
+			new_state = 1;
+			c1k_gemac_setspeed(dev, phydev->speed);
+			priv->oldspeed = phydev->speed;
+		}
+
+		if (!priv->oldlink) {
+			new_state = 1;
+			priv->oldlink = 1;
+		}
+
+	} else if (priv->oldlink) {
+		new_state = 1;
+		priv->oldlink = 0;
+		priv->oldspeed = 0;
+		priv->oldduplex = -1;
+	}
+
+	if (new_state && netif_msg_link(priv))
+		phy_print_status(phydev);
+
+	spin_unlock_irqrestore(&priv->txlock, flags);
+}
+
+static int c1k_eth_remove(struct platform_device *pdev)
+{
+	struct net_device *dev = platform_get_drvdata(pdev);
+#if defined (USE_IOREMAP)
+	struct eth_c1k_priv *priv = netdev_priv(dev);
+#endif
+	unregister_netdevice(dev);
+
+	platform_set_drvdata(pdev, NULL);
+
+#if defined (USE_IOREMAP)
+	iounmap(priv->ARAM_expt_baseaddr_v);
+	iounmap(priv->ARAM_baseaddr_v);
+	iounmap(priv->baseaddr);
+#endif
+	free_netdev(dev);
+
+	return 0;
+}
+
+#ifdef CONFIG_PM
+static int c1k_eth_suspend(struct platform_device *pdev, pm_message_t state)
+{
+	struct net_device *dev = platform_get_drvdata(pdev);
+
+	c1k_eth_close(dev);
+
+	return 0;
+}
+
+static int c1k_eth_resume(struct platform_device *pdev)
+{
+	struct net_device *dev = platform_get_drvdata(pdev);
+
+	c1k_eth_open(dev);
+
+	return 0;
+}
+#endif
+
+/* Structure for a device driver */
+static struct platform_driver c1k_eth_driver = {
+	.probe = c1k_eth_probe,
+	.remove = c1k_eth_remove,
+#ifdef CONFIG_PM
+	.suspend = c1k_eth_suspend,
+	.resume = c1k_eth_resume,
+#endif
+	.driver	= {
+		.name = "c1000-eth",
+	},
+};
+
+/*
+ *	This is a kernel command line parameter. The format is as following:
+ *	hwaddress=<interface name>,<mac address>,<interface name>,<mac address> ....
+ *	This parameter is mainly use when mounting the root filesytem over NFS.
+ *	For all the other cases, the MAC address will be given through the ifconfig application
+ *	i.e: ifconfig <interface name> hw ether <mac address>
+ */
+static int __init hwaddress_setup(char *str)
+{
+	int index = 0;
+	int i = 0;
+	unsigned char *pchar = (unsigned char *) str;
+
+	for (i = 0; i < 2; i++) {
+		if (strncmp(pchar, ETH0, 4) == 0) {
+			pchar = strpbrk(pchar, ",");
+			++pchar;
+			index = 0;
+
+			while (pchar && (index < ETH_ALEN)) {
+				if (pchar) {
+					unsigned char tmp = simple_strtol(pchar, NULL, 16);
+					comcerto_gem0_pdata.mac_addr[index++] = (unsigned char)tmp;
+					pchar += 3;
+				}
+			}
+		} else if (strncmp(pchar, WAN0, 4) == 0) {
+			pchar = strpbrk(pchar, ",");
+			++pchar;
+			index = 0;
+
+			while (pchar && (index < ETH_ALEN)) {
+				if (pchar) {
+					unsigned char tmp = simple_strtol(pchar, NULL, 16);
+					comcerto_gem1_pdata.mac_addr[index++] = (unsigned char)tmp;
+					pchar += 3;
+				}
+			}
+		}
+	}
+
+	return 1;
+}
+
+__setup("hwaddress=", hwaddress_setup);
+
+static int __init c1k_eth_init(void)
+{
+	return platform_driver_register(&c1k_eth_driver);
+}
+
+static void __exit c1k_eth_exit(void)
+{
+	platform_driver_unregister(&c1k_eth_driver);
+}
+
+module_init(c1k_eth_init);
+module_exit(c1k_eth_exit);
diff --git a/drivers/net/comcerto/c1000_eth.h b/drivers/net/comcerto/c1000_eth.h
new file mode 100644
index 0000000..56f992b
--- /dev/null
+++ b/drivers/net/comcerto/c1000_eth.h
@@ -0,0 +1,226 @@
+/*
+  *  linux/drivers/net/comcerto/c1000_eth.h
+  *
+  *  Copyright (C) 2006 Mindspeed Technologies, Inc.
+  *
+  * This program is free software; you can redistribute it and/or modify
+  * it under the terms of the GNU General Public License as published by
+  * the Free Software Foundation; either version 2 of the License, or
+  * (at your option) any later version.
+  *
+  * This program is distributed in the hope that it will be useful,
+  * but WITHOUT ANY WARRANTY; without even the implied warranty of
+  * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+  * GNU General Public License for more details.
+  *
+  * You should have received a copy of the GNU General Public License
+  * along with this program; if not, write to the Free Software
+  * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
+  */
+#ifndef _C1000_ETH_H
+#define _C1000_ETH_H
+
+#include <linux/netdevice.h>	/* struct device, and other headers */
+#include <linux/mii.h>
+#include <linux/ethtool.h>
+#include "comcerto_gemac.h"
+
+
+#define DEVICE_NAME "%s: C1000 Ethernet Controller"
+#define DRV_NAME "c1000-geth"
+#define COMCERTO_INFOSTR_LEN	32
+#define COMCERTO_DEV_WEIGHT 16
+
+#define DEFAULT_RX_DESC_NT	512
+#define DEFAULT_TX_DESC_NT	128
+#define SLOW_PATH_DESC_NT	128
+
+#define FPP_SKB_SIZE		SZ_4K
+#define FPP_TX_SKB_HEADROOM	128
+#define FPP_TX_SKB_TAILROOM	128
+
+#define DEFAULT_PRIORITY	7
+
+#define ETH0		"eth0"
+#define WAN0		"eth2"
+
+#define FAST_PATH_DISABLE	0
+#define FAST_PATH_ENABLE	1
+#define DEFAULT_FAST_PATH_STATE	FAST_PATH_DISABLE
+
+#define DEFAULT_RX_COAL_TIME	500 /* us */
+#define DEFAULT_RX_COAL_PKTS	8
+
+#undef USE_IOREMAP
+
+struct tRXdesc {
+	volatile u32 rx_data;
+	volatile u32 rx_status;
+	volatile u32 rx_extstatus;
+	volatile u32 pad;
+};
+
+/* gemac rx controls
+ * Wrap flag - marks last descriptor in a queue when set
+ * goes to the status word (offset 0x4)
+ */
+#define GEMRX_WRAP 		(1<<28)
+/* Ownership flag - when 0 gem can use the descriptor
+ * goes to the extended status word (offset 0x8)
+ */
+#define GEMRX_OWN		(1<<15)
+
+/* gemac rx status */
+#define RX_STA_BCAST		(1UL<<31)
+#define RX_STA_MCAST		(1<<30)
+#define RX_STA_UM		(1<<29)
+#define RX_MAC_MATCH_FLAG 	(0x4<<25)
+#define RX_MAC_MATCH_NUM_MASK	(0x3<<25)
+#define RX_MAC_MATCH_POS 	25
+#define RX_INT 			(1<<24)
+#define RX_IPSEC_OUT		(1<<23)
+#define RX_IPSEC_IN	 	(1<<22)
+#define RX_STA_VLAN 		(1<<21)
+#define RX_STA_VLAN_802p 	(1<<20)
+#define RX_STA_VLAN_PRI_MASK	(7<<17)
+#define RX_STA_VLAN_PRI_POS	17
+#define RX_STA_VLAN_CFI		(1<<16)
+#define RX_STA_SOF		(1<<15)
+#define RX_STA_EOF		(1<<14)
+#define RX_STA_PACKET		(RX_STA_SOF|RX_STA_EOF)
+#define RX_STA_CRCERR		(1<<13)
+#define RX_STA_LEN_MASK		0xfff
+#define RX_STA_LEN_POS		0
+#define	RX_CHECK_ERROR		RX_STA_CRCERR
+/* gemac rx extended status(word2) */
+#define RX_STA_L4OFF_MASK	(0xff<<24)
+#define RX_STA_L4OFF_POS	24
+#define RX_STA_L3OFF_MASK	(0xff<<16)
+#define RX_STA_L3OFF_POS	16
+
+#define RX_STA_L3_CKSUM		(1<<11)
+#define RX_STA_L3_GOOD		(1<<12)
+#define RX_STA_L4_CKSUM		(1<<13)
+#define RX_STA_L4_GOOD		(1<<14)
+#define RX_STA_CKSUM_MASK	(RX_STA_L3_CKSUM | RX_STA_L3_GOOD | RX_STA_L4_CKSUM | RX_STA_L4_GOOD)
+
+#define RX_STA_TCP		(1<<9)
+#define RX_STA_UDP		(1<<8)
+#define RX_STA_IPV6		(1<<7)
+#define RX_STA_IPV4		(1<<6)
+#define RX_STA_PPPOE		(1<<5)
+#define RX_STA_WILLHANDLE	(RX_STA_IPV6 | RX_STA_IPV4)
+#define RX_STA_QinQ		(1<<4)
+#define RX_STA_TYPEID_MATCH_FLAG (0x8 << 0)
+#define RX_STA_TYPEID		(0x7 << 0)
+#define RX_STA_TYPEID_POS	0
+
+
+struct tTXdesc {
+	volatile u32 txdata;
+	union {
+		volatile u32 txctl;
+		volatile u32 txstatus;
+
+	} ;
+};
+
+
+#define GEMTX_USED_MASK 	(1UL<<31)
+#define GEMTX_WRAP		(1UL<<30)
+#define GEMTX_IE		(1UL<<29)
+#define GEMTX_OFFSET_MASK 	0xff
+#define GEMTX_OFFSET_SHIFT 	16
+#define GEMTX_LENGTH_MASK	0x1fff
+#define GEMTX_LENGTH_SHIFT	0
+#define GEMTX_LENGTH_MAX	0x1fff
+#define	GEMTX_L4_CSUM		(1UL<<26)
+#define	GEMTX_L3_CSUM		(1UL<<25)
+#define	GEMTX_FCS		(1UL<<24)
+#define	GEMTX_LAST		(1UL<<15)
+#define GEMTX_POOLB		(1UL<<14)
+#define	GEMTX_BUFRET		(1UL<<13)
+
+/* Exception path bits re-definition */
+/* bits 27/28/29 are used to sepcify QoS bits between CSP and FPP */
+#define GEMTX_EXPT_QOS_SHIFT	25
+#define GEMTX_EXPT_QOS_MASK	(31UL << GEMTX_EXPT_QOS_SHIFT)
+#define GEMTX_EXPT_L34_CSUM     GEMTX_FCS   /* the FCS bit is ignored by FPP expt path */
+/* bits 13/14 are used to identify IPsec packets -> usage of ext descriptors array */
+#define GEMTX_EXPT_IPSEC_IN	(1UL<<13)
+#define GEMTX_EXPT_IPSEC_OUT	(1UL<<14)
+
+
+/* Expt extented desc (IPsec) */
+struct tTXextdesc {
+	volatile u16 SAhandle[2];
+	u32 RSVD;
+};
+
+
+#define FPP_SMI_CTRL	0x00
+#define FPP_SMI_RXBASE	0x04
+#define FPP_SMI_TXBASE	0x08
+#define FPP_SMI_TXEXTBASE	0x0C
+
+#define RX_CSUM_OFFLOAD_ENABLED		(1 << 0)
+#define TX_CSUM_OFFLOAD_ENABLED		(1 << 1)
+
+struct eth_c1k_priv {
+	struct tRXdesc *RxBase;
+	u32 RxRingSize;
+	u32 RxtocleanIndex;
+	u32 RxtofillIndex;
+	u32 default_priority;
+	u8 fast_path_enabled;
+
+	struct c1k_fbpool *poolA;
+	struct c1k_fbpool *poolB;
+
+	spinlock_t txlock;
+	struct tTXdesc *TxBase;
+	struct tTXextdesc *TxextBase;
+
+	u32 TxRingSize;
+	u32 Txtosend;
+	u32 Txdone;
+	int Txavail;
+
+	struct napi_struct   napi;
+
+	/* Network Statistics */
+	struct net_device_stats stats;
+	struct net_device *dev;
+	int id;
+
+	struct comcerto_eth_platform_data *einfo;
+
+	/* platform resources */
+	int		rx_coal_count;
+	int		rx_coal_time;	/* in us */
+	int		gem_int;
+	int		phys_rx_coal_int;
+	int		virt_rx_coal_int;
+	int		virt_tx_int;
+
+	void			*baseaddr;
+	/* virtual registers in NCNB ARAM */
+	void			*fpp_smi_baseaddr;
+	void			*ARAM_baseaddr_v;
+	void 			*expt_baseaddr_v;
+	dma_addr_t		expt_baseaddr_p;
+
+	GEM_DEVICE		gemdev; /* used for gem AL.c */
+
+	/* PHY stuff */
+	struct phy_device *phydev;
+	int oldspeed;
+	int oldduplex;
+	int oldlink;
+
+	u32 msg_enable;
+	u32 flags;
+};
+
+
+#endif /* _C1000_ETH_H */
-- 
1.6.5.2

