From f02a999c107a8fa23f16029ce2248263007b0549 Mon Sep 17 00:00:00 2001
From: Guoqing Jiang <Guoqing.Jiang@windriver.com>
Date: Thu, 5 Sep 2013 11:05:39 +0800
Subject: [PATCH 20/20] net: keystone: add ethernet support for keystone
 platform

Keystone introduced a new ethernet subsystem for keystone platform since it owns
a different IP which doesn't used for other TI chips, and this IP is named as
network coprocessor (NETCP).

NETCP is a hardware accelerator that processes data packets with a main focus on
processing Ethernet packets. NETCP has two gigabit Ethernet (GbE) modules to send
and receive packets from an IEEE 802.3 compliant network. The NETCP also includes
a packet accelerator (PA) to perform packet classification operations such as header
matching, and packet modification operations such as checksum generation. The NETCP
also provides a security accelerator (SA) to encrypt and decrypt data packets.
The NETCP can receive packets from the Ethernet modules, or packets can be delivered
to the NETCP through packet DMA from the DSP or another supported peripheral.

Refer to the following commits from arago tree
git://arago-project.org/git/projects/linux-keystone.git releases/03.00.00.11/master

Signed-off-by: Guoqing Jiang <Guoqing.Jiang@windriver.com>
---
 .../devicetree/bindings/net/keystone-net.txt       |   63 +
 drivers/net/ethernet/ti/Kconfig                    |   65 +-
 drivers/net/ethernet/ti/Makefile                   |   12 +
 drivers/net/ethernet/ti/cpmac.c                    |   13 +-
 drivers/net/ethernet/ti/cpsw.c                     |  825 ++-----
 drivers/net/ethernet/ti/cpsw_ale.c                 |  900 ++++++-
 drivers/net/ethernet/ti/cpsw_ale.h                 |   71 +-
 drivers/net/ethernet/ti/cpts.c                     |    2 +-
 drivers/net/ethernet/ti/davinci_cpdma.c            |  113 +-
 drivers/net/ethernet/ti/davinci_cpdma.h            |   12 +-
 drivers/net/ethernet/ti/davinci_emac.c             |   80 +-
 drivers/net/ethernet/ti/davinci_mdio.c             |   32 +-
 drivers/net/ethernet/ti/keystone_ethss.c           | 2377 ++++++++++++++++++
 drivers/net/ethernet/ti/keystone_net.h             |  224 ++
 drivers/net/ethernet/ti/keystone_net_core.c        | 2052 ++++++++++++++++
 drivers/net/ethernet/ti/keystone_net_sa.c          |  254 ++
 drivers/net/ethernet/ti/keystone_pa.c              | 2473 +++++++++++++++++++
 drivers/net/ethernet/ti/keystone_pa.h              |  741 ++++++
 drivers/net/ethernet/ti/keystone_pasahost.h        |  390 +++
 drivers/net/ethernet/ti/keystone_qos.c             |  300 +++
 drivers/net/ethernet/ti/keystone_sgmii.c           |  307 +++
 drivers/net/ethernet/ti/keystone_xgemdio.c         |  525 ++++
 drivers/net/ethernet/ti/keystone_xgepcsr.c         |  268 +++
 drivers/net/ethernet/ti/keystone_xgess.c           | 2524 ++++++++++++++++++++
 drivers/net/ethernet/ti/tlan.c                     |    5 +-
 25 files changed, 13700 insertions(+), 928 deletions(-)
 create mode 100644 Documentation/devicetree/bindings/net/keystone-net.txt
 create mode 100644 drivers/net/ethernet/ti/keystone_ethss.c
 create mode 100644 drivers/net/ethernet/ti/keystone_net.h
 create mode 100644 drivers/net/ethernet/ti/keystone_net_core.c
 create mode 100644 drivers/net/ethernet/ti/keystone_net_sa.c
 create mode 100644 drivers/net/ethernet/ti/keystone_pa.c
 create mode 100644 drivers/net/ethernet/ti/keystone_pa.h
 create mode 100644 drivers/net/ethernet/ti/keystone_pasahost.h
 create mode 100644 drivers/net/ethernet/ti/keystone_qos.c
 create mode 100644 drivers/net/ethernet/ti/keystone_sgmii.c
 create mode 100644 drivers/net/ethernet/ti/keystone_xgemdio.c
 create mode 100644 drivers/net/ethernet/ti/keystone_xgepcsr.c
 create mode 100644 drivers/net/ethernet/ti/keystone_xgess.c

diff --git a/Documentation/devicetree/bindings/net/keystone-net.txt b/Documentation/devicetree/bindings/net/keystone-net.txt
new file mode 100644
index 0000000..ac23f54
--- /dev/null
+++ b/Documentation/devicetree/bindings/net/keystone-net.txt
@@ -0,0 +1,63 @@
+This document describes the device tree bindings associated with the
+keystone network driver support.
+
+reg:		the register start address and the size that will be
+		used by the driver. The second index is for the efuse
+		mac address
+tx-channel:	the packet dma channel name associated with tx
+tx-queue-depth: the number of descriptors to be used
+rx-channel:	the packet dma channel name associated with rx
+rx-queue-depth: there can be at present a maximum of 4 queues per packet
+		dma channel. We can specify the number of descriptors for
+		each queue
+rx-buffer-size: For each receive queue, we can specify a buffer size.
+efuse-mac:	If this is 1, then the mac address is obtained from the
+		device efuse mac address register
+local-mac-address:the driver is designed to use the of_get_mac_address api
+		  only if efuse-mac is 0. When efuse-mac is 0, the mac address
+		  is obtained from local-mac-address.
+link-interface: for each slave port that is intended to be used, a
+                link-interface binding has to be used appropriatley
+		initialized. The different options are as follows
+		- for mac mac auto negotiate mode use -> 0
+		- for mac phy mode use -> 1
+		- for mac mac forced mode use -> 2
+		- for mac fiber mode use -> 3
+		- for mac phy mode with no mdio use -> 4
+
+The keystone network device can open other modules such as cpsw or pa.
+
+Below we provide an example.
+
+netcp: netcp@2090000 {
+			reg = <0x2090000 0xf00
+				0x2620110 0x8>;
+			compatible = "ti,keystone-netcp";
+
+			tx-channel = "nettx";
+			tx-queue-depth = <128>;
+
+			rx-channel = "netrx";
+			rx-queue-depth = < 128 128 0 0>;
+			rx-buffer-size = <1500 4096 0 0>;
+
+			efuse-mac = <1>;
+			local-mac-address = [00 18 31 7e 3e 6e];
+
+			slaves {
+					slave0 {
+						label		= "slave0";
+						link-interface	= <1>;
+					};
+					slave1 {
+						label		= "slave1";
+						link-interface	= <1>;
+					};
+
+			};
+
+			pa: pa@2000000 {
+				label = "keystone-pa";
+			};
+		};
+
diff --git a/drivers/net/ethernet/ti/Kconfig b/drivers/net/ethernet/ti/Kconfig
index de71b1e..1f2c351 100644
--- a/drivers/net/ethernet/ti/Kconfig
+++ b/drivers/net/ethernet/ti/Kconfig
@@ -5,7 +5,7 @@
 config NET_VENDOR_TI
 	bool "Texas Instruments (TI) devices"
 	default y
-	depends on PCI || EISA || AR7 || (ARM && (ARCH_DAVINCI || ARCH_OMAP3 || SOC_AM33XX))
+	depends on PCI || EISA || AR7 || (ARM && (ARCH_DAVINCI || ARCH_OMAP3 || SOC_AM33XX || ARCH_KEYSTONE))
 	---help---
 	  If you have a network (Ethernet) card belonging to this class, say Y
 	  and read the Ethernet-HOWTO, available from
@@ -32,7 +32,7 @@ config TI_DAVINCI_EMAC
 
 config TI_DAVINCI_MDIO
 	tristate "TI DaVinci MDIO Support"
-	depends on ARM && ( ARCH_DAVINCI || ARCH_OMAP3 || SOC_AM33XX )
+	depends on ARM && ( ARCH_DAVINCI || ARCH_OMAP3 || SOC_AM33XX || ARCH_KEYSTONE )
 	select PHYLIB
 	---help---
 	  This driver supports TI's DaVinci MDIO module.
@@ -69,6 +69,44 @@ config TI_CPTS
 	  the CPSW Ethernet Switch. The unit can time stamp PTP UDP/IPv4
 	  and Layer 2 packets, and the driver offers a PTP Hardware Clock.
 
+config TI_KEYSTONE_NET
+	tristate "TI Keystone Ethernet Support"
+	depends on TI_KEYSTONE
+	default y if TI_KEYSTONE
+	---help---
+	  This driver supports TI's Keystone Ethernet.
+
+	  To compile this driver as a module, choose M here: the module
+	  will be called keystone_net.
+
+config TI_KEYSTONE_PA
+	tristate "TI Keystone Packet Accelerator Support"
+	depends on TI_KEYSTONE
+	default y if TI_KEYSTONE
+	---help---
+	  This driver supports TI's Keystone Packet Accelerator.
+
+	  To compile this driver as a module, choose M here: the module
+	  will be called keystone_pa.
+
+config TI_KEYSTONE_QOS
+	tristate "TI Keystone Quality of Service Support"
+	depends on TI_KEYSTONE
+	default y if TI_KEYSTONE
+	---help---
+	  This driver supports TI's Keystone Quality of Service support.
+
+	  To compile this driver as a module, choose M here: the module
+	  will be called keystone_qos.
+
+config TI_KEYSTONE_NET_SA
+	tristate "TI Keystone inflow Security Accelerator mode Support"
+	depends on TI_KEYSTONE
+	default y if TI_KEYSTONE
+	---help---
+	  This driver supports TI's Keystone inflow Security Accelerator
+          mode.
+
 config TLAN
 	tristate "TI ThunderLAN support"
 	depends on (PCI || EISA)
@@ -88,10 +126,29 @@ config TLAN
 	  Please email feedback to <torben.mathiasen@compaq.com>.
 
 config CPMAC
-	tristate "TI AR7 CPMAC Ethernet support"
-	depends on AR7
+	tristate "TI AR7 CPMAC Ethernet support (EXPERIMENTAL)"
+	depends on EXPERIMENTAL && AR7
 	select PHYLIB
 	---help---
 	  TI AR7 CPMAC Ethernet support
 
+config TI_KEYSTONE_XGE
+	tristate "TI Keystone 10Gig Ethernet Support"
+	depends on TI_KEYSTONE && TI_KEYSTONE_NET
+	---help---
+	  This driver supports TI's Keystone 10Gig Ethernet.
+
+	  To compile this driver as a module, choose M here: the module
+	  will be called keystone_xge.
+
+config TI_KEYSTONE_XGE_MDIO
+	tristate "TI Keystone 10GE MDIO Support"
+	depends on ARM && ARCH_KEYSTONE
+	select PHYLIB
+	---help---
+	  This driver supports TI's Keystone 10GE MDIO module.
+
+	  To compile this driver as a module, choose M here: the module
+	  will be called keystone_xgemdio.  This is recommended.
+
 endif # NET_VENDOR_TI
diff --git a/drivers/net/ethernet/ti/Makefile b/drivers/net/ethernet/ti/Makefile
index c65148e..de33d9f 100644
--- a/drivers/net/ethernet/ti/Makefile
+++ b/drivers/net/ethernet/ti/Makefile
@@ -9,3 +9,15 @@ obj-$(CONFIG_TI_DAVINCI_MDIO) += davinci_mdio.o
 obj-$(CONFIG_TI_DAVINCI_CPDMA) += davinci_cpdma.o
 obj-$(CONFIG_TI_CPSW) += ti_cpsw.o
 ti_cpsw-y := cpsw_ale.o cpsw.o cpts.o
+obj-$(CONFIG_TI_KEYSTONE_NET) += keystone_net.o
+obj-$(CONFIG_TI_KEYSTONE_PA) += keystone_pa.o
+obj-$(CONFIG_TI_KEYSTONE_QOS) += keystone_qos.o
+obj-$(CONFIG_TI_KEYSTONE_NET_SA) += keystone_net_sa.o
+keystone_net-y += cpsw_ale.o		\
+		  keystone_ethss.o	\
+		  keystone_sgmii.o	\
+		  keystone_net_core.o
+obj-$(CONFIG_TI_KEYSTONE_XGE) += keystone_xge.o
+obj-$(CONFIG_TI_KEYSTONE_XGE_MDIO) += keystone_xgemdio.o
+keystone_xge-y += keystone_xgess.o	\
+		  keystone_xgepcsr.o
diff --git a/drivers/net/ethernet/ti/cpmac.c b/drivers/net/ethernet/ti/cpmac.c
index 31bbbca..c8130fc 100644
--- a/drivers/net/ethernet/ti/cpmac.c
+++ b/drivers/net/ethernet/ti/cpmac.c
@@ -904,9 +904,10 @@ static int cpmac_set_ringparam(struct net_device *dev,
 static void cpmac_get_drvinfo(struct net_device *dev,
 			      struct ethtool_drvinfo *info)
 {
-	strlcpy(info->driver, "cpmac", sizeof(info->driver));
-	strlcpy(info->version, CPMAC_VERSION, sizeof(info->version));
-	snprintf(info->bus_info, sizeof(info->bus_info), "%s", "cpmac");
+	strcpy(info->driver, "cpmac");
+	strcpy(info->version, CPMAC_VERSION);
+	info->fw_version[0] = '\0';
+	sprintf(info->bus_info, "%s", "cpmac");
 	info->regdump_len = 0;
 }
 
@@ -919,7 +920,7 @@ static const struct ethtool_ops cpmac_ethtool_ops = {
 	.set_ringparam = cpmac_set_ringparam,
 };
 
-static void cpmac_adjust_link(struct net_device *dev)
+static void cpmac_adjust_link(struct net_device *dev, void *context)
 {
 	struct cpmac_priv *priv = netdev_priv(dev);
 	int new_state = 0;
@@ -1172,8 +1173,8 @@ static int cpmac_probe(struct platform_device *pdev)
 	snprintf(priv->phy_name, MII_BUS_ID_SIZE, PHY_ID_FMT,
 						mdio_bus_id, phy_id);
 
-	priv->phy = phy_connect(dev, priv->phy_name, cpmac_adjust_link,
-				PHY_INTERFACE_MODE_MII);
+	priv->phy = phy_connect(dev, priv->phy_name, cpmac_adjust_link, 0,
+				PHY_INTERFACE_MODE_MII, NULL);
 
 	if (IS_ERR(priv->phy)) {
 		if (netif_msg_drv(priv))
diff --git a/drivers/net/ethernet/ti/cpsw.c b/drivers/net/ethernet/ti/cpsw.c
index d1a769f..a159ae0 100644
--- a/drivers/net/ethernet/ti/cpsw.c
+++ b/drivers/net/ethernet/ti/cpsw.c
@@ -32,7 +32,6 @@
 #include <linux/of.h>
 #include <linux/of_net.h>
 #include <linux/of_device.h>
-#include <linux/if_vlan.h>
 
 #include <linux/platform_data/cpsw.h>
 
@@ -40,6 +39,8 @@
 #include "cpts.h"
 #include "davinci_cpdma.h"
 
+#define CPSW_NON_VLAN_ADDR	-1
+
 #define CPSW_DEBUG	(NETIF_MSG_HW		| NETIF_MSG_WOL		| \
 			 NETIF_MSG_DRV		| NETIF_MSG_LINK	| \
 			 NETIF_MSG_IFUP		| NETIF_MSG_INTR	| \
@@ -119,20 +120,6 @@ do {								\
 #define TX_PRIORITY_MAPPING	0x33221100
 #define CPDMA_TX_PRIORITY_MAP	0x76543210
 
-#define CPSW_VLAN_AWARE		BIT(1)
-#define CPSW_ALE_VLAN_AWARE	1
-
-#define CPSW_FIFO_NORMAL_MODE		(0 << 15)
-#define CPSW_FIFO_DUAL_MAC_MODE		(1 << 15)
-#define CPSW_FIFO_RATE_LIMIT_MODE	(2 << 15)
-
-#define CPSW_INTPACEEN		(0x3f << 16)
-#define CPSW_INTPRESCALE_MASK	(0x7FF << 0)
-#define CPSW_CMINTMAX_CNT	63
-#define CPSW_CMINTMIN_CNT	2
-#define CPSW_CMINTMAX_INTVL	(1000 / CPSW_CMINTMIN_CNT)
-#define CPSW_CMINTMIN_INTVL	((1000 / CPSW_CMINTMAX_CNT) + 1)
-
 #define cpsw_enable_irq(priv)	\
 	do {			\
 		u32 i;		\
@@ -146,10 +133,6 @@ do {								\
 			disable_irq_nosync(priv->irqs_table[i]); \
 	} while (0);
 
-#define cpsw_slave_index(priv)				\
-		((priv->data.dual_emac) ? priv->emac_port :	\
-		priv->data.active_slave)
-
 static int debug_level;
 module_param(debug_level, int, 0);
 MODULE_PARM_DESC(debug_level, "cpsw debug level (NETIF_MSG bits)");
@@ -171,15 +154,6 @@ struct cpsw_wr_regs {
 	u32	rx_en;
 	u32	tx_en;
 	u32	misc_en;
-	u32	mem_allign1[8];
-	u32	rx_thresh_stat;
-	u32	rx_stat;
-	u32	tx_stat;
-	u32	misc_stat;
-	u32	mem_allign2[8];
-	u32	rx_imax;
-	u32	tx_imax;
-
 };
 
 struct cpsw_ss_regs {
@@ -278,7 +252,7 @@ struct cpsw_ss_regs {
 struct cpsw_host_regs {
 	u32	max_blks;
 	u32	blk_cnt;
-	u32	tx_in_ctl;
+	u32	flow_thresh;
 	u32	port_vlan;
 	u32	tx_pri_map;
 	u32	cpdma_tx_pri_map;
@@ -305,9 +279,6 @@ struct cpsw_slave {
 	u32				mac_control;
 	struct cpsw_slave_data		*data;
 	struct phy_device		*phy;
-	struct net_device		*ndev;
-	u32				port_vlan;
-	u32				open_stat;
 };
 
 static inline u32 slave_read(struct cpsw_slave *slave, u32 offset)
@@ -334,8 +305,6 @@ struct cpsw_priv {
 	struct cpsw_host_regs __iomem	*host_port_regs;
 	u32				msg_enable;
 	u32				version;
-	u32				coal_intvl;
-	u32				bus_freq_mhz;
 	struct net_device_stats		stats;
 	int				rx_packet_max;
 	int				host_port;
@@ -348,69 +317,17 @@ struct cpsw_priv {
 	/* snapshot of IRQ numbers */
 	u32 irqs_table[4];
 	u32 num_irqs;
-	bool irq_enabled;
-	struct cpts *cpts;
-	u32 emac_port;
+	struct cpts cpts;
 };
 
 #define napi_to_priv(napi)	container_of(napi, struct cpsw_priv, napi)
-#define for_each_slave(priv, func, arg...)				\
-	do {								\
-		struct cpsw_slave *slave;				\
-		int n;							\
-		if (priv->data.dual_emac)				\
-			(func)((priv)->slaves + priv->emac_port, ##arg);\
-		else							\
-			for (n = (priv)->data.slaves,			\
-					slave = (priv)->slaves;		\
-					n; n--)				\
-				(func)(slave++, ##arg);			\
-	} while (0)
-#define cpsw_get_slave_ndev(priv, __slave_no__)				\
-	(priv->slaves[__slave_no__].ndev)
-#define cpsw_get_slave_priv(priv, __slave_no__)				\
-	((priv->slaves[__slave_no__].ndev) ?				\
-		netdev_priv(priv->slaves[__slave_no__].ndev) : NULL)	\
-
-#define cpsw_dual_emac_src_port_detect(status, priv, ndev, skb)		\
-	do {								\
-		if (!priv->data.dual_emac)				\
-			break;						\
-		if (CPDMA_RX_SOURCE_PORT(status) == 1) {		\
-			ndev = cpsw_get_slave_ndev(priv, 0);		\
-			priv = netdev_priv(ndev);			\
-			skb->dev = ndev;				\
-		} else if (CPDMA_RX_SOURCE_PORT(status) == 2) {		\
-			ndev = cpsw_get_slave_ndev(priv, 1);		\
-			priv = netdev_priv(ndev);			\
-			skb->dev = ndev;				\
-		}							\
-	} while (0)
-#define cpsw_add_mcast(priv, addr)					\
-	do {								\
-		if (priv->data.dual_emac) {				\
-			struct cpsw_slave *slave = priv->slaves +	\
-						priv->emac_port;	\
-			int slave_port = cpsw_get_slave_port(priv,	\
-						slave->slave_num);	\
-			cpsw_ale_add_mcast(priv->ale, addr,		\
-				1 << slave_port | 1 << priv->host_port,	\
-				ALE_VLAN, slave->port_vlan, 0);		\
-		} else {						\
-			cpsw_ale_add_mcast(priv->ale, addr,		\
-				ALE_ALL_PORTS << priv->host_port,	\
-				0, 0, 0);				\
-		}							\
+#define for_each_slave(priv, func, arg...)			\
+	do {							\
+		int idx;					\
+		for (idx = 0; idx < (priv)->data.slaves; idx++)	\
+			(func)((priv)->slaves + idx, ##arg);	\
 	} while (0)
 
-static inline int cpsw_get_slave_port(struct cpsw_priv *priv, u32 slave_num)
-{
-	if (priv->host_port == 0)
-		return slave_num + 1;
-	else
-		return slave_num;
-}
-
 static void cpsw_ndo_set_rx_mode(struct net_device *ndev)
 {
 	struct cpsw_priv *priv = netdev_priv(ndev);
@@ -429,7 +346,8 @@ static void cpsw_ndo_set_rx_mode(struct net_device *ndev)
 
 		/* program multicast address list into ALE register */
 		netdev_for_each_mc_addr(ha, ndev) {
-			cpsw_add_mcast(priv, (u8 *)ha->addr);
+			cpsw_ale_add_mcast(priv->ale, (u8 *)ha->addr,
+				ALE_ALL_PORTS << priv->host_port, 0, 0);
 		}
 	}
 }
@@ -458,12 +376,9 @@ void cpsw_tx_handler(void *token, int len, int status)
 	struct net_device	*ndev = skb->dev;
 	struct cpsw_priv	*priv = netdev_priv(ndev);
 
-	/* Check whether the queue is stopped due to stalled tx dma, if the
-	 * queue is stopped then start the queue as we have free desc for tx
-	 */
 	if (unlikely(netif_queue_stopped(ndev)))
-		netif_wake_queue(ndev);
-	cpts_tx_timestamp(priv->cpts, skb);
+		netif_start_queue(ndev);
+	cpts_tx_timestamp(&priv->cpts, skb);
 	priv->stats.tx_packets++;
 	priv->stats.tx_bytes += len;
 	dev_kfree_skb_any(skb);
@@ -472,69 +387,61 @@ void cpsw_tx_handler(void *token, int len, int status)
 void cpsw_rx_handler(void *token, int len, int status)
 {
 	struct sk_buff		*skb = token;
-	struct sk_buff		*new_skb;
 	struct net_device	*ndev = skb->dev;
 	struct cpsw_priv	*priv = netdev_priv(ndev);
 	int			ret = 0;
 
-	cpsw_dual_emac_src_port_detect(status, priv, ndev, skb);
-
-	if (unlikely(status < 0)) {
-		/* the interface is going down, skbs are purged */
+	/* free and bail if we are shutting down */
+	if (unlikely(!netif_running(ndev)) ||
+			unlikely(!netif_carrier_ok(ndev))) {
 		dev_kfree_skb_any(skb);
 		return;
 	}
-
-	new_skb = netdev_alloc_skb_ip_align(ndev, priv->rx_packet_max);
-	if (new_skb) {
+	if (likely(status >= 0)) {
 		skb_put(skb, len);
-		cpts_rx_timestamp(priv->cpts, skb);
+		cpts_rx_timestamp(&priv->cpts, skb);
 		skb->protocol = eth_type_trans(skb, ndev);
 		netif_receive_skb(skb);
 		priv->stats.rx_bytes += len;
 		priv->stats.rx_packets++;
-	} else {
-		priv->stats.rx_dropped++;
-		new_skb = skb;
+		skb = NULL;
+	}
+
+	if (unlikely(!netif_running(ndev))) {
+		if (skb)
+			dev_kfree_skb_any(skb);
+		return;
 	}
 
-	ret = cpdma_chan_submit(priv->rxch, new_skb, new_skb->data,
-			skb_tailroom(new_skb), 0);
-	if (WARN_ON(ret < 0))
-		dev_kfree_skb_any(new_skb);
+	if (likely(!skb)) {
+		skb = netdev_alloc_skb_ip_align(ndev, priv->rx_packet_max);
+		if (WARN_ON(!skb))
+			return;
+
+		ret = cpdma_chan_submit(priv->rxch, skb, skb->data,
+					skb_tailroom(skb), GFP_KERNEL);
+	}
+	WARN_ON(ret < 0);
 }
 
 static irqreturn_t cpsw_interrupt(int irq, void *dev_id)
 {
 	struct cpsw_priv *priv = dev_id;
-	u32 rx, tx, rx_thresh;
 
-	rx_thresh = __raw_readl(&priv->wr_regs->rx_thresh_stat);
-	rx = __raw_readl(&priv->wr_regs->rx_stat);
-	tx = __raw_readl(&priv->wr_regs->tx_stat);
-	if (!rx_thresh && !rx && !tx)
-		return IRQ_NONE;
-
-	cpsw_intr_disable(priv);
-	if (priv->irq_enabled == true) {
+	if (likely(netif_running(priv->ndev))) {
+		cpsw_intr_disable(priv);
 		cpsw_disable_irq(priv);
-		priv->irq_enabled = false;
-	}
-
-	if (netif_running(priv->ndev)) {
 		napi_schedule(&priv->napi);
-		return IRQ_HANDLED;
 	}
+	return IRQ_HANDLED;
+}
 
-	priv = cpsw_get_slave_priv(priv, 1);
-	if (!priv)
-		return IRQ_NONE;
-
-	if (netif_running(priv->ndev)) {
-		napi_schedule(&priv->napi);
-		return IRQ_HANDLED;
-	}
-	return IRQ_NONE;
+static inline int cpsw_get_slave_port(struct cpsw_priv *priv, u32 slave_num)
+{
+	if (priv->host_port == 0)
+		return slave_num + 1;
+	else
+		return slave_num;
 }
 
 static int cpsw_poll(struct napi_struct *napi, int budget)
@@ -543,27 +450,19 @@ static int cpsw_poll(struct napi_struct *napi, int budget)
 	int			num_tx, num_rx;
 
 	num_tx = cpdma_chan_process(priv->txch, 128);
-	if (num_tx)
-		cpdma_ctlr_eoi(priv->dma, CPDMA_EOI_TX);
-
 	num_rx = cpdma_chan_process(priv->rxch, budget);
-	if (num_rx < budget) {
-		struct cpsw_priv *prim_cpsw;
-
-		napi_complete(napi);
-		cpsw_intr_enable(priv);
-		cpdma_ctlr_eoi(priv->dma, CPDMA_EOI_RX);
-		prim_cpsw = cpsw_get_slave_priv(priv, 0);
-		if (prim_cpsw->irq_enabled == false) {
-			prim_cpsw->irq_enabled = true;
-			cpsw_enable_irq(priv);
-		}
-	}
 
 	if (num_rx || num_tx)
 		cpsw_dbg(priv, intr, "poll %d rx, %d tx pkts\n",
 			 num_rx, num_tx);
 
+	if (num_rx < budget) {
+		napi_complete(napi);
+		cpsw_intr_enable(priv);
+		cpdma_ctlr_eoi(priv->dma);
+		cpsw_enable_irq(priv);
+	}
+
 	return num_rx;
 }
 
@@ -634,7 +533,7 @@ static void _cpsw_adjust_link(struct cpsw_slave *slave,
 	slave->mac_control = mac_control;
 }
 
-static void cpsw_adjust_link(struct net_device *ndev)
+static void cpsw_adjust_link(struct net_device *ndev, void *context)
 {
 	struct cpsw_priv	*priv = netdev_priv(ndev);
 	bool			link = false;
@@ -651,77 +550,6 @@ static void cpsw_adjust_link(struct net_device *ndev)
 	}
 }
 
-static int cpsw_get_coalesce(struct net_device *ndev,
-				struct ethtool_coalesce *coal)
-{
-	struct cpsw_priv *priv = netdev_priv(ndev);
-
-	coal->rx_coalesce_usecs = priv->coal_intvl;
-	return 0;
-}
-
-static int cpsw_set_coalesce(struct net_device *ndev,
-				struct ethtool_coalesce *coal)
-{
-	struct cpsw_priv *priv = netdev_priv(ndev);
-	u32 int_ctrl;
-	u32 num_interrupts = 0;
-	u32 prescale = 0;
-	u32 addnl_dvdr = 1;
-	u32 coal_intvl = 0;
-
-	if (!coal->rx_coalesce_usecs)
-		return -EINVAL;
-
-	coal_intvl = coal->rx_coalesce_usecs;
-
-	int_ctrl =  readl(&priv->wr_regs->int_control);
-	prescale = priv->bus_freq_mhz * 4;
-
-	if (coal_intvl < CPSW_CMINTMIN_INTVL)
-		coal_intvl = CPSW_CMINTMIN_INTVL;
-
-	if (coal_intvl > CPSW_CMINTMAX_INTVL) {
-		/* Interrupt pacer works with 4us Pulse, we can
-		 * throttle further by dilating the 4us pulse.
-		 */
-		addnl_dvdr = CPSW_INTPRESCALE_MASK / prescale;
-
-		if (addnl_dvdr > 1) {
-			prescale *= addnl_dvdr;
-			if (coal_intvl > (CPSW_CMINTMAX_INTVL * addnl_dvdr))
-				coal_intvl = (CPSW_CMINTMAX_INTVL
-						* addnl_dvdr);
-		} else {
-			addnl_dvdr = 1;
-			coal_intvl = CPSW_CMINTMAX_INTVL;
-		}
-	}
-
-	num_interrupts = (1000 * addnl_dvdr) / coal_intvl;
-	writel(num_interrupts, &priv->wr_regs->rx_imax);
-	writel(num_interrupts, &priv->wr_regs->tx_imax);
-
-	int_ctrl |= CPSW_INTPACEEN;
-	int_ctrl &= (~CPSW_INTPRESCALE_MASK);
-	int_ctrl |= (prescale & CPSW_INTPRESCALE_MASK);
-	writel(int_ctrl, &priv->wr_regs->int_control);
-
-	cpsw_notice(priv, timer, "Set coalesce to %d usecs.\n", coal_intvl);
-	if (priv->data.dual_emac) {
-		int i;
-
-		for (i = 0; i < priv->data.slaves; i++) {
-			priv = netdev_priv(priv->slaves[i].ndev);
-			priv->coal_intvl = coal_intvl;
-		}
-	} else {
-		priv->coal_intvl = coal_intvl;
-	}
-
-	return 0;
-}
-
 static inline int __show_stat(char *buf, int maxlen, const char *name, u32 val)
 {
 	static char *leader = "........................................";
@@ -733,54 +561,6 @@ static inline int __show_stat(char *buf, int maxlen, const char *name, u32 val)
 				leader + strlen(name), val);
 }
 
-static int cpsw_common_res_usage_state(struct cpsw_priv *priv)
-{
-	u32 i;
-	u32 usage_count = 0;
-
-	if (!priv->data.dual_emac)
-		return 0;
-
-	for (i = 0; i < priv->data.slaves; i++)
-		if (priv->slaves[i].open_stat)
-			usage_count++;
-
-	return usage_count;
-}
-
-static inline int cpsw_tx_packet_submit(struct net_device *ndev,
-			struct cpsw_priv *priv, struct sk_buff *skb)
-{
-	if (!priv->data.dual_emac)
-		return cpdma_chan_submit(priv->txch, skb, skb->data,
-				  skb->len, 0);
-
-	if (ndev == cpsw_get_slave_ndev(priv, 0))
-		return cpdma_chan_submit(priv->txch, skb, skb->data,
-				  skb->len, 1);
-	else
-		return cpdma_chan_submit(priv->txch, skb, skb->data,
-				  skb->len, 2);
-}
-
-static inline void cpsw_add_dual_emac_def_ale_entries(
-		struct cpsw_priv *priv, struct cpsw_slave *slave,
-		u32 slave_port)
-{
-	u32 port_mask = 1 << slave_port | 1 << priv->host_port;
-
-	if (priv->version == CPSW_VERSION_1)
-		slave_write(slave, slave->port_vlan, CPSW1_PORT_VLAN);
-	else
-		slave_write(slave, slave->port_vlan, CPSW2_PORT_VLAN);
-	cpsw_ale_add_vlan(priv->ale, slave->port_vlan, port_mask,
-			  port_mask, port_mask, 0);
-	cpsw_ale_add_mcast(priv->ale, priv->ndev->broadcast,
-			   port_mask, ALE_VLAN, slave->port_vlan, 0);
-	cpsw_ale_add_ucast(priv->ale, priv->mac_addr,
-		priv->host_port, ALE_VLAN, slave->port_vlan);
-}
-
 static void cpsw_slave_open(struct cpsw_slave *slave, struct cpsw_priv *priv)
 {
 	char name[32];
@@ -810,14 +590,13 @@ static void cpsw_slave_open(struct cpsw_slave *slave, struct cpsw_priv *priv)
 
 	slave_port = cpsw_get_slave_port(priv, slave->slave_num);
 
-	if (priv->data.dual_emac)
-		cpsw_add_dual_emac_def_ale_entries(priv, slave, slave_port);
-	else
-		cpsw_ale_add_mcast(priv->ale, priv->ndev->broadcast,
-				   1 << slave_port, 0, 0, ALE_MCAST_FWD_2);
+	cpsw_ale_add_mcast(priv->ale, priv->ndev->broadcast,
+			   1 << slave_port, 0,
+			   ALE_MCAST_FWD_2, CPSW_NON_VLAN_ADDR);
 
 	slave->phy = phy_connect(priv->ndev, slave->data->phy_id,
-				 &cpsw_adjust_link, slave->data->phy_if);
+				 &cpsw_adjust_link, 0,
+				 slave->data->phy_if, NULL);
 	if (IS_ERR(slave->phy)) {
 		dev_err(priv->dev, "phy %s not found on slave %d\n",
 			slave->data->phy_id, slave->slave_num);
@@ -829,44 +608,14 @@ static void cpsw_slave_open(struct cpsw_slave *slave, struct cpsw_priv *priv)
 	}
 }
 
-static inline void cpsw_add_default_vlan(struct cpsw_priv *priv)
-{
-	const int vlan = priv->data.default_vlan;
-	const int port = priv->host_port;
-	u32 reg;
-	int i;
-
-	reg = (priv->version == CPSW_VERSION_1) ? CPSW1_PORT_VLAN :
-	       CPSW2_PORT_VLAN;
-
-	writel(vlan, &priv->host_port_regs->port_vlan);
-
-	for (i = 0; i < priv->data.slaves; i++)
-		slave_write(priv->slaves + i, vlan, reg);
-
-	cpsw_ale_add_vlan(priv->ale, vlan, ALE_ALL_PORTS << port,
-			  ALE_ALL_PORTS << port, ALE_ALL_PORTS << port,
-			  (ALE_PORT_1 | ALE_PORT_2) << port);
-}
-
 static void cpsw_init_host_port(struct cpsw_priv *priv)
 {
-	u32 control_reg;
-	u32 fifo_mode;
-
 	/* soft reset the controller and initialize ale */
 	soft_reset("cpsw", &priv->regs->soft_reset);
 	cpsw_ale_start(priv->ale);
 
 	/* switch to vlan unaware mode */
-	cpsw_ale_control_set(priv->ale, priv->host_port, ALE_VLAN_AWARE,
-			     CPSW_ALE_VLAN_AWARE);
-	control_reg = readl(&priv->regs->control);
-	control_reg |= CPSW_VLAN_AWARE;
-	writel(control_reg, &priv->regs->control);
-	fifo_mode = (priv->data.dual_emac) ? CPSW_FIFO_DUAL_MAC_MODE :
-		     CPSW_FIFO_NORMAL_MODE;
-	writel(fifo_mode, &priv->host_port_regs->tx_in_ctl);
+	cpsw_ale_control_set(priv->ale, 0, ALE_VLAN_AWARE, 0);
 
 	/* setup host port priority mapping */
 	__raw_writel(CPDMA_TX_PRIORITY_MAP,
@@ -876,32 +625,20 @@ static void cpsw_init_host_port(struct cpsw_priv *priv)
 	cpsw_ale_control_set(priv->ale, priv->host_port,
 			     ALE_PORT_STATE, ALE_PORT_STATE_FORWARD);
 
-	if (!priv->data.dual_emac) {
-		cpsw_ale_add_ucast(priv->ale, priv->mac_addr, priv->host_port,
-				   0, 0);
-		cpsw_ale_add_mcast(priv->ale, priv->ndev->broadcast,
-				   1 << priv->host_port, 0, 0, ALE_MCAST_FWD_2);
-	}
-}
-
-static void cpsw_slave_stop(struct cpsw_slave *slave, struct cpsw_priv *priv)
-{
-	if (!slave->phy)
-		return;
-	phy_stop(slave->phy);
-	phy_disconnect(slave->phy);
-	slave->phy = NULL;
+	cpsw_ale_add_ucast(priv->ale, priv->mac_addr, priv->host_port,
+			   0, CPSW_NON_VLAN_ADDR);
+	cpsw_ale_add_mcast(priv->ale, priv->ndev->broadcast,
+			   1 << priv->host_port, 0,
+			   ALE_MCAST_FWD_2, CPSW_NON_VLAN_ADDR);
 }
 
 static int cpsw_ndo_open(struct net_device *ndev)
 {
 	struct cpsw_priv *priv = netdev_priv(ndev);
-	struct cpsw_priv *prim_cpsw;
 	int i, ret;
 	u32 reg;
 
-	if (!cpsw_common_res_usage_state(priv))
-		cpsw_intr_disable(priv);
+	cpsw_intr_disable(priv);
 	netif_carrier_off(ndev);
 
 	pm_runtime_get_sync(&priv->pdev->dev);
@@ -913,81 +650,53 @@ static int cpsw_ndo_open(struct net_device *ndev)
 		 CPSW_RTL_VERSION(reg));
 
 	/* initialize host and slave ports */
-	if (!cpsw_common_res_usage_state(priv))
-		cpsw_init_host_port(priv);
+	cpsw_init_host_port(priv);
 	for_each_slave(priv, cpsw_slave_open, priv);
 
-	/* Add default VLAN */
-	if (!priv->data.dual_emac)
-		cpsw_add_default_vlan(priv);
-
-	if (!cpsw_common_res_usage_state(priv)) {
-		/* setup tx dma to fixed prio and zero offset */
-		cpdma_control_set(priv->dma, CPDMA_TX_PRIO_FIXED, 1);
-		cpdma_control_set(priv->dma, CPDMA_RX_BUFFER_OFFSET, 0);
-
-		/* disable priority elevation */
-		__raw_writel(0, &priv->regs->ptype);
-
-		/* enable statistics collection only on all ports */
-		__raw_writel(0x7, &priv->regs->stat_port_en);
-
-		if (WARN_ON(!priv->data.rx_descs))
-			priv->data.rx_descs = 128;
-
-		for (i = 0; i < priv->data.rx_descs; i++) {
-			struct sk_buff *skb;
-
-			ret = -ENOMEM;
-			skb = __netdev_alloc_skb_ip_align(priv->ndev,
-					priv->rx_packet_max, GFP_KERNEL);
-			if (!skb)
-				goto err_cleanup;
-			ret = cpdma_chan_submit(priv->rxch, skb, skb->data,
-					skb_tailroom(skb), 0);
-			if (ret < 0) {
-				kfree_skb(skb);
-				goto err_cleanup;
-			}
-		}
-		/* continue even if we didn't manage to submit all
-		 * receive descs
-		 */
-		cpsw_info(priv, ifup, "submitted %d rx descriptors\n", i);
-	}
+	/* setup tx dma to fixed prio and zero offset */
+	cpdma_control_set(priv->dma, CPDMA_TX_PRIO_FIXED, 1);
+	cpdma_control_set(priv->dma, CPDMA_RX_BUFFER_OFFSET, 0);
 
-	/* Enable Interrupt pacing if configured */
-	if (priv->coal_intvl != 0) {
-		struct ethtool_coalesce coal;
+	/* disable priority elevation and enable statistics on all ports */
+	__raw_writel(0, &priv->regs->ptype);
 
-		coal.rx_coalesce_usecs = (priv->coal_intvl << 4);
-		cpsw_set_coalesce(ndev, &coal);
-	}
+	/* enable statistics collection only on the host port */
+	__raw_writel(0x7, &priv->regs->stat_port_en);
 
-	prim_cpsw = cpsw_get_slave_priv(priv, 0);
-	if (prim_cpsw->irq_enabled == false) {
-		if ((priv == prim_cpsw) || !netif_running(prim_cpsw->ndev)) {
-			prim_cpsw->irq_enabled = true;
-			cpsw_enable_irq(prim_cpsw);
-		}
+	if (WARN_ON(!priv->data.rx_descs))
+		priv->data.rx_descs = 128;
+
+	for (i = 0; i < priv->data.rx_descs; i++) {
+		struct sk_buff *skb;
+
+		ret = -ENOMEM;
+		skb = netdev_alloc_skb_ip_align(priv->ndev,
+						priv->rx_packet_max);
+		if (!skb)
+			break;
+		ret = cpdma_chan_submit(priv->rxch, skb, skb->data,
+					skb_tailroom(skb), GFP_KERNEL);
+		if (WARN_ON(ret < 0))
+			break;
 	}
+	/* continue even if we didn't manage to submit all receive descs */
+	cpsw_info(priv, ifup, "submitted %d rx descriptors\n", i);
 
 	cpdma_ctlr_start(priv->dma);
 	cpsw_intr_enable(priv);
 	napi_enable(&priv->napi);
-	cpdma_ctlr_eoi(priv->dma, CPDMA_EOI_RX);
-	cpdma_ctlr_eoi(priv->dma, CPDMA_EOI_TX);
+	cpdma_ctlr_eoi(priv->dma);
 
-	if (priv->data.dual_emac)
-		priv->slaves[priv->emac_port].open_stat = true;
 	return 0;
+}
 
-err_cleanup:
-	cpdma_ctlr_stop(priv->dma);
-	for_each_slave(priv, cpsw_slave_stop, priv);
-	pm_runtime_put_sync(&priv->pdev->dev);
-	netif_carrier_off(priv->ndev);
-	return ret;
+static void cpsw_slave_stop(struct cpsw_slave *slave, struct cpsw_priv *priv)
+{
+	if (!slave->phy)
+		return;
+	phy_stop(slave->phy);
+	phy_disconnect(slave->phy);
+	slave->phy = NULL;
 }
 
 static int cpsw_ndo_stop(struct net_device *ndev)
@@ -998,17 +707,12 @@ static int cpsw_ndo_stop(struct net_device *ndev)
 	netif_stop_queue(priv->ndev);
 	napi_disable(&priv->napi);
 	netif_carrier_off(priv->ndev);
-
-	if (cpsw_common_res_usage_state(priv) <= 1) {
-		cpsw_intr_disable(priv);
-		cpdma_ctlr_int_ctrl(priv->dma, false);
-		cpdma_ctlr_stop(priv->dma);
-		cpsw_ale_stop(priv->ale);
-	}
+	cpsw_intr_disable(priv);
+	cpdma_ctlr_int_ctrl(priv->dma, false);
+	cpdma_ctlr_stop(priv->dma);
+	cpsw_ale_stop(priv->ale);
 	for_each_slave(priv, cpsw_slave_stop, priv);
 	pm_runtime_put_sync(&priv->pdev->dev);
-	if (priv->data.dual_emac)
-		priv->slaves[priv->emac_port].open_stat = false;
 	return 0;
 }
 
@@ -1026,24 +730,18 @@ static netdev_tx_t cpsw_ndo_start_xmit(struct sk_buff *skb,
 		return NETDEV_TX_OK;
 	}
 
-	if (skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP &&
-				priv->cpts->tx_enable)
+	if (skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP && priv->cpts.tx_enable)
 		skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
 
 	skb_tx_timestamp(skb);
 
-	ret = cpsw_tx_packet_submit(ndev, priv, skb);
+	ret = cpdma_chan_submit(priv->txch, skb, skb->data,
+				skb->len, GFP_KERNEL);
 	if (unlikely(ret != 0)) {
 		cpsw_err(priv, tx_err, "desc submit failed\n");
 		goto fail;
 	}
 
-	/* If there is no more tx desc left free then we need to
-	 * tell the kernel to stop sending us tx frames.
-	 */
-	if (unlikely(!cpdma_check_free_tx_desc(priv->txch)))
-		netif_stop_queue(ndev);
-
 	return NETDEV_TX_OK;
 fail:
 	priv->stats.tx_dropped++;
@@ -1078,10 +776,10 @@ static void cpsw_ndo_change_rx_flags(struct net_device *ndev, int flags)
 
 static void cpsw_hwtstamp_v1(struct cpsw_priv *priv)
 {
-	struct cpsw_slave *slave = &priv->slaves[priv->data.active_slave];
+	struct cpsw_slave *slave = &priv->slaves[priv->data.cpts_active_slave];
 	u32 ts_en, seq_id;
 
-	if (!priv->cpts->tx_enable && !priv->cpts->rx_enable) {
+	if (!priv->cpts.tx_enable && !priv->cpts.rx_enable) {
 		slave_write(slave, 0, CPSW1_TS_CTL);
 		return;
 	}
@@ -1089,10 +787,10 @@ static void cpsw_hwtstamp_v1(struct cpsw_priv *priv)
 	seq_id = (30 << CPSW_V1_SEQ_ID_OFS_SHIFT) | ETH_P_1588;
 	ts_en = EVENT_MSG_BITS << CPSW_V1_MSG_TYPE_OFS;
 
-	if (priv->cpts->tx_enable)
+	if (priv->cpts.tx_enable)
 		ts_en |= CPSW_V1_TS_TX_EN;
 
-	if (priv->cpts->rx_enable)
+	if (priv->cpts.rx_enable)
 		ts_en |= CPSW_V1_TS_RX_EN;
 
 	slave_write(slave, ts_en, CPSW1_TS_CTL);
@@ -1101,21 +799,16 @@ static void cpsw_hwtstamp_v1(struct cpsw_priv *priv)
 
 static void cpsw_hwtstamp_v2(struct cpsw_priv *priv)
 {
-	struct cpsw_slave *slave;
+	struct cpsw_slave *slave = &priv->slaves[priv->data.cpts_active_slave];
 	u32 ctrl, mtype;
 
-	if (priv->data.dual_emac)
-		slave = &priv->slaves[priv->emac_port];
-	else
-		slave = &priv->slaves[priv->data.active_slave];
-
 	ctrl = slave_read(slave, CPSW2_CONTROL);
 	ctrl &= ~CTRL_ALL_TS_MASK;
 
-	if (priv->cpts->tx_enable)
+	if (priv->cpts.tx_enable)
 		ctrl |= CTRL_TX_TS_BITS;
 
-	if (priv->cpts->rx_enable)
+	if (priv->cpts.rx_enable)
 		ctrl |= CTRL_RX_TS_BITS;
 
 	mtype = (30 << TS_SEQ_ID_OFFSET_SHIFT) | EVENT_MSG_BITS;
@@ -1128,7 +821,7 @@ static void cpsw_hwtstamp_v2(struct cpsw_priv *priv)
 static int cpsw_hwtstamp_ioctl(struct net_device *dev, struct ifreq *ifr)
 {
 	struct cpsw_priv *priv = netdev_priv(dev);
-	struct cpts *cpts = priv->cpts;
+	struct cpts *cpts = &priv->cpts;
 	struct hwtstamp_config cfg;
 
 	if (copy_from_user(&cfg, ifr->ifr_data, sizeof(cfg)))
@@ -1192,26 +885,14 @@ static int cpsw_hwtstamp_ioctl(struct net_device *dev, struct ifreq *ifr)
 
 static int cpsw_ndo_ioctl(struct net_device *dev, struct ifreq *req, int cmd)
 {
-	struct cpsw_priv *priv = netdev_priv(dev);
-	struct mii_ioctl_data *data = if_mii(req);
-	int slave_no = cpsw_slave_index(priv);
-
 	if (!netif_running(dev))
 		return -EINVAL;
 
-	switch (cmd) {
 #ifdef CONFIG_TI_CPTS
-	case SIOCSHWTSTAMP:
+	if (cmd == SIOCSHWTSTAMP)
 		return cpsw_hwtstamp_ioctl(dev, req);
 #endif
-	case SIOCGMIIPHY:
-		data->phy_id = priv->slaves[slave_no].phy->addr;
-		break;
-	default:
-		return -ENOTSUPP;
-	}
-
-	return 0;
+	return -ENOTSUPP;
 }
 
 static void cpsw_ndo_tx_timeout(struct net_device *ndev)
@@ -1226,9 +907,7 @@ static void cpsw_ndo_tx_timeout(struct net_device *ndev)
 	cpdma_chan_start(priv->txch);
 	cpdma_ctlr_int_ctrl(priv->dma, true);
 	cpsw_intr_enable(priv);
-	cpdma_ctlr_eoi(priv->dma, CPDMA_EOI_RX);
-	cpdma_ctlr_eoi(priv->dma, CPDMA_EOI_TX);
-
+	cpdma_ctlr_eoi(priv->dma);
 }
 
 static struct net_device_stats *cpsw_ndo_get_stats(struct net_device *ndev)
@@ -1247,79 +926,10 @@ static void cpsw_ndo_poll_controller(struct net_device *ndev)
 	cpsw_interrupt(ndev->irq, priv);
 	cpdma_ctlr_int_ctrl(priv->dma, true);
 	cpsw_intr_enable(priv);
-	cpdma_ctlr_eoi(priv->dma, CPDMA_EOI_RX);
-	cpdma_ctlr_eoi(priv->dma, CPDMA_EOI_TX);
-
+	cpdma_ctlr_eoi(priv->dma);
 }
 #endif
 
-static inline int cpsw_add_vlan_ale_entry(struct cpsw_priv *priv,
-				unsigned short vid)
-{
-	int ret;
-
-	ret = cpsw_ale_add_vlan(priv->ale, vid,
-				ALE_ALL_PORTS << priv->host_port,
-				0, ALE_ALL_PORTS << priv->host_port,
-				(ALE_PORT_1 | ALE_PORT_2) << priv->host_port);
-	if (ret != 0)
-		return ret;
-
-	ret = cpsw_ale_add_ucast(priv->ale, priv->mac_addr,
-				 priv->host_port, ALE_VLAN, vid);
-	if (ret != 0)
-		goto clean_vid;
-
-	ret = cpsw_ale_add_mcast(priv->ale, priv->ndev->broadcast,
-				 ALE_ALL_PORTS << priv->host_port,
-				 ALE_VLAN, vid, 0);
-	if (ret != 0)
-		goto clean_vlan_ucast;
-	return 0;
-
-clean_vlan_ucast:
-	cpsw_ale_del_ucast(priv->ale, priv->mac_addr,
-			    priv->host_port, ALE_VLAN, vid);
-clean_vid:
-	cpsw_ale_del_vlan(priv->ale, vid, 0);
-	return ret;
-}
-
-static int cpsw_ndo_vlan_rx_add_vid(struct net_device *ndev,
-				    __be16 proto, u16 vid)
-{
-	struct cpsw_priv *priv = netdev_priv(ndev);
-
-	if (vid == priv->data.default_vlan)
-		return 0;
-
-	dev_info(priv->dev, "Adding vlanid %d to vlan filter\n", vid);
-	return cpsw_add_vlan_ale_entry(priv, vid);
-}
-
-static int cpsw_ndo_vlan_rx_kill_vid(struct net_device *ndev,
-				     __be16 proto, u16 vid)
-{
-	struct cpsw_priv *priv = netdev_priv(ndev);
-	int ret;
-
-	if (vid == priv->data.default_vlan)
-		return 0;
-
-	dev_info(priv->dev, "removing vlanid %d from vlan filter\n", vid);
-	ret = cpsw_ale_del_vlan(priv->ale, vid, 0);
-	if (ret != 0)
-		return ret;
-
-	ret = cpsw_ale_del_ucast(priv->ale, priv->mac_addr,
-				 priv->host_port, ALE_VLAN, vid);
-	if (ret != 0)
-		return ret;
-
-	return cpsw_ale_del_mcast(priv->ale, priv->ndev->broadcast,
-				  0, ALE_VLAN, vid);
-}
-
 static const struct net_device_ops cpsw_netdev_ops = {
 	.ndo_open		= cpsw_ndo_open,
 	.ndo_stop		= cpsw_ndo_stop,
@@ -1334,18 +944,15 @@ static const struct net_device_ops cpsw_netdev_ops = {
 #ifdef CONFIG_NET_POLL_CONTROLLER
 	.ndo_poll_controller	= cpsw_ndo_poll_controller,
 #endif
-	.ndo_vlan_rx_add_vid	= cpsw_ndo_vlan_rx_add_vid,
-	.ndo_vlan_rx_kill_vid	= cpsw_ndo_vlan_rx_kill_vid,
 };
 
 static void cpsw_get_drvinfo(struct net_device *ndev,
 			     struct ethtool_drvinfo *info)
 {
 	struct cpsw_priv *priv = netdev_priv(ndev);
-
-	strlcpy(info->driver, "TI CPSW Driver v1.0", sizeof(info->driver));
-	strlcpy(info->version, "1.0", sizeof(info->version));
-	strlcpy(info->bus_info, priv->pdev->name, sizeof(info->bus_info));
+	strcpy(info->driver, "TI CPSW Driver v1.0");
+	strcpy(info->version, "1.0");
+	strcpy(info->bus_info, priv->pdev->name);
 }
 
 static u32 cpsw_get_msglevel(struct net_device *ndev)
@@ -1373,7 +980,7 @@ static int cpsw_get_ts_info(struct net_device *ndev,
 		SOF_TIMESTAMPING_RX_SOFTWARE |
 		SOF_TIMESTAMPING_SOFTWARE |
 		SOF_TIMESTAMPING_RAW_HARDWARE;
-	info->phc_index = priv->cpts->phc_index;
+	info->phc_index = priv->cpts.phc_index;
 	info->tx_types =
 		(1 << HWTSTAMP_TX_OFF) |
 		(1 << HWTSTAMP_TX_ON);
@@ -1392,39 +999,12 @@ static int cpsw_get_ts_info(struct net_device *ndev,
 	return 0;
 }
 
-static int cpsw_get_settings(struct net_device *ndev,
-			     struct ethtool_cmd *ecmd)
-{
-	struct cpsw_priv *priv = netdev_priv(ndev);
-	int slave_no = cpsw_slave_index(priv);
-
-	if (priv->slaves[slave_no].phy)
-		return phy_ethtool_gset(priv->slaves[slave_no].phy, ecmd);
-	else
-		return -EOPNOTSUPP;
-}
-
-static int cpsw_set_settings(struct net_device *ndev, struct ethtool_cmd *ecmd)
-{
-	struct cpsw_priv *priv = netdev_priv(ndev);
-	int slave_no = cpsw_slave_index(priv);
-
-	if (priv->slaves[slave_no].phy)
-		return phy_ethtool_sset(priv->slaves[slave_no].phy, ecmd);
-	else
-		return -EOPNOTSUPP;
-}
-
 static const struct ethtool_ops cpsw_ethtool_ops = {
 	.get_drvinfo	= cpsw_get_drvinfo,
 	.get_msglevel	= cpsw_get_msglevel,
 	.set_msglevel	= cpsw_set_msglevel,
 	.get_link	= ethtool_op_get_link,
 	.get_ts_info	= cpsw_get_ts_info,
-	.get_settings	= cpsw_get_settings,
-	.set_settings	= cpsw_set_settings,
-	.get_coalesce	= cpsw_get_coalesce,
-	.set_coalesce	= cpsw_set_coalesce,
 };
 
 static void cpsw_slave_init(struct cpsw_slave *slave, struct cpsw_priv *priv,
@@ -1437,7 +1017,6 @@ static void cpsw_slave_init(struct cpsw_slave *slave, struct cpsw_priv *priv,
 	slave->data	= data;
 	slave->regs	= regs + slave_reg_ofs;
 	slave->sliver	= regs + sliver_reg_ofs;
-	slave->port_vlan = data->dual_emac_res_vlan;
 }
 
 static int cpsw_probe_dt(struct cpsw_platform_data *data,
@@ -1457,12 +1036,12 @@ static int cpsw_probe_dt(struct cpsw_platform_data *data,
 	}
 	data->slaves = prop;
 
-	if (of_property_read_u32(node, "active_slave", &prop)) {
-		pr_err("Missing active_slave property in the DT.\n");
+	if (of_property_read_u32(node, "cpts_active_slave", &prop)) {
+		pr_err("Missing cpts_active_slave property in the DT.\n");
 		ret = -EINVAL;
 		goto error_ret;
 	}
-	data->active_slave = prop;
+	data->cpts_active_slave = prop;
 
 	if (of_property_read_u32(node, "cpts_clock_mult", &prop)) {
 		pr_err("Missing cpts_clock_mult property in the DT.\n");
@@ -1478,10 +1057,12 @@ static int cpsw_probe_dt(struct cpsw_platform_data *data,
 	}
 	data->cpts_clock_shift = prop;
 
-	data->slave_data = kcalloc(data->slaves, sizeof(struct cpsw_slave_data),
-				   GFP_KERNEL);
-	if (!data->slave_data)
+	data->slave_data = kzalloc(sizeof(struct cpsw_slave_data) *
+				   data->slaves, GFP_KERNEL);
+	if (!data->slave_data) {
+		pr_err("Could not allocate slave memory.\n");
 		return -EINVAL;
+	}
 
 	if (of_property_read_u32(node, "cpdma_channels", &prop)) {
 		pr_err("Missing cpdma_channels property in the DT.\n");
@@ -1518,9 +1099,6 @@ static int cpsw_probe_dt(struct cpsw_platform_data *data,
 	}
 	data->mac_control = prop;
 
-	if (!of_property_read_u32(node, "dual_emac", &prop))
-		data->dual_emac = prop;
-
 	/*
 	 * Populate all the child nodes here...
 	 */
@@ -1539,7 +1117,7 @@ static int cpsw_probe_dt(struct cpsw_platform_data *data,
 		struct platform_device *mdio;
 
 		parp = of_get_property(slave_node, "phy_id", &lenp);
-		if ((parp == NULL) || (lenp != (sizeof(void *) * 2))) {
+		if ((parp == NULL) && (lenp != (sizeof(void *) * 2))) {
 			pr_err("Missing slave[%d] phy_id property\n", i);
 			ret = -EINVAL;
 			goto error_ret;
@@ -1554,18 +1132,6 @@ static int cpsw_probe_dt(struct cpsw_platform_data *data,
 		if (mac_addr)
 			memcpy(slave_data->mac_addr, mac_addr, ETH_ALEN);
 
-		if (data->dual_emac) {
-			if (of_property_read_u32(slave_node, "dual_emac_res_vlan",
-						 &prop)) {
-				pr_err("Missing dual_emac_res_vlan in DT.\n");
-				slave_data->dual_emac_res_vlan = i+1;
-				pr_err("Using %d as Reserved VLAN for %d slave\n",
-				       slave_data->dual_emac_res_vlan, i);
-			} else {
-				slave_data->dual_emac_res_vlan = prop;
-			}
-		}
-
 		i++;
 	}
 
@@ -1576,84 +1142,9 @@ error_ret:
 	return ret;
 }
 
-static int cpsw_probe_dual_emac(struct platform_device *pdev,
-				struct cpsw_priv *priv)
-{
-	struct cpsw_platform_data	*data = &priv->data;
-	struct net_device		*ndev;
-	struct cpsw_priv		*priv_sl2;
-	int ret = 0, i;
-
-	ndev = alloc_etherdev(sizeof(struct cpsw_priv));
-	if (!ndev) {
-		pr_err("cpsw: error allocating net_device\n");
-		return -ENOMEM;
-	}
-
-	priv_sl2 = netdev_priv(ndev);
-	spin_lock_init(&priv_sl2->lock);
-	priv_sl2->data = *data;
-	priv_sl2->pdev = pdev;
-	priv_sl2->ndev = ndev;
-	priv_sl2->dev  = &ndev->dev;
-	priv_sl2->msg_enable = netif_msg_init(debug_level, CPSW_DEBUG);
-	priv_sl2->rx_packet_max = max(rx_packet_max, 128);
-
-	if (is_valid_ether_addr(data->slave_data[1].mac_addr)) {
-		memcpy(priv_sl2->mac_addr, data->slave_data[1].mac_addr,
-			ETH_ALEN);
-		pr_info("cpsw: Detected MACID = %pM\n", priv_sl2->mac_addr);
-	} else {
-		random_ether_addr(priv_sl2->mac_addr);
-		pr_info("cpsw: Random MACID = %pM\n", priv_sl2->mac_addr);
-	}
-	memcpy(ndev->dev_addr, priv_sl2->mac_addr, ETH_ALEN);
-
-	priv_sl2->slaves = priv->slaves;
-	priv_sl2->clk = priv->clk;
-
-	priv_sl2->coal_intvl = 0;
-	priv_sl2->bus_freq_mhz = priv->bus_freq_mhz;
-
-	priv_sl2->cpsw_res = priv->cpsw_res;
-	priv_sl2->regs = priv->regs;
-	priv_sl2->host_port = priv->host_port;
-	priv_sl2->host_port_regs = priv->host_port_regs;
-	priv_sl2->wr_regs = priv->wr_regs;
-	priv_sl2->dma = priv->dma;
-	priv_sl2->txch = priv->txch;
-	priv_sl2->rxch = priv->rxch;
-	priv_sl2->ale = priv->ale;
-	priv_sl2->emac_port = 1;
-	priv->slaves[1].ndev = ndev;
-	priv_sl2->cpts = priv->cpts;
-	priv_sl2->version = priv->version;
-
-	for (i = 0; i < priv->num_irqs; i++) {
-		priv_sl2->irqs_table[i] = priv->irqs_table[i];
-		priv_sl2->num_irqs = priv->num_irqs;
-	}
-	ndev->features |= NETIF_F_HW_VLAN_CTAG_FILTER;
-
-	ndev->netdev_ops = &cpsw_netdev_ops;
-	SET_ETHTOOL_OPS(ndev, &cpsw_ethtool_ops);
-	netif_napi_add(ndev, &priv_sl2->napi, cpsw_poll, CPSW_POLL_WEIGHT);
-
-	/* register the network device */
-	SET_NETDEV_DEV(ndev, &pdev->dev);
-	ret = register_netdev(ndev);
-	if (ret) {
-		pr_err("cpsw: error registering net device\n");
-		free_netdev(ndev);
-		ret = -ENODEV;
-	}
-
-	return ret;
-}
-
 static int cpsw_probe(struct platform_device *pdev)
 {
-	struct cpsw_platform_data	*data;
+	struct cpsw_platform_data	*data = pdev->dev.platform_data;
 	struct net_device		*ndev;
 	struct cpsw_priv		*priv;
 	struct cpdma_params		dma_params;
@@ -1677,12 +1168,6 @@ static int cpsw_probe(struct platform_device *pdev)
 	priv->dev  = &ndev->dev;
 	priv->msg_enable = netif_msg_init(debug_level, CPSW_DEBUG);
 	priv->rx_packet_max = max(rx_packet_max, 128);
-	priv->cpts = devm_kzalloc(&pdev->dev, sizeof(struct cpts), GFP_KERNEL);
-	priv->irq_enabled = true;
-	if (!priv->cpts) {
-		pr_err("error allocating cpts\n");
-		goto clean_ndev_ret;
-	}
 
 	/*
 	 * This may be required here for child devices.
@@ -1715,17 +1200,12 @@ static int cpsw_probe(struct platform_device *pdev)
 	for (i = 0; i < data->slaves; i++)
 		priv->slaves[i].slave_num = i;
 
-	priv->slaves[0].ndev = ndev;
-	priv->emac_port = 0;
-
 	priv->clk = clk_get(&pdev->dev, "fck");
 	if (IS_ERR(priv->clk)) {
 		dev_err(&pdev->dev, "fck is not found\n");
 		ret = -ENODEV;
 		goto clean_slave_ret;
 	}
-	priv->coal_intvl = 0;
-	priv->bus_freq_mhz = clk_get_rate(priv->clk) / 1000000;
 
 	priv->cpsw_res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
 	if (!priv->cpsw_res) {
@@ -1774,7 +1254,7 @@ static int cpsw_probe(struct platform_device *pdev)
 	switch (priv->version) {
 	case CPSW_VERSION_1:
 		priv->host_port_regs = ss_regs + CPSW1_HOST_PORT_OFFSET;
-		priv->cpts->reg       = ss_regs + CPSW1_CPTS_OFFSET;
+		priv->cpts.reg       = ss_regs + CPSW1_CPTS_OFFSET;
 		dma_params.dmaregs   = ss_regs + CPSW1_CPDMA_OFFSET;
 		dma_params.txhdp     = ss_regs + CPSW1_STATERAM_OFFSET;
 		ale_params.ale_regs  = ss_regs + CPSW1_ALE_OFFSET;
@@ -1785,7 +1265,7 @@ static int cpsw_probe(struct platform_device *pdev)
 		break;
 	case CPSW_VERSION_2:
 		priv->host_port_regs = ss_regs + CPSW2_HOST_PORT_OFFSET;
-		priv->cpts->reg       = ss_regs + CPSW2_CPTS_OFFSET;
+		priv->cpts.reg       = ss_regs + CPSW2_CPTS_OFFSET;
 		dma_params.dmaregs   = ss_regs + CPSW2_CPDMA_OFFSET;
 		dma_params.txhdp     = ss_regs + CPSW2_STATERAM_OFFSET;
 		ale_params.ale_regs  = ss_regs + CPSW2_ALE_OFFSET;
@@ -1867,12 +1347,12 @@ static int cpsw_probe(struct platform_device *pdev)
 				goto clean_ale_ret;
 			}
 			priv->irqs_table[k] = i;
-			priv->num_irqs = k + 1;
+			priv->num_irqs = k;
 		}
 		k++;
 	}
 
-	ndev->features |= NETIF_F_HW_VLAN_CTAG_FILTER;
+	ndev->flags |= IFF_ALLMULTI;	/* see cpsw_ndo_change_rx_flags() */
 
 	ndev->netdev_ops = &cpsw_netdev_ops;
 	SET_ETHTOOL_OPS(ndev, &cpsw_ethtool_ops);
@@ -1887,26 +1367,17 @@ static int cpsw_probe(struct platform_device *pdev)
 		goto clean_irq_ret;
 	}
 
-	if (cpts_register(&pdev->dev, priv->cpts,
+	if (cpts_register(&pdev->dev, &priv->cpts,
 			  data->cpts_clock_mult, data->cpts_clock_shift))
 		dev_err(priv->dev, "error registering cpts device\n");
 
 	cpsw_notice(priv, probe, "initialized device (regs %x, irq %d)\n",
 		  priv->cpsw_res->start, ndev->irq);
 
-	if (priv->data.dual_emac) {
-		ret = cpsw_probe_dual_emac(pdev, priv);
-		if (ret) {
-			cpsw_err(priv, probe, "error probe slave 2 emac interface\n");
-			goto clean_irq_ret;
-		}
-	}
-
 	return 0;
 
 clean_irq_ret:
-	for (i = 0; i < priv->num_irqs; i++)
-		free_irq(priv->irqs_table[i], priv);
+	free_irq(ndev->irq, priv);
 clean_ale_ret:
 	cpsw_ale_destroy(priv->ale);
 clean_dma_ret:
@@ -1929,8 +1400,7 @@ clean_slave_ret:
 	pm_runtime_disable(&pdev->dev);
 	kfree(priv->slaves);
 clean_ndev_ret:
-	kfree(priv->data.slave_data);
-	free_netdev(priv->ndev);
+	free_netdev(ndev);
 	return ret;
 }
 
@@ -1938,17 +1408,12 @@ static int cpsw_remove(struct platform_device *pdev)
 {
 	struct net_device *ndev = platform_get_drvdata(pdev);
 	struct cpsw_priv *priv = netdev_priv(ndev);
-	int i;
 
+	pr_info("removing device");
 	platform_set_drvdata(pdev, NULL);
-	if (priv->data.dual_emac)
-		unregister_netdev(cpsw_get_slave_ndev(priv, 1));
-	unregister_netdev(ndev);
-
-	cpts_unregister(priv->cpts);
-	for (i = 0; i < priv->num_irqs; i++)
-		free_irq(priv->irqs_table[i], priv);
 
+	cpts_unregister(&priv->cpts);
+	free_irq(ndev->irq, priv);
 	cpsw_ale_destroy(priv->ale);
 	cpdma_chan_destroy(priv->txch);
 	cpdma_chan_destroy(priv->rxch);
@@ -1962,10 +1427,8 @@ static int cpsw_remove(struct platform_device *pdev)
 	pm_runtime_disable(&pdev->dev);
 	clk_put(priv->clk);
 	kfree(priv->slaves);
-	kfree(priv->data.slave_data);
-	if (priv->data.dual_emac)
-		free_netdev(cpsw_get_slave_ndev(priv, 1));
 	free_netdev(ndev);
+
 	return 0;
 }
 
@@ -1973,12 +1436,9 @@ static int cpsw_suspend(struct device *dev)
 {
 	struct platform_device	*pdev = to_platform_device(dev);
 	struct net_device	*ndev = platform_get_drvdata(pdev);
-	struct cpsw_priv	*priv = netdev_priv(ndev);
 
 	if (netif_running(ndev))
 		cpsw_ndo_stop(ndev);
-	soft_reset("sliver 0", &priv->slaves[0].sliver->soft_reset);
-	soft_reset("sliver 1", &priv->slaves[1].sliver->soft_reset);
 	pm_runtime_put_sync(&pdev->dev);
 
 	return 0;
@@ -2004,7 +1464,6 @@ static const struct of_device_id cpsw_of_mtable[] = {
 	{ .compatible = "ti,cpsw", },
 	{ /* sentinel */ },
 };
-MODULE_DEVICE_TABLE(of, cpsw_of_mtable);
 
 static struct platform_driver cpsw_driver = {
 	.driver = {
diff --git a/drivers/net/ethernet/ti/cpsw_ale.c b/drivers/net/ethernet/ti/cpsw_ale.c
index 7fa60d6..6e325a7 100644
--- a/drivers/net/ethernet/ti/cpsw_ale.c
+++ b/drivers/net/ethernet/ti/cpsw_ale.c
@@ -25,8 +25,11 @@
 #include "cpsw_ale.h"
 
 #define BITMASK(bits)		(BIT(bits) - 1)
+#define ADDR_FMT_STR		"%02x:%02x:%02x:%02x:%02x:%02x"
+#define ADDR_FMT_ARGS(addr)	(addr)[0], (addr)[1], (addr)[2], \
+				(addr)[3], (addr)[4], (addr)[5]
 #define ALE_ENTRY_BITS		68
-#define ALE_ENTRY_WORDS	DIV_ROUND_UP(ALE_ENTRY_BITS, 32)
+#define ALE_ENTRY_WORDS		DIV_ROUND_UP(ALE_ENTRY_BITS, 32)
 
 #define ALE_VERSION_MAJOR(rev)	((rev >> 8) & 0xff)
 #define ALE_VERSION_MINOR(rev)	(rev & 0xff)
@@ -52,6 +55,9 @@
 #define ALE_UCAST_OUI			2
 #define ALE_UCAST_TOUCHED		3
 
+#define ALE_TBL_ENTRY_SHOW_LEN		160
+#define ALE_RAW_TBL_ENTRY_SHOW_LEN	32
+
 static inline int cpsw_ale_get_field(u32 *ale_entry, u32 start, u32 bits)
 {
 	int idx;
@@ -148,38 +154,45 @@ static int cpsw_ale_write(struct cpsw_ale *ale, int idx, u32 *ale_entry)
 	return idx;
 }
 
-int cpsw_ale_match_addr(struct cpsw_ale *ale, u8 *addr, u16 vid)
+static int cpsw_ale_match_vlan(struct cpsw_ale *ale, int vid)
 {
 	u32 ale_entry[ALE_ENTRY_WORDS];
 	int type, idx;
 
 	for (idx = 0; idx < ale->params.ale_entries; idx++) {
-		u8 entry_addr[6];
-
 		cpsw_ale_read(ale, idx, ale_entry);
 		type = cpsw_ale_get_entry_type(ale_entry);
-		if (type != ALE_TYPE_ADDR && type != ALE_TYPE_VLAN_ADDR)
-			continue;
-		if (cpsw_ale_get_vlan_id(ale_entry) != vid)
+		if (type != ALE_TYPE_VLAN && type != ALE_TYPE_VLAN_ADDR)
 			continue;
-		cpsw_ale_get_addr(ale_entry, entry_addr);
-		if (memcmp(entry_addr, addr, 6) == 0)
+
+		if (vid == cpsw_ale_get_vlan_id(ale_entry))
 			return idx;
 	}
+
 	return -ENOENT;
 }
 
-int cpsw_ale_match_vlan(struct cpsw_ale *ale, u16 vid)
+static int cpsw_ale_match_addr(struct cpsw_ale *ale, u8 *addr, int vid)
 {
 	u32 ale_entry[ALE_ENTRY_WORDS];
 	int type, idx;
+	int match_type = ALE_TYPE_VLAN_ADDR;
 
 	for (idx = 0; idx < ale->params.ale_entries; idx++) {
+		u8 entry_addr[6];
+
 		cpsw_ale_read(ale, idx, ale_entry);
 		type = cpsw_ale_get_entry_type(ale_entry);
-		if (type != ALE_TYPE_VLAN)
+		if (type != ALE_TYPE_ADDR && type != ALE_TYPE_VLAN_ADDR)
 			continue;
-		if (cpsw_ale_get_vlan_id(ale_entry) == vid)
+
+		if (vid < 0)
+			match_type = ALE_TYPE_ADDR;
+
+		cpsw_ale_get_addr(ale_entry, entry_addr);
+		if ((memcmp(entry_addr, addr, 6) == 0) &&
+		    (vid < 0 || vid == cpsw_ale_get_vlan_id(ale_entry)) &&
+		    (match_type == type))
 			return idx;
 	}
 	return -ENOENT;
@@ -292,32 +305,126 @@ int cpsw_ale_flush(struct cpsw_ale *ale, int port_mask)
 	return 0;
 }
 
-static inline void cpsw_ale_set_vlan_entry_type(u32 *ale_entry,
-						int flags, u16 vid)
+static int cpsw_ale_dump_mcast(u32 *ale_entry, char *buf, int len)
 {
-	if (flags & ALE_VLAN) {
-		cpsw_ale_set_entry_type(ale_entry, ALE_TYPE_VLAN_ADDR);
-		cpsw_ale_set_vlan_id(ale_entry, vid);
-	} else {
-		cpsw_ale_set_entry_type(ale_entry, ALE_TYPE_ADDR);
+	int outlen = 0;
+	static const char * const str_mcast_state[] = {"f", "blf", "lf", "f"};
+	int mcast_state = cpsw_ale_get_mcast_state(ale_entry);
+	int port_mask   = cpsw_ale_get_port_mask(ale_entry);
+	int super       = cpsw_ale_get_super(ale_entry);
+
+	outlen += snprintf(buf + outlen, len - outlen,
+			   "mcstate: %s(%d), ", str_mcast_state[mcast_state],
+			   mcast_state);
+	outlen += snprintf(buf + outlen, len - outlen,
+			   "port mask: %x, %ssuper\n", port_mask,
+			   super ? "" : "no ");
+	return outlen;
+}
+
+static int cpsw_ale_dump_ucast(u32 *ale_entry, char *buf, int len)
+{
+	int outlen = 0;
+	static const char * const str_ucast_type[] = {"persistant", "untouched",
+							"oui", "touched"};
+	int ucast_type  = cpsw_ale_get_ucast_type(ale_entry);
+	int port_num    = cpsw_ale_get_port_num(ale_entry);
+	int secure      = cpsw_ale_get_secure(ale_entry);
+	int blocked     = cpsw_ale_get_blocked(ale_entry);
+
+	outlen += snprintf(buf + outlen, len - outlen,
+			   "uctype: %s(%d)", str_ucast_type[ucast_type],
+			   ucast_type);
+	if (ucast_type == ALE_UCAST_OUI)
+		outlen += snprintf(buf + outlen, len - outlen, "\n");
+	else
+		outlen += snprintf(buf + outlen, len - outlen,
+				", port: %d%s%s\n", port_num,
+				secure ? ", Secure" : "",
+				blocked ? ", Blocked" : "");
+	return outlen;
+}
+
+static int cpsw_ale_dump_vlan(u32 *ale_entry, char *buf, int len)
+{
+	int outlen = 0;
+	int force_utag_egress	= cpsw_ale_get_vlan_untag_force(ale_entry);
+	int reg_mc_fld		= cpsw_ale_get_vlan_reg_mcast(ale_entry);
+	int unreg_mc_fld	= cpsw_ale_get_vlan_unreg_mcast(ale_entry);
+	int mem_list		= cpsw_ale_get_vlan_member_list(ale_entry);
+
+	outlen += snprintf(buf + outlen, len - outlen,
+			   "force_untag_egress: %02x, ", force_utag_egress);
+	outlen += snprintf(buf + outlen, len - outlen,
+			   "reg_fld: %02x, ", reg_mc_fld);
+	outlen += snprintf(buf + outlen, len - outlen,
+			   "unreg_fld: %02x, ", unreg_mc_fld);
+	outlen += snprintf(buf + outlen, len - outlen,
+			   "mem_list: %02x\n", mem_list);
+	return outlen;
+}
+
+static int cpsw_ale_dump_entry(int idx, u32 *ale_entry, char *buf, int len)
+{
+	int type, outlen = 0;
+	u8 addr[6];
+	static const char * const str_type[] = {"free", "addr",
+						"vlan", "vlan+addr"};
+
+	type = cpsw_ale_get_entry_type(ale_entry);
+	if (type == ALE_TYPE_FREE)
+		return outlen;
+
+	if (len < ALE_TBL_ENTRY_SHOW_LEN)
+		return outlen;
+
+	if (idx >= 0) {
+		outlen += snprintf(buf + outlen, len - outlen,
+				   "index %d, ", idx);
 	}
+
+	outlen += snprintf(buf + outlen, len - outlen, "raw: %08x %08x %08x, ",
+			   ale_entry[0], ale_entry[1], ale_entry[2]);
+
+	outlen += snprintf(buf + outlen, len - outlen,
+			   "type: %s(%d), ", str_type[type], type);
+
+	if (type != ALE_TYPE_VLAN) {
+		cpsw_ale_get_addr(ale_entry, addr);
+		outlen += snprintf(buf + outlen, len - outlen,
+			   "addr: " ADDR_FMT_STR ", ", ADDR_FMT_ARGS(addr));
+	}
+
+	if (type == ALE_TYPE_VLAN || type == ALE_TYPE_VLAN_ADDR) {
+		outlen += snprintf(buf + outlen, len - outlen, "vlan: %d, ",
+				   cpsw_ale_get_vlan_id(ale_entry));
+	}
+
+	if (type == ALE_TYPE_VLAN)
+		outlen += cpsw_ale_dump_vlan(ale_entry,
+				buf + outlen, len - outlen);
+	else
+		outlen += cpsw_ale_get_mcast(ale_entry) ?
+		  cpsw_ale_dump_mcast(ale_entry, buf + outlen, len - outlen) :
+		  cpsw_ale_dump_ucast(ale_entry, buf + outlen, len - outlen);
+
+	return outlen;
 }
 
-int cpsw_ale_add_ucast(struct cpsw_ale *ale, u8 *addr, int port,
-		       int flags, u16 vid)
+int cpsw_ale_add_vlan(struct cpsw_ale *ale, int vid, int member_list,
+		      int reg_mcast, int unreg_mcast, int force_untag_egress)
 {
 	u32 ale_entry[ALE_ENTRY_WORDS] = {0, 0, 0};
 	int idx;
 
-	cpsw_ale_set_vlan_entry_type(ale_entry, flags, vid);
-
-	cpsw_ale_set_addr(ale_entry, addr);
-	cpsw_ale_set_ucast_type(ale_entry, ALE_UCAST_PERSISTANT);
-	cpsw_ale_set_secure(ale_entry, (flags & ALE_SECURE) ? 1 : 0);
-	cpsw_ale_set_blocked(ale_entry, (flags & ALE_BLOCKED) ? 1 : 0);
-	cpsw_ale_set_port_num(ale_entry, port);
+	cpsw_ale_set_entry_type(ale_entry, ALE_TYPE_VLAN);
+	cpsw_ale_set_vlan_id(ale_entry, vid);
+	cpsw_ale_set_vlan_untag_force(ale_entry, force_untag_egress);
+	cpsw_ale_set_vlan_reg_mcast(ale_entry, reg_mcast);
+	cpsw_ale_set_vlan_unreg_mcast(ale_entry, unreg_mcast);
+	cpsw_ale_set_vlan_member_list(ale_entry, member_list);
 
-	idx = cpsw_ale_match_addr(ale, addr, (flags & ALE_VLAN) ? vid : 0);
+	idx = cpsw_ale_match_vlan(ale, vid);
 	if (idx < 0)
 		idx = cpsw_ale_match_free(ale);
 	if (idx < 0)
@@ -326,44 +433,44 @@ int cpsw_ale_add_ucast(struct cpsw_ale *ale, u8 *addr, int port,
 		return -ENOMEM;
 
 	cpsw_ale_write(ale, idx, ale_entry);
+
 	return 0;
 }
 
-int cpsw_ale_del_ucast(struct cpsw_ale *ale, u8 *addr, int port,
-		       int flags, u16 vid)
+int cpsw_ale_del_vlan(struct cpsw_ale *ale, int vid)
 {
 	u32 ale_entry[ALE_ENTRY_WORDS] = {0, 0, 0};
 	int idx;
 
-	idx = cpsw_ale_match_addr(ale, addr, (flags & ALE_VLAN) ? vid : 0);
+	idx = cpsw_ale_match_vlan(ale, vid);
 	if (idx < 0)
 		return -ENOENT;
 
 	cpsw_ale_set_entry_type(ale_entry, ALE_TYPE_FREE);
 	cpsw_ale_write(ale, idx, ale_entry);
+
 	return 0;
 }
 
-int cpsw_ale_add_mcast(struct cpsw_ale *ale, u8 *addr, int port_mask,
-		       int flags, u16 vid, int mcast_state)
+int cpsw_ale_add_ucast(struct cpsw_ale *ale, u8 *addr, int port,
+		       int flags, int vid)
 {
 	u32 ale_entry[ALE_ENTRY_WORDS] = {0, 0, 0};
-	int idx, mask;
-
-	idx = cpsw_ale_match_addr(ale, addr, (flags & ALE_VLAN) ? vid : 0);
-	if (idx >= 0)
-		cpsw_ale_read(ale, idx, ale_entry);
+	int idx;
 
-	cpsw_ale_set_vlan_entry_type(ale_entry, flags, vid);
+	if (vid >= 0) {
+		cpsw_ale_set_entry_type(ale_entry, ALE_TYPE_VLAN_ADDR);
+		cpsw_ale_set_vlan_id(ale_entry, vid);
+	} else
+		cpsw_ale_set_entry_type(ale_entry, ALE_TYPE_ADDR);
 
 	cpsw_ale_set_addr(ale_entry, addr);
-	cpsw_ale_set_super(ale_entry, (flags & ALE_BLOCKED) ? 1 : 0);
-	cpsw_ale_set_mcast_state(ale_entry, mcast_state);
-
-	mask = cpsw_ale_get_port_mask(ale_entry);
-	port_mask |= mask;
-	cpsw_ale_set_port_mask(ale_entry, port_mask);
+	cpsw_ale_set_ucast_type(ale_entry, ALE_UCAST_PERSISTANT);
+	cpsw_ale_set_secure(ale_entry, (flags & ALE_SECURE) ? 1 : 0);
+	cpsw_ale_set_blocked(ale_entry, (flags & ALE_BLOCKED) ? 1 : 0);
+	cpsw_ale_set_port_num(ale_entry, port);
 
+	idx = cpsw_ale_match_addr(ale, addr, vid);
 	if (idx < 0)
 		idx = cpsw_ale_match_free(ale);
 	if (idx < 0)
@@ -375,44 +482,43 @@ int cpsw_ale_add_mcast(struct cpsw_ale *ale, u8 *addr, int port_mask,
 	return 0;
 }
 
-int cpsw_ale_del_mcast(struct cpsw_ale *ale, u8 *addr, int port_mask,
-		       int flags, u16 vid)
+int cpsw_ale_del_ucast(struct cpsw_ale *ale, u8 *addr, int port, int vid)
 {
 	u32 ale_entry[ALE_ENTRY_WORDS] = {0, 0, 0};
 	int idx;
 
-	idx = cpsw_ale_match_addr(ale, addr, (flags & ALE_VLAN) ? vid : 0);
+	idx = cpsw_ale_match_addr(ale, addr, vid);
 	if (idx < 0)
-		return -EINVAL;
-
-	cpsw_ale_read(ale, idx, ale_entry);
-
-	if (port_mask)
-		cpsw_ale_set_port_mask(ale_entry, port_mask);
-	else
-		cpsw_ale_set_entry_type(ale_entry, ALE_TYPE_FREE);
+		return -ENOENT;
 
+	cpsw_ale_set_entry_type(ale_entry, ALE_TYPE_FREE);
 	cpsw_ale_write(ale, idx, ale_entry);
 	return 0;
 }
 
-int cpsw_ale_add_vlan(struct cpsw_ale *ale, u16 vid, int port, int untag,
-		      int reg_mcast, int unreg_mcast)
+int cpsw_ale_add_mcast(struct cpsw_ale *ale, u8 *addr, int port_mask,
+			int super, int mcast_state, int vid)
 {
 	u32 ale_entry[ALE_ENTRY_WORDS] = {0, 0, 0};
-	int idx;
+	int idx, mask;
 
-	idx = cpsw_ale_match_vlan(ale, vid);
+	idx = cpsw_ale_match_addr(ale, addr, vid);
 	if (idx >= 0)
 		cpsw_ale_read(ale, idx, ale_entry);
 
-	cpsw_ale_set_entry_type(ale_entry, ALE_TYPE_VLAN);
-	cpsw_ale_set_vlan_id(ale_entry, vid);
+	if (vid >= 0) {
+		cpsw_ale_set_entry_type(ale_entry, ALE_TYPE_VLAN_ADDR);
+		cpsw_ale_set_vlan_id(ale_entry, vid);
+	} else
+		cpsw_ale_set_entry_type(ale_entry, ALE_TYPE_ADDR);
 
-	cpsw_ale_set_vlan_untag_force(ale_entry, untag);
-	cpsw_ale_set_vlan_reg_mcast(ale_entry, reg_mcast);
-	cpsw_ale_set_vlan_unreg_mcast(ale_entry, unreg_mcast);
-	cpsw_ale_set_vlan_member_list(ale_entry, port);
+	cpsw_ale_set_addr(ale_entry, addr);
+	cpsw_ale_set_super(ale_entry, super);
+	cpsw_ale_set_mcast_state(ale_entry, mcast_state);
+
+	mask = cpsw_ale_get_port_mask(ale_entry);
+	port_mask |= mask;
+	cpsw_ale_set_port_mask(ale_entry, port_mask);
 
 	if (idx < 0)
 		idx = cpsw_ale_match_free(ale);
@@ -425,19 +531,19 @@ int cpsw_ale_add_vlan(struct cpsw_ale *ale, u16 vid, int port, int untag,
 	return 0;
 }
 
-int cpsw_ale_del_vlan(struct cpsw_ale *ale, u16 vid, int port_mask)
+int cpsw_ale_del_mcast(struct cpsw_ale *ale, u8 *addr, int port_mask, int vid)
 {
 	u32 ale_entry[ALE_ENTRY_WORDS] = {0, 0, 0};
 	int idx;
 
-	idx = cpsw_ale_match_vlan(ale, vid);
+	idx = cpsw_ale_match_addr(ale, addr, vid);
 	if (idx < 0)
-		return -ENOENT;
+		return -EINVAL;
 
 	cpsw_ale_read(ale, idx, ale_entry);
 
 	if (port_mask)
-		cpsw_ale_set_vlan_member_list(ale_entry, port_mask);
+		cpsw_ale_set_port_mask(ale_entry, port_mask);
 	else
 		cpsw_ale_set_entry_type(ale_entry, ALE_TYPE_FREE);
 
@@ -453,6 +559,14 @@ struct ale_control_info {
 };
 
 static const struct ale_control_info ale_controls[ALE_NUM_CONTROLS] = {
+	[ALE_VERSION]		= {
+		.name		= "version",
+		.offset		= ALE_IDVER,
+		.port_offset	= 0,
+		.shift		= 0,
+		.port_shift	= 0,
+		.bits		= 32,
+	},
 	[ALE_ENABLE]		= {
 		.name		= "enable",
 		.offset		= ALE_CONTROL,
@@ -477,6 +591,14 @@ static const struct ale_control_info ale_controls[ALE_NUM_CONTROLS] = {
 		.port_shift	= 0,
 		.bits		= 1,
 	},
+	[ALE_UNI_FLOOD]		= {
+		.name		= "p0_uni_flood_en",
+		.offset		= ALE_CONTROL,
+		.port_offset	= 0,
+		.shift		= 8,
+		.port_shift	= 0,
+		.bits		= 1,
+	},
 	[ALE_VLAN_NOLEARN]	= {
 		.name		= "vlan_nolearn",
 		.offset		= ALE_CONTROL,
@@ -614,7 +736,7 @@ static const struct ale_control_info ale_controls[ALE_NUM_CONTROLS] = {
 		.bits		= 6,
 	},
 	[ALE_PORT_UNTAGGED_EGRESS] = {
-		.name		= "untagged_egress",
+		.name		= "unknown_force_untag_egress",
 		.offset		= ALE_UNKNOWNVLAN,
 		.port_offset	= 0,
 		.shift		= 24,
@@ -677,6 +799,622 @@ int cpsw_ale_control_get(struct cpsw_ale *ale, int port, int control)
 	return tmp & BITMASK(info->bits);
 }
 
+static ssize_t cpsw_ale_control_show(struct device *dev,
+				     struct device_attribute *attr,
+				     char *buf)
+{
+	int i, port, len = 0;
+	const struct ale_control_info *info;
+	struct cpsw_ale *ale = control_attr_to_ale(attr);
+	u32 reg;
+
+	for (i = 0, info = ale_controls; i < ALE_NUM_CONTROLS; i++, info++) {
+		if (i == ALE_VERSION) {
+			reg = cpsw_ale_control_get(ale, 0, i);
+			len += snprintf(buf + len, SZ_4K - len,
+					"%s=(ALE_ID=0x%04x) Rev %d.%d\n",
+					info->name,
+					(reg & 0xffff0000) >> 16,
+					ALE_VERSION_MAJOR(reg),
+					ALE_VERSION_MINOR(reg));
+			continue;
+		}
+
+		/* global controls */
+		if (info->port_shift == 0 &&  info->port_offset == 0) {
+			len += snprintf(buf + len, SZ_4K - len,
+					"%s=%d\n", info->name,
+					cpsw_ale_control_get(ale, 0, i));
+			continue;
+		}
+
+		/* port specific controls */
+		for (port = 0; port < ale->params.ale_ports; port++) {
+			len += snprintf(buf + len, SZ_4K - len,
+					"%s.%d=%d\n", info->name, port,
+					cpsw_ale_control_get(ale, port, i));
+		}
+	}
+
+	return len;
+}
+
+static ssize_t cpsw_ale_control_store(struct device *dev,
+				      struct device_attribute *attr,
+				      const char *buf, size_t count)
+{
+	char ctrl_str[33], tmp_str[9];
+	int port = 0, value, len, ret, control;
+	unsigned long end;
+	struct cpsw_ale *ale = control_attr_to_ale(attr);
+
+	len = strcspn(buf, ".=");
+	if (len >= 32)
+		return -ENOMEM;
+
+	strncpy(ctrl_str, buf, len);
+	ctrl_str[len] = '\0';
+	buf += len;
+
+	if (*buf == '.') {
+		++buf;
+		len = strcspn(buf, "=");
+		if (len >= 8)
+			return -ENOMEM;
+		strncpy(tmp_str, buf, len);
+		tmp_str[len] = '\0';
+		if (kstrtoul(tmp_str, 0, &end))
+			return -EINVAL;
+		port = (int)end;
+		buf += len;
+	}
+
+	if (*buf != '=')
+		return -EINVAL;
+
+	if (kstrtoul(buf + 1, 0, &end))
+		return -EINVAL;
+
+	value = (int)end;
+
+	for (control = 0; control < ALE_NUM_CONTROLS; control++)
+		if (strcmp(ctrl_str, ale_controls[control].name) == 0)
+			break;
+
+	if (control >= ALE_NUM_CONTROLS)
+		return -ENOENT;
+
+	dev_dbg(ale->params.dev, "processing command %s.%d=%d\n",
+		ale_controls[control].name, port, value);
+
+	ret = cpsw_ale_control_set(ale, port, control, value);
+	if (ret < 0)
+		return ret;
+
+	return count;
+}
+DEVICE_ATTR(ale_control, S_IRUGO | S_IWUSR,
+	cpsw_ale_control_show, cpsw_ale_control_store);
+
+static ssize_t cpsw_ale_table_show(struct device *dev,
+				   struct device_attribute *attr,
+				   char *buf)
+{
+	int len = SZ_4K, outlen = 0, idx, start;
+	u32 ale_entry[ALE_ENTRY_WORDS];
+	struct cpsw_ale *ale = table_attr_to_ale(attr);
+	int not_shown = 0, total_outlen = 0, type, shown = 0;
+
+	start = ale->show_next;
+
+	for (idx = start; (idx < ale->params.ale_entries) &&
+			(len > total_outlen); idx++) {
+		cpsw_ale_read(ale, idx, ale_entry);
+		outlen = cpsw_ale_dump_entry(idx, ale_entry,
+				buf + total_outlen, len - total_outlen);
+		if (outlen == 0) {
+			type = cpsw_ale_get_entry_type(ale_entry);
+			if (type != ALE_TYPE_FREE) {
+				++not_shown;
+				break;
+			}
+		} else {
+			total_outlen += outlen;
+			++shown;
+		}
+	}
+
+	/* update next show index */
+	if (idx >= ale->params.ale_entries)
+		ale->show_next = 0;
+	else
+		ale->show_next = idx;
+
+	if (len > total_outlen + 32)
+		total_outlen += snprintf(buf + total_outlen, len - total_outlen,
+				"[%d..%d]: %d entries%s\n", start, idx - 1,
+				shown, not_shown ? ", +" : "");
+
+	return total_outlen;
+}
+
+struct ale_table_param {
+	const char *name;
+	union	{
+		int	val;
+		u8	addr[6];
+	};
+};
+
+struct ale_table_cmd {
+	const char *name;
+	int (*process)(struct cpsw_ale *ale,
+		const u8 *params_str, size_t str_len);
+};
+
+static struct ale_table_param vlan_params[] = {
+	[ALE_VP_VID]		= { .name = "vid", },
+	[ALE_VP_FORCE_UT_EGR]	= { .name = "force_untag_egress", },
+	[ALE_VP_REG_FLD]	= { .name = "reg_fld_mask", },
+	[ALE_VP_UNREG_FLD]	= { .name = "unreg_fld_mask", },
+	[ALE_VP_M_LIST]		= { .name = "mem_list", },
+};
+
+static struct ale_table_param vlan_ucast_params[] = {
+	[ALE_UP_PORT]		= { .name = "port", },
+	[ALE_UP_BLOCK]		= { .name = "block", },
+	[ALE_UP_SECURE]		= { .name = "secure", },
+	[ALE_UP_AGEABLE]	= { .name = "ageable", },
+	[ALE_UP_ADDR]		= { .name = "addr", },
+	[ALE_UP_VID]		= { .name = "vid", },
+};
+
+static struct ale_table_param vlan_mcast_params[] = {
+	[ALE_MP_PORT_MASK]	= { .name = "port_mask", },
+	[ALE_MP_SUPER]		= { .name = "supervisory", },
+	[ALE_MP_FW_ST]		= { .name = "mc_fw_st", },
+	[ALE_MP_ADDR]		= { .name = "addr", },
+	[ALE_MP_VID]		= { .name = "vid", },
+};
+
+static struct ale_table_param oui_params[] = {
+	{ .name	= "addr", },
+};
+
+void cpsw_ale_table_store_init_params(
+	struct ale_table_param *params, int param_num)
+{
+	int i;
+
+	for (i = 0; i < param_num; i++)
+		memset(params[i].addr, 0, 6);
+}
+
+int cpsw_ale_table_store_get_params(struct cpsw_ale *ale,
+	struct ale_table_param *params, int param_num,
+	const u8 *params_str, size_t str_len)
+{
+	char param_name[33], val_str[33];
+	size_t tmp_len = str_len;
+	unsigned int iaddr[6];
+	unsigned long end;
+	int len, i, n, addr_len;
+
+	while (tmp_len > 0) {
+		len = strcspn(params_str, "=");
+		if (len >= 32)
+			return -ENOMEM;
+
+		strncpy(param_name, params_str, len);
+		param_name[len] = '\0';
+		params_str += len;
+		tmp_len -= len;
+
+		if (*params_str != '=')
+			return -EINVAL;
+
+		++params_str;
+		--tmp_len;
+
+		len = strcspn(params_str, ".");
+		if (len >= 32)
+			return -ENOMEM;
+
+		strncpy(val_str, params_str, len);
+		val_str[len] = '\0';
+		params_str += len;
+		tmp_len -= len;
+
+		if (*params_str == '.') {
+			++params_str;
+			--tmp_len;
+		}
+
+		for (n = 0; n < param_num; n++) {
+			if (strcmp(param_name, params[n].name) != 0)
+				continue;
+
+			if (strcmp(param_name, "addr") == 0) {
+				addr_len = sscanf(val_str,
+					"%02x:%02x:%02x:%02x:%02x:%02x",
+					&iaddr[0], &iaddr[1], &iaddr[2],
+					&iaddr[3], &iaddr[4], &iaddr[5]);
+				if (addr_len != 6 && addr_len != 3)
+					return -EINVAL;
+
+				for (i = 0; i < addr_len; i++)
+					params[n].addr[i] = iaddr[i];
+
+				break;
+			}
+
+			if (kstrtoul(val_str, 0, &end))
+				return -EINVAL;
+
+			params[n].val = (int)end;
+			break;
+		}
+
+		if (n >= param_num)
+			return -EINVAL;
+	}
+
+	return str_len;
+}
+
+int cpsw_ale_table_store_vlan(struct cpsw_ale *ale,
+			const u8 *params_str, size_t str_len)
+{
+	int ret;
+
+	cpsw_ale_table_store_init_params(vlan_params, ALE_VP_NUM);
+	vlan_params[ALE_VP_VID].val = -1;
+
+	ret = cpsw_ale_table_store_get_params(ale,
+		vlan_params, ALE_VP_NUM, params_str, str_len);
+
+	if (ret < 0)
+		return ret;
+
+	ret = cpsw_ale_add_vlan(ale,
+		vlan_params[ALE_VP_VID].val,
+		vlan_params[ALE_VP_M_LIST].val,
+		vlan_params[ALE_VP_REG_FLD].val,
+		vlan_params[ALE_VP_UNREG_FLD].val,
+		vlan_params[ALE_VP_FORCE_UT_EGR].val);
+
+	if (ret < 0)
+		return ret;
+	else
+		return str_len;
+}
+
+int cpsw_ale_table_store_vlan_ucast(struct cpsw_ale *ale,
+		const u8 *params_str, size_t str_len, int has_vid)
+{
+	int ret, flags = 0;
+
+	cpsw_ale_table_store_init_params(vlan_ucast_params, ALE_UP_NUM);
+	vlan_ucast_params[ALE_UP_VID].val = -1;
+
+	ret = cpsw_ale_table_store_get_params(ale,
+		vlan_ucast_params, ALE_UP_NUM,
+		params_str, str_len);
+
+	if (ret < 0)
+		return ret;
+
+	if (!has_vid && vlan_ucast_params[ALE_UP_VID].val >= 0)
+		return -EINVAL;
+
+	if (vlan_ucast_params[ALE_UP_BLOCK].val)
+		flags |= ALE_BLOCKED;
+
+	if (vlan_ucast_params[ALE_UP_SECURE].val)
+		flags |= ALE_SECURE;
+
+	cpsw_ale_add_ucast(ale,
+		vlan_ucast_params[ALE_UP_ADDR].addr,
+		vlan_ucast_params[ALE_UP_PORT].val,
+		flags,
+		vlan_ucast_params[ALE_UP_VID].val);
+
+	return str_len;
+}
+
+int cpsw_ale_table_store_u_proc(struct cpsw_ale *ale,
+		const u8 *params_str, size_t str_len)
+{
+	return  cpsw_ale_table_store_vlan_ucast(ale, params_str, str_len, 0);
+}
+
+int cpsw_ale_table_store_vu_proc(struct cpsw_ale *ale,
+		const u8 *params_str, size_t str_len)
+{
+	return  cpsw_ale_table_store_vlan_ucast(ale, params_str, str_len, 1);
+}
+
+int cpsw_ale_table_store_vlan_mcast(struct cpsw_ale *ale,
+		const u8 *params_str, size_t str_len, int has_vid)
+{
+	int ret;
+
+	cpsw_ale_table_store_init_params(vlan_mcast_params, ALE_MP_NUM);
+	vlan_mcast_params[ALE_MP_VID].val = -1;
+
+	ret = cpsw_ale_table_store_get_params(ale,
+		vlan_mcast_params, ALE_MP_NUM,
+		params_str, str_len);
+
+	if (ret < 0)
+		return ret;
+
+	if (!has_vid && vlan_mcast_params[ALE_MP_VID].val >= 0)
+		return -EINVAL;
+
+	cpsw_ale_add_mcast(ale,
+		vlan_mcast_params[ALE_MP_ADDR].addr,
+		vlan_mcast_params[ALE_MP_PORT_MASK].val,
+		vlan_mcast_params[ALE_MP_SUPER].val,
+		vlan_mcast_params[ALE_MP_FW_ST].val,
+		vlan_mcast_params[ALE_MP_VID].val);
+
+	return str_len;
+}
+
+int cpsw_ale_table_store_m_proc(struct cpsw_ale *ale,
+			const u8 *params_str, size_t str_len)
+{
+	return  cpsw_ale_table_store_vlan_mcast(ale, params_str, str_len, 0);
+}
+
+int cpsw_ale_table_store_vm_proc(struct cpsw_ale *ale,
+			const u8 *params_str, size_t str_len)
+{
+	return  cpsw_ale_table_store_vlan_mcast(ale, params_str, str_len, 1);
+}
+
+int cpsw_ale_add_oui(struct cpsw_ale *ale, u8 *addr)
+{
+	u32 ale_entry[ALE_ENTRY_WORDS] = {0, 0, 0};
+	int idx;
+
+	cpsw_ale_set_entry_type(ale_entry, ALE_TYPE_ADDR);
+
+	cpsw_ale_set_addr(ale_entry, addr);
+	cpsw_ale_set_ucast_type(ale_entry, ALE_UCAST_OUI);
+
+	idx = cpsw_ale_match_addr(ale, addr, -1);
+	if (idx < 0)
+		idx = cpsw_ale_match_free(ale);
+	if (idx < 0)
+		idx = cpsw_ale_find_ageable(ale);
+	if (idx < 0)
+		return -ENOMEM;
+
+	cpsw_ale_write(ale, idx, ale_entry);
+	return 0;
+}
+
+int cpsw_ale_table_store_oui(struct cpsw_ale *ale,
+			const u8 *params_str, size_t str_len)
+{
+	int ret;
+
+	cpsw_ale_table_store_init_params(oui_params, 1);
+
+	ret = cpsw_ale_table_store_get_params(ale,
+		oui_params, 1, params_str, str_len);
+
+	if (ret < 0)
+		return ret;
+
+	/* Clear out the don't cares */
+	oui_params[0].addr[3] = 0;
+	oui_params[0].addr[4] = 0;
+	oui_params[0].addr[5] = 0;
+
+	cpsw_ale_add_oui(ale, oui_params[0].addr);
+
+	return str_len;
+}
+
+int cpsw_ale_table_store_del(struct cpsw_ale *ale, int idx)
+{
+	u32 ale_entry[ALE_ENTRY_WORDS];
+	int type;
+
+	dev_dbg(ale->params.dev, "deleting entry[%d] ...\n", idx);
+
+	if (idx >= ale->params.ale_entries)
+		return -EINVAL;
+
+	cpsw_ale_read(ale, idx, ale_entry);
+
+	type = cpsw_ale_get_entry_type(ale_entry);
+	if (type == ALE_TYPE_FREE)
+		return -EINVAL;
+
+	cpsw_ale_set_entry_type(ale_entry, ALE_TYPE_FREE);
+	cpsw_ale_write(ale, idx, ale_entry);
+	return 0;
+}
+
+static struct ale_table_cmd ale_table_cmds[] = {
+	{
+		.name		= "v",
+		.process	= cpsw_ale_table_store_vlan,
+	},
+	{
+		.name		= "m",
+		.process	= cpsw_ale_table_store_m_proc,
+	},
+	{
+		.name		= "vm",
+		.process	= cpsw_ale_table_store_vm_proc,
+	},
+	{
+		.name		= "u",
+		.process	= cpsw_ale_table_store_u_proc,
+	},
+	{
+		.name		= "vu",
+		.process	= cpsw_ale_table_store_vu_proc,
+	},
+	{
+		.name		= "o",
+		.process	= cpsw_ale_table_store_oui,
+	},
+};
+
+static ssize_t cpsw_ale_table_store_proc(struct cpsw_ale *ale,
+				const char *buf, size_t count)
+{
+	char ctrl_str[33];
+	unsigned long end;
+	int len, i, tmp_count = count, ret = -EINVAL;
+
+	len = strcspn(buf, ".:");
+	if (len >= 5)
+		return -ENOMEM;
+
+	strncpy(ctrl_str, buf, len);
+	ctrl_str[len] = '\0';
+
+	/* skip to param beginning */
+	buf += len;
+	tmp_count -= len;
+
+	if (*buf == ':') {
+		/* delete cmd */
+		if (kstrtoul(ctrl_str, 0, &end))
+			return -EINVAL;
+		ret = cpsw_ale_table_store_del(ale, end);
+		if (ret != 0)
+			return ret;
+		else
+			return count;
+	}
+
+	if (len >= 3)
+		return -ENOMEM;
+
+	if (*buf != '.')
+		return -EINVAL;
+
+	++buf;
+	--tmp_count;
+
+	for (i = 0; i < ARRAY_SIZE(ale_table_cmds); i++) {
+		if (strcmp(ale_table_cmds[i].name, ctrl_str) == 0) {
+			ret = ale_table_cmds[i].process(ale, buf, tmp_count);
+			break;
+		}
+	}
+
+	if (ret < 0)
+		return ret;
+	else
+		return count;
+}
+
+static ssize_t cpsw_ale_table_store(struct device *dev,
+				      struct device_attribute *attr,
+				      const char *buf, size_t count)
+{
+	struct cpsw_ale *ale = table_attr_to_ale(attr);
+
+	return cpsw_ale_table_store_proc(ale, buf, count);
+}
+DEVICE_ATTR(ale_table, S_IRUGO | S_IWUSR,
+	cpsw_ale_table_show, cpsw_ale_table_store);
+
+static int cpsw_ale_dump_entry_raw(int idx, u32 *ale_entry, char *buf, int len)
+{
+	int type, outlen = 0;
+
+	type = cpsw_ale_get_entry_type(ale_entry);
+	if (type == ALE_TYPE_FREE)
+		return outlen;
+
+	if (len < ALE_RAW_TBL_ENTRY_SHOW_LEN)
+		return outlen;
+
+	if (idx >= 0)
+		outlen += snprintf(buf + outlen, len - outlen,
+				   "%d: ", idx);
+
+	outlen += snprintf(buf + outlen, len - outlen, "%02x %08x %08x\n",
+			   ale_entry[0], ale_entry[1], ale_entry[2]);
+
+	return outlen;
+}
+
+static ssize_t cpsw_ale_table_raw_show(struct device *dev,
+				   struct device_attribute *attr,
+				   char *buf)
+{
+	struct cpsw_ale *ale = table_raw_attr_to_ale(attr);
+	int not_shown = 0, total_outlen = 0, shown = 0;
+	int outlen = 0, idx, start, type;
+	u32 ale_entry[ALE_ENTRY_WORDS];
+
+	start = ale->raw_show_next;
+
+	for (idx = start; (idx < ale->params.ale_entries) &&
+				(PAGE_SIZE > total_outlen); idx++) {
+		cpsw_ale_read(ale, idx, ale_entry);
+		outlen = cpsw_ale_dump_entry_raw(idx, ale_entry,
+					buf + total_outlen,
+					PAGE_SIZE - total_outlen);
+		if (outlen == 0) {
+			type = cpsw_ale_get_entry_type(ale_entry);
+			if (type != ALE_TYPE_FREE) {
+				++not_shown;
+				break;
+			}
+		} else {
+			total_outlen += outlen;
+			++shown;
+		}
+	}
+
+	/* update next show index */
+	if (idx >= ale->params.ale_entries)
+		ale->raw_show_next = 0;
+	else
+		ale->raw_show_next = idx;
+
+	if (PAGE_SIZE > total_outlen + 32)
+		total_outlen += snprintf(buf + total_outlen,
+			PAGE_SIZE - total_outlen,
+			"[%d..%d]: %d entries%s\n",
+			start, idx - 1, shown, not_shown ? ", +" : "");
+
+	return total_outlen;
+}
+
+static ssize_t cpsw_ale_table_raw_store(struct device *dev,
+			      struct device_attribute *attr,
+			      const char *buf, size_t count)
+{
+	struct cpsw_ale *ale = table_raw_attr_to_ale(attr);
+	unsigned long end;
+
+	if (kstrtoul(buf, 0, &end) == 0) {
+		/* set start-show-index command */
+		ale->raw_show_next = (int)end;
+		if (ale->raw_show_next >= ale->params.ale_entries)
+			ale->raw_show_next = 0;
+		return count;
+	}
+
+	/* add or delete command */
+	return cpsw_ale_table_store_proc(ale, buf, count);
+}
+DEVICE_ATTR(ale_table_raw, S_IRUGO | S_IWUSR,
+	cpsw_ale_table_raw_show, cpsw_ale_table_raw_store);
+
 static void cpsw_ale_timer(unsigned long arg)
 {
 	struct cpsw_ale *ale = (struct cpsw_ale *)arg;
@@ -703,13 +1441,26 @@ int cpsw_ale_set_ageout(struct cpsw_ale *ale, int ageout)
 void cpsw_ale_start(struct cpsw_ale *ale)
 {
 	u32 rev;
+	int ret;
 
 	rev = __raw_readl(ale->params.ale_regs + ALE_IDVER);
-	dev_dbg(ale->params.dev, "initialized cpsw ale revision %d.%d\n",
+	dev_info(ale->params.dev, "initialized cpsw ale revision %d.%d\n",
 		ALE_VERSION_MAJOR(rev), ALE_VERSION_MINOR(rev));
 	cpsw_ale_control_set(ale, 0, ALE_ENABLE, 1);
 	cpsw_ale_control_set(ale, 0, ALE_CLEAR, 1);
 
+	ale->ale_control_attr = dev_attr_ale_control;
+	ret = device_create_file(ale->params.dev, &ale->ale_control_attr);
+	WARN_ON(ret < 0);
+
+	ale->ale_table_attr = dev_attr_ale_table;
+	ret = device_create_file(ale->params.dev, &ale->ale_table_attr);
+	WARN_ON(ret < 0);
+
+	ale->ale_table_raw_attr = dev_attr_ale_table_raw;
+	ret = device_create_file(ale->params.dev, &ale->ale_table_raw_attr);
+	WARN_ON(ret < 0);
+
 	init_timer(&ale->timer);
 	ale->timer.data	    = (unsigned long)ale;
 	ale->timer.function = cpsw_ale_timer;
@@ -722,6 +1473,9 @@ void cpsw_ale_start(struct cpsw_ale *ale)
 void cpsw_ale_stop(struct cpsw_ale *ale)
 {
 	del_timer_sync(&ale->timer);
+	device_remove_file(ale->params.dev, &ale->ale_table_attr);
+	device_remove_file(ale->params.dev, &ale->ale_control_attr);
+	device_remove_file(ale->params.dev, &ale->ale_table_raw_attr);
 }
 
 struct cpsw_ale *cpsw_ale_create(struct cpsw_ale_params *params)
diff --git a/drivers/net/ethernet/ti/cpsw_ale.h b/drivers/net/ethernet/ti/cpsw_ale.h
index 30daa12..ed33762 100644
--- a/drivers/net/ethernet/ti/cpsw_ale.h
+++ b/drivers/net/ethernet/ti/cpsw_ale.h
@@ -27,13 +27,26 @@ struct cpsw_ale {
 	struct cpsw_ale_params	params;
 	struct timer_list	timer;
 	unsigned long		ageout;
+	struct device_attribute ale_control_attr;
+#define control_attr_to_ale(attr)	\
+	container_of(attr, struct cpsw_ale, ale_control_attr)
+	struct device_attribute ale_table_attr;
+#define table_attr_to_ale(attr)		\
+	container_of(attr, struct cpsw_ale, ale_table_attr)
+	struct device_attribute ale_table_raw_attr;
+#define table_raw_attr_to_ale(attr)		\
+	container_of(attr, struct cpsw_ale, ale_table_raw_attr)
+	int show_next;
+	int raw_show_next;
 };
 
 enum cpsw_ale_control {
 	/* global */
+	ALE_VERSION,
 	ALE_ENABLE,
 	ALE_CLEAR,
 	ALE_AGEOUT,
+	ALE_UNI_FLOOD,
 	ALE_VLAN_NOLEARN,
 	ALE_NO_PORT_VLAN,
 	ALE_OUI_DENY,
@@ -64,14 +77,8 @@ enum cpsw_ale_port_state {
 };
 
 /* ALE unicast entry flags - passed into cpsw_ale_add_ucast() */
-#define ALE_SECURE			BIT(0)
-#define ALE_BLOCKED			BIT(1)
-#define ALE_SUPER			BIT(2)
-#define ALE_VLAN			BIT(3)
-
-#define ALE_PORT_HOST			BIT(0)
-#define ALE_PORT_1			BIT(1)
-#define ALE_PORT_2			BIT(2)
+#define ALE_SECURE			1
+#define ALE_BLOCKED			2
 
 #define ALE_MCAST_FWD			0
 #define ALE_MCAST_BLOCK_LEARN_FWD	1
@@ -88,19 +95,49 @@ int cpsw_ale_set_ageout(struct cpsw_ale *ale, int ageout);
 int cpsw_ale_flush(struct cpsw_ale *ale, int port_mask);
 int cpsw_ale_flush_multicast(struct cpsw_ale *ale, int port_mask);
 int cpsw_ale_add_ucast(struct cpsw_ale *ale, u8 *addr, int port,
-		       int flags, u16 vid);
-int cpsw_ale_del_ucast(struct cpsw_ale *ale, u8 *addr, int port,
-		       int flags, u16 vid);
+		       int flags, int vid);
+int cpsw_ale_del_ucast(struct cpsw_ale *ale, u8 *addr, int port, int vid);
 int cpsw_ale_add_mcast(struct cpsw_ale *ale, u8 *addr, int port_mask,
-		       int flags, u16 vid, int mcast_state);
-int cpsw_ale_del_mcast(struct cpsw_ale *ale, u8 *addr, int port_mask,
-		       int flags, u16 vid);
-int cpsw_ale_add_vlan(struct cpsw_ale *ale, u16 vid, int port, int untag,
-			int reg_mcast, int unreg_mcast);
-int cpsw_ale_del_vlan(struct cpsw_ale *ale, u16 vid, int port);
+			int super, int mcast_state, int vid);
+int cpsw_ale_del_mcast(struct cpsw_ale *ale, u8 *addr, int port_mask, int vid);
+
+int cpsw_ale_add_vlan(struct cpsw_ale *ale, int vid, int member_list,
+		      int reg_mcast, int unreg_mcast, int force_untag_egress);
+int cpsw_ale_del_vlan(struct cpsw_ale *ale, int vid);
 
 int cpsw_ale_control_get(struct cpsw_ale *ale, int port, int control);
 int cpsw_ale_control_set(struct cpsw_ale *ale, int port,
 			 int control, int value);
 
+/* ALE Table store VLAN command param indices */
+enum {
+	ALE_VP_VID,
+	ALE_VP_FORCE_UT_EGR,
+	ALE_VP_REG_FLD,
+	ALE_VP_UNREG_FLD,
+	ALE_VP_M_LIST,
+	ALE_VP_NUM,
+};
+
+/* ALE Table store UCAST command param indices */
+enum {
+	ALE_UP_PORT,
+	ALE_UP_BLOCK,
+	ALE_UP_SECURE,
+	ALE_UP_AGEABLE,
+	ALE_UP_ADDR,
+	ALE_UP_VID,
+	ALE_UP_NUM,
+};
+
+/* ALE Table store MCAST command param indices */
+enum {
+	ALE_MP_PORT_MASK,
+	ALE_MP_SUPER,
+	ALE_MP_FW_ST,
+	ALE_MP_ADDR,
+	ALE_MP_VID,
+	ALE_MP_NUM
+};
+
 #endif
diff --git a/drivers/net/ethernet/ti/cpts.c b/drivers/net/ethernet/ti/cpts.c
index 8c351f1..463597f 100644
--- a/drivers/net/ethernet/ti/cpts.c
+++ b/drivers/net/ethernet/ti/cpts.c
@@ -94,7 +94,7 @@ static int cpts_fifo_read(struct cpts *cpts, int match)
 		case CPTS_EV_HW:
 			break;
 		default:
-			pr_err("cpts: unknown event type\n");
+			pr_err("cpts: unkown event type\n");
 			break;
 		}
 		if (type == match)
diff --git a/drivers/net/ethernet/ti/davinci_cpdma.c b/drivers/net/ethernet/ti/davinci_cpdma.c
index 053c84f..4995673 100644
--- a/drivers/net/ethernet/ti/davinci_cpdma.c
+++ b/drivers/net/ethernet/ti/davinci_cpdma.c
@@ -20,7 +20,6 @@
 #include <linux/err.h>
 #include <linux/dma-mapping.h>
 #include <linux/io.h>
-#include <linux/delay.h>
 
 #include "davinci_cpdma.h"
 
@@ -61,9 +60,6 @@
 #define CPDMA_DESC_EOQ		BIT(28)
 #define CPDMA_DESC_TD_COMPLETE	BIT(27)
 #define CPDMA_DESC_PASS_CRC	BIT(26)
-#define CPDMA_DESC_TO_PORT_EN	BIT(20)
-#define CPDMA_TO_PORT_SHIFT	16
-#define CPDMA_DESC_PORT_MASK	(BIT(18) | BIT(17) | BIT(16))
 
 #define CPDMA_TEARDOWN_VALUE	0xfffffffc
 
@@ -109,13 +105,13 @@ struct cpdma_ctlr {
 };
 
 struct cpdma_chan {
-	struct cpdma_desc __iomem	*head, *tail;
-	void __iomem			*hdp, *cp, *rxfree;
 	enum cpdma_state		state;
 	struct cpdma_ctlr		*ctlr;
 	int				chan_num;
 	spinlock_t			lock;
+	struct cpdma_desc __iomem	*head, *tail;
 	int				count;
+	void __iomem			*hdp, *cp, *rxfree;
 	u32				mask;
 	cpdma_handler_fn		handler;
 	enum dma_data_direction		dir;
@@ -136,14 +132,6 @@ struct cpdma_chan {
 #define chan_write(chan, fld, v)	__raw_writel(v, (chan)->fld)
 #define desc_write(desc, fld, v)	__raw_writel((u32)(v), &(desc)->fld)
 
-#define cpdma_desc_to_port(chan, mode, directed)			\
-	do {								\
-		if (!is_rx_chan(chan) && ((directed == 1) ||		\
-					  (directed == 2)))		\
-			mode |= (CPDMA_DESC_TO_PORT_EN |		\
-				 (directed << CPDMA_TO_PORT_SHIFT));	\
-	} while (0)
-
 /*
  * Utility constructs for a cpdma descriptor pool.  Some devices (e.g. davinci
  * emac) have dedicated on-chip memory for these descriptors.  Some other
@@ -229,27 +217,17 @@ desc_from_phys(struct cpdma_desc_pool *pool, dma_addr_t dma)
 }
 
 static struct cpdma_desc __iomem *
-cpdma_desc_alloc(struct cpdma_desc_pool *pool, int num_desc, bool is_rx)
+cpdma_desc_alloc(struct cpdma_desc_pool *pool, int num_desc)
 {
 	unsigned long flags;
 	int index;
-	int desc_start;
-	int desc_end;
 	struct cpdma_desc __iomem *desc = NULL;
 
 	spin_lock_irqsave(&pool->lock, flags);
 
-	if (is_rx) {
-		desc_start = 0;
-		desc_end = pool->num_desc/2;
-	 } else {
-		desc_start = pool->num_desc/2;
-		desc_end = pool->num_desc;
-	}
-
-	index = bitmap_find_next_zero_area(pool->bitmap,
-				desc_end, desc_start, num_desc, 0);
-	if (index < desc_end) {
+	index = bitmap_find_next_zero_area(pool->bitmap, pool->num_desc, 0,
+					   num_desc, 0);
+	if (index < pool->num_desc) {
 		bitmap_set(pool->bitmap, index, num_desc);
 		desc = pool->iomap + pool->desc_size * index;
 		pool->used_desc++;
@@ -313,16 +291,14 @@ int cpdma_ctlr_start(struct cpdma_ctlr *ctlr)
 	}
 
 	if (ctlr->params.has_soft_reset) {
-		unsigned timeout = 10 * 100;
+		unsigned long timeout = jiffies + HZ/10;
 
 		dma_reg_write(ctlr, CPDMA_SOFTRESET, 1);
-		while (timeout) {
+		while (time_before(jiffies, timeout)) {
 			if (dma_reg_read(ctlr, CPDMA_SOFTRESET) == 0)
 				break;
-			udelay(10);
-			timeout--;
 		}
-		WARN_ON(!timeout);
+		WARN_ON(!time_before(jiffies, timeout));
 	}
 
 	for (i = 0; i < ctlr->num_chan; i++) {
@@ -463,8 +439,10 @@ int cpdma_ctlr_destroy(struct cpdma_ctlr *ctlr)
 	if (ctlr->state != CPDMA_STATE_IDLE)
 		cpdma_ctlr_stop(ctlr);
 
-	for (i = 0; i < ARRAY_SIZE(ctlr->channels); i++)
-		cpdma_chan_destroy(ctlr->channels[i]);
+	for (i = 0; i < ARRAY_SIZE(ctlr->channels); i++) {
+		if (ctlr->channels[i])
+			cpdma_chan_destroy(ctlr->channels[i]);
+	}
 
 	cpdma_desc_pool_destroy(ctlr->pool);
 	spin_unlock_irqrestore(&ctlr->lock, flags);
@@ -495,13 +473,11 @@ int cpdma_ctlr_int_ctrl(struct cpdma_ctlr *ctlr, bool enable)
 	spin_unlock_irqrestore(&ctlr->lock, flags);
 	return 0;
 }
-EXPORT_SYMBOL_GPL(cpdma_ctlr_int_ctrl);
 
-void cpdma_ctlr_eoi(struct cpdma_ctlr *ctlr, u32 value)
+void cpdma_ctlr_eoi(struct cpdma_ctlr *ctlr)
 {
-	dma_reg_write(ctlr, CPDMA_MACEOIVECTOR, value);
+	dma_reg_write(ctlr, CPDMA_MACEOIVECTOR, 0);
 }
-EXPORT_SYMBOL_GPL(cpdma_ctlr_eoi);
 
 struct cpdma_chan *cpdma_chan_create(struct cpdma_ctlr *ctlr, int chan_num,
 				     cpdma_handler_fn handler)
@@ -676,7 +652,7 @@ static void __cpdma_chan_submit(struct cpdma_chan *chan,
 }
 
 int cpdma_chan_submit(struct cpdma_chan *chan, void *token, void *data,
-		      int len, int directed)
+		      int len, gfp_t gfp_mask)
 {
 	struct cpdma_ctlr		*ctlr = chan->ctlr;
 	struct cpdma_desc __iomem	*desc;
@@ -692,7 +668,7 @@ int cpdma_chan_submit(struct cpdma_chan *chan, void *token, void *data,
 		goto unlock_ret;
 	}
 
-	desc = cpdma_desc_alloc(ctlr->pool, 1, is_rx_chan(chan));
+	desc = cpdma_desc_alloc(ctlr->pool, 1);
 	if (!desc) {
 		chan->stats.desc_alloc_fail++;
 		ret = -ENOMEM;
@@ -705,15 +681,7 @@ int cpdma_chan_submit(struct cpdma_chan *chan, void *token, void *data,
 	}
 
 	buffer = dma_map_single(ctlr->dev, data, len, chan->dir);
-	ret = dma_mapping_error(ctlr->dev, buffer);
-	if (ret) {
-		cpdma_desc_free(ctlr->pool, desc, 1);
-		ret = -EINVAL;
-		goto unlock_ret;
-	}
-
 	mode = CPDMA_DESC_OWNER | CPDMA_DESC_SOP | CPDMA_DESC_EOP;
-	cpdma_desc_to_port(chan, mode, directed);
 
 	desc_write(desc, hw_next,   0);
 	desc_write(desc, hw_buffer, buffer);
@@ -736,29 +704,6 @@ unlock_ret:
 }
 EXPORT_SYMBOL_GPL(cpdma_chan_submit);
 
-bool cpdma_check_free_tx_desc(struct cpdma_chan *chan)
-{
-	unsigned long flags;
-	int index;
-	bool ret;
-	struct cpdma_ctlr	*ctlr = chan->ctlr;
-	struct cpdma_desc_pool	*pool = ctlr->pool;
-
-	spin_lock_irqsave(&pool->lock, flags);
-
-	index = bitmap_find_next_zero_area(pool->bitmap,
-				pool->num_desc, pool->num_desc/2, 1, 0);
-
-	if (index < pool->num_desc)
-		ret = true;
-	else
-		ret = false;
-
-	spin_unlock_irqrestore(&pool->lock, flags);
-	return ret;
-}
-EXPORT_SYMBOL_GPL(cpdma_check_free_tx_desc);
-
 static void __cpdma_chan_free(struct cpdma_chan *chan,
 			      struct cpdma_desc __iomem *desc,
 			      int outlen, int status)
@@ -783,7 +728,6 @@ static int __cpdma_chan_process(struct cpdma_chan *chan)
 	struct cpdma_ctlr		*ctlr = chan->ctlr;
 	struct cpdma_desc __iomem	*desc;
 	int				status, outlen;
-	int				cb_status = 0;
 	struct cpdma_desc_pool		*pool = ctlr->pool;
 	dma_addr_t			desc_dma;
 	unsigned long			flags;
@@ -805,8 +749,7 @@ static int __cpdma_chan_process(struct cpdma_chan *chan)
 		status = -EBUSY;
 		goto unlock_ret;
 	}
-	status	= status & (CPDMA_DESC_EOQ | CPDMA_DESC_TD_COMPLETE |
-			    CPDMA_DESC_PORT_MASK);
+	status	= status & (CPDMA_DESC_EOQ | CPDMA_DESC_TD_COMPLETE);
 
 	chan->head = desc_from_phys(pool, desc_read(desc, hw_next));
 	chan_write(chan, cp, desc_dma);
@@ -819,12 +762,8 @@ static int __cpdma_chan_process(struct cpdma_chan *chan)
 	}
 
 	spin_unlock_irqrestore(&chan->lock, flags);
-	if (unlikely(status & CPDMA_DESC_TD_COMPLETE))
-		cb_status = -ENOSYS;
-	else
-		cb_status = status;
 
-	__cpdma_chan_free(chan, desc, outlen, cb_status);
+	__cpdma_chan_free(chan, desc, outlen, status);
 	return status;
 
 unlock_ret:
@@ -883,7 +822,7 @@ int cpdma_chan_stop(struct cpdma_chan *chan)
 	struct cpdma_desc_pool	*pool = ctlr->pool;
 	unsigned long		flags;
 	int			ret;
-	unsigned		timeout;
+	unsigned long		timeout;
 
 	spin_lock_irqsave(&chan->lock, flags);
 	if (chan->state != CPDMA_STATE_ACTIVE) {
@@ -898,15 +837,14 @@ int cpdma_chan_stop(struct cpdma_chan *chan)
 	dma_reg_write(ctlr, chan->td, chan_linear(chan));
 
 	/* wait for teardown complete */
-	timeout = 100 * 100; /* 100 ms */
-	while (timeout) {
+	timeout = jiffies + HZ/10;	/* 100 msec */
+	while (time_before(jiffies, timeout)) {
 		u32 cp = chan_read(chan, cp);
 		if ((cp & CPDMA_TEARDOWN_VALUE) == CPDMA_TEARDOWN_VALUE)
 			break;
-		udelay(10);
-		timeout--;
+		cpu_relax();
 	}
-	WARN_ON(!timeout);
+	WARN_ON(!time_before(jiffies, timeout));
 	chan_write(chan, cp, CPDMA_TEARDOWN_VALUE);
 
 	/* handle completed packets */
@@ -1046,6 +984,3 @@ unlock_ret:
 	spin_unlock_irqrestore(&ctlr->lock, flags);
 	return ret;
 }
-EXPORT_SYMBOL_GPL(cpdma_control_set);
-
-MODULE_LICENSE("GPL");
diff --git a/drivers/net/ethernet/ti/davinci_cpdma.h b/drivers/net/ethernet/ti/davinci_cpdma.h
index 86dee48..afa19a0 100644
--- a/drivers/net/ethernet/ti/davinci_cpdma.h
+++ b/drivers/net/ethernet/ti/davinci_cpdma.h
@@ -24,13 +24,6 @@
 #define __chan_linear(chan_num)	((chan_num) & (CPDMA_MAX_CHANNELS - 1))
 #define chan_linear(chan)	__chan_linear((chan)->chan_num)
 
-#define CPDMA_RX_SOURCE_PORT(__status__)	((__status__ >> 16) & 0x7)
-
-#define CPDMA_EOI_RX_THRESH	0x0
-#define CPDMA_EOI_RX		0x1
-#define CPDMA_EOI_TX		0x2
-#define CPDMA_EOI_MISC		0x3
-
 struct cpdma_params {
 	struct device		*dev;
 	void __iomem		*dmaregs;
@@ -89,13 +82,12 @@ int cpdma_chan_dump(struct cpdma_chan *chan);
 int cpdma_chan_get_stats(struct cpdma_chan *chan,
 			 struct cpdma_chan_stats *stats);
 int cpdma_chan_submit(struct cpdma_chan *chan, void *token, void *data,
-		      int len, int directed);
+		      int len, gfp_t gfp_mask);
 int cpdma_chan_process(struct cpdma_chan *chan, int quota);
 
 int cpdma_ctlr_int_ctrl(struct cpdma_ctlr *ctlr, bool enable);
-void cpdma_ctlr_eoi(struct cpdma_ctlr *ctlr, u32 value);
+void cpdma_ctlr_eoi(struct cpdma_ctlr *ctlr);
 int cpdma_chan_int_ctrl(struct cpdma_chan *chan, bool enable);
-bool cpdma_check_free_tx_desc(struct cpdma_chan *chan);
 
 enum cpdma_control {
 	CPDMA_CMD_IDLE,			/* write-only */
diff --git a/drivers/net/ethernet/ti/davinci_emac.c b/drivers/net/ethernet/ti/davinci_emac.c
index 860e15d..7c75b16 100644
--- a/drivers/net/ethernet/ti/davinci_emac.c
+++ b/drivers/net/ethernet/ti/davinci_emac.c
@@ -120,6 +120,7 @@ static const char emac_version_string[] = "TI DaVinci EMAC Linux v6.1";
 #define EMAC_DEF_TX_CH			(0) /* Default 0th channel */
 #define EMAC_DEF_RX_CH			(0) /* Default 0th channel */
 #define EMAC_DEF_RX_NUM_DESC		(128)
+#define EMAC_DEF_TX_NUM_DESC		(128)
 #define EMAC_DEF_MAX_TX_CH		(1) /* Max TX channels configured */
 #define EMAC_DEF_MAX_RX_CH		(1) /* Max RX channels configured */
 #define EMAC_POLL_WEIGHT		(64) /* Default NAPI poll weight */
@@ -341,6 +342,7 @@ struct emac_priv {
 	u32 mac_hash2;
 	u32 multicast_hash_cnt[EMAC_NUM_MULTICAST_BITS];
 	u32 rx_addr_type;
+	atomic_t cur_tx;
 	const char *phy_id;
 #ifdef CONFIG_OF
 	struct device_node *phy_node;
@@ -478,8 +480,8 @@ static void emac_dump_regs(struct emac_priv *priv)
 static void emac_get_drvinfo(struct net_device *ndev,
 			     struct ethtool_drvinfo *info)
 {
-	strlcpy(info->driver, emac_version_string, sizeof(info->driver));
-	strlcpy(info->version, EMAC_MODULE_VERSION, sizeof(info->version));
+	strcpy(info->driver, emac_version_string);
+	strcpy(info->version, EMAC_MODULE_VERSION);
 }
 
 /**
@@ -1037,7 +1039,7 @@ static void emac_rx_handler(void *token, int len, int status)
 
 recycle:
 	ret = cpdma_chan_submit(priv->rxchan, skb, skb->data,
-			skb_tailroom(skb), 0);
+			skb_tailroom(skb), GFP_KERNEL);
 
 	WARN_ON(ret == -ENOMEM);
 	if (unlikely(ret < 0))
@@ -1048,12 +1050,12 @@ static void emac_tx_handler(void *token, int len, int status)
 {
 	struct sk_buff		*skb = token;
 	struct net_device	*ndev = skb->dev;
+	struct emac_priv	*priv = netdev_priv(ndev);
+
+	atomic_dec(&priv->cur_tx);
 
-	/* Check whether the queue is stopped due to stalled tx dma, if the
-	 * queue is stopped then start the queue as we have free desc for tx
-	 */
 	if (unlikely(netif_queue_stopped(ndev)))
-		netif_wake_queue(ndev);
+		netif_start_queue(ndev);
 	ndev->stats.tx_packets++;
 	ndev->stats.tx_bytes += len;
 	dev_kfree_skb_any(skb);
@@ -1092,17 +1094,14 @@ static int emac_dev_xmit(struct sk_buff *skb, struct net_device *ndev)
 	skb_tx_timestamp(skb);
 
 	ret_code = cpdma_chan_submit(priv->txchan, skb, skb->data, skb->len,
-				     0);
+				     GFP_KERNEL);
 	if (unlikely(ret_code != 0)) {
 		if (netif_msg_tx_err(priv) && net_ratelimit())
 			dev_err(emac_dev, "DaVinci EMAC: desc submit failed");
 		goto fail_tx;
 	}
 
-	/* If there is no more tx desc left free then we need to
-	 * tell the kernel to stop sending us tx frames.
-	 */
-	if (unlikely(!cpdma_check_free_tx_desc(priv->txchan)))
+	if (atomic_inc_return(&priv->cur_tx) >= EMAC_DEF_TX_NUM_DESC)
 		netif_stop_queue(ndev);
 
 	return NETDEV_TX_OK;
@@ -1265,6 +1264,7 @@ static int emac_dev_setmac_addr(struct net_device *ndev, void *addr)
 	/* Store mac addr in priv and rx channel and set it in EMAC hw */
 	memcpy(priv->mac_addr, sa->sa_data, ndev->addr_len);
 	memcpy(ndev->dev_addr, sa->sa_data, ndev->addr_len);
+	ndev->addr_assign_type &= ~NET_ADDR_RANDOM;
 
 	/* MAC address is configured only after the interface is enabled. */
 	if (netif_running(ndev)) {
@@ -1438,7 +1438,7 @@ static int emac_poll(struct napi_struct *napi, int budget)
  * Polled functionality used by netconsole and others in non interrupt mode
  *
  */
-static void emac_poll_controller(struct net_device *ndev)
+void emac_poll_controller(struct net_device *ndev)
 {
 	struct emac_priv *priv = netdev_priv(ndev);
 
@@ -1448,7 +1448,7 @@ static void emac_poll_controller(struct net_device *ndev)
 }
 #endif
 
-static void emac_adjust_link(struct net_device *ndev)
+static void emac_adjust_link(struct net_device *ndev, void *context)
 {
 	struct emac_priv *priv = netdev_priv(ndev);
 	struct phy_device *phydev = priv->phydev;
@@ -1558,7 +1558,7 @@ static int emac_dev_open(struct net_device *ndev)
 			break;
 
 		ret = cpdma_chan_submit(priv->rxchan, skb, skb->data,
-					skb_tailroom(skb), 0);
+					skb_tailroom(skb), GFP_KERNEL);
 		if (WARN_ON(ret < 0))
 			break;
 	}
@@ -1600,8 +1600,8 @@ static int emac_dev_open(struct net_device *ndev)
 
 	if (priv->phy_id && *priv->phy_id) {
 		priv->phydev = phy_connect(ndev, priv->phy_id,
-					   &emac_adjust_link,
-					   PHY_INTERFACE_MODE_MII);
+					   &emac_adjust_link, 0,
+					   PHY_INTERFACE_MODE_MII, NULL);
 
 		if (IS_ERR(priv->phydev)) {
 			dev_err(emac_dev, "could not connect to phy %s\n",
@@ -1865,18 +1865,21 @@ static int davinci_emac_probe(struct platform_device *pdev)
 
 
 	/* obtain emac clock from kernel */
-	emac_clk = devm_clk_get(&pdev->dev, NULL);
+	emac_clk = clk_get(&pdev->dev, NULL);
 	if (IS_ERR(emac_clk)) {
 		dev_err(&pdev->dev, "failed to get EMAC clock\n");
 		return -EBUSY;
 	}
 	emac_bus_frequency = clk_get_rate(emac_clk);
+	clk_put(emac_clk);
 
 	/* TODO: Probe PHY here if possible */
 
 	ndev = alloc_etherdev(sizeof(struct emac_priv));
-	if (!ndev)
-		return -ENOMEM;
+	if (!ndev) {
+		rc = -ENOMEM;
+		goto no_ndev;
+	}
 
 	platform_set_drvdata(pdev, ndev);
 	priv = netdev_priv(ndev);
@@ -1890,7 +1893,7 @@ static int davinci_emac_probe(struct platform_device *pdev)
 	if (!pdata) {
 		dev_err(&pdev->dev, "no platform data\n");
 		rc = -ENODEV;
-		goto no_pdata;
+		goto probe_quit;
 	}
 
 	/* MAC addr and PHY mask , RMII enable info from platform_data */
@@ -1910,23 +1913,23 @@ static int davinci_emac_probe(struct platform_device *pdev)
 	if (!res) {
 		dev_err(&pdev->dev,"error getting res\n");
 		rc = -ENOENT;
-		goto no_pdata;
+		goto probe_quit;
 	}
 
 	priv->emac_base_phys = res->start + pdata->ctrl_reg_offset;
 	size = resource_size(res);
-	if (!devm_request_mem_region(&pdev->dev, res->start,
-				     size, ndev->name)) {
+	if (!request_mem_region(res->start, size, ndev->name)) {
 		dev_err(&pdev->dev, "failed request_mem_region() for regs\n");
 		rc = -ENXIO;
-		goto no_pdata;
+		goto probe_quit;
 	}
 
-	priv->remap_addr = devm_ioremap(&pdev->dev, res->start, size);
+	priv->remap_addr = ioremap(res->start, size);
 	if (!priv->remap_addr) {
 		dev_err(&pdev->dev, "unable to map IO\n");
 		rc = -ENOMEM;
-		goto no_pdata;
+		release_mem_region(res->start, size);
+		goto probe_quit;
 	}
 	priv->emac_base = priv->remap_addr + pdata->ctrl_reg_offset;
 	ndev->base_addr = (unsigned long)priv->remap_addr;
@@ -1959,7 +1962,7 @@ static int davinci_emac_probe(struct platform_device *pdev)
 	if (!priv->dma) {
 		dev_err(&pdev->dev, "error initializing DMA\n");
 		rc = -ENOMEM;
-		goto no_pdata;
+		goto no_dma;
 	}
 
 	priv->txchan = cpdma_chan_create(priv->dma, tx_chan_num(EMAC_DEF_TX_CH),
@@ -1968,14 +1971,14 @@ static int davinci_emac_probe(struct platform_device *pdev)
 				       emac_rx_handler);
 	if (WARN_ON(!priv->txchan || !priv->rxchan)) {
 		rc = -ENOMEM;
-		goto no_cpdma_chan;
+		goto no_irq_res;
 	}
 
 	res = platform_get_resource(pdev, IORESOURCE_IRQ, 0);
 	if (!res) {
 		dev_err(&pdev->dev, "error getting irq res\n");
 		rc = -ENOENT;
-		goto no_cpdma_chan;
+		goto no_irq_res;
 	}
 	ndev->irq = res->start;
 
@@ -1997,7 +2000,7 @@ static int davinci_emac_probe(struct platform_device *pdev)
 	if (rc) {
 		dev_err(&pdev->dev, "error in register_netdev\n");
 		rc = -ENODEV;
-		goto no_cpdma_chan;
+		goto no_irq_res;
 	}
 
 
@@ -2012,14 +2015,20 @@ static int davinci_emac_probe(struct platform_device *pdev)
 
 	return 0;
 
-no_cpdma_chan:
+no_irq_res:
 	if (priv->txchan)
 		cpdma_chan_destroy(priv->txchan);
 	if (priv->rxchan)
 		cpdma_chan_destroy(priv->rxchan);
 	cpdma_ctlr_destroy(priv->dma);
-no_pdata:
+no_dma:
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	release_mem_region(res->start, resource_size(res));
+	iounmap(priv->remap_addr);
+
+probe_quit:
 	free_netdev(ndev);
+no_ndev:
 	return rc;
 }
 
@@ -2032,12 +2041,14 @@ no_pdata:
  */
 static int davinci_emac_remove(struct platform_device *pdev)
 {
+	struct resource *res;
 	struct net_device *ndev = platform_get_drvdata(pdev);
 	struct emac_priv *priv = netdev_priv(ndev);
 
 	dev_notice(&ndev->dev, "DaVinci EMAC: davinci_emac_remove()\n");
 
 	platform_set_drvdata(pdev, NULL);
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
 
 	if (priv->txchan)
 		cpdma_chan_destroy(priv->txchan);
@@ -2045,7 +2056,10 @@ static int davinci_emac_remove(struct platform_device *pdev)
 		cpdma_chan_destroy(priv->rxchan);
 	cpdma_ctlr_destroy(priv->dma);
 
+	release_mem_region(res->start, resource_size(res));
+
 	unregister_netdev(ndev);
+	iounmap(priv->remap_addr);
 	free_netdev(ndev);
 
 	return 0;
diff --git a/drivers/net/ethernet/ti/davinci_mdio.c b/drivers/net/ethernet/ti/davinci_mdio.c
index c47f0db..44ab57a0d 100644
--- a/drivers/net/ethernet/ti/davinci_mdio.c
+++ b/drivers/net/ethernet/ti/davinci_mdio.c
@@ -37,6 +37,7 @@
 #include <linux/pm_runtime.h>
 #include <linux/davinci_emac.h>
 #include <linux/of.h>
+#include <linux/of_mdio.h>
 #include <linux/of_device.h>
 
 /*
@@ -143,6 +144,10 @@ static int davinci_mdio_reset(struct mii_bus *bus)
 	dev_info(data->dev, "davinci mdio revision %d.%d\n",
 		 (ver >> 8) & 0xff, ver & 0xff);
 
+	/* OF explicitly registers phy devices without a bus scan */
+	if (data->dev->of_node)
+		return 0;
+
 	/* get phy mask from the alive register */
 	phy_mask = __raw_readl(&data->regs->alive);
 	if (phy_mask) {
@@ -313,6 +318,7 @@ static int davinci_mdio_probe_dt(struct mdio_platform_data *data,
 static int davinci_mdio_probe(struct platform_device *pdev)
 {
 	struct mdio_platform_data *pdata = pdev->dev.platform_data;
+	struct device_node *node = pdev->dev.of_node;
 	struct device *dev = &pdev->dev;
 	struct davinci_mdio_data *data;
 	struct resource *res;
@@ -320,8 +326,10 @@ static int davinci_mdio_probe(struct platform_device *pdev)
 	int ret, addr;
 
 	data = kzalloc(sizeof(*data), GFP_KERNEL);
-	if (!data)
+	if (!data) {
+		dev_err(dev, "failed to alloc device data\n");
 		return -ENOMEM;
+	}
 
 	data->bus = mdiobus_alloc();
 	if (!data->bus) {
@@ -346,9 +354,9 @@ static int davinci_mdio_probe(struct platform_device *pdev)
 	data->bus->reset	= davinci_mdio_reset,
 	data->bus->parent	= dev;
 	data->bus->priv		= data;
+	data->dev = dev;
 
 	pm_runtime_enable(&pdev->dev);
-	pm_runtime_get_sync(&pdev->dev);
 	data->clk = clk_get(&pdev->dev, "fck");
 	if (IS_ERR(data->clk)) {
 		dev_err(dev, "failed to get device clock\n");
@@ -357,8 +365,9 @@ static int davinci_mdio_probe(struct platform_device *pdev)
 		goto bail_out;
 	}
 
+	clk_prepare(data->clk);
+	pm_runtime_get_sync(&pdev->dev);
 	dev_set_drvdata(dev, data);
-	data->dev = dev;
 	spin_lock_init(&data->lock);
 
 	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
@@ -384,7 +393,7 @@ static int davinci_mdio_probe(struct platform_device *pdev)
 	}
 
 	/* register the mii bus */
-	ret = mdiobus_register(data->bus);
+	ret = of_mdiobus_register(data->bus, node);
 	if (ret)
 		goto bail_out;
 
@@ -449,9 +458,10 @@ static int davinci_mdio_suspend(struct device *dev)
 	__raw_writel(ctrl, &data->regs->control);
 	wait_for_idle(data);
 
+	pm_runtime_put_sync(data->dev);
+
 	data->suspended = true;
 	spin_unlock(&data->lock);
-	pm_runtime_put_sync(data->dev);
 
 	return 0;
 }
@@ -459,12 +469,15 @@ static int davinci_mdio_suspend(struct device *dev)
 static int davinci_mdio_resume(struct device *dev)
 {
 	struct davinci_mdio_data *data = dev_get_drvdata(dev);
+	u32 ctrl;
 
+	spin_lock(&data->lock);
 	pm_runtime_get_sync(data->dev);
 
-	spin_lock(&data->lock);
 	/* restart the scan state machine */
-	__davinci_mdio_reset(data);
+	ctrl = __raw_readl(&data->regs->control);
+	ctrl |= CONTROL_ENABLE;
+	__raw_writel(ctrl, &data->regs->control);
 
 	data->suspended = false;
 	spin_unlock(&data->lock);
@@ -473,15 +486,14 @@ static int davinci_mdio_resume(struct device *dev)
 }
 
 static const struct dev_pm_ops davinci_mdio_pm_ops = {
-	.suspend_late	= davinci_mdio_suspend,
-	.resume_early	= davinci_mdio_resume,
+	.suspend	= davinci_mdio_suspend,
+	.resume		= davinci_mdio_resume,
 };
 
 static const struct of_device_id davinci_mdio_of_mtable[] = {
 	{ .compatible = "ti,davinci_mdio", },
 	{ /* sentinel */ },
 };
-MODULE_DEVICE_TABLE(of, davinci_mdio_of_mtable);
 
 static struct platform_driver davinci_mdio_driver = {
 	.driver = {
diff --git a/drivers/net/ethernet/ti/keystone_ethss.c b/drivers/net/ethernet/ti/keystone_ethss.c
new file mode 100644
index 0000000..299db15
--- /dev/null
+++ b/drivers/net/ethernet/ti/keystone_ethss.c
@@ -0,0 +1,2377 @@
+/*
+ * Copyright (C) 2012 Texas Instruments Incorporated
+ * Authors: Sandeep Paulraj <s-paulraj@ti.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation version 2.
+ *
+ * This program is distributed "as is" WITHOUT ANY WARRANTY of any
+ * kind, whether express or implied; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/io.h>
+#include <linux/of.h>
+#include <linux/clk.h>
+#include <linux/phy.h>
+#include <linux/timer.h>
+#include <linux/module.h>
+#include <linux/device.h>
+#include <linux/uaccess.h>
+#include <linux/if_vlan.h>
+#include <linux/of_mdio.h>
+#include <linux/ethtool.h>
+#include <linux/if_ether.h>
+#include <linux/net_tstamp.h>
+#include <linux/netdevice.h>
+#include <linux/interrupt.h>
+#include <linux/dmaengine.h>
+#include <linux/dma-mapping.h>
+#include <linux/scatterlist.h>
+#include <linux/etherdevice.h>
+#include <linux/platform_device.h>
+
+#include "cpsw_ale.h"
+#include "keystone_net.h"
+
+#define NETCP_DRIVER_NAME	"TI KeyStone Ethernet Driver"
+#define NETCP_DRIVER_VERSION	"v1.2.2"
+
+#define CPSW_SGMII_IDENT(reg)		((reg >> 16) & 0xffff)
+#define CPSW_MAJOR_VERSION(reg)		(reg >> 8 & 0x7)
+#define CPSW_MINOR_VERSION(reg)		(reg & 0xff)
+#define CPSW_RTL_VERSION(reg)		((reg >> 11) & 0x1f)
+
+#define TCI6614_SS_BASE				0x02090000
+#define DEVICE_N_GMACSL_PORTS			2
+#define DEVICE_EMACSL_RESET_POLL_COUNT		100
+
+#define	CPSW_TIMER_INTERVAL			(HZ / 10)
+
+/* Soft reset register values */
+#define SOFT_RESET_MASK				BIT(0)
+#define SOFT_RESET				BIT(0)
+
+#define MACSL_RX_ENABLE_CSF			BIT(23)
+#define MACSL_RX_ENABLE_EXT_CTL			BIT(18)
+#define MACSL_ENABLE				BIT(5)
+#define GMACSL_RET_WARN_RESET_INCOMPLETE	-2
+
+#define CPSW_NUM_PORTS		                3
+#define CPSW_CTL_P0_ENABLE			BIT(2)
+#define CPSW_CTL_VLAN_AWARE			BIT(1)
+#define CPSW_REG_VAL_STAT_ENABLE_ALL		0xf
+
+#define CPSW_MASK_ALL_PORTS			7
+#define CPSW_MASK_PHYS_PORTS			6
+#define CPSW_MASK_NO_PORTS			0
+#define CPSW_NON_VLAN_ADDR			-1
+
+#define CPSW_STATSA_MODULE			0
+#define CPSW_STATSB_MODULE			1
+
+#define MAX_SIZE_STREAM_BUFFER		        9504
+
+struct cpsw_slave {
+	struct cpsw_slave_regs __iomem	*regs;
+	struct cpsw_sliver_regs __iomem	*sliver;
+	int				 slave_num;
+	int				 port_num;
+	u32				 mac_control;
+	struct phy_device		*phy;
+	const char			*phy_id;
+	struct cpsw_ale			*ale;
+	u32				 link_interface;
+	u8				 phy_port_t;
+};
+
+struct cpsw_ss_regs {
+	u32	id_ver;
+	u32	soft_reset;
+	u32	control;
+	u32	int_control;
+	u32	rx_thresh_en;
+	u32	rx_en;
+	u32	tx_en;
+	u32	misc_en;
+	u32	mem_allign1[8];
+	u32	rx_thresh_stat;
+	u32	rx_stat;
+	u32	tx_stat;
+	u32	misc_stat;
+	u32	mem_allign2[8];
+	u32	rx_imax;
+	u32	tx_imax;
+};
+
+struct cpsw_regs {
+	u32	id_ver;
+	u32	control;
+	u32	soft_reset;
+	u32	stat_port_en;
+	u32	ptype;
+	u32	soft_idle;
+	u32	thru_rate;
+	u32	gap_thresh;
+	u32	tx_start_wds;
+	u32	flow_control;
+};
+
+struct cpsw_slave_regs {
+	u32	max_blks;
+	u32	blk_cnt;
+	u32	port_vlan;
+	u32	tx_pri_map;
+	u32	sa_lo;
+	u32	sa_hi;
+	u32	ts_ctl;
+	u32	ts_seq_ltype;
+	u32	ts_vlan;
+};
+
+struct cpsw_host_regs {
+	u32	src_id;
+	u32	port_vlan;
+	u32	rx_pri_map;
+	u32	rx_maxlen;
+};
+
+struct cpsw_sliver_regs {
+	u32	id_ver;
+	u32	mac_control;
+	u32	mac_status;
+	u32	soft_reset;
+	u32	rx_maxlen;
+	u32	__reserved_0;
+	u32	rx_pause;
+	u32	tx_pause;
+	u32	__reserved_1;
+	u32	rx_pri_map;
+	u32	rsvd[6];
+};
+
+struct cpsw_hw_stats {
+	u32	rx_good_frames;
+	u32	rx_broadcast_frames;
+	u32	rx_multicast_frames;
+	u32	rx_pause_frames;
+	u32	rx_crc_errors;
+	u32	rx_align_code_errors;
+	u32	rx_oversized_frames;
+	u32	rx_jabber_frames;
+	u32	rx_undersized_frames;
+	u32	rx_fragments;
+	u32	__pad_0[2];
+	u32	rx_bytes;
+	u32	tx_good_frames;
+	u32	tx_broadcast_frames;
+	u32	tx_multicast_frames;
+	u32	tx_pause_frames;
+	u32	tx_deferred_frames;
+	u32	tx_collision_frames;
+	u32	tx_single_coll_frames;
+	u32	tx_mult_coll_frames;
+	u32	tx_excessive_collisions;
+	u32	tx_late_collisions;
+	u32	tx_underrun;
+	u32	tx_carrier_sense_errors;
+	u32	tx_bytes;
+	u32	tx_64byte_frames;
+	u32	tx_65_to_127byte_frames;
+	u32	tx_128_to_255byte_frames;
+	u32	tx_256_to_511byte_frames;
+	u32	tx_512_to_1023byte_frames;
+	u32	tx_1024byte_frames;
+	u32	net_bytes;
+	u32	rx_sof_overruns;
+	u32	rx_mof_overruns;
+	u32	rx_dma_overruns;
+};
+
+struct cpsw_ale_regs {
+	u32	ale_idver;
+	u32	rsvd0;
+	u32	ale_control;
+	u32	rsvd1;
+	u32	ale_prescale;
+	u32	rsvd2;
+	u32	ale_unknown_vlan;
+	u32	rsvd3;
+	u32	ale_tblctl;
+	u32	rsvd4[4];
+	u32	ale_tblw2;
+	u32	ale_tblw1;
+	u32	ale_tblw0;
+	u32	ale_portctl[6];
+};
+
+struct cpsw_priv {
+	struct device			*dev;
+	struct clk			*cpgmac;
+	struct netcp_device		*netcp_device;
+	u32				 num_slaves;
+	u32				 ale_ageout;
+	u32				 ale_entries;
+	u32				 ale_ports;
+	u32				 sgmii_module_ofs;
+	u32				 switch_module_ofs;
+	u32				 host_port_reg_ofs;
+	u32				 slave_reg_ofs;
+	u32				 sliver_reg_ofs;
+	u32				 hw_stats_reg_ofs;
+	u32				 ale_reg_ofs;
+
+	int				 host_port;
+	u32				 rx_packet_max;
+
+	struct cpsw_regs __iomem	*regs;
+	struct cpsw_ss_regs __iomem	*ss_regs;
+	struct cpsw_hw_stats __iomem	*hw_stats_regs[2];
+	struct cpsw_host_regs __iomem	*host_port_regs;
+	struct cpsw_ale_regs __iomem	*ale_reg;
+
+	void __iomem			*sgmii_port_regs;
+
+	struct cpsw_ale			*ale;
+	u32				 ale_refcnt;
+
+	u32				 link[5];
+	struct device_node		*phy_node[4];
+
+	u32				 intf_tx_queues;
+
+	u32				 multi_if;
+	u32				 slaves_per_interface;
+	u32				 num_interfaces;
+	struct device_node		*interfaces;
+	struct list_head		 cpsw_intf_head;
+
+	u64				 hw_stats[72];
+	int				 init_serdes_at_probe;
+	struct kobject			kobj;
+	struct kobject			tx_pri_kobj;
+	struct kobject			pvlan_kobj;
+	struct kobject			stats_kobj;
+	struct mutex			hw_stats_lock;
+};
+
+struct cpsw_intf {
+	struct net_device	*ndev;
+	struct device		*dev;
+	struct cpsw_priv	*cpsw_priv;
+	struct device_node	*phy_node;
+	u32			 num_slaves;
+	u32			 slave_port;
+	struct cpsw_slave	*slaves;
+	u32			 intf_tx_queues;
+	const char		*tx_chan_name;
+	u32			 tx_queue_depth;
+	struct netcp_tx_pipe	 tx_pipe;
+	u32			 multi_if;
+	struct list_head	 cpsw_intf_list;
+	struct timer_list	 timer;
+	u32			 sgmii_link;
+	unsigned long		 active_vlans[BITS_TO_LONGS(VLAN_N_VID)];
+};
+
+static struct cpsw_priv *priv;		/* FIXME: REMOVE THIS!! */
+
+/*
+ * Statistic management
+ */
+struct netcp_ethtool_stat {
+	char desc[ETH_GSTRING_LEN];
+	int type;
+	u32 size;
+	int offset;
+};
+
+#define for_each_slave(priv, func, arg...)				\
+	do {								\
+		int idx, port;						\
+		port = (priv)->slave_port;				\
+		if ((priv)->multi_if)					\
+			(func)((priv)->slaves, ##arg);			\
+		else							\
+			for (idx = 0; idx < (priv)->num_slaves; idx++)	\
+				(func)((priv)->slaves + idx, ##arg);	\
+	} while (0)
+
+#define FIELDINFO(_struct, field)       FIELD_SIZEOF(_struct, field),	\
+		                                offsetof(_struct, field)
+#define CPSW_STATSA_INFO(field) 	"CPSW_A:"#field, CPSW_STATSA_MODULE,\
+					FIELDINFO(struct cpsw_hw_stats,\
+						field)
+#define CPSW_STATSB_INFO(field) 	"CPSW_B:"#field, CPSW_STATSB_MODULE,\
+					FIELDINFO(struct cpsw_hw_stats,\
+						field)
+
+static const struct netcp_ethtool_stat et_stats[] = {
+	/* CPSW module A */
+	{CPSW_STATSA_INFO(rx_good_frames)},
+	{CPSW_STATSA_INFO(rx_broadcast_frames)},
+	{CPSW_STATSA_INFO(rx_multicast_frames)},
+	{CPSW_STATSA_INFO(rx_pause_frames)},
+	{CPSW_STATSA_INFO(rx_crc_errors)},
+	{CPSW_STATSA_INFO(rx_align_code_errors)},
+	{CPSW_STATSA_INFO(rx_oversized_frames)},
+	{CPSW_STATSA_INFO(rx_jabber_frames)},
+	{CPSW_STATSA_INFO(rx_undersized_frames)},
+	{CPSW_STATSA_INFO(rx_fragments)},
+	{CPSW_STATSA_INFO(rx_bytes)},
+	{CPSW_STATSA_INFO(tx_good_frames)},
+	{CPSW_STATSA_INFO(tx_broadcast_frames)},
+	{CPSW_STATSA_INFO(tx_multicast_frames)},
+	{CPSW_STATSA_INFO(tx_pause_frames)},
+	{CPSW_STATSA_INFO(tx_deferred_frames)},
+	{CPSW_STATSA_INFO(tx_collision_frames)},
+	{CPSW_STATSA_INFO(tx_single_coll_frames)},
+	{CPSW_STATSA_INFO(tx_mult_coll_frames)},
+	{CPSW_STATSA_INFO(tx_excessive_collisions)},
+	{CPSW_STATSA_INFO(tx_late_collisions)},
+	{CPSW_STATSA_INFO(tx_underrun)},
+	{CPSW_STATSA_INFO(tx_carrier_sense_errors)},
+	{CPSW_STATSA_INFO(tx_bytes)},
+	{CPSW_STATSA_INFO(tx_64byte_frames)},
+	{CPSW_STATSA_INFO(tx_65_to_127byte_frames)},
+	{CPSW_STATSA_INFO(tx_128_to_255byte_frames)},
+	{CPSW_STATSA_INFO(tx_256_to_511byte_frames)},
+	{CPSW_STATSA_INFO(tx_512_to_1023byte_frames)},
+	{CPSW_STATSA_INFO(tx_1024byte_frames)},
+	{CPSW_STATSA_INFO(net_bytes)},
+	{CPSW_STATSA_INFO(rx_sof_overruns)},
+	{CPSW_STATSA_INFO(rx_mof_overruns)},
+	{CPSW_STATSA_INFO(rx_dma_overruns)},
+	/* CPSW module B */
+	{CPSW_STATSB_INFO(rx_good_frames)},
+	{CPSW_STATSB_INFO(rx_broadcast_frames)},
+	{CPSW_STATSB_INFO(rx_multicast_frames)},
+	{CPSW_STATSB_INFO(rx_pause_frames)},
+	{CPSW_STATSB_INFO(rx_crc_errors)},
+	{CPSW_STATSB_INFO(rx_align_code_errors)},
+	{CPSW_STATSB_INFO(rx_oversized_frames)},
+	{CPSW_STATSB_INFO(rx_jabber_frames)},
+	{CPSW_STATSB_INFO(rx_undersized_frames)},
+	{CPSW_STATSB_INFO(rx_fragments)},
+	{CPSW_STATSB_INFO(rx_bytes)},
+	{CPSW_STATSB_INFO(tx_good_frames)},
+	{CPSW_STATSB_INFO(tx_broadcast_frames)},
+	{CPSW_STATSB_INFO(tx_multicast_frames)},
+	{CPSW_STATSB_INFO(tx_pause_frames)},
+	{CPSW_STATSB_INFO(tx_deferred_frames)},
+	{CPSW_STATSB_INFO(tx_collision_frames)},
+	{CPSW_STATSB_INFO(tx_single_coll_frames)},
+	{CPSW_STATSB_INFO(tx_mult_coll_frames)},
+	{CPSW_STATSB_INFO(tx_excessive_collisions)},
+	{CPSW_STATSB_INFO(tx_late_collisions)},
+	{CPSW_STATSB_INFO(tx_underrun)},
+	{CPSW_STATSB_INFO(tx_carrier_sense_errors)},
+	{CPSW_STATSB_INFO(tx_bytes)},
+	{CPSW_STATSB_INFO(tx_64byte_frames)},
+	{CPSW_STATSB_INFO(tx_65_to_127byte_frames)},
+	{CPSW_STATSB_INFO(tx_128_to_255byte_frames)},
+	{CPSW_STATSB_INFO(tx_256_to_511byte_frames)},
+	{CPSW_STATSB_INFO(tx_512_to_1023byte_frames)},
+	{CPSW_STATSB_INFO(tx_1024byte_frames)},
+	{CPSW_STATSB_INFO(net_bytes)},
+	{CPSW_STATSB_INFO(rx_sof_overruns)},
+	{CPSW_STATSB_INFO(rx_mof_overruns)},
+	{CPSW_STATSB_INFO(rx_dma_overruns)},
+};
+
+#define ETHTOOL_STATS_NUM ARRAY_SIZE(et_stats)
+
+struct cpsw_attribute {
+	struct attribute attr;
+	ssize_t (*show)(struct cpsw_priv *cpsw_dev,
+		struct cpsw_attribute *attr, char *buf);
+	ssize_t	(*store)(struct cpsw_priv *cpsw_dev,
+		struct cpsw_attribute *attr, const char *, size_t);
+	const struct cpsw_mod_info *info;
+	ssize_t info_size;
+	void *context;
+};
+#define to_cpsw_attr(_attr) container_of(_attr, struct cpsw_attribute, attr)
+
+#define to_cpsw_dev(obj) container_of(obj, struct cpsw_priv, kobj)
+#define tx_pri_to_cpsw_dev(obj) container_of(obj, struct cpsw_priv, tx_pri_kobj)
+#define pvlan_to_cpsw_dev(obj) container_of(obj, struct cpsw_priv, pvlan_kobj)
+#define stats_to_cpsw_dev(obj) container_of(obj, struct cpsw_priv, stats_kobj)
+
+#define BITS(x)			(BIT(x) - 1)
+#define BITMASK(n, s)		(BITS(n) << (s))
+#define cpsw_mod_info_field_val(r, i) \
+	((r & BITMASK(i->bits, i->shift)) >> i->shift)
+
+#define for_each_intf(i, priv) \
+	list_for_each_entry((i), &(priv)->cpsw_intf_head, cpsw_intf_list)
+
+#define __CPSW_ATTR_FULL(_name, _mode, _show, _store, _info,	\
+				_info_size, _ctxt)		\
+	{ \
+		.attr = {.name = __stringify(_name), .mode = _mode },	\
+		.show	= _show,		\
+		.store	= _store,		\
+		.info	= _info,		\
+		.info_size = _info_size,	\
+		.context = (_ctxt),		\
+	}
+
+#define __CPSW_ATTR(_name, _mode, _show, _store, _info) \
+		__CPSW_ATTR_FULL(_name, _mode, _show, _store, _info, \
+					(ARRAY_SIZE(_info)), NULL)
+
+#define __CPSW_CTXT_ATTR(_name, _mode, _show, _store, _info, _ctxt) \
+		__CPSW_ATTR_FULL(_name, _mode, _show, _store, _info, \
+					(ARRAY_SIZE(_info)), _ctxt)
+
+struct cpsw_mod_info {
+	const char	*name;
+	int		shift;
+	int		bits;
+};
+
+struct cpsw_parse_result {
+	int control;
+	int port;
+	u32 value;
+};
+
+static ssize_t cpsw_attr_info_show(const struct cpsw_mod_info *info,
+				int info_size, u32 reg, char *buf)
+{
+	int i, len = 0;
+
+	for (i = 0; i < info_size; i++, info++) {
+		len += snprintf(buf + len, PAGE_SIZE - len,
+			"%s=%d\n", info->name,
+			(int)cpsw_mod_info_field_val(reg, info));
+	}
+
+	return len;
+}
+
+static ssize_t cpsw_attr_parse_set_command(struct cpsw_priv *cpsw_dev,
+			      struct cpsw_attribute *attr,
+			      const char *buf, size_t count,
+				struct cpsw_parse_result *res)
+{
+	char ctrl_str[33], tmp_str[9];
+	int port = -1, value, len, control;
+	unsigned long end;
+	const struct cpsw_mod_info *info = attr->info;
+
+	len = strcspn(buf, ".=");
+	if (len >= 32)
+		return -ENOMEM;
+
+	strncpy(ctrl_str, buf, len);
+	ctrl_str[len] = '\0';
+	buf += len;
+
+	if (*buf == '.') {
+		++buf;
+		len = strcspn(buf, "=");
+		if (len >= 8)
+			return -ENOMEM;
+		strncpy(tmp_str, buf, len);
+		tmp_str[len] = '\0';
+		if (kstrtoul(tmp_str, 0, &end))
+			return -EINVAL;
+		port = (int)end;
+		buf += len;
+	}
+
+	if (*buf != '=')
+		return -EINVAL;
+
+	if (kstrtoul(buf + 1, 0, &end))
+		return -EINVAL;
+
+	value = (int)end;
+
+	for (control = 0; control < attr->info_size; control++)
+		if (strcmp(ctrl_str, info[control].name) == 0)
+			break;
+
+	if (control >= attr->info_size)
+		return -ENOENT;
+
+	res->control = control;
+	res->port = port;
+	res->value = value;
+
+	dev_info(cpsw_dev->dev, "parsed command %s.%d=%d\n",
+		attr->info[control].name, port, value);
+
+	return 0;
+}
+
+static inline void cpsw_info_set_reg_field(void __iomem *r,
+		const struct cpsw_mod_info *info, int val)
+{
+	u32 rv;
+
+	rv = __raw_readl(r);
+	rv = ((rv & ~BITMASK(info->bits, info->shift)) | (val << info->shift));
+	__raw_writel(rv, r);
+}
+
+static ssize_t cpsw_version_show(struct cpsw_priv *cpsw_dev,
+		     struct cpsw_attribute *attr,
+		     char *buf)
+{
+	u32 reg;
+
+	reg = __raw_readl(&cpsw_dev->regs->id_ver);
+
+	return snprintf(buf, PAGE_SIZE,
+		"cpsw version %d.%d (%d) SGMII identification value 0x%x\n",
+		 CPSW_MAJOR_VERSION(reg), CPSW_MINOR_VERSION(reg),
+		 CPSW_RTL_VERSION(reg), CPSW_SGMII_IDENT(reg));
+}
+
+static struct cpsw_attribute cpsw_version_attribute =
+	__ATTR(version, S_IRUGO, cpsw_version_show, NULL);
+
+static const struct cpsw_mod_info cpsw_controls[] = {
+	{
+		.name		= "fifo_loopback",
+		.shift		= 0,
+		.bits		= 1,
+	},
+	{
+		.name		= "vlan_aware",
+		.shift		= 1,
+		.bits		= 1,
+	},
+	{
+		.name		= "p0_enable",
+		.shift		= 2,
+		.bits		= 1,
+	},
+	{
+		.name		= "p0_pass_pri_tagged",
+		.shift		= 3,
+		.bits		= 1,
+	},
+	{
+		.name		= "p1_pass_pri_tagged",
+		.shift		= 4,
+		.bits		= 1,
+	},
+	{
+		.name		= "p2_pass_pri_tagged",
+		.shift		= 5,
+		.bits		= 1,
+	},
+};
+
+static ssize_t cpsw_control_show(struct cpsw_priv *cpsw_dev,
+		     struct cpsw_attribute *attr,
+		     char *buf)
+{
+	u32 reg;
+
+	reg = __raw_readl(&cpsw_dev->regs->control);
+	return cpsw_attr_info_show(attr->info, attr->info_size, reg, buf);
+}
+
+static ssize_t cpsw_control_store(struct cpsw_priv *cpsw_dev,
+			      struct cpsw_attribute *attr,
+			      const char *buf, size_t count)
+{
+	const struct cpsw_mod_info *i;
+	struct cpsw_parse_result res;
+	void __iomem *r = NULL;
+	int ret;
+
+
+	ret = cpsw_attr_parse_set_command(cpsw_dev, attr, buf, count, &res);
+	if (ret)
+		return ret;
+
+	i = &(attr->info[res.control]);
+	r = &cpsw_dev->regs->control;
+
+	cpsw_info_set_reg_field(r, i, res.value);
+	return count;
+}
+
+static struct cpsw_attribute cpsw_control_attribute =
+	__CPSW_ATTR(control, S_IRUGO | S_IWUSR,
+		cpsw_control_show, cpsw_control_store, cpsw_controls);
+
+static const struct cpsw_mod_info cpsw_ptypes[] = {
+	{
+		.name		= "escalate_pri_load_val",
+		.shift		= 0,
+		.bits		= 5,
+	},
+	{
+		.name		= "port0_pri_type_escalate",
+		.shift		= 8,
+		.bits		= 1,
+	},
+	{
+		.name		= "port1_pri_type_escalate",
+		.shift		= 9,
+		.bits		= 1,
+	},
+	{
+		.name		= "port2_pri_type_escalate",
+		.shift		= 10,
+		.bits		= 1,
+	},
+};
+
+static ssize_t cpsw_pri_type_show(struct cpsw_priv *cpsw_dev,
+		     struct cpsw_attribute *attr,
+		     char *buf)
+{
+	u32 reg;
+
+	reg = __raw_readl(&cpsw_dev->regs->ptype);
+
+	return cpsw_attr_info_show(attr->info, attr->info_size, reg, buf);
+}
+
+static ssize_t cpsw_pri_type_store(struct cpsw_priv *cpsw_dev,
+			      struct cpsw_attribute *attr,
+			      const char *buf, size_t count)
+{
+	const struct cpsw_mod_info *i;
+	struct cpsw_parse_result res;
+	void __iomem *r = NULL;
+	int ret;
+
+
+	ret = cpsw_attr_parse_set_command(cpsw_dev, attr, buf, count, &res);
+	if (ret)
+		return ret;
+
+	i = &(attr->info[res.control]);
+	r = &cpsw_dev->regs->ptype;
+
+	cpsw_info_set_reg_field(r, i, res.value);
+	return count;
+}
+
+static struct cpsw_attribute cpsw_pri_type_attribute =
+	__CPSW_ATTR(priority_type, S_IRUGO | S_IWUSR,
+			cpsw_pri_type_show,
+			cpsw_pri_type_store,
+			cpsw_ptypes);
+
+static const struct cpsw_mod_info cpsw_flow_controls[] = {
+	{
+		.name		= "port0_flow_control_en",
+		.shift		= 0,
+		.bits		= 1,
+	},
+	{
+		.name		= "port1_flow_control_en",
+		.shift		= 1,
+		.bits		= 1,
+	},
+	{
+		.name		= "port2_flow_control_en",
+		.shift		= 2,
+		.bits		= 1,
+	},
+};
+
+static ssize_t cpsw_flow_control_show(struct cpsw_priv *cpsw_dev,
+		     struct cpsw_attribute *attr, char *buf)
+{
+	u32 reg;
+
+	reg = __raw_readl(&cpsw_dev->regs->flow_control);
+
+	return cpsw_attr_info_show(attr->info, attr->info_size, reg, buf);
+}
+
+static ssize_t cpsw_flow_control_store(struct cpsw_priv *cpsw_dev,
+			      struct cpsw_attribute *attr,
+			      const char *buf, size_t count)
+{
+	const struct cpsw_mod_info *i;
+	struct cpsw_parse_result res;
+	void __iomem *r = NULL;
+	int ret;
+
+
+	ret = cpsw_attr_parse_set_command(cpsw_dev, attr, buf, count, &res);
+	if (ret)
+		return ret;
+
+	i = &(attr->info[res.control]);
+	r = &cpsw_dev->regs->flow_control;
+
+	cpsw_info_set_reg_field(r, i, res.value);
+	return count;
+}
+
+static struct cpsw_attribute cpsw_flow_control_attribute =
+	__CPSW_ATTR(flow_control, S_IRUGO | S_IWUSR,
+		cpsw_flow_control_show,
+		cpsw_flow_control_store,
+		cpsw_flow_controls);
+
+static const struct cpsw_mod_info cpsw_port_tx_pri_maps[] = {
+	{
+		.name		= "port_tx_pri_0",
+		.shift		= 0,
+		.bits		= 3,
+	},
+	{
+		.name		= "port_tx_pri_1",
+		.shift		= 4,
+		.bits		= 3,
+	},
+	{
+		.name		= "port_tx_pri_2",
+		.shift		= 8,
+		.bits		= 3,
+	},
+	{
+		.name		= "port_tx_pri_3",
+		.shift		= 12,
+		.bits		= 3,
+	},
+	{
+		.name		= "port_tx_pri_4",
+		.shift		= 16,
+		.bits		= 3,
+	},
+	{
+		.name		= "port_tx_pri_5",
+		.shift		= 20,
+		.bits		= 3,
+	},
+	{
+		.name		= "port_tx_pri_6",
+		.shift		= 24,
+		.bits		= 3,
+	},
+	{
+		.name		= "port_tx_pri_7",
+		.shift		= 28,
+		.bits		= 3,
+	},
+};
+
+static ssize_t cpsw_port_tx_pri_map_show(struct cpsw_priv *cpsw_dev,
+		     struct cpsw_attribute *attr,
+		     char *buf)
+{
+	int idx, len = 0, total_len = 0, port;
+	struct cpsw_intf *cpsw_intf;
+	struct cpsw_slave *slave;
+	u32 reg;
+
+	port = (int)(attr->context);
+
+	for_each_intf(cpsw_intf, cpsw_dev) {
+		if (cpsw_intf->multi_if) {
+			slave = cpsw_intf->slaves;
+			if (slave->port_num != port)
+				continue;
+			reg = __raw_readl(&slave->regs->tx_pri_map);
+			len = cpsw_attr_info_show(attr->info, attr->info_size,
+						reg, buf+total_len);
+			total_len += len;
+		} else {
+			for (idx = 0; idx < cpsw_intf->num_slaves; idx++) {
+				slave = cpsw_intf->slaves + idx;
+				if (slave->port_num != port)
+					continue;
+				reg = __raw_readl(&slave->regs->tx_pri_map);
+				len = cpsw_attr_info_show(attr->info,
+					attr->info_size, reg, buf+total_len);
+				total_len += len;
+			}
+		}
+	}
+	return total_len;
+}
+
+static ssize_t cpsw_port_tx_pri_map_store(struct cpsw_priv *cpsw_dev,
+			      struct cpsw_attribute *attr,
+			      const char *buf, size_t count)
+{
+	const struct cpsw_mod_info *i;
+	struct cpsw_parse_result res;
+	struct cpsw_intf *cpsw_intf;
+	struct cpsw_slave *slave;
+	void __iomem *r = NULL;
+	int ret, idx, port;
+
+	port = (int)(attr->context);
+
+	ret = cpsw_attr_parse_set_command(cpsw_dev, attr, buf, count, &res);
+	if (ret)
+		return ret;
+
+	i = &(attr->info[res.control]);
+
+	/* Slave port */
+	for_each_intf(cpsw_intf, cpsw_dev) {
+		if (cpsw_intf->multi_if) {
+			slave = cpsw_intf->slaves;
+			if (slave->port_num == port) {
+				r = &slave->regs->tx_pri_map;
+				goto set;
+			}
+		} else
+			for (idx = 0; idx < cpsw_intf->num_slaves; idx++) {
+				slave = cpsw_intf->slaves + idx;
+				if (slave->port_num == port) {
+					r = &slave->regs->tx_pri_map;
+					goto set;
+				}
+			}
+	}
+
+	if (!r)
+		return  -ENOENT;
+
+set:
+	cpsw_info_set_reg_field(r, i, res.value);
+	return count;
+}
+
+static struct cpsw_attribute cpsw_tx_pri_1_attribute =
+	__CPSW_CTXT_ATTR(1, S_IRUGO | S_IWUSR,
+			cpsw_port_tx_pri_map_show,
+			cpsw_port_tx_pri_map_store,
+			cpsw_port_tx_pri_maps, (void *)1);
+
+static struct cpsw_attribute cpsw_tx_pri_2_attribute =
+	__CPSW_CTXT_ATTR(2, S_IRUGO | S_IWUSR,
+			cpsw_port_tx_pri_map_show,
+			cpsw_port_tx_pri_map_store,
+			cpsw_port_tx_pri_maps, (void *)2);
+
+static struct cpsw_attribute cpsw_tx_pri_3_attribute =
+	__CPSW_CTXT_ATTR(3, S_IRUGO | S_IWUSR,
+			cpsw_port_tx_pri_map_show,
+			cpsw_port_tx_pri_map_store,
+			cpsw_port_tx_pri_maps, (void *)3);
+
+static struct cpsw_attribute cpsw_tx_pri_4_attribute =
+	__CPSW_CTXT_ATTR(4, S_IRUGO | S_IWUSR,
+			cpsw_port_tx_pri_map_show,
+			cpsw_port_tx_pri_map_store,
+			cpsw_port_tx_pri_maps, (void *)4);
+
+static struct attribute *cpsw_tx_pri_default_attrs[] = {
+	&cpsw_tx_pri_1_attribute.attr,
+	&cpsw_tx_pri_2_attribute.attr,
+	&cpsw_tx_pri_3_attribute.attr,
+	&cpsw_tx_pri_4_attribute.attr,
+	NULL
+};
+
+static ssize_t cpsw_tx_pri_attr_show(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	struct cpsw_attribute *attribute = to_cpsw_attr(attr);
+	struct cpsw_priv *cpsw_dev = tx_pri_to_cpsw_dev(kobj);
+
+	if (!attribute->show)
+		return -EIO;
+
+	return attribute->show(cpsw_dev, attribute, buf);
+}
+
+static ssize_t cpsw_tx_pri_attr_store(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	struct cpsw_attribute *attribute = to_cpsw_attr(attr);
+	struct cpsw_priv *cpsw_dev = tx_pri_to_cpsw_dev(kobj);
+
+	if (!attribute->store)
+		return -EIO;
+
+	return attribute->store(cpsw_dev, attribute, buf, count);
+}
+
+static const struct sysfs_ops cpsw_tx_pri_sysfs_ops = {
+	.show = cpsw_tx_pri_attr_show,
+	.store = cpsw_tx_pri_attr_store,
+};
+
+static struct kobj_type cpsw_tx_pri_ktype = {
+	.sysfs_ops = &cpsw_tx_pri_sysfs_ops,
+	.default_attrs = cpsw_tx_pri_default_attrs,
+};
+
+static const struct cpsw_mod_info cpsw_port_vlans[] = {
+	{
+		.name		= "port_vlan_id",
+		.shift		= 0,
+		.bits		= 12,
+	},
+	{
+		.name		= "port_cfi",
+		.shift		= 12,
+		.bits		= 1,
+	},
+	{
+		.name		= "port_vlan_pri",
+		.shift		= 13,
+		.bits		= 3,
+	},
+};
+
+static ssize_t cpsw_port_vlan_show(struct cpsw_priv *cpsw_dev,
+		     struct cpsw_attribute *attr,
+		     char *buf)
+{
+	int idx, len = 0, total_len = 0, port;
+	struct cpsw_intf *cpsw_intf;
+	struct cpsw_slave *slave;
+	u32 reg;
+
+	port = (int)(attr->context);
+
+	if (port == cpsw_dev->host_port) {
+		/* Host port */
+		reg = __raw_readl(&cpsw_dev->host_port_regs->port_vlan);
+		len = cpsw_attr_info_show(attr->info, attr->info_size,
+					reg, buf);
+		return len;
+	}
+
+	/* Slave ports */
+	for_each_intf(cpsw_intf, cpsw_dev) {
+		if (cpsw_intf->multi_if) {
+			slave = cpsw_intf->slaves;
+			if (slave->port_num != port)
+				continue;
+			reg = __raw_readl(&slave->regs->port_vlan);
+			len = cpsw_attr_info_show(attr->info, attr->info_size,
+					reg, buf+total_len);
+			total_len += len;
+		} else {
+			for (idx = 0; idx < cpsw_intf->num_slaves; idx++) {
+				slave = cpsw_intf->slaves + idx;
+				if (slave->port_num != port)
+					continue;
+				reg = __raw_readl(&slave->regs->port_vlan);
+				len = cpsw_attr_info_show(attr->info,
+					attr->info_size, reg, buf+total_len);
+				total_len += len;
+			}
+		}
+	}
+	return total_len;
+}
+
+static ssize_t cpsw_port_vlan_store(struct cpsw_priv *cpsw_dev,
+			      struct cpsw_attribute *attr,
+			      const char *buf, size_t count)
+{
+	const struct cpsw_mod_info *i;
+	struct cpsw_parse_result res;
+	struct cpsw_intf *cpsw_intf;
+	struct cpsw_slave *slave;
+	void __iomem *r = NULL;
+	int ret, idx, port;
+
+	port = (int)(attr->context);
+
+	ret = cpsw_attr_parse_set_command(cpsw_dev, attr, buf, count, &res);
+	if (ret)
+		return ret;
+
+	i = &(attr->info[res.control]);
+
+	/* Host port */
+	if (port == cpsw_dev->host_port) {
+		r = &cpsw_dev->host_port_regs->port_vlan;
+		goto set;
+	}
+
+	/* Slave port */
+	for_each_intf(cpsw_intf, cpsw_dev) {
+		if (cpsw_intf->multi_if) {
+			slave = cpsw_intf->slaves;
+			if (slave->port_num == port) {
+				r = &slave->regs->port_vlan;
+				goto set;
+			}
+		} else
+			for (idx = 0; idx < cpsw_intf->num_slaves; idx++) {
+				slave = cpsw_intf->slaves + idx;
+				if (slave->port_num == port) {
+					r = &slave->regs->port_vlan;
+					goto set;
+				}
+			}
+	}
+
+	if (!r)
+		return  -ENOENT;
+
+set:
+	cpsw_info_set_reg_field(r, i, res.value);
+	return count;
+}
+
+static struct cpsw_attribute cpsw_pvlan_0_attribute =
+	__CPSW_CTXT_ATTR(0, S_IRUGO | S_IWUSR,
+			cpsw_port_vlan_show,
+			cpsw_port_vlan_store,
+			cpsw_port_vlans, (void *)0);
+
+static struct cpsw_attribute cpsw_pvlan_1_attribute =
+	__CPSW_CTXT_ATTR(1, S_IRUGO | S_IWUSR,
+			cpsw_port_vlan_show,
+			cpsw_port_vlan_store,
+			cpsw_port_vlans, (void *)1);
+
+static struct cpsw_attribute cpsw_pvlan_2_attribute =
+	__CPSW_CTXT_ATTR(2, S_IRUGO | S_IWUSR,
+			cpsw_port_vlan_show,
+			cpsw_port_vlan_store,
+			cpsw_port_vlans, (void *)2);
+
+static struct cpsw_attribute cpsw_pvlan_3_attribute =
+	__CPSW_CTXT_ATTR(3, S_IRUGO | S_IWUSR,
+			cpsw_port_vlan_show,
+			cpsw_port_vlan_store,
+			cpsw_port_vlans, (void *)3);
+
+static struct cpsw_attribute cpsw_pvlan_4_attribute =
+	__CPSW_CTXT_ATTR(4, S_IRUGO | S_IWUSR,
+			cpsw_port_vlan_show,
+			cpsw_port_vlan_store,
+			cpsw_port_vlans, (void *)4);
+
+static struct attribute *cpsw_pvlan_default_attrs[] = {
+	&cpsw_pvlan_0_attribute.attr,
+	&cpsw_pvlan_1_attribute.attr,
+	&cpsw_pvlan_2_attribute.attr,
+	&cpsw_pvlan_3_attribute.attr,
+	&cpsw_pvlan_4_attribute.attr,
+	NULL
+};
+
+static ssize_t cpsw_pvlan_attr_show(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	struct cpsw_attribute *attribute = to_cpsw_attr(attr);
+	struct cpsw_priv *cpsw_dev = pvlan_to_cpsw_dev(kobj);
+
+	if (!attribute->show)
+		return -EIO;
+
+	return attribute->show(cpsw_dev, attribute, buf);
+}
+
+static ssize_t cpsw_pvlan_attr_store(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	struct cpsw_attribute *attribute = to_cpsw_attr(attr);
+	struct cpsw_priv *cpsw_dev = pvlan_to_cpsw_dev(kobj);
+
+	if (!attribute->store)
+		return -EIO;
+
+	return attribute->store(cpsw_dev, attribute, buf, count);
+}
+
+static const struct sysfs_ops cpsw_pvlan_sysfs_ops = {
+	.show = cpsw_pvlan_attr_show,
+	.store = cpsw_pvlan_attr_store,
+};
+
+static struct kobj_type cpsw_pvlan_ktype = {
+	.sysfs_ops = &cpsw_pvlan_sysfs_ops,
+	.default_attrs = cpsw_pvlan_default_attrs,
+};
+
+static void cpsw_reset_mod_stats(struct cpsw_priv *cpsw_dev, int stat_mod)
+{
+	struct cpsw_hw_stats __iomem *cpsw_statsa = cpsw_dev->hw_stats_regs[0];
+	struct cpsw_hw_stats __iomem *cpsw_statsb = cpsw_dev->hw_stats_regs[1];
+	void *p = NULL;
+	int i;
+
+	if (stat_mod == CPSW_STATSA_MODULE)
+		p = cpsw_statsa;
+	else
+		p = cpsw_statsb;
+
+	for (i = 0; i < ETHTOOL_STATS_NUM; i++) {
+		if (et_stats[i].type == stat_mod) {
+			cpsw_dev->hw_stats[i] = 0;
+			p = (u8 *)p + et_stats[i].offset;
+			*(u32 *)p = 0xffffffff;
+		}
+	}
+	return;
+}
+
+static ssize_t cpsw_stats_mod_store(struct cpsw_priv *cpsw_dev,
+			      struct cpsw_attribute *attr,
+			      const char *buf, size_t count)
+{
+	unsigned long end;
+	int stat_mod;
+
+	if (kstrtoul(buf, 0, &end) != 0 || (end != 0))
+		return -EINVAL;
+
+	stat_mod = (int)(attr->context);
+	mutex_lock(&cpsw_dev->hw_stats_lock);
+	cpsw_reset_mod_stats(cpsw_dev, stat_mod);
+	mutex_unlock(&cpsw_dev->hw_stats_lock);
+	return count;
+}
+
+static struct cpsw_attribute cpsw_stats_a_attribute =
+	__CPSW_ATTR_FULL(A, S_IWUSR, NULL, cpsw_stats_mod_store,
+			NULL, 0, (void *)CPSW_STATSA_MODULE);
+
+static struct cpsw_attribute cpsw_stats_b_attribute =
+	__CPSW_ATTR_FULL(B, S_IWUSR, NULL, cpsw_stats_mod_store,
+			NULL, 0, (void *)CPSW_STATSB_MODULE);
+
+static struct attribute *cpsw_stats_default_attrs[] = {
+	&cpsw_stats_a_attribute.attr,
+	&cpsw_stats_b_attribute.attr,
+	NULL
+};
+
+static ssize_t cpsw_stats_attr_store(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	struct cpsw_attribute *attribute = to_cpsw_attr(attr);
+	struct cpsw_priv *cpsw_dev = stats_to_cpsw_dev(kobj);
+
+	if (!attribute->store)
+		return -EIO;
+
+	return attribute->store(cpsw_dev, attribute, buf, count);
+}
+
+static const struct sysfs_ops cpsw_stats_sysfs_ops = {
+	.store = cpsw_stats_attr_store,
+};
+
+static struct kobj_type cpsw_stats_ktype = {
+	.sysfs_ops = &cpsw_stats_sysfs_ops,
+	.default_attrs = cpsw_stats_default_attrs,
+};
+
+static struct attribute *cpsw_default_attrs[] = {
+	&cpsw_version_attribute.attr,
+	&cpsw_control_attribute.attr,
+	&cpsw_pri_type_attribute.attr,
+	&cpsw_flow_control_attribute.attr,
+	NULL
+};
+
+static ssize_t cpsw_attr_show(struct kobject *kobj, struct attribute *attr,
+				  char *buf)
+{
+	struct cpsw_attribute *attribute = to_cpsw_attr(attr);
+	struct cpsw_priv *cpsw_dev = to_cpsw_dev(kobj);
+
+	if (!attribute->show)
+		return -EIO;
+
+	return attribute->show(cpsw_dev, attribute, buf);
+}
+
+static ssize_t cpsw_attr_store(struct kobject *kobj, struct attribute *attr,
+				   const char *buf, size_t count)
+{
+	struct cpsw_attribute *attribute = to_cpsw_attr(attr);
+	struct cpsw_priv *cpsw_dev = to_cpsw_dev(kobj);
+
+	if (!attribute->store)
+		return -EIO;
+
+	return attribute->store(cpsw_dev, attribute, buf, count);
+}
+
+static const struct sysfs_ops cpsw_sysfs_ops = {
+	.show = cpsw_attr_show,
+	.store = cpsw_attr_store,
+};
+
+static struct kobj_type cpsw_ktype = {
+	.sysfs_ops = &cpsw_sysfs_ops,
+	.default_attrs = cpsw_default_attrs,
+};
+
+static void keystone_get_drvinfo(struct net_device *ndev,
+			     struct ethtool_drvinfo *info)
+{
+	strncpy(info->driver, NETCP_DRIVER_NAME, sizeof(info->driver));
+	strncpy(info->version, NETCP_DRIVER_VERSION, sizeof(info->version));
+}
+
+static u32 keystone_get_msglevel(struct net_device *ndev)
+{
+	struct netcp_priv *netcp = netdev_priv(ndev);
+	return netcp->msg_enable;
+}
+
+static void keystone_set_msglevel(struct net_device *ndev, u32 value)
+{
+	struct netcp_priv *netcp = netdev_priv(ndev);
+	netcp->msg_enable = value;
+}
+
+static void keystone_get_stat_strings(struct net_device *ndev,
+				   uint32_t stringset, uint8_t *data)
+{
+	int i;
+
+	switch (stringset) {
+	case ETH_SS_STATS:
+		for (i = 0; i < ETHTOOL_STATS_NUM; i++) {
+			memcpy(data, et_stats[i].desc, ETH_GSTRING_LEN);
+			data += ETH_GSTRING_LEN;
+		}
+		break;
+	case ETH_SS_TEST:
+		break;
+	}
+}
+
+static int keystone_get_sset_count(struct net_device *ndev, int stringset)
+{
+	switch (stringset) {
+	case ETH_SS_TEST:
+		return 0;
+	case ETH_SS_STATS:
+		return ETHTOOL_STATS_NUM;
+	default:
+		return -EINVAL;
+	}
+}
+
+static void cpsw_update_stats(struct cpsw_priv *cpsw_dev, uint64_t *data)
+{
+	struct cpsw_hw_stats __iomem *cpsw_statsa = cpsw_dev->hw_stats_regs[0];
+	struct cpsw_hw_stats __iomem *cpsw_statsb = cpsw_dev->hw_stats_regs[1];
+	void *p = NULL;
+	u32 tmp = 0;
+	int i;
+
+	for (i = 0; i < ETHTOOL_STATS_NUM; i++) {
+		switch (et_stats[i].type) {
+		case CPSW_STATSA_MODULE:
+			p = cpsw_statsa;
+			break;
+		case CPSW_STATSB_MODULE:
+			p  = cpsw_statsb;
+			break;
+		}
+
+		p = (u8 *)p + et_stats[i].offset;
+		tmp = *(u32 *)p;
+		cpsw_dev->hw_stats[i] = cpsw_dev->hw_stats[i] + tmp;
+		if (data)
+			data[i] = cpsw_dev->hw_stats[i];
+		*(u32 *)p = tmp;
+	}
+
+	return;
+}
+
+static void keystone_get_ethtool_stats(struct net_device *ndev,
+				       struct ethtool_stats *stats,
+				       uint64_t *data)
+{
+	mutex_lock(&priv->hw_stats_lock);
+	cpsw_update_stats(priv, data);
+	mutex_unlock(&priv->hw_stats_lock);
+
+	return;
+}
+
+static int keystone_get_settings(struct net_device *ndev,
+			      struct ethtool_cmd *cmd)
+{
+	struct phy_device *phy = ndev->phydev;
+	struct cpsw_slave *slave;
+	int ret;
+
+	if (!phy)
+		return -EINVAL;
+
+	slave = (struct cpsw_slave *)phy->context;
+	if (!slave)
+		return -EINVAL;
+
+	ret = phy_ethtool_gset(phy, cmd);
+	if (!ret)
+		cmd->port = slave->phy_port_t;
+
+	return ret;
+}
+
+static int keystone_set_settings(struct net_device *ndev,
+				struct ethtool_cmd *cmd)
+{
+	struct phy_device *phy = ndev->phydev;
+	struct cpsw_slave *slave;
+	u32 features = cmd->advertising & cmd->supported;
+
+	if (!phy)
+		return -EINVAL;
+
+	slave = (struct cpsw_slave *)phy->context;
+	if (!slave)
+		return -EINVAL;
+
+	if (cmd->port != slave->phy_port_t) {
+		if ((cmd->port == PORT_TP) && !(features & ADVERTISED_TP))
+			return -EINVAL;
+
+		if ((cmd->port == PORT_AUI) && !(features & ADVERTISED_AUI))
+			return -EINVAL;
+
+		if ((cmd->port == PORT_BNC) && !(features & ADVERTISED_BNC))
+			return -EINVAL;
+
+		if ((cmd->port == PORT_MII) && !(features & ADVERTISED_MII))
+			return -EINVAL;
+
+		if ((cmd->port == PORT_FIBRE) && !(features & ADVERTISED_FIBRE))
+			return -EINVAL;
+	}
+
+	slave->phy_port_t = cmd->port;
+
+	return phy_ethtool_sset(phy, cmd);
+}
+
+static const struct ethtool_ops keystone_ethtool_ops = {
+	.get_drvinfo		= keystone_get_drvinfo,
+	.get_link		= ethtool_op_get_link,
+	.get_msglevel		= keystone_get_msglevel,
+	.set_msglevel		= keystone_set_msglevel,
+	.get_strings		= keystone_get_stat_strings,
+	.get_sset_count		= keystone_get_sset_count,
+	.get_ethtool_stats	= keystone_get_ethtool_stats,
+	.get_settings		= keystone_get_settings,
+	.set_settings		= keystone_set_settings,
+};
+
+#define mac_hi(mac)	(((mac)[0] << 0) | ((mac)[1] << 8) |	\
+			 ((mac)[2] << 16) | ((mac)[3] << 24))
+#define mac_lo(mac)	(((mac)[4] << 0) | ((mac)[5] << 8))
+
+static void cpsw_set_slave_mac(struct cpsw_slave *slave,
+			       struct cpsw_intf *cpsw_intf)
+{
+	struct net_device *ndev = cpsw_intf->ndev;
+
+	__raw_writel(mac_hi(ndev->dev_addr), &slave->regs->sa_hi);
+	__raw_writel(mac_lo(ndev->dev_addr), &slave->regs->sa_lo);
+}
+
+static inline int cpsw_get_slave_port(struct cpsw_priv *priv, u32 slave_num)
+{
+	if (priv->host_port == 0)
+		return slave_num + 1;
+	else
+		return slave_num;
+}
+
+static void _cpsw_adjust_link(struct cpsw_slave *slave, bool *link)
+{
+	struct phy_device *phy = slave->phy;
+	u32 mac_control = 0;
+	u32 slave_port;
+
+	if (!phy)
+		return;
+
+	slave_port = slave->port_num;
+
+	if (phy->link) {
+		mac_control = slave->mac_control;
+		mac_control |= MACSL_ENABLE | MACSL_RX_ENABLE_EXT_CTL |
+				MACSL_RX_ENABLE_CSF;
+		/* enable forwarding */
+		cpsw_ale_control_set(slave->ale, slave_port,
+				     ALE_PORT_STATE, ALE_PORT_STATE_FORWARD);
+
+		if (phy->duplex)
+			mac_control |= BIT(0);	/* FULLDUPLEXEN	*/
+		else
+			mac_control &= ~0x1;
+
+		*link = true;
+	} else {
+		mac_control = 0;
+		/* disable forwarding */
+		cpsw_ale_control_set(slave->ale, slave_port,
+				     ALE_PORT_STATE, ALE_PORT_STATE_DISABLE);
+	}
+
+	if (mac_control != slave->mac_control) {
+		phy_print_status(phy);
+		__raw_writel(mac_control, &slave->sliver->mac_control);
+	}
+
+	slave->mac_control = mac_control;
+}
+
+static void cpsw_adjust_link(struct net_device *n_dev, void *context)
+{
+	struct cpsw_slave *slave = (struct cpsw_slave *)context;
+	struct netcp_priv *netcp = netdev_priv(n_dev);
+	bool link = false;
+
+	_cpsw_adjust_link(slave, &link);
+
+	if (link)
+		netcp->link_state |= BIT(slave->slave_num);
+	else
+		netcp->link_state &= ~BIT(slave->slave_num);
+}
+
+/*
+ * Reset the the mac sliver
+ * Soft reset is set and polled until clear, or until a timeout occurs
+ */
+static int cpsw_port_reset(struct cpsw_slave *slave)
+{
+	u32 i, v;
+
+	/* Set the soft reset bit */
+	__raw_writel(SOFT_RESET,
+		     &slave->sliver->soft_reset);
+
+	/* Wait for the bit to clear */
+	for (i = 0; i < DEVICE_EMACSL_RESET_POLL_COUNT; i++) {
+		v = __raw_readl(&slave->sliver->soft_reset);
+		if ((v & SOFT_RESET_MASK) !=
+		    SOFT_RESET)
+			return 0;
+	}
+
+	/* Timeout on the reset */
+	return GMACSL_RET_WARN_RESET_INCOMPLETE;
+}
+
+/*
+ * Configure the mac sliver
+ */
+static void cpsw_port_config(struct cpsw_slave *slave, int max_rx_len)
+{
+	if (max_rx_len > MAX_SIZE_STREAM_BUFFER)
+		max_rx_len = MAX_SIZE_STREAM_BUFFER;
+
+	__raw_writel(max_rx_len, &slave->sliver->rx_maxlen);
+	
+	__raw_writel(MACSL_ENABLE | MACSL_RX_ENABLE_EXT_CTL |
+		     MACSL_RX_ENABLE_CSF, &slave->sliver->mac_control);
+}
+
+static void cpsw_slave_stop(struct cpsw_slave *slave, struct cpsw_priv *priv)
+{
+	cpsw_port_reset(slave);
+
+	if (!slave->phy)
+		return;
+
+	phy_stop(slave->phy);
+	phy_disconnect(slave->phy);
+	slave->phy = NULL;
+}
+
+static void cpsw_slave_link(struct cpsw_slave *slave,
+			    struct cpsw_intf *cpsw_intf)
+{
+	struct netcp_priv *netcp = netdev_priv(cpsw_intf->ndev);
+
+	if (slave->link_interface == SGMII_LINK_MAC_PHY) {
+		if (netcp->link_state)
+			cpsw_intf->sgmii_link |= BIT(slave->slave_num);
+		else
+			cpsw_intf->sgmii_link &= ~BIT(slave->slave_num);
+	}
+}
+
+static void cpsw_slave_open(struct cpsw_slave *slave,
+			    struct cpsw_intf *cpsw_intf)
+{
+	struct cpsw_priv *priv = cpsw_intf->cpsw_priv;
+	char name[32];		/* FIXME: Unused variable */
+	u32 slave_port;
+
+	snprintf(name, sizeof(name), "slave-%d", slave->slave_num);
+
+	keystone_sgmii_reset(priv->sgmii_port_regs, slave->slave_num);
+
+	keystone_sgmii_config(priv->sgmii_port_regs, slave->slave_num,
+				slave->link_interface);
+
+	cpsw_port_reset(slave);
+
+	cpsw_port_config(slave, priv->rx_packet_max);
+
+	cpsw_set_slave_mac(slave, cpsw_intf);
+
+	slave->mac_control = MACSL_ENABLE | MACSL_RX_ENABLE_EXT_CTL |
+				MACSL_RX_ENABLE_CSF;
+
+	slave_port = cpsw_get_slave_port(priv, slave->slave_num);
+
+	slave->port_num = slave_port;
+	slave->ale = priv->ale;
+
+	/* enable forwarding */
+	cpsw_ale_control_set(priv->ale, slave_port,
+			     ALE_PORT_STATE, ALE_PORT_STATE_FORWARD);
+
+	cpsw_ale_add_mcast(priv->ale, cpsw_intf->ndev->broadcast,
+			   1 << slave_port, 0, ALE_MCAST_FWD_2,
+			   CPSW_NON_VLAN_ADDR);
+
+	if (slave->link_interface == SGMII_LINK_MAC_PHY) {
+		slave->phy = of_phy_connect(cpsw_intf->ndev,
+					    cpsw_intf->phy_node,
+					    &cpsw_adjust_link, 0,
+					    PHY_INTERFACE_MODE_SGMII,
+					    slave);
+		if (IS_ERR_OR_NULL(slave->phy)) {
+			dev_err(priv->dev, "phy not found on slave %d\n",
+				slave->slave_num);
+			slave->phy = NULL;
+		} else {
+			dev_info(priv->dev, "phy found: id is: 0x%s\n",
+				 dev_name(&slave->phy->dev));
+			cpsw_intf->ndev->phydev = slave->phy;
+			slave->phy_port_t = PORT_MII;
+			phy_start(slave->phy);
+		}
+	}
+}
+
+static void cpsw_init_host_port(struct cpsw_priv *priv,
+				struct cpsw_intf *cpsw_intf)
+{
+	/* Max length register */
+	__raw_writel(MAX_SIZE_STREAM_BUFFER,
+		     &priv->host_port_regs->rx_maxlen);
+
+	if (priv->ale_refcnt == 1)
+		cpsw_ale_start(priv->ale);
+
+	if (priv->multi_if)
+		cpsw_ale_control_set(priv->ale, 0, ALE_BYPASS, 1);
+
+	cpsw_ale_control_set(priv->ale, 0, ALE_NO_PORT_VLAN, 1);
+
+	cpsw_ale_control_set(priv->ale, priv->host_port,
+			     ALE_PORT_STATE, ALE_PORT_STATE_FORWARD);
+
+	cpsw_ale_control_set(priv->ale, 0,
+			     ALE_PORT_UNKNOWN_VLAN_MEMBER,
+			     CPSW_MASK_ALL_PORTS);
+
+	cpsw_ale_control_set(priv->ale, 0,
+			     ALE_PORT_UNKNOWN_MCAST_FLOOD,
+			     CPSW_MASK_PHYS_PORTS);
+
+	cpsw_ale_control_set(priv->ale, 0,
+			     ALE_PORT_UNKNOWN_REG_MCAST_FLOOD,
+			     CPSW_MASK_ALL_PORTS);
+
+	cpsw_ale_control_set(priv->ale, 0,
+			     ALE_PORT_UNTAGGED_EGRESS,
+			     CPSW_MASK_ALL_PORTS);
+}
+
+static void cpsw_slave_init(struct cpsw_slave *slave, struct cpsw_priv *priv)
+{
+	void __iomem		*regs = priv->ss_regs;
+	int			slave_num = slave->slave_num;
+
+	slave->regs	= regs + priv->slave_reg_ofs + (0x30 * slave_num);
+	slave->sliver	= regs + priv->sliver_reg_ofs + (0x40 * slave_num);
+}
+
+static void cpsw_add_mcast_addr(struct cpsw_intf *cpsw_intf, u8 *addr)
+{
+	struct cpsw_priv *cpsw_dev = cpsw_intf->cpsw_priv;
+	u16 vlan_id;
+
+	cpsw_ale_add_mcast(cpsw_dev->ale, addr, CPSW_MASK_ALL_PORTS, 0,
+			   ALE_MCAST_FWD_2, -1);
+	for_each_set_bit(vlan_id, cpsw_intf->active_vlans, VLAN_N_VID) {
+		cpsw_ale_add_mcast(cpsw_dev->ale, addr, CPSW_MASK_ALL_PORTS, 0,
+				   ALE_MCAST_FWD_2, vlan_id);
+	}
+}
+
+static void cpsw_add_ucast_addr(struct cpsw_intf *cpsw_intf, u8 *addr)
+{
+	struct cpsw_priv *cpsw_dev = cpsw_intf->cpsw_priv;
+	u16 vlan_id;
+
+	cpsw_ale_add_ucast(cpsw_dev->ale, addr, cpsw_dev->host_port, 0, -1);
+
+	for_each_set_bit(vlan_id, cpsw_intf->active_vlans, VLAN_N_VID)
+		cpsw_ale_add_ucast(cpsw_dev->ale, addr, cpsw_dev->host_port, 0,
+				   vlan_id);
+}
+
+static void cpsw_del_mcast_addr(struct cpsw_intf *cpsw_intf, u8 *addr)
+{
+	struct cpsw_priv *cpsw_dev = cpsw_intf->cpsw_priv;
+	u16 vlan_id;
+
+	cpsw_ale_del_mcast(cpsw_dev->ale, addr, CPSW_MASK_ALL_PORTS, -1);
+
+	for_each_set_bit(vlan_id, cpsw_intf->active_vlans, VLAN_N_VID) {
+		cpsw_ale_del_mcast(cpsw_dev->ale, addr, CPSW_MASK_ALL_PORTS,
+				   vlan_id);
+	}
+}
+
+static void cpsw_del_ucast_addr(struct cpsw_intf *cpsw_intf, u8 *addr)
+{
+	struct cpsw_priv *cpsw_dev = cpsw_intf->cpsw_priv;
+	u16 vlan_id;
+
+	cpsw_ale_del_ucast(cpsw_dev->ale, addr, cpsw_dev->host_port, -1);
+
+	for_each_set_bit(vlan_id, cpsw_intf->active_vlans, VLAN_N_VID) {
+		cpsw_ale_del_ucast(cpsw_dev->ale, addr, cpsw_dev->host_port,
+				   vlan_id);
+	}
+}
+
+int cpsw_add_addr(void *intf_priv, struct netcp_addr *naddr)
+{
+	struct cpsw_intf *cpsw_intf = intf_priv;
+	struct cpsw_priv *cpsw_dev = cpsw_intf->cpsw_priv;
+
+	dev_dbg(cpsw_dev->dev, "ethss adding address %pM, type %d\n",
+		naddr->addr, naddr->type);
+
+	switch (naddr->type) {
+	case ADDR_MCAST:
+	case ADDR_BCAST:
+		cpsw_add_mcast_addr(cpsw_intf, naddr->addr);
+		break;
+	case ADDR_UCAST:
+	case ADDR_DEV:
+		cpsw_add_ucast_addr(cpsw_intf, naddr->addr);
+		break;
+	case ADDR_ANY:
+		/* nothing to do for promiscuous */
+	default:
+		break;
+	}
+
+	return 0;
+}
+
+int cpsw_del_addr(void *intf_priv, struct netcp_addr *naddr)
+{
+	struct cpsw_intf *cpsw_intf = intf_priv;
+	struct cpsw_priv *cpsw_dev = cpsw_intf->cpsw_priv;
+
+	dev_dbg(cpsw_dev->dev, "ethss deleting address %pM, type %d\n",
+		naddr->addr, naddr->type);
+
+	switch (naddr->type) {
+	case ADDR_MCAST:
+	case ADDR_BCAST:
+		cpsw_del_mcast_addr(cpsw_intf, naddr->addr);
+		break;
+	case ADDR_UCAST:
+	case ADDR_DEV:
+		cpsw_del_ucast_addr(cpsw_intf, naddr->addr);
+		break;
+	case ADDR_ANY:
+		/* nothing to do for promiscuous */
+	default:
+		break;
+	}
+
+	return 0;
+}
+
+int cpsw_add_vid(void *intf_priv, int vid)
+{
+	struct cpsw_intf *cpsw_intf = intf_priv;
+	struct cpsw_priv *cpsw_dev = cpsw_intf->cpsw_priv;
+
+	set_bit(vid, cpsw_intf->active_vlans);
+
+	cpsw_ale_add_vlan(cpsw_dev->ale, vid, CPSW_MASK_ALL_PORTS,
+			  CPSW_MASK_ALL_PORTS, CPSW_MASK_PHYS_PORTS,
+			  CPSW_MASK_NO_PORTS);
+
+	return 0;
+}
+
+int cpsw_del_vid(void *intf_priv, int vid)
+{
+	struct cpsw_intf *cpsw_intf = intf_priv;
+	struct cpsw_priv *cpsw_dev = cpsw_intf->cpsw_priv;
+
+	cpsw_ale_del_vlan(cpsw_dev->ale, vid);
+
+	clear_bit(vid, cpsw_intf->active_vlans);
+
+	return 0;
+}
+
+int cpsw_ioctl(void *intf_priv, struct ifreq *req, int cmd)
+{
+	struct cpsw_intf *cpsw_intf = intf_priv;
+	struct cpsw_slave *slave = cpsw_intf->slaves;
+	struct phy_device *phy = slave->phy;
+	int ret;
+
+	if (!phy)
+		return -EOPNOTSUPP;
+
+	ret = phy_mii_ioctl(phy, req, cmd);
+	if ((cmd == SIOCSHWTSTAMP) && (ret == -ERANGE))
+		ret = -EOPNOTSUPP;
+
+	return ret;
+}
+
+static void cpsw_timer(unsigned long arg)
+{
+	struct cpsw_intf *cpsw_intf = (struct cpsw_intf *)arg;
+	struct cpsw_priv *cpsw_dev = cpsw_intf->cpsw_priv;
+	
+	if (cpsw_dev->multi_if)
+		cpsw_intf->sgmii_link =
+			keystone_sgmii_get_port_link(cpsw_dev->sgmii_port_regs,
+						     cpsw_intf->slave_port);
+	else
+		cpsw_intf->sgmii_link =
+			keystone_sgmii_link_status(cpsw_dev->sgmii_port_regs,
+						   cpsw_intf->num_slaves);
+
+	for_each_slave(cpsw_intf, cpsw_slave_link, cpsw_intf);
+
+	/* FIXME: Don't aggregate link statuses in multi-interface case */
+	if (cpsw_intf->sgmii_link) {
+		/* link ON */
+		if (!netif_carrier_ok(cpsw_intf->ndev))
+			netif_carrier_on(cpsw_intf->ndev);
+		/*
+		 * reactivate the transmit queue if
+		 * it is stopped
+		 */
+		if (netif_running(cpsw_intf->ndev) &&
+		    netif_queue_stopped(cpsw_intf->ndev))
+			netif_wake_queue(cpsw_intf->ndev);
+	} else {
+		/* link OFF */
+		if (netif_carrier_ok(cpsw_intf->ndev))
+			netif_carrier_off(cpsw_intf->ndev);
+		if (!netif_queue_stopped(cpsw_intf->ndev))
+			netif_stop_queue(cpsw_intf->ndev);
+	}
+
+	mutex_lock(&cpsw_dev->hw_stats_lock);
+	cpsw_update_stats(cpsw_dev, NULL);
+	mutex_unlock(&cpsw_dev->hw_stats_lock);
+
+	cpsw_intf->timer.expires = jiffies + (HZ/10);
+	add_timer(&cpsw_intf->timer);
+
+	return;
+}
+
+static int cpsw_tx_hook(int order, void *data, struct netcp_packet *p_info)
+{
+	struct cpsw_intf *cpsw_intf = data;
+
+	p_info->tx_pipe = &cpsw_intf->tx_pipe;
+	return 0;
+}
+
+#define	CPSW_TXHOOK_ORDER	0
+
+static int cpsw_open(void *intf_priv, struct net_device *ndev)
+{
+	struct cpsw_intf *cpsw_intf = intf_priv;
+	struct cpsw_priv *cpsw_dev = cpsw_intf->cpsw_priv;
+	struct netcp_priv *netcp = netdev_priv(ndev);
+	struct cpsw_ale_params ale_params;
+	int ret = 0;
+	u32 reg;
+
+	cpsw_dev->cpgmac = clk_get(cpsw_dev->dev, "clk_cpgmac");
+	if (IS_ERR(cpsw_dev->cpgmac)) {
+		ret = PTR_ERR(cpsw_dev->cpgmac);
+		cpsw_dev->cpgmac = NULL;
+		dev_err(cpsw_dev->dev, "unable to get Keystone CPGMAC"
+			" clock: %d\n", ret);
+		return ret;
+	}
+
+	ret = clk_prepare_enable(cpsw_dev->cpgmac);
+	if (ret)
+		goto clk_fail;
+
+	reg = __raw_readl(&cpsw_dev->regs->id_ver);
+
+	dev_info(cpsw_dev->dev, "initializing cpsw version %d.%d (%d) "
+		 "SGMII identification value 0x%x\n",
+		 CPSW_MAJOR_VERSION(reg), CPSW_MINOR_VERSION(reg),
+		 CPSW_RTL_VERSION(reg), CPSW_SGMII_IDENT(reg));
+
+	ret = netcp_txpipe_open(&cpsw_intf->tx_pipe);
+	if (ret)
+		goto txpipe_fail;
+
+	dev_dbg(cpsw_dev->dev, "opened TX channel %s: %p with psflags %d\n",
+		cpsw_intf->tx_pipe.dma_chan_name,
+		cpsw_intf->tx_pipe.dma_channel,
+		cpsw_intf->tx_pipe.dma_psflags);
+
+	cpsw_dev->ale_refcnt++;
+	if (cpsw_dev->ale_refcnt == 1) {
+		memset(&ale_params, 0, sizeof(ale_params));
+
+		ale_params.dev			= cpsw_dev->dev;
+		ale_params.ale_regs		= (void *)((u32)priv->ale_reg);
+		ale_params.ale_ageout		= cpsw_dev->ale_ageout;
+		ale_params.ale_entries		= cpsw_dev->ale_entries;
+		ale_params.ale_ports		= cpsw_dev->ale_ports;
+
+		cpsw_dev->ale = cpsw_ale_create(&ale_params);
+		if (!cpsw_dev->ale) {
+			dev_err(cpsw_dev->dev, "error initializing ale engine\n");
+			ret = -ENODEV;
+			goto ale_fail;
+		} else
+			dev_info(cpsw_dev->dev, "Created a cpsw ale engine\n");
+	}
+
+	for_each_slave(cpsw_intf, cpsw_slave_init, cpsw_dev);
+
+	for_each_slave(cpsw_intf, cpsw_slave_stop, cpsw_dev);
+
+	/* Serdes init */
+	if (cpsw_dev->init_serdes_at_probe == 0) {
+		serdes_init();
+	}
+
+	/* initialize host and slave ports */
+	cpsw_init_host_port(cpsw_dev, cpsw_intf);
+
+	/* disable priority elevation and enable statistics on all ports */
+	__raw_writel(0, &cpsw_dev->regs->ptype);
+
+	/* Control register */
+	__raw_writel(CPSW_CTL_P0_ENABLE, &cpsw_dev->regs->control);
+
+	/* All statistics enabled by default */
+	__raw_writel(CPSW_REG_VAL_STAT_ENABLE_ALL,
+		     &cpsw_dev->regs->stat_port_en);
+
+	for_each_slave(cpsw_intf, cpsw_slave_open, cpsw_intf);
+
+	init_timer(&cpsw_intf->timer);
+	cpsw_intf->timer.data		= (unsigned long)cpsw_intf;
+	cpsw_intf->timer.function	= cpsw_timer;
+	cpsw_intf->timer.expires	= jiffies + CPSW_TIMER_INTERVAL;
+	add_timer(&cpsw_intf->timer);
+	dev_dbg(cpsw_dev->dev, "%s(): cpsw_timer = %p\n", __func__, cpsw_timer);
+
+	netcp_register_txhook(netcp, CPSW_TXHOOK_ORDER,
+			      cpsw_tx_hook, cpsw_intf);
+
+	/* Configure the streaming switch */
+#define	PSTREAM_ROUTE_DMA	6
+	netcp_set_streaming_switch(cpsw_dev->netcp_device, netcp->cpsw_port,
+				   PSTREAM_ROUTE_DMA);
+
+	return 0;
+
+ale_fail:
+	netcp_txpipe_close(&cpsw_intf->tx_pipe);
+txpipe_fail:
+	clk_disable_unprepare(cpsw_dev->cpgmac);
+clk_fail:
+	clk_put(cpsw_dev->cpgmac);
+	cpsw_dev->cpgmac = NULL;
+	return ret;
+}
+
+static int cpsw_close(void *intf_priv, struct net_device *ndev)
+{
+	struct cpsw_intf *cpsw_intf = intf_priv;
+	struct cpsw_priv *cpsw_dev = cpsw_intf->cpsw_priv;
+	struct netcp_priv *netcp = netdev_priv(ndev);
+
+	del_timer_sync(&cpsw_intf->timer);
+
+	cpsw_dev->ale_refcnt--;
+	if (!cpsw_dev->ale_refcnt)
+		cpsw_ale_stop(cpsw_dev->ale);
+	
+	for_each_slave(cpsw_intf, cpsw_slave_stop, cpsw_dev);
+
+	if (!cpsw_dev->ale_refcnt)
+		cpsw_ale_destroy(cpsw_dev->ale);
+
+	netcp_unregister_txhook(netcp, CPSW_TXHOOK_ORDER, cpsw_tx_hook,
+				cpsw_intf);
+	netcp_txpipe_close(&cpsw_intf->tx_pipe);
+
+	clk_disable_unprepare(cpsw_dev->cpgmac);
+	clk_put(cpsw_dev->cpgmac);
+
+	return 0;
+}
+
+static int cpsw_remove(struct netcp_device *netcp_device, void *inst_priv)
+{
+	struct cpsw_priv *cpsw_dev = inst_priv;
+	struct cpsw_intf *cpsw_intf, *tmp;
+
+	of_node_put(cpsw_dev->interfaces);
+
+	list_for_each_entry_safe(cpsw_intf, tmp, &cpsw_dev->cpsw_intf_head,
+				 cpsw_intf_list) {
+		netcp_delete_interface(netcp_device, cpsw_intf->ndev);
+	}
+	BUG_ON(!list_empty(&cpsw_dev->cpsw_intf_head));
+
+	iounmap(cpsw_dev->ss_regs);
+	memset(cpsw_dev, 0x00, sizeof(*cpsw_dev));	/* FIXME: Poison */
+	kfree(cpsw_dev);
+	return 0;
+}
+
+static int init_slave(struct cpsw_priv *cpsw_dev,
+		      struct device_node *node, int slave_num)
+{
+	int ret = 0;
+
+	ret = of_property_read_u32(node, "link-interface",
+				   &cpsw_dev->link[slave_num]);
+	if (ret < 0) {
+		dev_err(cpsw_dev->dev,
+			"missing link-interface value"
+			"defaulting to mac-phy link\n");
+		cpsw_dev->link[slave_num] = 1;
+	}
+
+	cpsw_dev->phy_node[slave_num] = of_parse_phandle(node, "phy-handle", 0);
+
+	return 0;
+}
+
+static int cpsw_create_sysfs_entries(struct cpsw_priv *cpsw_dev)
+{
+	struct device *dev = cpsw_dev->dev;
+	int ret;
+
+	ret = kobject_init_and_add(&cpsw_dev->kobj, &cpsw_ktype,
+		kobject_get(&dev->kobj), "cpsw");
+
+	if (ret) {
+		dev_err(dev, "failed to create cpsw sysfs entry\n");
+		kobject_put(&cpsw_dev->kobj);
+		kobject_put(&dev->kobj);
+		return ret;
+	}
+
+	ret = kobject_init_and_add(&cpsw_dev->tx_pri_kobj,
+		&cpsw_tx_pri_ktype,
+		kobject_get(&cpsw_dev->kobj), "port_tx_pri_map");
+
+	if (ret) {
+		dev_err(dev, "failed to create sysfs port_tx_pri_map entry\n");
+		kobject_put(&cpsw_dev->tx_pri_kobj);
+		kobject_put(&cpsw_dev->kobj);
+		return ret;
+	}
+
+	ret = kobject_init_and_add(&cpsw_dev->pvlan_kobj,
+		&cpsw_pvlan_ktype,
+		kobject_get(&cpsw_dev->kobj), "port_vlan");
+
+	if (ret) {
+		dev_err(dev, "failed to create sysfs port_vlan entry\n");
+		kobject_put(&cpsw_dev->pvlan_kobj);
+		kobject_put(&cpsw_dev->kobj);
+		return ret;
+	}
+
+	ret = kobject_init_and_add(&cpsw_dev->stats_kobj,
+		&cpsw_stats_ktype,
+		kobject_get(&cpsw_dev->kobj), "stats");
+
+	if (ret) {
+		dev_err(dev, "failed to create sysfs stats entry\n");
+		kobject_put(&cpsw_dev->stats_kobj);
+		kobject_put(&cpsw_dev->kobj);
+		return ret;
+	}
+
+	return 0;
+}
+
+static int cpsw_probe(struct netcp_device *netcp_device,
+			struct device *dev,
+			struct device_node *node,
+			void **inst_priv)
+{
+	struct cpsw_priv *cpsw_dev;
+	struct device_node *slaves, *slave, *interfaces;
+	void __iomem *regs;
+	struct net_device *ndev;
+	int slave_num = 0;
+	int i, ret = 0;
+
+	cpsw_dev = devm_kzalloc(dev, sizeof(struct cpsw_priv), GFP_KERNEL);
+	if (!cpsw_dev) {
+		dev_err(dev, "cpsw_dev memory allocation failed\n");
+		return -ENOMEM;
+	}
+	*inst_priv = cpsw_dev;
+	dev_dbg(dev, "%s(): cpsw_priv = %p\n", __func__, cpsw_dev);
+
+	if (!node) {
+		dev_err(dev, "device tree info unavailable\n");
+		ret = -ENODEV;
+		goto exit;
+	}
+
+	cpsw_dev->dev = dev;
+	cpsw_dev->netcp_device = netcp_device;
+	
+	priv = cpsw_dev;	/* FIXME: Remove this!! */
+
+	regs = ioremap(TCI6614_SS_BASE, 0xf00);
+	BUG_ON(!regs);
+
+	ret = of_property_read_u32(node, "serdes_at_probe", &cpsw_dev->init_serdes_at_probe);
+	if (ret < 0) {
+		dev_err(dev, "missing serdes_at_probe parameter, err %d\n", ret);
+		cpsw_dev->init_serdes_at_probe = 0;
+	}
+	dev_dbg(dev, "serdes_at_probe %u\n", cpsw_dev->init_serdes_at_probe);
+#if 0
+	if (cpsw_dev->init_serdes_at_probe == 1) {
+		serdes_init_6638_156p25Mhz();
+	}
+#endif
+	ret = of_property_read_u32(node, "sgmii_module_ofs",
+				   &cpsw_dev->sgmii_module_ofs);
+	if (ret < 0)
+		dev_err(dev, "missing sgmii module offset, err %d\n", ret);
+	
+	ret = of_property_read_u32(node, "switch_module_ofs",
+				   &cpsw_dev->switch_module_ofs);
+	if (ret < 0)
+		dev_err(dev, "missing switch module offset, err %d\n", ret);
+
+	ret = of_property_read_u32(node, "host_port_reg_ofs",
+				   &cpsw_dev->host_port_reg_ofs);
+	if (ret < 0)
+		dev_err(dev, "missing host port reg offset, err %d\n", ret);
+
+	ret = of_property_read_u32(node, "slave_reg_ofs",
+				   &cpsw_dev->slave_reg_ofs);
+	if (ret < 0)
+		dev_err(dev, "missing slave reg offset, err %d\n", ret);
+
+	ret = of_property_read_u32(node, "sliver_reg_ofs",
+				   &cpsw_dev->sliver_reg_ofs);
+	if (ret < 0)
+		dev_err(dev, "missing sliver reg offset, err %d\n", ret);
+
+	ret = of_property_read_u32(node, "hw_stats_reg_ofs",
+				   &cpsw_dev->hw_stats_reg_ofs);
+	if (ret < 0)
+		dev_err(dev, "missing hw stats reg offset, err %d\n", ret);
+
+	ret = of_property_read_u32(node, "ale_reg_ofs",
+				   &cpsw_dev->ale_reg_ofs);
+	if (ret < 0)
+		dev_err(dev, "missing ale reg offset, err %d\n", ret);
+
+
+	ret = of_property_read_u32(node, "num_slaves", &cpsw_dev->num_slaves);
+	if (ret < 0) {
+		dev_err(dev, "missing num_slaves parameter, err %d\n", ret);
+		cpsw_dev->num_slaves = 2;
+	}
+
+	ret = of_property_read_u32(node, "ale_ageout", &cpsw_dev->ale_ageout);
+	if (ret < 0) {
+		dev_err(dev, "missing ale_ageout parameter, err %d\n", ret);
+		cpsw_dev->ale_ageout = 10;
+	}
+
+	ret = of_property_read_u32(node, "ale_entries", &cpsw_dev->ale_entries);
+	if (ret < 0) {
+		dev_err(dev, "missing ale_entries parameter, err %d\n", ret);
+		cpsw_dev->ale_entries = 1024;
+	}
+
+	ret = of_property_read_u32(node, "ale_ports", &cpsw_dev->ale_ports);
+	if (ret < 0) {
+		dev_err(dev, "missing ale_ports parameter, err %d\n", ret);
+		cpsw_dev->ale_ports = 2;
+	}
+
+	ret = of_property_read_u32(node, "intf_tx_queues", &cpsw_dev->intf_tx_queues);
+	if (ret < 0) {
+		dev_err(dev, "missing intf_tx_queues parameter, err %d\n", ret);
+		cpsw_dev->intf_tx_queues = 1;
+	}
+
+	if (of_find_property(node, "multi-interface", NULL))
+		cpsw_dev->multi_if = 1;
+
+	ret = of_property_read_u32(node, "num-interfaces",
+				   &cpsw_dev->num_interfaces);
+	if (ret < 0) {
+		dev_err(dev, "missing num-interfaces parameter\n");
+		cpsw_dev->num_interfaces = 1;
+	}
+
+	ret = of_property_read_u32(node, "slaves-per-interface",
+				   &cpsw_dev->slaves_per_interface);
+	if (ret < 0) {
+		dev_err(dev, "missing slaves-per_interface parameter\n");
+		cpsw_dev->slaves_per_interface = 2;
+	}
+
+	/* FIXME: TCI6614_SS_BASE needs to come from the device tree */
+	regs = ioremap(TCI6614_SS_BASE, 0xf00);
+	BUG_ON(!regs);
+
+	cpsw_dev->ss_regs = regs;
+	cpsw_dev->sgmii_port_regs	= regs + cpsw_dev->sgmii_module_ofs;
+	cpsw_dev->regs = regs + cpsw_dev->switch_module_ofs;
+	cpsw_dev->host_port_regs = regs + cpsw_dev->host_port_reg_ofs;
+	cpsw_dev->hw_stats_regs[0] = regs + cpsw_dev->hw_stats_reg_ofs;
+	cpsw_dev->hw_stats_regs[1] = regs + cpsw_dev->hw_stats_reg_ofs + 0x100;
+	cpsw_dev->ale_reg	  = regs + cpsw_dev->ale_reg_ofs;
+
+	cpsw_dev->host_port = 0;
+	cpsw_dev->rx_packet_max = 9500;
+
+	dev_dbg(dev, "num_slaves = %d\n", cpsw_dev->num_slaves);
+	dev_dbg(dev, "ale_ageout = %d\n", cpsw_dev->ale_ageout);
+	dev_dbg(dev, "ale_entries = %d\n", cpsw_dev->ale_entries);
+	dev_dbg(dev, "ale_ports = %d\n", cpsw_dev->ale_ports);
+
+	slaves = of_get_child_by_name(node, "slaves");
+	if (!slaves) {
+		dev_err(dev, "could not find slaves\n");
+		ret = -ENODEV;
+		goto exit;
+	}
+
+	for_each_child_of_node(slaves, slave) {
+			init_slave(cpsw_dev, slave, slave_num);
+			slave_num++;
+	}
+
+	of_node_put(slaves);
+
+	interfaces = of_get_child_by_name(node, "interfaces");
+	if (!interfaces)
+		dev_err(dev, "could not find interfaces\n");
+
+	cpsw_dev->interfaces = interfaces;
+
+	/* Create the interface */
+	INIT_LIST_HEAD(&cpsw_dev->cpsw_intf_head);
+	if (cpsw_dev->multi_if)
+		for (i = 0; i < cpsw_dev->num_interfaces; i++)
+			netcp_create_interface(netcp_device, &ndev,
+					       NULL, cpsw_dev->intf_tx_queues,
+					       1, (i + 1));
+	else
+		netcp_create_interface(netcp_device, &ndev,
+					       NULL, cpsw_dev->intf_tx_queues,
+					       1, 0);
+	/* init the hw stats lock */
+	mutex_init(&cpsw_dev->hw_stats_lock);
+
+	ret = cpsw_create_sysfs_entries(cpsw_dev);
+	if (ret)
+		goto exit;
+
+	return 0;
+
+exit:
+	if (cpsw_dev->ss_regs)
+		iounmap(cpsw_dev->ss_regs);
+	*inst_priv = NULL;
+	kfree(cpsw_dev);
+	return ret;
+}
+
+static int cpsw_attach(void *inst_priv, struct net_device *ndev,
+		       void **intf_priv)
+{
+	struct cpsw_priv *cpsw_dev = inst_priv;
+	struct cpsw_intf *cpsw_intf;
+	struct netcp_priv *netcp = netdev_priv(ndev);
+	struct device_node *interface;
+	int i = 0, ret = 0;
+	char node_name[24];
+
+	cpsw_intf = devm_kzalloc(cpsw_dev->dev,
+				 sizeof(struct cpsw_intf), GFP_KERNEL);
+	if (!cpsw_intf) {
+		dev_err(cpsw_dev->dev, "cpsw interface memory "
+			"allocation failed\n");
+		return -ENOMEM;
+	}
+	cpsw_intf->ndev = ndev;
+	cpsw_intf->dev = cpsw_dev->dev;
+	cpsw_intf->cpsw_priv = cpsw_dev;
+	cpsw_intf->multi_if = cpsw_dev->multi_if;
+
+	if (cpsw_dev->multi_if)
+		snprintf(node_name, sizeof(node_name), "interface-%d",
+			 netcp->cpsw_port - 1);
+	else
+		snprintf(node_name, sizeof(node_name), "interface-%d",
+			 0);
+
+	interface = of_get_child_by_name(cpsw_dev->interfaces, node_name);
+	if (!interface) {
+		dev_err(cpsw_dev->dev, "interface data not available\n");
+		devm_kfree(cpsw_dev->dev, cpsw_intf);
+		return -ENODEV;
+	}
+	ret = of_property_read_u32(interface, "slave_port",
+				   &cpsw_intf->slave_port);
+	if (ret < 0) {
+		dev_err(cpsw_dev->dev, "missing slave_port paramater\n");
+		return -EINVAL;
+	}
+
+	ret = of_property_read_string(interface, "tx-channel",
+				      &cpsw_intf->tx_chan_name);
+	if (ret < 0) {
+		dev_err(cpsw_dev->dev, "missing tx-channel "
+			"parameter, err %d\n", ret);
+		cpsw_intf->tx_chan_name = "nettx";
+	}
+	dev_info(cpsw_dev->dev, "dma_chan_name %s\n", cpsw_intf->tx_chan_name);
+
+	ret = of_property_read_u32(interface, "tx_queue_depth",
+				   &cpsw_intf->tx_queue_depth);
+	if (ret < 0) {
+		dev_err(cpsw_dev->dev, "missing tx_queue_depth "
+			"parameter, err %d\n", ret);
+		cpsw_intf->tx_queue_depth = 32;
+	}
+	dev_dbg(cpsw_dev->dev, "tx_queue_depth %u\n",
+		cpsw_intf->tx_queue_depth);
+
+	of_node_put(interface);
+
+	cpsw_intf->num_slaves = cpsw_dev->slaves_per_interface;
+
+	cpsw_intf->slaves = devm_kzalloc(cpsw_dev->dev,
+					 sizeof(struct cpsw_slave) *
+					 cpsw_intf->num_slaves, GFP_KERNEL);
+
+	if (!cpsw_intf->slaves) {
+		dev_err(cpsw_dev->dev, "cpsw interface slave memory "
+			"allocation failed\n");
+		devm_kfree(cpsw_dev->dev, cpsw_intf);
+		return -ENOMEM;
+	}
+
+	if (cpsw_dev->multi_if) {
+		cpsw_intf->slaves[i].slave_num = cpsw_intf->slave_port;
+		cpsw_intf->slaves[i].link_interface =
+			cpsw_dev->link[cpsw_intf->slave_port];
+		cpsw_intf->phy_node = cpsw_dev->phy_node[cpsw_intf->slave_port];
+	} else {
+		for (i = 0; i < cpsw_intf->num_slaves; i++) {
+			cpsw_intf->slaves[i].slave_num = i;
+			cpsw_intf->slaves[i].link_interface = cpsw_dev->link[i];
+		}
+	}
+
+	netcp_txpipe_init(&cpsw_intf->tx_pipe, netdev_priv(ndev),
+			  cpsw_intf->tx_chan_name, cpsw_intf->tx_queue_depth);
+
+	cpsw_intf->tx_pipe.dma_psflags	= netcp->cpsw_port;
+
+	SET_ETHTOOL_OPS(ndev, &keystone_ethtool_ops);
+
+	list_add(&cpsw_intf->cpsw_intf_list, &cpsw_dev->cpsw_intf_head);
+
+	*intf_priv = cpsw_intf;
+	return 0;
+}
+
+static int cpsw_release(void *intf_priv)
+{
+	struct cpsw_intf *cpsw_intf = intf_priv;
+
+	SET_ETHTOOL_OPS(cpsw_intf->ndev, NULL);
+
+	list_del(&cpsw_intf->cpsw_intf_list);
+
+	devm_kfree(cpsw_intf->dev, cpsw_intf->slaves);
+	devm_kfree(cpsw_intf->dev, cpsw_intf);
+
+	return 0;
+}
+
+
+static struct netcp_module cpsw_module = {
+	.name		= "keystone-cpsw",
+	.owner		= THIS_MODULE,
+	.probe		= cpsw_probe,
+	.open		= cpsw_open,
+	.close		= cpsw_close,
+	.remove		= cpsw_remove,
+	.attach		= cpsw_attach,
+	.release	= cpsw_release,
+	.add_addr	= cpsw_add_addr,
+	.del_addr	= cpsw_del_addr,
+	.add_vid	= cpsw_add_vid,
+	.del_vid	= cpsw_del_vid,
+	.ioctl		= cpsw_ioctl,
+};
+
+int __init keystone_cpsw_init(void)
+{
+	return netcp_register_module(&cpsw_module);
+}
+//module_init(keystone_cpsw_init);
+
+void __exit keystone_cpsw_exit(void)
+{
+	netcp_unregister_module(&cpsw_module);
+}
+//module_exit(keystone_cpsw_exit);
+
+MODULE_LICENSE("GPL v2");
+MODULE_AUTHOR("Sandeep Paulraj <s-paulraj@ti.com>");
+MODULE_DESCRIPTION("CPSW driver for Keystone devices");
diff --git a/drivers/net/ethernet/ti/keystone_net.h b/drivers/net/ethernet/ti/keystone_net.h
new file mode 100644
index 0000000..f369b24
--- /dev/null
+++ b/drivers/net/ethernet/ti/keystone_net.h
@@ -0,0 +1,224 @@
+/*
+ * Copyright (C) 2012 Texas Instruments
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation version 2.
+ *
+ * This program is distributed "as is" WITHOUT ANY WARRANTY of any
+ * kind, whether express or implied; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+#ifndef __KEYSTONE_NECTP_H__
+#define __KEYSTONE_NECTP_H__
+
+#include <linux/skbuff.h>
+#include <linux/if_vlan.h>
+#include <linux/ethtool.h>
+#include <linux/if_ether.h>
+#include <linux/netdevice.h>
+#include <linux/keystone-dma.h>
+#include <linux/interrupt.h>
+
+/* Maximum Ethernet frame size supported by Keystone switch */
+#define NETCP_MAX_FRAME_SIZE	9504
+#define NETCP_MAX_MCAST_ADDR	16
+
+#define SGMII_LINK_MAC_MAC_AUTONEG	0
+#define SGMII_LINK_MAC_PHY		1
+#define SGMII_LINK_MAC_MAC_FORCED	2
+#define SGMII_LINK_MAC_FIBER		3
+#define SGMII_LINK_MAC_PHY_NO_MDIO	4
+#define XGMII_LINK_MAC_PHY		10
+#define XGMII_LINK_MAC_MAC_FORCED	11
+
+int serdes_init(void);
+void serdes_init_6638_156p25Mhz(void);
+int keystone_sgmii_reset(void __iomem *sgmii_ofs, int port);
+int keystone_sgmii_link_status(void __iomem *sgmii_ofs, int ports);
+int keystone_sgmii_get_port_link(void __iomem *sgmii_ofs, int port);
+int keystone_sgmii_config(void __iomem *sgmii_ofs,
+			  int port, u32 interface);
+
+struct netcp_device;
+
+enum netcp_rx_state {
+	RX_STATE_INTERRUPT,
+	RX_STATE_SCHEDULED,
+	RX_STATE_POLL,
+	RX_STATE_TEARDOWN,
+	RX_STATE_INVALID,
+};
+
+enum netcp_tx_state {
+	TX_STATE_INVALID,
+	TX_STATE_INTERRUPT,
+	TX_STATE_SCHEDULED,
+	TX_STATE_POLL,
+};
+
+struct netcp_tx_pipe {
+	struct netcp_priv		*netcp_priv;
+	struct dma_chan			*dma_channel;
+	const char			*dma_chan_name;
+	u8				 dma_psflags;
+	u8				 filler1;
+	u16				 dma_queue;
+	unsigned int			 dma_queue_depth;
+	unsigned int			 dma_poll_threshold;
+	unsigned int			 dma_pause_threshold;
+	unsigned int			 dma_resume_threshold;
+	atomic_t			 dma_poll_count;
+	struct tasklet_struct		 dma_poll_tasklet;
+	enum netcp_tx_state		 dma_poll_state;
+};
+
+#define ADDR_NEW	BIT(0)
+#define ADDR_VALID	BIT(1)
+
+enum netcp_addr_type {
+	ADDR_ANY,
+	ADDR_DEV,
+	ADDR_UCAST,
+	ADDR_MCAST,
+	ADDR_BCAST
+};
+
+struct netcp_addr {
+	struct netcp_priv	*netcp;
+	unsigned char		 addr[MAX_ADDR_LEN];
+	enum netcp_addr_type	 type;
+	unsigned int		 flags;
+	struct list_head	 node;
+};
+
+struct netcp_priv {
+	spinlock_t			 lock;
+	struct netcp_device		*netcp_device;
+	struct platform_device		*pdev;
+	struct net_device		*ndev;
+	struct napi_struct		 napi;
+	struct device			*dev;
+	int				 cpsw_port;
+	u32				 msg_enable;
+	struct net_device_stats		 stats;
+	int				 rx_packet_max;
+
+	struct dma_chan			*rx_channel;
+	const char			*rx_chan_name;
+
+	u32				 link_state;
+
+	enum netcp_rx_state		 rx_state;
+	struct list_head		 module_head;
+	struct list_head		 interface_list;
+	struct list_head		 txhook_list_head;
+	struct list_head		 rxhook_list_head;
+	struct list_head		 addr_list;
+
+	/* PktDMA configuration data */
+	u32				 rx_queue_depths[KEYSTONE_QUEUES_PER_CHAN];
+	u32				 rx_buffer_sizes[KEYSTONE_QUEUES_PER_CHAN];
+};
+
+#define NETCP_SGLIST_SIZE	(MAX_SKB_FRAGS + 2)
+#define	NETCP_PSDATA_LEN	16
+struct netcp_packet {
+	struct scatterlist		 sg[NETCP_SGLIST_SIZE];
+	int				 sg_ents;
+	struct sk_buff			*skb;
+	u32				 epib[4];
+	u32				 psdata[NETCP_PSDATA_LEN];
+	unsigned int			 psdata_len;
+	struct netcp_priv		*netcp;
+	dma_cookie_t			 cookie;
+	struct netcp_tx_pipe		*tx_pipe;
+};
+
+static inline int netcp_prepend_psdata(struct netcp_packet *p_info, u32 *data, unsigned len)
+{
+	if ((len + p_info->psdata_len) > NETCP_PSDATA_LEN)
+		return -ENOBUFS;
+	p_info->psdata_len += len;
+
+	memcpy(&p_info->psdata[NETCP_PSDATA_LEN - p_info->psdata_len],
+			data, len * sizeof(u32));
+	return 0;
+}
+
+struct netcp_module {
+	const char		*name;
+	struct module		*owner;
+	struct list_head	 module_list;
+	struct list_head	 interface_list;
+
+	/* probe/remove: called once per NETCP instance */
+	int			(*probe)(struct netcp_device *netcp_device,
+					 struct device *device,
+					 struct device_node *node,
+					 void **inst_priv);
+	int			(*remove)(struct netcp_device *netcp_device,
+					  void *inst_priv);
+
+	/* attach/release: called once per network interface */
+	int			(*attach)(void *inst_priv, struct net_device *ndev,
+					  void **intf_priv);
+	int			(*release)(void *intf_priv);
+
+	int			(*open)(void *intf_priv, struct net_device *ndev);
+	int			(*close)(void *intf_priv, struct net_device *ndev);
+	int			(*add_addr)(void *intf_priv,
+					    struct netcp_addr *naddr);
+	int			(*del_addr)(void *intf_priv,
+					    struct netcp_addr *naddr);
+	int			(*add_vid)(void *intf_priv, int vid);
+	int			(*del_vid)(void *intf_priv, int vid);
+	int			(*ioctl)(void *intf_priv, struct ifreq *req,
+					 int cmd);
+};
+
+int netcp_register_module(struct netcp_module *module);
+void netcp_unregister_module(struct netcp_module *module);
+
+u32 netcp_get_streaming_switch(struct netcp_device *netcp_device, int port);
+u32 netcp_set_streaming_switch(struct netcp_device *netcp_device,
+				int port, u32 new_value);
+
+int netcp_create_interface(struct netcp_device *netcp_device,
+			   struct net_device **ndev_p,
+			   const char *ifname_proto,
+			   int tx_queues, int rx_queues,
+			   int cpsw_port);
+void netcp_delete_interface(struct netcp_device *netcp_device,
+			    struct net_device *ndev);
+
+int netcp_txpipe_init(struct netcp_tx_pipe *tx_pipe,
+		struct netcp_priv *netcp_priv,
+		const char *chan_name,
+		int queue_depth);
+int netcp_txpipe_open(struct netcp_tx_pipe *tx_pipe);
+int netcp_txpipe_close(struct netcp_tx_pipe *tx_pipe);
+
+struct dma_chan *netcp_get_rx_chan(struct netcp_priv *priv);
+struct dma_chan *netcp_get_tx_chan(struct netcp_priv *priv);
+
+u32 *netcp_push_psdata(struct netcp_packet *p_info, unsigned words);
+int netcp_align_psdata(struct netcp_packet *p_info, unsigned word_align);
+
+typedef int netcp_hook_rtn(int order, void *data, struct netcp_packet *packet);
+
+int netcp_register_txhook(struct netcp_priv *netcp_priv, int order,
+		netcp_hook_rtn *hook_rtn, void *hook_data);
+int netcp_unregister_txhook(struct netcp_priv *netcp_priv, int order,
+		netcp_hook_rtn *hook_rtn, void *hook_data);
+int netcp_register_rxhook(struct netcp_priv *netcp_priv, int order,
+		netcp_hook_rtn *hook_rtn, void *hook_data);
+int netcp_unregister_rxhook(struct netcp_priv *netcp_priv, int order,
+		netcp_hook_rtn *hook_rtn, void *hook_data);
+
+void *netcp_device_find_module(struct netcp_device *netcp_device,
+		const char *name);
+void xge_serdes_init_156p25Mhz(void);
+int keystone_pcsr_config(void __iomem *pcsr_ofs, int port, u32 interface);
+#endif
diff --git a/drivers/net/ethernet/ti/keystone_net_core.c b/drivers/net/ethernet/ti/keystone_net_core.c
new file mode 100644
index 0000000..81abea1
--- /dev/null
+++ b/drivers/net/ethernet/ti/keystone_net_core.c
@@ -0,0 +1,2052 @@
+/*
+ * Copyright (C) 2012 Texas Instruments Incorporated
+ * Authors: Cyril Chemparathy <cyril@ti.com>
+ *	    Sandeep Paulraj <s-paulraj@ti.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation version 2.
+ *
+ * This program is distributed "as is" WITHOUT ANY WARRANTY of any
+ * kind, whether express or implied; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/io.h>
+#include <linux/of.h>
+#include <linux/clk.h>
+#include <linux/phy.h>
+#include <linux/timer.h>
+#include <linux/module.h>
+#include <linux/device.h>
+#include <linux/uaccess.h>
+#include <linux/if_vlan.h>
+#include <linux/ethtool.h>
+#include <linux/if_ether.h>
+#include <linux/netdevice.h>
+#include <linux/interrupt.h>
+#include <linux/dmaengine.h>
+#include <linux/net_tstamp.h>
+#include <linux/of_net.h>
+#include <linux/of_address.h>
+#include <linux/dma-mapping.h>
+#include <linux/scatterlist.h>
+#include <linux/etherdevice.h>
+#include <linux/platform_device.h>
+#include <linux/delay.h>
+#include <linux/keystone-dma.h>
+
+#include "keystone_net.h"
+
+/* Read the e-fuse value as 32 bit values to be endian independent */
+static inline int emac_arch_get_mac_addr(char *x,
+					 void __iomem *efuse_mac)
+{
+	unsigned int addr0, addr1;
+
+	addr1 = __raw_readl(efuse_mac + 4);
+	addr0 = __raw_readl(efuse_mac);
+
+	x[0] = (addr1 & 0x0000ff00) >> 8;
+	x[1] = addr1 & 0x000000ff;
+	x[2] = (addr0 & 0xff000000) >> 24;
+	x[3] = (addr0 & 0x00ff0000) >> 16;
+	x[4] = (addr0 & 0x0000ff00) >> 8;
+	x[5] = addr0 & 0x000000ff;
+
+	return 0;
+}
+
+static unsigned int sg_count(struct scatterlist *sg, unsigned int max_ents)
+{
+	unsigned int count;
+
+	for (count = 0; sg && (count < max_ents); count++, sg = sg_next(sg))
+		;
+
+	return count;
+}
+
+static const char *netcp_node_name(struct device_node *node)
+{
+	const char *name;
+
+	if (of_property_read_string(node, "label", &name) < 0)
+		name = node->name;
+	if (!name)
+		name = "unknown";
+	return name;
+}
+
+
+/*
+ *  Module management structures
+ */
+struct netcp_device {
+	struct list_head	 device_list;
+	struct list_head	 interface_head;
+	struct list_head	 modpriv_head;
+	struct platform_device	*platform_device;
+	void __iomem		*streaming_switch;
+};
+
+struct netcp_inst_modpriv {
+	struct netcp_device	*netcp_device;
+	struct netcp_module	*netcp_module;
+	struct list_head	 inst_list;
+	void			*module_priv;
+};
+
+struct netcp_intf_modpriv {
+	struct netcp_priv	*netcp_priv;
+	struct netcp_module	*netcp_module;
+	struct list_head	 intf_list;
+	void			*module_priv;
+};
+
+static LIST_HEAD(netcp_devices);
+static LIST_HEAD(netcp_modules);
+static DEFINE_MUTEX(netcp_modules_lock);
+
+static struct kmem_cache *netcp_pinfo_cache;
+
+/*
+ *  Module management routines
+ */
+#define for_each_netcp_module(module)			\
+	list_for_each_entry(module, &netcp_modules, module_list)
+
+#define for_each_netcp_device_module(netcp_device, inst_modpriv) \
+	list_for_each_entry(inst_modpriv, \
+		&((netcp_device)->modpriv_head), inst_list)
+
+int netcp_register_module(struct netcp_module *module)
+{
+	struct netcp_module *tmp;
+	struct netcp_device *netcp_device;
+	int ret;
+
+	BUG_ON(!module->name);
+	BUG_ON(!module->probe);
+
+	mutex_lock(&netcp_modules_lock);
+
+	ret = -EEXIST;
+	for_each_netcp_module(tmp) {
+		if (!strcasecmp(tmp->name, module->name))
+			goto done;
+	}
+
+	list_add_tail(&module->module_list, &netcp_modules);
+
+	list_for_each_entry(netcp_device, &netcp_devices, device_list) {
+		struct platform_device *pdev = netcp_device->platform_device;
+		struct device_node *node = pdev->dev.of_node;
+		struct device_node *child;
+		struct netcp_inst_modpriv *inst_modpriv;
+		struct netcp_priv *netcp_priv;
+
+		/* Find this module in the sub-tree for this device */
+		for_each_child_of_node(node, child) {
+			const char *name = netcp_node_name(child);
+			if (!strcasecmp(module->name, name))
+				break;
+		}
+
+		/* If module not used for this device, skip it */
+		if (child == NULL)
+			continue;
+
+		inst_modpriv = kzalloc(sizeof(*inst_modpriv), GFP_KERNEL);
+		if (!inst_modpriv) {
+			dev_err(&pdev->dev, "Failed to allocate instance for for %s\n", pdev->name);
+			continue;	/* FIXME: Fail instead? */
+		}
+		inst_modpriv->netcp_device = netcp_device;
+		inst_modpriv->netcp_module = module;
+		list_add_tail(&inst_modpriv->inst_list, &netcp_device->modpriv_head);
+
+		dev_dbg(&pdev->dev, "%s(): probing module \"%s\"\n", __func__, module->name);
+
+		ret = module->probe(netcp_device, &pdev->dev, child, &inst_modpriv->module_priv);
+		if (ret) {
+			dev_warn(&pdev->dev, "Probe of module %s failed with %d\n",
+					module->name, ret);
+			list_del(&inst_modpriv->inst_list);
+			kfree(inst_modpriv);
+			continue;
+		}
+
+		/* Attach module to interfaces */
+		list_for_each_entry(netcp_priv, &netcp_device->interface_head, interface_list) {
+			struct netcp_intf_modpriv *intf_modpriv;
+			int found = 0;
+
+			list_for_each_entry(intf_modpriv, &netcp_priv->module_head, intf_list) {
+				if (intf_modpriv->netcp_module == module) {
+					found = 1;
+					break;
+				}
+			}
+
+			if (!found) {
+				intf_modpriv = kzalloc(sizeof(*intf_modpriv), GFP_KERNEL);
+				if (!intf_modpriv) {
+					dev_err(&pdev->dev, "Error allocating intf_modpriv for %s\n",
+							module->name);
+					continue;
+				}
+
+				intf_modpriv->netcp_priv = netcp_priv;
+				intf_modpriv->netcp_module = module;
+				list_add_tail(&intf_modpriv->intf_list, &netcp_priv->module_head);
+
+				dev_dbg(&pdev->dev, "Attaching module \"%s\"\n", module->name);
+				ret = module->attach(inst_modpriv->module_priv, netcp_priv->ndev, &intf_modpriv->module_priv);
+				if (ret) {
+					dev_info(&pdev->dev, "Attach of module %s declined with %d\n",
+							module->name, ret);
+					list_del(&intf_modpriv->intf_list);
+					kfree(intf_modpriv);
+					continue;
+				}
+
+			}
+		}
+	}
+
+	ret = 0;
+
+done:
+	mutex_unlock(&netcp_modules_lock);
+	return ret;
+}
+EXPORT_SYMBOL(netcp_register_module);
+
+static struct netcp_module *__netcp_find_module(const char *name)
+{
+	struct netcp_module *tmp;
+
+	for_each_netcp_module(tmp) {
+		if (!strcasecmp(tmp->name, name))
+			return tmp;
+	}
+	return NULL;
+}
+
+static struct netcp_module *netcp_find_module(const char *name)
+{
+	struct netcp_module *module;
+
+	mutex_lock(&netcp_modules_lock);
+	module = __netcp_find_module(name);
+	mutex_unlock(&netcp_modules_lock);
+	return module;
+}
+
+static void *__netcp_device_find_module(struct netcp_device *netcp_device,
+					 const char *name)
+{
+	struct netcp_inst_modpriv *tmp;
+
+	for_each_netcp_device_module(netcp_device, tmp) {
+		if (!strcasecmp(tmp->netcp_module->name, name))
+			return tmp->module_priv;
+	}
+	return NULL;
+}
+
+void *netcp_device_find_module(struct netcp_device *netcp_device,
+		const char *name)
+{
+	void *module;
+
+	mutex_lock(&netcp_modules_lock);
+	module = __netcp_device_find_module(netcp_device, name);
+	mutex_unlock(&netcp_modules_lock);
+	return module;
+}
+EXPORT_SYMBOL(netcp_device_find_module);
+
+void netcp_unregister_module(struct netcp_module *module)
+{
+	struct netcp_device *netcp_device;
+	struct netcp_module *module_tmp;
+
+	mutex_lock(&netcp_modules_lock);
+
+	list_for_each_entry(netcp_device, &netcp_devices, device_list) {
+		struct netcp_priv *netcp_priv, *netcp_tmp;
+		struct netcp_inst_modpriv *inst_modpriv, *inst_tmp;
+
+		/* Release the module from each interface */
+		list_for_each_entry_safe(netcp_priv, netcp_tmp, 
+				&netcp_device->interface_head, interface_list) {
+			struct netcp_intf_modpriv *intf_modpriv, *intf_tmp;
+
+			list_for_each_entry_safe(intf_modpriv, intf_tmp,
+					&netcp_priv->module_head, intf_list) {
+				if (intf_modpriv->netcp_module == module) {
+					module->release(intf_modpriv->module_priv);
+					list_del(&intf_modpriv->intf_list);
+					kfree(intf_modpriv);
+					break;
+				}
+			}
+		}
+
+		/* Remove the module from each instance */
+		list_for_each_entry_safe(inst_modpriv, inst_tmp,
+				&netcp_device->modpriv_head, inst_list) {
+			if (inst_modpriv->netcp_module == module) {
+				module->remove(netcp_device, inst_modpriv->module_priv);
+				list_del(&inst_modpriv->inst_list);
+				kfree(inst_modpriv);
+				break;
+			}
+		}
+	}
+
+	/* Remove the module from the module list */
+	for_each_netcp_module(module_tmp) {
+		if (module == module_tmp) {
+			list_del(&module->module_list);
+			break;
+		}
+	}
+
+	mutex_unlock(&netcp_modules_lock);
+}
+EXPORT_SYMBOL(netcp_unregister_module);
+
+
+/*
+ *  Module TX and RX Hook management
+ */
+struct netcp_hook_list {
+	struct list_head	 list;
+	netcp_hook_rtn		*hook_rtn;
+	void			*hook_data;
+	int			 order;
+};
+
+
+int netcp_register_txhook(struct netcp_priv *netcp_priv, int order,
+		netcp_hook_rtn *hook_rtn, void *hook_data)
+{
+	struct netcp_hook_list	*entry;
+	struct netcp_hook_list	*next;
+
+	entry = kzalloc(sizeof(*entry), GFP_KERNEL);
+	if (!entry)
+		return -ENOMEM;
+
+	entry->hook_rtn  = hook_rtn;
+	entry->hook_data = hook_data;
+	entry->order     = order;
+
+	list_for_each_entry(next, &netcp_priv->txhook_list_head, list) {
+		if (next->order > order)
+			break;
+	}
+	__list_add(&entry->list, next->list.prev, &next->list);
+
+	return 0;
+}
+EXPORT_SYMBOL(netcp_register_txhook);
+
+int netcp_unregister_txhook(struct netcp_priv *netcp_priv, int order,
+		netcp_hook_rtn *hook_rtn, void *hook_data)
+{
+	struct netcp_hook_list	*next;
+
+	list_for_each_entry(next, &netcp_priv->txhook_list_head, list) {
+		if ((next->order     == order) &&
+		    (next->hook_rtn  == hook_rtn) &&
+		    (next->hook_data == hook_data)) {
+			list_del(&next->list);
+			kfree(next);
+			return 0;
+		}
+	}
+
+	return -ENOENT;
+}
+EXPORT_SYMBOL(netcp_unregister_txhook);
+
+int netcp_register_rxhook(struct netcp_priv *netcp_priv, int order,
+		netcp_hook_rtn *hook_rtn, void *hook_data)
+{
+	struct netcp_hook_list	*entry;
+	struct netcp_hook_list	*next;
+
+	entry = kzalloc(sizeof(*entry), GFP_KERNEL);
+	if (!entry)
+		return -ENOMEM;
+
+	entry->hook_rtn  = hook_rtn;
+	entry->hook_data = hook_data;
+	entry->order     = order;
+
+	list_for_each_entry(next, &netcp_priv->rxhook_list_head, list) {
+		if (next->order > order)
+			break;
+	}
+	__list_add(&entry->list, next->list.prev, &next->list);
+
+	return 0;
+}
+EXPORT_SYMBOL(netcp_register_rxhook);
+
+int netcp_unregister_rxhook(struct netcp_priv *netcp_priv, int order,
+		netcp_hook_rtn *hook_rtn, void *hook_data)
+{
+	struct netcp_hook_list	*next;
+
+	list_for_each_entry(next, &netcp_priv->rxhook_list_head, list) {
+		if ((next->order     == order) &&
+		    (next->hook_rtn  == hook_rtn) &&
+		    (next->hook_data == hook_data)) {
+			list_del(&next->list);
+			kfree(next);
+			return 0;
+		}
+	}
+
+	return -ENOENT;
+}
+EXPORT_SYMBOL(netcp_unregister_rxhook);
+
+u32 *netcp_push_psdata(struct netcp_packet *p_info, unsigned bytes)
+{
+	u32		*buf;
+	unsigned	 words;
+
+	if ((bytes & 0x03) != 0)
+		return NULL;
+	words = bytes >> 2;
+
+	if ((p_info->psdata_len + words) > NETCP_PSDATA_LEN)
+		return NULL;
+
+	p_info->psdata_len += words;
+	buf = &p_info->psdata[NETCP_PSDATA_LEN - p_info->psdata_len];
+
+	memset(buf, 0, bytes);
+
+	return buf;
+}
+EXPORT_SYMBOL(netcp_push_psdata);
+
+int netcp_align_psdata(struct netcp_packet *p_info, unsigned byte_align)
+{
+	int	padding;
+
+	switch (byte_align) {
+	case 0:
+		padding = -EINVAL;
+		break;
+	case 1:
+		padding = 0;
+		break;
+	default:
+		padding = (p_info->psdata_len << 2) % byte_align;
+		break;
+	}
+
+	return padding;
+}
+EXPORT_SYMBOL(netcp_align_psdata);
+
+#define NETCP_DEBUG (NETIF_MSG_HW	| NETIF_MSG_WOL		|	\
+		    NETIF_MSG_DRV	| NETIF_MSG_LINK	|	\
+		    NETIF_MSG_IFUP	| NETIF_MSG_INTR	|	\
+		    NETIF_MSG_PROBE	| NETIF_MSG_TIMER	|	\
+		    NETIF_MSG_IFDOWN	| NETIF_MSG_RX_ERR	|	\
+		    NETIF_MSG_TX_ERR	| NETIF_MSG_TX_DONE	|	\
+		    NETIF_MSG_PKTDATA	| NETIF_MSG_TX_QUEUED	|	\
+		    NETIF_MSG_RX_STATUS)
+
+#define NETCP_NAPI_WEIGHT	128
+#define NETCP_TX_TIMEOUT	40
+#define NETCP_MIN_PACKET_SIZE	64
+#define NETCP_MAX_PACKET_SIZE	(VLAN_ETH_FRAME_LEN + ETH_FCS_LEN)
+
+static int netcp_rx_packet_max = NETCP_MAX_PACKET_SIZE;
+static int netcp_debug_level;
+
+#define for_each_module(netcp, intf_modpriv)			\
+	list_for_each_entry(intf_modpriv, &netcp->module_head, intf_list)
+
+static const char *netcp_rx_state_str(struct netcp_priv *netcp)
+{
+	static const char * const state_str[] = {
+		[RX_STATE_POLL]		= "poll",
+		[RX_STATE_SCHEDULED]	= "scheduled",
+		[RX_STATE_TEARDOWN]	= "teardown",
+		[RX_STATE_INTERRUPT]	= "interrupt",
+		[RX_STATE_INVALID]	= "invalid",
+	};
+
+	if (netcp->rx_state < 0 || netcp->rx_state >= ARRAY_SIZE(state_str))
+		return state_str[RX_STATE_INVALID];
+	else
+		return state_str[netcp->rx_state];
+}
+
+static inline void netcp_set_rx_state(struct netcp_priv *netcp,
+				     enum netcp_rx_state state)
+{
+	netcp->rx_state = state;
+	cpu_relax();
+}
+
+static inline bool netcp_is_alive(struct netcp_priv *netcp)
+{
+	return (netcp->rx_state == RX_STATE_POLL ||
+		netcp->rx_state == RX_STATE_INTERRUPT);
+}
+
+static void netcp_dump_packet(struct netcp_packet *p_info, const char *cause)
+{
+	struct netcp_priv *netcp = p_info->netcp;
+	struct sk_buff *skb = p_info->skb;
+	unsigned char *head, *tail;
+
+	head = skb->data;
+	tail = skb_tail_pointer(skb) - 16;
+
+	dev_dbg(netcp->dev, "packet %p %s, size %d (%d): "
+		"%02x%02x%02x%02x%02x%02x%02x%02x"
+		"%02x%02x%02x%02x%02x%02x%02x%02x"
+		"%02x%02x%02x%02x%02x%02x%02x%02x"
+		"%02x%02x%02x%02x%02x%02x%02x%02x\n",
+		p_info, cause, skb->len, p_info->sg[2].length,
+		head[0x00], head[0x01], head[0x02], head[0x03],
+		head[0x04], head[0x05], head[0x06], head[0x07],
+		head[0x08], head[0x09], head[0x0a], head[0x0b],
+		head[0x0c], head[0x0d], head[0x0e], head[0x0f],
+		tail[0x00], tail[0x01], tail[0x02], tail[0x03],
+		tail[0x04], tail[0x05], tail[0x06], tail[0x07],
+		tail[0x08], tail[0x09], tail[0x0a], tail[0x0b],
+		tail[0x0c], tail[0x0d], tail[0x0e], tail[0x0f]);
+}
+
+static void netcp_rx_complete(void *data)
+{
+	struct netcp_packet *p_info = data;
+	struct netcp_priv *netcp = p_info->netcp;
+	struct sk_buff *skb = p_info->skb;
+	struct scatterlist *sg;
+	enum dma_status status;
+	unsigned int frags;
+	struct netcp_hook_list *rx_hook;
+
+	status = dma_async_is_tx_complete(netcp->rx_channel,
+					  p_info->cookie, NULL, NULL);
+	WARN_ON(status != DMA_SUCCESS && status != DMA_ERROR);
+	WARN_ON(netcp->rx_state != RX_STATE_INTERRUPT	&&
+		netcp->rx_state != RX_STATE_POLL	&&
+		netcp->rx_state != RX_STATE_TEARDOWN);
+
+	/* sg[2] describes the buffer already attached to the sk_buff. */
+	skb_put(skb, sg_dma_len(&p_info->sg[2]));
+
+	/* Fill in the page fragment list from sg[3] and later */
+	for (frags = 0, sg = sg_next(&p_info->sg[2]);
+			frags < NETCP_SGLIST_SIZE-3 && sg;
+			++frags, sg = sg_next(sg)) {
+		skb_add_rx_frag(skb, frags, sg_page(sg), sg->offset,
+				sg_dma_len(sg), sg_dma_len(sg));
+	}
+
+	dma_unmap_sg(netcp->dev, &p_info->sg[2], frags+1, DMA_FROM_DEVICE);
+
+	if (unlikely(netcp->rx_state == RX_STATE_TEARDOWN)) {
+		dev_dbg(netcp->dev,
+			"receive: reclaimed packet %p, status %d, state %s\n",
+			p_info, status, netcp_rx_state_str(netcp));
+		dev_kfree_skb_any(skb);
+		kmem_cache_free(netcp_pinfo_cache, p_info);
+		netcp->ndev->stats.rx_dropped++;
+		return;
+	}
+
+	if (unlikely(status != DMA_SUCCESS)) {
+		dev_warn(netcp->dev,
+			 "receive: reclaimed packet %p, status %d, state %s\n",
+			 p_info, status, netcp_rx_state_str(netcp));
+		dev_kfree_skb_any(skb);
+		kmem_cache_free(netcp_pinfo_cache, p_info);
+		netcp->ndev->stats.rx_errors++;
+		return;
+	}
+
+	if (unlikely(!skb->len)) {
+		dev_warn(netcp->dev, "receive: zero length packet\n");
+		dev_kfree_skb_any(skb);
+		kmem_cache_free(netcp_pinfo_cache, p_info);
+		netcp->ndev->stats.rx_errors++;
+		return;
+	}
+
+	BUG_ON(netcp->rx_state != RX_STATE_POLL);
+
+
+	netcp->ndev->last_rx = jiffies;
+
+#ifdef DEBUG
+	netcp_dump_packet(p_info, "rx");
+#endif
+
+	/* Call each of the RX hooks */
+	list_for_each_entry(rx_hook, &netcp->rxhook_list_head, list) {
+		int ret;
+		ret = rx_hook->hook_rtn(rx_hook->order, rx_hook->hook_data, p_info);
+		if (ret) {
+			dev_err(netcp->dev, "RX hook %d failed: %d\n", rx_hook->order, ret);
+			dev_kfree_skb_any(skb);
+			kmem_cache_free(netcp_pinfo_cache, p_info);
+			return;
+		}
+	}
+
+	netcp->ndev->stats.rx_packets++;
+	netcp->ndev->stats.rx_bytes += skb->len;
+
+	p_info->skb = NULL;
+	kmem_cache_free(netcp_pinfo_cache, p_info);
+
+	/* push skb up the stack */
+	skb->protocol = eth_type_trans(skb, netcp->ndev);
+	netif_receive_skb(skb);
+}
+
+/* Release a free receive buffer */
+static void netcp_rxpool_free(void *arg, unsigned q_num, unsigned bufsize,
+		struct dma_async_tx_descriptor *desc)
+{
+	struct netcp_priv *netcp = arg;
+
+	if (q_num == 0) {
+		struct netcp_packet *p_info = desc->callback_param;
+		struct sk_buff *skb = p_info->skb;
+
+		dma_unmap_sg(netcp->dev, &p_info->sg[2], 1, DMA_FROM_DEVICE);
+		dev_kfree_skb_any(skb);
+		kmem_cache_free(netcp_pinfo_cache, p_info);
+	} else {
+		void *bufptr = desc->callback_param;
+		struct scatterlist sg[1];
+
+		sg_init_table(sg, 1);
+		sg_set_buf(&sg[0], bufptr, PAGE_SIZE);
+		sg_dma_address(&sg[0]) = virt_to_dma(netcp->dev, bufptr);
+		dma_unmap_sg(netcp->dev, sg, 1, DMA_FROM_DEVICE);
+		free_page((unsigned long)bufptr);
+	}
+}
+
+static void netcp_rx_complete2nd(void *data)
+{
+	WARN(1, "Attempt to complete secondary receive buffer!\n");
+}
+
+/* Allocate a free receive buffer */
+static struct dma_async_tx_descriptor *netcp_rxpool_alloc(void *arg,
+		unsigned q_num, unsigned bufsize)
+{
+	struct netcp_priv *netcp = arg;
+	struct dma_async_tx_descriptor *desc;
+	struct dma_device *device;
+	u32 err = 0;
+
+	device = netcp->rx_channel->device;
+
+	if (q_num == 0) {
+		struct netcp_packet *p_info;
+		struct sk_buff *skb;
+
+		/* Allocate a primary receive queue entry */
+		p_info = kmem_cache_alloc(netcp_pinfo_cache, GFP_ATOMIC);
+		if (!p_info) {
+			dev_err(netcp->dev, "packet alloc failed\n");
+			return NULL;
+		}
+		p_info->netcp = netcp;
+
+		skb = netdev_alloc_skb(netcp->ndev, bufsize);
+		if (!skb) {
+			dev_err(netcp->dev, "skb alloc failed\n");
+			kmem_cache_free(netcp_pinfo_cache, p_info);
+			return NULL;
+		}
+		skb->dev = netcp->ndev;
+		p_info->skb = skb;
+
+		sg_init_table(p_info->sg, NETCP_SGLIST_SIZE);
+		sg_set_buf(&p_info->sg[0], p_info->epib, sizeof(p_info->epib));
+		sg_set_buf(&p_info->sg[1], p_info->psdata, sizeof(p_info->psdata));
+		sg_set_buf(&p_info->sg[2], skb_tail_pointer(skb), skb_tailroom(skb));
+
+		p_info->sg_ents = 2 + dma_map_sg(netcp->dev, &p_info->sg[2],
+						 1, DMA_FROM_DEVICE);
+		if (p_info->sg_ents != 3) {
+			dev_err(netcp->dev, "dma map failed\n");
+			dev_kfree_skb_any(skb);
+			kmem_cache_free(netcp_pinfo_cache, p_info);
+			return NULL;
+		}
+
+		desc = dmaengine_prep_slave_sg(netcp->rx_channel, p_info->sg,
+					       3, DMA_DEV_TO_MEM,
+					       DMA_HAS_EPIB | DMA_HAS_PSINFO);
+		if (IS_ERR_OR_NULL(desc)) {
+			dma_unmap_sg(netcp->dev, &p_info->sg[2], 1, DMA_FROM_DEVICE);
+			dev_kfree_skb_any(skb);
+			kmem_cache_free(netcp_pinfo_cache, p_info);
+			err = PTR_ERR(desc);
+			if (err != -ENOMEM) {
+				dev_err(netcp->dev,
+					"dma prep failed, error %d\n", err);
+			}
+			return NULL;
+		}
+
+		desc->callback_param = p_info;
+		desc->callback = netcp_rx_complete;
+		p_info->cookie = desc->cookie;
+
+	} else {
+
+		/* Allocate a secondary receive queue entry */
+		struct scatterlist sg[1];
+		void *bufptr;
+
+		bufptr = (void *)__get_free_page(GFP_ATOMIC);
+		if (!bufptr) {
+			dev_warn(netcp->dev, "page alloc failed for pool %d\n", q_num);
+			return NULL;
+		}
+
+		sg_init_table(sg, 1);
+		sg_set_buf(&sg[0], bufptr, PAGE_SIZE);
+
+		err = dma_map_sg(netcp->dev, sg, 1, DMA_FROM_DEVICE);
+		if (err != 1) {
+			dev_warn(netcp->dev, "map error for pool %d\n", q_num);
+			free_page((unsigned long)bufptr);
+			return NULL;
+		}
+
+		desc = dmaengine_prep_slave_sg(netcp->rx_channel, sg, 1,
+					       DMA_DEV_TO_MEM,
+					       q_num << DMA_QNUM_SHIFT);
+		if (IS_ERR_OR_NULL(desc)) {
+			dma_unmap_sg(netcp->dev, sg, 1, DMA_FROM_DEVICE);
+			free_page((unsigned long)bufptr);
+
+			err = PTR_ERR(desc);
+			if (err != -ENOMEM) {
+				dev_err(netcp->dev,
+					"dma prep failed, error %d\n", err);
+			}
+			return NULL;
+		}
+
+		desc->callback_param = bufptr;
+		desc->callback = netcp_rx_complete2nd;
+	}
+
+	return desc;
+}
+
+/* NAPI poll */
+static int netcp_poll(struct napi_struct *napi, int budget)
+{
+	struct netcp_priv *netcp = container_of(napi, struct netcp_priv, napi);
+	unsigned long flags;
+	unsigned packets;
+
+	spin_lock_irqsave(&netcp->lock, flags);
+
+	BUG_ON(netcp->rx_state != RX_STATE_SCHEDULED);
+	netcp_set_rx_state(netcp, RX_STATE_POLL);
+
+	spin_unlock_irqrestore(&netcp->lock, flags);
+
+	packets = dma_poll(netcp->rx_channel, budget);
+
+	if (packets < budget) {
+		netcp_set_rx_state(netcp, RX_STATE_INTERRUPT);
+		napi_complete(&netcp->napi);
+		dmaengine_resume(netcp->rx_channel);
+	} else {
+		netcp_set_rx_state(netcp, RX_STATE_SCHEDULED);
+	}
+
+	dma_rxfree_refill(netcp->rx_channel);
+
+	return packets;
+}
+
+static const char *netcp_tx_state_str(enum netcp_tx_state tx_state)
+{
+	static const char * const state_str[] = {
+		[TX_STATE_INTERRUPT]	= "interrupt",
+		[TX_STATE_POLL]		= "poll",
+		[TX_STATE_SCHEDULED]	= "scheduled",
+		[TX_STATE_INVALID]	= "invalid",
+	};
+
+	if (tx_state < 0 || tx_state >= ARRAY_SIZE(state_str))
+		return state_str[TX_STATE_INVALID];
+	else
+		return state_str[tx_state];
+}
+
+static inline void netcp_set_txpipe_state(struct netcp_tx_pipe *tx_pipe,
+					  enum netcp_tx_state new_state)
+{
+	dev_dbg(tx_pipe->netcp_priv->dev, "txpipe %s: %s -> %s\n",
+		tx_pipe->dma_chan_name,
+		netcp_tx_state_str(tx_pipe->dma_poll_state),
+		netcp_tx_state_str(new_state));
+
+	tx_pipe->dma_poll_state = new_state;
+	cpu_relax();
+}
+
+static void netcp_tx_complete(void *data)
+{
+	struct netcp_packet *p_info = data;
+	struct netcp_priv *netcp = p_info->netcp;
+	struct netcp_tx_pipe *tx_pipe = p_info->tx_pipe;
+	struct sk_buff *skb = p_info->skb;
+	enum dma_status status;
+	unsigned int sg_ents;
+	int poll_count;
+
+	if (unlikely(p_info->cookie <= 0))
+		WARN(1, "invalid dma cookie == %d", p_info->cookie);
+	else {
+		status = dma_async_is_tx_complete(p_info->tx_pipe->dma_channel,
+						  p_info->cookie, NULL, NULL);
+		WARN((status != DMA_SUCCESS && status != DMA_ERROR),
+				"invalid dma status %d", status);
+		if (status != DMA_SUCCESS)
+			netcp->ndev->stats.tx_errors++;
+	}
+
+	sg_ents = sg_count(&p_info->sg[2], p_info->sg_ents);
+	dma_unmap_sg(netcp->dev, &p_info->sg[2], sg_ents, DMA_TO_DEVICE);
+
+	netcp_dump_packet(p_info, "txc");
+
+	poll_count = atomic_add_return(sg_ents, &tx_pipe->dma_poll_count);
+	if ((poll_count >= tx_pipe->dma_resume_threshold) &&
+	    netif_subqueue_stopped(netcp->ndev, skb)) {
+		u16 subqueue = skb_get_queue_mapping(skb);
+		dev_dbg(netcp->dev, "waking subqueue %hu\n", subqueue);
+		netif_wake_subqueue(netcp->ndev, subqueue);
+	}
+
+	dev_kfree_skb_any(skb);
+	kfree(p_info);
+}
+
+static void netcp_tx_tasklet(unsigned long data)
+{
+	struct netcp_tx_pipe *tx_pipe = (void *)data;
+	int poll_count, packets;
+
+	if (unlikely(tx_pipe->dma_poll_state != TX_STATE_SCHEDULED)) {
+		WARN(1, "spurious tasklet activation, txpipe %s state %d",
+			tx_pipe->dma_chan_name, tx_pipe->dma_poll_state);
+		return;
+	}
+
+	packets = dma_poll(tx_pipe->dma_channel, -1);
+
+	poll_count = atomic_read(&tx_pipe->dma_poll_count);
+	if (poll_count >= tx_pipe->dma_resume_threshold)
+		netcp_set_txpipe_state(tx_pipe, TX_STATE_POLL);
+	else {
+		netcp_set_txpipe_state(tx_pipe, TX_STATE_INTERRUPT);
+		dmaengine_resume(tx_pipe->dma_channel);
+	}
+
+	dev_dbg(tx_pipe->netcp_priv->dev,
+		"txpipe %s poll count %d, packets %d\n",
+		tx_pipe->dma_chan_name, poll_count, packets);
+}
+
+static void netcp_tx_notify(struct dma_chan *chan, void *arg)
+{
+	struct netcp_tx_pipe *tx_pipe = arg;
+
+	BUG_ON(tx_pipe->dma_poll_state != TX_STATE_INTERRUPT);
+	dmaengine_pause(tx_pipe->dma_channel);
+	netcp_set_txpipe_state(tx_pipe, TX_STATE_SCHEDULED);
+	tasklet_schedule(&tx_pipe->dma_poll_tasklet);
+}
+
+/* Push an outcoming packet */
+static int netcp_ndo_start_xmit(struct sk_buff *skb, struct net_device *ndev)
+{
+	struct netcp_priv *netcp = netdev_priv(ndev);
+	struct dma_async_tx_descriptor *desc;
+	struct netcp_tx_pipe *tx_pipe = NULL;
+	struct netcp_hook_list *tx_hook;
+	struct netcp_packet *p_info;
+	int subqueue = skb_get_queue_mapping(skb);
+	int real_sg_ents = 0;
+	int poll_count;
+	int ret = 0;
+
+	ndev->stats.tx_packets++;
+	ndev->stats.tx_bytes += skb->len;
+
+	p_info = kmalloc(sizeof(*p_info), GFP_ATOMIC);
+	if (!p_info) {
+		ndev->stats.tx_dropped++;
+		dev_kfree_skb_any(skb);
+		dev_warn(netcp->dev, "failed to alloc packet info\n");
+		return -ENOMEM;
+	}
+
+	p_info->netcp = netcp;
+	p_info->skb = skb;
+	p_info->tx_pipe = NULL;
+	p_info->psdata_len = 0;
+	memset(p_info->epib, 0, sizeof(p_info->epib));
+
+	/* Find out where to inject the packet for transmission */
+	list_for_each_entry(tx_hook, &netcp->txhook_list_head, list) {
+		ret = tx_hook->hook_rtn(tx_hook->order, tx_hook->hook_data,
+					p_info);
+		if (ret) {
+			dev_err(netcp->dev, "TX hook %d "
+				"rejected the packet: %d\n",
+				tx_hook->order, ret);
+			dev_kfree_skb_any(skb);
+			kfree(p_info);
+			return ret;
+		}
+	}
+
+	/* Make sure some TX hook claimed the packet */
+	tx_pipe = p_info->tx_pipe;
+	if (tx_pipe == NULL) {
+		dev_err(netcp->dev, "No TX hook claimed the packet!\n");
+		dev_kfree_skb_any(skb);
+		kfree(p_info);
+		return -ENXIO;
+	}
+
+	if (unlikely(skb->len < NETCP_MIN_PACKET_SIZE)) {
+		ret = skb_padto(skb, NETCP_MIN_PACKET_SIZE);
+		if (ret < 0) {
+			dev_warn(netcp->dev, "padding failed (%d), "
+				 "packet dropped\n", ret);
+			kfree(p_info);
+			return ret;
+		}
+		skb->len = NETCP_MIN_PACKET_SIZE;
+	}
+
+	netcp_dump_packet(p_info, "txs");
+
+	skb_tx_timestamp(skb);
+
+	sg_init_table(p_info->sg, NETCP_SGLIST_SIZE);
+	sg_set_buf(&p_info->sg[0], p_info->epib, sizeof(p_info->epib));
+	sg_set_buf(&p_info->sg[1], &p_info->psdata[NETCP_PSDATA_LEN - p_info->psdata_len],
+			p_info->psdata_len * sizeof(u32));
+
+	/* Map all the packet fragments	into the scatterlist */
+	real_sg_ents = skb_to_sgvec(skb, &p_info->sg[2], 0, skb->len);
+	p_info->sg_ents = 2 + dma_map_sg(netcp->dev, &p_info->sg[2],
+					 real_sg_ents, DMA_TO_DEVICE);
+	if (p_info->sg_ents != (real_sg_ents + 2)) {
+		ndev->stats.tx_dropped++;
+		dev_kfree_skb_any(skb);
+		kfree(p_info);
+		real_sg_ents = 0;
+		dev_warn(netcp->dev, "failed to map transmit packet\n");
+		ret = -ENXIO;
+		goto out;
+	}
+
+	desc = dmaengine_prep_slave_sg(tx_pipe->dma_channel, p_info->sg,
+				       p_info->sg_ents, DMA_MEM_TO_DEV,
+				       (DMA_HAS_EPIB | DMA_HAS_PSINFO |
+					tx_pipe->dma_psflags));
+
+	if (IS_ERR_OR_NULL(desc)) {
+		ndev->stats.tx_dropped++;
+		dma_unmap_sg(netcp->dev, &p_info->sg[2], real_sg_ents,
+			     DMA_TO_DEVICE);
+		dev_kfree_skb_any(skb);
+		kfree(p_info);
+		real_sg_ents = 0;
+		dev_dbg(netcp->dev, "failed to prep slave dma\n");
+		ret = -ENOBUFS;
+		goto out;
+	}
+
+	desc->callback_param = p_info;
+	desc->callback = netcp_tx_complete;
+	p_info->cookie = dmaengine_submit(desc);
+
+	ndev->trans_start = jiffies;
+
+	ret = NETDEV_TX_OK;
+
+out:
+	poll_count = atomic_sub_return(real_sg_ents, &tx_pipe->dma_poll_count);
+	if ((poll_count < tx_pipe->dma_poll_threshold) || (ret < 0)) {
+		dev_dbg(netcp->dev, "polling %s, poll count %d\n",
+			tx_pipe->dma_chan_name, poll_count);
+
+		tasklet_disable(&tx_pipe->dma_poll_tasklet);
+		if (unlikely(tx_pipe->dma_poll_state == TX_STATE_INTERRUPT)) {
+			dmaengine_pause(tx_pipe->dma_channel);
+			netcp_set_txpipe_state(tx_pipe, TX_STATE_POLL);
+		}
+		dma_poll(tx_pipe->dma_channel, -1);
+		tasklet_enable(&tx_pipe->dma_poll_tasklet);
+	}
+
+	if (atomic_read(&tx_pipe->dma_poll_count) < tx_pipe->dma_pause_threshold) {
+		dev_dbg(netcp->dev, "pausing subqueue %d, %s poll count %d\n",
+			subqueue, tx_pipe->dma_chan_name, poll_count);
+		netif_stop_subqueue(ndev, subqueue);
+		if (likely(tx_pipe->dma_poll_state == TX_STATE_POLL)) {
+			netcp_set_txpipe_state(tx_pipe, TX_STATE_INTERRUPT);
+			dmaengine_resume(tx_pipe->dma_channel);
+		}
+	}
+
+	return ret;
+}
+
+
+int netcp_txpipe_close(struct netcp_tx_pipe *tx_pipe)
+{
+	if (tx_pipe->dma_channel) {
+		tasklet_disable(&tx_pipe->dma_poll_tasklet);
+		dmaengine_pause(tx_pipe->dma_channel);
+		dma_poll(tx_pipe->dma_channel, -1);
+		dma_release_channel(tx_pipe->dma_channel);
+		tx_pipe->dma_channel = NULL;
+		netcp_set_txpipe_state(tx_pipe, TX_STATE_INVALID);
+
+		dev_dbg(tx_pipe->netcp_priv->dev, "closed tx pipe %s\n",
+			tx_pipe->dma_chan_name);
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL(netcp_txpipe_close);
+
+int netcp_txpipe_open(struct netcp_tx_pipe *tx_pipe)
+{
+	struct dma_keystone_info config;
+	dma_cap_mask_t mask;
+	int ret;
+
+	dma_cap_zero(mask);
+	dma_cap_set(DMA_SLAVE, mask);
+
+	tx_pipe->dma_channel = dma_request_channel_by_name(mask, tx_pipe->dma_chan_name);
+	if (IS_ERR_OR_NULL(tx_pipe->dma_channel)) {
+		dev_err(tx_pipe->netcp_priv->dev,
+			"Could not get DMA channel \"%s\"\n",
+			tx_pipe->dma_chan_name);
+		tx_pipe->dma_channel = NULL;
+		return -ENODEV;
+	}
+
+	memset(&config, 0, sizeof(config));
+	config.direction = DMA_MEM_TO_DEV;
+	config.tx_queue_depth = tx_pipe->dma_queue_depth;
+	ret = dma_keystone_config(tx_pipe->dma_channel, &config);
+	if (ret) {
+		dev_err(tx_pipe->netcp_priv->dev,
+			"Could not configure DMA channel \"%s\": %d\n",
+			tx_pipe->dma_chan_name, ret);
+		dma_release_channel(tx_pipe->dma_channel);
+		tx_pipe->dma_channel = NULL;
+		return ret;
+	}
+
+	dma_set_notify(tx_pipe->dma_channel, netcp_tx_notify, tx_pipe);
+	dmaengine_pause(tx_pipe->dma_channel);
+	netcp_set_txpipe_state(tx_pipe, TX_STATE_POLL);
+
+	tx_pipe->dma_queue = dma_get_tx_queue(tx_pipe->dma_channel);
+	atomic_set(&tx_pipe->dma_poll_count, tx_pipe->dma_queue_depth);
+	tasklet_enable(&tx_pipe->dma_poll_tasklet);
+
+
+	dev_dbg(tx_pipe->netcp_priv->dev, "opened tx pipe %s\n",
+		tx_pipe->dma_chan_name);
+	return 0;
+}
+EXPORT_SYMBOL(netcp_txpipe_open);
+
+int netcp_txpipe_init(struct netcp_tx_pipe *tx_pipe,
+		struct netcp_priv *netcp_priv,
+		const char *chan_name,
+		int queue_depth)
+{
+	memset(tx_pipe, 0, sizeof(*tx_pipe));
+
+	tx_pipe->netcp_priv = netcp_priv;
+	tx_pipe->dma_chan_name = chan_name;
+	tx_pipe->dma_queue_depth = queue_depth;
+
+	tx_pipe->dma_poll_threshold = queue_depth / 2;
+	tx_pipe->dma_pause_threshold = (MAX_SKB_FRAGS < (queue_depth / 4)) ?
+					MAX_SKB_FRAGS : (queue_depth / 4);
+	tx_pipe->dma_resume_threshold = tx_pipe->dma_pause_threshold;
+
+	tasklet_init(&tx_pipe->dma_poll_tasklet, netcp_tx_tasklet, (unsigned long)tx_pipe);
+	tasklet_disable_nosync(&tx_pipe->dma_poll_tasklet);
+	netcp_set_txpipe_state(tx_pipe, TX_STATE_INVALID);
+
+	dev_dbg(tx_pipe->netcp_priv->dev, "initialized tx pipe %s, %d/%d/%d\n",
+		tx_pipe->dma_chan_name, tx_pipe->dma_poll_threshold,
+		tx_pipe->dma_pause_threshold, tx_pipe->dma_resume_threshold);
+	return 0;
+}
+EXPORT_SYMBOL(netcp_txpipe_init);
+
+static struct netcp_addr *
+netcp_addr_find(struct netcp_priv *netcp, const u8 *addr,
+	       enum netcp_addr_type type)
+{
+	struct netcp_addr *naddr;
+
+	list_for_each_entry(naddr, &netcp->addr_list, node) {
+		if (naddr->type != type)
+			continue;
+		if (addr && memcmp(addr, naddr->addr, ETH_ALEN))
+			continue;
+		return naddr;
+	}
+
+	return NULL;
+}
+
+static struct netcp_addr *
+netcp_addr_add(struct netcp_priv *netcp, const u8 *addr,
+	       enum netcp_addr_type type)
+{
+	struct netcp_addr *naddr;
+
+	naddr = kmalloc(sizeof(struct netcp_addr), GFP_KERNEL);
+	if (!naddr)
+		return NULL;
+
+	naddr->type = type;
+	naddr->flags = 0;
+	naddr->netcp = netcp;
+	if (addr)
+		memcpy(naddr->addr, addr, ETH_ALEN);
+	else
+		memset(naddr->addr, 0, ETH_ALEN);
+	list_add_tail(&naddr->node, &netcp->addr_list);
+
+	return naddr;
+}
+
+static void netcp_addr_del(struct netcp_addr *naddr)
+{
+	list_del(&naddr->node);
+	kfree(naddr);
+}
+
+static void netcp_addr_clear_mark(struct netcp_priv *netcp)
+{
+	struct netcp_addr *naddr;
+
+	list_for_each_entry(naddr, &netcp->addr_list, node)
+		naddr->flags = 0;
+}
+
+static void netcp_addr_add_mark(struct netcp_priv *netcp, const u8 *addr,
+				enum netcp_addr_type type)
+{
+	struct netcp_addr *naddr;
+
+	naddr = netcp_addr_find(netcp, addr, type);
+	if (naddr) {
+		naddr->flags |= ADDR_VALID;
+		return;
+	}
+
+	naddr = netcp_addr_add(netcp, addr, type);
+	if (!WARN_ON(!naddr))
+		naddr->flags |= ADDR_NEW;
+}
+
+static void netcp_addr_sweep_del(struct netcp_priv *netcp)
+{
+	struct netcp_addr *naddr, *tmp;
+	struct netcp_intf_modpriv *priv;
+	struct netcp_module *module;
+	int error;
+
+	list_for_each_entry_safe(naddr, tmp, &netcp->addr_list, node) {
+		if (naddr->flags & (ADDR_VALID | ADDR_NEW))
+			continue;
+		dev_dbg(netcp->dev, "deleting address %pM, type %x\n",
+			naddr->addr, naddr->type);
+		for_each_module(netcp, priv) {
+			module = priv->netcp_module;
+			if (!module->del_addr)
+				continue;
+			error = module->del_addr(priv->module_priv,
+						 naddr);
+			WARN_ON(error);
+		}
+		netcp_addr_del(naddr);
+	}
+}
+
+static void netcp_addr_sweep_add(struct netcp_priv *netcp)
+{
+	struct netcp_addr *naddr, *tmp;
+	struct netcp_intf_modpriv *priv;
+	struct netcp_module *module;
+	int error;
+
+	list_for_each_entry_safe(naddr, tmp, &netcp->addr_list, node) {
+		if (!(naddr->flags & ADDR_NEW))
+			continue;
+		dev_dbg(netcp->dev, "adding address %pM, type %x\n",
+			naddr->addr, naddr->type);
+		for_each_module(netcp, priv) {
+			module = priv->netcp_module;
+			if (!module->add_addr)
+				continue;
+			error = module->add_addr(priv->module_priv, naddr);
+			WARN_ON(error);
+		}
+	}
+}
+
+static void netcp_set_rx_mode(struct net_device *ndev)
+{
+	struct netcp_priv *netcp = netdev_priv(ndev);
+	struct netdev_hw_addr *ndev_addr;
+	bool promisc;
+
+	promisc = (ndev->flags & IFF_PROMISC ||
+		   ndev->flags & IFF_ALLMULTI ||
+		   netdev_mc_count(ndev) > NETCP_MAX_MCAST_ADDR);
+
+	spin_lock(&netcp->lock);
+
+	/* first clear all marks */
+	netcp_addr_clear_mark(netcp);
+
+	/* next add new entries, mark existing ones */
+	netcp_addr_add_mark(netcp, ndev->broadcast, ADDR_BCAST);
+	for_each_dev_addr(ndev, ndev_addr)
+		netcp_addr_add_mark(netcp, ndev_addr->addr, ADDR_DEV);
+	netdev_for_each_uc_addr(ndev_addr, ndev)
+		netcp_addr_add_mark(netcp, ndev_addr->addr, ADDR_UCAST);
+	netdev_for_each_mc_addr(ndev_addr, ndev)
+		netcp_addr_add_mark(netcp, ndev_addr->addr, ADDR_MCAST);
+
+	if (promisc)
+		netcp_addr_add_mark(netcp, NULL, ADDR_ANY);
+
+	/* finally sweep and callout into modules */
+	netcp_addr_sweep_del(netcp);
+	netcp_addr_sweep_add(netcp);
+
+	spin_unlock(&netcp->lock);
+}
+
+struct dma_chan *netcp_get_rx_chan(struct netcp_priv *netcp)
+{
+	return netcp->rx_channel;
+}
+EXPORT_SYMBOL(netcp_get_rx_chan);
+
+static void netcp_rx_notify(struct dma_chan *chan, void *arg)
+{
+	struct netcp_priv *netcp = arg;
+
+	BUG_ON(netcp->rx_state != RX_STATE_INTERRUPT);
+	dmaengine_pause(netcp->rx_channel);
+	netcp_set_rx_state(netcp, RX_STATE_SCHEDULED);
+	napi_schedule(&netcp->napi);
+}
+
+/* Open the device */
+static int netcp_ndo_open(struct net_device *ndev)
+{
+	struct netcp_priv *netcp = netdev_priv(ndev);
+	struct netcp_intf_modpriv *intf_modpriv;
+	struct netcp_module *module;
+	struct dma_keystone_info config;
+	dma_cap_mask_t mask;
+	int err = -ENODEV;
+	const char *name;
+	int i;
+
+	netif_carrier_off(ndev);
+
+	BUG_ON(netcp->rx_state != RX_STATE_INVALID);
+
+	dma_cap_zero(mask);
+	dma_cap_set(DMA_SLAVE, mask);
+
+	name = netcp->rx_chan_name;
+	netcp->rx_channel = dma_request_channel_by_name(mask, name);
+	if (IS_ERR_OR_NULL(netcp->rx_channel))
+		goto fail;
+
+	memset(&config, 0, sizeof(config));
+	config.direction		= DMA_DEV_TO_MEM;
+	config.scatterlist_size		= NETCP_SGLIST_SIZE;
+	config.rxpool_allocator		= netcp_rxpool_alloc;
+	config.rxpool_destructor	= netcp_rxpool_free;
+	config.rxpool_param		= netcp;
+	config.rxpool_thresh_enable	= DMA_THRESH_NONE;
+
+	for (i = 0; i < KEYSTONE_QUEUES_PER_CHAN &&
+		    netcp->rx_queue_depths[i] &&
+		    netcp->rx_buffer_sizes[i]; ++i) {
+		config.rxpools[i].pool_depth  = netcp->rx_queue_depths[i];
+		config.rxpools[i].buffer_size = netcp->rx_buffer_sizes[i];
+	}
+	config.rxpool_count = i;
+
+	err = dma_keystone_config(netcp->rx_channel, &config);
+	if (err) {
+		dev_err(netcp->dev, "%d error configuring RX channel\n",
+				err);
+		goto fail;
+	}
+
+	dma_set_notify(netcp->rx_channel, netcp_rx_notify, netcp);
+
+	dev_dbg(netcp->dev, "opened RX channel: %p\n", netcp->rx_channel);
+
+	netcp_set_rx_state(netcp, RX_STATE_INTERRUPT);
+
+	for_each_module(netcp, intf_modpriv) {
+		module = intf_modpriv->netcp_module;
+		if (module->open != NULL) {
+			err = module->open(intf_modpriv->module_priv, ndev);
+			if (err != 0) {
+				dev_err(netcp->dev, "Open failed\n");
+				goto fail;
+			}
+		}
+	}
+
+	dma_rxfree_refill(netcp->rx_channel);
+	napi_enable(&netcp->napi);
+	netif_tx_wake_all_queues(ndev);
+
+	dev_info(netcp->dev, "netcp device %s opened\n", ndev->name);
+
+	return 0;
+fail:
+	if (netcp->rx_channel) {
+		dma_release_channel(netcp->rx_channel);
+		netcp->rx_channel = NULL;
+	}
+	return err;
+}
+
+/* Close the device */
+static int netcp_ndo_stop(struct net_device *ndev)
+{
+	struct netcp_priv *netcp = netdev_priv(ndev);
+	struct netcp_intf_modpriv *intf_modpriv;
+	struct netcp_module *module;
+	unsigned long flags;
+	int err = 0;
+
+	spin_lock_irqsave(&netcp->lock, flags);
+
+	netif_tx_stop_all_queues(ndev);
+	netif_carrier_off(ndev);
+
+	BUG_ON(!netcp_is_alive(netcp));
+
+	netcp_set_rx_state(netcp, RX_STATE_TEARDOWN);
+
+	dmaengine_pause(netcp->rx_channel);
+
+	spin_unlock_irqrestore(&netcp->lock, flags);
+
+	napi_disable(&netcp->napi);
+
+	if (netcp->rx_channel) {
+		dma_release_channel(netcp->rx_channel);
+		netcp->rx_channel = NULL;
+	}
+
+	netcp_set_rx_state(netcp, RX_STATE_INVALID);
+
+	netcp_addr_clear_mark(netcp);
+	netcp_addr_sweep_del(netcp);
+
+	for_each_module(netcp, intf_modpriv) {
+		module = intf_modpriv->netcp_module;
+		if (module->close != NULL) {
+			err = module->close(intf_modpriv->module_priv, ndev);
+			if (err != 0) {
+				dev_err(netcp->dev, "Close failed\n");
+				goto out;
+			}
+		}
+	}
+out:
+	dev_dbg(netcp->dev, "netcp device %s stopped\n", ndev->name);
+
+	return 0;
+}
+
+static int netcp_ndo_ioctl(struct net_device *ndev,
+			   struct ifreq *req, int cmd)
+{
+	struct netcp_priv *netcp = netdev_priv(ndev);
+	struct netcp_intf_modpriv *intf_modpriv;
+	struct netcp_module *module;
+	int ret = -1, err = -EOPNOTSUPP;
+
+	if (!netif_running(ndev))
+		return -EINVAL;
+
+	for_each_module(netcp, intf_modpriv) {
+		module = intf_modpriv->netcp_module;
+		if (!module->ioctl)
+			continue;
+
+		err = module->ioctl(intf_modpriv->module_priv, req, cmd);
+		if ((err < 0) && (err != -EOPNOTSUPP))
+			return err;
+		if (err == 0)
+			ret = err;
+	}
+
+	return (ret == 0) ? 0 : err;
+}
+
+static int netcp_ndo_change_mtu(struct net_device *ndev, int new_mtu)
+{
+	struct netcp_priv *netcp = netdev_priv(ndev);
+	int old_max_frame = ndev->mtu + ETH_HLEN + ETH_FCS_LEN;
+	int max_frame = new_mtu + ETH_HLEN + ETH_FCS_LEN;
+	unsigned long flags;
+	int ret = 0;
+
+	spin_lock_irqsave(&netcp->lock, flags);
+
+	netif_tx_stop_all_queues(ndev);
+	netif_carrier_off(ndev);
+
+	BUG_ON(!netcp_is_alive(netcp));
+
+	netcp_set_rx_state(netcp, RX_STATE_TEARDOWN);
+
+	dmaengine_pause(netcp->rx_channel);
+
+	spin_unlock_irqrestore(&netcp->lock, flags);
+
+	napi_disable(&netcp->napi);
+
+	netcp_set_rx_state(netcp, RX_STATE_INVALID);
+
+	/* MTU < 68 is an error for IPv4 traffic, just don't allow it */
+	if ((new_mtu < 68) ||
+	    (max_frame > NETCP_MAX_FRAME_SIZE)) {
+		dev_err(netcp->dev, "Invalid mtu size = %d\n", new_mtu);
+		ret = -EINVAL;
+		goto out_change_mtu;
+	}
+
+	if (old_max_frame == max_frame) {
+		ret = 0;
+		goto out_change_mtu;
+	}
+
+	netcp->rx_packet_max = max_frame;
+
+	ndev->mtu = new_mtu;
+
+out_change_mtu:
+	netcp_set_rx_state(netcp, RX_STATE_INTERRUPT);
+
+	dmaengine_resume(netcp->rx_channel);
+
+	napi_enable(&netcp->napi);
+
+	netif_tx_start_all_queues(ndev);
+	netif_carrier_on(ndev);
+
+	return ret;
+}
+
+static void netcp_ndo_tx_timeout(struct net_device *ndev)
+{
+	struct netcp_priv *netcp = netdev_priv(ndev);
+
+	dev_err(netcp->dev, "transmit timed out\n");
+	/* FIXME: Need to unstall the DMAs */
+	netif_tx_wake_all_queues(ndev);
+}
+
+static int netcp_rx_add_vid(struct net_device *ndev, u16 vid)
+{
+	struct netcp_priv *netcp = netdev_priv(ndev);
+	struct netcp_intf_modpriv *intf_modpriv;
+	struct netcp_module *module;
+	unsigned long flags;
+	int err = 0;
+
+	dev_info(netcp->dev, "adding rx vlan id: %d\n", vid);
+
+	spin_lock_irqsave(&netcp->lock, flags);
+
+	for_each_module(netcp, intf_modpriv) {
+		module = intf_modpriv->netcp_module;
+		if ((module->add_vid != NULL) && (vid != 0)) {
+			err = module->add_vid(intf_modpriv->module_priv, vid);
+			if (err != 0) {
+				dev_err(netcp->dev, "Could not add "
+					"vlan id = %d\n", vid);
+				return -ENODEV;
+			}
+		}
+	}
+
+	spin_unlock_irqrestore(&netcp->lock, flags);
+
+	return 0;
+}
+
+static int netcp_rx_kill_vid(struct net_device *ndev, u16 vid)
+{
+	struct netcp_priv *netcp = netdev_priv(ndev);
+	struct netcp_intf_modpriv *intf_modpriv;
+	struct netcp_module *module;
+	int err = 0;
+
+	dev_info(netcp->dev, "removing rx vlan id: %d\n", vid);
+
+	for_each_module(netcp, intf_modpriv) {
+		module = intf_modpriv->netcp_module;
+		if (module->del_vid != NULL) {
+			err = module->del_vid(intf_modpriv->module_priv, vid);
+			if (err != 0) {
+				dev_err(netcp->dev, "Could not delete "
+					"vlan id = %d\n", vid);
+				return -ENODEV;
+			}
+		}
+	}
+
+	return 0;
+}
+
+static u16 netcp_select_queue(struct net_device *dev, struct sk_buff *skb)
+{
+	return 0;
+}
+
+static int netcp_setup_tc(struct net_device *dev, u8 num_tc)
+{
+	int i;
+
+	/* setup tc must be called under rtnl lock */
+	ASSERT_RTNL();
+
+	/* Sanity-check the number of traffic classes requested */
+	if ((dev->real_num_tx_queues <= 1) || (dev->real_num_tx_queues < num_tc))
+		return -EINVAL;
+
+	/* Configure traffic class to queue mappings */
+	if (num_tc) {
+		netdev_set_num_tc(dev, num_tc);
+		for (i = 0; i < num_tc; i++)
+			netdev_set_tc_queue(dev, i, 1, i);
+	} else {
+		netdev_reset_tc(dev);
+	}
+
+	return 0;
+}
+
+u32 netcp_get_streaming_switch(struct netcp_device *netcp_device, int port)
+{
+	u32	reg;
+
+	reg = __raw_readl(netcp_device->streaming_switch);
+	return (port == 0) ? reg : (reg >> ((port - 1) * 8)) & 0xff;
+}
+EXPORT_SYMBOL(netcp_get_streaming_switch);
+
+u32 netcp_set_streaming_switch(struct netcp_device *netcp_device,
+				int port, u32 new_value)
+{
+	u32	reg;
+	u32	old_value;
+
+	reg = __raw_readl(netcp_device->streaming_switch);
+
+	if (port == 0) {
+		old_value = reg;
+		reg = (new_value << 24) | (new_value << 16) |
+			(new_value << 8) | new_value;
+	} else {
+		int shift = (port - 1) * 8;
+		old_value = (reg >> shift) & 0xff;
+		reg &= ~(0xff << shift);
+		reg |= (new_value & 0xff) << shift;
+	}
+
+	__raw_writel(reg, netcp_device->streaming_switch);
+	return old_value;
+}
+EXPORT_SYMBOL(netcp_set_streaming_switch);
+
+
+static const struct net_device_ops netcp_netdev_ops = {
+	.ndo_open		= netcp_ndo_open,
+	.ndo_stop		= netcp_ndo_stop,
+	.ndo_start_xmit		= netcp_ndo_start_xmit,
+	.ndo_set_rx_mode	= netcp_set_rx_mode,
+	.ndo_do_ioctl           = netcp_ndo_ioctl,
+	.ndo_change_mtu		= netcp_ndo_change_mtu,
+	.ndo_set_mac_address	= eth_mac_addr,
+	.ndo_validate_addr	= eth_validate_addr,
+	.ndo_vlan_rx_add_vid	= netcp_rx_add_vid,
+	.ndo_vlan_rx_kill_vid	= netcp_rx_kill_vid,
+	.ndo_tx_timeout		= netcp_ndo_tx_timeout,
+	.ndo_select_queue	= netcp_select_queue,
+	.ndo_setup_tc		= netcp_setup_tc,
+};
+
+int netcp_create_interface(struct netcp_device *netcp_device,
+			   struct net_device **ndev_p,
+			   const char *ifname_proto,
+			   int tx_queues, int rx_queues, int cpsw_port)
+{
+	struct platform_device *pdev = netcp_device->platform_device;
+	struct device_node *node = pdev->dev.of_node;
+	struct device_node *node_ifgroup;
+	struct device_node *node_interface;
+	struct netcp_inst_modpriv *inst_modpriv;
+	struct netcp_priv *netcp;
+	struct net_device *ndev;
+	resource_size_t size;
+	struct resource res;
+	void __iomem *efuse = NULL;
+	u32 efuse_mac = 0;
+	const void *mac_addr;
+	u8 efuse_mac_addr[6];
+	int ret = 0;
+
+	ndev = alloc_netdev_mqs(sizeof(struct netcp_priv),
+			(ifname_proto ? ifname_proto : "eth%d"),
+			ether_setup, tx_queues, rx_queues);
+	*ndev_p = ndev;
+	if (!ndev) {
+		dev_err(&pdev->dev, "Error allocating net_device\n");
+		ret = -ENOMEM;
+		goto probe_quit;
+	}
+	dev_dbg(&pdev->dev, "%s(): ndev = %p\n", __func__, ndev);
+	ndev->features |= NETIF_F_SG;
+	ndev->features |= NETIF_F_FRAGLIST;
+
+	ndev->hw_features = ndev->features;
+
+	ndev->vlan_features |= NETIF_F_TSO |
+				NETIF_F_TSO6 |
+				NETIF_F_IP_CSUM |
+				NETIF_F_IPV6_CSUM |
+				NETIF_F_SG|
+				NETIF_F_FRAGLIST;
+
+	netcp = netdev_priv(ndev);
+	spin_lock_init(&netcp->lock);
+	INIT_LIST_HEAD(&netcp->module_head);
+	INIT_LIST_HEAD(&netcp->txhook_list_head);
+	INIT_LIST_HEAD(&netcp->rxhook_list_head);
+	INIT_LIST_HEAD(&netcp->addr_list);
+	netcp->netcp_device = netcp_device;
+	netcp->pdev = netcp_device->platform_device;
+	netcp->ndev = ndev;
+	netcp->dev  = &ndev->dev;
+	netcp->cpsw_port = cpsw_port;
+	netcp->msg_enable = netif_msg_init(netcp_debug_level, NETCP_DEBUG);
+	netcp->rx_packet_max = netcp_rx_packet_max;
+	netcp_set_rx_state(netcp, RX_STATE_INVALID);
+
+	node_ifgroup = of_get_child_by_name(node, "interfaces");
+	if (!node_ifgroup) {
+		dev_info(&pdev->dev, "could not find group \"interfaces\", "
+				     "defaulting to parent\n");
+		node_interface = node;
+	} else {
+		char node_name[24];
+		snprintf(node_name, sizeof(node_name), "interface-%d",
+			 (cpsw_port == 0) ? 0 : (cpsw_port - 1));
+		node_interface = of_get_child_by_name(node_ifgroup, node_name);
+		if (!node_interface) {
+			dev_err(&pdev->dev, "could not find %s\n", node_name);
+			of_node_put(node_ifgroup);
+			ret = -ENODEV;
+			goto probe_quit;
+		}
+		dev_dbg(&pdev->dev, "Using node \"%s\"\n", node_name);
+	}
+
+	ret = of_property_read_u32(node_interface, "efuse-mac", &efuse_mac);
+	if (efuse_mac) {
+		if (of_address_to_resource(node, 1, &res)) {
+			dev_err(&pdev->dev, "could not find resource\n");
+			ret = -ENODEV;
+			goto probe_quit;
+		}
+		size = resource_size(&res);
+
+		if (!devm_request_mem_region(&pdev->dev, res.start, size,
+					     dev_name(&pdev->dev))) {
+			dev_err(&pdev->dev, "could not reserve resource\n");
+			ret = -ENOMEM;
+			goto probe_quit;
+		}
+
+		efuse = devm_ioremap_nocache(&pdev->dev, res.start, size);
+		if (!efuse) {
+			dev_err(&pdev->dev, "could not map resource\n");
+			devm_release_mem_region(&pdev->dev, res.start, size);
+			ret = -ENOMEM;
+			goto probe_quit;
+		}
+
+		emac_arch_get_mac_addr(efuse_mac_addr, efuse);
+		if (is_valid_ether_addr(efuse_mac_addr))
+			memcpy(ndev->dev_addr, efuse_mac_addr, ETH_ALEN);
+		else
+			random_ether_addr(ndev->dev_addr);
+
+		devm_iounmap(&pdev->dev, efuse);
+		devm_release_mem_region(&pdev->dev, res.start, size);
+	} else {
+		mac_addr = of_get_mac_address(node_interface);
+		if (mac_addr)
+			memcpy(ndev->dev_addr, mac_addr, ETH_ALEN);
+		else
+			random_ether_addr(ndev->dev_addr);
+	}
+
+	ret = of_property_read_string(node_interface, "rx-channel",
+				      &netcp->rx_chan_name);
+	if (ret < 0) {
+		dev_err(&pdev->dev, "missing \"rx-channel\" parameter\n");
+		netcp->rx_chan_name = "netrx";
+	}
+
+	ret = of_property_read_u32_array(node_interface, "rx-queue-depth",
+			netcp->rx_queue_depths, KEYSTONE_QUEUES_PER_CHAN);
+	if (ret < 0) {
+		dev_err(&pdev->dev, "missing \"rx-queue-depth\" parameter\n");
+		netcp->rx_queue_depths[0] = 128;
+	}
+
+	ret = of_property_read_u32_array(node_interface, "rx-buffer-size",
+			netcp->rx_buffer_sizes, KEYSTONE_QUEUES_PER_CHAN);
+	if (ret) {
+		dev_err(&pdev->dev, "missing \"rx-buffer-size\" parameter\n");
+		netcp->rx_buffer_sizes[0] = 1500;
+	}
+
+	if (node_ifgroup) {
+		of_node_put(node_interface);
+		of_node_put(node_ifgroup);
+		node_interface = NULL;
+		node_ifgroup = NULL;
+	}
+
+	/* NAPI register */
+	netif_napi_add(ndev, &netcp->napi, netcp_poll, NETCP_NAPI_WEIGHT);
+
+	/* Register the network device */
+	ndev->dev_id		= 0;
+	ndev->watchdog_timeo	= NETCP_TX_TIMEOUT;
+	ndev->netdev_ops	= &netcp_netdev_ops;
+
+	SET_NETDEV_DEV(ndev, &pdev->dev);
+
+	ret = register_netdev(ndev);
+	if (ret) {
+		dev_err(netcp->dev, "Error registering net device\n");
+		ret = -ENODEV;
+		goto clean_ndev_ret;
+	}
+	dev_info(&pdev->dev, "Created interface \"%s\"\n", ndev->name);
+	list_add_tail(&netcp->interface_list, &netcp_device->interface_head);
+
+	/* Notify each registered module of the new interface */
+	list_for_each_entry(inst_modpriv, &netcp_device->modpriv_head, inst_list) {
+		struct netcp_module *module = inst_modpriv->netcp_module;
+		struct netcp_intf_modpriv *intf_modpriv;
+
+		intf_modpriv = kzalloc(sizeof(*intf_modpriv), GFP_KERNEL);
+		if (!intf_modpriv) {
+			dev_err(&pdev->dev, "Error allocating intf_modpriv for %s\n",
+					module->name);
+			continue;
+		}
+
+		intf_modpriv->netcp_priv = netcp;
+		intf_modpriv->netcp_module = module;
+		list_add_tail(&intf_modpriv->intf_list, &netcp->module_head);
+
+		dev_dbg(&pdev->dev, "Attaching module \"%s\"\n", module->name);
+		ret = module->attach(inst_modpriv->module_priv, ndev, &intf_modpriv->module_priv);
+		if (ret) {
+			dev_info(&pdev->dev, "Attach of module %s declined with %d\n",
+					module->name, ret);
+			list_del(&intf_modpriv->intf_list);
+			kfree(intf_modpriv);
+			continue;
+		}
+	}
+
+	return 0;
+
+clean_ndev_ret:
+	free_netdev(ndev);
+probe_quit:
+	*ndev_p = NULL;
+	return ret;
+}
+EXPORT_SYMBOL(netcp_create_interface);
+
+void netcp_delete_interface(struct netcp_device *netcp_device,
+			    struct net_device *ndev)
+{
+	struct netcp_intf_modpriv *intf_modpriv, *tmp;
+	struct netcp_priv *netcp = netdev_priv(ndev);
+	struct netcp_module *module;
+
+	dev_info(&netcp_device->platform_device->dev,
+			"Removing interface \"%s\"\n", ndev->name);
+
+	/* Notify each of the modules that the interface is going away */
+	list_for_each_entry_safe(intf_modpriv, tmp, &netcp->module_head, intf_list) {
+		module = intf_modpriv->netcp_module;
+		dev_dbg(&netcp_device->platform_device->dev,
+				"Releasing module \"%s\"\n", module->name);
+		if (module->release)
+			module->release(intf_modpriv->module_priv);
+		list_del(&intf_modpriv->intf_list);
+		kfree(intf_modpriv);
+	}
+	WARN(!list_empty(&netcp->module_head),
+			"%s interface module list is not empty!\n", ndev->name);
+
+	list_del(&netcp->interface_list);
+
+	unregister_netdev(ndev);
+	netif_napi_del(&netcp->napi);
+	free_netdev(ndev);
+}
+EXPORT_SYMBOL(netcp_delete_interface);
+
+
+static int netcp_probe(struct platform_device *pdev)
+{
+	struct device_node *node = pdev->dev.of_node;
+	struct device_node *child;
+	struct netcp_device *netcp_device;
+	struct netcp_module *module;
+	struct netcp_inst_modpriv *inst_modpriv;
+	const char *name;
+	u32 temp[4];
+	int ret;
+
+	if (!node) {
+		dev_err(&pdev->dev, "could not find device info\n");
+		return -EINVAL;
+	}
+
+	/* Allocate a new NETCP device instance */
+	netcp_device = kzalloc(sizeof(*netcp_device), GFP_KERNEL);
+	if (!netcp_device) {
+		dev_err(&pdev->dev, "failure allocating NETCP device\n");
+		return -ENOMEM;
+	}
+	dev_dbg(&pdev->dev, "%s(): netcp_device = %p\n", __func__, netcp_device);
+
+	/* Initialize the NETCP device instance */
+	INIT_LIST_HEAD(&netcp_device->interface_head);
+	INIT_LIST_HEAD(&netcp_device->modpriv_head);
+	netcp_device->platform_device = pdev;
+	platform_set_drvdata(pdev, netcp_device);
+
+	/* Map the Streaming Switch */
+	if (of_property_read_u32_array(node, "streaming-regs",
+					(u32 *)&(temp[0]), 2)) {
+		dev_err(&pdev->dev, "No streaming regs defined\n");
+	} else {
+		netcp_device->streaming_switch =
+			devm_ioremap_nocache(&pdev->dev, temp[0], temp[1]);
+		if (!netcp_device->streaming_switch) {
+			dev_err(&pdev->dev, "can't map streaming switch\n");
+			ret = -ENOMEM;
+			goto probe_quit;
+		}
+	}
+
+	/* Add the device instance to the list */
+	list_add_tail(&netcp_device->device_list, &netcp_devices);
+
+	/* Probe any modules already registered */
+	for_each_child_of_node(node, child) {
+		name = netcp_node_name(child);
+		module = netcp_find_module(name);
+		if (!module)
+			continue;
+
+		inst_modpriv = kzalloc(sizeof(*inst_modpriv), GFP_KERNEL);
+		if (!inst_modpriv) {
+			dev_err(&pdev->dev, "Failed to allocate instance for for %s\n", name);
+			continue;	/* FIXME: Fail instead? */
+		}
+		inst_modpriv->netcp_device = netcp_device;
+		inst_modpriv->netcp_module = module;
+		list_add_tail(&inst_modpriv->inst_list, &netcp_device->modpriv_head);
+
+		dev_dbg(&pdev->dev, "%s(): probing module \"%s\"\n", __func__, name);
+		ret = module->probe(netcp_device, &pdev->dev, child, &inst_modpriv->module_priv);
+		if (ret) {
+			dev_warn(&pdev->dev, "Probe of module %s failed with %d\n", name, ret);
+			list_del(&inst_modpriv->inst_list);
+			kfree(inst_modpriv);
+			continue;
+		}
+	}
+
+	return 0;
+
+probe_quit:
+	platform_set_drvdata(pdev, NULL);
+	kfree(netcp_device);
+	return ret;
+}
+
+static int netcp_remove(struct platform_device *pdev)
+{
+	struct netcp_device *netcp_device = platform_get_drvdata(pdev);
+	struct netcp_inst_modpriv *inst_modpriv, *tmp;
+	struct netcp_module *module;
+
+	list_for_each_entry_safe(inst_modpriv, tmp, &netcp_device->modpriv_head, inst_list) {
+		module = inst_modpriv->netcp_module;
+		dev_dbg(&pdev->dev, "Removing module \"%s\"\n", module->name);
+		module->remove(netcp_device, inst_modpriv->module_priv);
+		list_del(&inst_modpriv->inst_list);
+		kfree(inst_modpriv);
+	}
+	WARN(!list_empty(&netcp_device->interface_head),
+			"%s interface list not empty!\n", pdev->name);
+
+	/* Unmap the Streaming Switch */
+	if (netcp_device->streaming_switch)
+		devm_iounmap(&pdev->dev, netcp_device->streaming_switch);
+
+	platform_set_drvdata(pdev, NULL);
+	kfree(netcp_device);
+	return 0;
+}
+
+static struct of_device_id of_match[] = {
+	{ .compatible = "ti,keystone-netcp", },
+	{},
+};
+MODULE_DEVICE_TABLE(of, of_match);
+
+static struct platform_driver netcp_driver = {
+	.driver = {
+		.name		= "keystone-netcp",
+		.owner		= THIS_MODULE,
+		.of_match_table	= of_match,
+	},
+	.probe = netcp_probe,
+	.remove = netcp_remove,
+};
+
+extern int  keystone_cpsw_init(void);
+extern void keystone_cpsw_exit(void);
+#ifdef CONFIG_TI_KEYSTONE_XGE
+extern int  keystone_cpswx_init(void);
+extern void keystone_cpswx_exit(void);
+#endif
+
+static int __init netcp_init(void)
+{
+	int err;
+
+	/* Create a cache for these commonly-used structures */
+	netcp_pinfo_cache = kmem_cache_create("netcp_pinfo_cache",
+			sizeof(struct netcp_packet), sizeof(void *),
+			0, NULL);
+	if (!netcp_pinfo_cache)
+		return -ENOMEM;
+
+	err = platform_driver_register(&netcp_driver);
+	if (err)
+		goto netcp_fail;
+
+	err = keystone_cpsw_init();
+	if (err)
+		goto cpsw_fail;
+
+#ifdef CONFIG_TI_KEYSTONE_XGE
+	err = keystone_cpswx_init();
+	if (err)
+		goto cpsw_fail;
+#endif
+
+	return 0;
+
+cpsw_fail:
+	platform_driver_unregister(&netcp_driver);
+netcp_fail:
+	kmem_cache_destroy(netcp_pinfo_cache);
+	netcp_pinfo_cache = NULL;
+	return err;
+}
+module_init(netcp_init);
+
+static void __exit netcp_exit(void)
+{
+	keystone_cpsw_exit();
+#ifdef CONFIG_TI_KEYSTONE_XGE
+	keystone_cpswx_exit();
+#endif
+	platform_driver_unregister(&netcp_driver);
+	kmem_cache_destroy(netcp_pinfo_cache);
+	netcp_pinfo_cache = NULL;
+}
+module_exit(netcp_exit);
+
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("TI Keystone Ethernet driver");
diff --git a/drivers/net/ethernet/ti/keystone_net_sa.c b/drivers/net/ethernet/ti/keystone_net_sa.c
new file mode 100644
index 0000000..dfec6fe
--- /dev/null
+++ b/drivers/net/ethernet/ti/keystone_net_sa.c
@@ -0,0 +1,254 @@
+/*
+ * Copyright (C) 2012 Texas Instruments Incorporated
+ * Authors: Sandeep Nair <sandeep_n@ti.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation version 2.
+ *
+ * This program is distributed "as is" WITHOUT ANY WARRANTY of any
+ * kind, whether express or implied; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/io.h>
+#include <linux/slab.h>
+#include <linux/delay.h>
+#include <linux/types.h>
+#include <linux/module.h>
+#include <linux/delay.h>
+#include <linux/of.h>
+#include <linux/of_device.h>
+#include <linux/of_address.h>
+#include <linux/interrupt.h>
+#include <linux/byteorder/generic.h>
+#include <linux/platform_device.h>
+#include <linux/keystone-dma.h>
+#include <linux/errqueue.h>
+#include <uapi/linux/udp.h>
+
+#include "keystone_net.h"
+#include "keystone_pasahost.h"
+
+struct sa_device {
+	struct netcp_device		*netcp_device;
+	struct net_device		*net_device;		/* FIXME */
+	struct device			*dev;
+	const char			*tx_chan_name;
+	u32				 tx_queue_depth;
+	struct netcp_tx_pipe		 tx_pipe;
+	struct device_node		*node;
+	u32				 multi_if;
+};
+
+struct ipsecmgr_mod_sa_ctx_info {
+	u32 word0;
+	u32 word1;
+	u16 flow_id;
+};
+
+/**
+ * Used to update the destination information within swInfo[2]
+ */
+
+#define SA_SWINFO_UPDATE_DEST_INFO(info, queueID, flowID) \
+{ \
+	(info[0]) |= 0x40000000L; \
+	(info[2]) = ((queueID)) | (((flowID) & 0xFF) << 16) | \
+	((info[2]) & 0xFF000000L); \
+}
+
+#define	SA_TXHOOK_ORDER	30
+
+static int sa_tx_hook(int order, void *data, struct netcp_packet *p_info)
+{
+	struct sa_device *sa_dev = data;
+	u16 offset, len, ihl;
+	u32 *psdata, *swinfo;
+	const struct iphdr *iph;
+	struct ipsecmgr_mod_sa_ctx_info *ctx_info =
+			(struct ipsecmgr_mod_sa_ctx_info *)p_info->skb->sp;
+
+	if (!ctx_info)
+		return 0;
+
+	iph = ip_hdr(p_info->skb);
+
+	if (iph->version != IPVERSION)
+		return 0;
+
+	ihl = iph->ihl * 4;
+
+	if (iph->protocol == IPPROTO_UDP) {
+		/* UDP encapsulation for IPSec NAT-T */
+		offset = (ulong)(skb_network_header(p_info->skb) -
+			p_info->skb->data) + ihl + sizeof(struct udphdr);
+		len = ntohs(iph->tot_len) - ihl - sizeof(struct udphdr);
+	} else if (iph->protocol == IPPROTO_ESP) {
+		offset = (ulong)(skb_network_header(p_info->skb) -
+			p_info->skb->data) + ihl;
+		len = ntohs(iph->tot_len) - ihl;
+	} else {
+	    return 0;
+	}
+
+	psdata = netcp_push_psdata(p_info, (2 * sizeof(u32)));
+	if (!psdata)
+		return -ENOMEM;
+
+	psdata[0] = PASAHO_SINFO_FORMAT_CMD(offset, len);
+	psdata[1] = 0;
+	swinfo = &p_info->epib[1];
+	swinfo[0] = ctx_info->word0;
+	swinfo[1] = ctx_info->word1;
+	SA_SWINFO_UPDATE_DEST_INFO(swinfo, p_info->tx_pipe->dma_queue,
+			ctx_info->flow_id);
+
+	p_info->tx_pipe = &sa_dev->tx_pipe;
+	kfree(ctx_info);
+	p_info->skb->sp = NULL;
+	return 0;
+}
+
+static int sa_close(void *intf_priv, struct net_device *ndev)
+{
+	struct sa_device *sa_dev = intf_priv;
+	struct netcp_priv *netcp_priv = netdev_priv(ndev);
+
+	netcp_unregister_txhook(netcp_priv, SA_TXHOOK_ORDER, sa_tx_hook, sa_dev);
+
+	netcp_txpipe_close(&sa_dev->tx_pipe);
+
+	return 0;
+}
+
+static int sa_open(void *intf_priv, struct net_device *ndev)
+{
+	struct sa_device *sa_dev = intf_priv;
+	struct netcp_priv *netcp_priv = netdev_priv(ndev);
+	int ret;
+
+	/* Open the SA IPSec data transmit channel */
+	ret = netcp_txpipe_open(&sa_dev->tx_pipe);
+	if (ret)
+		goto fail;
+
+	netcp_register_txhook(netcp_priv, SA_TXHOOK_ORDER, sa_tx_hook, sa_dev);
+	return 0;
+
+fail:
+	sa_close(intf_priv, ndev);
+	return ret;
+}
+
+static int sa_attach(void *inst_priv, struct net_device *ndev, void **intf_priv)
+{
+	struct netcp_priv *netcp = netdev_priv(ndev);
+	struct sa_device *sa_dev = inst_priv;
+	char node_name[24];
+
+	snprintf(node_name, sizeof(node_name), "interface-%d",
+		 (sa_dev->multi_if) ? (netcp->cpsw_port - 1) : 0);
+
+	if (of_find_property(sa_dev->node, node_name, NULL)) {
+		sa_dev->net_device = ndev;
+		*intf_priv = sa_dev;
+
+		netcp_txpipe_init(&sa_dev->tx_pipe, netdev_priv(ndev),
+				  sa_dev->tx_chan_name, sa_dev->tx_queue_depth);
+
+		sa_dev->tx_pipe.dma_psflags = netcp->cpsw_port;
+
+		return 0;
+	} else
+		return -ENODEV;
+}
+
+static int sa_release(void *intf_priv)
+{
+	struct sa_device *sa_dev = intf_priv;
+
+	printk("%s() called for interface %s\n", __func__, sa_dev->net_device->name);
+	sa_dev->net_device = NULL;
+	return 0;
+}
+
+static int sa_remove(struct netcp_device *netcp_device, void *inst_priv)
+{
+	struct sa_device *sa_dev = inst_priv;
+	kfree(sa_dev);
+	return 0;
+}
+
+static int sa_probe(struct netcp_device *netcp_device,
+		    struct device *dev,
+		    struct device_node *node,
+		    void **inst_priv)
+{
+	struct sa_device *sa_dev;
+	int ret = 0;
+
+	if (!node) {
+		dev_err(dev, "device tree info unavailable\n");
+		return -ENODEV;
+	}
+
+	sa_dev = devm_kzalloc(dev, sizeof(struct sa_device), GFP_KERNEL);
+	if (!sa_dev) {
+		dev_err(dev, "memory allocation failed\n");
+		return -ENOMEM;
+	}
+	*inst_priv = sa_dev;
+	sa_dev->dev = dev;
+
+	sa_dev->node = node;
+
+	if (of_find_property(node, "multi-interface", NULL))
+		sa_dev->multi_if = 1;
+
+	ret = of_property_read_string(node, "tx-channel", &sa_dev->tx_chan_name);
+	if (ret < 0) {
+		dev_err(dev, "missing \"tx-channel\" parameter, err %d\n", ret);
+		sa_dev->tx_chan_name = "satx";
+	}
+	dev_dbg(dev, "tx-channel \"%s\"\n", sa_dev->tx_chan_name);
+
+	ret = of_property_read_u32(node, "tx_queue_depth",
+				   &sa_dev->tx_queue_depth);
+	if (ret < 0) {
+		dev_err(dev, "missing tx_queue_depth parameter, err %d\n", ret);
+		sa_dev->tx_queue_depth = 32;
+	}
+	dev_dbg(dev, "tx_queue_depth %u\n", sa_dev->tx_queue_depth);
+
+	return 0;
+}
+
+static struct netcp_module sa_module = {
+	.name		= "keystone-sa",
+	.owner		= THIS_MODULE,
+	.probe		= sa_probe,
+	.open		= sa_open,
+	.close		= sa_close,
+	.remove		= sa_remove,
+	.attach		= sa_attach,
+	.release	= sa_release,
+};
+
+static int __init keystone_sa_init(void)
+{
+	return netcp_register_module(&sa_module);
+}
+module_init(keystone_sa_init);
+
+static void __exit keystone_sa_exit(void)
+{
+	netcp_unregister_module(&sa_module);
+}
+module_exit(keystone_sa_exit);
+
+MODULE_LICENSE("GPL v2");
+MODULE_AUTHOR("Sandeep Nair <sandeep_n@ti.com>");
+MODULE_DESCRIPTION("IPSec driver for Keystone devices");
+
diff --git a/drivers/net/ethernet/ti/keystone_pa.c b/drivers/net/ethernet/ti/keystone_pa.c
new file mode 100644
index 0000000..4fc4e75
--- /dev/null
+++ b/drivers/net/ethernet/ti/keystone_pa.c
@@ -0,0 +1,2473 @@
+/*
+ * Copyright (C) 2012 Texas Instruments Incorporated
+ * Authors: Sandeep Paulraj <s-paulraj@ti.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation version 2.
+ *
+ * This program is distributed "as is" WITHOUT ANY WARRANTY of any
+ * kind, whether express or implied; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/io.h>
+#include <linux/clk.h>
+#include <linux/slab.h>
+#include <linux/delay.h>
+#include <linux/types.h>
+#include <linux/module.h>
+#include <linux/delay.h>
+#include <linux/of.h>
+#include <linux/of_device.h>
+#include <linux/of_address.h>
+#include <linux/firmware.h>
+#include <linux/spinlock.h>
+#include <linux/highmem.h>
+#include <linux/interrupt.h>
+#include <linux/dmaengine.h>
+#include <linux/net_tstamp.h>
+#include <linux/dma-mapping.h>
+#include <linux/scatterlist.h>
+#include <linux/byteorder/generic.h>
+#include <linux/platform_device.h>
+#include <linux/keystone-dma.h>
+#include <linux/phy.h>
+#include <linux/errqueue.h>
+#include <linux/ptp_classify.h>
+#include <net/sctp/checksum.h>
+#include <linux/clocksource.h>
+
+#include "keystone_net.h"
+#include "keystone_pa.h"
+#include "keystone_pasahost.h"
+
+#define DEVICE_PA_PDSP02_FIRMWARE "keystone/pa_pdsp02_classify1.fw"
+#define DEVICE_PA_PDSP3_FIRMWARE "keystone/pa_pdsp3_classify2.fw"
+#define DEVICE_PA_PDSP45_FIRMWARE "keystone/pa_pdsp45_pam.fw"
+
+#define	PA_NETIF_FEATURES	(NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM)
+
+#define PSTREAM_ROUTE_PDSP0	0
+
+#define PA_PDSP_ALREADY_ACTIVE	0
+#define PA_PDSP_RESET_RELEASED	1
+#define PA_PDSP_NO_RESTART	2
+#define PA_MAX_PDSP_ENABLE_LOOP_COUNT	100000
+
+#define PA_INVALID_PORT			0xff
+
+#define PA_STATE_RESET			0  /* Sub-system state reset */
+#define PA_STATE_ENABLE			1  /* Sub-system state enable  */
+#define PA_STATE_QUERY			2  /* Query the Sub-system state */
+#define PA_STATE_INCONSISTENT		3  /* Sub-system is partially enabled */
+#define PA_STATE_INVALID_REQUEST	4  /* Invalid state command to the Sub-system */
+#define PA_STATE_ENABLE_FAILED		5  /*  The Sub-system did not respond after restart */
+
+/* System Timestamp */
+#define PAFRM_SRAM_SIZE			0x2000
+#define PAFRM_SYS_TIMESTAMP_ADDR	0x6460
+
+/* PDSP Versions */
+#define PAFRM_PDSP_VERSION_BASE		0x7F04
+
+#define DEVICE_PA_BASE				0x02000000
+#define DEVICE_PA_REGION_SIZE			0x48000
+#define DEVICE_PA_NUM_PDSPS			6
+
+#define PA_MEM_PDSP_IRAM(pdsp)			((pdsp) * 0x8000)
+#define PA_MEM_PDSP_SRAM(num)			((num) * 0x2000)
+#define PA_REG_PKTID_SOFT_RESET	                0x00404
+#define PA_REG_LUT2_SOFT_RESET	                0x00504
+#define PA_REG_STATS_SOFT_RESET	                0x06004
+
+#define PA_PDSP_CONST_REG_INDEX_C25_C24     0
+#define PA_PDSP_CONST_REG_INDEX_C27_C26     1
+#define PA_PDSP_CONST_REG_INDEX_C29_C28     2
+#define PA_PDSP_CONST_REG_INDEX_C31_C30     3
+
+/* The pdsp control register */
+#define PA_REG_VAL_PDSP_CTL_DISABLE_PDSP	1
+#define PA_REG_VAL_PDSP_CTL_RESET_PDSP	        0
+#define PA_REG_VAL_PDSP_CTL_STATE               (1 << 15)
+#define PA_REG_VAL_PDSP_CTL_ENABLE              (1 << 1)
+#define PA_REG_VAL_PDSP_CTL_SOFT_RESET          (1 << 0)
+#define PA_REG_VAL_PDSP_CTL_ENABLE_PDSP(pcval)	(((pcval) << 16)	\
+						 | PA_REG_VAL_PDSP_CTL_ENABLE \
+						 | PA_REG_VAL_PDSP_CTL_SOFT_RESET)
+
+/* Number of mailbox slots for each PDPS */
+#define PA_NUM_MAILBOX_SLOTS	4
+#define TEST_SWINFO0_TIMESTAMP	0x12340002
+
+#define PACKET_DROP	0
+#define PACKET_PARSE	1
+#define PACKET_HST	2
+
+#define NT 32
+
+#define PA_SGLIST_SIZE	3
+
+static struct sock_filter ptp_filter[] = {
+	PTP_FILTER
+};
+
+static bool need_timestamp(const struct sk_buff *skb)
+{
+	unsigned type = PTP_CLASS_NONE;
+
+	if (likely(skb->dev &&
+		   skb->dev->phydev &&
+		   skb->dev->phydev->drv &&
+		   skb->dev->phydev->drv->txtstamp))
+		type = sk_run_filter(skb, ptp_filter);
+
+	/* if timestamping is handled at the phy, we don't timestamp */
+	switch (type) {
+	case PTP_CLASS_V1_IPV4:
+	case PTP_CLASS_V1_IPV6:
+	case PTP_CLASS_V2_IPV4:
+	case PTP_CLASS_V2_IPV6:
+	case PTP_CLASS_V2_L2:
+	case PTP_CLASS_V2_VLAN:
+		return false;
+	}
+
+	return true;
+}
+
+const u32 pap_pdsp_const_reg_map[6][4] =
+{
+	/* PDSP0: C24-C31 */
+	{
+		0x0000007F,		/* C25-C24 */
+		0x0000006E,		/* C27-C26 */
+		0x00000000,		/* C29-C28 */
+		0x00000000		/* C31-C30 */
+	},
+	/* PDSP1: C24-C31 */
+	{
+		0x0001007F,		/* C25-C24 */
+		0x00480040,		/* C27-C26 */
+		0x00000000,		/* C29-C28 */
+		0x00000000		/* C31-C30 */
+	},
+	/* PDSP2: C24-C31 */
+	{
+		0x0002007F,		/* C25-C24 */
+		0x00490044,		/* C27-C26 */
+		0x00000000,		/* C29-C28 */
+		0x00000000		/* C31-C30 */
+	},
+	/* PDSP3: C24-C31 */
+	{
+		0x0003007F,		/* C25-C24 */
+		0x0000006E,		/* C27-C26 */
+		0x00000000,		/* C29-C28 */
+		0x00000000		/* C31-C30 */
+	},
+	/* PDSP4: C24-C31 */
+	{
+		0x0070007F,		/* C25-C24 */
+		0x00000000,		/* C27-C26 */
+		0x04080404,		/* C29-C28 */
+		0x00000000		/* C31-C30 */
+	},
+	/* PDSP5: C24-C31 */
+	{
+		0x0071007F,		/* C25-C24 */
+		0x00000000,		/* C27-C26 */
+		0x04080404,		/* C29-C28 */
+		0x00000000		/* C31-C30 */
+	}
+};
+
+struct pa_mailbox_regs {
+	u32 pdsp_mailbox_slot0;
+	u32 pdsp_mailbox_slot1;
+	u32 pdsp_mailbox_slot2;
+	u32 pdsp_mailbox_slot3;
+};
+
+struct pa_packet_id_alloc_regs {
+	u32 revision;
+	u32 soft_reset;
+	u32 range_limit;
+	u32 idvalue;
+};
+
+struct pa_lut2_control_regs {
+	u32 revision;
+	u32 soft_reset;
+	u32 rsvd[6];
+	u32 add_data0;
+	u32 add_data1;
+	u32 add_data2;
+	u32 add_data3;
+	u32 add_del_key;
+	u32 add_del_control;
+};
+
+struct pa_pdsp_control_regs {
+	u32 control;
+	u32 status;
+	u32 wakeup_enable;
+	u32 cycle_count;
+	u32 stall_count;
+	u32 rsvd[3];
+	u32 const_tbl_blk_index0;
+	u32 const_tbl_blk_index1;
+	u32 const_tbl_prog_pointer0;
+	u32 const_tbl_prog_pointer1;
+	u32 rsvd1[52];
+};
+
+struct pa_pdsp_timer_regs {
+	u32 timer_control;
+	u32 timer_load;
+	u32 timer_value;
+	u32 timer_interrupt;
+	u32 rsvd[60];
+};
+
+struct pa_statistics_regs {
+	u32 revision;
+	u32 soft_reset;
+	u32 incr_flags;
+	u32 stats_capture;
+	u32 rsvd[4];
+	u32 stats_red[32];
+};
+
+#define	CSUM_OFFLOAD_NONE	0
+#define	CSUM_OFFLOAD_HARD	1
+#define	CSUM_OFFLOAD_SOFT	2
+
+#define	PA_TXHOOK_ORDER	10
+#define	PA_RXHOOK_ORDER	10
+
+static DEFINE_MUTEX(pa_modules_lock);
+
+struct pa_lut_entry {
+	int			index;
+	bool			valid, in_use;
+	struct netcp_addr	*naddr;
+};
+
+struct pa_timestamp_info {
+	u32	mult;
+	u32	shift;
+	u64	system_offset;
+};
+
+struct pa_intf {
+	struct pa_device		*pa_device;
+	struct net_device		*net_device;
+	struct netcp_tx_pipe		 tx_pipe;
+	unsigned			 data_flow_num;
+	unsigned			 data_queue_num;
+	u32				 saved_ss_state;
+	char				 tx_chan_name[24];
+
+	bool				 tx_timestamp_enable;
+	bool				 rx_timestamp_enable;
+};
+
+struct pa_device {
+	struct netcp_device		*netcp_device;
+	struct device			*dev;
+	struct clk			*clk;
+	struct dma_chan			*pdsp0_tx_channel;
+	struct dma_chan			*rx_channel;
+	const char			*rx_chan_name;
+	unsigned			 cmd_flow_num;
+	unsigned			 cmd_queue_num;
+
+	struct pa_timestamp_info	timestamp_info;
+
+	struct pa_mailbox_regs __iomem		*reg_mailbox;
+	struct pa_packet_id_alloc_regs __iomem	*reg_packet_id;
+	struct pa_lut2_control_regs __iomem	*reg_lut2;
+	struct pa_pdsp_control_regs __iomem	*reg_control;
+	struct pa_pdsp_timer_regs   __iomem	*reg_timer;
+	struct pa_statistics_regs   __iomem	*reg_stats;
+	void __iomem				*pa_sram;
+	void __iomem				*pa_iram;
+
+
+	u8				*mc_list;
+	u8				 addr_count;
+	struct tasklet_struct		 task;
+	spinlock_t			 lock;
+
+	u32				 tx_cmd_queue_depth;
+	u32				 tx_data_queue_depth;
+	u32				 rx_pool_depth;
+	u32				 rx_buffer_size;
+	u32				 csum_offload;
+	u32				 txhook_order;
+	u32				 txhook_softcsum;
+	u32				 rxhook_order;
+	u32				 multi_if;
+	u32				 inuse_if_count;
+	u32				 lut_inuse_count;
+	struct pa_lut_entry		 *lut;
+	u32				 lut_size;
+
+	u32				 ts_not_req;
+};
+
+#define pa_from_module(data)	container_of(data, struct pa_device, module)
+#define pa_to_module(pa)	(&(pa)->module)
+
+struct pa_packet {
+	struct scatterlist		 sg[PA_SGLIST_SIZE];
+	int				 sg_ents;
+	struct pa_device		*priv;
+	struct dma_chan			*chan;
+	struct dma_async_tx_descriptor	*desc;
+	dma_cookie_t			 cookie;
+	u32				 epib[4];
+	u32				 psdata[6];
+	struct completion		 complete;
+	void				*data;
+};
+
+static void pdsp_fw_put(u32 *dest, const u32 *src, u32 wc)
+{
+	int i;
+
+	for (i = 0; i < wc; i++)
+		*dest++ = be32_to_cpu(*src++);
+}
+
+static inline void swizFwd (struct pa_frm_forward *fwd)
+{
+	fwd->flow_id = fwd->flow_id;
+	fwd->queue   = cpu_to_be16(fwd->queue);
+
+	if (fwd->forward_type == PAFRM_FORWARD_TYPE_HOST) {
+		fwd->u.host.context      = cpu_to_be32(fwd->u.host.context);
+		fwd->u.host.multi_route  = fwd->u.host.multi_route;
+		fwd->u.host.multi_idx    = fwd->u.host.multi_idx;
+		fwd->u.host.pa_pdsp_router = fwd->u.host.pa_pdsp_router;
+	} else if (fwd->forward_type == PAFRM_FORWARD_TYPE_SA) {
+		fwd->u.sa.sw_info_0 = cpu_to_be32(fwd->u.sa.sw_info_0);
+		fwd->u.sa.sw_info_1 = cpu_to_be32(fwd->u.sa.sw_info_1);
+	} else if (fwd->forward_type == PAFRM_FORWARD_TYPE_SRIO) {
+		fwd->u.srio.ps_info0 = cpu_to_be32(fwd->u.srio.ps_info0);
+		fwd->u.srio.ps_info1 = cpu_to_be32(fwd->u.srio.ps_info1);
+		fwd->u.srio.pkt_type = fwd->u.srio.pkt_type;
+	} else if (fwd->forward_type == PAFRM_FORWARD_TYPE_ETH) {
+		fwd->u.eth.ps_flags	= fwd->u.eth.ps_flags;
+	} else if (fwd->forward_type == PAFRM_FORWARD_TYPE_PA) {
+		fwd->u.pa.pa_dest	= fwd->u.pa.pa_dest;
+		fwd->u.pa.custom_type	= fwd->u.pa.custom_type;
+		fwd->u.pa.custom_idx	= fwd->u.pa.custom_idx;
+	}
+
+	fwd->forward_type = fwd->forward_type;
+}
+
+static inline void swizFcmd (struct pa_frm_command *fcmd)
+{
+	fcmd->command_result =  cpu_to_be32(fcmd->command_result);
+	fcmd->command	     =  fcmd->command;
+	fcmd->magic          =  fcmd->magic;
+	fcmd->com_id         =  cpu_to_be16(fcmd->com_id);
+	fcmd->ret_context    =  cpu_to_be32(fcmd->ret_context);
+	fcmd->reply_queue    =  cpu_to_be16(fcmd->reply_queue);
+	fcmd->reply_dest     =  fcmd->reply_dest;
+	fcmd->flow_id        =  fcmd->flow_id;
+}
+
+static inline void swizAl1 (struct pa_frm_cmd_add_lut1 *al1)
+{
+	al1->index         =  al1->index;
+	al1->type          =  al1->type;
+	al1->cust_index    =  al1->cust_index;
+
+	if (al1->type == PAFRM_COM_ADD_LUT1_STANDARD) {
+		al1->u.eth_ip.etype = cpu_to_be16(al1->u.eth_ip.etype);
+		al1->u.eth_ip.vlan  = cpu_to_be16(al1->u.eth_ip.vlan);
+		al1->u.eth_ip.spi   = cpu_to_be32(al1->u.eth_ip.spi);
+		al1->u.eth_ip.flow  = cpu_to_be32(al1->u.eth_ip.flow);
+
+		if (al1->u.eth_ip.key & PAFRM_LUT1_KEY_MPLS)
+			al1->u.eth_ip.pm.mpls     =  cpu_to_be32(al1->u.eth_ip.pm.mpls);
+		else {
+			al1->u.eth_ip.pm.ports[0] =  cpu_to_be16(al1->u.eth_ip.pm.ports[0]);
+			al1->u.eth_ip.pm.ports[1] =  cpu_to_be16(al1->u.eth_ip.pm.ports[1]);
+		}
+
+		al1->u.eth_ip.proto_next  =  al1->u.eth_ip.proto_next;
+		al1->u.eth_ip.tos_tclass  =  al1->u.eth_ip.tos_tclass;
+		al1->u.eth_ip.inport      =  al1->u.eth_ip.inport;
+		al1->u.eth_ip.key         =  al1->u.eth_ip.key;
+		al1->u.eth_ip.match_flags =  cpu_to_be16(al1->u.eth_ip.match_flags);
+	} else if (al1->type == PAFRM_COM_ADD_LUT1_SRIO) {
+		al1->u.srio.src_id	= cpu_to_be16(al1->u.srio.src_id);
+		al1->u.srio.dest_id	= cpu_to_be16(al1->u.srio.dest_id);
+		al1->u.srio.etype	= cpu_to_be16(al1->u.srio.etype);
+		al1->u.srio.vlan	= cpu_to_be16(al1->u.srio.vlan);
+		al1->u.srio.pri         = al1->u.srio.pri;
+		al1->u.srio.type_param1 = cpu_to_be16(al1->u.srio.type_param1);
+		al1->u.srio.type_param2 = al1->u.srio.type_param2;
+		al1->u.srio.key         = al1->u.srio.key;
+		al1->u.srio.match_flags = cpu_to_be16(al1->u.srio.match_flags);
+		al1->u.srio.next_hdr    = al1->u.srio.next_hdr;
+		al1->u.srio.next_hdr_offset = cpu_to_be16(al1->u.srio.next_hdr_offset);
+	} else {
+		al1->u.custom.etype		=  cpu_to_be16(al1->u.custom.etype);
+		al1->u.custom.vlan		=  cpu_to_be16(al1->u.custom.vlan);
+		al1->u.custom.key		=  al1->u.custom.key;
+		al1->u.custom.match_flags	=  cpu_to_be16(al1->u.custom.match_flags);
+	}
+
+	swizFwd(&(al1->match));
+	swizFwd(&(al1->next_fail));
+}
+
+static int pa_conv_routing_info(struct	pa_frm_forward *fwd_info,
+			 struct	pa_route_info *route_info,
+			 int cmd_dest, u16 fail_route)
+{
+	u8 *pcmd = NULL;
+	fwd_info->flow_id = route_info->flow_id;
+	fwd_info->queue   = route_info->queue;
+
+	if (route_info->dest == PA_DEST_HOST) {
+		fwd_info->forward_type   = PAFRM_FORWARD_TYPE_HOST;
+		fwd_info->u.host.context = route_info->sw_info_0;
+
+		if (route_info->m_route_index >= 0) {
+			if (route_info->m_route_index >= PA_MAX_MULTI_ROUTE_SETS) {
+				return (PA_ERR_CONFIG);
+			}
+
+			fwd_info->u.host.multi_route	= 1;
+			fwd_info->u.host.multi_idx	= route_info->m_route_index;
+			fwd_info->u.host.pa_pdsp_router	= PAFRM_DEST_PA_M_0;
+		}
+		pcmd = fwd_info->u.host.cmd;
+	} else if (route_info->dest == PA_DEST_DISCARD)	{
+		fwd_info->forward_type = PAFRM_FORWARD_TYPE_DISCARD;
+	} else if (route_info->dest == PA_DEST_EMAC) {
+		fwd_info->forward_type = PAFRM_FORWARD_TYPE_ETH;
+		fwd_info->u.eth.ps_flags = (route_info->pkt_type_emac_ctrl &
+					    PA_EMAC_CTRL_CRC_DISABLE)?
+			PAFRM_ETH_PS_FLAGS_DISABLE_CRC:0;
+		fwd_info->u.eth.ps_flags |= ((route_info->pkt_type_emac_ctrl &
+					      PA_EMAC_CTRL_PORT_MASK) <<
+					     PAFRM_ETH_PS_FLAGS_PORT_SHIFT);
+	} else if (fail_route) {
+		return (PA_ERR_CONFIG);
+
+	} else if (((route_info->dest == PA_DEST_CONTINUE_PARSE_LUT1) &&
+		    (route_info->custom_type != PA_CUSTOM_TYPE_LUT2)) ||
+		   ((route_info->dest == PA_DEST_CONTINUE_PARSE_LUT2) &&
+		    (route_info->custom_type != PA_CUSTOM_TYPE_LUT1))) {
+
+		/* Custom Error check */
+		if (((route_info->custom_type == PA_CUSTOM_TYPE_LUT1) &&
+		     (route_info->custom_index >= PA_MAX_CUSTOM_TYPES_LUT1)) ||
+		    ((route_info->custom_type == PA_CUSTOM_TYPE_LUT2) &&
+		     (route_info->custom_index >= PA_MAX_CUSTOM_TYPES_LUT2)))
+			return(PA_ERR_CONFIG);
+
+		fwd_info->forward_type = PAFRM_FORWARD_TYPE_PA;
+		fwd_info->u.pa.custom_type = (u8)route_info->custom_type;
+		fwd_info->u.pa.custom_idx  = route_info->custom_index;
+
+		if (route_info->dest == PA_DEST_CONTINUE_PARSE_LUT2) {
+			fwd_info->u.pa.pa_dest = PAFRM_DEST_PA_C2;
+		} else {
+			/*
+			 * cmd_dest is provided by calling function
+			 * There is no need to check error case
+			 */
+			fwd_info->u.pa.pa_dest = (cmd_dest == PA_CMD_TX_DEST_0)?
+				PAFRM_DEST_PA_C1_1:PAFRM_DEST_PA_C1_2;
+		}
+	} else if (route_info->dest == PA_DEST_SASS) {
+		fwd_info->forward_type   = PAFRM_FORWARD_TYPE_SA;
+		fwd_info->u.sa.sw_info_0 = route_info->sw_info_0;
+		fwd_info->u.sa.sw_info_1 = route_info->sw_info_1;
+		pcmd = fwd_info->u.sa.cmd;
+	} else if (route_info->dest == PA_DEST_SRIO) {
+		fwd_info->forward_type		= PAFRM_FORWARD_TYPE_SRIO;
+		fwd_info->u.srio.ps_info0	= route_info->sw_info_0;
+		fwd_info->u.srio.ps_info1	= route_info->sw_info_1;
+		fwd_info->u.srio.pkt_type	= route_info->pkt_type_emac_ctrl;
+	} else {
+		return (PA_ERR_CONFIG);
+	}
+
+	if (pcmd && route_info->pcmd) {
+		struct pa_cmd_info *pacmd = route_info->pcmd;
+		struct pa_patch_info *patch_info;
+		struct pa_cmd_set *cmd_set;
+
+		switch (pacmd->cmd) {
+		case PA_CMD_PATCH_DATA:
+			patch_info = &pacmd->params.patch;
+			if ((patch_info->n_patch_bytes > 2) ||
+			    (patch_info->overwrite) ||
+			    (patch_info->patch_data == NULL))
+				return (PA_ERR_CONFIG);
+
+			pcmd[0] = PAFRM_RX_CMD_CMDSET;
+			pcmd[1] = patch_info->n_patch_bytes;
+			pcmd[2] = patch_info->patch_data[0];
+			pcmd[3] = patch_info->patch_data[1];
+			break;
+
+		case PA_CMD_CMDSET:
+			cmd_set = &pacmd->params.cmd_set;
+			if(cmd_set->index >= PA_MAX_CMD_SETS)
+				return (PA_ERR_CONFIG);
+
+			pcmd[0] = PAFRM_RX_CMD_CMDSET;
+			pcmd[1] = (u8)cmd_set->index;
+			break;
+		default:
+			return(PA_ERR_CONFIG);
+		}
+	}
+	return (PA_OK);
+}
+
+static int keystone_pa_reset(struct pa_device *pa_dev)
+{
+	struct pa_packet_id_alloc_regs __iomem	*packet_id_regs = pa_dev->reg_packet_id;
+	struct pa_lut2_control_regs __iomem	*lut2_regs = pa_dev->reg_lut2;
+	struct pa_statistics_regs   __iomem	*stats_regs = pa_dev->reg_stats;
+	u32 i;
+
+	/* Reset and disable all PDSPs */
+	for (i = 0; i < DEVICE_PA_NUM_PDSPS; i++) {
+		struct pa_pdsp_control_regs __iomem *ctrl_reg = &pa_dev->reg_control[i];
+		__raw_writel(PA_REG_VAL_PDSP_CTL_RESET_PDSP,
+			     &ctrl_reg->control);
+
+		while((__raw_readl(&ctrl_reg->control)
+		       & PA_REG_VAL_PDSP_CTL_STATE));
+	}
+
+	/* Reset packet Id */
+	__raw_writel(1, &packet_id_regs->soft_reset);
+
+	/* Reset LUT2 */
+	__raw_writel(1, &lut2_regs->soft_reset);
+
+	/* Reset statistic */
+	__raw_writel(1, &stats_regs->soft_reset);
+
+	/* Reset timers */
+	for (i = 0; i < DEVICE_PA_NUM_PDSPS; i++) {
+		struct pa_pdsp_timer_regs __iomem *timer_reg = &pa_dev->reg_timer[i];
+		__raw_writel(0, &timer_reg->timer_control);
+	}
+
+	return 0;
+}
+
+/*
+ *  Convert a raw PA timer count to nanoseconds
+ */
+static inline u64 tstamp_raw_to_ns(struct pa_device *pa_dev, u64 raw)
+{
+	return (raw * pa_dev->timestamp_info.mult)
+		>> pa_dev->timestamp_info.shift;
+}
+
+static u64 pa_to_sys_time(struct pa_device *pa_dev, u64 pa_ticks)
+{
+	s64 temp;
+	u64 result;
+
+	/* we need to compute difference from wallclock
+	*  to time from boot dynamically since
+	*  it will change whenever absolute time is adjusted by
+	*  protocols above (ntp, ptpv2)
+	*/
+
+	temp = ktime_to_ns(ktime_get_monotonic_offset());
+	result = (u64)((s64)pa_dev->timestamp_info.system_offset - temp +
+			(s64)tstamp_raw_to_ns(pa_dev, pa_ticks));
+
+	return result;
+}
+
+static inline u64 tstamp_get_raw(struct pa_device *pa_dev)
+{
+	struct pa_pdsp_timer_regs __iomem *timer_reg = &pa_dev->reg_timer[0];
+	u32 low, high, high2;
+	u64 raw;
+	int count;
+
+	count = 0;
+	do {
+		high  = __raw_readl(pa_dev->pa_sram + 0x6460);
+		low   = __raw_readl(&timer_reg->timer_value);
+		high2 = __raw_readl(pa_dev->pa_sram + 0x6460);
+	} while((high != high2) && (++count < 32));
+
+	raw = (((u64)high) << 16) | (u64)(0x0000ffff - (low & 0x0000ffff));
+
+	return raw;
+}
+
+/*
+ * calibrate the PA timer to the system time
+ * ktime_get gives montonic time 
+ * ktime_to_ns converts ktime to ns
+ * this needs to be called before doing conversions
+ */
+static void pa_calibrate_with_system_timer(struct pa_device *pa_dev)
+{
+	ktime_t ktime1, ktime2;
+	u64 pa_ticks;
+	u64 pa_ns;
+	u64 sys_ns1, sys_ns2;
+
+	/* Get the two values with minimum delay between */
+	ktime1 = ktime_get();
+	pa_ticks = tstamp_get_raw(pa_dev);
+	ktime2 = ktime_get();
+
+	/* Convert both values to nanoseconds */
+	sys_ns1 = ktime_to_ns(ktime1);
+	pa_ns   = tstamp_raw_to_ns(pa_dev, pa_ticks);
+	sys_ns2 = ktime_to_ns(ktime2);
+
+	/* compute offset */
+	pa_dev->timestamp_info.system_offset = sys_ns1 +
+		((sys_ns2 - sys_ns1) / 2) - pa_ns;
+}
+
+static void pa_get_version(struct pa_device *pa_dev)
+{
+	u32 version;
+
+	version = __raw_readl(pa_dev->pa_sram + PAFRM_PDSP_VERSION_BASE);
+
+	dev_info(pa_dev->dev, "Using Packet Accelerator Firmware version "
+				"0x%08x\n", version);
+}
+
+static int pa_pdsp_run(struct pa_device *pa_dev, int pdsp)
+{
+	struct pa_pdsp_control_regs __iomem *ctrl_reg = &pa_dev->reg_control[pdsp];
+	struct pa_mailbox_regs __iomem *mailbox_reg = &pa_dev->reg_mailbox[pdsp];
+	u32 i, v;
+
+	/* Check for enabled PDSP */
+	v = __raw_readl(&ctrl_reg->control);
+	if ((v & PA_REG_VAL_PDSP_CTL_ENABLE) ==
+	    PA_REG_VAL_PDSP_CTL_ENABLE) {
+		/* Already enabled */
+		return (PA_PDSP_ALREADY_ACTIVE);
+	}
+
+	/* Clear the mailbox */
+	__raw_writel(0, &mailbox_reg->pdsp_mailbox_slot0);
+
+	/* Set PDSP PC to 0, enable the PDSP */
+	__raw_writel(PA_REG_VAL_PDSP_CTL_ENABLE |
+		     PA_REG_VAL_PDSP_CTL_SOFT_RESET,
+		     &ctrl_reg->control);
+
+	/* Wait for the mailbox to become non-zero */
+	for (i = 0; i < PA_MAX_PDSP_ENABLE_LOOP_COUNT; i++)
+		v = __raw_readl(&mailbox_reg->pdsp_mailbox_slot0);
+		if (v != 0)
+			return (PA_PDSP_RESET_RELEASED);
+
+	return (PA_PDSP_NO_RESTART);
+}
+
+static int keystone_pa_reset_control(struct pa_device *pa_dev, int new_state)
+{
+	struct pa_mailbox_regs __iomem *mailbox_reg = &pa_dev->reg_mailbox[0];
+	int do_global_reset = 1;
+	int i, res;
+	int ret;
+
+	if (new_state == PA_STATE_ENABLE) {
+		ret = PA_STATE_ENABLE;
+
+		/*
+		 * Do nothing if a pdsp is already out of reset.
+		 * If any PDSPs are out of reset
+		 * a global init is not performed
+		 */
+		for (i = 0; i < 6; i++) {
+			res = pa_pdsp_run(pa_dev, i);
+
+			if (res == PA_PDSP_ALREADY_ACTIVE)
+				do_global_reset = 0;
+
+			if (res == PA_PDSP_NO_RESTART) {
+				ret = PA_STATE_ENABLE_FAILED;
+				do_global_reset = 0;
+			}
+		}
+
+		/* If global reset is required any PDSP can do it */
+		if (do_global_reset) {
+			__raw_writel(1, &mailbox_reg->pdsp_mailbox_slot1);
+			__raw_writel(0, &mailbox_reg->pdsp_mailbox_slot0);
+
+			while (__raw_readl(&mailbox_reg->pdsp_mailbox_slot1) != 0);
+
+			for (i = 1; i < 6; i++) {
+				struct pa_mailbox_regs __iomem *mbox_reg =
+					&pa_dev->reg_mailbox[i];
+				__raw_writel(0,
+					     &mbox_reg->pdsp_mailbox_slot0);
+			}
+		} else {
+			for (i = 0; i < 6; i++) {
+				struct pa_mailbox_regs __iomem *mbox_reg =
+					&pa_dev->reg_mailbox[i];
+				__raw_writel(0,
+					     &mbox_reg->pdsp_mailbox_slot0);
+			}
+
+		}
+
+		return (ret);
+	}
+
+	return (PA_STATE_INVALID_REQUEST);
+}
+
+static int keystone_pa_set_firmware(struct pa_device *pa_dev,
+			     int pdsp, const unsigned int *buffer, int len)
+{
+	struct pa_pdsp_control_regs __iomem *ctrl_reg = &pa_dev->reg_control[pdsp];
+
+	if ((pdsp < 0) || (pdsp >= DEVICE_PA_NUM_PDSPS))
+		return -EINVAL;
+
+	pdsp_fw_put((u32 *)(pa_dev->pa_iram + PA_MEM_PDSP_IRAM(pdsp)), buffer,
+		    len >> 2);
+
+	__raw_writel(pap_pdsp_const_reg_map[pdsp][PA_PDSP_CONST_REG_INDEX_C25_C24],
+		     &ctrl_reg->const_tbl_blk_index0);
+
+	__raw_writel(pap_pdsp_const_reg_map[pdsp][PA_PDSP_CONST_REG_INDEX_C27_C26],
+		     &ctrl_reg->const_tbl_blk_index1);
+
+	__raw_writel(pap_pdsp_const_reg_map[pdsp][PA_PDSP_CONST_REG_INDEX_C29_C28],
+		     &ctrl_reg->const_tbl_prog_pointer0);
+
+	__raw_writel(pap_pdsp_const_reg_map[pdsp][PA_PDSP_CONST_REG_INDEX_C31_C30],
+		     &ctrl_reg->const_tbl_prog_pointer1);
+
+	return 0;
+}
+
+static struct pa_packet *pa_alloc_packet(struct pa_device *pa_dev,
+					 unsigned cmd_size,
+					 struct dma_chan *dma_chan)
+{
+	struct pa_packet *p_info;
+
+	p_info = kzalloc(sizeof(*p_info) + cmd_size, GFP_KERNEL);
+	if (!p_info)
+		return NULL;
+
+	p_info->priv = pa_dev;
+	p_info->data = p_info + 1;
+	p_info->chan = dma_chan;
+
+	sg_init_table(p_info->sg, PA_SGLIST_SIZE);
+	sg_set_buf(&p_info->sg[0], p_info->epib, sizeof(p_info->epib));
+	sg_set_buf(&p_info->sg[1], p_info->psdata, sizeof(p_info->psdata));
+	sg_set_buf(&p_info->sg[2], p_info->data, cmd_size);
+
+	return p_info;
+}
+
+static void pa_tx_dma_callback(void *data)
+{
+	struct pa_packet *p_info = data;
+	struct pa_device *pa_dev = p_info->priv;
+	enum dma_status status;
+	unsigned long irqsave;
+	dma_cookie_t cookie;
+
+	spin_lock_irqsave(&pa_dev->lock, irqsave);
+	cookie = p_info->cookie;
+	spin_unlock_irqrestore(&pa_dev->lock, irqsave);
+
+	if (unlikely(cookie <= 0))
+		WARN(1, "invalid dma cookie == %d", cookie);
+	else {
+		status = dma_async_is_tx_complete(p_info->chan,
+						  cookie, NULL, NULL);
+		WARN((status != DMA_SUCCESS),
+				"dma completion failure, status == %d", status);
+	}
+
+	dma_unmap_sg(pa_dev->dev, &p_info->sg[2], 1, DMA_TO_DEVICE);
+
+	p_info->desc = NULL;
+
+	kfree(p_info);
+}
+
+static int pa_submit_tx_packet(struct pa_packet *p_info)
+{
+	unsigned flags = DMA_HAS_EPIB | DMA_HAS_PSINFO;
+	struct pa_device *pa_dev = p_info->priv;
+	unsigned long irqsave;
+	int ret;
+
+	ret = dma_map_sg(pa_dev->dev, &p_info->sg[2], 1, DMA_TO_DEVICE);
+	if (ret < 0)
+		return ret;
+
+	p_info->desc = dmaengine_prep_slave_sg(p_info->chan, p_info->sg, 3,
+					       DMA_TO_DEVICE, flags);
+	if (IS_ERR_OR_NULL(p_info->desc)) {
+		dma_unmap_sg(pa_dev->dev, &p_info->sg[2], 1, DMA_TO_DEVICE);
+		return PTR_ERR(p_info->desc);
+	}
+
+	p_info->desc->callback = pa_tx_dma_callback;
+	p_info->desc->callback_param = p_info;
+
+	spin_lock_irqsave(&pa_dev->lock, irqsave);
+	p_info->cookie = dmaengine_submit(p_info->desc);
+	spin_unlock_irqrestore(&pa_dev->lock, irqsave);
+
+	return dma_submit_error(p_info->cookie) ? p_info->cookie : 0;
+}
+
+#define	PA_CONTEXT_MASK		0xffff0000
+#define	PA_CONTEXT_CONFIG	0xdead0000
+#define	PA_CONTEXT_TSTAMP	0xbeef0000
+
+#define	TSTAMP_TIMEOUT	(HZ * 5)	/* 5 seconds (arbitrary) */
+
+struct tstamp_pending {
+	struct list_head	 list;
+	u32			 context;
+	struct sock		*sock;
+	struct sk_buff		*skb;
+	struct pa_device	*pa_dev;
+	struct timer_list	 timeout;
+};
+
+static spinlock_t		 tstamp_lock;
+static atomic_t			 tstamp_sequence = ATOMIC_INIT(0);
+static struct list_head		 tstamp_pending = LIST_HEAD_INIT(tstamp_pending);
+
+static struct tstamp_pending *tstamp_remove_pending(u32 context)
+{
+	struct tstamp_pending	*pend;
+
+	spin_lock(&tstamp_lock);
+	list_for_each_entry(pend, &tstamp_pending, list) {
+		if (pend->context == context) {
+			del_timer(&pend->timeout);
+			list_del(&pend->list);
+			spin_unlock(&tstamp_lock);
+			return pend;
+		}
+	}
+	spin_unlock(&tstamp_lock);
+
+	return NULL;
+}
+
+static void tstamp_complete(u32, struct pa_packet *);
+
+static void tstamp_purge_pending(struct pa_device *pa_dev)
+{
+	struct tstamp_pending	*pend;
+	int			 found;
+
+	/* This is ugly and inefficient, but very rarely executed */
+	do {
+		found = 0;
+
+		spin_lock(&tstamp_lock);
+		list_for_each_entry(pend, &tstamp_pending, list) {
+			if (pend->pa_dev == pa_dev) {
+				found = 1;
+				break;
+			}
+		}
+		spin_unlock(&tstamp_lock);
+
+		if (found)
+			tstamp_complete(pend->context, NULL);
+	} while(found);
+}
+
+static void tstamp_timeout(unsigned long context)
+{
+	tstamp_complete((u32)context, NULL);
+}
+
+static int tstamp_add_pending(struct tstamp_pending *pend)
+{
+	init_timer(&pend->timeout);
+	pend->timeout.expires = jiffies + TSTAMP_TIMEOUT;
+	pend->timeout.function = tstamp_timeout;
+	pend->timeout.data = (unsigned long)pend->context;
+
+	spin_lock(&tstamp_lock);
+	add_timer(&pend->timeout);
+	list_add_tail(&pend->list, &tstamp_pending);
+	spin_unlock(&tstamp_lock);
+
+	return 0;
+}
+
+static void tstamp_complete(u32 context, struct pa_packet *p_info)
+{
+	struct tstamp_pending	*pend;
+	struct sock_exterr_skb 	*serr;
+	struct sk_buff 		*skb;
+	struct skb_shared_hwtstamps *sh_hw_tstamps;
+	u64			 tx_timestamp;
+	u64			 sys_time;
+	int			 err;
+
+	pend = tstamp_remove_pending(context);
+	if (!pend)
+		return;
+
+
+	skb = pend->skb;
+	if (!p_info) {
+		dev_warn(pend->pa_dev->dev, "Timestamp completion timeout\n");
+		kfree_skb(skb);
+	} else {
+		tx_timestamp = p_info->epib[0];
+		tx_timestamp |= ((u64)(p_info->epib[2] & 0x0000ffff)) << 32;
+
+		sys_time = pa_to_sys_time(pend->pa_dev, tx_timestamp);
+
+		sh_hw_tstamps = skb_hwtstamps(skb);
+		memset(sh_hw_tstamps, 0, sizeof(*sh_hw_tstamps));
+		sh_hw_tstamps->hwtstamp =
+			ns_to_ktime(tstamp_raw_to_ns(pend->pa_dev,
+							tx_timestamp));
+		sh_hw_tstamps->syststamp = ns_to_ktime(sys_time);
+
+		serr = SKB_EXT_ERR(skb);
+		memset(serr, 0, sizeof(*serr));
+		serr->ee.ee_errno = ENOMSG;
+		serr->ee.ee_origin = SO_EE_ORIGIN_TIMESTAMPING;
+
+		err = sock_queue_err_skb(pend->sock, skb);
+		if (err)
+			kfree_skb(skb);
+	}
+
+	kfree(pend);
+}
+
+static void pa_rx_complete(void *param)
+{
+	struct pa_packet *p_info = param;
+	struct pa_device *pa_dev = p_info->priv;
+	struct pa_frm_command *fcmd;
+
+	dma_unmap_sg(pa_dev->dev, &p_info->sg[2], 1, DMA_FROM_DEVICE);
+
+	switch (p_info->epib[1] & PA_CONTEXT_MASK) {
+	case PA_CONTEXT_CONFIG:
+		fcmd = p_info->data;
+		swizFcmd(fcmd);
+
+		if (fcmd->command_result != PAFRM_COMMAND_RESULT_SUCCESS) {
+			dev_dbg(pa_dev->dev, "Command Result = 0x%x\n", fcmd->command_result);
+			dev_dbg(pa_dev->dev, "Command = 0x%x\n", fcmd->command);
+			dev_dbg(pa_dev->dev, "Magic = 0x%x\n", fcmd->magic);
+			dev_dbg(pa_dev->dev, "Com ID = 0x%x\n", fcmd->com_id);
+			dev_dbg(pa_dev->dev, "ret Context = 0x%x\n", fcmd->ret_context);
+			dev_dbg(pa_dev->dev, "Flow ID = 0x%x\n", fcmd->flow_id);
+			dev_dbg(pa_dev->dev, "reply Queue = 0x%x\n", fcmd->reply_queue);
+			dev_dbg(pa_dev->dev, "reply dest = 0x%x\n", fcmd->reply_dest);
+		}
+		dev_dbg(pa_dev->dev, "command response complete\n");
+		break;
+
+	case PA_CONTEXT_TSTAMP:
+		tstamp_complete(p_info->epib[1], p_info);
+		break;
+
+	default:
+		dev_warn(pa_dev->dev, "bad response context, got 0x%08x\n", p_info->epib[1]);
+		break;
+	}
+
+	p_info->desc = NULL;
+	kfree(p_info);
+}
+
+/* Release a free receive buffer */
+static void pa_rxpool_free(void *arg, unsigned q_num, unsigned bufsize,
+		struct dma_async_tx_descriptor *desc)
+{
+	struct pa_device *pa_dev = arg;
+	struct pa_packet *p_info = desc->callback_param;
+
+	dma_unmap_sg(pa_dev->dev, &p_info->sg[2], 1, DMA_FROM_DEVICE);
+
+	p_info->desc = NULL;
+
+	kfree(p_info);
+}
+
+static void pa_chan_work_handler(unsigned long data)
+{
+	struct pa_device *pa_dev = (struct pa_device *)data;
+
+	dma_poll(pa_dev->rx_channel, -1);
+
+	dma_rxfree_refill(pa_dev->rx_channel);
+
+	dmaengine_resume(pa_dev->rx_channel);
+}
+
+static void pa_chan_notify(struct dma_chan *dma_chan, void *arg)
+{
+	struct pa_device *pa_dev = arg;
+
+	dmaengine_pause(pa_dev->rx_channel);
+
+	tasklet_schedule(&pa_dev->task);
+
+	return;
+}
+
+/* Allocate a free receive buffer */
+static struct dma_async_tx_descriptor *pa_rxpool_alloc(void *arg,
+		unsigned q_num, unsigned bufsize)
+{
+	struct pa_device *pa_dev = arg;
+	struct dma_async_tx_descriptor *desc;
+	struct dma_device *device;
+	u32 err = 0;
+
+	struct pa_packet *rx;
+
+	rx = pa_alloc_packet(pa_dev, bufsize, pa_dev->rx_channel);
+	if (!rx) {
+		dev_err(pa_dev->dev, "could not allocate cmd rx packet\n");
+		kfree(rx);
+		return NULL;
+	}
+
+	rx->sg_ents = 2 + dma_map_sg(pa_dev->dev, &rx->sg[2],
+				1, DMA_FROM_DEVICE);
+	if (rx->sg_ents != 3) {
+		dev_err(pa_dev->dev, "dma map failed\n");
+
+		kfree(rx);
+		return NULL;
+	}
+
+	device = rx->chan->device;
+
+	desc = dmaengine_prep_slave_sg(rx->chan, rx->sg, 3, DMA_DEV_TO_MEM,
+				       DMA_HAS_EPIB | DMA_HAS_PSINFO);
+
+	if (IS_ERR_OR_NULL(desc)) {
+		dma_unmap_sg(pa_dev->dev, &rx->sg[2], 1, DMA_FROM_DEVICE);
+		kfree(rx);
+		err = PTR_ERR(desc);
+		if (err != -ENOMEM) {
+			dev_err(pa_dev->dev,
+				"dma prep failed, error %d\n", err);
+		}
+
+		return NULL;
+	}
+
+	desc->callback_param = rx;
+	desc->callback = pa_rx_complete;
+	rx->cookie = desc->cookie;
+
+	return desc;
+}
+
+static int keystone_pa_add_mac(struct pa_intf *pa_intf, int index,
+			       const u8 *smac, const u8 *dmac, int rule,
+			       unsigned etype, int port)
+{
+	struct pa_route_info route_info, fail_info;
+	struct pa_frm_command *fcmd;
+	struct pa_frm_cmd_add_lut1 *al1;
+	struct pa_packet *tx;
+	struct pa_device *priv = pa_intf->pa_device;
+	u32 context = PA_CONTEXT_CONFIG;
+	int size, ret;
+
+	dev_dbg(priv->dev, "add mac, index %d, smac %pM, dmac %pM, rule %d, type %x, port %d\n",
+		index, smac, dmac, rule, etype, port);
+
+	memset(&fail_info, 0, sizeof(fail_info));
+
+	memset(&route_info, 0, sizeof(route_info));
+
+	if (rule == PACKET_HST) {
+		route_info.dest			= PA_DEST_HOST;
+		route_info.flow_id		= pa_intf->data_flow_num;
+		route_info.queue		= pa_intf->data_queue_num;
+		route_info.m_route_index	= -1;
+		fail_info.dest			= PA_DEST_HOST;
+		fail_info.flow_id		= pa_intf->data_flow_num;
+		fail_info.queue			= pa_intf->data_queue_num;
+		fail_info.m_route_index		= -1;
+	} else if (rule == PACKET_PARSE) {
+		route_info.dest			= PA_DEST_CONTINUE_PARSE_LUT1;
+		route_info.m_route_index	= -1;
+		fail_info.dest			= PA_DEST_HOST;
+		fail_info.flow_id		= pa_intf->data_flow_num;
+		fail_info.queue			= pa_intf->data_queue_num;
+		fail_info.m_route_index		= -1;
+	} else if (rule == PACKET_DROP) {
+		route_info.dest			= PA_DEST_DISCARD;
+		route_info.m_route_index	= -1;
+		fail_info.dest			= PA_DEST_DISCARD;
+		fail_info.m_route_index		= -1;
+	}
+
+	size = (sizeof(struct pa_frm_command) +
+		sizeof(struct pa_frm_cmd_add_lut1) + 4);
+	tx = pa_alloc_packet(priv, size, priv->pdsp0_tx_channel);
+	if (!tx) {
+		dev_err(priv->dev, "could not allocate cmd tx packet\n");
+		return -ENOMEM;
+	}
+
+	fcmd = tx->data;
+	al1 = (struct pa_frm_cmd_add_lut1 *) &(fcmd->cmd);
+
+	fcmd->command_result	= 0;
+	fcmd->command		= PAFRM_CONFIG_COMMAND_ADDREP_LUT1;
+	fcmd->magic		= PAFRM_CONFIG_COMMAND_SEC_BYTE;
+	fcmd->com_id		= PA_COMID_L2;
+	fcmd->ret_context	= context;
+	fcmd->flow_id		= priv->cmd_flow_num;
+	fcmd->reply_queue	= priv->cmd_queue_num;
+	fcmd->reply_dest	= PAFRM_DEST_PKTDMA;
+
+	al1->index		= index;
+	al1->type		= PAFRM_COM_ADD_LUT1_STANDARD;
+
+	if (etype) {
+		al1->u.eth_ip.etype	= etype;
+		al1->u.eth_ip.match_flags |= PAFRM_LUT1_MATCH_ETYPE;
+	}
+
+	al1->u.eth_ip.vlan	= 0;
+	al1->u.eth_ip.pm.mpls	= 0;
+	if (port) {
+		al1->u.eth_ip.inport    = port;
+		al1->u.eth_ip.match_flags |= PAFRM_LUT1_MATCH_PORT;
+	}
+
+	if (dmac) {
+		al1->u.eth_ip.dmac[0] = dmac[0];
+		al1->u.eth_ip.dmac[1] = dmac[1];
+		al1->u.eth_ip.dmac[2] = dmac[2];
+		al1->u.eth_ip.dmac[3] = dmac[3];
+		al1->u.eth_ip.dmac[4] = dmac[4];
+		al1->u.eth_ip.dmac[5] = dmac[5];
+		al1->u.eth_ip.match_flags |= PAFRM_LUT1_MATCH_DMAC;
+	}
+	if (smac) {
+		al1->u.eth_ip.smac[0] = smac[0];
+		al1->u.eth_ip.smac[1] = smac[1];
+		al1->u.eth_ip.smac[2] = smac[2];
+		al1->u.eth_ip.smac[3] = smac[3];
+		al1->u.eth_ip.smac[4] = smac[4];
+		al1->u.eth_ip.smac[5] = smac[5];
+		al1->u.eth_ip.match_flags |= PAFRM_LUT1_MATCH_SMAC;
+	}
+
+	al1->u.eth_ip.key |= PAFRM_LUT1_KEY_MAC;
+	al1->u.eth_ip.match_flags |= PAFRM_LUT1_CUSTOM_MATCH_KEY;
+
+	ret = pa_conv_routing_info(&al1->match, &route_info, 0, 0);
+	if (ret != 0)
+		dev_err(priv->dev, "route info config failed\n");
+
+	ret = pa_conv_routing_info(&al1->next_fail, &fail_info, 0, 1);
+	if (ret != 0)
+		dev_err(priv->dev, "fail info config failed\n");
+
+	swizFcmd(fcmd);
+	swizAl1((struct pa_frm_cmd_add_lut1 *)&(fcmd->cmd));
+
+	tx->psdata[0] = ((u32)(4 << 5) << 24);
+
+	tx->epib[1] = 0x11112222;
+	tx->epib[2] = 0x33334444;
+	tx->epib[3] = 0;
+
+	pa_submit_tx_packet(tx);
+	dev_dbg(priv->dev, "waiting for command transmit complete\n");
+
+	return 0;
+}
+
+static void pa_init_crc_table4(u32 polynomial, u32 *crc_table4)
+{
+	int i, bit;
+
+	/* 16 values representing all possible 4-bit values */
+	for(i = 0; i < PARAM_CRC_TABLE_SIZE; i++) {
+		crc_table4[i] = i << 28;
+		for (bit = 0; bit < 4; bit++) {
+			/* If shifting out a zero, then just shift */
+			if (!(crc_table4[i] & 0x80000000))
+				crc_table4[i] = (crc_table4[i] << 1);
+			/* Else add in the polynomial as well */
+			else
+				crc_table4[i] = (crc_table4[i] << 1) ^ polynomial;
+		}
+		crc_table4[i] = cpu_to_be32(crc_table4[i]);
+	}
+}
+
+#define	CRC32C_POLYNOMIAL	0x1EDC6F41
+#define	SCTP_CRC_INITVAL	0xFFFFFFFF
+static int pa_config_crc_engine(struct pa_device *priv)
+{
+	struct pa_frm_command *fcmd;
+	struct pa_frm_config_crc *ccrc;
+	struct pa_packet *tx;
+	int size;
+
+	/* Verify that there is enough room to create the command */
+	size = sizeof(*fcmd) + sizeof(*ccrc) - sizeof(u32);
+	tx = pa_alloc_packet(priv, size, priv->pdsp0_tx_channel);
+	if (!tx) {
+		dev_err(priv->dev, "could not allocate cmd tx packet\n");
+		return -ENOMEM;
+	}
+
+	/* Create the command */
+	fcmd = tx->data;
+	fcmd->command_result	= 0;
+	fcmd->command		= PAFRM_CONFIG_COMMAND_CRC_ENGINE;
+	fcmd->magic		= PAFRM_CONFIG_COMMAND_SEC_BYTE;
+	fcmd->com_id		= 0;
+	fcmd->ret_context	= PA_CONTEXT_CONFIG;
+	fcmd->flow_id		= priv->cmd_flow_num;
+	fcmd->reply_queue	= priv->cmd_queue_num;
+	fcmd->reply_dest	= PAFRM_DEST_PKTDMA;
+	swizFcmd(fcmd);
+
+	ccrc = (struct pa_frm_config_crc *)&(fcmd->cmd);
+	ccrc->ctrl_bitmap  = PARAM_CRC_SIZE_32;
+	ccrc->ctrl_bitmap |= PARAM_CRC_CTRL_RIGHT_SHIFT;
+	ccrc->ctrl_bitmap |= PARAM_CRC_CTRL_INV_RESULT;
+	ccrc->init_val = cpu_to_be32(SCTP_CRC_INITVAL);
+
+	/* Magic polynomial value is CRC32c defined by RFC4960 */
+	pa_init_crc_table4(CRC32C_POLYNOMIAL, ccrc->crc_tbl);
+
+	tx->psdata[0] = ((u32)(4 << 5) << 24);
+
+	tx->epib[1] = 0x11112222;
+	tx->epib[2] = 0x33334444;
+	tx->epib[3] = 0;
+
+	pa_submit_tx_packet(tx);
+	dev_dbg(priv->dev, "waiting for command transmit complete\n");
+
+	return 0;
+}
+
+
+static int pa_fmtcmd_tx_csum(struct netcp_packet *p_info)
+{
+	struct sk_buff *skb = p_info->skb;
+	struct pasaho_com_chk_crc *ptx;
+	int start, len;
+	int size;
+
+	size = sizeof(*ptx);
+	ptx = (struct pasaho_com_chk_crc *)netcp_push_psdata(p_info, size);
+
+	start = skb_checksum_start_offset(skb);
+	len = skb->len - start;
+	if (len & 0x01) {
+		int err = skb_pad(skb, 1);
+		if (err < 0) {
+			if (unlikely(net_ratelimit())) {
+				dev_warn(p_info->netcp->dev,
+					"padding failed (%d), packet discarded\n",
+					err);
+			}
+			p_info->skb = NULL;
+			return err;
+		}
+		dev_dbg(p_info->netcp->dev, "padded packet to even length");
+		++len;
+	}
+
+	PASAHO_SET_CMDID(ptx, PASAHO_PAMOD_CMPT_CHKSUM);
+	PASAHO_CHKCRC_SET_START(ptx, start);
+	PASAHO_CHKCRC_SET_LEN(ptx, len);
+	PASAHO_CHKCRC_SET_RESULT_OFF(ptx, skb->csum_offset);
+	PASAHO_CHKCRC_SET_INITVAL(ptx, 0);
+	PASAHO_CHKCRC_SET_NEG0(ptx, 0);
+
+	return size;
+}
+
+static int pa_fmtcmd_tx_crc32c(struct netcp_packet *p_info)
+{
+	struct sk_buff *skb = p_info->skb;
+	struct pasaho_com_chk_crc *ptx;
+	int start, len;
+	int size;
+
+	size = sizeof(*ptx);
+	ptx = (struct pasaho_com_chk_crc *)netcp_push_psdata(p_info, size);
+
+	start = skb_checksum_start_offset(skb);
+	len = skb->len - start;
+
+	PASAHO_SET_CMDID             (ptx, PASAHO_PAMOD_CMPT_CRC);
+	PASAHO_CHKCRC_SET_START      (ptx, start);
+	PASAHO_CHKCRC_SET_LEN        (ptx, len);
+	PASAHO_CHKCRC_SET_CTRL       (ptx, PAFRM_CRC_FLAG_CRC_OFFSET_VALID);
+	PASAHO_CHKCRC_SET_RESULT_OFF (ptx, skb->csum_offset);
+
+	return size;
+}
+
+static int pa_fmtcmd_next_route(struct netcp_packet *p_info, const struct pa_cmd_next_route *route)
+{
+	struct pasaho_next_route	*nr;
+	int	size;
+	u16	pdest;
+
+	/* Make sure the destination is valid */
+	switch (route->dest) {
+	case PA_DEST_HOST:
+		pdest = PAFRM_DEST_PKTDMA;
+		break;
+	case PA_DEST_EMAC:
+		pdest = PAFRM_DEST_ETH;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	size = route->pkt_type_emac_ctrl ? sizeof(*nr) : (sizeof(*nr) - sizeof(nr->word1));
+	nr = (struct pasaho_next_route *)netcp_push_psdata(p_info, size);
+	if (!nr)
+		return -ENOMEM;
+
+	if (route->pkt_type_emac_ctrl) {
+		u8 ps_flags;
+		PASAHO_SET_E(nr, 1);
+
+		ps_flags = (route->pkt_type_emac_ctrl & PA_EMAC_CTRL_CRC_DISABLE) ?
+				PAFRM_ETH_PS_FLAGS_DISABLE_CRC : 0;
+
+		ps_flags |= ((route->pkt_type_emac_ctrl & PA_EMAC_CTRL_PORT_MASK) <<
+				PAFRM_ETH_PS_FLAGS_PORT_SHIFT);
+
+		PASAHO_SET_PKTTYPE(nr, ps_flags);
+	}
+
+	PASAHO_SET_CMDID(nr, PASAHO_PAMOD_NROUTE);
+	PASAHO_SET_DEST(nr, pdest);
+	PASAHO_SET_FLOW(nr, route->flow_id);
+	PASAHO_SET_QUEUE (nr, route->queue);
+
+	if (route->ctrl_bit_field & PA_NEXT_ROUTE_PROC_NEXT_CMD)
+		PASAHO_SET_N  (nr, 1);
+
+	nr->sw_info0 = route->sw_info_0;
+	nr->sw_info1 = route->sw_info_1;
+
+	return size;
+}
+
+static int pa_fmtcmd_tx_timestamp(struct netcp_packet *p_info, const struct pa_cmd_tx_timestamp *tx_ts)
+{
+	struct pasaho_report_timestamp	*rt_info;
+	int				 size;
+
+	size = sizeof(*rt_info);
+	rt_info = (struct pasaho_report_timestamp *)netcp_push_psdata(p_info, size);
+	if (!rt_info)
+		return -ENOMEM;
+
+	PASAHO_SET_CMDID(rt_info, PASAHO_PAMOD_REPORT_TIMESTAMP);
+	PASAHO_SET_REPORT_FLOW(rt_info, (u8)tx_ts->flow_id);
+	PASAHO_SET_REPORT_QUEUE(rt_info, tx_ts->dest_queue);
+	rt_info->sw_info0 = tx_ts->sw_info0;
+
+	return size;
+}
+
+static int pa_fmtcmd_align(struct netcp_packet *p_info, const unsigned bytes)
+{
+	struct pasaho_cmd_info	*paCmdInfo;
+	int i;
+
+	if ((bytes & 0x03) != 0)
+		return -EINVAL;
+
+	paCmdInfo = (struct pasaho_cmd_info *)netcp_push_psdata(p_info, bytes);
+
+	for (i = bytes/sizeof(u32); i > 0; --i ) {
+		PASAHO_SET_CMDID(paCmdInfo, PASAHO_PAMOD_DUMMY);
+		++paCmdInfo;
+	}
+
+	return bytes;
+}
+
+static inline int extract_l4_proto(struct netcp_packet *p_info)
+{
+	struct sk_buff *skb = p_info->skb;
+	int l4_proto = 0;
+	__be16 l3_proto;
+
+	l3_proto = skb->protocol;
+	if (l3_proto == __constant_htons(ETH_P_8021Q)) {
+		/* Can't use vlan_eth_hdr() here, skb->mac_header isn't valid */
+		struct vlan_ethhdr *vhdr = (struct vlan_ethhdr *)skb->data;
+		l3_proto = vhdr->h_vlan_encapsulated_proto;
+	}
+
+	switch (l3_proto) {
+	case __constant_htons(ETH_P_IP):
+		l4_proto = ip_hdr(skb)->protocol;
+		break;
+	case __constant_htons(ETH_P_IPV6):
+		l4_proto = ipv6_hdr(skb)->nexthdr;
+		break;
+	default:
+		if (unlikely(net_ratelimit())) {
+			dev_warn(p_info->netcp->dev,
+				 "partial checksum but L3 proto = 0x%04hx!\n",
+				 ntohs(l3_proto));
+		}
+	}
+
+	return l4_proto;
+}
+
+static int pa_tx_hook(int order, void *data, struct netcp_packet *p_info)
+{
+	struct pa_intf *pa_intf = data;
+	struct pa_device *pa_dev = pa_intf->pa_device;
+	struct netcp_priv *netcp_priv = netdev_priv(pa_intf->net_device);
+	struct sk_buff *skb = p_info->skb;
+	struct sock *sk = skb->sk;
+	struct pa_cmd_tx_timestamp tx_ts;
+	int size, total = 0;
+	struct pa_cmd_next_route route_cmd;
+	struct tstamp_pending *pend;
+
+	/* Generate the route_cmd */
+	memset(&route_cmd, 0, sizeof(route_cmd));
+	route_cmd.dest = PA_DEST_EMAC;
+	if (pa_dev->multi_if)
+		route_cmd.pkt_type_emac_ctrl = netcp_priv->cpsw_port;
+
+	/* Generate the next route command */
+	size = pa_fmtcmd_next_route(p_info, &route_cmd);
+	if (unlikely(size < 0))
+		return size;
+	total += size;
+
+	/* If TX Timestamp required, request it */
+	if (unlikely((skb_shinfo(p_info->skb)->tx_flags & SKBTX_HW_TSTAMP) &&
+		     p_info->skb->sk && pa_intf->tx_timestamp_enable &&
+		     need_timestamp(p_info->skb))) {
+		pend = kzalloc(sizeof(*pend), GFP_ATOMIC);
+		if (pend) {
+			if (!atomic_inc_not_zero(&sk->sk_refcnt))
+				return -ENODEV;
+			pend->skb = skb_clone(p_info->skb, GFP_ATOMIC);
+			if (!pend->skb) {
+				sock_put(sk);
+				kfree(pend);
+				return -ENOMEM;
+			} else {
+				pend->sock = p_info->skb->sk;
+				pend->pa_dev = pa_dev;
+				pend->context =  PA_CONTEXT_TSTAMP |
+					(~PA_CONTEXT_MASK &
+					 atomic_inc_return(&tstamp_sequence));
+				tstamp_add_pending(pend);
+
+				memset(&tx_ts, 0, sizeof(tx_ts));
+				tx_ts.dest_queue = pa_dev->cmd_queue_num;
+				tx_ts.flow_id    = pa_dev->cmd_flow_num;
+				tx_ts.sw_info0   = pend->context;
+
+				size = pa_fmtcmd_tx_timestamp(p_info,
+							      &tx_ts);
+				if (unlikely(size < 0))
+					return size;
+				total += size;
+			}
+		}
+	}
+
+	/* If checksum offload required, request it */
+	if ((skb->ip_summed == CHECKSUM_PARTIAL) &&
+	    (pa_dev->csum_offload == CSUM_OFFLOAD_HARD)) {
+		int l4_proto;
+
+		l4_proto = extract_l4_proto(p_info);
+		switch (l4_proto) {
+		case IPPROTO_TCP:
+		case IPPROTO_UDP:
+			size = pa_fmtcmd_tx_csum(p_info);
+			break;
+		case IPPROTO_SCTP:
+			size = pa_fmtcmd_tx_crc32c(p_info);
+			break;
+		default:
+			if (unlikely(net_ratelimit())) {
+				dev_warn(p_info->netcp->dev,
+					 "partial checksum but L4 proto = %d!\n",
+					 l4_proto);
+			}
+			size = 0;
+			break;
+		}
+
+		if (unlikely(size < 0))
+			return size;
+		total += size;
+	}
+
+	/* The next hook may require the command stack to be 8-byte aligned */
+	size = netcp_align_psdata(p_info, 8);
+	if (unlikely(size < 0))
+		return size;
+	if (size > 0) {
+		size = pa_fmtcmd_align(p_info, size);
+		if (unlikely(size < 0))
+			return size;
+		total += size;
+	}
+
+	p_info->tx_pipe = &pa_intf->tx_pipe;
+	return 0;
+}
+
+
+/* This code adapted from net/core/skbuff.c:skb_checksum() */
+static __wsum skb_sctp_csum(struct sk_buff *skb, int offset,
+			  int len, __wsum csum)
+{
+	int start = skb_headlen(skb);
+	int i, copy = start - offset;
+	struct sk_buff *frag_iter;
+
+	/* Checksum header. */
+	if (copy > 0) {
+		if (copy > len)
+			copy = len;
+		csum = sctp_update_cksum(skb->data + offset, copy, csum);
+		if ((len -= copy) == 0)
+			return csum;
+		offset += copy;
+	}
+
+	for (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {
+		int end;
+		skb_frag_t *frag = &skb_shinfo(skb)->frags[i];
+
+		WARN_ON(start > offset + len);
+
+		end = start + skb_frag_size(frag);
+		if ((copy = end - offset) > 0) {
+			u8 *vaddr;
+			skb_frag_t *frag = &skb_shinfo(skb)->frags[i];
+
+			if (copy > len)
+				copy = len;
+			vaddr = kmap_atomic(skb_frag_page(frag));
+			csum = sctp_update_cksum(vaddr + frag->page_offset +
+					 offset - start, copy, csum);
+			kunmap_atomic(vaddr);
+			if (!(len -= copy))
+				return csum;
+			offset += copy;
+		}
+		start = end;
+	}
+
+	skb_walk_frags(skb, frag_iter) {
+		int end;
+
+		WARN_ON(start > offset + len);
+
+		end = start + frag_iter->len;
+		if ((copy = end - offset) > 0) {
+			if (copy > len)
+				copy = len;
+			csum = skb_sctp_csum(frag_iter,
+						offset - start, copy, csum);
+			if ((len -= copy) == 0)
+				return csum;
+			offset += copy;
+		}
+		start = end;
+	}
+	BUG_ON(len);
+
+	return csum;
+}
+
+static void skb_warn_bad_offload(const struct sk_buff *skb)
+{
+	static const netdev_features_t null_features = 0;
+	struct net_device *dev = skb->dev;
+	const char *driver = "";
+
+	if (dev && dev->dev.parent)
+		driver = dev_driver_string(dev->dev.parent);
+
+	WARN(1, "%s: caps=(%pNF, %pNF) len=%d data_len=%d gso_size=%d "
+	     "gso_type=%d ip_summed=%d\n",
+	     driver, dev ? &dev->features : &null_features,
+	     skb->sk ? &skb->sk->sk_route_caps : &null_features,
+	     skb->len, skb->data_len, skb_shinfo(skb)->gso_size,
+	     skb_shinfo(skb)->gso_type, skb->ip_summed);
+}
+
+/* This code adapted from net/core/dev.c:skb_checksum_help() */
+static int skb_sctp_csum_help(struct sk_buff *skb)
+{
+	__wsum csum;
+	int ret = 0, offset;
+
+	if (skb->ip_summed == CHECKSUM_COMPLETE)
+		goto out_set_summed;
+
+	if (unlikely(skb_shinfo(skb)->gso_size)) {
+		skb_warn_bad_offload(skb);
+		return -EINVAL;
+	}
+
+	offset = skb_checksum_start_offset(skb);
+	BUG_ON(offset >= skb_headlen(skb));
+	csum = skb_sctp_csum(skb, offset, skb->len - offset, ~0);
+
+	offset += skb->csum_offset;
+	BUG_ON(offset + sizeof(__le32) > skb_headlen(skb));
+
+	if (skb_cloned(skb) &&
+	    !skb_clone_writable(skb, offset + sizeof(__le32))) {
+		ret = pskb_expand_head(skb, 0, 0, GFP_ATOMIC);
+		if (ret)
+			goto out;
+	}
+
+	*(__le32 *)(skb->data + offset) = sctp_end_cksum(csum);
+out_set_summed:
+	skb->ip_summed = CHECKSUM_NONE;
+out:
+	return ret;
+}
+
+static int pa_txhook_softcsum(int order, void *data, struct netcp_packet *p_info)
+{
+	struct pa_intf *pa_intf = data;
+	struct pa_device *pa_dev = pa_intf->pa_device;
+	struct sk_buff *skb = p_info->skb;
+	int l4_proto;
+	int ret = 0;
+
+	if ((skb->ip_summed != CHECKSUM_PARTIAL) ||
+	    (pa_dev->csum_offload != CSUM_OFFLOAD_SOFT))
+		return 0;
+
+	l4_proto = extract_l4_proto(p_info);
+	if (unlikely(!l4_proto))
+		return 0;
+
+	switch (l4_proto) {
+	case IPPROTO_TCP:
+	case IPPROTO_UDP:
+		ret = skb_checksum_help(skb);
+		break;
+	case IPPROTO_SCTP:
+		ret = skb_sctp_csum_help(skb);
+		break;
+	default:
+		if (unlikely(net_ratelimit())) {
+			dev_warn(p_info->netcp->dev,
+				 "partial checksum but L4 proto = %d!\n",
+				 l4_proto);
+		}
+		return 0;
+	}
+
+	return ret;
+}
+
+
+static int pa_rx_timestamp(int order, void *data, struct netcp_packet *p_info)
+{
+	struct pa_intf *pa_intf = data;
+	struct pa_device *pa_dev = pa_intf->pa_device;
+	struct sk_buff *skb = p_info->skb;
+	struct skb_shared_hwtstamps *sh_hw_tstamps;
+	u64 rx_timestamp;
+	u64 sys_time;
+
+	if (!pa_intf->rx_timestamp_enable)
+		return 0;
+
+	rx_timestamp = p_info->epib[0];
+	rx_timestamp |= ((u64)(p_info->psdata[5] & 0x0000ffff)) << 32;
+
+	sys_time = pa_to_sys_time(pa_dev, rx_timestamp);
+
+	sh_hw_tstamps = skb_hwtstamps(skb);
+	memset(sh_hw_tstamps, 0, sizeof(*sh_hw_tstamps));
+	sh_hw_tstamps->hwtstamp = ns_to_ktime(tstamp_raw_to_ns(pa_dev,
+							rx_timestamp));
+	sh_hw_tstamps->syststamp = ns_to_ktime(sys_time);
+
+	return 0;
+}
+
+static int pa_close(void *intf_priv, struct net_device *ndev)
+{
+	struct pa_intf *pa_intf = intf_priv;
+	struct pa_device *pa_dev = pa_intf->pa_device;
+	struct netcp_priv *netcp_priv = netdev_priv(ndev);
+
+	netcp_unregister_txhook(netcp_priv, pa_dev->txhook_order,
+				pa_tx_hook, intf_priv);
+	if (pa_dev->csum_offload == CSUM_OFFLOAD_SOFT)
+		netcp_unregister_txhook(netcp_priv, pa_dev->txhook_softcsum,
+					pa_txhook_softcsum, intf_priv);
+	netcp_unregister_rxhook(netcp_priv, pa_dev->rxhook_order,
+				pa_rx_timestamp, intf_priv);
+
+	netcp_txpipe_close(&pa_intf->tx_pipe);
+
+	/* De-Configure the streaming switch */
+	netcp_set_streaming_switch(pa_dev->netcp_device,
+				   netcp_priv->cpsw_port,
+				   pa_intf->saved_ss_state);
+
+
+	mutex_lock(&pa_modules_lock);
+	if (!--pa_dev->inuse_if_count) {
+		/* Do pa disable related stuff only if this is the last
+		 * interface to go down
+		 */
+		tasklet_disable(&pa_dev->task);
+
+		tstamp_purge_pending(pa_dev);
+
+		if (pa_dev->pdsp0_tx_channel) {
+			dmaengine_pause(pa_dev->pdsp0_tx_channel);
+			dma_release_channel(pa_dev->pdsp0_tx_channel);
+			pa_dev->pdsp0_tx_channel = NULL;
+		}
+		if (pa_dev->rx_channel) {
+			dmaengine_pause(pa_dev->rx_channel);
+			dma_release_channel(pa_dev->rx_channel);
+			pa_dev->rx_channel = NULL;
+		}
+
+		if (pa_dev->clk) {
+			clk_disable_unprepare(pa_dev->clk);
+			clk_put(pa_dev->clk);
+		}
+		pa_dev->clk = NULL;
+	}
+
+	mutex_unlock(&pa_modules_lock);
+	return 0;
+}
+
+static int pa_open(void *intf_priv, struct net_device *ndev)
+{
+	struct pa_intf *pa_intf = intf_priv;
+	struct pa_device *pa_dev = pa_intf->pa_device;
+	struct netcp_priv *netcp_priv = netdev_priv(ndev);
+	struct dma_keystone_info config;
+	struct pa_pdsp_timer_regs __iomem *timer_reg = &pa_dev->reg_timer[0];
+	const struct firmware *fw;
+	struct dma_chan *chan;
+	dma_cap_mask_t mask;
+	int i, ret, err;
+	unsigned long pa_rate;
+	u64 max_sec;
+
+	/* The first time an open is being called */
+	mutex_lock(&pa_modules_lock);
+
+	dev_dbg(pa_dev->dev, "pa_open() called for port: %d\n",
+		 netcp_priv->cpsw_port);
+
+	if (++pa_dev->inuse_if_count == 1) {
+
+		/* Do pa enable, load firmware only for the first interface
+		 * that comes up
+		 */
+		dev_dbg(pa_dev->dev, "pa_open() called for first time"
+			" initializing per dev stuff\n");
+
+		pa_dev->clk = clk_get(pa_dev->dev, "clk_pa");
+		if (IS_ERR_OR_NULL(pa_dev->clk)) {
+			dev_warn(pa_dev->dev, "unable to get Packet Accelerator clock\n");
+			pa_dev->clk = NULL;
+		}
+
+		if (pa_dev->clk)
+			clk_prepare_enable(pa_dev->clk);
+
+		keystone_pa_reset(pa_dev);
+
+		for (i = 0; i <= 5; i++) {
+			if (i <= 2)
+				ret = request_firmware(&fw,
+					       DEVICE_PA_PDSP02_FIRMWARE,
+					       pa_dev->dev);
+			else if (i == 3)
+				ret = request_firmware(&fw,
+					       DEVICE_PA_PDSP3_FIRMWARE,
+					       pa_dev->dev);
+			else if (i > 3)
+				ret = request_firmware(&fw,
+						DEVICE_PA_PDSP45_FIRMWARE,
+						pa_dev->dev);
+			if (ret != 0) {
+				dev_err(pa_dev->dev, "cant find fw for pdsp %d",
+					i);
+				ret = -ENODEV;
+				goto fail;
+			}
+
+			/* Download the firmware to the PDSP */
+			keystone_pa_set_firmware(pa_dev, i,
+					(const unsigned int *) fw->data,
+					fw->size);
+
+			release_firmware(fw);
+		}
+
+		ret = keystone_pa_reset_control(pa_dev, PA_STATE_ENABLE);
+		if (ret != 1) {
+			dev_err(pa_dev->dev, "enable failed, ret = %d\n", ret);
+			ret = -ENODEV;
+			goto fail;
+		}
+
+		pa_get_version(pa_dev);
+
+		/* Start PDSP timer at a prescaler of divide by 2 */
+		__raw_writel(0xffff, &timer_reg->timer_load);
+		__raw_writel((PA_SS_TIMER_CNTRL_REG_GO |
+			      PA_SS_TIMER_CNTRL_REG_MODE |
+			      PA_SS_TIMER_CNTRL_REG_PSE |
+			      (0 << PA_SS_TIMER_CNTRL_REG_PRESCALE_SHIFT)),
+			      &timer_reg->timer_control);
+
+		/* calculate the multiplier/shift to
+		 * convert PA counter ticks to ns. */
+		pa_rate = clk_get_rate(pa_dev->clk) / 2;
+
+		max_sec = ((1ULL << 48) - 1) + (pa_rate - 1);
+		do_div(max_sec, pa_rate);
+
+		clocks_calc_mult_shift(&pa_dev->timestamp_info.mult,
+				&pa_dev->timestamp_info.shift, pa_rate,
+				NSEC_PER_SEC, max_sec);
+
+		dev_info(pa_dev->dev, "pa_clk_rate(%lu HZ),mult(%u),shift(%u)\n",
+				pa_rate, pa_dev->timestamp_info.mult,
+				pa_dev->timestamp_info.shift);
+
+		pa_dev->timestamp_info.system_offset = 0;
+
+		pa_calibrate_with_system_timer(pa_dev);
+
+		dma_cap_zero(mask);
+		dma_cap_set(DMA_SLAVE, mask);
+
+		/* Open the PA Command transmit channel */
+		pa_dev->pdsp0_tx_channel = dma_request_channel_by_name(mask,
+								"patx-pdsp0");
+		if (IS_ERR_OR_NULL(pa_dev->pdsp0_tx_channel)) {
+			dev_err(pa_dev->dev, "Couldnt get PATX cmd channel\n");
+			pa_dev->pdsp0_tx_channel = NULL;
+			ret = -ENODEV;
+			goto fail;
+		}
+
+		memset(&config, 0, sizeof(config));
+		config.direction	= DMA_MEM_TO_DEV;
+		config.tx_queue_depth	= pa_dev->tx_cmd_queue_depth;
+
+		err = dma_keystone_config(pa_dev->pdsp0_tx_channel, &config);
+		if (err)
+			goto fail;
+
+		/* Open the PA common response channel */
+		pa_dev->rx_channel = dma_request_channel_by_name(mask, "parx");
+		if (IS_ERR_OR_NULL(pa_dev->rx_channel)) {
+			dev_err(pa_dev->dev, "Could not get PA RX channel\n");
+			pa_dev->rx_channel = NULL;
+			ret = -ENODEV;
+			goto fail;
+		}
+
+		memset(&config, 0, sizeof(config));
+
+		config.direction		= DMA_DEV_TO_MEM;
+		config.scatterlist_size		= PA_SGLIST_SIZE;
+		config.rxpool_allocator		= pa_rxpool_alloc;
+		config.rxpool_destructor	= pa_rxpool_free;
+		config.rxpool_param		= pa_dev;
+		config.rxpool_count		= 1;
+		config.rxpool_thresh_enable	= DMA_THRESH_NONE;
+		config.rxpools[0].pool_depth	= pa_dev->rx_pool_depth;
+		config.rxpools[0].buffer_size	= pa_dev->rx_buffer_size;
+
+		err = dma_keystone_config(pa_dev->rx_channel, &config);
+		if (err)
+			goto fail;
+
+		tasklet_init(&pa_dev->task, pa_chan_work_handler,
+			     (unsigned long) pa_dev);
+
+		dma_set_notify(pa_dev->rx_channel, pa_chan_notify, pa_dev);
+
+		pa_dev->cmd_flow_num = dma_get_rx_flow(pa_dev->rx_channel);
+		pa_dev->cmd_queue_num = dma_get_rx_queue(pa_dev->rx_channel);
+
+		dev_dbg(pa_dev->dev, "command receive flow %d, queue %d\n",
+			pa_dev->cmd_flow_num, pa_dev->cmd_queue_num);
+
+		pa_dev->addr_count = 0;
+
+		dma_rxfree_refill(pa_dev->rx_channel);
+
+		ret = pa_config_crc_engine(pa_dev);
+		if (ret < 0)
+			goto fail;
+
+		/* make lut entries invalid */
+		for (i = 0; i < pa_dev->lut_size; i++) {
+			if (!pa_dev->lut[i].valid)
+				continue;
+			keystone_pa_add_mac(pa_intf, i, NULL, NULL, PACKET_DROP,
+					    0, PA_INVALID_PORT);
+		}
+	}
+	mutex_unlock(&pa_modules_lock);
+
+	pa_intf->saved_ss_state = netcp_get_streaming_switch(
+						     pa_dev->netcp_device,
+						     netcp_priv->cpsw_port);
+	dev_dbg(pa_dev->dev, "saved_ss_state for port %d is %d\n",
+		 netcp_priv->cpsw_port, pa_intf->saved_ss_state);
+
+	chan = netcp_get_rx_chan(netcp_priv);
+	pa_intf->data_flow_num = dma_get_rx_flow(chan);
+	pa_intf->data_queue_num = dma_get_rx_queue(chan);
+
+	dev_dbg(pa_dev->dev, "configuring data receive flow %d, queue %d\n",
+		 pa_intf->data_flow_num, pa_intf->data_queue_num);
+
+	/* Configure the streaming switch */
+	netcp_set_streaming_switch(pa_dev->netcp_device, netcp_priv->cpsw_port,
+				   PSTREAM_ROUTE_PDSP0);
+
+	/* Open the PA Data transmit channel */
+	ret = netcp_txpipe_open(&pa_intf->tx_pipe);
+	if (ret)
+		goto fail;
+
+	netcp_register_txhook(netcp_priv, pa_dev->txhook_order,
+			      pa_tx_hook, intf_priv);
+	if (pa_dev->csum_offload == CSUM_OFFLOAD_SOFT)
+		netcp_register_txhook(netcp_priv, pa_dev->txhook_softcsum,
+				      pa_txhook_softcsum, intf_priv);
+	netcp_register_rxhook(netcp_priv, pa_dev->rxhook_order,
+			      pa_rx_timestamp, intf_priv);
+
+	return 0;
+
+fail:
+	mutex_unlock(&pa_modules_lock);
+	pa_close(intf_priv, ndev);
+	return ret;
+}
+
+static struct pa_lut_entry *pa_lut_alloc(struct pa_device *pa_dev,
+					 bool backwards)
+{
+	struct pa_lut_entry *entry;
+	int i;
+
+	if (!backwards) {
+		for (i = 0; i < pa_dev->lut_size; i++) {
+			entry = pa_dev->lut + i;
+			if (!entry->valid || entry->in_use)
+				continue;
+			entry->in_use = true;
+			return entry;
+		}
+	} else {
+		for (i = pa_dev->lut_size - 1; i >= 0; i--) {
+			entry = pa_dev->lut + i;
+			if (!entry->valid || entry->in_use)
+				continue;
+			entry->in_use = true;
+			return entry;
+		}
+	}
+	return NULL;
+}
+
+static inline int pa_lut_entry_count(enum netcp_addr_type type)
+{
+	return (type == ADDR_DEV || type == ADDR_UCAST || type == ADDR_ANY) ? 3 : 1;
+}
+
+int pa_add_addr(void *intf_priv, struct netcp_addr *naddr)
+{
+	struct pa_intf *pa_intf = intf_priv;
+	struct pa_device *pa_dev = pa_intf->pa_device;
+	struct netcp_priv *netcp_priv = netdev_priv(pa_intf->net_device);
+	int count = pa_lut_entry_count(naddr->type);
+	struct pa_lut_entry *entries[count];
+	int port = netcp_priv->cpsw_port;
+	int idx, error;
+	const u8 *addr;
+
+	for (idx = 0; idx < count; idx++) {
+		entries[idx] = pa_lut_alloc(pa_dev, naddr->type == ADDR_ANY);
+		entries[idx]->naddr = naddr;
+		if (!entries[idx])
+			goto fail_alloc;
+	}
+
+	addr = (naddr->type == ADDR_ANY) ? NULL : naddr->addr;
+	idx = 0;
+
+	if (naddr->type == ADDR_ANY) {
+		error = keystone_pa_add_mac(pa_intf, entries[idx++]->index,
+					    NULL, addr, PACKET_HST, 0, port);
+		if (error)
+			return error;
+	}
+
+	if (count > 1) {
+		error = keystone_pa_add_mac(pa_intf, entries[idx++]->index,
+					    NULL, addr, PACKET_PARSE,
+					    0x0800, port);
+		if (error)
+			return error;
+
+		error = keystone_pa_add_mac(pa_intf, entries[idx++]->index,
+					    NULL, addr, PACKET_PARSE,
+					    0x86dd, port);
+		if (error)
+			return error;
+	}
+
+	if (naddr->type != ADDR_ANY) {
+		error = keystone_pa_add_mac(pa_intf, entries[idx++]->index,
+					    NULL, addr, PACKET_HST, 0, port);
+		if (error)
+			return error;
+	}
+
+	return error;
+
+fail_alloc:
+	for (idx--; idx >= 0; idx--)
+		entries[idx]->in_use = false;
+	return -ENOMEM;
+}
+
+static int pa_del_addr(void *intf_priv, struct netcp_addr *naddr)
+{
+	struct pa_intf *pa_intf = intf_priv;
+	struct pa_device *pa_dev = pa_intf->pa_device;
+	struct pa_lut_entry *entry;
+	int idx;
+
+	for (idx = 0; idx < pa_dev->lut_size; idx++) {
+		entry = pa_dev->lut + idx;
+		if (!entry->valid || !entry->in_use || entry->naddr != naddr)
+			continue;
+		keystone_pa_add_mac(pa_intf, entry->index, NULL, NULL,
+				    PACKET_DROP, 0, PA_INVALID_PORT);
+		entry->in_use = false;
+	}
+
+	return 0;
+}
+
+static int pa_hwtstamp_ioctl(struct pa_intf *pa_intf,
+			     struct ifreq *ifr, int cmd)
+{
+	struct hwtstamp_config cfg;
+
+	if (copy_from_user(&cfg, ifr->ifr_data, sizeof(cfg)))
+		return -EFAULT;
+
+	if (cfg.flags)
+		return -EINVAL;
+
+	switch (cfg.tx_type) {
+	case HWTSTAMP_TX_OFF:
+		pa_intf->tx_timestamp_enable = false;
+		break;
+	case HWTSTAMP_TX_ON:
+		pa_intf->tx_timestamp_enable = true;
+		break;
+	default:
+		return -ERANGE;
+	}
+
+	switch (cfg.rx_filter) {
+	case HWTSTAMP_FILTER_NONE:
+		pa_intf->rx_timestamp_enable = false;
+		break;
+	default:
+		pa_intf->rx_timestamp_enable = true;
+		break;
+	}
+
+	return copy_to_user(ifr->ifr_data, &cfg, sizeof(cfg)) ? -EFAULT : 0;
+}
+
+int pa_ioctl(void *intf_priv, struct ifreq *req, int cmd)
+{
+	struct pa_intf *pa_intf = intf_priv;
+
+	if (cmd == SIOCSHWTSTAMP)
+		return pa_hwtstamp_ioctl(pa_intf, req, cmd);
+
+	return -EOPNOTSUPP;
+}
+
+static int pa_attach(void *inst_priv, struct net_device *ndev, void **intf_priv)
+{
+	struct pa_device *pa_dev = inst_priv;
+	struct netcp_priv *netcp_priv = netdev_priv(ndev);
+	struct pa_intf *pa_intf;
+	int chan_id = 0;
+
+	if (netcp_priv->cpsw_port)
+		pa_dev->multi_if = 1;
+
+	dev_dbg(pa_dev->dev, "pa_attach, port %d\n", netcp_priv->cpsw_port);
+	pa_intf = devm_kzalloc(pa_dev->dev, sizeof(struct pa_intf), GFP_KERNEL);
+	if (!pa_intf) {
+		dev_err(pa_dev->dev, "memory allocation failed\n");
+		return -ENOMEM;
+	}
+
+	pa_intf->net_device = ndev;
+	pa_intf->pa_device = pa_dev;
+	*intf_priv = pa_intf;
+
+	/* Use pdsp5 with 0 as base */
+	if (netcp_priv->cpsw_port)
+		chan_id = netcp_priv->cpsw_port - 1;
+
+	snprintf(pa_intf->tx_chan_name, sizeof(pa_intf->tx_chan_name),
+		 "patx-pdsp5-%d", chan_id);
+	netcp_txpipe_init(&pa_intf->tx_pipe, netdev_priv(ndev),
+			  pa_intf->tx_chan_name, pa_dev->tx_data_queue_depth);
+
+	if (pa_dev->csum_offload) {
+		rtnl_lock();
+		ndev->features		|= PA_NETIF_FEATURES;
+		ndev->hw_features	|= PA_NETIF_FEATURES;
+		ndev->wanted_features	|= PA_NETIF_FEATURES;
+		netdev_update_features(ndev);
+		rtnl_unlock();
+	}
+	return 0;
+}
+
+static int pa_release(void *intf_priv)
+{
+	struct pa_intf *pa_intf = intf_priv;
+	struct pa_device *pa_dev = pa_intf->pa_device;
+	struct net_device *ndev = pa_intf->net_device;
+
+	mutex_lock(&pa_modules_lock);
+	if ((!--pa_dev->inuse_if_count) && (pa_dev->csum_offload)) {
+		rtnl_lock();
+		ndev->features		&= ~PA_NETIF_FEATURES;
+		ndev->hw_features	&= ~PA_NETIF_FEATURES;
+		ndev->wanted_features	&= ~PA_NETIF_FEATURES;
+		netdev_update_features(ndev);
+		rtnl_unlock();
+	}
+	mutex_unlock(&pa_modules_lock);
+
+	devm_kfree(pa_dev->dev, pa_intf);
+	return 0;
+}
+
+
+
+#define pa_cond_unmap(field)					\
+	do {							\
+		if (pa_dev->field)				\
+			devm_iounmap(dev, pa_dev->field);	\
+	} while(0)
+
+static int pa_remove(struct netcp_device *netcp_device, void *inst_priv)
+{
+	struct pa_device *pa_dev = inst_priv;
+	struct device *dev = pa_dev->dev;
+
+	pa_cond_unmap(reg_mailbox);
+	pa_cond_unmap(reg_packet_id);
+	pa_cond_unmap(reg_lut2);
+	pa_cond_unmap(reg_control);
+	pa_cond_unmap(reg_timer);
+	pa_cond_unmap(reg_stats);
+	pa_cond_unmap(pa_iram);
+	pa_cond_unmap(pa_sram);
+
+	kfree(pa_dev);
+
+	return 0;
+}
+
+static int pa_probe(struct netcp_device *netcp_device,
+		    struct device *dev,
+		    struct device_node *node,
+		    void **inst_priv)
+{
+	struct pa_device *pa_dev;
+	int ret, len = 0, start, end, i, j;
+	int table_size, num_ranges;
+	u32 *prange;
+
+	if (!node) {
+		dev_err(dev, "device tree info unavailable\n");
+		return -ENODEV;
+	}
+
+	pa_dev = devm_kzalloc(dev, sizeof(struct pa_device), GFP_KERNEL);
+	if (!pa_dev) {
+		dev_err(dev, "memory allocation failed\n");
+		return -ENOMEM;
+	}
+	*inst_priv = pa_dev;
+
+	pa_dev->netcp_device = netcp_device;
+	pa_dev->dev = dev;
+
+	ret = of_property_read_u32(node, "tx_cmd_queue_depth",
+				   &pa_dev->tx_cmd_queue_depth);
+	if (ret < 0) {
+		dev_err(dev, "missing tx_cmd_queue_depth parameter, err %d\n",
+			ret);
+		pa_dev->tx_cmd_queue_depth = 32;
+	}
+	dev_dbg(dev, "tx_cmd_queue_depth %u\n", pa_dev->tx_cmd_queue_depth);
+
+	ret = of_property_read_u32(node, "tx_data_queue_depth",
+				   &pa_dev->tx_data_queue_depth);
+	if (ret < 0) {
+		dev_err(dev, "missing tx_data_queue_depth parameter, err %d\n",
+			ret);
+		pa_dev->tx_data_queue_depth = 32;
+	}
+	dev_dbg(dev, "tx_data_queue_depth %u\n", pa_dev->tx_data_queue_depth);
+
+	ret = of_property_read_u32(node, "rx_pool_depth",
+				   &pa_dev->rx_pool_depth);
+	if (ret < 0) {
+		dev_err(dev, "missing rx_pool_depth parameter, err %d\n",
+			ret);
+		pa_dev->rx_pool_depth = 32;
+	}
+	dev_dbg(dev, "rx_pool_depth %u\n", pa_dev->rx_pool_depth);
+
+	ret = of_property_read_u32(node, "rx_buffer_size",
+				   &pa_dev->rx_buffer_size);
+	if (ret < 0) {
+		dev_err(dev, "missing rx_buffer_size parameter, err %d\n",
+			ret);
+		pa_dev->rx_buffer_size = 128;
+	}
+	dev_dbg(dev, "rx_buffer_size %u\n", pa_dev->rx_buffer_size);
+
+	pa_dev->reg_mailbox	= devm_ioremap(dev, 0x2000000, 0x60);
+	pa_dev->reg_packet_id	= devm_ioremap(dev, 0x2000400, 0x10);
+	pa_dev->reg_lut2	= devm_ioremap(dev, 0x2000500, 0x40);
+	pa_dev->reg_control	= devm_ioremap(dev, 0x2001000, 0x600);
+	pa_dev->reg_timer	= devm_ioremap(dev, 0x2003000, 0x600);
+	pa_dev->reg_stats	= devm_ioremap(dev, 0x2006000, 0x100);
+	pa_dev->pa_iram		= devm_ioremap(dev, 0x2010000, 0x30000);
+	pa_dev->pa_sram		= devm_ioremap(dev, 0x2040000, 0x8000);
+
+	if (!pa_dev->reg_mailbox || !pa_dev->reg_packet_id ||
+	    !pa_dev->reg_lut2 || !pa_dev->reg_control ||
+	    !pa_dev->reg_timer || !pa_dev->reg_stats ||
+	    !pa_dev->pa_sram || !pa_dev->pa_iram) {
+		dev_err(dev, "failed to set up register areas\n");
+		ret = -ENOMEM;
+		goto exit;
+	}
+
+	ret = of_property_read_u32(node, "checksum-offload",
+				   &pa_dev->csum_offload);
+	if (ret < 0) {
+		dev_warn(dev, "missing checksum-offload parameter, err %d\n",
+			ret);
+		pa_dev->csum_offload = CSUM_OFFLOAD_NONE;
+	}
+	if (pa_dev->csum_offload > CSUM_OFFLOAD_SOFT) {
+		dev_err(dev, "invalid checksum-offload parameter %d, err %d\n",
+			ret, pa_dev->csum_offload);
+		pa_dev->csum_offload = CSUM_OFFLOAD_NONE;
+	}
+	dev_dbg(dev, "checksum-offload %u\n", pa_dev->csum_offload);
+
+	ret = of_property_read_u32(node, "txhook-order",
+				   &pa_dev->txhook_order);
+	if (ret < 0) {
+		dev_err(dev, "missing txhook-order parameter, err %d\n",
+			ret);
+		pa_dev->txhook_order = PA_TXHOOK_ORDER;
+	}
+	dev_dbg(dev, "txhook-order %u\n", pa_dev->txhook_order);
+
+	if (pa_dev->csum_offload == CSUM_OFFLOAD_SOFT) {
+		ret = of_property_read_u32(node, "txhook-softcsum",
+					   &pa_dev->txhook_softcsum);
+		if (ret < 0) {
+			dev_err(dev, "missing txhook-softcsum parameter, err %d\n",
+				ret);
+			pa_dev->csum_offload = CSUM_OFFLOAD_NONE;
+			pa_dev->txhook_order = ~0;
+		}
+		dev_dbg(dev, "txhook-softcsum %u\n", pa_dev->txhook_softcsum);
+	}
+
+	ret = of_property_read_u32(node, "rxhook-order",
+				   &pa_dev->rxhook_order);
+	if (ret < 0) {
+		dev_err(dev, "missing rxhook-order parameter, err %d\n",
+			ret);
+		pa_dev->rxhook_order = PA_RXHOOK_ORDER;
+	}
+	dev_dbg(dev, "rxhook-order %u\n", pa_dev->rxhook_order);
+
+	if (!of_get_property(node, "lut-ranges", &len)) {
+		dev_err(dev, "No lut-entry array in dt bindings for PA\n");
+		return -ENODEV;
+	}
+
+	prange = devm_kzalloc(dev, len, GFP_KERNEL);
+	if (!prange) {
+		dev_err(dev, "memory allocation failed at PA lut entry range\n");
+		return -ENOMEM;
+	}
+	len = len / sizeof(u32);
+	if ((len % 2) != 0) {
+		dev_err(dev, "invalid address map in dt binding\n");
+		return -EINVAL;
+	}
+	num_ranges = len / 2;
+	if (of_property_read_u32_array(node, "lut-ranges", prange, len)) {
+		dev_err(dev, "No range-map array  in dt bindings\n");
+		return -ENODEV;
+	}
+
+	table_size = prange[2 * num_ranges - 1] + 1;
+	dev_dbg(dev, "lut size = %d\n", table_size);
+
+	/* Initialize a table for storing entry listings locally */
+	len = table_size * sizeof(struct pa_lut_entry);
+	pa_dev->lut  = devm_kzalloc(dev, len, GFP_KERNEL);
+	if (!pa_dev->lut) {
+		dev_err(dev, "devm_kzalloc mapping failed\n");
+		return -ENOMEM;
+	}
+	pa_dev->lut_size = table_size;
+	dev_dbg(dev, "lut size = %d\n", table_size);
+
+	for (i = 0; i < num_ranges; i++) {
+		start = prange[i * 2];
+		end   = prange[i * 2 + 1];
+		for (j = start; j <= end; j++) {
+			pa_dev->lut[j].valid = true;
+			pa_dev->lut[j].index = j;
+			dev_dbg(dev, "setting entry %d to valid\n", j);
+		}
+	}
+
+	devm_kfree(pa_dev->dev, prange);
+
+	spin_lock_init(&pa_dev->lock);
+	spin_lock_init(&tstamp_lock);
+
+	return 0;
+
+exit:
+	pa_remove(netcp_device, pa_dev);
+	*inst_priv = NULL;
+	return ret;
+}
+
+
+static struct netcp_module pa_module = {
+	.name		= "keystone-pa",
+	.owner		= THIS_MODULE,
+	.probe		= pa_probe,
+	.open		= pa_open,
+	.close		= pa_close,
+	.remove		= pa_remove,
+	.attach		= pa_attach,
+	.release	= pa_release,
+	.add_addr	= pa_add_addr,
+	.del_addr	= pa_del_addr,
+	.ioctl		= pa_ioctl,
+};
+
+static int __init keystone_pa_init(void)
+{
+	return netcp_register_module(&pa_module);
+}
+module_init(keystone_pa_init);
+
+static void __exit keystone_pa_exit(void)
+{
+	netcp_unregister_module(&pa_module);
+}
+module_exit(keystone_pa_exit);
+
+MODULE_LICENSE("GPL v2");
+MODULE_AUTHOR("Sandeep Paulraj <s-paulraj@ti.com>");
+MODULE_DESCRIPTION("Packet Accelerator driver for Keystone devices");
diff --git a/drivers/net/ethernet/ti/keystone_pa.h b/drivers/net/ethernet/ti/keystone_pa.h
new file mode 100644
index 0000000..a7d61cf
--- /dev/null
+++ b/drivers/net/ethernet/ti/keystone_pa.h
@@ -0,0 +1,741 @@
+/*
+ * Copyright (C) 2012 Texas Instruments Incorporated
+ * Author: Sandeep Paulraj <s-paulraj@ti.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation version 2.
+ *
+ * This program is distributed "as is" WITHOUT ANY WARRANTY of any
+ * kind, whether express or implied; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef KEYSTONE_PA_H
+#define KEYSTONE_PA_H
+
+#ifdef __KERNEL__
+
+
+struct pa_pdsp_config {
+
+	u32 pdsp[6];
+
+	char *pdsp_fw[6];
+};
+
+#define PAFRM_MAX_CMD_SET_SIZE		124
+
+#define	PA_DEST_DISCARD  3  /**< Packet is discarded */
+
+/** 
+ *  @def  PA_DEST_CONTINUE_PARSE_LUT1
+ *        packet remains in PA sub-system for more parsing and LUT1 classification
+ */
+#define PA_DEST_CONTINUE_PARSE_LUT1  4 /**< Packet remains in PA sub-system for more parsing and LUT1 classification */
+
+/** 
+ *  @def  PA_DEST_CONTINUE_PARSE_LUT2
+ *        packet remains in PA sub-system for more parsing and LUT2 classification. 
+ */
+#define PA_DEST_CONTINUE_PARSE_LUT2  5  /**< Packet remains in PA sub-system for more parsing and LUT2 classification */
+
+/**
+ *  @def  PA_DEST_HOST
+ *        host thread 
+ */
+#define PA_DEST_HOST   6   /**< Packet is routed to host */
+
+/** 
+ *  @def  PA_DEST_EMAC
+ *        ethernet mac port (of the switch)
+ */
+#define PA_DEST_EMAC   7   /**< Packet is routed to  EMAC */
+
+/** 
+ *  @def  PA_DEST_SASS
+ *        security accelerator destination 
+ */
+#define PA_DEST_SASS   8   /**< Packet is routed to SA */
+
+#define PA_DEST_SRIO   9
+
+#define PA_NO_MULTI_ROUTE		-1
+#define PA_MAX_MULTI_ROUTE_SETS		32
+#define PA_MAX_MULTI_ROUTE_ENTRIES	8
+#define PA_MULTI_ROUTE_DESCRIPTOR_ONLY	0x01
+
+#define PA_EMAC_CTRL_PORT_MASK		0x0F 
+#define PA_EMAC_CTRL_CRC_DISABLE	0x80 
+#define PA_CUSTOM_TYPE_NONE		0   
+#define PA_CUSTOM_TYPE_LUT1		1   
+#define PA_CUSTOM_TYPE_LUT2		2   
+#define PA_MAX_CUSTOM_TYPES_LUT1	4
+#define PA_MAX_CUSTOM_TYPES_LUT2	4
+
+#define PA_CMD_TX_DEST_0	0  /* Packet is sent to PDSP0 */
+#define PA_CMD_TX_DEST_1	1  /* Packet is sent to PDSP1 */
+#define PA_CMD_TX_DEST_2	2  /* Packet is sent to PDSP2 */
+#define PA_CMD_TX_DEST_3	3  /* Packet is sent to PDSP3 */
+#define PA_CMD_TX_DEST_4	4  /* Packet is sent to PDSP4 */
+#define PA_CMD_TX_DEST_5	5  /* Packet is sent to PDSP5 */
+
+#define PA_CMD_NONE			0   
+#define PA_CMD_NEXT_ROUTE		1   
+#define PA_CMD_CRC_OP			2   
+#define PA_CMD_COPY_DATA_TO_PSINFO	3
+#define PA_CMD_PATCH_DATA		4  
+#define PA_CMD_TX_CHECKSUM		5 
+#define PA_CMD_MULTI_ROUTE		6  
+#define PA_CMD_REPORT_TX_TIMESTAMP	7 
+#define PA_CMD_REMOVE_HEADER		8 
+#define PA_CMD_REMOVE_TAIL		9 
+#define PA_CMD_CMDSET			10   
+#define PA_CMD_SA_PAYLOAD		11
+#define PA_CMD_IP_FRAGMENT		12
+#define PA_CMD_USR_STATS		13
+#define PA_CMD_CMDSET_AND_USR_STATS	14
+
+struct pa_frm_forward_host {
+
+	u32	context;	/* Context returned as swInfo0 for matched packet */
+	u8	multi_route;	/* True if multiple destination enabled */
+	u8	multi_idx;	/* Index of the multiple destination set */
+	u8	pa_pdsp_router; /* PA PDSP number used as multi-route router */
+	u8	rsvd2;
+	u8	cmd[4];		/* optional simple command: 0 means no command */
+}; /* 12 bytes */
+
+/*
+ * Routing information used to forward packets to the SA (via PKTDMA) 
+ */
+struct pa_frm_forward_sa {
+
+	u32	sw_info_0;	/* Packet descriptor swInfo0 required by SA operation */
+	u32	sw_info_1;	/* Packet descriptor swInfo1 required by SA operation */
+	u8	cmd[4];		/* optional simple command: 0 means no command */
+};
+
+/*
+ * Routing information used to forward packets to the SRIO (via PKTDMA) 
+ */
+struct pa_frm_forward_srio {
+
+	u32  ps_info0;		/* 8-byte protocol-specific information required by SRIO  */
+	u32  ps_info1;		/* routing */
+	u8   pkt_type;		/* Packet type specified for SRIO operation */
+	u8   rsv4[3];
+};
+
+/*
+ * Routing information used to forward packets to the Ethernet port 
+ */
+struct pa_frm_forward_eth {
+	u8	ps_flags;  /* use the bit 7:4 bit 7: Disable CRC, bit 6:4 port number (0/1/2) bit 3:0 errflags = 0*/
+	u8	rsvd1;
+	u16	rsvd2;
+	u32	rsvd3;
+	u32	rsvd4;
+};
+
+#define PAFRM_ETH_PS_FLAGS_DISABLE_CRC          0x80
+#define PAFRM_ETH_PS_FLAGS_PORT_MASK            0x70
+#define PAFRM_ETH_PS_FLAGS_PORT_SHIFT              4
+
+
+/* Routing information used to forward packets within PA */
+struct pa_frm_forward_pa {
+
+	u8	pa_dest;      /* PDSP destination */
+	u8	custom_type;  /* None, LUT1, LUT2 */
+	u8	custom_idx;   /* Index of the custom type if LUT1 or LUT2 custom */
+	u8	rsvd2;
+	u32	rsvd3;
+	u32	rsvd4;
+};
+
+#define PAFRM_CUSTOM_TYPE_NONE PA_CUSTOM_TYPE_NONE    /* 0 */
+#define PAFRM_CUSTOM_TYPE_LUT1 PA_CUSTOM_TYPE_LUT1    /* 1 */
+#define PAFRM_CUSTOM_TYPE_LUT2 PA_CUSTOM_TYPE_LUT2    /* 2 */
+
+/*
+ * Routing information used to forward packets fromm PA sub-system to various destinations
+ */
+struct pa_frm_forward  {
+
+	u8 forward_type;	/* Forwarding type as defined below */
+	u8 flow_id;		/* PKTDMA flow Id, valid if forwarding via PKTDMA */
+	u16 queue;		/* Destination queue number, valid if forwarding via PKTDMA */
+  
+	union {
+		struct pa_frm_forward_host	host;    /* Host specific routing information */
+		struct pa_frm_forward_sa	sa;      /* SA specific routing information */
+		struct pa_frm_forward_srio	srio;    /* SRIO specific routing information */
+		struct pa_frm_forward_eth	eth;     /* Ethernet specific routing information */
+		struct pa_frm_forward_pa	pa;      /* PA internal routing information */
+	} u;
+};
+
+enum {
+	PAFRM_FORWARD_TYPE_HOST = 0,	/* use PAFRM_DEST_CDMA */
+	PAFRM_FORWARD_TYPE_SA,		/* use PAFRM_DEST_CDMA */
+	PAFRM_FORWARD_TYPE_PA,		/* use pa.paDest */
+	PAFRM_FORWARD_TYPE_ETH,		/* use PAFRM_DEST_ETH */
+	PAFRM_FORWARD_TYPE_SRIO,	/* use PAFRM_DEST_CDMA */
+	PAFRM_FORWARD_TYPE_DISCARD
+};
+
+/* Custom match flag bits */
+#define	PAFRM_LUT1_CUSTOM_MATCH_ETYPE	(1 << 2)
+#define	PAFRM_LUT1_CUSTOM_MATCH_VLAN	(1 << 3)
+#define	PAFRM_LUT1_CUSTOM_MATCH_MATCH	(3 << 4)  /* Ipv6 source and dest entries */
+#define	PAFRM_LUT1_CUSTOM_MATCH_KEY	(1 << 13)
+#define	PAFRM_LUT1_CUSTOM_MATCH_VALID	(1 << 15)
+
+/* Key values. The PDSP will set these bits as it parses the SRIO header */
+#define PAFRM_LUT1_CUSTOM_KEY_CUSTOM		PAFRM_LUT1_KEY_CUSTOM
+#define PAFRM_LUT1_CUSTOM_KEY_INDEX(index)	((index) << 0)  /* Vaild if custom type is set */
+
+/* Add entry to LUT1 */
+/* if PA_LUT1_INDEX_LAST_FREE is used then when the command returns, the value of index
+ * will be replaced with the actual index used */
+#define PAFRM_HW_LUT1_ENTRIES		64
+#define PAFRM_LUT1_INDEX_LAST_FREE	PAFRM_HW_LUT1_ENTRIES
+
+/* Standard match flag bits */
+#define PAFRM_LUT1_MATCH_DMAC		(1 << 0)
+#define PAFRM_LUT1_MATCH_SMAC		(1 << 1)
+#define PAFRM_LUT1_MATCH_ETYPE		(1 << 2)
+#define PAFRM_LUT1_MATCH_VLAN		(1 << 3)
+#define PAFRM_LUT1_MATCH_SIP		(1 << 4)
+#define PAFRM_LUT1_MATCH_DIP		(1 << 5)
+#define PAFRM_LUT1_MATCH_SPI_GRE_SCTP	(1 << 6)
+#define PAFRM_LUT1_MATCH_FLOW		(1 << 7)
+#define PAFRM_LUT1_MATCH_SPORT		(1 << 8)
+#define PAFRM_LUT1_MATCH_DPORT		(1 << 9)
+#define PAFRM_LUT1_MATCH_PROTO		(1 << 10)
+#define PAFRM_LUT1_MATCH_TOS		(1 << 11)
+#define PAFRM_LUT1_MATCH_PORT		(1 << 12)
+#define PAFRM_LUT1_MATCH_KEY		(1 << 13)
+#define PAFRM_LUT1_MATCH_VALID		(1 << 15)
+
+#define PAFRM_LUT1_MATCH_MPLS		(PAFRM_LUT1_MATCH_SPORT | PAFRM_LUT1_MATCH_DPORT)
+
+/* Key values. The PDSP will set these bits as it parses the headers. */
+/* LUT1_1 and LUT1_2 (L3): The following bit fields are used */
+#define PAFRM_LUT1_KEY_SPI	(1 << 0)
+#define PAFRM_LUT1_KEY_GRE	(1 << 1)
+#define PAFRM_LUT1_KEY_MPLS	(1 << 2)
+#define PAFRM_LUT1_KEY_IPV4	(1 << 3)
+#define PAFRM_LUT1_KEY_IPV6	(1 << 4)
+#define PAFRM_LUT1_KEY_SCTP	(1 << 5)
+
+/* LUT1: Custom  (L3) */
+#define PAFRM_LUT1_KEY_CUSTOM	(1 << 7)     
+
+/* LUT1_0: MAC and SRIO (L0-l2): The following bit fields are used */
+#define PAFRM_LUT1_KEY_SRIO	(1 << 7)
+
+#define PAFRM_LUT1_KEY_MAC    (1 << 0)
+
+struct pa_frm_com_l1_standard {
+
+	/* LUT1 view 1 */
+	u8	dmac[6];	/* Destination mac */
+	u8	smac[6];	/* Source mac */
+	u16	etype;		/* Ethernrt type, the field is also used for the previous match PDSP number */
+	u16	vlan;		/* VLAN tag, the field is also used for the previous match LUT1 index */
+  
+	/* LUT1 view 2 */
+	u8	src_ip[16];	/* Source IP address */
+	u8	dst_ip[16];	/* Destination IP address */
+  
+	/* LUT1 view 3 */
+	u32	spi;		/* ESP or AH header Security Parameters Index */
+				/* The field is also used for GRE protocol or SCTP destination port */
+	u32	flow;		/* IPv6 flow label in 20 lsbs */
+  
+	union {
+		u16	ports[2];   /* UDP/TCP Source port (0), destination port (1) */
+		u32	mpls;       /* mpls label in 20 Lsbs */
+	} pm;
+  
+	u8	proto_next;	/* Ipv4 Protocol fields, IPv6 next */
+	u8	tos_tclass;	/* Ipv4 TOS, Ipv6 traffic class */
+	u8	inport;		/* reserved field: not used */
+	u8	key;		/* IP: Distinguishs spi/gre and mpls and ports
+					* LUT1_0: MAC/SRIO, 
+					* LUT1_1/LUT1_2: custom or standard 
+					*/
+	/* end LUT1 view 3 */
+  
+	/* Lookup cares/don't cares */
+	u16	match_flags;	/* lookup matching valid flags as defined below */
+	u16	rsvd;		/* reserved for alignment */
+};
+
+struct pa_frm_com_l1_srio {
+
+	/* LUT1 view 1 */
+	u8	rsvd1[4];	/* unused field: All zero's */
+	u16	src_id;	/* Source ID */
+	u16	dest_id;	/* Destination ID */
+	u8	rsvd2[4];	/* unused field: All zero's */
+	u16	etype;	/* upper link (previous match PDSP number) */
+	u16	vlan;	/* upper link (previous match LUT1 index) */
+  
+	/* LUT1 view 2 */
+	u8	rsvd3[16];		/* unused field: All zero's */
+	u8	rsvd4[14];		/* unused field: All zero's */
+	u16	type_param1;	/* stream ID or mailbox */
+  
+	/* LUT1 view 3 */
+	u32	spi;	/* unused field: All zero's */
+	u32	flow;	/* unused field: All zero's */
+  
+	u16	next_hdr_offset;	/* unused field: All zero's */
+	u8	next_hdr;		/* place holder for nextHdr and nextOffset */
+	u8	rsvd5;			/* unused field: All zero's */
+	u8	pri;			/* 3-bit Priority */
+	u8	type_param2;		/* cos or letter */
+	u8	inport;			/* unused field: All zero's */
+	u8	key;			/* IP: Distinguishs spi/gre and mpls and ports
+					 * LUT1_0: MAC/SRIO, 
+					 * LUT1_1/LUT1_2: custom or standard 
+					 */
+	/* end LUT1 view 3 */
+	/* Lookup cares/don't cares */
+	u16	match_flags;		/* lookup matching valid flags as defined below */
+	u16	rsvd;			/* reserved for alignment */
+};
+
+struct pa_frm_com_l1_custom{
+
+	/* LUT1 view 1 */
+	u8	dmac[6];	/* unused field: All zero's */
+	u8	smac[6];	/* unused field: All zero's */
+	u16	etype;		/* upper link (previous match PDSP number) */
+	u16	vlan;		/* upper link (previous match LUT1 index) */
+  
+	/* LUT1 view 2 */
+	u8	match_values[32];	/* 32 bytes to match   */
+  
+	/* LUT1 view 3 - offset from start */
+	u32	rsvd0;		/* unused field: All zero's */
+	u32	rsvd1;		/* unused field: All zero's */
+	u32	rsvd2;		/* unused field: All zero's */
+  
+	u8	rsvd3;		/* unused field: All zero's */
+	u8	rsvd4;		/* unused field: All zero's */
+	u8	inport;		/* unused field: All zero's */
+	u8	key;		/* IP: Distinguishs spi/gre and mpls and ports
+				 * LUT1_0: MAC/SRIO, 
+				 * LUT1_1/LUT1_2: custom or standard 
+				 */
+  
+	/* Lookup cares/dont cares */
+	u16	match_flags;	/* lookup matching valid flags as defined below */
+	u16	rsvd5;		/* reserved for alignment */ 
+};
+
+enum {
+	PAFRM_CONFIG_COMMAND_RSVD	= 0,
+	PAFRM_CONFIG_COMMAND_ADDREP_LUT1,
+	PAFRM_CONFIG_COMMAND_DEL_LUT1,
+	PAFRM_CONFIG_COMMAND_ADDREP_LUT2,
+	PAFRM_CONFIG_COMMAND_DEL_LUT2,
+	PAFRM_CONFIG_COMMAND_CONFIG_PA,
+	PAFRM_CONFIG_COMMAND_REQ_STATS,
+	PAFRM_CONFIG_COMMAND_REQ_VERSION,
+	PAFRM_CONFIG_COMMAND_MULTI_ROUTE,
+	PAFRM_CONFIG_COMMAND_CRC_ENGINE,
+	PAFRM_CONFIG_COMMAND_CMD_SET
+};
+
+/* Command magic value */
+#define PAFRM_CONFIG_COMMAND_SEC_BYTE  0xce
+
+/* Command return values */
+enum {
+
+	PAFRM_COMMAND_RESULT_SUCCESS = 0,              /* Must be 0 */
+	PAFRM_COMMAND_RESULT_NO_COMMAND_MAGIC,         /* Command magic value not found */
+  
+	PAFRM_COMMAND_RESULT_INVALID_CMD,              /* Invalid command identifier */
+  
+	/* Add entry to LUT1 fails */
+	PAFRM_COMMAND_RESULT_LUT1_TYPE_INVALID,        /* Invalid type, custom or standard IP/ethernet */
+	PAFRM_COMMAND_RESULT_LUT1_INDEX_INVALID,       /* Invalid LUT1 index (0-63) or no free indices available */
+	PAFRM_COMMAND_RESULT_LUT1_MATCH_DEST_INVALID,  /* Sent a match packet to q0 on c1 or c2 - this is illegal. */
+	PAFRM_COMMAND_RESULT_LUT1_NMATCH_INVALID,      /* Previous match forward info was somewhere in chunk domain */
+	PAFRM_COMMAND_RESULT_LUT1_INVALID_KEYS,        /* Invalid combination found in the key value */
+  
+	/* Lut 2 entry warnings since the lut can be configured without pdsp */
+	PAFRM_COMMAND_RESULT_WARN_OVER_MAX_ENTRIES,
+	PAFRM_COMMAND_RESULT_WARN_NEGATIVE_ENTRY_COUNT,
+  
+	/* Lut 2 entry failures */
+	PAFRM_COMMAND_RESULT_LUT2_ADD_BUSY,            /* LUT2 had a lookup and pending config */
+  
+	/* Not enough room in stats request packet for the reply */
+	PAFRM_COMMAND_RESULT_WARN_STATS_REPLY_SIZE,
+  
+	/* Command sent to PDSP which couldn't handle it */
+	PAFRM_COMMAND_RESULT_INVALID_DESTINATION,
+  
+	/* Add/Delete/Read entries to multi route table */
+	PAFRM_COMMAND_RESULT_MULTI_ROUTE_NO_FREE_ENTRIES,    /* Asked to use a free entry, but none found */
+	PAFRM_COMMAND_RESULT_MULTI_ROUTE_INVALID_IDX,        /* Illegal index value used */
+	PAFRM_COMMAND_RESULT_MULTI_ROUTE_INVALID_MODE,       /* Illegal multi route mode used */
+  
+	/* Packet size didn't match command */
+	PAFRM_COMMAND_RESULT_INVALID_PKT_SIZE,
+  
+	/* Coustom and Command set index */
+	PAFRM_COMMAND_RESULT_INVALID_C1_CUSTOM_IDX,          /* Illegal Custom LUT1 index value used */
+	PAFRM_COMMAND_RESULT_INVALID_C2_CUSTOM_IDX,          /* Illegal Custom LUT2 index value used */
+	PAFRM_COMMAND_RESULT_INVALID_CMDSET_IDX              /* Illegal Custom Command Set index value used */
+};
+
+#define PA_SS_TIMER_CNTRL_REG_GO		0x00000001u
+#define PA_SS_TIMER_CNTRL_REG_MODE		0x00000002u
+#define PA_SS_TIMER_CNTRL_REG_PSE		0x00008000u
+#define PA_SS_TIMER_CNTRL_REG_PRESCALE_SHIFT	0x00000002u
+
+/* Destination (route) values */
+#define PAFRM_DEST_PDSP0	0
+#define PAFRM_DEST_PDSP1	1
+#define PAFRM_DEST_PDSP2	2
+#define PAFRM_DEST_PDSP3	3
+#define PAFRM_DEST_PDSP4	4
+#define PAFRM_DEST_PDSP5	5
+#define PAFRM_DEST_PKTDMA	6   
+#define PAFRM_DEST_ETH		7
+
+#define PAFRM_DEST_DISCARD	10
+
+/* Assigning names based on PDSP functions */
+#define PAFRM_DEST_PA_C1_0	PAFRM_DEST_PDSP0
+#define PAFRM_DEST_PA_C1_1	PAFRM_DEST_PDSP1
+#define PAFRM_DEST_PA_C1_2	PAFRM_DEST_PDSP2 
+#define PAFRM_DEST_PA_C2	PAFRM_DEST_PDSP3
+#define PAFRM_DEST_PA_M_0	PAFRM_DEST_PDSP4
+#define PAFRM_DEST_PA_M_1	PAFRM_DEST_PDSP5
+
+/* The default queue for packets that arrive at the PA and don't match in
+ * classify1 (right at init time) */
+#define PAFRM_DEFAULT_INIT_Q	0x100
+
+/* Ethertypes recognized by the firmware. */
+#define PAFRM_ETHERTYPE_IP		0x0800
+#define PAFRM_ETHERTYPE_IPV6		0x86dd
+#define PAFRM_ETHERTYPE_VLAN		0x8100
+#define PAFRM_ETHERTYPE_SPVLAN		0x88a8
+#define PAFRM_ETHERTYPE_MPLS		0x8847
+#define PAFRM_ETHERTYPE_MPLS_MULTI	0x8848
+
+/* Next header type values  */
+#define PAFRM_HDR_MAC			0
+#define PAFRM_HDR_VLAN			1
+#define PAFRM_HDR_MPLS			2
+#define PAFRM_HDR_IPv4			3
+#define PAFRM_HDR_IPv6			4
+#define PAFRM_HDR_IPv6_EXT_HOP		5
+#define PAFRM_HDR_IPv6_EXT_ROUTE	6
+#define PAFRM_HDR_IPv6_EXT_FRAG		7
+#define PAFRM_HDR_IPv6_EXT_DEST		8
+#define PAFRM_HDR_GRE			9
+#define PAFRM_HDR_ESP			10
+#define PAFRM_HDR_ESP_DECODED		11
+#define PAFRM_HDR_AUTH			12
+#define PAFRM_HDR_CUSTOM_C1		13
+#define PAFRM_HDR_FORCE_LOOKUP		14   /* A contrived header type used with custom SRIO to force
+                                           a parse after looking at only the RIO L0-L2 */
+#define PAFRM_HDR_SCTP			15
+#define PAFRM_HDR_UNKNOWN		16
+#define PAFRM_HDR_UDP			17
+#define PAFRM_HDR_UDP_LITE		18
+#define PAFRM_HDR_TCP			19
+#define PAFRM_HDR_GTPU			20
+#define PAFRM_HDR_ESP_DECODED_C2	21
+#define PAFRM_HDR_CUSTOM_C2		22
+
+/* Command related definitions */
+#define PAFRM_CRC_FLAG_CRC_OFFSET_VALID		0x01
+#define PAFRM_CRC_FLAG_CRC_OFFSET_FROM_DESC	0x02
+#define PAFRM_CHKSUM_FALG_NEGATIVE		0x01
+
+#define PA_NEXT_ROUTE_PARAM_PRESENT		0x0001
+#define PA_NEXT_ROUTE_PROC_NEXT_CMD		0x0002
+#define PA_NEXT_ROUTE_PROC_MULTI_ROUTE		0x0004
+
+/* PAFRM receive commands related definitions */
+
+/* 
+ * There are the following two groups of PAFRM receive commands:
+ * PAFRM short commands which can be used as part of the routing info 
+ * PAFRM commands which can be used within a command set
+ */
+ 
+#define PAFRM_RX_CMD_NONE		0           /* Dummy command */
+
+/* short commands */
+#define PAFRM_RX_CMD_CMDSET		1           /* Execute a command set */
+#define PAFRM_RX_CMD_INSERT		2           /* Insert up to two types at the current location */
+
+/* command set commands */
+#define PAFRM_RX_CMD_NEXT_ROUTE		3           /* Specify the next route */
+#define PAFRM_RX_CMD_CRC_OP		4           /* CRC generation or verification */
+#define PAFRM_RX_CMD_COPY_DATA		5           /* Copy data to the PS Info section */
+#define PAFRM_RX_CMD_PATCH_DATA		6           /* Insert or pacth packet data at the specific location */
+#define PAFRM_RX_CMD_REMOVE_HDR		7           /* Remove the parsed packet header */
+#define PAFRM_RX_CMD_REMOVE_TAIL	8           /* Remove the parsed packet tail */
+#define PAFRM_RX_CMD_MULTI_ROUTE	9           /* Duplicate packet to multiple destinations */
+
+/*
+ * PASS command ID formatting
+ * Bit 15 is used to distinguish the L2 table from
+ * the L3 table in the command comId field
+ */
+#define PA_COMID_L2		(0 << 15)
+#define PA_COMID_L3		(1 << 15)
+#define PA_COMID_L_MASK		(1 << 15)
+#define PA_COMID_IDX_MASK	(~(1 << 15))
+
+/* define LUT1 entry types */
+#define PAFRM_COM_ADD_LUT1_STANDARD	0	/* MAC/IP */
+#define PAFRM_COM_ADD_LUT1_SRIO		1	/* SRIO */
+#define PAFRM_COM_ADD_LUT1_CUSTOM	2   /* Custom LUT1 */
+
+struct pa_frm_cmd_add_lut1 {
+
+	u8	index;		/* LUT1 index. */
+	u8	type;		/* Custom or standard */
+	u8	rsvd;		/* reserved for alignment */
+	u8	cust_index;     /* Vaild only if type is custom */
+	
+	union {
+		struct	pa_frm_com_l1_standard	eth_ip;   /* matching information for MAC/IP entry */
+		struct	pa_frm_com_l1_srio	srio;
+		struct	pa_frm_com_l1_custom	custom;
+	} u;
+
+	struct	pa_frm_forward match;	/* Routing information when a match is found */
+  
+	/*
+	 * Routing information when subsequent match fails - a fragmented
+	 * packet orinner route
+	 */
+	struct	pa_frm_forward next_fail;
+};
+
+/* CRC Engine Configuration */
+#define PARAM_CRC_TABLE_SIZE    16
+
+struct pa_frm_config_crc {
+	u8	ctrl_bitmap;			/* Control bit maps as defined below */
+#define PARAM_CRC_SIZE_8         0
+#define PARAM_CRC_SIZE_16        1
+#define PARAM_CRC_SIZE_24        2
+#define PARAM_CRC_SIZE_32        3
+
+#define PARAM_CRC_CTRL_CRC_SIZE_MASK    0x3
+#define PARAM_CRC_CTRL_LEFT_SHIFT       0x0
+#define PARAM_CRC_CTRL_RIGHT_SHIFT      0x4
+#define PARAM_CRC_CTRL_INV_RESULT       0x8
+
+	u8	rsvd1;				/* reserved for alignment */
+	u16	rsvd2;				/* reserved for alignment */
+	u32	init_val;			/* Initial value to use in the CRC calcualtion */
+	u32	crc_tbl[PARAM_CRC_TABLE_SIZE];	/* CRC table */
+};
+
+/* Commands to PA */
+struct pa_frm_command {
+
+	u32	command_result; /* Returned to the host, ignored on entry to the PASS */
+	u8	command;	/* Command value */
+	u8	magic;		/* Magic value */
+	u16	com_id;		/* Used by the host to identify command results */
+	u32	ret_context;	/* Returned in swInfo to identify packet as a command */
+	u16	reply_queue;	/* Specifies the queue number for the message reply. 0xffff to toss the reply */
+	u8	reply_dest;	/* Reply destination (host0, host1, discard are the only valid values) */
+	u8	flow_id;	/* Flow ID used to assign packet at reply */
+	u32	cmd;		/* First word of the command */
+};
+
+struct pa_cmd_next_route {
+	u16	ctrl_bit_field;		/* Routing control information as defined at @ref routeCtrlInfo */	
+	int	dest;			/* Packet destination as defined at @ref pktDest */
+	u8	pkt_type_emac_ctrl;	/*  For destination SRIO, specify the 5-bit packet type toward SRIO 
+                                     For destination EMAC, specify the EMAC control @ref emcOutputCtrlBits to the network */
+	u8	flow_id;	/* For host, SA or SRIO destinations, specifies return free descriptor setup */
+	u16	queue;		/*For host, SA or SRIO destinations, specifies the dest queue */
+	u32	sw_info_0;	/* Placed in SwInfo0 for packets to host or SA */
+	u32	sw_info_1;         /* Placed in SwInfo1 for packets to the SA */
+	u16	multi_route_index; /* Multi-route index. It is valid in the from-network direction only */
+};
+
+struct pa_cmd_crcOp {
+	u16	ctrl_bit_field;    /* CRC operation control information as defined at @ref crcOpCtrlInfo */
+	u16	start_offset;     /* Byte location, from SOP/Protocol Header, where the CRC computation begins 
+                                    if frame type is not specified
+                                    Byte location, from SOP/Protocol header, where the specific frame header begins
+                                    if frame type is specified
+                                    In to-network direction: offset from SOP
+                                    In from-network direction: offset from the current parsed header 
+                                    */
+	u16	len;             /* Number of bytes covered by the CRC computation 
+                                    valid only if pa_CRC_OP_PAYLOAD_LENGTH_IN_HEADER is clear */
+	u16	len_offset;       /* Payload length field offset in the custom header */
+	u16	len_mask;         /* Payload length field mask */
+	u16	len_adjust;       /* Payload length adjustment: valid only if PA_CRC_OP_PAYLOAD_LENGTH_IN_HEADER is set */
+	u16	crc_offset;       /* Offset from SOP/Protocol Header to the CRC field 
+                                    In to-network direction: offset from SOP
+                                    In from-network direction: offset from the current parsed header */
+	u16	frame_yype;       /* Frame type @ref crcFrameTypes, vaild if
+			    PA_CRC_OP_CRC_FRAME_TYPE is set */
+};
+
+/**
+ *  @ingroup palld_api_structures
+ *  @brief  Transmit checksum configuration
+ *
+ *  @details  paTxChksum_t is used in the call to @ref Pa_formatTxRoute or @ref Pa_formatTxCmd to create a tx 
+ *            command header that instructs the packet accelerator sub-system to generate ones' complement
+ *             checksums into network packets. The checksums are typically used for TCP and UDP payload checksums as
+ *            well as IPv4 header checksums. In the case of TCP and UDP payload checksums the psuedo header
+ *            checksum must be pre-calculated and provided, the sub-system does not calculate it.
+ */
+struct pa_tx_chksum {
+	u16	start_offset;   /* Byte location, from SOP, where the checksum calculation begins */
+	u16	length_bytes;   /* Number of bytes covered by the checksum. Must be even */
+	u16	result_offset;  /* Byte offset, from startOffset, to place the resulting checksum */
+	u16	initial_sum;    /* Initial value of the checksum */
+	u16	negative_0;     /* If TRUE, a computed value of 0 is written as -0 */
+};
+
+struct pa_cmd_copy {
+	u16	ctrl_bitfield;    /* Copy operation control information as defined at @ref copyCtrlInfo */
+	u16	src_offset;       /* Offset from the start of current protocol header for the data copy to begin */
+	u16	dest_offset;      /* Offset from the top of the PSInfo for the data to be copied to */
+	u16	num_bytes;        /* Number of bytes to be copied */   
+};
+
+struct pa_patch_info{
+	unsigned int	n_patch_bytes;              /**<  The number of bytes to be patched */
+	unsigned int	total_patch_size;           /**<  The number of patch bytes in the patch command, must be >= to nPatchBytes and a multiple of 4 bytes */
+	unsigned int	offset;                   /**<  Offset from the start of the packet for the patch to begin in the to-network direction 
+                                                 Offset from the start of the current header for the patch to begin in the from-network direction */
+	u16		overwrite;                /**<  If TRUE the patch data replaces existing packet data. If false the data is added */
+	u8		*patch_data;                /**<  Pointer to the patch data */
+};
+
+
+/**
+ *  @ingroup palld_api_structures
+ *  @brief  paPayloadInfo_t defines the packet payload information in the short format.
+ *          It is required by the Security Accelerator sub-system (SASS)
+ *
+ *  @details paPayloadInfo_t defines the packet parsing information in terms of
+ *           payload offset and payload length as described below
+ *  @li      SRTP:      offset to the RTP header; RTP payload length including ICV
+ *  @li      IPSEC AH:  offset to the Outer IP; IP payload length
+ *  @li      IPSEC ESP: offset to the ESP header; ESP papload length including ICV
+ */
+
+struct pa_payload_info  {
+	u16	offset;	/* The offset to where the SA packet parsing starts */
+	u16	len;	/* The total length of the protocal payload to be processed by SA */
+};
+
+struct pa_cmd_multi_route {
+	u16	index;        /*  Multi-route set Index */
+};
+
+/**
+ *   @def  PA_MAX_CMD_SETS
+ *         The maximum number of command sets supported
+ */
+#define PA_MAX_CMD_SETS     8
+
+#define PA_OK					0
+#define PA_ERR_CONFIG				-10
+#define PA_INSUFFICIENT_CMD_BUFFER_SIZE		-11
+#define PA_INVALID_CMD_REPLY_DEST		-12
+
+/**
+ *  @ingroup palld_api_structures
+ *  @brief  Command Set Command
+ *
+ *  @details paCmdSet_t is used to specify the desired PA command set. The command set command 
+ *           instructs the PASS to execute a list of commands after a LUT1 or LUT2 match occurs. 
+ *           It is one of the command which can be embedded within the @ref paRouteInfo_t. 
+ */
+struct pa_cmd_set {
+	u16	index;        /*Command Set Index */
+};
+
+struct pa_cmd_tx_timestamp {
+	u16	dest_queue;	/* Host queue for the tx timestamp reporting packet */
+	u16	flow_id;	/* CPPI flow */
+	u32	sw_info0;	/* 32 bit value returned in the descriptor */
+};
+
+struct pa_cmd_ip_frag {
+	u16	ip_offset;	/* Offset to the IP header. */
+	u16	mtu_size;	/* Size of the maximum transmission unit (>= 68) */
+};
+
+struct pa_cmd_usr_stats {
+	u16	index;		/* User-defined statistics index */
+};
+
+struct pa_cmd_set_usr_stats {
+	u16	set_index;	/* Commad Set Index */
+	u16	stats_index;    /* User-defined statistics index */
+};
+
+struct pa_cmd_info {
+	u16	cmd;			/*Specify the PA command code as defined at @ref paCmdCode */
+	union {
+		struct pa_cmd_next_route route;	/* Specify nextRoute command specific parameters */
+		struct pa_tx_chksum	chksum;	/* Specify Tx Checksum command specific parameters */
+		struct pa_cmd_crcOp     crcOp;    /* Specify CRC operation command specific parameters */
+		struct pa_cmd_copy	copy;     /* Specify Copy command specific parameters */
+		struct pa_patch_info	patch;    /* Specify Patch command specific parameters */
+		struct pa_payload_info	payload;  /* Specify the payload information required by SA */
+		struct pa_cmd_set	cmd_set;   /* Specify Command Set command specific parameters */
+		struct pa_cmd_multi_route m_route;   /* Specify Multi-route command specific parameters */
+		struct pa_cmd_tx_timestamp tx_ts;     /*Specify Report Tx Timestamp command specific parameters */
+		struct pa_cmd_ip_frag	ip_frag;   /* Specify IP fragmentation command specific parameters */
+		struct pa_cmd_usr_stats usr_stats; /* Specify User-defined Statistics command specific parameters */
+		struct pa_cmd_set_usr_stats cmd_set_usr_stats;  
+	} params;
+};
+
+struct pa_route_info {
+	int	dest;
+	u8	flow_id;
+	u16	queue;
+	int	m_route_index;
+	u32	sw_info_0;
+	u32	sw_info_1;
+	int	custom_type;
+	u8	custom_index;                                    
+	u8	pkt_type_emac_ctrl;
+	struct pa_cmd_info *pcmd;
+};
+
+struct pa_cmd_reply {
+	int	dest;		/* Packet destination, must be pa_DEST_HOST or PA_DEST_DISCARD, see @ref pktDest */
+	u32	reply_id;	/*  Value placed in swinfo0 in reply packet */
+	u16	queue;		/*  Destination queue for destination PA_DEST_HOST */
+	u8	flow_id;	/*  Flow ID used on command reply from PASS */
+};
+
+#endif /* __KERNEL__ */
+
+#endif /* KEYSTONE_PA_H */
+
diff --git a/drivers/net/ethernet/ti/keystone_pasahost.h b/drivers/net/ethernet/ti/keystone_pasahost.h
new file mode 100644
index 0000000..33e21f8
--- /dev/null
+++ b/drivers/net/ethernet/ti/keystone_pasahost.h
@@ -0,0 +1,390 @@
+/*
+ * Copyright (C) 2012 Texas Instruments Incorporated
+ * Author: Sandeep Paulraj <s-paulraj@ti.com> 
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation version 2.
+ *
+ * This program is distributed "as is" WITHOUT ANY WARRANTY of any
+ * kind, whether express or implied; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef KEYSTONE_PASAHOST_H
+#define KEYSTONE_PASAHOST_H
+
+#ifdef __KERNEL__
+
+#define	PASAHO_CONFIGURE		4
+#define PASAHO_PARX_PARSECMD		0
+#define PASAHO_PARX_MULTI_ROUTE		5
+#define PASAHO_PAMOD_CMPT_CHKSUM	0
+#define PASAHO_PAMOD_CMPT_CRC		1
+#define PASAHO_PAMOD_PATCH		2
+#define PASAHO_PAMOD_NROUTE		3
+#define PASAHO_PAMOD_MULTI_ROUTE	5
+#define PASAHO_PAMOD_REPORT_TIMESTAMP	6   
+#define PASAHO_PAMOD_GROUP_7		7   
+#define PASAHO_PAMOD_DUMMY		PASAHO_PAMOD_GROUP_7
+#define PASAHO_PAMOD_IP_FRAGMENT	PASAHO_PAMOD_GROUP_7
+#define PASAHO_SA_LONG_INFO		0
+#define PASAHO_SA_SHORT_INFO		1
+#define PASAHO_SA_AIR_INFO		2
+
+#define PASAHO_READ_BITFIELD(a,b,c)	(((a)>>(b)) & ((1UL<<(c))-1))
+
+#define PASAHO_SET_BITFIELD(a,x,b,c)	(a) &= ~(((1UL<<(c))-1)<<(b)), \
+					(a) |= (((x) & ((1UL<<(c))-1))<<(b))
+
+#define PASAHO_SET_CMDID(x,v)	PASAHO_SET_BITFIELD((x)->word0, (v), 29,3)
+
+#define PASAHO_PACFG_CMD	(((u32)PASAHO_CONFIGURE << 5) << 24)
+
+enum pasaho_header_type {
+	PASAHO_HDR_MAC        = 0,        /* MAC */
+	PASAHO_HDR_VLAN,                  /* VLAN */
+	PASAHO_HDR_MPLS,                  /* MPLS */
+	PASAHO_HDR_IPv4,                  /* IPv4 */
+	PASAHO_HDR_IPv6,                  /* IPv6 */
+	PASAHO_HDR_IPv6_EXT_HOP,          /* IPv6 hop by hop extenstion header */
+	PASAHO_HDR_IPv6_EXT_ROUTE,        /* IPv6 routing extenstion header */
+	PASAHO_HDR_IPv6_EXT_FRAG,         /* IPv6 fragmentation extention header */
+	PASAHO_HDR_IPv6_EXT_DEST,         /* IPv6 destination options header */
+	PASAHO_HDR_GRE,                   /* Generic Routing Encapsulation header */
+	PASAHO_HDR_ESP,                   /* Encapsulating Security Payload header */
+	PASAHO_HDR_ESP_DECODED,           /* Decoded Encapsulating Security Payload header */
+	PASAHO_HDR_AUTH,                  /* Authentication header */
+	PASAHO_HDR_CUSTOM_C1,             /* Custom classify 1 header */
+	PASAHO_HDR_FORCE_LOOKUP,          /* A contrived header type used with custom SRIO to force
+                                        a parse after looking at only the SRIO L0-L2 */
+	PASAHO_HDR_UNKNOWN,               /* Next header type is unknown */
+	PASAHO_HDR_UDP,                   /* User Datagram Protocol header */
+	PASAHO_HDR_UDP_LITE,              /* Lightweight User Datagram Protocol header */
+	PASAHO_HDR_TCP,                   /* Transmission Control Protocol header */
+	PASAHO_HDR_CUSTOM_C2              /* Custom classify 2 header */
+};
+
+/**
+ *  @defgroup pasahoSubCmdCode PASS Sub-Command Code
+ *  @ingroup pasaho_if_constants
+ *  @{
+ *
+ *  @name PASS Sub-Command Code
+ *  Definition of the 5-bit sub-command codes which is used to specify the group 7 commands. 
+ */ 
+
+enum pasaho_sub_cmd_code {
+	PASAHO_SUB_CMD_DUMMY	= 0,	/* Dummy */
+	PASAHO_SUB_CMD_IP_FRAG		/* IPv4 fragmentation */
+};
+
+/**
+ *  @ingroup pasaho_if_structures
+ *  @brief  pasahoCmdInfo_t defines the general short command information
+ *
+ */
+struct pasaho_cmd_info {
+	u32	word0;		/* Control block word 0 */
+};
+
+/**
+ *  @ingroup pasaho_if_structures
+ *  @brief  pasahoLongInfo_t defines the packet parsing information in the long format. 
+ *          The information is structured as an array of 32 bit values. These values
+ *          are broken down through macros. This allows the representation to be
+ *          endian independent to the hardware which operates only on 32 bit values.
+ *
+ *  @details  
+ */
+struct pasaho_long_info {
+	u32   word0;	/* Control block word 0 */
+	u32   word1;	/* Control block word 1 */
+	u32   word2;	/* Control block word 2 */
+	u32   word3;	/* Control block word 3 */
+	u32   word4;	/* Control block word 4 */
+};
+
+/** 
+ *  @defgroup PASAHO_long_info_command_macros  PASAHO Long Info Command Macros
+ *  @ingroup pasaho_if_macros
+ *  @{
+ *  @name PASAHO Long Info Command Macros
+ *  Macros used by the PASAHO Long Info Command
+ */
+
+/* Extract the command ID defined at */
+#define PASAHO_LINFO_READ_CMDID(x)		PASAHO_READ_BITFIELD((x)->word0,29,3)
+
+/* Extract the block length */
+#define PASAHO_LINFO_READ_RECLEN(x)		PASAHO_READ_BITFIELD((x)->word0,24,5)
+
+/* Extract the next parse start offset */
+#define PASAHO_LINFO_READ_START_OFFSET(x)	PASAHO_READ_BITFIELD((x)->word0,0,16)
+
+/* Extract the end of packet parse offset */
+#define PASAHO_LINFO_READ_END_OFFSET(x)		PASAHO_READ_BITFIELD((x)->word1,16,16)
+
+/* Extract the error index */
+#define PASAHO_LINFO_READ_EIDX(x)		PASAHO_READ_BITFIELD((x)->word1,11,5)
+
+/* Extract the previous match flag */
+#define PASAHO_LINFO_READ_PMATCH(x)		PASAHO_READ_BITFIELD((x)->word1,10,1)
+
+/* Extract the custom classify flag */
+#define PASAHO_LINFO_READ_C2C(x)		PASAHO_READ_BITFIELD((x)->word1,9,1)
+
+/* Extract the first parse module ID */
+#define PASAHO_LINFO_READ_L1_PDSP_ID(x)		PASAHO_READ_BITFIELD((x)->word1,6,3)
+
+/* Extract the first parse module match index */
+#define PASAHO_LINFO_READ_L1_IDX(x)		PASAHO_READ_BITFIELD((x)->word1,0,6)
+
+/* Extract the offset to the level 3 header */
+#define PASAHO_LINFO_READ_L3_OFFSET(x)		PASAHO_READ_BITFIELD((x)->word2,24,8)
+
+/* Extract the offset to the level 4 header */
+#define PASAHO_LINFO_READ_L4_OFFSET(x)		PASAHO_READ_BITFIELD((x)->word2,16,8)
+
+/* Extract the offset to the level 5 header */
+#define PASAHO_LINFO_READ_L5_OFFSET(x)		PASAHO_READ_BITFIELD((x)->word2,8,8)
+
+/* Extract the offset to the security header */
+#define PASAHO_LINFO_READ_ESP_AH_OFFSET(x)	PASAHO_READ_BITFIELD((x)->word2,0,8)
+
+/* Extract the bitmask of parsed header types */
+#define PASAHO_LINFO_READ_HDR_BITMASK(x)	PASAHO_READ_BITFIELD((x)->word3,21,11)
+
+/* Extract the next header to parse type */
+#define PASAHO_LINFO_READ_NXT_HDR_TYPE(x)	PASAHO_READ_BITFIELD((x)->word3,16,5)
+
+/* Extract the number of VLAN tags found */
+#define PASAHO_LINFO_READ_VLAN_COUNT(x)		PASAHO_READ_BITFIELD((x)->word3,12,4)
+
+/* Extract the number of IP headers found */
+#define PASAHO_LINFO_READ_IP_COUNT(x)		PASAHO_READ_BITFIELD((x)->word3,8,4)
+
+/* Extract the number of GRE headers found */
+#define PASAHO_LINFO_READ_GRE_COUNT(x)		PASAHO_READ_BITFIELD((x)->word3,4,4)
+
+/* Extract the fragmentation found flag */
+#define PASAHO_LINFO_READ_FLAG_FRAG(x)		PASAHO_READ_BITFIELD((x)->word3,3,1)
+
+/* Extract the incomplete IP route flag */
+#define PASAHO_LINFO_READ_FLAG_ROUTE(x)		PASAHO_READ_BITFIELD((x)->word3,2,1)
+
+/* Extract the (1-based) input EMAC port number */
+/*  0: Indicates that the packet does not enter PASS through CPSW */
+#define PASAHO_LINFO_READ_INPORT(x)		PASAHO_READ_BITFIELD((x)->word3,0,3)
+
+/* Extract the last pseudo-header checksum computed */
+#define PASAHO_LINFO_READ_PSEUDO_CHKSM(x)	PASAHO_READ_BITFIELD((x)->word4,16,16)
+
+#define PASAHO_LINFO_READ_TSTAMP_MSB(x)		PASAHO_READ_BITFIELD((x)->word4,0,16)
+
+/* Extract the IP Reassembly Traffic Flow Index */
+#define PASAHO_LINFO_READ_TFINDEX(x)		PASAHO_READ_BITFIELD((x)->word4,24,8)
+
+/* Extract the IP Reassembly Fragment count */
+#define PASAHO_LINFO_READ_FRANCNT(x)		PASAHO_READ_BITFIELD((x)->word4,16,8)
+
+/* Set the IP Reassembly Traffic Flow Index */
+#define PASAHO_LINFO_SET_TFINDEX(x, v)		PASAHO_SET_BITFIELD((x)->word4,(v),24,8)
+
+/* Set the IP Reassembly Fragment count */
+#define PASAHO_LINFO_SET_FRANCNT(x, v)		PASAHO_SET_BITFIELD((x)->word4,(v),16,8)
+
+/* Indicate whether it is an IPSEC packet */
+#define PASAHO_LINFO_IS_IPSEC(x)		PASAHO_READ_BITFIELD((x)->word3,25,2)
+
+/* Indicate whether it is an IPSEC ESP packet */
+#define PASAHO_LINFO_IS_IPSEC_ESP(x)		PASAHO_READ_BITFIELD((x)->word3,26,1)
+
+/* Indicate whether it is an IPSEC AH packet */
+#define PASAHO_LINFO_IS_IPSEC_AH(x)		PASAHO_READ_BITFIELD((x)->word3,25,1)
+
+/* Clear IPSEC indication bits */
+#define PASAHO_LINFO_CLR_IPSEC(x)		PASAHO_SET_BITFIELD((x)->word3,0,25,2)
+
+/* Clear IPSEC ESP indication bit */
+#define PASAHO_LINFO_CLR_IPSEC_ESP(x)		PASAHO_SET_BITFIELD((x)->word3,0,26,1)
+
+/* Clear IPSEC AH indication bit */
+#define PASAHO_LINFO_CLR_IPSEC_AH(x)		PASAHO_SET_BITFIELD((x)->word3,0,25,1)
+
+/* Clear the fragmentation found flag */
+#define PASAHO_LINFO_CLR_FLAG_FRAG(x)		PASAHO_SET_BITFIELD((x)->word3,0,3,1)
+
+/* Update the next parse start offset */
+#define PASAHO_LINFO_SET_START_OFFSET(x, v)	PASAHO_SET_BITFIELD((x)->word0,(v),0,16)
+
+/* Update the end of packet parse offset */
+#define PASAHO_LINFO_SET_END_OFFSET(x, v)	PASAHO_SET_BITFIELD((x)->word1,(v),16,16)
+
+
+/*
+ * Set the null packet flag which indicates that the packet should be dropped.
+ * This flag should be set for the null packet to be delivered to PASS when
+ * the reassembly timeout occurs
+ */
+#define PASAHO_LINFO_SET_NULL_PKT_IND(x, v)	PASAHO_SET_BITFIELD((x)->word0,(v),21,1)
+
+/*
+ * PA_INV_TF_INDEX
+ * PASS-asssited IP reassembly traffic flow index to indicate
+ * that no traffic flow is available
+ */
+#define PA_INV_TF_INDEX		0xFF
+
+struct pasaho_short_info {
+	u32	word0;
+	u32	word1;
+};
+
+/* Extract the command ID defined at */
+#define PASAHO_SINFO_READ_CMDID(x)		PASAHO_READ_BITFIELD((x)->word0,29,3)
+
+/* Extract the offset to the packet payload */
+#define PASAHO_SINFO_RESD_PAYLOAD_OFFSET(x)	PASAHO_READ_BITFIELD((x)->word0,16,8)
+
+/* Extract the byte length of the payload */
+#define PASAHO_SINFO_READ_PAYLOAD_LENGTH(x)	PASAHO_READ_BITFIELD((x)->word0,0,16)
+
+/* Set the offset to the payload */
+#define PASAHO_SINFO_SET_PAYLOAD_OFFSET(x, v)	PASAHO_SET_BITFIELD((x)->word0, (v), 16, 8)
+
+/* Set the payload length */
+#define PASAHO_SINFO_SET_PAYLOAD_LENGTH(x, v)	PASAHO_SET_BITFIELD((x)->word0, (v), 0,  16)
+
+/* Format the entire short info command */
+#define PASAHO_SINFO_FORMAT_CMD(offset, len)	(((offset) << 16) | (len) | (PASAHO_SA_SHORT_INFO << 29))
+
+#define PASAHO_HDR_BITMASK_MAC		(1 << 0)	/* MAC present */
+#define PASAHO_HDR_BITMASK_VLAN		(1 << 1)	/* VLAN present */
+#define PASAHO_HDR_BITMASK_MPLS		(1 << 2)	/* MPLS present */
+#define PASAHO_HDR_BITMASK_IP		(1 << 3)	/* IP present */
+#define PASAHO_HDR_BITMASK_ESP		(1 << 4)	/* IPSEC/ESP present */
+#define PASAHO_HDR_BITMASK_AH		(1 << 5)	/* IPSEC/AH present */
+#define PASAHO_HDR_BITMASK_UDP		(1 << 6)	/* UDP present */
+#define PASAHO_HDR_BITMASK_UDPLITE	(1 << 7)	/* UDPLITE present */
+#define PASAHO_HDR_BITMASK_TCP		(1 << 8)	/* TCP present */
+#define PASAHO_HDR_BITMASK_GRE		(1 << 9)	/* GRE present */
+#define PASAHO_HDR_BITMASK_CUSTOM	(1 << 10)	/* Custom header */
+
+struct pasaho_next_route {
+	u32  word0;          
+	u32  sw_info0;        
+	u32  sw_info1;        
+	u32  word1;          
+};
+
+/*
+ * Sets the N bit which indicates the next command
+ * should be executed prior to the route command
+ */
+#define PASAHO_SET_N(x,v)	PASAHO_SET_BITFIELD((x)->word0, (v), 28, 1)
+
+/*
+ * Sets the E bit which indicates the extened
+ * parameters (packet type) are present for SRIO
+ */
+#define PASAHO_SET_E(x,v)	PASAHO_SET_BITFIELD((x)->word0, (v), 27, 1)
+
+/*
+ * Sets the destination of the route defined */
+#define PASAHO_SET_DEST(x,v)	PASAHO_SET_BITFIELD((x)->word0, (v), 24, 3)
+
+/* Specifies the flow to use for packets sent to the host */
+#define PASAHO_SET_FLOW(x,v)	PASAHO_SET_BITFIELD((x)->word0, (v), 16, 8)
+
+/* Specifies the queue to use for packets send to the host */
+#define PASAHO_SET_QUEUE(x,v)   PASAHO_SET_BITFIELD((x)->word0, (v), 0,  16)
+
+/* Specifies the packet type to use for packets send to the SRIO */
+#define PASAHO_SET_PKTTYPE(x,v) PASAHO_SET_BITFIELD((x)->word1, (v), 24, 8)
+
+struct pasaho_com_chk_crc {
+	u32	word0;		/* PASAHO_chksum_command_macros */
+	u32	word1;		/* PASAHO_chksum_command_macros */
+	u32	word2;		/* PASAHO_chksum_command_macros */
+};
+
+/*
+ * Sets the negative 0 flag - if set a
+ * checksum computed as 0 will be sent as 0xffff
+ */
+#define PASAHO_CHKCRC_SET_NEG0(x,v)	PASAHO_SET_BITFIELD((x)->word0, (v), 23, 1)
+
+/* Sets the optional flags of the CRC/Checksum command */
+#define PASAHO_CHKCRC_SET_CTRL(x,v)	PASAHO_SET_BITFIELD((x)->word0, (v), 16, 8)
+
+/* Sets the start offset of the checksum/crc */
+#define PASAHO_CHKCRC_SET_START(x,v)	PASAHO_SET_BITFIELD((x)->word0, (v), 0,  16)
+
+/* Sets the length of the checksum/crc */
+#define PASAHO_CHKCRC_SET_LEN(x,v)	PASAHO_SET_BITFIELD((x)->word1, (v), 16, 16)
+
+/* Sets the offset to where to paste the checksum/crc into the packet */
+#define PASAHO_CHKCRC_SET_RESULT_OFF(x,v)	PASAHO_SET_BITFIELD((x)->word1, (v), 0,  16)
+
+/* Sets the initial value of the checksum/crc */
+#define PASAHO_CHKCRC_SET_INITVAL(x,v)	PASAHO_SET_BITFIELD((x)->word2, (v), 16, 16)
+
+#define PASAHO_BPATCH_MAX_PATCH_WORDS	4
+
+struct pasaho_com_blind_patch {
+	u32	word0;
+	u32	patch[PASAHO_BPATCH_MAX_PATCH_WORDS];
+};
+
+
+#define PASAHO_BPATCH_SET_PATCH_NBYTES(x,v)	PASAHO_SET_BITFIELD((x)->word0, v, 24,  5)
+
+/* Sets the number of bytes to patch */
+#define PASAHO_BPATCH_SET_PATCH_CMDSIZE(x,v)	PASAHO_SET_BITFIELD((x)->word0, v, 20, 4)
+
+/* Sets the size of the command in 32 bit word units */                        
+#define PASAHO_BPATCH_SET_OVERWRITE(x,v)	PASAHO_SET_BITFIELD((x)->word0, v, 19, 1)
+
+/*
+ * Sets the overwrite flag. If set the patch will
+ * overwrite existing packet data, otherwise data is inserted
+ */                         
+#define PASAHO_BPATCH_SET_OFFSET(x,v)		PASAHO_SET_BITFIELD((x)->word0, v, 0,  16)
+
+/* Sets the offset to the start of the patch */
+#define PASAHO_BPATCH_SET_PATCH_BYTE(x, byteNum, byte)  (x)->patch[(byteNum) >> 2] = \
+		PASAHO_SET_BITFIELD((x)->patch[(byteNum) >> 2], byte, ((3 - (byteNum & 0x3)) << 3), 8)
+
+
+struct pasaho_report_timestamp {
+	u32	word0;
+	u32	sw_info0;
+};
+
+/* Specifies the flow to use for report packets sent to the host */
+
+#define PASAHO_SET_REPORT_FLOW(x,v)	PASAHO_SET_BITFIELD((x)->word0, (v), 16, 8)
+
+/* Specifies the queue to use for report packets send to the host */
+#define PASAHO_SET_REPORT_QUEUE(x,v)	PASAHO_SET_BITFIELD((x)->word0, (v), 0,  16)
+
+struct pasaho_ip_frag {
+	u32	word0;
+};
+
+/* Set sub-command code to indicate IP Fragmentation command */
+#define PASAHO_SET_SUB_CODE_IP_FRAG(x) PASAHO_SET_BITFIELD((x)->word0, PASAHO_SUB_CMD_IP_FRAG, 24, 5)
+
+/* Specifies the sub-command code */
+#define PASAHO_SET_SUB_CODE(x,v)	PASAHO_SET_BITFIELD((x)->word0, (v), 24, 5)
+
+/* Specifies the offset to the IP header to be fragmented */
+#define PASAHO_SET_IP_OFFSET(x,v)	PASAHO_SET_BITFIELD((x)->word0, (v), 16, 8)
+
+/* Specifies the MTU size */
+#define PASAHO_SET_MTU_SIZE(x,v)	PASAHO_SET_BITFIELD((x)->word0, (v), 0,  16)
+
+#endif /* __KERNEL__ */
+#endif /* KEYSTONE_PASAHOST_H */
diff --git a/drivers/net/ethernet/ti/keystone_qos.c b/drivers/net/ethernet/ti/keystone_qos.c
new file mode 100644
index 0000000..4cfcc4b
--- /dev/null
+++ b/drivers/net/ethernet/ti/keystone_qos.c
@@ -0,0 +1,300 @@
+/*
+ * Copyright (C) 2012 Texas Instruments Incorporated
+ * Authors: Reece Pollack <reece@theptrgroup.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation version 2.
+ *
+ * This program is distributed "as is" WITHOUT ANY WARRANTY of any
+ * kind, whether express or implied; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/io.h>
+#include <linux/clk.h>
+#include <linux/slab.h>
+#include <linux/delay.h>
+#include <linux/types.h>
+#include <linux/module.h>
+#include <linux/delay.h>
+#include <linux/of.h>
+#include <linux/of_device.h>
+#include <linux/of_address.h>
+#include <linux/firmware.h>
+#include <linux/spinlock.h>
+#include <linux/interrupt.h>
+#include <linux/dmaengine.h>
+#include <linux/dma-mapping.h>
+#include <linux/scatterlist.h>
+#include <linux/byteorder/generic.h>
+#include <linux/platform_device.h>
+#include <linux/keystone-dma.h>
+#include <linux/errqueue.h>
+
+#include "keystone_net.h"
+
+#define	QOS_TXHOOK_ORDER	20
+
+#define	MAX_CHANNELS	8
+
+struct qos_channel {
+	const char		*tx_chan_name;
+	u32			 tx_queue_depth;
+	struct netcp_tx_pipe	 tx_pipe;
+};
+
+struct qos_device {
+	struct netcp_device		*netcp_device;
+	struct device			*dev;
+	struct device_node		*node;
+	u32				 multi_if;
+};
+
+struct qos_intf {
+	struct net_device		*ndev;
+	struct device			*dev;
+	int				 num_channels;
+	struct qos_channel		 channels[MAX_CHANNELS];
+};
+
+static int qos_tx_hook(int order, void *data, struct netcp_packet *p_info)
+{
+	struct qos_intf *qos_intf = data;
+	struct sk_buff *skb = p_info->skb;
+
+	dev_dbg(qos_intf->dev,
+		"priority: %u, queue_mapping: %04x\n",
+		skb->priority, skb_get_queue_mapping(skb));
+
+	if (skb->queue_mapping < qos_intf->num_channels)
+		p_info->tx_pipe =
+			&qos_intf->channels[skb->queue_mapping].tx_pipe;
+	else {
+		dev_warn(qos_intf->dev,
+			"queue mapping (%d) >= num chans (%d) QoS bypassed\n",
+			 skb_get_queue_mapping(skb), qos_intf->num_channels);
+	}
+
+	return 0;
+}
+
+
+static int qos_close(void *intf_priv, struct net_device *ndev)
+{
+	struct qos_intf *qos_intf = intf_priv;
+	struct netcp_priv *netcp_priv = netdev_priv(ndev);
+	int i;
+
+	netcp_unregister_txhook(netcp_priv, QOS_TXHOOK_ORDER, qos_tx_hook,
+				qos_intf);
+
+	for (i = 0; i < qos_intf->num_channels; ++i) {
+		struct qos_channel *qchan = &qos_intf->channels[i];
+
+		netcp_txpipe_close(&qchan->tx_pipe);
+	}
+
+	return 0;
+}
+
+static int qos_open(void *intf_priv, struct net_device *ndev)
+{
+	struct qos_intf *qos_intf = intf_priv;
+	struct netcp_priv *netcp_priv = netdev_priv(ndev);
+	int ret;
+	int i;
+
+	/* Open the QoS input queues */
+	for (i = 0; i < qos_intf->num_channels; ++i) {
+		struct qos_channel *qchan = &qos_intf->channels[i];
+
+		ret = netcp_txpipe_open(&qchan->tx_pipe);
+		if (ret)
+			goto fail;
+	}
+
+	netcp_register_txhook(netcp_priv, QOS_TXHOOK_ORDER, qos_tx_hook,
+			      intf_priv);
+
+	return 0;
+
+fail:
+	qos_close(intf_priv, ndev);
+	return ret;
+}
+
+static int init_channel(struct qos_intf *qos_intf,
+			int index,
+			struct device_node *node)
+{
+	struct qos_channel *qchan = &qos_intf->channels[index];
+	int ret;
+
+	ret = of_property_read_string(node, "tx-channel", &qchan->tx_chan_name);
+	if (ret < 0) {
+		dev_err(qos_intf->dev,
+			"missing tx-channel parameter, err %d\n", ret);
+		qchan->tx_chan_name = "qos";
+	}
+	dev_dbg(qos_intf->dev, "tx-channel \"%s\"\n", qchan->tx_chan_name);
+
+	ret = of_property_read_u32(node, "tx_queue_depth",
+				   &qchan->tx_queue_depth);
+	if (ret < 0) {
+		dev_err(qos_intf->dev,
+			"missing tx_queue_depth parameter, err %d\n", ret);
+		qchan->tx_queue_depth = 16;
+	}
+	dev_dbg(qos_intf->dev, "tx_queue_depth %u\n", qchan->tx_queue_depth);
+
+	return 0;
+}
+
+static int qos_attach(void *inst_priv, struct net_device *ndev,
+		      void **intf_priv)
+{
+	struct netcp_priv *netcp = netdev_priv(ndev);
+	struct qos_device *qos_dev = inst_priv;
+	struct qos_intf *qos_intf;
+	struct device_node *interface, *channel;
+	char node_name[24];
+	int i, ret;
+
+	qos_intf = devm_kzalloc(qos_dev->dev,
+				 sizeof(struct qos_intf), GFP_KERNEL);
+	if (!qos_intf) {
+		dev_err(qos_dev->dev,
+			"qos interface memory allocation failed\n");
+		return -ENOMEM;
+	}
+
+	qos_intf->ndev = ndev;
+	qos_intf->dev = qos_dev->dev;
+
+	snprintf(node_name, sizeof(node_name), "interface-%d",
+		 (qos_dev->multi_if) ? (netcp->cpsw_port - 1) : 0);
+
+	*intf_priv = qos_intf;
+
+	interface = of_get_child_by_name(qos_dev->node, node_name);
+	if (!interface) {
+		dev_err(qos_intf->dev,
+			"could not find interface %d node in device tree\n",
+			(netcp->cpsw_port - 1));
+		ret = -ENODEV;
+		goto exit;
+	}
+
+	qos_intf->num_channels = 0;
+	for_each_child_of_node(interface, channel) {
+		if (qos_intf->num_channels >= MAX_CHANNELS) {
+			dev_err(qos_intf->dev,
+				"too many QoS input channels defined\n");
+			break;
+		}
+		init_channel(qos_intf, qos_intf->num_channels, channel);
+		++qos_intf->num_channels;
+	}
+
+	of_node_put(interface);
+
+	/* Initialize the QoS input queues */
+	for (i = 0; i < qos_intf->num_channels; ++i) {
+		struct qos_channel *qchan = &qos_intf->channels[i];
+
+		netcp_txpipe_init(&qchan->tx_pipe, netdev_priv(ndev),
+				  qchan->tx_chan_name,
+				  qchan->tx_queue_depth);
+
+		qchan->tx_pipe.dma_psflags = netcp->cpsw_port;
+	}
+
+	return 0;
+exit:
+	devm_kfree(qos_dev->dev, qos_intf);
+	return ret;
+}
+
+static int qos_release(void *intf_priv)
+{
+	struct qos_intf *qos_intf = intf_priv;
+
+	kfree(qos_intf);
+
+	return 0;
+}
+
+static int qos_remove(struct netcp_device *netcp_device, void *inst_priv)
+{
+	struct qos_device *qos_dev = inst_priv;
+
+	kfree(qos_dev);
+
+	return 0;
+}
+
+static int qos_probe(struct netcp_device *netcp_device,
+		    struct device *dev,
+		    struct device_node *node,
+		    void **inst_priv)
+{
+	struct qos_device *qos_dev;
+	int ret = 0;
+	
+	qos_dev = devm_kzalloc(dev, sizeof(struct qos_device), GFP_KERNEL);
+	if (!qos_dev) {
+		dev_err(dev, "memory allocation failed\n");
+		return -ENOMEM;
+	}
+	*inst_priv = qos_dev;
+
+	if (!node) {
+		dev_err(dev, "device tree info unavailable\n");
+		ret = -ENODEV;
+		goto exit;
+	}
+
+	qos_dev->netcp_device = netcp_device;
+	qos_dev->dev = dev;
+	qos_dev->node = node;
+
+	if (of_find_property(node, "multi-interface", NULL))
+		qos_dev->multi_if = 1;
+
+	return 0;
+
+exit:
+	qos_remove(netcp_device, qos_dev);
+	*inst_priv = NULL;
+	return ret;
+}
+
+
+static struct netcp_module qos_module = {
+	.name		= "keystone-qos",
+	.owner		= THIS_MODULE,
+	.probe		= qos_probe,
+	.open		= qos_open,
+	.close		= qos_close,
+	.remove		= qos_remove,
+	.attach		= qos_attach,
+	.release	= qos_release,
+};
+
+static int __init keystone_qos_init(void)
+{
+	return netcp_register_module(&qos_module);
+}
+module_init(keystone_qos_init);
+
+static void __exit keystone_qos_exit(void)
+{
+	netcp_unregister_module(&qos_module);
+}
+module_exit(keystone_qos_exit);
+
+MODULE_LICENSE("GPL v2");
+MODULE_AUTHOR("Reece Pollack <reece@theptrgroup.com");
+MODULE_DESCRIPTION("Quality of Service driver for Keystone devices");
diff --git a/drivers/net/ethernet/ti/keystone_sgmii.c b/drivers/net/ethernet/ti/keystone_sgmii.c
new file mode 100644
index 0000000..02030bc
--- /dev/null
+++ b/drivers/net/ethernet/ti/keystone_sgmii.c
@@ -0,0 +1,307 @@
+/*
+ * Copyright (C) 2012 Texas Instruments Incorporated
+ * Authors: Sandeep Paulraj <s-paulraj@ti.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation version 2.
+ *
+ * This program is distributed "as is" WITHOUT ANY WARRANTY of any
+ * kind, whether express or implied; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/io.h>
+#include <linux/err.h>
+#include <linux/delay.h>
+
+#include "keystone_net.h"
+
+#define SGMII_REG_BASE                  0x02090100
+#define SGMII_SERDES_BASE		0x02620340
+#define SGMII_SERDES_CFGPLL		0x0
+#define SGMII_SERDES_CFGRX0		0x4
+#define SGMII_SERDES_CFGTX0		0x8
+#define SGMII_SERDES_CFGRX1		0xC
+#define SGMII_SERDES_CFGTX1		0x10
+#define SGMII_SERDES_SIZE		0x14
+
+#define SGMII_SRESET_RESET		0x1
+#define SGMII_SRESET_RTRESET		0x2
+#define SGMII_CTL_AUTONEG		0x01
+#define SGMII_CTL_LOOPBACK		0x10
+#define SGMII_CTL_MASTER		0x20
+#define SGMII_REG_STATUS_LOCK		BIT(4)
+#define	SGMII_REG_STATUS_LINK		BIT(0)
+#define SGMII_REG_STATUS_AUTONEG	BIT(2)
+#define SGMII_REG_CONTROL_AUTONEG	BIT(0)
+
+#define SGMII_OFFSET(x)	((x <= 1)? (x * 0x100): ((x * 0x100) + 0x100))
+/*
+ * SGMII registers
+ */
+#define SGMII_IDVER_REG(x)    (SGMII_OFFSET(x) + 0x000)
+#define SGMII_SRESET_REG(x)   (SGMII_OFFSET(x) + 0x004)
+#define SGMII_CTL_REG(x)      (SGMII_OFFSET(x) + 0x010)
+#define SGMII_STATUS_REG(x)   (SGMII_OFFSET(x) + 0x014)
+#define SGMII_MRADV_REG(x)    (SGMII_OFFSET(x) + 0x018)
+#define SGMII_LPADV_REG(x)    (SGMII_OFFSET(x) + 0x020)
+#define SGMII_TXCFG_REG(x)    (SGMII_OFFSET(x) + 0x030)
+#define SGMII_RXCFG_REG(x)    (SGMII_OFFSET(x) + 0x034)
+#define SGMII_AUXCFG_REG(x)   (SGMII_OFFSET(x) + 0x038)
+
+struct sgmii_config {
+	u32	interface;
+	u32	mr_adv_ability;
+	u32	control;
+};
+
+static inline void sgmii_write_reg(void __iomem *base, int reg, u32 val)
+{
+	__raw_writel(val, base + reg);
+}
+
+static inline u32 sgmii_read_reg(void __iomem *base, int reg)
+{
+	return __raw_readl(base + reg);
+}
+
+static inline void sgmii_write_reg_bit(void __iomem *base, int reg, u32 val)
+{
+	__raw_writel((__raw_readl(base + reg) | val),
+			base + reg);
+}
+
+int serdes_init(void)
+{
+	void __iomem *sgmii_serdes;
+
+	sgmii_serdes = ioremap(SGMII_SERDES_BASE, SGMII_SERDES_SIZE);
+
+	sgmii_write_reg(sgmii_serdes, SGMII_SERDES_CFGPLL, 0x00000041);
+	
+	udelay(2000);
+
+	sgmii_write_reg(sgmii_serdes, SGMII_SERDES_CFGRX0, 0x00700621);
+	sgmii_write_reg(sgmii_serdes, SGMII_SERDES_CFGRX1, 0x00700621);
+
+	sgmii_write_reg(sgmii_serdes, SGMII_SERDES_CFGTX0, 0x000108a1);
+	sgmii_write_reg(sgmii_serdes, SGMII_SERDES_CFGTX1, 0x000108a1);
+
+	udelay(2000);
+
+	iounmap(sgmii_serdes);
+
+	return 0;
+}	
+
+int keystone_sgmii_reset(void __iomem *sgmii_ofs, int port)
+{
+	/* Soft reset */
+	sgmii_write_reg_bit(sgmii_ofs, SGMII_SRESET_REG(port), 0x1);
+	while(sgmii_read_reg(sgmii_ofs, SGMII_SRESET_REG(port)) != 0x0);
+
+	return 0;
+}
+
+int keystone_sgmii_link_status(void __iomem *sgmii_ofs, int ports)
+{
+	u32 status = 0, link = 0;
+	u32 i;
+
+	for (i = 0; i < ports; i++) {
+		status = sgmii_read_reg(sgmii_ofs, SGMII_STATUS_REG(i));
+		if ((status & SGMII_REG_STATUS_LINK) != 0)
+			link |= BIT(i);
+		else
+			link &= ~BIT(i);
+	}
+
+	return link;
+}
+
+int keystone_sgmii_get_port_link(void __iomem *sgmii_ofs, int port)
+{
+	u32 status = 0, link = 0;
+
+	status = sgmii_read_reg(sgmii_ofs, SGMII_STATUS_REG(port));
+	if ((status & SGMII_REG_STATUS_LINK) != 0)
+		link |= BIT(port);
+	else
+		link &= ~BIT(port);
+
+	return link;
+}
+
+
+int keystone_sgmii_config(void __iomem *sgmii_ofs,
+			  int port, u32 interface)
+{
+	unsigned int i, status, mask;
+	struct sgmii_config *config;
+
+	config = kzalloc(sizeof(*config), GFP_KERNEL);
+	if (!config)
+		return -ENOMEM;
+
+	switch (interface) {
+	case SGMII_LINK_MAC_MAC_AUTONEG:
+		config->mr_adv_ability	= 0x9801;
+		config->control		= 0x21;
+
+		break;
+	case SGMII_LINK_MAC_PHY:
+	case SGMII_LINK_MAC_PHY_NO_MDIO:
+		config->mr_adv_ability	= 1;
+		config->control		= 1;
+
+		break;
+	case SGMII_LINK_MAC_MAC_FORCED:
+		config->mr_adv_ability	= 0x9801;
+		config->control		= 0x20;
+
+		break;
+	case SGMII_LINK_MAC_FIBER:
+		config->mr_adv_ability	= 0x20;
+		config->control		= 0x1;
+
+		break;
+	}
+
+	sgmii_write_reg(sgmii_ofs, SGMII_CTL_REG(port), 0);
+
+	/*
+	 * Wait for the SerDes pll to lock,
+	 * but don't trap if lock is never read
+	 */
+	for (i = 0; i < 1000; i++)  {
+		udelay(2000);
+		status = sgmii_read_reg(sgmii_ofs, SGMII_STATUS_REG(port));
+		if ((status & SGMII_REG_STATUS_LOCK) != 0)
+			break;
+	}
+
+	sgmii_write_reg(sgmii_ofs,
+			SGMII_MRADV_REG(port), config->mr_adv_ability);
+	sgmii_write_reg(sgmii_ofs,
+			SGMII_CTL_REG(port), config->control);
+
+
+	mask = SGMII_REG_STATUS_LINK;
+
+	if (config->control & SGMII_REG_CONTROL_AUTONEG)
+		mask |= SGMII_REG_STATUS_AUTONEG;
+
+	for (i = 0; i < 1000; i++)  {
+		status = sgmii_read_reg(sgmii_ofs, SGMII_STATUS_REG(port));
+		if ((status & mask) == mask)
+			break;
+	}
+
+	kfree(config);
+	return 0;
+}
+
+#define reg_rmw(addr, value, mask) \
+	__raw_writel(((__raw_readl(addr) & (~(mask))) | (value) ), (addr) )
+
+void serdes_init_6638_156p25Mhz()
+{
+	void __iomem *regs;
+	unsigned int cnt;
+
+	regs = ioremap(0x0232a000, 0x2000);
+
+	reg_rmw(regs + 0x000, 0x00800000, 0xffff0000);
+	reg_rmw(regs + 0x014, 0x00008282, 0x0000ffff);
+	reg_rmw(regs + 0x060, 0x00142438, 0x00ffffff);
+	reg_rmw(regs + 0x064, 0x00c3c700, 0x00ffff00);
+	reg_rmw(regs + 0x078, 0x0000c000, 0x0000ff00);
+	reg_rmw(regs + 0x204, 0x38000080, 0xff0000ff);
+	reg_rmw(regs + 0x208, 0x00000000, 0x000000ff);
+	reg_rmw(regs + 0x20c, 0x02000000, 0xff000000);
+	reg_rmw(regs + 0x210, 0x1b000000, 0xff000000);
+	reg_rmw(regs + 0x214, 0x00006fb8, 0x0000ffff);
+	reg_rmw(regs + 0x218, 0x758000e4, 0xffff00ff);
+	reg_rmw(regs + 0x2ac, 0x00004400, 0x0000ff00);
+	reg_rmw(regs + 0x22c, 0x00100800, 0x00ffff00);
+	reg_rmw(regs + 0x280, 0x00820082, 0x00ff00ff);
+	reg_rmw(regs + 0x284, 0x1D0F0385, 0xFFFFFFFF);
+	reg_rmw(regs + 0x404, 0x38000080, 0xff0000ff);
+	reg_rmw(regs + 0x408, 0x00000000, 0x000000ff);
+	reg_rmw(regs + 0x40c, 0x02000000, 0xff000000);
+	reg_rmw(regs + 0x410, 0x1b000000, 0xff000000);
+	reg_rmw(regs + 0x414, 0x00006FB8, 0x0000FFFF);
+	reg_rmw(regs + 0x418, 0x758000E4, 0xFFFF00FF);
+	reg_rmw(regs + 0x4AC, 0x00004400, 0x0000FF00);
+	reg_rmw(regs + 0x42C, 0x00100800, 0x00FFFF00);
+	reg_rmw(regs + 0x480, 0x00820082, 0x00FF00FF);
+	reg_rmw(regs + 0x484, 0x1D0F0385, 0xFFFFFFFF);
+	reg_rmw(regs + 0x604, 0x38000080, 0xFF0000FF);
+	reg_rmw(regs + 0x608, 0x00000000, 0x000000FF);
+	reg_rmw(regs + 0x60C, 0x02000000, 0xFF000000);
+	reg_rmw(regs + 0x610, 0x1B000000, 0xFF000000);
+	reg_rmw(regs + 0x614, 0x00006FB8, 0x0000FFFF);
+	reg_rmw(regs + 0x618, 0x758000E4, 0xFFFF00FF);
+	reg_rmw(regs + 0x6AC, 0x00004400, 0x0000FF00);
+	reg_rmw(regs + 0x62C, 0x00100800, 0x00FFFF00);
+	reg_rmw(regs + 0x680, 0x00820082, 0x00FF00FF);
+	reg_rmw(regs + 0x684, 0x1D0F0385, 0xFFFFFFFF);
+	reg_rmw(regs + 0x804, 0x38000080, 0xFF0000FF);
+	reg_rmw(regs + 0x808, 0x00000000, 0x000000FF);
+	reg_rmw(regs + 0x80C, 0x02000000, 0xFF000000);
+	reg_rmw(regs + 0x810, 0x1B000000, 0xFF000000);
+	reg_rmw(regs + 0x814, 0x00006FB8, 0x0000FFFF);
+	reg_rmw(regs + 0x818, 0x758000E4, 0xFFFF00FF);
+	reg_rmw(regs + 0x8AC, 0x00004400, 0x0000FF00);
+	reg_rmw(regs + 0x82C, 0x00100800, 0x00FFFF00);
+	reg_rmw(regs + 0x880, 0x00820082, 0x00FF00FF);
+	reg_rmw(regs + 0x884, 0x1D0F0385, 0xFFFFFFFF);
+	reg_rmw(regs + 0xa00, 0x00000800, 0x0000FF00);
+	reg_rmw(regs + 0xa08, 0x38A20000, 0xFFFF0000);
+	reg_rmw(regs + 0xa30, 0x008A8A00, 0x00FFFF00);
+	reg_rmw(regs + 0xa84, 0x00000600, 0x0000FF00);
+	reg_rmw(regs + 0xa94, 0x10000000, 0xFF000000);
+	reg_rmw(regs + 0xaa0, 0x81000000, 0xFF000000);
+	reg_rmw(regs + 0xabc, 0xFF000000, 0xFF000000);
+	reg_rmw(regs + 0xac0, 0x0000008B, 0x000000FF);
+	reg_rmw(regs + 0xb08, 0x583F0000, 0xFFFF0000);
+	reg_rmw(regs + 0xb0c, 0x0000004e, 0x000000FF);
+	reg_rmw(regs + 0x000, 0x00000003, 0x000000FF);
+	reg_rmw(regs + 0xa00, 0x0000005F, 0x000000FF);
+
+	/* Enable TX and RX via the LANExCTL_STS 0x0000 + x*4 */
+	__raw_writel(0xF800F8C0, regs + 0x1fe0);
+	__raw_writel(0xF800F8C0, regs + 0x1fe4);
+	__raw_writel(0xF800F8C0, regs + 0x1fe8);
+	__raw_writel(0xF800F8C0, regs + 0x1fec);
+
+	/*Enable pll via the pll_ctrl 0x0014*/
+	__raw_writel(0xe0000000, regs + 0x1ff4);
+
+	iounmap(regs);
+	regs = ioremap(0x02090000, 0x1000);
+
+	/*Waiting for SGMII Serdes PLL lock.*/
+	for (cnt = 10000;
+	     cnt > 0 && ((__raw_readl(regs + 0x114) & 0x10) == 0);
+	     cnt--);
+
+	for (cnt = 10000;
+	     cnt > 0 && ((__raw_readl(regs + 0x214) & 0x10) == 0);
+	     cnt--);
+
+	for (cnt = 10000;
+	     cnt > 0 && ((__raw_readl(regs + 0x414) & 0x10) == 0);
+	     cnt--);
+
+	for (cnt = 10000;
+	     cnt > 0 && ((__raw_readl(regs + 0x514) & 0x10) == 0);
+	     cnt--);
+
+	iounmap(regs);
+	udelay(200);
+}
diff --git a/drivers/net/ethernet/ti/keystone_xgemdio.c b/drivers/net/ethernet/ti/keystone_xgemdio.c
new file mode 100644
index 0000000..6ef634e
--- /dev/null
+++ b/drivers/net/ethernet/ti/keystone_xgemdio.c
@@ -0,0 +1,525 @@
+/*
+ * Keystone MDIO Module driver
+ *
+ * Copyright (C) 2013 Texas Instruments.
+ *
+ * Shamelessly cloned from davinci_mdio.c, original copyrights follow:
+ *
+ * Copyright (C) 2009 Texas Instruments.
+ *
+ * ---------------------------------------------------------------------------
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ * ---------------------------------------------------------------------------
+ */
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/platform_device.h>
+#include <linux/delay.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/phy.h>
+#include <linux/clk.h>
+#include <linux/err.h>
+#include <linux/io.h>
+#include <linux/pm_runtime.h>
+#include <linux/davinci_emac.h>
+#include <linux/of.h>
+#include <linux/of_mdio.h>
+#include <linux/of_device.h>
+#include "keystone_net.h"
+
+/*
+ * This timeout definition is a worst-case ultra defensive measure against
+ * unexpected controller lock ups.  Ideally, we should never ever hit this
+ * scenario in practice.
+ */
+#define MDIO_TIMEOUT		100 /* msecs */
+
+#define PHY_REG_MASK		0x1f
+#define PHY_ID_MASK		0x1f
+
+#define DEF_OUT_FREQ		2200000		/* 2.2 MHz */
+
+struct keystone_mdiox_regs {
+	u32	version;
+	u32	control;
+#define CONTROL_IDLE		BIT(31)
+#define CONTROL_ENABLE		BIT(30)
+#define CONTROL_MAX_DIV		(0xffff)
+
+	u32	alive;
+	u32	link;
+	u32	linkintraw;
+	u32	linkintmasked;
+	u32	__reserved_0[2];
+	u32	userintraw;
+	u32	userintmasked;
+	u32	userintmaskset;
+	u32	userintmaskclr;
+	u32	__reserved_1[20];
+
+	struct {
+		u32	access;
+#define USERACCESS_GO		BIT(31)
+#define USERACCESS_WRITE	BIT(30)
+#define USERACCESS_ACK		BIT(29)
+#define USERACCESS_READ		(0)
+#define USERACCESS_DATA		(0xffff)
+
+		u32	physel;
+	}	user[0];
+};
+
+struct mdio_platform_data k_default_pdata = {
+	.bus_freq = DEF_OUT_FREQ,
+};
+
+struct keystone_mdiox_data {
+	struct mdio_platform_data pdata;
+	struct keystone_mdiox_regs __iomem *regs;
+	spinlock_t	lock;
+	struct clk	*clk;
+	struct device	*dev;
+	struct mii_bus	*bus;
+	bool		suspended;
+	unsigned long	access_time; /* jiffies */
+};
+
+static void __keystone_mdiox_reset(struct keystone_mdiox_data *data)
+{
+	u32 mdio_in, div, mdio_out_khz, access_time;
+
+	mdio_in = clk_get_rate(data->clk);
+	div = (mdio_in / data->pdata.bus_freq) - 1;
+	if (div > CONTROL_MAX_DIV)
+		div = CONTROL_MAX_DIV;
+
+	/* set enable and clock divider */
+	__raw_writel(div | CONTROL_ENABLE, &data->regs->control);
+
+	/*
+	 * One mdio transaction consists of:
+	 *	32 bits of preamble
+	 *	32 bits of transferred data
+	 *	24 bits of bus yield (not needed unless shared?)
+	 */
+	mdio_out_khz = mdio_in / (1000 * (div + 1));
+	access_time  = (88 * 1000) / mdio_out_khz;
+
+	/*
+	 * In the worst case, we could be kicking off a user-access immediately
+	 * after the mdio bus scan state-machine triggered its own read.  If
+	 * so, our request could get deferred by one access cycle.  We
+	 * defensively allow for 4 access cycles.
+	 */
+	data->access_time = usecs_to_jiffies(access_time * 4);
+	if (!data->access_time)
+		data->access_time = 1;
+}
+
+static int keystone_mdiox_reset(struct mii_bus *bus)
+{
+	struct keystone_mdiox_data *data = bus->priv;
+	u32 phy_mask, ver;
+
+	__keystone_mdiox_reset(data);
+
+	/* wait for scan logic to settle */
+	msleep(PHY_MAX_ADDR * data->access_time);
+
+	/* dump hardware version info */
+	ver = __raw_readl(&data->regs->version);
+	dev_info(data->dev, "keystone mdio revision %d.%d\n",
+		 (ver >> 8) & 0xff, ver & 0xff);
+
+	/* OF explicitly registers phy devices without a bus scan */
+	if (data->dev->of_node)
+		return 0;
+
+	/* get phy mask from the alive register */
+	phy_mask = __raw_readl(&data->regs->alive);
+	if (phy_mask) {
+		/* restrict mdio bus to live phys only */
+		dev_info(data->dev, "detected phy mask %x\n", ~phy_mask);
+		phy_mask = ~phy_mask;
+	} else {
+		/* desperately scan all phys */
+		dev_warn(data->dev, "no live phy, scanning all\n");
+		phy_mask = 0;
+	}
+	data->bus->phy_mask = phy_mask;
+
+	return 0;
+}
+
+/* wait until hardware is ready for another user access */
+static inline int wait_for_user_access(struct keystone_mdiox_data *data)
+{
+	struct keystone_mdiox_regs __iomem *regs = data->regs;
+	unsigned long timeout = jiffies + msecs_to_jiffies(MDIO_TIMEOUT);
+	u32 reg;
+
+	while (time_after(timeout, jiffies)) {
+		reg = __raw_readl(&regs->user[0].access);
+		if ((reg & USERACCESS_GO) == 0)
+			return 0;
+
+		reg = __raw_readl(&regs->control);
+		if ((reg & CONTROL_IDLE) == 0)
+			continue;
+
+		/*
+		 * An emac soft_reset may have clobbered the mdio controller's
+		 * state machine.  We need to reset and retry the current
+		 * operation
+		 */
+		dev_warn(data->dev, "resetting idled controller\n");
+		__keystone_mdiox_reset(data);
+		return -EAGAIN;
+	}
+
+	reg = __raw_readl(&regs->user[0].access);
+	if ((reg & USERACCESS_GO) == 0)
+		return 0;
+
+	dev_err(data->dev, "timed out waiting for user access\n");
+	return -ETIMEDOUT;
+}
+
+/* wait until hardware state machine is idle */
+static inline int wait_for_idle(struct keystone_mdiox_data *data)
+{
+	struct keystone_mdiox_regs __iomem *regs = data->regs;
+	unsigned long timeout = jiffies + msecs_to_jiffies(MDIO_TIMEOUT);
+
+	while (time_after(timeout, jiffies)) {
+		if (__raw_readl(&regs->control) & CONTROL_IDLE)
+			return 0;
+	}
+	dev_err(data->dev, "timed out waiting for idle\n");
+	return -ETIMEDOUT;
+}
+
+static int keystone_mdiox_read(struct mii_bus *bus, int phy_id, int phy_reg)
+{
+	struct keystone_mdiox_data *data = bus->priv;
+	u32 reg;
+	int ret;
+
+	if (phy_reg & ~PHY_REG_MASK || phy_id & ~PHY_ID_MASK)
+		return -EINVAL;
+
+	spin_lock(&data->lock);
+
+	if (data->suspended) {
+		spin_unlock(&data->lock);
+		return -ENODEV;
+	}
+
+	reg = (USERACCESS_GO | USERACCESS_READ | (phy_reg << 21) |
+	       (phy_id << 16));
+
+	while (1) {
+		ret = wait_for_user_access(data);
+		if (ret == -EAGAIN)
+			continue;
+		if (ret < 0)
+			break;
+
+		__raw_writel(reg, &data->regs->user[0].access);
+
+		ret = wait_for_user_access(data);
+		if (ret == -EAGAIN)
+			continue;
+		if (ret < 0)
+			break;
+
+		reg = __raw_readl(&data->regs->user[0].access);
+		ret = (reg & USERACCESS_ACK) ? (reg & USERACCESS_DATA) : -EIO;
+		break;
+	}
+
+	spin_unlock(&data->lock);
+
+	return ret;
+}
+
+static int keystone_mdiox_write(struct mii_bus *bus, int phy_id,
+			      int phy_reg, u16 phy_data)
+{
+	struct keystone_mdiox_data *data = bus->priv;
+	u32 reg;
+	int ret;
+
+	if (phy_reg & ~PHY_REG_MASK || phy_id & ~PHY_ID_MASK)
+		return -EINVAL;
+
+	spin_lock(&data->lock);
+
+	if (data->suspended) {
+		spin_unlock(&data->lock);
+		return -ENODEV;
+	}
+
+	reg = (USERACCESS_GO | USERACCESS_WRITE | (phy_reg << 21) |
+		   (phy_id << 16) | (phy_data & USERACCESS_DATA));
+
+	while (1) {
+		ret = wait_for_user_access(data);
+		if (ret == -EAGAIN)
+			continue;
+		if (ret < 0)
+			break;
+
+		__raw_writel(reg, &data->regs->user[0].access);
+
+		ret = wait_for_user_access(data);
+		if (ret == -EAGAIN)
+			continue;
+		break;
+	}
+
+	spin_unlock(&data->lock);
+
+	return 0;
+}
+
+static int keystone_mdiox_probe_dt(struct mdio_platform_data *data,
+			 struct platform_device *pdev)
+{
+	struct device_node *node = pdev->dev.of_node;
+	u32 prop;
+
+	if (!node)
+		return -EINVAL;
+
+	if (of_property_read_u32(node, "bus_freq", &prop)) {
+		pr_err("Missing bus_freq property in the DT.\n");
+		return -EINVAL;
+	}
+	data->bus_freq = prop;
+
+	return 0;
+}
+
+
+static int keystone_mdiox_probe(struct platform_device *pdev)
+{
+	struct mdio_platform_data *pdata = pdev->dev.platform_data;
+	struct device_node *node = pdev->dev.of_node;
+	struct device *dev = &pdev->dev;
+	struct keystone_mdiox_data *data;
+	struct resource *res;
+	struct phy_device *phy;
+	int ret, addr;
+
+	data = kzalloc(sizeof(*data), GFP_KERNEL);
+	if (!data) {
+		dev_err(dev, "failed to alloc device data\n");
+		return -ENOMEM;
+	}
+
+	data->bus = mdiobus_alloc();
+	if (!data->bus) {
+		dev_err(dev, "failed to alloc mii bus\n");
+		ret = -ENOMEM;
+		goto bail_out;
+	}
+
+	if (dev->of_node) {
+		if (keystone_mdiox_probe_dt(&data->pdata, pdev))
+			data->pdata = k_default_pdata;
+		snprintf(data->bus->id, MII_BUS_ID_SIZE, "%s", pdev->name);
+	} else {
+		data->pdata = pdata ? (*pdata) : k_default_pdata;
+		snprintf(data->bus->id, MII_BUS_ID_SIZE, "%s-%x",
+			 pdev->name, pdev->id);
+	}
+
+	data->bus->name		= dev_name(dev);
+	data->bus->read		= keystone_mdiox_read,
+	data->bus->write	= keystone_mdiox_write,
+	data->bus->reset	= keystone_mdiox_reset,
+	data->bus->parent	= dev;
+	data->bus->priv		= data;
+	data->dev = dev;
+
+	pm_runtime_enable(&pdev->dev);
+	data->clk = clk_get(&pdev->dev, "fck");
+	if (IS_ERR(data->clk)) {
+		dev_err(dev, "failed to get device clock\n");
+		ret = PTR_ERR(data->clk);
+		data->clk = NULL;
+		goto bail_out;
+	}
+
+	clk_prepare(data->clk);
+	pm_runtime_get_sync(&pdev->dev);
+	dev_set_drvdata(dev, data);
+	spin_lock_init(&data->lock);
+
+	xge_serdes_init_156p25Mhz();
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!res) {
+		dev_err(dev, "could not find register map resource\n");
+		ret = -ENOENT;
+		goto bail_out;
+	}
+
+	res = devm_request_mem_region(dev, res->start, resource_size(res),
+					    dev_name(dev));
+	if (!res) {
+		dev_err(dev, "could not allocate register map resource\n");
+		ret = -ENXIO;
+		goto bail_out;
+	}
+
+	data->regs = devm_ioremap_nocache(dev, res->start, resource_size(res));
+	if (!data->regs) {
+		dev_err(dev, "could not map mdio registers\n");
+		ret = -ENOMEM;
+		goto bail_out;
+	}
+
+	/* register the mii bus */
+	ret = of_mdiobus_register(data->bus, node);
+	if (ret)
+		goto bail_out;
+
+	/* scan and dump the bus */
+	for (addr = 0; addr < PHY_MAX_ADDR; addr++) {
+		phy = data->bus->phy_map[addr];
+		if (phy) {
+			dev_info(dev, "phy[%d]: device %s, driver %s\n",
+				 phy->addr, dev_name(&phy->dev),
+				 phy->drv ? phy->drv->name : "unknown");
+		}
+	}
+	return 0;
+
+bail_out:
+	if (data->bus)
+		mdiobus_free(data->bus);
+
+	if (data->clk)
+		clk_put(data->clk);
+	pm_runtime_put_sync(&pdev->dev);
+	pm_runtime_disable(&pdev->dev);
+
+	kfree(data);
+
+	return ret;
+
+}
+
+static int keystone_mdiox_remove(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct keystone_mdiox_data *data = dev_get_drvdata(dev);
+
+	if (data->bus) {
+		mdiobus_unregister(data->bus);
+		mdiobus_free(data->bus);
+	}
+
+	if (data->clk)
+		clk_put(data->clk);
+	pm_runtime_put_sync(&pdev->dev);
+	pm_runtime_disable(&pdev->dev);
+
+	dev_set_drvdata(dev, NULL);
+
+	kfree(data);
+
+	return 0;
+}
+
+static int keystone_mdiox_suspend(struct device *dev)
+{
+	struct keystone_mdiox_data *data = dev_get_drvdata(dev);
+	u32 ctrl;
+
+	spin_lock(&data->lock);
+
+	/* shutdown the scan state machine */
+	ctrl = __raw_readl(&data->regs->control);
+	ctrl &= ~CONTROL_ENABLE;
+	__raw_writel(ctrl, &data->regs->control);
+	wait_for_idle(data);
+
+	pm_runtime_put_sync(data->dev);
+
+	data->suspended = true;
+	spin_unlock(&data->lock);
+
+	return 0;
+}
+
+static int keystone_mdiox_resume(struct device *dev)
+{
+	struct keystone_mdiox_data *data = dev_get_drvdata(dev);
+	u32 ctrl;
+
+	spin_lock(&data->lock);
+	pm_runtime_put_sync(data->dev);
+
+	/* restart the scan state machine */
+	ctrl = __raw_readl(&data->regs->control);
+	ctrl |= CONTROL_ENABLE;
+	__raw_writel(ctrl, &data->regs->control);
+
+	data->suspended = false;
+	spin_unlock(&data->lock);
+
+	return 0;
+}
+
+static const struct dev_pm_ops keystone_mdiox_pm_ops = {
+	.suspend	= keystone_mdiox_suspend,
+	.resume		= keystone_mdiox_resume,
+};
+
+static const struct of_device_id keystone_mdiox_of_mtable[] = {
+	{ .compatible = "ti,keystone_mdiox", },
+	{ /* sentinel */ },
+};
+
+static struct platform_driver keystone_mdiox_driver = {
+	.driver = {
+		.name	 = "keystone_mdiox",
+		.owner	 = THIS_MODULE,
+		.pm	 = &keystone_mdiox_pm_ops,
+		.of_match_table = of_match_ptr(keystone_mdiox_of_mtable),
+	},
+	.probe = keystone_mdiox_probe,
+	.remove = keystone_mdiox_remove,
+};
+
+static int __init keystone_mdiox_init(void)
+{
+	return platform_driver_register(&keystone_mdiox_driver);
+}
+device_initcall(keystone_mdiox_init);
+
+static void __exit keystone_mdiox_exit(void)
+{
+	platform_driver_unregister(&keystone_mdiox_driver);
+}
+module_exit(keystone_mdiox_exit);
+
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("Keystone XGE MDIO driver");
diff --git a/drivers/net/ethernet/ti/keystone_xgepcsr.c b/drivers/net/ethernet/ti/keystone_xgepcsr.c
new file mode 100644
index 0000000..37b074c
--- /dev/null
+++ b/drivers/net/ethernet/ti/keystone_xgepcsr.c
@@ -0,0 +1,268 @@
+/*
+ * Copyright (C) 2012 Texas Instruments Incorporated
+ * Author: WingMan Kwok <w-kwok2@ti.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation version 2.
+ *
+ * This program is distributed "as is" WITHOUT ANY WARRANTY of any
+ * kind, whether express or implied; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/io.h>
+#include <linux/err.h>
+#include <linux/delay.h>
+
+#include "keystone_net.h"
+
+#define XGE_SERDES_BASE		0x0231E000
+#define XGE_SERDES_SIZE		0x2000
+
+#define XGE_SGMII_1_STATUS	0x02f00114
+#define XGE_SGMII_2_STATUS	0x02f00214
+
+#define PCSR_OFFSET(x)	((x == 0) ? (0x600) : (0x680))
+
+/*
+ * PCS-R registers
+ */
+#define PCSR_TX_CTL(x)		(PCSR_OFFSET(x) + 0x00)
+#define PCSR_TX_STATUS(x)	(PCSR_OFFSET(x) + 0x04)
+#define PCSR_RX_CTL(x)		(PCSR_OFFSET(x) + 0x08)
+#define PCSR_RX_STATUS(x)	(PCSR_OFFSET(x) + 0x0C)
+
+#define PCSR_SEED_A_LO(x)	(PCSR_OFFSET(x) + 0x10)
+#define PCSR_SEED_A_HI(x)	(PCSR_OFFSET(x) + 0x14)
+#define PCSR_SEED_B_LO(x)	(PCSR_OFFSET(x) + 0x18)
+#define PCSR_SEED_B_HI(x)	(PCSR_OFFSET(x) + 0x1C)
+
+#define PCSR_FEC(x)		(PCSR_OFFSET(x) + 0x20)
+#define PCSR_CTL(x)		(PCSR_OFFSET(x) + 0x24)
+#define PCSR_RX_FEC_CNT(x)	(PCSR_OFFSET(x) + 0x28)
+#define PCSR_RX_ERR_FIFO(x)	(PCSR_OFFSET(x) + 0x2C)
+
+#define PCSR_SIGNAL_OK_EN	BIT(1)
+
+#define reg_rmw(addr, value, mask) \
+	__raw_writel(((__raw_readl(addr) & (~(mask))) | (value)), (addr))
+
+struct serdes_cfg {
+	u32 ofs;
+	u32 val;
+	u32 mask;
+};
+
+struct serdes_cfg cfg_cmu0_1g_slow[] = {
+	{0x0000, 0x00800002, 0x00ff00ff},
+	{0x0014, 0x00003838, 0x0000ffff},
+	{0x0060, 0x1c44e438, 0xffffffff},
+	{0x0064, 0x00c18400, 0x00ffffff},
+	{0x0068, 0x17078200, 0xffffff00},
+	{0x006c, 0x00000014, 0x000000ff},
+	{0x0078, 0x0000c000, 0x0000ff00},
+	{0x0000, 0x00000003, 0x000000ff}
+};
+
+struct serdes_cfg cfg_cmu1_10g_slow[] = {
+	{0x0c00, 0x00030002, 0x00ff00ff},
+	{0x0c14, 0x00005252, 0x0000ffff},
+	{0x0c28, 0x80000000, 0xff000000},
+	{0x0c2c, 0x000000f6, 0x000000ff},
+	{0x0c3c, 0x04000405, 0xff00ffff},
+	{0x0c40, 0xc0800000, 0xffff0000},
+	{0x0c44, 0x5a202062, 0xffffffff},
+	{0x0c48, 0x40040424, 0xffffffff},
+	{0x0c4c, 0x00004002, 0x0000ffff},
+	{0x0c50, 0x19001c00, 0xff00ff00},
+	{0x0c54, 0x00002100, 0x0000ff00},
+	{0x0c58, 0x00000060, 0x000000ff},
+	{0x0c60, 0x80131e7c, 0xffffffff},
+	{0x0c64, 0x00008b02, 0x0000ffff},
+	{0x0c68, 0x17078200, 0xffffff00},
+	{0x0c6c, 0x0000001a, 0x000000ff},
+	{0x0c74, 0x00000400, 0x0000ff00},
+	{0x0c78, 0x0000c000, 0x0000ff00},
+	{0x0c00, 0x00000003, 0x000000ff}
+};
+
+struct serdes_cfg cfg_lane1_10g_slow[] = {
+	{0x0204, 0x00000080, 0x000000ff},
+	{0x0208, 0x0000920d, 0x0000ffff},
+	{0x0204, 0xfc000000, 0xff000000},
+	{0x0208, 0x00009104, 0x0000ffff},
+	{0x0210, 0x1a000000, 0xff000000},
+	/* 58 Lane address 0x14 pma_ln_ctrl[32] should be set to 1.
+	   This is dlpf_div2_ena bit and controls the clock speed for
+	   updating the DLPF integral path in the CDR.
+	*/
+	{0x0214, 0x0000b65c, 0x00ffffff},
+	{0x0218, 0x75800084, 0xffff00ff},
+	{0x022c, 0x00300000, 0x00ff0000},
+	{0x0230, 0x00003800, 0x0000ff00},
+	{0x024c, 0x008f0000, 0x00ff0000},
+	{0x0250, 0x30000000, 0xff000000},
+	{0x0260, 0x00000002, 0x000000ff},
+	{0x0264, 0x00000057, 0x000000ff},
+	{0x0268, 0x00575700, 0x00ffff00},
+	{0x0278, 0xff000000, 0xff000000},
+	{0x0280, 0x00500050, 0x00ff00ff},
+	{0x0284, 0x00001f15, 0x0000ffff},
+	{0x028c, 0x00006f00, 0x0000ff00},
+	{0x0294, 0x00000000, 0xffffff00},
+	{0x0298, 0x00002640, 0xff00ffff},
+	{0x029c, 0x00000003, 0x000000ff},
+	{0x02a4, 0x00000f13, 0x0000ffff},
+	{0x02a8, 0x0001b600, 0x00ffff00},
+	/* CJT, sb=0x30, disable aneg */
+	{0x0380, 0x00000020, 0x000000ff},
+	/* CJT, sb=0x02, disable training */
+	{0x03c0, 0x00000000, 0x0000ff00},
+	{0x03cc, 0x00000018, 0x000000ff},
+	{0x03cc, 0x00000000, 0x000000ff},
+};
+
+struct serdes_cfg cfg_lane2_10g_slow[] = {
+	{0x0404, 0x00000080, 0x000000ff},
+	{0x0408, 0x0000920d, 0x0000ffff},
+	{0x0404, 0xfc000000, 0xff000000},
+	{0x0408, 0x00009104, 0x0000ffff},
+	{0x0410, 0x1a000000, 0xff000000},
+	/* 58 Lane address 0x14 pma_ln_ctrl[32] should be set to 1.
+	   This is dlpf_div2_ena bit and controls the clock speed
+	   for updating the DLPF integral path in the CDR.
+	*/
+	{0x0414, 0x0000b6b5c, 0x00ffffff},
+	{0x0418, 0x75800084, 0xffff00ff},
+	{0x042c, 0x00300000, 0x00ff0000},
+	{0x0430, 0x00003800, 0x0000ff00},
+	{0x044c, 0x008f0000, 0x00ff0000},
+	{0x0450, 0x30000000, 0xff000000},
+	{0x0460, 0x00000002, 0x000000ff},
+	{0x0464, 0x00000057, 0x000000ff},
+	{0x0468, 0x00575700, 0x00ffff00},
+	{0x0478, 0xff000000, 0xff000000},
+	{0x0480, 0x00500050, 0x00ff00ff},
+	{0x0484, 0x00001f15, 0x0000ffff},
+	{0x048c, 0x00006f00, 0x0000ff00},
+	{0x0494, 0x00000000, 0xffffff00},
+	{0x0498, 0x00002640, 0xff00ffff},
+	{0x049c, 0x00000003, 0x000000ff},
+	{0x04a4, 0x00000f13, 0x0000ffff},
+	{0x04a8, 0x0001b600, 0x00ffff00},
+	/* CJT, sb=0x30, disable aneg */
+	{0x0580, 0x00000020, 0x000000ff},
+	/* CJT, sb=0x30, disable training */
+	{0x05c0, 0x00000000, 0x0000ff00},
+	{0x05cc, 0x00000018, 0x000000ff},
+	{0x05cc, 0x00000000, 0x000000ff}
+};
+
+struct serdes_cfg cfg_comlane_10g_slow[] = {
+	{0x0a00, 0x00000800, 0x0000ff00},
+	{0x0a84, 0x00000000, 0x000000ff},
+	{0x0a8c, 0x00130000, 0x00ff0000},
+	{0x0a90, 0x77a00000, 0xffff0000},
+	{0x0a94, 0x00007777, 0x0000ffff},
+	{0x0b08, 0x000f0000, 0xffff0000},
+	{0x0b0c, 0x000f0000, 0x00ffffff},
+	{0x0b10, 0xbe000000, 0xff000000},
+	{0x0b14, 0x000000ff, 0x000000ff},
+	{0x0b18, 0x00000014, 0x000000ff},
+	{0x0b5c, 0x981b0000, 0xffff0000},
+	{0x0b64, 0x00001100, 0x0000ff00},
+	{0x0b78, 0x00000c00, 0x0000ff00},
+	{0x0abc, 0xff000000, 0xff000000},
+	{0x0ac0, 0x0000008b, 0x000000ff},
+};
+
+void xge_serdes_10gbps_setup_phy_b(void)
+{
+	void __iomem *regs, *sgmii_1_status, *sgmii_2_status;
+	u32 i, val;
+
+	regs = ioremap(XGE_SERDES_BASE, XGE_SERDES_SIZE);
+	sgmii_1_status = ioremap(XGE_SGMII_1_STATUS, 4);
+	sgmii_2_status = ioremap(XGE_SGMII_2_STATUS, 4);
+
+	/* preamble */
+	__raw_writel(37500, regs + 0x1ffc);
+
+	/* cmu0_1g_slow */
+	for (i = 0; i < ARRAY_SIZE(cfg_cmu0_1g_slow); i++) {
+		reg_rmw(regs + cfg_cmu0_1g_slow[i].ofs,
+			cfg_cmu0_1g_slow[i].val,
+			cfg_cmu0_1g_slow[i].mask);
+	}
+
+	/* cmu1_10g_slow */
+	for (i = 0; i < ARRAY_SIZE(cfg_cmu1_10g_slow); i++) {
+		reg_rmw(regs + cfg_cmu1_10g_slow[i].ofs,
+			cfg_cmu1_10g_slow[i].val,
+			cfg_cmu1_10g_slow[i].mask);
+	}
+
+	/* lane1_10g_slow */
+	for (i = 0; i < ARRAY_SIZE(cfg_lane1_10g_slow); i++) {
+		reg_rmw(regs + cfg_lane1_10g_slow[i].ofs,
+			cfg_lane1_10g_slow[i].val,
+			cfg_lane1_10g_slow[i].mask);
+	}
+
+	/* lane2_10g_slow */
+	for (i = 0; i < ARRAY_SIZE(cfg_lane2_10g_slow); i++) {
+		reg_rmw(regs + cfg_lane2_10g_slow[i].ofs,
+			cfg_lane2_10g_slow[i].val,
+			cfg_lane2_10g_slow[i].mask);
+	}
+
+	/* comlane_10g_slow */
+	for (i = 0; i < ARRAY_SIZE(cfg_comlane_10g_slow); i++) {
+		reg_rmw(regs + cfg_comlane_10g_slow[i].ofs,
+			cfg_comlane_10g_slow[i].val,
+			cfg_comlane_10g_slow[i].mask);
+	}
+
+	/* reset_clr */
+	reg_rmw(regs + 0x0a00, 0x0000005f, 0x000000ff);
+
+	/*Enable pll via the pll_ctrl 0x0014*/
+	__raw_writel(0xee000000, regs + 0x1ff4);
+
+	/* Enable TX and RX via the LANExCTL_STS 0x0000 + x*4 */
+	/* Full Rate mode, 16b width */
+	__raw_writel(0xe0e9e038, regs + 0x1fe0);
+	__raw_writel(0xe0e9e038, regs + 0x1fe4);
+	udelay(200);
+
+	/* Wait for SGMII Serdes PLL lock */
+	do {
+		val = __raw_readl(sgmii_1_status);
+	} while ((val & 0x10) != 0x10);
+
+	do {
+		/* sgmii_2 status */
+	} while ((val & 0x10) != 0x10);
+
+	iounmap(regs);
+}
+
+static int keystone_xge_serdes_configured;  /* FIXME */
+
+void xge_serdes_init_156p25Mhz(void)
+{
+	/* Serdes should only be configured once */
+	if (keystone_xge_serdes_configured)
+		return;
+
+	xge_serdes_10gbps_setup_phy_b();
+	keystone_xge_serdes_configured = 1;
+}
+
+int keystone_pcsr_config(void __iomem *pcsr_ofs, int port, u32 interface)
+{
+	return 0;
+}
diff --git a/drivers/net/ethernet/ti/keystone_xgess.c b/drivers/net/ethernet/ti/keystone_xgess.c
new file mode 100644
index 0000000..47173de
--- /dev/null
+++ b/drivers/net/ethernet/ti/keystone_xgess.c
@@ -0,0 +1,2524 @@
+/*
+ * Copyright (C) 2012 Texas Instruments Incorporated
+ * Authors: Sandeep Paulraj <s-paulraj@ti.com>
+ * Authors: WingMan Kwok <w-kwok2@ti.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation version 2.
+ *
+ * This program is distributed "as is" WITHOUT ANY WARRANTY of any
+ * kind, whether express or implied; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+#include <linux/io.h>
+#include <linux/of.h>
+#include <linux/clk.h>
+#include <linux/phy.h>
+#include <linux/timer.h>
+#include <linux/module.h>
+#include <linux/device.h>
+#include <linux/uaccess.h>
+#include <linux/if_vlan.h>
+#include <linux/of_mdio.h>
+#include <linux/ethtool.h>
+#include <linux/if_ether.h>
+#include <linux/net_tstamp.h>
+#include <linux/netdevice.h>
+#include <linux/interrupt.h>
+#include <linux/dmaengine.h>
+#include <linux/dma-mapping.h>
+#include <linux/scatterlist.h>
+#include <linux/etherdevice.h>
+#include <linux/platform_device.h>
+
+#include "cpsw_ale.h"
+#include "keystone_net.h"
+
+#define CPSW_MODULE_NAME	"keystone-cpswx"
+#define NETCP_DRIVER_NAME	"TI KeyStone XGE Driver"
+#define NETCP_DRIVER_VERSION	"v0.0.0"
+
+#define CPSW_IDENT(reg)			((reg >> 16) & 0xffff)
+#define CPSW_MAJOR_VERSION(reg)		(reg >> 8 & 0x7)
+#define CPSW_MINOR_VERSION(reg)		(reg & 0xff)
+#define CPSW_RTL_VERSION(reg)		((reg >> 11) & 0x1f)
+
+#define DEVICE_N_GMACSL_PORTS			2
+#define DEVICE_EMACSL_RESET_POLL_COUNT		100
+
+#define	CPSW_TIMER_INTERVAL			(HZ / 10)
+
+/* Soft reset register values */
+#define SOFT_RESET_MASK				BIT(0)
+#define SOFT_RESET				BIT(0)
+
+#define MACSL_RX_ENABLE_CSF			BIT(23)
+#define MACSL_RX_ENABLE_EXT_CTL			BIT(18)
+#define MACSL_XGMII_ENABLE			BIT(13)
+#define MACSL_XGIG_MODE				BIT(8)
+#define MACSL_GMII_ENABLE			BIT(5)
+#define GMACSL_RET_WARN_RESET_INCOMPLETE	-2
+
+#define SLAVE_LINK_IS_XGMII(s) \
+	((s)->link_interface >= XGMII_LINK_MAC_PHY)
+
+#define MACSL_SIG_ENABLE(s) \
+		(SLAVE_LINK_IS_XGMII((s)) ?   \
+		(MACSL_XGMII_ENABLE | MACSL_XGIG_MODE) : \
+		MACSL_GMII_ENABLE)
+
+#define CPSW_NUM_PORTS		                3
+#define CPSW_CTL_P0_ENABLE			BIT(2)
+#define CPSW_CTL_VLAN_AWARE			BIT(1)
+#define CPSW_REG_VAL_STAT_ENABLE_ALL		0xf
+
+#define CPSW_MASK_ALL_PORTS			7
+#define CPSW_MASK_PHYS_PORTS			6
+#define CPSW_MASK_NO_PORTS			0
+#define CPSW_NON_VLAN_ADDR			-1
+
+#define CPSW_STATS0_MODULE			0
+#define CPSW_STATS1_MODULE			1
+#define CPSW_STATS2_MODULE			2
+
+#define MAX_SIZE_STREAM_BUFFER		        9504
+
+struct cpswx_slave {
+	struct cpswx_slave_regs __iomem		*regs;
+	struct cpswx_sliver_regs __iomem	*sliver;
+	int				 slave_num;
+	int				 port_num;
+	u32				 mac_control;
+	struct phy_device		*phy;
+	const char			*phy_id;
+	struct cpsw_ale			*ale;
+	u32				 link_interface;
+	u8				 phy_port_t;
+};
+
+/* 0x0000 */
+struct cpswx_ss_regs {
+	u32	id_ver;
+	u32	synce_count;
+	u32	synce_mux;
+	u32	control;
+};
+
+/* 0x1000 */
+struct cpswx_regs {
+	u32	id_ver;
+	u32	control;
+	u32	emcontrol;
+	u32	stat_port_en;
+	u32	ptype;
+	u32	soft_idle;
+	u32	thru_rate;
+	u32	gap_thresh;
+	u32	tx_start_wds;
+	u32	flow_control;
+	u32	cppi_thresh;
+};
+
+/* 0x1064, 0x1094 */
+struct cpswx_slave_regs {
+	u32	blk_cnt;
+	u32	port_vlan;
+	u32	tx_pri_map;
+	u32	sa_lo;
+	u32	sa_hi;
+	u32	ts_ctl;
+	u32	ts_seq_ltype;
+	u32	ts_vlan;
+	u32	ts_ctl_ltype2;
+	u32	ts_ctl2;
+	u32	control;
+};
+
+/* 0x1034 */
+struct cpswx_host_regs {
+	u32	blk_cnt;
+	u32	port_vlan;
+	u32	tx_pri_map;
+	u32	src_id;
+	u32	rx_pri_map;
+	u32	rx_maxlen;
+};
+
+/* 0x1400, 0x1440 */
+struct cpswx_sliver_regs {
+	u32	id_ver;
+	u32	mac_control;
+	u32	mac_status;
+	u32	soft_reset;
+	u32	rx_maxlen;
+	u32	__reserved_0;
+	u32	rx_pause;
+	u32	tx_pause;
+	u32	em_control;
+	u32	__reserved_1;
+	u32	tx_gap;
+	u32	rsvd[4];
+};
+
+struct cpswx_host_hw_stats {
+	u32	rx_good_frames;
+	u32	rx_broadcast_frames;
+	u32	rx_multicast_frames;
+	u32	__rsvd_0[3];
+	u32	rx_oversized_frames;
+	u32	__rsvd_1;
+	u32	rx_undersized_frames;
+	u32	__rsvd_2;
+	u32	overrun_type4;
+	u32	overrun_type5;
+	u32	rx_bytes;
+	u32	tx_good_frames;
+	u32	tx_broadcast_frames;
+	u32	tx_multicast_frames;
+	u32	__rsvd_3[9];
+	u32	tx_bytes;
+	u32	tx_64byte_frames;
+	u32	tx_65_to_127byte_frames;
+	u32	tx_128_to_255byte_frames;
+	u32	tx_256_to_511byte_frames;
+	u32	tx_512_to_1023byte_frames;
+	u32	tx_1024byte_frames;
+	u32	net_bytes;
+	u32	rx_sof_overruns;
+	u32	rx_mof_overruns;
+	u32	rx_dma_overruns;
+};
+
+/* 0x1900, 0x1a00 */
+struct cpswx_hw_stats {
+	u32	rx_good_frames;
+	u32	rx_broadcast_frames;
+	u32	rx_multicast_frames;
+	u32	rx_pause_frames;
+	u32	rx_crc_errors;
+	u32	rx_align_code_errors;
+	u32	rx_oversized_frames;
+	u32	rx_jabber_frames;
+	u32	rx_undersized_frames;
+	u32	rx_fragments;
+	u32	overrun_type4;
+	u32	overrun_type5;
+	u32	rx_bytes;
+	u32	tx_good_frames;
+	u32	tx_broadcast_frames;
+	u32	tx_multicast_frames;
+	u32	tx_pause_frames;
+	u32	tx_deferred_frames;
+	u32	tx_collision_frames;
+	u32	tx_single_coll_frames;
+	u32	tx_mult_coll_frames;
+	u32	tx_excessive_collisions;
+	u32	tx_late_collisions;
+	u32	tx_underrun;
+	u32	tx_carrier_sense_errors;
+	u32	tx_bytes;
+	u32	tx_64byte_frames;
+	u32	tx_65_to_127byte_frames;
+	u32	tx_128_to_255byte_frames;
+	u32	tx_256_to_511byte_frames;
+	u32	tx_512_to_1023byte_frames;
+	u32	tx_1024byte_frames;
+	u32	net_bytes;
+	u32	rx_sof_overruns;
+	u32	rx_mof_overruns;
+	u32	rx_dma_overruns;
+};
+
+/* 0x1700 */
+struct cpswx_ale_regs {
+	u32	ale_idver;
+	u32	__rsvd0;
+	u32	ale_control;
+	u32	__rsvd1;
+	u32	ale_prescale;
+	u32	ale_aging_timer;  /* +++FIXME: no description found in manual */
+	u32	ale_unknown_vlan;
+	u32	__rsvd3;
+	u32	ale_tblctl;
+	u32	__rsvd4[4];
+	u32	ale_tblw2;
+	u32	ale_tblw1;
+	u32	ale_tblw0;
+	u32	ale_portctl[3];
+};
+
+struct cpswx_priv {
+	struct device			*dev;
+	struct clk			*clk;
+	struct netcp_device		*netcp_device;
+	u32				 num_slaves;
+	u32				 ale_ageout;
+	u32				 ale_entries;
+	u32				 ale_ports;
+	u32				 sgmii_module_ofs;
+	u32				 pcsr_module_ofs;
+	u32				 switch_module_ofs;
+	u32				 host_port_reg_ofs;
+	u32				 slave_reg_ofs;
+	u32				 sliver_reg_ofs;
+	u32				 hw_stats_reg_ofs;
+	u32				 ale_reg_ofs;
+
+	int				 host_port;
+	u32				 rx_packet_max;
+
+	struct cpswx_regs __iomem		*regs;
+	struct cpswx_ss_regs __iomem		*ss_regs;
+	struct cpswx_host_hw_stats __iomem	*host_hw_stats_regs;
+	struct cpswx_hw_stats __iomem		*hw_stats_regs[2];
+	struct cpswx_host_regs __iomem		*host_port_regs;
+	struct cpswx_ale_regs __iomem		*ale_reg;
+
+	void __iomem				*sgmii_port_regs;
+	void __iomem				*pcsr_port_regs;
+
+	struct cpsw_ale				*ale;
+	u32				 ale_refcnt;
+
+	u32				 link[5];
+	struct device_node		*phy_node[4];
+
+	u32				 intf_tx_queues;
+
+	u32				 multi_if;
+	u32				 slaves_per_interface;
+	u32				 num_interfaces;
+	struct device_node		*interfaces;
+	struct list_head		 cpsw_intf_head;
+
+	u64				 hw_stats[96];
+	int				 init_serdes_at_probe;
+	struct kobject			kobj;
+	struct kobject			tx_pri_kobj;
+	struct kobject			pvlan_kobj;
+	struct kobject			stats_kobj;
+	struct mutex			hw_stats_lock;
+};
+
+struct cpswx_intf {
+	struct net_device	*ndev;
+	struct device		*dev;
+	struct cpswx_priv	*cpsw_priv;
+	struct device_node	*phy_node;
+	u32			 num_slaves;
+	u32			 slave_port;
+	struct cpswx_slave	*slaves;
+	u32			 intf_tx_queues;
+	const char		*tx_chan_name;
+	u32			 tx_queue_depth;
+	struct netcp_tx_pipe	 tx_pipe;
+	u32			 multi_if;
+	struct list_head	 cpsw_intf_list;
+	struct timer_list	 timer;
+	u32			 sgmii_link;
+	unsigned long		 active_vlans[BITS_TO_LONGS(VLAN_N_VID)];
+};
+
+/*
+ * Statistic management
+ */
+struct netcp_ethtool_stat {
+	char desc[ETH_GSTRING_LEN];
+	int type;
+	u32 size;
+	int offset;
+};
+
+/* +++FIXME: do we need the port?? */
+#define for_each_slave(priv, func, arg...)				\
+	do {								\
+		int idx, port;						\
+		port = (priv)->slave_port;				\
+		if ((priv)->multi_if)					\
+			(func)((priv)->slaves, ##arg);			\
+		else							\
+			for (idx = 0; idx < (priv)->num_slaves; idx++)	\
+				(func)((priv)->slaves + idx, ##arg);	\
+	} while (0)
+
+#define FIELDINFO(_struct, field) FIELD_SIZEOF(_struct, field), \
+					offsetof(_struct, field)
+
+#define CPSW_STATS0_INFO(field)	"CPSW_0:"#field, CPSW_STATS0_MODULE, \
+				FIELDINFO(struct cpswx_host_hw_stats, field)
+
+#define CPSW_STATSA_INFO(field)	"CPSW_1:"#field, CPSW_STATS1_MODULE, \
+				FIELDINFO(struct cpswx_hw_stats, field)
+#define CPSW_STATSB_INFO(field)	"CPSW_2:"#field, CPSW_STATS2_MODULE, \
+				FIELDINFO(struct cpswx_hw_stats, field)
+
+static const struct netcp_ethtool_stat et_stats[] = {
+	/* CPSW module 0 */
+	{CPSW_STATS0_INFO(rx_good_frames)},
+	{CPSW_STATS0_INFO(rx_broadcast_frames)},
+	{CPSW_STATS0_INFO(rx_multicast_frames)},
+	{CPSW_STATS0_INFO(rx_oversized_frames)},
+	{CPSW_STATS0_INFO(rx_undersized_frames)},
+	{CPSW_STATS0_INFO(overrun_type4)},
+	{CPSW_STATS0_INFO(overrun_type5)},
+	{CPSW_STATS0_INFO(rx_bytes)},
+	{CPSW_STATS0_INFO(tx_good_frames)},
+	{CPSW_STATS0_INFO(tx_broadcast_frames)},
+	{CPSW_STATS0_INFO(tx_multicast_frames)},
+	{CPSW_STATS0_INFO(tx_bytes)},
+	{CPSW_STATS0_INFO(tx_64byte_frames)},
+	{CPSW_STATS0_INFO(tx_65_to_127byte_frames)},
+	{CPSW_STATS0_INFO(tx_128_to_255byte_frames)},
+	{CPSW_STATS0_INFO(tx_256_to_511byte_frames)},
+	{CPSW_STATS0_INFO(tx_512_to_1023byte_frames)},
+	{CPSW_STATS0_INFO(tx_1024byte_frames)},
+	{CPSW_STATS0_INFO(net_bytes)},
+	{CPSW_STATS0_INFO(rx_sof_overruns)},
+	{CPSW_STATS0_INFO(rx_mof_overruns)},
+	{CPSW_STATS0_INFO(rx_dma_overruns)},
+	/* CPSW module 1 */
+	{CPSW_STATSA_INFO(rx_good_frames)},
+	{CPSW_STATSA_INFO(rx_broadcast_frames)},
+	{CPSW_STATSA_INFO(rx_multicast_frames)},
+	{CPSW_STATSA_INFO(rx_pause_frames)},
+	{CPSW_STATSA_INFO(rx_crc_errors)},
+	{CPSW_STATSA_INFO(rx_align_code_errors)},
+	{CPSW_STATSA_INFO(rx_oversized_frames)},
+	{CPSW_STATSA_INFO(rx_jabber_frames)},
+	{CPSW_STATSA_INFO(rx_undersized_frames)},
+	{CPSW_STATSA_INFO(rx_fragments)},
+	{CPSW_STATSA_INFO(overrun_type4)},
+	{CPSW_STATSA_INFO(overrun_type5)},
+	{CPSW_STATSA_INFO(rx_bytes)},
+	{CPSW_STATSA_INFO(tx_good_frames)},
+	{CPSW_STATSA_INFO(tx_broadcast_frames)},
+	{CPSW_STATSA_INFO(tx_multicast_frames)},
+	{CPSW_STATSA_INFO(tx_pause_frames)},
+	{CPSW_STATSA_INFO(tx_deferred_frames)},
+	{CPSW_STATSA_INFO(tx_collision_frames)},
+	{CPSW_STATSA_INFO(tx_single_coll_frames)},
+	{CPSW_STATSA_INFO(tx_mult_coll_frames)},
+	{CPSW_STATSA_INFO(tx_excessive_collisions)},
+	{CPSW_STATSA_INFO(tx_late_collisions)},
+	{CPSW_STATSA_INFO(tx_underrun)},
+	{CPSW_STATSA_INFO(tx_carrier_sense_errors)},
+	{CPSW_STATSA_INFO(tx_bytes)},
+	{CPSW_STATSA_INFO(tx_64byte_frames)},
+	{CPSW_STATSA_INFO(tx_65_to_127byte_frames)},
+	{CPSW_STATSA_INFO(tx_128_to_255byte_frames)},
+	{CPSW_STATSA_INFO(tx_256_to_511byte_frames)},
+	{CPSW_STATSA_INFO(tx_512_to_1023byte_frames)},
+	{CPSW_STATSA_INFO(tx_1024byte_frames)},
+	{CPSW_STATSA_INFO(net_bytes)},
+	{CPSW_STATSA_INFO(rx_sof_overruns)},
+	{CPSW_STATSA_INFO(rx_mof_overruns)},
+	{CPSW_STATSA_INFO(rx_dma_overruns)},
+	/* CPSW module 2 */
+	{CPSW_STATSB_INFO(rx_good_frames)},
+	{CPSW_STATSB_INFO(rx_broadcast_frames)},
+	{CPSW_STATSB_INFO(rx_multicast_frames)},
+	{CPSW_STATSB_INFO(rx_pause_frames)},
+	{CPSW_STATSB_INFO(rx_crc_errors)},
+	{CPSW_STATSB_INFO(rx_align_code_errors)},
+	{CPSW_STATSB_INFO(rx_oversized_frames)},
+	{CPSW_STATSB_INFO(rx_jabber_frames)},
+	{CPSW_STATSB_INFO(rx_undersized_frames)},
+	{CPSW_STATSB_INFO(rx_fragments)},
+	{CPSW_STATSB_INFO(overrun_type4)},
+	{CPSW_STATSB_INFO(overrun_type5)},
+	{CPSW_STATSB_INFO(rx_bytes)},
+	{CPSW_STATSB_INFO(tx_good_frames)},
+	{CPSW_STATSB_INFO(tx_broadcast_frames)},
+	{CPSW_STATSB_INFO(tx_multicast_frames)},
+	{CPSW_STATSB_INFO(tx_pause_frames)},
+	{CPSW_STATSB_INFO(tx_deferred_frames)},
+	{CPSW_STATSB_INFO(tx_collision_frames)},
+	{CPSW_STATSB_INFO(tx_single_coll_frames)},
+	{CPSW_STATSB_INFO(tx_mult_coll_frames)},
+	{CPSW_STATSB_INFO(tx_excessive_collisions)},
+	{CPSW_STATSB_INFO(tx_late_collisions)},
+	{CPSW_STATSB_INFO(tx_underrun)},
+	{CPSW_STATSB_INFO(tx_carrier_sense_errors)},
+	{CPSW_STATSB_INFO(tx_bytes)},
+	{CPSW_STATSB_INFO(tx_64byte_frames)},
+	{CPSW_STATSB_INFO(tx_65_to_127byte_frames)},
+	{CPSW_STATSB_INFO(tx_128_to_255byte_frames)},
+	{CPSW_STATSB_INFO(tx_256_to_511byte_frames)},
+	{CPSW_STATSB_INFO(tx_512_to_1023byte_frames)},
+	{CPSW_STATSB_INFO(tx_1024byte_frames)},
+	{CPSW_STATSB_INFO(net_bytes)},
+	{CPSW_STATSB_INFO(rx_sof_overruns)},
+	{CPSW_STATSB_INFO(rx_mof_overruns)},
+	{CPSW_STATSB_INFO(rx_dma_overruns)},
+};
+
+#define ETHTOOL_STATS_NUM ARRAY_SIZE(et_stats)
+
+struct cpswx_attribute {
+	struct attribute attr;
+	ssize_t (*show)(struct cpswx_priv *cpsw_dev,
+		struct cpswx_attribute *attr, char *buf);
+	ssize_t	(*store)(struct cpswx_priv *cpsw_dev,
+		struct cpswx_attribute *attr, const char *, size_t);
+	const struct cpswx_mod_info *info;
+	ssize_t info_size;
+	void *context;
+};
+#define to_cpswx_attr(_attr) container_of(_attr, struct cpswx_attribute, attr)
+
+#define to_cpswx_dev(obj) container_of(obj, struct cpswx_priv, kobj)
+
+#define tx_pri_to_cpswx_dev(obj) \
+	container_of(obj, struct cpswx_priv, tx_pri_kobj)
+
+#define pvlan_to_cpswx_dev(obj) \
+	container_of(obj, struct cpswx_priv, pvlan_kobj)
+
+#define stats_to_cpswx_dev(obj) \
+	container_of(obj, struct cpswx_priv, stats_kobj)
+
+#define BITS(x)			(BIT(x) - 1)
+#define BITMASK(n, s)		(BITS(n) << (s))
+#define cpsw_mod_info_field_val(r, i) \
+	((r & BITMASK(i->bits, i->shift)) >> i->shift)
+
+#define for_each_intf(i, priv) \
+	list_for_each_entry((i), &(priv)->cpsw_intf_head, cpsw_intf_list)
+
+#define __CPSW_ATTR_FULL(_name, _mode, _show, _store, _info,	\
+				_info_size, _ctxt)		\
+	{ \
+		.attr = {.name = __stringify(_name), .mode = _mode },	\
+		.show	= _show,		\
+		.store	= _store,		\
+		.info	= _info,		\
+		.info_size = _info_size,	\
+		.context = (_ctxt),		\
+	}
+
+#define __CPSW_ATTR(_name, _mode, _show, _store, _info) \
+		__CPSW_ATTR_FULL(_name, _mode, _show, _store, _info, \
+					(ARRAY_SIZE(_info)), NULL)
+
+#define __CPSW_CTXT_ATTR(_name, _mode, _show, _store, _info, _ctxt) \
+		__CPSW_ATTR_FULL(_name, _mode, _show, _store, _info, \
+					(ARRAY_SIZE(_info)), _ctxt)
+
+struct cpswx_mod_info {
+	const char	*name;
+	int		shift;
+	int		bits;
+};
+
+struct cpswx_parse_result {
+	int control;
+	int port;
+	u32 value;
+};
+
+static ssize_t cpsw_attr_info_show(const struct cpswx_mod_info *info,
+				int info_size, u32 reg, char *buf)
+{
+	int i, len = 0;
+
+	for (i = 0; i < info_size; i++, info++) {
+		len += snprintf(buf + len, PAGE_SIZE - len,
+			"%s=%d\n", info->name,
+			(int)cpsw_mod_info_field_val(reg, info));
+	}
+
+	return len;
+}
+
+static ssize_t cpsw_attr_parse_set_command(struct cpswx_priv *cpsw_dev,
+			      struct cpswx_attribute *attr,
+			      const char *buf, size_t count,
+				struct cpswx_parse_result *res)
+{
+	char ctrl_str[33], tmp_str[9];
+	int port = -1, value, len, control;
+	unsigned long end;
+	const struct cpswx_mod_info *info = attr->info;
+
+	len = strcspn(buf, ".=");
+	if (len >= 32)
+		return -ENOMEM;
+
+	strncpy(ctrl_str, buf, len);
+	ctrl_str[len] = '\0';
+	buf += len;
+
+	if (*buf == '.') {
+		++buf;
+		len = strcspn(buf, "=");
+		if (len >= 8)
+			return -ENOMEM;
+		strncpy(tmp_str, buf, len);
+		tmp_str[len] = '\0';
+		if (kstrtoul(tmp_str, 0, &end))
+			return -EINVAL;
+		port = (int)end;
+		buf += len;
+	}
+
+	if (*buf != '=')
+		return -EINVAL;
+
+	if (kstrtoul(buf + 1, 0, &end))
+		return -EINVAL;
+
+	value = (int)end;
+
+	for (control = 0; control < attr->info_size; control++)
+		if (strcmp(ctrl_str, info[control].name) == 0)
+			break;
+
+	if (control >= attr->info_size)
+		return -ENOENT;
+
+	res->control = control;
+	res->port = port;
+	res->value = value;
+
+	dev_info(cpsw_dev->dev, "parsed command %s.%d=%d\n",
+		attr->info[control].name, port, value);
+
+	return 0;
+}
+
+static inline void cpsw_info_set_reg_field(void __iomem *r,
+		const struct cpswx_mod_info *info, int val)
+{
+	u32 rv;
+
+	rv = __raw_readl(r);
+	rv = ((rv & ~BITMASK(info->bits, info->shift)) | (val << info->shift));
+	__raw_writel(rv, r);
+}
+
+static ssize_t cpsw_version_show(struct cpswx_priv *cpsw_dev,
+		     struct cpswx_attribute *attr,
+		     char *buf)
+{
+	u32 reg;
+
+	reg = __raw_readl(&cpsw_dev->regs->id_ver);
+
+	return snprintf(buf, PAGE_SIZE,
+		"cpsw version %d.%d (%d) SGMII identification value 0x%x\n",
+		 CPSW_MAJOR_VERSION(reg), CPSW_MINOR_VERSION(reg),
+		 CPSW_RTL_VERSION(reg), CPSW_IDENT(reg));
+}
+
+static struct cpswx_attribute cpsw_version_attribute =
+	__ATTR(version, S_IRUGO, cpsw_version_show, NULL);
+
+static const struct cpswx_mod_info cpsw_controls[] = {
+	{
+		.name		= "fifo_loopback",
+		.shift		= 0,
+		.bits		= 1,
+	},
+	{
+		.name		= "vlan_aware",
+		.shift		= 1,
+		.bits		= 1,
+	},
+	{
+		.name		= "p0_enable",
+		.shift		= 2,
+		.bits		= 1,
+	},
+	{
+		.name		= "p0_pass_pri_tagged",
+		.shift		= 3,
+		.bits		= 1,
+	},
+	{
+		.name		= "p1_pass_pri_tagged",
+		.shift		= 4,
+		.bits		= 1,
+	},
+	{
+		.name		= "p2_pass_pri_tagged",
+		.shift		= 5,
+		.bits		= 1,
+	},
+	{
+		.name		= "p0_tx_crc_type",
+		.shift		= 12,
+		.bits		= 1,
+	},
+};
+
+static ssize_t cpsw_control_show(struct cpswx_priv *cpsw_dev,
+		     struct cpswx_attribute *attr,
+		     char *buf)
+{
+	u32 reg;
+
+	reg = __raw_readl(&cpsw_dev->regs->control);
+	return cpsw_attr_info_show(attr->info, attr->info_size, reg, buf);
+}
+
+static ssize_t cpsw_control_store(struct cpswx_priv *cpsw_dev,
+			      struct cpswx_attribute *attr,
+			      const char *buf, size_t count)
+{
+	const struct cpswx_mod_info *i;
+	struct cpswx_parse_result res;
+	void __iomem *r = NULL;
+	int ret;
+
+
+	ret = cpsw_attr_parse_set_command(cpsw_dev, attr, buf, count, &res);
+	if (ret)
+		return ret;
+
+	i = &(attr->info[res.control]);
+	r = &cpsw_dev->regs->control;
+
+	cpsw_info_set_reg_field(r, i, res.value);
+	return count;
+}
+
+static struct cpswx_attribute cpsw_control_attribute =
+	__CPSW_ATTR(control, S_IRUGO | S_IWUSR,
+		cpsw_control_show, cpsw_control_store, cpsw_controls);
+
+static const struct cpswx_mod_info cpsw_ptypes[] = {
+	{
+		.name		= "escalate_pri_load_val",
+		.shift		= 0,
+		.bits		= 5,
+	},
+	{
+		.name		= "port0_pri_type_escalate",
+		.shift		= 8,
+		.bits		= 1,
+	},
+	{
+		.name		= "port1_pri_type_escalate",
+		.shift		= 9,
+		.bits		= 1,
+	},
+	{
+		.name		= "port2_pri_type_escalate",
+		.shift		= 10,
+		.bits		= 1,
+	},
+};
+
+static ssize_t cpsw_pri_type_show(struct cpswx_priv *cpsw_dev,
+		     struct cpswx_attribute *attr,
+		     char *buf)
+{
+	u32 reg;
+
+	reg = __raw_readl(&cpsw_dev->regs->ptype);
+
+	return cpsw_attr_info_show(attr->info, attr->info_size, reg, buf);
+}
+
+static ssize_t cpsw_pri_type_store(struct cpswx_priv *cpsw_dev,
+			      struct cpswx_attribute *attr,
+			      const char *buf, size_t count)
+{
+	const struct cpswx_mod_info *i;
+	struct cpswx_parse_result res;
+	void __iomem *r = NULL;
+	int ret;
+
+
+	ret = cpsw_attr_parse_set_command(cpsw_dev, attr, buf, count, &res);
+	if (ret)
+		return ret;
+
+	i = &(attr->info[res.control]);
+	r = &cpsw_dev->regs->ptype;
+
+	cpsw_info_set_reg_field(r, i, res.value);
+	return count;
+}
+
+static struct cpswx_attribute cpsw_pri_type_attribute =
+	__CPSW_ATTR(priority_type, S_IRUGO | S_IWUSR,
+			cpsw_pri_type_show,
+			cpsw_pri_type_store,
+			cpsw_ptypes);
+
+static const struct cpswx_mod_info cpsw_flow_controls[] = {
+	{
+		.name		= "port0_flow_control_en",
+		.shift		= 0,
+		.bits		= 1,
+	},
+	{
+		.name		= "port1_flow_control_en",
+		.shift		= 1,
+		.bits		= 1,
+	},
+	{
+		.name		= "port2_flow_control_en",
+		.shift		= 2,
+		.bits		= 1,
+	},
+};
+
+static ssize_t cpsw_flow_control_show(struct cpswx_priv *cpsw_dev,
+		     struct cpswx_attribute *attr, char *buf)
+{
+	u32 reg;
+
+	reg = __raw_readl(&cpsw_dev->regs->flow_control);
+
+	return cpsw_attr_info_show(attr->info, attr->info_size, reg, buf);
+}
+
+static ssize_t cpsw_flow_control_store(struct cpswx_priv *cpsw_dev,
+			      struct cpswx_attribute *attr,
+			      const char *buf, size_t count)
+{
+	const struct cpswx_mod_info *i;
+	struct cpswx_parse_result res;
+	void __iomem *r = NULL;
+	int ret;
+
+
+	ret = cpsw_attr_parse_set_command(cpsw_dev, attr, buf, count, &res);
+	if (ret)
+		return ret;
+
+	i = &(attr->info[res.control]);
+	r = &cpsw_dev->regs->flow_control;
+
+	cpsw_info_set_reg_field(r, i, res.value);
+	return count;
+}
+
+static struct cpswx_attribute cpsw_flow_control_attribute =
+	__CPSW_ATTR(flow_control, S_IRUGO | S_IWUSR,
+		cpsw_flow_control_show,
+		cpsw_flow_control_store,
+		cpsw_flow_controls);
+
+static const struct cpswx_mod_info cpsw_port_tx_pri_maps[] = {
+	{
+		.name		= "port_tx_pri_0",
+		.shift		= 0,
+		.bits		= 3,
+	},
+	{
+		.name		= "port_tx_pri_1",
+		.shift		= 4,
+		.bits		= 3,
+	},
+	{
+		.name		= "port_tx_pri_2",
+		.shift		= 8,
+		.bits		= 3,
+	},
+	{
+		.name		= "port_tx_pri_3",
+		.shift		= 12,
+		.bits		= 3,
+	},
+	{
+		.name		= "port_tx_pri_4",
+		.shift		= 16,
+		.bits		= 3,
+	},
+	{
+		.name		= "port_tx_pri_5",
+		.shift		= 20,
+		.bits		= 3,
+	},
+	{
+		.name		= "port_tx_pri_6",
+		.shift		= 24,
+		.bits		= 3,
+	},
+	{
+		.name		= "port_tx_pri_7",
+		.shift		= 28,
+		.bits		= 3,
+	},
+};
+
+static ssize_t cpsw_port_tx_pri_map_show(struct cpswx_priv *cpsw_dev,
+		     struct cpswx_attribute *attr,
+		     char *buf)
+{
+	int idx, len = 0, total_len = 0, port;
+	struct cpswx_intf *cpsw_intf;
+	struct cpswx_slave *slave;
+	u32 reg;
+
+	port = (int)(attr->context);
+
+	for_each_intf(cpsw_intf, cpsw_dev) {
+		if (cpsw_intf->multi_if) {
+			slave = cpsw_intf->slaves;
+			if (slave->port_num != port)
+				continue;
+			reg = __raw_readl(&slave->regs->tx_pri_map);
+			len = cpsw_attr_info_show(attr->info, attr->info_size,
+						reg, buf+total_len);
+			total_len += len;
+		} else {
+			for (idx = 0; idx < cpsw_intf->num_slaves; idx++) {
+				slave = cpsw_intf->slaves + idx;
+				if (slave->port_num != port)
+					continue;
+				reg = __raw_readl(&slave->regs->tx_pri_map);
+				len = cpsw_attr_info_show(attr->info,
+					attr->info_size, reg, buf+total_len);
+				total_len += len;
+			}
+		}
+	}
+	return total_len;
+}
+
+static ssize_t cpsw_port_tx_pri_map_store(struct cpswx_priv *cpsw_dev,
+			      struct cpswx_attribute *attr,
+			      const char *buf, size_t count)
+{
+	const struct cpswx_mod_info *i;
+	struct cpswx_parse_result res;
+	struct cpswx_intf *cpsw_intf;
+	struct cpswx_slave *slave;
+	void __iomem *r = NULL;
+	int ret, idx, port;
+
+	port = (int)(attr->context);
+
+	ret = cpsw_attr_parse_set_command(cpsw_dev, attr, buf, count, &res);
+	if (ret)
+		return ret;
+
+	i = &(attr->info[res.control]);
+
+	/* Slave port */
+	for_each_intf(cpsw_intf, cpsw_dev) {
+		if (cpsw_intf->multi_if) {
+			slave = cpsw_intf->slaves;
+			if (slave->port_num == port) {
+				r = &slave->regs->tx_pri_map;
+				goto set;
+			}
+		} else
+			for (idx = 0; idx < cpsw_intf->num_slaves; idx++) {
+				slave = cpsw_intf->slaves + idx;
+				if (slave->port_num == port) {
+					r = &slave->regs->tx_pri_map;
+					goto set;
+				}
+			}
+	}
+
+	if (!r)
+		return  -ENOENT;
+
+set:
+	cpsw_info_set_reg_field(r, i, res.value);
+	return count;
+}
+
+static struct cpswx_attribute cpsw_tx_pri_1_attribute =
+	__CPSW_CTXT_ATTR(1, S_IRUGO | S_IWUSR,
+			cpsw_port_tx_pri_map_show,
+			cpsw_port_tx_pri_map_store,
+			cpsw_port_tx_pri_maps, (void *)1);
+
+static struct cpswx_attribute cpsw_tx_pri_2_attribute =
+	__CPSW_CTXT_ATTR(2, S_IRUGO | S_IWUSR,
+			cpsw_port_tx_pri_map_show,
+			cpsw_port_tx_pri_map_store,
+			cpsw_port_tx_pri_maps, (void *)2);
+
+static struct attribute *cpsw_tx_pri_default_attrs[] = {
+	&cpsw_tx_pri_1_attribute.attr,
+	&cpsw_tx_pri_2_attribute.attr,
+	NULL
+};
+
+static ssize_t cpsw_tx_pri_attr_show(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	struct cpswx_attribute *attribute = to_cpswx_attr(attr);
+	struct cpswx_priv *cpsw_dev = tx_pri_to_cpswx_dev(kobj);
+
+	if (!attribute->show)
+		return -EIO;
+
+	return attribute->show(cpsw_dev, attribute, buf);
+}
+
+static ssize_t cpsw_tx_pri_attr_store(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	struct cpswx_attribute *attribute = to_cpswx_attr(attr);
+	struct cpswx_priv *cpsw_dev = tx_pri_to_cpswx_dev(kobj);
+
+	if (!attribute->store)
+		return -EIO;
+
+	return attribute->store(cpsw_dev, attribute, buf, count);
+}
+
+static const struct sysfs_ops cpsw_tx_pri_sysfs_ops = {
+	.show = cpsw_tx_pri_attr_show,
+	.store = cpsw_tx_pri_attr_store,
+};
+
+static struct kobj_type cpsw_tx_pri_ktype = {
+	.sysfs_ops = &cpsw_tx_pri_sysfs_ops,
+	.default_attrs = cpsw_tx_pri_default_attrs,
+};
+
+static const struct cpswx_mod_info cpsw_port_vlans[] = {
+	{
+		.name		= "port_vlan_id",
+		.shift		= 0,
+		.bits		= 12,
+	},
+	{
+		.name		= "port_cfi",
+		.shift		= 12,
+		.bits		= 1,
+	},
+	{
+		.name		= "port_vlan_pri",
+		.shift		= 13,
+		.bits		= 3,
+	},
+};
+
+static ssize_t cpsw_port_vlan_show(struct cpswx_priv *cpsw_dev,
+		     struct cpswx_attribute *attr,
+		     char *buf)
+{
+	int idx, len = 0, total_len = 0, port;
+	struct cpswx_intf *cpsw_intf;
+	struct cpswx_slave *slave;
+	u32 reg;
+
+	port = (int)(attr->context);
+
+	if (port == cpsw_dev->host_port) {
+		/* Host port */
+		reg = __raw_readl(&cpsw_dev->host_port_regs->port_vlan);
+		len = cpsw_attr_info_show(attr->info, attr->info_size,
+					reg, buf);
+		return len;
+	}
+
+	/* Slave ports */
+	for_each_intf(cpsw_intf, cpsw_dev) {
+		if (cpsw_intf->multi_if) {
+			slave = cpsw_intf->slaves;
+			if (slave->port_num != port)
+				continue;
+			reg = __raw_readl(&slave->regs->port_vlan);
+			len = cpsw_attr_info_show(attr->info, attr->info_size,
+					reg, buf+total_len);
+			total_len += len;
+		} else {
+			for (idx = 0; idx < cpsw_intf->num_slaves; idx++) {
+				slave = cpsw_intf->slaves + idx;
+				if (slave->port_num != port)
+					continue;
+				reg = __raw_readl(&slave->regs->port_vlan);
+				len = cpsw_attr_info_show(attr->info,
+					attr->info_size, reg, buf+total_len);
+				total_len += len;
+			}
+		}
+	}
+	return total_len;
+}
+
+static ssize_t cpsw_port_vlan_store(struct cpswx_priv *cpsw_dev,
+			      struct cpswx_attribute *attr,
+			      const char *buf, size_t count)
+{
+	const struct cpswx_mod_info *i;
+	struct cpswx_parse_result res;
+	struct cpswx_intf *cpsw_intf;
+	struct cpswx_slave *slave;
+	void __iomem *r = NULL;
+	int ret, idx, port;
+
+	port = (int)(attr->context);
+
+	ret = cpsw_attr_parse_set_command(cpsw_dev, attr, buf, count, &res);
+	if (ret)
+		return ret;
+
+	i = &(attr->info[res.control]);
+
+	/* Host port */
+	if (port == cpsw_dev->host_port) {
+		r = &cpsw_dev->host_port_regs->port_vlan;
+		goto set;
+	}
+
+	/* Slave port */
+	for_each_intf(cpsw_intf, cpsw_dev) {
+		if (cpsw_intf->multi_if) {
+			slave = cpsw_intf->slaves;
+			if (slave->port_num == port) {
+				r = &slave->regs->port_vlan;
+				goto set;
+			}
+		} else
+			for (idx = 0; idx < cpsw_intf->num_slaves; idx++) {
+				slave = cpsw_intf->slaves + idx;
+				if (slave->port_num == port) {
+					r = &slave->regs->port_vlan;
+					goto set;
+				}
+			}
+	}
+
+	if (!r)
+		return  -ENOENT;
+
+set:
+	cpsw_info_set_reg_field(r, i, res.value);
+	return count;
+}
+
+static struct cpswx_attribute cpsw_pvlan_0_attribute =
+	__CPSW_CTXT_ATTR(0, S_IRUGO | S_IWUSR,
+			cpsw_port_vlan_show,
+			cpsw_port_vlan_store,
+			cpsw_port_vlans, (void *)0);
+
+static struct cpswx_attribute cpsw_pvlan_1_attribute =
+	__CPSW_CTXT_ATTR(1, S_IRUGO | S_IWUSR,
+			cpsw_port_vlan_show,
+			cpsw_port_vlan_store,
+			cpsw_port_vlans, (void *)1);
+
+static struct cpswx_attribute cpsw_pvlan_2_attribute =
+	__CPSW_CTXT_ATTR(2, S_IRUGO | S_IWUSR,
+			cpsw_port_vlan_show,
+			cpsw_port_vlan_store,
+			cpsw_port_vlans, (void *)2);
+
+static struct attribute *cpsw_pvlan_default_attrs[] = {
+	&cpsw_pvlan_0_attribute.attr,
+	&cpsw_pvlan_1_attribute.attr,
+	&cpsw_pvlan_2_attribute.attr,
+	NULL
+};
+
+static ssize_t cpsw_pvlan_attr_show(struct kobject *kobj,
+			struct attribute *attr, char *buf)
+{
+	struct cpswx_attribute *attribute = to_cpswx_attr(attr);
+	struct cpswx_priv *cpsw_dev = pvlan_to_cpswx_dev(kobj);
+
+	if (!attribute->show)
+		return -EIO;
+
+	return attribute->show(cpsw_dev, attribute, buf);
+}
+
+static ssize_t cpsw_pvlan_attr_store(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	struct cpswx_attribute *attribute = to_cpswx_attr(attr);
+	struct cpswx_priv *cpsw_dev = pvlan_to_cpswx_dev(kobj);
+
+	if (!attribute->store)
+		return -EIO;
+
+	return attribute->store(cpsw_dev, attribute, buf, count);
+}
+
+static const struct sysfs_ops cpsw_pvlan_sysfs_ops = {
+	.show = cpsw_pvlan_attr_show,
+	.store = cpsw_pvlan_attr_store,
+};
+
+static struct kobj_type cpsw_pvlan_ktype = {
+	.sysfs_ops = &cpsw_pvlan_sysfs_ops,
+	.default_attrs = cpsw_pvlan_default_attrs,
+};
+
+static void cpsw_reset_mod_stats(struct cpswx_priv *cpsw_dev, int stat_mod)
+{
+	struct cpswx_host_hw_stats __iomem *cpsw_stats0;
+	struct cpswx_hw_stats __iomem *cpsw_statsa;
+	struct cpswx_hw_stats __iomem *cpsw_statsb;
+	void *p = NULL;
+	int i;
+
+	cpsw_stats0 = cpsw_dev->host_hw_stats_regs;
+	cpsw_statsa = cpsw_dev->hw_stats_regs[0];
+	cpsw_statsb = cpsw_dev->hw_stats_regs[1];
+
+	switch (stat_mod) {
+	case CPSW_STATS0_MODULE:
+		p = cpsw_stats0;
+		break;
+	case CPSW_STATS1_MODULE:
+		p = cpsw_statsa;
+		break;
+	case CPSW_STATS2_MODULE:
+		p  = cpsw_statsb;
+		break;
+	}
+
+	for (i = 0; i < ETHTOOL_STATS_NUM; i++) {
+		if (et_stats[i].type == stat_mod) {
+			cpsw_dev->hw_stats[i] = 0;
+			p = (u8 *)p + et_stats[i].offset;
+			*(u32 *)p = 0xffffffff;
+		}
+	}
+	return;
+}
+
+static ssize_t cpsw_stats_mod_store(struct cpswx_priv *cpsw_dev,
+			      struct cpswx_attribute *attr,
+			      const char *buf, size_t count)
+{
+	unsigned long end;
+	int stat_mod;
+
+	if (kstrtoul(buf, 0, &end) != 0 || (end != 0))
+		return -EINVAL;
+
+	stat_mod = (int)(attr->context);
+	mutex_lock(&cpsw_dev->hw_stats_lock);
+	cpsw_reset_mod_stats(cpsw_dev, stat_mod);
+	mutex_unlock(&cpsw_dev->hw_stats_lock);
+	return count;
+}
+
+static struct cpswx_attribute cpsw_stats_0_attribute =
+	__CPSW_ATTR_FULL(0, S_IWUSR, NULL, cpsw_stats_mod_store,
+			NULL, 0, (void *)CPSW_STATS0_MODULE);
+
+static struct cpswx_attribute cpsw_stats_1_attribute =
+	__CPSW_ATTR_FULL(1, S_IWUSR, NULL, cpsw_stats_mod_store,
+			NULL, 0, (void *)CPSW_STATS1_MODULE);
+
+static struct cpswx_attribute cpsw_stats_2_attribute =
+	__CPSW_ATTR_FULL(2, S_IWUSR, NULL, cpsw_stats_mod_store,
+			NULL, 0, (void *)CPSW_STATS2_MODULE);
+
+static struct attribute *cpsw_stats_default_attrs[] = {
+	&cpsw_stats_0_attribute.attr,
+	&cpsw_stats_1_attribute.attr,
+	&cpsw_stats_2_attribute.attr,
+	NULL
+};
+
+static ssize_t cpsw_stats_attr_store(struct kobject *kobj,
+			struct attribute *attr, const char *buf, size_t count)
+{
+	struct cpswx_attribute *attribute = to_cpswx_attr(attr);
+	struct cpswx_priv *cpsw_dev = stats_to_cpswx_dev(kobj);
+
+	if (!attribute->store)
+		return -EIO;
+
+	return attribute->store(cpsw_dev, attribute, buf, count);
+}
+
+static const struct sysfs_ops cpsw_stats_sysfs_ops = {
+	.store = cpsw_stats_attr_store,
+};
+
+static struct kobj_type cpsw_stats_ktype = {
+	.sysfs_ops = &cpsw_stats_sysfs_ops,
+	.default_attrs = cpsw_stats_default_attrs,
+};
+static struct attribute *cpsw_default_attrs[] = {
+	&cpsw_version_attribute.attr,
+	&cpsw_control_attribute.attr,
+	&cpsw_pri_type_attribute.attr,
+	&cpsw_flow_control_attribute.attr,
+	NULL
+};
+
+static ssize_t cpsw_attr_show(struct kobject *kobj, struct attribute *attr,
+				  char *buf)
+{
+	struct cpswx_attribute *attribute = to_cpswx_attr(attr);
+	struct cpswx_priv *cpsw_dev = to_cpswx_dev(kobj);
+
+	if (!attribute->show)
+		return -EIO;
+
+	return attribute->show(cpsw_dev, attribute, buf);
+}
+
+static ssize_t cpsw_attr_store(struct kobject *kobj, struct attribute *attr,
+				   const char *buf, size_t count)
+{
+	struct cpswx_attribute *attribute = to_cpswx_attr(attr);
+	struct cpswx_priv *cpsw_dev = to_cpswx_dev(kobj);
+
+	if (!attribute->store)
+		return -EIO;
+
+	return attribute->store(cpsw_dev, attribute, buf, count);
+}
+
+static const struct sysfs_ops cpsw_sysfs_ops = {
+	.show = cpsw_attr_show,
+	.store = cpsw_attr_store,
+};
+
+static struct kobj_type cpsw_ktype = {
+	.sysfs_ops = &cpsw_sysfs_ops,
+	.default_attrs = cpsw_default_attrs,
+};
+
+static void keystone_get_drvinfo(struct net_device *ndev,
+			     struct ethtool_drvinfo *info)
+{
+	strncpy(info->driver, NETCP_DRIVER_NAME, sizeof(info->driver));
+	strncpy(info->version, NETCP_DRIVER_VERSION, sizeof(info->version));
+}
+
+static u32 keystone_get_msglevel(struct net_device *ndev)
+{
+	struct netcp_priv *netcp = netdev_priv(ndev);
+	return netcp->msg_enable;
+}
+
+static void keystone_set_msglevel(struct net_device *ndev, u32 value)
+{
+	struct netcp_priv *netcp = netdev_priv(ndev);
+	netcp->msg_enable = value;
+}
+
+static void keystone_get_stat_strings(struct net_device *ndev,
+				   uint32_t stringset, uint8_t *data)
+{
+	int i;
+
+	switch (stringset) {
+	case ETH_SS_STATS:
+		for (i = 0; i < ETHTOOL_STATS_NUM; i++) {
+			memcpy(data, et_stats[i].desc, ETH_GSTRING_LEN);
+			data += ETH_GSTRING_LEN;
+		}
+		break;
+	case ETH_SS_TEST:
+		break;
+	}
+}
+
+static int keystone_get_sset_count(struct net_device *ndev, int stringset)
+{
+	switch (stringset) {
+	case ETH_SS_TEST:
+		return 0;
+	case ETH_SS_STATS:
+		return ETHTOOL_STATS_NUM;
+	default:
+		return -EINVAL;
+	}
+}
+
+static void cpswx_update_stats(struct cpswx_priv *cpsw_dev, uint64_t *data)
+{
+	struct cpswx_host_hw_stats __iomem *cpsw_stats0;
+	struct cpswx_hw_stats __iomem *cpsw_statsa;
+	struct cpswx_hw_stats __iomem *cpsw_statsb;
+	void *p = NULL;
+	u32 tmp = 0;
+	int i;
+
+	cpsw_stats0 = cpsw_dev->host_hw_stats_regs;
+	cpsw_statsa = cpsw_dev->hw_stats_regs[0];
+	cpsw_statsb = cpsw_dev->hw_stats_regs[1];
+
+	for (i = 0; i < ETHTOOL_STATS_NUM; i++) {
+		switch (et_stats[i].type) {
+		case CPSW_STATS0_MODULE:
+			p = cpsw_stats0;
+			break;
+		case CPSW_STATS1_MODULE:
+			p = cpsw_statsa;
+			break;
+		case CPSW_STATS2_MODULE:
+			p  = cpsw_statsb;
+			break;
+		}
+
+		p = (u8 *)p + et_stats[i].offset;
+		tmp = *(u32 *)p;
+		cpsw_dev->hw_stats[i] = cpsw_dev->hw_stats[i] + tmp;
+		if (data)
+			data[i] = cpsw_dev->hw_stats[i];
+		*(u32 *)p = tmp;
+	}
+
+	return;
+}
+
+static void keystone_get_ethtool_stats(struct net_device *ndev,
+				       struct ethtool_stats *stats,
+				       uint64_t *data)
+{
+	struct netcp_priv *netcp = netdev_priv(ndev);
+	struct netcp_device *netcp_device = netcp->netcp_device;
+	struct cpswx_priv *priv;
+
+	/* find the instance of the module registered to the netcp_device */
+	priv = (struct cpswx_priv *)netcp_device_find_module(netcp_device,
+							CPSW_MODULE_NAME);
+	if (priv) {
+		mutex_lock(&priv->hw_stats_lock);
+		cpswx_update_stats(priv, data);
+		mutex_unlock(&priv->hw_stats_lock);
+	}
+
+	return;
+}
+
+static int keystone_get_settings(struct net_device *ndev,
+			      struct ethtool_cmd *cmd)
+{
+	struct phy_device *phy = ndev->phydev;
+	struct cpswx_slave *slave;
+	int ret;
+
+	if (!phy)
+		return -EINVAL;
+
+	slave = (struct cpswx_slave *)phy->context;
+	if (!slave)
+		return -EINVAL;
+
+	ret = phy_ethtool_gset(phy, cmd);
+	if (!ret)
+		cmd->port = slave->phy_port_t;
+
+	return ret;
+}
+
+static int keystone_set_settings(struct net_device *ndev,
+				struct ethtool_cmd *cmd)
+{
+	struct phy_device *phy = ndev->phydev;
+	struct cpswx_slave *slave;
+	u32 features = cmd->advertising & cmd->supported;
+
+	if (!phy)
+		return -EINVAL;
+
+	slave = (struct cpswx_slave *)phy->context;
+	if (!slave)
+		return -EINVAL;
+
+	if (cmd->port != slave->phy_port_t) {
+		if ((cmd->port == PORT_TP) && !(features & ADVERTISED_TP))
+			return -EINVAL;
+
+		if ((cmd->port == PORT_AUI) && !(features & ADVERTISED_AUI))
+			return -EINVAL;
+
+		if ((cmd->port == PORT_BNC) && !(features & ADVERTISED_BNC))
+			return -EINVAL;
+
+		if ((cmd->port == PORT_MII) && !(features & ADVERTISED_MII))
+			return -EINVAL;
+
+		if ((cmd->port == PORT_FIBRE) && !(features & ADVERTISED_FIBRE))
+			return -EINVAL;
+	}
+
+	slave->phy_port_t = cmd->port;
+
+	return phy_ethtool_sset(phy, cmd);
+}
+
+static const struct ethtool_ops keystone_ethtool_ops = {
+	.get_drvinfo		= keystone_get_drvinfo,
+	.get_link		= ethtool_op_get_link,
+	.get_msglevel		= keystone_get_msglevel,
+	.set_msglevel		= keystone_set_msglevel,
+	.get_strings		= keystone_get_stat_strings,
+	.get_sset_count		= keystone_get_sset_count,
+	.get_ethtool_stats	= keystone_get_ethtool_stats,
+	.get_settings		= keystone_get_settings,
+	.set_settings		= keystone_set_settings,
+};
+
+#define mac_hi(mac)	(((mac)[0] << 0) | ((mac)[1] << 8) |	\
+			 ((mac)[2] << 16) | ((mac)[3] << 24))
+#define mac_lo(mac)	(((mac)[4] << 0) | ((mac)[5] << 8))
+
+static void cpsw_set_slave_mac(struct cpswx_slave *slave,
+			       struct cpswx_intf *cpsw_intf)
+{
+	struct net_device *ndev = cpsw_intf->ndev;
+
+	__raw_writel(mac_hi(ndev->dev_addr), &slave->regs->sa_hi);
+	__raw_writel(mac_lo(ndev->dev_addr), &slave->regs->sa_lo);
+}
+
+static inline int cpsw_get_slave_port(struct cpswx_priv *priv, u32 slave_num)
+{
+	if (priv->host_port == 0)
+		return slave_num + 1;
+	else
+		return slave_num;
+}
+
+static void _cpsw_adjust_link(struct cpswx_slave *slave, bool *link)
+{
+	struct phy_device *phy = slave->phy;
+	u32 mac_control = 0;
+	u32 slave_port;
+
+	if (!phy)
+		return;
+
+	slave_port = slave->port_num;
+
+	if (phy->link) {
+		mac_control = slave->mac_control;
+		mac_control |= MACSL_SIG_ENABLE(slave) |
+				MACSL_RX_ENABLE_EXT_CTL |
+				MACSL_RX_ENABLE_CSF;
+		/* enable forwarding */
+		cpsw_ale_control_set(slave->ale, slave_port,
+				     ALE_PORT_STATE, ALE_PORT_STATE_FORWARD);
+
+		if (phy->duplex)
+			mac_control |= BIT(0);	/* FULLDUPLEXEN	*/
+		else
+			mac_control &= ~0x1;
+
+		*link = true;
+	} else {
+		mac_control = 0;
+		/* disable forwarding */
+		cpsw_ale_control_set(slave->ale, slave_port,
+				     ALE_PORT_STATE, ALE_PORT_STATE_DISABLE);
+	}
+
+	if (mac_control != slave->mac_control) {
+		phy_print_status(phy);
+		__raw_writel(mac_control, &slave->sliver->mac_control);
+	}
+
+	slave->mac_control = mac_control;
+}
+
+static void cpsw_adjust_link(struct net_device *n_dev, void *context)
+{
+	struct cpswx_slave *slave = (struct cpswx_slave *)context;
+	struct netcp_priv *netcp = netdev_priv(n_dev);
+	bool link = false;
+
+	_cpsw_adjust_link(slave, &link);
+
+	if (link)
+		netcp->link_state |= BIT(slave->slave_num);
+	else
+		netcp->link_state &= ~BIT(slave->slave_num);
+}
+
+/*
+ * Reset the the mac sliver
+ * Soft reset is set and polled until clear, or until a timeout occurs
+ */
+static int cpsw_port_reset(struct cpswx_slave *slave)
+{
+	u32 i, v;
+
+	/* Set the soft reset bit */
+	__raw_writel(SOFT_RESET,
+		     &slave->sliver->soft_reset);
+
+	/* Wait for the bit to clear */
+	for (i = 0; i < DEVICE_EMACSL_RESET_POLL_COUNT; i++) {
+		v = __raw_readl(&slave->sliver->soft_reset);
+		if ((v & SOFT_RESET_MASK) !=
+		    SOFT_RESET)
+			return 0;
+	}
+
+	/* Timeout on the reset */
+	return GMACSL_RET_WARN_RESET_INCOMPLETE;
+}
+
+/*
+ * Configure the mac sliver
+ */
+static void cpsw_port_config(struct cpswx_slave *slave, int max_rx_len)
+{
+	u32 mac_control;
+
+	if (max_rx_len > MAX_SIZE_STREAM_BUFFER)
+		max_rx_len = MAX_SIZE_STREAM_BUFFER;
+
+	__raw_writel(max_rx_len, &slave->sliver->rx_maxlen);
+
+	mac_control = (MACSL_SIG_ENABLE(slave) |
+			MACSL_RX_ENABLE_EXT_CTL |
+			MACSL_RX_ENABLE_CSF);
+	__raw_writel(mac_control, &slave->sliver->mac_control);
+}
+
+static void cpsw_slave_stop(struct cpswx_slave *slave, struct cpswx_priv *priv)
+{
+	cpsw_port_reset(slave);
+
+	if (!slave->phy)
+		return;
+
+	phy_stop(slave->phy);
+	phy_disconnect(slave->phy);
+	slave->phy = NULL;
+}
+
+static void cpsw_slave_link(struct cpswx_slave *slave,
+			    struct cpswx_intf *cpsw_intf)
+{
+	struct netcp_priv *netcp = netdev_priv(cpsw_intf->ndev);
+
+	if ((slave->link_interface == SGMII_LINK_MAC_PHY) ||
+		(slave->link_interface == XGMII_LINK_MAC_PHY)) {
+		if (netcp->link_state)
+			cpsw_intf->sgmii_link |= BIT(slave->slave_num);
+		else
+			cpsw_intf->sgmii_link &= ~BIT(slave->slave_num);
+	} else if (slave->link_interface == XGMII_LINK_MAC_MAC_FORCED)
+		cpsw_intf->sgmii_link |= BIT(slave->slave_num);
+}
+
+static void cpsw_slave_open(struct cpswx_slave *slave,
+			    struct cpswx_intf *cpsw_intf)
+{
+	struct cpswx_priv *priv = cpsw_intf->cpsw_priv;
+	char name[32];		/* FIXME: Unused variable */
+	u32 slave_port;
+	int has_phy = 0;
+	phy_interface_t phy_mode;
+
+	snprintf(name, sizeof(name), "slave-%d", slave->slave_num);
+
+	if (!SLAVE_LINK_IS_XGMII(slave)) {
+		keystone_sgmii_reset(priv->sgmii_port_regs, slave->slave_num);
+
+		keystone_sgmii_config(priv->sgmii_port_regs, slave->slave_num,
+				slave->link_interface);
+	} else
+		keystone_pcsr_config(priv->pcsr_port_regs, slave->slave_num,
+				slave->link_interface);
+
+	cpsw_port_reset(slave);
+
+	cpsw_port_config(slave, priv->rx_packet_max);
+
+	cpsw_set_slave_mac(slave, cpsw_intf);
+
+	slave->mac_control = MACSL_SIG_ENABLE(slave) | MACSL_RX_ENABLE_EXT_CTL |
+				MACSL_RX_ENABLE_CSF;
+
+	slave_port = cpsw_get_slave_port(priv, slave->slave_num);
+
+	slave->port_num = slave_port;
+	slave->ale = priv->ale;
+
+	/* enable forwarding */
+	cpsw_ale_control_set(priv->ale, slave_port,
+			     ALE_PORT_STATE, ALE_PORT_STATE_FORWARD);
+
+	cpsw_ale_add_mcast(priv->ale, cpsw_intf->ndev->broadcast,
+			   1 << slave_port, 0, ALE_MCAST_FWD_2,
+			   CPSW_NON_VLAN_ADDR);
+
+	if (slave->link_interface == SGMII_LINK_MAC_PHY) {
+		has_phy = 1;
+		phy_mode = PHY_INTERFACE_MODE_SGMII;
+		slave->phy_port_t = PORT_MII;
+	} else if (slave->link_interface == XGMII_LINK_MAC_PHY) {
+		has_phy = 1;
+		/* +++FIXME: PHY_INTERFACE_MODE_XGMII ?? */
+		phy_mode = PHY_INTERFACE_MODE_NA;
+		slave->phy_port_t = PORT_FIBRE;
+	}
+
+	if (has_phy) {
+		slave->phy = of_phy_connect(cpsw_intf->ndev,
+					    cpsw_intf->phy_node,
+					    &cpsw_adjust_link, 0,
+					    phy_mode,
+					    slave);
+		if (IS_ERR_OR_NULL(slave->phy)) {
+			dev_err(priv->dev, "phy not found on slave %d\n",
+				slave->slave_num);
+			slave->phy = NULL;
+		} else {
+			dev_info(priv->dev, "phy found: id is: 0x%s\n",
+				 dev_name(&slave->phy->dev));
+			cpsw_intf->ndev->phydev = slave->phy;
+			phy_start(slave->phy);
+		}
+	}
+}
+
+static void cpsw_init_host_port(struct cpswx_priv *priv,
+				struct cpswx_intf *cpsw_intf)
+{
+	/* Max length register */
+	__raw_writel(MAX_SIZE_STREAM_BUFFER,
+		     &priv->host_port_regs->rx_maxlen);
+
+	if (priv->ale_refcnt == 1)
+		cpsw_ale_start(priv->ale);
+
+	if (priv->multi_if)
+		cpsw_ale_control_set(priv->ale, 0, ALE_BYPASS, 1);
+
+	cpsw_ale_control_set(priv->ale, 0, ALE_NO_PORT_VLAN, 1);
+
+	cpsw_ale_control_set(priv->ale, priv->host_port,
+			     ALE_PORT_STATE, ALE_PORT_STATE_FORWARD);
+
+	cpsw_ale_control_set(priv->ale, 0,
+			     ALE_PORT_UNKNOWN_VLAN_MEMBER,
+			     CPSW_MASK_ALL_PORTS);
+
+	cpsw_ale_control_set(priv->ale, 0,
+			     ALE_PORT_UNKNOWN_MCAST_FLOOD,
+			     CPSW_MASK_PHYS_PORTS);
+
+	cpsw_ale_control_set(priv->ale, 0,
+			     ALE_PORT_UNKNOWN_REG_MCAST_FLOOD,
+			     CPSW_MASK_ALL_PORTS);
+
+	cpsw_ale_control_set(priv->ale, 0,
+			     ALE_PORT_UNTAGGED_EGRESS,
+			     CPSW_MASK_ALL_PORTS);
+}
+
+static void cpsw_slave_init(struct cpswx_slave *slave, struct cpswx_priv *priv)
+{
+	void __iomem		*regs = priv->ss_regs;
+	int			slave_num = slave->slave_num;
+
+	slave->regs	= regs + priv->slave_reg_ofs + (0x30 * slave_num);
+	slave->sliver	= regs + priv->sliver_reg_ofs + (0x40 * slave_num);
+}
+
+static void cpsw_add_mcast_addr(struct cpswx_intf *cpsw_intf, u8 *addr)
+{
+	struct cpswx_priv *cpsw_dev = cpsw_intf->cpsw_priv;
+	u16 vlan_id;
+
+	cpsw_ale_add_mcast(cpsw_dev->ale, addr, CPSW_MASK_ALL_PORTS, 0,
+			   ALE_MCAST_FWD_2, -1);
+	for_each_set_bit(vlan_id, cpsw_intf->active_vlans, VLAN_N_VID) {
+		cpsw_ale_add_mcast(cpsw_dev->ale, addr, CPSW_MASK_ALL_PORTS, 0,
+				   ALE_MCAST_FWD_2, vlan_id);
+	}
+}
+
+static void cpsw_add_ucast_addr(struct cpswx_intf *cpsw_intf, u8 *addr)
+{
+	struct cpswx_priv *cpsw_dev = cpsw_intf->cpsw_priv;
+	u16 vlan_id;
+
+	cpsw_ale_add_ucast(cpsw_dev->ale, addr, cpsw_dev->host_port, 0, -1);
+
+	for_each_set_bit(vlan_id, cpsw_intf->active_vlans, VLAN_N_VID)
+		cpsw_ale_add_ucast(cpsw_dev->ale, addr, cpsw_dev->host_port, 0,
+				   vlan_id);
+}
+
+static void cpsw_del_mcast_addr(struct cpswx_intf *cpsw_intf, u8 *addr)
+{
+	struct cpswx_priv *cpsw_dev = cpsw_intf->cpsw_priv;
+	u16 vlan_id;
+
+	cpsw_ale_del_mcast(cpsw_dev->ale, addr, CPSW_MASK_ALL_PORTS, -1);
+
+	for_each_set_bit(vlan_id, cpsw_intf->active_vlans, VLAN_N_VID) {
+		cpsw_ale_del_mcast(cpsw_dev->ale, addr, CPSW_MASK_ALL_PORTS,
+				   vlan_id);
+	}
+}
+
+static void cpsw_del_ucast_addr(struct cpswx_intf *cpsw_intf, u8 *addr)
+{
+	struct cpswx_priv *cpsw_dev = cpsw_intf->cpsw_priv;
+	u16 vlan_id;
+
+	cpsw_ale_del_ucast(cpsw_dev->ale, addr, cpsw_dev->host_port, -1);
+
+	for_each_set_bit(vlan_id, cpsw_intf->active_vlans, VLAN_N_VID) {
+		cpsw_ale_del_ucast(cpsw_dev->ale, addr, cpsw_dev->host_port,
+				   vlan_id);
+	}
+}
+
+static int cpswx_add_addr(void *intf_priv, struct netcp_addr *naddr)
+{
+	struct cpswx_intf *cpsw_intf = intf_priv;
+	struct cpswx_priv *cpsw_dev = cpsw_intf->cpsw_priv;
+
+	dev_dbg(cpsw_dev->dev, "xgess adding address %pM, type %d\n",
+		naddr->addr, naddr->type);
+
+	switch (naddr->type) {
+	case ADDR_MCAST:
+	case ADDR_BCAST:
+		cpsw_add_mcast_addr(cpsw_intf, naddr->addr);
+		break;
+	case ADDR_UCAST:
+	case ADDR_DEV:
+		cpsw_add_ucast_addr(cpsw_intf, naddr->addr);
+		break;
+	case ADDR_ANY:
+		/* nothing to do for promiscuous */
+	default:
+		break;
+	}
+
+	return 0;
+}
+
+static int cpswx_del_addr(void *intf_priv, struct netcp_addr *naddr)
+{
+	struct cpswx_intf *cpsw_intf = intf_priv;
+	struct cpswx_priv *cpsw_dev = cpsw_intf->cpsw_priv;
+
+	dev_dbg(cpsw_dev->dev, "xgess deleting address %pM, type %d\n",
+		naddr->addr, naddr->type);
+
+	switch (naddr->type) {
+	case ADDR_MCAST:
+	case ADDR_BCAST:
+		cpsw_del_mcast_addr(cpsw_intf, naddr->addr);
+		break;
+	case ADDR_UCAST:
+	case ADDR_DEV:
+		cpsw_del_ucast_addr(cpsw_intf, naddr->addr);
+		break;
+	case ADDR_ANY:
+		/* nothing to do for promiscuous */
+	default:
+		break;
+	}
+
+	return 0;
+}
+
+static int cpswx_add_vid(void *intf_priv, int vid)
+{
+	struct cpswx_intf *cpsw_intf = intf_priv;
+	struct cpswx_priv *cpsw_dev = cpsw_intf->cpsw_priv;
+
+	set_bit(vid, cpsw_intf->active_vlans);
+
+	cpsw_ale_add_vlan(cpsw_dev->ale, vid, CPSW_MASK_ALL_PORTS,
+			  CPSW_MASK_ALL_PORTS, CPSW_MASK_PHYS_PORTS,
+			  CPSW_MASK_NO_PORTS);
+
+	return 0;
+}
+
+static int cpswx_del_vid(void *intf_priv, int vid)
+{
+	struct cpswx_intf *cpsw_intf = intf_priv;
+	struct cpswx_priv *cpsw_dev = cpsw_intf->cpsw_priv;
+
+	cpsw_ale_del_vlan(cpsw_dev->ale, vid);
+
+	clear_bit(vid, cpsw_intf->active_vlans);
+
+	return 0;
+}
+
+static int cpswx_ioctl(void *intf_priv, struct ifreq *req, int cmd)
+{
+	struct cpswx_intf *cpsw_intf = intf_priv;
+	struct cpswx_slave *slave = cpsw_intf->slaves;
+	struct phy_device *phy = slave->phy;
+	int ret;
+
+	if (!phy)
+		return -EOPNOTSUPP;
+
+	ret = phy_mii_ioctl(phy, req, cmd);
+	if ((cmd == SIOCSHWTSTAMP) && (ret == -ERANGE))
+		ret = -EOPNOTSUPP;
+
+	return ret;
+}
+
+static void cpswx_timer(unsigned long arg)
+{
+	struct cpswx_intf *cpsw_intf = (struct cpswx_intf *)arg;
+	struct cpswx_priv *cpsw_dev = cpsw_intf->cpsw_priv;
+
+	/*
+	 * if the slave's link_interface is not XGMII, sgmii_link bit
+	 * will not be set
+	 */
+	if (cpsw_dev->multi_if)
+		cpsw_intf->sgmii_link =
+			keystone_sgmii_get_port_link(cpsw_dev->sgmii_port_regs,
+						     cpsw_intf->slave_port);
+	else
+		cpsw_intf->sgmii_link =
+			keystone_sgmii_link_status(cpsw_dev->sgmii_port_regs,
+						   cpsw_intf->num_slaves);
+
+	for_each_slave(cpsw_intf, cpsw_slave_link, cpsw_intf);
+
+	/* FIXME: Don't aggregate link statuses in multi-interface case */
+	if (cpsw_intf->sgmii_link) {
+		/* link ON */
+		if (!netif_carrier_ok(cpsw_intf->ndev))
+			netif_carrier_on(cpsw_intf->ndev);
+		/*
+		 * reactivate the transmit queue if
+		 * it is stopped
+		 */
+		if (netif_running(cpsw_intf->ndev) &&
+		    netif_queue_stopped(cpsw_intf->ndev))
+			netif_wake_queue(cpsw_intf->ndev);
+	} else {
+		/* link OFF */
+		if (netif_carrier_ok(cpsw_intf->ndev))
+			netif_carrier_off(cpsw_intf->ndev);
+		if (!netif_queue_stopped(cpsw_intf->ndev))
+			netif_stop_queue(cpsw_intf->ndev);
+	}
+
+	mutex_lock(&cpsw_dev->hw_stats_lock);
+	cpswx_update_stats(cpsw_dev, NULL);
+	mutex_unlock(&cpsw_dev->hw_stats_lock);
+
+	cpsw_intf->timer.expires = jiffies + (HZ/10);
+	add_timer(&cpsw_intf->timer);
+
+	return;
+}
+
+static int cpsw_tx_hook(int order, void *data, struct netcp_packet *p_info)
+{
+	struct cpswx_intf *cpsw_intf = data;
+
+	p_info->tx_pipe = &cpsw_intf->tx_pipe;
+	return 0;
+}
+
+#define	CPSW_TXHOOK_ORDER	0
+
+static int cpswx_open(void *intf_mod_priv, struct net_device *ndev)
+{
+	struct cpswx_intf *cpsw_intf = intf_mod_priv;
+	struct cpswx_priv *cpsw_dev = cpsw_intf->cpsw_priv;
+	struct netcp_priv *netcp = netdev_priv(ndev);
+	struct cpsw_ale_params ale_params;
+	u32 xgmii_mode = 0;
+	int ret = 0;
+	u32 reg, i;
+
+	cpsw_dev->clk = clk_get(cpsw_dev->dev, "clk_xge");
+	if (IS_ERR(cpsw_dev->clk)) {
+		ret = PTR_ERR(cpsw_dev->clk);
+		cpsw_dev->clk = NULL;
+		dev_err(cpsw_dev->dev,
+			"unable to get Keystone XGE clock: %d\n", ret);
+		return ret;
+	}
+
+	ret = clk_prepare_enable(cpsw_dev->clk);
+	if (ret)
+		goto clk_fail;
+
+	reg = __raw_readl(&cpsw_dev->regs->id_ver);
+
+	dev_info(cpsw_dev->dev,
+	 "initialize cpsw version %d.%d (%d) CPSW identification value 0x%x\n",
+	 CPSW_MAJOR_VERSION(reg), CPSW_MINOR_VERSION(reg),
+	 CPSW_RTL_VERSION(reg), CPSW_IDENT(reg));
+
+	ret = netcp_txpipe_open(&cpsw_intf->tx_pipe);
+	if (ret)
+		goto txpipe_fail;
+
+	dev_dbg(cpsw_dev->dev, "opened TX channel %s: %p with psflags %d\n",
+		cpsw_intf->tx_pipe.dma_chan_name,
+		cpsw_intf->tx_pipe.dma_channel,
+		cpsw_intf->tx_pipe.dma_psflags);
+
+	cpsw_dev->ale_refcnt++;
+	if (cpsw_dev->ale_refcnt == 1) {
+		memset(&ale_params, 0, sizeof(ale_params));
+
+		ale_params.dev		= cpsw_dev->dev;
+		ale_params.ale_regs	= (void *)((u32)cpsw_dev->ale_reg);
+		ale_params.ale_ageout	= cpsw_dev->ale_ageout;
+		ale_params.ale_entries	= cpsw_dev->ale_entries;
+		ale_params.ale_ports	= cpsw_dev->ale_ports;
+
+		cpsw_dev->ale = cpsw_ale_create(&ale_params);
+		if (!cpsw_dev->ale) {
+			dev_err(cpsw_dev->dev, "error initializing ale engine\n");
+			ret = -ENODEV;
+			goto ale_fail;
+		} else
+			dev_info(cpsw_dev->dev, "Created a cpsw ale engine\n");
+	}
+
+	for_each_slave(cpsw_intf, cpsw_slave_init, cpsw_dev);
+
+	for_each_slave(cpsw_intf, cpsw_slave_stop, cpsw_dev);
+
+	/* Enable correct MII mode at SS level */
+	for (i = 0; i < cpsw_dev->num_slaves; i++)
+		if (cpsw_dev->link[i] >= XGMII_LINK_MAC_PHY)
+			xgmii_mode |= (1 << i);
+	__raw_writel(xgmii_mode, &cpsw_dev->ss_regs->control);
+
+	/* initialize host and slave ports */
+	cpsw_init_host_port(cpsw_dev, cpsw_intf);
+
+	/* disable priority elevation and enable statistics on all ports */
+	__raw_writel(0, &cpsw_dev->regs->ptype);
+
+	/* Control register */
+	__raw_writel(CPSW_CTL_P0_ENABLE, &cpsw_dev->regs->control);
+
+	/* All statistics enabled by default */
+	__raw_writel(CPSW_REG_VAL_STAT_ENABLE_ALL,
+		     &cpsw_dev->regs->stat_port_en);
+
+	for_each_slave(cpsw_intf, cpsw_slave_open, cpsw_intf);
+
+	init_timer(&cpsw_intf->timer);
+	cpsw_intf->timer.data		= (unsigned long)cpsw_intf;
+	cpsw_intf->timer.function	= cpswx_timer;
+	cpsw_intf->timer.expires	= jiffies + CPSW_TIMER_INTERVAL;
+	add_timer(&cpsw_intf->timer);
+	dev_dbg(cpsw_dev->dev,
+		"%s(): cpswx_timer = %p\n", __func__, cpswx_timer);
+
+	netcp_register_txhook(netcp, CPSW_TXHOOK_ORDER,
+			      cpsw_tx_hook, cpsw_intf);
+
+#if 0
+	/* Configure the streaming switch */
+#define	PSTREAM_ROUTE_DMA	6
+	netcp_set_streaming_switch(cpsw_dev->netcp_device, netcp->cpsw_port,
+				   PSTREAM_ROUTE_DMA);
+#endif
+
+	return 0;
+
+ale_fail:
+	netcp_txpipe_close(&cpsw_intf->tx_pipe);
+txpipe_fail:
+	clk_disable_unprepare(cpsw_dev->clk);
+clk_fail:
+	clk_put(cpsw_dev->clk);
+	cpsw_dev->clk = NULL;
+	return ret;
+}
+
+static int cpswx_close(void *intf_modpriv, struct net_device *ndev)
+{
+	struct cpswx_intf *cpsw_intf = intf_modpriv;
+	struct cpswx_priv *cpsw_dev = cpsw_intf->cpsw_priv;
+	struct netcp_priv *netcp = netdev_priv(ndev);
+
+	del_timer_sync(&cpsw_intf->timer);
+
+	cpsw_dev->ale_refcnt--;
+	if (!cpsw_dev->ale_refcnt)
+		cpsw_ale_stop(cpsw_dev->ale);
+
+	for_each_slave(cpsw_intf, cpsw_slave_stop, cpsw_dev);
+
+	if (!cpsw_dev->ale_refcnt)
+		cpsw_ale_destroy(cpsw_dev->ale);
+
+	netcp_unregister_txhook(netcp, CPSW_TXHOOK_ORDER, cpsw_tx_hook,
+				cpsw_intf);
+	netcp_txpipe_close(&cpsw_intf->tx_pipe);
+
+	clk_disable_unprepare(cpsw_dev->clk);
+	clk_put(cpsw_dev->clk);
+
+	return 0;
+}
+
+static int cpswx_remove(struct netcp_device *netcp_device, void *inst_priv)
+{
+	struct cpswx_priv *cpsw_dev = inst_priv;
+	struct cpswx_intf *cpsw_intf, *tmp;
+
+	of_node_put(cpsw_dev->interfaces);
+
+	list_for_each_entry_safe(cpsw_intf, tmp, &cpsw_dev->cpsw_intf_head,
+				 cpsw_intf_list) {
+		netcp_delete_interface(netcp_device, cpsw_intf->ndev);
+	}
+	BUG_ON(!list_empty(&cpsw_dev->cpsw_intf_head));
+
+	iounmap(cpsw_dev->ss_regs);
+	memset(cpsw_dev, 0x00, sizeof(*cpsw_dev));	/* FIXME: Poison */
+	kfree(cpsw_dev);
+	return 0;
+}
+
+static int init_slave(struct cpswx_priv *cpsw_dev,
+		      struct device_node *node, int slave_num)
+{
+	int ret = 0;
+
+	ret = of_property_read_u32(node, "link-interface",
+				   &cpsw_dev->link[slave_num]);
+	if (ret < 0) {
+		dev_err(cpsw_dev->dev,
+			"missing link-interface value"
+			"defaulting to mac-phy link\n");
+		cpsw_dev->link[slave_num] = XGMII_LINK_MAC_PHY;
+	}
+
+	cpsw_dev->phy_node[slave_num] = of_parse_phandle(node, "phy-handle", 0);
+
+	return 0;
+}
+
+static int cpsw_create_sysfs_entries(struct cpswx_priv *cpsw_dev)
+{
+	struct device *dev = cpsw_dev->dev;
+	int ret;
+
+	ret = kobject_init_and_add(&cpsw_dev->kobj, &cpsw_ktype,
+		kobject_get(&dev->kobj), "cpsw");
+
+	if (ret) {
+		dev_err(dev, "failed to create cpsw sysfs entry\n");
+		kobject_put(&cpsw_dev->kobj);
+		kobject_put(&dev->kobj);
+		return ret;
+	}
+
+	ret = kobject_init_and_add(&cpsw_dev->tx_pri_kobj,
+		&cpsw_tx_pri_ktype,
+		kobject_get(&cpsw_dev->kobj), "port_tx_pri_map");
+
+	if (ret) {
+		dev_err(dev, "failed to create sysfs port_tx_pri_map entry\n");
+		kobject_put(&cpsw_dev->tx_pri_kobj);
+		kobject_put(&cpsw_dev->kobj);
+		return ret;
+	}
+
+	ret = kobject_init_and_add(&cpsw_dev->pvlan_kobj,
+		&cpsw_pvlan_ktype,
+		kobject_get(&cpsw_dev->kobj), "port_vlan");
+
+	if (ret) {
+		dev_err(dev, "failed to create sysfs port_vlan entry\n");
+		kobject_put(&cpsw_dev->pvlan_kobj);
+		kobject_put(&cpsw_dev->kobj);
+		return ret;
+	}
+
+	ret = kobject_init_and_add(&cpsw_dev->stats_kobj,
+		&cpsw_stats_ktype,
+		kobject_get(&cpsw_dev->kobj), "stats");
+
+	if (ret) {
+		dev_err(dev, "failed to create sysfs stats entry\n");
+		kobject_put(&cpsw_dev->stats_kobj);
+		kobject_put(&cpsw_dev->kobj);
+		return ret;
+	}
+
+	return 0;
+}
+
+
+static int cpswx_probe(struct netcp_device *netcp_device,
+			struct device *dev,
+			struct device_node *node,
+			void **inst_priv)
+{
+	struct cpswx_priv *cpsw_dev;
+	struct device_node *slaves, *slave, *interfaces;
+	void __iomem *regs = NULL;
+	struct net_device *ndev;
+	int slave_num = 0;
+	int i, ret = 0;
+	u32 temp[4];
+
+	cpsw_dev = devm_kzalloc(dev, sizeof(struct cpswx_priv), GFP_KERNEL);
+	if (!cpsw_dev) {
+		dev_err(dev, "cpsw_dev memory allocation failed\n");
+		return -ENOMEM;
+	}
+	*inst_priv = cpsw_dev;
+	dev_dbg(dev, "%s(): cpsw_priv = %p\n", __func__, cpsw_dev);
+
+	if (!node) {
+		dev_err(dev, "device tree info unavailable\n");
+		ret = -ENODEV;
+		goto exit;
+	}
+
+	cpsw_dev->dev = dev;
+	cpsw_dev->netcp_device = netcp_device;
+
+	ret = of_property_read_u32(node, "serdes_at_probe",
+				   &cpsw_dev->init_serdes_at_probe);
+	if (ret < 0) {
+		dev_err(dev,
+			"missing serdes_at_probe parameter, err %d\n", ret);
+		cpsw_dev->init_serdes_at_probe = 0;
+	}
+	dev_dbg(dev, "serdes_at_probe %u\n", cpsw_dev->init_serdes_at_probe);
+
+	ret = of_property_read_u32(node, "sgmii_module_ofs",
+				   &cpsw_dev->sgmii_module_ofs);
+	if (ret < 0)
+		dev_err(dev, "missing sgmii module offset, err %d\n", ret);
+
+	ret = of_property_read_u32(node, "pcsr_module_ofs",
+				   &cpsw_dev->pcsr_module_ofs);
+	if (ret < 0)
+		dev_err(dev, "missing pcsr module offset, err %d\n", ret);
+
+	ret = of_property_read_u32(node, "switch_module_ofs",
+				   &cpsw_dev->switch_module_ofs);
+	if (ret < 0)
+		dev_err(dev, "missing switch module offset, err %d\n", ret);
+
+	ret = of_property_read_u32(node, "host_port_reg_ofs",
+				   &cpsw_dev->host_port_reg_ofs);
+	if (ret < 0)
+		dev_err(dev, "missing host port reg offset, err %d\n", ret);
+
+	ret = of_property_read_u32(node, "slave_reg_ofs",
+				   &cpsw_dev->slave_reg_ofs);
+	if (ret < 0)
+		dev_err(dev, "missing slave reg offset, err %d\n", ret);
+
+	ret = of_property_read_u32(node, "sliver_reg_ofs",
+				   &cpsw_dev->sliver_reg_ofs);
+	if (ret < 0)
+		dev_err(dev, "missing sliver reg offset, err %d\n", ret);
+
+	ret = of_property_read_u32(node, "hw_stats_reg_ofs",
+				   &cpsw_dev->hw_stats_reg_ofs);
+	if (ret < 0)
+		dev_err(dev, "missing hw stats reg offset, err %d\n", ret);
+
+	ret = of_property_read_u32(node, "ale_reg_ofs",
+				   &cpsw_dev->ale_reg_ofs);
+	if (ret < 0)
+		dev_err(dev, "missing ale reg offset, err %d\n", ret);
+
+
+	ret = of_property_read_u32(node, "num_slaves", &cpsw_dev->num_slaves);
+	if (ret < 0) {
+		dev_err(dev, "missing num_slaves parameter, err %d\n", ret);
+		cpsw_dev->num_slaves = 2;
+	}
+
+	ret = of_property_read_u32(node, "ale_ageout", &cpsw_dev->ale_ageout);
+	if (ret < 0) {
+		dev_err(dev, "missing ale_ageout parameter, err %d\n", ret);
+		cpsw_dev->ale_ageout = 10;
+	}
+
+	ret = of_property_read_u32(node, "ale_entries", &cpsw_dev->ale_entries);
+	if (ret < 0) {
+		dev_err(dev, "missing ale_entries parameter, err %d\n", ret);
+		cpsw_dev->ale_entries = 1024;
+	}
+
+	ret = of_property_read_u32(node, "ale_ports", &cpsw_dev->ale_ports);
+	if (ret < 0) {
+		dev_err(dev, "missing ale_ports parameter, err %d\n", ret);
+		cpsw_dev->ale_ports = 2;
+	}
+
+	ret = of_property_read_u32(node,
+				   "intf_tx_queues", &cpsw_dev->intf_tx_queues);
+	if (ret < 0) {
+		dev_err(dev, "missing intf_tx_queues parameter, err %d\n", ret);
+		cpsw_dev->intf_tx_queues = 1;
+	}
+
+	if (of_find_property(node, "multi-interface", NULL))
+		cpsw_dev->multi_if = 1;
+
+	ret = of_property_read_u32(node, "num-interfaces",
+				   &cpsw_dev->num_interfaces);
+	if (ret < 0) {
+		dev_err(dev, "missing num-interfaces parameter\n");
+		cpsw_dev->num_interfaces = 1;
+	}
+
+	ret = of_property_read_u32(node, "slaves-per-interface",
+				   &cpsw_dev->slaves_per_interface);
+	if (ret < 0) {
+		dev_err(dev, "missing slaves-per_interface parameter\n");
+		cpsw_dev->slaves_per_interface = 2;
+	}
+
+	/* Sub-sys regs base */
+	if (of_property_read_u32_array(node, "reg", (u32 *)&(temp[0]), 2))
+		dev_err(dev, "No reg defined\n");
+	else
+		regs = ioremap(temp[0], temp[1]);
+	BUG_ON(!regs);
+
+	cpsw_dev->ss_regs = regs;
+	cpsw_dev->sgmii_port_regs	= regs + cpsw_dev->sgmii_module_ofs;
+	cpsw_dev->pcsr_port_regs	= regs + cpsw_dev->pcsr_module_ofs;
+	cpsw_dev->regs			= regs + cpsw_dev->switch_module_ofs;
+	cpsw_dev->host_port_regs	= regs + cpsw_dev->host_port_reg_ofs;
+	cpsw_dev->host_hw_stats_regs	= regs + cpsw_dev->hw_stats_reg_ofs;
+	cpsw_dev->hw_stats_regs[0] = regs + cpsw_dev->hw_stats_reg_ofs + 0x100;
+	cpsw_dev->hw_stats_regs[1] = regs + cpsw_dev->hw_stats_reg_ofs + 0x200;
+	cpsw_dev->ale_reg		= regs + cpsw_dev->ale_reg_ofs;
+
+	ret = of_property_read_u32(node, "host_port", &cpsw_dev->host_port);
+	if (ret < 0) {
+		dev_err(dev, "missing host_port parameter\n");
+		cpsw_dev->host_port = 0;
+	}
+	cpsw_dev->rx_packet_max = 9500;
+
+	dev_dbg(dev, "num_slaves = %d\n", cpsw_dev->num_slaves);
+	dev_dbg(dev, "ale_ageout = %d\n", cpsw_dev->ale_ageout);
+	dev_dbg(dev, "ale_entries = %d\n", cpsw_dev->ale_entries);
+	dev_dbg(dev, "ale_ports = %d\n", cpsw_dev->ale_ports);
+
+	slaves = of_get_child_by_name(node, "slaves");
+	if (!slaves) {
+		dev_err(dev, "could not find slaves\n");
+		ret = -ENODEV;
+		goto exit;
+	}
+
+	for_each_child_of_node(slaves, slave) {
+		init_slave(cpsw_dev, slave, slave_num);
+		slave_num++;
+	}
+
+	of_node_put(slaves);
+
+	interfaces = of_get_child_by_name(node, "interfaces");
+	if (!interfaces)
+		dev_err(dev, "could not find interfaces\n");
+
+	cpsw_dev->interfaces = interfaces;
+
+	if (cpsw_dev->init_serdes_at_probe == 1) {
+		cpsw_dev->clk = clk_get(cpsw_dev->dev, "clk_xge");
+		if (IS_ERR(cpsw_dev->clk)) {
+			ret = PTR_ERR(cpsw_dev->clk);
+			cpsw_dev->clk = NULL;
+			dev_err(cpsw_dev->dev,
+				"unable to get Keystone XGE clock: %d\n", ret);
+			return ret;
+		}
+
+		ret = clk_prepare_enable(cpsw_dev->clk);
+		if (ret)
+			goto exit;
+
+		xge_serdes_init_156p25Mhz();
+	}
+
+	/* Create the interface */
+	INIT_LIST_HEAD(&cpsw_dev->cpsw_intf_head);
+	if (cpsw_dev->multi_if)
+		for (i = 0; i < cpsw_dev->num_interfaces; i++)
+			netcp_create_interface(netcp_device, &ndev,
+					       NULL, cpsw_dev->intf_tx_queues,
+					       1, (i + 1));
+	else
+		netcp_create_interface(netcp_device, &ndev,
+					       NULL, cpsw_dev->intf_tx_queues,
+					       1, 0);
+
+	/* init the hw stats lock */
+	mutex_init(&cpsw_dev->hw_stats_lock);
+
+	ret = cpsw_create_sysfs_entries(cpsw_dev);
+	if (ret)
+		goto exit;
+
+	return 0;
+
+exit:
+	if (cpsw_dev->ss_regs)
+		iounmap(cpsw_dev->ss_regs);
+	*inst_priv = NULL;
+	kfree(cpsw_dev);
+	return ret;
+}
+
+static int cpswx_attach(void *inst_priv, struct net_device *ndev,
+		       void **intf_priv)
+{
+	struct cpswx_priv *cpsw_dev = inst_priv;
+	struct cpswx_intf *cpsw_intf;
+	struct netcp_priv *netcp = netdev_priv(ndev);
+	struct device_node *interface;
+	int i = 0, ret = 0;
+	char node_name[24];
+
+	cpsw_intf = devm_kzalloc(cpsw_dev->dev,
+				 sizeof(struct cpswx_intf), GFP_KERNEL);
+	if (!cpsw_intf) {
+		dev_err(cpsw_dev->dev,
+			"cpswx interface memory allocation failed\n");
+		return -ENOMEM;
+	}
+	cpsw_intf->ndev = ndev;
+	cpsw_intf->dev = cpsw_dev->dev;
+	cpsw_intf->cpsw_priv = cpsw_dev;
+	cpsw_intf->multi_if = cpsw_dev->multi_if;
+
+	if (cpsw_dev->multi_if)
+		snprintf(node_name, sizeof(node_name), "interface-%d",
+			 netcp->cpsw_port - 1);
+	else
+		snprintf(node_name, sizeof(node_name), "interface-%d",
+			 0);
+
+	interface = of_get_child_by_name(cpsw_dev->interfaces, node_name);
+	if (!interface) {
+		dev_err(cpsw_dev->dev, "interface data not available\n");
+		devm_kfree(cpsw_dev->dev, cpsw_intf);
+		return -ENODEV;
+	}
+	ret = of_property_read_u32(interface, "slave_port",
+				   &cpsw_intf->slave_port);
+	if (ret < 0) {
+		dev_err(cpsw_dev->dev, "missing slave_port paramater\n");
+		return -EINVAL;
+	}
+
+	ret = of_property_read_string(interface, "tx-channel",
+				      &cpsw_intf->tx_chan_name);
+	if (ret < 0) {
+		dev_err(cpsw_dev->dev,
+			"missing tx-channel parameter, err %d\n", ret);
+		cpsw_intf->tx_chan_name = "nettx";
+	}
+	dev_info(cpsw_dev->dev, "dma_chan_name %s\n", cpsw_intf->tx_chan_name);
+
+	ret = of_property_read_u32(interface, "tx_queue_depth",
+				   &cpsw_intf->tx_queue_depth);
+	if (ret < 0) {
+		dev_err(cpsw_dev->dev,
+			"missing tx_queue_depth parameter, err %d\n", ret);
+		cpsw_intf->tx_queue_depth = 32;
+	}
+	dev_dbg(cpsw_dev->dev, "tx_queue_depth %u\n",
+		cpsw_intf->tx_queue_depth);
+
+	of_node_put(interface);
+
+	cpsw_intf->num_slaves = cpsw_dev->slaves_per_interface;
+
+	cpsw_intf->slaves = devm_kzalloc(cpsw_dev->dev,
+					 sizeof(struct cpswx_slave) *
+					 cpsw_intf->num_slaves, GFP_KERNEL);
+
+	if (!cpsw_intf->slaves) {
+		dev_err(cpsw_dev->dev,
+			"cpsw interface slave memory allocation failed\n");
+		devm_kfree(cpsw_dev->dev, cpsw_intf);
+		return -ENOMEM;
+	}
+
+	if (cpsw_dev->multi_if) {
+		cpsw_intf->slaves[i].slave_num = cpsw_intf->slave_port;
+		cpsw_intf->slaves[i].link_interface =
+			cpsw_dev->link[cpsw_intf->slave_port];
+		cpsw_intf->phy_node = cpsw_dev->phy_node[cpsw_intf->slave_port];
+	} else {
+		for (i = 0; i < cpsw_intf->num_slaves; i++) {
+			cpsw_intf->slaves[i].slave_num = i;
+			cpsw_intf->slaves[i].link_interface = cpsw_dev->link[i];
+		}
+	}
+
+	netcp_txpipe_init(&cpsw_intf->tx_pipe, netdev_priv(ndev),
+			  cpsw_intf->tx_chan_name, cpsw_intf->tx_queue_depth);
+
+	cpsw_intf->tx_pipe.dma_psflags	= netcp->cpsw_port;
+
+	SET_ETHTOOL_OPS(ndev, &keystone_ethtool_ops);
+
+	list_add(&cpsw_intf->cpsw_intf_list, &cpsw_dev->cpsw_intf_head);
+
+	*intf_priv = cpsw_intf;
+	return 0;
+}
+
+static int cpswx_release(void *intf_modpriv)
+{
+	struct cpswx_intf *cpsw_intf = intf_modpriv;
+
+	SET_ETHTOOL_OPS(cpsw_intf->ndev, NULL);
+
+	list_del(&cpsw_intf->cpsw_intf_list);
+
+	devm_kfree(cpsw_intf->dev, cpsw_intf->slaves);
+	devm_kfree(cpsw_intf->dev, cpsw_intf);
+
+	return 0;
+}
+
+
+static struct netcp_module cpsw_module = {
+	.name		= CPSW_MODULE_NAME,
+	.owner		= THIS_MODULE,
+	.probe		= cpswx_probe,
+	.open		= cpswx_open,
+	.close		= cpswx_close,
+	.remove		= cpswx_remove,
+	.attach		= cpswx_attach,
+	.release	= cpswx_release,
+	.add_addr	= cpswx_add_addr,
+	.del_addr	= cpswx_del_addr,
+	.add_vid	= cpswx_add_vid,
+	.del_vid	= cpswx_del_vid,
+	.ioctl		= cpswx_ioctl,
+};
+
+int __init keystone_cpswx_init(void)
+{
+	return netcp_register_module(&cpsw_module);
+}
+
+void __exit keystone_cpswx_exit(void)
+{
+	netcp_unregister_module(&cpsw_module);
+}
+
+MODULE_LICENSE("GPL v2");
+MODULE_AUTHOR("Sandeep Paulraj <s-paulraj@ti.com>");
+MODULE_DESCRIPTION("CPSW driver for Keystone 10GE devices");
diff --git a/drivers/net/ethernet/ti/tlan.c b/drivers/net/ethernet/ti/tlan.c
index 60c400f..2272538 100644
--- a/drivers/net/ethernet/ti/tlan.c
+++ b/drivers/net/ethernet/ti/tlan.c
@@ -320,7 +320,6 @@ static void tlan_remove_one(struct pci_dev *pdev)
 	free_netdev(dev);
 
 	pci_set_drvdata(pdev, NULL);
-	cancel_work_sync(&priv->tlan_tqueue);
 }
 
 static void tlan_start(struct net_device *dev)
@@ -1912,8 +1911,10 @@ static void tlan_reset_lists(struct net_device *dev)
 		list->frame_size = TLAN_MAX_FRAME_SIZE;
 		list->buffer[0].count = TLAN_MAX_FRAME_SIZE | TLAN_LAST_BUFFER;
 		skb = netdev_alloc_skb_ip_align(dev, TLAN_MAX_FRAME_SIZE + 5);
-		if (!skb)
+		if (!skb) {
+			netdev_err(dev, "Out of memory for received data\n");
 			break;
+		}
 
 		list->buffer[0].address = pci_map_single(priv->pci_dev,
 							 skb->data,
-- 
1.8.4.93.g57e4c17

