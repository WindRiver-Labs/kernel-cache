From 59d6a28cce3be6d069aea8c88bc5559875311261 Mon Sep 17 00:00:00 2001
From: Jiang Lu <lu.jiang@windriver.com>
Date: Fri, 31 May 2013 09:06:20 +0800
Subject: [PATCH] powerpc: Added support for ACP performance counters

Signed-off-by: Paul Butler <paul.butler@windriver.com>
---
 arch/powerpc/include/asm/Kbuild           |    1 +
 arch/powerpc/include/asm/cputable.h       |    1 +
 arch/powerpc/include/asm/oprofile_impl.h  |    1 +
 arch/powerpc/include/asm/perf_event.h     |    4 +
 arch/powerpc/include/asm/perf_event_acp.h |   42 ++
 arch/powerpc/include/asm/reg_acp_pmu.h    |  118 ++++++
 arch/powerpc/include/asm/reg_acp_pmu_fn.h |  166 ++++++++
 arch/powerpc/kernel/Makefile              |    3 +
 arch/powerpc/kernel/cputable.c            |    9 +
 arch/powerpc/kernel/perf_event_acp.c      |  652 +++++++++++++++++++++++++++++
 arch/powerpc/kernel/pmc.c                 |    9 +
 arch/powerpc/kernel/ppc476_pmu.c          |  125 ++++++
 arch/powerpc/oprofile/Makefile            |    1 +
 arch/powerpc/oprofile/op_model_acp_pmu.c  |  282 +++++++++++++
 arch/powerpc/platforms/Kconfig.cputype    |   16 +
 15 files changed, 1430 insertions(+), 0 deletions(-)
 create mode 100644 arch/powerpc/include/asm/perf_event_acp.h
 create mode 100644 arch/powerpc/include/asm/reg_acp_pmu.h
 create mode 100644 arch/powerpc/include/asm/reg_acp_pmu_fn.h
 create mode 100644 arch/powerpc/kernel/perf_event_acp.c
 create mode 100644 arch/powerpc/kernel/ppc476_pmu.c
 create mode 100644 arch/powerpc/oprofile/op_model_acp_pmu.c

diff --git a/arch/powerpc/include/asm/Kbuild b/arch/powerpc/include/asm/Kbuild
index 7e313f1..b308e90 100644
--- a/arch/powerpc/include/asm/Kbuild
+++ b/arch/powerpc/include/asm/Kbuild
@@ -34,5 +34,6 @@ header-y += termios.h
 header-y += types.h
 header-y += ucontext.h
 header-y += unistd.h
+header-y += reg_acp_pmu.h
 
 generic-y += rwsem.h
diff --git a/arch/powerpc/include/asm/cputable.h b/arch/powerpc/include/asm/cputable.h
index 3727508..3c3dca1 100644
--- a/arch/powerpc/include/asm/cputable.h
+++ b/arch/powerpc/include/asm/cputable.h
@@ -56,6 +56,7 @@ enum powerpc_oprofile_type {
 	PPC_OPROFILE_FSL_EMB = 4,
 	PPC_OPROFILE_CELL = 5,
 	PPC_OPROFILE_PA6T = 6,
+	PPC_OPROFILE_ACP_PMU = 7,
 };
 
 enum powerpc_pmc_type {
diff --git a/arch/powerpc/include/asm/oprofile_impl.h b/arch/powerpc/include/asm/oprofile_impl.h
index 639dc96..39e5c1f 100644
--- a/arch/powerpc/include/asm/oprofile_impl.h
+++ b/arch/powerpc/include/asm/oprofile_impl.h
@@ -66,6 +66,7 @@ extern struct op_powerpc_model op_model_power4;
 extern struct op_powerpc_model op_model_7450;
 extern struct op_powerpc_model op_model_cell;
 extern struct op_powerpc_model op_model_pa6t;
+extern struct op_powerpc_model op_model_acp_pmu;
 
 
 /* All the classic PPC parts use these */
diff --git a/arch/powerpc/include/asm/perf_event.h b/arch/powerpc/include/asm/perf_event.h
index 5c16b89..0cc855f 100644
--- a/arch/powerpc/include/asm/perf_event.h
+++ b/arch/powerpc/include/asm/perf_event.h
@@ -22,6 +22,10 @@
 #include <asm/perf_event_fsl_emb.h>
 #endif
 
+#ifdef CONFIG_ACP_PMU_PERF_EVENT
+#include <asm/perf_event_acp.h>
+#endif
+
 #ifdef CONFIG_PERF_EVENTS
 #include <asm/ptrace.h>
 #include <asm/reg.h>
diff --git a/arch/powerpc/include/asm/perf_event_acp.h b/arch/powerpc/include/asm/perf_event_acp.h
new file mode 100644
index 0000000..0a57dd3
--- /dev/null
+++ b/arch/powerpc/include/asm/perf_event_acp.h
@@ -0,0 +1,42 @@
+#ifndef PERF_EVENT_ACP_H
+#define PERF_EVENT_ACP_H
+
+/*
+ * Performance counter support for LSI Axxia3400
+ *
+ * Based on earlier code:
+ *
+ * perf_event_fsl_emb.h
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+
+#include <linux/types.h>
+#include <asm/hw_irq.h>
+
+#define MAX_HWEVENTS 8
+
+/* event flags */
+#define ACP_EVENT_VALID      1
+#define ACP_EVENT_RESTRICTED 2
+
+
+struct acp_pmu {
+	const char	*name;
+	int		n_counter; /* total number of counters */
+	/* Returns event flags */
+	u64		(*xlate_event)(u64 event_id);
+
+	int		n_generic;
+	int		*generic_events;
+	int		(*cache_events)[PERF_COUNT_HW_CACHE_MAX]
+			       [PERF_COUNT_HW_CACHE_OP_MAX]
+			       [PERF_COUNT_HW_CACHE_RESULT_MAX];
+};
+
+int register_acp_pmu(struct acp_pmu *);
+
+#endif
diff --git a/arch/powerpc/include/asm/reg_acp_pmu.h b/arch/powerpc/include/asm/reg_acp_pmu.h
new file mode 100644
index 0000000..8ab82eb
--- /dev/null
+++ b/arch/powerpc/include/asm/reg_acp_pmu.h
@@ -0,0 +1,118 @@
+/*
+ * Register definitions for the LSI Axxia3400 Embedded Performance
+ * Monitor.
+ *
+ * Portions derived from CPP platform legacy perf driver
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+#ifndef __ASM_POWERPC_REG_ACP_PMU_H__
+#define __ASM_POWERPC_REG_ACP_PMU_H__
+
+#define L2_LWARX_COMPLETE            (0x00)
+#define L2_STWCX_SUCCESS             (0x01)
+#define L2_MISS_EVICTION             (0x02)
+#define L2_MISS_D_FETCH              (0x03)
+#define L2_MISS_I_FETCH              (0x04)
+#define L2_MISS_STORE                (0x05)
+#define L2_HIT_D_FETCH               (0x06)
+#define L2_HIT_I_FETCH               (0x07)
+#define L2_HIT_STORE                 (0x08)
+#define L2_READ_AFTER_WRITE          (0x09)
+#define L2_WRITE_AFTER_WRITE         (0x0a)
+#define PLB_MASTER_COMMAND           (0x0b)
+#define PLB_MASTER_READ              (0x0c)
+#define PLB_MASTER_RWITM             (0x0d)
+#define PLB_MASTER_DCLAIM            (0x0e)
+#define PLB_MASTER_WRITE             (0x0f)
+#define PLB_READ_OCCUPANCY           (0x10)
+#define PLB_MASTER_INTVN_M           (0x11)
+#define PLB_MASTER_INTVN_S           (0x12)
+#define PLB_MASTER_MEM_DATA          (0x13)
+#define PLB_SNOOP_CMD                (0x14)
+#define PLB_SNOOP_L2_CMD             (0x15)
+#define PLB_SNOOP_HIT_INTVN          (0x16)
+#define PLB_SNOOP_HIT                (0x17)
+#define PLB_SNOOP_RETRY              (0x18)
+#define CPU_COMMITTED_INST           (0x19)
+#define CPU_DCACHE_HIT               (0x1a)
+#define CPU_DTLB_RELOAD              (0x1b)
+#define CPU_ICACHE_HIT               (0x1c)
+#define CPU_ITLB_RELOAD              (0x1d)
+#define L2_CYCLE_COUNT               (0x1e)
+#define CPU_CYCLE_COUNT              (0x1f)
+
+#ifdef __KERNEL__
+
+#include <asm/dcr-native.h>
+
+/* LSI ACP ppc476  Performance Monitor Registers */
+
+/* Address and Data Indirect Register */
+#define PMUDCRAI(core)               (0x80 + 0x300 + (core) * 0x100)
+#define PMUDCRDI(core)               (0x84 + 0x300 + (core) * 0x100)
+
+/* Global Control Registers */
+
+#define PMRN_PMUGS0     0x000 /* PMU Global Status Register */
+
+#define PMUGS_PMC_STATE(nr) (1<<(31-(nr))) /* Stop/Start state of PMUC(nr),
+						1== start */
+#define PMUGS_CPUFAC    (1 << (31-29)) /* PMU_C476FAC signal input */
+#define PMUGS_CPUR      (1 << (31-30)) /* PMU_C476PR signal input */
+#define PMUGS_CPUMM     (1 << (31-31)) /* PMU_C476MM signal input */
+
+#define PMRN_PMUGC0     0x004 /* PMU Global Control Register */
+
+#define PMUGC0_PMCC      (1 << (31-15)) /* Returns all counters to zero */
+#define PMUGC0_LFAC      (1 << (31-30)) /* Freeze all counters */
+#define PMUGC0_FCEC      (1 << (31-31)) /* Freeze cntrs on enabled Condition */
+
+#define PMRN_PMUIS0     0x010 /* PMU Global Interrupt Status Register */
+
+#define PMUIS_ISTAT(nr) (1 << (31-(nr))) /* Interrupt status of PMUC(nr),
+					 1== counter has an enable condition */
+
+#define PMRN_PMUIE0     0x014 /* PMU Global Interrupt Control Register */
+
+#define PMUIE_IE(nr)	(1 << (31-(nr))) /* Interrupt enable of PMUC(nr),
+					 1== Interrupt enable for PMUC(nr) */
+
+#define PMRN_REVID      0xC00 /* PMU Revision ID register */
+
+/* Local Counter registers */
+
+#define PMRN_PMCA0	0x808	/* Performance Monitor Counter 0 */
+#define PMRN_PMCA1	0x818	/* Performance Monitor Counter 1 */
+#define PMRN_PMCA2	0x828	/* Performance Monitor Counter 2 */
+#define PMRN_PMCA3	0x838	/* Performance Monitor Counter 3 */
+#define PMRN_PMCA4	0x908	/* Performance Monitor Counter 4 */
+#define PMRN_PMCA5	0x918	/* Performance Monitor Counter 5 */
+#define PMRN_PMCA6	0x928	/* Performance Monitor Counter 6 */
+#define PMRN_PMCA7	0x938	/* Performance Monitor Counter 7 */
+#define PMRN_PMLCA0	0x800	/* PM Local Counter Control Register 0 */
+#define PMRN_PMLCA1	0x810	/* PM Local Counter Control Register 1 */
+#define PMRN_PMLCA2	0x820	/* PM Local Counter Control Register 2 */
+#define PMRN_PMLCA3	0x830	/* PM Local Counter Control Register 3 */
+#define PMRN_PMLCA4	0x900	/* PM Local Counter Control Register 4 */
+#define PMRN_PMLCA5	0x910	/* PM Local Counter Control Register 5 */
+#define PMRN_PMLCA6	0x920	/* PM Local Counter Control Register 6 */
+#define PMRN_PMLCA7	0x930	/* PM Local Counter Control Register 7 */
+
+
+#define PMLCA_FC	(1 << (31-10))	/* Freeze Counter */
+#define PMLCA_FCS	(1 << (31-11))	/* Freeze in Supervisor */
+#define PMLCA_FCU	(1 << (31-12))	/* Freeze in User */
+#define PMLCA_FCM1	(1 << (31-13))	/* Freeze Counter while Mark is Set */
+#define PMLCA_FCM0	(1 << (31-14))	/* Freeze Counter while Mark is Clrd */
+#define PMLCA_CE	(1 << (31-15))	/* Condition Enable */
+
+#define PMLCA_EVENT_MASK 0x0000003f	/* Event field */
+#define PMLCA_EVENT_SHIFT 0
+
+#endif /* __KERNEL__ */
+
+#endif /* __ASM_POWERPC_REG_ACP_PMU_H__ */
diff --git a/arch/powerpc/include/asm/reg_acp_pmu_fn.h b/arch/powerpc/include/asm/reg_acp_pmu_fn.h
new file mode 100644
index 0000000..8811b6b
--- /dev/null
+++ b/arch/powerpc/include/asm/reg_acp_pmu_fn.h
@@ -0,0 +1,166 @@
+/*
+ * Register read/write for the LSI Axxia3400 Embedded Performance
+ * Monitor.
+ *
+ * Portions derived from CPP platform legacy perf driver
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+
+#ifdef __KERNEL__
+#ifndef __ASM_POWERPC_REG_ACP_PMU_FN_H__
+#define __ASM_POWERPC_REG_ACP_PMU_FN_H__
+
+#include <asm/reg_acp_pmu.h>
+
+/* LSI ACP ppc476  Performance Monitor Registers */
+
+/* common oprofile and perf event read/write pmu registers */
+
+/* Get local counter control register */
+
+static inline u32 get_pmlc(int core, int ctr)
+{
+	switch (ctr) {
+	case 0:
+		mtdcrx(PMUDCRAI(core),  PMRN_PMLCA0);
+		break;
+	case 1:
+		mtdcrx(PMUDCRAI(core),  PMRN_PMLCA1);
+		break;
+	case 2:
+		mtdcrx(PMUDCRAI(core),  PMRN_PMLCA2);
+		break;
+	case 3:
+		mtdcrx(PMUDCRAI(core),  PMRN_PMLCA3);
+		break;
+	case 4:
+		mtdcrx(PMUDCRAI(core),  PMRN_PMLCA4);
+		break;
+	case 5:
+		mtdcrx(PMUDCRAI(core),  PMRN_PMLCA5);
+		break;
+	case 6:
+		mtdcrx(PMUDCRAI(core),  PMRN_PMLCA6);
+		break;
+	case 7:
+		mtdcrx(PMUDCRAI(core),  PMRN_PMLCA7);
+		break;
+	default:
+		printk(KERN_ERR "oops trying to read PMC%d\n", ctr);
+		return 0;
+	}
+	return mfdcrx(PMUDCRDI(core));
+}
+
+/* Set local counter control register */
+
+static inline void set_pmlc(int core, int ctr, u32 pmlc)
+{
+	switch (ctr) {
+	case 0:
+		mtdcrx(PMUDCRAI(core),  PMRN_PMLCA0);
+		break;
+	case 1:
+		mtdcrx(PMUDCRAI(core),  PMRN_PMLCA1);
+		break;
+	case 2:
+		mtdcrx(PMUDCRAI(core),  PMRN_PMLCA2);
+		break;
+	case 3:
+		mtdcrx(PMUDCRAI(core),  PMRN_PMLCA3);
+		break;
+	case 4:
+		mtdcrx(PMUDCRAI(core),  PMRN_PMLCA4);
+		break;
+	case 5:
+		mtdcrx(PMUDCRAI(core),  PMRN_PMLCA5);
+		break;
+	case 6:
+		mtdcrx(PMUDCRAI(core),  PMRN_PMLCA6);
+		break;
+	case 7:
+		mtdcrx(PMUDCRAI(core),  PMRN_PMLCA7);
+		break;
+	default:
+		printk(KERN_ERR "oops trying to read PMC%d\n", ctr);
+		return;
+	}
+	mtdcrx(PMUDCRDI(core), pmlc);
+}
+
+/* Get local counter register */
+
+static inline unsigned int ctr_read(int core, unsigned int i)
+{
+	switch (i) {
+	case 0:
+		mtdcrx(PMUDCRAI(core),  PMRN_PMCA0);
+		break;
+	case 1:
+		mtdcrx(PMUDCRAI(core),  PMRN_PMCA1);
+		break;
+	case 2:
+		mtdcrx(PMUDCRAI(core),  PMRN_PMCA2);
+		break;
+	case 3:
+		mtdcrx(PMUDCRAI(core),  PMRN_PMCA3);
+		break;
+	case 4:
+		mtdcrx(PMUDCRAI(core),  PMRN_PMCA4);
+		break;
+	case 5:
+		mtdcrx(PMUDCRAI(core),  PMRN_PMCA5);
+		break;
+	case 6:
+		mtdcrx(PMUDCRAI(core),  PMRN_PMCA6);
+		break;
+	case 7:
+		mtdcrx(PMUDCRAI(core),  PMRN_PMCA7);
+		break;
+	default:
+		return 0;
+	}
+	return mfdcrx(PMUDCRDI(core));
+}
+
+/* Set local counter register */
+
+static inline void ctr_write(int core, unsigned int i, unsigned int val)
+{
+	switch (i) {
+	case 0:
+		mtdcrx(PMUDCRAI(core),  PMRN_PMCA0);
+		break;
+	case 1:
+		mtdcrx(PMUDCRAI(core),  PMRN_PMCA1);
+		break;
+	case 2:
+		mtdcrx(PMUDCRAI(core),  PMRN_PMCA2);
+		break;
+	case 3:
+		mtdcrx(PMUDCRAI(core),  PMRN_PMCA3);
+		break;
+	case 4:
+		mtdcrx(PMUDCRAI(core),  PMRN_PMCA4);
+		break;
+	case 5:
+		mtdcrx(PMUDCRAI(core),  PMRN_PMCA5);
+		break;
+	case 6:
+		mtdcrx(PMUDCRAI(core),  PMRN_PMCA6);
+		break;
+	case 7:
+		mtdcrx(PMUDCRAI(core),  PMRN_PMCA7);
+		break;
+	default:
+		return;
+	}
+	mtdcrx(PMUDCRDI(core), val);
+}
+
+#endif /* __ASM_POWERPC_REG_ACP_PMU_H__ */
+#endif /* __KERNEL__ */
diff --git a/arch/powerpc/kernel/Makefile b/arch/powerpc/kernel/Makefile
index 825c37f..0e509dd 100644
--- a/arch/powerpc/kernel/Makefile
+++ b/arch/powerpc/kernel/Makefile
@@ -116,6 +116,9 @@ obj-$(CONFIG_DYNAMIC_FTRACE)	+= ftrace.o
 obj-$(CONFIG_FUNCTION_GRAPH_TRACER)	+= ftrace.o
 obj-$(CONFIG_FTRACE_SYSCALLS)	+= ftrace.o
 
+obj-$(CONFIG_ACP_PMU_PERF_EVENT) += perf_event_acp.o
+obj-$(CONFIG_ACP_PMU_PERF_EVENT_PPC476) += ppc476_pmu.o
+
 obj-$(CONFIG_8XX_MINIMAL_FPEMU) += softemu8xx.o
 
 ifneq ($(CONFIG_PPC_INDIRECT_IO),y)
diff --git a/arch/powerpc/kernel/cputable.c b/arch/powerpc/kernel/cputable.c
index 88479f1..38d2e6c 100644
--- a/arch/powerpc/kernel/cputable.c
+++ b/arch/powerpc/kernel/cputable.c
@@ -1842,6 +1842,9 @@ static struct cpu_spec __initdata cpu_specs[] = {
 		.icache_bsize		= 32,
 		.dcache_bsize		= 128,
 		.machine_check		= machine_check_47x,
+		.num_pmcs		= 8,
+		.oprofile_cpu_type      = "ppc/476",
+		.oprofile_type          = PPC_OPROFILE_ACP_PMU,
 		.platform		= "ppc470",
 	},
 	{ /* X2 DD2 core */
@@ -1884,6 +1887,9 @@ static struct cpu_spec __initdata cpu_specs[] = {
 		.icache_bsize		= 32,
 		.dcache_bsize		= 128,
 		.machine_check		= machine_check_47x,
+		.num_pmcs		= 8,
+		.oprofile_cpu_type      = "ppc/476",
+		.oprofile_type          = PPC_OPROFILE_ACP_PMU,
 		.platform		= "ppc470",
 	},
 	{ /* 476 DD3 core */
@@ -1899,6 +1905,9 @@ static struct cpu_spec __initdata cpu_specs[] = {
 		.dcache_bsize		= 128,
 		.machine_check		= machine_check_47x,
 		.platform		= "ppc470",
+		.num_pmcs		= 8,
+		.oprofile_cpu_type      = "ppc/476",
+		.oprofile_type          = PPC_OPROFILE_ACP_PMU,
 	},
 	{ /* 476 ACP25xx */
 		.pvr_mask		= 0x7ff520c1,
diff --git a/arch/powerpc/kernel/perf_event_acp.c b/arch/powerpc/kernel/perf_event_acp.c
new file mode 100644
index 0000000..eb76435
--- /dev/null
+++ b/arch/powerpc/kernel/perf_event_acp.c
@@ -0,0 +1,652 @@
+/*
+ * Performance event support - LSI ACP Embedded Performance Monitor
+ *
+ * Based on earlier code:
+ *
+ * perf_event_fsl_emb.c
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/perf_event.h>
+#include <linux/percpu.h>
+#include <linux/hardirq.h>
+#include <asm/reg_acp_pmu_fn.h>
+#include <asm/pmc.h>
+#include <asm/machdep.h>
+#include <asm/firmware.h>
+#include <asm/ptrace.h>
+
+
+struct cpu_hw_events {
+	int n_events;
+	int disabled;
+	u8  pmcs_enabled;
+	struct perf_event *event[MAX_HWEVENTS];
+};
+static DEFINE_PER_CPU(struct cpu_hw_events, cpu_hw_events);
+
+static struct acp_pmu *ppmu;
+
+/* Number of perf_events counting hardware events */
+static atomic_t num_events;
+/* Used to avoid races in calling reserve/release_pmc_hardware */
+static DEFINE_MUTEX(pmc_reserve_mutex);
+
+/*
+ * If interrupts were soft-disabled when a PMU interrupt occurs, treat
+ * it as an NMI.
+ */
+static inline int perf_intr_is_nmi(struct pt_regs *regs)
+{
+#ifdef __powerpc64__
+	return !regs->softe;
+#else
+	return 0;
+#endif
+}
+
+static void perf_event_interrupt(struct pt_regs *regs);
+
+
+static void acp_pmu_read(struct perf_event *event)
+{
+	int core = smp_processor_id();
+	s64 val, delta, prev;
+
+	if (event->hw.state & PERF_HES_STOPPED)
+		return;
+
+	/*
+	 * Performance monitor interrupts come even when interrupts
+	 * are soft-disabled, as long as interrupts are hard-enabled.
+	 * Therefore we treat them like NMIs.
+	 */
+	do {
+		prev = local64_read(&event->hw.prev_count);
+		barrier();
+		val = ctr_read(core, event->hw.idx);
+	} while (local64_cmpxchg(&event->hw.prev_count, prev, val) != prev);
+
+	/* The counters are only 32 bits wide */
+	delta = (val - prev) & 0xfffffffful;
+	local64_add(delta, &event->count);
+	local64_sub(delta, &event->hw.period_left);
+}
+
+/*
+ * Disable all events to prevent PMU interrupts and to allow
+ * events to be added or removed.
+ */
+static void acp_pmu_disable(struct pmu *pmu)
+{
+	int core = smp_processor_id();
+	struct cpu_hw_events *cpuhw;
+	unsigned long flags;
+
+	local_irq_save(flags);
+	cpuhw = &__get_cpu_var(cpu_hw_events);
+
+	pr_debug("--- %s ---\n", __func__);
+
+	if (!cpuhw->disabled) {
+		cpuhw->disabled = 1;
+
+		/*
+		 * Check if we ever enabled the PMU on this cpu.
+		 */
+		if (!cpuhw->pmcs_enabled) {
+			ppc_enable_pmcs();
+			cpuhw->pmcs_enabled = 1;
+		}
+
+		if (atomic_read(&num_events)) {
+			u32 pmgc0;
+			/*
+			 * Set the 'freeze all counters' bit, and disable
+			 * interrupts.  The barrier is to make sure the
+			 * mtpmr has been executed and the PMU has frozen
+			 * the events before we return.
+			 */
+			mtdcrx(PMUDCRAI(core), PMRN_PMUGC0);
+			pmgc0 = mfdcrx(PMUDCRDI(core));
+			pmgc0 |= PMUGC0_LFAC;
+			pmgc0 &= ~PMUGC0_FCEC;
+			mtdcrx(PMUDCRDI(core), pmgc0);
+			mtdcrx(PMUDCRAI(core), PMRN_PMUIE0);
+			mtdcrx(PMUDCRDI(core), 0);
+
+			isync();
+		}
+	}
+	local_irq_restore(flags);
+}
+
+/*
+ * Re-enable all events if disable == 0.
+ * If we were previously disabled and events were added, then
+ * put the new config on the PMU.
+ */
+static void acp_pmu_enable(struct pmu *pmu)
+{
+	int core = smp_processor_id();
+	struct cpu_hw_events *cpuhw;
+	unsigned long flags;
+
+	pr_debug("--- %s ---\n", __func__);
+	local_irq_save(flags);
+	cpuhw = &__get_cpu_var(cpu_hw_events);
+	if (!cpuhw->disabled)
+		goto out;
+
+	cpuhw->disabled = 0;
+	ppc_set_pmu_inuse(cpuhw->n_events != 0);
+
+	if (cpuhw->n_events > 0) {
+		u32 pmgc0;
+		u32 pmuie0 = 0;
+		int i;
+		int num_counters = ppmu->n_counter;
+
+		for (i = 0; i < num_counters; i++) {
+			if (cpuhw->event[i])
+				pmuie0 |= PMUIE_IE(i);
+		}
+		mtdcrx(PMUDCRAI(core), PMRN_PMUIE0);
+		mtdcrx(PMUDCRDI(core), pmuie0);
+
+		mtdcrx(PMUDCRAI(core), PMRN_PMUGC0);
+		pmgc0 = mfdcrx(PMUDCRDI(core));
+		pmgc0 &= ~PMUGC0_LFAC; /* un-freeze all counters */
+		pmgc0 |= PMUGC0_FCEC; /* enable freeze all ctrs on of */
+		mtdcrx(PMUDCRDI(core), pmgc0);
+
+		isync();
+	}
+
+ out:
+	local_irq_restore(flags);
+}
+
+static int collect_events(struct perf_event *group, int max_count,
+			  struct perf_event *ctrs[])
+{
+	int n = 0;
+	struct perf_event *event;
+
+	if (!is_software_event(group)) {
+		if (n >= max_count)
+			return -1;
+		ctrs[n] = group;
+		n++;
+	}
+	list_for_each_entry(event, &group->sibling_list, group_entry) {
+		if (!is_software_event(event) &&
+		    event->state != PERF_EVENT_STATE_OFF) {
+			if (n >= max_count)
+				return -1;
+			ctrs[n] = event;
+			n++;
+		}
+	}
+	return n;
+}
+
+/* context locked on entry */
+static int acp_pmu_add(struct perf_event *event, int flags)
+{
+	struct cpu_hw_events *cpuhw;
+	int core = smp_processor_id();
+	int ret = -EAGAIN;
+	int num_counters = ppmu->n_counter;
+	u64 val;
+	int i;
+	u32 pmuie0;
+
+	perf_pmu_disable(event->pmu);
+	cpuhw = &get_cpu_var(cpu_hw_events);
+	/*
+	 * Allocate counters from top-down, so that restricted-capable
+	 * counters are kept free as long as possible.
+	 */
+/*	for (i = num_counters - 1; i >= 0; i--) { */
+	for (i = 0; i < num_counters; i++) {
+		if (cpuhw->event[i])
+			continue;
+
+		break;
+	}
+
+	if (i < 0)
+		goto out;
+
+	event->hw.idx = i;
+	cpuhw->event[i] = event;
+	++cpuhw->n_events;
+
+	pr_debug("--- %s --- cnt id %d config_base %lx\n", __func__,
+			 event->hw.idx, event->hw.config_base);
+
+	val = 0;
+	if (event->hw.sample_period) {
+		s64 left = local64_read(&event->hw.period_left);
+		if (left < 0x80000000L)
+			val = 0x80000000L - left;
+	}
+	local64_set(&event->hw.prev_count, val);
+
+	if (!(flags & PERF_EF_START)) {
+		event->hw.state = PERF_HES_STOPPED | PERF_HES_UPTODATE;
+		val = 0;
+	}
+
+	ctr_write(core, i, val);
+	perf_event_update_userpage(event);
+
+	set_pmlc(core, i, event->hw.config_base);
+
+	/* Enable counter interrupt on overflow condition */
+	mtdcrx(PMUDCRAI(core), PMRN_PMUIE0);
+	pmuie0 = mfdcrx(PMUDCRDI(core));
+	pmuie0 |= PMUIE_IE(i);
+	mtdcrx(PMUDCRDI(core), pmuie0);
+
+	ret = 0;
+ out:
+	put_cpu_var(cpu_hw_events);
+	perf_pmu_enable(event->pmu);
+	return ret;
+}
+
+/* context locked on entry */
+static void acp_pmu_del(struct perf_event *event, int flags)
+{
+	int core = smp_processor_id();
+	struct cpu_hw_events *cpuhw;
+	int i = event->hw.idx;
+	pr_debug("--- %s --- cnt %d\n", __func__, i);
+	perf_pmu_disable(event->pmu);
+	if (i < 0)
+		goto out;
+
+	acp_pmu_read(event);
+
+	cpuhw = &get_cpu_var(cpu_hw_events);
+
+	WARN_ON(event != cpuhw->event[event->hw.idx]);
+
+	set_pmlc(core, i, 0);
+	ctr_write(core, i, 0);
+
+	cpuhw->event[i] = NULL;
+	event->hw.idx = -1;
+
+	/*
+	 * TODO: if at least one restricted event exists, and we
+	 * just freed up a non-restricted-capable counter, and
+	 * there is a restricted-capable counter occupied by
+	 * a non-restricted event, migrate that event to the
+	 * vacated counter.
+	 */
+
+	cpuhw->n_events--;
+
+ out:
+	perf_pmu_enable(event->pmu);
+	put_cpu_var(cpu_hw_events);
+}
+
+static void acp_pmu_start(struct perf_event *event, int ef_flags)
+{
+	int core = smp_processor_id();
+	unsigned long flags;
+	s64 left;
+	pr_debug("--- %s --- cnt %d\n", __func__, event->hw.idx);
+	if (event->hw.idx < 0 || !event->hw.sample_period)
+		return;
+
+	if (!(event->hw.state & PERF_HES_STOPPED))
+		return;
+
+	if (ef_flags & PERF_EF_RELOAD)
+		WARN_ON_ONCE(!(event->hw.state & PERF_HES_UPTODATE));
+
+	local_irq_save(flags);
+	perf_pmu_disable(event->pmu);
+
+	event->hw.state = 0;
+	left = local64_read(&event->hw.period_left);
+	ctr_write(core, event->hw.idx, left);
+
+	perf_event_update_userpage(event);
+	perf_pmu_enable(event->pmu);
+	local_irq_restore(flags);
+}
+
+static void acp_pmu_stop(struct perf_event *event, int ef_flags)
+{
+	unsigned long flags;
+	int core = smp_processor_id();
+	pr_debug("--- %s --- cnt %d\n", __func__, event->hw.idx);
+	if (event->hw.idx < 0 || !event->hw.sample_period)
+		return;
+
+	if (event->hw.state & PERF_HES_STOPPED)
+		return;
+
+	local_irq_save(flags);
+	perf_pmu_disable(event->pmu);
+
+	acp_pmu_read(event);
+	event->hw.state |= PERF_HES_STOPPED | PERF_HES_UPTODATE;
+	ctr_write(core, event->hw.idx, 0);
+
+	perf_event_update_userpage(event);
+	perf_pmu_enable(event->pmu);
+	local_irq_restore(flags);
+}
+
+/*
+ * Release the PMU if this is the last perf_event.
+ */
+static void hw_perf_event_destroy(struct perf_event *event)
+{
+	pr_debug("--- %s ---\n", __func__);
+
+	if (!atomic_add_unless(&num_events, -1, 1)) {
+		mutex_lock(&pmc_reserve_mutex);
+		if (atomic_dec_return(&num_events) == 0)
+			release_pmc_hardware();
+		mutex_unlock(&pmc_reserve_mutex);
+	}
+}
+
+/*
+ * Translate a generic cache event_id config to a raw event_id code.
+ */
+static int hw_perf_cache_event(u64 config, u64 *eventp)
+{
+	unsigned long type, op, result;
+	int ev;
+
+	if (!ppmu->cache_events)
+		return -EINVAL;
+
+	/* unpack config */
+	type = config & 0xff;
+	op = (config >> 8) & 0xff;
+	result = (config >> 16) & 0xff;
+	pr_debug("--- %s --- hw cache event type %lu op %lu result %lu\n",
+		 __func__, type, op, result);
+
+	if (type >= PERF_COUNT_HW_CACHE_MAX ||
+	    op >= PERF_COUNT_HW_CACHE_OP_MAX ||
+	    result >= PERF_COUNT_HW_CACHE_RESULT_MAX)
+		return -EINVAL;
+
+	ev = (*ppmu->cache_events)[type][op][result];
+	if (ev == 0)
+		return -EOPNOTSUPP;
+	if (ev == -1)
+		return -EINVAL;
+	*eventp = ev;
+	return 0;
+}
+
+static int acp_pmu_event_init(struct perf_event *event)
+{
+	u64 ev;
+	struct perf_event *events[MAX_HWEVENTS];
+	int n;
+	int err;
+
+	switch (event->attr.type) {
+	case PERF_TYPE_HARDWARE:
+		ev = event->attr.config;
+		if (ev >= ppmu->n_generic || ppmu->generic_events[ev] == 0)
+			return -EOPNOTSUPP;
+		pr_debug("--- %s --- hardware event %llu\n",
+			 __func__, ev);
+		ev = ppmu->generic_events[ev];
+		pr_debug("--- %s --- acp map event %llu\n",
+			 __func__, ev);
+		break;
+
+	case PERF_TYPE_HW_CACHE:
+		err = hw_perf_cache_event(event->attr.config, &ev);
+		if (err)
+			return err;
+		pr_debug("--- %s --- acp map event %llu\n",
+			 __func__, ev);
+		break;
+
+	case PERF_TYPE_RAW:
+		ev = event->attr.config;
+		pr_debug("--- %s --- raw event %llu\n",
+			 __func__, ev);
+		break;
+
+	default:
+		return -ENOENT;
+	}
+
+	event->hw.config = ppmu->xlate_event(ev);
+	if (!(event->hw.config & ACP_EVENT_VALID))
+		return -EINVAL;
+
+	/*
+	 * If this is in a group, check if it can go on with all the
+	 * other hardware events in the group.  We assume the event
+	 * hasn't been linked into its leader's sibling list at this point.
+	 */
+	n = 0;
+	if (event->group_leader != event) {
+		n = collect_events(event->group_leader,
+					ppmu->n_counter - 1, events);
+		if (n < 0)
+			return -EINVAL;
+		pr_debug("--- %s --- Add new event to group\n",
+			 __func__);
+	}
+	event->hw.idx = -1;
+
+	event->hw.config_base = PMLCA_CE | PMLCA_FCM1 |
+				(u32)((ev) & PMLCA_EVENT_MASK);
+
+	if (event->attr.exclude_user)
+		event->hw.config_base |= PMLCA_FCU;
+	if (event->attr.exclude_kernel)
+		event->hw.config_base |= PMLCA_FCS;
+	if (event->attr.exclude_idle)
+		return -ENOTSUPP;
+
+	pr_debug("--- %s --- hwconfig base %lx\n",
+		 __func__, event->hw.config_base);
+
+	event->hw.last_period = event->hw.sample_period;
+	local64_set(&event->hw.period_left, event->hw.last_period);
+
+	/*
+	 * See if we need to reserve the PMU.
+	 * If no events are currently in use, then we have to take a
+	 * mutex to ensure that we don't race with another task doing
+	 * reserve_pmc_hardware or release_pmc_hardware.
+	 */
+	err = 0;
+	if (!atomic_inc_not_zero(&num_events)) {
+
+		mutex_lock(&pmc_reserve_mutex);
+		if (atomic_read(&num_events) == 0 &&
+		    reserve_pmc_hardware(perf_event_interrupt))
+			err = -EBUSY;
+		else
+			atomic_inc(&num_events);
+		mutex_unlock(&pmc_reserve_mutex);
+		isync();
+	}
+	event->destroy = hw_perf_event_destroy;
+
+	return err;
+}
+
+static struct pmu acp_pmu = {
+	.pmu_enable	= acp_pmu_enable,
+	.pmu_disable	= acp_pmu_disable,
+	.event_init	= acp_pmu_event_init,
+	.add		= acp_pmu_add,
+	.del		= acp_pmu_del,
+	.start		= acp_pmu_start,
+	.stop		= acp_pmu_stop,
+	.read		= acp_pmu_read,
+};
+
+/*
+ * A counter has overflowed; update its count and record
+ * things if requested.  Note that interrupts are hard-disabled
+ * here so there is no possibility of being interrupted.
+ */
+static void record_and_restart(int core,
+			       struct perf_event *event,
+			       unsigned long val,
+			       struct pt_regs *regs)
+{
+	u64 period = event->hw.sample_period;
+	s64 prev, delta, left;
+	int record = 0;
+
+	pr_debug("--- %s --- cnt %d\n",
+		 __func__, event->hw.idx);
+
+	if (event->hw.state & PERF_HES_STOPPED) {
+		ctr_write(core, event->hw.idx, 0);
+		return;
+	}
+
+	/* we don't have to worry about interrupts here */
+	prev = local64_read(&event->hw.prev_count);
+	delta = (val - prev) & 0xfffffffful;
+	local64_add(delta, &event->count);
+
+	/*
+	 * See if the total period for this event has expired,
+	 * and update for the next period.
+	 */
+	val = 0;
+	left = local64_read(&event->hw.period_left) - delta;
+	if (period) {
+		if (left <= 0) {
+			left += period;
+			if (left <= 0)
+				left = period;
+			record = 1;
+			event->hw.last_period = event->hw.sample_period;
+		}
+		if (left < 0x80000000LL)
+			val = 0x80000000LL - left;
+	}
+
+	ctr_write(core, event->hw.idx, val);
+	local64_set(&event->hw.prev_count, val);
+	local64_set(&event->hw.period_left, left);
+	perf_event_update_userpage(event);
+
+	/*
+	 * Finally record data if requested.
+	 */
+	if (record) {
+		struct perf_sample_data data;
+
+		perf_sample_data_init(&data, 0);
+		data.period = event->hw.last_period;
+
+		if (perf_event_overflow(event, &data, regs))
+			acp_pmu_stop(event, 0);
+	}
+}
+
+static void perf_event_interrupt(struct pt_regs *regs)
+{
+	int i;
+	int core = smp_processor_id();
+	struct cpu_hw_events *cpuhw = &__get_cpu_var(cpu_hw_events);
+	struct perf_event *event;
+	unsigned long val;
+	int found = 0;
+	int nmi;
+	u32 pmgc0;
+
+	nmi = perf_intr_is_nmi(regs);
+	if (nmi)
+		nmi_enter();
+	else
+		irq_enter();
+
+	pr_debug("--- %s ---\n",
+		 __func__);
+
+	for (i = 0; i < ppmu->n_counter; ++i) {
+		event = cpuhw->event[i];
+		val = ctr_read(core, i);
+		if ((int)val < 0) {
+			if (event) {
+				/* event has overflowed */
+				found = 1;
+				record_and_restart(core, event, val, regs);
+			} else {
+				/*
+				 * Disabled counter is negative,
+				 * reset it just in case.
+				 */
+				ctr_write(core, i, 0);
+			}
+		}
+	}
+
+	/* PMM will keep counters frozen until we return from the interrupt. */
+	mtmsr(mfmsr() | MSR_PMM);
+
+	mtdcrx(PMUDCRAI(core), PMRN_PMUGC0);
+	pmgc0 = mfdcrx(PMUDCRDI(core));
+	pmgc0 &= ~PMUGC0_LFAC; /* un-freeze all counters */
+	pmgc0 |= PMUGC0_FCEC; /* enable freeze all ctrs on of */
+	mtdcrx(PMUDCRDI(core), pmgc0);
+
+	isync();
+
+	if (nmi)
+		nmi_exit();
+	else
+		irq_exit();
+}
+
+int register_acp_pmu(struct acp_pmu *pmu)
+{
+	int core;
+
+	if (ppmu)
+		return -EBUSY;		/* something's already registered */
+
+	/* ACP PMU is enabled and may fire after reset
+	 * disable until someone is listening
+	 */
+	for_each_possible_cpu(core) {
+		u32 pmgc0;
+
+		mtdcrx(PMUDCRAI(core), PMRN_PMUGC0);
+		pmgc0 = mfdcrx(PMUDCRDI(core));
+		pmgc0 |= (PMUGC0_LFAC|PMUGC0_PMCC);
+		pmgc0 &= ~PMUGC0_FCEC;
+		mtdcrx(PMUDCRDI(core), pmgc0);
+	}
+	ppmu = pmu;
+	pr_info("%s performance monitor hardware support registered\n",
+		pmu->name);
+
+	perf_pmu_register(&acp_pmu, "cpu", PERF_TYPE_RAW);
+
+	return 0;
+}
diff --git a/arch/powerpc/kernel/pmc.c b/arch/powerpc/kernel/pmc.c
index 58eaa3d..9a67914 100644
--- a/arch/powerpc/kernel/pmc.c
+++ b/arch/powerpc/kernel/pmc.c
@@ -20,6 +20,10 @@
 #include <asm/processor.h>
 #include <asm/cputable.h>
 #include <asm/pmc.h>
+#if defined(CONFIG_ACP_PMU_PERFMON)
+#include <asm/smp.h>
+#include <asm/reg_acp_pmu.h>
+#endif
 
 #ifndef MMCR0_PMAO
 #define MMCR0_PMAO	0
@@ -32,6 +36,11 @@ static void dummy_perf(struct pt_regs *regs)
 #elif defined(CONFIG_PPC64) || defined(CONFIG_6xx)
 	if (cur_cpu_spec->pmc_type == PPC_PMC_IBM)
 		mtspr(SPRN_MMCR0, mfspr(SPRN_MMCR0) & ~(MMCR0_PMXE|MMCR0_PMAO));
+#elif defined(CONFIG_ACP_PMU_PERFMON) && defined(CONFIG_SMP)
+	int core = raw_smp_processor_id();
+
+	mtdcrx(PMUDCRAI(core), PMRN_PMUIE0);
+	mtdcrx(PMUDCRDI(core), 0);
 #else
 	mtspr(SPRN_MMCR0, mfspr(SPRN_MMCR0) & ~MMCR0_PMXE);
 #endif
diff --git a/arch/powerpc/kernel/ppc476_pmu.c b/arch/powerpc/kernel/ppc476_pmu.c
new file mode 100644
index 0000000..e85f69f
--- /dev/null
+++ b/arch/powerpc/kernel/ppc476_pmu.c
@@ -0,0 +1,125 @@
+/*
+ * Performance counter support for LSI Axxia3400
+ *
+ * Based on earlier code:
+ *
+ * e500mc-pmu.c
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+#include <linux/string.h>
+#include <linux/perf_event.h>
+#include <asm/reg.h>
+#include <asm/cputable.h>
+#include <asm/perf_event_acp.h>
+#include <asm/reg_acp_pmu.h>
+
+
+/*
+ * Map of generic hardware event types to hardware events
+ * Zero if unsupported
+ */
+static int ppc476_generic_events[] = {
+	[PERF_COUNT_HW_CPU_CYCLES] = CPU_CYCLE_COUNT,
+	[PERF_COUNT_HW_INSTRUCTIONS] = CPU_COMMITTED_INST,
+	[PERF_COUNT_HW_CACHE_REFERENCES] = CPU_DCACHE_HIT, /* hm...? */
+};
+
+#define C(x)	PERF_COUNT_HW_CACHE_##x
+
+/**
+ * The PMU do have a lot of cache events but they don't
+ * fit directly into this model.
+ * Need to combine several PM events to get the numbers
+ * perf are looking for:
+ * - maybe use some restricted mode settings to fix this
+ * TBD...
+ * For now - use raw events!
+ */
+static int ppc476_cache_events[C(MAX)][C(OP_MAX)][C(RESULT_MAX)] = {
+	[C(L1D)] = {		/* RESULT_ACCESS	RESULT_MISS */
+		[C(OP_READ)] = {	0,		0	},
+		[C(OP_WRITE)] = {	0,		0	},
+		[C(OP_PREFETCH)] = {	0,		0	},
+	},
+	[C(L1I)] = {		/* RESULT_ACCESS	RESULT_MISS */
+		[C(OP_READ)] = {	0,		0	},
+		[C(OP_WRITE)] = {	0,		0	},
+		[C(OP_PREFETCH)] = {	0,		0	},
+	},
+
+	[C(LL)] = {		/* RESULT_ACCESS	RESULT_MISS */
+		[C(OP_READ)] = {        0,		0	},
+		[C(OP_WRITE)] = {       0,	        0	},
+		[C(OP_PREFETCH)] = {	0,		0	},
+	},
+	/*
+	 * There are data/instruction MMU misses, but that's a miss on
+	 * the chip's internal level-one TLB which is probably not
+	 * what the user wants.  Instead, unified level-two TLB misses
+	 * are reported here.
+	 */
+	[C(DTLB)] = {		/* RESULT_ACCESS	RESULT_MISS */
+		[C(OP_READ)] = {	0,	        0	},
+		[C(OP_WRITE)] = {       0,		0	},
+		[C(OP_PREFETCH)] = {    0,		0	},
+	},
+	[C(ITLB)] = {		/* RESULT_ACCESS	RESULT_MISS */
+		[C(OP_READ)] = {	0,	        0	},
+		[C(OP_WRITE)] = {	0,		0	},
+		[C(OP_PREFETCH)] = {	0,		0	},
+	},
+	[C(BPU)] = {		/* RESULT_ACCESS	RESULT_MISS */
+		[C(OP_READ)] = {	-1, -1	},
+		[C(OP_WRITE)] = {	-1, -1	},
+		[C(OP_PREFETCH)] = {	-1,		-1	},
+	},
+	[C(NODE)] = {		/* RESULT_ACCESS	RESULT_MISS */
+		[C(OP_READ)] = {	-1,		-1	},
+		[C(OP_WRITE)] = {	-1,		-1	},
+		[C(OP_PREFETCH)] = {	-1,		-1	},
+	},
+};
+
+static int num_events = 32;
+
+
+static u64 ppc476_xlate_event(u64 event_id)
+{
+	u32 event_low = (u32)event_id;
+	u64 ret;
+
+	if (event_low >= num_events)
+		return 0;
+
+	ret = ACP_EVENT_VALID;
+
+	return ret;
+}
+
+static struct acp_pmu ppc476_pmu = {
+	.name			= "ppc476 family",
+	.n_counter		= 8,
+	.xlate_event		= ppc476_xlate_event,
+	.n_generic		= ARRAY_SIZE(ppc476_generic_events),
+	.generic_events		= ppc476_generic_events,
+	.cache_events		= &ppc476_cache_events,
+};
+
+static int init_ppc476_pmu(void)
+{
+	if (!cur_cpu_spec->oprofile_cpu_type)
+		return -ENODEV;
+
+	if (!strcmp(cur_cpu_spec->oprofile_cpu_type, "ppc/476"))
+		num_events = 32;
+	else
+		return -ENODEV;
+
+	return register_acp_pmu(&ppc476_pmu);
+}
+
+early_initcall(init_ppc476_pmu);
diff --git a/arch/powerpc/oprofile/Makefile b/arch/powerpc/oprofile/Makefile
index 73456c4..576c32d 100644
--- a/arch/powerpc/oprofile/Makefile
+++ b/arch/powerpc/oprofile/Makefile
@@ -16,4 +16,5 @@ oprofile-$(CONFIG_OPROFILE_CELL) += op_model_cell.o \
 		cell/spu_task_sync.o
 oprofile-$(CONFIG_PPC_BOOK3S_64) += op_model_rs64.o op_model_power4.o op_model_pa6t.o
 oprofile-$(CONFIG_FSL_EMB_PERFMON) += op_model_fsl_emb.o
+oprofile-$(CONFIG_ACP_PMU_PERFMON) += op_model_acp_pmu.o
 oprofile-$(CONFIG_6xx) += op_model_7450.o
diff --git a/arch/powerpc/oprofile/op_model_acp_pmu.c b/arch/powerpc/oprofile/op_model_acp_pmu.c
new file mode 100644
index 0000000..fbe91f3
--- /dev/null
+++ b/arch/powerpc/oprofile/op_model_acp_pmu.c
@@ -0,0 +1,282 @@
+/*
+ * Freescale Embedded oprofile support, based on ppc64 oprofile support
+ * Copyright (C) 2004 Anton Blanchard <anton@au.ibm.com>, IBM
+ *
+ * Copyright (c) 2004, 2010 Freescale Semiconductor, Inc
+ *
+ * Author: Andy Fleming
+ * Maintainer: Kumar Gala <galak@kernel.crashing.org>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+
+#include <linux/oprofile.h>
+#include <linux/init.h>
+#include <linux/smp.h>
+#include <asm/ptrace.h>
+#include <asm/processor.h>
+#include <asm/cputable.h>
+#include <asm/reg_acp_pmu_fn.h>
+#include <asm/page.h>
+#include <asm/pmc.h>
+#include <asm/oprofile_impl.h>
+
+static unsigned long reset_value[OP_MAX_COUNTER];
+
+static int num_counters;
+static int oprofile_running;
+
+
+
+static void init_pmc_stop(int core, int ctr)
+{
+	u32 pmlc = (PMLCA_FC | PMLCA_FCS | PMLCA_FCU |
+		    PMLCA_FCM1 | PMLCA_FCM0);
+
+	set_pmlc(core, ctr, pmlc);
+}
+
+static void set_pmc_event(int core, int ctr, int event)
+{
+	u32 pmlc;
+
+	pmlc = get_pmlc(core, ctr);
+
+	pmlc = (pmlc & ~PMLCA_EVENT_MASK) |
+		((event << PMLCA_EVENT_SHIFT) &
+		 PMLCA_EVENT_MASK);
+
+	set_pmlc(core, ctr, pmlc);
+}
+
+static void set_pmc_user_kernel(int core, int ctr, int user, int kernel)
+{
+	u32 pmlc;
+
+	pmlc = get_pmlc(core, ctr);
+
+	if (user)
+		pmlc &= ~PMLCA_FCU;
+	else
+		pmlc |= PMLCA_FCU;
+
+	if (kernel)
+		pmlc &= ~PMLCA_FCS;
+	else
+		pmlc |= PMLCA_FCS;
+
+	set_pmlc(core, ctr, pmlc);
+}
+
+static void set_pmc_marked(int core, int ctr, int mark0, int mark1)
+{
+	u32 pmlc = get_pmlc(core, ctr);
+
+	if (mark0)
+		pmlc &= ~PMLCA_FCM0;
+	else
+		pmlc |= PMLCA_FCM0;
+
+	if (mark1)
+		pmlc &= ~PMLCA_FCM1;
+	else
+		pmlc |= PMLCA_FCM1;
+
+	set_pmlc(core, ctr, pmlc);
+}
+
+static void pmc_start_ctr(int core, int ctr, int enable)
+{
+	u32 pmlc = get_pmlc(core, ctr);
+
+	pmlc &= ~PMLCA_FC;
+
+	if (enable)
+		pmlc |= PMLCA_CE;
+	else
+		pmlc &= ~PMLCA_CE;
+
+	set_pmlc(core, ctr, pmlc);
+}
+
+static void pmc_start_ctrs(int core, int enable, u32 ie_mask)
+{
+	u32 pmgc0;
+
+	/* Enable interrupt on overflow condition for enabled counters */
+	mtdcrx(PMUDCRAI(core), PMRN_PMUIE0);
+	if (enable)
+		mtdcrx(PMUDCRDI(core), ie_mask);
+	else
+		mtdcrx(PMUDCRDI(core), 0);
+
+	/* Start counters */
+	mtdcrx(PMUDCRAI(core), PMRN_PMUGC0);
+	pmgc0 = mfdcrx(PMUDCRDI(core));
+	pmgc0 &= ~PMUGC0_LFAC; /* un-freeze all counters */
+	pmgc0 |= PMUGC0_FCEC; /* enable freeze all ctrs on of */
+
+	mtdcrx(PMUDCRDI(core), pmgc0);
+}
+
+static void pmc_stop_ctrs(int core)
+{
+	u32 pmgc0;
+
+	mtdcrx(PMUDCRAI(core), PMRN_PMUGC0);
+	pmgc0 = mfdcrx(PMUDCRDI(core));
+
+
+	pmgc0 |= (PMUGC0_LFAC|PMUGC0_PMCC);
+
+	pmgc0 &= ~PMUGC0_FCEC;
+	mtdcrx(PMUDCRDI(core), pmgc0);
+	mtdcrx(PMUDCRAI(core), PMRN_PMUIE0);
+	mtdcrx(PMUDCRDI(core), 0);
+
+}
+
+static int acp_pmu_cpu_setup(struct op_counter_config *ctr)
+{
+	int i;
+	int core = smp_processor_id();
+
+	/* freeze all counters */
+	pmc_stop_ctrs(core);
+
+	for (i = 0; i < num_counters; i++) {
+		init_pmc_stop(core, i);
+
+		set_pmc_event(core, i, ctr[i].event);
+
+		set_pmc_user_kernel(core, i, ctr[i].user, ctr[i].kernel);
+	}
+
+	return 0;
+}
+
+static int acp_pmu_reg_setup(struct op_counter_config *ctr,
+			     struct op_system_config *sys,
+			     int num_ctrs)
+{
+	int i;
+
+	num_counters = num_ctrs;
+
+	/* Our counters count up, and "count" refers to
+	 * how much before the next interrupt, and we interrupt
+	 * on overflow.  So we calculate the starting value
+	 * which will give us "count" until overflow.
+	 * Then we set the events on the enabled counters */
+	for (i = 0; i < num_counters; ++i)
+		reset_value[i] = 0x80000000UL - ctr[i].count;
+
+	return 0;
+}
+
+static int acp_pmu_start(struct op_counter_config *ctr)
+{
+	int i;
+	u32 ie_mask = 0;
+	int core = smp_processor_id();
+
+	/* Freeze counters during update */
+
+	mtmsr(mfmsr() | MSR_PMM);
+
+	for (i = 0; i < num_counters; ++i) {
+		if (ctr[i].enabled) {
+			ie_mask |= PMUIE_IE(i);
+
+			ctr_write(core, i, reset_value[i]);
+			/* Set each enabled counter to only
+			 * count when the Mark bit is *not* set */
+			set_pmc_marked(core, i, 1, 0);
+			pmc_start_ctr(core, i, 1);
+		} else {
+			ctr_write(core, i, 0);
+
+			/* Set the ctr to be stopped */
+			pmc_start_ctr(core, i, 0);
+		}
+	}
+
+	/* Clear the freeze bit, and enable the interrupt.
+	 * The counters won't actually start until the rfi clears
+	 * the PMM bit */
+	pmc_start_ctrs(core, 1, ie_mask);
+
+	oprofile_running = 1;
+#ifdef DEBUG
+	mtdcrx(PMUDCRAI(core), PMRN_PMUGC0);
+#endif
+	pr_debug("start on cpu %d, pmgc0 %x\n", core,
+		 mfdcrx(PMUDCRDI(core)));
+
+	return 0;
+}
+
+static void acp_pmu_stop(void)
+{
+	int core = smp_processor_id();
+
+	/* freeze counters */
+	pmc_stop_ctrs(core);
+
+	oprofile_running = 0;
+
+	mtdcrx(PMUDCRAI(core), PMRN_PMUGC0);
+	pr_debug("stop on cpu %d, pmgc0 %x\n", smp_processor_id(),
+		 mfdcrx(PMUDCRDI(core)));
+
+	mb();
+}
+
+
+static void acp_pmu_handle_interrupt(struct pt_regs *regs,
+				    struct op_counter_config *ctr)
+{
+	int core = smp_processor_id();
+	u32 ie_mask = 0;
+	unsigned long pc;
+	int is_kernel;
+	int val;
+	int i;
+
+	pc = regs->nip;
+	is_kernel = is_kernel_addr(pc);
+
+	for (i = 0; i < num_counters; ++i) {
+		val = ctr_read(core, i);
+		if (val < 0) {
+			if (oprofile_running && ctr[i].enabled) {
+				ie_mask |= PMUIE_IE(i);
+				oprofile_add_ext_sample(pc, regs, i, is_kernel);
+				ctr_write(core, i, reset_value[i]);
+			} else {
+				ctr_write(core, i, 0);
+			}
+		}
+	}
+
+	/* The freeze bit was set by the interrupt. */
+	/* Clear the freeze bit, and reenable the interrupt.  The
+	 * counters won't actually start until the rfi clears the PMM
+	 * bit.  The PMM bit should not be set until after the interrupt
+	 * is cleared to avoid it getting lost in some hypervisor
+	 * environments.
+	 */
+	mtmsr(mfmsr() | MSR_PMM);
+	pmc_start_ctrs(core, 1, ie_mask);
+}
+
+struct op_powerpc_model op_model_acp_pmu = {
+	.reg_setup		= acp_pmu_reg_setup,
+	.cpu_setup		= acp_pmu_cpu_setup,
+	.start			= acp_pmu_start,
+	.stop			= acp_pmu_stop,
+	.handle_interrupt	= acp_pmu_handle_interrupt,
+};
diff --git a/arch/powerpc/platforms/Kconfig.cputype b/arch/powerpc/platforms/Kconfig.cputype
index 425db18..f9f5809 100644
--- a/arch/powerpc/platforms/Kconfig.cputype
+++ b/arch/powerpc/platforms/Kconfig.cputype
@@ -166,6 +166,22 @@ config 4xx
 	depends on 40x || 44x
 	default y
 
+config ACP_PMU_PERFMON
+       bool "LSI Embedded Perfmon"
+       depends on 44x && ACP
+       help
+         This is the Performance Monitor support found on the Axxia3400 ppc476 chip
+
+config ACP_PMU_PERF_EVENT
+       bool
+       depends on ACP_PMU_PERFMON && PERF_EVENTS && !PPC_PERF_CTRS
+       default y
+
+config ACP_PMU_PERF_EVENT_PPC476
+       bool
+       depends on ACP_PMU_PERFMON && PERF_EVENTS && !PPC_PERF_CTRS
+       default y
+
 config BOOKE
 	bool
 	depends on E200 || E500 || 44x || PPC_BOOK3E
-- 
1.7.5.4

