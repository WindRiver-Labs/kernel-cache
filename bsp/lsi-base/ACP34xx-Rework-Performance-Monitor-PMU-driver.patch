From 0942fd6cfb6fa6fb30dfeb07faf95eb367a58276 Mon Sep 17 00:00:00 2001
From: Jiang Lu <lu.jiang@windriver.com>
Date: Mon, 3 Jun 2013 10:07:57 +0800
Subject: [PATCH] ACP34xx: Rework Performance Monitor (PMU) driver

This patch rework PMU driver for the LSI ACP34xx platform.

Move ppc476-pmu.c & perf_event_acp.c into arch/powerpc/perf, where
these files exist for other PowerPC machines in linux 3.x.

Signed-off-by: David Mercado <david.mercado@windriver.com>
---
 arch/powerpc/include/asm/perf_event_acp.h |    1 -
 arch/powerpc/kernel/Makefile              |    3 -
 arch/powerpc/kernel/cputable.c            |   15 +-
 arch/powerpc/kernel/perf_event_acp.c      |  652 -----------------------------
 arch/powerpc/kernel/pmc.c                 |    2 +-
 arch/powerpc/kernel/ppc476_pmu.c          |  125 ------
 arch/powerpc/oprofile/common.c            |    5 +
 arch/powerpc/oprofile/op_model_acp_pmu.c  |    2 +-
 arch/powerpc/perf/Makefile                |    3 +
 arch/powerpc/perf/core-lsi-acp.c          |  611 +++++++++++++++++++++++++++
 arch/powerpc/perf/ppc476-pmu.c            |  194 +++++++++
 11 files changed, 824 insertions(+), 789 deletions(-)
 delete mode 100644 arch/powerpc/kernel/perf_event_acp.c
 delete mode 100644 arch/powerpc/kernel/ppc476_pmu.c
 create mode 100644 arch/powerpc/perf/core-lsi-acp.c
 create mode 100644 arch/powerpc/perf/ppc476-pmu.c

diff --git a/arch/powerpc/include/asm/perf_event_acp.h b/arch/powerpc/include/asm/perf_event_acp.h
index 0a57dd3..356179b 100644
--- a/arch/powerpc/include/asm/perf_event_acp.h
+++ b/arch/powerpc/include/asm/perf_event_acp.h
@@ -21,7 +21,6 @@
 
 /* event flags */
 #define ACP_EVENT_VALID      1
-#define ACP_EVENT_RESTRICTED 2
 
 
 struct acp_pmu {
diff --git a/arch/powerpc/kernel/Makefile b/arch/powerpc/kernel/Makefile
index 0e509dd..825c37f 100644
--- a/arch/powerpc/kernel/Makefile
+++ b/arch/powerpc/kernel/Makefile
@@ -116,9 +116,6 @@ obj-$(CONFIG_DYNAMIC_FTRACE)	+= ftrace.o
 obj-$(CONFIG_FUNCTION_GRAPH_TRACER)	+= ftrace.o
 obj-$(CONFIG_FTRACE_SYSCALLS)	+= ftrace.o
 
-obj-$(CONFIG_ACP_PMU_PERF_EVENT) += perf_event_acp.o
-obj-$(CONFIG_ACP_PMU_PERF_EVENT_PPC476) += ppc476_pmu.o
-
 obj-$(CONFIG_8XX_MINIMAL_FPEMU) += softemu8xx.o
 
 ifneq ($(CONFIG_PPC_INDIRECT_IO),y)
diff --git a/arch/powerpc/kernel/cputable.c b/arch/powerpc/kernel/cputable.c
index 38d2e6c..6c0265a 100644
--- a/arch/powerpc/kernel/cputable.c
+++ b/arch/powerpc/kernel/cputable.c
@@ -1859,6 +1859,9 @@ static struct cpu_spec __initdata cpu_specs[] = {
 		.icache_bsize		= 32,
 		.dcache_bsize		= 128,
 		.machine_check		= machine_check_47x,
+		.num_pmcs		= 8,
+		.oprofile_cpu_type	= "ppc/476",
+		.oprofile_type		= PPC_OPROFILE_ACP_PMU,
 		.platform		= "ppc470",
 	},
 	{ /* 476fpe */
@@ -1887,9 +1890,6 @@ static struct cpu_spec __initdata cpu_specs[] = {
 		.icache_bsize		= 32,
 		.dcache_bsize		= 128,
 		.machine_check		= machine_check_47x,
-		.num_pmcs		= 8,
-		.oprofile_cpu_type      = "ppc/476",
-		.oprofile_type          = PPC_OPROFILE_ACP_PMU,
 		.platform		= "ppc470",
 	},
 	{ /* 476 DD3 core */
@@ -1904,10 +1904,10 @@ static struct cpu_spec __initdata cpu_specs[] = {
 		.icache_bsize		= 32,
 		.dcache_bsize		= 128,
 		.machine_check		= machine_check_47x,
-		.platform		= "ppc470",
 		.num_pmcs		= 8,
-		.oprofile_cpu_type      = "ppc/476",
-		.oprofile_type          = PPC_OPROFILE_ACP_PMU,
+		.oprofile_cpu_type	= "ppc/476",
+		.oprofile_type		= PPC_OPROFILE_ACP_PMU,
+		.platform		= "ppc470",
 	},
 	{ /* 476 ACP25xx */
 		.pvr_mask		= 0x7ff520c1,
@@ -1921,6 +1921,9 @@ static struct cpu_spec __initdata cpu_specs[] = {
 		.icache_bsize		= 32,
 		.dcache_bsize		= 128,
 		.machine_check		= machine_check_47x,
+		.num_pmcs		= 8,
+		.oprofile_cpu_type      = "ppc/476",
+		.oprofile_type          = PPC_OPROFILE_ACP_PMU,
 		.platform		= "ppc470",
 	},
 	{ /* 476 others */
diff --git a/arch/powerpc/kernel/perf_event_acp.c b/arch/powerpc/kernel/perf_event_acp.c
deleted file mode 100644
index eb76435..0000000
--- a/arch/powerpc/kernel/perf_event_acp.c
+++ /dev/null
@@ -1,652 +0,0 @@
-/*
- * Performance event support - LSI ACP Embedded Performance Monitor
- *
- * Based on earlier code:
- *
- * perf_event_fsl_emb.c
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public License
- * as published by the Free Software Foundation; either version
- * 2 of the License, or (at your option) any later version.
- */
-#include <linux/kernel.h>
-#include <linux/sched.h>
-#include <linux/perf_event.h>
-#include <linux/percpu.h>
-#include <linux/hardirq.h>
-#include <asm/reg_acp_pmu_fn.h>
-#include <asm/pmc.h>
-#include <asm/machdep.h>
-#include <asm/firmware.h>
-#include <asm/ptrace.h>
-
-
-struct cpu_hw_events {
-	int n_events;
-	int disabled;
-	u8  pmcs_enabled;
-	struct perf_event *event[MAX_HWEVENTS];
-};
-static DEFINE_PER_CPU(struct cpu_hw_events, cpu_hw_events);
-
-static struct acp_pmu *ppmu;
-
-/* Number of perf_events counting hardware events */
-static atomic_t num_events;
-/* Used to avoid races in calling reserve/release_pmc_hardware */
-static DEFINE_MUTEX(pmc_reserve_mutex);
-
-/*
- * If interrupts were soft-disabled when a PMU interrupt occurs, treat
- * it as an NMI.
- */
-static inline int perf_intr_is_nmi(struct pt_regs *regs)
-{
-#ifdef __powerpc64__
-	return !regs->softe;
-#else
-	return 0;
-#endif
-}
-
-static void perf_event_interrupt(struct pt_regs *regs);
-
-
-static void acp_pmu_read(struct perf_event *event)
-{
-	int core = smp_processor_id();
-	s64 val, delta, prev;
-
-	if (event->hw.state & PERF_HES_STOPPED)
-		return;
-
-	/*
-	 * Performance monitor interrupts come even when interrupts
-	 * are soft-disabled, as long as interrupts are hard-enabled.
-	 * Therefore we treat them like NMIs.
-	 */
-	do {
-		prev = local64_read(&event->hw.prev_count);
-		barrier();
-		val = ctr_read(core, event->hw.idx);
-	} while (local64_cmpxchg(&event->hw.prev_count, prev, val) != prev);
-
-	/* The counters are only 32 bits wide */
-	delta = (val - prev) & 0xfffffffful;
-	local64_add(delta, &event->count);
-	local64_sub(delta, &event->hw.period_left);
-}
-
-/*
- * Disable all events to prevent PMU interrupts and to allow
- * events to be added or removed.
- */
-static void acp_pmu_disable(struct pmu *pmu)
-{
-	int core = smp_processor_id();
-	struct cpu_hw_events *cpuhw;
-	unsigned long flags;
-
-	local_irq_save(flags);
-	cpuhw = &__get_cpu_var(cpu_hw_events);
-
-	pr_debug("--- %s ---\n", __func__);
-
-	if (!cpuhw->disabled) {
-		cpuhw->disabled = 1;
-
-		/*
-		 * Check if we ever enabled the PMU on this cpu.
-		 */
-		if (!cpuhw->pmcs_enabled) {
-			ppc_enable_pmcs();
-			cpuhw->pmcs_enabled = 1;
-		}
-
-		if (atomic_read(&num_events)) {
-			u32 pmgc0;
-			/*
-			 * Set the 'freeze all counters' bit, and disable
-			 * interrupts.  The barrier is to make sure the
-			 * mtpmr has been executed and the PMU has frozen
-			 * the events before we return.
-			 */
-			mtdcrx(PMUDCRAI(core), PMRN_PMUGC0);
-			pmgc0 = mfdcrx(PMUDCRDI(core));
-			pmgc0 |= PMUGC0_LFAC;
-			pmgc0 &= ~PMUGC0_FCEC;
-			mtdcrx(PMUDCRDI(core), pmgc0);
-			mtdcrx(PMUDCRAI(core), PMRN_PMUIE0);
-			mtdcrx(PMUDCRDI(core), 0);
-
-			isync();
-		}
-	}
-	local_irq_restore(flags);
-}
-
-/*
- * Re-enable all events if disable == 0.
- * If we were previously disabled and events were added, then
- * put the new config on the PMU.
- */
-static void acp_pmu_enable(struct pmu *pmu)
-{
-	int core = smp_processor_id();
-	struct cpu_hw_events *cpuhw;
-	unsigned long flags;
-
-	pr_debug("--- %s ---\n", __func__);
-	local_irq_save(flags);
-	cpuhw = &__get_cpu_var(cpu_hw_events);
-	if (!cpuhw->disabled)
-		goto out;
-
-	cpuhw->disabled = 0;
-	ppc_set_pmu_inuse(cpuhw->n_events != 0);
-
-	if (cpuhw->n_events > 0) {
-		u32 pmgc0;
-		u32 pmuie0 = 0;
-		int i;
-		int num_counters = ppmu->n_counter;
-
-		for (i = 0; i < num_counters; i++) {
-			if (cpuhw->event[i])
-				pmuie0 |= PMUIE_IE(i);
-		}
-		mtdcrx(PMUDCRAI(core), PMRN_PMUIE0);
-		mtdcrx(PMUDCRDI(core), pmuie0);
-
-		mtdcrx(PMUDCRAI(core), PMRN_PMUGC0);
-		pmgc0 = mfdcrx(PMUDCRDI(core));
-		pmgc0 &= ~PMUGC0_LFAC; /* un-freeze all counters */
-		pmgc0 |= PMUGC0_FCEC; /* enable freeze all ctrs on of */
-		mtdcrx(PMUDCRDI(core), pmgc0);
-
-		isync();
-	}
-
- out:
-	local_irq_restore(flags);
-}
-
-static int collect_events(struct perf_event *group, int max_count,
-			  struct perf_event *ctrs[])
-{
-	int n = 0;
-	struct perf_event *event;
-
-	if (!is_software_event(group)) {
-		if (n >= max_count)
-			return -1;
-		ctrs[n] = group;
-		n++;
-	}
-	list_for_each_entry(event, &group->sibling_list, group_entry) {
-		if (!is_software_event(event) &&
-		    event->state != PERF_EVENT_STATE_OFF) {
-			if (n >= max_count)
-				return -1;
-			ctrs[n] = event;
-			n++;
-		}
-	}
-	return n;
-}
-
-/* context locked on entry */
-static int acp_pmu_add(struct perf_event *event, int flags)
-{
-	struct cpu_hw_events *cpuhw;
-	int core = smp_processor_id();
-	int ret = -EAGAIN;
-	int num_counters = ppmu->n_counter;
-	u64 val;
-	int i;
-	u32 pmuie0;
-
-	perf_pmu_disable(event->pmu);
-	cpuhw = &get_cpu_var(cpu_hw_events);
-	/*
-	 * Allocate counters from top-down, so that restricted-capable
-	 * counters are kept free as long as possible.
-	 */
-/*	for (i = num_counters - 1; i >= 0; i--) { */
-	for (i = 0; i < num_counters; i++) {
-		if (cpuhw->event[i])
-			continue;
-
-		break;
-	}
-
-	if (i < 0)
-		goto out;
-
-	event->hw.idx = i;
-	cpuhw->event[i] = event;
-	++cpuhw->n_events;
-
-	pr_debug("--- %s --- cnt id %d config_base %lx\n", __func__,
-			 event->hw.idx, event->hw.config_base);
-
-	val = 0;
-	if (event->hw.sample_period) {
-		s64 left = local64_read(&event->hw.period_left);
-		if (left < 0x80000000L)
-			val = 0x80000000L - left;
-	}
-	local64_set(&event->hw.prev_count, val);
-
-	if (!(flags & PERF_EF_START)) {
-		event->hw.state = PERF_HES_STOPPED | PERF_HES_UPTODATE;
-		val = 0;
-	}
-
-	ctr_write(core, i, val);
-	perf_event_update_userpage(event);
-
-	set_pmlc(core, i, event->hw.config_base);
-
-	/* Enable counter interrupt on overflow condition */
-	mtdcrx(PMUDCRAI(core), PMRN_PMUIE0);
-	pmuie0 = mfdcrx(PMUDCRDI(core));
-	pmuie0 |= PMUIE_IE(i);
-	mtdcrx(PMUDCRDI(core), pmuie0);
-
-	ret = 0;
- out:
-	put_cpu_var(cpu_hw_events);
-	perf_pmu_enable(event->pmu);
-	return ret;
-}
-
-/* context locked on entry */
-static void acp_pmu_del(struct perf_event *event, int flags)
-{
-	int core = smp_processor_id();
-	struct cpu_hw_events *cpuhw;
-	int i = event->hw.idx;
-	pr_debug("--- %s --- cnt %d\n", __func__, i);
-	perf_pmu_disable(event->pmu);
-	if (i < 0)
-		goto out;
-
-	acp_pmu_read(event);
-
-	cpuhw = &get_cpu_var(cpu_hw_events);
-
-	WARN_ON(event != cpuhw->event[event->hw.idx]);
-
-	set_pmlc(core, i, 0);
-	ctr_write(core, i, 0);
-
-	cpuhw->event[i] = NULL;
-	event->hw.idx = -1;
-
-	/*
-	 * TODO: if at least one restricted event exists, and we
-	 * just freed up a non-restricted-capable counter, and
-	 * there is a restricted-capable counter occupied by
-	 * a non-restricted event, migrate that event to the
-	 * vacated counter.
-	 */
-
-	cpuhw->n_events--;
-
- out:
-	perf_pmu_enable(event->pmu);
-	put_cpu_var(cpu_hw_events);
-}
-
-static void acp_pmu_start(struct perf_event *event, int ef_flags)
-{
-	int core = smp_processor_id();
-	unsigned long flags;
-	s64 left;
-	pr_debug("--- %s --- cnt %d\n", __func__, event->hw.idx);
-	if (event->hw.idx < 0 || !event->hw.sample_period)
-		return;
-
-	if (!(event->hw.state & PERF_HES_STOPPED))
-		return;
-
-	if (ef_flags & PERF_EF_RELOAD)
-		WARN_ON_ONCE(!(event->hw.state & PERF_HES_UPTODATE));
-
-	local_irq_save(flags);
-	perf_pmu_disable(event->pmu);
-
-	event->hw.state = 0;
-	left = local64_read(&event->hw.period_left);
-	ctr_write(core, event->hw.idx, left);
-
-	perf_event_update_userpage(event);
-	perf_pmu_enable(event->pmu);
-	local_irq_restore(flags);
-}
-
-static void acp_pmu_stop(struct perf_event *event, int ef_flags)
-{
-	unsigned long flags;
-	int core = smp_processor_id();
-	pr_debug("--- %s --- cnt %d\n", __func__, event->hw.idx);
-	if (event->hw.idx < 0 || !event->hw.sample_period)
-		return;
-
-	if (event->hw.state & PERF_HES_STOPPED)
-		return;
-
-	local_irq_save(flags);
-	perf_pmu_disable(event->pmu);
-
-	acp_pmu_read(event);
-	event->hw.state |= PERF_HES_STOPPED | PERF_HES_UPTODATE;
-	ctr_write(core, event->hw.idx, 0);
-
-	perf_event_update_userpage(event);
-	perf_pmu_enable(event->pmu);
-	local_irq_restore(flags);
-}
-
-/*
- * Release the PMU if this is the last perf_event.
- */
-static void hw_perf_event_destroy(struct perf_event *event)
-{
-	pr_debug("--- %s ---\n", __func__);
-
-	if (!atomic_add_unless(&num_events, -1, 1)) {
-		mutex_lock(&pmc_reserve_mutex);
-		if (atomic_dec_return(&num_events) == 0)
-			release_pmc_hardware();
-		mutex_unlock(&pmc_reserve_mutex);
-	}
-}
-
-/*
- * Translate a generic cache event_id config to a raw event_id code.
- */
-static int hw_perf_cache_event(u64 config, u64 *eventp)
-{
-	unsigned long type, op, result;
-	int ev;
-
-	if (!ppmu->cache_events)
-		return -EINVAL;
-
-	/* unpack config */
-	type = config & 0xff;
-	op = (config >> 8) & 0xff;
-	result = (config >> 16) & 0xff;
-	pr_debug("--- %s --- hw cache event type %lu op %lu result %lu\n",
-		 __func__, type, op, result);
-
-	if (type >= PERF_COUNT_HW_CACHE_MAX ||
-	    op >= PERF_COUNT_HW_CACHE_OP_MAX ||
-	    result >= PERF_COUNT_HW_CACHE_RESULT_MAX)
-		return -EINVAL;
-
-	ev = (*ppmu->cache_events)[type][op][result];
-	if (ev == 0)
-		return -EOPNOTSUPP;
-	if (ev == -1)
-		return -EINVAL;
-	*eventp = ev;
-	return 0;
-}
-
-static int acp_pmu_event_init(struct perf_event *event)
-{
-	u64 ev;
-	struct perf_event *events[MAX_HWEVENTS];
-	int n;
-	int err;
-
-	switch (event->attr.type) {
-	case PERF_TYPE_HARDWARE:
-		ev = event->attr.config;
-		if (ev >= ppmu->n_generic || ppmu->generic_events[ev] == 0)
-			return -EOPNOTSUPP;
-		pr_debug("--- %s --- hardware event %llu\n",
-			 __func__, ev);
-		ev = ppmu->generic_events[ev];
-		pr_debug("--- %s --- acp map event %llu\n",
-			 __func__, ev);
-		break;
-
-	case PERF_TYPE_HW_CACHE:
-		err = hw_perf_cache_event(event->attr.config, &ev);
-		if (err)
-			return err;
-		pr_debug("--- %s --- acp map event %llu\n",
-			 __func__, ev);
-		break;
-
-	case PERF_TYPE_RAW:
-		ev = event->attr.config;
-		pr_debug("--- %s --- raw event %llu\n",
-			 __func__, ev);
-		break;
-
-	default:
-		return -ENOENT;
-	}
-
-	event->hw.config = ppmu->xlate_event(ev);
-	if (!(event->hw.config & ACP_EVENT_VALID))
-		return -EINVAL;
-
-	/*
-	 * If this is in a group, check if it can go on with all the
-	 * other hardware events in the group.  We assume the event
-	 * hasn't been linked into its leader's sibling list at this point.
-	 */
-	n = 0;
-	if (event->group_leader != event) {
-		n = collect_events(event->group_leader,
-					ppmu->n_counter - 1, events);
-		if (n < 0)
-			return -EINVAL;
-		pr_debug("--- %s --- Add new event to group\n",
-			 __func__);
-	}
-	event->hw.idx = -1;
-
-	event->hw.config_base = PMLCA_CE | PMLCA_FCM1 |
-				(u32)((ev) & PMLCA_EVENT_MASK);
-
-	if (event->attr.exclude_user)
-		event->hw.config_base |= PMLCA_FCU;
-	if (event->attr.exclude_kernel)
-		event->hw.config_base |= PMLCA_FCS;
-	if (event->attr.exclude_idle)
-		return -ENOTSUPP;
-
-	pr_debug("--- %s --- hwconfig base %lx\n",
-		 __func__, event->hw.config_base);
-
-	event->hw.last_period = event->hw.sample_period;
-	local64_set(&event->hw.period_left, event->hw.last_period);
-
-	/*
-	 * See if we need to reserve the PMU.
-	 * If no events are currently in use, then we have to take a
-	 * mutex to ensure that we don't race with another task doing
-	 * reserve_pmc_hardware or release_pmc_hardware.
-	 */
-	err = 0;
-	if (!atomic_inc_not_zero(&num_events)) {
-
-		mutex_lock(&pmc_reserve_mutex);
-		if (atomic_read(&num_events) == 0 &&
-		    reserve_pmc_hardware(perf_event_interrupt))
-			err = -EBUSY;
-		else
-			atomic_inc(&num_events);
-		mutex_unlock(&pmc_reserve_mutex);
-		isync();
-	}
-	event->destroy = hw_perf_event_destroy;
-
-	return err;
-}
-
-static struct pmu acp_pmu = {
-	.pmu_enable	= acp_pmu_enable,
-	.pmu_disable	= acp_pmu_disable,
-	.event_init	= acp_pmu_event_init,
-	.add		= acp_pmu_add,
-	.del		= acp_pmu_del,
-	.start		= acp_pmu_start,
-	.stop		= acp_pmu_stop,
-	.read		= acp_pmu_read,
-};
-
-/*
- * A counter has overflowed; update its count and record
- * things if requested.  Note that interrupts are hard-disabled
- * here so there is no possibility of being interrupted.
- */
-static void record_and_restart(int core,
-			       struct perf_event *event,
-			       unsigned long val,
-			       struct pt_regs *regs)
-{
-	u64 period = event->hw.sample_period;
-	s64 prev, delta, left;
-	int record = 0;
-
-	pr_debug("--- %s --- cnt %d\n",
-		 __func__, event->hw.idx);
-
-	if (event->hw.state & PERF_HES_STOPPED) {
-		ctr_write(core, event->hw.idx, 0);
-		return;
-	}
-
-	/* we don't have to worry about interrupts here */
-	prev = local64_read(&event->hw.prev_count);
-	delta = (val - prev) & 0xfffffffful;
-	local64_add(delta, &event->count);
-
-	/*
-	 * See if the total period for this event has expired,
-	 * and update for the next period.
-	 */
-	val = 0;
-	left = local64_read(&event->hw.period_left) - delta;
-	if (period) {
-		if (left <= 0) {
-			left += period;
-			if (left <= 0)
-				left = period;
-			record = 1;
-			event->hw.last_period = event->hw.sample_period;
-		}
-		if (left < 0x80000000LL)
-			val = 0x80000000LL - left;
-	}
-
-	ctr_write(core, event->hw.idx, val);
-	local64_set(&event->hw.prev_count, val);
-	local64_set(&event->hw.period_left, left);
-	perf_event_update_userpage(event);
-
-	/*
-	 * Finally record data if requested.
-	 */
-	if (record) {
-		struct perf_sample_data data;
-
-		perf_sample_data_init(&data, 0);
-		data.period = event->hw.last_period;
-
-		if (perf_event_overflow(event, &data, regs))
-			acp_pmu_stop(event, 0);
-	}
-}
-
-static void perf_event_interrupt(struct pt_regs *regs)
-{
-	int i;
-	int core = smp_processor_id();
-	struct cpu_hw_events *cpuhw = &__get_cpu_var(cpu_hw_events);
-	struct perf_event *event;
-	unsigned long val;
-	int found = 0;
-	int nmi;
-	u32 pmgc0;
-
-	nmi = perf_intr_is_nmi(regs);
-	if (nmi)
-		nmi_enter();
-	else
-		irq_enter();
-
-	pr_debug("--- %s ---\n",
-		 __func__);
-
-	for (i = 0; i < ppmu->n_counter; ++i) {
-		event = cpuhw->event[i];
-		val = ctr_read(core, i);
-		if ((int)val < 0) {
-			if (event) {
-				/* event has overflowed */
-				found = 1;
-				record_and_restart(core, event, val, regs);
-			} else {
-				/*
-				 * Disabled counter is negative,
-				 * reset it just in case.
-				 */
-				ctr_write(core, i, 0);
-			}
-		}
-	}
-
-	/* PMM will keep counters frozen until we return from the interrupt. */
-	mtmsr(mfmsr() | MSR_PMM);
-
-	mtdcrx(PMUDCRAI(core), PMRN_PMUGC0);
-	pmgc0 = mfdcrx(PMUDCRDI(core));
-	pmgc0 &= ~PMUGC0_LFAC; /* un-freeze all counters */
-	pmgc0 |= PMUGC0_FCEC; /* enable freeze all ctrs on of */
-	mtdcrx(PMUDCRDI(core), pmgc0);
-
-	isync();
-
-	if (nmi)
-		nmi_exit();
-	else
-		irq_exit();
-}
-
-int register_acp_pmu(struct acp_pmu *pmu)
-{
-	int core;
-
-	if (ppmu)
-		return -EBUSY;		/* something's already registered */
-
-	/* ACP PMU is enabled and may fire after reset
-	 * disable until someone is listening
-	 */
-	for_each_possible_cpu(core) {
-		u32 pmgc0;
-
-		mtdcrx(PMUDCRAI(core), PMRN_PMUGC0);
-		pmgc0 = mfdcrx(PMUDCRDI(core));
-		pmgc0 |= (PMUGC0_LFAC|PMUGC0_PMCC);
-		pmgc0 &= ~PMUGC0_FCEC;
-		mtdcrx(PMUDCRDI(core), pmgc0);
-	}
-	ppmu = pmu;
-	pr_info("%s performance monitor hardware support registered\n",
-		pmu->name);
-
-	perf_pmu_register(&acp_pmu, "cpu", PERF_TYPE_RAW);
-
-	return 0;
-}
diff --git a/arch/powerpc/kernel/pmc.c b/arch/powerpc/kernel/pmc.c
index 9a67914..9bc82fb 100644
--- a/arch/powerpc/kernel/pmc.c
+++ b/arch/powerpc/kernel/pmc.c
@@ -21,7 +21,7 @@
 #include <asm/cputable.h>
 #include <asm/pmc.h>
 #if defined(CONFIG_ACP_PMU_PERFMON)
-#include <asm/smp.h>
+#include <linux/smp.h>
 #include <asm/reg_acp_pmu.h>
 #endif
 
diff --git a/arch/powerpc/kernel/ppc476_pmu.c b/arch/powerpc/kernel/ppc476_pmu.c
deleted file mode 100644
index e85f69f..0000000
--- a/arch/powerpc/kernel/ppc476_pmu.c
+++ /dev/null
@@ -1,125 +0,0 @@
-/*
- * Performance counter support for LSI Axxia3400
- *
- * Based on earlier code:
- *
- * e500mc-pmu.c
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public License
- * as published by the Free Software Foundation; either version
- * 2 of the License, or (at your option) any later version.
- */
-#include <linux/string.h>
-#include <linux/perf_event.h>
-#include <asm/reg.h>
-#include <asm/cputable.h>
-#include <asm/perf_event_acp.h>
-#include <asm/reg_acp_pmu.h>
-
-
-/*
- * Map of generic hardware event types to hardware events
- * Zero if unsupported
- */
-static int ppc476_generic_events[] = {
-	[PERF_COUNT_HW_CPU_CYCLES] = CPU_CYCLE_COUNT,
-	[PERF_COUNT_HW_INSTRUCTIONS] = CPU_COMMITTED_INST,
-	[PERF_COUNT_HW_CACHE_REFERENCES] = CPU_DCACHE_HIT, /* hm...? */
-};
-
-#define C(x)	PERF_COUNT_HW_CACHE_##x
-
-/**
- * The PMU do have a lot of cache events but they don't
- * fit directly into this model.
- * Need to combine several PM events to get the numbers
- * perf are looking for:
- * - maybe use some restricted mode settings to fix this
- * TBD...
- * For now - use raw events!
- */
-static int ppc476_cache_events[C(MAX)][C(OP_MAX)][C(RESULT_MAX)] = {
-	[C(L1D)] = {		/* RESULT_ACCESS	RESULT_MISS */
-		[C(OP_READ)] = {	0,		0	},
-		[C(OP_WRITE)] = {	0,		0	},
-		[C(OP_PREFETCH)] = {	0,		0	},
-	},
-	[C(L1I)] = {		/* RESULT_ACCESS	RESULT_MISS */
-		[C(OP_READ)] = {	0,		0	},
-		[C(OP_WRITE)] = {	0,		0	},
-		[C(OP_PREFETCH)] = {	0,		0	},
-	},
-
-	[C(LL)] = {		/* RESULT_ACCESS	RESULT_MISS */
-		[C(OP_READ)] = {        0,		0	},
-		[C(OP_WRITE)] = {       0,	        0	},
-		[C(OP_PREFETCH)] = {	0,		0	},
-	},
-	/*
-	 * There are data/instruction MMU misses, but that's a miss on
-	 * the chip's internal level-one TLB which is probably not
-	 * what the user wants.  Instead, unified level-two TLB misses
-	 * are reported here.
-	 */
-	[C(DTLB)] = {		/* RESULT_ACCESS	RESULT_MISS */
-		[C(OP_READ)] = {	0,	        0	},
-		[C(OP_WRITE)] = {       0,		0	},
-		[C(OP_PREFETCH)] = {    0,		0	},
-	},
-	[C(ITLB)] = {		/* RESULT_ACCESS	RESULT_MISS */
-		[C(OP_READ)] = {	0,	        0	},
-		[C(OP_WRITE)] = {	0,		0	},
-		[C(OP_PREFETCH)] = {	0,		0	},
-	},
-	[C(BPU)] = {		/* RESULT_ACCESS	RESULT_MISS */
-		[C(OP_READ)] = {	-1, -1	},
-		[C(OP_WRITE)] = {	-1, -1	},
-		[C(OP_PREFETCH)] = {	-1,		-1	},
-	},
-	[C(NODE)] = {		/* RESULT_ACCESS	RESULT_MISS */
-		[C(OP_READ)] = {	-1,		-1	},
-		[C(OP_WRITE)] = {	-1,		-1	},
-		[C(OP_PREFETCH)] = {	-1,		-1	},
-	},
-};
-
-static int num_events = 32;
-
-
-static u64 ppc476_xlate_event(u64 event_id)
-{
-	u32 event_low = (u32)event_id;
-	u64 ret;
-
-	if (event_low >= num_events)
-		return 0;
-
-	ret = ACP_EVENT_VALID;
-
-	return ret;
-}
-
-static struct acp_pmu ppc476_pmu = {
-	.name			= "ppc476 family",
-	.n_counter		= 8,
-	.xlate_event		= ppc476_xlate_event,
-	.n_generic		= ARRAY_SIZE(ppc476_generic_events),
-	.generic_events		= ppc476_generic_events,
-	.cache_events		= &ppc476_cache_events,
-};
-
-static int init_ppc476_pmu(void)
-{
-	if (!cur_cpu_spec->oprofile_cpu_type)
-		return -ENODEV;
-
-	if (!strcmp(cur_cpu_spec->oprofile_cpu_type, "ppc/476"))
-		num_events = 32;
-	else
-		return -ENODEV;
-
-	return register_acp_pmu(&ppc476_pmu);
-}
-
-early_initcall(init_ppc476_pmu);
diff --git a/arch/powerpc/oprofile/common.c b/arch/powerpc/oprofile/common.c
index 0912563..0541f00 100644
--- a/arch/powerpc/oprofile/common.c
+++ b/arch/powerpc/oprofile/common.c
@@ -220,6 +220,11 @@ int __init oprofile_arch_init(struct oprofile_operations *ops)
 			model = &op_model_pa6t;
 			break;
 #endif
+#ifdef CONFIG_ACP_PMU_PERFMON
+	case PPC_OPROFILE_ACP_PMU:
+		model = &op_model_acp_pmu;
+		break;
+#endif
 #ifdef CONFIG_6xx
 		case PPC_OPROFILE_G4:
 			model = &op_model_7450;
diff --git a/arch/powerpc/oprofile/op_model_acp_pmu.c b/arch/powerpc/oprofile/op_model_acp_pmu.c
index fbe91f3..0d4ff9e 100644
--- a/arch/powerpc/oprofile/op_model_acp_pmu.c
+++ b/arch/powerpc/oprofile/op_model_acp_pmu.c
@@ -16,7 +16,7 @@
 #include <linux/oprofile.h>
 #include <linux/init.h>
 #include <linux/smp.h>
-#include <asm/ptrace.h>
+#include <linux/ptrace.h>
 #include <asm/processor.h>
 #include <asm/cputable.h>
 #include <asm/reg_acp_pmu_fn.h>
diff --git a/arch/powerpc/perf/Makefile b/arch/powerpc/perf/Makefile
index af3fac2..e4906ad 100644
--- a/arch/powerpc/perf/Makefile
+++ b/arch/powerpc/perf/Makefile
@@ -7,6 +7,9 @@ obj64-$(CONFIG_PPC_PERF_CTRS)	+= power4-pmu.o ppc970-pmu.o power5-pmu.o \
 				   power5+-pmu.o power6-pmu.o power7-pmu.o
 obj32-$(CONFIG_PPC_PERF_CTRS)	+= mpc7450-pmu.o
 
+obj-$(CONFIG_ACP_PMU_PERF_EVENT) += core-lsi-acp.o
+obj-$(CONFIG_ACP_PMU_PERF_EVENT_PPC476) += ppc476-pmu.o
+
 obj-$(CONFIG_FSL_EMB_PERF_EVENT) += core-fsl-emb.o
 obj-$(CONFIG_FSL_EMB_PERF_EVENT_E500) += e500-pmu.o
 
diff --git a/arch/powerpc/perf/core-lsi-acp.c b/arch/powerpc/perf/core-lsi-acp.c
new file mode 100644
index 0000000..9e8986d
--- /dev/null
+++ b/arch/powerpc/perf/core-lsi-acp.c
@@ -0,0 +1,611 @@
+/*
+ * Performance event support - LSI ACP Embedded Performance Monitor
+ *
+ * Based on earlier code:
+ *
+ * perf_event_fsl_emb.c
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/perf_event.h>
+#include <linux/percpu.h>
+#include <linux/hardirq.h>
+#include <asm/reg_acp_pmu_fn.h>
+#include <asm/pmc.h>
+#include <asm/machdep.h>
+#include <linux/firmware.h>
+#include <linux/ptrace.h>
+
+
+struct cpu_hw_events {
+	int n_events;
+	int disabled;
+	u8  pmcs_enabled;
+	struct perf_event *event[MAX_HWEVENTS];
+};
+static DEFINE_PER_CPU(struct cpu_hw_events, cpu_hw_events);
+
+static struct acp_pmu *ppmu;
+
+/* Number of perf_events counting hardware events */
+static atomic_t num_events;
+/* Used to avoid races in calling reserve/release_pmc_hardware */
+static DEFINE_MUTEX(pmc_reserve_mutex);
+
+/*
+ * If interrupts were soft-disabled when a PMU interrupt occurs, treat
+ * it as an NMI.
+ */
+static inline int perf_intr_is_nmi(struct pt_regs *regs)
+{
+#ifdef __powerpc64__
+	return !regs->softe;
+#else
+	return 0;
+#endif
+}
+
+static void perf_event_interrupt(struct pt_regs *regs);
+
+
+static void acp_pmu_read(struct perf_event *event)
+{
+	int core = smp_processor_id();
+	s64 val, delta, prev;
+
+	if (event->hw.state & PERF_HES_STOPPED)
+		return;
+
+	/*
+	 * Performance monitor interrupts come even when interrupts
+	 * are soft-disabled, as long as interrupts are hard-enabled.
+	 * Therefore we treat them like NMIs.
+	 */
+	do {
+		prev = local64_read(&event->hw.prev_count);
+		barrier();
+		val = ctr_read(core, event->hw.idx);
+	} while (local64_cmpxchg(&event->hw.prev_count, prev, val) != prev);
+
+	/* The counters are only 32 bits wide */
+	delta = (val - prev) & 0xfffffffful;
+	local64_add(delta, &event->count);
+	local64_sub(delta, &event->hw.period_left);
+}
+
+/*
+ * Disable all events to prevent PMU interrupts and to allow
+ * events to be added or removed.
+ */
+static void acp_pmu_disable(struct pmu *pmu)
+{
+	int core = smp_processor_id();
+	struct cpu_hw_events *cpuhw;
+	unsigned long flags;
+
+	local_irq_save(flags);
+	cpuhw = &__get_cpu_var(cpu_hw_events);
+
+	if (!cpuhw->disabled) {
+		cpuhw->disabled = 1;
+
+		/*
+		 * Check if we ever enabled the PMU on this cpu.
+		 */
+		if (!cpuhw->pmcs_enabled) {
+			ppc_enable_pmcs();
+			cpuhw->pmcs_enabled = 1;
+		}
+
+		if (atomic_read(&num_events)) {
+			u32 pmgc0;
+			/*
+			 * Set the 'freeze all counters' bit, and disable
+			 * interrupts.  The barrier is to make sure the
+			 * mtpmr has been executed and the PMU has frozen
+			 * the events before we return.
+			 */
+			mtdcrx(PMUDCRAI(core), PMRN_PMUGC0);
+			pmgc0 = mfdcrx(PMUDCRDI(core));
+			pmgc0 |= PMUGC0_LFAC;
+			pmgc0 &= ~PMUGC0_FCEC;
+			mtdcrx(PMUDCRDI(core), pmgc0);
+			mtdcrx(PMUDCRAI(core), PMRN_PMUIE0);
+			mtdcrx(PMUDCRDI(core), 0);
+
+			isync();
+		}
+	}
+	local_irq_restore(flags);
+}
+
+/*
+ * Re-enable all events if disable == 0.
+ * If we were previously disabled and events were added, then
+ * put the new config on the PMU.
+ */
+static void acp_pmu_enable(struct pmu *pmu)
+{
+	int core = smp_processor_id();
+	struct cpu_hw_events *cpuhw;
+	unsigned long flags;
+
+	local_irq_save(flags);
+	cpuhw = &__get_cpu_var(cpu_hw_events);
+	if (!cpuhw->disabled)
+		goto out;
+
+	cpuhw->disabled = 0;
+	ppc_set_pmu_inuse(cpuhw->n_events != 0);
+
+	if (cpuhw->n_events > 0) {
+		u32 pmgc0;
+		u32 pmuie0 = 0;
+		int i;
+		int num_counters = ppmu->n_counter;
+		for (i = 0; i < num_counters; i++) {
+			if (cpuhw->event[i])
+				pmuie0 |= PMUIE_IE(i);
+		}
+		mtdcrx(PMUDCRAI(core), PMRN_PMUIE0);
+		mtdcrx(PMUDCRDI(core), pmuie0);
+
+		mtdcrx(PMUDCRAI(core), PMRN_PMUGC0);
+		pmgc0 = mfdcrx(PMUDCRDI(core));
+		pmgc0 &= ~PMUGC0_LFAC; /* un-freeze all counters */
+		pmgc0 |= PMUGC0_FCEC; /* enable freeze all ctrs on of */
+		mtdcrx(PMUDCRDI(core), pmgc0);
+
+		isync();
+	}
+
+out:
+	local_irq_restore(flags);
+}
+
+static int collect_events(struct perf_event *group, int max_count,
+			  struct perf_event *ctrs[])
+{
+	int n = 0;
+	struct perf_event *event;
+
+	if (!is_software_event(group)) {
+		if (n >= max_count)
+			return -1;
+		ctrs[n] = group;
+		n++;
+	}
+	list_for_each_entry(event, &group->sibling_list, group_entry) {
+		if (!is_software_event(event) &&
+		    event->state != PERF_EVENT_STATE_OFF) {
+			if (n >= max_count)
+				return -1;
+			ctrs[n] = event;
+			n++;
+		}
+	}
+	return n;
+}
+
+/* context locked on entry */
+static int acp_pmu_add(struct perf_event *event, int flags)
+{
+	struct cpu_hw_events *cpuhw;
+	int core = smp_processor_id();
+	int ret = -EAGAIN;
+	int num_counters = ppmu->n_counter;
+	u64 val;
+	int i;
+	u32 pmuie0;
+
+	perf_pmu_disable(event->pmu);
+	cpuhw = &get_cpu_var(cpu_hw_events);
+
+	/* Allocate counters */
+	for (i = 0; i < num_counters; i++) {
+		if (cpuhw->event[i])
+			continue;
+
+		break;
+	}
+
+	if (i < 0)
+		goto out;
+
+	event->hw.idx = i;
+	cpuhw->event[i] = event;
+	++cpuhw->n_events;
+
+	val = 0;
+	if (event->hw.sample_period) {
+		s64 left = local64_read(&event->hw.period_left);
+		if (left < 0x80000000L)
+			val = 0x80000000L - left;
+	}
+	local64_set(&event->hw.prev_count, val);
+
+	if (!(flags & PERF_EF_START)) {
+		event->hw.state = PERF_HES_STOPPED | PERF_HES_UPTODATE;
+		val = 0;
+	}
+
+	ctr_write(core, i, val);
+	perf_event_update_userpage(event);
+
+	set_pmlc(core, i, event->hw.config_base);
+
+	/* Enable counter interrupt on overflow condition */
+	mtdcrx(PMUDCRAI(core), PMRN_PMUIE0);
+	pmuie0 = mfdcrx(PMUDCRDI(core));
+	pmuie0 |= PMUIE_IE(i);
+	mtdcrx(PMUDCRDI(core), pmuie0);
+
+	ret = 0;
+out:
+	put_cpu_var(cpu_hw_events);
+	perf_pmu_enable(event->pmu);
+	return ret;
+}
+
+/* context locked on entry */
+static void acp_pmu_del(struct perf_event *event, int flags)
+{
+	int core = smp_processor_id();
+	struct cpu_hw_events *cpuhw;
+	int i = event->hw.idx;
+
+	perf_pmu_disable(event->pmu);
+	if (i < 0)
+		goto out;
+
+	acp_pmu_read(event);
+
+	cpuhw = &get_cpu_var(cpu_hw_events);
+
+	WARN_ON(event != cpuhw->event[event->hw.idx]);
+
+	set_pmlc(core, i, 0);
+	ctr_write(core, i, 0);
+
+	cpuhw->event[i] = NULL;
+	event->hw.idx = -1;
+
+out:
+	perf_pmu_enable(event->pmu);
+	put_cpu_var(cpu_hw_events);
+}
+
+static void acp_pmu_start(struct perf_event *event, int ef_flags)
+{
+	int core = smp_processor_id();
+	unsigned long flags;
+	s64 left;
+
+	if (event->hw.idx < 0 || !event->hw.sample_period)
+		return;
+
+	if (!(event->hw.state & PERF_HES_STOPPED))
+		return;
+
+	if (ef_flags & PERF_EF_RELOAD)
+		WARN_ON_ONCE(!(event->hw.state & PERF_HES_UPTODATE));
+
+	local_irq_save(flags);
+	perf_pmu_disable(event->pmu);
+
+	event->hw.state = 0;
+	left = local64_read(&event->hw.period_left);
+	ctr_write(core, event->hw.idx, left);
+
+	perf_event_update_userpage(event);
+	perf_pmu_enable(event->pmu);
+	local_irq_restore(flags);
+}
+
+static void acp_pmu_stop(struct perf_event *event, int ef_flags)
+{
+	unsigned long flags;
+	int core = smp_processor_id();
+
+	if (event->hw.idx < 0 || !event->hw.sample_period)
+		return;
+
+	if (event->hw.state & PERF_HES_STOPPED)
+		return;
+
+	local_irq_save(flags);
+	perf_pmu_disable(event->pmu);
+
+	acp_pmu_read(event);
+	event->hw.state |= PERF_HES_STOPPED | PERF_HES_UPTODATE;
+	ctr_write(core, event->hw.idx, 0);
+
+	perf_event_update_userpage(event);
+	perf_pmu_enable(event->pmu);
+	local_irq_restore(flags);
+}
+
+/*
+ * Release the PMU if this is the last perf_event.
+ */
+static void hw_perf_event_destroy(struct perf_event *event)
+{
+	if (!atomic_add_unless(&num_events, -1, 1)) {
+		mutex_lock(&pmc_reserve_mutex);
+		if (atomic_dec_return(&num_events) == 0)
+			release_pmc_hardware();
+		mutex_unlock(&pmc_reserve_mutex);
+	}
+}
+
+/*
+ * Translate a generic cache event_id config to a raw event_id code.
+ */
+static int hw_perf_cache_event(u64 config, u64 *eventp)
+{
+	unsigned long type, op, result;
+	int ev;
+
+	if (!ppmu->cache_events)
+		return -EINVAL;
+
+	/* unpack config */
+	type = config & 0xff;
+	op = (config >> 8) & 0xff;
+	result = (config >> 16) & 0xff;
+
+	if (type >= PERF_COUNT_HW_CACHE_MAX ||
+	    op >= PERF_COUNT_HW_CACHE_OP_MAX ||
+	    result >= PERF_COUNT_HW_CACHE_RESULT_MAX)
+		return -EINVAL;
+
+	ev = (*ppmu->cache_events)[type][op][result];
+	if (ev == 0)
+		return -EOPNOTSUPP;
+	if (ev == -1)
+		return -EINVAL;
+	*eventp = ev;
+	return 0;
+}
+
+static int acp_pmu_event_init(struct perf_event *event)
+{
+	u64 ev;
+	struct perf_event *events[MAX_HWEVENTS];
+	int n;
+	int err;
+
+	switch (event->attr.type) {
+	case PERF_TYPE_HARDWARE:
+		ev = event->attr.config;
+		if (ev >= ppmu->n_generic || ppmu->generic_events[ev] == 0)
+			return -EOPNOTSUPP;
+		ev = ppmu->generic_events[ev];
+		break;
+
+	case PERF_TYPE_HW_CACHE:
+		err = hw_perf_cache_event(event->attr.config, &ev);
+		if (err)
+			return err;
+		break;
+
+	case PERF_TYPE_RAW:
+		ev = event->attr.config;
+		break;
+
+	default:
+		return -ENOENT;
+	}
+
+	event->hw.config = ppmu->xlate_event(ev);
+	if (!(event->hw.config & ACP_EVENT_VALID))
+		return -EINVAL;
+
+	/*
+	 * If this is in a group, check if it can go on with all the
+	 * other hardware events in the group.  We assume the event
+	 * hasn't been linked into its leader's sibling list at this point.
+	 */
+	n = 0;
+	if (event->group_leader != event) {
+		n = collect_events(event->group_leader,
+				   ppmu->n_counter - 1, events);
+		if (n < 0)
+			return -EINVAL;
+	}
+	event->hw.idx = -1;
+
+	event->hw.config_base = PMLCA_CE | PMLCA_FCM1 |
+				(u32)((ev) & PMLCA_EVENT_MASK);
+
+	if (event->attr.exclude_user)
+		event->hw.config_base |= PMLCA_FCU;
+	if (event->attr.exclude_kernel)
+		event->hw.config_base |= PMLCA_FCS;
+	if (event->attr.exclude_idle)
+		return -ENOTSUPP;
+
+	event->hw.last_period = event->hw.sample_period;
+	local64_set(&event->hw.period_left, event->hw.last_period);
+
+	/*
+	 * See if we need to reserve the PMU.
+	 * If no events are currently in use, then we have to take a
+	 * mutex to ensure that we don't race with another task doing
+	 * reserve_pmc_hardware or release_pmc_hardware.
+	 */
+	err = 0;
+	if (!atomic_inc_not_zero(&num_events)) {
+
+		mutex_lock(&pmc_reserve_mutex);
+		if (atomic_read(&num_events) == 0 &&
+		    reserve_pmc_hardware(perf_event_interrupt))
+			err = -EBUSY;
+		else
+			atomic_inc(&num_events);
+		mutex_unlock(&pmc_reserve_mutex);
+		isync();
+	}
+	event->destroy = hw_perf_event_destroy;
+
+	return err;
+}
+
+static struct pmu acp_pmu = {
+	.pmu_enable	= acp_pmu_enable,
+	.pmu_disable	= acp_pmu_disable,
+	.event_init	= acp_pmu_event_init,
+	.add		= acp_pmu_add,
+	.del		= acp_pmu_del,
+	.start		= acp_pmu_start,
+	.stop		= acp_pmu_stop,
+	.read		= acp_pmu_read,
+};
+
+/*
+ * A counter has overflowed; update its count and record
+ * things if requested.  Note that interrupts are hard-disabled
+ * here so there is no possibility of being interrupted.
+ */
+static void record_and_restart(int core, struct perf_event *event,
+			       unsigned long val, struct pt_regs *regs)
+{
+	u64 period = event->hw.sample_period;
+	s64 prev, delta, left;
+	int record = 0;
+
+	if (event->hw.state & PERF_HES_STOPPED) {
+		ctr_write(core, event->hw.idx, 0);
+		return;
+	}
+
+	/* we don't have to worry about interrupts here */
+	prev = local64_read(&event->hw.prev_count);
+	delta = (val - prev) & 0xfffffffful;
+	local64_add(delta, &event->count);
+
+	/*
+	 * See if the total period for this event has expired,
+	 * and update for the next period.
+	 */
+	val = 0;
+	left = local64_read(&event->hw.period_left) - delta;
+	if (period) {
+		if (left <= 0) {
+			left += period;
+			if (left <= 0)
+				left = period;
+			record = 1;
+			event->hw.last_period = event->hw.sample_period;
+		}
+		if (left < 0x80000000LL)
+			val = 0x80000000LL - left;
+	}
+
+	ctr_write(core, event->hw.idx, val);
+	local64_set(&event->hw.prev_count, val);
+	local64_set(&event->hw.period_left, left);
+	perf_event_update_userpage(event);
+
+	/*
+	 * Finally record data if requested.
+	 */
+	if (record) {
+		struct perf_sample_data data;
+
+		perf_sample_data_init(&data, 0);
+		data.period = event->hw.last_period;
+
+		if (perf_event_overflow(event, &data, regs))
+			acp_pmu_stop(event, 0);
+	}
+}
+
+static void perf_event_interrupt(struct pt_regs *regs)
+{
+	int i;
+	int core = smp_processor_id();
+	struct cpu_hw_events *cpuhw = &__get_cpu_var(cpu_hw_events);
+	struct perf_event *event;
+	unsigned long val;
+	int found = 0;
+	int nmi;
+	u32 pmgc0;
+
+	nmi = perf_intr_is_nmi(regs);
+	if (nmi)
+		nmi_enter();
+	else
+		irq_enter();
+
+	for (i = 0; i < ppmu->n_counter; ++i) {
+		event = cpuhw->event[i];
+		val = ctr_read(core, i);
+		if ((int)val < 0) {
+			if (event) {
+				/* event has overflowed */
+				found = 1;
+				record_and_restart(core, event, val, regs);
+			} else {
+				/*
+				 * Disabled counter is negative,
+				 * reset it just in case.
+				 */
+				ctr_write(core, i, 0);
+			}
+		}
+	}
+
+	/* PMM will keep counters frozen until we return from the interrupt. */
+	mtmsr(mfmsr() | MSR_PMM);
+
+	mtdcrx(PMUDCRAI(core), PMRN_PMUGC0);
+	pmgc0 = mfdcrx(PMUDCRDI(core));
+	pmgc0 &= ~PMUGC0_LFAC; /* un-freeze all counters */
+	pmgc0 |= PMUGC0_FCEC; /* enable freeze all ctrs on of */
+	mtdcrx(PMUDCRDI(core), pmgc0);
+
+	isync();
+
+	if (nmi)
+		nmi_exit();
+	else
+		irq_exit();
+}
+
+int register_acp_pmu(struct acp_pmu *pmu)
+{
+	int core;
+
+	if (ppmu)
+		return -EBUSY;		/* something's already registered */
+
+	/*
+	 * ACP PMU is enabled and may fire after reset
+	 * disable until someone is listening
+	 */
+
+	for_each_possible_cpu(core) {
+		u32 pmgc0;
+
+		mtdcrx(PMUDCRAI(core), PMRN_PMUGC0);
+		pmgc0 = mfdcrx(PMUDCRDI(core));
+		pmgc0 |= (PMUGC0_LFAC|PMUGC0_PMCC);
+		pmgc0 &= ~PMUGC0_FCEC;
+		mtdcrx(PMUDCRDI(core), pmgc0);
+	}
+
+	ppmu = pmu;
+	pr_info("%s performance monitor hardware support registered\n",
+		pmu->name);
+
+	perf_pmu_register(&acp_pmu, "cpu", PERF_TYPE_RAW);
+
+	return 0;
+}
diff --git a/arch/powerpc/perf/ppc476-pmu.c b/arch/powerpc/perf/ppc476-pmu.c
new file mode 100644
index 0000000..9bd9060
--- /dev/null
+++ b/arch/powerpc/perf/ppc476-pmu.c
@@ -0,0 +1,194 @@
+/*
+ * Performance counter support for LSI Axxia3400
+ *
+ * Based on earlier code:
+ *
+ * e500mc-pmu.c
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+#include <linux/interrupt.h>
+#include <linux/string.h>
+#include <linux/perf_event.h>
+#include <asm/pmc.h>
+#include <asm/reg.h>
+#include <asm/cputable.h>
+#include <asm/perf_event_acp.h>
+#include <asm/reg_acp_pmu.h>
+
+/* PMU IRQ handler */
+static irqreturn_t acp_pmu_isr(int irq, void *dev_id)
+{
+	__get_cpu_var(irq_stat).pmu_irqs++;
+	perf_irq(get_irq_regs());
+	return IRQ_HANDLED;
+}
+
+/*
+ * Map of generic hardware event types to hardware events
+ * Zero if unsupported
+ */
+static int ppc476_generic_events[] = {
+	[PERF_COUNT_HW_CPU_CYCLES]              = CPU_CYCLE_COUNT,
+	[PERF_COUNT_HW_INSTRUCTIONS]            = CPU_COMMITTED_INST,
+	[PERF_COUNT_HW_CACHE_REFERENCES]        = CPU_DCACHE_HIT,
+	[PERF_COUNT_HW_CACHE_MISSES]            = CPU_DTLB_RELOAD,
+
+};
+
+/*
+ * Table of generalized cache-related events.
+ *
+ * 0 means not supported, -1 means nonsensical, other values
+ * are event codes.
+ */
+
+#define C(x)	PERF_COUNT_HW_CACHE_##x
+
+static int ppc476_cache_events[C(MAX)][C(OP_MAX)][C(RESULT_MAX)] = {
+
+	/*
+	 * The PPC476 PMU does have a few cache events but they don't
+	 * fit directly into this model. Therefore, we need to combine
+	 * several PM events to get the numbers that perf is looking for.
+	 *
+	 */
+
+	[C(L1D)] = {            /*      RESULT_ACCESS   RESULT_MISS */
+		[C(OP_READ)] = {        CPU_DCACHE_HIT, CPU_DTLB_RELOAD  },
+		[C(OP_WRITE)] = {       CPU_DCACHE_HIT, CPU_DTLB_RELOAD  },
+		[C(OP_PREFETCH)] = {    0,              0                },
+	},
+	[C(L1I)] = {            /*      RESULT_ACCESS   RESULT_MISS */
+		[C(OP_READ)] = {        CPU_ICACHE_HIT, CPU_ITLB_RELOAD  },
+		[C(OP_WRITE)] = {       CPU_ICACHE_HIT, CPU_ITLB_RELOAD  },
+		[C(OP_PREFETCH)] = {    0,              0                },
+	},
+	[C(LL)] = {             /*      RESULT_ACCESS   RESULT_MISS */
+		[C(OP_READ)] = {        0,              0       },
+		[C(OP_WRITE)] = {       0,              0       },
+		[C(OP_PREFETCH)] = {    0,              0       },
+	},
+
+	/*
+	 * There are data/instruction MMU misses, but that's a miss on
+	 * the chip's internal level-one TLB which is probably not
+	 * what the user wants.  Instead, unified level-two TLB misses
+	 * are reported here.
+	 */
+
+	[C(DTLB)] = {		/*	RESULT_ACCESS	RESULT_MISS */
+		[C(OP_READ)] = {	0,	        0	},
+		[C(OP_WRITE)] = {       0,		0	},
+		[C(OP_PREFETCH)] = {    0,		0	},
+	},
+	[C(ITLB)] = {		/*	RESULT_ACCESS	RESULT_MISS */
+		[C(OP_READ)] = {	0,	        0	},
+		[C(OP_WRITE)] = {	0,		0	},
+		[C(OP_PREFETCH)] = {	0,		0	},
+	},
+	[C(BPU)] = {		/*	RESULT_ACCESS	RESULT_MISS */
+		[C(OP_READ)] = {	-1,		-1	},
+		[C(OP_WRITE)] = {	-1,		-1	},
+		[C(OP_PREFETCH)] = {	-1,		-1	},
+	},
+	[C(NODE)] = {		/*	RESULT_ACCESS	RESULT_MISS */
+		[C(OP_READ)] = {	-1,		-1	},
+		[C(OP_WRITE)] = {	-1,		-1	},
+		[C(OP_PREFETCH)] = {	-1,		-1	},
+	},
+};
+
+static int num_events = 32;
+
+
+static u64 ppc476_xlate_event(u64 event_id)
+{
+	u32 event_low = (u32)event_id;
+	u64 ret;
+
+	if (event_low >= num_events)
+		return 0;
+
+	ret = ACP_EVENT_VALID;
+
+	return ret;
+}
+
+static struct acp_pmu ppc476_pmu = {
+	.name			= "ppc476 family",
+	.n_counter		= 8,
+	.xlate_event		= ppc476_xlate_event,
+	.n_generic		= ARRAY_SIZE(ppc476_generic_events),
+	.generic_events		= ppc476_generic_events,
+	.cache_events		= &ppc476_cache_events,
+};
+
+static int init_ppc476_pmu(void)
+{
+	unsigned int irq;
+	int intNum, core;
+	static const char * const irqname[] = { "pmu-core0",
+						"pmu-core1",
+						"pmu-core2",
+						"pmu-core3" };
+
+	if (!cur_cpu_spec->oprofile_cpu_type)
+		return -ENODEV;
+
+	if (!strcmp(cur_cpu_spec->oprofile_cpu_type, "ppc/476"))
+		num_events = 32;
+	else
+		return -ENODEV;
+
+	/*
+	 * Install the PMU interrupt handlers:
+	 *
+	 * NOTE: On the LSI ACP platform, the PMU interrupts are
+	 * hard-wired as inputs to the MPIC. The irq numbers are
+	 * fixed as follows:
+	 *
+	 *   Core 0 PMU: IRQ 95
+	 *   Core 1 PMU: IRQ 94
+	 *   Core 2 PMU: IRQ 93
+	 *   Core 3 PMU: IRQ 92
+	 *
+	 * The IRQ assignment should probably be done in the DTB,
+	 * like ARM does, but no other PowerPC platform does this.
+	 * So for now, we hard-code the numbers here.
+	 */
+	for_each_possible_cpu(core) {
+		if (core == 0)
+			intNum = 95;
+		else if (core == 1)
+			intNum = 94;
+		else if (core == 2)
+			intNum = 93;
+		else if (core == 3)
+			intNum = 92;
+		else
+			break;
+
+		irq = irq_create_mapping(NULL, intNum);
+		if (irq == NO_IRQ) {
+			pr_err("PMU irq_create_mapping() failed\n");
+			break;
+		}
+		if (irq_set_affinity(irq, get_cpu_mask(core))) {
+			pr_warning("PMU IRQ affinity failed (irq=%d, cpu=%d)\n",
+				   irq, core);
+			continue;
+		}
+		if (request_irq(irq, acp_pmu_isr,
+				IRQF_DISABLED | IRQF_NOBALANCING,
+				irqname[core], NULL))
+			pr_err("PMU reqeust for IRQ%d failed\n", irq);
+	}
+
+	return register_acp_pmu(&ppc476_pmu);
+}
+
+early_initcall(init_ppc476_pmu);
-- 
1.7.5.4

