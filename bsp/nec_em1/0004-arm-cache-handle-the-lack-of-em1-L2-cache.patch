From f123b633624fa61ed94029ca83a944cc6d060c1e Mon Sep 17 00:00:00 2001
From: Kazuya Tsukamoto <kazuya.tsukamoto@necel.com>
Date: Mon, 9 Nov 2009 10:55:56 +0900
Subject: [PATCH 04/29] arm/cache: handle the lack of em1 L2 cache

Modify L2 cache flushing to only call cache_sync, because
the "EMMA Mobile 1" does not have L2 cache memory.

Signed-off-by: Kazuya Tsukamoto <kazuya.tsukamoto@necel.com>
---
 arch/arm/kernel/setup.c      |    5 +-
 arch/arm/mm/Kconfig          |    4 +-
 arch/arm/mm/cache-l2x0.c     |   22 ++-
 arch/arm/mm/cache-v6.S       |   54 ++++-
 arch/arm/mm/flush.c          |   49 +++-
 arch/arm/mm/proc-v6.S        |   59 +++++-
 include/asm-arm/cacheflush.h |  565 ++++++++++++++++++++++++++++++++++++++++++
 7 files changed, 731 insertions(+), 27 deletions(-)
 create mode 100644 include/asm-arm/cacheflush.h

diff --git a/arch/arm/kernel/setup.c b/arch/arm/kernel/setup.c
index a69e034..3506c50 100644
--- a/arch/arm/kernel/setup.c
+++ b/arch/arm/kernel/setup.c
@@ -98,6 +98,7 @@ struct cpu_cache_fns cpu_cache;
 #endif
 #ifdef CONFIG_OUTER_CACHE
 struct outer_cache_fns outer_cache;
+EXPORT_SYMBOL(outer_cache);
 #endif
 
 struct stack {
@@ -315,7 +316,7 @@ static inline void dump_cache(const char *prefix, int cpu, unsigned int cache)
 static void dump_v7_cache(const char *type, int cpu, unsigned int level)
 {
 	unsigned int cachesize;
-                    
+
 	write_extended_cpuid(2,0,0,0,level);  /* Set the cache size selection register */
 	write_extended_cpuid(0,7,5,4,0);      /* Prefetch flush to wait for above */
 	cachesize = read_extended_cpuid(1,0,0,0);
@@ -1001,7 +1002,7 @@ static void c_show_v7_cache(struct seq_file *m, const char *type, unsigned int l
 {
 	unsigned int cachesize;
 	unsigned int level = (levelselect >> 1) + 1;
-                    
+
 	write_extended_cpuid(2,0,0,0,levelselect);  /* Set the cache size selection register */
 	write_extended_cpuid(0,7,5,4,0);      /* Prefetch flush to wait for above */
 	cachesize = read_extended_cpuid(1,0,0,0);
diff --git a/arch/arm/mm/Kconfig b/arch/arm/mm/Kconfig
index 546d7e7..6211f89 100644
--- a/arch/arm/mm/Kconfig
+++ b/arch/arm/mm/Kconfig
@@ -400,7 +400,7 @@ config CPU_FEROCEON_OLD_ID
 # ARMv6
 config CPU_V6
 	bool "Support ARM V6 processor"
-	depends on ARCH_INTEGRATOR || MACH_REALVIEW_EB || ARCH_OMAP2 || ARCH_MX3 || ARCH_MSM7X00A || MACH_REALVIEW_PB11MP || MACH_REALVIEW_PB1176
+	depends on ARCH_INTEGRATOR || MACH_REALVIEW_EB || ARCH_OMAP2 || ARCH_MX3 || ARCH_MSM7X00A || MACH_REALVIEW_PB11MP || MACH_REALVIEW_PB1176 || ARCH_MP200
 	default y if ARCH_MX3
 	default y if ARCH_MSM7X00A
 	select CPU_32v6
@@ -737,7 +737,7 @@ config CACHE_FEROCEON_L2
 
 config CACHE_L2X0
 	bool "Enable the L2x0 outer cache controller"
-	depends on REALVIEW_EB_ARM11MP || MACH_REALVIEW_PB11MP || MACH_REALVIEW_PB1176
+	depends on REALVIEW_EB_ARM11MP || MACH_REALVIEW_PB11MP || MACH_REALVIEW_PB1176 || ARCH_MP200
 	default y
 	select OUTER_CACHE
 	help
diff --git a/arch/arm/mm/cache-l2x0.c b/arch/arm/mm/cache-l2x0.c
index 76b800a..f1cb206 100644
--- a/arch/arm/mm/cache-l2x0.c
+++ b/arch/arm/mm/cache-l2x0.c
@@ -17,10 +17,9 @@
  * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
  */
 #include <linux/init.h>
-#include <linux/spinlock.h>
+#include <linux/io.h>
 
 #include <asm/cacheflush.h>
-#include <asm/io.h>
 #include <asm/hardware/cache-l2x0.h>
 
 #define CACHE_LINE_SIZE		32
@@ -53,8 +52,16 @@ static inline void l2x0_inv_all(void)
 	cache_sync();
 }
 
+static void l2x0_flush_all(void)
+{
+	/* invalidate all ways */
+	sync_writel(0xff, L2X0_CLEAN_INV_WAY, 0xff);
+	cache_sync();
+}
+
 static void l2x0_inv_range(unsigned long start, unsigned long end)
 {
+#ifndef CONFIG_ARCH_MP200
 	unsigned long addr;
 
 	if (start & (CACHE_LINE_SIZE - 1)) {
@@ -70,26 +77,31 @@ static void l2x0_inv_range(unsigned long start, unsigned long end)
 
 	for (addr = start; addr < end; addr += CACHE_LINE_SIZE)
 		sync_writel(addr, L2X0_INV_LINE_PA, 1);
+#endif
 	cache_sync();
 }
 
 static void l2x0_clean_range(unsigned long start, unsigned long end)
 {
+#ifndef CONFIG_ARCH_MP200
 	unsigned long addr;
 
 	start &= ~(CACHE_LINE_SIZE - 1);
 	for (addr = start; addr < end; addr += CACHE_LINE_SIZE)
 		sync_writel(addr, L2X0_CLEAN_LINE_PA, 1);
+#endif
 	cache_sync();
 }
 
 static void l2x0_flush_range(unsigned long start, unsigned long end)
 {
+#ifndef CONFIG_ARCH_MP200
 	unsigned long addr;
 
 	start &= ~(CACHE_LINE_SIZE - 1);
 	for (addr = start; addr < end; addr += CACHE_LINE_SIZE)
 		sync_writel(addr, L2X0_CLEAN_INV_LINE_PA, 1);
+#endif
 	cache_sync();
 }
 
@@ -102,6 +114,11 @@ void __init l2x0_init(void __iomem *base, __u32 aux_val, __u32 aux_mask)
 	/* disable L2X0 */
 	writel(0, l2x0_base + L2X0_CTRL);
 
+	while (readl(l2x0_base + L2X0_CTRL) & 0x1) {
+		/* loop */
+		;
+	}
+
 	aux = readl(l2x0_base + L2X0_AUX_CTRL);
 	aux &= aux_mask;
 	aux |= aux_val;
@@ -115,6 +132,7 @@ void __init l2x0_init(void __iomem *base, __u32 aux_val, __u32 aux_mask)
 	outer_cache.inv_range = l2x0_inv_range;
 	outer_cache.clean_range = l2x0_clean_range;
 	outer_cache.flush_range = l2x0_flush_range;
+	outer_cache.flush_all = l2x0_flush_all;
 
 	printk(KERN_INFO "L2X0 cache controller enabled\n");
 }
diff --git a/arch/arm/mm/cache-v6.S b/arch/arm/mm/cache-v6.S
index 8f5c13f..24c276b 100644
--- a/arch/arm/mm/cache-v6.S
+++ b/arch/arm/mm/cache-v6.S
@@ -20,7 +20,40 @@
 #define D_CACHE_LINE_SIZE	32
 #define BTB_FLUSH_SIZE		8
 
-#ifdef CONFIG_ARM_ERRATA_411920
+#ifndef CONFIG_SMP
+/*
+ * Invalidate the entire I cache (this code is a workaround for the ARM1136
+ * Errata 411920 - Invalidate Instruction Cache operation can fail. This
+ * Errata is present in 1136, 1156 and 1176. It does not affect the MPCore
+ *
+ * Registers:
+ *   r0 - set to 0
+ *   r1 - corrupted
+ */
+ENTRY(v6_icache_inval_all)
+	mov	r0, #0
+	mrs	r1, cpsr
+	cpsid	ifa				@ disable interrupts
+	mcr	p15, 0, r0, c7, c5, 0		@ invalidate entire I-cache
+	mcr	p15, 0, r0, c7, c5, 0		@ invalidate entire I-cache
+	mcr	p15, 0, r0, c7, c5, 0		@ invalidate entire I-cache
+	mcr	p15, 0, r0, c7, c5, 0		@ invalidate entire I-cache
+	msr	cpsr_cx, r1			@ restore interrupts
+	nop
+	nop
+	nop
+	nop
+	nop
+	nop
+	nop
+	nop
+	nop
+	nop
+	nop
+	mov	pc, lr
+#endif
+
+#ifdef CONFIG_ARM_ERRATA_411920	
 /*
  * Invalidate the entire I cache (this code is a workaround for the ARM1136
  * erratum 411920 - Invalidate Instruction Cache operation can fail. This
@@ -57,10 +90,12 @@ ENTRY(v6_flush_kern_cache_all)
 #ifdef HARVARD_CACHE
 	mcr	p15, 0, r0, c7, c14, 0		@ D cache clean+invalidate
 #ifndef CONFIG_ARM_ERRATA_411920
+#ifdef CONFIG_SMP
 	mcr	p15, 0, r0, c7, c5, 0		@ I+BTB cache invalidate
 #else
 	b	v6_icache_inval_all
 #endif
+#endif
 #else
 	mcr	p15, 0, r0, c7, c15, 0		@ Cache clean+invalidate
 #endif
@@ -132,11 +167,14 @@ ENTRY(v6_coherent_user_range)
 	mov	r0, #0
 #ifdef HARVARD_CACHE
 	mcr	p15, 0, r0, c7, c10, 4		@ drain write buffer
+
 #ifndef CONFIG_ARM_ERRATA_411920
+#ifdef CONFIG_SMP
 	mcr	p15, 0, r0, c7, c5, 0		@ I+BTB cache invalidate
 #else
 	b	v6_icache_inval_all
 #endif
+#endif
 #else
 	mcr	p15, 0, r0, c7, c5, 6		@ invalidate BTB
 #endif
@@ -154,10 +192,12 @@ ENTRY(v6_flush_kern_dcache_page)
 	add	r1, r0, #PAGE_SZ
 1:
 #ifdef HARVARD_CACHE
-	mcr	p15, 0, r0, c7, c14, 1		@ clean & invalidate D line
+	mcr	p15, 0, r0, c7, c14, 1		@ clean &
+						@ invalidate D line
 #else
-	mcr	p15, 0, r0, c7, c15, 1		@ clean & invalidate unified line
-#endif	
+	mcr	p15, 0, r0, c7, c15, 1		@ clean &
+						@ invalidate unified line
+#endif
 	add	r0, r0, #D_CACHE_LINE_SIZE
 	cmp	r0, r1
 	blo	1b
@@ -189,9 +229,11 @@ ENTRY(v6_dma_inv_range)
 	tst	r1, #D_CACHE_LINE_SIZE - 1
 	bic	r1, r1, #D_CACHE_LINE_SIZE - 1
 #ifdef HARVARD_CACHE
-	mcrne	p15, 0, r1, c7, c14, 1		@ clean & invalidate D line
+	mcrne	p15, 0, r1, c7, c14, 1		@ clean &
+						@ invalidate D line
 #else
-	mcrne	p15, 0, r1, c7, c15, 1		@ clean & invalidate unified line
+	mcrne	p15, 0, r1, c7, c15, 1		@ clean &
+						@ invalidate unified line
 #endif
 1:
 #ifdef HARVARD_CACHE
diff --git a/arch/arm/mm/flush.c b/arch/arm/mm/flush.c
index ddacdba..6313c90 100644
--- a/arch/arm/mm/flush.c
+++ b/arch/arm/mm/flush.c
@@ -27,17 +27,23 @@ extern void v6_icache_inval_all(void);
 
 static void flush_pfn_alias(unsigned long pfn, unsigned long vaddr)
 {
-	unsigned long to = ALIAS_FLUSH_START + (CACHE_COLOUR(vaddr) << PAGE_SHIFT);
+	unsigned long to;
 	const int zero = 0;
 
+	to = ALIAS_FLUSH_START + (CACHE_COLOUR(vaddr) << PAGE_SHIFT);
+
 	set_pte_ext(TOP_PTE(to), pfn_pte(pfn, PAGE_KERNEL), 0);
 	flush_tlb_kernel_page(to);
 
-	asm(	"mcrr	p15, 0, %1, %0, c14\n"
+	asm("mcrr	p15, 0, %1, %0, c14\n"
 	"	mcr	p15, 0, %2, c7, c10, 4\n"
 #ifndef CONFIG_ARM_ERRATA_411920
+#if __LINUX_ARM_ARCH__ != 6 || defined(CONFIG_SMP)
 	"	mcr	p15, 0, %2, c7, c5, 0\n"
 #endif
+#else
+		"bl	v6_icache_inval_all"
+#endif
 	    :
 	    : "r" (to), "r" (to + PAGE_SIZE - L1_CACHE_BYTES), "r" (zero)
 	    : "cc");
@@ -55,10 +61,14 @@ void flush_cache_mm(struct mm_struct *mm)
 	}
 
 	if (cache_is_vipt_aliasing()) {
-		asm(	"mcr	p15, 0, %0, c7, c14, 0\n"
-		"	mcr	p15, 0, %0, c7, c10, 4\n"
+		asm("mcr	p15, 0, %0, c7, c14, 0\n"
+    		"	mcr	p15, 0, %0, c7, c10, 4\n"
 #ifndef CONFIG_ARM_ERRATA_411920
+#if __LINUX_ARM_ARCH__ != 6 || defined(CONFIG_SMP)
 		"	mcr	p15, 0, %0, c7, c5, 0\n"
+#else
+		"	bl	v6_icache_inval_all\n"
+#endif
 #endif
 		    :
 		    : "r" (0)
@@ -69,20 +79,26 @@ void flush_cache_mm(struct mm_struct *mm)
 	}
 }
 
-void flush_cache_range(struct vm_area_struct *vma, unsigned long start, unsigned long end)
+void flush_cache_range(struct vm_area_struct *vma, unsigned long start,
+	unsigned long end)
 {
 	if (cache_is_vivt()) {
 		if (cpu_isset(smp_processor_id(), vma->vm_mm->cpu_vm_mask))
-			__cpuc_flush_user_range(start & PAGE_MASK, PAGE_ALIGN(end),
-						vma->vm_flags);
+			__cpuc_flush_user_range(start & PAGE_MASK,
+				PAGE_ALIGN(end), vma->vm_flags);
 		return;
 	}
 
 	if (cache_is_vipt_aliasing()) {
-		asm(	"mcr	p15, 0, %0, c7, c14, 0\n"
+		asm("mcr	p15, 0, %0, c7, c14, 0\n"
 		"	mcr	p15, 0, %0, c7, c10, 4\n"
 #ifndef CONFIG_ARM_ERRATA_411920
+#if __LINUX_ARM_ARCH__ != 6 || defined(CONFIG_SMP)
 		"	mcr	p15, 0, %0, c7, c5, 0\n"
+
+#else
+		"	bl	v6_icache_inval_all\n"
+#endif
 #endif
 		    :
 		    : "r" (0)
@@ -93,12 +109,14 @@ void flush_cache_range(struct vm_area_struct *vma, unsigned long start, unsigned
 	}
 }
 
-void flush_cache_page(struct vm_area_struct *vma, unsigned long user_addr, unsigned long pfn)
+void flush_cache_page(struct vm_area_struct *vma, unsigned long user_addr,
+	unsigned long pfn)
 {
 	if (cache_is_vivt()) {
 		if (cpu_isset(smp_processor_id(), vma->vm_mm->cpu_vm_mask)) {
 			unsigned long addr = user_addr & PAGE_MASK;
-			__cpuc_flush_user_range(addr, addr + PAGE_SIZE, vma->vm_flags);
+			__cpuc_flush_user_range(addr, addr + PAGE_SIZE,
+				vma->vm_flags);
 		}
 		return;
 	}
@@ -133,7 +151,7 @@ void flush_ptrace_access(struct vm_area_struct *vma, struct page *page,
 	}
 }
 #else
-#define flush_pfn_alias(pfn,vaddr)	do { } while (0)
+#define flush_pfn_alias(pfn, vaddr)	do { } while (0)
 #endif
 
 void __flush_dcache_page(struct address_space *mapping, struct page *page)
@@ -155,7 +173,8 @@ void __flush_dcache_page(struct address_space *mapping, struct page *page)
 				page->index << PAGE_CACHE_SHIFT);
 }
 
-static void __flush_dcache_aliases(struct address_space *mapping, struct page *page)
+static void __flush_dcache_aliases(struct address_space *mapping,
+	struct page *page)
 {
 	struct mm_struct *mm = current->active_mm;
 	struct vm_area_struct *mpnt;
@@ -182,7 +201,8 @@ static void __flush_dcache_aliases(struct address_space *mapping, struct page *p
 		if (!(mpnt->vm_flags & VM_MAYSHARE))
 			continue;
 		offset = (pgoff - mpnt->vm_pgoff) << PAGE_SHIFT;
-		flush_cache_page(mpnt, mpnt->vm_start + offset, page_to_pfn(page));
+		flush_cache_page(mpnt, mpnt->vm_start + offset,
+			page_to_pfn(page));
 	}
 	flush_dcache_mmap_unlock(mapping);
 }
@@ -233,7 +253,8 @@ EXPORT_SYMBOL(flush_dcache_page);
  *  memcpy() to/from page
  *  if written to page, flush_dcache_page()
  */
-void __flush_anon_page(struct vm_area_struct *vma, struct page *page, unsigned long vmaddr)
+void __flush_anon_page(struct vm_area_struct *vma, struct page *page,
+	unsigned long vmaddr)
 {
 	unsigned long pfn;
 
diff --git a/arch/arm/mm/proc-v6.S b/arch/arm/mm/proc-v6.S
index 5702ec5..a811c80 100644
--- a/arch/arm/mm/proc-v6.S
+++ b/arch/arm/mm/proc-v6.S
@@ -30,7 +30,15 @@
 #define TTB_RGN_WB	(3 << 3)
 
 #ifndef CONFIG_SMP
+#ifdef CONFIG_MP200_L220
+#ifdef CONFIG_MP200_L220_WT
+#define TTB_FLAGS	TTB_RGN_WT
+#else
+#define TTB_FLAGS	TTB_RGN_WB
+#endif
+#else
 #define TTB_FLAGS	TTB_RGN_WBWA
+#endif
 #else
 #define TTB_FLAGS	TTB_RGN_WBWA|TTB_S
 #endif
@@ -70,9 +78,50 @@ ENTRY(cpu_v6_reset)
  *
  *	IRQs are already disabled.
  */
+#ifdef CONFIG_MP200_L220
+#include <mach/hardware.h>
+	.align	5
+ENTRY(cpu_v6_do_idle)
+	ldr	r2, =IO_ADDRESS(MP200_SMU_BASE)
+	ldr	r0, =0x00244202		@ modify CPU div rate
+	str	r0, [r2, #0xf8]
+	ldr	r3, [r2, #0x80]		@ backup current div slot
+	mov	r0, #3			@ change with Normal-C slot
+	str	r0, [r2, #0x80]
+2:	ldr	r0, [r2, #0x80]
+	lsr	r0, #8
+	cmp	r0, #3
+	bne	2b
+
+	ldr	r2, =IO_ADDRESS(MP200_L220_BASE)
+	ldr	r0, =1f
+	mcr	p15, 0, r0, c7, c13, 1  @ prefetch I cache
+	ldr	r0, =3f
+	mcr	p15, 0, r0, c7, c13, 1  @ prefetch I cache
+
+	mov	r0, #0
+	mcr	p15, 0, r0, c7, c10, 4	@ drain write buffer
+
+	mov	r0, #0x1
+	str	r0, [r2, #0x730]	@ L2 Sync
+L2_sync_loop:
+	ldr	r0, [r2, #0x730]
+	cmp	r0, #0
+	bne	L2_sync_loop
+
+
+1:	mcr	p15, 0, r1, c7, c0, 4		@ wait for interrupt
+	b	2f
+2:	ldr	r2, =IO_ADDRESS(MP200_SMU_BASE)
+	str	r3, [r2, #0x80]		@ restore CPU div rate
+	mov	pc, lr
+3:
+
+#else
 ENTRY(cpu_v6_do_idle)
 	mcr	p15, 0, r1, c7, c0, 4		@ wait for interrupt
 	mov	pc, lr
+#endif
 
 ENTRY(cpu_v6_dcache_clean_area)
 #ifndef TLB_CAN_READ_FROM_L1_CACHE
@@ -134,6 +183,13 @@ ENTRY(cpu_v6_set_pte_ext)
 	orr	r3, r3, r2
 	orr	r3, r3, #PTE_EXT_AP0 | 2
 
+#ifdef CONFIG_ARCH_MP200
+	tst	r1, #L_PTE_CACHEABLE
+	tstne	r1, #L_PTE_BUFFERABLE
+	orrne	r3, r3, #PTE_EXT_TEX(4)
+	orrne	r3, r3, #PTE_BUFFERABLE | PTE_CACHEABLE
+#endif
+
 	tst	r1, #L_PTE_WRITE
 	tstne	r1, #L_PTE_DIRTY
 	orreq	r3, r3, #PTE_EXT_APX
@@ -253,7 +309,8 @@ __v6_proc_info:
 		PMD_SECT_BUFFERABLE | \
 		PMD_SECT_CACHEABLE | \
 		PMD_SECT_AP_WRITE | \
-		PMD_SECT_AP_READ
+		PMD_SECT_AP_READ | \
+		PMD_SECT_TEX(4)
 	.long   PMD_TYPE_SECT | \
 		PMD_SECT_XN | \
 		PMD_SECT_AP_WRITE | \
diff --git a/include/asm-arm/cacheflush.h b/include/asm-arm/cacheflush.h
new file mode 100644
index 0000000..98c317a
--- /dev/null
+++ b/include/asm-arm/cacheflush.h
@@ -0,0 +1,565 @@
+/*
+ *  arch/arm/include/asm/cacheflush.h
+ *
+ *  Copyright (C) 1999-2002 Russell King
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+#ifndef _ASMARM_CACHEFLUSH_H
+#define _ASMARM_CACHEFLUSH_H
+
+#include <linux/sched.h>
+#include <linux/mm.h>
+
+#include <asm/glue.h>
+#include <asm/shmparam.h>
+
+#define CACHE_COLOUR(vaddr)	((vaddr & (SHMLBA - 1)) >> PAGE_SHIFT)
+
+/*
+ *	Cache Model
+ *	===========
+ */
+#undef _CACHE
+#undef MULTI_CACHE
+
+#if defined(CONFIG_CPU_CACHE_V3)
+# ifdef _CACHE
+#  define MULTI_CACHE 1
+# else
+#  define _CACHE v3
+# endif
+#endif
+
+#if defined(CONFIG_CPU_CACHE_V4)
+# ifdef _CACHE
+#  define MULTI_CACHE 1
+# else
+#  define _CACHE v4
+# endif
+#endif
+
+#if defined(CONFIG_CPU_ARM920T) || defined(CONFIG_CPU_ARM922T) || \
+    defined(CONFIG_CPU_ARM925T) || defined(CONFIG_CPU_ARM1020)
+# define MULTI_CACHE 1
+#endif
+
+#if defined(CONFIG_CPU_ARM926T)
+# ifdef _CACHE
+#  define MULTI_CACHE 1
+# else
+#  define _CACHE arm926
+# endif
+#endif
+
+#if defined(CONFIG_CPU_ARM940T)
+# ifdef _CACHE
+#  define MULTI_CACHE 1
+# else
+#  define _CACHE arm940
+# endif
+#endif
+
+#if defined(CONFIG_CPU_ARM946E)
+# ifdef _CACHE
+#  define MULTI_CACHE 1
+# else
+#  define _CACHE arm946
+# endif
+#endif
+
+#if defined(CONFIG_CPU_CACHE_V4WB)
+# ifdef _CACHE
+#  define MULTI_CACHE 1
+# else
+#  define _CACHE v4wb
+# endif
+#endif
+
+#if defined(CONFIG_CPU_XSCALE)
+# ifdef _CACHE
+#  define MULTI_CACHE 1
+# else
+#  define _CACHE xscale
+# endif
+#endif
+
+#if defined(CONFIG_CPU_XSC3)
+# ifdef _CACHE
+#  define MULTI_CACHE 1
+# else
+#  define _CACHE xsc3
+# endif
+#endif
+
+#if defined(CONFIG_CPU_FEROCEON)
+# define MULTI_CACHE 1
+#endif
+
+#if defined(CONFIG_CPU_V6)
+/*# ifdef _CACHE      */
+#  define MULTI_CACHE 1
+/*# else              */
+/*#  define _CACHE v6 */
+/*# endif             */
+#endif
+
+#if defined(CONFIG_CPU_V7)
+/*# ifdef _CACHE      */
+#  define MULTI_CACHE 1
+/*# else              */
+/*#  define _CACHE v7 */
+/*# endif             */
+#endif
+
+#if !defined(_CACHE) && !defined(MULTI_CACHE)
+#error Unknown cache maintainence model
+#endif
+
+/*
+ * This flag is used to indicate that the page pointed to by a pte
+ * is dirty and requires cleaning before returning it to the user.
+ */
+#define PG_dcache_dirty PG_arch_1
+
+/*
+ *	MM Cache Management
+ *	===================
+ *
+ *	The arch/arm/mm/cache-*.S and arch/arm/mm/proc-*.S files
+ *	implement these methods.
+ *
+ *	Start addresses are inclusive and end addresses are exclusive;
+ *	start addresses should be rounded down, end addresses up.
+ *
+ *	See Documentation/cachetlb.txt for more information.
+ *	Please note that the implementation of these, and the required
+ *	effects are cache-type (VIVT/VIPT/PIPT) specific.
+ *
+ *	flush_cache_kern_all()
+ *
+ *		Unconditionally clean and invalidate the entire cache.
+ *
+ *	flush_cache_user_mm(mm)
+ *
+ *		Clean and invalidate all user space cache entries
+ *		before a change of page tables.
+ *
+ *	flush_cache_user_range(start, end, flags)
+ *
+ *		Clean and invalidate a range of cache entries in the
+ *		specified address space before a change of page tables.
+ *		- start - user start address (inclusive, page aligned)
+ *		- end   - user end address   (exclusive, page aligned)
+ *		- flags - vma->vm_flags field
+ *
+ *	coherent_kern_range(start, end)
+ *
+ *		Ensure coherency between the Icache and the Dcache in the
+ *		region described by start, end.  If you have non-snooping
+ *		Harvard caches, you need to implement this function.
+ *		- start  - virtual start address
+ *		- end    - virtual end address
+ *
+ *	DMA Cache Coherency
+ *	===================
+ *
+ *	dma_inv_range(start, end)
+ *
+ *		Invalidate (discard) the specified virtual address range.
+ *		May not write back any entries.  If 'start' or 'end'
+ *		are not cache line aligned, those lines must be written
+ *		back.
+ *		- start  - virtual start address
+ *		- end    - virtual end address
+ *
+ *	dma_clean_range(start, end)
+ *
+ *		Clean (write back) the specified virtual address range.
+ *		- start  - virtual start address
+ *		- end    - virtual end address
+ *
+ *	dma_flush_range(start, end)
+ *
+ *		Clean and invalidate the specified virtual address range.
+ *		- start  - virtual start address
+ *		- end    - virtual end address
+ */
+
+struct cpu_cache_fns {
+	void (*flush_kern_all)(void);
+	void (*flush_user_all)(void);
+	void (*flush_user_range)(unsigned long, unsigned long, unsigned int);
+
+	void (*coherent_kern_range)(unsigned long, unsigned long);
+	void (*coherent_user_range)(unsigned long, unsigned long);
+	void (*flush_kern_dcache_page)(void *);
+
+	void (*dma_inv_range)(const void *, const void *);
+	void (*dma_clean_range)(const void *, const void *);
+	void (*dma_flush_range)(const void *, const void *);
+};
+
+struct outer_cache_fns {
+	void (*inv_range)(unsigned long, unsigned long);
+	void (*clean_range)(unsigned long, unsigned long);
+	void (*flush_range)(unsigned long, unsigned long);
+	void (*flush_all)(void);
+};
+
+/*
+ * Select the calling method
+ */
+#ifdef MULTI_CACHE
+
+extern struct cpu_cache_fns cpu_cache;
+
+#define __cpuc_flush_kern_all		cpu_cache.flush_kern_all
+#define __cpuc_flush_user_all		cpu_cache.flush_user_all
+#define __cpuc_flush_user_range		cpu_cache.flush_user_range
+#define __cpuc_coherent_kern_range	cpu_cache.coherent_kern_range
+#define __cpuc_coherent_user_range	cpu_cache.coherent_user_range
+#define __cpuc_flush_dcache_page	cpu_cache.flush_kern_dcache_page
+
+/*
+ * These are private to the dma-mapping API.  Do not use directly.
+ * Their sole purpose is to ensure that data held in the cache
+ * is visible to DMA, or data written by DMA to system memory is
+ * visible to the CPU.
+ */
+#define dmac_inv_range			cpu_cache.dma_inv_range
+#define dmac_clean_range		cpu_cache.dma_clean_range
+#define dmac_flush_range		cpu_cache.dma_flush_range
+
+#else
+
+#define __cpuc_flush_kern_all		__glue(_CACHE, _flush_kern_cache_all)
+#define __cpuc_flush_user_all		__glue(_CACHE, _flush_user_cache_all)
+#define __cpuc_flush_user_range		__glue(_CACHE, _flush_user_cache_range)
+#define __cpuc_coherent_kern_range	__glue(_CACHE, _coherent_kern_range)
+#define __cpuc_coherent_user_range	__glue(_CACHE, _coherent_user_range)
+#define __cpuc_flush_dcache_page	__glue(_CACHE, _flush_kern_dcache_page)
+
+extern void __cpuc_flush_kern_all(void);
+extern void __cpuc_flush_user_all(void);
+extern void __cpuc_flush_user_range(unsigned long, unsigned long, unsigned int);
+extern void __cpuc_coherent_kern_range(unsigned long, unsigned long);
+extern void __cpuc_coherent_user_range(unsigned long, unsigned long);
+extern void __cpuc_flush_dcache_page(void *);
+
+/*
+ * These are private to the dma-mapping API.  Do not use directly.
+ * Their sole purpose is to ensure that data held in the cache
+ * is visible to DMA, or data written by DMA to system memory is
+ * visible to the CPU.
+ */
+#define dmac_inv_range			__glue(_CACHE, _dma_inv_range)
+#define dmac_clean_range		__glue(_CACHE, _dma_clean_range)
+#define dmac_flush_range		__glue(_CACHE, _dma_flush_range)
+
+extern void dmac_inv_range(const void *, const void *);
+extern void dmac_clean_range(const void *, const void *);
+extern void dmac_flush_range(const void *, const void *);
+
+#endif
+
+#ifdef CONFIG_OUTER_CACHE
+
+extern struct outer_cache_fns outer_cache;
+
+static inline void outer_inv_range(unsigned long start, unsigned long end)
+{
+	if (outer_cache.inv_range)
+		outer_cache.inv_range(start, end);
+}
+static inline void outer_clean_range(unsigned long start, unsigned long end)
+{
+	if (outer_cache.clean_range)
+		outer_cache.clean_range(start, end);
+}
+static inline void outer_flush_range(unsigned long start, unsigned long end)
+{
+	if (outer_cache.flush_range)
+		outer_cache.flush_range(start, end);
+}
+static inline void outer_flush_all(void)
+{
+	if (outer_cache.flush_all)
+		outer_cache.flush_all();
+}
+
+#else
+
+static inline void outer_inv_range(unsigned long start, unsigned long end)
+{ }
+static inline void outer_clean_range(unsigned long start, unsigned long end)
+{ }
+static inline void outer_flush_range(unsigned long start, unsigned long end)
+{ }
+static inline void outer_flush_all(void)
+{}
+
+#endif
+
+/*
+ * flush_cache_vmap() is used when creating mappings (eg, via vmap,
+ * vmalloc, ioremap etc) in kernel space for pages.  Since the
+ * direct-mappings of these pages may contain cached data, we need
+ * to do a full cache flush to ensure that writebacks don't corrupt
+ * data placed into these pages via the new mappings.
+ */
+#define flush_cache_vmap(start, end)		flush_cache_all()
+#define flush_cache_vunmap(start, end)		flush_cache_all()
+
+/*
+ * Copy user data from/to a page which is mapped into a different
+ * processes address space.  Really, we want to allow our "user
+ * space" model to handle this.
+ */
+#define copy_to_user_page(vma, page, vaddr, dst, src, len) \
+	do {							\
+		memcpy(dst, src, len);				\
+		flush_ptrace_access(vma, page, vaddr, dst, len, 1);\
+	} while (0)
+
+#define copy_from_user_page(vma, page, vaddr, dst, src, len) \
+	do {							\
+		memcpy(dst, src, len);				\
+	} while (0)
+
+/*
+ * Convert calls to our calling convention.
+ */
+#define flush_cache_all()		__cpuc_flush_kern_all()
+#ifndef CONFIG_CPU_CACHE_VIPT
+static inline void flush_cache_mm(struct mm_struct *mm)
+{
+	if (cpu_isset(smp_processor_id(), mm->cpu_vm_mask))
+		__cpuc_flush_user_all();
+}
+
+static inline void flush_cache_range(struct vm_area_struct *vma,
+	unsigned long start, unsigned long end)
+{
+	if (cpu_isset(smp_processor_id(), vma->vm_mm->cpu_vm_mask))
+		__cpuc_flush_user_range(start & PAGE_MASK, PAGE_ALIGN(end),
+					vma->vm_flags);
+}
+
+static inline void flush_cache_page(struct vm_area_struct *vma,
+	unsigned long user_addr, unsigned long pfn)
+{
+	if (cpu_isset(smp_processor_id(), vma->vm_mm->cpu_vm_mask)) {
+		unsigned long addr = user_addr & PAGE_MASK;
+		__cpuc_flush_user_range(addr, addr + PAGE_SIZE, vma->vm_flags);
+	}
+}
+
+static inline void flush_ptrace_access(struct vm_area_struct *vma,
+	struct page *page, unsigned long uaddr, void *kaddr,
+	unsigned long len, int write)
+{
+	if (cpu_isset(smp_processor_id(), vma->vm_mm->cpu_vm_mask)) {
+		unsigned long addr = (unsigned long)kaddr;
+		__cpuc_coherent_kern_range(addr, addr + len);
+	}
+}
+#else
+extern void flush_cache_mm(struct mm_struct *mm);
+extern void flush_cache_range(struct vm_area_struct *vma, unsigned long start,
+	unsigned long end);
+extern void flush_cache_page(struct vm_area_struct *vma,
+	unsigned long user_addr, unsigned long pfn);
+extern void flush_ptrace_access(struct vm_area_struct *vma, struct page *page,
+	unsigned long uaddr, void *kaddr, unsigned long len, int write);
+#endif
+
+#define flush_cache_dup_mm(mm) flush_cache_mm(mm)
+
+/*
+ * flush_cache_user_range is used when we want to ensure that the
+ * Harvard caches are synchronised for the user space address range.
+ * This is used for the ARM private sys_cacheflush system call.
+ */
+#define flush_cache_user_range(vma, start, end) \
+	__cpuc_coherent_user_range((start) & PAGE_MASK, PAGE_ALIGN(end))
+
+/*
+ * Perform necessary cache operations to ensure that data previously
+ * stored within this range of addresses can be executed by the CPU.
+ */
+#define flush_icache_range(s, e)	__cpuc_coherent_kern_range(s, e)
+
+/*
+ * Perform necessary cache operations to ensure that the TLB will
+ * see data written in the specified area.
+ */
+#define clean_dcache_area(start, size)	cpu_dcache_clean_area(start, size)
+
+/*
+ * flush_dcache_page is used when the kernel has written to the page
+ * cache page at virtual address page->virtual.
+ *
+ * If this page isn't mapped (ie, page_mapping == NULL), or it might
+ * have userspace mappings, then we _must_ always clean + invalidate
+ * the dcache entries associated with the kernel mapping.
+ *
+ * Otherwise we can defer the operation, and clean the cache when we are
+ * about to change to user space.  This is the same method as used on SPARC64.
+ * See update_mmu_cache for the user space part.
+ */
+extern void flush_dcache_page(struct page *);
+
+extern void __flush_dcache_page(struct address_space *mapping,
+	struct page *page);
+
+static inline void __flush_icache_all(void)
+{
+	asm("mcr	p15, 0, %0, c7, c5, 0	@ invalidate I-cache\n"
+	    :
+	    : "r" (0));
+}
+
+#define ARCH_HAS_FLUSH_ANON_PAGE
+static inline void flush_anon_page(struct vm_area_struct *vma,
+			 struct page *page, unsigned long vmaddr)
+{
+	extern void __flush_anon_page(struct vm_area_struct *vma,
+				struct page *, unsigned long);
+	if (PageAnon(page))
+		__flush_anon_page(vma, page, vmaddr);
+}
+
+#define flush_dcache_mmap_lock(mapping) \
+	spin_lock_irq(&(mapping)->tree_lock)
+#define flush_dcache_mmap_unlock(mapping) \
+	spin_unlock_irq(&(mapping)->tree_lock)
+
+#define flush_icache_user_range(vma, page, addr, len) \
+	flush_dcache_page(page)
+
+/*
+ * We don't appear to need to do anything here.  In fact, if we did, we'd
+ * duplicate cache flushing elsewhere performed by flush_dcache_page().
+ */
+#define flush_icache_page(vma, page)	do { } while (0)
+
+static inline void flush_ioremap_region(unsigned long phys, void __iomem *virt,
+	unsigned offset, size_t size)
+{
+	const void *start = (void __force *)virt + offset;
+	dmac_inv_range(start, start + size);
+}
+
+#define __cacheid_present(val)			\
+	(val != read_cpuid(CPUID_ID))
+
+#define __cacheid_type_v7(val)			\
+	((val & (7 << 29)) == (4 << 29))
+
+#define __cacheid_vivt_prev7(val)		\
+	((val & (15 << 25)) != (14 << 25))
+
+#define __cacheid_vipt_prev7(val)		\
+	((val & (15 << 25)) == (14 << 25))
+
+#define __cacheid_vipt_nonaliasing_prev7(val)	\
+	((val & (15 << 25 | 1 << 23)) == (14 << 25))
+
+#define __cacheid_vipt_aliasing_prev7(val)	\
+	((val & (15 << 25 | 1 << 23)) == (14 << 25 | 1 << 23))
+
+#define __cacheid_vivt(val)			\
+	(__cacheid_type_v7(val) ? 0 : __cacheid_vivt_prev7(val))
+
+#define __cacheid_vipt(val)			\
+	(__cacheid_type_v7(val) ? 1 : __cacheid_vipt_prev7(val))
+
+#define __cacheid_vipt_nonaliasing(val)		\
+	(__cacheid_type_v7(val) ? 1 : __cacheid_vipt_nonaliasing_prev7(val))
+
+#define __cacheid_vipt_aliasing(val)		\
+	(__cacheid_type_v7(val) ? 0 : __cacheid_vipt_aliasing_prev7(val))
+
+#define __cacheid_vivt_asid_tagged_instr(val)	\
+	(__cacheid_type_v7(val) ? ((val & (3 << 14)) == (1 << 14)) : 0)
+
+#if defined(CONFIG_CPU_CACHE_VIVT) && !defined(CONFIG_CPU_CACHE_VIPT)
+/*
+ * VIVT caches only
+ */
+#define cache_is_vivt()			1
+#define cache_is_vipt()			0
+#define cache_is_vipt_nonaliasing()	0
+#define cache_is_vipt_aliasing()	0
+#define icache_is_vivt_asid_tagged()	0
+
+#elif !defined(CONFIG_CPU_CACHE_VIVT) && defined(CONFIG_CPU_CACHE_VIPT)
+/*
+ * VIPT caches only
+ */
+#define cache_is_vivt()			0
+#define cache_is_vipt()			1
+#define cache_is_vipt_nonaliasing()					\
+	({								\
+		unsigned int __val = read_cpuid(CPUID_CACHETYPE);	\
+		__cacheid_vipt_nonaliasing(__val);			\
+	})
+
+#define cache_is_vipt_aliasing()					\
+	({								\
+		unsigned int __val = read_cpuid(CPUID_CACHETYPE);	\
+		__cacheid_vipt_aliasing(__val);				\
+	})
+
+#define icache_is_vivt_asid_tagged()					\
+	({								\
+		unsigned int __val = read_cpuid(CPUID_CACHETYPE);	\
+		__cacheid_vivt_asid_tagged_instr(__val);		\
+	})
+
+#else
+/*
+ * VIVT or VIPT caches.  Note that this is unreliable since ARM926
+ * and V6 CPUs satisfy the "(val & (15 << 25)) == (14 << 25)" test.
+ * There's no way to tell from the CacheType register what type (!)
+ * the cache is.
+ */
+#define cache_is_vivt()							\
+	({								\
+		unsigned int __val = read_cpuid(CPUID_CACHETYPE);	\
+		(!__cacheid_present(__val)) || __cacheid_vivt(__val);	\
+	})
+
+#define cache_is_vipt()							\
+	({								\
+		unsigned int __val = read_cpuid(CPUID_CACHETYPE);	\
+		__cacheid_present(__val) && __cacheid_vipt(__val);	\
+	})
+
+#define cache_is_vipt_nonaliasing()					\
+	({								\
+		unsigned int __val = read_cpuid(CPUID_CACHETYPE);	\
+		__cacheid_present(__val) &&				\
+		 __cacheid_vipt_nonaliasing(__val);			\
+	})
+
+#define cache_is_vipt_aliasing()					\
+	({								\
+		unsigned int __val = read_cpuid(CPUID_CACHETYPE);	\
+		__cacheid_present(__val) &&				\
+		 __cacheid_vipt_aliasing(__val);			\
+	})
+
+#define icache_is_vivt_asid_tagged()					\
+	({								\
+		unsigned int __val = read_cpuid(CPUID_CACHETYPE);	\
+		__cacheid_present(__val) &&				\
+		 __cacheid_vivt_asid_tagged_instr(__val);		\
+	})
+
+#endif
+
+#endif
-- 
1.6.5.2

