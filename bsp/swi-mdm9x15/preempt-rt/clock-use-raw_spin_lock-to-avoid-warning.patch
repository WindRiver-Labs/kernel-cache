From 864a48879ab530e294a0069eef472ac9bda0504c Mon Sep 17 00:00:00 2001
From: Weiwei Wang <weiwei.wang@windriver.com>
Date: Fri, 30 Aug 2013 20:20:58 +0800
Subject: [PATCH 62/70] clock: use raw_spin_lock to avoid warning

msm_rpm_set_noirq can only run when irq is disabled, so
use raw_spin_lock_irqsave/_irqrestore to avoid the warning

Signed-off-by: Weiwei Wang <weiwei.wang@windriver.com>
Signed-off-by: Catalin Enache <catalin.enache@windriver.com>
---
 arch/arm/mach-msm/clock-rpm.c     |   14 +++++++-------
 arch/arm/mach-msm/clock.c         |    8 ++++----
 arch/arm/mach-msm/clock.h         |    4 ++--
 arch/arm/mach-msm/rpm-regulator.c |   24 ++++++++++++------------
 4 files changed, 25 insertions(+), 25 deletions(-)

diff --git a/arch/arm/mach-msm/clock-rpm.c b/arch/arm/mach-msm/clock-rpm.c
index e3bd327..6486ccf 100644
--- a/arch/arm/mach-msm/clock-rpm.c
+++ b/arch/arm/mach-msm/clock-rpm.c
@@ -18,7 +18,7 @@
 #include "clock.h"
 #include "clock-rpm.h"
 
-static DEFINE_SPINLOCK(rpm_clock_lock);
+static DEFINE_RAW_SPINLOCK(rpm_clock_lock);
 
 static int rpm_clk_enable(struct clk *clk)
 {
@@ -30,7 +30,7 @@ static int rpm_clk_enable(struct clk *clk)
 	unsigned long peer_khz = 0, peer_sleep_khz = 0;
 	struct rpm_clk *peer = r->peer;
 
-	spin_lock_irqsave(&rpm_clock_lock, flags);
+	raw_spin_lock_irqsave(&rpm_clock_lock, flags);
 
 	this_khz = r->last_set_khz;
 	/* Don't send requests to the RPM if the rate has not been set. */
@@ -66,7 +66,7 @@ out:
 	if (!rc)
 		r->enabled = true;
 
-	spin_unlock_irqrestore(&rpm_clock_lock, flags);
+	raw_spin_unlock_irqrestore(&rpm_clock_lock, flags);
 
 	return rc;
 }
@@ -76,7 +76,7 @@ static void rpm_clk_disable(struct clk *clk)
 	unsigned long flags;
 	struct rpm_clk *r = to_rpm_clk(clk);
 
-	spin_lock_irqsave(&rpm_clock_lock, flags);
+	raw_spin_lock_irqsave(&rpm_clock_lock, flags);
 
 	if (r->last_set_khz) {
 		struct msm_rpm_iv_pair iv = { .id = r->rpm_clk_id };
@@ -100,7 +100,7 @@ static void rpm_clk_disable(struct clk *clk)
 	}
 	r->enabled = false;
 out:
-	spin_unlock_irqrestore(&rpm_clock_lock, flags);
+	raw_spin_unlock_irqrestore(&rpm_clock_lock, flags);
 
 	return;
 }
@@ -114,7 +114,7 @@ static int rpm_clk_set_rate(struct clk *clk, unsigned long rate)
 
 	this_khz = DIV_ROUND_UP(rate, 1000);
 
-	spin_lock_irqsave(&rpm_clock_lock, flags);
+	raw_spin_lock_irqsave(&rpm_clock_lock, flags);
 
 	/* Ignore duplicate requests. */
 	if (r->last_set_khz == this_khz)
@@ -154,7 +154,7 @@ static int rpm_clk_set_rate(struct clk *clk, unsigned long rate)
 	}
 
 out:
-	spin_unlock_irqrestore(&rpm_clock_lock, flags);
+	raw_spin_unlock_irqrestore(&rpm_clock_lock, flags);
 
 	return rc;
 }
diff --git a/arch/arm/mach-msm/clock.c b/arch/arm/mach-msm/clock.c
index 16b6b66..83075f5 100644
--- a/arch/arm/mach-msm/clock.c
+++ b/arch/arm/mach-msm/clock.c
@@ -75,12 +75,12 @@ int vote_vdd_level(struct clk_vdd_class *vdd_class, int level)
 	unsigned long flags;
 	int rc;
 
-	spin_lock_irqsave(&vdd_class->lock, flags);
+	raw_spin_lock_irqsave(&vdd_class->lock, flags);
 	vdd_class->level_votes[level]++;
 	rc = update_vdd(vdd_class);
 	if (rc)
 		vdd_class->level_votes[level]--;
-	spin_unlock_irqrestore(&vdd_class->lock, flags);
+	raw_spin_unlock_irqrestore(&vdd_class->lock, flags);
 
 	return rc;
 }
@@ -91,7 +91,7 @@ int unvote_vdd_level(struct clk_vdd_class *vdd_class, int level)
 	unsigned long flags;
 	int rc = 0;
 
-	spin_lock_irqsave(&vdd_class->lock, flags);
+	raw_spin_lock_irqsave(&vdd_class->lock, flags);
 	if (WARN(!vdd_class->level_votes[level],
 			"Reference counts are incorrect for %s level %d\n",
 			vdd_class->class_name, level))
@@ -101,7 +101,7 @@ int unvote_vdd_level(struct clk_vdd_class *vdd_class, int level)
 	if (rc)
 		vdd_class->level_votes[level]++;
 out:
-	spin_unlock_irqrestore(&vdd_class->lock, flags);
+	raw_spin_unlock_irqrestore(&vdd_class->lock, flags);
 	return rc;
 }
 
diff --git a/arch/arm/mach-msm/clock.h b/arch/arm/mach-msm/clock.h
index f0ddfcc..8aee6d9 100644
--- a/arch/arm/mach-msm/clock.h
+++ b/arch/arm/mach-msm/clock.h
@@ -68,7 +68,7 @@ struct clk_vdd_class {
 	int (*set_vdd)(struct clk_vdd_class *v_class, int level);
 	int level_votes[MAX_VDD_LEVELS];
 	unsigned long cur_level;
-	spinlock_t lock;
+	raw_spinlock_t lock;
 };
 
 #define DEFINE_VDD_CLASS(_name, _set_vdd) \
@@ -76,7 +76,7 @@ struct clk_vdd_class {
 		.class_name = #_name, \
 		.set_vdd = _set_vdd, \
 		.cur_level = ARRAY_SIZE(_name.level_votes), \
-		.lock = __SPIN_LOCK_UNLOCKED(lock) \
+		.lock = __RAW_SPIN_LOCK_UNLOCKED(lock) \
 	}
 
 enum handoff {
diff --git a/arch/arm/mach-msm/rpm-regulator.c b/arch/arm/mach-msm/rpm-regulator.c
index 7a6e617..3a25f04 100644
--- a/arch/arm/mach-msm/rpm-regulator.c
+++ b/arch/arm/mach-msm/rpm-regulator.c
@@ -302,7 +302,7 @@ static struct clk *tcxo_handle;
 static struct wake_lock tcxo_wake_lock;
 static DEFINE_MUTEX(tcxo_mutex);
 /* Spin lock needed for sleep-selectable regulators. */
-static DEFINE_SPINLOCK(tcxo_noirq_lock);
+static DEFINE_RAW_SPINLOCK(tcxo_noirq_lock);
 static bool tcxo_is_enabled;
 /*
  * TCXO must be kept on for at least the duration of its warmup (4 ms);
@@ -356,7 +356,7 @@ static void tcxo_delayed_disable_work(struct work_struct *work)
 	unsigned long flags = 0;
 
 	if (tcxo_workaround_noirq)
-		spin_lock_irqsave(&tcxo_noirq_lock, flags);
+		raw_spin_lock_irqsave(&tcxo_noirq_lock, flags);
 	else
 		mutex_lock(&tcxo_mutex);
 
@@ -365,7 +365,7 @@ static void tcxo_delayed_disable_work(struct work_struct *work)
 	wake_unlock(&tcxo_wake_lock);
 
 	if (tcxo_workaround_noirq)
-		spin_unlock_irqrestore(&tcxo_noirq_lock, flags);
+		raw_spin_unlock_irqrestore(&tcxo_noirq_lock, flags);
 	else
 		mutex_unlock(&tcxo_mutex);
 }
@@ -386,7 +386,7 @@ static void tcxo_delayed_disable(void)
 }
 
 /* Spin lock needed for sleep-selectable regulators. */
-static DEFINE_SPINLOCK(rpm_noirq_lock);
+static DEFINE_RAW_SPINLOCK(rpm_noirq_lock);
 
 static int voltage_from_req(struct vreg *vreg)
 {
@@ -469,7 +469,7 @@ static int vreg_send_request(struct vreg *vreg, enum rpm_vreg_voter voter,
 		    && (set == MSM_RPM_CTX_SET_0)
 		    && (GET_PART(vreg, uV) > GET_PART_PREV_ACT(vreg, uV))) {
 			voltage_increased = true;
-			spin_lock_irqsave(&tcxo_noirq_lock, flags);
+			raw_spin_lock_irqsave(&tcxo_noirq_lock, flags);
 			tcxo_enabled = tcxo_enable();
 		}
 
@@ -500,7 +500,7 @@ static int vreg_send_request(struct vreg *vreg, enum rpm_vreg_voter voter,
 		if (voltage_increased) {
 			if (tcxo_enabled)
 				tcxo_delayed_disable();
-			spin_unlock_irqrestore(&tcxo_noirq_lock, flags);
+			raw_spin_unlock_irqrestore(&tcxo_noirq_lock, flags);
 		}
 	} else if (msm_rpm_vreg_debug_mask & MSM_RPM_VREG_DEBUG_DUPLICATE) {
 		rpm_regulator_duplicate(vreg, set, cnt);
@@ -521,7 +521,7 @@ static int vreg_set_noirq(struct vreg *vreg, enum rpm_vreg_voter voter,
 	if (voter < 0 || voter >= RPM_VREG_VOTER_COUNT)
 		return -EINVAL;
 
-	spin_lock_irqsave(&rpm_noirq_lock, flags);
+	raw_spin_lock_irqsave(&rpm_noirq_lock, flags);
 
 	/*
 	 * Send sleep set request first so that subsequent set_mode, etc calls
@@ -557,7 +557,7 @@ static int vreg_set_noirq(struct vreg *vreg, enum rpm_vreg_voter voter,
 	rc = vreg_send_request(vreg, voter, MSM_RPM_CTX_SET_0, mask0, val0,
 					mask1, val1, cnt, update_voltage);
 
-	spin_unlock_irqrestore(&rpm_noirq_lock, flags);
+	raw_spin_unlock_irqrestore(&rpm_noirq_lock, flags);
 
 	return rc;
 }
@@ -1019,7 +1019,7 @@ static int vreg_store(struct vreg *vreg, unsigned mask0, unsigned val0,
 	unsigned long flags = 0;
 
 	if (vreg->pdata.sleep_selectable)
-		spin_lock_irqsave(&rpm_noirq_lock, flags);
+		raw_spin_lock_irqsave(&rpm_noirq_lock, flags);
 
 	vreg->req[0].value &= ~mask0;
 	vreg->req[0].value |= val0 & mask0;
@@ -1028,7 +1028,7 @@ static int vreg_store(struct vreg *vreg, unsigned mask0, unsigned val0,
 	vreg->req[1].value |= val1 & mask1;
 
 	if (vreg->pdata.sleep_selectable)
-		spin_unlock_irqrestore(&rpm_noirq_lock, flags);
+		raw_spin_unlock_irqrestore(&rpm_noirq_lock, flags);
 
 	return 0;
 }
@@ -1072,7 +1072,7 @@ static int vreg_set(struct vreg *vreg, unsigned mask0, unsigned val0,
 		if (!tcxo_handle)
 			tcxo_get_handle();
 		if (tcxo_workaround_noirq)
-			spin_lock_irqsave(&tcxo_noirq_lock, flags);
+			raw_spin_lock_irqsave(&tcxo_noirq_lock, flags);
 		else
 			mutex_lock(&tcxo_mutex);
 
@@ -1107,7 +1107,7 @@ static int vreg_set(struct vreg *vreg, unsigned mask0, unsigned val0,
 			tcxo_delayed_disable();
 
 		if (tcxo_workaround_noirq)
-			spin_unlock_irqrestore(&tcxo_noirq_lock, flags);
+			raw_spin_unlock_irqrestore(&tcxo_noirq_lock, flags);
 		else
 			mutex_unlock(&tcxo_mutex);
 	}
-- 
1.7.5.4

