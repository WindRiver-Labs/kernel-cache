From 591e2c68dfb9e0419d6a2bb80ea7fc7562ca06be Mon Sep 17 00:00:00 2001
From: Weiwei Wang <weiwei.wang@windriver.com>
Date: Sun, 2 Dec 2012 13:17:32 +0800
Subject: [PATCH 04/70] lib: introduce memory_alloc lib

Extracted from git tree
git://codeaurora.org/quic/le/kernel/msm

Signed-off-by: Weiwei Wang <weiwei.wang@windriver.com>
Signed-off-by: Catalin Enache <catalin.enache@windriver.com>
---
 include/linux/bitmap.h       |   24 ++-
 include/linux/genalloc.h     |   20 ++-
 include/linux/memory_alloc.h |   58 ++++++
 lib/Makefile                 |    2 +-
 lib/bitmap.c                 |   24 ++-
 lib/genalloc.c               |   30 ++-
 lib/memory_alloc.c           |  425 ++++++++++++++++++++++++++++++++++++++++++
 7 files changed, 555 insertions(+), 28 deletions(-)
 create mode 100644 include/linux/memory_alloc.h
 create mode 100644 lib/memory_alloc.c

diff --git a/include/linux/bitmap.h b/include/linux/bitmap.h
index 7ad6345..01ab451 100644
--- a/include/linux/bitmap.h
+++ b/include/linux/bitmap.h
@@ -45,6 +45,7 @@
  * bitmap_set(dst, pos, nbits)			Set specified bit area
  * bitmap_clear(dst, pos, nbits)		Clear specified bit area
  * bitmap_find_next_zero_area(buf, len, pos, n, mask)	Find bit free area
+ * bitmap_find_next_zero_area_off(buf, len, pos, n, mask)	as above
  * bitmap_shift_right(dst, src, n, nbits)	*dst = *src >> n
  * bitmap_shift_left(dst, src, n, nbits)	*dst = *src << n
  * bitmap_remap(dst, src, old, new, nbits)	*dst = map(old, new)(src)
@@ -114,11 +115,24 @@ extern int __bitmap_weight(const unsigned long *bitmap, int bits);
 
 extern void bitmap_set(unsigned long *map, int i, int len);
 extern void bitmap_clear(unsigned long *map, int start, int nr);
-extern unsigned long bitmap_find_next_zero_area(unsigned long *map,
-					 unsigned long size,
-					 unsigned long start,
-					 unsigned int nr,
-					 unsigned long align_mask);
+
+extern unsigned long bitmap_find_next_zero_area_off(unsigned long *map,
+						    unsigned long size,
+						    unsigned long start,
+						    unsigned int nr,
+						    unsigned long align_mask,
+						    unsigned long align_offset);
+
+static inline unsigned long
+bitmap_find_next_zero_area(unsigned long *map,
+			   unsigned long size,
+			   unsigned long start,
+			   unsigned int nr,
+			   unsigned long align_mask)
+{
+	return bitmap_find_next_zero_area_off(map, size, start, nr,
+					      align_mask, 0);
+}
 
 extern int bitmap_scnprintf(char *buf, unsigned int len,
 			const unsigned long *src, int nbits);
diff --git a/include/linux/genalloc.h b/include/linux/genalloc.h
index 5e98eeb..a87246c 100644
--- a/include/linux/genalloc.h
+++ b/include/linux/genalloc.h
@@ -72,10 +72,28 @@ static inline int gen_pool_add(struct gen_pool *pool, unsigned long addr,
 	return gen_pool_add_virt(pool, addr, -1, size, nid);
 }
 extern void gen_pool_destroy(struct gen_pool *);
-extern unsigned long gen_pool_alloc(struct gen_pool *, size_t);
 extern void gen_pool_free(struct gen_pool *, unsigned long, size_t);
 extern void gen_pool_for_each_chunk(struct gen_pool *,
 	void (*)(struct gen_pool *, struct gen_pool_chunk *, void *), void *);
 extern size_t gen_pool_avail(struct gen_pool *);
 extern size_t gen_pool_size(struct gen_pool *);
+
+unsigned long __must_check
+gen_pool_alloc_aligned(struct gen_pool *pool, size_t size,
+                       unsigned alignment_order);
+
+/**
+ * gen_pool_alloc() - allocate special memory from the pool
+ * @pool:       Pool to allocate from.
+ * @size:       Number of bytes to allocate from the pool.
+ *
+ * Allocate the requested number of bytes from the specified pool.
+ * Uses a first-fit algorithm.
+ */
+static inline unsigned long __must_check
+gen_pool_alloc(struct gen_pool *pool, size_t size)
+{
+        return gen_pool_alloc_aligned(pool, size, 0);
+}
+
 #endif /* __GENALLOC_H__ */
diff --git a/include/linux/memory_alloc.h b/include/linux/memory_alloc.h
new file mode 100644
index 0000000..e7049f8
--- /dev/null
+++ b/include/linux/memory_alloc.h
@@ -0,0 +1,58 @@
+/* Copyright (c) 2011, Code Aurora Forum. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 and
+ * only version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef _LINUX_MEMALLOC_H
+#define	_LINUX_MEMALLOC_H
+
+#include <linux/mutex.h>
+#include <linux/genalloc.h>
+#include <linux/rbtree.h>
+
+struct mem_pool {
+	struct mutex pool_mutex;
+	struct gen_pool *gpool;
+	unsigned long paddr;
+	unsigned long size;
+	unsigned long free;
+	unsigned int id;
+};
+
+struct alloc {
+	struct rb_node rb_node;
+	void *vaddr;
+	unsigned long paddr;
+	struct mem_pool *mpool;
+	unsigned long len;
+	void *caller;
+};
+
+struct mem_pool *initialize_memory_pool(unsigned long start,
+	unsigned long size, int mem_type);
+
+void *allocate_contiguous_memory(unsigned long size,
+	int mem_type, unsigned long align, int cached);
+
+unsigned long _allocate_contiguous_memory_nomap(unsigned long size,
+	int mem_type, unsigned long align, void *caller);
+
+unsigned long allocate_contiguous_memory_nomap(unsigned long size,
+	int mem_type, unsigned long align);
+
+void free_contiguous_memory(void *addr);
+void free_contiguous_memory_by_paddr(unsigned long paddr);
+
+unsigned long memory_pool_node_paddr(void *vaddr);
+
+unsigned long memory_pool_node_len(void *vaddr);
+
+int memory_pool_init(void);
+#endif	/* _LINUX_MEMALLOC_H */
diff --git a/lib/Makefile b/lib/Makefile
index a8da407..09ff6a3 100644
--- a/lib/Makefile
+++ b/lib/Makefile
@@ -12,7 +12,7 @@ lib-y := ctype.o string.o vsprintf.o cmdline.o \
 	 idr.o int_sqrt.o extable.o prio_tree.o \
 	 sha1.o md5.o irq_regs.o reciprocal_div.o argv_split.o \
 	 proportions.o prio_heap.o ratelimit.o show_mem.o \
-	 is_single_threaded.o plist.o decompress.o
+	 is_single_threaded.o plist.o decompress.o memory_alloc.o
 
 lib-$(CONFIG_MMU) += ioremap.o
 lib-$(CONFIG_SMP) += cpumask.o
diff --git a/lib/bitmap.c b/lib/bitmap.c
index b5a8b6a..f6821ec 100644
--- a/lib/bitmap.c
+++ b/lib/bitmap.c
@@ -315,30 +315,32 @@ void bitmap_clear(unsigned long *map, int start, int nr)
 }
 EXPORT_SYMBOL(bitmap_clear);
 
-/*
- * bitmap_find_next_zero_area - find a contiguous aligned zero area
+/**
+ * bitmap_find_next_zero_area_off - find a contiguous aligned zero area
  * @map: The address to base the search on
  * @size: The bitmap size in bits
  * @start: The bitnumber to start searching at
  * @nr: The number of zeroed bits we're looking for
  * @align_mask: Alignment mask for zero area
+ * @align_offset: Alignment offset for zero area.
  *
  * The @align_mask should be one less than a power of 2; the effect is that
- * the bit offset of all zero areas this function finds is multiples of that
- * power of 2. A @align_mask of 0 means no alignment is required.
+ * the bit offset of all zero areas this function finds plus @align_offset
+ * is multiple of that power of 2.
  */
-unsigned long bitmap_find_next_zero_area(unsigned long *map,
-					 unsigned long size,
-					 unsigned long start,
-					 unsigned int nr,
-					 unsigned long align_mask)
+unsigned long bitmap_find_next_zero_area_off(unsigned long *map,
+					     unsigned long size,
+					     unsigned long start,
+					     unsigned int nr,
+					     unsigned long align_mask,
+					     unsigned long align_offset)
 {
 	unsigned long index, end, i;
 again:
 	index = find_next_zero_bit(map, size, start);
 
 	/* Align allocation */
-	index = __ALIGN_MASK(index, align_mask);
+	index = __ALIGN_MASK(index + align_offset, align_mask) - align_offset;
 
 	end = index + nr;
 	if (end > size)
@@ -350,7 +352,7 @@ again:
 	}
 	return index;
 }
-EXPORT_SYMBOL(bitmap_find_next_zero_area);
+EXPORT_SYMBOL(bitmap_find_next_zero_area_off);
 
 /*
  * Bitmap printing & parsing functions: first version by Bill Irwin,
diff --git a/lib/genalloc.c b/lib/genalloc.c
index 7cb7a5d..e6ea9b7 100644
--- a/lib/genalloc.c
+++ b/lib/genalloc.c
@@ -250,20 +250,24 @@ void gen_pool_destroy(struct gen_pool *pool)
 EXPORT_SYMBOL(gen_pool_destroy);
 
 /**
- * gen_pool_alloc - allocate special memory from the pool
+ * gen_pool_alloc_aligned - allocate special memory from the pool
  * @pool: pool to allocate from
  * @size: number of bytes to allocate from the pool
+ * @alignment_order: Order the allocated space should be
+ *                   aligned to (eg. 20 means allocated space
+ *                   must be aligned to 1MiB).
  *
  * Allocate the requested number of bytes from the specified pool.
  * Uses a first-fit algorithm. Can not be used in NMI handler on
  * architectures without NMI-safe cmpxchg implementation.
  */
-unsigned long gen_pool_alloc(struct gen_pool *pool, size_t size)
+unsigned long gen_pool_alloc_aligned(struct gen_pool *pool, size_t size,
+				     unsigned alignment_order)
 {
 	struct gen_pool_chunk *chunk;
-	unsigned long addr = 0;
+	unsigned long addr = 0, align_mask = 0;
 	int order = pool->min_alloc_order;
-	int nbits, start_bit = 0, end_bit, remain;
+	int nbits, start_bit = 0, remain;
 
 #ifndef CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG
 	BUG_ON(in_nmi());
@@ -272,17 +276,23 @@ unsigned long gen_pool_alloc(struct gen_pool *pool, size_t size)
 	if (size == 0)
 		return 0;
 
+	if (alignment_order > order)
+		align_mask = (1 << (alignment_order - order)) - 1;
+
 	nbits = (size + (1UL << order) - 1) >> order;
+
 	rcu_read_lock();
 	list_for_each_entry_rcu(chunk, &pool->chunks, next_chunk) {
+		unsigned long chunk_size;
 		if (size > atomic_read(&chunk->avail))
 			continue;
+		chunk_size = (chunk->end_addr - chunk->start_addr) >> order;
 
-		end_bit = (chunk->end_addr - chunk->start_addr) >> order;
 retry:
-		start_bit = bitmap_find_next_zero_area(chunk->bits, end_bit,
-						       start_bit, nbits, 0);
-		if (start_bit >= end_bit)
+		start_bit = bitmap_find_next_zero_area_off(chunk->bits, chunk_size,
+						   0, nbits, align_mask,
+						   chunk->start_addr >> order);
+		if (start_bit >= chunk_size)
 			continue;
 		remain = bitmap_set_ll(chunk->bits, start_bit, nbits);
 		if (remain) {
@@ -293,14 +303,14 @@ retry:
 		}
 
 		addr = chunk->start_addr + ((unsigned long)start_bit << order);
-		size = nbits << order;
+		size = nbits << pool->min_alloc_order;
 		atomic_sub(size, &chunk->avail);
 		break;
 	}
 	rcu_read_unlock();
 	return addr;
 }
-EXPORT_SYMBOL(gen_pool_alloc);
+EXPORT_SYMBOL(gen_pool_alloc_aligned);
 
 /**
  * gen_pool_free - free allocated special memory back to the pool
diff --git a/lib/memory_alloc.c b/lib/memory_alloc.c
new file mode 100644
index 0000000..d931e14
--- /dev/null
+++ b/lib/memory_alloc.c
@@ -0,0 +1,425 @@
+/* Copyright (c) 2011, Code Aurora Forum. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 and
+ * only version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include <asm/page.h>
+#include <linux/io.h>
+#include <linux/memory_alloc.h>
+#include <linux/mm.h>
+#include <linux/vmalloc.h>
+#include <linux/slab.h>
+#include <linux/module.h>
+#include <linux/err.h>
+#include <linux/log2.h>
+#include <linux/debugfs.h>
+#include <linux/seq_file.h>
+
+
+#define MAX_MEMPOOLS 8
+
+struct mem_pool mpools[MAX_MEMPOOLS];
+
+/* The tree contains all allocations over all memory pools */
+static struct rb_root alloc_root;
+static struct mutex alloc_mutex;
+
+static void *s_start(struct seq_file *m, loff_t *pos)
+	__acquires(&alloc_mutex)
+{
+	loff_t n = *pos;
+	struct rb_node *r;
+
+	mutex_lock(&alloc_mutex);
+	r = rb_first(&alloc_root);
+
+	while (n > 0 && r) {
+		n--;
+		r = rb_next(r);
+	}
+	if (!n)
+		return r;
+	return NULL;
+}
+
+static void *s_next(struct seq_file *m, void *p, loff_t *pos)
+{
+	struct rb_node *r = p;
+	++*pos;
+	return rb_next(r);
+}
+
+static void s_stop(struct seq_file *m, void *p)
+	__releases(&alloc_mutex)
+{
+	mutex_unlock(&alloc_mutex);
+}
+
+static int s_show(struct seq_file *m, void *p)
+{
+	struct rb_node *r = p;
+	struct alloc *node = rb_entry(r, struct alloc, rb_node);
+
+	seq_printf(m, "0x%lx 0x%p %ld %u %pS\n", node->paddr, node->vaddr,
+		   node->len, node->mpool->id, node->caller);
+	return 0;
+}
+
+static const struct seq_operations mempool_op = {
+	.start = s_start,
+	.next = s_next,
+	.stop = s_stop,
+	.show = s_show,
+};
+
+static int mempool_open(struct inode *inode, struct file *file)
+{
+	return seq_open(file, &mempool_op);
+}
+
+static struct alloc *find_alloc(void *addr)
+{
+	struct rb_root *root = &alloc_root;
+	struct rb_node *p = root->rb_node;
+
+	mutex_lock(&alloc_mutex);
+
+	while (p) {
+		struct alloc *node;
+
+		node = rb_entry(p, struct alloc, rb_node);
+		if (addr < node->vaddr)
+			p = p->rb_left;
+		else if (addr > node->vaddr)
+			p = p->rb_right;
+		else {
+			mutex_unlock(&alloc_mutex);
+			return node;
+		}
+	}
+	mutex_unlock(&alloc_mutex);
+	return NULL;
+}
+
+static int add_alloc(struct alloc *node)
+{
+	struct rb_root *root = &alloc_root;
+	struct rb_node **p = &root->rb_node;
+	struct rb_node *parent = NULL;
+
+	mutex_lock(&alloc_mutex);
+	while (*p) {
+		struct alloc *tmp;
+		parent = *p;
+
+		tmp = rb_entry(parent, struct alloc, rb_node);
+
+		if (node->vaddr < tmp->vaddr)
+			p = &(*p)->rb_left;
+		else if (node->vaddr > tmp->vaddr)
+			p = &(*p)->rb_right;
+		else {
+			WARN(1, "memory at %p already allocated", tmp->vaddr);
+			mutex_unlock(&alloc_mutex);
+			return -EINVAL;
+		}
+	}
+	rb_link_node(&node->rb_node, parent, p);
+	rb_insert_color(&node->rb_node, root);
+	mutex_unlock(&alloc_mutex);
+	return 0;
+}
+
+static int remove_alloc(struct alloc *victim_node)
+{
+	struct rb_root *root = &alloc_root;
+	if (!victim_node)
+		return -EINVAL;
+
+	mutex_lock(&alloc_mutex);
+	rb_erase(&victim_node->rb_node, root);
+	mutex_unlock(&alloc_mutex);
+	return 0;
+}
+
+static struct gen_pool *initialize_gpool(unsigned long start,
+	unsigned long size)
+{
+	struct gen_pool *gpool;
+
+	gpool = gen_pool_create(PAGE_SHIFT, -1);
+
+	if (!gpool)
+		return NULL;
+	if (gen_pool_add(gpool, start, size, -1)) {
+		gen_pool_destroy(gpool);
+		return NULL;
+	}
+
+	return gpool;
+}
+
+static void *__alloc(struct mem_pool *mpool, unsigned long size,
+	unsigned long align, int cached, void *caller)
+{
+	unsigned long paddr;
+	void __iomem *vaddr;
+
+	unsigned long aligned_size;
+	int log_align = ilog2(align);
+
+	struct alloc *node;
+
+	aligned_size = PFN_ALIGN(size);
+	paddr = gen_pool_alloc_aligned(mpool->gpool, aligned_size, log_align);
+	if (!paddr)
+		return NULL;
+
+	node = kmalloc(sizeof(struct alloc), GFP_KERNEL);
+	if (!node)
+		goto out;
+
+	if (cached)
+		vaddr = ioremap_cached(paddr, aligned_size);
+	else
+		vaddr = ioremap(paddr, aligned_size);
+
+	if (!vaddr)
+		goto out_kfree;
+
+	node->vaddr = vaddr;
+	node->paddr = paddr;
+	node->len = aligned_size;
+	node->mpool = mpool;
+	node->caller = caller;
+	if (add_alloc(node))
+		goto out_kfree;
+
+	mpool->free -= aligned_size;
+
+	return vaddr;
+out_kfree:
+	if (vaddr)
+		iounmap(vaddr);
+	kfree(node);
+out:
+	gen_pool_free(mpool->gpool, paddr, aligned_size);
+	return NULL;
+}
+
+static void __free(void *vaddr, bool unmap)
+{
+	struct alloc *node = find_alloc(vaddr);
+
+	if (!node)
+		return;
+
+	if (unmap)
+		iounmap(node->vaddr);
+
+	gen_pool_free(node->mpool->gpool, node->paddr, node->len);
+	node->mpool->free += node->len;
+
+	remove_alloc(node);
+	kfree(node);
+}
+
+static struct mem_pool *mem_type_to_memory_pool(int mem_type)
+{
+	struct mem_pool *mpool = &mpools[mem_type];
+
+	if (!mpool->size)
+		return NULL;
+
+	mutex_lock(&mpool->pool_mutex);
+	if (!mpool->gpool)
+		mpool->gpool = initialize_gpool(mpool->paddr, mpool->size);
+	mutex_unlock(&mpool->pool_mutex);
+	if (!mpool->gpool)
+		return NULL;
+
+	return mpool;
+}
+
+struct mem_pool *initialize_memory_pool(unsigned long start,
+	unsigned long size, int mem_type)
+{
+	int id = mem_type;
+
+	if (id >= MAX_MEMPOOLS || size <= PAGE_SIZE || size % PAGE_SIZE)
+		return NULL;
+
+	mutex_lock(&mpools[id].pool_mutex);
+
+	mpools[id].paddr = start;
+	mpools[id].size = size;
+	mpools[id].free = size;
+	mpools[id].id = id;
+	mutex_unlock(&mpools[id].pool_mutex);
+
+	pr_info("memory pool %d (start %lx size %lx) initialized\n",
+		id, start, size);
+	return &mpools[id];
+}
+EXPORT_SYMBOL_GPL(initialize_memory_pool);
+
+void *allocate_contiguous_memory(unsigned long size,
+	int mem_type, unsigned long align, int cached)
+{
+	unsigned long aligned_size = PFN_ALIGN(size);
+	struct mem_pool *mpool;
+
+	mpool = mem_type_to_memory_pool(mem_type);
+	if (!mpool)
+		return NULL;
+	return __alloc(mpool, aligned_size, align, cached,
+		__builtin_return_address(0));
+
+}
+EXPORT_SYMBOL_GPL(allocate_contiguous_memory);
+
+unsigned long _allocate_contiguous_memory_nomap(unsigned long size,
+	int mem_type, unsigned long align, void *caller)
+{
+	unsigned long paddr;
+	unsigned long aligned_size;
+
+	struct alloc *node;
+	struct mem_pool *mpool;
+	int log_align = ilog2(align);
+
+	mpool = mem_type_to_memory_pool(mem_type);
+	if (!mpool || !mpool->gpool)
+		return 0;
+
+	aligned_size = PFN_ALIGN(size);
+	paddr = gen_pool_alloc_aligned(mpool->gpool, aligned_size, log_align);
+	if (!paddr)
+		return 0;
+
+	node = kmalloc(sizeof(struct alloc), GFP_KERNEL);
+	if (!node)
+		goto out;
+
+	node->paddr = paddr;
+
+	/* We search the tree using node->vaddr, so set
+	 * it to something unique even though we don't
+	 * use it for physical allocation nodes.
+	 * The virtual and physical address ranges
+	 * are disjoint, so there won't be any chance of
+	 * a duplicate node->vaddr value.
+	 */
+	node->vaddr = (void *)paddr;
+	node->len = aligned_size;
+	node->mpool = mpool;
+	node->caller = caller;
+	if (add_alloc(node))
+		goto out_kfree;
+
+	mpool->free -= aligned_size;
+	return paddr;
+out_kfree:
+	kfree(node);
+out:
+	gen_pool_free(mpool->gpool, paddr, aligned_size);
+	return 0;
+}
+EXPORT_SYMBOL_GPL(_allocate_contiguous_memory_nomap);
+
+unsigned long allocate_contiguous_memory_nomap(unsigned long size,
+	int mem_type, unsigned long align)
+{
+	return _allocate_contiguous_memory_nomap(size, mem_type, align,
+		__builtin_return_address(0));
+}
+EXPORT_SYMBOL_GPL(allocate_contiguous_memory_nomap);
+
+void free_contiguous_memory(void *addr)
+{
+	if (!addr)
+		return;
+	__free(addr, true);
+	return;
+}
+EXPORT_SYMBOL_GPL(free_contiguous_memory);
+
+void free_contiguous_memory_by_paddr(unsigned long paddr)
+{
+	if (!paddr)
+		return;
+	__free((void *)paddr, false);
+	return;
+}
+EXPORT_SYMBOL_GPL(free_contiguous_memory_by_paddr);
+
+unsigned long memory_pool_node_paddr(void *vaddr)
+{
+	struct alloc *node = find_alloc(vaddr);
+
+	if (!node)
+		return -EINVAL;
+
+	return node->paddr;
+}
+EXPORT_SYMBOL_GPL(memory_pool_node_paddr);
+
+unsigned long memory_pool_node_len(void *vaddr)
+{
+	struct alloc *node = find_alloc(vaddr);
+
+	if (!node)
+		return -EINVAL;
+
+	return node->len;
+}
+EXPORT_SYMBOL_GPL(memory_pool_node_len);
+
+static const struct file_operations mempool_operations = {
+	.owner		= THIS_MODULE,
+	.open           = mempool_open,
+	.read           = seq_read,
+	.llseek         = seq_lseek,
+	.release        = seq_release_private,
+};
+
+int __init memory_pool_init(void)
+{
+	int i;
+
+	alloc_root = RB_ROOT;
+	mutex_init(&alloc_mutex);
+	for (i = 0; i < ARRAY_SIZE(mpools); i++) {
+		mutex_init(&mpools[i].pool_mutex);
+		mpools[i].gpool = NULL;
+	}
+
+	return 0;
+}
+
+static int __init debugfs_mempool_init(void)
+{
+	struct dentry *entry, *dir = debugfs_create_dir("mempool", NULL);
+
+	if (!dir) {
+		pr_err("Cannot create /sys/kernel/debug/mempool");
+		return -EINVAL;
+	}
+
+	entry = debugfs_create_file("map", S_IRUSR, dir,
+		NULL, &mempool_operations);
+
+	if (!entry)
+		pr_err("Cannot create /sys/kernel/debug/mempool/map");
+
+	return entry ? 0 : -EINVAL;
+}
+
+module_init(debugfs_mempool_init);
-- 
1.7.5.4

