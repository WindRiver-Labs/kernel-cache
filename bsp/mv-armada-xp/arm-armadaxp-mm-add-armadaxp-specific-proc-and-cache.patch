From 6dce27a006376df4099f0dbf09265f39acfc0de9 Mon Sep 17 00:00:00 2001
From: Wang Hui <Hui.Wang@windriver.com>
Date: Tue, 15 Jan 2013 18:10:28 +0800
Subject: [PATCH 15/50] arm: armadaxp: mm: add armadaxp specific proc and cache operations

Marvell has a custom derivative ARM implementation off the  ARM v7a
ISA. As such, we need to add its specific operations for processor
core, for L1 and L2 cache.

The code is extracted from linux-3.2.27-axp_a370-2012_Q4.1, which
can be downloaded from:
https://extranet.marvell.com/extranet/dms/documents.do?groupID=4&\
subGroupID=53015

Signed-off-by: Wang Hui <Hui.Wang@windriver.com>
---
 arch/arm/include/asm/glue-proc.h     |   18 +
 arch/arm/mm/Kconfig                  |  350 +++++++++++++++++
 arch/arm/mm/Makefile                 |   18 +
 arch/arm/mm/cache-aurora-l2.c        |  712 ++++++++++++++++++++++++++++++++++
 arch/arm/mm/cache-v7.S               |    1 +
 arch/arm/mm/mmu.c                    |    7 +
 arch/arm/mm/proc-sheeva_pj4bv6.S     |  300 ++++++++++++++
 arch/arm/mm/proc-sheeva_pj4bv7.S     |  675 ++++++++++++++++++++++++++++++++
 arch/arm/mm/proc-sheeva_pj4bv7lpae.S |  491 +++++++++++++++++++++++
 arch/arm/mm/sheeva_pj4b-macros.S     |   72 ++++
 10 files changed, 2644 insertions(+), 0 deletions(-)
 create mode 100644 arch/arm/mm/cache-aurora-l2.c
 create mode 100644 arch/arm/mm/proc-sheeva_pj4bv6.S
 create mode 100644 arch/arm/mm/proc-sheeva_pj4bv7.S
 create mode 100644 arch/arm/mm/proc-sheeva_pj4bv7lpae.S
 create mode 100644 arch/arm/mm/sheeva_pj4b-macros.S

diff --git a/arch/arm/include/asm/glue-proc.h b/arch/arm/include/asm/glue-proc.h
index e2be7f1..30e9b89 100644
--- a/arch/arm/include/asm/glue-proc.h
+++ b/arch/arm/include/asm/glue-proc.h
@@ -248,6 +248,24 @@
 # endif
 #endif
 
+#ifdef CONFIG_CPU_SHEEVA_PJ4B_V6
+# ifdef CPU_NAME
+#  undef  MULTI_CPU
+#  define MULTI_CPU
+# else
+#  define CPU_NAME cpu_sheeva_pj4b_v6
+# endif
+#endif
+
+#ifdef CONFIG_CPU_SHEEVA_PJ4B_V7
+# ifdef CPU_NAME
+#  undef  MULTI_CPU
+#  define MULTI_CPU
+# else
+#  define CPU_NAME cpu_sheeva_pj4b_v7
+# endif
+#endif
+
 #ifndef MULTI_CPU
 #define cpu_proc_init			__glue(CPU_NAME,_proc_init)
 #define cpu_proc_fin			__glue(CPU_NAME,_proc_fin)
diff --git a/arch/arm/mm/Kconfig b/arch/arm/mm/Kconfig
index 38ab7fd..e879124 100644
--- a/arch/arm/mm/Kconfig
+++ b/arch/arm/mm/Kconfig
@@ -382,6 +382,23 @@ config CPU_FEROCEON_OLD_ID
 	  for which the CPU ID is equal to the ARM926 ID.
 	  Relevant for Feroceon-1850 and early Feroceon-2850.
 
+choice
+	prompt "Marvell Sheeva CPU Architecture"
+	default CPU_SHEEVA_PJ4B_V7
+
+config CPU_SHEEVA_PJ4B_V6
+	bool "Support Sheeva processor in V6 mode" if ARCH_ARMADA_XP
+	select CPU_V6
+	select CPU_32v6K
+	select CPU_V6K
+	select DMA_CACHE_RWFO if SMP
+
+config CPU_SHEEVA_PJ4B_V7
+	bool "Support Sheeva processor in V7 mode" if ARCH_ARMADA_XP || ARCH_ARMADA370
+	select CPU_V7
+
+endchoice
+
 # Marvell PJ4
 config CPU_PJ4
 	bool
@@ -817,6 +834,14 @@ config DMA_CACHE_RWFO
 	  in hardware, other workarounds are needed (e.g. cache
 	  maintenance broadcasting in software via FIQ).
 
+config SMP_ENABLE_LAZY_FLUSH
+	bool "Enable lazy flush for v6 smp"
+	depends on CPU_V6 && SMP
+	default y
+	help
+	  Enable the lazy flush when calling flush_dcache_page() for smp
+	  v6 mode where cache maintenance breadcasting done in sotfware.
+
 config OUTER_CACHE
 	bool
 
@@ -855,6 +880,331 @@ config MIGHT_HAVE_CACHE_L2X0
 	  instead of this option, thus preventing the user from
 	  inadvertently configuring a broken kernel.
 
+config SHEEVA_ERRATA_ARM_CPU_4742
+	bool "Sheeva Errata 4742: Enable sync barriers after WFI idle"
+	depends on CPU_SHEEVA_PJ4B_V6 || CPU_SHEEVA_PJ4B_V7
+	help
+	  When coming out of WFI IDLE state, a specific timing sensitivity exists
+	  between the retiring WFI instruction and the newly issued subsequent
+	  instructions. This sensitivity can result in a CPU hang scenario.
+	  WA: The software must insert either a Data Synchronization Barrier (DSB)
+	  or Data Memory Barrier (DMB) command immediately after the WFI instruction.
+
+config SHEEVA_ERRATA_ARM_CPU_4786
+	bool "Sheeva Errata 4786: Disable coprocessor dual issue mode"
+	depends on (CPU_SHEEVA_PJ4B_V6 || CPU_SHEEVA_PJ4B_V7) && ARM_THUMB && VFP
+	help
+	  If the last instruction inside a Thumb IT block is a branch, and the
+	  following instruction is a VFP instruction, the logic may incorrectly
+	  dual-issue the VFP instruction along with the branch instruction. This
+	  causes the VFP instruction to be executed, even though the branch
+	  instruction may be taken.
+	  WA: Set the CP15 coprocessor dual-issue disable bit in the Auxiliary
+	  Debug Modes Control 0 register (bit[15]). This setting disables the
+	  dual-issuing of VFP instructions before entering an IT block.
+
+config SHEEVA_ERRATA_ARM_CPU_5315
+	bool "Sheeva Errata 5315: Disable Data Speculative prefetch from MBU/LSU"
+	depends on CPU_SHEEVA_PJ4B_V6 || CPU_SHEEVA_PJ4B_V7
+	default n
+	help
+	  When a PLD instruction is used as a memory hint, using a fast load
+	  bypass (load data is used as the address for a subsequent memory access)
+	  can result in data corruption.
+	  WA: Do not operate in Speculative mode
+
+config SHEEVA_ERRATA_ARM_CPU_4413
+	bool "Sheeva Errata 4413: Add SB before L1 Invalidate by MVA"
+	depends on CPU_SHEEVA_PJ4B_V6 || CPU_SHEEVA_PJ4B_V7
+	help
+	  Potentially, invalidates by a Modified Virtual Address (MVA) entry can
+	  cause lines to fill into the L1 cache in an "intermediate" state without
+	  the lines being subsequently updated to a valid state. As a result, a
+	  subsequent "multi-hit" scenario and potential data corruption can occur
+	  when a future operation allocates the same address to a separate line.
+	  WA: The Invalidate by MVA operation (or sequence of MVA operations) must
+	  be preceded by a barrier to ensure that the preceding pre-condition for
+	  the cache miss is completed.
+
+config SHEEVA_ERRATA_ARM_CPU_4659
+	bool "Sheeva Errata 4659: Add ISB following L1 I$ Invalidate by MVA"
+	depends on CPU_SHEEVA_PJ4B_V6 || CPU_SHEEVA_PJ4B_V7
+	help
+	  I-Cache maintenance by using an MVA command can cause other instructions
+	  in I-Fetch to be executed twice.
+	  WA: Follow I-Cache maintenance by an MVA command with an Instruction
+	  Synchronization Barrier (ISB) instruction. This instruction flushes the
+	  pipeline after the maintenance operation, deleting any double-instructions.
+
+config SHEEVA_ERRATA_ARM_CPU_5114
+	bool "Sheeva Errata 5114: Force all MMU pages to be Shared"
+	depends on CPU_SHEEVA_PJ4B_V6 && AURORA_IO_CACHE_COHERENCY
+	help
+	  When a non-shared line fill request causes a shared cacheable eviction
+	  in close proximity to an incoming snoop to the victim line, the incoming
+	  snoop can incorrectly miss and result in data corruption.
+	  WA: When using cacheable shared memory, set all memory pages/descriptors
+	  to be shared in the Translation Table Base Register 0
+
+config SHEEVA_ERRATA_ARM_CPU_4611
+	bool "Sheeva Errata 4611: Preceed every L1 Clean operation with DSB"
+	depends on CPU_SHEEVA_PJ4B_V6 || CPU_SHEEVA_PJ4B_V7
+	help
+	  A CP15 clean operation can result in a dead lock state if it is hit by
+	  an incoming snoop evento.
+	  WA: Before any CP15 clean type operation in Cache Coherency mode, issue
+	  a Data Memory Barrier (DMB) or a Data Synchronization Barrier (DSB)
+	  instruction.
+
+config SHEEVA_ERRATA_ARM_CPU_BTS61
+	bool "Sheeva Errata BTS61: Disable WFI and WFE instructions in SMP or Coherent systems"
+	depends on (CPU_SHEEVA_PJ4B_V6 || CPU_SHEEVA_PJ4B_V7) && (SMP || AURORA_IO_CACHE_COHERENCY)
+	help
+	  When the CPU is set to WFI mode in the system with a Snoop command, a
+	  snoop response may not occur and that will cause a system hang.
+	  WA: Do not use the WFI mode in SMP/coherent systems.
+
+config SHEEVA_ERRATA_ARM_CPU_4948
+	bool "Sheeva Errata 4948: Disable L0 cache"
+	depends on CPU_SHEEVA_PJ4B_V6 || CPU_SHEEVA_PJ4B_V7
+	default n
+	help
+	  In a very rare case, the core can fail to observe the correct data
+	  as follows:
+	    1. After a first load, a second load operation to the same address
+	       occurs, and an invalidate snoop to the same line is arbitrated
+	       between both reads.
+	    2. The core may observe the old data for the second read, while the
+	       first load got the new data laying in this address.
+	    3. A third load to the same address will see the new data.
+	  WA: This event can occur when the first load access crosses a cache
+	  line boundary (i.e., unaligned access). If it does occur, L0 data
+	  cache can be disabled
+
+config SHEEVA_ERRATA_ARM_CPU_6409
+
+        bool "Sheeva Errata 6409: Processor may execute incorrect instructions when two ARM-mode conditional branches are back-toback and double-word-aligned"
+        depends on  CPU_SHEEVA_PJ4B_V7 && (ARMADA_XP_REV_A0 || ARMADA_XP_A0_WITH_B0)
+        default y
+        help
+	In a certain rare case the processor will evaluate a branch incorrectly leading to incorrect execution of instructions.
+	The bug occurs when the static predictor is incorrectly applied to a branch instruction when a dynamic predictor
+	predicts that a branch should not be taken
+	Workaround:
+		To avoid this bug with minimal performance impact, the static branch predictor can be disabled by writing a "0" to bit[2]
+		of the Auxiliary Debug Modes Control Register as follows:
+		mrc p15, 1, r0, c15, c1, 1
+		bic r0, r0, 0x4; disable static prediction
+		mcr p15, 1, r0, c15, c1, 1
+
+config SHEEVA_ERRATA_ARM_CPU_5980
+
+        bool "Sheeva Errata 5980: Ordering Issues associated with Streaming device write"
+        depends on  CPU_SHEEVA_PJ4B_V7 && (ARMADA_XP_REV_A0 || ARMADA_XP_A0_WITH_B0)
+	default y
+        help
+	A stream of device writes are performed with the same AXI ID, followed by a write-back, write-allocate cacheable
+	exclusive write that has the same AXI ID.  There is an issue with the System L2 AXI logic that allows the response of
+	the exclusive write to return with EXOK prior to the response of all of the devices.
+	WA:
+		Disable the device write streaming capability.  This can be done by:
+		;; use read/modify/write
+		;; set bit 29 to .1. (default is 0)
+		MCR p15, 1, <Rd>, c15, c1,2
+
+config SHEEVA_ERRATA_ARM_CPU_6043
+
+        bool "Sheeva Errata 6043: clean operations can cause victim data to be written out of order"
+        depends on  CPU_SHEEVA_PJ4B_V7 && (ARMADA_XP_REV_A0 || ARMADA_XP_A0_WITH_B0)
+        default y
+        help
+		Cache maintenance clean operations leave the L1 in a "right to modify" state.  If a clean operation is stalled due to the
+		memory buffers being full, another store transaction to the same address in L1 can occur, leaving the line in a dirty
+		state.  At this time, if a separate request causes this newly modified line to get evicted, there is a scenario where the
+		more recently modified data can be written out to memory ahead of the original cache maintenance clean operation's
+		data, resulting in data corruption.
+		Workaround:
+		 Note the failure is very rare, and more likely to occur when multiple ways of the L1 cache are locked, increasing the
+		victim traffic.
+		One possible workaround is to replace "Clean" with "Clean & Invalidate" operations.
+		Alternatively, if a DMB or DSB instruction is issued after completing a group of clean operations (before executing any
+		stores to the cleaned lines), the issue also cannot occur.
+
+config SHEEVA_ERRATA_ARM_CPU_6075
+
+        bool "Sheeva Errata 6075: Barriers fail to enforce ordering among pended loads to non-cacheable memory"
+        depends on  CPU_SHEEVA_PJ4B_V7 && (ARMADA_XP_REV_A0 || ARMADA_XP_A0_WITH_B0)
+        default y
+        help
+		When a DMB or DSB is used, the architecture requires that the barrier order explicit memory accesses from before the
+		barrier to ones after the barrier.  Because non-cacheable accesses are permitted to pend to the memory buffers, there
+		exists a rare scenario where the prescribed ordering is not honored.  Consider the following multiprocessor example:
+		Processor 0:
+		STR Data
+		DMB
+		STR Flag
+		Processor 1:
+		LDR Unrelated
+		LDR Flag
+		DMB
+		LDR Data ; Data & Unrelated in same non-cacheable chunk
+		In the above scenario, the LDR Data happens to pend and return the stale (prior to Flag update by Processor 0)
+		version of data.
+		Workaround
+		This issue can be avoided by replacing a DMB with a DSB SYS.
+
+config SHEEVA_ERRATA_ARM_CPU_6076
+
+        bool "Sheeva Errata 6076: Multiple writeback entries hit by snoops can result in data corruption"
+        depends on  CPU_SHEEVA_PJ4B_V7 && (ARMADA_XP_REV_A0 || ARMADA_XP_A0_WITH_B0)
+        default y
+        help
+		In a rare case where a L1 cache eviction (either a WriteEvict or WriteBack) is allocated concurrently with a previous
+		WriteClean request to the same address (as the result of a cache maintenance clean operation), both sets of victim
+		data can be hit by an incoming snoop.  Under a specific scenario, this may result in the WriteBack actually being
+		considered complete.  A future fetch to memory can then potentially read stale data prior to the writeback occurring.
+		Workaround
+		Replacing all clean cache maintenance operations with clean & invalidate will avoid the issue
+		NOTE: This errata is rare and was hit in a scenario where 7/8 ways of the L1 cache were locked to force additional L1
+		cache evictions to the same address.  If L1 locking is not used, this bug is unlikely to occur.
+		Fix
+		-
+config SHEEVA_ERRATA_ARM_CPU_6136
+
+        bool "Sheeva Errata 6136: Base value gets corrupted after a page-crossing, boundary-crossing Load-Store-Multiple (LDSTM)"
+        depends on  CPU_SHEEVA_PJ4B_V7 && (ARMADA_XP_REV_A0 || ARMADA_XP_A0_WITH_B0)
+        default y
+        help
+	The scenario required to hit the bug starts with an LDM which is a cross-boundary, cross-page address.  The first part
+	hits in the cache, while the second part suffers an MMU fault.  Under this scenario, the source register does not get
+	restored after the fault occurs, causing data corruption.  NOTE: This can only happen on an LDM whose source is also
+	one of the destinations.  This can also only happen if the LDM has two destination registers in the register list.
+	Workaround
+	Set the "ldstm_first Two Sgl" bit[12] in the Auxiliary Debug Modes Control 0 Register.  This effectively forces the first
+	issue of a LDSTM to be a single word, avoiding the buggy condition.
+
+config SHEEVA_ERRATA_ARM_CPU_6124
+
+        bool "Sheeva Errata 6124: Clean operation might cause data corruption"
+        depends on  CPU_SHEEVA_PJ4B_V7 && (ARMADA_XP_REV_B0 || ARMADA_XP_A0_WITH_B0)
+        default y
+        help
+		Clean operation might cause data corruption
+		Workaround:
+		Replace "Clean" with "Clean & Invalidate" operations.
+
+config SHEEVA_ERRATA_ARM_CPU_PMU_RESET
+	bool "Sheeva Errata CPU Performance counters reset"
+	depends on CPU_SHEEVA_PJ4B_V6 || CPU_SHEEVA_PJ4B_V7
+	help
+	  CPU Performance counters are not reset by writing '1' to Performance
+	  Monitor Control register bit 2 as expected.
+	  WA: Write 0 and then 1 to bit 2 in Performance Monitor Control register.
+
+config SHEEVA_ERRATA_ARM_CPU_ADD_DELAY_FOR_STOP_MACHINE
+	bool "stop_machine function can livelock"
+	depends on CPU_V6 && SMP
+	default y
+	help
+	  add delay when polling the stop_machine state.
+
+config SHEEVA_ERRATA_ARM_CPU_ADD_DELAY_FOR_STREX
+	bool "Spinlocks using LDREX and STREX instructions can livelock"
+	depends on CPU_V6 && SMP
+	default n
+	help
+	  add delay after strex.
+
+config SHEEVA_DEEP_IDLE
+	bool "Enable CPU/L2 Deep Idle Power Management"
+	depends on (ARCH_ARMADA_XP || ARCH_ARMADA370) && CPU_IDLE
+
+config CPU_FREQ_ARMADA_XP
+	bool "Enable CPU frequency scaling"
+	depends on ARCH_ARMADA_XP && CPU_FREQ
+
+config STANDBY_UART_WAKE
+	bool "Enable wake up from standby by UART"
+	depends on ARCH_ARMADA_XP && CPU_IDLE && HOTPLUG_CPU
+
+config ARMADA_SUPPORT_DEEP_IDLE_DRAM_SR
+	bool "Support DDR Self-Refresh in Deep-Idle"
+	default n
+	depends on SHEEVA_DEEP_IDLE
+
+config ARMADA_SUPPORT_DEEP_IDLE_FAST_EXIT
+	bool "Enable Fast Exit from Deep-Idle"
+	default y
+	depends on SHEEVA_DEEP_IDLE
+	help
+	  Enable fast exit from Deep-Idle by using a reserved block in crypto engine SRAM.
+
+config CACHE_AURORA_L2
+	bool "Enable Marvell Aurora L2 cache controller"
+	depends on ARCH_ARMADA_XP || ARCH_ARMADA370
+	default y
+    select OUTER_CACHE if ARCH_ARMADA370
+	help
+	  This option enables the Marvell Aurora L2 cache controller.
+
+config AURORA_L2_PT_WALK
+	bool "Enable Marvell page table walk in L2 to improve performance"
+	depends on CACHE_AURORA_L2
+	default y
+	help
+	  This option enables PTE caching in L2 to improve performance.
+
+config AURORA_L2_OUTER
+	bool
+	depends on CACHE_AURORA_L2 && (CPU_SHEEVA_PJ4B_V6 || ARMADA_XP_REV_Z1)
+	default y
+	select OUTER_CACHE
+	help
+	  This option enables all outer cache operations in V6 mode.
+
+config AURORA_L2_OUTER_WA
+        bool "Enable outer cache (L2) WriteAllocate mode as inner cache (L1)"
+        depends on CACHE_AURORA_L2
+        default n
+        help
+          This option enables outer cache (L2) write-allocate in NMRRR mode 7.
+          If disabled, the L2 cache will be WB no WA.
+
+config AURORA_SF_ENABLED
+        bool "Enable Marvell Aurora Snoop Filter "
+        depends on CPU_SHEEVA_PJ4B_V7 && (ARMADA_XP_REV_A0 || ARMADA_XP_A0_WITH_B0 || ARMADA_XP_REV_B0) && SMP
+        default y
+        help
+          This option enables Snoop Filter feature.
+
+config ENABLE_UNALINGED_ACCESS_FAULT
+	bool "Enable S/W handling for Unaligned Access"
+	default n
+	help
+	  This flag enables S/W handling of unaligned access
+
+config AURORA_IO_CACHE_COHERENCY
+	bool "Enable Marvell Aurora I/O cache coherency"
+	depends on ARCH_ARMADA_XP || ARCH_ARMADA370
+	default y
+	help
+	  This option enables the hardware mechanism for I/O cache coherency.
+
+config CPU_SHEEVA_PJ4B_PMC_ACCESS_IN_USERMODE
+	bool "Enabled User mode access for PMC"
+	depends on CPU_SHEEVA_PJ4B_V6 || CPU_SHEEVA_PJ4B_V7
+	default n
+	help
+	  Say Y if you allow user mode application to access Performance
+	  Monitor Counter of PJ4 in user mode
+
+config MV_SUPPORT_L2_DEPOSIT
+	bool "Support L2 Deposit"
+	depends on ARCH_ARMADA_XP
+	default n
+	help
+	  This option enables L2 deposit.
+
 config CACHE_L2X0
 	bool "Enable the L2x0 outer cache controller" if MIGHT_HAVE_CACHE_L2X0
 	default MIGHT_HAVE_CACHE_L2X0
diff --git a/arch/arm/mm/Makefile b/arch/arm/mm/Makefile
index d223854..6213f00 100644
--- a/arch/arm/mm/Makefile
+++ b/arch/arm/mm/Makefile
@@ -2,6 +2,10 @@
 # Makefile for the linux arm-specific parts of the memory manager.
 #
 
+ifeq ($(CONFIG_ARCH_ARMADA_XP),y)
+include $(srctree)/arch/arm/mach-armadaxp/config/mvRules.mk
+endif
+
 obj-y				:= dma-mapping.o extable.o fault.o init.o \
 				   iomap.o
 
@@ -90,14 +94,28 @@ obj-$(CONFIG_CPU_XSCALE)	+= proc-xscale.o
 obj-$(CONFIG_CPU_XSC3)		+= proc-xsc3.o
 obj-$(CONFIG_CPU_MOHAWK)	+= proc-mohawk.o
 obj-$(CONFIG_CPU_FEROCEON)	+= proc-feroceon.o
+ifneq ($(CONFIG_ARCH_ARMADA_XP),y)
 obj-$(CONFIG_CPU_V6)		+= proc-v6.o
 obj-$(CONFIG_CPU_V6K)		+= proc-v6.o
 obj-$(CONFIG_CPU_V7)		+= proc-v7.o
+else
+obj-$(CONFIG_CPU_SHEEVA_PJ4B_V6)	+= proc-sheeva_pj4bv6.o
+ifeq ($(CONFIG_ARM_LPAE),y)
+obj-$(CONFIG_CPU_SHEEVA_PJ4B_V7)	+= proc-sheeva_pj4bv7lpae.o
+else
+obj-$(CONFIG_CPU_SHEEVA_PJ4B_V7)	+= proc-sheeva_pj4bv7.o
+endif
+endif
 
 AFLAGS_proc-v6.o	:=-Wa,-march=armv6
 AFLAGS_proc-v7.o	:=-Wa,-march=armv7-a
 
+AFLAGS_proc-sheeva_pj4bv6.o        :=-Wa,-march=armv6
+AFLAGS_proc-sheeva_pj4bv7.o        :=-Wa,-march=armv7-a
+AFLAGS_proc-sheeva_pj4bv7lpae.o    :=-Wa,-march=armv7-a
+
 obj-$(CONFIG_CACHE_FEROCEON_L2)	+= cache-feroceon-l2.o
 obj-$(CONFIG_CACHE_L2X0)	+= cache-l2x0.o
 obj-$(CONFIG_CACHE_XSC3L2)	+= cache-xsc3l2.o
 obj-$(CONFIG_CACHE_TAUROS2)	+= cache-tauros2.o
+obj-$(CONFIG_CACHE_AURORA_L2)	+= cache-aurora-l2.o
diff --git a/arch/arm/mm/cache-aurora-l2.c b/arch/arm/mm/cache-aurora-l2.c
new file mode 100644
index 0000000..515cfad
--- /dev/null
+++ b/arch/arm/mm/cache-aurora-l2.c
@@ -0,0 +1,712 @@
+/*
+ * arch/arm/mm/cache-aurora-l2.c - AURORA shared L2 cache controller support
+ *
+ * Copyright (C) 2008 Marvell Semiconductor
+ *
+ * This file is licensed under the terms of the GNU General Public
+ * License version 2.  This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ *
+ * References:
+ * - Unified Shared Layer 2 Cache for Armada CP SoC devices,
+ *   Document ID MV-S104858-00, Rev. A, October 23 2007.
+ */
+
+#include <linux/init.h>
+#include <linux/delay.h>
+#include <asm/cacheflush.h>
+#include <asm/cp15.h>
+#include <linux/proc_fs.h>
+
+#include <plat/cache-aurora-l2.h>
+#include <asm/io.h>
+#include <mach/smp.h>
+#include "ctrlEnv/mvCtrlEnvSpec.h"
+#include "ctrlEnv/mvCtrlEnvLib.h"
+#include "ctrlEnv/mvSemaphore.h"
+
+/*
+ * L2 registers offsets
+ */
+#define L2_CONTROL		0x100
+#define L2_AUX_CONTROL		0x104
+#define L2_SYNC			0x700
+#define L2_RANGE_BASE		0x710
+#define L2_INVALIDATE_PA	0x770
+#define L2_INVALIDATE_RANGE	0x774
+#define L2_CLEAN_PA		0x7B0
+#define L2_CLEAN_RANGE		0x7B4
+#define L2_FLUSH_PA		0x7F0
+#define L2_FLUSH_RANGE		0x7F4
+#define L2_LOCKDOWN_DATA	0x900
+#define L2_LOCKDOWN_INSTR	0x904
+#define L2_LOCKDOWN_FPU		0x980
+#define L2_LOCKDOWN_IO_BRG	0x984
+
+static unsigned int l2rep = L2ACR_REPLACEMENT_TYPE_SEMIPLRU;
+static int __init l2rep_setup(char *str)
+{
+	if (!strcmp(str, "WayRR"))
+		l2rep = L2ACR_REPLACEMENT_TYPE_WAYRR;
+	else if (!strcmp(str, "LFSR"))
+		l2rep = L2ACR_REPLACEMENT_TYPE_LFSR;
+	else if (!strcmp(str, "pLRU"))
+		l2rep = L2ACR_REPLACEMENT_TYPE_SEMIPLRU;
+	else
+		return 0;
+	return 1;
+}
+__setup("l2rep=", l2rep_setup);
+
+void __iomem *auroraL2_base = NULL;
+bool auroraL2_enable = 0;
+#ifdef CONFIG_PROC_FS
+static unsigned char *replacement[] = {"WayRR",
+				       "LFSR",
+				       "semi pLRU",
+				       "reserved",
+};
+static unsigned char *associativity[] ={"reserved",
+					"reserved", 
+					"reserved", 
+					"4-way", 
+					"reserved", 
+					"reserved", 
+					"reserved", 
+					"8-way",
+					"reserved", 
+					"reserved", 
+					"reserved", 
+					"12-way", 
+					"reserved", 
+					"reserved", 
+					"reserved", 
+					"16-way", 
+					};
+static unsigned char *wsize[]={   "reserved(16KB)", 
+                                  "16KB",
+                                  "32KB",
+                                  "64KB",
+                                  "128KB",
+                                  "256KB",
+                                  "512KB",
+                                  "reserved(512KB)"
+                              };
+static unsigned char *wa_mode[] = {"Requester Attribute", 
+                                  "force no write allocate",
+                                  "force write allocate",
+                                  "reserved"
+                              	};
+static unsigned char *wbwt_mode[] = {"PageAttribute", 
+                                  "force WB",
+                                  "force WT",
+                                  "reserved"
+                              	};
+
+
+
+static int proc_auroraL2_info_read(char *page, char **start, off_t off, int count, int *eof,
+		    void *data)
+{
+	char *p = page;
+	int len;
+    	__u32 aux;
+
+	p += sprintf(p, "AuroraL2 Information:\n");
+			 
+	aux = readl(auroraL2_base + L2_AUX_CTRL_REG);		 
+	p += sprintf(p, "Replacement   : %s\n", replacement[(aux >> L2ACR_REPLACEMENT_OFFSET) & 0x3]);
+	p += sprintf(p, "Associativity : %s\n", associativity[(aux >> 13) & 0xf]);
+	p += sprintf(p, "Way size      : %s\n", wsize[(aux >> 17) & 0xF]);
+	p += sprintf(p, "Data ECC      : %s\n", ((aux >> 20) & 0x1) ? "Enabled" : "Disabled");
+	p += sprintf(p, "TAG parity    : %s\n", ((aux >> 21) & 0x1) ? "Enabled" : "Disabled");
+ 	p += sprintf(p, "Write mode forcing     : %s\n", wbwt_mode[(aux >> 0) & 0x3]);
+	p += sprintf(p, "Write allocate forcing : %s\n", wa_mode[(aux >> 23) & 0x3]);
+   
+	len = (p - page) - off;
+	if (len < 0)
+		len = 0;
+
+	*eof = (len <= count) ? 1 : 0;
+	*start = page + off;
+
+	return len;
+}
+
+#ifdef CONFIG_CACHE_AURORAL2_EVENT_MONITOR_ENABLE
+static unsigned int last_counter[2][2] = {{0,0},{0,0}};
+
+static unsigned char *event_name[]= {    
+                                        "Counter Disabled", 
+                                        "CastOut",
+                                        "DataRdHit",
+                                        "DataRdReq",
+                                        "DataWrHit",
+                                        "DataWrReq",
+                                        "DataWTReq",
+                                        "InstRdHit",
+                                        "InstRdReq",
+                                        "MmuRdHit",
+                                        "MmuRdReq",
+                                        "WriteWAMiss",
+                                        "WriteWACLReq",
+                                        "WriteWANoCLReq",
+                                        "SRAMWr",
+                                        "SRAMrRd",
+                                        "RMWWrite"
+                                        "SpeculativeInstReq"
+                                        "SpeculativeInstHit"
+                                        "RGFStall"
+                                        "EBStall"
+                                        "LRBStall"
+                                        "Idle"
+                                        "Active"
+                                    };
+static int proc_auroraL2_counter_read(char *page, char **start, off_t off, int count, int *eof,
+		    void *data)
+{
+	char *p = page;
+	int len, i, cfg;
+    	unsigned int counter[2][2], delta_counter[2][2];
+
+	p += sprintf(p, "AuroraL2 Event Counter Information:\n\n");			 
+	p += sprintf(p, "L2_CNTR_CTRL_REG    : %#08x\n", readl(auroraL2_base + L2_CNTR_CTRL_REG));
+
+	for (i = 0; i < L2_MAX_COUNTERS; i++){
+		cfg = readl(auroraL2_base + L2_CNTR_CONFIG_REG(i));
+		p += sprintf(p, "L2_CNTR%d_CONFIG_REG : %#08x[%s]\n", cfg, event_name[(cfg >> 2) & 0x3F]);
+	     	counter[i][0] = readl(auroraL2_base + L2_CNTR_VAL_LOW_REG(i));
+    		counter[i][1] = readl(auroraL2_base + L2_CNTR_VAL_HIGH_REG(i));
+        	delta_counter[i][0] = counter[i][0] - last_counter[i][0];
+        	delta_counter[i][1] = counter[i][1] - last_counter[i][1];
+	}
+    
+    	p += sprintf(p, "\n=========================================================================\n");
+    	p += sprintf(p, "currnet counter 0 1: %12u%12u     %12u%12u     %12u%12u\n",  counter[0][1]  counter[0][0],  counter[1][1], counter[1][0]);
+    	p += sprintf(p, "delta   counter 0 1: %12u%12u     %12u%12u     %12u%12u\n",  delta_counter[0][1],  delta_counter[0][0],  delta_counter[1][1], delta_counter[1][0]);
+
+	len = (p - page) - off;
+	if (len < 0)
+		len = 0;
+
+	*eof = (len <= count) ? 1 : 0;
+	*start = page + off;
+
+    	memcpy((unsigned char *)last_counter, (unsigned char *)counter, sizeof(last_counter));
+    
+	return len;
+}
+
+
+static int proc_auroraL2_counter_write(struct file *file, const char __user *buffer,
+				unsigned long count, void *data)
+{
+    u8 param[3][32];
+    u32 configs[3] = {0};
+    u8 *buffer_tmp = kmalloc(count+16, GFP_KERNEL);
+    int i, cfg;
+
+    memset(buffer_tmp, 0x0, sizeof(buffer_tmp));
+
+    if(copy_from_user(buffer_tmp, buffer, count))
+    {
+        if (buffer_tmp)        
+            kfree(buffer_tmp);
+        return -EFAULT;    
+    }
+
+    sscanf(buffer_tmp, "%s %s %s %s\n",  param[0], param[1], param[2]);
+
+    if (strcmp(param[0], "reset") == 0)
+    {
+	for (i = 0; i < L2_MAX_COUNTERS; i++){
+		/* Stop counters */
+		cfg = readl(auroraL2_base + L2_CNTR_CONFIG_REG(i));
+		cfg &= ~(0x3F << 2)
+        	writel(cfg, auroraL2_base + L2_CNTR_CONFIG_REG(i)); 
+	}
+        writel(0x101, auroraL2_base + L2_CNTR_CTRL_REG); /* reset counter values */
+
+        memset((unsigned char *)last_counter, 0, sizeof(last_counter));         
+        
+        goto out;
+    }
+
+    for (i = 0; i < 3; i++)
+        configs[i] = simple_strtoul(param[i], NULL, 0);             
+
+    writel(configs[0], auroraL2_base + L2_CNTR_CTRL_REG); 
+    writel(configs[1], auroraL2_base + L2_CNTR_CONFIG_REG(0)); 
+    writel(configs[2], auroraL2_base + L2_CNTR_CONFIG_REG(1)); 	 
+
+out:
+   
+    if (buffer_tmp)        
+        kfree(buffer_tmp);
+    
+	return count;
+}
+#endif /* CONFIG_CACHE_TAUROS3_EVENT_MONITOR_ENABLE */
+#endif /* CONFIG_PROC_FS */
+
+#define CACHE_LINE_SIZE		32
+#define MAX_RANGE_SIZE		1024
+#define RANGE_OP
+
+static int l2_wt_override = 0;
+static DEFINE_SPINLOCK(smp_l2cache_lock);
+
+/*
+ * Low-level cache maintenance operations.
+ *
+ *
+ * Cache range operations are initiated by writing the start and
+ * end addresses to successive cp15 registers, and process every
+ * cache line whose first byte address lies in the inclusive range
+ * [start:end-1] (the end address is inclusive).
+ *
+ * The cache range operations stall the CPU pipeline until completion.
+ *
+ * The range operations require two successive cp15 writes, in
+ * between which we don't want to be preempted.
+ */
+
+static inline void cache_sync(void)
+{    
+    writel(0, auroraL2_base+L2_SYNC);  /* flush L2 write buffer (barrier) */ 
+}
+
+#ifdef CONFIG_AURORA_L2_OUTER
+inline void l2_clean_pa(unsigned int addr)
+{
+    	if (!auroraL2_enable)
+        	return;        
+
+	writel(addr & ~0x1f, auroraL2_base+L2_CLEAN_PA);
+	cache_sync();
+}
+
+static inline void l2_inv_pa(unsigned long addr)
+{
+	if (!auroraL2_enable)
+        	return;        
+
+	writel(addr & ~0x1f, auroraL2_base+L2_INVALIDATE_PA);
+}
+
+static inline void l2_clean_inv_pa(unsigned long addr)
+{
+	if (!auroraL2_enable)
+        	return;        
+
+	writel(addr & ~0x1f, auroraL2_base+L2_FLUSH_PA);
+	cache_sync();
+}
+
+void l2_clean_va(unsigned int addr)
+{
+	l2_clean_pa(__pa(addr));
+}
+
+static inline void l2_clean_pa_range(unsigned long start, unsigned long end)
+{
+	unsigned long flags;
+
+	/*
+	 * Make sure 'start' and 'end' reference the same page, as
+	 * L2 is PIPT and range operations only do a TLB lookup on
+	 * the start address.
+	 */
+	BUG_ON((start ^ end) & ~(PAGE_SIZE - 1));
+#ifdef RANGE_OP
+	spin_lock_irqsave(&smp_l2cache_lock, flags);
+#ifdef CONFIG_SMP
+	writel(start, auroraL2_base+L2_RANGE_BASE + (4 * hard_smp_processor_id()));
+#else	
+	writel(start, auroraL2_base+L2_RANGE_BASE);
+#endif
+	writel(end, auroraL2_base+L2_CLEAN_RANGE);
+	spin_unlock_irqrestore(&smp_l2cache_lock, flags);
+#else
+	for(; start <= end; start += CACHE_LINE_SIZE)
+		writel(start, auroraL2_base+L2_CLEAN_PA);
+#endif
+	cache_sync();
+}
+
+static inline void l2_flush_pa_range(unsigned long start, unsigned long end)
+{
+	unsigned long flags;
+
+	/*
+	 * Make sure 'start' and 'end' reference the same page, as
+	 * L2 is PIPT and range operations only do a TLB lookup on
+	 * the start address.
+	 */
+	BUG_ON((start ^ end) & ~(PAGE_SIZE - 1));
+#ifdef RANGE_OP
+	spin_lock_irqsave(&smp_l2cache_lock, flags);
+#ifdef CONFIG_SMP
+	writel(start, auroraL2_base+L2_RANGE_BASE + (4 * hard_smp_processor_id()));
+#else
+	writel(start, auroraL2_base+L2_RANGE_BASE);
+#endif
+	writel(end, auroraL2_base+L2_FLUSH_RANGE);
+	spin_unlock_irqrestore(&smp_l2cache_lock, flags);
+#else
+	for ( ; start <= end; start += CACHE_LINE_SIZE)
+		writel(start, auroraL2_base+L2_FLUSH_PA);
+#endif
+	cache_sync();
+}
+
+static inline void l2_inv_pa_range(unsigned long start, unsigned long end)
+{
+	unsigned long flags;
+
+	/*
+	 * Make sure 'start' and 'end' reference the same page, as
+	 * L2 is PIPT and range operations only do a TLB lookup on
+	 * the start address.
+	 */
+	BUG_ON((start ^ end) & ~(PAGE_SIZE - 1));
+#ifdef RANGE_OP
+	spin_lock_irqsave(&smp_l2cache_lock, flags);
+#ifdef CONFIG_SMP
+	writel(start, auroraL2_base+L2_RANGE_BASE + (4 * hard_smp_processor_id()));
+#else	
+	writel(start, auroraL2_base+L2_RANGE_BASE);
+#endif
+	writel(end, auroraL2_base+L2_INVALIDATE_RANGE);
+	spin_unlock_irqrestore(&smp_l2cache_lock, flags);
+#else
+	for(; start <= end; start += CACHE_LINE_SIZE)
+		writel(start, auroraL2_base+L2_INVALIDATE_PA);
+#endif
+
+	cache_sync();
+}
+
+
+/*
+ * Linux primitives.
+ *
+ * Note that the end addresses passed to Linux primitives are
+ * noninclusive, while the hardware cache range operations use
+ * inclusive start and end addresses.
+ */
+
+static inline unsigned long calc_range_end(unsigned long start, unsigned long end)
+{
+	unsigned long range_end;
+
+	BUG_ON(start & (CACHE_LINE_SIZE - 1));
+	BUG_ON(end & (CACHE_LINE_SIZE - 1));
+
+	/*
+	 * Try to process all cache lines between 'start' and 'end'.
+	 */
+	range_end = end;
+
+	/*
+	 * Limit the number of cache lines processed at once,
+	 * since cache range operations stall the CPU pipeline
+	 * until completion.
+	 */
+	if (range_end > start + MAX_RANGE_SIZE)
+		range_end = start + MAX_RANGE_SIZE;
+
+	/*
+	 * Cache range operations can't straddle a page boundary.
+	 */
+	if (range_end > (start | (PAGE_SIZE - 1)) + 1)
+		range_end = (start | (PAGE_SIZE - 1)) + 1;
+
+	return range_end;
+}
+
+static void aurora_l2_inv_range(unsigned long start, unsigned long end)
+{
+    	if (!auroraL2_enable)
+        	return;        
+	/*
+	 * Clean and invalidate partial first cache line.
+	 */
+	if (start & (CACHE_LINE_SIZE - 1)) {
+		l2_clean_inv_pa(start & ~(CACHE_LINE_SIZE - 1));
+		start = (start | (CACHE_LINE_SIZE - 1)) + 1;
+	}
+
+	/*
+	 * Clean and invalidate partial last cache line.
+	 */
+	if (start < end && end & (CACHE_LINE_SIZE - 1)) {
+		l2_clean_inv_pa(end & ~(CACHE_LINE_SIZE - 1));
+		end &= ~(CACHE_LINE_SIZE - 1);
+	}
+
+	/*
+	 * Invalidate all full cache lines between 'start' and 'end'.
+	 */
+	while (start < end) {
+		unsigned long range_end = calc_range_end(start, end);
+		l2_inv_pa_range(start, range_end - CACHE_LINE_SIZE);
+		start = range_end;
+	}
+
+	dsb();
+}
+
+void aurora_l2_clean_range(unsigned long start, unsigned long end)
+{
+    	if (!auroraL2_enable)
+        	return;        
+	/*
+	 * If L2 is forced to WT, the L2 will always be clean and we
+	 * don't need to do anything here.
+	 */
+	if (!l2_wt_override) {
+		start &= ~(CACHE_LINE_SIZE - 1);
+		end = (end + CACHE_LINE_SIZE - 1) & ~(CACHE_LINE_SIZE - 1);
+		while (start != end) {
+			unsigned long range_end = calc_range_end(start, end);
+			l2_clean_pa_range(start, range_end - CACHE_LINE_SIZE);
+			start = range_end;
+		}
+	}
+
+	dsb();
+}
+
+void aurora_l2_flush_range(unsigned long start, unsigned long end)
+{
+    	if (!auroraL2_enable)
+        	return;        
+
+	start &= ~(CACHE_LINE_SIZE - 1);
+	end = (end + CACHE_LINE_SIZE - 1) & ~(CACHE_LINE_SIZE - 1);
+	while (start != end) {
+		unsigned long range_end = calc_range_end(start, end);
+		if (!l2_wt_override)
+			l2_flush_pa_range(start, range_end - CACHE_LINE_SIZE);
+		start = range_end;
+	}
+
+	dsb();
+}
+#endif /* #ifdef CONFIG_AURORA_L2_OUTER */
+
+/*
+ * Routines to disable and re-enable the D-cache and I-cache at run
+ * time.  
+ */
+static u32 __init invalidate_and_disable_cache(void)
+{
+	int dummy;
+	volatile u32 cr;
+
+	cr = get_cr();
+	if (cr & CR_C) {
+		unsigned long flags;
+
+		raw_local_irq_save(flags);
+		flush_cache_all();
+		set_cr(cr & ~CR_C);
+		raw_local_irq_restore(flags);
+	}
+	if (cr & CR_I) {
+		set_cr(cr & ~CR_I);
+		__asm__ __volatile__("mcr p15, 0, %0, c7, c5, 0\n" : "=r" (dummy));
+	}
+	
+	return ((cr & CR_C) | (cr & CR_I));
+}
+
+static void __init enable_cache(u32 mask)
+{
+	volatile u32 cr1;
+
+	cr1 = get_cr();
+	set_cr(cr1 | mask);
+}
+
+static void __init enable_l2(void)
+{
+	u32 u, mask;
+#if 0
+	/* Enable SMP (SMPnAMP) in Aux Control Reg */
+	__asm__ __volatile__("mrc p15, 0, %0, c1, c0, 1" : "=r" (u));
+	u |= 0x40; /* Set SMPnAMP bit */
+	__asm__ __volatile__("mcr p15, 0, %0, c1, c0, 1\n" : : "r" (u));
+#endif
+	/* Enable Broadcasring (FW) in Extra Features Reg */	
+	__asm__ __volatile__("mrc p15, 1, %0, c15, c2, 0" : "=r" (u));
+	u |= 0x100; /* Set the FW bit */
+	__asm__ __volatile__("mcr p15, 1, %0, c15, c2, 0\n" : : "r" (u));
+
+	u = readl(auroraL2_base+L2_CONTROL);
+	if (!(u & 1)) {
+		printk(KERN_INFO "Aurora L2 Cache Enabled\n");
+		u |= 1;
+		mask = invalidate_and_disable_cache();
+		writel(u, auroraL2_base+L2_CONTROL);
+		enable_cache(mask);
+	}
+}
+
+/* lock_mask is a bit map. '1' means way is locked. '0' means way is unlocked */
+void __init aurora_l2_lockdown(u32 cpuId, u32 lock_mask)
+{
+	lock_mask &= 0xFF;
+	writel(lock_mask, auroraL2_base+L2_LOCKDOWN_DATA+(cpuId*8));
+	writel(lock_mask, auroraL2_base+L2_LOCKDOWN_INSTR+(cpuId*8));
+	writel(lock_mask, auroraL2_base+L2_LOCKDOWN_FPU);
+	writel(lock_mask, auroraL2_base+L2_LOCKDOWN_IO_BRG);
+}
+
+
+void auroraL2_inv_all(void)
+{
+	u32 u   = 0xffff; // all ways
+	writel(u, auroraL2_base+L2_INVAL_WAY_REG);
+}
+
+void auroraL2_flush_all(void)
+{
+	u32 u   = 0xffff; // all ways
+
+    	if (!auroraL2_enable)
+		return;
+
+	writel(u, auroraL2_base + L2_FLUSH_WAY_REG);
+	cache_sync();
+}
+
+void auroraL2_disable(void)
+{
+	u32 u;
+	auroraL2_flush_all();
+	auroraL2_inv_all();
+	u = readl(auroraL2_base+L2_CONTROL);
+	u &= ~1;
+	writel(u, auroraL2_base+L2_CONTROL);
+
+}
+
+struct regs_entry {
+	u32             reg_address;
+	u32             reg_value;
+};
+
+static struct regs_entry aurora_l2_regs[] = {
+	{L2_AUX_CONTROL,0},
+	{L2_CONTROL, 0}
+};
+
+/* L2 Power management function */
+int aurora_l2_pm_enter(void)
+{
+	int i;
+
+    	if (!auroraL2_enable)
+		return 0;
+	for ( i = 0; i < ARRAY_SIZE(aurora_l2_regs); i++)
+		aurora_l2_regs[i].reg_value = readl(auroraL2_base+ aurora_l2_regs[i].reg_address);
+
+	return 0;
+}
+int aurora_l2_pm_exit(void)
+{
+	int i;
+	u32	u;
+    	if (!auroraL2_enable)
+		return 0;
+
+	u = readl(auroraL2_base+L2_CONTROL);
+        if (!(u & 1)) {
+		pr_debug("Aurora: Enabling L2\n");
+
+		for ( i = 0; i < ARRAY_SIZE(aurora_l2_regs); i++)
+			writel(aurora_l2_regs[i].reg_value, auroraL2_base+ aurora_l2_regs[i].reg_address);
+	}
+	return 0;
+}
+
+int __init aurora_l2_init(void __iomem *base)
+{
+	__u32 aux;
+
+#ifdef CONFIG_PROC_FS
+	struct proc_dir_entry *res;
+	struct proc_dir_entry *res_file;
+
+	res = proc_mkdir("AuroraL2", NULL);
+	if (!res)
+		return -ENOMEM;
+
+    	/* Create information proc file */
+	res_file = create_proc_entry("info", S_IWUSR | S_IRUGO, res);
+	if (!res)
+		return -ENOMEM;
+
+	res_file->read_proc = proc_auroraL2_info_read;
+	res_file->write_proc = NULL;
+
+#ifdef CONFIG_CACHE_AURORAL2_EVENT_MONITOR_ENABLE
+	/* Create counter proc file */
+	res_file = create_proc_entry("counter", S_IWUSR | S_IRUGO, res);
+	if (!res)
+		return -ENOMEM;
+
+	res_file->read_proc = proc_auroraL2_counter_read;
+	res_file->write_proc = proc_auroraL2_counter_write;
+#endif /* CONFIG_CACHE_AURORAL2_EVENT_MONITOR_ENABLE */
+#endif
+
+#ifdef CONFIG_AURORA_L2_OUTER
+	outer_cache.inv_range = aurora_l2_inv_range;
+	outer_cache.clean_range = aurora_l2_clean_range;
+	outer_cache.flush_range = aurora_l2_flush_range;
+#endif
+
+	auroraL2_base = base;
+
+#ifdef CONFIG_MV_AMP_ENABLE
+	if(is_primary_amp())
+#endif
+	{
+
+		/* 1. Write to AuroraL2 Auxiliary Control Register, 0x104
+		*    Setting up Associativity, Way Size, and Latencies
+		*/
+		aux = readl(auroraL2_base + L2_AUX_CTRL_REG);
+		aux &= ~L2ACR_REPLACEMENT_MASK;
+		aux |= l2rep;
+#ifdef CONFIG_MV_SUPPORT_L2_DEPOSIT
+		aux &= ~L2ACR_FORCE_WRITE_POLICY_MASK;
+		aux |= L2ACR_FORCE_WRITE_BACK_POLICY;
+#endif
+		writel(aux, auroraL2_base + L2_AUX_CTRL_REG);
+
+		l2_wt_override = ((aux & (0x3)) == 0x2 ? 1:0);
+		/* 3. Secure write to AuroraL2 Invalidate by Way, 0x77c
+		*/
+		auroraL2_inv_all();
+
+		/* 4. Write to the Lockdown D and Lockdown I Register 9 if required
+		*/
+
+		/* 5. Write to interrupt clear register, 0x220, to clear any residual
+		*    raw interrupt set.
+		*/
+		writel(0x1FF, auroraL2_base + L2_INT_CAUSE_REG);
+
+		/* 6. Enable L2 cache
+		*/
+		enable_l2();
+	}
+
+	auroraL2_enable = 1;
+
+	return 0;
+}
diff --git a/arch/arm/mm/cache-v7.S b/arch/arm/mm/cache-v7.S
index 2b7d70f..03c32e4 100644
--- a/arch/arm/mm/cache-v7.S
+++ b/arch/arm/mm/cache-v7.S
@@ -16,6 +16,7 @@
 #include <asm/unwind.h>
 
 #include "proc-macros.S"
+#include "sheeva_pj4b-macros.S"
 
 /*
  *	v7_flush_icache_all()
diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 85e448e..4c26632 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -118,6 +118,7 @@ static int __init early_cachepolicy(char *p)
 	}
 	if (i == ARRAY_SIZE(cache_policies))
 		printk(KERN_ERR "ERROR: unknown or unsupported cache policy\n");
+#if !defined (CONFIG_CPU_SHEEVA_PJ4B_V6) && !defined(CONFIG_CPU_SHEEVA_PJ4B_V7)
 	/*
 	 * This restriction is partly to do with the way we boot; it is
 	 * unpredictable to have memory mapped using two different sets of
@@ -129,6 +130,7 @@ static int __init early_cachepolicy(char *p)
 		printk(KERN_WARNING "Only cachepolicy=writeback supported on ARMv6 and later\n");
 		cachepolicy = CPOLICY_WRITEBACK;
 	}
+#endif
 	flush_cache_all();
 	set_cr(cr_alignment);
 	return 0;
@@ -329,6 +331,9 @@ static void __init build_mem_type_table(void)
 	if (is_smp())
 		cachepolicy = CPOLICY_WRITEALLOC;
 
+#ifdef CONFIG_AURORA_IO_CACHE_COHERENCY
+	cachepolicy = CPOLICY_WRITEALLOC;
+#endif
 	/*
 	 * Strip out features not present on earlier architectures.
 	 * Pre-ARMv5 CPUs don't have TEX bits.  Pre-ARMv6 CPUs or those
@@ -999,6 +1004,7 @@ void __init sanity_check_meminfo(void)
 
 		j++;
 	}
+#ifndef CONFIG_ARCH_ARMADA_XP
 #ifdef CONFIG_HIGHMEM
 	if (highmem) {
 		const char *reason = NULL;
@@ -1019,6 +1025,7 @@ void __init sanity_check_meminfo(void)
 		}
 	}
 #endif
+#endif
 	meminfo.nr_banks = j;
 	high_memory = __va(arm_lowmem_limit - 1) + 1;
 	memblock_set_current_limit(arm_lowmem_limit);
diff --git a/arch/arm/mm/proc-sheeva_pj4bv6.S b/arch/arm/mm/proc-sheeva_pj4bv6.S
new file mode 100644
index 0000000..0291e0c
--- /dev/null
+++ b/arch/arm/mm/proc-sheeva_pj4bv6.S
@@ -0,0 +1,300 @@
+/*
+ *  linux/arch/arm/mm/proc-sheeva_pj4bv6.S
+ *
+ *  Copyright (C) 2001 Deep Blue Solutions Ltd.
+ *  Modified by Catalin Marinas for noMMU support
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ *  This is the "shell" of the ARMv6 processor support.
+ */
+#include <linux/init.h>
+#include <linux/linkage.h>
+#include <asm/assembler.h>
+#include <asm/asm-offsets.h>
+#include <asm/hwcap.h>
+#include <asm/pgtable-hwdef.h>
+#include <asm/pgtable.h>
+#include <mach/armadaxp.h>
+
+#include "proc-macros.S"
+#include "sheeva_pj4b-macros.S"
+
+#define D_CACHE_LINE_SIZE	32
+
+#define TTB_C		(1 << 0)
+#define TTB_S		(1 << 1)
+#define TTB_IMP		(1 << 2)
+#define TTB_RGN_NC	(0 << 3)
+#define TTB_RGN_WBWA	(1 << 3)
+#define TTB_RGN_WT	(2 << 3)
+#define TTB_RGN_WB	(3 << 3)
+
+#ifdef CONFIG_AURORA_L2_PT_WALK
+#define TTB_FLAGS_OUTER	TTB_RGN_WB
+#else
+#define TTB_FLAGS_OUTER	0
+#endif
+
+#ifndef CONFIG_SMP
+#define TTB_FLAGS	TTB_FLAGS_OUTER
+#define PMD_FLAGS	PMD_SECT_WB
+#else
+#define TTB_FLAGS	TTB_FLAGS_OUTER|TTB_S
+#define PMD_FLAGS	PMD_SECT_WBWA|PMD_SECT_S
+#endif
+
+ENTRY(cpu_sheeva_pj4b_v6_proc_init)
+	mov	pc, lr
+
+ENTRY(cpu_sheeva_pj4b_v6_proc_fin)
+#ifdef CONFIG_CACHE_AURORA_L2
+	mcr	p15, 0, r0, c7, c10, 4		@ Data Synchronization Barrier
+	ldr	r0, =0xffff			@ L2C clean all 16 ways
+	ldr	r1, =AXP_L2_CLEAN_WAY_REG
+	str	r0, [r1]
+	mrc	p15, 0, r0, c0, c0, 5		@ Read CPU core number
+	and	r0, r0, #0xF
+	mov	r1, #0x1
+	lsl	r0, r1, r0
+1:	ldr	r1, =AXP_L2_MNTNC_STAT_REG	@ Read maintanence status to check done per CPU
+	ldr	r1, [r1]
+	tst	r0, r1
+	bne	1b
+	mcr	p15, 0, r0, c7, c10, 4		@ Data Synchronization Barrier
+#endif
+	mrc	p15, 0, r0, c1, c0, 0		@ ctrl register
+	bic	r0, r0, #0x1000			@ ...i............
+	bic	r0, r0, #0x0006			@ .............ca.
+	mcr	p15, 0, r0, c1, c0, 0		@ disable caches
+	ldmfd	sp!, {pc}
+
+/*
+ *	cpu_sheeva_pj4b_v6_reset(loc)
+ *
+ *	Perform a soft reset of the system.  Put the CPU into the
+ *	same state as it would be if it had been reset, and branch
+ *	to what would be the reset vector.
+ *
+ *	- loc   - location to jump to for soft reset
+ */
+	.align	5
+ENTRY(cpu_sheeva_pj4b_v6_reset)
+	mov	pc, r0
+/*
+ *	cpu_sheeva_pj4b_v6_do_idle()
+ *
+ *	Idle the processor (eg, wait for interrupt).
+ *
+ *	IRQs are already disabled.
+ */
+ENTRY(cpu_sheeva_pj4b_v6_do_idle)
+	mov	r1, #0
+	mcr	p15, 0, r1, c7, c10, 4		@ DWB - WFI may enter a low-power mode
+#ifndef CONFIG_SHEEVA_ERRATA_ARM_CPU_BTS61
+	mcr	p15, 0, r1, c7, c0, 4		@ wait for interrupt
+#endif
+#ifdef CONFIG_SHEEVA_ERRATA_ARM_CPU_4742
+	mcr	p15, 0, r0, c7, c10, 4		@barrier
+#endif
+	mov	pc, lr
+ENTRY(cpu_sheeva_pj4b_v6_dcache_clean_area)
+#ifndef TLB_CAN_READ_FROM_L1_CACHE
+#if !defined(CONFIG_HAVE_GENERIC_DMA_COHERENT) || defined(CONFIG_SHEEVA_ERRATA_ARM_CPU_4611)
+	mrs     r2, cpsr
+        orr     r3, r2, #PSR_F_BIT | PSR_I_BIT
+        msr     cpsr_c, r3                      @ Disable interrupts
+	mcr     p15, 0, r0, c7, c10, 4          @ Data Synchronization Barrier
+#endif
+1:	mcr	p15, 0, r0, c7, c10, 1		@ clean D entry
+#ifndef CONFIG_AURORA_L2_PT_WALK
+	mcr     p15, 1, r0, c7, c11, 1		@ clean L2C D entry
+#endif
+	mcr     p15, 0, r0, c7, c10, 4          @ Data Synchronization Barrier
+	add	r0, r0, #D_CACHE_LINE_SIZE
+	subs	r1, r1, #D_CACHE_LINE_SIZE
+	bhi	1b
+#if !defined(CONFIG_HAVE_GENERIC_DMA_COHERENT) || defined(CONFIG_SHEEVA_ERRATA_ARM_CPU_4611)
+        msr     cpsr_c, r2                      @ Restore interrupts
+	mcr	p15, 0, r0, c7, c10, 4		@ Data Synchronization Barrier
+#endif
+#endif
+	mov	pc, lr
+/*
+ *	cpu_arm926_switch_mm(pgd_phys, tsk)
+ *
+ *	Set the translation table base pointer to be pgd_phys
+ *
+ *	- pgd_phys - physical address of new TTB
+ *
+ *	It is assumed that:
+ *	- we are not using split page tables
+ */
+ENTRY(cpu_sheeva_pj4b_v6_switch_mm)
+#ifdef CONFIG_MMU
+	mov	r2, #0
+	ldr	r1, [r1, #MM_CONTEXT_ID]	@ get mm->context.id
+	ALT_SMP(orr	r0, r0, #TTB_FLAGS_SMP)
+	ALT_UP(orr	r0, r0, #TTB_FLAGS_UP)
+	mcr	p15, 0, r2, c7, c5, 6		@ flush BTAC/BTB
+	mcr	p15, 0, r2, c7, c10, 4		@ drain write buffer
+	mcr	p15, 0, r0, c2, c0, 0		@ set TTB 0
+	mcr	p15, 0, r0, c7, c10, 4		@ Data Synchronization Barrier
+	mcr	p15, 0, r1, c13, c0, 1		@ set context ID
+#endif
+	mov	pc, lr
+/*
+ *	cpu_sheeva_pj4b_v6_set_pte_ext(ptep, pte, ext)
+ *
+ *	Set a level 2 translation table entry.
+ *
+ *	- ptep  - pointer to level 2 translation table entry
+ *		  (hardware version is stored at -1024 bytes)
+ *	- pte   - PTE value to store
+ *	- ext	- value for extended PTE bits
+ */
+	armv6_mt_table cpu_v6
+ENTRY(cpu_sheeva_pj4b_v6_set_pte_ext)
+#ifdef CONFIG_MMU
+	armv6_set_pte_ext cpu_v6
+#endif
+	mov	pc, lr
+	.type	cpu_sheeva_pj4b_v6_name, #object
+cpu_sheeva_pj4b_v6_name:
+	.asciz	"Marvell PJ4Bv6 processor"
+	.size	cpu_sheeva_pj4b_v6_name, . - cpu_sheeva_pj4b_v6_name
+	.type	cpu_pj4_name, #object
+cpu_pj4_name:
+	.asciz	"Marvell PJ4 processor"
+	.size	cpu_pj4_name, . - cpu_pj4_name
+	.align
+	__INIT
+
+/*
+ *	__pj4bv6_setup
+ *
+ *	Initialise TLB, Caches, and MMU state ready to switch the MMU
+ *	on.  Return in r0 the new CP15 C1 control register setting.
+ *
+ *	We automatically detect if we have a Harvard cache, and use the
+ *	Harvard cache control instructions insead of the unified cache
+ *	control instructions.
+ *
+ *	This should be able to cover all ARMv6 cores.
+ *
+ *	It is assumed that:
+ *	- cache type register is implemented
+ */
+__pj4bv6_setup:
+	sheeva_pj4b_config
+
+#if defined(CONFIG_SMP) || defined (CONFIG_AURORA_IO_CACHE_COHERENCY)
+	ALT_SMP(mrc	p15, 0, r0, c1, c0, 1)	@ Enable SMP/nAMP mode
+	ALT_UP(nop)
+	orr	r0, r0, #0x20
+	ALT_SMP(mcr	p15, 0, r0, c1, c0, 1)
+	mrc 	p15, 1, r0, c15, c2, 0
+	orr	r0, r0, #0x2			@ SMP enable 
+	mcr 	p15, 1, r0, c15, c2, 0
+	ALT_UP(nop)
+#endif
+#ifdef CONFIG_CPU_SHEEVA_PJ4B_PMC_ACCESS_IN_USERMODE
+	@ Enable performance counters user access
+	mrc     p15, 0, r0, c9, c14, 0
+	orr     r0, r0, #0x1
+	mcr     p15, 0, r0, c9, c14, 0
+#endif /* CONFIG_CPU_SHEEVA_PJ4B_PMC_ACCESS_IN_USERMODE */
+	mov	r0, #0
+	mcr	p15, 0, r0, c7, c10, 4		@ Data Synchronization Barrier
+	mcr	p15, 0, r0, c7, c14, 0		@ clean+invalidate D cache
+	mcr	p15, 0, r0, c7, c5, 0		@ invalidate I cache
+	mcr	p15, 0, r0, c7, c15, 0		@ clean+invalidate cache
+	mcr	p15, 0, r0, c7, c10, 4		@ drain write buffer
+#ifdef CONFIG_MMU
+	mcr	p15, 0, r0, c8, c7, 0		@ invalidate I + D TLBs
+	mcr	p15, 0, r0, c2, c0, 2		@ TTB control register
+	ALT_SMP(orr	r4, r4, #TTB_FLAGS_SMP)
+	ALT_UP(orr	r4, r4, #TTB_FLAGS_UP)
+	ALT_SMP(orr	r8, r8, #TTB_FLAGS_SMP)
+	ALT_UP(orr	r8, r8, #TTB_FLAGS_UP)
+	mcr	p15, 0, r8, c2, c0, 1		@ load TTB1
+	mcr	p15, 0, r0, c7, c10, 4		@ Data Synchronization Barrier
+#endif /* CONFIG_MMU */
+	adr	r5, v6_crval
+	ldmia	r5, {r5, r6}
+#ifdef CONFIG_CPU_ENDIAN_BE8
+	orr	r6, r6, #1 << 25		@ big-endian page tables
+#endif
+	mrc	p15, 0, r0, c1, c0, 0		@ read control register
+	bic	r0, r0, r5			@ clear bits them
+	orr	r0, r0, r6			@ set them
+	mov	pc, lr				@ return to head.S:__ret
+
+	/*
+	 *         V X F   I D LR
+	 * .... ...E PUI. .T.T 4RVI ZFRS BLDP WCAM
+	 * rrrr rrrx xxx0 0101 xxxx xxxx x111 xxxx < forced
+	 *         0 110       0011 1.00 .111 1101 < we want
+	 */
+	.type	v6_crval, #object
+v6_crval:
+	crval	clear=0x01e0fb7f, mmuset=0x00c0387d, ucset=0x00c0187c
+
+	__INITDATA
+	.type	v6_processor_functions, #object
+ENTRY(v6_processor_functions)
+	.word	v6_early_abort
+	.word	v6_pabort
+	.word	cpu_sheeva_pj4b_v6_proc_init
+	.word	cpu_sheeva_pj4b_v6_proc_fin
+	.word	cpu_sheeva_pj4b_v6_reset
+	.word	cpu_sheeva_pj4b_v6_do_idle
+	.word	cpu_sheeva_pj4b_v6_dcache_clean_area
+	.word	cpu_sheeva_pj4b_v6_switch_mm
+	.word	cpu_sheeva_pj4b_v6_set_pte_ext
+	.size	v6_processor_functions, . - v6_processor_functions
+	.section ".rodata"
+	.type	cpu_arch_name, #object
+cpu_arch_name:
+	.asciz	"armv6"
+	.size	cpu_arch_name, . - cpu_arch_name
+	.type	cpu_elf_name, #object
+cpu_elf_name:
+	.asciz	"v6"
+	.size	cpu_elf_name, . - cpu_elf_name
+	.align
+	.section ".proc.info.init", #alloc, #execinstr
+	/*
+	 * Match any ARMv6 processor core.
+	 */
+	.type	__pj4bv6_proc_info, #object
+__pj4bv6_proc_info:
+	.long	0x000f0000
+	.long	0x000f0000
+	ALT_SMP(.long \
+		PMD_TYPE_SECT | \
+		PMD_SECT_AP_WRITE | \
+		PMD_SECT_AP_READ | \
+		PMD_FLAGS_SMP)
+
+	ALT_UP(.long \
+		PMD_TYPE_SECT | \
+		PMD_SECT_AP_WRITE | \
+		PMD_SECT_AP_READ | \
+		PMD_FLAGS_UP)
+	.long   PMD_TYPE_SECT | \
+		PMD_SECT_XN | \
+		PMD_SECT_AP_WRITE | \
+		PMD_SECT_AP_READ
+	b	__pj4bv6_setup
+	.long	cpu_arch_name
+	.long	cpu_elf_name
+	.long	HWCAP_SWP|HWCAP_HALF|HWCAP_THUMB|HWCAP_FAST_MULT|HWCAP_EDSP|HWCAP_JAVA|HWCAP_TLS
+	.long	cpu_pj4_name
+	.long	v6_processor_functions
+	.long	v6wbi_tlb_fns
+	.long	v6_user_fns
+	.long	v6_cache_fns
+	.size	__pj4_v6_proc_info, . - __pj4_v6_proc_info
diff --git a/arch/arm/mm/proc-sheeva_pj4bv7.S b/arch/arm/mm/proc-sheeva_pj4bv7.S
new file mode 100644
index 0000000..66aa0d6
--- /dev/null
+++ b/arch/arm/mm/proc-sheeva_pj4bv7.S
@@ -0,0 +1,675 @@
+/*
+ *  linux/arch/arm/mm/proc-sheeva_pj4bv7.S
+ *
+ *  Copyright (C) 2001 Deep Blue Solutions Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ *  This is the "shell" of the ARMv7 processor support.
+ */
+#include <linux/init.h>
+#include <linux/linkage.h>
+#include <asm/assembler.h>
+#include <asm/asm-offsets.h>
+#include <asm/hwcap.h>
+#include <asm/pgtable-hwdef.h>
+#include <asm/pgtable.h>
+#if defined(CONFIG_ARCH_ARMADA_XP)
+#include <mach/armadaxp.h>
+#elif defined(CONFIG_ARCH_ARMADA370)
+#include <mach/armada370.h>
+#include <mach/kw_macro.h>
+#endif
+
+#include "proc-macros.S"
+#include "sheeva_pj4b-macros.S"
+
+#define TTB_S		(1 << 1)
+#define TTB_RGN_NC	(0 << 3)
+#define TTB_RGN_OC_WBWA	(1 << 3)
+#define TTB_RGN_OC_WT	(2 << 3)
+#define TTB_RGN_OC_WB	(3 << 3)
+#define TTB_NOS		(1 << 5)
+#define TTB_IRGN_NC	((0 << 0) | (0 << 6))
+#define TTB_IRGN_WBWA	((0 << 0) | (1 << 6))
+#define TTB_IRGN_WT	((1 << 0) | (0 << 6))
+#define TTB_IRGN_WB	((1 << 0) | (1 << 6))
+
+/* PTWs cacheable, inner WB not shareable, outer WB not shareable */
+#define TTB_FLAGS_UP	TTB_IRGN_WB|TTB_RGN_OC_WB
+#define PMD_FLAGS_UP	PMD_SECT_WB
+
+
+/* PTWs cacheable, inner WBWA shareable, outer WBWA not shareable */
+#define TTB_FLAGS_SMP	TTB_IRGN_WBWA|TTB_S|TTB_NOS|TTB_RGN_OC_WBWA
+#define PMD_FLAGS_SMP	PMD_SECT_WBWA|PMD_SECT_S
+
+ENTRY(cpu_pj4bv7_proc_init)
+	mov	pc, lr
+ENDPROC(cpu_pj4bv7_proc_init)
+
+ENTRY(cpu_pj4bv7_proc_fin)
+	
+#ifdef CONFIG_CACHE_AURORA_L2
+	mcr	p15, 0, r0, c7, c10, 4		@ Data Synchronization Barrier
+	ldr	r0, =0xffff			@ L2C clean all 16 ways
+	ldr	r1, =AXP_L2_CLEAN_WAY_REG
+	str	r0, [r1]
+	mrc	p15, 0, r0, c0, c0, 5		@ Read CPU core number
+	and	r0, r0, #0xF
+	mov	r1, #0x1
+	lsl	r0, r1, r0
+1:	ldr	r1, =AXP_L2_MNTNC_STAT_REG	@ Read maintanence status to check done per CPU
+	ldr	r1, [r1]
+	tst	r0, r1
+	bne	1b
+	mcr	p15, 0, r0, c7, c10, 4		@ Data Synchronization Barrier
+#endif
+	mrc	p15, 0, r0, c1, c0, 0		@ ctrl register
+	bic	r0, r0, #0x1000			@ ...i............
+	bic	r0, r0, #0x0006			@ .............ca.
+	mcr	p15, 0, r0, c1, c0, 0		@ disable caches
+	mov	pc, lr
+	
+ENDPROC(cpu_pj4bv7_proc_fin)
+
+/*
+++*	cpu_pj4bv7_reset(loc)
+ *
+ *	Perform a soft reset of the system.  Put the CPU into the
+ *	same state as it would be if it had been reset, and branch
+ *	to what would be the reset vector.
+ *
+ *	- loc   - location to jump to for soft reset
+ */
+	.align	5
+ENTRY(cpu_pj4bv7_reset)
+	mov	pc, r0
+ENDPROC(cpu_pj4bv7_reset)
+
+/*
+++ *	cpu_pj4bv7_do_idle()
+ *
+ *	Idle the processor (eg, wait for interrupt).
+ *
+ *	IRQs are already disabled.
+ */
+ENTRY(cpu_pj4bv7_do_idle)
+#ifndef CONFIG_SHEEVA_ERRATA_ARM_CPU_BTS61
+	dsb					@ WFI may enter a low-power mode
+	wfi
+#ifdef CONFIG_SHEEVA_ERRATA_ARM_CPU_4742
+	mcr	p15, 0, r0, c7, c10, 4		@barrier
+#endif
+#endif	
+	mov	pc, lr
+ENDPROC(cpu_pj4bv7_do_idle)
+
+ENTRY(cpu_pj4bv7_dcache_clean_area)
+#ifndef TLB_CAN_READ_FROM_L1_CACHE
+	dcache_line_size r2, r3
+1:
+#if defined CONFIG_SHEEVA_ERRATA_ARM_CPU_6043 || defined(CONFIG_SHEEVA_ERRATA_ARM_CPU_6124) || defined(CONFIG_SHEEVA_ERRATA_ARM_CPU_6076)
+	mcr     p15, 0, r0, c7, c14, 1			@ clean & invalidate D entry
+#else
+	mcr	p15, 0, r0, c7, c10, 1		@ clean D entry
+#endif
+	add	r0, r0, r2
+	subs	r1, r1, r2
+	bhi	1b
+	dsb
+#endif
+	mov	pc, lr
+ENDPROC(cpu_pj4bv7_dcache_clean_area)
+
+/*
+ *	cpu_pj4bv7_switch_mm(pgd_phys, tsk)
+ *
+ *	Set the translation table base pointer to be pgd_phys
+ *
+ *	- pgd_phys - physical address of new TTB
+ *
+ *	It is assumed that:
+ *	- we are not using split page tables
+ */
+ENTRY(cpu_pj4bv7_switch_mm)
+#ifdef CONFIG_MMU
+	mov	r2, #0
+	ldr	r1, [r1, #MM_CONTEXT_ID]	@ get mm->context.id
+#ifdef CONFIG_AURORA_L2_PT_WALK 
+	ALT_SMP(orr	r0, r0, #TTB_FLAGS_SMP)
+	ALT_UP(orr	r0, r0, #TTB_FLAGS_UP)
+#else
+	bic     r0, r0, #0x18                   @ DONOT Cache the page table in L2
+#endif
+#ifdef CONFIG_ARM_ERRATA_430973
+	mcr	p15, 0, r2, c7, c5, 6		@ flush BTAC/BTB
+#endif
+	mrc	p15, 0, r2, c2, c0, 1		@ load TTB 1
+	mcr	p15, 0, r2, c2, c0, 0		@ into TTB 0
+	isb
+#ifdef CONFIG_ARM_ERRATA_754322
+	dsb
+#endif
+	mcr	p15, 0, r1, c13, c0, 1		@ set context ID
+	isb
+	mcr	p15, 0, r0, c2, c0, 0		@ set TTB 0
+	isb
+#endif
+	mov	pc, lr
+ENDPROC(cpu_pj4bv7_switch_mm)
+
+/*
+ *	cpu_pj4bv7_set_pte_ext(ptep, pte)
+ *
+ *	Set a level 2 translation table entry.
+ *
+ *	- ptep  - pointer to level 2 translation table entry
+ *		  (hardware version is stored at -1024 bytes)
+ *	- pte   - PTE value to store
+ *	- ext	- value for extended PTE bits
+ */
+ENTRY(cpu_pj4bv7_set_pte_ext)
+#ifdef CONFIG_MMU
+ 
+str	r1, [r0]			@ linux version
+ 
+#ifdef CONFIG_MV_SUPPORT_64KB_PAGE_SIZE
+	mov     r3, #0x7C
+	and     r3, r3, r0
+	mov     r3, r3, lsl #4
+	bic     r0, r0, #0x3FC
+	bic     r0, r0, #0x400
+	orr     r0, r0, r3
+#endif
+	bic	r3, r1, #0x000003f0
+#ifdef CONFIG_MV_SUPPORT_64KB_PAGE_SIZE
+	bic	r3, r3, #0x00000F000
+#endif
+	bic	r3, r3, #PTE_TYPE_MASK
+	orr	r3, r3, r2
+#ifdef CONFIG_MV_SUPPORT_64KB_PAGE_SIZE
+	orr	r3, r3, #PTE_EXT_AP0 | 1
+#else
+	orr	r3, r3, #PTE_EXT_AP0 | 2
+#endif
+	tst	r1, #1 << 4
+	orrne	r3, r3, #PTE_EXT_TEX(1)
+
+	eor	r1, r1, #L_PTE_DIRTY
+	tst	r1, #L_PTE_RDONLY | L_PTE_DIRTY
+	orrne	r3, r3, #PTE_EXT_APX
+
+	tst	r1, #L_PTE_USER
+	orrne	r3, r3, #PTE_EXT_AP1
+#ifdef CONFIG_CPU_USE_DOMAINS
+	@ allow kernel read/write access to read-only user pages
+	tstne	r3, #PTE_EXT_APX
+	bicne	r3, r3, #PTE_EXT_APX | PTE_EXT_AP0
+#endif
+
+	tst	r1, #L_PTE_XN
+	orrne	r3, r3, #PTE_EXT_XN
+
+	tst	r1, #L_PTE_YOUNG
+	tstne	r1, #L_PTE_PRESENT
+	moveq	r3, #0
+
+ ARM(	str	r3, [r0, #2048]! )
+ THUMB(	add	r0, r0, #2048 )
+ THUMB(	str	r3, [r0] )
+#ifdef CONFIG_MV_SUPPORT_64KB_PAGE_SIZE
+       @ Need to duplicate the entry 16 times because of overlapping in PTE index bits.
+       str     r3, [r0, #4]
+       str     r3, [r0, #8]
+       str     r3, [r0, #12]
+       str     r3, [r0, #16]
+       str     r3, [r0, #20]
+       str     r3, [r0, #24]
+       str     r3, [r0, #28]
+#if defined CONFIG_SHEEVA_ERRATA_ARM_CPU_6043 || defined(CONFIG_SHEEVA_ERRATA_ARM_CPU_6124) || defined(CONFIG_SHEEVA_ERRATA_ARM_CPU_6076)
+	mcr     p15, 0, r0, c7, c14, 1			@ clean & invalidate D entry
+#else
+        mcr     p15, 0, r0, c7, c10, 1          @ flush_pte
+#endif
+       add     r0, r0, #32
+       str     r3, [r0]
+       str     r3, [r0, #4]
+       str     r3, [r0, #8]
+       str     r3, [r0, #12]
+       str     r3, [r0, #16]
+       str     r3, [r0, #20]
+       str     r3, [r0, #24]
+       str     r3, [r0, #28]
+#else /* CONFIG_MV_SUPPORT_64KB_PAGE_SIZE */
+#if defined CONFIG_SHEEVA_ERRATA_ARM_CPU_6043 || defined(CONFIG_SHEEVA_ERRATA_ARM_CPU_6124) || defined(CONFIG_SHEEVA_ERRATA_ARM_CPU_6076)
+	mcr		p15, 0, r0, c7, c14, 1			@ clean & invalidate D entry
+#else
+	mcr		p15, 0, r0, c7, c10, 1			@ flush_pte
+#endif
+#if defined CONFIG_SHEEVA_ERRATA_ARM_CPU_6043 || defined(CONFIG_SHEEVA_ERRATA_ARM_CPU_6124) || defined(CONFIG_SHEEVA_ERRATA_ARM_CPU_6076)
+	mcr		p15, 0, r0, c7, c14, 1			@ clean & invalidate D entry
+#else
+	mcr		p15, 0, r0, c7, c10, 1			@ flush_pte
+#endif
+#endif /* CONFIG_MV_SUPPORT_64KB_PAGE_SIZE */
+#endif
+	mov	pc, lr
+ENDPROC(cpu_pj4bv7_set_pte_ext)
+
+cpu_pj4bv7_name:
+	.ascii	"Marvell PJ4Bv7 Processor\0"
+	.align
+
+/*
+	 * Memory region attributes with SCTLR.TRE=1
+	 *
+	 *   n = TEX[0],C,B
+	 *   TR = PRRR[2n+1:2n]		- memory type
+	 *   IR = NMRR[2n+1:2n]		- inner cacheable property
+	 *   OR = NMRR[2n+17:2n+16]	- outer cacheable property
+	 *
+	 *			n	TR	IR	OR
+	 *   UNCACHED		000	00
+	 *   BUFFERABLE		001	10	00	00
+	 *   WRITETHROUGH	010	10	10	10
+	 *   WRITEBACK		011	10	11	11
+	 *   reserved		110
+	 *   WRITEALLOC		111	10	01	01
+	 *   DEV_SHARED		100	01
+	 *   DEV_NONSHARED	100	01
+	 *   DEV_WC		001	10
+	 *   DEV_CACHED		011	10
+	 *
+	 * Other attributes:
+	 *
+	 *   DS0 = PRRR[16] = 0		- device shareable property
+	 *   DS1 = PRRR[17] = 1		- device shareable property
+	 *   NS0 = PRRR[18] = 0		- normal shareable property
+	 *   NS1 = PRRR[19] = 1		- normal shareable property
+	 *   NOS = PRRR[24+n] = 1	- not outer shareable
+	 */
+.equ	PRRR,	0xff0a81a8
+#ifdef CONFIG_AURORA_L2_OUTER_WA
+.equ	NMRR,	0x40e040e0		/* Inner and outer WBWA */
+#else
+.equ	NMRR,	0xc0e040e0		/* Inner WBWA, outer WBnoWA*/
+#endif
+
+/* Suspend/resume support: derived from arch/arm/mach-s5pv210/sleep.S */
+.globl	cpu_v7_suspend_size
+.equ	cpu_v7_suspend_size, 4 * 9
+#ifdef CONFIG_PM_SLEEP
+ENTRY(cpu_v7_do_suspend)
+	stmfd	sp!, {r4 - r11, lr}
+	mrc	p15, 0, r4, c13, c0, 0	@ FCSE/PID
+	mrc	p15, 0, r5, c13, c0, 1	@ Context ID
+	mrc	p15, 0, r6, c13, c0, 3	@ User r/o thread ID
+	stmia	r0!, {r4 - r6}
+	mrc	p15, 0, r6, c3, c0, 0	@ Domain ID
+	mrc	p15, 0, r7, c2, c0, 0	@ TTB 0
+	mrc	p15, 0, r8, c2, c0, 1	@ TTB 1
+	mrc	p15, 0, r9, c1, c0, 0	@ Control register
+	mrc	p15, 0, r10, c1, c0, 1	@ Auxiliary control register
+	mrc	p15, 0, r11, c1, c0, 2	@ Co-processor access control
+	stmia	r0, {r6 - r11}
+	ldmfd	sp!, {r4 - r11, pc}
+ENDPROC(cpu_v7_do_suspend)
+
+ENTRY(cpu_v7_do_resume)
+	mov	ip, #0
+	mcr	p15, 0, ip, c8, c7, 0	@ invalidate TLBs
+	mcr	p15, 0, ip, c7, c5, 0	@ invalidate I cache
+	ldmia	r0!, {r4 - r6}
+	mcr	p15, 0, r4, c13, c0, 0	@ FCSE/PID
+	mcr	p15, 0, r5, c13, c0, 1	@ Context ID
+	mcr	p15, 0, r6, c13, c0, 3	@ User r/o thread ID
+	ldmia	r0, {r6 - r11}
+	mcr	p15, 0, r6, c3, c0, 0	@ Domain ID
+	mcr	p15, 0, r7, c2, c0, 0	@ TTB 0
+	mcr	p15, 0, r8, c2, c0, 1	@ TTB 1
+	mcr	p15, 0, ip, c2, c0, 2	@ TTB control register
+	mcr	p15, 0, r10, c1, c0, 1	@ Auxiliary control register
+	mcr	p15, 0, r11, c1, c0, 2	@ Co-processor access control
+	ldr	r4, =PRRR		@ PRRR
+	ldr	r5, =NMRR		@ NMRR
+	mcr	p15, 0, r4, c10, c2, 0	@ write PRRR
+	mcr	p15, 0, r5, c10, c2, 1	@ write NMRR
+	isb
+	mov	r0, r9			@ control register
+	mov	r2, r7, lsr #14		@ get TTB0 base
+	mov	r2, r2, lsl #14
+	ldr	r3, cpu_resume_l1_flags
+	b	cpu_resume_mmu
+ENDPROC(cpu_v7_do_resume)
+cpu_resume_l1_flags:
+	ALT_SMP(.long PMD_TYPE_SECT | PMD_SECT_AP_WRITE | PMD_FLAGS_SMP)
+	ALT_UP(.long  PMD_TYPE_SECT | PMD_SECT_AP_WRITE | PMD_FLAGS_UP)
+#else
+#define cpu_v7_do_suspend	0
+#define cpu_v7_do_resume	0
+#endif
+
+	__CPUINIT
+
+	
+/*
+++ *	__pj4bv7_setup
+ *
+ *	Initialise TLB, Caches, and MMU state ready to switch the MMU
+ *	on.  Return in r0 the new CP15 C1 control register setting.
+ *
+ *	We automatically detect if we have a Harvard cache, and use the
+ *	Harvard cache control instructions insead of the unified cache
+ *	control instructions.
+ *
+ *	This should be able to cover all ARMv7 cores.
+ *
+ *	It is assumed that:
+ *	- cache type register is implemented
+ */
+__pj4bv7_setup:
+	sheeva_pj4b_config
+
+#if 0
+@ CURRENTLY NOT SUPPORTED 
+defined(CONFIG_SMP)
+	mrc	p15, 0, r0, c1, c0, 1		@ Enable SMP/nAMP mode
+	orr	r0, r0, #0x20
+	mcr	p15, 0, r0, c1, c0, 1
+#if 1
+	mrc 	p15, 1, r0, c15, c2, 0
+	orr	r0, r0, #0x2			@ SMP enable 
+	mcr 	p15, 1, r0, c15, c2, 0
+#endif
+#endif
+
+#if defined(CONFIG_ARMADA_XP_REV_A0) || defined(CONFIG_ARMADA_XP_REV_B0)
+
+#ifdef CONFIG_ARMADA_XP_A0_WITH_B0
+/* Read and save SoC revision */
+	ldr r0, =0xd0018220		/* POWER_MNG_CTRL_REG */
+	ldr r5, [r0]
+	orr r0, r5, #(1 << 5)	/* Enable PEX0 Clk */
+	ldr r0, =0xd0040008
+	ldr r0, [r0]
+	and r0, r0, #0x3
+	str r0, soc_revision
+	ldr r0, =0xd0018220		/* POWER_MNG_CTRL_REG */
+	str r5, [r0]
+#endif
+
+/* DSMP A0/B0 */
+	/* Auxiliary Debug Modes Control 1 Register */
+	mrc        p15, 1, r0, c15, c1, 1                         /* Read */
+	orr        r0, r0, #0x00020                                /* BIT5 STREX backoff_disable--> '1' enable the back off of STREX instr */
+	orr	   r0, r0, #0x00100                                /* BIT8 Internal Parity Handling Disable--> '1' Disable Internal Parity Handling */
+#ifdef CONFIG_SHEEVA_ERRATA_ARM_CPU_6409
+#ifdef CONFIG_ARMADA_XP_A0_WITH_B0
+	skip_errata_if_axp_b0 r5
+#endif
+	bic        r0, r0, #0x4                                 /* Disable Static BP */
+1:
+#endif
+	mcr        p15, 1, r0, c15, c1, 1                         /* Write */
+	
+/* Auxiliary Functional Modes Control Register 0 */
+	mrc        p15, 1, r0, c15, c2, 0                         /* Read */
+#ifdef CONFIG_SMP
+	orr          r0, r0, #0x00002                                /* BIT1 SMP/nAMP --> '1' taking part in coherency */
+#endif
+	orr     r0, r0, #0x00004                                /* BIT2 L1 parity --> '1' Enabled */
+	orr     r0, r0, #0x00100                                /* BIT8 FW --> '1' enable Cache and TLB maintenance broadcast */
+	mcr        p15, 1, r0, c15, c2, 0                         /* Write */
+
+	/* Auxiliary Debug Modes Control 0 Register */
+	mrc        p15, 1, r0, c15, c1, 0                         /* Read */
+#ifdef CONFIG_SHEEVA_ERRATA_ARM_CPU_6136
+#ifdef CONFIG_ARMADA_XP_A0_WITH_B0
+	skip_errata_if_axp_b0 r5
+#endif
+	orr          r0, r0, #0x001000                              /* BIT12 ldstm_first Two_sgl --> '1' Force first issue to be single */
+	b 2f
+1:
+	bic          r0, r0, #0x001000                              /* BIT12 ldstm_first Two_sgl --> '0' The first issue is double word */
+2:
+#else
+	bic          r0, r0, #0x001000                              /* BIT12 ldstm_first Two_sgl --> '0' The first issue is double word */
+#endif
+	orr          r0, r0, #0x400000                              /* BIT22 dvm_wakeup_dis --> '1' WFI/WFE will serve the DVM and back to idle */
+	mcr        p15, 1, r0, c15, c1, 0                         /* Write */
+	/* Auxiliary Debug Modes Control 2 Register */
+	mrc        p15, 1, r0, c15, c1, 2                         /* Read */
+	orr          r0, r0, #0x02000000                         /* BIT25 Intervention Interleave Disable--> '1'  Disable Interleaving with Intervention Data */
+	orr          r0, r0, #0x08000000                         /* BIT27 CWF Disable--> '1' Disable critical word first sequencing */
+#ifdef CONFIG_SHEEVA_ERRATA_ARM_CPU_5980
+#ifdef CONFIG_ARMADA_XP_A0_WITH_B0
+	skip_errata_if_axp_b0 r5
+#endif
+	orr          r0, r0, #0x20000000                         /* BIT29 DEV Stream Disable--> '1' Disable MO device read / write */
+1:
+#endif
+	orr          r0, r0, #0x40000000                         /* BIT30 L1 Replacement Policy Config--> '1' Strict Round-Robin Replacement Policy  */
+	mcr        p15, 1, r0, c15, c1, 2                         /* Write */
+
+#else 
+#ifdef CONFIG_SMP 
+	ALT_SMP(mrc	p15, 0, r0, c1, c0, 1)
+	ALT_UP(mov	r0, #(1 << 6))		@ fake it for UP
+	tst	r0, #(1 << 6)			@ SMP/nAMP mode enabled?
+	orreq	r0, r0, #(1 << 6) | (1 << 0)	@ Enable SMP/nAMP mode and
+	mcreq	p15, 0, r0, c1, c0, 1		@ TLB ops broadcasting
+#endif
+#endif
+#ifdef CONFIG_CPU_SHEEVA_PJ4B_PMC_ACCESS_IN_USERMODE
+	@ Enable performance counters user access
+	mrc     p15, 0, r0, c9, c14, 0
+	orr     r0, r0, #0x1
+	mcr     p15, 0, r0, c9, c14, 0
+#endif /* CONFIG_CPU_SHEEVA_PJ4B_PMC_ACCESS_IN_USERMODE */
+adr	r12, __pj4bv7_setup_stack		@ the local stack
+	stmia	r12, {r0-r5, r7, r9, r11, lr}
+	bl	v7_flush_dcache_all
+	ldmia	r12, {r0-r5, r7, r9, r11, lr}
+
+	mrc	p15, 0, r0, c0, c0, 0		@ read main ID register
+	and	r10, r0, #0xff000000		@ ARM?
+	teq	r10, #0x41000000
+	bne	3f
+	and	r5, r0, #0x00f00000		@ variant
+	and	r6, r0, #0x0000000f		@ revision
+	orr	r6, r6, r5, lsr #20-4		@ combine variant and revision
+	ubfx	r0, r0, #4, #12			@ primary part number
+
+	/* Cortex-A8 Errata */ 
+	ldr	r10, =0x00000c08		@ Cortex-A8 primary part number
+	teq	r0, r10
+	bne	2f
+
+
+#ifdef CONFIG_ARM_ERRATA_430973
+	teq	r5, #0x00100000			@ only present in r1p*
+	mrceq	p15, 0, r10, c1, c0, 1		@ read aux control register
+	orreq	r10, r10, #(1 << 6)		@ set IBE to 1
+	mcreq	p15, 0, r10, c1, c0, 1		@ write aux control register
+#endif
+#ifdef CONFIG_ARM_ERRATA_458693
+	teq	r6, #0x20			@ only present in r2p0
+	mrceq	p15, 0, r10, c1, c0, 1		@ read aux control register
+	orreq	r10, r10, #(1 << 5)		@ set L1NEON to 1
+	orreq	r10, r10, #(1 << 9)		@ set PLDNOP to 1
+	mcreq	p15, 0, r10, c1, c0, 1		@ write aux control register
+#endif
+#ifdef CONFIG_ARM_ERRATA_460075
+	teq	r6, #0x20			@ only present in r2p0
+	mrceq	p15, 1, r10, c9, c0, 2		@ read L2 cache aux ctrl register
+	tsteq	r10, #1 << 22
+	orreq	r10, r10, #(1 << 22)		@ set the Write Allocate disable bit
+	mcreq	p15, 1, r10, c9, c0, 2		@ write the L2 cache aux ctrl register
+#endif
+	b	3f
+	/* Cortex-A9 Errata */
+2:	ldr	r10, =0x00000c09		@ Cortex-A9 primary part number
+	teq	r0, r10
+	bne	3f
+#ifdef CONFIG_ARM_ERRATA_742230
+	cmp	r6, #0x22			@ only present up to r2p2
+	mrcle	p15, 0, r10, c15, c0, 1		@ read diagnostic register
+	orrle	r10, r10, #1 << 4		@ set bit #4
+	mcrle	p15, 0, r10, c15, c0, 1		@ write diagnostic register
+#endif
+#ifdef CONFIG_ARM_ERRATA_742231
+	teq	r6, #0x20			@ present in r2p0
+	teqne	r6, #0x21			@ present in r2p1
+	teqne	r6, #0x22			@ present in r2p2
+	mrceq	p15, 0, r10, c15, c0, 1		@ read diagnostic register
+	orreq	r10, r10, #1 << 12		@ set bit #12
+	orreq	r10, r10, #1 << 22		@ set bit #22
+	mcreq	p15, 0, r10, c15, c0, 1		@ write diagnostic register
+#endif
+#ifdef CONFIG_ARM_ERRATA_743622
+	teq	r6, #0x20			@ present in r2p0
+	teqne	r6, #0x21			@ present in r2p1
+	teqne	r6, #0x22			@ present in r2p2
+	mrceq	p15, 0, r10, c15, c0, 1		@ read diagnostic register
+	orreq	r10, r10, #1 << 6		@ set bit #6
+	mcreq	p15, 0, r10, c15, c0, 1		@ write diagnostic register
+#endif
+#ifdef CONFIG_ARM_ERRATA_751472
+	cmp	r6, #0x30			@ present prior to r3p0
+	mrclt	p15, 0, r10, c15, c0, 1		@ read diagnostic register
+	orrlt	r10, r10, #1 << 11		@ set bit #11
+	mcrlt	p15, 0, r10, c15, c0, 1		@ write diagnostic register
+#endif
+
+3:	mov	r10, #0
+#ifdef HARVARD_CACHE
+	mcr	p15, 0, r10, c7, c5, 0		@ I+BTB cache invalidate
+#endif
+	dsb
+#ifdef CONFIG_MMU
+	mcr	p15, 0, r10, c8, c7, 0		@ invalidate I + D TLBs
+	mcr	p15, 0, r10, c2, c0, 2		@ TTB control register
+			
+#ifdef CONFIG_AURORA_L2_PT_WALK
+	ALT_SMP(orr	r4, r4, #TTB_FLAGS_SMP)
+	ALT_UP(orr	r4, r4, #TTB_FLAGS_UP)
+	ALT_SMP(orr	r8, r8, #TTB_FLAGS_SMP)
+	ALT_UP(orr	r8, r8, #TTB_FLAGS_UP)
+#else
+						@ OC bits in TTB1 register
+	bic	r4, r4, #0x18                   @ DONOT Cache the page table in L2
+#endif
+	mcr	p15, 0, r8, c2, c0, 1		@ load TTB1
+	
+	ldr	r5, =PRRR			@ PRRR
+	ldr	r6, =NMRR			@ NMRR
+	mcr	p15, 0, r5, c10, c2, 0		@ write PRRR
+	mcr	p15, 0, r6, c10, c2, 1		@ write NMRR
+#endif
+	adr	r5, v7_crval
+	ldmia	r5, {r5, r6}
+#ifdef CONFIG_CPU_ENDIAN_BE8
+	orr	r6, r6, #1 << 25		@ big-endian page tables
+#endif
+#ifdef CONFIG_SWP_EMULATE
+	orr     r5, r5, #(1 << 10)              @ set SW bit in "clear"
+	bic     r6, r6, #(1 << 10)              @ clear it in "mmuset"
+#endif
+   	mrc	p15, 0, r0, c1, c0, 0		@ read control register
+	bic	r0, r0, r5			@ clear bits them
+	orr	r0, r0, r6			@ set them
+ THUMB(	orr	r0, r0, #1 << 30	)	@ Thumb exceptions
+	mov	pc, lr				@ return to head.S:__ret
+ENDPROC(__pj4bv7_setup)
+
+.global soc_revision
+soc_revision:
+	.word 0
+
+	/*   AT
+	 *  TFR   EV X F   I D LR    S
+	 * .EEE ..EE PUI. .T.T 4RVI ZWRS BLDP WCAM
+	 * rxxx rrxx xxx0 0101 xxxx xxxx x111 xxxx < forced
+	 *    1    0 110       0011 1100 .111 1101 < we want
+	 */
+	.type	v7_crval, #object
+v7_crval:
+	crval	clear=0x0120c302, mmuset=0x10c03c7d, ucset=0x00c01c7c
+
+__pj4bv7_setup_stack:
+	.space	4 * 11				@ 11 registers
+__INITDATA
+	.type	v7_processor_functions, #object
+ENTRY(v7_processor_functions)
+	.word	v7_early_abort
+	.word	v7_pabort
+	.word	cpu_pj4bv7_proc_init
+	.word	cpu_pj4bv7_proc_fin
+	.word	cpu_pj4bv7_reset
+	.word	cpu_pj4bv7_do_idle
+	.word	cpu_pj4bv7_dcache_clean_area
+	.word	cpu_pj4bv7_switch_mm
+	.word	cpu_pj4bv7_set_pte_ext
+	.word	cpu_v7_suspend_size
+	.word	cpu_v7_do_suspend
+	.word	cpu_v7_do_resume
+	
+	/*
+	this whole 
+		.word	cpu_v7_suspend_size
+	.word	cpu_v7_do_suspend
+	.word	cpu_v7_do_resume
+	section above is a copy paste from proc-v7.s and need to be revisted
+	*/
+	
+	.size	v7_processor_functions, . - v7_processor_functions
+
+	.section ".rodata"
+	.type	cpu_arch_name, #object
+cpu_arch_name:
+	.asciz	"armv7"
+	.size	cpu_arch_name, . - cpu_arch_name
+
+	.type	cpu_elf_name, #object
+cpu_elf_name:
+	.asciz	"v7"
+	.size	cpu_elf_name, . - cpu_elf_name
+	.align
+
+	.section ".proc.info.init", #alloc, #execinstr
+
+	/*
+	 * Match any ARMv7 processor core.
+	 */
+	.type	__v7_proc_info, #object
+__v7_proc_info:
+	.long	0x000f0000		@ Required ID value
+	.long	0x000f0000		@ Mask for ID
+
+	ALT_SMP(.long \
+		PMD_TYPE_SECT | \
+		PMD_SECT_AP_WRITE | \
+		PMD_SECT_AP_READ | \
+		PMD_FLAGS_SMP)
+	ALT_UP(.long \
+		PMD_TYPE_SECT | \
+		PMD_SECT_AP_WRITE | \
+		PMD_SECT_AP_READ | \
+		PMD_FLAGS_UP)
+	.long   PMD_TYPE_SECT | \
+		PMD_SECT_XN | \
+		PMD_SECT_AP_WRITE | \
+		PMD_SECT_AP_READ
+	W(b)	__pj4bv7_setup
+	.long	cpu_arch_name
+	.long	cpu_elf_name
+	.long	HWCAP_SWP|HWCAP_HALF|HWCAP_THUMB|HWCAP_FAST_MULT|HWCAP_EDSP|HWCAP_TLS
+	.long	cpu_pj4bv7_name
+	.long	v7_processor_functions
+	.long	v7wbi_tlb_fns
+	.long	v6_user_fns
+	.long	v7_cache_fns
+	.size	__v7_proc_info, . - __v7_proc_info
diff --git a/arch/arm/mm/proc-sheeva_pj4bv7lpae.S b/arch/arm/mm/proc-sheeva_pj4bv7lpae.S
new file mode 100644
index 0000000..55ebc78
--- /dev/null
+++ b/arch/arm/mm/proc-sheeva_pj4bv7lpae.S
@@ -0,0 +1,491 @@
+/*
+ *  linux/arch/arm/mm/proc-sheeva_pj4bv7lpae.S
+ *
+ * Copyright (C) 2001 Deep Blue Solutions Ltd.
+ * Copyright (C) 2011 ARM Ltd.
+ * Author: Catalin Marinas <catalin.marinas@arm.com>
+ *   based on arch/arm/mm/proc-v7.S
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ */
+#include <linux/init.h>
+#include <linux/linkage.h>
+#include <asm/assembler.h>
+#include <asm/asm-offsets.h>
+#include <asm/hwcap.h>
+#include <asm/pgtable-hwdef.h>
+#include <asm/pgtable.h>
+#include <mach/armadaxp.h>
+
+#include "proc-macros.S"
+#include "sheeva_pj4b-macros.S"
+
+#define TTB_IRGN_NC	(0 << 8)
+#define TTB_IRGN_WBWA	(1 << 8)
+#define TTB_IRGN_WT	(2 << 8)
+#define TTB_IRGN_WB	(3 << 8)
+#define TTB_RGN_NC	(0 << 10)
+#define TTB_RGN_OC_WBWA	(1 << 10)
+#define TTB_RGN_OC_WT	(2 << 10)
+#define TTB_RGN_OC_WB	(3 << 10)
+#define TTB_S		(3 << 12)
+#define TTB_EAE		(1 << 31)
+
+/* PTWs cacheable, inner WB not shareable, outer WB not shareable */
+#define TTB_FLAGS_UP	(TTB_IRGN_WB|TTB_RGN_OC_WB)
+#define PMD_FLAGS_UP	(PMD_SECT_WB)
+
+/* PTWs cacheable, inner WBWA shareable, outer WBWA not shareable */
+#define TTB_FLAGS_SMP	(TTB_IRGN_WBWA|TTB_S|TTB_RGN_OC_WBWA)
+#define PMD_FLAGS_SMP	(PMD_SECT_WBWA|PMD_SECT_S)
+
+ENTRY(cpu_pj4bv7_proc_init)
+	mov	pc, lr
+ENDPROC(cpu_pj4bv7_proc_init)
+
+ENTRY(cpu_pj4bv7_proc_fin)
+	mrc	p15, 0, r0, c1, c0, 0		@ ctrl register
+	bic	r0, r0, #0x1000			@ ...i............
+	bic	r0, r0, #0x0006			@ .............ca.
+	mcr	p15, 0, r0, c1, c0, 0		@ disable caches
+	mov	pc, lr
+ENDPROC(cpu_pj4bv7_proc_fin)
+
+/*
+ *	cpu_pj4bv7_reset(loc)
+ *
+ *	Perform a soft reset of the system.  Put the CPU into the
+ *	same state as it would be if it had been reset, and branch
+ *	to what would be the reset vector.
+ *
+ *	- loc   - location to jump to for soft reset
+ */
+	.align	5
+ENTRY(cpu_pj4bv7_reset)
+	mov	pc, r0
+ENDPROC(cpu_pj4bv7_reset)
+
+/*
+ *	cpu_pj4bv7_do_idle()
+ *
+ *	Idle the processor (eg, wait for interrupt).
+ *
+ *	IRQs are already disabled.
+ */
+ENTRY(cpu_pj4bv7_do_idle)
+	dsb					@ WFI may enter a low-power mode
+	wfi
+	mov	pc, lr
+ENDPROC(cpu_pj4bv7_do_idle)
+
+ENTRY(cpu_pj4bv7_dcache_clean_area)
+#ifndef TLB_CAN_READ_FROM_L1_CACHE
+	dcache_line_size r2, r3
+1:	mcr	p15, 0, r0, c7, c10, 1		@ clean D entry
+	add	r0, r0, r2
+	subs	r1, r1, r2
+	bhi	1b
+	dsb
+#endif
+	mov	pc, lr
+ENDPROC(cpu_pj4bv7_dcache_clean_area)
+
+/*
+ *	cpu_v7_switch_mm(pgd_phys, tsk)
+ *
+ *	Set the translation table base pointer to be pgd_phys
+ *
+ *	- pgd_phys - physical address of new TTB
+ *
+ *	It is assumed that:
+ *	- we are not using split page tables
+ */
+ENTRY(cpu_pj4bv7_switch_mm)
+#ifdef CONFIG_MMU
+	ldr	r1, [r1, #MM_CONTEXT_ID]	@ get mm->context.id
+	mov	r2, #0
+	and	r3, r1, #0xff
+	mov	r3, r3, lsl #(48 - 32)		@ ASID
+	mcrr	p15, 0, r0, r3, c2		@ set TTB 0
+	isb
+#endif
+	mov	pc, lr
+ENDPROC(cpu_pj4bv7_switch_mm)
+
+/*
+ *	cpu_pj4bv7_set_pte_ext(ptep, pte)
+ *
+ *	Set a level 2 translation table entry.
+ *
+ *	- ptep  - pointer to level 2 translation table entry
+ *		  (hardware version is stored at +2048 bytes)
+ *	- pte   - PTE value to store
+ *	- ext	- value for extended PTE bits
+ */
+ENTRY(cpu_pj4bv7_set_pte_ext)
+#ifdef CONFIG_MMU
+	tst	r2, #L_PTE_PRESENT
+	beq	1f
+	tst	r3, #1 << (55 - 32)		@ L_PTE_DIRTY
+	orreq	r2, #L_PTE_RDONLY
+1:	strd	r2, r3, [r0]
+	mcr	p15, 0, r0, c7, c10, 1		@ flush_pte
+#endif
+	mov	pc, lr
+ENDPROC(cpu_pj4bv7_set_pte_ext)
+
+cpu_pj4bv7_name:
+	.ascii	"Marvell - PJ4Bv7 Processor"
+	.align
+
+	/*
+	 * Memory region attributes for LPAE (defined in pgtable-3level.h):
+	 *
+	 *   n = AttrIndx[2:0]
+	 *
+	 *			n	MAIR
+	 *   UNCACHED		000	00000000
+	 *   BUFFERABLE		001	01000100
+	 *   DEV_WC		001	01000100
+	 *   WRITETHROUGH	010	10101010
+	 *   WRITEBACK		011	11101110
+	 *   DEV_CACHED		011	11101110
+	 *   DEV_SHARED		100	00000100
+	 *   DEV_NONSHARED	100	00000100
+	 *   unused		101
+	 *   unused		110
+	 *   WRITEALLOC		111	11111111
+	 */
+.equ	MAIR0,	0xeeaa4400			@ MAIR0
+.equ	MAIR1,	0xff000004			@ MAIR1
+
+/* Suspend/resume support: derived from arch/arm/mach-s5pv210/sleep.S */
+.globl	cpu_v7_suspend_size
+.equ	cpu_v7_suspend_size, 4 * 10
+#ifdef CONFIG_PM_SLEEP
+ENTRY(cpu_v7_do_suspend)
+	stmfd	sp!, {r4 - r11, lr}
+	mrc	p15, 0, r4, c13, c0, 0	@ FCSE/PID
+	mrc	p15, 0, r5, c13, c0, 1	@ Context ID
+	mrc	p15, 0, r6, c3, c0, 0	@ Domain ID
+	mrrc	p15, 0, r7, r8, c2	@ TTB 0
+	mrrc	p15, 1, r2, r3, c2	@ TTB 1
+	mrc	p15, 0, r9, c1, c0, 0	@ Control register
+	mrc	p15, 0, r10, c1, c0, 1	@ Auxiliary control register
+	mrc	p15, 0, r11, c1, c0, 2	@ Co-processor access control
+	stmia	r0, {r2 - r11}
+	ldmfd	sp!, {r4 - r11, pc}
+ENDPROC(cpu_v7_do_suspend)
+
+ENTRY(cpu_v7_do_resume)
+	mov	ip, #0
+	mcr	p15, 0, ip, c8, c7, 0	@ invalidate TLBs
+	mcr	p15, 0, ip, c7, c5, 0	@ invalidate I cache
+	ldmia	r0, {r2 - r11}
+	mcr	p15, 0, r4, c13, c0, 0	@ FCSE/PID
+	mcr	p15, 0, r5, c13, c0, 1	@ Context ID
+	mcr	p15, 0, r6, c3, c0, 0	@ Domain ID
+	mcrr	p15, 0, r7, r8, c2	@ TTB 0
+	mcrr	p15, 1, r2, r3, c2	@ TTB 1
+	mcr	p15, 0, ip, c2, c0, 2	@ TTB control register
+	mcr	p15, 0, r10, c1, c0, 1	@ Auxiliary control register
+	mcr	p15, 0, r11, c1, c0, 2	@ Co-processor access control
+	ldr	r4, =MAIR0
+	ldr	r5, =MAIR1
+	mcr	p15, 0, r4, c10, c2, 0	@ write MAIR0
+	mcr	p15, 0, r5, c10, c2, 1	@ write MAIR1
+	isb
+	mov	r0, r9			@ control register
+	mov	r2, r7, lsr #14		@ get TTB0 base
+	mov	r2, r2, lsl #14
+	ldr	r3, cpu_resume_l1_flags
+	b	cpu_resume_mmu
+ENDPROC(cpu_v7_do_resume)
+cpu_resume_l1_flags:
+	ALT_SMP(.long PMD_TYPE_SECT | PMD_SECT_AP_WRITE | PMD_FLAGS_SMP)
+	ALT_UP(.long  PMD_TYPE_SECT | PMD_SECT_AP_WRITE | PMD_FLAGS_UP)
+#else
+#define cpu_v7_do_suspend	0
+#define cpu_v7_do_resume	0
+#endif
+
+	__CPUINIT
+
+/*
+ *	__pj4bv7_setup
+ *
+ *	Initialise TLB, Caches, and MMU state ready to switch the MMU
+ *	on. Return in r0 the new CP15 C1 control register setting.
+ *
+ *	This should be able to cover all ARMv7 cores with LPAE.
+ *
+ *	It is assumed that:
+ *	- cache type register is implemented
+ */
+__pj4bv7_setup:
+	sheeva_pj4b_config
+
+#ifdef CONFIG_ARMADA_XP_REV_A0
+/* DSMP A0 */
+	/* Auxiliary Debug Modes Control 1 Register */
+	mrc        p15, 1, r0, c15, c1, 1                         /* Read */
+	orr        r0, r0, #0x00020                                /* BIT5 STREX backoff_disable--> '1' enable the back off of STREX instr */
+	orr	   r0, r0, #0x00100                                /* BIT8 Internal Parity Handling Disable--> '1' Disable Internal Parity Handling */
+	mcr        p15, 1, r0, c15, c1, 1                         /* Write */
+
+/* Auxiliary Functional Modes Control Register 0 */
+	mrc        p15, 1, r0, c15, c2, 0                         /* Read */
+#ifdef CONFIG_SMP
+	orr          r0, r0, #0x00002                                /* BIT1 SMP/nAMP --> '1' taking part in coherency */
+#endif
+	orr     r0, r0, #0x00004                                /* BIT2 L1 parity --> '1' Enabled */
+	orr     r0, r0, #0x00100                                /* BIT8 FW --> '1' enable Cache and TLB maintenance broadcast */
+	mcr        p15, 1, r0, c15, c2, 0                         /* Write */
+
+	/* Auxiliary Debug Modes Control 0 Register */
+	mrc        p15, 1, r0, c15, c1, 0                         /* Read */
+	orr          r0, r0, #0x001000                              /* BIT12 ldstm_first Two_sgl --> '1' Force first issue to be single */
+	orr          r0, r0, #0x400000                              /* BIT22 dvm_wakeup_dis --> '1' WFI/WFE will serve the DVM and back to idle */
+	mcr        p15, 1, r0, c15, c1, 0                         /* Write */
+	/* Auxiliary Debug Modes Control 2 Register */
+	mrc        p15, 1, r0, c15, c1, 2                         /* Read */
+	orr          r0, r0, #0x02000000                         /* BIT25 Intervention Interleave Disable--> '1'  Disable Interleaving with Intervention Data */
+	orr          r0, r0, #0x08000000                         /* BIT27 CWF Disable--> '1' Disable critical word first sequencing */
+	orr          r0, r0, #0x20000000                         /* BIT29 DEV Stream Disable--> '1' Disable MO device read / write */
+	orr          r0, r0, #0x40000000                         /* BIT30 L1 Replacement Policy Config--> '1' Strict Round-Robin Replacement Policy  */
+	mcr        p15, 1, r0, c15, c1, 2                         /* Write */
+
+#else
+	mov	r10, #0
+1:
+#ifdef CONFIG_SMP
+	ALT_SMP(mrc	p15, 0, r0, c1, c0, 1)
+	ALT_UP(mov	r0, #(1 << 6))		@ fake it for UP
+	tst	r0, #(1 << 6)			@ SMP/nAMP mode enabled?
+	orreq	r0, r0, #(1 << 6)		@ Enable SMP/nAMP mode
+	orreq	r0, r0, r10			@ Enable CPU-specific SMP bits
+	mcreq	p15, 0, r0, c1, c0, 1
+#endif
+	/* Auxiliary Debug Modes Control 1 Register */
+	mrc        p15, 1, r0, c15, c1, 1                         /* Read */
+	orr        r0, r0, #0x00020                                /* BIT5 STREX backoff_disable--> '1' enable the back off of STREX instr */
+	orr	   r0, r0, #0x00100                                /* BIT8 Internal Parity Handling Disable--> '1' Disable Internal Parity Handling */
+	mcr        p15, 1, r0, c15, c1, 1                         /* Write */
+
+/* Auxiliary Functional Modes Control Register 0 */
+	mrc        p15, 1, r0, c15, c2, 0                         /* Read */
+#ifdef CONFIG_SMP
+	orr          r0, r0, #0x00002                                /* BIT1 SMP/nAMP --> '1' taking part in coherency */
+#endif
+	orr     r0, r0, #0x00004                                /* BIT2 L1 parity --> '1' Enabled */
+	orr     r0, r0, #0x00100                                /* BIT8 FW --> '1' enable Cache and TLB maintenance broadcast */
+	mcr        p15, 1, r0, c15, c2, 0                         /* Write */
+
+	/* Auxiliary Debug Modes Control 0 Register */
+	mrc        p15, 1, r0, c15, c1, 0                         /* Read */
+	orr          r0, r0, #0x400000                              /* BIT22 dvm_wakeup_dis --> '1' WFI/WFE will serve the DVM and back to idle */
+	mcr        p15, 1, r0, c15, c1, 0                         /* Write */
+	/* Auxiliary Debug Modes Control 2 Register */
+	mrc        p15, 1, r0, c15, c1, 2                         /* Read */
+	orr          r0, r0, #0x02000000                         /* BIT25 Intervention Interleave Disable--> '1'  Disable Interleaving with Intervention Data */
+	orr          r0, r0, #0x08000000                         /* BIT27 CWF Disable--> '1' Disable critical word first sequencing */
+	orr          r0, r0, #0x20000000                         /* BIT29 DEV Stream Disable--> '1' Disable MO device read / write */
+	orr          r0, r0, #0x40000000                         /* BIT30 L1 Replacement Policy Config--> '1' Strict Round-Robin Replacement Policy  */
+	mcr        p15, 1, r0, c15, c1, 2                         /* Write */
+
+#endif
+#ifdef CONFIG_CPU_SHEEVA_PJ4B_PMC_ACCESS_IN_USERMODE
+	@ Enable performance counters user access
+	mrc     p15, 0, r0, c9, c14, 0
+	orr     r0, r0, #0x1
+	mcr     p15, 0, r0, c9, c14, 0
+#endif /* CONFIG_CPU_SHEEVA_PJ4B_PMC_ACCESS_IN_USERMODE */
+__v7_setup:
+	adr	r12, __pj4bv7_setup_stack	@ the local stack
+	stmia	r12, {r0-r5, r7, r9, r11, lr}
+	bl	v7_flush_dcache_all
+	ldmia	r12, {r0-r5, r7, r9, r11, lr}
+
+	mov	r10, #0
+	mcr	p15, 0, r10, c7, c5, 0		@ I+BTB cache invalidate
+	dsb
+#ifdef CONFIG_MMU
+	mcr	p15, 0, r10, c8, c7, 0		@ invalidate I + D TLBs
+	mov	r5, #TTB_EAE
+	ALT_SMP(orr	r5, r5, #TTB_FLAGS_SMP)
+	ALT_SMP(orr	r5, r5, #TTB_FLAGS_SMP << 16)
+	ALT_UP(orr	r5, r5, #TTB_FLAGS_UP)
+	ALT_UP(orr	r5, r5, #TTB_FLAGS_UP << 16)
+	mrc	p15, 0, r10, c2, c0, 2
+	orr	r10, r10, r5
+#if PHYS_OFFSET <= PAGE_OFFSET
+	/*
+	 * TTBR0/TTBR1 split (PAGE_OFFSET):
+	 *   0x40000000: T0SZ = 2, T1SZ = 0 (not used)
+	 *   0x80000000: T0SZ = 0, T1SZ = 1
+	 *   0xc0000000: T0SZ = 0, T1SZ = 2
+	 *
+	 * Only use this feature if PAGE_OFFSET <=  PAGE_OFFSET, otherwise
+	 * booting secondary CPUs would end up using TTBR1 for the identity
+	 * mapping set up in TTBR0.
+	 */
+	orr	r10, r10, #(((PAGE_OFFSET >> 30) - 1) << 16)	@ TTBCR.T1SZ
+#endif
+	mcr	p15, 0, r10, c2, c0, 2		@ TTB control register
+	mov	r5, #0
+#if defined CONFIG_VMSPLIT_2G
+	/* PAGE_OFFSET == 0x80000000, T1SZ == 1 */
+	add	r6, r8, #1 << 4			@ skip two L1 entries
+#elif defined CONFIG_VMSPLIT_3G
+	/* PAGE_OFFSET == 0xc0000000, T1SZ == 2 */
+	add	r6, r8, #4096 * (1 + 3)		@ only L2 used, skip pgd+3*pmd
+#else
+	mov	r6, r8
+#endif
+	mcrr	p15, 1, r6, r5, c2		@ load TTBR1
+	ldr	r5, =MAIR0
+	ldr	r6, =MAIR1
+	mcr	p15, 0, r5, c10, c2, 0		@ write MAIR0
+	mcr	p15, 0, r6, c10, c2, 1		@ write MAIR1
+#endif
+	adr	r5, v7_crval
+	ldmia	r5, {r5, r6}
+#ifdef CONFIG_CPU_ENDIAN_BE8
+	orr	r6, r6, #1 << 25		@ big-endian page tables
+#endif
+#ifdef CONFIG_SWP_EMULATE
+	orr     r5, r5, #(1 << 10)              @ set SW bit in "clear"
+	bic     r6, r6, #(1 << 10)              @ clear it in "mmuset"
+#endif
+	mrc	p15, 0, r0, c1, c0, 0		@ read control register
+	bic	r0, r0, r5			@ clear bits them
+	orr	r0, r0, r6			@ set them
+ THUMB(	orr	r0, r0, #1 << 30	)	@ Thumb exceptions
+	mov	pc, lr				@ return to head.S:__ret
+ENDPROC(__pj4bv7_setup)
+
+	/*   AT
+	 *  TFR   EV X F   IHD LR    S
+	 * .EEE ..EE PUI. .TAT 4RVI ZWRS BLDP WCAM
+	 * rxxx rrxx xxx0 0101 xxxx xxxx x111 xxxx < forced
+	 *   11    0 110    1  0011 1100 .111 1101 < we want
+	 */
+	.type	v7_crval, #object
+v7_crval:
+	crval	clear=0x0120c302, mmuset=0x30c23c7d, ucset=0x00c01c7c
+
+__pj4bv7_setup_stack:
+	.space	4 * 11				@ 11 registers
+
+	__INITDATA
+
+	.type	v7_processor_functions, #object
+ENTRY(v7_processor_functions)
+	.word	v7_early_abort
+	.word	v7_pabort
+	.word	cpu_pj4bv7_proc_init
+	.word	cpu_pj4bv7_proc_fin
+	.word	cpu_pj4bv7_reset
+	.word	cpu_pj4bv7_do_idle
+	.word	cpu_pj4bv7_dcache_clean_area
+	.word	cpu_pj4bv7_switch_mm
+	.word	cpu_pj4bv7_set_pte_ext
+	.word	0
+	.word	0
+	.word	0
+	.size	v7_processor_functions, . - v7_processor_functions
+
+	.section ".rodata"
+
+	.type	cpu_arch_name, #object
+cpu_arch_name:
+	.asciz	"armv7"
+	.size	cpu_arch_name, . - cpu_arch_name
+
+	.type	cpu_elf_name, #object
+cpu_elf_name:
+	.asciz	"v7"
+	.size	cpu_elf_name, . - cpu_elf_name
+	.align
+
+	.section ".proc.info.init", #alloc, #execinstr
+
+	.type	__pj4bv7_proc_info, #object
+__pj4bv7_proc_info:
+	.long	0x560f5840		@ Required ID value
+	.long	0xff0ffff0		@ Mask for ID
+	ALT_SMP(.long \
+		PMD_TYPE_SECT | \
+		PMD_SECT_AP_WRITE | \
+		PMD_SECT_AP_READ | \
+		PMD_SECT_AF | \
+		PMD_FLAGS_SMP)
+	ALT_UP(.long \
+		PMD_TYPE_SECT | \
+		PMD_SECT_AP_WRITE | \
+		PMD_SECT_AP_READ | \
+		PMD_SECT_AF | \
+		PMD_FLAGS_UP)
+		/* PMD_SECT_XN is set explicitly in head.S for LPAE */
+	.long   PMD_TYPE_SECT | \
+		PMD_SECT_XN | \
+		PMD_SECT_AP_WRITE | \
+		PMD_SECT_AP_READ | \
+		PMD_SECT_AF
+	b	__pj4bv7_setup
+	.long	cpu_arch_name
+	.long	cpu_elf_name
+	.long	HWCAP_SWP|HWCAP_HALF|HWCAP_THUMB|HWCAP_FAST_MULT|HWCAP_EDSP|HWCAP_TLS
+	.long	cpu_pj4bv7_name
+	.long	v7_processor_functions
+	.long	v7wbi_tlb_fns
+	.long	v6_user_fns
+	.long	v7_cache_fns
+	.size	__pj4bv7_proc_info, . - __pj4bv7_proc_info
+
+	/*
+	 * Match any ARMv7 processor core.
+	 */
+	.type	__v7_proc_info, #object
+__v7_proc_info:
+	.long	0x000f0000		@ Required ID value
+	.long	0x000f0000		@ Mask for ID
+	ALT_SMP(.long \
+		PMD_TYPE_SECT | \
+		PMD_SECT_AP_WRITE | \
+		PMD_SECT_AP_READ | \
+		PMD_SECT_AF | \
+		PMD_FLAGS_SMP)
+	ALT_UP(.long \
+		PMD_TYPE_SECT | \
+		PMD_SECT_AP_WRITE | \
+		PMD_SECT_AP_READ | \
+		PMD_SECT_AF | \
+		PMD_FLAGS_UP)
+		/* PMD_SECT_XN is set explicitly in head.S for LPAE */
+	.long   PMD_TYPE_SECT | \
+		PMD_SECT_XN | \
+		PMD_SECT_AP_WRITE | \
+		PMD_SECT_AP_READ | \
+		PMD_SECT_AF
+	W(b)	__pj4bv7_setup
+	.long	cpu_arch_name
+	.long	cpu_elf_name
+	.long	HWCAP_SWP|HWCAP_HALF|HWCAP_THUMB|HWCAP_FAST_MULT|HWCAP_EDSP|HWCAP_TLS
+	.long	cpu_pj4bv7_name
+	.long	v7_processor_functions
+	.long	v7wbi_tlb_fns
+	.long	v6_user_fns
+	.long	v7_cache_fns
+	.size	__v7_proc_info, . - __v7_proc_info
diff --git a/arch/arm/mm/sheeva_pj4b-macros.S b/arch/arm/mm/sheeva_pj4b-macros.S
new file mode 100644
index 0000000..93f56e6
--- /dev/null
+++ b/arch/arm/mm/sheeva_pj4b-macros.S
@@ -0,0 +1,72 @@
+	.macro	sheeva_pj4b_config
+#ifdef CONFIG_SHEEVA_ERRATA_ARM_CPU_4948
+	mrc 	p15, 1, r0, c15, c1, 0
+	orr     r0, r0, #1			@ Disable L0 cache.
+	mcr 	p15, 1, r0, c15, c1, 0
+#endif
+#ifdef CONFIG_SHEEVA_ERRATA_ARM_CPU_4786
+	mrc 	p15, 1, r0, c15, c1, 0
+	orr     r0, r0, #(1 << 15)		@ Disable Dual issue for coprocessor instructions
+	mcr 	p15, 1, r0, c15, c1, 0
+#endif
+	mrc 	p15, 1, r0, c15, c1, 1
+	orr     r0, r0, #(1 << 16)		@ Disable data transfer for clean line.
+	mcr 	p15, 1, r0, c15, c1, 1
+
+	mrc 	p15, 1, r0, c15, c1, 2 
+	bic     r0, r0, #(1 << 23)		@ Enable fast LDR.
+	orr     r0, r0, #(1 << 25)		@ Disable interleave between normal write and snoop data intervene.
+	orr     r0, r0, #(1 << 27)		@ Disable Critical Word First feature.
+	orr     r0, r0, #(1 << 29)		@ Disable SO/device/EX non cacheable request to get out outstanding.
+	orr     r0, r0, #(1 << 30) 		@ MBU change the way when it get second linefill
+	mcr 	p15, 1, r0, c15, c1, 2
+
+#ifdef CONFIG_SHEEVA_ERRATA_ARM_CPU_5315
+	mrc     p15, 1, r0, c15, c2, 0
+	orr     r0, r0, #(1 << 7)              @ Disable spec read from the MBu/LSU
+        mcr     p15, 1, r0, c15, c2, 0
+#else
+	/* Speculative Prefetch is not enabled here - managed at runtime */
+#endif
+
+#ifdef CONFIG_AURORA_SF_ENABLED
+	mrc 	p15, 1, r0, c15, c1, 2
+	orr     r0, r0, #(1 << 31)		@ Enable Write Evict
+	mcr 	p15, 1, r0, c15, c1, 2
+
+	ldr     r0, =AXP_SNOOP_FILTER_PHYS_REG
+        ldr     r10, [r0]
+        orr     r10 , r10, #0x1                 @ SF_Enable (bit 0)
+        str     r10,[r0] 
+#endif
+
+	.endm
+
+/*
+ * skip_errata_if_axp_b0 - get the chip revision and branch if B0 to skip errata.
+ */
+	.macro	skip_errata_if_axp_b0, reg
+	ldr		\reg, soc_revision
+	cmp		\reg, #0x2		/* MV_78XX0_B0_REV */
+	beq		1f
+	.endm
+
+/*
+ * a0_with_b0_errata_6075 - get the chip revision and branch if B0 to skip errata.
+ */
+	.macro	a0_with_b0_errata_6075, reg
+	mrc      p15, 0, \reg, c1, c0, 0
+	ands	\reg, \reg, #0x1
+	ldr		\reg, =soc_revision
+	bne		1f
+	and		\reg, \reg, #0xFFFFFFF
+1:
+	ldr		\reg, [\reg]
+	cmp		\reg, #0x2		/* MV_78XX0_B0_REV */
+	beq		2f
+	dsb
+	b 		3f
+2:
+	dmb
+3:
+	.endm
-- 
1.7.0

