From bd4eb8160612ea18974f0b086551844dee6b66ea Mon Sep 17 00:00:00 2001
From: Dovrat <dovrat@marvell.com>
Date: Sun, 13 Apr 2014 16:43:23 +0300
Subject: [PATCH 1564/1825] pp3: bm: pool_pair is added to gp pool api

https://github.com/MISL-EBU-System-SW/misl-windriver.git linux-3.4.69-14t2-read
commit 85ce231553b61947ed1ff7b9a1bae635a01495d3

	verification was added/fixes
	error printout were fixed
        macro fixes

Change-Id: I655d76efa50514f5bbba66587863fdba55ecfafc
Signed-off-by: Dovrat <dovrat@marvell.com>
Reviewed-on: http://vgitil04.il.marvell.com:8080/7123
Reviewed-by: Eliezer Ben Zeev <eliezerb@marvell.com>
Tested-by: Star_Automation <star@marvell.com>
Reviewed-by: Uri Eliyahu <uriel@marvell.com>
Reviewed-by: Dmitri Epshtein <dima@marvell.com>
Tested-by: Dmitri Epshtein <dima@marvell.com>
Signed-off-by: Zhong Hongbo <hongbo.zhong@windriver.com>
---
 drivers/net/ethernet/marvell/pp3/bm/mv_bm.c       |  155 ++++++++++++++-------
 drivers/net/ethernet/marvell/pp3/bm/mv_bm.h       |   47 ++++---
 drivers/net/ethernet/marvell/pp3/bm/mv_bm_sysfs.c |   64 +++++++--
 3 files changed, 188 insertions(+), 78 deletions(-)

diff --git a/drivers/net/ethernet/marvell/pp3/bm/mv_bm.c b/drivers/net/ethernet/marvell/pp3/bm/mv_bm.c
index ccea47b..4ca025d 100644
--- a/drivers/net/ethernet/marvell/pp3/bm/mv_bm.c
+++ b/drivers/net/ethernet/marvell/pp3/bm/mv_bm.c
@@ -303,11 +303,6 @@ int bm_qm_gpm_pools_def_quick_init(u32 num_of_buffers, struct mv_a40 *qece_base_
 	int rc = -BM_INPUT_NOT_IN_RANGE;
 	u32 ae_thr, af_thr, cache_vmid, cache_attr, cache_so_thr, cache_si_thr, cache_num_of_buffers;
 
-	if (num_of_buffers != BM_NUM_OF_BUFFERS_QM_GPM_MAX)	{
-		pr_err("Default number of Buffer should be %d, Make sure you allocate the correct pool size\n",
-			BM_NUM_OF_BUFFERS_QM_GPM_MAX);
-		return rc;
-	}
 	ae_thr               = BM_AE_THR_DEF(num_of_buffers);
 	af_thr               = BM_AF_THR_DEF(num_of_buffers);
 	cache_vmid           = BM_CACHE_VMID_DEF;
@@ -327,11 +322,6 @@ int bm_qm_dram_pools_def_quick_init(u32 num_of_buffers, struct mv_a40 *qece_base
 	int rc = -BM_INPUT_NOT_IN_RANGE;
 	u32 ae_thr, af_thr, cache_vmid, cache_attr, cache_so_thr, cache_si_thr, cache_num_of_buffers;
 
-	if (num_of_buffers != BM_NUM_OF_BUFFERS_QM_GPM_MAX)	{
-		pr_err("Default number of Buffer should be %d, Make sure you allocate the correct pool size\n",
-			BM_NUM_OF_BUFFERS_QM_GPM_MAX);
-		return rc;
-	}
 	ae_thr               = BM_AE_THR_DEF(num_of_buffers);
 	af_thr               = BM_AF_THR_DEF(num_of_buffers);
 	cache_vmid           = BM_CACHE_VMID_DEF;
@@ -370,7 +360,7 @@ int bm_qm_gpm_pools_quick_init(u32 num_of_buffers, struct mv_a40 *qece_base_addr
 	granularity_of_pe_in_cache = GRANULARITY_OF_64_BYTES / QM_PE_SIZE_IN_BYTES_IN_CACHE;	/* 64/8 */
 
 	if (num_of_buffers < cache_num_of_buffers) {
-		pr_err("Number of buffers in DRAM should be equal or larger than cache number of buffers\n");
+		pr_err("number of buffers in dram should be equal or larger than cache number of buffers\n");
 		return rc;
 	}
 	if       ((num_of_buffers % granularity_of_pe_in_dram)  != 0) {
@@ -474,7 +464,7 @@ int bm_qm_gpm_pools_quick_init(u32 num_of_buffers, struct mv_a40 *qece_base_addr
 		return rc;
 
 	if (bm_req_rcv_en == 1) {
-		pr_err("QM GPM pool should be enabled before BM module is enabled\n");
+		pr_err("qm gpm pool should be enabled before BM module is enabled\n");
 		rc = -BM_ATTR_CHANGE_AFTER_BM_ENABLE;
 		return rc;
 	}
@@ -502,7 +492,7 @@ int bm_qm_gpm_pools_quick_init(u32 num_of_buffers, struct mv_a40 *qece_base_addr
 	if (rc != OK)
 		return rc;
 
-	pr_info("Pool 0 is enabled\n");
+	pr_info("pool 0 is enabled\n");
 	pool = 1;
 	base_address.dma_msb = qece_base_address->dma_msb;
 	base_address.dma_lsb = qece_base_address->dma_lsb;
@@ -523,7 +513,7 @@ int bm_qm_gpm_pools_quick_init(u32 num_of_buffers, struct mv_a40 *qece_base_addr
 	if (rc != OK)
 		return rc;
 	rc = bm_pool_enable(pool, quick_init);
-	pr_info("Pool 1 is enabled\n");
+	pr_info("pool 1 is enabled\n");
 	return rc;
 }
 
@@ -542,7 +532,7 @@ int bm_qm_dram_pools_quick_init(u32 num_of_buffers, struct mv_a40 *qece_base_add
 	granularity_of_pe_in_cache = GRANULARITY_OF_64_BYTES / QM_PE_SIZE_IN_BYTES_IN_CACHE;	/* 64/4 */
 
 	if (num_of_buffers < cache_num_of_buffers) {
-		pr_err("Number of Buffer in DRAM should be equal or larger than cache number of buffers\n");
+		pr_err("number of buffers in dram should be equal or larger than cache number of buffers\n");
 		return rc;
 	}
 	if       ((num_of_buffers % granularity_of_pe_in_dram)  != 0)
@@ -696,15 +686,15 @@ int bm_pool_quick_init_status_get(u32 pool, u32 *completed)
 	struct bm_pool_st   reg_pool_st;
 
 	if ((pool           <     BM_POOL_MIN) || (pool           >     BM_POOL_MAX)) {
-		pr_err("Wrong pool number %d\n", pool);
+		pr_err("wrong pool number %d\n", pool);
 		return rc;
 	}
 	if ((pool           >  BM_POOL_QM_MAX) && (pool           <  BM_POOL_GP_MIN)) {
-		pr_err("Wrong pool number %d\n", pool);
+		pr_err("wrong pool number %d\n", pool);
 		return rc; /* pools 4, 5, 6, 7 don't exist */
 	}
 	if (((u32)completed < BM_DATA_PTR_MIN) || ((u32)completed > BM_DATA_PTR_MAX)) {
-		pr_err("Something is wrong\n");
+		pr_err("something is wrong\n");
 		return rc;
 	}
 
@@ -725,14 +715,15 @@ int bm_pool_quick_init_status_get(u32 pool, u32 *completed)
 	return rc;
 }
 
-int bm_gp_pool_def_basic_init(u32 pool, u32 num_of_buffers, struct mv_a40 *base_address, u32 partition_model)
+int bm_gp_pool_def_basic_init(u32 pool, u32 num_of_buffers, struct mv_a40 *base_address,
+	u32 partition_model, u32 pool_pair)
 {
 	int rc = -BM_INPUT_NOT_IN_RANGE;
-	u32 pe_size, pool_pair, ae_thr, af_thr, cache_vmid, cache_attr,
+	u32 pe_size, ae_thr, af_thr, cache_vmid, cache_attr,
 			cache_so_thr, cache_si_thr, cache_num_of_buffers;
 
 	pe_size              = BM_PE_SIZE_DEF;
-	pool_pair            = BM_POOL_PAIR_GP_DEF;
+	/*pool_pair            = BM_POOL_PAIR_GP_DEF;*/
 	ae_thr               = BM_AE_THR_DEF(num_of_buffers);
 	af_thr               = BM_AF_THR_DEF(num_of_buffers);
 	cache_vmid           = BM_CACHE_VMID_DEF;
@@ -746,8 +737,10 @@ int bm_gp_pool_def_basic_init(u32 pool, u32 num_of_buffers, struct mv_a40 *base_
 		cache_si_thr         = BM_CACHE_SI_THR_GP_SMALL_DEF;
 		cache_so_thr         = BM_CACHE_SO_THR_GP_SMALL_DEF;
 		cache_num_of_buffers = BM_CACHE_NUM_OF_BUFFERS_GP_SMALL_DEF;
-	} else
+	} else {
+		pr_err("wrong partition module %d. should be 0 for large, 1 for small\n", partition_model);
 		return rc;
+	}
 
 	rc = bm_gp_pool_basic_init(pool, num_of_buffers, base_address, pe_size, pool_pair,
 					ae_thr, af_thr, cache_vmid, cache_attr, cache_so_thr, cache_si_thr,
@@ -774,60 +767,122 @@ int bm_gp_pool_basic_init(u32 pool, u32 num_of_buffers, struct mv_a40 *base_addr
 			GRANULARITY_OF_64_BYTES / GP_PE_SIZE_OF_32_BITS_IN_BYTES_IN_DRAM;	/* 64/4 */
 		granularity_of_pe_in_cache =
 			GRANULARITY_OF_64_BYTES / GP_PE_SIZE_IN_BYTES_IN_CACHE;			/* 64/8 */
-	} else
+	} else {
+		pr_err("wrong pe size %d. should be %d or %d\n", pe_size, BM_PE_SIZE_IS_32_BITS, BM_PE_SIZE_IS_40_BITS);
 		return rc;
+	}
 
-	if       ((num_of_buffers %  granularity_of_pe_in_dram) != 0)
+	if       ((num_of_buffers %  granularity_of_pe_in_dram) != 0) {
+		pr_err("wrong number of buffers %d. should be in granularity of %d\n",
+			num_of_buffers, granularity_of_pe_in_dram);
 		return rc;
-	if               ((ae_thr %  granularity_of_pe_in_dram) != 0)
+	}
+	if               ((ae_thr %  granularity_of_pe_in_dram) != 0) {
+		pr_err("wrong ae threshold %d. should be in granularity of %d\n", ae_thr, granularity_of_pe_in_dram);
 		return rc;
-	if               ((af_thr %  granularity_of_pe_in_dram) != 0)
+	}
+	if               ((af_thr %  granularity_of_pe_in_dram) != 0) {
+		pr_err("wrong af hreshold %d. should be in granularity of %d\n", af_thr, granularity_of_pe_in_dram);
 		return rc;
-	if ((base_address->dma_lsb %  granularity_of_pe_in_dram) != 0)
+	}
+	if ((base_address->dma_lsb %  granularity_of_pe_in_dram) != 0) {
+		pr_err("wrong address %x. should be in granularity of %d\n",
+			base_address->dma_lsb, granularity_of_pe_in_dram);
 		return rc;
-	if ((cache_num_of_buffers % granularity_of_pe_in_cache) != 0)
+	}
+	if ((cache_num_of_buffers % granularity_of_pe_in_cache) != 0) {
+		pr_err("wrong number of buffers in cache %d. should be in granularity of %d\n",
+			cache_num_of_buffers, granularity_of_pe_in_cache);
 		return rc;
-	if (af_thr       < ae_thr)
+	}
+	if (af_thr				  < ae_thr) {
+		pr_err("almost full threshold %d should be larger or equal to almost empty threshold %d\n",
+			af_thr, ae_thr);
 		return rc;
-	if (cache_so_thr < cache_si_thr + 16)
+	}
+	if (cache_so_thr		  < cache_si_thr + 16) {
+		pr_err("cache so threshold %d should be larger to cache si threshold %d +16\n",
+			cache_so_thr, cache_si_thr);
 		return rc;
-
-	if ((pool                 <          BM_POOL_GP_MIN) || (pool                 >          BM_POOL_GP_MAX))
+	}
+	if ((pool                 <          BM_POOL_GP_MIN) || (pool                 >          BM_POOL_GP_MAX)) {
+		pr_err("wrong pool number %d. should be in range %d..%d\n", pool, BM_POOL_GP_MIN, BM_POOL_GP_MAX);
 		return rc;
-	if ((num_of_buffers       < BM_NUM_OF_BUFFERS_GP_MIN) || (num_of_buffers       > BM_NUM_OF_BUFFERS_GP_MAX))
+	}
+	if ((num_of_buffers       < BM_NUM_OF_BUFFERS_GP_MIN) || (num_of_buffers       > BM_NUM_OF_BUFFERS_GP_MAX)) {
+		pr_err("wrong number of buffer %d. should be in range %d..%d\n",
+			num_of_buffers, BM_NUM_OF_BUFFERS_GP_MIN, BM_NUM_OF_BUFFERS_GP_MAX);
 		return rc;
+	}
 	if ((base_address->dma_msb < BM_DRAM_ADDRESS_MIN_MSB) ||
-		(base_address->dma_msb > BM_DRAM_ADDRESS_MAX_MSB))
+		(base_address->dma_msb > BM_DRAM_ADDRESS_MAX_MSB)) {
+		pr_err("problem with base address\n");
 		return rc;
+	}
 	if ((base_address->dma_msb == BM_DRAM_ADDRESS_MIN_MSB) &&
 		(base_address->dma_lsb <  BM_DRAM_ADDRESS_MIN_LSB)) {
-		pr_err("problem with qece base address\n");
+		pr_err("problem with base address\n");
 		return rc;
 	}
 	if ((base_address->dma_msb == BM_DRAM_ADDRESS_MAX_MSB) &&
 		(base_address->dma_lsb >  BM_DRAM_ADDRESS_MAX_LSB)) {
-		pr_err("problem with qece base address\n");
+		pr_err("problem with base address\n");
 		return rc;
 	}
-	if ((pe_size              <          BM_PE_SIZE_MIN) || (pe_size              >          BM_PE_SIZE_MAX))
+	if ((pe_size              <          BM_PE_SIZE_MIN) ||
+		(pe_size              >          BM_PE_SIZE_MAX)) {
+		pr_err("wrong pe_size %d. should be in range %d..%d\n",
+			pe_size, BM_PE_SIZE_MIN, BM_PE_SIZE_MAX);
 		return rc;
-	if ((pool_pair            <        BM_POOL_PAIR_MIN) || (pool_pair            >        BM_POOL_PAIR_MAX))
+	}
+	if ((pool_pair            <        BM_POOL_PAIR_MIN) ||
+		(pool_pair            >        BM_POOL_PAIR_MAX)) {
+		pr_err("wrong pool_pair %d. should be in range %d..%d\n",
+			pool_pair, BM_POOL_PAIR_MIN, BM_POOL_PAIR_MAX);
 		return rc;
-	if ((ae_thr               < BM_AE_THR_MIN(num_of_buffers)) || (ae_thr         > BM_AE_THR_MAX(num_of_buffers)))
+	}
+	if ((ae_thr               < BM_AE_THR_MIN(num_of_buffers)) ||
+		(ae_thr               > BM_AE_THR_MAX(num_of_buffers))) {
+		pr_err("wrong ae_thr %d. should be in range %d..%d\n",
+			ae_thr, BM_AE_THR_MIN(num_of_buffers), BM_AE_THR_MAX(num_of_buffers));
 		return rc;
-	if ((af_thr               < BM_AF_THR_MIN(num_of_buffers)) || (af_thr         > BM_AF_THR_MAX(num_of_buffers)))
+	}
+	if ((af_thr               < BM_AF_THR_MIN(num_of_buffers)) ||
+		(af_thr               > BM_AF_THR_MAX(num_of_buffers))) {
+		pr_err("wrong af_thr %d. should be in range %d..%d\n",
+			af_thr, BM_AF_THR_MIN(num_of_buffers), BM_AF_THR_MAX(num_of_buffers));
 		return rc;
-	if ((cache_vmid           <       BM_CACHE_VMID_MIN) || (cache_vmid           >       BM_CACHE_VMID_MAX))
+	}
+	if ((cache_vmid           <       BM_CACHE_VMID_MIN) ||
+		(cache_vmid           >       BM_CACHE_VMID_MAX)) {
+		pr_err("wrong cache_vmid %d. should be in range %d..%d\n",
+			cache_vmid, BM_CACHE_VMID_MIN, BM_CACHE_VMID_MAX);
 		return rc;
-	if ((cache_attr           <       BM_CACHE_ATTR_MIN) || (cache_attr           >       BM_CACHE_ATTR_MAX))
+	}
+	if ((cache_attr           <       BM_CACHE_ATTR_MIN) ||
+		(cache_attr           >       BM_CACHE_ATTR_MAX)) {
+		pr_err("wrong cache_attr %d. should be in range %d..%d\n",
+			cache_attr, BM_CACHE_ATTR_MIN, BM_CACHE_ATTR_MAX);
 		return rc;
-	if ((cache_so_thr         < BM_CACHE_SO_THR_MIN) || (cache_so_thr > BM_CACHE_SO_THR_MAX(cache_num_of_buffers)))
+	}
+	if ((cache_so_thr         < BM_CACHE_SO_THR_MIN) ||
+		(cache_so_thr         > BM_CACHE_SO_THR_MAX(cache_num_of_buffers))) {
+		pr_err("wrong cache_so_thr %d. should be in range %d..%d\n",
+			cache_so_thr, BM_CACHE_SO_THR_MIN, BM_CACHE_SO_THR_MAX(cache_num_of_buffers));
 		return rc;
-	if ((cache_si_thr         < BM_CACHE_SI_THR_MIN) || (cache_si_thr > BM_CACHE_SI_THR_MAX(cache_num_of_buffers)))
+	}
+	if ((cache_si_thr         < BM_CACHE_SI_THR_MIN) ||
+		(cache_si_thr         > BM_CACHE_SI_THR_MAX(cache_num_of_buffers))) {
+		pr_err("wrong cache_si_thr %d. should be in range %d..%d\n",
+			cache_si_thr, BM_CACHE_SI_THR_MIN, BM_CACHE_SI_THR_MAX(cache_num_of_buffers));
 		return rc;
+	}
 	if ((cache_num_of_buffers < BM_CACHE_NUM_OF_BUFFERS_GP_MIN)	||
-		(cache_num_of_buffers > BM_CACHE_NUM_OF_BUFFERS_GP_MAX))
+		(cache_num_of_buffers > BM_CACHE_NUM_OF_BUFFERS_GP_MAX)) {
+		pr_err("wrong cache_num_of_buffers %d. should be in range %d..%d\n",
+			cache_num_of_buffers, BM_CACHE_NUM_OF_BUFFERS_GP_MIN, BM_CACHE_NUM_OF_BUFFERS_GP_MAX);
 		return rc;
+	}
 
 	rc = bm_pool_dram_set(pool, num_of_buffers, pe_size, base_address, ae_thr, af_thr);
 	if (rc != OK)
@@ -977,15 +1032,15 @@ int bm_vmid_set(u32 bm_vmid)
 }
 
 int bm_gp_pool_def_quick_init(u32 pool, u32 num_of_buffers, u32 fill_level,
-					struct mv_a40 *base_address, u32 partition_model)
+					struct mv_a40 *base_address, u32 partition_model, u32 pool_pair)
 {
 	int rc = -BM_INPUT_NOT_IN_RANGE;
-	u32 pe_size, pool_pair, ae_thr, af_thr, cache_vmid, cache_attr,
+	u32 pe_size, ae_thr, af_thr, cache_vmid, cache_attr,
 					cache_so_thr, cache_si_thr, cache_num_of_buffers;
 /*	u32 large_cache;*/
 
 	pe_size              = BM_PE_SIZE_IS_32_BITS;
-	pool_pair            = BM_POOL_PAIR_GP_DEF;
+	/*pool_pair            = BM_POOL_PAIR_GP_DEF;*/
 	ae_thr               = BM_AE_THR_DEF(num_of_buffers);
 	af_thr               = BM_AF_THR_DEF(num_of_buffers);
 	cache_vmid           = BM_CACHE_VMID_DEF;
@@ -2868,6 +2923,10 @@ int bm_pool_cache_set(u32 pool, u32 cache_vmid, u32 cache_attr, u32 cache_so_thr
 		return rc;
 	if (cache_so_thr < cache_si_thr + 16)
 		return rc;
+	if (cache_so_thr % 4 != 0)
+		return rc;
+	if (cache_si_thr % 4 != 0)
+		return rc;
 
 	if ((pool                 <          BM_POOL_MIN) || (pool                 >          BM_POOL_MAX))
 		return rc;
diff --git a/drivers/net/ethernet/marvell/pp3/bm/mv_bm.h b/drivers/net/ethernet/marvell/pp3/bm/mv_bm.h
index 746d92b..9048ed1 100644
--- a/drivers/net/ethernet/marvell/pp3/bm/mv_bm.h
+++ b/drivers/net/ethernet/marvell/pp3/bm/mv_bm.h
@@ -238,33 +238,33 @@ SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 #define BM_DATA_PTR_MAX				0xFFFFFFFF
 
 #define BM_PID_TO_BANK(_pid)                \
-	(((_pid >= 0) && (_pid   <=  3)) ?	0 :	\
-	(((_pid >= 8) && (_pid%4 ==  0)) ?	1 :	\
-	(((_pid >= 8) && (_pid%4 ==  1)) ?	2 :	\
-	(((_pid >= 8) && (_pid%4 ==  2)) ?	3 :	\
-	(((_pid >= 8) && (_pid%4 ==  3)) ?	4 :	\
+	((((_pid) >= 0) && ((_pid)   <=  3)) ?	0 :	\
+	((((_pid) >= 8) && ((_pid)%4 ==  0)) ?	1 :	\
+	((((_pid) >= 8) && ((_pid)%4 ==  1)) ?	2 :	\
+	((((_pid) >= 8) && ((_pid)%4 ==  2)) ?	3 :	\
+	((((_pid) >= 8) && ((_pid)%4 ==  3)) ?	4 :	\
 	-1)))))
 
 #define BM_PID_TO_PID_LOCAL(_pid)                    \
-	(((_pid >= 0) && (_pid   <=  3)) ?  _pid       : \
-	(((_pid >= 8) && (_pid   <= 35)) ? (_pid-8)>>2 : \
+	((((_pid) >= 0) && ((_pid)   <=  3)) ?  (_pid)       : \
+	((((_pid) >= 8) && ((_pid)   <= 35)) ? ((_pid)-8)>>2 : \
 	-1))
 
 #define BM_PID_TO_GLOBAL_POOL_IDX(_pid)              \
-	(((_pid >= 0) && (_pid   <=  3)) ?  _pid       : \
-	(((_pid >= 8) && (_pid   <= 35)) ? (_pid-4)    : \
+	((((_pid) >= 0) && ((_pid)   <=  3)) ?  (_pid)       : \
+	((((_pid) >= 8) && ((_pid)   <= 35)) ? ((_pid)-4)    : \
 	-1))
 
 #define BM_GLOBAL_POOL_IDX_TO_PID(_pid)              \
-	(((_pid >= 0) && (_pid   <=  3)) ?  _pid       : \
-	(((_pid >= 4) && (_pid   <= 31)) ? (_pid+4)    : \
+	((((_pid) >= 0) && ((_pid)   <=  3)) ?  (_pid)       : \
+	((((_pid) >= 4) && ((_pid)   <= 31)) ? ((_pid)+4)    : \
 	-1))
 
-#define TRUNCATE(_truncated_value, _truncating_value) (_truncated_value - (_truncated_value % _truncating_value))
+#define TRUNCATE(_truncated_value, _truncating_value) ((_truncated_value) - ((_truncated_value) % (_truncating_value)))
 
-#define BM_QM_PE_UNITS_TO_BYTES(_num_of_pes) (_num_of_pes * 4)
+#define BM_QM_PE_UNITS_TO_BYTES(_num_of_pes) ((_num_of_pes) * 4)
 #define BM_GP_PE_UNITS_TO_BYTES(_num_of_pes, _pe_size)	\
-				(((_pe_size) == (40)) ? (_num_of_pes * 8) : (_num_of_pes * 4))
+				(((_pe_size) == (40)) ? ((_num_of_pes) * 8) : ((_num_of_pes) * 4))
 
 /**
  *  Initialize BM module
@@ -370,7 +370,10 @@ int bm_gp_pool_def_basic_init(
 							u32 num_of_buffers, /* equal or less
 								than num_of_buffers passed when initializing pool */
 							struct mv_a40 *base_address,  /* DRAM base address */
-							u32 partition_model);  /* for small partition in cache */
+							u32 partition_model, /* 1 - small partition in cache,
+										0 - large partition in cache*/
+							u32 pool_pair); /* 1 - alloc/release of PE is in pairs,
+										0 - not in pairs */
 
 /**
  *  Basic initialization of general purpose pools
@@ -380,7 +383,7 @@ int bm_gp_pool_def_basic_init(
 int bm_gp_pool_basic_init(
 							u32 pool, /* pool number: general purpose pools 8 to 35 */
 							u32 num_of_buffers, /* equal or less
-								than num_of_buffers passed when initializing pool */
+								than num_of_buffers used for allocating the pool */
 							struct mv_a40 *base_address,  /* DRAM base address */
 							u32 pe_size, /* PE size can be either 32bits or 40 bits */
 							u32 pool_pair, /* Pool_pair is
@@ -432,8 +435,16 @@ int bm_vmid_set(u32 bm_vmid);
  *  Return values:
  *		0 - success
  */
-int bm_gp_pool_def_quick_init(u32 pool, u32 num_of_buffers, u32 fill_level,
-							struct mv_a40 *base_address, u32 partition_model);
+int bm_gp_pool_def_quick_init(
+							u32 pool, /* pool number: general purpose pools 8 to 35 */
+							u32 num_of_buffers, /* equal or less
+								than num_of_buffers used for allocating the pool */
+							u32 fill_level, /* how filled is the pool with PE's */
+							struct mv_a40 *base_address, /* base address of pool in DRAM */
+							u32 partition_model, /* 1 - small partition in cache,
+										0 - large partition in cache */
+							u32 pool_pair); /* 1 - alloc/release of PE is in pairs,
+										0 - alloc/release not in pairs */
 
 /**
  *  Configure BM registers and allocate memory for pools
diff --git a/drivers/net/ethernet/marvell/pp3/bm/mv_bm_sysfs.c b/drivers/net/ethernet/marvell/pp3/bm/mv_bm_sysfs.c
index 75bdc72..0f26597 100644
--- a/drivers/net/ethernet/marvell/pp3/bm/mv_bm_sysfs.c
+++ b/drivers/net/ethernet/marvell/pp3/bm/mv_bm_sysfs.c
@@ -118,7 +118,8 @@ static ssize_t mv_bm_help(char *b)
 	o += scnprintf(b+o, s-o, "echo p qi > pool_enable                  - Enables BM pool\n");
 	o += scnprintf(b+o, s-o, "echo p ps > gp_pool_pe_size_set          - Set PE pointer size in GP pool\n");
 	o += scnprintf(b+o, s-o, "echo p pp > gp_pool_pair_set             - Configure pool works in pairs\n");
-	o += scnprintf(b+o, s-o, "echo p nb pm       > gp_pool_def_basic_init - Default basic init of gp pools\n");
+	o += scnprintf(b+o, s-o, "echo p nb pm pp    > gp_pool_def_basic_init - Default basic init of gp pools\n");
+	o += scnprintf(b+o, s-o, "echo p nb fl pm pp > gp_pool_def_quick_init - Default quick init of gp pools\n");
 	o += scnprintf(b+o, s-o, "echo p nb ps qi    > pool_fill_level_set    - Conf Fill level of pool in DRAM\n");
 	o += scnprintf(b+o, s-o, "echo p pn dpe dpf  > pool_status_get        - not empty almost full/empty\n");
 	o += scnprintf(b+o, s-o, "echo p nb ps et ft > pool_dram_set          - Conf Fill level of pool in DRAM\n");
@@ -129,7 +130,6 @@ static ssize_t mv_bm_help(char *b)
 	o += scnprintf(b+o, s-o, "echo p cid ca cot cit cnb > pool_cache_set                       - Conf cache\n");
 	o += scnprintf(b+o, s-o, "echo nb et ft id ca cot cit, cnb  > qm_gpm_pools_quick_init      - Ini QM GPM\n");
 	o += scnprintf(b+o, s-o, "echo p nb ps pp et ft id ca cot cit cnb > gp_pool_basic_init     - B initi GP\n");
-	o += scnprintf(b+o, s-o, "echo p nb fl pm > gp_pool_def_quick_init                         - Def GP ini\n");
 	o += scnprintf(b+o, s-o, "echo p nb fl ps pp et at cid ca cot cit cnb > gp_pool_quick_init - GP ini\n");
 	o += scnprintf(b+o, s-o, "echo nb et ft id ca cot cit, cnb > qm_dram_pools_quick_init      - Ini QM DRAM\n");
 
@@ -167,7 +167,7 @@ static ssize_t mv_bm_show(struct device *dev,
 	if (!strcmp(name, "status")) {
 		u32 status = 0;
 		pr_info("bm_enable_status_get: ");
-		/*bm_enable_status_get(&status);*/
+		bm_enable_status_get(&status);
 		pr_info("status is %d\n", status);
 	} else if (!strcmp(name, "help")) {
 		off = mv_bm_help(buf);
@@ -245,6 +245,10 @@ static ssize_t mv_bm_config(struct device *dev,
 		PR_INFO_CALLED
 		num_of_buffers = qece_base_address.dma_msb = qece_base_address.virt_msb = pl_base_address.dma_msb = pl_base_address.virt_msb = 0;
 		sscanf(buf, "%d", &num_of_buffers);
+		if (num_of_buffers == 0) {
+			pr_err("wrong number of buffers. should be larger than 0\n");
+			return -1;
+		}
 		qece_base_address.virt_lsb = (u32)dma_alloc_coherent(NULL, num_of_buffers*4, &qece_base_address.dma_lsb, GFP_KERNEL);
 		pl_base_address.virt_lsb   = (u32)dma_alloc_coherent(NULL, num_of_buffers*4, &pl_base_address.dma_lsb, GFP_KERNEL);
 		rc = bm_qm_gpm_pools_def_quick_init(num_of_buffers, &qece_base_address, &pl_base_address);
@@ -257,9 +261,13 @@ static ssize_t mv_bm_config(struct device *dev,
 		/* Read input values */
 		PR_INFO_CALLED
 		num_of_buffers = qece_base_address.dma_msb = qece_base_address.virt_msb = pl_base_address.dma_msb = pl_base_address.virt_msb = 0;
+		sscanf(buf, "%d", &num_of_buffers);
+		if (num_of_buffers == 0) {
+			pr_err("wrong number of buffers. should be larger than 0\n");
+			return -1;
+		}
 		qece_base_address.virt_lsb = (u32)dma_alloc_coherent(NULL, num_of_buffers*4, &qece_base_address.dma_lsb, GFP_KERNEL);
 		pl_base_address.virt_lsb   = (u32)dma_alloc_coherent(NULL, num_of_buffers*4, &pl_base_address.dma_lsb, GFP_KERNEL);
-		sscanf(buf, "%d", &num_of_buffers);
 		rc = bm_qm_dram_pools_def_quick_init(num_of_buffers, &qece_base_address, &pl_base_address);
 		if (rc != OK)
 			PR_ERR_CODE(rc)
@@ -272,6 +280,10 @@ static ssize_t mv_bm_config(struct device *dev,
 		num_of_buffers = qece_base_address.dma_msb = qece_base_address.virt_msb = pl_base_address.dma_msb = pl_base_address.virt_msb = 0;
 		ae_thr = af_thr = cache_vmid = cache_attr = cache_so_thr = cache_si_thr = cache_num_of_buffers = 0xFFFFFFFF;
 		sscanf(buf, "%d %d %d %d %d %d %d %d", &num_of_buffers, &ae_thr, &af_thr, &cache_vmid, &cache_attr, &cache_so_thr, &cache_si_thr, &cache_num_of_buffers);
+		if (num_of_buffers == 0) {
+			pr_err("wrong number of buffers. should be larger than 0\n");
+			return -1;
+		}
 		qece_base_address.virt_lsb = (u32)dma_alloc_coherent(NULL, num_of_buffers*4, &qece_base_address.dma_lsb, GFP_KERNEL);
 		pl_base_address.virt_lsb   = (u32)dma_alloc_coherent(NULL, num_of_buffers*4, &pl_base_address.dma_lsb, GFP_KERNEL);
 		rc = bm_qm_gpm_pools_quick_init(num_of_buffers, &qece_base_address, &pl_base_address, ae_thr, af_thr, cache_vmid, cache_attr, cache_so_thr, cache_si_thr, cache_num_of_buffers);
@@ -286,6 +298,10 @@ static ssize_t mv_bm_config(struct device *dev,
 		num_of_buffers = qece_base_address.dma_msb = qece_base_address.virt_msb = pl_base_address.dma_msb = pl_base_address.virt_msb = 0;
 		ae_thr = af_thr = cache_vmid = cache_attr = cache_so_thr = cache_si_thr = cache_num_of_buffers = 0xFFFFFFFF;
 		sscanf(buf, "%d %d %d %d %d %d %d %d", &num_of_buffers, &ae_thr, &af_thr, &cache_vmid, &cache_attr, &cache_so_thr, &cache_si_thr, &cache_num_of_buffers);
+		if (num_of_buffers == 0) {
+			pr_err("wrong number of buffers. should be larger than 0\n");
+			return -1;
+		}
 		qece_base_address.virt_lsb = (u32)dma_alloc_coherent(NULL, num_of_buffers*4, &qece_base_address.dma_lsb, GFP_KERNEL);
 		pl_base_address.virt_lsb   = (u32)dma_alloc_coherent(NULL, num_of_buffers*4, &pl_base_address.dma_lsb, GFP_KERNEL);
 		rc = bm_qm_dram_pools_quick_init(num_of_buffers, &qece_base_address, &pl_base_address, ae_thr, af_thr, cache_vmid, cache_attr, cache_so_thr, cache_si_thr, cache_num_of_buffers);
@@ -303,16 +319,20 @@ static ssize_t mv_bm_config(struct device *dev,
 			PR_ERR_CODE(rc)
 		pr_info("\t completed = %d\n", completed);
 	} else if (!strcmp(name, "gp_pool_def_basic_init")) {
-		u32 pool, num_of_buffers, partition_model;
+		u32 pool, num_of_buffers, partition_model, pool_pair;
 		struct mv_a40 base_address;
 
 		/* Read input values */
 		PR_INFO_CALLED
 		num_of_buffers = base_address.dma_msb = base_address.virt_msb = 0;
-		pool = partition_model = 0xFFFFFFFF;
-		sscanf(buf, "%d %d %d", &pool, &num_of_buffers, &partition_model);
+		pool = partition_model = pool_pair = 0xFFFFFFFF;
+		sscanf(buf, "%d %d %d %d", &pool, &num_of_buffers, &partition_model, &pool_pair);
+		if (num_of_buffers == 0) {
+			pr_err("wrong number of buffers. should be larger than 0\n");
+			return -1;
+		}
 		base_address.virt_lsb = (u32)dma_alloc_coherent(NULL, num_of_buffers*4, &base_address.dma_lsb, GFP_KERNEL);
-		rc = bm_gp_pool_def_basic_init(pool, num_of_buffers, &base_address, partition_model);
+		rc = bm_gp_pool_def_basic_init(pool, num_of_buffers, &base_address, partition_model, pool_pair);
 		if (rc != OK)
 			PR_ERR_CODE(rc)
 	} else if (!strcmp(name, "gp_pool_basic_init")) {
@@ -324,6 +344,10 @@ static ssize_t mv_bm_config(struct device *dev,
 		num_of_buffers = base_address.dma_msb = base_address.virt_msb = 0;
 		pool = pe_size = pool_pair = ae_thr = af_thr = cache_vmid = cache_attr = cache_so_thr = cache_si_thr = cache_num_of_buffers = 0xFFFFFFFF;
 		sscanf(buf, "%d %d %d %d %d %d %d %d %d %d %d", &pool, &num_of_buffers, &pe_size, &pool_pair, &ae_thr, &af_thr, &cache_vmid, &cache_attr, &cache_so_thr, &cache_si_thr, &cache_num_of_buffers);
+		if (num_of_buffers == 0) {
+			pr_err("wrong number of buffers. should be larger than 0\n");
+			return -1;
+		}
 		base_address.virt_lsb = (u32)dma_alloc_coherent(NULL, num_of_buffers*4, &base_address.dma_lsb, GFP_KERNEL);
 		rc = bm_gp_pool_basic_init(pool, num_of_buffers, &base_address, pe_size, pool_pair, ae_thr, af_thr, cache_vmid, cache_attr, cache_so_thr, cache_si_thr, cache_num_of_buffers);
 		if (rc != OK)
@@ -360,16 +384,20 @@ static ssize_t mv_bm_config(struct device *dev,
 		if (rc != OK)
 			PR_ERR_CODE(rc)
 	} else if (!strcmp(name, "gp_pool_def_quick_init")) {
-		u32 pool, num_of_buffers, fill_level, partition_model;
+		u32 pool, num_of_buffers, fill_level, partition_model, pool_pair;
 		struct mv_a40 base_address;
 
 		/* Read input values */
 		PR_INFO_CALLED
 		num_of_buffers = base_address.dma_msb = base_address.virt_msb = fill_level = 0;
-		pool = partition_model = 0xFFFFFFFF;
-		sscanf(buf, "%d %d %d %d", &pool, &num_of_buffers, &fill_level, &partition_model);
+		pool = partition_model = pool_pair = 0xFFFFFFFF;
+		sscanf(buf, "%d %d %d %d %d", &pool, &num_of_buffers, &fill_level, &partition_model, &pool_pair);
+		if (num_of_buffers == 0) {
+			pr_err("wrong number of buffers. should be larger than 0\n");
+			return -1;
+		}
 		base_address.virt_lsb = (u32)dma_alloc_coherent(NULL, num_of_buffers*4, &base_address.dma_lsb, GFP_KERNEL);
-		rc = bm_gp_pool_def_quick_init(pool, num_of_buffers, fill_level, &base_address, partition_model);
+		rc = bm_gp_pool_def_quick_init(pool, num_of_buffers, fill_level, &base_address, partition_model, pool_pair);
 		if (rc != OK)
 			PR_ERR_CODE(rc)
 	} else if (!strcmp(name, "gp_pool_quick_init")) {
@@ -381,6 +409,10 @@ static ssize_t mv_bm_config(struct device *dev,
 		num_of_buffers = base_address.dma_msb = base_address.virt_msb = fill_level = 0;
 		pool = pe_size = pool_pair = ae_thr = af_thr = cache_vmid = cache_attr = cache_so_thr = cache_si_thr = cache_num_of_buffers = 0xFFFFFFFF;
 		sscanf(buf, "%d %d %d %d %d %d %d %d %d %d %d %d", &pool, &num_of_buffers, &fill_level, &pe_size, &pool_pair, &ae_thr, &af_thr, &cache_vmid, &cache_attr, &cache_so_thr, &cache_si_thr, &cache_num_of_buffers);
+		if (num_of_buffers == 0) {
+			pr_err("wrong number of buffers. should be larger than 0\n");
+			return -1;
+		}
 		base_address.virt_lsb = (u32)dma_alloc_coherent(NULL, num_of_buffers*4, &base_address.dma_lsb, GFP_KERNEL);
 		rc = bm_gp_pool_quick_init(pool, num_of_buffers, fill_level, &base_address, pe_size, pool_pair, ae_thr, af_thr, cache_vmid, cache_attr, cache_so_thr, cache_si_thr, cache_num_of_buffers);
 		if (rc != OK)
@@ -461,6 +493,10 @@ static ssize_t mv_bm_config(struct device *dev,
 		num_of_buffers = base_address.dma_msb = base_address.virt_msb = 0;
 		pool = 0xFFFFFFFF;
 		sscanf(buf, "%d %d", &pool, &num_of_buffers);
+		if (num_of_buffers == 0) {
+			pr_err("wrong number of buffers. should be larger than 0\n");
+			return -1;
+		}
 		base_address.virt_lsb = (u32)dma_alloc_coherent(NULL, num_of_buffers*4, &base_address.dma_lsb, GFP_KERNEL);
 		rc = bm_pool_memory_fill(pool, num_of_buffers, &base_address);
 		if (rc != OK)
@@ -474,6 +510,10 @@ static ssize_t mv_bm_config(struct device *dev,
 		num_of_buffers = base_address.dma_msb = base_address.virt_msb = 0;
 		pool = pe_size = ae_thr = af_thr = 0xFFFFFFFF;
 		sscanf(buf, "%d %d %d %d %d", &pool, &num_of_buffers, &pe_size, &ae_thr, &af_thr);
+		if (num_of_buffers == 0) {
+			pr_err("wrong number of buffers. should be larger than 0\n");
+			return -1;
+		}
 		base_address.virt_lsb = (u32)dma_alloc_coherent(NULL, num_of_buffers*4, &base_address.dma_lsb, GFP_KERNEL);
 		rc = bm_pool_dram_set(pool,  num_of_buffers, pe_size,  &base_address, ae_thr, af_thr);
 		if (rc != OK)
-- 
1.7.5.4

