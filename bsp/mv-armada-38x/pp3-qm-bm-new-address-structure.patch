From 44a1490cad65c05f51fcefdfb8d6e9e961451a28 Mon Sep 17 00:00:00 2001
From: Dovrat <dovrat@marvell.com>
Date: Thu, 10 Apr 2014 11:11:57 +0300
Subject: [PATCH 1548/1825] pp3: qm: bm: new address structure

https://github.com/MISL-EBU-System-SW/misl-windriver.git linux-3.4.69-14t2-read
commit 6472ae534268822c419ae8a0a6f4e93f8932d64a

Change-Id: Ic48a63fbbffb92e3e7110a58ba56b50743030c1d
Signed-off-by: Dovrat <dovrat@marvell.com>
Reviewed-on: http://vgitil04.il.marvell.com:8080/7063
Reviewed-by: Eliezer Ben Zeev <eliezerb@marvell.com>
Reviewed-by: Uri Eliyahu <uriel@marvell.com>
Tested-by: Star_Automation <star@marvell.com>
Reviewed-by: Dmitri Epshtein <dima@marvell.com>
Tested-by: Dmitri Epshtein <dima@marvell.com>
Signed-off-by: Zhong Hongbo <hongbo.zhong@windriver.com>
---
 drivers/net/ethernet/marvell/pp3/bm/mv_bm.c        |  331 +++++++++++---------
 drivers/net/ethernet/marvell/pp3/bm/mv_bm.h        |  184 +++---------
 drivers/net/ethernet/marvell/pp3/bm/mv_bm_regs.h   |    8 +-
 drivers/net/ethernet/marvell/pp3/bm/mv_bm_sysfs.c  |  129 ++++----
 drivers/net/ethernet/marvell/pp3/common/mv_hw_if.h |   10 +-
 drivers/net/ethernet/marvell/pp3/qm/mv_qm.c        |   13 +-
 drivers/net/ethernet/marvell/pp3/qm/mv_qm.h        |    4 +-
 7 files changed, 297 insertions(+), 382 deletions(-)

diff --git a/drivers/net/ethernet/marvell/pp3/bm/mv_bm.c b/drivers/net/ethernet/marvell/pp3/bm/mv_bm.c
index 6e88dfd..da1d869 100644
--- a/drivers/net/ethernet/marvell/pp3/bm/mv_bm.c
+++ b/drivers/net/ethernet/marvell/pp3/bm/mv_bm.c
@@ -298,13 +298,13 @@ int bm_enable_status_get(u32 *bm_req_rcv_en)
 	return rc;
 }
 
-int bm_qm_gpm_pools_def_quick_init(u32 num_of_buffers, u32 *qece_base_address, u32 *pl_base_address)
+int bm_qm_gpm_pools_def_quick_init(u32 num_of_buffers, struct mv_a40 *qece_base_address, struct mv_a40 *pl_base_address)
 {
 	int rc = -BM_INPUT_NOT_IN_RANGE;
 	u32 ae_thr, af_thr, cache_vmid, cache_attr, cache_so_thr, cache_si_thr, cache_num_of_buffers;
 
-	ae_thr               = BM_AE_THR_DEF;
-	af_thr               = BM_AF_THR_DEF;
+	ae_thr               = BM_AE_THR_DEF(num_of_buffers);
+	af_thr               = BM_AF_THR_DEF(num_of_buffers);
 	cache_vmid           = BM_CACHE_VMID_DEF;
 	cache_attr           = BM_CACHE_ATTR_DEF;
 	cache_si_thr         = BM_CACHE_SI_THR_QM_DEF;
@@ -316,13 +316,14 @@ int bm_qm_gpm_pools_def_quick_init(u32 num_of_buffers, u32 *qece_base_address, u
 	return rc;
 }
 
-int bm_qm_dram_pools_def_quick_init(u32 num_of_buffers, u32 *qece_base_address,	u32 *pl_base_address)
+int bm_qm_dram_pools_def_quick_init(u32 num_of_buffers, struct mv_a40 *qece_base_address,
+									struct mv_a40 *pl_base_address)
 {
 	int rc = -BM_INPUT_NOT_IN_RANGE;
 	u32 ae_thr, af_thr, cache_vmid, cache_attr, cache_so_thr, cache_si_thr, cache_num_of_buffers;
 
-	ae_thr               = BM_AE_THR_DEF;
-	af_thr               = BM_AF_THR_DEF;
+	ae_thr               = BM_AE_THR_DEF(num_of_buffers);
+	af_thr               = BM_AF_THR_DEF(num_of_buffers);
 	cache_vmid           = BM_CACHE_VMID_DEF;
 	cache_attr           = BM_CACHE_ATTR_DEF;
 	cache_si_thr         = BM_CACHE_SI_THR_QM_DEF;
@@ -335,20 +336,21 @@ int bm_qm_dram_pools_def_quick_init(u32 num_of_buffers, u32 *qece_base_address,
 	return rc;
 }
 
-int bm_qm_gpm_pools_quick_init(u32 num_of_buffers, u32 *qece_base_address,
-						u32 *pl_base_address, u32 ae_thr, u32 af_thr,
+int bm_qm_gpm_pools_quick_init(u32 num_of_buffers, struct mv_a40 *qece_base_address,
+						struct mv_a40 *pl_base_address, u32 ae_thr, u32 af_thr,
 						u32 cache_vmid, u32 cache_attr, u32 cache_so_thr, u32 cache_si_thr,
 						u32 cache_num_of_buffers)
 {
 	int rc = -BM_INPUT_NOT_IN_RANGE;
 	u32 pool, quick_init, pe_size, bm_req_rcv_en;
-	struct mv_word40 base_address;
+	struct mv_a40 base_address;
 	u32 granularity_of_pe_in_dram, granularity_of_pe_in_cache;
 
 	pr_info("%s:\n", __func__);
 	pr_info("  num_of_buffers    %d\n", num_of_buffers);
-	pr_info("  qece_base_address %x pl_base_address %x\n",
-		((struct mv_word40 *)qece_base_address)->lo, ((struct mv_word40 *)pl_base_address)->hi);
+	pr_info(" qece_base_address 0x%02X%08X pl_base_address 0x%02X%08X\n",
+		qece_base_address->dma_msb, qece_base_address->dma_lsb,
+		pl_base_address->dma_msb,   pl_base_address->dma_lsb);
 	pr_info("  ae_thr            %d af_thr          %d\n", ae_thr, af_thr);
 	pr_info("  cache_vmid        %d cache_attr      %d\n", cache_vmid, cache_attr);
 	pr_info("  cache_so_thr      %d cache_si_thr    %d  cache_num_of_buffers %d\n",
@@ -386,38 +388,44 @@ int bm_qm_gpm_pools_quick_init(u32 num_of_buffers, u32 *qece_base_address,
 			BM_NUM_OF_BUFFERS_QM_MIN, BM_NUM_OF_BUFFERS_QM_GPM_MAX);
 		return rc;
 	}
-/*	if ((qece_base_address_hi <  BM_DRAM_ADDRESS_HI_MIN) || (qece_base_address_hi >  BM_DRAM_ADDRESS_HI_MAX)) */
-	if ((((struct mv_word40 *)qece_base_address)->hi < BM_DRAM_ADDRESS_HI_MIN) ||
-		(((struct mv_word40 *)qece_base_address)->hi > BM_DRAM_ADDRESS_HI_MAX)) {
+	if ((qece_base_address->dma_msb < BM_DRAM_ADDRESS_MIN_MSB) ||
+		(qece_base_address->dma_msb > BM_DRAM_ADDRESS_MAX_MSB)) {
 		pr_err("problem with qece base address\n");
 		return rc;
 	}
-/*	if ((qece_base_address_lo <  BM_DRAM_ADDRESS_LO_MIN) || (qece_base_address_lo >  BM_DRAM_ADDRESS_LO_MAX)) */
-	if ((((struct mv_word40 *)qece_base_address)->lo < BM_DRAM_ADDRESS_LO_MIN) ||
-		(((struct mv_word40 *)qece_base_address)->lo > BM_DRAM_ADDRESS_LO_MAX)) {
+	if ((qece_base_address->dma_msb == BM_DRAM_ADDRESS_MIN_MSB) &&
+		(qece_base_address->dma_lsb <  BM_DRAM_ADDRESS_MIN_LSB)) {
 		pr_err("problem with qece base address\n");
 		return rc;
 	}
-/*	if ((pl_base_address_hi   <  BM_DRAM_ADDRESS_HI_MIN) || (pl_base_address_hi   >  BM_DRAM_ADDRESS_HI_MAX)) */
-	if ((((struct mv_word40 *)pl_base_address)->hi < BM_DRAM_ADDRESS_HI_MIN) ||
-		(((struct mv_word40 *)pl_base_address)->hi > BM_DRAM_ADDRESS_HI_MAX)) {
+	if ((qece_base_address->dma_msb == BM_DRAM_ADDRESS_MAX_MSB) &&
+		(qece_base_address->dma_lsb >  BM_DRAM_ADDRESS_MAX_LSB)) {
+		pr_err("problem with qece base address\n");
+		return rc;
+	}
+	if ((pl_base_address->dma_msb < BM_DRAM_ADDRESS_MIN_MSB) ||
+		(pl_base_address->dma_msb > BM_DRAM_ADDRESS_MAX_MSB)) {
 		pr_err("problem with pl base address\n");
 		return rc;
 	}
-/*	if ((pl_base_address_lo   <  BM_DRAM_ADDRESS_LO_MIN) || (pl_base_address_lo   >  BM_DRAM_ADDRESS_LO_MAX)) */
-	if ((((struct mv_word40 *)pl_base_address)->lo < BM_DRAM_ADDRESS_LO_MIN) ||
-		(((struct mv_word40 *)pl_base_address)->lo > BM_DRAM_ADDRESS_LO_MAX)) {
+	if ((pl_base_address->dma_msb == BM_DRAM_ADDRESS_MIN_MSB) &&
+		(pl_base_address->dma_lsb <  BM_DRAM_ADDRESS_MIN_LSB)) {
 		pr_err("problem with pl base address\n");
 		return rc;
 	}
-	if ((ae_thr       <       BM_AE_THR_MIN) || (ae_thr       >       BM_AE_THR_MAX)) {
+	if ((pl_base_address->dma_msb == BM_DRAM_ADDRESS_MAX_MSB) &&
+		(pl_base_address->dma_lsb >  BM_DRAM_ADDRESS_MAX_LSB)) {
+		pr_err("problem with pl base address\n");
+		return rc;
+	}
+	if ((ae_thr       < BM_AE_THR_MIN(num_of_buffers)) || (ae_thr       >       BM_AE_THR_MAX(num_of_buffers))) {
 		pr_err("almost empty threhold should be in the rage %d ... %d\n",
-			BM_AE_THR_MIN, BM_AE_THR_MAX);
+			BM_AE_THR_MIN(num_of_buffers), BM_AE_THR_MAX(num_of_buffers));
 		return rc;
 	}
-	if ((af_thr       <       BM_AF_THR_MIN) || (af_thr       >       BM_AF_THR_MAX)) {
+	if ((af_thr       <  BM_AF_THR_MIN(num_of_buffers)) || (af_thr       >       BM_AF_THR_MAX(num_of_buffers))) {
 		pr_err("almost full threhold should be in the rage %d ... %d\n",
-			BM_AF_THR_MIN, BM_AF_THR_MAX);
+			BM_AF_THR_MIN(num_of_buffers), BM_AF_THR_MAX(num_of_buffers));
 		return rc;
 	}
 	if ((cache_vmid   <         BM_VMID_MIN) || (cache_vmid   >         BM_VMID_MAX)) {
@@ -430,14 +438,14 @@ int bm_qm_gpm_pools_quick_init(u32 num_of_buffers, u32 *qece_base_address,
 			BM_CACHE_ATTR_MIN, BM_CACHE_ATTR_MAX);
 		return rc;
 	}
-	if ((cache_so_thr < BM_CACHE_SO_THR_MIN) || (cache_so_thr > BM_CACHE_SO_THR_MAX)) {
+	if ((cache_so_thr < BM_CACHE_SO_THR_MIN) || (cache_so_thr > BM_CACHE_SO_THR_MAX(cache_num_of_buffers))) {
 		pr_err("cache swap out threshold should be in the rage %d ... %d\n",
-			BM_CACHE_SO_THR_MIN, BM_CACHE_SO_THR_MAX);
+			BM_CACHE_SO_THR_MIN, BM_CACHE_SO_THR_MAX(cache_num_of_buffers));
 		return rc;
 	}
-	if ((cache_si_thr < BM_CACHE_SI_THR_MIN) || (cache_si_thr > BM_CACHE_SI_THR_MAX)) {
+	if ((cache_si_thr < BM_CACHE_SI_THR_MIN) || (cache_si_thr > BM_CACHE_SI_THR_MAX(cache_num_of_buffers))) {
 		pr_err("cache swap in threshold should be in the rage %d ... %d\n",
-			BM_CACHE_SI_THR_MIN, BM_CACHE_SI_THR_MAX);
+			BM_CACHE_SI_THR_MIN, BM_CACHE_SI_THR_MAX(cache_num_of_buffers));
 		return rc;
 	}
 	if ((cache_num_of_buffers < BM_CACHE_NUM_OF_BUFFERS_QM_MIN)	||
@@ -452,16 +460,19 @@ int bm_qm_gpm_pools_quick_init(u32 num_of_buffers, u32 *qece_base_address,
 		return rc;
 
 	if (bm_req_rcv_en == 1) {
+		pr_err("QM GPM pool should be enabled before BM module is enabled\n");
 		rc = -BM_ATTR_CHANGE_AFTER_BM_ENABLE;
 		return rc;
 	}
 
 	pool = 0;
-	base_address.hi = ((struct mv_word40 *)pl_base_address)->hi;
-	base_address.lo = ((struct mv_word40 *)pl_base_address)->lo;
+	base_address.dma_msb = pl_base_address->dma_msb;
+	base_address.dma_lsb = pl_base_address->dma_lsb;
+	base_address.virt_msb = pl_base_address->virt_msb;
+	base_address.virt_lsb = pl_base_address->virt_lsb;
 	quick_init = 1;
 	pe_size = 1;
-	rc = bm_pool_dram_set(pool, num_of_buffers, pe_size, (u32 *)&base_address, ae_thr, af_thr);
+	rc = bm_pool_dram_set(pool, num_of_buffers, pe_size, &base_address, ae_thr, af_thr);
 	if (rc != OK)
 		return rc;
 	rc = bm_pool_cache_set(pool, cache_vmid, cache_attr, cache_so_thr, cache_si_thr, cache_num_of_buffers);
@@ -470,7 +481,7 @@ int bm_qm_gpm_pools_quick_init(u32 num_of_buffers, u32 *qece_base_address,
 	rc = bm_pool_fill_level_set(pool, num_of_buffers, pe_size, quick_init);
 	if (rc != OK)
 		return rc;
-	rc = bm_pool_memory_fill(pool, num_of_buffers, (u32 *)&base_address);
+	rc = bm_pool_memory_fill(pool, num_of_buffers, &base_address);
 	if (rc != OK)
 		return rc;
 	rc = bm_pool_enable(pool, quick_init);
@@ -478,11 +489,13 @@ int bm_qm_gpm_pools_quick_init(u32 num_of_buffers, u32 *qece_base_address,
 		return rc;
 
 	pool = 1;
-	base_address.hi = ((struct mv_word40 *)qece_base_address)->hi;
-	base_address.lo = ((struct mv_word40 *)qece_base_address)->lo;
+	base_address.dma_msb = qece_base_address->dma_msb;
+	base_address.dma_lsb = qece_base_address->dma_lsb;
+	base_address.virt_msb = pl_base_address->virt_msb;
+	base_address.virt_lsb = pl_base_address->virt_lsb;
 	quick_init = 1;
 	pe_size = 1;
-	rc = bm_pool_dram_set(pool, num_of_buffers, pe_size, (u32 *)&base_address, ae_thr, af_thr);
+	rc = bm_pool_dram_set(pool, num_of_buffers, pe_size, &base_address, ae_thr, af_thr);
 	if (rc != OK)
 		return rc;
 	rc = bm_pool_cache_set(pool, cache_vmid, cache_attr, cache_so_thr, cache_si_thr, cache_num_of_buffers);
@@ -491,21 +504,22 @@ int bm_qm_gpm_pools_quick_init(u32 num_of_buffers, u32 *qece_base_address,
 	rc = bm_pool_fill_level_set(pool, num_of_buffers, pe_size, quick_init);
 	if (rc != OK)
 		return rc;
-	rc = bm_pool_memory_fill(pool, num_of_buffers, (u32 *)&base_address);
+	rc = bm_pool_memory_fill(pool, num_of_buffers, &base_address);
 	if (rc != OK)
 		return rc;
 	rc = bm_pool_enable(pool, quick_init);
 	return rc;
 }
 
-int bm_qm_dram_pools_quick_init(u32 num_of_buffers, u32 *qece_base_address,
-			u32 *pl_base_address, u32 ae_thr, u32 af_thr,
+int bm_qm_dram_pools_quick_init(u32 num_of_buffers, struct mv_a40 *qece_base_address,
+			struct mv_a40 *pl_base_address, u32 ae_thr, u32 af_thr,
 			u32 cache_vmid, u32 cache_attr, u32 cache_so_thr, u32 cache_si_thr,
 			u32 cache_num_of_buffers)
 {
 	int rc = -BM_INPUT_NOT_IN_RANGE;
-	u32 pool, base_address_allocate, quick_init, pe_size, bm_req_rcv_en, buffer_size;
-	struct mv_word40 base_address;
+	struct mv_a40 qece_buf_address_allocate, pl_buf_address_allocate;
+	u32 pool, quick_init, pe_size, /*bm_req_rcv_en, */buffer_size;
+	struct mv_a40 base_address;
 	u32 granularity_of_pe_in_dram, granularity_of_pe_in_cache;
 
 	granularity_of_pe_in_dram  = GRANULARITY_OF_64_BYTES / QM_PE_SIZE_IN_BYTES_IN_DRAM;		/* 64/4 */
@@ -526,47 +540,50 @@ int bm_qm_dram_pools_quick_init(u32 num_of_buffers, u32 *qece_base_address,
 
 	if ((num_of_buffers < BM_NUM_OF_BUFFERS_QM_MIN) || (num_of_buffers > BM_NUM_OF_BUFFERS_QM_DRAM_MAX))
 		return rc;
-/*	if ((qece_base_address_hi <  BM_DRAM_ADDRESS_HI_MIN) || (qece_base_address_hi >  BM_DRAM_ADDRESS_HI_MAX)) */
-	if ((((struct mv_word40 *)qece_base_address)->hi < BM_DRAM_ADDRESS_HI_MIN) ||
-		(((struct mv_word40 *)qece_base_address)->hi > BM_DRAM_ADDRESS_HI_MAX)) {
+	if ((qece_base_address->dma_msb < BM_DRAM_ADDRESS_MIN_MSB) ||
+		(qece_base_address->dma_msb > BM_DRAM_ADDRESS_MAX_MSB))
+		return rc;
+	if ((qece_base_address->dma_msb == BM_DRAM_ADDRESS_MIN_MSB) &&
+		(qece_base_address->dma_lsb <  BM_DRAM_ADDRESS_MIN_LSB)) {
 		pr_err("problem with qece base address\n");
 		return rc;
 	}
-/*	if ((qece_base_address_lo <  BM_DRAM_ADDRESS_LO_MIN) || (qece_base_address_lo >  BM_DRAM_ADDRESS_LO_MAX)) */
-	if ((((struct mv_word40 *)qece_base_address)->lo < BM_DRAM_ADDRESS_LO_MIN) ||
-		(((struct mv_word40 *)qece_base_address)->lo > BM_DRAM_ADDRESS_LO_MAX)) {
+	if ((qece_base_address->dma_msb == BM_DRAM_ADDRESS_MAX_MSB) &&
+		(qece_base_address->dma_lsb >  BM_DRAM_ADDRESS_MAX_LSB)) {
 		pr_err("problem with qece base address\n");
 		return rc;
 	}
-/*	if ((pl_base_address_hi   <  BM_DRAM_ADDRESS_HI_MIN) || (pl_base_address_hi   >  BM_DRAM_ADDRESS_HI_MAX)) */
-	if ((((struct mv_word40 *)pl_base_address)->hi < BM_DRAM_ADDRESS_HI_MIN) ||
-		(((struct mv_word40 *)pl_base_address)->hi > BM_DRAM_ADDRESS_HI_MAX)) {
-		pr_err("problem with pl base address\n");
+	if ((pl_base_address->dma_msb < BM_DRAM_ADDRESS_MIN_MSB) ||
+		(pl_base_address->dma_msb > BM_DRAM_ADDRESS_MAX_MSB))
+		return rc;
+	if ((pl_base_address->dma_msb == BM_DRAM_ADDRESS_MIN_MSB) &&
+		(pl_base_address->dma_lsb <  BM_DRAM_ADDRESS_MIN_LSB)) {
+		pr_err("problem with qece base address\n");
 		return rc;
 	}
-/*	if ((pl_base_address_lo   <  BM_DRAM_ADDRESS_LO_MIN) || (pl_base_address_lo   >  BM_DRAM_ADDRESS_LO_MAX)) */
-	if ((((struct mv_word40 *)pl_base_address)->hi < BM_DRAM_ADDRESS_LO_MIN) ||
-		(((struct mv_word40 *)pl_base_address)->hi > BM_DRAM_ADDRESS_LO_MAX)) {
-		pr_err("problem with pl base address\n");
+	if ((pl_base_address->dma_msb == BM_DRAM_ADDRESS_MAX_MSB) &&
+		(pl_base_address->dma_lsb >  BM_DRAM_ADDRESS_MAX_LSB)) {
+		pr_err("problem with qece base address\n");
 		return rc;
 	}
 
-	if ((ae_thr       <       BM_AE_THR_MIN) || (ae_thr       >       BM_AE_THR_MAX))
+	if ((ae_thr       < BM_AE_THR_MIN(num_of_buffers)) || (ae_thr       > BM_AE_THR_MAX(num_of_buffers)))
 		return rc;
-	if ((af_thr       <       BM_AF_THR_MIN) || (af_thr       >       BM_AF_THR_MAX))
+	if ((af_thr       < BM_AF_THR_MIN(num_of_buffers)) || (af_thr       > BM_AF_THR_MAX(num_of_buffers)))
 		return rc;
 	if ((cache_vmid   <         BM_VMID_MIN) || (cache_vmid   >         BM_VMID_MAX))
 		return rc;
 	if ((cache_attr   <   BM_CACHE_ATTR_MIN) || (cache_attr   >   BM_CACHE_ATTR_MAX))
 		return rc;
-	if ((cache_so_thr < BM_CACHE_SO_THR_MIN) || (cache_so_thr > BM_CACHE_SO_THR_MAX))
+	if ((cache_so_thr < BM_CACHE_SO_THR_MIN) || (cache_so_thr > BM_CACHE_SO_THR_MAX(cache_num_of_buffers)))
 		return rc;
-	if ((cache_si_thr < BM_CACHE_SI_THR_MIN) || (cache_si_thr > BM_CACHE_SI_THR_MAX))
+	if ((cache_si_thr < BM_CACHE_SI_THR_MIN) || (cache_si_thr > BM_CACHE_SI_THR_MAX(cache_num_of_buffers)))
 		return rc;
 	if ((cache_num_of_buffers < BM_CACHE_NUM_OF_BUFFERS_QM_MIN)	||
 		(cache_num_of_buffers > BM_CACHE_NUM_OF_BUFFERS_QM_MAX))
 		return rc;
 
+	/* BM can be enabled before QM DRAM pool are enabled
 	rc = bm_enable_status_get(&bm_req_rcv_en);
 	if (rc != OK)
 		return rc;
@@ -574,14 +591,17 @@ int bm_qm_dram_pools_quick_init(u32 num_of_buffers, u32 *qece_base_address,
 		rc = -BM_ATTR_CHANGE_AFTER_BM_ENABLE;
 		return rc;
 	}
+	*/
 
 	pool = 2;
-	base_address.hi = ((struct mv_word40 *)pl_base_address)->hi;
-	base_address.lo = ((struct mv_word40 *)pl_base_address)->lo;
+	base_address.dma_msb = pl_base_address->dma_msb;
+	base_address.dma_lsb = pl_base_address->dma_lsb;
+	base_address.virt_msb = pl_base_address->virt_msb;
+	base_address.virt_lsb = pl_base_address->virt_lsb;
 	quick_init = 1;
 	pe_size = 1;
 	buffer_size = BM_BUFFER_SIZE_P2;
-	rc = bm_pool_dram_set(pool, num_of_buffers, pe_size, (u32 *)&base_address, ae_thr, af_thr);
+	rc = bm_pool_dram_set(pool, num_of_buffers, pe_size, &base_address, ae_thr, af_thr);
 	if (rc != OK)
 		return rc;
 	rc = bm_pool_cache_set(pool, cache_vmid, cache_attr, cache_so_thr, cache_si_thr, cache_num_of_buffers);
@@ -592,28 +612,31 @@ int bm_qm_dram_pools_quick_init(u32 num_of_buffers, u32 *qece_base_address,
 		return rc;
 	/*	for pools 2&3 it allocates the buffer memory before filling the pool	*/
 	rc = ENOMEM;
-	base_address_allocate = (u32)MV_MALLOC((num_of_buffers + 1) * buffer_size, GFP_KERNEL);
-	if (base_address_allocate == (u32)NULL)
+	pl_buf_address_allocate.virt_lsb =
+			(u32)dma_alloc_coherent(NULL, num_of_buffers*4, &pl_buf_address_allocate.dma_lsb, GFP_KERNEL);
+	if (pl_buf_address_allocate.virt_lsb == (u32)NULL)
 		return rc;
 		/*base_address_hi need to be resolved - ???*/
-	base_address.hi = 0;
-	base_address.lo = base_address_allocate;
-	rc = bm_pool_memory_fill(pool, num_of_buffers, (u32 *)&base_address);
+	base_address.dma_msb = 0;
+	base_address.dma_lsb = pl_buf_address_allocate.dma_lsb;
+	base_address.virt_msb = 0;
+	base_address.virt_lsb = pl_buf_address_allocate.virt_lsb;
+	rc = bm_pool_memory_fill(pool, num_of_buffers, &base_address);
 	if (rc != OK)
 		return rc;
 	rc = bm_pool_enable(pool, quick_init);
 	if (rc != OK)
 		return rc;
-	((struct mv_word40 *)pl_base_address)->hi = base_address.hi;
-	((struct mv_word40 *)pl_base_address)->lo = base_address.lo;
 
 	pool = 3;
-	base_address.hi = ((struct mv_word40 *)qece_base_address)->hi;
-	base_address.lo = ((struct mv_word40 *)qece_base_address)->lo;
+	base_address.dma_msb = qece_base_address->dma_msb;
+	base_address.dma_lsb = qece_base_address->dma_lsb;
+	base_address.virt_msb = qece_base_address->virt_msb;
+	base_address.virt_lsb = qece_base_address->virt_lsb;
 	quick_init = 1;
 	pe_size = 1;
 	buffer_size = BM_BUFFER_SIZE_P3;
-	rc = bm_pool_dram_set(pool, num_of_buffers, pe_size, (u32 *)&base_address, ae_thr, af_thr);
+	rc = bm_pool_dram_set(pool, num_of_buffers, pe_size, &base_address, ae_thr, af_thr);
 	if (rc != OK)
 		return rc;
 	rc = bm_pool_cache_set(pool, cache_vmid, cache_attr, cache_so_thr, cache_si_thr, cache_num_of_buffers);
@@ -624,22 +647,24 @@ int bm_qm_dram_pools_quick_init(u32 num_of_buffers, u32 *qece_base_address,
 		return rc;
 	/*	for pools 2&3 it allocates the buffer memory before filling the pool	*/
 	rc = ENOMEM;
-	base_address_allocate = (u32)MV_MALLOC((num_of_buffers + 1) * buffer_size, GFP_KERNEL);
-	if (base_address_allocate == (u32)NULL)
+	qece_buf_address_allocate.virt_lsb =
+			(u32)dma_alloc_coherent(NULL, num_of_buffers*4, &qece_buf_address_allocate.dma_lsb, GFP_KERNEL);
+	if (qece_buf_address_allocate.virt_lsb == (u32)NULL)
 		return rc;
 		/*base_address_hi need to be resolved - ???*/
-	base_address.hi = 0;
-	base_address.lo = base_address_allocate;
-	rc = bm_pool_memory_fill(pool, num_of_buffers, (u32 *)&base_address);
+	base_address.dma_msb = 0;
+	base_address.dma_lsb = qece_buf_address_allocate.dma_lsb;
+	base_address.virt_msb = 0;
+	base_address.virt_lsb = qece_buf_address_allocate.virt_lsb;
+
+	rc = bm_pool_memory_fill(pool, num_of_buffers, &base_address);
 	if (rc != OK)
 		return rc;
 	rc = bm_pool_enable(pool, quick_init);
 	if (rc != OK)
 		return rc;
-	((struct mv_word40 *)qece_base_address)->hi = base_address.hi;
-	((struct mv_word40 *)qece_base_address)->lo = base_address.lo;
 
-	rc = qm_pfe_base_address_pool_set(pl_base_address, qece_base_address);
+	rc = qm_pfe_base_address_pool_set(&qece_buf_address_allocate, &pl_buf_address_allocate);
 	return rc;
 }
 
@@ -674,7 +699,7 @@ int bm_pool_quick_init_status_get(u32 pool, u32 *completed)
 	return rc;
 }
 
-int bm_gp_pool_def_basic_init(u32 pool, u32 num_of_buffers, u32 *base_address, u32 partition_model)
+int bm_gp_pool_def_basic_init(u32 pool, u32 num_of_buffers, struct mv_a40 *base_address, u32 partition_model)
 {
 	int rc = -BM_INPUT_NOT_IN_RANGE;
 	u32 pe_size, pool_pair, ae_thr, af_thr, cache_vmid, cache_attr,
@@ -682,8 +707,8 @@ int bm_gp_pool_def_basic_init(u32 pool, u32 num_of_buffers, u32 *base_address, u
 
 	pe_size              = BM_PE_SIZE_DEF;
 	pool_pair            = BM_POOL_PAIR_GP_DEF;
-	ae_thr               = BM_AE_THR_DEF;
-	af_thr               = BM_AF_THR_DEF;
+	ae_thr               = BM_AE_THR_DEF(num_of_buffers);
+	af_thr               = BM_AF_THR_DEF(num_of_buffers);
 	cache_vmid           = BM_CACHE_VMID_DEF;
 	cache_attr           = BM_CACHE_ATTR_DEF;
 
@@ -704,7 +729,7 @@ int bm_gp_pool_def_basic_init(u32 pool, u32 num_of_buffers, u32 *base_address, u
 	return rc;
 }
 
-int bm_gp_pool_basic_init(u32 pool, u32 num_of_buffers, u32 *base_address,
+int bm_gp_pool_basic_init(u32 pool, u32 num_of_buffers, struct mv_a40 *base_address,
 				u32 pe_size, u32 pool_pair, u32 ae_thr, u32 af_thr,
 				u32 cache_vmid, u32 cache_attr, u32 cache_so_thr, u32 cache_si_thr,
 				u32 cache_num_of_buffers)
@@ -732,7 +757,7 @@ int bm_gp_pool_basic_init(u32 pool, u32 num_of_buffers, u32 *base_address,
 		return rc;
 	if               ((af_thr %  granularity_of_pe_in_dram) != 0)
 		return rc;
-	if ((((struct mv_word40 *)base_address)->lo %  granularity_of_pe_in_dram) != 0)
+	if ((base_address->dma_lsb %  granularity_of_pe_in_dram) != 0)
 		return rc;
 	if ((cache_num_of_buffers % granularity_of_pe_in_cache) != 0)
 		return rc;
@@ -741,22 +766,20 @@ int bm_gp_pool_basic_init(u32 pool, u32 num_of_buffers, u32 *base_address,
 	if (cache_so_thr < cache_si_thr + 16)
 		return rc;
 
-
-
-
 	if ((pool                 <          BM_POOL_GP_MIN) || (pool                 >          BM_POOL_GP_MAX))
 		return rc;
 	if ((num_of_buffers       < BM_NUM_OF_BUFFERS_GP_MIN) || (num_of_buffers       > BM_NUM_OF_BUFFERS_GP_MAX))
 		return rc;
-/*	if ((base_address_hi      <  BM_DRAM_ADDRESS_HI_MIN) || (base_address_hi      >  BM_DRAM_ADDRESS_HI_MAX)) */
-	if ((((struct mv_word40 *)base_address)->hi < BM_DRAM_ADDRESS_HI_MIN) ||
-		(((struct mv_word40 *)base_address)->hi > BM_DRAM_ADDRESS_HI_MAX)) {
+	if ((base_address->dma_msb < BM_DRAM_ADDRESS_MIN_MSB) ||
+		(base_address->dma_msb > BM_DRAM_ADDRESS_MAX_MSB))
+		return rc;
+	if ((base_address->dma_msb == BM_DRAM_ADDRESS_MIN_MSB) &&
+		(base_address->dma_lsb <  BM_DRAM_ADDRESS_MIN_LSB)) {
 		pr_err("problem with qece base address\n");
 		return rc;
 	}
-/*	if ((base_address_lo      <  BM_DRAM_ADDRESS_LO_MIN) || (base_address_lo      >  BM_DRAM_ADDRESS_LO_MAX)) */
-	if ((((struct mv_word40 *)base_address)->lo < BM_DRAM_ADDRESS_LO_MIN) ||
-		(((struct mv_word40 *)base_address)->lo > BM_DRAM_ADDRESS_LO_MAX)) {
+	if ((base_address->dma_msb == BM_DRAM_ADDRESS_MAX_MSB) &&
+		(base_address->dma_lsb >  BM_DRAM_ADDRESS_MAX_LSB)) {
 		pr_err("problem with qece base address\n");
 		return rc;
 	}
@@ -764,17 +787,17 @@ int bm_gp_pool_basic_init(u32 pool, u32 num_of_buffers, u32 *base_address,
 		return rc;
 	if ((pool_pair            <        BM_POOL_PAIR_MIN) || (pool_pair            >        BM_POOL_PAIR_MAX))
 		return rc;
-	if ((ae_thr               <           BM_AE_THR_MIN) || (ae_thr               >           BM_AE_THR_MAX))
+	if ((ae_thr               < BM_AE_THR_MIN(num_of_buffers)) || (ae_thr         > BM_AE_THR_MAX(num_of_buffers)))
 		return rc;
-	if ((af_thr               <           BM_AF_THR_MIN) || (af_thr               >           BM_AF_THR_MAX))
+	if ((af_thr               < BM_AF_THR_MIN(num_of_buffers)) || (af_thr         > BM_AF_THR_MAX(num_of_buffers)))
 		return rc;
 	if ((cache_vmid           <       BM_CACHE_VMID_MIN) || (cache_vmid           >       BM_CACHE_VMID_MAX))
 		return rc;
 	if ((cache_attr           <       BM_CACHE_ATTR_MIN) || (cache_attr           >       BM_CACHE_ATTR_MAX))
 		return rc;
-	if ((cache_so_thr         <     BM_CACHE_SO_THR_MIN) || (cache_so_thr         >     BM_CACHE_SO_THR_MAX))
+	if ((cache_so_thr         < BM_CACHE_SO_THR_MIN) || (cache_so_thr > BM_CACHE_SO_THR_MAX(cache_num_of_buffers)))
 		return rc;
-	if ((cache_si_thr         <     BM_CACHE_SI_THR_MIN) || (cache_si_thr         >     BM_CACHE_SI_THR_MAX))
+	if ((cache_si_thr         < BM_CACHE_SI_THR_MIN) || (cache_si_thr > BM_CACHE_SI_THR_MAX(cache_num_of_buffers)))
 		return rc;
 	if ((cache_num_of_buffers < BM_CACHE_NUM_OF_BUFFERS_GP_MIN)	||
 		(cache_num_of_buffers > BM_CACHE_NUM_OF_BUFFERS_GP_MAX))
@@ -928,7 +951,7 @@ int bm_vmid_set(u32 bm_vmid)
 }
 
 int bm_gp_pool_def_quick_init(u32 pool, u32 num_of_buffers, u32 fill_level,
-					u32 *base_address, u32 partition_model)
+					struct mv_a40 *base_address, u32 partition_model)
 {
 	int rc = -BM_INPUT_NOT_IN_RANGE;
 	u32 pe_size, pool_pair, ae_thr, af_thr, cache_vmid, cache_attr,
@@ -937,8 +960,8 @@ int bm_gp_pool_def_quick_init(u32 pool, u32 num_of_buffers, u32 fill_level,
 
 	pe_size              = BM_PE_SIZE_IS_32_BITS;
 	pool_pair            = BM_POOL_PAIR_GP_DEF;
-	ae_thr               = BM_AE_THR_DEF;
-	af_thr               = BM_AF_THR_DEF;
+	ae_thr               = BM_AE_THR_DEF(num_of_buffers);
+	af_thr               = BM_AF_THR_DEF(num_of_buffers);
 	cache_vmid           = BM_CACHE_VMID_DEF;
 	cache_attr           = BM_CACHE_ATTR_DEF;
 
@@ -961,13 +984,13 @@ int bm_gp_pool_def_quick_init(u32 pool, u32 num_of_buffers, u32 fill_level,
 	return rc;
 }
 
-int bm_gp_pool_quick_init(u32 pool, u32 num_of_buffers, u32 fill_level, u32 *base_address,
+int bm_gp_pool_quick_init(u32 pool, u32 num_of_buffers, u32 fill_level, struct mv_a40 *base_address,
 				u32 pe_size, u32 pool_pair, u32 ae_thr, u32 af_thr,
 				u32 cache_vmid, u32 cache_attr, u32 cache_so_thr, u32 cache_si_thr,
 				u32 cache_num_of_buffers)
 {
 	int rc = -BM_INPUT_NOT_IN_RANGE;
-	u32 quick_init, bm_req_rcv_en;
+	u32 quick_init/*, bm_req_rcv_en*/;
 	u32 granularity_of_pe_in_dram, granularity_of_pe_in_cache;
 
 	quick_init = 1;	/*quick_init is TRUE*/
@@ -991,7 +1014,7 @@ int bm_gp_pool_quick_init(u32 pool, u32 num_of_buffers, u32 fill_level, u32 *bas
 		return rc;
 	if         ((af_thr % granularity_of_pe_in_dram) != 0)
 		return rc;
-	if ((((struct mv_word40 *)base_address)->lo % granularity_of_pe_in_dram)  != 0)
+	if ((base_address->dma_lsb % granularity_of_pe_in_dram)  != 0)
 		return rc;
 	if ((cache_num_of_buffers % granularity_of_pe_in_cache) != 0)
 		return rc;
@@ -1005,31 +1028,32 @@ int bm_gp_pool_quick_init(u32 pool, u32 num_of_buffers, u32 fill_level, u32 *bas
 		return rc;
 	if ((num_of_buffers       < BM_NUM_OF_BUFFERS_GP_MIN) || (num_of_buffers      > BM_NUM_OF_BUFFERS_GP_MAX))
 		return rc;
-	if ((fill_level           <       BM_FILL_LEVEL_MIN) || (fill_level           >       BM_FILL_LEVEL_MAX))
+	if ((fill_level           < BM_FILL_LEVEL_MIN) || (fill_level           > BM_FILL_LEVEL_MAX(num_of_buffers)))
 		return rc;
-/*	if ((base_address_hi      <  BM_DRAM_ADDRESS_HI_MIN) || (base_address_hi      >  BM_DRAM_ADDRESS_HI_MAX)) */
-	if ((((struct mv_word40 *)base_address)->hi < BM_DRAM_ADDRESS_HI_MIN) ||
-		(((struct mv_word40 *)base_address)->hi > BM_DRAM_ADDRESS_HI_MAX))
+	if ((base_address->dma_msb < BM_DRAM_ADDRESS_MIN_MSB) ||
+		(base_address->dma_msb > BM_DRAM_ADDRESS_MAX_MSB))
 		return rc;
-/*	if ((base_address_lo      <  BM_DRAM_ADDRESS_LO_MIN) || (base_address_lo      >  BM_DRAM_ADDRESS_LO_MAX)) */
-	if ((((struct mv_word40 *)base_address)->lo < BM_DRAM_ADDRESS_LO_MIN) ||
-		(((struct mv_word40 *)base_address)->lo > BM_DRAM_ADDRESS_LO_MAX))
+	if ((base_address->dma_msb == BM_DRAM_ADDRESS_MIN_MSB) &&
+		(base_address->dma_lsb <  BM_DRAM_ADDRESS_MIN_LSB))
+		return rc;
+	if ((base_address->dma_msb == BM_DRAM_ADDRESS_MAX_MSB) &&
+		(base_address->dma_lsb >  BM_DRAM_ADDRESS_MAX_LSB))
 		return rc;
 	if ((pe_size      <      BM_PE_SIZE_MIN) || (pe_size      >      BM_PE_SIZE_MAX))
 		return rc;
 	if ((pool_pair    <    BM_POOL_PAIR_MIN) || (pool_pair    >    BM_POOL_PAIR_MAX))
 		return rc;
-	if ((ae_thr       <       BM_AE_THR_MIN) || (ae_thr       >       BM_AE_THR_MAX))
+	if ((ae_thr       < BM_AE_THR_MIN(num_of_buffers)) || (ae_thr       > BM_AE_THR_MAX(num_of_buffers)))
 		return rc;
-	if ((af_thr       <       BM_AF_THR_MIN) || (af_thr       >       BM_AF_THR_MAX))
+	if ((af_thr       < BM_AF_THR_MIN(num_of_buffers)) || (af_thr       > BM_AF_THR_MAX(num_of_buffers)))
 		return rc;
 	if ((cache_vmid   <         BM_VMID_MIN) || (cache_vmid   >         BM_VMID_MAX))
 		return rc;
 	if ((cache_attr   <   BM_CACHE_ATTR_MIN) || (cache_attr   >   BM_CACHE_ATTR_MAX))
 		return rc;
-	if ((cache_so_thr < BM_CACHE_SO_THR_MIN) || (cache_so_thr > BM_CACHE_SI_THR_MAX))
+	if ((cache_so_thr < BM_CACHE_SO_THR_MIN) || (cache_so_thr > BM_CACHE_SI_THR_MAX(cache_num_of_buffers)))
 		return rc;
-	if ((cache_si_thr < BM_CACHE_SI_THR_MIN) || (cache_si_thr > BM_CACHE_SI_THR_MAX))
+	if ((cache_si_thr < BM_CACHE_SI_THR_MIN) || (cache_si_thr > BM_CACHE_SI_THR_MAX(cache_num_of_buffers)))
 		return rc;
 	if ((cache_num_of_buffers < BM_CACHE_NUM_OF_BUFFERS_GP_MIN)	||
 		(cache_num_of_buffers > BM_CACHE_NUM_OF_BUFFERS_GP_MAX))
@@ -1050,7 +1074,7 @@ int bm_gp_pool_quick_init(u32 pool, u32 num_of_buffers, u32 fill_level, u32 *bas
 	rc = bm_pool_enable(pool, quick_init);
 	if (rc != OK)
 		return rc;
-
+	/* BM can be enabled before GP pool is enabled
 	rc = bm_enable_status_get(&bm_req_rcv_en);
 	if (rc != OK)
 		return rc;
@@ -1058,6 +1082,7 @@ int bm_gp_pool_quick_init(u32 pool, u32 num_of_buffers, u32 fill_level, u32 *bas
 		rc = -BM_ATTR_CHANGE_AFTER_BM_ENABLE;
 		return rc;
 	}
+	*/
 
 	rc = OK;
 	return rc;
@@ -1504,7 +1529,7 @@ int bm_pool_registers_dump(u32 pool)
 	pr_info("\t dpr_d_mng_ball_stat.dram_ae_thr = 0x%08X\n",     tab_dpr_d_mng_ball_stat_data.dram_ae_thr);
 	pr_info("\t dpr_d_mng_ball_stat.dram_af_thr = 0x%08X\n",     tab_dpr_d_mng_ball_stat_data.dram_af_thr);
 	pr_info("\t dpr_d_mng_ball_stat.dram_start  = 0x%02X%08X\n",
-		tab_dpr_d_mng_ball_stat_data.dram_start_hi, tab_dpr_d_mng_ball_stat_data.dram_start_lo);
+		tab_dpr_d_mng_ball_stat_data.dram_start_msb, tab_dpr_d_mng_ball_stat_data.dram_start_lsb);
 	pr_info("\t dpr_d_mng_ball_stat.dram_size   = 0x%08X\n",     tab_dpr_d_mng_ball_stat_data.dram_size);
 
 	reg_base_address =      bm.tpr_dro_mng_ball_dyn;
@@ -1807,8 +1832,8 @@ int bm_cache_memory_dump(u32 bank)
 			reg_base_address + reg_offset, *((u32 *)&reg_cache_bgp_data));
 
 			pr_info("\t line %04d: sram_b%d_cache.cache_bgp_data = 0x%02X_%08X\n",
-				line, bid, reg_cache_bgp_data.cache_bgp_data_hi,
-							reg_cache_bgp_data.cache_bgp_data_lo);
+				line, bid, reg_cache_bgp_data.cache_bgp_data_msb,
+							reg_cache_bgp_data.cache_bgp_data_lsb);
 		}
 	} else {
 		rc = -BM_INPUT_NOT_IN_RANGE;
@@ -2456,7 +2481,7 @@ int bm_error_dump(void)
 }
 
 /*BM Internal functions*/
-int bm_pool_memory_fill(u32 pool, u32 num_of_buffers, u32 *base_address)
+int bm_pool_memory_fill(u32 pool, u32 num_of_buffers, struct mv_a40 *base_address)
 {
 	int rc = -BM_INPUT_NOT_IN_RANGE;
 	u32 granularity_of_pe_in_dram;
@@ -2481,12 +2506,16 @@ int bm_pool_memory_fill(u32 pool, u32 num_of_buffers, u32 *base_address)
 			return rc;
 	}
 /*	if ((base_address_hi < BM_DRAM_ADDRESS_HI_MIN) || (base_address_hi > BM_DRAM_ADDRESS_HI_MAX)) */
-	if ((((struct mv_word40 *)base_address)->hi < BM_DRAM_ADDRESS_HI_MIN) ||
-		(((struct mv_word40 *)base_address)->hi > BM_DRAM_ADDRESS_HI_MAX))
+	if ((base_address->dma_msb < BM_DRAM_ADDRESS_MIN_MSB) ||
+		(base_address->dma_msb > BM_DRAM_ADDRESS_MAX_MSB))
 		return rc;
-/*	if ((base_address_lo < BM_DRAM_ADDRESS_LO_MIN) || (base_address_lo > BM_DRAM_ADDRESS_LO_MAX)) */
-	if ((((struct mv_word40 *)base_address)->lo < BM_DRAM_ADDRESS_LO_MIN) ||
-		(((struct mv_word40 *)base_address)->lo > BM_DRAM_ADDRESS_LO_MAX))
+/*	if (base_address_lo < BM_DRAM_ADDRESS_LO_MIN) */
+	if ((base_address->dma_msb == BM_DRAM_ADDRESS_MIN_MSB) &&
+		(base_address->dma_lsb <  BM_DRAM_ADDRESS_MIN_LSB))
+		return rc;
+/*	if (base_address_lo > BM_DRAM_ADDRESS_LO_MAX) */
+	if ((base_address->dma_msb == BM_DRAM_ADDRESS_MAX_MSB) &&
+		(base_address->dma_lsb >  BM_DRAM_ADDRESS_MAX_LSB))
 		return rc;
 
 /*
@@ -2494,10 +2523,10 @@ int bm_pool_memory_fill(u32 pool, u32 num_of_buffers, u32 *base_address)
 	Takes pool_base_address and use it as a pointer to fill all PE's with incrementing value (starting with 1)
 	Write in Dram in BM pool section an incrementing index */
 
-	p = (u32 *)(((struct mv_word40 *)base_address)->lo);
+	p = (u32 *)(base_address->virt_lsb);
 /*
-	base_address->hi - ???
-	p = (u32 *)((base_address->hi << 32) + base_address->lo);
+	base_address->dma_msb - ???
+	p = (u32 *)((base_address->dma_msb << 32) + base_address->dma_lsb);
 	p = (u32 *)temp;
 	p++;*/
 	for (i = 0; i < num_of_buffers; i++)
@@ -2507,7 +2536,7 @@ int bm_pool_memory_fill(u32 pool, u32 num_of_buffers, u32 *base_address)
 	return rc;
 }
 
-int bm_pool_dram_set(u32 pool, u32 num_of_buffers, u32 pe_size, u32 *base_address,
+int bm_pool_dram_set(u32 pool, u32 num_of_buffers, u32 pe_size, struct mv_a40 *base_address,
 						u32 ae_thr, u32 af_thr)
 {
 	u32 reg_base_address, reg_size, reg_offset;
@@ -2536,7 +2565,7 @@ int bm_pool_dram_set(u32 pool, u32 num_of_buffers, u32 pe_size, u32 *base_addres
 		return rc;
 	if         ((af_thr % granularity_of_pe_in_dram) != 0)
 		return rc;
-	if ((((struct mv_word40 *)base_address)->lo % GRANULARITY_OF_64_BYTES) != 0)
+	if ((base_address->dma_lsb % GRANULARITY_OF_64_BYTES) != 0)
 		return rc;
 
 	if (af_thr       < ae_thr)
@@ -2562,16 +2591,20 @@ int bm_pool_dram_set(u32 pool, u32 num_of_buffers, u32 pe_size, u32 *base_addres
 		return rc;
 
 /*	if ((base_address_hi < BM_DRAM_ADDRESS_HI_MIN) || (base_address_hi > BM_DRAM_ADDRESS_HI_MAX)) */
-	if ((((struct mv_word40 *)base_address)->hi < BM_DRAM_ADDRESS_HI_MIN) ||
-		(((struct mv_word40 *)base_address)->hi > BM_DRAM_ADDRESS_HI_MAX))
+	if ((base_address->dma_msb < BM_DRAM_ADDRESS_MIN_MSB) ||
+		(base_address->dma_msb > BM_DRAM_ADDRESS_MAX_MSB))
+		return rc;
+/*	if (base_address_lo < BM_DRAM_ADDRESS_LO_MIN) */
+	if ((base_address->dma_msb == BM_DRAM_ADDRESS_MIN_MSB) &&
+		(base_address->dma_lsb <  BM_DRAM_ADDRESS_MIN_LSB))
 		return rc;
-/*	if ((base_address_lo < BM_DRAM_ADDRESS_LO_MIN) || (base_address_lo > BM_DRAM_ADDRESS_LO_MAX)) */
-	if ((((struct mv_word40 *)base_address)->lo < BM_DRAM_ADDRESS_LO_MIN) ||
-		(((struct mv_word40 *)base_address)->lo > BM_DRAM_ADDRESS_LO_MAX))
+/*	if (base_address_lo > BM_DRAM_ADDRESS_LO_MAX) */
+	if ((base_address->dma_msb == BM_DRAM_ADDRESS_MAX_MSB) &&
+		(base_address->dma_lsb >  BM_DRAM_ADDRESS_MAX_LSB))
 		return rc;
-	if ((ae_thr          <          BM_AE_THR_MIN) || (ae_thr          >          BM_AE_THR_MAX))
+	if ((ae_thr          < BM_AE_THR_MIN(num_of_buffers)) || (ae_thr          > BM_AE_THR_MAX(num_of_buffers)))
 		return rc;
-	if ((af_thr          <          BM_AF_THR_MIN) || (af_thr          >          BM_AF_THR_MAX))
+	if ((af_thr          < BM_AF_THR_MIN(num_of_buffers)) || (af_thr          > BM_AF_THR_MAX(num_of_buffers)))
 		return rc;
 
 	pid       = (int)pool;
@@ -2586,8 +2619,8 @@ int bm_pool_dram_set(u32 pool, u32 num_of_buffers, u32 pe_size, u32 *base_addres
 	if (rc != OK)
 		return rc;
 
-	tab_dpr_d_mng_ball_stat.dram_start_hi = ((struct mv_word40 *)base_address)->hi;
-	tab_dpr_d_mng_ball_stat.dram_start_lo = ((struct mv_word40 *)base_address)->lo;
+	tab_dpr_d_mng_ball_stat.dram_start_msb = base_address->dma_msb;
+	tab_dpr_d_mng_ball_stat.dram_start_lsb = base_address->dma_lsb;
 	if (bid == 0) {
 		dram_ae_thr = BM_QM_PE_UNITS_TO_BYTES(ae_thr);
 		dram_af_thr = BM_QM_PE_UNITS_TO_BYTES(af_thr);
@@ -2815,9 +2848,9 @@ int bm_pool_cache_set(u32 pool, u32 cache_vmid, u32 cache_attr, u32 cache_so_thr
 		return rc;
 	if ((cache_attr           <    BM_CACHE_ATTR_MIN) || (cache_attr           >    BM_CACHE_ATTR_MAX))
 		return rc;
-	if ((cache_so_thr         <  BM_CACHE_SO_THR_MIN) || (cache_so_thr         >  BM_CACHE_SO_THR_MAX))
+	if ((cache_so_thr         < BM_CACHE_SO_THR_MIN) || (cache_so_thr > BM_CACHE_SO_THR_MAX(cache_num_of_buffers)))
 		return rc;
-	if ((cache_si_thr         <  BM_CACHE_SI_THR_MIN) || (cache_si_thr         >  BM_CACHE_SI_THR_MAX))
+	if ((cache_si_thr         < BM_CACHE_SI_THR_MIN) || (cache_si_thr > BM_CACHE_SI_THR_MAX(cache_num_of_buffers)))
 		return rc;
 	if ((pool >= BM_POOL_QM_MIN) || (pool <= BM_POOL_QM_MAX)) { /* QM pools */
 		if ((cache_num_of_buffers < BM_CACHE_NUM_OF_BUFFERS_QM_MIN) ||
@@ -2964,12 +2997,6 @@ int bm_register_read(u32 base_address, u32 offset, u32 wordsNumber, u32 *dataPtr
 	u32 *temp;
 	u32 i;
 
-	if ((base_address <   BM_ADDRESS_MIN) || (base_address >   BM_ADDRESS_MAX))
-		return rc;
-	if ((offset       <    BM_OFFSET_MIN) || (offset       >    BM_OFFSET_MAX))
-		return rc;
-	if ((wordsNumber  < BM_DATA_SIZE_MIN) || (wordsNumber  > BM_DATA_SIZE_MAX))
-		return rc;
 	if (((u32)dataPtr <  BM_DATA_PTR_MIN) || ((u32)dataPtr >  BM_DATA_PTR_MAX))
 		return rc;
 
@@ -3010,12 +3037,6 @@ int bm_register_write(u32 base_address, u32 offset, u32 wordsNumber, u32 *dataPt
 	u32 *temp;
 	u32 i;
 
-	if ((base_address <   BM_ADDRESS_MIN) || (base_address >   BM_ADDRESS_MAX))
-		return rc;
-	if ((offset       <    BM_OFFSET_MIN) || (offset       >    BM_OFFSET_MAX))
-		return rc;
-	if ((wordsNumber  < BM_DATA_SIZE_MIN) || (wordsNumber  > BM_DATA_SIZE_MAX))
-		return rc;
 	if (((u32)dataPtr <  BM_DATA_PTR_MIN) || ((u32)dataPtr >  BM_DATA_PTR_MAX))
 		return rc;
 
diff --git a/drivers/net/ethernet/marvell/pp3/bm/mv_bm.h b/drivers/net/ethernet/marvell/pp3/bm/mv_bm.h
index 5ed89a1..f0d46c6 100644
--- a/drivers/net/ethernet/marvell/pp3/bm/mv_bm.h
+++ b/drivers/net/ethernet/marvell/pp3/bm/mv_bm.h
@@ -100,16 +100,6 @@ SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 #define GP_PE_SIZE_OF_32_BITS_IN_BYTES_IN_DRAM   4
 #define GP_PE_SIZE_OF_40_BITS_IN_BYTES_IN_DRAM   8
 #define GP_PE_SIZE_IN_BYTES_IN_CACHE	         8
-/*
-#define PE_SIZE_OF_32_BITS_IN_BYTES		         4
-#define PE_SIZE_OF_40_BITS_IN_BYTES		         8
-#define GP_PE_SIZE 32
-#define PE_SIZE 1 / * for pe = 22 bits or pe = 32 bits * /
-#define PE_SIZE 2 / * for pe = 40 bits * /
-#define BM_PE_SIZE_IS_4_BYTES			         4
-#define BM_PE_SIZE_IS_8_BYTES			         8
-#define BM_GRANULARITY_OF_16_PE			(16*PE_SIZE)
-*/
 
 #define BM_BUFFER_SIZE_P2				      1024
 #define BM_BUFFER_SIZE_P3				        16
@@ -120,16 +110,16 @@ SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 #define BM_PE_SIZE_DEF					BM_PE_SIZE_IS_32_BITS	/* 0 - 40 bits, 1 - 32 bits */
 #define BM_POOL_PAIR_GP_DEF				         0	/* 0 -   false, 1 -    true */
 
-#define BM_AE_THR_DEF TRUNCATE((num_of_buffers * 1/4), GRANULARITY_OF_16) /*
-#define BM_AE_THR_DEF \
-	(((num_of_buffers * 1/4) + (GRANULARITY_OF_16-1))&(0xFFFFFFF0))
-	Almost empty default threshold is ¼ of num_of_buffers truncated to multiplication of 16,
+/* Almost empty default threshold is ¼ of num_of_buffers truncated to multiplication of 16,
 		otherwise the range is 0 or 16 to num_of_buffers-32 */
-#define BM_AF_THR_DEF TRUNCATE((num_of_buffers * 3/4), GRANULARITY_OF_16) /*
-#define BM_AF_THR_DEF \
-	(((num_of_buffers * 3/4) + (GRANULARITY_OF_16-1))&(0xFFFFFFF0))
-	Almost full  default threshold is ¾ of num_of_buffers rounded   to multiplication of 16,
+#define BM_AE_THR_DEF(_num_of_buffers) \
+	(((_num_of_buffers * 1/4) > GRANULARITY_OF_16) ? \
+	TRUNCATE((_num_of_buffers * 1/4),  GRANULARITY_OF_16) : GRANULARITY_OF_16)
+
+/*	Almost full  default threshold is ¾ of num_of_buffers rounded   to multiplication of 16,
 		otherwise the range is 0 or 32 to num_of_buffers-16 */
+#define BM_AF_THR_DEF(_num_of_buffers)			TRUNCATE((_num_of_buffers * 3/4), GRANULARITY_OF_16)
+
 #define BM_CACHE_VMID_DEF                        0
 #define BM_CACHE_ATTR_DEF                        1
 
@@ -187,7 +177,6 @@ SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 #define BM_AQOS_MIN					         0
 #define BM_AQOS_MAX					0x00000003	/*   3 */
 
-/* #define BM_BUFFERS_MIN				0x00000030 */	/*  48 */
 #define BM_NUM_OF_BUFFERS_QM_MIN			0x00000030	/*  48 */
 #define BM_NUM_OF_BUFFERS_QM_GPM_MAX		(0x00001400 - 16)	/* 5120 - 16 for P0 & P1 */
 #define BM_NUM_OF_BUFFERS_QM_DRAM_MAX		(0x00400000 - 16)	/*   4M - 16 for P2 & P3 */
@@ -195,18 +184,20 @@ SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 			MV_MAX(BM_NUM_OF_BUFFERS_QM_GPM_MAX, BM_NUM_OF_BUFFERS_QM_DRAM_MAX)
 #define BM_NUM_OF_BUFFERS_GP_MIN			0x00000030	/*  48 */
 #define BM_NUM_OF_BUFFERS_GP_MAX			(0x00200000 - 16)	/*  2M - 16 */
-/* #define BM_NUM_OF_BUFFERS_MAX				MV_MAX(BM_BUFFERS_QM_MAX, BM_BUFFERS_GP_MAX) */
+
+/* Cache number of buffer is different for QM pools - bank 0 and GP pools - bank 1..4
+   In bank 0 the cache is distributed between 4 pools (recommended to be equaly)
+   In bank 1..4 there are 7 pools and we recommend to give a big portion to one pool and small
+   portion to the others */
 #define BM_CACHE_NUM_OF_BUFFERS_MIN			0x00000030	/*  48 */
 #define BM_CACHE_NUM_OF_BUFFERS_QM_MIN		BM_CACHE_NUM_OF_BUFFERS_MIN
 #define BM_CACHE_NUM_OF_BUFFERS_QM_MAX		(0x00000800 - (4-1)*BM_CACHE_NUM_OF_BUFFERS_QM_MIN) /*
 	the sum for bank 0 is up to 2048, then for one pool it is 2048-3*48   */
 #define BM_CACHE_NUM_OF_BUFFERS_GP_MIN		BM_CACHE_NUM_OF_BUFFERS_MIN
 #define BM_CACHE_NUM_OF_BUFFERS_GP_MAX		(0x00000400 - 8)	/* 1024-8 */
-/* #define BM_CACHE_BUFFERS_MAX		MV_MAX(BM_CACHE_BUFFERS_QM_MAX, BM_CACHE_BUFFERS_GP_MAX) */
+
 #define BM_FILL_LEVEL_MIN			         0
-#define BM_FILL_LEVEL_MAX			num_of_buffers
-#define BM_ADDRESS_MIN				         0
-#define BM_ADDRESS_MAX				0xFFFFFFFF
+#define BM_FILL_LEVEL_MAX(_num_of_buffers)	_num_of_buffers
 #define BM_QUICK_INIT_MIN			         0
 #define BM_QUICK_INIT_MAX			0x00000001
 #define BM_POOL_PAIR_MIN			         0
@@ -219,85 +210,33 @@ SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 #define BM_CACHE_VMID_MAX			0x0000003F	/*  63 */
 #define BM_CACHE_ATTR_MIN			         0
 #define BM_CACHE_ATTR_MAX			0x000000FF	/* 255 */
-#define BM_AE_THR_MIN				MV_MIN(0x00000010, num_of_buffers) /*
+#define BM_AE_THR_MIN(_num_of_buffers)		MV_MIN(0x00000010, _num_of_buffers) /*
 	16                unless number of buffers is 0 and then it is also 0 */
-#define BM_AE_THR_MAX				MV_MAX((num_of_buffers - 0x00000020), 0) /*
+#define BM_AE_THR_MAX(_num_of_buffers)			MV_MAX((_num_of_buffers - 0x00000020), 0) /*
 	num_of_buffers-32 unless number of buffers is 0 and then it is also 0 */
-#define BM_AF_THR_MIN				MV_MIN(0x00000020, num_of_buffers) /*
+#define BM_AF_THR_MIN(_num_of_buffers)		MV_MIN(0x00000020, _num_of_buffers) /*
 	32                unless number of buffers is 0 and then it is also 0 */
-#define BM_AF_THR_MAX				MV_MAX((num_of_buffers - 0x00000010), 0) /*
+#define BM_AF_THR_MAX(_num_of_buffers)		MV_MAX((_num_of_buffers - 0x00000010), 0) /*
 	num_of_buffers-16 unless number of buffers is 0 and then it is also 0 */
 #define BM_CACHE_SI_THR_MIN			0x00000010	/*  16 */
-#define BM_CACHE_SI_THR_MAX			(cache_num_of_buffers - 0x00000010)	/* cache_num_of_buffers - 16 */
+#define BM_CACHE_SI_THR_MAX(_cache_num_of_buffers)		(_cache_num_of_buffers - 0x00000010)	/*
+	cache_num_of_buffers - 16 */
 #define BM_CACHE_SO_THR_MIN			0x00000018	/*  24 */
-#define BM_CACHE_SO_THR_MAX			(cache_num_of_buffers - 0x00000008)	/* cache_num_of_buffers -  8 */
+#define BM_CACHE_SO_THR_MAX(_cache_num_of_buffers)		(_cache_num_of_buffers - 0x00000008)	/*
+	cache_num_of_buffers -  8 */
 #define BM_CACHE_START_MIN			         0
 #define BM_CACHE_START_MAX			0xFFFFFFFF
 #define BM_CACHE_END_MIN			0x00000020	/*  32 */
 #define BM_CACHE_END_MAX			0x0000007F	/* 127 */
 
-#define BM_START_MIN				         0
-#define BM_START_MAX				0xFFFFFFFF
-#define BM_OFFSET_MIN				         0
-#define BM_OFFSET_MAX				0xFFFFFFFF
-#define BM_DATA_SIZE_MIN			         0
-#define BM_DATA_SIZE_MAX			0xFFFFFFFF
-#define BM_DRAM_ADDRESS_LO_MIN		         0
-#define BM_DRAM_ADDRESS_LO_MAX		0xFFFFFFFF
-#define BM_DRAM_ADDRESS_HI_MIN		         0
-#define BM_DRAM_ADDRESS_HI_MAX		0xFFFFFFFF
+#define BM_DRAM_ADDRESS_MIN_MSB		         0
+#define BM_DRAM_ADDRESS_MIN_LSB		         0
+#define BM_DRAM_ADDRESS_MAX_MSB		0xFFFFFFFF
+#define BM_DRAM_ADDRESS_MAX_LSB		0xFFFFFFFF
 
 #define BM_DATA_PTR_MIN				         0
 #define BM_DATA_PTR_MAX				0xFFFFFFFF
 
-/*
-#define BM_QM_BUFFERS_MIN			48
-#define BM_GP_BUFFERS_MIN			48 (but can also be 0)
-
-#define BM_AE_THR_QM_MIN			16
-#define BM_AE_THR_GP_MIN			16 (unless number of buffers is 0 and then it is also 0)
-#define BM_AE_THR_QM_MAX			(num_of_buffers-32)
-#define BM_AE_THR_GP_MAX			(num_of_buffers-32) (unless number of buffers is
-										0 and then it is also 0)
-#define BM_AF_THR_QM_MIN			32
-#define BM_AF_THR_GP_MIN			32 (unless number of buffers is 0 and then it is 0)
-#define BM_AF_THR_QM_MAX			(num_of_buffers-16)
-#define BM_AF_THR_GP_MAX			(num_of_buffers-16)	(unless number of buffers is
-										0 and then it is also 0)
-
-#define BM_QM_BUFFERS_CACHE_MIN		32
-#define BM_GP_BUFFERS_CACHE_MIN		32
-#define BM_GP_BUFFERS_CACHE_MAX		1024 (if there is one pool then it is (1024-8)=1016
-
-#define BM_QM_SI_THR_MIN			8
-#define BM_GP_SI_THR_MIN			8
-#define BM_QM_SI_THR_MAX			cache_num_of_buffers - 16
-#define BM_GP_SI_THR_MAX			cache_num_of_buffers - 16
-#define BM_QM_SO_THR_MIN			24
-#define BM_GP_SO_THR_MIN			24
-#define BM_QM_SO_THR_MAX			cache_num_of_buffers - 8
-#define BM_GP_SO_THR_MAX			cache_num_of_buffers - 8
-
-#define BM_END_MIN					32
-*/
-
-/*
-#define BM_PID_TO_BANK(_pid)                \
-{									        \
-	_bid =                                  \
-	(((_pid >= 0) && (_pid   <=  7)) ?	0 :	\
-	(((_pid >= 8) && (_pid%4 ==  0)) ?	1 :	\
-	(((_pid >= 8) && (_pid%4 ==  1)) ?	2 :	\
-	(((_pid >= 8) && (_pid%4 ==  2)) ?	3 :	\
-	(((_pid >= 8) && (_pid%4 ==  3)) ?	4 :	\
-	-1)))))                                 \
-	if ((_bid >= 0) && (_bid <= 4))         \
-		return _bid;                        \
-	else                                    \
-		return EDOM;                        \
-}
-*/
-
 #define BM_PID_TO_BANK(_pid)                \
 	(((_pid >= 0) && (_pid   <=  3)) ?	0 :	\
 	(((_pid >= 8) && (_pid%4 ==  0)) ?	1 :	\
@@ -321,48 +260,11 @@ SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 	(((_pid >= 4) && (_pid   <= 31)) ? (_pid+4)    : \
 	-1))
 
-/*
-#define BM_MAGIC 0x24051974
-
-#define  DECLARE_BM_CTL_PTR(name, value)	{struct  bm_ctl *name = (struct  bm_ctl *)value; }
-
-#define CHECK_BM_CTL_PTR(ptr)		\
-{									\
-	if (!ptr)						\
-		return -EINVAL;				\
-	if (ptr->magic !=  BM_MAGIC)	\
-		return -EBADF;				\
-}
-
-#define  BM_CTL(name, handle)    {DECLARE_BM_CTL_PTR(name, handle);  CHECK_BM_CTL_PTR(name); }
-#define  BM_ENV(var) (var->hEnv)
-*/
-
 #define TRUNCATE(_truncated_value, _truncating_value) (_truncated_value - (_truncated_value % _truncating_value))
 
-#define BM_QM_PE_UNITS_TO_BYTES(_num_of_buffers) (_num_of_buffers * 4)
-#define BM_GP_PE_UNITS_TO_BYTES(_num_of_buffers, _pe_size)	\
-				(((_pe_size) == (40)) ? (_num_of_buffers * 8) : (_num_of_buffers * 4))
-/*
-#define BM_GP_POOL_DRAM_SIZE_IN_BYTES(_num_of_buffers, _pe_size)
-	if (_pe_size==32) (_num_of_buffers * 4)
-	if (_pe_size==40) (_num_of_buffers * 8)
-#if (_pe_size==32) (_num_of_buffers * 4)
-#if (_pe_size==40) (_num_of_buffers * 8)
-#define MIN(a,b) (((a)<(b))?(a):(b))
-
-#define CHECK_BM_CTL_PTR(ptr)		\
-{									\
-	if (!ptr)						\
-		return -EINVAL;				\
-	if (ptr->magic !=  BM_MAGIC)	\
-		return -EBADF;				\
-}
-*/
-
-/*
-typedef void * bm_handle;
-*/
+#define BM_QM_PE_UNITS_TO_BYTES(_num_of_pes) (_num_of_pes * 4)
+#define BM_GP_PE_UNITS_TO_BYTES(_num_of_pes, _pe_size)	\
+				(((_pe_size) == (40)) ? (_num_of_pes * 8) : (_num_of_pes * 4))
 
 /**
  *  Initialize BM module
@@ -409,7 +311,8 @@ int bm_enable_status_get(u32 *bm_req_rcv_en);
  *  Return values:
  *		0 - success
  */
-int bm_qm_gpm_pools_def_quick_init(u32 num_of_buffers, u32 *qece_base_address, u32 *pl_base_address);
+int bm_qm_gpm_pools_def_quick_init(u32 num_of_buffers, struct mv_a40 *qece_base_address,
+									struct mv_a40 *pl_base_address);
 
 /**
  *  Initiates of DRAM pools with default values
@@ -417,7 +320,8 @@ int bm_qm_gpm_pools_def_quick_init(u32 num_of_buffers, u32 *qece_base_address, u
  *  Return values:
  *		0 - success
  */
-int bm_qm_dram_pools_def_quick_init(u32 num_of_buffers, u32 *qece_base_address, u32 *pl_base_address);
+int bm_qm_dram_pools_def_quick_init(u32 num_of_buffers, struct mv_a40 *qece_base_address,
+									struct mv_a40 *pl_base_address);
 
 /**
  *  Initiates QM GPM pools
@@ -427,8 +331,8 @@ int bm_qm_dram_pools_def_quick_init(u32 num_of_buffers, u32 *qece_base_address,
  *  Return values:
  *		0 - success
  */
-int bm_qm_gpm_pools_quick_init(u32 num_of_buffers, u32 *qece_base_address,
-				u32 *pl_base_address, u32 ae_thr, u32 af_thr,
+int bm_qm_gpm_pools_quick_init(u32 num_of_buffers, struct mv_a40 *qece_base_address,
+				struct mv_a40 *pl_base_address, u32 ae_thr, u32 af_thr,
 				u32 cache_vmid, u32 cache_attr, u32 cache_so_thr, u32 cache_si_thr,
 				u32 cache_num_of_buffers);
 
@@ -440,8 +344,8 @@ int bm_qm_gpm_pools_quick_init(u32 num_of_buffers, u32 *qece_base_address,
  *  Return values:
  *		0 - success
  */
-int bm_qm_dram_pools_quick_init(u32 num_of_buffers, u32 *qece_base_address,
-					u32 *pl_base_address, u32 ae_thr, u32 af_thr,
+int bm_qm_dram_pools_quick_init(u32 num_of_buffers, struct mv_a40 *qece_base_address,
+					struct mv_a40 *pl_base_address, u32 ae_thr, u32 af_thr,
 					u32 cache_vmid, u32 cache_attr, u32 cache_so_thr, u32 cache_si_thr,
 					u32 cache_num_of_buffers);
 
@@ -465,7 +369,7 @@ int bm_gp_pool_def_basic_init(
 							u32 pool, /* pool number: general purpose pools 8 to 35 */
 							u32 num_of_buffers, /* equal or less
 								than num_of_buffers passed when initializing pool */
-							u32 *base_address,  /* DRAM base address */
+							struct mv_a40 *base_address,  /* DRAM base address */
 							u32 partition_model);  /* for small partition in cache */
 
 /**
@@ -477,7 +381,7 @@ int bm_gp_pool_basic_init(
 							u32 pool, /* pool number: general purpose pools 8 to 35 */
 							u32 num_of_buffers, /* equal or less
 								than num_of_buffers passed when initializing pool */
-							u32 *base_address,  /* DRAM base address */
+							struct mv_a40 *base_address,  /* DRAM base address */
 							u32 pe_size, /* PE size can be either 32bits or 40 bits */
 							u32 pool_pair, /* Pool_pair is
 								either 0 for false or 1 for true */
@@ -529,14 +433,14 @@ int bm_vmid_set(u32 bm_vmid);
  *		0 - success
  */
 int bm_gp_pool_def_quick_init(u32 pool, u32 num_of_buffers, u32 fill_level,
-							u32 *base_address, u32 partition_model);
+							struct mv_a40 *base_address, u32 partition_model);
 
 /**
  *  Configure BM registers and allocate memory for pools
  *  Return values:
  *		0 - success
  */
-int bm_gp_pool_quick_init(u32 pool, u32 num_of_buffers, u32 fill_level, u32 *base_address,
+int bm_gp_pool_quick_init(u32 pool, u32 num_of_buffers, u32 fill_level, struct mv_a40 *base_address,
 					u32 pe_size, u32 pool_pair, u32 ae_thr, u32 af_thr,
 					u32 cache_vmid, u32 cache_attr,	u32 cache_so_thr, u32 cache_si_thr,
 					u32 cache_num_of_buffers);
@@ -619,7 +523,7 @@ int bm_pool_memory_fill(
 						u32 pool, /* pool number: 0 to 3 */
 						u32 num_of_buffers, /* equal or less
 						than num_of_buffers passed when initializing pool */
-						u32 *base_address); /* pool base address.
+						struct mv_a40 *base_address); /* pool base address.
 						same as the address passed when initializing pool */
 
 /**
@@ -632,7 +536,7 @@ int bm_pool_dram_set(
 					u32 pool, /* pool number: any pool QM and general purpose */
 					u32 num_of_buffers,  /* number of buffers/PEs in pool*/
 					u32 pe_size, /* PE size can be either 32bits or 40 bits */
-					u32 *base_address, /* DRAM base address */
+					struct mv_a40 *base_address, /* DRAM base address */
 					u32 ae_thr, /* almost empty threshold for pool */
 					u32 af_thr); /* almost full threshold for pool */
 
diff --git a/drivers/net/ethernet/marvell/pp3/bm/mv_bm_regs.h b/drivers/net/ethernet/marvell/pp3/bm/mv_bm_regs.h
index 137bb5e..4571b6b 100644
--- a/drivers/net/ethernet/marvell/pp3/bm/mv_bm_regs.h
+++ b/drivers/net/ethernet/marvell/pp3/bm/mv_bm_regs.h
@@ -143,8 +143,8 @@ struct bm_d_mng_ball_stat_data {
 	u32 dram_af_thr:18;			/* byte[ 0-20],bit[32-49]	0x0 */
 	u32 _reserved_2:14;			/* byte[ 0-20],bit[50-63] */
 /*	int64_t dram_start:40;		 byte[ 0-20],bit[ 64-103]	0x0 */
-	u32 dram_start_lo:32;		/* byte[ 0-20],bit[ 64- 95]	0x0 */
-	u32 dram_start_hi:8;		/* byte[ 0-20],bit[ 97-103]	0x0 */
+	u32 dram_start_lsb:32;		/* byte[ 0-20],bit[ 64- 95]	0x0 */
+	u32 dram_start_msb:8;		/* byte[ 0-20],bit[ 97-103]	0x0 */
 	u32 _reserved_3:24;			/* byte[ 0-20],bit[104-127] */
 	u32 dram_size:18;			/* byte[ 0-20],bit[128-145]	0x0 */
 	u32 _reserved_4:14;			/* byte[ 0-20],bit[146-159] */
@@ -213,8 +213,8 @@ struct bm_cache_b0_mem_data {
 
 struct bm_cache_bgp_data {
 /*	uint64_t cache_bgp_data:40; */	/* byte[ 0- 7],bit[ 0-39]	0x0 */
-	u32 cache_bgp_data_lo:32;		/* byte[ 0- 7],bit[ 0-31]	0x0 */
-	u32 cache_bgp_data_hi:8;		/* byte[ 0- 7],bit[32-39]	0x0 */
+	u32 cache_bgp_data_lsb:32;		/* byte[ 0- 7],bit[ 0-31]	0x0 */
+	u32 cache_bgp_data_msb:8;		/* byte[ 0- 7],bit[32-39]	0x0 */
 	u32 _reserved:24;				/* byte[ 0- 7],bit[40-63] */
 } __ATTRIBUTE_PACKED__;
 
diff --git a/drivers/net/ethernet/marvell/pp3/bm/mv_bm_sysfs.c b/drivers/net/ethernet/marvell/pp3/bm/mv_bm_sysfs.c
index b47d162..eed4788 100644
--- a/drivers/net/ethernet/marvell/pp3/bm/mv_bm_sysfs.c
+++ b/drivers/net/ethernet/marvell/pp3/bm/mv_bm_sysfs.c
@@ -89,15 +89,6 @@ SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 	pr_info("%s is called\n", attr->attr.name);	\
 }
 
-#define BM_MALLOC_SIZE 0x00000100
-
-#ifdef __linux__
-#define BM_MALLOC(_size)	((u32)kmalloc(_size, GFP_KERNEL))
-#else
-#define BM_MALLOC(_size)	 ((u32)malloc(_size))
-#endif
-
-
 static ssize_t mv_bm_help(char *b)
 {
 	int o = 0; /* buffer offset */
@@ -152,6 +143,12 @@ static ssize_t mv_bm_help(char *b)
 	o += scnprintf(b+o, s-o, "            [pp]   pool pair: 1 - alloc in pairs, 0 - alloc not in pairs\n");
 	o += scnprintf(b+o, s-o, "            [fl]   fill level\n");
 	o += scnprintf(b+o, s-o, "            [pm]   partition model: 1 - small partition in cache, 0 - big\n");
+	o += scnprintf(b+o, s-o, "            [rD]   arDomain\n");
+	o += scnprintf(b+o, s-o, "            [wD]   awDomain\n");
+	o += scnprintf(b+o, s-o, "            [rC]   arCache\n");
+	o += scnprintf(b+o, s-o, "            [wC]   awCache\n");
+	o += scnprintf(b+o, s-o, "            [rQ]   arQOS\n");
+	o += scnprintf(b+o, s-o, "            [wQ]   awQOS\n");
 	o += scnprintf(b+o, s-o, "\n");
 
 	return o;
@@ -242,62 +239,56 @@ static ssize_t mv_bm_config(struct device *dev,
 		pr_info("\t bm_req_rcv_en = %d\n", bm_req_rcv_en);
 	} else if (!strcmp(name, "qm_gpm_pools_def_quick_init")) {
 		u32 num_of_buffers;
-		struct mv_word40 qece_base_address, pl_base_address;
+		struct mv_a40 qece_base_address, pl_base_address;
 
 		/* Read input values */
 		PR_INFO_CALLED
-		num_of_buffers = qece_base_address.hi = qece_base_address.lo = pl_base_address.hi = pl_base_address.lo = 0xFFFFFFFF;
+		num_of_buffers = qece_base_address.dma_msb = qece_base_address.virt_msb = pl_base_address.dma_msb = pl_base_address.virt_msb = 0;
 		sscanf(buf, "%d", &num_of_buffers);
-		qece_base_address.hi = 0;
-		qece_base_address.lo = BM_MALLOC(num_of_buffers*4);
-		pl_base_address.hi = 0;
-		pl_base_address.lo = BM_MALLOC(num_of_buffers*4);
-		rc = bm_qm_gpm_pools_def_quick_init(num_of_buffers, (u32 *)&qece_base_address, (u32 *)&pl_base_address);
+		qece_base_address.virt_lsb = (u32)dma_alloc_coherent(NULL, num_of_buffers*4, &qece_base_address.dma_lsb, GFP_KERNEL);
+		pl_base_address.virt_lsb   = (u32)dma_alloc_coherent(NULL, num_of_buffers*4, &pl_base_address.dma_lsb, GFP_KERNEL);
+		rc = bm_qm_gpm_pools_def_quick_init(num_of_buffers, &qece_base_address, &pl_base_address);
 		if (rc != OK)
 			PR_ERR_CODE(rc)
 	} else if (!strcmp(name, "qm_dram_pools_def_quick_init")) {
 		u32 num_of_buffers;
-		struct mv_word40 qece_base_address, pl_base_address;
+		struct mv_a40 qece_base_address, pl_base_address;
 
 		/* Read input values */
 		PR_INFO_CALLED
-		num_of_buffers = qece_base_address.hi = qece_base_address.lo = pl_base_address.hi = pl_base_address.lo = 0xFFFFFFFF;
-		qece_base_address.hi = 0;
-		qece_base_address.lo = BM_MALLOC(BM_MALLOC_SIZE);
-		pl_base_address.hi = 0;
-		pl_base_address.lo = BM_MALLOC(BM_MALLOC_SIZE);
+		num_of_buffers = qece_base_address.dma_msb = qece_base_address.virt_msb = pl_base_address.dma_msb = pl_base_address.virt_msb = 0;
+		qece_base_address.virt_lsb = (u32)dma_alloc_coherent(NULL, num_of_buffers*4, &qece_base_address.dma_lsb, GFP_KERNEL);
+		pl_base_address.virt_lsb   = (u32)dma_alloc_coherent(NULL, num_of_buffers*4, &pl_base_address.dma_lsb, GFP_KERNEL);
 		sscanf(buf, "%d", &num_of_buffers);
-		rc = bm_qm_dram_pools_def_quick_init(num_of_buffers, (u32 *)&qece_base_address, (u32 *)&pl_base_address);
+		rc = bm_qm_dram_pools_def_quick_init(num_of_buffers, &qece_base_address, &pl_base_address);
 		if (rc != OK)
 			PR_ERR_CODE(rc)
 	} else if (!strcmp(name, "qm_gpm_pools_quick_init")) {
 		u32 num_of_buffers, ae_thr, af_thr, cache_vmid, cache_attr, cache_so_thr, cache_si_thr, cache_num_of_buffers;
-		struct mv_word40 qece_base_address, pl_base_address;
+		struct mv_a40 qece_base_address, pl_base_address;
 
 		/* Read input values */
 		PR_INFO_CALLED
-		num_of_buffers = qece_base_address.hi = qece_base_address.lo = pl_base_address.hi = pl_base_address.lo = ae_thr = af_thr = cache_vmid = cache_attr = cache_so_thr = cache_si_thr = cache_num_of_buffers = 0xFFFFFFFF;
-		qece_base_address.hi = 0;
-		qece_base_address.lo = BM_MALLOC(BM_MALLOC_SIZE);
-		pl_base_address.hi = 0;
-		pl_base_address.lo = BM_MALLOC(BM_MALLOC_SIZE);
+		num_of_buffers = qece_base_address.dma_msb = qece_base_address.virt_msb = pl_base_address.dma_msb = pl_base_address.virt_msb = 0;
+		ae_thr = af_thr = cache_vmid = cache_attr = cache_so_thr = cache_si_thr = cache_num_of_buffers = 0xFFFFFFFF;
 		sscanf(buf, "%d %d %d %d %d %d %d %d", &num_of_buffers, &ae_thr, &af_thr, &cache_vmid, &cache_attr, &cache_so_thr, &cache_si_thr, &cache_num_of_buffers);
-		rc = bm_qm_gpm_pools_quick_init(num_of_buffers, (u32 *)&qece_base_address, (u32 *)&pl_base_address, ae_thr, af_thr, cache_vmid, cache_attr, cache_so_thr, cache_si_thr, cache_num_of_buffers);
+		qece_base_address.virt_lsb = (u32)dma_alloc_coherent(NULL, num_of_buffers*4, &qece_base_address.dma_lsb, GFP_KERNEL);
+		pl_base_address.virt_lsb   = (u32)dma_alloc_coherent(NULL, num_of_buffers*4, &pl_base_address.dma_lsb, GFP_KERNEL);
+		rc = bm_qm_gpm_pools_quick_init(num_of_buffers, &qece_base_address, &pl_base_address, ae_thr, af_thr, cache_vmid, cache_attr, cache_so_thr, cache_si_thr, cache_num_of_buffers);
 		if (rc != OK)
 			PR_ERR_CODE(rc)
 	} else if (!strcmp(name, "qm_dram_pools_quick_init")) {
 		u32 num_of_buffers, ae_thr, af_thr, cache_vmid, cache_attr, cache_so_thr, cache_si_thr, cache_num_of_buffers;
-		struct mv_word40 qece_base_address, pl_base_address;
+		struct mv_a40 qece_base_address, pl_base_address;
 
 		/* Read input values */
 		PR_INFO_CALLED
-		num_of_buffers = qece_base_address.hi = qece_base_address.lo = pl_base_address.hi = pl_base_address.lo = ae_thr = af_thr = cache_vmid = cache_attr = cache_so_thr = cache_si_thr = cache_num_of_buffers = 0xFFFFFFFF;
-		qece_base_address.hi = 0;
-		qece_base_address.lo = BM_MALLOC(BM_MALLOC_SIZE);
-		pl_base_address.hi = 0;
-		pl_base_address.lo = BM_MALLOC(BM_MALLOC_SIZE);
+		num_of_buffers = qece_base_address.dma_msb = qece_base_address.virt_msb = pl_base_address.dma_msb = pl_base_address.virt_msb = 0;
+		ae_thr = af_thr = cache_vmid = cache_attr = cache_so_thr = cache_si_thr = cache_num_of_buffers = 0xFFFFFFFF;
 		sscanf(buf, "%d %d %d %d %d %d %d %d", &num_of_buffers, &ae_thr, &af_thr, &cache_vmid, &cache_attr, &cache_so_thr, &cache_si_thr, &cache_num_of_buffers);
-		rc = bm_qm_dram_pools_quick_init(num_of_buffers, (u32 *)&qece_base_address, (u32 *)&pl_base_address, ae_thr, af_thr, cache_vmid, cache_attr, cache_so_thr, cache_si_thr, cache_num_of_buffers);
+		qece_base_address.virt_lsb = (u32)dma_alloc_coherent(NULL, num_of_buffers*4, &qece_base_address.dma_lsb, GFP_KERNEL);
+		pl_base_address.virt_lsb   = (u32)dma_alloc_coherent(NULL, num_of_buffers*4, &pl_base_address.dma_lsb, GFP_KERNEL);
+		rc = bm_qm_dram_pools_quick_init(num_of_buffers, &qece_base_address, &pl_base_address, ae_thr, af_thr, cache_vmid, cache_attr, cache_so_thr, cache_si_thr, cache_num_of_buffers);
 		if (rc != OK)
 			PR_ERR_CODE(rc)
 	} else if (!strcmp(name, "pool_quick_init_status_get")) {
@@ -313,28 +304,28 @@ static ssize_t mv_bm_config(struct device *dev,
 		pr_info("\t completed = %d\n", completed);
 	} else if (!strcmp(name, "gp_pool_def_basic_init")) {
 		u32 pool, num_of_buffers, partition_model;
-		struct mv_word40 base_address;
+		struct mv_a40 base_address;
 
 		/* Read input values */
 		PR_INFO_CALLED
-		pool = num_of_buffers = base_address.hi = base_address.lo = partition_model = 0xFFFFFFFF;
-		base_address.hi = 0;
-		base_address.lo = BM_MALLOC(BM_MALLOC_SIZE);
+		num_of_buffers = base_address.dma_msb = base_address.virt_msb = 0;
+		pool = partition_model = 0xFFFFFFFF;
 		sscanf(buf, "%d %d %d", &pool, &num_of_buffers, &partition_model);
-		rc = bm_gp_pool_def_basic_init(pool, num_of_buffers, (u32 *)&base_address, partition_model);
+		base_address.virt_lsb = (u32)dma_alloc_coherent(NULL, num_of_buffers*4, &base_address.dma_lsb, GFP_KERNEL);
+		rc = bm_gp_pool_def_basic_init(pool, num_of_buffers, &base_address, partition_model);
 		if (rc != OK)
 			PR_ERR_CODE(rc)
 	} else if (!strcmp(name, "gp_pool_basic_init")) {
 		u32 pool, num_of_buffers, pe_size, pool_pair, ae_thr, af_thr, cache_vmid, cache_attr, cache_so_thr, cache_si_thr, cache_num_of_buffers;
-		struct mv_word40 base_address;
+		struct mv_a40 base_address;
 
 		/* Read input values */
 		PR_INFO_CALLED
-		pool = num_of_buffers = base_address.hi = base_address.lo = pe_size = pool_pair = ae_thr = af_thr = cache_vmid = cache_attr = cache_so_thr = cache_si_thr = cache_num_of_buffers = 0xFFFFFFFF;
-		base_address.hi = 0;
-		base_address.lo = BM_MALLOC(BM_MALLOC_SIZE);
+		num_of_buffers = base_address.dma_msb = base_address.virt_msb = 0;
+		pool = pe_size = pool_pair = ae_thr = af_thr = cache_vmid = cache_attr = cache_so_thr = cache_si_thr = cache_num_of_buffers = 0xFFFFFFFF;
 		sscanf(buf, "%d %d %d %d %d %d %d %d %d %d %d", &pool, &num_of_buffers, &pe_size, &pool_pair, &ae_thr, &af_thr, &cache_vmid, &cache_attr, &cache_so_thr, &cache_si_thr, &cache_num_of_buffers);
-		rc = bm_gp_pool_basic_init(pool, num_of_buffers, (u32 *)&base_address, pe_size, pool_pair, ae_thr, af_thr, cache_vmid, cache_attr, cache_so_thr, cache_si_thr, cache_num_of_buffers);
+		base_address.virt_lsb = (u32)dma_alloc_coherent(NULL, num_of_buffers*4, &base_address.dma_lsb, GFP_KERNEL);
+		rc = bm_gp_pool_basic_init(pool, num_of_buffers, &base_address, pe_size, pool_pair, ae_thr, af_thr, cache_vmid, cache_attr, cache_so_thr, cache_si_thr, cache_num_of_buffers);
 		if (rc != OK)
 			PR_ERR_CODE(rc)
 	} else if (!strcmp(name, "enable")) {
@@ -370,28 +361,28 @@ static ssize_t mv_bm_config(struct device *dev,
 			PR_ERR_CODE(rc)
 	} else if (!strcmp(name, "gp_pool_def_quick_init")) {
 		u32 pool, num_of_buffers, fill_level, partition_model;
-		struct mv_word40 base_address;
+		struct mv_a40 base_address;
 
 		/* Read input values */
 		PR_INFO_CALLED
-		pool = num_of_buffers = fill_level = base_address.hi = base_address.lo = partition_model = 0xFFFFFFFF;
-		base_address.hi = 0;
-		base_address.lo = BM_MALLOC(BM_MALLOC_SIZE);
+		num_of_buffers = base_address.dma_msb = base_address.virt_msb = fill_level = 0;
+		pool = partition_model = 0xFFFFFFFF;
 		sscanf(buf, "%d %d %d %d", &pool, &num_of_buffers, &fill_level, &partition_model);
-		rc = bm_gp_pool_def_quick_init(pool, num_of_buffers, fill_level, (u32 *)&base_address, partition_model);
+		base_address.virt_lsb = (u32)dma_alloc_coherent(NULL, num_of_buffers*4, &base_address.dma_lsb, GFP_KERNEL);
+		rc = bm_gp_pool_def_quick_init(pool, num_of_buffers, fill_level, &base_address, partition_model);
 		if (rc != OK)
 			PR_ERR_CODE(rc)
 	} else if (!strcmp(name, "gp_pool_quick_init")) {
 		u32 pool, num_of_buffers, fill_level, pe_size, pool_pair, ae_thr, af_thr, cache_vmid, cache_attr, cache_so_thr, cache_si_thr, cache_num_of_buffers;
-		struct mv_word40 base_address;
+		struct mv_a40 base_address;
 
 		/* Read input values */
 		PR_INFO_CALLED
-		pool = num_of_buffers = fill_level = base_address.hi = base_address.lo = pe_size = pool_pair = ae_thr = af_thr = cache_vmid = cache_attr = cache_so_thr = cache_si_thr = cache_num_of_buffers = 0xFFFFFFFF;
-		base_address.hi = 0;
-		base_address.lo = BM_MALLOC(BM_MALLOC_SIZE);
+		num_of_buffers = base_address.dma_msb = base_address.virt_msb = fill_level = 0;
+		pool = pe_size = pool_pair = ae_thr = af_thr = cache_vmid = cache_attr = cache_so_thr = cache_si_thr = cache_num_of_buffers = 0xFFFFFFFF;
 		sscanf(buf, "%d %d %d %d %d %d %d %d %d %d %d %d", &pool, &num_of_buffers, &fill_level, &pe_size, &pool_pair, &ae_thr, &af_thr, &cache_vmid, &cache_attr, &cache_so_thr, &cache_si_thr, &cache_num_of_buffers);
-		rc = bm_gp_pool_quick_init(pool, num_of_buffers, fill_level, (u32 *)&base_address, pe_size, pool_pair, ae_thr, af_thr, cache_vmid, cache_attr, cache_so_thr, cache_si_thr, cache_num_of_buffers);
+		base_address.virt_lsb = (u32)dma_alloc_coherent(NULL, num_of_buffers*4, &base_address.dma_lsb, GFP_KERNEL);
+		rc = bm_gp_pool_quick_init(pool, num_of_buffers, fill_level, &base_address, pe_size, pool_pair, ae_thr, af_thr, cache_vmid, cache_attr, cache_so_thr, cache_si_thr, cache_num_of_buffers);
 		if (rc != OK)
 			PR_ERR_CODE(rc)
 	} else if (!strcmp(name, "global_registers_dump")) {
@@ -461,28 +452,28 @@ static ssize_t mv_bm_config(struct device *dev,
 			PR_ERR_CODE(rc)
 	} else if (!strcmp(name, "pool_memory_fill")) {
 		u32 pool,  num_of_buffers;
-		struct mv_word40 base_address;
+		struct mv_a40 base_address;
 
 		/* Read input values */
 		PR_INFO_CALLED
-		pool = num_of_buffers = base_address.hi = base_address.lo = 0xFFFFFFFF;
-		base_address.hi = 0;
-		base_address.lo = BM_MALLOC(BM_MALLOC_SIZE);
+		num_of_buffers = base_address.dma_msb = base_address.virt_msb = 0;
+		pool = 0xFFFFFFFF;
 		sscanf(buf, "%d %d", &pool, &num_of_buffers);
-		rc = bm_pool_memory_fill(pool, num_of_buffers, (u32 *)&base_address);
+		base_address.virt_lsb = (u32)dma_alloc_coherent(NULL, num_of_buffers*4, &base_address.dma_lsb, GFP_KERNEL);
+		rc = bm_pool_memory_fill(pool, num_of_buffers, &base_address);
 		if (rc != OK)
 			PR_ERR_CODE(rc)
 	} else if (!strcmp(name, "pool_dram_set"))	{
 		u32 pool,  num_of_buffers, pe_size, ae_thr, af_thr;
-		struct mv_word40 base_address;
+		struct mv_a40 base_address;
 
 		/* Read input values */
 		PR_INFO_CALLED
-		pool = num_of_buffers = pe_size = base_address.hi = base_address.lo = ae_thr = af_thr = 0xFFFFFFFF;
-		base_address.hi = 0;
-		base_address.lo = BM_MALLOC(BM_MALLOC_SIZE);
+		num_of_buffers = base_address.dma_msb = base_address.virt_msb = 0;
+		pool = pe_size = ae_thr = af_thr = 0xFFFFFFFF;
 		sscanf(buf, "%d %d %d %d %d", &pool, &num_of_buffers, &pe_size, &ae_thr, &af_thr);
-		rc = bm_pool_dram_set(pool,  num_of_buffers, pe_size,  (u32 *)&base_address, ae_thr, af_thr);
+		base_address.virt_lsb = (u32)dma_alloc_coherent(NULL, num_of_buffers*4, &base_address.dma_lsb, GFP_KERNEL);
+		rc = bm_pool_dram_set(pool,  num_of_buffers, pe_size,  &base_address, ae_thr, af_thr);
 		if (rc != OK)
 			PR_ERR_CODE(rc)
 	} else if (!strcmp(name, "pool_fill_level_set")) {
@@ -553,7 +544,7 @@ static ssize_t mv_bm_config(struct device *dev,
 		base_address = offset = wordsNumber = dataPtr = 0xFFFFFFFF;
 		/* Read input values */
 		sscanf(buf, "%x %x %x %x", &base_address, &offset, &wordsNumber, &dataPtr);
-		rc = bm_register_read(base_address, offset, wordsNumber, (u32 *)&dataPtr);
+		rc = bm_register_read(base_address, offset, wordsNumber, &dataPtr);
 		if (rc != OK)
 			PR_ERR_CODE(rc)
 	} else if (!strcmp(name, "register_write")) {
@@ -563,7 +554,7 @@ static ssize_t mv_bm_config(struct device *dev,
 		base_address = offset = wordsNumber = dataPtr = 0xFFFFFFFF;
 		/* Read input values */
 		sscanf(buf, "%x %x %x %x", &base_address, &offset, &wordsNumber, &dataPtr);
-		rc = bm_register_write(base_address, offset, wordsNumber, (u32 *)&dataPtr);
+		rc = bm_register_write(base_address, offset, wordsNumber, &dataPtr);
 		if (rc != OK)
 			PR_ERR_CODE(rc)
 	} else {
diff --git a/drivers/net/ethernet/marvell/pp3/common/mv_hw_if.h b/drivers/net/ethernet/marvell/pp3/common/mv_hw_if.h
index 4fbd9be..30b2498 100644
--- a/drivers/net/ethernet/marvell/pp3/common/mv_hw_if.h
+++ b/drivers/net/ethernet/marvell/pp3/common/mv_hw_if.h
@@ -68,10 +68,12 @@ SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 #include <linux/kernel.h>
 #include <linux/io.h>
 
-struct mv_word40 {
-	u32 lo;				/* byte[ 0- 7],bit[ 0-31] */
-	u8  hi;				/* byte[ 0- 7],bit[32-39] */
-	u8 _reserved_6[3];		/* byte[ 0- 7],bit[40-63] */
+struct mv_a40 {
+	u32 virt_lsb;			/* byte[ 0-11] ,bit[ 0-31] */
+	u32 dma_lsb;			/* byte[ 0-11] ,bit[32-63] */
+	u8  virt_msb;			/* byte[ 0-11] ,bit[64-71] */
+	u8  dma_msb;			/* byte[ 0-11] ,bit[72-79] */
+	u8 _reserved[2];		/* byte[ 0-11] ,bit[80-95] */
 };
 
 
diff --git a/drivers/net/ethernet/marvell/pp3/qm/mv_qm.c b/drivers/net/ethernet/marvell/pp3/qm/mv_qm.c
index f09bba7..a812b53 100644
--- a/drivers/net/ethernet/marvell/pp3/qm/mv_qm.c
+++ b/drivers/net/ethernet/marvell/pp3/qm/mv_qm.c
@@ -107,7 +107,7 @@ int qm_restart(void)
 	return rc;
 }
 
-int qm_pfe_base_address_pool_set(u32 *qece_base_address, u32 *pyld_base_address)
+int qm_pfe_base_address_pool_set(struct mv_a40 *qece_base_address, struct mv_a40 *pyld_base_address)
 {
 	int rc = -QM_INPUT_NOT_IN_RANGE;
 	u32 reg_base_address, reg_size, reg_offset;
@@ -124,8 +124,7 @@ int qm_pfe_base_address_pool_set(u32 *qece_base_address, u32 *pyld_base_address)
 	rc = qm_register_read(reg_base_address, reg_offset, reg_size, (u32 *)&reg_qece_dram_base_address_hi);
 	if (rc != OK)
 		return rc;
-
-	reg_qece_dram_base_address_hi.qece_dram_base_address_hi	 = ((struct mv_word40 *)qece_base_address)->hi;
+	reg_qece_dram_base_address_hi.qece_dram_base_address_hi	= qece_base_address->dma_msb;
 	rc = qm_register_write(reg_base_address, reg_offset, reg_size, (u32 *)&reg_qece_dram_base_address_hi);
 	if (rc != OK)
 		return rc;
@@ -137,8 +136,7 @@ int qm_pfe_base_address_pool_set(u32 *qece_base_address, u32 *pyld_base_address)
 	rc = qm_register_read(reg_base_address, reg_offset, reg_size, (u32 *)&reg_qece_dram_base_address_lo);
 	if (rc != OK)
 		return rc;
-
-	reg_qece_dram_base_address_lo.qece_dram_base_address_low = ((struct mv_word40 *)qece_base_address)->lo;
+	reg_qece_dram_base_address_lo.qece_dram_base_address_low = qece_base_address->dma_lsb;
 	rc = qm_register_write(reg_base_address, reg_offset, reg_size, (u32 *)&reg_qece_dram_base_address_lo);
 	if (rc != OK)
 		return rc;
@@ -150,7 +148,7 @@ int qm_pfe_base_address_pool_set(u32 *qece_base_address, u32 *pyld_base_address)
 	rc = qm_register_read(reg_base_address, reg_offset, reg_size, (u32 *)&reg_pyld_dram_base_address_hi);
 	if (rc != OK)
 		return rc;
-	reg_pyld_dram_base_address_hi.pyld_dram_base_address_hi	 = ((struct mv_word40 *)pyld_base_address)->hi;
+	reg_pyld_dram_base_address_hi.pyld_dram_base_address_hi	 = pyld_base_address->dma_msb;
 	rc = qm_register_write(reg_base_address, reg_offset, reg_size, (u32 *)&reg_pyld_dram_base_address_hi);
 	if (rc != OK)
 		return rc;
@@ -162,8 +160,7 @@ int qm_pfe_base_address_pool_set(u32 *qece_base_address, u32 *pyld_base_address)
 	rc = qm_register_read(reg_base_address, reg_offset, reg_size, (u32 *)&reg_pyld_dram_base_address_lo);
 	if (rc != OK)
 		return rc;
-
-	reg_pyld_dram_base_address_lo.pyld_dram_base_address_low = ((struct mv_word40 *)pyld_base_address)->lo;
+	reg_pyld_dram_base_address_lo.pyld_dram_base_address_low = pyld_base_address->dma_lsb;
 	rc = qm_register_write(reg_base_address, reg_offset, reg_size, (u32 *)&reg_pyld_dram_base_address_lo);
 
 	return rc;
diff --git a/drivers/net/ethernet/marvell/pp3/qm/mv_qm.h b/drivers/net/ethernet/marvell/pp3/qm/mv_qm.h
index 248c895..3ee73c0 100644
--- a/drivers/net/ethernet/marvell/pp3/qm/mv_qm.h
+++ b/drivers/net/ethernet/marvell/pp3/qm/mv_qm.h
@@ -331,8 +331,8 @@ int qm_restart(void);
  *		0 - success
  */
 int qm_pfe_base_address_pool_set(
-							u32 *pl_base_address, /* Payload DRAM base address */
-							u32 *qece_base_address); /* QE/CE DRAM base address */
+							struct mv_a40 *pl_base_address, /* Payload DRAM base address */
+							struct mv_a40 *qece_base_address); /* QE/CE DRAM base address */
 
 /**
  *  Enables QM,
-- 
1.7.5.4

