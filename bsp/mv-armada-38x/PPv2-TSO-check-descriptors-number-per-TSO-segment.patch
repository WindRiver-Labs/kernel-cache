From 87f8fb94d97a30fc2228a558e9911a285ff77ace Mon Sep 17 00:00:00 2001
From: Yoni Farhadian <yonif@marvell.com>
Date: Wed, 24 Jul 2013 17:42:21 +0300
Subject: [PATCH 0892/1825] PPv2: TSO: check descriptors number per TSO
 segment

https://github.com/MISL-EBU-System-SW/misl-windriver.git linux-3.4.69-14t2-read
commit 41166105afb97e1c50a6c861aa2082f7c625ce42

	- TSO function used to check if there are enough descriptors for the whole TCP segment
		- i.e. required number of descriptors was: 2 * GSO_Segments + SKB_Fragments
	- This is changed to check if there are enough descriptors per TSO segment
		- i.e. required number of descriptor per segment is: remaining fragments + 2
		- + 2: one descriptor for TCP header, one for current fragment (or SKB->data)
	- To improve resource checking, TX done operation is done when necessary

Change-Id: I043c808f79f6c5a84800576f70a75b416794eefd
Signed-off-by: Yoni Farhadian <yonif@marvell.com>
Reviewed-on: http://vgitil04.il.marvell.com:8080/2845
Reviewed-by: Nadav Haklai <nadavh@marvell.com>
Tested-by: Nadav Haklai <nadavh@marvell.com>
Signed-off-by: Zhong Hongbo <hongbo.zhong@windriver.com>
---
 .../mv_drivers_lsp/mv_pp2/net_dev/mv_netdev.c      |  167 +++++++++++---------
 .../mv_drivers_lsp/mv_pp2/net_dev/mv_netdev.h      |    1 +
 2 files changed, 90 insertions(+), 78 deletions(-)

diff --git a/arch/arm/plat-armada/mv_drivers_lsp/mv_pp2/net_dev/mv_netdev.c b/arch/arm/plat-armada/mv_drivers_lsp/mv_pp2/net_dev/mv_netdev.c
index eac014f..7716c77 100644
--- a/arch/arm/plat-armada/mv_drivers_lsp/mv_pp2/net_dev/mv_netdev.c
+++ b/arch/arm/plat-armada/mv_drivers_lsp/mv_pp2/net_dev/mv_netdev.c
@@ -139,7 +139,7 @@ static int mv_eth_pool_destroy(int pool);
 static struct bm_pool *mv_eth_pool_use(int pool, enum mv_eth_bm_type type, int pkt_size);
 #ifdef CONFIG_MV_ETH_TSO
 int mv_eth_tx_tso(struct sk_buff *skb, struct net_device *dev, struct mv_eth_tx_spec *tx_spec,
-			struct txq_cpu_ctrl *txq_ctrl, struct aggr_tx_queue *aggr_txq_ctrl);
+			struct tx_queue *txq_ctrl, struct aggr_tx_queue *aggr_txq_ctrl);
 #endif
 
 /* Get the configuration string from the Kernel Command Line */
@@ -1889,7 +1889,7 @@ static int mv_eth_tx(struct sk_buff *skb, struct net_device *dev)
 #ifdef CONFIG_MV_ETH_TSO
 	/* GSO/TSO */
 	if (skb_is_gso(skb)) {
-		frags = mv_eth_tx_tso(skb, dev, tx_spec_ptr, txq_cpu_ptr, aggr_txq_ctrl);
+		frags = mv_eth_tx_tso(skb, dev, tx_spec_ptr, txq_ctrl, aggr_txq_ctrl);
 		goto out;
 	}
 #endif /* CONFIG_MV_ETH_TSO */
@@ -2015,28 +2015,26 @@ out:
 static inline int mv_eth_tso_validate(struct sk_buff *skb, struct net_device *dev)
 {
 	if (!(dev->features & NETIF_F_TSO)) {
-		printk(KERN_ERR "error: (skb_is_gso(skb) returns true but features is not NETIF_F_TSO\n");
+		pr_err("error: (skb_is_gso(skb) returns true but features is not NETIF_F_TSO\n");
 		return 1;
 	}
-
 	if (skb_shinfo(skb)->frag_list != NULL) {
-		printk(KERN_ERR "***** ERROR: frag_list is not null\n");
+		pr_err("***** ERROR: frag_list is not null\n");
 		return 1;
 	}
-
 	if (skb_shinfo(skb)->gso_segs == 1) {
-		printk(KERN_ERR "***** ERROR: only one TSO segment\n");
+		pr_err("***** ERROR: only one TSO segment\n");
 		return 1;
 	}
-
 	if (skb->len <= skb_shinfo(skb)->gso_size) {
-		printk(KERN_ERR "***** ERROR: total_len (%d) less than gso_size (%d)\n", skb->len, skb_shinfo(skb)->gso_size);
+		pr_err("***** ERROR: total_len (%d) less than gso_size (%d)\n", skb->len, skb_shinfo(skb)->gso_size);
 		return 1;
 	}
 	if ((htons(ETH_P_IP) != skb->protocol) || (ip_hdr(skb)->protocol != IPPROTO_TCP) || (tcp_hdr(skb) == NULL)) {
-		printk(KERN_ERR "***** ERROR: Protocol is not TCP over IP\n");
+		pr_err("***** ERROR: Protocol is not TCP over IP\n");
 		return 1;
 	}
+
 	return 0;
 }
 
@@ -2130,42 +2128,26 @@ static inline int mv_eth_tso_build_data_desc(struct pp2_tx_desc *tx_desc, struct
  *   send a packet.                                        *
  ***********************************************************/
 int mv_eth_tx_tso(struct sk_buff *skb, struct net_device *dev, struct mv_eth_tx_spec *tx_spec,
-			struct txq_cpu_ctrl *txq_ctrl, struct aggr_tx_queue *aggr_txq_ctrl)
+			struct tx_queue *txq_ctrl, struct aggr_tx_queue *aggr_txq_ctrl)
 {
-	int frag = 0;
+	int ptxq, frag = 0;
 	int total_len, hdr_len, size, frag_size, data_left;
+	int total_desc_num, seg_desc_num, total_bytes = 0;
 	char *frag_ptr;
-	int totalDescNum, totalBytes = 0;
 	struct pp2_tx_desc *tx_desc;
-	MV_U16 ip_id;
+	struct txq_cpu_ctrl *txq_cpu_ptr = NULL;
+	MV_U16 ip_id, *mh = NULL;
 	MV_U32 tcp_seq = 0;
 	skb_frag_t *skb_frag_ptr;
 	const struct tcphdr *th = tcp_hdr(skb);
 	struct eth_port *priv = MV_ETH_PRIV(dev);
-	MV_U16 *mh = NULL;
-	int i;
 
 	STAT_DBG(priv->stats.tx_tso++);
-/*
-	printk(KERN_ERR "mv_eth_tx_tso_%d ENTER: skb=%p, total_len=%d\n", priv->stats.tx_tso, skb, skb->len);
-*/
+
 	if (mv_eth_tso_validate(skb, dev))
 		return 0;
 
-	/* Calculate expected number of TX descriptors */
-	totalDescNum = skb_shinfo(skb)->gso_segs * 2 + skb_shinfo(skb)->nr_frags;
-
-	/* check if there is enough descriptors */
-	if ((!mv_eth_phys_desc_num_check(txq_ctrl, totalDescNum)) ||
-		(!mv_eth_aggr_desc_num_check(aggr_txq_ctrl, totalDescNum))) {
-
-		printk(KERN_ERR "%s: no TX descriptors - txq_count=%d, len=%d, nr_frags=%d, gso_segs=%d\n",
-					__func__, txq_ctrl->txq_count, skb->len, skb_shinfo(skb)->nr_frags,
-					skb_shinfo(skb)->gso_segs);
-
-		STAT_ERR(txq_ctrl->stats.txq_err++);
-		return 0;
-	}
+	txq_cpu_ptr = &txq_ctrl->txq_cpu[smp_processor_id()];
 
 	total_len = skb->len;
 	hdr_len = (skb_transport_offset(skb) + tcp_hdrlen(skb));
@@ -2178,12 +2160,15 @@ int mv_eth_tx_tso(struct sk_buff *skb, struct net_device *dev, struct mv_eth_tx_
 	frag_ptr = skb->data;
 
 	if (frag_size < hdr_len) {
-		printk(KERN_ERR "***** ERROR: frag_size=%d, hdr_len=%d\n", frag_size, hdr_len);
+		pr_err("***** ERROR: frag_size=%d, hdr_len=%d\n", frag_size, hdr_len);
 		return 0;
 	}
 
+	/* Skip header - we'll add header in another buffer (from extra pool) */
 	frag_size -= hdr_len;
 	frag_ptr += hdr_len;
+
+	/* A special case where the first skb's frag contains only the packet's header */
 	if (frag_size == 0) {
 		skb_frag_ptr = &skb_shinfo(skb)->frags[frag];
 
@@ -2192,58 +2177,94 @@ int mv_eth_tx_tso(struct sk_buff *skb, struct net_device *dev, struct mv_eth_tx_
 		frag_ptr = page_address(skb_frag_ptr->page.p) + skb_frag_ptr->page_offset;
 		frag++;
 	}
-	totalDescNum = 0;
+	total_desc_num = 0;
+	ptxq = MV_PPV2_TXQ_PHYS(priv->port, tx_spec->txp, tx_spec->txq);
 
+	/* Each iteration - create new TCP segment */
 	while (total_len > 0) {
 		data_left = MV_MIN(skb_shinfo(skb)->gso_size, total_len);
 
+		/* Calculate maximum number of descriptors needed for the current TCP segment			  *
+		 * At worst case we'll transmit all remaining frags including skb->data (one descriptor per frag) *
+		 * We also need one descriptor for packet header						  */
+		seg_desc_num = skb_shinfo(skb)->nr_frags - frag + 2;
+
+		if (!mv_eth_aggr_desc_num_check(aggr_txq_ctrl, seg_desc_num)) {
+			STAT_DBG(priv->stats.tx_tso_no_resource++);
+			return 0;
+		}
+
+		if (!mv_eth_phys_desc_num_check(txq_cpu_ptr, seg_desc_num)) {
+			/* Try TX done and check resource again */
+#ifndef CONFIG_MV_ETH_TXDONE_ISR
+#ifdef CONFIG_MV_ETH_STAT_DIST
+			u32 tx_done = mv_eth_txq_done(priv, txq_ctrl);
+
+			if (tx_done < priv->dist_stats.tx_done_dist_size)
+				priv->dist_stats.tx_done_dist[tx_done]++;
+#else
+			mv_eth_txq_done(priv, txq_ctrl);
+#endif /* CONFIG_MV_ETH_STAT_DIST */
+			if (!mv_eth_phys_desc_num_check(txq_cpu_ptr, seg_desc_num)) {
+				STAT_DBG(priv->stats.tx_tso_no_resource++);
+				return 0;
+			}
+#else
+			STAT_DBG(priv->stats.tx_tso_no_resource++);
+			return 0;
+#endif /* CONFIG_MV_ETH_TXDONE_ISR */
+		}
+
+		seg_desc_num = 0;
+
 		tx_desc = mvPp2AggrTxqNextDescGet(aggr_txq_ctrl->q);
-		if (tx_desc == NULL)
-			goto outNoTxDesc;
-		tx_desc->physTxq = MV_PPV2_TXQ_PHYS(priv->port, tx_spec->txp, tx_spec->txq);
+		tx_desc->physTxq = ptxq;
 
-		totalDescNum++;
+		seg_desc_num++;
+		total_desc_num++;
 		total_len -= data_left;
 
 		aggr_txq_ctrl->txq_count++;
-		txq_ctrl->txq_count++;
+		txq_cpu_ptr->txq_count++;
 
 		if (tx_spec->flags & MV_ETH_TX_F_MH)
 			mh = &tx_spec->tx_mh;
 
 		/* prepare packet headers: MAC + IP + TCP */
-		size = mv_eth_tso_build_hdr_desc(tx_desc, priv, skb, txq_ctrl, mh,
+		size = mv_eth_tso_build_hdr_desc(tx_desc, priv, skb, txq_cpu_ptr, mh,
 					hdr_len, data_left, tcp_seq, ip_id, total_len);
-		if (size == 0)
-			goto outNoTxDesc;
+		if (size == 0) {
+			aggr_txq_ctrl->txq_count--;
+			txq_cpu_ptr->txq_count--;
+			mv_eth_shadow_dec_put(txq_cpu_ptr);
+			mvPp2AggrTxqPrevDescGet(aggr_txq_ctrl->q);
 
-		totalBytes += size;
-/*
-		printk(KERN_ERR "Header desc: tx_desc=%p, skb=%p, hdr_len=%d, data_left=%d\n",
-						tx_desc, skb, hdr_len, data_left);
-*/
+			STAT_DBG(priv->stats.tx_tso_no_resource++);
+			return 0;
+		}
+		total_bytes += size;
+
+		/* Update packet's IP ID */
 		ip_id++;
 
 		while (data_left > 0) {
 			tx_desc = mvPp2AggrTxqNextDescGet(aggr_txq_ctrl->q);
-			if (tx_desc == NULL)
-				goto outNoTxDesc;
-			tx_desc->physTxq = MV_PPV2_TXQ_PHYS(priv->port, tx_spec->txp, tx_spec->txq);
+			tx_desc->physTxq = ptxq;
 
-			totalDescNum++;
+			seg_desc_num++;
+			total_desc_num++;
 			aggr_txq_ctrl->txq_count++;
-			txq_ctrl->txq_count++;
+			txq_cpu_ptr->txq_count++;
 
-			size = mv_eth_tso_build_data_desc(tx_desc, skb, txq_ctrl,
+			size = mv_eth_tso_build_data_desc(tx_desc, skb, txq_cpu_ptr,
 							  frag_ptr, frag_size, data_left, total_len);
-			totalBytes += size;
-/*
-			printk(KERN_ERR "Data desc: tx_desc=%p, skb=%p, size=%d, frag_size=%d, data_left=%d\n",
-							tx_desc, skb, size, frag_size, data_left);
- */
+			total_bytes += size;
 			data_left -= size;
+
+			/* Update TCP sequence number */
 			tcp_seq += size;
 
+			/* Update frag size, and offset */
 			frag_size -= size;
 			frag_ptr += size;
 
@@ -2255,29 +2276,18 @@ int mv_eth_tx_tso(struct sk_buff *skb, struct net_device *dev, struct mv_eth_tx_
 				frag_ptr = page_address(skb_frag_ptr->page.p) + skb_frag_ptr->page_offset;
 				frag++;
 			}
-		}		/* of while data_left > 0 */
-	}			/* of while (total_len > 0) */
-
-	STAT_DBG(priv->stats.tx_tso_bytes += totalBytes);
-	STAT_DBG(txq_ctrl->stats.txq_tx += totalDescNum);
+		}
 
-	wmb();
-	mvPp2AggrTxqPendDescAdd(totalDescNum);
+		/* TCP segment is ready - transmit it */
+		wmb();
+		mvPp2AggrTxqPendDescAdd(seg_desc_num);
 
-	return totalDescNum;
+		STAT_DBG(txq_ctrl->stats.txq_tx += seg_desc_num);
+	}
 
-outNoTxDesc:
-	/* No enough TX descriptors for the whole skb - rollback */
-	printk(KERN_ERR "%s: No TX descriptors - rollback %d, txq_count=%d, nr_frags=%d, skb=%p, len=%d, gso_segs=%d\n",
-			__func__, totalDescNum, txq_ctrl->txq_count, skb_shinfo(skb)->nr_frags,
-			skb, skb->len, skb_shinfo(skb)->gso_segs);
+	STAT_DBG(priv->stats.tx_tso_bytes += total_bytes);
 
-	for (i = 0; i < totalDescNum; i++) {
-		txq_ctrl->txq_count--;
-		mv_eth_shadow_dec_put(txq_ctrl);
-		mvPp2AggrTxqPrevDescGet(aggr_txq_ctrl->q);
-	}
-	return 0;
+	return total_desc_num;
 }
 #endif /* CONFIG_MV_ETH_TSO */
 
@@ -4982,6 +4992,7 @@ void mv_eth_port_stats_print(unsigned int port)
 
 		printk(KERN_ERR "tx_tso....................%10u\n", stat->tx_tso);
 		printk(KERN_ERR "tx_tso_bytes .............%10u\n", stat->tx_tso_bytes);
+		printk(KERN_ERR "tx_tso_no_resource........%10u\n", stat->tx_tso_no_resource);
 
 		printk(KERN_ERR "rx_netif..................%10u\n", stat->rx_netif);
 		printk(KERN_ERR "rx_drop_sw................%10u\n", stat->rx_drop_sw);
diff --git a/arch/arm/plat-armada/mv_drivers_lsp/mv_pp2/net_dev/mv_netdev.h b/arch/arm/plat-armada/mv_drivers_lsp/mv_pp2/net_dev/mv_netdev.h
index e9c929a..73739c4 100644
--- a/arch/arm/plat-armada/mv_drivers_lsp/mv_pp2/net_dev/mv_netdev.h
+++ b/arch/arm/plat-armada/mv_drivers_lsp/mv_pp2/net_dev/mv_netdev.h
@@ -201,6 +201,7 @@ struct port_stats {
 	u32 tx_skb_free;
 	u32 tx_sg;
 	u32 tx_tso;
+	u32 tx_tso_no_resource;
 	u32 tx_tso_bytes;
 	u32 ext_stack_put;
 	u32 ext_stack_get;
-- 
1.7.5.4

