From e974ed43d0f544c7a148d338c52cb7c0db389ba9 Mon Sep 17 00:00:00 2001
From: Yoni Farhadia <yonif@marvell.com>
Date: Sun, 29 Sep 2013 14:19:23 +0200
Subject: [PATCH 0996/1825] neta: merge with KW2 and netmap changes

https://github.com/MISL-EBU-System-SW/misl-windriver.git linux-3.4.69-14t2-read
commit 137b5518e9dcc0cc5802942450b11d1189f44361

	- Fix HWF BM usage
	- Add TXQ clean support
	- Fix Kconfig defaults according to SoC
	- En/Disable HWF in link up/down events
	- Ignore PON link events when port is down
	- Add volatile for BM address for BM get/put

Change-Id: Ic7ecda8b002ed593d28817f58daf27f6aa3bb0d1
Signed-off-by: Yoni Farhadia <yonif@marvell.com>
Signed-off-by: Zhong Hongbo <hongbo.zhong@windriver.com>
---
 .../arm/plat-armada/mv_drivers_lsp/mv_neta/Kconfig |   22 +-
 .../mv_drivers_lsp/mv_neta/hwf/hwf_bm.c            |    2 +
 .../mv_drivers_lsp/mv_neta/net_dev/mv_eth_sysfs.c  |    5 +
 .../mv_drivers_lsp/mv_neta/net_dev/mv_netdev.c     |  334 ++++++++++++++------
 arch/arm/plat-armada/mv_hal/neta/bm/mvBm.h         |    7 +-
 arch/arm/plat-armada/mv_hal/neta/gbe/mvHwf.c       |   13 +
 arch/arm/plat-armada/mv_hal/neta/gbe/mvNeta.c      |    2 -
 arch/arm/plat-armada/mv_hal/neta/gbe/mvNeta.h      |   25 ++
 arch/arm/plat-armada/mv_hal/neta/gbe/mvNetaDebug.c |    5 +-
 arch/arm/plat-armada/mv_hal/neta/gbe/mvNetaRegs.h  |    4 +
 10 files changed, 314 insertions(+), 105 deletions(-)

diff --git a/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/Kconfig b/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/Kconfig
index 21ef6c6..edbb853 100644
--- a/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/Kconfig
+++ b/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/Kconfig
@@ -8,9 +8,10 @@ config MV_ETH_PORTS_NUM
 menu "BM configuration"
 
 config MV_ETH_BM
-	depends on MV_ETH_NETA
+	depends on MV_ETH_NETA && (ARCH_FEROCEON_KW2 || ARCH_ARMADA_XP)
 	bool "Buffer Management support (BM)"
-	default y
+	default y if ARCH_FEROCEON_KW2
+	default n if ARCH_ARMADA_XP
         ---help---
 
 config MV_ETH_BM_CPU
@@ -47,7 +48,8 @@ config MV_ETH_BM_2_PKT_SIZE
 config MV_ETH_BM_3_PKT_SIZE
 	depends on MV_ETH_BM
 	int "Packet size [bytes] can use buffers from pool #3"
-	default 256
+	default 256 if (MV_ETH_PORTS_NUM != 4)
+	default 0
 	---help---
         0 - means that packet size for the pool will be defined accordingly
         with MTU of the port that use this pool.
@@ -72,7 +74,8 @@ config  MV_ETH_BM_PORT_0_SHORT_POOL
         int "Short BM pool for GbE #0"
         depends on MV_ETH_BM_PORT_0
         range 0 3
-        default 3
+        default 3 if (MV_ETH_PORTS_NUM != 4)
+	default 0
 	---help---
 	BM pool to be used for GbE #0 port to process short packets
 
@@ -112,7 +115,8 @@ config  MV_ETH_BM_PORT_1_SHORT_POOL
         int "Short BM pool for GbE #1"
         depends on MV_ETH_BM_PORT_1
         range 0 3
-        default 3
+        default 3 if (MV_ETH_PORTS_NUM != 4)
+	default 1
         ---help---
         BM pool to be used for GbE #1 port to process short packets.
 
@@ -152,7 +156,8 @@ config  MV_ETH_BM_PORT_2_SHORT_POOL
         int "Short BM pool for GbE #2"
         depends on MV_ETH_BM_PORT_2
         range 0 3
-        default 3
+        default 3 if (MV_ETH_PORTS_NUM != 4)
+        default 2
         ---help---
 	BM pool to be used for GbE #2 port to process short packets.
 
@@ -221,7 +226,7 @@ config MV_ETH_LEGACY_PARSER
 	---help---
 
 menuconfig MV_ETH_PNC
-        depends on MV_ETH_NETA
+        depends on MV_ETH_NETA && (ARCH_FEROCEON_KW2 || ARCH_ARMADA_XP)
         bool "PnC support"
 	default y
         ---help---
@@ -281,7 +286,6 @@ config MV_ETH_PNC_L3_FLOW_LINES
         ---help---
         Number of PNC L3 flows entries
 
-
 config MV_ETH_PNC_WOL
 	depends on MV_ETH_PNC
 	bool "Use PNC for Wake On LAN support"
@@ -308,7 +312,7 @@ config MV_ETH_HWF_TXQ_DROP_RND
         ---help---
 
 config MV_ETH_PMT
-        depends on MV_ETH_NETA
+        depends on (MV_ETH_NETA && ARCH_FEROCEON_KW2)
         bool "Packet Modification Table (PMT)"
         default n
         ---help---
diff --git a/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/hwf/hwf_bm.c b/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/hwf/hwf_bm.c
index 6397337..818b0c6 100644
--- a/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/hwf/hwf_bm.c
+++ b/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/hwf/hwf_bm.c
@@ -127,6 +127,7 @@ MV_STATUS mv_eth_hwf_bm_create(int port, int mtuPktSize)
 		pBmPool->pool = long_pool;
 		pBmPool->capacity = MV_BM_POOL_CAP_MAX;
 		pBmPool->bufSize = long_buf_size;
+		mvNetaBmPoolBufSizeSet(port, long_pool, long_buf_size);
 	} else {
 		/* Share pool with other port - check buffer size */
 		if (long_buf_size > pBmPool->bufSize) {
@@ -154,6 +155,7 @@ MV_STATUS mv_eth_hwf_bm_create(int port, int mtuPktSize)
 			pBmPool->pool = short_pool;
 			pBmPool->capacity = MV_BM_POOL_CAP_MAX;
 			pBmPool->bufSize = short_buf_size;
+			mvNetaBmPoolBufSizeSet(port, short_pool, short_buf_size);
 		} else {
 			/* Share pool with other port - check buffer size */
 			if (short_buf_size > pBmPool->bufSize) {
diff --git a/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/mv_eth_sysfs.c b/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/mv_eth_sysfs.c
index 0e1860d..778992c 100644
--- a/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/mv_eth_sysfs.c
+++ b/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/mv_eth_sysfs.c
@@ -85,6 +85,7 @@ static ssize_t mv_eth_help(char *buf)
 	off += sprintf(buf+off, "echo p rxq t       > rxq_type      - set RXQ for different packet types. t=0-bpdu, 1-arp, 2-tcp, 3-udp\n");
 	off += sprintf(buf+off, "echo p             > rx_reset      - reset RX part of the port <p>\n");
 	off += sprintf(buf+off, "echo p txp         > txp_reset     - reset TX part of the port <p/txp>\n");
+	off += sprintf(buf+off, "echo p txp txq     > txq_clean     - clean TXQ <p/txp/txq> - free descriptors and buffers\n");
 	off += sprintf(buf+off, "echo p txq cpu tos > txq_tos       - set <txq> for outgoing IP packets with <tos> handeled by <cpu>\n");
 	off += sprintf(buf+off, "echo p txp txq cpu > txq_def       - set default <txp/txq> for packets sent to port <p> by <cpu>\n");
 	off += sprintf(buf+off, "echo p txp {0|1}   > ejp           - enable/disable EJP mode for <port/txp>\n");
@@ -462,6 +463,8 @@ static ssize_t mv_eth_4_store(struct device *dev,
 		mvNetaTxqShow(p, txp, txq, v);
 	} else if (!strcmp(name, "txq_regs")) {
 		mvNetaTxqRegs(p, txp, txq);
+	} else if (!strcmp(name, "txq_clean")) {
+		err = mv_eth_txq_clean(p, txp, txq);
 	} else if (!strcmp(name, "txq_shared")) {
 		err = mv_eth_shared_set(p, txp, txq, v);
 	} else {
@@ -488,6 +491,7 @@ static DEVICE_ATTR(txq_def,     S_IWUSR, mv_eth_show, mv_eth_4_store);
 static DEVICE_ATTR(txq_wrr,     S_IWUSR, mv_eth_show, mv_eth_4_store);
 static DEVICE_ATTR(txq_rate,    S_IWUSR, mv_eth_show, mv_eth_4_store);
 static DEVICE_ATTR(txq_burst,   S_IWUSR, mv_eth_show, mv_eth_4_store);
+static DEVICE_ATTR(txq_clean,   S_IWUSR, mv_eth_show, mv_eth_4_store);
 static DEVICE_ATTR(txp_rate,    S_IWUSR, mv_eth_show, mv_eth_3_store);
 static DEVICE_ATTR(txp_burst,   S_IWUSR, mv_eth_show, mv_eth_3_store);
 static DEVICE_ATTR(txp_reset,   S_IWUSR, mv_eth_show, mv_eth_3_store);
@@ -547,6 +551,7 @@ static struct attribute *mv_eth_attrs[] = {
 	&dev_attr_txq_wrr.attr,
 	&dev_attr_txq_rate.attr,
 	&dev_attr_txq_burst.attr,
+	&dev_attr_txq_clean.attr,
 	&dev_attr_txp_rate.attr,
 	&dev_attr_txp_burst.attr,
 	&dev_attr_txp_reset.attr,
diff --git a/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/mv_netdev.c b/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/mv_netdev.c
index 4799f36..9b764f9 100644
--- a/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/mv_netdev.c
+++ b/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/mv_netdev.c
@@ -374,7 +374,7 @@ static int mv_eth_port_config_parse(struct eth_port *pp)
 	printk(KERN_ERR "\n");
 	if (pp == NULL) {
 		printk(KERN_ERR "  o mv_eth_port_config_parse: got NULL pp\n");
-		return MV_ERROR;
+		return -1;
 	}
 
 	switch (pp->port) {
@@ -392,20 +392,20 @@ static int mv_eth_port_config_parse(struct eth_port *pp)
 		break;
 	default:
 		printk(KERN_ERR "  o mv_eth_port_config_parse: got unknown port %d\n", pp->port);
-		return MV_ERROR;
+		return -1;
 	}
 
 	if (str != NULL) {
 		if ((!strcmp(str, "disconnected")) || (!strcmp(str, "Disconnected"))) {
 			printk(KERN_ERR "  o Port %d is disconnected from Linux netdevice\n", pp->port);
 			clear_bit(MV_ETH_F_CONNECT_LINUX_BIT, &(pp->flags));
-			return MV_OK;
+			return 0;
 		}
 	}
 
 	printk(KERN_ERR "  o Port %d is connected to Linux netdevice\n", pp->port);
 	set_bit(MV_ETH_F_CONNECT_LINUX_BIT, &(pp->flags));
-	return MV_OK;
+	return 0;
 }
 
 #ifdef ETH_SKB_DEBUG
@@ -497,7 +497,7 @@ static inline int mv_eth_skb_mh_add(struct sk_buff *skb, u16 mh)
 	skb->data -= MV_ETH_MH_SIZE;
 	*((u16 *) skb->data) = mh;
 
-	return MV_OK;
+	return 0;
 }
 
 void mv_eth_ctrl_txdone(int num)
@@ -526,7 +526,7 @@ int mv_eth_ctrl_flag(int port, u32 flag, u32 val)
 	if (flag == MV_ETH_F_MH)
 		mvNetaMhSet(pp->port, val ? MV_NETA_MH : MV_NETA_MH_NONE);
 
-	return MV_OK;
+	return 0;
 }
 
 int mv_eth_ctrl_port_buf_num_set(int port, int long_num, int short_num)
@@ -562,7 +562,7 @@ int mv_eth_ctrl_port_buf_num_set(int port, int long_num, int short_num)
 	pp->pool_short_num = short_num;
 #endif /* CONFIG_MV_ETH_BM_CPU */
 
-	return MV_OK;
+	return 0;
 }
 
 #ifdef CONFIG_MV_ETH_BM
@@ -619,7 +619,7 @@ int mv_eth_ctrl_pool_size_set(int pool, int pkt_size)
 	else
 		mvBmPoolBufSizeSet(pool, RX_BUF_SIZE(pkt_size));
 
-	return MV_OK;
+	return 0;
 }
 #endif /* CONFIG_MV_ETH_BM */
 
@@ -649,7 +649,7 @@ int mv_eth_ctrl_set_poll_rx_weight(int port, u32 weight)
 			cpuCtrl->napi->weight = pp->weight;
 	}
 
-	return MV_OK;
+	return 0;
 }
 
 int mv_eth_ctrl_rxq_size_set(int port, int rxq, int value)
@@ -677,7 +677,7 @@ int mv_eth_ctrl_rxq_size_set(int port, int rxq, int value)
 	pp->rxq_ctrl[rxq].rxq_size = value;
 
 	/* New RXQ will be created during mv_eth_start_internals */
-	return MV_OK;
+	return 0;
 }
 
 int mv_eth_ctrl_txq_size_set(int port, int txp, int txq, int value)
@@ -696,14 +696,20 @@ int mv_eth_ctrl_txq_size_set(int port, int txp, int txq, int value)
 	}
 	txq_ctrl = &pp->txq_ctrl[txp * CONFIG_MV_ETH_TXQ + txq];
 	if ((txq_ctrl->q) && (txq_ctrl->txq_size != value)) {
+		/* Reset of port/txp is required to change TXQ ring size */
+		if ((mvNetaTxqNextIndexGet(pp->port, txq_ctrl->txp, txq_ctrl->txq) != 0) ||
+			(mvNetaTxqPendDescNumGet(pp->port, txq_ctrl->txp, txq_ctrl->txq) != 0) ||
+			(mvNetaTxqSentDescNumGet(pp->port, txq_ctrl->txp, txq_ctrl->txq) != 0)) {
+			printk(KERN_ERR "%s: port=%d, txp=%d, txq=%d must be in its initial state\n",
+				__func__, port, txq_ctrl->txp, txq_ctrl->txq);
+			return -EINVAL;
+		}
 		mv_eth_txq_delete(pp, txq_ctrl);
-		/* Reset of port/txp is required when TXQ ring size is changed */
-		/* Reset done before as part of stop_internals function */
 	}
 	txq_ctrl->txq_size = value;
 
 	/* New TXQ will be created during mv_eth_start_internals */
-	return MV_OK;
+	return 0;
 }
 
 int mv_eth_ctrl_txq_mode_get(int port, int txp, int txq, int *value)
@@ -712,6 +718,9 @@ int mv_eth_ctrl_txq_mode_get(int port, int txp, int txq, int *value)
 	struct tx_queue *txq_ctrl;
 	struct eth_port *pp = mv_eth_port_by_id(port);
 
+	if (pp == NULL)
+		return -ENODEV;
+
 	txq_ctrl = &pp->txq_ctrl[txp * CONFIG_MV_ETH_TXQ + txq];
 	for_each_possible_cpu(cpu)
 		if (txq_ctrl->cpu_owner[cpu]) {
@@ -760,7 +769,7 @@ int mv_eth_ctrl_txq_cpu_own(int port, int txp, int txq, int add, int cpu)
 
 	mv_eth_txq_update_shared(txq_ctrl, pp);
 
-	return MV_OK;
+	return 0;
 }
 
 /* Set TXQ ownership to HWF from the RX port.  rxp=-1 - free TXQ ownership */
@@ -788,7 +797,7 @@ int mv_eth_ctrl_txq_hwf_own(int port, int txp, int txq, int rxp)
 
 	txq_ctrl->hwf_rxp = (MV_U8) rxp;
 
-	return MV_OK;
+	return 0;
 }
 
 /* set or clear shared bit for this txq, txp=1 for pon , 0 for gbe */
@@ -863,7 +872,7 @@ int mv_eth_ctrl_txq_cpu_def(int port, int txp, int txq, int cpu)
 	pp->txp = txp;
 	cpuCtrl->txq = txq;
 
-	return MV_OK;
+	return 0;
 }
 
 
@@ -927,7 +936,7 @@ int	mv_eth_cpu_txq_mask_set(int port, int cpu, int txqMask)
 	mvNetaTxqCpuMaskSet(port, txqMask, cpu);
 	cpuCtrl->cpuTxqMask = txqMask;
 
-	return MV_OK;
+	return 0;
 }
 
 
@@ -940,7 +949,7 @@ int mv_eth_ctrl_tx_cmd(int port, u32 tx_cmd)
 
 	pp->hw_cmd = tx_cmd;
 
-	return MV_OK;
+	return 0;
 }
 
 int mv_eth_ctrl_tx_mh(int port, u16 mh)
@@ -952,7 +961,7 @@ int mv_eth_ctrl_tx_mh(int port, u16 mh)
 
 	pp->tx_mh = mh;
 
-	return MV_OK;
+	return 0;
 }
 
 #ifdef CONFIG_MV_ETH_TX_SPECIAL
@@ -963,7 +972,8 @@ void mv_eth_tx_special_check_func(int port,
 {
 	struct eth_port *pp = mv_eth_port_by_id(port);
 
-	pp->tx_special_check = func;
+	if (pp)
+		pp->tx_special_check = func;
 }
 #endif /* CONFIG_MV_ETH_TX_SPECIAL */
 
@@ -1469,6 +1479,125 @@ static struct sk_buff *mv_eth_skb_alloc(struct bm_pool *pool, struct eth_pbuf *p
 	return skb;
 }
 
+static inline void mv_eth_txq_buf_free(struct eth_port *pp, u32 shadow)
+{
+	if (!shadow)
+		return;
+
+	if (shadow & MV_ETH_SHADOW_SKB) {
+		shadow &= ~MV_ETH_SHADOW_SKB;
+		dev_kfree_skb_any((struct sk_buff *)shadow);
+		STAT_DBG(pp->stats.tx_skb_free++);
+	} else {
+		if (shadow & MV_ETH_SHADOW_EXT) {
+			shadow &= ~MV_ETH_SHADOW_EXT;
+			mv_eth_extra_pool_put(pp, (void *)shadow);
+		} else {
+			/* packet from NFP without BM */
+			struct eth_pbuf *pkt = (struct eth_pbuf *)shadow;
+			struct bm_pool *pool = &mv_eth_pool[pkt->pool];
+
+			if (mv_eth_pool_bm(pool)) {
+				/* Refill BM pool */
+				STAT_DBG(pool->stats.bm_put++);
+				mvBmPoolPut(pkt->pool, (MV_ULONG) pkt->physAddr);
+			} else {
+				mv_eth_pool_put(pool, pkt);
+			}
+		}
+	}
+}
+
+static inline void mv_eth_txq_cpu_clean(struct eth_port *pp, struct tx_queue *txq_ctrl)
+{
+	int hw_txq_i, last_txq_i, i, count;
+	u32 shadow;
+
+	hw_txq_i = mvNetaTxqNextIndexGet(pp->port, txq_ctrl->txp, txq_ctrl->txq);
+	last_txq_i = txq_ctrl->shadow_txq_put_i;
+
+	i = hw_txq_i;
+	count = 0;
+	while (i != last_txq_i) {
+		shadow = txq_ctrl->shadow_txq[i];
+		mv_eth_txq_buf_free(pp, shadow);
+		txq_ctrl->shadow_txq[i] = (u32)NULL;
+
+		i = MV_NETA_QUEUE_NEXT_DESC(&txq_ctrl->q->queueCtrl, i);
+		count++;
+	}
+	printk(KERN_INFO "\n%s: port=%d, txp=%d, txq=%d, mode=CPU\n",
+			__func__, pp->port, txq_ctrl->txp, txq_ctrl->txq);
+	printk(KERN_INFO "Free %d buffers: from desc=%d to desc=%d, tx_count=%d\n",
+			count, hw_txq_i, last_txq_i, txq_ctrl->txq_count);
+}
+
+#ifdef CONFIG_MV_ETH_HWF
+static inline void mv_eth_txq_hwf_clean(struct eth_port *pp, struct tx_queue *txq_ctrl, int rx_port)
+{
+	int pool, hw_txq_i, last_txq_i, i, count;
+	struct neta_tx_desc *tx_desc;
+
+	hw_txq_i = mvNetaTxqNextIndexGet(pp->port, txq_ctrl->txp, txq_ctrl->txq);
+
+	if (mvNetaHwfTxqNextIndexGet(rx_port, pp->port, txq_ctrl->txp, txq_ctrl->txq, &last_txq_i) != MV_OK) {
+		printk(KERN_ERR "%s: mvNetaHwfTxqNextIndexGet failed\n", __func__);
+		return;
+	}
+
+	i = hw_txq_i;
+	count = 0;
+	while (i != last_txq_i) {
+		tx_desc = (struct neta_tx_desc *)MV_NETA_QUEUE_DESC_PTR(&txq_ctrl->q->queueCtrl, i);
+		if (mvNetaTxqDescIsValid(tx_desc)) {
+			mvNetaTxqDescInv(tx_desc);
+			mv_eth_tx_desc_flush(tx_desc);
+
+			pool = (tx_desc->command & NETA_TX_BM_POOL_ID_ALL_MASK) >> NETA_TX_BM_POOL_ID_OFFS;
+			mvBmPoolPut(pool, (MV_ULONG)tx_desc->bufPhysAddr);
+			count++;
+		}
+		i = MV_NETA_QUEUE_NEXT_DESC(&txq_ctrl->q->queueCtrl, i);
+	}
+	printk(KERN_INFO "\n%s: port=%d, txp=%d, txq=%d, mode=HWF-%d\n",
+			__func__, pp->port, txq_ctrl->txp, txq_ctrl->txq, rx_port);
+	printk(KERN_INFO "Free %d buffers to BM: from desc=%d to desc=%d\n",
+			count, hw_txq_i, last_txq_i);
+}
+#endif /* CONFIG_MV_ETH_HWF */
+
+int mv_eth_txq_clean(int port, int txp, int txq)
+{
+	int mode, rx_port;
+	struct eth_port *pp;
+	struct tx_queue *txq_ctrl;
+
+	if (mvNetaTxpCheck(port, txp))
+		return -EINVAL;
+
+	if (mvNetaMaxCheck(txq, CONFIG_MV_ETH_TXQ, "txq"))
+		return -EINVAL;
+
+	pp = mv_eth_port_by_id(port);
+	if ((pp == NULL) || (pp->txq_ctrl == NULL))
+		return -ENODEV;
+
+	txq_ctrl = &pp->txq_ctrl[txp * CONFIG_MV_ETH_TXQ + txq];
+
+	mode = mv_eth_ctrl_txq_mode_get(pp->port, txq_ctrl->txp, txq_ctrl->txq, &rx_port);
+	if (mode == MV_ETH_TXQ_CPU)
+		mv_eth_txq_cpu_clean(pp, txq_ctrl);
+#ifdef CONFIG_MV_ETH_HWF
+	else if (mode == MV_ETH_TXQ_HWF)
+		mv_eth_txq_hwf_clean(pp, txq_ctrl, rx_port);
+#endif /* CONFIG_MV_ETH_HWF */
+	else
+		printk(KERN_ERR "%s: port=%d, txp=%d, txq=%d is not in use\n",
+			__func__, pp->port, txp, txq);
+
+	return 0;
+}
+
 static inline void mv_eth_txq_bufs_free(struct eth_port *pp, struct tx_queue *txq_ctrl, int num)
 {
 	u32 shadow;
@@ -1478,32 +1607,7 @@ static inline void mv_eth_txq_bufs_free(struct eth_port *pp, struct tx_queue *tx
 	for (i = 0; i < num; i++) {
 		shadow = txq_ctrl->shadow_txq[txq_ctrl->shadow_txq_get_i];
 		mv_eth_shadow_inc_get(txq_ctrl);
-
-		if (!shadow)
-			continue;
-
-		if (shadow & MV_ETH_SHADOW_SKB) {
-			shadow &= ~MV_ETH_SHADOW_SKB;
-			dev_kfree_skb_any((struct sk_buff *)shadow);
-			STAT_DBG(pp->stats.tx_skb_free++);
-		} else {
-			if (shadow & MV_ETH_SHADOW_EXT) {
-				shadow &= ~MV_ETH_SHADOW_EXT;
-				mv_eth_extra_pool_put(pp, (void *)shadow);
-			} else {
-				/* packet from NFP without BM */
-				struct eth_pbuf *pkt = (struct eth_pbuf *)shadow;
-				struct bm_pool *pool = &mv_eth_pool[pkt->pool];
-
-				if (mv_eth_pool_bm(pool)) {
-					/* Refill BM pool */
-					STAT_DBG(pool->stats.bm_put++);
-					mvBmPoolPut(pkt->pool, (MV_ULONG) pkt->physAddr);
-				} else {
-					mv_eth_pool_put(pool, pkt);
-				}
-			}
-		}
+		mv_eth_txq_buf_free(pp, shadow);
 	}
 }
 
@@ -1580,7 +1684,7 @@ inline int mv_eth_refill(struct eth_port *pp, int rxq,
 	}
 	mv_eth_rxq_refill(pp, rxq, pkt, pool, rx_desc);
 
-	return MV_OK;
+	return 0;
 }
 EXPORT_SYMBOL(mv_eth_refill);
 
@@ -1717,6 +1821,7 @@ static inline int mv_eth_rx(struct eth_port *pp, int rx_todo, int rxq)
 #else
 		dev = pp->dev;
 #endif /* CONFIG_MV_ETH_SWITCH */
+
 		STAT_DBG(pp->stats.rxq[rxq]++);
 		dev->stats.rx_packets++;
 
@@ -2057,7 +2162,7 @@ static inline int mv_eth_tso_validate(struct sk_buff *skb, struct net_device *de
 		printk(KERN_ERR "***** ERROR: Protocol is not TCP over IP\n");
 		return 1;
 	}
-	return MV_OK;
+	return 0;
 }
 
 static inline int mv_eth_tso_build_hdr_desc(struct neta_tx_desc *tx_desc, struct eth_port *priv, struct sk_buff *skb,
@@ -2071,7 +2176,7 @@ static inline int mv_eth_tso_build_hdr_desc(struct neta_tx_desc *tx_desc, struct
 
 	data = mv_eth_extra_pool_get(priv);
 	if (!data)
-		return MV_OK;
+		return 0;
 
 	txq_ctrl->shadow_txq[txq_ctrl->shadow_txq_put_i] = ((MV_ULONG)data | MV_ETH_SHADOW_EXT);
 
@@ -2173,7 +2278,7 @@ int mv_eth_tx_tso(struct sk_buff *skb, struct net_device *dev,
 	printk(KERN_ERR "mv_eth_tx_tso_%d ENTER: skb=%p, total_len=%d\n", priv->stats.tx_tso, skb, skb->len);
 */
 	if (mv_eth_tso_validate(skb, dev))
-		return MV_OK;
+		return 0;
 
 	/* Calculate expected number of TX descriptors */
 	totalDescNum = skb_shinfo(skb)->gso_segs * 2 + skb_shinfo(skb)->nr_frags;
@@ -2185,7 +2290,7 @@ int mv_eth_tx_tso(struct sk_buff *skb, struct net_device *dev,
 					skb_shinfo(skb)->gso_segs);
 */
 		STAT_ERR(txq_ctrl->stats.txq_err++);
-		return MV_OK;
+		return 0;
 	}
 
 	total_len = skb->len;
@@ -2200,7 +2305,7 @@ int mv_eth_tx_tso(struct sk_buff *skb, struct net_device *dev,
 
 	if (frag_size < hdr_len) {
 		printk(KERN_ERR "***** ERROR: frag_size=%d, hdr_len=%d\n", frag_size, hdr_len);
-		return MV_OK;
+		return 0;
 	}
 
 	frag_size -= hdr_len;
@@ -2346,12 +2451,8 @@ static void mv_eth_txq_done_force(struct eth_port *pp, struct tx_queue *txq_ctrl
 
 	mv_eth_txq_bufs_free(pp, txq_ctrl, tx_done);
 
+	txq_ctrl->txq_count -= tx_done;
 	STAT_DBG(txq_ctrl->stats.txq_txdone += tx_done);
-
-	/* reset txq */
-	txq_ctrl->txq_count = 0;
-	txq_ctrl->shadow_txq_put_i = 0;
-	txq_ctrl->shadow_txq_get_i = 0;
 }
 
 inline u32 mv_eth_tx_done_pon(struct eth_port *pp, int *tx_todo)
@@ -2623,7 +2724,7 @@ static int mv_eth_pool_add(int pool, int buf_num)
 
 	if ((pool < 0) || (pool >= MV_ETH_BM_POOLS)) {
 		printk(KERN_ERR "%s: invalid pool number %d\n", __func__, pool);
-		return MV_OK;
+		return 0;
 	}
 
 	bm_pool = &mv_eth_pool[pool];
@@ -2632,7 +2733,7 @@ static int mv_eth_pool_add(int pool, int buf_num)
 	if (bm_pool->pkt_size == 0) {
 		printk(KERN_ERR "%s: invalid pool #%d state: pkt_size=%d, buf_size=%d, buf_num=%d\n",
 		       __func__, pool, bm_pool->pkt_size, RX_BUF_SIZE(bm_pool->pkt_size), bm_pool->buf_num);
-		return MV_OK;
+		return 0;
 	}
 
 	/* Insure buf_num is smaller than capacity */
@@ -2640,7 +2741,7 @@ static int mv_eth_pool_add(int pool, int buf_num)
 
 		printk(KERN_ERR "%s: can't add %d buffers into bm_pool=%d: capacity=%d, buf_num=%d\n",
 		       __func__, buf_num, pool, bm_pool->capacity, bm_pool->buf_num);
-		return MV_OK;
+		return 0;
 	}
 
 	MV_ETH_LOCK(&bm_pool->lock, flags);
@@ -2789,13 +2890,38 @@ irqreturn_t mv_eth_isr(int irq, void *dev_id)
 	} else {
 		STAT_INFO(pp->stats.irq_err[cpu]++);
 #ifdef CONFIG_MV_ETH_DEBUG_CODE
-		pr_debug("%s: IRQ=%d, port=%d, cpu=%d - NAPI already scheduled\n",
+		pr_warning("%s: IRQ=%d, port=%d, cpu=%d - NAPI already scheduled\n",
 			__func__, irq, pp->port, cpu);
 #endif /* CONFIG_MV_ETH_DEBUG_CODE */
 	}
 	return IRQ_HANDLED;
 }
 
+/* Enable / Disable HWF to the TXQs of the [port] that in MV_ETH_TXQ_HWF mode */
+int mv_eth_hwf_ctrl(int port, MV_BOOL enable)
+{
+#ifdef CONFIG_MV_ETH_HWF
+	int txp, txq, rx_port, mode;
+	struct eth_port *pp;
+
+	if (mvNetaPortCheck(port))
+		return -EINVAL;
+
+	pp = mv_eth_port_by_id(port);
+	if (pp == NULL)
+		return -ENODEV;
+
+	for (txp = 0; txp < pp->txp_num; txp++) {
+		for (txq = 0; txq < CONFIG_MV_ETH_TXQ; txq++) {
+			mode = mv_eth_ctrl_txq_mode_get(port, txp, txq, &rx_port);
+			if (mode == MV_ETH_TXQ_HWF)
+				mvNetaHwfTxqEnable(rx_port, port, txp, txq, enable);
+		}
+	}
+#endif /* CONFIG_MV_ETH_HWF */
+	return 0;
+}
+
 void mv_eth_link_event(struct eth_port *pp, int print)
 {
 	struct net_device *dev = pp->dev;
@@ -2821,7 +2947,10 @@ void mv_eth_link_event(struct eth_port *pp, int print)
 				netif_tx_wake_all_queues(dev);
 			}
 		}
+		mv_eth_hwf_ctrl(pp->port, MV_TRUE);
 	} else {
+		mv_eth_hwf_ctrl(pp->port, MV_FALSE);
+
 		if (dev) {
 			netif_carrier_off(dev);
 			netif_tx_stop_all_queues(dev);
@@ -4270,19 +4399,19 @@ int mv_eth_napi_set_cpu_affinity(int port, int group, int affinity)
 	struct eth_port *pp = mv_eth_port_by_id(port);
 	if (pp == NULL) {
 		printk(KERN_ERR "%s: pp == NULL, port=%d\n", __func__, port);
-		return MV_ERROR;
+		return -1;
 	}
 
 	if (group >= CONFIG_MV_ETH_NAPI_GROUPS) {
 		printk(KERN_ERR "%s: group number is higher than %d\n", __func__, CONFIG_MV_ETH_NAPI_GROUPS-1);
-		return MV_ERROR;
+		return -1;
 		}
 	if (pp->flags & MV_ETH_F_STARTED) {
 		printk(KERN_ERR "Port %d must be stopped before\n", port);
 		return -EINVAL;
 	}
 	set_cpu_affinity(pp, affinity, group);
-	return MV_OK;
+	return 0;
 
 }
 void handle_group_affinity(int port)
@@ -4330,7 +4459,7 @@ int	mv_eth_napi_set_rxq_affinity(int port, int group, int rxqAffinity)
 	}
 	if (group >= CONFIG_MV_ETH_NAPI_GROUPS) {
 		printk(KERN_ERR "%s: group number is higher than %d\n", __func__, CONFIG_MV_ETH_NAPI_GROUPS-1);
-		return MV_ERROR;
+		return -1;
 		}
 	if (pp->flags & MV_ETH_F_STARTED) {
 		printk(KERN_ERR "Port %d must be stopped before\n", port);
@@ -4338,7 +4467,7 @@ int	mv_eth_napi_set_rxq_affinity(int port, int group, int rxqAffinity)
 	}
 
 	set_rxq_affinity(pp, rxqAffinity, group);
-	return MV_OK;
+	return 0;
 }
 
 
@@ -4430,7 +4559,7 @@ static int mv_eth_rxq_fill(struct eth_port *pp, int rxq, int num)
 	rx_ctrl = pp->rxq_ctrl[rxq].q;
 	if (!rx_ctrl) {
 		printk(KERN_ERR "%s: rxq %d is not initialized\n", __func__, rxq);
-		return MV_OK;
+		return 0;
 	}
 
 	for (i = 0; i < num; i++) {
@@ -4476,7 +4605,7 @@ static int mv_eth_txq_create(struct eth_port *pp, struct tx_queue *txq_ctrl)
 	mvNetaHwfTxqInit(pp->port, txq_ctrl->txp, txq_ctrl->txq);
 #endif /* CONFIG_MV_ETH_HWF */
 
-	return MV_OK;
+	return 0;
 
 no_mem:
 	mv_eth_txq_delete(pp, txq_ctrl);
@@ -4513,7 +4642,7 @@ static int mv_force_port_link_speed_fc(int port, MV_ETH_PORT_SPEED port_speed, i
 			return -EIO;
 		}
 	}
-	return MV_OK;
+	return 0;
 }
 
 static void mv_eth_txq_delete(struct eth_port *pp, struct tx_queue *txq_ctrl)
@@ -4544,8 +4673,25 @@ int mv_eth_txp_reset(int port, int txp)
 	for (queue = 0; queue < CONFIG_MV_ETH_TXQ; queue++) {
 		struct tx_queue *txq_ctrl = &pp->txq_ctrl[txp * CONFIG_MV_ETH_TXQ + queue];
 
-		if (txq_ctrl->q)
-			mv_eth_txq_done_force(pp, txq_ctrl);
+		if (txq_ctrl->q) {
+			int mode, rx_port;
+
+			mode = mv_eth_ctrl_txq_mode_get(pp->port, txp, queue, &rx_port);
+			if (mode == MV_ETH_TXQ_CPU) {
+				/* Free all buffers in TXQ */
+				mv_eth_txq_done_force(pp, txq_ctrl);
+				/* reset txq */
+				txq_ctrl->shadow_txq_put_i = 0;
+				txq_ctrl->shadow_txq_get_i = 0;
+			}
+#ifdef CONFIG_MV_ETH_HWF
+			else if (mode == MV_ETH_TXQ_HWF)
+				mv_eth_txq_hwf_clean(pp, txq_ctrl, rx_port);
+#endif /* CONFIG_MV_ETH_HWF */
+			else
+				printk(KERN_ERR "%s: port=%d, txp=%d, txq=%d is not in use\n",
+					__func__, pp->port, txp, queue);
+		}
 	}
 	mvNetaTxpReset(port, txp);
 
@@ -4907,7 +5053,7 @@ int mv_eth_suspend_internals(struct eth_port *pp)
 int mv_eth_stop_internals(struct eth_port *pp)
 {
 
-	int queue;
+	int txp, queue;
 	struct cpu_ctrl	*cpuCtrl;
 
 	if (!test_and_clear_bit(MV_ETH_F_STARTED_BIT, &(pp->flags))) {
@@ -4928,14 +5074,11 @@ int mv_eth_stop_internals(struct eth_port *pp)
 
 #ifdef CONFIG_MV_ETH_HWF
 	mvNetaHwfEnable(pp->port, 0);
-#else
-	{
-		int txp;
-		/* Reset TX port here. If HWF is supported reset must be called externally */
-		for (txp = 0; txp < pp->txp_num; txp++)
-			mv_eth_txp_reset(pp->port, txp);
-	}
-#endif /* !CONFIG_MV_ETH_HWF */
+#endif /* CONFIG_MV_ETH_HWF */
+
+	for (txp = 0; txp < pp->txp_num; txp++)
+		for (queue = 0; queue < CONFIG_MV_ETH_TXQ; queue++)
+			mv_eth_txq_clean(pp->port, txp, queue);
 
 	if (mv_eth_ctrl_is_tx_enabled(pp)) {
 		int cpu;
@@ -4951,11 +5094,11 @@ int mv_eth_stop_internals(struct eth_port *pp)
 	for (queue = 0; queue < CONFIG_MV_ETH_RXQ; queue++)
 		mv_eth_rxq_drop_pkts(pp, queue);
 
-	return MV_OK;
+	return 0;
 
 error:
 	printk(KERN_ERR "GbE port %d: stop internals failed\n", pp->port);
-	return MV_ERROR;
+	return -1;
 }
 
 /* return positive if MTU is valid */
@@ -5008,7 +5151,7 @@ int mv_eth_check_mtu_internals(struct net_device *dev, int mtu)
 		}
 	}
 #endif /* CONFIG_MV_ETH_BM_CPU */
-	return MV_OK;
+	return 0;
 }
 
 /***********************************************************
@@ -5079,7 +5222,7 @@ int mv_eth_change_mtu_internals(struct net_device *dev, int mtu)
 
 	netdev_update_features(dev);
 
-	return MV_OK;
+	return 0;
 }
 
 /***********************************************************
@@ -5470,7 +5613,7 @@ static int mv_eth_priv_init(struct eth_port *pp, int port)
 		       pp->port, sizeof(u32) * (pp->txp_num * CONFIG_MV_ETH_TXQ * CONFIG_MV_ETH_TXQ_DESC + 1));
 #endif /* CONFIG_MV_ETH_STAT_DIST */
 
-	return MV_OK;
+	return 0;
 }
 
 /***********************************************************************************
@@ -5932,11 +6075,11 @@ static int mv_eth_port_cleanup(int port)
 	pp = mv_eth_port_by_id(port);
 
 	if (pp == NULL)
-		return MV_ERROR;
+		return -1;
 
 	if (pp->flags & MV_ETH_F_STARTED) {
 		printk(KERN_ERR "%s: port %d is started, cannot cleanup\n", __func__, port);
-		return MV_ERROR;
+		return -1;
 	}
 
 	/* Reset Tx ports */
@@ -6006,7 +6149,7 @@ static int mv_eth_port_cleanup(int port)
 		for (i = 0; i < CONFIG_MV_ETH_NAPI_GROUPS; i++)
 			netif_napi_del(pp->napiGroup[i]);
 
-	return MV_OK;
+	return 0;
 }
 
 
@@ -6033,7 +6176,7 @@ int mv_eth_all_ports_cleanup(void)
 	memset(mv_eth_ports, 0, (mv_eth_ports_num * sizeof(struct eth_port *)));
 	/* Note: not freeing mv_eth_ports - we will reuse them */
 
-	return MV_OK;
+	return 0;
 }
 
 #ifdef CONFIG_MV_ETH_PNC_WOL
@@ -6104,7 +6247,7 @@ int mv_eth_wol_pkts_check(int port)
 			return 1;
 		}
 	}
-	return MV_OK;
+	return 0;
 }
 
 void mv_eth_wol_wakeup(int port)
@@ -6164,7 +6307,7 @@ int mv_eth_wol_sleep(int port)
 
 	mv_eth_interrupts_unmask(pp);
 
-	return MV_OK;
+	return 0;
 }
 #endif /* CONFIG_MV_ETH_PNC_WOL */
 
@@ -6176,6 +6319,15 @@ PONLINKSTATUSPOLLFUNC pon_link_status_polling_func;
 void pon_link_status_notify_func(MV_BOOL link_state)
 {
 	struct eth_port *pon_port = mv_eth_port_by_id(MV_PON_PORT_ID_GET());
+
+	if ((pon_port->flags & MV_ETH_F_STARTED) == 0) {
+		/* Ignore link event if port is down - link status will be updated on start */
+#ifdef CONFIG_MV_ETH_DEBUG_CODE
+		pr_info("PON port: Link event (%s) when port is down\n",
+			link_state ? "Up" : "Down");
+#endif /* CONFIG_MV_ETH_DEBUG_CODE */
+		return;
+	}
 	mv_eth_link_event(pon_port, 1);
 }
 
diff --git a/arch/arm/plat-armada/mv_hal/neta/bm/mvBm.h b/arch/arm/plat-armada/mv_hal/neta/bm/mvBm.h
index caf48da..3b657b3 100644
--- a/arch/arm/plat-armada/mv_hal/neta/bm/mvBm.h
+++ b/arch/arm/plat-armada/mv_hal/neta/bm/mvBm.h
@@ -100,12 +100,15 @@ extern MV_U8 *mvBmVirtBase;
 /* INLINE functions */
 static INLINE void mvBmPoolPut(int poolId, MV_ULONG bufPhysAddr)
 {
-	*((MV_ULONG *)((unsigned)mvBmVirtBase | (poolId << BM_POOL_ACCESS_OFFS))) = (MV_ULONG)(MV_32BIT_LE(bufPhysAddr));
+	volatile MV_U32 *poolAddr = (MV_U32 *)((unsigned)mvBmVirtBase | (poolId << BM_POOL_ACCESS_OFFS));
+
+	*poolAddr = MV_32BIT_LE((MV_U32)bufPhysAddr);
 }
 
 static INLINE MV_ULONG mvBmPoolGet(int poolId)
 {
-	MV_U32	bufPhysAddr = *(MV_U32 *)((unsigned)mvBmVirtBase | (poolId << 8));
+	volatile MV_U32 *poolAddr = (MV_U32 *)((unsigned)mvBmVirtBase | (poolId << BM_POOL_ACCESS_OFFS));
+	MV_U32	bufPhysAddr = *poolAddr;
 
 	return (MV_ULONG)(MV_32BIT_LE(bufPhysAddr));
 }
diff --git a/arch/arm/plat-armada/mv_hal/neta/gbe/mvHwf.c b/arch/arm/plat-armada/mv_hal/neta/gbe/mvHwf.c
index 2c60d0c..f693f29 100644
--- a/arch/arm/plat-armada/mv_hal/neta/gbe/mvHwf.c
+++ b/arch/arm/plat-armada/mv_hal/neta/gbe/mvHwf.c
@@ -235,6 +235,19 @@ MV_STATUS mvNetaHwfTxqInit(int tx_port, int txp, int txq)
 	return MV_OK;
 }
 
+MV_STATUS mvNetaHwfTxqNextIndexGet(int port, int tx_port, int txp, int txq, int *val)
+{
+	MV_U32				regVal;
+
+	regVal = NETA_HWF_TX_PORT_MASK(tx_port + txp) | NETA_HWF_TXQ_MASK(txq) | NETA_HWF_REG_MASK(3);
+	MV_REG_WRITE(NETA_HWF_TX_PTR_REG(port), regVal);
+
+	regVal = MV_REG_READ(NETA_HWF_MEMORY_REG(port));
+	if (val)
+		*val = (int)((regVal >> 16) & 0x3fff);
+
+	return MV_OK;
+}
 
 /*******************************************************************************
  * mvNetaHwfTxqEnable - Enable / Disable HWF from the rx_port to tx_port/txp/txq
diff --git a/arch/arm/plat-armada/mv_hal/neta/gbe/mvNeta.c b/arch/arm/plat-armada/mv_hal/neta/gbe/mvNeta.c
index 2ef74c0..28cc933 100644
--- a/arch/arm/plat-armada/mv_hal/neta/gbe/mvNeta.c
+++ b/arch/arm/plat-armada/mv_hal/neta/gbe/mvNeta.c
@@ -2195,7 +2195,6 @@ void mvNetaRxReset(int port)
 	MV_REG_WRITE(NETA_PORT_RX_RESET_REG(port), 0);
 }
 
-
 /* Reset all TXQs */
 void mvNetaTxpReset(int port, int txp)
 {
@@ -2271,7 +2270,6 @@ void mvNetaRxqAddrSet(int port, int queue, int descrNum)
 	MV_REG_WRITE(NETA_RXQ_SIZE_REG(pPortCtrl->portNo, queue), descrNum);
 }
 
-
 /*******************************************************************************
 * mvNetaTxqInit - Allocate required memory and initialize TXQ descriptor ring.
 *
diff --git a/arch/arm/plat-armada/mv_hal/neta/gbe/mvNeta.h b/arch/arm/plat-armada/mv_hal/neta/gbe/mvNeta.h
index 01166c3..2b5470a 100644
--- a/arch/arm/plat-armada/mv_hal/neta/gbe/mvNeta.h
+++ b/arch/arm/plat-armada/mv_hal/neta/gbe/mvNeta.h
@@ -533,6 +533,30 @@ static INLINE int mvNetaTxqSentDescNumGet(int port, int txp, int txq)
 	return sent_desc;
 }
 
+/* Invalidate TXQ descriptor - buffer will not be sent, buffer will not be returned */
+static INLINE void mvNetaTxqDescInv(NETA_TX_DESC *pTxDesc)
+{
+	pTxDesc->command |= NETA_TX_HWF_MASK;
+	pTxDesc->command &= ~NETA_TX_BM_ENABLE_MASK;
+	pTxDesc->hw_cmd |= NETA_TX_ES_MASK;
+}
+
+/* Return: 1 - TX descriptor is valid, 0 - TX descriptor is invalid */
+static INLINE int mvNetaTxqDescIsValid(NETA_TX_DESC *pTxDesc)
+{
+	return ((pTxDesc->hw_cmd & NETA_TX_ES_MASK) == 0);
+}
+
+/* Get index of descripotor to be processed next in the specific TXQ */
+static INLINE int mvNetaTxqNextIndexGet(int port, int txp, int txq)
+{
+	MV_U32 regVal;
+
+	regVal = MV_REG_READ(NETA_TXQ_INDEX_REG(port, txp, txq));
+
+	return (regVal & NETA_TXQ_NEXT_DESC_INDEX_MASK) >> NETA_TXQ_NEXT_DESC_INDEX_OFFS;
+}
+
 /* Get number of TX descriptors didn't send by HW yet and waiting for TX */
 static INLINE int mvNetaTxqPendDescNumGet(int port, int txp, int txq)
 {
@@ -827,6 +851,7 @@ MV_STATUS mvNetaHwfTxqEnable(int port, int p, int txp, int txq, int enable);
 MV_STATUS mvNetaHwfTxqDropSet(int port, int p, int txp, int txq, int thresh, int bits);
 MV_STATUS mvNetaHwfMhSrcSet(int port, MV_NETA_HWF_MH_SRC src);
 MV_STATUS mvNetaHwfMhSelSet(int port, MV_U8 mhSel);
+MV_STATUS mvNetaHwfTxqNextIndexGet(int port, int tx_port, int txp, int txq, int *val);
 
 void mvNetaHwfRxpRegs(int port);
 void mvNetaHwfTxpRegs(int port, int p, int txp);
diff --git a/arch/arm/plat-armada/mv_hal/neta/gbe/mvNetaDebug.c b/arch/arm/plat-armada/mv_hal/neta/gbe/mvNetaDebug.c
index 325830a..e180c91 100644
--- a/arch/arm/plat-armada/mv_hal/neta/gbe/mvNetaDebug.c
+++ b/arch/arm/plat-armada/mv_hal/neta/gbe/mvNetaDebug.c
@@ -592,7 +592,10 @@ void mvEthPortCounters(int port, int mib)
 	mvEthMibPrint(port, mib, ETH_MIB_EXCESSIVE_COLLISION, "EXCESSIVE_COLLISION");
 	mvEthMibPrint(port, mib, ETH_MIB_COLLISION, "COLLISION");
 	mvEthMibPrint(port, mib, ETH_MIB_LATE_COLLISION, "LATE_COLLISION");
-
+#ifdef MV_ETH_PMT_NEW
+	mvEthRegPrint0(NETA_TX_BAD_FCS_CNTR_REG(port, mib), "NETA_TX_BAD_FCS_CNTR_REG");
+	mvEthRegPrint0(NETA_TX_DROP_CNTR_REG(port, mib), "NETA_TX_DROP_CNTR_REG");
+#endif
 	mvOsPrintf("\n[FC control]\n");
 	mvEthMibPrint(port, mib, ETH_MIB_UNREC_MAC_CONTROL_RECEIVED, "UNREC_MAC_CONTROL_RECEIVED");
 	mvEthMibPrint(port, mib, ETH_MIB_GOOD_FC_RECEIVED, "GOOD_FC_RECEIVED");
diff --git a/arch/arm/plat-armada/mv_hal/neta/gbe/mvNetaRegs.h b/arch/arm/plat-armada/mv_hal/neta/gbe/mvNetaRegs.h
index 801dda8..f4e315b 100644
--- a/arch/arm/plat-armada/mv_hal/neta/gbe/mvNetaRegs.h
+++ b/arch/arm/plat-armada/mv_hal/neta/gbe/mvNetaRegs.h
@@ -315,6 +315,10 @@ extern "C" {
 #define NETA_HWF_TXQ_OFFS                   8
 #define NETA_HWF_TXQ_ALL_MASK               (0x7 << NETA_HWF_TXQ_OFFS)
 #define NETA_HWF_TXQ_MASK(txq)              ((txq) << NETA_HWF_TXQ_OFFS)
+
+#define NETA_HWF_REG_OFFS                   0
+#define NETA_HWF_REG_ALL_MASK               (0x7 << NETA_HWF_REG_OFFS)
+#define NETA_HWF_REG_MASK(reg)              ((reg) << NETA_HWF_REG_OFFS)
 /*-----------------------------------------------------------------------------------*/
 
 #define NETA_HWF_DROP_TH_REG(p)             (NETA_REG_BASE(p) + 0x1d40)
-- 
1.7.5.4

