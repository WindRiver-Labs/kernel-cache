From 58bc1278463d877bcecccf9af2c814fca4334586 Mon Sep 17 00:00:00 2001
From: Dmitri Epshtein <dima@marvell.com>
Date: Thu, 18 Jul 2013 18:13:13 -0400
Subject: [PATCH 0850/1825] NETA: Merge NETA driver with Linux kernel 3.2.46

https://github.com/MISL-EBU-System-SW/misl-windriver.git linux-3.4.69-14t2-read
commit 340cdd06a4069840ade387ec822857a338919e68

		Some fixes taken from kernel 3.2.46
		- NETA: fix: hot unplug bug with timer crushed
		- NETA: Fix rate limit default configuration - support 2 Gbps
		Fix compilation issues
		Fix checkpatch errors and warnings
		Ready to create shared component.

Change-Id: Iff582927c021903246ad1017159c1a8184e8a88e
Signed-off-by: Dmitri Epshtein <dima@marvell.com>
Reviewed-on: http://vgitil04.il.marvell.com:8080/2730
Tested-by: Star_Automation <star@marvell.com>
Reviewed-by: Jonatan Farhadian <yonif@marvell.com>
Reviewed-by: Igor Patrik <igorp@marvell.com>
Signed-off-by: Zhong Hongbo <hongbo.zhong@windriver.com>
---
 .../arm/plat-armada/mv_drivers_lsp/mv_neta/Kconfig |  240 +----
 .../mv_drivers_lsp/mv_neta/l2fw/l2fw_sysfs.c       |  186 ++--
 .../mv_drivers_lsp/mv_neta/l2fw/mv_eth_l2fw.c      |  521 ++++-----
 .../mv_drivers_lsp/mv_neta/l2fw/mv_eth_l2fw.h      |   62 +-
 .../mv_drivers_lsp/mv_neta/l2fw/mv_eth_l2sec.c     |  822 +++++++++------
 .../mv_drivers_lsp/mv_neta/l2fw/mv_eth_l2sec.h     |  519 ++--------
 .../mv_drivers_lsp/mv_neta/net_dev/Makefile        |   17 +-
 .../mv_drivers_lsp/mv_neta/net_dev/mv_eth_nfp.c    | 1068 ++++++++++++++++++
 .../mv_drivers_lsp/mv_neta/net_dev/mv_eth_switch.c |   65 ++
 .../mv_drivers_lsp/mv_neta/net_dev/mv_eth_sysfs.c  |   58 +-
 .../mv_drivers_lsp/mv_neta/net_dev/mv_eth_tool.c   |   80 ++-
 .../mv_drivers_lsp/mv_neta/net_dev/mv_eth_tool.h   |   32 +-
 .../mv_drivers_lsp/mv_neta/net_dev/mv_netdev.c     | 1190 +++++++++++---------
 .../mv_drivers_lsp/mv_neta/net_dev/mv_netdev.h     |   67 +-
 14 files changed, 3023 insertions(+), 1904 deletions(-)
 create mode 100644 arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/mv_eth_nfp.c

diff --git a/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/Kconfig b/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/Kconfig
index 9e7d325..ab472b9 100644
--- a/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/Kconfig
+++ b/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/Kconfig
@@ -229,7 +229,8 @@ menuconfig MV_ETH_PNC
 config MV_PNC_TCAM_LINES
         depends on MV_ETH_PNC
         int "Number of TCAM lines supported by PNC"
-        default 512
+        default 512  if ARCH_FEROCEON_KW2
+	default 1024 if ARCH_ARMADA_XP
         ---help---
 
 config MV_ETH_PNC_MCAST_NUM
@@ -668,216 +669,58 @@ config MV_ETH_EXTRA_BUF_NUM
 	Number of extra buffers allocated for each port
 endmenu
 
-menu "NFP support"
+menu "Network Fast Processing (NFP) support"
 
 config  MV_ETH_NFP
-        bool "Use Network Fast Processing (NFP)"
-	default n
-        ---help---
-        Choosing this option will enable Network Fast Processing.
-
-config MV_ETH_NFP_DEF
-        depends on MV_ETH_NFP
-        int "Default value for NFP state:  0 - OFF, 1 - ON"
-        default 0
-
-config MV_ETH_NFP_LEARN
-        tristate "NFP Dynamic Learning"
-		depends on MV_ETH_NFP
-	default n
-        ---help---
-        Choosing this option will enable NFP Dynamic Learning.
-
-config MV_ETH_NFP_MODE_DEF
-        depends on MV_ETH_NFP
-        int "Default value for NFP mode:  1 - 2 tuple mode, 2 - 5 tuple mode"
-        default 1
-	range 1 2
-        ---help---
-
-config  MV_ETH_NFP_EXT
-        bool "Support NFP for External (non GBE) network interfaces"
-        depends on MV_ETH_NFP
-        default n
-         ---help---
-        Choosing this option will enable NFP for non-GBE network interfaces.
-
-config  MV_ETH_NFP_EXT_NUM
-        int "Maximum number of External (non-Gbe) interfaces"
-        depends on MV_ETH_NFP_EXT
-        default 1
-	range 1 4
-         ---help---
-
-config MV_ETH_NFP_BRIDGE
-        bool "Support NFP Bridging"
-        default y
-        depends on MV_ETH_NFP
-        ---help---
-        Choosing this option will enable NFP bridging support.
-
-choice
-        prompt "NFP Bridging Mode"
-        depends on MV_ETH_NFP_BRIDGE
-        default MV_ETH_NFP_FDB_MODE
-
-config	MV_ETH_NFP_FDB_MODE
-	bool "NFP FDB"
-	depends on MV_ETH_NFP_BRIDGE
-	---help---
-	  enable NFP FDB mode
-
-config  MV_ETH_NFP_BRIDGE_MODE
-	bool "NFP Bridge"
-	depends on MV_ETH_NFP_BRIDGE
-	---help---
-	  enable NFP Bridge Mode
-
-endchoice
-
-config	MV_ETH_NFP_FDB_LEARN
-	depends on MV_ETH_NFP_BRIDGE && MV_ETH_NFP_FDB_MODE && BRIDGE && MV_ETH_NFP_LEARN
-	bool "Support NFP FDB Dynamic Learning"
-	default y
-	---help---
-	  enable NFP bridging dynamic learning via NFP hooks
-
-config MV_ETH_NFP_FDB_LEARN_DEF
-        depends on MV_ETH_NFP_FDB_LEARN
-        int "Default value for NFP FDB Dynamic Learning:  0 - disable, 1 - enable"
-        default 1
-	range 0 1
-        ---help---
-
-config MV_ETH_NFP_VLAN
-        bool "Support NFP VLANs processing"
-        default y
-        depends on MV_ETH_NFP
-        ---help---
-        Choosing this option will enable NFP VLANs support.
-
-config MV_ETH_NFP_VLAN_LEARN
-        depends on MV_ETH_NFP_VLAN && VLAN_8021Q && MV_ETH_NFP_LEARN
-        bool "Support NFP VLAN Dynamic Learning"
-        default y
-        ---help---
-        Choosing this option will enable NFP VLAN dynamic learning via NFP hooks
-	in Linux Network stack.
-
-config MV_ETH_NFP_VLAN_LEARN_DEF
-        depends on MV_ETH_NFP_VLAN_LEARN
-        int "Default value for NFP VLAN Dynamic Learning:  0 - disable, 1 - enable"
-        default 1
-	range 0 1
-        ---help---
-
-config MV_ETH_NFP_FIB
-        bool "Support NFP Routing"
-	default y
-        depends on MV_ETH_NFP
-        ---help---
-        Choosing this option will enable NFP routing support.
-
-config MV_ETH_NFP_FIB_LEARN
-        depends on MV_ETH_NFP_FIB && MV_ETH_NFP_LEARN
-        bool "Support NFP Routing Dynamic Learning"
-        default y
-        ---help---
-        Choosing this option will enable NFP Routing dynamic learning via NFP hooks
-	in Linux Network stack.
-
-config MV_ETH_NFP_FIB_LEARN_DEF
-        depends on MV_ETH_NFP_FIB_LEARN
-        int "Default value for NFP Routing Dynamic Learning:  0 - disable, 1 - enable"
-        default 1
-	range 0 1
-        ---help---
-
-config MV_ETH_NFP_CT
-        bool "Support NFP 5 Tuple Rules"
-        depends on MV_ETH_NFP_FIB
-	default y
-        ---help---
-        Choosing this option will enable NFP 5 Tuple Rules support.
-
-config MV_ETH_NFP_CT_LEARN
-        depends on MV_ETH_NFP_CT && NF_CONNTRACK && MV_ETH_NFP_LEARN
-        bool "Support NFP 5 Tuple Dynamic Learning"
-        default y
-        ---help---
-        Choosing this option will enable NFP 5 Tuple dynamic learning via NFP hooks
-	in Linux Network stack.
-
-config MV_ETH_NFP_CT_LEARN_DEF
-        depends on MV_ETH_NFP_CT_LEARN
-        int "Default value for NFP 5 Tuple Dynamic Learning:  0 - disable, 1 - enable"
-        default 1
-	range 0 1
-        ---help---
-
-config MV_ETH_NFP_NAT
-        bool "Support NFP NAT"
-        depends on MV_ETH_NFP_CT
-	default y
-        ---help---
-        Choosing this option will enable NFP NAT support.
-
-config MV_ETH_NFP_LIMIT
-        bool "Support NFP Ingress Rate Limiting"
-        depends on MV_ETH_NFP_CT
-	default n
-        ---help---
-        Choosing this option will enable NFP rate limitation support based on 5 tuple rule.
-
-config  MV_ETH_NFP_CLASSIFY
-	bool "Support NFP Classification rules"
-	depends on MV_ETH_NFP_CT || MV_ETH_NFP_BRIDGE
-	default y
-	---help---
-	Choosing this option will enable NFP classification rules (DSCP and VLAN modification and Tx queue selection)
-
-config  MV_ETH_NFP_PPP
-	bool "Support NFP PPPoE"
-	depends on MV_ETH_NFP_FIB && MV_ETH_PNC
+	depends on MV_ETH_NETA
+        bool "NFP support"
 	default n
-	 ---help---
-	Choosing this option will enable NFP PPPoE protocol.
-
-config MV_ETH_NFP_PPP_LEARN
-        depends on MV_ETH_NFP_PPP && PPPOE && MV_ETH_NFP_LEARN
-        bool "Support NFP PPPoE Dynamic Learning"
-        default y
-        ---help---
-        Choosing this option will enable NFP PPPoE dynamic learning via NFP hooks
-	in Linux Network stack.
-
-config MV_ETH_NFP_PPP_LEARN_DEF
-        depends on MV_ETH_NFP_PPP_LEARN
-        int "Default value for NFP PPPoE Dynamic Learning:  0 - disable, 1 - enable"
-        default 1
-	range 0 1
         ---help---
+        Choosing this option will enable Network Fast Processing support.
+	Kernel image will be able to use NFP modules.
+	NFP provided as different package and must be compiled separately.
+	NFP support include two modules:
+	NFP core functionality and NFP dynamic learning.
 
-config  MV_ETH_NFP_STATS
-        bool "Collect NFP Statistics"
+config MV_ETH_NFP_HOOKS
+        bool "NFP IP stack Hooks"
 	depends on MV_ETH_NFP
-        default n
+	default y
         ---help---
-        Collect NFP statistics. Can be displayed using mv_eth_tool.
+        Choosing this option will enable NFP Dynamic Learning.
+	Marvell specific code was added to few files
+	in Linux Network Stack. Without this configration option only
+	static NFP configuration is enabled.
 
-config  MV_ETH_NFP_DEBUG
-	bool "Add NFP debug code"
+config MV_ETH_NFP_EXT
+	bool "Support NFP for External (non GBE) network interfaces"
 	depends on MV_ETH_NFP
 	default n
-        ---help---
-	Add NFP sanity check code
+	---help---
+	Enable NFP support for External (non GBE) network interfaces.
+	It doesn't require special changes in external network drivers,
+	but NFP can be used only for NAPI capable drivers.
+	Leave default if unsure.
 
+config MV_ETH_NFP_EXT_NUM
+	depends on MV_ETH_NFP_EXT
+	int "Maximum number of External (non-Gbe) interfaces"
+	default 1
+	range 1 4
 endmenu
 
-menu "NAPI Groups"
+menuconfig MV_ETH_NAPI
+        bool "NAPI configuration"
+        default y
+	---help---
+	This menu used for NAPI groups configuration.
+	Leave default if RSS support is not required.
+	Enable create multiple NAPI groups per port and attach RXQs and CPUs to NAPI groups.
+	Each CPU and each RXQ can be attached to single NAPI group.
 
 config  MV_ETH_NAPI_GROUPS
         int "Number of NAPI instances can be used per port"
+	depends on MV_ETH_NAPI
 	range 1 NR_CPUS if SMP
 	range 1 1 if !SMP
         default 1
@@ -885,6 +728,7 @@ config  MV_ETH_NAPI_GROUPS
 	Different RXQs and TXQs can be processed by different CPU using different NAPI instances
 
 menu "NAPI group #0 configuration"
+	depends on MV_ETH_NAPI
 
 config MV_ETH_GROUP0_CPU
 	hex "CPU affinity for group0"
@@ -901,6 +745,7 @@ config MV_ETH_GROUP0_RXQ
 endmenu
 
 menu "NAPI group #1 configuration"
+        depends on MV_ETH_NAPI
 	depends on (MV_ETH_NAPI_GROUPS != 1)
 
 config MV_ETH_GROUP1_CPU
@@ -918,6 +763,7 @@ config MV_ETH_GROUP1_RXQ
 endmenu
 
 menu "NAPI group #2 configuration"
+	depends on MV_ETH_NAPI
 	depends on (MV_ETH_NAPI_GROUPS != 1) && (MV_ETH_NAPI_GROUPS != 2)
 
 config MV_ETH_GROUP2_CPU
@@ -935,6 +781,7 @@ config MV_ETH_GROUP2_RXQ
 endmenu
 
 menu "NAPI group #3 configuration"
+	depends on MV_ETH_NAPI
 	depends on (MV_ETH_NAPI_GROUPS != 1) && (MV_ETH_NAPI_GROUPS != 2) && (MV_ETH_NAPI_GROUPS != 3)
 
 config MV_ETH_GROUP3_CPU
@@ -950,7 +797,6 @@ config MV_ETH_GROUP3_RXQ
 	range 0x0 0xff
 	default 0x0
 endmenu
-endmenu
 
 menu "PON support for Network driver"
 
diff --git a/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/l2fw/l2fw_sysfs.c b/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/l2fw/l2fw_sysfs.c
index 4e8c2b7..54ecced 100644
--- a/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/l2fw/l2fw_sysfs.c
+++ b/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/l2fw/l2fw_sysfs.c
@@ -1,4 +1,30 @@
-/* l2fw_sysfs.c */
+/*******************************************************************************
+Copyright (C) Marvell International Ltd. and its affiliates
+
+This software file (the "File") is owned and distributed by Marvell
+International Ltd. and/or its affiliates ("Marvell") under the following
+alternative licensing terms.  Once you have made an election to distribute the
+File under one of the following license alternatives, please (i) delete this
+introductory statement regarding license alternatives, (ii) delete the two
+license alternatives that you have not elected to use and (iii) preserve the
+Marvell copyright notice above.
+
+
+********************************************************************************
+Marvell GPL License Option
+
+If you received this File from Marvell, you may opt to use, redistribute and/or
+modify this File in accordance with the terms and conditions of the General
+Public License Version 2, June 1991 (the "GPL License"), a copy of which is
+available along with the File in the license.txt file or by writing to the Free
+Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 or
+on the worldwide web at http://www.gnu.org/licenses/gpl.txt.
+
+THE FILE IS DISTRIBUTED AS-IS, WITHOUT WARRANTY OF ANY KIND, AND THE IMPLIED
+WARRANTIES OF MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE ARE EXPRESSLY
+DISCLAIMED.  The GPL License provides additional details about this warranty
+disclaimer.
+*******************************************************************************/
 #include <linux/module.h>
 #include <linux/kernel.h>
 #include <linux/types.h>
@@ -8,24 +34,33 @@
 
 #include "mvTypes.h"
 #include "mv_eth_l2fw.h"
-#include "linux/inet.h"
-
 #ifdef CONFIG_MV_ETH_L2SEC
-extern int l2fw_set_cesa_chan(int port, int cesaChan);
+#include "mv_eth_l2sec.h"
 #endif
+#include "linux/inet.h"
+
 
 static ssize_t l2fw_help(char *buf)
 {
 	int off = 0;
-	off += sprintf(buf+off, "help\n");
-	off += sprintf(buf+off, "echo mode rxp txp > l2fw - set l2f <rxp>->");
-	off += sprintf(buf+off, "<txp><mode> 0-dis,1-as_is,2-swap,3-copy\n");
-	off += sprintf(buf+off, "echo threshold > l2fw_xor: set threshold\n");
+
+	off += sprintf(buf+off, "cat rules_dump                - display L2fw rules DB\n");
+	off += sprintf(buf+off, "cat ports_dump                - display L2fw ports DB\n");
+	off += sprintf(buf+off, "cat stats                     - show debug information\n");
+
+	/* inputs in decimal */
+	off += sprintf(buf+off, "echo rxp txp mode > l2fw      - set L2FW mode: 0-dis,1-as_is,2-swap,3-copy,4-ipsec\n");
+	off += sprintf(buf+off, "echo rxp thresh   > l2fw_xor  - set XOR threshold in bytes for port <rxp>\n");
+	off += sprintf(buf+off, "echo rxp en       > lookup    - enable/disable hash lookup for <rxp>\n");
+	off += sprintf(buf+off, "echo 1            > flush     - flush L2fw rules DB\n");
+
+	/* inputs in hex */
+	off += sprintf(buf+off, "echo sip dip txp  > rule_add  - set rule for SIP and DIP pair. [x.x.x.x]\n");
+
 #ifdef CONFIG_MV_ETH_L2SEC
-	off += sprintf(buf+off, "echo 1 > esp   - enable ESP\n");
+	off += sprintf(buf+off, "echo p chan       > cesa_chan - set cesa channel <chan> for port <p>.\n");
 #endif
-	off += sprintf(buf+off, "cat dump - display L2fw rules DB\n");
-	off += sprintf(buf+off, "echo 1 > flush - flush L2fw rules DB\n");
+
 	return off;
 }
 
@@ -41,33 +76,17 @@ static ssize_t l2fw_show(struct device *dev,
 	if (!strcmp(name, "help")) {
 	    off = l2fw_help(buf);
 		return off;
-	}
-	if (!strcmp(name, "dump")) {
-		l2fw_dump();
+	} else if (!strcmp(name, "rules_dump")) {
+		l2fw_rules_dump();
 		return off;
-	}
-	if (!strcmp(name, "numHashEntries")) {
-		l2fw_show_numHashEntries();
+	} else if (!strcmp(name, "ports_dump")) {
+		l2fw_ports_dump();
 		return off;
-	}
-#ifdef CONFIG_MV_ETH_L2SEC
-	if (!strcmp(name, "esp")) {
-		l2fw_esp_show();
-		return off;
-	}
-#endif
-	if (!strcmp(name, "help")) {
-	    off = l2fw_help(buf);
+	} else if (!strcmp(name, "stats")) {
+		l2fw_stats();
 		return off;
 	}
 
-#ifdef CONFIG_MV_ETH_L2SEC
-	if (!strcmp(name, "stats")) {
-	    l2fw_stats();
-		return off;
-	}
-#endif
-
 	return off;
 }
 
@@ -81,26 +100,18 @@ static ssize_t l2fw_hex_store(struct device *dev, struct device_attribute *attr,
 	unsigned int    addr1, addr2;
 	int port;
 	unsigned long   flags;
-#ifdef CONFIG_MV_ETH_L2SEC
-	int             enableEsp;
-#endif
+
 	if (!capable(CAP_NET_ADMIN))
 		return -EPERM;
 	err = addr1 = addr2 = port = 0;
 
 	local_irq_save(flags);
-	if (!strcmp(name, "l2fw_add")) {
-		sscanf(buf, "%x %x %d", &addr1, &addr2, &port);
-		l2fw_add(addr1, addr2, port);
-	} else if (!strcmp(name, "l2fw_add_ip")) {
-		l2fw_add_ip(buf);
-#ifdef CONFIG_MV_ETH_L2SEC
-	} else if (!strcmp(name, "esp")) {
-		sscanf(buf, "%d", &enableEsp);
-		l2fw_esp_set(enableEsp);
-#endif
-	} else if (!strcmp(name, "flush")) {
+
+	if (!strcmp(name, "flush")) {
 		l2fw_flush();
+	} else {
+		err = 1;
+		printk(KERN_ERR "%s: illegal operation <%s>\n", __func__, attr->attr.name);
 	}
 
 	local_irq_restore(flags);
@@ -108,31 +119,72 @@ static ssize_t l2fw_hex_store(struct device *dev, struct device_attribute *attr,
 	return err ? -EINVAL : len;
 }
 
+static ssize_t l2fw_ip_store(struct device *dev,
+			 struct device_attribute *attr, const char *buf, size_t len)
+{
+	const char *name = attr->attr.name;
+
+	unsigned int err = 0;
+	unsigned int srcIp = 0, dstIp = 0;
+	unsigned char *sipArr = (unsigned char *)&srcIp;
+	unsigned char *dipArr = (unsigned char *)&dstIp;
+	int port;
+	unsigned long flags;
+
+	if (!capable(CAP_NET_ADMIN))
+		return -EPERM;
+
+	sscanf(buf, "%hhu.%hhu.%hhu.%hhu %hhu.%hhu.%hhu.%hhu %d",
+		sipArr, sipArr+1, sipArr+2, sipArr+3,
+		dipArr, dipArr+1, dipArr+2, dipArr+3, &port);
+
+	printk(KERN_INFO "0x%x->0x%x in %s\n", srcIp, dstIp, __func__);
+	local_irq_save(flags);
+
+	if (!strcmp(name, "l2fw_add_ip"))
+		l2fw_add(srcIp, dstIp, port);
+	else {
+		err = 1;
+		printk(KERN_ERR "%s: illegal operation <%s>\n", __func__, attr->attr.name);
+	}
+
+	local_irq_restore(flags);
+
+	if (err)
+		printk(KERN_ERR "%s: <%s>, error %d\n", __func__, attr->attr.name, err);
+
+	return err ? -EINVAL : len;
+}
+
+
+
 static ssize_t l2fw_store(struct device *dev,
 				   struct device_attribute *attr, const char *buf, size_t len)
 {
 	const char	*name = attr->attr.name;
 	int             err;
 
-	unsigned int    p, txp, txq, v;
+	unsigned int    a, b, c;
 	unsigned long   flags;
 
 	if (!capable(CAP_NET_ADMIN))
 		return -EPERM;
 
-	err = p = txp = txq = v = 0;
-	sscanf(buf, "%d %d %d %d", &p, &txp, &txq, &v);
+	err = a = b = c = 0;
+	sscanf(buf, "%d %d %d", &a, &b, &c);
 
 	local_irq_save(flags);
 
 	if (!strcmp(name, "l2fw_xor"))
-		l2fw_xor(p);
+		l2fw_xor(a, b);
+	else if (!strcmp(name, "lookup"))
+		l2fw_lookupEn(a, b);
 
 	else if (!strcmp(name, "l2fw"))
-		l2fw(p, txp, txq);
+		l2fw(c, a, b);
 #ifdef CONFIG_MV_ETH_L2SEC
 	else if (!strcmp(name, "cesa_chan"))
-		err = l2fw_set_cesa_chan(p, txp);
+		err = mv_l2sec_set_cesa_chan(a, b);
 #endif
 	local_irq_restore(flags);
 
@@ -144,35 +196,35 @@ static ssize_t l2fw_store(struct device *dev,
 }
 
 
-static DEVICE_ATTR(l2fw,			S_IWUSR, l2fw_show, l2fw_store);
+static DEVICE_ATTR(l2fw,		S_IWUSR, l2fw_show, l2fw_store);
 static DEVICE_ATTR(l2fw_xor,		S_IWUSR, l2fw_show, l2fw_store);
-static DEVICE_ATTR(l2fw_add,		S_IWUSR, l2fw_show, l2fw_hex_store);
-static DEVICE_ATTR(l2fw_add_ip,		S_IWUSR, l2fw_show, l2fw_hex_store);
-static DEVICE_ATTR(help,			S_IRUSR, l2fw_show,  NULL);
-static DEVICE_ATTR(dump,			S_IRUSR, l2fw_show,  NULL);
-static DEVICE_ATTR(numHashEntries,	S_IRUSR, l2fw_show,  NULL);
+static DEVICE_ATTR(lookup,		S_IWUSR, l2fw_show, l2fw_store);
+static DEVICE_ATTR(l2fw_add_ip,		S_IWUSR, l2fw_show, l2fw_ip_store);
+static DEVICE_ATTR(help,		S_IRUSR, l2fw_show,  NULL);
+static DEVICE_ATTR(rules_dump,		S_IRUSR, l2fw_show,  NULL);
+static DEVICE_ATTR(ports_dump,		S_IRUSR, l2fw_show,  NULL);
+static DEVICE_ATTR(flush,		S_IWUSR, NULL,	l2fw_hex_store);
+static DEVICE_ATTR(stats,		S_IRUSR, l2fw_show, NULL);
+
 #ifdef CONFIG_MV_ETH_L2SEC
-static DEVICE_ATTR(stats,			S_IRUSR, l2fw_show, NULL);
-static DEVICE_ATTR(esp,				S_IWUSR, l2fw_show,  l2fw_hex_store);
 static DEVICE_ATTR(cesa_chan,		S_IWUSR, NULL,  l2fw_store);
 #endif
-static DEVICE_ATTR(flush,			S_IWUSR, NULL,  	 l2fw_hex_store);
+
 
 
 static struct attribute *l2fw_attrs[] = {
 	&dev_attr_l2fw.attr,
 	&dev_attr_l2fw_xor.attr,
-	&dev_attr_l2fw_add.attr,
+	&dev_attr_lookup.attr,
 	&dev_attr_l2fw_add_ip.attr,
 	&dev_attr_help.attr,
-	&dev_attr_dump.attr,
+	&dev_attr_rules_dump.attr,
+	&dev_attr_ports_dump.attr,
 	&dev_attr_flush.attr,
-#ifdef CONFIG_MV_ETH_L2SEC
-	&dev_attr_esp.attr,
 	&dev_attr_stats.attr,
+#ifdef CONFIG_MV_ETH_L2SEC
 	&dev_attr_cesa_chan.attr,
 #endif
-	&dev_attr_numHashEntries.attr,
 	NULL
 };
 
diff --git a/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/l2fw/mv_eth_l2fw.c b/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/l2fw/mv_eth_l2fw.c
index 3a51ff9..a7c94f0 100644
--- a/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/l2fw/mv_eth_l2fw.c
+++ b/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/l2fw/mv_eth_l2fw.c
@@ -1,5 +1,34 @@
-/* mv_eth_l2fw.c */
+/*******************************************************************************
+Copyright (C) Marvell International Ltd. and its affiliates
+
+This software file (the "File") is owned and distributed by Marvell
+International Ltd. and/or its affiliates ("Marvell") under the following
+alternative licensing terms.  Once you have made an election to distribute the
+File under one of the following license alternatives, please (i) delete this
+introductory statement regarding license alternatives, (ii) delete the two
+license alternatives that you have not elected to use and (iii) preserve the
+Marvell copyright notice above.
+
+
+********************************************************************************
+Marvell GPL License Option
+
+If you received this File from Marvell, you may opt to use, redistribute and/or
+modify this File in accordance with the terms and conditions of the General
+Public License Version 2, June 1991 (the "GPL License"), a copy of which is
+available along with the File in the license.txt file or by writing to the Free
+Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 or
+on the worldwide web at http://www.gnu.org/licenses/gpl.txt.
+
+THE FILE IS DISTRIBUTED AS-IS, WITHOUT WARRANTY OF ANY KIND, AND THE IMPLIED
+WARRANTIES OF MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE ARE EXPRESSLY
+DISCLAIMED.  The GPL License provides additional details about this warranty
+disclaimer.
+*******************************************************************************/
+
 #include <linux/ctype.h>
+#include <linux/module.h>
+#include  <linux/interrupt.h>
 
 #include "xor/mvXor.h"
 #include "xor/mvXorRegs.h"
@@ -10,17 +39,16 @@
 #include "mv_neta/net_dev/mv_netdev.h"
 #include "gbe/mvNeta.h"
 #include "gbe/mvNetaRegs.h"
-
-#include "mv_eth_l2fw.h"
+#include "mvDebug.h"
+#include "mv_eth_l2sec.h"
 #include "ctrlEnv/mvCtrlEnvLib.h"
+#include "gbe/mvNeta.h"
 
 #ifdef CONFIG_MV_ETH_L2SEC
-extern int cesa_init(void);
-extern MV_STATUS handleEsp(struct eth_pbuf *pkt, struct neta_rx_desc *rx_desc,
-							struct eth_port  *new_pp, int inPort);
+#include "mv_eth_l2sec.h"
 #endif
 
-int espEnabled = 0;
+static int numHashEntries;
 
 struct eth_pbuf *mv_eth_pool_get(struct bm_pool *pool);
 
@@ -28,11 +56,10 @@ static int mv_eth_ports_l2fw_num;
 
 static L2FW_RULE **l2fw_hash = NULL;
 
-#define	L2FW_HASH_MASK   (L2FW_HASH_SIZE - 1)
-
 static MV_U32 l2fw_jhash_iv;
 
-static int numHashEntries;
+static MV_XOR_DESC *eth_xor_desc;
+static MV_LONG      eth_xor_desc_phys_addr;
 
 struct eth_port_l2fw **mv_eth_ports_l2fw;
 static inline int       mv_eth_l2fw_rx(struct eth_port *pp, int rx_todo, int rxq);
@@ -40,56 +67,6 @@ static inline MV_STATUS mv_eth_l2fw_tx(struct eth_pbuf *pkt, struct eth_port *pp
 					   int withXor, struct neta_rx_desc *rx_desc);
 
 
-void printBufVirtPtr(MV_BUF_INFO *pBuf)
-{
-	int i;
-	if (pBuf->bufVirtPtr == NULL) {
-		printk(KERN_INFO "pBuf->bufVirtPt==NULL in %s\n", __func__);
-		return;
-	}
-	for (i = 0; i < 40; i++) {
-		printk(KERN_INFO "KERN_INFO [%d]=%x ", i, pBuf->bufVirtPtr[i]);
-		if (!(i%10) && i > 1)
-			printk(KERN_INFO "\n");
-	}
-	printk(KERN_INFO "\n****************** %s\n", __func__);
-
-}
-void printBufInfo(MV_BUF_INFO *pbuf)
-{
-	printk(KERN_INFO "bufSize=%d\n"      , pbuf->bufSize);
-	printk(KERN_INFO "dataSize=%d\n"     , pbuf->dataSize);
-	printk(KERN_INFO "memHandle=%d\n"    , pbuf->memHandle);
-	printk(KERN_INFO "bufAddrShift=%d\n" , pbuf->bufAddrShift);
-	printk(KERN_INFO "*****************************************\n\n");
-
-}
-
-
-static s32 atoi(char *psz_buf)
-{
-	char *pch = psz_buf;
-	s32 base = 0;
-	unsigned long res;
-	int ret_val;
-
-	while (isspace(*pch))
-			pch++;
-
-	if (*pch == '-' || *pch == '+') {
-			base = 10;
-			pch++;
-	} else if (*pch && tolower(pch[strlen(pch) - 1]) == 'h') {
-			base = 16;
-	}
-
-	ret_val = strict_strtoul(pch, base, &res);
-
-	return ret_val ? : res;
-}
-
-
-
 static L2FW_RULE *l2fw_lookup(MV_U32 srcIP, MV_U32 dstIP)
 {
 	MV_U32 hash;
@@ -98,6 +75,7 @@ static L2FW_RULE *l2fw_lookup(MV_U32 srcIP, MV_U32 dstIP)
 	hash = mv_jhash_3words(srcIP, dstIP, (MV_U32) 0, l2fw_jhash_iv);
 	hash &= L2FW_HASH_MASK;
 	rule = l2fw_hash[hash];
+
 #ifdef CONFIG_MV_ETH_L2FW_DEBUG
 	if (rule)
 		printk(KERN_INFO "rule is not NULL in %s\n", __func__);
@@ -110,6 +88,7 @@ static L2FW_RULE *l2fw_lookup(MV_U32 srcIP, MV_U32 dstIP)
 
 		rule = rule->next;
 	}
+
 	return NULL;
 }
 
@@ -131,7 +110,7 @@ void l2fw_flush(void)
 }
 
 
-void l2fw_dump(void)
+void l2fw_rules_dump(void)
 {
 	MV_U32 i = 0;
 	L2FW_RULE *currRule;
@@ -146,7 +125,7 @@ void l2fw_dump(void)
 		dstIP = (MV_U8 *)&(currRule->dstIP);
 
 		while (currRule != NULL) {
-			mvOsPrintf("%u.%u.%u.%u->%u.%u.%u.%u    out port=%d (hash=%x)\n",
+			mvOsPrintf("%u.%u.%u.%u->%u.%u.%u.%u     out port=%d (hash=%x)\n",
 				MV_IPQUAD(srcIP), MV_IPQUAD(dstIP),
 				currRule->port, i);
 			currRule = currRule->next;
@@ -155,6 +134,22 @@ void l2fw_dump(void)
 
 }
 
+void l2fw_ports_dump(void)
+{
+	MV_U32 rx_port = 0;
+	struct eth_port_l2fw *ppl2fw;
+
+	mvOsPrintf("\nPrinting L2fw ports Database:\n");
+	mvOsPrintf("*******************************\n");
+
+	for (rx_port = 0; rx_port < mv_eth_ports_l2fw_num; rx_port++) {
+		ppl2fw = mv_eth_ports_l2fw[rx_port];
+		mvOsPrintf("rx_port=%d cmd = %d tx_port=%d lookup=%d xor_threshold = %d\n",
+				rx_port, ppl2fw->cmd, ppl2fw->txPort, ppl2fw->lookupEn, ppl2fw->xorThreshold);
+
+	}
+}
+
 
 MV_STATUS l2fw_add(MV_U32 srcIP, MV_U32 dstIP, int port)
 {
@@ -178,102 +173,9 @@ MV_STATUS l2fw_add(MV_U32 srcIP, MV_U32 dstIP, int port)
 #endif
 
 	l2fw_rule = l2fw_lookup(srcIP, dstIP);
-	if (l2fw_rule)
-		return MV_OK;
-
-	l2fw_rule = (L2FW_RULE *)mvOsMalloc(sizeof(L2FW_RULE));
-	if (!l2fw_rule) {
-		mvOsPrintf("%s: OOM\n", __func__);
-		return MV_FAIL;
-	}
-#ifdef CONFIG_MV_ETH_L2FW_DEBUG
-	mvOsPrintf("adding a rule to l2fw hash in %s\n", __func__);
-#endif
-	l2fw_rule->srcIP = srcIP;
-	l2fw_rule->dstIP = dstIP;
-	l2fw_rule->port = port;
-
-	l2fw_rule->next = l2fw_hash[hash];
-	l2fw_hash[hash] = l2fw_rule;
-	numHashEntries++;
-    return MV_OK;
-}
-
-MV_STATUS l2fw_add_ip(const char *buf)
-{
-	char *addr1, *addr2;
-	L2FW_RULE *l2fw_rule;
-	MV_U32 srcIP;
-	MV_U32 dstIP;
-	MV_U8	  *srcIPchr, *dstIPchr;
-	char dest1[15];
-	char dest2[15];
-	char *portStr;
-	int offset1, offset2, port;
-	MV_U32 hash    = 0;
-	if (numHashEntries == L2FW_HASH_SIZE) {
-		printk(KERN_INFO "cannot add entry, hash table is full, there are %d entires \n", L2FW_HASH_SIZE);
-		return MV_ERROR;
-	}
-
-	memset(dest1,   0, sizeof(dest1));
-	memset(dest2,   0, sizeof(dest2));
-
-	addr1 = strchr(buf, ',');
-	addr2 =	strchr(addr1+1, ',');
-	offset1 = addr1-buf;
-	offset2 = addr2-addr1;
-	if (!addr1) {
-			printk(KERN_INFO "first separating comma (',') missing in input in %s\n", __func__);
-			return MV_FAIL;
-	}
-	if (!addr2) {
-			printk(KERN_INFO "second separating comma (',') missing in input in %s\n", __func__);
-			return MV_FAIL;
-	}
-
-	strncpy(dest1, buf, addr1-buf);
-	srcIP = in_aton(dest1);
-	strncpy(dest2, buf+offset1+1, addr2-addr1-1);
-	dstIP = in_aton(dest2);
-	srcIPchr = (MV_U8 *)&(srcIP);
-	dstIPchr = (MV_U8 *)&(dstIP);
-	portStr = addr2+1;
-	if (*portStr == 'D') {
-		L2FW_RULE *l2fw_rule_to_del, *prev;
-		hash = mv_jhash_3words(srcIP, dstIP, (MV_U32) 0, l2fw_jhash_iv);
-		hash &= L2FW_HASH_MASK;
-		l2fw_rule_to_del = l2fw_hash[hash];
-		prev = NULL;
-
-		while (l2fw_rule_to_del) {
-		if ((l2fw_rule_to_del->srcIP == srcIP) &&
-			(l2fw_rule_to_del->dstIP == dstIP)) {
-			if (prev)
-				prev->next = l2fw_rule_to_del->next;
-			else
-				l2fw_hash[hash] = l2fw_rule_to_del->next;
-			mvOsPrintf("%u.%u.%u.%u->%u.%u.%u.%u deleted\n", MV_IPQUAD(srcIPchr), MV_IPQUAD(dstIPchr));
-			mvOsFree(l2fw_rule_to_del);
-			numHashEntries--;
-			return MV_OK;
-		}
-
-		prev = l2fw_rule_to_del;
-		l2fw_rule_to_del = l2fw_rule_to_del->next;
-	}
-		mvOsPrintf("%u.%u.%u.%u->%u.%u.%u.%u : entry not found\n", MV_IPQUAD(srcIPchr), MV_IPQUAD(dstIPchr));
-		return MV_NOT_FOUND;
-	}
-
-	port = atoi(portStr);
-	hash = mv_jhash_3words(srcIP, dstIP, (MV_U32) 0, l2fw_jhash_iv);
-	hash &= L2FW_HASH_MASK;
-
-	l2fw_rule = l2fw_lookup(srcIP, dstIP);
 	if (l2fw_rule) {
-		mvOsPrintf("%u.%u.%u.%u->%u.%u.%u.%u : entry already exist\n",
-				MV_IPQUAD(srcIPchr), MV_IPQUAD(dstIPchr));
+		/* overwite port */
+		l2fw_rule->port = port;
 		return MV_OK;
 	}
 
@@ -293,16 +195,8 @@ MV_STATUS l2fw_add_ip(const char *buf)
 	l2fw_hash[hash] = l2fw_rule;
 	numHashEntries++;
     return MV_OK;
-
 }
 
-void l2fw_esp_show(void)
-{
-	if (espEnabled)
-		printk(KERN_INFO "ESP is enabled in %s\n", __func__);
-	else
-		printk(KERN_INFO "ESP is not enabled in %s\n", __func__);
-}
 
 #ifdef CONFIG_MV_INCLUDE_XOR
 static void dump_xor(void)
@@ -334,25 +228,11 @@ static void dump_xor(void)
 #endif
 
 
-/* L2fw defines */
-#define L2FW_DISABLE				0
-#define TX_AS_IS					1
-#define SWAP_MAC					2
-#define COPY_AND_SWAP		        3
-
-#define XOR_CAUSE_DONE_MASK(chan) ((BIT0|BIT1) << (chan * 16))
-
-static int         l2fw_xor_threshold = 200;
-static MV_XOR_DESC *eth_xor_desc = NULL;
-static MV_LONG      eth_xor_desc_phys_addr;
-
-
 static int mv_eth_poll_l2fw(struct napi_struct *napi, int budget)
 {
 	int rx_done = 0;
 	MV_U32 causeRxTx;
 	struct eth_port *pp = MV_ETH_PRIV(napi->dev);
-	read_lock(&pp->rwlock);
 
 	STAT_INFO(pp->stats.poll[smp_processor_id()]++);
 
@@ -373,7 +253,7 @@ static int mv_eth_poll_l2fw(struct napi_struct *napi, int budget)
 		MV_REG_WRITE(NETA_INTR_MISC_CAUSE_REG(pp->port), 0);
 	}
 
-	causeRxTx |= pp->causeRxTx[smp_processor_id()];
+	causeRxTx |= pp->cpu_config[smp_processor_id()]->causeRxTx;
 #ifdef CONFIG_MV_ETH_TXDONE_ISR
 	if (causeRxTx & MV_ETH_TXDONE_INTR_MASK) {
 		/* TX_DONE process */
@@ -420,21 +300,25 @@ static int mv_eth_poll_l2fw(struct napi_struct *napi, int budget)
 
 		local_irq_restore(flags);
 	}
-	pp->causeRxTx[smp_processor_id()] = causeRxTx;
-
-	read_unlock(&pp->rwlock);
+	pp->cpu_config[smp_processor_id()]->causeRxTx = causeRxTx;
 
 	return rx_done;
 }
 
 
-void mv_eth_set_l2fw(int cmd, int rx_port, int out_tx_port)
+void mv_eth_set_l2fw(struct eth_port_l2fw *ppl2fw, int cmd, int rx_port, int tx_port)
 {
 	struct eth_port *pp;
 	struct net_device *dev;
 	int group;
 
-	pp     = mv_eth_ports[rx_port];
+#ifndef CONFIG_MV_ETH_L2SEC
+	if (cmd == CMD_L2FW_CESA) {
+		mvOsPrintf("Invalid command (%d) - Ipsec is not defined (%s)\n", cmd, __func__);
+		return;
+	}
+#endif
+	pp = mv_eth_ports[rx_port];
 	if (!pp) {
 		mvOsPrintf("pp is NULL in setting L2FW (%s)\n", __func__);
 		return;
@@ -442,7 +326,7 @@ void mv_eth_set_l2fw(int cmd, int rx_port, int out_tx_port)
 
 	dev = pp->dev;
 	if (dev == NULL) {
-		mvOsPrintf("device is NULL in in setting L2FW (%s)\n", __func__);
+		mvOsPrintf("device is NULL in setting L2FW (%s)\n", __func__);
 		return;
 	}
 	if (!test_bit(MV_ETH_F_STARTED_BIT, &(pp->flags))) {
@@ -451,37 +335,41 @@ void mv_eth_set_l2fw(int cmd, int rx_port, int out_tx_port)
 		return;
 	}
 
-	/* when disabling l2fw, and then ifdown/up, we should
-	   enable MV_ETH_F_CONNECT_LINUX_BIT bit so that the port will be started ok.
-	   TBD: remember last state */
+	if (cmd == ppl2fw->cmd) {
+		ppl2fw->txPort = tx_port;
+		return;
+	}
 
-	if (cmd == L2FW_DISABLE)
-		set_bit(MV_ETH_F_CONNECT_LINUX_BIT, &(pp->flags));
-	else
-		clear_bit(MV_ETH_F_CONNECT_LINUX_BIT, &(pp->flags));
+	if ((cmd != CMD_L2FW_DISABLE) && (ppl2fw->cmd != CMD_L2FW_DISABLE) && (ppl2fw->cmd != CMD_L2FW_LAST)) {
+		ppl2fw->txPort = tx_port;
+		ppl2fw->cmd	= cmd;
+		return;
+	}
+
+	/*TODO disconnect from linux in case that command != 0, connact back if cmd == 0
+	 use netif_carrier_on/netif_carrier_off
+	 netif_tx_stop_all_queues/netif_tx_wake_all_queues
+	*/
+
+	ppl2fw->txPort = tx_port;
+	ppl2fw->cmd	= cmd;
 
 	for (group = 0; group < CONFIG_MV_ETH_NAPI_GROUPS; group++) {
-		if (cmd == L2FW_DISABLE) {
-			if (test_bit(MV_ETH_F_STARTED_BIT, &(pp->flags)))
-				napi_disable(pp->napiGroup[group]);
-			netif_napi_del(pp->napiGroup[group]);
-			netif_napi_add(dev, pp->napiGroup[group], mv_eth_poll,
-				pp->weight);
-			if (test_bit(MV_ETH_F_STARTED_BIT, &(pp->flags)))
-				napi_enable(pp->napiGroup[group]);
-		} else {
-			if (test_bit(MV_ETH_F_STARTED_BIT, &(pp->flags)))
-				napi_disable(pp->napiGroup[group]);
-			netif_napi_del(pp->napiGroup[group]);
-			printk(KERN_INFO "pp->weight=%d in %s\n", pp->weight, __func__);
-			netif_napi_add(dev, pp->napiGroup[group], mv_eth_poll_l2fw,
-				pp->weight);
-			if (test_bit(MV_ETH_F_STARTED_BIT, &(pp->flags)))
-				napi_enable(pp->napiGroup[group]);
-			}
+		if (test_bit(MV_ETH_F_STARTED_BIT, &(pp->flags)))
+			napi_disable(pp->napiGroup[group]);
+
+		netif_napi_del(pp->napiGroup[group]);
+
+		if (cmd == CMD_L2FW_DISABLE)
+			netif_napi_add(dev, pp->napiGroup[group], mv_eth_poll, pp->weight);
+		else
+			netif_napi_add(dev, pp->napiGroup[group], mv_eth_poll_l2fw, pp->weight);
+
+		if (test_bit(MV_ETH_F_STARTED_BIT, &(pp->flags)))
+			napi_enable(pp->napiGroup[group]);
 	}
-}
 
+}
 
 static inline struct eth_pbuf *l2fw_swap_mac(struct eth_pbuf *pRxPktInfo)
 {
@@ -503,13 +391,14 @@ static inline void l2fw_copy_mac(struct eth_pbuf *pRxPktInfo,
 					 struct eth_pbuf *pTxPktInfo)
 	{
 	/* copy 30 bytes (start after MH header) */
-    /* 12 for SA + DA */
+	/* 12 for SA + DA */
 	/* 18 for the rest */
 	MV_U16 *pSrc;
 	MV_U16 *pDst;
 	int i;
 	pSrc = (MV_U16 *)(pRxPktInfo->pBuf + pRxPktInfo->offset + MV_ETH_MH_SIZE);
 	pDst = (MV_U16 *)(pTxPktInfo->pBuf + pTxPktInfo->offset + MV_ETH_MH_SIZE);
+
 	/* swap mac SA and DA */
 	for (i = 0; i < 3; i++) {
 		pDst[i]   = pSrc[i+3];
@@ -597,7 +486,7 @@ struct eth_pbuf *eth_l2fw_copy_packet_withXor(struct eth_pbuf *pRxPktInfo)
 	l2fw_copy_mac(pRxPktInfo, pTxPktInfo);
 	mvOsCacheLineFlush(NULL, pTxPktInfo->pBuf + pTxPktInfo->offset);
 
-    /* Update TxPktInfo */
+	/* Update TxPktInfo */
 	pTxPktInfo->bytes = pRxPktInfo->bytes;
 	return pTxPktInfo;
 }
@@ -618,6 +507,7 @@ void setXorDesc(void)
 
     MV_REG_WRITE(XOR_NEXT_DESC_PTR_REG(1, XOR_CHAN(0)), eth_xor_desc_phys_addr);
 	dump_xor();
+	/* TODO mask xor intterupts*/
 }
 #endif
 
@@ -643,22 +533,66 @@ static inline int xorReady(void)
 void l2fw(int cmd, int rx_port, int tx_port)
 {
 	struct eth_port_l2fw *ppl2fw;
+	int max_port = CONFIG_MV_ETH_PORTS_NUM - 1;
 
-	mv_eth_ports_l2fw_num = mvCtrlEthMaxPortGet();
 	ppl2fw = mv_eth_ports_l2fw[rx_port];
-	mvOsPrintf("cmd=%d rx_port=%d tx_port=%d in %s \n",
-				cmd, rx_port, tx_port, __func__);
-	ppl2fw->txPort = tx_port;
-	ppl2fw->cmd	= cmd;
-	mv_eth_set_l2fw(cmd, rx_port, tx_port);
+
+	if ((cmd < CMD_L2FW_DISABLE) || (cmd > CMD_L2FW_LAST)) {
+		mvOsPrintf("Error: invalid command %d\n", cmd);
+		return;
+	}
+
+	if ((rx_port > max_port) || (rx_port < 0)) {
+		mvOsPrintf("Error: invalid rx port %d\n", rx_port);
+		return;
+	}
+
+	if ((tx_port > max_port) || (tx_port < 0)) {
+		mvOsPrintf("Error: invalid tx port %d\n", tx_port);
+		return;
+	}
+
+	pr_info("cmd=%d rx_port=%d tx_port=%d in %s\n", cmd, rx_port, tx_port, __func__);
+
+	mv_eth_set_l2fw(ppl2fw, cmd, rx_port, tx_port);
 }
 
-void l2fw_xor(int threshold)
+void l2fw_xor(int rx_port, int threshold)
 {
-	mvOsPrintf("setting threshold to %d in %s\n", threshold, __func__);
-	l2fw_xor_threshold = threshold;
+	int max_port = CONFIG_MV_ETH_PORTS_NUM - 1;
+
+	if (rx_port > max_port) {
+		mvOsPrintf("Error: invalid rx port %d\n", rx_port);
+		return;
+	}
+
+	mvOsPrintf("setting port %d threshold to %d in %s\n", rx_port, threshold, __func__);
+	mv_eth_ports_l2fw[rx_port]->xorThreshold = threshold;
 }
 
+void l2fw_lookupEn(int rx_port, int enable)
+{
+	int max_port = CONFIG_MV_ETH_PORTS_NUM - 1;
+
+	if (rx_port > max_port) {
+		mvOsPrintf("Error: invalid rx port %d\n", rx_port);
+		return;
+	}
+	mvOsPrintf("setting port %d lookup mode to %s in %s\n", rx_port, (enable == 1) ? "enable" : "disable", __func__);
+	mv_eth_ports_l2fw[rx_port]->lookupEn = enable;
+}
+
+void l2fw_stats(void)
+{
+	int i;
+	for (i = 0; i < CONFIG_MV_ETH_PORTS_NUM; i++) {
+		mvOsPrintf("number of errors in port[%d]=%d\n", i, mv_eth_ports_l2fw[i]->statErr);
+		mvOsPrintf("number of drops  in port[%d]=%d\n", i, mv_eth_ports_l2fw[i]->statDrop);
+	}
+#ifdef CONFIG_MV_ETH_L2SEC
+	mv_l2sec_stats();
+#endif
+}
 
 static inline MV_STATUS mv_eth_l2fw_tx(struct eth_pbuf *pkt, struct eth_port *pp, int withXor,
 									   struct neta_rx_desc *rx_desc)
@@ -666,14 +600,16 @@ static inline MV_STATUS mv_eth_l2fw_tx(struct eth_pbuf *pkt, struct eth_port *pp
 	struct neta_tx_desc *tx_desc;
 	u32 tx_cmd = 0;
 	struct tx_queue *txq_ctrl;
+	unsigned long flags = 0;
+
 	/* assigning different txq for each rx port , to avoid waiting on the
 	same txq lock when traffic on several rx ports are destined to the same
 	outgoing interface */
-	int txq = pp->txq[smp_processor_id()];
-	read_lock(&pp->rwlock);
+	int txq = pp->cpu_config[smp_processor_id()]->txq;
+
 	txq_ctrl = &pp->txq_ctrl[pp->txp * CONFIG_MV_ETH_TXQ + txq];
 
-	mv_eth_lock(txq_ctrl, pp);
+	mv_eth_lock(txq_ctrl, flags);
 
 	if (txq_ctrl->txq_count >= mv_ctrl_txdone)
 		mv_eth_txq_done(pp, txq_ctrl);
@@ -681,9 +617,9 @@ static inline MV_STATUS mv_eth_l2fw_tx(struct eth_pbuf *pkt, struct eth_port *pp
 	tx_desc = mv_eth_tx_desc_get(txq_ctrl, 1);
 	if (tx_desc == NULL) {
 
-		mv_eth_unlock(txq_ctrl, pp);
+		mv_eth_unlock(txq_ctrl, flags);
 
-		read_unlock(&pp->rwlock);
+		/*read_unlock(&pp->rwlock);*/
 		/* No resources: Drop */
 		pp->dev->stats.tx_dropped++;
 		if (withXor)
@@ -692,8 +628,13 @@ static inline MV_STATUS mv_eth_l2fw_tx(struct eth_pbuf *pkt, struct eth_port *pp
 	}
 	txq_ctrl->txq_count++;
 
+#ifdef CONFIG_MV_ETH_BM_CPU
 	tx_cmd |= NETA_TX_BM_ENABLE_MASK | NETA_TX_BM_POOL_ID_MASK(pkt->pool);
 	txq_ctrl->shadow_txq[txq_ctrl->shadow_txq_put_i] = (u32) NULL;
+#else
+	txq_ctrl->shadow_txq[txq_ctrl->shadow_txq_put_i] = (u32) pkt;
+#endif /* CONFIG_MV_ETH_BM_CPU */
+
 	mv_eth_shadow_inc_put(txq_ctrl);
 
 	tx_desc->command = tx_cmd | NETA_TX_L4_CSUM_NOT |
@@ -710,17 +651,15 @@ static inline MV_STATUS mv_eth_l2fw_tx(struct eth_pbuf *pkt, struct eth_port *pp
 		if (!xorReady()) {
 			mvOsPrintf("MV_DROPPED in %s\n", __func__);
 
-			mv_eth_unlock(txq_ctrl, pp);
+			mv_eth_unlock(txq_ctrl, flags);
 
-			read_unlock(&pp->rwlock);
+			/*read_unlock(&pp->rwlock);*/
 			return MV_DROPPED;
 		}
 	}
 	mvNetaTxqPendDescAdd(pp->port, pp->txp, txq, 1);
 
-	mv_eth_unlock(txq_ctrl, pp);
-
-	read_unlock(&pp->rwlock);
+	mv_eth_unlock(txq_ctrl, flags);
 
 	return MV_OK;
 }
@@ -808,79 +747,96 @@ static inline int mv_eth_l2fw_rx(struct eth_port *pp, int rx_todo, int rxq)
 			srcIP = (MV_U8 *)&(pIph->srcIP);
 			dstIP = (MV_U8 *)&(pIph->dstIP);
 			printk(KERN_INFO "%u.%u.%u.%u->%u.%u.%u.%u in %s\n", MV_IPQUAD(srcIP), MV_IPQUAD(dstIP), __func__);
+			printk(KERN_INFO "0x%x->0x%x in %s\n", pIph->srcIP, pIph->dstIP, __func__);
 		} else
 			printk(KERN_INFO "pIph is NULL in %s\n", __func__);
 #endif
-		if (espEnabled)
-			new_pp  = mv_eth_ports[ppl2fw->txPort];
-		else {
-			 l2fw_rule = l2fw_lookup(pIph->srcIP, pIph->dstIP);
 
-			 if (!l2fw_rule) {
+		if (ppl2fw->lookupEn) {
+			l2fw_rule = l2fw_lookup(pIph->srcIP, pIph->dstIP);
+
+			if (!l2fw_rule) {
+
 #ifdef CONFIG_MV_ETH_L2FW_DEBUG
 				printk(KERN_INFO "l2fw_lookup() failed in %s\n", __func__);
 #endif
-				mv_eth_rxq_refill(pp, rxq, pkt, pool, rx_desc);
-				continue;
-			 }
 
-#ifdef CONFIG_MV_ETH_L2FW_DEBUG
-				printk(KERN_INFO "l2fw_lookup() is ok l2fw_rule->port=%d in %s\n", l2fw_rule->port, __func__);
-#endif
-			new_pp  = mv_eth_ports[l2fw_rule->port];
-			}
+				new_pp  = mv_eth_ports[ppl2fw->txPort];
+			} else
+				new_pp  = mv_eth_ports[l2fw_rule->port];
+		} else
+			new_pp  = mv_eth_ports[ppl2fw->txPort];
 
 		switch (ppl2fw->cmd) {
-		case TX_AS_IS:
-#ifdef CONFIG_MV_ETH_L2SEC
-					if (espEnabled) {
-						status = handleEsp(pkt, rx_desc, new_pp, pp->port);
-					}
+		case CMD_L2FW_AS_IS:
+			status = mv_eth_l2fw_tx(pkt, new_pp, 0, rx_desc);
+			break;
+
+		case CMD_L2FW_SWAP_MAC:
+			mvOsCacheLineInv(NULL, pkt->pBuf + pkt->offset);
+			l2fw_swap_mac(pkt);
+			mvOsCacheLineFlush(NULL, pkt->pBuf+pkt->offset);
+			status = mv_eth_l2fw_tx(pkt, new_pp, 0, rx_desc);
+			break;
+
+		case CMD_L2FW_COPY_SWAP:
+			if (pkt->bytes >= ppl2fw->xorThreshold) {
+				newpkt = eth_l2fw_copy_packet_withXor(pkt);
+				if (newpkt)
+					status = mv_eth_l2fw_tx(newpkt, new_pp, 1, rx_desc);
 				else
-#endif
-					status = mv_eth_l2fw_tx(pkt, new_pp, 0, rx_desc);
-				break;
-
-		case SWAP_MAC:
-				mvOsCacheLineInv(NULL, pkt->pBuf + pkt->offset);
-				l2fw_swap_mac(pkt);
-				mvOsCacheLineFlush(NULL, pkt->pBuf+pkt->offset);
-				status = mv_eth_l2fw_tx(pkt, new_pp, 0, rx_desc);
-				break;
-
-		case COPY_AND_SWAP:
-				if (pkt->bytes >= l2fw_xor_threshold) {
-					newpkt = eth_l2fw_copy_packet_withXor(pkt);
+					status = MV_ERROR;
+			} else {
+					newpkt = eth_l2fw_copy_packet_withoutXor(pkt);
 					if (newpkt)
-						status = mv_eth_l2fw_tx(newpkt, new_pp, 1, rx_desc);
+						status = mv_eth_l2fw_tx(newpkt, new_pp, 0, rx_desc);
 					else
 						status = MV_ERROR;
-				} else {
-						newpkt = eth_l2fw_copy_packet_withoutXor(pkt);
-						if (newpkt)
-							status = mv_eth_l2fw_tx(newpkt, new_pp, 0, rx_desc);
-						else
-							status = MV_ERROR;
-				}
-		}
+			}
+			break;
+#ifdef CONFIG_MV_ETH_L2SEC
+		case CMD_L2FW_CESA:
+			status = mv_l2sec_handle_esp(pkt, rx_desc, new_pp, pp->port);
+			break;
+#endif
+		default:
+			pr_err("WARNING:in %s invalid mode %d for rx port %d\n",
+				__func__, ppl2fw->cmd, pp->port);
+			mv_eth_rxq_refill(pp, rxq, pkt, pool, rx_desc);
+		} /*of switch*/
+
 		if (status == MV_OK) {
-			mvOsCacheLineInv(NULL, rx_desc);
+			if (mv_eth_pool_bm(pool)) {
+				/* BM - no refill */
+				mvOsCacheLineInv(NULL, rx_desc);
+			} else {
+				if (mv_eth_refill(pp, rxq, NULL, pool, rx_desc)) {
+					printk(KERN_ERR "%s: Linux processing - Can't refill\n", __func__);
+					pp->rxq_ctrl[rxq].missed++;
+				}
+			}
 			/* we do not need the pkt , we do not do anything with it*/
-			if  ((ppl2fw->cmd	== COPY_AND_SWAP) && !(espEnabled))
+			if  (ppl2fw->cmd == CMD_L2FW_COPY_SWAP)
 				mv_eth_pool_put(pool, pkt);
+
 			continue;
+
 		} else if (status == MV_DROPPED) {
+			ppl2fw->statDrop++;
 			mv_eth_rxq_refill(pp, rxq, pkt, pool, rx_desc);
-			if ((ppl2fw->cmd	== COPY_AND_SWAP) && !(espEnabled))
+			if (ppl2fw->cmd == CMD_L2FW_COPY_SWAP)
 				mv_eth_pool_put(pool, newpkt);
 
 			continue;
+
 		} else if (status == MV_ERROR) {
-			printk(KERN_INFO "MV_ERROR in %s\n", __func__);
+			ppl2fw->statErr++;
 			mv_eth_rxq_refill(pp, rxq, pkt, pool, rx_desc);
 		}
 
+
 	} /* of while */
+
 	/* Update RxQ management counters */
 	mvOsCacheIoSync();
 
@@ -897,6 +853,7 @@ int __devinit mv_l2fw_init(void)
 	MV_U32 regVal;
 	mv_eth_ports_l2fw_num = mvCtrlEthMaxPortGet();
 	mvOsPrintf("in %s: mv_eth_ports_l2fw_num=%d\n", __func__, mv_eth_ports_l2fw_num);
+
 	size = mv_eth_ports_l2fw_num * sizeof(struct eth_port_l2fw *);
 	mv_eth_ports_l2fw = mvOsMalloc(size);
 	if (!mv_eth_ports_l2fw)
@@ -907,8 +864,12 @@ int __devinit mv_l2fw_init(void)
 			mvOsMalloc(sizeof(struct eth_port_l2fw));
 		if (!mv_eth_ports_l2fw[port])
 			goto oom1;
-		mv_eth_ports_l2fw[port]->cmd    = L2FW_DISABLE;
+		mv_eth_ports_l2fw[port]->cmd    = CMD_L2FW_LAST/*CMD_L2FW_DISABLE*/;
 		mv_eth_ports_l2fw[port]->txPort = -1;
+		mv_eth_ports_l2fw[port]->lookupEn = 0;
+		mv_eth_ports_l2fw[port]->xorThreshold = XOR_THRESHOLD_DEF;
+		mv_eth_ports_l2fw[port]->statErr = 0;
+		mv_eth_ports_l2fw[port]->statDrop = 0;
 	}
 
 	bytes = sizeof(L2FW_RULE *) * L2FW_HASH_SIZE;
@@ -925,7 +886,7 @@ int __devinit mv_l2fw_init(void)
 	mvOsPrintf("L2FW hash init %d entries, %d bytes\n", L2FW_HASH_SIZE, bytes);
 	regVal = 0;
 #ifdef CONFIG_MV_ETH_L2SEC
-	cesa_init();
+	mv_l2sec_cesa_init();
 #endif
 
 #ifdef CONFIG_MV_INCLUDE_XOR
diff --git a/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/l2fw/mv_eth_l2fw.h b/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/l2fw/mv_eth_l2fw.h
index c25ef58..5d03e39 100644
--- a/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/l2fw/mv_eth_l2fw.h
+++ b/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/l2fw/mv_eth_l2fw.h
@@ -1,4 +1,30 @@
-/* l2fw/mv_eth_l2fw.h */
+/*******************************************************************************
+Copyright (C) Marvell International Ltd. and its affiliates
+
+This software file (the "File") is owned and distributed by Marvell
+International Ltd. and/or its affiliates ("Marvell") under the following
+alternative licensing terms.  Once you have made an election to distribute the
+File under one of the following license alternatives, please (i) delete this
+introductory statement regarding license alternatives, (ii) delete the two
+license alternatives that you have not elected to use and (iii) preserve the
+Marvell copyright notice above.
+
+
+********************************************************************************
+Marvell GPL License Option
+
+If you received this File from Marvell, you may opt to use, redistribute and/or
+modify this File in accordance with the terms and conditions of the General
+Public License Version 2, June 1991 (the "GPL License"), a copy of which is
+available along with the File in the license.txt file or by writing to the Free
+Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 or
+on the worldwide web at http://www.gnu.org/licenses/gpl.txt.
+
+THE FILE IS DISTRIBUTED AS-IS, WITHOUT WARRANTY OF ANY KIND, AND THE IMPLIED
+WARRANTIES OF MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE ARE EXPRESSLY
+DISCLAIMED.  The GPL License provides additional details about this warranty
+disclaimer.
+*******************************************************************************/
 
 #ifndef L2FW_MV_ETH_L2FW_H
 #define L2FW_MV_ETH_L2FW_H
@@ -7,11 +33,27 @@
 #include "mv_neta/net_dev/mv_netdev.h"
 
 #define	L2FW_HASH_SIZE   (1 << 17)
-extern int espEnabled;
+#define	L2FW_HASH_MASK   (L2FW_HASH_SIZE - 1)
+
+/* L2fw defines */
+#define CMD_L2FW_DISABLE			0
+#define CMD_L2FW_AS_IS				1
+#define CMD_L2FW_SWAP_MAC			2
+#define CMD_L2FW_COPY_SWAP			3
+#define CMD_L2FW_CESA				4
+#define CMD_L2FW_LAST				5
+
+#define XOR_CAUSE_DONE_MASK(chan) ((BIT0|BIT1) << (chan * 16))
+#define XOR_THRESHOLD_DEF			2000;
 
 struct eth_port_l2fw {
 	int cmd;
+	int lookupEn;
+	int xorThreshold;
 	int txPort;
+	/* stats */
+	int statErr;
+	int statDrop;
 };
 
 typedef struct l2fw_rule {
@@ -21,20 +63,14 @@ typedef struct l2fw_rule {
 	struct l2fw_rule *next;
 } L2FW_RULE;
 
-
+MV_STATUS l2fw_add(MV_U32 srcIP, MV_U32 dstIP, int port);
 
 void l2fw(int cmd, int rx_port, int tx_port);
-void l2fw_xor(int threshold);
-MV_STATUS l2fw_add(MV_U32 srcIP, MV_U32 dstIP, int port);
-MV_STATUS l2fw_add_ip(const char *buf);
-void l2fw_esp_show(void);
-void l2fw_esp_set(int enableEsp);
+void l2fw_xor(int rx_port, int threshold);
+void l2fw_lookupEn(int rx_port, int enable);
 void l2fw_flush(void);
-void l2fw_dump(void);
-void l2fw_show_numHashEntries(void);
+void l2fw_rules_dump(void);
+void l2fw_ports_dump(void);
 void l2fw_stats(void);
-void l2fw_mode_show(void);
-void l2fw_mode(int mode);
-
 
 #endif
diff --git a/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/l2fw/mv_eth_l2sec.c b/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/l2fw/mv_eth_l2sec.c
index 09ab2e7..4bc6c24 100644
--- a/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/l2fw/mv_eth_l2sec.c
+++ b/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/l2fw/mv_eth_l2sec.c
@@ -1,12 +1,105 @@
-/* mv_eth_l2sec.c */
-
+/*******************************************************************************
+Copyright (C) Marvell International Ltd. and its affiliates
+
+This software file (the "File") is owned and distributed by Marvell
+International Ltd. and/or its affiliates ("Marvell") under the following
+alternative licensing terms.  Once you have made an election to distribute the
+File under one of the following license alternatives, please (i) delete this
+introductory statement regarding license alternatives, (ii) delete the two
+license alternatives that you have not elected to use and (iii) preserve the
+Marvell copyright notice above.
+
+
+********************************************************************************
+Marvell GPL License Option
+
+If you received this File from Marvell, you may opt to use, redistribute and/or
+modify this File in accordance with the terms and conditions of the General
+Public License Version 2, June 1991 (the "GPL License"), a copy of which is
+available along with the File in the license.txt file or by writing to the Free
+Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 or
+on the worldwide web at http://www.gnu.org/licenses/gpl.txt.
+
+THE FILE IS DISTRIBUTED AS-IS, WITHOUT WARRANTY OF ANY KIND, AND THE IMPLIED
+WARRANTIES OF MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE ARE EXPRESSLY
+DISCLAIMED.  The GPL License provides additional details about this warranty
+disclaimer.
+*******************************************************************************/
+
+#include "mvOs.h"
+#include  <linux/interrupt.h>
+#include "ctrlEnv/mvCtrlEnvLib.h"
+#include "mv_neta/net_dev/mv_netdev.h"
 #include "mv_eth_l2sec.h"
+#include "mv_eth_l2fw.h"
+#include "mvDebug.h"
+#include "gbe/mvNetaRegs.h"
+#include <linux/spinlock.h>
+
+static struct tasklet_struct l2sec_tasklet;
+static MV_L2FW_SEC_CESA_PRIV *req_array[MV_L2FW_SEC_REQ_Q_SIZE];
+unsigned int req_empty;
+unsigned int req_ready;
+atomic_t req_count;
+spinlock_t cesa_lock[CESA_CHAN];
+
+static int cesaChanPort[CONFIG_MV_ETH_PORTS_NUM];
+
+static MV_BUF_INFO *pBufInfoArray[CESA_CHAN];
+
+static int cesaPrivIndx[CESA_CHAN];
+static int cesaCmdIndx[CESA_CHAN];
+int cesaFullResBuf[CESA_CHAN];
+
+MV_L2FW_SEC_CESA_PRIV *cesaPrivArray[CESA_CHAN];
+MV_CESA_COMMAND *cesaCmdArray[CESA_CHAN];
+static MV_CESA_MBUF *cesaMbufArray[CESA_CHAN];
+void *cesaOSHandle;
+
+
+static MV_L2FW_SEC_SA_ENTRY sa;
+
+
+#define MALLOC_AND_CLEAR(_ptr_, _size_) {\
+	(_ptr_) = mvOsMalloc(_size_);\
+	if ((_ptr_) == NULL) {\
+		mvOsPrintf("Can't allocate %d bytes of memory\n", (_size_));\
+		return;\
+	 } \
+	memset((_ptr_), 0, (_size_));\
+}
+
 
-static inline MV_STATUS mv_eth_cesa_l2fw_tx(struct eth_pbuf *pkt, struct eth_port *pp)
+void mv_l2sec_stats()
+{
+	int chan;
+	for (chan = 0; chan < CESA_CHAN; chan++)
+		printk(KERN_INFO "number of l2sec channel %d full result buffer events = %d\n", chan, cesaFullResBuf[chan]);
+}
+
+void printEspHdr(MV_ESP_HEADER *pEspHdr)
+{
+	printk(KERN_INFO "pEspHdr->spi=%d in %s\n"  , pEspHdr->spi, __func__);
+	printk(KERN_INFO "pEspHdr->seqNum=%d in %s\n", pEspHdr->seqNum, __func__);
+}
+
+void printIpHdr(MV_IP_HEADER *pIpHdr)
+{
+	MV_U8	  *srcIP, *dstIP;
+
+	srcIP = (MV_U8 *)&(pIpHdr->srcIP);
+	dstIP = (MV_U8 *)&(pIpHdr->dstIP);
+
+	pr_info("%u.%u.%u.%u->%u.%u.%u.%u in %s\n", MV_IPQUAD(srcIP), MV_IPQUAD(dstIP), __func__);
+	pr_info("MV_16BIT_BE(pIpHdr->totalLength)=%d  in %s\n", MV_16BIT_BE(pIpHdr->totalLength), __func__);
+	pr_info("pIpHdr->protocol=%d\n", pIpHdr->protocol);
+}
+
+static inline MV_STATUS mv_eth_l2sec_tx(struct eth_pbuf *pkt, struct eth_port *pp)
 {
 	struct neta_tx_desc *tx_desc;
-	u32 tx_cmd = 0;
 	struct tx_queue *txq_ctrl;
+	int l3_status;
 
 	/* assigning different txq for each rx port , to avoid waiting on the
 	same txq lock when traffic on several rx ports are destined to the same
@@ -14,105 +107,103 @@ static inline MV_STATUS mv_eth_cesa_l2fw_tx(struct eth_pbuf *pkt, struct eth_por
 	int txq = 0;
 	txq_ctrl = &pp->txq_ctrl[pp->txp * CONFIG_MV_ETH_TXQ + txq];
 
-	mv_eth_lock(txq_ctrl, pp);
-
 	if (txq_ctrl->txq_count >= mv_ctrl_txdone)
 		mv_eth_txq_done(pp, txq_ctrl);
+
 	/* Get next descriptor for tx, single buffer, so FIRST & LAST */
 	tx_desc = mv_eth_tx_desc_get(txq_ctrl, 1);
-	if (tx_desc == NULL) {
-		/* printk("tx_desc == NULL pp->port=%d in %s\n", pp->port, ,__func__); */
-
-		mv_eth_unlock(txq_ctrl, pp);
 
+	if (tx_desc == NULL) {
 		/* No resources: Drop */
 		pp->dev->stats.tx_dropped++;
 		return MV_DROPPED;
 	}
 	txq_ctrl->txq_count++;
 
+#ifdef CONFIG_MV_ETH_BM_CPU
 	tx_cmd |= NETA_TX_BM_ENABLE_MASK | NETA_TX_BM_POOL_ID_MASK(pkt->pool);
 	txq_ctrl->shadow_txq[txq_ctrl->shadow_txq_put_i] = (u32) NULL;
+#else
+	txq_ctrl->shadow_txq[txq_ctrl->shadow_txq_put_i] = (u32) pkt;
+#endif /* CONFIG_MV_ETH_BM_CPU */
+
 	mv_eth_shadow_inc_put(txq_ctrl);
+	l3_status = (0xE << NETA_TX_L3_OFFSET_OFFS) | NETA_TX_IP_CSUM_MASK | (0x5 << NETA_TX_IP_HLEN_OFFS);
 
-	tx_desc->command = tx_cmd | NETA_TX_L4_CSUM_NOT |
-		NETA_TX_FLZ_DESC_MASK | NETA_TX_F_DESC_MASK
-		| NETA_TX_L_DESC_MASK |
-		NETA_TX_PKT_OFFSET_MASK(pkt->offset + MV_ETH_MH_SIZE);
+	tx_desc->command = l3_status | NETA_TX_L4_CSUM_NOT | NETA_TX_FLZ_DESC_MASK | NETA_TX_F_DESC_MASK
+				| NETA_TX_L_DESC_MASK | NETA_TX_PKT_OFFSET_MASK(pkt->offset + MV_ETH_MH_SIZE);
 
 	tx_desc->dataSize    = pkt->bytes;
 	tx_desc->bufPhysAddr = pkt->physAddr;
 	mv_eth_tx_desc_flush(tx_desc);
 	mvNetaTxqPendDescAdd(pp->port, pp->txp, 0, 1);
 
-	mv_eth_unlock(txq_ctrl, pp);
-
 	return MV_OK;
 }
 
-static inline void nfp_sec_complete_out(unsigned long data)
+static inline void mv_l2sec_complete_out(unsigned long data)
 
 {
-	MV_NFP_SEC_CESA_PRIV_L2FW *nfp_sec_cesa_priv = (MV_NFP_SEC_CESA_PRIV_L2FW *)data;		MV_U32            ifout;
-	MV_PKT_INFO       *pkt;
+	MV_L2FW_SEC_CESA_PRIV *sec_cesa_priv = (MV_L2FW_SEC_CESA_PRIV *)data;
+	MV_U32            ifout;
 	MV_BUF_INFO       *pBuf;
 	struct eth_port   *pp;
 	struct eth_pbuf   *pPkt;
 	int oldOfsset;
 	MV_STATUS status = MV_FAIL;
 	static int counterOfFailed = 0;
-	if (!nfp_sec_cesa_priv) {
-		printk(KERN_INFO "nfp_sec_cesa_priv is NULL in %s\n", __func__);
-		return;
-	}
-	ifout = nfp_sec_cesa_priv->ifout;
 
-	pkt = nfp_sec_cesa_priv->pPktInfo;
-	if (!pkt) {
-		printk(KERN_INFO "pPktInfo is NULL in %s\n", __func__);
+	if (!sec_cesa_priv) {
+		printk(KERN_INFO "sec_cesa_priv is NULL in %s\n", __func__);
 		return;
 	}
-	pBuf = pkt->pFrags;
+	ifout = sec_cesa_priv->ifout;
+
+	pBuf = sec_cesa_priv->pBufInfo;
 	if (!pBuf) {
 		printk(KERN_INFO "pBuf is NULL in %s\n", __func__);
 		return;
 	}
-	pPkt = nfp_sec_cesa_priv->pPkt;
+	pPkt = sec_cesa_priv->pPkt;
 	if (!pPkt) {
 		printk(KERN_INFO "!pPkt) in %s\n", __func__);
 		return;
 	}
 	pPkt->bytes    = pBuf->dataSize;
-	pPkt->bytes += MV_NFP_SEC_ESP_OFFSET;
+	pPkt->bytes   += MV_L2FW_SEC_ESP_OFFSET;
 	oldOfsset      = pPkt->offset;
 	pPkt->offset   = pPkt->offset - (sizeof(MV_ESP_HEADER) + sizeof(MV_IP_HEADER) + MV_CESA_AES_BLOCK_SIZE);
 
 	pp     = mv_eth_ports[ifout];
 
-	status = 	mv_eth_cesa_l2fw_tx(pPkt, pp);
-	if (status == MV_DROPPED)
+	status = mv_eth_l2sec_tx(pPkt, pp);
+
+	pPkt->offset = oldOfsset;
+
+	if (status == MV_DROPPED) {
+		struct bm_pool *pool = &mv_eth_pool[pPkt->pool];
 		counterOfFailed++;
-	 else
-		pPkt->offset = oldOfsset;
+		mv_eth_pool_put(pool, pPkt);
+	 }
 }
 
-int l2fw_set_cesa_chan(int port, int cesaChan)
+int mv_l2sec_set_cesa_chan(int port, int cesaChan)
 {
-//	struct eth_port *pp;
-	printk(KERN_INFO "setting cesaChan to %d for port=%d \n", cesaChan, port);
-	if ((cesaChan != CESA_0) && (cesaChan != CESA_1))  {
-		printk(KERN_INFO "non permitted value for CESA channel \n");
+	if (cesaChan > (MV_CESA_CHANNELS - 1)) {
+		pr_info("non permitted value for CESA channel\n");
 		return -EINVAL;
 	}
-//	pp = mv_eth_ports[port];
-//	if (pp)
-//		pp->cesaChan = cesaChan;
+
+	pr_info("setting cesaChan to %d for port=%d\n", cesaChan, port);
+
 	cesaChanPort[port] = cesaChan;
+
 	return 0;
 }
 
 MV_STATUS my_mvSysCesaInit(int numOfSession, int queueDepth, void *osHandle)
 {
+
 	MV_CESA_HAL_DATA halData;
 	MV_UNIT_WIN_INFO addrWinMap[MAX_TARGETS + 1];
 	MV_STATUS status;
@@ -137,262 +228,156 @@ MV_STATUS my_mvSysCesaInit(int numOfSession, int queueDepth, void *osHandle)
 		halData.ctrlRev = mvCtrlRevGet();
 			status = mvCesaHalInit(numOfSession, queueDepth,
 					osHandle, &halData);
-	}
+		}
 	}
 
 	return status;
+
 }
 
-void cesaStart(void)
+void mv_l2sec_cesa_start(void)
 {
-	int bufNum, bufSize;
-	int i, j, idx;
-	MV_CESA_MBUF *pMbufSrc_0, *pMbufDst_0;
-	MV_BUF_INFO *pFragsSrc_0, *pFragsDst_0;
-	char *pBuf_0;
-
-	MV_CESA_MBUF *pMbufSrc_1, *pMbufDst_1;
-	MV_BUF_INFO *pFragsSrc_1, *pFragsDst_1;
-	char *pBuf_1;
-
-	printk(KERN_INFO "in %s\n", __func__);
-
-	cesaCmdArray_0 = 	mvOsMalloc(sizeof(MV_CESA_COMMAND) * CESA_DEF_REQ_SIZE);
-
-	if (cesaCmdArray_0 == NULL) {
-		mvOsPrintf("Can't allocate %d bytes of memory\n",
-			   (int)(sizeof(MV_CESA_COMMAND) * CESA_DEF_REQ_SIZE));
-		return;
-	}
-	memset(cesaCmdArray_0, 0, sizeof(MV_CESA_COMMAND) * CESA_DEF_REQ_SIZE);
-	/* CESA_DEF_BUF_NUM */
-	bufNum    =  1;
-	/* CESA_DEF_BUF_SIZE */
-	bufSize   = 1500;
-
-	pMbufSrc_0  = mvOsMalloc(sizeof(MV_CESA_MBUF) * CESA_DEF_REQ_SIZE);
-	pFragsSrc_0 = mvOsMalloc(sizeof(MV_BUF_INFO) * bufNum * CESA_DEF_REQ_SIZE);
-
-	pMbufDst_0  = mvOsMalloc(sizeof(MV_CESA_MBUF) * CESA_DEF_REQ_SIZE);
-	pFragsDst_0 = mvOsMalloc(sizeof(MV_BUF_INFO) * bufNum * CESA_DEF_REQ_SIZE);
-
-	if ((pMbufSrc_0 == NULL) || (pFragsSrc_0 == NULL) ||
-		(pMbufDst_0 == NULL) || (pFragsDst_0 == NULL)) {
-		mvOsPrintf(" Can't malloc Src and Dst pMbuf and pFrags structures.\n");
-		return;
-	}
-
-	memset(pMbufSrc_0,  0, sizeof(MV_CESA_MBUF) * CESA_DEF_REQ_SIZE);
-	memset(pFragsSrc_0, 0, sizeof(MV_BUF_INFO) * bufNum * CESA_DEF_REQ_SIZE);
-
-	memset(pMbufDst_0,  0, sizeof(MV_CESA_MBUF) * CESA_DEF_REQ_SIZE);
-	memset(pFragsDst_0, 0, sizeof(MV_BUF_INFO) * bufNum * CESA_DEF_REQ_SIZE);
-
-	idx = 0;
-	for (i = 0; i < CESA_DEF_REQ_SIZE; i++) {
-		pBuf_0 = mvOsIoCachedMalloc(cesaOSHandle, bufSize * bufNum * 2,
-					  &cesaBufs_0[i].bufPhysAddr, &cesaBufs_0[i].memHandle);
-		if (pBuf_0 == NULL) {
-			mvOsPrintf("testStart: Can't malloc %d bytes for pBuf\n", bufSize * bufNum * 2);
-			return;
-		}
-
-		memset(pBuf_0, 0, bufSize * bufNum * 2);
-		mvOsCacheFlush(cesaOSHandle, pBuf_0, bufSize * bufNum * 2);
-		if (pBuf_0 == NULL) {
-			mvOsPrintf("Can't allocate %d bytes for req_%d buffers\n",
-				   bufSize * bufNum * 2, i);
-			return;
-		}
-
-		cesaBufs_0[i].bufVirtPtr = (MV_U8 *) pBuf_0;
-		cesaBufs_0[i].bufSize = bufSize * bufNum * 2;
-
-		cesaCmdArray_0[i].pSrc = &pMbufSrc_0[i];
-		cesaCmdArray_0[i].pSrc->pFrags = &pFragsSrc_0[idx];
-		cesaCmdArray_0[i].pSrc->numFrags = bufNum;
-		cesaCmdArray_0[i].pSrc->mbufSize = 0;
-
-		cesaCmdArray_0[i].pDst = &pMbufDst_0[i];
-		cesaCmdArray_0[i].pDst->pFrags = &pFragsDst_0[idx];
-		cesaCmdArray_0[i].pDst->numFrags = bufNum;
-		cesaCmdArray_0[i].pDst->mbufSize = 0;
-
-		for (j = 0; j < bufNum; j++) {
-			cesaCmdArray_0[i].pSrc->pFrags[j].bufVirtPtr = (MV_U8 *) pBuf_0;
-			cesaCmdArray_0[i].pSrc->pFrags[j].bufSize = bufSize;
-			pBuf_0 += bufSize;
-			cesaCmdArray_0[i].pDst->pFrags[j].bufVirtPtr = (MV_U8 *) pBuf_0;
-
-			cesaCmdArray_0[i].pDst->pFrags[j].bufSize = bufSize;
-			pBuf_0 += bufSize;
-		}
-		idx += bufNum;
-	}
-
-	cesaMbufArray_0 = mvOsMalloc(sizeof(MV_CESA_MBUF) * CESA_DEF_REQ_SIZE);
-	if (cesaMbufArray_0 == NULL) {
-		mvOsPrintf("Can't allocate %d bytes of memory\n",
-			   (int)(sizeof(MV_CESA_MBUF) * CESA_DEF_REQ_SIZE));
-		return;
-	}
-	memset(cesaMbufArray_0, 0, sizeof(MV_CESA_MBUF) * CESA_DEF_REQ_SIZE);
-
-	cesaPrivArray_0 = mvOsMalloc(sizeof(MV_NFP_SEC_CESA_PRIV_L2FW) * (CESA_DEF_REQ_SIZE + MV_NFP_SEC_REQ_Q_SIZE));
-	memset(cesaPrivArray_0, 0, sizeof(MV_NFP_SEC_CESA_PRIV_L2FW) * (CESA_DEF_REQ_SIZE + MV_NFP_SEC_REQ_Q_SIZE));
-
-	/* second engine */
-	cesaCmdArray_1 = 	mvOsMalloc(sizeof(MV_CESA_COMMAND) * CESA_DEF_REQ_SIZE);
 
-	if (cesaCmdArray_1 == NULL) {
-		mvOsPrintf("Can't allocate %d bytes of memory\n",
-			   (int)(sizeof(MV_CESA_COMMAND) * CESA_DEF_REQ_SIZE));
-		return;
-	}
-	memset(cesaCmdArray_1, 0, sizeof(MV_CESA_COMMAND) * CESA_DEF_REQ_SIZE);
-
-	/* CESA_DEF_BUF_NUM */
-	bufNum    =  1;
-	/* CESA_DEF_BUF_SIZE */
-	bufSize   = 1500;
-
-	pMbufSrc_1  = mvOsMalloc(sizeof(MV_CESA_MBUF) * CESA_DEF_REQ_SIZE);
-	pFragsSrc_1 = mvOsMalloc(sizeof(MV_BUF_INFO) * bufNum * CESA_DEF_REQ_SIZE);
-
-	pMbufDst_1  = mvOsMalloc(sizeof(MV_CESA_MBUF) * CESA_DEF_REQ_SIZE);
-	pFragsDst_1 = mvOsMalloc(sizeof(MV_BUF_INFO) * bufNum * CESA_DEF_REQ_SIZE);
-
-	if ((pMbufSrc_1 == NULL) || (pFragsSrc_1 == NULL) || (pMbufDst_1 == NULL)
-		|| (pFragsDst_1 == NULL)) {
-		mvOsPrintf(" Can't malloc Src and Dst pMbuf and pFrags structures.\n");
-		return;
-	}
-
-	memset(pMbufSrc_1,  0, sizeof(MV_CESA_MBUF) * CESA_DEF_REQ_SIZE);
-	memset(pFragsSrc_1, 0, sizeof(MV_BUF_INFO) * bufNum * CESA_DEF_REQ_SIZE);
-
-	memset(pMbufDst_1,  0, sizeof(MV_CESA_MBUF) * CESA_DEF_REQ_SIZE);
-	memset(pFragsDst_1, 0, sizeof(MV_BUF_INFO) * bufNum * CESA_DEF_REQ_SIZE);
-
-	idx = 0;
-	for (i = 0; i < CESA_DEF_REQ_SIZE; i++) {
-		pBuf_1 = mvOsIoCachedMalloc(cesaOSHandle, bufSize * bufNum * 2,
-					  &cesaBufs_1[i].bufPhysAddr, &cesaBufs_1[i].memHandle);
-		if (pBuf_1 == NULL) {
-			mvOsPrintf("testStart: Can't malloc %d bytes for pBuf\n", bufSize * bufNum * 2);
-			return;
-		}
+	MV_CESA_MBUF *pMbufSrc[CESA_CHAN], *pMbufDst[CESA_CHAN];
+	MV_BUF_INFO *pCesaBufs[CESA_CHAN], *pFragsSrc[CESA_CHAN], *pFragsDst[CESA_CHAN];
+	MV_CESA_COMMAND *cesaCmdArrTmp;
+	MV_BUF_INFO *pCesaBufsTmp;
+	int chan;
+	int i, j, idx;
+	char *pBuf;
+
+	for (chan = 0; chan < CESA_CHAN; chan++) {
+		MALLOC_AND_CLEAR(cesaCmdArray[chan], sizeof(MV_CESA_COMMAND) * CESA_DEF_REQ_SIZE);
+		MALLOC_AND_CLEAR(pMbufSrc[chan], sizeof(MV_CESA_MBUF) * CESA_DEF_REQ_SIZE);
+		MALLOC_AND_CLEAR(pFragsSrc[chan], sizeof(MV_BUF_INFO) * L2SEC_CESA_BUF_NUM * CESA_DEF_REQ_SIZE);
+		MALLOC_AND_CLEAR(pMbufDst[chan], sizeof(MV_CESA_MBUF) * CESA_DEF_REQ_SIZE);
+		MALLOC_AND_CLEAR(pFragsDst[chan], sizeof(MV_BUF_INFO) * L2SEC_CESA_BUF_NUM * CESA_DEF_REQ_SIZE);
+		MALLOC_AND_CLEAR(pCesaBufs[chan], sizeof(MV_BUF_INFO) * L2SEC_CESA_BUF_NUM * CESA_DEF_REQ_SIZE);
+
+		idx = 0;
+		pCesaBufsTmp = pCesaBufs[chan];
+		cesaCmdArrTmp = cesaCmdArray[chan];
+
+		for (i = 0; i < CESA_DEF_REQ_SIZE; i++) {
+			pBuf = mvOsIoCachedMalloc(cesaOSHandle, L2SEC_CESA_BUF_SIZE * L2SEC_CESA_BUF_NUM * 2,
+					  &pCesaBufsTmp[i].bufPhysAddr, &pCesaBufsTmp[i].memHandle);
+			if (pBuf == NULL) {
+				mvOsPrintf("testStart: Can't malloc %d bytes for pBuf\n", L2SEC_CESA_BUF_SIZE * L2SEC_CESA_BUF_NUM * 2);
+				return;
+			}
 
-		memset(pBuf_1, 0, bufSize * bufNum * 2);
-		mvOsCacheFlush(cesaOSHandle, pBuf_1, bufSize * bufNum * 2);
-		if (pBuf_1 == NULL) {
-			mvOsPrintf("Can't allocate %d bytes for req_%d buffers\n",
-				   bufSize * bufNum * 2, i);
-			return;
-		}
+			memset(pBuf, 0, L2SEC_CESA_BUF_SIZE * L2SEC_CESA_BUF_NUM * 2);
+			mvOsCacheFlush(cesaOSHandle, pBuf, L2SEC_CESA_BUF_SIZE * L2SEC_CESA_BUF_NUM * 2);
+			if (pBuf == NULL) {
+				mvOsPrintf("Can't allocate %d bytes for req_%d buffers\n",
+						L2SEC_CESA_BUF_SIZE * L2SEC_CESA_BUF_NUM * 2, i);
+				return;
+			}
 
-		cesaBufs_1[i].bufVirtPtr = (MV_U8 *) pBuf_1;
-		cesaBufs_1[i].bufSize = bufSize * bufNum * 2;
+			pCesaBufsTmp[i].bufVirtPtr = (MV_U8 *) pBuf;
+			pCesaBufsTmp[i].bufSize = L2SEC_CESA_BUF_SIZE * L2SEC_CESA_BUF_NUM * 2;
 
-		cesaCmdArray_1[i].pSrc = &pMbufSrc_1[i];
-		cesaCmdArray_1[i].pSrc->pFrags = &pFragsSrc_1[idx];
-		cesaCmdArray_1[i].pSrc->numFrags = bufNum;
-		cesaCmdArray_1[i].pSrc->mbufSize = 0;
+			cesaCmdArrTmp[i].pSrc = &pMbufSrc[chan][i];
+			cesaCmdArrTmp[i].pSrc->pFrags = &pFragsSrc[chan][idx];
+			cesaCmdArrTmp[i].pSrc->numFrags = L2SEC_CESA_BUF_NUM;
+			cesaCmdArrTmp[i].pSrc->mbufSize = 0;
 
-		cesaCmdArray_1[i].pDst = &pMbufDst_1[i];
-		cesaCmdArray_1[i].pDst->pFrags = &pFragsDst_1[idx];
-		cesaCmdArray_1[i].pDst->numFrags = bufNum;
-		cesaCmdArray_1[i].pDst->mbufSize = 0;
+			cesaCmdArrTmp[i].pDst = &pMbufDst[chan][i];
+			cesaCmdArrTmp[i].pDst->pFrags = &pFragsDst[chan][idx];
+			cesaCmdArrTmp[i].pDst->numFrags = L2SEC_CESA_BUF_NUM;
+			cesaCmdArrTmp[i].pDst->mbufSize = 0;
 
-		for (j = 0; j < bufNum; j++) {
-			cesaCmdArray_1[i].pSrc->pFrags[j].bufVirtPtr = (MV_U8 *) pBuf_1;
-			cesaCmdArray_1[i].pSrc->pFrags[j].bufSize = bufSize;
-			pBuf_1 += bufSize;
-			cesaCmdArray_1[i].pDst->pFrags[j].bufVirtPtr = (MV_U8 *) pBuf_1;
+			for (j = 0; j < L2SEC_CESA_BUF_NUM; j++) {
+				cesaCmdArrTmp[i].pSrc->pFrags[j].bufVirtPtr = (MV_U8 *) pBuf;
+				cesaCmdArrTmp[i].pSrc->pFrags[j].bufSize = L2SEC_CESA_BUF_SIZE;
+				pBuf += L2SEC_CESA_BUF_SIZE;
+				cesaCmdArrTmp[i].pDst->pFrags[j].bufVirtPtr = (MV_U8 *) pBuf;
 
-			cesaCmdArray_1[i].pDst->pFrags[j].bufSize = bufSize;
-			pBuf_1 += bufSize;
+				cesaCmdArrTmp[i].pDst->pFrags[j].bufSize = L2SEC_CESA_BUF_SIZE;
+				pBuf += L2SEC_CESA_BUF_SIZE;
+			}
+		idx += L2SEC_CESA_BUF_NUM;
 		}
-		idx += bufNum;
-	}
 
-	cesaMbufArray_1 = mvOsMalloc(sizeof(MV_CESA_MBUF) * CESA_DEF_REQ_SIZE);
-	if (cesaMbufArray_1 == NULL) {
-		mvOsPrintf("Can't allocate %d bytes of memory\n",
-			   (int)(sizeof(MV_CESA_MBUF) * CESA_DEF_REQ_SIZE));
-		return;
-	}
-	memset(cesaMbufArray_1, 0, sizeof(MV_CESA_MBUF) * CESA_DEF_REQ_SIZE);
-
-	cesaPrivArray_1 = mvOsMalloc(sizeof(MV_NFP_SEC_CESA_PRIV_L2FW) * (CESA_DEF_REQ_SIZE + MV_NFP_SEC_REQ_Q_SIZE));
-	memset(cesaPrivArray_1, 0, sizeof(MV_NFP_SEC_CESA_PRIV_L2FW) * (CESA_DEF_REQ_SIZE + MV_NFP_SEC_REQ_Q_SIZE));
+		MALLOC_AND_CLEAR(cesaMbufArray[chan], sizeof(MV_CESA_MBUF) * CESA_DEF_REQ_SIZE);
+		MALLOC_AND_CLEAR(pBufInfoArray[chan], sizeof(MV_BUF_INFO) * MV_L2FW_SEC_REQ_Q_SIZE);
+		MALLOC_AND_CLEAR(cesaPrivArray[chan],
+				sizeof(MV_L2FW_SEC_CESA_PRIV) * (CESA_DEF_REQ_SIZE + MV_L2FW_SEC_REQ_Q_SIZE));
 
-	pPktInfoNewArray_0 = mvOsMalloc(sizeof(MV_PKT_INFO) * MV_NFP_SEC_REQ_Q_SIZE);
+	} /*for chan*/
 
-	if (!pPktInfoNewArray_0) {
-		printk(KERN_INFO "mvOsMalloc() failed in %s\n", __func__);
-		return;
-	}
+	printk(KERN_INFO "start finished in %s\n", __func__);
+}
 
-	pBufInfoArray_0 = mvOsMalloc(sizeof(MV_BUF_INFO) * MV_NFP_SEC_REQ_Q_SIZE);
-	if (!pBufInfoArray_0) {
-		printk(KERN_INFO "could not allocate MV_BUF_INFO in %s\n", __func__);
-		return;
-	}
+/*
+ * nfp sec Interrupt handler routine.
+ */
 
-	pPktInfoNewArray_1 = mvOsMalloc(sizeof(MV_PKT_INFO) * MV_NFP_SEC_REQ_Q_SIZE);
 
-	if (!pPktInfoNewArray_1) {
-		printk(KERN_INFO "mvOsMalloc() failed in %s\n", __func__);
-		return;
-	}
-	pBufInfoArray_1 = mvOsMalloc(sizeof(MV_BUF_INFO) * MV_NFP_SEC_REQ_Q_SIZE);
-	if (!pBufInfoArray_0) {
-		printk(KERN_INFO "could not allocate MV_BUF_INFO in %s\n", __func__);
-		return;
-	}
-	printk(KERN_INFO "start finished in %s\n", __func__);
-}
 
-static irqreturn_t nfp_sec_interrupt_handler_0(int irq, void *arg)
+static irqreturn_t
+mv_l2sec_interrupt_handler(int irq, void *arg)
 {
 	MV_CESA_RESULT  	result;
-	MV_STATUS           status;
-	MV_U8 chan = 0;
+	MV_STATUS               status;
+	int chan;
 
-    MV_REG_WRITE(MV_CESA_ISR_CAUSE_REG(chan), 0);
+	chan = (irq == CESA_IRQ(0)) ? 0 : 1;
 
+	/* clear interrupts */
+	MV_REG_WRITE(MV_CESA_ISR_CAUSE_REG(chan), 0);
+#ifndef CONFIG_MV_CESA_INT_PER_PACKET
 	while (1) {
+#endif
 	/* Get Ready requests */
-
 	status = mvCesaReadyGet(chan, &result);
-	if (status != MV_OK)
-		break;
+	if (status != MV_OK) {
+#ifdef CONFIG_MV_CESA_INT_PER_PACKET
+		printk(KERN_ERR "ERROR: Ready get return %d\n", status);
+		return IRQ_HANDLED;
+#else
+			break;
+#endif
+	}
+	/* handle result */
+	if (atomic_read(&req_count) > (MV_L2FW_SEC_REQ_Q_SIZE - 4)) {
+		/*must take sure that no tx_done will happen on the same time.. */
+		MV_L2FW_SEC_CESA_PRIV *req_priv = (MV_L2FW_SEC_CESA_PRIV *)result.pReqPrv;
+		struct eth_pbuf *pPkt = req_priv->pPkt;
+		struct bm_pool *pool = &mv_eth_pool[pPkt->pool];
+		printk(KERN_ERR "Error: Q request is full - TBD test.\n");
+		mv_eth_pool_put(pool, pPkt);
+		cesaFullResBuf[chan]++;
+		return IRQ_HANDLED;
+	}
 
-	nfp_sec_complete_out((unsigned long)((MV_NFP_SEC_CESA_PRIV_L2FW *)result.pReqPrv));
+	req_array[req_empty] = (MV_L2FW_SEC_CESA_PRIV *)result.pReqPrv;
+	req_empty = (req_empty + 1) % MV_L2FW_SEC_REQ_Q_SIZE;
+	atomic_inc(&req_count);
+#ifndef CONFIG_MV_CESA_INT_PER_PACKET
 	}
+#endif
+	tasklet_hi_schedule(&l2sec_tasklet);
+
 	return IRQ_HANDLED;
 }
 
-static irqreturn_t nfp_sec_interrupt_handler_1(int irq, void *arg)
+void mv_l2sec_req_handler(unsigned long dummy)
 {
-	MV_CESA_RESULT  	result;
-	MV_STATUS           status;
-	MV_U8 chan = 1;
-    MV_REG_WRITE(MV_CESA_ISR_CAUSE_REG(chan), 0);
-	while (1) {
-	/* Get Ready requests */
-	status = mvCesaReadyGet(chan, &result);
-	if (status != MV_OK)
-		break;
+	int req_count_init = atomic_read(&req_count);
+	int counter = req_count_init;
 
-	nfp_sec_complete_out((unsigned long)((MV_NFP_SEC_CESA_PRIV_L2FW *)result.pReqPrv));
+	while (counter != 0) {
+		mv_l2sec_complete_out((unsigned long)req_array[req_ready]);
+		req_ready = (req_ready + 1) % MV_L2FW_SEC_REQ_Q_SIZE;
+		counter--;
 	}
+	atomic_sub(req_count_init, &req_count);
 
-	return IRQ_HANDLED;
 }
 
-void openCesaSession(void)
+
+void mv_l2sec_open_cesa_session(void)
 {
 	unsigned char sha1Key[]  = {0x12, 0x34, 0x56, 0x78, 0x9a, 0xbc, 0xde, 0xf0,
 								0x24, 0x68, 0xac, 0xe0, 0x24, 0x68, 0xac, 0xe0,
@@ -402,17 +387,18 @@ void openCesaSession(void)
 									0x02, 0x46, 0x8a, 0xce, 0x13, 0x57, 0x9b, 0xdf};
 
 	int i;
-	MV_NFP_SEC_SA_ENTRY sa;
+	MV_L2FW_SEC_SA_ENTRY sa;
 	MV_CESA_OPEN_SESSION os;
 	unsigned short digest_size = 0;
-	memset(&sa, 0, sizeof(MV_NFP_SEC_SA_ENTRY));
+	memset(&sa, 0, sizeof(MV_L2FW_SEC_SA_ENTRY));
 	memset(&os, 0, sizeof(MV_CESA_OPEN_SESSION));
 
-	os.operation 		= MV_CESA_MAC_THEN_CRYPTO;
-	os.cryptoAlgorithm  = MV_CESA_CRYPTO_AES;
-	os.macMode  		= MV_CESA_MAC_HMAC_SHA1;
-	digest_size 		= MV_CESA_SHA1_DIGEST_SIZE;
-	os.cryptoMode 		= MV_CESA_CRYPTO_ECB;
+	os.operation       = MV_CESA_MAC_THEN_CRYPTO;
+	os.cryptoAlgorithm = MV_CESA_CRYPTO_AES;
+	os.macMode         = MV_CESA_MAC_HMAC_SHA1;
+	digest_size        = MV_CESA_SHA1_DIGEST_SIZE;
+	os.cryptoMode      = MV_CESA_CRYPTO_CBC;
+
 	for (i = 0; i < sizeof(cryptoKey); i++)
 		os.cryptoKey[i] = cryptoKey[i];
 
@@ -427,76 +413,272 @@ void openCesaSession(void)
 		printk(KERN_INFO "mvCesaSessionOpen failed in %s\n", __func__);
 }
 
-void l2fw_esp_set(int enableEsp)
+static void mv_l2sec_casa_param_init(void)
 {
-	if (enableEsp) {
-		openCesaSession();
-		printk(KERN_INFO "calling cesaStart() in %s\n", __func__);
-		cesaStart();
-	} else
-		printk(KERN_INFO "enableEsp=%d disabling ESP in %s\n", enableEsp, __func__);
-	espEnabled = enableEsp;
+	const u8 da_addr[] = {0xaa, 0xbb, 0xcc, 0xdd, 0xee, 0xff};
+	const u8 sa_addr[] = {0xab, 0xac, 0xad, 0xae, 0xaf, 0xaa};
+
+	memset(&sa, 0, sizeof(MV_L2FW_SEC_SA_ENTRY));
+	sa.digestSize = MV_CESA_SHA1_DIGEST_SIZE;
+	sa.ivSize = MV_CESA_AES_BLOCK_SIZE;
+	sa.spi = 3;
+
+	sa.tunProt = MV_L2FW_SEC_TUNNEL;
+	sa.encap   = MV_L2FW_SEC_ESP;
+	sa.seqNum  = 4;
+	sa.tunnelHdr.sIp = 0x6400A8C0;
+	sa.tunnelHdr.dIp = 0x6401A8C0;
+	sa.tunnelHdr.outIfIndex = 0;
+	sa.lifeTime = 0;
+
+	sa.secOp = MV_L2FW_SEC_ENCRYPT;
+	memcpy(sa.tunnelHdr.dstMac, da_addr, 6);
+	memcpy(sa.tunnelHdr.srcMac, sa_addr, 6);
+
 }
 
-int cesa_init(void)
+int mv_l2sec_cesa_init(void)
 {
-	u8 chan = 0;
-	int i;
-	const char *irq_str[] = {"cesa0", "cesa1"};
-	printk(KERN_INFO "in %s\n", __func__);
-	for (i = 0; i < 2; i++)
-		spin_lock_init(&cesa_lock[i]);
+	int chan, mask;
+	printk(KERN_INFO "%s: start.\n", __func__);
 	if (mvCtrlPwrClckGet(CESA_UNIT_ID, 0) == MV_FALSE)
 		return 0;
+
 	if (MV_OK != my_mvSysCesaInit(1, 256, NULL)) {
-		printk(KERN_INFO "%s,%d: mvCesaInit Failed. \n", __FILE__, __LINE__);
+		pr_err("%s: cesa init failed.\n", __func__);
 		return EINVAL;
 	}
 
-	/* clear and unmask Int */
-	MV_REG_WRITE(MV_CESA_ISR_CAUSE_REG(chan), 0);
-	MV_REG_WRITE(MV_CESA_ISR_MASK_REG(chan), MV_CESA_CAUSE_ACC_DMA_MASK);
-	if (request_irq(CESA_IRQ(0), nfp_sec_interrupt_handler_0,
-							(IRQF_DISABLED) , irq_str[chan], NULL)) {
-				printk(KERN_INFO "%s,%d: cannot assign irq %x\n", __FILE__, __LINE__, CESA_IRQ(chan));
-		return EINVAL;
+#ifdef CONFIG_MV_CESA_INT_COALESCING_SUPPORT
+	mask = MV_CESA_CAUSE_EOP_COAL_MASK;
+#else
+	mask = MV_CESA_CAUSE_ACC_DMA_MASK;
+#endif
+
+	for (chan = 0 ; chan < CESA_CHAN; chan++) {
+		/* clear and unmask channel 0 interrupt */
+		MV_REG_WRITE(MV_CESA_ISR_CAUSE_REG(chan), 0);
+		MV_REG_WRITE(MV_CESA_ISR_MASK_REG(chan), mask);
+
+		/* register channel 0 interrupt */
+		if (request_irq(CESA_IRQ(chan), mv_l2sec_interrupt_handler, (IRQF_DISABLED), "cesa", NULL)) {
+			printk(KERN_INFO "%s: cannot assign irq %x\n", __func__, CESA_IRQ(chan));
+			return -EINVAL;
+		}
+
+		cesaChanPort[chan] = 0;
+		cesaPrivIndx[chan] = 0;
+		cesaCmdIndx[chan] = 0;
+		cesaFullResBuf[chan] = 0;
+		spin_lock_init(&cesa_lock[chan]);
 	}
 
-	chan = 1;
-	MV_REG_WRITE(MV_CESA_ISR_CAUSE_REG(chan), 0);
-	MV_REG_WRITE(MV_CESA_ISR_MASK_REG(chan), MV_CESA_CAUSE_ACC_DMA_MASK);
+	tasklet_init(&l2sec_tasklet, mv_l2sec_req_handler, (unsigned long) 0);
+	atomic_set(&req_count, 0);
 
-	if (request_irq(CESA_IRQ(1), nfp_sec_interrupt_handler_1,
-							(IRQF_DISABLED) , irq_str[chan], NULL)) {
-				printk(KERN_INFO "%s,%d: cannot assign irq %x\n", __FILE__, __LINE__, CESA_IRQ(chan));
-		return EINVAL;
-		}
+	mv_l2sec_casa_param_init();
+	mv_l2sec_open_cesa_session();
+	mv_l2sec_cesa_start();
+	printk(KERN_INFO "%s: done.\n", __func__);
 
-	atomic_set(&req_count[0], 0);
-	atomic_set(&req_count[1], 0);
-	mvOsPrintf("MV_CESA_TDMA_CTRL_REG address 0 %08x\n\n", MV_CESA_TDMA_CTRL_REG(0));
-	mvOsPrintf("MV_CESA_TDMA_CTRL_REG address 1 %08x\n\n", MV_CESA_TDMA_CTRL_REG(1));
-	mvOsPrintf("MV_CESA_TDMA_CTRL_REG(0)  %08x\n",
-		MV_REG_READ(MV_CESA_TDMA_CTRL_REG(0)));
-	mvOsPrintf("MV_CESA_TDMA_CTRL_REG(1)  %08x\n",
-		MV_REG_READ(MV_CESA_TDMA_CTRL_REG(1)));
+	return 0;
+}
 
-	memset(&sa, 0, sizeof(MV_NFP_SEC_SA_ENTRY));
-	sa.digestSize = MV_CESA_SHA1_DIGEST_SIZE;
-	sa.ivSize = MV_CESA_AES_BLOCK_SIZE;
-	sa.spi = 3;
 
-	sa.tunProt = MV_NFP_SEC_TUNNEL;
-	sa.encap   = MV_NFP_SEC_ESP;
-	sa.seqNum  = 4;
-	sa.tunnelHdr.sIp = 0x6400A8C0;
-	sa.tunnelHdr.dIp = 0x6401A8C0;
-	sa.tunnelHdr.outIfIndex = 0;
-	sa.lifeTime = 0;
+void mv_l2sec_build_tunnel(MV_BUF_INFO *pBuf, MV_L2FW_SEC_SA_ENTRY *pSAEntry)
+{
+	MV_IP_HEADER *pIpHdr, *pIntIpHdr;
+	MV_U16 newIpTotalLength;
+
+	newIpTotalLength = pBuf->dataSize - sizeof(MV_802_3_HEADER);
+	pIpHdr = (MV_IP_HEADER *) (pBuf->bufVirtPtr + sizeof(MV_802_3_HEADER));
+
+	pIntIpHdr = (MV_IP_HEADER *) ((MV_U8 *) (pIpHdr) + sizeof(MV_IP_HEADER) + sizeof(MV_ESP_HEADER) +
+				      pSAEntry->ivSize);
+
+	/* TBD - review below settings in RFC */
+	pIpHdr->version = 0x45;
+	pIpHdr->tos = 0;
+	pIpHdr->checksum = 0;
+	pIpHdr->totalLength = MV_16BIT_BE(newIpTotalLength);
+	pIpHdr->identifier = 0;
+	pIpHdr->fragmentCtrl = 0;
+	pIpHdr->ttl = pIntIpHdr->ttl - 1;
+	pIpHdr->protocol = MV_IP_PROTO_ESP;
+	pIpHdr->srcIP = pSAEntry->tunnelHdr.sIp;
+	pIpHdr->dstIP = pSAEntry->tunnelHdr.dIp;
+
+	return;
+}
 
-	sa.secOp = MV_NFP_SEC_ENCRYPT;
-	strcpy(sa.tunnelHdr.dstMac, "aabbccddeeff");
-	strcpy(sa.tunnelHdr.srcMac, "abacadaeafaa");
 
-	return 0;
+/* Append sequence number and spi, save some space for IV */
+void mv_l2sec_build_esp_hdr(MV_BUF_INFO *pBuf, MV_L2FW_SEC_SA_ENTRY *pSAEntry)
+{
+	MV_ESP_HEADER *pEspHdr;
+
+	pEspHdr = (MV_ESP_HEADER *) (pBuf->bufVirtPtr + sizeof(MV_802_3_HEADER) + sizeof(MV_IP_HEADER));
+	pEspHdr->spi = pSAEntry->spi;
+	pSAEntry->seqNum++;
+	pEspHdr->seqNum = MV_32BIT_BE(pSAEntry->seqNum);
+}
+
+void mv_l2sec_build_mac(MV_BUF_INFO *pBuf, MV_L2FW_SEC_SA_ENTRY *pSAEntry)
+{
+	MV_802_3_HEADER *pMacHdr;
+	pMacHdr = (MV_802_3_HEADER *) ((MV_U8 *) (pBuf->bufVirtPtr));
+
+	memcpy(pMacHdr, &pSAEntry->tunnelHdr.dstMac, 12);
+	pMacHdr->typeOrLen = 0x08;/* stands for IP protocol code 16bit swapped */
+	return;
+}
+
+
+MV_STATUS mv_l2sec_esp_process(struct eth_pbuf *pPkt, MV_BUF_INFO *pBuf, MV_L2FW_SEC_SA_ENTRY *pSAEntry,
+				struct eth_port *newpp, int channel, int inPort)
+{
+	MV_CESA_COMMAND	*pCesaCmd;
+	MV_CESA_MBUF *pCesaMbuf;
+	MV_L2FW_SEC_CESA_PRIV *pCesaPriv;
+	MV_STATUS status;
+	MV_IP_HEADER *pIpHdr;
+	int cmdIndx = cesaCmdIndx[channel];
+	int privIndx = cesaPrivIndx[channel];
+
+	pCesaCmd  = &cesaCmdArray[channel][cmdIndx];
+	pCesaMbuf = &cesaMbufArray[channel][cmdIndx];
+
+	cmdIndx = (cmdIndx + 1) % CESA_DEF_REQ_SIZE;
+	cesaCmdIndx[channel] = cmdIndx;
+
+	pCesaPriv = &cesaPrivArray[channel][privIndx];
+
+	privIndx = (privIndx + 1) % (CESA_DEF_REQ_SIZE + MV_L2FW_SEC_REQ_Q_SIZE);
+	cesaPrivIndx[channel] = privIndx;
+
+	pCesaPriv->pBufInfo = pBuf;
+	pCesaPriv->pSaEntry = pSAEntry;
+	pCesaPriv->pCesaCmd = pCesaCmd;
+
+	pCesaPriv->pPkt   = pPkt;
+	pCesaPriv->ifout  = newpp->port;
+	pCesaPriv->inPort = inPort;
+	/*
+	 *  Fix, encrypt/decrypt the IP payload only, --BK 20091027
+	 */
+	pIpHdr = (MV_IP_HEADER *)(pBuf->bufVirtPtr + sizeof(MV_802_3_HEADER));
+	pBuf->dataSize = MV_16BIT_BE(pIpHdr->totalLength) + sizeof(MV_802_3_HEADER);
+
+	/* after next command, pBuf->bufVirtPtr will point to ESP */
+	pBuf->bufVirtPtr += MV_L2FW_SEC_ESP_OFFSET;
+	pBuf->bufPhysAddr += MV_L2FW_SEC_ESP_OFFSET;
+	pBuf->dataSize -= MV_L2FW_SEC_ESP_OFFSET;
+
+	pBuf->bufAddrShift -= MV_L2FW_SEC_ESP_OFFSET;
+	pCesaMbuf->pFrags = pBuf;
+	pCesaMbuf->numFrags = 1;
+	pCesaMbuf->mbufSize = pBuf->dataSize;
+
+	pCesaMbuf->pFrags->bufSize = pBuf->dataSize;
+
+	pCesaCmd->pReqPrv = (void *)pCesaPriv;
+	pCesaCmd->sessionId = pSAEntry->sid;
+	pCesaCmd->pSrc = pCesaMbuf;
+	pCesaCmd->pDst = pCesaMbuf;
+	pCesaCmd->skipFlush = MV_TRUE;
+
+	/* Assume ESP */
+	pCesaCmd->cryptoOffset = sizeof(MV_ESP_HEADER) + pSAEntry->ivSize;
+	pCesaCmd->cryptoLength =  pBuf->dataSize - (sizeof(MV_ESP_HEADER)
+				  + pSAEntry->ivSize + pSAEntry->digestSize);
+	pCesaCmd->ivFromUser = 0; /* relevant for encode only */
+	pCesaCmd->ivOffset = sizeof(MV_ESP_HEADER);
+	pCesaCmd->macOffset = 0;
+	pCesaCmd->macLength = pBuf->dataSize - pSAEntry->digestSize;
+
+
+	if ((pCesaCmd->digestOffset != 0) && ((pCesaCmd->digestOffset%4)))  {
+		printk(KERN_INFO "pBuf->dataSize=%d pSAEntry->digestSize=%d in %s\n",
+			pBuf->dataSize, pSAEntry->digestSize, __func__);
+		printk(KERN_INFO "pCesaCmd->digestOffset=%d in %s\n",
+			pCesaCmd->digestOffset, __func__);
+	}
+
+	pCesaCmd->digestOffset = pBuf->dataSize - pSAEntry->digestSize ;
+
+	disable_irq(CESA_IRQ(channel));
+	status = mvCesaAction(channel, pCesaCmd);
+	enable_irq(CESA_IRQ(channel));
+
+
+	if (status != MV_OK) {
+		pSAEntry->stats.rejected++;
+		mvOsPrintf("%s: mvCesaAction failed %d\n", __func__, status);
+	}
+	return status;
+}
+
+MV_STATUS mv_l2sec_out_going(struct eth_pbuf *pkt, MV_BUF_INFO *pBuf, MV_L2FW_SEC_SA_ENTRY *pSAEntry,
+			struct eth_port *new_pp, int inPort, int chan)
+{
+	MV_U8 *pTmp;
+	MV_U32 cryptoSize, encBlockMod, dSize;
+	/* CESA Q is full drop. */
+	if (cesaReqResources[chan] <= 1)
+		return MV_DROPPED;
+
+	cryptoSize = pBuf->dataSize - sizeof(MV_802_3_HEADER);
+
+	/* Align buffer address to beginning of new packet - TBD handle VLAN tag, LLC */
+	dSize = pSAEntry->ivSize + sizeof(MV_ESP_HEADER) + sizeof(MV_IP_HEADER);
+	pBuf->bufVirtPtr -= dSize;
+	pBuf->bufPhysAddr -= dSize;
+	pBuf->dataSize += dSize;
+	pBuf->bufAddrShift += dSize;
+
+	encBlockMod = (cryptoSize % MV_L2FW_SEC_ENC_BLOCK_SIZE);
+	/* leave space for padLen + Protocol */
+	if (encBlockMod > 14) {
+		encBlockMod =  MV_L2FW_SEC_ENC_BLOCK_SIZE - encBlockMod;
+		encBlockMod += MV_L2FW_SEC_ENC_BLOCK_SIZE;
+	} else
+		encBlockMod =  MV_L2FW_SEC_ENC_BLOCK_SIZE - encBlockMod;
+
+	pBuf->dataSize += encBlockMod;
+
+	pTmp = pBuf->bufVirtPtr + pBuf->dataSize;
+	memset(pTmp - encBlockMod, 0, encBlockMod - 2);
+	*((MV_U8 *)(pTmp-2)) = (MV_U8)(encBlockMod-2);
+	*((MV_U8 *)(pTmp-1)) = (MV_U8)4;
+
+	pBuf->dataSize += pSAEntry->digestSize;
+
+	mv_l2sec_build_esp_hdr(pBuf, pSAEntry);
+	mv_l2sec_build_tunnel(pBuf, pSAEntry);
+	mv_l2sec_build_mac(pBuf, pSAEntry);
+
+	return mv_l2sec_esp_process(pkt, pBuf, pSAEntry, new_pp, chan, inPort);
+}
+
+MV_STATUS mv_l2sec_handle_esp(struct eth_pbuf *pkt, struct neta_rx_desc *rx_desc, struct eth_port  *new_pp, int inPort)
+{
+	MV_STATUS res;
+	int chan = cesaChanPort[inPort];
+	MV_BUF_INFO *pBufInfoArr = pBufInfoArray[chan];
+	int cmdIndx = cesaCmdIndx[chan];
+	spin_lock(&cesa_lock[chan]);
+
+	pBufInfoArr[cmdIndx].bufAddrShift = 0;
+	pBufInfoArr[cmdIndx].dataSize    = pkt->bytes;
+
+	pBufInfoArr[cmdIndx].bufSize     = pkt->bytes;
+	pBufInfoArr[cmdIndx].bufVirtPtr  = pkt->pBuf + pkt->offset + MV_ETH_MH_SIZE;
+
+	pBufInfoArr[cmdIndx].bufPhysAddr = mvOsIoVirtToPhy(NULL, pBufInfoArr[cmdIndx].bufVirtPtr);
+	pBufInfoArr[cmdIndx].memHandle   = 0;
+
+	res = mv_l2sec_out_going(pkt, &pBufInfoArr[cmdIndx], &sa, new_pp, inPort, chan);
+
+	spin_unlock(&cesa_lock[chan]);
+	return res;
 }
diff --git a/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/l2fw/mv_eth_l2sec.h b/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/l2fw/mv_eth_l2sec.h
index f77e7fe..a760b06 100644
--- a/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/l2fw/mv_eth_l2sec.h
+++ b/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/l2fw/mv_eth_l2sec.h
@@ -1,493 +1,124 @@
-/* l2sec/mv_eth_l2sec.h */
+/*******************************************************************************
+Copyright (C) Marvell International Ltd. and its affiliates
+
+This software file (the "File") is owned and distributed by Marvell
+International Ltd. and/or its affiliates ("Marvell") under the following
+alternative licensing terms.  Once you have made an election to distribute the
+File under one of the following license alternatives, please (i) delete this
+introductory statement regarding license alternatives, (ii) delete the two
+license alternatives that you have not elected to use and (iii) preserve the
+Marvell copyright notice above.
+
+
+********************************************************************************
+Marvell GPL License Option
+
+If you received this File from Marvell, you may opt to use, redistribute and/or
+modify this File in accordance with the terms and conditions of the General
+Public License Version 2, June 1991 (the "GPL License"), a copy of which is
+available along with the File in the license.txt file or by writing to the Free
+Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 or
+on the worldwide web at http://www.gnu.org/licenses/gpl.txt.
+
+THE FILE IS DISTRIBUTED AS-IS, WITHOUT WARRANTY OF ANY KIND, AND THE IMPLIED
+WARRANTIES OF MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE ARE EXPRESSLY
+DISCLAIMED.  The GPL License provides additional details about this warranty
+disclaimer.
+*******************************************************************************/
 
 #ifndef L2SEC_MV_ETH_L2SEC_H
 #define L2SEC_MV_ETH_L2SEC_H
-
-#include "mvOs.h"
 #include "cesa/mvCesa.h"
 
-#include "mv_neta/l2fw/mv_eth_l2fw.h"
-#include "ctrlEnv/mvCtrlEnvLib.h"
-#include "mv_neta/net_dev/mv_netdev.h"
+extern u32 mv_crypto_virt_base_get(void);
 
-/* Taken from mvNfpSec.h */
 /* IPSec defines */
-#define MV_NFP_SEC_MAX_PACKET		1540
-#define MV_NFP_SEC_ENC_BLOCK_SIZE	16
+#define MV_L2FW_SEC_MAX_PACKET		1540
+#define MV_L2FW_SEC_ENC_BLOCK_SIZE	16
+#define MV_L2FW_SEC_ESP_OFFSET		34
+
+#define L2SEC_CESA_BUF_NUM	1	/* CESA_DEF_BUF_NUM */
+#define L2SEC_CESA_BUF_SIZE  1500	/* CESA_DEF_BUF_SIZE */
 
-#define MV_NFP_SEC_ESP_OFFSET		34
 
 /* IPSec Enumerators */
 typedef enum {
-	MV_NFP_SEC_TUNNEL = 0,
-	MV_NFP_SEC_TRANSPORT,
-} MV_NFP_SEC_PROT;
+	MV_L2FW_SEC_TUNNEL = 0,
+	MV_L2FW_SEC_TRANSPORT,
+} MV_L2FW_SEC_PROT;
 
 typedef enum {
-	MV_NFP_SEC_ESP = 0,
-	MV_NFP_SEC_AH,
-} MV_NFP_SEC_ENCAP;
+	MV_L2FW_SEC_ESP = 0,
+	MV_L2FW_SEC_AH,
+} MV_L2FW_SEC_ENCAP;
 
 
 typedef enum {
-	MV_NFP_SEC_ENCRYPT = 0,
-	MV_NFP_SEC_DECRYPT,
-} MV_NFP_SEC_OP;
+	MV_L2FW_SEC_ENCRYPT = 0,
+	MV_L2FW_SEC_DECRYPT,
+} MV_L2FW_SEC_OP;
 
-typedef struct _mv_nfp_sa_stats {
+struct mv_l2fw_sa_stats {
 	MV_U32 encrypt;
 	MV_U32 decrypt;
 	MV_U32 rejected;	/* slow path */
 	MV_U32 dropped;		/* packet drop */
 	MV_U32 bytes;
-} MV_NFP_SA_STATS;
+} MV_L2FW_SA_STATS;
 
 /* IPSec Structures */
-typedef struct _mv_nfp_sec_tunnel_hdr {
-	MV_U32 sIp;		/*  BE */
-	MV_U32 dIp;		/* BE */
+struct mv_l2fw_sec_tunnel_hdr {
+	MV_U32 sIp;			/* BE */
+	MV_U32 dIp;			/* BE */
 	/* dstMac should be 2 byte aligned */
 	MV_U8 dstMac[MV_MAC_ADDR_SIZE];	/* BE */
 	MV_U8 srcMac[MV_MAC_ADDR_SIZE];	/* BE */
 	MV_U8 outIfIndex;
-} MV_NFP_SEC_TUNNEL_HDR;
+} MV_L2FW_SEC_TUNNEL_HDR;
 
-typedef struct _mv_nfp_sec_sa_entry {
-	MV_U32 spi;		/* BE */
-	MV_NFP_SEC_PROT tunProt;
-	MV_NFP_SEC_ENCAP encap;
+struct mv_l2fw_sec_sa_entry {
+	MV_U32 spi;			/* BE */
+	MV_L2FW_SEC_PROT tunProt;
+	MV_L2FW_SEC_ENCAP encap;
 	MV_U16 sid;
-	MV_U32 seqNum;		/* LE  */
-	MV_NFP_SEC_TUNNEL_HDR tunnelHdr;
+	MV_U32 seqNum;			/* LE  */
+	MV_L2FW_SEC_TUNNEL_HDR tunnelHdr;
 	MV_U32 lifeTime;
 	MV_U8 ivSize;
 	MV_U8 cipherBlockSize;
 	MV_U8 digestSize;
-	MV_NFP_SEC_OP secOp;
-	MV_NFP_SA_STATS stats;
-} MV_NFP_SEC_SA_ENTRY;
-
-typedef struct _mv_nfp_sec_cesa_priv {
-	MV_NFP_SEC_SA_ENTRY *pSaEntry;
-	MV_PKT_INFO *pPktInfo;
-	MV_U8 orgDigest[MV_CESA_MAX_DIGEST_SIZE];
-	MV_CESA_COMMAND *pCesaCmd;
-} MV_NFP_SEC_CESA_PRIV;
+	MV_L2FW_SEC_OP secOp;
+	MV_L2FW_SA_STATS stats;
+} MV_L2FW_SEC_SA_ENTRY;
 
-int cesaChanPort[CONFIG_MV_ETH_PORTS_NUM];
 
 #define CESA_0    0
 #define CESA_1    1
-/* for future - handle by CPU */
-#define CESA_NONE 2
 
-#define MV_NFP_SEC_REQ_Q_SIZE 1000
-#define CESA_DEF_REQ_SIZE       (256*4)
-int counterNoResources[4]  = {0, 0, 0, 0};
-spinlock_t cesa_lock[2];
-
-extern u32 mv_crypto_virt_base_get(u8 chan);
-static MV_PKT_INFO *pPktInfoNewArray_0;
-static MV_PKT_INFO *pPktInfoNewArray_1;
-static MV_BUF_INFO *pBufInfoArray_0;
-static MV_BUF_INFO *pBufInfoArray_1;
-
-MV_BUF_INFO cesaBufs_0[CESA_DEF_REQ_SIZE];
-MV_BUF_INFO cesaBufs_1[CESA_DEF_REQ_SIZE];
+/* define number of channels */
+#ifdef CONFIG_ARMADA_XP
+#define CESA_CHAN 2
+#else
+#define CESA_CHAN 1
+#endif
 
-static int cesaPrivIndx_0 = 0;
-static int cesaPrivIndx_1 = 0;
 
-static int cesaCmdIndx_0 = 0;
-static int cesaCmdIndx_1 = 0;
+#define MV_L2FW_SEC_REQ_Q_SIZE   1000
+#define CESA_DEF_REQ_SIZE       (256*4)
 
-typedef struct _mv_nfp_sec_cesa_priv_l2fw {
-	MV_NFP_SEC_SA_ENTRY *pSaEntry;
-	MV_PKT_INFO *pPktInfo;
+struct mv_l2fw_sec_cesa_priv {
+	MV_L2FW_SEC_SA_ENTRY *pSaEntry;
+	MV_BUF_INFO *pBufInfo;
 	MV_U8 orgDigest[MV_CESA_MAX_DIGEST_SIZE];
 	MV_CESA_COMMAND *pCesaCmd;
 	struct eth_pbuf *pPkt;
 	int ifout;
 	int ownerId;
 	int inPort;
-} MV_NFP_SEC_CESA_PRIV_L2FW;
-
-MV_NFP_SEC_CESA_PRIV_L2FW *cesaPrivArray_0;
-MV_NFP_SEC_CESA_PRIV_L2FW *cesaPrivArray_1;
-
-void *cesaOSHandle = NULL;
-static MV_CESA_MBUF *cesaMbufArray_0;
-static MV_CESA_MBUF *cesaMbufArray_1;
-
-static MV_CESA_COMMAND *cesaCmdArray_0;
-static MV_CESA_COMMAND *cesaCmdArray_1;
-
-
-static MV_NFP_SEC_SA_ENTRY sa;
-atomic_t req_count[2];
-int l2fw_set_cesa_chan(int port, int cesaChan);
-int cesa_init(void);
-
-
-/* from mv_hal/eth/gbe/mvEthRegs.h */
-
-/* Tx descriptor bits */
-#define ETH_TX_ERROR_CODE_OFFSET            1
-#define ETH_TX_ERROR_CODE_MASK              (3<<ETH_TX_ERROR_CODE_OFFSET)
-#define ETH_TX_LATE_COLLISION_ERROR         (0<<ETH_TX_ERROR_CODE_OFFSET)
-#define ETH_TX_UNDERRUN_ERROR               (1<<ETH_TX_ERROR_CODE_OFFSET)
-#define ETH_TX_EXCESSIVE_COLLISION_ERROR    (2<<ETH_TX_ERROR_CODE_OFFSET)
-
-#define ETH_TX_LLC_SNAP_FORMAT_BIT          9
-#define ETH_TX_LLC_SNAP_FORMAT_MASK         (1<<ETH_TX_LLC_SNAP_FORMAT_BIT)
-
-#define ETH_TX_IP_FRAG_BIT                  10
-#define ETH_TX_IP_FRAG_MASK                 (1<<ETH_TX_IP_FRAG_BIT)
-#define ETH_TX_IP_FRAG                      (0<<ETH_TX_IP_FRAG_BIT)
-#define ETH_TX_IP_NO_FRAG                   (1<<ETH_TX_IP_FRAG_BIT)
-
-#define ETH_TX_IP_HEADER_LEN_OFFSET         11
-#define ETH_TX_IP_HEADER_LEN_ALL_MASK       (0xF<<ETH_TX_IP_HEADER_LEN_OFFSET)
-#define ETH_TX_IP_HEADER_LEN_MASK(len)      ((len)<<ETH_TX_IP_HEADER_LEN_OFFSET)
-
-#define ETH_TX_VLAN_TAGGED_FRAME_BIT        15
-#define ETH_TX_VLAN_TAGGED_FRAME_MASK       (1<<ETH_TX_VLAN_TAGGED_FRAME_BIT)
-
-#define ETH_TX_L4_TYPE_BIT                  16
-#define ETH_TX_L4_TCP_TYPE                  (0<<ETH_TX_L4_TYPE_BIT)
-#define ETH_TX_L4_UDP_TYPE                  (1<<ETH_TX_L4_TYPE_BIT)
-
-#define ETH_TX_GENERATE_L4_CHKSUM_BIT       17
-#define ETH_TX_GENERATE_L4_CHKSUM_MASK      (1<<ETH_TX_GENERATE_L4_CHKSUM_BIT)
-
-#define ETH_TX_GENERATE_IP_CHKSUM_BIT       18
-#define ETH_TX_GENERATE_IP_CHKSUM_MASK      (1<<ETH_TX_GENERATE_IP_CHKSUM_BIT)
-
-#define ETH_TX_ZERO_PADDING_BIT             19
-#define ETH_TX_ZERO_PADDING_MASK            (1<<ETH_TX_ZERO_PADDING_BIT)
-
-#define ETH_TX_LAST_DESC_BIT                20
-#define ETH_TX_LAST_DESC_MASK               (1<<ETH_TX_LAST_DESC_BIT)
-
-#define ETH_TX_FIRST_DESC_BIT               21
-#define ETH_TX_FIRST_DESC_MASK              (1<<ETH_TX_FIRST_DESC_BIT)
-
-#define ETH_TX_GENERATE_CRC_BIT             22
-#define ETH_TX_GENERATE_CRC_MASK            (1<<ETH_TX_GENERATE_CRC_BIT)
-
-#define ETH_TX_ENABLE_INTERRUPT_BIT         23
-#define ETH_TX_ENABLE_INTERRUPT_MASK        (1<<ETH_TX_ENABLE_INTERRUPT_BIT)
-
-#define ETH_TX_AUTO_MODE_BIT                30
-#define ETH_TX_AUTO_MODE_MASK               (1<<ETH_TX_AUTO_MODE_BIT)
-
-
-inline MV_VOID mvNfpSecBuildIPTunnel(MV_PKT_INFO *pPktInfo, MV_NFP_SEC_SA_ENTRY *pSAEntry)
-{
-	MV_IP_HEADER *pIpHdr, *pIntIpHdr;
-	MV_U16 newIpTotalLength;
-
-	newIpTotalLength = pPktInfo->pFrags[0].dataSize - sizeof(MV_802_3_HEADER);
-
-	pIpHdr = (MV_IP_HEADER *) (pPktInfo->pFrags[0].bufVirtPtr + sizeof(MV_802_3_HEADER));
-	pIntIpHdr = (MV_IP_HEADER *) ((MV_U8 *) (pIpHdr) + sizeof(MV_IP_HEADER) + sizeof(MV_ESP_HEADER) +
-				      pSAEntry->ivSize);
-
-	/* TBD - review below settings in RFC */
-	pIpHdr->version = 0x45;
-	pIpHdr->tos = 0;
-	pIpHdr->checksum = 0;
-	pIpHdr->totalLength = MV_16BIT_BE(newIpTotalLength);
-	pIpHdr->identifier = 0;
-	pIpHdr->fragmentCtrl = 0;
-	pIpHdr->ttl = pIntIpHdr->ttl - 1;
-	pIpHdr->protocol = MV_IP_PROTO_ESP;
-	pIpHdr->srcIP = pSAEntry->tunnelHdr.sIp;
-	pIpHdr->dstIP = pSAEntry->tunnelHdr.dIp;
-
-	pPktInfo->status = ETH_TX_IP_NO_FRAG | ETH_TX_GENERATE_IP_CHKSUM_MASK | (0x5 << ETH_TX_IP_HEADER_LEN_OFFSET);
-
-	return;
-}
-
-
-/* Append sequence number and spi, save some space for IV */
-inline MV_VOID mvNfpSecBuildEspHdr(MV_PKT_INFO *pPktInfo, MV_NFP_SEC_SA_ENTRY *pSAEntry)
-{
-	MV_ESP_HEADER *pEspHdr;
-
-	pEspHdr = (MV_ESP_HEADER *) (pPktInfo->pFrags[0].bufVirtPtr + sizeof(MV_802_3_HEADER) + sizeof(MV_IP_HEADER));
-	pEspHdr->spi = pSAEntry->spi;
-	pSAEntry->seqNum = (pSAEntry->seqNum++);
-	pEspHdr->seqNum = MV_32BIT_BE(pSAEntry->seqNum);
-}
-
-inline MV_VOID mvNfpSecBuildMac(MV_PKT_INFO *pPktInfo, MV_NFP_SEC_SA_ENTRY *pSAEntry)
-{
-	MV_802_3_HEADER *pMacHdr;
-
-	pMacHdr = (MV_802_3_HEADER *) ((MV_U8 *) (pPktInfo->pFrags[0].bufVirtPtr));
-	memcpy(pMacHdr, &pSAEntry->tunnelHdr.dstMac, 12);
-	pMacHdr->typeOrLen = 0x08;	/* stands for IP protocol code 16bit swapped */
-	return;
-}
+} MV_L2FW_SEC_CESA_PRIV;
 
-
-inline MV_STATUS mvSecEspProcess_0(struct eth_pbuf *pPkt, MV_PKT_INFO *pPktInfo,
-							MV_NFP_SEC_SA_ENTRY *pSAEntry, struct eth_port *newpp,
-							MV_U8 channel, int inPort)
-{
-	MV_CESA_COMMAND	*pCesaCmd;
-	MV_CESA_MBUF *pCesaMbuf;
-	MV_NFP_SEC_CESA_PRIV_L2FW *pCesaPriv;
-	MV_STATUS status;
-	MV_IP_HEADER *pIpHdr;
-	MV_BUF_INFO  *pBuf;
-
-	pCesaCmd  = &cesaCmdArray_0[cesaCmdIndx_0];
-	pCesaMbuf = &cesaMbufArray_0[cesaCmdIndx_0];
-	cesaCmdIndx_0++;
-
-	cesaCmdIndx_0 %= CESA_DEF_REQ_SIZE;
-	pCesaPriv = &cesaPrivArray_0[cesaPrivIndx_0++];
-
-	cesaPrivIndx_0 = cesaPrivIndx_0%(CESA_DEF_REQ_SIZE + MV_NFP_SEC_REQ_Q_SIZE);
-
-	pCesaPriv->pPktInfo = pPktInfo;
-	pCesaPriv->pSaEntry = pSAEntry;
-	pCesaPriv->pCesaCmd = pCesaCmd;
-
-	pCesaPriv->pPkt   = pPkt;
-	pCesaPriv->ifout  = newpp->port;
-	pCesaPriv->inPort = inPort;
-	/*
-	 *  Fix, encrypt/decrypt the IP payload only, --BK 20091027
-	 */
-	pBuf = pPktInfo->pFrags;
-	pIpHdr = (MV_IP_HEADER *)(pBuf->bufVirtPtr + sizeof(MV_802_3_HEADER));
-	pBuf->dataSize = MV_16BIT_BE(pIpHdr->totalLength) + sizeof(MV_802_3_HEADER);
-	/* after next command, pBuf->bufVirtPtr will point to ESP */
-	pBuf->bufVirtPtr += MV_NFP_SEC_ESP_OFFSET;
-	pBuf->bufPhysAddr += MV_NFP_SEC_ESP_OFFSET;
-	pBuf->dataSize -= MV_NFP_SEC_ESP_OFFSET;
-
-	pBuf->bufAddrShift -= MV_NFP_SEC_ESP_OFFSET;
-	pCesaMbuf->pFrags = pPktInfo->pFrags;
-	pCesaMbuf->numFrags = 1;
-	pCesaMbuf->mbufSize = pBuf->dataSize;
-
-	pCesaMbuf->pFrags->bufSize = pBuf->dataSize;
-
-	pCesaCmd->pReqPrv = (MV_VOID *)pCesaPriv;
-	pCesaCmd->sessionId = pSAEntry->sid;
-	pCesaCmd->pSrc = pCesaMbuf;
-	pCesaCmd->pDst = pCesaMbuf;
-	pCesaCmd->skipFlush = MV_TRUE;
-
-	/* Assume ESP */
-	pCesaCmd->cryptoOffset = sizeof(MV_ESP_HEADER) + pSAEntry->ivSize;
-	pCesaCmd->cryptoLength =  pBuf->dataSize - (sizeof(MV_ESP_HEADER)
-				  + pSAEntry->ivSize + pSAEntry->digestSize);
-	pCesaCmd->ivFromUser = 0; /* relevant for encode only */
-	pCesaCmd->ivOffset = sizeof(MV_ESP_HEADER);
-	pCesaCmd->macOffset = 0;
-	pCesaCmd->macLength = pBuf->dataSize - pSAEntry->digestSize;
-	if ((pCesaCmd->digestOffset != 0) && ((pCesaCmd->digestOffset%4)))  {
-		printk(KERN_INFO "pBuf->dataSize=%d pSAEntry->digestSize=%d in %s\n",
-			pBuf->dataSize, pSAEntry->digestSize, __func__);
-		printk(KERN_INFO "pCesaCmd->digestOffset=%d in %s\n",
-			pCesaCmd->digestOffset, __func__);
-	}
-	pCesaCmd->digestOffset = pBuf->dataSize - pSAEntry->digestSize ;
-
-	disable_irq(CESA_IRQ(channel));
-
-	status = mvCesaAction(channel, pCesaCmd);
-	enable_irq(CESA_IRQ(channel));
-	if (status != MV_OK) {
-		pSAEntry->stats.rejected++;
-		mvOsPrintf("%s: mvCesaAction failed %d\n", __func__, status);
-	}
-	return status;
-}
-
-inline MV_STATUS mvSecEspProcess_1(struct eth_pbuf *pPkt, MV_PKT_INFO *pPktInfo,
-						  MV_NFP_SEC_SA_ENTRY *pSAEntry, struct eth_port *newpp,
-						  MV_U8 channel, int inPort)
-
-{
-	MV_CESA_COMMAND	*pCesaCmd;
-	MV_CESA_MBUF *pCesaMbuf;
-	MV_NFP_SEC_CESA_PRIV_L2FW *pCesaPriv;
-	MV_STATUS status;
-	MV_IP_HEADER *pIpHdr;
-	MV_BUF_INFO  *pBuf;
-	pCesaCmd  = &cesaCmdArray_1[cesaCmdIndx_1];
-	pCesaMbuf = &cesaMbufArray_1[cesaCmdIndx_1];
-	cesaCmdIndx_1++;
-	cesaCmdIndx_1 %= CESA_DEF_REQ_SIZE;
-	pCesaPriv = &cesaPrivArray_1[cesaPrivIndx_1++];
-	cesaPrivIndx_1 = cesaPrivIndx_1%(CESA_DEF_REQ_SIZE + MV_NFP_SEC_REQ_Q_SIZE);
-
-	pCesaPriv->pPktInfo = pPktInfo;
-	pCesaPriv->pSaEntry = pSAEntry;
-	pCesaPriv->pCesaCmd = pCesaCmd;
-
-	pCesaPriv->pPkt   = pPkt;
-	pCesaPriv->ifout  = newpp->port;
-	pCesaPriv->inPort = inPort;
-	/*
-	 *  Fix, encrypt/decrypt the IP payload only, --BK 20091027
-	 */
-	pBuf = pPktInfo->pFrags;
-	pIpHdr = (MV_IP_HEADER *)(pBuf->bufVirtPtr + sizeof(MV_802_3_HEADER));
-	pBuf->dataSize = MV_16BIT_BE(pIpHdr->totalLength) + sizeof(MV_802_3_HEADER);
-	/* after next command, pBuf->bufVirtPtr will point to ESP */
-	pBuf->bufVirtPtr += MV_NFP_SEC_ESP_OFFSET;
-	pBuf->bufPhysAddr += MV_NFP_SEC_ESP_OFFSET;
-	pBuf->dataSize -= MV_NFP_SEC_ESP_OFFSET;
-	pBuf->bufAddrShift -= MV_NFP_SEC_ESP_OFFSET;
-	pCesaMbuf->pFrags = pPktInfo->pFrags;
-	pCesaMbuf->numFrags = 1;
-	pCesaMbuf->mbufSize = pBuf->dataSize;
-	pCesaMbuf->pFrags->bufSize = pBuf->dataSize;
-
-	pCesaCmd->pReqPrv = (MV_VOID *)pCesaPriv;
-	pCesaCmd->sessionId = pSAEntry->sid;
-	pCesaCmd->pSrc = pCesaMbuf;
-	pCesaCmd->pDst = pCesaMbuf;
-	pCesaCmd->skipFlush = MV_TRUE;
-
-	/* Assume ESP */
-	pCesaCmd->cryptoOffset = sizeof(MV_ESP_HEADER) + pSAEntry->ivSize;
-	pCesaCmd->cryptoLength =  pBuf->dataSize - (sizeof(MV_ESP_HEADER)
-				  + pSAEntry->ivSize + pSAEntry->digestSize);
-	pCesaCmd->ivFromUser = 0; /* relevant for encode only */
-	pCesaCmd->ivOffset = sizeof(MV_ESP_HEADER);
-	pCesaCmd->macOffset = 0;
-	pCesaCmd->macLength = pBuf->dataSize - pSAEntry->digestSize;
-	if ((pCesaCmd->digestOffset != 0) && ((pCesaCmd->digestOffset%4)))  {
-		printk(KERN_INFO "pBuf->dataSize=%d pSAEntry->digestSize=%d in %s\n",
-			pBuf->dataSize, pSAEntry->digestSize, __func__);
-		printk(KERN_INFO "pCesaCmd->digestOffset=%d in %s\n",
-			pCesaCmd->digestOffset, __func__);
-	}
-	pCesaCmd->digestOffset = pBuf->dataSize - pSAEntry->digestSize ;
-
-	disable_irq(CESA_IRQ(channel));
-
-	status = mvCesaAction(channel, pCesaCmd);
-	enable_irq(CESA_IRQ(channel));
-	if (status != MV_OK) {
-		pSAEntry->stats.rejected++;
-		mvOsPrintf("%s: mvCesaAction failed %d\n", __func__, status);
-	}
-
-	return status;
-}
-
-inline MV_STATUS mvSecOutgoing(struct eth_pbuf *pkt, MV_PKT_INFO *pPktInfo,
-						MV_NFP_SEC_SA_ENTRY *pSAEntry, struct eth_port *new_pp,
-						int inPort, MV_U8 chan)
-{
-	MV_U8 *pTmp;
-	MV_U32 cryptoSize, encBlockMod, dSize;
-	MV_BUF_INFO *pBuf = pPktInfo->pFrags;
-	/* CESA Q is full drop. */
-	if (cesaReqResources[chan] <= 1) {
-		counterNoResources[inPort]++;
-		return MV_DROPPED;
-	}
-	cryptoSize = pBuf->dataSize - sizeof(MV_802_3_HEADER);
-
-	/* Align buffer address to beginning of new packet - TBD handle VLAN tag, LLC */
-	dSize = pSAEntry->ivSize + sizeof(MV_ESP_HEADER) + sizeof(MV_IP_HEADER);
-	pBuf->bufVirtPtr -= dSize;
-	pBuf->bufPhysAddr -= dSize;
-	pBuf->dataSize += dSize;
-	pBuf->bufAddrShift += dSize;
-
-	encBlockMod = (cryptoSize % MV_NFP_SEC_ENC_BLOCK_SIZE);
-	/* leave space for padLen + Protocol */
-	if (encBlockMod > 14) {
-		encBlockMod =  MV_NFP_SEC_ENC_BLOCK_SIZE - encBlockMod;
-		encBlockMod += MV_NFP_SEC_ENC_BLOCK_SIZE;
-	} else
-		encBlockMod =  MV_NFP_SEC_ENC_BLOCK_SIZE - encBlockMod;
-	/* expected frame size */
-	dSize = pBuf->dataSize + encBlockMod + pSAEntry->digestSize;
-
-	pBuf->dataSize += encBlockMod;
-	pTmp = pBuf->bufVirtPtr + pBuf->dataSize;
-	memset(pTmp - encBlockMod, 0, encBlockMod - 2);
-	*((MV_U8 *)(pTmp-2)) = (MV_U8)(encBlockMod-2);
-	*((MV_U8 *)(pTmp-1)) = (MV_U8)4;
-
-	pBuf->dataSize += pSAEntry->digestSize;
-
-	mvNfpSecBuildEspHdr(pPktInfo, pSAEntry);
-	mvNfpSecBuildIPTunnel(pPktInfo, pSAEntry);
-	mvNfpSecBuildMac(pPktInfo, pSAEntry);
-
-	/* flush & invalidate new MAC, IP, & ESP headers + old ip*/
-	dSize = pBuf->bufAddrShift + sizeof(MV_IP_HEADER) + sizeof(MV_802_3_HEADER);
-
-	if (chan == 0)
-	  return mvSecEspProcess_0(pkt, pPktInfo, pSAEntry, new_pp, chan, inPort);
-	else
-	  return mvSecEspProcess_1(pkt, pPktInfo, pSAEntry, new_pp, chan, inPort);
-}
-
-inline MV_STATUS handleEsp(struct eth_pbuf *pkt, struct neta_rx_desc *rx_desc,
-							struct eth_port  *new_pp, int inPort)
-{
-	MV_STATUS res;
-	int chan = 	cesaChanPort[inPort];
-
-	spin_lock(&cesa_lock[chan]);
-
-	if (chan == 0) {
-		pBufInfoArray_0[cesaCmdIndx_0].bufAddrShift = 0;
-		pBufInfoArray_0[cesaCmdIndx_0].dataSize    = pkt->bytes;
-
-		pBufInfoArray_0[cesaCmdIndx_0].bufSize     = pkt->bytes;
-		pBufInfoArray_0[cesaCmdIndx_0].bufVirtPtr  = pkt->pBuf + pkt->offset + MV_ETH_MH_SIZE;
-
-		pBufInfoArray_0[cesaCmdIndx_0].bufPhysAddr = mvOsIoVirtToPhy(NULL, pkt->pBuf + pkt->offset + MV_ETH_MH_SIZE);
-		pBufInfoArray_0[cesaCmdIndx_0].memHandle   = 0;
-
-		pPktInfoNewArray_0[cesaCmdIndx_0].pFrags = &pBufInfoArray_0[cesaCmdIndx_0];
-		pPktInfoNewArray_0[cesaCmdIndx_0].numFrags = 1;
-	} else {
-		pBufInfoArray_1[cesaCmdIndx_1].bufAddrShift = 0;
-		pBufInfoArray_1[cesaCmdIndx_1].dataSize    = pkt->bytes;
-
-		pBufInfoArray_1[cesaCmdIndx_1].bufSize     = pkt->bytes;
-		pBufInfoArray_1[cesaCmdIndx_1].bufVirtPtr  = pkt->pBuf + pkt->offset + MV_ETH_MH_SIZE;
-
-		pBufInfoArray_1[cesaCmdIndx_1].bufPhysAddr = mvOsIoVirtToPhy(NULL, pkt->pBuf + pkt->offset + MV_ETH_MH_SIZE);
-		pBufInfoArray_1[cesaCmdIndx_1].memHandle   = 0;
-
-		pPktInfoNewArray_1[cesaCmdIndx_1].pFrags = &pBufInfoArray_1[cesaCmdIndx_1];
-		pPktInfoNewArray_1[cesaCmdIndx_1].numFrags = 1;
-	}
-
-	if (chan == 0)
-		res = mvSecOutgoing(pkt, &pPktInfoNewArray_0[cesaCmdIndx_0], &sa, new_pp, inPort, chan);
-	else
-		res = mvSecOutgoing(pkt, &pPktInfoNewArray_1[cesaCmdIndx_1], &sa, new_pp, inPort, chan);
-
-	spin_unlock(&cesa_lock[chan]);
-	return res;
-}
-
-void l2fw_stats(void)
-{
-	int i;
-	for (i = 0; i < 4; i++) {
-		mvOsPrintf("number of Cesa No Resources error is port[%d]=%d \n", i, counterNoResources[i]);
-		counterNoResources[i] = 0;
-	}
-}
-
-#endif
+MV_STATUS mv_l2sec_handle_esp(struct eth_pbuf *pkt, struct neta_rx_desc *rx_desc, struct eth_port  *new_pp, int inPort);
+int mv_l2sec_cesa_init(void);
+void mv_l2sec_stats(void);
+int mv_l2sec_set_cesa_chan(int port, int cesaChan);
+#endif /*L2SEC_MV_ETH_L2SEC_H*/
diff --git a/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/Makefile b/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/Makefile
index 4b9d12d..9c090ff 100644
--- a/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/Makefile
+++ b/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/Makefile
@@ -6,16 +6,9 @@ ifneq ($(MACHINE),)
 include $(srctree)/$(MACHINE)/config/mvRules.mk
 endif
 
-ifeq ($(CONFIG_MV_ETH_NFP_LIB),y)
-	obj-$(CONFIG_MV_ETHERNET) += mv_netdev.o mv_ethernet.o mv_eth_sysfs.o
-	obj-$(CONFIG_MV_PON)      += mv_pon_sysfs.o
-	obj-$(CONFIG_MV_ETH_SWITCH) +=  mv_eth_switch.o
-	obj-$(CONFIG_MV_ETH_TOOL) += mv_eth_tool.o
-	obj-y += ../nfplib.a
-else
-	obj-$(CONFIG_MV_ETHERNET) += mv_netdev.o mv_ethernet.o mv_eth_sysfs.o
-	obj-$(CONFIG_MV_PON)      += mv_pon_sysfs.o
-	obj-$(CONFIG_MV_ETH_SWITCH) +=  mv_eth_switch.o
-	obj-$(CONFIG_MV_ETH_TOOL) += mv_eth_tool.o
-endif
+obj-$(CONFIG_MV_ETH_NFP) += mv_eth_nfp.o
+obj-$(CONFIG_MV_ETHERNET) += mv_netdev.o mv_ethernet.o mv_eth_sysfs.o
+obj-$(CONFIG_MV_PON)      += mv_pon_sysfs.o
+obj-$(CONFIG_MV_ETH_SWITCH) +=  mv_eth_switch.o
+obj-$(CONFIG_MV_ETH_TOOL) += mv_eth_tool.o
 
diff --git a/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/mv_eth_nfp.c b/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/mv_eth_nfp.c
new file mode 100644
index 0000000..66de419
--- /dev/null
+++ b/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/mv_eth_nfp.c
@@ -0,0 +1,1068 @@
+/*******************************************************************************
+Copyright (C) Marvell International Ltd. and its affiliates
+
+This software file (the "File") is owned and distributed by Marvell
+International Ltd. and/or its affiliates ("Marvell") under the following
+alternative licensing terms.  Once you have made an election to distribute the
+File under one of the following license alternatives, please (i) delete this
+introductory statement regarding license alternatives, (ii) delete the two
+license alternatives that you have not elected to use and (iii) preserve the
+Marvell copyright notice above.
+
+********************************************************************************
+Marvell GPL License Option
+
+If you received this File from Marvell, you may opt to use, redistribute and/or
+modify this File in accordance with the terms and conditions of the General
+Public License Version 2, June 1991 (the "GPL License"), a copy of which is
+available along with the File in the license.txt file or by writing to the Free
+Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 or
+on the worldwide web at http://www.gnu.org/licenses/gpl.txt.
+
+THE FILE IS DISTRIBUTED AS-IS, WITHOUT WARRANTY OF ANY KIND, AND THE IMPLIED
+WARRANTIES OF MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE ARE EXPRESSLY
+DISCLAIMED.  The GPL License provides additional details about this warranty
+disclaimer.
+*******************************************************************************/
+
+#include "mvCommon.h"
+#include <linux/kernel.h>
+#include <linux/version.h>
+#include <linux/module.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/platform_device.h>
+#include <linux/skbuff.h>
+#include <linux/inetdevice.h>
+#include <linux/mv_nfp.h>
+#include <net/ip.h>
+#include <net/ipv6.h>
+
+#include "mvOs.h"
+#include "mvDebug.h"
+#include "dbg-trace.h"
+#include "mvSysHwConfig.h"
+#include "boardEnv/mvBoardEnvLib.h"
+#include "ctrlEnv/mvCtrlEnvLib.h"
+#include "eth-phy/mvEthPhy.h"
+#include "mvSysEthPhyApi.h"
+#include "mvSysNetaApi.h"
+
+#include "gbe/mvNeta.h"
+#include "bm/mvBm.h"
+#include "pnc/mvPnc.h"
+#include "pnc/mvTcam.h"
+#include "pmt/mvPmt.h"
+
+#include "mv_switch.h"
+#include "mv_netdev.h"
+#include "mv_eth_tool.h"
+#include "cpu/mvCpuCntrs.h"
+
+
+#ifdef CONFIG_MV_ETH_NFP_EXT
+int                mv_ctrl_nfp_ext_port[NFP_EXT_NUM];
+int                mv_ctrl_nfp_ext_en[NFP_EXT_NUM];
+struct net_device *mv_ctrl_nfp_ext_netdev[NFP_EXT_NUM];
+
+static void mv_eth_nfp_ext_skb_destructor(struct sk_buff *skb)
+{
+	consume_skb(skb_shinfo(skb)->destructor_arg);
+}
+
+static int mv_eth_nfp_ext_tx(struct eth_port *pp, struct eth_pbuf *pkt, MV_NFP_RESULT *res);
+#endif /* CONFIG_MV_ETH_NFP_EXT */
+
+static INLINE int mv_eth_nfp_need_fragment(MV_NFP_RESULT *res)
+{
+	if (res->flags & MV_NFP_RES_IP_INFO_VALID)
+		return (res->ipInfo.ipLen > res->mtu);
+
+	return 0;
+}
+
+/* Enable NFP */
+int mv_eth_nfp_ctrl(struct net_device *dev, int en)
+{
+	struct eth_port *pp = MV_ETH_PRIV(dev);
+
+	if (pp == NULL)
+		return 1;
+
+	if (en) {
+		pp->flags |= MV_ETH_F_NFP_EN;
+		printk(KERN_INFO "%s: NFP enabled\n", dev->name);
+	} else {
+		pp->flags &= ~MV_ETH_F_NFP_EN;
+		printk(KERN_INFO "%s: NFP disabled\n", dev->name);
+	}
+	return 0;
+}
+EXPORT_SYMBOL(mv_eth_nfp_ctrl);
+
+#ifdef CONFIG_MV_ETH_NFP_EXT
+int mv_eth_nfp_ext_add(struct net_device *dev, int port)
+{
+	int i;
+
+	/* find free place in mv_ctrl_nfp_ext_netdev */
+	for (i = 0; i < NFP_EXT_NUM; i++) {
+		if (mv_ctrl_nfp_ext_netdev[i] == NULL) {
+			mv_ctrl_nfp_ext_netdev[i] = dev;
+			mv_ctrl_nfp_ext_port[i] = port;
+			mv_ctrl_nfp_ext_en[i] = 0;
+			return 0;
+		}
+	}
+	printk(KERN_INFO "External interface %s can't be bound to NFP\n", dev->name);
+	return 1;
+}
+EXPORT_SYMBOL(mv_eth_nfp_ext_add);
+
+int mv_eth_nfp_ext_del(struct net_device *dev)
+{
+	int i;
+
+	/* find free place in mv_ctrl_nfp_ext_netdev */
+	for (i = 0; i < NFP_EXT_NUM; i++) {
+		if (mv_ctrl_nfp_ext_netdev[i] == dev) {
+			mv_ctrl_nfp_ext_netdev[i] = NULL;
+			return 0;
+		}
+	}
+	printk(KERN_INFO "External interface %s is not bound to NFP\n", dev->name);
+	return 1;
+}
+EXPORT_SYMBOL(mv_eth_nfp_ext_del);
+
+int mv_eth_nfp_ext_ctrl(struct net_device *dev, int en)
+{
+	int i;
+
+	/* find net_device in mv_ctrl_nfp_ext_netdev */
+	for (i = 0; i < NFP_EXT_NUM; i++) {
+		if (mv_ctrl_nfp_ext_netdev[i] == dev) {
+			if (en)
+				printk(KERN_INFO "%s: NFP enabled for external interface\n", dev->name);
+			 else
+				printk(KERN_INFO "%s: NFP disabled for external interface\n", dev->name);
+
+			mv_ctrl_nfp_ext_en[i] = en;
+			return 0;
+		}
+	}
+	printk(KERN_INFO "External interface %s is not bind to NFP\n", dev->name);
+	return 1;
+}
+EXPORT_SYMBOL(mv_eth_nfp_ext_ctrl);
+#else
+int mv_eth_nfp_ext_add(struct net_device *dev, int port)
+{
+	printk(KERN_INFO "NFP doesn't support external interfaces\n");
+	return 1;
+}
+EXPORT_SYMBOL(mv_eth_nfp_ext_add);
+
+int mv_eth_nfp_ext_del(struct net_device *dev)
+{
+	printk(KERN_INFO "NFP doesn't support external interfaces\n");
+	return 1;
+}
+EXPORT_SYMBOL(mv_eth_nfp_ext_del);
+
+int mv_eth_nfp_ext_ctrl(struct net_device *dev, int en)
+{
+	printk(KERN_INFO "NFP doesn't support external interfaces\n");
+	return 1;
+}
+EXPORT_SYMBOL(mv_eth_nfp_ext_ctrl);
+#endif /* CONFIG_MV_ETH_NFP_EXT */
+
+
+static inline int mv_eth_frag_build_hdr_desc(struct eth_port *priv, struct tx_queue *txq_ctrl,
+					MV_U8 *pktData, int mac_hdr_len, int ip_hdr_len,
+					     int frag_size, int left_len, int frag_offset)
+{
+	struct neta_tx_desc *tx_desc;
+	struct iphdr        *iph;
+	MV_U8               *data;
+	int                 align;
+	MV_U16              frag_ctrl;
+
+	tx_desc = mv_eth_tx_desc_get(txq_ctrl, 1);
+	if (tx_desc == NULL)
+		return -1;
+
+	txq_ctrl->txq_count++;
+
+	data = mv_eth_extra_pool_get(priv);
+	if (data == NULL)
+		return -1;
+
+	tx_desc->command = mvNetaTxqDescCsum(mac_hdr_len, MV_16BIT_BE(MV_IP_TYPE), ip_hdr_len, 0);
+	tx_desc->command |= NETA_TX_F_DESC_MASK;
+	tx_desc->dataSize = mac_hdr_len + ip_hdr_len;
+
+	txq_ctrl->shadow_txq[txq_ctrl->shadow_txq_put_i] = ((MV_ULONG)data | MV_ETH_SHADOW_EXT);
+	mv_eth_shadow_inc_put(txq_ctrl);
+
+	/* Check for IP header alignment */
+	align = 4 - (mac_hdr_len & 3);
+	data += align;
+	memcpy(data, pktData, mac_hdr_len + ip_hdr_len);
+
+	iph = (struct iphdr *)(data + mac_hdr_len);
+
+	iph->tot_len = htons(frag_size + ip_hdr_len);
+
+	/* update frag_offset and MF flag in IP header - packet can be already fragmented */
+	frag_ctrl = ntohs(iph->frag_off);
+	frag_offset += ((frag_ctrl & IP_OFFSET) << 3);
+	frag_ctrl &= ~IP_OFFSET;
+	frag_ctrl |= ((frag_offset >> 3) & IP_OFFSET);
+
+	if (((frag_ctrl & IP_MF) == 0) && (left_len != frag_size))
+		frag_ctrl |= IP_MF;
+
+	iph->frag_off = htons(frag_ctrl);
+
+	/* if it was PPPoE, update the PPPoE payload fields  */
+	if ((*((char *)iph - MV_PPPOE_HDR_SIZE - 1) == 0x64) &&
+		(*((char *)iph - MV_PPPOE_HDR_SIZE - 2) == 0x88)) {
+		PPPoE_HEADER *pPPPNew = (PPPoE_HEADER *)((char *)iph - MV_PPPOE_HDR_SIZE);
+		pPPPNew->len = htons(frag_size + ip_hdr_len + MV_PPP_HDR_SIZE);
+	}
+	tx_desc->bufPhysAddr = mvOsCacheFlush(NULL, data, tx_desc->dataSize);
+	mv_eth_tx_desc_flush(tx_desc);
+
+	return 0;
+}
+
+static inline int mv_eth_frag_build_data_desc(struct tx_queue *txq_ctrl, MV_U8 *frag_ptr, int frag_size,
+						int data_left, struct eth_pbuf *pkt)
+{
+	struct neta_tx_desc *tx_desc;
+
+	tx_desc = mv_eth_tx_desc_get(txq_ctrl, 1);
+	if (tx_desc == NULL)
+		return -1;
+
+	txq_ctrl->txq_count++;
+	tx_desc->dataSize = frag_size;
+	tx_desc->bufPhysAddr = pkt->physAddr + (frag_ptr - pkt->pBuf);
+	tx_desc->command = (NETA_TX_L_DESC_MASK | NETA_TX_Z_PAD_MASK);
+
+	if (frag_size == data_left)
+		txq_ctrl->shadow_txq[txq_ctrl->shadow_txq_put_i] = (u32) pkt;
+	else
+		txq_ctrl->shadow_txq[txq_ctrl->shadow_txq_put_i] = 0;
+
+	mv_eth_shadow_inc_put(txq_ctrl);
+	mv_eth_tx_desc_flush(tx_desc);
+
+	return 0;
+}
+
+static int mv_eth_nfp_fragment_tx(struct eth_port *pp, struct net_device *dev, MV_NFP_RESULT *res,
+					   struct tx_queue *txq_ctrl, struct eth_pbuf *pkt)
+{
+	MV_IP_HEADER_INFO *pIpInfo = &res->ipInfo;
+	int   pkt_offset = (pkt->offset + res->shift);
+	int   ip_offset = (pIpInfo->ipOffset - res->shift);
+	int   frag_size = MV_ALIGN_DOWN((res->mtu - res->ipInfo.ipHdrLen), 8);
+	int   data_left = pIpInfo->ipLen - res->ipInfo.ipHdrLen;
+	int   pktNum = (data_left / frag_size) + ((data_left % frag_size) ? 1 : 0);
+	MV_U8 *pData = pkt->pBuf + pkt_offset;
+	MV_U8 *payloadStart = pData + ip_offset + pIpInfo->ipHdrLen;
+	MV_U8 *frag_ptr = payloadStart;
+	int   i, total_bytes = 0;
+	int   save_txq_count = txq_ctrl->txq_count;
+
+	if ((txq_ctrl->txq_count + (pktNum * 2)) >= txq_ctrl->txq_size) {
+		/*
+		printk(KERN_ERR "%s: no TX descriptors - txq_count=%d, len=%d, frag_size=%d\n",
+					__func__, txq_ctrl->txq_count, data_left, frag_size);
+		*/
+		STAT_ERR(txq_ctrl->stats.txq_err++);
+		goto outNoTxDesc;
+	}
+
+	for (i = 0; i < pktNum; i++) {
+
+		if (mv_eth_frag_build_hdr_desc(pp, txq_ctrl, pData, ip_offset, pIpInfo->ipHdrLen,
+					frag_size, data_left, frag_ptr - payloadStart))
+			goto outNoTxDesc;
+
+		total_bytes += (ip_offset + pIpInfo->ipHdrLen);
+
+		if (mv_eth_frag_build_data_desc(txq_ctrl, frag_ptr, frag_size, data_left, pkt))
+			goto outNoTxDesc;
+
+		total_bytes += frag_size;
+		frag_ptr += frag_size;
+		data_left -= frag_size;
+		frag_size = MV_MIN(frag_size, data_left);
+	}
+	/* Flush + Invalidate cache for MAC + IP header + L4 header */
+	pData = pkt->pBuf + pkt->offset;
+	if (res->shift < 0)
+		pData += res->shift;
+
+	mvOsCacheMultiLineFlushInv(NULL, pData, (res->pWrite - pData));
+
+#ifdef CONFIG_MV_PON
+	if (MV_PON_PORT(pp->port))
+		mvNetaPonTxqBytesAdd(pp->port, txq_ctrl->txp, txq_ctrl->txq, total_bytes);
+#endif /* CONFIG_MV_PON */
+
+	dev->stats.tx_packets += pktNum;
+	dev->stats.tx_bytes += total_bytes;
+	STAT_DBG(txq_ctrl->stats.txq_tx += (pktNum * 2));
+
+	mvNetaTxqPendDescAdd(pp->port, txq_ctrl->txp, txq_ctrl->txq, pktNum * 2);
+
+	return pktNum * 2;
+
+outNoTxDesc:
+	while (save_txq_count < txq_ctrl->txq_count) {
+		txq_ctrl->txq_count--;
+		mv_eth_shadow_dec_put(txq_ctrl);
+		mvNetaTxqPrevDescGet(txq_ctrl->q);
+	}
+	/* Invalidate cache for MAC + IP header + L4 header */
+	pData = pkt->pBuf + pkt->offset;
+	if (res->shift < 0)
+		pData += res->shift;
+
+	mvOsCacheMultiLineInv(NULL, pData, (res->pWrite - pData));
+
+	return 0;
+}
+
+
+static MV_STATUS mv_eth_nfp_tx(struct eth_pbuf *pkt, MV_NFP_RESULT *res)
+{
+	struct net_device *dev = (struct net_device *)res->dev;
+	struct eth_port *pp = MV_ETH_PRIV(dev);
+	struct neta_tx_desc *tx_desc;
+	u32 tx_cmd, physAddr;
+	MV_STATUS status = MV_OK;
+	struct tx_queue *txq_ctrl;
+	int use_bm, pkt_offset, frags = 1;
+
+	if (!test_bit(MV_ETH_F_STARTED_BIT, &(pp->flags))) {
+		STAT_INFO(pp->stats.netdev_stop++);
+#ifdef CONFIG_MV_ETH_DEBUG_CODE
+		if (pp->flags & MV_ETH_F_DBG_TX)
+			printk(KERN_ERR "%s: STARTED_BIT = 0 , packet is dropped.\n", __func__);
+#endif /* CONFIG_MV_ETH_DEBUG_CODE */
+		return MV_DROPPED;
+	}
+
+	/* Get TxQ to send packet */
+	/* Check TXQ classification */
+	if ((res->flags & MV_NFP_RES_TXQ_VALID) == 0)
+		res->txq = pp->cpu_config[smp_processor_id()]->txq;
+
+	if ((res->flags & MV_NFP_RES_TXP_VALID) == 0)
+		res->txp = pp->txp;
+
+	txq_ctrl = &pp->txq_ctrl[res->txp * CONFIG_MV_ETH_TXQ + res->txq];
+
+	if (txq_ctrl->flags & MV_ETH_F_TX_SHARED)
+		spin_lock(&txq_ctrl->queue_lock);
+
+	/* Do fragmentation if needed */
+	if (mv_eth_nfp_need_fragment(res)) {
+		frags = mv_eth_nfp_fragment_tx(pp, dev, res, txq_ctrl, pkt);
+		if (frags == 0) {
+			dev->stats.tx_dropped++;
+			status = MV_DROPPED;
+		}
+		STAT_INFO(pp->stats.tx_fragment++);
+		goto out;
+	}
+
+	/* Get next descriptor for tx, single buffer, so FIRST & LAST */
+	tx_desc = mv_eth_tx_desc_get(txq_ctrl, 1);
+	if (tx_desc == NULL) {
+
+		/* No resources: Drop */
+		dev->stats.tx_dropped++;
+		status = MV_DROPPED;
+		goto out;
+	}
+
+	if (res->flags & MV_NFP_RES_L4_CSUM_NEEDED) {
+		MV_U8 *pData = pkt->pBuf + pkt->offset;
+
+		if (res->shift < 0)
+			pData += res->shift;
+
+		mvOsCacheMultiLineFlushInv(NULL, pData, (res->pWrite - pData));
+	}
+
+	txq_ctrl->txq_count++;
+
+	/* tx_cmd - word accumulated by NFP processing */
+	tx_cmd = res->tx_cmd;
+
+	if (res->flags & MV_NFP_RES_IP_INFO_VALID) {
+		if (res->ipInfo.family == MV_INET) {
+			tx_cmd |= NETA_TX_L3_IP4 | NETA_TX_IP_CSUM_MASK |
+				((res->ipInfo.ipOffset - res->shift) << NETA_TX_L3_OFFSET_OFFS) |
+				((res->ipInfo.ipHdrLen >> 2) << NETA_TX_IP_HLEN_OFFS);
+		} else {
+			tx_cmd |= NETA_TX_L3_IP6 |
+				((res->ipInfo.ipOffset - res->shift) << NETA_TX_L3_OFFSET_OFFS) |
+				((res->ipInfo.ipHdrLen >> 2) << NETA_TX_IP_HLEN_OFFS);
+		}
+	}
+
+#ifdef CONFIG_MV_ETH_BM_CPU
+	use_bm = 1;
+#else
+	use_bm = 0;
+#endif /* CONFIG_MV_ETH_BM_CPU */
+
+	pkt_offset = pkt->offset + res->shift;
+	physAddr = pkt->physAddr;
+	if (pkt_offset > NETA_TX_PKT_OFFSET_MAX) {
+		use_bm = 0;
+		physAddr += pkt_offset;
+		pkt_offset = 0;
+	}
+
+	if ((pkt->pool >= 0) && (pkt->pool < MV_ETH_BM_POOLS)) {
+		if (use_bm) {
+			tx_cmd |= NETA_TX_BM_ENABLE_MASK | NETA_TX_BM_POOL_ID_MASK(pkt->pool);
+			txq_ctrl->shadow_txq[txq_ctrl->shadow_txq_put_i] = (u32) NULL;
+		} else
+			txq_ctrl->shadow_txq[txq_ctrl->shadow_txq_put_i] = (u32) pkt;
+	} else {
+		/* skb from external interface */
+		txq_ctrl->shadow_txq[txq_ctrl->shadow_txq_put_i] = ((u32)pkt->osInfo | MV_ETH_SHADOW_SKB);
+	}
+
+	mv_eth_shadow_inc_put(txq_ctrl);
+
+	tx_cmd |= NETA_TX_PKT_OFFSET_MASK(pkt_offset);
+
+	tx_desc->command = tx_cmd | NETA_TX_FLZ_DESC_MASK;
+	tx_desc->dataSize = pkt->bytes;
+	tx_desc->bufPhysAddr = physAddr;
+
+	/* FIXME: PON only? --BK */
+	tx_desc->hw_cmd = pp->hw_cmd;
+
+#ifdef CONFIG_MV_ETH_DEBUG_CODE
+	if (pp->flags & MV_ETH_F_DBG_TX) {
+		printk(KERN_ERR "%s - nfp_tx_%lu: port=%d, txp=%d, txq=%d\n",
+		       dev->name, dev->stats.tx_packets, pp->port, res->txp, res->txq);
+		mv_eth_tx_desc_print(tx_desc);
+		mv_eth_pkt_print(pkt);
+	}
+#endif /* CONFIG_MV_ETH_DEBUG_CODE */
+
+	mv_eth_tx_desc_flush(tx_desc);
+
+#ifdef CONFIG_MV_PON
+	if (MV_PON_PORT(pp->port))
+		mvNetaPonTxqBytesAdd(pp->port, res->txp, res->txq, pkt->bytes);
+#endif /* CONFIG_MV_PON */
+
+	/* Enable transmit by update PENDING counter */
+	mvNetaTxqPendDescAdd(pp->port, res->txp, res->txq, 1);
+
+	/* FIXME: stats includes MH --BK */
+	dev->stats.tx_packets++;
+	dev->stats.tx_bytes += pkt->bytes;
+	STAT_DBG(txq_ctrl->stats.txq_tx++);
+
+out:
+#ifndef CONFIG_MV_ETH_TXDONE_ISR
+	if (txq_ctrl->txq_count >= mv_ctrl_txdone) {
+		u32 tx_done = mv_eth_txq_done(pp, txq_ctrl);
+
+		STAT_DIST((tx_done < pp->dist_stats.tx_done_dist_size) ? pp->dist_stats.tx_done_dist[tx_done]++ : 0);
+	}
+	/* If after calling mv_eth_txq_done, txq_ctrl->txq_count equals frags, we need to set the timer */
+	if ((txq_ctrl->txq_count == frags) && (frags > 0))
+		mv_eth_add_tx_done_timer(pp->cpu_config[smp_processor_id()]);
+
+#endif /* CONFIG_MV_ETH_TXDONE_ISR */
+
+	if (txq_ctrl->flags & MV_ETH_F_TX_SHARED)
+		spin_unlock(&txq_ctrl->queue_lock);
+
+	return status;
+}
+
+/* Main NFP function returns the following error codes:
+ *  MV_OK - packet processed and sent successfully by NFP
+ *  MV_TERMINATE - packet can't be processed by NFP - pass to Linux processing
+ *  MV_DROPPED - packet processed by NFP, but not sent (dropped)
+ */
+MV_STATUS mv_eth_nfp(struct eth_port *pp, int rxq, struct neta_rx_desc *rx_desc,
+				struct eth_pbuf *pkt, struct bm_pool *pool)
+{
+	MV_STATUS       status;
+	MV_NFP_RESULT   res;
+	bool            tx_external = false;
+
+#ifdef CONFIG_MV_ETH_DEBUG_CODE
+	if (pp->flags & MV_ETH_F_DBG_RX) {
+		mv_eth_rx_desc_print(rx_desc);
+		mv_eth_pkt_print(pkt);
+	}
+#endif /* CONFIG_MV_ETH_DEBUG_CODE */
+
+	status = nfp_core_p->nfp_rx(pp->port, rx_desc, pkt, &res);
+	tx_external = (res.flags & MV_NFP_RES_NETDEV_EXT);
+
+	if (status == MV_OK) {
+
+		if (res.flags & MV_NFP_RES_L4_CSUM_NEEDED) {
+			MV_IP_HEADER_INFO *pIpInfo = &res.ipInfo;
+			MV_U8 *pIpHdr = pIpInfo->ip_hdr.l3;
+
+			if (pIpInfo->ipProto == MV_IP_PROTO_TCP) {
+				MV_TCP_HEADER *pTcpHdr = (MV_TCP_HEADER *) ((char *)pIpHdr + pIpInfo->ipHdrLen);
+
+				pTcpHdr->chksum = csum_fold(csum_partial((char *)res.diffL4, sizeof(res.diffL4),
+									~csum_unfold(pTcpHdr->chksum)));
+				res.pWrite = (MV_U8 *)pTcpHdr + sizeof(MV_TCP_HEADER);
+			} else {
+				MV_UDP_HEADER *pUdpHdr = (MV_UDP_HEADER *) ((char *)pIpHdr + pIpInfo->ipHdrLen);
+
+				pUdpHdr->check = csum_fold(csum_partial((char *)res.diffL4, sizeof(res.diffL4),
+									~csum_unfold(pUdpHdr->check)));
+				res.pWrite = (MV_U8 *)pUdpHdr + sizeof(MV_UDP_HEADER);
+			}
+		}
+
+#ifdef CONFIG_MV_ETH_NFP_EXT
+		if  (tx_external) {
+			/* INT RX -> EXT TX */
+			mv_eth_nfp_ext_tx(pp, pkt, &res);
+			status = MV_OK;
+		} else
+#endif /* CONFIG_MV_ETH_NFP_EXT */
+			/* INT RX -> INT TX */
+			status = mv_eth_nfp_tx(pkt, &res);
+	}
+	if (status == MV_OK) {
+		STAT_DBG(pp->stats.rx_nfp++);
+
+		/* Packet transmited - refill now */
+		if (!tx_external && mv_eth_pool_bm(pool)) {
+			/* BM - no refill */
+			mvOsCacheLineInv(NULL, rx_desc);
+			return MV_OK;
+		}
+
+		if (!tx_external || mv_eth_is_recycle())
+			pkt = NULL;
+
+		if (mv_eth_refill(pp, rxq, pkt, pool, rx_desc)) {
+			printk(KERN_ERR "Linux processing - Can't refill\n");
+			pp->rxq_ctrl[rxq].missed++;
+			mv_eth_add_cleanup_timer(pp->cpu_config[smp_processor_id()]);
+			return MV_FAIL;
+		}
+		return MV_OK;
+	}
+	if (status == MV_DROPPED) {
+		/* Refill the same buffer */
+		STAT_DBG(pp->stats.rx_nfp_drop++);
+		mv_eth_rxq_refill(pp, rxq, pkt, pool, rx_desc);
+		return MV_OK;
+	}
+	return status;
+}
+
+#ifdef CONFIG_MV_ETH_NFP_EXT
+static int mv_eth_nfp_ext_tx_fragment(struct net_device *dev, struct sk_buff *skb, MV_NFP_RESULT *res)
+{
+	unsigned int      dlen, doff, error, flen, fsize, l, max_dlen, max_plen;
+	unsigned int      hdrlen, offset;
+	struct iphdr      *ip, *nip;
+	struct sk_buff    *new;
+	struct page       *page;
+	int               mac_header_len;
+	MV_IP_HEADER_INFO *pIpInfo = &res->ipInfo;
+
+	max_plen = dev->mtu + dev->hard_header_len;
+
+	SKB_LINEAR_ASSERT(skb);
+
+	mac_header_len = (pIpInfo->ipOffset - res->shift);
+	ip = (struct iphdr *)(skb->data + mac_header_len);
+
+	hdrlen = mac_header_len + res->ipInfo.ipHdrLen;
+
+	doff = hdrlen;
+	dlen = skb_headlen(skb) - hdrlen;
+	offset = ntohs(ip->frag_off) & IP_OFFSET;
+	max_dlen = (max_plen - hdrlen) & ~0x07;
+
+	do {
+		new = dev_alloc_skb(hdrlen);
+		if (!new)
+			break;
+
+		/* Setup new packet metadata */
+		new->protocol = IPPROTO_IP;
+		new->ip_summed = CHECKSUM_PARTIAL;
+		skb_set_network_header(new, mac_header_len);
+
+		/* Copy original IP header */
+		memcpy(skb_put(new, hdrlen), skb->data, hdrlen);
+
+		/* Append data portion */
+		fsize = flen = min(max_dlen, dlen);
+
+		skb_get(skb);
+		skb_shinfo(new)->destructor_arg = skb;
+		new->destructor = mv_eth_nfp_ext_skb_destructor;
+
+		while (fsize) {
+			l = PAGE_SIZE - ((unsigned long)(skb->data + doff) & ~PAGE_MASK);
+			if (l > fsize)
+				l = fsize;
+
+			page = virt_to_page(skb->data + doff);
+			get_page(page);
+			skb_add_rx_frag(new, skb_shinfo(new)->nr_frags, page,
+					(unsigned long)(skb->data + doff) &
+								~PAGE_MASK, l);
+			dlen -= l;
+			doff += l;
+			fsize -= l;
+		}
+
+		/* Fixup IP header */
+		nip = ip_hdr(new);
+		nip->tot_len = htons((4 * ip->ihl) + flen);
+		nip->frag_off = htons(offset |
+				(dlen ? IP_MF : (IP_MF & ntohs(ip->frag_off))));
+
+		/* if it was PPPoE, update the PPPoE payload fields
+		adapted from  mv_eth_frag_build_hdr_desc */
+		if ((*((char *)nip - MV_PPPOE_HDR_SIZE - 1) == 0x64) &&
+			(*((char *)nip - MV_PPPOE_HDR_SIZE - 2) == 0x88)) {
+			PPPoE_HEADER *pPPPNew = (PPPoE_HEADER *)((char *)nip - MV_PPPOE_HDR_SIZE);
+			pPPPNew->len = htons(flen + 4*ip->ihl + MV_PPP_HDR_SIZE);
+	    }
+
+		offset += flen / 8;
+
+		/* Recalculate IP checksum */
+		new->ip_summed = CHECKSUM_NONE;
+		nip->check = 0;
+		nip->check = ip_fast_csum(nip, nip->ihl);
+
+		/* TX packet */
+		error = dev->netdev_ops->ndo_start_xmit(new, dev);
+		if (error)
+			break;
+	} while (dlen);
+
+	if (!new)
+		return -ENOMEM;
+
+	if (error) {
+		consume_skb(new);
+		return error;
+	}
+
+	/* We are no longer use original skb */
+	consume_skb(skb);
+	return 0;
+}
+
+static int mv_eth_nfp_ext_tx(struct eth_port *pp, struct eth_pbuf *pkt, MV_NFP_RESULT *res)
+{
+	struct sk_buff *skb;
+	struct net_device *dev = (struct net_device *)res->dev;
+
+	/* prepare SKB for transmit */
+	skb = (struct sk_buff *)(pkt->osInfo);
+
+	skb->data += res->shift;
+	skb->tail = skb->data + pkt->bytes ;
+	skb->len = pkt->bytes;
+
+	skb_reset_mac_header(skb);
+	skb_reset_network_header(skb);
+
+	if (res->flags & MV_NFP_RES_IP_INFO_VALID) {
+
+		if (res->ipInfo.family == MV_INET) {
+			struct iphdr *iph = (struct iphdr *)res->ipInfo.ip_hdr.ip4;
+
+			if (mv_eth_nfp_need_fragment(res))
+				return mv_eth_nfp_ext_tx_fragment(dev, skb, res);
+
+			/* Recalculate IP checksum for IPv4 if necessary */
+			skb->ip_summed = CHECKSUM_NONE;
+			iph->check = 0;
+			iph->check = ip_fast_csum((unsigned char *)iph, iph->ihl);
+		}
+		skb_set_network_header(skb, res->ipInfo.ipOffset - res->shift);
+	}
+
+	if (pp) {
+		/* ingress port is GBE */
+#ifdef ETH_SKB_DEBUG
+		mv_eth_skb_check(skb);
+#endif /* ETH_SKB_DEBUG */
+
+#ifdef CONFIG_NET_SKB_RECYCLE
+		if (mv_eth_is_recycle()) {
+			skb->skb_recycle = mv_eth_skb_recycle;
+			skb->hw_cookie = pkt;
+		}
+#endif /* CONFIG_NET_SKB_RECYCLE */
+	}
+	return dev->netdev_ops->ndo_start_xmit(skb, dev);
+}
+
+
+static MV_STATUS mv_eth_nfp_ext_rxd_from_info(MV_EXT_PKT_INFO *pktInfo, struct neta_rx_desc *rxd)
+{
+	if (pktInfo->flags & MV_EXT_VLAN_EXIST_MASK)
+		NETA_RX_SET_VLAN(rxd);
+
+	if (pktInfo->flags & MV_EXT_PPP_EXIST_MASK)
+		NETA_RX_SET_PPPOE(rxd);
+
+	if (pktInfo->l3_type == ETH_P_IP)
+		NETA_RX_L3_SET_IP4(rxd);
+	else if (pktInfo->l3_type == ETH_P_IPV6)
+		NETA_RX_L3_SET_IP6(rxd);
+	else {
+		NETA_RX_L3_SET_UN(rxd);
+		return MV_OK;
+	}
+
+	if (pktInfo->flags & MV_EXT_IP_FRAG_MASK)
+		NETA_RX_IP_SET_FRAG(rxd);
+
+
+	if (!pktInfo->l3_offset || !pktInfo->l3_hdrlen)
+		return -1;
+
+	NETA_RX_SET_IPHDR_OFFSET(rxd, pktInfo->l3_offset + MV_ETH_MH_SIZE);
+	NETA_RX_SET_IPHDR_HDRLEN(rxd, (pktInfo->l3_hdrlen >> 2));
+
+	if ((pktInfo->flags & MV_EXT_L3_VALID_MASK) == 0) {
+		NETA_RX_L3_SET_IP4_ERR(rxd);
+		return MV_OK;
+	}
+
+	switch (pktInfo->l4_proto) {
+	case IPPROTO_TCP:
+		NETA_RX_L4_SET_TCP(rxd);
+		break;
+
+	case IPPROTO_UDP:
+		NETA_RX_L4_SET_UDP(rxd);
+		break;
+
+	default:
+		NETA_RX_L4_SET_OTHER(rxd);
+		break;
+	}
+
+	if (pktInfo->flags & MV_EXT_L4_VALID_MASK)
+		NETA_RX_L4_CSUM_SET_OK(rxd);
+
+	return MV_OK;
+}
+
+
+static MV_STATUS mv_eth_nfp_ext_rxd_from_ipv4(int ofs, struct iphdr *iph, struct sk_buff *skb, struct neta_rx_desc *rxd)
+{
+	int l4_proto = 0;
+	int hdrlen;
+	int tmp;
+
+	NETA_RX_L3_SET_IP4(rxd);
+	hdrlen = iph->ihl << 2;
+	NETA_RX_SET_IPHDR_HDRLEN(rxd, iph->ihl);
+
+	if (ip_fast_csum((unsigned char *)iph, iph->ihl)) {
+		NETA_RX_L3_SET_IP4_ERR(rxd);
+		return MV_OK;
+	}
+
+	switch ((l4_proto = iph->protocol)) {
+	case IPPROTO_TCP:
+		NETA_RX_L4_SET_TCP(rxd);
+		break;
+	case IPPROTO_UDP:
+		NETA_RX_L4_SET_UDP(rxd);
+		break;
+	default:
+		NETA_RX_L4_SET_OTHER(rxd);
+		l4_proto = 0;
+		break;
+	}
+
+	tmp = ntohs(iph->frag_off);
+	if ((tmp & IP_MF) != 0 || (tmp & IP_OFFSET) != 0) {
+		NETA_RX_IP_SET_FRAG(rxd);
+		return MV_OK; /* cannot checksum fragmented */
+	}
+
+	if (!l4_proto)
+		return MV_OK; /* can't proceed without l4_proto in {UDP, TCP} */
+
+	if (skb->ip_summed == CHECKSUM_UNNECESSARY) {
+		NETA_RX_L4_CSUM_SET_OK(rxd);
+		return MV_OK;
+	}
+
+	if (l4_proto == IPPROTO_UDP) {
+		struct udphdr *uh = (struct udphdr *)((char *)iph + hdrlen);
+
+		if (uh->check == 0)
+			NETA_RX_L4_CSUM_SET_OK(rxd);
+	}
+
+	/* Complete checksum with pseudo header */
+	if (skb->ip_summed == CHECKSUM_COMPLETE) {
+		if (!csum_tcpudp_magic(iph->saddr, iph->daddr, skb->len - hdrlen - ofs,
+			       l4_proto, skb->csum)) {
+			NETA_RX_L4_CSUM_SET_OK(rxd);
+			return MV_OK;
+		}
+	}
+
+	return MV_OK;
+}
+
+static MV_STATUS mv_eth_nfp_ext_rxd_from_ipv6(int ofs, struct sk_buff *skb, struct neta_rx_desc *rxd)
+{
+	struct ipv6hdr *ip6h;
+	int l4_proto = 0;
+	int hdrlen;
+	__u8 nexthdr;
+
+	NETA_RX_L3_SET_IP6(rxd);
+
+	hdrlen = sizeof(struct ipv6hdr);
+	NETA_RX_SET_IPHDR_HDRLEN(rxd, (hdrlen >> 2));
+
+	ip6h = (struct ipv6hdr *)(skb->data + ofs);
+
+	nexthdr = ip6h->nexthdr;
+
+	/* No support for extension headers. Only TCP or UDP */
+	if (nexthdr == NEXTHDR_TCP) {
+		l4_proto = IPPROTO_TCP;
+		NETA_RX_L4_SET_TCP(rxd);
+	} else if (nexthdr == NEXTHDR_UDP) {
+		l4_proto = IPPROTO_UDP;
+		NETA_RX_L4_SET_UDP(rxd);
+	} else {
+		NETA_RX_L4_SET_OTHER(rxd);
+		return MV_OK;
+	}
+
+	if (skb->ip_summed == CHECKSUM_COMPLETE) {
+		if (!csum_ipv6_magic(&ip6h->saddr, &ip6h->daddr, skb->len,
+				      l4_proto , skb->csum)) {
+			NETA_RX_L4_CSUM_SET_OK(rxd);
+			return MV_OK;
+		}
+	}
+	return MV_OK;
+}
+
+
+static MV_STATUS mv_eth_nfp_ext_rxd_build(struct sk_buff *skb, MV_EXT_PKT_INFO *pktInfo, struct neta_rx_desc *rxd)
+{
+	struct iphdr *iph;
+	int l3_proto = 0;
+	int ofs = 0;
+	MV_U16 tmp;
+
+	rxd->status = 0;
+	rxd->pncInfo = 0;
+
+	if (pktInfo)
+		return mv_eth_nfp_ext_rxd_from_info(pktInfo, rxd);
+
+	tmp = ntohs(skb->protocol);
+
+ ll:
+	switch (tmp) {
+	case ETH_P_IP:
+	case ETH_P_IPV6:
+		l3_proto = tmp;
+		break;
+
+	case ETH_P_PPP_SES:
+		NETA_RX_SET_PPPOE(rxd);
+		ofs += MV_PPPOE_HDR_SIZE;
+		switch (tmp = ntohs(*((MV_U16 *)&skb->data[ofs - 2]))) {
+		case 0x0021:
+			l3_proto = ETH_P_IP;
+			break;
+		case 0x0057:
+			l3_proto = ETH_P_IPV6;
+			break;
+		default:
+			goto non_ip;
+		}
+		break;
+
+	case ETH_P_8021Q:
+		/* Don't support double VLAN for now */
+		if (NETA_RX_IS_VLAN(rxd))
+			goto non_ip;
+
+		NETA_RX_SET_VLAN(rxd);
+		ofs = MV_VLAN_HLEN;
+
+		tmp = ntohs(*((MV_U16 *)&skb->data[2]));
+			goto ll;
+
+	default:
+	  goto non_ip;
+	}
+
+#ifndef CONFIG_MV_ETH_PNC
+	rxd->status |= ETH_RX_NOT_LLC_SNAP_FORMAT_MASK;
+#endif /* CONFIG_MV_ETH_PNC */
+
+	NETA_RX_SET_IPHDR_OFFSET(rxd, ETH_HLEN + MV_ETH_MH_SIZE + ofs);
+
+	iph = (struct iphdr *)(skb->data + ofs);
+
+	if (l3_proto == ETH_P_IP)
+		return mv_eth_nfp_ext_rxd_from_ipv4(ofs, iph, skb, rxd);
+
+	return mv_eth_nfp_ext_rxd_from_ipv6(ofs, skb, rxd);
+
+non_ip:
+	 NETA_RX_L3_SET_UN(rxd);
+	 return MV_OK;
+}
+
+void mv_eth_nfp_ext_pkt_info_print(MV_EXT_PKT_INFO *pktInfo)
+{
+	if (pktInfo == NULL)
+		return;
+
+	if (pktInfo->flags & MV_EXT_VLAN_EXIST_MASK)
+		printk(KERN_INFO "VLAN");
+
+	if (pktInfo->flags & MV_EXT_PPP_EXIST_MASK)
+		printk(KERN_INFO " PPPoE");
+
+	if (pktInfo->l3_type == ETH_P_IP)
+		printk(KERN_INFO " ipv4");
+	else if (pktInfo->l3_type == ETH_P_IPV6)
+		printk(KERN_INFO " ipv6");
+	else
+		printk(KERN_INFO " non-ip");
+
+	if (pktInfo->flags & MV_EXT_IP_FRAG_MASK)
+		printk(KERN_INFO " FRAG");
+
+	if (pktInfo->flags & MV_EXT_L3_VALID_MASK)
+		printk(KERN_INFO " L3CSUM_OK");
+
+	printk(" offset=%d, hdrlen=%d", pktInfo->l3_offset, pktInfo->l3_hdrlen);
+
+	if (pktInfo->l4_proto == IPPROTO_TCP)
+		printk(KERN_INFO " TCP");
+	else if (pktInfo->l4_proto == IPPROTO_UDP)
+		printk(KERN_INFO " UDP");
+
+	if (pktInfo->flags & MV_EXT_L4_VALID_MASK)
+		printk(KERN_INFO " L4CSUM_OK");
+
+	printk(KERN_INFO "\n");
+}
+
+
+/* Return values:   0 - packet successfully processed by NFP (transmitted or dropped) */
+/*                  1 - packet can't be processed by NFP  */
+/*                  2 - skb is not valid for NFP (not enough headroom or nonlinear) */
+/*                  3 - not enough info in pktInfo   */
+int mv_eth_nfp_ext(struct net_device *dev, struct sk_buff *skb, MV_EXT_PKT_INFO *pktInfo)
+{
+	MV_STATUS           status;
+	MV_NFP_RESULT       res;
+	struct neta_rx_desc rx_desc;
+	struct eth_pbuf     pkt;
+	int                 err = 1;
+	int                 i, port = -1;
+
+#define NEEDED_HEADROOM (MV_PPPOE_HDR_SIZE + MV_VLAN_HLEN)
+
+	/* Check that NFP is enabled for this external interface */
+	for (i = 0; i < NFP_EXT_NUM; i++) {
+		if ((mv_ctrl_nfp_ext_netdev[i] == dev) && (mv_ctrl_nfp_ext_en[i])) {
+			port = mv_ctrl_nfp_ext_port[i];
+			break;
+		}
+	}
+	if (port == -1) /* NFP is disabled */
+		return 1;
+
+	if (skb_is_nonlinear(skb)) {
+		printk(KERN_ERR "%s: skb=%p is nonlinear\n", __func__, skb);
+		return 2;
+	}
+
+	/* Prepare pkt structure */
+	pkt.offset = skb_headroom(skb) - (ETH_HLEN + MV_ETH_MH_SIZE);
+	if (pkt.offset < NEEDED_HEADROOM) {
+		/* we don't know at this stage if there will be added any of vlans or pppoe or both */
+		printk(KERN_ERR "%s: Possible problem: not enough headroom: %d < %d\n",
+				__func__, pkt.offset, NEEDED_HEADROOM);
+		return 2;
+	}
+
+	pkt.pBuf = skb->head;
+	pkt.bytes = skb->len + ETH_HLEN + MV_ETH_MH_SIZE;
+
+	/* Set invalid pool to prevent BM usage */
+	pkt.pool = MV_ETH_BM_POOLS;
+	pkt.physAddr = mvOsIoVirtToPhys(NULL, skb->head);
+	pkt.osInfo = (void *)skb;
+
+	/* prepare rx_desc structure */
+	status = mv_eth_nfp_ext_rxd_build(skb, pktInfo,  &rx_desc);
+	if (status != MV_OK)
+		return 3;
+
+/*	read_lock(&nfp_lock);*/
+	status = nfp_core_p->nfp_rx(port, &rx_desc, &pkt, &res);
+
+/*	read_unlock(&nfp_lock);*/
+
+	if (status == MV_OK) {
+		if  (res.flags & MV_NFP_RES_NETDEV_EXT) {
+			/* EXT RX -> EXT TX */
+			mv_eth_nfp_ext_tx(NULL, &pkt, &res);
+		} else {
+			/* EXT RX -> INT TX */
+			mvOsCacheFlush(NULL, pkt.pBuf + pkt.offset, pkt.bytes);
+			status = mv_eth_nfp_tx(&pkt, &res);
+			if (status != MV_OK)
+				dev_kfree_skb_any(skb);
+		}
+		err = 0;
+	} else if (status == MV_DROPPED) {
+		dev_kfree_skb_any(skb);
+		err = 0;
+	}
+	return err;
+}
+#endif /* CONFIG_MV_ETH_NFP_EXT */
diff --git a/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/mv_eth_switch.c b/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/mv_eth_switch.c
index 8af8738..fe676e1 100644
--- a/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/mv_eth_switch.c
+++ b/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/mv_eth_switch.c
@@ -344,6 +344,71 @@ int    mv_eth_switch_set_mac_addr(struct net_device *dev, void *mac)
 	return 0;
 }
 
+void    mv_eth_switch_set_multicast_list(struct net_device *dev)
+{
+	struct eth_netdev *dev_priv = MV_DEV_PRIV(dev);
+
+	if (dev->flags & IFF_PROMISC) {
+		/* promiscuous mode - connect the CPU port to the VLAN (port based + 802.1q) */
+		/* printk(KERN_ERR "mv_eth_switch: setting promiscuous mode\n"); */
+		if (mv_switch_promisc_set(dev_priv->vlan_grp_id, dev_priv->port_map, dev_priv->cpu_port, 1))
+			printk(KERN_ERR "mv_switch_promisc_set to 1 failed\n");
+	} else {
+		/* not in promiscuous mode - disconnect the CPU port to the VLAN (port based + 802.1q) */
+		if (mv_switch_promisc_set(dev_priv->vlan_grp_id, dev_priv->port_map, dev_priv->cpu_port, 0))
+			printk(KERN_ERR "mv_switch_promisc_set to 0 failed\n");
+
+		if (dev->flags & IFF_ALLMULTI) {
+			/* allmulticast - not supported. There is no way to tell the Switch to accept only	*/
+			/* the multicast addresses but not Unicast addresses, so the alternatives are:	*/
+			/* 1) Don't support multicast and do nothing					*/
+			/* 2) Support multicast with same implementation as promiscuous			*/
+			/* 3) Don't rely on Switch for MAC filtering, but use PnC			*/
+			/* Currently option 1 is selected						*/
+			printk(KERN_ERR "mv_eth_switch: setting all-multicast mode is not supported\n");
+		}
+
+		/* Add or delete specific multicast addresses:						*/
+		/* Linux provides a list of the current multicast addresses for the device.		*/
+		/* First, we delete all the multicast addresses in the ATU.				*/
+		/* Then we add the specific multicast addresses Linux provides.				*/
+		if (mv_switch_all_multicasts_del(MV_SWITCH_VLAN_TO_GROUP(dev_priv->vlan_grp_id)))
+			printk(KERN_ERR "mv_eth_switch: mv_switch_all_multicasts_del failed\n");
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 34)
+		if (!netdev_mc_empty(dev)) {
+			struct netdev_hw_addr *ha;
+
+			netdev_for_each_mc_addr(ha, dev) {
+				mv_switch_mac_addr_set(ha->addr,
+					MV_SWITCH_VLAN_TO_GROUP(dev_priv->vlan_grp_id),
+					(dev_priv->port_map | (1 << dev_priv->cpu_port)), 1);
+			}
+		}
+#else
+		{
+			int i;
+			struct dev_mc_list *curr_addr = dev->mc_list;
+
+			/* accept specific multicasts */
+			for (i = 0; i < dev->mc_count; i++, curr_addr = curr_addr->next) {
+				if (!curr_addr)
+					break;
+
+				/*
+				printk(KERN_ERR "Setting MC = %02X:%02X:%02X:%02X:%02X:%02X\n",
+				curr_addr->dmi_addr[0], curr_addr->dmi_addr[1], curr_addr->dmi_addr[2],
+				curr_addr->dmi_addr[3], curr_addr->dmi_addr[4], curr_addr->dmi_addr[5]);
+				*/
+				mv_switch_mac_addr_set(curr_addr->dmi_addr,
+					MV_SWITCH_VLAN_TO_GROUP(dev_priv->vlan_grp_id),
+					(dev_priv->port_map | (1 << dev_priv->cpu_port)), 1);
+			}
+		}
+#endif /* KERNEL_VERSION >= 2.6.34 */
+	}
+}
+
 int     mv_eth_switch_change_mtu(struct net_device *dev, int mtu)
 {
 	int i;
diff --git a/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/mv_eth_sysfs.c b/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/mv_eth_sysfs.c
index a984470..15dcc94 100644
--- a/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/mv_eth_sysfs.c
+++ b/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/mv_eth_sysfs.c
@@ -45,7 +45,9 @@ static ssize_t mv_eth_help(char *buf)
 	off += mvOsSPrintf(buf+off, "\n");
 
 	off += sprintf(buf+off, "cat                ports           - show all ports info\n");
+	off += sprintf(buf+off, "echo p d           > stack         - show pools stack for port <p>. d=0-brief, d=1-full\n");
 	off += sprintf(buf+off, "echo p             > port          - show a port info\n");
+	off += sprintf(buf+off, "echo [if_name]     > netdev        - show <if_name> net_device status\n");
 	off += sprintf(buf+off, "echo p             > stats         - show a port statistics\n");
 	off += sprintf(buf+off, "echo p txp         > cntrs         - show a port counters\n");
 	off += sprintf(buf+off, "echo p             > tos           - show RX and TX TOS map for port <p>\n");
@@ -96,7 +98,9 @@ static ssize_t mv_eth_help(char *buf)
 	off += sprintf(buf+off, "echo p d           > rx_weight     - set weight for the poll function; <d> - new weight, max val: 255\n");
 	off += sprintf(buf+off, "echo p cpu mask    > txq_mask      - set cpu <cpu> accessible txq bitmap <mask>.\n");
 	off += sprintf(buf+off, "echo p txp txq d   > txq_shared    - set/reset shared bit for <port/txp/txq>. <d> - 1/0 for set/reset.\n");
-	off += sprintf(buf+off, "echo p {0|1|2}     > pm_mode       - set port <p> pm mode. 0 wol, 1 clock, 2 disabled.\n");
+#ifdef CONFIG_MV_ETH_PNC_WOL
+	off += sprintf(buf+off, "echo p wol         > wol_mode      - set port <p> pm mode. 0 wol, 1 suspend.\n");
+#endif
 	return off;
 }
 
@@ -180,6 +184,38 @@ static ssize_t mv_eth_show(struct device *dev,
 	return off;
 }
 
+static ssize_t mv_eth_netdev_store(struct device *dev,
+				struct device_attribute *attr, const char *buf, size_t len)
+{
+	const char        *name = attr->attr.name;
+	int               err = 0;
+	char              dev_name[IFNAMSIZ];
+	struct net_device *netdev;
+
+	if (!capable(CAP_NET_ADMIN))
+		return -EPERM;
+
+	sscanf(buf, "%s", dev_name);
+	netdev = dev_get_by_name(&init_net, dev_name);
+	if (netdev == NULL) {
+		printk(KERN_ERR "%s: network interface <%s> doesn't exist\n",
+			__func__, dev_name);
+		err = 1;
+	} else {
+		if (!strcmp(name, "netdev"))
+			mv_eth_netdev_print(netdev);
+		else {
+			err = 1;
+			printk(KERN_ERR "%s: illegal operation <%s>\n", __func__, attr->attr.name);
+		}
+		dev_put(netdev);
+	}
+	if (err)
+		printk(KERN_ERR "%s: error %d\n", __func__, err);
+
+	return err ? -EINVAL : len;
+}
+
 static ssize_t mv_eth_port_store(struct device *dev,
 				   struct device_attribute *attr, const char *buf, size_t len)
 {
@@ -217,6 +253,8 @@ static ssize_t mv_eth_port_store(struct device *dev,
 		mv_eth_status_print();
 		mvNetaPortStatus(p);
 		mv_eth_port_status_print(p);
+	} else if (!strcmp(name, "stack")) {
+		mv_eth_stack_print(p, v);
 	} else if (!strcmp(name, "stats")) {
 		mv_eth_port_stats_print(p);
 	} else if (!strcmp(name, "tos")) {
@@ -231,8 +269,6 @@ static ssize_t mv_eth_port_store(struct device *dev,
 		printk(KERN_INFO "\n");
 		mvEthPortRegs(p);
 		mvNetaPortRegs(p);
-	} else if (!strcmp(name, "pm_mode")) {
-		err = mv_eth_pm_mode_set(p, v);
 #ifdef MV_ETH_GMAC_NEW
 	} else if (!strcmp(name, "gmac_regs")) {
 		mvNetaGmacRegs(p);
@@ -241,6 +277,10 @@ static ssize_t mv_eth_port_store(struct device *dev,
 	} else if (!strcmp(name, "pnc")) {
 		mv_eth_ctrl_pnc(p);
 #endif /* CONFIG_MV_ETH_PNC */
+#ifdef CONFIG_MV_ETH_PNC_WOL
+	} else if (!strcmp(name, "wol_mode")) {
+		err = mv_eth_wol_mode_set(p, v);
+#endif /* CONFIG_MV_ETH_PNC_WOL */
 	} else if (!strcmp(name, "napi")) {
 		mv_eth_napi_group_show(p);
 	} else {
@@ -463,6 +503,7 @@ static DEVICE_ATTR(debug,       S_IWUSR, mv_eth_show, mv_eth_port_store);
 static DEVICE_ATTR(wrr_regs,    S_IWUSR, mv_eth_show, mv_eth_4_store);
 static DEVICE_ATTR(cntrs,       S_IWUSR, mv_eth_show, mv_eth_4_store);
 static DEVICE_ATTR(port,        S_IWUSR, mv_eth_show, mv_eth_port_store);
+static DEVICE_ATTR(stack,        S_IWUSR, mv_eth_show, mv_eth_port_store);
 static DEVICE_ATTR(rxq_regs,    S_IWUSR, mv_eth_show, mv_eth_3_store);
 static DEVICE_ATTR(txq_regs,    S_IWUSR, mv_eth_show, mv_eth_4_store);
 static DEVICE_ATTR(mac,         S_IWUSR, mv_eth_show, mv_eth_port_store);
@@ -481,12 +522,15 @@ static DEVICE_ATTR(tx_done,     S_IWUSR, mv_eth_show, mv_eth_3_store);
 #ifdef CONFIG_MV_ETH_PNC
 static DEVICE_ATTR(pnc,         S_IWUSR, NULL, mv_eth_port_store);
 #endif /* CONFIG_MV_ETH_PNC */
+#ifdef CONFIG_MV_ETH_PNC_WOL
+static DEVICE_ATTR(wol_mode,	S_IWUSR, mv_eth_show, mv_eth_port_store);
+#endif /* CONFIG_MV_ETH_PNC_WOL */
 static DEVICE_ATTR(cpu_group,   S_IWUSR, mv_eth_show, mv_eth_3_hex_store);
 static DEVICE_ATTR(rxq_group,   S_IWUSR, mv_eth_show, mv_eth_3_hex_store);
 static DEVICE_ATTR(napi,        S_IWUSR, mv_eth_show, mv_eth_port_store);
 static DEVICE_ATTR(txq_mask,    S_IWUSR, mv_eth_show, mv_eth_3_hex_store);
 static DEVICE_ATTR(txq_shared,  S_IWUSR, mv_eth_show, mv_eth_4_store);
-static DEVICE_ATTR(pm_mode,	S_IWUSR, mv_eth_show, mv_eth_port_store);
+static DEVICE_ATTR(netdev,       S_IWUSR, NULL, mv_eth_netdev_store);
 
 static struct attribute *mv_eth_attrs[] = {
 
@@ -519,9 +563,11 @@ static struct attribute *mv_eth_attrs[] = {
 	&dev_attr_rxq_regs.attr,
 	&dev_attr_txq_regs.attr,
 	&dev_attr_port.attr,
+	&dev_attr_stack.attr,
 	&dev_attr_stats.attr,
 	&dev_attr_cntrs.attr,
 	&dev_attr_ports.attr,
+	&dev_attr_netdev.attr,
 	&dev_attr_tos.attr,
 	&dev_attr_mac.attr,
 	&dev_attr_vprio.attr,
@@ -536,12 +582,14 @@ static struct attribute *mv_eth_attrs[] = {
 #ifdef CONFIG_MV_ETH_PNC
     &dev_attr_pnc.attr,
 #endif /* CONFIG_MV_ETH_PNC */
+#ifdef CONFIG_MV_ETH_PNC_WOL
+	&dev_attr_wol_mode.attr,
+#endif
 	&dev_attr_cpu_group.attr,
 	&dev_attr_rxq_group.attr,
 	&dev_attr_napi.attr,
 	&dev_attr_txq_mask.attr,
 	&dev_attr_txq_shared.attr,
-	&dev_attr_pm_mode.attr,
 	NULL
 };
 
diff --git a/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/mv_eth_tool.c b/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/mv_eth_tool.c
index 31194e3..9b673bb 100644
--- a/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/mv_eth_tool.c
+++ b/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/mv_eth_tool.c
@@ -289,9 +289,6 @@ int mv_eth_tool_set_settings(struct net_device *dev, struct ethtool_cmd *cmd)
 	return err;
 }
 
-
-
-
 /******************************************************************************
 * mv_eth_tool_get_regs_len
 * Description:
@@ -646,7 +643,6 @@ int mv_eth_tool_set_pauseparam(struct net_device *netdev,
 void mv_eth_tool_get_strings(struct net_device *netdev,
 			     uint32_t stringset, uint8_t *data)
 {
-/*	printk("in %s \n",__FUNCTION__);*/
 }
 
 /******************************************************************************
@@ -663,12 +659,12 @@ void mv_eth_tool_get_strings(struct net_device *netdev,
 *******************************************************************************/
 int mv_eth_tool_get_stats_count(struct net_device *netdev)
 {
-/*	printk("in %s \n",__FUNCTION__);*/
 	return 0;
 }
 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3, 4, 25)
 static int mv_eth_tool_get_rxfh_indir(struct net_device *netdev,
-										struct ethtool_rxfh_indir *indir)
+					struct ethtool_rxfh_indir *indir)
 {
 #if defined(MV_ETH_PNC_LB) && defined(CONFIG_MV_ETH_PNC)
 	struct eth_port 	*priv = MV_ETH_PRIV(netdev);
@@ -686,7 +682,7 @@ static int mv_eth_tool_get_rxfh_indir(struct net_device *netdev,
 }
 
 static int mv_eth_tool_set_rxfh_indir(struct net_device *netdev,
-							   const struct ethtool_rxfh_indir *indir)
+				   const struct ethtool_rxfh_indir *indir)
 {
 #if defined(MV_ETH_PNC_LB) && defined(CONFIG_MV_ETH_PNC)
 	int i;
@@ -702,7 +698,7 @@ static int mv_eth_tool_set_rxfh_indir(struct net_device *netdev,
 }
 
 static int mv_eth_tool_get_rxnfc(struct net_device *dev, struct ethtool_rxnfc *info,
-									void *rules)
+									u32 *rules)
 {
 	if (info->cmd == ETHTOOL_GRXRINGS) {
 		struct eth_port *pp = MV_ETH_PRIV(dev);
@@ -713,6 +709,62 @@ static int mv_eth_tool_get_rxnfc(struct net_device *dev, struct ethtool_rxnfc *i
 }
 
 /******************************************************************************
+* mv_eth_tool_set_rx_ntuple
+* Description:
+*	ethtool set mapping from 2t/5t rule to rxq/drop
+*	ignore mask parameters (assume mask=0xFF for each byte provided)
+*	support only tcp4 / udp4 protocols
+*	support only full 2t/5t rules:
+*		** 2t - must provide src-ip, dst-ip
+*		** 5t - must provide src-ip, dst-ip, src-port, dst-port
+* INPUT:
+*	netdev		Network device structure pointer
+*	ntuple
+* OUTPUT
+*	None
+* RETURN:
+*
+*******************************************************************************/
+static int mv_eth_tool_set_rx_ntuple(struct net_device *dev, struct ethtool_rx_ntuple *ntuple)
+{
+#ifdef CONFIG_MV_ETH_PNC_L3_FLOW
+	unsigned int sip, dip, ports, sport, dport, proto;
+	struct eth_port *pp;
+
+	if ((ntuple->fs.flow_type != TCP_V4_FLOW) && (ntuple->fs.flow_type != UDP_V4_FLOW))
+		return -EOPNOTSUPP;
+
+	if ((ntuple->fs.action >= CONFIG_MV_ETH_RXQ) || (ntuple->fs.action < ETHTOOL_RXNTUPLE_ACTION_CLEAR))
+		return -EINVAL;
+
+	if (ntuple->fs.flow_type == TCP_V4_FLOW)
+		proto = 6; /* tcp */
+	else
+		proto = 17; /* udp */
+
+	sip = ntuple->fs.h_u.tcp_ip4_spec.ip4src;
+	dip = ntuple->fs.h_u.tcp_ip4_spec.ip4dst;
+	sport = ntuple->fs.h_u.tcp_ip4_spec.psrc;
+	dport = ntuple->fs.h_u.tcp_ip4_spec.pdst;
+	if (!sip || !dip)
+		return -EINVAL;
+
+	pp = MV_ETH_PRIV(dev);
+	if (!sport || !dport) { /* 2-tuple */
+		pnc_ip4_2tuple_rxq(pp->port, sip, dip, ntuple->fs.action);
+	} else {
+		ports = (dport << 16) | ((sport << 16) >> 16);
+		pnc_ip4_5tuple_rxq(pp->port, sip, dip, ports, proto, ntuple->fs.action);
+	}
+
+	return 0;
+#else
+	return 1;
+#endif /* CONFIG_MV_ETH_PNC_L3_FLOW */
+}
+#endif /* #if LINUX_VERSION_CODE < KERNEL_VERSION(3, 4, 25) */
+
+/******************************************************************************
 * mv_eth_tool_get_ethtool_stats
 * Description:
 *	ethtool get statistics
@@ -744,10 +796,18 @@ const struct ethtool_ops mv_eth_tool_ops = {
 	.get_ringparam  			= mv_eth_tool_get_ringparam,
 	.get_pauseparam				= mv_eth_tool_get_pauseparam,
 	.set_pauseparam				= mv_eth_tool_set_pauseparam,
-	.get_strings				= mv_eth_tool_get_strings,
+	.get_strings				= mv_eth_tool_get_strings,/*TODO: complete implementation */
+	.get_ethtool_stats			= mv_eth_tool_get_ethtool_stats,
+
 #if LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 32)
 	.get_stats_count			= mv_eth_tool_get_stats_count,
 #endif
-	.get_ethtool_stats			= mv_eth_tool_get_ethtool_stats,
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3, 4, 25)
+	.get_rxfh_indir				= mv_eth_tool_get_rxfh_indir,
+	.set_rxfh_indir				= mv_eth_tool_set_rxfh_indir,
+	.get_rxnfc				= mv_eth_tool_get_rxnfc,
+	.set_rx_ntuple				= mv_eth_tool_set_rx_ntuple,
+#endif
 };
 
diff --git a/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/mv_eth_tool.h b/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/mv_eth_tool.h
index 4e673e7..3c8f3ce 100644
--- a/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/mv_eth_tool.h
+++ b/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/mv_eth_tool.h
@@ -1,6 +1,32 @@
-// mv_eth_tool.h
-#ifndef NET_DEV_MV_ETH_TOOL_H
-#define NET_DEV_MV_ETH_TOOL_H
+/*******************************************************************************
+Copyright (C) Marvell International Ltd. and its affiliates
+
+This software file (the "File") is owned and distributed by Marvell
+International Ltd. and/or its affiliates ("Marvell") under the following
+alternative licensing terms.  Once you have made an election to distribute the
+File under one of the following license alternatives, please (i) delete this
+introductory statement regarding license alternatives, (ii) delete the two
+license alternatives that you have not elected to use and (iii) preserve the
+Marvell copyright notice above.
+
+
+********************************************************************************
+Marvell GPL License Option
+
+If you received this File from Marvell, you may opt to use, redistribute and/or
+modify this File in accordance with the terms and conditions of the General
+Public License Version 2, June 1991 (the "GPL License"), a copy of which is
+available along with the File in the license.txt file or by writing to the Free
+Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 or
+on the worldwide web at http://www.gnu.org/licenses/gpl.txt.
+
+THE FILE IS DISTRIBUTED AS-IS, WITHOUT WARRANTY OF ANY KIND, AND THE IMPLIED
+WARRANTIES OF MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE ARE EXPRESSLY
+DISCLAIMED.  The GPL License provides additional details about this warranty
+disclaimer.
+*******************************************************************************/
+#ifndef __MV_ETH_TOOL_H__
+#define __MV_ETH_TOOL_H__
 
 #include <linux/ethtool.h>
 
diff --git a/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/mv_netdev.c b/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/mv_netdev.c
index 4b5e67f..575203b45 100644
--- a/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/mv_netdev.c
+++ b/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/mv_netdev.c
@@ -78,6 +78,9 @@ static inline int mv_eth_tx_policy(struct eth_port *pp, struct sk_buff *skb);
 /* uncomment if you want to debug the SKB recycle feature */
 /* #define ETH_SKB_DEBUG */
 
+static int pm_flag;
+static int wol_ports_bmp;
+
 #ifdef CONFIG_MV_ETH_PNC
 unsigned int mv_eth_pnc_ctrl_en = 1;
 
@@ -133,6 +136,7 @@ static void mv_eth_txq_delete(struct eth_port *pp, struct tx_queue *txq_ctrl);
 static void mv_eth_tx_timeout(struct net_device *dev);
 static int  mv_eth_tx(struct sk_buff *skb, struct net_device *dev);
 static void mv_eth_tx_frag_process(struct eth_port *pp, struct sk_buff *skb, struct tx_queue *txq_ctrl, u16 flags);
+static int mv_eth_rxq_fill(struct eth_port *pp, int rxq, int num);
 
 static void mv_eth_config_show(void);
 static int  mv_eth_priv_init(struct eth_port *pp, int port);
@@ -141,8 +145,7 @@ static int  mv_eth_config_get(struct eth_port *pp, u8 *mac);
 static int  mv_eth_hal_init(struct eth_port *pp);
 struct net_device *mv_eth_netdev_init(struct eth_port *pp, int mtu, u8 *mac,
 					struct platform_device *pdev);
-static void mv_eth_netdev_set_features(struct net_device *dev);
-static void mv_eth_netdev_update_features(struct net_device *dev);
+static void mv_eth_netdev_init_features(struct net_device *dev);
 
 static MV_STATUS mv_eth_pool_create(int pool, int capacity);
 static int mv_eth_pool_add(int pool, int buf_num);
@@ -165,27 +168,6 @@ __setup("mv_port2_config=", mv_eth_cmdline_port2_config);
 int mv_eth_cmdline_port3_config(char *s);
 __setup("mv_port3_config=", mv_eth_cmdline_port3_config);
 
-#if defined(CONFIG_MV_ETH_NFP) || defined(CONFIG_MV_ETH_NFP_MODULE)
-struct mv_eth_nfp_ops *mv_eth_nfp_ops = NULL;
-int mv_eth_nfp_register(mv_eth_nfp_func_t *func)
-{
-	mv_eth_nfp_ops = kmalloc(sizeof(struct mv_eth_nfp_ops), GFP_ATOMIC);
-	if (mv_eth_nfp_ops == NULL) {
-		printk(KERN_ERR "%s: Error allocating memory for mv_eth_nfp_ops\n", __func__);
-		return -ENOMEM;
-	}
-	mv_eth_nfp_ops->mv_eth_nfp = func;
-	return 0;
-}
-EXPORT_SYMBOL(mv_eth_nfp_register);
-
-void mv_eth_nfp_unregister(void)
-{
-	kfree(mv_eth_nfp_ops);
-	mv_eth_nfp_ops = NULL;
-}
-EXPORT_SYMBOL(mv_eth_nfp_unregister);
-#endif /* CONFIG_MV_ETH_NFP || CONFIG_MV_ETH_NFP_MODULE */
 
 int mv_eth_cmdline_port0_config(char *s)
 {
@@ -210,6 +192,34 @@ int mv_eth_cmdline_port3_config(char *s)
 	port3_config_str = s;
 	return 1;
 }
+void mv_eth_stack_print(int port, MV_BOOL isPrintElements)
+{
+	struct eth_port *pp = mv_eth_port_by_id(port);
+
+	if (pp == NULL) {
+		printk(KERN_INFO "%s: Invalid port [%d]\n", __func__, port);
+		return;
+	}
+
+	if (pp->pool_long == NULL) {
+		printk(KERN_ERR "%s: Error - long pool is null\n", __func__);
+		return;
+	}
+
+	printk(KERN_INFO "Long pool (%d) stack\n", pp->pool_long->pool);
+	mvStackStatus(pp->pool_long->stack, isPrintElements);
+
+#ifdef CONFIG_MV_ETH_BM
+	if (pp->pool_short == NULL) {
+		printk(KERN_ERR "%s: Error - short pool is null\n", __func__);
+		return;
+	}
+
+	printk(KERN_INFO "Short pool (%d) stack\n", pp->pool_short->pool);
+	mvStackStatus(pp->pool_short->stack, isPrintElements);
+#endif /* CONFIG_MV_ETH_BM */
+}
+
 
 void set_cpu_affinity(struct eth_port *pp, MV_U32 cpuAffinity, int group)
 {
@@ -283,36 +293,36 @@ void set_rxq_affinity(struct eth_port *pp, MV_U32 rxqAffinity, int group)
 		return;
 	}
 
-   for (cpu = 0; cpu < CONFIG_NR_CPUS; cpu++) {
+	for (cpu = 0; cpu < CONFIG_NR_CPUS; cpu++) {
 		if (!(MV_BIT_CHECK(pp->cpuMask, cpu)))
 			continue;
-	   tmpRxqAffinity = rxqAffinity;
+		tmpRxqAffinity = rxqAffinity;
 
-	   regVal = MV_REG_READ(NETA_CPU_MAP_REG(pp->port, cpu));
-	   cpuCtrl = pp->cpu_config[cpu];
+		regVal = MV_REG_READ(NETA_CPU_MAP_REG(pp->port, cpu));
+		cpuCtrl = pp->cpu_config[cpu];
 
-	   if (cpuCtrl->napiCpuGroup == group) {
-		   cpuInGroup = 1;
-		   /* init TXQ Access Enable bits */
-		   regVal = regVal & 0xff00;
-	   } else {
-		   cpuInGroup = 0;
+		if (cpuCtrl->napiCpuGroup == group) {
+			cpuInGroup = 1;
+			/* init TXQ Access Enable bits */
+			regVal = regVal & 0xff00;
+		} else {
+			cpuInGroup = 0;
 		}
 
-	   for (rxq = 0; rxq < CONFIG_MV_ETH_RXQ; rxq++) {
-		   /* set rxq affinity for this cpu */
-		   if (tmpRxqAffinity & 1) {
-			   if (cpuInGroup)
-				   regVal |= NETA_CPU_RXQ_ACCESS_MASK(rxq);
-			   else
-				   regVal &= ~NETA_CPU_RXQ_ACCESS_MASK(rxq);
+		for (rxq = 0; rxq < CONFIG_MV_ETH_RXQ; rxq++) {
+			/* set rxq affinity for this cpu */
+			if (tmpRxqAffinity & 1) {
+				if (cpuInGroup)
+					regVal |= NETA_CPU_RXQ_ACCESS_MASK(rxq);
+				else
+					regVal &= ~NETA_CPU_RXQ_ACCESS_MASK(rxq);
 			}
 			tmpRxqAffinity >>= 1;
-	   }
+		}
 
-	MV_REG_WRITE(NETA_CPU_MAP_REG(pp->port, cpu), regVal);
-	cpuCtrl->cpuRxqMask = regVal;
-   }
+		MV_REG_WRITE(NETA_CPU_MAP_REG(pp->port, cpu), regVal);
+		cpuCtrl->cpuRxqMask = regVal;
+	}
 }
 
 static int mv_eth_port_config_parse(struct eth_port *pp)
@@ -322,7 +332,7 @@ static int mv_eth_port_config_parse(struct eth_port *pp)
 	printk(KERN_ERR "\n");
 	if (pp == NULL) {
 		printk(KERN_ERR "  o mv_eth_port_config_parse: got NULL pp\n");
-		return -1;
+		return MV_ERROR;
 	}
 
 	switch (pp->port) {
@@ -340,20 +350,20 @@ static int mv_eth_port_config_parse(struct eth_port *pp)
 		break;
 	default:
 		printk(KERN_ERR "  o mv_eth_port_config_parse: got unknown port %d\n", pp->port);
-		return -1;
+		return MV_ERROR;
 	}
 
 	if (str != NULL) {
 		if ((!strcmp(str, "disconnected")) || (!strcmp(str, "Disconnected"))) {
 			printk(KERN_ERR "  o Port %d is disconnected from Linux netdevice\n", pp->port);
 			clear_bit(MV_ETH_F_CONNECT_LINUX_BIT, &(pp->flags));
-			return 0;
+			return MV_OK;
 		}
 	}
 
 	printk(KERN_ERR "  o Port %d is connected to Linux netdevice\n", pp->port);
 	set_bit(MV_ETH_F_CONNECT_LINUX_BIT, &(pp->flags));
-	return 0;
+	return MV_OK;
 }
 
 #ifdef ETH_SKB_DEBUG
@@ -445,7 +455,7 @@ static inline int mv_eth_skb_mh_add(struct sk_buff *skb, u16 mh)
 	skb->data -= MV_ETH_MH_SIZE;
 	*((u16 *) skb->data) = mh;
 
-	return 0;
+	return MV_OK;
 }
 
 void mv_eth_ctrl_txdone(int num)
@@ -474,7 +484,7 @@ int mv_eth_ctrl_flag(int port, u32 flag, u32 val)
 	if (flag == MV_ETH_F_MH)
 		mvNetaMhSet(pp->port, val ? MV_NETA_MH : MV_NETA_MH_NONE);
 
-	return 0;
+	return MV_OK;
 }
 
 int mv_eth_ctrl_port_buf_num_set(int port, int long_num, int short_num)
@@ -510,7 +520,7 @@ int mv_eth_ctrl_port_buf_num_set(int port, int long_num, int short_num)
 	pp->pool_short_num = short_num;
 #endif /* CONFIG_MV_ETH_BM_CPU */
 
-	return 0;
+	return MV_OK;
 }
 
 #ifdef CONFIG_MV_ETH_BM
@@ -567,7 +577,7 @@ int mv_eth_ctrl_pool_size_set(int pool, int pkt_size)
 	else
 		mvBmPoolBufSizeSet(pool, RX_BUF_SIZE(pkt_size));
 
-	return 0;
+	return MV_OK;
 }
 #endif /* CONFIG_MV_ETH_BM */
 
@@ -597,7 +607,7 @@ int mv_eth_ctrl_set_poll_rx_weight(int port, u32 weight)
 			cpuCtrl->napi->weight = pp->weight;
 	}
 
-	return 0;
+	return MV_OK;
 }
 
 int mv_eth_ctrl_rxq_size_set(int port, int rxq, int value)
@@ -625,7 +635,7 @@ int mv_eth_ctrl_rxq_size_set(int port, int rxq, int value)
 	pp->rxq_ctrl[rxq].rxq_size = value;
 
 	/* New RXQ will be created during mv_eth_start_internals */
-	return 0;
+	return MV_OK;
 }
 
 int mv_eth_ctrl_txq_size_set(int port, int txp, int txq, int value)
@@ -651,7 +661,7 @@ int mv_eth_ctrl_txq_size_set(int port, int txp, int txq, int value)
 	txq_ctrl->txq_size = value;
 
 	/* New TXQ will be created during mv_eth_start_internals */
-	return 0;
+	return MV_OK;
 }
 
 int mv_eth_ctrl_txq_mode_get(int port, int txp, int txq, int *value)
@@ -708,7 +718,7 @@ int mv_eth_ctrl_txq_cpu_own(int port, int txp, int txq, int add, int cpu)
 
 	mv_eth_txq_update_shared(txq_ctrl, pp);
 
-	return 0;
+	return MV_OK;
 }
 
 /* Set TXQ ownership to HWF from the RX port.  rxp=-1 - free TXQ ownership */
@@ -736,7 +746,7 @@ int mv_eth_ctrl_txq_hwf_own(int port, int txp, int txq, int rxp)
 
 	txq_ctrl->hwf_rxp = (MV_U8) rxp;
 
-	return 0;
+	return MV_OK;
 }
 
 /* set or clear shared bit for this txq, txp=1 for pon , 0 for gbe */
@@ -811,7 +821,7 @@ int mv_eth_ctrl_txq_cpu_def(int port, int txp, int txq, int cpu)
 	pp->txp = txp;
 	cpuCtrl->txq = txq;
 
-	return 0;
+	return MV_OK;
 }
 
 
@@ -888,7 +898,7 @@ int mv_eth_ctrl_tx_cmd(int port, u32 tx_cmd)
 
 	pp->hw_cmd = tx_cmd;
 
-	return 0;
+	return MV_OK;
 }
 
 int mv_eth_ctrl_tx_mh(int port, u16 mh)
@@ -900,7 +910,7 @@ int mv_eth_ctrl_tx_mh(int port, u16 mh)
 
 	pp->tx_mh = mh;
 
-	return 0;
+	return MV_OK;
 }
 
 #ifdef CONFIG_MV_ETH_TX_SPECIAL
@@ -922,22 +932,52 @@ void mv_eth_rx_special_proc_func(int port, void (*func)(int port, int rxq, struc
 {
 	struct eth_port *pp = mv_eth_port_by_id(port);
 
-	pp->rx_special_proc = func;
+	if (pp)
+		pp->rx_special_proc = func;
 }
 #endif /* CONFIG_MV_ETH_RX_SPECIAL */
 
-static inline u16 mv_eth_select_txq(struct net_device *dev,
-									struct sk_buff *skb)
+static inline u16 mv_eth_select_txq(struct net_device *dev, struct sk_buff *skb)
 {
 	struct eth_port *pp = MV_ETH_PRIV(dev);
 	return mv_eth_tx_policy(pp, skb);
 }
 
-#if defined(MV_ETH_PNC_LB) && defined(CONFIG_MV_ETH_PNC)
-static int mv_eth_set_features(struct net_device *dev, u32 features)
+/* Update network device features after changing MTU.	*/
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3, 4, 25)
+static u32 mv_eth_netdev_fix_features(struct net_device *dev, u32 features)
+#else
+static netdev_features_t mv_eth_netdev_fix_features(struct net_device *dev, netdev_features_t features)
+#endif
+{
+#ifdef CONFIG_MV_ETH_TX_CSUM_OFFLOAD
+	if (dev->mtu > MV_ETH_TX_CSUM_MAX_SIZE) {
+		if (features & (NETIF_F_IP_CSUM | NETIF_F_TSO)) {
+			features &= ~(NETIF_F_IP_CSUM | NETIF_F_TSO);
+			printk(KERN_ERR "%s: NETIF_F_IP_CSUM and NETIF_F_TSO not supported for mtu larger %d bytes\n",
+					dev->name, MV_ETH_TX_CSUM_MAX_SIZE);
+		}
+	}
+#endif /* CONFIG_MV_ETH_TX_CSUM_OFFLOAD */
+	return features;
+}
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3, 4, 25)
+static int mv_eth_netdev_set_features(struct net_device *dev, u32 features)
+#else
+static int mv_eth_netdev_set_features(struct net_device *dev, netdev_features_t features)
+#endif
 {
 	u32 changed = dev->features ^ features;
 
+/*
+	pr_info("%s: dev->features=0x%x, features=0x%x, changed=0x%x\n",
+		 __func__, dev->features, features, changed);
+*/
+	if (changed == 0)
+		return 0;
+
+#if defined(MV_ETH_PNC_LB) && defined(CONFIG_MV_ETH_PNC)
 	if (changed & NETIF_F_RXHASH) {
 		if (features & NETIF_F_RXHASH) {
 			dev->features |= NETIF_F_RXHASH;
@@ -951,9 +991,10 @@ static int mv_eth_set_features(struct net_device *dev, u32 features)
 			mvPncLbModeL4(LB_DISABLE_VALUE);
 		}
 	}
+#endif /* MV_ETH_PNC_LB && CONFIG_MV_ETH_PNC */
+
 	return 0;
 }
-#endif
 
 static const struct net_device_ops mv_eth_netdev_ops = {
 	.ndo_open = mv_eth_open,
@@ -964,9 +1005,8 @@ static const struct net_device_ops mv_eth_netdev_ops = {
 	.ndo_change_mtu = mv_eth_change_mtu,
 	.ndo_tx_timeout = mv_eth_tx_timeout,
 	.ndo_select_queue = mv_eth_select_txq,
-#if defined(MV_ETH_PNC_LB) && defined(CONFIG_MV_ETH_PNC)
-	.ndo_set_features = mv_eth_set_features,
-#endif
+	.ndo_fix_features = mv_eth_netdev_fix_features,
+	.ndo_set_features = mv_eth_netdev_set_features,
 };
 
 #ifdef CONFIG_MV_ETH_SWITCH
@@ -1229,13 +1269,15 @@ EXPORT_SYMBOL(mv_eth_pkt_print);
 static inline void mv_eth_rx_csum(struct eth_port *pp, struct neta_rx_desc *rx_desc, struct sk_buff *skb)
 {
 #if defined(CONFIG_MV_ETH_RX_CSUM_OFFLOAD)
-	if (pp->rx_csum_offload &&
-	    ((NETA_RX_L3_IS_IP4(rx_desc->status) ||
-	      NETA_RX_L3_IS_IP6(rx_desc->status)) && (rx_desc->status & NETA_RX_L4_CSUM_OK_MASK))) {
-		skb->csum = 0;
-		skb->ip_summed = CHECKSUM_UNNECESSARY;
-		STAT_DBG(pp->stats.rx_csum_hw++);
-		return;
+	if (pp->dev->features & NETIF_F_RXCSUM) {
+
+		if ((NETA_RX_L3_IS_IP4(rx_desc->status) ||
+	      NETA_RX_L3_IS_IP6(rx_desc->status)) && (rx_desc->status & NETA_RX_L4_CSUM_OK_MASK)) {
+			skb->csum = 0;
+			skb->ip_summed = CHECKSUM_UNNECESSARY;
+			STAT_DBG(pp->stats.rx_csum_hw++);
+			return;
+		}
 	}
 #endif /* CONFIG_MV_ETH_RX_CSUM_OFFLOAD */
 
@@ -1326,7 +1368,7 @@ int mv_eth_skb_recycle(struct sk_buff *skb)
 			mv_eth_skb_save(skb, "recycle");
 #endif /* ETH_SKB_DEBUG */
 
-		return 0;
+		return MV_OK;
 	}
 	STAT_DBG(pool->stats.skb_recycled_err++);
 
@@ -1341,11 +1383,11 @@ EXPORT_SYMBOL(mv_eth_skb_recycle);
 
 #endif /* CONFIG_NET_SKB_RECYCLE */
 
-static struct sk_buff *mv_eth_skb_alloc(struct bm_pool *pool, struct eth_pbuf *pkt)
+static struct sk_buff *mv_eth_skb_alloc(struct bm_pool *pool, struct eth_pbuf *pkt, gfp_t gfp_mask)
 {
 	struct sk_buff *skb;
 
-	skb = dev_alloc_skb(pool->pkt_size);
+	skb = __dev_alloc_skb(pool->pkt_size, gfp_mask);
 	if (!skb) {
 		STAT_ERR(pool->stats.skb_alloc_oom++);
 		return NULL;
@@ -1456,7 +1498,7 @@ inline struct eth_pbuf *mv_eth_pool_get(struct bm_pool *pool)
 	/* Try to allocate new pkt + skb */
 	pkt = mvOsMalloc(sizeof(struct eth_pbuf));
 	if (pkt) {
-		skb = mv_eth_skb_alloc(pool, pkt);
+		skb = mv_eth_skb_alloc(pool, pkt, GFP_ATOMIC);
 		if (!skb) {
 			mvOsFree(pkt);
 			pkt = NULL;
@@ -1477,7 +1519,7 @@ inline int mv_eth_refill(struct eth_port *pp, int rxq,
 		struct sk_buff *skb;
 
 		/* No recycle -  alloc new skb */
-		skb = mv_eth_skb_alloc(pool, pkt);
+		skb = mv_eth_skb_alloc(pool, pkt, GFP_ATOMIC);
 		if (!skb) {
 			mvOsFree(pkt);
 			pool->missed++;
@@ -1487,7 +1529,7 @@ inline int mv_eth_refill(struct eth_port *pp, int rxq,
 	}
 	mv_eth_rxq_refill(pp, rxq, pkt, pool, rx_desc);
 
-	return 0;
+	return MV_OK;
 }
 EXPORT_SYMBOL(mv_eth_refill);
 
@@ -1671,14 +1713,14 @@ static inline int mv_eth_rx(struct eth_port *pp, int rx_todo, int rxq)
 		}
 #endif /* CONFIG_MV_ETH_PNC && CONFIG_MV_ETH_RX_SPECIAL */
 
-#if defined(CONFIG_MV_ETH_NFP) || defined(CONFIG_MV_ETH_NFP_MODULE)
+#if defined(CONFIG_MV_ETH_NFP)
 		if (pp->flags & MV_ETH_F_NFP_EN) {
 			MV_STATUS status;
 
 			pkt->bytes = rx_bytes + MV_ETH_MH_SIZE;
 			pkt->offset = NET_SKB_PAD;
 
-			status = mv_eth_nfp_ops->mv_eth_nfp(pp, rxq, rx_desc, pkt, pool);
+			status = mv_eth_nfp(pp, rxq, rx_desc, pkt, pool);
 			if (status == MV_OK)
 				continue;
 			if (status == MV_FAIL) {
@@ -1687,7 +1729,7 @@ static inline int mv_eth_rx(struct eth_port *pp, int rx_todo, int rxq)
 			}
 			/* MV_TERMINATE - packet returned to slow path */
 		}
-#endif /* CONFIG_MV_ETH_NFP || CONFIG_MV_ETH_NFP_MODULE */
+#endif /* CONFIG_MV_ETH_NFP */
 
 		/* Linux processing */
 		skb = (struct sk_buff *)(pkt->osInfo);
@@ -1886,8 +1928,8 @@ static int mv_eth_tx(struct sk_buff *skb, struct net_device *dev)
 	if (pp->flags & MV_ETH_F_DBG_TX) {
 		printk(KERN_ERR "\n");
 		printk(KERN_ERR "%s - eth_tx_%lu: cpu=%d, in_intr=0x%lx, port=%d, txp=%d, txq=%d\n",
-			dev->name, dev->stats.tx_packets, smp_processor_id(), in_interrupt(),
-			pp->port, tx_spec.txp, tx_spec.txq);
+		       dev->name, dev->stats.tx_packets, smp_processor_id(),
+			in_interrupt(), pp->port, tx_spec.txp, tx_spec.txq);
 		printk(KERN_ERR "\t skb=%p, head=%p, data=%p, size=%d\n", skb, skb->head, skb->data, skb->len);
 		mv_eth_tx_desc_print(tx_desc);
 		/*mv_eth_skb_print(skb);*/
@@ -1931,9 +1973,8 @@ out:
 	}
 #endif /* CONFIG_MV_ETH_TXDONE_ISR */
 
-	if (txq_ctrl) {
+	if (txq_ctrl)
 		mv_eth_unlock(txq_ctrl, flags);
-	}
 
 	return NETDEV_TX_OK;
 }
@@ -1965,7 +2006,7 @@ static inline int mv_eth_tso_validate(struct sk_buff *skb, struct net_device *de
 		printk(KERN_ERR "***** ERROR: Protocol is not TCP over IP\n");
 		return 1;
 	}
-	return 0;
+	return MV_OK;
 }
 
 static inline int mv_eth_tso_build_hdr_desc(struct neta_tx_desc *tx_desc, struct eth_port *priv, struct sk_buff *skb,
@@ -1979,7 +2020,7 @@ static inline int mv_eth_tso_build_hdr_desc(struct neta_tx_desc *tx_desc, struct
 
 	data = mv_eth_extra_pool_get(priv);
 	if (!data)
-		return 0;
+		return MV_OK;
 
 	txq_ctrl->shadow_txq[txq_ctrl->shadow_txq_put_i] = ((MV_ULONG)data | MV_ETH_SHADOW_EXT);
 
@@ -2081,7 +2122,7 @@ int mv_eth_tx_tso(struct sk_buff *skb, struct net_device *dev,
 	printk(KERN_ERR "mv_eth_tx_tso_%d ENTER: skb=%p, total_len=%d\n", priv->stats.tx_tso, skb, skb->len);
 */
 	if (mv_eth_tso_validate(skb, dev))
-		return 0;
+		return MV_OK;
 
 	/* Calculate expected number of TX descriptors */
 	totalDescNum = skb_shinfo(skb)->gso_segs * 2 + skb_shinfo(skb)->nr_frags;
@@ -2093,7 +2134,7 @@ int mv_eth_tx_tso(struct sk_buff *skb, struct net_device *dev,
 					skb_shinfo(skb)->gso_segs);
 */
 		STAT_ERR(txq_ctrl->stats.txq_err++);
-		return 0;
+		return MV_OK;
 	}
 
 	total_len = skb->len;
@@ -2108,7 +2149,7 @@ int mv_eth_tx_tso(struct sk_buff *skb, struct net_device *dev,
 
 	if (frag_size < hdr_len) {
 		printk(KERN_ERR "***** ERROR: frag_size=%d, hdr_len=%d\n", frag_size, hdr_len);
-		return 0;
+		return MV_OK;
 	}
 
 	frag_size -= hdr_len;
@@ -2211,7 +2252,7 @@ outNoTxDesc:
 		mv_eth_shadow_dec_put(txq_ctrl);
 		mvNetaTxqPrevDescGet(txq_ctrl->q);
 	}
-	return 0;
+	return MV_OK;
 }
 #endif /* CONFIG_MV_ETH_TSO */
 
@@ -2376,6 +2417,37 @@ static void mv_eth_tx_frag_process(struct eth_port *pp, struct sk_buff *skb, str
 	}
 }
 
+/***********************************************************
+ * mv_eth_port_pools_free                                  *
+ *   per port - free all the buffers from pools		   *
+ *   disable pool if empty				   *
+ ***********************************************************/
+static int mv_eth_port_pools_free(int port)
+{
+	struct eth_port *pp;
+
+	pp = mv_eth_port_by_id(port);
+	if (!pp)
+		return MV_OK;
+
+	if (pp->pool_long) {
+		mv_eth_pool_free(pp->pool_long->pool, pp->pool_long_num);
+#ifndef CONFIG_MV_ETH_BM_CPU
+	}
+#else
+		if (pp->pool_long->buf_num == 0)
+			mvBmPoolDisable(pp->pool_long->pool);
+
+		/*empty pools*/
+		if (pp->pool_short && (pp->pool_long->pool != pp->pool_short->pool)) {
+			mv_eth_pool_free(pp->pool_short->pool, pp->pool_short_num);
+			if (pp->pool_short->buf_num == 0)
+				mvBmPoolDisable(pp->pool_short->pool);
+		}
+	}
+#endif /*CONFIG_MV_ETH_BM_CPU*/
+	return MV_OK;
+}
 
 /* Free "num" buffers from the pool */
 static int mv_eth_pool_free(int pool, int num)
@@ -2500,7 +2572,7 @@ static int mv_eth_pool_add(int pool, int buf_num)
 
 	if ((pool < 0) || (pool >= MV_ETH_BM_POOLS)) {
 		printk(KERN_ERR "%s: invalid pool number %d\n", __func__, pool);
-		return 0;
+		return MV_OK;
 	}
 
 	bm_pool = &mv_eth_pool[pool];
@@ -2509,7 +2581,7 @@ static int mv_eth_pool_add(int pool, int buf_num)
 	if (bm_pool->pkt_size == 0) {
 		printk(KERN_ERR "%s: invalid pool #%d state: pkt_size=%d, buf_size=%d, buf_num=%d\n",
 		       __func__, pool, bm_pool->pkt_size, RX_BUF_SIZE(bm_pool->pkt_size), bm_pool->buf_num);
-		return 0;
+		return MV_OK;
 	}
 
 	/* Insure buf_num is smaller than capacity */
@@ -2517,7 +2589,7 @@ static int mv_eth_pool_add(int pool, int buf_num)
 
 		printk(KERN_ERR "%s: can't add %d buffers into bm_pool=%d: capacity=%d, buf_num=%d\n",
 		       __func__, buf_num, pool, bm_pool->capacity, bm_pool->buf_num);
-		return 0;
+		return MV_OK;
 	}
 
 	MV_ETH_LOCK(&bm_pool->lock, flags);
@@ -2529,7 +2601,7 @@ static int mv_eth_pool_add(int pool, int buf_num)
 			break;
 		}
 
-		skb = mv_eth_skb_alloc(bm_pool, pkt);
+		skb = mv_eth_skb_alloc(bm_pool, pkt, GFP_KERNEL);
 		if (!skb) {
 			kfree(pkt);
 			break;
@@ -2615,7 +2687,7 @@ static MV_STATUS mv_eth_pool_create(int pool, int capacity)
 	memset(bm_pool, 0, sizeof(struct bm_pool));
 
 #ifdef CONFIG_MV_ETH_BM_CPU
-	bm_pool->bm_pool = mv_eth_bm_pool_create(pool, capacity, NULL);
+	bm_pool->bm_pool = mv_eth_bm_pool_create(pool, capacity, &bm_pool->physAddr);
 	if (bm_pool->bm_pool == NULL)
 		return MV_FAIL;
 #endif /* CONFIG_MV_ETH_BM_CPU */
@@ -2641,17 +2713,18 @@ static MV_STATUS mv_eth_pool_create(int pool, int capacity)
 irqreturn_t mv_eth_isr(int irq, void *dev_id)
 {
 	struct eth_port *pp = (struct eth_port *)dev_id;
-	struct napi_struct *napi = pp->cpu_config[smp_processor_id()]->napi;
+	int cpu = smp_processor_id();
+	struct napi_struct *napi = pp->cpu_config[cpu]->napi;
 
 #ifdef CONFIG_MV_ETH_DEBUG_CODE
 	if (pp->flags & MV_ETH_F_DBG_ISR) {
 		printk(KERN_ERR "%s: port=%d, cpu=%d, mask=0x%x, cause=0x%x\n",
-			__func__, pp->port, smp_processor_id(),
+			__func__, pp->port, cpu,
 			MV_REG_READ(NETA_INTR_NEW_MASK_REG(pp->port)), MV_REG_READ(NETA_INTR_NEW_CAUSE_REG(pp->port)));
 	}
 #endif /* CONFIG_MV_ETH_DEBUG_CODE */
 
-	STAT_INFO(pp->stats.irq++);
+	STAT_INFO(pp->stats.irq[cpu]++);
 
 	/* Mask all interrupts */
 	MV_REG_WRITE(NETA_INTR_NEW_MASK_REG(pp->port), 0);
@@ -2663,9 +2736,10 @@ irqreturn_t mv_eth_isr(int irq, void *dev_id)
 		/* schedule the work (rx+txdone+link) out of interrupt contxet */
 		__napi_schedule(napi);
 	} else {
-		STAT_INFO(pp->stats.irq_err++);
+		STAT_INFO(pp->stats.irq_err[cpu]++);
 #ifdef CONFIG_MV_ETH_DEBUG_CODE
-		printk(KERN_ERR "mv_eth_isr ERROR: port=%d, cpu=%d\n", pp->port, smp_processor_id());
+		pr_debug("%s: IRQ=%d, port=%d, cpu=%d - NAPI already scheduled\n",
+			__func__, irq, pp->port, cpu);
 #endif /* CONFIG_MV_ETH_DEBUG_CODE */
 	}
 	return IRQ_HANDLED;
@@ -2947,7 +3021,7 @@ static MV_STATUS mv_eth_bm_pools_init(void)
 		mv_eth_pool[i].pkt_size = 0;
 #endif /* CONFIG_MV_ETH_BM */
 	}
-	return 0;
+	return MV_OK;
 }
 
 /* Note: call this function only after mv_eth_ports_num is initialized */
@@ -3066,7 +3140,7 @@ static int mv_eth_load_network_interfaces(MV_U32 portMask, MV_U32 cpuMask,
 
 	mv_net_devs_num = dev_i;
 
-	return 0;
+	return MV_OK;
 }
 
 
@@ -3100,140 +3174,346 @@ int mv_eth_resume_network_interfaces(struct eth_port *pp)
 		mvNetaTxqCpuMaskSet(pp->port, pp->cpu_config[cpu]->cpuTxqMask, cpu);
 	}
 
-	return 0;
+	return MV_OK;
 }
 
-/***********************************************************
- * mv_eth_port_resume                                      *
- ***********************************************************/
-
-int mv_eth_port_resume(int port)
+#ifdef CONFIG_MV_ETH_BM
+int     mv_eth_bm_pool_restore(struct bm_pool *bm_pool)
 {
-	struct eth_port *pp;
-	int cpu;
-
-	pp = mv_eth_port_by_id(port);
+		MV_UNIT_WIN_INFO        winInfo;
+		MV_STATUS               status;
+		int pool = bm_pool->pool;
 
-	if (pp == NULL) {
-		printk(KERN_ERR "%s: pp == NULL, port=%d\n", __func__, port);
-		return -1;
-	}
+		mvBmPoolInit(bm_pool->pool, bm_pool->bm_pool, bm_pool->physAddr, bm_pool->capacity);
+		status = mvCtrlAddrWinInfoGet(&winInfo, bm_pool->physAddr);
 
-	if (!(pp->flags & MV_ETH_F_SUSPEND)) {
-		printk(KERN_ERR "%s: port %d is not suspend.\n", __func__, port);
-		return -1;
-	}
-	mvNetaPortPowerUp(port, mvBoardIsPortInSgmii(port), mvBoardIsPortInRgmii(port));
+		if (status != MV_OK) {
+			printk(KERN_ERR "%s: Can't map BM pool #%d. phys_addr=0x%x, status=%d\n",
+				__func__, bm_pool->pool, (unsigned)bm_pool->physAddr, status);
+			mvOsIoCachedFree(NULL, sizeof(MV_U32) *  bm_pool->capacity, bm_pool->physAddr, bm_pool->bm_pool, 0);
+			return MV_ERROR;
+		}
+		mvBmPoolTargetSet(pool, winInfo.targetId, winInfo.attrib);
+		mvBmPoolEnable(pool);
 
-	mv_eth_win_init(port);
+		return MV_OK;
+}
+#endif /*CONFIG_MV_ETH_BM*/
 
-	mv_eth_resume_network_interfaces(pp);
 
-#ifndef CONFIG_MV_ETH_PNC
-	if (mvNetaMacAddrSet(port, pp->dev->dev_addr, CONFIG_MV_ETH_RXQ_DEF) != MV_OK) {
-		printk(KERN_ERR "%s: ethSetMacAddr failed\n", pp->dev->name);
-		return -1;
-	}
-#endif /* CONFIG_MV_ETH_PNC */
+/* Refill port pools */
+static int mv_eth_resume_port_pools(struct eth_port *pp)
+{
+	int num;
 
-	for_each_possible_cpu(cpu)
-		pp->cpu_config[cpu]->causeRxTx = 0;
+	if (!pp)
+		return -ENODEV;
 
-	set_bit(MV_ETH_F_STARTED_BIT, &(pp->flags));
 
-	mv_eth_restore_registers(pp, pp->dev->mtu);
+	/* fill long pool */
+	if (pp->pool_long) {
+		num = mv_eth_pool_add(pp->pool_long->pool, pp->pool_long_num);
 
-	if (pp->flags & MV_ETH_F_STARTED_OLD) {
-		mv_eth_resume_internals(pp, pp->dev->mtu);
-		clear_bit(MV_ETH_F_STARTED_OLD_BIT, &(pp->flags));
-		if (pp->flags & MV_ETH_F_CONNECT_LINUX) {
-			mv_eth_interrupts_unmask(pp);
-			smp_call_function_many(cpu_online_mask, (smp_call_func_t)mv_eth_interrupts_unmask, (void *)pp, 1);
+		if (num != pp->pool_long_num) {
+			printk(KERN_ERR "%s FAILED long: pool=%d, pkt_size=%d, only %d of %d allocated\n",
+			       __func__, pp->pool_long->pool, pp->pool_long->pkt_size, num, pp->pool_long_num);
+			return MV_ERROR;
 		}
-	} else
-		clear_bit(MV_ETH_F_STARTED_BIT, &(pp->flags));
-
-
-	clear_bit(MV_ETH_F_SUSPEND_BIT, &(pp->flags));
-	printk(KERN_NOTICE "port %d resumed.\n", port);
 
-	return 0;
-}
+#ifndef CONFIG_MV_ETH_BM_CPU
+	} /*fill long pool */
+#else
+		mvNetaBmPoolBufSizeSet(pp->port, pp->pool_long->pool, RX_BUF_SIZE(pp->pool_long->pkt_size));
+	}
 
-/***********************************************************
- * mv_eth_win_init --                                      *
- *   Win initilization                                     *
- ***********************************************************/
-void 	mv_eth_win_init(int port)
-{
+	if (pp->pool_short) {
+		if (pp->pool_short->pool != pp->pool_long->pool) {
+				/* fill short pool */
+				num = mv_eth_pool_add(pp->pool_short->pool, pp->pool_short_num);
+				if (num != pp->pool_short_num) {
+					printk(KERN_ERR "%s FAILED short: pool=%d, pkt_size=%d - %d of %d buffers added\n",
+					   __func__, pp->pool_short->pool, pp->pool_short->pkt_size, num, pp->pool_short_num);
+					return MV_ERROR;
+				}
 
-	MV_UNIT_WIN_INFO addrWinMap[MAX_TARGETS + 1];
-	MV_STATUS status;
-	int i;
+				mvNetaBmPoolBufSizeSet(pp->port, pp->pool_short->pool, RX_BUF_SIZE(pp->pool_short->pkt_size));
 
-	status = mvCtrlAddrWinMapBuild(addrWinMap, MAX_TARGETS + 1);
-	if (status != MV_OK)
-		return;
+		} else {
 
-	for (i = 0; i < MAX_TARGETS; i++) {
-		if (addrWinMap[i].enable == MV_FALSE)
-			continue;
+			int dummy_short_pool = (pp->pool_short->pool + 1) % MV_BM_POOLS;
+			/* To disable short pool we choose unused pool and set pkt size to 0 (buffer size = pkt offset) */
+			mvNetaBmPoolBufSizeSet(pp->port, dummy_short_pool, NET_SKB_PAD);
 
-#ifdef CONFIG_MV_SUPPORT_L2_DEPOSIT
-		/* Setting DRAM windows attribute to :
-		   0x3 - Shared transaction + L2 write allocate (L2 Deposit) */
-		if (MV_TARGET_IS_DRAM(i)) {
-			addrWinMap[i].attrib &= ~(0x30);
-			addrWinMap[i].attrib |= 0x30;
 		}
-#endif
 	}
-	mvNetaWinInit(port, addrWinMap);
-	return;
+
+#endif /* CONFIG_MV_ETH_BM_CPU */
+
+	return MV_OK;
 }
 
-/***********************************************************
- * mv_eth_port_suspend                                     *
- *   main driver initialization. loading the interfaces.   *
- ***********************************************************/
-int mv_eth_port_suspend(int port)
+static int mv_eth_resume_rxq_txq(struct eth_port *pp, int mtu)
 {
-	struct eth_port *pp;
+	int rxq, txp, txq = 0;
 
-	/* NAPI DEBUG */
 
-	pp = mv_eth_port_by_id(port);
-	if (!pp)
-		return 0;
+	if (mvBoardIsPortInSgmii(pp->port))
+		MV_REG_WRITE(SGMII_SERDES_CFG_REG(pp->port), pp->sgmii_serdes);
 
-	if (pp->flags & MV_ETH_F_SUSPEND) {
-		printk(KERN_ERR "%s: port %d is allready suspend.\n", __func__, port);
-		return -1;
-	}
+	for (txp = 0; txp < pp->txp_num; txp++)
+		mvNetaTxpReset(pp->port, txp);
 
-	if (mvBoardIsPortInSgmii(pp->port))
-		pp->sgmii_serdes = MV_REG_READ(SGMII_SERDES_CFG_REG(port));
+	mvNetaRxReset(pp->port);
 
-	if (pp->flags & MV_ETH_F_STARTED) {
-		set_bit(MV_ETH_F_STARTED_OLD_BIT, &(pp->flags));
-		clear_bit(MV_ETH_F_STARTED_BIT, &(pp->flags));
-		mv_eth_suspend_internals(pp);
-	} else
-		clear_bit(MV_ETH_F_STARTED_OLD_BIT, &(pp->flags));
+	for (rxq = 0; rxq < CONFIG_MV_ETH_RXQ; rxq++) {
 
-	/* Reset RX port, free the empty buffers form queue */
-	mv_eth_rx_reset(port);
+		if (pp->rxq_ctrl[rxq].q) {
+			/* Set Rx descriptors queue starting address */
+			mvNetaRxqAddrSet(pp->port, rxq,  pp->rxq_ctrl[rxq].rxq_size);
 
-	set_bit(MV_ETH_F_SUSPEND_BIT, &(pp->flags));
-	printk(KERN_NOTICE "port %d suspend.\n", port);
-	return 0;
-}
+			/* Set Offset */
+			mvNetaRxqOffsetSet(pp->port, rxq, NET_SKB_PAD);
+
+			/* Set coalescing pkts and time */
+			mv_eth_rx_ptks_coal_set(pp->port, rxq, pp->rxq_ctrl[rxq].rxq_pkts_coal);
+			mv_eth_rx_time_coal_set(pp->port, rxq, pp->rxq_ctrl[rxq].rxq_time_coal);
+
+
+#if defined(CONFIG_MV_ETH_BM_CPU)
+			/* Enable / Disable - BM support */
+			if (pp->pool_long && pp->pool_short) {
+
+				if (pp->pool_short->pool == pp->pool_long->pool) {
+					int dummy_short_pool = (pp->pool_short->pool + 1) % MV_BM_POOLS;
+
+					/* To disable short pool we choose unused pool and set pkt size to 0 (buffer size = pkt offset) */
+					mvNetaRxqBmEnable(pp->port, rxq, dummy_short_pool, pp->pool_long->pool);
+				} else
+					mvNetaRxqBmEnable(pp->port, rxq, pp->pool_short->pool, pp->pool_long->pool);
+			}
+#else
+			/* Fill RXQ with buffers from RX pool */
+			mvNetaRxqBufSizeSet(pp->port, rxq, RX_BUF_SIZE(pp->pool_long->pkt_size));
+			mvNetaRxqBmDisable(pp->port, rxq);
+#endif /* CONFIG_MV_ETH_BM_CPU */
+			if (mvNetaRxqFreeDescNumGet(pp->port, rxq) == 0)
+				mv_eth_rxq_fill(pp, rxq, pp->rxq_ctrl[rxq].rxq_size);
+		}
+	}
+
+	for (txp = 0; txp < pp->txp_num; txp++) {
+		for (txq = 0; txq < CONFIG_MV_ETH_TXQ; txq++) {
+			struct tx_queue *txq_ctrl = &pp->txq_ctrl[txp * CONFIG_MV_ETH_TXQ + txq];
+
+			if (txq_ctrl->q != NULL) {
+				mvNetaTxqAddrSet(pp->port, txq_ctrl->txp, txq_ctrl->txq, txq_ctrl->txq_size);
+				mv_eth_tx_done_ptks_coal_set(pp->port, txp, txq,
+							pp->txq_ctrl[txp * CONFIG_MV_ETH_TXQ + txq].txq_done_pkts_coal);
+			}
+			mvNetaTxqBandwidthSet(pp->port, txp, txq);
+		}
+		mvNetaTxpRateMaxSet(pp->port, txp);
+		mvNetaTxpMaxTxSizeSet(pp->port, txp, RX_PKT_SIZE(mtu));
+	}
+
+	return MV_OK;
+}
+
+/**********************************************************
+ * mv_eth_pnc_resume                                      *
+ **********************************************************/
+#ifdef CONFIG_MV_ETH_PNC
+static void mv_eth_pnc_resume(void)
+{
+	/* TODO - in clock standby ,DO we want to keep old pnc TCAM/SRAM entries ? */
+	if (wol_ports_bmp != 0)
+		return;
+
+	/* Not in WOL, clock standby or suspend to ram mode*/
+	tcam_hw_init();
+
+	if (pnc_default_init())
+		printk(KERN_ERR "%s: Warning PNC init failed\n", __func__);
+
+	/* TODO: load balancing resume */
+}
+#endif /* CONFIG_MV_ETH_PNC */
 
 /***********************************************************
- * mv_eth_pm_mode_set --                                   *
- *   set pm_mode. (power menegment mod)			   *
+ * mv_eth_port_resume                                      *
  ***********************************************************/
-int	mv_eth_pm_mode_set(int port, int mode)
+
+int mv_eth_port_resume(int port)
+{
+	struct eth_port *pp;
+	int cpu;
+
+	pp = mv_eth_port_by_id(port);
+
+	if (pp == NULL) {
+		printk(KERN_ERR "%s: pp == NULL, port=%d\n", __func__, port);
+		return  MV_ERROR;
+	}
+
+	if (!(pp->flags & MV_ETH_F_SUSPEND)) {
+		printk(KERN_ERR "%s: port %d is not suspend.\n", __func__, port);
+		return MV_ERROR;
+	}
+	mvNetaPortPowerUp(port, mvBoardIsPortInSgmii(port), mvBoardIsPortInRgmii(port));
+
+	mv_eth_win_init(port);
+
+	mv_eth_resume_network_interfaces(pp);
+
+	/* only once for all ports*/
+	if (pm_flag == 0) {
+
+#ifdef CONFIG_MV_ETH_BM
+		{
+			struct bm_pool *ppool;
+			int pool;
+
+			mvBmControl(MV_START);
+
+			mvBmRegsInit();
+
+			for (pool = 0; pool < MV_ETH_BM_POOLS; pool++) {
+				ppool = &mv_eth_pool[pool];
+				if (mv_eth_bm_pool_restore(ppool)) {
+					printk(KERN_ERR "%s: port #%d pool #%d resrote failed.\n",
+						__func__, port, pool);
+					return MV_ERROR;
+				}
+			}
+		}
+#endif /*CONFIG_MV_ETH_BM*/
+
+#ifdef CONFIG_MV_ETH_PNC
+		mv_eth_pnc_resume();
+#endif /* CONFIG_MV_ETH_PNC */
+
+		pm_flag = 1;
+	}
+
+	if (pp->flags & MV_ETH_F_STARTED_OLD)
+		(*pp->dev->netdev_ops->ndo_set_rx_mode)(pp->dev);
+
+	for_each_possible_cpu(cpu)
+		pp->cpu_config[cpu]->causeRxTx = 0;
+
+	set_bit(MV_ETH_F_STARTED_BIT, &(pp->flags));
+
+	mv_eth_resume_port_pools(pp);
+
+	mv_eth_resume_rxq_txq(pp, pp->dev->mtu);
+
+	if (pp->flags & MV_ETH_F_STARTED_OLD) {
+		mv_eth_resume_internals(pp, pp->dev->mtu);
+		clear_bit(MV_ETH_F_STARTED_OLD_BIT, &(pp->flags));
+		if (pp->flags & MV_ETH_F_CONNECT_LINUX) {
+			mv_eth_interrupts_unmask(pp);
+			smp_call_function_many(cpu_online_mask, (smp_call_func_t)mv_eth_interrupts_unmask, (void *)pp, 1);
+		}
+	} else
+		clear_bit(MV_ETH_F_STARTED_BIT, &(pp->flags));
+
+
+	clear_bit(MV_ETH_F_SUSPEND_BIT, &(pp->flags));
+
+	printk(KERN_NOTICE "Exit suspend mode on port #%d\n", port);
+
+	return MV_OK;
+}
+
+
+
+/***********************************************************
+ * mv_eth_win_init --                                      *
+ *   Win initilization                                     *
+ ***********************************************************/
+void mv_eth_win_init(int port)
+{
+	MV_UNIT_WIN_INFO addrWinMap[MAX_TARGETS + 1];
+	MV_STATUS status;
+	int i;
+
+	status = mvCtrlAddrWinMapBuild(addrWinMap, MAX_TARGETS + 1);
+	if (status != MV_OK)
+		return;
+
+	for (i = 0; i < MAX_TARGETS; i++) {
+		if (addrWinMap[i].enable == MV_FALSE)
+			continue;
+
+#ifdef CONFIG_MV_SUPPORT_L2_DEPOSIT
+		/* Setting DRAM windows attribute to :
+		   0x3 - Shared transaction + L2 write allocate (L2 Deposit) */
+		if (MV_TARGET_IS_DRAM(i)) {
+			addrWinMap[i].attrib &= ~(0x30);
+			addrWinMap[i].attrib |= 0x30;
+		}
+#endif
+	}
+	mvNetaWinInit(port, addrWinMap);
+	return;
+}
+
+/***********************************************************
+ * mv_eth_port_suspend                                     *
+ *   main driver initialization. loading the interfaces.   *
+ ***********************************************************/
+int mv_eth_port_suspend(int port)
+{
+	struct eth_port *pp;
+
+
+	pp = mv_eth_port_by_id(port);
+	if (!pp)
+		return MV_OK;
+
+	if (pp->flags & MV_ETH_F_SUSPEND) {
+		printk(KERN_ERR "%s: port %d is allready suspend.\n", __func__, port);
+		return MV_ERROR;
+	}
+
+	if (mvBoardIsPortInSgmii(pp->port))
+		pp->sgmii_serdes = MV_REG_READ(SGMII_SERDES_CFG_REG(port));
+
+	if (pp->flags & MV_ETH_F_STARTED) {
+		set_bit(MV_ETH_F_STARTED_OLD_BIT, &(pp->flags));
+		clear_bit(MV_ETH_F_STARTED_BIT, &(pp->flags));
+		mv_eth_suspend_internals(pp);
+	} else
+		clear_bit(MV_ETH_F_STARTED_OLD_BIT, &(pp->flags));
+
+
+#ifdef CONFIG_MV_ETH_HWF
+	mvNetaHwfEnable(pp->port, 0);
+#else
+	{
+		int txp;
+		/* Reset TX port, transmit all pending packets */
+		for (txp = 0; txp < pp->txp_num; txp++)
+			mv_eth_txp_reset(pp->port, txp);
+	}
+#endif  /* !CONFIG_MV_ETH_HWF */
+
+	/* Reset RX port, free the empty buffers form queue */
+	mv_eth_rx_reset(pp->port);
+
+	mv_eth_port_pools_free(port);
+
+	set_bit(MV_ETH_F_SUSPEND_BIT, &(pp->flags));
+
+	printk(KERN_NOTICE "Enter suspend mode on port #%d\n", port);
+	return MV_OK;
+}
+
+/***********************************************************
+ * mv_eth_wol_mode_set --                                   *
+ *   set wol_mode. (power menegment mod)		    *
+ ***********************************************************/
+int	mv_eth_wol_mode_set(int port, int mode)
 {
 	struct eth_port *pp = mv_eth_port_by_id(port);
 
@@ -3242,7 +3522,7 @@ int	mv_eth_pm_mode_set(int port, int mode)
 		return -EINVAL;
 	}
 
-	if ((mode < MV_ETH_PM_WOL) || (mode > MV_ETH_PM_LAST)) {
+	if ((mode < 0) || (mode > 1)) {
 		printk(KERN_ERR "%s: mode = %d, Invalid value.\n", __func__, mode);
 		return -EINVAL;
 	}
@@ -3251,9 +3531,14 @@ int	mv_eth_pm_mode_set(int port, int mode)
 		printk(KERN_ERR "Port %d must resumed before\n", port);
 		return -EINVAL;
 	}
-	pp->pm_mode = mode;
+	pp->wol_mode = mode;
 
-	return 0;
+	if (mode)
+		wol_ports_bmp |= (1 << port);
+	else
+		wol_ports_bmp &= ~(1 << port);
+
+	return MV_OK;
 }
 
 /***********************************************************
@@ -3366,6 +3651,7 @@ static int mv_eth_probe(struct platform_device *pdev)
 	mv_eth_initialized = 1;
 
 	return 0;
+
 oom:
 	if (mv_eth_ports)
 		mvOsFree(mv_eth_ports);
@@ -3448,7 +3734,7 @@ static int mv_eth_config_get(struct eth_port *pp, MV_U8 *mac_addr)
 
 	default:
 		printk(KERN_ERR "eth_get_config: Unexpected port number %d\n", pp->port);
-		return -1;
+		return MV_ERROR;
 	}
 	if ((mac_str != NULL) && (mac_addr != NULL))
 		mvMacStrToHex(mac_str, mac_addr);
@@ -3481,7 +3767,7 @@ struct net_device *mv_eth_netdev_init(struct eth_port *pp, int mtu, u8 *mac,
 	int cpu, i;
 	struct net_device *dev;
 	struct eth_dev_priv *dev_priv;
-	struct cpu_ctrl *cpuCtrl;
+	struct cpu_ctrl	*cpuCtrl;
 
 	dev = alloc_etherdev_mq(sizeof(struct eth_dev_priv), CONFIG_MV_ETH_TXQ);
 	if (!dev)
@@ -3546,18 +3832,13 @@ struct net_device *mv_eth_netdev_init(struct eth_port *pp, int mtu, u8 *mac,
 		for (i = 0; i < CONFIG_MV_ETH_NAPI_GROUPS; i++)
 			netif_napi_add(dev, pp->napiGroup[i], mv_eth_poll, pp->weight);
 	}
-	for_each_possible_cpu(cpu) {
-		cpuCtrl = pp->cpu_config[cpu];
-		cpuCtrl->tx_done_timer.data = (unsigned long)dev;
-		cpuCtrl->cleanup_timer.data = (unsigned long)dev;
-	}
 
 	SET_NETDEV_DEV(dev, &pdev->dev);
 
 	dev->priv_flags |= IFF_UNICAST_FLT;
 
 	if (pp->flags & MV_ETH_F_CONNECT_LINUX) {
-		mv_eth_netdev_set_features(dev);
+		mv_eth_netdev_init_features(dev);
 		if (register_netdev(dev)) {
 			printk(KERN_ERR "failed to register %s\n", dev->name);
 			free_netdev(dev);
@@ -3598,7 +3879,6 @@ void mv_eth_netdev_update(int dev_index, struct eth_port *pp)
 {
 	int i;
 	struct eth_dev_priv *dev_priv;
-	struct cpu_ctrl *cpuCtrl;
 
 #ifdef CONFIG_MV_ETH_SWITCH
 	struct eth_netdev *eth_netdev_priv;
@@ -3616,12 +3896,6 @@ void mv_eth_netdev_update(int dev_index, struct eth_port *pp)
 			netif_napi_add(dev, pp->napiGroup[i], mv_eth_poll, pp->weight);
 	}
 
-	for_each_possible_cpu(i) {
-		cpuCtrl = pp->cpu_config[i];
-		cpuCtrl->tx_done_timer.data = (unsigned long)dev;
-		cpuCtrl->cleanup_timer.data = (unsigned long)dev;
-	}
-
 	printk(KERN_ERR "    o %s, ifindex = %d, GbE port = %d", dev->name, dev->ifindex, pp->port);
 
 #ifdef CONFIG_MV_ETH_SWITCH
@@ -3713,7 +3987,7 @@ int mv_eth_hal_init(struct eth_port *pp)
 	pp->advertise_cfg = 0x2f;
 #endif /* CONFIG_MV_ETH_TOOL */
 
-	return 0;
+	return MV_OK;
 oom:
 	printk(KERN_ERR "%s: port=%d: out of memory\n", __func__, pp->port);
 	return -ENODEV;
@@ -3788,9 +4062,17 @@ void mv_eth_config_show(void)
 	printk(KERN_ERR "  o Transmit checksum offload supported\n");
 #endif
 
-#if defined(CONFIG_MV_ETH_NFP) || defined(CONFIG_MV_ETH_NFP_MODULE)
-	printk(KERN_ERR "  o Network Fast Processing (NFP) supported\n");
-#endif /* CONFIG_MV_ETH_NFP || CONFIG_MV_ETH_NFP_MODULE */
+#if defined(CONFIG_MV_ETH_NFP)
+	printk(KERN_ERR "  o NFP is supported\n");
+#endif /* CONFIG_MV_ETH_NFP */
+
+#if defined(CONFIG_MV_ETH_NFP_HOOKS)
+	printk(KERN_ERR "  o NFP Hooks are supported\n");
+#endif /* CONFIG_MV_ETH_NFP_HOOKS */
+
+#if defined(CONFIG_MV_ETH_NFP_EXT)
+	printk(KERN_ERR "  o NFP External drivers supported: up to %d interfaces\n", NFP_EXT_NUM);
+#endif /* CONFIG_MV_ETH_NFP_EXT */
 
 #ifdef CONFIG_MV_ETH_STAT_ERR
 	printk(KERN_ERR "  o Driver ERROR statistics enabled\n");
@@ -3817,45 +4099,39 @@ void mv_eth_config_show(void)
 }
 
 /* Set network device features on initialization. Take into account default compile time configuration. */
-static void mv_eth_netdev_set_features(struct net_device *dev)
+static void mv_eth_netdev_init_features(struct net_device *dev)
 {
-	dev->features 	 = NETIF_F_SG  | NETIF_F_LLTX;
-	dev->hw_features = NETIF_F_IP_CSUM | NETIF_F_SG;
+	dev->features |= NETIF_F_SG | NETIF_F_LLTX;
+	dev->hw_features |= NETIF_F_SG;
 
 #ifdef CONFIG_MV_ETH_PNC_L3_FLOW
 	dev->features |= NETIF_F_NTUPLE;
+	dev->hw_features |= NETIF_F_NTUPLE;
 #endif /* CONFIG_MV_ETH_PNC_L3_FLOW */
+
 #if defined(MV_ETH_PNC_LB) && defined(CONFIG_MV_ETH_PNC)
 	dev->hw_features |= NETIF_F_RXHASH;
 #endif
 
+#ifdef CONFIG_MV_ETH_TX_CSUM_OFFLOAD
+	dev->hw_features |= NETIF_F_IP_CSUM;
 #ifdef CONFIG_MV_ETH_TX_CSUM_OFFLOAD_DEF
-	if (dev->mtu <= MV_ETH_TX_CSUM_MAX_SIZE)
-		dev->features |= NETIF_F_IP_CSUM;
+	dev->features |= NETIF_F_IP_CSUM;
 #endif /* CONFIG_MV_ETH_TX_CSUM_OFFLOAD_DEF */
-
-#ifdef CONFIG_MV_ETH_TSO_DEF
-	if (dev->features & NETIF_F_IP_CSUM)
-		dev->features |= NETIF_F_TSO;
-#endif /* CONFIG_MV_ETH_TSO_DEF */
-
-}
-
-/* Update network device features after changing MTU.	*/
-static void mv_eth_netdev_update_features(struct net_device *dev)
-{
-#ifdef CONFIG_MV_ETH_TX_CSUM_OFFLOAD
-	if (dev->mtu > MV_ETH_TX_CSUM_MAX_SIZE) {
-		dev->features &= ~NETIF_F_IP_CSUM;
-		printk(KERN_ERR "Removing NETIF_F_IP_CSUM in device %s features\n", dev->name);
-	}
 #endif /* CONFIG_MV_ETH_TX_CSUM_OFFLOAD */
 
+#ifdef CONFIG_MV_ETH_RX_CSUM_OFFLOAD
+	dev->hw_features |= NETIF_F_RXCSUM;
+#ifdef CONFIG_MV_ETH_RX_CSUM_OFFLOAD_DEF
+	dev->features |= NETIF_F_RXCSUM;
+#endif /* CONFIG_MV_ETH_RX_CSUM_OFFLOAD_DEF */
+#endif /* CONFIG_MV_ETH_RX_CSUM_OFFLOAD */
+
 #ifdef CONFIG_MV_ETH_TSO
-	if (!(dev->features & NETIF_F_IP_CSUM)) {
-		dev->features &= ~NETIF_F_TSO;
-		printk(KERN_ERR "Removing NETIF_F_TSO in device %s features\n", dev->name);
-	}
+	dev->hw_features |= NETIF_F_TSO;
+#ifdef CONFIG_MV_ETH_TSO_DEF
+	dev->features |= NETIF_F_TSO;
+#endif /* CONFIG_MV_ETH_TSO_DEF */
 #endif /* CONFIG_MV_ETH_TSO */
 }
 
@@ -3864,19 +4140,19 @@ int mv_eth_napi_set_cpu_affinity(int port, int group, int affinity)
 	struct eth_port *pp = mv_eth_port_by_id(port);
 	if (pp == NULL) {
 		printk(KERN_ERR "%s: pp == NULL, port=%d\n", __func__, port);
-		return -1;
+		return MV_ERROR;
 	}
 
 	if (group >= CONFIG_MV_ETH_NAPI_GROUPS) {
 		printk(KERN_ERR "%s: group number is higher than %d\n", __func__, CONFIG_MV_ETH_NAPI_GROUPS-1);
-		return -1;
+		return MV_ERROR;
 		}
 	if (pp->flags & MV_ETH_F_STARTED) {
 		printk(KERN_ERR "Port %d must be stopped before\n", port);
 		return -EINVAL;
 	}
 	set_cpu_affinity(pp, affinity, group);
-	return 0;
+	return MV_OK;
 
 }
 void handle_group_affinity(int port)
@@ -3912,7 +4188,6 @@ void handle_group_affinity(int port)
 		set_cpu_affinity(pp, group_cpu_affinity[group], group);
 	for (group = 0; group < CONFIG_MV_ETH_NAPI_GROUPS; group++)
 		set_rxq_affinity(pp, rxq_affinity[group], group);
-
 }
 
 int	mv_eth_napi_set_rxq_affinity(int port, int group, int rxqAffinity)
@@ -3925,7 +4200,7 @@ int	mv_eth_napi_set_rxq_affinity(int port, int group, int rxqAffinity)
 	}
 	if (group >= CONFIG_MV_ETH_NAPI_GROUPS) {
 		printk(KERN_ERR "%s: group number is higher than %d\n", __func__, CONFIG_MV_ETH_NAPI_GROUPS-1);
-		return -1;
+		return MV_ERROR;
 		}
 	if (pp->flags & MV_ETH_F_STARTED) {
 		printk(KERN_ERR "Port %d must be stopped before\n", port);
@@ -4025,7 +4300,7 @@ static int mv_eth_rxq_fill(struct eth_port *pp, int rxq, int num)
 	rx_ctrl = pp->rxq_ctrl[rxq].q;
 	if (!rx_ctrl) {
 		printk(KERN_ERR "%s: rxq %d is not initialized\n", __func__, rxq);
-		return 0;
+		return MV_OK;
 	}
 
 	for (i = 0; i < num; i++) {
@@ -4071,7 +4346,7 @@ static int mv_eth_txq_create(struct eth_port *pp, struct tx_queue *txq_ctrl)
 	mvNetaHwfTxqInit(pp->port, txq_ctrl->txp, txq_ctrl->txq);
 #endif /* CONFIG_MV_ETH_HWF */
 
-	return 0;
+	return MV_OK;
 
 no_mem:
 	mv_eth_txq_delete(pp, txq_ctrl);
@@ -4108,7 +4383,7 @@ static int mv_force_port_link_speed_fc(int port, MV_ETH_PORT_SPEED port_speed, i
 			return -EIO;
 		}
 	}
-	return 0;
+	return MV_OK;
 }
 
 static void mv_eth_txq_delete(struct eth_port *pp, struct tx_queue *txq_ctrl)
@@ -4143,7 +4418,7 @@ int mv_eth_txp_reset(int port, int txp)
 			mv_eth_txq_done_force(pp, txq_ctrl);
 	}
 	mvNetaTxpReset(port, txp);
-	return 0;
+	return MV_OK;
 }
 
 /* Free received packets from all RXQs and reset RX of the port */
@@ -4189,7 +4464,7 @@ int mv_eth_rx_reset(int port)
 #endif /* CONFIG_MV_ETH_BM_CPU */
 
 	mvNetaRxReset(port);
-	return 0;
+	return MV_OK;
 }
 
 /***********************************************************
@@ -4263,6 +4538,7 @@ int mv_eth_start_internals(struct eth_port *pp, int mtu)
 				err = -EINVAL;
 				goto out;
 			}
+
 			if (mv_eth_ctrl_txq_cpu_own(pp->port, pp->txp, cpuCtrl->txq, 1, cpu) < 0) {
 				err = -EINVAL;
 				goto out;
@@ -4307,6 +4583,7 @@ int mv_eth_start_internals(struct eth_port *pp, int mtu)
 
 #ifdef CONFIG_MV_ETH_BM_CPU
 	mvNetaBmPoolBufSizeSet(pp->port, pp->pool_long->pool, RX_BUF_SIZE(pp->pool_long->pkt_size));
+
 	if (pp->pool_short == NULL) {
 		int short_pool = mv_eth_bm_config_short_pool_get(pp->port);
 
@@ -4476,111 +4753,26 @@ int mv_eth_resume_internals(struct eth_port *pp, int mtu)
 	}
 #endif /* CONFIG_MV_PON */
 
-	return 0;
-
-}
-
-
-int mv_eth_restore_registers(struct eth_port *pp, int mtu)
-{
-	int rxq, txp, txq = 0;
-
-
-	if (mvBoardIsPortInSgmii(pp->port))
-		MV_REG_WRITE(SGMII_SERDES_CFG_REG(pp->port), pp->sgmii_serdes);
-
-	for (txp = 0; txp < pp->txp_num; txp++) {
-		MV_REG_WRITE(NETA_PORT_TX_RESET_REG(pp->port, txp), NETA_PORT_TX_DMA_RESET_MASK);
-		MV_REG_WRITE(NETA_PORT_TX_RESET_REG(pp->port, txp), 0);
-	}
-
-	MV_REG_WRITE(NETA_PORT_RX_RESET_REG(pp->port), NETA_PORT_RX_DMA_RESET_MASK);
-	MV_REG_WRITE(NETA_PORT_RX_RESET_REG(pp->port), 0);
-
-#ifdef CONFIG_MV_ETH_BM_CPU
-	if (pp->pool_long != NULL) {
-		mvNetaBmPoolBufSizeSet(pp->port, pp->pool_long->pool, RX_BUF_SIZE(pp->pool_long->pkt_size));
-		if (pp->pool_short != NULL) {
-			if (pp->pool_short->pool != pp->pool_long->pool)
-				mvNetaBmPoolBufSizeSet(pp->port, pp->pool_short->pool, RX_BUF_SIZE(pp->pool_short->pkt_size));
-			else {
-				int dummy_short_pool = (pp->pool_short->pool + 1) % MV_BM_POOLS;
-
-				/* To disable short pool we choose unused pool and set pkt size to 0 (buffer size = pkt offset) */
-				mvNetaBmPoolBufSizeSet(pp->port, dummy_short_pool, NET_SKB_PAD);
-			}
-		}
-	}
-#endif /* CONFIG_MV_ETH_BM_CPU */
-
-	for (rxq = 0; rxq < CONFIG_MV_ETH_RXQ; rxq++) {
-		if (pp->rxq_ctrl[rxq].q) {
-			/* Set Rx descriptors queue starting address */
-			mvNetaRxqAddrSet(pp->port, rxq,  pp->rxq_ctrl[rxq].rxq_size);
-
-			/* Set Offset */
-			mvNetaRxqOffsetSet(pp->port, rxq, NET_SKB_PAD);
-
-			/* Set coalescing pkts and time */
-			mv_eth_rx_ptks_coal_set(pp->port, rxq, pp->rxq_ctrl[rxq].rxq_pkts_coal);
-			mv_eth_rx_time_coal_set(pp->port, rxq, pp->rxq_ctrl[rxq].rxq_time_coal);
-
-#if defined(CONFIG_MV_ETH_BM_CPU)
-			/* Enable / Disable - BM support */
-			if (pp->pool_long && pp->pool_short) {
-
-				if (pp->pool_short->pool == pp->pool_long->pool) {
-					int dummy_short_pool = (pp->pool_short->pool + 1) % MV_BM_POOLS;
-
-					/* To disable short pool we choose unused pool and set pkt size to 0 (buffer size = pkt offset) */
-					mvNetaRxqBmEnable(pp->port, rxq, dummy_short_pool, pp->pool_long->pool);
-				} else
-					mvNetaRxqBmEnable(pp->port, rxq, pp->pool_short->pool, pp->pool_long->pool);
-			}
-#else
-			/* Fill RXQ with buffers from RX pool */
-			mvNetaRxqBufSizeSet(pp->port, rxq, RX_BUF_SIZE(pp->pool_long->pkt_size));
-			mvNetaRxqBmDisable(pp->port, rxq);
-#endif /* CONFIG_MV_ETH_BM_CPU */
-			if (mvNetaRxqFreeDescNumGet(pp->port, rxq) == 0)
-				mv_eth_rxq_fill(pp, rxq, pp->rxq_ctrl[rxq].rxq_size);
-		}
-	}
-
-	for (txp = 0; txp < pp->txp_num; txp++) {
-		for (txq = 0; txq < CONFIG_MV_ETH_TXQ; txq++) {
-			struct tx_queue *txq_ctrl = &pp->txq_ctrl[txp * CONFIG_MV_ETH_TXQ + txq];
-
-			if (txq_ctrl->q != NULL) {
-				mvNetaTxqAddrSet(pp->port, txq_ctrl->txp, txq_ctrl->txq, txq_ctrl->txq_size);
-				mv_eth_tx_done_ptks_coal_set(pp->port, txp, txq,
-							pp->txq_ctrl[txp * CONFIG_MV_ETH_TXQ + txq].txq_done_pkts_coal);
-			}
-			mvNetaTxqBandwidthSet(pp->port, txp, txq);
-
-		}
-		mvNetaTxpMaxTxSizeSet(pp->port, txp, RX_PKT_SIZE(mtu));
-	}
+	return MV_OK;
 
-	return 0;
 }
 
 
 /***********************************************************
- * mv_eth_suspend_internals --                                *
+ * mv_eth_suspend_internals --                             *
  *   stop port rx/tx activity. free skb's from rx/tx rings.*
  ***********************************************************/
 int mv_eth_suspend_internals(struct eth_port *pp)
 {
-	int queue;
-	int txp, cpu;
+	int cpu;
 
-	/* stop the port activity, mask all interrupts */
+	/* stop the port activity*/
 	if (mvNetaPortDisable(pp->port) != MV_OK) {
-		printk(KERN_ERR "GbE port %d: ethPortDisable failed\n", pp->port);
-		goto error;
+		printk(KERN_ERR "%s: GbE port %d: mvNetaPortDisable failed\n", __func__, pp->port);
+		return MV_ERROR;
 	}
 
+	/* mask all interrupts */
 	mv_eth_interrupts_mask(pp);
 	smp_call_function_many(cpu_online_mask, (smp_call_func_t)mv_eth_interrupts_mask, (void *)pp, 1);
 
@@ -4594,22 +4786,7 @@ int mv_eth_suspend_internals(struct eth_port *pp)
 
 	mdelay(10);
 
-#ifdef CONFIG_MV_ETH_HWF
-	mvNetaHwfEnable(pp->port, 0);
-#endif /* !CONFIG_MV_ETH_HWF */
-
-	/* Reset TX port here. If HWF is supported reset must be called externally */
-	for (txp = 0; txp < pp->txp_num; txp++)
-		mv_eth_txp_reset(pp->port, txp);
-
-	/* free the skb's in the hal rx ring */
-	for (queue = 0; queue < CONFIG_MV_ETH_RXQ; queue++)
-		mv_eth_rxq_drop_pkts(pp, queue);
-
-	return 0;
-error:
-	printk(KERN_ERR "GbE port %d: suspend internals failed\n", pp->port);
-	return -1;
+	return MV_OK;
 }
 
 
@@ -4664,11 +4841,11 @@ int mv_eth_stop_internals(struct eth_port *pp)
 	for (queue = 0; queue < CONFIG_MV_ETH_RXQ; queue++)
 		mv_eth_rxq_drop_pkts(pp, queue);
 
-	return 0;
+	return MV_OK;
 
 error:
 	printk(KERN_ERR "GbE port %d: stop internals failed\n", pp->port);
-	return -1;
+	return MV_ERROR;
 }
 
 /* return positive if MTU is valid */
@@ -4721,7 +4898,7 @@ int mv_eth_check_mtu_internals(struct net_device *dev, int mtu)
 		}
 	}
 #endif /* CONFIG_MV_ETH_BM_CPU */
-	return 0;
+	return MV_OK;
 }
 
 /***********************************************************
@@ -4740,7 +4917,7 @@ int mv_eth_change_mtu_internals(struct net_device *dev, int mtu)
 		STAT_ERR(pp->stats.state_err++);
 		if (pp->flags & MV_ETH_F_DBG_RX)
 			printk(KERN_ERR "%s: port %d, STARTED_BIT = 0, Invalid value.\n", __func__, pp->port);
-		return -1;
+		return MV_ERROR;
 	}
 
 	if ((mtu != dev->mtu) && (pp->pool_long)) {
@@ -4782,7 +4959,7 @@ int mv_eth_change_mtu_internals(struct net_device *dev, int mtu)
 
 		/* DIMA debug; Free all buffers from short pool */
 /*
-		if(pp->pool_short) {
+		if (pp->pool_short) {
 			mv_eth_pool_free(pp->pool_short->pool, pp->pool_short_num);
 			pp->pool_short = NULL;
 		}
@@ -4790,9 +4967,9 @@ int mv_eth_change_mtu_internals(struct net_device *dev, int mtu)
 	}
 	dev->mtu = mtu;
 
-	mv_eth_netdev_update_features(dev);
+	netdev_update_features(dev);
 
-	return 0;
+	return MV_OK;
 }
 
 /***********************************************************
@@ -4801,15 +4978,12 @@ int mv_eth_change_mtu_internals(struct net_device *dev, int mtu)
  ***********************************************************/
 static void mv_eth_tx_done_timer_callback(unsigned long data)
 {
-	struct cpu_ctrl *cpuCtrl;
-	struct net_device *dev = (struct net_device *)data;
-	struct eth_port *pp = MV_ETH_PRIV(dev);
+	struct cpu_ctrl *cpuCtrl = (struct cpu_ctrl *)data;
+	struct eth_port *pp = cpuCtrl->pp;
 	int tx_done = 0, tx_todo = 0;
 	unsigned int txq_mask;
 
-	STAT_INFO(pp->stats.tx_done_timer++);
-
-	cpuCtrl = pp->cpu_config[smp_processor_id()];
+	STAT_INFO(pp->stats.tx_done_timer_event[smp_processor_id()]++);
 
 	clear_bit(MV_ETH_F_TX_DONE_TIMER_BIT, &(cpuCtrl->flags));
 
@@ -4832,6 +5006,10 @@ static void mv_eth_tx_done_timer_callback(unsigned long data)
 		tx_done = mv_eth_tx_done_gbe(pp, txq_mask, &tx_todo);
 	}
 
+	if (cpuCtrl->cpu != smp_processor_id()) {
+		pr_warning("%s: Called on other CPU - %d != %d\n", __func__, cpuCtrl->cpu, smp_processor_id());
+		cpuCtrl = pp->cpu_config[smp_processor_id()];
+	}
 	if (tx_todo > 0)
 		mv_eth_add_tx_done_timer(cpuCtrl);
 }
@@ -5033,7 +5211,7 @@ int mv_eth_txq_tos_map_set(int port, int txq, int cpu, unsigned int tos)
 
 	/* The same txq - do nothing */
 	if (old_txq == (MV_U8) txq)
-		return 0;
+		return MV_OK;
 
 	if (txq == -1) {
 		/* delete tos to txq mapping - free TXQ */
@@ -5042,7 +5220,7 @@ int mv_eth_txq_tos_map_set(int port, int txq, int cpu, unsigned int tos)
 
 		cpuCtrl->txq_tos_map[tos] = MV_ETH_TXQ_INVALID;
 		printk(KERN_ERR "Successfully deleted\n");
-		return 0;
+		return MV_OK;
 	}
 
 	if (mvNetaMaxCheck(txq, CONFIG_MV_ETH_TXQ, "txq"))
@@ -5059,7 +5237,7 @@ int mv_eth_txq_tos_map_set(int port, int txq, int cpu, unsigned int tos)
 
 	cpuCtrl->txq_tos_map[tos] = (MV_U8) txq;
 	printk(KERN_ERR "Successfully added\n");
-	return 0;
+	return MV_OK;
 }
 
 static int mv_eth_priv_init(struct eth_port *pp, int port)
@@ -5079,13 +5257,15 @@ static int mv_eth_priv_init(struct eth_port *pp, int port)
 	pp->port = port;
 	pp->txp_num = 1;
 	pp->txp = 0;
-	pp->pm_mode = MV_ETH_PM_DISABLE;
+	pp->wol_mode = 0;
 	for_each_possible_cpu(cpu) {
 		cpuCtrl = pp->cpu_config[cpu];
 		cpuCtrl->txq = CONFIG_MV_ETH_TXQ_DEF;
 		cpuCtrl->cpuTxqOwner = (1 << CONFIG_MV_ETH_TXQ_DEF);
 		cpuCtrl->cpuTxqMask = 0xFF;
 		mvNetaTxqCpuMaskSet(port, 0xFF , cpu);
+		cpuCtrl->pp = pp;
+		cpuCtrl->cpu = cpu;
 	}
 
 	pp->flags = 0;
@@ -5123,10 +5303,6 @@ static int mv_eth_priv_init(struct eth_port *pp, int port)
 	}
 #endif /* CONFIG_MV_PON */
 
-#if defined(CONFIG_MV_ETH_RX_CSUM_OFFLOAD_DEF)
-	pp->rx_csum_offload = 1;
-#endif /* CONFIG_MV_ETH_RX_CSUM_OFFLOAD_DEF */
-
 #ifdef CONFIG_MV_INCLUDE_SWITCH
 	if (mvBoardSwitchConnectedPortGet(port) != -1) {
 		set_bit(MV_ETH_F_SWITCH_BIT, &(pp->flags));
@@ -5137,10 +5313,12 @@ static int mv_eth_priv_init(struct eth_port *pp, int port)
 		cpuCtrl = pp->cpu_config[cpu];
 		memset(&cpuCtrl->tx_done_timer, 0, sizeof(struct timer_list));
 		cpuCtrl->tx_done_timer.function = mv_eth_tx_done_timer_callback;
+		cpuCtrl->tx_done_timer.data = (unsigned long)cpuCtrl;
 		init_timer(&cpuCtrl->tx_done_timer);
 		clear_bit(MV_ETH_F_TX_DONE_TIMER_BIT, &(cpuCtrl->flags));
 		memset(&cpuCtrl->cleanup_timer, 0, sizeof(struct timer_list));
 		cpuCtrl->cleanup_timer.function = mv_eth_cleanup_timer_callback;
+		cpuCtrl->cleanup_timer.data = (unsigned long)cpuCtrl;
 		init_timer(&cpuCtrl->cleanup_timer);
 		clear_bit(MV_ETH_F_CLEANUP_TIMER_BIT, &(cpuCtrl->flags));
 	}
@@ -5184,7 +5362,7 @@ static int mv_eth_priv_init(struct eth_port *pp, int port)
 		       pp->port, sizeof(u32) * (pp->txp_num * CONFIG_MV_ETH_TXQ * CONFIG_MV_ETH_TXQ_DESC + 1));
 #endif /* CONFIG_MV_ETH_STAT_DIST */
 
-	return 0;
+	return MV_OK;
 }
 
 /***********************************************************************************
@@ -5258,17 +5436,26 @@ void mv_eth_ext_pool_print(struct eth_port *pp)
  ***********************************************************************************/
 void mv_eth_netdev_print(struct net_device *dev)
 {
-	struct eth_netdev *dev_priv = MV_DEV_PRIV(dev);
+	printk(KERN_ERR "%s net_device status: dev=%p\n\n", dev->name, dev);
+	printk(KERN_ERR "ifIdx=%d, mtu=%u, pkt_size=%d, buf_size=%d, MAC=" MV_MACQUAD_FMT "\n",
+	       dev->ifindex, dev->mtu, RX_PKT_SIZE(dev->mtu),
+		RX_BUF_SIZE(RX_PKT_SIZE(dev->mtu)), MV_MACQUAD(dev->dev_addr));
 
-	printk(KERN_ERR "%s net_device status: dev=%p, pp=%p\n", dev->name, dev, MV_ETH_PRIV(dev));
-	printk(KERN_ERR "ifIdx=%d, features=0x%x, flags=0x%x, mtu=%u, size=%d, MAC=" MV_MACQUAD_FMT "\n",
-	       dev->ifindex, (unsigned int)(dev->features), (unsigned int)(dev->flags),
-	       dev->mtu, RX_PKT_SIZE(dev->mtu), MV_MACQUAD(dev->dev_addr));
+	printk(KERN_ERR "features=0x%x, hw_features=0x%x, wanted_features=0x%x, vlan_features=0x%x\n",
+			(unsigned int)(dev->features), (unsigned int)(dev->hw_features),
+			(unsigned int)(dev->wanted_features), (unsigned int)(dev->vlan_features));
 
-	if (dev_priv)
-		printk(KERN_ERR "group=%d, tx_vlan_mh=0x%04x, switch_port_map=0x%x, switch_port_link_map=0x%x\n",
-		       dev_priv->group, dev_priv->tx_vlan_mh, dev_priv->port_map, dev_priv->link_map);
-	printk(KERN_ERR "\n");
+	printk(KERN_ERR "flags=0x%x, gflags=0x%x: running=%d, oper_up=%d\n",
+			(unsigned int)(dev->flags), (unsigned int)(dev->gflags),
+			netif_running(dev), netif_oper_up(dev));
+
+	if (mv_eth_netdev_find(dev->ifindex)) {
+		struct eth_netdev *dev_priv = MV_DEV_PRIV(dev);
+		if (dev_priv)
+			printk(KERN_ERR "group=%d, tx_vlan_mh=0x%04x, switch_port_map=0x%x, switch_port_link_map=0x%x\n",
+					dev_priv->group, dev_priv->tx_vlan_mh, dev_priv->port_map, dev_priv->link_map);
+		printk(KERN_ERR "\n");
+	}
 }
 
 void mv_eth_status_print(void)
@@ -5313,12 +5500,10 @@ void mv_eth_port_status_print(unsigned int port)
 	else
 		printk(KERN_CONT "Disabled\n");
 #endif /* CONFIG_MV_ETH_NFP */
-	if (pp->pm_mode == MV_ETH_PM_WOL)
+	if (pp->wol_mode == 1)
 		printk(KERN_CONT "pm - wol\n");
-	else if (pp->pm_mode == MV_ETH_PM_CLOCK)
-		printk(KERN_CONT "pm - clock\n");
 	else
-		printk(KERN_CONT "pm - disabled\n");
+		printk(KERN_CONT "pm - suspend\n");
 
 	printk(KERN_ERR "rxq_coal(pkts)[ q]   = ");
 	for (q = 0; q < CONFIG_MV_ETH_RXQ; q++)
@@ -5389,10 +5574,9 @@ void mv_eth_port_status_print(unsigned int port)
 			cpuCtrl = pp->cpu_config[cpu];
 			if (MV_BIT_CHECK(pp->cpuMask, cpu))
 				printk(KERN_ERR "  %d:   %d   0x%08x   %d    0x%02x    0x%02x    0x%02x    %d\n",
-					cpu, cpuCtrl->txq, cpuCtrl->causeRxTx,
-					test_bit(NAPI_STATE_SCHED, &cpuCtrl->napi->state),
-					cpuCtrl->cpuTxqMask, cpuCtrl->cpuTxqOwner, (unsigned)cpuCtrl->flags,
-					timer_pending(&cpuCtrl->tx_done_timer));
+					cpu, cpuCtrl->txq, cpuCtrl->causeRxTx, test_bit(NAPI_STATE_SCHED, &cpuCtrl->napi->state),
+					cpuCtrl->cpuTxqMask, cpuCtrl->cpuTxqOwner,
+					(unsigned)cpuCtrl->flags, timer_pending(&cpuCtrl->tx_done_timer));
 		}
 	}
 
@@ -5433,12 +5617,12 @@ void mv_eth_port_stats_print(unsigned int port)
 	struct eth_port *pp = mv_eth_port_by_id(port);
 	struct port_stats *stat = NULL;
 	struct tx_queue *txq_ctrl;
-	int txp, queue;
+	int txp, queue, cpu = smp_processor_id();
 	u32 total_rx_ok, total_rx_fill_ok;
-#ifdef CONFIG_MV_ETH_STAT_INF
-	int i;
-#endif
 
+	pr_info("\n====================================================\n");
+	pr_info("ethPort_%d: Statistics (running on cpu#%d)", port, cpu);
+	pr_info("----------------------------------------------------\n\n");
 
 	if (pp == NULL) {
 		printk(KERN_ERR "eth_stats_print: wrong port number %d\n", port);
@@ -5447,9 +5631,7 @@ void mv_eth_port_stats_print(unsigned int port)
 	stat = &(pp->stats);
 
 #ifdef CONFIG_MV_ETH_STAT_ERR
-	printk(KERN_ERR "\n====================================================\n");
-	printk(KERN_ERR "ethPort_%d: Errors", port);
-	printk(KERN_CONT "\n-------------------------------\n");
+	printk(KERN_ERR "Errors:\n");
 	printk(KERN_ERR "rx_error......................%10u\n", stat->rx_error);
 	printk(KERN_ERR "tx_timeout....................%10u\n", stat->tx_timeout);
 	printk(KERN_ERR "tx_netif_stop.................%10u\n", stat->netif_stop);
@@ -5460,22 +5642,35 @@ void mv_eth_port_stats_print(unsigned int port)
 #endif /* CONFIG_MV_ETH_STAT_ERR */
 
 #ifdef CONFIG_MV_ETH_STAT_INF
-	printk(KERN_ERR "\n====================================================\n");
-	printk(KERN_ERR "ethPort_%d: interrupt statistics", port);
-	printk(KERN_CONT "\n-------------------------------\n");
-	printk(KERN_ERR "irq...........................%10u\n", stat->irq);
-	printk(KERN_ERR "irq_err.......................%10u\n", stat->irq_err);
-
-	printk(KERN_ERR "\n====================================================\n");
-	printk(KERN_ERR "ethPort_%d: Events", port);
-	printk(KERN_CONT "\n-------------------------------\n");
-	for (i = 0; i < CONFIG_NR_CPUS; i++) {
-		printk(KERN_ERR "poll[%d]......................%10u\n", i, stat->poll[i]);
-		printk(KERN_ERR "poll_exit[%d].................%10u\n", i, stat->poll_exit[i]);
-	}
+	pr_info("\nEvents:\n");
+
+	pr_info("irq[cpu]            = ");
+	for_each_possible_cpu(cpu)
+		printk(KERN_CONT "%8d ", stat->irq[cpu]);
+
+	pr_info("irq_none[cpu]       = ");
+	for_each_possible_cpu(cpu)
+		printk(KERN_CONT "%8d ", stat->irq_err[cpu]);
+
+	pr_info("poll[cpu]           = ");
+	for_each_possible_cpu(cpu)
+		printk(KERN_CONT "%8d ", stat->poll[cpu]);
+
+	pr_info("poll_exit[cpu]      = ");
+	for_each_possible_cpu(cpu)
+		printk(KERN_CONT "%8d ", stat->poll_exit[cpu]);
+
+	pr_info("tx_timer_event[cpu] = ");
+	for_each_possible_cpu(cpu)
+		printk(KERN_CONT "%8d ", stat->tx_done_timer_event[cpu]);
+
+	pr_info("tx_timer_add[cpu]   = ");
+	for_each_possible_cpu(cpu)
+		printk(KERN_CONT "%8d ", stat->tx_done_timer_add[cpu]);
+
+	pr_info("\n");
 	printk(KERN_ERR "tx_fragmentation..............%10u\n", stat->tx_fragment);
 	printk(KERN_ERR "tx_done_event.................%10u\n", stat->tx_done);
-	printk(KERN_ERR "tx_done_timer_event...........%10u\n", stat->tx_done_timer);
 	printk(KERN_ERR "cleanup_timer_event...........%10u\n", stat->cleanup_timer);
 	printk(KERN_ERR "link..........................%10u\n", stat->link);
 	printk(KERN_ERR "netdev_stop...................%10u\n", stat->netdev_stop);
@@ -5629,11 +5824,11 @@ static int mv_eth_port_cleanup(int port)
 	pp = mv_eth_port_by_id(port);
 
 	if (pp == NULL)
-		return -1;
+		return MV_ERROR;
 
 	if (pp->flags & MV_ETH_F_STARTED) {
 		printk(KERN_ERR "%s: port %d is started, cannot cleanup\n", __func__, port);
-		return -1;
+		return MV_ERROR;
 	}
 
 	/* Reset Tx ports */
@@ -5703,7 +5898,7 @@ static int mv_eth_port_cleanup(int port)
 		for (i = 0; i < CONFIG_MV_ETH_NAPI_GROUPS; i++)
 			netif_napi_del(pp->napiGroup[i]);
 
-	return 0;
+	return MV_OK;
 }
 
 
@@ -5730,7 +5925,7 @@ int mv_eth_all_ports_cleanup(void)
 	memset(mv_eth_ports, 0, (mv_eth_ports_num * sizeof(struct eth_port *)));
 	/* Note: not freeing mv_eth_ports - we will reuse them */
 
-	return 0;
+	return MV_OK;
 }
 
 #ifdef CONFIG_MV_ETH_PNC_WOL
@@ -5801,13 +5996,14 @@ int mv_eth_wol_pkts_check(int port)
 			return 1;
 		}
 	}
-	return 0;
+	return MV_OK;
 }
 
 void mv_eth_wol_wakeup(int port)
 {
 	int rxq;
 
+
 	/* Restore RXQ coalescing */
 	for (rxq = 0; rxq < CONFIG_MV_ETH_RXQ; rxq++) {
 		mvNetaRxqPktsCoalSet(port, rxq, CONFIG_MV_ETH_RX_COAL_PKTS);
@@ -5860,7 +6056,7 @@ int mv_eth_wol_sleep(int port)
 
 	mv_eth_interrupts_unmask(pp);
 
-	return 0;
+	return MV_OK;
 }
 #endif /* CONFIG_MV_ETH_PNC_WOL */
 
@@ -5895,87 +6091,44 @@ MV_BOOL mv_pon_link_status(void)
 
 #ifdef CONFIG_CPU_IDLE
 
-
-int mv_eth_suspend_clock(int port)
-{
-	/* TODO remove define , add to reg h file */
-	#define PM_CLOCK_GATING_REG	0x18220
-	#define PM_CLOCK_GATING_MASK(port)  (1 << (4-(port)))
-
-	int regVal;
-
-	if (mv_eth_port_suspend(port)) {
-		printk(KERN_ERR "%s: Error, can not suspend port=%d \n", __func__, port);
-		return -1;
-	}
-
-	/* TODO - remove clock_gating reg write to pm.c */
-	regVal = MV_REG_READ(PM_CLOCK_GATING_REG);
-	regVal &= ~PM_CLOCK_GATING_MASK(port);
-	MV_REG_WRITE(PM_CLOCK_GATING_REG, regVal);
-
-
-	return 0;
-}
-
-
 int mv_eth_suspend(struct platform_device *pdev, pm_message_t state)
 {
-
 	struct eth_port *pp;
 	int port;
 
+	pm_flag = 0;
+
 	for (port = 0 ; port < CONFIG_MV_ETH_PORTS_NUM ; port++) {
 		pp = mv_eth_port_by_id(port);
 		if (!pp)
 			continue;
-
-		if (state.event & PM_EVENT_SUSPEND)
-			pp->pm_mode = MV_ETH_PM_CLOCK;
-		else
-			pp->pm_mode = MV_ETH_PM_WOL;
-
-		if (pp->pm_mode == MV_ETH_PM_CLOCK) {
-			if (mv_eth_suspend_clock(pp->port)) {
-				printk(KERN_ERR "%s: Error, port %d clock suspend failed.\n", __func__, port);
-				return -1;
+		if (pp->wol_mode == 0) {
+			if (mv_eth_port_suspend(port)) {
+				printk(KERN_ERR "%s: port #%d suspend failed.\n", __func__, port);
+				return MV_ERROR;
 			}
-		}
-#ifdef CONFIG_MV_ETH_PNC_WOL
-		else if (pp->pm_mode == MV_ETH_PM_WOL) {
 
-			/*Configure ETH port to be in WoL mode*/
-			if (mv_eth_wol_sleep(port)) {
-				printk(KERN_ERR "%s: Error, port %d wol suspend failed.\n", __func__, port);
-				return -1;
-			}
+			/* BUG WA - if port 0 clock is down, we cant interrupt by magic packet */
+			if ((port != 0) || (wol_ports_bmp == 0))
+				/* Set Port Power State to 0 */
+				mvCtrlPwrClckSet(ETH_GIG_UNIT_ID, port, 0);
 		}
-#endif	/*CONFIG_MV_ETH_PNC_WOL*/
-		else
-			printk(KERN_INFO "Port %d power manegment mode is disabled.\n", port);
-
-	}
-
-	return 0;
-}
-
 
-int mv_eth_resume_clock(int port)
-{
-	int regVal;
-
-	/* TODO - remove clock_gating reg write to pm.c */
-	regVal = MV_REG_READ(PM_CLOCK_GATING_REG);
-	regVal |= PM_CLOCK_GATING_MASK(port);
-	MV_REG_WRITE(PM_CLOCK_GATING_REG, regVal);
+		else {
 
-	mdelay(10);
+#ifdef CONFIG_MV_ETH_PNC_WOL
+			if (pp->flags & MV_ETH_F_STARTED)
+				if (mv_eth_wol_sleep(port)) {
+					printk(KERN_ERR "%s: port #%d  WOL failed.\n", __func__, port);
+					return MV_ERROR;
+				}
+#else
+			printk(KERN_INFO "%s:WARNING port #%d in WOL mode but PNC WOL is not defined.\n", __func__, port);
 
-	if (mv_eth_port_resume(port)) {
-		printk(KERN_ERR "%s: Error, port %d resume failed.\n", __func__, port);
-		return -1;
+#endif /*CONFIG_MV_ETH_PNC_WOL*/
+		}
 	}
-	return 0;
+	return MV_OK;
 }
 
 
@@ -5984,37 +6137,40 @@ int mv_eth_resume(struct platform_device *pdev)
 	struct eth_port *pp;
 	int port;
 
+	pm_flag = 0;
+
 	for (port = 0 ; port < CONFIG_MV_ETH_PORTS_NUM ; port++) {
 		pp = mv_eth_port_by_id(port);
 		if (!pp)
 			continue;
 
-		if (pp->pm_mode == MV_ETH_PM_CLOCK) {
-			if (mv_eth_resume_clock(pp->port)) {
-				printk(KERN_ERR "%s: Error, port %d clock resume failed.\n", __func__, port);
-				return -1;
+		if (pp->wol_mode == 0) {
+			/* Set Port Power State to 1 */
+			mvCtrlPwrClckSet(ETH_GIG_UNIT_ID, port, 1);
+			mdelay(10);
+			if (mv_eth_port_resume(port)) {
+				printk(KERN_ERR "%s: port #%d resume failed.\n", __func__, port);
+				return MV_ERROR;
 			}
-		}
-
+		} else
 #ifdef CONFIG_MV_ETH_PNC_WOL
-		else if (pp->pm_mode == MV_ETH_PM_WOL)
 			mv_eth_wol_wakeup(port);
+#else
+			printk(KERN_ERR "%s:WARNING port #%d in WOL mode but PNC WOL is not defined.\n", __func__, port);
+#endif /*CONFIG_MV_ETH_PNC_WOL*/
 
-#endif	/*CONFIG_MV_ETH_PNC_WOL*/
-
-		else
-			printk(KERN_INFO "Port %d power manegment mode is disabled.\n", port);
 	}
 
-	return 0;
+	return MV_OK;
 }
 
+
 #endif	/*CONFIG_CPU_IDLE*/
 
 static int mv_eth_remove(struct platform_device *pdev)
 {
-    printk(KERN_INFO "Removing Marvell Ethernet Driver\n");
-    return 0;
+	printk(KERN_INFO "Removing Marvell Ethernet Driver\n");
+	return MV_OK;
 }
 
 static void mv_eth_shutdown(struct platform_device *pdev)
diff --git a/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/mv_netdev.h b/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/mv_netdev.h
index daa1db0..3e6c35c 100644
--- a/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/mv_netdev.h
+++ b/arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/mv_netdev.h
@@ -25,7 +25,6 @@ WARRANTIES OF MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE ARE EXPRESSLY
 DISCLAIMED.  The GPL License provides additional details about this warranty
 disclaimer.
 *******************************************************************************/
-
 #ifndef __mv_netdev_h__
 #define __mv_netdev_h__
 
@@ -43,6 +42,7 @@ disclaimer.
 #include "bm/mvBmRegs.h"
 #include "bm/mvBm.h"
 
+
 /******************************************************
  * driver statistics control --                       *
  ******************************************************/
@@ -95,8 +95,6 @@ int mv_eth_skb_recycle(struct sk_buff *skb);
 #define mv_eth_is_recycle()     0
 #endif /* CONFIG_NET_SKB_RECYCLE */
 
-
-
 /******************************************************
  * interrupt control --                               *
  ******************************************************/
@@ -142,17 +140,21 @@ int mv_eth_skb_recycle(struct sk_buff *skb);
 		local_irq_restore(flags);
 
 
-#define mv_eth_lock(txq_ctrl, flags)    		     \
+#define mv_eth_lock(txq_ctrl, flags)			     \
+{							     \
 	if (txq_ctrl->flags & MV_ETH_F_TX_SHARED)	     \
 		MV_ETH_LOCK(&txq_ctrl->queue_lock, flags)    \
 	else                                                 \
-		MV_ETH_LIGHT_LOCK(flags)
+		MV_ETH_LIGHT_LOCK(flags)		     \
+}
 
-#define mv_eth_unlock(txq_ctrl, flags)    		     \
-	if (txq_ctrl->flags & MV_ETH_F_TX_SHARED)	     \
-		MV_ETH_UNLOCK(&txq_ctrl->queue_lock, flags)  \
-	else                                                 \
-		MV_ETH_LIGHT_UNLOCK(flags)
+#define mv_eth_unlock(txq_ctrl, flags)                        \
+{							      \
+	if (txq_ctrl->flags & MV_ETH_F_TX_SHARED)	      \
+		MV_ETH_UNLOCK(&txq_ctrl->queue_lock, flags)   \
+	else                                                  \
+		MV_ETH_LIGHT_UNLOCK(flags)		      \
+}
 
 
 /******************************************************
@@ -185,13 +187,14 @@ struct port_stats {
 #endif /* CONFIG_MV_ETH_STAT_ERR */
 
 #ifdef CONFIG_MV_ETH_STAT_INF
-	u32 irq;
-	u32 irq_err;
+	u32 irq[CONFIG_NR_CPUS];
+	u32 irq_err[CONFIG_NR_CPUS];
 	u32 poll[CONFIG_NR_CPUS];
 	u32 poll_exit[CONFIG_NR_CPUS];
+	u32 tx_done_timer_event[CONFIG_NR_CPUS];
+	u32 tx_done_timer_add[CONFIG_NR_CPUS];
 	u32 tx_fragment;
 	u32 tx_done;
-	u32 tx_done_timer;
 	u32 cleanup_timer;
 	u32 link;
 	u32 netdev_stop;
@@ -339,9 +342,11 @@ struct cpu_ctrl {
 	MV_U8			cpuTxqOwner;
 	MV_U8  			txq_tos_map[256];
 	MV_U32			causeRxTx;
+	struct eth_port		*pp;
 	struct napi_struct	*napi;
 	int			napiCpuGroup;
 	int             	txq;
+	int                     cpu;
 	struct timer_list   	tx_done_timer;
 	struct timer_list   	cleanup_timer;
 	unsigned long       	flags;
@@ -379,9 +384,6 @@ struct eth_port {
 	__u8                autoneg_cfg;
 	__u16		        advertise_cfg;
 #endif/* CONFIG_MV_ETH_TOOL */
-#ifdef CONFIG_MV_ETH_RX_CSUM_OFFLOAD
-	MV_U32              rx_csum_offload;
-#endif /* CONFIG_MV_ETH_RX_CSUM_OFFLOAD */
 #ifdef CONFIG_MV_ETH_RX_SPECIAL
 	void    (*rx_special_proc)(int port, int rxq, struct net_device *dev,
 					struct sk_buff *skb, struct neta_rx_desc *rx_desc);
@@ -394,14 +396,7 @@ struct eth_port {
 	MV_U32 rx_indir_table[256];
 	struct cpu_ctrl	*cpu_config[CONFIG_NR_CPUS];
 	MV_U32  sgmii_serdes;
-	int	pm_mode;
-};
-
-enum eth_pm_mode {
-	MV_ETH_PM_WOL = 0,
-	MV_ETH_PM_CLOCK,
-	MV_ETH_PM_DISABLE,
-	MV_ETH_PM_LAST
+	int	wol_mode;
 };
 
 struct eth_netdev {
@@ -451,6 +446,7 @@ struct bm_pool {
 	int         capacity;
 	int         buf_num;
 	int         pkt_size;
+	MV_ULONG    physAddr;
 	u32         *bm_pool;
 	MV_STACK    *stack;
 	spinlock_t  lock;
@@ -480,6 +476,7 @@ int mv_eth_bm_config_long_buf_num_get(int port);
 void mv_eth_bm_config_print(void);
 #endif /* CONFIG_MV_ETH_BM */
 
+void mv_eth_stack_print(int port, MV_BOOL isPrintElements);
 extern struct bm_pool mv_eth_pool[MV_ETH_BM_POOLS];
 extern struct eth_port **mv_eth_ports;
 
@@ -611,6 +608,7 @@ static inline void mv_eth_add_tx_done_timer(struct cpu_ctrl *cpuCtrl)
 	if (test_and_set_bit(MV_ETH_F_TX_DONE_TIMER_BIT, &(cpuCtrl->flags)) == 0) {
 
 		cpuCtrl->tx_done_timer.expires = jiffies + ((HZ * CONFIG_MV_ETH_TX_DONE_TIMER_PERIOD) / 1000); /* ms */
+		STAT_INFO(cpuCtrl->pp->stats.tx_done_timer_add[smp_processor_id()]++);
 		add_timer_on(&cpuCtrl->tx_done_timer, smp_processor_id());
 	}
 }
@@ -734,21 +732,10 @@ int         mv_eth_restore_registers(struct eth_port *pp, int mtu);
 
 void        mv_eth_win_init(int port);
 int         mv_eth_resume_network_interfaces(struct eth_port *pp);
-int         mv_eth_pm_mode_set(int port, int mode);
+int         mv_eth_wol_mode_set(int port, int mode);
 
 int	    mv_eth_cpu_txq_mask_set(int port, int cpu, int txqMask);
 
-#if defined(CONFIG_MV_ETH_NFP) || defined(CONFIG_MV_ETH_NFP_MODULE)
-typedef MV_STATUS mv_eth_nfp_func_t(struct eth_port *pp, int rxq, struct neta_rx_desc *rx_desc,
-					struct eth_pbuf *pkt, struct bm_pool *pool);
-struct mv_eth_nfp_ops {
-	MV_STATUS (*mv_eth_nfp)(struct eth_port *pp, int rxq, struct neta_rx_desc *rx_desc,
-					struct eth_pbuf *pkt, struct bm_pool *pool);
-};
-int         mv_eth_nfp_register(mv_eth_nfp_func_t *func);
-void        mv_eth_nfp_unregister(void);
-#endif /* CONFIG_MV_ETH_NFP || CONFIG_MV_ETH_NFP_MODULE */
-
 irqreturn_t mv_eth_isr(int irq, void *dev_id);
 int         mv_eth_start_internals(struct eth_port *pp, int mtu);
 int         mv_eth_stop_internals(struct eth_port *pp);
@@ -852,5 +839,13 @@ void      mv_hwf_bm_dump(void);
 #endif /* CONFIG_MV_ETH_HWF && !CONFIG_MV_ETH_BM_CPU */
 
 
+#ifdef CONFIG_MV_ETH_NFP
+int         mv_eth_nfp_ctrl(struct net_device *dev, int en);
+int         mv_eth_nfp_ext_ctrl(struct net_device *dev, int en);
+int         mv_eth_nfp_ext_add(struct net_device *dev, int port);
+int         mv_eth_nfp_ext_del(struct net_device *dev);
+MV_STATUS   mv_eth_nfp(struct eth_port *pp, int rxq, struct neta_rx_desc *rx_desc,
+					struct eth_pbuf *pkt, struct bm_pool *pool);
+#endif /* CONFIG_MV_ETH_NFP */
 
 #endif /* __mv_netdev_h__ */
-- 
1.7.5.4

