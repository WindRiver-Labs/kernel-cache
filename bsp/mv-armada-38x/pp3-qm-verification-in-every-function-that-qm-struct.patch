From f8e5809a62893072d9d60e8677c6c3f05f15130a Mon Sep 17 00:00:00 2001
From: Dovrat <dovrat@marvell.com>
Date: Sun, 27 Apr 2014 10:03:13 +0300
Subject: [PATCH 1607/1825] pp3: qm: verification in every function that qm
 structure is initialized

https://github.com/MISL-EBU-System-SW/misl-windriver.git linux-3.4.69-14t2-read
commit 1168fb65b4a68a6933d2247830ae2ac1972b6a17

Change-Id: I21a84b521b2957c9a6ceb1ff5ad212dd1b28c281
Signed-off-by: Dovrat <dovrat@marvell.com>
Reviewed-on: http://vgitil04.il.marvell.com:8080/7707
Tested-by: Star_Automation <star@marvell.com>
Reviewed-by: Yelena Krivosheev <yelena@marvell.com>
Reviewed-by: Dmitri Epshtein <dima@marvell.com>
Tested-by: Dmitri Epshtein <dima@marvell.com>
Signed-off-by: Zhong Hongbo <hongbo.zhong@windriver.com>
---
 drivers/net/ethernet/marvell/pp3/qm/mv_qm.c       |  187 +++++++++++++++++----
 drivers/net/ethernet/marvell/pp3/qm/mv_qm.h       |    5 +-
 drivers/net/ethernet/marvell/pp3/qm/mv_qm_sysfs.c |    2 +-
 3 files changed, 163 insertions(+), 31 deletions(-)

diff --git a/drivers/net/ethernet/marvell/pp3/qm/mv_qm.c b/drivers/net/ethernet/marvell/pp3/qm/mv_qm.c
index a373182..a20441f 100644
--- a/drivers/net/ethernet/marvell/pp3/qm/mv_qm.c
+++ b/drivers/net/ethernet/marvell/pp3/qm/mv_qm.c
@@ -68,6 +68,18 @@ SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 #include "qm/mv_qm.h"
 #include "qm/mv_qm_regs.h"
 
+static u32 qm_open_status = QM_CLOSED;
+
+/**
+ */
+int qm_open_check(void)
+{
+	if (qm_open_status != QM_OPENED) {
+		pr_info("qm open should be called before any call to qm\n");
+		return -1;
+	}
+	return 0;
+}
 /**
  */
 int qm_open(void)
@@ -81,11 +93,9 @@ int qm_open(void)
 	if (rc != OK)
 		return rc;
 	rc = qm_reg_offset_alias_init();
-/*
 	if (rc != OK)
 		return rc;
-	rc = qm_pid_bid_init();
-*/
+	qm_open_status = QM_OPENED;
 	return rc;
 }
 
@@ -94,7 +104,7 @@ int qm_open(void)
 int qm_close(void)
 {
 	int rc = OK;
-
+	qm_open_status = QM_CLOSED;
 	return rc;
 }
 
@@ -117,6 +127,9 @@ int qm_pfe_base_address_pool_set(struct mv_a40 *qece_base_address, struct mv_a40
 	struct pfe_pyld_dram_base_address_hi         reg_pyld_dram_base_address_hi;
 	struct pfe_pyld_dram_base_address_lo         reg_pyld_dram_base_address_lo;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	reg_base_address =      qm.pfe.qece_dram_base_address_hi;
 	reg_size   =   qm_reg_size.pfe.qece_dram_base_address_hi;
 	reg_offset = qm_reg_offset.pfe.qece_dram_base_address_hi * 0;
@@ -166,34 +179,15 @@ int qm_pfe_base_address_pool_set(struct mv_a40 *qece_base_address, struct mv_a40
 	return rc;
 }
 
-/*int qm_enable(u32 qe_thr_hi, u32 qe_thr_lo, u32 pl_thr_hi, u32 pl_thr_lo)
-int qm_enable(u32 gpm_qe_thr_hi, u32 gpm_qe_thr_lo, u32 gpm_pl_thr_hi, u32 gpm_pl_thr_lo,
-			u32 dram_qe_thr_hi, u32 dram_qe_thr_lo, u32 dram_pl_thr_hi, u32 dram_pl_thr_lo)
-{
-	int rc = -QM_INPUT_NOT_IN_RANGE;
-	u32 reg_base_address, reg_size, reg_offset;
-
-	if ((gpm_qe_thr_hi  <  QM_GPM_QE_THR_HI_MIN) || (gpm_qe_thr_hi  >  QM_GPM_QE_THR_HI_MAX)) return rc;
-	if ((gpm_qe_thr_lo  <  QM_GPM_QE_THR_LO_MIN) || (gpm_qe_thr_lo  >  QM_GPM_QE_THR_LO_MAX)) return rc;
-	if ((gpm_pl_thr_hi  <  QM_GPM_PL_THR_HI_MIN) || (gpm_pl_thr_hi  >  QM_GPM_PL_THR_HI_MAX)) return rc;
-	if ((gpm_pl_thr_lo  <  QM_GPM_PL_THR_LO_MIN) || (gpm_pl_thr_lo  >  QM_GPM_PL_THR_LO_MAX)) return rc;
-	if ((dram_qe_thr_hi < QM_DRAM_QE_THR_HI_MIN) || (dram_qe_thr_hi > QM_DRAM_QE_THR_HI_MAX)) return rc;
-	if ((dram_qe_thr_lo < QM_DRAM_QE_THR_LO_MIN) || (dram_qe_thr_lo > QM_DRAM_QE_THR_LO_MAX)) return rc;
-	if ((dram_pl_thr_hi < QM_DRAM_PL_THR_HI_MIN) || (dram_pl_thr_hi > QM_DRAM_PL_THR_HI_MAX)) return rc;
-	if ((dram_pl_thr_lo < QM_DRAM_PL_THR_LO_MIN) || (dram_pl_thr_lo > QM_DRAM_PL_THR_LO_MAX)) return rc;
-
-	rc =  qm_gpm_pool_thr_set(gpm_qe_thr_hi,  gpm_qe_thr_lo,  gpm_pl_thr_hi,  gpm_pl_thr_lo); if (rc) return rc;
-	rc = qm_dram_pool_thr_set(dram_qe_thr_hi, dram_qe_thr_lo, dram_pl_thr_hi, dram_pl_thr_lo); if (rc) return rc;
-
-	return rc;
-}
-*/
 
 int qm_dma_gpm_pools_def_enable(void)
 {
 	int rc = OK;
 	u32 qece_thr_hi, qece_thr_lo, pl_thr_hi, pl_thr_lo;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	qece_thr_hi = QM_GPM_QE_THR_HI_DEF;
 	qece_thr_lo = QM_GPM_QE_THR_LO_DEF;
 	pl_thr_hi   = QM_GPM_PL_THR_HI_DEF;
@@ -209,6 +203,9 @@ int qm_dma_gpm_pools_enable(u32 qece_thr_hi, u32 qece_thr_lo, u32 pl_thr_hi, u32
 	struct dma_gpm_thresholds reg_gpm_thresholds;	/* RW */
 	u32 reg_base_address, reg_size, reg_offset;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	if ((qece_thr_hi < QM_GPM_QE_THR_HI_MIN) || (qece_thr_hi > QM_GPM_QE_THR_HI_MAX))
 		return rc;
 	if ((qece_thr_lo < QM_GPM_QE_THR_LO_MIN) || (qece_thr_lo > QM_GPM_QE_THR_LO_MAX))
@@ -239,6 +236,9 @@ int qm_dma_dram_pools_def_enable(void)
 	int rc = OK;
 	u32 qece_thr_hi, qece_thr_lo, pl_thr_hi, pl_thr_lo;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	qece_thr_hi = QM_DRAM_QE_THR_HI_DEF;
 	qece_thr_lo = QM_DRAM_QE_THR_LO_DEF;
 	pl_thr_hi   = QM_DRAM_PL_THR_HI_DEF;
@@ -254,6 +254,9 @@ int qm_dma_dram_pools_enable(u32 qece_thr_hi, u32 qece_thr_lo, u32 pl_thr_hi, u3
 	struct dma_dram_thresholds reg_dram_thresholds;	/* RW */
 	u32 reg_base_address, reg_size, reg_offset;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	if ((qece_thr_hi < QM_GPM_QE_THR_HI_MIN) || (qece_thr_hi > QM_GPM_QE_THR_HI_MAX))
 		return rc;
 	if ((qece_thr_lo < QM_GPM_QE_THR_LO_MIN) || (qece_thr_lo > QM_GPM_QE_THR_LO_MAX))
@@ -284,6 +287,9 @@ int qm_dma_queue_memory_type_set(u32 queue, u32 memory_type)
 	struct dma_q_memory_allocation        reg_q_memory_allocation;
 	u32 reg_base_address, reg_size, reg_offset;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	if ((queue       <       QM_QUEUE_MIN) || (queue       >       QM_QUEUE_MAX)) {
 		pr_err("queue %d is out of range %d .. %d\n", queue, QM_QUEUE_MIN, QM_QUEUE_MAX);
 		return rc;
@@ -337,6 +343,9 @@ int qm_packets_in_queues(u32 *status)
 	struct ql_qlen reg_qlen;
 	u32 reg_base_address, reg_size, reg_offset;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	*status = 0;
 
 	for (queue = 0; queue <= QM_QUEUE_MAX; queue++) {
@@ -359,6 +368,9 @@ int qm_default_set(void)
 {
 	int rc = OK;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	rc = qm_ru_port_to_class_def_set();
 	if (rc != OK) {
 		pr_err("setting ru_port_to_class failed\n");
@@ -433,6 +445,9 @@ int qm_ru_pool_sid_number_def_set(void)
 	int rc = OK;
 	u32 pool0_sid_num, pool1_sid_num;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	pool0_sid_num = QM_POOL0_SID_NUM_DEF;
 	pool1_sid_num = QM_POOL1_SID_NUM_DEF;
 
@@ -446,6 +461,9 @@ int qm_ru_pool_sid_number_set(u32 pool0_sid_num, u32 pool1_sid_num)
 	struct reorder_ru_pool         reg_ru_pool;
 	u32 reg_base_address, reg_size, reg_offset;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	if ((pool0_sid_num < QM_POOL0_SID_NUM_MIN) || (pool0_sid_num > QM_POOL0_SID_NUM_MAX)) {
 		pr_err("pool 0 eorder number of SID is 4096\n");
 		return rc;
@@ -499,6 +517,9 @@ int qm_ru_port_to_class_def_set(void)
 	int rc = OK;
 	u32 port_class, port_pool, input_port;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	port_pool  = QM_INPUT_PORT_POOL_DEF;
 
 	/* cMac and eMac */
@@ -545,7 +566,6 @@ int qm_ru_port_to_class_def_set(void)
 		}
 	}
 
-	rc = OK;
 	return rc;
 }
 
@@ -556,6 +576,9 @@ int qm_ru_port_to_class_set(u32 input_port, u32 port_class, u32 port_pool)
 	u32 reg_base_address, reg_size, reg_offset;
 	struct reorder_ru_port2class reg_ru_port2class;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	if ((port_class < QM_CLASS_ARR_MIN)   || (port_class > QM_CLASS_ARR_MAX)) {
 		pr_err("port class %d is not in range %d .. %d\n", port_class, QM_CLASS_ARR_MIN, QM_CLASS_ARR_MAX);
 		return rc;
@@ -587,6 +610,9 @@ int qm_dqf_port_data_fifo_def_set(void)
 	int rc = OK;
 	u32 port_depth_arr[QM_PORT_MAX + 1];
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	port_depth_arr[0]  = QM_PORT_DEPTH_ARR_PPC0_DEF;	/* 2*144B for PPC0		*/
 	port_depth_arr[1]  = QM_PORT_DEPTH_ARR_PPC1_DEF;	/* 1*144B for PPC1-mnt0	*/
 	port_depth_arr[2]  = QM_PORT_DEPTH_ARR_PPC1_DEF;	/* 1*144B for PPC1-mnt1	*/
@@ -617,6 +643,9 @@ int qm_dqf_port_data_fifo_set(u32 *port_depth_arr)
 	u32 mac_port_depth_arr_sum = 0, ppc_port_depth_arr_sum = 0;
 	u32 data_fifo_ppc_counter = 0, data_fifo_mac_counter = 0;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	/*
 	 * Ports that are connected to Mac (ports 3 to 10) share the same fifo between them.
 	 * Each line in the fifo holds 16B that is why depth should be multiplication of 16B.
@@ -748,6 +777,9 @@ int qm_dqf_port_credit_thr_def_set(void)
 	int rc = OK;
 	u32 port_credit_thr_arr[QM_PORT_MAX + 1];
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	port_credit_thr_arr[0]  = 0;
 	port_credit_thr_arr[1]  = 0;
 	port_credit_thr_arr[2]  = 0;
@@ -777,6 +809,9 @@ int qm_dqf_port_credit_thr_set(u32 *port_credit_thr_arr)
 	u32 reg_base_address, reg_size, reg_offset;
 	u32 port, data_fifo_depth_p;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	for (port = QM_PORT_MAC_MIN; port <= QM_PORT_MAC_MAX; port++) {
 		reg_base_address =      qm.dqf.Data_FIFO_params_p;
 		reg_size   =   qm_reg_size.dqf.Data_FIFO_params_p;
@@ -820,6 +855,9 @@ int qm_dqf_port_ppc_map_def_set(void)
 	u32 port_ppc_arr[QM_PORT_MAX + 1];
 	u32 port;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	port_ppc_arr[0]  = QM_PORT_PPC_ARR_PPC0_DEF;	/* 1 for PPC0      */
 	port_ppc_arr[1]  = QM_PORT_PPC_ARR_PPC1_DEF;	/* 2 for PPC1-mnt0 */
 	port_ppc_arr[2]  = QM_PORT_PPC_ARR_PPC1_DEF;	/* 2 for PPC1-mnt1 */
@@ -844,7 +882,6 @@ int qm_dqf_port_ppc_map_def_set(void)
 			return rc;
 	}
 
-	rc = OK;
 	return rc;
 }
 
@@ -854,6 +891,9 @@ int qm_dqf_port_ppc_map_set(u32 *port_ppc, u32 port)
 	struct dqf_PPC_port_map_p              reg_PPC_port_map_p;
 	u32 reg_base_address, reg_size, reg_offset;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	if ((port          < QM_PORT_PPC_MIN) || (port          >  QM_PORT_PPC_MAX))
 		return rc;
 
@@ -874,6 +914,9 @@ int qm_dma_qos_attr_def_set(void)
 	int rc = OK;
 	u32 swf_awqos, rdma_awqos, hwf_qe_ce_awqos, hwf_sfh_pl_awqos;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	swf_awqos        = QM_SWF_AWQOS_DEF;		/* 1 for QM_SWF_AWQOS_DEF        */
 	rdma_awqos       = QM_RDMA_AWQOS_DEF;		/* 1 for QM_RDMA_AWQOS_DEF       */
 	hwf_qe_ce_awqos  = QM_HWF_QE_CE_AWQOS_DEF;	/* 1 for QM_HWF_QE_CE_AWQOS_DEF  */
@@ -898,6 +941,9 @@ int qm_dma_qos_attr_set(u32 swf_awqos, u32 rdma_awqos, u32 hwf_qe_ce_awqos, u32
 	struct dma_AXI_write_attributes_for_hwf_pyld  reg_AXI_write_attributes_for_hwf_pyld;
 	u32 reg_base_address, reg_size, reg_offset;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	if ((swf_awqos       <         QM_SWF_AWQOS_MIN) || (swf_awqos        >        QM_SWF_AWQOS_MAX))
 		return rc;
 	if ((rdma_awqos      <        QM_RDMA_AWQOS_MIN) || (rdma_awqos       >       QM_RDMA_AWQOS_MAX))
@@ -960,6 +1006,9 @@ int qm_dma_cache_attr_def_set(void)
 	int rc = OK;
 	u32 swf_awcache, rdma_awcache, hwf_qe_ce_awcache, hwf_sfh_pl_awcache;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	swf_awcache        = QM_SWF_AWCACHE_DEF;		/* 11 for QM_SWF_AWQOS_DEF        */
 	rdma_awcache       = QM_RDMA_AWCACHE_DEF;		/* 11 for QM_RDMA_AWQOS_DEF       */
 	hwf_qe_ce_awcache  = QM_HWF_QE_CE_AWCACHE_DEF;	/*  3 for QM_HWF_QE_CE_AWQOS_DEF  */
@@ -978,6 +1027,9 @@ int qm_dma_cache_attr_set(u32 swf_awcache, u32 rdma_awcache, u32 hwf_qe_ce_awcac
 	struct dma_AXI_write_attributes_for_hwf_pyld  reg_AXI_write_attributes_for_hwf_pyld;
 	u32 reg_base_address, reg_size, reg_offset;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	if ((swf_awcache       <         QM_SWF_AWCACHE_MIN) || (swf_awcache        >        QM_SWF_AWCACHE_MAX)) {
 		pr_err("swf cache write attribute %d is out of range %d..%d\n",
 			swf_awcache, QM_SWF_AWCACHE_MIN, QM_SWF_AWCACHE_MAX);
@@ -1056,6 +1108,9 @@ int qm_dma_domain_attr_def_set(void)
 	int rc = !OK;
 	u32 swf_awdomain, rdma_awdomain, hwf_qe_ce_awdomain, hwf_sfh_pl_awdomain;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	swf_awdomain        = QM_SWF_AWDOMAIN_DEF;			/* 2 for QM_SWF_AWDOMAIN_DEF        */
 	rdma_awdomain       = QM_RDMA_AWDOMAIN_DEF;			/* 2 for QM_RDMA_AWDOMAIN_DEF       */
 	hwf_qe_ce_awdomain  = QM_HWF_QE_CE_AWDOMAIN_DEF;	/* 0 for QM_HWF_QE_CE_AWDOMAIN_DEF  */
@@ -1074,6 +1129,9 @@ int qm_dma_domain_attr_set(u32 swf_awdomain, u32 rdma_awdomain, u32 hwf_qe_ce_aw
 	struct dma_AXI_write_attributes_for_hwf_pyld  reg_AXI_write_attributes_for_hwf_pyld;
 	u32 reg_base_address, reg_size, reg_offset;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	if ((swf_awdomain       <         QM_SWF_AWDOMAIN_MIN) || (swf_awdomain        >        QM_SWF_AWDOMAIN_MAX))
 		return rc;
 	if ((rdma_awdomain      <        QM_RDMA_AWDOMAIN_MIN) || (rdma_awdomain       >       QM_RDMA_AWDOMAIN_MAX))
@@ -1136,6 +1194,9 @@ int qm_pfe_qos_attr_def_set(void)
 	int rc = OK;
 	u32 swf_arqos, rdma_arqos, hwf_qe_ce_arqos, hwf_sfh_pl_arqos;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	swf_arqos        = QM_SWF_ARQOS_DEF;		/* 1 for QM_SWF_ARQOS_DEF        */
 	rdma_arqos       = QM_RDMA_ARQOS_DEF;		/* 1 for QM_RDMA_ARQOS_DEF       */
 	hwf_qe_ce_arqos  = QM_HWF_QE_CE_ARQOS_DEF;	/* 1 for QM_HWF_QE_CE_ARQOS_DEF  */
@@ -1160,6 +1221,9 @@ int qm_pfe_qos_attr_set(u32 swf_arqos, u32 rdma_arqos, u32 hwf_qe_ce_arqos, u32
 	struct pfe_AXI_read_attributes_for_hwf_pyld  reg_AXI_read_attributes_for_hwf_pyld;
 	u32 reg_base_address, reg_size, reg_offset;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	if ((swf_arqos       <         QM_SWF_ARQOS_MIN) || (swf_arqos        >        QM_SWF_ARQOS_MAX))
 		return rc;
 	if ((rdma_arqos      <        QM_RDMA_ARQOS_MIN) || (rdma_arqos       >       QM_RDMA_ARQOS_MAX))
@@ -1222,6 +1286,9 @@ int qm_pfe_cache_attr_def_set(void)
 	int rc = !OK;
 	u32 swf_arcache, rdma_arcache, hwf_qe_ce_arcache, hwf_sfh_pl_arcache;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	swf_arcache        = QM_SWF_ARCACHE_DEF;		/* 1 for QM_SWF_ARCACHE_DEF	       */
 	rdma_arcache       = QM_RDMA_ARCACHE_DEF;		/* 1 for QM_RDMA_ARCACHE_DEF       */
 	hwf_qe_ce_arcache  = QM_HWF_QE_CE_ARCACHE_DEF;	/* 1 for QM_HWF_QE_CE_ARCACHE_DEF  */
@@ -1240,6 +1307,9 @@ int qm_pfe_cache_attr_set(u32 swf_arcache, u32 rdma_arcache, u32 hwf_qe_ce_arcac
 	struct pfe_AXI_read_attributes_for_hwf_pyld  reg_AXI_read_attributes_for_hwf_pyld;
 	u32 reg_base_address, reg_size, reg_offset;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	if ((swf_arcache       <         QM_SWF_ARCACHE_MIN) || (swf_arcache        >        QM_SWF_ARCACHE_MAX))
 		return rc;
 	if ((rdma_arcache      <        QM_RDMA_ARCACHE_MIN) || (rdma_arcache       >       QM_RDMA_ARCACHE_MAX))
@@ -1302,6 +1372,9 @@ int qm_pfe_domain_attr_def_set(void)
 	int rc = OK;
 	u32 swf_ardomain, rdma_ardomain, hwf_qe_ce_ardomain, hwf_sfh_pl_ardomain;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	swf_ardomain        = QM_SWF_ARDOMAIN_DEF;			/* 1 for QM_SWF_ARDOMAIN_DEF        */
 	rdma_ardomain       = QM_RDMA_ARDOMAIN_DEF;			/* 1 for QM_RDMA_ARDOMAIN_DEF       */
 	hwf_qe_ce_ardomain  = QM_HWF_QE_CE_ARDOMAIN_DEF;	/* 1 for QM_HWF_QE_CE_ARDOMAIN_DEF  */
@@ -1320,6 +1393,9 @@ int qm_pfe_domain_attr_set(u32 swf_ardomain, u32 rdma_ardomain, u32 hwf_qe_ce_ar
 	struct pfe_AXI_read_attributes_for_hwf_pyld  reg_AXI_read_attributes_for_hwf_pyld;
 	u32 reg_base_address, reg_size, reg_offset;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	if ((swf_ardomain       <         QM_SWF_ARDOMAIN_MIN) || (swf_ardomain        >        QM_SWF_ARDOMAIN_MAX))
 		return rc;
 	if ((rdma_ardomain      <        QM_RDMA_ARDOMAIN_MIN) || (rdma_ardomain       >       QM_RDMA_ARDOMAIN_MAX))
@@ -1382,6 +1458,9 @@ int qm_ql_q_profile_def_set(void)
 	int rc = OK;
 	u32 queue_profile, queue;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	for (queue = 0; queue <= 191; queue++) {
 		queue_profile = QM_QUEUE_PROFILE_0;
 		rc = qm_ql_q_profile_set(queue_profile, queue);
@@ -1431,7 +1510,6 @@ int qm_ql_q_profile_def_set(void)
 			return rc;
 	}
 
-	rc = OK;
 	return rc;
 }
 
@@ -1441,6 +1519,9 @@ int qm_ql_q_profile_set(u32 queue_profile, u32 queue)
 	struct ql_qptr_entry reg_qptr_entry;
 	u32 reg_base_address, reg_size, reg_offset;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	if ((queue_profile < QM_QUEUE_PROFILE_MIN) || (queue_profile >  QM_QUEUE_PROFILE_MAX))
 		return rc;
 	if ((queue         <         QM_QUEUE_MIN) || (queue         >          QM_QUEUE_MAX))
@@ -1492,6 +1573,9 @@ int qm_ql_thr_def_set(void)
 	int rc = OK;
 	u32 low_threshold, pause_threshold, high_threshold, traffic_source;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 /*
 #define QM_LOW_THRESHOLD_DEF	QM_THR_HI_DEF
 #define QM_QUEUE_PROFILE_INVALID			0x00000000	/ *    0 * /
@@ -1600,6 +1684,9 @@ int qm_ql_thr_set(u32 low_threshold, u32 pause_threshold, u32 high_threshold, u3
 	struct ql_traffic_source	reg_traffic_source;
 	u32 reg_base_address, reg_size, reg_offset;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	if ((queue_profile   <   QM_QUEUE_PROFILE_MIN) || (queue_profile   >   QM_QUEUE_PROFILE_MAX))
 		return rc;
 	if ((low_threshold   <   QM_LOW_THRESHOLD_MIN) || (low_threshold   >   QM_LOW_THRESHOLD_MAX))
@@ -1666,6 +1753,9 @@ int qm_vmid_set(u32 qm_vmid)
 {
 	int rc = -QM_INPUT_NOT_IN_RANGE;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	if ((qm_vmid < QM_VMID_MIN) || (qm_vmid > QM_VMID_MAX))
 		return rc;
 
@@ -1687,6 +1777,9 @@ int dma_vmid_set(u32 qm_vmid)
 	struct dma_DRAM_VMID				  reg_DRAM_VMID;
 	u32 reg_base_address, reg_size, reg_offset;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	if ((qm_vmid < QM_VMID_MIN) || (qm_vmid > QM_VMID_MAX))
 		return rc;
 
@@ -1711,6 +1804,9 @@ int pfe_vmid_set(u32 qm_vmid)
 	struct pfe_QM_VMID     reg_QM_VMID;
 	u32 reg_base_address, reg_size, reg_offset;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	if ((qm_vmid < QM_VMID_MIN) || (qm_vmid > QM_VMID_MAX))
 		return rc;
 
@@ -1735,6 +1831,9 @@ int qm_queue_flush_start(u32 queue)
 	struct pfe_queue_flush reg_queue_flush;
 	u32 reg_base_address, reg_size, reg_offset;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	if ((queue   <   QM_QUEUE_MIN) || (queue   >   QM_QUEUE_MAX))
 		return rc;
 
@@ -1760,6 +1859,9 @@ int qm_port_flush_start(u32 port)
 	struct pfe_port_flush reg_port_flush;
 	u32 reg_base_address, reg_size, reg_offset;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	if ((port            <    QM_PORT_MIN) || (port            >    QM_PORT_MAX))
 		return rc;
 
@@ -1786,6 +1888,9 @@ int ql_queue_length_get(u32 queue, u32 *length, u32 *status)
 	u32 reg_base_address, reg_size, reg_offset;
 	struct ql_qlen     reg_qlen;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	if ((queue   <   QM_QUEUE_MIN) || (queue   >   QM_QUEUE_MAX))
 		return rc;
 
@@ -1808,6 +1913,9 @@ int qm_idle_status_get(u32 *status)
 	struct dma_idle_status reg_idle_status;
 	u32 reg_base_address, reg_size, reg_offset;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	reg_base_address =      qm.dma.idle_status;
 	reg_size   =   qm_reg_size.dma.idle_status;
 	reg_offset = qm_reg_offset.dma.idle_status * 0;
@@ -1876,6 +1984,9 @@ int	qm_ru_class_cmd_set(u32 host, u32 host_class, u32 host_sid, u32 cmd)
 	struct reorder_ru_task_permission reg_ru_task_permission;
 	u32 reg_base_address, reg_size, reg_offset;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	if ((host       <          QM_HOST_MIN) || (host       >          QM_HOST_MAX))
 		return rc; /* Host: cpu 0 or cpu 1 (how does neta knows who It is Â– ask DIma?) */
 	if ((host_class < QM_REORDER_CLASS_MIN) || (host_class > QM_REORDER_CLASS_MAX))
@@ -1936,6 +2047,9 @@ int qm_errors_dump(void)
 	struct reorder_ru_ser_error_cause           reg_ru_ser_error_cause;
 	u32 reg_base_address, reg_size, reg_offset;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	pr_info("\n-------------- QL  errors dump -----------\n");
 
 	reg_base_address =      qm.ql.ECC_error_cause;
@@ -2314,6 +2428,9 @@ int qm_global_dump(void)
 	u32 reg_base_address, reg_size, reg_offset;
 	u32 t;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	pr_info("\n-------------- QL  global registers dump -----------\n");
 
 	for (t = 0; t < QM_QUEUE_PROFILE_MAX; t++) {
@@ -2692,6 +2809,9 @@ int qm_queue_dump(u32 queue)
 
 	u32 reg_base_address, reg_size, reg_offset;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	if ((queue         <         QM_QUEUE_MIN) || (queue         >          QM_QUEUE_MAX))
 		return rc;
 
@@ -2793,6 +2913,9 @@ int qm_queue_no_traffic_dump(u32 queue)
 
 	u32 reg_base_address, reg_size, reg_offset;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	if ((queue         <         QM_QUEUE_MIN) || (queue         >          QM_QUEUE_MAX))
 		return rc;
 
@@ -2821,6 +2944,9 @@ int qm_nempty_queue_len_get(void)
 	u32 reg_base_address, reg_size, reg_offset;
 	u32 queue;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	pr_info("\n-------------- QL  nempty queue len get -----------\n");
 
 	for (queue = QM_QUEUE_MIN; queue <= QM_QUEUE_MAX; queue++) {
@@ -2852,6 +2978,9 @@ int qm_dqf_port_dump(u32 port)
 	struct dqf_PPC_port_map_p       reg_ppc_port_map_p;
 	struct dqf_data_fifo_pointers_p reg_data_fifo_pointers_p;
 
+	if (qm_open_check())
+		return -EINVAL;
+
 	pr_info("\n-------------- DQF port dump for port = 0x%08X -----------\n", port);
 
 	reg_base_address =      qm.dqf.Data_FIFO_params_p;
diff --git a/drivers/net/ethernet/marvell/pp3/qm/mv_qm.h b/drivers/net/ethernet/marvell/pp3/qm/mv_qm.h
index f67bb26..0f94d66 100644
--- a/drivers/net/ethernet/marvell/pp3/qm/mv_qm.h
+++ b/drivers/net/ethernet/marvell/pp3/qm/mv_qm.h
@@ -73,6 +73,10 @@ SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 #define QM_ALIAS_ERROR                 -EINVAL
 #define QM_INPUT_NOT_IN_RANGE          -EINVAL
 
+/* Global Definitions */
+#define QM_OPENED				1
+#define QM_CLOSED				0
+
 /* Input definitions*/
 #define GPM_MEMORY_TYPE				 0
 #define DRAM_MEMORY_TYPE			 1
@@ -303,7 +307,6 @@ SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 #define QM_CMD_MIN				         0
 #define QM_CMD_MAX				0x7FFFFFFF
 
-/* typedef void *      qm_handle; */
 
 /**
  *
diff --git a/drivers/net/ethernet/marvell/pp3/qm/mv_qm_sysfs.c b/drivers/net/ethernet/marvell/pp3/qm/mv_qm_sysfs.c
index ed8519a..b4e04ef 100644
--- a/drivers/net/ethernet/marvell/pp3/qm/mv_qm_sysfs.c
+++ b/drivers/net/ethernet/marvell/pp3/qm/mv_qm_sysfs.c
@@ -70,7 +70,7 @@ SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
 #define PR_ERR_CODE(_rc)	\
 {							\
-	pr_err("%s: error code = 0x%08X in function <%s>\n", __func__, _rc, attr->attr.name);	\
+	pr_err("%s: operation failed. probably wrong input (rc=%d)\n", __func__, _rc);	\
 }
 
 #define PR_INFO_CALLED		\
-- 
1.7.5.4

