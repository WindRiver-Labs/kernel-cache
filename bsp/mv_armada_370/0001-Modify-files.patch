From 00e66ea4f072766d7cfaa38bc154c2ec7c7157a0 Mon Sep 17 00:00:00 2001
From: Aaron Ma <pengyu.ma@windriver.com>
Date: Wed, 30 May 2012 17:10:26 +0800
Subject: [PATCH 1/3] Modify files

---
 arch/arm/Kconfig                                   |   50 ++-
 arch/arm/Makefile                                  |    8 +-
 arch/arm/boot/compressed/head.S                    |    9 +-
 arch/arm/include/asm/cacheflush.h                  |   12 +
 arch/arm/include/asm/dma-mapping.h                 |   12 +
 arch/arm/include/asm/elf.h                         |    6 +-
 arch/arm/include/asm/fixmap.h                      |    5 +
 arch/arm/include/asm/hardirq.h                     |    5 +-
 arch/arm/include/asm/highmem.h                     |   28 +
 arch/arm/include/asm/io.h                          |   33 +-
 arch/arm/include/asm/ioctls.h                      |    3 +
 arch/arm/include/asm/mach/pci.h                    |    3 +-
 arch/arm/include/asm/memory.h                      |    4 +
 arch/arm/include/asm/page.h                        |    4 +
 arch/arm/include/asm/perf_event.h                  |    1 +
 arch/arm/include/asm/pgalloc.h                     |   17 +
 arch/arm/include/asm/pgtable-hwdef.h               |    8 +
 arch/arm/include/asm/pgtable.h                     |   18 +-
 arch/arm/include/asm/pmu.h                         |    5 +
 arch/arm/include/asm/proc-fns.h                    |   18 +
 arch/arm/include/asm/processor.h                   |    4 +
 arch/arm/include/asm/ptrace.h                      |   37 ++
 arch/arm/include/asm/setup.h                       |   32 +-
 arch/arm/include/asm/shmparam.h                    |    4 +
 arch/arm/include/asm/siginfo.h                     |    4 +-
 arch/arm/include/asm/smp.h                         |    4 +-
 arch/arm/include/asm/spinlock.h                    |    8 +-
 arch/arm/include/asm/tlbflush.h                    |   45 ++-
 arch/arm/include/asm/vfp.h                         |    6 +
 arch/arm/kernel/Makefile                           |    3 +
 arch/arm/kernel/dma.c                              |   36 ++
 arch/arm/kernel/entry-armv.S                       |   16 +-
 arch/arm/kernel/entry-common.S                     |    6 +-
 arch/arm/kernel/head-common.S                      |    6 +
 arch/arm/kernel/irq.c                              |    3 +
 arch/arm/kernel/module.c                           |   16 +
 arch/arm/kernel/perf_event.c                       |  627 +++++++++++++++++++-
 arch/arm/kernel/pmu.c                              |    4 +
 arch/arm/kernel/process.c                          |   34 +-
 arch/arm/kernel/ptrace.c                           |   97 +++
 arch/arm/kernel/smp.c                              |   16 +-
 arch/arm/kernel/vmlinux.lds.S                      |    3 +
 arch/arm/lib/copy_page.S                           |    4 +
 arch/arm/lib/copy_template.S                       |   91 +++-
 arch/arm/mach-kirkwood/common.c                    |   26 +-
 arch/arm/mach-kirkwood/common.h                    |    2 +
 arch/arm/mach-kirkwood/include/mach/kirkwood.h     |    4 +
 arch/arm/mach-kirkwood/mpp.c                       |    3 +-
 arch/arm/mm/Kconfig                                |  210 +++++++-
 arch/arm/mm/Makefile                               |   10 +-
 arch/arm/mm/abort-ev6.S                            |    2 +
 arch/arm/mm/alignment.c                            |    2 +
 arch/arm/mm/cache-v6.S                             |   85 +++
 arch/arm/mm/cache-v7.S                             |   12 +
 arch/arm/mm/copypage-v6.c                          |    9 +
 arch/arm/mm/fault.c                                |   12 +
 arch/arm/mm/init.c                                 |   17 +-
 arch/arm/mm/mm.h                                   |    4 +
 arch/arm/mm/mmu.c                                  |   50 ++-
 arch/arm/mm/proc-macros.S                          |   58 ++-
 arch/arm/oprofile/common.c                         |    2 +
 arch/arm/tools/mach-types                          |    7 +-
 arch/arm/vfp/vfpdouble.c                           |  277 ++++++---
 arch/arm/vfp/vfpmodule.c                           |   26 +-
 arch/arm/vfp/vfpsingle.c                           |  226 ++++++--
 drivers/ata/libata-pmp.c                           |    7 +
 drivers/ata/sata_mv.c                              |    2 +
 drivers/dma/Kconfig                                |   12 +-
 drivers/dma/dmaengine.c                            |   76 +++
 drivers/dma/iovlock.c                              |   67 +++
 drivers/dma/mv_xor.c                               |    4 +
 drivers/hwmon/Kconfig                              |    8 +
 drivers/i2c/busses/Kconfig                         |    2 +-
 drivers/mmc/card/queue.c                           |    6 +-
 drivers/mmc/core/core.c                            |    7 +-
 drivers/mmc/host/Kconfig                           |    2 +-
 drivers/mtd/maps/Makefile                          |    5 +
 drivers/mtd/nand/Makefile                          |    1 +
 drivers/mtd/nand/nand_base.c                       |   37 ++-
 drivers/mtd/nand/nand_bbt.c                        |   34 +-
 drivers/mtd/nand/nand_ids.c                        |   10 +
 drivers/net/Makefile                               |    2 +
 drivers/rtc/Kconfig                                |    2 +-
 drivers/scsi/Kconfig                               |    6 +
 drivers/scsi/Makefile                              |    2 +
 drivers/serial/8250.c                              |   18 +
 drivers/telephony/Makefile                         |    1 +
 drivers/usb/gadget/Kconfig                         |   17 +-
 drivers/usb/gadget/Makefile                        |    1 +
 drivers/usb/gadget/gadget_chips.h                  |   14 +-
 drivers/usb/gadget/storage_common.c                |    7 +-
 drivers/usb/host/ehci-hcd.c                        |   11 +
 drivers/usb/misc/usbtest.c                         |  179 ++++---
 fs/ext4/file.c                                     |    1 +
 fs/read_write.c                                    |    5 +
 fs/splice.c                                        |  444 ++++++++++++++-
 fs/xfs/linux-2.6/xfs_file.c                        |   55 ++
 include/linux/dmaengine.h                          |    5 +
 include/linux/fs.h                                 |    7 +
 include/linux/libata.h                             |    1 +
 include/linux/miscdevice.h                         |    6 +-
 include/linux/mtd/bbm.h                            |    5 +
 include/linux/mtd/nand.h                           |    9 +-
 include/linux/skbuff.h                             |   11 +
 include/linux/socket.h                             |    3 +
 include/linux/splice.h                             |    8 +
 include/mtd/mtd-abi.h                              |    4 +
 include/net/ip6_fib.h                              |    4 +
 include/net/neighbour.h                            |    3 +
 include/net/netfilter/nf_conntrack.h               |   25 +
 include/net/netfilter/nf_conntrack_tuple.h         |    9 +
 include/net/route.h                                |    4 +
 net/8021q/vlan.c                                   |   35 ++
 net/bridge/br_fdb.c                                |   44 ++-
 net/bridge/br_if.c                                 |   18 +
 net/bridge/br_private.h                            |    4 +
 net/core/datagram.c                                |   59 ++
 net/core/iovec.c                                   |   20 +
 net/core/neighbour.c                               |   71 +++-
 net/core/skbuff.c                                  |   28 +-
 net/ipv4/netfilter/Kconfig                         |    7 +
 net/ipv4/netfilter/Makefile                        |    1 +
 .../netfilter/nf_conntrack_l3proto_ipv4_compat.c   |   13 +
 net/ipv4/route.c                                   |   78 +++
 net/ipv4/tcp.c                                     |   41 ++-
 net/ipv6/ip6_fib.c                                 |   81 +++
 net/ipv6/netfilter/Makefile                        |    1 +
 net/ipv6/route.c                                   |    8 +
 net/netfilter/nf_conntrack_core.c                  |  196 ++++++
 net/netfilter/nf_conntrack_proto_tcp.c             |   16 +-
 net/netfilter/nf_conntrack_standalone.c            |   19 +
 net/socket.c                                       |    1 +
 132 files changed, 3944 insertions(+), 345 deletions(-)

diff --git a/arch/arm/Kconfig b/arch/arm/Kconfig
index d9447bc..62ce6e7 100644
--- a/arch/arm/Kconfig
+++ b/arch/arm/Kconfig
@@ -482,6 +482,34 @@ config ARCH_L7200
 	  If you have any questions or comments about the Linux kernel port
 	  to this board, send e-mail to <sjhill@cotw.com>.
 
+config ARCH_ARMADA370
+	bool "Marvell Armada-370"
+	select PCI
+	select ARCH_REQUIRE_GPIOLIB
+	select GENERIC_GPIO
+	select GENERIC_TIME
+	select GENERIC_CLOCKEVENTS
+	select PLAT_ARMADA
+	select HAVE_REGS_AND_STACK_ACCESS_API
+	select COMMON_CLKDEV
+	help
+	  Support for the Marvell Armada-370 SoC Family
+
+config ARCH_ARMADA_XP
+	bool "Marvell Armada XP"
+	select PCI
+	select ARCH_REQUIRE_GPIOLIB
+	select GENERIC_GPIO
+	select GENERIC_TIME
+	select GENERIC_CLOCKEVENTS
+	select PLAT_ARMADA
+#	select PLAT_ORION
+	select HAVE_REGS_AND_STACK_ACCESS_API
+	select COMMON_CLKDEV
+	help
+	  Support for the Marvell Armada SoC Family:
+	  MV781x0 and MV782x0
+
 config ARCH_DOVE
 	bool "Marvell Dove"
 	select PCI
@@ -842,6 +870,10 @@ endchoice
 #
 source "arch/arm/mach-aaec2000/Kconfig"
 
+source "arch/arm/mach-armada370/Kconfig"
+source "arch/arm/mach-armadaxp/Kconfig"
+source "arch/arm/plat-armada/Kconfig"
+
 source "arch/arm/mach-at91/Kconfig"
 
 source "arch/arm/mach-bcmring/Kconfig"
@@ -956,6 +988,9 @@ source "arch/arm/mach-versatile/Kconfig"
 source "arch/arm/mach-w90x900/Kconfig"
 
 # Definitions to make life easier
+config PLAT_ARMADA
+	bool
+
 config ARCH_ACORN
 	bool
 
@@ -1082,8 +1117,11 @@ source "arch/arm/common/Kconfig"
 
 config FORCE_MAX_ZONEORDER
 	int
-	depends on SA1111
-	default "9"
+	depends on SA1111 || ARCH_ARMADA_XP || ARCH_ARMADA370
+	default "9" if SA1111
+        default "19" if ARCH_ARMADA_XP
+	default "19" if ARCH_ARMADA370
+
 
 menu "Bus support"
 
@@ -1150,10 +1188,10 @@ source "kernel/time/Kconfig"
 config SMP
 	bool "Symmetric Multi-Processing (EXPERIMENTAL)"
 	depends on EXPERIMENTAL && (REALVIEW_EB_ARM11MP || REALVIEW_EB_A9MP ||\
-		 MACH_REALVIEW_PB11MP || MACH_REALVIEW_PBX || ARCH_OMAP4 || ARCH_U8500)
+		 MACH_REALVIEW_PB11MP || MACH_REALVIEW_PBX || ARCH_OMAP4 || ARCH_U8500 || ARCH_ARMADA_XP)
 	depends on GENERIC_CLOCKEVENTS
 	select USE_GENERIC_SMP_HELPERS
-	select HAVE_ARM_SCU if (ARCH_REALVIEW || ARCH_OMAP4 || ARCH_U8500)
+	select HAVE_ARM_SCU if (ARCH_REALVIEW || ARCH_OMAP4 || ARCH_U8500 || ARCH_ARMADA_XP)
 	help
 	  This enables support for systems with more than one CPU. If you have
 	  a system with only one CPU, like most personal computers, say N. If
@@ -1222,9 +1260,9 @@ config HOTPLUG_CPU
 config LOCAL_TIMERS
 	bool "Use local timer interrupts"
 	depends on SMP && (REALVIEW_EB_ARM11MP || MACH_REALVIEW_PB11MP || \
-		REALVIEW_EB_A9MP || MACH_REALVIEW_PBX || ARCH_OMAP4 || ARCH_U8500)
+		REALVIEW_EB_A9MP || MACH_REALVIEW_PBX || ARCH_OMAP4 || ARCH_U8500 || ARCH_ARMADA_XP)
 	default y
-	select HAVE_ARM_TWD if (ARCH_REALVIEW || ARCH_OMAP4 || ARCH_U8500)
+	select HAVE_ARM_TWD if (ARCH_REALVIEW || ARCH_OMAP4 || ARCH_U8500 || ARCH_ARMADA_XP)
 	help
 	  Enable support for local timers on SMP platforms, rather then the
 	  legacy IPI broadcast method.  Local timers allows the system
diff --git a/arch/arm/Makefile b/arch/arm/Makefile
index fc0db26..1eef110 100644
--- a/arch/arm/Makefile
+++ b/arch/arm/Makefile
@@ -13,6 +13,7 @@
 LDFLAGS_vmlinux	:=-p --no-undefined -X
 ifeq ($(CONFIG_CPU_ENDIAN_BE8),y)
 LDFLAGS_vmlinux	+= --be8
+LDFLAGS_MODULE	+= --be8
 endif
 
 OBJCOPYFLAGS	:=-O binary -R .note -R .note.gnu.build-id -R .comment -S
@@ -119,6 +120,8 @@ endif
 # Machine directory name.  This list is sorted alphanumerically
 # by CONFIG_* macro name.
 machine-$(CONFIG_ARCH_AAEC2000)		:= aaec2000
+machine-$(CONFIG_ARCH_ARMADA_XP)	:= armadaxp
+machine-$(CONFIG_ARCH_ARMADA370)	:= armada370
 machine-$(CONFIG_ARCH_AT91)		:= at91
 machine-$(CONFIG_ARCH_BCMRING)		:= bcmring
 machine-$(CONFIG_ARCH_CLPS711X)		:= clps711x
@@ -188,6 +191,7 @@ plat-$(CONFIG_ARCH_MXC)		:= mxc
 plat-$(CONFIG_ARCH_OMAP)	:= omap
 plat-$(CONFIG_ARCH_S3C64XX)	:= samsung
 plat-$(CONFIG_ARCH_STMP3XXX)	:= stmp3xxx
+plat-$(CONFIG_PLAT_ARMADA)	:= armada
 plat-$(CONFIG_PLAT_VERSATILE)	:= versatile
 plat-$(CONFIG_PLAT_IOP)		:= iop
 plat-$(CONFIG_PLAT_NOMADIK)	:= nomadik
@@ -230,7 +234,9 @@ else
 KBUILD_CPPFLAGS += $(patsubst %,-I$(srctree)/%include,$(machdirs) $(platdirs))
 endif
 
-export	TEXT_OFFSET GZFLAGS MMUEXT
+# Remove trailing '/' from MACHINE
+MACH_DIR :=  $(patsubst %/,%,$(MACHINE))
+export	TEXT_OFFSET GZFLAGS MMUEXT MACHINE MACH_DIR
 
 # Do we have FASTFPE?
 FASTFPE		:=arch/arm/fastfpe
diff --git a/arch/arm/boot/compressed/head.S b/arch/arm/boot/compressed/head.S
index c5191b1..9ff9780 100644
--- a/arch/arm/boot/compressed/head.S
+++ b/arch/arm/boot/compressed/head.S
@@ -131,6 +131,9 @@ start:
 		mov	r0, r0
 		.endr
 
+#ifdef CONFIG_BE8_ON_LE
+		setend	be
+#endif
 		b	1f
 		.word	0x016f2818		@ Magic numbers to help the loader
 		.word	start			@ absolute load/run zImage address
@@ -737,9 +740,13 @@ proc_types:
 		W(b)	__armv4_mmu_cache_on
 		W(b)	__armv4_mmu_cache_off
 		W(b)	__armv5tej_mmu_cache_flush
-
+#ifdef CONFIG_CPU_SHEEVA_PJ4B_V6
+		.word	0x000f0000		@ Marvell PJ4B ARMv6
+		.word	0x000f0000
+#else
 		.word	0x0007b000		@ ARMv6
 		.word	0x000ff000
+#endif
 		W(b)	__armv4_mmu_cache_on
 		W(b)	__armv4_mmu_cache_off
 		W(b)	__armv6_mmu_cache_flush
diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 4656a24..556e4ee 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -132,6 +132,10 @@
 //# endif
 #endif
 
+#if defined(CONFIG_CPU_SHEEVA_PJ4B_V6) || defined(CONFIG_CPU_SHEEVA_PJ4B_V7)
+#  define MULTI_CACHE 1
+#endif
+
 #if !defined(_CACHE) && !defined(MULTI_CACHE)
 #error Unknown cache maintainence model
 #endif
@@ -349,7 +353,15 @@ extern void flush_cache_page(struct vm_area_struct *vma, unsigned long user_addr
  * Perform necessary cache operations to ensure that the TLB will
  * see data written in the specified area.
  */
+#if defined (CONFIG_CACHE_AURORA_L2) && !defined (CONFIG_AURORA_L2_PT_WALK)
+/*#warning "clean_dcache_area: Using D$ FLUSH instead of CLEAN. To be Checked\n"*/
+extern void aurora_l2_flush_range(unsigned long start, unsigned long end);
+
+#define clean_dcache_area(start,size)	do {cpu_dcache_clean_area(start, size);	\
+	aurora_l2_flush_range(__pa(start), __pa(start) + size);} while (0)			
+#else
 #define clean_dcache_area(start,size)	cpu_dcache_clean_area(start, size)
+#endif
 
 /*
  * flush_dcache_page is used when the kernel has written to the page
diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 69ce072..073786c 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -78,6 +78,10 @@ static inline void __dma_single_cpu_to_dev(const void *kaddr, size_t size,
 
 	if (!arch_is_coherent())
 		___dma_single_cpu_to_dev(kaddr, size, dir);
+#ifdef CONFIG_AURORA_IOCC_DISABLE_WRITE_ALLOCATE
+	else
+		dsb();
+#endif
 }
 
 static inline void __dma_single_dev_to_cpu(const void *kaddr, size_t size,
@@ -88,6 +92,8 @@ static inline void __dma_single_dev_to_cpu(const void *kaddr, size_t size,
 
 	if (!arch_is_coherent())
 		___dma_single_dev_to_cpu(kaddr, size, dir);
+	else if (dir != DMA_TO_DEVICE)
+		dma_io_sync();
 }
 
 static inline void __dma_page_cpu_to_dev(struct page *page, unsigned long off,
@@ -98,6 +104,10 @@ static inline void __dma_page_cpu_to_dev(struct page *page, unsigned long off,
 
 	if (!arch_is_coherent())
 		___dma_page_cpu_to_dev(page, off, size, dir);
+#ifdef CONFIG_AURORA_IOCC_DISABLE_WRITE_ALLOCATE
+	else
+		dsb();
+#endif
 }
 
 static inline void __dma_page_dev_to_cpu(struct page *page, unsigned long off,
@@ -108,6 +118,8 @@ static inline void __dma_page_dev_to_cpu(struct page *page, unsigned long off,
 
 	if (!arch_is_coherent())
 		___dma_page_dev_to_cpu(page, off, size, dir);
+	else if (dir != DMA_TO_DEVICE)
+		dma_io_sync();
 }
 
 /*
diff --git a/arch/arm/include/asm/elf.h b/arch/arm/include/asm/elf.h
index 4d0e730..8cb9fe1 100644
--- a/arch/arm/include/asm/elf.h
+++ b/arch/arm/include/asm/elf.h
@@ -106,7 +106,11 @@ struct task_struct;
 int dump_task_regs(struct task_struct *t, elf_gregset_t *elfregs);
 #define ELF_CORE_COPY_TASK_REGS dump_task_regs
 
-#define ELF_EXEC_PAGESIZE	4096
+#ifdef CONFIG_MV_SUPPORT_64KB_PAGE_SIZE
+#define ELF_EXEC_PAGESIZE       PAGE_SIZE
+#else
+#define ELF_EXEC_PAGESIZE       4096
+#endif
 
 /* This is the location that an ET_DYN program is loaded if exec'ed.  Typical
    use of this is to invoke "./ld.so someprog" to test out a new version of
diff --git a/arch/arm/include/asm/fixmap.h b/arch/arm/include/asm/fixmap.h
index bbae919..dc364ca 100644
--- a/arch/arm/include/asm/fixmap.h
+++ b/arch/arm/include/asm/fixmap.h
@@ -13,8 +13,13 @@
  * 0xfffe0000 and 0xfffeffff.
  */
 
+#if defined(CONFIG_MV_SUPPORT_64KB_PAGE_SIZE) && defined(CONFIG_HIGHMEM)
+#define FIXADDR_START		0xffc00000UL
+#define FIXADDR_TOP		0xfff00000UL
+#else
 #define FIXADDR_START		0xfff00000UL
 #define FIXADDR_TOP		0xfffe0000UL
+#endif
 #define FIXADDR_SIZE		(FIXADDR_TOP - FIXADDR_START)
 
 #define FIX_KMAP_BEGIN		0
diff --git a/arch/arm/include/asm/hardirq.h b/arch/arm/include/asm/hardirq.h
index 182310b..2514070 100644
--- a/arch/arm/include/asm/hardirq.h
+++ b/arch/arm/include/asm/hardirq.h
@@ -8,11 +8,14 @@
 typedef struct {
 	unsigned int __softirq_pending;
 	unsigned int local_timer_irqs;
+	unsigned int local_pmu_irqs;
 } ____cacheline_aligned irq_cpustat_t;
 
 #include <linux/irq_cpustat.h>	/* Standard mappings for irq_cpustat_t above */
 
-#if NR_IRQS > 256
+#if NR_IRQS > 512
+#define HARDIRQ_BITS	10
+#elif NR_IRQS > 256
 #define HARDIRQ_BITS	9
 #else
 #define HARDIRQ_BITS	8
diff --git a/arch/arm/include/asm/highmem.h b/arch/arm/include/asm/highmem.h
index feb988a..5e53d87 100644
--- a/arch/arm/include/asm/highmem.h
+++ b/arch/arm/include/asm/highmem.h
@@ -29,6 +29,34 @@ extern void *kmap_high_l1_vipt(struct page *page, pte_t *saved_pte);
 extern void kunmap_high_l1_vipt(struct page *page, pte_t saved_pte);
 
 /*
+ * The reason for kmap_high_get() is to ensure that the currently kmap'd
+ * page usage count does not decrease to zero while we're using its
+ * existing virtual mapping in an atomic context.  With a VIVT cache this
+ * is essential to do, but with a VIPT cache this is only an optimization
+ * so not to pay the price of establishing a second mapping if an existing
+ * one can be used.  However, on platforms without hardware TLB maintainence
+ * broadcast, we simply cannot use ARCH_NEEDS_KMAP_HIGH_GET at all since
+ * the locking involved must also disable IRQs which is incompatible with
+ * the IPI mechanism used by global TLB operations.
+ */
+#define ARCH_NEEDS_KMAP_HIGH_GET
+#if defined(CONFIG_SMP) && defined(CONFIG_CPU_TLB_V6)
+#undef ARCH_NEEDS_KMAP_HIGH_GET
+#if defined(CONFIG_HIGHMEM) && defined(CONFIG_CPU_CACHE_VIVT)
+#error "The sum of feature in your kernel config cannot be supported together"
+#endif
+#endif
+
+#ifdef ARCH_NEEDS_KMAP_HIGH_GET
+extern void *kmap_high_get(struct page *page);
+#else
+static inline void *kmap_high_get(struct page *page)
+{
+	return NULL;
+}
+#endif
+
+/*
  * The following functions are already defined by <linux/highmem.h>
  * when CONFIG_HIGHMEM is not set.
  */
diff --git a/arch/arm/include/asm/io.h b/arch/arm/include/asm/io.h
index 9db072d..1261b1f9 100644
--- a/arch/arm/include/asm/io.h
+++ b/arch/arm/include/asm/io.h
@@ -26,6 +26,7 @@
 #include <linux/types.h>
 #include <asm/byteorder.h>
 #include <asm/memory.h>
+#include <asm/system.h>
 
 /*
  * ISA I/O bus memory addresses are 1:1 with the physical address.
@@ -191,13 +192,21 @@ extern void _memset_io(volatile void __iomem *, int, size_t);
 #define writel_relaxed(v,c)	((void)__raw_writel((__force u32) \
 					cpu_to_le32(v),__mem_pci(c)))
 
-#define readb(c)		readb_relaxed(c)
-#define readw(c)		readw_relaxed(c)
-#define readl(c)		readl_relaxed(c)
+#ifdef CONFIG_ARM_DMA_MEM_BUFFERABLE
+#define __iormb()		rmb()
+#define __iowmb()		wmb()
+#else
+#define __iormb()		do { } while (0)
+#define __iowmb()		do { } while (0)
+#endif
+
+#define readb(c)		({ u8  __v = readb_relaxed(c); __iormb(); __v; })
+#define readw(c)		({ u16 __v = readw_relaxed(c); __iormb(); __v; })
+#define readl(c)		({ u32 __v = readl_relaxed(c); __iormb(); __v; })
 
-#define writeb(v,c)		writeb_relaxed(v,c)
-#define writew(v,c)		writew_relaxed(v,c)
-#define writel(v,c)		writel_relaxed(v,c)
+#define writeb(v,c)		({ __iowmb(); writeb_relaxed(v,c); })
+#define writew(v,c)		({ __iowmb(); writew_relaxed(v,c); })
+#define writel(v,c)		({ __iowmb(); writel_relaxed(v,c); })
 
 #define readsb(p,d,l)		__raw_readsb(__mem_pci(p),d,l)
 #define readsw(p,d,l)		__raw_readsw(__mem_pci(p),d,l)
@@ -249,13 +258,13 @@ extern void _memset_io(volatile void __iomem *, int, size_t);
  * io{read,write}{8,16,32} macros
  */
 #ifndef ioread8
-#define ioread8(p)	({ unsigned int __v = __raw_readb(p); __v; })
-#define ioread16(p)	({ unsigned int __v = le16_to_cpu((__force __le16)__raw_readw(p)); __v; })
-#define ioread32(p)	({ unsigned int __v = le32_to_cpu((__force __le32)__raw_readl(p)); __v; })
+#define ioread8(p)	({ unsigned int __v = __raw_readb(p); __iormb(); __v; })
+#define ioread16(p)	({ unsigned int __v = le16_to_cpu((__force __le16)__raw_readw(p)); __iormb(); __v; })
+#define ioread32(p)	({ unsigned int __v = le32_to_cpu((__force __le32)__raw_readl(p)); __iormb(); __v; })
 
-#define iowrite8(v,p)	__raw_writeb(v, p)
-#define iowrite16(v,p)	__raw_writew((__force __u16)cpu_to_le16(v), p)
-#define iowrite32(v,p)	__raw_writel((__force __u32)cpu_to_le32(v), p)
+#define iowrite8(v,p)	({ __iowmb(); (void)__raw_writeb(v, p); })
+#define iowrite16(v,p)	({ __iowmb(); (void)__raw_writew((__force __u16)cpu_to_le16(v), p); })
+#define iowrite32(v,p)	({ __iowmb(); (void)__raw_writel((__force __u32)cpu_to_le32(v), p); })
 
 #define ioread8_rep(p,d,c)	__raw_readsb(p,d,c)
 #define ioread16_rep(p,d,c)	__raw_readsw(p,d,c)
diff --git a/arch/arm/include/asm/ioctls.h b/arch/arm/include/asm/ioctls.h
index a91d8a1..7f0b6d1 100644
--- a/arch/arm/include/asm/ioctls.h
+++ b/arch/arm/include/asm/ioctls.h
@@ -53,6 +53,9 @@
 #define TIOCGPTN	_IOR('T',0x30, unsigned int) /* Get Pty Number (of pty-mux device) */
 #define TIOCSPTLCK	_IOW('T',0x31, int)  /* Lock/unlock Pty */
 
+#define TIOCGRS485      0x542E
+#define TIOCSRS485      0x542F
+
 #define FIONCLEX	0x5450  /* these numbers need to be adjusted. */
 #define FIOCLEX		0x5451
 #define FIOASYNC	0x5452
diff --git a/arch/arm/include/asm/mach/pci.h b/arch/arm/include/asm/mach/pci.h
index 1653b4b..e8125b1 100644
--- a/arch/arm/include/asm/mach/pci.h
+++ b/arch/arm/include/asm/mach/pci.h
@@ -33,7 +33,7 @@ struct hw_pci {
 struct pci_sys_data {
 #ifdef CONFIG_PCI_DOMAINS
 	int		domain;
-#endif	
+#endif
 	struct list_head node;
 	int		busnr;		/* primary bus number			*/
 	u64		mem_offset;	/* bus->cpu memory mapping offset	*/
@@ -45,6 +45,7 @@ struct pci_sys_data {
 					/* IRQ mapping				*/
 	int		(*map_irq)(struct pci_dev *, u8, u8);
 	struct hw_pci	*hw;
+	int    mv_controller_num;
 };
 
 /*
diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 4312ee5..6a4deaa 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -84,7 +84,11 @@
 #define CONSISTENT_DMA_SIZE 	SZ_2M
 #endif
 
+#if defined(CONFIG_MV_SUPPORT_64KB_PAGE_SIZE) && defined(CONFIG_HIGHMEM)
+#define CONSISTENT_END		(0xffc00000UL)
+#else
 #define CONSISTENT_END		(0xffe00000UL)
+#endif
 #define CONSISTENT_BASE		(CONSISTENT_END - CONSISTENT_DMA_SIZE)
 
 #else /* CONFIG_MMU */
diff --git a/arch/arm/include/asm/page.h b/arch/arm/include/asm/page.h
index a485ac3..9cb401a 100644
--- a/arch/arm/include/asm/page.h
+++ b/arch/arm/include/asm/page.h
@@ -11,7 +11,11 @@
 #define _ASMARM_PAGE_H
 
 /* PAGE_SHIFT determines the page size */
+#ifdef CONFIG_MV_SUPPORT_64KB_PAGE_SIZE
+#define PAGE_SHIFT		16
+#else
 #define PAGE_SHIFT		12
+#endif
 #define PAGE_SIZE		(_AC(1,UL) << PAGE_SHIFT)
 #define PAGE_MASK		(~(PAGE_SIZE-1))
 
diff --git a/arch/arm/include/asm/perf_event.h b/arch/arm/include/asm/perf_event.h
index 48837e6..c9a9057 100644
--- a/arch/arm/include/asm/perf_event.h
+++ b/arch/arm/include/asm/perf_event.h
@@ -36,6 +36,7 @@ enum arm_perf_pmu_ids {
 	ARM_PERF_PMU_ID_V6MP,
 	ARM_PERF_PMU_ID_CA8,
 	ARM_PERF_PMU_ID_CA9,
+	MRVL_PERF_PMU_ID_PJ4B,
 	ARM_NUM_PMU_IDS,
 };
 
diff --git a/arch/arm/include/asm/pgalloc.h b/arch/arm/include/asm/pgalloc.h
index b12cc98..c8566e9 100644
--- a/arch/arm/include/asm/pgalloc.h
+++ b/arch/arm/include/asm/pgalloc.h
@@ -61,8 +61,13 @@ pte_alloc_one_kernel(struct mm_struct *mm, unsigned long addr)
 
 	pte = (pte_t *)__get_free_page(PGALLOC_GFP);
 	if (pte) {
+#ifdef CONFIG_MV_SUPPORT_64KB_PAGE_SIZE
+		clean_dcache_area(pte, sizeof(pte_t) * HW_PTRS_PER_PTE);
+		pte += HW_PTRS_PER_PTE;
+#else
 		clean_dcache_area(pte, sizeof(pte_t) * PTRS_PER_PTE);
 		pte += PTRS_PER_PTE;
+#endif
 	}
 
 	return pte;
@@ -81,7 +86,11 @@ pte_alloc_one(struct mm_struct *mm, unsigned long addr)
 	if (pte) {
 		if (!PageHighMem(pte)) {
 			void *page = page_address(pte);
+#ifdef CONFIG_MV_SUPPORT_64KB_PAGE_SIZE
+			clean_dcache_area(page, sizeof(pte_t) * HW_PTRS_PER_PTE);
+#else
 			clean_dcache_area(page, sizeof(pte_t) * PTRS_PER_PTE);
+#endif
 		}
 		pgtable_page_ctor(pte);
 	}
@@ -95,7 +104,11 @@ pte_alloc_one(struct mm_struct *mm, unsigned long addr)
 static inline void pte_free_kernel(struct mm_struct *mm, pte_t *pte)
 {
 	if (pte) {
+#ifdef CONFIG_MV_SUPPORT_64KB_PAGE_SIZE
+		pte -= HW_PTRS_PER_PTE;
+#else
 		pte -= PTRS_PER_PTE;
+#endif
 		free_page((unsigned long)pte);
 	}
 }
@@ -128,7 +141,11 @@ pmd_populate_kernel(struct mm_struct *mm, pmd_t *pmdp, pte_t *ptep)
 	 * The pmd must be loaded with the physical
 	 * address of the PTE table
 	 */
+#ifdef CONFIG_MV_SUPPORT_64KB_PAGE_SIZE
+	pte_ptr -= HW_PTRS_PER_PTE * sizeof(void *);
+#else
 	pte_ptr -= PTRS_PER_PTE * sizeof(void *);
+#endif
 	__pmd_populate(pmdp, __pa(pte_ptr) | _PAGE_KERNEL_TABLE);
 }
 
diff --git a/arch/arm/include/asm/pgtable-hwdef.h b/arch/arm/include/asm/pgtable-hwdef.h
index fd1521d..95b5457 100644
--- a/arch/arm/include/asm/pgtable-hwdef.h
+++ b/arch/arm/include/asm/pgtable-hwdef.h
@@ -64,7 +64,11 @@
 /*
  *   - extended small page/tiny page
  */
+#ifdef CONFIG_MV_SUPPORT_64KB_PAGE_SIZE
+#define PTE_EXT_XN		(1 << 15)	/* v6 */
+#else
 #define PTE_EXT_XN		(1 << 0)	/* v6 */
+#endif
 #define PTE_EXT_AP_MASK		(3 << 4)
 #define PTE_EXT_AP0		(1 << 4)
 #define PTE_EXT_AP1		(2 << 4)
@@ -72,7 +76,11 @@
 #define PTE_EXT_AP_UNO_SRW	(PTE_EXT_AP0)
 #define PTE_EXT_AP_URO_SRW	(PTE_EXT_AP1)
 #define PTE_EXT_AP_URW_SRW	(PTE_EXT_AP1|PTE_EXT_AP0)
+#ifdef CONFIG_MV_SUPPORT_64KB_PAGE_SIZE
+#define PTE_EXT_TEX(x)		((x) << 12)	/* Large Page */
+#else
 #define PTE_EXT_TEX(x)		((x) << 6)	/* v5 */
+#endif
 #define PTE_EXT_APX		(1 << 9)	/* v6 */
 #define PTE_EXT_COHERENT	(1 << 9)	/* XScale3 */
 #define PTE_EXT_SHARED		(1 << 10)	/* v6 */
diff --git a/arch/arm/include/asm/pgtable.h b/arch/arm/include/asm/pgtable.h
index 1139768..85f4539 100644
--- a/arch/arm/include/asm/pgtable.h
+++ b/arch/arm/include/asm/pgtable.h
@@ -98,7 +98,12 @@
  * until either the TLB entry is evicted under pressure, or a context
  * switch which changes the user space mapping occurs.
  */
+#ifdef CONFIG_MV_SUPPORT_64KB_PAGE_SIZE
+#define PTRS_PER_PTE		32	/* (512 / (64K / 4K)) */
+#define HW_PTRS_PER_PTE		512
+#else
 #define PTRS_PER_PTE		512
+#endif
 #define PTRS_PER_PMD		1
 #define PTRS_PER_PGD		2048
 
@@ -276,7 +281,13 @@ extern struct page *empty_zero_page;
 #define __pte_unmap(pte,km)	kunmap_atomic((pte - PTRS_PER_PTE), km)
 #endif
 
+#if defined (CONFIG_CACHE_AURORA_L2) && !defined (CONFIG_AURORA_L2_PT_WALK)
+extern void l2_clean_va(unsigned int addr);
+#define set_pte_ext(ptep,pte,ext) do {cpu_set_pte_ext(ptep,pte,ext); \
+	l2_clean_va((unsigned int)ptep);} while (0)
+#else
 #define set_pte_ext(ptep,pte,ext) cpu_set_pte_ext(ptep,pte,ext)
+#endif
 
 #define set_pte_at(mm,addr,ptep,pteval) do { \
 	set_pte_ext(ptep, pteval, (addr) >= TASK_SIZE ? 0 : PTE_EXT_NG); \
@@ -314,7 +325,7 @@ static inline pte_t pte_mkspecial(pte_t pte) { return pte; }
 	__pgprot_modify(prot, L_PTE_MT_MASK, L_PTE_MT_UNCACHED)
 #define pgprot_writecombine(prot) \
 	__pgprot_modify(prot, L_PTE_MT_MASK, L_PTE_MT_BUFFERABLE)
-#if __LINUX_ARM_ARCH__ >= 7
+#ifdef CONFIG_ARM_DMA_MEM_BUFFERABLE
 #define pgprot_dmacoherent(prot) \
 	__pgprot_modify(prot, L_PTE_MT_MASK|L_PTE_EXEC, L_PTE_MT_BUFFERABLE)
 #else
@@ -344,8 +355,13 @@ static inline pte_t *pmd_page_vaddr(pmd_t pmd)
 {
 	unsigned long ptr;
 
+#ifdef CONFIG_MV_SUPPORT_64KB_PAGE_SIZE
+        ptr = pmd_val(pmd) & ~(HW_PTRS_PER_PTE * sizeof(void *) - 1);
+        ptr += HW_PTRS_PER_PTE * sizeof(void *);
+#else
 	ptr = pmd_val(pmd) & ~(PTRS_PER_PTE * sizeof(void *) - 1);
 	ptr += PTRS_PER_PTE * sizeof(void *);
+#endif
 
 	return __va(ptr);
 }
diff --git a/arch/arm/include/asm/pmu.h b/arch/arm/include/asm/pmu.h
index 2829b9f..907e341 100644
--- a/arch/arm/include/asm/pmu.h
+++ b/arch/arm/include/asm/pmu.h
@@ -19,6 +19,11 @@ struct pmu_irqs {
 	int	    num_irqs;
 };
 
+enum arm_pmu_type {
+        ARM_PMU_DEVICE_CPU      = 0,
+        ARM_NUM_PMU_DEVICES,
+};
+
 /**
  * reserve_pmu() - reserve the hardware performance counters
  *
diff --git a/arch/arm/include/asm/proc-fns.h b/arch/arm/include/asm/proc-fns.h
index 15ea310..5584c76 100644
--- a/arch/arm/include/asm/proc-fns.h
+++ b/arch/arm/include/asm/proc-fns.h
@@ -249,6 +249,24 @@
 # endif
 #endif
 
+#ifdef CONFIG_CPU_SHEEVA_PJ4B_V6
+# ifdef CPU_NAME
+#  undef  MULTI_CPU
+#  define MULTI_CPU
+# else
+#  define CPU_NAME cpu_sheeva_pj4b_v6
+# endif
+#endif
+
+#ifdef CONFIG_CPU_SHEEVA_PJ4B_V7
+# ifdef CPU_NAME
+#  undef  MULTI_CPU
+#  define MULTI_CPU
+# else
+#  define CPU_NAME cpu_sheeva_pj4b_v7
+# endif
+#endif
+
 #ifndef __ASSEMBLY__
 
 #ifndef MULTI_CPU
diff --git a/arch/arm/include/asm/processor.h b/arch/arm/include/asm/processor.h
index 09a431d..c150142 100644
--- a/arch/arm/include/asm/processor.h
+++ b/arch/arm/include/asm/processor.h
@@ -92,7 +92,11 @@ extern void release_thread(struct task_struct *);
 
 unsigned long get_wchan(struct task_struct *p);
 
+#if __LINUX_ARM_ARCH__ == 6
+#define cpu_relax()			smp_mb()
+#else
 #define cpu_relax()			barrier()
+#endif
 
 /*
  * Create a new kernel thread
diff --git a/arch/arm/include/asm/ptrace.h b/arch/arm/include/asm/ptrace.h
index 2d83f14..1c1dd6c 100644
--- a/arch/arm/include/asm/ptrace.h
+++ b/arch/arm/include/asm/ptrace.h
@@ -195,6 +195,43 @@ extern unsigned long profile_pc(struct pt_regs *regs);
 #define predicate(x)		((x) & 0xf0000000)
 #define PREDICATE_ALWAYS	0xe0000000
 
+/*
+ * kprobe-based event tracer support
+ */
+#include <linux/stddef.h>
+#include <linux/types.h>
+#define MAX_REG_OFFSET (offsetof(struct pt_regs, ARM_ORIG_r0))
+
+extern int regs_query_register_offset(const char *name);
+extern const char *regs_query_register_name(unsigned int offset);
+extern bool regs_within_kernel_stack(struct pt_regs *regs, unsigned long addr);
+extern unsigned long regs_get_kernel_stack_nth(struct pt_regs *regs,
+					       unsigned int n);
+
+/**
+ * regs_get_register() - get register value from its offset
+ * @regs:	   pt_regs from which register value is gotten
+ * @offset:    offset number of the register.
+ *
+ * regs_get_register returns the value of a register whose offset from @regs.
+ * The @offset is the offset of the register in struct pt_regs.
+ * If @offset is bigger than MAX_REG_OFFSET, this returns 0.
+ */
+static inline unsigned long regs_get_register(struct pt_regs *regs,
+					      unsigned int offset)
+{
+	if (unlikely(offset > MAX_REG_OFFSET))
+		return 0;
+	return *(unsigned long *)((unsigned long)regs + offset);
+}
+
+/* Valid only for Kernel mode traps. */
+static inline unsigned long kernel_stack_pointer(struct pt_regs *regs)
+{
+	return regs->ARM_sp;
+}
+
+
 #endif /* __KERNEL__ */
 
 #endif /* __ASSEMBLY__ */
diff --git a/arch/arm/include/asm/setup.h b/arch/arm/include/asm/setup.h
index f392fb4..76c3e94 100644
--- a/arch/arm/include/asm/setup.h
+++ b/arch/arm/include/asm/setup.h
@@ -143,6 +143,20 @@ struct tag_memclk {
 	__u32 fmemclk;
 };
 
+/* Marvell uboot parameters */
+#define ATAG_MV_UBOOT   	0x41000403
+#define MV_UBOOT_MAX_PORT	4
+struct tag_mv_uboot {
+	__u32 uboot_version;
+	__u32 tclk;
+	__u32 sysclk;
+	__u32 isUsbHost;
+	__u8  macAddr[MV_UBOOT_MAX_PORT][6];
+	__u16 mtu[MV_UBOOT_MAX_PORT];
+	__u32 nand_ecc;
+	__u32 bit_mask_config;
+};                     
+
 struct tag {
 	struct tag_header hdr;
 	union {
@@ -165,6 +179,11 @@ struct tag {
 		 * DC21285 specific
 		 */
 		struct tag_memclk	memclk;
+
+		/*
+		 * Marvell specific
+		 */
+		struct tag_mv_uboot     mv_uboot;
 	} u;
 };
 
@@ -173,15 +192,20 @@ struct tagtable {
 	int (*parse)(const struct tag *);
 };
 
+#ifdef CONFIG_BE8_ON_LE
+#define read_tag(a)	le32_to_cpu(a)
+#else
+#define read_tag(a)	a
+#endif
+
 #define tag_member_present(tag,member)				\
 	((unsigned long)(&((struct tag *)0L)->member + 1)	\
-		<= (tag)->hdr.size * 4)
-
-#define tag_next(t)	((struct tag *)((__u32 *)(t) + (t)->hdr.size))
+		<= read_tag((tag)->hdr.size) * 4)
+#define tag_next(t)	((struct tag *)((__u32 *)(t) + read_tag((t)->hdr.size)))
 #define tag_size(type)	((sizeof(struct tag_header) + sizeof(struct type)) >> 2)
 
 #define for_each_tag(t,base)		\
-	for (t = base; t->hdr.size; t = tag_next(t))
+	for (t = base; read_tag((t)->hdr.size); t = tag_next(t))
 
 #ifdef __KERNEL__
 
diff --git a/arch/arm/include/asm/shmparam.h b/arch/arm/include/asm/shmparam.h
index a5223b3..8e977b6 100644
--- a/arch/arm/include/asm/shmparam.h
+++ b/arch/arm/include/asm/shmparam.h
@@ -6,7 +6,11 @@
  * or page size, whichever is greater since the cache aliases
  * every size/ways bytes.
  */
+#ifdef CONFIG_MV_SUPPORT_64KB_PAGE_SIZE
+#define	SHMLBA	(16 << 10)		 /* attach addr a multiple of this */
+#else
 #define	SHMLBA	(4 * PAGE_SIZE)		 /* attach addr a multiple of this */
+#endif
 
 /*
  * Enforce SHMLBA in shmat
diff --git a/arch/arm/include/asm/siginfo.h b/arch/arm/include/asm/siginfo.h
index 5e21852..503e161 100644
--- a/arch/arm/include/asm/siginfo.h
+++ b/arch/arm/include/asm/siginfo.h
@@ -2,5 +2,7 @@
 #define _ASMARM_SIGINFO_H
 
 #include <asm-generic/siginfo.h>
-
+#define	FPE_FLTISN	(__SI_FAULT|9)	/* Input Subnormal (VFPv2) */
+#undef 	NSIGFPE
+#define NSIGFPE		9
 #endif
diff --git a/arch/arm/include/asm/smp.h b/arch/arm/include/asm/smp.h
index e0d763b..f3018ea 100644
--- a/arch/arm/include/asm/smp.h
+++ b/arch/arm/include/asm/smp.h
@@ -98,5 +98,7 @@ extern void arch_send_call_function_ipi_mask(const struct cpumask *mask);
  * show local interrupt info
  */
 extern void show_local_irqs(struct seq_file *);
-
+#if defined(CONFIG_ARCH_ARMADA_XP) && defined(CONFIG_PERF_EVENTS)
+extern void show_local_pmu_irqs(struct seq_file *);
+#endif
 #endif /* ifndef __ASM_ARM_SMP_H */
diff --git a/arch/arm/include/asm/spinlock.h b/arch/arm/include/asm/spinlock.h
index 17eb355..4082445 100644
--- a/arch/arm/include/asm/spinlock.h
+++ b/arch/arm/include/asm/spinlock.h
@@ -47,8 +47,10 @@ static inline void arch_spin_lock(arch_spinlock_t *lock)
 "1:	ldrex	%0, [%1]\n"
 "	teq	%0, #0\n"
 #ifdef CONFIG_CPU_32v6K
+#if !defined(CONFIG_ARCH_ARMADA370) && !defined(CONFIG_ARCH_ARMADA_XP)
 "	wfene\n"
 #endif
+#endif
 "	strexeq	%0, %2, [%1]\n"
 "	teqeq	%0, #0\n"
 "	bne	1b"
@@ -108,8 +110,10 @@ static inline void arch_write_lock(arch_rwlock_t *rw)
 "1:	ldrex	%0, [%1]\n"
 "	teq	%0, #0\n"
 #ifdef CONFIG_CPU_32v6K
+#if !defined(CONFIG_ARCH_ARMADA370) && !defined(CONFIG_ARCH_ARMADA_XP)
 "	wfene\n"
 #endif
+#endif
 "	strexeq	%0, %2, [%1]\n"
 "	teq	%0, #0\n"
 "	bne	1b"
@@ -177,7 +181,9 @@ static inline void arch_read_lock(arch_rwlock_t *rw)
 "	adds	%0, %0, #1\n"
 "	strexpl	%1, %0, [%2]\n"
 #ifdef CONFIG_CPU_32v6K
-"	wfemi\n"
+#ifndef CONFIG_SHEEVA_ERRATA_ARM_CPU_BTS61
+"	wfene\n"
+#endif
 #endif
 "	rsbpls	%0, %1, #0\n"
 "	bmi	1b"
diff --git a/arch/arm/include/asm/tlbflush.h b/arch/arm/include/asm/tlbflush.h
index 3e6b2d9..6d1f116 100644
--- a/arch/arm/include/asm/tlbflush.h
+++ b/arch/arm/include/asm/tlbflush.h
@@ -176,8 +176,13 @@
 			 TLB_V6_I_ASID | TLB_V6_D_ASID)
 
 #ifdef CONFIG_CPU_TLB_V6
+#if (defined(CONFIG_ARCH_ARMADA370) || defined(CONFIG_ARCH_ARMADA_XP)) && !defined(CONFIG_AURORA_L2_PT_WALK)
+# define v6wbi_possible_flags	(v6wbi_tlb_flags | TLB_L2CLEAN_FR)
+# define v6wbi_always_flags	(v6wbi_tlb_flags | TLB_L2CLEAN_FR)
+#else
 # define v6wbi_possible_flags	v6wbi_tlb_flags
 # define v6wbi_always_flags	v6wbi_tlb_flags
+#endif
 # ifdef _TLB
 #  define MULTI_TLB 1
 # else
@@ -197,8 +202,13 @@
 #endif
 
 #ifdef CONFIG_CPU_TLB_V7
+#if (defined(CONFIG_ARCH_ARMADA370) || defined(CONFIG_ARCH_ARMADA_XP)) && !defined(CONFIG_AURORA_L2_PT_WALK)
+# define v7wbi_possible_flags	(v7wbi_tlb_flags | TLB_L2CLEAN_FR)
+# define v7wbi_always_flags	(v7wbi_tlb_flags | TLB_L2CLEAN_FR)
+#else
 # define v7wbi_possible_flags	v7wbi_tlb_flags
 # define v7wbi_always_flags	v7wbi_tlb_flags
+#endif
 # ifdef _TLB
 #  define MULTI_TLB 1
 # else
@@ -216,7 +226,9 @@
 #ifndef __ASSEMBLY__
 
 #include <linux/sched.h>
-
+#ifdef CONFIG_CACHE_AURORA_L2
+extern void l2_clean_pa(unsigned int pa); 
+#endif
 struct cpu_tlb_fns {
 	void (*flush_user_range)(unsigned long, unsigned long, struct vm_area_struct *);
 	void (*flush_kern_range)(unsigned long, unsigned long);
@@ -528,13 +540,27 @@ static inline void flush_pmd_entry(pmd_t *pmd)
 	const unsigned int __tlb_flag = __cpu_tlb_flags;
 
 	if (tlb_flag(TLB_DCLEAN))
+#ifdef CONFIG_SHEEVA_ERRATA_ARM_CPU_4611
+	{
+		unsigned long flags;
+        	raw_local_irq_save(flags);
+		dmb();
+#endif
 		asm("mcr	p15, 0, %0, c7, c10, 1	@ flush_pmd"
 			: : "r" (pmd) : "cc");
+#ifdef CONFIG_SHEEVA_ERRATA_ARM_CPU_4611
+		raw_local_irq_restore(flags);
+	}
+#endif
 
+#ifdef CONFIG_CACHE_AURORA_L2
+	if (tlb_flag(TLB_L2CLEAN_FR)) 
+        	l2_clean_pa(__pa((unsigned long)pmd));
+#else
 	if (tlb_flag(TLB_L2CLEAN_FR))
 		asm("mcr	p15, 1, %0, c15, c9, 1  @ L2 flush_pmd"
 			: : "r" (pmd) : "cc");
-
+#endif
 	if (tlb_flag(TLB_WB))
 		dsb();
 }
@@ -544,12 +570,27 @@ static inline void clean_pmd_entry(pmd_t *pmd)
 	const unsigned int __tlb_flag = __cpu_tlb_flags;
 
 	if (tlb_flag(TLB_DCLEAN))
+#ifdef CONFIG_SHEEVA_ERRATA_ARM_CPU_4611
+	{
+		unsigned long flags;
+                raw_local_irq_save(flags);
+                dmb();
+#endif
 		asm("mcr	p15, 0, %0, c7, c10, 1	@ flush_pmd"
 			: : "r" (pmd) : "cc");
+#ifdef CONFIG_SHEEVA_ERRATA_ARM_CPU_4611
+		raw_local_irq_restore(flags);
+	}
+#endif
 
+#ifdef CONFIG_CACHE_AURORA_L2
+	if (tlb_flag(TLB_L2CLEAN_FR))
+		l2_clean_pa(__pa((unsigned long)pmd));
+#else
 	if (tlb_flag(TLB_L2CLEAN_FR))
 		asm("mcr	p15, 1, %0, c15, c9, 1  @ L2 flush_pmd"
 			: : "r" (pmd) : "cc");
+#endif
 }
 
 #undef tlb_flag
diff --git a/arch/arm/include/asm/vfp.h b/arch/arm/include/asm/vfp.h
index f4ab34f..bdb7539 100644
--- a/arch/arm/include/asm/vfp.h
+++ b/arch/arm/include/asm/vfp.h
@@ -82,3 +82,9 @@
 #define VFPOPDESC_UNUSED_BIT	(24)
 #define VFPOPDESC_UNUSED_MASK	(0xFF << VFPOPDESC_UNUSED_BIT)
 #define VFPOPDESC_OPDESC_MASK	(~(VFPOPDESC_LENGTH_MASK | VFPOPDESC_UNUSED_MASK))
+
+#ifndef __ASSEMBLY__
+extern void vfp_save(void);
+extern void vfp_restore(void);
+#endif /* __ASSEMBLY__ */
+
diff --git a/arch/arm/kernel/Makefile b/arch/arm/kernel/Makefile
index 390d16f..87682ee 100644
--- a/arch/arm/kernel/Makefile
+++ b/arch/arm/kernel/Makefile
@@ -12,6 +12,9 @@ endif
 CFLAGS_REMOVE_return_address.o = -pg
 
 # Object file lists.
+ifeq ($(CONFIG_PLAT_ARMADA),y)
+	include $(srctree)/$(MACHINE)/config/mvRules.mk
+endif
 
 obj-y		:= compat.o elf.o entry-armv.o entry-common.o irq.o \
 		   process.o ptrace.o return_address.o setup.o signal.o \
diff --git a/arch/arm/kernel/dma.c b/arch/arm/kernel/dma.c
index 7d5b9fb..2c4a185 100644
--- a/arch/arm/kernel/dma.c
+++ b/arch/arm/kernel/dma.c
@@ -16,6 +16,8 @@
 #include <linux/spinlock.h>
 #include <linux/errno.h>
 #include <linux/scatterlist.h>
+#include <linux/seq_file.h>
+#include <linux/proc_fs.h>
 
 #include <asm/dma.h>
 
@@ -264,3 +266,37 @@ int get_dma_residue(unsigned int chan)
 	return ret;
 }
 EXPORT_SYMBOL(get_dma_residue);
+
+#ifdef CONFIG_PROC_FS
+static int proc_dma_show(struct seq_file *m, void *v)
+{
+	int i;
+
+	for (i = 0 ; i < MAX_DMA_CHANNELS ; i++) {
+		dma_t *dma = dma_channel(i);
+		if (dma && dma->lock)
+			seq_printf(m, "%2d: %s\n", i, dma->device_id);
+	}
+	return 0;
+}
+
+static int proc_dma_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, proc_dma_show, NULL);
+}
+
+static const struct file_operations proc_dma_operations = {
+	.open		= proc_dma_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+static int __init proc_dma_init(void)
+{
+	proc_create("dma", 0, NULL, &proc_dma_operations);
+	return 0;
+}
+
+__initcall(proc_dma_init);
+#endif
diff --git a/arch/arm/kernel/entry-armv.S b/arch/arm/kernel/entry-armv.S
index a288111..795f102 100644
--- a/arch/arm/kernel/entry-armv.S
+++ b/arch/arm/kernel/entry-armv.S
@@ -42,6 +42,13 @@
 	adrne	lr, BSYM(1b)
 	bne	asm_do_IRQ
 
+#if (defined(CONFIG_ARCH_ARMADA_XP) || defined(CONFIG_ARCH_ARMADA370)) && defined(CONFIG_PERF_EVENTS) && defined(CONFIG_HW_PERF_EVENTS)
+	test_for_pmuirq r0, r6, r5, lr
+	movne	r0, sp
+	adrne	lr, BSYM(1b)
+	bne	do_mrvl_pj4b_pmu_event
+#endif
+
 #ifdef CONFIG_SMP
 	/*
 	 * XXX
@@ -212,7 +219,7 @@ __dabt_svc:
 	@
 	@ IRQs off again before pulling preserved data off the stack
 	@
-	disable_irq
+	disable_irq_notrace
 
 	@
 	@ restore SPSR and restart the instruction
@@ -226,6 +233,9 @@ ENDPROC(__dabt_svc)
 __irq_svc:
 	svc_entry
 
+#ifdef CONFIG_TRACE_IRQFLAGS
+	bl	trace_hardirqs_off
+#endif
 #ifdef CONFIG_PREEMPT
 	get_thread_info tsk
 	ldr	r8, [tsk, #TI_PREEMPT]		@ get preempt count
@@ -336,7 +346,7 @@ __und_svc:
 	@
 	@ IRQs off again before pulling preserved data off the stack
 	@
-1:	disable_irq
+1:	disable_irq_notrace
 
 	@
 	@ restore SPSR and restart the instruction
@@ -375,7 +385,7 @@ __pabt_svc:
 	@
 	@ IRQs off again before pulling preserved data off the stack
 	@
-	disable_irq
+	disable_irq_notrace
 
 	@
 	@ restore SPSR and restart the instruction
diff --git a/arch/arm/kernel/entry-common.S b/arch/arm/kernel/entry-common.S
index 7252517..9ed02e8 100644
--- a/arch/arm/kernel/entry-common.S
+++ b/arch/arm/kernel/entry-common.S
@@ -423,9 +423,9 @@ ENDPROC(sys_fstatfs64_wrapper)
  * offset, we return EINVAL.
  */
 sys_mmap2:
-#if PAGE_SHIFT > 12
-		tst	r5, #PGOFF_MASK
-		moveq	r5, r5, lsr #PAGE_SHIFT - 12
+#ifdef CONFIG_MV_SUPPORT_64KB_PAGE_SIZE
+		tst	r5, #0xF
+		moveq	r5, r5, lsr #4
 		streq	r5, [sp, #4]
 		beq	sys_mmap_pgoff
 		mov	r0, #-EINVAL
diff --git a/arch/arm/kernel/head-common.S b/arch/arm/kernel/head-common.S
index b9505aa..a108d5d 100644
--- a/arch/arm/kernel/head-common.S
+++ b/arch/arm/kernel/head-common.S
@@ -252,10 +252,16 @@ __vet_atags:
 	bne	1f
 
 	ldr	r5, [r2, #0]			@ is first tag ATAG_CORE?
+#ifdef CONFIG_BE8_ON_LE
+	rev	r5, r5
+#endif
 	cmp	r5, #ATAG_CORE_SIZE
 	cmpne	r5, #ATAG_CORE_SIZE_EMPTY
 	bne	1f
 	ldr	r5, [r2, #4]
+#ifdef CONFIG_BE8_ON_LE
+	rev	r5, r5
+#endif
 	ldr	r6, =ATAG_CORE
 	cmp	r5, r6
 	bne	1f
diff --git a/arch/arm/kernel/irq.c b/arch/arm/kernel/irq.c
index 3b3d2c8..53b2ed0 100644
--- a/arch/arm/kernel/irq.c
+++ b/arch/arm/kernel/irq.c
@@ -91,6 +91,9 @@ unlock:
 #ifdef CONFIG_SMP
 		show_ipi_list(p);
 		show_local_irqs(p);
+#if (defined(CONFIG_ARCH_ARMADA_XP) || defined(CONFIG_ARCH_ARMADA370)) && defined(CONFIG_PERF_EVENTS)
+		show_local_pmu_irqs(p);
+#endif
 #endif
 		seq_printf(p, "Err: %10lu\n", irq_err_count);
 	}
diff --git a/arch/arm/kernel/module.c b/arch/arm/kernel/module.c
index aab7fca..185cf70 100644
--- a/arch/arm/kernel/module.c
+++ b/arch/arm/kernel/module.c
@@ -88,6 +88,18 @@ int module_frob_arch_sections(Elf_Ehdr *hdr,
 	return 0;
 }
 
+#ifdef CONFIG_CPU_ENDIAN_BE8
+#define read_instr32(c)			__swab32(*(u32 *)c)
+#define read_instr16(c)			__swab16(*(u16 *)c)
+#define write_instr32(v,a)		(*(u32 *)(a) = __swab32((__force __u32)(v)))
+#define write_instr16(v,a)		(*(u16 *)(a) = __swab16((__force __u16)(v)))
+#else
+#define read_instr32(c)			(*(u32 *)c)
+#define read_instr16(c)			(*(u16 *)c)
+#define write_instr32(v,a)		(*(u32 *)(a) = (v))
+#define write_instr16(v,a)		(*(u16 *)(a) = (v))
+#endif
+
 int
 apply_relocate(Elf32_Shdr *sechdrs, const char *strtab, unsigned int symindex,
 	       unsigned int relindex, struct module *module)
@@ -102,7 +114,9 @@ apply_relocate(Elf32_Shdr *sechdrs, const char *strtab, unsigned int symindex,
 		unsigned long loc;
 		Elf32_Sym *sym;
 		s32 offset;
+#ifdef CONFIG_THUMB2_KERNEL
 		u32 upper, lower, sign, j1, j2;
+#endif
 
 		offset = ELF32_R_SYM(rel->r_info);
 		if (offset < 0 || offset > (symsec->sh_size / sizeof(Elf32_Sym))) {
@@ -185,6 +199,7 @@ apply_relocate(Elf32_Shdr *sechdrs, const char *strtab, unsigned int symindex,
 					(offset & 0x0fff);
 			break;
 
+#ifdef CONFIG_THUMB2_KERNEL
 		case R_ARM_THM_CALL:
 		case R_ARM_THM_JUMP24:
 			upper = *(u16 *)loc;
@@ -267,6 +282,7 @@ apply_relocate(Elf32_Shdr *sechdrs, const char *strtab, unsigned int symindex,
 						  (offset & 0x00ff));
 			break;
 
+#endif
 		default:
 			printk(KERN_ERR "%s: unknown relocation: %u\n",
 			       module->name, ELF32_R_TYPE(rel->r_info));
diff --git a/arch/arm/kernel/perf_event.c b/arch/arm/kernel/perf_event.c
index b2d6945..e23eb3b 100644
--- a/arch/arm/kernel/perf_event.c
+++ b/arch/arm/kernel/perf_event.c
@@ -18,6 +18,7 @@
 #include <linux/kernel.h>
 #include <linux/module.h>
 #include <linux/perf_event.h>
+#include <linux/platform_device.h>
 #include <linux/spinlock.h>
 #include <linux/uaccess.h>
 
@@ -27,6 +28,11 @@
 #include <asm/pmu.h>
 #include <asm/stacktrace.h>
 
+#if defined(CONFIG_ARCH_ARMADA_XP) || defined(CONFIG_ARCH_ARMADA370)
+int pmu_request_irq(int irq, irq_handler_t handler);
+void pmu_free_irq(int irq);
+#endif
+
 static const struct pmu_irqs *pmu_irqs;
 
 /*
@@ -76,6 +82,7 @@ static const char *arm_pmu_names[] = {
 	[ARM_PERF_PMU_ID_V6MP]	  = "v6mpcore",
 	[ARM_PERF_PMU_ID_CA8]	  = "ARMv7 Cortex-A8",
 	[ARM_PERF_PMU_ID_CA9]	  = "ARMv7 Cortex-A9",
+	[MRVL_PERF_PMU_ID_PJ4B]	  = "Marvell Sheeva-PJ4B",
 };
 
 struct arm_pmu {
@@ -372,9 +379,14 @@ armpmu_reserve_hardware(void)
 	}
 
 	for (i = 0; i < pmu_irqs->num_irqs; ++i) {
+
+#if defined(CONFIG_ARCH_ARMADA_XP) || defined(CONFIG_ARCH_ARMADA370)
+		err = pmu_request_irq(pmu_irqs->irqs[i], armpmu->handle_irq);
+#else
 		err = request_irq(pmu_irqs->irqs[i], armpmu->handle_irq,
 				  IRQF_DISABLED | IRQF_NOBALANCING,
 				  "armpmu", NULL);
+#endif
 		if (err) {
 			pr_warning("unable to request IRQ%d for ARM "
 				   "perf counters\n", pmu_irqs->irqs[i]);
@@ -383,8 +395,13 @@ armpmu_reserve_hardware(void)
 	}
 
 	if (err) {
-		for (i = i - 1; i >= 0; --i)
+		for (i = i - 1; i >= 0; --i){
+#if defined(CONFIG_ARCH_ARMADA_XP) || defined(CONFIG_ARCH_ARMADA370)
+				pmu_free_irq(pmu_irqs->irqs[i]);
+#else
 			free_irq(pmu_irqs->irqs[i], NULL);
+#endif
+                }
 		release_pmu(pmu_irqs);
 		pmu_irqs = NULL;
 	}
@@ -397,8 +414,13 @@ armpmu_release_hardware(void)
 {
 	int i;
 
-	for (i = pmu_irqs->num_irqs - 1; i >= 0; --i)
+	for (i = pmu_irqs->num_irqs - 1; i >= 0; --i){
+#if defined(CONFIG_ARCH_ARMADA_XP) || defined(CONFIG_ARCH_ARMADA370)
+			pmu_free_irq(pmu_irqs->irqs[i]);
+#else
 		free_irq(pmu_irqs->irqs[i], NULL);
+#endif
+        }
 	armpmu->stop();
 
 	release_pmu(pmu_irqs);
@@ -1698,7 +1720,7 @@ static inline int armv7_pmnc_has_overflowed(unsigned long pmnc)
 static inline int armv7_pmnc_counter_has_overflowed(unsigned long pmnc,
 					enum armv7_counters counter)
 {
-	int ret;
+	int ret=0;
 
 	if (counter == ARMV7_CYCLE_COUNTER)
 		ret = pmnc & ARMV7_FLAG_C;
@@ -2116,6 +2138,573 @@ static u32 __init armv7_reset_read_pmnc(void)
 	return nb_cnt + 1;
 }
 
+/*
+ * Marvell Sheeva PJ4B CPU support
+ */
+#define	MRVL_PJ4B_PMU_ENABLE	0x001	/* Enable counters */
+#define MRVL_PJ4B_PMN_RESET	0x002	/* Reset event counters */
+#define	MRVL_PJ4B_CCNT_RESET	0x004	/* Reset cycles counter */
+#define	MRVL_PJ4B_PMU_RESET	(MRVL_PJ4B_CCNT_RESET | MRVL_PJ4B_PMN_RESET)
+#define MRVL_PJ4B_PMU_CNT64	0x008	/* Make CCNT count every 64th cycle */
+
+/*
+* Different types of events that can be counted by the Marvell PJ4 Performance Monitor
+* 
+*/
+enum mrvl_pj4b_perf_types  {
+	MRVL_PJ4B_SOFTWARE_INCR = 0x00,	/* software increment */
+	MRVL_PJ4B_IFU_IFETCH_REFILL = 0x01,	/* instruction fetch that cause a refill at the lowest level of instruction or unified cache */
+	MRVL_PJ4B_IF_TLB_REFILL = 0x02,	/* instruction fetch that cause a TLB refill at the lowest level of TLB */
+	MRVL_PJ4B_DATA_RW_CACHE_REFILL = 0x03,	/* data read or write operation that causes a refill of at the lowest level of data or unified cache */
+	MRVL_PJ4B_DATA_RW_CACHE_ACCESS = 0x04,	/* data read or write operation that causes a cache access at the lowest level of data or unified cache */
+	MRVL_PJ4B_DATA_RW_TLB_REFILL = 0x05,	/* data read or write operation that causes a TLB refill at the lowest level of TLB */
+	MRVL_PJ4B_DATA_READ_INST_EXEC = 0x06,	/* data read architecturally executed */
+	MRVL_PJ4B_DATA_WRIT_INST_EXEC = 0x07,	/* data write architecturally executed */
+	MRVL_PJ4B_INSN_EXECUTED = 0x08,	/* instruction architecturally executed */
+	MRVL_PJ4B_EXCEPTION_TAKEN = 0x09,	/* exception taken */
+	MRVL_PJ4B_EXCEPTION_RETURN = 0x0a,	/* exception return architecturally executed */
+	MRVL_PJ4B_INSN_WR_CONTEXTIDR = 0x0b,	/* instruction that writes to the Context ID Register architecturally executed */
+	MRVL_PJ4B_SW_CHANGE_PC = 0x0c,	/* software change of PC, except by an exception, architecturally executed */
+	MRVL_PJ4B_BR_EXECUTED = 0x0d,	/* immediate branch architecturally executed, taken or not taken */
+	MRVL_PJ4B_PROCEDURE_RETURN = 0x0e,	/* procedure return architecturally executed */
+	MRVL_PJ4B_UNALIGNED_ACCESS = 0x0f,	/* unaligned access architecturally executed */
+	MRVL_PJ4B_BR_INST_MISS_PRED = 0x10,	/* branch mispredicted or not predicted */
+	MRVL_PJ4B_CYCLE_COUNT = 0x11,	/* cycle count */
+	MRVL_PJ4B_BR_PRED_TAKEN = 0x12,	/* branches or other change in the program flow that could have been predicted by the branch prediction resources of the processor */
+	MRVL_PJ4B_DCACHE_READ_HIT = 0x40,	/* counts the number of Data Cache read hits */
+	MRVL_PJ4B_DCACHE_READ_MISS = 0x41,	/* connts the number of Data Cache read misses */
+	MRVL_PJ4B_DCACHE_WRITE_HIT = 0x42,	/* counts the number of Data Cache write hits */
+	MRVL_PJ4B_DCACHE_WRITE_MISS = 0x43,	/* counts the number of Data Cache write misses */
+	MRVL_PJ4B_MMU_BUS_REQUEST = 0x44,	/* counts the number of cycles of request to the MMU Bus */
+	MRVL_PJ4B_ICACHE_BUS_REQUEST = 0x45,	/* counts the number of cycles the Instruction Cache requests the bus until the data return */
+	MRVL_PJ4B_WB_WRITE_LATENCY = 0x46,	/* counts the number of cycles the Write Buffer requests the bus */
+	MRVL_PJ4B_HOLD_LDM_STM = 0x47,	/* counts the number of cycles the pipeline is held because of a load/store multiple instruction */
+	MRVL_PJ4B_NO_DUAL_CFLAG = 0x48,	/* counts the number of cycles the processor cannot dual issue because of a Carry flag dependency */
+	MRVL_PJ4B_NO_DUAL_REGISTER_PLUS = 0x49,	/* counts the number of cycles the processor cannot dual issue because the register file does not have enough read ports and at least one other reason */
+	MRVL_PJ4B_LDST_ROB0_ON_HOLD = 0x4a,	/* counts the number of cycles a load or store instruction waits to retire from ROB0 */
+	MRVL_PJ4B_LDST_ROB1_ON_HOLD = 0x4b,	/* counts the number of cycles a load or store instruction waits to retire from ROB0=1 */
+	MRVL_PJ4B_DATA_WRITE_ACCESS_COUNT = 0x4c, 	/* counts the number of any Data write access */
+	MRVL_PJ4B_DATA_READ_ACCESS_COUNT = 0x4d, 	/* counts the number of any Data read access */
+	MRVL_PJ4B_A2_STALL = 0x4e, 	/* counts the number of cycles ALU A2 is stalled */
+	/*TODO: implement with fabric counters*/
+	MRVL_PJ4B_L2C_WRITE_HIT = 0x4f, 	/* counts the number of write accesses to addresses already in the L2C */
+	MRVL_PJ4B_L2C_WRITE_MISS = 0x50,	/* counts the number of write accesses to addresses not in the L2C */
+	MRVL_PJ4B_L2C_READ_COUNT = 0x51,	/* counts the number of L2C cache-to-bus external read request */
+	/*TODO: end*/
+	MRVL_PJ4B_ICACHE_READ_MISS = 0x60, 	/* counts the number of Instruction Cache read misses */
+	MRVL_PJ4B_ITLB_MISS = 0x61, 	/* counts the number of instruction TLB miss */
+	MRVL_PJ4B_SINGLE_ISSUE = 0x62, 	/* counts the number of cycles the processor single issues */
+	MRVL_PJ4B_BR_RETIRED = 0x63, 	/* counts the number of times one branch retires */
+	MRVL_PJ4B_ROB_FULL = 0x64, 	/* counts the number of cycles the Re-order Buffer (ROB) is full */
+	MRVL_PJ4B_MMU_READ_BEAT = 0x65, 	/* counts the number of times the bus returns RDY to the MMU */
+	MRVL_PJ4B_WB_WRITE_BEAT = 0x66, 	/* counts the number times the bus returns ready to the Write Buffer */
+	MRVL_PJ4B_DUAL_ISSUE = 0x67, 	/* counts the number of cycles the processor dual issues */
+	MRVL_PJ4B_NO_DUAL_RAW = 0x68, 	/* counts the number of cycles the processor cannot dual issue because of a Read after Write hazard */
+	MRVL_PJ4B_HOLD_IS = 0x69, 	/* counts the number of cycles the issue is held */
+	/*TODO: implement with fabric counters*/
+	MRVL_PJ4B_L2C_LATENCY = 0x6a, 	/* counts the latency for the most recent L2C read from the external bus Counts cycles */
+	/*TODO: end*/
+	MRVL_PJ4B_DCACHE_ACCESS = 0x70, 	/* counts the number of times the Data cache is accessed */
+	MRVL_PJ4B_DTLB_MISS = 0x71, 	/* counts the number of data TLB misses */
+	MRVL_PJ4B_BR_PRED_MISS = 0x72, 	/* counts the number of mispredicted branches */
+	MRVL_PJ4B_A1_STALL = 0x74, 	/* counts the number of cycles ALU A1 is stalled */
+	MRVL_PJ4B_DCACHE_READ_LATENCY = 0x75, 	/* counts the number of cycles the Data cache requests the bus for a read */
+	MRVL_PJ4B_DCACHE_WRITE_LATENCY = 0x76, 	/* counts the number of cycles the Data cache requests the bus for a write */
+	MRVL_PJ4B_NO_DUAL_REGISTER_FILE = 0x77, 	/* counts the number of cycles the processor cannot dual issue because the register file doesn't have enough read ports */
+	MRVL_PJ4B_BIU_SIMULTANEOUS_ACCESS = 0x78, 	/* BIU Simultaneous Access */
+	MRVL_PJ4B_L2C_READ_HIT = 0x79, 	/* counts the number of L2C cache-to-bus external read requests */
+	MRVL_PJ4B_L2C_READ_MISS = 0x7a, 	/* counts the number of L2C read accesses that resulted in an external read request */
+	MRVL_PJ4B_L2C_EVICTION = 0x7b, 	/* counts the number of evictions (CastOUT) of a line from the L2 cache */
+	MRVL_PJ4B_TLB_MISS = 0x80, 	/* counts the number of instruction and data TLB misses */
+	MRVL_PJ4B_BR_TAKEN = 0x81, 	/* counts the number of taken branches */
+	MRVL_PJ4B_WB_FULL = 0x82, 	/* counts the number of cycles WB is full */
+	MRVL_PJ4B_DCACHE_READ_BEAT = 0x83, 	/* counts the number of times the bus returns Data to the Data cache during read request */
+	MRVL_PJ4B_DCACHE_WRITE_BEAT = 0x84, 	/* counts the number of times the bus returns ready to the Data cache during write request */
+	MRVL_PJ4B_NO_DUAL_HW = 0x85, 	/* counts the number of cycles the processor cannot dual issue because of hardware conflict */
+	MRVL_PJ4B_NO_DUAL_MULTIPLE = 0x86, 	/* counts the number of cycles the processor cannot dual issue because of multiple reasons */
+	MRVL_PJ4B_BIU_ANY_ACCESS = 0x87, 	/* counts the number of cycles the BIU is accessed by any unit */
+	MRVL_PJ4B_MAIN_TLB_REFILL_BY_ICACHE = 0x88, 	/* counts the number of instruction fetch operations that causes a Main TLB walk */
+	MRVL_PJ4B_MAIN_TLB_REFILL_BY_DCACHE = 0x89, 	/* counts the number of Data read or write operations that causes a Main TLB walk */
+	MRVL_PJ4B_ICACHE_READ_BEAT = 0x8a, 	/* counts the number of times the bus returns RDY to the instruction cache */
+	MRVL_PJ4B_PMUEXT_IN0 = 0x90, 	/* counts any event from external input source PMUEXTIN[0] */
+	MRVL_PJ4B_PMUEXT_IN1 = 0x91, 	/* counts any event from external input source PMUEXTIN[1] */
+	MRVL_PJ4B_PMUEXT_IN0_IN1 = 0x92, 	/* counts any event from both external input sources PMUEXTIN[0] and PMUEXTIN[1] */
+	MRVL_PJ4B_WMMX2_STORE_FIFO_FULL = 0xc0, 	/* counts the number of cycles when the WMMX2 store FIFO is full */
+	MRVL_PJ4B_WMMX2_FINISH_FIFO_FULL = 0xc1, 	/* counts the number of cycles when the WMMX2 finish FIFO is full */
+	MRVL_PJ4B_WMMX2_INST_FIFO_FULL = 0xc2, 	/* counts the number of cycles when the WMMX2 instruction FIFO is full */
+	MRVL_PJ4B_WMMX2_INST_RETIRED = 0xc3, 	/* counts the number of retired WMMX2 instructions */
+	MRVL_PJ4B_WMMX2_BUSY = 0xc4, 	/* counts the number of cycles when the WMMX2 is busy */
+	MRVL_PJ4B_WMMX2_HOLD_MI = 0xc5, 	/* counts the number of cycles when WMMX2 holds the issue stage */
+	MRVL_PJ4B_WMMX2_HOLD_MW = 0xc6, 	/* counts the number of cycles when WMMX2 holds the write back stage */
+	/* EVT_CCNT is not hardware defined */
+	MRVL_PJ4B_EVT_CCNT = 0xFE,		/* CPU_CYCLE */
+	MRVL_PJ4B_EVT_UNUSED = 0xFF, 
+};
+
+enum  pj4b_pmu_counters {MRVL_PJ4B_CCNT=0, 
+						MRVL_PJ4B_PMN0, 
+						MRVL_PJ4B_PMN1, 
+						MRVL_PJ4B_PMN2, 
+						MRVL_PJ4B_PMN3, 
+						MRVL_PJ4B_PMN4, 
+						MRVL_PJ4B_PMN5, 
+						MRVL_PJ4B_MAX_COUNTERS};
+
+#define MRVL_PJ4B_CCNT_BIT_OFFSET 	31
+#define MRVL_PJ4B_PMN_BIT_OFFSET 	0
+
+#define MRVL_PJ4B_ALL_CNTRS			(0x8000003F)
+
+/*
+ * The hardware events that we support. We do support cache operations but
+ * we have harvard caches and no way to combine instruction and data
+ * accesses/misses in hardware.
+ */
+static const unsigned mrvl_pj4b_perf_map[PERF_COUNT_HW_MAX] = {
+	[PERF_COUNT_HW_CPU_CYCLES]	    = MRVL_PJ4B_EVT_CCNT,
+	[PERF_COUNT_HW_INSTRUCTIONS]	    = MRVL_PJ4B_INSN_EXECUTED,
+	[PERF_COUNT_HW_CACHE_REFERENCES]    = HW_OP_UNSUPPORTED,
+	[PERF_COUNT_HW_CACHE_MISSES]	    = HW_OP_UNSUPPORTED,
+	[PERF_COUNT_HW_BRANCH_INSTRUCTIONS] = MRVL_PJ4B_BR_RETIRED,
+	[PERF_COUNT_HW_BRANCH_MISSES]	    = MRVL_PJ4B_BR_PRED_MISS,
+	[PERF_COUNT_HW_BUS_CYCLES]	    = HW_OP_UNSUPPORTED,
+};
+
+static const unsigned mrvl_pj4b_perf_cache_map[PERF_COUNT_HW_CACHE_MAX]
+					[PERF_COUNT_HW_CACHE_OP_MAX]
+					[PERF_COUNT_HW_CACHE_RESULT_MAX] = {
+	[C(L1D)] = {
+		[C(OP_READ)] = {
+			[C(RESULT_ACCESS)]  = MRVL_PJ4B_DCACHE_ACCESS,
+			[C(RESULT_MISS)]    = MRVL_PJ4B_DCACHE_READ_MISS,
+		},
+		[C(OP_WRITE)] = {
+			[C(RESULT_ACCESS)]  = MRVL_PJ4B_DCACHE_ACCESS,
+			[C(RESULT_MISS)]    = MRVL_PJ4B_DCACHE_WRITE_MISS,
+		},
+		[C(OP_PREFETCH)] = {
+			[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,
+			[C(RESULT_MISS)]    = CACHE_OP_UNSUPPORTED,
+		},
+	},
+	[C(L1I)] = {
+		[C(OP_READ)] = {
+			[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,
+			[C(RESULT_MISS)]    = MRVL_PJ4B_ICACHE_READ_MISS,
+		},
+		[C(OP_WRITE)] = {
+			[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,
+			[C(RESULT_MISS)]    = CACHE_OP_UNSUPPORTED,
+		},
+		[C(OP_PREFETCH)] = {
+			[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,
+			[C(RESULT_MISS)]    = CACHE_OP_UNSUPPORTED,
+		},
+	},
+	/*TODO add L2 counters*/
+	[C(LL)] = {
+		[C(OP_READ)] = {
+			[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,
+			[C(RESULT_MISS)]    = CACHE_OP_UNSUPPORTED,
+		},
+		[C(OP_WRITE)] = {
+			[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,
+			[C(RESULT_MISS)]    = CACHE_OP_UNSUPPORTED,
+		},
+		[C(OP_PREFETCH)] = {
+			[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,
+			[C(RESULT_MISS)]    = CACHE_OP_UNSUPPORTED,
+		},
+	},
+	[C(DTLB)] = {
+		/*
+		 * The ARM performance counters can count micro DTLB misses,
+		 * micro ITLB misses and main TLB misses. There isn't an event
+		 * for TLB misses, so use the micro misses here and if users
+		 * want the main TLB misses they can use a raw counter.
+		 */
+		[C(OP_READ)] = {
+			[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,
+			[C(RESULT_MISS)]    = MRVL_PJ4B_DTLB_MISS,
+		},
+		[C(OP_WRITE)] = {
+			[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,
+			[C(RESULT_MISS)]    = MRVL_PJ4B_DTLB_MISS,
+		},
+		[C(OP_PREFETCH)] = {
+			[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,
+			[C(RESULT_MISS)]    = CACHE_OP_UNSUPPORTED,
+		},
+	},
+	[C(ITLB)] = {
+		[C(OP_READ)] = {
+			[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,
+			[C(RESULT_MISS)]    = MRVL_PJ4B_ITLB_MISS,
+		},
+		[C(OP_WRITE)] = {
+			[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,
+			[C(RESULT_MISS)]    = MRVL_PJ4B_ITLB_MISS,
+		},
+		[C(OP_PREFETCH)] = {
+			[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,
+			[C(RESULT_MISS)]    = CACHE_OP_UNSUPPORTED,
+		},
+	},
+	[C(BPU)] = {
+		[C(OP_READ)] = {
+			[C(RESULT_ACCESS)]  = MRVL_PJ4B_BR_RETIRED,
+			[C(RESULT_MISS)]    = MRVL_PJ4B_BR_PRED_MISS,
+		},
+		[C(OP_WRITE)] = {
+			[C(RESULT_ACCESS)]  = MRVL_PJ4B_BR_RETIRED,
+			[C(RESULT_MISS)]    = MRVL_PJ4B_BR_PRED_MISS,
+		},
+		[C(OP_PREFETCH)] = {
+			[C(RESULT_ACCESS)]  = CACHE_OP_UNSUPPORTED,
+			[C(RESULT_MISS)]    = CACHE_OP_UNSUPPORTED,
+		},
+	},
+};
+
+/*Helper functions*/
+static inline void mrvl_pj4b_pmu_cntr_disable(u32 val)
+{
+	asm volatile("mcr p15, 0, %0, c9, c14, 2" : : "r"(val));
+	asm volatile("mcr p15, 0, %0, c9, c12, 2" : : "r"(val));	
+}
+
+static inline void mrvl_pj4b_pmu_cntr_enable(u32 val)
+{
+	asm volatile("mcr p15, 0, %0, c9, c12, 1" : : "r"(val));	
+	asm volatile("mcr p15, 0, %0, c9, c14, 1" : : "r"(val));
+}
+
+static inline void mrvl_pj4b_pmu_select_event(u32 cntr, u32 evt)
+{
+	asm volatile("mcr p15, 0, %0, c9, c12, 5" : : "r"(cntr));
+	asm volatile("mcr p15, 0, %0, c9, c13, 1" : : "r"(evt));
+}
+
+static inline void mrvl_pj4b_pmu_clear_events(u32 val)
+{
+	asm volatile("mcr p15, 0, %0, c9, c12, 2" : : "r"(val));	
+}
+
+static inline void mrvl_pj4b_pmu_enable_events(u32 val)
+{
+	asm volatile("mcr p15, 0, %0, c9, c12, 1": : "r"(val));
+}
+
+static inline u32 mrvl_pj4b_pmu_read_events(void)
+{
+	u32 val;
+	asm volatile("mcr p15, 0, %0, c9, c12, 1": "=r"(val));
+	return val;
+}
+
+
+static inline void mrvl_pj4b_write_pmnc(u32 val)
+{
+	asm volatile("mcr p15, 0, %0, c9, c12, 0": : "r"(val));
+} 
+
+static inline u32 mrvl_pj4b_read_pmnc(void)
+{
+	u32 val;
+
+	asm volatile("mrc p15, 0, %0, c9, c12, 0" : "=r"(val));
+
+	return val;
+}
+
+static inline void mrvl_pj4b_pmu_clear_overflow(u32 val)
+{
+	/* writeback clears overflow bits */
+	asm volatile("mcr p15, 0, %0, c9, c12, 3": : "r"(val));
+}
+
+static inline int mrvl_pj4b_pmu_counter_has_overflowed(unsigned long val,
+				  enum pj4b_pmu_counters counter)
+{
+	int ret = 0;
+	
+	if (counter == MRVL_PJ4B_CCNT)
+		ret = (val & (1 << MRVL_PJ4B_CCNT_BIT_OFFSET));
+	else if (counter < MRVL_PJ4B_MAX_COUNTERS)
+		ret = (val & (1 << (counter-MRVL_PJ4B_PMN0)));
+	else
+		WARN_ONCE(1, "invalid counter number (%d)\n", counter);
+
+	return ret;
+
+}
+
+static inline u32 mrvl_pj4b_pmu_read_overflow(void)
+{
+	u32 val;
+	/* check counter */
+	asm volatile("mrc p15, 0, %0, c9, c12, 3" : "=r"(val));
+	return val;
+}
+
+
+/*API functions*/
+static u32 mrvl_pj4b_pmu_read_counter(int counter)
+{
+	u32 val = 0;
+	
+	switch (counter) {
+	case MRVL_PJ4B_CCNT:
+		asm volatile("mrc p15, 0, %0, c9, c13, 0" : "=r" (val));		
+		break;
+	case MRVL_PJ4B_PMN0:
+	case MRVL_PJ4B_PMN1:
+	case MRVL_PJ4B_PMN2:
+	case MRVL_PJ4B_PMN3:
+	case MRVL_PJ4B_PMN4:
+	case MRVL_PJ4B_PMN5:
+		asm volatile("mcr p15, 0, %0, c9, c12, 5": : "r"(counter - MRVL_PJ4B_PMN0));
+		asm volatile("mrc p15, 0, %0, c9, c13, 2" : "=r"(val));
+		break;
+ 	}	
+	return val;
+} 
+
+static void mrvl_pj4b_pmu_write_counter(int counter, u32 val)
+{
+	switch (counter) {
+	case MRVL_PJ4B_CCNT:
+		asm volatile("mcr p15, 0, %0, c9, c13, 0" : : "r" (val));
+		break;
+	case MRVL_PJ4B_PMN0:
+	case MRVL_PJ4B_PMN1:
+	case MRVL_PJ4B_PMN2:
+	case MRVL_PJ4B_PMN3:
+	case MRVL_PJ4B_PMN4:
+	case MRVL_PJ4B_PMN5:
+		asm volatile("mcr p15, 0, %0, c9, c12, 5": : "r"(counter - MRVL_PJ4B_PMN0));
+		asm volatile("mcr p15, 0, %0, c9, c13, 2": : "r"(val));
+		break;
+	}
+}
+
+
+static u64 mrvl_pj4b_pmu_raw_event(u64 config)
+{
+	return config & 0xff;
+}
+
+static inline int mrvl_pj4b_pmu_event_map(int config)
+{
+	int mapping = mrvl_pj4b_perf_map[config];
+	if (HW_OP_UNSUPPORTED == mapping)
+		mapping = -EOPNOTSUPP;
+	return mapping;
+}
+
+static int mrvl_pj4b_pmu_get_event_idx(struct cpu_hw_events *cpuc,
+				  struct hw_perf_event *event)
+{
+	int idx;
+	/* Always place a cycle counter into the cycle counter. */
+	if (event->config_base == MRVL_PJ4B_EVT_CCNT) {		
+		if (test_and_set_bit(MRVL_PJ4B_CCNT, cpuc->used_mask)) {		
+			return -EAGAIN;
+		}
+		return MRVL_PJ4B_CCNT;
+	} else {
+		/*
+		 * For anything other than a cycle counter, try and use
+		 * the events counters
+		 */
+		for (idx = MRVL_PJ4B_PMN0; idx < armpmu->num_events; ++idx) {
+			if (!test_and_set_bit(idx, cpuc->used_mask)) {			
+				return idx;
+			}
+		}
+		/* The counters are all in use. */
+		return -EAGAIN;
+	}	
+}
+
+void mrvl_pj4b_pmu_enable_event(struct hw_perf_event *hwc, int idx)
+{
+	u32 enable;
+	unsigned long flags;
+	spin_lock_irqsave(&pmu_lock, flags);
+	if (idx == MRVL_PJ4B_CCNT) {	
+		enable = (1 << MRVL_PJ4B_CCNT_BIT_OFFSET);
+	} 
+	else if (idx < MRVL_PJ4B_MAX_COUNTERS) {
+		enable   = (1 << (idx - MRVL_PJ4B_PMN0));
+	} 
+	else {
+		WARN_ONCE(1, "invalid counter number (%d)\n", idx);
+		return;
+	}
+	mrvl_pj4b_pmu_cntr_disable(enable);
+	/*select event*/
+	if (idx != MRVL_PJ4B_CCNT) {		
+		/*select event*/
+		u32 evt = (hwc->config_base & 0xFF);		
+		mrvl_pj4b_pmu_select_event((idx-MRVL_PJ4B_PMN0), evt);
+	}	
+	mrvl_pj4b_pmu_cntr_enable(enable);
+	spin_unlock_irqrestore(&pmu_lock, flags);	
+}
+
+
+void mrvl_pj4b_pmu_disable_event(struct hw_perf_event *hwc, int idx)
+{
+	u32 enable;
+	unsigned long flags;
+	spin_lock_irqsave(&pmu_lock, flags);
+	if (idx == MRVL_PJ4B_CCNT) {	
+		enable = (1 << MRVL_PJ4B_CCNT_BIT_OFFSET);
+	} 
+	else if (idx < MRVL_PJ4B_MAX_COUNTERS) {
+		enable   = (1 << (idx - MRVL_PJ4B_PMN0));
+	} 
+	else {
+		WARN_ONCE(1, "invalid counter number (%d)\n", idx);
+		return;
+	}	
+	mrvl_pj4b_pmu_cntr_disable(enable);
+	spin_unlock_irqrestore(&pmu_lock, flags);
+}
+
+
+static irqreturn_t mrvl_pj4b_pmu_handle_irq(int irq, void *arg)
+{
+	int i = 0;
+	u32 flag;
+	struct pt_regs *regs;
+	struct perf_sample_data data;	
+	struct cpu_hw_events *cpuc;	
+	u32 pmnc;
+	pmnc = mrvl_pj4b_read_pmnc();	
+	pmnc &= ~MRVL_PJ4B_PMU_ENABLE;
+	mrvl_pj4b_write_pmnc(pmnc);	
+
+	flag = mrvl_pj4b_pmu_read_overflow();	
+	mrvl_pj4b_pmu_clear_overflow(flag);
+
+	/*
+	 * Did an overflow occur?
+	 */
+	if (!flag) {		
+		pmnc |= MRVL_PJ4B_PMU_ENABLE;
+		mrvl_pj4b_write_pmnc(pmnc);
+		return IRQ_NONE;
+	}	
+	/*
+	 * Handle the counter(s) overflow(s)
+	 */
+	regs = get_irq_regs();
+	perf_sample_data_init(&data, 0);
+
+	cpuc = &__get_cpu_var(cpu_hw_events);
+
+	for (i = MRVL_PJ4B_CCNT; i < armpmu->num_events; i++) {
+
+		struct perf_event *event = cpuc->events[i];
+		struct hw_perf_event *hwc;
+
+		if (!test_bit(i, cpuc->active_mask)) {
+			continue;
+		}
+		if (!mrvl_pj4b_pmu_counter_has_overflowed(flag, i)) {		
+			continue;
+		}
+
+		hwc = &event->hw;
+		armpmu_event_update(event, hwc, i);
+		data.period = event->hw.last_period;
+
+		if (!armpmu_event_set_period(event, hwc, i)) {		
+			continue;
+		}
+
+		if (perf_event_overflow(event, 0, &data, regs))
+			armpmu->disable(hwc, i);
+ 	}	
+	pmnc |= MRVL_PJ4B_PMU_ENABLE;
+	mrvl_pj4b_write_pmnc(pmnc);	
+	
+	/*
+	 * Handle the pending perf events.
+	 *
+	 * Note: this call *must* be run with interrupts enabled. For
+	 * platforms that can have the PMU interrupts raised as a PMI, this
+	 * will not work.
+	 */
+	perf_event_do_pending();
+	
+	return IRQ_HANDLED;
+} 
+
+
+
+asmlinkage void __exception do_mrvl_pj4b_pmu_event(struct pt_regs *regs)
+{	
+	struct pt_regs *old_regs = set_irq_regs(regs);
+	int cpu = smp_processor_id();
+	irq_enter();
+	irq_stat[cpu].local_pmu_irqs++;
+	armpmu->handle_irq(IRQ_AURORA_MP, NULL);    
+	irq_exit();	 	
+	set_irq_regs(old_regs);
+}
+
+
+static void mrvl_pj4b_pmu_stop(void)
+{
+	u32 pmnc;
+	unsigned long flags;
+	spin_lock_irqsave(&pmu_lock, flags);
+	pmnc = mrvl_pj4b_read_pmnc();
+	pmnc &= ~MRVL_PJ4B_PMU_ENABLE;
+	mrvl_pj4b_write_pmnc(pmnc);	
+	spin_unlock_irqrestore(&pmu_lock, flags);
+} 
+
+
+static void mrvl_pj4b_pmu_start(void)
+{
+	u32 pmnc;
+	unsigned long flags;
+
+	spin_lock_irqsave(&pmu_lock, flags);		
+	pmnc = mrvl_pj4b_read_pmnc();
+	pmnc |= (MRVL_PJ4B_PMU_ENABLE);
+	mrvl_pj4b_write_pmnc(pmnc);
+	spin_unlock_irqrestore(&pmu_lock, flags);
+} 
+
+static u32 __init mrvl_pj4b_read_reset_pmnc(void)
+{
+	u32 pmnc = mrvl_pj4b_read_pmnc();
+	pmnc |= (MRVL_PJ4B_PMU_RESET);
+	mrvl_pj4b_write_pmnc(pmnc);
+	return ((pmnc >> 11) & 0x1F)+1;
+}
+
+
+static const struct arm_pmu mrvl_pj4b_pmu = {
+	.id			= MRVL_PERF_PMU_ID_PJ4B,
+	.handle_irq		= mrvl_pj4b_pmu_handle_irq, /*v*/
+	.enable			= mrvl_pj4b_pmu_enable_event, /*v*/
+	.disable		= mrvl_pj4b_pmu_disable_event, /*v*/
+	.event_map		= mrvl_pj4b_pmu_event_map, /*v*/
+	.raw_event		= mrvl_pj4b_pmu_raw_event, /*v*/
+	.read_counter	= mrvl_pj4b_pmu_read_counter, /*v*/
+	.write_counter	= mrvl_pj4b_pmu_write_counter, /*v*/
+	.get_event_idx	= mrvl_pj4b_pmu_get_event_idx,/*v*/
+	.start			= mrvl_pj4b_pmu_start, /*v*/
+	.stop			= mrvl_pj4b_pmu_stop, /*v*/
+	.num_events		= MRVL_PJ4B_MAX_COUNTERS,
+	.max_period		= (1LLU << 32) - 1,
+};
 static int __init
 init_hw_perf_events(void)
 {
@@ -2135,11 +2724,20 @@ init_hw_perf_events(void)
 			perf_max_events	= armv6pmu.num_events;
 			break;
 		case 0xB020:	/* ARM11mpcore */
+#if defined(CONFIG_ARCH_ARMADA_XP) || defined(CONFIG_ARCH_ARMADA370)
+			armpmu = &mrvl_pj4b_pmu;
+			memcpy(armpmu_perf_cache_map, mrvl_pj4b_perf_cache_map,
+					sizeof(mrvl_pj4b_perf_cache_map));			
+			mrvl_pj4b_read_reset_pmnc();
+			perf_max_events = mrvl_pj4b_pmu.num_events;
+			printk(KERN_INFO "Armada-XP Performance Monitor Unit detected (ARM MPcore ID)!!!\n");
+#else		
 			armpmu = &armv6mpcore_pmu;
 			memcpy(armpmu_perf_cache_map,
 			       armv6mpcore_perf_cache_map,
 			       sizeof(armv6mpcore_perf_cache_map));
 			perf_max_events = armv6mpcore_pmu.num_events;
+#endif
 			break;
 		case 0xC080:	/* Cortex-A8 */
 			armv7pmu.id = ARM_PERF_PMU_ID_CA8;
@@ -2169,11 +2767,30 @@ init_hw_perf_events(void)
 			pr_info("no hardware support available\n");
 			perf_max_events = -1;
 		}
+#if defined(CONFIG_ARCH_ARMADA_XP) || defined(CONFIG_ARCH_ARMADA370)
+	/* Marvell ArmadaXP OR Armada370 CPUs */
+	} else if (0x56 == implementor) {
+		part_number = (cpuid >> 4) & 0xFFF;
+		switch (part_number) {
+		case 0x581:
+			printk(KERN_INFO "Armada-XP Performance Monitor Unit detected (Marvell ID)!!!\n");
+			armpmu = &mrvl_pj4b_pmu;
+			memcpy(armpmu_perf_cache_map, mrvl_pj4b_perf_cache_map,
+					sizeof(mrvl_pj4b_perf_cache_map));
+			perf_max_events	= mrvl_pj4b_pmu.num_events;
+			break;
+		}
 	}
-
-	if (armpmu)
+#else
+	}
+#endif
+	if (armpmu) {
 		pr_info("enabled with %s PMU driver, %d counters available\n",
 			arm_pmu_names[armpmu->id], armpmu->num_events);
+	} else {
+		pr_info("no hardware support available\n");
+		perf_max_events = -1;
+	}
 
 	return 0;
 }
diff --git a/arch/arm/kernel/pmu.c b/arch/arm/kernel/pmu.c
index 66c86c8..c442803 100644
--- a/arch/arm/kernel/pmu.c
+++ b/arch/arm/kernel/pmu.c
@@ -83,6 +83,9 @@ static int
 set_irq_affinity(int irq,
 		 unsigned int cpu)
 {
+#if defined (CONFIG_CPU_SHEEVA_PJ4B_V6) || defined (CONFIG_CPU_SHEEVA_PJ4B_V7)
+	return 0;
+#else
 #ifdef CONFIG_SMP
 	int err = irq_set_affinity(irq, cpumask_of(cpu));
 	if (err)
@@ -92,6 +95,7 @@ set_irq_affinity(int irq,
 #else
 	return 0;
 #endif
+#endif
 }
 
 int
diff --git a/arch/arm/kernel/process.c b/arch/arm/kernel/process.c
index f8ccbd9..a4a9cc8 100644
--- a/arch/arm/kernel/process.c
+++ b/arch/arm/kernel/process.c
@@ -28,7 +28,6 @@
 #include <linux/tick.h>
 #include <linux/utsname.h>
 #include <linux/uaccess.h>
-#include <trace/sched.h>
 
 #include <asm/leds.h>
 #include <asm/processor.h>
@@ -37,8 +36,6 @@
 #include <asm/stacktrace.h>
 #include <asm/mach/time.h>
 
-DEFINE_TRACE(sched_kthread_create);
-
 static const char *processor_modes[] = {
   "USER_26", "FIQ_26" , "IRQ_26" , "SVC_26" , "UK4_26" , "UK5_26" , "UK6_26" , "UK7_26" ,
   "UK8_26" , "UK9_26" , "UK10_26", "UK11_26", "UK12_26", "UK13_26", "UK14_26", "UK15_26",
@@ -325,8 +322,6 @@ copy_thread(unsigned long clone_flags, unsigned long stack_start,
 	if (clone_flags & CLONE_SETTLS)
 		thread->tp_value = regs->ARM_r3;
 
-	thread_notify(THREAD_NOTIFY_COPY, thread);
-
 	return 0;
 }
 
@@ -356,17 +351,21 @@ EXPORT_SYMBOL(dump_fpu);
 
 /*
  * Shuffle the argument into the correct register before calling the
- * thread function.  r1 is the thread argument, r2 is the pointer to
- * the thread function, and r3 points to the exit function.
+ * thread function.  r4 is the thread argument, r5 is the pointer to
+ * the thread function, and r6 points to the exit function.
  */
 extern void kernel_thread_helper(void);
 asm(	".pushsection .text\n"
 "	.align\n"
 "	.type	kernel_thread_helper, #function\n"
 "kernel_thread_helper:\n"
-"	mov	r0, r1\n"
-"	mov	lr, r3\n"
-"	mov	pc, r2\n"
+#ifdef CONFIG_TRACE_IRQFLAGS
+"	bl	trace_hardirqs_on\n"
+#endif
+"	msr	cpsr_c, r7\n"
+"	mov	r0, r4\n"
+"	mov	lr, r6\n"
+"	mov	pc, r5\n"
 "	.size	kernel_thread_helper, . - kernel_thread_helper\n"
 "	.popsection");
 
@@ -393,20 +392,17 @@ asm(	".pushsection .text\n"
 pid_t kernel_thread(int (*fn)(void *), void *arg, unsigned long flags)
 {
 	struct pt_regs regs;
-	long pid;
 
 	memset(&regs, 0, sizeof(regs));
 
-	regs.ARM_r1 = (unsigned long)arg;
-	regs.ARM_r2 = (unsigned long)fn;
-	regs.ARM_r3 = (unsigned long)kernel_thread_exit;
+	regs.ARM_r4 = (unsigned long)arg;
+	regs.ARM_r5 = (unsigned long)fn;
+	regs.ARM_r6 = (unsigned long)kernel_thread_exit;
+	regs.ARM_r7 = SVC_MODE | PSR_ENDSTATE | PSR_ISETSTATE;
 	regs.ARM_pc = (unsigned long)kernel_thread_helper;
-	regs.ARM_cpsr = SVC_MODE | PSR_ENDSTATE | PSR_ISETSTATE;
-
-	pid = do_fork(flags|CLONE_VM|CLONE_UNTRACED, 0, &regs, 0, NULL, NULL);
+	regs.ARM_cpsr = regs.ARM_r7 | PSR_I_BIT;
 
-	trace_sched_kthread_create(fn, pid);
-	return pid;
+	return do_fork(flags|CLONE_VM|CLONE_UNTRACED, 0, &regs, 0, NULL, NULL);
 }
 EXPORT_SYMBOL(kernel_thread);
 
diff --git a/arch/arm/kernel/ptrace.c b/arch/arm/kernel/ptrace.c
index 68b2b37..f46afbd 100644
--- a/arch/arm/kernel/ptrace.c
+++ b/arch/arm/kernel/ptrace.c
@@ -113,6 +113,103 @@ static inline int ptrace_access_process_vm(struct task_struct *tsk,
 #define BREAKINST_THUMB	0xde01
 #endif
 
+#if defined(CONFIG_ARCH_ARMADA_XP) || defined(CONFIG_ARCH_ARMADA370)
+struct pt_regs_offset {
+	const char *name;
+	int offset;
+};
+
+#define REG_OFFSET_NAME(r) \
+	{.name = #r, .offset = offsetof(struct pt_regs, ARM_##r)}
+#define REG_OFFSET_END {.name = NULL, .offset = 0}
+
+static const struct pt_regs_offset regoffset_table[] = {
+	REG_OFFSET_NAME(r0),
+	REG_OFFSET_NAME(r1),
+	REG_OFFSET_NAME(r2),
+	REG_OFFSET_NAME(r3),
+	REG_OFFSET_NAME(r4),
+	REG_OFFSET_NAME(r5),
+	REG_OFFSET_NAME(r6),
+	REG_OFFSET_NAME(r7),
+	REG_OFFSET_NAME(r8),
+	REG_OFFSET_NAME(r9),
+	REG_OFFSET_NAME(r10),
+	REG_OFFSET_NAME(fp),
+	REG_OFFSET_NAME(ip),
+	REG_OFFSET_NAME(sp),
+	REG_OFFSET_NAME(lr),
+	REG_OFFSET_NAME(pc),
+	REG_OFFSET_NAME(cpsr),
+	REG_OFFSET_NAME(ORIG_r0),
+	REG_OFFSET_END,
+};
+
+/**
+ * regs_query_register_offset() - query register offset from its name
+ * @name:	the name of a register
+ *
+ * regs_query_register_offset() returns the offset of a register in struct
+ * pt_regs from its name. If the name is invalid, this returns -EINVAL;
+ */
+int regs_query_register_offset(const char *name)
+{
+	const struct pt_regs_offset *roff;
+	for (roff = regoffset_table; roff->name != NULL; roff++)
+		if (!strcmp(roff->name, name))
+			return roff->offset;
+	return -EINVAL;
+}
+
+/**
+ * regs_query_register_name() - query register name from its offset
+ * @offset:	the offset of a register in struct pt_regs.
+ *
+ * regs_query_register_name() returns the name of a register from its
+ * offset in struct pt_regs. If the @offset is invalid, this returns NULL;
+ */
+const char *regs_query_register_name(unsigned int offset)
+{
+	const struct pt_regs_offset *roff;
+	for (roff = regoffset_table; roff->name != NULL; roff++)
+		if (roff->offset == offset)
+			return roff->name;
+	return NULL;
+}
+
+/**
+ * regs_within_kernel_stack() - check the address in the stack
+ * @regs:      pt_regs which contains kernel stack pointer.
+ * @addr:      address which is checked.
+ *
+ * regs_within_kernel_stack() checks @addr is within the kernel stack page(s).
+ * If @addr is within the kernel stack, it returns true. If not, returns false.
+ */
+bool regs_within_kernel_stack(struct pt_regs *regs, unsigned long addr)
+{
+	return ((addr & ~(THREAD_SIZE - 1))  ==
+		(kernel_stack_pointer(regs) & ~(THREAD_SIZE - 1)));
+}
+
+/**
+ * regs_get_kernel_stack_nth() - get Nth entry of the stack
+ * @regs:	pt_regs which contains kernel stack pointer.
+ * @n:		stack entry number.
+ *
+ * regs_get_kernel_stack_nth() returns @n th entry of the kernel stack which
+ * is specified by @regs. If the @n th entry is NOT in the kernel stack,
+ * this returns 0.
+ */
+unsigned long regs_get_kernel_stack_nth(struct pt_regs *regs, unsigned int n)
+{
+	unsigned long *addr = (unsigned long *)kernel_stack_pointer(regs);
+	addr += n;
+	if (regs_within_kernel_stack(regs, (unsigned long)addr))
+		return *addr;
+	else
+		return 0;
+}
+#endif /* CONFIG_ARCH_ARMADA_XP || CONFIG_ARCH_ARMADA370 */
 
 DEFINE_TRACE(syscall_entry);
 DEFINE_TRACE(syscall_exit);
diff --git a/arch/arm/kernel/smp.c b/arch/arm/kernel/smp.c
index f28d3b2..d1210cf 100644
--- a/arch/arm/kernel/smp.c
+++ b/arch/arm/kernel/smp.c
@@ -325,7 +325,7 @@ asmlinkage void __cpuinit secondary_start_kernel(void)
 	 * OK, now it's safe to let the boot CPU continue
 	 */
 	set_cpu_online(cpu, true);
-
+		
 	/*
 	 * OK, it's off to the idle thread for us
 	 */
@@ -422,6 +422,20 @@ void show_local_irqs(struct seq_file *p)
 	seq_putc(p, '\n');
 }
 
+#if defined(CONFIG_ARCH_ARMADA_XP) && defined(CONFIG_PERF_EVENTS)
+void show_local_pmu_irqs(struct seq_file *p)
+{
+	 unsigned int cpu;
+
+	 seq_printf(p, "PMU: ");
+	 
+	 for_each_present_cpu(cpu)
+		seq_printf(p, "%10u ", irq_stat[cpu].local_pmu_irqs);
+	 
+	 seq_putc(p, '\n');
+}
+#endif
+
 /*
  * Timer (local or broadcast) support
  */
diff --git a/arch/arm/kernel/vmlinux.lds.S b/arch/arm/kernel/vmlinux.lds.S
index faa4c67..8c80188 100644
--- a/arch/arm/kernel/vmlinux.lds.S
+++ b/arch/arm/kernel/vmlinux.lds.S
@@ -91,6 +91,9 @@ SECTIONS
 			__exception_text_start = .;
 			*(.exception.text)
 			__exception_text_end = .;
+/* Then all the functions that are "hot" in profiles, to group them onto the same hugetlb entry */
+#include "functionlist"
+ /* Then the rest */
 			TEXT_TEXT
 			SCHED_TEXT
 			LOCK_TEXT
diff --git a/arch/arm/lib/copy_page.S b/arch/arm/lib/copy_page.S
index 6ee2f67..8ab251b 100644
--- a/arch/arm/lib/copy_page.S
+++ b/arch/arm/lib/copy_page.S
@@ -28,7 +28,11 @@ ENTRY(copy_page)
 		stmfd	sp!, {r4, lr}			@	2
 	PLD(	pld	[r1, #0]		)
 	PLD(	pld	[r1, #L1_CACHE_BYTES]		)
+#ifdef CONFIG_MV_SUPPORT_64KB_PAGE_SIZE
+		ldr	r2, =COPY_COUNT
+#else
 		mov	r2, #COPY_COUNT			@	1
+#endif
 		ldmia	r1!, {r3, r4, ip, lr}		@	4+1
 1:	PLD(	pld	[r1, #2 * L1_CACHE_BYTES])
 	PLD(	pld	[r1, #3 * L1_CACHE_BYTES])
diff --git a/arch/arm/lib/copy_template.S b/arch/arm/lib/copy_template.S
index 805e3f8..b576ac3 100644
--- a/arch/arm/lib/copy_template.S
+++ b/arch/arm/lib/copy_template.S
@@ -65,6 +65,23 @@
  *	the ldr1w or str1w instructions (some of these macros may expand to
  *	than one 32bit instruction in Thumb-2)
  */
+        .macro  preload, reg, lines
+	        mov     r3, \reg
+	        mov     r4, #\lines
+222:
+	        pld     [r3, #28]
+	        subs    r4, r4, #1
+	        add     r3, r3, #32
+	        bne 222b
+	        .endm
+
+        .macro  loadstore, lines
+	        mov     r9, #\lines
+444:		ldr8w	r1, r3, r4, r5, r6, r7, r8, ip, lr, abort=20f
+		subs	r9, r9, #1
+		str8w	r0, r3, r4, r5, r6, r7, r8, ip, lr, abort=20f
+		bgt	444b
+	        .endm
 
 
 		enter	r4, lr
@@ -78,7 +95,7 @@
 		bne	10f
 
 1:		subs	r2, r2, #(28)
-		stmfd	sp!, {r5 - r8}
+		stmfd	sp!, {r5 - r9}
 		blt	5f
 
 	CALGN(	ands	ip, r0, #31		)
@@ -88,7 +105,73 @@
 	CALGN(	adr	r4, 6f			)
 	CALGN(	subs	r2, r2, r3		)  @ C gets set
 	CALGN(	add	pc, r4, ip		)
-
+#ifdef CONFIG_AURORA_IO_CACHE_COHERENCY 
+/* DSMP - Z1 optimized for Write Allocate + L2 Write Through */
+copy8:
+		movs	r9, r2, lsr #8
+		beq	copy4
+		preload	r1, 8
+		preload r0, 8
+		loadstore 8 
+		sub	r2, #32*8
+		b	copy8
+copy4:	
+		movs	r9, r2, lsr #7
+		beq	copy2
+		preload	r1, 4
+		preload r0, 4
+		loadstore 4
+		sub	r2, #32*4
+		b	copy4
+#else
+copy128:
+                movs    r9, r2, lsr #12
+                beq     copy64
+                preload r1, 128
+                loadstore 128
+                sub     r2, #32*128
+                b       copy128
+copy64:
+                movs    r9, r2, lsr #11
+                beq     copy32
+                preload r1, 64
+                loadstore 64
+                sub     r2, #32*64
+                b       copy64
+
+copy32:
+		movs	r9, r2, lsr #10
+		beq	copy16
+		preload	r1, 32
+		loadstore 32
+		sub	r2, #32*32
+		b 	copy32
+copy16:	
+		movs	r9, r2, lsr #9
+		beq	copy8
+		preload	r1, 16
+		loadstore 16
+		sub	r2, #32*16
+		b	copy16
+
+copy8:
+		movs	r9, r2, lsr #8
+		beq	copy4
+		preload	r1, 8
+		loadstore 8 
+		sub	r2, #32*8
+		b	copy8
+copy4:	
+		movs	r9, r2, lsr #7
+		beq	copy2
+		preload	r1, 4
+		loadstore 4
+		sub	r2, #32*4
+		b	copy4
+#endif
+copy2:
+	
+	
 	PLD(	pld	[r1, #0]		)
 2:	PLD(	subs	r2, r2, #96		)
 	PLD(	pld	[r1, #28]		)
@@ -143,7 +226,7 @@
 
 	CALGN(	bcs	2b			)
 
-7:		ldmfd	sp!, {r5 - r8}
+7:		ldmfd	sp!, {r5 - r9}
 
 8:		movs	r2, r2, lsl #31
 		ldr1b	r1, r3, ne, abort=21f
@@ -257,7 +340,7 @@
 	.macro	copy_abort_preamble
 19:	ldmfd	sp!, {r5 - r9}
 	b	21f
-20:	ldmfd	sp!, {r5 - r8}
+20:	ldmfd	sp!, {r5 - r9}
 21:
 	.endm
 
diff --git a/arch/arm/mach-kirkwood/common.c b/arch/arm/mach-kirkwood/common.c
index f759ca2..4ccfdf9 100644
--- a/arch/arm/mach-kirkwood/common.c
+++ b/arch/arm/mach-kirkwood/common.c
@@ -305,6 +305,15 @@ void __init kirkwood_nand_init(struct mtd_partition *parts, int nr_parts,
 	platform_device_register(&kirkwood_nand_flash);
 }
 
+void __init kirkwood_nand_init_rnb(struct mtd_partition *parts, int nr_parts,
+				   int (*dev_ready)(struct mtd_info *))
+{
+	kirkwood_clk_ctrl |= CGC_RUNIT;
+	kirkwood_nand_data.parts = parts;
+	kirkwood_nand_data.nr_parts = nr_parts;
+	kirkwood_nand_data.dev_ready = dev_ready;
+	platform_device_register(&kirkwood_nand_flash);
+}
 
 /*****************************************************************************
  * SoC RTC
@@ -393,7 +402,7 @@ void __init kirkwood_sdio_init(struct mvsdio_platform_data *mvsdio_data)
 	u32 dev, rev;
 
 	kirkwood_pcie_id(&dev, &rev);
-	if (rev == 0)  /* catch all Kirkwood Z0's */
+	if (rev == 0 && dev != MV88F6282_DEV_ID) /* catch all Kirkwood Z0's */
 		mvsdio_data->clock = 100000000;
 	else
 		mvsdio_data->clock = 200000000;
@@ -838,8 +847,10 @@ int __init kirkwood_find_tclk(void)
 	u32 dev, rev;
 
 	kirkwood_pcie_id(&dev, &rev);
-	if (dev == MV88F6281_DEV_ID && (rev == MV88F6281_REV_A0 ||
-					rev == MV88F6281_REV_A1))
+
+	if ((dev == MV88F6281_DEV_ID && (rev == MV88F6281_REV_A0 ||
+					rev == MV88F6281_REV_A1)) ||
+	    (dev == MV88F6282_DEV_ID))
 		return 200000000;
 
 	return 166666667;
@@ -882,13 +893,22 @@ static char * __init kirkwood_id(void)
 			return "MV88F6192-Z0";
 		else if (rev == MV88F6192_REV_A0)
 			return "MV88F6192-A0";
+		else if (rev == MV88F6192_REV_A1)
+			return "MV88F6192-A1";
 		else
 			return "MV88F6192-Rev-Unsupported";
 	} else if (dev == MV88F6180_DEV_ID) {
 		if (rev == MV88F6180_REV_A0)
 			return "MV88F6180-Rev-A0";
+		else if (rev == MV88F6180_REV_A1)
+			return "MV88F6180-Rev-A1";
 		else
 			return "MV88F6180-Rev-Unsupported";
+	} else if (dev == MV88F6282_DEV_ID) {
+		if (rev == MV88F6282_REV_A0)
+			return "MV88F6282-Rev-A0";
+		else
+			return "MV88F6282-Rev-Unsupported";
 	} else {
 		return "Device-Unknown";
 	}
diff --git a/arch/arm/mach-kirkwood/common.h b/arch/arm/mach-kirkwood/common.h
index d7de434..05e8a8a 100644
--- a/arch/arm/mach-kirkwood/common.h
+++ b/arch/arm/mach-kirkwood/common.h
@@ -16,6 +16,7 @@ struct mv643xx_eth_platform_data;
 struct mv_sata_platform_data;
 struct mvsdio_platform_data;
 struct mtd_partition;
+struct mtd_info;
 
 /*
  * Basic Kirkwood init functions used early by machine-setup.
@@ -41,6 +42,7 @@ void kirkwood_i2c_init(void);
 void kirkwood_uart0_init(void);
 void kirkwood_uart1_init(void);
 void kirkwood_nand_init(struct mtd_partition *parts, int nr_parts, int delay);
+void kirkwood_nand_init_rnb(struct mtd_partition *parts, int nr_parts, int (*dev_ready)(struct mtd_info *));
 
 extern int kirkwood_tclk;
 extern struct sys_timer kirkwood_timer;
diff --git a/arch/arm/mach-kirkwood/include/mach/kirkwood.h b/arch/arm/mach-kirkwood/include/mach/kirkwood.h
index a15cf0e..dd7eddb 100644
--- a/arch/arm/mach-kirkwood/include/mach/kirkwood.h
+++ b/arch/arm/mach-kirkwood/include/mach/kirkwood.h
@@ -107,8 +107,12 @@
 #define MV88F6192_DEV_ID	0x6192
 #define MV88F6192_REV_Z0	0
 #define MV88F6192_REV_A0	2
+#define MV88F6192_REV_A1	3
 
 #define MV88F6180_DEV_ID	0x6180
 #define MV88F6180_REV_A0	2
+#define MV88F6180_REV_A1	3
 
+#define MV88F6282_DEV_ID	0x6282
+#define MV88F6282_REV_A0	0
 #endif
diff --git a/arch/arm/mach-kirkwood/mpp.c b/arch/arm/mach-kirkwood/mpp.c
index a5900f6..065187d 100644
--- a/arch/arm/mach-kirkwood/mpp.c
+++ b/arch/arm/mach-kirkwood/mpp.c
@@ -23,7 +23,8 @@ static unsigned int __init kirkwood_variant(void)
 
 	kirkwood_pcie_id(&dev, &rev);
 
-	if (dev == MV88F6281_DEV_ID && rev >= MV88F6281_REV_A0)
+	if ((dev == MV88F6281_DEV_ID && rev >= MV88F6281_REV_A0) ||
+	    (dev == MV88F6282_DEV_ID))
 		return MPP_F6281_MASK;
 	if (dev == MV88F6192_DEV_ID && rev >= MV88F6192_REV_A0)
 		return MPP_F6192_MASK;
diff --git a/arch/arm/mm/Kconfig b/arch/arm/mm/Kconfig
index 6a04fe1..0423508 100644
--- a/arch/arm/mm/Kconfig
+++ b/arch/arm/mm/Kconfig
@@ -382,6 +382,23 @@ config CPU_FEROCEON_OLD_ID
 	  for which the CPU ID is equal to the ARM926 ID.
 	  Relevant for Feroceon-1850 and early Feroceon-2850.
 
+choice
+	prompt "Marvell Sheeva CPU Architecture"
+	default CPU_SHEEVA_PJ4B_V6
+
+config CPU_SHEEVA_PJ4B_V6
+	bool "Support Sheeva processor in V6 mode" if ARCH_ARMADA_XP || ARCH_ARMADA370
+	select CPU_V6
+	select CPU_32v6K
+	select DMA_CACHE_RWFO if SMP
+
+config CPU_SHEEVA_PJ4B_V7
+	bool "Support Sheeva processor in V7 mode" if ARCH_ARMADA_XP || ARCH_ARMADA370
+	select CPU_V7
+	select CPU_USE_DOMAINS if MMU
+
+endchoice
+
 # ARMv6
 config CPU_V6
 	bool "Support ARM V6 processor" if ARCH_INTEGRATOR || MACH_REALVIEW_EB || MACH_REALVIEW_PBX || ARCH_DOVE
@@ -648,19 +665,26 @@ config CPU_BIG_ENDIAN
 	  of your chipset/board/processor.
 
 config CPU_ENDIAN_BE8
-	bool
+	bool "Support BE8 Mode"
 	depends on CPU_BIG_ENDIAN
 	default CPU_V6 || CPU_V7
 	help
 	  Support for the BE-8 (big-endian) mode on ARMv6 and ARMv7 processors.
 
 config CPU_ENDIAN_BE32
-	bool
+	bool "Support BE32 Mode"
 	depends on CPU_BIG_ENDIAN
 	default !CPU_ENDIAN_BE8
 	help
 	  Support for the BE-32 (big-endian) mode on pre-ARMv6 processors.
 
+config BE8_ON_LE
+	bool "Run BE8 kernel on a little endian machine"
+	depends on CPU_V6 || CPU_V7
+	select CPU_BIG_ENDIAN
+	help
+	  Run BE8 kernel on a little endian machine.
+
 config CPU_HIGH_VECTOR
 	depends on !MMU && CPU_CP15 && !CPU_ARM740T
 	bool "Select the High exception vector"
@@ -788,6 +812,188 @@ config CACHE_FEROCEON_L2_WRITETHROUGH
 	  Say Y here to use the Feroceon L2 cache in writethrough mode.
 	  Unless you specifically require this, say N for writeback mode.
 
+config SHEEVA_ERRATA_ARM_CPU_4742
+	bool "Sheeva Errata 4742: Enable sync barriers after WFI idle"
+	depends on CPU_SHEEVA_PJ4B_V6 || CPU_SHEEVA_PJ4B_V7
+	help
+	  When coming out of WFI IDLE state, a specific timing sensitivity exists
+	  between the retiring WFI instruction and the newly issued subsequent 
+	  instructions. This sensitivity can result in a CPU hang scenario.
+	  WA: The software must insert either a Data Synchronization Barrier (DSB)
+	  or Data Memory Barrier (DMB) command immediately after the WFI instruction.
+
+config SHEEVA_ERRATA_ARM_CPU_4786
+	bool "Sheeva Errata 4786: Disable coprocessor dual issue mode"
+	depends on (CPU_SHEEVA_PJ4B_V6 || CPU_SHEEVA_PJ4B_V7) && ARM_THUMB && VFP
+	help
+	  If the last instruction inside a Thumb IT block is a branch, and the
+	  following instruction is a VFP instruction, the logic may incorrectly
+	  dual-issue the VFP instruction along with the branch instruction. This
+	  causes the VFP instruction to be executed, even though the branch 
+	  instruction may be taken.
+	  WA: Set the CP15 coprocessor dual-issue disable bit in the Auxiliary
+	  Debug Modes Control 0 register (bit[15]). This setting disables the
+	  dual-issuing of VFP instructions before entering an IT block.
+
+config SHEEVA_ERRATA_ARM_CPU_5315
+	bool "Sheeva Errata 5315: Disable Data Speculative prefetch from MBU/LSU"
+	depends on CPU_SHEEVA_PJ4B_V6 || CPU_SHEEVA_PJ4B_V7
+	default n
+	help
+	  When a PLD instruction is used as a memory hint, using a fast load
+	  bypass (load data is used as the address for a subsequent memory access)
+	  can result in data corruption.
+	  WA: Do not operate in Speculative mode
+
+config SHEEVA_ERRATA_ARM_CPU_4413
+	bool "Sheeva Errata 4413: Add SB before L1 Invalidate by MVA"
+	depends on CPU_SHEEVA_PJ4B_V6 || CPU_SHEEVA_PJ4B_V7
+	help
+	  Potentially, invalidates by a Modified Virtual Address (MVA) entry can
+	  cause lines to fill into the L1 cache in an “intermediate” state without
+	  the lines being subsequently updated to a valid state. As a result, a 
+	  subsequent “multi-hit” scenario and potential data corruption can occur
+	  when a future operation allocates the same address to a separate line.
+	  WA: The Invalidate by MVA operation (or sequence of MVA operations) must
+	  be preceded by a barrier to ensure that the preceding pre-condition for
+	  the cache miss is completed.
+
+config SHEEVA_ERRATA_ARM_CPU_4659
+	bool "Sheeva Errata 4659: Add ISB following L1 I$ Invalidate by MVA"
+	depends on CPU_SHEEVA_PJ4B_V6 || CPU_SHEEVA_PJ4B_V7
+	help
+	  I-Cache maintenance by using an MVA command can cause other instructions
+	  in I-Fetch to be executed twice.
+	  WA: Follow I-Cache maintenance by an MVA command with an Instruction 
+	  Synchronization Barrier (ISB) instruction. This instruction flushes the
+	  pipeline after the maintenance operation, deleting any double-instructions.
+
+config SHEEVA_ERRATA_ARM_CPU_5114
+	bool "Sheeva Errata 5114: Force all MMU pages to be Shared"
+	depends on CPU_SHEEVA_PJ4B_V6 && AURORA_IO_CACHE_COHERENCY
+	help
+	  When a non-shared line fill request causes a shared cacheable eviction
+	  in close proximity to an incoming snoop to the victim line, the incoming
+	  snoop can incorrectly miss and result in data corruption.
+	  WA: When using cacheable shared memory, set all memory pages/descriptors
+	  to be shared in the Translation Table Base Register 0
+
+config SHEEVA_ERRATA_ARM_CPU_4611
+	bool "Sheeva Errata 4611: Preceed every L1 Clean operation with DSB"
+	depends on CPU_SHEEVA_PJ4B_V6 || CPU_SHEEVA_PJ4B_V7
+	help
+	  A CP15 clean operation can result in a dead lock state if it is hit by
+	  an incoming snoop evento.
+	  WA: Before any CP15 clean type operation in Cache Coherency mode, issue
+	  a Data Memory Barrier (DMB) or a Data Synchronization Barrier (DSB)
+	  instruction.
+
+config SHEEVA_ERRATA_ARM_CPU_BTS61
+	bool "Sheeva Errata BTS61: Disable WFI and WFE instructions in SMP or Coherent systems"
+	depends on (CPU_SHEEVA_PJ4B_V6 || CPU_SHEEVA_PJ4B_V7) && (SMP || AURORA_IO_CACHE_COHERENCY)
+	help
+	  When the CPU is set to WFI mode in the system with a Snoop command, a
+	  snoop response may not occur and that will cause a system hang.
+	  WA: Do not use the WFI mode in SMP/coherent systems.
+
+config SHEEVA_ERRATA_ARM_CPU_4948
+	bool "Sheeva Errata 4948: Disable L0 cache"
+	depends on CPU_SHEEVA_PJ4B_V6 || CPU_SHEEVA_PJ4B_V7
+	default n
+	help
+	  In a very rare case, the core can fail to observe the correct data
+	  as follows:
+	    1. After a first load, a second load operation to the same address
+	       occurs, and an invalidate snoop to the same line is arbitrated
+	       between both reads.
+	    2. The core may observe the old data for the second read, while the
+	       first load got the new data laying in this address.
+	    3. A third load to the same address will see the new data.
+	  WA: This event can occur when the first load access crosses a cache
+	  line boundary (i.e., unaligned access). If it does occur, L0 data
+	  cache can be disabled
+
+config SHEEVA_ERRATA_ARM_CPU_ADD_DELAY_FOR_STOP_MACHINE
+	bool "stop_machine function can livelock"
+	depends on CPU_V6 && SMP
+	default y
+	help
+	  add delay when polling the stop_machine state.
+
+config SHEEVA_DEEP_IDLE
+	bool "Enable CPU/L2 Deep Idle Power Management"
+	depends on (ARCH_ARMADA_XP || ARCH_ARMADA370) && CPU_IDLE && !SMP
+
+config ARMADA_XP_DEEP_IDLE_L2_WA
+        bool "Manage Aurora L2 on deepIdle"
+        depends on SHEEVA_DEEP_IDLE && CACHE_AURORA_L2
+
+config ARMADA_XP_DEEP_IDLE_UNMASK_INTS_WA
+        bool "Enable deepIdle workaround for regret mode"
+        depends on SHEEVA_DEEP_IDLE
+
+config ARMADA_SUPPORT_DEEP_IDLE_DRAM_SR
+        bool "Support DDR Self-Refresh in Deep-Idle"
+	default n
+        depends on SHEEVA_DEEP_IDLE
+
+config ARMADA_SUPPORT_DEEP_IDLE_FAST_EXIT
+	bool "Enable Fast Exit from Deep-Idle"
+	default y
+	depends on SHEEVA_DEEP_IDLE
+	help
+	  Enable fast exit from Deep-Idle by using a reserved block in crypto engine SRAM.
+
+config CACHE_AURORA_L2
+	bool "Enable Marvell Aurora L2 cache controller"
+	depends on ARCH_ARMADA_XP || ARCH_ARMADA370
+	default y
+	select OUTER_CACHE
+	help
+	  This option enables the Marvell Aurora L2 cache controller.
+
+config AURORA_L2_PT_WALK
+	bool "Enable Marvell page table walk in L2 to improve performance"
+	depends on CACHE_AURORA_L2
+	default y
+	help
+	  This option enables PTE caching in L2 to improve performance.
+
+config ENABLE_UNALINGED_ACCESS_FAULT
+	bool "Enable S/W handling for Unaligned Access"
+	default n
+	help
+	  This flag enables S/W handling of unaligned access
+
+config AURORA_IO_CACHE_COHERENCY
+	bool "Enable Marvell Aurora I/O cache coherency"
+	depends on ARCH_ARMADA_XP || ARCH_ARMADA370
+	default y
+	help
+	  This option enables the hardware mechanism for I/O cache coherency.
+
+#config AURORA_IOCC_DISABLE_WRITE_ALLOCATE
+#	bool "Disable Write Allocate for I/O cache coherency"
+#	depends on AURORA_IO_CACHE_COHERENCY && ARCH_ARMADA370
+#	default n
+#	help
+#	  This option disables write-allocate when working in I/O cache coherency.
+
+config CPU_SHEEVA_PJ4B_PMC_ACCESS_IN_USERMODE
+	bool "Enabled User mode access for PMC"
+	depends on CPU_SHEEVA_PJ4B_V6 || CPU_SHEEVA_PJ4B_V7
+	default n
+	help
+	  Say Y if you allow user mode application to access Performance 
+	  Monitor Counter of PJ4 in user mode
+
+config MV_SUPPORT_64KB_PAGE_SIZE
+	bool "Support 64KB page size"
+	depends on ARCH_ARMADA_XP || ARCH_ARMADA370
+	default n
+	help
+	  This option enables 64KB pages.
+
 config CACHE_L2X0
 	bool "Enable the L2x0 outer cache controller"
 	depends on REALVIEW_EB_ARM11MP || MACH_REALVIEW_PB11MP || MACH_REALVIEW_PB1176 || \
diff --git a/arch/arm/mm/Makefile b/arch/arm/mm/Makefile
index e8d34a80..e9117f8 100644
--- a/arch/arm/mm/Makefile
+++ b/arch/arm/mm/Makefile
@@ -1,6 +1,9 @@
 #
 # Makefile for the linux arm-specific parts of the memory manager.
 #
+ifeq ($(CONFIG_PLAT_ARMADA),y)
+	include $(srctree)/$(MACHINE)/config/mvRules.mk
+endif
 
 obj-y				:= dma-mapping.o extable.o fault.o init.o \
 				   iomap.o
@@ -90,8 +93,10 @@ obj-$(CONFIG_CPU_XSCALE)	+= proc-xscale.o
 obj-$(CONFIG_CPU_XSC3)		+= proc-xsc3.o
 obj-$(CONFIG_CPU_MOHAWK)	+= proc-mohawk.o
 obj-$(CONFIG_CPU_FEROCEON)	+= proc-feroceon.o
-obj-$(CONFIG_CPU_V6)		+= proc-v6.o
-obj-$(CONFIG_CPU_V7)		+= proc-v7.o
+#obj-$(CONFIG_CPU_V6)		+= proc-v6.o
+#obj-$(CONFIG_CPU_V7)		+= proc-v7.o
+obj-$(CONFIG_CPU_SHEEVA_PJ4B_V6)	+= proc-sheeva_pj4bv6.o
+obj-$(CONFIG_CPU_SHEEVA_PJ4B_V7)	+= proc-sheeva_pj4bv7.o
 
 AFLAGS_proc-v6.o	:=-Wa,-march=armv6
 AFLAGS_proc-v7.o	:=-Wa,-march=armv7-a
@@ -100,3 +105,4 @@ obj-$(CONFIG_CACHE_FEROCEON_L2)	+= cache-feroceon-l2.o
 obj-$(CONFIG_CACHE_L2X0)	+= cache-l2x0.o
 obj-$(CONFIG_CACHE_XSC3L2)	+= cache-xsc3l2.o
 obj-$(CONFIG_CACHE_TAUROS2)	+= cache-tauros2.o
+obj-$(CONFIG_CACHE_AURORA_L2)	+= cache-aurora-l2.o
diff --git a/arch/arm/mm/abort-ev6.S b/arch/arm/mm/abort-ev6.S
index f332df7..4ac9839 100644
--- a/arch/arm/mm/abort-ev6.S
+++ b/arch/arm/mm/abort-ev6.S
@@ -28,6 +28,7 @@ ENTRY(v6_early_abort)
 #endif
 	mrc	p15, 0, r1, c5, c0, 0		@ get FSR
 	mrc	p15, 0, r0, c6, c0, 0		@ get FAR
+#ifndef CONFIG_SMP	
 /*
  * Faulty SWP instruction on 1136 doesn't set bit 11 in DFSR (erratum 326103).
  * The test below covers all the write situations, including Java bytecodes
@@ -43,6 +44,7 @@ ENTRY(v6_early_abort)
 	do_ldrd_abort
 	tst	r3, #1 << 20			@ L = 0 -> write
 	orreq	r1, r1, #1 << 11		@ yes.
+#endif /* !CONFIG_SMP */	
 	mov	pc, lr
 
 
diff --git a/arch/arm/mm/alignment.c b/arch/arm/mm/alignment.c
index a2ab51f..31b4bc4 100644
--- a/arch/arm/mm/alignment.c
+++ b/arch/arm/mm/alignment.c
@@ -919,8 +919,10 @@ static int __init alignment_init(void)
 	 * making any progress.
 	 */
 	if (cpu_architecture() >= CPU_ARCH_ARMv6 && (cr_alignment & CR_U)) {
+#ifndef CONFIG_ENABLE_UNALINGED_ACCESS_FAULT
 		cr_alignment &= ~CR_A;
 		cr_no_alignment &= ~CR_A;
+#endif
 		set_cr(cr_alignment);
 		ai_usermode = UM_FIXUP;
 	}
diff --git a/arch/arm/mm/cache-v6.S b/arch/arm/mm/cache-v6.S
index 86aa689..0653671 100644
--- a/arch/arm/mm/cache-v6.S
+++ b/arch/arm/mm/cache-v6.S
@@ -56,14 +56,32 @@ ENTRY(v6_icache_inval_all)
 ENTRY(v6_flush_kern_cache_all)
 	mov	r0, #0
 #ifdef HARVARD_CACHE
+#ifdef CONFIG_SHEEVA_ERRATA_ARM_CPU_4611
+        mrs     r2, cpsr
+        orr     r3, r2, #PSR_F_BIT | PSR_I_BIT
+        msr     cpsr_c, r3                      @ Disable interrupts
+        mcr     p15, 0, r0, c7, c10, 5          @ DMB
+#endif
 	mcr	p15, 0, r0, c7, c14, 0		@ D cache clean+invalidate
+#ifdef CONFIG_SHEEVA_ERRATA_ARM_CPU_4611
+	msr     cpsr_c, r2                      @ Restore interrupts
+#endif
 #ifndef CONFIG_ARM_ERRATA_411920
 	mcr	p15, 0, r0, c7, c5, 0		@ I+BTB cache invalidate
 #else
 	b	v6_icache_inval_all
 #endif
 #else
+#ifdef CONFIG_SHEEVA_ERRATA_ARM_CPU_4611
+        mrs     r2, cpsr
+        orr     r3, r2, #PSR_F_BIT | PSR_I_BIT
+        msr     cpsr_c, r3                      @ Disable interrupts
+        mcr     p15, 0, r0, c7, c10, 5          @ DMB
+#endif
 	mcr	p15, 0, r0, c7, c15, 0		@ Cache clean+invalidate
+#ifdef CONFIG_SHEEVA_ERRATA_ARM_CPU_4611
+	msr     cpsr_c, r2                      @ Restore interrupts
+#endif
 #endif
 	mov	pc, lr
 
@@ -125,12 +143,21 @@ ENTRY(v6_coherent_user_range)
  UNWIND(.fnstart		)
 #ifdef HARVARD_CACHE
 	bic	r0, r0, #CACHE_LINE_SIZE - 1
+#endif
+#if defined (CONFIG_SHEEVA_ERRATA_ARM_CPU_4611) || !defined(CONFIG_HAVE_GENERIC_DMA_COHERENT)
+        mrs     r2, cpsr
+        orr     r3, r2, #PSR_F_BIT | PSR_I_BIT
+        msr     cpsr_c, r3                      @ Disable interrupts
+        mcr     p15, 0, r0, c7, c10, 5          @ DMB
+#endif
 1:
  USER(	mcr	p15, 0, r0, c7, c10, 1	)	@ clean D line
 	add	r0, r0, #CACHE_LINE_SIZE
 2:
 	cmp	r0, r1
 	blo	1b
+#if defined (CONFIG_SHEEVA_ERRATA_ARM_CPU_4611) || !defined(CONFIG_HAVE_GENERIC_DMA_COHERENT)
+        msr     cpsr_c, r2                      @ Restore interrupts
 #endif
 	mov	r0, #0
 #ifdef HARVARD_CACHE
@@ -150,9 +177,15 @@ ENTRY(v6_coherent_user_range)
  * isn't mapped, just try the next page.
  */
 9001:
+#ifdef CONFIG_MV_SUPPORT_64KB_PAGE_SIZE
+	mov	r0, r0, lsr #16
+	mov	r0, r0, lsl #16
+	add	r0, r0, #0x10000
+#else
 	mov	r0, r0, lsr #12
 	mov	r0, r0, lsl #12
 	add	r0, r0, #4096
+#endif
 	b	2b
  UNWIND(.fnend		)
 ENDPROC(v6_coherent_user_range)
@@ -169,6 +202,12 @@ ENDPROC(v6_coherent_kern_range)
  */
 ENTRY(v6_flush_kern_dcache_area)
 	add	r1, r0, r1
+#if defined (CONFIG_SHEEVA_ERRATA_ARM_CPU_4611) || !defined(CONFIG_HAVE_GENERIC_DMA_COHERENT)
+        mrs     r2, cpsr
+        orr     r3, r2, #PSR_F_BIT | PSR_I_BIT
+        msr     cpsr_c, r3                      @ Disable interrupts
+        mcr     p15, 0, r0, c7, c10, 5          @ DMB
+#endif
 1:
 #ifdef HARVARD_CACHE
 	mcr	p15, 0, r0, c7, c14, 1		@ clean & invalidate D line
@@ -178,6 +217,9 @@ ENTRY(v6_flush_kern_dcache_area)
 	add	r0, r0, #D_CACHE_LINE_SIZE
 	cmp	r0, r1
 	blo	1b
+#if defined (CONFIG_SHEEVA_ERRATA_ARM_CPU_4611) || !defined(CONFIG_HAVE_GENERIC_DMA_COHERENT)
+	msr     cpsr_c, r2                      @ Restore interrupts
+#endif
 #ifdef HARVARD_CACHE
 	mov	r0, #0
 	mcr	p15, 0, r0, c7, c10, 4
@@ -198,6 +240,13 @@ ENTRY(v6_flush_kern_dcache_area)
 v6_dma_inv_range:
 	tst	r0, #D_CACHE_LINE_SIZE - 1
 	bic	r0, r0, #D_CACHE_LINE_SIZE - 1
+#if defined (CONFIG_SHEEVA_ERRATA_ARM_CPU_4611) || !defined(CONFIG_HAVE_GENERIC_DMA_COHERENT) 
+        mrs     r2, cpsr
+        orr     r3, r2, #PSR_F_BIT | PSR_I_BIT
+        msr     cpsr_c, r3                      @ Disable interrupts
+	mov	r3, r2
+        mcr     p15, 0, r0, c7, c10, 5          @ DMB
+#endif
 #ifdef HARVARD_CACHE
 	mcrne	p15, 0, r0, c7, c10, 1		@ clean D line
 #else
@@ -215,6 +264,11 @@ v6_dma_inv_range:
 	ldr	r2, [r0]			@ read for ownership
 	str	r2, [r0]			@ write for ownership
 #endif
+
+#if defined (CONFIG_SHEEVA_ERRATA_ARM_CPU_4413) || !defined(CONFIG_HAVE_GENERIC_DMA_COHERENT) 
+	mcr 	p15, 0, r0, c7, c10, 5		@ Data Memory Barrier
+#endif
+
 #ifdef HARVARD_CACHE
 	mcr	p15, 0, r0, c7, c6, 1		@ invalidate D line
 #else
@@ -223,6 +277,9 @@ v6_dma_inv_range:
 	add	r0, r0, #D_CACHE_LINE_SIZE
 	cmp	r0, r1
 	blo	1b
+#if defined (CONFIG_SHEEVA_ERRATA_ARM_CPU_4611) || !defined(CONFIG_HAVE_GENERIC_DMA_COHERENT) 
+	msr     cpsr_c, r3                      @ Restore interrupts
+#endif
 	mov	r0, #0
 	mcr	p15, 0, r0, c7, c10, 4		@ drain write buffer
 	mov	pc, lr
@@ -234,10 +291,21 @@ v6_dma_inv_range:
  */
 v6_dma_clean_range:
 	bic	r0, r0, #D_CACHE_LINE_SIZE - 1
+#if defined (CONFIG_SHEEVA_ERRATA_ARM_CPU_4611) || !defined(CONFIG_HAVE_GENERIC_DMA_COHERENT) 
+	mrs     r2, cpsr
+       	orr     r3, r2, #PSR_F_BIT | PSR_I_BIT
+	msr     cpsr_c, r3                      @ Disable interrupts
+	mov	r3, r2
+#endif
 1:
 #ifdef CONFIG_DMA_CACHE_RWFO
 	ldr	r2, [r0]			@ read for ownership
 #endif
+
+#if defined (CONFIG_SHEEVA_ERRATA_ARM_CPU_4611) || !defined(CONFIG_HAVE_GENERIC_DMA_COHERENT) 
+        mcr     p15, 0, r0, c7, c10, 5          @ DMB
+#endif
+
 #ifdef HARVARD_CACHE
 	mcr	p15, 0, r0, c7, c10, 1		@ clean D line
 #else
@@ -246,6 +314,9 @@ v6_dma_clean_range:
 	add	r0, r0, #D_CACHE_LINE_SIZE
 	cmp	r0, r1
 	blo	1b
+#if defined (CONFIG_SHEEVA_ERRATA_ARM_CPU_4611) || !defined(CONFIG_HAVE_GENERIC_DMA_COHERENT) 
+	msr     cpsr_c, r3                      @ Restore interrupts
+#endif
 	mov	r0, #0
 	mcr	p15, 0, r0, c7, c10, 4		@ drain write buffer
 	mov	pc, lr
@@ -257,11 +328,22 @@ v6_dma_clean_range:
  */
 ENTRY(v6_dma_flush_range)
 	bic	r0, r0, #D_CACHE_LINE_SIZE - 1
+#if defined (CONFIG_SHEEVA_ERRATA_ARM_CPU_4611) || !defined(CONFIG_HAVE_GENERIC_DMA_COHERENT) 
+        mrs     r2, cpsr
+        orr     r3, r2, #PSR_F_BIT | PSR_I_BIT
+        msr     cpsr_c, r3                      @ Disable interrupts
+	mov	r3, r2
+#endif
 1:
 #ifdef CONFIG_DMA_CACHE_RWFO
 	ldr	r2, [r0]			@ read for ownership
 	str	r2, [r0]			@ write for ownership
 #endif
+
+#if defined (CONFIG_SHEEVA_ERRATA_ARM_CPU_4611) || !defined(CONFIG_HAVE_GENERIC_DMA_COHERENT) 
+        mcr     p15, 0, r0, c7, c10, 5          @ DMB
+#endif
+
 #ifdef HARVARD_CACHE
 	mcr	p15, 0, r0, c7, c14, 1		@ clean & invalidate D line
 #else
@@ -270,6 +352,9 @@ ENTRY(v6_dma_flush_range)
 	add	r0, r0, #D_CACHE_LINE_SIZE
 	cmp	r0, r1
 	blo	1b
+#if defined (CONFIG_SHEEVA_ERRATA_ARM_CPU_4611) || !defined(CONFIG_HAVE_GENERIC_DMA_COHERENT) 
+	msr     cpsr_c, r3                      @ Restore interrupts
+#endif
 	mov	r0, #0
 	mcr	p15, 0, r0, c7, c10, 4		@ drain write buffer
 	mov	pc, lr
diff --git a/arch/arm/mm/cache-v7.S b/arch/arm/mm/cache-v7.S
index 37c8157..e4539df 100644
--- a/arch/arm/mm/cache-v7.S
+++ b/arch/arm/mm/cache-v7.S
@@ -181,6 +181,9 @@ ENTRY(v7_coherent_user_range)
  USER(	mcr	p15, 0, r0, c7, c11, 1	)	@ clean D line to the point of unification
 	dsb
  USER(	mcr	p15, 0, r0, c7, c5, 1	)	@ invalidate I line
+#ifdef CONFIG_SHEEVA_ERRATA_ARM_CPU_4659
+	isb
+#endif
 	add	r0, r0, r2
 2:
 	cmp	r0, r1
@@ -200,9 +203,15 @@ ENTRY(v7_coherent_user_range)
  * isn't mapped, just try the next page.
  */
 9001:
+#ifdef CONFIG_MV_SUPPORT_64KB_PAGE_SIZE
+	mov	r0, r0, lsr #16
+	mov	r0, r0, lsl #16
+	add	r0, r0, #0x10000
+#else
 	mov	r0, r0, lsr #12
 	mov	r0, r0, lsl #12
 	add	r0, r0, #4096
+#endif
 	b	2b
 #endif
  UNWIND(.fnend		)
@@ -257,6 +266,9 @@ v7_dma_inv_range:
 	bic	r1, r1, r3
 	mcrne	p15, 0, r1, c7, c14, 1		@ clean & invalidate D / U line
 1:
+#if defined (CONFIG_SHEEVA_ERRATA_ARM_CPU_4413)
+	dmb
+#endif
 	mcr	p15, 0, r0, c7, c6, 1		@ invalidate D / U line
 	add	r0, r0, r2
 	cmp	r0, r1
diff --git a/arch/arm/mm/copypage-v6.c b/arch/arm/mm/copypage-v6.c
index f55fa10..58ac16f 100644
--- a/arch/arm/mm/copypage-v6.c
+++ b/arch/arm/mm/copypage-v6.c
@@ -63,11 +63,20 @@ static void v6_clear_user_highpage_nonaliasing(struct page *page, unsigned long
  */
 static void discard_old_kernel_data(void *kto)
 {
+#ifdef CONFIG_SHEEVA_ERRATA_ARM_CPU_4611
+	unsigned long flags;
+
+	raw_local_irq_save(flags);
+	dmb();	
+#endif
 	__asm__("mcrr	p15, 0, %1, %0, c6	@ 0xec401f06"
 	   :
 	   : "r" (kto),
 	     "r" ((unsigned long)kto + PAGE_SIZE - L1_CACHE_BYTES)
 	   : "cc");
+#ifdef CONFIG_SHEEVA_ERRATA_ARM_CPU_4611
+	raw_local_irq_restore(flags);
+#endif
 }
 
 /*
diff --git a/arch/arm/mm/fault.c b/arch/arm/mm/fault.c
index 8ad75e9..753b76a 100644
--- a/arch/arm/mm/fault.c
+++ b/arch/arm/mm/fault.c
@@ -108,7 +108,19 @@ void show_pte(struct mm_struct *mm, unsigned long addr)
 
 		pte = pte_offset_map(pmd, addr);
 		printk(", *pte=%08lx", pte_val(*pte));
+#ifdef CONFIG_MV_SUPPORT_64KB_PAGE_SIZE
+		{
+			unsigned long pte_ptr = (unsigned long)pte;
+			unsigned long tmp = pte_ptr;
+			pte_ptr -= (HW_PTRS_PER_PTE * sizeof(void *));
+			pte_ptr &= ~0x7FC;
+			tmp &= 0x7C;
+			pte_ptr += (tmp << 4);
+			printk(", *ppte=%08lx", pte_val((pte_t *)pte_ptr));
+		}
+#else
 		printk(", *ppte=%08lx", pte_val(pte[-PTRS_PER_PTE]));
+#endif
 		pte_unmap(pte);
 	} while(0);
 
diff --git a/arch/arm/mm/init.c b/arch/arm/mm/init.c
index 55d07c8..17da905 100644
--- a/arch/arm/mm/init.c
+++ b/arch/arm/mm/init.c
@@ -31,6 +31,14 @@
 
 #include "mm.h"
 
+#ifdef CONFIG_MV_SUPPORT_64KB_PAGE_SIZE
+#define FREEAREA_ROUND_START(addr)	((((unsigned long)addr) + ((PAGE_SIZE) - 1)) & (~((PAGE_SIZE) - 1)))
+#define FREEAREA_ROUND_END(addr)	(((unsigned long)addr) & (~((PAGE_SIZE) - 1)))
+#else
+#define FREEAREA_ROUND_START(addr)	(addr)
+#define FREEAREA_ROUND_END(addr)	(addr)
+#endif
+
 static unsigned long phys_initrd_start __initdata = 0;
 static unsigned long phys_initrd_size __initdata = 0;
 
@@ -477,7 +485,7 @@ void __init bootmem_init(void)
 
 static inline int free_area(unsigned long pfn, unsigned long end, char *s)
 {
-	unsigned int pages = 0, size = (end - pfn) << (PAGE_SHIFT - 10);
+	unsigned int pages = 0, size = ((end > pfn) ? ((end - pfn) << (PAGE_SHIFT - 10)) : 0);
 
 	for (; pfn < end; pfn++) {
 		struct page *page = pfn_to_page(pfn);
@@ -718,10 +726,9 @@ void free_initmem(void)
 				    __phys_to_pfn(__pa(&__tcm_end)),
 				    "TCM link");
 #endif
-
 	if (!machine_is_integrator() && !machine_is_cintegrator())
-		totalram_pages += free_area(__phys_to_pfn(__pa(__init_begin)),
-					    __phys_to_pfn(__pa(__init_end)),
+		totalram_pages += free_area(__phys_to_pfn(__pa(FREEAREA_ROUND_START(__init_begin))),
+					    __phys_to_pfn(__pa(FREEAREA_ROUND_END(__init_end))),
 					    "init");
 }
 
@@ -731,6 +738,8 @@ static int keep_initrd;
 
 void free_initrd_mem(unsigned long start, unsigned long end)
 {
+	start = FREEAREA_ROUND_START(start);
+	end = FREEAREA_ROUND_END(end);
 	if (!keep_initrd)
 		totalram_pages += free_area(__phys_to_pfn(__pa(start)),
 					    __phys_to_pfn(__pa(end)),
diff --git a/arch/arm/mm/mm.h b/arch/arm/mm/mm.h
index a888363..fd89274 100644
--- a/arch/arm/mm/mm.h
+++ b/arch/arm/mm/mm.h
@@ -3,7 +3,11 @@
 /* the upper-most page table pointer */
 extern pmd_t *top_pmd;
 
+#ifdef CONFIG_MV_SUPPORT_64KB_PAGE_SIZE
+#define TOP_PTE(x)	pte_offset_kernel(pmd_off_k(x), x)
+#else
 #define TOP_PTE(x)	pte_offset_kernel(top_pmd, x)
+#endif
 
 static inline pmd_t *pmd_off(pgd_t *pgd, unsigned long virt)
 {
diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 5296763..dcffbff 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -24,6 +24,9 @@
 #include <asm/smp_plat.h>
 #include <asm/tlb.h>
 #include <asm/highmem.h>
+#if defined(CONFIG_MV_SUPPORT_64KB_PAGE_SIZE) && defined(CONFIG_HIGHMEM)
+#include <asm/fixmap.h>
+#endif
 #include <asm/traps.h>
 
 #include <asm/mach/arch.h>
@@ -121,6 +124,7 @@ static int __init early_cachepolicy(char *p)
 	}
 	if (i == ARRAY_SIZE(cache_policies))
 		printk(KERN_ERR "ERROR: unknown or unsupported cache policy\n");
+#if !defined (CONFIG_CPU_SHEEVA_PJ4B_V6) && !defined(CONFIG_CPU_SHEEVA_PJ4B_V7)
 	/*
 	 * This restriction is partly to do with the way we boot; it is
 	 * unpredictable to have memory mapped using two different sets of
@@ -132,6 +136,7 @@ static int __init early_cachepolicy(char *p)
 		printk(KERN_WARNING "Only cachepolicy=writeback supported on ARMv6 and later\n");
 		cachepolicy = CPOLICY_WRITEBACK;
 	}
+#endif
 	flush_cache_all();
 	set_cr(cr_alignment);
 	return 0;
@@ -295,7 +300,7 @@ static void __init build_mem_type_table(void)
 			cachepolicy = CPOLICY_WRITEBACK;
 		ecc_mask = 0;
 	}
-#ifdef CONFIG_SMP
+#if (defined(CONFIG_SMP) || defined (CONFIG_AURORA_IO_CACHE_COHERENCY)) && !defined(CONFIG_AURORA_IOCC_DISABLE_WRITE_ALLOCATE)
 	cachepolicy = CPOLICY_WRITEALLOC;
 #endif
 
@@ -418,7 +423,7 @@ static void __init build_mem_type_table(void)
 		mem_types[MT_MINICLEAN].prot_sect |= PMD_SECT_APX|PMD_SECT_AP_WRITE;
 		mem_types[MT_CACHECLEAN].prot_sect |= PMD_SECT_APX|PMD_SECT_AP_WRITE;
 
-#ifdef CONFIG_SMP
+#if defined(CONFIG_SMP) || defined (CONFIG_SHEEVA_ERRATA_ARM_CPU_5114)
 		/*
 		 * Mark memory with the "shared" attribute for SMP systems
 		 */
@@ -499,7 +504,11 @@ static void __init alloc_init_pte(pmd_t *pmd, unsigned long addr,
 	pte_t *pte;
 
 	if (pmd_none(*pmd)) {
+#ifdef CONFIG_MV_SUPPORT_64KB_PAGE_SIZE
+		pte = alloc_bootmem_low_pages(2 * HW_PTRS_PER_PTE * sizeof(pte_t));
+#else
 		pte = alloc_bootmem_low_pages(2 * PTRS_PER_PTE * sizeof(pte_t));
+#endif
 		__pmd_populate(pmd, __pa(pte) | type->prot_l1);
 	}
 
@@ -769,6 +778,7 @@ static void __init sanity_check_meminfo(void)
 #endif
 		j++;
 	}
+#if !defined(CONFIG_ARCH_ARMADA_XP) && !defined(CONFIG_ARCH_ARMADA370)
 #ifdef CONFIG_HIGHMEM
 	if (highmem) {
 		const char *reason = NULL;
@@ -801,6 +811,7 @@ static void __init sanity_check_meminfo(void)
 		}
 	}
 #endif
+#endif
 	meminfo.nr_banks = j;
 }
 
@@ -927,6 +938,30 @@ void __init reserve_node_zero(pg_data_t *pgdat)
 				BOOTMEM_DEFAULT);
 }
 
+#if defined(CONFIG_MV_SUPPORT_64KB_PAGE_SIZE) && defined(CONFIG_HIGHMEM)
+/* Create L1 Mapping for High-Mem pages. */
+static void __init map_highmem_pages(void)
+{
+	struct map_desc map;
+	unsigned long addr;
+	pmd_t *pmd;
+	pte_t *pte;
+
+	for (addr = FIXADDR_START; addr < FIXADDR_TOP; addr += SZ_1M) {
+		map.pfn = __phys_to_pfn(virt_to_phys((void*)addr));
+		map.virtual = addr;
+		map.length = PAGE_SIZE;
+		map.type = MT_DEVICE;
+		create_mapping(&map);
+
+		/* Clear the L2 entry. */
+		pmd = pmd_offset(pgd_offset_k(addr), addr);
+		pte = pte_offset_kernel(pmd, addr);
+		set_pte_ext(pte, __pte(0), 0);
+	}
+}
+#endif
+
 #ifdef CONFIG_WRHV
 extern struct vb_config *wr_config;
 
@@ -1085,6 +1120,9 @@ static void __init devicemaps_init(struct machine_desc *mdesc)
 #else
 	wrhv_mapping();
 #endif
+#if defined(CONFIG_MV_SUPPORT_64KB_PAGE_SIZE) && defined(CONFIG_HIGHMEM)
+	map_highmem_pages();
+#endif
 
 	/*
 	 * Ask the machine support to map in the statically mapped devices.
@@ -1106,11 +1144,19 @@ static void __init kmap_init(void)
 {
 #ifdef CONFIG_HIGHMEM
 	pmd_t *pmd = pmd_off_k(PKMAP_BASE);
+#ifdef CONFIG_MV_SUPPORT_64KB_PAGE_SIZE
+	pte_t *pte = alloc_bootmem_low_pages(2 * HW_PTRS_PER_PTE * sizeof(pte_t));
+#else
 	pte_t *pte = alloc_bootmem_low_pages(2 * PTRS_PER_PTE * sizeof(pte_t));
+#endif
 	BUG_ON(!pmd_none(*pmd) || !pte);
 	__pmd_populate(pmd, __pa(pte) | _PAGE_KERNEL_TABLE);
+#ifdef CONFIG_MV_SUPPORT_64KB_PAGE_SIZE
+	pkmap_page_table = pte + HW_PTRS_PER_PTE;
+#else
 	pkmap_page_table = pte + PTRS_PER_PTE;
 #endif
+#endif
 }
 
 /*
diff --git a/arch/arm/mm/proc-macros.S b/arch/arm/mm/proc-macros.S
index d5f32ec..3b0d5d3 100644
--- a/arch/arm/mm/proc-macros.S
+++ b/arch/arm/mm/proc-macros.S
@@ -133,11 +133,26 @@
 	.macro	armv6_set_pte_ext pfx
 	str	r1, [r0], #-2048		@ linux version
 
+#ifdef CONFIG_MV_SUPPORT_64KB_PAGE_SIZE
+	mov     r3, #0x7C
+	and     r3, r3, r0
+	mov     r3, r3, lsl #4
+	bic     r0, r0, #0x3FC
+	bic     r0, r0, #0x400
+	orr     r0, r0, r3
+#endif
+
 	bic	r3, r1, #0x000003fc
+#ifdef CONFIG_MV_SUPPORT_64KB_PAGE_SIZE
+	bic	r3, r3, #0x00000F000
+#endif
 	bic	r3, r3, #PTE_TYPE_MASK
 	orr	r3, r3, r2
+#ifdef CONFIG_MV_SUPPORT_64KB_PAGE_SIZE
+	orr	r3, r3, #PTE_EXT_AP0 | 1
+#else
 	orr	r3, r3, #PTE_EXT_AP0 | 2
-
+#endif
 	adr	ip, \pfx\()_mt_table
 	and	r2, r1, #L_PTE_MT_MASK
 	ldr	r2, [ip, r2]
@@ -164,7 +179,48 @@
 	moveq	r3, #0
 
 	str	r3, [r0]
+#ifdef CONFIG_MV_SUPPORT_64KB_PAGE_SIZE
+	@ Need to duplicate the entry 16 times because of overlapping in PTE index bits.
+	str     r3, [r0, #4]
+	str     r3, [r0, #8]
+	str     r3, [r0, #12]
+	str     r3, [r0, #16]
+	str     r3, [r0, #20]
+	str     r3, [r0, #24]
+	str     r3, [r0, #28]
+	str     r3, [r0, #32]
+	str     r3, [r0, #36]
+	str     r3, [r0, #40]
+	str     r3, [r0, #44]
+	str     r3, [r0, #48]
+	str     r3, [r0, #52]
+	str     r3, [r0, #56]
+	str     r3, [r0, #60]
+#endif /* CONFIG_MV_SUPPORT_64KB_PAGE_SIZE */
+#ifdef CONFIG_SHEEVA_ERRATA_ARM_CPU_4611
+        mrs     r2, cpsr
+        orr     r3, r2, #PSR_F_BIT | PSR_I_BIT
+        msr     cpsr_c, r3                      @ Disable interrupts
+
+#if __LINUX_ARM_ARCH__ >= 7
+	dmb					@ DMB for V7
+#elif __LINUX_ARM_ARCH__ == 6
+        mcr     p15, 0, r0, c7, c10, 5          @ DMB for V6
+#endif
+#endif
+	mcr	p15, 0, r0, c7, c10, 1		@ flush_pte
+#ifdef CONFIG_MV_SUPPORT_64KB_PAGE_SIZE
+	add	r0, r0, #32
 	mcr	p15, 0, r0, c7, c10, 1		@ flush_pte
+#endif
+#if defined (CONFIG_CACHE_AURORA_L2) && !defined (CONFIG_AURORA_L2_PT_WALK)
+#error "armv6_set_pte_ext: calling l2_clean_va corrupts r2. SHOULD BE FIXED"
+	bl	l2_clean_va
+#endif
+#ifdef CONFIG_SHEEVA_ERRATA_ARM_CPU_4611
+	msr     cpsr_c, r2                      @ Restore interrupts
+	mcr     p15, 0, r0, c7, c10, 4          @ drain write buffer
+#endif	
 	.endm
 
 
diff --git a/arch/arm/oprofile/common.c b/arch/arm/oprofile/common.c
index 11c6294..c26a877 100644
--- a/arch/arm/oprofile/common.c
+++ b/arch/arm/oprofile/common.c
@@ -234,6 +234,8 @@ char *op_name_from_perf_id(void)
 		return "arm/armv7";
 	case ARM_PERF_PMU_ID_CA9:
 		return "arm/armv7-ca9";
+	case MRVL_PERF_PMU_ID_PJ4B:
+		return "arm/mrvl_pj4b";
 	default:
 		return NULL;
 	}
diff --git a/arch/arm/tools/mach-types b/arch/arm/tools/mach-types
index e744687..ed1029d 100644
--- a/arch/arm/tools/mach-types
+++ b/arch/arm/tools/mach-types
@@ -12,7 +12,7 @@
 #
 #   http://www.arm.linux.org.uk/developer/machines/?action=new
 #
-# Last update: Sat May 1 10:36:42 2010
+# Last update: Mon Jul 12 21:10:14 2010
 #
 # machine_is_xxx	CONFIG_xxxx		MACH_TYPE_xxx		number
 #
@@ -541,7 +541,7 @@ akcpmxl			MACH_AKCPMXL		AKCPMXL			524
 av3xx			MACH_AV3XX		AV3XX			525
 avila			MACH_AVILA		AVILA			526
 pxa_mpm10		MACH_PXA_MPM10		PXA_MPM10		527
-pxa_kyanite		MACH_PXA_KYANITE	PXA_KYANITE		528
+armada_xp_fpga		MACH_ARMADA_XP_FPGA	ARMADA_XP_FPGA		528
 sgold			MACH_SGOLD		SGOLD			529
 oscar			MACH_OSCAR		OSCAR			530
 epxa4usb2		MACH_EPXA4USB2		EPXA4USB2		531
@@ -2805,3 +2805,6 @@ snapper9g45		MACH_SNAPPER9G45	SNAPPER9G45		2817
 tam3517			MACH_TAM3517		TAM3517			2818
 pdc100			MACH_PDC100		PDC100			2819
 spear1310		MACH_SPEAR1310		SPEAR1310		2881
+armada_xp_db           MACH_ARMADA_XP_DB       ARMADA_XP_DB            3036
+armada_xp_rdsrv                MACH_ARMADA_XP_RDSRV    ARMADA_XP_RDSRV         3037
+armada_370_db          MACH_ARMADA_370         ARMADA_370              3038
diff --git a/arch/arm/vfp/vfpdouble.c b/arch/arm/vfp/vfpdouble.c
index 6cac43b..c2f043c 100644
--- a/arch/arm/vfp/vfpdouble.c
+++ b/arch/arm/vfp/vfpdouble.c
@@ -51,14 +51,29 @@ static void vfp_double_dump(const char *str, struct vfp_double *d)
 		 str, d->sign != 0, d->exponent, d->significand);
 }
 
-static void vfp_double_normalise_denormal(struct vfp_double *vd)
+static void vfp_double_normalise_denormal(struct vfp_double *vd, u32 fpscr, u32 *exceptions)
 {
-	int bits = 31 - fls(vd->significand >> 32);
-	if (bits == 31)
-		bits = 63 - fls(vd->significand);
+	int bits;
 
 	vfp_double_dump("normalise_denormal: in", vd);
 
+    /* If we are in Flush to zero mode, just zero the fraction */
+    if (fpscr & FPSCR_FLUSHTOZERO) {
+        vd->significand = 0ULL;
+        vfp_double_dump("normalise_denormal: Flushed to zero.", vd);
+
+        /* IDC is set whenever the FPU coprocessor is in flush-to-zero 
+         * mode and a subnormal input operand is replaced by a positive zero.
+         */
+        *exceptions |= FPSCR_IDC;
+        return;
+    }
+
+	bits = 31 - fls(((u32)(vd->significand >> 32)));
+
+	if (bits == 31)
+		bits = 63 - fls(((u32)(vd->significand)));
+	
 	if (bits) {
 		vd->exponent -= bits - 1;
 		vd->significand <<= bits;
@@ -69,7 +84,7 @@ static void vfp_double_normalise_denormal(struct vfp_double *vd)
 
 u32 vfp_double_normaliseround(int dd, struct vfp_double *vd, u32 fpscr, u32 exceptions, const char *func)
 {
-	u64 significand, incr;
+	u64 old_significand, significand, incr;
 	int exponent, shift, underflow;
 	u32 rmode;
 
@@ -92,9 +107,9 @@ u32 vfp_double_normaliseround(int dd, struct vfp_double *vd, u32 fpscr, u32 exce
 	exponent = vd->exponent;
 	significand = vd->significand;
 
-	shift = 32 - fls(significand >> 32);
+	shift = 32 - fls(((u32)(significand >> 32)));	
 	if (shift == 32)
-		shift = 64 - fls(significand);
+		shift = 64 - fls(((u32)(significand)));
 	if (shift) {
 		exponent -= shift;
 		significand <<= shift;
@@ -118,11 +133,39 @@ u32 vfp_double_normaliseround(int dd, struct vfp_double *vd, u32 fpscr, u32 exce
 		vd->significand = significand;
 		vfp_double_dump("pack: tiny number", vd);
 #endif
-		if (!(significand & ((1ULL << (VFP_DOUBLE_LOW_BITS + 1)) - 1)))
+
+	/* 
+	 * When an underflow trap is not implemented, or is not 
+	 * enabled (the default case), underflow shall be signaled only 
+	 * when both tininess and loss of accuracy have been detected. 
+	 * When an underflow trap has been implemented and is enabled, 
+	 * underflow shall be signaled when tininess is detected 
+	 * regardless of loss of accuracy.
+	 */
+		if (!(fpscr & FPSCR_UFE)) 
+            if (!(significand & ((1ULL << (VFP_DOUBLE_LOW_BITS + 1)) - 1)))
+                underflow = 0;
+
+	/* 
+	 * Exp = 0 			=> 2^-1022
+	 * significant = 0x40000000	=> 0*(2^1) + 1*(2^0) + 0*(2^-1) + 0*(2^-2) + ....
+	 * Total 			=> 2^-1022 (minimum normal number)
+	 */
+		if (significand >= 0x8000000000000000ULL) 
 			underflow = 0;
-	}
 
 	/*
+	 * In flush-to-zero mode, UFC is set whenever a result is 
+	 * below the threshold for normal numbers before rounding, and the 
+	 * result is flushed to zero.
+	 */
+		if (underflow && (fpscr & FPSCR_FLUSHTOZERO)){
+			significand = 0;
+			exceptions |= FPSCR_UFC;
+			pr_debug("VFP: Flushed to zero.");
+		}
+	}
+	/*
 	 * Select rounding increment.
 	 */
 	incr = 0;
@@ -163,6 +206,7 @@ u32 vfp_double_normaliseround(int dd, struct vfp_double *vd, u32 fpscr, u32 exce
 	/*
 	 * Do our rounding.
 	 */
+	old_significand = significand;
 	significand += incr;
 
 	/*
@@ -180,8 +224,6 @@ u32 vfp_double_normaliseround(int dd, struct vfp_double *vd, u32 fpscr, u32 exce
 	} else {
 		if (significand >> (VFP_DOUBLE_LOW_BITS + 1) == 0)
 			exponent = 0;
-		if (exponent || significand > 0x8000000000000000ULL)
-			underflow = 0;
 		if (underflow)
 			exceptions |= FPSCR_UFC;
 		vd->exponent = exponent;
@@ -194,7 +236,11 @@ u32 vfp_double_normaliseround(int dd, struct vfp_double *vd, u32 fpscr, u32 exce
 		s64 d = vfp_double_pack(vd);
 		pr_debug("VFP: %s: d(d%d)=%016llx exceptions=%08x\n", func,
 			 dd, d, exceptions);
-		vfp_put_double(d, dd);
+		if (-1 != dd){	
+			vfp_put_double(d, dd);
+		} else {
+			vfp_double_unpack(vd, d);
+		}
 	}
 	return exceptions;
 }
@@ -268,26 +314,30 @@ static u32 vfp_double_fneg(int dd, int unused, int dm, u32 fpscr)
 static u32 vfp_double_fsqrt(int dd, int unused, int dm, u32 fpscr)
 {
 	struct vfp_double vdm, vdd;
-	int ret, tm;
+	int tm;
+	u32 exceptions = 0;
 
 	vfp_double_unpack(&vdm, vfp_get_double(dm));
-	tm = vfp_double_type(&vdm);
+
+	if (vdm.exponent == 0 && vdm.significand)
+		vfp_double_normalise_denormal(&vdm, fpscr, &exceptions);
+
+    tm = vfp_double_type(&vdm);
 	if (tm & (VFP_NAN|VFP_INFINITY)) {
 		struct vfp_double *vdp = &vdd;
 
 		if (tm & VFP_NAN)
-			ret = vfp_propagate_nan(vdp, &vdm, NULL, fpscr);
+			exceptions |= vfp_propagate_nan(vdp, &vdm, NULL, fpscr);
 		else if (vdm.sign == 0) {
  sqrt_copy:
 			vdp = &vdm;
-			ret = 0;
 		} else {
  sqrt_invalid:
 			vdp = &vfp_double_default_qnan;
-			ret = FPSCR_IOC;
+			exceptions |= FPSCR_IOC;
 		}
 		vfp_put_double(vfp_double_pack(vdp), dd);
-		return ret;
+		return exceptions;
 	}
 
 	/*
@@ -297,12 +347,6 @@ static u32 vfp_double_fsqrt(int dd, int unused, int dm, u32 fpscr)
 		goto sqrt_copy;
 
 	/*
-	 * Normalise a denormalised number
-	 */
-	if (tm & VFP_DENORMAL)
-		vfp_double_normalise_denormal(&vdm);
-
-	/*
 	 * sqrt(<0) = invalid
 	 */
 	if (vdm.sign)
@@ -380,6 +424,11 @@ static u32 vfp_compare(int dd, int signal_on_qnan, int dm, u32 fpscr)
 			ret |= FPSCR_IOC;
 	}
 
+	if (fpscr & FPSCR_FLUSHTOZERO) 
+		if ((vfp_double_packed_exponent(m) == 0 && vfp_double_packed_mantissa(m)) || 
+		    (vfp_double_packed_exponent(d) == 0 && vfp_double_packed_mantissa(d)))
+			ret |= FPSCR_IDC;
+
 	if (ret == 0) {
 		if (d == m || vfp_double_packed_abs(d | m) == 0) {
 			/*
@@ -412,7 +461,6 @@ static u32 vfp_compare(int dd, int signal_on_qnan, int dm, u32 fpscr)
 			ret |= FPSCR_C;
 		}
 	}
-
 	return ret;
 }
 
@@ -451,10 +499,10 @@ static u32 vfp_double_fcvts(int sd, int unused, int dm, u32 fpscr)
 	 * If we have a signalling NaN, signal invalid operation.
 	 */
 	if (tm == VFP_SNAN)
-		exceptions = FPSCR_IOC;
+		exceptions |= FPSCR_IOC;
 
 	if (tm & VFP_DENORMAL)
-		vfp_double_normalise_denormal(&vdm);
+		vfp_double_normalise_denormal(&vdm, fpscr, &exceptions);
 
 	vsd.sign = vdm.sign;
 	vsd.significand = vfp_hi64to32jamming(vdm.significand);
@@ -511,20 +559,21 @@ static u32 vfp_double_ftoui(int sd, int unused, int dm, u32 fpscr)
 	int tm;
 
 	vfp_double_unpack(&vdm, vfp_get_double(dm));
-
+	vfp_double_dump("VDM", &vdm);
+	
 	/*
 	 * Do we have a denormalised number?
 	 */
 	tm = vfp_double_type(&vdm);
 	if (tm & VFP_DENORMAL)
-		exceptions |= FPSCR_IDC;
+		vfp_double_normalise_denormal(&vdm, fpscr, &exceptions);
 
 	if (tm & VFP_NAN)
 		vdm.sign = 0;
 
 	if (vdm.exponent >= 1023 + 32) {
 		d = vdm.sign ? 0 : 0xffffffff;
-		exceptions = FPSCR_IOC;
+		exceptions |= FPSCR_IOC;
 	} else if (vdm.exponent >= 1023 - 1) {
 		int shift = 1023 + 63 - vdm.exponent;
 		u64 rem, incr = 0;
@@ -548,8 +597,10 @@ static u32 vfp_double_ftoui(int sd, int unused, int dm, u32 fpscr)
 		if ((rem + incr) < rem) {
 			if (d < 0xffffffff)
 				d += 1;
-			else
+			else{
 				exceptions |= FPSCR_IOC;
+				rem = 0; /* Make sure IXC will not raise */
+			}
 		}
 
 		if (d && vdm.sign) {
@@ -560,13 +611,14 @@ static u32 vfp_double_ftoui(int sd, int unused, int dm, u32 fpscr)
 	} else {
 		d = 0;
 		if (vdm.exponent | vdm.significand) {
-			exceptions |= FPSCR_IXC;
-			if (rmode == FPSCR_ROUND_PLUSINF && vdm.sign == 0)
-				d = 1;
-			else if (rmode == FPSCR_ROUND_MINUSINF && vdm.sign) {
-				d = 0;
+			if (rmode == FPSCR_ROUND_MINUSINF && vdm.sign) {
 				exceptions |= FPSCR_IOC;
 			}
+			else {
+				exceptions |= FPSCR_IXC; 
+				if (rmode == FPSCR_ROUND_PLUSINF && vdm.sign == 0)
+					d = 1;
+			}
 		}
 	}
 
@@ -579,7 +631,7 @@ static u32 vfp_double_ftoui(int sd, int unused, int dm, u32 fpscr)
 
 static u32 vfp_double_ftouiz(int sd, int unused, int dm, u32 fpscr)
 {
-	return vfp_double_ftoui(sd, unused, dm, FPSCR_ROUND_TOZERO);
+	return vfp_double_ftoui(sd, unused, dm, (fpscr | FPSCR_ROUND_TOZERO));
 }
 
 static u32 vfp_double_ftosi(int sd, int unused, int dm, u32 fpscr)
@@ -597,7 +649,7 @@ static u32 vfp_double_ftosi(int sd, int unused, int dm, u32 fpscr)
 	 */
 	tm = vfp_double_type(&vdm);
 	if (tm & VFP_DENORMAL)
-		exceptions |= FPSCR_IDC;
+		vfp_double_normalise_denormal(&vdm, fpscr, &exceptions);
 
 	if (tm & VFP_NAN) {
 		d = 0;
@@ -654,7 +706,7 @@ static u32 vfp_double_ftosi(int sd, int unused, int dm, u32 fpscr)
 
 static u32 vfp_double_ftosiz(int dd, int unused, int dm, u32 fpscr)
 {
-	return vfp_double_ftosi(dd, unused, dm, FPSCR_ROUND_TOZERO);
+	return vfp_double_ftosi(dd, unused, dm, (fpscr | FPSCR_ROUND_TOZERO));
 }
 
 
@@ -668,8 +720,8 @@ static struct op fops_ext[32] = {
 	[FEXT_TO_IDX(FEXT_FCMPZ)]	= { vfp_double_fcmpz,  OP_SCALAR },
 	[FEXT_TO_IDX(FEXT_FCMPEZ)]	= { vfp_double_fcmpez, OP_SCALAR },
 	[FEXT_TO_IDX(FEXT_FCVT)]	= { vfp_double_fcvts,  OP_SCALAR|OP_SD },
-	[FEXT_TO_IDX(FEXT_FUITO)]	= { vfp_double_fuito,  OP_SCALAR|OP_SM },
-	[FEXT_TO_IDX(FEXT_FSITO)]	= { vfp_double_fsito,  OP_SCALAR|OP_SM },
+	[FEXT_TO_IDX(FEXT_FUITO)]	= { vfp_double_fuito,  OP_SCALAR },
+	[FEXT_TO_IDX(FEXT_FSITO)]	= { vfp_double_fsito,  OP_SCALAR },
 	[FEXT_TO_IDX(FEXT_FTOUI)]	= { vfp_double_ftoui,  OP_SCALAR|OP_SD },
 	[FEXT_TO_IDX(FEXT_FTOUIZ)]	= { vfp_double_ftouiz, OP_SCALAR|OP_SD },
 	[FEXT_TO_IDX(FEXT_FTOSI)]	= { vfp_double_ftosi,  OP_SCALAR|OP_SD },
@@ -687,6 +739,11 @@ vfp_double_fadd_nonnumber(struct vfp_double *vdd, struct vfp_double *vdn,
 	u32 exceptions = 0;
 	int tn, tm;
 
+	pr_debug("VFP: in fadd_nonnumber\n");
+
+	vfp_double_dump("VDN", vdn);
+	vfp_double_dump("VDM", vdm);
+
 	tn = vfp_double_type(vdn);
 	tm = vfp_double_type(vdm);
 
@@ -698,7 +755,7 @@ vfp_double_fadd_nonnumber(struct vfp_double *vdd, struct vfp_double *vdn,
 			/*
 			 * different signs -> invalid
 			 */
-			exceptions = FPSCR_IOC;
+			exceptions |= FPSCR_IOC;
 			vdp = &vfp_double_default_qnan;
 		} else {
 			/*
@@ -744,6 +801,7 @@ vfp_double_add(struct vfp_double *vdd, struct vfp_double *vdn,
 		struct vfp_double *t = vdn;
 		vdn = vdm;
 		vdm = t;
+		pr_debug("VFP: swapping M <-> N\n");
 	}
 
 	/*
@@ -851,27 +909,36 @@ static u32
 vfp_double_multiply_accumulate(int dd, int dn, int dm, u32 fpscr, u32 negate, char *func)
 {
 	struct vfp_double vdd, vdp, vdn, vdm;
-	u32 exceptions;
+	u32 exceptions = 0;
 
 	vfp_double_unpack(&vdn, vfp_get_double(dn));
 	if (vdn.exponent == 0 && vdn.significand)
-		vfp_double_normalise_denormal(&vdn);
+		vfp_double_normalise_denormal(&vdn, fpscr, &exceptions);
 
 	vfp_double_unpack(&vdm, vfp_get_double(dm));
 	if (vdm.exponent == 0 && vdm.significand)
-		vfp_double_normalise_denormal(&vdm);
+		vfp_double_normalise_denormal(&vdm, fpscr, &exceptions);
+
+	exceptions |= vfp_double_multiply(&vdp, &vdn, &vdm, fpscr);
+	exceptions |= vfp_double_normaliseround(-1, &vdp, fpscr, exceptions, "fmac-mul");
+
+	if (vdp.exponent == 0 && vdp.significand)
+		vfp_double_normalise_denormal(&vdp, fpscr, &exceptions);
 
-	exceptions = vfp_double_multiply(&vdp, &vdn, &vdm, fpscr);
 	if (negate & NEG_MULTIPLY)
 		vdp.sign = vfp_sign_negate(vdp.sign);
 
 	vfp_double_unpack(&vdn, vfp_get_double(dd));
+
+	if (vdn.exponent == 0 && vdn.significand)
+		vfp_double_normalise_denormal(&vdn, fpscr, &exceptions);
+
 	if (negate & NEG_SUBTRACT)
 		vdn.sign = vfp_sign_negate(vdn.sign);
 
 	exceptions |= vfp_double_add(&vdd, &vdn, &vdp, fpscr);
 
-	return vfp_double_normaliseround(dd, &vdd, fpscr, exceptions, func);
+	return vfp_double_normaliseround(dd, &vdd, fpscr, exceptions, "fmac-add");
 }
 
 /*
@@ -916,17 +983,17 @@ static u32 vfp_double_fnmsc(int dd, int dn, int dm, u32 fpscr)
 static u32 vfp_double_fmul(int dd, int dn, int dm, u32 fpscr)
 {
 	struct vfp_double vdd, vdn, vdm;
-	u32 exceptions;
+	u32 exceptions = 0;
 
 	vfp_double_unpack(&vdn, vfp_get_double(dn));
 	if (vdn.exponent == 0 && vdn.significand)
-		vfp_double_normalise_denormal(&vdn);
+		vfp_double_normalise_denormal(&vdn, fpscr, &exceptions);
 
 	vfp_double_unpack(&vdm, vfp_get_double(dm));
 	if (vdm.exponent == 0 && vdm.significand)
-		vfp_double_normalise_denormal(&vdm);
+		vfp_double_normalise_denormal(&vdm, fpscr, &exceptions);
 
-	exceptions = vfp_double_multiply(&vdd, &vdn, &vdm, fpscr);
+	exceptions |= vfp_double_multiply(&vdd, &vdn, &vdm, fpscr);
 	return vfp_double_normaliseround(dd, &vdd, fpscr, exceptions, "fmul");
 }
 
@@ -936,19 +1003,23 @@ static u32 vfp_double_fmul(int dd, int dn, int dm, u32 fpscr)
 static u32 vfp_double_fnmul(int dd, int dn, int dm, u32 fpscr)
 {
 	struct vfp_double vdd, vdn, vdm;
-	u32 exceptions;
+	u32 exceptions = 0;
 
 	vfp_double_unpack(&vdn, vfp_get_double(dn));
 	if (vdn.exponent == 0 && vdn.significand)
-		vfp_double_normalise_denormal(&vdn);
+		vfp_double_normalise_denormal(&vdn, fpscr, &exceptions);
 
 	vfp_double_unpack(&vdm, vfp_get_double(dm));
 	if (vdm.exponent == 0 && vdm.significand)
-		vfp_double_normalise_denormal(&vdm);
+		vfp_double_normalise_denormal(&vdm, fpscr, &exceptions);
 
-	exceptions = vfp_double_multiply(&vdd, &vdn, &vdm, fpscr);
-	vdd.sign = vfp_sign_negate(vdd.sign);
+	exceptions |= vfp_double_multiply(&vdd, &vdn, &vdm, fpscr);
+	exceptions |= vfp_double_normaliseround(-1, &vdd, fpscr, exceptions, "fnmul-mul");
+	
+	if (vdd.exponent == 0 && vdd.significand)
+		vfp_double_normalise_denormal(&vdd, fpscr, &exceptions);
 
+	vdd.sign = vfp_sign_negate(vdd.sign);
 	return vfp_double_normaliseround(dd, &vdd, fpscr, exceptions, "fnmul");
 }
 
@@ -958,17 +1029,25 @@ static u32 vfp_double_fnmul(int dd, int dn, int dm, u32 fpscr)
 static u32 vfp_double_fadd(int dd, int dn, int dm, u32 fpscr)
 {
 	struct vfp_double vdd, vdn, vdm;
-	u32 exceptions;
+	u32 exceptions = 0;
+	pr_debug("VFP: fadd: dn %d\n", dn);
+	pr_debug("VFP: fadd: dm %d\n", dm);
+
+	pr_debug("VFP: fadd: dn=%016llx\n", vfp_get_double(dn));
+	pr_debug("VFP: fadd: dm=%016llx\n", vfp_get_double(dm));
 
 	vfp_double_unpack(&vdn, vfp_get_double(dn));
 	if (vdn.exponent == 0 && vdn.significand)
-		vfp_double_normalise_denormal(&vdn);
+		vfp_double_normalise_denormal(&vdn, fpscr, &exceptions);
 
 	vfp_double_unpack(&vdm, vfp_get_double(dm));
 	if (vdm.exponent == 0 && vdm.significand)
-		vfp_double_normalise_denormal(&vdm);
+		vfp_double_normalise_denormal(&vdm, fpscr, &exceptions);
 
-	exceptions = vfp_double_add(&vdd, &vdn, &vdm, fpscr);
+	vfp_double_dump("VDN", &vdn);
+	vfp_double_dump("VDM", &vdm);
+
+	exceptions |= vfp_double_add(&vdd, &vdn, &vdm, fpscr);
 
 	return vfp_double_normaliseround(dd, &vdd, fpscr, exceptions, "fadd");
 }
@@ -979,24 +1058,47 @@ static u32 vfp_double_fadd(int dd, int dn, int dm, u32 fpscr)
 static u32 vfp_double_fsub(int dd, int dn, int dm, u32 fpscr)
 {
 	struct vfp_double vdd, vdn, vdm;
-	u32 exceptions;
+	u32 exceptions = 0;
+	int tn, tm;
 
-	vfp_double_unpack(&vdn, vfp_get_double(dn));
+	pr_debug("VFP: fsub: dd=%d, dn=%d, dm=%d\n",dd, dn, dm);
+	pr_debug("VFP: fsub: dn=%016llx\n", vfp_get_double(dn));
+	pr_debug("VFP: fsub: dm=%016llx\n", vfp_get_double(dm));
+
+	/*
+	 * Subtraction is like addition, but with a negated operand.
+	 * Problem is when you use same register as source operands. 
+	 * For example fsub d6, d7, d7. In that case negate d7  
+	 * operand will result wrong value...
+	 */
+  	vfp_double_unpack(&vdn, vfp_get_double(dn));
 	if (vdn.exponent == 0 && vdn.significand)
-		vfp_double_normalise_denormal(&vdn);
+		vfp_double_normalise_denormal(&vdn, fpscr, &exceptions);
 
 	vfp_double_unpack(&vdm, vfp_get_double(dm));
 	if (vdm.exponent == 0 && vdm.significand)
-		vfp_double_normalise_denormal(&vdm);
+		vfp_double_normalise_denormal(&vdm, fpscr, &exceptions);
+
+	vfp_double_dump("VDN", &vdn);
+	vfp_double_dump("VDM", &vdm);
 
 	/*
-	 * Subtraction is like addition, but with a negated operand.
+	 * Is either of parameters is NaN do not negate their sign.
+	 * In that case result is based on the input NaN.
 	 */
-	vdm.sign = vfp_sign_negate(vdm.sign);
+	tn = vfp_double_type(&vdn);
+	tm = vfp_double_type(&vdm);
 
-	exceptions = vfp_double_add(&vdd, &vdn, &vdm, fpscr);
+	if (!((tn & (VFP_QNAN | VFP_SNAN)) || (tm & (VFP_QNAN | VFP_SNAN))))
+		/* Negate dm value */
+		vdm.sign = vfp_sign_negate(vdm.sign);
+	else
+		pr_debug("VFP: SUB canceled minus signe. One of parameters is NaN tn=0x%x tm=0x%x\n", tn, tm);
+       
+	exceptions |= vfp_double_add(&vdd, &vdn, &vdm, fpscr);
 
 	return vfp_double_normaliseround(dd, &vdd, fpscr, exceptions, "fsub");
+
 }
 
 /*
@@ -1011,6 +1113,11 @@ static u32 vfp_double_fdiv(int dd, int dn, int dm, u32 fpscr)
 	vfp_double_unpack(&vdn, vfp_get_double(dn));
 	vfp_double_unpack(&vdm, vfp_get_double(dm));
 
+	if (vdn.exponent == 0 && vdn.significand)
+		vfp_double_normalise_denormal(&vdn, fpscr, &exceptions);
+	if (vdm.exponent == 0 && vdm.significand)
+		vfp_double_normalise_denormal(&vdm, fpscr, &exceptions);
+
 	vdd.sign = vdn.sign ^ vdm.sign;
 
 	tn = vfp_double_type(&vdn);
@@ -1053,11 +1160,6 @@ static u32 vfp_double_fdiv(int dd, int dn, int dm, u32 fpscr)
 	if (tm & VFP_INFINITY || tn & VFP_ZERO)
 		goto zero;
 
-	if (tn & VFP_DENORMAL)
-		vfp_double_normalise_denormal(&vdn);
-	if (tm & VFP_DENORMAL)
-		vfp_double_normalise_denormal(&vdm);
-
 	/*
 	 * Ok, we have two numbers, we can perform division.
 	 */
@@ -1081,13 +1183,13 @@ static u32 vfp_double_fdiv(int dd, int dn, int dm, u32 fpscr)
 	return vfp_double_normaliseround(dd, &vdd, fpscr, 0, "fdiv");
 
  vdn_nan:
-	exceptions = vfp_propagate_nan(&vdd, &vdn, &vdm, fpscr);
+	exceptions |= vfp_propagate_nan(&vdd, &vdn, &vdm, fpscr);
  pack:
 	vfp_put_double(vfp_double_pack(&vdd), dd);
 	return exceptions;
 
  vdm_nan:
-	exceptions = vfp_propagate_nan(&vdd, &vdm, &vdn, fpscr);
+	exceptions |= vfp_propagate_nan(&vdd, &vdm, &vdn, fpscr);
 	goto pack;
 
  zero:
@@ -1096,7 +1198,7 @@ static u32 vfp_double_fdiv(int dd, int dn, int dm, u32 fpscr)
 	goto pack;
 
  divzero:
-	exceptions = FPSCR_DZC;
+	exceptions |= FPSCR_DZC;
  infinity:
 	vdd.exponent = 2047;
 	vdd.significand = 0;
@@ -1104,7 +1206,7 @@ static u32 vfp_double_fdiv(int dd, int dn, int dm, u32 fpscr)
 
  invalid:
 	vfp_put_double(vfp_double_pack(&vfp_double_default_qnan), dd);
-	return FPSCR_IOC;
+    return (exceptions | FPSCR_IOC);
 }
 
 static struct op fops[16] = {
@@ -1128,17 +1230,18 @@ u32 vfp_double_cpdo(u32 inst, u32 fpscr)
 	u32 exceptions = 0;
 	unsigned int dest;
 	unsigned int dn = vfp_get_dn(inst);
-	unsigned int dm;
+	unsigned int dm = vfp_get_dm(inst);
 	unsigned int vecitr, veclen, vecstride;
 	struct op *fop;
 
-	vecstride = (1 + ((fpscr & FPSCR_STRIDE_MASK) == FPSCR_STRIDE_MASK));
+	vecstride = (1 + ((fpscr & FPSCR_STRIDE_MASK) == FPSCR_STRIDE_MASK)) * 2;
 
 	fop = (op == FOP_EXT) ? &fops_ext[FEXT_TO_IDX(inst)] : &fops[FOP_TO_IDX(op)];
 
 	/*
-	 * fcvtds takes an sN register number as destination, not dN.
-	 * It also always operates on scalars.
+	 * fcvtds, ftosid, ftosizd, ftouid and ftouizd takes an sN 
+	 * register number as destination, not dN. It also always 
+         * operates on scalars.
 	 */
 	if (fop->flags & OP_SD)
 		dest = vfp_get_sd(inst);
@@ -1146,14 +1249,6 @@ u32 vfp_double_cpdo(u32 inst, u32 fpscr)
 		dest = vfp_get_dd(inst);
 
 	/*
-	 * f[us]ito takes a sN operand, not a dN operand.
-	 */
-	if (fop->flags & OP_SM)
-		dm = vfp_get_sm(inst);
-	else
-		dm = vfp_get_dm(inst);
-
-	/*
 	 * If destination bank is zero, vector length is always '1'.
 	 * ARM DDI0100F C5.1.3, C5.3.2.
 	 */
@@ -1192,10 +1287,10 @@ u32 vfp_double_cpdo(u32 inst, u32 fpscr)
 		 * CHECK: It appears to be undefined whether we stop when
 		 * we encounter an exception.  We continue.
 		 */
-		dest = FREG_BANK(dest) + ((FREG_IDX(dest) + vecstride) & 3);
-		dn = FREG_BANK(dn) + ((FREG_IDX(dn) + vecstride) & 3);
+		dest = FREG_BANK(dest) + ((FREG_IDX(dest) + vecstride) & 6);
+		dn = FREG_BANK(dn) + ((FREG_IDX(dn) + vecstride) & 6);
 		if (FREG_BANK(dm) != 0)
-			dm = FREG_BANK(dm) + ((FREG_IDX(dm) + vecstride) & 3);
+			dm = FREG_BANK(dm) + ((FREG_IDX(dm) + vecstride) & 6);
 	}
 	return exceptions;
 
diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 11cb869..78fb07a 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -240,6 +240,7 @@ static void vfp_raise_exceptions(u32 exceptions, u32 inst, u32 fpscr, struct pt_
 	RAISE(FPSCR_UFC, FPSCR_UFE, FPE_FLTUND);
 	RAISE(FPSCR_OFC, FPSCR_OFE, FPE_FLTOVF);
 	RAISE(FPSCR_IOC, FPSCR_IOE, FPE_FLTINV);
+        RAISE(FPSCR_IDC, FPSCR_IDE, FPE_FLTISN);
 
 	if (si_code)
 		vfp_raise_sigfpe(si_code, regs);
@@ -288,6 +289,10 @@ void VFP_bounce(u32 trigger, u32 fpexc, struct pt_regs *regs)
 {
 	u32 fpscr, orig_fpscr, fpsid, exceptions;
 
+	/* Adjust saved PC for thumb-2 user program */
+	if (regs->ARM_cpsr & PSR_T_BIT)
+		regs->ARM_pc += 2;
+
 	pr_debug("VFP: bounce: trigger %08x fpexc %08x\n", trigger, fpexc);
 
 	/*
@@ -393,7 +398,7 @@ static void vfp_enable(void *unused)
 	set_copro_access(access | CPACC_FULL(10) | CPACC_FULL(11));
 }
 
-#ifdef CONFIG_PM
+#if defined(CONFIG_PM) || defined(CONFIG_CPU_IDLE)
 #include <linux/sysdev.h>
 
 static int vfp_pm_suspend(struct sys_device *dev, pm_message_t state)
@@ -427,6 +432,25 @@ static int vfp_pm_resume(struct sys_device *dev)
 	return 0;
 }
 
+void vfp_save(void)
+{
+        struct pm_message temp;
+        /*
+         * if VFP was not initialized yet, then do nothing
+         */
+        if (VFP_arch)
+                vfp_pm_suspend(NULL , temp);
+}
+
+void vfp_restore(void)
+{
+        if (VFP_arch)
+                vfp_pm_resume(NULL);
+}
+
+#endif
+
+#ifdef CONFIG_PM
 static struct sysdev_class vfp_pm_sysclass = {
 	.name		= "vfp",
 	.suspend	= vfp_pm_suspend,
diff --git a/arch/arm/vfp/vfpsingle.c b/arch/arm/vfp/vfpsingle.c
index b252631..7b6bff9 100644
--- a/arch/arm/vfp/vfpsingle.c
+++ b/arch/arm/vfp/vfpsingle.c
@@ -51,11 +51,26 @@ static void vfp_single_dump(const char *str, struct vfp_single *s)
 		 str, s->sign != 0, s->exponent, s->significand);
 }
 
-static void vfp_single_normalise_denormal(struct vfp_single *vs)
+static void vfp_single_normalise_denormal(struct vfp_single *vs, u32 fpscr, u32 *exceptions)
 {
-	int bits = 31 - fls(vs->significand);
+	int bits;
 
 	vfp_single_dump("normalise_denormal: in", vs);
+	
+	pr_debug("VFP: fpscr =%08x\n", fpscr);
+	/* If we are in Flush to zero mode, just zero the fraction */
+	if (fpscr & FPSCR_FLUSHTOZERO) {
+		vs->significand = 0;
+		vfp_single_dump("normalise_denormal: Flushed to zero.", vs);
+		
+		/* IDC is set whenever the FPU coprocessor is in flush-to-zero 
+		 * mode and a subnormal input operand is replaced by a positive zero.
+		 */
+		*exceptions |= FPSCR_IDC;
+		return;
+	}
+
+	bits = 31 - fls(vs->significand);
 
 	if (bits) {
 		vs->exponent -= bits - 1;
@@ -72,7 +87,7 @@ u32 __vfp_single_normaliseround(int sd, struct vfp_single *vs, u32 fpscr, u32 ex
 u32 vfp_single_normaliseround(int sd, struct vfp_single *vs, u32 fpscr, u32 exceptions, const char *func)
 #endif
 {
-	u32 significand, incr, rmode;
+	u32 old_significand, significand, incr, rmode;
 	int exponent, shift, underflow;
 
 	vfp_single_dump("pack: in", vs);
@@ -118,13 +133,42 @@ u32 vfp_single_normaliseround(int sd, struct vfp_single *vs, u32 fpscr, u32 exce
 	if (underflow) {
 		significand = vfp_shiftright32jamming(significand, -exponent);
 		exponent = 0;
+
 #ifdef DEBUG
 		vs->exponent = exponent;
 		vs->significand = significand;
 		vfp_single_dump("pack: tiny number", vs);
 #endif
-		if (!(significand & ((1 << (VFP_SINGLE_LOW_BITS + 1)) - 1)))
+	/* 
+	 * When an underflow trap is not implemented, or is not 
+	 * enabled (the default case), underflow shall be signaled only 
+	 * when both tininess and loss of accuracy have been detected. 
+	 * When an underflow trap has been implemented and is enabled, 
+	 * underflow shall be signaled when tininess is detected 
+	 * regardless of loss of accuracy.
+	 */
+		if (!(fpscr & FPSCR_UFE)) 
+			if (!(significand & ((1 << (VFP_SINGLE_LOW_BITS + 1)) - 1)))
+				underflow = 0;
+
+	/* 
+	 * Exp = 0 			=> 2^-126
+	 * significant = 0x40000000	=> 0*(2^1) + 1*(2^0) + 0*(2^-1) + 0*(2^-2) + ....
+	 * Total 			=> 2^-126 (minimum normal number)
+	 */
+		if (significand >= 0x80000000) 
 			underflow = 0;
+
+	/*
+	 * In flush-to-zero mode, UFC is set whenever a result is 
+	 * below the threshold for normal numbers before rounding, and the 
+	 * result is flushed to zero.
+	 */
+		if (underflow && (fpscr & FPSCR_FLUSHTOZERO)){
+			significand = 0;
+			exceptions |= FPSCR_UFC;
+			pr_debug("VFP: Flushed to zero.");
+		}
 	}
 
 	/*
@@ -168,6 +212,7 @@ u32 vfp_single_normaliseround(int sd, struct vfp_single *vs, u32 fpscr, u32 exce
 	/*
 	 * Do our rounding.
 	 */
+	old_significand = significand;
 	significand += incr;
 
 	/*
@@ -185,8 +230,6 @@ u32 vfp_single_normaliseround(int sd, struct vfp_single *vs, u32 fpscr, u32 exce
 	} else {
 		if (significand >> (VFP_SINGLE_LOW_BITS + 1) == 0)
 			exponent = 0;
-		if (exponent || significand > 0x80000000)
-			underflow = 0;
 		if (underflow)
 			exceptions |= FPSCR_UFC;
 		vs->exponent = exponent;
@@ -201,7 +244,11 @@ u32 vfp_single_normaliseround(int sd, struct vfp_single *vs, u32 fpscr, u32 exce
 		pr_debug("VFP: %s: d(s%d)=%08x exceptions=%08x\n", func,
 			 sd, d, exceptions);
 #endif
-		vfp_put_float(d, sd);
+		if (-1 != sd){
+			vfp_put_float(d, sd);
+		} else {
+			vfp_single_unpack(vs, d);
+		}
 	}
 
 	return exceptions;
@@ -316,28 +363,34 @@ u32 vfp_estimate_sqrt_significand(u32 exponent, u32 significand)
 static u32 vfp_single_fsqrt(int sd, int unused, s32 m, u32 fpscr)
 {
 	struct vfp_single vsm, vsd;
-	int ret, tm;
+	int tm;
+	u32 exceptions = 0;
 
 	vfp_single_unpack(&vsm, m);
+
+	if (vsm.exponent == 0 && vsm.significand)
+		vfp_single_normalise_denormal(&vsm, fpscr, &exceptions);
+
+
 	tm = vfp_single_type(&vsm);
 	if (tm & (VFP_NAN|VFP_INFINITY)) {
 		struct vfp_single *vsp = &vsd;
 
 		if (tm & VFP_NAN)
-			ret = vfp_propagate_nan(vsp, &vsm, NULL, fpscr);
+			exceptions |= vfp_propagate_nan(vsp, &vsm, NULL, fpscr);
 		else if (vsm.sign == 0) {
  sqrt_copy:
 			vsp = &vsm;
-			ret = 0;
 		} else {
  sqrt_invalid:
 			vsp = &vfp_single_default_qnan;
-			ret = FPSCR_IOC;
+			exceptions |= FPSCR_IOC;
 		}
 		vfp_put_float(vfp_single_pack(vsp), sd);
-		return ret;
+		return exceptions;
 	}
 
+
 	/*
 	 * sqrt(+/- 0) == +/- 0
 	 */
@@ -345,12 +398,6 @@ static u32 vfp_single_fsqrt(int sd, int unused, s32 m, u32 fpscr)
 		goto sqrt_copy;
 
 	/*
-	 * Normalise a denormalised number
-	 */
-	if (tm & VFP_DENORMAL)
-		vfp_single_normalise_denormal(&vsm);
-
-	/*
 	 * sqrt(<0) = invalid
 	 */
 	if (vsm.sign)
@@ -391,7 +438,7 @@ static u32 vfp_single_fsqrt(int sd, int unused, s32 m, u32 fpscr)
 	}
 	vsd.significand = vfp_shiftright32jamming(vsd.significand, 1);
 
-	return vfp_single_normaliseround(sd, &vsd, fpscr, 0, "fsqrt");
+	return vfp_single_normaliseround(sd, &vsd, fpscr, exceptions, "fsqrt");
 }
 
 /*
@@ -424,6 +471,11 @@ static u32 vfp_compare(int sd, int signal_on_qnan, s32 m, u32 fpscr)
 			ret |= FPSCR_IOC;
 	}
 
+	if (fpscr & FPSCR_FLUSHTOZERO) 
+		if ((vfp_single_packed_exponent(m) == 0 && vfp_single_packed_mantissa(m)) || 
+		    (vfp_single_packed_exponent(d) == 0 && vfp_single_packed_mantissa(d)))
+			ret |= FPSCR_IDC;
+
 	if (ret == 0) {
 		if (d == m || vfp_single_packed_abs(d | m) == 0) {
 			/*
@@ -497,7 +549,7 @@ static u32 vfp_single_fcvtd(int dd, int unused, s32 m, u32 fpscr)
 		exceptions = FPSCR_IOC;
 
 	if (tm & VFP_DENORMAL)
-		vfp_single_normalise_denormal(&vsm);
+		vfp_single_normalise_denormal(&vsm, fpscr, &exceptions);
 
 	vdd.sign = vsm.sign;
 	vdd.significand = (u64)vsm.significand << 32;
@@ -559,7 +611,7 @@ static u32 vfp_single_ftoui(int sd, int unused, s32 m, u32 fpscr)
 	 */
 	tm = vfp_single_type(&vsm);
 	if (tm & VFP_DENORMAL)
-		exceptions |= FPSCR_IDC;
+		vfp_single_normalise_denormal(&vsm, fpscr, &exceptions);
 
 	if (tm & VFP_NAN)
 		vsm.sign = 0;
@@ -602,13 +654,15 @@ static u32 vfp_single_ftoui(int sd, int unused, s32 m, u32 fpscr)
 	} else {
 		d = 0;
 		if (vsm.exponent | vsm.significand) {
-			exceptions |= FPSCR_IXC;
-			if (rmode == FPSCR_ROUND_PLUSINF && vsm.sign == 0)
-				d = 1;
-			else if (rmode == FPSCR_ROUND_MINUSINF && vsm.sign) {
+			if (rmode == FPSCR_ROUND_MINUSINF && vsm.sign) {
 				d = 0;
 				exceptions |= FPSCR_IOC;
 			}
+			else {
+				exceptions |= FPSCR_IXC; 
+				if (rmode == FPSCR_ROUND_PLUSINF && vsm.sign == 0)
+					d = 1;
+			}
 		}
 	}
 
@@ -621,7 +675,7 @@ static u32 vfp_single_ftoui(int sd, int unused, s32 m, u32 fpscr)
 
 static u32 vfp_single_ftouiz(int sd, int unused, s32 m, u32 fpscr)
 {
-	return vfp_single_ftoui(sd, unused, m, FPSCR_ROUND_TOZERO);
+	return vfp_single_ftoui(sd, unused, m, (fpscr | FPSCR_ROUND_TOZERO));
 }
 
 static u32 vfp_single_ftosi(int sd, int unused, s32 m, u32 fpscr)
@@ -639,7 +693,7 @@ static u32 vfp_single_ftosi(int sd, int unused, s32 m, u32 fpscr)
 	 */
 	tm = vfp_single_type(&vsm);
 	if (vfp_single_type(&vsm) & VFP_DENORMAL)
-		exceptions |= FPSCR_IDC;
+		vfp_single_normalise_denormal(&vsm, fpscr, &exceptions);
 
 	if (tm & VFP_NAN) {
 		d = 0;
@@ -700,7 +754,7 @@ static u32 vfp_single_ftosi(int sd, int unused, s32 m, u32 fpscr)
 
 static u32 vfp_single_ftosiz(int sd, int unused, s32 m, u32 fpscr)
 {
-	return vfp_single_ftosi(sd, unused, m, FPSCR_ROUND_TOZERO);
+	return vfp_single_ftosi(sd, unused, m, (fpscr | FPSCR_ROUND_TOZERO));
 }
 
 static struct op fops_ext[32] = {
@@ -713,8 +767,8 @@ static struct op fops_ext[32] = {
 	[FEXT_TO_IDX(FEXT_FCMPZ)]	= { vfp_single_fcmpz,  OP_SCALAR },
 	[FEXT_TO_IDX(FEXT_FCMPEZ)]	= { vfp_single_fcmpez, OP_SCALAR },
 	[FEXT_TO_IDX(FEXT_FCVT)]	= { vfp_single_fcvtd,  OP_SCALAR|OP_DD },
-	[FEXT_TO_IDX(FEXT_FUITO)]	= { vfp_single_fuito,  OP_SCALAR },
-	[FEXT_TO_IDX(FEXT_FSITO)]	= { vfp_single_fsito,  OP_SCALAR },
+	[FEXT_TO_IDX(FEXT_FUITO)]	= { vfp_single_fuito,  OP_SCALAR|OP_DD },
+	[FEXT_TO_IDX(FEXT_FSITO)]	= { vfp_single_fsito,  OP_SCALAR|OP_DD },
 	[FEXT_TO_IDX(FEXT_FTOUI)]	= { vfp_single_ftoui,  OP_SCALAR },
 	[FEXT_TO_IDX(FEXT_FTOUIZ)]	= { vfp_single_ftouiz, OP_SCALAR },
 	[FEXT_TO_IDX(FEXT_FTOSI)]	= { vfp_single_ftosi,  OP_SCALAR },
@@ -789,6 +843,7 @@ vfp_single_add(struct vfp_single *vsd, struct vfp_single *vsn,
 		struct vfp_single *t = vsn;
 		vsn = vsm;
 		vsm = t;
+		pr_debug("VFP: swapping M <-> N\n");
 	}
 
 	/*
@@ -895,32 +950,43 @@ static u32
 vfp_single_multiply_accumulate(int sd, int sn, s32 m, u32 fpscr, u32 negate, char *func)
 {
 	struct vfp_single vsd, vsp, vsn, vsm;
-	u32 exceptions;
+	u32 exceptions = 0;
 	s32 v;
 
 	v = vfp_get_float(sn);
 	pr_debug("VFP: s%u = %08x\n", sn, v);
 	vfp_single_unpack(&vsn, v);
 	if (vsn.exponent == 0 && vsn.significand)
-		vfp_single_normalise_denormal(&vsn);
+		vfp_single_normalise_denormal(&vsn, fpscr, &exceptions);
 
 	vfp_single_unpack(&vsm, m);
 	if (vsm.exponent == 0 && vsm.significand)
-		vfp_single_normalise_denormal(&vsm);
+		vfp_single_normalise_denormal(&vsm, fpscr, &exceptions);
+
+	exceptions |= vfp_single_multiply(&vsp, &vsn, &vsm, fpscr);
+	vfp_single_dump("Midterm result", &vsp);
+	exceptions |= vfp_single_normaliseround(-1, &vsp, fpscr, exceptions, "fmac-mul");
+
+	vfp_single_dump("Midterm result", &vsp);
+
+	if (vsp.exponent == 0 && vsp.significand)
+		vfp_single_normalise_denormal(&vsp, fpscr, &exceptions);
 
-	exceptions = vfp_single_multiply(&vsp, &vsn, &vsm, fpscr);
 	if (negate & NEG_MULTIPLY)
 		vsp.sign = vfp_sign_negate(vsp.sign);
 
 	v = vfp_get_float(sd);
 	pr_debug("VFP: s%u = %08x\n", sd, v);
 	vfp_single_unpack(&vsn, v);
+	if (vsn.exponent == 0 && vsn.significand)
+		vfp_single_normalise_denormal(&vsn, fpscr, &exceptions);
+
 	if (negate & NEG_SUBTRACT)
 		vsn.sign = vfp_sign_negate(vsn.sign);
 
 	exceptions |= vfp_single_add(&vsd, &vsn, &vsp, fpscr);
 
-	return vfp_single_normaliseround(sd, &vsd, fpscr, exceptions, func);
+	return vfp_single_normaliseround(sd, &vsd, fpscr, exceptions, "fmac-add");
 }
 
 /*
@@ -965,20 +1031,20 @@ static u32 vfp_single_fnmsc(int sd, int sn, s32 m, u32 fpscr)
 static u32 vfp_single_fmul(int sd, int sn, s32 m, u32 fpscr)
 {
 	struct vfp_single vsd, vsn, vsm;
-	u32 exceptions;
+	u32 exceptions = 0;
 	s32 n = vfp_get_float(sn);
 
 	pr_debug("VFP: s%u = %08x\n", sn, n);
 
 	vfp_single_unpack(&vsn, n);
 	if (vsn.exponent == 0 && vsn.significand)
-		vfp_single_normalise_denormal(&vsn);
+		vfp_single_normalise_denormal(&vsn, fpscr, &exceptions);
 
 	vfp_single_unpack(&vsm, m);
 	if (vsm.exponent == 0 && vsm.significand)
-		vfp_single_normalise_denormal(&vsm);
+		vfp_single_normalise_denormal(&vsm, fpscr, &exceptions);
 
-	exceptions = vfp_single_multiply(&vsd, &vsn, &vsm, fpscr);
+	exceptions |= vfp_single_multiply(&vsd, &vsn, &vsm, fpscr);
 	return vfp_single_normaliseround(sd, &vsd, fpscr, exceptions, "fmul");
 }
 
@@ -988,20 +1054,25 @@ static u32 vfp_single_fmul(int sd, int sn, s32 m, u32 fpscr)
 static u32 vfp_single_fnmul(int sd, int sn, s32 m, u32 fpscr)
 {
 	struct vfp_single vsd, vsn, vsm;
-	u32 exceptions;
+	u32 exceptions = 0;
 	s32 n = vfp_get_float(sn);
 
 	pr_debug("VFP: s%u = %08x\n", sn, n);
 
 	vfp_single_unpack(&vsn, n);
 	if (vsn.exponent == 0 && vsn.significand)
-		vfp_single_normalise_denormal(&vsn);
+		vfp_single_normalise_denormal(&vsn, fpscr, &exceptions);
 
 	vfp_single_unpack(&vsm, m);
 	if (vsm.exponent == 0 && vsm.significand)
-		vfp_single_normalise_denormal(&vsm);
+		vfp_single_normalise_denormal(&vsm, fpscr, &exceptions);
+
+	exceptions |= vfp_single_multiply(&vsd, &vsn, &vsm, fpscr);
+	exceptions |= vfp_single_normaliseround(-1, &vsd, fpscr, exceptions, "fnmul-mul");
+
+	if (vsd.exponent == 0 && vsd.significand)
+		vfp_single_normalise_denormal(&vsd, fpscr, &exceptions);
 
-	exceptions = vfp_single_multiply(&vsd, &vsn, &vsm, fpscr);
 	vsd.sign = vfp_sign_negate(vsd.sign);
 	return vfp_single_normaliseround(sd, &vsd, fpscr, exceptions, "fnmul");
 }
@@ -1012,7 +1083,7 @@ static u32 vfp_single_fnmul(int sd, int sn, s32 m, u32 fpscr)
 static u32 vfp_single_fadd(int sd, int sn, s32 m, u32 fpscr)
 {
 	struct vfp_single vsd, vsn, vsm;
-	u32 exceptions;
+	u32 exceptions = 0;
 	s32 n = vfp_get_float(sn);
 
 	pr_debug("VFP: s%u = %08x\n", sn, n);
@@ -1022,13 +1093,13 @@ static u32 vfp_single_fadd(int sd, int sn, s32 m, u32 fpscr)
 	 */
 	vfp_single_unpack(&vsn, n);
 	if (vsn.exponent == 0 && vsn.significand)
-		vfp_single_normalise_denormal(&vsn);
+		vfp_single_normalise_denormal(&vsn, fpscr, &exceptions);
 
 	vfp_single_unpack(&vsm, m);
 	if (vsm.exponent == 0 && vsm.significand)
-		vfp_single_normalise_denormal(&vsm);
+		vfp_single_normalise_denormal(&vsm, fpscr, &exceptions);
 
-	exceptions = vfp_single_add(&vsd, &vsn, &vsm, fpscr);
+	exceptions |= vfp_single_add(&vsd, &vsn, &vsm, fpscr);
 
 	return vfp_single_normaliseround(sd, &vsd, fpscr, exceptions, "fadd");
 }
@@ -1038,12 +1109,50 @@ static u32 vfp_single_fadd(int sd, int sn, s32 m, u32 fpscr)
  */
 static u32 vfp_single_fsub(int sd, int sn, s32 m, u32 fpscr)
 {
+	struct vfp_single vsd, vsn, vsm;
+	u32 exceptions = 0;
+	int tn, tm;
+
 	/*
-	 * Subtraction is addition with one sign inverted.
+	 * Subtraction is like addition, but with a negated operand.
+	 * Problem is when you use same register as source operands. 
+	 * For example fsub s6, s7, s7. In that case negate s7  
+	 * operand will result wrong value...
 	 */
-	return vfp_single_fadd(sd, sn, vfp_single_packed_negate(m), fpscr);
+	
+	s32 n = vfp_get_float(sn);
+	pr_debug("VFP: s%u = %08x\n", sn, n);
+
+	/*
+	 * Unpack and normalise denormals.
+	 */
+	vfp_single_unpack(&vsn, n);
+	if (vsn.exponent == 0 && vsn.significand)
+		vfp_single_normalise_denormal(&vsn, fpscr, &exceptions);
+
+	vfp_single_unpack(&vsm, m);
+	if (vsm.exponent == 0 && vsm.significand)
+		vfp_single_normalise_denormal(&vsm, fpscr, &exceptions);
+
+	/*
+	 * Is either of parameters is NaN do not negate their sign.
+	 * In that case result is based on the input NaN.
+	 */
+	tn = vfp_single_type(&vsn);
+	tm = vfp_single_type(&vsm);
+
+	if (!((tn & (VFP_QNAN | VFP_SNAN)) || (tm & (VFP_QNAN | VFP_SNAN))))	
+		/* Negate m value */
+		vsm.sign = vfp_sign_negate(vsm.sign);
+	else
+		pr_debug("VFP: SUB canceled minus signe. One of parameters is NaN tn=0x%x tm=0x%x\n", tn, tm);
+	
+	exceptions |= vfp_single_add(&vsd, &vsn, &vsm, fpscr);
+
+	return vfp_single_normaliseround(sd, &vsd, fpscr, exceptions, "fsub");
 }
 
+
 /*
  * sd = sn / sm
  */
@@ -1059,6 +1168,11 @@ static u32 vfp_single_fdiv(int sd, int sn, s32 m, u32 fpscr)
 	vfp_single_unpack(&vsn, n);
 	vfp_single_unpack(&vsm, m);
 
+	if (vsn.exponent == 0 && vsn.significand)
+		vfp_single_normalise_denormal(&vsn, fpscr, &exceptions);
+	if (vsm.exponent == 0 && vsm.significand)
+		vfp_single_normalise_denormal(&vsm, fpscr, &exceptions);
+
 	vsd.sign = vsn.sign ^ vsm.sign;
 
 	tn = vfp_single_type(&vsn);
@@ -1101,10 +1215,6 @@ static u32 vfp_single_fdiv(int sd, int sn, s32 m, u32 fpscr)
 	if (tm & VFP_INFINITY || tn & VFP_ZERO)
 		goto zero;
 
-	if (tn & VFP_DENORMAL)
-		vfp_single_normalise_denormal(&vsn);
-	if (tm & VFP_DENORMAL)
-		vfp_single_normalise_denormal(&vsm);
 
 	/*
 	 * Ok, we have two numbers, we can perform division.
@@ -1123,16 +1233,16 @@ static u32 vfp_single_fdiv(int sd, int sn, s32 m, u32 fpscr)
 	if ((vsd.significand & 0x3f) == 0)
 		vsd.significand |= ((u64)vsm.significand * vsd.significand != (u64)vsn.significand << 32);
 
-	return vfp_single_normaliseround(sd, &vsd, fpscr, 0, "fdiv");
+	return vfp_single_normaliseround(sd, &vsd, fpscr, exceptions, "fdiv");
 
  vsn_nan:
-	exceptions = vfp_propagate_nan(&vsd, &vsn, &vsm, fpscr);
+	exceptions |= vfp_propagate_nan(&vsd, &vsn, &vsm, fpscr);
  pack:
 	vfp_put_float(vfp_single_pack(&vsd), sd);
 	return exceptions;
 
  vsm_nan:
-	exceptions = vfp_propagate_nan(&vsd, &vsm, &vsn, fpscr);
+	exceptions |= vfp_propagate_nan(&vsd, &vsm, &vsn, fpscr);
 	goto pack;
 
  zero:
@@ -1141,7 +1251,7 @@ static u32 vfp_single_fdiv(int sd, int sn, s32 m, u32 fpscr)
 	goto pack;
 
  divzero:
-	exceptions = FPSCR_DZC;
+	exceptions |= FPSCR_DZC;
  infinity:
 	vsd.exponent = 255;
 	vsd.significand = 0;
@@ -1149,7 +1259,7 @@ static u32 vfp_single_fdiv(int sd, int sn, s32 m, u32 fpscr)
 
  invalid:
 	vfp_put_float(vfp_single_pack(&vfp_single_default_qnan), sd);
-	return FPSCR_IOC;
+	return (exceptions | FPSCR_IOC);
 }
 
 static struct op fops[16] = {
diff --git a/drivers/ata/libata-pmp.c b/drivers/ata/libata-pmp.c
index 00305f4..8228054 100644
--- a/drivers/ata/libata-pmp.c
+++ b/drivers/ata/libata-pmp.c
@@ -379,6 +379,13 @@ static void sata_pmp_quirks(struct ata_port *ap)
 		 * otherwise.  Don't try hard to recover it.
 		 */
 		ap->pmp_link[ap->nr_pmp_links - 1].flags |= ATA_LFLAG_NO_RETRY;
+	} else if (vendor == 0x11ab && devid == 0x4140) {
+		/* Marvell 4140 quirks */
+		ata_for_each_link(link, ap, EDGE) {
+			/* port 4 is for SEMB device and it doesn't like SRST */
+			if (link->pmp == 4)
+				link->flags |= ATA_LFLAG_DISABLED;
+		}
 	}
 }
 
diff --git a/drivers/ata/sata_mv.c b/drivers/ata/sata_mv.c
index 39af57a..78f04a6 100644
--- a/drivers/ata/sata_mv.c
+++ b/drivers/ata/sata_mv.c
@@ -2782,6 +2782,8 @@ static void mv_process_crpb_entries(struct ata_port *ap, struct mv_port_priv *pp
 	in_index = (readl(port_mmio + EDMA_RSP_Q_IN_PTR)
 			>> EDMA_RSP_Q_PTR_SHIFT) & MV_MAX_Q_DEPTH_MASK;
 
+	dma_io_sync();
+
 	/* Process new responses from since the last time we looked */
 	while (in_index != pp->resp_idx) {
 		unsigned int tag;
diff --git a/drivers/dma/Kconfig b/drivers/dma/Kconfig
index 71a06c7..9976374 100644
--- a/drivers/dma/Kconfig
+++ b/drivers/dma/Kconfig
@@ -104,7 +104,7 @@ config MPC512X_DMA
 
 config MV_XOR
 	bool "Marvell XOR engine support"
-	depends on PLAT_ORION
+	depends on PLAT_ORION || PLAT_ARMADA
 	select DMA_ENGINE
 	---help---
 	  Enable support for the Marvell XOR engine.
@@ -179,6 +179,16 @@ config NET_DMA
 	  Say Y here if you enabled INTEL_IOATDMA or FSL_DMA, otherwise
 	  say N.
 
+config SPLICE_NET_DMA_SUPPORT
+        bool "Net DMA support for splice"
+	depends on NET_DMA
+	default n
+	help
+	  This enables the use of DMA engines in the network stack to
+	  offload splice operations, freeing CPU cycles.
+
+	  If unsure, say N.
+
 config ASYNC_TX_DMA
 	bool "Async_tx: Offload support for the async_tx api"
 	depends on DMA_ENGINE
diff --git a/drivers/dma/dmaengine.c b/drivers/dma/dmaengine.c
index d18b5d0..9364341 100644
--- a/drivers/dma/dmaengine.c
+++ b/drivers/dma/dmaengine.c
@@ -59,11 +59,13 @@
 #include <linux/rculist.h>
 #include <linux/idr.h>
 #include <linux/slab.h>
+#include <linux/pagemap.h>
 
 static DEFINE_MUTEX(dma_list_mutex);
 static LIST_HEAD(dma_device_list);
 static long dmaengine_ref_count;
 static struct idr dma_idr;
+static struct page *temp_page = NULL;
 
 /* --- sysfs implementation --- */
 
@@ -879,6 +881,8 @@ dma_async_memcpy_buf_to_buf(struct dma_chan *chan, void *dest,
 }
 EXPORT_SYMBOL(dma_async_memcpy_buf_to_buf);
 
+#define DMA_ENGINE_MIN_OP_SIZE 128
+
 /**
  * dma_async_memcpy_buf_to_pg - offloaded copy from address to page
  * @chan: DMA channel to offload copy to
@@ -902,6 +906,40 @@ dma_async_memcpy_buf_to_pg(struct dma_chan *chan, struct page *page,
 	dma_cookie_t cookie;
 	unsigned long flags;
 
+	if (!page) {
+		printk(KERN_ERR "%s page %x\n", __FUNCTION__, (void*)page);
+		return -EFAULT;
+	}
+	/*
+	  This code snippet is for Marvell XOR engine that supports operation on len < 128
+	  So if we get a copy operation smaller than 128, we use memcpy
+	  Also, we're creating a dummy dma operation in order to satisfy upper layers waiting 
+	  for a valid cookie return code.
+	*/
+	if (len < DMA_ENGINE_MIN_OP_SIZE)
+	{
+		void * dst = kmap_atomic(page, KM_USER0) + offset;
+		memcpy(dst, kdata, len);
+		kunmap_atomic(dst, KM_USER0);
+
+		dma_src = dma_map_page(dev->dev, temp_page, 0, PAGE_SIZE, DMA_TO_DEVICE);
+		dma_dest = dma_map_page(dev->dev, temp_page, 0, PAGE_SIZE, DMA_FROM_DEVICE);
+
+		flags = DMA_CTRL_ACK;
+		tx = dev->device_prep_dma_memcpy(chan, dma_dest, dma_src, DMA_ENGINE_MIN_OP_SIZE, flags);
+
+		if (!tx) {
+			dma_unmap_page(dev->dev, dma_src, PAGE_SIZE, DMA_TO_DEVICE);
+			dma_unmap_page(dev->dev, dma_dest, PAGE_SIZE, DMA_FROM_DEVICE);
+			return -ENOMEM;
+		}
+
+		tx->callback = NULL;
+		cookie = tx->tx_submit(tx);
+
+		return cookie;
+	}
+
 	dma_src = dma_map_single(dev->dev, kdata, len, DMA_TO_DEVICE);
 	dma_dest = dma_map_page(dev->dev, page, offset, len, DMA_FROM_DEVICE);
 	flags = DMA_CTRL_ACK | DMA_COMPL_SRC_UNMAP_SINGLE;
@@ -950,6 +988,40 @@ dma_async_memcpy_pg_to_pg(struct dma_chan *chan, struct page *dest_pg,
 	dma_cookie_t cookie;
 	unsigned long flags;
 
+	if (!dest_pg || !src_pg) {
+		printk(KERN_ERR "%s dest_pg %x src_pg %x\n", __FUNCTION__, (void*)dest_pg, (void*)src_pg);
+		return -EFAULT;
+	}
+
+	/*
+	  This code snippet is for Marvell XOR engine that doesn't support operations on len < 128
+	  So if we get a copy operation smaller than 128, we use memcpy
+	  Also, we're creating a dummy dma operation in order to satisfy upper layers waiting 
+	  for a valid cookie return code.
+	*/
+	if (len < DMA_ENGINE_MIN_OP_SIZE)
+	{
+		void * dst = kmap_atomic(dest_pg, KM_USER0) + dest_off;
+		memcpy(dst, src_pg+src_off, len);
+		kunmap_atomic(dst, KM_USER0);
+		
+		dma_src = dma_map_page(dev->dev, temp_page, 0, PAGE_SIZE, DMA_TO_DEVICE);
+		dma_dest = dma_map_page(dev->dev, temp_page, 0, PAGE_SIZE, DMA_FROM_DEVICE);
+		flags = DMA_CTRL_ACK;
+		tx = dev->device_prep_dma_memcpy(chan, dma_dest, dma_src, DMA_ENGINE_MIN_OP_SIZE, flags);
+
+		if (!tx) {
+			dma_unmap_page(dev->dev, dma_src, PAGE_SIZE, DMA_TO_DEVICE);
+			dma_unmap_page(dev->dev, dma_dest, PAGE_SIZE, DMA_FROM_DEVICE);
+			return -ENOMEM;
+		}
+
+		tx->callback = NULL;
+		cookie = tx->tx_submit(tx);
+
+		return cookie;
+	}
+
 	dma_src = dma_map_page(dev->dev, src_pg, src_off, len, DMA_TO_DEVICE);
 	dma_dest = dma_map_page(dev->dev, dest_pg, dest_off, len,
 				DMA_FROM_DEVICE);
@@ -1047,6 +1119,10 @@ static int __init dma_bus_init(void)
 {
 	idr_init(&dma_idr);
 	mutex_init(&dma_list_mutex);
+	temp_page = alloc_pages(GFP_KERNEL, 0);
+	if (!temp_page)
+		BUG();
+
 	return class_register(&dma_devclass);
 }
 arch_initcall(dma_bus_init);
diff --git a/drivers/dma/iovlock.c b/drivers/dma/iovlock.c
index bb48a57..a04a995 100644
--- a/drivers/dma/iovlock.c
+++ b/drivers/dma/iovlock.c
@@ -121,6 +121,73 @@ out:
 	return NULL;
 }
 
+#ifdef CONFIG_SPLICE_NET_DMA_SUPPORT
+struct dma_pinned_list *dma_pin_kernel_iovec_pages(struct iovec *iov, size_t len)
+{
+	struct dma_pinned_list *local_list;
+	struct page **pages;
+	int i, j;
+	int nr_iovecs = 0;
+	int iovec_len_used = 0;
+	int iovec_pages_used = 0;
+
+	/* determine how many iovecs/pages there are, up front */
+	do {
+		iovec_len_used += iov[nr_iovecs].iov_len;
+		iovec_pages_used += num_pages_spanned(&iov[nr_iovecs]);
+		nr_iovecs++;
+	} while (iovec_len_used < len);
+
+	/* single kmalloc for pinned list, page_list[], and the page arrays */
+	local_list = kmalloc(sizeof(*local_list)
+		+ (nr_iovecs * sizeof (struct dma_page_list))
+		+ (iovec_pages_used * sizeof (struct page*)), GFP_KERNEL);
+	if (!local_list)
+		goto out;
+
+	/* list of pages starts right after the page list array */
+	pages = (struct page **) &local_list->page_list[nr_iovecs];
+
+	local_list->nr_iovecs = 0;
+
+	for (i = 0; i < nr_iovecs; i++) {
+		struct dma_page_list *page_list = &local_list->page_list[i];
+		int offset;
+
+		len -= iov[i].iov_len;
+
+		if (!access_ok(VERIFY_WRITE, iov[i].iov_base, iov[i].iov_len))
+			goto unpin;
+
+		page_list->nr_pages = num_pages_spanned(&iov[i]);
+		page_list->base_address = iov[i].iov_base;
+
+		page_list->pages = pages;
+		pages += page_list->nr_pages;
+
+		for (offset=0, j=0; j < page_list->nr_pages; j++, offset+=PAGE_SIZE) {
+			page_list->pages[j] = phys_to_page(__pa((unsigned int)page_list->base_address) + offset);
+		}
+		local_list->nr_iovecs = i + 1;
+	}
+
+	return local_list;
+
+unpin:
+	kfree(local_list);
+out:
+	return NULL;
+}
+
+void dma_unpin_kernel_iovec_pages(struct dma_pinned_list *pinned_list)
+{
+	if (!pinned_list)
+		return;
+
+	kfree(pinned_list);
+}
+#endif
+
 void dma_unpin_iovec_pages(struct dma_pinned_list *pinned_list)
 {
 	int i, j;
diff --git a/drivers/dma/mv_xor.c b/drivers/dma/mv_xor.c
index 35d0d8c..1e3dfe2 100644
--- a/drivers/dma/mv_xor.c
+++ b/drivers/dma/mv_xor.c
@@ -442,6 +442,7 @@ static void
 mv_xor_slot_cleanup(struct mv_xor_chan *mv_chan)
 {
 	spin_lock_bh(&mv_chan->lock);
+	dma_io_sync();
 	__mv_xor_slot_cleanup(mv_chan);
 	spin_unlock_bh(&mv_chan->lock);
 }
@@ -826,6 +827,7 @@ static enum dma_status mv_xor_is_complete(struct dma_chan *chan,
 
 	last_used = chan->cookie;
 	last_complete = mv_chan->completed_cookie;
+	spin_lock_bh(&mv_chan->lock);
 	mv_chan->is_complete_cookie = cookie;
 	if (done)
 		*done = last_complete;
@@ -835,8 +837,10 @@ static enum dma_status mv_xor_is_complete(struct dma_chan *chan,
 	ret = dma_async_is_complete(cookie, last_complete, last_used);
 	if (ret == DMA_SUCCESS) {
 		mv_xor_clean_completed_slots(mv_chan);
+		spin_unlock_bh(&mv_chan->lock);
 		return ret;
 	}
+	spin_unlock_bh(&mv_chan->lock);
 	mv_xor_slot_cleanup(mv_chan);
 
 	last_used = chan->cookie;
diff --git a/drivers/hwmon/Kconfig b/drivers/hwmon/Kconfig
index 533b6ec..5f073cc 100644
--- a/drivers/hwmon/Kconfig
+++ b/drivers/hwmon/Kconfig
@@ -212,6 +212,14 @@ config SENSORS_ADT7475
 	  This driver can also be build as a module.  If so, the module
 	  will be called adt7475.
 
+config SENSORS_ARMADA
+	tristate "Armada XP SoC Thermal Sensor"
+	depends on ARCH_ARMADA_XP || ARCH_ARMADA370
+	default y
+	help
+	  This driver provides support for the Marvell's Armada XP SoC thermal
+	  Controller, which provides onchip temperature sensors.
+
 config SENSORS_ASC7621
 	tristate "Andigilog aSC7621"
 	depends on HWMON && I2C
diff --git a/drivers/i2c/busses/Kconfig b/drivers/i2c/busses/Kconfig
index 6833516..f9ee39f 100644
--- a/drivers/i2c/busses/Kconfig
+++ b/drivers/i2c/busses/Kconfig
@@ -434,7 +434,7 @@ config I2C_MPC
 
 config I2C_MV64XXX
 	tristate "Marvell mv64xxx I2C Controller"
-	depends on (MV64X60 || PLAT_ORION) && EXPERIMENTAL
+	depends on (MV64X60 || PLAT_ORION || PLAT_ARMADA) && EXPERIMENTAL
 	help
 	  If you say yes to this option, support will be included for the
 	  built-in I2C interface on the Marvell 64xxx line of host bridges.
diff --git a/drivers/mmc/card/queue.c b/drivers/mmc/card/queue.c
index 7d39fe5..689f924 100644
--- a/drivers/mmc/card/queue.c
+++ b/drivers/mmc/card/queue.c
@@ -20,8 +20,10 @@
 #include <linux/mmc/host.h>
 #include "queue.h"
 
-#ifdef CONFIG_OPTIMIZE_SD_PERFORMANCE
-#define MMC_QUEUE_BOUNCESZ	262144
+#ifdef CONFIG_PLAT_ARMADA
+#define MMC_QUEUE_BOUNCESZ	(65536 << 2)
+//#ifdef CONFIG_OPTIMIZE_SD_PERFORMANCE
+//#define MMC_QUEUE_BOUNCESZ	262144
 #else
 #define MMC_QUEUE_BOUNCESZ	65536
 #endif
diff --git a/drivers/mmc/core/core.c b/drivers/mmc/core/core.c
index b3e2835..53e5c0d 100644
--- a/drivers/mmc/core/core.c
+++ b/drivers/mmc/core/core.c
@@ -296,7 +296,10 @@ void mmc_set_data_timeout(struct mmc_data *data, const struct mmc_card *card)
 		timeout_us = data->timeout_ns / 1000;
 		timeout_us += data->timeout_clks * 1000 /
 			(card->host->ios.clock / 1000);
-
+#ifdef CONFIG_MV_MMC_TIMEOUT_OVERRIDE
+		/* Set to 500ms for both Rd/Wr */
+		limit_us = 500000;
+#else
 		if (data->flags & MMC_DATA_WRITE)
 			/*
 			 * The limit is really 250 ms, but that is
@@ -305,7 +308,7 @@ void mmc_set_data_timeout(struct mmc_data *data, const struct mmc_card *card)
 			limit_us = 300000;
 		else
 			limit_us = 100000;
-
+#endif
 		/*
 		 * SDHC cards always use these fixed values.
 		 */
diff --git a/drivers/mmc/host/Kconfig b/drivers/mmc/host/Kconfig
index 3f1dd7b..5f22f09 100644
--- a/drivers/mmc/host/Kconfig
+++ b/drivers/mmc/host/Kconfig
@@ -279,7 +279,7 @@ config MMC_TIFM_SD
 
 config MMC_MVSDIO
 	tristate "Marvell MMC/SD/SDIO host driver"
-	depends on PLAT_ORION
+	depends on PLAT_ORION || PLAT_ARMADA
 	---help---
 	  This selects the Marvell SDIO host driver.
 	  SDIO may currently be found on the Kirkwood 88F6281 and 88F6192
diff --git a/drivers/mtd/maps/Makefile b/drivers/mtd/maps/Makefile
index bb035cd..60dbd9f 100644
--- a/drivers/mtd/maps/Makefile
+++ b/drivers/mtd/maps/Makefile
@@ -59,3 +59,8 @@ obj-$(CONFIG_MTD_BFIN_ASYNC)	+= bfin-async-flash.o
 obj-$(CONFIG_MTD_RBTX4939)	+= rbtx4939-flash.o
 obj-$(CONFIG_MTD_VMU)		+= vmu-flash.o
 obj-$(CONFIG_MTD_GPIO_ADDR)	+= gpio-addr-flash.o
+
+ifeq ($(CONFIG_PLAT_ARMADA),y)
+	include $(srctree)/$(MACH_DIR)/config/mvRules.mk
+	obj-y += ../../../$(MACH_DIR)/flashmap.o
+endif
diff --git a/drivers/mtd/nand/Makefile b/drivers/mtd/nand/Makefile
index 8abc265..2408ffc 100644
--- a/drivers/mtd/nand/Makefile
+++ b/drivers/mtd/nand/Makefile
@@ -43,5 +43,6 @@ obj-$(CONFIG_MTD_NAND_TXX9NDFMC)	+= txx9ndfmc.o
 obj-$(CONFIG_MTD_NAND_W90P910)		+= w90p910_nand.o
 obj-$(CONFIG_MTD_NAND_NOMADIK)		+= nomadik_nand.o
 obj-$(CONFIG_MTD_NAND_BCM_UMI)		+= bcm_umi_nand.o nand_bcm_umi.o
+obj-$(CONFIG_MTD_NAND_NFC)			+= ../../../arch/arm/plat-armada/mv_drivers_lsp/mv_mtd/
 
 nand-objs := nand_base.o nand_bbt.o
diff --git a/drivers/mtd/nand/nand_base.c b/drivers/mtd/nand/nand_base.c
index 8f2958f..b6d5f30 100644
--- a/drivers/mtd/nand/nand_base.c
+++ b/drivers/mtd/nand/nand_base.c
@@ -52,6 +52,10 @@
 #include <linux/mtd/partitions.h>
 #endif
 
+#ifdef CONFIG_MTD_NAND_NFC_GANG_SUPPORT
+static char nand_name[128];
+#endif
+
 /* Define default oob placement schemes for large and small page devices */
 static struct nand_ecclayout nand_oob_8 = {
 	.eccbytes = 3,
@@ -2635,10 +2639,21 @@ static struct nand_flash_dev *nand_get_flash_type(struct mtd_info *mtd,
 	if (!type)
 		return ERR_PTR(-ENODEV);
 
-	if (!mtd->name)
+	if (!mtd->name) {
+#ifdef CONFIG_MTD_NAND_NFC_GANG_SUPPORT
+		sprintf(nand_name, "%s%s", type->name,
+				(chip->num_devs == 2) ? " - Ganged" : "");
+		type->name = nand_name;
+#endif
 		mtd->name = type->name;
+	}
 
 	chip->chipsize = (uint64_t)type->chipsize << 20;
+#ifdef CONFIG_MTD_NAND_NFC_GANG_SUPPORT
+	chip->chipsize *= chip->num_devs;
+	if(chip->num_devs > 1)
+		type->options |= NAND_BUSWIDTH_16;
+#endif
 
 	/* Newer devices have all the information in additional id bytes */
 	if (!type->pagesize) {
@@ -2649,16 +2664,25 @@ static struct nand_flash_dev *nand_get_flash_type(struct mtd_info *mtd,
 		extid = chip->read_byte(mtd);
 		/* Calc pagesize */
 		mtd->writesize = 1024 << (extid & 0x3);
+#ifdef CONFIG_MTD_NAND_NFC_GANG_SUPPORT
+			mtd->writesize *= chip->num_devs;
+#endif
 		extid >>= 2;
 		/* Calc oobsize */
 		mtd->oobsize = (8 << (extid & 0x01)) * (mtd->writesize >> 9);
 		extid >>= 2;
 		/* Calc blocksize. Blocksize is multiples of 64KiB */
 		mtd->erasesize = (64 * 1024) << (extid & 0x03);
+#ifdef CONFIG_MTD_NAND_NFC_GANG_SUPPORT
+			mtd->erasesize *= chip->num_devs;
+#endif
 		extid >>= 2;
 		/* Get buswidth information */
 		busw = (extid & 0x01) ? NAND_BUSWIDTH_16 : 0;
-
+#ifdef CONFIG_MTD_NAND_NFC_GANG_SUPPORT
+			if(chip->num_devs > 1)
+				busw = NAND_BUSWIDTH_16;
+#endif
 	} else {
 		/*
 		 * Old devices have chip data hardcoded in the device id table
@@ -2666,7 +2690,16 @@ static struct nand_flash_dev *nand_get_flash_type(struct mtd_info *mtd,
 		mtd->erasesize = type->erasesize;
 		mtd->writesize = type->pagesize;
 		mtd->oobsize = mtd->writesize / 32;
+#ifdef CONFIG_MTD_NAND_NFC_MLC_SUPPORT
+		/* New devices have non standard OOB size */
+		if (chip->oobsize_ovrd)
+			mtd->oobsize = chip->oobsize_ovrd;
+#endif
 		busw = type->options & NAND_BUSWIDTH_16;
+#ifdef CONFIG_MTD_NAND_NFC_GANG_SUPPORT
+		mtd->erasesize *= chip->num_devs;
+		mtd->writesize *= chip->num_devs;
+#endif
 	}
 
 	/* Try to identify manufacturer */
diff --git a/drivers/mtd/nand/nand_bbt.c b/drivers/mtd/nand/nand_bbt.c
index 55c23e5..9c05d5b 100644
--- a/drivers/mtd/nand/nand_bbt.c
+++ b/drivers/mtd/nand/nand_bbt.c
@@ -301,7 +301,34 @@ static int read_abs_bbts(struct mtd_info *mtd, uint8_t *buf,
 	}
 	return 1;
 }
+#ifdef CONFIG_MTD_NAND_NFC_MLC_SUPPORT
+/*
+ * Scan a given block in the custom location based on Naked symantics
+ */
+static int scan_block_custom(struct mtd_info *mtd, struct nand_bbt_descr *bd,
+			loff_t offs, uint8_t *buf, int page, int pos)
+{	
+	int ret;
+	struct mtd_oob_ops ops;
 
+	ops.mode = MTD_OOB_RAW;
+	ops.ooboffs = 0;
+	ops.ooblen = mtd->oobsize;
+	ops.oobbuf = (buf + mtd->writesize);
+	ops.datbuf = buf;
+	ops.len = mtd->writesize;
+
+	ret = mtd->read_oob(mtd, (offs + (mtd->writesize * page)), &ops);
+	if (ret)
+		return ret;
+
+	/* Check 2 bytes to cover the ganaged case */
+	if ((buf[pos] != 0xFF) || (buf[pos+1] != 0xFF))
+		return 1;
+
+	return 0;
+}
+#endif
 /*
  * Scan a given block full
  */
@@ -416,7 +443,12 @@ static int create_bbt(struct mtd_info *mtd, uint8_t *buf,
 
 	for (i = startblock; i < numblocks;) {
 		int ret;
-
+#ifdef CONFIG_MTD_NAND_NFC_MLC_SUPPORT
+		if (bd->options & NAND_BBT_SCANMVCUSTOM)
+			ret = scan_block_custom(mtd, bd, from, buf,
+						this->bb_page, this->bb_location);
+		else
+#endif
 		if (bd->options & NAND_BBT_SCANALLPAGES)
 			ret = scan_block_full(mtd, bd, from, buf, readlen,
 					      scanlen, len);
diff --git a/drivers/mtd/nand/nand_ids.c b/drivers/mtd/nand/nand_ids.c
index 69ee2c9..5f40f24 100644
--- a/drivers/mtd/nand/nand_ids.c
+++ b/drivers/mtd/nand/nand_ids.c
@@ -100,6 +100,7 @@ struct nand_flash_dev nand_flash_ids[] = {
 	/* 8 Gigabit */
 	{"NAND 1GiB 1,8V 8-bit",	0xA3, 0, 1024, 0, LP_OPTIONS},
 	{"NAND 1GiB 3,3V 8-bit",	0xD3, 0, 1024, 0, LP_OPTIONS},
+	{"NAND 1GiB 3,3V 8-bit",	0x38, 4096, 1024, 524288, LP_OPTIONS},
 	{"NAND 1GiB 1,8V 16-bit",	0xB3, 0, 1024, 0, LP_OPTIONS16},
 	{"NAND 1GiB 3,3V 16-bit",	0xC3, 0, 1024, 0, LP_OPTIONS16},
 
@@ -109,6 +110,15 @@ struct nand_flash_dev nand_flash_ids[] = {
 	{"NAND 2GiB 1,8V 16-bit",	0xB5, 0, 2048, 0, LP_OPTIONS16},
 	{"NAND 2GiB 3,3V 16-bit",	0xC5, 0, 2048, 0, LP_OPTIONS16},
 
+#ifdef CONFIG_MTD_NAND_NFC_MLC_SUPPORT
+	/* 32 Gigabit - wrongly detected due to changes in READ_ID decoding */
+	{"NAND 4GiB 3,3V 8-bit",	0xD7, 4096, 4096, 524288, LP_OPTIONS},
+	/* 32 Gigabit - wrongly detected due to changes in READ_ID decoding */
+	{"NAND 8GiB 3,3V 8-bit",	0x88, 8192, 8192, 2097152, LP_OPTIONS},
+#else
+	/* 32 Gigabit */
+	{"NAND 4GiB 3,3V 8-bit",	0xD7, 0, 4096, 0, LP_OPTIONS},
+#endif
 	/*
 	 * Renesas AND 1 Gigabit. Those chips do not support extended id and
 	 * have a strange page/block layout !  The chosen minimum erasesize is
diff --git a/drivers/net/Makefile b/drivers/net/Makefile
index 4066345..84ffa9a 100644
--- a/drivers/net/Makefile
+++ b/drivers/net/Makefile
@@ -1,6 +1,8 @@
 #
 # Makefile for the Linux network (ethercard) device drivers.
 #
+obj-$(CONFIG_MV_ETH_LEGACY) += ../../arch/arm/plat-armada/mv_drivers_lsp/mv_network/mv_ethernet/
+obj-$(CONFIG_MV_ETH_NETA) += ../../arch/arm/plat-armada/mv_drivers_lsp/mv_neta/net_dev/
 
 obj-$(CONFIG_MII) += mii.o
 obj-$(CONFIG_MDIO) += mdio.o
diff --git a/drivers/rtc/Kconfig b/drivers/rtc/Kconfig
index 7f914b1..dd019c6 100644
--- a/drivers/rtc/Kconfig
+++ b/drivers/rtc/Kconfig
@@ -853,7 +853,7 @@ config RTC_DRV_TX4939
 
 config RTC_DRV_MV
 	tristate "Marvell SoC RTC"
-	depends on ARCH_KIRKWOOD || ARCH_DOVE
+	depends on ARCH_KIRKWOOD || ARCH_DOVE || ARCH_ARMADA_XP || ARCH_ARMADA370
 	help
 	  If you say yes here you will get support for the in-chip RTC
 	  that can be found in some of Marvell's SoC devices, such as
diff --git a/drivers/scsi/Kconfig b/drivers/scsi/Kconfig
index 75f2336..2697224 100644
--- a/drivers/scsi/Kconfig
+++ b/drivers/scsi/Kconfig
@@ -346,6 +346,12 @@ menuconfig SCSI_LOWLEVEL
 
 if SCSI_LOWLEVEL && SCSI
 
+config SCSI_MV_THOR
+	tristate "Marvell Storage Controller 6121/6122/6141/6145"
+	depends on SCSI && BLK_DEV_SD
+	help
+		Provides support for Marvell thor Storage Controller series.
+
 config ISCSI_TCP
 	tristate "iSCSI Initiator over TCP/IP"
 	depends on SCSI && INET
diff --git a/drivers/scsi/Makefile b/drivers/scsi/Makefile
index 92a8c50..5a4de13 100644
--- a/drivers/scsi/Makefile
+++ b/drivers/scsi/Makefile
@@ -49,6 +49,7 @@ obj-$(CONFIG_A2091_SCSI)	+= a2091.o	wd33c93.o
 obj-$(CONFIG_GVP11_SCSI)	+= gvp11.o	wd33c93.o
 obj-$(CONFIG_MVME147_SCSI)	+= mvme147.o	wd33c93.o
 obj-$(CONFIG_SGIWD93_SCSI)	+= sgiwd93.o	wd33c93.o
+obj-$(CONFIG_SCSI_MV_THOR)      += thor/
 obj-$(CONFIG_ATARI_SCSI)	+= atari_scsi.o
 obj-$(CONFIG_MAC_SCSI)		+= mac_scsi.o
 obj-$(CONFIG_SCSI_MAC_ESP)	+= esp_scsi.o	mac_esp.o
@@ -140,6 +141,7 @@ obj-$(CONFIG_VMWARE_PVSCSI)	+= vmw_pvscsi.o
 
 obj-$(CONFIG_ARM)		+= arm/
 
+obj-$(CONFIG_SCSI_MVSATA)       += ../../arch/arm/plat-armada/mv_drivers_lsp/mv_sata/
 obj-$(CONFIG_CHR_DEV_ST)	+= st.o
 obj-$(CONFIG_CHR_DEV_OSST)	+= osst.o
 obj-$(CONFIG_BLK_DEV_SD)	+= sd_mod.o
diff --git a/drivers/serial/8250.c b/drivers/serial/8250.c
index c1c7aaf..300f4d9 100644
--- a/drivers/serial/8250.c
+++ b/drivers/serial/8250.c
@@ -608,6 +608,22 @@ static void dwapb_serial_out(struct uart_port *p, int offset, int value)
 {
 	int save_offset = offset;
 	offset = map_8250_out_reg(p, offset) << p->regshift;
+
+#ifdef CONFIG_PLAT_ARMADA
+	/* If we are accessing DLH (0x4), DLL (0x0), LCR(0xC) or 0x1C
+	** we need to make sure that the busy bit is cleared in USR register.
+	*/
+	if ((((readb(p->membase + 0xC) & 0x80) &&
+	     ((save_offset == UART_DLL) || (save_offset == UART_DLM) ||
+	      (offset == 0x1C))) ||
+	     (save_offset == UART_LCR)) && !(readb(p->membase + 0x14) & 0x1)) {
+		unsigned int status;
+		do {
+			status = *((volatile u32 *)p->private_data);
+		} while (status & 0x1);
+	}
+#endif
+
 	/* Save the LCR value so it can be re-written when a
 	 * Busy Detect interrupt occurs. */
 	if (save_offset == UART_LCR) {
@@ -1927,7 +1943,9 @@ static void serial8250_timeout(unsigned long data)
 	unsigned int iir;
 
 	iir = serial_in(up, UART_IIR);
+#ifndef CONFIG_PLAT_ARMADA
 	if (!(iir & UART_IIR_NO_INT))
+#endif
 		serial8250_handle_port(up);
 	mod_timer(&up->timer, jiffies + poll_timeout(up->port.timeout));
 }
diff --git a/drivers/telephony/Makefile b/drivers/telephony/Makefile
index 1206615..14a3b89 100644
--- a/drivers/telephony/Makefile
+++ b/drivers/telephony/Makefile
@@ -5,3 +5,4 @@
 obj-$(CONFIG_PHONE) += phonedev.o
 obj-$(CONFIG_PHONE_IXJ) += ixj.o
 obj-$(CONFIG_PHONE_IXJ_PCMCIA) += ixj_pcmcia.o
+obj-$(CONFIG_MV_PHONE) += ../../arch/arm/plat-armada/mv_drivers_lsp/mv_phone/
diff --git a/drivers/usb/gadget/Kconfig b/drivers/usb/gadget/Kconfig
index b42547f..f4571f0 100644
--- a/drivers/usb/gadget/Kconfig
+++ b/drivers/usb/gadget/Kconfig
@@ -81,8 +81,9 @@ config USB_GADGET_DEBUG_FS
 
 config USB_GADGET_VBUS_DRAW
 	int "Maximum VBUS Power usage (2-500 mA)"
-	range 2 500
-	default 2
+	range 0 500
+	default 2 if !PLAT_ARMADA
+	default 0 if PLAT_ARMADA
 	help
 	   Some devices need to draw power from USB when they are
 	   configured, perhaps to operate circuitry or to recharge
@@ -522,6 +523,18 @@ config USB_LANGWELL
 #
 # LAST -- dummy/emulated controller
 #
+config USB_GADGET_MRVL
+        boolean "Marvell USB Device Port"
+        depends on (USB=y || (USB=m && USB_GADGET=m)) && EXPERIMENTAL
+	select USB_GADGET_DUALSPEED
+        help
+	  ...
+
+config USB_MRVL
+        tristate
+        depends on USB_GADGET_MRVL
+        default USB_GADGET
+        select USB_GADGET_SELECTED
 
 config USB_GADGET_DUMMY_HCD
 	boolean "Dummy HCD (DEVELOPMENT)"
diff --git a/drivers/usb/gadget/Makefile b/drivers/usb/gadget/Makefile
index 295567f..b205391 100644
--- a/drivers/usb/gadget/Makefile
+++ b/drivers/usb/gadget/Makefile
@@ -29,6 +29,7 @@ obj-$(CONFIG_USB_CI13XXX)	+= ci13xxx_udc.o
 obj-$(CONFIG_USB_S3C_HSOTG)	+= s3c-hsotg.o
 obj-$(CONFIG_USB_LANGWELL)	+= langwell_udc.o
 obj-$(CONFIG_PCH_USBDEV) 	+= pch_udc.o
+obj-$(CONFIG_USB_MRVL)       	+= ../../../arch/arm/plat-armada/mv_drivers_lsp/mv_udc/
 
 #
 # USB gadget drivers
diff --git a/drivers/usb/gadget/gadget_chips.h b/drivers/usb/gadget/gadget_chips.h
index 39bd669..70d81c3 100644
--- a/drivers/usb/gadget/gadget_chips.h
+++ b/drivers/usb/gadget/gadget_chips.h
@@ -142,12 +142,18 @@
 #define gadget_is_s3c_hsotg(g)    0
 #endif
 
-#ifdef CONFIG_USB_GADGET_PCH
-#define	gadget_is_pch(g)	(!strcmp("pch_udc", (g)->name))
+#ifdef CONFIG_USB_GADGET_MRVL
+#define gadget_is_mrvl(g)    !strcmp("mv_udc", (g)->name)
 #else
-#define	gadget_is_pch(g)	0
+#define gadget_is_mrvl(g)    0
 #endif
 
+//#ifdef CONFIG_USB_GADGET_PCH
+//#define	gadget_is_pch(g)	(!strcmp("pch_udc", (g)->name))
+//#else
+//#define	gadget_is_pch(g)	0
+//#endif
+
 #if defined(CONFIG_USB_CNS3XXX_OTG_BOTH) || \
 	defined(CONFIG_USB_CNS3XXX_OTG_PCD_ONLY)
 #define	gadget_is_dwc_otg_pcd(g)	(!strcmp("dwc_otg_pcd", (g)->name))
@@ -212,7 +218,7 @@ static inline int usb_gadget_controller_number(struct usb_gadget *gadget)
 		return 0x25;
 	else if (gadget_is_s3c_hsotg(gadget))
 		return 0x26;
-	else if (gadget_is_pch(gadget))
+	else if (gadget_is_mrvl(gadget))
 		return 0x27;
 	else if (gadget_is_dwc_otg_pcd(gadget))
 		return 0x28;
diff --git a/drivers/usb/gadget/storage_common.c b/drivers/usb/gadget/storage_common.c
index 868d8ee..17935de 100644
--- a/drivers/usb/gadget/storage_common.c
+++ b/drivers/usb/gadget/storage_common.c
@@ -61,7 +61,8 @@
  *
  * DO NOT REUSE THESE IDs with any other driver!!  Ever!!
  * Instead:  allocate your own, using normal USB-IF procedures. */
-#define FSG_VENDOR_ID	0x0525	/* NetChip */
+//#define FSG_VENDOR_ID	0x0525	/* NetChip */
+#define FSG_VENDOR_ID	0x1286	/* Marvell */
 #define FSG_PRODUCT_ID	0xa4a5	/* Linux-USB File-backed Storage Gadget */
 
 
@@ -292,8 +293,8 @@ static struct fsg_lun *fsg_lun_from_dev(struct device *dev)
 /* Number of buffers we will use.  2 is enough for double-buffering */
 #define FSG_NUM_BUFFERS	2
 
-/* Default size of buffer length. */
-#define FSG_BUFLEN	((u32)16384)
+/* Default size of buffer length. ((u32)16384)*/
+#define FSG_BUFLEN	((u32)32768)
 
 /* Maximal number of LUNs supported in mass storage function */
 #define FSG_MAX_LUNS	8
diff --git a/drivers/usb/host/ehci-hcd.c b/drivers/usb/host/ehci-hcd.c
index 97af916..ebc74a3 100644
--- a/drivers/usb/host/ehci-hcd.c
+++ b/drivers/usb/host/ehci-hcd.c
@@ -229,6 +229,12 @@ static void tdi_reset (struct ehci_hcd *ehci)
 	reg_ptr = (u32 __iomem *)(((u8 __iomem *)ehci->regs) + USBMODE);
 	tmp = ehci_readl(ehci, reg_ptr);
 	tmp |= USBMODE_CM_HC;
+
+	/*
+	 * MRVL: Disable USB Streaming
+	 */
+	tmp |= (1 << 4);
+
 	/* The default byte access to MMR space is LE after
 	 * controller reset. Set the required endian mode
 	 * for transfer buffers to match the host microprocessor
@@ -1157,6 +1163,11 @@ MODULE_LICENSE ("GPL");
 #define	PLATFORM_DRIVER		ehci_orion_driver
 #endif
 
+#ifdef CONFIG_PLAT_ARMADA
+#include "ehci_marvell.c"
+#define	PLATFORM_DRIVER		ehci_marvell_driver
+#endif
+
 #ifdef CONFIG_ARCH_IXP4XX
 #include "ehci-ixp4xx.c"
 #define	PLATFORM_DRIVER		ixp4xx_ehci_driver
diff --git a/drivers/usb/misc/usbtest.c b/drivers/usb/misc/usbtest.c
index 55f654f..8600ae7 100644
--- a/drivers/usb/misc/usbtest.c
+++ b/drivers/usb/misc/usbtest.c
@@ -13,17 +13,16 @@
 
 /*-------------------------------------------------------------------------*/
 
-// FIXME make these public somewhere; usbdevfs.h?
-//
+/* FIXME make these public somewhere; usbdevfs.h? */
 struct usbtest_param {
-	// inputs
+	/* inputs */
 	unsigned		test_num;	/* 0..(TEST_CASES-1) */
 	unsigned		iterations;
 	unsigned		length;
 	unsigned		vary;
 	unsigned		sglen;
 
-	// outputs
+	/* outputs */
 	struct timeval		duration;
 };
 #define USBTEST_REQUEST	_IOWR('U', 100, struct usbtest_param)
@@ -62,6 +61,7 @@ struct usbtest_dev {
 	struct usbtest_info	*info;
 	int			in_pipe;
 	int			out_pipe;
+	struct usb_endpoint_descriptor	*in_desc, *out_desc;
 	int			in_iso_pipe;
 	int			out_iso_pipe;
 	struct usb_endpoint_descriptor	*iso_in, *iso_out;
@@ -77,7 +77,7 @@ static struct usb_device *testdev_to_usbdev (struct usbtest_dev *test)
 }
 
 /* set up all urbs so they can be used with either bulk or interrupt */
-#define	INTERRUPT_RATE		1	/* msec/transfer */
+#define UNLINK_RATE     1   /* msec */
 
 #define ERROR(tdev, fmt, args...) \
 	dev_err(&(tdev)->intf->dev , fmt , ## args)
@@ -103,7 +103,7 @@ get_endpoints (struct usbtest_dev *dev, struct usb_interface *intf)
 		alt = intf->altsetting + tmp;
 
 		/* take the first altsetting with in-bulk + out-bulk;
-		 * ignore other endpoints and altsetttings.
+		 * ignore other endpoints and altsettings.
 		 */
 		for (ep = 0; ep < alt->desc.bNumEndpoints; ep++) {
 			struct usb_host_endpoint	*e;
@@ -111,11 +111,12 @@ get_endpoints (struct usbtest_dev *dev, struct usb_interface *intf)
 			e = alt->endpoint + ep;
 			switch (e->desc.bmAttributes) {
 			case USB_ENDPOINT_XFER_BULK:
+			case USB_ENDPOINT_XFER_INT:
 				break;
 			case USB_ENDPOINT_XFER_ISOC:
 				if (dev->info->iso)
 					goto try_iso;
-				// FALLTHROUGH
+				/* FALLTHROUGH */
 			default:
 				continue;
 			}
@@ -142,9 +143,9 @@ try_iso:
 	return -EINVAL;
 
 found:
-	udev = testdev_to_usbdev (dev);
+	udev = testdev_to_usbdev(dev);
 	if (alt->desc.bAlternateSetting != 0) {
-		tmp = usb_set_interface (udev,
+		tmp = usb_set_interface(udev,
 				alt->desc.bInterfaceNumber,
 				alt->desc.bAlternateSetting);
 		if (tmp < 0)
@@ -152,18 +153,33 @@ found:
 	}
 
 	if (in) {
-		dev->in_pipe = usb_rcvbulkpipe (udev,
-			in->desc.bEndpointAddress & USB_ENDPOINT_NUMBER_MASK);
-		dev->out_pipe = usb_sndbulkpipe (udev,
-			out->desc.bEndpointAddress & USB_ENDPOINT_NUMBER_MASK);
+        if(in->desc.bmAttributes == USB_ENDPOINT_XFER_INT) {
+            dev->in_pipe = usb_rcvintpipe (udev, 
+                        in->desc.bEndpointAddress & USB_ENDPOINT_NUMBER_MASK);
+            if(out)         
+                dev->out_pipe = usb_sndintpipe (udev, 
+                        out->desc.bEndpointAddress & USB_ENDPOINT_NUMBER_MASK);
+        }
+        else {
+			dev->in_pipe = usb_rcvbulkpipe (udev,
+				in->desc.bEndpointAddress & USB_ENDPOINT_NUMBER_MASK);
+			if(out)
+				dev->out_pipe = usb_sndbulkpipe (udev,
+					out->desc.bEndpointAddress & USB_ENDPOINT_NUMBER_MASK);
+        }
+        dev->in_desc = &in->desc;
+        dev->out_desc = &out->desc;
 	}
 	if (iso_in) {
 		dev->iso_in = &iso_in->desc;
-		dev->in_iso_pipe = usb_rcvisocpipe (udev,
+		dev->in_iso_pipe = usb_rcvisocpipe(udev,
 				iso_in->desc.bEndpointAddress
 					& USB_ENDPOINT_NUMBER_MASK);
+	}
+
+	if (iso_out) {
 		dev->iso_out = &iso_out->desc;
-		dev->out_iso_pipe = usb_sndisocpipe (udev,
+		dev->out_iso_pipe = usb_sndisocpipe(udev,
 				iso_out->desc.bEndpointAddress
 					& USB_ENDPOINT_NUMBER_MASK);
 	}
@@ -186,19 +202,29 @@ static void simple_callback (struct urb *urb)
 
 static struct urb *simple_alloc_urb (
 	struct usb_device	*udev,
-	int			pipe,
+	int					pipe,
+	struct usb_endpoint_descriptor *desc,
 	unsigned long		bytes
 )
 {
 	struct urb		*urb;
 
+	if (bytes < 0)
+		return NULL;
+
 	urb = usb_alloc_urb (0, GFP_KERNEL);
 	if (!urb)
 		return urb;
+
 	usb_fill_bulk_urb (urb, udev, pipe, NULL, bytes, simple_callback, NULL);
-	urb->interval = (udev->speed == USB_SPEED_HIGH)
-			? (INTERRUPT_RATE << 3)
-			: INTERRUPT_RATE;
+
+    if(desc != NULL) {
+        if( (udev->speed == USB_SPEED_HIGH) || 
+            ((desc->bmAttributes & USB_ENDPOINT_XFERTYPE_MASK) == USB_ENDPOINT_XFER_ISOC) )
+            urb->interval = (1 << (desc->bInterval - 1));
+        else
+            urb->interval = desc->bInterval;
+    }
 	urb->transfer_flags = URB_NO_TRANSFER_DMA_MAP;
 	if (usb_pipein (pipe))
 		urb->transfer_flags |= URB_SHORT_NOT_OK;
@@ -249,12 +275,12 @@ static inline int simple_check_buf(struct usbtest_dev *tdev, struct urb *urb)
 		case 0:
 			expected = 0;
 			break;
-		/* mod63 stays in sync with short-terminated transfers,
+		/* mod255 stays in sync with short-terminated transfers,
 		 * or otherwise when host and gadget agree on how large
 		 * each usb transfer request should be.  resync is done
 		 * with set_interface or set_config.
 		 */
-		case 1:			/* mod63 */
+		case 1:         /* mod 255 */
 			expected = i % 63;
 			break;
 		/* always fail unsupported patterns */
@@ -272,9 +298,12 @@ static inline int simple_check_buf(struct usbtest_dev *tdev, struct urb *urb)
 
 static void simple_free_urb (struct urb *urb)
 {
-	usb_buffer_free (urb->dev, urb->transfer_buffer_length,
-			urb->transfer_buffer, urb->transfer_dma);
-	usb_free_urb (urb);
+	if(urb != NULL)
+	{	
+		if((urb->transfer_buffer != NULL) && (urb->transfer_buffer_length > 0))
+			usb_free_coherent(urb->dev, urb->transfer_buffer_length, urb->transfer_buffer, urb->transfer_dma);
+		usb_free_urb (urb);
+	}
 }
 
 static int simple_io (
@@ -358,6 +387,7 @@ alloc_sglist (int nents, int max, int vary)
 	sg = kmalloc (nents * sizeof *sg, GFP_KERNEL);
 	if (!sg)
 		return NULL;
+	memset(sg, 0, nents * sizeof *sg);
 	sg_init_table(sg, nents);
 
 	for (i = 0; i < nents; i++) {
@@ -396,25 +426,32 @@ alloc_sglist (int nents, int max, int vary)
 
 static int perform_sglist (
 	struct usbtest_dev	*tdev,
-	unsigned		iterations,
-	int			pipe,
+	unsigned			iterations,
+	int					pipe,
+	struct usb_endpoint_descriptor *desc,
 	struct usb_sg_request	*req,
 	struct scatterlist	*sg,
-	int			nents
+	int					nents
 )
 {
 	struct usb_device	*udev = testdev_to_usbdev(tdev);
 	int			retval = 0;
+	int         interval = 0;
 
 	while (retval == 0 && iterations-- > 0) {
-		retval = usb_sg_init (req, udev, pipe,
-				(udev->speed == USB_SPEED_HIGH)
-					? (INTERRUPT_RATE << 3)
-					: INTERRUPT_RATE,
-				sg, nents, 0, GFP_KERNEL);
-
+        if(desc != NULL) {
+            if( (udev->speed == USB_SPEED_HIGH) || 
+                ( (desc->bmAttributes & USB_ENDPOINT_XFERTYPE_MASK) == USB_ENDPOINT_XFER_ISOC) )
+                interval = (1 << (desc->bInterval - 1));
+            else
+                interval = desc->bInterval;
+        }
+
+		retval = usb_sg_init (req, udev, pipe, interval,
+				                sg, nents, 0, GFP_KERNEL);		
 		if (retval)
 			break;
+
 		usb_sg_wait (req);
 		retval = req->status;
 
@@ -973,7 +1010,7 @@ test_ctrl_queue (struct usbtest_dev *dev, struct usbtest_param *param)
 			goto cleanup;
 		}
 		req.wLength = cpu_to_le16 (len);
-		urb [i] = u = simple_alloc_urb (udev, pipe, len);
+		urb [i] = u = simple_alloc_urb (udev, pipe, NULL, len);
 		if (!u)
 			goto cleanup;
 
@@ -1044,14 +1081,15 @@ static void unlink1_callback (struct urb *urb)
 	}
 }
 
-static int unlink1 (struct usbtest_dev *dev, int pipe, int size, int async)
+static int unlink1 (struct usbtest_dev *dev, int pipe, 
+                    struct usb_endpoint_descriptor *desc, int size, int async)
 {
 	struct urb		*urb;
 	struct completion	completion;
 	int			retval = 0;
 
 	init_completion (&completion);
-	urb = simple_alloc_urb (testdev_to_usbdev (dev), pipe, size);
+	urb = simple_alloc_urb (testdev_to_usbdev (dev), pipe, desc, size);
 	if (!urb)
 		return -ENOMEM;
 	urb->context = &completion;
@@ -1071,7 +1109,7 @@ static int unlink1 (struct usbtest_dev *dev, int pipe, int size, int async)
 	/* unlinking that should always work.  variable delay tests more
 	 * hcd states and code paths, even with little other system load.
 	 */
-	msleep (jiffies % (2 * INTERRUPT_RATE));
+	msleep (jiffies % (2 * UNLINK_RATE));
 	if (async) {
 		while (!completion_done(&completion)) {
 			retval = usb_unlink_urb(urb);
@@ -1113,14 +1151,15 @@ static int unlink1 (struct usbtest_dev *dev, int pipe, int size, int async)
 				0 : retval - 2000;
 }
 
-static int unlink_simple (struct usbtest_dev *dev, int pipe, int len)
+static int unlink_simple (struct usbtest_dev *dev, int pipe, 
+                          struct usb_endpoint_descriptor *desc, int len)
 {
 	int			retval = 0;
 
 	/* test sync and async paths */
-	retval = unlink1 (dev, pipe, len, 1);
+	retval = unlink1 (dev, pipe, desc, len, 1);
 	if (!retval)
-		retval = unlink1 (dev, pipe, len, 0);
+		retval = unlink1 (dev, pipe, desc, len, 0);
 	return retval;
 }
 
@@ -1217,7 +1256,7 @@ static int halt_simple (struct usbtest_dev *dev)
 	int		retval = 0;
 	struct urb	*urb;
 
-	urb = simple_alloc_urb (testdev_to_usbdev (dev), 0, 512);
+	urb = simple_alloc_urb (testdev_to_usbdev (dev), 0, NULL, 512);
 	if (urb == NULL)
 		return -ENOMEM;
 
@@ -1448,12 +1487,12 @@ test_iso_queue (struct usbtest_dev *dev, struct usbtest_param *param,
 {
 	struct iso_context	context;
 	struct usb_device	*udev;
-	unsigned		i;
+	unsigned			i;
 	unsigned long		packets = 0;
-	int			status = 0;
-	struct urb		*urbs[10];	/* FIXME no limit */
+	int					status = 0;
+	struct urb			*urbs[50];  /* FIXME no limit */
 
-	if (param->sglen > 10)
+	if (param->sglen > 50)
 		return -EDOM;
 
 	memset(&context, 0, sizeof context);
@@ -1497,9 +1536,6 @@ test_iso_queue (struct usbtest_dev *dev, struct usbtest_param *param,
 				spin_unlock_irq (&context.lock);
 				goto fail;
 			}
-
-			simple_free_urb (urbs [i]);
-			urbs[i] = NULL;
 			context.pending--;
 			context.submit_error = 1;
 			break;
@@ -1509,10 +1545,6 @@ test_iso_queue (struct usbtest_dev *dev, struct usbtest_param *param,
 
 	wait_for_completion (&context.done);
 
-	for (i = 0; i < param->sglen; i++) {
-		if (urbs[i])
-			simple_free_urb(urbs[i]);
-	}
 	/*
 	 * Isochronous transfers are expected to fail sometimes.  As an
 	 * arbitrary limit, we will report an error if any submissions
@@ -1524,12 +1556,11 @@ test_iso_queue (struct usbtest_dev *dev, struct usbtest_param *param,
 		status = -EACCES;
 	else if (context.errors > context.packet_count / 10)
 		status = -EIO;
-	return status;
 
 fail:
 	for (i = 0; i < param->sglen; i++) {
 		if (urbs [i])
-			simple_free_urb (urbs [i]);
+			simple_free_urb (urbs [i]);	
 	}
 	return status;
 }
@@ -1630,7 +1661,7 @@ usbtest_ioctl (struct usb_interface *intf, unsigned int code, void *buf)
 		dev_info(&intf->dev,
 				"TEST 1:  write %d bytes %u times\n",
 				param->length, param->iterations);
-		urb = simple_alloc_urb (udev, dev->out_pipe, param->length);
+		urb = simple_alloc_urb (udev, dev->out_pipe, dev->out_desc, param->length);
 		if (!urb) {
 			retval = -ENOMEM;
 			break;
@@ -1645,7 +1676,7 @@ usbtest_ioctl (struct usb_interface *intf, unsigned int code, void *buf)
 		dev_info(&intf->dev,
 				"TEST 2:  read %d bytes %u times\n",
 				param->length, param->iterations);
-		urb = simple_alloc_urb (udev, dev->in_pipe, param->length);
+		urb = simple_alloc_urb (udev, dev->in_pipe, dev->in_desc, param->length);
 		if (!urb) {
 			retval = -ENOMEM;
 			break;
@@ -1660,7 +1691,7 @@ usbtest_ioctl (struct usb_interface *intf, unsigned int code, void *buf)
 		dev_info(&intf->dev,
 				"TEST 3:  write/%d 0..%d bytes %u times\n",
 				param->vary, param->length, param->iterations);
-		urb = simple_alloc_urb (udev, dev->out_pipe, param->length);
+		urb = simple_alloc_urb (udev, dev->out_pipe, dev->out_desc, param->length);
 		if (!urb) {
 			retval = -ENOMEM;
 			break;
@@ -1676,7 +1707,7 @@ usbtest_ioctl (struct usb_interface *intf, unsigned int code, void *buf)
 		dev_info(&intf->dev,
 				"TEST 4:  read/%d 0..%d bytes %u times\n",
 				param->vary, param->length, param->iterations);
-		urb = simple_alloc_urb (udev, dev->in_pipe, param->length);
+		urb = simple_alloc_urb (udev, dev->in_pipe, dev->in_desc, param->length);
 		if (!urb) {
 			retval = -ENOMEM;
 			break;
@@ -1702,7 +1733,7 @@ usbtest_ioctl (struct usb_interface *intf, unsigned int code, void *buf)
 		}
 		// FIRMWARE:  bulk sink (maybe accepts short writes)
 		retval = perform_sglist(dev, param->iterations, dev->out_pipe,
-				&req, sg, param->sglen);
+				dev->out_desc, &req, sg, param->sglen);
 		free_sglist (sg, param->sglen);
 		break;
 
@@ -1720,7 +1751,7 @@ usbtest_ioctl (struct usb_interface *intf, unsigned int code, void *buf)
 		}
 		// FIRMWARE:  bulk source (maybe generates short writes)
 		retval = perform_sglist(dev, param->iterations, dev->in_pipe,
-				&req, sg, param->sglen);
+				dev->in_desc, &req, sg, param->sglen);
 		free_sglist (sg, param->sglen);
 		break;
 	case 7:
@@ -1737,7 +1768,7 @@ usbtest_ioctl (struct usb_interface *intf, unsigned int code, void *buf)
 		}
 		// FIRMWARE:  bulk sink (maybe accepts short writes)
 		retval = perform_sglist(dev, param->iterations, dev->out_pipe,
-				&req, sg, param->sglen);
+				dev->out_desc, &req, sg, param->sglen);
 		free_sglist (sg, param->sglen);
 		break;
 	case 8:
@@ -1754,7 +1785,7 @@ usbtest_ioctl (struct usb_interface *intf, unsigned int code, void *buf)
 		}
 		// FIRMWARE:  bulk source (maybe generates short writes)
 		retval = perform_sglist(dev, param->iterations, dev->in_pipe,
-				&req, sg, param->sglen);
+				dev->in_desc, &req, sg, param->sglen);
 		free_sglist (sg, param->sglen);
 		break;
 
@@ -1791,7 +1822,7 @@ usbtest_ioctl (struct usb_interface *intf, unsigned int code, void *buf)
 		dev_info(&intf->dev, "TEST 11:  unlink %d reads of %d\n",
 				param->iterations, param->length);
 		for (i = param->iterations; retval == 0 && i--; /* NOP */)
-			retval = unlink_simple (dev, dev->in_pipe,
+			retval = unlink_simple (dev, dev->in_pipe, dev->in_desc,
 						param->length);
 		if (retval)
 			dev_err(&intf->dev, "unlink reads failed %d, "
@@ -1804,7 +1835,7 @@ usbtest_ioctl (struct usb_interface *intf, unsigned int code, void *buf)
 		dev_info(&intf->dev, "TEST 12:  unlink %d writes of %d\n",
 				param->iterations, param->length);
 		for (i = param->iterations; retval == 0 && i--; /* NOP */)
-			retval = unlink_simple (dev, dev->out_pipe,
+			retval = unlink_simple (dev, dev->out_pipe, dev->out_desc,
 						param->length);
 		if (retval)
 			dev_err(&intf->dev, "unlink writes failed %d, "
@@ -1970,10 +2001,22 @@ usbtest_probe (struct usb_interface *intf, const struct usb_device_id *id)
 				dev->out_pipe = usb_sndbulkpipe (udev,
 							info->ep_out);
 		}
-		if (dev->in_pipe)
-			rtest = " bulk-in";
-		if (dev->out_pipe)
-			wtest = " bulk-out";
+        if (dev->in_pipe) {
+            if(usb_pipeint(dev->in_pipe)) {
+                rtest = " intr-in";
+            }
+            else {
+                rtest = " bulk-in";
+            }
+        }           
+        if (dev->out_pipe){
+            if(usb_pipeint(dev->out_pipe)) {
+                wtest = " intr-out";
+            }
+            else {
+                wtest = " bulk-out";
+            }
+        }           
 		if (dev->in_iso_pipe)
 			irtest = " iso-in";
 		if (dev->out_iso_pipe)
diff --git a/fs/ext4/file.c b/fs/ext4/file.c
index 5313ae4..47b7fb6 100644
--- a/fs/ext4/file.c
+++ b/fs/ext4/file.c
@@ -145,6 +145,7 @@ const struct file_operations ext4_file_operations = {
 	.fsync		= ext4_sync_file,
 	.splice_read	= generic_file_splice_read,
 	.splice_write	= generic_file_splice_write,
+	.splice_from_socket = generic_splice_from_socket,
 };
 
 const struct inode_operations ext4_file_inode_operations = {
diff --git a/fs/read_write.c b/fs/read_write.c
index 2c3faa9..d1c9f8b 100644
--- a/fs/read_write.c
+++ b/fs/read_write.c
@@ -852,6 +852,8 @@ static ssize_t do_sendfile(int out_fd, int in_fd, loff_t *ppos,
 	if (!(out_file->f_mode & FMODE_WRITE))
 		goto fput_out;
 	retval = -EINVAL;
+	if (!out_file->f_op || !out_file->f_op->sendpage)
+		goto fput_out;
 	in_inode = in_file->f_path.dentry->d_inode;
 	out_inode = out_file->f_path.dentry->d_inode;
 	retval = rw_verify_area(WRITE, out_file, &out_file->f_pos, count);
@@ -863,6 +865,9 @@ static ssize_t do_sendfile(int out_fd, int in_fd, loff_t *ppos,
 		max = min(in_inode->i_sb->s_maxbytes, out_inode->i_sb->s_maxbytes);
 
 	pos = *ppos;
+	retval = -EINVAL;
+	if (unlikely(pos < 0))
+		goto fput_out;
 	if (unlikely(pos + count > max)) {
 		retval = -EOVERFLOW;
 		if (pos >= max)
diff --git a/fs/splice.c b/fs/splice.c
index f0273a5..6d5c2a2 100644
--- a/fs/splice.c
+++ b/fs/splice.c
@@ -32,6 +32,14 @@
 #include <linux/security.h>
 #include <linux/gfp.h>
 #include <linux/socket.h>
+#include <net/sock.h>
+#include <linux/net.h>
+#include <linux/genalloc.h>
+
+struct common_mempool;
+static struct common_mempool/*struct gen_pool*/ * rcv_pool = NULL;
+static struct common_mempool/*struct gen_pool*/ * kvec_pool = NULL;
+
 
 /*
  * Attempt to steal a page from a pipe buffer. This should perhaps go into
@@ -496,7 +505,6 @@ ssize_t generic_file_splice_read(struct file *in, loff_t *ppos,
 	ret = __generic_file_splice_read(in, ppos, pipe, len, flags);
 	if (ret > 0) {
 		*ppos += ret;
-		file_accessed(in);
 	}
 
 	return ret;
@@ -645,11 +645,8 @@ static int pipe_to_sendpage(struct pipe_inode_info *pipe,
 		if (sd->len < sd->total_len && pipe->nrbufs > 1)
 			more |= MSG_SENDPAGE_NOTLAST;
 
-		if (file->f_op && file->f_op->sendpage)
 			ret = file->f_op->sendpage(file, buf->page, buf->offset,
 						   sd->len, &pos, more);
-		else
-			ret = -EINVAL;
 	}
 
 	return ret;
@@ -959,10 +965,8 @@ generic_file_splice_write(struct pipe_inode_info *pipe, struct file *out,
 
 		mutex_lock_nested(&inode->i_mutex, I_MUTEX_CHILD);
 		ret = file_remove_suid(out);
-		if (!ret) {
-			file_update_time(out);
+		if (!ret)
 			ret = splice_from_pipe_feed(pipe, &sd, pipe_to_file);
-		}
 		mutex_unlock(&inode->i_mutex);
 	} while (ret > 0);
 	splice_from_pipe_end(pipe, &sd);
@@ -1062,9 +1066,8 @@ long vfs_splice_from(struct pipe_inode_info *pipe, struct file *out,
 	if (unlikely(ret < 0))
 		return ret;
 
-	if (out->f_op && out->f_op->splice_write)
 		splice_write = out->f_op->splice_write;
-	else
+	if (!splice_write)
 		splice_write = default_file_splice_write;
 
 	return splice_write(pipe, out, ppos, len, flags);
@@ -1089,9 +1092,8 @@ long vfs_splice_to(struct file *in, loff_t *ppos,
 	if (unlikely(ret < 0))
 		return ret;
 
-	if (in->f_op && in->f_op->splice_read)
 		splice_read = in->f_op->splice_read;
-	else
+	if (!splice_read)
 		splice_read = default_file_splice_read;
 
 	return splice_read(in, ppos, pipe, len, flags);
@@ -1354,6 +1356,402 @@ static long do_splice(struct file *in, loff_t __user *off_in,
 	return -EINVAL;
 }
 
+/****************************** POOL MANAGER *************************************/
+/* Forward declarations */
+typedef struct common_mempool common_mempool_t;
+void* common_mempool_alloc(common_mempool_t* pool);
+void common_mempool_free(common_mempool_t* pool, void* mem);
+common_mempool_t* common_mempool_get(void* mem);
+common_mempool_t*  common_mempool_create(uint32_t number_of_entries, uint32_t entry_size);
+void  common_mempool_destroy(common_mempool_t* pool);
+int32_t common_mempool_get_number_of_free_entries(common_mempool_t* pool);
+int32_t common_mempool_get_number_of_entries(common_mempool_t* pool);
+int32_t common_mempool_get_entry_size(common_mempool_t* pool);
+
+/* Implementation */
+#define COMMON_MPOOL_HDR_FLAGS_ALLOCATED 0x00000001
+#define COMMON_MPOOL_HDR_MAGIC           0xa5a5a508
+#define COMMON_MPOOL_FTR_MAGIC           0xa5a5a509
+#define COMMON_MPOOL_ALIGN4(size) ((size)+4) & 0xFFFFFFFC;
+#define COMMON_MPOOL_CHECK_ALIGNED4(ptr) ((((uint32_t)(ptr)) & 0x00000003) == 0)
+
+typedef struct common_mpool_hdr
+{
+  struct common_mpool_hdr* next;
+  common_mempool_t*        pool;
+  uint32_t flags;
+  uint32_t magic;
+} common_mpool_hdr_t;
+
+typedef struct
+{
+	uint32_t magic;
+	common_mempool_t* pool;
+} common_mpool_ftr_t;
+
+struct common_mempool
+{
+	common_mpool_hdr_t*  head;
+	common_mpool_hdr_t*  tail;
+	uint32_t		number_of_free_entries;
+	spinlock_t		lock;
+	uint32_t                 data_size; /* size of data section in pool entry */
+	uint32_t                 pool_entry_size; /* size of pool entry */
+	/* parameters passed on init */
+	uint32_t                 number_of_entries;
+	uint32_t                 entry_size;
+	uint8_t*                 mem;
+};
+
+bool common_mempool_check_internal(common_mempool_t * pool,
+                                          void * ptr, 
+                                          common_mpool_hdr_t * hdr,
+                                          common_mpool_ftr_t * ftr)
+{
+	if (!ptr) {
+		printk(KERN_ERR "illegal ptr NULL");
+		return false;
+	}
+
+	if (!COMMON_MPOOL_CHECK_ALIGNED4(ptr)) {
+		printk(KERN_ERR "ptr not aligned %p",ptr);
+		return false;
+	}
+
+	if (hdr->magic != COMMON_MPOOL_HDR_MAGIC) {
+		printk(KERN_ERR "illegal hdr magic %x for ptr %p",hdr->magic,ptr);
+		return false;
+	}
+
+	if (ftr->magic != COMMON_MPOOL_FTR_MAGIC) {
+		printk(KERN_ERR "illegal ftr magic %x for ptr %p",ftr->magic,ptr);
+		return false;
+	}
+
+	if (hdr->pool != pool || ftr->pool != pool) {
+		printk(KERN_ERR "inconsistent size hdr->pool: %p ftr->pool: %p for ptr %p",hdr->pool,ftr->pool,ptr);
+		return false;
+	}
+
+	if (!(hdr->flags & COMMON_MPOOL_HDR_FLAGS_ALLOCATED)) {
+		printk(KERN_ERR "ptr %p was not allocated",ptr);
+		return false;
+	}
+	return true;
+}
+
+void* common_mempool_alloc(common_mempool_t* pool)
+{
+	common_mpool_hdr_t* hdr;
+
+	if (!pool || !pool->head || pool->number_of_free_entries == 0) {
+		return NULL;
+	}
+	spin_lock_bh(&pool->lock);
+	hdr = pool->head; 
+	pool->head = pool->head->next;
+
+	if (!pool->head) {
+		pool->tail = NULL;
+	}
+
+	hdr->flags = COMMON_MPOOL_HDR_FLAGS_ALLOCATED;
+	pool->number_of_free_entries--;
+	spin_unlock_bh(&pool->lock);
+	return ((uint8_t*)hdr+sizeof(common_mpool_hdr_t));
+}
+
+void common_mempool_free(common_mempool_t* pool, void* ptr)
+{
+	common_mpool_hdr_t* hdr;
+	common_mpool_ftr_t* ftr;
+
+	if (!pool || !ptr) {
+		return;
+	}
+	if (!COMMON_MPOOL_CHECK_ALIGNED4(ptr)) {
+		printk(KERN_ERR "ptr not aligned %p",ptr);
+		return;
+	}
+	spin_lock_bh(&pool->lock);
+	hdr = (common_mpool_hdr_t*)((uint8_t*)ptr-sizeof(common_mpool_hdr_t));
+	ftr = (common_mpool_ftr_t*)((uint8_t*)ptr+pool->data_size);
+
+	if (!common_mempool_check_internal(pool,ptr,hdr,ftr)) {
+		printk(KERN_ERR "invalid ptr %p",ptr);
+		spin_unlock_bh(&pool->lock);
+		return;
+	}
+
+	hdr->flags ^= COMMON_MPOOL_HDR_FLAGS_ALLOCATED;
+	hdr->next = NULL;
+
+	if (!pool->head) {
+		pool->head = pool->tail = hdr;
+	} else {
+		pool->tail->next = hdr;
+		pool->tail = hdr;
+	}
+
+	pool->number_of_free_entries++;
+	spin_unlock_bh(&pool->lock);
+}
+
+common_mempool_t*  common_mempool_create(uint32_t number_of_entries,
+						uint32_t entry_size)
+{
+	uint32_t i;
+	uint32_t aligned_entry_size = COMMON_MPOOL_ALIGN4(entry_size);
+	uint32_t pool_entry_size = COMMON_MPOOL_ALIGN4(sizeof(common_mpool_hdr_t)+aligned_entry_size+sizeof(common_mpool_ftr_t));
+	common_mpool_hdr_t* hdr;
+	common_mpool_hdr_t* next_hdr;
+	common_mpool_ftr_t* ftr;
+	common_mempool_t* pool;
+
+	pool = kmalloc((sizeof(common_mempool_t) + pool_entry_size*number_of_entries), GFP_ATOMIC);
+
+	if (!pool) {
+		return NULL;
+	}
+
+	pool->entry_size = entry_size;
+	pool->number_of_entries = number_of_entries;
+	pool->data_size  = aligned_entry_size;
+	pool->pool_entry_size = pool_entry_size;
+	pool->number_of_free_entries = number_of_entries;
+	pool->mem = (uint8_t*)(pool+1);
+	pool->head = (common_mpool_hdr_t*)pool->mem;
+	spin_lock_init(&pool->lock);
+
+	for (i=0;i<number_of_entries;i++) {
+		hdr = (common_mpool_hdr_t*)&pool->mem[pool_entry_size*i];
+		ftr = (common_mpool_ftr_t*)((uint8_t*)hdr+sizeof(common_mpool_hdr_t)+aligned_entry_size);
+		hdr->magic = COMMON_MPOOL_HDR_MAGIC;
+		hdr->pool = pool;
+		hdr->flags = 0;
+		ftr->magic = COMMON_MPOOL_FTR_MAGIC;
+		ftr->pool = pool;
+
+		if (i < (number_of_entries-1)) {
+			next_hdr = (common_mpool_hdr_t*)&pool->mem[pool_entry_size*(i+1)];
+		} else {
+			pool->tail = hdr;
+			next_hdr = NULL;
+		}
+
+		hdr->next = next_hdr;
+	}
+	return pool;
+}
+
+void  common_mempool_destroy(common_mempool_t* pool)
+{
+	if (!pool) {
+		return;
+	}
+
+	kfree(pool);
+}
+
+int32_t common_mempool_get_number_of_free_entries(common_mempool_t* pool)
+{
+	if (!pool) {
+		return -1;
+	}
+
+	return (int32_t)pool->number_of_free_entries;
+}
+
+int32_t common_mempool_get_number_of_entries(common_mempool_t* pool)
+{
+	if (!pool) {
+		return -1;
+	}
+	return (int32_t)pool->number_of_entries;
+}
+
+int32_t common_mempool_get_entry_size(common_mempool_t* pool)
+{
+	if (!pool) {
+		return -1;
+	}
+	return (int32_t)pool->entry_size;
+}
+
+common_mempool_t* common_mempool_get(void* ptr)
+{
+	common_mpool_hdr_t* hdr;
+	common_mpool_ftr_t* ftr;
+
+	if (!ptr) {
+		return NULL;
+	}
+	if (!COMMON_MPOOL_CHECK_ALIGNED4(ptr)) {
+		return NULL;
+	}
+	hdr = (common_mpool_hdr_t*)((uint8_t*)ptr-sizeof(common_mpool_hdr_t));
+	ftr = (common_mpool_ftr_t*)((uint8_t*)ptr + hdr->pool->data_size);
+
+	if (hdr->magic != COMMON_MPOOL_HDR_MAGIC) {
+		printk(KERN_ERR "illegal hdr magic %x for ptr %p",hdr->magic,ptr);
+		return NULL;
+	}
+	if (ftr->magic != COMMON_MPOOL_FTR_MAGIC) {
+		printk(KERN_ERR "illegal ftr magic %x for ptr %p",ftr->magic,ptr);
+		return NULL;
+	}
+	if (hdr->pool != ftr->pool || !hdr->pool) {
+		printk(KERN_ERR "inconsistent size hdr->pool: %p ftr->pool: %p for ptr %p",hdr->pool,ftr->pool,ptr);
+		return false;
+	}
+	return hdr->pool;
+}
+/****************************** POOL MANAGER *************************************/
+
+ssize_t generic_splice_from_socket(struct file *file, struct socket *sock,
+				     loff_t __user *ppos, size_t count)
+{
+	struct address_space *mapping = file->f_mapping;
+	struct inode *inode = mapping->host;
+	loff_t pos;
+	int count_tmp;
+	int err = 0;
+	int i = 0;
+	int nr_pages = 0;
+	int page_cnt_est= count/PAGE_SIZE + 1;
+	struct recvfile_ctl_blk *rv_cb;
+	struct kvec *iov;
+	struct msghdr msg;
+	long rcvtimeo;
+	int ret;
+
+	if (copy_from_user(&pos, ppos, sizeof(loff_t)))
+		return -EFAULT;
+
+	if (count > MAX_PAGES_PER_RECVFILE * PAGE_SIZE) {
+		printk("%s: count(%u) exceeds maxinum\n", __func__, count);
+		return -EINVAL;
+	}
+	mutex_lock(&inode->i_mutex);
+
+	vfs_check_frozen(inode->i_sb, SB_FREEZE_WRITE);
+
+	/* We can write back this queue in page reclaim */
+	current->backing_dev_info = mapping->backing_dev_info;
+
+	err = generic_write_checks(file, &pos, &count, S_ISBLK(inode->i_mode));
+	if (err != 0 || count == 0)
+		goto done;
+
+	file_remove_suid(file);
+	file_update_time(file);
+
+	if (unlikely(!rcv_pool || !kvec_pool))
+	{
+		printk(KERN_ERR "rcv_pool %p kvec_pool %p uninitialized %d\n", rcv_pool, kvec_pool);
+		return -ENOMEM;
+	}
+
+	rv_cb = (struct recvfile_ctl_blk *)common_mempool_alloc(rcv_pool);
+	iov = (struct kvec *)common_mempool_alloc(kvec_pool);
+
+	if (!rv_cb || !iov)
+	{
+		printk(KERN_ERR "Failed to get pool mem for %d pages (rv_cb %p iov %p)\n", page_cnt_est, rv_cb, iov);
+		return -ENOMEM;
+	}
+
+	count_tmp = count;
+	do {
+		unsigned long bytes;	/* Bytes to write to page */
+		unsigned long offset;	/* Offset into pagecache page */
+		struct page *pageP;
+		void *fsdata;
+
+		offset = (pos & (PAGE_CACHE_SIZE - 1));
+		bytes = PAGE_CACHE_SIZE - offset;
+		if (bytes > count_tmp)
+			bytes = count_tmp;
+		ret = mapping->a_ops->write_begin(file, mapping, pos, bytes,
+						  AOP_FLAG_UNINTERRUPTIBLE,
+						  &pageP, &fsdata);
+
+		if (unlikely(ret)) {
+			err = ret;
+			goto cleanup;
+		}
+
+		rv_cb[nr_pages].rv_page = pageP;
+		rv_cb[nr_pages].rv_pos = pos;
+		rv_cb[nr_pages].rv_count = bytes;
+		rv_cb[nr_pages].rv_fsdata = fsdata;
+		iov[nr_pages].iov_base = kmap(pageP) + offset;
+		iov[nr_pages].iov_len = bytes;
+		nr_pages++;
+		count_tmp -= bytes;
+		pos += bytes;
+	} while (count_tmp);
+
+	/* IOV is ready, receive the date from socket now */
+	msg.msg_name = NULL;
+	msg.msg_namelen = 0;
+	msg.msg_iov = (struct iovec *)&iov[0];
+	msg.msg_iovlen = nr_pages ;
+	msg.msg_control = NULL;
+	msg.msg_controllen = 0;
+	msg.msg_flags = MSG_KERNSPACE;
+	rcvtimeo = sock->sk->sk_rcvtimeo;
+	sock->sk->sk_rcvtimeo = 8 * HZ;
+
+	ret = kernel_recvmsg(sock, &msg, &iov[0], nr_pages, count,
+			     MSG_WAITALL | MSG_NOCATCHSIG);
+
+	sock->sk->sk_rcvtimeo = rcvtimeo;
+	if(ret != count)
+		err = -EPIPE;
+	else
+		err = 0;
+
+	if (unlikely(err < 0)) {
+		goto cleanup;
+	}
+
+	for(i=0,count=0;i < nr_pages;i++) {
+		kunmap(rv_cb[i].rv_page);
+		ret = mapping->a_ops->write_end(file, mapping,
+						rv_cb[i].rv_pos,
+						rv_cb[i].rv_count,
+						rv_cb[i].rv_count,
+						rv_cb[i].rv_page,
+						rv_cb[i].rv_fsdata);
+		if (unlikely(ret < 0))
+			printk("%s: write_end fail,ret = %d\n", __func__, ret);
+		count += rv_cb[i].rv_count;
+	}
+	balance_dirty_pages_ratelimited_nr(mapping, nr_pages);
+	if (copy_to_user(ppos, &pos, sizeof(loff_t)))
+		err = -EFAULT;
+done:
+	current->backing_dev_info = NULL;
+	common_mempool_free(rcv_pool, (void*)rv_cb);
+	common_mempool_free(kvec_pool, (void*)iov);
+
+	mutex_unlock(&inode->i_mutex);
+	return err ? err : count;
+cleanup:
+	for(i = 0; i < nr_pages; i++) {
+		kunmap(rv_cb[i].rv_page);
+		ret = mapping->a_ops->write_end(file, mapping,
+						rv_cb[i].rv_pos,
+						rv_cb[i].rv_count,
+						rv_cb[i].rv_count,
+						rv_cb[i].rv_page,
+						rv_cb[i].rv_fsdata);
+	}
+
+	goto done;
+}
+
 /*
  * Map an iov into an array of pages and offset/length tupples. With the
  * partial_page structure, we can map several non-contiguous ranges into
@@ -1656,11 +2054,35 @@ SYSCALL_DEFINE6(splice, int, fd_in, loff_t __user *, off_in,
 	long error;
 	struct file *in, *out;
 	int fput_in, fput_out;
+	struct socket *sock = NULL;
 
 	if (unlikely(!len))
 		return 0;
 
 	error = -EBADF;
+
+	/* check if fd_in is a socket */
+	sock = sockfd_lookup(fd_in, &error);
+	if (sock) {
+		out = NULL;
+		if (!sock->sk)
+			goto done;
+		out = fget_light(fd_out, &fput_out);
+
+		if (out) {
+			if (!(out->f_mode & FMODE_WRITE))
+				goto done;
+			if (!out->f_op->splice_from_socket)
+				goto done;
+			error = out->f_op->splice_from_socket(out, sock, off_out, len);
+		}
+done:
+		if(out)
+			fput_light(out, fput_out);
+		fput(sock->file);
+		return error;
+	}
+
 	in = fget_light(fd_in, &fput_in);
 	if (in) {
 		if (in->f_mode & FMODE_READ) {
diff --git a/fs/xfs/linux-2.6/xfs_file.c b/fs/xfs/linux-2.6/xfs_file.c
index 42dd3bc..03dcb3d 100644
--- a/fs/xfs/linux-2.6/xfs_file.c
+++ b/fs/xfs/linux-2.6/xfs_file.c
@@ -894,6 +894,60 @@ write_retry:
 	return -error;
 }
 
+STATIC ssize_t
+xfs_file_splice_from_socket(
+	struct file	*file,
+	struct socket	*sock,
+	loff_t __user	*ppos,
+	size_t count)
+{
+	struct inode		*inode = file->f_mapping->host;
+	struct xfs_inode	*ip = XFS_I(inode);
+	ssize_t			ret;
+	xfs_fsize_t		isize, new_size;
+
+	XFS_STATS_INC(xs_write_calls);
+	if (XFS_FORCED_SHUTDOWN(ip->i_mount))
+		return -EIO;
+
+	xfs_ilock(ip, XFS_IOLOCK_EXCL);
+
+	new_size = *ppos + count;
+
+	xfs_ilock(ip, XFS_ILOCK_EXCL);
+	if (new_size > ip->i_size)
+		ip->i_new_size = new_size;
+	xfs_iunlock(ip, XFS_ILOCK_EXCL);
+
+//	xfs_rw_enter_trace(XFS_SPLICE_WRITE_ENTER, ip,
+//			   pipe, count, *ppos, ioflags);
+	ret = generic_splice_from_socket(file, sock, ppos, count);
+
+	if (ret > 0)
+		XFS_STATS_ADD(xs_write_bytes, ret);
+
+	isize = i_size_read(inode);
+	if (unlikely(ret < 0 && ret != -EFAULT && *ppos > isize))
+		*ppos = isize;
+
+	if (*ppos > ip->i_size) {
+		xfs_ilock(ip, XFS_ILOCK_EXCL);
+		if (*ppos > ip->i_size)
+			ip->i_size = *ppos;
+		xfs_iunlock(ip, XFS_ILOCK_EXCL);
+	}
+
+	if (ip->i_new_size) {
+		xfs_ilock(ip, XFS_ILOCK_EXCL);
+		ip->i_new_size = 0;
+		if (ip->i_d.di_size > ip->i_size)
+			ip->i_d.di_size = ip->i_size;
+		xfs_iunlock(ip, XFS_ILOCK_EXCL);
+	}
+	xfs_iunlock(ip, XFS_IOLOCK_EXCL);
+	return ret;
+}
+
 STATIC int
 xfs_file_open(
 	struct inode	*inode,
@@ -1004,6 +1058,7 @@ const struct file_operations xfs_file_operations = {
 	.aio_write	= xfs_file_aio_write,
 	.splice_read	= xfs_file_splice_read,
 	.splice_write	= xfs_file_splice_write,
+	.splice_from_socket = xfs_file_splice_from_socket,
 	.unlocked_ioctl	= xfs_file_ioctl,
 #ifdef CONFIG_COMPAT
 	.compat_ioctl	= xfs_file_compat_ioctl,
diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 20ea12c..f8820c6 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -627,6 +627,11 @@ struct dma_pinned_list {
 struct dma_pinned_list *dma_pin_iovec_pages(struct iovec *iov, size_t len);
 void dma_unpin_iovec_pages(struct dma_pinned_list* pinned_list);
 
+#ifdef CONFIG_SPLICE_NET_DMA_SUPPORT
+struct dma_pinned_list *dma_pin_kernel_iovec_pages(struct iovec *iov, size_t len);
+void dma_unpin_kernel_iovec_pages(struct dma_pinned_list* pinned_list);
+#endif
+
 dma_cookie_t dma_memcpy_to_iovec(struct dma_chan *chan, struct iovec *iov,
 	struct dma_pinned_list *pinned_list, unsigned char *kdata, size_t len);
 dma_cookie_t dma_memcpy_pg_to_iovec(struct dma_chan *chan, struct iovec *iov,
diff --git a/include/linux/fs.h b/include/linux/fs.h
index e5cc0c0..984ad39 100644
--- a/include/linux/fs.h
+++ b/include/linux/fs.h
@@ -363,6 +363,8 @@ struct inodes_stat_t {
 #define SYNC_FILE_RANGE_WRITE		2
 #define SYNC_FILE_RANGE_WAIT_AFTER	4
 
+#define MAX_PAGES_PER_RECVFILE		64
+
 #ifdef __KERNEL__
 
 #include <linux/linkage.h>
@@ -398,6 +400,7 @@ struct kstatfs;
 struct vm_area_struct;
 struct vfsmount;
 struct cred;
+struct socket;
 
 extern void __init inode_init(void);
 extern void __init inode_init_early(void);
@@ -1513,6 +1516,8 @@ struct file_operations {
 	int (*flock) (struct file *, int, struct file_lock *);
 	ssize_t (*splice_write)(struct pipe_inode_info *, struct file *, loff_t *, size_t, unsigned int);
 	ssize_t (*splice_read)(struct file *, loff_t *, struct pipe_inode_info *, size_t, unsigned int);
+	ssize_t (*splice_from_socket)(struct file *file, struct socket *sock,
+				     loff_t __user *ppos, size_t count);
 	int (*setlease)(struct file *, long, struct file_lock **);
 };
 
@@ -2232,6 +2237,8 @@ extern ssize_t generic_splice_sendpage(struct pipe_inode_info *pipe,
 		struct file *out, loff_t *, size_t len, unsigned int flags);
 extern long do_splice_direct(struct file *in, loff_t *ppos, struct file *out,
 		size_t len, unsigned int flags);
+extern ssize_t generic_splice_from_socket(struct file *file, struct socket *sock,
+				     loff_t __user *ppos, size_t count);
 
 extern void
 file_ra_state_init(struct file_ra_state *ra, struct address_space *mapping);
diff --git a/include/linux/libata.h b/include/linux/libata.h
index 79878c2..59b7dde 100644
--- a/include/linux/libata.h
+++ b/include/linux/libata.h
@@ -727,6 +727,7 @@ struct ata_port {
 
 #ifdef CONFIG_ATA_SFF
 	struct ata_ioports	ioaddr;	/* ATA cmd/ctl/dma register blocks */
+	struct ata_link*	sff_pio_task_link; /* link currently used */
 #endif /* CONFIG_ATA_SFF */
 
 	u8			ctl;	/* cache of ATA control register */
diff --git a/include/linux/miscdevice.h b/include/linux/miscdevice.h
index 6ae208b..f4377af 100644
--- a/include/linux/miscdevice.h
+++ b/include/linux/miscdevice.h
@@ -18,7 +18,11 @@
 #define APOLLO_MOUSE_MINOR	7
 #define PC110PAD_MINOR		9
 /*#define ADB_MOUSE_MINOR	10	FIXME OBSOLETE */
-#define CRYPTODEV_MINOR		70	/* /dev/crypto */
+#define CRYPTODEV_MINOR         70      /* OCF async crypto */
+#define CESADEV_MINOR		71	/* marvell CESA     */
+#define BTNSDEV_MINOR  		72	/* Marvell btns */
+#define SLICDEV_MINOR		73	/* Marvell SLIC control device */
+#define TDMDEV_MINOR		74	/* Marvell TDM test device */
 #define WATCHDOG_MINOR		130	/* Watchdog timer     */
 #define TEMP_MINOR		131	/* Temperature Sensor */
 #define RTC_MINOR		135
diff --git a/include/linux/mtd/bbm.h b/include/linux/mtd/bbm.h
index 9c3757c..353b43d 100644
--- a/include/linux/mtd/bbm.h
+++ b/include/linux/mtd/bbm.h
@@ -83,6 +83,11 @@ struct nand_bbt_descr {
 /* Search good / bad pattern on the first and the second page */
 #define NAND_BBT_SCAN2NDPAGE	0x00004000
 
+#ifdef CONFIG_MTD_NAND_NFC_MLC_SUPPORT
+/* Search the bad block indicators according to Marvell's Naked symantics */
+#define NAND_BBT_SCANMVCUSTOM	0x10000000
+#endif
+
 /* The maximum number of blocks to scan for a bbt */
 #define NAND_BBT_SCAN_MAXBLOCKS	4
 
diff --git a/include/linux/mtd/nand.h b/include/linux/mtd/nand.h
index ccab9df..400da28 100644
--- a/include/linux/mtd/nand.h
+++ b/include/linux/mtd/nand.h
@@ -379,7 +379,14 @@ struct nand_chip {
 
 	int		chip_delay;
 	unsigned int	options;
-
+#ifdef CONFIG_MTD_NAND_NFC_GANG_SUPPORT
+	unsigned int	num_devs;
+#endif
+#ifdef CONFIG_MTD_NAND_NFC_MLC_SUPPORT
+	unsigned int	oobsize_ovrd;
+	unsigned int	bb_location;
+	unsigned int	bb_page;
+#endif
 	int		page_shift;
 	int		phys_erase_shift;
 	int		bbt_erase_shift;
diff --git a/include/linux/skbuff.h b/include/linux/skbuff.h
index ae43750..997cfa1 100644
--- a/include/linux/skbuff.h
+++ b/include/linux/skbuff.h
@@ -30,6 +30,10 @@
 #include <linux/dmaengine.h>
 #include <linux/hrtimer.h>
 
+#if defined(CONFIG_NET_SKB_HEADROOM)
+# define NET_SKB_PAD  CONFIG_NET_SKB_HEADROOM
+#endif
+
 /* Don't change this without changing skb_csum_unnecessary! */
 #define CHECKSUM_NONE 0
 #define CHECKSUM_UNNECESSARY 1
@@ -353,6 +357,10 @@ struct sk_buff {
 	__be16			protocol;
 
 	void			(*destructor)(struct sk_buff *skb);
+#ifdef CONFIG_NET_SKB_RECYCLE
+	int				(*skb_recycle) (struct sk_buff *skb);
+	void			*hw_cookie;
+#endif /* CONFIG_NET_SKB_RECYCLE */
 #if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)
 	struct nf_conntrack	*nfct;
 	struct sk_buff		*nfct_reasm;
@@ -1755,6 +1763,9 @@ extern unsigned int    datagram_poll(struct file *file, struct socket *sock,
 extern int	       skb_copy_datagram_iovec(const struct sk_buff *from,
 					       int offset, struct iovec *to,
 					       int size);
+extern int	       skb_copy_datagram_to_kernel_iovec(const struct sk_buff *from,
+					       int offset, struct iovec *to,
+					       int size);
 extern int	       skb_copy_and_csum_datagram_iovec(struct sk_buff *skb,
 							int hlen,
 							struct iovec *iov);
diff --git a/include/linux/socket.h b/include/linux/socket.h
index 354cc56..7dd9254 100644
--- a/include/linux/socket.h
+++ b/include/linux/socket.h
@@ -257,6 +257,9 @@ struct ucred {
 #define MSG_MORE	0x8000	/* Sender will send more */
 #define MSG_WAITFORONE	0x10000	/* recvmmsg(): block until 1+ packets avail */
 #define MSG_SENDPAGE_NOTLAST 0x20000 /* sendpage() internal : not the last page */
+#define MSG_KERNSPACE   0x40000
+#define MSG_NOCATCHSIG	0x80000
+
 #define MSG_EOF         MSG_FIN
 
 #define MSG_CMSG_CLOEXEC 0x40000000	/* Set close_on_exit for file
@@ -321,6 +324,7 @@ extern int memcpy_toiovecend(const struct iovec *v, unsigned char *kdata,
 extern int move_addr_to_user(struct sockaddr *kaddr, int klen, void __user *uaddr, int __user *ulen);
 extern int move_addr_to_kernel(void __user *uaddr, int ulen, struct sockaddr *kaddr);
 extern int put_cmsg(struct msghdr*, int level, int type, int len, void *data);
+extern void memcpy_tokerneliovec(struct iovec *iov, unsigned char *kdata, int len);
 
 struct timespec;
 
diff --git a/include/linux/splice.h b/include/linux/splice.h
index af56841..ebf9c02 100644
--- a/include/linux/splice.h
+++ b/include/linux/splice.h
@@ -57,6 +57,14 @@ struct splice_pipe_desc {
 	void (*spd_release)(struct splice_pipe_desc *, unsigned int);
 };
 
+struct recvfile_ctl_blk
+{
+	struct page *rv_page;
+	loff_t rv_pos;
+	size_t rv_count;
+	void *rv_fsdata;
+};
+
 typedef int (splice_actor)(struct pipe_inode_info *, struct pipe_buffer *,
 			   struct splice_desc *);
 typedef int (splice_direct_actor)(struct pipe_inode_info *,
diff --git a/include/mtd/mtd-abi.h b/include/mtd/mtd-abi.h
index 1fc324a..ec5a028 100644
--- a/include/mtd/mtd-abi.h
+++ b/include/mtd/mtd-abi.h
@@ -135,7 +135,11 @@ struct nand_oobfree {
  */
 struct nand_ecclayout {
 	__u32 eccbytes;
+#ifdef CONFIG_MTD_NAND_NFC
+	__u32 eccpos[128];
+#else
 	__u32 eccpos[64];
+#endif
 	__u32 oobavail;
 	struct nand_oobfree oobfree[MTD_MAX_OOBFREE_ENTRIES];
 };
diff --git a/include/net/ip6_fib.h b/include/net/ip6_fib.h
index 86f46c4..15bdf9b 100644
--- a/include/net/ip6_fib.h
+++ b/include/net/ip6_fib.h
@@ -116,6 +116,10 @@ struct rt6_info {
 #endif
 
 	struct rt6key			rt6i_src;
+#if defined (CONFIG_MV_ETH_NFP_FIB_LEARN)
+	int				rt6i_iifindex;
+	bool 			nfp;
+#endif /* CONFIG_MV_ETH_NFP_FIB_LEARN */
 };
 
 static inline struct inet6_dev *ip6_dst_idev(struct dst_entry *dst)
diff --git a/include/net/neighbour.h b/include/net/neighbour.h
index da1d58b..76db510 100644
--- a/include/net/neighbour.h
+++ b/include/net/neighbour.h
@@ -111,6 +111,9 @@ struct neighbour {
 	struct sk_buff_head	arp_queue;
 	struct timer_list	timer;
 	const struct neigh_ops	*ops;
+#if defined(CONFIG_MV_ETH_NFP_FIB_LEARN)
+	bool 			nfp;
+#endif /* CONFIG_MV_ETH_NFP_FIB_LEARN */
 	u8			primary_key[0];
 };
 
diff --git a/include/net/netfilter/nf_conntrack.h b/include/net/netfilter/nf_conntrack.h
index bde095f..f859421 100644
--- a/include/net/netfilter/nf_conntrack.h
+++ b/include/net/netfilter/nf_conntrack.h
@@ -116,6 +116,22 @@ struct nf_conn {
 	u_int32_t secmark;
 #endif
 
+#if defined(CONFIG_NETFILTER_XT_MATCH_LAYER7) || \
+    defined(CONFIG_NETFILTER_XT_MATCH_LAYER7_MODULE)
+	struct {
+		/*
+		 * e.g. "http". NULL before decision. "unknown" after decision
+		 * if no match.
+		 */
+		char *app_proto;
+		/*
+		 * application layer data so far. NULL after match decision.
+		 */
+		char *app_data;
+		unsigned int app_data_len;
+	} layer7;
+#endif
+
 	/* Storage reserved for other modules: */
 	union nf_conntrack_proto proto;
 
@@ -298,6 +314,13 @@ extern int nf_conntrack_set_hashsize(const char *val, struct kernel_param *kp);
 extern unsigned int nf_conntrack_htable_size;
 extern unsigned int nf_conntrack_max;
 
+#ifdef CONFIG_MV_LINUX_COUNTERS_DISABLE
+
+#define NF_CT_STAT_INC(net, count)
+#define NF_CT_STAT_INC_ATOMIC(net, count)
+
+#else 
+
 #define NF_CT_STAT_INC(net, count)	\
 	__this_cpu_inc((net)->ct.stat->count)
 #define NF_CT_STAT_INC_ATOMIC(net, count)		\
@@ -307,6 +330,8 @@ do {							\
 	local_bh_enable();				\
 } while (0)
 
+#endif /* CONFIG_MV_LINUX_COUNTERS_DISABLE */
+
 #define MODULE_ALIAS_NFCT_HELPER(helper) \
         MODULE_ALIAS("nfct-helper-" helper)
 
diff --git a/include/net/netfilter/nf_conntrack_tuple.h b/include/net/netfilter/nf_conntrack_tuple.h
index 4ee44c8..8b52340 100644
--- a/include/net/netfilter/nf_conntrack_tuple.h
+++ b/include/net/netfilter/nf_conntrack_tuple.h
@@ -95,6 +95,15 @@ struct nf_conntrack_tuple {
 		/* The direction (for tuplehash) */
 		u_int8_t dir;
 	} dst;
+
+#if defined(CONFIG_MV_ETH_NFP_CT_LEARN)
+	/* If true, this connection is handled by NFP */
+	bool nfp;
+	int ifindex;
+	bool nfpCapable;
+	bool udpCsum;
+	struct ipt_nfp_info *info;
+#endif /* CONFIG_MV_ETH_NFP_CT_LEARN */
 };
 
 struct nf_conntrack_tuple_mask {
diff --git a/include/net/route.h b/include/net/route.h
index 2c9fba7..71c9535 100644
--- a/include/net/route.h
+++ b/include/net/route.h
@@ -73,6 +73,10 @@ struct rtable {
 	/* Miscellaneous cached information */
 	__be32			rt_spec_dst; /* RFC1122 specific destination */
 	struct inet_peer	*peer; /* long-living peer info */
+
+#if defined(CONFIG_MV_ETH_NFP_FIB_LEARN)
+	bool 			nfp;
+#endif /* CONFIG_MV_ETH_NFP_FIB_LEARN */
 };
 
 struct ip_rt_acct {
diff --git a/net/8021q/vlan.c b/net/8021q/vlan.c
index 97da977..0f8040b 100644
--- a/net/8021q/vlan.c
+++ b/net/8021q/vlan.c
@@ -40,6 +40,13 @@
 
 #define DRV_VERSION "1.8"
 
+
+#if defined(CONFIG_MV_ETH_NFP_VLAN_LEARN)
+extern int nfp_hook_vlan_add(int if_index, struct net_device *dev,
+					int real_if_index, int vlan_id);
+extern int nfp_hook_vlan_del(int if_index);
+#endif /* CONFIG_MV_ETH_NFP_VLAN_LEARN */
+
 /* Global VLAN variables */
 
 int vlan_net_id __read_mostly;
@@ -166,6 +173,10 @@ void unregister_vlan_dev(struct net_device *dev, struct list_head *head)
 	if (!grp->killall)
 		synchronize_net();
 
+#if defined(CONFIG_MV_ETH_NFP_VLAN_LEARN)
+	nfp_hook_vlan_del(dev->ifindex);
+#endif /* CONFIG_MV_ETH_NFP_VLAN_LEARN */
+
 	unregister_netdevice_queue(dev, head);
 
 	/* If the group is now empty, kill off the group. */
@@ -337,6 +348,10 @@ static int register_vlan_device(struct net_device *real_dev, u16 vlan_id)
 	if (err < 0)
 		goto out_free_newdev;
 
+#if defined(CONFIG_MV_ETH_NFP_VLAN_LEARN)
+	nfp_hook_vlan_add(new_dev->ifindex, new_dev, real_dev->ifindex, vlan_id);
+#endif /* CONFIG_MV_ETH_NFP_VLAN_LEARN */
+
 	return 0;
 
 out_free_newdev:
@@ -748,6 +763,26 @@ static void __exit vlan_cleanup_module(void)
 	vlan_gvrp_uninit();
 }
 
+#if defined(CONFIG_MV_ETH_NFP_VLAN_LEARN)
+void vlan_sync(void)
+{
+	struct net_device *dev;
+
+	rtnl_lock();
+	for_each_netdev(&init_net, dev) {
+		if (dev->priv_flags & IFF_802_1Q_VLAN) {
+			struct vlan_dev_info *vlan = vlan_dev_info(dev);
+			struct net_device *real_dev = vlan->real_dev;
+			u16 vlan_id = vlan->vlan_id;
+
+			if (nfp_hook_vlan_add(dev->ifindex, dev, real_dev->ifindex, vlan_id))
+				printk("nfp_hook_vlan_add failed in %s\n",__func__);
+		}
+	}
+	rtnl_unlock();
+}
+#endif /* CONFIG_MV_ETH_NFP_VLAN_LEARN */
+
 module_init(vlan_proto_init);
 module_exit(vlan_cleanup_module);
 
diff --git a/net/bridge/br_fdb.c b/net/bridge/br_fdb.c
index 5c96374..01573f4 100644
--- a/net/bridge/br_fdb.c
+++ b/net/bridge/br_fdb.c
@@ -31,6 +31,12 @@ static int fdb_insert(struct net_bridge *br, struct net_bridge_port *source,
 
 static u32 fdb_salt __read_mostly;
 
+#if defined(CONFIG_MV_ETH_NFP_FDB_LEARN)
+int nfp_hook_fdb_rule_add(int br_index, int if_index, const u8 *mac, int is_local);
+int nfp_hook_fdb_rule_age(int br_index, int if_index, const u8 *mac);
+int nfp_hook_fdb_rule_del(int br_index, int if_index, const u8 *mac);
+#endif /* CONFIG_MV_ETH_NFP_FDB_LEARN */
+
 int __init br_fdb_init(void)
 {
 	br_fdb_cache = kmem_cache_create("bridge_fdb_cache",
@@ -59,10 +65,20 @@ static inline unsigned long hold_time(const struct net_bridge *br)
 }
 
 static inline int has_expired(const struct net_bridge *br,
-				  const struct net_bridge_fdb_entry *fdb)
+				struct net_bridge_fdb_entry *fdb)
 {
-	return !fdb->is_static &&
-		time_before_eq(fdb->ageing_timer + hold_time(br), jiffies);
+	if (fdb->is_static)
+		return 0;
+
+#if defined(CONFIG_MV_ETH_NFP_FDB_LEARN)
+	if (fdb->nfp) {
+		if (nfp_hook_fdb_rule_age(fdb->dst->br->dev->ifindex,
+					fdb->dst->dev->ifindex, fdb->addr.addr) > 0)
+			fdb->ageing_timer = jiffies + fdb->dst->br->forward_delay;
+	}
+#endif /* CONFIG_MV_ETH_NFP_FDB_LEARN */
+
+	return time_before_eq(fdb->ageing_timer + hold_time(br), jiffies);
 }
 
 static inline int br_mac_hash(const unsigned char *mac)
@@ -81,6 +97,13 @@ static void fdb_rcu_free(struct rcu_head *head)
 
 static inline void fdb_delete(struct net_bridge_fdb_entry *f)
 {
+#if defined(CONFIG_MV_ETH_NFP_FDB_LEARN)
+	if (f->nfp) {
+		nfp_hook_fdb_rule_del(f->dst->br->dev->ifindex,
+				f->dst->dev->ifindex, f->addr.addr);
+	}
+#endif /* CONFIG_MV_ETH_NFP_FDB_LEARN */
+
 	hlist_del_rcu(&f->hlist);
 	call_rcu(&f->rcu, fdb_rcu_free);
 }
@@ -140,6 +163,15 @@ void br_fdb_cleanup(unsigned long _data)
 			unsigned long this_timer;
 			if (f->is_static)
 				continue;
+
+#if defined(CONFIG_MV_ETH_NFP_FDB_LEARN)
+			if (f->nfp) {
+				if (nfp_hook_fdb_rule_age(f->dst->br->dev->ifindex,
+							f->dst->dev->ifindex, f->addr.addr) > 0)
+					f->ageing_timer = jiffies + f->dst->br->forward_delay;
+			}
+#endif /* CONFIG_MV_ETH_NFP_FDB_LEARN */
+
 			this_timer = f->ageing_timer + delay;
 			if (time_before_eq(this_timer, jiffies))
 				fdb_delete(f);
@@ -331,6 +363,12 @@ static struct net_bridge_fdb_entry *fdb_create(struct hlist_head *head,
 		fdb->is_local = is_local;
 		fdb->is_static = is_local;
 		fdb->ageing_timer = jiffies;
+
+#if defined(CONFIG_MV_ETH_NFP_FDB_LEARN)
+		fdb->nfp = false;
+		if (!nfp_hook_fdb_rule_add(fdb->dst->br->dev->ifindex, fdb->dst->dev->ifindex, addr, is_local))
+			fdb->nfp = true;
+#endif /* CONFIG_MV_ETH_NFP_FDB_LEARN */
 	}
 	return fdb;
 }
diff --git a/net/bridge/br_if.c b/net/bridge/br_if.c
index 0b6b1f2..3107a92 100644
--- a/net/bridge/br_if.c
+++ b/net/bridge/br_if.c
@@ -24,6 +24,11 @@
 
 #include "br_private.h"
 
+#if defined(CONFIG_MV_ETH_NFP_FDB_LEARN)
+int nfp_hook_del_br(int ifindex);
+int nfp_hook_del_port_from_br(int bridge_if, int port_if);
+#endif /* CONFIG_MV_ETH_NFP_FDB_LEARN */
+
 /*
  * Determine initial path cost based on speed.
  * using recommendations from 802.1d standard
@@ -162,11 +167,20 @@ static void del_br(struct net_bridge *br, struct list_head *head)
 	struct net_bridge_port *p, *n;
 
 	list_for_each_entry_safe(p, n, &br->port_list, list) {
+	
+#if defined(CONFIG_MV_ETH_NFP_FDB_LEARN)
+	nfp_hook_del_port_from_br(br->dev->ifindex, p->dev->ifindex);
+#endif /* CONFIG_MV_ETH_NFP_FDB_LEARN */
+
 		del_nbp(p);
 	}
 
 	del_timer_sync(&br->gc_timer);
 
+#if defined(CONFIG_MV_ETH_NFP_FDB_LEARN)
+	nfp_hook_del_br(br->dev->ifindex);
+#endif /* CONFIG_MV_ETH_NFP_FDB_LEARN */
+
 	br_sysfs_delbr(br->dev);
 	unregister_netdevice_queue(br->dev, head);
 }
@@ -460,6 +474,10 @@ int br_del_if(struct net_bridge *br, struct net_device *dev)
 	if (!p || p->br != br)
 		return -EINVAL;
 
+#if defined(CONFIG_MV_ETH_NFP_FDB_LEARN)
+	nfp_hook_del_port_from_br(br->dev->ifindex, p->dev->ifindex);
+#endif /* CONFIG_MV_ETH_NFP_FDB_LEARN */
+
 	del_nbp(p);
 
 	spin_lock_bh(&br->lock);
diff --git a/net/bridge/br_private.h b/net/bridge/br_private.h
index 846d7d1..600c002 100644
--- a/net/bridge/br_private.h
+++ b/net/bridge/br_private.h
@@ -55,6 +55,10 @@ struct net_bridge_fdb_entry
 	mac_addr			addr;
 	unsigned char			is_local;
 	unsigned char			is_static;
+
+#if defined(CONFIG_MV_ETH_NFP_FDB_LEARN)
+	bool 				nfp;
+#endif /* CONFIG_MV_ETH_NFP_FDB_LEARN */
 };
 
 struct net_bridge_port_group {
diff --git a/net/core/datagram.c b/net/core/datagram.c
index 2dccd4e..83cf642 100644
--- a/net/core/datagram.c
+++ b/net/core/datagram.c
@@ -128,6 +128,65 @@ out_noerr:
 	goto out;
 }
 
+/*
+ *	skb_copy_datagram_to_kernel_iovec - Copy a datagram to a kernel iovec structure.
+ *	@skb: buffer to copy
+ *	@offset: offset in the buffer to start copying from
+ *	@to: io vector to copy to
+ *	@len: amount of data to copy from buffer to iovec
+ *
+ *	Note: the iovec is modified during the copy.
+ */
+int skb_copy_datagram_to_kernel_iovec(const struct sk_buff *skb, int offset,
+				      struct iovec *to, int len)
+{
+	int i, fraglen, end = 0;
+	struct sk_buff *next = skb_shinfo(skb)->frag_list;
+
+	if (!len)
+		return 0;
+
+next_skb:
+	fraglen = skb_headlen(skb);
+	i = -1;
+
+	while (1) {
+		int start = end;
+
+		if ((end += fraglen) > offset) {
+			int copy = end - offset;
+			int o = offset - start;
+
+			if (copy > len)
+				copy = len;
+			if (i == -1)
+				memcpy_tokerneliovec(to, skb->data + o, copy);
+			else {
+				skb_frag_t *frag = &skb_shinfo(skb)->frags[i];
+				struct page *page = frag->page;
+				void *p = kmap(page) + frag->page_offset + o;
+				memcpy_tokerneliovec(to, p, copy);
+				kunmap(page);
+			}
+
+			if (!(len -= copy))
+				return 0;
+			offset += copy;
+		}
+		if (++i >= skb_shinfo(skb)->nr_frags)
+			break;
+		fraglen = skb_shinfo(skb)->frags[i].size;
+	}
+	if (next) {
+		skb = next;
+		BUG_ON(skb_shinfo(skb)->frag_list);
+		next = skb->next;
+		goto next_skb;
+	}
+
+	return -EFAULT;
+}
+
 /**
  *	__skb_recv_datagram - Receive a datagram skbuff
  *	@sk: socket
diff --git a/net/core/iovec.c b/net/core/iovec.c
index 16ed584..5e53c56 100644
--- a/net/core/iovec.c
+++ b/net/core/iovec.c
@@ -121,6 +121,26 @@ int memcpy_toiovecend(const struct iovec *iov, unsigned char *kdata,
 }
 
 /*
+ *	In kernel copy to iovec. Returns -EFAULT on error.
+ *
+ *	Note: this modifies the original iovec.
+ */
+void memcpy_tokerneliovec(struct iovec *iov, unsigned char *kdata, int len)
+{
+	while (len > 0) {
+		if (iov->iov_len) {
+			int copy = min_t(unsigned int, iov->iov_len, len);
+			memcpy(iov->iov_base, kdata, copy);
+			len -= copy;
+			kdata += copy;
+			iov->iov_base += copy;
+			iov->iov_len -= copy;
+		}
+		iov++;
+	}
+}
+
+/*
  *	Copy iovec to kernel. Returns -EFAULT on error.
  *
  *	Note: this modifies the original iovec.
diff --git a/net/core/neighbour.c b/net/core/neighbour.c
index 7d53105..dfe85d1 100644
--- a/net/core/neighbour.c
+++ b/net/core/neighbour.c
@@ -37,6 +37,12 @@
 #include <linux/string.h>
 #include <linux/log2.h>
 
+#if defined(CONFIG_MV_ETH_NFP_FIB_LEARN)
+extern int nfp_hook_arp_add(int family, const u8 *ip, u8 *mac, int if_index);
+extern int nfp_hook_arp_delete(int family, const u8 *ip);
+extern int nfp_hook_arp_is_confirmed(int family, const u8 *ip);
+#endif /* CONFIG_MV_ETH_NFP_FIB_LEARN */
+
 #define NEIGH_DEBUG 1
 
 #define NEIGH_PRINTK(x...) printk(x)
@@ -636,6 +642,15 @@ void neigh_destroy(struct neighbour *neigh)
 	if (neigh_del_timer(neigh))
 		printk(KERN_WARNING "Impossible event.\n");
 
+#if defined(CONFIG_MV_ETH_NFP_FIB_LEARN)
+	if (neigh->nfp) {
+		nfp_hook_arp_delete(neigh->tbl->family, neigh->primary_key);
+
+		NEIGH_PRINTK2("0x%8lx: neigh %p, ref=%d, state=%d, nfp=%d is connected in %s.\n",
+			jiffies, neigh, atomic_read(&neigh->refcnt), neigh->nud_state, neigh->nfp, __func__);
+	}
+#endif /* CONFIG_MV_ETH_NFP_FIB_LEARN */
+
 	while ((hh = neigh->hh) != NULL) {
 		neigh->hh = hh->hh_next;
 		hh->hh_next = NULL;
@@ -685,7 +700,15 @@ static void neigh_connect(struct neighbour *neigh)
 {
 	struct hh_cache *hh;
 
-	NEIGH_PRINTK2("neigh %p is connected.\n", neigh);
+#if defined(CONFIG_MV_ETH_NFP_FIB_LEARN)
+	neigh->nfp = false;
+	if (!nfp_hook_arp_add(neigh->tbl->family, neigh->primary_key, neigh->ha, neigh->dev->ifindex)) {
+		neigh->nfp = true;
+
+		NEIGH_PRINTK2("0x%8lx: neigh %p, ref=%d, state=%d, nfp=%d is connected in %s.\n",
+			jiffies, neigh, atomic_read(&neigh->refcnt), neigh->nud_state, neigh->nfp, __func__);
+	}
+#endif /* CONFIG_MV_ETH_NFP_FIB_LEARN */
 
 	neigh->output = neigh->ops->connected_output;
 
@@ -732,6 +755,19 @@ static void neigh_periodic_work(struct work_struct *work)
 			if (time_before(n->used, n->confirmed))
 				n->used = n->confirmed;
 
+#if defined(CONFIG_MV_ETH_NFP_FIB_LEARN)
+			if ((atomic_read(&n->refcnt) == 1) && (state != NUD_FAILED) &&
+				time_after(jiffies, n->used + n->parms->gc_staletime)) {
+				if (n->nfp) {
+					if (nfp_hook_arp_is_confirmed(n->tbl->family, n->primary_key)) {
+						neigh_event_send(n, NULL);
+					}
+					NEIGH_PRINTK2("0x%8lx: neigh %p ref=%d, state=%d, NFP ARP aging in %s\n",
+						jiffies, n, atomic_read(&n->refcnt), n->nud_state, __func__);
+				}
+			}
+#endif /* CONFIG_MV_ETH_NFP_FIB_LEARN */
+
 			if (atomic_read(&n->refcnt) == 1 &&
 			    (state == NUD_FAILED ||
 			     time_after(jiffies, n->used + n->parms->gc_staletime))) {
@@ -934,6 +970,7 @@ int __neigh_event_send(struct neighbour *neigh, struct sk_buff *skb)
 				kfree_skb(buff);
 				NEIGH_CACHE_STAT_INC(neigh->tbl, unres_discards);
 			}
+//			skb_dst_force(skb);
 			__skb_queue_tail(&neigh->arp_queue, skb);
 		}
 		rc = 1;
@@ -2786,6 +2823,38 @@ EXPORT_SYMBOL(neigh_sysctl_unregister);
 
 #endif	/* CONFIG_SYSCTL */
 
+#if defined(CONFIG_MV_ETH_NFP_FIB_LEARN)
+void neigh_sync(int family)
+{
+	struct neigh_table *tbl;
+	struct neighbour *n;
+	int t, h, idx;
+
+	read_lock(&neigh_tbl_lock);
+	for (tbl = neigh_tables, t = 0; tbl; tbl = tbl->next, t++) {
+		if (tbl->family == family) {
+			for (h = 0; h <= tbl->hash_mask; h++) {
+				for (n = tbl->hash_buckets[h], idx = 0; n; n = n->next) {
+					if (!(n->nfp)) {
+						if (n->dev == NULL) {
+							continue;
+						}
+						n->nfp = false;
+						if (!nfp_hook_arp_add(n->tbl->family,
+								n->primary_key,
+								n->ha,
+								n->dev->ifindex)) {
+							n->nfp = true;
+						}
+					}
+				}
+			}
+		}
+	}
+	read_unlock(&neigh_tbl_lock);
+}
+#endif /* CONFIG_MV_ETH_NFP_FIB_LEARN */
+
 static int __init neigh_init(void)
 {
 	rtnl_register(PF_UNSPEC, RTM_NEWNEIGH, neigh_add, NULL);
diff --git a/net/core/skbuff.c b/net/core/skbuff.c
index 3666cf5..baa3c28 100644
--- a/net/core/skbuff.c
+++ b/net/core/skbuff.c
@@ -413,7 +413,7 @@ static void skb_clone_fraglist(struct sk_buff *skb)
 		skb_get(list);
 }
 
-static void skb_release_data(struct sk_buff *skb)
+static inline void skb_release_data(struct sk_buff *skb)
 {
 	if (!skb->cloned ||
 	    !atomic_sub_return(skb->nohdr ? (1 << SKB_DATAREF_SHIFT) + 1 : 1,
@@ -429,12 +429,20 @@ static void skb_release_data(struct sk_buff *skb)
 
 		kfree(skb->head);
 	}
+#ifdef CONFIG_NET_SKB_RECYCLE
+	/* Workaround for the cases when recycle callback was not called */
+	if (skb->hw_cookie) {
+		kfree(skb->hw_cookie);
+		skb->hw_cookie = NULL;
+	}
+	skb->skb_recycle = NULL;
+#endif /* CONFIG_NET_SKB_RECYCLE */	
 }
 
 /*
  *	Free an skbuff by memory without cleaning the state.
  */
-static void kfree_skbmem(struct sk_buff *skb)
+static inline void kfree_skbmem(struct sk_buff *skb)
 {
 	struct sk_buff *other;
 	atomic_t *fclone_ref;
@@ -465,7 +473,7 @@ static void kfree_skbmem(struct sk_buff *skb)
 	}
 }
 
-static void skb_release_head_state(struct sk_buff *skb)
+static inline void skb_release_head_state(struct sk_buff *skb)
 {
 	skb_dst_drop(skb);
 #ifdef CONFIG_XFRM
@@ -476,7 +484,10 @@ static void skb_release_head_state(struct sk_buff *skb)
 		skb->destructor(skb);
 	}
 #if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)
+	if(skb->nfct) 
 	nf_conntrack_put(skb->nfct);
+	
+	if(skb->nfct_reasm) 
 	nf_conntrack_put_reasm(skb->nfct_reasm);
 #endif
 #ifdef CONFIG_BRIDGE_NETFILTER
@@ -509,6 +520,11 @@ static void skb_release_all(struct sk_buff *skb)
 
 void __kfree_skb(struct sk_buff *skb)
 {
+#ifdef CONFIG_NET_SKB_RECYCLE
+	if (skb->skb_recycle && !skb->skb_recycle(skb))
+		return;
+#endif /* CONFIG_NET_SKB_RECYCLE */
+ 
 	skb_release_all(skb);
 	kfree_skbmem(skb);
 }
@@ -658,6 +674,12 @@ static struct sk_buff *__skb_clone(struct sk_buff *n, struct sk_buff *skb)
 	n->cloned = 1;
 	n->nohdr = 0;
 	n->destructor = NULL;
+
+#ifdef CONFIG_NET_SKB_RECYCLE
+	n->skb_recycle = NULL;
+	n->hw_cookie = NULL;
+#endif /* CONFIG_NET_SKB_RECYCLE */
+
 	C(tail);
 	C(end);
 	C(head);
diff --git a/net/ipv4/netfilter/Kconfig b/net/ipv4/netfilter/Kconfig
index 1833bdb..30d913e 100644
--- a/net/ipv4/netfilter/Kconfig
+++ b/net/ipv4/netfilter/Kconfig
@@ -192,6 +192,13 @@ config IP_NF_TARGET_NETMAP
 
 	  To compile it as a module, choose M here.  If unsure, say N.
 
+config IP_NF_TARGET_NFP
+	tristate "NFP target support"
+	depends on NF_CONNTRACK_IPV4 && MV_ETH_NFP_NAT
+	help
+	  This target allows to process selected connections in NFP.
+	  To compile it as a module, choose M here.  If unsure, say N.
+
 config IP_NF_TARGET_REDIRECT
 	tristate "REDIRECT target support"
 	depends on NF_NAT
diff --git a/net/ipv4/netfilter/Makefile b/net/ipv4/netfilter/Makefile
index 4811159..6d1a456 100644
--- a/net/ipv4/netfilter/Makefile
+++ b/net/ipv4/netfilter/Makefile
@@ -58,6 +58,7 @@ obj-$(CONFIG_IP_NF_TARGET_ECN) += ipt_ECN.o
 obj-$(CONFIG_IP_NF_TARGET_LOG) += ipt_LOG.o
 obj-$(CONFIG_IP_NF_TARGET_MASQUERADE) += ipt_MASQUERADE.o
 obj-$(CONFIG_IP_NF_TARGET_NETMAP) += ipt_NETMAP.o
+obj-$(CONFIG_IP_NF_TARGET_NFP) += ipt_NFP.o
 obj-$(CONFIG_IP_NF_TARGET_REDIRECT) += ipt_REDIRECT.o
 obj-$(CONFIG_IP_NF_TARGET_REJECT) += ipt_REJECT.o
 obj-$(CONFIG_IP_NF_TARGET_ULOG) += ipt_ULOG.o
diff --git a/net/ipv4/netfilter/nf_conntrack_l3proto_ipv4_compat.c b/net/ipv4/netfilter/nf_conntrack_l3proto_ipv4_compat.c
index 2fb7b76..11731ae 100644
--- a/net/ipv4/netfilter/nf_conntrack_l3proto_ipv4_compat.c
+++ b/net/ipv4/netfilter/nf_conntrack_l3proto_ipv4_compat.c
@@ -153,6 +153,19 @@ static int ct_seq_show(struct seq_file *s, void *v)
 		goto release;
 #endif
 
+#if defined(CONFIG_MV_ETH_NFP_CT_LEARN)
+	if ((ct->tuplehash[IP_CT_DIR_ORIGINAL].tuple.nfp) && (ct->tuplehash[IP_CT_DIR_REPLY].tuple.nfp)) {
+		if (seq_printf(s, "[NFP (both)] "))
+			goto release;
+	} else if (ct->tuplehash[IP_CT_DIR_ORIGINAL].tuple.nfp) {
+		if (seq_printf(s, "[NFP (orig)] "))
+			goto release;
+	} else if (ct->tuplehash[IP_CT_DIR_REPLY].tuple.nfp) {
+		if (seq_printf(s, "[NFP (reply)] "))
+			goto release;
+	}
+#endif /* CONFIG_MV_ETH_NFP_CT_LEARN */
+
 	if (seq_printf(s, "use=%u\n", atomic_read(&ct->ct_general.use)))
 		goto release;
 	ret = 0;
diff --git a/net/ipv4/route.c b/net/ipv4/route.c
index 325b43c..ceff29a 100644
--- a/net/ipv4/route.c
+++ b/net/ipv4/route.c
@@ -110,6 +110,13 @@
 #endif
 #include <net/secure_seq.h>
 
+#if defined(CONFIG_MV_ETH_NFP_FIB_LEARN)
+extern int nfp_fib_learn_en;
+extern int nfp_hook_fib_rule_add(int family, u8 *src_l3, u8 *dst_l3, u8 *gtw_l3, int iif, int oif);
+extern int nfp_hook_fib_rule_del(int family, u8 *src_l3, u8 *dst_l3, int iif, int oif);
+extern int nfp_hook_fib_rule_age(int family, u8 *src_l3, u8 *dst_l3, int iif, int oif);
+#endif /* CONFIG_MV_ETH_NFP_FIB_LEARN */
+
 #define RT_FL_TOS(oldflp) \
     ((u32)(oldflp->fl4_tos & (IPTOS_RT_MASK | RTO_ONLINK)))
 
@@ -610,6 +617,12 @@ static inline int ip_rt_proc_init(void)
 
 static inline void rt_free(struct rtable *rt)
 {
+#if defined(CONFIG_MV_ETH_NFP_FIB_LEARN)
+		if (rt->nfp)	
+			nfp_hook_fib_rule_del(AF_INET, (u8*)(&rt->rt_src),(u8*)(&rt->rt_dst), 
+							rt->rt_iif, rt->u.dst.dev->ifindex);
+#endif /* CONFIG_MV_ETH_NFP_FIB_LEARN */
+
 	call_rcu_bh(&rt->u.dst.rcu_head, dst_rcu_free);
 }
 
@@ -646,6 +659,14 @@ static int rt_may_expire(struct rtable *rth, unsigned long tmo1, unsigned long t
 	    time_after_eq(jiffies, rth->u.dst.expires))
 		goto out;
 
+#if defined(CONFIG_MV_ETH_NFP_FIB_LEARN)
+	if (rth->nfp) {
+		if (nfp_hook_fib_rule_age(AF_INET,(u8 *)(&rth->rt_src), (u8 *)(&rth->rt_dst), 
+						rth->rt_iif, rth->u.dst.dev->ifindex))
+			rth->u.dst.lastuse = jiffies;
+	}
+#endif /* CONFIG_MV_ETH_NFP_FIB_LEARN */
+
 	age = jiffies - rth->u.dst.lastuse;
 	ret = 0;
 	if ((age <= tmo1 && !rt_fast_clean(rth)) ||
@@ -769,6 +790,54 @@ static void rt_do_flush(int process_context)
 	}
 }
 
+#if defined(CONFIG_MV_ETH_NFP_FIB_LEARN)
+void nfp_fib_sync(void)
+{
+	
+	unsigned int i;
+	struct rtable *rth, *next;
+	struct rtable *tail;
+	struct rtable **prev, *p;
+
+	for (i = 0; i <= rt_hash_mask; i++) {
+		rth = rt_hash_table[i].chain;
+		if (!rth)
+			continue;
+
+		spin_lock_bh(rt_hash_lock_addr(i));
+		rth = rt_hash_table[i].chain;
+
+		/* defer releasing the head of the list after spin_unlock */
+		for (tail = rth; tail; tail = tail->u.dst.rt_next)
+			if (rth != tail)
+				rt_hash_table[i].chain = tail;
+
+		prev = &rt_hash_table[i].chain;
+		for (p = *prev; p; p = next) {
+			next = p->u.dst.rt_next;
+			if (!(p->rt_flags & (RTCF_MULTICAST | RTCF_BROADCAST | RTCF_LOCAL | RTCF_REJECT))) {
+				if (!nfp_hook_fib_rule_add(AF_INET, (u8*)(&p->rt_src), (u8*)(&p->rt_dst), 
+						  (u8*)(&p->rt_gateway), p->rt_iif, p->u.dst.dev->ifindex)) 
+					rth->nfp = true;
+			}
+			prev = &p->u.dst.rt_next;
+		}
+	
+		spin_unlock_bh(rt_hash_lock_addr(i));
+
+		for (; rth != tail; rth = next) {
+			next = rth->u.dst.rt_next;
+			if (!(rth->rt_flags & (RTCF_MULTICAST | RTCF_BROADCAST | RTCF_LOCAL | RTCF_REJECT))) {
+				if (!nfp_hook_fib_rule_add(AF_INET, (u8*)(&rth->rt_src), (u8*)(&rth->rt_dst), 
+					  (u8*)(&rth->rt_gateway), rth->rt_iif, rth->u.dst.dev->ifindex)) 
+					rth->nfp = true;
+			}
+		}
+	}
+}
+#endif /* CONFIG_MV_ETH_NFP_FIB_LEARN */
+
+
 /*
  * While freeing expired entries, we compute average chain length
  * and standard deviation, using fixed-point arithmetic.
@@ -2073,6 +2142,15 @@ static int __mkroute_input(struct sk_buff *skb,
 
 	rth->rt_flags = flags;
 
+#if defined(CONFIG_MV_ETH_NFP_FIB_LEARN)
+	rth->nfp = false;
+	if (!(rth->rt_flags & (RTCF_MULTICAST | RTCF_BROADCAST | RTCF_LOCAL | RTCF_REJECT))) {
+		if (!nfp_hook_fib_rule_add(AF_INET, (u8*)(&rth->rt_src), (u8*)(&rth->rt_dst), 
+					(u8*)(&rth->rt_gateway), rth->rt_iif, rth->u.dst.dev->ifindex))
+			rth->nfp = true;
+	}
+#endif /* CONFIG_MV_ETH_NFP_FIB_LEARN */
+
 	*result = rth;
 	err = 0;
  cleanup:
diff --git a/net/ipv4/tcp.c b/net/ipv4/tcp.c
index b2869b4..d5d8329 100644
--- a/net/ipv4/tcp.c
+++ b/net/ipv4/tcp.c
@@ -1440,6 +1440,20 @@ int tcp_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
 
 		if (skb)
 			available = TCP_SKB_CB(skb)->seq + skb->len - (*seq);
+#ifdef CONFIG_SPLICE_NET_DMA_SUPPORT
+		if (msg->msg_flags & MSG_KERNSPACE) {
+			if ((available >= target) &&
+			    (len > sysctl_tcp_dma_copybreak) && !(flags & MSG_PEEK) &&
+			    !sysctl_tcp_low_latency &&
+			    dma_find_channel(DMA_MEMCPY)) {
+				preempt_enable_no_resched();
+				tp->ucopy.pinned_list =
+						dma_pin_kernel_iovec_pages(msg->msg_iov, len);
+			} else {
+				preempt_enable_no_resched();
+			}
+		}
+#else
 		if ((available < target) &&
 		    (len > sysctl_tcp_dma_copybreak) && !(flags & MSG_PEEK) &&
 		    !sysctl_tcp_low_latency &&
@@ -1450,14 +1464,30 @@ int tcp_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
 		} else {
 			preempt_enable_no_resched();
 		}
+#endif
 	}
 #endif
 
 	do {
 		u32 offset;
 
+		if (flags & MSG_NOCATCHSIG) {
+			if (signal_pending(current)) {
+				if (sigismember(&current->pending.signal, SIGQUIT) ||
+				    sigismember(&current->pending.signal, SIGABRT) ||
+				    sigismember(&current->pending.signal, SIGKILL) ||
+				    sigismember(&current->pending.signal, SIGTERM) ||
+				    sigismember(&current->pending.signal, SIGSTOP)) {
+
+					if (copied)
+						break;
+					copied = timeo ? sock_intr_errno(timeo) : -EAGAIN;
+					break;
+				}
+			}
+		}
 		/* Are we at urgent data? Stop if we have read anything or have SIGURG pending. */
-		if (tp->urg_data && tp->urg_seq == *seq) {
+		else if (tp->urg_data && tp->urg_seq == *seq) {
 			if (copied)
 				break;
 			if (signal_pending(current)) {
@@ -1686,6 +1716,10 @@ do_prequeue:
 			} else
 #endif
 			{
+				if (msg->msg_flags & MSG_KERNSPACE)
+					err = skb_copy_datagram_to_kernel_iovec(skb,
+							offset, msg->msg_iov, used);
+				else
 				err = skb_copy_datagram_iovec(skb, offset,
 						msg->msg_iov, used);
 				if (err) {
@@ -1753,6 +1787,11 @@ skip_copy:
 	tp->ucopy.dma_chan = NULL;
 
 	if (tp->ucopy.pinned_list) {
+#ifdef CONFIG_SPLICE_NET_DMA_SUPPORT
+		if(msg->msg_flags & MSG_KERNSPACE)
+			dma_unpin_kernel_iovec_pages(tp->ucopy.pinned_list);
+		else
+#endif
 		dma_unpin_iovec_pages(tp->ucopy.pinned_list);
 		tp->ucopy.pinned_list = NULL;
 	}
diff --git a/net/ipv6/ip6_fib.c b/net/ipv6/ip6_fib.c
index 6b82e02..4e10c09 100644
--- a/net/ipv6/ip6_fib.c
+++ b/net/ipv6/ip6_fib.c
@@ -47,6 +47,13 @@
 #define RT6_TRACE(x...) do { ; } while (0)
 #endif
 
+#if defined (CONFIG_MV_ETH_NFP_FIB_LEARN)
+extern int nfp_hook_fib_rule_add(int family, const u8 *src_l3, const u8 *dst_l3,
+				const u8 *def_gtw_l3, int iif, int oif);
+extern int nfp_hook_fib_rule_del(int family, const u8 *src_l3, const u8 *dst_l3, int iif, int oif);
+extern int nfp_hook_fib_rule_age(int family, u8 *src_l3, u8 *dst_l3, int iif, int oif);
+#endif /*  CONFIG_MV_ETH_NFP_FIB_LEARN */
+
 static struct kmem_cache * fib6_node_kmem __read_mostly;
 
 enum fib_walk_state_t
@@ -405,6 +412,53 @@ out:
 	return res;
 }
 
+
+#if defined(CONFIG_MV_ETH_NFP_FIB_LEARN)
+static int fib6_add_node(struct fib6_walker_t *w)
+{
+	struct rt6_info *rt;
+	for (rt = w->leaf; rt; rt = rt->u.dst.rt6_next) {
+		if (rt->rt6i_flags & RTF_CACHE) {
+			if (!nfp_hook_fib_rule_add(AF_INET6, (u8 *)&rt->rt6i_src.addr,
+				(u8 *)&rt->rt6i_dst.addr,
+				(u8 *)&rt->rt6i_gateway,
+				rt->rt6i_iifindex,
+				rt->rt6i_dev->ifindex))
+				rt->nfp = true;
+		}
+	}
+	w->leaf = rt;
+	return 0;
+}
+
+void nfp_fib6_sync(void)
+{
+	struct fib6_table *table;
+	struct hlist_node *node;
+	struct hlist_head *head;
+	unsigned int h;
+	int res;
+	struct net *net = &init_net;
+	struct fib6_walker_t w;
+
+	w.func = fib6_add_node;
+	w.prune = 0;
+
+	rcu_read_lock();
+	for (h = 0; h < FIB6_TABLE_HASHSZ; h++) {
+		head = &net->ipv6.fib_table_hash[h];
+		hlist_for_each_entry_rcu(table, node, head, tb6_hlist) {
+			write_lock_bh(&table->tb6_lock);
+			w.root = &table->tb6_root;
+			res = fib6_walk(&w);
+			write_unlock_bh(&table->tb6_lock);
+		}
+	}
+	rcu_read_unlock();
+
+}
+#endif /* CONFIG_MV_ETH_NFP_FIB_LEARN */
+
 /*
  *	Routing Table
  *
@@ -658,6 +712,18 @@ static int fib6_add_rt2node(struct fib6_node *fn, struct rt6_info *rt,
 		fn->fn_flags |= RTN_RTINFO;
 	}
 
+#if defined (CONFIG_MV_ETH_NFP_FIB_LEARN)
+	rt->nfp = false;
+	if (rt->rt6i_flags & RTF_CACHE)	{
+		if (!nfp_hook_fib_rule_add(AF_INET6, (u8 *)&rt->rt6i_src.addr,
+				(u8 *)&rt->rt6i_dst.addr,
+				(u8 *)&rt->rt6i_gateway,
+				rt->rt6i_iifindex,
+				rt->rt6i_dev->ifindex))
+				rt->nfp = true; 
+	}
+#endif /* CONFIG_MV_ETH_NFP_FIB_LEARN */
+
 	return 0;
 }
 
@@ -1146,6 +1212,14 @@ static void fib6_del_route(struct fib6_node *fn, struct rt6_info **rtp,
 	}
 
 	inet6_rt_notify(RTM_DELROUTE, rt, info);
+
+#if defined (CONFIG_MV_ETH_NFP_FIB_LEARN)    
+	if (rt->rt6i_flags & RTF_CACHE)
+		if (rt->nfp)
+			nfp_hook_fib_rule_del(AF_INET6, (u8 *)&rt->rt6i_src.addr, (u8 *)&rt->rt6i_dst.addr,
+							rt->rt6i_iifindex, rt->rt6i_dev->ifindex);				
+#endif /*  CONFIG_MV_ETH_NFP_FIB_LEARN  */
+	
 	rt6_release(rt);
 }
 
@@ -1446,6 +1520,13 @@ static int fib6_age(struct rt6_info *rt, void *arg)
 				  rt);
 			return -1;
 		}
+#if defined(CONFIG_MV_ETH_NFP_FIB_LEARN)
+		if (rt->nfp) {
+			if (nfp_hook_fib_rule_age(AF_INET6,(u8 *)(&rt->rt6i_src.addr), (u8 *)(&rt->rt6i_dst.addr),
+					rt->rt6i_iifindex, rt->rt6i_dev->ifindex))
+				rt->u.dst.lastuse = now;
+		}
+#endif /* CONFIG_MV_ETH_NFP_FIB_LEARN */
 		gc_args.more++;
 	}
 
diff --git a/net/ipv6/netfilter/Makefile b/net/ipv6/netfilter/Makefile
index aafbba3..75a186c 100644
--- a/net/ipv6/netfilter/Makefile
+++ b/net/ipv6/netfilter/Makefile
@@ -24,6 +24,7 @@ obj-$(CONFIG_IP6_NF_MATCH_IPV6HEADER) += ip6t_ipv6header.o
 obj-$(CONFIG_IP6_NF_MATCH_MH) += ip6t_mh.o
 obj-$(CONFIG_IP6_NF_MATCH_OPTS) += ip6t_hbh.o
 obj-$(CONFIG_IP6_NF_MATCH_RT) += ip6t_rt.o
+obj-$(CONFIG_IP6_NF_TARGET_NFP) += ip6t_NFP.o
 
 # targets
 obj-$(CONFIG_IP6_NF_TARGET_LOG) += ip6t_LOG.o
diff --git a/net/ipv6/route.c b/net/ipv6/route.c
index 4f4b21d..4040bd6 100644
--- a/net/ipv6/route.c
+++ b/net/ipv6/route.c
@@ -745,6 +745,14 @@ restart:
 
 	dst_hold(&rt->u.dst);
 	if (nrt) {
+#if defined(CONFIG_MV_ETH_NFP_FIB_LEARN)
+			if ((rt->rt6i_flags & RTF_CACHE)) {
+				ipv6_addr_copy(&rt->rt6i_src.addr, &fl->fl6_src);
+				rt->rt6i_src.plen = 128;
+				rt->rt6i_iifindex = fl->iif;
+			}
+#endif /* CONFIG_MV_ETH_NFP_FIB_LEARN */
+		
 		err = ip6_ins_rt(nrt);
 		if (!err)
 			goto out2;
diff --git a/net/netfilter/nf_conntrack_core.c b/net/netfilter/nf_conntrack_core.c
index a1af890..b5c1930 100644
--- a/net/netfilter/nf_conntrack_core.c
+++ b/net/netfilter/nf_conntrack_core.c
@@ -46,6 +46,20 @@
 #include <net/netfilter/nf_nat.h>
 #include <net/netfilter/nf_nat_core.h>
 
+#include <linux/netfilter/ipt_NFP.h>
+
+#if defined(CONFIG_MV_ETH_NFP_CT_LEARN)
+extern void nfp_hook_ct_del(int family, u8 *src_l3, u8 *dst_l3, u16 sport, u16 dport, u8 proto);
+extern int  nfp_hook_ct_age(int family, u8 *src_l3, u8 *dst_l3, u16 sport, u16 dport, u8 proto);
+extern void nfp_ct_sync(int family);
+#ifdef CONFIG_MV_ETH_NFP_NAT
+extern void move_nat_to_nfp(const struct nf_conntrack_tuple *tuple,
+							const struct nf_conntrack_tuple *target,
+							enum nf_nat_manip_type maniptype);
+#endif /* CONFIG_MV_ETH_NFP_NAT */
+extern int move_fwd_to_nfp(const struct nf_conntrack_tuple *tuple, int mode);
+#endif /* CONFIG_MV_ETH_NFP_CT_LEARN */
+
 #define NF_CONNTRACK_VERSION	"0.5.0"
 
 int (*nfnetlink_parse_nat_setup_hook)(struct nf_conn *ct,
@@ -202,6 +216,13 @@ destroy_conntrack(struct nf_conntrack *nfct)
 	 * too. */
 	nf_ct_remove_expectations(ct);
 
+#if defined(CONFIG_NETFILTER_XT_MATCH_LAYER7) || defined(CONFIG_NETFILTER_XT_MATCH_LAYER7_MODULE)
+	if(ct->layer7.app_proto)
+		kfree(ct->layer7.app_proto);
+	if(ct->layer7.app_data)
+		kfree(ct->layer7.app_data);
+#endif
+
 	/* We overload first tuple to link into unconfirmed list. */
 	if (!nf_ct_is_confirmed(ct)) {
 		BUG_ON(hlist_nulls_unhashed(&ct->tuplehash[IP_CT_DIR_ORIGINAL].hnnode));
@@ -273,6 +294,82 @@ static void death_by_timeout(unsigned long ul_conntrack)
 {
 	struct nf_conn *ct = (void *)ul_conntrack;
 
+#if defined (CONFIG_MV_ETH_NFP_CT_LEARN)
+	struct nf_conntrack_tuple *t0 = &ct->tuplehash[IP_CT_DIR_ORIGINAL].tuple;
+	struct nf_conntrack_tuple *t1 = &ct->tuplehash[IP_CT_DIR_REPLY].tuple;
+	int confirmed_org = 0, confirmed_reply = 0;
+
+	if (t0 && t0->nfp) {
+		if (t0->src.l3num == AF_INET)
+			confirmed_org = nfp_hook_ct_age(t0->src.l3num, (u8 *)&(t0->src.u3.ip),
+					(u8 *)&(t0->dst.u3.ip),
+					ntohs(t0->src.u.all),
+					ntohs(t0->dst.u.all),
+					t0->dst.protonum);
+		else
+			confirmed_org = nfp_hook_ct_age(t0->src.l3num, (u8 *)&(t0->src.u3.ip6),
+					(u8 *)&(t0->dst.u3.ip6),
+					ntohs(t0->src.u.all),
+					ntohs(t0->dst.u.all),
+					t0->dst.protonum);
+	}
+
+	if (t1 && t1->nfp) {
+		if (t1->src.l3num == AF_INET)
+			confirmed_reply = nfp_hook_ct_age(t1->src.l3num, (u8 *)&(t1->src.u3.ip),
+					(u8 *)&(t1->dst.u3.ip),
+					ntohs(t1->src.u.all),
+					ntohs(t1->dst.u.all),
+					t1->dst.protonum);
+		else
+			confirmed_reply = nfp_hook_ct_age(t1->src.l3num, (u8 *)&(t1->src.u3.ip6),
+					(u8 *)&(t1->dst.u3.ip6),
+					ntohs(t1->src.u.all),
+					ntohs(t1->dst.u.all),
+					t1->dst.protonum);
+	}
+
+	if (confirmed_org || confirmed_reply) {
+		ct->timeout.expires = jiffies + (20*HZ);
+		add_timer(&ct->timeout);
+		return;
+	}
+
+	if (t0 && t0->nfp) {
+		t0->nfp = false;
+
+		if (t0->src.l3num == AF_INET)
+			nfp_hook_ct_del(t0->src.l3num, (u8 *)&(t0->src.u3.ip),
+					(u8 *)&(t0->dst.u3.ip),
+					ntohs(t0->src.u.all),
+					ntohs(t0->dst.u.all),
+					t0->dst.protonum);
+		else
+			nfp_hook_ct_del(t0->src.l3num, (u8 *)&(t0->src.u3.ip6),
+					(u8 *)&(t0->dst.u3.ip6),
+					ntohs(t0->src.u.all),
+					ntohs(t0->dst.u.all),
+					t0->dst.protonum);
+	}
+
+	if (t1 && t1->nfp) {
+		t1->nfp = false;
+
+		if (t1->src.l3num == AF_INET)
+			nfp_hook_ct_del(t1->src.l3num, (u8 *)&(t1->src.u3.ip),
+					(u8 *)&(t1->dst.u3.ip),
+					ntohs(t1->src.u.all),
+					ntohs(t1->dst.u.all),
+					t1->dst.protonum);
+		else
+			nfp_hook_ct_del(t1->src.l3num, (u8 *)&(t1->src.u3.ip6),
+					(u8 *)&(t1->dst.u3.ip6),
+					ntohs(t1->src.u.all),
+					ntohs(t1->dst.u.all),
+					t1->dst.protonum);
+	}
+#endif /* CONFIG_MV_ETH_NFP_CT_LEARN */
+
 	if (!test_bit(IPS_DYING_BIT, &ct->status) &&
 	    unlikely(nf_conntrack_event(IPCT_DESTROY, ct) < 0)) {
 		/* destroy event was not delivered */
@@ -610,6 +707,19 @@ struct nf_conn *nf_conntrack_alloc(struct net *net, u16 zone,
 #ifdef CONFIG_NET_NS
 	ct->ct_net = net;
 #endif
+
+#if defined(CONFIG_MV_ETH_NFP_CT_LEARN)
+	ct->tuplehash[IP_CT_DIR_ORIGINAL].tuple.nfp = false;
+	ct->tuplehash[IP_CT_DIR_ORIGINAL].tuple.ifindex = -1;
+	ct->tuplehash[IP_CT_DIR_ORIGINAL].tuple.nfpCapable = false;
+	ct->tuplehash[IP_CT_DIR_ORIGINAL].tuple.info = NULL;
+
+	ct->tuplehash[IP_CT_DIR_REPLY].tuple.nfp = false;
+	ct->tuplehash[IP_CT_DIR_REPLY].tuple.ifindex = -1;
+	ct->tuplehash[IP_CT_DIR_REPLY].tuple.nfpCapable = false;
+	ct->tuplehash[IP_CT_DIR_REPLY].tuple.info = NULL;
+#endif /* CONFIG_MV_ETH_NFP_CT_LEARN */
+
 #ifdef CONFIG_NF_CONNTRACK_ZONES
 	if (zone) {
 		struct nf_conntrack_zone *nf_ct_zone;
@@ -639,6 +749,14 @@ void nf_conntrack_free(struct nf_conn *ct)
 {
 	struct net *net = nf_ct_net(ct);
 
+#if defined(CONFIG_MV_ETH_NFP_CT_LEARN)
+	if (ct->tuplehash[IP_CT_DIR_ORIGINAL].tuple.info)
+		kfree(ct->tuplehash[IP_CT_DIR_ORIGINAL].tuple.info);
+
+	if (ct->tuplehash[IP_CT_DIR_REPLY].tuple.info)
+		kfree(ct->tuplehash[IP_CT_DIR_REPLY].tuple.info);
+#endif /* CONFIG_MV_ETH_NFP_CT_LEARN */
+
 	nf_ct_ext_destroy(ct);
 	atomic_dec(&net->ct.count);
 	nf_ct_ext_free(ct);
@@ -1310,6 +1428,84 @@ EXPORT_SYMBOL_GPL(nf_conntrack_set_hashsize);
 module_param_call(hashsize, nf_conntrack_set_hashsize, param_get_uint,
 		  &nf_conntrack_htable_size, 0600);
 
+#if defined(CONFIG_MV_ETH_NFP_CT_LEARN)
+void nfp_ct_sync(int family)
+{
+	struct nf_conntrack_tuple_hash *h;
+	struct nf_conn *ct;
+	struct nf_conntrack_tuple *tuple;
+	struct nf_conntrack_tuple target_tuple;
+	struct hlist_nulls_node *n;
+	unsigned int bucket = 0;
+	enum ip_conntrack_dir dir;
+	struct net *net = &init_net;
+	unsigned long status;
+
+	spin_lock_bh(&nf_conntrack_lock);
+	/* Go over all tuples in the Linux database */
+	for (; bucket < net->ct.htable_size; bucket++) {
+		hlist_nulls_for_each_entry(h, n, &net->ct.hash[bucket], hnnode) {
+			dir = NF_CT_DIRECTION(h);
+			tuple = &h[IP_CT_DIR_ORIGINAL].tuple;
+
+			ct = nf_ct_tuplehash_to_ctrack(h);
+			if (tuple->src.l3num != family)
+				continue;
+
+			/* We want to add only NFP capable rules*/
+			if (!tuple->nfpCapable)
+				continue;
+
+			if ((tuple->info->mode != IPT_NFP_DROP) && (tuple->info->mode != IPT_NFP_FWD))
+				continue;
+
+			tuple->nfp = true;
+
+			if (tuple->info->mode == IPT_NFP_DROP) {
+				move_fwd_to_nfp(tuple, 0);
+				continue;
+			}
+
+			status = ct->status;
+
+#ifdef CONFIG_MV_ETH_NFP_NAT
+			if (status & IPS_NAT_MASK) {
+				/* NFP NAT is supported only in IPv4 */
+				if (tuple->src.l3num == AF_INET) {
+					/* status says if the original direction requires SNAT or DNAT (or both) */
+					/* if we currently work on the reply direction, we need to "reverse" the NAT status, */
+					/* e.g. if original direction was SNAT, reply should be DNAT. */
+					if (dir != IP_CT_DIR_ORIGINAL)
+						status ^= IPS_NAT_MASK;
+
+					nf_ct_invert_tuplepr(&target_tuple, &ct->tuplehash[!dir].tuple);
+
+					if ((status & IPS_NAT_MASK) == IPS_DST_NAT) {
+						move_nat_to_nfp(tuple, &target_tuple, IP_NAT_MANIP_DST);
+					} else if ((status & IPS_NAT_MASK) == IPS_SRC_NAT) {
+						move_nat_to_nfp(tuple, &target_tuple, IP_NAT_MANIP_SRC);
+					} else {
+						move_nat_to_nfp(tuple, &target_tuple, IP_NAT_MANIP_DST);
+						move_nat_to_nfp(tuple, &target_tuple, IP_NAT_MANIP_SRC);
+					}
+
+					continue;
+				} else {
+					/* NFP does not support NAT for IPv6, so nothing to do with this tuple */
+					tuple->nfp = false;
+					continue;
+				}
+			}
+#endif /* CONFIG_MV_ETH_NFP_NAT */
+			/* If we got till here, it must be IPT_NFP_FWD */
+			move_fwd_to_nfp(tuple, 1);
+		}
+	}
+
+	spin_unlock_bh(&nf_conntrack_lock);
+}
+#endif /* CONFIG_MV_ETH_NFP_CT_LEARN */
+
 static int nf_conntrack_init_init_net(void)
 {
 	int max_factor = 8;
diff --git a/net/netfilter/nf_conntrack_proto_tcp.c b/net/netfilter/nf_conntrack_proto_tcp.c
index 9dd8cd4..b9b0e2b 100644
--- a/net/netfilter/nf_conntrack_proto_tcp.c
+++ b/net/netfilter/nf_conntrack_proto_tcp.c
@@ -824,6 +824,9 @@ static int tcp_packet(struct nf_conn *ct,
 {
 	struct net *net = nf_ct_net(ct);
 	struct nf_conntrack_tuple *tuple;
+#if defined(CONFIG_MV_ETH_NFP_CT_LEARN)
+	struct nf_conntrack_tuple *tupleInverseDir;
+#endif
 	enum tcp_conntrack new_state, old_state;
 	enum ip_conntrack_dir dir;
 	const struct tcphdr *th;
@@ -840,7 +843,9 @@ static int tcp_packet(struct nf_conn *ct,
 	index = get_conntrack_index(th);
 	new_state = tcp_conntracks[dir][index][old_state];
 	tuple = &ct->tuplehash[dir].tuple;
-
+#if defined(CONFIG_MV_ETH_NFP_CT_LEARN)
+	tupleInverseDir	= &ct->tuplehash[!dir].tuple;
+#endif
 	switch (new_state) {
 	case TCP_CONNTRACK_SYN_SENT:
 		if (old_state < TCP_CONNTRACK_TIME_WAIT)
@@ -992,6 +997,15 @@ static int tcp_packet(struct nf_conn *ct,
 		break;
 	}
 
+#if defined(CONFIG_MV_ETH_NFP_CT_LEARN)
+	/*
+	 * When connection is handled by NFP, we have to relax TCP tracking
+	 * rules as not all packets goes through Linux conntrack.
+	 */
+	if ((tuple->nfp) || (tupleInverseDir->nfp))
+		goto in_window;
+#endif /* CONFIG_MV_ETH_NFP_CT_LEARN */
+
 	if (!tcp_in_window(ct, &ct->proto.tcp, dir, index,
 			   skb, dataoff, th, pf)) {
 		spin_unlock_bh(&ct->lock);
diff --git a/net/netfilter/nf_conntrack_standalone.c b/net/netfilter/nf_conntrack_standalone.c
index faa8eb3..bddbb1e 100644
--- a/net/netfilter/nf_conntrack_standalone.c
+++ b/net/netfilter/nf_conntrack_standalone.c
@@ -178,6 +178,25 @@ static int ct_seq_show(struct seq_file *s, void *v)
 		goto release;
 #endif
 
+#if defined(CONFIG_NETFILTER_XT_MATCH_LAYER7) || defined(CONFIG_NETFILTER_XT_MATCH_LAYER7_MODULE)
+	if(ct->layer7.app_proto &&
+           seq_printf(s, "l7proto=%s ", ct->layer7.app_proto))
+		goto release;
+#endif
+
+#if defined(CONFIG_MV_ETH_NFP_CT_LEARN)
+	if ((ct->tuplehash[IP_CT_DIR_ORIGINAL].tuple.nfp) && (ct->tuplehash[IP_CT_DIR_REPLY].tuple.nfp)) {
+		if (seq_printf(s, "[NFP (both)] "))
+			goto release;
+	} else if (ct->tuplehash[IP_CT_DIR_ORIGINAL].tuple.nfp) {
+		if (seq_printf(s, "[NFP (orig)] "))
+			goto release;
+	} else if (ct->tuplehash[IP_CT_DIR_REPLY].tuple.nfp) {
+		if (seq_printf(s, "[NFP (reply)] "))
+			goto release;
+	}
+#endif /* CONFIG_MV_ETH_NFP_CT_LEARN */
+
 	if (seq_printf(s, "use=%u\n", atomic_read(&ct->ct_general.use)))
 		goto release;
 
diff --git a/net/socket.c b/net/socket.c
index 25d3742..df26076 100644
--- a/net/socket.c
+++ b/net/socket.c
@@ -1721,6 +1721,7 @@ SYSCALL_DEFINE6(sendto, int, fd, void __user *, buff, size_t, len,
 	msg.msg_iovlen = 1;
 	msg.msg_control = NULL;
 	msg.msg_controllen = 0;
+	msg.msg_flags = 0;
 	msg.msg_namelen = 0;
 	if (addr) {
 		err = move_addr_to_kernel(addr, addr_len, (struct sockaddr *)&address);
-- 
1.7.3.4

