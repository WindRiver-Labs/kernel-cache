From 329b9ba115ab28eb6d40a7beae8563a5bd1c7644 Mon Sep 17 00:00:00 2001
From: Michael Gill <michael.gill@xilinx.com>
Date: Tue, 20 Sep 2016 10:34:57 -0700
Subject: [PATCH 204/223] staging: apf: Make xlnk driver thread-safe

This patch comes from:
  https://github.com/Xilinx/linux-xlnx.git

This patch enables the first stage of enabling threading in SDSoC.
Accelerators cannot be shared among threads, but concurrent
use of disjoint accelerators is now supported.

Signed-off-by: Michael Gill <gill@xilinx.com>
Signed-off-by: Michal Simek <michal.simek@xilinx.com>
(cherry picked from commit 9c0b46ee9448cf742d5ffa22436559e3d1d8c6a1)
---
 drivers/staging/apf/xilinx-dma-apf.c | 93 +++++++++++++++++-------------------
 drivers/staging/apf/xilinx-dma-apf.h |  1 +
 drivers/staging/apf/xlnk.c           | 82 +++++++++++++++++++++----------
 3 files changed, 102 insertions(+), 74 deletions(-)

diff --git a/drivers/staging/apf/xilinx-dma-apf.c b/drivers/staging/apf/xilinx-dma-apf.c
index 7e4c6cf..0a7729c 100644
--- a/drivers/staging/apf/xilinx-dma-apf.c
+++ b/drivers/staging/apf/xilinx-dma-apf.c
@@ -533,37 +533,41 @@ out_unlock:
 	return status;
 }
 
-#define XDMA_SGL_MAX_LEN	XDMA_MAX_BD_CNT
-static struct scatterlist sglist_array[XDMA_SGL_MAX_LEN];
-
 /*
  *  create minimal length scatter gather list for physically contiguous buffer
  *  that starts at phy_buf and has length phy_buf_len bytes
  */
-static unsigned int phy_buf_to_sgl(void *phy_buf, unsigned int phy_buf_len,
-			struct scatterlist **sgl)
+static unsigned int phy_buf_to_sgl(xlnk_intptr_type phy_buf,
+				   unsigned int phy_buf_len,
+				   struct scatterlist *sgl)
 {
 	unsigned int sgl_cnt = 0;
 	struct scatterlist *sgl_head;
 	unsigned int dma_len;
+	unsigned int num_bd;
 
 	if (!phy_buf || !phy_buf_len) {
 		pr_err("phy_buf is NULL or phy_buf_len = 0\n");
 		return sgl_cnt;
 	}
 
-	*sgl = sglist_array;
-	sgl_head = *sgl;
+	num_bd = (phy_buf_len + (XDMA_MAX_TRANS_LEN - 1))
+		/ XDMA_MAX_TRANS_LEN;
+	sgl_head = sgl;
+	sg_init_table(sgl, num_bd);
 
 	while (phy_buf_len > 0) {
+		xlnk_intptr_type page_id = phy_buf >> PAGE_SHIFT;
+		unsigned int offset = phy_buf - (page_id << PAGE_SHIFT);
 
 		sgl_cnt++;
-		if (sgl_cnt > XDMA_SGL_MAX_LEN)
+		if (sgl_cnt > XDMA_MAX_BD_CNT)
 			return 0;
 
 		dma_len = (phy_buf_len > XDMA_MAX_TRANS_LEN) ?
 				XDMA_MAX_TRANS_LEN : phy_buf_len;
 
+		sg_set_page(sgl_head, pfn_to_page(page_id), dma_len, offset);
 		sg_dma_address(sgl_head) = (dma_addr_t)phy_buf;
 		sg_dma_len(sgl_head) = dma_len;
 		sgl_head = sg_next(sgl_head);
@@ -577,14 +581,13 @@ static unsigned int phy_buf_to_sgl(void *phy_buf, unsigned int phy_buf_len,
 
 /*  merge sg list, sgl, with length sgl_len, to sgl_merged, to save dma bds */
 static unsigned int sgl_merge(struct scatterlist *sgl, unsigned int sgl_len,
-			struct scatterlist **sgl_merged)
+			struct scatterlist *sgl_merged)
 {
 	struct scatterlist *sghead, *sgend, *sgnext, *sg_merged_head;
 	unsigned int sg_visited_cnt = 0, sg_merged_num = 0;
 	unsigned int dma_len = 0;
 
-	*sgl_merged = sglist_array;
-	sg_merged_head = *sgl_merged;
+	sg_merged_head = sgl_merged;
 	sghead = sgl;
 
 	while (sghead && (sg_visited_cnt < sgl_len)) {
@@ -611,7 +614,7 @@ static unsigned int sgl_merge(struct scatterlist *sgl, unsigned int sgl_len,
 		}
 
 		sg_merged_num++;
-		if (sg_merged_num > XDMA_SGL_MAX_LEN)
+		if (sg_merged_num > XDMA_MAX_BD_CNT)
 			return 0;
 
 		memcpy(sg_merged_head, sghead, sizeof(struct scatterlist));
@@ -625,8 +628,6 @@ static unsigned int sgl_merge(struct scatterlist *sgl, unsigned int sgl_len,
 	return sg_merged_num;
 }
 
-static int mapped_pages_count;
-static struct page **mapped_pages;
 static int pin_user_pages(xlnk_intptr_type uaddr,
 			  unsigned int ulen,
 			  int write,
@@ -641,6 +642,7 @@ static int pin_user_pages(xlnk_intptr_type uaddr,
 	unsigned int last_page;
 	unsigned int num_pages;
 	struct scatterlist *sglist;
+	struct page **mapped_pages;
 
 	unsigned int pgidx;
 	unsigned int pglen;
@@ -650,15 +652,9 @@ static int pin_user_pages(xlnk_intptr_type uaddr,
 	first_page = uaddr / PAGE_SIZE;
 	last_page = (uaddr + ulen - 1) / PAGE_SIZE;
 	num_pages = last_page - first_page + 1;
-	if (mapped_pages_count < num_pages) {
-		if (mapped_pages)
-			vfree(mapped_pages);
-		mapped_pages_count = num_pages * 2;
-		mapped_pages = vmalloc(sizeof(*mapped_pages) *
-				       mapped_pages_count);
-		if (!mapped_pages)
-			return -ENOMEM;
-	}
+	mapped_pages = vmalloc(sizeof(*mapped_pages) * num_pages);
+	if (!mapped_pages)
+		return -ENOMEM;
 
 	down_read(&mm->mmap_sem);
 	status = get_user_pages(uaddr, num_pages, write, 1,
@@ -672,6 +668,7 @@ static int pin_user_pages(xlnk_intptr_type uaddr,
 		if (sglist == NULL) {
 			pr_err("%s: kcalloc failed to create sg list\n",
 			       __func__);
+			vfree(mapped_pages);
 			return -ENOMEM;
 		}
 		sg_init_table(sglist, num_pages);
@@ -703,16 +700,17 @@ static int pin_user_pages(xlnk_intptr_type uaddr,
 		*scatterpp = sglist;
 		*cntp = num_pages;
 
+		vfree(mapped_pages);
 		return 0;
 	} else {
 		pr_err("Failed to pin user pages\n");
 		for (pgidx = 0; pgidx < status; pgidx++) {
 			put_page(mapped_pages[pgidx]);
 		}
+		vfree(mapped_pages);
 		return -ENOMEM;
 	}
 }
-
 static int unpin_user_pages(struct scatterlist *sglist, unsigned int cnt)
 {
 	struct page *pg;
@@ -821,7 +819,6 @@ int xdma_submit(struct xdma_chan *chan,
 	dmahead->userflag = user_flags;
 	dmadir = chan->direction;
 	if (dp) {
-
 		if (!dp->is_mapped) {
 			struct scatterlist *sg;
 			int cpy_size;
@@ -868,35 +865,25 @@ int xdma_submit(struct xdma_chan *chan,
 		dmahead->userbuf = (xlnk_intptr_type)sglist->dma_address;
 		dmahead->is_dmabuf = 1;
 	} else if (user_flags & CF_FLAG_PHYSICALLY_CONTIGUOUS) {
-		/*
-		 * convert physically contiguous buffer into
-		 * minimal length sg list
-		 */
-		sgcnt = phy_buf_to_sgl(userbuf, size, &sglist);
+		sglist = chan->scratch_sglist;
+		sgcnt = phy_buf_to_sgl(userbuf, size, sglist);
 		if (!sgcnt)
 			return -ENOMEM;
 
 		sglist_dma = sglist;
 		sgcnt_dma = sgcnt;
-		if (user_flags & CF_FLAG_CACHE_FLUSH_INVALIDATE) {
-			if (!kaddr) {
-				pr_err("Whoops, cannot flush without an address\n");
-				return -EINVAL;
-			}
-#if XLNK_SYS_BIT_WIDTH == 32
-			__cpuc_flush_dcache_area(kaddr, size);
-			outer_clean_range(userbuf,
-					  userbuf + size);
-			if (dmadir == DMA_FROM_DEVICE) {
-				outer_inv_range(userbuf,
-						userbuf + size);
-			}
-#else
-			__dma_map_area(kaddr, size, dmadir);
-#endif
+
+		status = get_dma_ops(chan->dev)->map_sg(chan->dev,
+							sglist,
+							sgcnt,
+							dmadir,
+							&attrs);
+
+		if (!status) {
+			pr_err("sg contiguous mapping failed\n");
+			return -ENOMEM;
 		}
 	} else {
-		/* pin user pages is monitored separately */
 		status = pin_user_pages(userbuf, size,
 					dmadir != DMA_TO_DEVICE,
 					&sglist, &sgcnt, user_flags);
@@ -916,7 +903,8 @@ int xdma_submit(struct xdma_chan *chan,
 		}
 
 		/* merge sg list to save dma bds */
-		sgcnt_dma = sgl_merge(sglist, sgcnt, &sglist_dma);
+		sglist_dma = chan->scratch_sglist;
+		sgcnt_dma = sgl_merge(sglist, sgcnt, sglist_dma);
 		if (!sgcnt_dma) {
 			get_dma_ops(chan->dev)->unmap_sg(chan->dev, sglist,
 							 sgcnt, dmadir, &attrs);
@@ -977,7 +965,14 @@ int xdma_wait(struct xdma_head *dmahead,
 
 	if (dmahead->is_dmabuf) {
 		dmahead->is_dmabuf = 0;
-	} else if (!(user_flags & CF_FLAG_PHYSICALLY_CONTIGUOUS)) {
+	} else if (user_flags & CF_FLAG_PHYSICALLY_CONTIGUOUS) {
+		if (user_flags & CF_FLAG_CACHE_FLUSH_INVALIDATE)
+			get_dma_ops(chan->dev)->unmap_sg(chan->dev,
+							 dmahead->sglist,
+							 dmahead->sgcnt,
+							 dmahead->dmadir,
+							 &attrs);
+	} else  {
 		if (!(user_flags & CF_FLAG_CACHE_FLUSH_INVALIDATE))
 			dma_set_attr(DMA_ATTR_SKIP_CPU_SYNC, &attrs);
 
diff --git a/drivers/staging/apf/xilinx-dma-apf.h b/drivers/staging/apf/xilinx-dma-apf.h
index 5a09cdb..e677d40 100644
--- a/drivers/staging/apf/xilinx-dma-apf.h
+++ b/drivers/staging/apf/xilinx-dma-apf.h
@@ -178,6 +178,7 @@ struct xdma_chan {
 	int    max_len;				/* Maximum len per transfer */
 	int    err;				/* Channel has errors */
 	int    client_count;
+	struct scatterlist scratch_sglist[XDMA_MAX_BD_CNT];
 };
 
 struct xdma_device {
diff --git a/drivers/staging/apf/xlnk.c b/drivers/staging/apf/xlnk.c
index f123155..a14da6db 100644
--- a/drivers/staging/apf/xlnk.c
+++ b/drivers/staging/apf/xlnk.c
@@ -94,7 +94,9 @@ static xlnk_intptr_type xlnk_userbuf[XLNK_BUF_POOL_SIZE];
 static dma_addr_t xlnk_phyaddr[XLNK_BUF_POOL_SIZE];
 static size_t xlnk_buflen[XLNK_BUF_POOL_SIZE];
 static unsigned int xlnk_bufcacheable[XLNK_BUF_POOL_SIZE];
+static spinlock_t xlnk_buf_lock;
 
+/* only used with standard DMA mode */
 static struct page **xlnk_page_store;
 static int xlnk_page_store_size;
 
@@ -145,11 +147,13 @@ struct xlnk_device_pack {
 
 };
 
+static spinlock_t xlnk_devpack_lock;
 static struct xlnk_device_pack *xlnk_devpacks[MAX_XLNK_DMAS];
 static void xlnk_devpacks_init(void)
 {
 	unsigned int i;
 
+	spin_lock_init(&xlnk_devpack_lock);
 	for (i = 0; i < MAX_XLNK_DMAS; i++)
 		xlnk_devpacks[i] = NULL;
 
@@ -169,12 +173,14 @@ static void xlnk_devpacks_add(struct xlnk_device_pack *devpack)
 {
 	unsigned int i;
 
+	spin_lock_irq(&xlnk_devpack_lock);
 	for (i = 0; i < MAX_XLNK_DMAS; i++) {
 		if (xlnk_devpacks[i] == NULL) {
 			xlnk_devpacks[i] = devpack;
 			break;
 		}
 	}
+	spin_unlock_irq(&xlnk_devpack_lock);
 }
 
 static struct xlnk_device_pack *xlnk_devpacks_find(xlnk_intptr_type base)
@@ -365,12 +371,6 @@ static int xlnk_allocbuf(unsigned int len, unsigned int cacheable)
 	dma_addr_t phys_addr_anchor;
 	unsigned int page_dst;
 
-	id = xlnk_buf_findnull();
-
-	if (id <= 0 || id >= XLNK_BUF_POOL_SIZE) {
-		pr_err("No id could be found in range\n");
-		return -ENOMEM;
-	}
 	if (cacheable)
 		kaddr = dma_alloc_noncoherent(xlnk_dev,
 					      len,
@@ -387,11 +387,22 @@ static int xlnk_allocbuf(unsigned int len, unsigned int cacheable)
 					   __GFP_REPEAT);
 	if (!kaddr)
 		return -ENOMEM;
-	xlnk_bufpool_alloc_point[id] = kaddr;
-	xlnk_bufpool[id] = kaddr;
-	xlnk_buflen[id] = len;
-	xlnk_bufcacheable[id] = cacheable;
-	xlnk_phyaddr[id] = phys_addr_anchor;
+
+	spin_lock(&xlnk_buf_lock);
+	id = xlnk_buf_findnull();
+	if (id > 0 && id < XLNK_BUF_POOL_SIZE) {
+		xlnk_bufpool_alloc_point[id] = kaddr;
+		xlnk_bufpool[id] = kaddr;
+		xlnk_buflen[id] = len;
+		xlnk_bufcacheable[id] = cacheable;
+		xlnk_phyaddr[id] = phys_addr_anchor;
+	}
+	spin_unlock(&xlnk_buf_lock);
+
+	if (id <= 0 || id >= XLNK_BUF_POOL_SIZE) {
+		pr_err("No id could be found in range\n");
+		return -ENOMEM;
+	}
 
 	return id;
 }
@@ -400,6 +411,7 @@ static int xlnk_init_bufpool(void)
 {
 	unsigned int i;
 
+	spin_lock_init(&xlnk_buf_lock);
 	xlnk_dev_buf = kmalloc(8192, GFP_KERNEL | GFP_DMA);
 	*((char *)xlnk_dev_buf) = '\0';
 
@@ -802,25 +814,37 @@ static int xlnk_allocbuf_ioctl(struct file *filp, unsigned int code,
 
 static int xlnk_freebuf(int id)
 {
+	void *alloc_point;
+	dma_addr_t p_addr;
+	size_t buf_len;
+	int cacheable;
 	if (id <= 0 || id >= xlnk_bufpool_size)
 		return -ENOMEM;
 
 	if (!xlnk_bufpool[id])
 		return -ENOMEM;
 
-	if (xlnk_bufcacheable[id])
-		dma_free_noncoherent(xlnk_dev,
-				     xlnk_buflen[id],
-				     xlnk_bufpool_alloc_point[id],
-				     xlnk_phyaddr[id]);
-	else
-		dma_free_coherent(xlnk_dev,
-				  xlnk_buflen[id],
-				  xlnk_bufpool_alloc_point[id],
-				  xlnk_phyaddr[id]);
+	spin_lock(&xlnk_buf_lock);
+	alloc_point = xlnk_bufpool_alloc_point[id];
+	p_addr = xlnk_phyaddr[id];
+	buf_len = xlnk_buflen[id];
 	xlnk_bufpool[id] = NULL;
 	xlnk_phyaddr[id] = (dma_addr_t)NULL;
 	xlnk_buflen[id] = 0;
+	cacheable = xlnk_bufcacheable[id];
+	xlnk_bufcacheable[id] = 0;
+	spin_unlock(&xlnk_buf_lock);
+
+	if (cacheable)
+		dma_free_noncoherent(xlnk_dev,
+				     buf_len,
+				     alloc_point,
+				     p_addr);
+	else
+		dma_free_coherent(xlnk_dev,
+				  buf_len,
+				  alloc_point,
+				  p_addr);
 
 	return 0;
 }
@@ -1156,16 +1180,20 @@ static int xlnk_dmasubmit_ioctl(struct file *filp, unsigned int code,
 		}
 		temp_args.dmasubmit.dmahandle = (xlnk_intptr_type)t;
 	} else {
-		int buf_id =
-			xlnk_buf_find_by_phys_addr(temp_args.dmasubmit.buf);
+		int buf_id;
 		void *kaddr = NULL;
 
+		spin_lock(&xlnk_buf_lock);
+		buf_id =
+			xlnk_buf_find_by_phys_addr(temp_args.dmasubmit.buf);
 		if (buf_id) {
 			xlnk_intptr_type addr_delta =
 				temp_args.dmasubmit.buf -
 				xlnk_phyaddr[buf_id];
 			kaddr = (u8 *)(xlnk_bufpool[buf_id]) + addr_delta;
 		}
+		spin_unlock(&xlnk_buf_lock);
+
 		status = xdma_submit((struct xdma_chan *)
 				     (temp_args.dmasubmit.dmachan),
 				     temp_args.dmasubmit.buf,
@@ -1419,12 +1447,17 @@ static int xlnk_cachecontrol_ioctl(struct file *filp, unsigned int code,
 
 	size = temp_args.cachecontrol.size;
 	paddr = temp_args.cachecontrol.phys_addr;
+
+	spin_lock(&xlnk_buf_lock);
 	buf_id = xlnk_buf_find_by_phys_addr(paddr);
+	kaddr = xlnk_bufpool[buf_id];
+	spin_unlock(&xlnk_buf_lock);
+
 	if (buf_id == 0) {
 		pr_err("Illegal cachecontrol on non-sds_alloc memory");
 		return -EINVAL;
 	}
-	kaddr = xlnk_bufpool[buf_id];
+
 #if XLNK_SYS_BIT_WIDTH == 32
 	__cpuc_flush_dcache_area(kaddr, size);
 	outer_flush_range(paddr, paddr + size);
@@ -1541,7 +1574,6 @@ static struct vm_operations_struct xlnk_vm_ops = {
 /* This function maps kernel space memory to user space memory. */
 static int xlnk_mmap(struct file *filp, struct vm_area_struct *vma)
 {
-
 	int bufid;
 	int status;
 
-- 
2.7.4

