From a6ab49c8807951eadb857169736e684bc5a35a75 Mon Sep 17 00:00:00 2001
From: Jack Tan <jack.tan@windriver.com>
Date: Wed, 17 Dec 2008 10:28:16 +0800
Subject: [PATCH] rmi xlr ethernet driver

RMI XLR on-chip ethernet driver.

Signed-off-by: Jack Tan <jack.tan@windriver.com>
---
 drivers/net/Kconfig               |   85 +
 drivers/net/Makefile              |    2 +
 drivers/net/phoenix_mac.c         | 3781 +++++++++++++++++++++++++++++++++++++
 drivers/net/phoenix_mac_devtree.c | 3227 +++++++++++++++++++++++++++++++
 drivers/net/phoenix_user_mac.c    |  939 +++++++++
 5 files changed, 8034 insertions(+), 0 deletions(-)
 create mode 100644 drivers/net/phoenix_mac.c
 create mode 100644 drivers/net/phoenix_mac_devtree.c
 create mode 100644 drivers/net/phoenix_user_mac.c

diff --git a/drivers/net/Kconfig b/drivers/net/Kconfig
index ddcf874..b10ad79 100644
--- a/drivers/net/Kconfig
+++ b/drivers/net/Kconfig
@@ -208,6 +208,91 @@ menuconfig NET_ETHERNET
 
 if NET_ETHERNET
 
+config PHOENIX_MAC
+	bool "Enable On-Chip Networking support"
+	depends on RMI_PHOENIX
+	default y
+
+config PHOENIX_GMAC_NAPI
+	bool "Enable support for NAPI mode on GMAC interfaces"
+	depends on RMI_PHOENIX
+	default y
+	help
+	  NAPI is a new driver API designed to reduce CPU and interrupt load
+	  when the driver is receiving lots of packets from the card.  
+
+	  See <file:Documentation/networking/NAPI_HOWTO.txt> for more
+	  information.
+
+	  If in doubt, say N.
+
+config PHOENIX_ON_CHIP_DEVICES_NAPI
+	bool "Demultiplex messages from on-chip devices inside NAPI poll loop"
+	depends on PHOENIX_GMAC_NAPI
+	default n
+	help
+	  Message ring interrupts may come from different sources not necessarily
+          related to GMAC/XGMAC devices (e.g. Security Engine, DMA) and NAPI 
+          message ring poll loop has to execute handlers accordingly. This 
+          option enables such demultiplexing at the expence of slight overhead in 
+          packet forwarding applications (about 100 Kpps).
+
+	  If you want to use XLR security or DMA engines along with "xlr_napi" boot 
+          option, say Y. 
+	  
+	  If you are interested in fast packet forwarding perfromance, say N.
+
+config PHOENIX_HW_BUFFER_MGMT
+	bool "Enable support for network buffer recycling via hardware"
+	depends on PHOENIX_GMAC_NAPI
+	default n
+	help
+	  Experimental addition to GMAC NAPI functionality allowing "recycling" of 
+          packet buffers by requesting HW to queue free elements upon Tx-complete 
+          back to Rx ring.
+          This type of performance ehancement is important to forwarder-like
+          applications where fast path should stay as lean as possible.
+
+	  If in doubt, say N.
+
+config PHOENIX_IP_FLOW_AFFINITY
+	bool "Enable support for IP flow affinity"
+	depends on RMI_PHOENIX
+	default n
+	help
+	  Experimental feature of GMAC driver guranteeing that IP flows are processed 
+          on logical CPUs corresponding to buckets assigned by packet classifier engine.
+          E.g. for XLR core #X, packets arriving to buckets 0 & 4 are processed by thread 0,
+          packets arriving to buckets 1 & 5 are processed by thread 1 and so on..
+          Such feature might be important for applications which require IP flows 
+          be seen on one logcal CPUs. Use of this feature involves performance cost.
+
+	  If in doubt, say N.
+
+config PHOENIX_IP_QUEUE_AFFINITY
+	bool "Enable multiprocess support for IP Queues"
+	depends on RMI_PHOENIX && IP_NF_QUEUE
+	default n
+	help
+	  Experimental feature extending IP Queues by allowing multiple user space 
+          processes to receive IP packets from the kernel. Client processes should come 
+          with CPU affinity set to single logical CPU and will get packets which are 
+          recieved and processed by network stack on that logical CPU.
+
+          Example:
+
+               Let's Process_1 has CPU affinity set to x
+               Let's Process_2 has CPU affinity set to y
+
+               Packet1 --> Interrupt on CPU x --> IP Queues --> Process_1
+               Packet1 --> Interrupt on CPU y --> IP Queues --> Process_2
+
+          This feature could be useful for packet processing architectures requiring user 
+          space handling of multiple IP flows.
+
+          If in doubt, say N.
+
+
 config MII
 	tristate "Generic Media Independent Interface device support"
 	help
diff --git a/drivers/net/Makefile b/drivers/net/Makefile
index ac4ce71..c34d616 100644
--- a/drivers/net/Makefile
+++ b/drivers/net/Makefile
@@ -125,6 +125,8 @@ obj-$(CONFIG_B44) += b44.o
 obj-$(CONFIG_FORCEDETH) += forcedeth.o
 obj-$(CONFIG_NE_H8300) += ne-h8300.o
 obj-$(CONFIG_AX88796) += ax88796.o
+obj-$(CONFIG_PHOENIX_MAC) += phoenix_mac.o phoenix_mac_devtree.o 
+obj-$(CONFIG_PHOENIX_MAC) += phoenix_user_mac.o
 
 obj-$(CONFIG_TSI108_ETH) += tsi108_eth.o
 obj-$(CONFIG_MV643XX_ETH) += mv643xx_eth.o
diff --git a/drivers/net/phoenix_mac.c b/drivers/net/phoenix_mac.c
new file mode 100644
index 0000000..f96d9f8
--- /dev/null
+++ b/drivers/net/phoenix_mac.c
@@ -0,0 +1,3781 @@
+/*********************************************************************
+
+  Copyright 2003-2006 Raza Microelectronics, Inc. (RMI). All rights
+  reserved.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+  1. Redistributions of source code must retain the above copyright
+  notice, this list of conditions and the following disclaimer.
+  2. Redistributions in binary form must reproduce the above copyright
+  notice, this list of conditions and the following disclaimer in
+  the documentation and/or other materials provided with the
+  distribution.
+
+  THIS SOFTWARE IS PROVIDED BY Raza Microelectronics, Inc. ``AS IS'' AND
+  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL RMI OR CONTRIBUTORS BE LIABLE
+  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+  THE POSSIBILITY OF SUCH DAMAGE.
+
+  *****************************#RMI_2#**********************************/
+
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/interrupt.h>
+#include <linux/fs.h>
+#include <linux/types.h>
+#include <linux/string.h>
+#include <linux/socket.h>
+#include <linux/errno.h>
+#include <linux/fcntl.h>
+#include <linux/in.h>
+#include <linux/init.h>
+#include <linux/pci.h>
+#include <linux/workqueue.h>
+#include <linux/kernel.h>
+#include <linux/inet.h>
+#include <linux/netdevice.h>
+#include <linux/ethtool.h>
+#include <linux/etherdevice.h>
+#include <linux/skbuff.h>
+#include <net/sock.h>
+#include <linux/if_ether.h>	/* For the statistics structure. */
+#include <linux/if_arp.h>	/* For ARPHRD_ETHER */
+#include <linux/autoconf.h>
+#include <linux/proc_fs.h>
+#include <linux/mii.h>
+#include <linux/delay.h>
+#include <asm/system.h>
+#include <asm/uaccess.h>
+#include <asm/io.h>
+#include <asm/cache.h>
+
+#include <asm/rmi/debug.h>
+#include <asm/rmi/pci.h>
+#include <asm/rmi/pic.h>
+#include <asm/rmi/phoenix_mac.h>
+#include <asm/rmi/mips-exts.h>
+#include <asm/rmi/msgring.h>
+#include <asm/rmi/sim.h>
+#include <asm/rmi/rmios_user_mac.h>
+#include <asm/rmi/phnx_user_mac.h>
+#include <asm/rmi/atx_cpld.h>
+#include <asm/rmi/xgmac_mdio.h>
+#include <asm/rmi/proc.h>
+#include <asm-mips/smp.h>
+#include <asm-mips/rmi/iomap.h>
+#include <asm-mips/rmi/gpio.h>
+#include <asm/rmi/user/phnx_user_mac.h>
+
+#define DRV_NAME	"rmi_phnx_mac"
+#define DRV_VERSION	"0.1"
+/* #define DEBUG */
+
+#ifdef DEBUG
+#undef dbg_msg
+int mac_debug = 1;
+#define dbg_msg(fmt, args...) \
+        do {\
+            if (mac_debug) {\
+                printk("[%s@%d|%s]: cpu_%d: " fmt, \
+                __FILE__, __LINE__, __func__,  smp_processor_id(), ##args);\
+            }\
+        } while(0);
+
+#define DUMP_PACKETS
+#else
+#undef dbg_msg
+#define dbg_msg(fmt, args...)
+int mac_debug = 0;
+#endif
+
+
+#ifdef CONFIG_PHOENIX_GMAC_NAPI
+
+/* XLR_NAPI global data strucutre */
+DEFINE_PER_CPU(struct net_device, xlr_per_cpu_net_device);  
+
+/* XLR NAPI per CPU packet counter */
+DEFINE_PER_CPU(unsigned long long, xlr_napi_rx_count); 
+
+int xlr_napi = 0; 
+int xlr_napi_ready = 0; 
+spinlock_t xlr_napi_msgrng_lock;
+
+/* We need this little hack to improve handler speed */
+static int *rxstn_to_txstn_ptr;
+#endif /* CONFIG_PHOENIX_GMAC_NAPI */
+
+
+#define MAC_B2B_IPG             88
+
+/* frame sizes need to be cacheline aligned */
+#define MAX_FRAME_SIZE          1600
+
+#define MAC_SKB_BACK_PTR_SIZE   SMP_CACHE_BYTES
+#define MAC_PREPAD              0
+#define BYTE_OFFSET             2
+#define PHNX_RX_BUF_SIZE (MAX_FRAME_SIZE + BYTE_OFFSET + MAC_PREPAD 	\
+							+ MAC_SKB_BACK_PTR_SIZE+SMP_CACHE_BYTES)
+#define MAC_CRC_LEN             4
+#define MAX_NUM_MSGRNG_STN_CC   128
+
+#define MAX_NUM_DESC		512
+
+/* 
+ * NOTE:
+ * Don't change this threshold to > 15 if MAX_NUM_DESC is 512. 
+ * When msgring_thread_mask is 0xf, each cpu could receive 16 packets 
+ * and replenishment may never happen.
+ * THRESHOLD should be less than 
+ * max_num_desc / (number of threads processing msgring * number of cores)
+*/
+#define MAC_FRIN_TO_BE_SENT_THRESHOLD max_frin_threshold
+
+/* Computed as described above */
+static int max_frin_threshold;
+
+/* 
+ * Total Nr of Free Descriptors to GMACs > 2816 for Usermac 
+ * If configuring max_num_desc use at least 2816/4.
+*/
+static int max_num_desc      = 0;
+static int max_frin_spill    = 0;
+static int max_frout_spill   = 0;
+static int max_class_0_spill = 0;
+static int max_class_1_spill = 0;
+static int max_class_2_spill = 0;
+static int max_class_3_spill = 0;
+
+#define PHNX_NUM_REG_DUMP 9 /* Register 0xa0 to 0xa8 */
+#define PHNX_ETHTOOL_REG_LEN (PHNX_NUM_REG_DUMP * 4) 
+extern void phnx_user_mac_int_handler(int bucket, int size, int code, int stid,
+				      struct msgrng_msg *msg,
+				      void *data /* ignored */ );
+extern int xlr_loader_support;
+extern int xlr_loader_sharedcore;
+extern int xlr_loader_own_gmac;
+
+int xlr_mac_optimized_tx = 0;
+static int xls_gmac0_sgmii = 0;
+static int boot_param_max_num_desc = 0;
+
+#ifdef CONFIG_PHOENIX_IP_FLOW_AFFINITY
+/* skb transfer statistics */
+unsigned long long skb_transfer_stat[NR_CPUS][NR_CPUS];
+
+void skb_transfer_finish(void);
+static void skb_transfer(int bucket, struct sk_buff *skb);
+static void serdes_regs_init(void);
+
+
+/* skb transfer queues, one per CPU */
+static struct sk_buff_head cpu_skb_tqueue[NR_CPUS];
+
+static void
+cpu_tx_queue_init(void)
+{
+  int i;
+
+  for (i = 0; i < NR_CPUS; i++)
+  {
+    skb_queue_head_init(&(cpu_skb_tqueue[i]));
+  }
+}
+
+#endif /* CONFIG_PHOENIX_IP_FLOW_AFFINITY */
+
+
+/* This message ring interrupt type, can be adjusted by NAPI setup callback */
+extern int msgring_int_type;
+
+/* This is cached version of is_xls function */
+static int rmi_board_type = 0;
+
+/* 
+ * Macro to access cached board type. This macro is opposed to
+ * to function is_xls(), which always performs register reads
+ * and case analysis on the fly.
+*/
+#define IS_XLS() (rmi_board_type)
+
+
+#ifdef CONFIG_PHOENIX_GMAC_NAPI
+static int mac_frin_replenish_one_msg(struct net_device *dev);
+#ifdef CONFIG_PHOENIX_HW_BUFFER_MGMT
+static int setup_auto_free(struct sk_buff *skb, int type, struct msgrng_msg *msg);
+#endif /* CONFIG_PHOENIX_HW_BUFFER_MGMT */
+#endif /* CONFIG_PHOENIX_GMAC_NAPI */
+
+static int own_gmac_only = 0;
+
+extern uint32_t dev_tree_en;
+
+/*****************************************************************
+ * Phoenix Generic Mac driver
+ *****************************************************************/
+
+typedef enum { phnx_mac_speed_10, phnx_mac_speed_100,
+	       phnx_mac_speed_1000, phnx_mac_speed_rsvd
+} phnx_mac_speed_t;
+
+typedef enum { phnx_mac_duplex_auto, phnx_mac_duplex_half,
+	       phnx_mac_duplex_full
+} phnx_mac_duplex_t;
+
+typedef enum { phnx_mac_fc_auto, phnx_mac_fc_disabled, phnx_mac_fc_frame,
+	       phnx_mac_fc_collision, phnx_mac_fc_carrier
+} phnx_mac_fc_t;
+
+#define MAC_FRIN_WORK_NUM 32
+static struct work_struct mac_frin_replenish_work[MAC_FRIN_WORK_NUM];
+
+struct cpu_stat {
+	unsigned long tx_packets;
+	unsigned long txc_packets;
+	unsigned long rx_packets;
+	unsigned long interrupts;
+};
+
+struct driver_data {
+
+	/* 
+	 * Let these be the first fields in this structure 
+	 * the structure is cacheline aligned when allocated in 
+	 * init_etherdev
+	*/
+	struct fr_desc *frin_spill;
+	struct fr_desc *frout_spill;
+	union rx_tx_desc *class_0_spill;
+	union rx_tx_desc *class_1_spill;
+	union rx_tx_desc *class_2_spill;
+	union rx_tx_desc *class_3_spill;
+	int spill_configured;
+
+	struct net_device *dev;			/* pointer to linux device */
+	struct timer_list link_timer;	/* for monitoring MII */
+	struct net_device_stats stats;
+	spinlock_t lock;
+
+	phoenix_reg_t *mmio;
+
+	__u8 hwaddr[6];
+	int phy_oldbmsr;
+	int phy_oldanlpar;
+	int phy_oldk1stsr;
+	int phy_oldlinkstat;
+	unsigned char phys_addr[2];
+
+	phnx_mac_speed_t speed;		/* current speed */
+	phnx_mac_duplex_t duplex;	/* current duplex */
+	phnx_mac_fc_t flow_ctrl;	/* current flow control setting */
+	int				advertising;
+
+	int id;
+	int type;
+	int instance;
+	int phy_addr;
+	atomic_t frin_to_be_sent[MAC_FRIN_WORK_NUM];
+	int init_frin_desc;
+
+	struct cpu_stat cpu_stats[32];
+};
+
+enum {
+	PORT_TX,
+	PORT_TX_COMPLETE,
+	PORT_STARTQ,
+	PORT_STOPQ,
+	PORT_START_DEV_STATE,
+	PORT_STOP_DEV_STATE,
+};
+
+#define port_inc_counter(port, counter) 		\
+	atomic_inc(&port_counters[port][(counter)])
+#define port_set_counter(port, counter, value)	\
+	atomic_set(&port_counters[port][(counter)], (value))
+
+static atomic_t port_counters[8][8] __cacheline_aligned;
+static spinlock_t pending_tx_lock[8] __cacheline_aligned;
+static int pending_tx[8] __cacheline_aligned;
+
+static __inline__ unsigned int ldadd_wu(unsigned int value, unsigned long *addr)
+{
+	__asm__ __volatile__(".set push\n"
+			     ".set noreorder\n"
+			     ".set mips64\n"
+			     "move $8, %2\n" "move $9, %3\n"
+#ifdef CONFIG_64BIT
+			     /* "ldadd $8, $9\n" */
+			     ".dword 0x71280012\n"
+#else
+			     /* "ldaddwu $8, $9\n" */
+			     ".word 0x71280011\n"
+#endif
+			     "move %0, $8\n"
+			     ".set pop\n":"=&r"(value), "+m"(*addr)
+			     :"0"(value), "r"((unsigned long)addr)
+			     :"$8", "$9");
+	return value;
+}
+
+#define mac_stats_add(x, val) ldadd_wu(val, &x)
+
+struct phnx_mac {
+	int instance;
+	int type;
+	int irq;
+	unsigned long phnx_io_offset;
+};
+
+/* The _order_ of the device definitions below should be preserved */
+static struct phnx_mac phnx_mac_devices[] = {
+	{.instance = 0,.type = TYPE_GMAC,
+	 .phnx_io_offset = PHOENIX_IO_GMAC_0_OFFSET,.irq = PIC_GMAC_0_IRQ},
+	{.instance = 1,.type = TYPE_GMAC,
+	 .phnx_io_offset = PHOENIX_IO_GMAC_1_OFFSET,.irq = PIC_GMAC_1_IRQ},
+	{.instance = 2,.type = TYPE_GMAC,
+	 .phnx_io_offset = PHOENIX_IO_GMAC_2_OFFSET,.irq = PIC_GMAC_2_IRQ},
+	{.instance = 3,.type = TYPE_GMAC,
+	 .phnx_io_offset = PHOENIX_IO_GMAC_3_OFFSET,.irq = PIC_GMAC_3_IRQ},
+#ifdef XLS
+	{.instance = 4,.type = TYPE_GMAC,
+	 .phnx_io_offset = PHOENIX_IO_GMAC_4_OFFSET,.irq = PIC_GMAC_4_IRQ},
+	{.instance = 5,.type = TYPE_GMAC,
+	 .phnx_io_offset = PHOENIX_IO_GMAC_5_OFFSET,.irq = PIC_GMAC_5_IRQ},
+	{.instance = 6,.type = TYPE_GMAC,
+	 .phnx_io_offset = PHOENIX_IO_GMAC_6_OFFSET,.irq = PIC_GMAC_6_IRQ},
+	{.instance = 7,.type = TYPE_GMAC,
+	 .phnx_io_offset = PHOENIX_IO_GMAC_7_OFFSET,.irq = PIC_GMAC_7_IRQ},
+#endif
+	{.instance = 0,.type = TYPE_XGMAC,
+	 .phnx_io_offset = PHOENIX_IO_XGMAC_0_OFFSET,.irq = PIC_XGS_0_IRQ},
+	{.instance = 1,.type = TYPE_XGMAC,
+	 .phnx_io_offset = PHOENIX_IO_XGMAC_1_OFFSET,.irq = PIC_XGS_1_IRQ}
+};
+
+#define PHOENIX_MAX_MACS	(int)(sizeof(phnx_mac_devices)/sizeof(struct phnx_mac))
+
+static struct net_device *dev_mac[PHOENIX_MAX_MACS];
+static int dev_mac_xgs0;
+static int dev_mac_gmac0;
+
+static int gmac_spill_configured;
+static int gmac1_spill_configured;
+
+static __inline__ int xls_rgmii_mode(struct driver_data *priv)
+{
+	if (is_xls() && !xls_gmac0_sgmii
+	    && priv->instance == 0 && priv->type == TYPE_GMAC)
+		return 1;
+	else
+		return 0;
+}
+
+static __inline__ int gmac_id_to_phy_addr(struct driver_data *priv)
+{
+	int id = priv->id;
+
+	if (is_xls()) {
+		if (xls_rgmii_mode(priv)) return 0;
+		return id + 16;
+	}
+	else {
+		if (xlr_board_atx_ii() && !xlr_board_atx_ii_b())
+			return id - 2;
+		else
+			return id;
+	}
+}
+
+static __inline__ int mac_active(int id)
+{
+	int instance = phnx_mac_devices[id].instance;
+	int type = phnx_mac_devices[id].type;
+    uint32_t *gpio_base = (uint32_t *)(DEFAULT_PHOENIX_IO_BASE +
+                                    PHOENIX_IO_GPIO_OFFSET);    
+	/* On XLS xgmac is not available */
+	if (is_xls() && type == TYPE_XGMAC)
+		return 0;
+
+        /* On XLS gmac0 to gmac7 are available */
+	if (is_xls() && type == TYPE_GMAC) {
+		int processor_id = ((read_c0_prid() & 0xff00) >> 8);
+
+		/* all XLS parts have gmac0, gmac1 */
+		if (instance < 2) return 1;
+
+		if (processor_id <= CHIP_PROCESSOR_ID_XLS_204) {
+			/* all XLS parts with processor_id < 204,
+			   have gmac3 and gmac4 */
+			if (instance < 4) return 1;
+		}
+		if ((processor_id >= CHIP_PROCESSOR_ID_XLS_608) &&
+		    (processor_id < CHIP_PROCESSOR_ID_XLS_208)) {
+			if (instance < 6) return 1;
+
+            if(((gpio_base[PHOENIX_GPIO_FUSE_BANK_REG] & (1<<28)) == 0)  &&
+                ((gpio_base[PHOENIX_GPIO_FUSE_BANK_REG] & (1<<29)) ==  0)){
+                /* 
+				 * Below bits are set when ports are disabled.
+                 * 28 - GMAC7
+                 * 29 - GMAC6
+                 * 30 - GMAC5
+                 * 31 - GMAC4
+                */
+                /*We found an XLS-408 with 8 gmacs*/
+                if (instance < 8) return 1;
+            }
+		}
+		if (processor_id == CHIP_PROCESSOR_ID_XLS_608) {
+			if (instance == 6 || instance == 7) return 1;
+		}
+		/* should never come here */
+		return 0;
+	}
+
+	/* On XLR gmac4 to gmac7 are unavailable */
+	if (!is_xls() && type == TYPE_GMAC && instance >= 4)
+		return 0;
+
+	/* On ATX-I ,ATX-III, ATX-IV, ATX-IV-B and ATX-V xgmac is not available */
+	if ( (xlr_board_atx_i() 
+	      || xlr_board_atx_iii() 
+	      || xlr_board_atx_iv()
+	      || xlr_board_atx_iv_b() 
+	      || xlr_board_atx_v()) && (type == TYPE_XGMAC))
+	{
+		return 0;
+	}
+
+	/* On ATX-II, gmac 0 and gmac 1 are not available */
+	if (xlr_board_atx_ii() && !xlr_board_atx_ii_b() && (type == TYPE_GMAC) && (instance < 2)) 
+		return 0;
+
+	/* On ATX-IV-B and ATX-V, gmac 3 is not available */
+	if ( (xlr_board_atx_v() || xlr_board_atx_iv_b()) && (type == TYPE_GMAC) && (instance > 2))
+	{
+		return 0;
+	}
+
+	return 1;	
+}
+
+static void rmi_phnx_mac_set_enable(struct driver_data *priv, int flag);
+static void rmi_phnx_xgmac_init(struct driver_data *priv);
+static void rmi_phnx_gmac_init(struct driver_data *priv);
+static void phnx_mac_set_rx_mode(struct net_device *dev);
+void rmi_phnx_mac_msgring_handler(int bucket, int size, int code,
+				  int stid, struct msgrng_msg *msg,
+				  void *data);
+static irqreturn_t rmi_phnx_mac_int_handler(int irq, void *dev_id);
+static int rmi_phnx_mac_open(struct net_device *dev);
+static int rmi_phnx_mac_xmit(struct sk_buff *skb, struct net_device *dev);
+static int rmi_phnx_mac_close(struct net_device *dev);
+static void rmi_phnx_mac_timer(unsigned long data);
+static struct net_device_stats *rmi_phnx_mac_get_stats(struct net_device *dev);
+static void rmi_phnx_mac_set_multicast_list(struct net_device *dev);
+static int rmi_phnx_mac_do_ioctl(struct net_device *dev,
+				 struct ifreq *rq, int cmd);
+static void rmi_phnx_mac_tx_timeout(struct net_device *dev);
+static int rmi_phnx_mac_change_mtu(struct net_device *dev, int new_mtu);
+static int rmi_phnx_mac_fill_rxfr(struct net_device *dev);
+static void rmi_phnx_config_spill_area(struct driver_data *priv);
+static void setup_mac_spill_sizes(int desc);
+
+
+/*****************************************************************
+ * Driver Helper Functions
+ *****************************************************************/
+
+static __inline__ int is_user_mac_xgmac(struct driver_data *priv)
+{
+	if(xlr_hybrid_user_mac_xgmac() && (priv->type == TYPE_XGMAC)) 
+		return 1;
+	return 0;
+}
+
+static __inline__ void compute_mac_spill_size(struct driver_data *priv)
+{
+	if(!boot_param_max_num_desc){
+		if (xlr_hybrid_user_mac() || is_user_mac_xgmac(priv))
+			setup_mac_spill_sizes(2048);
+		else
+			setup_mac_spill_sizes(MAX_NUM_DESC);
+	}else{
+		setup_mac_spill_sizes(boot_param_max_num_desc);
+	}
+}
+
+static __inline__ struct sk_buff *mac_get_skb_back_ptr(unsigned long addr)
+{
+	unsigned long *back_ptr =
+		(unsigned long *)(addr - MAC_SKB_BACK_PTR_SIZE);
+	dbg_msg("addr = %lx,  skb = %lx\n", addr, *back_ptr);
+	/* 
+	 * this function should be used only for newly allocated packets. It assumes
+	 * the first cacheline is for the back pointer related book keeping info
+	*/
+	return (struct sk_buff *)(*back_ptr);
+}
+
+static __inline__ void mac_put_skb_back_ptr(struct sk_buff *skb)
+{
+	unsigned long *back_ptr = (unsigned long *)skb->data;
+
+	/* 
+	 * this function should be used only for newly allocated packets. It assumes
+	 * the first cacheline is for the back pointer related book keeping info
+	*/
+	skb_reserve(skb, MAC_SKB_BACK_PTR_SIZE);
+	*back_ptr = (unsigned long)skb;
+	dbg_msg("p=%p, skb=%p\n", back_ptr, skb);
+}
+
+#define CACHELINE_ALIGNED_ADDR(addr) 		\
+	(((unsigned long)(addr)) & ~(SMP_CACHE_BYTES-1))
+
+static __inline__ void *cacheline_aligned_kmalloc(int size, int gfp_mask)
+{
+	void *buf = kmalloc(size + SMP_CACHE_BYTES, gfp_mask);
+	if (buf)
+		buf =
+			(void
+			 *)(CACHELINE_ALIGNED_ADDR((unsigned long)buf +
+						   SMP_CACHE_BYTES));
+	return buf;
+}
+
+static __inline__ struct sk_buff *rmi_phnx_alloc_skb(void)
+{
+	int offset = 0;
+	struct sk_buff *skb = __dev_alloc_skb(PHNX_RX_BUF_SIZE, GFP_KERNEL);
+
+	if (!skb) {
+		return NULL;
+	}
+
+	/* align the data to the next cache line */
+	offset = ((unsigned long)skb->data + SMP_CACHE_BYTES) &
+		~(SMP_CACHE_BYTES - 1);
+	skb_reserve(skb, (offset - (unsigned long)skb->data));
+
+	return skb;
+}
+
+static void rmi_phnx_mac_set_enable(struct driver_data *priv, int flag)
+{
+	uint32_t regval;
+	int tx_threshold = 1518;
+
+	if (flag) {
+		regval = phoenix_read_reg(priv->mmio, R_TX_CONTROL);
+		regval |= (1 << O_TX_CONTROL__TxEnable) |
+			(tx_threshold << O_TX_CONTROL__TxThreshold);
+
+		phoenix_write_reg(priv->mmio, R_TX_CONTROL, regval);
+
+		regval = phoenix_read_reg(priv->mmio, R_RX_CONTROL);
+		regval |= 1 << O_RX_CONTROL__RxEnable;
+		if (xls_rgmii_mode(priv))
+			regval |= 1 << O_RX_CONTROL__RGMII;
+		phoenix_write_reg(priv->mmio, R_RX_CONTROL, regval);
+	} else {
+		regval = phoenix_read_reg(priv->mmio, R_TX_CONTROL);
+		regval &= ~((1 << O_TX_CONTROL__TxEnable) |
+			    (tx_threshold << O_TX_CONTROL__TxThreshold));
+
+		phoenix_write_reg(priv->mmio, R_TX_CONTROL, regval);
+
+		regval = phoenix_read_reg(priv->mmio, R_RX_CONTROL);
+		regval &= ~(1 << O_RX_CONTROL__RxEnable);
+		phoenix_write_reg(priv->mmio, R_RX_CONTROL, regval);
+	}
+}
+
+static __inline__ int phnx_mac_send_fr(struct driver_data *priv,
+				       unsigned long addr, int len)
+{
+	int stid = 0;
+	struct msgrng_msg msg;
+
+	stid = mac_make_desc_b0_rfr(&msg, priv->instance, priv->type,
+				     addr);
+
+	/* Send the packet to MAC */
+	dbg_msg("mac_%d: Sending free packet to stid %d\n",
+		priv->instance, stid);
+	__sync();
+	if (priv->type == TYPE_XGMAC) {
+		while (message_send(1, MSGRNG_CODE_XGMAC, stid, &msg)) ;
+	} else {
+		while (message_send(1, MSGRNG_CODE_MAC, stid, &msg)) ;
+	}
+
+	/* phnx_inc_counter(NETIF_REG_FRIN); */
+
+	/* Let the mac keep the free descriptor */
+	return 0;
+}
+
+static void xgmac_mdio_setup(volatile unsigned int *_mmio)
+{
+	int i;
+	uint32_t rd_data;
+	for (i = 0; i < 4; i++) {
+		rd_data = xmdio_read(_mmio, 1, 0x8000 + i);
+		rd_data = rd_data & 0xffffdfff;			/* clear isolate bit */
+		xmdio_write(_mmio, 1, 0x8000 + i, rd_data);
+	}
+}
+
+/**********************************************************************
+ *  Init MII interface
+ *  
+ *  Input parameters: 
+ *  	   s - priv structure
+ ********************************************************************* */
+#define PHY_STATUS_RETRIES 25000
+
+static void rmi_phnx_mac_mii_init(struct driver_data *priv)
+{
+    phoenix_reg_t *mmio;
+
+#ifdef XLS
+    if (xls_rgmii_mode(priv) && !xlr_board_atx_vii())
+	    mmio = phoenix_io_mmio(PHOENIX_IO_GMAC_4_OFFSET);
+    else
+#endif
+	    mmio = phoenix_io_mmio(PHOENIX_IO_GMAC_0_OFFSET);
+
+    /* use the lowest clock divisor - divisor 28 */
+    phoenix_write_reg(mmio, R_MII_MGMT_CONFIG, 0x07);
+}
+
+/**********************************************************************
+ *  Read a PHY register.
+ *  
+ *  Input parameters: 
+ *  	   s - priv structure
+ *  	   phyaddr - PHY's address
+ *  	   regidx = index of register to read
+ *  	   
+ *  Return value:
+ *  	   value read (16 bits), or 0xffffffff if an error occurred.
+ ********************************************************************* */
+static unsigned int rmi_phnx_mac_mii_read(struct driver_data *priv, int regidx, int rgmii)
+{
+	int i = 0;
+	phoenix_reg_t *mmio;
+	int phyaddr;
+
+	if (rgmii) {
+		if(xls_rgmii_mode(priv)){
+			/*Ack sgmii interrupt, rgmii interrupt is already acked*/
+			phyaddr = 16;
+		}
+		else{
+			phyaddr = 0;
+		}
+#ifdef XLS
+		if (!xlr_board_atx_vii() && xls_rgmii_mode(priv))
+			mmio = phoenix_io_mmio(PHOENIX_IO_GMAC_4_OFFSET);
+		else 
+#endif
+			mmio = phoenix_io_mmio(PHOENIX_IO_GMAC_0_OFFSET);
+	}
+	else {
+		phyaddr= priv->phy_addr;
+#ifdef XLS
+		if (xls_rgmii_mode(priv) && !xlr_board_atx_vii())
+			mmio = phoenix_io_mmio(PHOENIX_IO_GMAC_4_OFFSET);
+		else
+#endif
+			mmio = phoenix_io_mmio(PHOENIX_IO_GMAC_0_OFFSET);
+	}
+
+	/* setup the phy reg to be used */
+	phoenix_write_reg(mmio, R_MII_MGMT_ADDRESS,
+			  (phyaddr << 8) | (regidx << 0));
+
+	/* Issue the read command */
+	phoenix_write_reg(mmio, R_MII_MGMT_COMMAND,
+			  (1 << O_MII_MGMT_COMMAND__rstat));
+
+	/* poll for the read cycle to complete */
+	for (i = 0; i < PHY_STATUS_RETRIES; i++) {
+		if (phoenix_read_reg(mmio, R_MII_MGMT_INDICATORS) == 0)
+			break;
+	}
+
+	/* clear the read cycle */
+	phoenix_write_reg(mmio, R_MII_MGMT_COMMAND, 0);
+
+	if (i == PHY_STATUS_RETRIES) {
+		return 0xffffffff;
+	}
+
+	/* Read the data back */
+	return phoenix_read_reg(mmio, R_MII_MGMT_STATUS);
+}
+
+/**********************************************************************
+ *  Write a value to a PHY register.
+ *  
+ *  Input parameters: 
+ *  	   s - priv structure
+ *  	   phyaddr - PHY to use
+ *  	   regidx - register within the PHY
+ *  	   regval - data to write to register
+ *  	   
+ *  Return value:
+ *  	   nothing
+ ********************************************************************* */
+static void rmi_phnx_mac_mii_write(struct driver_data *priv, int regidx, 
+		unsigned int regval, int internal)
+{
+	int i = 0;
+	phoenix_reg_t *mmio;
+	int phyaddr = 0;
+
+	if (internal) {
+		phyaddr = (priv->instance & 0x3) + 27;
+#ifdef XLS
+		if ((priv->instance & 0x4) && !xlr_board_atx_vii())
+			mmio = phoenix_io_mmio(PHOENIX_IO_GMAC_4_OFFSET);
+		else
+#endif
+			mmio = phoenix_io_mmio(PHOENIX_IO_GMAC_0_OFFSET);
+	}
+	else {
+		phyaddr = priv->phy_addr;
+#ifdef XLS
+		if (xls_rgmii_mode(priv) && !xlr_board_atx_vii())
+			mmio = phoenix_io_mmio(PHOENIX_IO_GMAC_4_OFFSET);
+		else
+#endif
+			mmio = phoenix_io_mmio(PHOENIX_IO_GMAC_0_OFFSET);
+	}
+
+	phoenix_write_reg(mmio, R_MII_MGMT_ADDRESS,
+			  (phyaddr << 8) | (regidx << 0));
+
+	/* Write the data which starts the write cycle */
+	phoenix_write_reg(mmio, R_MII_MGMT_WRITE_DATA, regval);
+
+	/* poll for the write cycle to complete */
+	for (i = 0; i < PHY_STATUS_RETRIES; i++) {
+		if (phoenix_read_reg(mmio, R_MII_MGMT_INDICATORS) == 0)
+			break;
+	}
+
+	return;
+}
+
+int arizona_gmac_phy_write (unsigned long gmac_offset, int num, 
+                            int reg, uint32_t data)
+{
+    phoenix_reg_t *mmio = phoenix_io_mmio(gmac_offset);
+    int i=0;
+
+    /* setup the phy reg to be read*/
+    phoenix_write_reg(mmio, R_MII_MGMT_ADDRESS, (num<<8)|reg);
+
+    /* write the data which starts the write cycle*/
+    phoenix_write_reg(mmio, R_MII_MGMT_WRITE_DATA, data);
+
+    /* poll for the write cycle to complete*/
+    for (i = 0;i<PHY_STATUS_RETRIES;i++) {
+        if ((phoenix_read_reg(mmio, R_MII_MGMT_INDICATORS) & 0x1)==0) 
+            break;
+    }
+
+    if (i == PHY_STATUS_RETRIES) {
+        return 0;
+    }
+
+    return 1;
+}
+
+
+
+/*****************************************************************
+ * Initialize GMAC
+ *****************************************************************/
+static void rmi_phnx_config_pde(struct driver_data *priv)
+{
+	int i = 0, cpu = 0, bucket = 0;
+	__u64 bucket_map = 0;
+
+	for (i = 0; i < 32; i++) {
+		if (cpu_isset(i, cpu_online_map)) {
+			cpu = cpu_logical_map(i);
+			if(xlr_hybrid_user_mac() || is_user_mac_xgmac(priv)){
+				bucket = 4 + (((cpu >> 2) << 3) | (cpu & 0x03));
+			}
+			else{
+				bucket = ((cpu >> 2) << 3) | (cpu & 0x03);
+			}
+			bucket_map |= (1ULL << bucket);
+			dbg_msg("i=%d, cpu=%d, bucket = %d, bucket_map=%llx\n",
+				i, cpu, bucket, bucket_map);
+		}
+	}
+	phoenix_write_reg(priv->mmio, R_PDE_CLASS_0, (bucket_map & 0xffffffff));
+	phoenix_write_reg(priv->mmio, R_PDE_CLASS_0 + 1,
+			  ((bucket_map >> 32) & 0xffffffff));
+
+	phoenix_write_reg(priv->mmio, R_PDE_CLASS_1, (bucket_map & 0xffffffff));
+	phoenix_write_reg(priv->mmio, R_PDE_CLASS_1 + 1,
+			  ((bucket_map >> 32) & 0xffffffff));
+
+	phoenix_write_reg(priv->mmio, R_PDE_CLASS_2, (bucket_map & 0xffffffff));
+	phoenix_write_reg(priv->mmio, R_PDE_CLASS_2 + 1,
+			  ((bucket_map >> 32) & 0xffffffff));
+
+	phoenix_write_reg(priv->mmio, R_PDE_CLASS_3, (bucket_map & 0xffffffff));
+	phoenix_write_reg(priv->mmio, R_PDE_CLASS_3 + 1,
+			  ((bucket_map >> 32) & 0xffffffff));
+}
+
+static void rmi_phnx_config_parser(struct driver_data *priv)
+{
+	/* 
+	 * Mark it as no classification 
+	 * The parser extract is gauranteed to be zero with no classfication
+	*/
+	phoenix_write_reg(priv->mmio, R_L2TYPE_0, 0x00);
+	
+	if (!xlr_hybrid_user_mac() && !is_user_mac_xgmac(priv)) return;
+	
+	phoenix_write_reg(priv->mmio, R_L2TYPE_0, 0x01);
+	
+	/* configure the parser : L2 Type is configured in the bootloader */
+	/* extract IP: src, dest protocol */
+	phoenix_write_reg(priv->mmio, R_L3CTABLE,
+			  (9 << 20) | (1 << 19) | (1 << 18) | (0x01 << 16) |
+			  (0x0800 << 0));
+	phoenix_write_reg(priv->mmio, R_L3CTABLE + 1,
+			  (12 << 25) | (4 << 21) | (16 << 14) | (4 << 10));
+
+	if (xlr_user_mac_l4_extract()) {
+		/* extract TCP: src port, dest port */
+		phoenix_write_reg(priv->mmio, R_L4CTABLE, (6 << 0));
+		phoenix_write_reg(priv->mmio, R_L4CTABLE + 1,
+				  (0 << 21) | (2 << 17) | (2 << 11) | (2 << 7));
+		/* extract UDP: src port, dest port */
+		phoenix_write_reg(priv->mmio, R_L4CTABLE + 2, (17 << 0));
+		phoenix_write_reg(priv->mmio, R_L4CTABLE + 3,
+				  (0 << 21) | (2 << 17) | (2 << 11) | (2 << 7));
+	}
+}
+
+static void rmi_phnx_config_classifier(struct driver_data *priv)
+{
+	int i = 0;
+
+	if (priv->type == TYPE_XGMAC) {
+		/* xgmac translation table doesn't have sane values on reset */
+		for(i=0;i<64;i++)
+			phoenix_write_reg(priv->mmio, R_TRANSLATETABLE + i, 0x0);		
+
+		/*
+		 * use upper 7 bits of the parser extract to index the translate
+		 * table
+		*/
+		phoenix_write_reg(priv->mmio, R_PARSERCONFIGREG, 0x0);
+	}
+}
+
+static void rmi_phnx_gmac_clr_pending_intr(struct driver_data *phy_priv)
+{
+	phoenix_reg_t *mmio = NULL;
+
+	if(!xlr_board_atx_vii())
+		return;
+
+	if(phy_priv->instance == 0){
+        /*All MDIO interrupts goes to mdio 0 - ack mac 0*/
+		mmio = phy_priv->mmio;
+		phoenix_write_reg(mmio, R_INTREG, 0xffffffff);
+	}
+}
+
+static void rmi_phnx_gmac_config_speed(struct driver_data *priv)
+{
+	phoenix_reg_t *mmio = priv->mmio;
+	int id = priv->instance;
+
+	priv->speed = rmi_phnx_mac_mii_read(priv, 28, 0);
+	priv->speed = (priv->speed >> 3) & 0x03;
+
+	if (priv->speed == phnx_mac_speed_10) {
+		if (is_xls())
+			phoenix_write_reg(mmio, R_INTERFACE_CONTROL, SGMII_SPEED_10);
+		phoenix_write_reg(mmio, R_MAC_CONFIG_2, 0x7117);
+		phoenix_write_reg(mmio, R_CORECONTROL, 0x02);
+		printk("configuring gmac_%d in 10Mbps mode\n", id);
+	} else if (priv->speed == phnx_mac_speed_100) {
+		if (is_xls())
+			phoenix_write_reg(mmio, R_INTERFACE_CONTROL, SGMII_SPEED_100);
+		phoenix_write_reg(mmio, R_MAC_CONFIG_2, 0x7117);
+		phoenix_write_reg(mmio, R_CORECONTROL, 0x01);
+		printk("configuring gmac_%d in 100Mbps mode\n", id);
+	} else {
+		if (is_xls())
+			phoenix_write_reg(mmio, R_INTERFACE_CONTROL, SGMII_SPEED_1000);
+		if (priv->speed != phnx_mac_speed_1000) {
+			printk("phy reported unknown mac speed, defaulting to 100Mbps mode\n");
+			phoenix_write_reg(mmio, R_MAC_CONFIG_2, 0x7117);
+			phoenix_write_reg(mmio, R_CORECONTROL, 0x01);
+		}
+		phoenix_write_reg(mmio, R_MAC_CONFIG_2, 0x7217);
+		phoenix_write_reg(mmio, R_CORECONTROL, 0x00);
+		printk("configuring gmac_%d in 1000Mbps mode\n", id);
+	}
+}
+
+/*****************************************************************
+ * Initialize XGMAC
+ *****************************************************************/
+static void rmi_phnx_xgmac_init(struct driver_data *priv)
+{
+	int i = 0;
+	phoenix_reg_t *mmio = priv->mmio;
+	int id = priv->instance;
+	volatile unsigned short *cpld;
+	uint32_t rx_control;
+
+	cpld = (volatile unsigned short *)(unsigned long)0xffffffffBD840000ULL;
+	phoenix_write_reg(priv->mmio, R_DESC_PACK_CTRL,
+			  (MAX_FRAME_SIZE << O_DESC_PACK_CTRL__RegularSize) | (4 << 20));
+	phoenix_write_reg(priv->mmio, R_BYTEOFFSET0, BYTE_OFFSET);
+	rmi_phnx_config_pde(priv);
+	rmi_phnx_config_parser(priv);
+	rmi_phnx_config_classifier(priv);
+
+	phoenix_write_reg(priv->mmio, R_MSG_TX_THRESHOLD, 1);
+
+	/* configure the XGMAC Registers */
+	phoenix_write_reg(mmio, R_XGMAC_CONFIG_1, 0x50000026);
+
+	/* configure the XGMAC_GLUE Registers */
+	phoenix_write_reg(mmio, R_DMACR0, 0xffffffff);
+	phoenix_write_reg(mmio, R_DMACR1, 0xffffffff);
+	phoenix_write_reg(mmio, R_DMACR2, 0xffffffff);
+	phoenix_write_reg(mmio, R_DMACR3, 0xffffffff);
+	phoenix_write_reg(mmio, R_STATCTRL, 0x04);
+	phoenix_write_reg(mmio, R_L2ALLOCCTRL, 0xffffffff);
+
+	phoenix_write_reg(mmio, R_XGMACPADCALIBRATION, 0x030);
+	phoenix_write_reg(mmio, R_EGRESSFIFOCARVINGSLOTS, 0x0f);
+	phoenix_write_reg(mmio, R_L2ALLOCCTRL, 0xffffffff);
+	phoenix_write_reg(mmio, R_XGMAC_MIIM_CONFIG, 0x3e);
+
+	/* 
+	 * Take XGMII phy out of reset 
+	 * we are pulling everything out of reset because writing a 0 would
+	 * reset other devices on the chip
+	 */
+	cpld[ATX_CPLD_RESET_1] = 0xffff;
+	cpld[ATX_CPLD_MISC_CTRL] = 0xffff;
+	cpld[ATX_CPLD_RESET_2] = 0xffff;
+
+	xgmac_mdio_setup(mmio);
+
+	compute_mac_spill_size(priv);	
+
+	rmi_phnx_config_spill_area(priv);
+	rmi_phnx_mac_set_enable(priv, 1);
+
+	if (id == 0) {
+		for (i = 0; i < 16; i++) {
+			phoenix_write_reg(mmio, R_XGS_TX0_BUCKET_SIZE + i,
+					  bucket_sizes.
+					  bucket[MSGRNG_STNID_XGS0_TX + i]);
+		}
+
+		phoenix_write_reg(mmio, R_XGS_JFR_BUCKET_SIZE,
+				  bucket_sizes.bucket[MSGRNG_STNID_XMAC0JFR]);
+		phoenix_write_reg(mmio, R_XGS_RFR_BUCKET_SIZE,
+				  bucket_sizes.bucket[MSGRNG_STNID_XMAC0RFR]);
+
+		for (i = 0; i < MAX_NUM_MSGRNG_STN_CC; i++) {
+			phoenix_write_reg(mmio, R_CC_CPU0_0 + i,
+					  cc_table_xgs_0.
+					  counters[i >> 3][i & 0x07]);
+		}
+	} else if (id == 1) {
+		for (i = 0; i < 16; i++) {
+			phoenix_write_reg(mmio, R_XGS_TX0_BUCKET_SIZE + i,
+					  bucket_sizes.
+					  bucket[MSGRNG_STNID_XGS1_TX + i]);
+		}
+
+		phoenix_write_reg(mmio, R_XGS_JFR_BUCKET_SIZE,
+				  bucket_sizes.bucket[MSGRNG_STNID_XMAC1JFR]);
+		phoenix_write_reg(mmio, R_XGS_RFR_BUCKET_SIZE,
+				  bucket_sizes.bucket[MSGRNG_STNID_XMAC1RFR]);
+
+		for (i = 0; i < MAX_NUM_MSGRNG_STN_CC; i++) {
+			phoenix_write_reg(mmio, R_CC_CPU0_0 + i,
+					  cc_table_xgs_1.
+					  counters[i >> 3][i & 0x07]);
+		}
+	}
+
+	priv->init_frin_desc = 1;
+
+    /* Clear the flagging of rx length check errors */
+    rx_control = phoenix_read_reg(mmio, R_RX_CONTROL);
+    rx_control &= ~(1 << 9);
+    phoenix_write_reg(mmio, R_RX_CONTROL, rx_control);
+}
+
+static void serdes_regs_init( void ) 
+{
+    int i;
+    volatile unsigned int *mmio_gpio;
+    mmio_gpio = (unsigned int *)(phoenix_io_base + PHOENIX_IO_GPIO_OFFSET);
+    /*
+       P Reg   Val
+       -------------
+       26 0     6DB0
+       26 1     0FFF
+       26 2     B6D0
+       26 3     00FF
+       26 4     0000
+       26 5     0000
+       26 6     0005
+       26 7     0001
+       26 8     0000
+       26 9     0000
+       26 10    0000
+     */
+    arizona_gmac_phy_write (PHOENIX_IO_GMAC_0_OFFSET, 26, 0, 0x6DB0);
+    arizona_gmac_phy_write (PHOENIX_IO_GMAC_0_OFFSET, 26, 1, 0xFFFF);  
+    arizona_gmac_phy_write (PHOENIX_IO_GMAC_0_OFFSET, 26, 2, 0xB6D0);
+    arizona_gmac_phy_write (PHOENIX_IO_GMAC_0_OFFSET, 26, 3, 0x00FF);
+    arizona_gmac_phy_write (PHOENIX_IO_GMAC_0_OFFSET, 26, 4, 0x0000);
+    arizona_gmac_phy_write (PHOENIX_IO_GMAC_0_OFFSET, 26, 5, 0x0000);
+    arizona_gmac_phy_write (PHOENIX_IO_GMAC_0_OFFSET, 26, 6, 0x0005);
+    arizona_gmac_phy_write (PHOENIX_IO_GMAC_0_OFFSET, 26, 7, 0x0001);
+    arizona_gmac_phy_write (PHOENIX_IO_GMAC_0_OFFSET, 26, 8, 0x0000);
+    arizona_gmac_phy_write (PHOENIX_IO_GMAC_0_OFFSET, 26, 9, 0x0000);
+    arizona_gmac_phy_write (PHOENIX_IO_GMAC_0_OFFSET, 26,10, 0x0000);
+
+#ifdef XLS
+    if ((is_xls6xx()) || (is_xls4xx())) {
+        arizona_gmac_phy_write (PHOENIX_IO_GMAC_4_OFFSET, 26, 0, 0x6DB0);
+        arizona_gmac_phy_write (PHOENIX_IO_GMAC_4_OFFSET, 26, 1, 0xFFFF); 
+        arizona_gmac_phy_write (PHOENIX_IO_GMAC_4_OFFSET, 26, 2, 0xB6D0);
+        arizona_gmac_phy_write (PHOENIX_IO_GMAC_4_OFFSET, 26, 3, 0x00FF);
+        arizona_gmac_phy_write (PHOENIX_IO_GMAC_4_OFFSET, 26, 4, 0x0000);
+        arizona_gmac_phy_write (PHOENIX_IO_GMAC_4_OFFSET, 26, 5, 0x0000);
+        arizona_gmac_phy_write (PHOENIX_IO_GMAC_4_OFFSET, 26, 6, 0x0005);
+        arizona_gmac_phy_write (PHOENIX_IO_GMAC_4_OFFSET, 26, 7, 0x0001);
+        arizona_gmac_phy_write (PHOENIX_IO_GMAC_4_OFFSET, 26, 8, 0x0000);
+        arizona_gmac_phy_write (PHOENIX_IO_GMAC_4_OFFSET, 26, 9, 0x0000);
+        arizona_gmac_phy_write (PHOENIX_IO_GMAC_4_OFFSET, 26,10, 0x0000);
+    }
+#endif
+
+    for(i=0;i<10000000;i++) {}
+
+    /* program  GPIO values for serdes init parameters */
+    mmio_gpio[0x20] = 0x7e6802;
+    mmio_gpio[0x10] = 0x7104;
+
+    for(i=0;i<100000000;i++) {}
+}
+
+static void serdes_autoconfig(void)
+{
+    int delay = 10;
+
+    /* Enable Auto negotiation in the PCS Layer*/
+    arizona_gmac_phy_write (PHOENIX_IO_GMAC_0_OFFSET, 27, 0, 0x1000);
+    mdelay(delay);
+    arizona_gmac_phy_write (PHOENIX_IO_GMAC_0_OFFSET, 27, 0, 0x0200);
+    mdelay(delay);
+
+    arizona_gmac_phy_write (PHOENIX_IO_GMAC_0_OFFSET, 28, 0, 0x1000);
+    mdelay(delay);
+    arizona_gmac_phy_write (PHOENIX_IO_GMAC_0_OFFSET, 28, 0, 0x0200);
+    mdelay(delay);
+
+    arizona_gmac_phy_write (PHOENIX_IO_GMAC_0_OFFSET, 29, 0, 0x1000);
+    mdelay(delay);
+    arizona_gmac_phy_write (PHOENIX_IO_GMAC_0_OFFSET, 29, 0, 0x0200);
+    mdelay(delay);
+
+    arizona_gmac_phy_write (PHOENIX_IO_GMAC_0_OFFSET, 30, 0, 0x1000);
+    mdelay(delay);
+    arizona_gmac_phy_write (PHOENIX_IO_GMAC_0_OFFSET, 30, 0, 0x0200);
+    mdelay(delay);
+
+#ifdef XLS
+    if((is_xls6xx()) || (is_xls4xx()))
+    {
+        /* Enable Auto negotiation in the PCS Layer*/
+
+        arizona_gmac_phy_write (PHOENIX_IO_GMAC_4_OFFSET, 27, 0, 0x1000);
+        mdelay(delay);
+        arizona_gmac_phy_write (PHOENIX_IO_GMAC_4_OFFSET, 27, 0, 0x0200);
+        mdelay(delay);
+
+        arizona_gmac_phy_write (PHOENIX_IO_GMAC_4_OFFSET, 28, 0, 0x1000);
+        mdelay(delay);
+        arizona_gmac_phy_write (PHOENIX_IO_GMAC_4_OFFSET, 28, 0, 0x0200);
+        mdelay(delay);
+
+        arizona_gmac_phy_write (PHOENIX_IO_GMAC_4_OFFSET, 29, 0, 0x1000);
+        mdelay(delay);
+        arizona_gmac_phy_write (PHOENIX_IO_GMAC_4_OFFSET, 29, 0, 0x0200);
+        mdelay(delay);
+
+        arizona_gmac_phy_write (PHOENIX_IO_GMAC_4_OFFSET, 30, 0, 0x1000);
+        mdelay(delay);
+        arizona_gmac_phy_write (PHOENIX_IO_GMAC_4_OFFSET, 30, 0, 0x0200);
+        mdelay(delay);
+
+    }
+#endif
+
+    return;
+
+}
+
+
+/*******************************************************
+ * Initialization gmac
+ *******************************************************/
+static void rmi_phnx_gmac_init_hybrid(struct driver_data *priv)
+{
+	int id = priv->instance;
+
+	printk("Initializing gmac %d in hybrid mode:\n", id);
+
+	rmi_phnx_mac_mii_init(priv);
+	rmi_phnx_gmac_config_speed(priv);
+}
+
+static void rmi_phnx_gmac_init(struct driver_data *priv)
+{
+	int i = 0;
+	phoenix_reg_t *mmio = priv->mmio;
+	int id = priv->instance;
+	__u32 value = 0;
+
+	compute_mac_spill_size(priv);
+
+	rmi_phnx_config_spill_area(priv);
+
+	phoenix_write_reg(priv->mmio, R_DESC_PACK_CTRL,
+			  (BYTE_OFFSET << O_DESC_PACK_CTRL__ByteOffset) |
+			  (1 << O_DESC_PACK_CTRL__MaxEntry) |
+			  (MAX_FRAME_SIZE << O_DESC_PACK_CTRL__RegularSize));
+
+	rmi_phnx_config_pde(priv);
+	rmi_phnx_config_parser(priv);
+	rmi_phnx_config_classifier(priv);
+
+	phoenix_write_reg(priv->mmio, R_MSG_TX_THRESHOLD, 3);
+
+	phoenix_write_reg(mmio, R_MAC_CONFIG_1, 0x35);
+
+	if (!xlr_hybrid_user_mac())
+		phoenix_write_reg(mmio,R_RX_CONTROL,(0x7<<6));
+
+	if (xls_rgmii_mode(priv)) {
+		value = phoenix_read_reg(priv->mmio, R_RX_CONTROL);
+		value |= 1 << O_RX_CONTROL__RGMII;
+		phoenix_write_reg(priv->mmio, R_RX_CONTROL, value);
+	}
+
+	rmi_phnx_mac_mii_init(priv);
+
+	priv->advertising = ADVERTISED_10baseT_Full | ADVERTISED_10baseT_Half | 
+			ADVERTISED_100baseT_Full | ADVERTISED_100baseT_Half |
+			ADVERTISED_1000baseT_Full | ADVERTISED_Autoneg |
+			ADVERTISED_MII;
+    
+	/*Clear pending mdio interrupt*/
+	rmi_phnx_gmac_clr_pending_intr(priv);
+    
+
+	/* 
+	 * Enable all MDIO interrupts in the phy 
+	 * RX_ER bit seems to be get set about every 1 sec in GigE mode,
+	 * ignore it for now...
+	*/
+	rmi_phnx_mac_mii_write(priv, 25, 0xfffffffe, 0);
+    
+	if (is_xls()){
+		rmi_phnx_mac_mii_write(priv, 0, 0x1000, 1);
+		mdelay(100);
+		rmi_phnx_mac_mii_write(priv, 0, 0x0200, 1);
+		mdelay(100);
+    }
+
+	if((is_xls())) {
+        serdes_regs_init();
+        mdelay(10);
+        serdes_autoconfig();
+    }
+	rmi_phnx_gmac_config_speed(priv);
+
+	value = phoenix_read_reg(mmio, R_IPG_IFG);
+	phoenix_write_reg(mmio, R_IPG_IFG, ((value & ~0x7f) | MAC_B2B_IPG));
+	phoenix_write_reg(mmio, R_DMACR0, 0xffffffff);
+	phoenix_write_reg(mmio, R_DMACR1, 0xffffffff);
+	phoenix_write_reg(mmio, R_DMACR2, 0xffffffff);
+	phoenix_write_reg(mmio, R_DMACR3, 0xffffffff);
+	phoenix_write_reg(mmio, R_STATCTRL, 0x04);
+	phoenix_write_reg(mmio, R_L2ALLOCCTRL, 0xffffffff);
+	phoenix_write_reg(mmio, R_INTMASK, 0);
+	phoenix_write_reg(mmio, R_FREEQCARVE, 0);
+
+	if (is_xls()) {
+		if (id < 4) {
+			phoenix_write_reg(mmio, R_GMAC_TX0_BUCKET_SIZE + id,
+                          xls_bucket_sizes.bucket[MSGRNG_STNID_GMAC0_TX0 + id]);
+        		phoenix_write_reg(mmio, R_GMAC_RFR0_BUCKET_SIZE,
+                          xls_bucket_sizes.bucket[MSGRNG_STNID_GMAC0_FR]);
+		} else {
+        		phoenix_write_reg(mmio, R_GMAC_TX0_BUCKET_SIZE,
+                          xls_bucket_sizes.bucket[MSGRNG_STNID_GMAC1_TX0 + id - 4]);
+        		phoenix_write_reg(mmio, R_GMAC_RFR0_BUCKET_SIZE,
+                          xls_bucket_sizes.bucket[MSGRNG_STNID_GMAC1_FR]);
+		}
+		for (i = 0; i < MAX_NUM_MSGRNG_STN_CC; i++) {
+			if (id < 4)
+				phoenix_write_reg(mmio, R_CC_CPU0_0 + i,
+					xls_cc_table_gmac0.counters[i >> 3][i & 0x07]);
+			else
+				phoenix_write_reg(mmio, R_CC_CPU0_0 + i,
+					xls_cc_table_gmac1.counters[i >> 3][i & 0x07]);
+		}
+		priv->init_frin_desc = 1;
+		return;
+	} else {
+		if(own_gmac_only) {
+			phoenix_write_reg(mmio, R_GMAC_TX0_BUCKET_SIZE + id,
+					shared_bucket_sizes.bucket[MSGRNG_STNID_GMACTX0 + id]);
+			phoenix_write_reg(mmio, R_GMAC_JFR0_BUCKET_SIZE,
+					shared_bucket_sizes.bucket[MSGRNG_STNID_GMACJFR_0]);
+			phoenix_write_reg(mmio, R_GMAC_RFR0_BUCKET_SIZE,
+					shared_bucket_sizes.bucket[MSGRNG_STNID_GMACRFR_0]);
+			phoenix_write_reg(mmio, R_GMAC_JFR1_BUCKET_SIZE,
+					shared_bucket_sizes.bucket[MSGRNG_STNID_GMACJFR_1]);
+			phoenix_write_reg(mmio, R_GMAC_RFR1_BUCKET_SIZE,
+					shared_bucket_sizes.bucket[MSGRNG_STNID_GMACRFR_1]);
+
+			for (i = 0; i < MAX_NUM_MSGRNG_STN_CC; i++) {
+				phoenix_write_reg(mmio, R_CC_CPU0_0 + i,
+						shared_cc_table_gmac.counters[i >> 3][i & 0x07]);
+			}
+		} else {
+			phoenix_write_reg(mmio, R_GMAC_TX0_BUCKET_SIZE + id,
+					bucket_sizes.bucket[MSGRNG_STNID_GMACTX0 + id]);
+			phoenix_write_reg(mmio, R_GMAC_JFR0_BUCKET_SIZE,
+					bucket_sizes.bucket[MSGRNG_STNID_GMACJFR_0]);
+			phoenix_write_reg(mmio, R_GMAC_RFR0_BUCKET_SIZE,
+					bucket_sizes.bucket[MSGRNG_STNID_GMACRFR_0]);
+			phoenix_write_reg(mmio, R_GMAC_JFR1_BUCKET_SIZE,
+					bucket_sizes.bucket[MSGRNG_STNID_GMACJFR_1]);
+			phoenix_write_reg(mmio, R_GMAC_RFR1_BUCKET_SIZE,
+					bucket_sizes.bucket[MSGRNG_STNID_GMACRFR_1]);
+
+			for (i = 0; i < MAX_NUM_MSGRNG_STN_CC; i++) {
+				phoenix_write_reg(mmio, R_CC_CPU0_0 + i,
+						cc_table_gmac.counters[i >> 3][i & 0x07]);
+			}
+		}
+		priv->init_frin_desc = 1;
+	}
+
+}
+
+/**********************************************************************
+ * Set promiscuous mode
+ **********************************************************************/
+static void phnx_mac_set_rx_mode(struct net_device *dev)
+{
+	struct driver_data *priv = netdev_priv(dev);
+	uint32_t regval;
+
+	regval = phoenix_read_reg(priv->mmio, R_MAC_FILTER_CONFIG);
+
+	if (dev->flags & IFF_PROMISC) {
+		regval |= (1 << O_MAC_FILTER_CONFIG__BROADCAST_EN) |
+			(1 << O_MAC_FILTER_CONFIG__PAUSE_FRAME_EN) |
+			(1 << O_MAC_FILTER_CONFIG__ALL_MCAST_EN) |
+			(1 << O_MAC_FILTER_CONFIG__ALL_UCAST_EN);
+	} else {
+		regval &= ~((1 << O_MAC_FILTER_CONFIG__PAUSE_FRAME_EN) |
+			    (1 << O_MAC_FILTER_CONFIG__ALL_UCAST_EN));
+#ifdef PA10401_1_GMAC_PKT_DISCARD
+	  if (!is_xls()){
+                regval |= (1 << O_MAC_FILTER_CONFIG__ALL_MCAST_EN) |
+		          (1 << O_MAC_FILTER_CONFIG__ALL_UCAST_EN);
+          } 
+#endif
+	}
+
+	phoenix_write_reg(priv->mmio, R_MAC_FILTER_CONFIG, regval);
+}
+
+/**********************************************************************
+ *  Configure LAN speed for the specified MAC.
+ ********************************************************************* */
+static int rmi_phnx_mac_set_speed(struct driver_data *s, phnx_mac_speed_t speed)
+{
+	return 0;
+}
+
+/**********************************************************************
+ *  Set Ethernet duplex and flow control options for this MAC
+ ********************************************************************* */
+static int rmi_phnx_mac_set_duplex(struct driver_data *s,
+				   phnx_mac_duplex_t duplex, phnx_mac_fc_t fc)
+{
+	return 0;
+}
+
+/*****************************************************************
+ * Kernel Net Stack <-> MAC Driver Interface
+ *****************************************************************/
+#define MAC_TX_FAIL 1
+#define MAC_TX_PASS 0
+#define MAC_TX_RETRY 1
+
+static inline int phnx_netif_queue_tx(struct net_device *dev, int result)
+{
+	unsigned int flags;
+	int port = ((struct driver_data *)netdev_priv(dev))->instance;
+	int ret = 0;
+
+	spin_lock_irqsave(&pending_tx_lock[port], flags);
+	if (result == MAC_TX_PASS) {
+		pending_tx[port]++;
+		ret = MAC_TX_PASS;
+	} else {
+		if (pending_tx[port] > 16) {
+			netif_stop_queue(dev);
+			ret = MAC_TX_FAIL;
+		} else {
+			ret = MAC_TX_RETRY;
+		}
+	}
+	spin_unlock_irqrestore(&pending_tx_lock[port], flags);
+	return ret;
+}
+
+static inline void phnx_netif_queue_tx_complete(struct net_device *dev)
+{
+	int port = ((struct driver_data *)netdev_priv(dev))->instance;
+
+	spin_lock(&pending_tx_lock[port]);
+	pending_tx[port]--;
+	netif_wake_queue(dev);
+	spin_unlock(&pending_tx_lock[port]);
+}
+
+static inline int mac_make_desc_b0_tx(struct msgrng_msg *msg, int id, int type,
+				      unsigned long addr, int len, struct sk_buff *skb)
+{
+	int tx_stid = 0;
+	int fr_stid = 0;
+	int cpu = phoenix_cpu_id();
+
+	if (type == TYPE_XGMAC) {
+		tx_stid = msgrng_xgmac_stid_tx(id);
+		fr_stid = (cpu << 3) + phoenix_thr_id() + 4;
+	} 
+	else {
+
+		tx_stid = msgrng_gmac_stid_tx(id);
+
+		/* 
+		 * In case of loader support and linux owning gmacs on
+		 * gmacs on shared core, use same bucket for fr_stid.
+		*/
+		if (own_gmac_only || xlr_hybrid_user_mac_xgmac())
+			fr_stid = (cpu << 3) + phoenix_thr_id();
+		else
+			fr_stid = (cpu << 3) + phoenix_thr_id() + 4;
+	}
+
+	msg->msg0 = ( ((uint64_t)1 << 63) | 
+		      ( ((uint64_t)127) << 54) | 
+		      ((uint64_t)len << 40) | 
+		      ((uint64_t)addr & 0xffffffffffULL)
+		);
+	msg->msg1 = ( ((uint64_t)1 << 63) |
+		      ((uint64_t)fr_stid << 54) |
+		      ((uint64_t)0 << 40) |
+#ifdef CONFIG_64BIT
+		      ((uint64_t)virt_to_phys(skb)&0xffffffffffULL)
+#else
+		      ((unsigned long)(skb)&0xffffffffUL)
+#endif
+		);
+		
+	msg->msg2 = msg->msg3 = 0;
+
+	return tx_stid;
+}
+
+static __inline__ void message_send_block(unsigned int size, unsigned int code,
+					  unsigned int stid, struct msgrng_msg *msg)
+{
+  unsigned int dest = 0;
+  unsigned long long status=0;
+
+  msgrng_load_tx_msg0(msg->msg0);
+  msgrng_load_tx_msg1(msg->msg1);
+  msgrng_load_tx_msg2(msg->msg2);
+  msgrng_load_tx_msg3(msg->msg3);
+
+  dest = ((size-1)<<16)|(code<<8)|(stid);
+
+  for(;;) {
+
+	  msgrng_send(dest);
+
+	  status = msgrng_read_status();
+
+	  if (status & 0x6) {
+		  continue;
+	  }
+	  else break;
+  }
+
+}
+
+static int mac_xmit(struct sk_buff *skb, struct net_device *dev,
+		    struct driver_data *priv, int txq)
+{
+	struct msgrng_msg msg;
+	int stid = 0;
+
+	stid = mac_make_desc_b0_tx(&msg, priv->instance, priv->type,
+			virt_to_phys(skb->data), skb->len, skb);
+
+#ifdef CONFIG_PHOENIX_HW_BUFFER_MGMT
+        if (xlr_napi)
+		setup_auto_free(skb, priv->type, &msg);
+#endif /* CONFIG_PHOENIX_HW_BUFFER_MGMT */
+
+	__sync();
+
+	if (message_send_retry(2, MSGRNG_CODE_MAC, stid, &msg))
+		return MAC_TX_FAIL;
+
+	port_inc_counter(priv->instance, PORT_TX);
+
+	/* Send the packet to MAC */
+	dbg_msg("Sent tx packet to stid %d, msg0=%llx, msg1=%llx \n", stid, msg.msg0, msg.msg1);
+#ifdef DUMP_PACKETS
+	{
+		int i = 0;
+		dbg_msg("Tx Packet: length=%d\n", skb->len);
+		for (i = 0; i < 64; i++) {
+			printk("%02x ", skb->data[i]);
+			if (i && (i % 16) == 0)
+				printk("\n");
+		}
+		printk("\n");
+	}
+#endif
+
+	phnx_inc_counter(NETIF_TX);
+
+	dev->trans_start = jiffies;
+
+	return MAC_TX_PASS;
+}
+
+#define MSGRING_PROCESS_FROUT_START_BUCKET 4
+#define MSGRING_PROCESS_FROUT_END_BUCKET 8
+#define MSGRING_PROCESS_FROUT_POP_BUCKET_MASK 0xf0
+extern void msgring_process_rx_msgs(int, int, __u32);
+
+static int rmi_phnx_mac_xmit(struct sk_buff *skb, struct net_device *dev)
+{
+	struct driver_data *priv = netdev_priv(dev);
+	int ret = -ENOSPC;
+	unsigned long mflags = 0;
+	int txq = hard_smp_processor_id();
+
+	dbg_msg("IN\n");
+	phnx_inc_counter(NETIF_STACK_TX);
+
+	if (xlr_mac_optimized_tx) {
+		irq_enter();
+		msgrng_flags_save(mflags);
+		for(;;) {
+			/* first drain all the pending messages */
+#ifdef CONFIG_PHOENIX_GMAC_NAPI
+			if (!xlr_napi) {
+				msgring_process_rx_msgs(MSGRING_PROCESS_FROUT_START_BUCKET,
+							MSGRING_PROCESS_FROUT_END_BUCKET,
+						        MSGRING_PROCESS_FROUT_POP_BUCKET_MASK);
+                        }
+#else
+			msgring_process_rx_msgs(MSGRING_PROCESS_FROUT_START_BUCKET,
+					MSGRING_PROCESS_FROUT_END_BUCKET,
+					MSGRING_PROCESS_FROUT_POP_BUCKET_MASK);
+#endif /* CONFIG_PHOENIX_GMAC_NAPI */
+
+			ret = mac_xmit(skb, dev, priv, txq);
+			if (ret == MAC_TX_PASS) break;
+		}
+		mac_stats_add(priv->cpu_stats[txq].tx_packets, 1);
+
+		msgrng_flags_restore(mflags);
+		irq_exit();
+	}
+	else {
+retry:
+		msgrng_access_enable(mflags);
+
+		ret = mac_xmit(skb, dev, priv, txq);
+
+		mac_stats_add(priv->cpu_stats[txq].tx_packets, 1);
+
+		msgrng_access_disable(mflags);
+
+		ret = phnx_netif_queue_tx(dev, ret);
+		if (ret == MAC_TX_RETRY)
+			goto retry;
+
+		dbg_msg("OUT, ret = %d, mflags=%lx\n", ret, mflags);
+		if (ret == MAC_TX_FAIL) {
+			/* FULL */
+			dbg_msg("Msg Ring Full. Stopping upper layer Q\n");
+			port_inc_counter(priv->instance, PORT_STOPQ);
+		}
+	}
+
+	return ret;
+}
+
+static void mac_frin_replenish(struct work_struct *args /* ignored */ )
+{
+	int cpu = hard_smp_processor_id();
+	int done = 0;
+	int i = 0;
+
+	phnx_inc_counter(REPLENISH_ENTER);
+	phnx_set_counter(REPLENISH_CPU, hard_smp_processor_id());
+
+	for (;;) {
+
+		done = 0;
+
+		for (i = 0; i < PHOENIX_MAX_MACS; i++) {
+			int offset = 0;
+			unsigned long msgrng_flags;
+			struct sk_buff *skb = 0;
+			__u32 cycles;
+			struct net_device *dev;
+			struct driver_data *priv;
+			atomic_t *frin_to_be_sent;
+
+			dev = dev_mac[i];
+			if (!dev)
+				goto skip;
+
+			priv = netdev_priv(dev);
+			frin_to_be_sent = &priv->frin_to_be_sent[cpu];
+
+			if (atomic_read(frin_to_be_sent) < 0) {
+				panic("BUG?: [%s]: gmac_%d illegal value for frin_to_be_sent=%d\n",
+						__func__, i, atomic_read(frin_to_be_sent));
+			}
+
+			if (!atomic_read(frin_to_be_sent))
+				goto skip;
+
+			cycles = read_c0_count();
+			{
+				skb = __dev_alloc_skb(PHNX_RX_BUF_SIZE,
+							GFP_ATOMIC | __GFP_REPEAT |
+							__GFP_NOWARN);
+				if (!skb) {
+					skb = __dev_alloc_skb(PHNX_RX_BUF_SIZE,
+								GFP_KERNEL);
+					if (!skb)
+						panic("[%s]: Unable to allocate skb!\n", __func__);
+				}
+			}
+			phnx_inc_counter(REPLENISH_FRIN);
+
+			/* align the data to the next cache line */
+			offset = (((unsigned long)skb->data + SMP_CACHE_BYTES) &
+					  ~(SMP_CACHE_BYTES - 1));
+			skb_reserve(skb, (offset - (unsigned long)skb->data));
+
+			msgrng_access_enable(msgrng_flags);
+			mac_put_skb_back_ptr(skb);
+
+			if (phnx_mac_send_fr
+			    (priv, virt_to_bus(skb->data), skb->len)) {
+				dev_kfree_skb(skb);
+				printk("[%s]: rx free message_send failed!\n", __func__);
+				break;
+			}
+			msgrng_access_disable(msgrng_flags);
+
+			phnx_set_counter(REPLENISH_CYCLES,
+					 (read_c0_count() - cycles));
+
+			atomic_dec(frin_to_be_sent);
+
+			continue;
+		skip:
+			done++;
+		}
+		if (done == PHOENIX_MAX_MACS)
+			break;
+	}
+}
+
+/*
+ * Send a packet back to ipsec rmios
+ */
+static void ipsec_drop_packet(IPSEC_PACKET * pbuf)
+{
+	int stid=0;
+	u32 addr;
+	struct msgrng_msg msg;
+        if (is_xls()) {
+            if (pbuf->src_id == MSGRNG_STNID_GMAC0)
+		stid = MSGRNG_STNID_GMAC0_FR;
+            else if (pbuf->src_id == MSGRNG_STNID_GMAC1)
+		stid = MSGRNG_STNID_GMAC1_FR;
+            else {
+                printk("[%s]: rx packet (0x%p) for unknown station %d? dropping packet\n",
+                       __func__, pbuf, stid);
+                return;
+	    }
+        }
+        else {
+	    if (pbuf->src_id == MSGRNG_STNID_XGS0FR)
+		stid = MSGRNG_STNID_XMAC0RFR;
+	    else if (pbuf->src_id == MSGRNG_STNID_XGS1FR)
+		stid = MSGRNG_STNID_XMAC1RFR;
+	    else
+		stid = MSGRNG_STNID_GMACRFR_0;
+        }
+	addr = virt_to_phys(pbuf->packet_data + SKBUF_HEAD);
+	msg.msg0 =
+		((u64) CTRL_REG_FREE << 61) | ((u64) stid << 52) | (u64) addr;
+	msg.msg1 = msg.msg2 = msg.msg3 = 0;
+	while (message_send(1, MSGRNG_CODE_MAC, stid, &msg)) ;
+}
+
+/*
+ * Receive a packet from rmios ipsec. This function is called by the message
+ * ring driver when the message source is a CPU and the message code indicates
+ * a packet from rmios. The message ring driver can also receive a fifo message
+ * from a CPU sending an event or response to a user space process.
+ */
+void rmi_phnx_rmios_msgring_handler(int bucket, int size, int code,
+				    int stid, struct msgrng_msg *msg,
+				    void *data /* ignored */ )
+{
+	unsigned long addr;
+	__u32 length;
+	int port;
+	struct sk_buff *skb;
+	struct driver_data *priv;
+	IPSEC_PACKET *ipsec_packet;
+	/*
+	 * Find the ipsec packet
+	 */
+	addr = (unsigned long)bus_to_virt(msg->msg0 & 0xffffffffe0ULL);
+	ipsec_packet = (IPSEC_PACKET *) (addr - SKBUF_HEAD -
+					 offsetof(IPSEC_PACKET, packet_data));
+	/*
+	 * Do nothing during the boot.
+	 */
+	if (system_state != SYSTEM_RUNNING) {
+		ipsec_drop_packet(ipsec_packet);
+		return;
+	}
+	/*
+	 * Allocate an skbuff, initialize it, and copy the data to it.
+	 */
+	length = ((msg->msg0 >> 40) & 0x3fff) - BYTE_OFFSET - MAC_CRC_LEN;
+	skb = __dev_alloc_skb(PHNX_RX_BUF_SIZE, GFP_ATOMIC);
+	if (!skb) {
+		printk("[%s] - no skbuff\n", __func__);
+		ipsec_drop_packet(ipsec_packet);
+		return;
+	}
+	port = code >> 4;
+	skb->dev = dev_mac[dev_mac_gmac0 + port];
+	skb_put(skb, length);
+	memcpy(skb->data, (char *)addr + 2, length);
+	ipsec_drop_packet(ipsec_packet);
+	skb->protocol = eth_type_trans(skb, skb->dev);
+	/*
+	 * Increment the driver stats counters.
+	 */
+	priv = netdev_priv(skb->dev);
+	mac_stats_add(priv->stats.rx_packets, 1);
+	mac_stats_add(priv->stats.rx_bytes, skb->len);
+	/*
+	 * Queue the packet to the upper layer.
+	 */
+	netif_rx(skb);
+}
+
+/* This function is called from an interrupt handler */
+void rmi_phnx_mac_msgring_handler(int bucket, int size, int code,
+				  int stid, struct msgrng_msg *msg,
+				  void *data /* ignored */ )
+{
+	unsigned long addr = 0;
+	__u32 length = 0;
+	int ctrl = 0, port = 0;
+	struct sk_buff *skb = 0;
+	struct driver_data *priv = 0;
+	int cpu = hard_smp_processor_id();
+
+	if (xlr_hybrid_rmios_tcpip_stack()) {
+		return;
+	}
+	dbg_msg("mac: bucket=%d, size=%d, code=%d, stid=%d, msg0=%llx msg1=%llx\n",
+				 bucket, size, code, stid, msg->msg0, msg->msg1);
+
+	addr = (unsigned long)bus_to_virt(msg->msg0 & 0xffffffffe0ULL);
+	length = (msg->msg0 >> 40) & 0x3fff;
+	if (length == 0) {
+#ifdef CONFIG_64BIT
+		unsigned long tmp;
+#endif
+		ctrl = CTRL_REG_FREE;
+		port = (msg->msg0 >> 54) & 0x0f;
+#ifdef CONFIG_64BIT
+		tmp = (unsigned long)(msg->msg0 & 0xffffffffffULL);
+		skb = (struct sk_buff *)phys_to_virt(tmp);
+#else
+		skb = (struct sk_buff *)(unsigned long)msg->msg0;
+#endif
+	} else {
+		ctrl = CTRL_SNGL;
+		length = length - BYTE_OFFSET - MAC_CRC_LEN;
+		port = msg->msg0 & 0x0f;
+	}
+
+	dbg_msg("msg0 = %llx, msg1 = %llx, stid = %d, port = %d, addr=%lx, length=%d, ctrl=%d\n", 
+				msg->msg0, msg->msg1, stid, port, addr, length, ctrl);
+
+	if (ctrl == CTRL_REG_FREE || ctrl == CTRL_JUMBO_FREE) {
+
+		/* Tx Complete */
+		phnx_inc_counter(NETIF_TX_COMPLETE);
+
+		dbg_msg("skb = %p\n", skb);
+		/* release the skb and update statistics outside the spinlock */
+		priv = netdev_priv(skb->dev);
+		mac_stats_add(priv->stats.tx_packets, 1);
+		mac_stats_add(priv->stats.tx_bytes, skb->len);
+		mac_stats_add(priv->cpu_stats[cpu].txc_packets, 1);
+		dev_kfree_skb_irq(skb);
+
+		if (!xlr_mac_optimized_tx) {
+			port_inc_counter(priv->instance, PORT_TX_COMPLETE);
+			phnx_netif_queue_tx_complete(skb->dev);
+		}
+
+		phnx_set_counter(NETIF_TX_COMPLETE_CYCLES,
+				 (read_c0_count() - msgrng_msg_cycles));
+	} else if (ctrl == CTRL_SNGL || ctrl == CTRL_START) {
+		/* Rx Packet */
+
+		struct driver_data *priv = 0;
+                unsigned int rxStatus=0;
+
+		dbg_msg("Received packet, port = %d\n", port);
+
+		skb = mac_get_skb_back_ptr(addr);
+		if (!skb) {
+			printk("[%s]: rx desc (0x%lx) for unknown skb? dropping packet\n",
+				 __func__, addr);
+			return;
+		}
+		
+		if (is_xls()) {
+			if (stid == MSGRNG_STNID_GMAC0)
+				skb->dev = dev_mac[dev_mac_gmac0 + port];
+			else if (stid == MSGRNG_STNID_GMAC1)
+				skb->dev = dev_mac[dev_mac_gmac0 + 4 + port];
+			else {
+				printk("[%s]: rx desc (0x%lx) for unknown station %d? dropping packet\n",
+					__func__, addr, stid);
+				return;
+			}
+		}
+		else {
+			if (stid == MSGRNG_STNID_XGS0FR)
+				skb->dev = dev_mac[dev_mac_xgs0];
+			else if (stid == MSGRNG_STNID_XGS1FR)
+				skb->dev = dev_mac[dev_mac_xgs0+1];
+			else
+				skb->dev = dev_mac[dev_mac_gmac0 + port];
+		}
+		
+		priv = netdev_priv(skb->dev);
+                
+		rxStatus = (msg->msg0 >> 56 ) & 0x7f;
+		if (rxStatus & 0x40)
+		{
+			dbg_msg("Rx err 0x%x\n",rxStatus);
+			mac_stats_add(priv->stats.rx_errors,1);
+
+			if (rxStatus & 0x02)
+				mac_stats_add(priv->stats.rx_crc_errors,1);
+
+			if (rxStatus & 0x01)
+				mac_stats_add(priv->stats.rx_length_errors,1);
+
+			if (atomic_inc_return(&priv->frin_to_be_sent[cpu]) >
+				MAC_FRIN_TO_BE_SENT_THRESHOLD) {
+				schedule_work(&mac_frin_replenish_work[cpu]);
+			}
+
+			dev_kfree_skb_irq(skb);
+
+			return;
+		}
+
+#ifdef PA10401_1_GMAC_PKT_DISCARD
+		if ((!is_xls()) && (!(skb->dev->flags & IFF_PROMISC)))
+		{
+			if (!(rxStatus & 0x20))
+			{
+				if ((*(uint64_t *)(skb->data+MAC_PREPAD + BYTE_OFFSET)>>16) 
+					!= ((*(uint64_t *)skb->dev->dev_addr)>>16))
+				{
+					if (atomic_inc_return(&priv->frin_to_be_sent[cpu]) 
+						> MAC_FRIN_TO_BE_SENT_THRESHOLD) {
+						schedule_work(&mac_frin_replenish_work[cpu]);
+					}
+					dev_kfree_skb_irq(skb);
+					return;
+				}
+			}
+		}
+#endif
+		/* 
+		 * if num frins to be sent exceeds threshold, wake up the helper 
+		 * thread
+		*/
+		if (atomic_inc_return(&priv->frin_to_be_sent[cpu]) >
+		    MAC_FRIN_TO_BE_SENT_THRESHOLD) {
+			schedule_work(&mac_frin_replenish_work[cpu]);
+		}
+#ifdef DUMP_PACKETS
+		{
+			int i = 0;
+			dbg_msg("Rx Packet: length=%d\n", length);
+			for (i = 0; i < 64; i++) {
+				printk("%02x ", skb->data[i]);
+				if (i && (i % 16) == 0)
+					printk("\n");
+			}
+			printk("\n");
+		}
+#endif
+
+		/* compensate for the prepend data, byte offset */
+		skb_reserve(skb, MAC_PREPAD + BYTE_OFFSET);
+
+		skb_put(skb, length);
+		skb->protocol = eth_type_trans(skb, skb->dev);
+                
+		dbg_msg("gmac_%d: rx packet: addr = %lx, length = %x, protocol=%d\n",
+					 priv->instance, addr, length, skb->protocol);
+
+		mac_stats_add(priv->stats.rx_packets, 1);
+		mac_stats_add(priv->stats.rx_bytes, skb->len);
+		mac_stats_add(priv->cpu_stats[cpu].rx_packets, 1);
+
+		phnx_inc_counter(NETIF_RX);
+		phnx_set_counter(NETIF_RX_CYCLES,
+				 (read_c0_count() - msgrng_msg_cycles));
+
+#ifdef CONFIG_PHOENIX_IP_QUEUE_AFFINITY
+		/* 
+		 * We pass bucket number in the last field of skb->cb[] structure 
+		 * it might be later picked up by multiprocess ip_queue
+		*/
+		skb->cb[sizeof(skb->cb) - 1] = bucket;
+#endif /* CONFIG_PHOENIX_IP_QUEUE_AFFINITY */
+
+
+#ifdef CONFIG_PHOENIX_IP_FLOW_AFFINITY
+		skb_transfer(bucket, skb);
+#else
+		netif_rx(skb);
+#endif /* CONFIG_PHOENIX_IP_FLOW_AFFINITY */
+
+	} else {
+		printk("[%s]: unrecognized ctrl=%d!\n", __func__, ctrl);
+	}
+}
+
+#ifdef CONFIG_PHOENIX_IP_FLOW_AFFINITY
+static void skb_transfer(int bucket, struct sk_buff *skb)
+{
+	u_long my_cpu_no, my_thread_no, my_core_no, target_cpu_no, target_thread_no;
+
+	target_thread_no = bucket & 0x3;
+	my_cpu_no = smp_processor_id();
+	my_thread_no = phoenix_thr_id();
+	my_core_no = phoenix_cpu_id();
+	target_cpu_no = cpu_number_map((my_core_no << 2) | target_thread_no);
+
+	/*
+	* Version with NETRX IPI aggregation
+	*/
+	if (target_thread_no != my_thread_no && cpu_isset(target_cpu_no, cpu_online_map))
+	{
+		unsigned long flags;
+		struct sk_buff_head *ptqueue = &cpu_skb_tqueue[target_cpu_no];
+
+		spin_lock_irqsave(&ptqueue->lock, flags);
+		if (ptqueue->qlen)
+		{
+			__skb_queue_tail(ptqueue, skb);
+		}
+		else
+		{
+			__skb_queue_tail(ptqueue, skb);
+			core_send_ipi(target_thread_no, SMP_NETRX_IPI);
+		}
+		spin_unlock_irqrestore(&ptqueue->lock, flags);
+
+		skb_transfer_stat[my_cpu_no][target_cpu_no]++;
+	}
+	else
+	{
+		skb_transfer_stat[my_cpu_no][my_cpu_no]++;
+
+		skb_queue_tail(&cpu_skb_tqueue[my_cpu_no], skb);
+		skb_transfer_finish();
+	}
+}
+
+/* second part of SKB transfer logic, called from IRQ_IPI_NETRX handler */
+void skb_transfer_finish(void)
+{
+  struct sk_buff *skb;
+  u_long cpu = smp_processor_id();
+
+  while ((skb = skb_dequeue(&cpu_skb_tqueue[cpu])) != NULL)
+  {
+    netif_rx(skb);
+  }
+}
+
+#endif /* CONFIG_PHOENIX_IP_FLOW_AFFINITY */
+
+static irqreturn_t rmi_phnx_mac_int_handler(int irq, void *dev_id)
+{
+    struct net_device *dev = (struct net_device *)dev_id;
+    struct driver_data *priv = netdev_priv(dev);
+    phoenix_reg_t *mmio = priv->mmio;
+    __u32 intreg = phoenix_read_reg(mmio, R_INTREG);
+    int cpu = hard_smp_processor_id();
+
+    mac_stats_add(priv->cpu_stats[cpu].interrupts, 1);
+
+    if (intreg & (1 << O_INTREG__MDInt)) {
+        __u32 phy_int_status = 0;
+        int i=0;
+
+		for(i=0; i<PHOENIX_MAX_MACS; i++) {
+            struct net_device *phy_dev = 0;
+            struct driver_data *phy_priv = 0;
+
+            /* skip the first 2 gmacs for atx_ii boards */
+            if (!mac_active(i)) continue;
+
+            phy_dev = dev_mac[i];
+            phy_priv = netdev_priv(phy_dev);
+
+            if (phy_priv->type == TYPE_XGMAC) continue;
+
+            phy_int_status = rmi_phnx_mac_mii_read(phy_priv, 26, 0);
+#ifdef DEBUG
+            printk(KERN_DEBUG"[%s]: Received MDIO interrupt from mac_%d (type=%d), "
+					  "phy_int_status = 0x%08x reconfiguring gmac speed \n",
+					  __func__, phy_priv->instance, phy_priv->type,
+					  phy_int_status);
+#endif
+            if (is_xls() && !phy_priv->instance && !xlr_board_atx_viii()) {
+                phy_int_status = rmi_phnx_mac_mii_read(phy_priv, 26, 1);
+#ifdef DEBUG
+                printk(KERN_DEBUG"[%s]: Received MDIO interrupt from mac_%d (type=%d), "
+						"phy_int_status = 0x%08x reconfiguring gmac speed \n",
+						__func__, phy_priv->instance, phy_priv->type,
+						phy_int_status);
+#endif
+            }
+
+            if((is_xls())) {
+                serdes_autoconfig();
+            }
+
+            rmi_phnx_gmac_config_speed(phy_priv);
+        }
+    } else {
+        printk("[%s]: mac type = %d, instance %d error "
+                "interrupt: INTREG = 0x%08x\n", 
+                __func__, priv->type, priv->instance, intreg);
+    }
+
+    /* clear all interrupts and hope to make progress */
+    phoenix_write_reg(mmio, R_INTREG, 0xffffffff);
+
+    /* on A0 and B0, xgmac interrupts are routed only to xgs_1 irq */
+    if ( (xlr_revision_b0()) && (priv->type == TYPE_XGMAC) ) {
+        struct net_device *xgs0_dev = dev_mac[dev_mac_xgs0];
+        struct driver_data *xgs0_priv = netdev_priv(xgs0_dev);
+        phoenix_reg_t *xgs0_mmio = xgs0_priv->mmio;			
+        __u32 xgs0_intreg = phoenix_read_reg(xgs0_mmio, R_INTREG);
+
+        if (xgs0_intreg) {
+            printk("[%s]: mac type = %d, instance %d error "
+                    "interrupt: INTREG = 0x%08x\n", 
+                    __func__, xgs0_priv->type, xgs0_priv->instance, xgs0_intreg);
+
+            phoenix_write_reg(xgs0_mmio, R_INTREG, 0xffffffff);
+        }
+    }
+
+    return IRQ_NONE;
+}
+
+static int rmi_phnx_mac_open(struct net_device *dev)
+{
+	struct driver_data *priv = netdev_priv(dev);
+	int i = 0;
+
+	dbg_msg("IN\n");
+	spin_lock_irq(&priv->lock);
+
+	if (!xlr_hybrid_rmios_ipsec() && !xlr_hybrid_rmios_tcpip_stack())
+		if (rmi_phnx_mac_fill_rxfr(dev)) {
+			spin_unlock_irq(&priv->lock);
+			return -1;
+		}
+	phnx_mac_set_rx_mode(dev);
+	if (!xlr_hybrid_rmios_ipsec() && !xlr_hybrid_rmios_tcpip_stack())
+	{
+		phoenix_write_reg(priv->mmio, R_INTMASK, 
+				  (1<<O_INTMASK__TxIllegal)       |
+				  (((priv->instance&0x3)==0)<<O_INTMASK__MDInt)		|
+				  (1<<O_INTMASK__TxFetchError)    |
+				  (1<<O_INTMASK__P2PSpillEcc)     |
+				  (1<<O_INTMASK__TagFull)         |
+				  (1<<O_INTMASK__Underrun)        |
+				  (1<<O_INTMASK__Abort)
+			);
+	}
+				  
+	else
+		phoenix_write_reg(priv->mmio, R_INTMASK, 0);
+
+	/* Set the timer to check for link beat. */
+	init_timer(&priv->link_timer);
+	priv->link_timer.expires = jiffies + 2 * HZ / 100;
+	priv->link_timer.data = (unsigned long)dev;
+	priv->link_timer.function = &rmi_phnx_mac_timer;
+	priv->phy_oldlinkstat = -1; /* set link state to undefined */
+	add_timer(&priv->link_timer);
+
+	netif_start_queue(dev);
+
+	/*
+	 * Configure the speed, duplex, and flow control
+	 */
+	rmi_phnx_mac_set_speed(priv, priv->speed);
+	rmi_phnx_mac_set_duplex(priv, priv->duplex, priv->flow_ctrl);
+	rmi_phnx_mac_set_enable(priv, 1);
+
+
+	spin_unlock_irq(&priv->lock);
+
+	for (i = 0; i < 8; i++)
+		atomic_set(&priv->frin_to_be_sent[i], 0);
+
+#ifdef CONFIG_PHOENIX_GMAC_NAPI
+	if (xlr_napi) {
+	  xlr_napi_ready = 1; 
+	}
+#endif /* CONFIG_PHOENIX_GMAC_NAPI */
+
+	return 0;
+}
+
+static int rmi_phnx_mac_close(struct net_device *dev)
+{
+	struct driver_data *priv = netdev_priv(dev);
+	spin_lock_irq(&priv->lock);
+
+	/* 
+	 * There may have left over skbs in the ring as well as in free in 
+	 * they will be reused next time open is called 
+	*/
+	rmi_phnx_mac_set_enable(priv, 0);
+
+	del_timer_sync(&priv->link_timer);
+
+	netif_stop_queue(dev);
+	phnx_inc_counter(NETIF_STOP_Q);
+	port_inc_counter(priv->instance, PORT_STOPQ);
+
+	spin_unlock_irq(&priv->lock);
+
+	return 0;
+}
+
+static void rmi_phnx_mac_timer(unsigned long data)
+{
+	struct net_device *dev = (struct net_device *)data;
+	struct driver_data *priv = netdev_priv(dev);
+	int next_tick = HZ;
+	int mii_status;
+
+	spin_lock_irq(&priv->lock);
+
+	/* read flag "Link established" (0x04) of MII status register (1) */
+	mii_status = rmi_phnx_mac_mii_read(priv, 1, 0) & 0x04;
+
+	if (mii_status != priv->phy_oldlinkstat) {
+		priv->phy_oldlinkstat = mii_status;
+		if (mii_status) {
+			netif_carrier_on(dev);
+		} else {
+			netif_carrier_off(dev);
+		}
+	}
+
+	spin_unlock_irq(&priv->lock);
+	priv->link_timer.expires = jiffies + next_tick;
+	add_timer(&priv->link_timer);
+}
+
+static struct net_device_stats *rmi_phnx_mac_get_stats(struct net_device *dev)
+{
+	struct driver_data *priv = netdev_priv(dev);
+	unsigned long flags;
+
+	spin_lock_irqsave(&priv->lock, flags);
+
+	/* XXX update other stats here */
+	spin_unlock_irqrestore(&priv->lock, flags);
+
+	return &priv->stats;
+}
+
+static void rmi_phnx_mac_set_multicast_list(struct net_device *dev)
+{
+	/* 
+	 * Clear out entire multicast table.  We do this by nuking
+	 * the entire hash table and all the direct matches except
+	 * the first one, which is used for our station address 
+	 */
+
+	/*
+	 * Clear the filter to say we don't want any multicasts.
+	 */
+
+	if (dev->flags & IFF_ALLMULTI) {
+		/* 
+		 * Enable ALL multicasts.  Do this by inverting the 
+		 * multicast enable bit. 
+		 */
+		return;
+	}
+
+	/* 
+	 * Progam new multicast entries.  For now, only use the
+	 * perfect filter.  In the future we'll need to use the
+	 * hash filter if the perfect filter overflows
+	*/
+}
+
+static int
+rmi_phnx_mac_do_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)
+{
+	int rc = 0;
+	switch (cmd) {
+	default:
+		rc = -EOPNOTSUPP;
+		break;
+	}
+
+	return rc;
+}
+
+static void rmi_phnx_mac_tx_timeout(struct net_device *dev)
+{
+	struct driver_data *priv = netdev_priv(dev);
+
+	spin_lock_irq(&priv->lock);
+
+	dev->trans_start = jiffies;
+	mac_stats_add(priv->stats.tx_errors, 1);
+
+	spin_unlock_irq(&priv->lock);
+
+	netif_wake_queue(dev);
+	phnx_inc_counter(NETIF_START_Q);
+	port_inc_counter(priv->instance, PORT_STARTQ);
+
+	printk(KERN_WARNING "%s: Transmit timed out\n", dev->name);
+}
+
+static int rmi_phnx_mac_change_mtu(struct net_device *dev, int new_mtu)
+{
+	struct driver_data *priv = netdev_priv(dev);
+	unsigned long flags;
+
+	if ((new_mtu > 9500) || (new_mtu < 64)) {
+		return -EINVAL;
+	}
+
+	spin_lock_irqsave(&priv->lock, flags);
+
+	dev->mtu = new_mtu;
+
+	if (netif_running(dev)) {
+		/* Disable MAC TX/RX */
+		rmi_phnx_mac_set_enable(priv, 0);
+
+		/* Flush RX FR IN */
+		/* Flush TX IN */
+		rmi_phnx_mac_set_enable(priv, 1);
+	}
+
+	spin_unlock_irqrestore(&priv->lock, flags);
+	return 0;
+}
+
+static int rmi_phnx_mac_fill_rxfr(struct net_device *dev)
+{
+	struct driver_data *priv = netdev_priv(dev);
+	struct sk_buff *skb = 0;
+	unsigned long msgrng_flags;
+	int i;
+	int ret = 0;
+
+	dbg_msg("\n");
+	if (!priv->init_frin_desc) return ret;
+	priv->init_frin_desc = 0;	
+
+	compute_mac_spill_size(priv);
+	
+	dbg_msg("\n");
+	for (i = 0; i < max_num_desc; i++) {
+		skb = rmi_phnx_alloc_skb();
+		if (!skb) {
+			ret = -ENOMEM;
+			break;
+		}
+
+		skb->dev = dev;
+
+#ifdef CONFIG_PHOENIX_HW_BUFFER_MGMT
+                if (xlr_napi) {
+			skb_shinfo(skb)->rmi_flags = 1;
+			skb_shinfo(skb)->rmi_owner = dev;
+			skb_shinfo(skb)->rmi_refill = mac_frin_replenish_one_msg;
+		}
+#endif /* CONFIG_PHOENIX_HW_BUFFER_MGMT */
+
+		/* Send the free Rx desc to the MAC */
+		msgrng_access_enable(msgrng_flags);
+		mac_put_skb_back_ptr(skb);
+		if (phnx_mac_send_fr(priv, virt_to_bus(skb->data), skb->len)) {
+			dev_kfree_skb(skb);
+			printk
+				("message_send failed!, unable to send free desc to mac\n");
+			ret = -EIO;
+			break;
+		}
+		msgrng_access_disable(msgrng_flags);
+	}
+
+	return ret;
+}
+
+static __inline__ void *xgmac_config_spill(phoenix_reg_t * mmio,
+					   int reg_start_0, int reg_start_1,
+					   int reg_size, int size)
+{
+	__u32 spill_size = CACHELINE_ALIGNED_ADDR(size);
+	void *spill = cacheline_aligned_kmalloc(spill_size, GFP_KERNEL);
+	__u64 phys_addr = 0;
+	if (!spill) {
+		panic("Unable to allocate memory for spill area!\n");
+	}
+	phys_addr = virt_to_phys(spill);
+	phoenix_write_reg(mmio, reg_start_0, (phys_addr >> 5) & 0xffffffff);
+	phoenix_write_reg(mmio, reg_start_1, (phys_addr >> 37) & 0x07);
+	phoenix_write_reg(mmio, reg_size, spill_size);
+	return spill;
+}
+
+static __inline__ void *rmi_phnx_config_spill(phoenix_reg_t * mmio,
+					      int reg_start_0, int reg_start_1,
+					      int reg_size, int size)
+{
+	__u32 spill_size = CACHELINE_ALIGNED_ADDR(size);
+	void *spill = cacheline_aligned_kmalloc(spill_size, GFP_KERNEL);
+	__u64 phys_addr = 0;
+
+	if (!spill) {
+		panic("Unable to allocate memory for spill area!\n");
+	}
+	phys_addr = virt_to_phys(spill);
+	phoenix_write_reg(mmio, reg_start_0, (phys_addr >> 5) & 0xffffffff);
+	phoenix_write_reg(mmio, reg_start_1, (phys_addr >> 37) & 0x07);
+	phoenix_write_reg(mmio, reg_size, spill_size);
+
+	return spill;
+}
+
+static void rmi_phnx_config_spill_area(struct driver_data *priv)
+{
+	/* 
+	 * if driver initialization is done parallely on multiple cpus
+	 * spill_configured needs synchronization 
+	*/
+	if(priv->spill_configured)
+		return;
+
+	if (!priv->spill_configured) priv->spill_configured = 1;
+
+	priv->frin_spill =
+		rmi_phnx_config_spill(priv->mmio,
+				      R_REG_FRIN_SPILL_MEM_START_0,
+				      R_REG_FRIN_SPILL_MEM_START_1,
+				      R_REG_FRIN_SPILL_MEM_SIZE,
+				      max_frin_spill *
+				      sizeof(struct fr_desc));
+
+	priv->class_0_spill =
+		rmi_phnx_config_spill(priv->mmio,
+				      R_CLASS0_SPILL_MEM_START_0,
+				      R_CLASS0_SPILL_MEM_START_1,
+				      R_CLASS0_SPILL_MEM_SIZE,
+				      max_class_0_spill *
+				      sizeof(union rx_tx_desc));
+	priv->class_1_spill =
+		rmi_phnx_config_spill(priv->mmio,
+				      R_CLASS1_SPILL_MEM_START_0,
+				      R_CLASS1_SPILL_MEM_START_1,
+				      R_CLASS1_SPILL_MEM_SIZE,
+				      max_class_1_spill *
+				      sizeof(union rx_tx_desc));
+
+	priv->frout_spill =
+		rmi_phnx_config_spill(priv->mmio, R_FROUT_SPILL_MEM_START_0,
+				      R_FROUT_SPILL_MEM_START_1,
+				      R_FROUT_SPILL_MEM_SIZE,
+				      max_frout_spill *
+				      sizeof(struct fr_desc));
+
+	priv->class_2_spill =
+		rmi_phnx_config_spill(priv->mmio,
+				      R_CLASS2_SPILL_MEM_START_0,
+				      R_CLASS2_SPILL_MEM_START_1,
+				      R_CLASS2_SPILL_MEM_SIZE,
+				      max_class_2_spill *
+				      sizeof(union rx_tx_desc));
+	priv->class_3_spill =
+		rmi_phnx_config_spill(priv->mmio,
+				      R_CLASS3_SPILL_MEM_START_0,
+				      R_CLASS3_SPILL_MEM_START_1,
+				      R_CLASS3_SPILL_MEM_SIZE,
+				      max_class_3_spill *
+				      sizeof(union rx_tx_desc));
+	priv->spill_configured = 1;
+}
+
+/*****************************************************************
+ * Write the MAC address to the PHNX registers
+ * All 4 addresses are the same for now
+ *****************************************************************/
+static void phnx_mac_setup_hwaddr(struct driver_data *priv)
+{
+	struct net_device *dev = priv->dev;
+
+	phoenix_write_reg(priv->mmio, R_MAC_ADDR0,
+			  ((dev->dev_addr[5] << 24) | (dev->dev_addr[4] << 16)
+			   | (dev->dev_addr[3] << 8) | (dev->dev_addr[2]))
+		);
+
+	phoenix_write_reg(priv->mmio, R_MAC_ADDR0 + 1,
+			  ((dev->dev_addr[1] << 24) | (dev->
+						       dev_addr[0] << 16)));
+
+	phoenix_write_reg(priv->mmio, R_MAC_ADDR_MASK2, 0xffffffff);
+
+	phoenix_write_reg(priv->mmio, R_MAC_ADDR_MASK2 + 1, 0xffffffff);
+
+	phoenix_write_reg(priv->mmio, R_MAC_ADDR_MASK3, 0xffffffff);
+
+	phoenix_write_reg(priv->mmio, R_MAC_ADDR_MASK3 + 1, 0xffffffff);
+
+	if (xlr_hybrid_user_mac() || is_user_mac_xgmac(priv)) {
+		phoenix_write_reg(priv->mmio, R_MAC_FILTER_CONFIG,
+				  (1 << O_MAC_FILTER_CONFIG__BROADCAST_EN) |
+				  (1 << O_MAC_FILTER_CONFIG__ALL_MCAST_EN) |
+				  (1 << O_MAC_FILTER_CONFIG__ALL_UCAST_EN) |
+				  (1 << O_MAC_FILTER_CONFIG__MAC_ADDR0_VALID)
+			);
+	} else {
+		phoenix_write_reg(priv->mmio, R_MAC_FILTER_CONFIG,
+				  (1 << O_MAC_FILTER_CONFIG__BROADCAST_EN) |
+				  (1 << O_MAC_FILTER_CONFIG__ALL_MCAST_EN) |
+				  (1 << O_MAC_FILTER_CONFIG__MAC_ADDR0_VALID)
+			);
+	}
+}
+
+/*****************************************************************
+ * Read the MAC address from the PHNX registers
+ * All 4 addresses are the same for now
+ *****************************************************************/
+static void phnx_mac_get_hwaddr(struct net_device *dev)
+{
+	struct driver_data *priv = netdev_priv(dev);
+
+	dev->dev_addr[0] = phoenix_base_mac_addr[0];
+	dev->dev_addr[1] = phoenix_base_mac_addr[1];
+	dev->dev_addr[2] = phoenix_base_mac_addr[2];
+	dev->dev_addr[3] = phoenix_base_mac_addr[3];
+	dev->dev_addr[4] = phoenix_base_mac_addr[4];
+	dev->dev_addr[5] = phoenix_base_mac_addr[5] + priv->id;
+}
+
+/**********************************************************************
+ * Set a new Ethernet address for the interface.
+ **********************************************************************/
+static int rmi_phnx_set_mac_address(struct net_device *dev, void *addr) {
+    struct driver_data *priv = netdev_priv(dev);
+    struct sockaddr *p_sockaddr = (struct sockaddr *) addr;
+
+    memcpy(dev->dev_addr, p_sockaddr->sa_data, 6);
+    phnx_mac_setup_hwaddr(priv);
+    return 0;
+}
+
+/*****************************************************************
+ * Mac Module Initialization
+ *****************************************************************/
+static void mac_common_init(void)
+{
+	int i = 0;
+
+	if (!xlr_hybrid_rmios_ipsec() && !xlr_hybrid_rmios_tcpip_stack())
+		for (i = 0; i < MAC_FRIN_WORK_NUM; i++)
+			INIT_WORK(&mac_frin_replenish_work[i],
+				  mac_frin_replenish);
+
+	if (xlr_hybrid_user_mac()) return;
+
+	if (is_xls()) {
+		if (register_msgring_handler
+		(TX_STN_GMAC0, rmi_phnx_mac_msgring_handler, NULL)) {
+			panic("Couldn't register msgring handler for TX_STN_GMAC0\n");
+		}
+		if (register_msgring_handler
+		(TX_STN_GMAC1, rmi_phnx_mac_msgring_handler, NULL)) {
+			panic("Couldn't register msgring handler for TX_STN_GMAC1\n");
+		}
+
+#ifdef CONFIG_PHOENIX_GMAC_NAPI
+		/* Cached pointer to station ID translation table, needed for NAPI */ 
+		rxstn_to_txstn_ptr = &xls_rxstn_to_txstn_map[0];
+#endif /* CONFIG_PHOENIX_GMAC_NAPI */
+
+		return;
+	}
+	else {
+		/* Register a bucket handler with the phoenix messaging subsystem */
+		if (register_msgring_handler
+	    	(TX_STN_GMAC, rmi_phnx_mac_msgring_handler, NULL)) {
+			panic("Couldn't register msgring handler\n");
+		}
+
+#ifdef CONFIG_PHOENIX_GMAC_NAPI
+		rxstn_to_txstn_ptr = &rxstn_to_txstn_map[0];
+#endif /* CONFIG_PHOENIX_GMAC_NAPI */
+
+	}
+
+	if (xlr_board_atx_ii() && !xlr_hybrid_user_mac_xgmac()){
+		if (register_msgring_handler
+		    (TX_STN_XGS_0, rmi_phnx_mac_msgring_handler, NULL)) {
+			panic("Couldn't register msgring handler for TX_STN_XGS_0\n");
+		}
+		if (register_msgring_handler
+		    (TX_STN_XGS_1, rmi_phnx_mac_msgring_handler, NULL)) {
+			panic("Couldn't register msgring handler for TX_STN_XGS_1\n");
+		}
+	}
+
+}
+
+extern struct user_mac_data *user_mac;
+
+static int xlr_mac_proc_read(char *page, char **start, off_t off,
+			     int count, int *eof, void *data)
+{
+	int len = 0;
+	off_t begin = 0;
+	int i = 0, cpu = 0;
+	struct net_device *dev = 0;
+	struct driver_data *priv = 0;
+
+#ifdef CONFIG_PHOENIX_GMAC_NAPI
+	if (xlr_napi) 
+	{
+		for (cpu = 0; cpu < 32; cpu++)
+		{
+			if (!cpu_isset(cpu, cpu_online_map))
+				continue;
+			len += sprintf(page + len, "napi: cpu=%02d: %16lld\n", cpu, 
+					   per_cpu(xlr_napi_rx_count, cpu));
+			if (!proc_pos_check(&begin, &len, off, count))
+				goto out;			 
+		}      
+	}
+#endif /* CONFIG_PHOENIX_GMAC_NAPI */
+
+	for(i=0; i<PHOENIX_MAX_MACS; i++) {
+			
+		/* skip the first 2 gmacs for atx_ii boards */
+		if (!mac_active(i)) continue;
+
+		dev = dev_mac[i];
+		priv = netdev_priv(dev);
+		
+		for(cpu=0;cpu<32;cpu++) {
+		
+			if (!cpu_isset(cpu, cpu_online_map))
+				continue; 
+
+			len += sprintf(page + len, "per_cpu: %d %d %d %d %lx %lx %lx %lx\n", 
+				       i, cpu, user_mac->time.hi, user_mac->time.lo,
+				       priv->cpu_stats[cpu].tx_packets,
+				       priv->cpu_stats[cpu].txc_packets,
+				       priv->cpu_stats[cpu].rx_packets,
+				       priv->cpu_stats[cpu].interrupts);
+			if (!proc_pos_check(&begin, &len, off, count))
+				goto out;			       
+		}
+
+		len += sprintf(page + len,
+			       "per_port: %d %d %d %lx %lx %lx %lx\n",
+			       i, user_mac->time.hi, user_mac->time.lo,
+			       priv->stats.rx_packets, priv->stats.rx_bytes,
+			       priv->stats.tx_packets, priv->stats.tx_bytes);
+		if (!proc_pos_check(&begin, &len, off, count))
+			goto out;	     
+	}
+
+	*eof = 1;
+
+out:
+	*start = page + (off - begin);
+	len -= (off - begin);
+	if (len > count)
+		len = count;
+	if (len < 0)
+		len = 0;
+
+	return len;	
+}
+
+#ifdef CONFIG_PHOENIX_HW_BUFFER_MGMT 
+static int setup_auto_free(struct sk_buff *skb, int type, struct msgrng_msg *msg)
+{
+	struct driver_data *priv;
+	struct skb_shared_info *shinfo;
+	int fr_stid, offset;
+
+	shinfo = skb_shinfo(skb);
+	if (!shinfo->rmi_flags)
+		return 0;
+
+	if (atomic_read(&skb->users) != 1) {
+		printk(KERN_ALERT "%s: Can't recycle because of users count\n",
+				__func__);
+		return 0;
+	}
+
+	if (skb->cloned || atomic_read(&(skb_shinfo(skb)->dataref)) != 1) {
+		printk(KERN_EMERG "%s: Can't recycle because of cloned or dataref\n",
+				__func__);
+		return 0;
+	}
+
+	/* Leak no dsk entries! */
+	dst_release(skb->dst);
+
+	/* Now reinitialize old skb, cut & paste from dev_alloc_skb */
+	memset(skb, 0, offsetof(struct sk_buff, truesize));
+	skb->data = skb->head;
+	skb->tail = skb->head;
+
+	atomic_set(&shinfo->dataref, 1);
+	shinfo->nr_frags  = 0;
+	shinfo->gso_size = 0;
+	shinfo->gso_segs = 0;
+	shinfo->gso_type = 0;
+	shinfo->ip6_frag_id = 0;
+	shinfo->frag_list = NULL;
+
+	offset = (((unsigned long)skb->data + SMP_CACHE_BYTES) 
+			& ~(SMP_CACHE_BYTES - 1));
+	skb_reserve(skb, (offset - (unsigned long)skb->data));
+
+	priv = netdev_priv(skb_shinfo(skb)->rmi_owner);
+	fr_stid = msgrng_stid_rfr(priv->instance, priv->type);
+
+	mac_put_skb_back_ptr(skb);
+
+	msg->msg1 = ( ((uint64_t) 1 << 63) |
+				((uint64_t) fr_stid << 54) |
+				((uint64_t) 0 << 40) |
+#ifdef CONFIG_64BIT
+				((uint64_t)virt_to_phys(skb->data) & 0xffffffffffULL)
+#else
+				((unsigned long)(virt_to_phys(skb->data) & 0xffffffffUL))
+#endif
+			  );
+
+	return 1;
+}
+#endif /* CONFIG_PHOENIX_HW_BUFFER_MGMT */
+
+
+#ifdef CONFIG_PHOENIX_GMAC_NAPI
+static int mac_frin_replenish_one_msg(struct net_device *dev)
+{
+	int offset = 0;
+	unsigned long msgrng_flags;
+	struct sk_buff *skb = 0;
+	struct driver_data *priv;
+
+	if (!dev)
+		return 0;
+
+	priv = netdev_priv(dev);
+	skb = __dev_alloc_skb(PHNX_RX_BUF_SIZE, GFP_ATOMIC);
+	if (!skb) {
+		printk(KERN_ALERT "%s: can't alloc skb\n", __func__);
+		return 0;
+	}
+	phnx_inc_counter(REPLENISH_FRIN);
+
+	/* align the data to the next cache line */
+	offset = (((unsigned long)skb->data + SMP_CACHE_BYTES) 
+			& ~(SMP_CACHE_BYTES - 1));
+	skb_reserve(skb, (offset - (unsigned long)skb->data));
+
+#ifdef CONFIG_PHOENIX_HW_BUFFER_MGMT
+	/* 
+	 * put skb under automatic buffer management, if we have just regular NAPI
+	 * w/o HW buffer mgmt, fields are not set.
+	*/
+	skb_shinfo(skb)->rmi_flags = 1;
+	skb_shinfo(skb)->rmi_owner = dev;
+	skb_shinfo(skb)->rmi_refill = mac_frin_replenish_one_msg;
+#endif /* CONFIG_PHOENIX_HW_BUFFER_MGMT */
+
+	mac_put_skb_back_ptr(skb);
+	msgrng_access_enable(msgrng_flags);
+	if (phnx_mac_send_fr(priv, virt_to_bus(skb->data), skb->len)) {
+		dev_kfree_skb(skb);
+		printk("[%s]: rx free message_send failed!\n", __func__);
+	}
+	msgrng_access_disable(msgrng_flags);
+
+	return 0; 
+}
+
+static void xlr_napi_enable_ints(void)
+{
+	unsigned int msgring_config;
+	unsigned long flags = 0, mflags = 0; 
+
+	msgrng_access_save(&xlr_napi_msgrng_lock, flags, mflags);
+	msgring_config = msgrng_read_config();
+
+	/* rewrite the interrupt mask */
+	msgring_config |= (1 << (phoenix_thr_id() + 8));
+	msgrng_write_config(msgring_config); 
+	msgrng_access_restore(&xlr_napi_msgrng_lock, flags, mflags);
+} 
+
+/* called from msgring_process_rx_msgs() from on_chip.c  */
+void xlr_napi_rx_schedule(void)
+{
+	unsigned int msgring_config;
+	unsigned long flags = 0, mflags = 0; 
+	struct net_device *dummy_dev;
+
+	if (!xlr_napi_ready)
+		return;
+
+	/* rewrite the interrupt mask */
+	msgrng_access_save(&xlr_napi_msgrng_lock, flags, mflags);
+
+	msgring_config = msgrng_read_config();
+	msgring_config = msgring_config & ~(1 << (phoenix_thr_id() + 8));
+	msgrng_write_config(msgring_config);
+
+	msgrng_access_restore(&xlr_napi_msgrng_lock, flags, mflags);
+
+	/* Acknowledge interrupt in eirr */
+	write_64bit_cp0_eirr(1ULL << IRQ_MSGRING);
+
+	/* schedule polling for this cpu using dummy_dev */
+	dummy_dev= &__get_cpu_var(xlr_per_cpu_net_device);
+
+	netif_rx_schedule(dummy_dev);
+}
+
+int xlr_napi_poll(struct net_device *dummy_dev, int *budget)
+{
+	struct msgrng_msg msg_body, *msg = &msg_body;
+	int bucket, stid = 0;
+	__u32 length = 0;
+	int ctrl = 0, port = 0;
+	struct sk_buff *skb;
+	int received = 0 ;
+	unsigned int bucket_freeback, bucket_data_rx; 
+	unsigned long addr = 0;
+
+	unsigned int bucket_empty_bm;
+	unsigned int pop_bucket_mask = 0xff; 
+	unsigned long mflags = 0;
+	unsigned int status;
+	int size = 0, code = 0;
+	int cpu = hard_smp_processor_id();
+
+#ifdef CONFIG_PHOENIX_ON_CHIP_DEVICES_NAPI
+	struct tx_stn_handler *handler;
+	int tx_stid;
+#endif /* CONFIG_PHOENIX_ON_CHIP_DEVICES_NAPI */
+
+	bucket_freeback = phoenix_thr_id() + 4;
+	bucket_data_rx = phoenix_thr_id(); 
+	msg_body.msg0 = 0;
+
+	while (1) {
+		/* Why do we need protection around msgrng_read_status ??? */
+		msgrng_access_enable(mflags);
+		bucket_empty_bm = (msgrng_read_status() >> 24) & pop_bucket_mask;
+		msgrng_access_disable(mflags);
+
+		if  (((~bucket_empty_bm)>> bucket_freeback) & 0x1)
+		  bucket = bucket_freeback; 
+		else if  (((~bucket_empty_bm)>> bucket_data_rx) & 0x1)
+		  bucket = bucket_data_rx;
+		else 
+		  break;
+
+		msgrng_access_enable(mflags);     
+		status = message_receive(bucket, &size, &code, &stid, msg);
+		msgrng_access_disable(mflags);
+
+		if (status)
+		  continue;
+
+#ifdef CONFIG_PHOENIX_ON_CHIP_DEVICES_NAPI
+		/* 
+		 * PR: this block is a quick check for messages arriving from 
+		 * other stations
+		*/
+		tx_stid = rxstn_to_txstn_ptr[stid];
+		if (tx_stns[tx_stid].handler.action != rmi_phnx_mac_msgring_handler) {
+			handler = &tx_stns[tx_stid].handler;
+			if (handler->action) {
+				(handler->action)(bucket, size, code, stid, msg, 
+								  handler->dev_id);
+			}
+			continue;
+		}
+#endif /* CONFIG_PHOENIX_ON_CHIP_DEVICES_NAPI */
+
+		length = (msg->msg0 >> 40) & 0x3fff;
+		addr = (unsigned long)bus_to_virt(msg->msg0 & 0xffffffffe0ULL);
+		if (length == 0) {
+#ifdef CONFIG_64BIT
+			unsigned long tmp;
+#endif
+			ctrl = CTRL_REG_FREE;
+			port = (msg->msg0 >> 54) & 0x0f;
+
+#ifdef CONFIG_64BIT
+			tmp = (unsigned long)(msg->msg0 & 0xffffffffffULL);
+			skb = (struct sk_buff *)phys_to_virt(tmp);
+#else
+			skb = (struct sk_buff *)(unsigned long)msg->msg0;
+#endif
+		} else {
+			ctrl = CTRL_SNGL;
+			length = length - BYTE_OFFSET - MAC_CRC_LEN;
+			port = msg->msg0 & 0x0f;
+			skb = mac_get_skb_back_ptr(addr);
+		}
+
+		if (ctrl == CTRL_REG_FREE) {
+			/* we got a tx complete event from the MAC */
+			struct driver_data *priv;
+
+			phnx_inc_counter(NETIF_TX_COMPLETE);
+				
+			/* release the skb and update statistics */
+			priv = netdev_priv(skb->dev);
+			mac_stats_add(priv->stats.tx_packets, 1);
+			mac_stats_add(priv->stats.tx_bytes, skb->len);
+			mac_stats_add(priv->cpu_stats[cpu].txc_packets, 1);
+
+			if (!xlr_mac_optimized_tx) {
+				port_inc_counter(priv->instance, PORT_TX_COMPLETE);
+				phnx_netif_queue_tx_complete(skb->dev);
+			}
+			dev_kfree_skb(skb);
+
+			phnx_set_counter(NETIF_TX_COMPLETE_CYCLES, read_c0_count() - msgrng_msg_cycles);
+			continue;
+		} else if (ctrl == CTRL_SNGL) {
+
+			/* we got a rx buffer with data from the MAC */
+			struct driver_data *priv = 0;
+			unsigned int rxStatus=0;
+					
+			if (IS_XLS()) {
+				if (stid == MSGRNG_STNID_GMAC0)
+					skb->dev = dev_mac[dev_mac_gmac0 + port];
+				else if (stid == MSGRNG_STNID_GMAC1)
+					skb->dev = dev_mac[dev_mac_gmac0 + 4 + port];
+				else {
+					printk("[%s]: rx desc (0x%lx) for unknown station %d? dropping packet\n",
+						   __func__, addr, stid);
+					continue;
+				}
+			} else {
+				if (stid == MSGRNG_STNID_XGS0FR)
+					skb->dev = dev_mac[dev_mac_xgs0];
+				else if (stid == MSGRNG_STNID_XGS1FR)
+					skb->dev = dev_mac[dev_mac_xgs0+1];
+				else
+					skb->dev = dev_mac[dev_mac_gmac0 + port];
+			}
+
+			priv = netdev_priv(skb->dev);
+
+			if (msg->msg0 & (0x40ULL << 56))
+			{
+				rxStatus = (msg->msg0 >> 56 ) & 0x7f;
+				dbg_msg("Rx err 0x%x\n",rxStatus);
+				mac_stats_add(priv->stats.rx_errors,1);
+				if (rxStatus & 0x02)
+					mac_stats_add(priv->stats.rx_crc_errors,1);
+				if (rxStatus & 0x01)
+					mac_stats_add(priv->stats.rx_length_errors,1);
+
+#ifndef CONFIG_PHOENIX_HW_BUFFER_MGMT
+				mac_frin_replenish_one_msg(skb->dev);
+#endif /* !CONFIG_PHOENIX_HW_BUFFER_MGMT */
+				dev_kfree_skb(skb);
+
+				continue;
+			}
+
+#ifdef PA10401_1_GMAC_PKT_DISCARD
+			if ((!is_xls()) && (!(skb->dev->flags & IFF_PROMISC)))
+			{
+				if (!(msg->msg0 & (0x20ULL << 56)))
+				{
+				   if ((*(uint64_t *)(skb->data+MAC_PREPAD + BYTE_OFFSET)>>16) !=
+					  ((*(uint64_t *)skb->dev->dev_addr)>>16))
+					{
+#ifndef CONFIG_PHOENIX_HW_BUFFER_MGMT
+						mac_frin_replenish_one_msg(skb->dev);
+#endif 
+						dev_kfree_skb(skb);
+						continue;
+					}
+				}
+			}
+#endif
+
+			/* compensate for the prepend data, byte offset */
+			skb_reserve(skb, MAC_PREPAD + BYTE_OFFSET);
+
+			skb_put(skb, length);
+			skb->protocol = eth_type_trans(skb, skb->dev);
+
+			mac_stats_add(priv->stats.rx_packets, 1);
+			mac_stats_add(priv->stats.rx_bytes, skb->len);
+			mac_stats_add(priv->cpu_stats[cpu].rx_packets, 1);
+
+			phnx_inc_counter(NETIF_RX);
+			phnx_set_counter(NETIF_RX_CYCLES, (read_c0_count() - msgrng_msg_cycles));
+
+#ifndef CONFIG_PHOENIX_HW_BUFFER_MGMT
+			mac_frin_replenish_one_msg(skb->dev);
+#endif /* !CONFIG_PHOENIX_HW_BUFFER_MGMT */
+
+			netif_receive_skb(skb);
+			received++; 
+			__get_cpu_var(xlr_napi_rx_count)++; 
+
+			if (received > dummy_dev->weight)
+				goto poll_not_done;
+		} 
+		else 
+			printk(KERN_ALERT "[%s]: Problem: unknown message received\n", __func__);
+	}
+
+	dummy_dev->quota -= received;
+	*budget -= received;
+	netif_rx_complete(dummy_dev);
+
+	/* enable interrupt */
+	xlr_napi_enable_ints(); 
+
+	return 0;
+
+poll_not_done:
+	/* PR: I don't understand this */
+	if (!received)
+		received = 1;
+
+	dummy_dev->quota -= received;
+	*budget -= received;
+
+	/* Return NAPI poll not done */
+	return 1; 
+}
+
+static int __init xlr_napi_setup(char *str)
+{
+	int i, cpu_count; 
+	struct net_device *dummy_dev; 
+	int weight_p = 300; 
+
+	if(xlr_hybrid_user_mac_xgmac())
+		return 0;
+
+	printk(KERN_ALERT "%s: Initializing RMI GMAC NAPI subsystem\n", __func__);
+
+	xlr_napi = 1;
+	msgring_int_type = 0x01;
+
+	for (cpu_count = 0; cpu_count < NR_CPUS; cpu_count++) {
+		dummy_dev= &per_cpu(xlr_per_cpu_net_device, cpu_count);
+		dummy_dev->poll = xlr_napi_poll;  
+		dummy_dev->weight = weight_p;
+		dummy_dev->quota =  0;
+		atomic_set(&(dummy_dev->refcnt), 1);
+		set_bit(__LINK_STATE_START, &dummy_dev->state); 
+	}
+
+	for(i = 0; i < NR_CPUS; i++) {
+		per_cpu(xlr_napi_rx_count, i) = 0; 
+	}
+
+	xlr_mac_optimized_tx = 1;
+
+	return 0;
+}
+early_param("xlr_napi", xlr_napi_setup);
+#endif /* CONFIG_PHOENIX_GMAC_NAPI */
+
+static void setup_mac_spill_sizes(int desc)
+{
+	max_num_desc = desc;
+	/* Ideally max_frin_threshold should depend on number of threads processing
+	   the rx packets. But considering the worst case here..
+	 */
+	max_frin_threshold = (max_num_desc/NR_CPUS);
+	if(max_frin_threshold)
+			max_frin_threshold -= 1;
+
+	max_frin_spill = max_num_desc << 2;
+	max_frout_spill = max_num_desc << 2;
+
+	max_class_0_spill = max_num_desc;
+	max_class_1_spill = max_num_desc;
+	max_class_2_spill = max_num_desc;
+	max_class_3_spill = max_num_desc;
+
+#ifdef DEBUG
+	printk("[%s]: max_num_desc = %d\n"
+	       "max_frin_spill = %d\n"
+	       "max_frout_spill = %d\n"
+	       "max_class_0_spill = %d\n"
+	       "max_class_1_spill = %d\n"
+	       "max_class_2_spill = %d\n"
+	       "max_class_3_spill = %d\n",
+	       __func__,
+	       max_num_desc, max_frin_spill, max_frout_spill,
+	       max_class_0_spill, max_class_1_spill,
+	       max_class_2_spill, max_class_3_spill);
+#endif
+}
+
+static int __init xlr_mac_desc_setup(char *str)
+{
+	int desc = simple_strtoul(str, 0, 10);
+
+	printk("[%s]: str = \"%s\", desc=%d\n", __func__, str, desc);
+	setup_mac_spill_sizes(desc);
+	boot_param_max_num_desc = desc;
+
+	return 1;
+}
+
+__setup("xlr_mac_desc=", xlr_mac_desc_setup);
+
+static int __init xls_gmac0_sgmii_setup(char *str)
+{
+	if (is_xls()) {
+		printk("[%s]: *********************************************\n", __func__);
+		printk("[%s]: Enabling SGMII mode for gmac0\n", __func__);
+		printk("[%s]: *********************************************\n", __func__);
+		xls_gmac0_sgmii = 1;
+	}
+
+	return 1;
+}
+__setup("xls_gmac0_sgmii=", xls_gmac0_sgmii_setup);
+
+static int xlr_get_settings(struct net_device *dev, struct ethtool_cmd *cmd)
+{
+	struct driver_data *priv = netdev_priv(dev);
+	int mii_status;
+
+	if(priv->type == TYPE_XGMAC){
+		cmd->supported = SUPPORTED_FIBRE|SUPPORTED_10000baseT_Full;
+		cmd->advertising = SUPPORTED_FIBRE|SUPPORTED_10000baseT_Full;
+		cmd->speed = SPEED_10000;
+		cmd->port = PORT_FIBRE;
+		cmd->duplex = DUPLEX_FULL;
+		cmd->phy_address = priv->instance;
+		cmd->autoneg = AUTONEG_DISABLE;
+		cmd->maxtxpkt = 0;
+		cmd->maxrxpkt = 0;
+
+	}else{
+
+		cmd->supported = SUPPORTED_10baseT_Full | 
+			SUPPORTED_10baseT_Half | 
+			SUPPORTED_100baseT_Full | SUPPORTED_100baseT_Half |
+			SUPPORTED_1000baseT_Full | SUPPORTED_MII |
+			SUPPORTED_Autoneg | SUPPORTED_TP;
+
+		cmd->advertising = priv->advertising;
+
+		mii_status = rmi_phnx_mac_mii_read(priv, MII_NCONFIG, 0);
+		priv->speed = (mii_status >> 3) & 0x03;
+
+		cmd->speed = (priv->speed == phnx_mac_speed_1000) ? SPEED_1000 :
+		(priv->speed == phnx_mac_speed_100) ? SPEED_100: SPEED_10;
+
+		cmd->duplex = (mii_status >> 5) & 0x1;
+		cmd->port = PORT_TP;
+		cmd->phy_address = priv->instance;
+		cmd->transceiver = XCVR_INTERNAL;
+		cmd->autoneg = (~(mii_status >> 14)) & 0x1;
+		cmd->maxtxpkt = 0;
+		cmd->maxrxpkt = 0;
+	}
+
+	return 0;
+}
+
+static int xlr_enable_autoneg(struct net_device *dev, u32 adv)
+{
+	struct driver_data *priv = netdev_priv(dev);
+	int mii_status;
+	u32 flags, adv1, adv2;
+
+	spin_lock_irqsave(&priv->lock, flags);
+	rmi_phnx_mac_set_enable(priv, 0);
+	/* advertising for 10/100 Mbps */
+	adv1 = rmi_phnx_mac_mii_read(priv, MII_ADVERTISE, 0);
+	adv1 &= ~(ADVERTISE_ALL | ADVERTISE_100BASE4);
+	/* advertising for 1000 Mbps */
+	adv2 = rmi_phnx_mac_mii_read(priv, 0x9, 0);
+	adv2 &= ~(0x300);
+
+	if(adv & ADVERTISED_10baseT_Half)
+		adv1 |= ADVERTISE_10HALF;
+	if(adv & ADVERTISED_10baseT_Full)
+		adv1 |= ADVERTISE_10FULL;
+	if(adv & ADVERTISED_100baseT_Full)
+		adv1 |= ADVERTISE_100FULL;
+	if(adv & ADVERTISED_100baseT_Half)
+		adv1 |= ADVERTISE_100HALF;
+
+	if(adv & ADVERTISED_1000baseT_Full)
+		adv2 |= 0x200;
+	if(adv & ADVERTISED_1000baseT_Half)
+		adv2 |= 0x100;
+
+	/* Set the advertising parameters */
+	rmi_phnx_mac_mii_write(priv, MII_ADVERTISE, adv1, 0);
+	rmi_phnx_mac_mii_write(priv, 0x9, adv2, 0);
+
+	priv->advertising = adv1 | adv2;
+
+	mii_status = rmi_phnx_mac_mii_read(priv, MII_BMCR, 0);
+	/* enable autoneg and force restart autoneg */
+	mii_status |= (BMCR_ANENABLE | BMCR_ANRESTART);
+	rmi_phnx_mac_mii_write(priv, MII_BMCR, mii_status, 0);
+
+	rmi_phnx_mac_set_enable(priv, 1);
+	spin_unlock_irqrestore(&priv->lock, flags);
+
+	return 0;
+}
+
+static int xlr_set_link_speed(struct net_device *dev, int speed, int duplex)
+{
+	struct driver_data *priv = netdev_priv(dev);
+	int mii_status = 0, tmp_status;
+	u32 flags;
+	int ret = 0;
+
+	switch(speed) {
+		case SPEED_10:
+			mii_status = 0;
+			priv->speed = phnx_mac_speed_10;
+			break;
+		case SPEED_100:
+			mii_status = 0x2000;
+			priv->speed = phnx_mac_speed_100;
+			break;
+		case SPEED_1000:
+			mii_status = 0x0040;
+			priv->speed = phnx_mac_speed_1000;
+			break;
+		default:
+			ret = -EINVAL;
+			return ret;
+	}
+	if(duplex == DUPLEX_FULL)
+		mii_status |= BMCR_FULLDPLX;
+
+	spin_lock_irqsave(&priv->lock, flags);
+	rmi_phnx_mac_set_enable(priv, 0);
+
+	tmp_status = rmi_phnx_mac_mii_read(priv, MII_BMCR, 0);
+	/* Turn off autoneg, speed and duplexity setting */
+	tmp_status &= ~(BMCR_ANENABLE | 0x0040 | 0x2000 | BMCR_FULLDPLX);
+
+	rmi_phnx_mac_mii_write(priv,
+				MII_BMCR, tmp_status | mii_status, 0);
+
+	if (priv->speed == phnx_mac_speed_10) {
+		phoenix_write_reg(priv->mmio, R_MAC_CONFIG_2, 0x7117);
+		phoenix_write_reg(priv->mmio, R_CORECONTROL, 0x02);
+		printk(KERN_INFO "configuring gmac_%d in 10Mbps mode\n", 
+						priv->instance);
+	}
+	else if (priv->speed == phnx_mac_speed_100) {
+		phoenix_write_reg(priv->mmio, R_MAC_CONFIG_2, 0x7117);
+		phoenix_write_reg(priv->mmio, R_CORECONTROL, 0x01);      
+		printk(KERN_INFO "configuring gmac_%d in 100Mbps mode\n", 
+						priv->instance);
+	}
+	else {
+		phoenix_write_reg(priv->mmio, R_MAC_CONFIG_2, 0x7217);
+		phoenix_write_reg(priv->mmio, R_CORECONTROL, 0x00);      
+		printk(KERN_INFO "configuring gmac_%d in 1000Mbps mode\n", 
+						priv->instance);
+	}
+	rmi_phnx_mac_set_enable(priv, 1);
+	spin_unlock_irqrestore(&priv->lock, flags);
+	return 0;
+
+}
+
+static int xlr_set_settings(struct net_device *dev, struct ethtool_cmd *cmd)
+{
+	int ret;
+	struct driver_data *priv = netdev_priv(dev);
+
+	if(priv->type == TYPE_XGMAC){
+		return -EIO;
+	}
+	if (cmd->autoneg == AUTONEG_ENABLE) {
+		ret = xlr_enable_autoneg(dev, cmd->advertising);
+	}else {
+		ret = xlr_set_link_speed(dev, cmd->speed, cmd->duplex);
+	}
+	return ret;
+}
+
+static void xlr_get_drvinfo(struct net_device *dev, 
+				struct ethtool_drvinfo *info)
+{
+	strcpy(info->driver, DRV_NAME);
+	strcpy(info->version, DRV_VERSION);
+}
+
+static int xlr_get_regs_len(struct net_device *dev) 
+{
+	return PHNX_ETHTOOL_REG_LEN;
+}
+
+static void xlr_get_regs(struct net_device *dev,
+				struct ethtool_regs *regs, void *p)
+{
+	u32 *data = (u32 *)p;
+	int i;
+	struct driver_data *priv = netdev_priv(dev);
+	u32 flags;
+
+	memset((void *)data, 0, PHNX_ETHTOOL_REG_LEN);
+
+	spin_lock_irqsave(&priv->lock, flags);
+	for(i=0; i <= PHNX_NUM_REG_DUMP; i++)
+		*(data + i) = phoenix_read_reg(priv->mmio,  R_TX_CONTROL + i);
+	spin_unlock_irqrestore(&priv->lock, flags);
+}
+
+static u32 xlr_get_msglevel(struct net_device *dev)
+{
+	return mac_debug;
+}
+
+static void xlr_set_msglevel(struct net_device *dev, u32 value)
+{
+	mac_debug = value;
+}
+
+static int xlr_nway_reset(struct net_device *dev)
+{
+	struct driver_data *priv = netdev_priv(dev);
+	int mii_status;
+	u32 flags;
+	int ret = -EINVAL;
+
+  if(priv->type == TYPE_XGMAC)
+    return -EIO;
+
+	spin_lock_irqsave(&priv->lock, flags);
+	mii_status = rmi_phnx_mac_mii_read(priv, MII_BMCR, 0);
+	if(mii_status & BMCR_ANENABLE)
+	{
+		rmi_phnx_mac_mii_write(priv, 
+				MII_BMCR, BMCR_ANRESTART | mii_status, 0);
+		ret = 0;
+	}
+	spin_unlock_irqrestore(&priv->lock, flags);
+	return ret;
+}
+
+static u32 xlr_get_link(struct net_device *dev)
+{
+	struct driver_data *priv = netdev_priv(dev);
+	int mii_status;
+	u32 flags;
+
+  if(priv->type == TYPE_XGMAC)
+    return -EIO;
+
+	spin_lock_irqsave(&priv->lock, flags);
+	mii_status = rmi_phnx_mac_mii_read(priv, MII_BMSR, 0);
+
+	spin_unlock_irqrestore(&priv->lock, flags);
+
+	if(mii_status & BMSR_LSTATUS)
+		return 1;
+	return 0;
+}
+
+#define PHNX_STATS_KEY_LEN  \
+		(sizeof(struct net_device_stats) / sizeof(unsigned long))
+
+static struct {
+	const char string[ETH_GSTRING_LEN];
+} phnx_ethtool_stats_keys[PHNX_STATS_KEY_LEN] = {
+	{ "rx_packets" },
+	{ "tx_packets" },
+	{ "rx_bytes" },
+	{ "tx_bytes" },
+	{ "rx_errors" },
+	{ "tx_errors" },
+	{ "rx_dropped" },
+	{ "tx_dropped" },
+	{ "multicast" },
+	{ "collisions" },
+	{ "rx_length_errors" },
+	{ "rx_over_errors" },
+	{ "rx_crc_errors" },
+	{ "rx_frame_errors" },
+	{ "rx_fifo_errors" },
+	{ "rx_missed_errors" },
+	{ "tx_aborted_errors" },
+	{ "tx_carrier_errors" },
+	{ "tx_fifo_errors" },
+	{ "tx_heartbeat_errors" },
+	{ "tx_window_errors" },
+	{ "rx_compressed" },
+	{ "tx_compressed" }
+};
+
+static int xlr_get_stats_count (struct net_device *dev)
+{
+	return PHNX_STATS_KEY_LEN;
+}
+
+static void xlr_get_strings (struct net_device *dev, u32 stringset, u8 *buf)
+{
+	switch (stringset) {
+	case ETH_SS_STATS:
+		memcpy(buf, &phnx_ethtool_stats_keys, 
+				sizeof(phnx_ethtool_stats_keys));
+		break;
+	default:
+		printk(KERN_WARNING "%s: Invalid stringset %d\n", 
+				__func__, stringset);
+		break;
+	}
+}
+
+static void xlr_get_mac_stats(struct net_device *dev, 
+					struct net_device_stats *stats)
+{
+	struct driver_data *priv = netdev_priv(dev);
+
+	stats->tx_errors = phoenix_read_reg(priv->mmio, TX_FCS_ERROR_COUNTER);
+	stats->rx_dropped = phoenix_read_reg(priv->mmio, 
+						RX_DROP_PACKET_COUNTER);
+	stats->tx_dropped = phoenix_read_reg(priv->mmio, TX_DROP_FRAME_COUNTER);
+
+	stats->multicast = phoenix_read_reg(priv->mmio, 
+						RX_MULTICAST_PACKET_COUNTER);
+	stats->collisions = phoenix_read_reg(priv->mmio, 
+						TX_TOTAL_COLLISION_COUNTER);
+
+	stats->rx_length_errors = phoenix_read_reg(priv->mmio, 
+						RX_FRAME_LENGTH_ERROR_COUNTER);
+	stats->rx_over_errors = phoenix_read_reg(priv->mmio, 
+						RX_DROP_PACKET_COUNTER);
+	stats->rx_crc_errors = phoenix_read_reg(priv->mmio, 
+						RX_FCS_ERROR_COUNTER);
+	stats->rx_frame_errors = phoenix_read_reg(priv->mmio, 
+						RX_ALIGNMENT_ERROR_COUNTER);
+
+	stats->rx_fifo_errors = phoenix_read_reg(priv->mmio,
+					    	RX_DROP_PACKET_COUNTER);
+	stats->rx_missed_errors = phoenix_read_reg(priv->mmio,
+					    	RX_CARRIER_SENSE_ERROR_COUNTER);
+
+	stats->rx_errors = (stats->rx_over_errors + stats->rx_crc_errors +
+			     stats->rx_frame_errors + stats->rx_fifo_errors +
+			     stats->rx_missed_errors);
+
+	stats->tx_aborted_errors = phoenix_read_reg(priv->mmio, 
+			TX_EXCESSIVE_COLLISION_PACKET_COUNTER);
+	stats->tx_carrier_errors = phoenix_read_reg(priv->mmio, 
+					TX_DROP_FRAME_COUNTER);
+	stats->tx_fifo_errors = phoenix_read_reg(priv->mmio, 
+					TX_DROP_FRAME_COUNTER);
+
+}
+
+static void xlr_get_ethtool_stats (struct net_device *dev,
+			struct ethtool_stats *estats, u64 *stats)
+{
+	int i;
+	struct driver_data *priv = netdev_priv(dev);
+	unsigned long flags;
+	u32 *tmp_stats;
+	
+	spin_lock_irqsave(&priv->lock, flags);
+	
+	xlr_get_mac_stats(dev, &priv->stats);
+	
+	spin_unlock_irqrestore(&priv->lock, flags);
+
+	tmp_stats = (u32 *)&priv->stats;
+	for(i=0; i < PHNX_STATS_KEY_LEN; i++) {
+		*stats = (u64)*tmp_stats;
+		stats++;
+		tmp_stats++;
+	}
+}
+
+static struct ethtool_ops xlr_ethtool_ops= {
+        .get_settings           = xlr_get_settings,
+        .set_settings           = xlr_set_settings,
+        .get_drvinfo            = xlr_get_drvinfo,
+        .get_regs_len           = xlr_get_regs_len,
+        .get_regs               = xlr_get_regs,
+        .get_msglevel           = xlr_get_msglevel,
+        .set_msglevel           = xlr_set_msglevel,
+        .nway_reset             = xlr_nway_reset,
+        .get_link               = xlr_get_link,
+        .get_strings            = xlr_get_strings,
+        .get_stats_count        = xlr_get_stats_count,
+        .get_ethtool_stats      = xlr_get_ethtool_stats,
+};
+
+void rmi_reset_gmacs(void)
+{
+    int i;
+    phoenix_reg_t *mmio;
+    volatile uint32_t val;
+
+    if(xlr_loader_support && xlr_loader_sharedcore)
+        return;
+
+    if (xlr_hybrid_rmios_ipsec() || xlr_hybrid_rmios_tcpip_stack())
+        return;
+
+	for (i = 0; i < PHOENIX_MAX_MACS; i++) {
+
+		if (phnx_mac_devices[i].type != TYPE_GMAC)
+			continue;
+
+		/* Arizona II has only gmac_2 and gmac_3 enabled */
+		if (!mac_active(i))  continue;
+
+		mmio = (phoenix_reg_t *)(phoenix_io_base +
+                phnx_mac_devices[i].phnx_io_offset);
+
+        /* Disable MAC RX */
+        val = phoenix_read_reg(mmio, R_MAC_CONFIG_1);
+        val &= ~0x4;
+        phoenix_write_reg(mmio, R_MAC_CONFIG_1, val);
+
+        /* Disable Core RX */
+        val = phoenix_read_reg(mmio, R_RX_CONTROL);
+        val &= ~0x1;
+        phoenix_write_reg(mmio, R_RX_CONTROL, val);
+
+        /* wait for rx to halt */
+        while(1) {
+            val = phoenix_read_reg(mmio, R_RX_CONTROL);
+            if(val & 0x2)
+                break;
+            mdelay(1);
+        }
+
+        /* Issue a soft reset */
+        val = phoenix_read_reg(mmio, R_RX_CONTROL);
+        val |= 0x4;
+        phoenix_write_reg(mmio, R_RX_CONTROL, val);
+           
+        /* wait for reset to complete */
+        while(1) {
+            val = phoenix_read_reg(mmio, R_RX_CONTROL);
+            if(val & 0x8)
+                break;
+            mdelay(1);
+        }
+
+        /* Clear the soft reset bit */
+        val = phoenix_read_reg(mmio, R_RX_CONTROL);
+        val &= ~0x4;
+        phoenix_write_reg(mmio, R_RX_CONTROL, val);
+    }
+}
+
+int rmi_phnx_mac_init_module(void)
+{
+	struct net_device *dev = 0;
+	struct driver_data *priv = 0;
+	unsigned long mmio_start = 0;
+	int i = 0;
+	int ret = 0;
+	struct proc_dir_entry *entry;
+	
+	if(dev_tree_en)
+		return 0;
+
+	/* LTE board does not have RGMII0 interface*/
+	if(xlr_board_atx_viii())
+		xls_gmac0_sgmii=1;
+
+        /* Initialize cached variable indicating board type XLS vs XLR */
+        rmi_board_type = is_xls();
+
+#ifdef CONFIG_PHOENIX_GMAC_NAPI
+        /* Initialize spinlock protecting NAPI msgring_config access */
+		spin_lock_init(&xlr_napi_msgrng_lock);
+#endif /* CONFIG_PHOENIX_GMAC_NAPI */
+
+	/* calculate mac spill sizes */
+	if (!max_num_desc) {
+		if (xlr_hybrid_user_mac())
+			setup_mac_spill_sizes(2048);
+		else
+			setup_mac_spill_sizes(MAX_NUM_DESC);
+	}
+
+	entry = create_proc_read_entry("xlr_mac", 0 /* def mode */ ,
+				       0 /* no parent */ ,
+				       xlr_mac_proc_read
+				       /* proc read function */ ,
+				       0	/* no client data */
+		);
+	if (!entry) {
+		printk("[%s]: Unable to create proc read entry for xlr_mac!\n",
+		       __func__);
+	}	
+
+	/* if support for loading apps on same core as Linux is enabled */
+	if(xlr_loader_support && xlr_loader_sharedcore && !xlr_hybrid_rmios_ipsec()) {
+		if (!xlr_loader_own_gmac)
+			return -EINVAL;
+		else
+			own_gmac_only = 1;
+	}
+
+    rmi_reset_gmacs();
+
+	for (i = 0; i < PHOENIX_MAX_MACS; i++) {
+
+		if (own_gmac_only && phnx_mac_devices[i].type != TYPE_GMAC)
+			continue;
+
+		/* Arizona II has only gmac_2 and gmac_3 enabled */
+		if (!mac_active(i))  continue;
+
+		dbg_msg("Registering phnx_mac[%d]\n", i);
+
+		dev = alloc_etherdev(sizeof(struct driver_data));
+		if (!dev) {
+			ret = -ENOMEM;
+			goto out;
+		}
+
+		priv = netdev_priv(dev);
+		priv->dev = dev;
+
+		mmio_start =
+			phoenix_io_base + phnx_mac_devices[i].phnx_io_offset;
+
+		priv->mmio = (phoenix_reg_t *) mmio_start;
+		if (!priv->mmio) {
+			dbg_panic
+				("Unable to ioremap MMIO region of size %x @ %lx\n",
+				 PHOENIX_IO_SIZE, mmio_start);
+		}
+
+		dbg_msg("mmio_start=%lx, priv->mmio=%p\n", mmio_start,
+			priv->mmio);
+
+		/* Initialize the net_device */
+		if (!own_gmac_only) {
+			dev->irq = phnx_mac_devices[i].irq;
+			if (request_irq(dev->irq, rmi_phnx_mac_int_handler,
+						IRQF_DISABLED, dev->name, dev)) {
+				ret = -EBUSY;
+				panic("Couldn't get mac interrupt line (%d)", dev->irq);
+			}
+		}
+
+		ether_setup(dev);
+
+		dev->base_addr = mmio_start;
+		dev->mem_end = mmio_start + PHOENIX_IO_SIZE - 1;
+
+		dev->open = rmi_phnx_mac_open;
+		dev->hard_start_xmit = rmi_phnx_mac_xmit;
+		dev->stop = rmi_phnx_mac_close;
+		dev->get_stats = rmi_phnx_mac_get_stats;
+		dev->set_multicast_list = rmi_phnx_mac_set_multicast_list;
+		dev->set_mac_address = rmi_phnx_set_mac_address;
+		dev->do_ioctl = rmi_phnx_mac_do_ioctl;
+		dev->tx_timeout = rmi_phnx_mac_tx_timeout;
+		dev->watchdog_timeo = (1000 * HZ);
+		dev->change_mtu = rmi_phnx_mac_change_mtu;
+
+		if (xlr_mac_optimized_tx) {
+			dev->tx_queue_len = 0;
+			dev->features |= NETIF_F_LLTX;
+		}
+		else {
+			dev->tx_queue_len = 10000;
+		}
+
+		SET_ETHTOOL_OPS(dev,&xlr_ethtool_ops);
+		/* Initialize the device specific driver data */
+		spin_lock_init(&priv->lock);
+
+		priv->id = i;
+		priv->instance = phnx_mac_devices[i].instance;
+		priv->type = phnx_mac_devices[i].type;
+		priv->phy_addr = gmac_id_to_phy_addr(priv);
+		priv->spill_configured = 0;
+
+		if (priv->type == TYPE_GMAC) {
+
+			if (priv->instance < 4) {
+				/* configure spill area only once for all gmacs */
+				if (gmac_spill_configured) priv->spill_configured = 1;
+				else gmac_spill_configured = 1;
+			}
+			else {
+				if (gmac1_spill_configured) priv->spill_configured = 1;
+				else gmac1_spill_configured = 1;
+			}
+
+			if (!xlr_hybrid_rmios_ipsec() && !xlr_hybrid_rmios_tcpip_stack())
+				rmi_phnx_gmac_init(priv);
+			else
+				rmi_phnx_gmac_init_hybrid(priv);
+		} else if (priv->type == TYPE_XGMAC) {
+			if (!xlr_hybrid_rmios_ipsec() && !xlr_hybrid_rmios_tcpip_stack())
+				rmi_phnx_xgmac_init(priv);
+		}
+
+		phnx_mac_get_hwaddr(dev);
+
+		if (!xlr_hybrid_rmios_ipsec() && !xlr_hybrid_rmios_tcpip_stack())
+			phnx_mac_setup_hwaddr(priv);
+
+		if (!xlr_hybrid_user_mac() && !xlr_hybrid_rmios_tcpip_stack() &&
+				!is_user_mac_xgmac(priv)) {
+			ret = register_netdev(dev);
+			if (ret) {
+				dbg_panic("Unable to register net device\n");
+			}
+			else {
+				if (priv->type == TYPE_GMAC)
+					printk("GMAC_%d initialized as %s\n", priv->instance, priv->dev->name);
+				else if (priv->type == TYPE_XGMAC)
+					printk("XGMAC_%d initialized as %s\n", priv->instance, priv->dev->name);
+			}
+
+		}
+
+		dbg_msg("%s: Phoenix Mac at 0x%p (mtu=%d)\n",
+			dev->name, priv->mmio, dev->mtu);
+		dev_mac[i] = dev;
+		if (priv->type == TYPE_XGMAC && priv->instance == 0) dev_mac_xgs0 = i;
+		if (priv->type == TYPE_GMAC && priv->instance == 0) dev_mac_gmac0 = i;
+		/* strictly speaking, user_mac driver should do this */
+		if(xlr_hybrid_user_mac())
+			rmi_phnx_mac_set_enable(priv, 1);
+	}
+
+	mac_common_init();
+	dbg_msg("port_counters = %p\n", port_counters);
+	dbg_msg("pending_tx_lock = %p, pending_tx = %p\n", port_counters,
+		pending_tx);
+
+#ifdef CONFIG_PHOENIX_IP_FLOW_AFFINITY
+        /* initialize cpu skb queues */
+        cpu_tx_queue_init();
+#endif /* CONFIG_PHOENIX_IP_FLOW_AFFINITY */
+
+ out:
+       if ( (xlr_board_atx_v() || xlr_board_atx_iv_b())) {
+               /* on atx-v and atx-iv-b read rgmii interrupt at least once */
+               struct driver_data data;
+               data.phy_addr = 3;
+               data.instance = 3;
+               data.type = TYPE_GMAC;
+               rmi_phnx_mac_mii_read(&data, 26, 0);
+         }
+ 
+        for (i = 0; i < PHOENIX_MAX_MACS; i++) {
+
+
+                if (phnx_mac_devices[i].type != TYPE_GMAC)
+                        continue;
+
+                if (!mac_active(i))  continue;
+
+		priv = netdev_priv(dev_mac[i]);
+                if (!xlr_hybrid_rmios_ipsec() && !xlr_hybrid_rmios_tcpip_stack())
+                {
+			if(!xlr_hybrid_user_mac())
+				phnx_mac_set_rx_mode(dev_mac[i]);
+                        phoenix_write_reg(priv->mmio, R_INTMASK,
+                                  (1<<O_INTMASK__TxIllegal)       |
+                                  (((priv->instance&0x3)==0)<<O_INTMASK__MDInt)           |
+                                  (1<<O_INTMASK__TxFetchError)    |
+                                  (1<<O_INTMASK__P2PSpillEcc)     |
+                                  (1<<O_INTMASK__TagFull)         |
+                                  (1<<O_INTMASK__Underrun)        |
+                                  (1<<O_INTMASK__Abort));
+                }
+	}
+	if (xlr_hybrid_rmios_ipsec()) {
+		uint64_t *linux_started = (uint64_t *)0xffffffff81000000ULL;
+		*linux_started = 1;
+	}
+	if (ret < 0) {
+		dbg_panic("Error, ret = %d\n", ret);
+	}
+	return ret;
+}
+
+void rmi_phnx_mac_exit_module(void)
+{
+	struct net_device *dev;
+	int idx;
+
+	if(dev_tree_en)
+		return;
+
+
+	for (idx = 0; idx < PHOENIX_MAX_MACS; idx++) {
+		dev = dev_mac[idx];
+		if (!dev)
+			continue;
+
+		unregister_netdev(dev);
+		free_netdev(dev);
+	}
+}
+
+module_init(rmi_phnx_mac_init_module);
+module_exit(rmi_phnx_mac_exit_module);
+
+/******************************************************************************
+ * TODO:
+ *   o Currently, if Tx completes do not come back, Tx hangs for ever. 
+ *     Though it is good for debugging, there should be a timeout mechanism.
+ *   o Right now, all cpu-threads across cpus are serialized for transmitting
+ *     packets. However, message_send is "atomic", hence all of them should
+ *     transmit without contending for the lock. some like per-cpu, per device
+ *     lock and handling tx complete on the cpu that did the transmit
+ *   o use fetchadd for stat variable. Currently, it not even atomic
+ *****************************************************************************/
diff --git a/drivers/net/phoenix_mac_devtree.c b/drivers/net/phoenix_mac_devtree.c
new file mode 100644
index 0000000..6cdbe4f
--- /dev/null
+++ b/drivers/net/phoenix_mac_devtree.c
@@ -0,0 +1,3227 @@
+/*********************************************************************
+
+  Copyright 2003-2008 Raza Microelectronics, Inc. (RMI). All rights
+  reserved.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+  1. Redistributions of source code must retain the above copyright
+  notice, this list of conditions and the following disclaimer.
+  2. Redistributions in binary form must reproduce the above copyright
+  notice, this list of conditions and the following disclaimer in
+  the documentation and/or other materials provided with the
+  distribution.
+
+  THIS SOFTWARE IS PROVIDED BY Raza Microelectronics, Inc. ``AS IS'' AND
+  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL RMI OR CONTRIBUTORS BE LIABLE
+  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+  THE POSSIBILITY OF SUCH DAMAGE.
+
+  *****************************#RMI_2#**********************************/
+
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/interrupt.h>
+#include <linux/fs.h>
+#include <linux/types.h>
+#include <linux/string.h>
+#include <linux/socket.h>
+#include <linux/errno.h>
+#include <linux/fcntl.h>
+#include <linux/in.h>
+#include <linux/init.h>
+#include <linux/pci.h>
+#include <linux/workqueue.h>
+#include <linux/kernel.h>
+#include <linux/inet.h>
+#include <linux/netdevice.h>
+#include <linux/ethtool.h>
+#include <linux/etherdevice.h>
+#include <linux/skbuff.h>
+#include <net/sock.h>
+#include <linux/if_ether.h>		/* For the statistics structure. */
+#include <linux/if_arp.h>		/* For ARPHRD_ETHER */
+#include <linux/autoconf.h>
+#include <linux/proc_fs.h>
+#include <linux/mii.h>
+#include <asm/system.h>
+#include <asm/uaccess.h>
+#include <asm/io.h>
+#include <asm/cache.h>
+
+#include <asm/rmi/debug.h>
+#include <asm/rmi/pci.h>
+#include <asm/rmi/pic.h>
+#include <asm/rmi/phoenix_mac.h>
+#include <asm/rmi/mips-exts.h>
+#include <asm/rmi/msgring.h>
+#include <asm/rmi/sim.h>
+#include <asm/rmi/rmios_user_mac.h>
+#include <asm/rmi/phnx_user_mac.h>
+#include <asm/rmi/atx_cpld.h>
+#include <asm/rmi/xgmac_mdio.h>
+#include <asm/rmi/proc.h>
+#include <asm/smp.h>
+#include <asm/rmi/iomap.h>
+#include <asm/rmi/gpio.h>
+#include <asm/rmi/user/phnx_user_mac.h>
+#include <linux/delay.h>
+
+#define DRV_NAME	"rmi_phnx_mac"
+#define DRV_VERSION	"0.1"
+
+/* #define DEBUG */
+
+#ifdef DEBUG
+#undef dbg_msg
+int mac_debug = 1;
+#define dbg_msg(fmt, args...) \
+        do {\
+            if (mac_debug) {\
+                printk("[%s@%d|%s]: cpu_%d: " fmt, \
+                __FILE__, __LINE__, __func__,  smp_processor_id(), ##args);\
+            }\
+        } while(0);
+
+#define DUMP_PACKETS
+#else
+#undef dbg_msg
+#define dbg_msg(fmt, args...)
+extern int mac_debug;
+#endif
+
+
+#ifdef CONFIG_PHOENIX_GMAC_NAPI
+
+/* XLR NAPI per CPU packet counter */
+DEFINE_PER_CPU(unsigned long long, xlr_napi_rx_count_);
+
+extern int xlr_napi;
+extern int xlr_napi_ready;
+extern spinlock_t xlr_napi_msgrng_lock;
+
+#endif /* CONFIG_PHOENIX_GMAC_NAPI */
+
+#define MAC_B2B_IPG             88
+
+/* frame sizes need to be cacheline aligned */
+#define MAX_FRAME_SIZE          1600
+
+#define MAC_SKB_BACK_PTR_SIZE   SMP_CACHE_BYTES
+#define MAC_PREPAD              0
+#define BYTE_OFFSET             2
+#define PHNX_RX_BUF_SIZE (MAX_FRAME_SIZE + BYTE_OFFSET + MAC_PREPAD 	\
+							+ MAC_SKB_BACK_PTR_SIZE + SMP_CACHE_BYTES)
+#define MAC_CRC_LEN             4
+#define MAX_NUM_MSGRNG_STN_CC   128
+#define MAX_NUM_GMAC_STNS   8
+#define MAX_NUM_XGMAC_STNS   18
+
+#define SEND_TO_GMAC0__FDISC_TO_GMAC 0x11
+#define SEND_TO_GMAC1__FDISC_TO_GMAC 0x22
+#define SEND_TO_GMAC2__FDISC_TO_GMAC 0x33
+#define SEND_TO_GMAC3__FDISC_TO_GMAC 0x44
+#define SEND_TO_GMAC4__FDISC_TO_GMAC1 0x55
+#define SEND_TO_GMAC5__FDISC_TO_GMAC1 0x66
+#define SEND_TO_GMAC6__FDISC_TO_GMAC1 0x77
+#define SEND_TO_GMAC7__FDISC_TO_GMAC1 0x88
+
+#define PHNX_GMAC_SHARED 0x1000
+
+#define MAX_NUM_DESC		512
+
+/* 
+ * NOTE:
+ * Don't change this threshold to > 15 if MAX_NUM_DESC is 512. 
+ * When msgring_thread_mask is 0xf, each cpu could receive 16 packets 
+ * and replenishment may never happen.
+ * THRESHOLD should be less than 
+ * MAX_NUM_DESC / (number of threads processing msgring * number of cores)
+*/
+#define MAC_FRIN_TO_BE_SENT_THRESHOLD 15
+
+/*
+ * Total Nr of Free Descriptors to GMACs > 2816 for Usermac 
+ * If configuring max_num_desc use at least 2816/4.
+*/
+static int max_num_desc = 0;
+static int max_frin_spill = 0;
+static int max_frout_spill = 0;
+static int max_class_0_spill = 0;
+static int max_class_1_spill = 0;
+static int max_class_2_spill = 0;
+static int max_class_3_spill = 0;
+
+#define PHNX_NUM_REG_DUMP 9		/* Register 0xa0 to 0xa8 */
+#define PHNX_ETHTOOL_REG_LEN (PHNX_NUM_REG_DUMP * 4)
+
+extern void phnx_user_mac_int_handler(int bucket, int size, int code,
+									  int stid, struct msgrng_msg *msg,
+									  void *data /* ignored */ );
+extern int xlr_loader_support;
+extern int xlr_loader_sharedcore;
+extern int xlr_loader_own_gmac;
+
+extern int xlr_mac_optimized_tx;
+static int xls_gmac0_sgmii = 0;
+
+static int gmac_spill_configured[] = { 0x0, 0x0 };
+static int gmac_own, xgmac_own;
+extern void *rmi_get_spill_mem(uint32_t);
+extern int arizona_gmac_phy_write(unsigned long gmac_offset, int num,
+								  int reg, uint32_t data);
+
+#ifdef CONFIG_PHOENIX_IP_FLOW_AFFINITY
+/* skb transfer statistics */
+unsigned long long skb_transfer_stat[NR_CPUS][NR_CPUS];
+
+void skb_transfer_finish(void);
+static void skb_transfer(int bucket, struct sk_buff *skb);
+
+/* skb transfer queues, one per CPU */
+static struct sk_buff_head cpu_skb_tqueue[NR_CPUS];
+
+static void cpu_tx_queue_init(void)
+{
+	int i;
+
+	for (i = 0; i < NR_CPUS; i++) {
+		skb_queue_head_init(&(cpu_skb_tqueue[i]));
+	}
+}
+
+#endif /* CONFIG_PHOENIX_IP_FLOW_AFFINITY */
+
+
+/* This message ring interrupt type, can be adjusted by NAPI setup callback */
+extern int msgring_int_type;
+
+/* This is cached version of is_xls function */
+static int rmi_board_type = 0;
+
+/* 
+ * Macro to access cached board type. This macro is opposed to
+ * to function is_xls(), which always performs register reads
+ * and case analysis on the fly.
+*/
+#define IS_XLS() (rmi_board_type)
+
+#ifdef CONFIG_PHOENIX_GMAC_NAPI
+static int mac_frin_replenish_one_msg(struct net_device *dev);
+#ifdef CONFIG_PHOENIX_HW_BUFFER_MGMT
+static int setup_auto_free(struct sk_buff *skb, int type,
+						   struct msgrng_msg *msg);
+#endif /* CONFIG_PHOENIX_HW_BUFFER_MGMT */
+#endif /* CONFIG_PHOENIX_GMAC_NAPI */
+
+extern void *rmi_frin_mem_alloc(int size);
+extern void *rmi_spill_mem_alloc(int spill_size);
+
+/*****************************************************************
+ * Phoenix Generic Mac driver
+ *****************************************************************/
+typedef enum
+{
+	phnx_mac_speed_10,
+	phnx_mac_speed_100,
+	phnx_mac_speed_1000,
+	phnx_mac_speed_rsvd
+} phnx_mac_speed_t;
+
+typedef enum
+{
+	phnx_mac_duplex_auto,
+	phnx_mac_duplex_half,
+	phnx_mac_duplex_full
+} phnx_mac_duplex_t;
+
+typedef enum
+{
+	phnx_mac_fc_auto,
+	phnx_mac_fc_disabled,
+	phnx_mac_fc_frame,
+	phnx_mac_fc_collision,
+	phnx_mac_fc_carrier
+} phnx_mac_fc_t;
+
+#define MAC_FRIN_WORK_NUM 32
+static struct work_struct mac_frin_replenish_work[MAC_FRIN_WORK_NUM];
+
+struct cpu_stat
+{
+	unsigned long tx_packets;
+	unsigned long txc_packets;
+	unsigned long rx_packets;
+	unsigned long interrupts;
+};
+
+struct driver_data
+{
+
+	/* 
+	 * Let these be the first fields in this structure 
+	 * the structure is cacheline aligned when allocated in 
+	 * init_etherdev
+	 */
+	struct fr_desc *frin_spill;
+	struct fr_desc *frout_spill;
+	union rx_tx_desc *class_0_spill;
+	union rx_tx_desc *class_1_spill;
+	union rx_tx_desc *class_2_spill;
+	union rx_tx_desc *class_3_spill;
+	int spill_configured;
+
+	struct net_device *dev;		/* pointer to linux device */
+	struct timer_list link_timer;	/* for monitoring MII */
+	struct net_device_stats stats;
+	spinlock_t lock;
+
+	phoenix_reg_t *mmio;
+
+	__u8 hwaddr[6];
+	int phy_oldbmsr;
+	int phy_oldanlpar;
+	int phy_oldk1stsr;
+	int phy_oldlinkstat;
+	unsigned char phys_addr[2];
+
+	phnx_mac_speed_t speed;		/* current speed */
+	phnx_mac_duplex_t duplex;	/* current duplex */
+	phnx_mac_fc_t flow_ctrl;	/* current flow control setting */
+	int advertising;
+
+	int id;
+	int type;
+	int instance;
+	int own;
+	int phy_addr;
+	atomic_t frin_to_be_sent[MAC_FRIN_WORK_NUM];
+	int init_frin_desc;
+
+	struct cpu_stat cpu_stats[32];
+};
+
+enum
+{
+	PORT_TX,
+	PORT_TX_COMPLETE,
+	PORT_STARTQ,
+	PORT_STOPQ,
+	PORT_START_DEV_STATE,
+	PORT_STOP_DEV_STATE,
+};
+
+#define port_inc_counter(port, counter) 	atomic_inc(&port_counters[port][(counter)])
+#define port_set_counter(port, counter, value) 	atomic_set(&port_counters[port][(counter)], (value))
+static atomic_t port_counters[8][8] __cacheline_aligned;
+static spinlock_t pending_tx_lock[8] __cacheline_aligned;
+static int pending_tx[8] __cacheline_aligned;
+
+static __inline__ unsigned int ldadd_wu(unsigned int value,
+										unsigned long *addr)
+{
+	__asm__ __volatile__(".set push\n"
+						 ".set noreorder\n"
+						 ".set mips64\n" "move $8, %2\n" "move $9, %3\n"
+#ifdef CONFIG_64BIT
+						 //"ldadd $8, $9\n"
+						 ".dword 0x71280012\n"
+#else
+						 //"ldaddwu $8, $9\n"
+						 ".word 0x71280011\n"
+#endif
+						 "move %0, $8\n"
+						 ".set pop\n":"=&r"(value), "+m"(*addr)
+						 :"0"(value), "r"((unsigned long) addr)
+						 :"$8", "$9");
+	return value;
+}
+
+#define mac_stats_add(x, val) ldadd_wu(val, &x)
+
+struct phnx_mac
+{
+	int instance;
+	int type;
+	int irq;
+	unsigned long phnx_io_offset;
+};
+
+/* The _order_ of the device definitions below should be preserved */
+static struct phnx_mac phnx_mac_devices[] = {
+	{.instance = 0,.type = TYPE_GMAC,
+	 .phnx_io_offset = PHOENIX_IO_GMAC_0_OFFSET,.irq = PIC_GMAC_0_IRQ},
+	{.instance = 1,.type = TYPE_GMAC,
+	 .phnx_io_offset = PHOENIX_IO_GMAC_1_OFFSET,.irq = PIC_GMAC_1_IRQ},
+	{.instance = 2,.type = TYPE_GMAC,
+	 .phnx_io_offset = PHOENIX_IO_GMAC_2_OFFSET,.irq = PIC_GMAC_2_IRQ},
+	{.instance = 3,.type = TYPE_GMAC,
+	 .phnx_io_offset = PHOENIX_IO_GMAC_3_OFFSET,.irq = PIC_GMAC_3_IRQ},
+#ifdef XLS
+	{.instance = 4,.type = TYPE_GMAC,
+	 .phnx_io_offset = PHOENIX_IO_GMAC_4_OFFSET,.irq = PIC_GMAC_4_IRQ},
+	{.instance = 5,.type = TYPE_GMAC,
+	 .phnx_io_offset = PHOENIX_IO_GMAC_5_OFFSET,.irq = PIC_GMAC_5_IRQ},
+	{.instance = 6,.type = TYPE_GMAC,
+	 .phnx_io_offset = PHOENIX_IO_GMAC_6_OFFSET,.irq = PIC_GMAC_6_IRQ},
+	{.instance = 7,.type = TYPE_GMAC,
+	 .phnx_io_offset = PHOENIX_IO_GMAC_7_OFFSET,.irq = PIC_GMAC_7_IRQ},
+#endif
+	{.instance = 0,.type = TYPE_XGMAC,
+	 .phnx_io_offset = PHOENIX_IO_XGMAC_0_OFFSET,.irq = PIC_XGS_0_IRQ},
+	{.instance = 1,.type = TYPE_XGMAC,
+	 .phnx_io_offset = PHOENIX_IO_XGMAC_1_OFFSET,.irq = PIC_XGS_1_IRQ}
+};
+
+#define PHOENIX_MAX_MACS	(int)(sizeof(phnx_mac_devices)/sizeof(struct phnx_mac))
+static struct net_device *dev_mac[PHOENIX_MAX_MACS];
+static int dev_mac_xgs0;
+static int dev_mac_gmac0;
+#define PHOENIX_MAX_XGMACS  2
+
+
+/* fdt functions */
+extern uint32_t dev_tree_en;
+extern int fdt_get_gmac_ppties(unsigned int *gmac_list,
+							   unsigned long *gmac_addr, int *gmac_own);
+extern int fdt_get_xgmac_ppties(int port, unsigned int *gmac_list,
+								unsigned long *gmac_addr, int *gmac_own);
+extern int fdt_get_xgmac_bucket_conf(int instance, char buckets[],
+									 int bklen, char credits[], int crlen);
+extern int fdt_get_gmac_bucket_conf(int gmac_id, char buckets[],
+									int bklen, char credits[], int crlen);
+
+/* changes because of fdt */
+extern int xlr_napi_poll(struct net_device *dummy_dev, int *budget);
+
+static __inline__ int xls_rgmii_mode(struct driver_data *priv)
+{
+	if (is_xls() && !xls_gmac0_sgmii
+		&& priv->instance == 0 && priv->type == TYPE_GMAC)
+		return 1;
+	else
+		return 0;
+}
+
+static __inline__ int gmac_id_to_phy_addr(struct driver_data *priv)
+{
+	int id = priv->id;
+
+	if (is_xls()) {
+		if (xls_rgmii_mode(priv))
+			return 0;
+		return id + 16;
+	}
+	else {
+		if (xlr_board_atx_ii() && !xlr_board_atx_ii_b())
+			return id - 2;
+		else
+			return id;
+	}
+}
+
+static void rmi_phnx_mac_set_enable(struct driver_data *priv, int flag);
+static void rmi_phnx_xgmac_init(struct driver_data *priv);
+static void rmi_phnx_gmac_init(struct driver_data *priv);
+static void phnx_mac_set_rx_mode(struct net_device *dev);
+void rmi_phnx_dev_tree_mac_msgring_handler(int bucket, int size, int code,
+										   int stid, struct msgrng_msg *msg,
+										   void *data);
+static irqreturn_t rmi_phnx_mac_int_handler(int irq, void *dev_id);
+static int rmi_phnx_mac_open(struct net_device *dev);
+static int rmi_phnx_mac_xmit(struct sk_buff *skb, struct net_device *dev);
+static int rmi_phnx_mac_close(struct net_device *dev);
+static void rmi_phnx_mac_timer(unsigned long data);
+static struct net_device_stats *rmi_phnx_mac_get_stats(struct net_device
+													   *dev);
+static void rmi_phnx_mac_set_multicast_list(struct net_device *dev);
+static int rmi_phnx_mac_do_ioctl(struct net_device *dev,
+								 struct ifreq *rq, int cmd);
+static void rmi_phnx_mac_tx_timeout(struct net_device *dev);
+static int rmi_phnx_mac_change_mtu(struct net_device *dev, int new_mtu);
+static int rmi_phnx_mac_fill_rxfr(struct net_device *dev);
+static void rmi_phnx_config_spill_area(struct driver_data *priv);
+
+/*****************************************************************
+ * Driver Helper Functions
+ *****************************************************************/
+static __inline__ struct sk_buff *mac_get_skb_back_ptr(unsigned long addr)
+{
+	unsigned long *back_ptr =
+		(unsigned long *) (addr - MAC_SKB_BACK_PTR_SIZE);
+	dbg_msg("addr = %lx,  skb = %lx\n", addr, *back_ptr);
+	/*
+	 * this function should be used only for newly allocated packets. It assumes
+	 * the first cacheline is for the back pointer related book keeping info
+	 */
+	return (struct sk_buff *) (*back_ptr);
+}
+
+static __inline__ void mac_put_skb_back_ptr(struct sk_buff *skb)
+{
+	unsigned long *back_ptr = (unsigned long *) skb->data;
+
+	/* 
+	 * this function should be used only for newly allocated packets. It assumes
+	 * the first cacheline is for the back pointer related book keeping info
+	 */
+	skb_reserve(skb, MAC_SKB_BACK_PTR_SIZE);
+	*back_ptr = (unsigned long) skb;
+	dbg_msg("p=%p, skb=%p\n", back_ptr, skb);
+}
+
+#define CACHELINE_ALIGNED_ADDR(addr) (((unsigned long)(addr)) 		\
+			& ~(SMP_CACHE_BYTES-1))
+
+static __inline__ void *cacheline_aligned_kmalloc(int size, int gfp_mask)
+{
+	void *buf = kmalloc(size + SMP_CACHE_BYTES, gfp_mask);
+	if (buf)
+		buf =
+			(void
+			 *) (CACHELINE_ALIGNED_ADDR((unsigned long) buf +
+										SMP_CACHE_BYTES));
+	return buf;
+}
+
+static __inline__ struct sk_buff *rmi_phnx_alloc_skb(void)
+{
+	int offset = 0;
+	struct sk_buff *skb = __dev_alloc_skb(PHNX_RX_BUF_SIZE, GFP_KERNEL);
+
+	if (!skb) {
+		return NULL;
+	}
+
+	/* align the data to the next cache line */
+	offset = ((unsigned long) skb->data + SMP_CACHE_BYTES) &
+		~(SMP_CACHE_BYTES - 1);
+	skb_reserve(skb, (offset - (unsigned long) skb->data));
+
+	return skb;
+}
+
+static void rmi_phnx_mac_set_enable(struct driver_data *priv, int flag)
+{
+	uint32_t regval;
+	int tx_threshold = 1518;
+
+	if (flag) {
+		regval = phoenix_read_reg(priv->mmio, R_TX_CONTROL);
+		regval |= (1 << O_TX_CONTROL__TxEnable) |
+			(tx_threshold << O_TX_CONTROL__TxThreshold);
+
+		phoenix_write_reg(priv->mmio, R_TX_CONTROL, regval);
+
+		regval = phoenix_read_reg(priv->mmio, R_RX_CONTROL);
+		regval |= 1 << O_RX_CONTROL__RxEnable;
+		if (xls_rgmii_mode(priv))
+			regval |= 1 << O_RX_CONTROL__RGMII;
+		phoenix_write_reg(priv->mmio, R_RX_CONTROL, regval);
+	}
+	else {
+		regval = phoenix_read_reg(priv->mmio, R_TX_CONTROL);
+		regval &= ~((1 << O_TX_CONTROL__TxEnable) |
+					(tx_threshold << O_TX_CONTROL__TxThreshold));
+
+		phoenix_write_reg(priv->mmio, R_TX_CONTROL, regval);
+
+		regval = phoenix_read_reg(priv->mmio, R_RX_CONTROL);
+		regval &= ~(1 << O_RX_CONTROL__RxEnable);
+		phoenix_write_reg(priv->mmio, R_RX_CONTROL, regval);
+	}
+}
+
+static __inline__ int phnx_mac_send_fr(struct driver_data *priv,
+									   unsigned long addr, int len)
+{
+	int stid = 0;
+	struct msgrng_msg msg;
+
+	stid = mac_make_desc_b0_rfr(&msg, priv->instance, priv->type, addr);
+
+	/* Send the packet to MAC */
+	dbg_msg("mac_%d: Sending free packet to stid %d\n", priv->instance, stid);
+	__sync();
+	if (priv->type == TYPE_XGMAC) {
+		while (message_send(1, MSGRNG_CODE_XGMAC, stid, &msg));
+	}
+	else {
+		while (message_send(1, MSGRNG_CODE_MAC, stid, &msg));
+	}
+
+	//phnx_inc_counter(NETIF_REG_FRIN);
+
+	/* Let the mac keep the free descriptor */
+	return 0;
+}
+
+static void xgmac_mdio_setup(volatile unsigned int *_mmio)
+{
+	int i;
+	uint32_t rd_data;
+	for (i = 0; i < 4; i++) {
+		rd_data = xmdio_read(_mmio, 1, 0x8000 + i);
+		rd_data = rd_data & 0xffffdfff;	// clear isolate bit
+		xmdio_write(_mmio, 1, 0x8000 + i, rd_data);
+	}
+}
+
+/**********************************************************************
+ *  Init MII interface
+ *  
+ *  Input parameters: 
+ *  	   s - priv structure
+ ********************************************************************* */
+#define PHY_STATUS_RETRIES 25000
+
+static void rmi_phnx_mac_mii_init(struct driver_data *priv)
+{
+	phoenix_reg_t *mmio;
+
+#ifdef XLS
+	if (xls_rgmii_mode(priv) && !xlr_board_atx_vii())
+		mmio = phoenix_io_mmio(PHOENIX_IO_GMAC_4_OFFSET);
+	else
+#endif
+		mmio = phoenix_io_mmio(PHOENIX_IO_GMAC_0_OFFSET);
+
+	/* use the lowest clock divisor - divisor 28 */
+	phoenix_write_reg(mmio, R_MII_MGMT_CONFIG, 0x07);
+}
+
+/**********************************************************************
+ *  Read a PHY register.
+ *  
+ *  Input parameters: 
+ *  	   s - priv structure
+ *  	   phyaddr - PHY's address
+ *  	   regidx = index of register to read
+ *  	   
+ *  Return value:
+ *  	   value read (16 bits), or 0xffffffff if an error occurred.
+ ********************************************************************* */
+static unsigned int rmi_phnx_mac_mii_read(struct driver_data *priv,
+										  int regidx, int rgmii)
+{
+	int i = 0;
+	phoenix_reg_t *mmio;
+	int phyaddr;
+
+	if (rgmii) {
+		phyaddr = 0;
+#ifdef XLS
+		if (!xlr_board_atx_vii())
+			mmio = phoenix_io_mmio(PHOENIX_IO_GMAC_4_OFFSET);
+		else
+#endif
+			mmio = phoenix_io_mmio(PHOENIX_IO_GMAC_0_OFFSET);
+	}
+	else {
+		phyaddr = priv->phy_addr;
+#ifdef XLS
+		if (xls_rgmii_mode(priv) && !xlr_board_atx_vii())
+			mmio = phoenix_io_mmio(PHOENIX_IO_GMAC_4_OFFSET);
+		else
+#endif
+			mmio = phoenix_io_mmio(PHOENIX_IO_GMAC_0_OFFSET);
+	}
+
+	/* setup the phy reg to be used */
+	phoenix_write_reg(mmio, R_MII_MGMT_ADDRESS,
+					  (phyaddr << 8) | (regidx << 0));
+
+	/* Issue the read command */
+	phoenix_write_reg(mmio, R_MII_MGMT_COMMAND,
+					  (1 << O_MII_MGMT_COMMAND__rstat));
+
+	/* poll for the read cycle to complete */
+	for (i = 0; i < PHY_STATUS_RETRIES; i++) {
+		if (phoenix_read_reg(mmio, R_MII_MGMT_INDICATORS) == 0)
+			break;
+	}
+
+	/* clear the read cycle */
+	phoenix_write_reg(mmio, R_MII_MGMT_COMMAND, 0);
+
+	if (i == PHY_STATUS_RETRIES) {
+		return 0xffffffff;
+	}
+
+	/* Read the data back */
+	return phoenix_read_reg(mmio, R_MII_MGMT_STATUS);
+}
+
+/**********************************************************************
+ *  Write a value to a PHY register.
+ *  
+ *  Input parameters: 
+ *  	   s - priv structure
+ *  	   phyaddr - PHY to use
+ *  	   regidx - register within the PHY
+ *  	   regval - data to write to register
+ *  	   
+ *  Return value:
+ *  	   nothing
+ ********************************************************************* */
+static void rmi_phnx_mac_mii_write(struct driver_data *priv, int regidx,
+								   unsigned int regval, int internal)
+{
+	int i = 0;
+	phoenix_reg_t *mmio;
+	int phyaddr = 0;
+
+	if (internal) {
+		phyaddr = (priv->instance & 0x3) + 27;
+#ifdef XLS
+		if ((priv->instance & 0x4) && !xlr_board_atx_vii())
+			mmio = phoenix_io_mmio(PHOENIX_IO_GMAC_4_OFFSET);
+		else
+#endif
+			mmio = phoenix_io_mmio(PHOENIX_IO_GMAC_0_OFFSET);
+	}
+	else {
+		phyaddr = priv->phy_addr;
+#ifdef XLS
+		if (xls_rgmii_mode(priv) && !xlr_board_atx_vii())
+			mmio = phoenix_io_mmio(PHOENIX_IO_GMAC_4_OFFSET);
+		else
+#endif
+			mmio = phoenix_io_mmio(PHOENIX_IO_GMAC_0_OFFSET);
+	}
+
+	phoenix_write_reg(mmio, R_MII_MGMT_ADDRESS,
+					  (phyaddr << 8) | (regidx << 0));
+
+	/* Write the data which starts the write cycle */
+	phoenix_write_reg(mmio, R_MII_MGMT_WRITE_DATA, regval);
+
+	/* poll for the write cycle to complete */
+	for (i = 0; i < PHY_STATUS_RETRIES; i++) {
+		if (phoenix_read_reg(mmio, R_MII_MGMT_INDICATORS) == 0)
+			break;
+	}
+
+	return;
+}
+
+static void rmi_phnx_gmac_clr_pending_intr(struct driver_data *phy_priv)
+{
+	phoenix_reg_t *mmio = NULL;
+
+	if (!xlr_board_atx_vii())
+		return;
+
+	if (phy_priv->instance == 0) {
+		/*All MDIO interrupts goes to mdio 0 - ack mac 0 */
+		mmio = phy_priv->mmio;
+		phoenix_write_reg(mmio, R_INTREG, 0xffffffff);
+	}
+}
+
+static void rmi_phnx_gmac_config_speed(struct driver_data *priv)
+{
+	phoenix_reg_t *mmio = priv->mmio;
+	int id = priv->instance;
+
+	priv->speed = rmi_phnx_mac_mii_read(priv, 28, 0);
+	priv->speed = (priv->speed >> 3) & 0x03;
+
+	if (priv->speed == phnx_mac_speed_10) {
+		if (is_xls())
+			phoenix_write_reg(mmio, R_INTERFACE_CONTROL, SGMII_SPEED_10);
+		phoenix_write_reg(mmio, R_MAC_CONFIG_2, 0x7137);
+		phoenix_write_reg(mmio, R_CORECONTROL, 0x02);
+		printk("configuring gmac_%d in 10Mbps mode\n", id);
+	}
+	else if (priv->speed == phnx_mac_speed_100) {
+		if (is_xls())
+			phoenix_write_reg(mmio, R_INTERFACE_CONTROL, SGMII_SPEED_100);
+		phoenix_write_reg(mmio, R_MAC_CONFIG_2, 0x7137);
+		phoenix_write_reg(mmio, R_CORECONTROL, 0x01);
+		printk("configuring gmac_%d in 100Mbps mode\n", id);
+	}
+	else {
+		if (is_xls())
+			phoenix_write_reg(mmio, R_INTERFACE_CONTROL, SGMII_SPEED_1000);
+		if (priv->speed != phnx_mac_speed_1000) {
+			printk
+				("phy reported unknown mac speed, defaulting to 100Mbps mode\n");
+			phoenix_write_reg(mmio, R_MAC_CONFIG_2, 0x7137);
+			phoenix_write_reg(mmio, R_CORECONTROL, 0x01);
+		}
+		phoenix_write_reg(mmio, R_MAC_CONFIG_2, 0x7237);
+		phoenix_write_reg(mmio, R_CORECONTROL, 0x00);
+		printk("configuring gmac_%d in 1000Mbps mode\n", id);
+	}
+}
+
+/*****************************************************************
+ * Initialize XGMAC
+ *****************************************************************/
+static void rmi_phnx_xgmac_init(struct driver_data *priv)
+{
+	int i = 0;
+	phoenix_reg_t *mmio = priv->mmio;
+	volatile unsigned short *cpld;
+	char buckets[MAX_NUM_XGMAC_STNS], credits[MAX_NUM_MSGRNG_STN_CC];
+
+	cpld = (volatile unsigned short *) (unsigned long) 0xffffffffBD840000ULL;
+	phoenix_write_reg(priv->mmio, R_DESC_PACK_CTRL,
+					  (MAX_FRAME_SIZE << O_DESC_PACK_CTRL__RegularSize) | (4
+																		   <<
+																		   20));
+	phoenix_write_reg(priv->mmio, R_BYTEOFFSET0, BYTE_OFFSET);
+
+	phoenix_write_reg(priv->mmio, R_MSG_TX_THRESHOLD, 1);
+
+	/* configure the XGMAC Registers */
+	phoenix_write_reg(mmio, R_XGMAC_CONFIG_1, 0x50000026);
+
+	/* configure the XGMAC_GLUE Registers */
+	phoenix_write_reg(mmio, R_DMACR0, 0xffffffff);
+	phoenix_write_reg(mmio, R_DMACR1, 0xffffffff);
+	phoenix_write_reg(mmio, R_DMACR2, 0xffffffff);
+	phoenix_write_reg(mmio, R_DMACR3, 0xffffffff);
+	phoenix_write_reg(mmio, R_STATCTRL, 0x04);
+	phoenix_write_reg(mmio, R_L2ALLOCCTRL, 0xffffffff);
+
+	phoenix_write_reg(mmio, R_XGMACPADCALIBRATION, 0x030);
+	phoenix_write_reg(mmio, R_EGRESSFIFOCARVINGSLOTS, 0x0f);
+	phoenix_write_reg(mmio, R_L2ALLOCCTRL, 0xffffffff);
+	phoenix_write_reg(mmio, R_XGMAC_MIIM_CONFIG, 0x3e);
+
+	/* take XGMII phy out of reset 
+	 */
+	/* we are pulling everything out of reset because writing a 0 would
+	 * reset other devices on the chip
+	 */
+	cpld[ATX_CPLD_RESET_1] = 0xffff;
+	cpld[ATX_CPLD_MISC_CTRL] = 0xffff;
+	cpld[ATX_CPLD_RESET_2] = 0xffff;
+
+	xgmac_mdio_setup(mmio);
+
+	rmi_phnx_config_spill_area(priv);
+	rmi_phnx_mac_set_enable(priv, 1);
+
+	/* each xgmac is a seperate station */
+	if (fdt_get_xgmac_bucket_conf
+		(priv->instance, buckets, MAX_NUM_XGMAC_STNS, credits,
+		 MAX_NUM_MSGRNG_STN_CC) < 0)
+		printk("********Error in bucket and credit get \n");
+
+	for (i = 0; i < MAX_NUM_XGMAC_STNS; i++)
+		phoenix_write_reg(mmio, R_XGS_TX0_BUCKET_SIZE + i, buckets[i]);
+
+	for (i = 0; i < MAX_NUM_MSGRNG_STN_CC; i++)
+		phoenix_write_reg(mmio, R_CC_CPU0_0 + i, credits[i]);
+
+
+	priv->init_frin_desc = 1;
+}
+
+static void serdes_regs_init(void)
+{
+	int i;
+	volatile unsigned int *mmio_gpio;
+	mmio_gpio = (unsigned int *) (phoenix_io_base + PHOENIX_IO_GPIO_OFFSET);
+	/*
+	   P Reg   Val
+	   -------------
+	   26 0     6DB0
+	   26 1     0FFF
+	   26 2     B6D0
+	   26 3     00FF
+	   26 4     0000
+	   26 5     0000
+	   26 6     0005
+	   26 7     0001
+	   26 8     0000
+	   26 9     0000
+	   26 10    0000
+	 */
+	arizona_gmac_phy_write(PHOENIX_IO_GMAC_0_OFFSET, 26, 0, 0x6DB0);
+	arizona_gmac_phy_write(PHOENIX_IO_GMAC_0_OFFSET, 26, 1, 0xFFFF);
+	arizona_gmac_phy_write(PHOENIX_IO_GMAC_0_OFFSET, 26, 2, 0xB6D0);
+	arizona_gmac_phy_write(PHOENIX_IO_GMAC_0_OFFSET, 26, 3, 0x00FF);
+	arizona_gmac_phy_write(PHOENIX_IO_GMAC_0_OFFSET, 26, 4, 0x0000);
+	arizona_gmac_phy_write(PHOENIX_IO_GMAC_0_OFFSET, 26, 5, 0x0000);
+	arizona_gmac_phy_write(PHOENIX_IO_GMAC_0_OFFSET, 26, 6, 0x0005);
+	arizona_gmac_phy_write(PHOENIX_IO_GMAC_0_OFFSET, 26, 7, 0x0001);
+	arizona_gmac_phy_write(PHOENIX_IO_GMAC_0_OFFSET, 26, 8, 0x0000);
+	arizona_gmac_phy_write(PHOENIX_IO_GMAC_0_OFFSET, 26, 9, 0x0000);
+	arizona_gmac_phy_write(PHOENIX_IO_GMAC_0_OFFSET, 26, 10, 0x0000);
+
+#ifdef XLS
+	if ((is_xls6xx()) || (is_xls4xx())) {
+		arizona_gmac_phy_write(PHOENIX_IO_GMAC_4_OFFSET, 26, 0, 0x6DB0);
+		arizona_gmac_phy_write(PHOENIX_IO_GMAC_4_OFFSET, 26, 1, 0xFFFF);
+		arizona_gmac_phy_write(PHOENIX_IO_GMAC_4_OFFSET, 26, 2, 0xB6D0);
+		arizona_gmac_phy_write(PHOENIX_IO_GMAC_4_OFFSET, 26, 3, 0x00FF);
+		arizona_gmac_phy_write(PHOENIX_IO_GMAC_4_OFFSET, 26, 4, 0x0000);
+		arizona_gmac_phy_write(PHOENIX_IO_GMAC_4_OFFSET, 26, 5, 0x0000);
+		arizona_gmac_phy_write(PHOENIX_IO_GMAC_4_OFFSET, 26, 6, 0x0005);
+		arizona_gmac_phy_write(PHOENIX_IO_GMAC_4_OFFSET, 26, 7, 0x0001);
+		arizona_gmac_phy_write(PHOENIX_IO_GMAC_4_OFFSET, 26, 8, 0x0000);
+		arizona_gmac_phy_write(PHOENIX_IO_GMAC_4_OFFSET, 26, 9, 0x0000);
+		arizona_gmac_phy_write(PHOENIX_IO_GMAC_4_OFFSET, 26, 10, 0x0000);
+	}
+#endif
+
+	for (i = 0; i < 10000000; i++) {
+	}
+
+	/* program  GPIO values for serdes init parameters */
+	mmio_gpio[0x20] = 0x7e6802;
+	mmio_gpio[0x10] = 0x7104;
+	for (i = 0; i < 100000000; i++) {
+	}
+}
+
+static void serdes_autoconfig(void)
+{
+	int delay = 100;
+
+	/* Enable Auto negotiation in the PCS Layer */
+	arizona_gmac_phy_write(PHOENIX_IO_GMAC_0_OFFSET, 27, 0, 0x1000);
+	mdelay(delay);
+	arizona_gmac_phy_write(PHOENIX_IO_GMAC_0_OFFSET, 27, 0, 0x0200);
+	mdelay(delay);
+
+	arizona_gmac_phy_write(PHOENIX_IO_GMAC_0_OFFSET, 28, 0, 0x1000);
+	mdelay(delay);
+	arizona_gmac_phy_write(PHOENIX_IO_GMAC_0_OFFSET, 28, 0, 0x0200);
+	mdelay(delay);
+
+	arizona_gmac_phy_write(PHOENIX_IO_GMAC_0_OFFSET, 29, 0, 0x1000);
+	mdelay(delay);
+	arizona_gmac_phy_write(PHOENIX_IO_GMAC_0_OFFSET, 29, 0, 0x0200);
+	mdelay(delay);
+
+	arizona_gmac_phy_write(PHOENIX_IO_GMAC_0_OFFSET, 30, 0, 0x1000);
+	mdelay(delay);
+	arizona_gmac_phy_write(PHOENIX_IO_GMAC_0_OFFSET, 30, 0, 0x0200);
+	mdelay(delay);
+
+#ifdef XLS
+	if ((is_xls6xx()) || (is_xls4xx())) {
+		/* Enable Auto negotiation in the PCS Layer */
+
+		arizona_gmac_phy_write(PHOENIX_IO_GMAC_4_OFFSET, 27, 0, 0x1000);
+		mdelay(delay);
+		arizona_gmac_phy_write(PHOENIX_IO_GMAC_4_OFFSET, 27, 0, 0x0200);
+		mdelay(delay);
+
+		arizona_gmac_phy_write(PHOENIX_IO_GMAC_4_OFFSET, 28, 0, 0x1000);
+		mdelay(delay);
+		arizona_gmac_phy_write(PHOENIX_IO_GMAC_4_OFFSET, 28, 0, 0x0200);
+		mdelay(delay);
+
+		arizona_gmac_phy_write(PHOENIX_IO_GMAC_4_OFFSET, 29, 0, 0x1000);
+		mdelay(delay);
+		arizona_gmac_phy_write(PHOENIX_IO_GMAC_4_OFFSET, 29, 0, 0x0200);
+		mdelay(delay);
+
+		arizona_gmac_phy_write(PHOENIX_IO_GMAC_4_OFFSET, 30, 0, 0x1000);
+		mdelay(delay);
+		arizona_gmac_phy_write(PHOENIX_IO_GMAC_4_OFFSET, 30, 0, 0x0200);
+		mdelay(delay);
+
+	}
+#endif
+
+	return;
+}
+
+static void rmi_phnx_gmac_init(struct driver_data *priv)
+{
+	int i = 0;
+	phoenix_reg_t *mmio = priv->mmio;
+	__u32 value = 0;
+	char buckets[MAX_NUM_GMAC_STNS], credits[MAX_NUM_MSGRNG_STN_CC];
+
+	rmi_phnx_config_spill_area(priv);
+
+	phoenix_write_reg(priv->mmio, R_DESC_PACK_CTRL,
+					  (BYTE_OFFSET << O_DESC_PACK_CTRL__ByteOffset) |
+					  (1 << O_DESC_PACK_CTRL__MaxEntry) |
+					  (MAX_FRAME_SIZE << O_DESC_PACK_CTRL__RegularSize));
+
+	phoenix_write_reg(priv->mmio, R_MSG_TX_THRESHOLD, 3);
+
+	phoenix_write_reg(mmio, R_MAC_CONFIG_1, 0x35);
+
+	if (xls_rgmii_mode(priv)) {
+		value = phoenix_read_reg(priv->mmio, R_RX_CONTROL);
+		value |= 1 << O_RX_CONTROL__RGMII;
+		phoenix_write_reg(priv->mmio, R_RX_CONTROL, value);
+	}
+
+	rmi_phnx_mac_mii_init(priv);
+
+	priv->advertising = ADVERTISED_10baseT_Full | ADVERTISED_10baseT_Half |
+		ADVERTISED_100baseT_Full | ADVERTISED_100baseT_Half |
+		ADVERTISED_1000baseT_Full | ADVERTISED_Autoneg | ADVERTISED_MII;
+
+	/*Clear pending mdio interrupt */
+	rmi_phnx_gmac_clr_pending_intr(priv);
+
+	/* Enable all MDIO interrupts in the phy 
+	 * RX_ER bit seems to be get set about every 1 sec in GigE mode,
+	 * ignore it for now...
+	 */
+	rmi_phnx_mac_mii_write(priv, 25, 0xfffffffe, 0);
+
+	if (is_xls()) {
+		rmi_phnx_mac_mii_write(priv, 0, 0x1000, 1);
+		mdelay(100);
+		rmi_phnx_mac_mii_write(priv, 0, 0x0200, 1);
+		mdelay(100);
+	}
+
+	if ((is_xls())) {
+		serdes_regs_init();
+		mdelay(100);
+		serdes_autoconfig();
+		mdelay(2000);
+		mdelay(2000);
+	}
+
+	rmi_phnx_gmac_config_speed(priv);
+
+	value = phoenix_read_reg(mmio, R_IPG_IFG);
+	phoenix_write_reg(mmio, R_IPG_IFG, ((value & ~0x7f) | MAC_B2B_IPG));
+	phoenix_write_reg(mmio, R_DMACR0, 0xffffffff);
+	phoenix_write_reg(mmio, R_DMACR1, 0xffffffff);
+	phoenix_write_reg(mmio, R_DMACR2, 0xffffffff);
+	phoenix_write_reg(mmio, R_DMACR3, 0xffffffff);
+	phoenix_write_reg(mmio, R_STATCTRL, 0x04);
+	phoenix_write_reg(mmio, R_L2ALLOCCTRL, 0xffffffff);
+	phoenix_write_reg(mmio, R_INTMASK, 0);
+	phoenix_write_reg(mmio, R_FREEQCARVE, 0);
+
+
+	if (fdt_get_gmac_bucket_conf
+		(priv->id / 4, buckets, MAX_NUM_GMAC_STNS, credits,
+		 MAX_NUM_MSGRNG_STN_CC) < 0)
+		printk("********Error in bucket and credit get \n");
+
+	for (i = 0; i < MAX_NUM_GMAC_STNS; i++)
+		phoenix_write_reg(mmio, R_GMAC_JFR0_BUCKET_SIZE + i, buckets[i]);
+
+	for (i = 0; i < MAX_NUM_MSGRNG_STN_CC; i++)
+		phoenix_write_reg(mmio, R_CC_CPU0_0 + i, credits[i]);
+
+
+	priv->init_frin_desc = 1;
+	return;
+
+}
+
+/**********************************************************************
+ * Set promiscuous mode
+ **********************************************************************/
+static void phnx_mac_set_rx_mode(struct net_device *dev)
+{
+	struct driver_data *priv = netdev_priv(dev);
+	uint32_t regval;
+
+	regval = phoenix_read_reg(priv->mmio, R_MAC_FILTER_CONFIG);
+
+	if (dev->flags & IFF_PROMISC) {
+		regval |= (1 << O_MAC_FILTER_CONFIG__BROADCAST_EN) |
+			(1 << O_MAC_FILTER_CONFIG__PAUSE_FRAME_EN) |
+			(1 << O_MAC_FILTER_CONFIG__ALL_MCAST_EN) |
+			(1 << O_MAC_FILTER_CONFIG__ALL_UCAST_EN);
+	}
+	else {
+		regval &= ~((1 << O_MAC_FILTER_CONFIG__PAUSE_FRAME_EN) |
+					(1 << O_MAC_FILTER_CONFIG__ALL_UCAST_EN));
+	}
+
+	phoenix_write_reg(priv->mmio, R_MAC_FILTER_CONFIG, regval);
+}
+
+/**********************************************************************
+ *  Configure LAN speed for the specified MAC.
+ ********************************************************************* */
+static int rmi_phnx_mac_set_speed(struct driver_data *s,
+								  phnx_mac_speed_t speed)
+{
+	return 0;
+}
+
+/**********************************************************************
+ *  Set Ethernet duplex and flow control options for this MAC
+ ********************************************************************* */
+static int rmi_phnx_mac_set_duplex(struct driver_data *s,
+								   phnx_mac_duplex_t duplex, phnx_mac_fc_t fc)
+{
+	return 0;
+}
+
+/*****************************************************************
+ * Kernel Net Stack <-> MAC Driver Interface
+ *****************************************************************/
+/**********************************************************************
+ **********************************************************************/
+#define MAC_TX_FAIL 1
+#define MAC_TX_PASS 0
+#define MAC_TX_RETRY 1
+
+static inline int phnx_netif_queue_tx(struct net_device *dev, int result)
+{
+	unsigned int flags;
+	int port = ((struct driver_data *) netdev_priv(dev))->instance;
+	int ret = 0;
+
+	spin_lock_irqsave(&pending_tx_lock[port], flags);
+	if (result == MAC_TX_PASS) {
+		pending_tx[port]++;
+		ret = MAC_TX_PASS;
+	}
+	else {
+		if (pending_tx[port] > 16) {
+			netif_stop_queue(dev);
+			ret = MAC_TX_FAIL;
+		}
+		else {
+			ret = MAC_TX_RETRY;
+		}
+	}
+	spin_unlock_irqrestore(&pending_tx_lock[port], flags);
+	return ret;
+}
+
+static inline void phnx_netif_queue_tx_complete(struct net_device *dev)
+{
+	int port = ((struct driver_data *) netdev_priv(dev))->instance;
+
+	spin_lock(&pending_tx_lock[port]);
+	pending_tx[port]--;
+	netif_wake_queue(dev);
+	spin_unlock(&pending_tx_lock[port]);
+}
+
+extern int own_bucket_start[], own_bucket_mask[];
+
+static inline int mac_make_desc_b0_tx(struct msgrng_msg *msg, int id,
+									  int type, unsigned long addr, int len,
+									  struct sk_buff *skb)
+{
+	int tx_stid = 0;
+	int fr_stid = 0;
+	int cpu = phoenix_cpu_id();
+
+	/* free stationid should be one of the bucket owned by linux */
+	fr_stid = own_bucket_start[cpu];
+
+	if (type == TYPE_XGMAC) {
+		tx_stid = msgrng_xgmac_stid_tx(id);
+		/* fr_stid = (cpu << 3) + phoenix_thr_id() + 4; */
+	}
+	else {
+
+		tx_stid = msgrng_gmac_stid_tx(id);
+
+		/* In case of loader support and linux owning gmacs on
+		 * gmacs on shared core, use same bucket for fr_stid.
+
+		 if (own_gmac_only)
+		 fr_stid = (cpu << 3) + phoenix_thr_id();
+		 else
+		 fr_stid = (cpu << 3) + phoenix_thr_id() + 4; */
+	}
+
+	msg->msg0 = (((uint64_t) 1 << 63) |
+				 (((uint64_t) 127) << 54) |
+				 ((uint64_t) len << 40) | ((uint64_t) addr & 0xffffffffffULL)
+		);
+	msg->msg1 = (((uint64_t) 1 << 63) |
+				 ((uint64_t) fr_stid << 54) | ((uint64_t) 0 << 40) |
+#ifdef CONFIG_64BIT
+				 ((uint64_t) virt_to_phys(skb) & 0xffffffffffULL)
+#else
+				 ((unsigned long) (skb) & 0xffffffffUL)
+#endif
+		);
+
+	msg->msg2 = msg->msg3 = 0;
+
+	return tx_stid;
+}
+
+static __inline__ void message_send_block(unsigned int size,
+										  unsigned int code,
+										  unsigned int stid,
+										  struct msgrng_msg *msg)
+{
+	unsigned int dest = 0;
+	unsigned long long status = 0;
+
+	msgrng_load_tx_msg0(msg->msg0);
+	msgrng_load_tx_msg1(msg->msg1);
+	msgrng_load_tx_msg2(msg->msg2);
+	msgrng_load_tx_msg3(msg->msg3);
+
+	dest = ((size - 1) << 16) | (code << 8) | (stid);
+
+	for (;;) {
+
+		msgrng_send(dest);
+
+		status = msgrng_read_status();
+
+		if (status & 0x6) {
+			continue;
+		}
+		else
+			break;
+	}
+
+}
+
+static int mac_xmit(struct sk_buff *skb, struct net_device *dev,
+					struct driver_data *priv, int txq)
+{
+	struct msgrng_msg msg;
+	int stid = 0;
+
+	if (own_bucket_mask[phoenix_cpu_id()] == 0x0) {
+		printk("No buckets are owned by this core\n");
+		return MAC_TX_FAIL;
+	}
+
+	stid = mac_make_desc_b0_tx(&msg, priv->instance, priv->type,
+							   virt_to_phys(skb->data), skb->len, skb);
+
+#ifdef CONFIG_PHOENIX_HW_BUFFER_MGMT
+	if (xlr_napi)
+		setup_auto_free(skb, priv->type, &msg);
+#endif /* CONFIG_PHOENIX_HW_BUFFER_MGMT */
+
+	__sync();
+
+	if (message_send_retry(2, MSGRNG_CODE_MAC, stid, &msg))
+		return MAC_TX_FAIL;
+
+	port_inc_counter(priv->instance, PORT_TX);
+
+#ifdef DUMP_PACKETS
+	/* Send the packet to MAC */
+	printk("Sent tx packet to stid %d, msg0=%llx, msg1=%llx \n", stid,
+		   msg.msg0, msg.msg1);
+
+	{
+		int i = 0;
+		printk("Tx Packet: length=%d\n", skb->len);
+		for (i = 0; i < 64; i++) {
+			printk("%02x ", skb->data[i]);
+			if (i && (i % 16) == 0)
+				printk("\n");
+		}
+		printk("\n");
+	}
+#endif
+
+	phnx_inc_counter(NETIF_TX);
+
+	dev->trans_start = jiffies;
+
+	return MAC_TX_PASS;
+}
+
+#define MSGRING_PROCESS_FROUT_START_BUCKET 4
+#define MSGRING_PROCESS_FROUT_END_BUCKET 8
+#define MSGRING_PROCESS_FROUT_POP_BUCKET_MASK 0xf0
+extern void msgring_process_rx_msgs(int, int, __u32);
+
+static int rmi_phnx_mac_xmit(struct sk_buff *skb, struct net_device *dev)
+{
+	struct driver_data *priv = netdev_priv(dev);
+	int ret = -ENOSPC;
+	unsigned long mflags = 0;
+	int txq = hard_smp_processor_id();
+
+	dbg_msg("IN\n");
+	phnx_inc_counter(NETIF_STACK_TX);
+
+	if (xlr_mac_optimized_tx) {
+		irq_enter();
+		msgrng_flags_save(mflags);
+		for (;;) {
+			/* first drain all the pending messages */
+#ifdef CONFIG_PHOENIX_GMAC_NAPI
+			if (!xlr_napi) {
+				msgring_process_rx_msgs(MSGRING_PROCESS_FROUT_START_BUCKET,
+										MSGRING_PROCESS_FROUT_END_BUCKET,
+										MSGRING_PROCESS_FROUT_POP_BUCKET_MASK);
+			}
+#else
+			msgring_process_rx_msgs(MSGRING_PROCESS_FROUT_START_BUCKET,
+									MSGRING_PROCESS_FROUT_END_BUCKET,
+									MSGRING_PROCESS_FROUT_POP_BUCKET_MASK);
+#endif /* CONFIG_PHOENIX_GMAC_NAPI */
+
+			ret = mac_xmit(skb, dev, priv, txq);
+			if (ret == MAC_TX_PASS)
+				break;
+		}
+		mac_stats_add(priv->cpu_stats[txq].tx_packets, 1);
+
+		msgrng_flags_restore(mflags);
+		irq_exit();
+	}
+	else {
+	  retry:
+		msgrng_access_enable(mflags);
+
+		ret = mac_xmit(skb, dev, priv, txq);
+
+		mac_stats_add(priv->cpu_stats[txq].tx_packets, 1);
+
+		msgrng_access_disable(mflags);
+
+		ret = phnx_netif_queue_tx(dev, ret);
+		if (ret == MAC_TX_RETRY)
+			goto retry;
+
+		dbg_msg("OUT, ret = %d, mflags=%lx\n", ret, mflags);
+		if (ret == MAC_TX_FAIL) {
+			/* FULL */
+			dbg_msg("Msg Ring Full. Stopping upper layer Q\n");
+			port_inc_counter(priv->instance, PORT_STOPQ);
+		}
+	}
+
+	return ret;
+}
+
+void mac_frin_skb_replenish(struct driver_data *priv)
+{
+	int offset = 0;
+	unsigned long msgrng_flags;
+	struct sk_buff *skb = 0;
+	__u32 cycles;
+
+	cycles = read_c0_count();
+	{
+		skb = __dev_alloc_skb(PHNX_RX_BUF_SIZE,
+							  GFP_ATOMIC | __GFP_REPEAT | __GFP_NOWARN);
+		if (!skb) {
+			skb = __dev_alloc_skb(PHNX_RX_BUF_SIZE, GFP_KERNEL);
+			if (!skb)
+				panic("[%s]: Unable to allocate skb!\n", __func__);
+		}
+	}
+	phnx_inc_counter(REPLENISH_FRIN);
+
+	/* align the data to the next cache line */
+	offset = (((unsigned long) skb->data + SMP_CACHE_BYTES) &
+			  ~(SMP_CACHE_BYTES - 1));
+	skb_reserve(skb, (offset - (unsigned long) skb->data));
+
+	//skb->dev = dev;
+
+	msgrng_access_enable(msgrng_flags);
+	mac_put_skb_back_ptr(skb);
+
+	if (phnx_mac_send_fr(priv, virt_to_bus(skb->data), skb->len)) {
+		dev_kfree_skb(skb);
+		printk("[%s]: rx free message_send failed!\n", __func__);
+		return;
+	}
+	msgrng_access_disable(msgrng_flags);
+	return;
+}
+
+
+static void mac_frin_replenish(struct work_struct *args /* ignored */ )
+{
+	int cpu = hard_smp_processor_id();
+	int done = 0;
+	int i = 0;
+	unsigned long msgrng_flags;
+
+	__u32 cycles = read_c0_count();
+
+	phnx_inc_counter(REPLENISH_ENTER);
+	//phnx_set_counter(REPLENISH_ENTER_COUNT, atomic_read(frin_to_be_sent));
+	phnx_set_counter(REPLENISH_CPU, hard_smp_processor_id());
+
+	for (;;) {
+
+		done = 0;
+
+		for (i = 0; i < PHOENIX_MAX_MACS; i++) {
+			struct net_device *dev;
+			struct driver_data *priv;
+			atomic_t *frin_to_be_sent;
+			void *phys_addr;
+
+			dev = dev_mac[i];
+			if (!dev)
+				goto skip;
+
+			priv = netdev_priv(dev);
+			if (priv->own == 0)
+				continue;
+
+			frin_to_be_sent = &priv->frin_to_be_sent[cpu];
+
+			if (atomic_read(frin_to_be_sent) < 0) {
+				panic
+					("BUG?: [%s]: gmac_%d illegal value for frin_to_be_sent=%d\n",
+					 __func__, i, atomic_read(frin_to_be_sent));
+			}
+
+			if (!atomic_read(frin_to_be_sent))
+				goto skip;
+
+			if (priv->own & PHNX_GMAC_SHARED) {
+				phys_addr = rmi_frin_mem_alloc(PHNX_RX_BUF_SIZE);
+				if (phys_addr == NULL) {
+					phys_addr =
+						cacheline_aligned_kmalloc(PHNX_RX_BUF_SIZE,
+												  GFP_KERNEL);
+					if (phys_addr == NULL)
+						panic("Out of memory\n");
+				}
+				msgrng_access_enable(msgrng_flags);
+				if (phnx_mac_send_fr
+					(priv, virt_to_bus(phys_addr), PHNX_RX_BUF_SIZE)) {
+					printk("[%s]: rx free message_send failed!\n",
+						   __func__);
+					return;
+				}
+				msgrng_access_disable(msgrng_flags);
+
+			}
+			else
+				mac_frin_skb_replenish(priv);
+
+			phnx_set_counter(REPLENISH_CYCLES, (read_c0_count() - cycles));
+
+			atomic_dec(frin_to_be_sent);
+
+			continue;
+		  skip:
+			done++;
+		}
+		if (done == PHOENIX_MAX_MACS)
+			break;
+	}
+}
+
+/*
+ * Send a packet back to ipsec rmios
+ */
+static void drop_message(int fbid, struct msgrng_msg *msg)
+{
+	u32 addr;
+	addr = (unsigned long) (msg->msg0 & 0xffffffffe0ULL);
+	msg->msg0 = ((u64) CTRL_REG_FREE << 61) | ((u64) fbid << 52) | (u64) addr;
+	msg->msg1 = msg->msg2 = msg->msg3 = 0;
+	while (message_send(1, MSGRNG_CODE_MAC, fbid, msg));
+}
+
+void rmi_phnx_free_skb(struct msgrng_msg *msg)
+{
+	struct sk_buff *skb;
+	struct driver_data *priv;
+	int cpu = hard_smp_processor_id();
+#ifdef CONFIG_64BIT
+	unsigned long tmp;
+	tmp = (unsigned long) (msg->msg0 & 0xffffffffffULL);
+	skb = (struct sk_buff *) phys_to_virt(tmp);
+#else
+	skb = (struct sk_buff *) (unsigned long) msg->msg0;
+#endif
+	/* Tx Complete */
+	phnx_inc_counter(NETIF_TX_COMPLETE);
+
+	dbg_msg("skb = %p\n", skb);
+	/* release the skb and update statistics outside the spinlock */
+	priv = netdev_priv(skb->dev);
+	mac_stats_add(priv->stats.tx_packets, 1);
+	mac_stats_add(priv->stats.tx_bytes, skb->len);
+	mac_stats_add(priv->cpu_stats[cpu].txc_packets, 1);
+	dev_kfree_skb_irq(skb);
+
+	if (!xlr_mac_optimized_tx) {
+		port_inc_counter(priv->instance, PORT_TX_COMPLETE);
+		phnx_netif_queue_tx_complete(skb->dev);
+	}
+
+	phnx_set_counter(NETIF_TX_COMPLETE_CYCLES,
+					 (read_c0_count() - msgrng_msg_cycles));
+}
+
+struct rmi_phnx_msg_codes
+{
+	int code;
+	int stid;
+	int *dev_mac;
+	int offset;					/* offset from dev_mac value */
+	int fbstid;
+};
+
+struct rmi_phnx_msg_codes msg_codes[] =
+	{ {SEND_TO_GMAC0__FDISC_TO_GMAC, MSGRNG_STNID_GMAC0,
+	   (int *) (long) &dev_mac_gmac0, 0, MSGRNG_STNID_GMAC0_FR},
+{SEND_TO_GMAC1__FDISC_TO_GMAC, MSGRNG_STNID_GMAC0,
+ (int *) (long) &dev_mac_gmac0, 1, MSGRNG_STNID_GMAC0_FR},
+{SEND_TO_GMAC2__FDISC_TO_GMAC, MSGRNG_STNID_GMAC0,
+ (int *) (long) &dev_mac_gmac0, 2, MSGRNG_STNID_GMAC0_FR},
+{SEND_TO_GMAC3__FDISC_TO_GMAC, MSGRNG_STNID_GMAC0,
+ (int *) (long) &dev_mac_gmac0, 3, MSGRNG_STNID_GMAC0_FR},
+{SEND_TO_GMAC4__FDISC_TO_GMAC1, MSGRNG_STNID_GMAC1,
+ (int *) (long) &dev_mac_gmac0, 4, MSGRNG_STNID_GMAC1_FR},
+{SEND_TO_GMAC5__FDISC_TO_GMAC1, MSGRNG_STNID_GMAC1,
+ (int *) (long) &dev_mac_gmac0, 5, MSGRNG_STNID_GMAC1_FR},
+{SEND_TO_GMAC6__FDISC_TO_GMAC1, MSGRNG_STNID_GMAC1,
+ (int *) (long) &dev_mac_gmac0, 6, MSGRNG_STNID_GMAC1_FR},
+{SEND_TO_GMAC7__FDISC_TO_GMAC1, MSGRNG_STNID_GMAC1,
+ (int *) (long) &dev_mac_gmac0, 7, MSGRNG_STNID_GMAC1_FR},
+{-1, 0, 0, 0, 0}
+};
+
+struct rmi_phnx_msg_codes msg_stns[] = {
+	{0x0, MSGRNG_STNID_GMAC0, (int *) (long) &dev_mac_gmac0, 0,
+	 MSGRNG_STNID_GMAC0_FR},
+	{0x0, MSGRNG_STNID_GMAC1, (int *) (long) &dev_mac_gmac0, 4,
+	 MSGRNG_STNID_GMAC1_FR},
+	{0x0, MSGRNG_STNID_GMAC, (int *) (long) &dev_mac_gmac0, 0,
+	 MSGRNG_STNID_GMACRFR_0},
+	{0x0, MSGRNG_STNID_XGS0FR, (int *) (long) &dev_mac_xgs0, 0,
+	 MSGRNG_STNID_XMAC0RFR},
+	{0x0, MSGRNG_STNID_XGS1FR, (int *) (long) &dev_mac_xgs0, 1,
+	 MSGRNG_STNID_XMAC1RFR},
+	{-1, 0, 0, 0, 0}
+};
+
+void rmi_phnx_dom_msgring_handler(int bucket, int size, int code,
+								  int stid, struct msgrng_msg *msg,
+								  void *data /* ignored */ )
+{
+	unsigned long addr;
+	__u32 length;
+	int port;
+	struct sk_buff *skb;
+	struct driver_data *priv;
+	int fbstid = 0x0, i = 0, found = 0;
+
+	addr = (unsigned long) bus_to_virt(msg->msg0 & 0xffffffffe0ULL);
+	port = ((msg->msg0) & 0x0f);
+	length = ((msg->msg0 >> 40) & 0x3fff);
+
+
+	for (i = 0; msg_codes[i].code != -1; i++) {
+		if (code == msg_codes[i].code) {
+			stid = msg_codes[i].stid;
+			port = *(msg_codes[i].dev_mac) + msg_codes[i].offset;
+			fbstid = msg_codes[i].fbstid;
+			found = 1;
+			break;
+		}
+	}
+	if (found == 0) {
+		for (i = 0; msg_stns[i].code != -1; i++) {
+			if (stid == msg_codes[i].stid) {
+				port = *(msg_codes[i].dev_mac) + msg_codes[i].offset + port;
+				fbstid = msg_codes[i].fbstid;
+				found = 1;
+				break;
+			}
+		}
+	}
+
+	if (found == 0) {
+		printk("Invalid packet received stid=%d code=0x%x\n", stid, code);
+		return;
+	}
+
+	/* free back should be the packets send by linux */
+	if (length == 0x0) {
+		return rmi_phnx_free_skb(msg);
+	}
+
+	/*
+	 * Do nothing during the boot.
+	 */
+	if (system_state != SYSTEM_RUNNING) {
+		printk("Invalid system state pkt received stid=%d\n", stid);
+		drop_message(fbstid, msg);
+		return;
+	}
+	/*
+	 * Allocate an skbuff, initialize it, and copy the data to it.
+	 */
+	skb = __dev_alloc_skb(PHNX_RX_BUF_SIZE, GFP_ATOMIC);
+	if (!skb) {
+		printk("[%s] - no skbuff\n", __func__);
+		drop_message(fbstid, msg);
+		return;
+	}
+
+	skb->dev = dev_mac[port];
+
+	length = length - (BYTE_OFFSET + MAC_CRC_LEN);
+	skb_put(skb, length);
+	memcpy(skb->data, (char *) addr + 2, length);
+	drop_message(fbstid, msg);
+
+#ifdef DUMP_PACKETS
+	{
+		int i = 0;
+		printk("Rx Packet: length=%d\n", length);
+		for (i = 0; i < 64; i++) {
+			printk("%02x ", skb->data[i]);
+			if (i && (i % 16) == 0)
+				printk("\n");
+		}
+		printk("\n");
+	}
+#endif
+
+	skb->protocol = eth_type_trans(skb, skb->dev);
+	/*
+	 * Increment the driver stats counters.
+	 */
+	priv = netdev_priv(skb->dev);
+	mac_stats_add(priv->stats.rx_packets, 1);
+	mac_stats_add(priv->stats.rx_bytes, skb->len);
+	/*
+	 * Queue the packet to the upper layer.
+	 */
+	netif_rx(skb);
+}
+
+
+/* This function is called from an interrupt handler */
+void rmi_phnx_dev_tree_mac_msgring_handler(int bucket, int size, int code,
+										   int stid, struct msgrng_msg *msg,
+										   void *data /* ignored */ )
+{
+	unsigned long addr = 0;
+	__u32 length = 0;
+	int ctrl = 0, port = 0;
+	struct sk_buff *skb = 0;
+	int cpu = hard_smp_processor_id();
+
+	if (xlr_hybrid_rmios_tcpip_stack()) {
+		return;
+	}
+	dbg_msg
+		("mac: bucket=%d, size=%d, code=%d, stid=%d, msg0=%llx msg1=%llx\n",
+		 bucket, size, code, stid, msg->msg0, msg->msg1);
+
+	addr = (unsigned long) bus_to_virt(msg->msg0 & 0xffffffffe0ULL);
+	length = (msg->msg0 >> 40) & 0x3fff;
+	if (length == 0) {
+		ctrl = CTRL_REG_FREE;
+		port = (msg->msg0 >> 54) & 0x0f;
+	}
+	else {
+		ctrl = CTRL_SNGL;
+		length = length - BYTE_OFFSET - MAC_CRC_LEN;
+		port = msg->msg0 & 0x0f;
+	}
+
+	dbg_msg
+		("msg0 = %llx, msg1 = %llx, stid = %d, port = %d, addr=%lx, length=%d, ctrl=%d\n",
+		 msg->msg0, msg->msg1, stid, port, addr, length, ctrl);
+
+	if (ctrl == CTRL_REG_FREE || ctrl == CTRL_JUMBO_FREE) {
+		rmi_phnx_free_skb(msg);
+	}
+	else if (ctrl == CTRL_SNGL || ctrl == CTRL_START) {
+		/* Rx Packet */
+
+		struct driver_data *priv = 0;
+
+		dbg_msg("Received packet, port = %d\n", port);
+
+		skb = mac_get_skb_back_ptr(addr);
+		if (!skb) {
+			printk("[%s]: rx desc (0x%lx) for unknown skb? dropping packet\n",
+					 __func__, addr);
+			return;
+		}
+
+		if (is_xls()) {
+			if (stid == MSGRNG_STNID_GMAC0)
+				skb->dev = dev_mac[dev_mac_gmac0 + port];
+			else if (stid == MSGRNG_STNID_GMAC1)
+				skb->dev = dev_mac[dev_mac_gmac0 + 4 + port];
+			else {
+				printk("[%s]: rx desc (0x%lx) for unknown station %d? dropping packet\n",
+						__func__, addr, stid);
+				return;
+			}
+		}
+		else {
+			if (stid == MSGRNG_STNID_XGS0FR)
+				skb->dev = dev_mac[dev_mac_xgs0];
+			else if (stid == MSGRNG_STNID_XGS1FR)
+				skb->dev = dev_mac[dev_mac_xgs0 + 1];
+			else
+				skb->dev = dev_mac[dev_mac_gmac0 + port];
+		}
+
+		priv = netdev_priv(skb->dev);
+
+		/* if num frins to be sent exceeds threshold, wake up the helper thread */
+		if (atomic_inc_return(&priv->frin_to_be_sent[cpu]) >
+			MAC_FRIN_TO_BE_SENT_THRESHOLD) {
+			schedule_work(&mac_frin_replenish_work[cpu]);
+		}
+#ifdef DUMP_PACKETS
+		{
+			int i = 0;
+			dbg_msg("Rx Packet: length=%d\n", length);
+			for (i = 0; i < 64; i++) {
+				printk("%02x ", skb->data[i]);
+				if (i && (i % 16) == 0)
+					printk("\n");
+			}
+			printk("\n");
+		}
+#endif
+
+		/* compensate for the prepend data, byte offset */
+		skb_reserve(skb, MAC_PREPAD + BYTE_OFFSET);
+
+		skb_put(skb, length);
+		skb->protocol = eth_type_trans(skb, skb->dev);
+
+		dbg_msg("gmac_%d: rx packet: addr = %lx, length = %x, protocol=%d\n",
+				 priv->instance, addr, length, skb->protocol);
+
+		mac_stats_add(priv->stats.rx_packets, 1);
+		mac_stats_add(priv->stats.rx_bytes, skb->len);
+		mac_stats_add(priv->cpu_stats[cpu].rx_packets, 1);
+
+		phnx_inc_counter(NETIF_RX);
+		phnx_set_counter(NETIF_RX_CYCLES,
+						 (read_c0_count() - msgrng_msg_cycles));
+
+#ifdef CONFIG_PHOENIX_IP_QUEUE_AFFINITY
+		/* 
+		 * We pass bucket number in the last field of skb->cb[] structure 
+		 * it might be later picked up by multiprocess ip_queue
+		 */
+		skb->cb[sizeof(skb->cb) - 1] = bucket;
+#endif /* CONFIG_PHOENIX_IP_QUEUE_AFFINITY */
+
+
+#ifdef CONFIG_PHOENIX_IP_FLOW_AFFINITY
+		skb_transfer(bucket, skb);
+#else
+		netif_rx(skb);
+#endif /* CONFIG_PHOENIX_IP_FLOW_AFFINITY */
+
+	}
+	else {
+		printk("[%s]: unrecognized ctrl=%d!\n", __func__, ctrl);
+	}
+}
+
+
+
+#ifdef CONFIG_PHOENIX_IP_FLOW_AFFINITY
+static void skb_transfer(int bucket, struct sk_buff *skb)
+{
+	u_long my_cpu_no, my_thread_no, my_core_no, target_cpu_no,
+		target_thread_no;
+
+
+	target_thread_no = bucket & 0x3;
+	my_cpu_no = smp_processor_id();
+	my_thread_no = phoenix_thr_id();
+	my_core_no = phoenix_cpu_id();
+	target_cpu_no = cpu_number_map((my_core_no << 2) | target_thread_no);
+
+	/*
+	 * Version with NETRX IPI aggregation
+	 */
+	if (target_thread_no != my_thread_no
+		&& cpu_isset(target_cpu_no, cpu_online_map)) {
+		unsigned long flags;
+		struct sk_buff_head *ptqueue = &cpu_skb_tqueue[target_cpu_no];
+
+		spin_lock_irqsave(&ptqueue->lock, flags);
+		if (ptqueue->qlen) {
+			__skb_queue_tail(ptqueue, skb);
+		}
+		else {
+			__skb_queue_tail(ptqueue, skb);
+			core_send_ipi(target_thread_no, SMP_NETRX_IPI);
+		}
+		spin_unlock_irqrestore(&ptqueue->lock, flags);
+
+		skb_transfer_stat[my_cpu_no][target_cpu_no]++;
+	}
+	else {
+		skb_transfer_stat[my_cpu_no][my_cpu_no]++;
+
+		skb_queue_tail(&cpu_skb_tqueue[my_cpu_no], skb);
+		skb_transfer_finish();
+	}
+
+}
+
+/* second part of SKB transfer logic, called from IRQ_IPI_NETRX handler */
+void skb_transfer_finish(void)
+{
+	struct sk_buff *skb;
+	u_long cpu = smp_processor_id();
+
+	while ((skb = skb_dequeue(&cpu_skb_tqueue[cpu])) != NULL) {
+		netif_rx(skb);
+	}
+}
+
+#endif /* CONFIG_PHOENIX_IP_FLOW_AFFINITY */
+
+
+/**********************************************************************
+ **********************************************************************/
+static irqreturn_t rmi_phnx_mac_int_handler(int irq, void *dev_id)
+{
+	struct net_device *dev = (struct net_device *) dev_id;
+	struct driver_data *priv = netdev_priv(dev);
+	phoenix_reg_t *mmio = priv->mmio;
+	__u32 intreg = phoenix_read_reg(mmio, R_INTREG);
+	int cpu = hard_smp_processor_id();
+
+	mac_stats_add(priv->cpu_stats[cpu].interrupts, 1);
+
+	if (intreg & (1 << O_INTREG__MDInt)) {
+		__u32 phy_int_status = 0;
+		int i = 0;
+
+		for (i = 0; i < PHOENIX_MAX_MACS; i++) {
+			struct net_device *phy_dev = 0;
+			struct driver_data *phy_priv = 0;
+
+			/* skip the first 2 gmacs for atx_ii boards */
+			phy_dev = dev_mac[i];
+			if (phy_dev == NULL)
+				continue;
+			if (priv->own == 0)
+				continue;
+
+			phy_priv = netdev_priv(phy_dev);
+
+			if (phy_priv->type == TYPE_XGMAC)
+				continue;
+
+			phy_int_status = rmi_phnx_mac_mii_read(phy_priv, 26, 0);
+
+			if (is_xls() && !phy_priv->instance) {
+				phy_int_status = rmi_phnx_mac_mii_read(phy_priv, 26, 1);
+			}
+
+			rmi_phnx_gmac_config_speed(phy_priv);
+		}
+	}
+	else {
+		printk("[%s]: mac type = %d, instance %d error "
+			   "interrupt: INTREG = 0x%08x\n",
+			   __func__, priv->type, priv->instance, intreg);
+	}
+
+	/* clear all interrupts and hope to make progress */
+	phoenix_write_reg(mmio, R_INTREG, 0xffffffff);
+
+	/* on A0 and B0, xgmac interrupts are routed only to xgs_1 irq */
+	if ((xlr_revision_b0()) && (priv->type == TYPE_XGMAC)) {
+		struct net_device *xgs0_dev = dev_mac[dev_mac_xgs0];
+		struct driver_data *xgs0_priv = netdev_priv(xgs0_dev);
+		phoenix_reg_t *xgs0_mmio = xgs0_priv->mmio;
+		__u32 xgs0_intreg = phoenix_read_reg(xgs0_mmio, R_INTREG);
+
+		if (xgs0_intreg) {
+			printk("[%s]: mac type = %d, instance %d error "
+				   "interrupt: INTREG = 0x%08x\n",
+				   __func__, xgs0_priv->type, xgs0_priv->instance,
+				   xgs0_intreg);
+
+			phoenix_write_reg(xgs0_mmio, R_INTREG, 0xffffffff);
+		}
+	}
+
+	return IRQ_NONE;
+}
+
+static int rmi_phnx_mac_open(struct net_device *dev)
+{
+	struct driver_data *priv = netdev_priv(dev);
+	int i = 0;
+
+	if (priv->own == 0) {
+		spin_lock_irq(&priv->lock);
+		netif_start_queue(dev);
+		spin_unlock_irq(&priv->lock);
+		return 0;
+	}
+
+	dbg_msg("IN\n");
+	spin_lock_irq(&priv->lock);
+
+	if (!xlr_hybrid_rmios_ipsec() && !xlr_hybrid_rmios_tcpip_stack())
+		if (rmi_phnx_mac_fill_rxfr(dev)) {
+			spin_unlock_irq(&priv->lock);
+			return -1;
+		}
+	phnx_mac_set_rx_mode(dev);
+	if (!xlr_hybrid_rmios_ipsec() && !xlr_hybrid_rmios_tcpip_stack()) {
+		phoenix_write_reg(priv->mmio, R_INTMASK,
+						  (1 << O_INTMASK__TxIllegal) |
+						  (((priv->instance & 0x3) ==
+							0) << O_INTMASK__MDInt) | (1 <<
+													   O_INTMASK__TxFetchError)
+						  | (1 << O_INTMASK__P2PSpillEcc) | (1 <<
+															 O_INTMASK__TagFull)
+						  | (1 << O_INTMASK__Underrun) | (1 <<
+														  O_INTMASK__Abort)
+			);
+	}
+
+	else
+		phoenix_write_reg(priv->mmio, R_INTMASK, 0);
+
+	/* Set the timer to check for link beat. */
+	init_timer(&priv->link_timer);
+	priv->link_timer.expires = jiffies + 2 * HZ / 100;
+	priv->link_timer.data = (unsigned long) dev;
+	priv->link_timer.function = &rmi_phnx_mac_timer;
+	priv->phy_oldlinkstat = -1;	/* set link state to undefined */
+	add_timer(&priv->link_timer);
+
+	/*
+	 * Configure the speed, duplex, and flow control
+	 */
+	rmi_phnx_mac_set_speed(priv, priv->speed);
+	rmi_phnx_mac_set_duplex(priv, priv->duplex, priv->flow_ctrl);
+	rmi_phnx_mac_set_enable(priv, 1);
+
+	netif_start_queue(dev);
+
+	spin_unlock_irq(&priv->lock);
+
+	for (i = 0; i < 8; i++)
+		atomic_set(&priv->frin_to_be_sent[i], 0);
+
+#ifdef CONFIG_PHOENIX_GMAC_NAPI
+	if (xlr_napi) {
+		xlr_napi_ready = 1;
+	}
+#endif /* CONFIG_PHOENIX_GMAC_NAPI */
+
+	return 0;
+}
+
+static int rmi_phnx_mac_close(struct net_device *dev)
+{
+	struct driver_data *priv = netdev_priv(dev);
+	spin_lock_irq(&priv->lock);
+
+	/* There may have left over skbs in the ring as well as in free in 
+	 * they will be reused next time open is called 
+	 */
+
+	rmi_phnx_mac_set_enable(priv, 0);
+
+	del_timer_sync(&priv->link_timer);
+
+	netif_stop_queue(dev);
+	phnx_inc_counter(NETIF_STOP_Q);
+	port_inc_counter(priv->instance, PORT_STOPQ);
+
+	spin_unlock_irq(&priv->lock);
+
+	return 0;
+}
+
+static void rmi_phnx_mac_timer(unsigned long data)
+{
+	struct net_device *dev = (struct net_device *) data;
+	struct driver_data *priv = netdev_priv(dev);
+	int next_tick = HZ;
+	int mii_status;
+
+	spin_lock_irq(&priv->lock);
+
+	/* read flag "Link established" (0x04) of MII status register (1) */
+	mii_status = rmi_phnx_mac_mii_read(priv, 1, 0) & 0x04;
+
+	if (mii_status != priv->phy_oldlinkstat) {
+		priv->phy_oldlinkstat = mii_status;
+		if (mii_status) {
+			netif_carrier_on(dev);
+		}
+		else {
+			netif_carrier_off(dev);
+		}
+	}
+
+	spin_unlock_irq(&priv->lock);
+	priv->link_timer.expires = jiffies + next_tick;
+	add_timer(&priv->link_timer);
+}
+
+static struct net_device_stats *rmi_phnx_mac_get_stats(struct net_device *dev)
+{
+	struct driver_data *priv = netdev_priv(dev);
+	unsigned long flags;
+
+	spin_lock_irqsave(&priv->lock, flags);
+
+	/* XXX update other stats here */
+	spin_unlock_irqrestore(&priv->lock, flags);
+
+	return &priv->stats;
+}
+
+static void rmi_phnx_mac_set_multicast_list(struct net_device *dev)
+{
+	/* 
+	 * Clear out entire multicast table.  We do this by nuking
+	 * the entire hash table and all the direct matches except
+	 * the first one, which is used for our station address 
+	 */
+
+	/*
+	 * Clear the filter to say we don't want any multicasts.
+	 */
+
+	if (dev->flags & IFF_ALLMULTI) {
+		/* 
+		 * Enable ALL multicasts.  Do this by inverting the 
+		 * multicast enable bit. 
+		 */
+		return;
+	}
+
+	/* 
+	 * Progam new multicast entries.  For now, only use the
+	 * perfect filter.  In the future we'll need to use the
+	 * hash filter if the perfect filter overflows
+	 */
+}
+
+static int
+rmi_phnx_mac_do_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)
+{
+	int rc = 0;
+	switch (cmd) {
+		default:
+			rc = -EOPNOTSUPP;
+			break;
+	}
+
+	return rc;
+}
+
+static void rmi_phnx_mac_tx_timeout(struct net_device *dev)
+{
+	struct driver_data *priv = netdev_priv(dev);
+
+	spin_lock_irq(&priv->lock);
+
+	dev->trans_start = jiffies;
+	mac_stats_add(priv->stats.tx_errors, 1);
+
+	spin_unlock_irq(&priv->lock);
+
+	netif_wake_queue(dev);
+	phnx_inc_counter(NETIF_START_Q);
+	port_inc_counter(priv->instance, PORT_STARTQ);
+
+	printk(KERN_WARNING "%s: Transmit timed out\n", dev->name);
+}
+
+static int rmi_phnx_mac_change_mtu(struct net_device *dev, int new_mtu)
+{
+	struct driver_data *priv = netdev_priv(dev);
+	unsigned long flags;
+
+	if ((new_mtu > 9500) || (new_mtu < 64)) {
+		return -EINVAL;
+	}
+
+	spin_lock_irqsave(&priv->lock, flags);
+
+	dev->mtu = new_mtu;
+
+	if (netif_running(dev)) {
+		/* Disable MAC TX/RX */
+		rmi_phnx_mac_set_enable(priv, 0);
+
+		/* Flush RX FR IN */
+		/* Flush TX IN */
+		rmi_phnx_mac_set_enable(priv, 1);
+	}
+
+	spin_unlock_irqrestore(&priv->lock, flags);
+	return 0;
+}
+
+static int rmi_phnx_mac_fill_rxfr(struct net_device *dev)
+{
+	struct driver_data *priv = netdev_priv(dev);
+	struct sk_buff *skb = 0;
+	unsigned long msgrng_flags;
+	int i;
+	int ret = 0;
+
+	dbg_msg("\n");
+	if (!priv->init_frin_desc)
+		return ret;
+	priv->init_frin_desc = 0;
+
+	dbg_msg("\n");
+	for (i = 0; i < max_num_desc; i++) {
+		skb = rmi_phnx_alloc_skb();
+		if (!skb) {
+			ret = -ENOMEM;
+			break;
+		}
+
+		skb->dev = dev;
+
+#ifdef CONFIG_PHOENIX_HW_BUFFER_MGMT
+		if (xlr_napi) {
+			skb_shinfo(skb)->rmi_flags = 1;
+			skb_shinfo(skb)->rmi_owner = dev;
+			skb_shinfo(skb)->rmi_refill = mac_frin_replenish_one_msg;
+		}
+#endif /* CONFIG_PHOENIX_HW_BUFFER_MGMT */
+
+		/* Send the free Rx desc to the MAC */
+		msgrng_access_enable(msgrng_flags);
+		mac_put_skb_back_ptr(skb);
+		if (phnx_mac_send_fr(priv, virt_to_bus(skb->data), skb->len)) {
+			dev_kfree_skb(skb);
+			printk("message_send failed!, unable to send free desc to mac\n");
+			ret = -EIO;
+			break;
+		}
+		msgrng_access_disable(msgrng_flags);
+	}
+
+	return ret;
+}
+
+static __inline__ void *xgmac_config_spill(phoenix_reg_t * mmio,
+										   int reg_start_0, int reg_start_1,
+										   int reg_size, int size)
+{
+	__u32 spill_size = CACHELINE_ALIGNED_ADDR(size);
+	void *spill = cacheline_aligned_kmalloc(spill_size, GFP_KERNEL);
+	__u64 phys_addr = 0;
+	if (!spill) {
+		panic("Unable to allocate memory for spill area!\n");
+	}
+	phys_addr = virt_to_phys(spill);
+	phoenix_write_reg(mmio, reg_start_0, (phys_addr >> 5) & 0xffffffff);
+	phoenix_write_reg(mmio, reg_start_1, (phys_addr >> 37) & 0x07);
+	phoenix_write_reg(mmio, reg_size, spill_size);
+	return spill;
+}
+
+static __inline__ void *rmi_phnx_config_spill(struct driver_data *priv,
+											  int reg_start_0,
+											  int reg_start_1, int reg_size,
+											  int size)
+{
+	phoenix_reg_t *mmio = priv->mmio;
+	__u64 phys_addr = 0;
+	void *spill = NULL;
+
+
+	__u32 spill_size = CACHELINE_ALIGNED_ADDR(size);
+	if (priv->own & PHNX_GMAC_SHARED)
+		spill = rmi_spill_mem_alloc(spill_size);
+
+	if (spill == NULL)
+		spill = cacheline_aligned_kmalloc(spill_size, GFP_KERNEL);
+
+	if (!spill) {
+		panic("Unable to allocate memory for spill area!\n");
+	}
+	phys_addr = virt_to_phys(spill);
+	phoenix_write_reg(mmio, reg_start_0, (phys_addr >> 5) & 0xffffffff);
+	phoenix_write_reg(mmio, reg_start_1, (phys_addr >> 37) & 0x07);
+	phoenix_write_reg(mmio, reg_size, spill_size);
+
+	return spill;
+}
+
+static void rmi_phnx_config_spill_area(struct driver_data *priv)
+{
+	/*
+	 * if driver initialization is done parallely on multiple cpus
+	 * spill_configured needs synchronization 
+	 */
+	if (priv->spill_configured)
+		return;
+
+	if (priv->own == 0x0) {
+		priv->spill_configured = 1;
+		return;
+	}
+
+	priv->frin_spill =
+		rmi_phnx_config_spill(priv,
+							  R_REG_FRIN_SPILL_MEM_START_0,
+							  R_REG_FRIN_SPILL_MEM_START_1,
+							  R_REG_FRIN_SPILL_MEM_SIZE,
+							  max_frin_spill * sizeof(struct fr_desc));
+
+	priv->class_0_spill =
+		rmi_phnx_config_spill(priv,
+							  R_CLASS0_SPILL_MEM_START_0,
+							  R_CLASS0_SPILL_MEM_START_1,
+							  R_CLASS0_SPILL_MEM_SIZE,
+							  max_class_0_spill * sizeof(union rx_tx_desc));
+	priv->class_1_spill =
+		rmi_phnx_config_spill(priv,
+							  R_CLASS1_SPILL_MEM_START_0,
+							  R_CLASS1_SPILL_MEM_START_1,
+							  R_CLASS1_SPILL_MEM_SIZE,
+							  max_class_1_spill * sizeof(union rx_tx_desc));
+
+	priv->frout_spill =
+		rmi_phnx_config_spill(priv, R_FROUT_SPILL_MEM_START_0,
+							  R_FROUT_SPILL_MEM_START_1,
+							  R_FROUT_SPILL_MEM_SIZE,
+							  max_frout_spill * sizeof(struct fr_desc));
+
+	priv->class_2_spill =
+		rmi_phnx_config_spill(priv,
+							  R_CLASS2_SPILL_MEM_START_0,
+							  R_CLASS2_SPILL_MEM_START_1,
+							  R_CLASS2_SPILL_MEM_SIZE,
+							  max_class_2_spill * sizeof(union rx_tx_desc));
+	priv->class_3_spill =
+		rmi_phnx_config_spill(priv,
+							  R_CLASS3_SPILL_MEM_START_0,
+							  R_CLASS3_SPILL_MEM_START_1,
+							  R_CLASS3_SPILL_MEM_SIZE,
+							  max_class_3_spill * sizeof(union rx_tx_desc));
+
+	priv->spill_configured = 1;
+}
+
+/*****************************************************************
+ * Write the MAC address to the PHNX registers
+ * All 4 addresses are the same for now
+ *****************************************************************/
+static void phnx_mac_setup_hwaddr(struct driver_data *priv)
+{
+	struct net_device *dev = priv->dev;
+
+	if (priv->own == 0)
+		return;
+
+	phoenix_write_reg(priv->mmio, R_MAC_ADDR0,
+					  ((dev->dev_addr[5] << 24) | (dev->dev_addr[4] << 16)
+					   | (dev->dev_addr[3] << 8) | (dev->dev_addr[2]))
+		);
+
+	phoenix_write_reg(priv->mmio, R_MAC_ADDR0 + 1,
+					  ((dev->dev_addr[1] << 24) | (dev->dev_addr[0] << 16)));
+
+	phoenix_write_reg(priv->mmio, R_MAC_ADDR_MASK2, 0xffffffff);
+
+	phoenix_write_reg(priv->mmio, R_MAC_ADDR_MASK2 + 1, 0xffffffff);
+
+	phoenix_write_reg(priv->mmio, R_MAC_ADDR_MASK3, 0xffffffff);
+
+	phoenix_write_reg(priv->mmio, R_MAC_ADDR_MASK3 + 1, 0xffffffff);
+
+	if (xlr_hybrid_user_mac()) {
+		phoenix_write_reg(priv->mmio, R_MAC_FILTER_CONFIG,
+						  (1 << O_MAC_FILTER_CONFIG__BROADCAST_EN) |
+						  (1 << O_MAC_FILTER_CONFIG__ALL_MCAST_EN) |
+						  (1 << O_MAC_FILTER_CONFIG__ALL_UCAST_EN) |
+						  (1 << O_MAC_FILTER_CONFIG__MAC_ADDR0_VALID)
+			);
+	}
+	else {
+		phoenix_write_reg(priv->mmio, R_MAC_FILTER_CONFIG,
+						  (1 << O_MAC_FILTER_CONFIG__BROADCAST_EN) |
+						  (1 << O_MAC_FILTER_CONFIG__ALL_MCAST_EN) |
+						  (1 << O_MAC_FILTER_CONFIG__MAC_ADDR0_VALID)
+			);
+	}
+}
+
+/*****************************************************************
+ * Read the MAC address from the PHNX registers
+ * All 4 addresses are the same for now
+ *****************************************************************/
+static void phnx_mac_get_hwaddr(struct net_device *dev)
+{
+	struct driver_data *priv = netdev_priv(dev);
+
+	dev->dev_addr[0] = phoenix_base_mac_addr[0];
+	dev->dev_addr[1] = phoenix_base_mac_addr[1];
+	dev->dev_addr[2] = phoenix_base_mac_addr[2];
+	dev->dev_addr[3] = phoenix_base_mac_addr[3];
+	dev->dev_addr[4] = phoenix_base_mac_addr[4];
+	dev->dev_addr[5] = phoenix_base_mac_addr[5] + priv->id;
+}
+
+/**********************************************************************
+ * Set a new Ethernet address for the interface.
+ **********************************************************************/
+static int rmi_phnx_set_mac_address(struct net_device *dev, void *addr)
+{
+	struct driver_data *priv = netdev_priv(dev);
+	struct sockaddr *p_sockaddr = (struct sockaddr *) addr;
+
+	memcpy(dev->dev_addr, p_sockaddr->sa_data, 6);
+	phnx_mac_setup_hwaddr(priv);
+	return 0;
+}
+
+
+extern struct user_mac_data *user_mac;
+
+static int xlr_mac_proc_read(char *page, char **start, off_t off,
+							 int count, int *eof, void *data)
+{
+	int len = 0;
+	off_t begin = 0;
+	int i = 0, cpu = 0;
+	struct net_device *dev = 0;
+	struct driver_data *priv = 0;
+
+
+#ifdef CONFIG_PHOENIX_GMAC_NAPI
+	if (xlr_napi) {
+		for (cpu = 0; cpu < 32; cpu++) {
+			if (!cpu_isset(cpu, cpu_online_map)) {
+				continue;
+			}
+			len += sprintf(page + len, "napi: cpu=%02d: %16lld\n", cpu,
+						   per_cpu(xlr_napi_rx_count_, cpu));
+			if (!proc_pos_check(&begin, &len, off, count))
+				goto out;
+		}
+	}
+#endif /* CONFIG_PHOENIX_GMAC_NAPI */
+
+
+	for (i = 0; i < PHOENIX_MAX_MACS; i++) {
+
+		/* skip the first 2 gmacs for atx_ii boards */
+		dev = dev_mac[i];
+		if (dev == NULL)
+			continue;
+
+		priv = netdev_priv(dev);
+
+		for (cpu = 0; cpu < 32; cpu++) {
+
+			if (!cpu_isset(cpu, cpu_online_map))
+				continue;
+
+			len +=
+				sprintf(page + len, "per_cpu: %d %d %d %d %lx %lx %lx %lx\n",
+						i, cpu, user_mac->time.hi, user_mac->time.lo,
+						priv->cpu_stats[cpu].tx_packets,
+						priv->cpu_stats[cpu].txc_packets,
+						priv->cpu_stats[cpu].rx_packets,
+						priv->cpu_stats[cpu].interrupts);
+			if (!proc_pos_check(&begin, &len, off, count))
+				goto out;
+		}
+
+		len += sprintf(page + len,
+					   "per_port: %d %d %d %lx %lx %lx %lx\n",
+					   i, user_mac->time.hi, user_mac->time.lo,
+					   priv->stats.rx_packets, priv->stats.rx_bytes,
+					   priv->stats.tx_packets, priv->stats.tx_bytes);
+		if (!proc_pos_check(&begin, &len, off, count))
+			goto out;
+	}
+
+	*eof = 1;
+
+out:
+	*start = page + (off - begin);
+	len -= (off - begin);
+	if (len > count)
+		len = count;
+	if (len < 0)
+		len = 0;
+
+	return len;
+}
+
+
+#ifdef CONFIG_PHOENIX_HW_BUFFER_MGMT
+static int
+setup_auto_free(struct sk_buff *skb, int type, struct msgrng_msg *msg)
+{
+	struct driver_data *priv;
+	struct skb_shared_info *shinfo;
+	int fr_stid, offset;
+
+
+	shinfo = skb_shinfo(skb);
+	if (!shinfo->rmi_flags)
+		return 0;
+
+	if (atomic_read(&skb->users) != 1) {
+		printk(KERN_ALERT "%s: Can't recycle because of users count\n",
+			   __func__);
+		return 0;
+	}
+
+	if (skb->cloned || atomic_read(&(skb_shinfo(skb)->dataref)) != 1) {
+		printk(KERN_EMERG "%s: Can't recycle because of cloned or dataref\n",
+			   __func__);
+		return 0;
+	}
+
+	/* Leak no dsk entries! */
+	dst_release(skb->dst);
+
+	/* Now reinitialize old skb, cut & paste from dev_alloc_skb */
+	memset(skb, 0, offsetof(struct sk_buff, truesize));
+	skb->data = skb->head;
+	skb->tail = skb->head;
+
+	atomic_set(&shinfo->dataref, 1);
+	shinfo->nr_frags = 0;
+	shinfo->gso_size = 0;
+	shinfo->gso_segs = 0;
+	shinfo->gso_type = 0;
+	shinfo->ip6_frag_id = 0;
+	shinfo->frag_list = NULL;
+
+	offset =
+		(((unsigned long) skb->data + SMP_CACHE_BYTES) & ~(SMP_CACHE_BYTES -
+														   1));
+	skb_reserve(skb, (offset - (unsigned long) skb->data));
+
+	priv = netdev_priv(skb_shinfo(skb)->rmi_owner);
+	fr_stid = msgrng_stid_rfr(priv->instance, priv->type);
+
+	mac_put_skb_back_ptr(skb);
+
+	msg->msg1 = (((uint64_t) 1 << 63) |
+				 ((uint64_t) fr_stid << 54) | ((uint64_t) 0 << 40) |
+#ifdef CONFIG_64BIT
+				 ((uint64_t) virt_to_phys(skb->data) & 0xffffffffffULL)
+#else
+				 ((unsigned long) (virt_to_phys(skb->data) & 0xffffffffUL))
+#endif
+		);
+
+	return 1;
+}
+#endif /* CONFIG_PHOENIX_HW_BUFFER_MGMT */
+
+
+
+#ifdef CONFIG_PHOENIX_GMAC_NAPI
+
+static int mac_frin_replenish_one_msg(struct net_device *dev)
+{
+	int offset = 0;
+	unsigned long msgrng_flags;
+	struct sk_buff *skb = 0;
+	struct driver_data *priv;
+
+
+	if (!dev)
+		return 0;
+
+	priv = netdev_priv(dev);
+
+	if (priv->own & PHNX_GMAC_SHARED) {
+		void *phys_addr;
+		phys_addr = rmi_frin_mem_alloc(PHNX_RX_BUF_SIZE);
+		if (phys_addr == NULL) {
+			phys_addr =
+				cacheline_aligned_kmalloc(PHNX_RX_BUF_SIZE, GFP_KERNEL);
+			if (phys_addr == NULL)
+				panic("Out of memory\n");
+		}
+
+		msgrng_access_enable(msgrng_flags);
+		if (phnx_mac_send_fr(priv, virt_to_bus(phys_addr), PHNX_RX_BUF_SIZE)) {
+			printk("[%s]: rx free message_send failed!\n", __func__);
+			return 0;
+		}
+		msgrng_access_disable(msgrng_flags);
+		return 0;
+	}
+
+
+	skb = __dev_alloc_skb(PHNX_RX_BUF_SIZE, GFP_ATOMIC);
+	if (!skb) {
+		printk(KERN_ALERT "%s: can't alloc skb\n", __func__);
+		return 0;
+	}
+	phnx_inc_counter(REPLENISH_FRIN);
+
+	/* align the data to the next cache line */
+	offset =
+		(((unsigned long) skb->data + SMP_CACHE_BYTES) & ~(SMP_CACHE_BYTES -
+														   1));
+	skb_reserve(skb, (offset - (unsigned long) skb->data));
+
+#ifdef CONFIG_PHOENIX_HW_BUFFER_MGMT
+	/* 
+	 * put skb under automatic buffer management, if we have just regular NAPI
+	 * w/o HW buffer mgmt, fields are not set.
+	 */
+	skb_shinfo(skb)->rmi_flags = 1;
+	skb_shinfo(skb)->rmi_owner = dev;
+	skb_shinfo(skb)->rmi_refill = mac_frin_replenish_one_msg;
+#endif /* CONFIG_PHOENIX_HW_BUFFER_MGMT */
+
+	mac_put_skb_back_ptr(skb);
+	msgrng_access_enable(msgrng_flags);
+	if (phnx_mac_send_fr(priv, virt_to_bus(skb->data), skb->len)) {
+		dev_kfree_skb(skb);
+		printk("[%s]: rx free message_send failed!\n", __func__);
+	}
+	msgrng_access_disable(msgrng_flags);
+
+	return 0;
+}
+
+
+#endif /* CONFIG_PHOENIX_GMAC_NAPI */
+
+
+
+
+
+
+
+
+static void setup_mac_spill_sizes(int desc)
+{
+	max_num_desc = desc;
+
+	max_frin_spill = max_num_desc << 2;
+	max_frout_spill = max_num_desc << 2;
+
+	max_class_0_spill = max_num_desc;
+	max_class_1_spill = max_num_desc;
+	max_class_2_spill = max_num_desc;
+	max_class_3_spill = max_num_desc;
+}
+
+static int __init xlr_mac_desc_setup(char *str)
+{
+	int desc = simple_strtoul(str, 0, 10);
+
+	printk("[%s]: str = \"%s\", desc=%d\n", __func__, str, desc);
+	setup_mac_spill_sizes(desc);
+
+	return 1;
+}
+
+__setup("xlr_mac_dt_desc=", xlr_mac_desc_setup);
+
+static int __init xls_gmac0_sgmii_setup(char *str)
+{
+	if (is_xls()) {
+		printk("[%s]: *********************************************\n",
+			   __func__);
+		printk("[%s]: Enabling SGMII mode for gmac0\n", __func__);
+		printk("[%s]: *********************************************\n",
+			   __func__);
+		xls_gmac0_sgmii = 1;
+	}
+
+	return 1;
+}
+
+__setup("xls_gmac0_sgmii=", xls_gmac0_sgmii_setup);
+
+static int xlr_get_settings(struct net_device *dev, struct ethtool_cmd *cmd)
+{
+	struct driver_data *priv = netdev_priv(dev);
+	int mii_status;
+
+	if (priv->type == TYPE_XGMAC) {
+		cmd->supported = SUPPORTED_FIBRE | SUPPORTED_10000baseT_Full;
+		cmd->advertising = SUPPORTED_FIBRE | SUPPORTED_10000baseT_Full;
+		cmd->speed = SPEED_10000;
+		cmd->port = PORT_FIBRE;
+		cmd->duplex = DUPLEX_FULL;
+		cmd->phy_address = priv->instance;
+		cmd->autoneg = AUTONEG_DISABLE;
+		cmd->maxtxpkt = 0;
+		cmd->maxrxpkt = 0;
+
+	}
+	else {
+
+		cmd->supported = SUPPORTED_10baseT_Full |
+			SUPPORTED_10baseT_Half |
+			SUPPORTED_100baseT_Full | SUPPORTED_100baseT_Half |
+			SUPPORTED_1000baseT_Full | SUPPORTED_MII |
+			SUPPORTED_Autoneg | SUPPORTED_TP;
+
+		cmd->advertising = priv->advertising;
+
+		mii_status = rmi_phnx_mac_mii_read(priv, MII_NCONFIG, 0);
+		priv->speed = (mii_status >> 3) & 0x03;
+
+		cmd->speed = (priv->speed == phnx_mac_speed_1000) ? SPEED_1000 :
+			(priv->speed == phnx_mac_speed_100) ? SPEED_100 : SPEED_10;
+
+		cmd->duplex = (mii_status >> 5) & 0x1;
+		cmd->port = PORT_TP;
+		cmd->phy_address = priv->instance;
+		cmd->transceiver = XCVR_INTERNAL;
+		cmd->autoneg = (~(mii_status >> 14)) & 0x1;
+		cmd->maxtxpkt = 0;
+		cmd->maxrxpkt = 0;
+	}
+
+	return 0;
+}
+static int xlr_enable_autoneg(struct net_device *dev, u32 adv)
+{
+	struct driver_data *priv = netdev_priv(dev);
+	int mii_status;
+	u32 flags, adv1, adv2;
+
+	spin_lock_irqsave(&priv->lock, flags);
+	rmi_phnx_mac_set_enable(priv, 0);
+	/* advertising for 10/100 Mbps */
+	adv1 = rmi_phnx_mac_mii_read(priv, MII_ADVERTISE, 0);
+	adv1 &= ~(ADVERTISE_ALL | ADVERTISE_100BASE4);
+	/* advertising for 1000 Mbps */
+	adv2 = rmi_phnx_mac_mii_read(priv, 0x9, 0);
+	adv2 &= ~(0x300);
+
+	if (adv & ADVERTISED_10baseT_Half)
+		adv1 |= ADVERTISE_10HALF;
+	if (adv & ADVERTISED_10baseT_Full)
+		adv1 |= ADVERTISE_10FULL;
+	if (adv & ADVERTISED_100baseT_Full)
+		adv1 |= ADVERTISE_100FULL;
+	if (adv & ADVERTISED_100baseT_Half)
+		adv1 |= ADVERTISE_100HALF;
+
+	if (adv & ADVERTISED_1000baseT_Full)
+		adv2 |= 0x200;
+	if (adv & ADVERTISED_1000baseT_Half)
+		adv2 |= 0x100;
+
+	/* Set the advertising parameters */
+	rmi_phnx_mac_mii_write(priv, MII_ADVERTISE, adv1, 0);
+	rmi_phnx_mac_mii_write(priv, 0x9, adv2, 0);
+
+	priv->advertising = adv1 | adv2;
+
+	mii_status = rmi_phnx_mac_mii_read(priv, MII_BMCR, 0);
+	/* enable autoneg and force restart autoneg */
+	mii_status |= (BMCR_ANENABLE | BMCR_ANRESTART);
+	rmi_phnx_mac_mii_write(priv, MII_BMCR, mii_status, 0);
+
+	rmi_phnx_mac_set_enable(priv, 1);
+	spin_unlock_irqrestore(&priv->lock, flags);
+
+	return 0;
+}
+
+static int xlr_set_link_speed(struct net_device *dev, int speed, int duplex)
+{
+	struct driver_data *priv = netdev_priv(dev);
+	int mii_status = 0, tmp_status;
+	u32 flags;
+	int ret = 0;
+
+	switch (speed) {
+		case SPEED_10:
+			mii_status = 0;
+			priv->speed = phnx_mac_speed_10;
+			break;
+		case SPEED_100:
+			mii_status = 0x2000;
+			priv->speed = phnx_mac_speed_100;
+			break;
+		case SPEED_1000:
+			mii_status = 0x0040;
+			priv->speed = phnx_mac_speed_1000;
+			break;
+		default:
+			ret = -EINVAL;
+			return ret;
+	}
+	if (duplex == DUPLEX_FULL)
+		mii_status |= BMCR_FULLDPLX;
+
+	spin_lock_irqsave(&priv->lock, flags);
+	rmi_phnx_mac_set_enable(priv, 0);
+
+	tmp_status = rmi_phnx_mac_mii_read(priv, MII_BMCR, 0);
+	/* Turn off autoneg, speed and duplexity setting */
+	tmp_status &= ~(BMCR_ANENABLE | 0x0040 | 0x2000 | BMCR_FULLDPLX);
+
+	rmi_phnx_mac_mii_write(priv, MII_BMCR, tmp_status | mii_status, 0);
+
+	if (priv->speed == phnx_mac_speed_10) {
+		phoenix_write_reg(priv->mmio, R_MAC_CONFIG_2, 0x7137);
+		phoenix_write_reg(priv->mmio, R_CORECONTROL, 0x02);
+		printk(KERN_INFO "configuring gmac_%d in 10Mbps mode\n",
+			   priv->instance);
+	}
+	else if (priv->speed == phnx_mac_speed_100) {
+		phoenix_write_reg(priv->mmio, R_MAC_CONFIG_2, 0x7137);
+		phoenix_write_reg(priv->mmio, R_CORECONTROL, 0x01);
+		printk(KERN_INFO "configuring gmac_%d in 100Mbps mode\n",
+			   priv->instance);
+	}
+	else {
+		phoenix_write_reg(priv->mmio, R_MAC_CONFIG_2, 0x7237);
+		phoenix_write_reg(priv->mmio, R_CORECONTROL, 0x00);
+		printk(KERN_INFO "configuring gmac_%d in 1000Mbps mode\n",
+			   priv->instance);
+	}
+	rmi_phnx_mac_set_enable(priv, 1);
+	spin_unlock_irqrestore(&priv->lock, flags);
+	return 0;
+
+}
+
+static int xlr_set_settings(struct net_device *dev, struct ethtool_cmd *cmd)
+{
+	int ret;
+	struct driver_data *priv = netdev_priv(dev);
+
+	if (priv->type == TYPE_XGMAC) {
+		return -EIO;
+	}
+	if (cmd->autoneg == AUTONEG_ENABLE) {
+		ret = xlr_enable_autoneg(dev, cmd->advertising);
+	}
+	else {
+		ret = xlr_set_link_speed(dev, cmd->speed, cmd->duplex);
+	}
+	return ret;
+}
+
+static void xlr_get_drvinfo(struct net_device *dev,
+							struct ethtool_drvinfo *info)
+{
+	strcpy(info->driver, DRV_NAME);
+	strcpy(info->version, DRV_VERSION);
+}
+
+static int xlr_get_regs_len(struct net_device *dev)
+{
+	return PHNX_ETHTOOL_REG_LEN;
+}
+static void xlr_get_regs(struct net_device *dev,
+						 struct ethtool_regs *regs, void *p)
+{
+	u32 *data = (u32 *) p;
+	int i;
+	struct driver_data *priv = netdev_priv(dev);
+	u32 flags;
+
+	memset((void *) data, 0, PHNX_ETHTOOL_REG_LEN);
+
+	spin_lock_irqsave(&priv->lock, flags);
+	for (i = 0; i <= PHNX_NUM_REG_DUMP; i++)
+		*(data + i) = phoenix_read_reg(priv->mmio, R_TX_CONTROL + i);
+	spin_unlock_irqrestore(&priv->lock, flags);
+}
+static u32 xlr_get_msglevel(struct net_device *dev)
+{
+	return mac_debug;
+}
+static void xlr_set_msglevel(struct net_device *dev, u32 value)
+{
+	mac_debug = value;
+}
+
+static int xlr_nway_reset(struct net_device *dev)
+{
+	struct driver_data *priv = netdev_priv(dev);
+	int mii_status;
+	u32 flags;
+	int ret = -EINVAL;
+
+	if (priv->type == TYPE_XGMAC)
+		return -EIO;
+
+	spin_lock_irqsave(&priv->lock, flags);
+	mii_status = rmi_phnx_mac_mii_read(priv, MII_BMCR, 0);
+	if (mii_status & BMCR_ANENABLE) {
+		rmi_phnx_mac_mii_write(priv,
+							   MII_BMCR, BMCR_ANRESTART | mii_status, 0);
+		ret = 0;
+	}
+	spin_unlock_irqrestore(&priv->lock, flags);
+	return ret;
+}
+static u32 xlr_get_link(struct net_device *dev)
+{
+	struct driver_data *priv = netdev_priv(dev);
+	int mii_status;
+	u32 flags;
+
+	if (priv->type == TYPE_XGMAC)
+		return -EIO;
+
+	spin_lock_irqsave(&priv->lock, flags);
+	mii_status = rmi_phnx_mac_mii_read(priv, MII_BMSR, 0);
+
+	spin_unlock_irqrestore(&priv->lock, flags);
+
+	if (mii_status & BMSR_LSTATUS)
+		return 1;
+	return 0;
+}
+
+#define PHNX_STATS_KEY_LEN  \
+		(sizeof(struct net_device_stats) / sizeof(unsigned long))
+static struct
+{
+	const char string[ETH_GSTRING_LEN];
+} phnx_ethtool_stats_keys[PHNX_STATS_KEY_LEN] = {
+	{
+	"rx_packets"}, {
+	"tx_packets"}, {
+	"rx_bytes"}, {
+	"tx_bytes"}, {
+	"rx_errors"}, {
+	"tx_errors"}, {
+	"rx_dropped"}, {
+	"tx_dropped"}, {
+	"multicast"}, {
+	"collisions"}, {
+	"rx_length_errors"}, {
+	"rx_over_errors"}, {
+	"rx_crc_errors"}, {
+	"rx_frame_errors"}, {
+	"rx_fifo_errors"}, {
+	"rx_missed_errors"}, {
+	"tx_aborted_errors"}, {
+	"tx_carrier_errors"}, {
+	"tx_fifo_errors"}, {
+	"tx_heartbeat_errors"}, {
+	"tx_window_errors"}, {
+	"rx_compressed"}, {
+	"tx_compressed"}
+};
+static int xlr_get_stats_count(struct net_device *dev)
+{
+	return PHNX_STATS_KEY_LEN;
+}
+
+static void xlr_get_strings(struct net_device *dev, u32 stringset, u8 * buf)
+{
+	switch (stringset) {
+		case ETH_SS_STATS:
+			memcpy(buf, &phnx_ethtool_stats_keys,
+				   sizeof(phnx_ethtool_stats_keys));
+			break;
+		default:
+			printk(KERN_WARNING "%s: Invalid stringset %d\n",
+				   __func__, stringset);
+			break;
+	}
+}
+
+static void xlr_get_mac_stats(struct net_device *dev,
+							  struct net_device_stats *stats)
+{
+	struct driver_data *priv = netdev_priv(dev);
+
+	stats->tx_errors = phoenix_read_reg(priv->mmio, TX_FCS_ERROR_COUNTER);
+	stats->rx_dropped = phoenix_read_reg(priv->mmio, RX_DROP_PACKET_COUNTER);
+	stats->tx_dropped = phoenix_read_reg(priv->mmio, TX_DROP_FRAME_COUNTER);
+
+	stats->multicast = phoenix_read_reg(priv->mmio,
+										RX_MULTICAST_PACKET_COUNTER);
+	stats->collisions = phoenix_read_reg(priv->mmio,
+										 TX_TOTAL_COLLISION_COUNTER);
+
+	stats->rx_length_errors = phoenix_read_reg(priv->mmio,
+											   RX_FRAME_LENGTH_ERROR_COUNTER);
+	stats->rx_over_errors = phoenix_read_reg(priv->mmio,
+											 RX_DROP_PACKET_COUNTER);
+	stats->rx_crc_errors = phoenix_read_reg(priv->mmio, RX_FCS_ERROR_COUNTER);
+	stats->rx_frame_errors = phoenix_read_reg(priv->mmio,
+											  RX_ALIGNMENT_ERROR_COUNTER);
+
+	stats->rx_fifo_errors = phoenix_read_reg(priv->mmio,
+											 RX_DROP_PACKET_COUNTER);
+	stats->rx_missed_errors = phoenix_read_reg(priv->mmio,
+											   RX_CARRIER_SENSE_ERROR_COUNTER);
+
+	stats->rx_errors = (stats->rx_over_errors + stats->rx_crc_errors +
+						stats->rx_frame_errors + stats->rx_fifo_errors +
+						stats->rx_missed_errors);
+
+	stats->tx_aborted_errors = phoenix_read_reg(priv->mmio,
+												TX_EXCESSIVE_COLLISION_PACKET_COUNTER);
+	stats->tx_carrier_errors = phoenix_read_reg(priv->mmio,
+												TX_DROP_FRAME_COUNTER);
+	stats->tx_fifo_errors = phoenix_read_reg(priv->mmio,
+											 TX_DROP_FRAME_COUNTER);
+
+}
+
+static void xlr_get_ethtool_stats(struct net_device *dev,
+								  struct ethtool_stats *estats, u64 * stats)
+{
+	int i;
+	struct driver_data *priv = netdev_priv(dev);
+	unsigned long flags;
+	u32 *tmp_stats;
+
+	spin_lock_irqsave(&priv->lock, flags);
+
+	xlr_get_mac_stats(dev, &priv->stats);
+
+	spin_unlock_irqrestore(&priv->lock, flags);
+
+	tmp_stats = (u32 *) & priv->stats;
+	for (i = 0; i < PHNX_STATS_KEY_LEN; i++) {
+		*stats = (u64) * tmp_stats;
+		stats++;
+		tmp_stats++;
+	}
+}
+
+static struct ethtool_ops xlr_ethtool_ops = {
+	.get_settings = xlr_get_settings,
+	.set_settings = xlr_set_settings,
+	.get_drvinfo = xlr_get_drvinfo,
+	.get_regs_len = xlr_get_regs_len,
+	.get_regs = xlr_get_regs,
+	.get_msglevel = xlr_get_msglevel,
+	.set_msglevel = xlr_set_msglevel,
+	.nway_reset = xlr_nway_reset,
+	.get_link = xlr_get_link,
+	.get_strings = xlr_get_strings,
+	.get_stats_count = xlr_get_stats_count,
+	.get_ethtool_stats = xlr_get_ethtool_stats,
+};
+
+int gmac_init(int type, int id, int instance, unsigned long gmac_addr)
+{
+	struct net_device *dev = 0;
+	struct driver_data *priv = 0;
+	int ret;
+
+	dev = alloc_etherdev(sizeof(struct driver_data));
+	if (!dev) {
+		ret = -ENOMEM;
+		return ret;
+	}
+
+	priv = netdev_priv(dev);
+	priv->dev = dev;
+	priv->type = type;
+
+	priv->mmio = (phoenix_reg_t *) gmac_addr;
+	if (!priv->mmio) {
+		dbg_panic("Invalid mmio addr 0x0\n");
+		return EINVAL;
+	}
+
+	dbg_msg("priv->mmio=%p\n", priv->mmio);
+
+	if (priv->type == TYPE_GMAC)
+		priv->own = gmac_own;
+	else
+		priv->own = xgmac_own;
+
+	/* Initialize the net_device  MOD */
+	if (priv->own) {
+		dev->irq = phnx_mac_devices[id].irq;
+		if (request_irq(dev->irq, rmi_phnx_mac_int_handler,
+						IRQF_DISABLED, dev->name, dev)) {
+			ret = -EBUSY;
+			panic("Couldn't get mac interrupt line (%d)", dev->irq);
+		}
+	}
+
+	ether_setup(dev);
+
+	dev->base_addr = (unsigned long) priv->mmio;
+	dev->mem_end = (unsigned long) priv->mmio + PHOENIX_IO_SIZE - 1;
+
+	dev->open = rmi_phnx_mac_open;
+	dev->hard_start_xmit = rmi_phnx_mac_xmit;
+	dev->stop = rmi_phnx_mac_close;
+	dev->get_stats = rmi_phnx_mac_get_stats;
+	dev->set_multicast_list = rmi_phnx_mac_set_multicast_list;
+	dev->set_mac_address = rmi_phnx_set_mac_address;
+	dev->do_ioctl = rmi_phnx_mac_do_ioctl;
+	dev->tx_timeout = rmi_phnx_mac_tx_timeout;
+	dev->watchdog_timeo = (1000 * HZ);
+	dev->change_mtu = rmi_phnx_mac_change_mtu;
+
+	if (xlr_mac_optimized_tx) {
+		dev->tx_queue_len = 0;
+		dev->features |= NETIF_F_LLTX;
+	}
+	else {
+		dev->tx_queue_len = 10000;
+	}
+
+	SET_ETHTOOL_OPS(dev, &xlr_ethtool_ops);
+	/* Initialize the device specific driver data */
+	spin_lock_init(&priv->lock);
+
+	priv->id = id;
+	priv->instance = instance;
+	priv->type = type;
+	priv->phy_addr = gmac_id_to_phy_addr(priv);
+	priv->spill_configured = 0;
+
+	phnx_mac_get_hwaddr(dev);
+
+	/* if linux is not the owner just register the device and continue */
+	if (priv->own == 0x0)
+		goto register_netdev;
+
+	if (priv->type == TYPE_GMAC) {
+		/* configure spill area only once for all gmacs */
+		if (gmac_spill_configured[id / 4])
+			priv->spill_configured = 1;
+		else
+			gmac_spill_configured[id / 4] = 1;
+
+		rmi_phnx_gmac_init(priv);
+
+	}
+	else if (priv->type == TYPE_XGMAC) {
+		rmi_phnx_xgmac_init(priv);
+	}
+
+	phnx_mac_setup_hwaddr(priv);
+  register_netdev:
+
+	ret = register_netdev(dev);
+	if (ret) {
+		dbg_panic("Unable to register net device\n");
+	}
+	else {
+		if (priv->type == TYPE_GMAC)
+			printk("GMAC_%d initialized as %s\n", priv->instance,
+				   priv->dev->name);
+		else if (priv->type == TYPE_XGMAC)
+			printk("XGMAC_%d initialized as %s\n", priv->instance,
+				   priv->dev->name);
+	}
+
+	dbg_msg("%s: Phoenix Mac at 0x%p (mtu=%d)\n",
+			dev->name, priv->mmio, dev->mtu);
+	dev_mac[id] = dev;
+	if (priv->type == TYPE_XGMAC && priv->instance == 0)
+		dev_mac_xgs0 = priv->id;
+	if (priv->type == TYPE_GMAC && priv->instance == 0)
+		dev_mac_gmac0 = priv->id;
+
+	return 0;
+}
+
+int rmi_phnx_dev_tree_mac_init_module(void)
+{
+	struct net_device *dev = 0;
+	struct driver_data *priv = 0;
+	int i = 0, tx_station;
+	int ret = 0, num_gmacs;
+	struct proc_dir_entry *entry;
+	unsigned long gmac_addr[PHOENIX_MAX_MACS], xgmac_addr;
+	unsigned int gmac_list = 0x0, xgmac_list = 0x0, instance = 0;
+
+	if (!dev_tree_en)
+		return 0;
+
+	/* Initialize cached variable indicating board type XLS vs XLR */
+	rmi_board_type = is_xls();
+
+#ifdef CONFIG_PHOENIX_GMAC_NAPI
+	/* Initialize spinlock protecting NAPI msgring_config access */
+	spin_lock_init(&xlr_napi_msgrng_lock);
+#endif /* CONFIG_PHOENIX_GMAC_NAPI */
+
+	/* calculate mac spill sizes */
+	if (!max_num_desc)
+		setup_mac_spill_sizes(MAX_NUM_DESC);
+
+	entry = create_proc_read_entry("xlr_mac", 0 /* def mode */ ,
+								   0 /* no parent */ ,
+								   xlr_mac_proc_read
+								   /* proc read function */ ,
+								   0 /* no client data */ );
+	if (!entry) {
+		printk("[%s]: Unable to create proc read entry for xlr_mac!\n",
+			   __func__);
+	}
+
+	/* Get the gmac properties from fdt */
+	fdt_get_gmac_ppties(&gmac_list, gmac_addr, &gmac_own);
+	for (i = 0; i < PHOENIX_MAX_MACS; i++) {
+		if (!(gmac_list & (1 << i)))
+			continue;
+		gmac_init(TYPE_GMAC, i, instance++, gmac_addr[i]);
+		dbg_msg("Registering phnx_gmac[%d]\n", i);
+	}
+	num_gmacs = i;
+
+
+	if (gmac_list & 0xf) {
+		tx_station = ((is_xls() == 0) ? TX_STN_GMAC : TX_STN_GMAC0);
+		if (gmac_own) {
+			if (register_msgring_handler
+				(tx_station, rmi_phnx_dev_tree_mac_msgring_handler, NULL))
+				panic("Couldn't register msgring handler for TX_STN_GMAC0\n");
+		}
+		else {
+			if (register_msgring_handler
+				(tx_station, rmi_phnx_dom_msgring_handler, NULL))
+				panic("Couldn't register msgring handler for TX_STN_GMAC0\n");
+		}
+	}
+
+	if (gmac_list >> 4) {
+		if (gmac_own) {
+			if (register_msgring_handler
+				(TX_STN_GMAC1, rmi_phnx_dev_tree_mac_msgring_handler, NULL))
+				panic("Couldn't register msgring handler for TX_STN_GMAC0\n");
+		}
+		else {
+			if (register_msgring_handler
+				(TX_STN_GMAC1, rmi_phnx_dom_msgring_handler, NULL))
+				panic("Couldn't register msgring handler for TX_STN_GMAC0\n");
+		}
+	}
+
+
+	printk("Gmac list = 0x%x\n", gmac_list);
+
+	/* Configure xgmacs */
+	instance = 0;
+	for (i = 0; i < PHOENIX_MAX_XGMACS; i++) {
+		fdt_get_xgmac_ppties(i, &xgmac_list, &xgmac_addr, &xgmac_own);
+		if (!xgmac_list)
+			continue;
+		gmac_init(TYPE_XGMAC, num_gmacs + i, instance++, xgmac_addr);
+		dbg_msg("Registering phnx_xgmac[%d]\n", i);
+
+		if (i == 0x0) {
+			if (register_msgring_handler
+				(TX_STN_XGS_0, rmi_phnx_dev_tree_mac_msgring_handler, NULL)) {
+				panic("Couldn't register msgring handler for TX_STN_GMAC0\n");
+			}
+		}
+		else if (i == 1) {
+			if (register_msgring_handler
+				(TX_STN_XGS_1, rmi_phnx_dev_tree_mac_msgring_handler, NULL)) {
+				panic("Couldn't register msgring handler for TX_STN_GMAC0\n");
+			}
+		}
+
+	}
+
+	printk("XGmac list = 0x%x\n", xgmac_list);
+
+	for (i = 0; i < MAC_FRIN_WORK_NUM; i++)
+		INIT_WORK(&mac_frin_replenish_work[i], mac_frin_replenish);
+
+
+#ifdef CONFIG_PHOENIX_IP_FLOW_AFFINITY
+	/* initialize cpu skb queues */
+	cpu_tx_queue_init();
+#endif /* CONFIG_PHOENIX_IP_FLOW_AFFINITY */
+
+	if (gmac_own == 0x0)
+		return ret;
+
+	if ((xlr_board_atx_v() || xlr_board_atx_iv_b())) {	/* MOD */
+		/* on atx-v and atx-iv-b read rgmii interrupt at least once */
+		struct driver_data data;
+		data.phy_addr = 3;
+		data.instance = 3;
+		data.type = TYPE_GMAC;
+		rmi_phnx_mac_mii_read(&data, 26, 0);
+	}
+
+	for (i = 0; i < PHOENIX_MAX_MACS; i++) {
+		dev = dev_mac[i];
+		if (!dev)
+			continue;
+		priv = netdev_priv(dev);
+		phnx_mac_set_rx_mode(dev);
+		phoenix_write_reg(priv->mmio, R_INTMASK,
+						  (1 << O_INTMASK__TxIllegal) |
+						  (((priv->instance & 0x3) ==
+							0) << O_INTMASK__MDInt) | (1 <<
+													   O_INTMASK__TxFetchError)
+						  | (1 << O_INTMASK__P2PSpillEcc) | (1 <<
+															 O_INTMASK__TagFull)
+						  | (1 << O_INTMASK__Underrun) | (1 <<
+														  O_INTMASK__Abort));
+	}
+
+	if (ret < 0) {
+		dbg_panic("Error, ret = %d\n", ret);
+	}
+
+
+	return ret;
+}
+
+void rmi_phnx_dev_tree_mac_exit_module(void)
+{
+	struct net_device *dev;
+	int idx;
+
+	for (idx = 0; idx < PHOENIX_MAX_MACS; idx++) {
+		dev = dev_mac[idx];
+		if (!dev)
+			continue;
+
+		unregister_netdev(dev);
+		free_netdev(dev);
+	}
+}
+
+module_init(rmi_phnx_dev_tree_mac_init_module);
+module_exit(rmi_phnx_dev_tree_mac_exit_module);
+
+/*************************************************************************
+ * TODO:
+ *     o Currently, if Tx completes do not come back, Tx hangs for ever. Though it is good
+ *       for debugging, there should be a timeout mechanism.
+ *     o Right now, all cpu-threads across cpus are serialized for transmitting
+ *       packets. However, message_send is "atomic", hence all of them should
+ *       transmit without contending for the lock. some like per-cpu, per device lock 
+ *       and handling tx complete on the cpu that did the transmit
+ *     o use fetchadd for stat variable. currently, it not even atomic
+ *************************************************************************/
diff --git a/drivers/net/phoenix_user_mac.c b/drivers/net/phoenix_user_mac.c
new file mode 100644
index 0000000..a574666
--- /dev/null
+++ b/drivers/net/phoenix_user_mac.c
@@ -0,0 +1,939 @@
+/*********************************************************************
+
+  Copyright 2003-2006 Raza Microelectronics, Inc. (RMI). All rights
+  reserved.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+  1. Redistributions of source code must retain the above copyright
+  notice, this list of conditions and the following disclaimer.
+  2. Redistributions in binary form must reproduce the above copyright
+  notice, this list of conditions and the following disclaimer in
+  the documentation and/or other materials provided with the
+  distribution.
+
+  THIS SOFTWARE IS PROVIDED BY Raza Microelectronics, Inc. ``AS IS'' AND
+  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL RMI OR CONTRIBUTORS BE LIABLE
+  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+  THE POSSIBILITY OF SUCH DAMAGE.
+
+  *****************************#RMI_2#**********************************/
+
+#include <linux/kernel.h>
+#include <linux/interrupt.h>
+#include <linux/fs.h>
+#include <linux/mm.h>
+#include <linux/vmalloc.h>
+#include <linux/poll.h>
+#include <linux/workqueue.h>
+#include <linux/proc_fs.h>
+#include <linux/cpumask.h>
+
+#include <asm/uaccess.h>
+#include <asm/mman.h>
+#include <asm/atomic.h>
+#include <asm/smp.h>
+
+#include <asm/rmi/pic.h>
+#include <asm/rmi/msgring.h>
+#include <asm/rmi/debug.h>
+#include <asm/rmi/sim.h>
+#include <asm/rmi/devices.h>
+#include <asm/rmi/phnx_user_mac.h>
+#include <asm-mips/rmi/gpio.h>
+#include <asm/rmi/rmios_user_mac.h>
+#include <asm/rmi/proc.h>
+
+#include <asm/rmi/user/phnx_user_mac.h>
+
+#ifdef dbg_msg
+#undef dbg_msg
+#endif
+#define dbg_msg(fmt, args...) printk(fmt, ##args)
+
+/* 
+ * this flag will be set by rmi spi4 driver, it indicates
+ * whether spi4 daughter cards are present on the board or not
+*/
+#ifdef CONFIG_PHOENIX_SPI4
+extern unsigned int g_spi4_card_flag;
+#else /* CONFIG_PHOENIX_SPI4 */
+static unsigned int g_spi4_card_flag;
+#endif /* CONFIG_PHOENIX_SPI4 */
+
+extern int xlr_loader_support, xlr_loader_sharedcore;
+
+extern void *phoenix_psb_shm;
+extern unsigned long phoenix_psb_shm_size;
+
+static unsigned long phoenix_psb_shm_paddr;
+static int user_mac_major;
+
+struct user_mac_data *user_mac;
+/* use per cpu data structures for the Queues below */
+static wait_queue_head_t rx_wqs[32];
+static wait_queue_head_t tx_wqs[32];
+static volatile unsigned long tx_drain_in_progress[32];
+static volatile unsigned long tx_in_progress[32];
+static uint32_t user_mac_base[32];
+static uint32_t user_mac_max_macs;
+extern int xlr_hybrid;
+
+#define USER_MAC_TX_THRESHOLD 1
+
+#define MAC_CRC_LEN 4
+#define BYTE_OFFSET 2
+#define MAC_PREPAD_LEN 32
+
+#define MAC_DEV_NULL_BUCKET 127
+
+static __inline__ int user_mac_ptr2index(unsigned long addr)
+{
+	int index = (addr - __pa(user_mac->pkt_data)) / USER_MAC_PKT_BUF_SIZE;
+
+	if (index < 0 || index >= MAX_USER_MAC_PKTS) {
+		printk("[%s]: bad index=%d, addr=%lx, pkt_data=%p\n",
+			   __func__, index, addr, &user_mac->pkt_data);
+		return -1;
+	}
+	return index;
+}
+
+#define MAC_TX_DESC_ALIGNMENT (SMP_CACHE_BYTES + MAC_PREPAD_LEN - 1)
+
+#define CTRL_RES0           0
+#define CTRL_RES1           1
+#define CTRL_REG_FREE       2
+#define CTRL_JUMBO_FREE     3
+#define CTRL_CONT           4
+#define CTRL_EOP            5
+#define CTRL_START          6
+#define CTRL_SNGL           7
+
+static __inline__ int user_mac_send_frin_num_xgs_pkts(void)
+{
+	int num_xgmac_pkts = MAX_USER_MAC_FRIN_PKTS / 2.5;
+
+	return num_xgmac_pkts;
+}
+
+static __inline__ int user_mac_send_frin_num_gmac_pkts(void)
+{
+	return user_mac_send_frin_num_xgs_pkts() / 2;
+}
+
+static __inline__ int user_mac_send_frin_is_desc_gmac(int index)
+{
+	int start_index = 0, end_index = 0;
+
+	start_index = 0;
+	end_index = start_index + user_mac_send_frin_num_gmac_pkts();
+	return ((index >= start_index) && (index < end_index)) ? 1 : 0;
+}
+
+static __inline__ int user_mac_send_frin_is_desc_xgs0(int index)
+{
+	int start_index = 0, end_index = 0;
+
+	start_index = user_mac_send_frin_num_gmac_pkts();
+	end_index = start_index + user_mac_send_frin_num_xgs_pkts();
+	return ((index >= start_index) && (index < end_index)) ? 1 : 0;
+}
+
+static __inline__ int user_mac_send_frin_is_desc_xgs1(int index)
+{
+	int start_index = 0, end_index = 0;
+
+	start_index =
+		user_mac_send_frin_num_gmac_pkts() +
+		user_mac_send_frin_num_xgs_pkts();
+	end_index = start_index + user_mac_send_frin_num_xgs_pkts();
+	return ((index >= start_index) && (index < end_index)) ? 1 : 0;
+}
+
+static __inline__ int user_mac_send_frin_is_desc_gmac0(int index)
+{
+	int start_index = 0, end_index = 0;
+
+	start_index = 0;
+	end_index = start_index + user_mac_send_frin_num_xgs_pkts();
+	return ((index >= start_index) && (index < end_index)) ? 1 : 0;
+}
+
+static __inline__ int user_mac_send_frin_is_desc_gmac1(int index)
+{
+	int start_index = 0, end_index = 0;
+
+	start_index = user_mac_send_frin_num_xgs_pkts();
+	end_index = start_index + user_mac_send_frin_num_xgs_pkts();
+	return ((index >= start_index) && (index < end_index)) ? 1 : 0;
+}
+
+static __inline__ int user_mac_send_frin_stid(int index)
+{
+	if (is_xls2xx()) {
+		return MSGRNG_STNID_GMAC0_FR;
+	}
+	else if (is_xls()) {
+		if (user_mac_send_frin_is_desc_gmac0(index))
+			return MSGRNG_STNID_GMAC0_FR;
+		if (user_mac_send_frin_is_desc_gmac1(index))
+			return MSGRNG_STNID_GMAC1_FR;
+	}
+	else if (xlr_board_atx_ii() || xlr_board_atx_ii_b() || (g_spi4_card_flag)) {
+		if (user_mac_send_frin_is_desc_gmac(index))
+			return MSGRNG_STNID_GMACRFR_0;
+		if (user_mac_send_frin_is_desc_xgs0(index))
+			return MSGRNG_STNID_XMAC0RFR;
+		if (user_mac_send_frin_is_desc_xgs1(index))
+			return MSGRNG_STNID_XMAC1RFR;
+	}
+	return MSGRNG_STNID_GMACRFR_0;
+}
+
+static void user_mac_send_frin(void)
+{
+	struct msgrng_msg msg;
+	struct packet_data *packet_data = user_mac->pkt_data;
+	int i = 0;
+	unsigned long addr = 0, mflags = 0;
+	int stid = 0;
+	int host_gen_num_pkts = (MAX_USER_MAC_PKTS - MAX_USER_MAC_FRIN_PKTS) / 32;
+
+	msgrng_flags_save(mflags);
+
+	if (xlr_board_atx_ii() || xlr_board_atx_ii_b()) {
+		printk
+			("[%s]: sending %d free descriptors to gmacs and %d to each xgs...\n",
+			 __func__, user_mac_send_frin_num_gmac_pkts(),
+			 user_mac_send_frin_num_xgs_pkts());
+	}
+	else if (g_spi4_card_flag) {
+		printk
+			("[%s]: sending %d free descriptors to gmacs and %d to each spi4...\n",
+			 __func__, user_mac_send_frin_num_gmac_pkts(),
+			 user_mac_send_frin_num_xgs_pkts());
+	}
+	else {
+		printk("[%s]: sending %d free descriptors to gmacs...\n",
+			   __func__, MAX_USER_MAC_FRIN_PKTS);
+	}
+
+	for (i = 0; i < MAX_USER_MAC_FRIN_PKTS; i++) {
+		addr = __pa(&packet_data[i].data);
+		user_mac->pkt_desc[i].free = 0;
+
+		msg.msg0 = (uint64_t) addr & ~(SMP_CACHE_BYTES - 1);
+		msg.msg1 = msg.msg2 = msg.msg3 = 0;
+
+		stid = user_mac_send_frin_stid(i);
+
+		if (xlr_board_atx_i()) {
+			if ((stid == MSGRNG_STNID_XMAC0RFR)
+				&& (!(g_spi4_card_flag & 0x01))) {
+				/*spi4-0 card is not present */
+				continue;
+			}
+			else if ((stid == MSGRNG_STNID_XMAC1RFR)
+					 && (!(g_spi4_card_flag & 0x02))) {
+				/*spi4-1 card is not present */
+				continue;
+			}
+		}
+
+		do {
+			if (!message_send_retry(1, MSGRNG_CODE_MAC, stid, &msg))
+				break;
+			printk
+				("[%s:%d]: retrying free_desc[%d] message send to stid=%d\n",
+				 __func__, __LINE__, i, stid);
+		} while (1);
+
+		phnx_inc_counter(USER_MAC_FRIN);
+	}
+	msgrng_flags_restore(mflags);
+	printk("[%s]:...done\n", __func__);
+
+	for (i = 0; i < 32; i++) {
+		user_mac->host_pkt_next_free[i] =
+			MAX_USER_MAC_FRIN_PKTS + (i * host_gen_num_pkts);
+	}
+
+	for (i = MAX_USER_MAC_FRIN_PKTS; i < MAX_USER_MAC_PKTS; i++)
+		user_mac->pkt_desc[i].free = 1;
+
+	printk
+		("[%s]: packet_data[first].data=%lx, packet_data[last].data=%lx\n",
+		 __func__, __pa(&packet_data[0].data),
+		 __pa(&packet_data[MAX_USER_MAC_PKTS - 1].data));
+}
+
+static void user_mac_send_frin_xgmac(void)
+{
+	struct msgrng_msg msg;
+	struct packet_data *packet_data = user_mac->pkt_data;
+	int i = 0;
+	unsigned long addr = 0, mflags = 0;
+	int stid = 0;
+	int host_gen_num_pkts = (MAX_USER_MAC_PKTS - MAX_USER_MAC_FRIN_PKTS) / 32;
+
+	if (!xlr_board_atx_ii())
+		return;
+
+	msgrng_flags_save(mflags);
+
+	printk("[%s]: sending %d free descriptors to each xgs...\n",
+		   __func__, user_mac_send_frin_num_xgs_pkts());
+
+	for (i = 0; i < MAX_USER_MAC_FRIN_PKTS; i++) {
+
+		stid = user_mac_send_frin_stid(i);
+
+		if (!(stid == MSGRNG_STNID_XMAC0RFR || stid == MSGRNG_STNID_XMAC1RFR))
+			continue;
+
+		addr = __pa(&packet_data[i].data);
+		user_mac->pkt_desc[i].free = 0;
+
+		msg.msg0 = (uint64_t) addr & ~(SMP_CACHE_BYTES - 1);
+		msg.msg1 = msg.msg2 = msg.msg3 = 0;
+
+		do {
+			if (!message_send_retry(1, MSGRNG_CODE_MAC, stid, &msg))
+				break;
+			printk
+				("[%s:%d]: retrying free_desc[%d] message send to stid=%d\n",
+				 __func__, __LINE__, i, stid);
+		} while (1);
+
+		phnx_inc_counter(USER_MAC_FRIN);
+	}
+	msgrng_flags_restore(mflags);
+	printk("[%s]:...done\n", __func__);
+
+	for (i = 0; i < 32; i++) {
+		user_mac->host_pkt_next_free[i] =
+			MAX_USER_MAC_FRIN_PKTS + (i * host_gen_num_pkts);
+	}
+
+	for (i = MAX_USER_MAC_FRIN_PKTS; i < MAX_USER_MAC_PKTS; i++)
+		user_mac->pkt_desc[i].free = 1;
+
+	printk
+		("[%s]: packet_data[first].data=%lx, packet_data[last].data=%lx\n",
+		 __func__, __pa(&packet_data[0].data),
+		 __pa(&packet_data[MAX_USER_MAC_PKTS - 1].data));
+}
+
+struct xlr_cpu_stat
+{
+	unsigned long long msgring_pic_int;
+	unsigned long long msgring_int;
+	unsigned long long msgring_cycles;
+};
+static struct xlr_cpu_stat xlr_cpu_stats[32];
+
+void phoenix_cpu_stat_update_msgring_int(void)
+{
+	int cpu = hard_smp_processor_id();
+
+	xlr_cpu_stats[cpu].msgring_int++;
+}
+
+void phoenix_cpu_stat_update_msgring_cycles(__u32 cycles)
+{
+	int cpu = hard_smp_processor_id();
+
+	xlr_cpu_stats[cpu].msgring_cycles += cycles;
+}
+
+void phoenix_user_mac_update_time(void)
+{
+	int cpu = hard_smp_processor_id();
+
+	xlr_cpu_stats[cpu].msgring_pic_int++;
+
+	if (user_mac) {
+		user_mac->time.lo++;
+		if (!user_mac->time.lo)
+			user_mac->time.hi++;
+	}
+}
+
+static int user_mac_open(struct inode *inode, struct file *filp)
+{
+	filp->private_data = NULL;
+	return 0;
+}
+
+static int user_mac_mmap(struct file *file, struct vm_area_struct *vma)
+{
+	unsigned long offset = vma->vm_pgoff << PAGE_SHIFT;
+	unsigned long shm_addr = phoenix_psb_shm_paddr;
+	unsigned long shm_size = phoenix_psb_shm_size;
+	unsigned long size = 0;
+	unsigned long vm_size = vma->vm_end - vma->vm_start;
+
+	dbg_msg
+		("[%s]: shm_addr=%lx, shm_size=%lx, offset = %lx, vm_start=%lx, vm_size=%lx, vm_flags=%lx, "
+		 "vm_page_prot=%lx\n", __func__, shm_addr, shm_size, offset,
+		 vma->vm_start, vm_size, vma->vm_flags,
+		 pgprot_val(vma->vm_page_prot));
+
+	if (vma->vm_start != (unsigned long) PHNX_USER_MAC_MMAP_VIRT_START)
+		return -EINVAL;
+
+	if (!shm_addr)
+		return -ENXIO;
+
+	if (offset >= shm_size)
+		return -ESPIPE;
+
+	if (vma->vm_flags & VM_LOCKED)
+		return -EPERM;
+
+	size = shm_size - offset;
+	if (vm_size > size)
+		return -ENOSPC;
+
+	vma->vm_flags |= (VM_RESERVED | VM_IO);
+
+	if (remap_pfn_range
+		(vma, vma->vm_start, (shm_addr >> PAGE_SHIFT), size,
+		 vma->vm_page_prot))
+		return -EAGAIN;
+
+	return 0;
+}
+
+static int is_4xx_8_gmacs(void)
+{
+	uint32_t *gpio_base = (uint32_t *) (DEFAULT_PHOENIX_IO_BASE +
+										PHOENIX_IO_GPIO_OFFSET);
+	if (((gpio_base[PHOENIX_GPIO_FUSE_BANK_REG] & (1 << 28)) == 0) &&
+		((gpio_base[PHOENIX_GPIO_FUSE_BANK_REG] & (1 << 29)) == 0)) {
+		/*Below bits are set when ports are disabled.
+		   28 - GMAC7
+		   29 - GMAC6
+		   30 - GMAC5
+		   31 - GMAC4
+		 */
+		/*We found an XLS-408 with 8 gmacs */
+		return 1;
+	}
+	return 0;
+}
+
+static uint32_t get_mac_base(uint32_t dev)
+{
+	if (is_xls2xx()) {
+		/*2xx xls has 4 gmacs */
+		if (!(dev >= 0 && dev <= 3))
+			return 0;
+		else
+			return dev + 0xc;
+	}
+	else if (is_xls4xx()) {
+		if (is_4xx_8_gmacs()) {
+			/*4xx xls with 8 gmacs */
+			if (!((dev >= 0 && dev <= 7)))
+				return 0;
+			if (dev < 4)
+				return dev + 0xc;
+			else
+				return dev + 0x1c;
+		}
+		else {
+			/*4xx xls with 6 gmacs */
+			if (!((dev >= 0 && dev <= 5)))
+				return 0;
+			if (dev < 4)
+				return dev + 0xc;
+			else
+				return dev + 0x1c;
+		}
+	}
+	else if (is_xls6xx()) {
+		/*6xx has 8 gmacs */
+		if (!((dev >= 0 && dev <= 7)))
+			return 0;
+		if (dev < 4)
+			return dev + 0xc;
+		else
+			return dev + 0x1c;
+	}
+	else {
+		/*xlr has max 4 gmacs and 2 spi4/xls slots */
+		if (xlr_board_atx_i()) {
+			/*ATX-I has 4GMAC and 2 SPI4 */
+			if (!(dev >= 0 && dev <= 5))
+				return 0;
+			if (dev < 4)
+				return dev + 0xc;
+			else if (dev == 4)
+				return 0x10;
+			else if (dev == 5)
+				return 0x12;
+
+		}
+		else if (xlr_board_atx_ii() && !xlr_board_atx_ii_b()) {
+			/*ATX-II has 2GMAC and 2XGMAC */
+			if (!(dev >= 0 && dev <= 3))
+				return 0;
+			if (dev < 2)
+				return dev + 0xc;
+			else if (dev == 2)
+				return 0x11;
+			else if (dev == 3)
+				return 0x13;
+
+		}
+		else if (xlr_board_atx_ii_b()) {
+			/*ATX-IIB has 4GMAC and 2XGMAC */
+			if (!(dev >= 0 && dev <= 5))
+				return 0;
+			if (dev < 4)
+				return dev + 0xc;
+			else if (dev == 4)
+				return 0x11;
+			else if (dev == 5)
+				return 0x13;
+
+
+		}
+		else if (xlr_board_atx_iii() || xlr_board_atx_iv()) {
+			/*ATX-III / ATX-IV has 4GMAC */
+			if (!(dev >= 0 && dev <= 3))
+				return 0;
+			return dev + 0xc;
+		}
+		else if (xlr_board_atx_iv_b() || xlr_board_atx_v()) {
+			/*ATX-IV_B / ATX-V has 3GMAC */
+			if (!(dev >= 0 && dev <= 2))
+				return 0;
+			return dev + 0xc;
+		}
+	}
+	return 0;
+}
+
+void user_mac_fill_mac_base(void)
+{
+	int i = 0;
+	for (i = 0; i < 32; i++) {
+		user_mac_base[i] = get_mac_base(i);
+		if (!user_mac_base[i]) {
+			user_mac_max_macs = i;
+			break;
+		}
+	}
+}
+
+int user_mac_ioctl(struct inode *inode, struct file *filp, unsigned int cmd,
+				   unsigned long arg)
+{
+	switch (cmd) {
+
+		case USER_MAC_IOC_GSHMPHYS:
+			{
+				put_user((unsigned int) phoenix_psb_shm_paddr,
+						 (unsigned int *) arg);
+			}
+			break;
+
+		case USER_MAC_IOC_GSHMVIRT:
+			{
+				put_user((unsigned int) (unsigned long) phoenix_psb_shm,
+						 (unsigned int *) arg);
+			}
+			break;
+
+		case USER_MAC_IOC_GSHMSIZE:
+			{
+				put_user((unsigned int) phoenix_psb_shm_size,
+						 (unsigned int *) arg);
+			}
+			break;
+
+		case USER_MAC_IOC_GMMAP_START:
+			{
+				put_user((unsigned int) PHNX_USER_MAC_MMAP_VIRT_START,
+						 (unsigned int *) arg);
+			}
+			break;
+
+		case USER_MAC_IOC_GREAD_REG:
+			{
+				__u32 *ptr = (__u32 *) arg;
+				__u32 dev = 0, reg = 0, value = 0, base = 0;
+				phoenix_reg_t *mmio = 0;
+
+				get_user(dev, ptr + 0);
+				get_user(reg, ptr + 1);
+
+				if (dev < 31)
+					base = user_mac_base[dev];
+
+				if (!base || (reg > (0x1000 >> 2))) {
+					printk("[%s]: bad args, dev=0x%x, reg=0x%x\n",
+						   __func__, dev, reg);
+					value = 0xdeadbeef;
+				}
+				else {
+					mmio = phoenix_io_mmio(base << 12);
+					printk("\nMMIO %#lx, REG %#lx\n", (unsigned long) mmio,
+						   (unsigned long) reg);
+					printk("\nReading @ Address %#lx-->%#x\n",
+						   (unsigned long) &mmio[reg], mmio[reg]);
+					value = phoenix_read_reg(mmio, reg);
+					dbg_msg
+						("[%s]: dev=0x%x, reg=0x%x, value=0x%x\n",
+						 __func__, dev, reg, value);
+				}
+				put_user(value, ptr + 2);
+			}
+			break;
+
+		case USER_MAC_IOC_SWRITE_REG:
+			{
+				__u32 *ptr = (__u32 *) arg;
+				__u32 dev = 0, reg = 0, value = 0, base = 0;
+				phoenix_reg_t *mmio = 0;
+
+				get_user(dev, ptr + 0);
+				get_user(reg, ptr + 1);
+				get_user(value, ptr + 2);
+				if (dev < 31)
+					base = user_mac_base[dev];
+				if (!base || (reg > (0x1000 >> 2))) {
+					printk("[%s]: bad args, dev=0x%x, reg=0x%x\n",
+						   __func__, dev, reg);
+				}
+				else {
+					dbg_msg
+						("[%s]: dev=0x%x, reg=0x%x, value=0x%x\n",
+						 __func__, dev, reg, value);
+
+					mmio = phoenix_io_mmio(base << 12);
+					phoenix_write_reg(mmio, reg, value);
+				}
+
+			}
+			break;
+
+		case USER_MAC_IOC_GPHYS_CPU_PRESENT_MAP:
+			{
+				put_user((unsigned int) phys_cpu_present_map.bits[0],
+						 (unsigned int *) arg);
+			}
+			break;
+
+		case USER_MAC_IOC_GCPU_ONLINE_MAP:
+			{
+				put_user((unsigned int) cpu_online_map.bits[0],
+						 (unsigned int *) arg);
+			}
+			break;
+
+		case USER_MAC_IOC_HYBRID_MODE_SETUP:
+			{
+				put_user((unsigned int) xlr_hybrid, (unsigned int *) arg);
+			}
+			break;
+
+		default:
+			{
+				printk("ioctl(): invalid command=0x%x\n", cmd);
+				return 0;
+			}
+
+	}
+
+	return 0;
+}
+
+long user_mac_compat_ioctl(struct file *filp, unsigned int cmd,
+						   unsigned long arg)
+{
+	unsigned long ret = -1;
+	lock_kernel();
+	ret = user_mac_ioctl(NULL, filp, cmd, arg);
+	unlock_kernel();
+	if (ret) {
+		printk("user_mac_ioctl returned with an error.\n");
+		return -ENOIOCTLCMD;
+	}
+	return ret;
+}
+
+  // called only when the reference count (maintained in inode) is zero
+static int user_mac_release(struct inode *inode, struct file *filp)
+{
+
+	return 0;
+}
+
+struct file_operations user_mac_fops = {
+	.mmap = user_mac_mmap,
+	.open = user_mac_open,
+	.ioctl = user_mac_ioctl,
+	.compat_ioctl = user_mac_compat_ioctl,
+	.release = user_mac_release,
+};
+
+static int proc_read_count;
+
+extern unsigned long long phnx_tlb_stats[];
+extern unsigned long long phnx_btlb_stats[];
+
+extern unsigned long long nr_cpu_context_switches(int cpu);
+
+extern struct psb_info *prom_info;
+__u64 xlr_cp2_exceptions[32];
+
+static int xlr_cpu_proc_read(char *page, char **start, off_t off,
+							 int count, int *eof, void *data)
+{
+	int i = 0;
+	int len = 0;
+	off_t begin = 0;
+
+	len += sprintf(page + len,
+				   "CPU Frequency: %d HZ\n",
+				   (unsigned int) prom_info->cpu_frequency);
+	if (!proc_pos_check(&begin, &len, off, count))
+		goto out;
+
+	for (i = 0; i < 32; i++) {
+
+		if (!xlr_cp2_exceptions[i])
+			continue;
+
+		len += sprintf(page + len,
+					   "cop2_exp: %d %d %d %llx\n",
+					   i, user_mac->time.hi, user_mac->time.lo,
+					   (unsigned long long) xlr_cp2_exceptions[i]);
+		if (!proc_pos_check(&begin, &len, off, count))
+			goto out;
+	}
+
+	for (i = 0; i < 32; i++) {
+
+		if (!xlr_cpu_stats[i].msgring_pic_int
+			&& !xlr_cpu_stats[i].msgring_int)
+			continue;
+
+		len += sprintf(page + len,
+					   "msgring: %d %d %d %llx %llx %llx\n",
+					   i, user_mac->time.hi, user_mac->time.lo,
+					   xlr_cpu_stats[i].msgring_pic_int,
+					   xlr_cpu_stats[i].msgring_int,
+					   xlr_cpu_stats[i].msgring_cycles);
+		if (!proc_pos_check(&begin, &len, off, count))
+			goto out;
+	}
+
+	for (i = 0; i < 32; i++) {
+
+		if (!phnx_tlb_stats[i] && !phnx_btlb_stats[i])
+			continue;
+
+		len += sprintf(page + len,
+					   "tlb: %d %d %d %llx %llx\n",
+					   i, user_mac->time.hi, user_mac->time.lo,
+					   phnx_tlb_stats[i], phnx_btlb_stats[i]);
+		if (!proc_pos_check(&begin, &len, off, count))
+			goto out;
+	}
+
+	*eof = 1;
+
+  out:
+	*start = page + (off - begin);
+	len -= (off - begin);
+	if (len > count)
+		len = count;
+	if (len < 0)
+		len = 0;
+
+	return len;
+}
+
+static int user_mac_proc_read(char *page, char **start, off_t off,
+							  int count, int *eof, void *data)
+{
+	int i = 0;
+	int len = 0;
+	off_t begin = 0;
+
+	proc_read_count++;
+
+	len += sprintf(page + len,
+				   "\n*************** USER MAC STATISTICS ****************\n"
+				   "cpu_%d: proc_read_count = %d\n",
+				   smp_processor_id(), proc_read_count);
+	if (!proc_pos_check(&begin, &len, off, count))
+		goto out;
+
+	len +=
+		sprintf(page + len,
+				"\nshm_paddr=%lx, shm_size=%lx, mmap_virt_start=%x\n"
+				"sizeof(user_mac_data)=0x%lx\n", phoenix_psb_shm_paddr,
+				phoenix_psb_shm_size, PHNX_USER_MAC_MMAP_VIRT_START,
+				sizeof(struct user_mac_data));
+	if (!proc_pos_check(&begin, &len, off, count))
+		goto out;
+
+	len += sprintf(page + len, "\nPer CPU TLB Stats: \n");
+	if (!proc_pos_check(&begin, &len, off, count))
+		goto out;
+
+	for (i = 0; i < 32; i++) {
+		if (!phnx_tlb_stats[i] && !phnx_btlb_stats[i])
+			continue;
+
+		len +=
+			sprintf(page + len,
+					"cpu_%d: reg tlb = %llu.0 big tlb = %llu.0\n", i,
+					phnx_tlb_stats[i], phnx_btlb_stats[i]);
+		if (!proc_pos_check(&begin, &len, off, count))
+			goto out;
+	}
+
+	len +=
+		sprintf(page + len,
+				"\noffsetof(time)=0x%lx, time.hi=%u, time.lo=%u\n",
+				offsetof(struct user_mac_data, time),
+				(unsigned int) user_mac->time.hi,
+				(unsigned int) user_mac->time.lo);
+	if (!proc_pos_check(&begin, &len, off, count))
+		goto out;
+
+	len += sprintf(page + len, "\n");
+	if (!proc_pos_check(&begin, &len, off, count))
+		goto out;
+
+	*eof = 1;
+
+  out:
+	*start = page + (off - begin);
+	len -= (off - begin);
+	if (len > count)
+		len = count;
+	if (len < 0)
+		len = 0;
+
+	return len;
+}
+
+struct xlr_user_mac_config xlr_user_mac;
+
+static int __init xlr_user_mac_setup(char *str)
+{
+	if ((strcmp(str, "=fast_syscall") == 0)
+		|| (strcmp(str, "fast_syscall") == 0)) {
+		xlr_user_mac.fast_syscall = 1;
+		printk("XLR: user_mac configured with fast syscalls\n");
+	}
+	else {
+		printk("XLR: user_mac configured with unknown args \"%s\"\n", str);
+	}
+
+	return 1;
+}
+
+early_param("xlr_user_mac", xlr_user_mac_setup);
+
+static int user_mac_init(void)
+{
+	int i = 0;
+	struct proc_dir_entry *entry;
+
+	/* if support for loading apps on same core as Linux is enabled */
+	if (xlr_loader_support && xlr_loader_sharedcore)
+		return -EINVAL;
+
+	user_mac = (struct user_mac_data *) phoenix_psb_shm;
+	if (!user_mac) {
+		panic("[%s]: Null Shared Memory Pointer?\n", __func__);
+	}
+
+	if (sizeof(struct user_mac_data) > phoenix_psb_shm_size) {
+		panic
+			("[%s]: psb shared memory is too small: user_mac_data=0x%lx, psb_shm_size=0x%lx\n",
+			 __func__, sizeof(struct user_mac_data),
+			 (unsigned long) phoenix_psb_shm_size);
+	}
+
+	user_mac_major =
+		register_chrdev(XLR_USER_MAC_MAJOR, PHNX_USER_MAC_CHRDEV_NAME,
+						&user_mac_fops);
+	if (user_mac_major < 0) {
+		printk("user_mac_init() register_chrdev() failed\n");
+		return user_mac_major;
+	}
+	printk("Registered user_mac driver: major=%d\n", XLR_USER_MAC_MAJOR);
+
+	for (i = 0; i < 32; i++) {
+		ufifo_init(&user_mac->rxqs[i]);
+		ufifo_init(&user_mac->txqs[i]);
+
+		init_waitqueue_head(&rx_wqs[i]);
+		init_waitqueue_head(&tx_wqs[i]);
+
+		clear_bit(0, &tx_drain_in_progress[i]);
+		clear_bit(0, &tx_in_progress[i]);
+	}
+
+	for (i = 0; i < MAX_USER_MAC_PKTS; i++)
+		user_mac->pkt_desc[i].free = 1;
+
+	phoenix_psb_shm_paddr = __pa(phoenix_psb_shm);
+
+	if (xlr_hybrid_user_mac())
+		user_mac_send_frin();
+	else if (xlr_hybrid_user_mac_xgmac())
+		user_mac_send_frin_xgmac();
+
+	entry =
+		create_proc_read_entry(PHNX_USER_MAC_CHRDEV_NAME, 0 /* def mode */ ,
+							   0 /* no parent */ ,
+							   user_mac_proc_read /* proc read function */ ,
+							   0 /* no client data */);
+
+	if (!entry) {
+		printk("[%s]: Unable to create proc read entry for %s!\n",
+			   __func__, PHNX_USER_MAC_CHRDEV_NAME);
+	}
+
+	entry = create_proc_read_entry("xlr_cpu", 0 /* def mode */ ,
+								   0 /* no parent */ ,
+								   xlr_cpu_proc_read /* proc read function */ ,
+								   0 /* no client data */);
+
+	if (!entry) {
+		printk("[%s]: Unable to create proc read entry for xlr_cpu!\n",
+			   __func__);
+	}
+
+	user_mac_fill_mac_base();
+
+	return 0;
+}
+
+static void user_mac_exit(void)
+{
+	unregister_chrdev(user_mac_major, PHNX_USER_MAC_CHRDEV_NAME);
+}
+
+module_init(user_mac_init);
+module_exit(user_mac_exit);
-- 
1.6.0.2.GIT

