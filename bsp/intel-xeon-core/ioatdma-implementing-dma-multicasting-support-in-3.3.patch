From af7161f62f5995bac48745e6f0dd66e5eac604f4 Mon Sep 17 00:00:00 2001
From: Dave Jiang <dave.jiang@intel.com>
Date: Tue, 26 Mar 2013 13:54:27 -0700
Subject: [PATCH 63/70] ioatdma: implementing dma multicasting support in 3.3

This patch is provided by dave.jiang@intel.com via e-mail, and not
committed to mainline yet.

Adding support for multi-cast. This allow a source to be copied to
multiple destinations

Signed-off-by: Dave Jiang <dave.jiang@intel.com>
Signed-off-by: Yunguo Wei <yunguo.wei@windriver.com>
---
 drivers/dma/ioat/dma.c       |    5 +-
 drivers/dma/ioat/dma_v2.h    |    1 +
 drivers/dma/ioat/dma_v3.c    |   86 ++++++++++++++++++++++++++++++++++++++++++
 drivers/dma/ioat/hw.h        |   27 +++++++++++++
 drivers/dma/ioat/registers.h |    1 +
 include/linux/dmaengine.h    |    4 ++
 6 files changed, 122 insertions(+), 2 deletions(-)

diff --git a/drivers/dma/ioat/dma.c b/drivers/dma/ioat/dma.c
index fd68e6d..a1e8bc2 100644
--- a/drivers/dma/ioat/dma.c
+++ b/drivers/dma/ioat/dma.c
@@ -1111,13 +1111,14 @@ static ssize_t cap_show(struct dma_chan *c, char *page)
 {
 	struct dma_device *dma = c->device;
 
-	return sprintf(page, "copy%s%s%s%s%s%s\n",
+	return sprintf(page, "copy%s%s%s%s%s%s%s\n",
 		       dma_has_cap(DMA_PQ, dma->cap_mask) ? " pq" : "",
 		       dma_has_cap(DMA_PQ_VAL, dma->cap_mask) ? " pq_val" : "",
 		       dma_has_cap(DMA_XOR, dma->cap_mask) ? " xor" : "",
 		       dma_has_cap(DMA_XOR_VAL, dma->cap_mask) ? " xor_val" : "",
 		       dma_has_cap(DMA_MEMSET, dma->cap_mask)  ? " fill" : "",
-		       dma_has_cap(DMA_INTERRUPT, dma->cap_mask) ? " intr" : "");
+		       dma_has_cap(DMA_INTERRUPT, dma->cap_mask) ? " intr" : "",
+		       dma_has_cap(DMA_MCAST, dma->cap_mask) ? " mcast" : "");
 
 }
 struct ioat_sysfs_entry ioat_cap_attr = __ATTR_RO(cap);
diff --git a/drivers/dma/ioat/dma_v2.h b/drivers/dma/ioat/dma_v2.h
index 54bef1e..8809294 100644
--- a/drivers/dma/ioat/dma_v2.h
+++ b/drivers/dma/ioat/dma_v2.h
@@ -129,6 +129,7 @@ struct ioat_ring_ent {
 		struct ioat_pq_descriptor *pq;
 		struct ioat_pq_ext_descriptor *pq_ex;
 		struct ioat_pq_update_descriptor *pqu;
+		struct ioat_mcast_descriptor *mcast;
 		struct ioat_raw_descriptor *raw;
 	};
 	size_t len;
diff --git a/drivers/dma/ioat/dma_v3.c b/drivers/dma/ioat/dma_v3.c
index 8c3ea8d..bfa3df4 100644
--- a/drivers/dma/ioat/dma_v3.c
+++ b/drivers/dma/ioat/dma_v3.c
@@ -319,6 +319,26 @@ static void ioat3_dma_unmap(struct ioat2_dma_chan *ioat,
 				   PCI_DMA_FROMDEVICE, flags, 1);
 		break;
 	}
+	case IOAT_OP_MCAST: {
+		struct ioat_mcast_descriptor *hw = desc->mcast;
+		int dsts = ndest_to_sw(hw->ctl_f.ndest) - 1;
+
+		if (!(flags & DMA_COMPL_SKIP_SRC_UNMAP))
+			ioat_unmap(pdev, hw->src_addr - offset, len,
+				   PCI_DMA_TODEVICE, flags, 0);
+
+		if (!(flags & DMA_COMPL_SKIP_DEST_UNMAP)) {
+			int i;
+			ioat_unmap(pdev, hw->dst_addr_1 - offset, len,
+				   PCI_DMA_FROMDEVICE, flags, 0);
+
+			for (i = 0; i < dsts; i++) {
+				ioat_unmap(pdev, hw->dst_addr_x[i] - offset,
+					   len, PCI_DMA_FROMDEVICE, flags, 0);
+			}
+		}
+		break;
+	}
 	case IOAT_OP_XOR_VAL:
 	case IOAT_OP_XOR: {
 		struct ioat_xor_descriptor *xor = desc->xor;
@@ -824,6 +844,67 @@ ioat3_tx_status(struct dma_chan *c, dma_cookie_t cookie,
 }
 
 static struct dma_async_tx_descriptor *
+ioat3_prep_mcast_lock(struct dma_chan *c, dma_addr_t *dma_dest,
+		      int dest_num, dma_addr_t dma_src, size_t len,
+		      unsigned long flags)
+{
+	struct ioat2_dma_chan *ioat = to_ioat2_chan(c);
+	struct ioat_ring_ent *desc;
+	struct ioat_mcast_descriptor *hw;
+	size_t total_len = len;
+	int num_descs, i, dsts, idx;
+	dma_addr_t dst_addr[5];
+	dma_addr_t src_addr = dma_src;
+
+	if (dest_num < 1)
+		return NULL;
+
+	num_descs = ioat2_xferlen_to_descs(ioat, len);
+	if (num_descs && ioat2_check_space_lock(ioat, num_descs) == 0)
+		idx = ioat->head;
+	else
+		return NULL;
+
+	for (i = 0; i < dest_num; i++)
+		dst_addr[i] = dma_dest[i];
+
+	i = 0;
+	do {
+		size_t copy = min_t(size_t, len, 1 << ioat->xfercap_log);
+
+		desc = ioat2_get_ring_ent(ioat, idx + i);
+		hw = (struct ioat_mcast_descriptor *)desc->hw;
+
+		hw->size = copy;
+		hw->ctl = 0;
+		hw->ctl_f.op = IOAT_OP_MCAST;
+		hw->src_addr = src_addr;
+		hw->ctl_f.ndest = ndest_to_hw(dest_num);
+
+		hw->dst_addr_1 = dst_addr[0];
+
+		for (dsts = 1; dsts < dest_num; dsts++) {
+			hw->dst_addr_x[dsts] = dst_addr[dsts];
+			dst_addr[dsts] += copy;
+		}
+
+		len -= copy;
+		src_addr += copy;
+
+		dump_desc_dbg(ioat, desc);
+	} while (++i < num_descs);
+
+	desc->txd.flags = flags;
+	desc->len = total_len;
+	hw->ctl_f.int_en = !!(flags & DMA_PREP_INTERRUPT);
+	hw->ctl_f.fence = !!(flags & DMA_PREP_FENCE);
+	hw->ctl_f.compl_write = 1;
+	dump_desc_dbg(ioat, desc);
+
+	return &desc->txd;
+}
+
+static struct dma_async_tx_descriptor *
 ioat3_prep_memset_lock(struct dma_chan *c, dma_addr_t dest, int value,
 		       size_t len, unsigned long flags)
 {
@@ -1959,6 +2040,11 @@ int ioat3_dma_probe(struct ioatdma_device *device, int dca)
 		dma->device_prep_dma_memset = ioat3_prep_memset_lock;
 	}
 
+	/* multicast capability */
+	if (device->cap & IOAT_CAP_DMAMC) {
+		dma_cap_set(DMA_MCAST, dma->cap_mask);
+		dma->device_prep_dma_mcast = ioat3_prep_mcast_lock;
+	}
 
 	dma->device_tx_status = ioat3_tx_status;
 	device->cleanup_fn = ioat3_cleanup_event;
diff --git a/drivers/dma/ioat/hw.h b/drivers/dma/ioat/hw.h
index bea3aa7..102a027 100644
--- a/drivers/dma/ioat/hw.h
+++ b/drivers/dma/ioat/hw.h
@@ -132,6 +132,33 @@ struct ioat_fill_descriptor {
 	uint64_t	user2;
 };
 
+struct ioat_mcast_descriptor {
+	uint32_t	size;
+	union {
+		uint32_t ctl;
+		struct {
+			unsigned int int_en:1;
+			unsigned int src_snoop_dis:1;
+			unsigned int dest_snoop_dis:1;
+			unsigned int compl_write:1;
+			unsigned int fence:1;
+			unsigned int null:1;
+			unsigned int rsvd1:2;
+			unsigned int bundle:1;
+			unsigned int rsvd2:1;
+			unsigned int hint:1;
+			unsigned int ndest:3;
+			unsigned int rsvd3:10;
+			#define IOAT_OP_MCAST 0x0a
+			unsigned int op:8;
+		} ctl_f;
+	};
+	uint64_t	src_addr;
+	uint64_t	dst_addr_1;
+	uint64_t	next;
+	uint64_t	dst_addr_x[4];
+};
+
 struct ioat_xor_descriptor {
 	uint32_t	size;
 	union {
diff --git a/drivers/dma/ioat/registers.h b/drivers/dma/ioat/registers.h
index 2f1cfa0..e30405d 100644
--- a/drivers/dma/ioat/registers.h
+++ b/drivers/dma/ioat/registers.h
@@ -81,6 +81,7 @@
 #define IOAT_CAP_PQ				0x00000200
 #define IOAT_CAP_DWBES				0x00002000
 #define IOAT_CAP_RAID16SS			0x00020000
+#define IOAT_CAP_DMAMC				0x00040000
 
 #define IOAT_CHANNEL_MMIO_SIZE			0x80	/* Each Channel MMIO space is this size */
 
diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 6ffd0ce..5a89920 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -72,6 +72,7 @@ enum dma_transaction_type {
 	DMA_PRIVATE,
 	DMA_ASYNC_TX,
 	DMA_SLAVE,
+	DMA_MCAST,
 	DMA_CYCLIC,
 	DMA_INTERLEAVE,
 /* last transaction type for creation of the capabilities mask */
@@ -576,6 +577,9 @@ struct dma_device {
 	struct dma_async_tx_descriptor *(*device_prep_dma_memset)(
 		struct dma_chan *chan, dma_addr_t dest, int value, size_t len,
 		unsigned long flags);
+	struct dma_async_tx_descriptor *(*device_prep_dma_mcast)(
+		struct dma_chan *chan, dma_addr_t *dest, int dest_num,
+		dma_addr_t src, size_t len, unsigned long flags);
 	struct dma_async_tx_descriptor *(*device_prep_dma_interrupt)(
 		struct dma_chan *chan, unsigned long flags);
 	struct dma_async_tx_descriptor *(*device_prep_dma_sg)(
-- 
1.7.5.4

