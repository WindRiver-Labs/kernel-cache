From 97ea2f6258d23fd3758a1f67606bb1ec60d52398 Mon Sep 17 00:00:00 2001
From: Dave Jiang <dave.jiang@intel.com>
Date: Tue, 26 Mar 2013 13:54:49 -0700
Subject: [PATCH 65/70] ioat: adding DIF support

This patch is provided by dave.jiang@intel.com via e-mail, and not
committed to mainline yet.

Signed-off-by: Dave Jiang <dave.jiang@intel.com>
Signed-off-by: Yunguo Wei <yunguo.wei@windriver.com>
---
 drivers/dma/ioat/dma.c       |    5 +-
 drivers/dma/ioat/dma_v2.h    |    3 +
 drivers/dma/ioat/dma_v3.c    |  340 +++++++++++++++++++++++++++++++++++++++++-
 drivers/dma/ioat/hw.h        |  138 +++++++++++++++++
 drivers/dma/ioat/registers.h |   14 ++-
 include/linux/dmaengine.h    |   38 +++++
 6 files changed, 533 insertions(+), 5 deletions(-)

diff --git a/drivers/dma/ioat/dma.c b/drivers/dma/ioat/dma.c
index a1e8bc2..b35e12a 100644
--- a/drivers/dma/ioat/dma.c
+++ b/drivers/dma/ioat/dma.c
@@ -1111,14 +1111,15 @@ static ssize_t cap_show(struct dma_chan *c, char *page)
 {
 	struct dma_device *dma = c->device;
 
-	return sprintf(page, "copy%s%s%s%s%s%s%s\n",
+	return sprintf(page, "copy%s%s%s%s%s%s%s%s\n",
 		       dma_has_cap(DMA_PQ, dma->cap_mask) ? " pq" : "",
 		       dma_has_cap(DMA_PQ_VAL, dma->cap_mask) ? " pq_val" : "",
 		       dma_has_cap(DMA_XOR, dma->cap_mask) ? " xor" : "",
 		       dma_has_cap(DMA_XOR_VAL, dma->cap_mask) ? " xor_val" : "",
 		       dma_has_cap(DMA_MEMSET, dma->cap_mask)  ? " fill" : "",
 		       dma_has_cap(DMA_INTERRUPT, dma->cap_mask) ? " intr" : "",
-		       dma_has_cap(DMA_MCAST, dma->cap_mask) ? " mcast" : "");
+		       dma_has_cap(DMA_MCAST, dma->cap_mask) ? " mcast" : "",
+		       dma_has_cap(DMA_DIF, dma->cap_mask) ? " dif" : "");
 
 }
 struct ioat_sysfs_entry ioat_cap_attr = __ATTR_RO(cap);
diff --git a/drivers/dma/ioat/dma_v2.h b/drivers/dma/ioat/dma_v2.h
index 8809294..0d60cf4 100644
--- a/drivers/dma/ioat/dma_v2.h
+++ b/drivers/dma/ioat/dma_v2.h
@@ -131,6 +131,9 @@ struct ioat_ring_ent {
 		struct ioat_pq_update_descriptor *pqu;
 		struct ioat_mcast_descriptor *mcast;
 		struct ioat_raw_descriptor *raw;
+		struct ioat_dif_gen_descriptor *difg;
+		struct ioat_dif_strip_descriptor *difs;
+		struct ioat_dif_update_descriptor *difu;
 	};
 	size_t len;
 	struct dma_async_tx_descriptor txd;
diff --git a/drivers/dma/ioat/dma_v3.c b/drivers/dma/ioat/dma_v3.c
index cc12c16..1f56dcd 100644
--- a/drivers/dma/ioat/dma_v3.c
+++ b/drivers/dma/ioat/dma_v3.c
@@ -296,6 +296,28 @@ void ioat3_free_sed(struct ioatdma_device *device, struct ioat_sed_ent *sed)
 	kmem_cache_free(device->sed_pool, sed);
 }
 
+static u8 _bytes_to_blksz(sector_t blk_sz)
+{
+	switch (blk_sz) {
+		case 512: return 0;
+		case 520: return 1;
+		case 4096: return 2;
+		case 4104: return 3;
+		default: return 0xff;
+	}
+}
+
+static unsigned int _blksz_to_bytes(u8 size)
+{
+	switch (size) {
+		case 0: return 512;
+		case 1: return 520;
+		case 2: return 4096;
+		case 3: return 4104;
+		default: return 0;
+	}
+}
+
 static void ioat3_dma_unmap(struct ioat2_dma_chan *ioat,
 			    struct ioat_ring_ent *desc, int idx)
 {
@@ -476,6 +498,46 @@ static void ioat3_dma_unmap(struct ioat2_dma_chan *ioat,
 		}
 		break;
 	}
+	case IOAT_OP_DIF_IN:
+	case IOAT_OP_DIF_ST:
+	case IOAT_OP_DIF_UP:
+	{
+		struct ioat_dif_update_descriptor *dif =
+			(struct ioat_dif_update_descriptor *)desc->hw;
+
+		if (!(flags & DMA_COMPL_SKIP_SRC_UNMAP))
+			ioat_unmap(pdev, dif->src_addr - offset, len,
+				   PCI_DMA_TODEVICE, flags, 0);
+
+		if (!(flags & DMA_COMPL_SKIP_DEST_UNMAP)) {
+			size_t size = len;
+
+			/* DIF insert */
+			if (desc->hw->ctl_f.op == IOAT_OP_DIF_IN) {
+				/*
+				 * need to calculate the destination address
+				 * with the size of DIF data
+				 * TS = TS + (TS / BLKSZ) * 8
+				 */
+				size = size + ((size /
+					_blksz_to_bytes(dif->ctl_f.dblk_sz))
+					<< 3);
+			}
+
+			/* DIF strip */
+			if (desc->hw->ctl_f.op == IOAT_OP_DIF_ST) {
+				/* TS = TS - (TS / (BLKSZ + 8)) * 8 */
+				size = size - (size /
+					((_blksz_to_bytes(dif->ctl_f.dblk_sz) +
+					 8)) << 3);
+			}
+
+			ioat_unmap(pdev, dif->dst_addr - offset, size,
+				   PCI_DMA_FROMDEVICE, flags, 0);
+		}
+
+		break;
+	}
 	default:
 		dev_err(&pdev->dev, "%s: unknown op type: %#x\n",
 			__func__, desc->hw->ctl_f.op);
@@ -555,8 +617,29 @@ desc_get_errstat(struct ioat2_dma_chan *ioat, struct ioat_ring_ent *desc)
 
 		return;
 	}
+	case IOAT_OP_DIF_ST:
+	{
+		struct ioat_dif_strip_descriptor *dif = desc->difs;
+
+		if (!dif->dwbes_f.wbes)
+			return;
+
+		if (dif->dwbes_f.chk_guard_err)
+			*desc->result |= DIF_CHECK_GUARD_RESULT;
+
+		if (dif->dwbes_f.chk_app_err)
+			*desc->result |= DIF_CHECK_APP_RESULT;
+
+		if (dif->dwbes_f.chk_ref_err)
+			*desc->result |= DIF_CHECK_REF_RESULT;
+
+		if (dif->dwbes_f.f_tag_err)
+			*desc->result |= DIF_CHECK_FTAG_RESULT;
+
+		break;
+	}
 	default:
-		return;
+		break;
 	}
 }
 
@@ -728,6 +811,26 @@ static void ioat3_eh(struct ioat2_dma_chan *ioat)
 			err_handled |= IOAT_CHANERR_XOR_Q_ERR;
 		}
 		break;
+	case IOAT_OP_DIF_ST:
+		if (chanerr & IOAT_CHANERR_DIF_RES_ALL_F_ERR) {
+			*desc->result |= DIF_CHECK_FTAG_RESULT;
+			err_handled |= IOAT_CHANERR_DIF_RES_ALL_F_ERR;
+		}
+
+		if (chanerr & IOAT_CHANERR_DIF_RES_GUARD_TAG_ERR) {
+			*desc->result |= DIF_CHECK_GUARD_RESULT;
+			err_handled |= IOAT_CHANERR_DIF_RES_GUARD_TAG_ERR;
+		}
+
+		if (chanerr & IOAT_CHANERR_DIF_RES_APP_TAG_ERR) {
+			*desc->result |= DIF_CHECK_APP_RESULT;
+			err_handled |= IOAT_CHANERR_DIF_RES_APP_TAG_ERR;
+		}
+
+		if (chanerr & IOAT_CHANERR_DIF_RES_REF_TAG_ERR) {
+			*desc->result |= DIF_CHECK_REF_RESULT;
+			err_handled |= IOAT_CHANERR_DIF_RES_REF_TAG_ERR;
+		}
 	}
 
 	/* fault on unhandled error or spurious halt */
@@ -1499,6 +1602,233 @@ ioat3_prep_interrupt_lock(struct dma_chan *c, unsigned long flags)
 	return &desc->txd;
 }
 
+static void ioat_dif_set_sflags(struct ioat_dif_sdc *sdc, unsigned long cflags)
+{
+	sdc->tag_f_err_en = !!(cflags & DMA_PREP_DIFS_DETECT_ERR);
+	sdc->tag_f_en = !!(cflags & DMA_PREP_DIFS_IGNORE_DIF);
+	sdc->app_tag_f_en = !!(cflags & DMA_PREP_DIFS_IGNORE_GRTAG);
+	sdc->app_ref_tag_f_en = !!(cflags & DMA_PREP_DIFS_IGNORE_GTAG);
+	sdc->atag_type = !!(cflags & DMA_PREP_DIFS_APP_TAG_INC);
+	sdc->guard_dis = !!(cflags & DMA_PREP_DIFS_GUARD_DIS);
+	sdc->rtag_dis = !!(cflags & DMA_PREP_DIFS_REF_TAG_DIS);
+	sdc->rtag_type = !!(cflags & DMA_PREP_DIFS_REF_TAG_FIXED);
+}
+
+static void ioat_dif_set_dflags(struct ioat_dif_ddc *ddc, unsigned long cflags)
+{
+	ddc->atag_type = !!(cflags & DMA_PREP_DIFD_APP_TAG_INC);
+	ddc->guard_dis = !!(cflags & DMA_PREP_DIFD_GUARD_DIS);
+	ddc->rtag_dis = !!(cflags & DMA_PREP_DIFD_REF_TAG_DIS);
+	ddc->rtag_type = !!(cflags & DMA_PREP_DIFD_REF_TAG_FIXED);
+}
+
+static struct dma_async_tx_descriptor *
+ioat3_prep_dif_gen_lock(struct dma_chan *c, sector_t blk_sz, dma_addr_t dma_src,
+			dma_addr_t dma_dest, size_t len, u64 tag,
+			unsigned long cflags, unsigned long flags)
+{
+	struct ioat2_dma_chan *ioat = to_ioat2_chan(c);
+	struct ioat_chan_common *chan = &ioat->base;
+	struct ioat_ring_ent *desc;
+	struct ioat_dif_gen_descriptor *hw;
+	struct device *dev = &chan->device->pdev->dev;
+	dma_addr_t dst = dma_dest;
+	dma_addr_t src = dma_src;
+	int num_descs, i, idx;
+	size_t total_len = len;
+	u8 bsz;
+
+	num_descs = ioat2_xferlen_to_descs(ioat, len);
+	if (ioat2_check_space_lock(ioat, 1) == 0)
+		idx = ioat->head;
+	else
+		return NULL;
+
+	bsz = _bytes_to_blksz(blk_sz);
+	if (bsz == 0xff) {
+		dev_warn(dev, "Invalid block size: %lu\n", blk_sz);
+		return NULL;
+	}
+
+	i = 0;
+	do {
+		size_t xfer = min_t(size_t, len, 1 << ioat->xfercap_log);
+
+		desc = ioat2_get_ring_ent(ioat, idx + i);
+		hw = desc->difg;
+
+		hw->ctl = 0;
+		hw->ctl_f.op = IOAT_OP_DIF_IN;
+		hw->size = xfer;
+		hw->ctl_f.dblk_sz = bsz;
+		hw->src_addr = src;
+		hw->dst_addr = dst;
+		ioat_dif_set_dflags(&hw->ddc_f, cflags);
+
+		hw->dst_tagc.app_tag = tag >> 48 & 0xffff;
+		hw->dst_tagc.app_mask = tag >> 32 & 0xffff;
+		hw->dst_tagc.ref_tag_seed = tag & 0xffffffff;
+
+		len -= xfer;
+		dst += xfer;
+		src += xfer;
+
+		dump_desc_dbg(ioat, desc);
+	} while (++i < num_descs);
+
+	desc->txd.flags = flags;
+	desc->len = total_len;
+	hw->ctl_f.int_en = !!(flags & DMA_PREP_INTERRUPT);
+	hw->ctl_f.fence = !!(flags & DMA_PREP_FENCE);
+	hw->ctl_f.compl_write = 1;
+	dump_desc_dbg(ioat, desc);
+	/* we leave the channel locked to ensure in order submission */
+
+	return &desc->txd;
+}
+
+static struct dma_async_tx_descriptor *
+ioat3_prep_dif_strip_lock(struct dma_chan *c, sector_t blk_sz,
+			  dma_addr_t dma_src, dma_addr_t dma_dest, size_t len,
+			  u64 tag, unsigned long cflags, unsigned long flags)
+{
+	struct ioat2_dma_chan *ioat = to_ioat2_chan(c);
+	struct ioat_chan_common *chan = &ioat->base;
+	struct ioat_ring_ent *desc;
+	struct ioat_dif_strip_descriptor *hw;
+	struct device *dev = &chan->device->pdev->dev;
+	dma_addr_t dst = dma_dest;
+	dma_addr_t src = dma_src;
+	int num_descs, i, idx;
+	size_t total_len = len;
+	u8 bsz;
+
+	num_descs = ioat2_xferlen_to_descs(ioat, len);
+	if (ioat2_check_space_lock(ioat, 1) == 0)
+		idx = ioat->head;
+	else
+		return NULL;
+
+	bsz = _bytes_to_blksz(blk_sz);
+	if (bsz == 0xff) {
+		dev_warn(dev, "Invalid block size: %lu\n", blk_sz);
+		return NULL;
+	}
+
+	i = 0;
+	do {
+		size_t xfer = min_t(size_t, len, 1 << ioat->xfercap_log);
+
+		desc = ioat2_get_ring_ent(ioat, idx + i);
+		hw = desc->difs;
+
+		hw->ctl = 0;
+		hw->ctl_f.op = IOAT_OP_DIF_ST;
+		hw->size = xfer;
+		hw->ctl_f.wb_en = 1;
+		hw->ctl_f.dblk_sz = bsz;
+		hw->src_addr = src;
+
+		hw->ctl_f.dst_cpy_dis = !!(flags & DMA_PREP_DEST_DISABLE);
+		if (!hw->ctl_f.dst_cpy_dis)
+			hw->dst_addr = dst;
+		else
+			hw->dst_addr = 0;
+
+		ioat_dif_set_sflags(&hw->sdc_f, cflags);
+
+		hw->src_tagc.app_tag = tag >> 48 & 0xffff;
+		hw->src_tagc.app_mask = tag >> 32 & 0xffff;
+		hw->src_tagc.ref_tag_seed = tag & 0xffffffff;
+
+		len -= xfer;
+		dst += xfer;
+		src += xfer;
+
+		dump_desc_dbg(ioat, desc);
+	} while (++i < num_descs);
+
+	desc->txd.flags = flags;
+	desc->len = total_len;
+	hw->ctl_f.int_en = !!(flags & DMA_PREP_INTERRUPT);
+	hw->ctl_f.fence = !!(flags & DMA_PREP_FENCE);
+	hw->ctl_f.compl_write = 1;
+	dump_desc_dbg(ioat, desc);
+	/* we leave the channel locked to ensure in order submission */
+
+	return &desc->txd;
+}
+
+static struct dma_async_tx_descriptor *
+ioat3_prep_dif_update_lock(struct dma_chan *c, sector_t blk_sz,
+			   dma_addr_t dma_src, dma_addr_t dma_dest, size_t len,
+			   u64 *tags, unsigned long cflags,
+			   unsigned long flags)
+{
+	struct ioat2_dma_chan *ioat = to_ioat2_chan(c);
+	struct ioat_chan_common *chan = &ioat->base;
+	struct ioat_ring_ent *desc;
+	struct ioat_dif_update_descriptor *hw;
+	struct device *dev = &chan->device->pdev->dev;
+	dma_addr_t dst = dma_dest;
+	dma_addr_t src = dma_src;
+	int num_descs, i, idx;
+	size_t total_len = len;
+	u8 bsz;
+
+	num_descs = ioat2_xferlen_to_descs(ioat, len);
+	if (ioat2_check_space_lock(ioat, 1) == 0)
+		idx = ioat->head;
+	else
+		return NULL;
+
+	bsz = _bytes_to_blksz(blk_sz);
+	if (bsz == 0xff) {
+		dev_warn(dev, "Invalid block size: %lu\n", blk_sz);
+		return NULL;
+	}
+
+	i = 0;
+	do {
+		size_t xfer = min_t(size_t, len, 1 << ioat->xfercap_log);
+
+		desc = ioat2_get_ring_ent(ioat, idx + i);
+		hw = desc->difu;
+
+		hw->ctl = 0;
+		hw->ctl_f.op = IOAT_OP_DIF_UP;
+		hw->size = xfer;
+		hw->ctl_f.dblk_sz = bsz;
+		hw->src_addr = src;
+		hw->dst_addr = dst;
+		ioat_dif_set_dflags(&hw->ddc_f, cflags);
+		ioat_dif_set_sflags(&hw->sdc_f, cflags);
+
+		hw->src_tagc.app_tag = tags[DIF_SRC_IDX] >> 48 & 0xffff;
+		hw->src_tagc.app_mask = tags[DIF_SRC_IDX] >> 32 & 0xffff;
+		hw->src_tagc.ref_tag_seed = tags[DIF_SRC_IDX] & 0xffffffff;
+		hw->dst_tagc.app_tag = tags[DIF_DST_IDX] >> 48 & 0xffff;
+		hw->dst_tagc.app_mask = tags[DIF_DST_IDX] >> 32 & 0xffff;
+		hw->dst_tagc.ref_tag_seed = tags[DIF_DST_IDX] & 0xffffffff;
+
+		len -= xfer;
+		dst += xfer;
+		src += xfer;
+
+		dump_desc_dbg(ioat, desc);
+	} while (++i < num_descs);
+
+	desc->txd.flags = flags;
+	desc->len = total_len;
+	hw->ctl_f.int_en = !!(flags & DMA_PREP_INTERRUPT);
+	hw->ctl_f.fence = !!(flags & DMA_PREP_FENCE);
+	hw->ctl_f.compl_write = 1;
+	dump_desc_dbg(ioat, desc);
+	/* we leave the channel locked to ensure in order submission */
+
+	return &desc->txd;
+}
+
 static void __devinit ioat3_dma_test_callback(void *dma_async_param)
 {
 	struct completion *cmp = dma_async_param;
@@ -2048,6 +2378,14 @@ int ioat3_dma_probe(struct ioatdma_device *device, int dca)
 		dma->device_prep_dma_mcast = ioat3_prep_mcast_lock;
 	}
 
+	/* DIF support */
+	if (device->cap & IOAT_CAP_DIF) {
+		dma_cap_set(DMA_DIF, dma->cap_mask);
+		dma->device_prep_dif_insert = ioat3_prep_dif_gen_lock;
+		dma->device_prep_dif_strip = ioat3_prep_dif_strip_lock;
+		dma->device_prep_dif_update = ioat3_prep_dif_update_lock;
+	}
+
 	dma->device_tx_status = ioat3_tx_status;
 	device->cleanup_fn = ioat3_cleanup_event;
 	device->timer_fn = ioat3_timer_event;
diff --git a/drivers/dma/ioat/hw.h b/drivers/dma/ioat/hw.h
index 102a027..499c652 100644
--- a/drivers/dma/ioat/hw.h
+++ b/drivers/dma/ioat/hw.h
@@ -68,6 +68,8 @@
 #define IOAT_VER_3_2            0x32    /* Version 3.2 */
 #define IOAT_VER_3_3            0x33    /* Version 3.3 */
 
+#define DIF_SRC_IDX	0
+#define DIF_DST_IDX	1
 
 int system_has_dca_enabled(struct pci_dev *pdev);
 
@@ -325,4 +327,140 @@ struct ioat_sed_raw_descriptor {
 	uint64_t	c[8];
 };
 
+struct ioat_dif_ddc {
+	uint8_t rsvd5:4;
+	uint8_t atag_type:1;
+	uint8_t guard_dis:1;
+	uint8_t rtag_dis:1;
+	uint8_t rtag_type:1;
+};
+
+struct ioat_dif_sdc {
+	uint8_t tag_f_err_en:1;
+	uint8_t tag_f_en:1;
+	uint8_t app_tag_f_en:1;
+	uint8_t app_ref_tag_f_en:1;
+	uint8_t atag_type:1;
+	uint8_t guard_dis:1;
+	uint8_t rtag_dis:1;
+	uint8_t rtag_type:1;
+};
+
+struct ioat_dif_tagc {
+	u32 ref_tag_seed;
+	u16 app_mask;
+	u16 app_tag;
+};
+
+struct ioat_dif_gen_descriptor {
+	uint32_t		size;
+	union {
+		uint32_t ctl;
+		struct {
+			unsigned int int_en:1;
+			unsigned int src_snoop_dis:1;
+			unsigned int dest_snoop_dis:1;
+			unsigned int compl_write:1;
+			unsigned int fence:1;
+			unsigned int rsvd1:3;
+			unsigned int bundle:1;
+			unsigned int rsvd2:4;
+			unsigned int dblk_sz:2;
+			unsigned int rsvd3:9;
+			#define IOAT_OP_DIF_IN 0x2
+			unsigned int op:8;
+		} ctl_f;
+	};
+	uint64_t		src_addr;
+	uint64_t		dst_addr;
+	uint64_t		next;
+	uint8_t			rsvd4[7];
+	union {
+		uint8_t ddc;
+		struct ioat_dif_ddc ddc_f;
+	};
+	uint64_t		rsvd7[2];
+	struct ioat_dif_tagc	dst_tagc;
+};
+
+struct ioat_dif_strip_descriptor {
+	union {
+		uint32_t	size;
+		uint32_t	dwbe_sts;
+		struct {
+			unsigned int rsvd0:27;
+			unsigned int f_tag_err;
+			unsigned int chk_ref_err;
+			unsigned int chk_app_err;
+			unsigned int chk_guard_err;
+			unsigned int wbes;
+		} dwbes_f;
+	};
+	union {
+		uint32_t ctl;
+		struct {
+			unsigned int int_en:1;
+			unsigned int src_snoop_dis:1;
+			unsigned int dest_snoop_dis:1;
+			unsigned int compl_write:1;
+			unsigned int fence:1;
+			unsigned int rsvd1:2;
+			unsigned int dst_cpy_dis:1;
+			unsigned int bundle:1;
+			unsigned int rsvd2:4;
+			unsigned int dblk_sz:2;
+			unsigned int wb_en:1;
+			unsigned int rsvd3:8;
+			#define IOAT_OP_DIF_ST 0x3
+			unsigned int op:8;
+		} ctl_f;
+	};
+	uint64_t		src_addr;
+	uint64_t		dst_addr;
+	uint64_t		next;
+	union {
+		uint8_t sdc;
+		struct ioat_dif_sdc sdc_f;
+	};
+	uint8_t			rsvd4[7];
+	uint64_t		rsvd7;
+	struct ioat_dif_tagc	src_tagc;
+	uint64_t		rsvd8;
+};
+
+struct ioat_dif_update_descriptor {
+	uint32_t		size;
+	union {
+		uint32_t ctl;
+		struct {
+			unsigned int int_en:1;
+			unsigned int src_snoop_dis:1;
+			unsigned int dest_snoop_dis:1;
+			unsigned int compl_write:1;
+			unsigned int fence:1;
+			unsigned int rsvd1:3;
+			unsigned int bundle:1;
+			unsigned int rsvd2:4;
+			unsigned int dblk_sz:2;
+			unsigned int rsvd3:9;
+			#define IOAT_OP_DIF_UP 0x4
+			unsigned int op:8;
+		} ctl_f;
+	};
+	uint64_t		src_addr;
+	uint64_t		dst_addr;
+	uint64_t		next;
+	union {
+		uint8_t sdc;
+		struct ioat_dif_sdc sdc_f;
+	};
+	uint8_t			rsvd4[6];
+	union {
+		uint8_t ddc;
+		struct ioat_dif_ddc ddc_f;
+	};
+	uint64_t		rsvd7;
+	struct ioat_dif_tagc	src_tagc;
+	struct ioat_dif_tagc	dst_tagc;
+};
 #endif
diff --git a/drivers/dma/ioat/registers.h b/drivers/dma/ioat/registers.h
index e30405d..746ef66 100644
--- a/drivers/dma/ioat/registers.h
+++ b/drivers/dma/ioat/registers.h
@@ -79,6 +79,7 @@
 #define IOAT_CAP_APIC				0x00000080
 #define IOAT_CAP_XOR				0x00000100
 #define IOAT_CAP_PQ				0x00000200
+#define IOAT_CAP_DIF				0x00000400
 #define IOAT_CAP_DWBES				0x00002000
 #define IOAT_CAP_RAID16SS			0x00020000
 #define IOAT_CAP_DMAMC				0x00040000
@@ -246,8 +247,17 @@
 #define IOAT_CHANERR_XOR_P_OR_CRC_ERR		0x10000
 #define IOAT_CHANERR_XOR_Q_ERR			0x20000
 #define IOAT_CHANERR_DESCRIPTOR_COUNT_ERR	0x40000
-
-#define IOAT_CHANERR_HANDLE_MASK (IOAT_CHANERR_XOR_P_OR_CRC_ERR | IOAT_CHANERR_XOR_Q_ERR)
+#define IOAT_CHANERR_DIF_ALL_F_ERR		0x80000
+#define IOAT_CHANERR_DIF_GUARD_TAG_ERR		0x100000
+#define IOAT_CHANERR_DIF_APP_TAG_ERR		0x200000
+#define IOAT_CHANERR_DIF_REF_TAG_ERR		0x400000
+#define IOAT_CHANERR_BUNDLE_BIT_ERR		0x800000
+#define IOAT_CHANERR_DIF_RES_ALL_F_ERR		0x1000000
+#define IOAT_CHANERR_DIF_RES_GUARD_TAG_ERR	0x2000000
+#define IOAT_CHANERR_DIF_RES_APP_TAG_ERR	0x4000000
+#define IOAT_CHANERR_DIF_RES_REF_TAG_ERR	0x8000000
+
+#define IOAT_CHANERR_HANDLE_MASK (IOAT_CHANERR_XOR_P_OR_CRC_ERR | IOAT_CHANERR_XOR_Q_ERR | IOAT_CHANERR_DIF_RES_ALL_F_ERR | IOAT_CHANERR_DIF_RES_GUARD_TAG_ERR | IOAT_CHANERR_DIF_RES_APP_TAG_ERR | IOAT_CHANERR_DIF_RES_REF_TAG_ERR)
 
 #define IOAT_CHANERR_MASK_OFFSET		0x2C	/* 32-bit Channel Error Register */
 
diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 5a89920..1daf72a 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -73,6 +73,7 @@ enum dma_transaction_type {
 	DMA_ASYNC_TX,
 	DMA_SLAVE,
 	DMA_MCAST,
+	DMA_DIF,
 	DMA_CYCLIC,
 	DMA_INTERLEAVE,
 /* last transaction type for creation of the capabilities mask */
@@ -195,6 +196,22 @@ enum dma_ctrl_flags {
 	DMA_PREP_PQ_DISABLE_Q = (1 << 7),
 	DMA_PREP_CONTINUE = (1 << 8),
 	DMA_PREP_FENCE = (1 << 9),
+	DMA_PREP_DEST_DISABLE = (1 << 10),
+};
+
+enum dma_dif_ctrl_flags {
+	DMA_PREP_DIFS_DETECT_ERR = (1 << 0),
+	DMA_PREP_DIFS_IGNORE_DIF = (1 << 1),
+	DMA_PREP_DIFS_IGNORE_GRTAG = (1 << 2),
+	DMA_PREP_DIFS_IGNORE_GTAG = (1 << 3),
+	DMA_PREP_DIFS_APP_TAG_INC = (1 << 4),
+	DMA_PREP_DIFS_GUARD_DIS = (1 << 5),
+	DMA_PREP_DIFS_REF_TAG_DIS = (1 << 6),
+	DMA_PREP_DIFS_REF_TAG_FIXED = (1 << 7),
+	DMA_PREP_DIFD_APP_TAG_INC = ( 1 << 20),
+	DMA_PREP_DIFD_GUARD_DIS = (1 << 21),
+	DMA_PREP_DIFD_REF_TAG_DIS = (1 << 22),
+	DMA_PREP_DIFD_REF_TAG_FIXED = (1 << 23),
 };
 
 /**
@@ -225,6 +242,10 @@ enum dma_ctrl_cmd {
 enum sum_check_bits {
 	SUM_CHECK_P = 0,
 	SUM_CHECK_Q = 1,
+	DIF_CHECK_GUARD = 2,
+	DIF_CHECK_APP = 3,
+	DIF_CHECK_REF = 4,
+	DIF_CHECK_FTAG = 5,
 };
 
 /**
@@ -235,6 +256,10 @@ enum sum_check_bits {
 enum sum_check_flags {
 	SUM_CHECK_P_RESULT = (1 << SUM_CHECK_P),
 	SUM_CHECK_Q_RESULT = (1 << SUM_CHECK_Q),
+	DIF_CHECK_GUARD_RESULT = (1 << DIF_CHECK_GUARD),
+	DIF_CHECK_APP_RESULT = (1 << DIF_CHECK_APP),
+	DIF_CHECK_REF_RESULT = (1 << DIF_CHECK_REF),
+	DIF_CHECK_FTAG_RESULT = (1 << DIF_CHECK_FTAG),
 };
 
 
@@ -602,6 +627,19 @@ struct dma_device {
 	int (*device_control)(struct dma_chan *chan, enum dma_ctrl_cmd cmd,
 		unsigned long arg);
 
+	struct dma_async_tx_descriptor *(*device_prep_dif_insert)(
+		struct dma_chan *chan, sector_t blk_sz, dma_addr_t dma_src,
+		dma_addr_t dma_dest, size_t len, u64 tag, unsigned long cflags,
+		unsigned long flags);
+	struct dma_async_tx_descriptor *(*device_prep_dif_strip)(
+		struct dma_chan *chan, sector_t blk_sz, dma_addr_t dma_src,
+		dma_addr_t dma_dest, size_t len, u64 tag, unsigned long cflags,
+		unsigned long flags);
+	struct dma_async_tx_descriptor *(*device_prep_dif_update)(
+		struct dma_chan *chan, sector_t blk_sz, dma_addr_t dma_src,
+		dma_addr_t dma_dest, size_t len, u64 *tags,
+		unsigned long cflags, unsigned long flags);
+
 	enum dma_status (*device_tx_status)(struct dma_chan *chan,
 					    dma_cookie_t cookie,
 					    struct dma_tx_state *txstate);
-- 
1.7.5.4

