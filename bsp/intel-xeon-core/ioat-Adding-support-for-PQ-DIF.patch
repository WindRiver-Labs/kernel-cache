From 8e0f75240aeb36b712edcb0ab0f58bb9ca7a2703 Mon Sep 17 00:00:00 2001
From: Dave Jiang <dave.jiang@intel.com>
Date: Tue, 26 Mar 2013 13:56:06 -0700
Subject: [PATCH 66/70] ioat: Adding support for PQ+DIF

This patch is provided by dave.jiang@intel.com via e-mail, and not
committed to mainline yet.

Signed-off-by: Dave Jiang <dave.jiang@intel.com>
Signed-off-by: Yunguo Wei <yunguo.wei@windriver.com>
---
 drivers/dma/ioat/dma.c       |    6 +-
 drivers/dma/ioat/dma_v2.h    |    3 +
 drivers/dma/ioat/dma_v3.c    |  581 ++++++++++++++++++++++++++++++++++++++++--
 drivers/dma/ioat/hw.h        |  149 ++++++++---
 drivers/dma/ioat/registers.h |    2 +
 include/linux/dmaengine.h    |   12 +
 6 files changed, 700 insertions(+), 53 deletions(-)

diff --git a/drivers/dma/ioat/dma.c b/drivers/dma/ioat/dma.c
index b35e12a..b656f55 100644
--- a/drivers/dma/ioat/dma.c
+++ b/drivers/dma/ioat/dma.c
@@ -1111,7 +1111,7 @@ static ssize_t cap_show(struct dma_chan *c, char *page)
 {
 	struct dma_device *dma = c->device;
 
-	return sprintf(page, "copy%s%s%s%s%s%s%s%s\n",
+	return sprintf(page, "copy%s%s%s%s%s%s%s%s%s%s\n",
 		       dma_has_cap(DMA_PQ, dma->cap_mask) ? " pq" : "",
 		       dma_has_cap(DMA_PQ_VAL, dma->cap_mask) ? " pq_val" : "",
 		       dma_has_cap(DMA_XOR, dma->cap_mask) ? " xor" : "",
@@ -1119,7 +1119,9 @@ static ssize_t cap_show(struct dma_chan *c, char *page)
 		       dma_has_cap(DMA_MEMSET, dma->cap_mask)  ? " fill" : "",
 		       dma_has_cap(DMA_INTERRUPT, dma->cap_mask) ? " intr" : "",
 		       dma_has_cap(DMA_MCAST, dma->cap_mask) ? " mcast" : "",
-		       dma_has_cap(DMA_DIF, dma->cap_mask) ? " dif" : "");
+		       dma_has_cap(DMA_DIF, dma->cap_mask) ? " dif" : "",
+		       dma_has_cap(DMA_PQ_DIF, dma->cap_mask) ? " pqdif" : "",
+		       dma_has_cap(DMA_PQ_VAL_DIF, dma->cap_mask) ? " pqdif_val" : "");
 
 }
 struct ioat_sysfs_entry ioat_cap_attr = __ATTR_RO(cap);
diff --git a/drivers/dma/ioat/dma_v2.h b/drivers/dma/ioat/dma_v2.h
index 0d60cf4..67f1d0a 100644
--- a/drivers/dma/ioat/dma_v2.h
+++ b/drivers/dma/ioat/dma_v2.h
@@ -134,6 +134,9 @@ struct ioat_ring_ent {
 		struct ioat_dif_gen_descriptor *difg;
 		struct ioat_dif_strip_descriptor *difs;
 		struct ioat_dif_update_descriptor *difu;
+		struct ioat_pqdif_descriptor *pqdif;
+		struct ioat_pqdif_ext1_descriptor *pqdif_ext1;
+		struct ioat_pqdif_ext2_descriptor *pqdif_ext2;
 	};
 	size_t len;
 	struct dma_async_tx_descriptor txd;
diff --git a/drivers/dma/ioat/dma_v3.c b/drivers/dma/ioat/dma_v3.c
index 1f56dcd..be144d1 100644
--- a/drivers/dma/ioat/dma_v3.c
+++ b/drivers/dma/ioat/dma_v3.c
@@ -83,10 +83,27 @@ static const u8 xor_idx_to_field[] = { 1, 4, 5, 6, 7, 0, 1, 2 };
 static const u8 pq_idx_to_desc = 0xf8;
 static const u8 pq16_idx_to_desc[] = { 0, 0, 1, 1, 1, 1, 1, 1, 1,
 				       2, 2, 2, 2, 2, 2, 2 };
+static const u8 pqdif_idx_to_desc[] = { 0, 0, 0, 1, 2, 2, 2, 2 };
+static const u8 pqdif_tagidx_to_desc = 0xe0;
+static const u8 pqdif16_idx_to_desc[] = { 0, 0, 1, 2, 2, 2, 2, 3,
+					  3, 3, 3, 4, 4, 4, 4, 5 };
+static const u8 pqdif16_tagidx_to_desc[] = { 1, 1, 1, 2, 2, 2, 2,
+					     3, 3, 3, 3, 4, 4, 4, 4, 5 };
+
 static const u8 pq_idx_to_field[] = { 1, 4, 5, 0, 1, 2, 4, 5 };
 static const u8 pq16_idx_to_field[] = { 1, 4, 1, 2, 3, 4, 5, 6, 7,
 					0, 1, 2, 3, 4, 5, 6 };
 
+static const u8 pqdif_idx_to_field[] __read_mostly = { 1, 4, 5, 6, 0, 2, 5, 7 };
+static const u8 pqdif_tagidx_to_field[] __read_mostly = { 1, 2, 4, 5, 7, 1, 4,
+							  6 };
+static const u8 pqdif16_idx_to_field[] __read_mostly = { 1, 4, 6, 1, 3, 5, 7,
+							 1, 3, 5, 7, 1, 3, 5,
+							 7, 1 };
+static const u8 pqdif16_tagidx_to_field[] __read_mostly = { 3, 4, 5, 1, 0, 2,
+							    4, 6, 0, 2, 4, 6,
+							    0, 2, 4, 6, 0 };
+
 /*
  * technically sources 1 and 2 do not require SED, but the op will have
  * at least 9 descriptors so that's irrelevant.
@@ -94,6 +111,9 @@ static const u8 pq16_idx_to_field[] = { 1, 4, 1, 2, 3, 4, 5, 6, 7,
 static const u8 pq16_idx_to_sed[] = { 0, 0, 0, 0, 0, 0, 0, 0, 0,
 				      1, 1, 1, 1, 1, 1, 1 };
 
+static const u8 pqdif16_idx_to_sed[] = { 0, 0, 0, 1, 1, 1, 1,
+					 2, 2, 2, 2, 3, 3, 3, 3, 4 };
+
 static void ioat3_eh(struct ioat2_dma_chan *ioat);
 
 static dma_addr_t xor_get_src(struct ioat_raw_descriptor *descs[2], int idx)
@@ -125,6 +145,20 @@ static dma_addr_t pq16_get_src(struct ioat_raw_descriptor *desc[3], int idx)
 	return raw->field[pq16_idx_to_field[idx]];
 }
 
+static dma_addr_t pqdif_get_src(struct ioat_raw_descriptor *descs[3], int idx)
+{
+	struct ioat_raw_descriptor *raw = descs[pqdif_idx_to_desc[idx]];
+
+	return raw->field[pqdif_idx_to_field[idx]];
+}
+
+static dma_addr_t pqdif16_get_src(struct ioat_raw_descriptor *desc[5], int idx)
+{
+	struct ioat_raw_descriptor *raw = desc[pqdif16_idx_to_desc[idx]];
+
+	return raw->field[pq16_idx_to_field[idx]];
+}
+
 static void pq_set_src(struct ioat_raw_descriptor *descs[2],
 		       dma_addr_t addr, u32 offset, u8 coef, int idx)
 {
@@ -141,6 +175,10 @@ static int sed_get_pq16_pool_idx(int src_cnt)
 	return pq16_idx_to_sed[src_cnt];
 }
 
+static int sed_get_pqdif16_pool_idx(int src_cnt) {
+	return pqdif16_idx_to_sed[src_cnt];
+}
+
 static bool is_jf_ioat(struct pci_dev *pdev)
 {
 	switch (pdev->device) {
@@ -266,6 +304,42 @@ static void pq16_set_src(struct ioat_raw_descriptor *desc[3],
 		pq16->coef[idx - 8] = coef;
 }
 
+static void pqdif_set_src(struct ioat_raw_descriptor *descs[3],
+			  dma_addr_t addr, u32 offset, u8 coef, u64 tag,
+			  int idx)
+{
+	struct ioat_pqdif_descriptor *pqdif =
+		(struct ioat_pqdif_descriptor *) descs[0];
+	struct ioat_raw_descriptor *raw = descs[pqdif_idx_to_desc[idx]];
+
+	raw->field[pqdif_idx_to_field[idx]] = addr + offset;
+	pqdif->coef[idx] = coef;
+
+	raw = descs[pqdif_tagidx_to_desc >> idx & 1];
+	raw->field[pqdif_tagidx_to_field[idx]] = tag;
+}
+
+static void pqdif16_set_src(struct ioat_raw_descriptor *descs[6],
+			    dma_addr_t addr, u32 offset, u8 coef, u64 tag,
+			    int idx)
+{
+	struct ioat_pqdif_descriptor *pqdif =
+		(struct ioat_pqdif_descriptor *)descs[0];
+	struct ioat_pqdif16_sed_descriptor *pqdif16 =
+		(struct ioat_pqdif16_sed_descriptor *)descs[1];
+	struct ioat_raw_descriptor *raw = descs[pqdif16_idx_to_desc[idx]];
+
+	raw->field[pqdif16_idx_to_field[idx]] = addr + offset;
+
+	if (idx <= 8)
+		pqdif->coef[idx] = coef;
+	else
+		pqdif16->coef[idx - 8] = coef;
+
+	raw = descs[pqdif16_tagidx_to_desc[idx]];
+	raw->field[pqdif16_tagidx_to_field[idx]] = tag;
+}
+
 struct ioat_sed_ent *
 ioat3_alloc_sed(struct ioatdma_device *device, unsigned int hw_pool)
 {
@@ -538,13 +612,105 @@ static void ioat3_dma_unmap(struct ioat2_dma_chan *ioat,
 
 		break;
 	}
+	case IOAT_OP_PQDIF_VAL:
+	case IOAT_OP_PQDIF:
+	{
+		struct ioat_pqdif_descriptor *pqdif = desc->pqdif;
+		struct ioat_ring_ent *ext1, *ext2;
+		struct ioat_pqdif_ext1_descriptor *pqdif_ext1 = NULL;
+		struct ioat_pqdif_ext2_descriptor *pqdif_ext2 = NULL;
+		int src_cnt = src_cnt_to_sw(pqdif->ctl_f.src_cnt);
+		struct ioat_raw_descriptor *descs[3];
+		int i;
+
+		/* there's always an extension for pqdif */
+		ext1 = ioat2_get_ring_ent(ioat, idx + 1);
+		pqdif_ext1 = desc->pqdif_ext1;
+
+		if (src_cnt > 4) {
+			ext2 = ioat2_get_ring_ent(ioat, idx + 2);
+			pqdif_ext2 = desc->pqdif_ext2;
+		}
+
+		/* in the 'continue' case don't unmap the dests as sources */
+		if (dmaf_p_disabled_continue(flags))
+			src_cnt--;
+		else if (dmaf_continue(flags))
+			src_cnt -= 3;
+
+		if (!(flags & DMA_COMPL_SKIP_SRC_UNMAP)) {
+			descs[0] = (struct ioat_raw_descriptor *) pqdif;
+			descs[1] = (struct ioat_raw_descriptor *) pqdif_ext1;
+			descs[2] = (struct ioat_raw_descriptor *) pqdif_ext2;
+
+			for (i = 0; i < src_cnt; i++) {
+				dma_addr_t src = pqdif_get_src(descs, i);
+
+				ioat_unmap(pdev, src - offset, len,
+					   PCI_DMA_TODEVICE, flags, 0);
+			}
+		}
+
+		/* Hmm....P and Q results would have PDIF and QDIF...
+		 * What would be the actual result size to unmap?
+		 */
+		if (!(flags & DMA_COMPL_SKIP_DEST_UNMAP)) {
+			if (!(flags & DMA_PREP_PQ_DISABLE_P))
+				ioat_unmap(pdev, pqdif->p_addr - offset, len,
+					   PCI_DMA_BIDIRECTIONAL, flags, 1);
+			if (!(flags & DMA_PREP_PQ_DISABLE_Q))
+				ioat_unmap(pdev, pqdif->q_addr - offset, len,
+					   PCI_DMA_BIDIRECTIONAL, flags, 1);
+		}
+		break;
+	}
+	case IOAT_OP_PQDIF_16S:
+	case IOAT_OP_PQDIF_VAL_16S: {
+		struct ioat_pqdif_descriptor *pqdif = desc->pqdif;
+		int src_cnt = src16_cnt_to_sw(pqdif->ctl_f.src_cnt);
+		struct ioat_raw_descriptor *descs[6];
+		int i;
+
+		/* in the 'continue' case don't unmap the dests as sources */
+		if (dmaf_p_disabled_continue(flags))
+			src_cnt--;
+		else if (dmaf_continue(flags))
+			src_cnt -= 3;
+
+		if (!(flags & DMA_COMPL_SKIP_SRC_UNMAP)) {
+			descs[0] = (struct ioat_raw_descriptor *)pqdif;
+			for (i = 0; i < 5; i++) {
+				descs[i + 1] = (struct ioat_raw_descriptor *)desc->sed->hw + i;
+			}
+
+			for (i = 0; i < src_cnt; i++) {
+				dma_addr_t src = pqdif16_get_src(descs, i);
+
+				ioat_unmap(pdev, src - offset, len,
+					   PCI_DMA_TODEVICE, flags, 0);
+			}
+		}
+
+		/* Hmm....P and Q results would have PDIF and QDIF...
+		 * What would be the actual result size to unmap?
+		 */
+		if (!(flags & DMA_COMPL_SKIP_DEST_UNMAP)) {
+			if (!(flags & DMA_PREP_PQ_DISABLE_P))
+				ioat_unmap(pdev, pqdif->p_addr - offset, len,
+					   PCI_DMA_BIDIRECTIONAL, flags, 1);
+			if (!(flags & DMA_PREP_PQ_DISABLE_Q))
+				ioat_unmap(pdev, pqdif->q_addr - offset, len,
+					   PCI_DMA_BIDIRECTIONAL, flags, 1);
+		}
+		break;
+	}
 	default:
 		dev_err(&pdev->dev, "%s: unknown op type: %#x\n",
 			__func__, desc->hw->ctl_f.op);
 	}
 }
 
-static bool desc_has_ext(struct ioat_ring_ent *desc)
+static int desc_has_ext(struct ioat_ring_ent *desc)
 {
 	struct ioat_dma_descriptor *hw = desc->hw;
 
@@ -553,16 +719,24 @@ static bool desc_has_ext(struct ioat_ring_ent *desc)
 		struct ioat_xor_descriptor *xor = desc->xor;
 
 		if (src_cnt_to_sw(xor->ctl_f.src_cnt) > 5)
-			return true;
+			return 1;
 	} else if (hw->ctl_f.op == IOAT_OP_PQ ||
 		   hw->ctl_f.op == IOAT_OP_PQ_VAL) {
 		struct ioat_pq_descriptor *pq = desc->pq;
 
 		if (src_cnt_to_sw(pq->ctl_f.src_cnt) > 3)
-			return true;
+			return 1;
+	} else if (hw->ctl_f.op == IOAT_OP_PQDIF ||
+		   hw->ctl_f.op == IOAT_OP_PQDIF_VAL) {
+		struct ioat_pqdif_descriptor *pqdif = desc->pqdif;
+
+		if (src_cnt_to_sw(pqdif->ctl_f.src_cnt) > 4)
+			return 2;
+		else
+			return 1;
 	}
 
-	return false;
+	return 0;
 }
 
 static u64 ioat3_get_current_completion(struct ioat_chan_common *chan)
@@ -614,8 +788,7 @@ desc_get_errstat(struct ioat2_dma_chan *ioat, struct ioat_ring_ent *desc)
 
 		if (pq->dwbes_f.q_val_err)
 			*desc->result |= SUM_CHECK_Q_RESULT;
-
-		return;
+		break;
 	}
 	case IOAT_OP_DIF_ST:
 	{
@@ -638,6 +811,36 @@ desc_get_errstat(struct ioat2_dma_chan *ioat, struct ioat_ring_ent *desc)
 
 		break;
 	}
+	case IOAT_OP_PQDIF_VAL:
+	case IOAT_OP_PQDIF_VAL_16S:
+	{
+		struct ioat_pqdif_descriptor *pqdif = desc->pqdif;
+
+		if (!pqdif->dwbes_f.wbes)
+			return;
+
+		if (pqdif->dwbes_f.p_val_err)
+			*desc->result |= SUM_CHECK_P_RESULT;
+
+		if (pqdif->dwbes_f.q_val_err)
+			*desc->result |= SUM_CHECK_Q_RESULT;
+
+		if (pqdif->dwbes_f.chk_guard_err)
+			*desc->result |= DIF_CHECK_GUARD_RESULT;
+
+		if (pqdif->dwbes_f.chk_app_err)
+			*desc->result |= DIF_CHECK_APP_RESULT;
+
+		if (pqdif->dwbes_f.chk_ref_err)
+			*desc->result |= DIF_CHECK_REF_RESULT;
+
+		if (pqdif->dwbes_f.f_tag_err)
+			*desc->result |= DIF_CHECK_FTAG_RESULT;
+
+		/* XXX what do we do about the source ID? */
+
+		break;
+	}
 	default:
 		break;
 	}
@@ -656,7 +859,7 @@ static void __cleanup(struct ioat2_dma_chan *ioat, dma_addr_t phys_complete)
 	struct ioatdma_device *device = chan->device;
 	struct ioat_ring_ent *desc;
 	bool seen_current = false;
-	int idx = ioat->tail, i;
+	int idx = ioat->tail, i, exts;
 	u16 active;
 
 	dev_dbg(to_dev(chan), "%s: head: %#x tail: %#x issued: %#x\n",
@@ -699,9 +902,10 @@ static void __cleanup(struct ioat2_dma_chan *ioat, dma_addr_t phys_complete)
 			seen_current = true;
 
 		/* skip extended descriptors */
-		if (desc_has_ext(desc)) {
-			BUG_ON(i + 1 >= active);
-			i++;
+		exts = desc_has_ext(desc);
+		if (exts) {
+			BUG_ON(i + exts >= active);
+			i += exts;
 		}
 
 		/* cleanup super extended descriptors */
@@ -831,6 +1035,33 @@ static void ioat3_eh(struct ioat2_dma_chan *ioat)
 			*desc->result |= DIF_CHECK_REF_RESULT;
 			err_handled |= IOAT_CHANERR_DIF_RES_REF_TAG_ERR;
 		}
+		break;
+	case IOAT_OP_PQDIF_VAL_16S:
+		if (chanerr & IOAT_CHANERR_XOR_P_OR_CRC_ERR) {
+			*desc->result |= SUM_CHECK_P_RESULT;
+			err_handled |= IOAT_CHANERR_XOR_P_OR_CRC_ERR;
+		}
+		if (chanerr & IOAT_CHANERR_XOR_Q_ERR) {
+			*desc->result |= SUM_CHECK_Q_RESULT;
+			err_handled |= IOAT_CHANERR_XOR_Q_ERR;
+		}
+		if (chanerr & IOAT_CHANERR_DIF_RES_ALL_F_ERR) {
+			*desc->result |= DIF_CHECK_FTAG_RESULT;
+			err_handled |= IOAT_CHANERR_DIF_RES_ALL_F_ERR;
+		}
+		if (chanerr & IOAT_CHANERR_DIF_RES_GUARD_TAG_ERR) {
+			*desc->result |= DIF_CHECK_GUARD_RESULT;
+			err_handled |= IOAT_CHANERR_DIF_RES_GUARD_TAG_ERR;
+		}
+		if (chanerr & IOAT_CHANERR_DIF_RES_APP_TAG_ERR) {
+			*desc->result |= DIF_CHECK_APP_RESULT;
+			err_handled |= IOAT_CHANERR_DIF_RES_APP_TAG_ERR;
+		}
+		if (chanerr & IOAT_CHANERR_DIF_RES_REF_TAG_ERR) {
+			*desc->result |= DIF_CHECK_REF_RESULT;
+			err_handled |= IOAT_CHANERR_DIF_RES_REF_TAG_ERR;
+		}
+		break;
 	}
 
 	/* fault on unhandled error or spurious halt */
@@ -1602,7 +1833,7 @@ ioat3_prep_interrupt_lock(struct dma_chan *c, unsigned long flags)
 	return &desc->txd;
 }
 
-static void ioat_dif_set_sflags(struct ioat_dif_sdc *sdc, unsigned long cflags)
+static void ioat_dif_set_sflags(union ioat_dif_sdc *sdc, unsigned long cflags)
 {
 	sdc->tag_f_err_en = !!(cflags & DMA_PREP_DIFS_DETECT_ERR);
 	sdc->tag_f_en = !!(cflags & DMA_PREP_DIFS_IGNORE_DIF);
@@ -1614,7 +1845,7 @@ static void ioat_dif_set_sflags(struct ioat_dif_sdc *sdc, unsigned long cflags)
 	sdc->rtag_type = !!(cflags & DMA_PREP_DIFS_REF_TAG_FIXED);
 }
 
-static void ioat_dif_set_dflags(struct ioat_dif_ddc *ddc, unsigned long cflags)
+static void ioat_dif_set_dflags(union ioat_dif_ddc *ddc, unsigned long cflags)
 {
 	ddc->atag_type = !!(cflags & DMA_PREP_DIFD_APP_TAG_INC);
 	ddc->guard_dis = !!(cflags & DMA_PREP_DIFD_GUARD_DIS);
@@ -1663,7 +1894,7 @@ ioat3_prep_dif_gen_lock(struct dma_chan *c, sector_t blk_sz, dma_addr_t dma_src,
 		hw->ctl_f.dblk_sz = bsz;
 		hw->src_addr = src;
 		hw->dst_addr = dst;
-		ioat_dif_set_dflags(&hw->ddc_f, cflags);
+		ioat_dif_set_dflags(&hw->ddc, cflags);
 
 		hw->dst_tagc.app_tag = tag >> 48 & 0xffff;
 		hw->dst_tagc.app_mask = tag >> 32 & 0xffff;
@@ -1735,7 +1966,7 @@ ioat3_prep_dif_strip_lock(struct dma_chan *c, sector_t blk_sz,
 		else
 			hw->dst_addr = 0;
 
-		ioat_dif_set_sflags(&hw->sdc_f, cflags);
+		ioat_dif_set_sflags(&hw->sdc, cflags);
 
 		hw->src_tagc.app_tag = tag >> 48 & 0xffff;
 		hw->src_tagc.app_mask = tag >> 32 & 0xffff;
@@ -1801,8 +2032,8 @@ ioat3_prep_dif_update_lock(struct dma_chan *c, sector_t blk_sz,
 		hw->ctl_f.dblk_sz = bsz;
 		hw->src_addr = src;
 		hw->dst_addr = dst;
-		ioat_dif_set_dflags(&hw->ddc_f, cflags);
-		ioat_dif_set_sflags(&hw->sdc_f, cflags);
+		ioat_dif_set_dflags(&hw->ddc, cflags);
+		ioat_dif_set_sflags(&hw->sdc, cflags);
 
 		hw->src_tagc.app_tag = tags[DIF_SRC_IDX] >> 48 & 0xffff;
 		hw->src_tagc.app_mask = tags[DIF_SRC_IDX] >> 32 & 0xffff;
@@ -1829,6 +2060,317 @@ ioat3_prep_dif_update_lock(struct dma_chan *c, sector_t blk_sz,
 	return &desc->txd;
 }
 
+static struct dma_async_tx_descriptor *
+__ioat3_prep_pqdif_lock(struct dma_chan *c, enum sum_check_flags *result,
+			const dma_addr_t *dst, const dma_addr_t *src,
+			unsigned int src_cnt, const unsigned char *scf,
+			size_t len, sector_t blk_sz, unsigned long cflags,
+			u64 *tags, unsigned long flags)
+{
+	struct ioat2_dma_chan *ioat = to_ioat2_chan(c);
+	struct ioat_chan_common *chan = &ioat->base;
+	struct ioat_ring_ent *desc;
+	struct ioat_ring_ent *ext1;
+	struct ioat_ring_ent *ext2;
+	size_t total_len = len;
+	struct ioat_pqdif_descriptor *pqdif;
+	struct ioat_pqdif_ext1_descriptor *pqdif_ext1 = NULL;
+	struct ioat_pqdif_ext2_descriptor *pqdif_ext2 = NULL;
+	u32 offset = 0;
+	u8 op = result ? IOAT_OP_PQDIF_VAL : IOAT_OP_PQDIF;
+	u8 bsz;
+	int i, s, idx, with_ext, num_descs;
+
+	dev_dbg(to_dev(chan), "%s\n", __func__);
+	/* the engine requires at least two sources (we provide
+	 * at least 1 implied source in the DMA_PREP_CONTINUE case)
+	 */
+	BUG_ON(src_cnt + dmaf_continue(flags) < 2);
+
+	num_descs = ioat2_xferlen_to_descs(ioat, len);
+
+	bsz = _bytes_to_blksz(blk_sz);
+	if (bsz == 0xff) {
+		dev_warn(to_dev(chan), "Invalid block size: %lu\n", blk_sz);
+		return NULL;
+	}
+
+	/* we need 3x the number of descriptors to cover greater than 4
+	 * sources (we need 1 extra source in the q-only continuation
+	 * case and 3 extra sources in the p+q continuation case.
+	 */
+	if (src_cnt + dmaf_p_disabled_continue(flags) > 4 ||
+	    (dmaf_continue(flags) && !dmaf_p_disabled_continue(flags))) {
+		with_ext = 1;
+		num_descs *= 3;
+	} else {
+		/* we still need 2x descriptors because of DIF */
+		with_ext = 0;
+		num_descs *= 2;
+	}
+
+	if (num_descs && ioat2_check_space_lock(ioat, num_descs) == 0)
+		idx = ioat->head;
+	else
+		return NULL;
+	i = 0;
+	do {
+		struct ioat_raw_descriptor *descs[3];
+		size_t xfer_size = min_t(size_t, len, 1 << ioat->xfercap_log);
+
+		desc = ioat2_get_ring_ent(ioat, idx + i);
+		pqdif = desc->pqdif;
+
+		/* save a branch by unconditionally retrieving the
+		 * extended descriptor pqdif_set_src() knows to not write
+		 * to it in the single descriptor case
+		 */
+		ext1 = ioat2_get_ring_ent(ioat, idx + i + 1);
+		pqdif_ext1 = ext1->pqdif_ext1;
+		ext2 = ioat2_get_ring_ent(ioat, idx + i + 1 + with_ext);
+		pqdif_ext2 = ext2->pqdif_ext2;
+
+		descs[0] = (struct ioat_raw_descriptor *) pqdif;
+		descs[1] = (struct ioat_raw_descriptor *) pqdif_ext1;
+		descs[2] = (struct ioat_raw_descriptor *) pqdif_ext2;
+
+		for (s = 0; s < src_cnt; s++)
+			ioat_dif_set_sflags(&pqdif_ext1->sdc[s], cflags);
+
+		for (s = 0; s < src_cnt; s++)
+			pqdif_set_src(descs, src[s], offset, scf[s], tags[s], s);
+
+		/* see the comment for dma_maxpq in include/linux/dmaengine.h */
+		if (dmaf_p_disabled_continue(flags)) {
+			pqdif_set_src(descs, dst[1], offset, 1, tags[s], s);
+			s++;
+		} else if (dmaf_continue(flags)) {
+			pqdif_set_src(descs, dst[0], offset, 0, tags[s], s);
+			s++;
+			pqdif_set_src(descs, dst[1], offset, 1, tags[s], s);
+			s++;
+			pqdif_set_src(descs, dst[1], offset, 0, tags[s], s);
+			s++;
+		}
+
+		pqdif->size = xfer_size;
+		pqdif->p_addr = dst[0] + offset;
+		pqdif->q_addr = dst[1] + offset;
+		pqdif->ctl = 0;
+		pqdif->ctl_f.dblk_sz = bsz;
+		pqdif->ctl_f.op = op;
+		pqdif->ctl_f.wb_en = result ? 1 : 0;
+		pqdif->ctl_f.src_cnt = src_cnt_to_hw(s);
+		pqdif->ctl_f.p_disable = !!(flags & DMA_PREP_PQ_DISABLE_P);
+		pqdif->ctl_f.q_disable = !!(flags & DMA_PREP_PQ_DISABLE_Q);
+
+		len -= xfer_size;
+		offset += xfer_size;
+	} while ((i += 2 + with_ext) < num_descs);
+
+	/* last pq descriptor carries the unmap parameters and fence bit */
+	desc->txd.flags = flags;
+	desc->len = total_len;
+	if (result)
+		desc->result = result;
+	pqdif->ctl_f.fence = !!(flags & DMA_PREP_FENCE);
+	pqdif->ctl_f.int_en = !!(flags & DMA_PREP_INTERRUPT);
+	pqdif->ctl_f.compl_write = 1;
+
+	dump_pq_desc_dbg(ioat, desc, ext1);
+
+	/* we leave the channel locked to ensure in order submission */
+	return &desc->txd;
+}
+
+static struct dma_async_tx_descriptor *
+__ioat3_prep_pqdif16_lock(struct dma_chan *c, enum sum_check_flags *result,
+			  const dma_addr_t *dst, const dma_addr_t *src,
+			  unsigned int src_cnt, const unsigned char *scf,
+			  size_t len, sector_t blk_sz, unsigned long cflags,
+			  u64 *tags, unsigned long flags)
+{
+	struct ioat2_dma_chan *ioat = to_ioat2_chan(c);
+	struct ioat_chan_common *chan = &ioat->base;
+	struct ioatdma_device *device = chan->device;
+	struct ioat_ring_ent *desc;
+	size_t total_len = len;
+	struct ioat_pqdif_descriptor *pqdif;
+	struct ioat_pqdif16_sed_descriptor *pqdif_sed = NULL;
+	u32 offset = 0;
+	u8 op, bsz;
+	int i, s, idx, num_descs;
+
+	op = result ? IOAT_OP_PQ_VAL_16S : IOAT_OP_PQ_16S;
+
+	dev_dbg(to_dev(chan), "%s\n", __func__);
+	/* the engine requires at least two sources (we provide
+	 * at least 1 implied source in the DMA_PREP_CONTINUE case)
+	 */
+	BUG_ON(src_cnt + dmaf_continue(flags) < 2);
+
+	num_descs = ioat2_xferlen_to_descs(ioat, len);
+
+	bsz = _bytes_to_blksz(blk_sz);
+	if (bsz == 0xff) {
+		dev_warn(to_dev(chan), "Invalid block size: %lu\n", blk_sz);
+		return NULL;
+	}
+
+	if (num_descs && ioat2_check_space_lock(ioat, num_descs) == 0)
+		idx = ioat->head;
+	else
+		return NULL;
+
+	i = 0;
+
+	do {
+		struct ioat_raw_descriptor *descs[6];
+		size_t xfer_size = min_t(size_t, len, 1 << ioat->xfercap_log);
+
+		desc = ioat2_get_ring_ent(ioat, idx + i);
+		pqdif = desc->pqdif;
+
+		descs[0] = (struct ioat_raw_descriptor *)pqdif;
+
+		desc->sed = ioat3_alloc_sed(device,
+					    sed_get_pqdif16_pool_idx(src_cnt));
+		if (!desc->sed)
+			return NULL;
+
+		pqdif->sed_addr = desc->sed->dma;
+
+		desc->sed->parent = desc;
+
+		pqdif_sed = (struct ioat_pqdif16_sed_descriptor *)desc->sed->hw;
+
+		for (s = 1; s < 6; s++)
+			descs[s] = (struct ioat_raw_descriptor *)desc->sed->hw + s;
+
+		for (s = 0; s < src_cnt; s++)
+			ioat_dif_set_sflags(&pqdif_sed->sdc[s], cflags);
+
+		for (s = 0; s < src_cnt; s++)
+			pqdif16_set_src(descs, src[s], offset, scf[s], tags[s], s);
+
+		/* see the comment for dma_maxpq in include/linux/dmaengine.h */
+		if (dmaf_p_disabled_continue(flags)) {
+			pqdif16_set_src(descs, dst[0], offset, 0, tags[s], s);
+			s++;
+		} else if (dmaf_continue(flags)) {
+			pqdif16_set_src(descs, dst[0], offset, 0, tags[s], s);
+			s++;
+			pqdif16_set_src(descs, dst[1], offset, 1, tags[s], s);
+			s++;
+			pqdif16_set_src(descs, dst[1], offset, 0, tags[s], s);
+			s++;
+		}
+
+		pqdif->size = xfer_size;
+		pqdif->p_addr = dst[0] + offset;
+		pqdif->q_addr = dst[1] + offset;
+		pqdif->ctl = 0;
+		pqdif->ctl_f.dblk_sz = bsz;
+		pqdif->ctl_f.op = op;
+		pqdif->ctl_f.wb_en = result ? 1 : 0;
+		pqdif->ctl_f.src_cnt = src_cnt_to_hw(s);
+		pqdif->ctl_f.p_disable = !!(flags & DMA_PREP_PQ_DISABLE_P);
+		pqdif->ctl_f.q_disable = !!(flags & DMA_PREP_PQ_DISABLE_Q);
+
+		len -= xfer_size;
+		offset += xfer_size;
+	} while ((i += 1) < num_descs);
+
+	/* last pq descriptor carries the unmap parameters and fence bit */
+	desc->txd.flags = flags;
+	desc->len = total_len;
+	if (result)
+		desc->result = result;
+	pqdif->ctl_f.fence = !!(flags & DMA_PREP_FENCE);
+
+	/* with cb3.3 we should be able to do completion w/o a null desc */
+	pqdif->ctl_f.int_en = !!(flags & DMA_PREP_INTERRUPT);
+	pqdif->ctl_f.compl_write = 1;
+
+	dump_pq_desc_dbg(ioat, desc, NULL);
+
+	/* we leave the channel locked to ensure in order submission */
+	return &desc->txd;
+}
+
+static struct dma_async_tx_descriptor *
+ioat3_prep_pqdif(struct dma_chan *chan, dma_addr_t *dst, dma_addr_t *src,
+		 unsigned int src_cnt, const unsigned char *scf, size_t len,
+		 sector_t blk_sz, unsigned long cflags, u64 *tags,
+		 unsigned long flags)
+{
+	struct dma_device *dma = chan->device;
+
+	/* specify valid address for disabled result */
+	if (flags & DMA_PREP_PQ_DISABLE_P)
+		dst[0] = dst[1];
+	if (flags & DMA_PREP_PQ_DISABLE_Q)
+		dst[1] = dst[0];
+
+	/* handle the single source multiply case from the raid6
+	 * recovery path
+	 */
+	if ((flags & DMA_PREP_PQ_DISABLE_P) && src_cnt == 1) {
+		dma_addr_t single_source[2];
+		unsigned char single_source_coef[2];
+
+		BUG_ON(flags & DMA_PREP_PQ_DISABLE_Q);
+		single_source[0] = src[0];
+		single_source[1] = src[0];
+		single_source_coef[0] = scf[0];
+		single_source_coef[1] = 0;
+
+		return (src_cnt >= 8) && (dma->max_pq > 8) ?
+			__ioat3_prep_pqdif16_lock(chan, NULL, dst,
+						  single_source, 2,
+						  single_source_coef, len,
+						  blk_sz, cflags, tags, flags) :
+			__ioat3_prep_pqdif_lock(chan, NULL, dst, single_source,
+						2, single_source_coef, len,
+						blk_sz, cflags, tags, flags);
+
+	} else {
+		return (src_cnt >= 8) && (dma->max_pq > 8) ?
+			__ioat3_prep_pqdif16_lock(chan, NULL, dst, src, src_cnt,
+						  scf, len, blk_sz, cflags,
+						  tags, flags) :
+			__ioat3_prep_pqdif_lock(chan, NULL, dst, src, src_cnt,
+						scf, len, blk_sz, cflags, tags,
+						flags);
+	}
+}
+
+struct dma_async_tx_descriptor *
+ioat3_prep_pqdif_val(struct dma_chan *chan, dma_addr_t *pq, dma_addr_t *src,
+		     unsigned int src_cnt, const unsigned char *scf, size_t len,
+		     sector_t blk_sz, unsigned long cflags, u64 *tags,
+		     enum sum_check_flags *pqres, unsigned long flags)
+{
+	struct dma_device *dma = chan->device;
+
+	/* specify valid address for disabled result */
+	if (flags & DMA_PREP_PQ_DISABLE_P)
+		pq[0] = pq[1];
+	if (flags & DMA_PREP_PQ_DISABLE_Q)
+		pq[1] = pq[0];
+
+	/* the cleanup routine only sets bits on validate failure, it
+	 * does not clear bits on validate success... so clear it here
+	 */
+	*pqres = 0;
+
+	return (src_cnt >= 8) && (dma->max_pq > 8) ?
+		__ioat3_prep_pqdif16_lock(chan, pqres, pq, src, src_cnt, scf,
+					  len, blk_sz, cflags, tags, flags) :
+		__ioat3_prep_pqdif_lock(chan, pqres, pq, src, src_cnt, scf, len,
+					blk_sz, cflags, tags, flags);
+}
+
 static void __devinit ioat3_dma_test_callback(void *dma_async_param)
 {
 	struct completion *cmp = dma_async_param;
@@ -2386,6 +2928,13 @@ int ioat3_dma_probe(struct ioatdma_device *device, int dca)
 		dma->device_prep_dif_update = ioat3_prep_dif_update_lock;
 	}
 
+	if (device->cap & IOAT_CAP_RAID16_DIF) {
+		dma_cap_set(DMA_PQ_DIF, dma->cap_mask);
+		dma_cap_set(DMA_PQ_VAL_DIF, dma->cap_mask);
+		dma->device_prep_pqdif = ioat3_prep_pqdif;
+		dma->device_prep_pqdif_val = ioat3_prep_pqdif_val;
+	}
+
 	dma->device_tx_status = ioat3_tx_status;
 	device->cleanup_fn = ioat3_cleanup_event;
 	device->timer_fn = ioat3_timer_event;
diff --git a/drivers/dma/ioat/hw.h b/drivers/dma/ioat/hw.h
index 499c652..f15e841 100644
--- a/drivers/dma/ioat/hw.h
+++ b/drivers/dma/ioat/hw.h
@@ -327,23 +327,29 @@ struct ioat_sed_raw_descriptor {
 	uint64_t	c[8];
 };
 
-struct ioat_dif_ddc {
-	uint8_t rsvd5:4;
-	uint8_t atag_type:1;
-	uint8_t guard_dis:1;
-	uint8_t rtag_dis:1;
-	uint8_t rtag_type:1;
+union ioat_dif_ddc {
+	uint8_t ddc;
+	struct {
+		uint8_t rsvd5:4;
+		uint8_t atag_type:1;
+		uint8_t guard_dis:1;
+		uint8_t rtag_dis:1;
+		uint8_t rtag_type:1;
+	};
 };
 
-struct ioat_dif_sdc {
-	uint8_t tag_f_err_en:1;
-	uint8_t tag_f_en:1;
-	uint8_t app_tag_f_en:1;
-	uint8_t app_ref_tag_f_en:1;
-	uint8_t atag_type:1;
-	uint8_t guard_dis:1;
-	uint8_t rtag_dis:1;
-	uint8_t rtag_type:1;
+union ioat_dif_sdc {
+	uint8_t sdc;
+	struct {
+		uint8_t tag_f_err_en:1;
+		uint8_t tag_f_en:1;
+		uint8_t app_tag_f_en:1;
+		uint8_t app_ref_tag_f_en:1;
+		uint8_t atag_type:1;
+		uint8_t guard_dis:1;
+		uint8_t rtag_dis:1;
+		uint8_t rtag_type:1;
+	};
 };
 
 struct ioat_dif_tagc {
@@ -375,10 +381,7 @@ struct ioat_dif_gen_descriptor {
 	uint64_t		dst_addr;
 	uint64_t		next;
 	uint8_t			rsvd4[7];
-	union {
-		uint8_t ddc;
-		struct ioat_dif_ddc ddc_f;
-	};
+	union ioat_dif_ddc	ddc;
 	uint64_t		rsvd7[2];
 	struct ioat_dif_tagc	dst_tagc;
 };
@@ -389,11 +392,11 @@ struct ioat_dif_strip_descriptor {
 		uint32_t	dwbe_sts;
 		struct {
 			unsigned int rsvd0:27;
-			unsigned int f_tag_err;
-			unsigned int chk_ref_err;
-			unsigned int chk_app_err;
-			unsigned int chk_guard_err;
-			unsigned int wbes;
+			unsigned int f_tag_err:1;
+			unsigned int chk_ref_err:1;
+			unsigned int chk_app_err:1;
+			unsigned int chk_guard_err:1;
+			unsigned int wbes:1;
 		} dwbes_f;
 	};
 	union {
@@ -418,10 +421,7 @@ struct ioat_dif_strip_descriptor {
 	uint64_t		src_addr;
 	uint64_t		dst_addr;
 	uint64_t		next;
-	union {
-		uint8_t sdc;
-		struct ioat_dif_sdc sdc_f;
-	};
+	union ioat_dif_sdc	sdc;
 	uint8_t			rsvd4[7];
 	uint64_t		rsvd7;
 	struct ioat_dif_tagc	src_tagc;
@@ -450,17 +450,96 @@ struct ioat_dif_update_descriptor {
 	uint64_t		src_addr;
 	uint64_t		dst_addr;
 	uint64_t		next;
+	union ioat_dif_sdc	sdc;
+	uint8_t			rsvd4[6];
+	union ioat_dif_ddc	ddc;
+	uint64_t		rsvd7;
+	struct ioat_dif_tagc	src_tagc;
+	struct ioat_dif_tagc	dst_tagc;
+};
+
+struct ioat_pqdif_descriptor {
 	union {
-		uint8_t sdc;
-		struct ioat_dif_sdc sdc_f;
+		uint32_t	size;
+		uint32_t	dwbe_sts;
+		struct {
+			unsigned int derr_src_id:8;
+			unsigned int rsvd0:17;
+			unsigned int p_val_err:1;
+			unsigned int q_val_err:1;
+			unsigned int f_tag_err:1;
+			unsigned int chk_ref_err:1;
+			unsigned int chk_app_err:1;
+			unsigned int chk_guard_err:1;
+			unsigned int wbes:1;
+		} dwbes_f;
 	};
-	uint8_t			rsvd4[6];
 	union {
-		uint8_t ddc;
-		struct ioat_dif_ddc ddc_f;
+		uint32_t ctl;
+		struct {
+			unsigned int int_en:1;
+			unsigned int src_snoop_dis:1;
+			unsigned int dest_snoop_dis:1;
+			unsigned int compl_write:1;
+			unsigned int fence:1;
+			unsigned int src_cnt:3;
+			unsigned int bundle:1;
+			unsigned int rsvd:2;
+			unsigned int p_disable:1;
+			unsigned int q_disable:1;
+			unsigned int dblk_sz:2;
+			unsigned int wb_en:1;
+			unsigned int rsvd3:8;
+			#define IOAT_OP_PQDIF 0x8c
+			#define IOAT_OP_PQDIF_VAL 0x8d
+			#define IOAT_OP_PQDIF_16S 0xa2
+			#define IOAT_OP_PQDIF_VAL_16S 0xa3
+			unsigned int op:8;
+		} ctl_f;
+	};
+	uint64_t	src_addr;
+	uint64_t	p_addr;
+	uint64_t	next;
+	uint64_t	src_addr2;
+	union {
+		uint64_t	src_addr3;
+		uint64_t	sed_addr;
 	};
-	uint64_t		rsvd7;
+	uint8_t		coef[8];
+	uint64_t	q_addr;
+};
+
+struct ioat_pqdif_tagaddr_grp {
 	struct ioat_dif_tagc	src_tagc;
-	struct ioat_dif_tagc	dst_tagc;
+	uint64_t		src_addr;
 };
+
+struct ioat_pqdif_ext1_descriptor {
+	union ioat_dif_sdc		sdc[8];
+	struct ioat_dif_tagc		src_tagc1;
+	struct ioat_dif_tagc		src_tagc2;
+	uint64_t			next;
+	struct ioat_dif_tagc		src_tagc3;
+	struct ioat_pqdif_tagaddr_grp	tag4;
+	struct ioat_dif_tagc		src_tagc5;
+};
+
+struct ioat_pqdif_ext2_descriptor {
+	uint64_t			src_addr5;
+	struct ioat_pqdif_tagaddr_grp	tag6;
+	uint64_t			next;
+	struct ioat_pqdif_tagaddr_grp	tag7_8[2];
+};
+
+struct ioat_pqdif16_sed_descriptor {
+	uint8_t				coef[8];
+	union ioat_dif_sdc		sdc[16];
+	struct ioat_dif_tagc		src_tagc[2];
+	struct ioat_pqdif_tagaddr_grp	tag3;
+	uint64_t			rsvd;
+
+	struct ioat_pqdif_tagaddr_grp	tag4_16[13];
+	uint64_t			rsvd1[6];
+};
+
 #endif
diff --git a/drivers/dma/ioat/registers.h b/drivers/dma/ioat/registers.h
index 746ef66..c108d92 100644
--- a/drivers/dma/ioat/registers.h
+++ b/drivers/dma/ioat/registers.h
@@ -81,6 +81,8 @@
 #define IOAT_CAP_PQ				0x00000200
 #define IOAT_CAP_DIF				0x00000400
 #define IOAT_CAP_DWBES				0x00002000
+#define IOAT_CAP_RAID8_DIF			0x00004000
+#define IOAT_CAP_RAID16_DIF			0x00010000
 #define IOAT_CAP_RAID16SS			0x00020000
 #define IOAT_CAP_DMAMC				0x00040000
 
diff --git a/include/linux/dmaengine.h b/include/linux/dmaengine.h
index 1daf72a..a43b12b 100644
--- a/include/linux/dmaengine.h
+++ b/include/linux/dmaengine.h
@@ -74,6 +74,8 @@ enum dma_transaction_type {
 	DMA_SLAVE,
 	DMA_MCAST,
 	DMA_DIF,
+	DMA_PQ_DIF,
+	DMA_PQ_VAL_DIF,
 	DMA_CYCLIC,
 	DMA_INTERLEAVE,
 /* last transaction type for creation of the capabilities mask */
@@ -639,6 +641,16 @@ struct dma_device {
 		struct dma_chan *chan, sector_t blk_sz, dma_addr_t dma_src,
 		dma_addr_t dma_dest, size_t len, u64 *tags,
 		unsigned long cflags, unsigned long flags);
+	struct dma_async_tx_descriptor *(*device_prep_pqdif)(
+		struct dma_chan *chan, dma_addr_t *dst, dma_addr_t *src,
+		unsigned int src_cnt, const unsigned char *scf, size_t len,
+		sector_t blk_sz, unsigned long cflags, u64 *tags,
+		unsigned long flags);
+	struct dma_async_tx_descriptor *(*device_prep_pqdif_val)(
+		struct dma_chan *chan, dma_addr_t *pq, dma_addr_t *src,
+		unsigned int src_cnt, const unsigned char *scf, size_t len,
+		sector_t blk_sz, unsigned long cflags, u64 *tags,
+		enum sum_check_flags *pqres, unsigned long flags);
 
 	enum dma_status (*device_tx_status)(struct dma_chan *chan,
 					    dma_cookie_t cookie,
-- 
1.7.5.4

