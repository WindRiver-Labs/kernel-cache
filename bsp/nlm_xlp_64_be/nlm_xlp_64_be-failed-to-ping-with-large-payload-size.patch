From 7051e672985b159ac8d84d1feba51478fbfbdc26 Mon Sep 17 00:00:00 2001
From: Yanjiang Jin <yanjiang.jin@windriver.com>
Date: Thu, 3 May 2012 14:36:10 +0800
Subject: [PATCH 29/47] nlm_xlp_64_be: failed to ping with large payload sizes

source: SDK <20120331_2.2.4_alpha>

In previous xlp SDKs, the NAEs would fail to ping with
large payload sizes. Updating the nae driver to
SDK <20120331_2.2.4_alpha> solves the problem by
changing the way NAE interrupts are handled.

Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 drivers/net/nae/xlp_nae.c |  782 +++++++++++++++++++++++++++++++++++----------
 1 files changed, 605 insertions(+), 177 deletions(-)

diff --git a/drivers/net/nae/xlp_nae.c b/drivers/net/nae/xlp_nae.c
index 8a585f2..795997f 100644
--- a/drivers/net/nae/xlp_nae.c
+++ b/drivers/net/nae/xlp_nae.c
@@ -123,10 +123,10 @@
    7 buffers. We need 32 bytes for prepad and another cacheline for storing
    s/w info
  */
-#ifdef CONFIG_64BIT
+#if defined(CONFIG_PAGE_SIZE_64KB) && defined(CONFIG_64BIT)
 #define DEFAULT_JUMBO_MTU	5568  // for mtu 16384 : 5568
 #else
-#define DEFAULT_JUMBO_MTU       3268  // for mtu 16384 : 5568
+#define DEFAULT_JUMBO_MTU       3268  // for mtu 16384 : 3268
 #endif
 #define JUMBO_RX_OFFSET		64
 #define PREPAD_LEN		0	
@@ -369,7 +369,7 @@ static const struct net_device_ops nlm_xlp_nae_ops = {
 	.ndo_do_ioctl = nlm_xlp_nae_ioctl,
 	.ndo_tx_timeout = nlm_xlp_nae_tx_timeout,
 	.ndo_change_mtu = nlm_xlp_nae_change_mtu,
-	.ndo_set_mac_address = eth_mac_addr,
+	.ndo_set_mac_address	= nlm_xlp_nae_set_hwaddr,
 	.ndo_get_stats = nlm_xlp_mac_get_stats,
 };
 
@@ -427,7 +427,7 @@ static inline void *cacheline_aligned_kmalloc(int size, int gfp_mask)
 {
 	void *buf = kmalloc(size + CACHELINE_SIZE, gfp_mask);
 	if (buf)
-	buf = (void *)(CACHELINE_ALIGNED_ADDR((unsigned long)buf +
+		buf = (void *)(CACHELINE_ALIGNED_ADDR((unsigned long)buf +
 							CACHELINE_SIZE));
 	return buf;
 }
@@ -557,6 +557,276 @@ static int mac_refill_frin_desc(unsigned long dev)
 	return ret;
 }
 
+static __inline__ int mac_send_fr(struct dev_data *priv, unsigned long addr, int len)
+{
+	struct xlp_msg msg;
+	int code = 0;
+	int ret;
+	msg.entry[0] = (unsigned long long)addr & 0xffffffffe0ULL;
+	msg.entry[1]= msg.entry[2] = msg.entry[3] = 0;
+	/* Send the packet to nae rx  */
+	__sync();
+        if ( (ret = nlm_hal_send_msg1(priv->nae_rx_qid, code, msg.entry[0])) ){
+		print_fmn_send_error(__func__, ret);
+		printk("Unable to send configured free desc, check freein carving (qid=%d) ret 0x%x\n", priv->nae_rx_qid, ret);
+		return ret;
+	}
+	return ret;
+
+}
+
+void build_skb(struct sk_buff *skb, uint64_t *rxp2d, uint32_t*p2d_len, int num_desc,
+			uint32_t hlen, uint32_t length)
+{
+	int idx;
+	struct skb_shared_info *sp = skb_shinfo(skb);
+	struct jumbo_rx_cookie *rx_cookie = get_rx_cookie(rxp2d[0]);
+	struct page *pg = rx_cookie->page;
+	int rx_offset =  JUMBO_RX_OFFSET;
+	uint8_t *va = page_address(pg) + rx_cookie->page_offset + rx_offset;
+	skb_frag_t *fp = &sp->frags[0];
+
+	/* actual data starts IP header align */
+	skb_reserve(skb, 2);
+
+	skb->len = skb->data_len = length;
+	skb->truesize = length + sizeof(struct sk_buff);
+
+	fp->page= pg;
+	fp->page_offset = rx_cookie->page_offset + rx_offset + hlen;
+	fp->size = p2d_len[0]- hlen;
+
+	skb_copy_to_linear_data(skb, va, hlen);
+	skb->data_len -= hlen;
+	skb->tail += hlen;
+
+	/*fill other frags*/
+	for(idx=1; idx<num_desc; idx++){
+		fp = &sp->frags[idx];
+		rx_cookie = get_rx_cookie(rxp2d[idx]);
+		pg = rx_cookie->page;
+		rx_offset =  JUMBO_RX_OFFSET;
+		va = page_address(pg) + rx_cookie->page_offset + rx_offset;
+		fp->page= pg;
+		fp->page_offset = rx_cookie->page_offset + rx_offset;
+		fp->size = p2d_len[idx];
+	}
+	skb_shinfo(skb)->nr_frags = num_desc;
+}
+
+/* assumes that buffer is setup correctly */
+static void recycle_rx_desc(uint64_t phys, struct net_device *dev)
+{
+	struct dev_data* priv = netdev_priv(dev);
+	unsigned long msgrng_flags;
+
+	msgrng_access_enable(msgrng_flags);
+	mac_send_fr(priv, phys, jumbo_mtu);
+	msgrng_access_disable(msgrng_flags);
+}
+
+static int mac_frin_replenish_one_normal_msg(struct net_device *dev)
+{
+	jumbo_rx_info_t *rx;
+	struct dev_data* priv = netdev_priv(dev);
+	int ret = 0,num_buff;
+	unsigned long msgrng_flags;
+	struct jumbo_rx_cookie *rx_cookie;
+	struct page *pg;
+	void *va;
+	uint64_t pa, phys;
+	int cpu = smp_processor_id();
+
+	rx = &jumbo_rx_buff[cpu];
+
+	if(rx->space >= jumbo_buffer_size){
+		pg = rx->page;
+	} else {
+		/* alloc a new page */
+		pg = alloc_pages_exact_node(priv->node, GFP_KERNEL, 0); //alloc_pages(GFP_KERNEL, 0);
+		if(pg == NULL) {
+			panic("alloc_pages failure\n");
+		}
+
+		rx->page = pg;
+		rx->page_offset = 0;
+		rx->space = PAGE_SIZE;
+		num_buff = (PAGE_SIZE/jumbo_buffer_size);
+		atomic_set(&pg->_count, num_buff);
+	}
+	va = page_address(pg) + rx->page_offset;
+	pa = page_to_phys(pg) + rx->page_offset;
+	rx_cookie = (struct jumbo_rx_cookie *)va;
+	rx_cookie->page = pg;
+	rx_cookie->page_offset = rx->page_offset;
+
+	msgrng_access_enable(msgrng_flags);
+	/* account for s/w space and prepad */
+	phys = pa + JUMBO_RX_OFFSET;
+	if (mac_send_fr(priv, phys, jumbo_mtu)) {
+		msgrng_access_disable(msgrng_flags);
+		put_page(pg);
+		printk
+			("message_send failed!, unable to send free desc to mac\n");
+		ret = -EIO;
+		return ret;
+	}
+	msgrng_access_disable(msgrng_flags);
+	rx->page_offset += jumbo_buffer_size;
+	rx->space -= jumbo_buffer_size;
+
+	return ret;
+}
+
+static int mac_frin_replenish_msgs(struct net_device *dev, int num)
+{
+	jumbo_rx_info_t *rx;
+	struct dev_data* priv = netdev_priv(dev);
+	int cpu = smp_processor_id();
+
+	rx = &jumbo_rx_buff[cpu];
+	atomic_add(num, &rx->alloc_fails[priv->node][priv->port]);
+	schedule_work(&mac_frin_replenish_work[cpu]);
+	//tasklet_schedule(&mac_frin_replenish_task[cpu]);
+	return 0;
+}
+
+static void mac_frin_replenish(unsigned long  arg/* ignored */)
+{
+	int node = 0;
+	jumbo_rx_info_t *rx;
+	int cpu = smp_processor_id();
+	int done = 0, i, j;
+
+	rx = &jumbo_rx_buff[cpu];
+
+	for(node = 0; node < maxnae; node++) {
+	    for (;;) {
+		done = 0;
+	     	for (i = 0; i < PHOENIX_MAX_MACS; i++) {
+			struct net_device *dev;
+			struct dev_data* priv;
+			atomic_t *frin_to_be_sent;
+			int num_fr_in=0;
+
+			dev = dev_mac[i];
+			if (dev == 0)
+				goto skip;
+
+			priv = netdev_priv(dev);
+			frin_to_be_sent = &rx->alloc_fails[priv->node][i];
+			num_fr_in = atomic_read(frin_to_be_sent);
+
+			if (atomic_read(frin_to_be_sent) < 0) {
+				panic
+					("BUG?: [%s]: gmac_%d illegal value for frin_to_be_sent=%d\n",
+					 __FUNCTION__, i,
+					 atomic_read(frin_to_be_sent));
+			}
+
+			if (!atomic_read(frin_to_be_sent))
+				goto skip;
+
+			for(j=0; j<num_fr_in; j++)
+				mac_frin_replenish_one_normal_msg(dev);
+
+			atomic_sub(num_fr_in, frin_to_be_sent);
+			atomic_add(num_fr_in, &priv->total_frin_sent);
+
+			continue;
+		skip:
+			done++;
+		}
+		if (done == PHOENIX_MAX_MACS)
+			break;
+	    }
+	}
+}
+
+static int mac_fill_rxfr(struct net_device *dev)
+{
+	struct dev_data *priv = netdev_priv(dev);
+	unsigned long msgrng_flags;
+	int i, j;
+	int ret = 0;
+	struct page *pg;
+	void *va;
+	phys_t pa, phys;
+	struct jumbo_rx_cookie *rx_cookie;
+	int nr_buffs,limit;
+
+#ifdef CONFIG_PHOENIX_HW_BUFFER_MGMT
+#error "Jumbo support cannot be enabled with CONFIG_PHOENIX_HW_BUFFER_MGMT"
+#endif /* CONFIG_PHOENIX_HW_BUFFER_MGMT */
+
+	limit = atomic_read(&priv->frin_to_be_sent);
+
+	for(i = 0; i < limit; i++){
+		/*  get a page */
+		pg = alloc_pages_exact_node(priv->node, GFP_KERNEL, 0);
+		if(pg == NULL) {
+			ret = -ENOMEM;
+			break;
+		}
+		nr_buffs = (PAGE_SIZE/jumbo_buffer_size);
+		if((i + nr_buffs) >= limit)
+			nr_buffs = (limit - i);
+		i += nr_buffs;
+
+		pa = page_to_phys(pg);
+		va = page_address(pg);
+		j = 0;
+		while(nr_buffs) {
+			rx_cookie = (struct jumbo_rx_cookie *)va;
+			rx_cookie->page = pg;
+			rx_cookie->page_offset = (j * jumbo_buffer_size) ;
+			/* Send the free Rx desc to the MAC */
+			msgrng_access_enable(msgrng_flags);
+			phys = pa + JUMBO_RX_OFFSET;
+			if (mac_send_fr(priv, phys, jumbo_mtu)) {
+				msgrng_access_disable(msgrng_flags);
+				if(j)
+					put_page(pg);
+				printk
+				("message_send failed!, unable to send free desc to mac\n");
+				ret = -EIO;
+				break;
+			}
+			msgrng_access_disable(msgrng_flags);
+			va += jumbo_buffer_size;
+			pa += jumbo_buffer_size;
+			/* increment ref count from second particle */
+			if(j)
+				get_page(pg);
+			j++;
+			nr_buffs--;
+			atomic_dec(&priv->frin_to_be_sent);
+
+			atomic_inc(&priv->total_frin_sent);
+		}
+	}
+	return 0;
+
+}
+
+static int nlm_initialize_vfbid(int node, int fbvc)
+{
+	int cpu, i, dst_node = 0;
+	uint32_t vfbid_tbl[128];
+
+	for(i = 0; i < 128; i++)
+		vfbid_tbl[i] = 0;
+
+	for (cpu = 0; cpu < NR_CPUS ; cpu++) {
+		if(!cpu_isset(cpu, phys_cpu_present_map))
+                        continue;
+		dst_node = cpu / 32;
+		vfbid_tbl[cpu] = (dst_node << 10) | (((cpu % 32) * 4) + fbvc);
+	}
+	nlm_config_vfbid_table(node, 0, NR_CPUS, vfbid_tbl);	//FIXME change mappings for 127 and 126
+	return 0;
+}
+
 static int gen_mac_address(void)
 {
 	struct eeprom_data *nlm_eeprom=NULL;
@@ -611,7 +881,7 @@ static void nlm_xlp_nae_init(void)
 {
 	struct net_device *dev = NULL;
 	struct dev_data *priv = NULL;
-	int i, node = 0, cpu, maxnae;
+	int i, node = 0, cpu;
 	struct proc_dir_entry *entry;
 	nlm_nae_config_ptr nae_cfg;
 	uint32_t cpu_mask[NLM_MAX_NODES];
@@ -640,83 +910,88 @@ static void nlm_xlp_nae_init(void)
 
 	if (initialize_nae(cpumask_to_uint32(&cpu_online_map), 0, 0, 0))
 		return;
+	p2p_desc_mem_init();
+
+	for (i = 0; i < ((node+1) * 32); i++) {
+		INIT_WORK(&mac_frin_replenish_work[i], mac_frin_replenish);
+	}
 
 	gen_mac_address();
 
 	maxnae = nlm_node_cfg.num_nodes;	
 	for(node = 0; node < maxnae; node++) {
 		nae_cfg = nlm_node_cfg.nae_cfg[node];
-		if (nae_cfg != NULL)
-			break;
-	}
-	if (nae_cfg == NULL)
-	{
-	printk(KERN_ERR "\nnc is NULL\n");
-	BUG();
-	}
-	for (i = 0; i < nae_cfg->num_ports; i++) {
-		/* Register only valid ports which are management */
-		if (!nae_cfg->ports[i].valid)
+		if (nae_cfg == NULL)
 			continue;
 
-		dev = alloc_etherdev(sizeof(struct dev_data));
-		if (!dev) {
-			pr_err("%s: Failed to alloc dev, port = %d\n", __func__, i);
-			return;
-		}
-
-		ether_setup(dev);
-
-		priv = netdev_priv(dev);
-		spin_lock_init(&priv->lock);
-		priv->dev = dev;
-		dev->netdev_ops = &nlm_xlp_nae_ops;
-
-		/* set ethtool_ops which is inside xlp_ethtool.c file */
-		xlp_set_ethtool_ops(dev);
-
-		/*netif_napi_add(dev, &priv->napi, nlm_xlp_napi_poll, 16); */
-
-		dev->dev_addr = eth_hw_addr[0][i];
-		priv->port = i;
-
-		atomic64_set(&priv->frin_to_be_sent, nae_cfg->ports[i].num_free_desc);
-		atomic64_set(&priv->num_replenishes, 0);
-		atomic64_set(&priv->total_frin_sent, 0);
+		for(i = 0; i < nae_cfg->num_ports; i++)
+		{
+			/* Register only valid ports which are management */
+			if (!nae_cfg->ports[i].valid)
+				continue;
 
-		priv->inited = 0;
-		priv->block = nae_cfg->ports[i].hw_port_id / 4;
-		priv->type = nae_cfg->ports[i].iftype;
-		switch (nae_cfg->ports[i].iftype) {
-		case SGMII_IF:
-			priv->index = nae_cfg->ports[i].hw_port_id & 0x3;
-			priv->phy.addr = nae_cfg->ports[i].hw_port_id;
-			break;
-		case XAUI_IF:
-			priv->index = XGMAC;
-			break;
-		case INTERLAKEN_IF:
-			priv->index = INTERLAKEN;
-			break;
-		default:
-			priv->index = 0;
-			break;
-		}
-		if (debug)
-			nlm_print("port%d hw %d block %d index %d type %d \n", i, nae_cfg->ports[i].hw_port_id,
-				priv->block, priv->index, priv->type);
+			dev = alloc_etherdev(sizeof(struct dev_data));
+			if(!dev)
+				return;
 
-		priv->nae_tx_qid = nae_cfg->ports[i].txq;
-		priv->nae_rx_qid = nae_cfg->ports[i].rxq;
+			ether_setup(dev);
+
+			priv = netdev_priv(dev);
+			spin_lock_init(&priv->lock);
+			priv->dev 	= dev;
+			dev->netdev_ops = &nlm_xlp_nae_ops;
+
+			/* set ethtool_ops which is inside xlp_ethtool.c file*/
+			xlp_set_ethtool_ops(dev);
+
+			dev->dev_addr = eth_hw_addr[node][i];
+			priv->port	= i;
+
+			priv->frin_desc_thres = nae_cfg->ports[i].num_free_desc / 3;
+			atomic_set(&priv->frin_to_be_sent, nae_cfg->ports[i].num_free_desc);
+			atomic_set(&priv->num_replenishes, 0);
+			atomic_set(&priv->total_frin_sent, 0);
+
+			priv->inited = 0;
+			priv->node = node;
+			priv->block 	= nae_cfg->ports[i].hw_port_id / 4;
+			priv->type = nae_cfg->ports[i].iftype;
+			switch(nae_cfg->ports[i].iftype) {
+				case SGMII_IF:
+					priv->index = nae_cfg->ports[i].hw_port_id & 0x3;
+					priv->phy.addr = nae_cfg->ports[i].hw_port_id;
+					break;
+				case XAUI_IF:
+					priv->index = XGMAC;
+					break;
+				case INTERLAKEN_IF:
+					priv->index = INTERLAKEN;
+					priv->phy.addr = nae_cfg->ports[i].ext_phy_addr;
+					if (nae_cfg->ports[i].hw_port_id == 0) {
+                        	               if (dev_alloc_name(dev, "ilk0-%d") < 0)
+                                	                printk("alloc name failed \n");
+		                        }
+                	                else {
+                        	                if (dev_alloc_name(dev, "ilk8-%d") < 0)
+                                	                printk("alloc name failed \n");
+	                                }
+					break;
+				default:
+					priv->index=0;
+					break;
+			}
+			printk("port%d hw %d block %d index %d type %d \n",i, nae_cfg->ports[i].hw_port_id,
+									priv->block, priv->index, priv->type);
+			priv->nae_tx_qid 	= nae_cfg->ports[i].txq;
+			priv->nae_rx_qid 	= nae_cfg->ports[i].rxq;
 
 		register_netdev(dev);
 
 		dev_mac[i] = dev;
 		xlp_mac_setup_hwaddr(priv);
 
-		tasklet_init(&mac_refill_task[priv->port],
-				(void (*)(long unsigned int))mac_refill_frin_desc,
-				(unsigned long)dev);
+		}
+		nlm_initialize_vfbid(node, nae_cfg->fb_vc);
 	}
 
 	entry = create_proc_read_entry("mac_stats", 0 /* def mode */,
@@ -740,18 +1015,21 @@ static int  nlm_xlp_nae_open (struct net_device *dev)
 	int i;
 	int ret = 0;
 
-	if (priv->inited)
-		return 0;
+	if (priv->inited) return 0;
+	tso_enable(dev, 1);
+	dev->features |= NETIF_F_FRAGLIST | NETIF_F_HIGHDMA | NETIF_F_SG;
 
-	if (register_xlp_msgring_handler(XLP_MSG_HANDLE_NAE_0, nlm_xlp_nae_msgring_handler, dev)) {
-		pr_info("Fatal error! Can't register msgring handler for TX_STN_GMAC0");
+	ret = mac_fill_rxfr(dev);
+	if (ret)
+		goto out;
+
+	if(register_xlp_msgring_handler( XLP_MSG_HANDLE_NAE_0 , nlm_xlp_nae_msgring_handler, dev))
+	{
+		printk("Fatal error! Can't register msgring handler for TX_STN_GMAC0");
 		ret = -1;
 		goto out;
 	}
 
-	ret = mac_refill_frin_desc((unsigned long)dev);
-	if (ret)
-		goto out;
 
 #ifdef ENABLE_NAE_PIC_INT
 	{
@@ -769,7 +1047,7 @@ static int  nlm_xlp_nae_open (struct net_device *dev)
 	init_timer(&priv->link_timer);
 	/* priv->link_timer.expires = jiffies + HZ; First timer after 1 sec */
 	priv->link_timer.expires = jiffies + HZ; /* First timer after 1s */
-	priv->link_timer.data = (unsigned long) priv->port;
+	priv->link_timer.data    = (unsigned long)((priv->node << 16) | priv->port);
 	priv->link_timer.function = &nlm_xlp_mac_timer;
 	priv->phy_oldlinkstat = -1;
 
@@ -822,6 +1100,89 @@ static int  nlm_xlp_nae_stop (struct net_device *dev)
 	return 0;
 }
 
+static int p2p_desc_mem_init(void)
+{
+	int cpu, cnt;
+	int dsize, tsize;
+	void *buf;
+	/* MAX_SKB_FRAGS + 4.  Out of 4, 2 will be used for skb and freeback storage */
+	dsize = ((((MAX_SKB_FRAGS + P2P_EXTRA_DESCS) * sizeof(uint64_t)) + CACHELINE_SIZE - 1) & (~((CACHELINE_SIZE)-1)));
+	tsize = dsize * MAX_TSO_SKB_PEND_REQS;
+
+
+	for(cpu = 0; cpu < NR_CPUS; cpu++) {
+		buf = cacheline_aligned_kmalloc(tsize, GFP_KERNEL);
+		//spin_lock_init(&p2p_desc_mem[cpu].lock);
+		if (!buf)
+			return -ENOMEM;
+		p2p_desc_mem[cpu].mem = buf;
+		for(cnt = 1; cnt < MAX_TSO_SKB_PEND_REQS; cnt++) {
+			*(unsigned long *)buf = (unsigned long)(buf + dsize);
+			buf += dsize;
+			*(unsigned long *)buf = 0;
+		}
+
+		p2p_desc_mem[cpu].dsize = dsize;
+	}
+	return 0;
+}
+
+static inline void *alloc_p2p_desc_mem(int cpu)
+{
+	void *buf;
+    	//unsigned long flags;
+    	//spin_lock_irqsave(&p2p_desc_mem[cpu].lock, flags);
+    	/*Disabling irq as the critical section shared between
+ 	inteerupt context and xmit path. */
+    	local_irq_disable();
+	buf = p2p_desc_mem[cpu].mem;
+	if(buf) {
+		p2p_desc_mem[cpu].mem = (void *)*(unsigned long *)(buf);
+
+	} else {
+		buf = cacheline_aligned_kmalloc(p2p_desc_mem[cpu].dsize, GFP_KERNEL);
+		if (!buf)
+		{
+			local_irq_enable();
+			return NULL;
+		}
+		p2p_dynamic_alloc_cnt[CPU_INDEX(cpu)]++;
+		//printk("alloc_p2p_desc_mem p2p_dynamic_alloc_cnt cpu=0x%x\n", cpu);
+	}
+    	local_irq_enable();
+	//spin_unlock_irqrestore(&p2p_desc_mem[cpu].lock, flags);
+	return buf;
+}
+
+static inline void free_p2p_desc_mem(int cpu, void *buf)
+{
+	unsigned long flags;
+    	//spin_lock_irqsave(&p2p_desc_mem[cpu].lock, flags);
+	*(unsigned long *)buf = (unsigned long)p2p_desc_mem[cpu].mem;
+	p2p_desc_mem[cpu].mem = buf;
+	//spin_unlock_irqrestore(&p2p_desc_mem[cpu].lock, flags);
+}
+
+static inline int create_p2p_desc(uint64_t paddr, uint64_t len, uint64_t *p2pmsg, int idx)
+{
+	int plen;
+	do {
+		plen = len >= MAX_PACKET_SZ_PER_MSG ? (MAX_PACKET_SZ_PER_MSG - 64): len;
+		p2pmsg[idx] = cpu_to_be64(nae_tx_desc(P2D_NEOP, 0, NULL_VFBID, plen, paddr));
+		len -= plen;
+		paddr += plen;
+		idx++;
+
+	} while(len > 0);
+	return idx;
+}
+
+static inline void create_last_p2p_desc(uint64_t *p2pmsg, struct sk_buff *skb, int idx)
+{
+	p2pmsg[idx - 1] |= cpu_to_be64(((uint64_t)P2D_EOP << 62));
+	p2pmsg[P2P_SKB_OFF] = (uint64_t)skb;
+}
+
 
 /*
  * nlm_xlp_nae_start_xmit - transmit a packet from buffer
@@ -830,58 +1191,108 @@ static int  nlm_xlp_nae_stop (struct net_device *dev)
  */
 static int nlm_xlp_nae_start_xmit(struct sk_buff *skb, struct net_device *dev)
 {
+	int mss  = 0, idx = 0, len, i ;
+	struct skb_shared_info *sp = skb_shinfo(skb);
+	struct iphdr *iph;
 	struct dev_data *priv = netdev_priv(dev);
+	uint64_t msg, mscmsg0, mscmsg1;
+	unsigned long *p2pdesc;
+	int cpu = hard_smp_processor_id();
+	int  ret, retry_cnt = 0;
 	unsigned long mflags = 0;
-	int cpu = hard_smp_processor_id(), ret = 0;
-	struct xlp_msg msg = { { 0, 0, 0, 0} };
 
-	if (!skb) {
-		pr_info("[%s] skb is NULL\n", __func__);
-		return -1;
-	}
-	if (skb->len == 0) {
-		pr_info("[%s] skb empty packet\n", __func__);
-		return -1;
+
+	p2pdesc = alloc_p2p_desc_mem(cpu);
+	if(p2pdesc == NULL) {
+		printk("Failed to allocate p2p desc\n");
+		dev_kfree_skb_any(skb);
+		goto out_unlock;
 	}
 
-	msg.entry[0] = nae_tx_desc(P2D_NEOP, 0, cpu, 0, virt_to_bus(skb));
-	msg.entry[1] = nae_tx_desc(P2D_EOP,
-					0,
-					NULL_VFBID,
-					skb->len,
-					virt_to_bus(skb->data));
+	if (((mss = sp->gso_size) != 0) || (skb->ip_summed == CHECKSUM_PARTIAL)) {
+		u32 iphdroff, pyldoff, tcppcsum, udppcsum, l4hoff;
 
-	msg.entry[2] = msg.entry[3] = 0;
+		if (skb_header_cloned(skb) &&
+				pskb_expand_head(skb, 0, 0, GFP_ATOMIC)) {
+			dev_kfree_skb_any(skb);
+			free_p2p_desc_mem(cpu, p2pdesc);
+			goto out_unlock;
+		}
 
-	DUMP_PKT(__func__, skb->data, skb->len);
+		iph = ip_hdr(skb);
+		iphdroff = (char *)iph - (char *)skb->data;
+		l4hoff = iphdroff + ip_hdrlen(skb);
+
+		if(ip_hdr(skb)->protocol == IPPROTO_UDP){
+			pyldoff = iphdroff + ip_hdrlen(skb) + sizeof(struct udphdr);
+			udppcsum = udp_pseuodo_chksum((uint16_t *)((char *)iph + 12));
+			udp_hdr(skb)->check = 0;
+		}else{
+			pyldoff = iphdroff + ip_hdrlen(skb) + sizeof(struct tcphdr) + tcp_optlen(skb);
+			//printk("iphdroff %d tcphdroff %d pyldoff %d\n", iphdroff, l4hoff, pyldoff);
+			tcppcsum = tcp_pseuodo_chksum(((uint16_t *)((char *)iph + 12)));
+			tcp_hdr(skb)->check = 0;
+		}
 
-	__sync();
+		if(mss) {
+			iph->check = 0;
+			iph->tot_len = 0;
+			mscmsg0 = nae_tso_desc0(MSC, 1, TSO_IP_TCP_CHKSUM,
+				iphdroff, l4hoff, (iphdroff + 10),
+				tcppcsum, l4hoff + 16, pyldoff);
+			mscmsg1 = nae_tso_desc1(MSC, 2, 0, mss, 0, 0);
+		} else {
+			if(ip_hdr(skb)->protocol == IPPROTO_UDP){
+				mscmsg0 = nae_tso_desc0(MSC, 0, UDP_CHKSUM,
+					iphdroff, l4hoff, (iphdroff + 10),
+					udppcsum, l4hoff + 6, pyldoff);
+			}else{
+				mscmsg0 = nae_tso_desc0(MSC, 0, TCP_CHKSUM,
+					iphdroff, l4hoff, (iphdroff + 10),
+					tcppcsum, l4hoff + 16, pyldoff);
+			}
+		}
+	}
 
-	if (debug) {
-		pr_info("[%s]: tx_qid = %d, entry0 = %llx, entry1 = %llx\n", __func__,
-			priv->nae_tx_qid, msg.entry[0], msg.entry[1]);
+	if((len = skb_headlen(skb)) != 0) {
+		idx = create_p2p_desc(virt_to_bus((char *)skb->data), len, p2pdesc, idx);
 	}
 
-	msgrng_access_enable(mflags);
-/* retry_send: */
-	ret = nlm_hal_send_msg2(priv->nae_tx_qid,
-					0,
-					msg.entry[0],
-					msg.entry[1]);
-	if (ret) {
-		print_fmn_send_error(__func__, ret);
-		pr_info("[%s] HACK ALERT! dropping packet(skb = %p)!\n", __func__, skb);
-		dev_kfree_skb_any(skb);
-		/* goto retry_send; */
+	for (i = 0; i < sp->nr_frags; i++)  {
+		skb_frag_t *fp = &sp->frags[i];
+		idx = create_p2p_desc(virt_to_bus(((char *)page_address(fp->page)) + fp->page_offset),
+				fp->size, p2pdesc, idx);
 	}
 
+	create_last_p2p_desc(p2pdesc, skb, idx);
+	msg = nae_tx_desc(P2P, 0, cpu, idx, virt_to_bus(p2pdesc));
+
+retry_send:
+	msgrng_access_enable(mflags);
+	if(mss)
+		ret = nlm_hal_send_msg3(priv->nae_tx_qid, 0, mscmsg0, mscmsg1, msg);
+	else if(skb->ip_summed == CHECKSUM_PARTIAL)
+		ret = nlm_hal_send_msg2(priv->nae_tx_qid, 0, mscmsg0, msg);
+	else
+		ret = nlm_hal_send_msg1(priv->nae_tx_qid, 0, msg);
 	msgrng_access_disable(mflags);
-	dev->trans_start = jiffies;
+	if(ret){
+		retry_cnt++;
+		if(retry_cnt >= 128) {
+			dev_kfree_skb_any(skb);
+			free_p2p_desc_mem(cpu, p2pdesc);
+			STATS_ADD(priv->stats.tx_errors, 1);
+			goto out_unlock;
+		}
+		goto retry_send;
+	}
 
+	dev->trans_start = jiffies;
 	STATS_ADD(priv->stats.tx_bytes, skb->len);
 	STATS_INC(priv->stats.tx_packets);
 	priv->cpu_stats[cpu].tx_packets++;
 
+out_unlock:
 	return NETDEV_TX_OK;
 }
 
@@ -1072,10 +1483,12 @@ static void nlm_xlp_nae_msgring_handler(uint32_t vc, uint32_t src_id,
 {
 	struct net_device *pdev;
 	struct dev_data *priv;
-	unsigned int len, port = 0, context;
-	uint64_t addr, vaddr;
+	unsigned int len, hlen, port = 0, context;
+	uint64_t addr, vaddr = 0;
 	struct sk_buff *skb;
+	struct jumbo_rx_cookie *rx_cookie = NULL;
 	int cpu = 0, node = 0;
+	uint64_t *p2pfbdesc;
 
 	cpu = hard_smp_processor_id();
 	vc &= 0x03;
@@ -1089,46 +1502,41 @@ static void nlm_xlp_nae_msgring_handler(uint32_t vc, uint32_t src_id,
 		/* Process Transmit Complete, addr is the skb pointer */
 		addr = msg0 & 0xffffffffffULL;
 
-		if (drop_uboot_pkt) {
-			if ((addr >= (192<<20)) && (addr < (256 << 20))) {
-				pr_info("%s: Dropping firmware TXC packet (addr = %llx, vc = %d, size = %d)!\n", __func__, addr, vc, size);
+
+		if (!addr || drop_uboot_pkt) {
+			if ( (addr >= (192<<20)) && (addr < (256 << 20)) ) {
+				printk("Dropping firmware TXC packet (addr=%llx)!\n", addr);
 				stats_uboot_pkts++;
 				return;
 			}
 		}
-
-		/* context field is currently unused */
+		node = (src_id >> 10) & 0x3;
 		context = (msg0 >> 40) & 0x3fff;
-		port = cntx2port[context];
-#ifdef DEBUG_CONTEXT_PORT_MAPPING
-		if (port == 0)
-			pr_info("FB context %d port %d\n", context, port);
-#endif
-		skb = (struct sk_buff *)bus_to_virt(addr);
-		if (skb) {
-			priv = netdev_priv(skb->dev);
-
-			if (debug) {
-				pr_info("[%s][TXC] addr = %llx, skb = %p, context = %d, port = %d\n",
-					__func__, addr, skb, context, port);
-			}
+		port = *(cntx2port[node] + context);
+		p2pfbdesc = bus_to_virt(addr);
+		skb = (struct sk_buff *)(p2pfbdesc[P2P_SKB_OFF]);
+		priv = netdev_priv(skb->dev);
+		free_p2p_desc_mem(cpu, p2pfbdesc);
+		if(skb)
 			dev_kfree_skb_any(skb);
-
-			priv->cpu_stats[cpu].txc_packets++;
-		} else {
-			pr_info("[%s]: [txc] Null skb? paddr = %llx (halting cpu!)\n", __func__, addr);
-			cpu_halt();
-		}
+		priv->cpu_stats[cpu].txc_packets++;
 	} else if (vc == nae_rx_vc && size == 2) {
 		int bad_pkt = 0;
+		int is_p2p, num_p2d=0, tot_desc=0, idx;
 		int err = (msg1 >> 4) & 0x1;
 		int ip_csum_valid = (msg1 >> 3) & 0x1;
 		int tcp_csum_valid = (msg1 >> 2) & 0x1;
+		uint64_t p2d_addr[MAX_SKB_FRAGS];
+		uint32_t p2d_len[MAX_SKB_FRAGS];
 
 		/* Rx packet */
-		addr = msg1 & 0xffffffffc0ULL;
-		len = (msg1 >> 40) & 0x3fff;
+		is_p2p  = msg1 & 0x1;
+		addr	= msg1 & 0xffffffffe0ULL;
+		len	= (msg1 >> 40) & 0x3fff;
 		context = (msg1 >> 54) & 0x3ff;
+		if(is_p2p){
+			num_p2d = len;
+		}
 
 #ifdef DEBUG_RXPKT_ADDR_NULL
 		if (addr == 0) {
@@ -1168,7 +1576,39 @@ static void nlm_xlp_nae_msgring_handler(uint32_t vc, uint32_t src_id,
 		}
 		priv = netdev_priv(pdev);
 
-		vaddr = (uint64_t)bus_to_virt(addr);
+		/*check what kind of desc we received*/
+		if(is_p2p){
+			uint64_t p2d;
+			int idx;
+			struct page *pg;
+			//printk("Total p2d in p2p desc are = 0x%x\n", len);
+			num_p2d = len;
+			len = 0;
+			rx_cookie = get_rx_cookie(addr);
+			pg = rx_cookie->page;
+			/*free page count for P2P desc as it is not going to network stack */
+			/*Get actual length*/
+			for(idx=0; idx<num_p2d; idx++){
+				vaddr = (uint64_t)bus_to_virt(addr + (8*idx)); //got p2d virt addr
+				p2d = be64_to_cpu(*(uint64_t*)vaddr);
+				p2d_addr[idx] = p2d & 0xffffffffe0ULL;
+				p2d_len[idx] = (p2d >> 40) & 0x3fff;
+				bad_pkt |= (p2d >> 4) & 0x1;
+				len += (p2d >> 40) & 0x3fff;
+				//printk("P2D is at = 0x%lx physbuff= 0x%lx len= 0x%x\n", vaddr, p2d_addr[idx],  p2d_len[idx] );
+			}
+			tot_desc = num_p2d +1; //p2ds + p2p
+			put_page(pg);
+			//printk("Total packet length is = 0x%x and desc = 0x%x\n", len, tot_desc);
+		}else{ /*only one P2Dl_skb*/
+			vaddr = (uint64_t)bus_to_virt(addr);
+			p2d_addr[0] = addr;
+			p2d_len[0] = len - MAC_CRC_LEN;
+			len = len - MAC_CRC_LEN;
+			num_p2d = 1;
+			tot_desc = 1;
+			//printk("P2D  len = %d\n", len);
+		}
 
 		if (debug) {
 			pr_info("[%s][RX] addr = %llx, len = %d, context = %d, port = %d, vaddr = %llx\n",
@@ -1176,60 +1616,48 @@ static void nlm_xlp_nae_msgring_handler(uint32_t vc, uint32_t src_id,
 		}
 
 		DUMP_PKT("RX Packet: ", (unsigned char *)vaddr, len);
-
-		len = len - MAC_CRC_LEN;
-
-		skb = mac_get_skb_back_ptr(vaddr);
-		if (!skb) {
-			STATS_INC(priv->stats.rx_errors);
+		if (bad_pkt) {
+			struct page *pg;
+			int rx_offset;
 			STATS_INC(priv->stats.rx_dropped);
-			pr_info("[%s] Null skb? addr = %llx, vaddr = %llx, drop it!\n",
-				__func__, addr, vaddr);
-			cpu_halt();
-			return;
-		}
-
-		if (debug) {
-			struct iphdr *iph = (struct iphdr *)(vaddr + 14);
-			int net_pkt_len = iph->tot_len + 14;
-			int eth_proto = *(unsigned short *)(vaddr + 12);
-
-			if ((eth_proto == 0x800) && (net_pkt_len != len))
-				bad_pkt = 1;
-
-			if (bad_pkt) {
-				pr_info("[%s]: vaddr = %llx (len:%d/%d) (ip:proto = %d) (%d/%d/%d))\n",
-					__func__, vaddr, net_pkt_len, len, iph->protocol,
-					err, ip_csum_valid, tcp_csum_valid);
+			/*increase free count for used pages*/
+			for(idx=0; idx<num_p2d; idx++){
+				rx_cookie = get_rx_cookie(p2d_addr[idx]);
+				rx_offset =  JUMBO_RX_OFFSET;
+				pg = rx_cookie->page;
+				put_page(pg);
 			}
+			mac_frin_replenish_msgs(dev_mac[port], tot_desc);
+			return;
 		}
+		mac_frin_replenish_msgs(dev_mac[port], tot_desc);
 
-		if (bad_pkt) {
-			STATS_INC(priv->stats.rx_errors);
+		/* allocate an skb for header */
+		skb = dev_alloc_skb(NETL_JUMBO_SKB_HDR_LEN + 16);
+		if(skb == NULL) {
+			printk("FAILED TO ALLOCATE skb\n");
 			STATS_INC(priv->stats.rx_dropped);
 
-			dev_kfree_skb_any(skb);
-			goto out;
+			recycle_rx_desc(addr, pdev);
+			return;
 		}
-
-		skb_put(skb, len);
 		skb->dev = dev_mac[port];
-		skb->protocol = eth_type_trans(skb, dev_mac[port]);
+		hlen = (len > NETL_JUMBO_SKB_HDR_LEN) ?
+				NETL_JUMBO_SKB_HDR_LEN: len;
+		/* after this call, skb->data is pointing to start of MAChdr */
+		build_skb(skb, &p2d_addr[0], &p2d_len[0], num_p2d, hlen, len);
+		if (hlen == len) {
+			put_page(skb_shinfo(skb)->frags[0].page);
+			skb_shinfo(skb)->nr_frags = 0;
+			skb->data_len = 0;
+		}
+		skb->protocol = eth_type_trans(skb, skb->dev);
 		skb->dev->last_rx = jiffies;
-
-		/* Pass the packet to Network stack */
-		netif_rx (skb);
-
+		netif_receive_skb(skb);
 		/* Update Stats */
 		STATS_ADD(priv->stats.rx_bytes, len);
 		STATS_INC(priv->stats.rx_packets);
 		priv->cpu_stats[cpu].rx_packets++;
-
-out:
-		if (atomic_inc_return(&priv->frin_to_be_sent) > frin_desc_thres) {
-			tasklet_schedule(&mac_refill_task[port]);
-			/* mac_refill_frin_desc((unsigned long) skb->dev); */
-		}
 	} else {
 		pr_info("[%s]: wrong vc = %d or size = %d?\n", __func__, vc, size);
 	}
-- 
1.7.0

