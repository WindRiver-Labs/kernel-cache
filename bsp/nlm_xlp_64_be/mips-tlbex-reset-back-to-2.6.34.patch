From 835998ef18b996565cead71da955bd79158add9a Mon Sep 17 00:00:00 2001
From: Yong Zhang <yong.zhang@windriver.com>
Date: Thu, 17 Jul 2014 14:22:58 +0800
Subject: [PATCH 03/16] mips:tlbex: reset back to 2.6.34

Prepare to adopt update from newer SDK.

Signed-off-by: Yong Zhang <yong.zhang@windriver.com>
---
 arch/mips/mm/tlbex.c |  483 +++++++++++++++++++++-----------------------------
 1 files changed, 202 insertions(+), 281 deletions(-)

diff --git a/arch/mips/mm/tlbex.c b/arch/mips/mm/tlbex.c
index 27b1ea7..86f004d 100644
--- a/arch/mips/mm/tlbex.c
+++ b/arch/mips/mm/tlbex.c
@@ -1,11 +1,3 @@
-/*-
- * Copyright 2009-2012 Broadcom Corporation
- *
- * This is a derived work from software originally provided by the entity or
- * entities identified below. The licensing terms, warranty terms and other
- * terms specified in the header of the original work apply to this derived work
- *
- * #BRCM_1# */
 /*
  * This file is subject to the terms and conditions of the GNU General Public
  * License.  See the file "COPYING" in the main directory of this archive
@@ -37,25 +29,17 @@
 
 #include <asm/mmu_context.h>
 #include <asm/war.h>
+#include <asm/uasm.h>
 
-#include "uasm.h"
-
-#ifdef CONFIG_NLM_COMMON
-#include <asm/netlogic/mips-exts.h>
-#endif
-
-#ifdef CONFIG_HUGETLB_PAGE
-/**
- * This is applied to XLP (xlp8xx/xlp3xx only) hugetlb support.
- * Basically, if the RAM TLB is enabled, a tlbprobe may return a
- * fake hit. The huge page should not be written into RAM TLB
- * due to page size mismatch. The following checks are needed
- * for hugetlb invalid(tlbl/tlbs/tlbm) exception:
- *   . check tlbprobe result, if it is not in CAM TLB, change to
- *     write random
+/*
+ * TLB load/store/modify handlers.
+ *
+ * Only the fastpath gets synthesized at runtime, the slowpath for
+ * do_page_fault remains normal asm.
  */
-#define XLP_TLB_WORKAROUND
-#endif
+extern void tlb_do_page_fault_0(void);
+extern void tlb_do_page_fault_1(void);
+
 
 static inline int r45k_bvahwbug(void)
 {
@@ -109,19 +93,10 @@ enum label_id {
 	label_nopage_tlbm,
 	label_smp_pgtable_change,
 	label_r3000_write_probe_fail,
+	label_large_segbits_fault,
 #ifdef CONFIG_HUGETLB_PAGE
 	label_tlb_huge_update,
 #endif
-	label_nohpage_tlbs,
-	label_hpage_tlb_leave,
-	label_non_btlb_process,
-	label_read_entrylo1,
-	label_illegal_access_tlbl,
-	label_exl_refill_exception,
-	label_r4000_write_probe_fail,
-#ifdef CONFIG_HUGETLB_PAGE
-	label_r4000_write_huge_probe_fail,
-#endif
 };
 
 UASM_L_LA(_second_part)
@@ -137,19 +112,10 @@ UASM_L_LA(_nopage_tlbs)
 UASM_L_LA(_nopage_tlbm)
 UASM_L_LA(_smp_pgtable_change)
 UASM_L_LA(_r3000_write_probe_fail)
+UASM_L_LA(_large_segbits_fault)
 #ifdef CONFIG_HUGETLB_PAGE
 UASM_L_LA(_tlb_huge_update)
 #endif
-UASM_L_LA(_nohpage_tlbs)
-UASM_L_LA(_hpage_tlb_leave)
-UASM_L_LA(_non_btlb_process)
-UASM_L_LA(_read_entrylo1)
-UASM_L_LA(_illegal_access_tlbl)
-UASM_L_LA(_exl_refill_exception)
-UASM_L_LA(_r4000_write_probe_fail)
-#ifdef CONFIG_HUGETLB_PAGE
-UASM_L_LA(_r4000_write_huge_probe_fail)
-#endif
 
 /*
  * For debug purposes.
@@ -182,9 +148,6 @@ static inline void dump_handler(const u32 *handler, int count)
 #define C0_ENTRYHI	10, 0
 #define C0_EPC		14, 0
 #define C0_XCONTEXT	20, 0
-#define C0_CONFIG6     16, 6
-#define C0_WIRED	6, 0
-
 
 #ifdef CONFIG_64BIT
 # define GET_CONTEXT(buf, reg) UASM_i_MFC0(buf, reg, C0_XCONTEXT)
@@ -200,16 +163,22 @@ static inline void dump_handler(const u32 *handler, int count)
  * We deliberately chose a buffer size of 128, so we won't scribble
  * over anything important on overflow before we panic.
  */
-#if defined(CONFIG_MAPPED_KERNEL)
-static u32 tlb_handler[128];
-#else
-static u32 tlb_handler[128] __cpuinitdata ;
-#endif
+static u32 tlb_handler[128] __cpuinitdata;
 
 /* simply assume worst case size for labels and relocs */
 static struct uasm_label labels[128] __cpuinitdata;
 static struct uasm_reloc relocs[128] __cpuinitdata;
 
+#ifdef CONFIG_64BIT
+static int check_for_high_segbits __cpuinitdata;
+#endif
+
+#ifndef CONFIG_MIPS_PGD_C0_CONTEXT
+/*
+ * CONFIG_MIPS_PGD_C0_CONTEXT implies 64 bit and lack of pgd_current,
+ * we cannot do r3000 under these circumstances.
+ */
+
 /*
  * The R3000 TLB handler is simple.
  */
@@ -249,6 +218,7 @@ static void __cpuinit build_r3000_tlb_refill_handler(void)
 
 	dump_handler((u32 *)ebase, 32);
 }
+#endif /* CONFIG_MIPS_PGD_C0_CONTEXT */
 
 /*
  * The R4000 TLB handler is much more complicated. We have two
@@ -257,10 +227,7 @@ static void __cpuinit build_r3000_tlb_refill_handler(void)
  * other one.To keep things simple, we first assume linear space,
  * then we relocate it to the final handler layout as needed.
  */
-
-#if !defined(CONFIG_MAPPED_KERNEL)
 static u32 final_handler[64] __cpuinitdata;
-#endif
 
 /*
  * Hazards
@@ -357,7 +324,6 @@ static void __cpuinit build_tlb_write_entry(u32 **p, struct uasm_label **l,
 	case CPU_5KC:
 	case CPU_TX49XX:
 	case CPU_PR4450:
-	case CPU_XLP:
 		uasm_i_nop(p);
 		tlbw(p);
 		break;
@@ -451,22 +417,14 @@ static void __cpuinit build_tlb_write_entry(u32 **p, struct uasm_label **l,
 }
 
 static __cpuinit __maybe_unused void build_convert_pte_to_entrylo(u32 **p,
-                                 unsigned int reg)
+								  unsigned int reg)
 {
 	if (kernel_uses_smartmips_rixi) {
-#ifdef CONFIG_NLM_XLP
-#ifdef CONFIG_64BIT_PHYS_ADDR
-		uasm_i_dsrl(p, reg, reg, ilog2(_PAGE_GLOBAL));
-#else
-		UASM_i_SRL(p, reg, reg, ilog2(_PAGE_GLOBAL));
-#endif
-#else
 		UASM_i_SRL(p, reg, reg, ilog2(_PAGE_NO_EXEC));
 		UASM_i_ROTR(p, reg, reg, ilog2(_PAGE_GLOBAL) - ilog2(_PAGE_NO_EXEC));
-#endif
 	} else {
 #ifdef CONFIG_64BIT_PHYS_ADDR
-		uasm_i_dsrl(p, reg, reg, ilog2(_PAGE_GLOBAL));
+		uasm_i_dsrl_safe(p, reg, reg, ilog2(_PAGE_GLOBAL));
 #else
 		UASM_i_SRL(p, reg, reg, ilog2(_PAGE_GLOBAL));
 #endif
@@ -476,9 +434,9 @@ static __cpuinit __maybe_unused void build_convert_pte_to_entrylo(u32 **p,
 #ifdef CONFIG_HUGETLB_PAGE
 
 static __cpuinit void build_restore_pagemask(u32 **p,
-                        struct uasm_reloc **r,
-                        unsigned int tmp,
-                        enum label_id lid)
+					     struct uasm_reloc **r,
+					     unsigned int tmp,
+					     enum label_id lid)
 {
 	/* Reset default page size */
 	if (PM_DEFAULT_MASK >> 16) {
@@ -510,7 +468,6 @@ static __cpuinit void build_huge_tlb_write_entry(u32 **p,
 	build_tlb_write_entry(p, l, r, wmode);
 
 	build_restore_pagemask(p, r, tmp, label_leave);
-
 }
 
 /*
@@ -529,14 +486,32 @@ static __cpuinit void build_huge_update_entries(u32 **p,
 						unsigned int pte,
 						unsigned int tmp)
 {
+	int small_sequence;
+
+	/*
+	 * A huge PTE describes an area the size of the
+	 * configured huge page size. This is twice the
+	 * of the large TLB entry size we intend to use.
+	 * A TLB entry half the size of the configured
+	 * huge page size is configured into entrylo0
+	 * and entrylo1 to cover the contiguous huge PTE
+	 * address space.
+	 */
+	small_sequence = (HPAGE_SIZE >> 7) < 0x10000;
+
+	/* We can clobber tmp.  It isn't used after this.*/
+	if (!small_sequence)
+		uasm_i_lui(p, tmp, HPAGE_SIZE >> (7 + 16));
+
 	build_convert_pte_to_entrylo(p, pte);
 	UASM_i_MTC0(p, pte, C0_ENTRYLO0); /* load it */
+	/* convert to entrylo1 */
+	if (small_sequence)
+		UASM_i_ADDIU(p, pte, pte, HPAGE_SIZE >> 7);
+	else
+		UASM_i_ADDU(p, pte, pte, tmp);
 
-	uasm_i_ld(p, pte, sizeof(pte_t), tmp);
-	build_convert_pte_to_entrylo(p, pte);
 	UASM_i_MTC0(p, pte, C0_ENTRYLO1); /* load it */
-
-	uasm_i_ehb(p);
 }
 
 static __cpuinit void build_huge_handler_tail(u32 **p,
@@ -552,45 +527,8 @@ static __cpuinit void build_huge_handler_tail(u32 **p,
 #else
 	UASM_i_SW(p, pte, 0, ptr);
 #endif
-
-	/* adjust the ptep pointer to be at even entry boundary.
-	 * this is needed to write back entries to tlb.
-	 */
-	uasm_i_ori(p, ptr, ptr, sizeof(pte_t));
-	uasm_i_xori(p, ptr, ptr, sizeof(pte_t));
-	UASM_i_LW(p, pte, 0, ptr);
-
 	build_huge_update_entries(p, pte, ptr);
-
-#ifdef CONFIG_NLM_XLP
-	/* Similar to no hugetlb case, checking probe result.
-	 * FIXME: this should not happen really.
-	 */
-	uasm_i_mfc0(p, ptr, C0_INDEX);
-
-	uasm_il_bltz(p, r, ptr, label_r4000_write_huge_probe_fail);
-	uasm_i_nop(p);
-
-#ifdef XLP_TLB_WORKAROUND
-	/* if it is index, check the index range, if the index points to ram tlb, use random */
-	uasm_i_mtc0(p, pte, OS_SCRATCH_REG3);
-	uasm_i_mfc0(p, pte, C0_CONFIG6);
-	uasm_i_srl(p, pte, pte, 6);
-	uasm_i_andi(p, pte, pte, 0x3ff); /* get the num of variable tlb -1 */
-	uasm_i_andi(p, ptr, ptr, 0x1fff); /* the index value */
-	uasm_i_subu(p, ptr, pte, ptr); /* compute (num_cam_tlb - 1 - index) */
-	uasm_i_mfc0(p, pte, OS_SCRATCH_REG3);
-	uasm_il_bltz(p, r, ptr, label_r4000_write_huge_probe_fail);
-	uasm_i_nop(p);
-#endif
-
 	build_huge_tlb_write_entry(p, l, r, pte, tlb_indexed);
-	uasm_l_r4000_write_huge_probe_fail(l, *p);
-
-	build_huge_tlb_write_entry(p, l, r, pte, tlb_random);
-#else
-	build_huge_tlb_write_entry(p, l, r, pte, tlb_indexed);
-#endif
 }
 #endif /* CONFIG_HUGETLB_PAGE */
 
@@ -603,30 +541,56 @@ static void __cpuinit
 build_get_pmde64(u32 **p, struct uasm_label **l, struct uasm_reloc **r,
 		 unsigned int tmp, unsigned int ptr)
 {
+#ifndef CONFIG_MIPS_PGD_C0_CONTEXT
 	long pgdc = (long)pgd_current;
-
+#endif
 	/*
 	 * The vmalloc handling is not in the hotpath.
 	 */
 	uasm_i_dmfc0(p, tmp, C0_BADVADDR);
-	uasm_il_bltz(p, r, tmp, label_vmalloc);
+
+	if (check_for_high_segbits) {
+		/*
+		 * The kernel currently implicitely assumes that the
+		 * MIPS SEGBITS parameter for the processor is
+		 * (PGDIR_SHIFT+PGDIR_BITS) or less, and will never
+		 * allocate virtual addresses outside the maximum
+		 * range for SEGBITS = (PGDIR_SHIFT+PGDIR_BITS). But
+		 * that doesn't prevent user code from accessing the
+		 * higher xuseg addresses.  Here, we make sure that
+		 * everything but the lower xuseg addresses goes down
+		 * the module_alloc/vmalloc path.
+		 */
+		uasm_i_dsrl_safe(p, ptr, tmp, PGDIR_SHIFT + PGD_ORDER + PAGE_SHIFT - 3);
+		uasm_il_bnez(p, r, ptr, label_vmalloc);
+	} else {
+		uasm_il_bltz(p, r, tmp, label_vmalloc);
+	}
 	/* No uasm_i_nop needed here, since the next insn doesn't touch TMP. */
 
-#ifdef CONFIG_SMP
+#ifdef CONFIG_MIPS_PGD_C0_CONTEXT
+	/*
+	 * &pgd << 11 stored in CONTEXT [23..63].
+	 */
+	UASM_i_MFC0(p, ptr, C0_CONTEXT);
+	uasm_i_dins(p, ptr, 0, 0, 23); /* Clear lower 23 bits of context. */
+	uasm_i_ori(p, ptr, ptr, 0x540); /* 1 0  1 0 1  << 6  xkphys cached */
+	uasm_i_drotr(p, ptr, ptr, 11);
+#elif defined(CONFIG_SMP)
 # ifdef  CONFIG_MIPS_MT_SMTC
 	/*
 	 * SMTC uses TCBind value as "CPU" index
 	 */
 	uasm_i_mfc0(p, ptr, C0_TCBIND);
-	uasm_i_dsrl(p, ptr, ptr, 19);
+	uasm_i_dsrl_safe(p, ptr, ptr, 19);
 # else
 	/*
 	 * 64 bit SMP running in XKPHYS has smp_processor_id() << 3
 	 * stored in CONTEXT.
 	 */
 	uasm_i_dmfc0(p, ptr, C0_CONTEXT);
-	uasm_i_dsrl(p, ptr, ptr, 23);
-#endif
+	uasm_i_dsrl_safe(p, ptr, ptr, 23);
+# endif
 	UASM_i_LA_mostly(p, tmp, pgdc);
 	uasm_i_daddu(p, ptr, ptr, tmp);
 	uasm_i_dmfc0(p, tmp, C0_BADVADDR);
@@ -638,42 +602,78 @@ build_get_pmde64(u32 **p, struct uasm_label **l, struct uasm_reloc **r,
 
 	uasm_l_vmalloc_done(l, *p);
 
-	if (PGDIR_SHIFT - 3 < 32)		/* get pgd offset in bytes */
-		uasm_i_dsrl(p, tmp, tmp, PGDIR_SHIFT-3);
-	else
-		uasm_i_dsrl32(p, tmp, tmp, PGDIR_SHIFT - 3 - 32);
+	/* get pgd offset in bytes */
+	uasm_i_dsrl_safe(p, tmp, tmp, PGDIR_SHIFT - 3);
 
 	uasm_i_andi(p, tmp, tmp, (PTRS_PER_PGD - 1)<<3);
 	uasm_i_daddu(p, ptr, ptr, tmp); /* add in pgd offset */
+#ifndef __PAGETABLE_PMD_FOLDED
 	uasm_i_dmfc0(p, tmp, C0_BADVADDR); /* get faulting address */
 	uasm_i_ld(p, ptr, 0, ptr); /* get pmd pointer */
-	uasm_i_dsrl(p, tmp, tmp, PMD_SHIFT-3); /* get pmd offset in bytes */
+	uasm_i_dsrl_safe(p, tmp, tmp, PMD_SHIFT-3); /* get pmd offset in bytes */
 	uasm_i_andi(p, tmp, tmp, (PTRS_PER_PMD - 1)<<3);
 	uasm_i_daddu(p, ptr, ptr, tmp); /* add in pmd offset */
+#endif
 }
 
+enum vmalloc64_mode {not_refill, refill};
 /*
  * BVADDR is the faulting address, PTR is scratch.
  * PTR will hold the pgd for vmalloc.
  */
 static void __cpuinit
 build_get_pgd_vmalloc64(u32 **p, struct uasm_label **l, struct uasm_reloc **r,
-			unsigned int bvaddr, unsigned int ptr)
+			unsigned int bvaddr, unsigned int ptr,
+			enum vmalloc64_mode mode)
 {
 	long swpd = (long)swapper_pg_dir;
+	int single_insn_swpd;
+	int did_vmalloc_branch = 0;
+
+	single_insn_swpd = uasm_in_compat_space_p(swpd) && !uasm_rel_lo(swpd);
 
 	uasm_l_vmalloc(l, *p);
 
-	if (uasm_in_compat_space_p(swpd) && !uasm_rel_lo(swpd)) {
-		uasm_il_b(p, r, label_vmalloc_done);
-		uasm_i_lui(p, ptr, uasm_rel_hi(swpd));
-	} else {
-		UASM_i_LA_mostly(p, ptr, swpd);
-		uasm_il_b(p, r, label_vmalloc_done);
-		if (uasm_in_compat_space_p(swpd))
-			uasm_i_addiu(p, ptr, ptr, uasm_rel_lo(swpd));
-		else
-			uasm_i_daddiu(p, ptr, ptr, uasm_rel_lo(swpd));
+	if (mode == refill && check_for_high_segbits) {
+		if (single_insn_swpd) {
+			uasm_il_bltz(p, r, bvaddr, label_vmalloc_done);
+			uasm_i_lui(p, ptr, uasm_rel_hi(swpd));
+			did_vmalloc_branch = 1;
+			/* fall through */
+		} else {
+			uasm_il_bgez(p, r, bvaddr, label_large_segbits_fault);
+		}
+	}
+	if (!did_vmalloc_branch) {
+		if (uasm_in_compat_space_p(swpd) && !uasm_rel_lo(swpd)) {
+			uasm_il_b(p, r, label_vmalloc_done);
+			uasm_i_lui(p, ptr, uasm_rel_hi(swpd));
+		} else {
+			UASM_i_LA_mostly(p, ptr, swpd);
+			uasm_il_b(p, r, label_vmalloc_done);
+			if (uasm_in_compat_space_p(swpd))
+				uasm_i_addiu(p, ptr, ptr, uasm_rel_lo(swpd));
+			else
+				uasm_i_daddiu(p, ptr, ptr, uasm_rel_lo(swpd));
+		}
+	}
+	if (mode == refill && check_for_high_segbits) {
+		uasm_l_large_segbits_fault(l, *p);
+		/*
+		 * We get here if we are an xsseg address, or if we are
+		 * an xuseg address above (PGDIR_SHIFT+PGDIR_BITS) boundary.
+		 *
+		 * Ignoring xsseg (assume disabled so would generate
+		 * (address errors?), the only remaining possibility
+		 * is the upper xuseg addresses.  On processors with
+		 * TLB_SEGBITS <= PGDIR_SHIFT+PGDIR_BITS, these
+		 * addresses would have taken an address error. We try
+		 * to mimic that here by taking a load/istream page
+		 * fault.
+		 */
+		UASM_i_LA(p, ptr, (unsigned long)tlb_do_page_fault_0);
+		uasm_i_jr(p, ptr);
+		uasm_i_nop(p);
 	}
 }
 
@@ -781,21 +781,15 @@ static void __cpuinit build_update_entries(u32 **p, unsigned int tmp,
 		uasm_i_ld(p, tmp, 0, ptep); /* get even pte */
 		uasm_i_ld(p, ptep, sizeof(pte_t), ptep); /* get odd pte */
 		if (kernel_uses_smartmips_rixi) {
-#ifdef CONFIG_NLM_XLP
-			uasm_i_dsrl(p, tmp, tmp, ilog2(_PAGE_GLOBAL)); /* convert to entrylo0 */
-			UASM_i_MTC0(p, tmp, C0_ENTRYLO0); /* load it */
-			uasm_i_dsrl(p, ptep, ptep, ilog2(_PAGE_GLOBAL)); /* convert to entrylo1 */
-#else
 			UASM_i_SRL(p, tmp, tmp, ilog2(_PAGE_NO_EXEC));
 			UASM_i_SRL(p, ptep, ptep, ilog2(_PAGE_NO_EXEC));
 			UASM_i_ROTR(p, tmp, tmp, ilog2(_PAGE_GLOBAL) - ilog2(_PAGE_NO_EXEC));
 			UASM_i_MTC0(p, tmp, C0_ENTRYLO0); /* load it */
 			UASM_i_ROTR(p, ptep, ptep, ilog2(_PAGE_GLOBAL) - ilog2(_PAGE_NO_EXEC));
-#endif
 		} else {
-			uasm_i_dsrl(p, tmp, tmp, ilog2(_PAGE_GLOBAL)); /* convert to entrylo0 */
+			uasm_i_dsrl_safe(p, tmp, tmp, ilog2(_PAGE_GLOBAL)); /* convert to entrylo0 */
 			UASM_i_MTC0(p, tmp, C0_ENTRYLO0); /* load it */
-			uasm_i_dsrl(p, ptep, ptep, ilog2(_PAGE_GLOBAL)); /* convert to entrylo1 */
+			uasm_i_dsrl_safe(p, ptep, ptep, ilog2(_PAGE_GLOBAL)); /* convert to entrylo1 */
 		}
 		UASM_i_MTC0(p, ptep, C0_ENTRYLO1); /* load it */
 	} else {
@@ -808,21 +802,12 @@ static void __cpuinit build_update_entries(u32 **p, unsigned int tmp,
 		uasm_i_lw(p, ptep, pte_off_odd, ptep); /* get odd pte */
 		UASM_i_MTC0(p, ptep, C0_ENTRYLO1); /* load it */
 	}
-#else /* CONFIG_64BIT_PHYS_ADDR */
+#else
 	UASM_i_LW(p, tmp, 0, ptep); /* get even pte */
 	UASM_i_LW(p, ptep, sizeof(pte_t), ptep); /* get odd pte */
 	if (r45k_bvahwbug())
 		build_tlb_probe_entry(p);
 	if (kernel_uses_smartmips_rixi) {
-#ifdef CONFIG_NLM_XLP
-		UASM_i_SRL(p, tmp, tmp, ilog2(_PAGE_GLOBAL)); /* convert to entrylo0 */
-		if (r4k_250MHZhwbug())
-			UASM_i_MTC0(p, 0, C0_ENTRYLO0);
-		UASM_i_MTC0(p, tmp, C0_ENTRYLO0); /* load it */
-		UASM_i_SRL(p, ptep, ptep, ilog2(_PAGE_GLOBAL)); /* convert to entrylo1 */
-		if (r45k_bvahwbug())
-			uasm_i_mfc0(p, tmp, C0_INDEX);
-#else
 		UASM_i_SRL(p, tmp, tmp, ilog2(_PAGE_NO_EXEC));
 		UASM_i_SRL(p, ptep, ptep, ilog2(_PAGE_NO_EXEC));
 		UASM_i_ROTR(p, tmp, tmp, ilog2(_PAGE_GLOBAL) - ilog2(_PAGE_NO_EXEC));
@@ -830,8 +815,7 @@ static void __cpuinit build_update_entries(u32 **p, unsigned int tmp,
 			UASM_i_MTC0(p, 0, C0_ENTRYLO0);
 		UASM_i_MTC0(p, tmp, C0_ENTRYLO0); /* load it */
 		UASM_i_ROTR(p, ptep, ptep, ilog2(_PAGE_GLOBAL) - ilog2(_PAGE_NO_EXEC));
-#endif
-	}else {
+	} else {
 		UASM_i_SRL(p, tmp, tmp, ilog2(_PAGE_GLOBAL)); /* convert to entrylo0 */
 		if (r4k_250MHZhwbug())
 			UASM_i_MTC0(p, 0, C0_ENTRYLO0);
@@ -859,77 +843,58 @@ static void __cpuinit build_r4000_tlb_refill_handler(void)
 	u32 *p = tlb_handler;
 	struct uasm_label *l = labels;
 	struct uasm_reloc *r = relocs;
-	unsigned int final_len;
-
-#if !defined(CONFIG_MAPPED_KERNEL)
 	u32 *f;
-
-	memset(final_handler, 0, sizeof(final_handler));
-#endif
+	unsigned int final_len;
 
 	memset(tlb_handler, 0, sizeof(tlb_handler));
 	memset(labels, 0, sizeof(labels));
 	memset(relocs, 0, sizeof(relocs));
+	memset(final_handler, 0, sizeof(final_handler));
 
 	/*
 	 * create the plain linear handler
 	 */
 	if (bcm1250_m3_war()) {
-		UASM_i_MFC0(&p, K0, C0_BADVADDR);
-		UASM_i_MFC0(&p, K1, C0_ENTRYHI);
+		unsigned int segbits = 44;
+
+		uasm_i_dmfc0(&p, K0, C0_BADVADDR);
+		uasm_i_dmfc0(&p, K1, C0_ENTRYHI);
 		uasm_i_xor(&p, K0, K0, K1);
-		UASM_i_SRL(&p, K0, K0, PAGE_SHIFT + 1);
+		uasm_i_dsrl_safe(&p, K1, K0, 62);
+		uasm_i_dsrl_safe(&p, K0, K0, 12 + 1);
+		uasm_i_dsll_safe(&p, K0, K0, 64 + 12 + 1 - segbits);
+		uasm_i_or(&p, K0, K0, K1);
 		uasm_il_bnez(&p, &r, K0, label_leave);
 		/* No need for uasm_i_nop */
 	}
 
-#ifdef CONFIG_NLM_COMMON
-        uasm_i_dmfc0(&p, K0, OS_SCRATCH_REG2);
-        uasm_i_daddiu(&p, K0, K0, 1);
-        uasm_i_dmtc0(&p, K0, OS_SCRATCH_REG2);
-#endif
-
 #ifdef CONFIG_64BIT
 	build_get_pmde64(&p, &l, &r, K0, K1); /* get pmd in K1 */
 #else
 	build_get_pgde32(&p, K0, K1); /* get pgd in K1 */
 #endif
 
-	build_get_ptep(&p, K0, K1);
 #ifdef CONFIG_HUGETLB_PAGE
 	build_is_huge_pte(&p, &r, K0, K1, label_tlb_huge_update);
 #endif
 
+	build_get_ptep(&p, K0, K1);
 	build_update_entries(&p, K0, K1);
 	build_tlb_write_entry(&p, &l, &r, tlb_random);
-
 	uasm_l_leave(&l, p);
-	/*
-	 * FIXME: Do we need the following ifdef functionality
-	 */
-#ifdef CONFIG_NLM_COMMON
-	/* this is to avoid split of the table at eret instruction 
-	   The code below does a split at 30th instruction.
-	 */
-	if((p - tlb_handler) == 30) {
-		uasm_i_nop(&p);
-	}
-#endif
 	uasm_i_eret(&p); /* return from trap */
 
 #ifdef CONFIG_HUGETLB_PAGE
 	uasm_l_tlb_huge_update(&l, p);
 	UASM_i_LW(&p, K0, 0, K1);
 	build_huge_update_entries(&p, K0, K1);
-
 	build_huge_tlb_write_entry(&p, &l, &r, K0, tlb_random);
 #endif
 
 #ifdef CONFIG_64BIT
-	build_get_pgd_vmalloc64(&p, &l, &r, K0, K1);
+	build_get_pgd_vmalloc64(&p, &l, &r, K0, K1, refill);
 #endif
 
-#if !defined(CONFIG_MAPPED_KERNEL)
 	/*
 	 * Overflow check: For the 64bit handler, we need at least one
 	 * free instruction slot for the wrap-around branch. In worst
@@ -1027,58 +992,15 @@ static void __cpuinit build_r4000_tlb_refill_handler(void)
 	}
 #endif /* CONFIG_64BIT */
 
-#endif /* !defined(CONFIG_MAPPED_KERNEL) */
-
 	uasm_resolve_relocs(relocs, labels);
 	pr_debug("Wrote TLB refill handler (%u instructions).\n",
 		 final_len);
 
-#if !defined(CONFIG_MAPPED_KERNEL)
 	memcpy((void *)ebase, final_handler, 0x100);
-#endif
 
 	dump_handler((u32 *)ebase, 64);
 }
 
-#if defined(CONFIG_MAPPED_KERNEL)
-
-static u32 tlb_handler_stub[32] __cpuinitdata;
-
-static void __cpuinit __attribute__((unused)) build_r4000_tlb_refill_handler_stub(void)
-{
-	u32 *p = tlb_handler_stub;
-
-	memset(tlb_handler_stub, 0, sizeof(tlb_handler_stub));
-	UASM_i_LA(&p, K0, (unsigned long) tlb_handler);
-	uasm_i_jr(&p, K0);
-	uasm_i_nop(&p);
-
-	/*
-	 * 32 instruction = 128 bytes
-	 */
-#ifdef CONFIG_64BIT
-	memcpy((void *)ebase + 0x80, tlb_handler_stub, 0x80); /* XTLB exception */
-#else
-	memcpy((void *)ebase, tlb_handler_stub, 0x80); /* TLB exception */
-#endif
-}
-
-#else
-
-static void __cpuinit __attribute__((unused)) build_r4000_tlb_refill_handler_stub(void) { }
-
-#endif /* defined(CONFIG_MAPPED_KERNEL) */
-
-/*
- * TLB load/store/modify handlers.
- *
- * Only the fastpath gets synthesized at runtime, the slowpath for
- * do_page_fault remains normal asm.
- */
-extern void tlb_do_page_fault_0(void);
-extern void tlb_do_page_fault_1(void);
-extern void tlb_do_page_fault_2(void);
-
 /*
  * 128 instructions for the fastpath handler is generous and should
  * never be exceeded.
@@ -1234,6 +1156,7 @@ build_pte_modifiable(u32 **p, struct uasm_reloc **r,
 	iPTE_LW(p, pte, ptr);
 }
 
+#ifndef CONFIG_MIPS_PGD_C0_CONTEXT
 /*
  * R3000 style TLB load/store/modify handlers.
  */
@@ -1385,6 +1308,7 @@ static void __cpuinit build_r3000_tlb_modify_handler(void)
 
 	dump_handler(handle_tlbm, ARRAY_SIZE(handle_tlbm));
 }
+#endif /* CONFIG_MIPS_PGD_C0_CONTEXT */
 
 /*
  * R4000 style TLB load/store/modify handlers.
@@ -1400,16 +1324,21 @@ build_r4000_tlbchange_handler_head(u32 **p, struct uasm_label **l,
 	build_get_pgde32(p, pte, ptr); /* get pgd in ptr */
 #endif
 
+#ifdef CONFIG_HUGETLB_PAGE
+	/*
+	 * For huge tlb entries, pmd doesn't contain an address but
+	 * instead contains the tlb pte. Check the PAGE_HUGE bit and
+	 * see if we need to jump to huge tlb processing.
+	 */
+	build_is_huge_pte(p, r, pte, ptr, label_tlb_huge_update);
+#endif
+
 	UASM_i_MFC0(p, pte, C0_BADVADDR);
 	UASM_i_LW(p, ptr, 0, ptr);
 	UASM_i_SRL(p, pte, pte, PAGE_SHIFT + PTE_ORDER - PTE_T_LOG2);
 	uasm_i_andi(p, pte, pte, (PTRS_PER_PTE - 1) << PTE_T_LOG2);
 	UASM_i_ADDU(p, ptr, ptr, pte);
 
-#ifdef CONFIG_HUGETLB_PAGE
-	build_is_huge_pte(p, r, pte, ptr, label_tlb_huge_update);
-#endif
-
 #ifdef CONFIG_SMP
 	uasm_l_smp_pgtable_change(l, *p);
 #endif
@@ -1426,22 +1355,12 @@ build_r4000_tlbchange_handler_tail(u32 **p, struct uasm_label **l,
 	uasm_i_ori(p, ptr, ptr, sizeof(pte_t));
 	uasm_i_xori(p, ptr, ptr, sizeof(pte_t));
 	build_update_entries(p, tmp, ptr);
-	uasm_i_tlbp(p);
-	uasm_i_ehb(p);
-	uasm_i_mfc0(p, ptr, C0_INDEX);
-	uasm_il_bltz(p, r, ptr, label_r4000_write_probe_fail);
-	uasm_i_nop(p);
-
 	build_tlb_write_entry(p, l, r, tlb_indexed);
-	uasm_i_eret(p);
-
-	uasm_l_r4000_write_probe_fail(l, *p);
-	build_tlb_write_entry(p, l, r, tlb_random);
 	uasm_l_leave(l, *p);
 	uasm_i_eret(p); /* return from trap */
 
 #ifdef CONFIG_64BIT
-	build_get_pgd_vmalloc64(p, l, r, tmp, ptr);
+	build_get_pgd_vmalloc64(p, l, r, tmp, ptr, not_refill);
 #endif
 }
 
@@ -1456,10 +1375,15 @@ static void __cpuinit build_r4000_tlb_load_handler(void)
 	memset(relocs, 0, sizeof(relocs));
 
 	if (bcm1250_m3_war()) {
-		UASM_i_MFC0(&p, K0, C0_BADVADDR);
-		UASM_i_MFC0(&p, K1, C0_ENTRYHI);
+		unsigned int segbits = 44;
+
+		uasm_i_dmfc0(&p, K0, C0_BADVADDR);
+		uasm_i_dmfc0(&p, K1, C0_ENTRYHI);
 		uasm_i_xor(&p, K0, K0, K1);
-		UASM_i_SRL(&p, K0, K0, PAGE_SHIFT + 1);
+		uasm_i_dsrl_safe(&p, K1, K0, 62);
+		uasm_i_dsrl_safe(&p, K0, K0, 12 + 1);
+		uasm_i_dsll_safe(&p, K0, K0, 64 + 12 + 1 - segbits);
+		uasm_i_or(&p, K0, K0, K1);
 		uasm_il_bnez(&p, &r, K0, label_leave);
 		/* No need for uasm_i_nop */
 	}
@@ -1471,21 +1395,13 @@ static void __cpuinit build_r4000_tlb_load_handler(void)
 
 	if (kernel_uses_smartmips_rixi) {
 		/*
-		* If the page is not _PAGE_VALID, RI or XI could not
-        * have triggered it.  Skip the expensive test..
-        */
+		 * If the page is not _PAGE_VALID, RI or XI could not
+		 * have triggered it.  Skip the expensive test..
+		 */
 		uasm_i_andi(&p, K0, K0, _PAGE_VALID);
 		uasm_il_beqz(&p, &r, K0, label_tlbl_goaround1);
 		uasm_i_nop(&p);
-#ifdef CONFIG_NLM_XLP
-		/*
-		 *if tlb probe has failed, skip to label_tlbl_goaround assuming
-		 *exception is due to valid bit not being set in entrylo.
-		 */
-		uasm_i_mfc0(&p, K0, C0_INDEX);
-		uasm_il_bltz(&p, &r, K0, label_tlbl_goaround1);
-		uasm_i_nop(&p);
-#endif
+
 		uasm_i_tlbr(&p);
 		/* Examine  entrylo 0 or 1 based on ptr. */
 		uasm_i_andi(&p, K0, K1, sizeof(pte_t));
@@ -1494,9 +1410,9 @@ static void __cpuinit build_r4000_tlb_load_handler(void)
 		UASM_i_MFC0(&p, K0, C0_ENTRYLO0); /* load it in the delay slot*/
 		UASM_i_MFC0(&p, K0, C0_ENTRYLO1); /* load it if ptr is odd */
 		/*
-		* If the entryLo (now in K0) is valid (bit 1), RI or
-		* XI must have triggered it.
-		*/
+		 * If the entryLo (now in K0) is valid (bit 1), RI or
+		 * XI must have triggered it.
+		 */
 		uasm_i_andi(&p, K0, K0, 2);
 		uasm_il_bnez(&p, &r, K0, label_nopage_tlbl);
 
@@ -1504,7 +1420,6 @@ static void __cpuinit build_r4000_tlb_load_handler(void)
 		/* Reload the PTE value */
 		iPTE_LW(&p, K0, K1);
 	}
-
 	build_make_valid(&p, &r, K0, K1);
 	build_r4000_tlbchange_handler_tail(&p, &l, &r, K0, K1);
 
@@ -1520,9 +1435,9 @@ static void __cpuinit build_r4000_tlb_load_handler(void)
 
 	if (kernel_uses_smartmips_rixi) {
 		/*
-		* If the page is not _PAGE_VALID, RI or XI could not
-        * have triggered it.  Skip the expensive test..
-        */
+		 * If the page is not _PAGE_VALID, RI or XI could not
+		 * have triggered it.  Skip the expensive test..
+		 */
 		uasm_i_andi(&p, K0, K0, _PAGE_VALID);
 		uasm_il_beqz(&p, &r, K0, label_tlbl_goaround2);
 		uasm_i_nop(&p);
@@ -1535,23 +1450,22 @@ static void __cpuinit build_r4000_tlb_load_handler(void)
 		UASM_i_MFC0(&p, K0, C0_ENTRYLO0); /* load it in the delay slot*/
 		UASM_i_MFC0(&p, K0, C0_ENTRYLO1); /* load it if ptr is odd */
 		/*
-		* If the entryLo (now in K0) is valid (bit 1), RI or
-		* XI must have triggered it.
-		*/
+		 * If the entryLo (now in K0) is valid (bit 1), RI or
+		 * XI must have triggered it.
+		 */
 		uasm_i_andi(&p, K0, K0, 2);
 		uasm_il_beqz(&p, &r, K0, label_tlbl_goaround2);
 		/* Reload the PTE value */
 		iPTE_LW(&p, K0, K1);
 
 		/*
-		* We clobbered C0_PAGEMASK, restore it.  On the other branch
-		* it is restored in build_huge_tlb_write_entry.
-		*/
+		 * We clobbered C0_PAGEMASK, restore it.  On the other branch
+		 * it is restored in build_huge_tlb_write_entry.
+		 */
 		build_restore_pagemask(&p, &r, K0, label_nopage_tlbl);
 
 		uasm_l_tlbl_goaround2(&l, p);
 	}
-
 	uasm_i_ori(&p, K0, K0, (_PAGE_ACCESSED | _PAGE_VALID));
 	build_huge_handler_tail(&p, &r, &l, K0, K1);
 #endif
@@ -1670,6 +1584,10 @@ void __cpuinit build_tlb_refill_handler(void)
 	 */
 	static int run_once = 0;
 
+#ifdef CONFIG_64BIT
+	check_for_high_segbits = current_cpu_data.vmbits > (PGDIR_SHIFT + PGD_ORDER + PAGE_SHIFT - 3);
+#endif
+
 	switch (current_cpu_type()) {
 	case CPU_R2000:
 	case CPU_R3000:
@@ -1678,6 +1596,7 @@ void __cpuinit build_tlb_refill_handler(void)
 	case CPU_TX3912:
 	case CPU_TX3922:
 	case CPU_TX3927:
+#ifndef CONFIG_MIPS_PGD_C0_CONTEXT
 		build_r3000_tlb_refill_handler();
 		if (!run_once) {
 			build_r3000_tlb_load_handler();
@@ -1685,6 +1604,9 @@ void __cpuinit build_tlb_refill_handler(void)
 			build_r3000_tlb_modify_handler();
 			run_once++;
 		}
+#else
+		panic("No R3000 TLB refill handler");
+#endif
 		break;
 
 	case CPU_R6000:
@@ -1696,15 +1618,14 @@ void __cpuinit build_tlb_refill_handler(void)
 		panic("No R8000 TLB refill handler yet");
 		break;
 
-		default:
-			build_r4000_tlb_refill_handler();
-			build_r4000_tlb_refill_handler_stub();
-			if (!run_once) {
-				build_r4000_tlb_load_handler();
-				build_r4000_tlb_store_handler();
-				build_r4000_tlb_modify_handler();
-				run_once++;
-			}
+	default:
+		build_r4000_tlb_refill_handler();
+		if (!run_once) {
+			build_r4000_tlb_load_handler();
+			build_r4000_tlb_store_handler();
+			build_r4000_tlb_modify_handler();
+			run_once++;
+		}
 	}
 }
 
-- 
1.7.0

