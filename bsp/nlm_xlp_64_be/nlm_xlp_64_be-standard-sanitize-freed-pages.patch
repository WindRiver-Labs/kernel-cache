From 34c71d2c92f3294b692cd8dfe32bedd7a694d167 Mon Sep 17 00:00:00 2001
From: Yanjiang Jin <yanjiang.jin@windriver.com>
Date: Thu, 3 May 2012 11:10:21 +0800
Subject: [PATCH 28/46] nlm_xlp_64_be-standard: sanitize freed pages

The problem:
When formatting a hard disk partition that is more
than 200G, the following oops may be triggered in
standard kernel:

Unhandled kernel unaligned access[#1]:
Cpu 9
$ 0   : 0000000000000000 ffffffff82226d64 0000000000000000 0000000000000000
14$ 4   : a8000000a03dba80 a7ffffffffffffc0 ffffffff82691acc 0000000000000000
$ 8   : a7ffffffffffffc0 0000400000000000 fb5f1fffbc41fdff 0000000000000040
$12   : 000000005400f8e1 000000001000001e 0000000000000000 a8000000a0400000
$16   : a8000000a03dba80 a7ffffffffffffc0 ffffffffdeb1ffc0 ffffffff82226d64
$20   : ffffffff82bd0000 a800000000000000 0000000000000000 ffffffff82bd19e0
$24   : 0000000000000000 000000002ac2aba4
$28   : a8000000a03d8000 a8000000a03db6f0 a8000000a03db6f0 ffffffff82226d64
Hi    : 0000000000000000
Lo    : 003d08f97a5acd00
epc   : ffffffff82226a08 emulate_load_store_insn+0x278/0x490
    Not tainted
ra    : ffffffff82226d64 do_ade+0x144/0x2d0
Status: 5400f8e3    KX SX UX KERNEL EXL IE
Cause : 00800010
BadVA : a7ffffffffffffc0
PrId  : 000c1100 (XLP316 Rev A0)
Modules linked in:
Process softirq-net-rx/ (pid: 115, threadinfo=a8000000a03d8000, task=a8000000a03c10f0, tls=0000000000000000)
Stack : 0000000000000000 a800000082aa6198 a800000082aa8aa0 a8000000a0090040
        0000000000000000 0000000000000009 ffffffff82b5d778 ffffffff82a9fa40
        0000000000000048 a8000000a03db748 ffffffff8224e088 a8000000a03db758
        ffffffff8225a408 0000000000000003 ffffffff82348e8c a8000000a0090040
        a800000082aa8a40 a8000000a03db788 ffffffff8225ace0 a8000000a03db7e8
        a8000000a0090000 a800000082aa8a40 0000000000000000 a8000000a0090000
        a800000082aa8a40 ffffffff82aa2508 ffffffff82a2e100 a8000000a03db7d8
        ffffffff82348e8c ffffffff82a2e0c0 0000000000000000 ffffffff82a2e0c0
        ffffffff82a2e100 a8000000a03db808 ffffffff828540d8 a8000000a03db818
        ffffffff82331950 0000000000000001 a800000081f28020 0000000000000000
        ...
Call Trace:
[<ffffffff82226a08>] emulate_load_store_insn+0x278/0x490
[<ffffffff82226d64>] do_ade+0x144/0x2d0
[<ffffffff8221e444>] ret_from_exception+0x0/0x10
[<ffffffff82691acc>] nlm_xlp_nae_msgring_handler+0x114/0x6e8
[<ffffffff8221acd0>] xlp_fmn_poll+0x138/0x2d8
[<ffffffff82762464>] net_rx_action+0x21c/0x420
[<ffffffff8226f2e8>] run_ksoftirqd+0x220/0x420
[<ffffffff82290380>] kthread+0x98/0xa0
[<ffffffff82220e14>] kernel_thread_helper+0x24/0x30

The solution:
Erase memory pages as soon as they are freed.

Grsec (b00cbf76228471315f00fc628f39f517d5d86ea4) has a
feature that erases memory pages as they are freed,
and that it can be applied to solve the problem.

Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 include/asm-generic/kmap_types.h |    3 ++-
 include/linux/highmem.h          |   12 ++++++++++++
 mm/page_alloc.c                  |   17 +++++++++++++++++
 3 files changed, 31 insertions(+), 1 deletions(-)

diff --git a/include/asm-generic/kmap_types.h b/include/asm-generic/kmap_types.h
index 97e807c..bb8a8d8 100644
--- a/include/asm-generic/kmap_types.h
+++ b/include/asm-generic/kmap_types.h
@@ -29,7 +29,8 @@ KMAP_D(16)	KM_IRQ_PTE,
 KMAP_D(17)	KM_NMI,
 KMAP_D(18)	KM_NMI_PTE,
 KMAP_D(19)	KM_KDB,
-KMAP_D(20)	KM_TYPE_NR
+KMAP_D(20)	KM_CLEARPAGE,
+KMAP_D(21)	KM_TYPE_NR
 };
 
 #undef KMAP_D
diff --git a/include/linux/highmem.h b/include/linux/highmem.h
index 74152c0..723fcc8 100644
--- a/include/linux/highmem.h
+++ b/include/linux/highmem.h
@@ -143,6 +143,18 @@ static inline void clear_highpage(struct page *page)
 	kunmap_atomic(kaddr, KM_USER0);
 }
 
+static inline void sanitize_highpage(struct page *page)
+{
+	void *kaddr;
+	unsigned long flags;
+
+	local_irq_save(flags);
+	kaddr = kmap_atomic(page, KM_CLEARPAGE);
+	clear_page(kaddr);
+	kunmap_atomic(kaddr, KM_CLEARPAGE);
+	local_irq_restore(flags);
+}
+
 static inline void zero_user_segments(struct page *page,
 	unsigned start1, unsigned end1,
 	unsigned start2, unsigned end2)
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index e89b02f..27b799b 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -611,6 +611,10 @@ static void __free_pages_ok(struct page *page, unsigned int order)
 	int bad = 0;
 	int wasMlocked = __TestClearPageMlocked(page);
 
+#ifdef CONFIG_NLM_XLP
+	unsigned long index = 1UL << order;
+#endif
+
 	trace_mm_page_free_direct(page, order);
 	kmemcheck_free_shadow(page, order);
 
@@ -626,6 +630,12 @@ static void __free_pages_ok(struct page *page, unsigned int order)
 		debug_check_no_obj_freed(page_address(page),
 					   PAGE_SIZE << order);
 	}
+
+#ifdef CONFIG_NLM_XLP
+	for (; index; --index)
+		sanitize_highpage(page + index - 1);
+#endif
+
 	arch_free_page(page, order);
 	kernel_map_pages(page, 1 << order, 0);
 
@@ -729,8 +739,10 @@ static int prep_new_page(struct page *page, int order, gfp_t gfp_flags)
 	arch_alloc_page(page, order);
 	kernel_map_pages(page, 1 << order, 1);
 
+#ifndef CONFIG_NLM_XLP
 	if (gfp_flags & __GFP_ZERO)
 		prep_zero_page(page, order, gfp_flags);
+#endif
 
 	if (order && (gfp_flags & __GFP_COMP))
 		prep_compound_page(page, order);
@@ -1128,6 +1140,11 @@ void free_hot_cold_page(struct page *page, int cold)
 		debug_check_no_locks_freed(page_address(page), PAGE_SIZE);
 		debug_check_no_obj_freed(page_address(page), PAGE_SIZE);
 	}
+
+#ifdef CONFIG_NLM_XLP
+	sanitize_highpage(page);
+#endif
+
 	arch_free_page(page, 0);
 	kernel_map_pages(page, 1, 0);
 
-- 
1.7.0

