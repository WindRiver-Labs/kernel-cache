From f2fe57ca8d26e6cc053bb8e7715c1b0843ce5f68 Mon Sep 17 00:00:00 2001
From: Yanjiang Jin <yanjiang.jin@windriver.com>
Date: Thu, 10 Jan 2013 12:17:38 +0800
Subject: [PATCH] nlm_xlp_64_be: restore some CPU_HOTPLUG functions

Restore some CPU_HOTPLUG relative functions.

Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 arch/mips/netlogic/xlp/on_chip.c |  214 +++++++++++++++++++++++++++++++++++++-
 1 files changed, 209 insertions(+), 5 deletions(-)

diff --git a/arch/mips/netlogic/xlp/on_chip.c b/arch/mips/netlogic/xlp/on_chip.c
index 11696f4..85e6724 100644
--- a/arch/mips/netlogic/xlp/on_chip.c
+++ b/arch/mips/netlogic/xlp/on_chip.c
@@ -393,7 +393,7 @@ void nlm_xlp_msgring_int_handler(unsigned int irq, struct pt_regs *regs)
 	int cpu = hard_smp_processor_id();
 	int pop_vc_mask = nlm_cpu_vc_mask[cpu];
 	uint32_t napi_vc_mask = xlp_napi_vc_mask & pop_vc_mask;
-	unsigned long mflags; /* Currently unused */
+	unsigned long __maybe_unused mflags; /* Currently unused */
 	unsigned int vcmask;
 
 	if (irq == XLP_IRQ_MSGRING_RVEC) {
@@ -653,6 +653,46 @@ void nlm_nmi_cpus(unsigned int mask)
 		nlh_pic_w64r(0, XLP_PIC_IPI_CTL, (nmi << 31) | (1 << 16) | (cpumask_hi));
 }
 
+/* need COP2 to be accessible */
+static void on_chip_msgring_drain_msgs(void)
+{
+	int vc = 0;
+	uint32_t size = 0, code = 0, src_id = 0;
+	uint64_t msg0, msg1, msg2, msg3;
+	unsigned int msg_status1 = 0, vc_empty_status = 0;
+	int cpu = hard_smp_processor_id();
+	int pop_vc_mask = nlm_cpu_vc_mask[cpu];
+
+	msg0 = msg1 = msg2 = msg3 = 0;
+
+	/* loop is just to be safe, but should only hit once in theory */
+	for ( ; ; ) {
+		/* Read latest VC empty mask */
+		msg_status1 = xlp_read_status1();
+
+		vc_empty_status = (msg_status1 >> 24) & pop_vc_mask;
+		if (vc_empty_status == pop_vc_mask)
+			break;
+
+		for (vc = 0; vc < 4; vc++) {
+			if (!(pop_vc_mask & (1<<vc)))
+				continue;
+			(void) xlp_message_receive(vc, &src_id, &size,
+				&code, &msg0, &msg1, &msg2, &msg3);
+		}
+	}
+}
+
+void on_chip_shutoff_msgring(void)
+{
+	uint32_t __maybe_unused flags;
+
+	/* Need write interrupt vector to cp2 msgconfig register */
+	msgrng_access_enable(flags);
+	on_chip_msgring_drain_msgs();
+	xlp_write_config(0);
+	msgrng_access_disable(flags);
+}
 
 /*********************************************************************
  * enable_msgconfig_int 
@@ -668,6 +708,153 @@ void enable_msgconfig_int(void)
 	msgrng_access_disable(flags);
 }
 
+/*
+ * Initializes PIC ITE entries PRM 9.5.6.26
+ * XLP restricts CPU affinity to 8 groups. They are,
+ * 0 =>	Only cpu0/thread0; mask = 1
+ * 1 => All CPUs/threads and nodes; mask = (~0 & online_cpu_mask) on all nodes
+ * 2 => cpu0-1 on all nodes. mask = 0x000000ff& online_cpu_mask  on all nodes
+ * 3 => cpu2-3 on all nodes; mask = 0x0000ff00 & online_cpu_mask on all nodes
+ * 4 => cpu4-5 on all nodes; mask = 0x00ff0000 & online_cpu_mask on all nodes
+ * 5 => cpu6-7 on all nodes; mask = 0xff000000 & online_cpu_mask on all nodes
+ * 6 => cpu0-15 on all nodes; mask = 0x0000ffff & online_cpu_mask on all nodes
+ * 7 => cpu15-31 on all nodes; mask = 0xffff0000 & online_cpu_mask on all nodes
+ *
+ * These are programmer defined groups and can be changed as warranted.
+ *
+ * FIXME: for NUMA, we assume all nodes will have identical intra-node cpu
+ * online masks.
+ */
+static struct cpumask xlp_ite_cpumask[XLP_ITE_ENTRIES];
+void xlp_pic_ite_init(const struct cpumask *tgt_mask)
+{
+	int i;
+	struct cpumask m;
+	u64 xlp_pic_base = XLP_BDF_BASE(0, 0, 4);
+	char buf[140];
+	u64 bitmask = 0;
+#ifdef CONFIG_NUMA
+	struct cpumask m1, m2;
+#endif
+
+#ifndef CONFIG_NUMA
+	pr_warn("Setting ITE entries only for 0-31 (Node 0) CPUs!\n");
+#endif
+	cpumask_clear(&m);
+	/* We manipulate only NODE0 ITE entries here */
+	for (i = 0; i < XLP_ITE_ENTRIES; i++)
+		cpumask_clear(&xlp_ite_cpumask[i]);
+	cpumask_set_cpu(cpumask_first(tgt_mask), &xlp_ite_cpumask[0]);
+
+	/* Set 0-31 cpus, if present in cpu_online mask */
+	for (i = cpumask_first(tgt_mask); i < 32;) {
+		bitmask |= (1ULL << i);
+		i = cpumask_next(i, tgt_mask);
+	}
+
+	/* Set 0-7 cpus */
+	for (i = 0; i < 8; i++)
+		cpumask_set_cpu(i, &m);
+
+	cpumask_scnprintf(buf, 140, tgt_mask);
+	fdebug("Target cpumask -> %s\n", buf);
+#ifndef CONFIG_NUMA
+	cpumask_copy(&xlp_ite_cpumask[1], tgt_mask);
+#else
+	cpumask_shift_left(&m1, &m, 8);
+	cpumask_or(&m1, &m1, &m);
+	cpumask_shift_left(&m2, &m1, 16);
+	cpumask_or(&m2, &m2, &m1);
+	cpumask_and(&m2, &m2, tgt_mask);
+	cpumask_copy(&xlp_ite_cpumask[1], &m2);
+#endif
+
+	/* logical and with cpuonline mask to get the actual mask */
+	cpumask_and(&xlp_ite_cpumask[2], &m, tgt_mask);
+	cpumask_shift_left(&xlp_ite_cpumask[3], &m, 8);
+	cpumask_and(&xlp_ite_cpumask[3], &xlp_ite_cpumask[3], tgt_mask);
+	cpumask_shift_left(&xlp_ite_cpumask[4], &m, 16);
+	cpumask_and(&xlp_ite_cpumask[4], &xlp_ite_cpumask[4], tgt_mask);
+	cpumask_shift_left(&xlp_ite_cpumask[5], &m, 24);
+	cpumask_and(&xlp_ite_cpumask[5], &xlp_ite_cpumask[5], tgt_mask);
+
+	cpumask_shift_left(&xlp_ite_cpumask[6], &m, 8);
+	cpumask_or(&xlp_ite_cpumask[6], &xlp_ite_cpumask[6], &m);
+	cpumask_shift_left(&xlp_ite_cpumask[7], &xlp_ite_cpumask[6], 16);
+	cpumask_and(&xlp_ite_cpumask[6], &xlp_ite_cpumask[6], tgt_mask);
+	cpumask_and(&xlp_ite_cpumask[7], &xlp_ite_cpumask[7], tgt_mask);
+
+
+	for (i = 0; i < XLP_ITE_ENTRIES; i++) {
+		cpumask_scnprintf(buf, 140, &xlp_ite_cpumask[i]);
+		pr_debug("Supported CPUMASK (%d) -> %s\n", i, buf);
+	}
+
+#ifndef CONFIG_NUMA
+	/* Right shift by 1 is required by HAL, _DO_NOT_REMOVE_ */
+	nlm_hal_write_64bit_reg(xlp_pic_base,
+				0x94 >> 1, (0x00000001 & bitmask));
+	nlm_hal_write_64bit_reg(xlp_pic_base,
+0x98 >> 1, (0xffffffff & bitmask));
+	nlm_hal_write_64bit_reg(xlp_pic_base,
+				0x9C >> 1, (0x000000ff & bitmask));
+	nlm_hal_write_64bit_reg(xlp_pic_base,
+				0xA0 >> 1, (0x0000ff00 & bitmask));
+	nlm_hal_write_64bit_reg(xlp_pic_base,
+				0xA4 >> 1, (0x00ff0000 & bitmask));
+	nlm_hal_write_64bit_reg(xlp_pic_base,
+				0xA8 >> 1, (0xff000000 & bitmask));
+	nlm_hal_write_64bit_reg(xlp_pic_base,
+				0xAC >> 1, (0x0000ffff & bitmask));
+	nlm_hal_write_64bit_reg(xlp_pic_base,
+				0xB0 >> 1, (0xffff0000 & bitmask));
+	/* We don't populate redirection to other nodes now */
+#else
+	for_each_online_node(i) {
+		/* Interrupt delivered only to local node */
+		int node_offset = (i >= 2) * 2;
+		int mask_shift = ((i == 1) || (i == 3)) * 32;
+
+		fdebug("Programming ITEs for Node %d\n", i);
+
+		xlp_pic_base = XLP_BDF_BASE(0, 0 + 8 * i, 4);
+
+		nlm_hal_write_64bit_reg(
+			xlp_pic_base,
+			(0x94 + node_offset) >> 1,
+			((uint64_t)(0x00000001 & bitmask)) << mask_shift);
+		nlm_hal_write_64bit_reg(
+			xlp_pic_base,
+			(0x98 + node_offset) >> 1,
+			((uint64_t)(0xffffffff & bitmask)) << mask_shift);
+		nlm_hal_write_64bit_reg(
+			xlp_pic_base,
+			(0x9C + node_offset) >> 1,
+			((uint64_t)(0x000000ff & bitmask)) << mask_shift);
+		nlm_hal_write_64bit_reg(
+			xlp_pic_base,
+			(0xA0 + node_offset) >> 1,
+			((uint64_t)(0x0000ff00 & bitmask)) << mask_shift);
+		nlm_hal_write_64bit_reg(
+			xlp_pic_base,
+			(0xA4 + node_offset) >> 1,
+			((uint64_t)(0x00ff0000 & bitmask)) << mask_shift);
+		nlm_hal_write_64bit_reg(
+			xlp_pic_base,
+			(0xA8 + node_offset) >> 1,
+			((uint64_t)(0xff000000 & bitmask)) << mask_shift);
+		nlm_hal_write_64bit_reg(
+			xlp_pic_base,
+			(0xAC + node_offset) >> 1,
+			((uint64_t)(0x0000ffff & bitmask)) << mask_shift);
+		nlm_hal_write_64bit_reg(
+			xlp_pic_base,
+			(0xB0 + node_offset) >> 1,
+			((uint64_t)(0xffff0000 & bitmask)) << mask_shift);
+	}
+#endif
+}
+
 atomic_t nlm_common_counters[NR_CPUS][NLM_MAX_COUNTERS] __cacheline_aligned;
 
 #if 0
@@ -710,12 +897,11 @@ void nlm_enable_vc_intr(void)
 	int vc_index = 0;
 	int i = 0;
 
-	for(cpu=0; cpu<NR_CPUS; cpu++){
+	for_each_possible_cpu(cpu) {
                 if(!cpu_isset(cpu, phys_cpu_present_map))
                         continue;
 		node = cpu / 32;
-		for(i=0; i<NLM_MAX_VC_PER_THREAD; i++)
-		{
+		for (i = 0; i < NLM_MAX_VC_PER_THREAD; i++) {
 			vc_index = (i + cpu*NLM_MAX_VC_PER_THREAD) & 0x7f;
 			if(nlm_cpu_vc_mask[cpu] & (1<<i)){
 				/*enable interrupts*/
@@ -727,6 +913,23 @@ void nlm_enable_vc_intr(void)
 	}
 }
 
+/*********************************************************************
+ * nlm_disable_vc_intr
+ *********************************************************************/
+void nlm_disable_vc_intr(void)
+{
+	int cpu = hard_smp_processor_id();
+	int vc_index = 0;
+	int i = 0;
+
+	for (i = 0; i < NLM_MAX_VC_PER_THREAD; i++) {
+		if (nlm_cpu_vc_mask[cpu] & (1<<i)) {
+			vc_index = (i + cpu*NLM_MAX_VC_PER_THREAD) & 0x7f;
+			/*enable interrupts*/
+			nlm_hal_disable_vc_intr(0, vc_index);
+		}
+	}
+}
 
 int xlp_fmn_poll(struct napi_struct *napi, int budget)
 {
@@ -765,7 +968,8 @@ int xlp_fmn_poll(struct napi_struct *napi, int budget)
 #ifdef CONFIG_32BIT
 			msgrng_access_enable(mflags);
 #endif
-			status = xlp_message_receive( vc, &src_id, &size, &code, &msg0, &msg1, &msg2, &msg3);
+			status = xlp_message_receive(vc, &src_id, &size, &code,
+						&msg0, &msg1, &msg2, &msg3);
 #ifdef CONFIG_32BIT
 			msgrng_access_disable(mflags);
 #endif
-- 
1.7.0

