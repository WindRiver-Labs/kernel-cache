From 4e5bd1cc981b446fda7f841dc6ef58513e342883 Mon Sep 17 00:00:00 2001
From: Yanjiang Jin <yanjiang.jin@windriver.com>
Date: Wed, 16 May 2012 15:09:52 +0800
Subject: [PATCH 3/3] nlm_xlp_64_be: default enable MAPPED_KERNEL

SDK 2.2.4 default enables the kernel option MAPPED_KERNEL, some XLP
special features(such as PAGE_WALKER) only run normally with this
option, else we sometimes get "BUS error" or "Segmentation fault"
when we running some userspace applications.
So following the SDK 2.2.4, we also enable MAPPED_KERNEL by default.

Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 arch/mips/Kconfig                                  |    2 +
 arch/mips/Makefile                                 |   22 ++
 arch/mips/boot/dts/xlp316.dts                      |    4 +-
 arch/mips/boot/dts/xlp832.dts                      |    4 +-
 arch/mips/include/asm/bootinfo.h                   |    2 +-
 arch/mips/include/asm/io.h                         |    4 -
 arch/mips/include/asm/mach-generic/spaces.h        |   18 ++-
 .../include/asm/mach-netlogic/kernel-entry-init.h  |   62 ++++++
 arch/mips/include/asm/netlogic/phnx_loader.h       |  210 +++++++++++++++++++
 arch/mips/include/asm/page.h                       |   40 +++-
 arch/mips/include/asm/pgalloc.h                    |    4 +-
 arch/mips/include/asm/pgtable-32.h                 |   13 +-
 arch/mips/include/asm/pgtable-64.h                 |   66 ++----
 arch/mips/include/asm/pgtable.h                    |    5 +-
 arch/mips/kernel/head.S                            |    9 +-
 arch/mips/kernel/setup.c                           |    8 +
 arch/mips/kernel/vmlinux.lds.S                     |   19 ++-
 arch/mips/mm/tlb-r4k.c                             |   40 ++++-
 arch/mips/mm/tlbex.c                               |  218 ++++++++++++--------
 arch/mips/netlogic/Kconfig                         |   24 ++-
 arch/mips/netlogic/common/memory.c                 |  184 +++++++++++++++--
 arch/mips/netlogic/xlp/Makefile                    |    2 +-
 arch/mips/netlogic/xlp/cpu_control_asm.S           |   18 +-
 arch/mips/netlogic/xlp/nmi.S                       |    1 -
 arch/mips/netlogic/xlp/setup.c                     |  110 +++++++---
 25 files changed, 868 insertions(+), 221 deletions(-)
 create mode 100644 arch/mips/include/asm/mach-netlogic/kernel-entry-init.h
 create mode 100644 arch/mips/include/asm/netlogic/phnx_loader.h

diff --git a/arch/mips/Kconfig b/arch/mips/Kconfig
index 9bbd9f0..f29c9a9 100644
--- a/arch/mips/Kconfig
+++ b/arch/mips/Kconfig
@@ -2000,6 +2000,8 @@ config ARCH_FLATMEM_ENABLE
 
 config ARCH_DISCONTIGMEM_ENABLE
 	bool
+	default y if NLM_XLP_EVP_BOARD
+	default y if NLM_XLP_EVP_N511_BOARD
 	default y if SGI_IP27
 	help
 	  Say Y to support efficient handling of discontiguous physical memory,
diff --git a/arch/mips/Makefile b/arch/mips/Makefile
index ca5afad..39a88d4 100644
--- a/arch/mips/Makefile
+++ b/arch/mips/Makefile
@@ -729,6 +729,28 @@ endif
 
 OBJCOPYFLAGS		+= --remove-section=.reginfo
 
+ifdef CONFIG_MAPPED_KERNEL
+PHYS_LOAD_ADDRESS = -D"PHYSADDR=$(CONFIG_PHYS_LOAD_ADDRESS)"
+endif
+
+#
+# Choosing incompatible machines durings configuration will result in
+# error messages during linking.  Select a default linkscript if
+# none has been choosen above.
+#
+
+
+CPPFLAGS_vmlinux.lds := \
+	$(CFLAGS) \
+	-D"LOADADDR=$(load-y)" $(PHYS_LOAD_ADDRESS) \
+	-D"JIFFIES=$(JIFFIES)" \
+	-D"DATAOFFSET=$(if $(dataoffset-y),$(dataoffset-y),0)"
+
+ifdef CONFIG_MAPPED_KERNEL
+KBUILD_CFLAGS += -D"LOADADDR=$(load-y)ULL" -D"PHYSADDR=$(CONFIG_PHYS_LOAD_ADDRESS)ULL"
+KBUILD_CFLAGS += -D"LOADADDR_ASM=$(load-y)" -D"PHYSADDR_ASM=$(CONFIG_PHYS_LOAD_ADDRESS)"
+endif
+
 head-y := arch/mips/kernel/head.o arch/mips/kernel/init_task.o
 
 libs-y			+= arch/mips/lib/
diff --git a/arch/mips/boot/dts/xlp316.dts b/arch/mips/boot/dts/xlp316.dts
index 4fa951f..e817c0d 100644
--- a/arch/mips/boot/dts/xlp316.dts
+++ b/arch/mips/boot/dts/xlp316.dts
@@ -46,8 +46,8 @@
 
 			memory {
 				/* <Start Size>, Unit: M */
-				reg = <0x00010000 0x13ff0000 // 320M@64K
-				       0x1d000000 0xa3000000 >; // 2608M@464M
+				reg = <0x01000000 0x0B000000	// 176M at 16M
+					0x20000000 0xa0000000>;  // 2586M at 512M
 			};
                         fmn {
                                 node_0_vc_mask = <0x33333333 0x33333333 0x33333333 0x33333333>;
diff --git a/arch/mips/boot/dts/xlp832.dts b/arch/mips/boot/dts/xlp832.dts
index ee34503..eaf927e 100644
--- a/arch/mips/boot/dts/xlp832.dts
+++ b/arch/mips/boot/dts/xlp832.dts
@@ -46,8 +46,8 @@
 
 			memory {
 				/* <Start Size>, Unit: M */
-				reg = <0x00010000 0x13ff0000 // 320M@64K
-				       0x1d000000 0xa3000000 >; // 2608M@464M
+				reg = <0x01000000 0x0B000000	// 176M at 16M
+					0x20000000 0xa0000000>;  // 2586M at 512M
 			};
                         fmn {
                                 node_0_vc_mask = <0x33333333 0x33333333 0x33333333 0x33333333>;
diff --git a/arch/mips/include/asm/bootinfo.h b/arch/mips/include/asm/bootinfo.h
index 2d27b1f..57431c7 100644
--- a/arch/mips/include/asm/bootinfo.h
+++ b/arch/mips/include/asm/bootinfo.h
@@ -91,7 +91,7 @@ struct boot_mem_map {
 		phys_t addr;	/* start of memory segment */
 		phys_t size;	/* size of memory segment */
 		long type;		/* type of memory segment */
-	} map[BOOT_MEM_MAP_MAX];
+	} map[BOOT_MEM_MAP_MAX + 1];
 };
 
 extern struct boot_mem_map boot_mem_map;
diff --git a/arch/mips/include/asm/io.h b/arch/mips/include/asm/io.h
index 7f88071..c98bf51 100644
--- a/arch/mips/include/asm/io.h
+++ b/arch/mips/include/asm/io.h
@@ -116,10 +116,6 @@ static inline void set_io_port_base(unsigned long base)
  */
 static inline unsigned long virt_to_phys(volatile const void *address)
 {
-#ifdef CONFIG_64BIT
-	if((unsigned long) address >= CKSEG0)
-		return (unsigned long)address - CKSEG0 + PHYS_OFFSET;
-#endif
 	return (unsigned long)address - PAGE_OFFSET + PHYS_OFFSET;
 }
 
diff --git a/arch/mips/include/asm/mach-generic/spaces.h b/arch/mips/include/asm/mach-generic/spaces.h
index c9fa4b1..f161f24 100644
--- a/arch/mips/include/asm/mach-generic/spaces.h
+++ b/arch/mips/include/asm/mach-generic/spaces.h
@@ -21,7 +21,11 @@
 
 #ifdef CONFIG_32BIT
 
+#if defined(CONFIG_MAPPED_KERNEL) && defined(CONFIG_KSEG2_LOWMEM)
+#define CAC_BASE                _AC(0xc0000000, UL)
+#else
 #define CAC_BASE		_AC(0x80000000, UL)
+#endif
 #define IO_BASE			_AC(0xa0000000, UL)
 #define UNCAC_BASE		_AC(0xa0000000, UL)
 
@@ -43,10 +47,18 @@
 #ifndef CAC_BASE
 #ifdef CONFIG_DMA_NONCOHERENT
 #define CAC_BASE		_AC(0x9800000000000000, UL)
-#else
+#else /* !CONFIG_DMA_NONCOHERENT */
+
+#if defined(CONFIG_MAPPED_KERNEL) && defined(CONFIG_KSEG2_LOWMEM)
+#define CAC_BASE                XKSEG
+#define PAGE_OFFSET	XKSEG
+#else /* !CONFIG_MAPPED_KERNEL */
 #define CAC_BASE		_AC(0xa800000000000000, UL)
-#endif
-#endif
+#endif /* CONFIG_MAPPED_KERNEL */
+
+#endif /* CONFIG_DMA_NONCOHERENT */
+
+#endif /* CAC_BASE */
 
 #ifndef IO_BASE
 #define IO_BASE			_AC(0x9000000000000000, UL)
diff --git a/arch/mips/include/asm/mach-netlogic/kernel-entry-init.h b/arch/mips/include/asm/mach-netlogic/kernel-entry-init.h
new file mode 100644
index 0000000..f353d2e
--- /dev/null
+++ b/arch/mips/include/asm/mach-netlogic/kernel-entry-init.h
@@ -0,0 +1,62 @@
+#ifndef __ASM_MACH_NLM_KERNEL_ENTRY_H
+#define __ASM_MACH_NLM_KERNEL_ENTRY_H
+
+/* XLP_MERGE_TODO */
+#if !defined(CKSSEG)
+#ifdef CONFIG_64BIT
+#define CKSSEG			0xffffffffc0000000
+#else
+#define CKSSEG                  0xc0000000
+#endif
+#endif
+
+#ifdef CONFIG_64BIT
+#define LA dla
+#define MTC0 dmtc0
+#define SW sd
+#else
+#define LA la
+#define MTC0 mtc0
+#define SW sw
+#endif
+
+#ifdef CONFIG_CPU_XLP
+#define JRHB jr.hb
+#define EHB ehb
+#else
+#define JRHB jr
+#define EHB
+#endif
+
+	/*
+	 * inputs are the text nasid in t1, data nasid in t2.
+	 */
+	.macro MAPPED_KERNEL_SETUP_TLB
+#ifdef CONFIG_MAPPED_KERNEL
+	/*
+	 * Drop in 0xffffffffc0000000 in tlbhi, 0+VG in tlblo_0,
+	 * 0+DVG in tlblo_1.
+	 */
+	dli	    t3, CKSSEG
+	dmtc0	t3, CP0_ENTRYHI
+	li      t1, 0x1f
+	MTC0	t1, CP0_ENTRYLO0	# physaddr, VG, cach exlwr
+	li	    t2, 0x1
+	MTC0	t2, CP0_ENTRYLO1	# physaddr, DVG, cach exlwr
+	li	    t1, 0x1fffe000		# MAPPED_KERN_TLBMASK, TLBPGMASK_256M
+	mtc0	t1, CP0_PAGEMASK
+    mtc0    zero, CP0_INDEX
+	tlbwi
+	li      t0, 1
+    mtc0	t0, CP0_WIRED
+	EHB
+    LA      v0, mapped_space
+	JRHB    v0
+	nop
+mapped_space:
+#else
+	mtc0	zero, CP0_WIRED
+#endif
+	.endm
+
+#endif /* __ASM_MACH_NLM_KERNEL_ENTRY_H */
diff --git a/arch/mips/include/asm/netlogic/phnx_loader.h b/arch/mips/include/asm/netlogic/phnx_loader.h
new file mode 100644
index 0000000..429884a
--- /dev/null
+++ b/arch/mips/include/asm/netlogic/phnx_loader.h
@@ -0,0 +1,210 @@
+/***********************************************************************
+ * Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+ * reserved.
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ * 1. Redistributions of source code must retain the above copyright
+ * notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ * notice, this list of conditions and the following disclaimer in
+ * the documentation and/or other materials provided with the
+ * distribution.
+ * THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+ * THE POSSIBILITY OF SUCH DAMAGE.
+ * *****************************#NETL_2#********************************/
+
+#ifndef __USER_RMI_PHNX_LOADER_H
+#define __USER_RMI_PHNX_LOADER_H
+
+#include <asm/ioctl.h>
+#include <asm/types.h>
+
+#define PHNX_LOADER_INFO_MAGIC 			0x600ddeed
+#define XLR_MAP_SLAVE_DEVICE 			0x1
+#define XLR_MAP_UNCACHED 			0x1
+#define XLR_MAP_CACHED 				0x2
+#define PHNX_APP_LOADER_CHRDEV_NAME 		"xlr_app_loader"
+#define MAX_NUM_LOADER_MEM_BLK 			4
+#define SHARED_MEM_ADDR            		(3 << 20)
+#define XLP_APP_LOADER_MAJOR       		(245)
+
+#define PSB_MEM_MAP_MAX 			32
+#define PSB_IO_MAP_MAX 				32
+#define MAX_FRAGMENTS 				32
+
+#define MAX_TLB_MAPPINGS 			16
+#define MAX_ARGS 				16
+#define MAX_ARGV_LEN 				16
+
+#define PHNX_LOADER_IOC_MAGIC 			'X'
+#define PHNX_LOADER_IOC_SHMEM_SIZE 		_IOR(PHNX_LOADER_IOC_MAGIC, 0, unsigned int)
+#define PHNX_LOADER_IOC_MMAP_SHMEM 		_IOR(PHNX_LOADER_IOC_MAGIC, 1, unsigned int)
+#define PHNX_LOADER_IOC_LIB_BKP 		_IOR(PHNX_LOADER_IOC_MAGIC, 2, unsigned int)
+#define PHNX_LOADER_IOC_MMAP_LOAD_ADDR  	_IOR(PHNX_LOADER_IOC_MAGIC, 3, unsigned int)
+#define PHNX_LOADER_IOC_START_IPI 		_IOR(PHNX_LOADER_IOC_MAGIC, 4, unsigned int)
+#define PHNX_LOADER_IOC_STOP_IPI  		_IOR(PHNX_LOADER_IOC_MAGIC, 5, unsigned int)
+#define PHNX_LOADER_IOC_ALLOC_PERSISTENT_MEM  	_IOR(PHNX_LOADER_IOC_MAGIC, 6, unsigned int)
+#define PHNX_LOADER_IOC_MMAP_PERSISTENT_MEM  	_IOR(PHNX_LOADER_IOC_MAGIC, 7, unsigned int)
+#define PHNX_LOADER_IOC_FREE_PERSISTENT_MEM  	_IOR(PHNX_LOADER_IOC_MAGIC, 8, unsigned int)
+#define PHNX_LOADER_IOC_SHMEM_KSEG_ADDR 	_IOR(PHNX_LOADER_IOC_MAGIC, 10, unsigned int)
+#define PHNX_LOADER_IOC_LAUNCH_KSEG 		_IOR(PHNX_LOADER_IOC_MAGIC, 15, unsigned int)
+#define PHNX_LOADER_IOC_APP_SHMEM_SIZE 		_IOR(PHNX_LOADER_IOC_MAGIC, 25, unsigned int)
+#define PHNX_LOADER_IOC_APP_SHMEM_RESERVE 	_IOR(PHNX_LOADER_IOC_MAGIC, 35, unsigned int)
+#define PHNX_LOADER_IOC_MMAP_APP_SHMEM 		_IOR(PHNX_LOADER_IOC_MAGIC, 45, unsigned int)
+#define PHNX_LOADER_IOC_APP_SHMEM_PHYS 		_IOR(PHNX_LOADER_IOC_MAGIC, 55, unsigned int)
+#define PHNX_LOADER_STORE_ENV 			_IOR(PHNX_LOADER_IOC_MAGIC, 65, unsigned int)
+#define PHNX_LOADER_SEND_IPI 			_IOR(PHNX_LOADER_IOC_MAGIC, 75, unsigned int)
+#define PHNX_LOADER_IOC_STORE_APP_SHMEM_INFO 	_IOR(PHNX_LOADER_IOC_MAGIC, 85, unsigned int)
+#define PHNX_LOADER_IOC_GET_APP_SHMEM_INFO 	_IOR(PHNX_LOADER_IOC_MAGIC, 95, unsigned int)
+#define PHNX_LOADER_IOC_FDT_CPUMASK 		_IOWR(PHNX_LOADER_IOC_MAGIC, 105, unsigned int)
+
+enum { KUSEG_MODE, KSEG0_MODE };
+typedef enum {
+	STOP_THREAD=0xbeef,
+	START_THREAD,
+	RUN_FUNCTION, /* Used by wakeup and wakeup_os call */
+
+}loader_cmd;
+
+typedef enum {
+	THREAD_STOPPED=0x600d,
+	THREAD_RUNNING,
+	THREAD_SCHEDULED,
+}thread_status;
+
+struct cpu_tlb_mapping {
+	int page_size;
+	int asid;
+	int coherency;
+	int attr;
+	unsigned long virt;
+	uint64_t phys;
+};
+
+struct cpu_wakeup_info {
+	int            master_cpu;
+	int            map_count;
+	int            valid;
+	unsigned long  func;
+	unsigned long  args;
+	int            argc;
+	uint32_t       buddy_mask;
+	uint32_t       cpu_mask;
+	char          *argv[32]; /* RMIOS LIB NEEDS this to be 32 */
+	char           buf[256];/* must be > MAX_ARGS * MAX_ARGV_LEN + some buffer */
+	struct cpu_tlb_mapping map[MAX_TLB_MAPPINGS];
+};
+
+/* SHARED memory structure b/w loader app, linux and RMIOS apps */
+typedef struct phnx_loader_shared_struct {
+	unsigned long park_entry;
+	loader_cmd    cmd;
+	thread_status thr_status;
+	unsigned long entry; /* Entry point address */
+	int 	      run_mode;
+	struct cpu_wakeup_info run_info;
+	uint32_t 	app_sh_mem_sz; /* Size of the shared memory */
+	unsigned long	sp;/* Used for reentry */
+	unsigned long	gp;
+}phnx_loader_shared_struct_t;
+
+
+/* This structure is passed to all applications launched from the linux
+   loader through OS 7 scratch register
+   */
+typedef struct phnx_loader_info {
+	uint32_t magic;
+	/* phnx_loader_shared_struct_t for CPU 0 will start here */
+	unsigned long sh_mem_start;
+	/* Size of the shared memory b/w linux apps and rmios apps  */
+	uint32_t app_sh_mem_size;
+	uint8_t printk_lock[16]; /* used for printk */
+}phnx_loader_info_t;
+
+struct psb_mem_map {
+	int nr_map;
+	struct psb_mem_map_entry {
+		uint64_t addr;  /* start of memory segment */
+		uint64_t size;  /* size of memory segment */
+		uint32_t type;      /* type of memory segment */
+	} map[PSB_MEM_MAP_MAX];
+};
+
+struct psb_io_map {
+	int nr_map;
+	struct psb_io_map_entry {
+		uint64_t addr;  /* start of IO segment */
+		uint64_t size;  /* size of IO segment */
+		long type;      /* type of IO segment */
+	} map[PSB_IO_MAP_MAX];
+};
+
+struct r_exception_region {
+	    unsigned int data[1024];
+};
+
+struct xlr_rmios_pt_regs {
+	unsigned long long pad0[6];
+
+	unsigned long long regs[32];
+
+	unsigned long long cp0_status;
+	unsigned long long hi;
+	unsigned long long lo;
+
+	/*
+	 * saved cp0 registers
+	 */
+	unsigned long long cp0_badvaddr;
+	unsigned long long cp0_cause;
+	unsigned long long cp0_epc;
+};
+
+struct domain_info
+{
+	uint32_t domain;
+	uint32_t cpumask;	////cpu mask -- currently 32bit.
+	uint32_t mastercpu;	///master cpu id
+	uint32_t mode; //// 0-smp, 1-amp
+	uint64_t app_addr;	//Hyperapp app load address
+	uint64_t fdt_blob;	//FDT blob address.
+};
+
+struct wakeup_info
+{
+       int vcpu;
+       unsigned long long func;
+       unsigned long long data;
+};
+
+struct xlr_load_addr
+{
+       uint64_t phys;
+       uint64_t size;
+       uint32_t flag;
+};
+
+struct xlr_lib_shared_mem
+{
+	uint64_t entries;
+	uint64_t tot_size;
+	uint64_t addr[MAX_FRAGMENTS];
+	uint64_t size[MAX_FRAGMENTS];
+};
+
+struct loader_mem_info{
+	uint64_t size;
+	uint64_t start_addr;
+};
+
+#endif
diff --git a/arch/mips/include/asm/page.h b/arch/mips/include/asm/page.h
index a39bcdd..b4d1c50 100644
--- a/arch/mips/include/asm/page.h
+++ b/arch/mips/include/asm/page.h
@@ -84,11 +84,19 @@ struct page;
 static inline void clear_user_page(void *addr, unsigned long vaddr,
 	struct page *page)
 {
+#ifdef CONFIG_NLM_COMMON
+	extern void nlm_common_flush_dcache_page(struct page *page);
+#else
 	extern void (*flush_data_cache_page)(unsigned long addr);
+#endif
 
 	clear_page(addr);
+#ifdef CONFIG_NLM_COMMON
+	nlm_common_flush_dcache_page(page);
+#else
 	if (pages_do_alias((unsigned long) addr, vaddr & PAGE_MASK))
 		flush_data_cache_page((unsigned long)addr);
+#endif
 }
 
 #ifdef CONFIG_NLM_COMMON
@@ -132,6 +140,18 @@ typedef struct { unsigned long pte; } pte_t;
 typedef struct page *pgtable_t;
 
 /*
+ * For 3-level pagetables we defines these ourselves, for 2-level the
+ * definitions are supplied by <asm-generic/pgtable-nopmd.h>.
+ */
+#ifdef CONFIG_64BIT
+
+typedef struct { unsigned long pmd; } pmd_t;
+#define pmd_val(x)	((x).pmd)
+#define __pmd(x)	((pmd_t) { (x) } )
+
+#endif
+
+/*
  * Right now we don't support 4-level pagetables, so all pud-related
  * definitions come from <asm-generic/pgtable-nopud.h>.
  */
@@ -175,7 +195,14 @@ typedef struct { unsigned long pgprot; } pgprot_t;
     ((unsigned long)(x) - PAGE_OFFSET + PHYS_OFFSET)
 #endif
 #define __va(x)		((void *)((unsigned long)(x) + PAGE_OFFSET - PHYS_OFFSET))
-#define __pa_symbol(x)	__pa(RELOC_HIDE((unsigned long)(x), 0))
+
+#ifndef __ASSEMBLY__
+#ifdef CONFIG_MAPPED_KERNEL
+#define __pa_symbol(x) (RELOC_HIDE((unsigned long)(x), 0) - (unsigned long)LOADADDR + ((unsigned long)PHYSADDR & 0x7fffffffUL))
+#else
+#define __pa_symbol(x) __pa(RELOC_HIDE((unsigned long)(x), 0))
+#endif
+#endif
 
 #define pfn_to_kaddr(pfn)	__va((pfn) << PAGE_SHIFT)
 
@@ -210,13 +237,12 @@ typedef struct { unsigned long pgprot; } pgprot_t;
 #define virt_to_page(kaddr)	pfn_to_page(PFN_DOWN(virt_to_phys(kaddr)))
 #define virt_addr_valid(kaddr)	pfn_valid(PFN_DOWN(virt_to_phys(kaddr)))
 
-#define VM_DATA_DEFAULT_FLAGS	(VM_READ | VM_WRITE | VM_EXEC | \
-				 VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)
+#define VM_DATA_DEFAULT_FLAGS (VM_READ | VM_WRITE | \
+    ((current->personality & READ_IMPLIES_EXEC) ? VM_EXEC : 0 ) | \
+         VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)
 
-#define UNCAC_ADDR(addr)	((addr) - PAGE_OFFSET + UNCAC_BASE + 	\
-								PHYS_OFFSET)
-#define CAC_ADDR(addr)		((addr) - UNCAC_BASE + PAGE_OFFSET -	\
-								PHYS_OFFSET)
+#define UNCAC_ADDR(addr)	((addr) - PAGE_OFFSET + UNCAC_BASE)
+#define CAC_ADDR(addr)		((addr) - UNCAC_BASE + PAGE_OFFSET)
 
 #include <asm-generic/memory_model.h>
 #include <asm-generic/getorder.h>
diff --git a/arch/mips/include/asm/pgalloc.h b/arch/mips/include/asm/pgalloc.h
index 881d18b..3738f4b 100644
--- a/arch/mips/include/asm/pgalloc.h
+++ b/arch/mips/include/asm/pgalloc.h
@@ -31,7 +31,7 @@ static inline void pmd_populate(struct mm_struct *mm, pmd_t *pmd,
  */
 extern void pmd_init(unsigned long page, unsigned long pagetable);
 
-#ifndef __PAGETABLE_PMD_FOLDED
+#ifdef CONFIG_64BIT
 
 static inline void pud_populate(struct mm_struct *mm, pud_t *pud, pmd_t *pmd)
 {
@@ -104,7 +104,7 @@ do {							\
 	tlb_remove_page((tlb), pte);			\
 } while (0)
 
-#ifndef __PAGETABLE_PMD_FOLDED
+#ifdef CONFIG_64BIT
 
 static inline pmd_t *pmd_alloc_one(struct mm_struct *mm, unsigned long address)
 {
diff --git a/arch/mips/include/asm/pgtable-32.h b/arch/mips/include/asm/pgtable-32.h
index ae90412..208b158 100644
--- a/arch/mips/include/asm/pgtable-32.h
+++ b/arch/mips/include/asm/pgtable-32.h
@@ -63,7 +63,12 @@ extern int add_temporary_entry(unsigned long entrylo0, unsigned long entrylo1,
 #define USER_PTRS_PER_PGD	(0x80000000UL/PGDIR_SIZE)
 #define FIRST_USER_ADDRESS	0
 
+#ifdef CONFIG_MAPPED_KERNEL
+extern unsigned long __vmalloc_start;
+#define VMALLOC_START     __vmalloc_start
+#else
 #define VMALLOC_START     MAP_BASE
+#endif
 
 #define PKMAP_BASE		(0xfe000000UL)
 
@@ -127,8 +132,12 @@ pfn_pte(unsigned long pfn, pgprot_t prot)
 #define pte_pfn(x)		((unsigned long)((x).pte >> (PAGE_SHIFT + 2)))
 #define pfn_pte(pfn, prot)	__pte(((pfn) << (PAGE_SHIFT + 2)) | pgprot_val(prot))
 #else
-#define pte_pfn(x)		((unsigned long)((x).pte >> _PFN_SHIFT))
-#define pfn_pte(pfn, prot)	__pte(((unsigned long long)(pfn) << _PFN_SHIFT) | pgprot_val(prot))
+#ifdef CONFIG_NLM_XLP
+#define pte_pfn(x)     ((unsigned long)(((x).pte & ~((1ULL<<_PAGE_NO_READ_SHIFT)|(1ULL<<_PAGE_NO_EXEC_SHIFT))) >> _PFN_SHIFT))
+#else
+#define pte_pfn(x)     ((unsigned long)((x).pte >> _PFN_SHIFT))
+#endif
+#define pfn_pte(pfn, prot) __pte(((unsigned long long)(pfn) << _PFN_SHIFT) | pgprot_val(prot))
 #endif
 #endif /* defined(CONFIG_64BIT_PHYS_ADDR) && defined(CONFIG_CPU_MIPS32) */
 
diff --git a/arch/mips/include/asm/pgtable-64.h b/arch/mips/include/asm/pgtable-64.h
index 1be4b0f..60d35c6 100644
--- a/arch/mips/include/asm/pgtable-64.h
+++ b/arch/mips/include/asm/pgtable-64.h
@@ -16,11 +16,7 @@
 #include <asm/cachectl.h>
 #include <asm/fixmap.h>
 
-#ifdef CONFIG_PAGE_SIZE_64KB
-#include <asm-generic/pgtable-nopmd.h>
-#else
 #include <asm-generic/pgtable-nopud.h>
-#endif
 
 /*
  * Each address space has 2 4K pages as its page directory, giving 1024
@@ -41,20 +37,13 @@
  * fault address - VMALLOC_START.
  */
 
-
-/* PGDIR_SHIFT determines what a third-level page table entry can map */
-#ifdef __PAGETABLE_PMD_FOLDED
-#define PGDIR_SHIFT	(PAGE_SHIFT + PAGE_SHIFT + PTE_ORDER - 3)
-#else
-
 /* PMD_SHIFT determines the size of the area a second-level page table can map */
 #define PMD_SHIFT	(PAGE_SHIFT + (PAGE_SHIFT + PTE_ORDER - 3))
 #define PMD_SIZE	(1UL << PMD_SHIFT)
 #define PMD_MASK	(~(PMD_SIZE-1))
 
-
+/* PGDIR_SHIFT determines what a third-level page table entry can map */
 #define PGDIR_SHIFT	(PMD_SHIFT + (PAGE_SHIFT + PMD_ORDER - 3))
-#endif
 #define PGDIR_SIZE	(1UL << PGDIR_SHIFT)
 #define PGDIR_MASK	(~(PGDIR_SIZE-1))
 
@@ -103,14 +92,12 @@
 #ifdef CONFIG_PAGE_SIZE_64KB
 #define PGD_ORDER		0
 #define PUD_ORDER		aieeee_attempt_to_allocate_pud
-#define PMD_ORDER		aieeee_attempt_to_allocate_pmd
+#define PMD_ORDER		0
 #define PTE_ORDER		0
 #endif
 
 #define PTRS_PER_PGD	((PAGE_SIZE << PGD_ORDER) / sizeof(pgd_t))
-#ifndef __PAGETABLE_PMD_FOLDED
 #define PTRS_PER_PMD	((PAGE_SIZE << PMD_ORDER) / sizeof(pmd_t))
-#endif
 #define PTRS_PER_PTE	((PAGE_SIZE << PTE_ORDER) / sizeof(pte_t))
 
 #if PGDIR_SIZE >= TASK_SIZE
@@ -120,50 +107,43 @@
 #endif
 #define FIRST_USER_ADDRESS	0UL
 
-/*
- * TLB refill handlers also map the vmalloc area into xuseg.  Avoid
- * the first couple of pages so NULL pointer dereferences will still
- * reliably trap.
- */
-#define VMALLOC_START		(MAP_BASE + (2 * PAGE_SIZE))
+#if defined(CONFIG_MAPPED_KERNEL) && defined(CONFIG_KSEG2_LOWMEM)
+extern unsigned long __vmalloc_start;
+#define VMALLOC_START		__vmalloc_start
+#else
+#define VMALLOC_START		MAP_BASE
+#endif
+
 #define VMALLOC_END	\
-	(MAP_BASE + \
+	(VMALLOC_START + \
 	 min(PTRS_PER_PGD * PTRS_PER_PMD * PTRS_PER_PTE * PAGE_SIZE, \
 	     (1UL << cpu_vmbits)) - (1UL << 32))
 
+#ifndef CONFIG_MAPPED_KERNEL
+
 #if defined(CONFIG_MODULES) && defined(KBUILD_64BIT_SYM32) && \
 	VMALLOC_START != CKSSEG
 /* Load modules into 32bit-compatible segment. */
 #define MODULE_START	CKSSEG
 #define MODULE_END	(FIXADDR_START-2*PAGE_SIZE)
 #endif
+#else
+#define MODULE_START    0xffffffffe0000000
+#define MODULE_END      (FIXADDR_START-2*PAGE_SIZE)
+
+#endif /* CONFIG_MAPPED_KERNEL */
 
 #define pte_ERROR(e) \
 	printk("%s:%d: bad pte %016lx.\n", __FILE__, __LINE__, pte_val(e))
-#ifndef __PAGETABLE_PMD_FOLDED
 #define pmd_ERROR(e) \
 	printk("%s:%d: bad pmd %016lx.\n", __FILE__, __LINE__, pmd_val(e))
-#endif
 #define pgd_ERROR(e) \
 	printk("%s:%d: bad pgd %016lx.\n", __FILE__, __LINE__, pgd_val(e))
 
 extern pte_t invalid_pte_table[PTRS_PER_PTE];
 extern pte_t empty_bad_page_table[PTRS_PER_PTE];
-
-
-#ifndef __PAGETABLE_PMD_FOLDED
-/*
- * For 3-level pagetables we defines these ourselves, for 2-level the
- * definitions are supplied by <asm-generic/pgtable-nopmd.h>.
- */
-typedef struct { unsigned long pmd; } pmd_t;
-#define pmd_val(x)	((x).pmd)
-#define __pmd(x)	((pmd_t) { (x) } )
-
-
 extern pmd_t invalid_pmd_table[PTRS_PER_PMD];
 extern pmd_t empty_bad_pmd_table[PTRS_PER_PMD];
-#endif
 
 /*
  * Empty pgd/pmd entries point to the invalid_pte_table.
@@ -184,7 +164,6 @@ static inline void pmd_clear(pmd_t *pmdp)
 {
 	pmd_val(*pmdp) = ((unsigned long) invalid_pte_table);
 }
-#ifndef __PAGETABLE_PMD_FOLDED
 
 /*
  * Empty pud entries point to the invalid_pmd_table.
@@ -208,7 +187,6 @@ static inline void pud_clear(pud_t *pudp)
 {
 	pud_val(*pudp) = ((unsigned long) invalid_pmd_table);
 }
-#endif
 
 #define pte_page(x)		pfn_to_page(pte_pfn(x))
 
@@ -216,8 +194,12 @@ static inline void pud_clear(pud_t *pudp)
 #define pte_pfn(x)		((unsigned long)((x).pte >> (PAGE_SHIFT + 2)))
 #define pfn_pte(pfn, prot)	__pte(((pfn) << (PAGE_SHIFT + 2)) | pgprot_val(prot))
 #else
-#define pte_pfn(x)		((unsigned long)((x).pte >> _PFN_SHIFT))
-#define pfn_pte(pfn, prot)	__pte(((pfn) << _PFN_SHIFT) | pgprot_val(prot))
+#ifdef CONFIG_NLM_XLP
+#define pte_pfn(x)     ((unsigned long)(((x).pte & ~((1ULL<<_PAGE_NO_READ_SHIFT)|(1ULL<<_PAGE_NO_EXEC_SHIFT))) >> _PFN_SHIFT))
+#else
+#define pte_pfn(x)     ((unsigned long)((x).pte >> _PFN_SHIFT))
+#endif
+#define pfn_pte(pfn, prot) __pte(((pfn) << _PFN_SHIFT) | pgprot_val(prot))
 #endif
 
 #define __pgd_offset(address)	pgd_index(address)
@@ -233,7 +215,6 @@ static inline void pud_clear(pud_t *pudp)
 /* to find an entry in a page-table-directory */
 #define pgd_offset(mm, addr)	((mm)->pgd + pgd_index(addr))
 
-#ifndef __PAGETABLE_PMD_FOLDED
 static inline unsigned long pud_page_vaddr(pud_t pud)
 {
 	return pud_val(pud);
@@ -246,7 +227,6 @@ static inline pmd_t *pmd_offset(pud_t * pud, unsigned long address)
 {
 	return (pmd_t *) pud_page_vaddr(*pud) + pmd_index(address);
 }
-#endif
 
 /* Find an entry in the third-level page table.. */
 #define __pte_offset(address)						\
diff --git a/arch/mips/include/asm/pgtable.h b/arch/mips/include/asm/pgtable.h
index 7e40f37..34a1ed7 100644
--- a/arch/mips/include/asm/pgtable.h
+++ b/arch/mips/include/asm/pgtable.h
@@ -137,6 +137,8 @@ static inline void pte_clear(struct mm_struct *mm, unsigned long addr, pte_t *pt
 
 #define pte_none(pte)		(!(pte_val(pte) & ~_PAGE_GLOBAL))
 #define pte_present(pte)	(pte_val(pte) & _PAGE_PRESENT)
+/* Just for compilation! */
+#define pte_user(pte)           (pte_val(pte) & _PAGE_PRESENT)
 
 /*
  * Certain architectures need to do special things when pte's
@@ -178,7 +180,7 @@ static inline void pte_clear(struct mm_struct *mm, unsigned long addr, pte_t *pt
  */
 #define set_pmd(pmdptr, pmdval) do { *(pmdptr) = (pmdval); } while(0)
 
-#ifndef __PAGETABLE_PMD_FOLDED
+#ifdef CONFIG_64BIT
 /*
  * (puds are folded into pgds so this doesn't get actually called,
  * but the define is needed for a generic inline function.)
@@ -362,6 +364,7 @@ static inline pte_t pte_modify(pte_t pte, pgprot_t newprot)
 #endif
 
 
+struct vm_area_struct;
 extern void __update_tlb(struct vm_area_struct *vma, unsigned long address,
 	pte_t pte);
 extern void __update_cache(struct vm_area_struct *vma, unsigned long address,
diff --git a/arch/mips/kernel/head.S b/arch/mips/kernel/head.S
index ea695d9..b22a058 100644
--- a/arch/mips/kernel/head.S
+++ b/arch/mips/kernel/head.S
@@ -26,7 +26,10 @@
 #include <asm/mipsregs.h>
 #include <asm/stackframe.h>
 
-#include <kernel-entry-init.h>
+#include <asm/mach-generic/kernel-entry-init.h>
+#if defined(CONFIG_NLM_XLP) && defined(CONFIG_MAPPED_KERNEL)
+#include <asm/mach-netlogic/kernel-entry-init.h>
+#else
 
 	/*
 	 * inputs are the text nasid in t1, data nasid in t2.
@@ -66,6 +69,7 @@
 	mtc0	zero, CP0_WIRED
 #endif
 	.endm
+#endif
 
 	/*
 	 * For the moment disable interrupts, mark the kernel mode and
@@ -145,6 +149,9 @@ FEXPORT(__kernel_entry)
 
 NESTED(kernel_entry, 16, sp)			# kernel entry point
 
+#if defined(CONFIG_NLM_XLP) && defined(CONFIG_MAPPED_KERNEL)
+	MAPPED_KERNEL_SETUP_TLB
+#endif
 	kernel_entry_setup			# cpu specific setup
 
 	setup_c0_status_pri
diff --git a/arch/mips/kernel/setup.c b/arch/mips/kernel/setup.c
index 4ab5377..5d4b49e 100644
--- a/arch/mips/kernel/setup.c
+++ b/arch/mips/kernel/setup.c
@@ -32,6 +32,8 @@
 #include <asm/smp-ops.h>
 #include <asm/system.h>
 
+#include <asm/mach-netlogic/mmu.h>
+
 struct cpuinfo_mips cpu_data[NR_CPUS] __read_mostly;
 
 EXPORT_SYMBOL(cpu_data);
@@ -341,6 +343,9 @@ static void __init bootmem_init(void)
 		max_low_pfn = PFN_DOWN(HIGHMEM_START);
 	}
 
+#if defined(CONFIG_NLM_XLP) && defined(CONFIG_MAPPED_KERNEL)
+	max_low_pfn = recalculate_max_low_pfn(max_low_pfn);
+#endif
 	/*
 	 * Initialize the boot-time allocator with low memory only.
 	 */
@@ -519,6 +524,9 @@ static void __init arch_mem_init(char **cmdline_p)
 	}
 
 	bootmem_init();
+#if defined(CONFIG_NLM_XLP) && defined(CONFIG_MAPPED_KERNEL)
+	setup_mapped_kernel_tlbs(FALSE, TRUE);
+#endif
 #ifdef CONFIG_KEXEC
 	pr_info("Kexeckernel info: start = %llu end = %llu\n",
 		kexeck_res.start, kexeck_res.end);
diff --git a/arch/mips/kernel/vmlinux.lds.S b/arch/mips/kernel/vmlinux.lds.S
index f25df73..2d969c6 100644
--- a/arch/mips/kernel/vmlinux.lds.S
+++ b/arch/mips/kernel/vmlinux.lds.S
@@ -5,12 +5,27 @@
 #undef mips
 #define mips mips
 OUTPUT_ARCH(mips)
-ENTRY(kernel_entry)
+#ifdef CONFIG_MAPPED_KERNEL
+#define AT_LOCATION
+#endif
 PHDRS {
-	text PT_LOAD FLAGS(7);	/* RWX */
+#ifdef CONFIG_MAPPED_KERNEL
+	text PT_LOAD AT_LOCATION FLAGS(7);	/* RWX */
+#else
+	text PT_LOAD FLAGS(7);  /* RWX */
+#endif
 	note PT_NOTE FLAGS(4);	/* R__ */
 }
 
+#ifdef CONFIG_MAPPED_KERNEL
+#ifdef PHYSADDR
+phys_entry = kernel_entry - LOADADDR_ASM + PHYSADDR_ASM;
+#endif
+ENTRY(phys_entry)
+#else
+ENTRY(kernel_entry)
+#endif
+
 #ifdef CONFIG_32BIT
 	#ifdef CONFIG_CPU_LITTLE_ENDIAN
 		jiffies  = jiffies_64;
diff --git a/arch/mips/mm/tlb-r4k.c b/arch/mips/mm/tlb-r4k.c
index ca658e3..7373d3e 100644
--- a/arch/mips/mm/tlb-r4k.c
+++ b/arch/mips/mm/tlb-r4k.c
@@ -21,7 +21,7 @@
 #include <asm/pgtable.h>
 #include <asm/system.h>
 
-#ifdef CONFIG_NLM_TLB_STAT
+#ifdef CONFIG_NLM_COMMON
 #include <asm/netlogic/mips-exts.h>
 #include <asm/mach-netlogic/mmu.h>
 #endif
@@ -446,6 +446,39 @@ out:
 	return ret;
 }
 
+static void __cpuinit probe_tlb(unsigned long config)
+{
+	struct cpuinfo_mips *c = &current_cpu_data;
+	unsigned int reg;
+
+	/*
+	 * If this isn't a MIPS32 / MIPS64 compliant CPU.  Config 1 register
+	 * is not supported, we assume R4k style.  Cpu probing already figured
+	 * out the number of tlb entries.
+	 */
+	if ((c->processor_id & 0xff0000) == PRID_COMP_LEGACY)
+		return;
+#ifdef CONFIG_MIPS_MT_SMTC
+	/*
+	 * If TLB is shared in SMTC system, total size already
+	 * has been calculated and written into cpu_data tlbsize
+	 */
+	if((smtc_status & SMTC_TLB_SHARED) == SMTC_TLB_SHARED)
+		return;
+#endif /* CONFIG_MIPS_MT_SMTC */
+
+	reg = read_c0_config1();
+	if (!((config >> 7) & 3))
+		panic("No TLB present");
+
+#if defined(CONFIG_NLM_XLP)
+	c->tlbsize = ((read_c0_config6() >> 16 ) & 0xffff) + 1;
+#else
+	c->tlbsize = ((reg >> 25) & 0x3f) + 1;
+#endif
+
+}
+
 static int __cpuinitdata ntlb;
 static int __init set_ntlb(char *str)
 {
@@ -464,6 +497,8 @@ static inline void nlm_tlb_stats_init(void)
 
 void __cpuinit tlb_init(void)
 {
+	unsigned int config = read_c0_config();
+
 	/*
 	 * You should never change this register:
 	 *   - On R4600 1.7 the tlbp never hits for pages smaller than
@@ -471,8 +506,11 @@ void __cpuinit tlb_init(void)
 	 *   - The entire mm handling assumes the c0_pagemask register to
 	 *     be set to fixed-size pages.
 	 */
+	probe_tlb(config);
 	write_c0_pagemask(PM_DEFAULT_MASK);
+#if !defined(CONFIG_MAPPED_KERNEL)
 	write_c0_wired(0);
+#endif
 	if (current_cpu_type() == CPU_R10000 ||
 	    current_cpu_type() == CPU_R12000 ||
 	    current_cpu_type() == CPU_R14000)
diff --git a/arch/mips/mm/tlbex.c b/arch/mips/mm/tlbex.c
index c117644..959fe63 100644
--- a/arch/mips/mm/tlbex.c
+++ b/arch/mips/mm/tlbex.c
@@ -93,10 +93,16 @@ enum label_id {
 	label_nopage_tlbm,
 	label_smp_pgtable_change,
 	label_r3000_write_probe_fail,
-	label_large_segbits_fault,
 #ifdef CONFIG_HUGETLB_PAGE
 	label_tlb_huge_update,
 #endif
+	label_nohpage_tlbs,
+	label_hpage_tlb_leave,
+	label_non_btlb_process,
+	label_read_entrylo1,
+	label_illegal_access_tlbl,
+	label_exl_refill_exception,
+	label_r4000_write_probe_fail,
 };
 
 UASM_L_LA(_second_part)
@@ -112,10 +118,16 @@ UASM_L_LA(_nopage_tlbs)
 UASM_L_LA(_nopage_tlbm)
 UASM_L_LA(_smp_pgtable_change)
 UASM_L_LA(_r3000_write_probe_fail)
-UASM_L_LA(_large_segbits_fault)
 #ifdef CONFIG_HUGETLB_PAGE
 UASM_L_LA(_tlb_huge_update)
 #endif
+UASM_L_LA(_nohpage_tlbs)
+UASM_L_LA(_hpage_tlb_leave)
+UASM_L_LA(_non_btlb_process)
+UASM_L_LA(_read_entrylo1)
+UASM_L_LA(_illegal_access_tlbl)
+UASM_L_LA(_exl_refill_exception)
+UASM_L_LA(_r4000_write_probe_fail)
 
 /*
  * For debug purposes.
@@ -134,6 +146,7 @@ static inline void dump_handler(const u32 *handler, int count)
 }
 
 /* The only general purpose registers allowed in TLB handlers. */
+#define ZERO 0
 #define K0		26
 #define K1		27
 
@@ -163,16 +176,16 @@ static inline void dump_handler(const u32 *handler, int count)
  * We deliberately chose a buffer size of 128, so we won't scribble
  * over anything important on overflow before we panic.
  */
-static u32 tlb_handler[128] __cpuinitdata;
+#if defined(CONFIG_MAPPED_KERNEL)
+static u32 tlb_handler[128];
+#else
+static u32 tlb_handler[128] __cpuinitdata ;
+#endif
 
 /* simply assume worst case size for labels and relocs */
 static struct uasm_label labels[128] __cpuinitdata;
 static struct uasm_reloc relocs[128] __cpuinitdata;
 
-#ifdef CONFIG_64BIT
-static int check_for_high_segbits __cpuinitdata;
-#endif
-
 #ifndef CONFIG_MIPS_PGD_C0_CONTEXT
 /*
  * CONFIG_MIPS_PGD_C0_CONTEXT implies 64 bit and lack of pgd_current,
@@ -227,7 +240,10 @@ static void __cpuinit build_r3000_tlb_refill_handler(void)
  * other one.To keep things simple, we first assume linear space,
  * then we relocate it to the final handler layout as needed.
  */
+
+#if !defined(CONFIG_MAPPED_KERNEL)
 static u32 final_handler[64] __cpuinitdata;
+#endif
 
 /*
  * Hazards
@@ -260,7 +276,6 @@ static void __cpuinit __maybe_unused build_tlb_probe_entry(u32 **p)
 	case CPU_R5000:
 	case CPU_R5000A:
 	case CPU_NEVADA:
-	case CPU_4KC:
 		uasm_i_nop(p);
 		uasm_i_tlbp(p);
 		break;
@@ -289,10 +304,6 @@ static void __cpuinit build_tlb_write_entry(u32 **p, struct uasm_label **l,
 	}
 
 	if (cpu_has_mips_r2) {
-#ifdef CONFIG_CPU_XLP
-		uasm_i_nop(p);
-		uasm_i_nop(p);
-#endif
 		if (cpu_has_mips_r2_exec_hazard)
 			uasm_i_ehb(p);
 		tlbw(p);
@@ -320,7 +331,6 @@ static void __cpuinit build_tlb_write_entry(u32 **p, struct uasm_label **l,
 	case CPU_R4700:
 	case CPU_R5000:
 	case CPU_R5000A:
-	case CPU_4KC:
 		uasm_i_nop(p);
 		tlbw(p);
 		uasm_i_nop(p);
@@ -330,6 +340,7 @@ static void __cpuinit build_tlb_write_entry(u32 **p, struct uasm_label **l,
 	case CPU_5KC:
 	case CPU_TX49XX:
 	case CPU_PR4450:
+	case CPU_XLP:
 		uasm_i_nop(p);
 		tlbw(p);
 		break;
@@ -337,6 +348,7 @@ static void __cpuinit build_tlb_write_entry(u32 **p, struct uasm_label **l,
 	case CPU_R10000:
 	case CPU_R12000:
 	case CPU_R14000:
+	case CPU_4KC:
 	case CPU_4KEC:
 	case CPU_SB1:
 	case CPU_SB1A:
@@ -425,8 +437,16 @@ static __cpuinit __maybe_unused void build_convert_pte_to_entrylo(u32 **p,
 								  unsigned int reg)
 {
 	if (kernel_uses_smartmips_rixi) {
+#ifdef CONFIG_NLM_XLP
+#ifdef CONFIG_64BIT_PHYS_ADDR
+		uasm_i_dsrl(p, reg, reg, ilog2(_PAGE_GLOBAL));
+#else
+		UASM_i_SRL(p, reg, reg, ilog2(_PAGE_GLOBAL));
+#endif
+#else
 		UASM_i_SRL(p, reg, reg, ilog2(_PAGE_NO_EXEC));
 		UASM_i_ROTR(p, reg, reg, ilog2(_PAGE_GLOBAL) - ilog2(_PAGE_NO_EXEC));
+#endif
 	} else {
 #ifdef CONFIG_64BIT_PHYS_ADDR
 		uasm_i_dsrl_safe(p, reg, reg, ilog2(_PAGE_GLOBAL));
@@ -553,24 +573,7 @@ build_get_pmde64(u32 **p, struct uasm_label **l, struct uasm_reloc **r,
 	 * The vmalloc handling is not in the hotpath.
 	 */
 	uasm_i_dmfc0(p, tmp, C0_BADVADDR);
-
-	if (check_for_high_segbits) {
-		/*
-		 * The kernel currently implicitely assumes that the
-		 * MIPS SEGBITS parameter for the processor is
-		 * (PGDIR_SHIFT+PGDIR_BITS) or less, and will never
-		 * allocate virtual addresses outside the maximum
-		 * range for SEGBITS = (PGDIR_SHIFT+PGDIR_BITS). But
-		 * that doesn't prevent user code from accessing the
-		 * higher xuseg addresses.  Here, we make sure that
-		 * everything but the lower xuseg addresses goes down
-		 * the module_alloc/vmalloc path.
-		 */
-		uasm_i_dsrl_safe(p, ptr, tmp, PGDIR_SHIFT + PGD_ORDER + PAGE_SHIFT - 3);
-		uasm_il_bnez(p, r, ptr, label_vmalloc);
-	} else {
-		uasm_il_bltz(p, r, tmp, label_vmalloc);
-	}
+	uasm_il_bltz(p, r, tmp, label_vmalloc);
 	/* No uasm_i_nop needed here, since the next insn doesn't touch TMP. */
 
 #ifdef CONFIG_MIPS_PGD_C0_CONTEXT
@@ -607,8 +610,10 @@ build_get_pmde64(u32 **p, struct uasm_label **l, struct uasm_reloc **r,
 
 	uasm_l_vmalloc_done(l, *p);
 
-	/* get pgd offset in bytes */
-	uasm_i_dsrl_safe(p, tmp, tmp, PGDIR_SHIFT - 3);
+	if (PGDIR_SHIFT - 3 < 32)		/* get pgd offset in bytes */
+		uasm_i_dsrl(p, tmp, tmp, PGDIR_SHIFT-3);
+	else
+		uasm_i_dsrl32(p, tmp, tmp, PGDIR_SHIFT - 3 - 32);
 
 	uasm_i_andi(p, tmp, tmp, (PTRS_PER_PGD - 1)<<3);
 	uasm_i_daddu(p, ptr, ptr, tmp); /* add in pgd offset */
@@ -632,53 +637,19 @@ build_get_pgd_vmalloc64(u32 **p, struct uasm_label **l, struct uasm_reloc **r,
 			enum vmalloc64_mode mode)
 {
 	long swpd = (long)swapper_pg_dir;
-	int single_insn_swpd;
-	int did_vmalloc_branch = 0;
-
-	single_insn_swpd = uasm_in_compat_space_p(swpd) && !uasm_rel_lo(swpd);
 
 	uasm_l_vmalloc(l, *p);
 
-	if (mode == refill && check_for_high_segbits) {
-		if (single_insn_swpd) {
-			uasm_il_bltz(p, r, bvaddr, label_vmalloc_done);
-			uasm_i_lui(p, ptr, uasm_rel_hi(swpd));
-			did_vmalloc_branch = 1;
-			/* fall through */
-		} else {
-			uasm_il_bgez(p, r, bvaddr, label_large_segbits_fault);
-		}
-	}
-	if (!did_vmalloc_branch) {
-		if (uasm_in_compat_space_p(swpd) && !uasm_rel_lo(swpd)) {
-			uasm_il_b(p, r, label_vmalloc_done);
-			uasm_i_lui(p, ptr, uasm_rel_hi(swpd));
-		} else {
-			UASM_i_LA_mostly(p, ptr, swpd);
-			uasm_il_b(p, r, label_vmalloc_done);
-			if (uasm_in_compat_space_p(swpd))
-				uasm_i_addiu(p, ptr, ptr, uasm_rel_lo(swpd));
-			else
-				uasm_i_daddiu(p, ptr, ptr, uasm_rel_lo(swpd));
-		}
-	}
-	if (mode == refill && check_for_high_segbits) {
-		uasm_l_large_segbits_fault(l, *p);
-		/*
-		 * We get here if we are an xsseg address, or if we are
-		 * an xuseg address above (PGDIR_SHIFT+PGDIR_BITS) boundary.
-		 *
-		 * Ignoring xsseg (assume disabled so would generate
-		 * (address errors?), the only remaining possibility
-		 * is the upper xuseg addresses.  On processors with
-		 * TLB_SEGBITS <= PGDIR_SHIFT+PGDIR_BITS, these
-		 * addresses would have taken an address error. We try
-		 * to mimic that here by taking a load/istream page
-		 * fault.
-		 */
-		UASM_i_LA(p, ptr, (unsigned long)tlb_do_page_fault_0);
-		uasm_i_jr(p, ptr);
-		uasm_i_nop(p);
+	if (uasm_in_compat_space_p(swpd) && !uasm_rel_lo(swpd)) {
+		uasm_il_b(p, r, label_vmalloc_done);
+		uasm_i_lui(p, ptr, uasm_rel_hi(swpd));
+	} else {
+		UASM_i_LA_mostly(p, ptr, swpd);
+		uasm_il_b(p, r, label_vmalloc_done);
+		if (uasm_in_compat_space_p(swpd))
+			uasm_i_addiu(p, ptr, ptr, uasm_rel_lo(swpd));
+		else
+			uasm_i_daddiu(p, ptr, ptr, uasm_rel_lo(swpd));
 	}
 }
 
@@ -786,11 +757,17 @@ static void __cpuinit build_update_entries(u32 **p, unsigned int tmp,
 		uasm_i_ld(p, tmp, 0, ptep); /* get even pte */
 		uasm_i_ld(p, ptep, sizeof(pte_t), ptep); /* get odd pte */
 		if (kernel_uses_smartmips_rixi) {
+#ifdef CONFIG_NLM_XLP
+			uasm_i_dsrl(p, tmp, tmp, ilog2(_PAGE_GLOBAL)); /* convert to entrylo0 */
+			UASM_i_MTC0(p, tmp, C0_ENTRYLO0); /* load it */
+			uasm_i_dsrl(p, ptep, ptep, ilog2(_PAGE_GLOBAL)); /* convert to entrylo1 */
+#else
 			UASM_i_SRL(p, tmp, tmp, ilog2(_PAGE_NO_EXEC));
 			UASM_i_SRL(p, ptep, ptep, ilog2(_PAGE_NO_EXEC));
 			UASM_i_ROTR(p, tmp, tmp, ilog2(_PAGE_GLOBAL) - ilog2(_PAGE_NO_EXEC));
 			UASM_i_MTC0(p, tmp, C0_ENTRYLO0); /* load it */
 			UASM_i_ROTR(p, ptep, ptep, ilog2(_PAGE_GLOBAL) - ilog2(_PAGE_NO_EXEC));
+#endif
 		} else {
 			uasm_i_dsrl_safe(p, tmp, tmp, ilog2(_PAGE_GLOBAL)); /* convert to entrylo0 */
 			UASM_i_MTC0(p, tmp, C0_ENTRYLO0); /* load it */
@@ -813,6 +790,15 @@ static void __cpuinit build_update_entries(u32 **p, unsigned int tmp,
 	if (r45k_bvahwbug())
 		build_tlb_probe_entry(p);
 	if (kernel_uses_smartmips_rixi) {
+#ifdef CONFIG_NLM_XLP
+		UASM_i_SRL(p, tmp, tmp, ilog2(_PAGE_GLOBAL)); /* convert to entrylo0 */
+		if (r4k_250MHZhwbug())
+			UASM_i_MTC0(p, 0, C0_ENTRYLO0);
+		UASM_i_MTC0(p, tmp, C0_ENTRYLO0); /* load it */
+		UASM_i_SRL(p, ptep, ptep, ilog2(_PAGE_GLOBAL)); /* convert to entrylo1 */
+		if (r45k_bvahwbug())
+			uasm_i_mfc0(p, tmp, C0_INDEX);
+#else
 		UASM_i_SRL(p, tmp, tmp, ilog2(_PAGE_NO_EXEC));
 		UASM_i_SRL(p, ptep, ptep, ilog2(_PAGE_NO_EXEC));
 		UASM_i_ROTR(p, tmp, tmp, ilog2(_PAGE_GLOBAL) - ilog2(_PAGE_NO_EXEC));
@@ -820,6 +806,7 @@ static void __cpuinit build_update_entries(u32 **p, unsigned int tmp,
 			UASM_i_MTC0(p, 0, C0_ENTRYLO0);
 		UASM_i_MTC0(p, tmp, C0_ENTRYLO0); /* load it */
 		UASM_i_ROTR(p, ptep, ptep, ilog2(_PAGE_GLOBAL) - ilog2(_PAGE_NO_EXEC));
+#endif
 	} else {
 		UASM_i_SRL(p, tmp, tmp, ilog2(_PAGE_GLOBAL)); /* convert to entrylo0 */
 		if (r4k_250MHZhwbug())
@@ -848,13 +835,16 @@ static void __cpuinit build_r4000_tlb_refill_handler(void)
 	u32 *p = tlb_handler;
 	struct uasm_label *l = labels;
 	struct uasm_reloc *r = relocs;
-	u32 *f;
+	u32 __maybe_unused *f;
 	unsigned int final_len;
 
+#if !defined(CONFIG_MAPPED_KERNEL)
+	memset(final_handler, 0, sizeof(final_handler));
+#endif
+
 	memset(tlb_handler, 0, sizeof(tlb_handler));
 	memset(labels, 0, sizeof(labels));
 	memset(relocs, 0, sizeof(relocs));
-	memset(final_handler, 0, sizeof(final_handler));
 
 	/*
 	 * create the plain linear handler
@@ -873,7 +863,7 @@ static void __cpuinit build_r4000_tlb_refill_handler(void)
 		/* No need for uasm_i_nop */
 	}
 
-#ifdef CONFIG_NLM_TLB_STAT
+#ifdef CONFIG_NLM_COMMON
 	uasm_i_dmfc0(&p, K0, OS_SCRATCH_REG2);
 	uasm_i_daddiu(&p, K0, K0, 1);
 	uasm_i_dmtc0(&p, K0, OS_SCRATCH_REG2);
@@ -896,7 +886,7 @@ static void __cpuinit build_r4000_tlb_refill_handler(void)
 	/*
 	 * FIXME: Do we need the following ifdef functionality
 	 */
-#ifdef CONFIG_NLM_TLB_STAT
+#ifdef CONFIG_NLM_COMMON
 	/* this is to avoid split of the table at eret instruction
 	 * The code below does a split at 30th instruction.
 	 */
@@ -916,6 +906,7 @@ static void __cpuinit build_r4000_tlb_refill_handler(void)
 	build_get_pgd_vmalloc64(&p, &l, &r, K0, K1, refill);
 #endif
 
+#if !defined(CONFIG_MAPPED_KERNEL)
 	/*
 	 * Overflow check: For the 64bit handler, we need at least one
 	 * free instruction slot for the wrap-around branch. In worst
@@ -1013,15 +1004,48 @@ static void __cpuinit build_r4000_tlb_refill_handler(void)
 	}
 #endif /* CONFIG_64BIT */
 
+#endif /* !defined(CONFIG_MAPPED_KERNEL) */
+
 	uasm_resolve_relocs(relocs, labels);
 	pr_debug("Wrote TLB refill handler (%u instructions).\n",
 		 final_len);
 
+#if !defined(CONFIG_MAPPED_KERNEL)
 	memcpy((void *)ebase, final_handler, 0x100);
+#endif
 
 	dump_handler((u32 *)ebase, 64);
 }
 
+#if defined(CONFIG_MAPPED_KERNEL)
+
+static u32 tlb_handler_stub[32] __cpuinitdata;
+
+static void __cpuinit __attribute__((unused)) build_r4000_tlb_refill_handler_stub(void)
+{
+	u32 *p = tlb_handler_stub;
+
+	memset(tlb_handler_stub, 0, sizeof(tlb_handler_stub));
+	UASM_i_LA(&p, K0, (unsigned long) tlb_handler);
+	uasm_i_jr(&p, K0);
+	uasm_i_nop(&p);
+
+	/*
+	 * 32 instruction = 128 bytes
+	 */
+#ifdef CONFIG_64BIT
+	memcpy((void *)ebase + 0x80, tlb_handler_stub, 0x80); /* XTLB exception */
+#else
+	memcpy((void *)ebase, tlb_handler_stub, 0x80); /* TLB exception */
+#endif
+}
+
+#else
+
+static void __cpuinit __attribute__((unused)) build_r4000_tlb_refill_handler_stub(void) { }
+
+#endif /* defined(CONFIG_MAPPED_KERNEL) */
+
 /*
  * 128 instructions for the fastpath handler is generous and should
  * never be exceeded.
@@ -1376,7 +1400,24 @@ build_r4000_tlbchange_handler_tail(u32 **p, struct uasm_label **l,
 	uasm_i_ori(p, ptr, ptr, sizeof(pte_t));
 	uasm_i_xori(p, ptr, ptr, sizeof(pte_t));
 	build_update_entries(p, tmp, ptr);
+	uasm_i_tlbp(p);
+	uasm_i_ehb(p);
+	uasm_i_mfc0(p, ptr, C0_INDEX);
+	uasm_il_bltz(p, r, ptr, label_r4000_write_probe_fail);
+	uasm_i_nop(p);
+
 	build_tlb_write_entry(p, l, r, tlb_indexed);
+
+#ifdef CONFIG_HUGETLBFS
+#ifdef CONFIG_PAGE_SIZE_4KB
+	uasm_i_mtc0(p, ZERO, C0_PAGEMASK);
+#else
+	uasm_i_mtc0(p, K0, C0_PAGEMASK);
+#endif
+#endif
+	uasm_i_eret(p);
+	uasm_l_r4000_write_probe_fail(l, *p);
+	build_tlb_write_entry(p, l, r, tlb_random);
 	uasm_l_leave(l, *p);
 	uasm_i_eret(p); /* return from trap */
 
@@ -1422,7 +1463,15 @@ static void __cpuinit build_r4000_tlb_load_handler(void)
 		uasm_i_andi(&p, K0, K0, _PAGE_VALID);
 		uasm_il_beqz(&p, &r, K0, label_tlbl_goaround1);
 		uasm_i_nop(&p);
-
+#ifdef CONFIG_NLM_XLP
+		/*
+		 *if tlb probe has failed, skip to label_tlbl_goaround assuming
+		 *exception is due to valid bit not being set in entrylo.
+		 */
+		uasm_i_mfc0(&p, K0, C0_INDEX);
+		uasm_il_bltz(&p, &r, K0, label_tlbl_goaround1);
+		uasm_i_nop(&p);
+#endif
 		uasm_i_tlbr(&p);
 		/* Examine  entrylo 0 or 1 based on ptr. */
 		uasm_i_andi(&p, K0, K1, sizeof(pte_t));
@@ -1605,10 +1654,6 @@ void __cpuinit build_tlb_refill_handler(void)
 	 */
 	static int run_once = 0;
 
-#ifdef CONFIG_64BIT
-	check_for_high_segbits = current_cpu_data.vmbits > (PGDIR_SHIFT + PGD_ORDER + PAGE_SHIFT - 3);
-#endif
-
 	switch (current_cpu_type()) {
 	case CPU_R2000:
 	case CPU_R3000:
@@ -1641,6 +1686,7 @@ void __cpuinit build_tlb_refill_handler(void)
 
 	default:
 		build_r4000_tlb_refill_handler();
+		build_r4000_tlb_refill_handler_stub();
 		if (!run_once) {
 			build_r4000_tlb_load_handler();
 			build_r4000_tlb_store_handler();
diff --git a/arch/mips/netlogic/Kconfig b/arch/mips/netlogic/Kconfig
index dea897a..99a1a0b 100644
--- a/arch/mips/netlogic/Kconfig
+++ b/arch/mips/netlogic/Kconfig
@@ -25,6 +25,11 @@ config PGWALKER
 	  it is disabled by default. Please consider the potential
 	  risk before enabling it.
 
+config KSEG2_LOWMEM
+       bool "Mapped Lowmem"
+       depends on MAPPED_KERNEL && (64BIT || 32BIT)
+       default y
+
 config NLM_ENABLE_COP2
 	bool "Enable Cop2 Access"
 	depends on NLM_XLP && 64BIT
@@ -38,11 +43,28 @@ config NLM_TLB_STAT
 	help
 	  This counts tlb refill exceptions and the results can be accessed
 	  from /proc/netlogic/xlp_cpu
+config MAPPED_KERNEL
+       bool "Mapped kernel"
+       default y
+       help
+         Select this option if you want the kernel's code and data to
+         be in mapped memory.  The kernel will be mapped using a
+         single wired TLB entry, thus reducing the number of
+         available TLB entries by one.  Kernel modules will be able
+         to use a more efficient calling convention.
+
+config PHYS_LOAD_ADDRESS
+       hex "Physical load address"
+       depends on MAPPED_KERNEL
+       default 0xffffffff81000000
+       help
+         The physical load address reflected as the program header
+         physical address in the kernel ELF image.
 
 config NLM_COMMON_LOAD_ADDRESS
 	hex "Netlogic Linux kernel start address"
 	depends on NLM_COMMON
-	default "0xffffffff80200000"
+	default "0xffffffffc1000000"
 	help
 	  This is start address for the linux kernel. Default value
           should be good for most of the applications unless specified
diff --git a/arch/mips/netlogic/common/memory.c b/arch/mips/netlogic/common/memory.c
index c69cf03..9c95467 100644
--- a/arch/mips/netlogic/common/memory.c
+++ b/arch/mips/netlogic/common/memory.c
@@ -30,6 +30,11 @@
 #include <asm/page.h>
 #include <asm/mach-netlogic/mmu.h>
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+#include <linux/bootmem.h>
+#include <asm/pgtable.h>
+#endif
+
 /*
  * the following structures and definitions are internal to this
  * file and hence not defined in a header file
@@ -76,26 +81,33 @@ static uint32_t tlb_mask(uint32_t size)
 	return mipstlbs[i].mask;
 }
 
-#define entrylo(paddr, attr) \
-	((((paddr & 0xffffffffffULL) >> 12) << 6) | (attr))
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+#define page_entrylo(paddr, attr) ((((paddr) >> 12) << 6) | (attr))
 
+/*
+ * External Function / APIs
+ */
 
-#ifdef CONFIG_CPU_HAS_PREFETCH
-
-#define PREF_BACKUP_MEM	512
-
-phys_t fix_prefetch_mem(phys_t size)
+static void setup_tlb(tlb_entry_t *tlb, unsigned long pagesize)
 {
-	/*
-	 * memcpy/__copy_user prefetch, which
-	 * will cause a bus error for
-	 * KSEG/KUSEG addrs not backed by RAM.
-	 * Hence, reserve some padding for the
-	 * prefetch distance.
-	 */
-	return size - ((size > PREF_BACKUP_MEM) ? PREF_BACKUP_MEM : 0);
+	write_c0_pagemask(tlb_mask(pagesize) << 13);
+	write_c0_entryhi(tlb->entryHi & ~0x1fff);
+	write_c0_entrylo0(tlb->entrylo0);
+	write_c0_entrylo1(tlb->entrylo1);
+
+	if (tlb->wired) {
+		write_c0_index(read_c0_wired());
+		tlb_write_indexed();
+		write_c0_wired(read_c0_wired() + 1);
+	}
+	else {
+		tlb_write_random();
+	}
 }
-#endif
+#else
+#define entrylo(paddr, attr) \
+	((((paddr & 0xffffffffffULL) >> 12) << 6) | (attr))
+
 
 /*
  * External Function / APIs
@@ -117,6 +129,7 @@ void setup_tlb(tlb_info_t *tlb)
 		tlb_write_random();
 	}
 }
+#endif
 
 #ifdef CONFIG_MAPPED_KERNEL
 
@@ -137,11 +150,141 @@ unsigned long __vmalloc_start = 0xe0000000;
 static volatile int max_low_pfn_set = 0;
 extern unsigned long max_low_pfn;
 
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+
+#ifdef CONFIG_64BIT
+#define KERNEL_SEG_START	XKSEG
+#else
+#define KERNEL_SEG_START	KSEG2
+#endif
+
+#define MIN(x,y)  ((x) < (y) ? (x) : (y))
+
+#undef alloc_bootmem_low
+
+static unsigned long
+alloc_bootmem_low(gfp_t gfp_mask, unsigned int order)
+{
+	return (unsigned long)__alloc_bootmem_low((1 << order) * PAGE_SIZE, SMP_CACHE_BYTES, 0);
+}
+
+unsigned long NONWIRED_START = ~0x0;
+unsigned long NONWIRED_END = ~0x0;
+
+void setup_mapped_kernel_tlbs(int firstpage, int primary_cpu)
+{
+	tlb_entry_t tlb;
+	unsigned long max_wired_size;
+	unsigned long pagesize;
+	unsigned long vaddr, paddr;
+	unsigned short attr;
+
+	pagesize = LARGEST_TLBPAGE_SZ; /* we set up the largest pages */
+
+	/*
+	 * In NetLogic's Linux kernel, the second 256MB of physical
+	 * address space is reserved for device configuration and
+	 * is not mapped to DRAM (to imply memory as opposed to IO
+	 * device space). Hence the attribute of the second part of
+	 * the first wired entry is invalid, while the both part of
+	 * other wired entries are symmetric. We handle the above
+	 * difference through the following unseemly if condition
+	 */
+	if (firstpage) {
+		tlb.entryHi = KERNEL_SEG_START;
+
+		attr = (KERNEL_PAGE_ATTR >> ENTRYLO_PFN_SHIFT);
+		tlb.entrylo0 = page_entrylo(0, attr); /* we start at pfn = 0 */
+
+		/* 256MB - 512 MB is IO config (invalid dram) */
+		attr = (_PAGE_GLOBAL >> ENTRYLO_PFN_SHIFT);
+		tlb.entrylo1 = page_entrylo(0, attr);
+
+		tlb.wired = TRUE;
+		setup_tlb(&tlb, pagesize);
+	}
+	else {
+		int retval;
+		/*
+		 * the primary cpu reads the memory map and records
+		 * the highest page frame number. Secondary cpus
+		 * must wait till the variable max_low_pfn is set
+		 */
+		if (!primary_cpu)
+			while (!max_low_pfn_set)
+				;
+		vaddr = KERNEL_SEG_START + 2 * LARGEST_TLBPAGE_SZ;
+		max_wired_size = PFN_PHYS(MIN(max_low_pfn, MAX_WIRED_PFN));
+		attr = (KERNEL_PAGE_ATTR >> ENTRYLO_PFN_SHIFT);
+
+		/*
+		 * the following loop assumes that pagesize is set to
+		 * 256 MB. change the logic if the pagesize ever changes
+		 */
+		paddr = 2 * LARGEST_TLBPAGE_SZ;
+		for (; paddr < max_wired_size;
+ 				paddr += 2 * pagesize, vaddr += 2 * pagesize) {
+#ifdef DEBUG
+			printk("(wired entry): vaddr = 0x%lx, paddr = 0x%lx\n", vaddr, paddr);
+#endif
+/* Skip 3 - 3.5GB range (PCI device space) */
+			if (paddr == PCIDEV_ADDRSPACE_START)
+				continue;
+			tlb.entryHi = vaddr;
+			tlb.entrylo0 = page_entrylo(paddr, attr);
+			tlb.entrylo1 = page_entrylo(paddr + pagesize, attr);
+			tlb.wired = TRUE;
+			setup_tlb(&tlb, pagesize);
+		}
+
+		if (primary_cpu) {
+			if (max_low_pfn > MAX_WIRED_PFN) {
+				__get_free_pages = alloc_bootmem_low;
+				retval = map_kernel_addrspace(vaddr, MAX_WIRED_PFN, max_low_pfn);
+				if (retval != 0)
+					panic("unable to map kernel addrspace\n");
+				NONWIRED_START = vaddr;
+				NONWIRED_END = vaddr + (PFN_PHYS(max_low_pfn - MAX_WIRED_PFN));
+				__get_free_pages = ____get_free_pages;
+			}
+#ifdef CONFIG_64BIT
+			__vmalloc_start = KERNEL_SEG_START + (1UL << PGDIR_SHIFT);
+#else
+			__vmalloc_start = vaddr;
+#endif
+		}
+	}
+}
+
+#define FLOOR(addr, alignment) ((addr) & ~((alignment) - 1)) /* alignment must be power of 2 */
+
+unsigned long recalculate_max_low_pfn(unsigned long max_low_pfn)
+{
+	/*
+	 * truncate max_low_pfn to 512MB boundary as largest tlb
+	 * pages are used to minimize the number of wired entries
+	 */
+	if ((max_low_pfn > PFN_DOWN(LARGEST_TLBPAGE_SZ << 1)) && (max_low_pfn <= MAX_WIRED_PFN))
+		max_low_pfn = PFN_DOWN(FLOOR(PFN_PHYS(max_low_pfn), LARGEST_TLBPAGE_SZ << 1));
+	max_low_pfn_set = TRUE;
+	__sync();
+
+	return max_low_pfn;
+}
+
+#else
+
+#ifdef CONFIG_64BIT
+#define TLB_VADDR	XKSEG
+#else
+#define TLB_VADDR       KSEG2
+#endif
+
 void setup_mapped_kernel_tlbs(int firstpage, int primary_cpu)
 {
 	tlb_info_t tlb;
 
-    tlb.pagesize = LARGEST_TLBPAGE_SZ; /* we set up the largest pages */
+	tlb.pagesize = LARGEST_TLBPAGE_SZ; /* we set up the largest pages */
 
 	/*
 	 * In NetLogic's Linux kernel, the second 256MB of physical
@@ -153,7 +296,7 @@ void setup_mapped_kernel_tlbs(int firstpage, int primary_cpu)
 	 * difference through the following unseemly if condition
 	 */
 	if (firstpage) {
-		tlb.vaddr = XKSEG;
+                tlb.vaddr = TLB_VADDR;
 		tlb.paddr1 = tlb.paddr0 = 0;
 		tlb.attr0 = ((_CACHE_CACHABLE_COW |_PAGE_DIRTY |  _PAGE_VALID | _PAGE_GLOBAL) >> ENTRYLO_PFN_SHIFT);
 		tlb.attr1 = _PAGE_GLOBAL >> ENTRYLO_PFN_SHIFT;
@@ -169,8 +312,7 @@ void setup_mapped_kernel_tlbs(int firstpage, int primary_cpu)
 		if (!primary_cpu)
 			while (!max_low_pfn_set)
 				;
-
-		tlb.vaddr = XKSEG + 2 * LARGEST_TLBPAGE_SZ; 
+		tlb.vaddr = TLB_VADDR + 2 * LARGEST_TLBPAGE_SZ;
 		tlb.paddr0 = 2 * LARGEST_TLBPAGE_SZ;
 		for (; tlb.paddr0 < (max_low_pfn << PAGE_SHIFT);
 			 tlb.paddr0 += 2 * tlb.pagesize, tlb.vaddr += 2 * tlb.pagesize) {
@@ -203,6 +345,8 @@ unsigned long recalculate_max_low_pfn(unsigned long max_low_pfn)
 	return max_low_pfn;
 }
 
+#endif
+
 #else
 
 void setup_mapped_kernel_tlbs(int index, int secondary_cpu) { }
diff --git a/arch/mips/netlogic/xlp/Makefile b/arch/mips/netlogic/xlp/Makefile
index 3b1eb51..97fe167 100644
--- a/arch/mips/netlogic/xlp/Makefile
+++ b/arch/mips/netlogic/xlp/Makefile
@@ -7,4 +7,4 @@ obj-$(CONFIG_NLM_XLP) 		+= platform.o board.o
 obj-$(CONFIG_NLM_XLP) 		+= xlp_hal_pic.o xlp_gpio.o i2c.o
 obj-$(CONFIG_SMP)       	+= smp.o
 
-obj-$(CONFIG_NLM_XLP) += cpu_control.o cpu_control_asm.o
+obj-$(CONFIG_NLM_XLP) += cpu_control.o cpu_control_asm.o nmi.o
diff --git a/arch/mips/netlogic/xlp/cpu_control_asm.S b/arch/mips/netlogic/xlp/cpu_control_asm.S
index b1660a0..8e933e4 100644
--- a/arch/mips/netlogic/xlp/cpu_control_asm.S
+++ b/arch/mips/netlogic/xlp/cpu_control_asm.S
@@ -6,7 +6,7 @@
 #include <asm/asmmacro.h>
 
 #include <asm/netlogic/mips-exts.h>
-#include <kernel-entry-init.h>
+#include <asm/mach-netlogic/kernel-entry-init.h>
 #include "cpu_control_macros.h"
 
 	.macro  prog_c0_status set clr
@@ -16,8 +16,6 @@
 	xor 	t0, 0x1f|\clr
 	mtc0    t0, CP0_STATUS
 	.set    noreorder
-	nop
-	nop
 	sll 	zero,3              # ehb
 	.set    pop
 	.endm
@@ -84,6 +82,10 @@
 .macro __config_lsu
 	.set push
 	.set noreorder
+	li      t0, SCHED_DEFEATURE
+	lui     t1, 0x0100 # Experimental: Disable BRU accepting ALU ops
+	mtcr    t1, t0
+
 	li      t0, LSU_DEFEATURE
 
 	mfcr    t1, t0
@@ -99,11 +101,6 @@
 	mfcr    t1, t0
 	li	t2, ~0xe    # S1RCM
 	and	t1, t1, t2
-
-	mtcr    t1, t0
-
-	li      t0, SCHED_DEFEATURE
-	lui     t1, 0x0100 # Experimental: Disable BRU accepting ALU ops
 	mtcr    t1, t0
 
 10:
@@ -267,8 +264,6 @@ END(ptr_smp_boot)
 	xor	t0, 0x1f|\clr
 	mtc0	t0, CP0_STATUS
 	.set	noreorder
-	nop
-	nop
 	sll	zero,3				# ehb
 	.set	pop
 	.endm
@@ -280,6 +275,9 @@ END(ptr_smp_boot)
  */
 NESTED(prom_pre_boot_secondary_cpus, 16, sp)
         SET_MIPS64
+#if defined(CONFIG_NLM_XLP) && defined(CONFIG_MAPPED_KERNEL)
+        MAPPED_KERNEL_SETUP_TLB
+#endif
 
 	/* Don't trust the bootstrapper to set cp0_status to what you want */
 #ifdef CONFIG_64BIT
diff --git a/arch/mips/netlogic/xlp/nmi.S b/arch/mips/netlogic/xlp/nmi.S
index 0a1d321..3a64433 100644
--- a/arch/mips/netlogic/xlp/nmi.S
+++ b/arch/mips/netlogic/xlp/nmi.S
@@ -50,4 +50,3 @@ NESTED(nlm_except_vec_nmi, 0, sp)
 	.set pop
 END(nlm_except_vec_nmi)
 
-#endif
diff --git a/arch/mips/netlogic/xlp/setup.c b/arch/mips/netlogic/xlp/setup.c
index 7b07f15..46444df 100644
--- a/arch/mips/netlogic/xlp/setup.c
+++ b/arch/mips/netlogic/xlp/setup.c
@@ -61,6 +61,7 @@
 #include <asm/netlogic/hal/nlm_hal_macros.h>
 #include <asm/netlogic/xlp_hal_pic.h>
 #include <asm/mach-netlogic/nlm_kexec.h>
+#include <asm/netlogic/phnx_loader.h>
 #include "../boot/ops.h"
 #include "cpu_control_macros.h"
 
@@ -156,36 +157,22 @@ struct xlp_stack_pages xlp_stack_pages_temp
 __attribute__((__section__(".data.init_task"),
 	       __aligned__(THREAD_SIZE)));
 
-struct boot_mem_map boot_physaddr_info = {
-	.nr_map = 5,
-	.map = {
-		[0] = {
-			.addr = 0ULL,
-			.size = 0x14000000ULL,
-			.type = BOOT_MEM_RAM
-		},
-		[1] = {
-			.addr = 0x14000000ULL,
-			.size = 0x09000000ULL,
-			.type = BOOT_MEM_RESERVED
-		},
-		[2] = {
-			.addr = 0x1D000000ULL,
-			.size = 0xA3000000ULL,
-			.type = BOOT_MEM_RAM
-		},
-		[3] = {
-			.addr = 0xC0000000ULL,
-			.size = 0x20000000ULL,
-			.type = BOOT_MEM_RESERVED
-		},
-		[4] = {
-			.addr = 0xE0000000ULL,
-			.size = 0x20000000ULL,
-			.type = BOOT_MEM_RAM
-		},
-	}
+struct boot_mem_map boot_physaddr_info;
+struct xlp_dram_mapping {
+                unsigned long low_pfn;
+                unsigned long high_pfn;
+                int node;
 };
+#define NLM_NODES_MAX_DRAM_REGION (NLM_MAX_DRAM_REGION * MAX_NUMNODES)
+struct xlp_dram_mapping  dram_map[NLM_NODES_MAX_DRAM_REGION];
+
+#define NLM_DRAM_BASE_REG_0     20
+#define NLM_DRAM_LIMIT_REG_0    28
+#define NLM_DRAM_NODEADDR_XLAT  36
+#define HDR_OFFSET      0x100
+#define BRIDGE  (0x00<<20) | (0x00<<15) | (0x00<<12)
+#define cpu_io_mmio_setup(node,offset)  ((__u32 *)(DEFAULT_NETLOGIC_IO_BASE + \
+                         (node<<18) + (offset) + HDR_OFFSET))
 
 int nlm_common_get_pgprot(unsigned long address)
 {
@@ -225,6 +212,51 @@ int valid_mmap_nlm_common_addr_range(unsigned long pfn)
 	return 0;
 }
 
+void read_node_bars(int node)
+{
+        int i, idx;
+        uint32_t *membase = cpu_io_mmio_setup(node, BRIDGE);
+
+        for (i = 0; i < NLM_MAX_DRAM_REGION; i++) {
+                uint64_t base_reg  = *(membase + NLM_DRAM_BASE_REG_0  + i);
+                uint64_t limit_reg = *(membase + NLM_DRAM_LIMIT_REG_0 + i);
+                uint32_t node_reg = *(membase + NLM_DRAM_NODEADDR_XLAT + i);
+
+                if(((node_reg >> 1) & 0x3) != node) {
+                        continue;
+                }
+                if(((limit_reg >> 12) << 20) == 0) {
+                        continue;
+                }
+
+                idx = (node * NLM_MAX_DRAM_REGION) + i;
+                dram_map[idx].low_pfn = ((base_reg >> 12) << 20) >> PAGE_SHIFT;
+                dram_map[idx].high_pfn =
+                        ((limit_reg >> 12) << 20) >> PAGE_SHIFT;
+                dram_map[idx].node = node;
+
+                if(dram_map[idx].high_pfn == dram_map[idx].low_pfn){
+                    continue;
+                }
+                boot_physaddr_info.map[boot_physaddr_info.nr_map].addr = dram_map[idx].low_pfn << PAGE_SHIFT;
+                boot_physaddr_info.map[boot_physaddr_info.nr_map].size =
+                        (dram_map[idx].high_pfn - dram_map[idx].low_pfn + (1<<(20-PAGE_SHIFT))) << PAGE_SHIFT;
+                boot_physaddr_info.map[boot_physaddr_info.nr_map].type = BOOT_MEM_RAM;
+                boot_physaddr_info.nr_map++;
+        }
+}
+
+void nlm_get_dram_mapping(void)
+{
+        int  node;
+        boot_physaddr_info.nr_map = 0;
+
+        for(node=0; node < MAX_NUMNODES; node++) {
+                read_node_bars(node);
+        }
+}
+
+
 const char *get_system_type(void)
 {
 	return "Netlogic XLP SoC";
@@ -360,6 +392,17 @@ static void prom_add_memory(uint64_t start, uint64_t size)
 	return;
 }
 
+
+void __init nlm_nmi_setup (void)
+{
+	void *base;
+	extern char nlm_except_vec_nmi;
+
+	printk("Setting up NMI Handler \n");
+	base = (void *)(unsigned long)0xffffffffbfc00000ULL;
+	memcpy(base, &nlm_except_vec_nmi, 0x80);
+}
+
 /* setup early serial port driver */
 #ifdef CONFIG_SERIAL_8250
 #define UART_CLK 133333333
@@ -406,6 +449,7 @@ static void nlm_early_serial_setup(int uart_id) {}
 #endif /* CONFIG_SERIAL_8250 */
 
 extern struct plat_smp_ops nlm_smp_ops;
+struct loader_mem_info loader_mem_map[MAX_NUM_LOADER_MEM_BLK];
 
 #define MAX_CPUMASK_CELLS 4
 #define fdt32_to_cpu(x) be32_to_cpu(x)
@@ -743,6 +787,9 @@ static int fdt_process(void)
 				lsb_addr = fdt32_to_cpu(regs[i + 1]);
 				msb_size = fdt32_to_cpu(regs[i + 2]);
 				lsb_size = fdt32_to_cpu(regs[i + 3]);
+				loader_mem_map[index].start_addr = lsb_addr | (msb_addr << 32);
+				loader_mem_map[index].size  = lsb_size | (msb_size << 32);
+				printk("LoaderMemory [%#llx] @ [%#llx]\n",loader_mem_map[index].size, loader_mem_map[index].start_addr);
 		}
 noloadermask:
 		cpumask_scnprintf(buf, CPUMASK_BUF, &fdt_loadermask);
@@ -909,8 +956,6 @@ void __init prom_init(void)
 
 	xen_init();
 
-	write_c0_ebase(read_c0_ebase() | 0x10000);
-
 	nlm_common_ebase = read_c0_ebase() & (~((1 << 12) - 1));
 
 	/* FIXME: we should also remove it for xlp8xx a2, but we do not have interface function
@@ -926,7 +971,10 @@ void __init prom_init(void)
 	cpumask_clear(&smp_boot.online_map);
 	cpumask_set_cpu(hard_smp_processor_id(), &smp_boot.online_map);
 
+	board_nmi_handler_setup = nlm_nmi_setup;
+
 	on_chip_init();
+	nlm_get_dram_mapping();
 
 	prom_reconfigure_thr_resources();
 
-- 
1.7.0.4

