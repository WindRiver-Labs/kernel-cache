From e4aa8b9283544b8e99c2caeac5725016636fddbf Mon Sep 17 00:00:00 2001
From: Yanjiang Jin <yanjiang.jin@windriver.com>
Date: Tue, 21 Aug 2012 13:23:11 +0800
Subject: [PATCH 1/2] nlm_xlp_64_be: add ehb after sync/sc

errata: add ehb after sync/sc for barrier semantics

Signed-off-by: Yonghong Song <ysong@broadcom.com>
Integrated-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 arch/mips/include/asm/barrier.h              |   29 ++++++++++++++++++++++++++
 arch/mips/include/asm/io.h                   |    4 +++
 arch/mips/include/asm/netlogic/nlm_rw_lock.h |    5 ++++
 3 files changed, 38 insertions(+), 0 deletions(-)

diff --git a/arch/mips/include/asm/barrier.h b/arch/mips/include/asm/barrier.h
index 1419b78..4873be0 100644
--- a/arch/mips/include/asm/barrier.h
+++ b/arch/mips/include/asm/barrier.h
@@ -64,6 +64,20 @@
 #define smp_read_barrier_depends()	do { } while(0)
 
 #ifdef CONFIG_CPU_HAS_SYNC
+
+#ifdef CONFIG_NLM_XLP
+#define __sync()				\
+	__asm__ __volatile__(			\
+		".set	push\n\t"		\
+		".set	noreorder\n\t"		\
+		".set	mips64r2\n\t"		\
+		"sync\n\t"			\
+		"ehb\n\t"			\
+		".set	pop"			\
+		: /* no output */		\
+		: /* no input */		\
+		: "memory")
+#else
 #define __sync()				\
 	__asm__ __volatile__(			\
 		".set	push\n\t"		\
@@ -74,6 +88,8 @@
 		: /* no output */		\
 		: /* no input */		\
 		: "memory")
+#endif
+
 #else
 #define __sync()	do { } while(0)
 #endif
@@ -140,6 +156,12 @@
 #endif /* !CONFIG_CPU_HAS_WB */
 
 #if defined(CONFIG_WEAK_ORDERING) && defined(CONFIG_SMP)
+#ifdef CONFIG_NLM_XLP
+#define __WEAK_ORDERING_MB	".set push\n .set mips64r2\n sync\n ehb\n .set pop\n"
+#define smp_mb()	__asm__ __volatile__(__WEAK_ORDERING_MB : : : "memory")
+#define smp_rmb()	__asm__ __volatile__(__WEAK_ORDERING_MB : : : "memory")
+#define smp_wmb()	__asm__ __volatile__(__WEAK_ORDERING_MB : : : "memory")
+#else
 # ifdef CONFIG_CPU_CAVIUM_OCTEON
 #  define smp_mb()	__sync()
 #  define smp_rmb()	barrier()
@@ -149,6 +171,7 @@
 #  define smp_rmb()	__asm__ __volatile__("sync" : : :"memory")
 #  define smp_wmb()	__asm__ __volatile__("sync" : : :"memory")
 # endif
+#endif
 #else
 #define smp_mb()	barrier()
 #define smp_rmb()	barrier()
@@ -156,7 +179,11 @@
 #endif
 
 #if defined(CONFIG_WEAK_REORDERING_BEYOND_LLSC) && defined(CONFIG_SMP)
+#ifdef CONFIG_NLM_XLP
+#define __WEAK_LLSC_MB	".set push\n .set mips64r2\n sync\n ehb\n .set pop\n"
+#else
 #define __WEAK_LLSC_MB		"       sync	\n"
+#endif
 #else
 #define __WEAK_LLSC_MB		"		\n"
 #endif
@@ -165,6 +192,8 @@
 	do { var = value; smp_mb(); } while (0)
 
 #define smp_llsc_mb()	__asm__ __volatile__(__WEAK_LLSC_MB : : :"memory")
+#define smp_llsc_rmb()	__asm__ __volatile__(__WEAK_LLSC_MB : : : "memory")
+#define smp_llsc_wmb()	__asm__ __volatile__(__WEAK_LLSC_MB : : : "memory")
 
 #ifdef CONFIG_CPU_CAVIUM_OCTEON
 #define smp_mb__before_llsc() smp_wmb()
diff --git a/arch/mips/include/asm/io.h b/arch/mips/include/asm/io.h
index 7f88071..cc3ccdf 100644
--- a/arch/mips/include/asm/io.h
+++ b/arch/mips/include/asm/io.h
@@ -540,8 +540,12 @@ BUILDSTRING(q, u64)
 #define mmiowb() wmb()
 #else
 /* Depends on MIPS II instruction set */
+#ifdef CONFIG_NLM_XLP
+#define mmiowb() asm volatile ("sync\n" "ehb" : : : "memory")
+#else
 #define mmiowb() asm volatile ("sync" ::: "memory")
 #endif
+#endif
 
 static inline void memset_io(volatile void __iomem *addr, unsigned char val, int count)
 {
diff --git a/arch/mips/include/asm/netlogic/nlm_rw_lock.h b/arch/mips/include/asm/netlogic/nlm_rw_lock.h
index fcbda8a..68ff6a1 100644
--- a/arch/mips/include/asm/netlogic/nlm_rw_lock.h
+++ b/arch/mips/include/asm/netlogic/nlm_rw_lock.h
@@ -32,7 +32,12 @@ typedef struct {
 	int write_cpu; /* CPU that is currently holding wr lock */
 } nlm_rwlock_t;
 
+#ifdef CONFIG_NLM_XLP
+#define nlm_sync() __asm__ __volatile__("sync\n" "ehb" : : : "memory")
+#else
 #define nlm_sync() __asm__ __volatile__("sync": : :"memory")
+#endif
+
 __asm__ (
 		".macro\tnlm_local_irq_save result\n\t"
 		".set\tpush\n\t"
-- 
1.7.0

