From fbb1888a4fff8c94b99c0f030f8ea4efe0a7a661 Mon Sep 17 00:00:00 2001
From: Yanjiang Jin <yanjiang.jin@windriver.com>
Date: Tue, 24 Jul 2012 10:44:02 +0800
Subject: [PATCH] nlm_xlp_64_be: add ICI support

Add ICI support. Based on SDK 2.2.4.

This commit contains a Workaround:
The file "arch/mips/netlogic/common/smp.c" includes the head file
"arch/mips/include/asm/smp.h", and it includes further the head file
"include/linux/smp.h".
There is a static definition smp_send_reschedule() in
"arch/mips/include/asm/smp.h", but "include/linux/smp.h" declares this
function as external. They conflict with each other.
So we add such a nasty #ifdef to avoid conflict.

Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 arch/mips/Kconfig                                  |    2 +-
 arch/mips/include/asm/mach-netlogic/mmu.h          |    4 +
 arch/mips/include/asm/mach-netlogic/mmzone.h       |    3 +-
 arch/mips/include/asm/mach-netlogic/topology.h     |    2 +-
 arch/mips/include/asm/netlogic/hal/nlm_hal.h       |    4 +-
 arch/mips/include/asm/netlogic/hal/nlm_hal_nae.h   |    3 +-
 arch/mips/include/asm/netlogic/hal/nlm_hal_sys.h   |   48 +-
 .../include/asm/netlogic/hal/nlm_hal_xlp_dev.h     |   33 +-
 arch/mips/include/asm/netlogic/nlm_dma.h           |    2 +-
 arch/mips/include/asm/netlogic/xlp_hal_pic.h       |  290 -----
 arch/mips/include/asm/netlogic/xlp_ici.h           |  184 +++
 arch/mips/include/asm/netlogic/xlp_irq.h           |  405 ++++---
 arch/mips/include/asm/smp.h                        |    2 +
 arch/mips/kernel/nlm_fs_handler.S                  |    5 +-
 arch/mips/kernel/setup.c                           |   47 +-
 arch/mips/kernel/smp.c                             |    1 +
 arch/mips/netlogic/common/Makefile                 |    4 +-
 arch/mips/netlogic/common/memory.c                 |   56 +-
 arch/mips/netlogic/common/nlm_evp_cpld.c           |   39 +-
 arch/mips/netlogic/common/nlm_hal.c                |   20 +-
 arch/mips/netlogic/common/nlm_hal_cpu_info.c       |  189 +++-
 arch/mips/netlogic/common/nlm_hal_fmn_config.c     |   37 +-
 arch/mips/netlogic/common/nlm_hal_nae.c            |   22 +-
 arch/mips/netlogic/common/nlm_hal_sys.c            |  125 +--
 arch/mips/netlogic/common/smp.c                    |  154 +++
 arch/mips/netlogic/xlp/Makefile                    |    2 +
 arch/mips/netlogic/xlp/irq.c                       | 1298 +++++++++++---------
 arch/mips/netlogic/xlp/numa.c                      |  402 ++++++
 arch/mips/netlogic/xlp/on_chip.c                   |  315 +++---
 arch/mips/netlogic/xlp/pic/Makefile                |    7 +
 arch/mips/netlogic/xlp/pic/ite.c                   |  303 +++++
 arch/mips/netlogic/xlp/pic/pic-timer.c             |  331 +++++
 arch/mips/netlogic/xlp/pic/timer-base.c            |   93 ++
 arch/mips/netlogic/xlp/pic/timer-base.h            |   42 +
 arch/mips/netlogic/xlp/pic/timer-wd.c              |   61 +
 arch/mips/netlogic/xlp/platform.c                  |    6 +-
 arch/mips/netlogic/xlp/setup.c                     |  352 +++++-
 arch/mips/netlogic/xlp/smp.c                       |  242 ++---
 arch/mips/netlogic/xlp/time.c                      |   47 +-
 arch/mips/netlogic/xlp/xlp_gpio.c                  |    2 +-
 arch/mips/netlogic/xlp/xlp_hal_pic.c               |  162 +--
 arch/mips/pci/pci-xlp.c                            |  763 +++++++-----
 drivers/mmc/host/xlpmmc.c                          |  507 +++++----
 drivers/net/nae/init_nae.c                         |    2 +-
 drivers/net/nae/xlp_nae.c                          |  144 +--
 drivers/usb/host/ehci-pci.c                        |   77 +-
 init/main.c                                        |   21 +
 47 files changed, 4404 insertions(+), 2456 deletions(-)
 delete mode 100644 arch/mips/include/asm/netlogic/xlp_hal_pic.h
 create mode 100644 arch/mips/include/asm/netlogic/xlp_ici.h
 create mode 100644 arch/mips/netlogic/common/smp.c
 create mode 100644 arch/mips/netlogic/xlp/numa.c
 create mode 100644 arch/mips/netlogic/xlp/pic/Makefile
 create mode 100644 arch/mips/netlogic/xlp/pic/ite.c
 create mode 100644 arch/mips/netlogic/xlp/pic/pic-timer.c
 create mode 100644 arch/mips/netlogic/xlp/pic/timer-base.c
 create mode 100644 arch/mips/netlogic/xlp/pic/timer-base.h
 create mode 100644 arch/mips/netlogic/xlp/pic/timer-wd.c

diff --git a/arch/mips/Kconfig b/arch/mips/Kconfig
index 80429ae..bce3e1b 100644
--- a/arch/mips/Kconfig
+++ b/arch/mips/Kconfig
@@ -2033,7 +2033,7 @@ config SYS_SUPPORTS_NUMA
 
 config NODES_SHIFT
 	int
-	default "6"
+	default "2"
 	depends on NEED_MULTIPLE_NODES
 
 config FORCE_MAX_ZONEORDER
diff --git a/arch/mips/include/asm/mach-netlogic/mmu.h b/arch/mips/include/asm/mach-netlogic/mmu.h
index 3478715..c866b59 100644
--- a/arch/mips/include/asm/mach-netlogic/mmu.h
+++ b/arch/mips/include/asm/mach-netlogic/mmu.h
@@ -60,6 +60,10 @@ nlm_write_os_scratch_3(~(((1ULL << HUGETLB_PAGE_ORDER) - 1) << ENTRYLO_PFN_SHIFT
 extern void setup_mapped_kernel_tlbs(int index, int secondary_cpu);
 extern unsigned long recalculate_max_low_pfn(unsigned long max_low_pfn);
 
+#if defined(CONFIG_NLM_XLP) && defined(CONFIG_NUMA)
+extern void __init nlm_numa_bootmem_init(unsigned long);
+#endif
+
 #ifndef CONFIG_NLM_XLP
 #define disable_pgwalker(flags) (void)flags
 #define enable_pgwalker(flags) (void) flags
diff --git a/arch/mips/include/asm/mach-netlogic/mmzone.h b/arch/mips/include/asm/mach-netlogic/mmzone.h
index c494729..33e1dbd 100644
--- a/arch/mips/include/asm/mach-netlogic/mmzone.h
+++ b/arch/mips/include/asm/mach-netlogic/mmzone.h
@@ -23,7 +23,6 @@ struct nlm_node_mem_frag {
 #define NLM_MAX_MEM_FRAGS_PER_NODE 16
 struct nlm_node_mem_info {
 	struct nlm_node_mem_frag mem[NLM_MAX_MEM_FRAGS_PER_NODE];
-	unsigned long free_addr; /* for node_data */
 	int frags;
 };
 
@@ -71,7 +70,7 @@ static inline unsigned int pa_to_nid(unsigned long addr)
 
 	/* it should not really reach here */
 	printk("Invalid address is %lx\n", addr);
-	panic("Invalid address in pa_to_nid\n");
+/*	panic("Invalid address in pa_to_nid\n");*/
 	return 0;
 }
 
diff --git a/arch/mips/include/asm/mach-netlogic/topology.h b/arch/mips/include/asm/mach-netlogic/topology.h
index 56e66ab..7f4930e 100644
--- a/arch/mips/include/asm/mach-netlogic/topology.h
+++ b/arch/mips/include/asm/mach-netlogic/topology.h
@@ -6,7 +6,7 @@
 #include <asm/mmzone.h>
 
 /* FIXME_XLP: only works for all cpus up */
-#define cpu_to_node(cpu)	(cpu >> 5)
+#define cpu_to_node(cpu)	(__cpu_logical_map[cpu] >> 5)
 #define hardcpu_to_node(cpu)	(cpu >> 5)
 
 #define cpumask_of_node(node)	(NODE_CPU_MASK(node))
diff --git a/arch/mips/include/asm/netlogic/hal/nlm_hal.h b/arch/mips/include/asm/netlogic/hal/nlm_hal.h
index c8866c8..7ce3b6b 100644
--- a/arch/mips/include/asm/netlogic/hal/nlm_hal.h
+++ b/arch/mips/include/asm/netlogic/hal/nlm_hal.h
@@ -115,9 +115,11 @@ extern uint32_t efuse_cfg1(void);
 extern uint32_t efuse_cfg6(void);
 extern uint32_t get_proc_id(void);
 
-extern void nlm_hal_set_sae_engine_sel(void);
+extern void nlm_hal_set_rsa_cge(int node, int enable);
+extern void nlm_hal_set_sae_engine_sel(int node);
 extern void nlm_hal_set_rsa_engine_sel(void);
 extern void nlm_hal_set_sae_freq(int node, int freq);
+extern void nlm_hal_set_rsa_freq(int node, int freq);
 extern void nlm_hal_get_crypto_vc_nums(int *vcbase, int *vclimit);
 extern void nlm_hal_get_rsa_vc_nums(int *vcbase, int *vclimit);
 
diff --git a/arch/mips/include/asm/netlogic/hal/nlm_hal_nae.h b/arch/mips/include/asm/netlogic/hal/nlm_hal_nae.h
index f87323e..933b5e9 100644
--- a/arch/mips/include/asm/netlogic/hal/nlm_hal_nae.h
+++ b/arch/mips/include/asm/netlogic/hal/nlm_hal_nae.h
@@ -413,7 +413,7 @@ enum {
  * block     = BLOCK_7  (NAE Block)
  * intf_type = LINE_CFG (0xF)
  */
-extern int nlm_hal_mdio_reset(int node, int type, int bus, int block, int intf_type);
+extern int nlm_hal_mdio_reset(int node, int type, int bus);
 extern int nlm_hal_mdio_wr(int node, int type, int bus, int phyaddr, int regidx, uint16_t val);
 extern int nlm_hal_mdio_rd(int node, int type, int bus, int phyaddr, int regidx);
 extern int nlm_hal_mdio_read(int node, int type, int bus, int block, int intf_type, int phyaddr, int regidx);
@@ -475,6 +475,7 @@ extern void nlm_hal_mac_enable(int node, int inf, int type);
 extern uint16_t nlm_hal_get_hwport(int node, uint32_t context);
 extern int nlm_hal_set_xaui_framesize(int node, int block, uint32_t tx_size, uint32_t rx_size);
 extern int nlm_hal_set_sgmii_framesize(int node, int block, int index, uint32_t size);
+extern int nlm_hal_set_ilk_framesize(int node, int block, int port, uint32_t size);
 extern int nlm_config_vfbid_table(int node, uint32_t start, uint32_t num_entries, uint32_t *vfbid_tbl);
 #endif //__ASSEMBLY__
 
diff --git a/arch/mips/include/asm/netlogic/hal/nlm_hal_sys.h b/arch/mips/include/asm/netlogic/hal/nlm_hal_sys.h
index c780243..aa52a06 100644
--- a/arch/mips/include/asm/netlogic/hal/nlm_hal_sys.h
+++ b/arch/mips/include/asm/netlogic/hal/nlm_hal_sys.h
@@ -38,26 +38,38 @@ THE POSSIBILITY OF SUCH DAMAGE.
 /* 1 MHz resolution for frequency setting */
 #define FREQ_RESOLUTION 1000000ULL
 
-enum soc_dfs_device {
-	DFS_DEVICE_NAE_2X = 1,
-	DFS_DEVICE_SAE,
-	DFS_DEVICE_RSA,
-	DFS_DEVICE_DTRE,
-	DFS_DEVICE_CMP,
-	DFS_DEVICE_KBP,
-	DFS_DEVICE_DMC,
-	DFS_DEVICE_NAND,
-	DFS_DEVICE_MMC,
-	DFS_DEVICE_NOR,
-	DFS_DEVICE_CORE,
-	DFS_DEVICE_REGEX_SLOW,
-	DFS_DEVICE_REGEX_FAST,
-	DFS_DEVICE_SATA,
+/* System Clock Reg: Device:6, Func:5
+ * 0x56 Clock Disable Control
+ * 0x57 Clock Reset Control
+ * 0x58 Clock Bypass Control
+ * 0x59 Clock Divider Increment Control
+ * 0x5A clock Divider Decrement Control
+ */
+typedef enum soc_dfs_device {
+	DFS_DEVICE_NAE_2X = 0,
+	DFS_DEVICE_SAE    = 1,
+	DFS_DEVICE_RSA    = 2,
+	DFS_DEVICE_DTRE   = 3,
+	DFS_DEVICE_CMP    = 4, /*xlp8xx only*/
+	DFS_DEVICE_KBP    = 5, /*xlp8xx only*/
+	DFS_DEVICE_DMC    = 6,
+	DFS_DEVICE_NAND   = 7,
+	DFS_DEVICE_MMC    = 8,
+	DFS_DEVICE_NOR    = 9,
+	DFS_DEVICE_CORE   = 10,
+	DFS_DEVICE_REGEX_SLOW    = 11, /*xlp3xx only*/
+	DFS_DEVICE_REGEX_FAST    = 12, /*xlp3xx only*/
+	DFS_DEVICE_SATA          = 13, /*xlp3xx only*/
 	INVALID_DFS_DEVICE = 0xFF
-};
+} soc_device_id_t;
 
-extern uint64_t nlm_hal_get_soc_freq(int node, enum soc_dfs_device device);
-extern uint64_t nlm_hal_set_soc_freq(int node, enum soc_dfs_device device, uint64_t freq);
+extern uint8_t nlm_hal_get_soc_clock_state(int node, soc_device_id_t device);
+extern void nlm_hal_soc_clock_enable(int node, soc_device_id_t device);
+extern void nlm_hal_soc_clock_disable(int node, soc_device_id_t device);
+extern void nlm_hal_soc_clock_reset(int node, soc_device_id_t device);
+
+extern uint64_t nlm_hal_get_soc_freq(int node, soc_device_id_t device);
+extern uint64_t nlm_hal_set_soc_freq(int node, soc_device_id_t device, uint64_t freq);
 extern uint64_t nlm_hal_get_core_freq(int node, uint8_t core);
 extern uint64_t nlm_hal_set_core_freq(int node, uint8_t core, uint64_t freq);
 extern unsigned long long nlm_hal_cpu_freq(void);
diff --git a/arch/mips/include/asm/netlogic/hal/nlm_hal_xlp_dev.h b/arch/mips/include/asm/netlogic/hal/nlm_hal_xlp_dev.h
index 9525ee4..9206f33 100644
--- a/arch/mips/include/asm/netlogic/hal/nlm_hal_xlp_dev.h
+++ b/arch/mips/include/asm/netlogic/hal/nlm_hal_xlp_dev.h
@@ -32,6 +32,9 @@ THE POSSIBILITY OF SUCH DAMAGE.
 
 #define XLP_CACHELINE_SIZE	64
 
+#ifndef NLM_NCPUS_PER_NODE
+#define NLM_NCPUS_PER_NODE	32
+#endif
 /*
  * This File has all the XLP Device specific Defines
  */
@@ -105,6 +108,7 @@ THE POSSIBILITY OF SUCH DAMAGE.
 
 #define CPU_EXTPID_XLP_3XX_INV		0xFE /* invalid */
 #define CPU_EXTPID_XLP_3XX_ANY		0xFF /* Any 3XX */
+
 /*
  *    FMN
  */
@@ -280,6 +284,9 @@ THE POSSIBILITY OF SUCH DAMAGE.
 		
 
 #ifndef __ASSEMBLY__
+
+extern int is_nlm_xlp(unsigned int chipid, unsigned int rev,  unsigned int ext);
+
 /* Device Id: Bus[8:6], Dev[5:3], func[2:0] */
 
 enum sae_cfg_regs {
@@ -737,25 +744,19 @@ enum sys_cfg_regs {
 #define nlm_hal_write_sys_reg(node, index, val) \
         nlm_hal_write_32bit_reg((xlp_sys_base[node] + 0x100), (index), (val))
 
-#define nlm_hal_read_sae_reg(reg) \
-        nlm_hal_read_32bit_reg((xlp_sae_base), (reg))
-
-#define nlm_hal_write_sae_reg(reg, val) \
-        nlm_hal_write_32bit_reg ((xlp_sae_base), (reg), (val))
-
 #define nlm_hal_read_rsa_reg(reg) \
         nlm_hal_read_32bit_reg((xlp_rsa_base), (reg))
 
 #define nlm_hal_write_rsa_reg(reg, val) \
         nlm_hal_write_32bit_reg ((xlp_rsa_base), (reg), (val))
 
-#if 0
 #define nlm_hal_read_sae_reg(node, reg) \
         nlm_hal_read_32bit_reg((xlp_sae_base[node]), (reg))
 
 #define nlm_hal_write_sae_reg(node, reg, val) \
         nlm_hal_write_32bit_reg ((xlp_sae_base[node]), (reg), (val))
 
+#if 0
 #define nlm_hal_read_rsa_reg(node, reg) \
         nlm_hal_read_32bit_reg((xlp_rsa_base[node]), (reg))
 
@@ -1368,12 +1369,18 @@ enum {
 #define XLP_GPIO_INTEN21		0x4B	/* irt 148 40:32  */
 #define XLP_GPIO_INTEN30		0x4C	/* irt 149 31:0  */
 #define XLP_GPIO_INTEN31		0x4D	/* irt 149 40:32  */
-#define XLP_GPIO_INT_POLAR0		0x4E	/* int polarity	31:0   */
-#define XLP_GPIO_INT_POLAR1		0x4F	/* int polarity	40:32  */
-#define XLP_GPIO_INT_TYPE0		0x50	/* int level type 31:0   */
-#define XLP_GPIO_INT_TYPE1		0x51	/* int level type 40:32  */
-#define XLP_GPIO_INT_STAT0		0x52	/* int status 31:0   */
-#define XLP_GPIO_INT_STAT1		0x53	/* int status 40:32  */
+#define XLP_GPIO_INT_POLAR0		0x5E	/* int polarity	31:0   */
+#define XLP_GPIO_INT_POLAR1		0x5F	/* int polarity	40:32  */
+#define XLP_GPIO_INT_TYPE0		0x60	/* int level type 31:0   */
+#define XLP_GPIO_INT_TYPE1		0x61	/* int level type 40:32  */
+#define XLP_GPIO_INT_STAT0		0x62	/* int status 31:0   */
+#define XLP_GPIO_INT_STAT1		0x63	/* int status 40:32  */
+#define XLP_8XX_GPIO_INT_POLAR0		0x4E	/* int polarity	31:0   */
+#define XLP_8XX_GPIO_INT_POLAR1		0x4F	/* int polarity	40:32  */
+#define XLP_8XX_GPIO_INT_TYPE0		0x50	/* int level type 31:0   */
+#define XLP_8XX_GPIO_INT_TYPE1		0x51	/* int level type 40:32  */
+#define XLP_8XX_GPIO_INT_STAT0		0x52	/* int status 31:0   */
+#define XLP_8XX_GPIO_INT_STAT1		0x53	/* int status 40:32  */
 #define XLP_GPIO_INT0_IRT       	146
 #define XLP_GPIO_INT1_IRT       	147
 #define XLP_GPIO_INT2_IRT       	148
diff --git a/arch/mips/include/asm/netlogic/nlm_dma.h b/arch/mips/include/asm/netlogic/nlm_dma.h
index afacc42..ead7b54 100644
--- a/arch/mips/include/asm/netlogic/nlm_dma.h
+++ b/arch/mips/include/asm/netlogic/nlm_dma.h
@@ -24,7 +24,7 @@
 
 #ifdef CONFIG_NLM_XLP
 #include <asm/netlogic/iomap.h>
-#include <asm/netlogic/xlp_hal_pic.h>
+#include <asm/netlogic/xlp_irq.h>
 #include <hal/nlm_hal.h>
 #include <hal/nlm_hal_macros.h>
 #include <hal/nlm_hal_fmn.h>
diff --git a/arch/mips/include/asm/netlogic/xlp_hal_pic.h b/arch/mips/include/asm/netlogic/xlp_hal_pic.h
deleted file mode 100644
index 0534bd1..0000000
--- a/arch/mips/include/asm/netlogic/xlp_hal_pic.h
+++ /dev/null
@@ -1,290 +0,0 @@
-/***********************************************************************
-Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
-reserved.
-Redistribution and use in source and binary forms, with or without
-modification, are permitted provided that the following conditions are
-met:
-1. Redistributions of source code must retain the above copyright
-notice, this list of conditions and the following disclaimer.
-2. Redistributions in binary form must reproduce the above copyright
-notice, this list of conditions and the following disclaimer in
-the documentation and/or other materials provided with the
-distribution.
-THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
-ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
-IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
-PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
-FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
-CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
-SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
-INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
-CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
-ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
-THE POSSIBILITY OF SUCH DAMAGE.
-*****************************#NETL_2#********************************/
-
-#ifndef _NLM_HAL_PIC_H
-#define _NLM_HAL_PIC_H
-
-#include "nlm_hal.h"
-/*
- *     Register Offsets
- */
-#define PIC_CTRL             0x00
-#define PIC_BYTESWAP         0x01
-#define PIC_STATUS           0x02
-#define PIC_INT_TIMEOUT      0x03
-#define PIC_ICI0_INT_TIMEOUT 0x04
-#define PIC_ICI1_INT_TIMEOUT 0x05
-#define PIC_ICI2_INT_TIMEOUT 0x06
-#define PIC_IPI_CTL          0x07
-#define PIC_INT_ACK          0x08
-#define PIC_INT_PENDING0     0x09
-#define PIC_INT_PENDING1     0x0a
-#define PIC_INT_PENDING2     0x0b
-
-#define PIC_WD0_MAX_VAL      0x0c
-#define PIC_WD0_COUNT        0x0d
-#define PIC_WD0_MASK_0       0x0e
-#define PIC_WD0_MASK_1       0x0f
-#define PIC_WD0_HEARBEATCMD  0x10
-#define PIC_WD0_HEARBEAT_0   0x11
-#define PIC_WD0_HEARBEAT_1   0x12
-#define PIC_SYS_TIMER_0_COUNTER   0x22
-
-#define PIC_INT_THR_ENABLE_0_N01   0x2a
-#define PIC_INT_THR_ENABLE_0_N23   0x2b
-#define PIC_INT_THR_ENABLE_N01(id) (PIC_INT_THR_ENABLE_0_N12 + ((id) * 2))
-#define PIC_INT_THR_ENABLE_N23(id) (PIC_INT_THR_ENABLE_0_N12 + ((id) * 2))
-
-#define PIC_IRT_0   0x3a
-#define PIC_IRT(id) (PIC_IRT_0 + (id))
-#define PIC_CLOCK_TIMER     7
-#define ASM_XLP_IO_PIC_OFFSET        0xffffffffb8004100 /* TODO: This will change in to function */
-#define C_XLP_IO_PIC_OFFSET        0xffffffffb8004100ULL /* TODO: This will change in to function */
-#define XLP_IO_PIC_OFFSET        C_XLP_IO_PIC_OFFSET
-
-#ifndef __ASSEMBLY__
-#define __nlm_hal_set_irq_to_cpu	__nlm_hal_set_irt_to_cpu
-void __nlm_hal_set_irt_to_cpu(int, int);
-void __nlm_hal_release_irq(int);
-int __nlm_hal_request_irq(int, int);
-#define CPUIDBITS01(X) ((X) & 0x3)
-#define CPUIDBIT2(X) ((X >> 2) & 0x1)
-
-#if 0
-static inline int nlm_hal_irt_to_irq(int irt_num)
-{
-	return __nlm_hal_find_irt_from_irq(irt_num);
-}
-
-static inline int nlm_hal_irq_to_irt(int irq_num)
-{
-	int irt = __nlm_hal_find_irt_from_irq(irq_num); // same function
-	return irt;
-}
-#endif
-#define PIC_IRQ_IS_EDGE_TRIGGERED(irq) 0 // XLP interrupts are level triggered
-#define NODE_OFFSET(node) ((node) << 18)
-#define CPU_TO_NODE(cpu) ((cpu) >> 5)
-
-static __inline__ int nlm_hal_cpu_id(void)
-{
-	int cpu;
-
-	__asm__ __volatile__ (
-		".set push\n"
-		".set noreorder\n"
-		".set mips32\n"
-		"mfc0 %0, $15, 1\n"
-		"andi %0, %0, 0x3ff\n"
-		".set pop\n"
-		: "=r"(cpu)
-		);
-
-	return cpu;
-}
-
-typedef volatile unsigned long long pic_reg_t;
-
-static __inline__ pic_reg_t* nlm_hal_pic_offset(void)
-{
-	uint32_t cpu = nlm_hal_cpu_id();
-
-	return ((pic_reg_t *) (XLP_IO_PIC_OFFSET + NODE_OFFSET(CPU_TO_NODE(cpu))));
-}
-
-#ifdef CONFIG_64BIT
-
-static __inline__ void nlm_hal_write_pic_reg(pic_reg_t *base,
-		unsigned int offset, unsigned long long value)
-{
-	base[offset] = value;
-}
-
-static __inline__ unsigned long long nlm_hal_read_pic_reg(pic_reg_t *base,
-		unsigned int offset)
-{
-	return ((base)[offset]);
-}
-
-#else
-
-static __inline__ void nlm_hal_write_pic_reg(pic_reg_t *base, unsigned int offset, unsigned long long value)
-{
-        uint32_t lsw, msw;
-        uint64_t val;
-        uint32_t ls, ms;
-        unsigned long flags;
-
-        lsw = (uint32_t) (base+offset);
-        msw = (uint32_t) 0xffffffffUL;
-        val = (uint64_t)value;
-
-        ls = (uint32_t) (val & 0xffffffff);
-        ms = (uint32_t) (val >> 32);
-
-        enable_KX(flags);
-        __asm__ __volatile__(".set push\n"
-                        ".set noreorder\n"
-                        ".set mips64\n"
-                        ".set noat\n"
-                        "dsll32 $1, %2, 0\n"
-                        "dsll32 %1, 0\n"
-                        "dsrl32 %1, 0\n"
-                        "or $1, $1, %1\n"
-                        "dsll32 $8, %4, 0\n"
-                        "dsll32 %3, 0\n"
-                        "dsrl32 %3, 0\n"
-                        "or $8, $8, %3\n"
-                        "sd $8, 0($1) \n"
-                        ".set at\n"
-                        ".set pop\n"
-                        :
-                        :"r"(val), "r"(lsw), "r"(msw), "r"(ls), "r"(ms)
-                        :"$1", "$8");
-        disable_KX(flags);
-}
- 
-static __inline__ unsigned long long nlm_hal_read_pic_reg(pic_reg_t *base, unsigned int offset)
-{
-        uint32_t lsw, msw;
-        uint64_t value = 0;
-        uint32_t lo, hi;
-        unsigned long flags;
-
-        lsw = (uint32_t) (base+offset);
-        msw = (uint32_t) 0xffffffffUL;
-
-        enable_KX(flags);
-        __asm__ __volatile__(".set push\n"
-                        ".set noreorder\n"
-                        ".set mips64\n"
-                        ".set noat\n"
-                        "dsll32 $1, %3, 0\n"
-                        "dsll32 %2, 0\n"
-                        "dsrl32 %2, 0\n"
-                        "or $1, $1, %2\n"
-                        "ld $8, 0($1) \n"
-                        "dsrl32 %1, $8, 0\n"
-                        "dsll32 $8, $8, 0\n"
-                        "dsrl32 %0, $8, 0\n"
-                        ".set at\n"
-                        ".set pop\n"
-                        :"=r"(lo), "=r"(hi)
-                        :"r"(lsw), "r"(msw)
-                        :"$1", "$8");
-
-        disable_KX(flags);
-        value = hi;
-        value = (uint64_t) ((value<<32) | lo);
-        return (value);
-}
-#endif
-
-static __inline__ void nlm_hal_pic_send_ipi(int nmi, int vec, int node, int cpu)
-{
-	pic_reg_t *mmio = nlm_hal_pic_offset();
-
-	unsigned long long ipi = (nmi << 31) | (vec << 20) | (node << 17) | (1 << (cpu & 0xf));
-	if (cpu > 15) {
-		ipi |= 0x10000; // Setting bit 16 to select cpus 16-31
-	}
-	nlm_hal_write_pic_reg(mmio, PIC_IPI_CTL, ipi);
-}
-
-static __inline__ unsigned long long nlm_hal_pic_read_control(void)
-{
-	pic_reg_t *mmio = nlm_hal_pic_offset();
-
-	return nlm_hal_read_pic_reg(mmio, PIC_CTRL);
-}
-
-static __inline__ void nlm_hal_pic_write_control(unsigned long long control)
-{
-	pic_reg_t *mmio = nlm_hal_pic_offset();
-
-	nlm_hal_write_pic_reg(mmio, PIC_CTRL, control);
-}
-
-static __inline__ void nlm_hal_pic_update_control(unsigned long long control)
-{
-	pic_reg_t *mmio = nlm_hal_pic_offset();
-
-	nlm_hal_write_pic_reg(mmio, PIC_CTRL,
-			(control | nlm_hal_read_pic_reg(mmio, PIC_CTRL)));
-}
-
-static __inline__ void nlm_hal_ack_pic(int irt_num)
-{
-	pic_reg_t *mmio = nlm_hal_pic_offset();
-
-	nlm_hal_write_pic_reg(mmio, PIC_INT_ACK, irt_num);
-
-	/* Ack the Status register for Watchdog & System timers */
-	if (irt_num < 12) {
-		nlm_hal_write_pic_reg(mmio, PIC_STATUS, (1 << irt_num));
-	}
-}
-
-static __inline__ unsigned long long nlm_hal_pic_read_irt(int irt_num)
-{
-	pic_reg_t *mmio = nlm_hal_pic_offset();
-
-	return nlm_hal_read_pic_reg(mmio, PIC_IRT(irt_num));
-}
-
-static __inline__ void nlm_hal_pic_write_irt(int irt_num, int en, int nmi, int sch, int vec, int dt, int db, int dte)
-{
-	pic_reg_t *mmio = nlm_hal_pic_offset();
-
-	unsigned long long val = (((en & 0x1) << 31) | ((nmi & 0x1) << 29) | ((sch & 0x1) << 28) |
-				  ((vec & 0x3f) << 20) | ((dt & 0x1 ) << 19) | ((db & 0x7) << 16) |
-				  (dte & 0xffff));
-
-	nlm_hal_write_pic_reg(mmio, PIC_IRT(irt_num), val);
-}
-
-static __inline__ void nlm_hal_pic_write_irt_direct(int irt_num, int en, int nmi, int sch, int vec, int cpu)
-{
-	nlm_hal_pic_write_irt(irt_num, en, nmi, sch, vec, 1, CPUIDBIT2(cpu), CPUIDBITS01(cpu));
-	/* Does not support multi node support yet */
-}
-
-static __inline__ unsigned long long nlm_hal_pic_read_timer(int timer)
-{
-	pic_reg_t *mmio = nlm_hal_pic_offset();
-
-	return nlm_hal_read_pic_reg(mmio, PIC_SYS_TIMER_COUNTER(timer));
-}
-
-static __inline__ void nlm_hal_pic_write_timer(int timer, pic_reg_t value)
-{
-	pic_reg_t *mmio = nlm_hal_pic_offset();
-
-	nlm_hal_write_pic_reg(mmio, PIC_SYS_TIMER_COUNTER(timer), value);
-}
-
-#endif /* __ASSEMBLY__ */
-
-#endif /* _NLM_HAL_PIC_H */
diff --git a/arch/mips/include/asm/netlogic/xlp_ici.h b/arch/mips/include/asm/netlogic/xlp_ici.h
new file mode 100644
index 0000000..c7a4267
--- /dev/null
+++ b/arch/mips/include/asm/netlogic/xlp_ici.h
@@ -0,0 +1,184 @@
+/*
+ * Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+ * reserved. Redistribution and use in source and binary forms, with 
+ * or without modification, are permitted provided that the following 
+ * conditions are met:
+ *
+ *	1. 	Redistributions of source code must retain the above copyright
+ *		notice, this list of conditions and the following disclaimer.
+ *
+ *	2. 	Redistributions in binary form must reproduce the above copyright
+ *		notice, this list of conditions and the following disclaimer in
+ *		the documentation and/or other materials provided with the
+ *		distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+ * THE POSSIBILITY OF SUCH DAMAGE.
+ * --------------------------------#NETL_2#--------------------------------
+ */
+#ifndef __XLP_ICI_H__
+#define __XLP_ICI_H__
+
+#ifndef __ASSEMBLY__
+
+#define ICI_NUM_LINKS 3
+#define ICI_RX_LANE_SYNC(x) ((x&0xff)==0xff)
+#define ICI_RX_LANE_ALIGN(x) ((x&(1<<18))!=0)
+#define ICI_SLAVE_LINKUP(x) ((x&(1<<9))!=0)
+#define ICI_CREDLD_SNDPRB  (1<<1)
+#define ICI_CREDLD_PRBACK  (1<<2)
+#define ICI_CREDLD_SNDNODE (1<<7)
+#define ICI_CREDLD_NODERCV (1<<9)
+#define ICI_CREDLD_PEERID(x) (x<<4)
+#define ICI_LINKUP         (1<<6)
+#define ICI_SFEN           (1<<10)
+#define ICI_CDRRSTEN       (1<<11) 
+#define ICI_CDRLARXRSTEN   (1<<12)
+
+#define ICI_MAX_VC_NUM		16
+#define ICI_GCU_VC_START	0
+#define ICI_GCU_VC_END		7
+#define ICI_FMN_VC_START	10
+#define ICI_FMN_VC_END		13
+#define ICI_PIC_VC_START	14
+#define ICI_PIC_VC_END		14
+#define ICI_MAX_GCU_VC		8
+#define ICI_MAX_FMN_VC		4
+#define ICI_MAX_PIC_VC		1
+
+enum processor_ici
+{
+	ICI_CTL = 0,
+	ICI_RTRY_TIMER = 1,
+	ICI_ICIINT = 2,
+	ICI_ICIINTMASK = 3,
+	ICI_CRED_RT_TH0 = 4,
+	ICI_CRED_RT_TH1 = 5,
+	ICI_CRED_RT_TH2 = 6,
+	ICI_CRED_RT_TH3 = 7,
+	ICI_TXWGHT0 = 8,
+	ICI_TXWGHT1 = 9,
+	ICI_TXWGHT2 = 10,
+	ICI_TXWGHT3 = 11,
+	ICI_TXWGHT4 = 12,
+	ICI_TXWGHT5 = 13,
+	ICI_TXWGHT6 = 14,
+	ICI_TXWGHT7 = 15,
+	ICI_TXWGHT8 = 16,
+	ICI_TXWGHT9 = 17,
+	ICI_TXWGHT10 = 18,
+	ICI_TXWGHT11 = 19,
+	ICI_TXWGHT12 = 20,
+	ICI_TXWGHT13 = 21,
+	ICI_TXWGHT14 = 22,
+	ICI_TXSEGTH0 = 23,
+	ICI_TXSEGTH1 = 24,
+	ICI_TXSEGTH2 = 25,
+	ICI_TXSEGTH3 = 26,
+	ICI_TXSEGTH4 = 27,
+	ICI_TXSEGTH5 = 28,
+	ICI_TXSEGTH6 = 29,
+	ICI_TXSEGTH7 = 30,
+	ICI_TXSEGTH8 = 31,
+	ICI_TXSEGTH9 = 32,
+	ICI_TXSEGTH10 = 33,
+	ICI_TXSEGTH11 = 34,
+	ICI_TXSEGTH12 = 35,
+	ICI_TXSEGTH13 = 36,
+	ICI_TXSEGTH14 = 37,
+	ICI_CREDSHARE0 = 38,
+	ICI_CREDSHARE1 = 39,
+	ICI_CREDSHARE2 = 40,
+	ICI_CREDSHARE3 = 41,
+	ICI_CREDSHARE4 = 42,
+	ICI_CREDSHARE5 = 43,
+	ICI_CREDSHARE6 = 44,
+	ICI_CREDSHARE7 = 45,
+	ICI_CREDSHARE8 = 46,
+	ICI_CREDSHARE9 = 47,
+	ICI_CREDSHARE10 = 48,
+	ICI_CREDSHARE11 = 49,
+	ICI_CREDSHARE12 = 50,
+	ICI_CREDSHARE13 = 51,
+	ICI_CREDSHARE14 = 52,
+	ICI_CREDOWN0 = 53,
+	ICI_CREDOWN1 = 54,
+	ICI_CREDOWN2 = 55,
+	ICI_CREDOWN3 = 56,
+	ICI_CREDOWN4 = 57,
+	ICI_CREDOWN5 = 58,
+	ICI_CREDOWN6 = 59,
+	ICI_CREDOWN7 = 60,
+	ICI_CREDOWN8 = 61,
+	ICI_CREDOWN9 = 62,
+	ICI_CREDOWN10 = 63,
+	ICI_CREDOWN11 = 64,
+	ICI_CREDOWN12 = 65,
+	ICI_CREDOWN13 = 66,
+	ICI_CREDOWN14 = 67,
+	ICI_CREDTOT = 68,
+	ICI_CREDLD = 69,
+	ICI_SPEPKTCNT = 70,
+	ICI_CRED_TOTRd = 71,
+	ICI_CREDPERVC0 = 72,
+	ICI_CREDPERVC1 = 73,
+	ICI_CREDPERVC2 = 74,
+	ICI_CREDPERVC3 = 75,
+	ICI_CREDPERVC4 = 76,
+	ICI_CREDPERVC5 = 77,
+	ICI_CREDPERVC6 = 78,
+	ICI_CREDPERVC7 = 79,
+	ICI_CREDPERVC8 = 80,
+	ICI_CREDPERVC9 = 81,
+	ICI_CREDPERVC10 = 82,
+	ICI_CREDPERVC11 = 83,
+	ICI_CREDPERVC12 = 84,
+	ICI_CREDPERVC13 = 85,
+	ICI_CREDPERVC14 = 86,
+	ICI_TXFREELIST = 87,
+	ICI_RXFREELIST0 = 88,
+	ICI_RXFREELIST1 = 89,
+	ICI_RXFREELIST2 = 90,
+	ICI_RXFREELIST3 = 91,
+	ICI_ECC_LOG = 92,
+	ICI_ECC_TRIG = 93,
+	ICI_CNT_CTL = 94,
+	ICI_PKT_CNT_CTL = 95,
+	ICI_PKT_TX_CNT = 96,
+	ICI_PKT_RX_CNT = 97,
+	ICI_TEST_CTL = 98,
+	ICI_TX_TEST_HEAD = 99,
+	ICI_TX_TEST_DATA0_L = 100,
+	ICI_TX_TEST_DATA0_H = 101,
+	ICI_RX_TEST_HEAD = 102,
+	ICI_RX_TEST_DATA0_L = 103,
+	ICI_RX_TEST_DATA0_H = 104,
+	ICI_LA_INT_REG = 105,
+	ICI_LA_INTMASK = 106,
+	ICI_LA_RX_DIAG = 107,
+	ICI_LA_RX_SYNC = 108,
+	ICI_LA_ERROR_REG1 = 109,
+	ICI_LA_ERROR_REG2 = 110,
+	ICI_LA_TX_ENABLE = 111,
+	ICI_LA_BAD_LANE = 112,
+	ICI_LA_TX_STAT = 113,
+	ICI_LA_TX_RLIM = 114,
+	ICI_LA_SETUP = 115,
+	ICI_LA_LEN = 116,
+	ICI_SER_CTL = 117,
+	ICI_SER_REG0 = 118,
+	ICI_SER_REG_ALL = 119,
+	ICI_IO_CONFIG_SWAP_DIS = 120
+};
+#endif
+
+#endif /* __XLP_ICI_H__ */
diff --git a/arch/mips/include/asm/netlogic/xlp_irq.h b/arch/mips/include/asm/netlogic/xlp_irq.h
index 2f98892..3c29059 100644
--- a/arch/mips/include/asm/netlogic/xlp_irq.h
+++ b/arch/mips/include/asm/netlogic/xlp_irq.h
@@ -26,161 +26,246 @@ THE POSSIBILITY OF SUCH DAMAGE.
 #ifndef _ASM_NLM_XLP_IRQ_H
 #define _ASM_NLM_XLP_IRQ_H
 
-#include <asm/netlogic/pic.h>
-
-/* Defines for the IRQ numbers */
-/* We define NR_IRQs to be 254, but IRT entries are 160 in size
- * Effectively, we cannot use anything more than 159 */
-#define NR_IRQS			384
-/* Maximum IRQ vector numbers supported by MIPS */
-#define XLP_EIRR_SIZE		64
-#define XLP_IRT_NUM	160
-#define XLP_IRQ_MAX	168	/* 0-7 are reserved + 160 IRT entries */
+/* This is the de-facto rvec bit associated with count compare timer */
+#define XLP_IRQ_TIMER_RVEC		7
+#define XLP_PIC_SYSTIMER_RVEC		26
+#define XLP_PIC_TIMERS_RVEC		10
+#define XLP_PIC_WATCHDOG_TIMERS_RVEC    9
 
 /* The following interrupt assignments (0-7) are special.
  * I need to find out what governs these assignments
  * XXX
  */
-#define fdebug(fmt,arg...)\
-	printk(KERN_DEBUG "%s:%d " fmt, __FILE__, __LINE__, ##arg)
 
-#define XLP_IRQ_DUMMY_UART           2
-#define XLP_IRQ_IPI_SMP_FUNCTION     3
-#define XLP_IRQ_IPI_SMP_RESCHEDULE   4
-// #define XLP_IRQ_REMOTE_DEBUG         5
-#define XLP_IRQ_MSGRING              5
-#define XLP_IRQ_OPROFILE             6
-#define XLP_IRQ_TIMER                7
-#define XLP_IRQ_RESERVED_MAX		8
+#define XLP_IRQ_DUMMY_UART_RVEC           2
+#define XLP_IRQ_IPI_SMP_FUNCTION_RVEC     3
+#define XLP_IRQ_IPI_SMP_RESCHEDULE_RVEC   4
+#define XLP_IRQ_MSGRING_RVEC              5
 
-#define XLP_IRQ_IPI_SMP_KGDB	     50
+#define XLP_IRQ_IPI_SMP_KGDB_RVEC	     50
+#define XLP_IRQ_OPROFILE    	  6
+#define PIC_IRQ_IS_EDGE_TRIGGERED(x)	0
 
 #ifdef CONFIG_NLMCOMMON_IP_FLOW_AFFINITY
-#define XLP_IRQ_IPI_NETRX		49
-#define SMP_NETRX_IPI			32
+#define XLP_IRQ_IPI_NETRX_RVEC		49
+#define SMP_NETRX_IPI_RVEC			32
 #endif /* CONFIG_NLMCOMMON_IP_FLOW_AFFINITY */
 
 /* if you want some common #defines, please do it here */
-#define NLM_IRQ_DUMMY_UART		XLP_IRQ_DUMMY_UART
-#define NLM_IRQ_IPI_SMP_FUNCTION	XLP_IRQ_IPI_SMP_FUNCTION
-#define NLM_IRQ_IPI_SMP_RESCHEDULE	XLP_IRQ_IPI_SMP_RESCHEDULE
-#define NLM_IRQ_REMOTE_DEBUG		XLP_IRQ_REMOTE_DEBUG
-#define NLM_IRQ_MSGRING			XLP_IRQ_MSGRING
-#define NLM_IRQ_TIMER			XLP_IRQ_TIMER
-#define NLM_IRQ_IPI_SMP_KGDB		XLP_IRQ_IPI_SMP_KGDB
+#define NLM_IRQ_DUMMY_UART		XLP_IRQ_DUMMY_UART_RVEC
+#define NLM_IRQ_IPI_SMP_FUNCTION	XLP_IRQ_IPI_SMP_FUNCTION_RVEC
+#define NLM_IRQ_IPI_SMP_RESCHEDULE	XLP_IRQ_IPI_SMP_RESCHEDULE_RVEC
+#define NLM_IRQ_REMOTE_DEBUG		XLP_IRQ_REMOTE_DEBUG_RVEC
+#define NLM_IRQ_MSGRING			XLP_IRQ_MSGRING_RVEC
+#define NLM_IRQ_IPI_SMP_KGDB		XLP_IRQ_IPI_SMP_KGDB_RVEC
 
 /* These are flags required for SMP
  * Not XLP specifc -- possibly mips specific.
  * Need to move out XXX
  */
-#define SMP_CALL_KGDB_HOOK	8
-#define SMP_OPROFILE_IPI        16
+#define SMP_CALL_KGDB_HOOK_RVEC	8
+#define SMP_OPROFILE_IPI_RVEC        16
 
-#define TIMER_CYCLES_MAXVAL        0xffffffffffffffffULL
+#if defined __ASSEMBLY__
+#define ASM_XLP_IO_PIC_OFFSET        0xffffffffb8004100 /* TODO: This will change in to function */
+#define C_XLP_IO_PIC_OFFSET        0xffffffffb8004100ULL /* TODO: This will change in to function */
+#define XLP_IO_PIC_OFFSET        C_XLP_IO_PIC_OFFSET
 
+#else
+#include <asm/netlogic/pic.h>
 /*
- *    IRT Map
+ *     Register Offsets
  */
+#define XLP_PIC_CTRL             0x40
+#define XLP_PIC_BYTESWAP         0x42
+#define XLP_PIC_STATUS           0x44
+#define XLP_PIC_INT_TIMEOUT      0x46
+#define XLP_PIC_ICI0_INT_TIMEOUT 0x48
+#define XLP_PIC_ICI1_INT_TIMEOUT 0x4A
+#define XLP_PIC_ICI2_INT_TIMEOUT 0x4C
+#define XLP_PIC_IPI_CTL          0x4E
+#define XLP_PIC_INT_ACK          0x50
+#define XLP_PIC_INT_PENDING0     0x52
+#define XLP_PIC_INT_PENDING1     0x54
+#define XLP_PIC_INT_PENDING2     0x56
+
+#define XLP_PIC_WD_MAXVAL(x)		(0x58 + (0xE) *(x))
+#define XLP_PIC_WD_COUNT(x)		(0x5A + (0xE) *(x))
+#define XLP_PIC_WD_THREN0(x)		(0x5C + (0xE) *(x))
+#define XLP_PIC_WD_THREN1(x)		(0x5E + (0xE) *(x))
+#define XLP_PIC_WD_BEATCMD(x)		(0x60 + (0xE) *(x))
+#define XLP_PIC_WD_HB1(x)		(0x62 + (0xE) *(x))
+#define XLP_PIC_WD_HB2(x)		(0x64 + (0xE) *(x))
+
+#define XLP_PIC_SYSTIMER_MAXVAL(x)	(0x74 + ((x) << 1))
+#define XLP_PIC_SYSTIMER_COUNT(x)	(0x84 + ((x) << 1))
+#define XLP_PIC_INT_THREADEN01(x)	(0x94 + ((x) << 2))
+#define XLP_PIC_INT_THREADEN23(x)	(0x96 + ((x) << 2))
+#define XLP_PIC_IRT_ENTRY(x)		(0xB4 + ((x) << 1))
+#define XLP_IRQ_RESERVED_MAX		8
+
+#define XLP_ITE_ENTRIES		8
+#define NLM_MAX_CPU_NODE	4
+#define XLP_8XX_MAX_CPUS	(NLM_MAX_CPU_NODE * NLM_MAX_CPU_PER_NODE)
+
+#define XLP_IRTENT_ENABLE	(1ULL << 31)
+#define XLP_IRTENT_NMI		(1ULL << 29)
+#define XLP_IRTENT_SCH_LCL	(1ULL << 28)
+#define XLP_IRTENT_RVEC(x)	(((x) & 0x3fULL) << 20)
+#define XLP_IRTENT_DT		(1ULL << 19)
+#define XLP_IRTENT_DB(x)	((x & 7) << 16)
+#define XLP_IRTENT_DTE(x)	((x) & 0xffff)
+
+#define fdebug(fmt,arg...)\
+	printk(KERN_DEBUG "%s:%d " fmt, __FILE__, __LINE__, ##arg)
+#define __nlm_hal_set_irq_to_cpu	__nlm_hal_set_irt_to_cpu
+void __nlm_hal_set_irt_to_cpu(int, int);
+void __nlm_hal_release_irq(u8, int);
+int __nlm_hal_request_irq(u8, int, int);
+struct xlp_nodefn_struct {
+	/* Structure to distinguish the controller function */
+	u8 node;
+	u8 fn;
+};
+
+#define XLP_PIT_TICK_RATE	133333333
+#define XLP_PIT_TIMER_MAX	(u64)(~0ULL)
+
+extern u64 __nlh_pic_r64o(u8, u64);
+extern void __nlh_pic_w64o(u8, u64, u64);
+extern u64 __nlh_pci_r64o(u8, u64);
+extern void __nlh_pci_w64o(u8, u64, u64);
+extern void xlp_ack_pic(u8, int);
+void nlm_hal_pic_send_ipi(int nmi, int vec, int node, int cpu);
+
+#if !defined CONFIG_XLP_REPLACE_R4K_TIMER
+void nlm_hal_pic_update_control(u64);
+#endif
+
+#define nlh_pic_r64r(nid, reg)	__nlh_pic_r64o(nid, (reg << 2))
+#define nlh_pic_w64r(nid, reg, val) __nlh_pic_w64o(nid, (reg << 2), val)
+/* Note PCI should take function into account, PIC has only one function */
+#define nlh_pci_r32r(nid, fn, reg)	__nlh_pci_r32o(nid, fn, (reg << 2))
+#define nlh_pci_w32r(nid, fn, reg, val) __nlh_pci_w32o(nid, fn, (reg << 2), val)
 
+void xlp_ite_cpu_op(u8 node, u8 cpu, u8 ite, u8 bitval);
+#define xlp_ite_cpu_set(node,cpu,ite) xlp_ite_cpu_op(node,cpu,ite,1)
+#define xlp_ite_cpu_clear(node,cpu,ite) xlp_ite_cpu_op(node,cpu,ite,0)
+
+int xlp_rvec_from_irq(int irq);
+
+/* We define NR_IRQs to be 254, but IRT entries are 160 in size
+ * Effectively, we cannot use anything more than 159 */
+#define XLP_IRQS_PER_NODE	384
+#define NR_IRQS			(XLP_IRQS_PER_NODE * NLM_MAX_CPU_NODE)
+/* Maximum IRQ vector numbers supported by MIPS */
+#define XLP_EIRR_SIZE		64
+#define XLP_IRT_NUM	160
+#define XLP_IRQ_MAX	168	/* 0-7 are reserved + 160 IRT entries */
+
+/*
+ *    IRT Map
+ */
 #define arch_setup_msi_irqs	arch_setup_msi_irqs /* defines arch. specific msi setup function */
-#define xlp_irq_to_irt(x)	((x) - XLP_IRQ_RESERVED_MAX)
-#define xlp_irt_to_irq(x)	((x) + XLP_IRQ_RESERVED_MAX)
+#define xlp_irq_to_irt(x) (((x) % XLP_IRQS_PER_NODE) - XLP_IRQ_RESERVED_MAX)
+#define xlp_irt_to_irq(n,x)\
+	(((n) * XLP_IRQS_PER_NODE + XLP_IRQ_RESERVED_MAX + (x)))
+#define XLP_WD_BASE(n)		(XLP_IRQ_RESERVED_MAX + (n * XLP_IRQS_PER_NODE))
 
-#define XLP_WD_BASE			(0 + XLP_IRQ_RESERVED_MAX)
-#define XLP_WD_IRQ_IRQ(x) (XLP_WD_BASE + (x))
+#define XLP_WD_IRT_OFFSET(n)            (XLP_WD_BASE(n))
+#define XLP_WD_IRQ(n,x)                 (XLP_WD_IRT_OFFSET(n) + (x))
 
-#define XLP_WD_NMI_IRT_OFFSET		(2 + XLP_IRQ_RESERVED_MAX)
-#define XLP_WD_NMI_IRQ(x) (XLP_WD_NMI_IRT_OFFSET + (x))
+#define XLP_WD_NMI_IRT_OFFSET(n)	(2 + XLP_WD_BASE(n))
+#define XLP_WD_NMI_IRQ(n,x)		(XLP_WD_NMI_IRT_OFFSET(n) + (x))
 
-#define XLP_TIMER_IRT_OFFSET		(4 + XLP_IRQ_RESERVED_MAX)
-#define XLP_TIMER_IRQ(x)	(XLP_TIMER_IRT_OFFSET + (x))
+#define XLP_TIMER_IRT_OFFSET(n)		(4 + XLP_WD_BASE(n))
+#define XLP_TIMER_IRQ(n,x)		(XLP_TIMER_IRT_OFFSET(n) + (x))
 
-#define XLP_MSGQ_IRT_OFFSET			(12 + XLP_IRQ_RESERVED_MAX)
-#define XLP_MSGQ_IRQ(x)	(XLP_MSGQ_IRT_OFFSET + (x))
+#define XLP_MSGQ_IRT_OFFSET(n)		(12 + XLP_WD_BASE(n))
+#define XLP_MSGQ_IRQ(n,x)		(XLP_MSGQ_IRT_OFFSET(n) + (x))
 
-#define XLP_MSG_IRT_OFFSET			(44 + XLP_IRQ_RESERVED_MAX)
-#define XLP_MSG_IRQ(x)	(XLP_MSG_IRT_OFFSET + (x))
+#define XLP_MSG_IRT_OFFSET(n)		(44 + XLP_WD_BASE(n))
+#define XLP_MSG_IRQ(n,x)		(XLP_MSG_IRT_OFFSET(n) + (x))
 
-#define XLP_PCIE_MSIX_IRT_OFFSET		(46 + XLP_IRQ_RESERVED_MAX)
-#define XLP_PCIE_MSIX_IRQ(x)	(XLP_PCIE_MSIX_IRT_OFFSET + (x))
+#define XLP_PCIE_MSIX_IRT_OFFSET(n)	(46 + XLP_WD_BASE(n))
+#define XLP_PCIE_MSIX_IRQ(n,x)		(XLP_PCIE_MSIX_IRT_OFFSET(n) + (x))
 
-#define XLP_PCIE_LINK_IRT_OFFSET		(78 + XLP_IRQ_RESERVED_MAX)
-#define XLP_PCIE_LINK_IRQ(x)	(XLP_PCIE_LINK_IRT_OFFSET + (x))
+#define XLP_PCIE_LINK_IRT_OFFSET(n)	(78 + XLP_WD_BASE(n))
+#define XLP_PCIE_LINK_IRQ(n,x)		(XLP_PCIE_LINK_IRT_OFFSET(n) + (x))
 
-#define XLP_NAE_IRT_OFFSET			(82 + XLP_IRQ_RESERVED_MAX)
-#define XLP_XLP_NAE_IRQ(x)	(XLP_XLP_NAE_IRT_OFFSET + (x))
+#define XLP_NAE_IRT_OFFSET(n)		(82 + XLP_WD_BASE(n))
+#define XLP_XLP_NAE_IRQ(n,x)		(XLP_XLP_NAE_IRT_OFFSET(n) + (x))
 
-#define XLP_POE_IRT_OFFSET			(114 + XLP_IRQ_RESERVED_MAX)
-#define XLP_POE_IRQ(x)	(XLP_POE_IRT_OFFSET + (x))
+#define XLP_POE_IRT_OFFSET(n)		(114 + XLP_WD_BASE(n))
+#define XLP_POE_IRQ(n,x)		(XLP_POE_IRT_OFFSET(n) + (x))
 
-#define XLP_USB_IRT_OFFSET			(115 + XLP_IRQ_RESERVED_MAX)
-#define XLP_USB_IRQ(x)	(XLP_USB_IRT_OFFSET + (x))
+#define XLP_USB_IRT_OFFSET(n)		(115 + XLP_WD_BASE(n))
+#define XLP_USB_IRQ(n,x)		(XLP_USB_IRT_OFFSET(n) + (x))
 
-#define XLP_DTR_IRT_OFFSET			(121 + XLP_IRQ_RESERVED_MAX)
-#define XLP_DTR_IRQ(x)	(XLP_DTR_IRT_OFFSET + (x))
+#define XLP_DTR_IRT_OFFSET(n)		(121 + XLP_WD_BASE(n))
+#define XLP_DTR_IRQ(n,x)		(XLP_DTR_IRT_OFFSET(n) + (x))
 
-#define XLP_SAE_IRT_OFFSET			(122 + XLP_IRQ_RESERVED_MAX)
-#define XLP_SAE_IRQ(x)	(XLP_SAE_IRT_OFFSET + (x))
+#define XLP_SAE_IRT_OFFSET(n)		(122 + XLP_WD_BASE(n))
+#define XLP_SAE_IRQ(n,x)		(XLP_SAE_IRT_OFFSET(n) + (x))
 
-#define XLP_RSA_IRT_OFFSET			(123 + XLP_IRQ_RESERVED_MAX)
-#define XLP_RSA_IRQ(x)	(XLP_RSA_IRT_OFFSET + (x))
+#define XLP_RSA_IRT_OFFSET(n)		(123 + XLP_WD_BASE(n))
+#define XLP_RSA_IRQ(n,x)		(XLP_RSA_IRT_OFFSET(n) + (x))
 
-#define XLP_COMP_IRT_OFFSET			(124 + XLP_IRQ_RESERVED_MAX)
-#define XLP_COMP_IRQ(x)	(XLP_COMP_IRT_OFFSET + (x))
+#define XLP_COMP_IRT_OFFSET(n)		(124 + XLP_WD_BASE(n))
+#define XLP_COMP_IRQ(n,x)		(XLP_COMP_IRT_OFFSET(n) + (x))
 
-#define XLP_FLASH_IRT_OFFSET		(128 + XLP_IRQ_RESERVED_MAX)
-#define XLP_FLASH_IRQ(x)	(XLP_FLASH_IRT_OFFSET + (x))
+#define XLP_FLASH_IRT_OFFSET(n)		(128 + XLP_WD_BASE(n))
+#define XLP_FLASH_IRQ(n,x)		(XLP_FLASH_IRT_OFFSET(n) + (x))
 
-#define	XLP_ICI_IRT_OFFSET			(131 + XLP_IRQ_RESERVED_MAX)
-#define XLP_ICI_IRQ(x)	(XLP_ICI_IRT_OFFSET + (x))
+#define	XLP_ICI_IRT_OFFSET(n)		(131 + XLP_WD_BASE(n))
+#define XLP_ICI_IRQ(n,x)		(XLP_ICI_IRT_OFFSET(n) + (x))
 
-#define	XLP_KBP_IRT_OFFSET			(132 + XLP_IRQ_RESERVED_MAX)
-#define XLP_KBP_IRQ(x)	(XLP_KBP_IRT_OFFSET + (x))
+#define	XLP_KBP_IRT_OFFSET(n)		(132 + XLP_WD_BASE(n))
+#define XLP_KBP_IRQ(n,x)		(XLP_KBP_IRT_OFFSET(n) + (x))
 
-#define XLP_UART_IRT_OFFSET			(133 + XLP_IRQ_RESERVED_MAX)
-#define XLP_UART_IRQ(x)	(XLP_UART_IRT_OFFSET + (x))
+#define XLP_UART_IRT_OFFSET(n)		(133 + XLP_WD_BASE(n))
+#define XLP_UART_IRQ(n,x)		(XLP_UART_IRT_OFFSET(n) + (x))
 
-#define XLP_I2C_IRT_OFFSET			(135 + XLP_IRQ_RESERVED_MAX)
-#define XLP_I2C_IRQ(x)	(XLP_I2C_IRT_OFFSET + (x))
+#define XLP_I2C_IRT_OFFSET(n)		(135 + XLP_WD_BASE(n))
+#define XLP_I2C_IRQ(n,x)		(XLP_I2C_IRT_OFFSET(n) + (x))
 
-#define XLP_SM_IRT_OFFSET			(137 + XLP_IRQ_RESERVED_MAX)
-#define XLP_SM_IRQ(x)	(XLP_SM_IRT_OFFSET + (x))
+#define XLP_SM_IRT_OFFSET(n)		(137 + XLP_WD_BASE(n))
+#define XLP_SM_IRQ(n,x)			(XLP_SM_IRT_OFFSET(n) + (x))
 
-#define	XLP_JTAG_IRT_OFFSET			(139 + XLP_IRQ_RESERVED_MAX)
-#define XLP_JTAG_IRQ(x)	(XLP_JTAG_IRT_OFFSET + (x))
+#define	XLP_JTAG_IRT_OFFSET(n)		(139 + XLP_WD_BASE(n))
+#define XLP_JTAG_IRQ(n,x)		(XLP_JTAG_IRT_OFFSET(n) + (x))
 
-#define XLP_PIC_IRT_OFFSET			(140 + XLP_IRQ_RESERVED_MAX)
-#define XLP_PIC_IRQ(x)	(XLP_PIC_IRT_OFFSET + (x))
+#define XLP_PIC_IRT_OFFSET(n)		(140 + XLP_WD_BASE(n))
+#define XLP_PIC_IRQ(n,x)		(XLP_PIC_IRT_OFFSET(n) + (x))
 
-#define XLP_MIOCB_IRT_OFFSET		(141 + XLP_IRQ_RESERVED_MAX)
-#define XLP_MIOCB_IRQ(x)	(XLP_MIOCB_IRT_OFFSET + (x))
+#define XLP_MIOCB_IRT_OFFSET(n)		(141 + XLP_WD_BASE(n))
+#define XLP_MIOCB_IRQ(n,x)		(XLP_MIOCB_IRT_OFFSET(n) + (x))
 
-#define XLP_TCU_IRT_OFFSET			(142 + XLP_IRQ_RESERVED_MAX)
-#define XLP_TCU_IRQ(x)	(XLP_TCU_IRT_OFFSET + (x))
+#define XLP_TCU_IRT_OFFSET(n)		(142 + XLP_WD_BASE(n))
+#define XLP_TCU_IRQ(n,x)		(XLP_TCU_IRT_OFFSET(n) + (x))
 
-#define XLP_GCU_IRT_OFFSET			(143 + XLP_IRQ_RESERVED_MAX)
-#define XLP_GCU_IRQ(x)	(XLP_GCU_IRT_OFFSET + (x))
+#define XLP_GCU_IRT_OFFSET(n)		(143 + XLP_WD_BASE(n))
+#define XLP_GCU_IRQ(n,x)		(XLP_GCU_IRT_OFFSET(n) + (x))
 
-#define XLP_SATA_IRT_OFFSET			(143 + XLP_IRQ_RESERVED_MAX)
-#define XLP_SATA_IRQ	(XLP_SATA_IRT_OFFSET)
+#define XLP_DRAM_IRT_OFFSET(n)		(144 + XLP_WD_BASE(n))
+#define XLP_DRAM_IRQ(n,x)		(XLP_DRAM_IRT_OFFSET(n) + (x))
 
-#define XLP_DRAM_IRT_OFFSET			(144 + XLP_IRQ_RESERVED_MAX)
-#define XLP_DRAM_IRQ(x)	(XLP_DRAM_IRT_OFFSET + (x))
+#define XLP_GPIO_IRT_OFFSET(n)		(146 + XLP_WD_BASE(n))
+#define XLP_GPIO_IRQ(n,x)		(XLP_GPIO_IRT_OFFSET(n) + (x))
 
-#define XLP_GPIO_IRT_OFFSET			(146 + XLP_IRQ_RESERVED_MAX)
-#define XLP_GPIO_IRQ(x)	(XLP_GPIO_IRT_OFFSET + (x))
+#define XLP_NOR_IRT_OFFSET(n)		(150 + XLP_WD_BASE(n))
+#define XLP_NOR_IRQ(n,x)		(XLP_NOR_IRT_OFFSET(n) + (x))
 
-#define XLP_NOR_IRT_OFFSET			(150 + XLP_IRQ_RESERVED_MAX)
-#define XLP_NOR_IRQ(x)	(XLP_NOR_IRT_OFFSET + (x))
+#define XLP_NAND_IRT_OFFSET(n)		(151 + XLP_WD_BASE(n))
+#define XLP_NAND_IRQ(n,x)		(XLP_NAND_IRT_OFFSET(n) + (x))
 
-#define XLP_NAND_IRT_OFFSET			(151 + XLP_IRQ_RESERVED_MAX)
-#define XLP_NAND_IRQ(x)	(XLP_NAND_IRT_OFFSET + (x))
+#define XLP_SPI_IRT_OFFSET(n)		(152 + XLP_WD_BASE(n))
+#define XLP_SPI_IRQ(n,x)		(XLP_SPI_IRT_OFFSET(n) + (x))
 
-#define XLP_SPI_IRT_OFFSET			(152 + XLP_IRQ_RESERVED_MAX)
-#define XLP_SPI_IRQ(x)	(XLP_SPI_IRT_OFFSET + (x))
+#define XLP_MMC_IRT_OFFSET(n)		(153 + XLP_WD_BASE(n))
+#define XLP_MMC_IRQ(n,x)		(XLP_MMC_IRT_OFFSET(n) + (x))
 
-#define XLP_MMC_IRT_OFFSET			(153 + XLP_IRQ_RESERVED_MAX)
-#define XLP_MMC_IRQ(x)	(XLP_MMC_IRT_OFFSET + (x))
+#define XLP_SATA_IRQ 151
 
 /* The following are the values supported per slot. A slot can have a device or
  * a bridge, but only this much MSI/MSI-X can be alloted on that slot
@@ -188,97 +273,59 @@ THE POSSIBILITY OF SUCH DAMAGE.
  * We are using IRQ 192 - 255 for MSI/MSI-X
  * */
 
-#define XLP_PIC_IRTREG_START 0xB4
-#define XLP_ITE_ENTRIES		8
+#define XLP_INTX_TO_CTRL_FN(irq)\
+({\
+ u32 lirq = irq % XLP_IRQS_PER_NODE;\
+ ((lirq - XLP_PCIE_LINK_IRQ(0,0)) & 0x3);\
+})
+#define XLP_IRQ_TO_NODE(x)		((u8)((x) / XLP_IRQS_PER_NODE))
+
+#define XLP_BDF_BASE(b,d,f)	(0x18000000 + ((b) << 20) + ((d) << 15) + ((f) << 12))
+#define XLP_MAX_SLOTS		4	/* Only 4 slots now */
+
 #ifdef CONFIG_XLP_MSI_ADDRESSES
 #define XLP_MSI_ADDR		0xFEE00000
 #endif
-#define XLP_INTX_TO_CTRL_FN(irq) ((irq - XLP_PCIE_LINK_IRQ(0)) & 0x3)
 #define XLP_MSI_ADDR_SIZE	0x00002000
-#define XLP_MSIX_ADDR_SIZE	0x00008000
-#define XLP_BDF_BASE(b,d,f)	(0x18000000 + ((b) << 20) + ((d) << 15) + ((f) << 12))
-#define XLP_MAX_SLOTS		4	/* Only 4 slots now */
-#define XLP_PCIE_CTRL_DEVFN(node, ctr)	PCI_DEVFN((node + 1), ctr)
-
+#define XLP_MSIX_ADDR_SIZE	0x00004000
 #ifdef CONFIG_PCI_MSI
+ /* We are using IRQ 256 - 383 for MSI */
 #define XLP_MSI_MM_CAP		5	/* Multiple message capability */
 #define XLP_MSI_PER_SLOT	(1 << XLP_MSI_MM_CAP)
 #define XLP_MSI_IRQ_OFFSET	256	/* Note IRQ not IRT */
-#define XLP_MSI_IRQ_START(fn)	(XLP_MSI_IRQ_OFFSET + (fn) * XLP_MSI_PER_SLOT)
-#define XLP_MSI_INDEX_START	XLP_MSI_IRQ_START(0)
-#define XLP_MSI_INDEX_END	(XLP_MSI_IRQ_START(XLP_MAX_SLOTS) - 1) /* 128 Vectors */
-#define XLP_MSI_TO_CTRL_FN(msi) (((msi) >> (XLP_MSI_MM_CAP)) & 3)
-
+#define XLP_MSI_IRQ_START(n,fn)	((XLP_IRQS_PER_NODE * (n)) + \
+			XLP_MSI_IRQ_OFFSET + (fn) * XLP_MSI_PER_SLOT)
+#define XLP_MSI_INDEX_START	XLP_MSI_IRQ_START(0, 0)
+#define XLP_MSI_INDEX_END	(XLP_MSI_IRQ_START(0, XLP_MAX_SLOTS) - 1) /* 128 Vectors */
+#define XLP_MSI_TO_CTRL_FN(msi) (((msi % XLP_IRQS_PER_NODE) >> (XLP_MSI_MM_CAP)) & 3)
+#define XLP_MSI_TO_NODE(x)	XLP_IRQ_TO_NODE(x)
+ /* We are using IRQ 192 - 223 for MSI-X */
 #define XLP_MSIX_PER_SLOT	8
 #define XLP_MSIX_IRQ_OFFSET	192
-#define XLP_MSIX_TO_CTRL_FN(msix) (((msix) >> 3) & 3)
-#define XLP_MSIX_IRQ_START(fn)	(XLP_MSIX_IRQ_OFFSET + (fn) * XLP_MSIX_PER_SLOT)
-#define XLP_MSIX_INDEX_START	XLP_MSIX_IRQ_START(0)
-#define XLP_MSIX_INDEX_END	(XLP_MSIX_IRQ_START(XLP_MAX_SLOTS) - 1)// 31 vectors
+#define XLP_MSIX_TO_CTRL_FN(msix) (((msix % XLP_IRQS_PER_NODE) >> 3) & 3)
+#define XLP_MSIX_TO_NODE(x)	XLP_IRQ_TO_NODE(x)
+#define XLP_MSIX_IRQ_START(n, fn)	((XLP_IRQS_PER_NODE * n) + XLP_MSIX_IRQ_OFFSET + (fn) * XLP_MSIX_PER_SLOT)
+#define XLP_MSIX_INDEX_START	XLP_MSIX_IRQ_START(0, 0)
+#define XLP_MSIX_INDEX_END	(XLP_MSIX_IRQ_START(0, XLP_MAX_SLOTS) - 1)// 31 vectors
 
 #endif
+enum xlp_intmode {
+	XLP_INTMODE_NONE = 0,
+	XLP_INTMODE_INTX = 1,
+	XLP_INTMODE_MSI = 2,
+	XLP_INTMODE_MSIX = 4,
+};
+
+#define xlp_incr_ctrl_intmode(n, fn, mode) xlp_ctrl_intmode_add(n, fn, mode, 1)
+#define xlp_decr_ctrl_intmode(n, fn, mode) xlp_ctrl_intmode_add(n, fn, mode, -1)
+#define xlp_soc_pcidev_to_node(dev) ((u8)(PCI_SLOT(dev->devfn)/8))
+#define nlm_xlp_request_irq(node, irt)	((irt) + XLP_WD_BASE(node))
+
+#if defined CONFIG_PCIEPORTBUS
+#define PORT_TYPE_MASK			0xf
+#define PCIE_CAPABILITIES_REG		0x2
+#endif
 
-#define	XLP_INTMODE_NONE  0
-#define	XLP_INTMODE_INTX  1
-#define	XLP_INTMODE_MSI  2
-#define	XLP_INTMODE_MSIX  4
-
-#define xlp_incr_ctrl_intmode(fn, mode) xlp_ctrl_intmode_add(fn, mode, 1)
-#define xlp_decr_ctrl_intmode(fn, mode) xlp_ctrl_intmode_add(fn, mode, -1)
-/*
- *     Register Offsets
- */
-#define PIC_CTRL             0x00
-#define PIC_BYTESWAP         0x01
-#define PIC_STATUS           0x02
-#define PIC_INT_TIMEOUT      0x03
-#define PIC_ICI0_INT_TIMEOUT 0x04
-#define PIC_ICI1_INT_TIMEOUT 0x05
-#define PIC_ICI2_INT_TIMEOUT 0x06
-#define PIC_IPI_CTL          0x07
-#define PIC_INT_ACK          0x08
-#define PIC_INT_PENDING0     0x09
-#define PIC_INT_PENDING1     0x0a
-#define PIC_INT_PENDING2     0x0b
-
-#define PIC_WD0_MAX_VAL      0x0c
-#define PIC_WD0_COUNT        0x0d
-#define PIC_WD0_MASK_0       0x0e
-#define PIC_WD0_MASK_1       0x0f
-#define PIC_WD0_HEARBEATCMD  0x10
-#define PIC_WD0_HEARBEAT_0   0x11
-#define PIC_WD0_HEARBEAT_1   0x12
-
-#define PIC_WD_MAX_VAL(id)    (PIC_WD0_MAX_VAL + ((id) ? 7 : 0))
-#define PIC_WD_COUNT(id)      (PIC_WD0_COUNT + ((id) ? 7 : 0))
-#define PIC_WD_MASK_0(id)     (PIC_WD0_MASK_0 + ((id) ? 7 : 0))
-#define PIC_WD_MASK_1(id)     (PIC_WD0_MASK_1 + ((id) ? 7 : 0))
-#define PIC_WD_HEARBEAT_0(id) (PIC_WD0_HEARTBEAT_0 + ((id) ? 7 : 0))
-#define PIC_WD_HEARBEAT_1(id) (PIC_WD0_HEARTBEAT_1 + ((id) ? 7 : 0))
-
-#define PIC_SYS_TIMER_0_MAX_VAL   0x1a
-#define PIC_SYS_TIMER_MAX_VAL(id) (PIC_SYS_TIMER_0_MAX_VAL + (id))
-
-#define PIC_SYS_TIMER_0_COUNTER   0x22
-#define PIC_SYS_TIMER_COUNTER(id) (PIC_SYS_TIMER_0_COUNTER + (id))
-
-#define PIC_TIMER_0_MAXVAL   PIC_SYS_TIMER_0_MAX_VAL
-#define PIC_TIMER_0_COUNTER  PIC_SYS_TIMER_0_COUNTER
-#define PIC_TIMER_7_MAXVAL   PIC_SYS_TIMER_MAX_VAL(7)
-#define PIC_TIMER_7_COUNTER  PIC_SYS_TIMER_COUNTER(7)
-#define PIC_TIMER_6_MAXVAL   PIC_SYS_TIMER_MAX_VAL(6)
-#define PIC_TIMER_6_COUNTER  PIC_SYS_TIMER_COUNTER(6)
-
-#define PIC_INT_THR_ENABLE_0_N01   0x2a
-#define PIC_INT_THR_ENABLE_0_N23   0x2b
-#define PIC_INT_THR_ENABLE_N01(id) (PIC_INT_THR_ENABLE_0_N12 + ((id) * 2))
-#define PIC_INT_THR_ENABLE_N23(id) (PIC_INT_THR_ENABLE_0_N12 + ((id) * 2))
-
-#define PIC_IRT_0   0x3a
-#define PIC_IRT(id) (PIC_IRT_0 + (id))
-
-
-#define ASM_XLP_IO_PIC_OFFSET        0xffffffffb8004100 /* TODO: This will change in to function */
-#define C_XLP_IO_PIC_OFFSET        0xffffffffb8004100ULL /* TODO: This will change in to function */
+#endif		/* __ASSEMBLY__ */
 
 #endif
diff --git a/arch/mips/include/asm/smp.h b/arch/mips/include/asm/smp.h
index af42385..3d2d285 100644
--- a/arch/mips/include/asm/smp.h
+++ b/arch/mips/include/asm/smp.h
@@ -13,7 +13,9 @@
 
 #include <linux/bitops.h>
 #include <linux/linkage.h>
+#ifndef CONFIG_NLM_COMMON
 #include <linux/smp.h>
+#endif
 #include <linux/threads.h>
 #include <linux/cpumask.h>
 
diff --git a/arch/mips/kernel/nlm_fs_handler.S b/arch/mips/kernel/nlm_fs_handler.S
index 33ed34d..e5f1454 100644
--- a/arch/mips/kernel/nlm_fs_handler.S
+++ b/arch/mips/kernel/nlm_fs_handler.S
@@ -348,7 +348,10 @@ END(nlm_fs_processorId)
 
 NESTED(nlm_fs_read_timer, PT_SIZE, sp)
 
-#include <asm/netlogic/xlp_hal_pic.h>
+#if defined(CONFIG_NLM_XLP)
+#include <asm/netlogic/xlp_irq.h>
+#endif
+#define PIC_TIMER_6_COUNTER	0x28
         MFC0    k0, CP0_PRID, 1
         andi    k0, k0, 0x3ff
         srl     k0, k0, 5  /* grab node id */
diff --git a/arch/mips/kernel/setup.c b/arch/mips/kernel/setup.c
index 5d4b49e..862cc6b 100644
--- a/arch/mips/kernel/setup.c
+++ b/arch/mips/kernel/setup.c
@@ -91,7 +91,13 @@ phys_t __weak fix_prefetch_mem(phys_t size)
 void __init add_memory_region(phys_t start, phys_t size, long type)
 {
 	int x = boot_mem_map.nr_map;
+#ifdef CONFIG_NLM_XLP
+#ifndef CONFIG_NUMA
 	struct boot_mem_map_entry *prev = boot_mem_map.map + x - 1;
+#endif
+#else /*not XLP*/
+	struct boot_mem_map_entry *prev = boot_mem_map.map + x - 1;
+#endif
 
 #ifdef CONFIG_CPU_HAS_PREFETCH
 	/*
@@ -112,10 +118,20 @@ void __init add_memory_region(phys_t start, phys_t size, long type)
 	 * Try to merge with previous entry if any.  This is far less than
 	 * perfect but is sufficient for most real world cases.
 	 */
+#ifdef CONFIG_NLM_XLP
+#ifndef CONFIG_NUMA
+	/* For numa, we want to avoid merging memories from different nodes */
 	if (x && prev->addr + prev->size == start && prev->type == type) {
 		prev->size += size;
 		return;
 	}
+#endif
+#else /*not XLP*/
+	if (x && prev->addr + prev->size == start && prev->type == type) {
+		prev->size += size;
+		return;
+	}
+#endif
 
 	if (x == BOOT_MEM_MAP_MAX) {
 		pr_err("Ooops! Too many entries in the memory map!\n");
@@ -271,7 +287,34 @@ static void __init bootmem_init(void)
 	finalize_initrd();
 }
 
-#else  /* !CONFIG_SGI_IP27 */
+#elif defined(CONFIG_NLM_XLP) && defined(CONFIG_NUMA)
+
+static void __init bootmem_init(void)
+{
+	unsigned long reserved_end;
+
+	/*
+	 * Init any data related to initrd. It's a nop if INITRD is
+	 * not selected. Once that done we can determine the low bound
+	 * of usable memory.
+	 */
+#ifdef CONFIG_XEN
+	reserved_end = max(init_initrd(),
+			   (unsigned long) PFN_UP(__pa_symbol(&_end) + PAGE_SIZE));
+#else
+	reserved_end = max(init_initrd(),
+			   (unsigned long) PFN_UP(__pa_symbol(&_end)));
+#endif
+
+	nlm_numa_bootmem_init(reserved_end);
+
+	/*
+	 * Reserve initrd memory if needed.
+	 */
+	finalize_initrd();
+}
+
+#else  /* !CONFIG_SGI_IP27 && !(defined(CONFIG_NLM_XLP) && defined(CONFIG_NUMA)) */
 
 static void __init bootmem_init(void)
 {
@@ -525,8 +568,10 @@ static void __init arch_mem_init(char **cmdline_p)
 
 	bootmem_init();
 #if defined(CONFIG_NLM_XLP) && defined(CONFIG_MAPPED_KERNEL)
+#ifndef CONFIG_NUMA
 	setup_mapped_kernel_tlbs(FALSE, TRUE);
 #endif
+#endif
 #ifdef CONFIG_KEXEC
 	pr_info("Kexeckernel info: start = %llu end = %llu\n",
 		kexeck_res.start, kexeck_res.end);
diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index c675c88..bf2a741 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -49,6 +49,7 @@
 volatile cpumask_t cpu_callin_map;	/* Bitmask of started secondaries */
 int __cpu_number_map[NR_CPUS];		/* Map physical to logical */
 int __cpu_logical_map[NR_CPUS];		/* Map logical to physical */
+EXPORT_SYMBOL(__cpu_logical_map);
 
 /* Number of TCs (or siblings in Intel speak) per CPU core */
 int smp_num_siblings = 1;
diff --git a/arch/mips/netlogic/common/Makefile b/arch/mips/netlogic/common/Makefile
index e6aad9c..f8d26ed 100644
--- a/arch/mips/netlogic/common/Makefile
+++ b/arch/mips/netlogic/common/Makefile
@@ -13,10 +13,12 @@ ifeq ($(CONFIG_NLM_CORTINA_SUPPORT),y)
 EXTRA_CFLAGS += -DNLM_CORTINA_SUPPORT
 endif
 
+obj-$(CONFIG_SMP)                 	+= smp.o
+
 EXTRA_AFLAGS := $(CFLAGS)
 
 clean-files += msgring.o msgring_xls.o msgring_shared.o nlm_hal_fmn_config.o nlm_hal_cpu_info.o
 clean-files += nlm_hal_sys.o nlm_hal.o
-clean-files += srio.o dma.o cpu_proc.o
+clean-files += srio.o dma.o smp.o cpu_proc.o
 clean-files += msgring.c msgring_xls.c msgring_shared.c nlm_hal.c
 
diff --git a/arch/mips/netlogic/common/memory.c b/arch/mips/netlogic/common/memory.c
index 9c95467..f0a145b 100644
--- a/arch/mips/netlogic/common/memory.c
+++ b/arch/mips/netlogic/common/memory.c
@@ -204,7 +204,10 @@ void setup_mapped_kernel_tlbs(int firstpage, int primary_cpu)
 		setup_tlb(&tlb, pagesize);
 	}
 	else {
-		int retval;
+#ifdef CONFIG_NUMA
+		int node;
+		extern struct nlm_node_mem_info node_mem_info[];
+#endif
 		/*
 		 * the primary cpu reads the memory map and records
 		 * the highest page frame number. Secondary cpus
@@ -237,23 +240,46 @@ void setup_mapped_kernel_tlbs(int firstpage, int primary_cpu)
 			setup_tlb(&tlb, pagesize);
 		}
 
-		if (primary_cpu) {
-			if (max_low_pfn > MAX_WIRED_PFN) {
-				__get_free_pages = alloc_bootmem_low;
-				retval = map_kernel_addrspace(vaddr, MAX_WIRED_PFN, max_low_pfn);
-				if (retval != 0)
-					panic("unable to map kernel addrspace\n");
-				NONWIRED_START = vaddr;
-				NONWIRED_END = vaddr + (PFN_PHYS(max_low_pfn - MAX_WIRED_PFN));
-				__get_free_pages = ____get_free_pages;
-			}
+#ifdef CONFIG_NUMA
+		/* For NUMA, for each node, we have to wire the minimum physical page for that node.
+		 * NUMA uses that piece of memory immediately to keep its internal data structure.
+		 */
+		for (node = 1; node < NLM_MAX_CPU_NODE; node ++) {
+			paddr = PFN_PHYS(node_mem_info[node].min_start_pfn);
+			if (paddr == 0)
+				continue;
+
+			vaddr = KERNEL_SEG_START + paddr;
+			tlb.entryHi = vaddr;
+			tlb.entrylo0 = page_entrylo(paddr, attr);
+			tlb.entrylo1 = page_entrylo(paddr + pagesize, attr);
+			tlb.wired = TRUE;
+			setup_tlb(&tlb, pagesize);
+		}
+#endif
+	}
+}
+
+void setup_mapped_kernel_pgtable(void)
+{
+	unsigned long vaddr = KERNEL_SEG_START + PFN_PHYS(MIN(max_low_pfn, MAX_WIRED_PFN));
+	int retval;
+
+	if (max_low_pfn > MAX_WIRED_PFN) {
+		__get_free_pages = alloc_bootmem_low;
+		retval = map_kernel_addrspace(vaddr, MAX_WIRED_PFN, max_low_pfn);
+		if (retval != 0)
+			panic("unable to map kernel addrspace\n");
+		NONWIRED_START = vaddr;
+		NONWIRED_END = vaddr + (PFN_PHYS(max_low_pfn - MAX_WIRED_PFN));
+		__get_free_pages = ____get_free_pages;
+	}
 #ifdef CONFIG_64BIT
-			__vmalloc_start = KERNEL_SEG_START + (1UL << PGDIR_SHIFT);
+	__vmalloc_start = KERNEL_SEG_START + (1UL << PGDIR_SHIFT);
 #else
-			__vmalloc_start = vaddr;
+	__vmalloc_start = vaddr;
 #endif
-		}
-	}
+	return;
 }
 
 #define FLOOR(addr, alignment) ((addr) & ~((alignment) - 1)) /* alignment must be power of 2 */
diff --git a/arch/mips/netlogic/common/nlm_evp_cpld.c b/arch/mips/netlogic/common/nlm_evp_cpld.c
index 6b8f9f1..ae3f32a 100644
--- a/arch/mips/netlogic/common/nlm_evp_cpld.c
+++ b/arch/mips/netlogic/common/nlm_evp_cpld.c
@@ -40,8 +40,8 @@ static nlm_xlp_nor_t xlp_nor_dev[8] = {
 { 0x16000000, SIZE_16MB, 0, 0x2e00 },
 { 0xffffffff, 0, 0, 0 },
 { 0x17000000, SIZE_1MB , 1, 0x2C00 },
-{ 0x17100000, SIZE_1MB , 0, 0x2f84 },
-{ 0x17200000, SIZE_1MB , 0, 0x2f84 },
+{ 0x17200000, SIZE_1MB , 0, 0x2C00 },
+{ 0x17300000, SIZE_1MB , 0, 0x2f84 },
 { 0xffffffff, 0, 0, 0 },
 { 0xffffffff, 0, 0, 0 },
 { 0xffffffff, 0, 0, 0 },
@@ -120,13 +120,35 @@ int is_ilk_card_onslot(int slot)
 		return 0;
 }
 
-int nlm_get_interface_type(int node, int slot)
+#if defined(NLM_HAL_LINUX_USER) || defined(NLM_HAL_LINUX_KERNEL)
+/* cop0 hwren register should be set */
+static inline int my_cpu_id(void)
 {
-#ifdef SKIP_INTERFACE_TYPE_FROMCPLD
-	return DC_NOT_PRSNT;
-#else
-	uint16_t data = nlm_hal_cpld_read_16(2, 6);
+	unsigned int cpu = 0;
+
+	__asm__ volatile (".set push\n"
+			".set noreorder\n"
+			".set arch=xlp\n"
+			"rdhwr %0, $0\n"
+			".set pop\n"
+			: "=r" (cpu)
+			:);
+
+	return cpu;
+}
+#endif
 
+int nlm_get_interface_type(int node, int slot)
+{
+	uint16_t data;
+       
+	/* there is no cpld in the existing multi node board for node 1-3 */
+#if defined(NLM_HAL_LINUX_USER) || defined(NLM_HAL_LINUX_KERNEL)
+	if(my_cpu_id() >= 32)
+		return DC_NOT_PRSNT;
+#endif
+	
+	data = nlm_hal_cpld_read_16(2, 6);
 	nlm_print("Slot present status 0x%x\n", (data & 0xFF));
         if (slot == 4)
                 return DC_SGMII;
@@ -134,6 +156,9 @@ int nlm_get_interface_type(int node, int slot)
         if (nlm_xlp_cpldver() == 0)
                 return DC_NOT_PRSNT;
 
+#ifdef SKIP_INTERFACE_TYPE_FROMCPLD
+	return DC_NOT_PRSNT;
+#else
 	if (slot == 2)
 		slot >>= 1;
 	else if (slot == 1)
diff --git a/arch/mips/netlogic/common/nlm_hal.c b/arch/mips/netlogic/common/nlm_hal.c
index 671d195..9350df0 100644
--- a/arch/mips/netlogic/common/nlm_hal.c
+++ b/arch/mips/netlogic/common/nlm_hal.c
@@ -66,7 +66,7 @@ THE POSSIBILITY OF SUCH DAMAGE.
 unsigned long xlp_io_base;
 unsigned long xlp_fmn_base[NLM_MAX_NODES];
 unsigned long xlp_nae_base[NLM_MAX_NODES];
-unsigned long xlp_sae_base;
+unsigned long xlp_sae_base[NLM_MAX_NODES];
 unsigned long xlp_rsa_base;
 unsigned long xlp_mac_base[NLM_MAX_NODES];
 unsigned long xlp_poe_base_pcie[NLM_MAX_NODES];
@@ -456,9 +456,9 @@ __inline__ void nlm_hal_init(void)
 		xlp_poe_base_pcie[node] = (xlp_io_base | XLP_CFG_BASE(node, XLP_POE)) & 0x1fffffff; /* For now . Will be fixed soon.*/
 
 		xlp_sys_base[node] = (xlp_io_base | XLP_CFG_BASE(node, XLP_SYS)) & 0x1fffffff; /*For now . Will be fixed soon.*/
+		xlp_sae_base[node] = (xlp_io_base | XLP_CFG_BASE(node, XLP_SAE)) & 0x1fffffff; /* For now . Will be fixed soon.*/
 	}
 
-	xlp_sae_base = (xlp_io_base | XLP_CFG_BASE(0, XLP_SAE)) & 0x1fffffff; /* For now . Will be fixed soon.*/
         xlp_rsa_base = (xlp_io_base | XLP_CFG_BASE(0, XLP_RSA)) & 0x1fffffff; /* For now . Will be fixed soon.*/
 
        	if (is_nlm_xlp3xx()) {
@@ -1520,7 +1520,18 @@ void nlm_hal_set_rsa_freq(int node, int freq)
 }
 
 
-void nlm_hal_set_sae_engine_sel(void ) 
+void nlm_hal_set_rsa_cge(int node, int enable)
+{
+#define NLM_RSA_CFG_REG 0x40
+	uint32_t d32 = nlm_hal_read_rsa_reg(NLM_RSA_CFG_REG);
+	if(enable)
+		d32 |= 1<<9;
+	else
+		d32 &= ~(1<<9);
+	nlm_hal_write_rsa_reg(NLM_RSA_CFG_REG, d32);
+}
+
+void nlm_hal_set_sae_engine_sel(int node)
 {
 	int i, n;
 #define NLM_SAE_ENGINE_SELECT_REG_0 0x41
@@ -1529,7 +1540,7 @@ void nlm_hal_set_sae_engine_sel(void )
 	else
 		n = 8;
     for (i = 0; i < n; i++) {
-        nlm_hal_write_sae_reg(NLM_SAE_ENGINE_SELECT_REG_0 + i, 0x00FFFFFF);	
+        nlm_hal_write_sae_reg(node, NLM_SAE_ENGINE_SELECT_REG_0 + i, 0x00FFFFFF);
     }
 }
 
@@ -1707,6 +1718,7 @@ EXPORT_SYMBOL(sgmii_scan_phys);
 EXPORT_SYMBOL(nlm_hal_get_dev_base);
 EXPORT_SYMBOL(nlm_hal_set_sae_freq);
 EXPORT_SYMBOL(nlm_hal_set_rsa_freq);
+EXPORT_SYMBOL(nlm_hal_set_rsa_cge);
 EXPORT_SYMBOL(nlm_hal_set_sae_engine_sel);
 EXPORT_SYMBOL(nlm_hal_set_rsa_engine_sel);
 EXPORT_SYMBOL(nlm_hal_get_crypto_vc_nums);
diff --git a/arch/mips/netlogic/common/nlm_hal_cpu_info.c b/arch/mips/netlogic/common/nlm_hal_cpu_info.c
index fe41dd0..790a125 100644
--- a/arch/mips/netlogic/common/nlm_hal_cpu_info.c
+++ b/arch/mips/netlogic/common/nlm_hal_cpu_info.c
@@ -37,6 +37,11 @@ __inline__ uint32_t get_proc_id(void)
 	return cpuid;
 }
 
+__inline__ int get_nlm_xlp8xx_rev(void)
+{
+	return nlm_read_prid() & 0xff;
+}
+
 __inline__ int is_nlm_rev_a0(void)
 {
 	return ((nlm_read_prid() & 0xff) == XLP_REVISION_A0);
@@ -104,6 +109,23 @@ inline int get_nlm_xlp3xx_rev(void)
 	return sw_rev;
 }
 
+/***************************************************************************************
+* match the chip revision with 'rev'
+ * rev:  revision number
+		single match: XLP_REVISION_A0 etc
+		multi-match:  XLP_REVISION_AX/_BX/_XX
+****************************************************************************************/
+inline int is_nlm_xlp8xx_rev_xx(uint32_t rev)
+{
+	int sw_rev=get_nlm_xlp8xx_rev();
+	uint32_t rev_b0 = XLP_REVISION_B0;
+	if( rev==sw_rev)        return 1;
+	if( rev==XLP_REVISION_AX && sw_rev<rev_b0)	return 1;
+	if( rev==XLP_REVISION_BX && (rev_b0<=sw_rev))  	return 1;
+	if( rev==XLP_REVISION_ANY)			return 1;
+	return 0;
+}
+
 /************************************************
 * match the chip revision with 'rev'
  * rev:  revision number
@@ -126,41 +148,129 @@ inline int is_nlm_xlp3xx_rev_xx(uint32_t rev)
 	return 0;
 }
 
-/******************************************************
+/***************************************************************************************
+match legacy eagle Ax: xlp832, xlp816, xlp432, xlp416, xlp408, xlp208, xlp204, xlp104
+chipid: 832, 816, 432, 416, 408, 208, 204, 104; match any of the chip in eagle Ax
+rev:    revision
+***************************************************************************************/
+inline int is_xlp8xx_legacy(int chipid, uint32_t rev)
+{
+	uint32_t pid=get_proc_id();
+	uint32_t sw_rev = get_nlm_xlp8xx_rev();
+
+	if( rev==XLP_REVISION_ANY )	rev=XLP_REVISION_AX;
+	if( sw_rev!=rev && rev!=XLP_REVISION_AX )	return 0;
+	if( XLP_REVISION_B0<=sw_rev ) return 0;
+	if(	(( (chipid==832) || (chipid==800) ) &&  (pid==CHIP_PROCESSOR_ID_XLP_832)) ||
+		(( (chipid==816) || (chipid==800) ) &&  (pid==CHIP_PROCESSOR_ID_XLP_816)) ||
+		(( (chipid==432) || (chipid==400) ) &&  (pid==CHIP_PROCESSOR_ID_XLP_432)) ||
+		(( (chipid==416) || (chipid==400) ) &&  (pid==CHIP_PROCESSOR_ID_XLP_416)) ||
+		(( (chipid==408) || (chipid==400) ) &&  (pid==CHIP_PROCESSOR_ID_XLP_408)) ||
+		(( (chipid==208) || (chipid==200) ) &&  (pid==CHIP_PROCESSOR_ID_XLP_208)) ||
+		(( (chipid==204) || (chipid==200) ) &&  (pid==CHIP_PROCESSOR_ID_XLP_204)) ||
+		(( (chipid==104) || (chipid==100) ) &&  (pid==CHIP_PROCESSOR_ID_XLP_104)) )
+		return 1;
+
+    return 0;
+}
+
+/***************************************************************************************
+match eagle 8xxBx
+num_cpu: 32-xlp832, 24-xlp824, 16-xlp816, 0 any cpu number is valid match
+rev:  chip revision,
+***************************************************************************************/
+#define CPU_NUM_ANY  0	//any cpu number will be matched
+inline int is_xlp8xx(uint8_t num_cpu, uint32_t rev)
+{
+	uint32_t pid, mask, cfg1;
+	int ret, cpuNum;
+
+	ret=is_nlm_xlp8xx_rev_xx(rev);
+	if(ret!=1)	return 0;
+
+	pid=get_proc_id();
+
+	if( pid == CHIP_PROCESSOR_ID_XLP_8_4_XX)
+	{
+		mask = efuse_cfg0() & 0xff;
+		cfg1 = efuse_cfg1() & 0x7;
+
+		cpuNum=(8-bitcount(mask))<<2;
+		if( (cpuNum==num_cpu || num_cpu==CPU_NUM_ANY ) && cfg1!=0x7 )	return 1;
+	}
+
+	return 0;
+}
+
+/***************************************************************************************
+match eagle 4xxBx
+num_cpu: 32-xlp832, 24-xlp824, 16-xlp816, 0xff any cpu number is valid match
+rev:  chip revision,
+***************************************************************************************/
+inline int is_xlp4xx(uint8_t num_cpu, uint32_t rev)
+{
+	uint32_t pid, mask, cfg1;
+	int ret, cpuNum;
+
+	ret=is_nlm_xlp8xx_rev_xx(rev);
+	if(ret!=1)  return 0;
+
+	pid=get_proc_id();
+
+	if( pid == CHIP_PROCESSOR_ID_XLP_8_4_XX )
+	{
+		mask = efuse_cfg0() & 0xff;
+		cfg1 = efuse_cfg1() & 0x7;
+
+		cpuNum=(8-bitcount(mask))<<2;
+		if( (cpuNum==num_cpu || num_cpu==CPU_NUM_ANY ) && cfg1==0x7 ) return 1;
+	}
+
+    return 0;
+}
+
+/*****************************************************************************************************
 match storm
 num_cpu: 16-xlp316,  8-xlp308, 4-xlp304;  0xff any cpu number is valid match
 rev:  chip revision,
-type: CPU_EXTPID_XLP_3XX_NONE, CPU_EXTPID_XLP_3XX_L,
-	CPU_EXTPID_XLP_3XX_LP, CPU_EXTPID_XLP_3XX_LP2
-*******************************************************/
-#define CPU_NUM_ANY  0	/*any cpu number will be matched*/
+type: CPU_EXTPID_XLP_3XX_NONE, CPU_EXTPID_XLP_3XX_L, CPU_EXTPID_XLP_3XX_LP, CPU_EXTPID_XLP_3XX_LP2
+********************************************************************************************************/
 inline int is_xlp3xx(uint8_t num_cpu, uint32_t rev, uint32_t exttype)
 {
 	uint32_t pid, cfg0, mask;
 	uint8_t storm;
 	int ret, cpuNum;
 
-	ret = is_nlm_xlp3xx_rev_xx(rev);
-	if (ret != 1)
-		return 0;
+	ret=is_nlm_xlp3xx_rev_xx(rev);
+	if(ret!=1)  return 0;
 
-	pid = get_proc_id();
+	pid=get_proc_id();
 
-	if (pid == CHIP_PROCESSOR_ID_XLP_3XX) {
-		cfg0 = efuse_cfg0();
-		mask = cfg0 & 0xf;
-		storm = (uint8_t)((cfg0 >> 4) & 0xf);
-
-		cpuNum = (4-bitcount(mask)) << 2;
-		if ((cpuNum == num_cpu || num_cpu == CPU_NUM_ANY) &&
-			(exttype == CPU_EXTPID_XLP_3XX_ANY
-			|| storm == exttype))
-			return 1;
+	if( pid == CHIP_PROCESSOR_ID_XLP_3XX )
+	{
+		cfg0=efuse_cfg0();
+		mask = cfg0  & 0xf;
+		storm = (uint8_t)(( cfg0>>4 )  & 0xf);
+
+		cpuNum=(4-bitcount(mask))<<2;
+		if( (cpuNum==num_cpu || num_cpu==CPU_NUM_ANY ) &&
+			( exttype==CPU_EXTPID_XLP_3XX_ANY || storm==exttype ) ) return 1;
 	}
 
     return 0;
 }
 
+/* match xlp2xx */
+inline int is_xlp2xx(uint8_t num_cpu, uint32_t rev, uint32_t exttype)
+{
+	uint32_t pid;
+
+	pid=get_proc_id();
+	if(pid == CHIP_PROCESSOR_ID_XLP_2XX)
+		return 1;
+	return 0;
+}
+
 static __inline__ int is_nlm_xlp_8_4_x(void)
 {
 	return (get_proc_id() == CHIP_PROCESSOR_ID_XLP_8_4_XX);
@@ -339,6 +449,49 @@ __inline__ int is_nlm_xlp3xx(void)
 	return (get_proc_id() == CHIP_PROCESSOR_ID_XLP_3XX);
 }
 
+/*****************************************************************************************************
+ * match storm
+ * chipid: 832, 316,  308, etc, 800: any in 8xx group, 0 for all xlp group
+ * rev:    XLP_REVISION_A0, XLP_REVISION_A0 etc, or XLP_REVISION_AX (a0,a1,a2) XLP_REVISION_ANY(both: ax,bx)
+ * type: current only for 3xx: CPU_EXTPID_XLP_3XX_NONE, CPU_EXTPID_XLP_3XX_L, CPU_EXTPID_XLP_3XX_LP, CPU_EXTPID_XLP_3XX_LP2
+ * ********************************************************************************************************/
+int is_nlm_xlp(unsigned int chipid, unsigned int rev, unsigned int ext)
+{
+	uint32_t group=chipid/100;
+	uint8_t num_cpu=(uint8_t)(chipid%100);
+	int b_rc=0;
+
+	if ( group==8 )
+	{
+		b_rc=is_xlp8xx(num_cpu, rev);
+		if( b_rc==1 )	return 1;
+	}
+
+	if ( group==4 )
+	{
+		b_rc=is_xlp4xx(num_cpu, rev);
+		if( b_rc==1 )	return 1;
+	}
+
+	if ( group==3 )
+	{
+		b_rc=is_xlp3xx(num_cpu, rev, ext);
+		return b_rc;
+	}
+
+	if ( group==2 )
+	{
+		b_rc=is_xlp2xx(num_cpu, rev, ext);
+		return b_rc;
+	}
+
+	/* for legacy chips: */
+	if( rev<=XLP_REVISION_A2 || rev==XLP_REVISION_AX || rev==XLP_REVISION_ANY )
+		b_rc=is_xlp8xx_legacy(chipid, rev);
+
+	return b_rc;
+}
+
 __inline__ int is_nlm_xlp3xx_a1(void)
 {
 	return (is_nlm_xlp3xx() && is_nlm_rev_a1());
diff --git a/arch/mips/netlogic/common/nlm_hal_fmn_config.c b/arch/mips/netlogic/common/nlm_hal_fmn_config.c
index b947350..c0e0c3e 100644
--- a/arch/mips/netlogic/common/nlm_hal_fmn_config.c
+++ b/arch/mips/netlogic/common/nlm_hal_fmn_config.c
@@ -36,6 +36,7 @@
 #endif
 #include "libfdt.h"
 #include "fdt_helper.h"
+#include "nlm_hal_sys.h"
 
 #define CPU		1
 #define PCIE	2
@@ -505,9 +506,13 @@ static void nlm_hal_write_fmn_credit(int node, int max_nodes)
 	uint32_t credits;
 
         if (is_nlm_xlp3xx()) {
-                nlm_print(" XLP3XX FMN configuration \n");
-		fmn_modify_qsize_credit_config(node, XLP_MSG_BLK_CPU, 4, XLP_CPU0_VC_BASE, XLP_CPU3_VC_LIMIT);
-		if(is_nlm_xlp308()){
+		nlm_print(" XLP3XX FMN configuration \n");
+		if(is_nlm_xlp316()){
+			nlm_print(" XLP316 FMN configuration \n");
+			fmn_modify_qsize_credit_config(node, XLP_MSG_BLK_CPU, 4, XLP_CPU0_VC_BASE, XLP_CPU3_VC_LIMIT);
+		}
+		else if(is_nlm_xlp308()){
+			nlm_print(" XLP308 FMN configuration \n");
                 	fmn_config = &xlp3xx_fmn_config[0];
 			fmn_config += XLP_MSG_HANDLE_CPU2;
 			for(hndl = 0; hndl<2; hndl++){
@@ -799,22 +804,14 @@ int parse_fdt_fmn_config(void *fdt)
 #endif
 	uint32_t qsize, credits, src_node, s_stn, d_stn;
 
-	sprintf(fmn_cfg_str,"/soc");
+	sprintf(fmn_cfg_str,"/soc/nodes");
 	nodeoffset = fdt_path_offset(fdt, fmn_cfg_str);
-
-        if(nodeoffset < 0) {
-		nlm_print("No 'soc' param in dtb \n");		
-	        return -1;  
-	}
-#ifdef NLM_HAL_LINUX_KERNEL
-        pval = (uint32_t *)fdt_getprop(fdt, nodeoffset, "num-nodes", &plen);
-        if(pval != NULL) {
-		max_nodes = fdt32_to_cpu(*(unsigned int *)pval);
-        }
-	else {
-		max_nodes = 1;
+        if(nodeoffset >= 0) {
+		pval = (uint32_t *)fdt_getprop(fdt, nodeoffset, "num-nodes", &plen);
+		if(pval != NULL) {
+			max_nodes = fdt32_to_cpu(*(unsigned int *)pval);
+		}
 	}
-#endif
 
 	nlm_print("Number of nodes %d \n",max_nodes);
 
@@ -861,7 +858,7 @@ int parse_fdt_fmn_config(void *fdt)
 			}
 		}
 #ifdef NLM_HAL_LINUX_KERNEL
-		if ((pval == NULL) || (nlm_node_cfg.fmn_cfg[node]->fmn_spill_base == 0ULL)){
+		if ((pval == NULL) || (spill_base == 0ULL)){
 			spill_base = nlm_spill_alloc(node, (nlm_node_cfg.fmn_cfg[node]->fmn_spill_size));
 			if (spill_base == 0ULL) {
 				nlm_print("Node %d FMN spill_mem alloc failed \n", node);
@@ -948,6 +945,10 @@ void nlm_hal_fmn_init(void *fdt)
 		while(1);
 	}
 
+	for (node = 0; node < max_nodes; node++) {
+		nlm_hal_soc_clock_enable(node, DFS_DEVICE_RSA);
+	}
+
 	//fmn_qsize_credit_cfg_extract(fdt);
 	/* verify out_q config 
 	 */
diff --git a/arch/mips/netlogic/common/nlm_hal_nae.c b/arch/mips/netlogic/common/nlm_hal_nae.c
index 0911474..4a94a00 100644
--- a/arch/mips/netlogic/common/nlm_hal_nae.c
+++ b/arch/mips/netlogic/common/nlm_hal_nae.c
@@ -129,12 +129,12 @@ static uint32_t nae_get_EXT_G_MDIO_DIV(void)
  */
 static uint32_t nae_get_INT_MDIO_DIV(void)
 {
-	return ((0x9 << INT_MDIO_CTRL_XDIV_POS) | (0 << INT_MDIO_CTRL_MCDIV_POS));
+	return ((0x7F << INT_MDIO_CTRL_XDIV_POS) | (2 << INT_MDIO_CTRL_MCDIV_POS));
 }
 
 static uint32_t nae_get_EXT_XG_MDIO_DIV(void)
 {
-	return ((0x9 << EXT_XG_MDIO_CTRL_XDIV_POS) | (0 << EXT_XG_MDIO_CTRL_MCDIV_POS));
+	return ((0x7F << EXT_XG_MDIO_CTRL_XDIV_POS) | (2 << EXT_XG_MDIO_CTRL_MCDIV_POS));
 }
 
 /*
@@ -592,7 +592,7 @@ int nlm_hal_mdio_wr(int node, int type, int bus, int phyaddr, int regidx, uint16
 	if (type == NLM_HAL_INT_MDIO) {
 		/* INT_MDIO_CTRL: 0x799 */
 		return nae_int_gmac_mdio_write(node, bus, phyaddr&0x1F, regidx, val);
-	} else if (type == NLM_HAL_INT_MDIO) {
+	} else if (type == NLM_HAL_INT_MDIO_C45) {
 		/* INT_MDIO_CTRL: 0x799 */
 		return nlm_hal_xgmac_imdio_write(node, phyaddr&0x1F, regidx, val);
 	} else if (type == NLM_HAL_EXT_MDIO) {
@@ -629,7 +629,7 @@ int nlm_hal_mdio_write(int node, int type, int bus, int block, int intf_type,
 * @ingroup hal_nae
 *
 */
-int nlm_hal_mdio_reset(int node, int type, int bus, int block, int intf_type)
+int nlm_hal_mdio_reset(int node, int type, int bus)
 {
 	if ((type == NLM_HAL_INT_MDIO) || (type == NLM_HAL_INT_MDIO_C45)) {
 		return internal_nae_gmac_mdio_reset(node, bus);
@@ -687,10 +687,12 @@ void xlp3xx_8xxb0_nae_lane_reset_txpll(int node, int block, int lane_ctrl, int m
 	val |= 0x100000 | (mode << PHY_LANE_CTRL_PHYMODE_POS);  /* Bit20: serdes reg reset Storm & Eagle B0 */
 	val &= ~(0x20000);
         nlm_hal_write_mac_reg(node, block, PHY, lane_ctrl,val);
+	nlm_mdelay(1);
 		
 	val = nlm_hal_read_mac_reg(node, block, PHY, lane_ctrl);
 	val |= 0x40000000; /* Unset the reset (inverse logic) : Bit30: epcs reset */
         nlm_hal_write_mac_reg(node, block, PHY, lane_ctrl,val);
+	nlm_mdelay(1);
 	NAE_DEBUG(" After serdes  de-assertion PMA value=0x%x\n", nlm_hal_read_mac_reg(node, block, PHY, lane_ctrl));
 
 
@@ -698,6 +700,7 @@ void xlp3xx_8xxb0_nae_lane_reset_txpll(int node, int block, int lane_ctrl, int m
 	val = nlm_hal_read_mac_reg(node, block, PHY, lane_ctrl);
 	val &= ~( (1 << 29) | (0x7ffff));
 	nlm_hal_write_mac_reg(node, block, PHY, lane_ctrl, (rext_sel | val));
+	nlm_mdelay(1);
 
 	NAE_DEBUG("Reset PLL done \n");
 }
@@ -896,11 +899,12 @@ static void xlp_nae_config_lane_gmac(int node, int cplx_mask)
 
 void nlm_hal_mdio_init(int node)
 {
-        nlm_hal_mdio_reset(node, NLM_HAL_INT_MDIO, 0, BLOCK_7, LANE_CFG);
-        nlm_hal_mdio_reset(node, NLM_HAL_EXT_MDIO, 0, BLOCK_7, LANE_CFG);
-        nlm_hal_mdio_reset(node, NLM_HAL_EXT_MDIO, 1, BLOCK_7, LANE_CFG);
-        nlm_hal_mdio_reset(node, NLM_HAL_EXT_MDIO_C45, 0, BLOCK_7, LANE_CFG);
-        nlm_hal_mdio_reset(node, NLM_HAL_EXT_MDIO_C45, 1, BLOCK_7, LANE_CFG);
+	nlm_hal_mdio_reset(node, NLM_HAL_INT_MDIO, 0);
+	nlm_hal_mdio_reset(node, NLM_HAL_EXT_MDIO, 0);
+	nlm_hal_mdio_reset(node, NLM_HAL_EXT_MDIO, 1);
+	nlm_hal_mdio_reset(node, NLM_HAL_EXT_MDIO_C45, 0);
+	nlm_hal_mdio_reset(node, NLM_HAL_EXT_MDIO_C45, 1);
+	nlm_mdelay(3);
 }
 
 /**
diff --git a/arch/mips/netlogic/common/nlm_hal_sys.c b/arch/mips/netlogic/common/nlm_hal_sys.c
index daf56d5..eaa0292 100644
--- a/arch/mips/netlogic/common/nlm_hal_sys.c
+++ b/arch/mips/netlogic/common/nlm_hal_sys.c
@@ -33,44 +33,6 @@ THE POSSIBILITY OF SUCH DAMAGE.
 #include "nlm_hal_xlp_dev.h"
 
 /**
- * SoC device indices for XLP8xx/4xx devices.
- */
-static uint8_t soc_device_index_xlp8xx[] = {
-	DFS_DEVICE_NAE_2X,
-	DFS_DEVICE_SAE,
-	DFS_DEVICE_RSA,
-	DFS_DEVICE_DTRE,
-	DFS_DEVICE_CMP,
-	DFS_DEVICE_KBP,
-	DFS_DEVICE_DMC,
-	DFS_DEVICE_NAND,
-	DFS_DEVICE_MMC,
-	DFS_DEVICE_NOR,
-	DFS_DEVICE_CORE
-};
-
-/**
- * SoC device indices for XLP3xx devices.
- */
-static uint8_t soc_device_index_xlp3xx[] = {
-	DFS_DEVICE_NAE_2X,
-	DFS_DEVICE_SAE,
-	DFS_DEVICE_RSA,
-	DFS_DEVICE_DTRE,
-	INVALID_DFS_DEVICE,
-	INVALID_DFS_DEVICE,
-	DFS_DEVICE_DMC,
-	DFS_DEVICE_NAND,
-	DFS_DEVICE_MMC,
-	DFS_DEVICE_NOR,
-	DFS_DEVICE_CORE,
-	DFS_DEVICE_REGEX_SLOW,
-	DFS_DEVICE_REGEX_FAST,
-	DFS_DEVICE_SATA
-};
-
-
-/**
  * Calculate the DFS divider value for the specified reference clock
  * and target output frequency.
  * @param [in] reference the reference clock frequency, in Hz.
@@ -101,32 +63,6 @@ static uint8_t fuzzy_divider(uint64_t reference, uint64_t target)
 }
 
 /**
- * Get the SoC device index for the specified soc_dfs_device.
- * @param [in] device SoC DFS device.
- * @returns device index if exists on current chip family.
- * @returns -1 if the device is not available on the current chip family.
- */
-static int8_t soc_device_index(enum soc_dfs_device device)
-{
-	uint8_t i;
-	if (device == INVALID_DFS_DEVICE)
-		return -1;
-
-	if (is_nlm_xlp8xx()) {
-		for (i=0; i < COUNT_OF(soc_device_index_xlp8xx); i++) {
-			if (soc_device_index_xlp8xx[i] == device)
-				return i;
-		}
-	} else if (is_nlm_xlp3xx()) {
-		for (i=0; i < COUNT_OF(soc_device_index_xlp3xx); i++) {
-			if (soc_device_index_xlp3xx[i] == device)
-				return i;
-		}
-	}
-	return -1;
-}
-
-/**
  * @returns the numerator for the reference clock frequency.
  */
 static inline uint64_t ref_clk_num(void)
@@ -302,11 +238,9 @@ static inline uint64_t ddr_pll_freq(int node)
  * Get the DFS divider value for the specified SoC device.
  * @param [in] device the SoC device.
  */
-static inline uint64_t soc_dfs_val(int node, enum soc_dfs_device device)
+static inline uint64_t soc_dfs_val(int node, soc_device_id_t device)
 {
-	uint8_t device_index = soc_device_index(device);
-	if (device_index == -1)
-		return 0;
+	uint8_t device_index = device;
 	if (device_index >= 8)
 	{
 		device_index -= 8;
@@ -330,11 +264,9 @@ static inline uint64_t core_dfs_val(int node, uint8_t core)
  * @returns 1 if the SoC device's DFS is bypassed.
  * @returns 0 if the SoC device's DFS is not bypassed.
  */
-static inline uint8_t is_soc_dfs_bypassed(int node, enum soc_dfs_device device)
+static inline uint8_t is_soc_dfs_bypassed(int node, soc_device_id_t device)
 {
-	uint8_t device_index = soc_device_index(device);
-	if (device_index == -1)
-		return 0;
+	uint8_t device_index = device;
 	return (nlm_hal_read_sys_reg(node, SYS_DFS_BYP_CTRL) >> device_index) & 1;
 }
 
@@ -343,13 +275,10 @@ static inline uint8_t is_soc_dfs_bypassed(int node, enum soc_dfs_device device)
  * @param [in] device the SoC device.
  * @param [in] bypass 1: bypass the DFS. 0: do not bypass DFS.
  */
-static inline void set_soc_dfs_bypass(int node, enum soc_dfs_device device, uint8_t bypass)
+static inline void set_soc_dfs_bypass(int node, soc_device_id_t device, uint8_t bypass)
 {
-	uint8_t device_index = soc_device_index(device);
+	uint8_t device_index = device;
 	uint32_t val;
-
-	if (device_index == -1)
-		return;
 	val = nlm_hal_read_sys_reg(node, SYS_DFS_BYP_CTRL) & ~(1 << device_index);
 	nlm_hal_write_sys_reg(node, SYS_DFS_BYP_CTRL, val | ((bypass? 0x1 : 0x0) << device_index));
 }
@@ -381,12 +310,9 @@ static inline void set_core_dfs_bypass(int node, uint8_t core, uint8_t bypass)
  * @param [in] device the SoC device.
  * @returns The SoC device operating frequency, in Hz.
  */
-uint64_t nlm_hal_get_soc_freq(int node, enum soc_dfs_device device)
+uint64_t nlm_hal_get_soc_freq(int node, soc_device_id_t device)
 {
 	uint64_t reference, den;
-	if (soc_device_index(device) == -1)
-		return 0;
-
 	switch (device) {
 		case DFS_DEVICE_NAND:
 		case DFS_DEVICE_NOR:
@@ -422,16 +348,13 @@ uint64_t nlm_hal_get_soc_freq(int node, enum soc_dfs_device device)
  * @param [in] device the SoC device.
  * @param [in] dfs_index DFS index (**not** the DFS value).
  */
-static void step_soc_dfs(int node, enum soc_dfs_device device, uint8_t dfs_index)
+static void step_soc_dfs(int node, soc_device_id_t device, uint8_t dfs_index)
 {
-	uint8_t device_index = soc_device_index(device);
+	uint8_t device_index = device;
 	uint8_t cur = closest_dfs_index(soc_dfs_val(node, device));
 	int8_t delta = cur - dfs_index;
 	int i;
 
-	if (device_index == -1)
-		return;
-
 	if (delta >= 0) {
 		/* positive delta, decrement dfs */
 		for (i=0; i < delta; i++)
@@ -450,7 +373,7 @@ static void step_soc_dfs(int node, enum soc_dfs_device device, uint8_t dfs_index
  * @param [in] freq target SoC device frequency, in Hz.
  * @returns the new SoC device operating frequency, in Hz.
  */
-uint64_t nlm_hal_set_soc_freq(int node, enum soc_dfs_device device, uint64_t freq)
+uint64_t nlm_hal_set_soc_freq(int node, soc_device_id_t device, uint64_t freq)
 {
 	uint64_t reference;
 	uint8_t  target;
@@ -562,9 +485,37 @@ unsigned long long nlm_hal_cpu_freq(void)
 	return nlm_hal_get_core_freq(nlm_node_id(), core);
 }
 
+uint8_t nlm_hal_get_soc_clock_state(int node, soc_device_id_t device)
+{
+	return nlm_hal_read_sys_reg(node, SYS_DFS_DIS_CTRL) >> device;
+}
+
+void nlm_hal_soc_clock_enable(int node, soc_device_id_t device)
+{
+	uint32_t d32 = nlm_hal_read_sys_reg(node, SYS_DFS_DIS_CTRL);
+	d32 &= ~(1<<device);
+	nlm_hal_write_sys_reg(node, SYS_DFS_DIS_CTRL, d32);
+}
+
+void nlm_hal_soc_clock_disable(int node, soc_device_id_t device)
+{
+	uint32_t d32 = nlm_hal_read_sys_reg(node, SYS_DFS_DIS_CTRL);
+	d32 |= (1<<device);
+	nlm_hal_write_sys_reg(node, SYS_DFS_DIS_CTRL, d32);
+}
+
+void nlm_hal_soc_clock_reset(int node, soc_device_id_t device)
+{
+	nlm_hal_write_sys_reg(node, SYS_DFS_RST_CTRL, 1 << device);
+}
+
 #ifdef NLM_HAL_LINUX_KERNEL
 #include <linux/types.h>
 #include <linux/module.h>
+EXPORT_SYMBOL(nlm_hal_get_soc_clock_state);
+EXPORT_SYMBOL(nlm_hal_soc_clock_enable);
+EXPORT_SYMBOL(nlm_hal_soc_clock_disable);
+EXPORT_SYMBOL(nlm_hal_soc_clock_reset);
 EXPORT_SYMBOL(nlm_hal_get_soc_freq);
 EXPORT_SYMBOL(nlm_hal_set_soc_freq);
 EXPORT_SYMBOL(nlm_hal_get_core_freq);
diff --git a/arch/mips/netlogic/common/smp.c b/arch/mips/netlogic/common/smp.c
new file mode 100644
index 0000000..6a1c325
--- /dev/null
+++ b/arch/mips/netlogic/common/smp.c
@@ -0,0 +1,154 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#NETL_2#********************************/
+
+#include <asm/addrspace.h>
+#include <asm/smp.h>
+#include <linux/sched.h>
+#include <linux/types.h>
+#include <linux/hardirq.h>
+#include <linux/module.h>
+
+
+#include <asm/netlogic/sim.h>
+#include <asm/netlogic/mips-exts.h>
+#include <asm/netlogic/pic.h>
+#include <asm/netlogic/msgring.h>
+#include <asm/netlogic/xlp_irq.h>
+
+/* ipi statistics counters for debugging */
+__u32 ipi_3_counter_tx[NR_CPUS][NR_CPUS];
+__u32 ipi_3_counter_rx[NR_CPUS];
+
+extern void save_epc(unsigned long *epc);
+extern void smp_call_function_interrupt(void);
+static int nlm_common_ipi_stats[NR_CPUS];
+static unsigned long nlm_common_ipi_epc[NR_CPUS];
+
+#ifdef CONFIG_NLM_XLR
+#define SET_IPI_VECTOR(x, y) x |= y
+
+void core_send_ipi(int logical_cpu, unsigned int action)
+{
+	int cpu = cpu_logical_map(logical_cpu);
+	__u32 tid = cpu & 0x3;
+	__u32 pid = (cpu >> 2) & 0x07;
+	__u32 ipi = (tid << 16) | (pid << 20);
+
+	if (action & SMP_CALL_FUNCTION) {
+		SET_IPI_VECTOR(ipi, IRQ_IPI_SMP_FUNCTION);
+	} else if (action & SMP_RESCHEDULE_YOURSELF) {
+		SET_IPI_VECTOR(ipi, IRQ_IPI_SMP_RESCHEDULE);
+	} else if (action & SMP_CALL_KGDB_HOOK) {
+		SET_IPI_VECTOR(ipi, IRQ_IPI_SMP_KGDB);
+		/* set NMI also for KGDB */
+		SET_IPI_VECTOR(ipi, (1 << 8));
+	} else if (action & SMP_OPROFILE_IPI) {
+		SET_IPI_VECTOR(ipi, IRQ_IPI_OPROFILE);
+	}
+#ifdef CONFIG_NLMCOMMON_IP_FLOW_AFFINITY
+	else if (action & SMP_NETRX_IPI) {
+		SET_IPI_VECTOR(ipi, IRQ_IPI_NETRX);
+	}
+#endif				/* CONFIG_NLMCOMMON_IP_FLOW_AFFINITY */
+	else
+		return;
+
+	pic_send_ipi(ipi);
+}
+#endif
+
+extern __u64 nlm_common_irq_mask;
+
+void nlm_common_smp_finish(void)
+{
+#if !defined(CONFIG_NLM_XLP)
+	nlm_common_msgring_cpu_init();
+#endif
+}
+
+#ifdef CONFIG_NLMCOMMON_IP_FLOW_AFFINITY
+extern void skb_transfer_finish(void);
+#endif				/* CONFIG_NLMCOMMON_IP_FLOW_AFFINITY */
+
+#ifdef CONFIG_SMP
+/* IRQ_IPI_SMP_FUNCTION Handler */
+void nlm_smp_function_ipi_handler(unsigned int irq, struct irq_desc *desc)
+{
+	nlm_common_ipi_stats[smp_processor_id()]++;
+	save_epc(&nlm_common_ipi_epc[smp_processor_id()]);
+	smp_call_function_interrupt();
+	nlm_common_ipi_stats[smp_processor_id()]--;
+}
+
+/* IRQ_IPI_SMP_RESCHEDULE  handler */
+void nlm_smp_resched_ipi_handler(unsigned int irq, struct irq_desc *desc)
+{
+	nlm_common_ipi_stats[smp_processor_id()]++;
+	save_epc(&nlm_common_ipi_epc[smp_processor_id()]);
+
+	/* Announce that we are for reschduling */
+	set_need_resched();
+	nlm_common_ipi_stats[smp_processor_id()]--;
+}
+
+#ifdef CONFIG_NLMCOMMON_IP_FLOW_AFFINITY
+/* nlm_ip_flow_ipi_handler HANDLER */
+void nlm_ip_flow_ipi_handler(unsigned int irq, struct irq_desc *desc)
+{
+	nlm_common_ipi_stats[smp_processor_id()]++;
+	save_epc(&nlm_common_ipi_epc[smp_processor_id()]);
+
+	/* do_IRQ called irq_enter() before calling this desc->handler */
+	skb_transfer_finish();
+	nlm_common_ipi_stats[smp_processor_id()]--;
+
+}
+#endif /* CONFIG_NLMCOMMON_IP_FLOW_AFFINITY */
+#endif
+
+void nlm_common_ipi_handler(int irq, struct pt_regs *regs)
+{
+	nlm_common_ipi_stats[smp_processor_id()]++;
+	save_epc(&nlm_common_ipi_epc[smp_processor_id()]);
+
+	if (irq == NLM_IRQ_IPI_SMP_FUNCTION) {
+		smp_call_function_interrupt();
+	}
+#ifdef CONFIG_NLMCOMMON_IP_FLOW_AFFINITY
+	else if (irq == IRQ_IPI_NETRX) {
+		irq_enter();
+
+		skb_transfer_finish();
+
+		/* run soft IRQ at the end */
+		irq_exit();
+	}
+#endif	/* CONFIG_NLMCOMMON_IP_FLOW_AFFINITY */
+	else {
+		/* Announce that we are for reschduling */
+		set_need_resched();
+	}
+	nlm_common_ipi_stats[smp_processor_id()]--;
+}
diff --git a/arch/mips/netlogic/xlp/Makefile b/arch/mips/netlogic/xlp/Makefile
index 97fe167..123c14f 100644
--- a/arch/mips/netlogic/xlp/Makefile
+++ b/arch/mips/netlogic/xlp/Makefile
@@ -6,5 +6,7 @@ obj-y 				+= irq.o time.o on_chip.o
 obj-$(CONFIG_NLM_XLP) 		+= platform.o board.o
 obj-$(CONFIG_NLM_XLP) 		+= xlp_hal_pic.o xlp_gpio.o i2c.o
 obj-$(CONFIG_SMP)       	+= smp.o
+obj-y				+= pic/
 
 obj-$(CONFIG_NLM_XLP) += cpu_control.o cpu_control_asm.o nmi.o
+obj-$(CONFIG_NUMA) += numa.o
diff --git a/arch/mips/netlogic/xlp/irq.c b/arch/mips/netlogic/xlp/irq.c
index 4b21ca9..096bfa3 100644
--- a/arch/mips/netlogic/xlp/irq.c
+++ b/arch/mips/netlogic/xlp/irq.c
@@ -39,10 +39,8 @@ THE POSSIBILITY OF SUCH DAMAGE.
 #include <asm/ptrace.h>
 #include <asm/kgdb.h>
 #include <asm/mipsregs.h>
-#include <asm/time.h>
 
-#include <asm/netlogic/cpumask.h>
-#include <asm/netlogic/xlp_hal_pic.h>
+#include <asm/netlogic/xlp_irq.h>
 #include <asm/netlogic/xlp.h>
 #include <asm/netlogic/xlp_irq.h>
 #include <asm/netlogic/msidef.h>
@@ -60,31 +58,28 @@ THE POSSIBILITY OF SUCH DAMAGE.
  */
 
 /* Externs */
+extern int xlp_span_multiple_nodes(const struct cpumask *);
+extern void constrict_mask_to_node(u8, struct cpumask *, const struct cpumask*);
 extern void nlm_common_timer_interrupt(struct pt_regs *, int);
 extern void nlm_oprofile_interrupt(struct pt_regs *, int);
 extern void nlm_xlp_msgring_int_handler(int , struct pt_regs *);
-extern int xlp_ctrl_fn_from_dev(const struct pci_dev *);
-extern const struct cpumask *xlp_closest_match_cpumask(struct cpumask *);
-int xlp_intx_enable(int);
-int xlp_intx_disable(int);
-extern void xlp_set_cpumask(const struct cpumask *m, int irt);
-void xlp_ctrl_intmode_add(int , int, int);
-extern int xlp_get_ctrl_intmode(int);
-extern int xlp_set_ctrl_intmode(int fn, int mode);
+extern int xlp_ctrl_fn_from_dev(const struct pci_dev *, struct xlp_nodefn_struct *);
+int xlp_intx_enable(u8, int);
+int xlp_intx_disable(u8, int);
+extern void xlp_set_cpumask_on_nodes(const struct cpumask *m, int irt);
+extern int xlp_get_ctrl_intmode(u8, int);
+extern int xlp_set_ctrl_intmode(u8, int fn, int mode);
 #if defined CONFIG_PCI_MSI
-extern int xlp_msi_status_clear(struct pci_dev *, int);
-int xlp_msi_enable(int, u32);
-extern int xlp_msi_base_vector(struct pci_dev *);
-extern int is_msi_set(int);
-extern int calc_msi_vector_offset(int);
-int xlp_msi_disable(int, int);
-extern u32 xlp_msi_set_mask(int, int, int);
-volatile const void *xlp_msix_addr_start(int);
-volatile const void *xlp_msi_addr_start(int);
-extern u32 xlp_msix_status_clear(int);
-extern u32 xlp_msix_set_mask(int, int, int);
-int xlp_msix_enable(int);
-int xlp_msix_disable(int);
+int xlp_msi_enable(u8, int, u32);
+extern int is_msi_set(u8, int);
+extern int calc_msi_vector_offset(u8, int);
+int xlp_msi_disable(u8, int, int);
+extern u32 xlp_msi_set_mask(u8, int, int, int);
+volatile const void *xlp_msix_addr_start(u8, int);
+volatile const void *xlp_msi_addr_start(u8, int);
+extern u32 xlp_msix_status_clear(u8, int);
+int xlp_msix_enable(u8,  int);
+int xlp_msix_disable(u8, int);
 void mask_msi_irq(unsigned int);
 void unmask_msi_irq(unsigned int);
 #endif
@@ -115,8 +110,8 @@ struct msi_alloc_bitmap {
 	u64 bitmap;	/* Can be any data structure to keep bits */
 	u32 count;	/* #of bits set at any point of time */
 };
-static struct msi_alloc_bitmap msix_vec[XLP_MAX_SLOTS];
-static struct msi_alloc_bitmap msi_vec[XLP_MAX_SLOTS];
+static struct msi_alloc_bitmap msix_vec[NLM_MAX_NODES][XLP_MAX_SLOTS];
+static struct msi_alloc_bitmap msi_vec[NLM_MAX_NODES][XLP_MAX_SLOTS];
 #endif
 
 /*
@@ -162,179 +157,179 @@ static struct msi_alloc_bitmap msi_vec[XLP_MAX_SLOTS];
  */
 struct irq_map_elem {
 	int rvec;
-	int usage;	/* This is the usage count of an rvec, not the number
+	int usage[NLM_MAX_NODES];	/* This is the usage count of an rvec, not the number
 			   of times a vector is used in s/w */
 };
 
 static struct irq_map_elem irq_map[NR_IRQS] = {
-	{0, 0},	/* Dummy			:	0 */
-	{0, 0},	/* Dummy			:	1 */
-	{0, 0},	/* Dummy			:	2 */
-	{0, 0},	/* Dummy			:	3 */
-	{0, 0},	/* Dummy			:	4 */
-	{0, 0},	/* Dummy			:	5 */
-	{0, 0},	/* Dummy			:	6 */
-	{0, 0},	/* Dummy			:	7 */
-        {9, 0}, /*XLP_WD_IDX(0)			:	8 */
-        {9, 0}, /*XLP_WD_IDX(1)			:	9 */
-        {19, 0}, /*XLP_WD_NMI_IDX(0)		:	10 */
-        {19, 0}, /*XLP_WD_NMI_IDX(1)		:	11 */
-        {10, 0}, /*XLP_TIMER_IDX(0)		:	12 */
-        {10, 0}, /*XLP_TIMER_IDX(1)		:	13 */
-        {10, 0}, /*XLP_TIMER_IDX(2)		:	14 */
-        {10, 0}, /*XLP_TIMER_IDX(3)		:	15 */
-        {10, 0}, /*XLP_TIMER_IDX(4)		:	16 */
-        {10, 0}, /*XLP_TIMER_IDX(5)		:	17 */
-        {10, 0}, /*XLP_TIMER_IDX(6)		:	18 */
-        {10, 0}, /*XLP_TIMER_IDX(7)		:	19 */
-        {59, 0}, /*XLP_MSGQ_IDX(0)		:	20 */
-        {59, 0}, /*XLP_MSGQ_IDX(1)		:	21 */
-        {59, 0}, /*XLP_MSGQ_IDX(2)		:	22 */
-        {59, 0}, /*XLP_MSGQ_IDX(3)		:	23 */
-        {59, 0}, /*XLP_MSGQ_IDX(4)		:	24 */
-        {59, 0}, /*XLP_MSGQ_IDX(5)		:	25 */
-        {59, 0}, /*XLP_MSGQ_IDX(6)		:	26 */
-        {59, 0}, /*XLP_MSGQ_IDX(7)		:	27 */
-        {59, 0}, /*XLP_MSGQ_IDX(8)		:	28 */
-        {59, 0}, /*XLP_MSGQ_IDX(9)		:	29 */
-        {59, 0}, /*XLP_MSGQ_IDX(10)		:	30 */
-        {59, 0}, /*XLP_MSGQ_IDX(11)		:	31 */
-        {59, 0}, /*XLP_MSGQ_IDX(12)		:	32 */
-        {59, 0}, /*XLP_MSGQ_IDX(13)		:	33 */
-        {59, 0}, /*XLP_MSGQ_IDX(14)		:	34 */
-        {59, 0}, /*XLP_MSGQ_IDX(15)		:	35 */
-        {59, 0}, /*XLP_MSGQ_IDX(16)		:	36 */
-        {59, 0}, /*XLP_MSGQ_IDX(17)		:	37 */
-        {59, 0}, /*XLP_MSGQ_IDX(18)		:	38 */
-        {59, 0}, /*XLP_MSGQ_IDX(19)		:	39 */
-        {59, 0}, /*XLP_MSGQ_IDX(20)		:	40 */
-        {59, 0}, /*XLP_MSGQ_IDX(21)		:	41 */
-        {59, 0}, /*XLP_MSGQ_IDX(22)		:	42 */
-        {59, 0}, /*XLP_MSGQ_IDX(23)		:	43 */
-        {59, 0}, /*XLP_MSGQ_IDX(24)		:	44 */
-        {59, 0}, /*XLP_MSGQ_IDX(25)		:	45 */
-        {59, 0}, /*XLP_MSGQ_IDX(26)		:	46 */
-        {59, 0}, /*XLP_MSGQ_IDX(27)		:	47 */
-        {59, 0}, /*XLP_MSGQ_IDX(28)		:	48 */
-        {59, 0}, /*XLP_MSGQ_IDX(29)		:	49 */
-        {59, 0}, /*XLP_MSGQ_IDX(30)		:	50 */
-        {59, 0}, /*XLP_MSGQ_IDX(31)		:	51 */
-        {49, 0}, /*XLP_MSG_IDX(0)		:	52 */
-        {48, 0}, /*XLP_MSG_IDX(1)		:	53 */
-        {32, 0}, /*XLP_PCIE_MSIX_IDX(0)		:	54 */
-        {32, 0}, /*XLP_PCIE_MSIX_IDX(1)		:	55 */
-        {32, 0}, /*XLP_PCIE_MSIX_IDX(2)		:	56 */
-        {32, 0}, /*XLP_PCIE_MSIX_IDX(3)		:	57 */
-        {32, 0}, /*XLP_PCIE_MSIX_IDX(4)		:	58 */
-        {32, 0}, /*XLP_PCIE_MSIX_IDX(5)		:	59 */
-        {32, 0}, /*XLP_PCIE_MSIX_IDX(6)		:	60 */
-        {32, 0}, /*XLP_PCIE_MSIX_IDX(7)		:	61 */
-        {33, 0}, /*XLP_PCIE_MSIX_IDX(8)		:	62 */
-        {33, 0}, /*XLP_PCIE_MSIX_IDX(9)		:	63 */
-        {33, 0}, /*XLP_PCIE_MSIX_IDX(10)	:	64 */
-        {33, 0}, /*XLP_PCIE_MSIX_IDX(11)	:	65 */
-        {33, 0}, /*XLP_PCIE_MSIX_IDX(12)	:	66 */
-        {33, 0}, /*XLP_PCIE_MSIX_IDX(13)	:	67 */
-        {33, 0}, /*XLP_PCIE_MSIX_IDX(14)	:	68 */
-        {33, 0}, /*XLP_PCIE_MSIX_IDX(15)	:	69 */
-        {34, 0}, /*XLP_PCIE_MSIX_IDX(16)	:	70 */
-        {34, 0}, /*XLP_PCIE_MSIX_IDX(17)	:	71 */
-        {34, 0}, /*XLP_PCIE_MSIX_IDX(18)	:	72 */
-        {34, 0}, /*XLP_PCIE_MSIX_IDX(19)	:	73 */
-        {34, 0}, /*XLP_PCIE_MSIX_IDX(20)	:	74 */
-        {34, 0}, /*XLP_PCIE_MSIX_IDX(21)	:	75 */
-        {34, 0}, /*XLP_PCIE_MSIX_IDX(22)	:	76 */
-        {34, 0}, /*XLP_PCIE_MSIX_IDX(23)	:	77 */
-        {35, 0}, /*XLP_PCIE_MSIX_IDX(24)	:	78 */
-        {35, 0}, /*XLP_PCIE_MSIX_IDX(25)	:	79 */
-        {35, 0}, /*XLP_PCIE_MSIX_IDX(26)	:	80 */
-        {35, 0}, /*XLP_PCIE_MSIX_IDX(27)	:	81 */
-        {35, 0}, /*XLP_PCIE_MSIX_IDX(28)	:	82 */
-        {35, 0}, /*XLP_PCIE_MSIX_IDX(29)	:	83 */
-        {35, 0}, /*XLP_PCIE_MSIX_IDX(30)	:	84 */
-        {35, 0}, /*XLP_PCIE_MSIX_IDX(31)	:	85 */
-        {41, 0}, /*XLP_PCIE_LINK_IRQ(0)		:	86 */
-        {42, 0}, /*XLP_PCIE_LINK_IRQ(1)		:	87 */
-        {43, 0}, /*XLP_PCIE_LINK_IRQ(2)		:	88 */
-        {44, 0}, /*XLP_PCIE_LINK_IRQ(3)		:	89 */
-        {58, 0}, /*XLP_NAE_IDX(0)		:	90 */
-        {58, 0}, /*XLP_NAE_IDX(1)		:	91 */
-        {58, 0}, /*XLP_NAE_IDX(2)		:	92 */
-        {58, 0}, /*XLP_NAE_IDX(3)		:	93 */
-        {58, 0}, /*XLP_NAE_IDX(4)		:	94 */
-        {58, 0}, /*XLP_NAE_IDX(5)		:	95 */
-        {58, 0}, /*XLP_NAE_IDX(6)		:	96 */
-        {58, 0}, /*XLP_NAE_IDX(7)		:	97 */
-        {58, 0}, /*XLP_NAE_IDX(8)		:	98 */
-        {58, 0}, /*XLP_NAE_IDX(9)		:	99 */
-        {58, 0}, /*XLP_NAE_IDX(10)		:	100 */
-        {58, 0}, /*XLP_NAE_IDX(11)		:	101 */
-        {58, 0}, /*XLP_NAE_IDX(12)		:	102 */
-        {58, 0}, /*XLP_NAE_IDX(13)		:	103 */
-        {58, 0}, /*XLP_NAE_IDX(14)		:	104 */
-        {58, 0}, /*XLP_NAE_IDX(15)		:	105 */
-        {58, 0}, /*XLP_NAE_IDX(16)		:	106 */
-        {58, 0}, /*XLP_NAE_IDX(17)		:	107 */
-        {58, 0}, /*XLP_NAE_IDX(18)		:	108 */
-        {58, 0}, /*XLP_NAE_IDX(19)		:	109 */
-        {58, 0}, /*XLP_NAE_IDX(20)		:	110 */
-        {58, 0}, /*XLP_NAE_IDX(21)		:	111 */
-        {58, 0}, /*XLP_NAE_IDX(22)		:	112 */
-        {58, 0}, /*XLP_NAE_IDX(23)		:	113 */
-        {58, 0}, /*XLP_NAE_IDX(24)		:	114 */
-        {58, 0}, /*XLP_NAE_IDX(25)		:	115 */
-        {58, 0}, /*XLP_NAE_IDX(26)		:	116 */
-        {58, 0}, /*XLP_NAE_IDX(27)		:	117 */
-        {58, 0}, /*XLP_NAE_IDX(28)		:	118 */
-        {58, 0}, /*XLP_NAE_IDX(29)		:	119 */
-        {58, 0}, /*XLP_NAE_IDX(30)		:	120 */
-        {58, 0}, /*XLP_NAE_IDX(31)		:	121 */
-        {60, 0}, /*XLP_POE_IDX			:	122 */
-        {24, 0}, /*XLP_USB_IDX(0)		:	123 */
-        {24, 0}, /*XLP_USB_IDX(1)		:	124 */
-        {24, 0}, /*XLP_USB_IDX(2)		:	125 */
-        {25, 0}, /*XLP_USB_IDX(3)		:	126 */
-        {25, 0}, /*XLP_USB_IDX(4)		:	127 */
-        {25, 0}, /*XLP_USB_IDX(5)		:	128 */
-        {61, 0}, /*XLP_GDX_IDX			:	129 */
-        {63, 0}, /*XLP_SEC_IDX			:	130 */
-        {62, 0}, /*XLP_RSA_IDX			:	131 */
-        {39, 0}, /*XLP_COMP_IDX(0)		:	132 */
-        {39, 0}, /*XLP_COMP_IDX(1)		:	133 */
-        {39, 0}, /*XLP_COMP_IDX(2)		:	134 */
-        {39, 0}, /*XLP_COMP_IDX(3)		:	135 */
-        {0, 0}, /*RESERVED_IDX			:	136 */
-        {37, 0}, /*XLP_ICC_IDX(0)		:	137  ICC - Inter Chip Coherency*/
-        {37, 0}, /*XLP_ICC_IDX(1)		:	138 */
-        {37, 0}, /*XLP_ICC_IDX(2)		:	139 */
-        {36, 0}, /*XLP_CAM_IDX			:	140 */
-        {17, 0}, /*XLP_UART_IDX(0)		:	141 */
-        {18, 0}, /*XLP_UART_IDX(0)		:	142 */
-        {11, 0}, /*XLP_I2C_IDX(0)		:	143 */
-        {11, 0}, /*XLP_I2C_IDX(0)		:	144 */
-        {12, 0}, /*XLP_SYS_IDX(0)		:	145 */
-        {12, 0}, /*XLP_SYS_IDX(1)		:	146 */
-        {55, 0}, /*XLP_JTAG_IDX			:	147 */
-        {50, 0}, /*XLP_PIC_IDX			:	148 */
-        {54, 0}, /*XLP_NBU_IDX			:	149 */
-        {53, 0}, /*XLP_TCU_IDX			:	150 */
-        {31, 0}, /*XLP_SATA			:	151 */
-        {38, 0}, /*XLP_DMC_IDX			:	152 */	/* collision */
-        {38, 0}, /*XLP_DMC_IDX			:	153 */
-        {13, 0}, /*XLP_GPIO_IDX(0)		:	154 */
-        {14, 0}, /*XLP_GPIO_IDX(1)		:	155 */
-        {15, 0}, /*XLP_GPIO_IDX(2)		:	156 */
-        {16, 0}, /*XLP_GPIO_IDX(3)		:	157 */
-        {20, 0}, /*XLP_NOR_IDX			:	158 */
-        {21, 0}, /*XLP_NAND_IDX			:	159 */
-        {22, 0}, /*XLP_SPI_IDX			:	160 */
-        {23, 0}, /*XLP_MMC_IDX			:	161 */
-        {0, 0}, /*			    162 */
-        {0, 0}, /*                          163 */
-        {0, 0}, /*                          164 */
-        {0, 0}, /*                          165 */
-        {0, 0}, /*                          166 */
-        {0, 0}, /*                          167 */
+	{0, {0,0,0,0}},	/* Dummy			:	0 */
+	{0, {0,0,0,0}},	/* Dummy			:	1 */
+	{0, {0,0,0,0}},	/* Dummy			:	2 */
+	{0, {0,0,0,0}},	/* Dummy			:	3 */
+	{0, {0,0,0,0}},	/* Dummy			:	4 */
+	{0, {0,0,0,0}},	/* Dummy			:	5 */
+	{0, {0,0,0,0}},	/* Dummy			:	6 */
+	{0, {0,0,0,0}},	/* Dummy			:	7 */
+        {9, {0,0,0,0}}, /*XLP_WD_IDX(0)			:	8 */
+        {9, {0,0,0,0}}, /*XLP_WD_IDX(1)			:	9 */
+        {19, {0,0,0,0}}, /*XLP_WD_NMI_IDX(0)		:	10 */
+        {19, {0,0,0,0}}, /*XLP_WD_NMI_IDX(1)		:	11 */
+        {26, {0,0,0,0}}, /*XLP_TIMER_IDX(0): Dedicated systimer	:12 */
+        {10, {0,0,0,0}}, /*XLP_TIMER_IDX(1): Dedicated clocksource:13 */
+        {10, {0,0,0,0}}, /*XLP_TIMER_IDX(2)		:	14 */
+        {10, {0,0,0,0}}, /*XLP_TIMER_IDX(3)		:	15 */
+        {10, {0,0,0,0}}, /*XLP_TIMER_IDX(4)		:	16 */
+        {10, {0,0,0,0}}, /*XLP_TIMER_IDX(5)		:	17 */
+        {10, {0,0,0,0}}, /*XLP_TIMER_IDX(6)		:	18 */
+        {10, {0,0,0,0}}, /*XLP_TIMER_IDX(7)		:	19 */
+        {30, {0,0,0,0}}, /*XLP_MSGQ_IDX(0)		:	20 */
+        {30, {0,0,0,0}}, /*XLP_MSGQ_IDX(1)		:	21 */
+        {30, {0,0,0,0}}, /*XLP_MSGQ_IDX(2)		:	22 */
+        {30, {0,0,0,0}}, /*XLP_MSGQ_IDX(3)		:	23 */
+        {30, {0,0,0,0}}, /*XLP_MSGQ_IDX(4)		:	24 */
+        {30, {0,0,0,0}}, /*XLP_MSGQ_IDX(5)		:	25 */
+        {30, {0,0,0,0}}, /*XLP_MSGQ_IDX(6)		:	26 */
+        {30, {0,0,0,0}}, /*XLP_MSGQ_IDX(7)		:	27 */
+        {30, {0,0,0,0}}, /*XLP_MSGQ_IDX(8)		:	28 */
+        {30, {0,0,0,0}}, /*XLP_MSGQ_IDX(9)		:	29 */
+        {30, {0,0,0,0}}, /*XLP_MSGQ_IDX(10)		:	30 */
+        {30, {0,0,0,0}}, /*XLP_MSGQ_IDX(11)		:	31 */
+        {59, {0,0,0,0}}, /*XLP_MSGQ_IDX(12)		:	32 */
+        {59, {0,0,0,0}}, /*XLP_MSGQ_IDX(13)		:	33 */
+        {59, {0,0,0,0}}, /*XLP_MSGQ_IDX(14)		:	34 */
+        {59, {0,0,0,0}}, /*XLP_MSGQ_IDX(15)		:	35 */
+        {59, {0,0,0,0}}, /*XLP_MSGQ_IDX(16)		:	36 */
+        {59, {0,0,0,0}}, /*XLP_MSGQ_IDX(17)		:	37 */
+        {59, {0,0,0,0}}, /*XLP_MSGQ_IDX(18)		:	38 */
+        {59, {0,0,0,0}}, /*XLP_MSGQ_IDX(19)		:	39 */
+        {59, {0,0,0,0}}, /*XLP_MSGQ_IDX(20)		:	40 */
+        {59, {0,0,0,0}}, /*XLP_MSGQ_IDX(21)		:	41 */
+        {59, {0,0,0,0}}, /*XLP_MSGQ_IDX(22)		:	42 */
+        {59, {0,0,0,0}}, /*XLP_MSGQ_IDX(23)		:	43 */
+        {59, {0,0,0,0}}, /*XLP_MSGQ_IDX(24)		:	44 */
+        {59, {0,0,0,0}}, /*XLP_MSGQ_IDX(25)		:	45 */
+        {59, {0,0,0,0}}, /*XLP_MSGQ_IDX(26)		:	46 */
+        {59, {0,0,0,0}}, /*XLP_MSGQ_IDX(27)		:	47 */
+        {59, {0,0,0,0}}, /*XLP_MSGQ_IDX(28)		:	48 */
+        {59, {0,0,0,0}}, /*XLP_MSGQ_IDX(29)		:	49 */
+        {59, {0,0,0,0}}, /*XLP_MSGQ_IDX(30)		:	50 */
+        {59, {0,0,0,0}}, /*XLP_MSGQ_IDX(31)		:	51 */
+        {49, {0,0,0,0}}, /*XLP_MSG_IDX(0)		:	52 */
+        {48, {0,0,0,0}}, /*XLP_MSG_IDX(1)		:	53 */
+        {32, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(0)		:	54 */
+        {32, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(1)		:	55 */
+        {32, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(2)		:	56 */
+        {32, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(3)		:	57 */
+        {32, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(4)		:	58 */
+        {32, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(5)		:	59 */
+        {32, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(6)		:	60 */
+        {32, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(7)		:	61 */
+        {33, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(8)		:	62 */
+        {33, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(9)		:	63 */
+        {33, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(10)	:	64 */
+        {33, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(11)	:	65 */
+        {33, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(12)	:	66 */
+        {33, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(13)	:	67 */
+        {33, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(14)	:	68 */
+        {33, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(15)	:	69 */
+        {34, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(16)	:	70 */
+        {34, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(17)	:	71 */
+        {34, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(18)	:	72 */
+        {34, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(19)	:	73 */
+        {34, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(20)	:	74 */
+        {34, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(21)	:	75 */
+        {34, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(22)	:	76 */
+        {34, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(23)	:	77 */
+        {35, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(24)	:	78 */
+        {35, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(25)	:	79 */
+        {35, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(26)	:	80 */
+        {35, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(27)	:	81 */
+        {35, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(28)	:	82 */
+        {35, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(29)	:	83 */
+        {35, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(30)	:	84 */
+        {35, {0,0,0,0}}, /*XLP_PCIE_MSIX_IDX(31)	:	85 */
+        {41, {0,0,0,0}}, /*XLP_PCIE_LINK_IRQ(0)		:	86 */
+        {42, {0,0,0,0}}, /*XLP_PCIE_LINK_IRQ(1)		:	87 */
+        {43, {0,0,0,0}}, /*XLP_PCIE_LINK_IRQ(2)		:	88 */
+        {44, {0,0,0,0}}, /*XLP_PCIE_LINK_IRQ(3)		:	89 */
+        {58, {0,0,0,0}}, /*XLP_NAE_IDX(0)		:	90 */
+        {58, {0,0,0,0}}, /*XLP_NAE_IDX(1)		:	91 */
+        {58, {0,0,0,0}}, /*XLP_NAE_IDX(2)		:	92 */
+        {58, {0,0,0,0}}, /*XLP_NAE_IDX(3)		:	93 */
+        {58, {0,0,0,0}}, /*XLP_NAE_IDX(4)		:	94 */
+        {58, {0,0,0,0}}, /*XLP_NAE_IDX(5)		:	95 */
+        {58, {0,0,0,0}}, /*XLP_NAE_IDX(6)		:	96 */
+        {58, {0,0,0,0}}, /*XLP_NAE_IDX(7)		:	97 */
+        {58, {0,0,0,0}}, /*XLP_NAE_IDX(8)		:	98 */
+        {58, {0,0,0,0}}, /*XLP_NAE_IDX(9)		:	99 */
+        {58, {0,0,0,0}}, /*XLP_NAE_IDX(10)		:	100 */
+        {58, {0,0,0,0}}, /*XLP_NAE_IDX(11)		:	101 */
+        {58, {0,0,0,0}}, /*XLP_NAE_IDX(12)		:	102 */
+        {58, {0,0,0,0}}, /*XLP_NAE_IDX(13)		:	103 */
+        {58, {0,0,0,0}}, /*XLP_NAE_IDX(14)		:	104 */
+        {58, {0,0,0,0}}, /*XLP_NAE_IDX(15)		:	105 */
+        {58, {0,0,0,0}}, /*XLP_NAE_IDX(16)		:	106 */
+        {58, {0,0,0,0}}, /*XLP_NAE_IDX(17)		:	107 */
+        {58, {0,0,0,0}}, /*XLP_NAE_IDX(18)		:	108 */
+        {58, {0,0,0,0}}, /*XLP_NAE_IDX(19)		:	109 */
+        {58, {0,0,0,0}}, /*XLP_NAE_IDX(20)		:	110 */
+        {58, {0,0,0,0}}, /*XLP_NAE_IDX(21)		:	111 */
+        {58, {0,0,0,0}}, /*XLP_NAE_IDX(22)		:	112 */
+        {58, {0,0,0,0}}, /*XLP_NAE_IDX(23)		:	113 */
+        {58, {0,0,0,0}}, /*XLP_NAE_IDX(24)		:	114 */
+        {58, {0,0,0,0}}, /*XLP_NAE_IDX(25)		:	115 */
+        {58, {0,0,0,0}}, /*XLP_NAE_IDX(26)		:	116 */
+        {58, {0,0,0,0}}, /*XLP_NAE_IDX(27)		:	117 */
+        {58, {0,0,0,0}}, /*XLP_NAE_IDX(28)		:	118 */
+        {58, {0,0,0,0}}, /*XLP_NAE_IDX(29)		:	119 */
+        {58, {0,0,0,0}}, /*XLP_NAE_IDX(30)		:	120 */
+        {58, {0,0,0,0}}, /*XLP_NAE_IDX(31)		:	121 */
+        {60, {0,0,0,0}}, /*XLP_POE_IDX			:	122 */
+        {24, {0,0,0,0}}, /*XLP_USB_IDX(0)		:	123 */
+        {24, {0,0,0,0}}, /*XLP_USB_IDX(1)		:	124 */
+        {24, {0,0,0,0}}, /*XLP_USB_IDX(2)		:	125 */
+        {25, {0,0,0,0}}, /*XLP_USB_IDX(3)		:	126 */
+        {25, {0,0,0,0}}, /*XLP_USB_IDX(4)		:	127 */
+        {25, {0,0,0,0}}, /*XLP_USB_IDX(5)		:	128 */
+        {61, {0,0,0,0}}, /*XLP_GDX_IDX			:	129 */
+        {63, {0,0,0,0}}, /*XLP_SEC_IDX			:	130 */
+        {62, {0,0,0,0}}, /*XLP_RSA_IDX			:	131 */
+        {39, {0,0,0,0}}, /*XLP_COMP_IDX(0)		:	132 */
+        {39, {0,0,0,0}}, /*XLP_COMP_IDX(1)		:	133 */
+        {39, {0,0,0,0}}, /*XLP_COMP_IDX(2)		:	134 */
+        {39, {0,0,0,0}}, /*XLP_COMP_IDX(3)		:	135 */
+        {0, {0,0,0,0}}, /*RESERVED_IDX			:	136 */
+        {37, {0,0,0,0}}, /*XLP_ICC_IDX(0)		:	137  ICC - Inter Chip Coherency*/
+        {37, {0,0,0,0}}, /*XLP_ICC_IDX(1)		:	138 */
+        {37, {0,0,0,0}}, /*XLP_ICC_IDX(2)		:	139 */
+        {36, {0,0,0,0}}, /*XLP_CAM_IDX			:	140 */
+        {17, {0,0,0,0}}, /*XLP_UART_IDX(0)		:	141 */
+        {18, {0,0,0,0}}, /*XLP_UART_IDX(0)		:	142 */
+        {11, {0,0,0,0}}, /*XLP_I2C_IDX(0)		:	143 */
+        {11, {0,0,0,0}}, /*XLP_I2C_IDX(0)		:	144 */
+        {12, {0,0,0,0}}, /*XLP_SYS_IDX(0)		:	145 */
+        {12, {0,0,0,0}}, /*XLP_SYS_IDX(1)		:	146 */
+        {55, {0,0,0,0}}, /*XLP_JTAG_IDX			:	147 */
+        {50, {0,0,0,0}}, /*XLP_PIC_IDX			:	148 */
+        {54, {0,0,0,0}}, /*XLP_NBU_IDX			:	149 */
+        {53, {0,0,0,0}}, /*XLP_TCU_IDX			:	150 */
+        {31, {0,0,0,0}}, /*XLP_SATA			:	151 */
+        {38, {0,0,0,0}}, /*XLP_DMC_IDX			:	152 */	/* collision */
+        {38, {0,0,0,0}}, /*XLP_DMC_IDX			:	153 */
+        {13, {0,0,0,0}}, /*XLP_GPIO_IDX(0)		:	154 */
+        {14, {0,0,0,0}}, /*XLP_GPIO_IDX(1)		:	155 */
+        {15, {0,0,0,0}}, /*XLP_GPIO_IDX(2)		:	156 */
+        {16, {0,0,0,0}}, /*XLP_GPIO_IDX(3)		:	157 */
+        {20, {0,0,0,0}}, /*XLP_NOR_IDX			:	158 */
+        {21, {0,0,0,0}}, /*XLP_NAND_IDX			:	159 */
+        {22, {0,0,0,0}}, /*XLP_SPI_IDX			:	160 */
+        {23, {0,0,0,0}}, /*XLP_MMC_IDX			:	161 */
+        {0, {0,0,0,0}}, /*			    162 */
+        {0, {0,0,0,0}}, /*                          163 */
+        {0, {0,0,0,0}}, /*                          164 */
+        {0, {0,0,0,0}}, /*                          165 */
+        {0, {0,0,0,0}}, /*                          166 */
+        {0, {0,0,0,0}}, /*                          167 */
 };
 
 /*
@@ -344,73 +339,73 @@ static struct irq_map_elem irq_map[NR_IRQS] = {
 struct rvec_map_elem {
 	/* irt = elem.irt + ffs(bitmap), where bitmap != 0 */
 	int irt;	/* The first IRT corresponding to this rvec */
-	volatile unsigned long bitmap;	/* bit set is the offset from irt */
+	volatile unsigned long bitmap[NLM_MAX_NODES];	/* bit set is the offset from irt */
 };
 static struct rvec_map_elem rvec_map[XLP_EIRR_SIZE] = {
-	{-1, 0},			/* 0 */
-	{-1, 0},			/* 1 */
-	{-1, 0},			/* 2 */
-	{-1, 0},			/* 3 */
-	{-1, 0},			/* 4 */
-	{-1, 0},			/* 5 */
-	{-1, 0},			/* 6 */
-	{-1, 0},			/* 7 */
-	{-1, 0},			/* 8 */
-	{0, 0},				/* 9  Watchdog timer */
-	{4, 0},				/* 10 PIC Timter */
-	{135, 0},			/* 11 */
-	{137, 0},			/* 12 */
-	{146, 0},			/* 13 */
-	{147, 0},			/* 14 */
-	{148, 0},			/* 15 */
-	{149, 0},			/* 16 */
-	{133, 0},			/* 17 */
-	{134, 0},			/* 18 */
-	{2, 0},				/* 19 , Watchdog NMI */
-	{150, 0},			/* 20 */
-	{151, 0},			/* 21 */
-	{152, 0},			/* 22 */
-	{153, 0},			/* 23 */
-	{115, 0},			/* 24 */
-	{118, 0},			/* 25 */
-	{-1, 0},			/* 26 */
-	{-1, 0},			/* 27 */
-	{-1, 0},			/* 28 */
-	{-1, 0},			/* 29 */
-	{-1, 0},			/* 30 */
-	{143, 0},			/* 31 */
-	{46, 0},			/* 32  MSIX - FN(0)*/
-	{54, 0},			/* 33  MSIX - FN(1)*/
-	{62, 0},			/* 34  MSIX - FN(2)*/
-	{70, 0},			/* 35  MSIX - FN(3)*/
-	{132, 0},			/* 36 */
-	{129, 0},			/* 37 */
-	{144, 0},			/* 38 */
-	{124, 0},			/* 39 */
-	{-1, 0},			/* 40 */
-	{78, 0},			/* 41 */
-	{79, 0},			/* 42 */
-	{80, 0},			/* 43 */
-	{81, 0},			/* 44 */
-	{-1, 0},			/* 45 */
-	{-1, 0},			/* 46 */
-	{-1, 0},			/* 47 */
-	{45, 0},			/* 48 */
-	{44, 0},			/* 49 XLP_MSG_IDX*/
-	{140, 0},			/* 50 */
-	{-1, 0},			/* 51 */
-	{143, 0},			/* 52 */
-	{142, 0},			/* 53 */
-	{141, 0},			/* 54 */
-	{139, 0},			/* 55 */
-	{-1, 0},			/* 56 */
-	{-1, 0},			/* 57 */
-	{82, 0},			/* 58 */
-	{12, 0},			/* 59 , MSGQ*/
-	{114, 0},			/* 60 */
-	{121, 0},			/* 61 */
-	{123, 0},			/* 62 */
-	{122, 0},			/* 63 */
+	{-1, {0,0,0,0}},			/* 0 */
+	{-1, {0,0,0,0}},			/* 1 */
+	{-1, {0,0,0,0}},			/* 2 */
+	{-1, {0,0,0,0}},			/* 3 */
+	{-1, {0,0,0,0}},			/* 4 */
+	{-1, {0,0,0,0}},			/* 5 */
+	{-1, {0,0,0,0}},			/* 6 */
+	{-1, {0,0,0,0}},			/* 7 */
+	{-1, {0,0,0,0}},			/* 8 */
+	{0, {0,0,0,0}},				/* 9  Watchdog timer */
+	{5, {0,0,0,0}},				/* 10 PIC Timers 2 through 7, 0 and 1 reserved*/
+	{135, {0,0,0,0}},			/* 11 */
+	{137, {0,0,0,0}},			/* 12 */
+	{146, {0,0,0,0}},			/* 13 */
+	{147, {0,0,0,0}},			/* 14 */
+	{148, {0,0,0,0}},			/* 15 */
+	{149, {0,0,0,0}},			/* 16 */
+	{133, {0,0,0,0}},			/* 17 */
+	{134, {0,0,0,0}},			/* 18 */
+	{2, {0,0,0,0}},				/* 19 , Watchdog NMI */
+	{150, {0,0,0,0}},			/* 20 */
+	{151, {0,0,0,0}},			/* 21 */
+	{152, {0,0,0,0}},			/* 22 */
+	{153, {0,0,0,0}},			/* 23 */
+	{115, {0,0,0,0}},			/* 24 */
+	{118, {0,0,0,0}},			/* 25 */
+	{4, {0,0,0,0}},				/* 26  Dedicated systimer(0) */
+	{-1, {0,0,0,0}},			/* 27 */
+	{-1, {0,0,0,0}},			/* 28 */
+	{-1, {0,0,0,0}},			/* 29 */
+	{12, {0,0,0,0}},			/* 30 */
+	{143, {0,0,0,0}},			/* 31 */
+	{46, {0,0,0,0}},			/* 32  MSIX - FN(0)*/
+	{54, {0,0,0,0}},			/* 33  MSIX - FN(1)*/
+	{62, {0,0,0,0}},			/* 34  MSIX - FN(2)*/
+	{70, {0,0,0,0}},			/* 35  MSIX - FN(3)*/
+	{132, {0,0,0,0}},			/* 36 */
+	{129, {0,0,0,0}},			/* 37 */
+	{144, {0,0,0,0}},			/* 38 */
+	{124, {0,0,0,0}},			/* 39 */
+	{-1, {0,0,0,0}},			/* 40 */
+	{78, {0,0,0,0}},			/* 41 */
+	{79, {0,0,0,0}},			/* 42 */
+	{80, {0,0,0,0}},			/* 43 */
+	{81, {0,0,0,0}},			/* 44 */
+	{-1, {0,0,0,0}},			/* 45 */
+	{-1, {0,0,0,0}},			/* 46 */
+	{-1, {0,0,0,0}},			/* 47 */
+	{45, {0,0,0,0}},			/* 48 */
+	{44, {0,0,0,0}},			/* 49 XLP_MSG_IDX*/
+	{140, {0,0,0,0}},			/* 50 */
+	{-1, {0,0,0,0}},			/* 51 */
+	{143, {0,0,0,0}},			/* 52 */
+	{142, {0,0,0,0}},			/* 53 */
+	{141, {0,0,0,0}},			/* 54 */
+	{139, {0,0,0,0}},			/* 55 */
+	{-1, {0,0,0,0}},			/* 56 */
+	{-1, {0,0,0,0}},			/* 57 */
+	{82, {0,0,0,0}},			/* 58 */
+	{24, {0,0,0,0}},			/* 59 , MSGQ*/
+	{114, {0,0,0,0}},			/* 60 */
+	{121, {0,0,0,0}},			/* 61 */
+	{123, {0,0,0,0}},			/* 62 */
+	{122, {0,0,0,0}},			/* 63 */
 };
 
 /*
@@ -443,15 +438,6 @@ void xlp_clear_eimr(void *param)
 	return;
 }
 
-void __xlp_setup_one_irq(u64 irq)
-{
-	int cpu;
-	preempt_disable();
-	cpu = smp_processor_id();
-	__nlm_hal_set_irt_to_cpu(xlp_irq_to_irt(irq), cpu);
-	preempt_enable();
-}
-
 /*
  * Returns the base IRQ (index of irq_map) from an rvec
  */
@@ -489,7 +475,8 @@ static inline int irqbase_from_rvec(int rvec)
  */
 int xlp_rvec_from_irq(int irq)
 {
-	if ((irq < XLP_IRQ_RESERVED_MAX) || (irq >= XLP_IRQ_MAX)) {
+	irq %= XLP_IRQS_PER_NODE;
+	if (irq < XLP_IRQ_RESERVED_MAX){
 		return -EINVAL;
 	}
 	return(irq_map[irq].rvec);
@@ -501,7 +488,7 @@ EXPORT_SYMBOL(xlp_rvec_from_irq);
  * Must NOT be called with xlp_pic_lock held
  * @irq : IRQ number
  */
-static void __nlm_irq_mask(unsigned int irq)
+static void __nlm_irq_mask(u8 node, unsigned int irq)
 {
 	int rvec;
 
@@ -517,6 +504,38 @@ static void __nlm_irq_mask(unsigned int irq)
 }
 
 /*
+ * This function checks if the irq has a valid range for INTX
+ * @irq : irq number to check
+ */
+int check_intx_range(u32 irq)
+{
+	u32 lirq = irq % XLP_IRQS_PER_NODE;
+	/* local intx has a range XLP_IRQ_RESERVED_MAX to XLP_IRQ_MAX */
+	if((lirq < XLP_IRQ_RESERVED_MAX) && (lirq >= 0)) {
+		return -EINVAL;
+	} else if(lirq >= XLP_IRQ_MAX) {
+		return -EINVAL;
+	}
+	return 0;
+}
+
+/*
+ * This function checks if the irq has a valid range for IRQ
+ * @irq : irq number to check
+ */
+int check_irq_range(u32 oirq)
+{
+	u32 irq = oirq % XLP_IRQS_PER_NODE;
+	if ((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
+		return -EINVAL;
+	} else if(irq >= XLP_IRQ_MAX) {
+		pr_err("irq = %d. Invalid irq requested\n", oirq);
+		return -EINVAL;
+	}
+	return 0;
+}
+
+/*
  * Interface function (unlocked version) to mask an IRQ
  * Calls helper function after input tests and spin_lock holds
  *
@@ -524,17 +543,13 @@ static void __nlm_irq_mask(unsigned int irq)
  */
 static void nlm_intx_mask(unsigned int irq)
 {
-	//unsigned long flags;
-
-	if((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
-		return;
-	} else if(irq >= XLP_IRQ_MAX) {
-		pr_err("irq = %d. Invalid irq requested\n", irq);
+	u8 node = XLP_IRQ_TO_NODE(irq);
+	if(check_intx_range(irq) < 0) {
 		return;
 	}
 	// Once enabled, we don't mask it out
 	//spin_lock_irqsave(&xlp_pic_lock, flags);	// Remove XXX
-	__nlm_irq_mask(irq);				// XXX
+	__nlm_irq_mask(node, irq);				// XXX
 	//spin_unlock_irqrestore(&xlp_pic_lock, flags);	// XXX remove
 	return;
 }
@@ -542,21 +557,22 @@ static void nlm_intx_mask(unsigned int irq)
 /*
  * Changes eimr bit value corresponding to IRT
  * @irq : IRQ number
+ * TODO : Need to find a method to send messages to a subset of cpus as
+ * target for this vector is a node and only the cpus in that node should
+ * change eimr
  */
-static int __nlm_irq_unmask(int irq)
+static void __nlm_irq_unmask(u8 node, int irq)
 {
-	int ret = 0;
 	int rvec = xlp_rvec_from_irq(irq);
 
-	if (rvec < 0)
-		return ret;
-
-	if (((1ULL << rvec) & read_64bit_cp0_eimr()) == 0) {
+	if (rvec < 0) {
+		return;
+	} else if (((1ULL << rvec) & read_64bit_cp0_eimr()) == 0) {
 		/* This is only for those interrupts which are not statically
 		 * set in EIMR. Could dump stack if spin lock held */
 		 on_each_cpu(xlp_set_eimr, (void *) (1ULL << rvec), 1);
 	}
-	return ret;
+	return;
 }
 
 /*
@@ -567,68 +583,68 @@ static int __nlm_irq_unmask(int irq)
  */
 static void nlm_intx_unmask(unsigned int irq)
 {
-	unsigned long flags;
-
-	if((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
-		return;
-	} else if(irq >= XLP_IRQ_MAX) {
-		pr_err("irq = %d. Invalid irq requested\n", irq);
+	u8 node = XLP_IRQ_TO_NODE(irq);
+	if(check_intx_range(irq) < 0) {
 		return;
 	}
-
-	spin_lock_irqsave(&xlp_pic_lock, flags);
-	__nlm_irq_unmask(irq);
-	spin_unlock_irqrestore(&xlp_pic_lock, flags);
-
+	__nlm_irq_unmask(node, irq);
 	return;
 }
 
-static void nlm_irq_ack(unsigned int irq)
+static void nlm_irq_ack(u8 node, unsigned int irt)
 {
 	unsigned long flags;
 	spin_lock_irqsave(&xlp_pic_lock, flags);
-#ifndef CONFIG_NUMA
-	nlm_hal_ack_pic(xlp_irq_to_irt(irq));
-#else
-	xlp_numa_ack_pic(xlp_irq_to_irt(irq));
-#endif
+	xlp_ack_pic(node, irt);
 	spin_unlock_irqrestore(&xlp_pic_lock, flags);
 }
 
 static void nlm_intx_ack(unsigned int irq)
 {
-	if((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
+	u32 irt = xlp_irq_to_irt(irq);
+	if (check_intx_range(irq) < 0) {
+		/* No need to ack. Not by PIC */
 		return;
-	} else if(irq >= XLP_IRQ_MAX) {
-		pr_err("irq = %d. Invalid irq requested\n", irq);
+	}
+#if !defined CONFIG_XLP_REPLACE_R4K_TIMER/* all are level triggered in XLP */
+	if (PIC_IRQ_IS_EDGE_TRIGGERED(irt)) {
+		nlm_irq_ack(XLP_IRQ_TO_NODE(irq), irt);
+	}
+#endif
+}
+
+static void nlm_irq_end(u8 node, unsigned int irq)
+{
+	if (check_irq_range(irq) < 0) {
+		/* No need to end(ack). Not by PIC */
 		return;
 	}
-	/* If edge triggered, ack it ASAP. Handle the interrupt later */
-	if (PIC_IRQ_IS_EDGE_TRIGGERED(xlp_irq_to_irt(irq))) {
-#ifndef CONFIG_NUMA
-		nlm_irq_ack(irq);
+	/* If level triggered, ack it after the device condition is cleared */
+#if defined CONFIG_XLP_REPLACE_R4K_TIMER	/* all are level triggered in XLP */
+	nlm_irq_ack(node, xlp_irq_to_irt(irq));
 #else
-		xlp_numa_ack_pic(xlp_irq_to_irt(irq));
-#endif
+	if (!PIC_IRQ_IS_EDGE_TRIGGERED(xlp_irq_to_irt(irq))) {
+		nlm_irq_ack(node, xlp_irq_to_irt(irq));
 	}
+#endif
+	return;
 }
 
-static void nlm_irq_end(unsigned int irq)
+static void nlm_intx_end(unsigned int irq)
 {
-	if((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
-		return;
-	} else if(irq >= XLP_IRQ_MAX) {
-		pr_err("irq = %d. Invalid irq requested\n", irq);
+	u8 node = XLP_IRQ_TO_NODE(irq);
+	if (check_intx_range(irq) < 0) {
+		/* No need to end(ack). Not by PIC */
 		return;
 	}
 	/* If level triggered, ack it after the device condition is cleared */
-	if (!PIC_IRQ_IS_EDGE_TRIGGERED(xlp_irq_to_irt(irq))) {
-#ifndef CONFIG_NUMA
-		nlm_irq_ack(irq);
+#if defined CONFIG_XLP_REPLACE_R4K_TIMER	/* all are level triggered in XLP */
+	nlm_irq_ack(node, xlp_irq_to_irt(irq));
 #else
-		xlp_numa_ack_pic(xlp_irq_to_irt(irq));
-#endif
+	if (!PIC_IRQ_IS_EDGE_TRIGGERED(xlp_irq_to_irt(irq))) {
+		nlm_irq_ack(node, xlp_irq_to_irt(irq));
 	}
+#endif
 	return;
 }
 
@@ -641,53 +657,49 @@ static void nlm_irq_end(unsigned int irq)
  * When an interrupt is started, we force it to be enabled only in cpu0, it can
  * be changed later by calling nlm_irq_set_affinity()
  */
-static unsigned int nlm_irq_startup(unsigned int irq)
+extern void xlp_set_cpumask_on_node(u8, const struct cpumask *, int);
+static unsigned int nlm_irq_startup(u8 node, unsigned int irq)
 {
 	__label__ __failure;
 	int ret = 0;
 	unsigned long flags;
 	int idx, rvec;
 	struct cpumask m;
-	struct cpumask const *n;
 
-	cpumask_clear(&m);
-	cpumask_set_cpu(0, &m);
-	if((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
-		return 0;
-	} else if(irq >= XLP_IRQ_MAX) {
-		return 0;
+	if (check_irq_range(irq) < 0) {
+		return -EFAULT;
 	}
-	if ((rvec = xlp_rvec_from_irq(irq)) < 0)
-		return -EINVAL;
-	n = xlp_closest_match_cpumask(&m);
+	cpumask_clear(&m);
+	cpumask_set_cpu(NLM_MAX_CPU_PER_NODE * node, &m);
 	spin_lock_irqsave(&xlp_pic_lock, flags);
-	if (irq_map[irq].usage == 0) {
+	rvec = xlp_rvec_from_irq(irq);
+	if (irq_map[irq].usage[node] == 0) {
 		/* Currently unused => not enabled. So, setup and enable */
-		xlp_set_cpumask(n, xlp_irq_to_irt(irq));
-		ret = __nlm_hal_request_irq(xlp_irq_to_irt(irq), rvec);
+		xlp_set_cpumask_on_node(node, &m, irq);
+		ret = __nlm_hal_request_irq(node, xlp_irq_to_irt(irq), rvec);
 		if (ret != 0) {
 			printk(KERN_WARNING "Failed to setup IRQ %d\n", irq);
 			goto __failure;
 		}
 		idx = irq - __irqbase_from_rvec(rvec);
-		set_bit(idx, &(rvec_map[rvec].bitmap));
-		irq_map[irq].usage++;
+		set_bit(idx, &(rvec_map[rvec].bitmap[node]));
+		irq_map[irq].usage[node]++;
 		/* At this point, make sure that each CPU has eimr bit
 		 * corresponding to this IRQ set. Later the driver can set
 		 * the cpu affinity of this interrupt. The rationale for
 		 * setting up EIMR here is that it can be moved to any CPUs
 		 * (well, a subset of any CPUs) later
 		 */
-		ret = __nlm_irq_unmask(irq);
-	} else if (irq_map[irq].usage > 0) {
+		__nlm_irq_unmask(node, irq);
+	} else if (irq_map[irq].usage[node] > 0) {
 		/* already being used. No need to check mask
 		 * if masked, will be unmasked later
 		 */
-		irq_map[irq].usage++;
+		irq_map[irq].usage[node]++;
 		ret = 0;
 	} else {
 		pr_err("Error irq = %d, rvec = %d, usage count %d\n", irq,
-				irq_map[irq].rvec, irq_map[irq].usage);
+				irq_map[irq].rvec, irq_map[irq].usage[node]);
 		ret = -EFAULT;
 	}
 __failure:
@@ -701,15 +713,25 @@ __failure:
 static unsigned int nlm_intx_startup(unsigned int irq)
 {
 	int fn, ret;
-	/* if this irq correspond to any of the pci slots, enable intx on
-	 * the controller */
-	if ((irq >= XLP_PCIE_LINK_IRQ(0)) && (irq <= XLP_PCIE_LINK_IRQ(3))) {
+	u8 node = XLP_IRQ_TO_NODE(irq);
+
+	if (check_intx_range(irq) < 0) {
+#if !defined CONFIG_XLP_REPLACE_R4K_TIMER
+		if (irq !=  XLP_IRQ_TIMER_RVEC)
+#endif
+		fdebug("Invalid irq %#x\n", irq);
+		return -EINVAL;
+	}
+	/* if this irq correspond to any of the pci slots in any node,
+	 * enable intx on the controller of node */
+	if ((irq >= XLP_PCIE_LINK_IRQ(node, 0))
+			&& (irq <= XLP_PCIE_LINK_IRQ(node, 3))) {
 		fn = XLP_INTX_TO_CTRL_FN(irq);
-		if ((ret = xlp_intx_enable(fn)) < 0) {
+		if ((ret = xlp_intx_enable(node, fn)) < 0) {
 			return (unsigned int)ret;
 		}
 	}
-	return nlm_irq_startup(irq);
+	return nlm_irq_startup(node, irq % XLP_IRQS_PER_NODE);
 }
 
 /*
@@ -722,60 +744,76 @@ static unsigned int nlm_intx_startup(unsigned int irq)
  * chip->shutdown(). In this function, the rvec bit in every EIMR is cleared if
  * usage falls to zero (in case of shared interrupts)
  */
-static void nlm_irq_shutdown(unsigned int irq)
+static void nlm_irq_shutdown(u8 node, unsigned int irq)
 {
 	unsigned long flags;
 	int idx, rvec;
 
-	if((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
-		return;
-	} else if(irq >= XLP_IRQ_MAX) {
-		pr_err("irq = %d. Invalid irq requested\n", irq);
+	if (check_irq_range(irq) < 0) {
 		return;
 	}
-	if ((rvec = xlp_rvec_from_irq(irq)) < 0)
-		return;
 	spin_lock_irqsave(&xlp_pic_lock, flags);
-	if (irq_map[irq].usage == 0) {
-		//fdebug("irq = %d, usage = %d\n", irq, irq_map[irq].usage);
+	if (irq_map[irq].usage[node] == 0) {
 		spin_unlock_irqrestore(&xlp_pic_lock, flags);
 		return;
-	} else if (irq_map[irq].usage > 0) {
-		irq_map[irq].usage--;
+	} else if (irq_map[irq].usage[node] > 0) {
+		irq_map[irq].usage[node]--;
+	}
+	if ((rvec = xlp_rvec_from_irq(irq)) < 0) {
+		return;
 	}
 	/* If the usage reaches zero as a result of above subtraction,
 	 * free up the rvec */
-	if (irq_map[irq].usage == 0) {
-		//fdebug("irq = %d, usage = %d\n", irq, irq_map[irq].usage);
+	if (irq_map[irq].usage[node] == 0) {
 		idx = irq - __irqbase_from_rvec(rvec);
-		clear_bit(idx, &(rvec_map[rvec].bitmap));
+		clear_bit(idx, &(rvec_map[rvec].bitmap[node]));
 		spin_unlock_irqrestore(&xlp_pic_lock, flags);
-		__nlm_irq_mask(irq); /* masks this IRQ */
+		__nlm_irq_mask(node, irq); /* masks this IRQ */
 	} else {
 		spin_unlock_irqrestore(&xlp_pic_lock, flags);
 	}
 	return;
 }
 
-
 /*
  * Shutdown function for intx
  */
 static void nlm_intx_shutdown(unsigned int irq)
 {
 	int fn, ret;
+	u8 node = XLP_IRQ_TO_NODE(irq);
+
+	if (check_intx_range(irq) < 0) {
+		fdebug("Invalid irq %#x\n", irq);
+		return;
+	}
 	/* if this irq correspond to any of the pci slots, disable intx on
 	 * the controller  before shutting it down */
-	if ((irq >= XLP_PCIE_LINK_IRQ(0)) && (irq <= XLP_PCIE_LINK_IRQ(3))) {
+	if ((irq >= XLP_PCIE_LINK_IRQ(node, 0))
+			&& (irq <= XLP_PCIE_LINK_IRQ(node, 3))) {
 		fn = XLP_INTX_TO_CTRL_FN(irq);
-		if ((ret = xlp_intx_disable(fn)) < 0) {
+		if ((ret = xlp_intx_disable(node, fn)) < 0) {
 			return;
 		}
 	}
-	return nlm_irq_shutdown(irq);
+	return nlm_irq_shutdown(node, irq % XLP_IRQS_PER_NODE);
 }
+
+static int nlm_irq_set_affinity(u8 node, unsigned int irq, const struct cpumask *mask)
+{
+	unsigned long flags;
+
+	if (check_irq_range(irq) < 0) {
+		return -EINVAL;
+	}
+	spin_lock_irqsave(&xlp_pic_lock, flags);
+	xlp_set_cpumask_on_node(node, mask, irq);
+	spin_unlock_irqrestore(&xlp_pic_lock, flags);
+	return 0;
+}
+
 /*
- * Set affinity for the irq for chips
+ * Set affinity for the intx for chips
  *
  * When an interrupt is setup, its EIMR bit is set in all online cpus. That is,
  * any cpu _can_ receive that interrupt. But it is the IRT entry that decides
@@ -793,99 +831,51 @@ static void nlm_intx_shutdown(unsigned int irq)
  * The actual bitmask can be different from the specified bitmask based
  * on the logic of xlp_closest_match_cpumask()
  */
-extern cpumask_t fdt_cpumask;
-static int nlm_irq_set_affinity(unsigned int irq, const struct cpumask *mask)
+static int nlm_intx_set_affinity(unsigned int irq, const struct cpumask *mask)
 {
-	unsigned long flags;
-	const struct cpumask *m;
-	struct cpumask n;
+	struct cpumask m;
+	u8 node = XLP_IRQ_TO_NODE(irq);
 
-	if((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
-		return 0;
-	} else if(irq >= XLP_IRQ_MAX) {
-		pr_err("irq = %d. Invalid irq requested\n", irq);
+	if (check_intx_range(irq) < 0) {
 		return -EINVAL;
 	}
-	/* We need present physical cpu mask. cpu_online_mask is logical, so
-	 * we can't use that. In XLP, hot pluggable CPUs are not
-	 * supported on rel 2.2.2 timeframe. So, we can use fdt_cpumask till
-	 * CPU Hotplugging gets implemented TODO CPU_HOTPLUG */
-#ifdef CONFIG_NUMA
-{
-	int i;
-
-	/* Interrupts are to be delivered locally */
-	cpumask_clear(&n);
-        for (i = 0; i < 32; i++) {
-                cpumask_set_cpu(i, &n);
-        }
-	cpumask_and(&n, &n, &fdt_cpumask);
-	cpumask_and(&n, mask, &n);
-}
-#else
-	cpumask_and(&n, mask, &fdt_cpumask);
-#endif
-	m = xlp_closest_match_cpumask(&n);
-	if (m == NULL) {
-		printk(KERN_WARNING "Could not find a match for specified cpumask\n");
-		return -EINVAL;
+	if (xlp_span_multiple_nodes(mask) != 0) {
+		/* this is the policy for MSI. Change later TODO */
+		constrict_mask_to_node(node, &m, mask);
+	} else {
+		cpumask_copy(&m, mask);
 	}
-	spin_lock_irqsave(&xlp_pic_lock, flags);
-	xlp_set_cpumask(m, xlp_irq_to_irt(irq));
-	spin_unlock_irqrestore(&xlp_pic_lock, flags);
-	return 0;
+	return nlm_irq_set_affinity(node, irq % XLP_IRQS_PER_NODE, &m);
 }
 
-static struct irq_chip nlm_irq_pic = {
-	.name = "XLP-PIC",
+static struct irq_chip nlm_intx_pic = {
+	.name = "XLP-INTX",
 	.mask = nlm_intx_mask,
 	.unmask = nlm_intx_unmask,
 	.startup = nlm_intx_startup,
 	.shutdown = nlm_intx_shutdown,
 	.ack = nlm_intx_ack,
-	.end = nlm_irq_end,
-	.set_affinity = nlm_irq_set_affinity
+	.end = nlm_intx_end,
+	.set_affinity = nlm_intx_set_affinity
 };
 
-static void rsvd_pic_handler_1_1(unsigned int irq)
-{
-	if((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
-		return;
-	} else if(irq >= XLP_IRQ_MAX) {
-		pr_err("irq = %d. Invalid irq requested\n", irq);
-		return;
-	}
-	pr_err("Requesting a reserved irq (%d)??", irq);
-	return;
-}
-
 static void rsvd_pic_handler_1(unsigned int irq)
 {
-	if((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
-		return;
-	} else if(irq >= XLP_IRQ_MAX) {
-		pr_err("irq = %d. Invalid irq requested\n", irq);
-		return;
+	if (check_irq_range(irq) < 0) {
+		pr_err("Operation on a reserved irq (%d)??", irq);
 	}
-	pr_err("Requesting a reserved irq (%d)??", irq);
 	return;
 }
 
 static int rsvd_pic_handler_2(unsigned int irq, const struct cpumask *mask)
 {
-	if((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
-		return 0;
-	} else if(irq >= XLP_IRQ_MAX) {
-		pr_err("irq = %d. Invalid irq requested\n", irq);
-		return -EINVAL;
-	}
-	pr_err("Requesting a reserved irq (%d)??", irq);
+	rsvd_pic_handler_1(irq);
 	return 0;
 }
 
 struct irq_chip nlm_common_rsvd_pic = {
 	.name = "Netlogic-RSVD-PIC",
-	.unmask = rsvd_pic_handler_1_1,
+	.unmask = rsvd_pic_handler_1,
 	.mask = rsvd_pic_handler_1,
 	.ack = rsvd_pic_handler_1,
 	.end = rsvd_pic_handler_1,
@@ -894,16 +884,21 @@ struct irq_chip nlm_common_rsvd_pic = {
 
 static irqreturn_t nlm_common_rsvd_irq_handler(int irq, void *dev_id)
 {
-	if((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
-		return 0;
-	} else if(irq >= XLP_IRQ_MAX) {
-		pr_err("irq = %d. Invalid irq requested\n", irq);
-		return -EINVAL;
+#if !defined CONFIG_XLP_REPLACE_R4K_TIMER
+	if ((irq % XLP_IRQS_PER_NODE) == XLP_IRQ_TIMER_RVEC) {
+		return IRQ_HANDLED;
 	}
-	if(irq == XLP_IRQ_TIMER) {
+#else
+	switch (irq) {
+		case XLP_TIMER_IRQ(0, 0) ... XLP_TIMER_IRQ(0, 7):
+		case XLP_TIMER_IRQ(1, 0) ... XLP_TIMER_IRQ(1, 7):
+		case XLP_TIMER_IRQ(2, 0) ... XLP_TIMER_IRQ(2, 7):
+		case XLP_TIMER_IRQ(3, 0) ... XLP_TIMER_IRQ(3, 7):
 		return IRQ_HANDLED;
+		default:
+		break;
 	}
-	pr_err("Requesting a reserved irq (%d)??", irq);
+#endif
 	return IRQ_NONE;
 }
 
@@ -937,20 +932,21 @@ struct irqaction netrx_action = {
 
 void do_nlm_common_IRQ(unsigned int irq, struct pt_regs *regs)
 {
-	if (irq == XLP_IRQ_IPI_SMP_FUNCTION || irq == XLP_IRQ_IPI_SMP_RESCHEDULE) {
-		kstat_incr_irqs_this_cpu(irq, irq_to_desc(irq));
-		nlm_common_ipi_handler(irq, regs);
+	int lirq = irq % XLP_IRQS_PER_NODE;
+
+	if (lirq == XLP_IRQ_IPI_SMP_FUNCTION_RVEC || lirq == XLP_IRQ_IPI_SMP_RESCHEDULE_RVEC) {
+		kstat_incr_irqs_this_cpu(lirq, irq_to_desc(lirq));
+		nlm_common_ipi_handler(lirq, regs);
 		return;
 	}
-	if (irq == XLP_IRQ_MSGRING) {
-		kstat_incr_irqs_this_cpu(irq, irq_to_desc(irq));
-		nlm_xlp_msgring_int_handler(irq, regs);
+	if (lirq == XLP_IRQ_MSGRING_RVEC) {
+		kstat_incr_irqs_this_cpu(lirq, irq_to_desc(lirq));
+		nlm_xlp_msgring_int_handler(lirq, regs);
 	}
-	else if (irq == XLP_IRQ_IPI_SMP_KGDB) {
+	else if (lirq == XLP_IRQ_IPI_SMP_KGDB_RVEC) {
 		/* ignore now */
-	}
-	else {
-		do_IRQ(irq);
+	} else {
+		do_IRQ(irq);	/* Pass IRQ; not lirq */
 	}
 }
 
@@ -971,6 +967,16 @@ void destroy_irq(unsigned int irq)
 
 #ifdef CONFIG_PCI_MSI
 
+static int check_msi_range(unsigned int msi)
+{
+	int lirq = msi % XLP_IRQS_PER_NODE;
+	if((lirq < XLP_IRQ_RESERVED_MAX) && (lirq >= 0)) {
+		return -EINVAL;
+	} else if(lirq < XLP_MSI_INDEX_START || lirq > XLP_MSI_INDEX_END) {
+		return -EINVAL;
+	}
+	return 0;
+}
 /*
  * The MSI and MSI-X functionality is supported only by the PCIe Controller.
  * Whenever there is a request for MSI/MSI-X, we need to find out the
@@ -992,72 +998,80 @@ void destroy_irq(unsigned int irq)
 static unsigned int nlm_msi_startup(unsigned int msi)
 {
 	int bit, irq, fn, base_msi, ret;
+	u8 node;
 
-	if ((msi < XLP_MSI_INDEX_START) || (msi > XLP_MSI_INDEX_END)){
-		return -EINVAL;
+	if (check_msi_range(msi) < 0) {
+		return 0;
 	}
 	fn = XLP_MSI_TO_CTRL_FN(msi);
-	irq = XLP_PCIE_LINK_IRQ(XLP_MSI_TO_CTRL_FN(msi));	/* actual irq line (irt + max reserved) */
-	base_msi = XLP_MSI_IRQ_START(fn);
+	node = XLP_MSI_TO_NODE(msi);
+	irq = XLP_PCIE_LINK_IRQ(0, XLP_MSI_TO_CTRL_FN(msi)); /*Note:NODE == 0*/
+	base_msi = XLP_MSI_IRQ_START(0, fn); /*Note:NODE == 0*/
 	bit = msi - base_msi;
-	if ((ret = xlp_msi_enable(fn, bit)) < 0) {
+	if ((ret = xlp_msi_enable(node, fn, bit)) < 0) {
 		return ret;
 	}
 	/* unmask MSI in the device */
 	unmask_msi_irq(msi);
-	return nlm_irq_startup(irq);
+	return nlm_irq_startup(node, irq);
 }
 
 static int nlm_msi_set_affinity(unsigned int msi, const struct cpumask *mask)
 {
 	struct cpumask m;
-	if ((msi < XLP_MSI_INDEX_START) || (msi > XLP_MSI_INDEX_END)){
+	u8 node = XLP_MSI_TO_NODE(msi);
+
+	if (check_msi_range(msi) < 0) {
 		return -EINVAL;
 	}
-
-	cpumask_and(&m, mask, cpu_online_mask);
-	if (cpumask_equal(&m, cpu_online_mask)){
-		return nlm_irq_set_affinity(XLP_PCIE_LINK_IRQ(XLP_MSI_TO_CTRL_FN(msi)), &m);
+	if (xlp_span_multiple_nodes(mask) != 0) {
+		/* this is the policy for MSI. Change later TODO */
+		constrict_mask_to_node(node, &m, mask);
 	} else {
-		printk(KERN_WARNING "MSI cpu affinity change not supported\n");
-		return -EINVAL;
+		cpumask_copy(&m, mask);
 	}
+	return nlm_irq_set_affinity(node,
+		XLP_PCIE_LINK_IRQ(0, XLP_MSI_TO_CTRL_FN(msi)), &m); /* Note : node == 0 passed for link_irq*/
 }
 
 static void nlm_msi_shutdown(unsigned int msi)
 {
 	int bit, irq, fn, base_msi;
+	u8 node = XLP_MSI_TO_NODE(msi);
 
+	if (check_msi_range(msi) < 0) {
+		return;
+	}
 	/* mask MSI in the device */
 	mask_msi_irq(msi);
 	fn = XLP_MSI_TO_CTRL_FN(msi);
-	irq = XLP_PCIE_LINK_IRQ(XLP_MSI_TO_CTRL_FN(msi));	/* actual irq line (irt + max reserved) */
-	base_msi = XLP_MSI_IRQ_START(fn);
+	irq = XLP_PCIE_LINK_IRQ(0, fn);	/* actual irq line (irt + max reserved) */
+	base_msi = XLP_MSI_IRQ_START(node, fn);
 	bit = msi - base_msi;
-	if ((msi < XLP_MSI_INDEX_START) || (msi > XLP_MSI_INDEX_END)){
-		return;
-	}
-	if (xlp_msi_disable(fn, bit) < 0) {
+	if (xlp_msi_disable(node, fn, bit) < 0) {
 		return;
 	}
-	return nlm_irq_shutdown(irq);
+	return nlm_irq_shutdown(node, irq);
 }
 
 static void nlm_msi_end(unsigned int msi)
 {
-	if ((msi < XLP_MSI_INDEX_START) || (msi > XLP_MSI_INDEX_END)){
+	if (check_msi_range(msi) < 0) {
 		return;
 	}
-	return nlm_irq_end(XLP_PCIE_LINK_IRQ(XLP_MSI_TO_CTRL_FN(msi)));
+	nlm_irq_end(XLP_MSI_TO_NODE(msi), /* Note 0 as node below */
+			XLP_PCIE_LINK_IRQ(0, XLP_MSI_TO_CTRL_FN(msi)));
+	return;
 }
 
-
 static void nlm_msi_ack(unsigned int msi)
 {
-	if ((msi < XLP_MSI_INDEX_START) || (msi > XLP_MSI_INDEX_END)){
+	if (check_msi_range(msi) < 0) {
 		return;
 	}
-	return nlm_irq_ack(XLP_PCIE_LINK_IRQ(XLP_MSI_TO_CTRL_FN(msi)));
+	return nlm_irq_ack(XLP_MSI_TO_NODE(msi),
+		/* Note 0 as node below */
+		xlp_irq_to_irt(XLP_PCIE_LINK_IRQ(0, XLP_MSI_TO_CTRL_FN(msi))));
 }
 
 /*
@@ -1070,17 +1084,20 @@ static u32 nlm_msi_change_mask(unsigned int msi, int val)
 	unsigned long flags;
 	int bit, fn;
 	u32 mask;
+	u8 node = XLP_MSI_TO_NODE(msi);
 
 	fn = XLP_MSI_TO_CTRL_FN(msi);
-	bit = msi - XLP_MSI_IRQ_START(fn);
+	bit = msi - XLP_MSI_IRQ_START(node, fn);
 	spin_lock_irqsave(&xlp_pic_lock, flags);
-	mask = xlp_msi_set_mask(fn, bit, val);
+	mask = xlp_msi_set_mask(node, fn, bit, val);
 	if (val == 0) {
 		if (mask == 0) { /* This was the last bit to clear */
-			__nlm_irq_mask(XLP_PCIE_LINK_IRQ(XLP_MSI_TO_CTRL_FN(msi)));
+			__nlm_irq_mask(node,
+				XLP_PCIE_LINK_IRQ(0, XLP_MSI_TO_CTRL_FN(msi)));
 		}
 		if ((mask & (mask - 1)) == 0) {	/* Just set the only bit*/
-			__nlm_irq_unmask(XLP_PCIE_LINK_IRQ(XLP_MSI_TO_CTRL_FN(msi)));
+			__nlm_irq_unmask(node,
+				XLP_PCIE_LINK_IRQ(0, XLP_MSI_TO_CTRL_FN(msi)));
 		}
 	}
 	spin_unlock_irqrestore(&xlp_pic_lock, flags);
@@ -1093,7 +1110,7 @@ static u32 nlm_msi_change_mask(unsigned int msi, int val)
  */
 static void nlm_msi_mask(unsigned int msi)
 {
-	if ((msi < XLP_MSI_INDEX_START) || (msi > XLP_MSI_INDEX_END)){
+	if (check_msi_range(msi) < 0) {
 		return ;
 	}
 	/* mask MSI in the device */
@@ -1111,7 +1128,7 @@ static void nlm_msi_mask(unsigned int msi)
  */
 static void nlm_msi_unmask(unsigned int msi)
 {
-	if ((msi < XLP_MSI_INDEX_START) || (msi > XLP_MSI_INDEX_END)){
+	if (check_msi_range(msi) < 0) {
 		return ;
 	}
 	/* unmask MSI in the device */
@@ -1141,29 +1158,59 @@ struct irq_chip nlm_msi_pic = {
  * These functions would find out the controller function using the
  * passed parameter and use nlm_irq_* function to operate on that IRT
  */
+static int check_msix_range(unsigned int msix)
+{
+	int lirq = msix % XLP_IRQS_PER_NODE;
+	if((lirq < XLP_IRQ_RESERVED_MAX) && (lirq >= 0)) {
+		return -EINVAL;
+	} else if(lirq < XLP_MSIX_INDEX_START || lirq > XLP_MSIX_INDEX_END) {
+		return -EINVAL;
+	}
+	return 0;
+}
 
 static int nlm_msix_set_affinity(unsigned int msix, const struct cpumask *mask)
 {
-	if ((msix < XLP_MSIX_INDEX_START) || (msix > XLP_MSIX_INDEX_END)){
+	struct cpumask m;
+	u8 node = XLP_MSIX_TO_NODE(msix);
+	u32 lmsix;
+
+	if (check_msix_range(msix) < 0) {
 		return -EINVAL;
 	}
-	return nlm_irq_set_affinity(XLP_PCIE_MSIX_IRQ(msix - XLP_MSIX_INDEX_START), mask);
+	if (xlp_span_multiple_nodes(mask) != 0) {
+		constrict_mask_to_node(node, &m, mask);
+	} else {
+		cpumask_copy(&m, mask);
+	}
+	lmsix = msix % XLP_IRQS_PER_NODE;
+	return nlm_irq_set_affinity(node, XLP_PCIE_MSIX_IRQ(0, lmsix - XLP_MSIX_INDEX_START), &m);
 }
 
 static void nlm_msix_end(unsigned int msix)
 {
-	if ((msix < XLP_MSIX_INDEX_START) || (msix > XLP_MSIX_INDEX_END)){
+	u32 lmsix;
+	u8 node = XLP_MSIX_TO_NODE(msix);
+
+	if (check_msix_range(msix) < 0) {
 		return;
 	}
-	return nlm_irq_end(XLP_PCIE_MSIX_IRQ(msix - XLP_MSIX_INDEX_START));
+	lmsix = msix % XLP_IRQS_PER_NODE;
+	nlm_irq_end(node, XLP_PCIE_MSIX_IRQ(0, lmsix - XLP_MSIX_INDEX_START));
+	return;
 }
 
 static void nlm_msix_ack(unsigned int msix)
 {
-	if ((msix < XLP_MSIX_INDEX_START) || (msix > XLP_MSIX_INDEX_END)){
+	u32 lmsix;
+	u8 node = XLP_MSIX_TO_NODE(msix);
+
+	if (check_msix_range(msix) < 0) {
 		return;
 	}
-	return nlm_irq_ack(XLP_PCIE_MSIX_IRQ(msix - XLP_MSIX_INDEX_START));
+	lmsix = msix % XLP_IRQS_PER_NODE;
+	nlm_irq_ack(node, xlp_irq_to_irt(XLP_PCIE_MSIX_IRQ(0, lmsix - XLP_MSIX_INDEX_START)));
+	return;
 }
 
 /*
@@ -1177,12 +1224,13 @@ static void nlm_msix_ack(unsigned int msix)
 static void nlm_msix_mask(unsigned int msix)
 {
 	unsigned long flags;
-	if ((msix < XLP_MSIX_INDEX_START) || (msix > XLP_MSIX_INDEX_END)){
-		return ;
+	if (check_msix_range(msix) < 0) {
+		return;
 	}
 	spin_lock_irqsave(&xlp_pic_lock, flags);
 	mask_msi_irq(msix); /* This function masks MSI-X -- please note */
-	__nlm_irq_mask(XLP_PCIE_MSIX_IRQ(msix - XLP_MSIX_INDEX_START));
+	__nlm_irq_mask(XLP_MSIX_TO_NODE(msix), XLP_PCIE_MSIX_IRQ(0,
+			(msix % XLP_IRQS_PER_NODE) - XLP_MSIX_INDEX_START));
 	spin_unlock_irqrestore(&xlp_pic_lock, flags);
 	return;
 }
@@ -1194,11 +1242,12 @@ static void nlm_msix_mask(unsigned int msix)
 static void nlm_msix_unmask(unsigned int msix)
 {
 	unsigned long flags;
-	if ((msix < XLP_MSIX_INDEX_START) || (msix > XLP_MSIX_INDEX_END)){
-		return ;
+	if (check_msix_range(msix) < 0) {
+		return;
 	}
 	spin_lock_irqsave(&xlp_pic_lock, flags);
-	__nlm_irq_unmask(XLP_PCIE_MSIX_IRQ(msix - XLP_MSIX_INDEX_START));
+	__nlm_irq_unmask(XLP_MSIX_TO_NODE(msix), XLP_PCIE_MSIX_IRQ(0,
+		(msix % XLP_IRQS_PER_NODE) - XLP_MSIX_INDEX_START));
 	unmask_msi_irq(msix); /* Enable MSI-X -- please note */
 	spin_unlock_irqrestore(&xlp_pic_lock, flags);
 	return;
@@ -1206,29 +1255,35 @@ static void nlm_msix_unmask(unsigned int msix)
 
 static unsigned int nlm_msix_startup(unsigned int msix)
 {
+	u8 node = XLP_MSIX_TO_NODE(msix);
 	int fn = XLP_MSIX_TO_CTRL_FN(msix), ret;
-	if ((msix < XLP_MSIX_INDEX_START) || (msix > XLP_MSIX_INDEX_END)){
-		printk(KERN_WARNING "Invalid msix #%d\n", msix);
+
+	if (check_msix_range(msix) < 0) {
 		return -EINVAL;
 	}
-	if ((ret = xlp_msix_enable(fn)) < 0) {
+	if ((ret = xlp_msix_enable(node, fn)) < 0) {
 		return ret;
 	}
 	nlm_msix_unmask(msix);
-	return nlm_irq_startup(XLP_PCIE_MSIX_IRQ(msix - XLP_MSIX_INDEX_START));
+	return nlm_irq_startup(node, XLP_PCIE_MSIX_IRQ(0,
+			(msix % XLP_IRQS_PER_NODE) - XLP_MSIX_INDEX_START));
 }
 
 static void nlm_msix_shutdown(unsigned int msix)
 {
 	int fn = XLP_MSIX_TO_CTRL_FN(msix), ret;
-	if ((msix < XLP_MSIX_INDEX_START) || (msix > XLP_MSIX_INDEX_END)){
+	u8 node = XLP_MSIX_TO_NODE(msix);
+
+	if (check_msix_range(msix) < 0) {
 		return;
 	}
 	nlm_msix_mask(msix);
-	if ((ret = xlp_msix_disable(fn)) < 0) {
+	if ((ret = xlp_msix_disable(node, fn)) < 0) {
 		return;
 	}
-	return nlm_irq_shutdown(XLP_PCIE_MSIX_IRQ(msix - XLP_MSIX_INDEX_START));
+	nlm_irq_shutdown(node, XLP_PCIE_MSIX_IRQ(0,
+			(msix % XLP_IRQS_PER_NODE) - XLP_MSIX_INDEX_START));
+	return;
 }
 
 /*
@@ -1255,26 +1310,30 @@ struct irq_chip nlm_msix_pic = {
 static int xlp_msi_compose_msg(struct pci_dev *pdev, struct msi_desc *desc,
 		unsigned int irq, struct msi_msg *msg)
 {
+	struct xlp_nodefn_struct nfn;
 	u8 offset;
-	int fn = xlp_ctrl_fn_from_dev(pdev);
 
-	if (fn < 0) return -EINVAL;
+	if( xlp_ctrl_fn_from_dev(pdev, &nfn) < 0) {
+		return -EINVAL;
+	}
 	if (desc->msi_attrib.is_msix) {
-		if (irq < XLP_MSIX_INDEX_START) {	/* enforce minimum */
+		if (check_msix_range(irq)) {	/* enforce minimum */
 			dev_err(&pdev->dev, "Invalid irq %d", irq);
 			return -EINVAL;
 		}
-		offset = irq - XLP_MSIX_INDEX_START;
-		msg->address_hi = (virt_to_phys(xlp_msix_addr_start(fn)) >> 32);
-		msg->address_lo = (virt_to_phys(xlp_msix_addr_start(fn)) & 0xffffffff);
-		//dev_dbg(&pdev->dev, "MSI-X hi = %#x, lo = %#x, data = %#x\n", msg->address_hi, msg->address_lo, offset);
+		offset = (irq % XLP_IRQS_PER_NODE) - XLP_MSIX_INDEX_START;
+		msg->address_hi = (virt_to_phys(xlp_msix_addr_start(nfn.node, nfn.fn)) >> 32);
+		msg->address_lo = (virt_to_phys(xlp_msix_addr_start(nfn.node, nfn.fn)) & 0xffffffff);
+		dev_err(&pdev->dev, "MSI-X hi = %#x, lo = %#x, data = %#x\n", msg->address_hi, msg->address_lo, offset);
 	} else {
-		if (irq < XLP_MSI_IRQ_OFFSET) {	/* enforce minimum */
+		if (check_msi_range(irq)) {	/* enforce minimum */
 			return -EINVAL;
 		}
-		offset = irq - (XLP_MSI_IRQ_START(xlp_ctrl_fn_from_dev(pdev)));
-		msg->address_hi = (virt_to_phys(xlp_msi_addr_start(fn)) >> 32) & 0xffffffff;
-		msg->address_lo = (virt_to_phys(xlp_msi_addr_start(fn)) & 0xffffffff);
+		offset = (irq % XLP_IRQS_PER_NODE) -
+					(XLP_MSI_IRQ_START(0, nfn.fn));
+		msg->address_hi = (virt_to_phys(xlp_msi_addr_start(nfn.node, nfn.fn)) >> 32) & 0xffffffff;
+		msg->address_lo = (virt_to_phys(xlp_msi_addr_start(nfn.node, nfn.fn)) & 0xffffffff);
+		dev_err(&pdev->dev, "MSI hi = %#x, lo = %#x, data = %#x\n", msg->address_hi, msg->address_lo, offset);
 	}
 	msg->data = offset;
 	return 0;
@@ -1286,18 +1345,20 @@ static int xlp_msi_compose_msg(struct pci_dev *pdev, struct msi_desc *desc,
  * Must call with lock held
  * @fn : controller number
  */
-u32 __xlp_msix_bitmask(int fn)
+# if 0
+u32 __xlp_msix_bitmask(u8 node, int fn)
 {
 	int idx = 0, ret = 0;
 
 	while (idx < XLP_MSIX_PER_SLOT) {
-		if (irq_map[XLP_MSIX_IRQ_START(fn) + idx].usage > 0) {
+		if (irq_map[XLP_MSIX_IRQ_START(fn) + idx].usage[node] > 0) {
 			ret |= (1ULL << idx);
 		}
 		idx++;
 	}
 	return ret;
 }
+#endif
 
 /*
  * Back end of disable_msi()/ disable_msix()
@@ -1306,28 +1367,30 @@ void arch_teardown_msi_irq(unsigned int msi)
 {
 	unsigned long flags;
 	int bit, fn;
+	u8 node = XLP_MSIX_TO_NODE(msi);
+	unsigned int lmsi = msi % XLP_IRQS_PER_NODE;
 
 	switch (msi) {
 	case XLP_MSI_INDEX_START ... XLP_MSI_INDEX_END:
 		spin_lock_irqsave(&xlp_pic_lock, flags);
 		fn = XLP_MSI_TO_CTRL_FN(msi);
-		bit = msi - XLP_MSI_IRQ_START(fn);
-		msi_vec[fn].count--;
-		msi_vec[fn].bitmap &= ~(1ULL << bit);
-		if (xlp_get_ctrl_intmode(fn) == XLP_INTMODE_MSI) {
-			if (xlp_set_ctrl_intmode(fn, XLP_INTMODE_NONE) < 0){
+		bit = lmsi - XLP_MSI_IRQ_START(0, fn);
+		msi_vec[node][fn].count--;
+		msi_vec[node][fn].bitmap &= ~(1ULL << bit);
+		if (xlp_get_ctrl_intmode(node, fn) == XLP_INTMODE_MSI) {
+			if (xlp_set_ctrl_intmode(node, fn, XLP_INTMODE_NONE) < 0){
 			}
 		}
 		spin_unlock_irqrestore(&xlp_pic_lock, flags);
 		return;
 	case XLP_MSIX_INDEX_START ... XLP_MSIX_INDEX_END:
 		fn = XLP_MSIX_TO_CTRL_FN(msi);
-		bit = msi - XLP_MSIX_IRQ_START(fn);
+		bit = (msi % XLP_IRQS_PER_NODE) - XLP_MSIX_IRQ_START(0, fn);
 		spin_lock_irqsave(&xlp_pic_lock, flags);
-		msix_vec[fn].count--;
-		msix_vec[fn].bitmap &= ~(1ULL << bit);
-		if (xlp_get_ctrl_intmode(fn) == XLP_INTMODE_MSIX) {
-			if (xlp_set_ctrl_intmode(fn, XLP_INTMODE_NONE) < 0){
+		msix_vec[node][fn].count--;
+		msix_vec[node][fn].bitmap &= ~(1ULL << bit);
+		if (xlp_get_ctrl_intmode(node, fn) == XLP_INTMODE_MSIX) {
+			if (xlp_set_ctrl_intmode(node, fn, XLP_INTMODE_NONE) < 0){
 			}
 		}
 		spin_unlock_irqrestore(&xlp_pic_lock, flags);
@@ -1355,21 +1418,29 @@ asmlinkage void plat_irq_dispatch(void)
 	struct pt_regs *pt_regs = current_thread_info()->regs;
 	int rvec = 0, idx = 0, base_irq, irq, fn;
 	unsigned long flags;
+	u8 node = hard_smp_processor_id() / NLM_MAX_CPU_PER_NODE;
 
 	eirr = read_64bit_cp0_eirr();
 	eimr = read_64bit_cp0_eimr();
 	eirr &= eimr;
-	if (eirr & (1ULL << XLP_IRQ_TIMER)) {
-		write_64bit_cp0_eirr(1ULL << XLP_IRQ_TIMER);
-		nlm_common_timer_interrupt(pt_regs, XLP_IRQ_TIMER);
+#if !defined CONFIG_XLP_REPLACE_R4K_TIMER
+	if (eirr & (1ULL << XLP_IRQ_TIMER_RVEC)) {
+		nlm_common_timer_interrupt(pt_regs, XLP_IRQ_TIMER_RVEC);
 		return;
 	}
-
-	if (eirr & (1ULL << XLP_IRQ_OPROFILE)) {
-		nlm_oprofile_interrupt(pt_regs, XLP_IRQ_OPROFILE);
+	if ( eirr & ( 1ULL << XLP_IRQ_OPROFILE)) {
+		nlm_oprofile_interrupt(pt_regs,XLP_IRQ_OPROFILE);
 		return;
 	}
-
+#else
+	/* Dedicated processing only for timer interrupt (RVEC 26, IRT 12) */
+	if (eirr & (1ULL << XLP_PIC_SYSTIMER_RVEC)) {
+		rvec = __ilog2_u64(eirr);
+		write_64bit_cp0_eirr(1ULL << rvec);
+		do_IRQ(XLP_TIMER_IRQ(node, 0));
+		eirr &= ~(1ULL << XLP_PIC_SYSTIMER_RVEC);
+	}
+#endif
 	/* Loop till all bits of eirr is cleared */
 	while (eirr) {
 
@@ -1390,34 +1461,39 @@ asmlinkage void plat_irq_dispatch(void)
 			return;
 		}
 		spin_lock_irqsave(&xlp_pic_lock, flags);
-		bitmap = rvec_map[rvec].bitmap;
+		bitmap = rvec_map[rvec].bitmap[node];
 		spin_unlock_irqrestore(&xlp_pic_lock, flags);
 		switch(base_irq) {
 		/* For INTX, bitmap and base irq already set */
 #if defined CONFIG_PCI_MSI
 		/* These are not MSI vector numbers, but IRT #s */
-		case XLP_PCIE_LINK_IRQ(0) ... XLP_PCIE_LINK_IRQ(3):
+		case XLP_PCIE_LINK_IRQ(0, 0) ... XLP_PCIE_LINK_IRQ(0, 3):
 			/* Here fn # of controller is easily calculated
 			 * Check the IRT table : 0 -> 78, 1-> 79 ..etc */
-			fn = base_irq - XLP_PCIE_LINK_IRQ(0);
-			if (is_msi_set(fn) != 0) { /* this is an MSI */
-				/* find vectors set */
-				bitmap = calc_msi_vector_offset(fn);
+			fn = base_irq - XLP_PCIE_LINK_IRQ(0, 0);
+			if (is_msi_set(node, fn) != 0) { /* this is an MSI */
+				/* find vectors set, overwrite bitmap */
+				bitmap = calc_msi_vector_offset(node, fn);
 				/* recalculate base_irq for MSI */
-				base_irq = XLP_MSI_IRQ_START(fn);
+				base_irq = XLP_MSI_IRQ_START(node, fn);
 				/* now handle it as any other interrupt */
+			} else { /* If MSI is not set, must be INTX */
+				base_irq += (node * XLP_IRQS_PER_NODE);
 			}
 			break;
-		case XLP_PCIE_MSIX_IRQ(0) ... XLP_PCIE_MSIX_IRQ(31):
-			fn = XLP_MSIX_TO_CTRL_FN(base_irq - XLP_PCIE_MSIX_IRQ(0));/* This _is_ correct because of((x >>3) &3) */
-			/* this is an MSI/MSI-X. Find vectors set */
-			bitmap = xlp_msix_status_clear(fn);
+		case XLP_PCIE_MSIX_IRQ(0, 0) ... XLP_PCIE_MSIX_IRQ(0, 31):
+			fn = XLP_MSIX_TO_CTRL_FN(base_irq -
+				XLP_PCIE_MSIX_IRQ(0, 0)); /* This _is_ correct because of((x >>3) &3) */
+			/* this is a MSI-X. Find vectors set */
+			bitmap = xlp_msix_status_clear(node, fn);
 			/* recalculate base_irq for MSI */
-			base_irq = XLP_MSIX_IRQ_START(fn);
+			base_irq = XLP_MSIX_IRQ_START(node, fn);
 			/* now handle it as any other interrupt */
 			break;
 #endif
 		default:
+			/* Except MSIX and MSI, all are treated INTX */
+			base_irq += (node * XLP_IRQS_PER_NODE);
 			break;
 		}
 		while (bitmap) {
@@ -1433,17 +1509,6 @@ asmlinkage void plat_irq_dispatch(void)
 	return;
 }
 
-int nlm_xlp_request_irq(int irq)
-{
-	if((irq < XLP_IRQ_RESERVED_MAX) && (irq >= 0)) {
-		return -ENODEV;
-	} else if(irq >= XLP_IRQ_MAX) {
-		return -EFAULT;
-	}
-	return(irq + XLP_IRQ_RESERVED_MAX);
-}
-EXPORT_SYMBOL(nlm_xlp_request_irq);
-
 #if defined CONFIG_PCI_MSI
 #ifdef arch_setup_msi_irqs
 /*
@@ -1462,22 +1527,19 @@ int xlp_setup_msi_irq(struct pci_dev *dev, struct msi_desc *desc, int nvec)
 	__label__ setup_end;
 	__label__ setup_fail;
 	struct msi_msg msg;
-	int ret, fn, bit, base_msi;
+	int ret, bit, base_msi;
 	unsigned long flags;
+	struct xlp_nodefn_struct nfn;
 
-	fn = xlp_ctrl_fn_from_dev(dev);
-	if (fn < 0) {
-		return -EFAULT;
-	}
-	base_msi = XLP_MSI_IRQ_START(fn);
-	if (base_msi < XLP_MSI_IRQ_OFFSET) {
+	if (xlp_ctrl_fn_from_dev(dev, &nfn) < 0) {
 		return -EFAULT;
 	}
+	base_msi = XLP_MSI_IRQ_START(nfn.node, nfn.fn);
 	if (nvec != 1) {
 		return -EINVAL;
 	}
 	spin_lock_irqsave(&xlp_pic_lock, flags);
-	ret = xlp_get_ctrl_intmode(fn);
+	ret = xlp_get_ctrl_intmode(nfn.node, nfn.fn);
 	if ((ret == XLP_INTMODE_MSIX ) || (ret == XLP_INTMODE_INTX)) {
 		ret = -EBUSY;
 		goto setup_end;
@@ -1486,17 +1548,17 @@ int xlp_setup_msi_irq(struct pci_dev *dev, struct msi_desc *desc, int nvec)
 	 * Please note that the usage is different from that of MSIX allocation
 	 * where we have the luxury of 1 irt entry per MSIX. Here we have to
 	 * multiplex in software */
-	if (msi_vec[fn].bitmap == 0) {
+	if (msi_vec[nfn.node][nfn.fn].bitmap == 0) {
 		bit = 0;
 	} else {
-		bit = ffz(msi_vec[fn].bitmap);
+		bit = ffz(msi_vec[nfn.node][nfn.fn].bitmap);
 	}
 	if (bit > (XLP_MSI_PER_SLOT - 1)) {
 		ret = -ENOSPC;
 		goto setup_end;
 	}
-	msi_vec[fn].bitmap |= (1ULL << bit);
-	msi_vec[fn].count++;
+	msi_vec[nfn.node][nfn.fn].bitmap |= (1ULL << bit);
+	msi_vec[nfn.node][nfn.fn].count++;
 	base_msi += bit;
 	set_irq_msi(base_msi, desc);
 	ret = xlp_msi_compose_msg(dev, desc, base_msi, &msg);
@@ -1504,13 +1566,13 @@ int xlp_setup_msi_irq(struct pci_dev *dev, struct msi_desc *desc, int nvec)
 		goto setup_fail;
 	}
 	write_msi_msg(base_msi, &msg);
-	ret = xlp_set_ctrl_intmode(fn, XLP_INTMODE_MSI);
-	if (ret == 0) {
+	ret = xlp_set_ctrl_intmode(nfn.node, nfn.fn, XLP_INTMODE_MSI);
+	if (ret == 0) {	/* success */
 		goto setup_end;	/* All done */
 	}
 setup_fail:
-	msi_vec[fn].bitmap &= ~(1ULL << bit);
-	msi_vec[fn].count--;
+	msi_vec[nfn.node][nfn.fn].bitmap &= ~(1ULL << bit);
+	msi_vec[nfn.node][nfn.fn].count--;
 setup_end:
 	spin_unlock_irqrestore(&xlp_pic_lock, flags);
 	return ret;
@@ -1531,38 +1593,35 @@ int xlp_setup_msix_irq(struct pci_dev *dev, int nvec)
 	__label__ fail_loop;
 	__label__ setup_end;
 	struct msi_msg msg;
-	int old_mode, ret, idx, base_msix, fn, bit, old_count;
+	int old_mode, ret, idx, base_msix, bit, old_count;
 	u32 old_bitmap;
 	unsigned long flags;
 	struct msi_desc *desc;
+	struct xlp_nodefn_struct nfn;
 
-	fn = xlp_ctrl_fn_from_dev(dev);
-	if (fn < 0) {
-		return -EFAULT;
-	}
-	base_msix = XLP_MSIX_IRQ_START(fn);
-	if (base_msix < XLP_MSIX_IRQ_OFFSET) {
+	if (xlp_ctrl_fn_from_dev(dev, &nfn) < 0) {
 		return -EFAULT;
 	}
+	base_msix = XLP_MSIX_IRQ_START(nfn.node, nfn.fn);
 	spin_lock_irqsave(&xlp_pic_lock, flags);
-	old_bitmap = msix_vec[fn].bitmap;
-	old_count = msix_vec[fn].count;
-	old_mode = xlp_get_ctrl_intmode(fn);
+	old_bitmap = msix_vec[nfn.node][nfn.fn].bitmap;
+	old_count = msix_vec[nfn.node][nfn.fn].count;
+	old_mode = xlp_get_ctrl_intmode(nfn.node, nfn.fn);
 	if ((old_mode == XLP_INTMODE_MSI ) || (old_mode == XLP_INTMODE_INTX)) {
 		ret = -EBUSY;
 		goto setup_end;
 	}
-	ret = xlp_set_ctrl_intmode(fn, XLP_INTMODE_MSIX);
+	ret = xlp_set_ctrl_intmode(nfn.node, nfn.fn, XLP_INTMODE_MSIX);
 	if (ret < 0) {
 		goto setup_end;
 	}
 	bit = 0, idx = 0;
 	list_for_each_entry(desc, &dev->msi_list, list) {
 		/* this loops exactly nvec times */
-		if (msix_vec[fn].bitmap == 0) {
+		if (msix_vec[nfn.node][nfn.fn].bitmap == 0) {
 			bit = 0;
 		} else {
-			bit = ffz(msix_vec[fn].bitmap);
+			bit = ffz(msix_vec[nfn.node][nfn.fn].bitmap);
 		}
 		if (bit > (XLP_MSIX_PER_SLOT - 1)) {
 			dev_err(&dev->dev, "No more vectors to allocate\n");
@@ -1576,8 +1635,8 @@ int xlp_setup_msix_irq(struct pci_dev *dev, int nvec)
 			goto setup_end;	/* could be a partial success */
 		}
 		/* We have allocated one bit, now get a vector for it */
-		msix_vec[fn].bitmap |= (1ULL << bit);
-		msix_vec[fn].count++;
+		msix_vec[nfn.node][nfn.fn].bitmap |= (1ULL << bit);
+		msix_vec[nfn.node][nfn.fn].count++;
 		set_irq_msi(base_msix + bit, desc);
 		ret = xlp_msi_compose_msg(dev, desc, base_msix + bit, &msg);
 		if (ret < 0) {
@@ -1589,9 +1648,9 @@ int xlp_setup_msix_irq(struct pci_dev *dev, int nvec)
 	spin_unlock_irqrestore(&xlp_pic_lock, flags);
 	return 0;
 fail_loop:
-	msi_vec[fn].bitmap = old_bitmap;
-	msi_vec[fn].count = old_count;
-	xlp_set_ctrl_intmode(fn, old_mode);
+	msi_vec[nfn.node][nfn.fn].bitmap = old_bitmap;
+	msi_vec[nfn.node][nfn.fn].count = old_count;
+	xlp_set_ctrl_intmode(nfn.node, nfn.fn, old_mode);
 setup_end:
 	spin_unlock_irqrestore(&xlp_pic_lock, flags);
 	return ret;
@@ -1634,47 +1693,55 @@ void __init init_nlm_common_irqs(void)
 {
 	int i;
 	u64	mask = 0;
+	u8 node;
 
-	for (i = 0; i < XLP_IRQ_MAX; i++) {	// IRQ : 0 - 167
-		set_irq_chip(i, &nlm_irq_pic);
-	}
+	for_each_online_node(node) {
+		for (i = 0; i < XLP_IRQ_MAX; i++) {	// IRQ : 0 - 167
+			set_irq_chip((node * XLP_IRQS_PER_NODE) + i, &nlm_intx_pic);
+		}
 #ifdef CONFIG_PCI_MSI
-	for (i = XLP_MSI_INDEX_START; i <= XLP_MSI_INDEX_END; i++) {
-		set_irq_chip(i, &nlm_msi_pic);
-	}
-	for (i = XLP_MSIX_INDEX_START; i <= XLP_MSIX_INDEX_END; i++) {
-		set_irq_chip(i, &nlm_msix_pic);
-	}
+		for (i = XLP_MSI_INDEX_START; i <= XLP_MSI_INDEX_END; i++) {
+			set_irq_chip((node * XLP_IRQS_PER_NODE) + i, &nlm_msi_pic);
+		}
+		for (i = XLP_MSIX_INDEX_START; i <= XLP_MSIX_INDEX_END; i++) {
+			set_irq_chip((node * XLP_IRQS_PER_NODE) + i, &nlm_msix_pic);
+		}
 #endif
+	}
 
+	for_each_online_node(node) {
 #ifdef CONFIG_REMOTE_DEBUG	/* REMOVE on XLP TODO */
-	irq_desc[XLP_IRQ_REMOTE_DEBUG].chip = &nlm_common_rsvd_pic;
-	irq_desc[XLP_IRQ_REMOTE_DEBUG].action = nlm_common_rsvd_action;
-	xlp_irq_mask |= (1ULL << XLP_IRQ_REMOTE_DEBUG);
+	irq_desc[(node * XLP_IRQS_PER_NODE) + XLP_IRQ_REMOTE_DEBUG].chip = &nlm_common_rsvd_pic;
+	irq_desc[(node * XLP_IRQS_PER_NODE) + XLP_IRQ_REMOTE_DEBUG].action = nlm_common_rsvd_action;
 #endif
 #ifdef CONFIG_SMP
-	irq_desc[XLP_IRQ_IPI_SMP_FUNCTION].chip = &nlm_common_rsvd_pic;
-	irq_desc[XLP_IRQ_IPI_SMP_FUNCTION].action = &ipi_smp_func_action;
+	irq_desc[(node * XLP_IRQS_PER_NODE) + XLP_IRQ_IPI_SMP_FUNCTION_RVEC].chip = &nlm_common_rsvd_pic;
+	irq_desc[(node * XLP_IRQS_PER_NODE) + XLP_IRQ_IPI_SMP_FUNCTION_RVEC].action = &ipi_smp_func_action;
 
-	irq_desc[XLP_IRQ_IPI_SMP_RESCHEDULE].chip = &nlm_common_rsvd_pic;
-	irq_desc[XLP_IRQ_IPI_SMP_RESCHEDULE].action = &ipi_smp_resched_action;
+	irq_desc[(node * XLP_IRQS_PER_NODE) + XLP_IRQ_IPI_SMP_RESCHEDULE_RVEC].chip = &nlm_common_rsvd_pic;
+	irq_desc[(node * XLP_IRQS_PER_NODE) + XLP_IRQ_IPI_SMP_RESCHEDULE_RVEC].action = &ipi_smp_resched_action;
 
 #ifdef CONFIG_NLMCOMMON_IP_FLOW_AFFINITY	/* REMOVE on XLP TODO */
 	/* PR: New IPI added here for netrx balancing */
-	irq_desc[XLP_IRQ_IPI_NETRX].chip = &nlm_common_rsvd_pic;
-	irq_desc[XLP_IRQ_IPI_NETRX].action = &netrx_action;
+	irq_desc[(node * XLP_IRQS_PER_NODE) + XLP_IRQ_IPI_NETRX].chip = &nlm_common_rsvd_pic;
+	irq_desc[(node * XLP_IRQS_PER_NODE) + XLP_IRQ_IPI_NETRX].action = &netrx_action;
 	//xlp_irq_mask |= (1ULL << XLP_IRQ_IPI_NETRX);
 #endif				/* CONFIG_NLMCOMMON_IP_FLOW_AFFINITY */
 
 #endif
+	}
 
 	/* msgring interrupt */
-	irq_desc[XLP_IRQ_MSGRING].chip = &nlm_common_rsvd_pic;
-	irq_desc[XLP_IRQ_MSGRING].action = &msgring_action;
+	irq_desc[XLP_IRQ_MSGRING_RVEC].chip = &nlm_common_rsvd_pic;
+	irq_desc[XLP_IRQ_MSGRING_RVEC].action = &msgring_action;
 
 	mask = (
-			(1ULL << XLP_IRQ_TIMER) |
-			(1ULL << 10) |	/* timer */
+#if defined CONFIG_XLP_REPLACE_R4K_TIMER
+			(1ULL << XLP_PIC_SYSTIMER_RVEC) | /* PIC Systimer (0)*/
+#else
+			(1ULL << XLP_IRQ_TIMER_RVEC) |
+#endif
+			(1ULL << XLP_PIC_TIMERS_RVEC) |	/* Other PIC timers */
 			(1ULL << 49) |	/* msg_idx */
 			(0x3ULL << 48) |	/* msg_idx */
 			(0xfULL << 32) |	/* pci msix */
@@ -1684,17 +1751,22 @@ void __init init_nlm_common_irqs(void)
 			(0x3ULL	<< 17) |	/* uart */
 			(0xfULL << 13) |	/* gpio */
 			(0x1ULL << 31) |	/* SATA on xlp3xx */
+			(0x1ULL << 30) |	/* SMSC  - xlp3xx */
+			(0x1ULL << 26) |	/* MMC  */
 #ifdef CONFIG_SMP
-			(1ULL << XLP_IRQ_IPI_SMP_FUNCTION) |
-			(1ULL << XLP_IRQ_IPI_SMP_RESCHEDULE) |
+			(1ULL << XLP_IRQ_IPI_SMP_FUNCTION_RVEC) |
+			(1ULL << XLP_IRQ_IPI_SMP_RESCHEDULE_RVEC) |
 #endif
 #ifdef CONFIG_OPROFILE
 			(1ULL << XLP_IRQ_OPROFILE) |
 #endif
 #ifdef CONFIG_KGDB
-			(1ULL << XLP_IRQ_IPI_SMP_KGDB) |
+			(1ULL << XLP_IRQ_IPI_SMP_KGDB_RVEC) |
+#endif
+			(1ULL << XLP_IRQ_MSGRING_RVEC) |
+#ifdef CONFIG_NLM_WATCHDOG
+                        (1ULL << XLP_PIC_WATCHDOG_TIMERS_RVEC) |
 #endif
-			(1ULL << XLP_IRQ_MSGRING) |
 			(0xfULL << 20)		/* nor, nand, spi and mmc */
 	       );
 	/* set interrupt mask for non-zero cpus */
@@ -1754,7 +1826,7 @@ void fixup_irqs(unsigned int cpu, int flag)
 		/* clear all pending interrupts */
 		write_64bit_cp0_eirr(0xffffffffffffffff);
 		/* set interrupt mask for non-zero cpus */
-		write_64bit_cp0_eimr(xlp_irq_mask | (1 << XLP_IRQ_TIMER));
+		write_64bit_cp0_eimr(xlp_irq_mask | (1 << XLP_IRQ_TIMER_RVEC));
 		enable_msgconfig_int();
 		nlm_enable_vc_intr();
 	}
diff --git a/arch/mips/netlogic/xlp/numa.c b/arch/mips/netlogic/xlp/numa.c
new file mode 100644
index 0000000..63e15e0
--- /dev/null
+++ b/arch/mips/netlogic/xlp/numa.c
@@ -0,0 +1,402 @@
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/bootmem.h>
+#include <linux/mm.h>
+#include <linux/percpu.h>
+#include <linux/mmzone.h>
+#include <linux/pfn.h>
+#include <linux/highmem.h>
+#include <linux/swap.h>
+
+#include <asm/addrspace.h>
+#include <asm/pgalloc.h>
+#include <asm/sections.h>
+#include <asm/bootinfo.h>
+#include <asm/mach-netlogic/mmu.h>
+#include <asm/netlogic/xlp.h>
+#include <asm/netlogic/iomap.h>
+
+extern unsigned long setup_zero_pages(void);
+
+struct nlm_node_data *__node_data[NLM_MAX_CPU_NODE];
+struct nlm_node_data __node_data_holder[NLM_MAX_CPU_NODE];
+struct nlm_mem_info __node_mem_data[MAX_NUMNODES];
+EXPORT_SYMBOL(__node_data);
+EXPORT_SYMBOL(__node_mem_data);
+
+struct xlp_dram_mapping {
+	unsigned long low_pfn;
+	unsigned long high_pfn;
+	int node;
+};
+#define NLM_NODES_MAX_DRAM_REGION (NLM_MAX_DRAM_REGION * MAX_NUMNODES)
+extern struct xlp_dram_mapping	dram_map[NLM_NODES_MAX_DRAM_REGION];
+extern void nlm_get_dram_mapping(void);
+extern int hcpu_to_lcpu[];
+
+#if defined (CONFIG_64BIT) && defined (CONFIG_MAPPED_KERNEL)
+#define _low_virt_to_phys(addr) ((unsigned long)(addr) & ~CKSSEG)
+#endif
+
+static uint8_t _node_map_mem[MAX_NUMNODES][PAGE_SIZE];
+
+extern cpumask_t fdt_cpumask;
+static void nlm_init_bootmem_node (unsigned long mapstart, unsigned long min_pfn, unsigned long max_pfn)
+{
+	int i;
+
+	for (i = 0; i < NLM_MAX_CPU_NODE; i++) {
+		unsigned long map_pfn, start_pfn, end_pfn, bootmem_size;
+		int j;
+
+		if(!node_online(i))
+			continue;
+
+		start_pfn = NODE_MEM_DATA(i)->low_pfn;
+		end_pfn   = NODE_MEM_DATA(i)->high_pfn;
+
+		if (start_pfn && start_pfn < min_pfn)
+			start_pfn = min_pfn;
+
+		if (end_pfn > max_pfn)
+			end_pfn = max_pfn;
+
+		/* in general, never hit the condition */
+		if (start_pfn && start_pfn >= end_pfn) {
+			NODE_MEM_DATA(i)->map_pfn = 0; /* indicate a bad map_pfn */
+			continue;
+		}
+
+		if (start_pfn > mapstart)
+			map_pfn = start_pfn;
+		else
+			map_pfn = mapstart;
+
+		if((start_pfn == 0) && (end_pfn == 0)) {
+			map_pfn = 
+			_low_virt_to_phys(&_node_map_mem[i][0]) >> PAGE_SHIFT;
+			__node_data[i] = __va(map_pfn << PAGE_SHIFT);
+		} else {
+			__node_data[i] = __va(map_pfn << PAGE_SHIFT);
+			map_pfn  += PFN_UP(sizeof(struct nlm_node_data));
+		}
+
+		NODE_DATA(i)->bdata = &bootmem_node_data[i];
+		NODE_DATA(i)->node_start_pfn = start_pfn;
+		NODE_DATA(i)->node_spanned_pages = end_pfn - start_pfn;
+
+		/* Set this up with logical cpu map :
+		   Assuming here that there are 32 cpus per node
+		   */
+		cpumask_clear(NODE_CPU_MASK(i));
+		for (j = 0; j < 32; j++)
+			if (cpumask_test_cpu((j + i * 32), &fdt_cpumask))
+				cpumask_set_cpu(hcpu_to_lcpu[(j+i*32)], 
+						NODE_CPU_MASK(i));
+
+		NODE_MEM_DATA(i)->map_pfn = map_pfn;
+
+		bootmem_size = init_bootmem_node(NODE_DATA(i), map_pfn, start_pfn, end_pfn);
+		NODE_MEM_DATA(i)->bootmem_size = bootmem_size;
+	}
+}
+
+static void nlm_reserve_bootmem(void)
+{
+	int i;
+	unsigned long size;
+
+	for (i = 0; i < NLM_MAX_CPU_NODE; i++) {
+
+		if(!node_online(i))
+			continue;
+		if(NODE_DATA(i)->node_spanned_pages == 0)
+			continue;
+		size = NODE_MEM_DATA(i)->map_pfn - NODE_DATA(i)->node_start_pfn;
+		size = PFN_PHYS(size);
+		reserve_bootmem_node(NODE_DATA(i), 
+			PFN_PHYS(NODE_DATA(i)->node_start_pfn),
+			(NODE_MEM_DATA(i)->bootmem_size + size),
+			 BOOTMEM_DEFAULT);
+	}
+}
+
+static int dram_get_node_id (unsigned long start_pfn, unsigned long end_pfn)
+{
+	int i;
+
+	for (i = 0; i < NLM_MAX_CPU_NODE; i++) {
+		if (NODE_MEM_DATA(i)->low_pfn <= start_pfn && end_pfn <= NODE_MEM_DATA(i)->high_pfn)
+			return i;
+	}
+
+	printk("Invalid start %lx end %lx\n", start_pfn, end_pfn);
+	/* should not reach here */
+	panic("dram_get_node_id: incorrect memory region\n");
+}
+
+/* This is used very early in boot process to build the node mem regions */
+int nlm_get_node(unsigned long pfn)
+{
+	int i;
+	for(i=0; i < NLM_NODES_MAX_DRAM_REGION; i++) {
+		if((pfn >= dram_map[i].low_pfn) && 
+				(pfn <= dram_map[i].high_pfn))
+			return dram_map[i].node;
+	}
+	panic("Invalid PFN Passed: Cannot get node id\n");
+}
+EXPORT_SYMBOL(nlm_get_node);
+
+/**
+ * boot memory initialization for NUMA architecture.
+ *
+ * The implementation here copies the implementation in
+ * arch/mips/kernel/setup, but added numa support.
+ */
+extern struct nlm_node_mem_info node_mem_info[];
+extern void prom_meminit(void);
+void __init nlm_numa_bootmem_init(unsigned long reserved_end)
+{
+	unsigned long mapstart = ~0UL;
+	int i;
+	int node, seg;
+
+	/* Get the hardware dram region info */
+	nlm_get_dram_mapping();
+
+	/*
+	 * max_low_pfn is not a number of pages. The number of pages
+	 * of the system is given by 'max_low_pfn - min_low_pfn'.
+	 */
+	min_low_pfn = ~0UL;
+	max_low_pfn = 0;
+
+	/*
+	 * Find the highest page frame number we have available.
+	 */
+	for (i = 0; i < boot_mem_map.nr_map; i++) {
+		unsigned long start, end;
+
+		if (boot_mem_map.map[i].type != BOOT_MEM_RAM)
+			continue;
+
+		start = PFN_UP(boot_mem_map.map[i].addr);
+		end = PFN_DOWN(boot_mem_map.map[i].addr
+				+ boot_mem_map.map[i].size);
+
+		node = nlm_get_node(start);
+		seg = node_mem_info[node].frags;
+		node_mem_info[node].mem[seg].start_pfn = start;
+		node_mem_info[node].mem[seg].end_pfn = end;
+		node_mem_info[node].frags++;
+
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+		if (seg == 0)
+			node_mem_info[node].min_start_pfn = start;
+		else if (start < node_mem_info[node].min_start_pfn)
+			node_mem_info[node].min_start_pfn = start;
+#endif
+
+		if (end > max_low_pfn)
+			max_low_pfn = end;
+		if (start < min_low_pfn)
+			min_low_pfn = start;
+		if (end <= reserved_end)
+			continue;
+		if (start >= mapstart)
+			continue;
+		mapstart = max(reserved_end, start);
+	}
+
+	if (min_low_pfn >= max_low_pfn)
+		panic("Incorrect memory mapping !!!");
+	if (min_low_pfn > ARCH_PFN_OFFSET) {
+		pr_info("Wasting %lu bytes for tracking %lu unused pages\n",
+			(min_low_pfn - ARCH_PFN_OFFSET) * sizeof(struct page),
+			min_low_pfn - ARCH_PFN_OFFSET);
+	} else if (min_low_pfn < ARCH_PFN_OFFSET) {
+		pr_info("%lu free pages won't be used\n",
+			ARCH_PFN_OFFSET - min_low_pfn);
+	}
+	min_low_pfn = ARCH_PFN_OFFSET;
+
+	/*
+	 * Determine low and high memory ranges
+	 */
+	max_pfn = max_low_pfn;
+	if (max_low_pfn > PFN_DOWN(HIGHMEM_START)) {
+#ifdef CONFIG_HIGHMEM
+		highstart_pfn = PFN_DOWN(HIGHMEM_START);
+		highend_pfn = max_low_pfn;
+#endif
+		max_low_pfn = PFN_DOWN(HIGHMEM_START);
+	}
+
+	max_low_pfn = recalculate_max_low_pfn(max_low_pfn);
+
+#ifdef DEBUG_MAPPED_KERNEL
+	printk("max_low_pfn = 0x%lx\n", max_low_pfn);
+#endif
+	setup_mapped_kernel_tlbs(FALSE, TRUE);
+	prom_meminit();
+	/*
+	 * Initialize the boot-time allocator with low memory only.
+	 */
+	nlm_init_bootmem_node(mapstart, min_low_pfn, max_low_pfn);
+
+	for (i = 0; i < boot_mem_map.nr_map; i++) {
+		unsigned long start, end;
+
+		start = PFN_UP(boot_mem_map.map[i].addr);
+		end = PFN_DOWN(boot_mem_map.map[i].addr
+				+ boot_mem_map.map[i].size);
+
+		if (start <= min_low_pfn)
+			start = min_low_pfn;
+		if (start >= end)
+			continue;
+
+#ifndef CONFIG_HIGHMEM
+		if (end > max_low_pfn)
+			end = max_low_pfn;
+
+		/*
+		 * ... finally, is the area going away?
+		 */
+		if (end <= start)
+			continue;
+#endif
+		add_active_range(dram_get_node_id(start, end), start, end);
+	}
+
+	/*
+	 * Register fully available low RAM pages with the bootmem allocator.
+	 */
+	for (i = 0; i < boot_mem_map.nr_map; i++) {
+		unsigned long start, end, size;
+
+		/*
+		 * Reserve usable memory.
+		 */
+		if (boot_mem_map.map[i].type != BOOT_MEM_RAM)
+			continue;
+
+		start = PFN_UP(boot_mem_map.map[i].addr);
+		end   = PFN_DOWN(boot_mem_map.map[i].addr
+				    + boot_mem_map.map[i].size);
+		/*
+		 * We are rounding up the start address of usable memory
+		 * and at the end of the usable range downwards.
+		 */
+		if (start >= max_low_pfn)
+			continue;
+		if (start < reserved_end)
+			start = reserved_end;
+		if (end > max_low_pfn)
+			end = max_low_pfn;
+
+		/*
+		 * ... finally, is the area going away?
+		 */
+		if (end <= start)
+			continue;
+		size = end - start;
+
+		/* Register lowmem ranges */
+		free_bootmem(PFN_PHYS(start), size << PAGE_SHIFT);
+		memory_present(dram_get_node_id(start, end), start, end);
+	}
+
+	/*
+	 * Reserve the bootmap memory.
+	 */
+	nlm_reserve_bootmem();
+}
+
+static int __init numa_page_is_ram(unsigned long pagenr)
+{
+	int i;
+
+	for (i = 0; i < boot_mem_map.nr_map; i++) {
+		unsigned long start, end;
+
+		if (boot_mem_map.map[i].type != BOOT_MEM_RAM)
+			continue;
+
+		start = PFN_UP(boot_mem_map.map[i].addr);
+		end   = PFN_DOWN(boot_mem_map.map[i].addr
+				    + boot_mem_map.map[i].size);
+		if(pagenr >= start && pagenr <= end)
+			return 1;
+	}
+	return 0;
+}
+
+void __init paging_init(void)
+{
+	unsigned long zones_size[MAX_NR_ZONES] = {0, };
+	unsigned node;
+
+	pagetable_init();
+#ifdef CONFIG_NLM_16G_MEM_SUPPORT
+	setup_mapped_kernel_pgtable();
+#endif
+
+#ifdef CONFIG_ZONE_DMA
+	zones_size[ZONE_DMA] = MAX_DMA_PFN;
+#endif
+
+
+	for_each_online_node(node) {
+		unsigned long start_pfn, end_pfn;
+
+		get_pfn_range_for_nid(node, &start_pfn, &end_pfn);
+
+		if (end_pfn > max_low_pfn)
+			max_low_pfn = end_pfn;
+	}
+	zones_size[ZONE_NORMAL] = max_low_pfn;
+	free_area_init_nodes(zones_size);
+}
+
+void __init mem_init(void)
+{
+	unsigned long codesize, reservedpages, datasize, initsize, tmp, ram;
+	unsigned int  node;
+
+	high_memory = (void *) __va(max_low_pfn<< PAGE_SHIFT);
+	for_each_online_node(node) {
+		/*
+		 * This will free up the bootmem, ie, slot 0 memory.
+		 */
+		totalram_pages += 
+			free_all_bootmem_node(NODE_DATA(node));
+	}
+
+	totalram_pages -= setup_zero_pages();   /* This comes from node 0 */
+
+	reservedpages = ram = 0;
+	for (tmp = 0; tmp < max_low_pfn; tmp++)
+		if (numa_page_is_ram(tmp)) {
+			ram++;
+			if (PageReserved(pfn_to_page(tmp)))
+				reservedpages++;
+		}
+	num_physpages = ram;
+
+	codesize =  (unsigned long) &_etext - (unsigned long) &_text;
+	datasize =  (unsigned long) &_edata - (unsigned long) &_etext;
+	initsize =  (unsigned long) &__init_end - (unsigned long) &__init_begin;
+
+	printk(KERN_INFO "Memory: %luk/%luk available (%ldk kernel code, "
+		"%ldk reserved, %ldk data, %ldk init, %ldk highmem)\n",
+		nr_free_pages() << (PAGE_SHIFT - 10),
+		ram << (PAGE_SHIFT - 10),
+		codesize >> 10,
+		reservedpages << (PAGE_SHIFT - 10),
+		datasize >> 10,
+		initsize >> 10,
+		(unsigned long) (totalhigh_pages << (PAGE_SHIFT - 10)));
+}
diff --git a/arch/mips/netlogic/xlp/on_chip.c b/arch/mips/netlogic/xlp/on_chip.c
index 562e4e2..534ed5c 100644
--- a/arch/mips/netlogic/xlp/on_chip.c
+++ b/arch/mips/netlogic/xlp/on_chip.c
@@ -31,6 +31,8 @@ THE POSSIBILITY OF SUCH DAMAGE.
 #include <linux/kernel.h>
 #include <linux/module.h>
 #include <linux/timer.h>
+#include <linux/cpumask.h>
+#include <linux/nodemask.h>
 
 #include <asm/netlogic/msgring.h>
 #include <asm/netlogic/iomap.h>
@@ -39,9 +41,10 @@ THE POSSIBILITY OF SUCH DAMAGE.
 #include <asm/netlogic/xlp.h>
 
 #include <asm/netlogic/hal/nlm_hal_fmn.h>
-#include <asm/netlogic/xlp_hal_pic.h>
+#include <asm/netlogic/xlp_irq.h>
 
 #include <linux/netdevice.h>
+#include <asm/netlogic/cpumask.h>
 
 #define MAX_VC	4096
 
@@ -61,8 +64,12 @@ uint32_t nlm_cpu_vc_mask[NLM_MAX_CPU_NODE*NLM_MAX_CPU_PER_NODE] = {0};
 
 uint32_t nlm_l1_lock[NR_CPUS/4] = {0};
 
-typedef void (*vchandler)(int vc);
-static vchandler xlp_vc_handlers[NLM_MAX_VC_PER_THREAD];
+typedef int (*vchandler)(int vc, int budget);
+static vchandler xlp_napi_vc_handlers[NLM_MAX_VC_PER_THREAD];
+
+typedef int (*intr_vchandler)(int vc);
+static intr_vchandler xlp_intr_vc_handler;
+unsigned int intr_vc_mask[NR_CPUS];
 
 /* make this a read/write spinlock */
 spinlock_t msgrng_lock;
@@ -71,8 +78,11 @@ static nlm_common_atomic_t msgring_registered;
 struct msgstn_handler {
         void (*action)(uint32_t, uint32_t, uint32_t, uint32_t, uint64_t, uint64_t, uint64_t, uint64_t, void *);
         void *dev_id;
+	void (*napi_final)(void *arg);
+	void *napi_final_arg;
 };
 
+static int napi_final_needed[NR_CPUS][XLP_MSG_HANDLE_MAX];
 struct net_device xlp_napi_fmn_dummy_dev;
 DEFINE_PER_CPU(struct napi_struct, xlp_napi_fmn_poll_struct);
 DEFINE_PER_CPU(unsigned long long, xlp_napi_fmn_rx_count);
@@ -251,30 +261,88 @@ void dummy_handler(uint32_t vc, uint32_t src_id, uint32_t size, uint32_t code,
  *
  ******************************************************************************************/
 struct msgstn_handler msg_handler_map[XLP_MSG_HANDLE_MAX] = {
-	[0 ... (XLP_MSG_HANDLE_MAX-1)] = {dummy_handler, NULL},
+	[0 ... (XLP_MSG_HANDLE_MAX-1)] = {dummy_handler, NULL, NULL, NULL},
 };
 
 
-int nlm_xlp_register_vc_handler(int vc, void (*handler)(int vc))
+int nlm_xlp_register_napi_vc_handler(int vc, int (*handler)(int vc, int budget))
 {
-	int i, node = 0;
 	if(vc < 0 || vc >= NLM_MAX_VC_PER_THREAD) {
-		printk("Invalid VC Passed %d\n", vc);
+		printk("%s, Error, invalid VC Passed %d\n", __FUNCTION__, vc);
 		return -1;
 	}
-	xlp_vc_handlers[vc] = handler;
 
-	for(i=0; i<NR_CPUS; i++){
-		if(!cpu_isset(i, cpu_present_map))
-			continue;
-		node = i / 32;
-		nlm_hal_enable_vc_intr(node, (i*NLM_MAX_VC_PER_THREAD + vc) & 0x7f);
+	if(!((1 << vc) & xlp_napi_vc_mask)) {
+		printk("%s , Error, VC is not specified in napi vc mask\n", __FUNCTION__);
+		return -1;
 	}
 
+	if(!xlp_fmn_init_done)
+		xlp_fmn_init_done = 1;
+	
+	xlp_napi_vc_handlers[vc] = handler;
+	return 0;
+}
+EXPORT_SYMBOL(nlm_xlp_register_napi_vc_handler);
+
+int nlm_xlp_unregister_napi_vc_handler(int vc)
+{
+	if(vc < 0 || vc >= NLM_MAX_VC_PER_THREAD) {
+		printk("%s, Error, invalid VC Passed %d\n", __FUNCTION__, vc);
+		return -1;
+	}
+	xlp_napi_vc_handlers[vc] = NULL;
+	return 0;
+}
+EXPORT_SYMBOL(nlm_xlp_unregister_napi_vc_handler);
+
+int nlm_xlp_register_intr_vc_handler(int (*handler)(int vc))
+{
+	xlp_intr_vc_handler = handler;
 	return 0;
 }
-EXPORT_SYMBOL(nlm_xlp_register_vc_handler);
+EXPORT_SYMBOL(nlm_xlp_register_intr_vc_handler);
 
+int nlm_xlp_register_intr_vc(int cpu, int vc)
+{
+	int node;
+	unsigned long flags;
+
+	if(vc < 0 || vc >= NLM_MAX_VC_PER_THREAD) {
+		printk("%s, Error, invalid VC Passed %d\n", __FUNCTION__, vc);
+		return -1;
+	}
+
+	if(!xlp_fmn_init_done)
+		xlp_fmn_init_done = 1;
+	
+	node = cpu / 32;
+	nlm_hal_enable_vc_intr(node, (cpu*4 + vc) & 0x7f);
+
+	spin_lock_irqsave(&msgrng_lock, flags);
+	intr_vc_mask[cpu] |= (1 << vc);
+	spin_unlock_irqrestore(&msgrng_lock, flags);
+	
+	/*printk("%s in, cpu %d intr_vc_mask %x\n", __FUNCTION__, cpu, intr_vc_mask[cpu]);*/
+	return 0;
+}
+EXPORT_SYMBOL(nlm_xlp_register_intr_vc);
+
+int nlm_xlp_unregister_intr_vc(int cpu, int vc)
+{
+	unsigned long flags;
+
+	if(vc < 0 || vc >= NLM_MAX_VC_PER_THREAD) {
+		printk("%s, Error, invalid VC Passed %d\n", __FUNCTION__, vc);
+		return -1;
+	}
+
+	spin_lock_irqsave(&msgrng_lock, flags);
+	intr_vc_mask[cpu] &= (~(1 << vc));
+	spin_unlock_irqrestore(&msgrng_lock, flags);
+	return 0;
+}
+EXPORT_SYMBOL(nlm_xlp_unregister_intr_vc);
 
 /*********************************************************************
  * nlm_xlp_msgring_int_handler 
@@ -297,8 +365,9 @@ void nlm_xlp_msgring_int_handler(unsigned int irq, struct pt_regs *regs)
 	int pop_vc_mask = nlm_cpu_vc_mask[cpu];
 	msg0 = msg1 = msg2 = msg3 = 0;
 	uint32_t napi_vc_mask = xlp_napi_vc_mask & pop_vc_mask;
-	
-	if (irq == XLP_IRQ_MSGRING) {
+	unsigned int vcmask;
+
+	if (irq == XLP_IRQ_MSGRING_RVEC) {
                 /* normal message ring interrupt */
                 /* xlr_inc_counter(MSGRNG_INT);  */
                 nlm_cpu_stat_update_msgring_int();
@@ -317,12 +386,15 @@ void nlm_xlp_msgring_int_handler(unsigned int irq, struct pt_regs *regs)
 		/* Read latest VC empty mask */
 		msg_status1 = xlp_read_status1();
 
-#ifdef CONFIG_NLM_EXCL_VC_NAPI_HANDLER_SUPPORT
-		for(vc = 0; vc < 4; vc++) {
-			if(xlp_vc_handlers[vc])
-				(xlp_vc_handlers)[vc](vc);
+		vcmask = (~(msg_status1>>24) & intr_vc_mask[cpu]);
+		if(vcmask && xlp_intr_vc_handler) {
+			for(vc = 0; vc < 4; vc++) {
+				if(!(vcmask & (1<<vc)))
+					continue;
+				xlp_intr_vc_handler(vc);
+			}
 		}
-#else
+
 		if((~(msg_status1>>24) & napi_vc_mask) && xlp_fmn_init_done) {
 			struct napi_struct *napi;
 
@@ -331,7 +403,6 @@ void nlm_xlp_msgring_int_handler(unsigned int irq, struct pt_regs *regs)
 		        napi_schedule(napi);
 			pop_vc_mask = pop_vc_mask & ~napi_vc_mask;
 		}
-#endif
 
 		vc_empty_status = (msg_status1 >> 24) & pop_vc_mask;
 		if (vc_empty_status == pop_vc_mask) break;  
@@ -380,7 +451,7 @@ static void msg_timer_handler(unsigned long data)
 	int cpu = smp_processor_id();
 	struct timer_list *timer = &per_cpu(msg_int_bkup_timer, cpu);
 */
-	nlm_xlp_msgring_int_handler(XLP_IRQ_MSGRING, NULL);
+	nlm_xlp_msgring_int_handler(XLP_IRQ_MSGRING_RVEC, NULL);
 /*
 	timer->expires = jiffies + (HZ/100);
 	add_timer(timer);
@@ -463,12 +534,13 @@ int register_xlp_msgring_handler(int major,
 	if(!xlp_fmn_init_done)
 		xlp_fmn_init_done = 1;
 
-	if(msg_handler_timer_enabled == 0) {
-		msg_handler_timer_enabled = 1;
-		spin_unlock_irqrestore(&msgrng_lock, flags);
-		// init_msg_bkp_timer(0);	Not required, taken care by on_each_cpu()
-		on_each_cpu(init_msg_bkp_timer, 0, 1);
-		spin_lock_irqsave(&msgrng_lock, flags);
+	if (is_nlm_xlp8xx_ax()) {
+		if(msg_handler_timer_enabled == 0) {
+			msg_handler_timer_enabled = 1;
+			spin_unlock_irqrestore(&msgrng_lock, flags);
+			on_each_cpu(init_msg_bkp_timer, 0, 1);
+			spin_lock_irqsave(&msgrng_lock, flags);
+		}
 	}
 
 	msg_handler_map[major].action = action;
@@ -498,6 +570,8 @@ int unregister_xlp_msgring_handler(int major, void *dev_id)
 	if(msg_handler_map[major].dev_id == dev_id){
 		msg_handler_map[major].action = dummy_handler;
 		msg_handler_map[major].dev_id = NULL;
+		msg_handler_map[major].napi_final = NULL;
+		msg_handler_map[major].napi_final_arg = NULL;
 	}
 	spin_unlock_irqrestore(&msgrng_lock, flags);
 	return 0;
@@ -505,7 +579,21 @@ int unregister_xlp_msgring_handler(int major, void *dev_id)
 
 EXPORT_SYMBOL(unregister_xlp_msgring_handler);
 
-#include <asm/netlogic/cpumask.h>
+int nlm_xlp_register_napi_final_handler(int major, void (*napi_final)(void *arg), void *arg) 
+{
+	if(major >= XLP_MSG_HANDLE_MAX){
+		printk(KERN_ALERT "%s:%d  Invalid parameter: major=%d, "
+		       "XLP_MAX_TX_STN=%d", __FUNCTION__, __LINE__, major,
+		       XLP_MAX_TX_STNS);
+		return -1;
+	}
+	msg_handler_map[major].napi_final = napi_final;
+	msg_handler_map[major].napi_final_arg = arg;
+	return 0;
+}
+
+EXPORT_SYMBOL(nlm_xlp_register_napi_final_handler);
+
 void nlm_nmi_cpus(unsigned int mask)
 {
 	uint32_t cpumask = cpumask_to_uint32(&cpu_present_map); /* doesn't handle non-n0 nodes */
@@ -513,8 +601,6 @@ void nlm_nmi_cpus(unsigned int mask)
 	uint32_t cpumask_hi;
 	const int nmi = 1;
 
-	pic_reg_t *mmio = nlm_hal_pic_offset();
-
 	cpumask = cpumask & mask;
 
 	cpumask_hi = cpumask >> 16;;
@@ -522,10 +608,10 @@ void nlm_nmi_cpus(unsigned int mask)
 
 	/* Send IRQ_MSGRING vector in an IPI to all cpus but the current one */
 	if (cpumask_lo)
-		nlm_hal_write_pic_reg(mmio, PIC_IPI_CTL, (nmi << 31) | cpumask_lo );
+		nlh_pic_w64r(0, XLP_PIC_IPI_CTL, (nmi << 31) | cpumask_lo );
 
 	if (cpumask_hi)
-		nlm_hal_write_pic_reg(mmio, PIC_IPI_CTL, (nmi << 31) | (1 << 16) | (cpumask_hi));
+		nlh_pic_w64r(0, XLP_PIC_IPI_CTL, (nmi << 31) | (1 << 16) | (cpumask_hi));
 }
 
 /* need COP2 to be accessible */
@@ -576,10 +662,9 @@ void on_chip_shutoff_msgring(void)
 void enable_msgconfig_int(void)
 {
 	unsigned long flags  = 0;
-
 	/* Need write interrupt vector to cp2 msgconfig register */
 	msgrng_access_enable(flags);
-	nlm_hal_set_fmn_interrupt(XLP_IRQ_MSGRING);
+	nlm_hal_set_fmn_interrupt(XLP_IRQ_MSGRING_RVEC);
 	msgrng_access_disable(flags);
 }
 
@@ -723,109 +808,6 @@ void xlp_pic_ite_init(const struct cpumask *tgt_mask)
 #endif
 }
 
-/*
- * This function returns closest match cpumask among the supported bitmasks
- * in XLP
- * Logic is moot, need to improve it later.
- * XXX
- *
- * @m	: user supplied cpumask
- */
-const struct cpumask *xlp_closest_match_cpumask(struct cpumask *m)
-{
-	int i;
-
-	for (i = 0; i < XLP_ITE_ENTRIES; i++) {
-		if (cpumask_equal(m, &xlp_ite_cpumask[i])) {
-			return &xlp_ite_cpumask[i];
-		}
-	}
-	return NULL;
-}
-
-/*
- * This function sets the cpumask for an interrupt vector
- * @m	: CPU mask resulting from xlp_closest_match_cpumask() call
- */
-void xlp_set_cpumask(const struct cpumask *m, int irt)
-{
-	int i;
-	u64 xlp_pic_base = XLP_BDF_BASE(0,0,4);
-	u64 val;
-	u32 offset = ((XLP_PIC_IRTREG_START + (irt << 1)) >> 1);	// Hal requires this nasty right shift
-#ifdef CONFIG_NUMA
-	int nid;
-#endif
-
-#ifndef CONFIG_NUMA
-	/* We set the following in IRT entry
-	 * 28 : clear to indicate global delivery
-	 * 19 : clear to indicate DB selects ITE
-	 * 16-18 : set to indicate ITE
-	 * 0-15 : Clear
-	 */
-	for (i = 0; i < XLP_ITE_ENTRIES; i++) {
-		if (m != &xlp_ite_cpumask[i]) {
-			continue;
-		}
-		val = nlm_hal_read_64bit_reg(xlp_pic_base, offset);
-		val &= ~((1 << 28) | (1 << 19) | (0x7 << 16) | 0xffff);
-		val |= (i << 16);
-		//fdebug("Writing val = %#llx\n", val);
-		nlm_hal_write_64bit_reg(xlp_pic_base, offset, val);
-		return;
-	}
-#else
-	/* We set the following in IRT entry
-	 * 28 : clear to indicate global delivery
-	 * 19 : clear to indicate DB selects ITE
-	 * 16-18 : set to indicate ITE
-	 * 0-15 : Clear
-	 */
-	for_each_online_node (nid) {
-		xlp_pic_base = XLP_BDF_BASE(0, 0 + 8 * nid, 4);
-		for (i = 0; i < XLP_ITE_ENTRIES; i++) {
-			if (m != &xlp_ite_cpumask[i]) {
-				continue;
-			}
-			val = nlm_hal_read_64bit_reg(xlp_pic_base, offset);
-			val &= ~((1 << 28) | (1 << 19) | (0x7 << 16) | 0xffff);
-			val |= (i << 16);
-			//fdebug("Writing val = %#llx\n", val);
-			nlm_hal_write_64bit_reg(xlp_pic_base, offset, val);
-			return;
-		}
-	}
-#endif
-	printk(KERN_WARNING "Failed to program IRT entry %d\n", irt);
-	return;
-}
-
-/*********************************************************************
- *  pic_init
- *  
- ********************************************************************/
-static void pic_init(void)
-{
-	int i = 0;
-	int level, vcpu;
-	uint32_t thread_mask;
-
-	vcpu = hard_smp_processor_id() & 0x1F;
-
-	thread_mask = (1 << vcpu);
-
-	for (i = XLP_IRQ_RESERVED_MAX; i < XLP_IRT_NUM; i++) {
-		level = PIC_IRQ_IS_EDGE_TRIGGERED(i);
-		/* Use local scheduling and high polarity for all IRTs
-		 * Invalidate all IRTs, by default */
-		nlm_hal_pic_write_irt(xlp_irq_to_irt(i), 0, 0, 1, xlp_rvec_from_irq(i), 1, 0, thread_mask);
-	}
-
-	/* On XLP, MSGRING config register is per hw-thread */
-	enable_msgconfig_int();
-}
-
 atomic_t nlm_common_counters[NR_CPUS][NLM_MAX_COUNTERS] __cacheline_aligned;
 
 /*********************************************************************
@@ -867,7 +849,7 @@ void nlm_enable_vc_intr(void)
 	int i = 0;
 
 	for(cpu=0; cpu<NR_CPUS; cpu++){
-                if(!cpu_isset(cpu, cpu_present_map))
+                if(!cpu_isset(cpu, phys_cpu_present_map))
                         continue;
 		node = cpu / 32;
 		for(i=0; i<NLM_MAX_VC_PER_THREAD; i++)
@@ -916,12 +898,25 @@ int xlp_fmn_poll(struct napi_struct *napi, int budget)
 #ifdef CONFIG_32BIT
 	unsigned long mflags;
 #endif
+	unsigned long napi_final_hndlr[XLP_MSG_HANDLE_MAX];
+	int hndlr_cnt = 0, hndlr_id, i, rv;
 
 	while(count < budget){
 		for( no_msg = 0, vc = 0; vc < 4; vc++)
 		{
 			if(!(napi_vc_mask & (1<<vc)))
 				continue;
+	
+			/* Explicit per vc napi handlers. Here the vc handler does the polling of
+			 all the packets */
+			if(xlp_napi_vc_handlers[vc]) {
+				rv = (xlp_napi_vc_handlers)[vc](vc, budget);
+				count += rv;
+				if(rv == 0)
+					no_msg++;
+				continue;
+			}
+			
 #ifdef CONFIG_32BIT
 			msgrng_access_enable(mflags);
 #endif
@@ -935,20 +930,35 @@ int xlp_fmn_poll(struct napi_struct *napi, int budget)
 			}
 			count++;
 			if (is_nlm_xlp3xx()) {
-				handler = &msg_handler_map[xlp3xx_vc_to_handle_map[src_id]];
+				hndlr_id = xlp3xx_vc_to_handle_map[src_id];
 			}
 			else {
-				handler = &msg_handler_map[vc_to_handle_map[src_id]];
+				hndlr_id = vc_to_handle_map[src_id];
 			}
 
+			handler = &msg_handler_map[hndlr_id];
+
 			/* Execute device driver fmn handler */
 			(handler->action)(vc, src_id, size, code,
 				  msg0, msg1, msg2, msg3, handler->dev_id);
+
+			if(handler->napi_final && (napi_final_needed[cpu][hndlr_id] == 0)) {
+				napi_final_needed[cpu][hndlr_id] = 1;
+				napi_final_hndlr[hndlr_cnt] = (unsigned long)handler;
+				hndlr_cnt++;
+			}
 		}
 		if(no_msg == xlp_napi_vc_count)
 			break;
 	}
 
+	for(i = 0; i < hndlr_cnt; i++) {
+		handler = (struct msgstn_handler *)napi_final_hndlr[i];
+		handler->napi_final(handler->napi_final_arg);
+		hndlr_id = handler - &msg_handler_map[0];
+		napi_final_needed[cpu][hndlr_id] = 0;
+	}
+	
 	/*Ack fmn interrupts.*/
 	if(count < budget) {
 		uint32_t val;
@@ -1013,26 +1023,25 @@ static int xlp_napi_fmn_setup(void)
  * on_chip_init
  *  
  ********************************************************************/
+extern void xlp_pic_init(u8);
 void on_chip_init(void)
 {
-	int i = 0, j = 0;
+	int i = 0, j = 0, node;
 
 	cpu_logical_map(0)  = hard_smp_processor_id();
-
+	node = hard_smp_processor_id() / NLM_MAX_CPU_PER_NODE;
 	/* Set netlogic_io_base to the run time value */
 	spin_lock_init(&msgrng_lock);
-
 	msgring_registered.value = 0;
-
 	nlm_hal_init();
-
-	pic_init(); 
-
-	for (i = 0; i < NR_CPUS; i++)
-	for (j = 0; j < NLM_MAX_COUNTERS; j++)
+	xlp_pic_init(node);
+	/* On XLP, MSGRING config register is per hw-thread */
+	enable_msgconfig_int();
+	for (i = 0; i < NR_CPUS; i++) {
+		for (j = 0; j < NLM_MAX_COUNTERS; j++) {
 			atomic_set(&nlm_common_counters[i][j], 0);
-
-	
+		}
+	}
 	if(xlp_napi_vc_mask)
 		xlp_napi_fmn_setup();
 }
diff --git a/arch/mips/netlogic/xlp/pic/Makefile b/arch/mips/netlogic/xlp/pic/Makefile
new file mode 100644
index 0000000..14a206a
--- /dev/null
+++ b/arch/mips/netlogic/xlp/pic/Makefile
@@ -0,0 +1,7 @@
+EXTRA_CFLAGS := -Werror
+EXTRA_CFLAGS := $(CFLAGS) -DNLM_HAL_LINUX_KERNEL -Iarch/mips/include/asm/netlogic/hal
+
+obj-$(CONFIG_XLP_PIC_TIMER)		+= timer-base.o
+obj-$(CONFIG_XLP_REPLACE_R4K_TIMER)	+= pic-timer.o
+obj-$(CONFIG_XLP_PIC_TIMER_GENERIC)	+= timer-wd.o
+obj-y					+= ite.o
diff --git a/arch/mips/netlogic/xlp/pic/ite.c b/arch/mips/netlogic/xlp/pic/ite.c
new file mode 100644
index 0000000..18c4f23
--- /dev/null
+++ b/arch/mips/netlogic/xlp/pic/ite.c
@@ -0,0 +1,303 @@
+#include <linux/types.h>
+#include <linux/init.h>
+#include <linux/smp.h>
+#include <linux/interrupt.h>
+#include <linux/spinlock.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/timer.h>
+#include <linux/cpumask.h>
+#include <linux/nodemask.h>
+#include <linux/netdevice.h>
+
+#include <asm/netlogic/msgring.h>
+#include <asm/netlogic/iomap.h>
+#include <asm/netlogic/mips-exts.h>
+#include <asm/netlogic/debug.h>
+#include <asm/netlogic/xlp.h>
+#include <asm/netlogic/hal/nlm_hal_fmn.h>
+#include <asm/netlogic/xlp_irq.h>
+#include <asm/netlogic/cpumask.h>
+
+/*
+ * xlp_ites[node][0-3] = {0x1, 0xffffffff, 0x0000ffff, 0xffff0000};//local only
+ */
+static struct cpumask xlp_ites[NLM_MAX_NODES][XLP_ITE_ENTRIES];
+static DEFINE_SPINLOCK(xlp_ites_lock);
+
+void dump_all_ites(void)
+{
+	u8 node, i;
+	char buf[140];
+
+	for_each_online_node(node) {
+	for (i = 0; i < XLP_ITE_ENTRIES; i++) {
+		cpumask_scnprintf(buf, 140, &xlp_ites[node][i]);
+		printk(KERN_DEBUG "node %d: Supported CPUMASK (%d) -> %s\n",
+				node, i, buf);
+	}
+	}
+	return;
+}
+
+
+/* This function sets the given ITE's cpu bit on node node.
+ *
+ * @node	: node on which ITE is to be set
+ * @cpu		: target cpu id
+ * @ite		: ITE index
+ * @bitval	: 0 to clear, 1 to set
+ */
+void xlp_ite_cpu_op(u8 node, u8 cpu, u8 ite, u8 bitval)
+{
+	unsigned long flags;
+	u64 val, reg;
+	u8 bit;
+
+	/* No param checking, must be checked before calling.
+	 * Target cpu id decices whether to use THREADEN01 or THREADEN23
+	 * i.e., if target cpu < 64, use THREADEN01 as base else THREADEN23 */
+	reg = (cpu < 64) ? XLP_PIC_INT_THREADEN01(ite) :
+					XLP_PIC_INT_THREADEN23(ite);
+	bit = cpu % 64;
+	spin_lock_irqsave(&xlp_ites_lock, flags);
+	val = nlh_pic_r64r(node, reg);
+	val = (bitval == 0) ?  (val & ~(1ULL << bit)) : (val | (1ULL << bit));
+	nlh_pic_w64r(node, reg, val);
+	spin_unlock_irqrestore(&xlp_ites_lock, flags);
+}
+extern struct cpumask phys_cpu_present_map;
+/* This function would program ITE values on node given by the cpumask
+ * @cpumask	: cpumask to program on ITE
+ * @node	: node on which ITE should be programmed
+ * @ite		: ITE to program
+ * @scope	: program ITE only on the given node (0) or all nodes (1)
+ */
+void xlp_cpumask_to_node_ite(const struct cpumask *m, u8 node, u8 ite, u8 scope)
+{
+	__label__ prog_all;
+	struct cpumask t;
+	int cpu = (node * NLM_MAX_CPU_PER_NODE), last;
+
+	if (scope != 0) goto prog_all;
+
+	/* When the scope is 0, program node ITEs with target as
+	 * local cpus only */
+	last = cpu + NLM_MAX_CPU_PER_NODE - 1;
+	if (last >= NR_CPUS) return;
+	cpumask_and(&t, m, &phys_cpu_present_map);
+	for (; cpu <= last; cpu++) {
+		cpumask_test_cpu(cpu, &t) ? xlp_ite_cpu_set(node, cpu, ite) : xlp_ite_cpu_clear(node, cpu, ite);
+	}
+	return;
+prog_all:
+	/* Here we program the specified ITE in all nodes with the cpumask
+	 * passed. */
+	/* TBD TODO */
+	return;
+}
+
+/* Once all CPUs are up, walk through the node mask and program all
+ * ITEs in the PICs */
+void xlp_prog_all_node_ites(void)
+{
+	u8 i, node;
+
+	dump_all_ites();
+	for_each_online_node(node) {
+		for (i = 0; i < XLP_ITE_ENTRIES; i++) {
+			/* 4 is the ITE that redirects int.s to all cpus */
+			xlp_cpumask_to_node_ite(&xlp_ites[node][i], node, i, (i == 4));
+		}
+	}
+}
+
+/* Checks if a mask spans multiple nodes
+ *
+ * @mask	: cpumask to check for multiple node span
+ */
+int xlp_span_multiple_nodes(const struct cpumask *mask)
+{
+	int l, f;
+	f = cpumask_first(mask);
+	l = find_last_bit(cpumask_bits(mask), NR_CPUS);
+	if ((f/NLM_MAX_CPU_PER_NODE) != (l/NLM_MAX_CPU_PER_NODE)) {
+		printk(KERN_DEBUG "Mask spans from cpu %#x to %#x. Spans across nodes are not supported\n", f, l);
+		return -EINVAL;
+	}
+	return 0;
+}
+
+/*
+ * In XLP cpu mask for setting affinity of an interrupt cannot span multiple
+ * nodes. Although this is not a h/w restriction, the effort to implement
+ * this feature does not justify the potential benefit; not only that handling
+ * non local interrupts are slightly slower, it could be expensive in terms of
+ * memory access and other resource utilization
+ *
+ * @node	: node to which mask `mask` to be restricted
+ * @src		: mask to restrict
+ * @dst		: restricted mask (result)
+ */
+void constrict_mask_to_node(u8 node, struct cpumask *dst, const struct cpumask *src)
+{
+//	char buf[140];
+	int i;
+
+	cpumask_clear(dst);
+	for (i = NLM_MAX_CPU_PER_NODE * node;
+			i < (NLM_MAX_CPU_PER_NODE *(node + 1)); i++) {
+		cpumask_set_cpu(i, dst);
+	}
+	cpumask_and(dst, dst, &phys_cpu_present_map);
+	cpumask_and(dst, dst, src);
+	return;
+}
+
+
+/*
+ * This function returns closest match cpumask among the supported bitmasks
+ * in XLP
+ * Logic is moot, need to improve it later.
+ *
+ * @m	: user supplied cpumask
+ */
+static int xlp_closest_match_cpumask(u8 node, const struct cpumask *m)
+{
+	int i;
+	struct cpumask t, a;
+	/* Check with current online physical cpu mask */
+	cpumask_and(&a, m, &phys_cpu_present_map);
+	constrict_mask_to_node(node, &t, &a);
+	cpumask_clear(&a);
+	for (i = 0; i < XLP_ITE_ENTRIES; i++) {
+		cpumask_and(&a, &xlp_ites[node][i], &phys_cpu_present_map);
+		if (cpumask_equal(&t, &a)) {
+			return i;
+		}
+	}
+	printk(KERN_WARNING "Could not find a match for specified cpumask\n");
+	return 1; /* if no match, point to all local cpus */
+}
+
+/*
+ * This function programs the ITE on the IRT entry on all online nodes
+ * @m	: CPU mask resulting from xlp_closest_match_cpumask() call
+ */
+void xlp_set_cpumask_on_node(u8 node, const struct cpumask *m, int irq)
+{
+	u8 ite;
+	u64 val;
+	int irt = xlp_irq_to_irt(irq);
+	int rvec = xlp_rvec_from_irq(irq);
+
+	//dump_all_ites();
+	ite = xlp_closest_match_cpumask(node, m);
+	/* xlp_pic_init() has set default values. Override them */
+	val = XLP_IRTENT_ENABLE | XLP_IRTENT_SCH_LCL |
+			XLP_IRTENT_RVEC(rvec) | XLP_IRTENT_DB(ite);
+	nlh_pic_w64r(node, XLP_PIC_IRT_ENTRY(irt), val);
+	//fdebug("Setting up mask: Wrote %#llx to (%d)%#llx\n", val, irt, XLP_PIC_IRT_ENTRY(irt));
+	return;
+}
+
+
+/* helper function to create cpumask from unsigned long
+ * Easiest way is to create it directly using bitmap_copy.
+ * For some reason, this was not successful.
+ *
+ * @m	: cpumask pointer to populate. Lower 32 bits must be 0
+ * @u	: u32 variable pointer with bitmask
+ */
+void u32_to_cpumask(struct cpumask *m, u32 bm)
+{
+	u8 bit = 0;
+
+	/* should not clear the mask passed */
+	for ( ; bit < sizeof(u32) * BITS_PER_BYTE; bit++) {
+		if (bm & (1 << bit)) {
+			cpumask_set_cpu(bit, m);
+		} else {
+			cpumask_clear_cpu(bit, m);
+		}
+	}
+}
+/*
+ * Initializes PIC ITE entries PRM 9.5.6.26
+ * XLP restricts CPU affinity to 8 groups. Though configurable, they are
+ * programmed to have the following patterns.
+ * 0 =>	Only 0th cpu on the node
+ * 1 => All local threads in node; mask = (0xffffffff) on node
+ * 2 => cpu0-15 on node; mask = 0x0000ffff & online_cpu_mask on nodes
+ * 3 => cpu15-31 on node; mask = 0xffff0000 & online_cpu_mask on node
+ * 4 => All cpus on all nodes; i.e., 
+ * mask = (0xffffffff_ffffffff_ffffffff_ffffffff & physical online cpu map)
+ * These are programmer defined groups and can be changed as warranted.
+ *
+ * There is a major issue that needs addressing when run in multi node mode
+ * Number of nodes must be determined and programmed correctly, if a bit in ITE
+ * is programmed without physical thread being present, when interrupt is
+ * dispatched to that CPU under global scheme, system would hang. Thus this
+ * scenario should be avoided. That is why phys_cpu_present_map is used
+ *
+ * This function simply initializes the xlp_ites entries with proposed
+ * CPUmasks.  */
+static void xlp_ites_init(void)
+{
+	u64 bm = 0x1;
+	u8 node;
+	struct cpumask m;
+
+	cpumask_clear(&m);
+	for_each_online_node(node) {
+	/* Simply set the static pattern in all */
+	bm = 1;
+	u32_to_cpumask(&xlp_ites[node][0], bm);
+	cpumask_shift_left(&xlp_ites[node][0], &xlp_ites[node][0], NLM_MAX_CPU_PER_NODE * node); /* directs only to cpu0 of node `node` */
+
+	bm = 0xffffffff;
+	u32_to_cpumask(&xlp_ites[node][1], bm);
+	cpumask_shift_left(&xlp_ites[node][1], &xlp_ites[node][1], NLM_MAX_CPU_PER_NODE * node); /* directs to all cpus of node `node` */
+	cpumask_or(&m, &m, &xlp_ites[node][1]);
+
+	bm = 0xffff;
+	u32_to_cpumask(&xlp_ites[node][2], bm);
+	cpumask_shift_left(&xlp_ites[node][2], &xlp_ites[node][2], NLM_MAX_CPU_PER_NODE * node); /* directs to specified cpus of node `node` */
+
+	bm = 0xffff0000;
+	u32_to_cpumask(&xlp_ites[node][3], bm);
+	cpumask_shift_left(&xlp_ites[node][3], &xlp_ites[node][3], NLM_MAX_CPU_PER_NODE * node); /* directs to specified cpus of node `node` */
+	}
+	for_each_online_node(node) {
+		cpumask_copy(&xlp_ites[node][4], &m);
+	}
+	dump_all_ites();
+}
+
+/* Initializes the PIC
+ * Mainly sets up the IRT entries default values
+ * called from on_chip.c:pic_init()
+ * */
+void xlp_pic_init(u8 node)
+{
+	u8 i;
+	u64 val;
+
+	if (node == 0) {
+		xlp_ites_init();
+		/* Program ITEs to direct interrupts only to cpu 0
+		 * This is mandatory for PIC timer to come up. */
+		xlp_cpumask_to_node_ite(&xlp_ites[node][0], node, 0, 0);
+	}
+	/* We set the following in IRT entry
+	 * 28 : clear to indicate global delivery
+	 * 19 : set to indicate DB
+	 * 16-18 : 0
+	 * 0-15 : 0 => cpu0 */
+	for (i = 0; i < XLP_IRT_NUM; i++) {
+		val = XLP_IRTENT_SCH_LCL |  XLP_IRTENT_DT |
+			XLP_IRTENT_DB(node<<1) | XLP_IRTENT_DTE(0);
+		nlh_pic_w64r(node, XLP_PIC_IRT_ENTRY(i), val);
+	}
+}
+
diff --git a/arch/mips/netlogic/xlp/pic/pic-timer.c b/arch/mips/netlogic/xlp/pic/pic-timer.c
new file mode 100644
index 0000000..c060ad5
--- /dev/null
+++ b/arch/mips/netlogic/xlp/pic/pic-timer.c
@@ -0,0 +1,331 @@
+/*
+ * Implements the constant frequency timer on PIC
+ * Rationale :
+ * For dynamic frequency changes, we need a constant frequency timer
+ * because, by default, MIPS goes with high resolution timer using
+ * count compare based on the frequency of the CPU. (For information on
+ * how this is implemented, check clock event timer implementation given
+ * in  arch/mips/kernel/cevt-r4k.c.
+ * As a solution, if dynamic cpu frequency change is configured, cevt
+ * is disabled and PIC timer running at constant frequency is used
+ * instead. Although there are 7 sys timers (systimer 0-6) available,
+ * we use only two of them
+ */
+
+/* Caution : this file cannot be compiled as a module */
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/jiffies.h>
+#include <linux/spinlock.h>
+#include <linux/interrupt.h>
+#include <linux/clocksource.h>
+#include <linux/clockchips.h>
+#include <linux/io.h>
+#include <linux/bug.h>
+#include <asm/setup.h>
+#include <asm/irq.h>
+#include <asm/time.h>
+#include <asm/system.h>
+#include <asm/mipsregs.h>
+#include <asm/netlogic/xlp_irq.h>
+#include <asm/netlogic/xlp.h>
+#include "timer-base.h"
+
+/* Clocksource functions start START_CS */
+static cycle_t xlp_cs_read(struct clocksource *cs)
+{
+	u8 nid = xlp_str2node(cs->name);
+	return (cycles_t) (XLP_PIT_TIMER_MAX - xlp_timer_ctread(nid, xlp_str2timer(cs->name)));
+}
+
+static int xlp_cs_enable(struct clocksource *cs)
+{
+	u8 nid = xlp_str2node(cs->name);
+	return xlp_timer_enable(nid, xlp_str2timer(cs->name));
+}
+
+static void xlp_cs_disable(struct clocksource *cs)
+{
+	u8 nid = xlp_str2node(cs->name);
+	xlp_timer_disable(nid, xlp_str2timer(cs->name));
+}
+
+/* There are two distinct features of timers from 2.6.17 onwards.
+ * clock source is the monotonic continuous timer which has nothing to with
+ * interrupts, TOD ..etc. We use systimer 6 for this.
+ * @133 MHz, one cycle is 7.5 ns => mult = 30 and shift = 2.
+ */
+static struct clocksource xlp_pit_cs[NLM_MAX_CPU_NODE] = {
+	[0 ... NLM_MAX_CPU_NODE-1] = {
+	.name		= XLP_CSDTIMER,
+	.rating		= 300,
+	.read		= xlp_cs_read,
+	.mask		= CLOCKSOURCE_MASK(64),		/* clock is 64 bit */
+	.flags		= CLOCK_SOURCE_IS_CONTINUOUS,
+	.mult		= 30,
+	.shift		= 2,	/* for 100 cycles, (100 * 30) >> 2 == 750(ns) */
+	.enable		= xlp_cs_enable,
+	.disable	= xlp_cs_disable,
+	}
+};
+
+#if defined ARCH_HAS_READ_CURRENT_TIMER
+int read_current_timer(unsigned long *tv)
+{
+	*tv = (unsigned long)(xlp_cs_read(&xlp_pit_cs[0]));
+	return 0;
+}
+#endif
+
+/* Run time initialization of the clock source structure */
+static void xlp_pit_cs_init(u8 node)
+{
+	u8 timer;
+	char *ptr = kmalloc(XLP_SYSTIMER_OFFSET + 3, GFP_KERNEL);
+
+	if(ptr == NULL) {
+		panic("No memory to allocate clocksource\n");
+	}
+	memcpy(ptr, XLP_CSDTIMER, strlen(XLP_CSDTIMER));
+	ptr[XLP_SYSTIMER_OFFSET + 1] = '0' + node;
+	ptr[XLP_SYSTIMER_OFFSET + 2] = 0;
+	xlp_pit_cs[node].name = ptr;
+	timer = xlp_str2timer(xlp_pit_cs[node].name);
+	xlp_timer_mvwrite(node, timer, XLP_PIT_TIMER_MAX);
+	xlp_timer_enable(node, xlp_str2timer(xlp_pit_cs[node].name));
+	/* No run time paramter calculation because we are sure of the
+	 * values */
+	if (clocksource_register(&xlp_pit_cs[node])) {
+		panic("Cannot register clocksource on node %d\n", node);
+	}
+}
+
+/* END of clocksource functions END_CS*/
+
+/* Clock event device functions start START_CED
+ * Clock event device is the interrupt source. We have to configure some
+ * features on this, mainly oneshot timer and periodic timer. Periodic feature
+ * is used during boot up and later moved to oneshot. The function .set_mode
+ * is called with these paramters and we do set the PIT systimer accordingly
+ * in that function.
+ */
+
+int xlp_pit_ced_next_event(unsigned long evt, struct clock_event_device *ced)
+{
+	u8 timer = xlp_str2timer(ced->name);
+	u8 nid = xlp_str2node(ced->name);
+
+	/* We have delta (cycles, I assume, in future) for an event
+	 * Program a one shot timer with the CED with that many cycles for
+	 * interrupt */
+	xlp_timer_mvwrite(nid, timer, (u64)evt);
+	return 0;
+}
+
+void xlp_pit_ced_mode(enum clock_event_mode mode, struct clock_event_device *ced)
+{
+	u8 timer = xlp_str2timer(ced->name);
+	u8 node = xlp_str2node(ced->name);
+
+	switch (mode) {
+	case CLOCK_EVT_MODE_PERIODIC:
+		printk(KERN_DEBUG "[%d] Timer[%d]: Periodic\n", node, timer);
+		__xlp_timer_ctrl(node, timer, 1); /* Enable timer */
+		xlp_timer_mvwrite(node, timer, (u64)(XLP_PIT_TICK_RATE/HZ));
+		__xlp_timer_ctrl(node, timer, 1); /* Enable timer */
+		break;
+	case CLOCK_EVT_MODE_SHUTDOWN:
+		__xlp_timer_ctrl(node, timer, 0); /* Disable timer */
+		break;
+	case CLOCK_EVT_MODE_UNUSED:
+		if (ced->mode == CLOCK_EVT_MODE_PERIODIC ||
+		    ced->mode == CLOCK_EVT_MODE_ONESHOT)
+			__xlp_timer_ctrl(node, timer, 0); /* Disable timer */
+		break;
+	case CLOCK_EVT_MODE_ONESHOT:
+		printk(KERN_DEBUG "Set timer %d to Oneshot\n", timer);
+		__xlp_timer_ctrl(node, timer, 1); /* Enable timer */
+		xlp_timer_mvwrite(node, timer, (u64)(XLP_PIT_TICK_RATE/HZ));
+		break;
+
+	case CLOCK_EVT_MODE_RESUME:
+		/* Nothing to do here */
+		break;
+	}
+}
+
+static struct clock_event_device xlp_pit_ced[NLM_MAX_CPU_NODE] = {
+	[0 ... NLM_MAX_CPU_NODE-1] = {
+	.features       = CLOCK_EVT_FEAT_PERIODIC,
+	.max_delta_ns	= 0,	/* Change in init func */
+	.min_delta_ns	= 0,	/* Change in init func */
+	.shift		= 2,
+	.mult		= 30,
+	.rating		= 300,
+	.set_next_event	= xlp_pit_ced_next_event,
+	.set_mode	= xlp_pit_ced_mode,
+	}
+};
+
+void constrict_mask_to_node(u8, struct cpumask *, const struct cpumask *);
+struct cpumask xlp_ced_mask[NLM_MAX_CPU_NODE];
+int __init xlp_pit_ced_init(u8 nid)
+{
+	char *ptr = kmalloc(XLP_SYSTIMER_OFFSET + 3, GFP_KERNEL);
+
+	if(ptr == NULL) {
+		panic("No memory to allocate CED on node %d\n", nid);
+	}
+	memcpy(ptr, XLP_CEDTIMER, strlen(XLP_CEDTIMER));
+	ptr[XLP_SYSTIMER_OFFSET + 1] = '0' + nid;
+	ptr[XLP_SYSTIMER_OFFSET + 2] = 0;
+	xlp_pit_ced[nid].name = ptr;
+
+	xlp_pit_ced[nid].cpumask = cpumask_of(nid * NLM_MAX_CPU_PER_NODE);
+	xlp_pit_ced[nid].max_delta_ns = clockevent_delta2ns((u64)~0,
+			&xlp_pit_ced[nid]);
+	xlp_pit_ced[nid].min_delta_ns = clockevent_delta2ns(1ULL,
+			&xlp_pit_ced[nid]);
+	xlp_pit_ced[nid].irq = XLP_TIMER_IRQ(nid, xlp_str2timer(XLP_CEDTIMER));
+	clockevents_register_device(&xlp_pit_ced[nid]);
+	return 0;
+}
+
+static irqreturn_t xlp_timer_handler(int irq, void *dev_id)
+{
+	u8 nid;
+
+	preempt_disable();
+	nid = hard_smp_processor_id() / NLM_MAX_CPU_PER_NODE;
+	preempt_enable();
+
+	xlp_ack_pic(nid, xlp_irq_to_irt(irq));
+	/* We don't have to use dev_id; this avoids some type casting */
+	if (xlp_pit_ced[nid].event_handler) {
+		xlp_pit_ced[nid].event_handler(&xlp_pit_ced[nid]);
+	}
+	//do_IRQ(irq);
+	return IRQ_HANDLED;
+}
+
+
+/*
+ * Restricts timer mask to cpu
+ *
+ * @cpu : cpu number
+ * @timer : timer
+ */
+void program_timer_mask(u8 node, u8 timer, struct cpumask *m)
+{
+	struct irq_desc *desc;
+	unsigned long flags;
+
+	/* The following is a nasty hack, but there is no other way to do it
+	 * as far as I know. We make sure that desc has affinity (and status
+	 * flag already set before calling setup_irq */
+	desc = irq_to_desc(XLP_TIMER_IRQ(node, xlp_str2timer(XLP_CEDTIMER)));
+	spin_lock_irqsave(&desc->lock, flags);
+	desc->status = IRQ_AFFINITY_SET;
+	cpumask_copy(desc->affinity, m);
+	spin_unlock_irqrestore(&desc->lock, flags);
+}
+
+/* Called from netlogic/xlp/smp.c:nlm_cpus_done()
+ *
+ * This function reassigns cpumask for timer interrupts in the system
+ */
+extern int irq_select_affinity_usr(unsigned int);
+void xlp_reprogram_timer_masks(void)
+{
+	struct cpumask m, m2;
+	u8 nid;
+	u8 timer = xlp_str2timer(XLP_CEDTIMER);
+
+	for_each_online_node(nid) {
+		cpumask_setall(&m);
+		constrict_mask_to_node(nid, &m2, &m);
+		program_timer_mask(nid, timer, &m2);
+		irq_select_affinity_usr(XLP_TIMER_IRQ(nid, xlp_str2timer(XLP_CEDTIMER)));
+	}
+}
+
+static struct irqaction timer_irqa = {
+	.handler = xlp_timer_handler,
+	.flags = IRQF_DISABLED | IRQF_TIMER,
+	.name = "xlp_pit_timer",
+	.dev_id = (void *)xlp_pit_ced,
+};
+
+void xlp_disable_count_compare(u32 cpu)
+{
+	u32 cause;
+	u64 val;
+
+	printk(KERN_DEBUG "Disabling count_compare register on cpu %d\n", cpu);
+	/* Set bit 27 so that count is disabled for this cpu
+	 * Then clear rvec 7, and write non zero val to the compare reg
+	 * so that count and compare do not match */
+
+	val = read_64bit_cp0_eirr();
+	val &= ~(1 << XLP_IRQ_TIMER_RVEC);
+	write_64bit_cp0_eirr(val);
+
+	cause = read_c0_cause();
+	cause |= (1 << 27);	/* MIPS_CAUSE_DISABLE_COUNT_COMPARE */
+	write_c0_cause(cause);
+
+	/* Now write a non zero value to compare register */
+	write_c0_compare(0xffff);
+}
+
+void busy_delay(u32 milli)
+{
+	volatile idx = 0;
+	u64 max = milli << 20;
+	while (idx < max) idx++;
+}
+/*
+ * This function is called from include/asm/time.h:mips_clockevent_init()
+ * This is executed in every cpu during bootup
+ */
+int __init xlp_pic_ced_timer_init(int ced)
+{
+	u8 node;
+	u32 cpu;
+	int ret = 0;
+	struct cpumask m;
+	u8 timer = xlp_str2timer(XLP_CEDTIMER);
+
+	cpu = hard_smp_processor_id();
+	node = (u8)(cpu / NLM_MAX_CPU_PER_NODE);
+	xlp_disable_count_compare(cpu);
+	/* This code is valid only for CPU0 of the node (0, 32, 64 and 96) */
+	if ((cpu % NLM_MAX_CPU_PER_NODE) != 0) {
+		return 0;
+	}
+	ret = xlp_pit_ced_init(node);
+	if (ret != 0) {
+		panic("Clock Event Device initialization failure\n");
+	}
+	cpumask_clear(&m);
+	busy_delay(10);
+	cpumask_set_cpu(cpu, &m);
+	program_timer_mask(node, timer, &m);
+	if (setup_irq(XLP_TIMER_IRQ(node, timer), &timer_irqa) < 0) {
+		printk(KERN_WARNING "Node (%d) timer irq(%d) setup failed\n",
+				node, XLP_TIMER_IRQ(node, timer));
+	}
+	return 0;
+}
+/*
+ * This function is called from include/asm/time.h:init_mips_clocksource()
+ * This is executed in every cpu during bootup
+ */
+void __init xlp_pic_cs_timer_init(int cs)
+{
+	u8 node;
+
+	node = (u8)(hard_smp_processor_id() / NLM_MAX_CPU_PER_NODE);
+	xlp_pit_cs_init(node);
+	return;
+}
diff --git a/arch/mips/netlogic/xlp/pic/timer-base.c b/arch/mips/netlogic/xlp/pic/timer-base.c
new file mode 100644
index 0000000..232f319
--- /dev/null
+++ b/arch/mips/netlogic/xlp/pic/timer-base.c
@@ -0,0 +1,93 @@
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/jiffies.h>
+#include <linux/spinlock.h>
+#include <linux/interrupt.h>
+#include <linux/clocksource.h>
+#include <linux/clockchips.h>
+#include <linux/io.h>
+#include <linux/bug.h>
+#include <asm/setup.h>
+#include <asm/irq.h>
+#include <asm/time.h>
+#include <asm/system.h>
+#include <asm/mipsregs.h>
+#include <asm/netlogic/xlp_irq.h>
+#include <asm/netlogic/xlp.h>
+#include "timer-base.h"
+
+
+DEFINE_SPINLOCK(pit_lock);
+/* Must be called with corresponding locks held
+ * Enables/Disables timer in PIC_CTRL */
+void __xlp_timer_ctrl(u8 node, u8 timer, u8 en)
+{
+	u64 val;
+
+	val = nlh_pic_r64r(node, XLP_PIC_CTRL);
+	switch (en) {
+	case 0:
+		if (!(val & (1 << (timer + 10)))) { /* EN : bit 10-17 incl */
+		return;
+		}
+		val &= ~(1 << (timer + 10));
+		break;
+	default:
+		if (val & (1 << (timer + 10))) { /* EN : bit 10-17 incl */
+			return;
+		}
+		val |= (1 << (timer + 10));
+		break;
+	}
+	nlh_pic_w64r(node, XLP_PIC_CTRL, val);
+}
+
+int xlp_timer_enable(u8 node, u8 timer)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&pit_lock, flags);
+	__xlp_timer_ctrl(node, timer, 1);
+	spin_unlock_irqrestore(&pit_lock, flags);
+	return 0;
+}
+
+EXPORT_SYMBOL(xlp_timer_enable);
+
+void xlp_timer_disable(u8 node, u8 timer)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&pit_lock, flags);
+	__xlp_timer_ctrl(node, timer, 0);
+	spin_unlock_irqrestore(&pit_lock, flags);
+}
+EXPORT_SYMBOL(xlp_timer_disable);
+
+u64 xlp_timer_regread(u8 node, u32 reg)
+{
+	u64 val;
+
+	preempt_disable();
+	/* read the count */
+	val = nlh_pic_r64r(node, reg);
+	preempt_enable();
+	return val;
+}
+EXPORT_SYMBOL(xlp_timer_regread);
+
+int xlp_timer_regwrite(u8 node, u8 reg, u64 val)
+{
+	unsigned long flags;
+
+	/* We have delta (cycles, I assume, in future) for an event
+	 * Program a one shot timer with the CED with that many cycles for
+	 * interrupt */
+	spin_lock_irqsave(&pit_lock, flags);
+	/* Simply writing maxval reg will cause count decrement from that val */
+	nlh_pic_w64r(node, reg, val);
+	spin_unlock_irqrestore(&pit_lock, flags);
+	return 0;
+}
+EXPORT_SYMBOL(xlp_timer_regwrite);
+
diff --git a/arch/mips/netlogic/xlp/pic/timer-base.h b/arch/mips/netlogic/xlp/pic/timer-base.h
new file mode 100644
index 0000000..646d3eb
--- /dev/null
+++ b/arch/mips/netlogic/xlp/pic/timer-base.h
@@ -0,0 +1,42 @@
+#if !defined XLP_PIC_TIMER_BASE_H
+#define XLP_PIC_TIMER_BASE_H
+
+/* These names follow an important convention.
+ * [8] should denote the systimer no. */
+#define XLP_SYSTIMER0	"sys_pit_0"
+#define XLP_SYSTIMER1	"sys_pit_1"
+#define XLP_SYSTIMER2	"sys_pit_2"
+#define XLP_SYSTIMER3	"sys_pit_3"
+#define XLP_SYSTIMER4	"sys_pit_4"
+#define XLP_SYSTIMER5	"sys_pit_5"
+
+#define XLP_CSDTIMER	"sys_pit_1"
+#define XLP_CEDTIMER	"sys_pit_0"
+
+#define XLP_SYSTIMER_OFFSET	8	/* Index of the above string denoting the timer number; we bank on the fact that there are 8 timers only */
+#define xlp_str2timer(str)\
+({\
+	(u32) (str[XLP_SYSTIMER_OFFSET] - '0');\
+})
+#define xlp_str2node(str)\
+({\
+	(u32) (str[XLP_SYSTIMER_OFFSET + 1] - '0');\
+})
+
+extern spinlock_t pit_lock;
+int xlp_timer_enable(u8 nid, u8 timer);
+void xlp_timer_disable(u8 nid, u8 timer);
+u64 xlp_timer_regread(u8 nid, u32 reg);	/* reads any register of timers after locking*/
+int xlp_timer_regwrite(u8 nid, u8 timer, u64 val);/* writes any reg. of timers after locking */
+void __xlp_timer_ctrl(u8 nid, u8 timer, u8 en);	/* enable and disable without locking */
+
+#define xlp_timer_mvwrite(nid, timer, val)\
+({\
+xlp_timer_regwrite(nid, (u32)XLP_PIC_SYSTIMER_MAXVAL(timer), val);\
+ })
+
+#define xlp_timer_ctread(nid, timer)\
+({\
+xlp_timer_regread(nid, (u32)XLP_PIC_SYSTIMER_COUNT(timer));\
+ })
+#endif
diff --git a/arch/mips/netlogic/xlp/pic/timer-wd.c b/arch/mips/netlogic/xlp/pic/timer-wd.c
new file mode 100644
index 0000000..2a31f4c
--- /dev/null
+++ b/arch/mips/netlogic/xlp/pic/timer-wd.c
@@ -0,0 +1,61 @@
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/jiffies.h>
+#include <linux/spinlock.h>
+#include <linux/interrupt.h>
+#include <linux/clocksource.h>
+#include <linux/clockchips.h>
+#include <linux/io.h>
+#include <linux/bug.h>
+#include <asm/setup.h>
+#include <asm/irq.h>
+#include <asm/time.h>
+#include <asm/system.h>
+#include <asm/mipsregs.h>
+#include <asm/netlogic/xlp_irq.h>
+#include <asm/netlogic/xlp.h>
+#include "timer-base.h"
+
+#define MY_TIMER	XLP_SYSTIMER2"0"
+static u8 timer;
+/* Ideally this should be a platform driver;
+ * For the time being, just make a dummy driver */
+static irqreturn_t xlp_systimer_handler(int irq, void *p)
+{
+	u8 node = XLP_IRQ_TO_NODE(irq);
+	xlp_timer_mvwrite(node, timer, XLP_PIT_TICK_RATE); /* One Hz */
+	printk(KERN_DEBUG "in timer handler\n");
+	return IRQ_HANDLED;
+}
+
+int timer_wd_init(void)
+{
+	int ret;
+	u8 nid;
+
+	for_each_online_node(nid) {
+		timer = xlp_str2timer(MY_TIMER);
+		xlp_timer_mvwrite(nid, timer, XLP_PIT_TICK_RATE); /* One Hz */
+		xlp_timer_enable(nid, timer);
+		ret = request_irq(XLP_TIMER_IRQ(nid, timer), xlp_systimer_handler, 0, MY_TIMER, NULL);
+		if (ret < 0) {
+			return -EINVAL;
+		}
+	}
+	return 0;
+}
+
+void timer_wd_exit(void)
+{
+	u8 nid;
+
+	for_each_online_node(nid) {
+		xlp_timer_disable(nid, timer);
+		free_irq(XLP_TIMER_IRQ(nid, timer), NULL);
+	}
+}
+
+module_init(timer_wd_init);
+module_exit(timer_wd_exit);
+MODULE_LICENSE("GPL");
+
diff --git a/arch/mips/netlogic/xlp/platform.c b/arch/mips/netlogic/xlp/platform.c
index e0e7dbd..1d78573 100644
--- a/arch/mips/netlogic/xlp/platform.c
+++ b/arch/mips/netlogic/xlp/platform.c
@@ -36,7 +36,7 @@ THE POSSIBILITY OF SUCH DAMAGE.
 
 #include <asm/time.h>
 #include <asm/netlogic/hal/nlm_hal_macros.h>
-#include <asm/netlogic/xlp_hal_pic.h>
+#include <asm/netlogic/xlp_irq.h>
 #include <asm/netlogic/xlp.h>
 #include <asm/netlogic/xlp_usb.h>
 
@@ -95,7 +95,7 @@ static void xlp_init_uart(int port_id)
         xlp_uart_port[port_id].mapbase       = DEFAULT_NETLOGIC_IO_BASE 
 						+ NETLOGIC_IO_UART_0_OFFSET + port_id * XLP_UART_PORTIO_OFFSET;
         xlp_uart_port[port_id].membase       = (void __iomem *)xlp_uart_port[port_id].mapbase;
-        xlp_uart_port[port_id].irq           = XLP_UART_IRQ(port_id);
+        xlp_uart_port[port_id].irq           = XLP_UART_IRQ(0, port_id);
 
         xlp_uart_port[port_id].uartclk       = UART_CLK;
         xlp_uart_port[port_id].iotype        = UPIO_NLM;
@@ -231,7 +231,7 @@ static int xlp_find_pci_dev(void)
 						pres[0].end	= mmio;
 						pres[0].flags	= IORESOURCE_MEM;
 						irt = (nlm_hal_read_32bit_reg(mmio, DEV_IRT_INFO) & 0xFFFF);
-						irq = xlp_irt_to_irq(irt);
+						irq = xlp_irt_to_irq(0, irt);
 
 						pres[1].start = irq;
 						pres[1].end = irq;
diff --git a/arch/mips/netlogic/xlp/setup.c b/arch/mips/netlogic/xlp/setup.c
index f9767d5..fbfd8a5 100644
--- a/arch/mips/netlogic/xlp/setup.c
+++ b/arch/mips/netlogic/xlp/setup.c
@@ -34,6 +34,7 @@
 #include <linux/init.h>
 #include <linux/pm.h>
 
+#include <asm/netlogic/xlp_irq.h>
 #include <asm/irq.h>
 #include <asm/io.h>
 #include <asm/bootinfo.h>
@@ -58,13 +59,12 @@
 #include <asm/mach-netlogic/mmu.h>
 #include <asm/netlogic/bootinfo.h>
 #include <asm/netlogic/cpumask.h>
-#include <asm/netlogic/hal/nlm_hal_macros.h>
-#include <asm/netlogic/xlp_hal_pic.h>
+#include <asm/netlogic/xlp_ici.h>
+#include <asm/netlogic/hal/nlm_hal.h>
 #include <asm/mach-netlogic/nlm_kexec.h>
 #include <asm/netlogic/phnx_loader.h>
 #include "../boot/ops.h"
 #include "cpu_control_macros.h"
-
 /* Certain macros for this file
  */
 
@@ -157,22 +157,38 @@ struct xlp_stack_pages xlp_stack_pages_temp
 __attribute__((__section__(".data.init_task"),
 	       __aligned__(THREAD_SIZE)));
 
-struct boot_mem_map boot_physaddr_info;
-struct xlp_dram_mapping {
-                unsigned long low_pfn;
-                unsigned long high_pfn;
-                int node;
+struct nlm_ici_vc_param
+{
+	int own_credit;
+	int shared_credit;
+	int txwght;
+	int segth;
 };
-#define NLM_NODES_MAX_DRAM_REGION (NLM_MAX_DRAM_REGION * MAX_NUMNODES)
-struct xlp_dram_mapping  dram_map[NLM_NODES_MAX_DRAM_REGION];
 
-#define NLM_DRAM_BASE_REG_0     20
-#define NLM_DRAM_LIMIT_REG_0    28
-#define NLM_DRAM_NODEADDR_XLAT  36
-#define HDR_OFFSET      0x100
-#define BRIDGE  (0x00<<20) | (0x00<<15) | (0x00<<12)
-#define cpu_io_mmio_setup(node,offset)  ((__u32 *)(DEFAULT_NETLOGIC_IO_BASE + \
-                         (node<<18) + (offset) + HDR_OFFSET))
+struct nlm_ici_config
+{
+	int enable_config;
+	int total_credits;
+	int node_link_mask[MAX_NUMNODES];
+	struct nlm_ici_vc_param gcu_vc[ICI_MAX_GCU_VC];
+	struct nlm_ici_vc_param fmn_vc[ICI_MAX_FMN_VC];
+	struct nlm_ici_vc_param pic_vc[ICI_MAX_PIC_VC];
+}nlm_ici_config;
+
+struct boot_mem_map boot_physaddr_info;
+struct xlp_dram_mapping {          
+                unsigned long low_pfn; 
+                unsigned long high_pfn;        
+                int node;   
+};        
+#define NLM_NODES_MAX_DRAM_REGION (NLM_MAX_DRAM_REGION * MAX_NUMNODES)      
+struct xlp_dram_mapping  dram_map[NLM_NODES_MAX_DRAM_REGION];        
+          
+#define NLM_DRAM_BASE_REG_0     20 
+#define NLM_DRAM_LIMIT_REG_0    28 
+#define NLM_DRAM_NODEADDR_XLAT  36 
+#define HDR_OFFSET      0x100      
+#define BRIDGE  (0x00<<20) | (0x00<<15) | (0x00<<12)        
 
 int nlm_common_get_pgprot(unsigned long address)
 {
@@ -212,39 +228,39 @@ int valid_mmap_nlm_common_addr_range(unsigned long pfn)
 	return 0;
 }
 
-void read_node_bars(int node)
-{
+void read_node_bars(int node)    
+{         
         int i, idx;
-        uint32_t *membase = cpu_io_mmio_setup(node, BRIDGE);
-
+        uint32_t *membase = cpu_io_mmio(node, BRIDGE);
+          
         for (i = 0; i < NLM_MAX_DRAM_REGION; i++) {
-                uint64_t base_reg  = *(membase + NLM_DRAM_BASE_REG_0  + i);
-                uint64_t limit_reg = *(membase + NLM_DRAM_LIMIT_REG_0 + i);
-                uint32_t node_reg = *(membase + NLM_DRAM_NODEADDR_XLAT + i);
-
-                if(((node_reg >> 1) & 0x3) != node) {
-                        continue;
-                }
-                if(((limit_reg >> 12) << 20) == 0) {
-                        continue;
-                }
-
-                idx = (node * NLM_MAX_DRAM_REGION) + i;
-                dram_map[idx].low_pfn = ((base_reg >> 12) << 20) >> PAGE_SHIFT;
+                uint64_t base_reg  = nlm_hal_read_32bit_reg((uint64_t)membase, NLM_DRAM_BASE_REG_0 + i);
+                uint64_t limit_reg = nlm_hal_read_32bit_reg((uint64_t)membase, NLM_DRAM_LIMIT_REG_0 + i);
+                uint32_t node_reg =  nlm_hal_read_32bit_reg((uint64_t)membase, NLM_DRAM_NODEADDR_XLAT + i);
+
+                if(((node_reg >> 1) & 0x3) != node) {       
+                        continue;  
+                }  
+                if(((limit_reg >> 12) << 20) == 0) {        
+                        continue;  
+                }  
+
+                idx = (node * NLM_MAX_DRAM_REGION) + i;     
+                dram_map[idx].low_pfn = ((base_reg >> 12) << 20) >> PAGE_SHIFT;          
                 dram_map[idx].high_pfn =
                         ((limit_reg >> 12) << 20) >> PAGE_SHIFT;
-                dram_map[idx].node = node;
-
+                dram_map[idx].node = node;         
+          
                 if(dram_map[idx].high_pfn == dram_map[idx].low_pfn){
                     continue;
                 }
-                boot_physaddr_info.map[boot_physaddr_info.nr_map].addr = dram_map[idx].low_pfn << PAGE_SHIFT;
-                boot_physaddr_info.map[boot_physaddr_info.nr_map].size =
-                        (dram_map[idx].high_pfn - dram_map[idx].low_pfn + (1<<(20-PAGE_SHIFT))) << PAGE_SHIFT;
-                boot_physaddr_info.map[boot_physaddr_info.nr_map].type = BOOT_MEM_RAM;
-                boot_physaddr_info.nr_map++;
-        }
-}
+                boot_physaddr_info.map[boot_physaddr_info.nr_map].addr = dram_map[idx].low_pfn << PAGE_SHIFT;     
+                boot_physaddr_info.map[boot_physaddr_info.nr_map].size = 
+                        (dram_map[idx].high_pfn - dram_map[idx].low_pfn + (1<<(20-PAGE_SHIFT))) << PAGE_SHIFT;  
+                boot_physaddr_info.map[boot_physaddr_info.nr_map].type = BOOT_MEM_RAM;   
+                boot_physaddr_info.nr_map++;       
+        } 
+}         
 
 void nlm_get_dram_mapping(void)
 {
@@ -351,18 +367,24 @@ void prom_reconfigure_thr_resources(void)
 }
 #endif
 
+#if !defined CONFIG_XLP_REPLACE_R4K_TIMER
 unsigned int __cpuinit get_c0_compare_int(void)
 {
-    return XLP_IRQ_TIMER;
+    return XLP_IRQ_TIMER_RVEC;
 }
+#endif
 
 /* TODO: Get this from FDT */
 void plat_time_init(void)
 {
+#if !defined CONFIG_XLP_REPLACE_R4K_TIMER
 	extern void nlm_common_timer_setup(void);
+#endif
 	mips_hpt_frequency = (unsigned int) get_cpu_freq(XLP_CPU0);
 	printk("mips_hpt_frequency = %u\n", mips_hpt_frequency);
+#if !defined CONFIG_XLP_REPLACE_R4K_TIMER
 	nlm_common_timer_setup();
+#endif
 }
 
 void __init plat_mem_setup(void)
@@ -405,7 +427,8 @@ void __init nlm_nmi_setup (void)
 
 /* setup early serial port driver */
 #ifdef CONFIG_SERIAL_8250
-#define UART_CLK 133333333
+#define UART_CLK_133MHz 133333333
+#define UART_CLK_66MHz   66666666
 
 static void nlm_early_serial_setup(int uart_id)
 {
@@ -418,11 +441,13 @@ static void nlm_early_serial_setup(int uart_id)
 	s.regshift = 2; /* registers are 4 bytes wide */
 	/* hardware int 4 - the serial int, is CPU int 6
 	 but poll for now */
-	s.uartclk = UART_CLK;
+
+	s.uartclk = UART_CLK_133MHz;
+
 	switch(uart_id){
 		default:
 		case 0:
-			s.irq = XLP_UART_IRQ(0);
+			s.irq = XLP_UART_IRQ(0, 0);
 			s.membase = (unsigned char __iomem *)
 			(DEFAULT_NETLOGIC_IO_BASE + NETLOGIC_IO_UART_0_OFFSET);
 			s.mapbase = (DEFAULT_NETLOGIC_IO_BASE +
@@ -430,7 +455,7 @@ static void nlm_early_serial_setup(int uart_id)
 			s.line = 0;
 			break;
 		case 1:
-			s.irq =  XLP_UART_IRQ(1);
+			s.irq =  XLP_UART_IRQ(0, 1);
 			s.membase = (unsigned char __iomem *)
 			(DEFAULT_NETLOGIC_IO_BASE + NETLOGIC_IO_UART_1_OFFSET);
 			s.mapbase = (DEFAULT_NETLOGIC_IO_BASE +
@@ -440,6 +465,7 @@ static void nlm_early_serial_setup(int uart_id)
 	}
 	s.serial_in	= xlp_uart_in;
 	s.serial_out	= xlp_uart_out;
+	printk(KERN_DEBUG "SETTING UP EARLY SERIAL ACCESS\n");
 	if (early_serial_setup(&s) != 0) {
 		printk(KERN_ERR "Serial setup failed!\n");
 	}
@@ -469,12 +495,15 @@ EXPORT_SYMBOL(nlm_get_fdt_app_param);
 int nae_rx_vc = -1, nae_fb_vc = -1;
 int sae_rx_vc = -1, sae_rx_sync_vc = -1;
 int ipsec_async_vc = -1, ipsec_sync_vc = -1;
-static void parse_fdt_sae_vc_config()
+static void parse_fdt_sae_vc_config(void)
 {
 	void *node;
+	void * valid_node;
 	int i;
-	node = finddevice("/doms/dom@0/cpu");
+	int num_nodes = 1;
 	extern uint32_t nlm_cpu_vc_mask[NLM_MAX_CPU_NODE*NLM_MAX_CPU_PER_NODE];
+
+	node = finddevice("/doms/dom@0/cpu");
 	if(node) {
 		if (getprop(node, "nae-rx-vc", &nae_rx_vc, 4) > 0)
                         nae_rx_vc = fdt32_to_cpu(nae_rx_vc);
@@ -493,12 +522,17 @@ static void parse_fdt_sae_vc_config()
 
 		if (getprop(node, "ipsec-sync-vc", &ipsec_sync_vc, 4) > 0)
 			ipsec_sync_vc = fdt32_to_cpu(ipsec_sync_vc);
-		
-		for(i =0 ; i < NLM_MAX_CPU_NODE*NLM_MAX_CPU_PER_NODE; i++) {
+
+		valid_node  = finddevice("/soc/nodes");
+
+		if (getprop(valid_node, "num-nodes", &num_nodes, 4) > 0 )
+			num_nodes = fdt32_to_cpu(num_nodes);
+
+		for(i =0 ; i < num_nodes*NLM_MAX_CPU_PER_NODE; i++) {
 			if(nlm_cpu_vc_mask[i] & (1 << ipsec_sync_vc)) {
 				ipsec_sync_vc = -1;
 			}
-				 
+
 		}
 	}
 
@@ -511,6 +545,143 @@ EXPORT_SYMBOL(sae_rx_sync_vc);
 EXPORT_SYMBOL(ipsec_async_vc);
 EXPORT_SYMBOL(ipsec_sync_vc);
 
+static void ici_read_vc_parameter(void *node, int max_vc, struct nlm_ici_vc_param *vc_index)
+{
+	uint32_t tmp_data[32];
+	int i = 0;
+
+	if(getprop(node, "own_credit", &tmp_data, sizeof(int)*max_vc) < 0) {
+		for(i=0; i<max_vc; i++){
+			(vc_index+i)->own_credit = 64;
+		}
+	}else{
+		for(i=0; i<max_vc; i++){
+			(vc_index+i)->own_credit = tmp_data[i];
+		}
+	}
+
+	if(getprop(node, "shared_credit", &tmp_data, sizeof(int)*max_vc) < 0) {
+		for(i=0; i<max_vc; i++){
+			(vc_index+i)->shared_credit = 1;
+		}
+	}else{
+		for(i=0; i<max_vc; i++){
+			(vc_index+i)->shared_credit = tmp_data[i];
+		}
+	}
+
+	if(getprop(node, "txwght", &tmp_data, sizeof(int)*max_vc) < 0) {
+		for(i=0; i<max_vc; i++){
+			(vc_index+i)->txwght = 64;
+		}
+	}else{
+		for(i=0; i<max_vc; i++){
+			(vc_index+i)->txwght = tmp_data[i];
+		}
+	}
+
+	if(getprop(node, "segth", &tmp_data, sizeof(int)*max_vc) < 0) {
+		for(i=0; i<max_vc; i++){
+			(vc_index+i)->segth = 2;
+		}
+	}else{
+		for(i=0; i<max_vc; i++){
+			(vc_index+i)->segth = tmp_data[i];
+		}
+	}
+}
+static void ici_dump_vc_info(const char *header, int max_vc, struct nlm_ici_vc_param *vc)
+{
+	printk("[=== %s ===]\n", header);
+	
+	if(max_vc == 8){
+		printk("OWN:    %d %d %d %d %d %d %d %d\n",(vc+0)->own_credit, (vc+1)->own_credit, (vc+2)->own_credit, 
+				(vc+3)->own_credit,	(vc+4)->own_credit, (vc+5)->own_credit, (vc+6)->own_credit, (vc+7)->own_credit);
+		printk("SHARED: %d %d %d %d %d %d %d %d\n",(vc+0)->shared_credit, (vc+1)->shared_credit, (vc+2)->shared_credit, 
+				(vc+3)->shared_credit,	(vc+4)->shared_credit, (vc+5)->shared_credit, (vc+6)->shared_credit, 
+				(vc+7)->shared_credit);
+		printk("TXWGHT: %d %d %d %d %d %d %d %d\n",(vc+0)->txwght, (vc+1)->txwght, (vc+2)->txwght, (vc+3)->txwght,
+				(vc+4)->txwght, (vc+5)->txwght, (vc+6)->txwght, (vc+7)->txwght);
+		printk("SEGTH:  %d %d %d %d %d %d %d %d\n",(vc+0)->segth, (vc+1)->segth, (vc+2)->segth, (vc+3)->segth,
+				(vc+4)->segth, (vc+5)->segth, (vc+6)->segth, (vc+7)->segth);
+	}else if (max_vc == 4){
+		printk("OWN:    %d %d %d %d\n",(vc+0)->own_credit, (vc+1)->own_credit, (vc+2)->own_credit, (vc+3)->own_credit);
+		printk("SHARED: %d %d %d %d \n",(vc+0)->shared_credit, (vc+1)->shared_credit, (vc+2)->shared_credit, 
+				(vc+3)->shared_credit);
+		printk("TXWGHT: %d %d %d %d\n",(vc+0)->txwght, (vc+1)->txwght, (vc+2)->txwght, (vc+3)->txwght);
+		printk("SEGTH:  %d %d %d %d\n",(vc+0)->segth, (vc+1)->segth, (vc+2)->segth, (vc+3)->segth);
+
+	}else if(max_vc == 1){
+		printk("OWN:    %d\n",(vc+0)->own_credit);
+		printk("SHARED: %d \n",(vc+0)->shared_credit);
+		printk("TXWGHT: %d \n",(vc+0)->txwght);
+		printk("SEGTH:  %d \n",(vc+0)->segth);
+	}
+}
+
+static void parse_ici_parameters(void)
+{
+	char domstr[32] = "";
+	void *node;
+	int temp;
+	int domain = 0;
+
+	sprintf(domstr, "/doms/dom@%d/ici-config", domain);
+	node = finddevice(domstr);
+	if(node){
+			if(getprop(node, "enable", &nlm_ici_config.enable_config, sizeof(int)) < 0){
+				nlm_ici_config.enable_config = 0;
+			}
+			if(getprop(node, "total_credits", &nlm_ici_config.total_credits, sizeof(int)) < 0){
+				nlm_ici_config.total_credits = 1023;
+			}
+			if(getprop(node, "node_0_link_mask", &nlm_ici_config.node_link_mask[0], sizeof(int))<0){
+				nlm_ici_config.node_link_mask[0] = 0;
+			}
+			if(getprop(node, "node_1_link_mask", &nlm_ici_config.node_link_mask[1], sizeof(int))<0){
+				nlm_ici_config.node_link_mask[1] = 0;
+			}
+			if(getprop(node, "node_2_link_mask", &nlm_ici_config.node_link_mask[2], sizeof(int))<0){
+				nlm_ici_config.node_link_mask[2] = 0;
+			}
+			if(getprop(node, "node_3_link_mask", &nlm_ici_config.node_link_mask[3], sizeof(int))<0){
+				nlm_ici_config.node_link_mask[3] = 0;
+			}
+	}
+	if(!nlm_ici_config.enable_config){
+		printk("ICI config not enabled\n");
+		goto end;
+	}
+
+	sprintf(domstr, "/doms/dom@%d/ici-config/gcu", domain);
+	node = finddevice(domstr);
+	if(node){
+			ici_read_vc_parameter(node, ICI_MAX_GCU_VC, &nlm_ici_config.gcu_vc[0]);
+	}
+
+	sprintf(domstr, "/doms/dom@%d/ici-config/fmn", domain);
+	node = finddevice(domstr);
+	if(node){
+			ici_read_vc_parameter(node, ICI_MAX_FMN_VC, &nlm_ici_config.fmn_vc[0]);
+	}
+
+	sprintf(domstr, "/doms/dom@%d/ici-config/pic", domain);
+	node = finddevice(domstr);
+	if(node){
+			ici_read_vc_parameter(node, ICI_MAX_PIC_VC, &nlm_ici_config.pic_vc[0]);
+	}
+#if 0
+	printk("\n:::::::::ICI CONFIGURATION:::::::::\n");
+	printk("TotalCredits %d, EnableConfig %d\n",nlm_ici_config.total_credits, nlm_ici_config.enable_config);
+	ici_dump_vc_info("GCU", ICI_MAX_GCU_VC, &nlm_ici_config.gcu_vc[0]);
+	ici_dump_vc_info("FMN", ICI_MAX_FMN_VC, &nlm_ici_config.fmn_vc[0]);
+	ici_dump_vc_info("PIC", ICI_MAX_PIC_VC, &nlm_ici_config.pic_vc[0]);
+	printk("LinkMask: Node0: %#x, Node1: %#x, Node2: %#x, Node3: %#x\n", nlm_ici_config.node_link_mask[0],
+			nlm_ici_config.node_link_mask[1], nlm_ici_config.node_link_mask[2], nlm_ici_config.node_link_mask[3]);
+#endif
+end:
+	return;
+}
 
 static int fdt_process(void)
 {
@@ -840,17 +1011,15 @@ noloadermask:
 	/* Parse the sae async/sync vcs for linux userspace model */
 	parse_fdt_sae_vc_config();
 
+	parse_ici_parameters();
 	return 0;
 }
 
 char* get_cpu_info(void)
 {
 	struct nlm_netl_proc_info cpu_info;
-	if(nlm_hal_get_cpuinfo(&cpu_info)){
-		strcpy(cpu_model_info, cpu_info.cpu_info_str);
-	}else{
-		strcpy(cpu_model_info, "Unknown CPU");
-	}
+	nlm_hal_get_cpuinfo(&cpu_info);
+	strcpy(cpu_model_info, cpu_info.cpu_info_str);
 	return cpu_model_info;
 }
 
@@ -927,7 +1096,71 @@ void __init build_node_cpu_map(void)
 		i++;
 	}
 	printk("Number of online nodes = %d\n", num_online_nodes());
+
 }
+
+static void nlm_update_ici_credits(int node, int link)
+{
+	volatile uint32_t *mmio;
+	volatile int i = 0;
+
+	mmio = (volatile uint32_t *)(0xffffffffb0000000ULL|(unsigned long)ici_io_mmio(node, link));
+
+	/*Configure GCU vcs*/
+	for(i=0; i<ICI_MAX_GCU_VC; i++){
+		mmio[ICI_CREDOWN0 + ICI_GCU_VC_START + i] = nlm_ici_config.gcu_vc[i].own_credit;
+		mmio[ICI_CREDSHARE0 + ICI_GCU_VC_START + i] = nlm_ici_config.gcu_vc[i].shared_credit;
+		mmio[ICI_TXWGHT0 + ICI_GCU_VC_START + i] = nlm_ici_config.gcu_vc[i].txwght;
+		mmio[ICI_TXSEGTH0 + ICI_GCU_VC_START + i] = nlm_ici_config.gcu_vc[i].segth;
+	}
+
+	/*Configure FMN vcs*/
+	for(i=0; i<ICI_MAX_FMN_VC; i++){
+		mmio[ICI_CREDOWN0 + ICI_FMN_VC_START + i] = nlm_ici_config.fmn_vc[i].own_credit;
+		mmio[ICI_CREDSHARE0 + ICI_FMN_VC_START + i] = nlm_ici_config.fmn_vc[i].shared_credit;
+		mmio[ICI_TXWGHT0 + ICI_FMN_VC_START + i] = nlm_ici_config.fmn_vc[i].txwght;
+		mmio[ICI_TXSEGTH0 + ICI_FMN_VC_START + i] = nlm_ici_config.fmn_vc[i].segth;
+	}
+
+	/*Configure PIC vcs*/
+	for(i=0; i<ICI_MAX_PIC_VC; i++){
+		mmio[ICI_CREDOWN0 + ICI_PIC_VC_START + i] = nlm_ici_config.pic_vc[i].own_credit;
+		mmio[ICI_CREDSHARE0 + ICI_PIC_VC_START + i] = nlm_ici_config.pic_vc[i].shared_credit;
+		mmio[ICI_TXWGHT0 + ICI_PIC_VC_START + i] = nlm_ici_config.pic_vc[i].txwght;
+		mmio[ICI_TXSEGTH0 + ICI_PIC_VC_START + i] = nlm_ici_config.pic_vc[i].segth;
+	}
+
+	/*Configure Total Credits*/
+	mmio[ICI_CREDTOT] = nlm_ici_config.total_credits;
+
+	/*Update Credit Load*/
+	mmio[ICI_CREDLD] = mmio[ICI_CREDLD] | 0x1;
+
+	printk("Reconfigured ICI Node %d, Link %d\n", node, link);
+}
+
+int nlm_config_ici(void)
+{
+	/*configure ICI*/
+	int node, link;
+	uint32_t active_link;
+
+	if(!nlm_ici_config.enable_config){
+		return -1;
+	}
+
+	for(node=0; node<MAX_NUMNODES; node++){
+		if(node_online(node)) {
+			active_link = nlm_ici_config.node_link_mask[node];
+			for(link=0; link<ICI_NUM_LINKS; link++){
+				if(active_link & (1<<link)){
+					nlm_update_ici_credits(node, link);
+				}
+			}
+		}
+	}
+}
+
 #endif
 
 #ifdef CONFIG_KEXEC
@@ -952,6 +1185,7 @@ void __init prom_init(void)
 
 #ifdef CONFIG_NUMA
 	build_node_cpu_map();
+	nlm_config_ici();
 #endif
 
 	xen_init();
@@ -972,8 +1206,10 @@ void __init prom_init(void)
 	c0status = read_c0_status() | ST0_CU2;
 	write_c0_status(c0status);
 #endif
+	cpumask_clear(&phys_cpu_present_map);
 	cpumask_clear(&smp_boot.online_map);
 	cpumask_set_cpu(hard_smp_processor_id(), &smp_boot.online_map);
+	cpumask_set_cpu(hard_smp_processor_id(), &phys_cpu_present_map);
 
 	board_nmi_handler_setup = nlm_nmi_setup;
 
diff --git a/arch/mips/netlogic/xlp/smp.c b/arch/mips/netlogic/xlp/smp.c
index 8521b31..64b2bfc 100644
--- a/arch/mips/netlogic/xlp/smp.c
+++ b/arch/mips/netlogic/xlp/smp.c
@@ -36,9 +36,10 @@ THE POSSIBILITY OF SUCH DAMAGE.
 #include <asm/cacheops.h>
 
 #include <asm/netlogic/xlp.h>
+#include <asm/netlogic/msgring.h>
 #include <asm/netlogic/mips-exts.h>
 #include <asm/netlogic/interrupt.h>
-#include <asm/netlogic/xlp_hal_pic.h>
+#include <asm/netlogic/xlp_irq.h>
 #include <asm/netlogic/hal/nlm_hal_fmn.h>
 #include <linux/module.h>
 
@@ -47,10 +48,12 @@ THE POSSIBILITY OF SUCH DAMAGE.
 #include <asm/processor.h>
 
 #include <asm/netlogic/cpumask.h>
+#include <linux/nodemask.h>
 
 #include <asm/mach-netlogic/mmu.h>
 
 #include "cpu_control_macros.h"
+#include "pic/timer-base.h"
 
 struct smp_boot_info smp_boot;
 EXPORT_SYMBOL(smp_boot);
@@ -67,32 +70,33 @@ extern void enable_cpus(unsigned int node, unsigned online_mask);
 extern void nlm_smp_irq_init(void);
 extern void asmlinkage smp_bootstrap(void);
 extern void enable_msgconfig_int(void);
-extern void xlp_pic_ite_init(const struct cpumask *);
 
-void nlm_send_ipi_single(int logical_cpu, unsigned int action)
+/*
+ * Input parameter is logical cpu number.
+ * Should convert to physical cpu before using it
+ */
+void nlm_send_ipi_single(int lcpu, unsigned int action)
 {
-	int cpu = cpu_logical_map(logical_cpu);
-        __u32 node = cpu / 32;
+	int phys_cpu = cpu_logical_map(lcpu);
+        __u32 node = phys_cpu / 32;
         __u32 ipi = 0;
 	__u8 nmi = 0;
-	cpu = cpu % 32;
+	phys_cpu = phys_cpu % 32;	/* This need to be changed for NUMA?? */
 
         if (action & SMP_CALL_FUNCTION) {
-                ipi |= XLP_IRQ_IPI_SMP_FUNCTION;
+                ipi |= XLP_IRQ_IPI_SMP_FUNCTION_RVEC;
 	} else if (action & SMP_RESCHEDULE_YOURSELF) {
-                ipi |= XLP_IRQ_IPI_SMP_RESCHEDULE;
-	} else if (action & SMP_CALL_KGDB_HOOK) {
-                ipi |= XLP_IRQ_IPI_SMP_KGDB;
+                ipi |= XLP_IRQ_IPI_SMP_RESCHEDULE_RVEC;
+	} else if (action & SMP_CALL_KGDB_HOOK_RVEC) {
+                ipi |= XLP_IRQ_IPI_SMP_KGDB_RVEC;
 		/* for KGDB enable NMI also */
 		nmi = 1;
-	} else if (action & SMP_OPROFILE_IPI) {
-                ipi |= 51;
         } else
 		return;
 
 	smp_mb();
 
-        nlm_hal_pic_send_ipi(nmi, (ipi & 0x3f), node, cpu);
+        nlm_hal_pic_send_ipi(nmi, (ipi & 0x3f), node, phys_cpu);
 }
 
 void nlm_send_ipi_mask(const struct cpumask * mask, unsigned int action)
@@ -104,28 +108,30 @@ void nlm_send_ipi_mask(const struct cpumask * mask, unsigned int action)
 	}
 }
 
+#if !defined CONFIG_XLP_REPLACE_R4K_TIMER
 extern void nlm_common_timer_setup(void);
+#endif
+
 /*
  * Code to run on secondary just after probing the CPU
  */
+
+void xlp_pic_init(u8 node);
 static void __cpuinit nlm_init_secondary(void)
 {
-    /* Time init for this cpu is done in mips_clockevent_init() */
-    nlm_smp_irq_init();
-    enable_msgconfig_int();
-#ifdef CONFIG_NLM_XLP_A0_WORKAROUNDS
-    /* Workaround for XLP A0 Multi-Node bug */
-    {
-	    int cpu = hard_smp_processor_id();
-
-	    if ( (cpu % 32) == 0) {
-		    /* If this cpu@0 of any of the nodes, initialize PIC */
-		    nlm_common_timer_setup();
-	    }
-    }
+	int cpu;
+	cpu = hard_smp_processor_id();
+	/* Time init for this cpu is done in mips_clockevent_init() */
+	nlm_smp_irq_init();
+	enable_msgconfig_int();
+
+	/* Workaround for XLP A0 Multi-Node bug */
+	if ((cpu % NLM_MAX_CPU_PER_NODE) == 0) {
+		xlp_pic_init(cpu/NLM_MAX_CPU_PER_NODE);
+#if defined CONFIG_NLM_XLP && !defined CONFIG_XLP_REPLACE_R4K_TIMER
+		nlm_common_timer_setup();
 #endif
-    /* Enable vc interupts for this thread*/
-    nlm_enable_vc_intr();
+	}
 }
 
 #ifdef CONFIG_HOTPLUG_CPU
@@ -183,9 +189,11 @@ late_initcall(register_nlm_notifier);
 void nlm_smp_finish(void)
 {
 #ifdef CONFIG_HOTPLUG_CPU
+	u8 ite, node;
 	int cpu = hard_smp_processor_id();
 	uint32_t bitmask = 0;
 	unsigned long flags;
+	node = cpu/32;
 
 	if (atomic_read(&cpu_hotplug_flag)) {
 		spin_lock_irqsave(&smp_reserve_lock, flags);
@@ -193,36 +201,58 @@ void nlm_smp_finish(void)
 		fixup_irqs(smp_processor_id(), 1);
 		bitmask = cpumask_to_uint32(&cpu_online_map);
 		bitmask = bitmask | (1 << cpu);
-		set_ite(bitmask);
+		for (ite = 0; ite < XLP_ITE_ENTRIES; ite++) {
+			xlp_ite_cpu_set(node, cpu, ite);
+		}
 		spin_unlock_irqrestore(&smp_reserve_lock, flags);
 	}
 #endif
 }
 
+void reassign_irq_mask(void);
+int irq_select_affinity_usr(unsigned int);
+void xlp_prog_all_node_ites(void );
+void xlp_reprogram_timer_masks(void);
+
 void nlm_cpus_done(void)
 {
+	unsigned long flags;
 	extern void *fdt;
+	u8 nid;
+	struct irq_desc *desc;
+	struct cpumask m;
+
 	nlm_hal_fmn_init(fdt);
+	/* Enable vc interupts for the online cpus */
+	nlm_enable_vc_intr();
+	xlp_prog_all_node_ites();
+#if defined CONFIG_XLP_REPLACE_R4K_TIMER
+	/* irq_select_affinity_usr(XLP_TIMER_IRQ(0, 0)); */
+	xlp_reprogram_timer_masks();
+#endif
 }
 
-/* Boot all other cpus in the system, initialize them, and
-   bring them into the boot fn */
-void nlm_boot_secondary(int logical_cpu, struct task_struct *idle)
+/*
+ * init/main.c : smp_init ==> cpu_up ==> _cpu_up => __cpu_up (arch/mips/kernel/
+ * smp.c) ==> mp_ops->boot_secondary
+ * The cpu argument is the bit number from cpu_present_mask (for_each_online_cpu
+ * ) => logical cpu id
+ */
+void nlm_boot_secondary(int cpu, struct task_struct *idle)
 {
 	unsigned long gp = (unsigned long)task_thread_info(idle);
 	unsigned long sp = (unsigned long)__KSTK_TOS(idle);
-	int cpu = cpu_logical_map(logical_cpu);
-	
-	printk("nlm_boot_secondary: logical cpu %d physical cpu %d\n", logical_cpu, cpu);
+	int phys_cpu = cpu_logical_map(cpu);
 
+	printk("nlm_boot_secondary: logical cpu %d physical cpu %d\n", cpu, phys_cpu);
 
-	smp_boot.boot_info[cpu].sp = sp;
-	smp_boot.boot_info[cpu].gp = gp;
-	smp_boot.boot_info[cpu].fn = (unsigned long)&smp_bootstrap;
+	smp_boot.boot_info[phys_cpu].sp = sp;
+	smp_boot.boot_info[phys_cpu].gp = gp;
+	smp_boot.boot_info[phys_cpu].fn = (unsigned long)&smp_bootstrap;
 
 	/* barrier */
 	__sync();
-	smp_boot.boot_info[cpu].ready = 1;
+	smp_boot.boot_info[phys_cpu].ready = 1;
 
 /*  	printk("[%s]: (PROM): sent a wakeup message to cpu %d\n", __FUNCTION__, cpu);  */
 }
@@ -258,21 +288,21 @@ void __init nlm_smp_setup(void)
 		if (cpumask_test_cpu(i, &fdt_cpumask)) {
 			__cpu_number_map[i] = num_cpus;
 			__cpu_logical_map[num_cpus] = i;
-			cpu_data[i].core = (int) (i/XLP_THREADS_PER_CORE);
+			cpu_data[num_cpus].core = (int) (__cpu_logical_map[num_cpus]/XLP_THREADS_PER_CORE);
+			cpu_set(num_cpus, cpu_possible_map);
 			cpu_set(i, phys_cpu_present_map);
 			num_cpus++;
 		}
 	}
-
-	cpumask_copy(&cpu_present_map, &fdt_cpumask);
-	cpumask_copy(&cpu_possible_map, &cpu_present_map);
-	cpumask_scnprintf(buf, CPUMASK_BUF, &fdt_cpumask);
+	cpumask_scnprintf(buf, CPUMASK_BUF, &cpu_online_mask);
+	printk("Online mask %s\n", buf);
+	cpu_present_map = cpu_possible_map;
+	cpumask_scnprintf(buf, CPUMASK_BUF, &cpu_present_map);
+	printk("Present CPU map %s\n", buf);
 	cpumask_scnprintf(buf, CPUMASK_BUF, &cpu_possible_map);
-	printk("Possible/Present CPU map %s\n", buf);
-
+	printk("Possible CPU map %s\n", buf);
 	printk("Detected %d Slave CPU(s)\n", num_cpus);
 	/* Setup PIC with cpu_present_mask */
-	xlp_pic_ite_init((const struct cpumask *)&cpu_present_map);
 }
 
 #define hw_enable_cpus enable_cpus
@@ -282,6 +312,9 @@ static inline void config_mmu(void)
 {
 	uint32_t tmp0;
 
+	if (!xlp8xx_a01_workaround_needed)
+		return;
+
 	/*
 	 * Dummy write for A0 bug in MMU
 	 */
@@ -290,8 +323,6 @@ static inline void config_mmu(void)
 		".set noreorder\n"
 		"li      %0, "STR(MMU_SETUP)"\n"
 		"mtcr    $0, %0\n"
-		"nop\n"
-		"nop\n"
 		"ehb\n"
 		".set pop\n"
 		: "=r" (tmp0)
@@ -309,7 +340,7 @@ int wakeup_secondary_cpus(void)
 	cpumask_clear(&mask32);
 	uint32_to_cpumask(&mask32, 0xffffffff);
 
-	for (node = 0; node < 1; node++) {
+	for (node = 0; node < 4; node++) {
 		cpumask_t tmpmask, nodemask;
 		cpumask_t tmploader_mask, loadernode_mask;
 		unsigned int onlinemask;
@@ -349,9 +380,11 @@ void nlm_prepare_cpus(unsigned int max_cpus)
 
 static int nlm_cpu_disable(void)
 {
+	u8 ite, node;
 	int cpu = hard_smp_processor_id();
 	uint32_t bitmask = 0;
 	atomic_set(&cpu_hotplug_flag, 1);
+	node = cpu/32;
 
 	if (cpu == 0)
 		return -EBUSY;
@@ -360,10 +393,12 @@ static int nlm_cpu_disable(void)
 
         bitmask = cpumask_to_uint32(&cpu_online_map);
 	bitmask = bitmask & ~(1 << cpu);
-	set_ite(bitmask);
 
 	cpu_clear(cpu, cpu_online_map);
 	cpu_clear(cpu, cpu_callin_map);
+	for (ite = 0; ite < XLP_ITE_ENTRIES; ite++) {
+		xlp_ite_cpu_clear(node, cpu, ite);
+	}
 
 	local_irq_disable();
 	fixup_irqs(cpu, 0);
@@ -451,108 +486,3 @@ void prom_boot_cpus_secondary(void *args)
 	ptr_smp_boot(smp_boot.boot_info[cpu].fn, smp_boot.boot_info[cpu].sp,
 		     smp_boot.boot_info[cpu].gp);
 }
-
-extern void save_epc(unsigned long *epc);
-extern void smp_call_function_interrupt(void);
-static int nlm_common_ipi_stats[NR_CPUS];
-static unsigned long nlm_common_ipi_epc[NR_CPUS];
-
-#ifdef CONFIG_NLM_XLR
-#define SET_IPI_VECTOR(x, y) x |= y
-
-void core_send_ipi(int logical_cpu, unsigned int action)
-{
-	int cpu = cpu_logical_map(logical_cpu);
-	__u32 tid = cpu & 0x3;
-	__u32 pid = (cpu >> 2) & 0x07;
-	__u32 ipi = (tid << 16) | (pid << 20);
-
-	if (action & SMP_CALL_FUNCTION) {
-		SET_IPI_VECTOR(ipi, IRQ_IPI_SMP_FUNCTION);
-	} else if (action & SMP_RESCHEDULE_YOURSELF) {
-		SET_IPI_VECTOR(ipi, IRQ_IPI_SMP_RESCHEDULE);
-	} else if (action & SMP_CALL_KGDB_HOOK) {
-		SET_IPI_VECTOR(ipi, IRQ_IPI_SMP_KGDB);
-		/* set NMI also for KGDB */
-		SET_IPI_VECTOR(ipi, (1 << 8));
-	} else if (action & SMP_OPROFILE_IPI) {
-		SET_IPI_VECTOR(ipi, IRQ_IPI_OPROFILE);
-	}
-#ifdef CONFIG_NLMCOMMON_IP_FLOW_AFFINITY
-	else if (action & SMP_NETRX_IPI) {
-		SET_IPI_VECTOR(ipi, IRQ_IPI_NETRX);
-	}
-#endif				/* CONFIG_NLMCOMMON_IP_FLOW_AFFINITY */
-	else
-		return;
-
-	pic_send_ipi(ipi);
-}
-#endif
-
-extern __u64 nlm_common_irq_mask;
-
-#ifdef CONFIG_NLMCOMMON_IP_FLOW_AFFINITY
-extern void skb_transfer_finish(void);
-#endif				/* CONFIG_NLMCOMMON_IP_FLOW_AFFINITY */
-
-#ifdef CONFIG_SMP
-/* IRQ_IPI_SMP_FUNCTION Handler */
-void nlm_smp_function_ipi_handler(unsigned int irq, struct irq_desc *desc)
-{
-	nlm_common_ipi_stats[smp_processor_id()]++;
-	save_epc(&nlm_common_ipi_epc[smp_processor_id()]);
-	smp_call_function_interrupt();
-	nlm_common_ipi_stats[smp_processor_id()]--;
-}
-
-/* IRQ_IPI_SMP_RESCHEDULE  handler */
-void nlm_smp_resched_ipi_handler(unsigned int irq, struct irq_desc *desc)
-{
-	nlm_common_ipi_stats[smp_processor_id()]++;
-	save_epc(&nlm_common_ipi_epc[smp_processor_id()]);
-
-	/* Announce that we are for reschduling */
-	set_need_resched();
-	nlm_common_ipi_stats[smp_processor_id()]--;
-}
-
-#ifdef CONFIG_NLMCOMMON_IP_FLOW_AFFINITY
-/* nlm_ip_flow_ipi_handler HANDLER */
-void nlm_ip_flow_ipi_handler(unsigned int irq, struct irq_desc *desc)
-{
-	nlm_common_ipi_stats[smp_processor_id()]++;
-	save_epc(&nlm_common_ipi_epc[smp_processor_id()]);
-
-	/* do_IRQ called irq_enter() before calling this desc->handler */
-	skb_transfer_finish();
-	nlm_common_ipi_stats[smp_processor_id()]--;
-
-}
-#endif /* CONFIG_NLMCOMMON_IP_FLOW_AFFINITY */
-#endif
-
-void nlm_common_ipi_handler(int irq, struct pt_regs *regs)
-{
-	nlm_common_ipi_stats[smp_processor_id()]++;
-	save_epc(&nlm_common_ipi_epc[smp_processor_id()]);
-
-	if (irq == NLM_IRQ_IPI_SMP_FUNCTION) {
-		smp_call_function_interrupt();
-	}
-#ifdef CONFIG_NLMCOMMON_IP_FLOW_AFFINITY
-	else if (irq == IRQ_IPI_NETRX) {
-		irq_enter();
-
-		skb_transfer_finish();
-
-		/* run soft IRQ at the end */
-		irq_exit();
-	}
-#endif	/* CONFIG_NLMCOMMON_IP_FLOW_AFFINITY */
-	else {
-		/* Announce that we are for reschduling */
-		set_need_resched();
-	}
-	nlm_common_ipi_stats[smp_processor_id()]--;
-}
diff --git a/arch/mips/netlogic/xlp/time.c b/arch/mips/netlogic/xlp/time.c
index 4245430..a5b71fe 100644
--- a/arch/mips/netlogic/xlp/time.c
+++ b/arch/mips/netlogic/xlp/time.c
@@ -36,7 +36,7 @@ THE POSSIBILITY OF SUCH DAMAGE.
 #include <linux/oprofile.h>
 
 #include <linux/proc_fs.h>
-#include <asm/netlogic/xlp_hal_pic.h>
+#include <asm/netlogic/xlp_irq.h>
 
 extern spinlock_t xlp_pic_lock;
 
@@ -44,6 +44,15 @@ extern spinlock_t xlp_pic_lock;
 #error "Cannot enable both VPERF and OProfile at the same time"
 #endif
 
+void save_epc(unsigned long *epc)
+{
+	__asm__ __volatile__(".set push\n"
+			     ".set noreorder\n"
+			     "mfc0 %0, $14\n" ".set pop\n":"=r"(*epc));
+}
+
+
+#if ! defined CONFIG_XLP_REPLACE_R4K_TIMER
 #ifndef CONFIG_NLMCOMMON_MAC
 void nlm_common_user_mac_update_time(void)
 {
@@ -56,13 +65,6 @@ extern void nlm_common_user_mac_update_time(void);
 extern void nlm_common_user_mac_update_ktime(void);
 #endif
  
-void save_epc(unsigned long *epc)
-{
-	__asm__ __volatile__(".set push\n"
-			     ".set noreorder\n"
-			     "mfc0 %0, $14\n" ".set pop\n":"=r"(*epc));
-}
-
 #ifdef CONFIG_OPROFILE
 extern void nlm_common_oprofile_int_handler(int irq, void *dev_id,
 					 struct pt_regs *regs);
@@ -71,33 +73,17 @@ void nlm_common_timer_interrupt(struct pt_regs *regs, int irq)
 {
 	int cpu = hard_smp_processor_id();
 
-#ifdef CONFIG_NLM_WATCHDOG
-        pic_reg_t *mmio = nlm_hal_pic_offset();
-
-	/* ack the watchdog */
-	/* Need to choose (?) the right heartbeat reg (0/1) and right chunk */
-	nlm_hal_write_pic_reg(mmio, PIC_WD_HEARTBEAT_0(0), 1 << cpu_logical_map(cpu));
-#endif
-
-#ifdef CONFIG_NLM_WATCHDOG
-	/* ack the watchdog */
-	netlogic_write_reg(mmio, 0x0c, 1 << cpu_logical_map(cpu));
-#endif
-
-	if (irq != XLP_IRQ_TIMER) {
+	if (irq != XLP_IRQ_TIMER_RVEC) {
 		printk("[%s]:cpu_%d: bad timer irq = %x\n", __FUNCTION__, cpu, irq);
 		BUG();
 	}
 
-    {
         do_IRQ(irq);
 
         if (cpu == 0) {
             nlm_common_user_mac_update_time();
 	    nlm_common_user_mac_update_ktime();
         }
-    }
-
 }
 
 void nlm_oprofile_interrupt(struct pt_regs *regs, int irq)
@@ -125,8 +111,7 @@ void nlm_oprofile_interrupt(struct pt_regs *regs, int irq)
 cycle_t xlr_hpt_read(void)
 {
 	uint32_t counter;
-	pic_reg_t *mmio = nlm_hal_pic_offset();
-	counter = (uint32_t) nlm_hal_read_pic_reg(mmio, PIC_TIMER_6_COUNTER);
+	counter = (uint32_t) nlh_pic_r64r(0, XLP_PIC_SYSTIMER_COUNT(6));
 	return (cycle_t)(PIC_FREE_RUNNING_TIMER_MAX_VAL - counter);
 }
 
@@ -140,20 +125,18 @@ int read_current_timer(unsigned long *timer_val)
 
 void nlm_common_timer_setup(void)
 {
-        pic_reg_t *mmio = nlm_hal_pic_offset();
-        unsigned long flags = 0;
+        u64 flags = 0;
 
         spin_lock_irqsave(&xlp_pic_lock, flags);
-
         /* Use PIC Timer 6 as a free running counter */
-        nlm_hal_write_pic_reg(mmio, PIC_TIMER_6_MAXVAL, 0xffffffffffffffffULL);
-
+        nlh_pic_w64r(0, XLP_PIC_SYSTIMER_MAXVAL(6), 0xffffffffffffffffULL);
 	/* enable the timer */
         nlm_hal_pic_update_control(1 << (10 + 6));
 
         spin_unlock_irqrestore(&xlp_pic_lock, flags);
 
 }
+#endif
 
 static int nlm_timer_proc_read(char *page, char **start, off_t off, int count,
 			       int *eof, void *data)
diff --git a/arch/mips/netlogic/xlp/xlp_gpio.c b/arch/mips/netlogic/xlp/xlp_gpio.c
index e37b66b..814c022 100644
--- a/arch/mips/netlogic/xlp/xlp_gpio.c
+++ b/arch/mips/netlogic/xlp/xlp_gpio.c
@@ -31,7 +31,7 @@
 #include <linux/platform_device.h>
 #include <asm/netlogic/xlp.h>
 #include <asm/netlogic/hal/nlm_hal.h>
-#include <asm/netlogic/xlp_hal_pic.h>
+#include <asm/netlogic/xlp_irq.h>
 #include <asm/netlogic/gpio.h>
 
 #define XLP_GPIO_MAX		41
diff --git a/arch/mips/netlogic/xlp/xlp_hal_pic.c b/arch/mips/netlogic/xlp/xlp_hal_pic.c
index 5e03194..4434e13 100644
--- a/arch/mips/netlogic/xlp/xlp_hal_pic.c
+++ b/arch/mips/netlogic/xlp/xlp_hal_pic.c
@@ -1,7 +1,24 @@
-#include <asm/netlogic/xlp_hal_pic.h>
-#ifdef CONFIG_NUMA
-#include <asm/topology.h>
-#endif
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/cpumask.h>
+#include <linux/nodemask.h>
+#include <asm/netlogic/xlp_irq.h>
+#include <asm/netlogic/xlp.h>
+#include <asm/smp.h>
+
+static u64 xlp_pic_base[NLM_MAX_CPU_NODE] = {XLP_BDF_BASE(0,0,4),
+	XLP_BDF_BASE(0,8,4), XLP_BDF_BASE(0,16,4), XLP_BDF_BASE(0,24,4)};
+u64 __nlh_pic_r64o(u8 nid, u64 offset)
+{
+	return (nlh_read_cfg_reg64(xlp_pic_base[nid] + offset));
+}
+EXPORT_SYMBOL(__nlh_pic_r64o);
+
+void __nlh_pic_w64o(u8 nid, u64 offset, u64 val)
+{
+	nlh_write_cfg_reg64(xlp_pic_base[nid] + offset, val);
+}
+EXPORT_SYMBOL(__nlh_pic_w64o);
 
 /*
  * __nlm_hal_request_irq
@@ -15,28 +32,14 @@
  *
  * return : irt entry index
  */
-int __nlm_hal_request_irq(int irt, int rvec)
+int __nlm_hal_request_irq(u8 node, int irt, int rvec)
 {
-        uint64_t  val;
-
-#ifndef CONFIG_NUMA
-	val = nlm_hal_read_pic_reg(nlm_hal_pic_offset(), PIC_IRT(irt));
-	/* clear DB and DTE field */
-	val &= ~(0x3f << 20);
-	val |= ((rvec << 20) | (1 << 31));
-	nlm_hal_write_pic_reg(nlm_hal_pic_offset(), PIC_IRT(irt), val);
-#else
-	uint8_t	nid;
-	
-	for_each_online_node (nid) {
-		uint64_t xlp_pic_base = XLP_BDF_BASE(0, 0 + 8 * nid, 4);
+        u64 val;
 
-		val = nlm_hal_read_64bit_reg(xlp_pic_base, (0xB4 >> 1) + irt);
-		val &= ~(0x3f << 20);
-		val |= ((rvec << 20) | (1 << 31));
-		nlm_hal_write_64bit_reg(xlp_pic_base, (0xB4 >> 1) + irt, val);
-	}
-#endif
+	val = nlh_pic_r64r(node, XLP_PIC_IRT_ENTRY(irt));
+	val &= ~(0x3fULL << 20);
+	val |= ((((u64)rvec) << 20) | (1ULL << 31));
+	nlh_pic_w64r(node, XLP_PIC_IRT_ENTRY(irt), val);
         return 0;
 }
 
@@ -49,89 +52,52 @@ int __nlm_hal_request_irq(int irt, int rvec)
  *
  * return : irt entry index
  */
-void __nlm_hal_release_irq(int irt)
+void __nlm_hal_release_irq(u8 node, int irt)
 {
-        uint64_t  val;
-
-#ifndef CONFIG_NUMA
-	val = nlm_hal_read_pic_reg(nlm_hal_pic_offset(), PIC_IRT(irt));
-	val &= ~(1 << 31);
-	nlm_hal_write_pic_reg(nlm_hal_pic_offset(), PIC_IRT(irt), val);
-#else
-	uint8_t	nid;
-	
-	for_each_online_node (nid) {
-		uint64_t xlp_pic_base = XLP_BDF_BASE(0, 0 + 8 * nid, 4);
+        u64 val;
 
-		val = nlm_hal_read_64bit_reg(xlp_pic_base, (0xB4 >> 1) + irt);
-		val &= ~(1 << 31);
-		nlm_hal_write_64bit_reg(xlp_pic_base, (0xB4 >> 1) + irt, val);
-	}
-#endif
+	val = nlh_pic_r64r(node, XLP_PIC_IRT_ENTRY(irt));
+	val &= ~(1ULL << 31);
+	nlh_pic_w64r(node, XLP_PIC_IRT_ENTRY(irt), val);
+        return;
 }
 
-/*
- * __nlm_hal_set_irt_to_cpu
- *
- * Sets DT and DB in an IRT entry
- */
-void __nlm_hal_set_irt_to_cpu(int irt, int cpu)
+void xlp_ack_pic(u8 node, int irt)
 {
-        uint64_t val;
-	uint cpuid, threadid;
-	uint nodeid;
-#ifdef CONFIG_NUMA
-	uint64_t xlp_pic_base;
-#endif
-
-	/* DT is set 1 ==> Destination thread is specificed in DB and
-	 * DTE fields.
-	 * DB : (18-17 : Node id)
-	 * DB : (16) : 1 ==> DTE selects cpu 0-15
-	 * DB : (16) : 0 ===> DTE selects cpu 16-31
-	 *
-	 * cpuid and thread id are found out from cpu# param as follows
-	 * threadid = (cpu & 0xf)
-	 * cpuid = (cpu >> 4)
-	 */
-#ifndef CONFIG_NUMA
-	nodeid = 0;
-	cpuid = (cpu >> 4) & 1;	/* DB group of CPU */
-	threadid = cpu & 0xf;	/* range 0 - 15 for threadid */
-	val = nlm_hal_read_pic_reg(nlm_hal_pic_offset(), PIC_IRT(irt));
-	val &= ~(0xfffff);	/* Clear DT, DB and DTE */
-	val |= ((1 << 19) | (nodeid << 17) | (cpuid << 16) | ( 1 << threadid));
-	nlm_hal_write_pic_reg(nlm_hal_pic_offset(), PIC_IRT(irt), val);
-#else
-	nodeid = cpu_to_node(cpu);
-	cpuid = (cpu >> 4) & 1;
-	threadid = cpu & 0xf;
-	xlp_pic_base = XLP_BDF_BASE(0, 0 + 8 * nodeid, 4);
-	val = nlm_hal_read_64bit_reg(xlp_pic_base, (0xB4 >> 1) + irt);
-	val &= ~(0xfffff);
-	val |= ((1 << 19) | (nodeid << 17) | (cpuid << 16) | ( 1 << threadid));
-	nlm_hal_write_64bit_reg(xlp_pic_base, (0xB4 >> 1) + irt, val);
-#endif
-}
-
-#ifdef CONFIG_NUMA
-void xlp_numa_ack_pic(int irt)
-{
-	int 	 nid = cpu_to_node(smp_processor_id());
-	uint64_t xlp_pic_base = XLP_BDF_BASE(0, 0 + 8 * nid, 4);
-
-	nlm_hal_write_64bit_reg(xlp_pic_base, 0x50 >> 1, irt);
-
-	/* Ack the Status register for Watchdog & System timers */
-	if (irt < 12)
-		nlm_hal_write_64bit_reg(xlp_pic_base, 0x44 >> 1, 1 << irt);
+	/* Order of ack-ing pic_status and pic_int_ack is very important
+	 * Otherwise you might end up double the interrupt rate */
+	if (irt < 12) { /* Ack status register for WD and Sys.Timers */
+		nlh_pic_w64r(node, XLP_PIC_STATUS, 1 << irt);
+	}
+	/* Currently we have no way of figuring out the source PIC for an
+	 * interrupt. So, we restrict interrupt delivery to local node only
+	 * Please make sure this is the case while interrupt thread enable
+	 * registers (0x94 onwards) */
+	nlh_pic_w64r(node, XLP_PIC_INT_ACK, irt);
+	return;
 }
-#endif
-
 #ifdef NLM_HAL_LINUX_KERNEL
 #include <linux/kernel.h>
 #include <linux/module.h>
-EXPORT_SYMBOL(__nlm_hal_set_irt_to_cpu);
 EXPORT_SYMBOL(__nlm_hal_request_irq);
 EXPORT_SYMBOL(__nlm_hal_release_irq);
 #endif
+
+void nlm_hal_pic_send_ipi(int nmi, int vec, int node, int cpu)
+{
+	unsigned long long ipi = (nmi << 31) | (vec << 20) | (node << 17) | (1 << (cpu & 0xf));
+	if (cpu > 15) {
+	/* Setting bit 16 to select cpus 16-31*/
+		ipi |= 0x10000;
+	}
+	nlh_pic_w64r(0, XLP_PIC_IPI_CTL, ipi);
+}
+
+#if !defined CONFIG_XLP_REPLACE_R4K_TIMER
+void nlm_hal_pic_update_control(u64 control)
+{
+	u64 val = nlh_pic_r64r(0, XLP_PIC_CTRL);
+	val |= control;
+	nlh_pic_w64r(0, XLP_PIC_CTRL, val);
+}
+#endif
diff --git a/arch/mips/pci/pci-xlp.c b/arch/mips/pci/pci-xlp.c
index 21e5d26..4f79ee9 100644
--- a/arch/mips/pci/pci-xlp.c
+++ b/arch/mips/pci/pci-xlp.c
@@ -41,30 +41,61 @@ THE POSSIBILITY OF SUCH DAMAGE.
 
 #include <asm/io.h>
 
-#include <asm/netlogic/xlp_irq.h>
-#include <asm/netlogic/pci.h>
-#include <asm/netlogic/io.h>
-#include <asm/netlogic/iomap.h>
-#include <asm/netlogic/sim.h>
-#include <asm/netlogic/xlp_hal_pic.h>
-
 #if defined CONFIG_PCIEPORTBUS
 #define PORT_TYPE_MASK			0xf
 #define PCIE_CAPABILITIES_REG		0x2
 #define PCIE_RC_PORT			4
 #endif
+#define NETL_VENDOR_ID			0x184e
+#include <asm/netlogic/xlp_irq.h>
+#include <asm/netlogic/pci.h>
+#include <asm/netlogic/io.h>
+#include <asm/netlogic/iomap.h>
+#include <asm/netlogic/sim.h>
+#include <asm/netlogic/xlp_irq.h>
+#include <asm/netlogic/hal/nlm_hal.h>
+#include <asm/mach-netlogic/mmu.h>
 
 extern int pci_probe_only;
 static void *pci_config_base;
 static const volatile void *pci_io_base;
 
-int xlp_intx_enable(int);
-int xlp_intx_disable(int);
-int xlp_msi_enable(int, u32);
-int xlp_msix_enable(int);
-int xlp_msi_disable(int, u32);
-int xlp_msix_disable(int);
-u32 xlp_msi_set_mask(int, int, int);
+int xlp_intx_enable(u8, int);
+int xlp_intx_disable(u8, int);
+int xlp_msi_enable(u8, int, u32);
+int xlp_msix_enable(u8, int);
+int xlp_msi_disable(u8, int, u32);
+int xlp_msix_disable(u8, int);
+u32 xlp_msi_set_mask(u8, int, int, int);
+
+u64 xlp_syscfg_base[NLM_MAX_CPU_NODE] = { XLP_BDF_BASE(0,6,5),
+	XLP_BDF_BASE(0,14,5), XLP_BDF_BASE(0,22,5), XLP_BDF_BASE(0,30,5) };
+
+u64 xlp_pci_base[NLM_MAX_CPU_NODE][XLP_MAX_SLOTS] = {
+	/* This should be accessed as xlp_pci_base[node][fn] */
+
+	{XLP_BDF_BASE(0,1,0), XLP_BDF_BASE(0,1,1),
+	XLP_BDF_BASE(0,1,2), XLP_BDF_BASE(0,1,3)},
+
+	{XLP_BDF_BASE(0,9,0), XLP_BDF_BASE(0,9,1),
+	XLP_BDF_BASE(0,9,2), XLP_BDF_BASE(0,9,3)},
+
+	{XLP_BDF_BASE(0,17,0), XLP_BDF_BASE(0,17,1),
+	XLP_BDF_BASE(0,17,2), XLP_BDF_BASE(0,17,3)},
+
+	{XLP_BDF_BASE(0,25,0), XLP_BDF_BASE(0,25,1),
+	XLP_BDF_BASE(0,25,2), XLP_BDF_BASE(0,25,3)}
+};
+
+u64 __nlh_pci_r32o(u8 nid, u8 fn, u64 offset)
+{
+	return (nlh_read_cfg_reg32(xlp_pci_base[nid][fn] + offset));
+}
+
+void __nlh_pci_w32o(u8 nid, u8 fn, u64 offset, u32 val)
+{
+	nlh_write_cfg_reg32(xlp_pci_base[nid][fn] + offset, val);
+}
 
 /*
  * Possible values are no more hard coded.
@@ -82,6 +113,7 @@ u32 xlp_msi_set_mask(int, int, int);
  * 2. Cards can be placed on any available slot
  * 3. The card can have a switch built in, thus giving rise to multiple devices
  * on the slot.
+ * 4. All these problems can occur on four different nodes.
  *
  * So, it is important to figure out the lanes on which cards are placed.
  * First we read the lane config from POWER_ON_RESET_CFG
@@ -89,15 +121,16 @@ u32 xlp_msi_set_mask(int, int, int);
  * Based on that we have to assign interrupt values; while keeping the
  * possibility of same interrupt assigned to multiple devices open.
  *
- * So, we have a map: XLP irq map is as follows
+ * So, we have a map: XLP irq map is as follows (on node 0, but similar for any
+ * node
  *  \fn 0	1	2	3
  *plc\
  * 0	86	0	88	89
  * 1	86	87	88	0
  * 2	86	0	88	89
  * 3	86	87	88	89
- * This map changes from processor to processor. check PRM or RTL because
- * the values are a function of XLP_PCIE_LINK_IRT_OFFSET. To make them
+ * This map can differ among different versions of processors. Check PRM or RTL
+ * because the values are a function of XLP_PCIE_LINK_IRT_OFFSET. To make them
  * somewhat independent, I have defined macros and used them here.
  *
  * This map is dynamically populated based on card presence in the slot.
@@ -105,6 +138,19 @@ u32 xlp_msi_set_mask(int, int, int);
  * numbers would be different. Based on this fact, we can figure out from
  * pci_dev structure the slot where a card is placed at run time.
  */
+
+/* There are 4 PCI lane config modes for every node in XLP8xx
+ * 0:	2x8 lanes (ctrl 0, 2)
+ * 1:	2x4 lanes (ctrl 0, 2), 1x8 lanes (ctrl 1)
+ * 2:	1x8 lanes (ctrl 0), 2x4 lanes (ctrl 2, 3)
+ * 3:	4x4 lanes (all ctrls 4 lanes)
+ * PRM 31.11.7.2 (power on reset config register)
+ *
+ * But XLP3xx{L,H,Q} have fixed configuration (no need to read plc)
+ * given in Section 22 of PRM
+ */
+#define XLP_PCI_LANE_CONFIG	4
+
 struct xlp_link_struct {
 	int intno;
 	int sec;
@@ -113,32 +159,63 @@ struct xlp_link_struct {
 
 struct xlp_plc_fn_struct {
 	int plc;
-	struct xlp_link_struct farray[4];
+	struct xlp_link_struct farray[XLP_MAX_SLOTS];
 };
 
-static struct xlp_plc_fn_struct xlp_irqmap[4] = {
-	{0, {{XLP_PCIE_LINK_IRQ(0), 0, 0}, {0, 0, 0},
-		{XLP_PCIE_LINK_IRQ(2), 0, 0}, {0, 0, 0}}},
-	{1, {{XLP_PCIE_LINK_IRQ(0), 0, 0}, {XLP_PCIE_LINK_IRQ(1), 0, 0},
-		{XLP_PCIE_LINK_IRQ(2), 0, 0}, {0, 0, 0}}},
-	{2, {{XLP_PCIE_LINK_IRQ(0), 0, 0}, {0, 0, 0},
-		{XLP_PCIE_LINK_IRQ(2), 0, 0}, {XLP_PCIE_LINK_IRQ(3), 0, 0}}},
-	{3, {{XLP_PCIE_LINK_IRQ(0), 0, 0}, {XLP_PCIE_LINK_IRQ(1), 0, 0},
-		{XLP_PCIE_LINK_IRQ(2), 0, 0}, {XLP_PCIE_LINK_IRQ(3), 0, 0}}},
+static struct xlp_plc_fn_struct
+	node_irqmap[NLM_MAX_CPU_NODE][XLP_PCI_LANE_CONFIG] = {
+{
+	{0, {{XLP_PCIE_LINK_IRQ(0,0), 0, 0}, {0, 0, 0},
+		{XLP_PCIE_LINK_IRQ(0,2), 0, 0}, {0, 0, 0}}},
+	{1, {{XLP_PCIE_LINK_IRQ(0,0), 0, 0}, {XLP_PCIE_LINK_IRQ(0,1), 0, 0},
+		{XLP_PCIE_LINK_IRQ(0,2), 0, 0}, {0, 0, 0}}},
+	{2, {{XLP_PCIE_LINK_IRQ(0,0), 0, 0}, {0, 0, 0},
+		{XLP_PCIE_LINK_IRQ(0,2), 0, 0}, {XLP_PCIE_LINK_IRQ(0,3),0,0}}},
+	{3, {{XLP_PCIE_LINK_IRQ(0,0), 0, 0}, {XLP_PCIE_LINK_IRQ(0,1), 0, 0},
+		{XLP_PCIE_LINK_IRQ(0,2), 0, 0}, {XLP_PCIE_LINK_IRQ(0,3),0,0}}},
+}, {
+	{0, {{XLP_PCIE_LINK_IRQ(1,0), 0, 0}, {0, 0, 0},
+		{XLP_PCIE_LINK_IRQ(1,2), 0, 0}, {0, 0, 0}}},
+	{1, {{XLP_PCIE_LINK_IRQ(1,0), 0, 0}, {XLP_PCIE_LINK_IRQ(1,1), 0, 0},
+		{XLP_PCIE_LINK_IRQ(1,2), 0, 0}, {0, 0, 0}}},
+	{2, {{XLP_PCIE_LINK_IRQ(1,0), 0, 0}, {0, 0, 0},
+		{XLP_PCIE_LINK_IRQ(1,2), 0, 0}, {XLP_PCIE_LINK_IRQ(1,3),0,0}}},
+	{3, {{XLP_PCIE_LINK_IRQ(1,0), 0, 0}, {XLP_PCIE_LINK_IRQ(1,1), 0, 0},
+		{XLP_PCIE_LINK_IRQ(1,2), 0, 0}, {XLP_PCIE_LINK_IRQ(1,3),0,0}}},
+}, {
+	{0, {{XLP_PCIE_LINK_IRQ(2,0), 0, 0}, {0, 0, 0},
+		{XLP_PCIE_LINK_IRQ(2,2), 0, 0}, {0, 0, 0}}},
+	{1, {{XLP_PCIE_LINK_IRQ(2,0), 0, 0}, {XLP_PCIE_LINK_IRQ(2,1), 0, 0},
+		{XLP_PCIE_LINK_IRQ(2,2), 0, 0}, {0, 0, 0}}},
+	{2, {{XLP_PCIE_LINK_IRQ(2,0), 0, 0}, {0, 0, 0},
+		{XLP_PCIE_LINK_IRQ(2,2), 0, 0}, {XLP_PCIE_LINK_IRQ(2,3),0,0}}},
+	{3, {{XLP_PCIE_LINK_IRQ(2,0), 0, 0}, {XLP_PCIE_LINK_IRQ(2,1), 0, 0},
+		{XLP_PCIE_LINK_IRQ(2,2), 0, 0}, {XLP_PCIE_LINK_IRQ(2,3),0,0}}},
+}, {
+	{0, {{XLP_PCIE_LINK_IRQ(3,0), 0, 0}, {0, 0, 0},
+		{XLP_PCIE_LINK_IRQ(3,2), 0, 0}, {0, 0, 0}}},
+	{1, {{XLP_PCIE_LINK_IRQ(3,0), 0, 0}, {XLP_PCIE_LINK_IRQ(3,1), 0, 0},
+		{XLP_PCIE_LINK_IRQ(3,2), 0, 0}, {0, 0, 0}}},
+	{2, {{XLP_PCIE_LINK_IRQ(3,0), 0, 0}, {0, 0, 0},
+		{XLP_PCIE_LINK_IRQ(3,2), 0, 0}, {XLP_PCIE_LINK_IRQ(3,3),0,0}}},
+	{3, {{XLP_PCIE_LINK_IRQ(3,0), 0, 0}, {XLP_PCIE_LINK_IRQ(3,1), 0, 0},
+		{XLP_PCIE_LINK_IRQ(3,2), 0, 0}, {XLP_PCIE_LINK_IRQ(3,3),0,0}}},
+}
 };
 
-/*
-static int xlp_irq_map[4][4][3] = {
-	{{XLP_PCIE_LINK_IRQ(0), 0, 0}, {0, 0, 0},
-		{XLP_PCIE_LINK_IRQ(2), 0, 0}, {0, 0, 0}},
-	{{XLP_PCIE_LINK_IRQ(0), 0, 0}, {XLP_PCIE_LINK_IRQ(1), 0, 0},
-		{XLP_PCIE_LINK_IRQ(2), 0, 0}, {0, 0, 0}},
-	{{XLP_PCIE_LINK_IRQ(0), 0, 0}, {0, 0, 0},
-		{XLP_PCIE_LINK_IRQ(2), 0, 0}, {XLP_PCIE_LINK_IRQ(3), 0, 0}},
-	{{XLP_PCIE_LINK_IRQ(0), 0, 0}, {XLP_PCIE_LINK_IRQ(1), 0, 0},
-		{XLP_PCIE_LINK_IRQ(2), 0, 0}, {XLP_PCIE_LINK_IRQ(3), 0, 0}},
-};
-*/
+void dump_node_irqmap(u8 node, u32 plc)
+{
+	struct xlp_plc_fn_struct *p = &node_irqmap[node][plc];
+	u8 idx;
+
+	printk(KERN_WARNING "node_irqmap[%d][%d] PLC %d\n", node, plc, p->plc);
+	for (idx = 0; idx < XLP_MAX_SLOTS; idx++) {
+		printk("p->farray[%d] : int %d, sec %d, sub %d\n",idx,
+		p->farray[idx].intno, p->farray[idx].sec, p->farray[idx].sub);
+	}
+	return;
+}
+
 /* The following is the table describing current interrupt modes of
  * XLP controllers. When an external switch is present, different devices
  * can request different interrupt mode on the same controller which might lead
@@ -155,33 +232,33 @@ struct xlp_intmode_struct {
 	u32 mode;
 	int usage;
 };
-static struct xlp_intmode_struct intmode[4];
+static struct xlp_intmode_struct intmode[NLM_MAX_CPU_NODE][XLP_MAX_SLOTS];
 
-int xlp_ctrl_intmode_add(int fn, int mode, int i)
+int xlp_ctrl_intmode_add(u8 node, int fn, int mode, int i)
 {
-	if (intmode[fn].mode != mode) {
+	if (intmode[node][fn].mode != mode) {
 		return -EBUSY;
 	}
-	intmode[fn].usage += i;
-	if ((intmode[fn].usage < 0) || (intmode[fn].usage == 0)) {
-		intmode[fn].usage = 0;
+	intmode[node][fn].usage += i;
+	if ((intmode[node][fn].usage < 0) || (intmode[node][fn].usage == 0)) {
+		intmode[node][fn].usage = 0;
 	}
-	return intmode[fn].usage;
+	return intmode[node][fn].usage;
 }
 
 
-int xlp_get_ctrl_intmode(int fn)
+int xlp_get_ctrl_intmode(u8 node, int fn)
 {
-	return intmode[fn].mode;
+	return intmode[node][fn].mode;
 }
 
-int xlp_set_ctrl_intmode(int fn, int mode)
+int xlp_set_ctrl_intmode(u8 node, int fn, int mode)
 {
 	int ret = 0;
-	if (intmode[fn].mode == mode) {
+	if (intmode[node][fn].mode == mode) {
 		/* do nothing */
-	} else if (intmode[fn].usage == 0) {
-		intmode[fn].mode = mode;
+	} else if (intmode[node][fn].usage == 0) {
+		intmode[node][fn].mode = mode;
 	} else {
 		ret = -EBUSY;
 	}
@@ -193,21 +270,19 @@ int xlp_set_ctrl_intmode(int fn, int mode)
  * fills up the table with subordinate and secondary bus numbers. These
  * numbers would be different only if the PCIe device has a switch inside.
  */
-static int xlp_map_helper(int row, int fn)
+static int xlp_map_helper(u8 node, int row, int fn)
 {
-	u64 xlp_pci_base;
 	u32 reg6, ltssm;
 
-	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
-	ltssm = nlm_hal_read_32bit_reg(xlp_pci_base, 0x25E);
+	ltssm = nlh_pci_r32r(node, fn,  0x25E);
 	if (ltssm != 0x00446000) {
 		printk(KERN_WARNING "LTSSM state is %#x. Fn %x link not up\n",
 				ltssm, fn);
 		return -ENODEV;
 	}
-	reg6 = nlm_hal_read_32bit_reg(xlp_pci_base, 0x6);
-	xlp_irqmap[row].farray[fn].sec = (reg6 >> 8) & 0xff;
-	xlp_irqmap[row].farray[fn].sub = (reg6 >> 16) & 0xff;
+	reg6 = nlh_pci_r32r(node, fn,  0x6);
+	node_irqmap[node][row].farray[fn].sec = (reg6 >> 8) & 0xff;
+	node_irqmap[node][row].farray[fn].sub = (reg6 >> 16) & 0xff;
 	return 0;
 }
 
@@ -235,35 +310,41 @@ int xlp_is_dev_rc(struct pci_dev *dev)
 /*
  * Iterates over buses to find out the slot (thus pci controller fn)
  */
-#define DISABLE_MSIX 0x3
-int xlp_ctrl_fn_from_dev(struct pci_dev *dev)
+int xlp_ctrl_fn_from_dev(const struct pci_dev *dev, struct xlp_nodefn_struct *n)
 {
 	__label__ out;
-	int row = 0, fn = 0;
+	int row = 0, fn = 0, node;
 
 #if defined CONFIG_PCIEPORTBUS
 	if (xlp_is_dev_rc(dev) == 0) {
-		fn = dev->devfn & DISABLE_MSIX;
-		return fn;
+		int slot = (dev->devfn >> 3) & 0x1f;
+		fn = dev->devfn & 0x3;
+		node = slot / 8;
+		n->node = node; n->fn = fn;
+		return 0;
 	}
 #endif
-
-	while (row < 4) {
-		fn = 0;
-		while (fn < 4) {
-			if ((dev->bus->number >= xlp_irqmap[row].farray[fn].sec)
-			&&(dev->bus->number <= xlp_irqmap[row].farray[fn].sub)){
-				goto out; /* No `break', note two loops */;
+	for_each_online_node(node){
+		row = 0;
+		while (row < XLP_MAX_SLOTS) {
+			fn = 0;
+			while (fn < XLP_MAX_SLOTS) {
+				if ((dev->bus->number >= node_irqmap[node][row].farray[fn].sec)
+				&&(dev->bus->number <= node_irqmap[node][row].farray[fn].sub)){
+					goto out; /* No `break', note two loops */;
+				}
+				fn++;
 			}
-			fn++;
+			row++;
 		}
-		row++;
 	}
 out:
 	if (fn >= 4) {
 		return -ENODEV;
 	}
-	return fn;
+	n->node = node;
+	n->fn = fn;
+	return 0;
 }
 
 /*
@@ -274,53 +355,54 @@ out:
 static u64 XLP_MSI_ADDR = 0;
 #endif
 
-volatile const void *xlp_msix_addr_start(int fn)
+volatile const void *xlp_msix_addr_start(u8 node, int fn)
 {
 	if (XLP_MSI_ADDR == 0) {
 		return 0;
 	}
-	return (volatile const void *)(XLP_MSI_ADDR + (fn * XLP_MSIX_ADDR_SIZE));
+	return (volatile const void *)(XLP_MSI_ADDR +
+		(XLP_MSIX_ADDR_SIZE * XLP_MAX_SLOTS * node) +
+		(fn * XLP_MSIX_ADDR_SIZE));
 }
 
-volatile const void *xlp_msi_addr_start(int fn)
+volatile const void *xlp_msi_addr_start(u8 node, int fn)
 {
 	if (XLP_MSI_ADDR == 0) {
 		return 0;
 	}
-	return (volatile const void *)(XLP_MSI_ADDR + (fn * XLP_MSI_ADDR_SIZE));
+	return (volatile const void *)(XLP_MSI_ADDR +
+		(XLP_MSI_ADDR_SIZE * XLP_MAX_SLOTS * node) +
+		(fn * XLP_MSI_ADDR_SIZE));
 }
 
 /* Irrespective of any device requesting MSI/MSI-X, we keep the controller
  * ready by programming the corresponding registers. This action, per se,
  * does not start MSI/MSI-X for they have to be enabled explicitly.
  */
-static void xlp_msi_controller_init(int fn)
+static void xlp_msi_controller_init(u8 node, int fn)
 {
-	u64 xlp_pci_base;
-	u8 mmc;
-	u32 msi;
+	u32 mmc, msi;
 
-	xlp_pci_base = XLP_BDF_BASE(0, 1, fn);
 	if (XLP_MSI_ADDR == 0) {
 		printk(KERN_ERR "MSI/MSI-X CANNOT be programmed\n");
 		return;
 	}
-	msi = nlm_hal_read_32bit_reg(xlp_pci_base, 0x14);
+	msi = nlh_pci_r32r(node, fn,  0x14);
 	mmc = (msi >> 17) & 0x7;
 	/* Initialize MSI Base register */
-	nlm_hal_write_32bit_reg(xlp_pci_base, 0x15,
-		virt_to_phys(xlp_msi_addr_start(fn)) & 0xffffffff);
-	nlm_hal_write_32bit_reg(xlp_pci_base, 0x16,
-		(virt_to_phys(xlp_msi_addr_start(fn)) >> 32) & 0xffffffff);
-	nlm_hal_write_32bit_reg(xlp_pci_base, 0x17, 0x0);
+	nlh_pci_w32r(node, fn, 0x15,
+		virt_to_phys(xlp_msi_addr_start(node, fn)) & 0xffffffff);
+	nlh_pci_w32r(node, fn, 0x16,
+		(virt_to_phys(xlp_msi_addr_start(node, fn)) >> 32) & 0xffffffff);
+	nlh_pci_w32r(node, fn, 0x17, 0x0);
 	msi |= ((mmc << 20) | (1 << 16));
-	nlm_hal_write_32bit_reg(xlp_pci_base, 0x14, msi);
+	nlh_pci_w32r(node, fn, 0x14, msi);
 	/* Initialize MSI-X Base and Address reg. Note >> 8 in the address.
 	 * This is how 40bit address goes in 32bit registers.*/
-	nlm_hal_write_32bit_reg(xlp_pci_base, 0x24F,
-		(virt_to_phys(xlp_msix_addr_start(fn)) >> 8));
-	nlm_hal_write_32bit_reg(xlp_pci_base, 0x250,
-		(virt_to_phys(xlp_msix_addr_start(fn) + XLP_MSIX_ADDR_SIZE) >> 8));
+	nlh_pci_w32r(node, fn, 0x24F,
+		(virt_to_phys(xlp_msix_addr_start(node, fn)) >> 8));
+	nlh_pci_w32r(node, fn, 0x250,
+		(virt_to_phys(xlp_msix_addr_start(node, fn) + XLP_MSIX_ADDR_SIZE) >> 8));
 }
 
 /*
@@ -328,24 +410,21 @@ static void xlp_msi_controller_init(int fn)
  *
  * @fn : controller function no.
  */
-void xlp_pcie_controller_setup(int fn)
+void xlp_pcie_controller_setup(u8 node, int fn)
 {
 	u32 reg;
-	u64 xlp_pci_base;
 
-	xlp_msi_controller_init(fn);
+	xlp_msi_controller_init(node, fn);
 	//xlp_msix_disable(fn);
 	//xlp_msi_disable(fn, 0xf);
 	/* By default, leave INTX enabled */
-	xlp_intx_enable(fn);
-	/*enable timeout to avoid system to hang,
-	when there is no device on slot*/
-	xlp_pci_base = XLP_BDF_BASE(0, 1, fn);
-	reg = nlm_hal_read_32bit_reg(xlp_pci_base, 0x240);
-	nlm_hal_write_32bit_reg(xlp_pci_base, 0x240, reg | (3<<23));
-	/*0.1 second delay, 250MHz clock*/
-	nlm_hal_write_32bit_reg(xlp_pci_base, 0x244, 25*1000*1000);
-	nlm_hal_write_32bit_reg(xlp_pci_base, 0x245, 25*1000*1000);
+	xlp_intx_enable(node, fn);
+
+	//enable timeout to void system hang, when there is no device on slot
+	reg=nlh_pci_r32r(node, fn,  0x240);
+	nlh_pci_w32r(node, fn, 0x240, reg | (3<<23) );
+	nlh_pci_w32r(node, fn, 0x244, 25*1000*1000);	//0.1 second delay, 250MHz clock
+	nlh_pci_w32r(node, fn, 0x245, 25*1000*1000);
 }
 
 /*
@@ -355,34 +434,17 @@ void xlp_pcie_controller_setup(int fn)
  */
 u32 xlp_get_power_on_reset_cfg(int node)
 {
-	u64 xlp_syscfg_base = XLP_BDF_BASE(0,6,5);
-	return nlm_hal_read_32bit_reg(xlp_syscfg_base, 0x41);
+	return nlm_hal_read_32bit_reg(xlp_syscfg_base[node], 0x41);
 }
 EXPORT_SYMBOL(xlp_get_power_on_reset_cfg);
 
 
-/*
- * Called from system startup routine
- */
-static void pcie_controller_init_done(void)
+static void xlp_8xx_pcie_controller_init(void)
 {
-	u32 plc, syscfg, mode, count = 0;
+	u32 plc, syscfg, mode, count = 0, node = 0;
 
-#ifndef CONFIG_XLP_MSI_ADDRESSES
-#ifdef CONFIG_32BIT
-	XLP_MSI_ADDR = (u64)__get_free_pages(GFP_KERNEL, get_order(0x100000));
-#else
-	XLP_MSI_ADDR = (u64)__get_free_pages(GFP_KERNEL, get_order(0x1000000));
-#endif
-	if (XLP_MSI_ADDR == 0) {
-		printk(KERN_ERR "Failed to get memory for MSI/MSI-X tables\n");
-	}
-#endif
-	if (!pci_probe_only){
-		printk(KERN_WARNING "PCIe bus IRQs configured incorrectly\n");
-		return;
-	}
-	syscfg = xlp_get_power_on_reset_cfg(0);
+	for_each_online_node(node) {
+	syscfg = xlp_get_power_on_reset_cfg(node);
 	/* We don't manipulate pci_address space.
 	 * Get the link status from pcie lane config from 34.9.7.2 XLP PRM */
 	mode = (syscfg >> 19) & 0xf;
@@ -392,7 +454,8 @@ static void pcie_controller_init_done(void)
 		count++;
 	}
 	plc = (syscfg >> 23) & 0x3;
-	printk(KERN_DEBUG "PLC = %#x, mode = %#x\n", plc, mode);
+	printk(KERN_DEBUG "node %d, PLC = %#x, mode = %#x\n", node, plc, mode);
+
 	switch (plc) {
 	/* The correlation between plc and lane config is very specific to XLP
 	 * and not very clear in PRM
@@ -400,68 +463,167 @@ static void pcie_controller_init_done(void)
 	case 0:
 		/* controller 0 and 2 are active with 8lanes each */
 		if (mode & 0x1){
-			xlp_map_helper(plc, 0);
-			xlp_pcie_controller_setup(0);
+			xlp_map_helper(node, plc, 0);
+			xlp_pcie_controller_setup(node, 0);
 		}
 		if (mode & 0x4) {
-			xlp_map_helper(plc, 2);
-			xlp_pcie_controller_setup(2);
+			xlp_map_helper(node, plc, 2);
+			xlp_pcie_controller_setup(node, 2);
 		}
 		break;
 	case 1:
 		/* controllers 0,1 and 2 are active */
 		if (mode & 0x1){
-			xlp_map_helper(plc, 0);
-			xlp_pcie_controller_setup(0);
+			xlp_map_helper(node, plc, 0);
+			xlp_pcie_controller_setup(node, 0);
 		}
 		if (mode & 0x2){
-			xlp_map_helper(plc, 1);
-			xlp_pcie_controller_setup(1);
+			xlp_map_helper(node, plc, 1);
+			xlp_pcie_controller_setup(node, 1);
 		}
 		if (mode & 0x4){
-			xlp_map_helper(plc, 2);
-			xlp_pcie_controller_setup(2);
+			xlp_map_helper(node, plc, 2);
+			xlp_pcie_controller_setup(node, 2);
 		}
 		break;
 	case 2:
 		/* controllers 0,2 and 3 are active */
 		if (mode & 0x1){
-			xlp_map_helper(plc, 0);
-			xlp_pcie_controller_setup(0);
+			xlp_map_helper(node, plc, 0);
+			xlp_pcie_controller_setup(node, 0);
 		}
 		if (mode & 0x4){
-			xlp_map_helper(plc, 2);
-			xlp_pcie_controller_setup(2);
+			xlp_map_helper(node, plc, 2);
+			xlp_pcie_controller_setup(node, 2);
 		}
 		if (mode & 0x8){
-			xlp_map_helper(plc, 3);
-			xlp_pcie_controller_setup(3);
+			xlp_map_helper(node, plc, 3);
+			xlp_pcie_controller_setup(node, 3);
 		}
 		break;
 	case 3:
 		/* All four controllers are active with 4 lanes each */
 		if (mode & 0x1){
-			xlp_map_helper(plc, 0);
-			xlp_pcie_controller_setup(0);
+			xlp_map_helper(node, plc, 0);
+			xlp_pcie_controller_setup(node, 0);
 		}
 		if (mode & 0x2){
-			xlp_map_helper(plc, 1);
-			xlp_pcie_controller_setup(1);
+			xlp_map_helper(node, plc, 1);
+			xlp_pcie_controller_setup(node, 1);
 		}
 		if (mode & 0x4){
-			xlp_map_helper(plc, 2);
-			xlp_pcie_controller_setup(2);
+			xlp_map_helper(node, plc, 2);
+			xlp_pcie_controller_setup(node, 2);
 		}
 		if (mode & 0x8){
-			xlp_map_helper(plc, 3);
-			xlp_pcie_controller_setup(3);
+			xlp_map_helper(node, plc, 3);
+			xlp_pcie_controller_setup(node, 3);
 		}
 		break;
 	}
-	printk(KERN_DEBUG "[%s]: PCIE Controller initialization done\n", __FUNCTION__);
+	dump_node_irqmap(node, plc);
+	}	/* for_each_online_node */
 	return;
 }
 
+static void xlp_3xx_pcie_controller_init(u32 pid)
+{
+	struct xlp_plc_fn_struct *nirqmap = &node_irqmap[0][0];
+	/* zero out node_irqmap */
+	memset(node_irqmap, 0, sizeof(struct xlp_plc_fn_struct) * NLM_MAX_CPU_NODE * XLP_PCI_LANE_CONFIG);
+	switch(pid) {
+		case CPU_EXTPID_XLP_3XX_L:
+			/* Single node, port 0 only in x4 lane mode*/
+			nirqmap->plc = 0;
+			nirqmap->farray[0].intno = XLP_PCIE_LINK_IRQ(0,0);
+			xlp_map_helper(0, 0, 0);
+			xlp_pcie_controller_setup(0, 0);
+			break;
+		case CPU_EXTPID_XLP_3XX_LP:
+			/* Port 0 and port 1 are active in x2 lane mode*/
+			nirqmap->plc = 0;
+			nirqmap->farray[0].intno = XLP_PCIE_LINK_IRQ(0,0);
+			nirqmap->farray[1].intno = XLP_PCIE_LINK_IRQ(0,1);
+			xlp_map_helper(0, 0, 0);
+			xlp_pcie_controller_setup(0, 0);
+			xlp_map_helper(0, 0, 1);
+			xlp_pcie_controller_setup(0, 1);
+			break;
+		case CPU_EXTPID_XLP_3XX_LP2:
+			/* All ports (0, 1, 2, 3) are active in x1 lane mode */
+			nirqmap->plc = 0;
+			nirqmap->farray[0].intno = XLP_PCIE_LINK_IRQ(0,0);
+			nirqmap->farray[1].intno = XLP_PCIE_LINK_IRQ(0,1);
+			nirqmap->farray[2].intno = XLP_PCIE_LINK_IRQ(0,2);
+			nirqmap->farray[3].intno = XLP_PCIE_LINK_IRQ(0,3);
+			xlp_map_helper(0, 0, 0);
+			xlp_pcie_controller_setup(0, 0);
+			xlp_map_helper(0, 0, 1);
+			xlp_pcie_controller_setup(0, 1);
+			xlp_map_helper(0, 0, 2);
+			xlp_pcie_controller_setup(0, 2);
+			xlp_map_helper(0, 0, 3);
+			xlp_pcie_controller_setup(0, 3);
+			break;
+		default:
+			printk(KERN_WARNING "Could not find a PCI interrupt allocation scheme\n");
+			break;
+	}
+	dump_node_irqmap(0, 0);
+}
+
+extern int nlm_hal_get_cpuinfo(struct nlm_netl_proc_info *);
+
+/*
+ * Called from system startup routine
+ */
+static void pcie_controller_init_done(void)
+{
+#ifndef CONFIG_XLP_MSI_ADDRESSES
+	/* The controller will never read from /write to this area.
+	 * Strictly speaking this allocation is not necessary.
+	 * But if we don't allocate, we will have to keep in different
+	 * processors/boards a range of address which is not used anywhere else
+	 * like physical mem, pci mem ..etc. It is just easier to allocate
+	 * some memory and not use it for anything else.  */
+#ifdef CONFIG_32BIT
+	XLP_MSI_ADDR = (u64)__get_free_pages(GFP_KERNEL, get_order(0x100000));
+#else
+	/* We don't need 16M, all we need is 2 pages per controller
+	 * => 2 * 4ctr * 4 nodes pages */
+	XLP_MSI_ADDR = (u64)__get_free_pages(GFP_KERNEL,
+			get_order(0x1000000));
+#endif
+	if (XLP_MSI_ADDR == 0) {
+		printk(KERN_ERR "Failed to get memory for MSI/MSI-X tables\n");
+	}
+#endif
+	if (!pci_probe_only){
+		printk(KERN_WARNING "PCIe bus IRQs configured incorrectly\n");
+		return;
+	}
+	/* Configure controllers for running cpu type */
+	if (is_nlm_xlp8xx()) {
+		printk(KERN_DEBUG "Initializing PCIe for xlp8xx/4xx\n");
+		xlp_8xx_pcie_controller_init();
+	} else if (is_nlm_xlp(300, XLP_REVISION_ANY, CPU_EXTPID_XLP_3XX_L)) {
+		printk(KERN_DEBUG "Initializing PCIe for xlp3xx_L\n");
+		xlp_3xx_pcie_controller_init(CPU_EXTPID_XLP_3XX_L);
+	} else if (is_nlm_xlp(300, XLP_REVISION_ANY, CPU_EXTPID_XLP_3XX_LP)) {
+		printk(KERN_DEBUG "Initializing PCIe for xlp3xx_LP\n");
+		xlp_3xx_pcie_controller_init(CPU_EXTPID_XLP_3XX_LP);
+	} else if (is_nlm_xlp(300, XLP_REVISION_ANY, CPU_EXTPID_XLP_3XX_LP2)) {
+		printk(KERN_DEBUG "Initializing PCIe for xlp3xx_LP2\n");
+		xlp_3xx_pcie_controller_init(CPU_EXTPID_XLP_3XX_LP2);
+	} else if (is_nlm_xlp(300, XLP_REVISION_ANY, CPU_EXTPID_XLP_3XX_NONE)) {
+		printk(KERN_DEBUG "Initializing PCIe for xlp3xx\n");
+		/* ordinary 3xx and 8xx has same controller init seq. */
+		xlp_8xx_pcie_controller_init();
+	} else {
+		panic("Can't configure PCIe controller for unknown CPU type\n");
+	}
+}
+
 static inline __u32 pci_cfg_read_32bit(__u32 addr)
 {
 	__u32 temp = 0;
@@ -472,6 +634,16 @@ static inline __u32 pci_cfg_read_32bit(__u32 addr)
 	return temp;
 }
 
+static inline __u16 pci_cfg_read_16bit(__u32 addr)
+{
+    return *((__u16*)(pci_config_base + (addr & ~1)));
+}
+
+static inline __u8 pci_cfg_read_8bit(__u32 addr)
+{
+    return *((__u8 *)(pci_config_base + (addr & ~0)));
+}
+
 static inline void pci_cfg_write_32bit(__u32 addr, __u32 data)
 {
         unsigned int *p = (unsigned int *)(pci_config_base + (addr & ~3));
@@ -479,6 +651,16 @@ static inline void pci_cfg_write_32bit(__u32 addr, __u32 data)
 	*p = data;
 }
 
+static inline void pci_cfg_write_16bit(__u32 addr, __u16 data)
+{
+    *((__u16*)(pci_config_base + (addr & ~1))) = data;
+}
+
+static inline void pci_cfg_write_8bit(__u32 addr, __u8 data)
+{
+    *((__u8 *)(pci_config_base + (addr & ~0))) = data;
+}
+
 static int pci_bus_status = 0;
 
 #define pci_cfg_offset(bus, devfn, where) (((bus)<<20)+((devfn)<<12)+(where))	//for PCIE config space
@@ -487,6 +669,7 @@ static int pci_bus_status = 0;
 static int xlp_pcibios_read(struct pci_bus *bus, unsigned int devfn,
 				int where, int size, u32 * val)
 {
+    if( (!bus->self) || (bus->self->vendor == NETL_VENDOR_ID)) {
 	__u32 data = 0;
 
 	if ((size == 2) && (where & 1))
@@ -513,8 +696,15 @@ static int xlp_pcibios_read(struct pci_bus *bus, unsigned int devfn,
 		*val = (data >> ((where & 3) << 3)) & 0xffff;
 	else
 		*val = data;
-
-	return PCIBIOS_SUCCESSFUL;
+    } else { /* other vendors */
+        if      (size == 1)
+            *val = pci_cfg_read_8bit (pci_cfg_offset((bus->number), devfn, where));
+        else if (size == 2)
+            *val = pci_cfg_read_16bit(pci_cfg_offset((bus->number), devfn, where));
+        else  /*(size == 4) */
+            *val = pci_cfg_read_32bit(pci_cfg_offset((bus->number), devfn, where));
+    }
+    return PCIBIOS_SUCCESSFUL;
 }
 
 static int xlp_pcibios_write(struct pci_bus *bus, unsigned int devfn,
@@ -531,14 +721,8 @@ static int xlp_pcibios_write(struct pci_bus *bus, unsigned int devfn,
 	if (!pci_bus_status)
 		return PCIBIOS_BAD_REGISTER_NUMBER;
 
-	/*Workaround: We can't access the offset over 2K
-	* bytes in the configuration space of the Host bridge,
-	* else system will hang on XLP316.*/
-	if (is_nlm_xlp3xx() && bus->number == 0
-		&& devfn == 0 && where >= 2048)
-		return PCIBIOS_BAD_REGISTER_NUMBER;
-
-	data = pci_cfg_read_32bit(cfgaddr);
+	if( (!bus->self) || (bus->self->vendor == NETL_VENDOR_ID)) {
+		data = pci_cfg_read_32bit(cfgaddr);
 
 	if (size == 1)
 		data = (data & ~(0xff << ((where & 3) << 3))) |
@@ -549,7 +733,15 @@ static int xlp_pcibios_write(struct pci_bus *bus, unsigned int devfn,
 	else
 		data = val;
 
-	pci_cfg_write_32bit(cfgaddr, data);
+		pci_cfg_write_32bit(cfgaddr, data);
+	} else { /* every other vendor can have byte, word or double word access */
+		if      (size == 1)
+			pci_cfg_write_8bit ( cfgaddr, (__u8 )val);
+		else if (size == 2)
+			pci_cfg_write_16bit( cfgaddr, (__u16)val);
+		else /* (size == 4) */
+			pci_cfg_write_32bit( cfgaddr,        val);
+	}
 
 	return PCIBIOS_SUCCESSFUL;
 }
@@ -588,85 +780,106 @@ struct pci_controller xlp_controller = {
 };
 
 /*
- * Apparently this function is called for all pci controller functions
- * viz. 0:1.0, 0:1.1, 0:1.2 and 0:1.3
- * In fact, we need not assign them any interrupt.
- * But for any devices connected on them, consult the populated table
- * and return corresponding interrupt.
+ * This function is called for all pci controller functions
+ * viz. 0:1.0, 0:1.1, 0:1.2 and 0:1.3, 0:9.X, 0:17.X and 0:25.X
+ * In fact, we need not assign them any interrupt because they are not
+ * interrupt generating devices.
+ * But for any devices connected on these controllers, consult the populated
+ * table and return corresponding interrupt.
  */
-int __init pcibios_map_irq(const struct pci_dev *dev, u8 slot, u8 pin)
+int __init xlp_8xx_pcibios_map_irq(const struct pci_dev *dev, u8 slot, u8 pin)
 {
-	int row = 0, fn = 0;
+	int row = 0;
+	struct xlp_nodefn_struct nfn;
 
-	switch (dev->devfn) {
-	case XLP_PCIE_CTRL_DEVFN(0, 0) ... XLP_PCIE_CTRL_DEVFN(0, 3):
+	if (xlp_ctrl_fn_from_dev(dev, &nfn) != 0) {
+		dev_printk(KERN_ERR, &dev->dev, "Could not resolve device to a node/bus pair\n");
+		return 0;
+	}
+	row = (xlp_get_power_on_reset_cfg(nfn.node) >> 23) & 0x3;
+	dev_printk(KERN_DEBUG, &dev->dev, "Assigning interrupt %#x\n", node_irqmap[nfn.node][row].farray[nfn.fn].intno);
+	return node_irqmap[nfn.node][row].farray[nfn.fn].intno;
+}
+
+int __init xlp_3xx_pcibios_map_irq(const struct pci_dev *dev, u8 slot, u8 pin)
+{
+	int row = 0;
+	struct xlp_nodefn_struct nfn;
+
+	if (xlp_ctrl_fn_from_dev(dev, &nfn) != 0) {
+		dev_printk(KERN_ERR, &dev->dev, "Could not resolve device to a node/bus pair\n");
+		return 0;
+	}
+	dev_printk(KERN_DEBUG, &dev->dev, "Assigning interrupt %#x\n", node_irqmap[nfn.node][row].farray[nfn.fn].intno);
+	return node_irqmap[nfn.node][row].farray[nfn.fn].intno;
+
+}
+
+int __init pcibios_map_irq(const struct pci_dev *dev, u8 slot, u8 pin)
+{
+	if (dev->bus->number == 0) {
 		return 0;
-	default:
-		break;
 	}
-	row = (xlp_get_power_on_reset_cfg(0) >> 23) & 0x3;
-	fn = xlp_ctrl_fn_from_dev((struct pci_dev *)dev);
-	dev_printk(KERN_DEBUG, &dev->dev, "Assigning interrupt %#x\n", xlp_irqmap[row].farray[fn].intno);
-	return xlp_irqmap[row].farray[fn].intno;
+	if (is_nlm_xlp8xx()) {
+		return xlp_8xx_pcibios_map_irq(dev, slot, pin);
+	} else if (is_nlm_xlp3xx()) {
+		return xlp_3xx_pcibios_map_irq(dev, slot, pin);
+	}
 }
 
 /*
  * Enables INTx on a controller
  */
-static int __xlp_intx_enable(int fn)
+static int __xlp_intx_enable(u8 node, int fn)
 {
-	u64 xlp_pci_base;
 	u32 pci;
 
-	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
-	pci = nlm_hal_read_32bit_reg(xlp_pci_base, 0x1);
+	pci = nlh_pci_r32r(node, fn,  0x1);
 	pci &= ~(1 << 10);	/* Enable IntX assertion */
-	nlm_hal_write_32bit_reg(xlp_pci_base, 0x1, pci);
-	pci = nlm_hal_read_32bit_reg(xlp_pci_base, 0x261);
+	nlh_pci_w32r(node, fn, 0x1, pci);
+	pci = nlh_pci_r32r(node, fn,  0x261);
 	pci |= 0xf;	/* Enable INT A,B,C,D */
-	nlm_hal_write_32bit_reg(xlp_pci_base, 0x261, pci);
+	nlh_pci_w32r(node, fn, 0x261, pci);
 	return 0;
 }
 
-int xlp_intx_enable(int fn)
+int xlp_intx_enable(u8 node, int fn)
 {
-	int mode = xlp_get_ctrl_intmode(fn);
+	int mode = xlp_get_ctrl_intmode(node, fn);
 
 	if ((mode & XLP_INTMODE_MSI) || (mode & XLP_INTMODE_MSIX)) {
 		return -EBUSY;
 	}
-	__xlp_intx_enable(fn);
-	xlp_incr_ctrl_intmode(fn, XLP_INTMODE_INTX);
+	__xlp_intx_enable(node, fn);
+	xlp_incr_ctrl_intmode(node, fn, XLP_INTMODE_INTX);
 	return 0;
 }
 
 /*
  * Disables INTx on a controller
  */
-static int __xlp_intx_disable(int fn)
+static int __xlp_intx_disable(u8 node, int fn)
 {
-	u64 xlp_pci_base;
 	u32 pci;
 
-	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
-	pci = nlm_hal_read_32bit_reg(xlp_pci_base, 0x1);
+	pci = nlh_pci_r32r(node, fn,  0x1);
 	pci |= (1 << 10);
-	nlm_hal_write_32bit_reg(xlp_pci_base, 0x1, pci);
-	pci = nlm_hal_read_32bit_reg(xlp_pci_base, 0x261);
+	nlh_pci_w32r(node, fn, 0x1, pci);
+	pci = nlh_pci_r32r(node, fn,  0x261);
 	pci &= ~(0xf);
-	nlm_hal_write_32bit_reg(xlp_pci_base, 0x261, pci);
+	nlh_pci_w32r(node, fn, 0x261, pci);
 	return 0;
 }
 
-int xlp_intx_disable(int fn)
+int xlp_intx_disable(u8 node, int fn)
 {
-	int mode = xlp_get_ctrl_intmode(fn);
+	int mode = xlp_get_ctrl_intmode(node, fn);
 
 	if (!(mode & XLP_INTMODE_INTX)) {
 		return -EBUSY;
 	}
-	__xlp_intx_disable(fn);
-	xlp_decr_ctrl_intmode(fn, XLP_INTMODE_INTX);
+	__xlp_intx_disable(node, fn);
+	xlp_decr_ctrl_intmode(node, fn, XLP_INTMODE_INTX);
 	return 0;
 }
 
@@ -675,28 +888,25 @@ int xlp_intx_disable(int fn)
  * MSI enable register on the _controller_ if not already enabled
  * @dev : pci device corresponding to this device
  */
-static int __xlp_msi_enable(int fn, u32 bit)
+static int __xlp_msi_enable(u8 node, int fn, u32 bit)
 {
-	u64 xlp_pci_base;
 	u32 msi_en;
 
-	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
-
 	/* First, set PCIe MSI Enable register. __KEEP_THIS_ORDER__ */
-	msi_en = nlm_hal_read_32bit_reg(xlp_pci_base, 0x261);
+	msi_en = nlh_pci_r32r(node, fn,  0x261);
 	msi_en &= ~(0xf);
 	if ((msi_en & (1 << 9)) == 0) {
 		msi_en |= (1 << 9);	/* controls ONLY MSI, Not MSI-X */
-		nlm_hal_write_32bit_reg(xlp_pci_base, 0x261, msi_en);
+		nlh_pci_w32r(node, fn, 0x261, msi_en);
 	}
 	/* Now, set the individual bit */
-	xlp_msi_set_mask(fn, bit, 1);
+	xlp_msi_set_mask(node, fn, bit, 1);
 	return 0;
 }
 
-int xlp_msi_enable(int fn, u32 bit)
+int xlp_msi_enable(u8 node, int fn, u32 bit)
 {
-	int tmp = xlp_get_ctrl_intmode(fn);
+	int tmp = xlp_get_ctrl_intmode(node, fn);
 
 	if ((tmp & XLP_INTMODE_INTX) || (tmp & XLP_INTMODE_MSIX)) {
 		return -EBUSY;
@@ -707,8 +917,8 @@ int xlp_msi_enable(int fn, u32 bit)
 	 * with a switch present. So, setting the bitmap should not depend on
 	 * present value of reg 0x25b or 0x261
 	 */
-	__xlp_msi_enable(fn, bit);
-	xlp_incr_ctrl_intmode(fn, XLP_INTMODE_MSI);
+	__xlp_msi_enable(node, fn, bit);
+	xlp_incr_ctrl_intmode(node, fn, XLP_INTMODE_MSI);
 	return 0;
 }
 
@@ -716,95 +926,88 @@ int xlp_msi_enable(int fn, u32 bit)
  * Finds the slot on which this device is placed and enables corresponding
  * MSI-X enable register on the controller
  */
-static int __xlp_msix_enable(int fn)
+static int __xlp_msix_enable(u8 node, int fn)
 {
-	u64 xlp_pci_base;
 	u32 msix_ctrl;
 
-	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
-	msix_ctrl = nlm_hal_read_32bit_reg(xlp_pci_base, 0x2C);
+	msix_ctrl = nlh_pci_r32r(node, fn,  0x2C);
 	if (!(msix_ctrl & 0x80000000)) {
 		msix_ctrl |= 0x80000000;	/* MSI-X enable */
-		nlm_hal_write_32bit_reg(xlp_pci_base, 0x2C, msix_ctrl);
+		nlh_pci_w32r(node, fn, 0x2C, msix_ctrl);
 	}
-	//nlm_hal_write_32bit_reg(xlp_pci_base, 0xf, 0xFF);
 	return 0;
 }
 
-int xlp_msix_enable(int fn)
+int xlp_msix_enable(u8 node, int fn)
 {
-	int mode = xlp_get_ctrl_intmode(fn);
+	int mode = xlp_get_ctrl_intmode(node, fn);
 
 	if ((mode & XLP_INTMODE_MSI) || (mode & XLP_INTMODE_INTX)) {
 		return -EBUSY;
 	}
-	__xlp_msix_enable(fn);
-	xlp_incr_ctrl_intmode(fn, XLP_INTMODE_MSIX);
+	__xlp_msix_enable(node, fn);
+	xlp_incr_ctrl_intmode(node, fn, XLP_INTMODE_MSIX);
 	return 0;
 }
 
 /*
  * Disables MSI on controller function
  */
-static int __xlp_msi_disable(int fn)
+static int __xlp_msi_disable(u8 node, int fn)
 {
-	u64 xlp_pci_base;
 	u32 msi_en;
 
 	/* We dont call xlp_decr_ctrl.... here because it has already been 
 	 * called before xlp_msi_disable is called */
-	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
+
 	/*set PCIe Int Enable register */
-	msi_en = nlm_hal_read_32bit_reg(xlp_pci_base, 0x261);
+	msi_en = nlh_pci_r32r(node, fn,  0x261);
 	if ((msi_en & (1 << 9)) != 0) {
 		msi_en &= ~(1 << 9);
 		msi_en |= 0xf;
-		nlm_hal_write_32bit_reg(xlp_pci_base, 0x261, msi_en);
+		nlh_pci_w32r(node, fn, 0x261, msi_en);
 	}
 	return 0;
 }
 
-int xlp_msi_disable(int fn, u32 bit)
+int xlp_msi_disable(u8 node, int fn, u32 bit)
 {
-	int tmp = xlp_get_ctrl_intmode(fn);
+	int tmp = xlp_get_ctrl_intmode(node, fn);
 	u32 r25b;
 
 	if (!(tmp & XLP_INTMODE_MSI)) {
 		return -EBUSY;
 	}
-	r25b = xlp_msi_set_mask(fn, bit, 0);
+	r25b = xlp_msi_set_mask(node, fn, bit, 0);
 	if (r25b == 0) {
-		__xlp_msi_disable(fn);
+		__xlp_msi_disable(node, fn);
 	}
-	xlp_decr_ctrl_intmode(fn, XLP_INTMODE_MSI);
+	xlp_decr_ctrl_intmode(node, fn, XLP_INTMODE_MSI);
 	return 0;
 }
 
 /*
  * Disables MSI-X on a controller function
  */
-static int __xlp_msix_disable(int fn)
+static int __xlp_msix_disable(u8 node, int fn)
 {
-	u64 xlp_pci_base;
 	u32 msix_ctrl;
 
-	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
-	msix_ctrl = nlm_hal_read_32bit_reg(xlp_pci_base, 0x2C);
+	msix_ctrl = nlh_pci_r32r(node, fn,  0x2C);
 	msix_ctrl &= ~(0x80000000);	/* MSI-X disable */
-	nlm_hal_write_32bit_reg(xlp_pci_base, 0x2C, msix_ctrl);
-	//nlm_hal_write_32bit_reg(xlp_pci_base, 0xf, 0xFF);	/* TODO Get from dev */
+	nlh_pci_w32r(node, fn, 0x2C, msix_ctrl);
 	return 0;
 }
 
-int xlp_msix_disable(int fn)
+int xlp_msix_disable(u8 node, int fn)
 {
-	int mode = xlp_get_ctrl_intmode(fn);
+	int mode = xlp_get_ctrl_intmode(node, fn);
 
 	if (!(mode & XLP_INTMODE_MSIX)) {
 		return -EBUSY;
 	}
-	if (xlp_decr_ctrl_intmode(fn, XLP_INTMODE_MSIX) == 0) {
-		__xlp_msix_disable(fn);
+	if (xlp_decr_ctrl_intmode(node, fn, XLP_INTMODE_MSIX) == 0) {
+		__xlp_msix_disable(node, fn);
 	}
 	return 0;
 }
@@ -813,27 +1016,23 @@ int xlp_msix_disable(int fn)
  * checks if msi is enabled for this controller
  * @fn	: controller function number
  */
-int is_msi_set(int fn)
+int is_msi_set(u8 node, int fn)
 {
-	u64 xlp_pci_base;
 	u32 msi_en, status;
 
-	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
-	msi_en = nlm_hal_read_32bit_reg(xlp_pci_base, 0x261);
+	msi_en = nlh_pci_r32r(node, fn,  0x261);
 	status = (msi_en >> 9) & 1 ;
 	return status;
 }
 
 
-u32 calc_msi_vector_offset(int fn)
+u32 calc_msi_vector_offset(u8 node, int fn)
 {
-	u64 xlp_pci_base;
 	u32 msi_en, msi_stat;
 
-	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
-	msi_en = nlm_hal_read_32bit_reg(xlp_pci_base, 0x25B);
-	msi_stat = nlm_hal_read_32bit_reg(xlp_pci_base, 0x25A);
-	nlm_hal_write_32bit_reg(xlp_pci_base, 0x25A, msi_stat);
+	msi_en = nlh_pci_r32r(node, fn,  0x25B);
+	msi_stat = nlh_pci_r32r(node, fn,  0x25A);
+	nlh_pci_w32r(node, fn, 0x25A, msi_stat);
 	msi_stat &= msi_en;
 	return msi_stat;
 }
@@ -846,80 +1045,34 @@ u32 calc_msi_vector_offset(int fn)
  * Figure out the mask (the bits corresponding to fn), read register, clear
  * them and return the bits corresponding to fn
  */
-#ifndef XLP_MSIX_PER_SLOT
-#define XLP_MSIX_PER_SLOT	8
-#endif
-u32 xlp_msix_status_clear(int fn)
+u32 xlp_msix_status_clear(u8 node, int fn)
 {
-	u64 xlp_pci_base;
 	u32 msix_stat;
 	u32 mask = ((XLP_MSIX_PER_SLOT - 1) << (fn * XLP_MSIX_PER_SLOT));
 
-	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
-	msix_stat = nlm_hal_read_32bit_reg(xlp_pci_base, 0x25D);
-	//fdebug("mask = %#x, fn = %d, MSIX status = %#x\n", mask, fn, msix_stat);
+	msix_stat = nlh_pci_r32r(node, fn,  0x25D);
 	msix_stat &= mask;
-	//fdebug("Masked MSIX status = %#x\n", msix_stat);
-	nlm_hal_write_32bit_reg(xlp_pci_base, 0x25D, msix_stat);
-	//fdebug("Stat cleared %#x\n", nlm_hal_read_32bit_reg(xlp_pci_base, 0x25D));
+	nlh_pci_w32r(node, fn, 0x25D, msix_stat);
 	return (msix_stat >> (fn * XLP_MSIX_PER_SLOT));
 }
 
-#if 0
-/* required only if xlp_ctrl_fn_from_dev() is static */
-int xlp_msi_base_vector(struct pci_dev *dev)
-{
-	return(XLP_MSI_IRQ_START(xlp_ctrl_fn_from_dev(dev)));
-}
-
-
-int xlp_msix_base_vector(struct pci_dev *dev)
-{
-	return(XLP_MSIX_IRQ_START(xlp_ctrl_fn_from_dev(dev)));
-}
-
-#endif
-
 /*
  * Masks the bit corresponding to an MSI and return the resulting bitmask
  */
-u32 xlp_msi_set_mask(int fn, int bit, int val)
+u32 xlp_msi_set_mask(u8 node, int fn, int bit, int val)
 {
-	u64 xlp_pci_base;
 	u32 bits;
 
-	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
-	bits = nlm_hal_read_32bit_reg(xlp_pci_base, 0x25B);
+	bits = nlh_pci_r32r(node, fn,  0x25B);
 	if (val == 0) {	/* Clear bit `bit` */
 		bits &= ~( 1 << bit);
 	} else {	/* Set bit `bit` */
 		bits |= ( 1 << bit);
 	}
-	nlm_hal_write_32bit_reg(xlp_pci_base, 0x25B, bits);
+	nlh_pci_w32r(node, fn, 0x25B, bits);
 	return bits;
 }
 
-/*
- * Finds the slot on which this device is placed and clears the MSI status
- * register on the controller
- * @dev : pci device corresponding to this device
- */
-int xlp_msi_status_clear(struct pci_dev *dev, int bit)
-{
-	int fn = 0;
-	u64 xlp_pci_base;
-	u32 msi_en;
-
-	fn = xlp_ctrl_fn_from_dev(dev);
-	if ((fn >= 4) || (fn < 0)) {
-		return -ENODEV;
-	}
-	xlp_pci_base = XLP_BDF_BASE(0,1,fn);
-	msi_en = 1 << bit;
-	nlm_hal_write_32bit_reg(xlp_pci_base, 0x25B, msi_en);
-	return 0;
-}
-
 /* Do platform specific device initialization at pci_enable_device() time */
 int pcibios_plat_dev_init(struct pci_dev *dev)
 {
diff --git a/drivers/mmc/host/xlpmmc.c b/drivers/mmc/host/xlpmmc.c
index cf978f9..b884ec1 100644
--- a/drivers/mmc/host/xlpmmc.c
+++ b/drivers/mmc/host/xlpmmc.c
@@ -46,7 +46,9 @@
 
 #include "xlpmmc.h"
 
-#undef XLP_MMC_DEBUG
+#undef XLP_MMC_DEBUG 
+#undef DEBUG 
+
 struct xlpmmc_host {
 	struct mmc_host *mmc;
 	struct mmc_request *mrq;
@@ -60,6 +62,7 @@ struct xlpmmc_host {
 	spinlock_t		irq_lock; /* Prevent races with irq handler */
 	int status;
         int present;
+        int slot;
 
 	struct {
 		int len;
@@ -72,12 +75,14 @@ struct xlpmmc_host {
 		int len;
 	} pio;
 
+        unsigned long workaround;
 	int irq;
         struct timer_list       timer;
         struct tasklet_struct   card_tasklet;   /* Tasklet structures */
 
 	struct xlpmmc_platform_data *platdata;
 	struct platform_device *pdev;
+        struct xlpmmc_host **host_array;
 };
 
 #define XLPMMC_DESCRIPTOR_SIZE (64<<10)
@@ -97,6 +102,10 @@ struct xlpmmc_host {
 #define PCIE_HDR_OFFSET 0x100
 #define GPIO_MMC_DETECT 29
 
+#define XLP_SLOT_SIZE   0x100
+#define XLP_3XX_NUM_SDMMC_SLOT 2
+#define XLP_8XX_NUM_SDMMC_SLOT 2
+
 __inline__ int32_t gpio_regread(int node, int regidx) 
 {
         volatile uint64_t mmio;
@@ -114,7 +123,7 @@ static __inline__ void gpio_regwrite(int node, int regidx, int32_t val)
 /* Low-level write-routines*/
 static inline void hc_wr32(void *iobase, int offset, u32 data, int slot) {
 
-        int reg_offset = PCIE_HDR_OFFSET + (slot*0x100);
+        int reg_offset = PCIE_HDR_OFFSET + (slot*XLP_SLOT_SIZE);
         volatile u32 *mmc_base = (u32 *)( (u8*)iobase + reg_offset );
         mmc_base[offset>>2] = data;
         return;
@@ -122,7 +131,7 @@ static inline void hc_wr32(void *iobase, int offset, u32 data, int slot) {
 
 static inline void hc_wr16(void *iobase, int offset, u16 data, int slot) {
 
-        int reg_offset = PCIE_HDR_OFFSET + (slot*0x100);
+        int reg_offset = PCIE_HDR_OFFSET + (slot*XLP_SLOT_SIZE);
         volatile u16 *mmc_base = (u16 *)( (u8*)iobase + reg_offset );
 
         mmc_base[offset>>1] = data;
@@ -133,7 +142,7 @@ static inline void hc_wr16(void *iobase, int offset, u16 data, int slot) {
  */
 static inline u32 hc_rd32(void *iobase,int offset, int slot) {
 
-        int reg_offset = PCIE_HDR_OFFSET + (slot*0x100);
+        int reg_offset = PCIE_HDR_OFFSET + (slot*XLP_SLOT_SIZE);
         volatile u32 *mmc_base = (u32 *)( (u8*)iobase + reg_offset );
 
         u32 data = mmc_base[offset>>2];
@@ -142,35 +151,49 @@ static inline u32 hc_rd32(void *iobase,int offset, int slot) {
 
 static inline u32 hc_rd16(void *iobase, int offset, int slot) {
 
-        int reg_offset = PCIE_HDR_OFFSET + (slot*0x100);
+        int reg_offset = PCIE_HDR_OFFSET + (slot*XLP_SLOT_SIZE);
         volatile u16 *mmc_base = (u16 *)( (u8*)iobase + reg_offset );
 
         u32 data = mmc_base[offset>>1];
         return data;
 }
+static int findmaxslots() {
+        if(is_nlm_xlp8xx())
+                return XLP_8XX_NUM_SDMMC_SLOT;
+        else if (is_nlm_xlp3xx())
+                return XLP_3XX_NUM_SDMMC_SLOT;
+        else
+                return 1;
+}
 
-static void dump_hc_regs(struct xlpmmc_host *host)
+static void dump_hc_regs(struct xlpmmc_host *host, int slot)
 {
 #ifdef XLP_MMC_DEBUG	
-	printk ("MMC_PRESENT STATE = 0x%x\n", hc_rd32((host->base), HC_PRESENT_STATE_LO, 0));
-	printk ("MMC_POWER_CTL = 0x%x\n", hc_rd16((host->base), HC_PC_HC, 0));
-	printk ("MMC_CLOK_CTL = 0x%x\n", hc_rd16((host->base), HC_CLOCK_CTRL, 0));
-	printk ("MMC_CAP0 = 0x%x\n", hc_rd32((host->base), 0x40, 0));
+	printk ("SLOT:%d MMC_PRESENT STATE = 0x%x\n", slot, hc_rd32((host->base), HC_PRESENT_STATE_LO, slot));
+	printk ("SLOT:%d MMC_POWER_CTL = 0x%x\n", slot, hc_rd16((host->base), HC_PC_HC, slot));
+	printk ("SLOT:%d MMC_CLOK_CTL = 0x%x\n", slot, hc_rd16((host->base), HC_CLOCK_CTRL, slot));
+	printk ("SLOT:%d MMC_CAP0 = 0x%x\n", slot, hc_rd32((host->base), 0x40, slot));
 #endif
 }
 
-static void xlpmmc_set_power(struct xlpmmc_host *host, int state)
+static void xlpmmc_set_power(struct xlpmmc_host *host, int state, int slot)
 {	
 	if(state)
-		hc_wr16(host->base, HC_PC_HC, 0x1f00, 0);
+		hc_wr16(host->base, HC_PC_HC, 0x1f00, slot);
 	else
-		hc_wr16(host->base, HC_PC_HC, 0x1e00, 0);
+		hc_wr16(host->base, HC_PC_HC, 0x1e00, slot);
 }		
 
+static int xlpmmc_find_slot(struct xlpmmc_host *host)
+{
+        return host->slot;
+}
+
 static int xlpmmc_card_inserted(struct mmc_host *mmc)
 {
         struct xlpmmc_host *host = mmc_priv(mmc);
-        host->present = !gpio_get_value(GPIO_MMC_DETECT);
+	int slot = xlpmmc_find_slot(host);
+        host->present = !gpio_get_value(GPIO_MMC_DETECT+slot);
         return host->present;
 }
 
@@ -182,8 +205,9 @@ static int xlpmmc_card_readonly(struct mmc_host *mmc)
 
 static irqreturn_t xlpmmc_det_irq(int irq, void *dev_id)
 {
-        struct xlpmmc_host *host = (struct xlpmmc_host *) dev_id;
-        int present = !gpio_get_value(GPIO_MMC_DETECT); 
+        struct xlpmmc_host **xlpmmc_host_data = dev_id;
+        int present, count; 
+        int maxslots = findmaxslots();
 #ifdef XLP_MMC_DEBUG
         printk("Entered xlpmmc_det_irq\n");
 #endif
@@ -191,18 +215,25 @@ static irqreturn_t xlpmmc_det_irq(int irq, void *dev_id)
         * we expect this irq on both insert and remove,
         * and use a short delay to debounce.
         */
-        gpio_regwrite(0, XLP_GPIO_INT_STAT0, gpio_regread(0, XLP_GPIO_INT_STAT0) & 0x20000000);
-        if(present)
-                gpio_regwrite(0, XLP_GPIO_INT_POLAR0, gpio_regread(0, XLP_GPIO_INT_POLAR0) & 0xDFFFFFFF);
-        else
-                gpio_regwrite(0, XLP_GPIO_INT_POLAR0, gpio_regread(0, XLP_GPIO_INT_POLAR0) | 0x20000000);
-
-        if (present != host->present) {
-                host->present = present;
-                pr_debug("%s: card %s\n", mmc_hostname(host->mmc),
-                        present ? "insert" : "remove");
-                //tasklet_schedule(&host->card_tasklet);
-                mmc_detect_change(host->mmc, msecs_to_jiffies(100));
+        for (count=0; count<maxslots; count ++) {
+	        struct xlpmmc_host *host = xlpmmc_host_data[count];
+                present = !gpio_get_value(GPIO_MMC_DETECT+count); 
+                gpio_regwrite(0, XLP_8XX_GPIO_INT_STAT0, gpio_regread(0, XLP_8XX_GPIO_INT_STAT0) 
+                        & 0x1<<(GPIO_MMC_DETECT+count));
+                        
+                if(present)
+                        gpio_regwrite(0, XLP_8XX_GPIO_INT_POLAR0, gpio_regread(0, XLP_8XX_GPIO_INT_POLAR0) 
+                                & ~(0x1<<(GPIO_MMC_DETECT+count)));
+                else
+                        gpio_regwrite(0, XLP_8XX_GPIO_INT_POLAR0, gpio_regread(0, XLP_8XX_GPIO_INT_POLAR0) 
+                                | 0x1<<(GPIO_MMC_DETECT+count));
+
+                if (present != host->present) {
+                        host->present = present;
+                        pr_debug("%s: card %s\n", mmc_hostname(host->mmc),
+                                present ? "insert" : "remove");
+                        mmc_detect_change(host->mmc, msecs_to_jiffies(100));
+                }
         }
         return IRQ_HANDLED;
 }
@@ -211,7 +242,7 @@ static void xlpmmc_finish_request(struct xlpmmc_host *host)
 {
 	struct mmc_request *mrq = host->mrq;
 #ifdef XLP_MMC_DEBUG	
-	printk("xlpmmc_finish_request mrq= %p\n", mrq);
+	printk("xlpmmc_finish_request mrq= %p host=%p slot %d\n", mrq, host, host->slot);
 #endif
 	host->mrq = NULL;
 	host->flags &= HOST_F_ACTIVE | HOST_F_DMA;
@@ -229,7 +260,7 @@ static void xlpmmc_finish_request(struct xlpmmc_host *host)
 }
 
 static int xlpmmc_send_command(struct xlpmmc_host *host, int wait,
-				struct mmc_command *cmd, struct mmc_data *data)
+				struct mmc_command *cmd, struct mmc_data *data, int slot)
 {
 	int rsp_type;
 	u32 mmccmd = 0;
@@ -263,7 +294,7 @@ static int xlpmmc_send_command(struct xlpmmc_host *host, int wait,
 	}
 
 #ifdef XLP_MMC_DEBUG
-	printk("xlpmmc_send_command : cmd-opcode=%d mmc_data= %p rep= 0x%x cm-arg=0x%x\n", cmd->opcode, data, rsp_type,cmd->arg);
+	printk("xlpmmc_send_command : cmd-opcode=%d mmc_data= %p rep= 0x%x cm-arg=0x%x slot%d \n", cmd->opcode, data, rsp_type,cmd->arg, slot);
 #endif
 	
 	/*Data direction and count*/
@@ -298,15 +329,15 @@ static int xlpmmc_send_command(struct xlpmmc_host *host, int wait,
 
 	/*Wait for CMD line to free. Safe to poll 
 	as HOST CMD line could have been resetted.*/
-	hcstate = hc_rd32(host->base, 0x24, 0);
+	hcstate = hc_rd32(host->base, 0x24, slot);
 	do{
 		/*Should never stuck here*/
-		hcstate = hc_rd32(host->base, 0x24, 0);
+		hcstate = hc_rd32(host->base, 0x24, slot);
 	}while(hcstate & 0x1);
         mod_timer(&host->timer, jiffies + 1 * HZ); 
 
-	hc_wr32(host->base, HC_ARG1_LO, cmd->arg, 0);
-	hc_wr32(host->base, HC_COMMAND, mmccmd, 0);	
+	hc_wr32(host->base, HC_ARG1_LO, cmd->arg, slot);
+	hc_wr32(host->base, HC_COMMAND, mmccmd, slot);	
 	return 0;
 }
 
@@ -331,15 +362,15 @@ static void xlpmmc_data_complete(struct xlpmmc_host *host)
 			data->bytes_xfered = (data->blocks * data->blksz); 
 		}else{
 			data->bytes_xfered = (data->blocks * data->blksz) - host->pio.len;
-			
+
 		}
 	}
-		
+
 	xlpmmc_finish_request(host);
 }
 
 
-static void xlpmmc_send_pio(struct xlpmmc_host *host)
+static void xlpmmc_send_pio(struct xlpmmc_host *host, int slot)
 {
 	struct mmc_data *data;
 	int sg_len, max, count;
@@ -370,7 +401,7 @@ static void xlpmmc_send_pio(struct xlpmmc_host *host)
 		write_data[2] = *sg_ptr++ & 0xff; 
 		write_data[1] = *sg_ptr++ & 0xff; 
 		write_data[0] = *sg_ptr++ & 0xff;
-		hc_wr32(host->base, HC_BUFF_DATA_PORT0, *(u32*)write_data, 0);
+		hc_wr32(host->base, HC_BUFF_DATA_PORT0, *(u32*)write_data, slot);
 	}
 	count=count<<2;
 
@@ -383,7 +414,7 @@ static void xlpmmc_send_pio(struct xlpmmc_host *host)
 	}
 }
 
-static void xlpmmc_receive_pio(struct xlpmmc_host *host)
+static void xlpmmc_receive_pio(struct xlpmmc_host *host, int slot)
 {
 	struct mmc_data *data;
 	int max, count, sg_len = 0;
@@ -414,7 +445,7 @@ static void xlpmmc_receive_pio(struct xlpmmc_host *host)
 
 	for (count = 0; count < max/4; count++) {
 		volatile uint32_t read_data;
-		read_data = hc_rd32(host->base, HC_BUFF_DATA_PORT0, 0);
+		read_data = hc_rd32(host->base, HC_BUFF_DATA_PORT0, slot);
 			*sg_ptr++ = (unsigned char)((read_data >>  0) & 0xFF);
 			*sg_ptr++ = (unsigned char)((read_data >>  8) & 0xFF);
 			*sg_ptr++ = (unsigned char)((read_data >>  16) & 0xFF);
@@ -437,7 +468,7 @@ static void xlpmmc_receive_pio(struct xlpmmc_host *host)
      and check for errors. Then start the data transfer if it is indicated.
   2) Notify the core about command completion 
 */
-static void xlpmmc_cmd_complete(struct xlpmmc_host *host, u32 status)
+static void xlpmmc_cmd_complete(struct xlpmmc_host *host, u32 status, int slot)
 {
 	struct mmc_request *mrq = host->mrq;
 	struct mmc_command *cmd;
@@ -451,10 +482,10 @@ static void xlpmmc_cmd_complete(struct xlpmmc_host *host, u32 status)
 
 	if (cmd->flags & MMC_RSP_PRESENT) {
 		if (cmd->flags & MMC_RSP_136) {
-			r[0] = hc_rd32(host->base, HC_RESPONSE6, 0);
-			r[1] = hc_rd32(host->base, HC_RESPONSE4, 0);
-			r[2] = hc_rd32(host->base, HC_RESPONSE2, 0);
-			r[3] = hc_rd32(host->base, HC_RESPONSE0, 0);
+			r[0] = hc_rd32(host->base, HC_RESPONSE6, slot);
+			r[1] = hc_rd32(host->base, HC_RESPONSE4, slot);
+			r[2] = hc_rd32(host->base, HC_RESPONSE2, slot);
+			r[3] = hc_rd32(host->base, HC_RESPONSE0, slot);
 			for (i = 0; i < 4; i++) {
 				cmd->resp[i] = (r[i] & 0x00FFFFFF) << 8;
 				if (i != 3)
@@ -464,7 +495,7 @@ static void xlpmmc_cmd_complete(struct xlpmmc_host *host, u32 status)
 			printk("RESP0 =0x%x RESP1 =0x%x RESP2 = 0x%x RESP3=0x%x\n ",  cmd->resp[0], cmd->resp[1], cmd->resp[2], cmd->resp[3]);
 #endif
 		}else{ 
-			cmd->resp[0] = hc_rd32(host->base, HC_RESPONSE0,0);
+			cmd->resp[0] = hc_rd32(host->base, HC_RESPONSE0,slot);
 		}
 	}
 
@@ -499,7 +530,7 @@ static int cal_range(int base_clk, int rate)
 	return 0;
 }
 
-static void xlpmmc_set_clock(struct xlpmmc_host *host, int rate)
+static void xlpmmc_set_clock(struct xlpmmc_host *host, int rate , int slot)
 {
 	u32 sd_bclk_fq;
 	u16 sd_ck_fs = 0;
@@ -513,16 +544,15 @@ static void xlpmmc_set_clock(struct xlpmmc_host *host, int rate)
 	sd_ck_fs = cal_range(sd_bclk_fq, rate);
 	
 	hc_clk_ctl  = (sd_ck_fs<<SD_CLK_FRQ_SHT) |  HCC_INT_CLK_EN | HCC_SD_CLK_EN;
-	hc_wr16(host->base, HC_CLOCK_CTRL, hc_clk_ctl, 0);
+	hc_wr16(host->base, HC_CLOCK_CTRL, hc_clk_ctl, slot);
 
 	/* Wait for stable clock */
-	while ((hc_rd16(host->base, HC_CLOCK_CTRL,0) & HCC_INT_CLK_STABLE) == 0) {};
-
+	while ((hc_rd16(host->base, HC_CLOCK_CTRL, slot) & HCC_INT_CLK_STABLE) == 0) {};
 	return;	
 }
 
 static int xlpmmc_prepare_data(struct xlpmmc_host *host,
-				struct mmc_data *data)
+				struct mmc_data *data, int slot)
 {
 	int datalen = data->blocks * data->blksz;
 	u32 mmc_blk_ctl = 0;
@@ -551,7 +581,8 @@ static int xlpmmc_prepare_data(struct xlpmmc_host *host,
 			dump_stack();
 		}
 #endif
-		hc_wr32(host->base, HC_SDMA_SA_OR_ARG2_LO, (unsigned long)sg_phys(sg), 0);
+		hc_wr32(host->base, HC_SDMA_SA_OR_ARG2_LO, (unsigned long)sg_phys(sg), slot);
+                host->workaround = (unsigned long)sg_phys(sg);
 	}else{	
 		host->pio.index = 0;
 		host->pio.offset = 0;
@@ -575,8 +606,10 @@ static int xlpmmc_prepare_data(struct xlpmmc_host *host,
 		mmc_blk_ctl |= BLK_SZ_HGH_BIT;
 	else
 		mmc_blk_ctl &= ~BLK_SZ_HGH_BIT;
-	hc_wr32(host->base, HC_BLOCK_SIZE, mmc_blk_ctl, 0);
-
+	hc_wr32(host->base, HC_BLOCK_SIZE, mmc_blk_ctl, slot);
+#ifdef XLP_MMC_DEBUG
+	printk("xlpmmc_prepare_data datablks=%d block sz= 0x%x\n", data->blocks,  data->blksz);
+#endif
 	
 	return 0;
 }
@@ -586,9 +619,15 @@ static void xlpmmc_request(struct mmc_host* mmc, struct mmc_request* mrq)
 {
 	struct xlpmmc_host *host = mmc_priv(mmc);
 	int ret = 0;
+	int slot = xlpmmc_find_slot(host);
 
 	WARN_ON(irqs_disabled());
 	WARN_ON(host->status != HOST_S_IDLE);
+        
+        while(host->host_array[0]->status != HOST_S_IDLE || host->host_array[1]->status != HOST_S_IDLE)
+        {
+                printk("Serializing commands to mutliple MMC Slots\n");
+        }
 
 	host->mrq = mrq;
 	host->status = HOST_S_CMD;
@@ -603,11 +642,14 @@ static void xlpmmc_request(struct mmc_host* mmc, struct mmc_request* mrq)
 	/* No platform support to know card detection */
 #endif
 	if (mrq->data) {
-		ret = xlpmmc_prepare_data(host, mrq->data);
+		ret = xlpmmc_prepare_data(host, mrq->data, slot);
 	}
 
 	if (!ret)
-		ret = xlpmmc_send_command(host, 0, mrq->cmd, mrq->data);
+		ret = xlpmmc_send_command(host, 0, mrq->cmd, mrq->data, slot);
+
+        /* Work around for slot0, when using two SD/MMCslots */
+        hc_wr32(host->base, HC_SDMA_SA_OR_ARG2_LO, host->workaround, slot);
 
 	if (ret) {
 		mrq->cmd->error = ret;
@@ -615,27 +657,25 @@ static void xlpmmc_request(struct mmc_host* mmc, struct mmc_request* mrq)
 	}
 }
 
-static void xlpmmc_reset_controller(struct xlpmmc_host *host)
+static void xlpmmc_reset_controller(struct xlpmmc_host *host, int slot)
 {
-	hc_wr32(host->base, HC_SYSCTRL,0x14, 0);
-
         /* S1: Clear any set INT Bits */
-        hc_wr32(host->base, HC_NORMAL_INT_STS, hc_rd32(host->base, HC_NORMAL_INT_STS,0), 0);
+        hc_wr32(host->base, HC_NORMAL_INT_STS, hc_rd32(host->base, HC_NORMAL_INT_STS, slot), slot);
 
         /* S2: Enable Interrupts */
-        hc_wr32(host->base, HC_NORMAL_INT_STS_EN, 0x37ff7fff, 0);
+        hc_wr32(host->base, HC_NORMAL_INT_STS_EN, 0x37ff7fff, slot);
 
         /* S3: Enable Interrupt Signals */
-        hc_wr32(host->base, HC_NORMAL_INT_SIGNAL_EN, 0x37ff7fff, 0);
+        hc_wr32(host->base, HC_NORMAL_INT_SIGNAL_EN, 0x37ff7fff, slot);
 
         /* S4: Send HW Reset to eMMC-4.4 Card */
-        hc_wr16(host->base, HC_PC_HC, 0x1e00, 0);
-        hc_wr16(host->base, HC_PC_HC, 0x1f00, 0);
+        hc_wr16(host->base, HC_PC_HC, 0x1e00, slot);
+        hc_wr16(host->base, HC_PC_HC, 0x1f00, slot);
 
         /* S4: Remove HW Reset to eMMC-4.4 Card */
-        hc_wr16(host->base, HC_PC_HC, 0x0f00, 0);
+        hc_wr16(host->base, HC_PC_HC, 0x0f00, slot);
 
-        hc_wr16(host->base, HC_SWRST_TIMEOUT_CTRL,0xe,0) ;
+        hc_wr16(host->base, HC_SWRST_TIMEOUT_CTRL,0xe, slot) ;
         printk("Initialized eMMC Host Controller\n");
 
         return;
@@ -645,25 +685,26 @@ static void xlpmmc_set_ios(struct mmc_host *mmc, struct mmc_ios *ios)
 {
 	struct xlpmmc_host *host = mmc_priv(mmc);
 	u16 hc_pc_hc;
+	int slot = xlpmmc_find_slot(host);
 #ifdef XLP_MMC_DEBUG
-	printk("xlpmmc_set_ios  ios = %p power = 0x%x clock = 0x%x bus-width=0x%x\n", ios, ios->power_mode, ios->clock, ios->bus_width);	
+	printk("xlpmmc_set_ios  host %lx, ios = %p power = 0x%x clock = 0x%x bus-width=0x%x, slot %d\n", host, ios, ios->power_mode, ios->clock, ios->bus_width, slot);	
 #endif
 
 	/*Power */
 	if (ios->power_mode == MMC_POWER_OFF)
-		xlpmmc_set_power(host, 0);
+		xlpmmc_set_power(host, 0, slot);
 	else if (ios->power_mode == MMC_POWER_ON) {
-		xlpmmc_set_power(host, 1);
+		xlpmmc_set_power(host, 1, slot);
 	}
 	msleep(5);
 	/* Clock */
 	if (ios->clock && ios->clock != host->clock) {
-		xlpmmc_set_clock(host, ios->clock);
+		xlpmmc_set_clock(host, ios->clock, slot);
 		host->clock = ios->clock;
 	}
 	
 	/*BUS width*/
-	hc_pc_hc = hc_rd16(host->base, HC_PC_HC, 0);
+	hc_pc_hc = hc_rd16(host->base, HC_PC_HC, slot);
 	
 	switch (ios->bus_width) {
 	case MMC_BUS_WIDTH_4:
@@ -673,16 +714,16 @@ static void xlpmmc_set_ios(struct mmc_host *mmc, struct mmc_ios *ios)
 		hc_pc_hc &= ~HC_HCR_4BIT_MODE;
 		break;
 	}
-	hc_wr16 (host->base, HC_PC_HC, hc_pc_hc, 0);
+	hc_wr16 (host->base, HC_PC_HC, hc_pc_hc, slot);
 	msleep(5);
-	dump_hc_regs(host);
+	dump_hc_regs(host, slot);
 }
 
-static void xlpmmc_timeout_timer(unsigned long data)
+static void xlpmmc_timeout_timer(long unsigned int data)
 {  
         struct xlpmmc_host *host = (struct xlpmmc_host *) data;
-        unsigned long flags;
         volatile short interrsts;
+	int slot = xlpmmc_find_slot(host);
 
 	spin_lock(&host->irq_lock);
         if (host->mrq) {
@@ -692,86 +733,91 @@ static void xlpmmc_timeout_timer(unsigned long data)
                                 xlpmmc_data_complete(host);
                         } else {                              
 			        /*Reset the CMD line*/
-			        interrsts =  hc_rd16(host->base, HC_SWRST_TIMEOUT_CTRL, 0);
+			        interrsts =  hc_rd16(host->base, HC_SWRST_TIMEOUT_CTRL, slot);
 			        interrsts |= SW_RST_CMD; 
-			        hc_wr16(host->base, HC_SWRST_TIMEOUT_CTRL, interrsts, 0);	
+			        hc_wr16(host->base, HC_SWRST_TIMEOUT_CTRL, interrsts, slot);	
                                 host->mrq->cmd->error = -ETIMEDOUT; 
-                                xlpmmc_cmd_complete(host, 0);
+                                xlpmmc_cmd_complete(host, 0, slot);
                         }                       
                     }
-        }                                                       
+        }
 	spin_unlock(&host->irq_lock);
 }
 
 static irqreturn_t xlpmmc_irq(int irq, void *dev_id)
 {
-	struct xlpmmc_host *host = dev_id;
-	volatile short intstatus;
-	spin_lock(&host->irq_lock);
+        struct xlpmmc_host **xlpmmc_host_data = dev_id;
+        volatile short intstatus;
+        int count;
+        int maxslots = findmaxslots();
+
+        for (count=0; count<maxslots; count ++) { 
+	        struct xlpmmc_host *host = xlpmmc_host_data[count];
 
-	intstatus = hc_rd16(host->base, HC_NORMAL_INT_STS, 0);
+	        intstatus = hc_rd16(host->base, HC_NORMAL_INT_STS, count);
 #ifdef XLP_MMC_DEBUG
 	printk("got xlpmmc_irq status = 0x%x\n", intstatus);	
 #endif
-	hc_wr16(host->base, HC_NORMAL_INT_STS, intstatus, 0);	
-
-/*	if (status & HNIS_CARD_INT )	
-		mmc_signal_sdio_irq(host->mmc); */
-	/*Error Interrupt*/
-	if(intstatus & HNIS_ERR){
-		volatile short interrsts;
-		u16 handle_err=0;
-		handle_err = 0x1; /*TODO : Add error codes here*/
-		interrsts = hc_rd16(host->base, HC_ERROR_INT_STS, 0);	  
+	        hc_wr16(host->base, HC_NORMAL_INT_STS, intstatus, count);	
+	        spin_lock(&host->irq_lock);
+
+	        /*Error Interrupt*/
+	        if(intstatus & HNIS_ERR){
+		        volatile short interrsts;
+		        u16 handle_err=0;
+		        handle_err = 0x1; /*TODO : Add error codes here*/
+		        interrsts = hc_rd16(host->base, HC_ERROR_INT_STS, count);	  
 #ifdef XLP_MMC_DEBUG
-		printk("INT ERR: error status  = 0x%x\n", interrsts);
+        printk("INT ERR: error status  = 0x%x\n", interrsts);
 #endif
-		hc_wr16(host->base, HC_ERROR_INT_STS, interrsts, 0);
-		if(interrsts & handle_err){	
-			/*Reset the CMD line*/
-			interrsts =  hc_rd16(host->base, HC_SWRST_TIMEOUT_CTRL, 0);
-			interrsts |= SW_RST_CMD; 
-			hc_wr16(host->base, HC_SWRST_TIMEOUT_CTRL, interrsts, 0);	
-			host->mrq->cmd->error = -ETIMEDOUT;
-			
-		} 
-		xlpmmc_cmd_complete(host, intstatus);
-		spin_unlock(&host->irq_lock);
-		return IRQ_HANDLED;
-	}
-
-	if(intstatus & HNIS_CMD_CMPL){	
-		if (host->status == HOST_S_CMD)
-			xlpmmc_cmd_complete(host, intstatus);
-
-	} 
-
-        if(intstatus & (HNIS_CINS | HNIS_CREM)){
-                tasklet_schedule(&host->card_tasklet);
-        }
-	if (!(host->flags & HOST_F_DMA)) {
-		if ((host->flags & HOST_F_XMIT) && (intstatus & HNIS_BUFF_WR_RDY))
-			xlpmmc_send_pio(host);
-		else if ((host->flags & HOST_F_RECV) && (intstatus & HNIS_BUFF_RD_RDY))
-			xlpmmc_receive_pio(host);
-		else if( ((host->flags & HOST_F_RECV) || (host->flags & HOST_F_XMIT))&&(intstatus & HNIS_TC_CMPL))
-			xlpmmc_data_complete(host);
-	}
+		        hc_wr16(host->base, HC_ERROR_INT_STS, interrsts, count);
+		        if(interrsts & handle_err){	
+			        /*Reset the CMD line*/
+			        interrsts =  hc_rd16(host->base, HC_SWRST_TIMEOUT_CTRL, count);
+			        interrsts |= SW_RST_CMD; 
+			        hc_wr16(host->base, HC_SWRST_TIMEOUT_CTRL, interrsts, count);	
+			        host->mrq->cmd->error = -ETIMEDOUT;
+		        } 
+		        xlpmmc_cmd_complete(host, intstatus, count);
+		        spin_unlock(&host->irq_lock);
+		        return IRQ_HANDLED;
+	        }
+
+	        if(intstatus & HNIS_CMD_CMPL){	
+		        if (host->status == HOST_S_CMD)
+			        xlpmmc_cmd_complete(host, intstatus, count);
+
+	        } 
+
+                if(intstatus & (HNIS_CINS | HNIS_CREM)){
+                        tasklet_schedule(&host->card_tasklet);
+                }
+	        if (!(host->flags & HOST_F_DMA)) {
+		        if ((host->flags & HOST_F_XMIT) && (intstatus & HNIS_BUFF_WR_RDY))
+			        xlpmmc_send_pio(host, count);
+		        else if ((host->flags & HOST_F_RECV) && (intstatus & HNIS_BUFF_RD_RDY))
+			        xlpmmc_receive_pio(host, count);
+		        else if( ((host->flags & HOST_F_RECV) || (host->flags & HOST_F_XMIT))
+                                                                &&(intstatus & HNIS_TC_CMPL))
+			        xlpmmc_data_complete(host);
+	        }
         
-	if((host->flags & HOST_F_DMA) && (intstatus & HNIS_TC_CMPL)){
-		xlpmmc_data_complete(host);
-	}
-	if(!(intstatus &= (HNIS_ERR 
-			| HNIS_CMD_CMPL
-			| HNIS_BUFF_WR_RDY | HNIS_BUFF_RD_RDY
-			| HNIS_TC_CMPL | HNIS_DMA ))){
-			//printk("Unhandled status 0x%x\n", intstatus);
-	}
+	        if((host->flags & HOST_F_DMA) && (intstatus & HNIS_TC_CMPL)){
+		        xlpmmc_data_complete(host);
+	        }
+
+	        if(!(intstatus &= (HNIS_ERR 
+		        	| HNIS_CMD_CMPL
+			        | HNIS_BUFF_WR_RDY | HNIS_BUFF_RD_RDY
+			        | HNIS_TC_CMPL | HNIS_DMA ))){
+			        //printk("Unhandled status 0x%x\n", intstatus);
+	        }
 	
-	spin_unlock(&host->irq_lock);
-	return IRQ_HANDLED;
+	        spin_unlock(&host->irq_lock);
+        }
+        return IRQ_HANDLED;
 }
-static void xlpmmc_tasklet_card(unsigned long param)
+static void xlpmmc_tasklet_card(long unsigned int param)
 {
         struct xlpmmc_host *host;
         host = (struct xlpmmc_host*)param;
@@ -788,40 +834,32 @@ static const struct mmc_host_ops xlpmmc_ops = {
 
 static int __devinit xlpmmc_probe(struct platform_device *pdev)
 {
-	struct mmc_host *mmc;
-	struct xlpmmc_host *host;
+	struct mmc_host *mmc=NULL;
+        struct xlpmmc_host **xlpmmc_host_data;
+	struct xlpmmc_host *host=NULL;
 	struct resource *r;
-	int ret;
-
-	mmc = mmc_alloc_host(sizeof(struct xlpmmc_host), &pdev->dev);
-	if (!mmc) {
-		dev_err(&pdev->dev, "no memory for mmc_host\n");
-		ret = -ENOMEM;
-		goto out0;
-	}
+	int count, irq, ret=0;
+        void  __iomem         *base;
+        void* ioarea;
+        int maxslots = findmaxslots();
 
-	host = mmc_priv(mmc);
-	host->mmc = mmc;
-	host->platdata = pdev->dev.platform_data;
-	host->pdev = pdev;
-	spin_lock_init(&host->irq_lock);
+        xlpmmc_host_data = kmalloc(sizeof(struct xlpmmc_host*) *maxslots, GFP_KERNEL);
 
-	ret = -ENODEV;
 	r = platform_get_resource(pdev, IORESOURCE_MEM, 0);
 	if (!r) {
 		dev_err(&pdev->dev, "no mmio defined\n");
 		goto out1;
 	}
 
-	host->ioarea = request_mem_region(r->start, r->end - r->start + 1,
+	ioarea = request_mem_region(r->start, r->end - r->start + 1,
 					   pdev->name);
-	if (!host->ioarea) {
+	if (!ioarea) {
 		dev_err(&pdev->dev, "mmio already in use\n");
 		goto out1;
 	}
 
-	host->base = (void *)ioremap_nocache(r->start, PAGE_SIZE);
-	if (!host->base) {
+	base = (void *)ioremap_nocache(r->start, PAGE_SIZE);
+	if (!base) {
 		dev_err(&pdev->dev, "cannot remap mmio\n");
 		goto out2;
 	}
@@ -832,80 +870,119 @@ static int __devinit xlpmmc_probe(struct platform_device *pdev)
 		goto out3;
 	}
 
-	host->irq = r->start;
-	ret = request_irq(host->irq, xlpmmc_irq, IRQF_SHARED,
-			  "xlp-mmc", host);
+
+	hc_wr32(base, HC_SYSCTRL,0x1C, 0);
+        msleep(5);
+
+        for (count=0; count<maxslots; count ++) {                                                            
+	        mmc = mmc_alloc_host(sizeof(struct xlpmmc_host), &pdev->dev);
+	        if (!mmc) {
+		        dev_err(&pdev->dev, "no memory for mmc_host\n");
+		        ret = -ENOMEM;
+		        goto out0;
+	        }
+
+	        host = mmc_priv(mmc);
+	        host->mmc = mmc;
+                host->base = base;
+	        spin_lock_init(&host->irq_lock);
+                xlpmmc_host_data[count] = host;
+                host->host_array = xlpmmc_host_data;
+                /* S1: Clear any set INT Bits */
+                hc_wr32(base, HC_NORMAL_INT_STS, hc_rd32(base, HC_NORMAL_INT_STS, count), count);
+        }
+
+	irq = r->start;
+	ret = request_irq(irq, xlpmmc_irq, IRQF_SHARED,
+			  "xlp-mmc", xlpmmc_host_data);
 	if (ret) {
 		dev_err(&pdev->dev, "cannot grab IRQ\n");
 		goto out3;
 	}
         
-        gpio_regwrite(0, XLP_GPIO_INTEN00,   gpio_regread(0, XLP_GPIO_INTEN00) | 0x20000000);
-        gpio_regwrite(0, XLP_GPIO_INT_POLAR0, gpio_regread(0, XLP_GPIO_INT_POLAR0) | 0x20000000);
-        gpio_regwrite(0, XLP_GPIO_INT_TYPE0, gpio_regread(0, XLP_GPIO_INT_TYPE0) | 0x20000000);
-        
-        ret = request_irq(xlp_irt_to_irq(XLP_GPIO_INT0_IRT), xlpmmc_det_irq,
-                IRQF_SHARED, "mmc-gpio", host);
+        ret = request_irq(xlp_irt_to_irq(0, XLP_GPIO_INT0_IRT), xlpmmc_det_irq,
+                IRQF_SHARED, "mmc-gpio", xlpmmc_host_data);
         if (ret) {
                 dev_warn(&pdev->dev, "request MMC detect irq failed\n");
-                free_irq(xlp_irt_to_irq(XLP_GPIO_INT0_IRT), host);
-        }
-                                                                   
-        if (gpio_is_valid(GPIO_MMC_DETECT)) {
-               if (gpio_request(GPIO_MMC_DETECT, "mmc_detect")) {
-                pr_debug("no detect pin available\n");
-               }
+                free_irq(xlp_irt_to_irq(0, XLP_GPIO_INT0_IRT), xlpmmc_host_data);
         }
+        
+        for (count=0; count<maxslots; count ++) {
+                host = xlpmmc_host_data[count];
+                mmc  = host->mmc;
+	        host->platdata = pdev->dev.platform_data;
+	        host->pdev = pdev;
+                host->slot=count;
+                host->irq = irq;
+                host->base = base;
+                host->ioarea = ioarea;
+                tasklet_init(&host->card_tasklet,
+                        xlpmmc_tasklet_card, (unsigned long)host);
+                setup_timer(&host->timer, xlpmmc_timeout_timer, (unsigned long)host);
+
+	        ret = -ENODEV;
+                        
+                if (gpio_is_valid(GPIO_MMC_DETECT + count)) {
+                        if (gpio_request(GPIO_MMC_DETECT+count, "mmc_detect")) {
+                                pr_debug("no detect pin available\n");
+                        }
+                }
+        
+                gpio_regwrite(0, XLP_GPIO_INTEN00,   gpio_regread(0, XLP_GPIO_INTEN00) | 
+                                0x1<<(GPIO_MMC_DETECT));
+                gpio_regwrite(0, XLP_8XX_GPIO_INT_POLAR0, gpio_regread(0, XLP_8XX_GPIO_INT_POLAR0) | 
+                                0x1<<(GPIO_MMC_DETECT+count));
+                gpio_regwrite(0, XLP_8XX_GPIO_INT_TYPE0, gpio_regread(0, XLP_8XX_GPIO_INT_TYPE0) | 
+                                0x1<<(GPIO_MMC_DETECT+count));
 
-	mmc->ops = &xlpmmc_ops;
+	        mmc->ops = &xlpmmc_ops;
 
-	mmc->f_min =     1039000;
-	mmc->f_max =   133000000;  
+	        mmc->f_min =     1039000;
+	        mmc->f_max =   133000000;  
 
-	mmc->max_blk_size = 512;
-	mmc->max_blk_count = 2048;
+	        mmc->max_blk_size = 512;
+	        mmc->max_blk_count = 2048;
 
-	mmc->max_seg_size = XLPMMC_DESCRIPTOR_SIZE;
-	mmc->max_phys_segs = XLPMMC_DESCRIPTOR_COUNT;	
+	        mmc->max_seg_size = XLPMMC_DESCRIPTOR_SIZE;
+	        mmc->max_phys_segs = XLPMMC_DESCRIPTOR_COUNT;	
 	
-	/* Enable DMA mode. Defualt is PIO*/
-	host->flags |= HOST_F_DMA;
+	        /* Enable DMA mode. Defualt is PIO*/
+	        host->flags |= HOST_F_DMA;
 
-	mmc->ocr_avail = XLPMMC_OCR; /* volt 2.70 ~ 3.60 */
+	        mmc->ocr_avail = XLPMMC_OCR; /* volt 2.70 ~ 3.60 */
 	
-	mmc->caps = MMC_CAP_4_BIT_DATA;
-
-	host->status = HOST_S_IDLE;
+	        mmc->caps = MMC_CAP_4_BIT_DATA;
 
-        tasklet_init(&host->card_tasklet,
-                xlpmmc_tasklet_card, (unsigned long)host);
-        setup_timer(&host->timer, xlpmmc_timeout_timer, (unsigned long)host);
+	        host->status = HOST_S_IDLE;
 
-	xlpmmc_reset_controller(host);
+	        xlpmmc_reset_controller(host, count);
+                msleep(5);
 
-	ret = mmc_add_host(mmc);
-	if (ret) {
-		dev_err(&pdev->dev, "cannot add mmc host\n");
-		goto out6;
-	}
+	        ret = mmc_add_host(mmc);
+	        if (ret) {
+		        dev_err(&pdev->dev, "cannot add mmc host\n");
+		        goto out6;
+	        }
 
-	platform_set_drvdata(pdev, host);
-
-	printk(KERN_INFO "xlp-mmc " ": MMC Controller %d set up at %p"
-		" (mode=%s)\n", pdev->id, host->base,
-		host->flags & HOST_F_DMA ? "dma" : "pio");
+	        printk(KERN_INFO "xlp-mmc " ": MMC Controller %d set up at %p"
+		        " (mode=%s)\n", pdev->id, host->base,
+		        host->flags & HOST_F_DMA ? "dma" : "pio");
+        }
+	platform_set_drvdata(pdev, xlpmmc_host_data);
 
 	return 0;	/*Everything is OK */
 
 out6:
 	/*Disable the host if init fails*/
 	mmc_remove_host(host->mmc);
-	xlpmmc_set_power(host, 0);
         
-        if (gpio_is_valid(GPIO_MMC_DETECT)) {
-                gpio_free(GPIO_MMC_DETECT);
+        for (count=0; count<maxslots; count ++) { 
+	        xlpmmc_set_power(host, 0, count);
+                if (gpio_is_valid(GPIO_MMC_DETECT+count)) {
+                        gpio_free(GPIO_MMC_DETECT+count);
+                }
         }
-	free_irq(xlp_irt_to_irq(XLP_GPIO_INT0_IRT), host);
+	free_irq(xlp_irt_to_irq(0, XLP_GPIO_INT0_IRT), host);
 	free_irq(host->irq, host);
 out3:
 	iounmap((void *)host->base);
@@ -920,19 +997,24 @@ out0:
 
 static int __devexit xlpmmc_remove(struct platform_device *pdev)
 {
-	struct xlpmmc_host *host = platform_get_drvdata(pdev);
+	void   *data = platform_get_drvdata(pdev);
+        struct xlpmmc_host **xlpmmc_host_data = data;
+        int count;
+        int maxslots = findmaxslots();
 	
-	if (host) {
+        for (count=0; count<maxslots; count ++) { 
+            struct xlpmmc_host *host = xlpmmc_host_data[count];
+	    if (host) {
 		mmc_remove_host(host->mmc);
 		
-		xlpmmc_set_power(host, 0);
-                if (gpio_is_valid(GPIO_MMC_DETECT)) {
-                        gpio_free(GPIO_MMC_DETECT);
+		xlpmmc_set_power(host, 0, count);
+                if (gpio_is_valid(GPIO_MMC_DETECT+count)) {
+                        gpio_free(GPIO_MMC_DETECT+count);
                 }
                 del_timer_sync(&host->timer);
                 tasklet_kill(&host->card_tasklet);
 		
-		free_irq(xlp_irt_to_irq(XLP_GPIO_INT0_IRT), host);
+		free_irq(xlp_irt_to_irq(0, XLP_GPIO_INT0_IRT), host);
 		free_irq(host->irq, host);
 		iounmap((void *)host->base);
 		release_resource(host->ioarea);
@@ -941,7 +1023,8 @@ static int __devexit xlpmmc_remove(struct platform_device *pdev)
 		mmc_free_host(host->mmc);
 		platform_set_drvdata(pdev, NULL);
 	
-	}
+            }
+        }
 	return 0;
 }
 
diff --git a/drivers/net/nae/init_nae.c b/drivers/net/nae/init_nae.c
index f9560ac..52de3a8 100644
--- a/drivers/net/nae/init_nae.c
+++ b/drivers/net/nae/init_nae.c
@@ -26,7 +26,7 @@ static void config_fmn(void)
 	/* Configure FMN again but only cpu credits */
 	msgrng_access_enable(mflags);
 
-	nlm_xlp_msgring_int_handler(XLP_IRQ_MSGRING, NULL);
+	nlm_xlp_msgring_int_handler(XLP_IRQ_MSGRING_RVEC, NULL);
 
 	msgrng_access_disable(mflags);
 
diff --git a/drivers/net/nae/xlp_nae.c b/drivers/net/nae/xlp_nae.c
index cfed0d7..c273cc6 100644
--- a/drivers/net/nae/xlp_nae.c
+++ b/drivers/net/nae/xlp_nae.c
@@ -53,7 +53,7 @@
 
 #include <asm/netlogic/hal/nlm_hal_fmn.h>
 #include <asm/netlogic/hal/nlm_hal_nae.h>
-#include <asm/netlogic/xlp_hal_pic.h>
+#include <asm/netlogic/xlp_irq.h>
 
 #include <asm/netlogic/hal/nlm_hal_macros.h>
 
@@ -350,10 +350,9 @@ static void nlm_xlp_nae_msgring_handler(uint32_t vc, uint32_t src_id,
 					uint64_t msg0, uint64_t msg1,
 					uint64_t msg2, uint64_t msg3, void *data);
 
-static void nlm_xlp_mac_timer(unsigned long data);
 static struct net_device_stats *nlm_xlp_mac_get_stats(struct net_device *dev);
 
-static struct net_device *dev_mac[MAX_GMAC_PORT];
+static struct net_device *dev_mac[NLM_MAX_NODES][MAX_GMAC_PORT];
 
 extern struct proc_dir_entry *nlm_root_proc;
 static struct tasklet_struct mac_refill_task[MAX_GMAC_PORT];
@@ -709,7 +708,7 @@ static void mac_frin_replenish(unsigned long  arg/* ignored */)
 			atomic_t *frin_to_be_sent;
 			int num_fr_in=0;
 
-			dev = dev_mac[i];
+			dev = dev_mac[node][i];
 			if (dev == 0)
 				goto skip;
 
@@ -987,7 +986,7 @@ static void nlm_xlp_nae_init(void)
 
 		register_netdev(dev);
 
-		dev_mac[i] = dev;
+		dev_mac[node][i] = dev;
 		xlp_mac_setup_hwaddr(priv);
 
 		}
@@ -1043,18 +1042,7 @@ static int  nlm_xlp_nae_open (struct net_device *dev)
 	}
 #endif
 
-	/* set timer to test rx routine */
-	init_timer(&priv->link_timer);
-	/* priv->link_timer.expires = jiffies + HZ; First timer after 1 sec */
-	priv->link_timer.expires = jiffies + HZ; /* First timer after 1s */
-	priv->link_timer.data    = (unsigned long)((priv->node << 16) | priv->port);
-	priv->link_timer.function = &nlm_xlp_mac_timer;
-	priv->phy_oldlinkstat = -1;
-
 	netif_tx_start_all_queues(dev);
-	add_timer(&priv->link_timer);
-
-	/* napi_enable(&priv->napi); */
 
 	STATS_SET(priv->stats.tx_packets, 0);
 	STATS_SET(priv->stats.tx_errors, 0);
@@ -1091,7 +1079,6 @@ static int  nlm_xlp_nae_stop (struct net_device *dev)
 	spin_lock_irq(&priv->lock);
 	nlm_xlp_mac_set_enable(priv, 0);
 	priv->inited = 0;
-	del_timer_sync(&priv->link_timer);
 
 	netif_tx_stop_all_queues(dev);
 
@@ -1569,7 +1556,7 @@ static void nlm_xlp_nae_msgring_handler(uint32_t vc, uint32_t src_id,
 			return;
 		}
 
-		pdev = (struct net_device *)dev_mac[port];
+		pdev = (struct net_device *)dev_mac[node][port];
 		if (!pdev) {
 			pr_info("[%s]: [RX] wrong port = %d(context = %d)? pdev = NULL!\n", __func__, port, context);
 			return;
@@ -1627,10 +1614,10 @@ static void nlm_xlp_nae_msgring_handler(uint32_t vc, uint32_t src_id,
 				pg = rx_cookie->page;
 				put_page(pg);
 			}
-			mac_frin_replenish_msgs(dev_mac[port], tot_desc);
+			mac_frin_replenish_msgs(dev_mac[node][port], tot_desc);
 			return;
 		}
-		mac_frin_replenish_msgs(dev_mac[port], tot_desc);
+		mac_frin_replenish_msgs(dev_mac[node][port], tot_desc);
 
 		/* allocate an skb for header */
 		skb = dev_alloc_skb(NETL_JUMBO_SKB_HDR_LEN + 16);
@@ -1641,7 +1628,7 @@ static void nlm_xlp_nae_msgring_handler(uint32_t vc, uint32_t src_id,
 			recycle_rx_desc(addr, pdev);
 			return;
 		}
-		skb->dev = dev_mac[port];
+		skb->dev = dev_mac[node][port];
 		hlen = (len > NETL_JUMBO_SKB_HDR_LEN) ?
 				NETL_JUMBO_SKB_HDR_LEN: len;
 		/* after this call, skb->data is pointing to start of MAChdr */
@@ -1675,47 +1662,49 @@ static int xlp_mac_proc_read(char *page, char * *start, off_t off,
 {
 	int len = 0;
 	off_t begin = 0;
-	int i = 0, cpu = 0;
+	int i = 0, cpu = 0, node;
 	struct net_device *dev = 0;
 	struct dev_data *priv = 0;
 
 	len += sprintf(page + len, "uboot_pkts = %ld\n", stats_uboot_pkts);
 
-	for (i = 0; i < MAX_GMAC_PORT; i++) {
+	for(node = 0; node < NLM_MAX_NODES; node++) {
+		for (i = 0; i < MAX_GMAC_PORT; i++) {
 
-		dev = dev_mac[i];
+			dev = dev_mac[node][i];
 
-		if (dev == 0)
-			continue;
+			if (dev == 0)
+				continue;
 
-		priv = netdev_priv(dev);
+			priv = netdev_priv(dev);
 
-		len += sprintf(page + len, "=============== port@%d ==================\n", i);
+			len += sprintf(page + len, "=============== port@%d ==================\n", i);
 
-		len += sprintf(page + len, "per port@%d: frin_to_be_sent = %ld num_replenishes = %ld frin_sent = %ld\n",
-				i, atomic64_read(&priv->frin_to_be_sent),
-				atomic64_read(&priv->num_replenishes),
-				atomic64_read(&priv->total_frin_sent));
+			len += sprintf(page + len, "per port@%d: frin_to_be_sent = %ld num_replenishes = %ld frin_sent = %ld\n",
+					i, atomic64_read(&priv->frin_to_be_sent),
+					atomic64_read(&priv->num_replenishes),
+					atomic64_read(&priv->total_frin_sent));
 
-		len += sprintf(page + len,
-				"per port@%d: %lu(rxp) %lu(rxb) %lu(txp) %lu(txb)\n",
-				i,
-				STATS_READ(priv->stats.rx_packets),
-				STATS_READ(priv->stats.rx_bytes),
-				STATS_READ(priv->stats.tx_packets),
-				STATS_READ(priv->stats.tx_bytes));
+			len += sprintf(page + len,
+					"per port@%d: %lu(rxp) %lu(rxb) %lu(txp) %lu(txb)\n",
+					i,
+					STATS_READ(priv->stats.rx_packets),
+					STATS_READ(priv->stats.rx_bytes),
+					STATS_READ(priv->stats.tx_packets),
+					STATS_READ(priv->stats.tx_bytes));
 
-		for (cpu = 0; cpu < NR_CPUS; cpu++) {
-			unsigned long tx = priv->cpu_stats[cpu].tx_packets;
-			unsigned long txc = priv->cpu_stats[cpu].txc_packets;
-			unsigned long rx = priv->cpu_stats[cpu].rx_packets;
-			unsigned long ints = priv->cpu_stats[cpu].interrupts;
+			for (cpu = 0; cpu < NR_CPUS; cpu++) {
+				unsigned long tx = priv->cpu_stats[cpu].tx_packets;
+				unsigned long txc = priv->cpu_stats[cpu].txc_packets;
+				unsigned long rx = priv->cpu_stats[cpu].rx_packets;
+				unsigned long ints = priv->cpu_stats[cpu].interrupts;
 
-			if (!tx && !txc && !rx && !ints)
-				continue;
+				if (!tx && !txc && !rx && !ints)
+					continue;
 
-			len += sprintf(page + len, "per cpu@%d: %lu(txp) %lu(txcp) %lu(rxp) %lu(int)\n",
-					cpu, tx, txc, rx, ints);
+				len += sprintf(page + len, "per cpu@%d: %lu(txp) %lu(txcp) %lu(rxp) %lu(int)\n",
+						cpu, tx, txc, rx, ints);
+			}
 		}
 	}
 
@@ -1731,45 +1720,7 @@ static int xlp_mac_proc_read(char *page, char * *start, off_t off,
 	return len;
 }
 
-/*
- * nlm_xlp_mac_timer - interrupt handler routine
- * @data - parameter passed in when timer interrupt handler is called.
- */
-static void nlm_xlp_mac_timer(unsigned long data)
-{
-	unsigned port = data;
-	struct net_device *dev = (struct net_device *)dev_mac[port];
-	struct dev_data *priv = netdev_priv(dev);
-	int next_tick = HZ / 1000; /* 1ms */
-
-	/* printk("[%s] A0 Workaround, forcing FMN int handling \n",__func__); */
-	if (priv->inited) {
-		uint32_t cpumask = cpumask_to_uint32(&cpu_present_map); /* doesn't handle non-n0 nodes */
-		uint32_t cpumask_lo;
-		uint32_t cpumask_hi;
 
-		pic_reg_t *mmio = nlm_hal_pic_offset();
-		int cpu = hard_smp_processor_id();
-
-		cpumask = cpumask & ~(1 << cpu);
-		cpumask_hi = cpumask >> 16;;
-		cpumask_lo = cpumask & 0xffff;
-
-		/* Send IRQ_MSGRING vector in an IPI to all cpus but the current one */
-		if (cpumask_lo)
-			nlm_hal_write_pic_reg(mmio, PIC_IPI_CTL, (XLP_IRQ_MSGRING << 20) | cpumask_lo);
-
-		if (cpumask_hi)
-			nlm_hal_write_pic_reg(mmio, PIC_IPI_CTL, (XLP_IRQ_MSGRING << 20) | (1 << 16)
-					      | (cpumask_hi));
-
-		/* Run IPI handler on this cpu too */
-		nlm_xlp_msgring_int_handler(XLP_IRQ_MSGRING, NULL);
-	}
-
-	priv->link_timer.expires = jiffies + next_tick;
-	add_timer(&priv->link_timer);
-}
 
 static int __devinit nlm_xlp_nae_pci_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
 {
@@ -1788,18 +1739,21 @@ static void nlm_xlp_nae_remove(void)
 	int i;
 	struct net_device *dev = 0;
 	struct dev_data *priv = 0;
+	int node = 0;
 
-	for (i = 0; i < MAX_GMAC_PORT; i++) {
-		dev = dev_mac[i];
+	for(node = 0; node < NLM_MAX_NODES; node++) {
+		for (i = 0; i < MAX_GMAC_PORT; i++) {
+			dev = dev_mac[node][i];
 
-		/* skip non-existant ports */
-		if (dev == 0)
-			continue;
+			/* skip non-existant ports */
+			if (dev == 0)
+				continue;
 
-		priv = netdev_priv(dev);
-		/* netif_napi_del(&priv->napi); */
-		unregister_netdev(dev);
-		free_netdev(dev);
+			priv = netdev_priv(dev);
+			/* netif_napi_del(&priv->napi); */
+			unregister_netdev(dev);
+			free_netdev(dev);
+		}
 	}
 
 	remove_proc_entry("mac_stats", nlm_root_proc /* parent dir */);
diff --git a/drivers/usb/host/ehci-pci.c b/drivers/usb/host/ehci-pci.c
index 85a709b..bd1b6e1 100644
--- a/drivers/usb/host/ehci-pci.c
+++ b/drivers/usb/host/ehci-pci.c
@@ -26,52 +26,33 @@
 #ifdef CONFIG_NLM_XLP
 
 #include <asm/netlogic/hal/nlm_hal.h>
-#include <asm/netlogic/xlp_hal_pic.h>
+#include <asm/netlogic/xlp_irq.h>
 #include <asm/netlogic/xlp.h>
 #include <asm/netlogic/xlp_usb.h>
 
-extern int nlm_xlp_request_irq(int irq);
-
-volatile uint64_t *ehci_regs;
-
-static void xlp_usb_hw_start(int ctrl_no)
+static void xlp_usb_hw_start(struct pci_dev *dev)
 {
-	int val;
+	int val, node, fun;
 
-	/* enable USB EHCI interrupts(Don't enable ohci interrupt this
-	 * time, otherwise, ohci will fail for no interrupt handler installed
-	 * before enabling the interrupts.
-	 */
-	/* val = USB_CTRL_INTERRUPT_EN  | USB_OHCI_INTERRUPT_EN | USB_OHCI_INTERRUPT1_EN; */
-	val = USB_CTRL_INTERRUPT_EN;
-	usb_reg_write(0, ctrl_no, XLP_USB_INT_EN, val);
+	fun = dev->devfn & 0x7;
+	node= xlp_soc_pcidev_to_node(dev);
 
-	return;
-}
-
-static void xlp_usb_hw_stop(int ctrl_no)
-{
-	int val;
-
-	/* enable USB EHCI interrupts(Don't enable ohci interrupt this
-	 * time, otherwise, ohci will fail for no interrupt handler installed
-	 * before enabling the interrupts.
-	 */
-	/* val = USB_CTRL_INTERRUPT_EN  | USB_OHCI_INTERRUPT_EN | USB_OHCI_INTERRUPT1_EN; */
-	val = ~USB_CTRL_INTERRUPT_EN;
-	usb_reg_write(0, ctrl_no, XLP_USB_INT_EN, val);
+	// enable USB interrupts
+	val = USB_CTRL_INTERRUPT_EN  | USB_OHCI_INTERRUPT_EN | USB_OHCI_INTERRUPT1_EN;
+	usb_reg_write(node, fun, XLP_USB_INT_EN, val);
 
 	return;
 }
 
 int xlp_ehci_hcd_pci_probe(struct pci_dev *dev, const struct pci_device_id *id)
 {
-	int irq, irt, ctrl_no, ret;
+	int irq, irt, fun, node;
 
-	ctrl_no = dev->devfn & 0xF;
+	fun = dev->devfn & 0x7;
+	node= xlp_soc_pcidev_to_node(dev);
 
-	irt = usb_reg_read(0, ctrl_no, 0x3D) & 0xFFFF;
-	irq = nlm_xlp_request_irq(irt);
+	irt = usb_reg_read(node, fun, 0x3D) & 0xFFFF;
+	irq = nlm_xlp_request_irq(node, irt);
 
 	if (!irq) {
 		pr_err("Found HC with no IRQ.  Check BIOS/PCI %s setup!\n",
@@ -80,25 +61,10 @@ int xlp_ehci_hcd_pci_probe(struct pci_dev *dev, const struct pci_device_id *id)
 	}
 
 	dev->irq = irq;
-	ret = usb_hcd_pci_probe(dev, id);
-	if (ret)
-		pr_err("%s: Fail to probe xlp ehci\n", __func__);
-	else
-		xlp_usb_hw_start(ctrl_no);
+	xlp_usb_hw_start(dev);
 
-	return ret;
+	return usb_hcd_pci_probe(dev, id);
 }
-
-void xlp_ehci_hcd_pci_remove(struct pci_dev *dev)
-{
-	int ctrl_no;
-
-	ctrl_no = dev->devfn & 0xF;
-
-	xlp_usb_hw_stop(ctrl_no);
-	usb_hcd_pci_remove(dev);
-}
-
 #endif
 
 /*-------------------------------------------------------------------------*/
@@ -219,16 +185,6 @@ static int ehci_pci_setup(struct usb_hcd *hcd)
 			break;
 		}
 		break;
-
-	case PCI_VENDOR_ID_NETLOGIC:
-		/* XLP USB controller doesn't work with >2GB RAM */
-		if (pci_set_consistent_dma_mask(pdev,
-					DMA_BIT_MASK(31)) < 0)
-			ehci_warn(ehci, "can't enable XLP "
-					"workaround for >2GB RAM\n");
-		else
-			ehci_info(ehci, "Enable XLP workaround for >2GB RAM\n");
-		break;
 	}
 
 	/* cache this readonly data; minimize chip reads */
@@ -579,11 +535,10 @@ static struct pci_driver ehci_pci_driver = {
 	.id_table =	pci_ids,
 #ifndef CONFIG_NLM_XLP
 	.probe =	usb_hcd_pci_probe,
-	.remove =	usb_hcd_pci_remove,
 #else
 	.probe =	xlp_ehci_hcd_pci_probe,
-	.remove =	xlp_ehci_hcd_pci_remove,
 #endif
+	.remove =	usb_hcd_pci_remove,
 	.shutdown = 	usb_hcd_pci_shutdown,
 
 #ifdef CONFIG_PM_SLEEP
diff --git a/init/main.c b/init/main.c
index 659f776..f1cdce3 100644
--- a/init/main.c
+++ b/init/main.c
@@ -870,6 +870,7 @@ static void run_init_process(char *init_filename)
 	kernel_execve(init_filename, argv_init, envp_init);
 }
 
+#include <asm/netlogic/cpumask.h>
 /* This is a non __init function. Force it to be noinline otherwise gcc
  * makes it inline to init() and it becomes part of init.text section
  */
@@ -916,6 +917,9 @@ static noinline int init_post(void)
 
 static int __init kernel_init(void * unused)
 {
+#if defined(CONFIG_NLM_XLP)
+	struct cpumask old_mask, new_mask;
+#endif
 	struct stat console_stat;
 	/*
 	 * Wait until kthreadd is all set-up.
@@ -942,8 +946,25 @@ static int __init kernel_init(void * unused)
 	smp_init();
 	sched_init_smp();
 
+#if defined(CONFIG_NLM_XLP)
+	/* On XLP Ax, PIC device specific registers cannot be accessed
+	 * cross different nodes. So the kernel_init has to run on node 0.
+	 * On XLP B0, the flash device cannot be accessed cross node, so flash
+	 * driver has to run on node 0.
+	 * For simplicity, just let is run on vcpu 0.
+	 */
+	cpumask_copy(&old_mask, &current->cpus_allowed);
+	cpumask_clear(&new_mask);
+	cpumask_set_cpu(0, &new_mask);
+	set_cpus_allowed_ptr(current, &new_mask);
+#endif
+
 	do_basic_setup();
 
+#if defined(CONFIG_NLM_XLP)
+	set_cpus_allowed_ptr(current, &old_mask);
+#endif
+
 	/* Use /dev/console to infer if the rootfs is setup properly */
 	if (sys_newlstat((char __user *) "/dev/console", (struct stat __user *) &console_stat)
 			|| !S_ISCHR(console_stat.st_mode)) {
-- 
1.7.4

