From 4e64217d7d63587288557283680ab9a48ccaea55 Mon Sep 17 00:00:00 2001
From: Wu Zhangjin <zhangjin.wu@windriver.com>
Date: Thu, 16 Jun 2011 15:19:19 +0800
Subject: [PATCH 19/37] nlm_xlp_64_be: add pagewalker support

[ Based on netlogic SDK 20110329 ]

XLP832 supports hardware page tlb walker, it provides hardware page tlb
refill handler.

So far PageWalker is not mature enough to be enabled, these are just
placeholder for further reference.

Signed-off-by: Venu Vadapalli <vvadapalli@netlogicmicro.com>
[ Make pgwalker configurable and disable prefetch for pgwalker
  Note: the tlbsize must be read after configuration, otherwise, the
content will be wrong. For example, the default tlbsize is 32, but read
it directly will return 64 for the 1st cpu, which is wrong and make the
system run in an 'undefined' status. ]
Signed-off-by: Wu Zhangjin <zhangjin.wu@windriver.com>
---
 arch/mips/Kconfig                             |    2 +-
 arch/mips/include/asm/mach-netlogic/xlp-mmu.h |    8 +-
 arch/mips/include/asm/mmu_context.h           |   23 +++
 arch/mips/kernel/traps.c                      |    3 +
 arch/mips/mm/tlb-r4k.c                        |   29 ++++-
 arch/mips/netlogic/Kconfig                    |   12 ++
 arch/mips/netlogic/xlp/Makefile               |    2 +-
 arch/mips/netlogic/xlp/mmu.c                  |  203 +++++++++++++++++++++++++
 8 files changed, 278 insertions(+), 4 deletions(-)
 create mode 100644 arch/mips/netlogic/xlp/mmu.c

diff --git a/arch/mips/Kconfig b/arch/mips/Kconfig
index 5b3cd29..6ea9439 100644
--- a/arch/mips/Kconfig
+++ b/arch/mips/Kconfig
@@ -1328,7 +1328,7 @@ config CPU_XLP
 	select CPU_HAS_LLSC
 	select WEAK_ORDERING
 	select WEAK_REORDERING_BEYOND_LLSC
-	select CPU_HAS_PREFETCH
+	select CPU_HAS_PREFETCH if !PGWALKER
 	help
 	  Netlogic Corporation XLP processors.
 
diff --git a/arch/mips/include/asm/mach-netlogic/xlp-mmu.h b/arch/mips/include/asm/mach-netlogic/xlp-mmu.h
index 28ca984..5ec7054 100644
--- a/arch/mips/include/asm/mach-netlogic/xlp-mmu.h
+++ b/arch/mips/include/asm/mach-netlogic/xlp-mmu.h
@@ -27,10 +27,12 @@ extern DEFINE_PER_CPU(unsigned long [NR_ADDR_SEGMENTS], pgd_bases);
 
 static inline void setup_user_pgd(pgd_t *pgd)
 {
+#ifdef CONFIG_PGWALKER
 	if (read_c0_config6() & ENABLE_PGWALKER) {
 		get_cpu_var(pgd_bases)[USER_SEG] = (unsigned long) pgd;
 		put_cpu_var(pgd_bases);
 	}
+#endif
 };
 
 static __inline__ void pipeline_flush(void)
@@ -49,12 +51,16 @@ static __inline__ void pipeline_flush(void)
 		);
 }
 
+#ifdef CONFIG_PGWALKER
 #define disable_pgwalker(flags)						\
 	({ flags = read_c0_config6();					\
 		pipeline_flush(); write_c0_config6(read_c0_config6() & ~ENABLE_PGWALKER); pipeline_flush();})
 
 #define enable_pgwalker(flags)						\
 	({ write_c0_config6(read_c0_config6() | (flags & ENABLE_PGWALKER)); })
-
+#else
+#define disable_pgwalker(flags) do { } while (0)
+#define enable_pgwalker(flags)	do { } while (0)
+#endif
 
 #endif
diff --git a/arch/mips/include/asm/mmu_context.h b/arch/mips/include/asm/mmu_context.h
index d959273..f385039 100644
--- a/arch/mips/include/asm/mmu_context.h
+++ b/arch/mips/include/asm/mmu_context.h
@@ -7,6 +7,7 @@
  *
  * Copyright (C) 1996, 1997, 1998, 1999 by Ralf Baechle
  * Copyright (C) 1999 Silicon Graphics, Inc.
+ * Copyright (C) 2003-2010 Netlogic Microsystems Inc.
  */
 #ifndef _ASM_MMU_CONTEXT_H
 #define _ASM_MMU_CONTEXT_H
@@ -24,6 +25,17 @@
 #endif /* SMTC */
 #include <asm-generic/mm_hooks.h>
 
+#ifdef CONFIG_NLM_COMMON
+#include <asm/netlogic/mips-exts.h>
+#include <asm/netlogic/debug.h>
+#include <asm/mach-netlogic/mmu.h>
+#endif
+
+#ifndef CONFIG_NLM_XLP
+static inline void setup_user_pgd(pgd_t *pgd) { }
+#endif
+
+
 #ifdef CONFIG_MIPS_PGD_C0_CONTEXT
 
 #define TLBMISS_HANDLER_SETUP_PGD(pgd)				\
@@ -162,6 +174,7 @@ static inline void switch_mm(struct mm_struct *prev, struct mm_struct *next,
 {
 	unsigned int cpu = smp_processor_id();
 	unsigned long flags;
+	unsigned int __maybe_unused pflags;
 #ifdef CONFIG_MIPS_MT_SMTC
 	unsigned long oldasid;
 	unsigned long mtflags;
@@ -200,9 +213,12 @@ static inline void switch_mm(struct mm_struct *prev, struct mm_struct *next,
 	ehb(); /* Make sure it propagates to TCStatus */
 	evpe(mtflags);
 #else
+	disable_pgwalker(pflags);
 	write_c0_entryhi(cpu_asid(cpu, next));
 #endif /* CONFIG_MIPS_MT_SMTC */
 	TLBMISS_HANDLER_SETUP_PGD(next->pgd);
+	setup_user_pgd(next->pgd);
+	enable_pgwalker(pflags);
 
 	/*
 	 * Mark current->active_mm as not "active" anymore.
@@ -233,6 +249,7 @@ activate_mm(struct mm_struct *prev, struct mm_struct *next)
 {
 	unsigned long flags;
 	unsigned int cpu = smp_processor_id();
+	unsigned int  __maybe_unused pflags;
 
 #ifdef CONFIG_MIPS_MT_SMTC
 	unsigned long oldasid;
@@ -260,9 +277,12 @@ activate_mm(struct mm_struct *prev, struct mm_struct *next)
 	ehb(); /* Make sure it propagates to TCStatus */
 	evpe(mtflags);
 #else
+	disable_pgwalker(pflags);
 	write_c0_entryhi(cpu_asid(cpu, next));
 #endif /* CONFIG_MIPS_MT_SMTC */
 	TLBMISS_HANDLER_SETUP_PGD(next->pgd);
+	setup_user_pgd(next->pgd);
+	enable_pgwalker(pflags);
 
 	/* mark mmu ownership change */
 	cpumask_clear_cpu(cpu, mm_cpumask(prev));
@@ -279,6 +299,7 @@ static inline void
 drop_mmu_context(struct mm_struct *mm, unsigned cpu)
 {
 	unsigned long flags;
+	unsigned int __maybe_unused pflags;
 #ifdef CONFIG_MIPS_MT_SMTC
 	unsigned long oldasid;
 	/* Can't use spinlock because called from TLB flush within DVPE */
@@ -305,7 +326,9 @@ drop_mmu_context(struct mm_struct *mm, unsigned cpu)
 		ehb(); /* Make sure it propagates to TCStatus */
 		evpe(prevvpe);
 #else /* not CONFIG_MIPS_MT_SMTC */
+		disable_pgwalker(pflags);
 		write_c0_entryhi(cpu_asid(cpu, mm));
+		enable_pgwalker(pflags);
 #endif /* CONFIG_MIPS_MT_SMTC */
 	} else {
 		/* will get a new context next time */
diff --git a/arch/mips/kernel/traps.c b/arch/mips/kernel/traps.c
index 2b4cdf5..200081e 100644
--- a/arch/mips/kernel/traps.c
+++ b/arch/mips/kernel/traps.c
@@ -63,6 +63,8 @@ DEFINE_TRACE(trap_exit);
 
 #ifdef CONFIG_NLM_COMMON
 extern unsigned long nlm_common_ebase;
+#else
+#define mmu_init()	do { } while (0)
 #endif
 
 extern void check_wait(void);
@@ -1588,6 +1590,7 @@ void __cpuinit per_cpu_trap_init(void)
 	if (bootTC) {
 #endif /* CONFIG_MIPS_MT_SMTC */
 		cpu_cache_init();
+		mmu_init();
 		tlb_init();
 #ifdef CONFIG_MIPS_MT_SMTC
 	} else if (!secondaryTC) {
diff --git a/arch/mips/mm/tlb-r4k.c b/arch/mips/mm/tlb-r4k.c
index 234e69d..ca658e3 100644
--- a/arch/mips/mm/tlb-r4k.c
+++ b/arch/mips/mm/tlb-r4k.c
@@ -7,6 +7,7 @@
  * Copyright (C) 1997, 1998, 1999, 2000 Ralf Baechle ralf@gnu.org
  * Carsten Langgaard, carstenl@mips.com
  * Copyright (C) 2002 MIPS Technologies, Inc.  All rights reserved.
+ * Copyright (C) 2003-2010 Netlogic Microsystems Inc.
  */
 #include <linux/init.h>
 #include <linux/sched.h>
@@ -75,10 +76,12 @@ extern void build_tlb_refill_handler(void);
 void local_flush_tlb_all(void)
 {
 	unsigned long flags;
+	unsigned long __maybe_unused pflags;
 	unsigned long old_ctx;
 	int entry;
 
 	ENTER_CRITICAL(flags);
+	disable_pgwalker(pflags);
 	/* Save old context and create impossible VPN2 value */
 	old_ctx = read_c0_entryhi();
 	write_c0_entrylo0(0);
@@ -98,6 +101,7 @@ void local_flush_tlb_all(void)
 	tlbw_use_hazard();
 	write_c0_entryhi(old_ctx);
 	FLUSH_ITLB;
+	enable_pgwalker(pflags);
 	EXIT_CRITICAL(flags);
 }
 
@@ -106,15 +110,17 @@ void local_flush_tlb_all(void)
 void local_flush_tlb_mm(struct mm_struct *mm)
 {
 	int cpu;
+	unsigned long __maybe_unused pflags;
 
 	preempt_disable();
 
 	cpu = smp_processor_id();
 
+	disable_pgwalker(pflags);
 	if (cpu_context(cpu, mm) != 0) {
 		drop_mmu_context(mm, cpu);
 	}
-
+	enable_pgwalker(pflags);
 	preempt_enable();
 }
 
@@ -126,8 +132,10 @@ void local_flush_tlb_range(struct vm_area_struct *vma, unsigned long start,
 
 	if (cpu_context(cpu, mm) != 0) {
 		unsigned long size, flags;
+		unsigned long __maybe_unused pflags;
 
 		ENTER_CRITICAL(flags);
+		disable_pgwalker(pflags);
 		size = (end - start + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
 		size = (size + 1) >> 1;
 		if (size <= current_cpu_data.tlbsize/2) {
@@ -161,6 +169,7 @@ void local_flush_tlb_range(struct vm_area_struct *vma, unsigned long start,
 			drop_mmu_context(mm, cpu);
 		}
 		FLUSH_ITLB;
+		enable_pgwalker(pflags);
 		EXIT_CRITICAL(flags);
 	}
 }
@@ -168,8 +177,10 @@ void local_flush_tlb_range(struct vm_area_struct *vma, unsigned long start,
 void local_flush_tlb_kernel_range(unsigned long start, unsigned long end)
 {
 	unsigned long size, flags;
+	unsigned long __maybe_unused pflags;
 
 	ENTER_CRITICAL(flags);
+	disable_pgwalker(pflags);
 	size = (end - start + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
 	size = (size + 1) >> 1;
 	if (size <= current_cpu_data.tlbsize / 2) {
@@ -203,6 +214,7 @@ void local_flush_tlb_kernel_range(unsigned long start, unsigned long end)
 		local_flush_tlb_all();
 	}
 	FLUSH_ITLB;
+	enable_pgwalker(pflags);
 	EXIT_CRITICAL(flags);
 }
 
@@ -212,11 +224,13 @@ void local_flush_tlb_page(struct vm_area_struct *vma, unsigned long page)
 
 	if (cpu_context(cpu, vma->vm_mm) != 0) {
 		unsigned long flags;
+		unsigned long __maybe_unused pflags;
 		int oldpid, newpid, idx;
 
 		newpid = cpu_asid(cpu, vma->vm_mm);
 		page &= (PAGE_MASK << 1);
 		ENTER_CRITICAL(flags);
+		disable_pgwalker(pflags);
 		oldpid = read_c0_entryhi();
 		write_c0_entryhi(page | newpid);
 		mtc0_tlbw_hazard();
@@ -236,6 +250,7 @@ void local_flush_tlb_page(struct vm_area_struct *vma, unsigned long page)
 	finish:
 		write_c0_entryhi(oldpid);
 		FLUSH_ITLB_VM(vma);
+		enable_pgwalker(pflags);
 		EXIT_CRITICAL(flags);
 	}
 }
@@ -247,9 +262,11 @@ void local_flush_tlb_page(struct vm_area_struct *vma, unsigned long page)
 void local_flush_tlb_one(unsigned long page)
 {
 	unsigned long flags;
+	unsigned long __maybe_unused pflags;
 	int oldpid, idx;
 
 	ENTER_CRITICAL(flags);
+	disable_pgwalker(pflags);
 	oldpid = read_c0_entryhi();
 	page &= (PAGE_MASK << 1);
 	write_c0_entryhi(page);
@@ -268,6 +285,7 @@ void local_flush_tlb_one(unsigned long page)
 	}
 	write_c0_entryhi(oldpid);
 	FLUSH_ITLB;
+	enable_pgwalker(pflags);
 	EXIT_CRITICAL(flags);
 }
 
@@ -284,6 +302,7 @@ void __update_tlb(struct vm_area_struct * vma, unsigned long address, pte_t pte)
 	pmd_t *pmdp;
 	pte_t *ptep;
 	int idx, pid;
+	unsigned long __maybe_unused pflags;
 
 	/*
 	 * Handle debugger faulting in for debugee.
@@ -292,6 +311,7 @@ void __update_tlb(struct vm_area_struct * vma, unsigned long address, pte_t pte)
 		return;
 
 	ENTER_CRITICAL(flags);
+	disable_pgwalker(pflags);
 
 	pid = read_c0_entryhi() & ASID_MASK;
 	address &= (PAGE_MASK << 1);
@@ -340,6 +360,7 @@ void __update_tlb(struct vm_area_struct * vma, unsigned long address, pte_t pte)
 	}
 	tlbw_use_hazard();
 	FLUSH_ITLB_VM(vma);
+	enable_pgwalker(pflags);
 	EXIT_CRITICAL(flags);
 }
 
@@ -350,8 +371,10 @@ void __init add_wired_entry(unsigned long entrylo0, unsigned long entrylo1,
 	unsigned long wired;
 	unsigned long old_pagemask;
 	unsigned long old_ctx;
+	unsigned long __maybe_unused pflags;
 
 	ENTER_CRITICAL(flags);
+	disable_pgwalker(pflags);
 	/* Save old context and create impossible VPN2 value */
 	old_ctx = read_c0_entryhi();
 	old_pagemask = read_c0_pagemask();
@@ -371,6 +394,7 @@ void __init add_wired_entry(unsigned long entrylo0, unsigned long entrylo1,
 	tlbw_use_hazard();	/* What is the hazard here? */
 	write_c0_pagemask(old_pagemask);
 	local_flush_tlb_all();
+	enable_pgwalker(pflags);
 	EXIT_CRITICAL(flags);
 }
 
@@ -390,8 +414,10 @@ __init int add_temporary_entry(unsigned long entrylo0, unsigned long entrylo1,
 	unsigned long wired;
 	unsigned long old_pagemask;
 	unsigned long old_ctx;
+	unsigned long __maybe_unused pflags;
 
 	ENTER_CRITICAL(flags);
+	disable_pgwalker(pflags);
 	/* Save old context and create impossible VPN2 value */
 	old_ctx = read_c0_entryhi();
 	old_pagemask = read_c0_pagemask();
@@ -415,6 +441,7 @@ __init int add_temporary_entry(unsigned long entrylo0, unsigned long entrylo1,
 	write_c0_entryhi(old_ctx);
 	write_c0_pagemask(old_pagemask);
 out:
+	enable_pgwalker(pflags);
 	EXIT_CRITICAL(flags);
 	return ret;
 }
diff --git a/arch/mips/netlogic/Kconfig b/arch/mips/netlogic/Kconfig
index 7bc8786..e0ec8b3 100644
--- a/arch/mips/netlogic/Kconfig
+++ b/arch/mips/netlogic/Kconfig
@@ -4,6 +4,18 @@ config NLM_COMMON
 config NLM_XLP
 	bool
 
+config PGWALKER
+	bool "PGWalker (experimental)"
+	depends on EXPERIMENTAL
+	default n
+	help
+	  Enable this for the hardware tlb flush support
+
+	  Note: It is still an experimental feature for it has
+	  triggered some problems when running ltpfull testsuite, so
+	  it is disabled by default. Please consider the potential
+	  risk before enabling it.
+
 config NLM_ENABLE_COP2
 	bool "Enable Cop2 Access"
 	depends on NLM_XLP && 64BIT
diff --git a/arch/mips/netlogic/xlp/Makefile b/arch/mips/netlogic/xlp/Makefile
index 3cc2115..8633c2c 100644
--- a/arch/mips/netlogic/xlp/Makefile
+++ b/arch/mips/netlogic/xlp/Makefile
@@ -1,7 +1,7 @@
 EXTRA_CFLAGS := -Werror
 EXTRA_CFLAGS := $(CFLAGS) -DNLM_HAL_LINUX_KERNEL -Iarch/mips/include/asm/netlogic/hal
 
-obj-y                    	= setup.o
+obj-y                    	= setup.o mmu.o
 obj-y 				+= irq.o time.o on_chip.o
 obj-$(CONFIG_NLM_XLP) 		+= platform.o board.o
 obj-$(CONFIG_SMP)       	+= smp.o
diff --git a/arch/mips/netlogic/xlp/mmu.c b/arch/mips/netlogic/xlp/mmu.c
new file mode 100644
index 0000000..44e13f0
--- /dev/null
+++ b/arch/mips/netlogic/xlp/mmu.c
@@ -0,0 +1,203 @@
+/***********************************************************************
+ * Copyright 2003-2010 Netlogic Microsystems ("Netlogic"). All rights
+ * reserved.
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ * 1. Redistributions of source code must retain the above copyright
+ * notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ * notice, this list of conditions and the following disclaimer in
+ * the documentation and/or other materials provided with the
+ * distribution.
+ * THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+ * THE POSSIBILITY OF SUCH DAMAGE.
+ * *****************************#NETL_2#********************************/
+#include <asm/mipsregs.h>
+#include <asm/mach-netlogic/mmu.h>
+#include <asm/mach-netlogic/pgwalker.h>
+#include <asm/netlogic/mips-exts.h>
+#include <asm/asm-offsets.h>
+#include <asm/page.h>
+#include <asm/pgtable.h>
+
+#ifdef CONFIG_PGWALKER
+static int __initdata tlb_config = (ENABLE_ETLB | ENABLE_128_TLB | ENABLE_PGWALKER);
+
+int __init disable_etlb(char *str)
+{
+	tlb_config &= ~ENABLE_ETLB;
+	return 1;
+}
+__setup("disable_etlb", disable_etlb);
+
+int __init disable_128tlb(char *str)
+{
+	tlb_config &= ~ENABLE_128_TLB;
+
+	return 1;
+}
+__setup("disable_128tlb", disable_128tlb);
+
+int __init disable_pgwalker_cmdline(char *str)
+{
+	tlb_config &= ~ENABLE_PGWALKER;
+
+	return 1;
+}
+__setup("disable_pgwalker", disable_pgwalker_cmdline);
+
+/*
+ * Page Walker
+ */
+
+DEFINE_PER_CPU(unsigned long [NR_ADDR_SEGMENTS], pgd_bases);
+
+#ifndef __PAGETABLE_PMD_FOLDED
+static int pgtable_levels = PGD | PMD | PTE;
+#else
+static int pgtable_levels = PGD | PTE;
+#endif
+
+static void pgwalker_init(void)
+{
+	unsigned int value;
+	int i = 0;
+
+	if (!(tlb_config & ENABLE_PGWALKER))
+		return;
+
+	/* Initialize pgd_bases to default values */
+	for (i = 0; i < NR_ADDR_SEGMENTS; i++) {
+		get_cpu_var(pgd_bases)[i] = (unsigned long)swapper_pg_dir;
+		put_cpu_var(pgd_bases);
+	}
+
+	/*
+	 * hardware page levels information:
+	 *
+	 * [15:8] no of top-most bits of vaddr used to form
+	 *        an index into the pgdirs table
+	 * [ 7:4] shift amount by which pfn (page frame number)
+	 *        needs to be left shifted for populating the
+	 *        entrylo0 and entrylo1 registers
+	 * [ 3:0] page table levels used. 32-bit kernels use
+	 *        pgd and pte levels, while 64-bits kernels
+	 *        use pgd, pmd, and pte
+	 */
+	value  = ((ffs(NR_ADDR_SEGMENTS) - 1) & 0xff) << 8;
+	value |= ENTRYLO_PFN_SHIFT << 4;
+	value |= pgtable_levels;
+	pgw_register_write_w(PGW_MMU_INFO, value);
+
+#ifdef CONFIG_64BIT
+	pgw_register_write_d(PGW_PGD_BASES, (unsigned long long)&(__get_cpu_var(pgd_bases)[0]));
+#else
+	pgw_register_write_w(PGW_PGD_BASES, (unsigned int)&(__get_cpu_var(pgd_bases)[0]));
+#endif
+
+	/* PGD shift and mask information */
+	pgw_register_write_w(PGW_PGD_SHIFT, _PGDIR_SHIFT - _PGD_T_LOG2);
+	pgw_register_write_w(PGW_PGD_MASK, (_PTRS_PER_PGD - 1) << _PGD_T_LOG2);
+#ifndef __PAGETABLE_PMD_FOLDED
+	/*
+	 * MIPS Linux currently does not use 4-level page tables
+	 * and hence it is not necessary to fill in pud information
+	 *
+	 * So, just fill in the PMD shift and mask information
+	 */
+	pgw_register_write_w(PGW_PMD_SHIFT, _PMD_SHIFT - _PMD_T_LOG2);
+	pgw_register_write_w(PGW_PMD_MASK, (_PTRS_PER_PMD - 1) << _PMD_T_LOG2);
+#endif
+
+	/* PTE shift and mask */
+	pgw_register_write_w(PGW_PTE_SHIFT, PAGE_SHIFT - _PTE_T_LOG2);
+	pgw_register_write_w(PGW_PTE_MASK, (_PTRS_PER_PTE - 1) << _PTE_T_LOG2);
+
+	/* PUD shift and mask */
+	pgw_register_write_w(PGW_PUD_SHIFT, 0);
+	pgw_register_write_w(PGW_PUD_MASK, 0);
+
+	get_cpu_var(pgd_bases)[VMALLOC_SEG] = (unsigned long)swapper_pg_dir;
+
+#ifdef MODULE_START
+	__get_cpu_var(pgd_bases)[MODULE_SEG] = (unsigned long)swapper_pg_dir;
+#endif
+
+
+	put_cpu_var(pgd_bases);
+
+	dump_pgwalker_config();
+	pr_info("Initialized Page Walker on cpu@%d\n", hard_smp_processor_id());
+}
+
+void dump_pgwalker_config(void)
+{
+#ifdef DEBUG
+	int i = 0;
+
+	pgw_print_w(PGW_MMU_INFO);
+	pgw_print_w(PGW_PGD_SHIFT);
+	pgw_print_w(PGW_PGD_MASK);
+	pgw_print_w(PGW_PMD_SHIFT);
+	pgw_print_w(PGW_PMD_MASK);
+	pgw_print_w(PGW_PTE_SHIFT);
+	pgw_print_w(PGW_PTE_MASK);
+	pgw_print_w(PGW_PUD_SHIFT);
+	pgw_print_w(PGW_PUD_MASK);
+
+	pr_info("swapper_pg_dir = %lx\n", (unsigned long)swapper_pg_dir);
+	for (i = 0; i < NR_ADDR_SEGMENTS; i++)
+		pr_info("pgd_bases[%d] = 0x%lx\n", i, __get_cpu_var(pgd_bases)[i]);
+#endif
+}
+
+void mmu_init(void)
+{
+	/*
+	 * shift right half the number of 1s in
+	 * the pagemask and populate that value
+	 */
+	write_c0_config7(PM_DEFAULT_MASK >> (13 + (ffz(PM_DEFAULT_MASK >> 13) / 2)));
+#ifdef DEBUG
+	pr_info("( %s ): write_c0_config7 = %d\n", __func__, read_c0_config7());
+#endif
+
+	pgwalker_init();
+	tlbstats_init();
+	entrylo0_mask_init();
+
+	/* Intialize after pgwalker and others are configured! */
+	write_c0_config6(read_c0_config6() | tlb_config);
+
+	/*
+	 * Read back TLB entries after configuration
+	 */
+	current_cpu_data.tlbsize = ((read_c0_config6() >> 16) & 0xffff) + 1;
+	pr_info("%s: cpu = %d, tlbsize = %d\n", __func__, smp_processor_id(), current_cpu_data.tlbsize);
+}
+
+#else	/* !CONFIG_PGWALKER */
+
+void mmu_init(void)
+{
+	/* Intialize after pgwalker and others are configured! */
+	write_c0_config6(read_c0_config6());
+
+	/*
+	 * Read back TLB entries after configuration
+	 */
+	current_cpu_data.tlbsize = ((read_c0_config6() >> 16) & 0xffff) + 1;
+	pr_info("%s: cpu = %d, tlbsize = %d\n", __func__, smp_processor_id(), current_cpu_data.tlbsize);
+}
+
+#endif	/* CONFIG_PGWALKER */
-- 
1.7.0.2

