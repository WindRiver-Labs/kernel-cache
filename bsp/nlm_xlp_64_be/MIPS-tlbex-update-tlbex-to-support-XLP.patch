From 28d7277a540d503bc85764b25dac4f04b0bca62a Mon Sep 17 00:00:00 2001
From: Yong Zhang <yong.zhang@windriver.com>
Date: Thu, 17 Jul 2014 14:56:34 +0800
Subject: [PATCH 13/16] MIPS:tlbex: update tlbex to support XLP

Based on Broadcom SDK 2.3.

Signed-off-by: Yong Zhang <yong.zhang@windriver.com>
---
 arch/mips/mm/tlbex.c |  215 +++++++++++++++++++++++++++++++++++++++++---------
 1 files changed, 176 insertions(+), 39 deletions(-)

diff --git a/arch/mips/mm/tlbex.c b/arch/mips/mm/tlbex.c
index e8bbbab..cb361b4 100644
--- a/arch/mips/mm/tlbex.c
+++ b/arch/mips/mm/tlbex.c
@@ -32,6 +32,7 @@
 #include <asm/pgtable.h>
 #include <asm/war.h>
 #include <asm/uasm.h>
+#include <asm/setup.h>
 
 /*
  * TLB load/store/modify handlers.
@@ -55,6 +56,11 @@ struct tlb_reg_save {
 
 static struct tlb_reg_save handler_reg_save[NR_CPUS];
 
+#ifdef CONFIG_NLM_XLP
+#include <asm/netlogic/mips-exts.h>
+#endif
+
+
 static inline int r45k_bvahwbug(void)
 {
 	/* XXX: We should probe for the presence of this bug, but we don't. */
@@ -159,6 +165,18 @@ enum label_id {
 #ifdef CONFIG_HUGETLB_PAGE
 	label_tlb_huge_update,
 #endif
+#ifdef CONFIG_NLM_XLP
+	label_nohpage_tlbs,
+	label_hpage_tlb_leave,
+	label_non_btlb_process,
+	label_read_entrylo1,
+	label_illegal_access_tlbl,
+	label_exl_refill_exception,
+	label_r4000_write_probe_fail,
+#ifdef CONFIG_HUGETLB_PAGE
+	label_r4000_write_huge_probe_fail,
+#endif
+#endif
 };
 
 UASM_L_LA(_second_part)
@@ -178,6 +196,18 @@ UASM_L_LA(_large_segbits_fault)
 #ifdef CONFIG_HUGETLB_PAGE
 UASM_L_LA(_tlb_huge_update)
 #endif
+#ifdef CONFIG_NLM_XLP
+UASM_L_LA(_nohpage_tlbs)
+UASM_L_LA(_hpage_tlb_leave)
+UASM_L_LA(_non_btlb_process)
+UASM_L_LA(_read_entrylo1)
+UASM_L_LA(_illegal_access_tlbl)
+UASM_L_LA(_exl_refill_exception)
+UASM_L_LA(_r4000_write_probe_fail)
+#ifdef CONFIG_HUGETLB_PAGE
+UASM_L_LA(_r4000_write_huge_probe_fail)
+#endif
+#endif
 
 /*
  * For debug purposes.
@@ -210,6 +240,8 @@ static inline void dump_handler(const u32 *handler, int count)
 #define C0_ENTRYHI	10, 0
 #define C0_EPC		14, 0
 #define C0_XCONTEXT	20, 0
+#define C0_CONFIG6     16, 6
+#define C0_WIRED        6, 0
 
 #ifdef CONFIG_64BIT
 # define GET_CONTEXT(buf, reg) UASM_i_MFC0(buf, reg, C0_XCONTEXT)
@@ -225,7 +257,11 @@ static inline void dump_handler(const u32 *handler, int count)
  * We deliberately chose a buffer size of 128, so we won't scribble
  * over anything important on overflow before we panic.
  */
+#if defined(CONFIG_MAPPED_KERNEL)
+static u32 tlb_handler[128];
+#else
 static u32 tlb_handler[128] __cpuinitdata;
+#endif
 
 /* simply assume worst case size for labels and relocs */
 static struct uasm_label labels[128] __cpuinitdata;
@@ -386,7 +422,10 @@ static void __cpuinit build_r3000_tlb_refill_handler(void)
  * other one.To keep things simple, we first assume linear space,
  * then we relocate it to the final handler layout as needed.
  */
+
+#if !defined(CONFIG_MAPPED_KERNEL)
 static u32 final_handler[64] __cpuinitdata;
+#endif
 
 /*
  * Hazards
@@ -431,7 +470,7 @@ static void __cpuinit __maybe_unused build_tlb_probe_entry(u32 **p)
 
 /*
  * Write random or indexed TLB entry, and care about the hazards from
- * the preceeding mtc0 and for the following eret.
+ * the preceding mtc0 and for the following eret.
  */
 enum tlb_write_entry { tlb_random, tlb_indexed };
 
@@ -483,6 +522,8 @@ static void __cpuinit build_tlb_write_entry(u32 **p, struct uasm_label **l,
 	case CPU_5KC:
 	case CPU_TX49XX:
 	case CPU_PR4450:
+	case CPU_XLR:
+	case CPU_XLP:
 		uasm_i_nop(p);
 		tlbw(p);
 		break;
@@ -497,13 +538,12 @@ static void __cpuinit build_tlb_write_entry(u32 **p, struct uasm_label **l,
 	case CPU_4KSC:
 	case CPU_20KC:
 	case CPU_25KF:
-	case CPU_BCM3302:
-	case CPU_BCM4710:
+	case CPU_BMIPS32:
+	case CPU_BMIPS3300:
+	case CPU_BMIPS4350:
+	case CPU_BMIPS4380:
+	case CPU_BMIPS5000:
 	case CPU_LOONGSON2:
-	case CPU_BCM6338:
-	case CPU_BCM6345:
-	case CPU_BCM6348:
-	case CPU_BCM6358:
 	case CPU_R5500:
 		if (m4kc_tlbp_war())
 			uasm_i_nop(p);
@@ -568,6 +608,11 @@ static void __cpuinit build_tlb_write_entry(u32 **p, struct uasm_label **l,
 		tlbw(p);
 		break;
 
+	case CPU_JZRISC:
+		tlbw(p);
+		uasm_i_nop(p);
+		break;
+
 	default:
 		panic("No TLB refill handler yet (CPU type: %d)",
 		      current_cpu_data.cputype);
@@ -579,8 +624,16 @@ static __cpuinit __maybe_unused void build_convert_pte_to_entrylo(u32 **p,
 								  unsigned int reg)
 {
 	if (kernel_uses_smartmips_rixi) {
+#ifdef CONFIG_NLM_XLP
+#ifdef CONFIG_64BIT_PHYS_ADDR
+		uasm_i_dsrl(p, reg, reg, ilog2(_PAGE_GLOBAL));
+#else
+		UASM_i_SRL(p, reg, reg, ilog2(_PAGE_GLOBAL));
+#endif
+#else /* CONFIG_NLM_XLP */
 		UASM_i_SRL(p, reg, reg, ilog2(_PAGE_NO_EXEC));
 		UASM_i_ROTR(p, reg, reg, ilog2(_PAGE_GLOBAL) - ilog2(_PAGE_NO_EXEC));
+#endif /* CONFIG_NLM_XLP */
 	} else {
 #ifdef CONFIG_64BIT_PHYS_ADDR
 		uasm_i_dsrl_safe(p, reg, reg, ilog2(_PAGE_GLOBAL));
@@ -672,32 +725,14 @@ static __cpuinit void build_huge_update_entries(u32 **p,
 						unsigned int pte,
 						unsigned int tmp)
 {
-	int small_sequence;
-
-	/*
-	 * A huge PTE describes an area the size of the
-	 * configured huge page size. This is twice the
-	 * of the large TLB entry size we intend to use.
-	 * A TLB entry half the size of the configured
-	 * huge page size is configured into entrylo0
-	 * and entrylo1 to cover the contiguous huge PTE
-	 * address space.
-	 */
-	small_sequence = (HPAGE_SIZE >> 7) < 0x10000;
-
-	/* We can clobber tmp.  It isn't used after this.*/
-	if (!small_sequence)
-		uasm_i_lui(p, tmp, HPAGE_SIZE >> (7 + 16));
-
 	build_convert_pte_to_entrylo(p, pte);
 	UASM_i_MTC0(p, pte, C0_ENTRYLO0); /* load it */
-	/* convert to entrylo1 */
-	if (small_sequence)
-		UASM_i_ADDIU(p, pte, pte, HPAGE_SIZE >> 7);
-	else
-		UASM_i_ADDU(p, pte, pte, tmp);
 
+	uasm_i_ld(p, pte, sizeof(pte_t), tmp);
+	build_convert_pte_to_entrylo(p, pte);
 	UASM_i_MTC0(p, pte, C0_ENTRYLO1); /* load it */
+
+	uasm_i_ehb(p);
 }
 
 static __cpuinit void build_huge_handler_tail(u32 **p,
@@ -713,8 +748,32 @@ static __cpuinit void build_huge_handler_tail(u32 **p,
 #else
 	UASM_i_SW(p, pte, 0, ptr);
 #endif
+
+	/* adjust the ptep pointer to be at even entry boundary.
+	 * this is needed to write back entries to tlb.
+	 */
+	uasm_i_ori(p, ptr, ptr, sizeof(pte_t));
+	uasm_i_xori(p, ptr, ptr, sizeof(pte_t));
+	UASM_i_LW(p, pte, 0, ptr);
+
 	build_huge_update_entries(p, pte, ptr);
+
+#ifdef CONFIG_NLM_XLP
+	/* Similar to no hugetlb case, checking probe result.
+	 * FIXME: this should not happen really.
+	 */
+	uasm_i_mfc0(p, ptr, C0_INDEX);
+
+	uasm_il_bltz(p, r, ptr, label_r4000_write_huge_probe_fail);
+	uasm_i_nop(p);
+
+	build_huge_tlb_write_entry(p, l, r, pte, tlb_indexed, 0);
+	uasm_l_r4000_write_huge_probe_fail(l, *p);
+
+	build_huge_tlb_write_entry(p, l, r, pte, tlb_random, 0);
+#else
 	build_huge_tlb_write_entry(p, l, r, pte, tlb_indexed, 0);
+#endif
 }
 #endif /* CONFIG_HUGETLB_PAGE */
 
@@ -983,11 +1042,17 @@ static void __cpuinit build_update_entries(u32 **p, unsigned int tmp,
 		uasm_i_ld(p, tmp, 0, ptep); /* get even pte */
 		uasm_i_ld(p, ptep, sizeof(pte_t), ptep); /* get odd pte */
 		if (kernel_uses_smartmips_rixi) {
+#ifdef CONFIG_NLM_XLP
+			uasm_i_dsrl_safe(p, tmp, tmp, ilog2(_PAGE_GLOBAL)); /* convert to entrylo0 */
+			UASM_i_MTC0(p, tmp, C0_ENTRYLO0); /* load it */
+			uasm_i_dsrl_safe(p, ptep, ptep, ilog2(_PAGE_GLOBAL)); /* convert to entrylo1 */
+#else /* CONFIG_NLM_XLP */
 			UASM_i_SRL(p, tmp, tmp, ilog2(_PAGE_NO_EXEC));
 			UASM_i_SRL(p, ptep, ptep, ilog2(_PAGE_NO_EXEC));
 			UASM_i_ROTR(p, tmp, tmp, ilog2(_PAGE_GLOBAL) - ilog2(_PAGE_NO_EXEC));
 			UASM_i_MTC0(p, tmp, C0_ENTRYLO0); /* load it */
 			UASM_i_ROTR(p, ptep, ptep, ilog2(_PAGE_GLOBAL) - ilog2(_PAGE_NO_EXEC));
+#endif /* CONFIG_NLM_XLP */
 		} else {
 			uasm_i_dsrl_safe(p, tmp, tmp, ilog2(_PAGE_GLOBAL)); /* convert to entrylo0 */
 			UASM_i_MTC0(p, tmp, C0_ENTRYLO0); /* load it */
@@ -1010,6 +1075,15 @@ static void __cpuinit build_update_entries(u32 **p, unsigned int tmp,
 	if (r45k_bvahwbug())
 		build_tlb_probe_entry(p);
 	if (kernel_uses_smartmips_rixi) {
+#ifdef CONFIG_NLM_XLP
+		UASM_i_SRL(p, tmp, tmp, ilog2(_PAGE_GLOBAL)); /* convert to entrylo0 */
+		if (r4k_250MHZhwbug())
+			UASM_i_MTC0(p, 0, C0_ENTRYLO0);
+		UASM_i_MTC0(p, tmp, C0_ENTRYLO0); /* load it */
+		UASM_i_SRL(p, ptep, ptep, ilog2(_PAGE_GLOBAL)); /* convert to entrylo1 */
+		if (r45k_bvahwbug())
+			uasm_i_mfc0(p, tmp, C0_INDEX);
+#else /* CONFIG_NLM_XLP */
 		UASM_i_SRL(p, tmp, tmp, ilog2(_PAGE_NO_EXEC));
 		UASM_i_SRL(p, ptep, ptep, ilog2(_PAGE_NO_EXEC));
 		UASM_i_ROTR(p, tmp, tmp, ilog2(_PAGE_GLOBAL) - ilog2(_PAGE_NO_EXEC));
@@ -1017,6 +1091,7 @@ static void __cpuinit build_update_entries(u32 **p, unsigned int tmp,
 			UASM_i_MTC0(p, 0, C0_ENTRYLO0);
 		UASM_i_MTC0(p, tmp, C0_ENTRYLO0); /* load it */
 		UASM_i_ROTR(p, ptep, ptep, ilog2(_PAGE_GLOBAL) - ilog2(_PAGE_NO_EXEC));
+#endif /* CONFIG_NLM_XLP */
 	} else {
 		UASM_i_SRL(p, tmp, tmp, ilog2(_PAGE_GLOBAL)); /* convert to entrylo0 */
 		if (r4k_250MHZhwbug())
@@ -1224,15 +1299,19 @@ static void __cpuinit build_r4000_tlb_refill_handler(void)
 	u32 *p = tlb_handler;
 	struct uasm_label *l = labels;
 	struct uasm_reloc *r = relocs;
+#if !defined(CONFIG_MAPPED_KERNEL)
 	u32 *f;
 	unsigned int final_len;
+#endif
 	struct mips_huge_tlb_info htlb_info __maybe_unused;
 	enum vmalloc64_mode vmalloc_mode __maybe_unused;
 
 	memset(tlb_handler, 0, sizeof(tlb_handler));
 	memset(labels, 0, sizeof(labels));
 	memset(relocs, 0, sizeof(relocs));
+#if !defined(CONFIG_MAPPED_KERNEL)
 	memset(final_handler, 0, sizeof(final_handler));
+#endif
 
 	if ((scratch_reg > 0 || scratchpad_available()) && use_bbit_insns()) {
 		htlb_info = build_fast_tlb_refill_handler(&p, &l, &r, K0, K1,
@@ -1259,20 +1338,34 @@ static void __cpuinit build_r4000_tlb_refill_handler(void)
 			/* No need for uasm_i_nop */
 		}
 
+#ifdef CONFIG_NLM_XLP
+		uasm_i_dmfc0(&p, K0, OS_SCRATCH_REG2);
+		uasm_i_daddiu(&p, K0, K0, 1);
+		uasm_i_dmtc0(&p, K0, OS_SCRATCH_REG2);
+#endif
+
 #ifdef CONFIG_64BIT
 		build_get_pmde64(&p, &l, &r, K0, K1); /* get pmd in K1 */
 #else
 		build_get_pgde32(&p, K0, K1); /* get pgd in K1 */
 #endif
 
+		build_get_ptep(&p, K0, K1);
 #ifdef CONFIG_HUGETLB_PAGE
 		build_is_huge_pte(&p, &r, K0, K1, label_tlb_huge_update);
 #endif
 
-		build_get_ptep(&p, K0, K1);
 		build_update_entries(&p, K0, K1);
 		build_tlb_write_entry(&p, &l, &r, tlb_random);
 		uasm_l_leave(&l, p);
+#ifdef CONFIG_NLM_XLP
+		/* this is to avoid split of the table at eret instruction
+		   The code below does a split at 30th instruction.
+		 */
+		if ((p - tlb_handler) == 30) {
+			uasm_i_nop(&p);
+		}
+#endif
 		uasm_i_eret(&p); /* return from trap */
 	}
 #ifdef CONFIG_HUGETLB_PAGE
@@ -1286,6 +1379,7 @@ static void __cpuinit build_r4000_tlb_refill_handler(void)
 	build_get_pgd_vmalloc64(&p, &l, &r, K0, K1, vmalloc_mode);
 #endif
 
+#if !defined(CONFIG_MAPPED_KERNEL)
 	/*
 	 * Overflow check: For the 64bit handler, we need at least one
 	 * free instruction slot for the wrap-around branch. In worst
@@ -1383,15 +1477,48 @@ static void __cpuinit build_r4000_tlb_refill_handler(void)
 	}
 #endif /* CONFIG_64BIT */
 
+#endif /* !defined(CONFIG_MAPPED_KERNEL) */
+
 	uasm_resolve_relocs(relocs, labels);
+#if !defined(CONFIG_MAPPED_KERNEL)
 	pr_debug("Wrote TLB refill handler (%u instructions).\n",
 		 final_len);
 
 	memcpy((void *)ebase, final_handler, 0x100);
+#endif
 
 	dump_handler((u32 *)ebase, 64);
 }
 
+#if defined(CONFIG_MAPPED_KERNEL)
+
+static u32 tlb_handler_stub[32] __cpuinitdata;
+
+static void __cpuinit __attribute__((unused)) build_r4000_tlb_refill_handler_stub(void)
+{
+	u32 *p = tlb_handler_stub;
+
+	memset(tlb_handler_stub, 0, sizeof(tlb_handler_stub));
+	UASM_i_LA(&p, K0, (unsigned long) tlb_handler);
+	uasm_i_jr(&p, K0);
+	uasm_i_nop(&p);
+
+	/*
+	 * 32 instruction = 128 bytes
+	 */
+#ifdef CONFIG_64BIT
+	memcpy((void *)ebase + 0x80, tlb_handler_stub, 0x80); /* XTLB exception */
+#else
+	memcpy((void *)ebase, tlb_handler_stub, 0x80); /* TLB exception */
+#endif
+}
+
+#else
+
+static void __cpuinit __attribute__((unused)) build_r4000_tlb_refill_handler_stub(void) { }
+
+#endif /* defined(CONFIG_MAPPED_KERNEL) */
+
 /*
  * 128 instructions for the fastpath handler is generous and should
  * never be exceeded.
@@ -1795,21 +1922,16 @@ build_r4000_tlbchange_handler_head(u32 **p, struct uasm_label **l,
 	build_get_pgde32(p, wr.r1, wr.r2); /* get pgd in ptr */
 #endif
 
-#ifdef CONFIG_HUGETLB_PAGE
-	/*
-	 * For huge tlb entries, pmd doesn't contain an address but
-	 * instead contains the tlb pte. Check the PAGE_HUGE bit and
-	 * see if we need to jump to huge tlb processing.
-	 */
-	build_is_huge_pte(p, r, wr.r1, wr.r2, label_tlb_huge_update);
-#endif
-
 	UASM_i_MFC0(p, wr.r1, C0_BADVADDR);
 	UASM_i_LW(p, wr.r2, 0, wr.r2);
 	UASM_i_SRL(p, wr.r1, wr.r1, PAGE_SHIFT + PTE_ORDER - PTE_T_LOG2);
 	uasm_i_andi(p, wr.r1, wr.r1, (PTRS_PER_PTE - 1) << PTE_T_LOG2);
 	UASM_i_ADDU(p, wr.r2, wr.r2, wr.r1);
 
+#ifdef CONFIG_HUGETLB_PAGE
+	build_is_huge_pte(p, r, wr.r1, wr.r2, label_tlb_huge_update);
+#endif
+
 #ifdef CONFIG_SMP
 	uasm_l_smp_pgtable_change(l, *p);
 #endif
@@ -1827,7 +1949,21 @@ build_r4000_tlbchange_handler_tail(u32 **p, struct uasm_label **l,
 	uasm_i_ori(p, ptr, ptr, sizeof(pte_t));
 	uasm_i_xori(p, ptr, ptr, sizeof(pte_t));
 	build_update_entries(p, tmp, ptr);
+#ifdef CONFIG_NLM_XLP
+	uasm_i_tlbp(p);
+	uasm_i_ehb(p);
+	uasm_i_mfc0(p, ptr, C0_INDEX);
+	uasm_il_bltz(p, r, ptr, label_r4000_write_probe_fail);
+	uasm_i_nop(p);
+#endif
 	build_tlb_write_entry(p, l, r, tlb_indexed);
+#ifdef CONFIG_NLM_XLP
+	uasm_il_b(p, r, label_leave);
+	uasm_i_nop(p);
+
+	uasm_l_r4000_write_probe_fail(l, *p);
+	build_tlb_write_entry(p, l, r, tlb_random);
+#endif
 	uasm_l_leave(l, *p);
 	build_restore_work_registers(p);
 	uasm_i_eret(p); /* return from trap */
@@ -2136,6 +2272,7 @@ void __cpuinit build_tlb_refill_handler(void)
 			run_once++;
 		}
 		build_r4000_tlb_refill_handler();
+		build_r4000_tlb_refill_handler_stub();
 	}
 }
 
-- 
1.7.0

