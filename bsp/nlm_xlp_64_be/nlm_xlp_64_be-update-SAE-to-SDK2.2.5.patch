From 321577839715189aa5484372a0bf57c7d07caa35 Mon Sep 17 00:00:00 2001
From: Yanjiang Jin <yanjiang.jin@windriver.com>
Date: Thu, 15 Nov 2012 15:09:16 +0800
Subject: [PATCH 4/5] nlm_xlp_64_be: update SAE to SDK2.2.5

Update SAE to SDK2.2.5 to suit 64K PageSize.

Signed-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 drivers/crypto/nlm-sae/nlm_aead.c      | 1378 +++++++++++++++++++++++++-------
 drivers/crypto/nlm-sae/nlm_async.h     |   36 +-
 drivers/crypto/nlm-sae/nlm_auth.c      |  318 +++++++-
 drivers/crypto/nlm-sae/nlm_crypto.c    |  228 +++++-
 drivers/crypto/nlm-sae/nlm_enc.c       |  240 ++++--
 drivers/crypto/nlm-sae/nlmcrypto.h     |  990 +++++++++++++++++++++++
 drivers/crypto/nlm-sae/nlmcrypto_ifc.h |   24 +-
 7 files changed, 2761 insertions(+), 453 deletions(-)
 mode change 100644 => 100755 drivers/crypto/nlm-sae/nlm_aead.c
 mode change 100644 => 100755 drivers/crypto/nlm-sae/nlm_enc.c
 create mode 100755 drivers/crypto/nlm-sae/nlmcrypto.h

diff --git a/drivers/crypto/nlm-sae/nlm_aead.c b/drivers/crypto/nlm-sae/nlm_aead.c
old mode 100644
new mode 100755
index a3daf10..ec3ccd0
--- a/drivers/crypto/nlm-sae/nlm_aead.c
+++ b/drivers/crypto/nlm-sae/nlm_aead.c
@@ -26,27 +26,31 @@ THE POSSIBILITY OF SUCH DAMAGE.
 #include <crypto/algapi.h>
 #include <crypto/aes.h>
 #include <crypto/des.h>
+#include <crypto/ctr.h>
 #include <crypto/sha.h>
 #include <crypto/aead.h>
 #include <crypto/authenc.h>
-#include <crypto/scatterwalk.h>
 
 #include <asm/netlogic/hal/nlm_hal_fmn.h>
-#include <asm/netlogic/crypto/nlmcrypto.h>
 #include <asm/netlogic/msgring.h>
 #include "nlm_async.h"
 
 #undef NLM_CRYPTO_DEBUG
 #define Message(a, b...) //printk("[%s @ %d] "a"\n",__FUNCTION__,__LINE__, ##b)
 
-#define AES_CTR_IV_SIZE         8
 #define XLP_CRYPT_PRIORITY      310
 
 #define XCBC_DIGEST_SIZE        16
 #define MD5_DIGEST_SIZE         16
 #define MD5_BLOCK_SIZE          64
-#define CTR_RFC3686_IV_SIZE 8
 
+#define GCM_RFC4106_IV_SIZE 8
+#define GCM_RFC4106_NONCE_SIZE 4
+#define GCM_RFC4106_DIGEST_SIZE 16
+
+#define CCM_RFC4309_NONCE_SIZE 3
+#define CCM_RFC4309_IV_SIZE 8
+#define CCM_RFC4309_DIGEST_SIZE 16
 
 /*
  						CTRL DESC MEMORY LAYOUT
@@ -63,11 +67,15 @@ struct nlm_aead_ctx
 	uint32_t iv_len;
 	int cbc;
 	uint16_t stat;
+	struct crypto_aead  * fallback;
 };
 
-#define MAX_FRAGS		18
+#define MAX_FRAGS		18	
 #define CTRL_DESC_SIZE		(sizeof(struct nlm_aead_ctx) + 128)
 #define DES3_CTRL_DESC_SIZE	(2*CTRL_DESC_SIZE + 2*64)	//Allocate 2 separate control desc for encryption and decryption
+#define CACHE_ALIGN		64
+#define IV_AEAD_PADDING         128
+#define TAG_LEN			64
 
 /*
  						PACKET DESC MEMORY LAYOUT
@@ -76,12 +84,11 @@ struct nlm_aead_ctx
 	|  for alignment | 18 * (2*64)			  | for alignment |			    | for hash |
 	 ------------------------------------------------------------------------------------------------------
  */
-
-#define PACKET_DESC_SIZE	(64 + sizeof(struct nlm_crypto_pkt_param) + MAX_FRAGS*(2*64) + 64 + sizeof(struct nlm_async_crypto) + 128 )
-#define NLM_CRYPTO_PKT_PARAM_OFFSET(addr)	(((unsigned long)addr + 64) & ~0x3fULL)
-#define NLM_ASYNC_PTR_PARAM_OFFSET(addr)	(((unsigned long)(addr + 64 + sizeof(struct nlm_crypto_pkt_param) + MAX_FRAGS*(2*64)) + 64) & ~0x3fULL)
-#define NLM_HASH_OFFSET(addr)			((unsigned long)addr + (PACKET_DESC_SIZE - 64))
-#define NLM_IV_OFFSET(addr)			((unsigned long)addr + (PACKET_DESC_SIZE - 128))
+#define PACKET_DESC_SIZE	(CACHE_ALIGN + sizeof(struct nlm_crypto_pkt_param) + (MAX_FRAGS*(2*8)) + 64 + sizeof(struct nlm_async_crypto) + 64 + IV_AEAD_PADDING + TAG_LEN)
+#define NLM_CRYPTO_PKT_PARAM_OFFSET(addr)	(((unsigned long)addr + CACHE_ALIGN ) & ~0x3fUL)
+#define NLM_ASYNC_PTR_PARAM_OFFSET(addr)	((unsigned long)addr + CACHE_ALIGN + sizeof(struct nlm_crypto_pkt_param) + (MAX_FRAGS*(2*8)) + 64 + ~0x3fUL) 
+#define NLM_HASH_OFFSET(addr)			((unsigned long)addr + (PACKET_DESC_SIZE - TAG_LEN))
+#define NLM_IV_OFFSET(addr)			((unsigned long)addr + (PACKET_DESC_SIZE - TAG_LEN - IV_AEAD_PADDING ))
 
 
 #define XLP_CRYPT_PRIORITY	310
@@ -106,7 +113,13 @@ extern uint32_t nlm_hal_send_msg3(uint32_t dst, uint32_t code, uint64_t data0, u
 extern uint32_t nlm_hal_recv_msg2(uint32_t dst, uint32_t *src, uint32_t *size, uint32_t *code, uint64_t *data0, uint64_t *data1);
 extern int auth_mode_key_len[NLM_HASH_MAX][NLM_HASH_MODE_MAX];
 extern int cipher_mode_iv_len[NLM_CIPHER_MAX][NLM_CIPHER_MODE_MAX];
+extern int alloc_pkt_param( struct nlm_async_crypto * async,struct nlm_crypto_pkt_param ** pkt_param, int max_frags );
+
+
+
+static int no_of_alg_registered = 0;
 
+#ifdef NLM_CRYPTO_DEBUG
 static void print_buf(unsigned char *msg, unsigned char *buf, int len)
 {
 #define TMP_BUF		50
@@ -128,15 +141,16 @@ static void print_buf(unsigned char *msg, unsigned char *buf, int len)
 		printk("[%s]\n",tmp_buf);
 	}
 }
-
+#endif
 static struct nlm_aead_ctx *nlm_crypto_aead_ctx(struct crypto_aead *tfm)
 {
-	return (struct nlm_aead_ctx *)(((unsigned long)((uint8_t *)crypto_aead_ctx(tfm) + 63)) & ~(0x3f));
+
+	return (struct nlm_aead_ctx *)(((unsigned long)((uint8_t *)crypto_aead_ctx(tfm) + 64)) & ~(0x3fUL));
 }
 
 static struct nlm_aead_ctx *nlm_crypto_tfm_ctx(struct crypto_tfm *tfm)
 {
-	return (struct nlm_aead_ctx *)(((unsigned long)((uint8_t *)crypto_tfm_ctx(tfm) + 63)) & ~(0x3f));
+	return (struct nlm_aead_ctx *)(((unsigned long)((uint8_t *)crypto_tfm_ctx(tfm) + 64)) & ~(0x3fUL));
 }
 
 
@@ -160,11 +174,29 @@ static int aead_cra_cbc_init(struct crypto_tfm *tfm)
 	return 0;
 }
 
+static int aead_cra_init_ccm(struct crypto_tfm *tfm)
+{
+	struct nlm_aead_ctx *nlm_ctx = (struct nlm_aead_ctx *)nlm_crypto_tfm_ctx(tfm);
+	nlm_ctx->cbc = 0;
+	tfm->crt_aead.reqsize = PACKET_DESC_SIZE; 
+	nlm_ctx->fallback = crypto_alloc_aead("rfc4309(ccm(aes-generic))",CRYPTO_ALG_TYPE_AEAD ,0) ;
+	return 0;
+}
+
+static int aead_cra_init_gcm(struct crypto_tfm *tfm)
+{
+	struct nlm_aead_ctx *nlm_ctx = (struct nlm_aead_ctx *)nlm_crypto_tfm_ctx(tfm);
+	nlm_ctx->cbc = 0;
+	tfm->crt_aead.reqsize = PACKET_DESC_SIZE; 
+	nlm_ctx->fallback = crypto_alloc_aead("rfc4106(gcm(aes-generic))",CRYPTO_ALG_TYPE_AEAD ,0) ;
+	return 0;
+}
+
 static int aead_cra_init(struct crypto_tfm *tfm)
 {
-	printk("[%s %d]\n",__FUNCTION__, __LINE__);
-	tfm->crt_aead.reqsize = PACKET_DESC_SIZE;
-	printk("[%s %d]\n",__FUNCTION__, __LINE__);
+	struct nlm_aead_ctx *nlm_ctx = (struct nlm_aead_ctx *)nlm_crypto_tfm_ctx(tfm);
+	nlm_ctx->cbc = 0;
+	tfm->crt_aead.reqsize = PACKET_DESC_SIZE; 
 	return 0;
 }
 
@@ -221,6 +253,27 @@ static int get_cipher_aes_algid(unsigned int cipher_keylen)
 	}
 }
 
+
+static int get_auth_aes_algid(unsigned int hash_keylen)
+{
+
+	switch (hash_keylen) {
+	case 16:
+		return NLM_HASH_AES128;
+		break;
+	case 24:
+		return NLM_HASH_AES192;
+		break;
+	case 32:
+		return NLM_HASH_AES256;
+		break;
+	default:
+		printk(KERN_WARNING "[%s]: Cannot handle keylen = %d\n",
+		       __FUNCTION__, hash_keylen);
+	    return -EINVAL;
+	}
+}
+
 /*
    All Setkey goes here.
  */
@@ -253,7 +306,6 @@ static int xlp_aes_cbc_setkey( struct crypto_aead *tfm, uint8_t *key, unsigned i
 		return -EINVAL;
 	}
 
-	printk("Initial authkeylen %d, cipherkeylen %d\n", cipher_keylen, auth_keylen);
 	key += RTA_ALIGN(rta->rta_len);
 	cipher_key = key + auth_keylen;
 	memcpy(auth_key, key, auth_keylen);
@@ -265,8 +317,11 @@ static int xlp_aes_cbc_setkey( struct crypto_aead *tfm, uint8_t *key, unsigned i
 
 	auth_keylen = auth_mode_key_len[hash][mode];
 
+	#ifdef 	NLM_CRYPTO_DEBUG
+	printk("Initial authkeylen %d, cipherkeylen %d\n", cipher_keylen, auth_keylen);
 	print_buf("ENC_KEY:", cipher_key, cipher_keylen);
 	print_buf("AUTH_KEY:", auth_key, auth_keylen);
+	#endif
 
 	ret =  nlm_crypto_fill_pkt_ctrl(ctrl, hmac, hash, 
 			mode, cipher_alg, NLM_CIPHER_MODE_CBC, 0, cipher_key, 
@@ -275,10 +330,70 @@ static int xlp_aes_cbc_setkey( struct crypto_aead *tfm, uint8_t *key, unsigned i
 	return ret;
 }
 
+static int xlp_aes_ctr_setkey( struct crypto_aead *tfm, u8 *key,
+					unsigned int keylen, int hash, int mode, uint16_t h_stat)
+{
+	struct nlm_aead_ctx *ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+	struct nlm_crypto_pkt_ctrl *ctrl = &ctx->ctrl;
+	unsigned int cipher_keylen=0, auth_keylen=0;
+	int ret;
+	int nonce_len = CTR_RFC3686_NONCE_SIZE;
+	int cipher_alg;
+	uint8_t auth_key[128];
+	uint8_t *cipher_key;
+	struct rtattr *rta = (struct rtattr *)key;
+	int hmac = ((mode == NLM_HASH_MODE_XCBC) ? 0: 1);
+
+	if ((ret = get_cipher_auth_keylen(key, keylen, &cipher_keylen,
+					  &auth_keylen)) < 0) {
+		crypto_aead_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+		printk("ERR: Bad key len\n");
+		return ret;
+	}
+
+	cipher_keylen -= nonce_len;
+
+	cipher_alg = get_cipher_aes_algid(cipher_keylen);
+	if (cipher_alg < 0) {
+		crypto_aead_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+		//sandip: check return val
+		printk("ERR: Bad aes key len\n");
+		return -EINVAL;
+	}
+	ctx->stat = cipher_alg - 1 + 3;
+	ctx->stat = ctx->stat | (h_stat << 8);
+	key += RTA_ALIGN(rta->rta_len);
+	memcpy(auth_key, key, auth_keylen);
+	cipher_key = key + auth_keylen;
+	memcpy(ctx->iv_buf,key+auth_keylen+cipher_keylen, nonce_len);
+	#ifdef 	NLM_CRYPTO_DEBUG
+	printk("Initial authkeylen %d, cipherkeylen %d\n", cipher_keylen, auth_keylen);
+	print_buf ( "key",key,128);
+	print_buf("NONCE:", &ctx->iv_buf[0] , nonce_len);
+	#endif
+
+	if ( auth_mode_key_len[hash][mode] > auth_keylen)
+		memset(auth_key + auth_keylen, 0,  auth_mode_key_len[hash][mode] - auth_keylen);
+	if(cipher_mode_iv_len[cipher_alg][NLM_CIPHER_MODE_CTR] > 0)
+		ctx->iv_len = cipher_mode_iv_len[cipher_alg][NLM_CIPHER_MODE_CTR];
+
+	auth_keylen = auth_mode_key_len[hash][mode];
+
+	#ifdef 	NLM_CRYPTO_DEBUG
+	print_buf("ENC_KEY:", cipher_key, cipher_keylen);
+	print_buf("AUTH_KEY:", auth_key, auth_keylen);
+	#endif
+
+	ret =  nlm_crypto_fill_pkt_ctrl(ctrl, hmac, hash,
+		mode, cipher_alg, NLM_CIPHER_MODE_CTR, 0, cipher_key,
+		cipher_keylen, auth_key, auth_keylen);
+
+	return ret;
+}
 static int  xlp_3des_setkey(struct crypto_aead *tfm, u8 *key, unsigned int keylen, int hash, int mode,uint16_t h_stat )
 {
 	struct nlm_aead_ctx *ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
-	struct nlm_aead_ctx * nlm_ctx = (struct  nlm_aead_ctx *)(( unsigned long )(( uint8_t *)ctx + CTRL_DESC_SIZE + 63) & ~(0x3f));
+	struct nlm_aead_ctx * nlm_ctx = (struct  nlm_aead_ctx *)(( unsigned long )(( uint8_t *)ctx + CTRL_DESC_SIZE + 64) & ~(0x3fUL));
         unsigned int cipher_keylen=0, auth_keylen=0;
 	struct nlm_crypto_pkt_ctrl *ctrl = &ctx->ctrl;
         int ret;
@@ -312,15 +427,23 @@ static int  xlp_3des_setkey(struct crypto_aead *tfm, u8 *key, unsigned int keyle
 		mode, cipher_alg,NLM_CIPHER_MODE_CBC,0,cipher_key,
 		 cipher_keylen, auth_key, auth_keylen);
 	
-	memcpy(d_key,&cipher_key[16],8);
-        memcpy(&d_key[1],&cipher_key[8],8);
-        memcpy(&d_key[2],&cipher_key[0],8);
+	nlm_ctx->stat = TDES_CBC_STAT | h_stat << 8;;
+	if( CHIP_SUPPORTS(DES3_KEY_SWAP) ) {
+		memcpy((uint8_t *)d_key,cipher_key,24);
+	}
+	else {
+		memcpy(d_key,&cipher_key[16],8);
+		memcpy(&d_key[1],&cipher_key[8],8);
+		memcpy(&d_key[2],&cipher_key[0],8);
+	}
 	ret =  nlm_crypto_fill_pkt_ctrl(&nlm_ctx->ctrl, hmac, hash,
-		mode, cipher_alg, NLM_CIPHER_MODE_CBC, 0, ( unsigned char * )d_key,
-		cipher_keylen, auth_key, auth_keylen);
+			mode, cipher_alg, NLM_CIPHER_MODE_CBC, 0, ( unsigned char * )d_key,
+			cipher_keylen, auth_key, auth_keylen);
+	#ifdef NLM_CRYPTO_DEBUG 
 	print_buf("ENC_KEY:", cipher_key, cipher_keylen);
 	print_buf("AUTH_KEY:", auth_key, auth_keylen);
 	print_buf("DECRY_KEY",(unsigned char * )&d_key[0],cipher_keylen);
+	#endif
 
         return ret;
 }
@@ -332,7 +455,6 @@ static int xlp_des_setkey( struct crypto_aead *tfm, uint8_t  *key, unsigned int
 	struct nlm_crypto_pkt_ctrl *ctrl = &ctx->ctrl;
         int ret;
 	uint8_t auth_key[128];
-	uint64_t d_key[3] ;
 	int cipher_alg = NLM_CIPHER_DES;
 	struct rtattr *rta = (struct rtattr *)key;
 	uint8_t *cipher_key;
@@ -361,14 +483,105 @@ static int xlp_des_setkey( struct crypto_aead *tfm, uint8_t  *key, unsigned int
 		mode, cipher_alg,NLM_CIPHER_MODE_CBC,0,cipher_key,
 		 cipher_keylen, auth_key, auth_keylen);
 	
+	#ifdef NLM_CRYPTO_DEBUG 
 	print_buf("ENC_KEY:", cipher_key, cipher_keylen);
 	print_buf("AUTH_KEY:", auth_key, auth_keylen);
-	print_buf("DECRY_KEY",(unsigned char *)&d_key[0],cipher_keylen);
+	#endif
 
         return ret;
 
 
 }
+static int aead_gcm_rfc4106_setkey( struct crypto_aead *tfm, const u8 *key,
+						unsigned int keylen)
+{
+	struct nlm_aead_ctx *ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+	int cipher_alg;
+	unsigned int cipher_keylen=0;
+	int auth_alg;
+	int ret;
+	struct nlm_crypto_pkt_ctrl *ctrl = &ctx->ctrl;
+	if (keylen < GCM_RFC4106_NONCE_SIZE)
+                return -EINVAL;
+	cipher_keylen = keylen - GCM_RFC4106_NONCE_SIZE;
+	cipher_alg = get_cipher_aes_algid(cipher_keylen);
+	ctx->stat = cipher_alg - 1;
+	ctx->stat = ctx->stat | (GCM_STAT << 8);
+
+	if (cipher_alg < 0) {
+		crypto_aead_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+		//sandip: check return val
+		printk("ERR: Bad aes key len\n");
+		return -EINVAL;
+	}
+	auth_alg = get_auth_aes_algid(cipher_keylen);
+	if ( auth_alg < 0) {
+		crypto_aead_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+		printk("ERR: Bad aes key len\n");
+		return -EINVAL;
+	}
+	
+	ret = nlm_crypto_fill_pkt_ctrl(ctrl,0,auth_alg,NLM_HASH_MODE_GCM,cipher_alg,
+		NLM_CIPHER_MODE_GCM,0,(u8*)key,cipher_keylen,(u8*)key,cipher_keylen);
+	ctx->iv_len = cipher_mode_iv_len[cipher_alg][NLM_HASH_MODE_GCM];
+#ifdef	NLM_CRYPTO_DEBUG
+	print_buf("KEY\n",key,cipher_keylen);	
+#endif
+	
+	/*copy by nonce*/
+	memcpy(ctx->iv_buf, key + cipher_keylen, GCM_RFC4106_NONCE_SIZE);
+	crypto_aead_setkey(ctx->fallback, key, keylen);
+
+	return ret ;
+
+}
+
+static int aead_ccm_rfc4309_setkey(struct crypto_aead *tfm, const u8 *key, 
+				   unsigned int keylen)
+{
+	struct nlm_aead_ctx *ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+	unsigned int cipher_keylen=0;
+	int ret;
+	int nonce_len = CCM_RFC4309_NONCE_SIZE;
+	int cipher_alg;
+	int auth_alg;
+	struct nlm_crypto_pkt_ctrl *ctrl = &ctx->ctrl;
+	if (keylen < CCM_RFC4309_NONCE_SIZE)
+                return -EINVAL;
+
+	cipher_keylen = keylen - nonce_len;
+	cipher_alg = get_cipher_aes_algid(cipher_keylen);
+	ctx->stat = cipher_alg - 1;
+	ctx->stat = ctx->stat | (CCM_STAT << 8);
+
+	cipher_alg = get_cipher_aes_algid(cipher_keylen);
+	if (cipher_alg < 0) {
+		crypto_aead_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+		//sandip: check return val
+		printk("ERR: Bad aes key len\n");
+		return -EINVAL;
+	}
+	auth_alg = get_auth_aes_algid(cipher_keylen);
+	if ( auth_alg < 0) {
+		crypto_aead_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+		printk("ERR: Bad aes key len\n");
+		return -EINVAL;
+	}
+	
+
+	
+	ret = nlm_crypto_fill_pkt_ctrl(ctrl,0,auth_alg,NLM_HASH_MODE_CCM,cipher_alg,NLM_CIPHER_MODE_CCM,0,(u8*)key,cipher_keylen,(u8*)key,cipher_keylen);
+	ctx->iv_len = cipher_mode_iv_len[cipher_alg][NLM_HASH_MODE_GCM];
+#ifdef	NLM_CRYPTO_DEBUG
+	print_buf("KEY\n",key,cipher_keylen);	
+#endif
+	
+	/*copy by nonce*/
+	memcpy(ctx->iv_buf, key + cipher_keylen, nonce_len);
+	crypto_aead_setkey(ctx->fallback, key, keylen);
+	return ret;
+}
+
 static int xlp_aes_cbc_hmac_sha256_setkey( struct crypto_aead *tfm, const u8 *key, unsigned int keylen)
 {
 	return xlp_aes_cbc_setkey(tfm, (uint8_t *)key, keylen,NLM_HASH_SHA,NLM_HASH_MODE_SHA256,H_SHA256_STAT);
@@ -436,178 +649,570 @@ static int xlp_des_cbc_hmac_md5_setkey( struct crypto_aead *tfm, const u8 *key,
 
 }
 
+static  int xlp_aes_ctr_hmac_sha256_setkey ( struct crypto_aead *tfm, const u8 *key,
+					unsigned int keylen)
+{
+	return xlp_aes_ctr_setkey(tfm, (uint8_t *)key, keylen,NLM_HASH_SHA,NLM_HASH_MODE_SHA256,H_SHA256_STAT);
+}
+static  int xlp_aes_ctr_hmac_sha1_setkey ( struct crypto_aead *tfm, const u8 *key,
+					unsigned int keylen)
+{
+	return xlp_aes_ctr_setkey(tfm, (uint8_t *)key, keylen,NLM_HASH_SHA,NLM_HASH_MODE_SHA1,H_SHA1_STAT);
+
+}
+static  int xlp_aes_ctr_aes_xcbc_mac_setkey ( struct crypto_aead *tfm, const u8 *key,
+					unsigned int keylen)
+{
+	return xlp_aes_ctr_setkey(tfm, (uint8_t *)key, keylen,NLM_HASH_AES128,NLM_HASH_MODE_XCBC,AES128_XCBC_STAT);
+}
+
+static int xlp_aes_ctr_hmac_md5_setkey(struct crypto_aead *tfm, const u8 *key,
+					unsigned int keylen)
+{
+	return xlp_aes_ctr_setkey(tfm, (uint8_t *)key, keylen,NLM_HASH_MD5,0,MD5_STAT);
+}
 //returns nr_aad_frags... -1 for error
-int fill_aead_aad(struct nlm_crypto_pkt_param *param, struct aead_request *req, unsigned int aad_len,int seg)
+unsigned int fill_aead_aad(struct nlm_crypto_pkt_param *param, struct aead_request *req, unsigned int aad_len,int seg,int max_frags)
 {
 	struct scatterlist *sg;
 	struct scatter_walk walk;
 	int len;
 	uint8_t *virt;
+	int rv = 0;
 	
-	for (sg = req->assoc; aad_len > 0; sg = scatterwalk_sg_next(sg), 
-			seg++) {
+	for (sg = req->assoc; aad_len > 0; sg = scatterwalk_sg_next(sg) ) {
 
 		len = min(aad_len, sg->length);
 		scatterwalk_start(&walk, sg);
 		//virt = scatterwalk_map(&walk, 1);
 		virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
-		nlm_crypto_fill_src_seg(param, seg, virt, len);
-		nlm_crypto_fill_dst_seg(param, seg, virt, len);
+		rv = nlm_crypto_fill_src_dst_seg(param, seg, max_frags, virt, len);
+		if ( rv <= seg ) 
+			return nlm_crypto_calc_rem_len(sg,aad_len) + max_frags;
+		else 
+			seg = rv;
 		aad_len -= len;
 	}
 	return seg;
 }
+		
+/*
+   Generic Encrypt / Decrypt Function
+ */
+//op is either encrypt or decrypt
 
-int fill_aead_crypt(struct aead_request *req, unsigned int cipher_len, 
-		struct nlm_crypto_pkt_param *param, unsigned char **actual_tag, int op, int seg)
+static void aead_request_callback(struct nlm_async_crypto *async, uint64_t msg1)
 {
-	struct scatterlist *sg;
-	struct scatter_walk walk;
-	unsigned int len = 0;
-	uint8_t *virt = NULL;
-	int nr_src_frags = 0;
-	int nr_dst_frags = 0;
-	int passed_len, i;
-	int index = 0;
-
-	if (req->src == req->dst) {
-		for (sg = req->src; cipher_len > 0; sg = scatterwalk_sg_next(sg), index++) {
-			len = min(cipher_len, sg->length);
-			scatterwalk_start(&walk, sg);
-			virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
-			nlm_crypto_fill_src_seg(param, seg + index, virt, len);
-			nlm_crypto_fill_dst_seg(param, seg + index, virt, len);
-			cipher_len -= len;
+	struct crypto_async_request *base = (struct crypto_async_request *)async->args;
+	int err = 0;
+	int cpu = hard_smp_processor_id();
+	int enc = async->stat & 0xff;
+	int auth = (async->stat >> 8 ) & 0xff;
+
+	if (msg1 & 0x7ff80) {
+		printk("\n Error: entry1 is %llx",msg1);
+		err = -EIO;
+		base->complete(base, err);
+		return;
+	}
+	if (async->op){
+		scatterwalk_map_and_copy(async->hash_addr, async->dst, async->bytes, async->authsize, 1);
+	}else{
+		char authtag[64];
+		scatterwalk_map_and_copy(authtag, async->src, async->bytes-async->authsize, async->authsize, 0);
+		if(memcmp(authtag, async->hash_addr, async->authsize)){
+			err = -EBADMSG;
 		}
-		*actual_tag = virt + len;
-		return index;
 	}
-	passed_len = cipher_len;
+	crypto_stat[cpu].enc[enc]++;
+	crypto_stat[cpu].auth[auth]++;	
+	crypto_stat[cpu].enc_tbytes[enc] += async->bytes;
+	crypto_stat[cpu].auth_tbytes[auth] += async->bytes;
+	if ( async->pkt_param)
+		kfree(async->pkt_param);
+	base->complete(base, err);
+	return;
+}
 
-	for (sg = req->src, index = 0; cipher_len > 0; sg = scatterwalk_sg_next(sg),
-	     nr_src_frags++, index++) {
-		len = min(cipher_len, sg->length);
-		scatterwalk_start(&walk, sg);
-		virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
-		nlm_crypto_fill_src_seg(param, seg + index, virt, len);
-		cipher_len -= len;
+static int aead_crypt_gcm(struct aead_request *req, unsigned int op)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_alg *alg = tfm->base.__crt_alg;
+	struct aead_alg *aead= &alg->cra_aead;
+	struct nlm_aead_ctx *ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+	struct nlm_crypto_pkt_param *param;
+	unsigned int cipher_off, auth_off, iv_off;
+	unsigned int auth_len, cipher_len;
+	int seg = 0,iv_size =16;
+	unsigned int hash_source;
+	uint64_t entry0, entry1, tx_id=0x12345678;
+	struct nlm_async_crypto *async =  (struct nlm_async_crypto *)NLM_ASYNC_PTR_PARAM_OFFSET(aead_request_ctx(req));
+	uint8_t *hash_addr;
+	int fb_vc; 
+	int err=0;
+	unsigned int authsize,maxauthsize;
+	struct nlm_crypto_pkt_ctrl *ctrl = &ctx->ctrl;
+	char * iv = (uint8_t *)NLM_IV_OFFSET(aead_request_ctx(req)); 
+	uint8_t *tmp_iv = iv;
+#ifdef CONFIG_32BIT
+	unsigned long msgrng_flags;
+#endif
+	unsigned int max_frags  = MAX_FRAGS;
+	int node_sae_base;
+	int node;
+
+	authsize = crypto_aead_authsize(tfm);
+	maxauthsize= aead->maxauthsize;
+	async->pkt_param = NULL;
+	async->src = req->src;
+	async->dst = req->dst;
+
+	if ( (op &&  (req->cryptlen == 0 )) || (!op && req->cryptlen <= aead->maxauthsize))
+	{
+		int ret =0;
+		ret = crypto_aead_setauthsize(ctx->fallback, authsize);
+		aead_request_set_tfm(req, ctx->fallback);
+		if ( op )
+		ret = crypto_aead_encrypt(req);
+		else 
+		ret = crypto_aead_decrypt(req);
+		aead_request_set_tfm(req,tfm);
+		
+		return ret;
 	}
+	
+	
 
-	if (op == NETL_OP_ENCRYPT)
-		*actual_tag = virt + len;
+	/*Copy nonce*/
+	memcpy(tmp_iv,  ctx->iv_buf, GCM_RFC4106_NONCE_SIZE);
+	tmp_iv += GCM_RFC4106_NONCE_SIZE;
 
-	cipher_len = passed_len;
+	/*Copy IV*/
+	memcpy(tmp_iv, req->iv, GCM_RFC4106_IV_SIZE);
+	tmp_iv += GCM_RFC4106_IV_SIZE;
 
-	for (sg = req->dst, index=0; cipher_len > 0; sg = scatterwalk_sg_next(sg),
-	     nr_dst_frags++, index++) {
-		len = min(cipher_len, sg->length);
-		scatterwalk_start(&walk, sg);
-		virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
-		nlm_crypto_fill_dst_seg(param, seg + index, virt, len);
-		cipher_len -= len;
+	/*Set counter*/
+	*((uint32_t*)tmp_iv) = cpu_to_be32((uint32_t)1);
+
+	param = (struct nlm_crypto_pkt_param *)NLM_CRYPTO_PKT_PARAM_OFFSET(aead_request_ctx(req));
+	hash_addr = (uint8_t *)NLM_HASH_OFFSET(aead_request_ctx(req));
+
+	iv_off = 0;
+	auth_off = iv_size;
+	cipher_off = req->assoclen + iv_size;
+
+	//check if it should be aip->tag_len or can be taken from tfm
+	cipher_len = op ? req->cryptlen: req->cryptlen - authsize;
+	auth_len = req->assoclen + cipher_len; 
+	hash_source = (op) ? 0 : 1;
+
+	/*Setup NONCE_IV_CTR COMBO*/
+	seg = nlm_crypto_fill_src_dst_seg(param, seg, max_frags, iv, iv_size);
+
+	do { 
+	
+	/*Setup AAD - SPI/SEQ Number*/
+	seg = fill_aead_aad(param, req, req->assoclen,seg,max_frags);
+	seg = fill_src_dst_sg(req->src,req->dst, cipher_len, param, seg, max_frags,op);
+
+	if ( seg > max_frags) {
+		max_frags = seg;
+		seg = alloc_pkt_param(async,&param,max_frags);
+		if ( seg == -1 )
+			return -1;
+		seg =1 ;
 	}
+	}while(seg == 1);  
 
-	if (op == NETL_OP_DECRYPT)
-		*actual_tag = virt + len;
+	nlm_crypto_fill_cipher_auth_pkt_param(ctrl, param, op, hash_source, iv_off, 
+		iv_size, auth_off, auth_len, 0, cipher_off, cipher_len, hash_addr);
+	
+	fb_vc = crypto_get_fb_vc(&node);
 
-	if (nr_src_frags > nr_dst_frags) {
-		for (i = 0; i < nr_src_frags - nr_dst_frags; i++)
-			param->segment[seg + nr_dst_frags + i][1] = 0ULL;
-		return nr_src_frags;
-	}else{
-		if (nr_src_frags < nr_dst_frags) {
-			for (i = 0; i < nr_dst_frags - nr_src_frags; i++)
-				param->segment[seg + nr_src_frags + i][0] = 0ULL;
-		}
-		return nr_dst_frags;
+	entry0 = nlm_crypto_form_pkt_fmn_entry0(fb_vc, 0, 0, ctrl->cipherkeylen, virt_to_phys(ctrl));
+	entry1 = nlm_crypto_form_pkt_fmn_entry1(0, ctrl->hashkeylen, (seg + 2)<<4, virt_to_phys(param));
+
+
+#ifdef NLM_CRYPTO_DEBUG
+	printk("*****ctrl descriptor phys addr %#lx\n",virt_to_phys(ctrl));
+	print_crypto_msg_desc(entry0, entry1, tx_id);
+	print_cntl_instr(ctrl->desc0);
+	print_buf("ENC_KEY:", (uint8_t *)ctrl->key, ctrl->cipherkeylen);
+	print_buf("AUTH_KEY:", (uint8_t *)ctrl->key + ctrl->cipherkeylen, ctrl->hashkeylen);
+	print_pkt_desc(param,seg);
+	printk("iv_off: %d, cipher_off: %d, auth_off: %d\n",iv_off, cipher_off, auth_off);
+	printk("auth_len: %d, cipher_len: %d\n", auth_len, cipher_len);
+#endif
+
+	async->callback =  aead_request_callback;
+	async->args = (void *)&req->base; 
+	async->op  = op;
+	async->hash_addr = hash_addr;
+	async->authsize = authsize;
+	async->stat = ctx->stat; 
+	async->bytes = req->cryptlen; 
+	tx_id = (uint64_t)(unsigned long)async;
+
+	//construct pkt, send to engine and receive reply
+#ifdef CONFIG_32BIT
+	msgrng_access_enable(msgrng_flags);
+#endif
+	node_sae_base = (node << NODE_BASE_SHIFT_BIT) | crypto_vc_base;
+	err = nlm_hal_send_msg3(node_sae_base, 0 /*code */ , entry0, entry1, tx_id);
+#ifdef CONFIG_32BIT
+	msgrng_access_disable(msgrng_flags);
+#endif
+	if(err){
+		return -EAGAIN;
 	}
-}
 
-/*
-   Generic Encrypt / Decrypt Function
- */
-//op is either encrypt or decrypt
 
-#if 1
-static inline void ncd_block_fast_send(unsigned int dest)
-{
-	int success;
-        __asm__ volatile (".set push\n"
-                          ".set noreorder\n"
-                          ".set arch=xlp\n"
-                          "sync\n"
-                          "1: msgsnds %0, %1\n"
-			  "beqz %0, 1b\n"
-			  "nop\n"
-                          ".set pop\n"
-			  : "=&r"(success)
-			  : "r" (dest));
-
-        return ;
+	return -EINPROGRESS;
+
 }
 
-static inline int ncd_fast_recv_msg2(uint32_t vc, uint64_t *msg0, uint64_t *msg1)
+static int aead_crypt_ccm(struct aead_request *req, unsigned int op)
 {
-	if (!xlp_receive(vc))
-		return -1;
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_alg *alg = tfm->base.__crt_alg;
+	struct aead_alg *aead= &alg->cra_aead;
+	struct nlm_aead_ctx *ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+	struct nlm_crypto_pkt_param *param;
+	unsigned int cipher_off, auth_off, iv_off;
+	unsigned int auth_len, cipher_len;
+	unsigned int hash_source;
+	uint64_t entry0, entry1, tx_id=0x12345678;
+	struct nlm_async_crypto *async =  (struct nlm_async_crypto *)NLM_ASYNC_PTR_PARAM_OFFSET(aead_request_ctx(req));
+	uint8_t *hash_addr;
+	int fb_vc; 
+	int err=0;
+	unsigned int authsize,maxauthsize;
+	struct nlm_crypto_pkt_ctrl *ctrl = &ctx->ctrl;
+	char * iv = (uint8_t *)NLM_IV_OFFSET(aead_request_ctx(req));
+	int iv_size = 16;
+	uint8_t *auth_iv = (uint8_t *)iv + iv_size;
+	unsigned int auth_iv_frag_len = iv_size; 
+	unsigned int extralen = 0, cipher_extralen =0;
+	int seg = 0;
+	uint8_t *tmp_iv = &iv[1];
+#ifdef CONFIG_32BIT
+	unsigned long msgrng_flags;
+#endif
+	unsigned int max_frags  = MAX_FRAGS;
+	int node_sae_base;
+	int node;
 
-	*msg0 = xlp_load_rx_msg0();
-	*msg1 = xlp_load_rx_msg1();
-	return 0;
-}
+	param = (struct nlm_crypto_pkt_param *)NLM_CRYPTO_PKT_PARAM_OFFSET(aead_request_ctx(req));
+	hash_addr = (uint8_t *)NLM_HASH_OFFSET(aead_request_ctx(req));
 
-static inline void ncd_fast_send_msg3(uint32_t dst, uint64_t data0, uint64_t data1, uint64_t data2)
-{
-  unsigned int dest = 0;
+	authsize = crypto_aead_authsize(tfm);
+	maxauthsize= aead->maxauthsize;
+	async->pkt_param = NULL;
+	async->src = req->src;
+	async->dst = req->dst;
+
+	if ( (op &&  (req->cryptlen == 0 )) || (!op && req->cryptlen <= aead->maxauthsize))
+	{
+		int ret =0;
+		ret = crypto_aead_setauthsize(ctx->fallback, authsize);
+		aead_request_set_tfm(req, ctx->fallback);
+		if ( op )
+		ret = crypto_aead_encrypt(req);
+		else 
+		ret = crypto_aead_decrypt(req);
+		aead_request_set_tfm(req,tfm);
+		
+		return ret;
+	}
+	
+		
+	/*Copy nonce*/
+	memcpy(tmp_iv,  &ctx->iv_buf, CCM_RFC4309_NONCE_SIZE);
+	tmp_iv += CCM_RFC4309_NONCE_SIZE;
 
+	/*Copy IV*/
+	memcpy(tmp_iv, req->iv, CCM_RFC4309_IV_SIZE);
+	tmp_iv += CCM_RFC4309_IV_SIZE;
 
-  xlp_load_tx_msg0(data0);
-  xlp_load_tx_msg1(data1);
-  xlp_load_tx_msg2(data2);
+	/*Set counter*/
+	*((uint32_t*)tmp_iv) = (uint32_t)0;
 
-  dest = ((2 << 16) | dst);
+	memcpy(auth_iv,iv,iv_size);
 
-#ifdef MSGRING_DUMP_MESSAGES
-  nlm_hal_dbg_msg("Sending msg<%llx, %llx, %llx> to dest = %x\n", 
-	  data0, data1, data2, dest);
-#endif
+	/* Encryption iv  7            Reserved (always zero)
+	   6            Reserved (always zero)
+	   5 ... 3      Zero
+	   2 ... 0      L' ( L -1 ) ( Length of the counter ) */
+
+	iv[0] = 3;
+
+	iv_off = 0;
+	auth_off = iv_size;
+	cipher_len = op ? req->cryptlen: req->cryptlen - authsize;
+	hash_source = (op) ? 0 : 1;
+
+
+	/*Setup ENCRYPTION IV*/
+
+	/*	7            Reserved (always zero)
+		6            Adata
+		5 ... 3      M' ( (tag_len -2) /2)
+		2 ... 0      L' */
+
+	auth_iv[0] = ((req->assoclen?1 : 0 ) << 6 )| ((authsize - 2 )/2) << 3 | 3;
+
+	auth_iv_frag_len = iv_size; 
+	extralen = req->assoclen;
+	if ( req->assoclen ) {
+
+
+		if ( req->assoclen  < 65280) { 
+			*(short*)(auth_iv + 16) = cpu_to_be16((short)req->assoclen);
+			extralen += 2;
+			auth_iv_frag_len = 18;
+		}
+		else
+		{
+			*(short*)(auth_iv+ 16) = cpu_to_be16((short)0xfffe);
+			*(short*)(auth_iv + 16) = cpu_to_be16((short)0xfffe);
+			*(uint32_t*)(auth_iv + 18) = cpu_to_be32((uint32_t)req->assoclen);
+			*(uint32_t*)(auth_iv + 18) = cpu_to_be32((uint32_t)req->assoclen);
+			auth_iv_frag_len = 22;
+			extralen += 6;
+		}
+	}
+	/*Setup AUTH IV*/
+	*(uint32_t*)&auth_iv[12] |= cpu_to_be32((uint32_t )cipher_len);
+
+
+	//one for cipher iv one for auth iv
+	cipher_off = iv_size + iv_size + extralen;  
+
+	if ( req->assoclen ) {
+		/* AAD has to be block aligned */
+		extralen = extralen % 16;
+		if ( extralen ) {
+			extralen = AES_BLOCK_SIZE - extralen;
+			memset(auth_iv + 22,0,extralen);
+			cipher_off += extralen;
+		}
+	}
+
+	cipher_extralen = cipher_len % 16;
+
+	if ( cipher_extralen ) {
+		cipher_extralen = AES_BLOCK_SIZE - cipher_extralen;
+		memset(auth_iv + 38,0,cipher_extralen);
+	}
+	auth_len = cipher_off + cipher_len + cipher_extralen - auth_off; 
+
+	seg = nlm_crypto_fill_src_dst_seg(param, seg, max_frags, iv, iv_size);
+
+	do {
+	auth_iv = (uint8_t *)iv + iv_size;
+	seg = nlm_crypto_fill_src_dst_seg(param, seg,max_frags, auth_iv,auth_iv_frag_len);
+	if ( req->assoclen ) {
+		/*Setup AAD - SPI/SEQ Number*/
+		seg = fill_aead_aad(param, req, req->assoclen,seg,max_frags);
+		if ( extralen ) 
+			seg = nlm_crypto_fill_src_dst_seg(param,seg,max_frags, (auth_iv+22),extralen);
+		
+	}
+	seg = fill_src_dst_sg(req->src,req->dst, cipher_len, param,seg, max_frags,op);
+
+
+	if ( cipher_extralen ) {
+		if ( seg >= max_frags ){
+			seg+=  nlm_crypto_sae_num_seg_reqd(auth_iv+38, cipher_extralen);
+		}
+		else 
+			seg = nlm_crypto_fill_src_dst_seg(param,seg,max_frags, (auth_iv+38),cipher_extralen);
+	}
 	
-  ncd_block_fast_send(dest);
 
-  return;
-}
+	if ( seg > max_frags) {
+		max_frags = seg;
+		seg = alloc_pkt_param(async,&param,max_frags);
+		if ( seg == -1 ) {
+			return -1;
+		}
+	}
+	}while(seg == 1);  
+
+	nlm_crypto_fill_cipher_auth_pkt_param(ctrl, param, op, hash_source, iv_off,
+			iv_size, auth_off, auth_len, 0, cipher_off, cipher_len, hash_addr);
+
+	fb_vc = crypto_get_fb_vc(&node);
+	entry0 = nlm_crypto_form_pkt_fmn_entry0(fb_vc, 0, 0, ctrl->cipherkeylen, virt_to_phys(ctrl))
+;
+	entry1 = nlm_crypto_form_pkt_fmn_entry1(0, ctrl->hashkeylen, (seg + 2)<<4, virt_to_phys(param));
+
+
+
+#ifdef NLM_CRYPTO_DEBUG
+	printk("*****ctrl descriptor phys addr %#lx\n",virt_to_phys(ctrl));
+	print_crypto_msg_desc(entry0, entry1, tx_id);
+	print_cntl_instr(ctrl->desc0);
+	print_buf("ENC_KEY:", (uint8_t *)ctrl->key, ctrl->cipherkeylen);
+	print_buf("AUTH_KEY:", (uint8_t *)ctrl->key + ctrl->cipherkeylen, ctrl->hashkeylen);
+	print_pkt_desc(param,seg);
+	printk("iv_off: %d, cipher_off: %d, auth_off: %d\n",iv_off, cipher_off, auth_off);
+	printk("auth_len: %d, cipher_len: %d\n", auth_len, cipher_len);
 #endif
 
-static void aead_request_callback(struct nlm_async_crypto *async, uint64_t msg1)
+
+	async->callback =  aead_request_callback;
+	async->args = (void *)&req->base; 
+	async->op  = op;
+	async->hash_addr = hash_addr;
+	async->authsize = authsize;
+	async->stat = ctx->stat; 
+	async->bytes = req->cryptlen; 
+	tx_id = (uint64_t)(unsigned long)async;
+
+	//construct pkt, send to engine and receive reply
+#ifdef CONFIG_32BIT
+	msgrng_access_enable(msgrng_flags);
+#endif
+	node_sae_base = (node << NODE_BASE_SHIFT_BIT) | crypto_vc_base;
+	err = nlm_hal_send_msg3(node_sae_base, 0 /*code */ , entry0, entry1, tx_id);
+#ifdef CONFIG_32BIT
+	msgrng_access_disable(msgrng_flags);
+#endif
+	if(err){
+		return -EAGAIN;
+	}
+
+
+	return -EINPROGRESS;
+
+
+}
+static int aead_crypt_ctr(struct aead_request *req, unsigned int op)
 {
-	struct crypto_async_request *base = (struct crypto_async_request *)async->args;
-	int err = 0;
-	int cpu = nlm_processor_id();
-	int enc = async->stat & 0xff;
-	int auth = (async->stat >> 8 ) & 0xff;
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_alg *alg = tfm->base.__crt_alg;
+	struct aead_alg *aead= &alg->cra_aead;
+	struct nlm_aead_ctx *ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+	struct nlm_crypto_pkt_param *param;
+	unsigned int cipher_off, iv_off;
+	unsigned int auth_len, cipher_len, auth_off;
+	int seg = 0,ivsize;
+	unsigned int hash_source;
+	uint64_t entry0, entry1;
+	uint64_t tx_id=0x12345678;
+	void  * addr = aead_request_ctx(req);
+	struct nlm_async_crypto *async =  (struct nlm_async_crypto *)NLM_ASYNC_PTR_PARAM_OFFSET(aead_request_ctx(req));
+	uint8_t *hash_addr;
+	int fb_vc; 
+	int err=0;
+	unsigned int authsize,maxauthsize;
+	uint8_t *iv = NULL; 
+	struct nlm_crypto_pkt_ctrl *ctrl = &ctx->ctrl;
+#ifdef CONFIG_32BIT
+	unsigned long msgrng_flags;
+#endif
+	unsigned int max_frags  = MAX_FRAGS;
+	int node_sae_base;
+	int node;
 
-	if (msg1 & 0x7ff80) {
-		printk("\n Error: entry1 is %llx",msg1);
-		err = -EIO;
-		base->complete(base, err);
-		return;
+	authsize = crypto_aead_authsize(tfm);
+	maxauthsize= aead->maxauthsize;
+	async->pkt_param = NULL;
+	async->src = req->src;
+	async->dst = req->dst;
+	param = (struct nlm_crypto_pkt_param *)NLM_CRYPTO_PKT_PARAM_OFFSET(addr);
+
+	iv = (uint8_t *)NLM_IV_OFFSET(addr);
+	if ( !op ) {
+		 uint8_t *tmp_iv = iv;
+		/*Copy nonce*/
+		memcpy(tmp_iv,  ctx->iv_buf, CTR_RFC3686_NONCE_SIZE);
+		tmp_iv += CTR_RFC3686_NONCE_SIZE;
+
+		memcpy(tmp_iv, req->iv, CTR_RFC3686_IV_SIZE);
+		tmp_iv += CTR_RFC3686_IV_SIZE;
+
+		/*Set counter*/
+		*((uint32_t *)tmp_iv) = cpu_to_be32((uint32_t)1);
 	}
-	if (async->op){
-		memcpy(async->actual_tag, async->hash_addr, async->authsize);
-	}else{
-		if(memcmp(async->actual_tag, async->hash_addr, async->authsize)){
-			err = -EBADMSG;
-			printk("TAG MISMATCH!!!\n");
-		}
+
+	hash_addr = (uint8_t *)NLM_HASH_OFFSET(addr);
+
+	ivsize = crypto_aead_ivsize(crypto_aead_reqtfm(req));
+
+	auth_off = CTR_RFC3686_BLOCK_SIZE;
+	cipher_off = auth_off + req->assoclen + ivsize;
+	iv_off = 0;
+
+	//check if it should be aip->tag_len or can be taken from tfm
+	cipher_len = op ? req->cryptlen:req->cryptlen - authsize;
+	auth_len = cipher_off - auth_off + cipher_len;
+	hash_source = op;
+	seg = nlm_crypto_fill_src_dst_seg(param, seg, max_frags, iv, CTR_RFC3686_BLOCK_SIZE);
+
+
+	do {
+	seg = fill_aead_aad(param, req, req->assoclen,seg,max_frags);
+
+	seg = 	nlm_crypto_fill_src_dst_seg(param, seg, max_frags, iv+CTR_RFC3686_NONCE_SIZE, ivsize);
+
+	seg = fill_src_dst_sg(req->src,req->dst, cipher_len, param, seg, max_frags,op);
+
+	if ( seg > max_frags) {
+		max_frags = seg;
+		seg = alloc_pkt_param(async,&param,max_frags);
+		if ( seg == -1 )
+			return -1;
+		seg = 1;
 	}
-	crypto_stat[cpu].enc[enc]++;
-	crypto_stat[cpu].auth[auth]++;	
-	crypto_stat[cpu].enc_tbytes[enc] += async->bytes;
-	crypto_stat[cpu].auth_tbytes[auth] += async->bytes;
-	base->complete(base, err);
-	return;
-}
+	}while(seg == 1);  
+
+	nlm_crypto_fill_cipher_auth_pkt_param(ctrl, param, op, op, iv_off, 
+			CTR_RFC3686_BLOCK_SIZE, auth_off, auth_len, 0, cipher_off, cipher_len, hash_addr);
+	
+	fb_vc = crypto_get_fb_vc(&node);
+
+	entry0 = nlm_crypto_form_pkt_fmn_entry0(fb_vc, 0, 0, ctrl->cipherkeylen, virt_to_phys(ctrl));
+	entry1 = nlm_crypto_form_pkt_fmn_entry1(0, ctrl->hashkeylen, (seg + 2)<<4, virt_to_phys(param));
+
+
+#ifdef NLM_CRYPTO_DEBUG
+	printk("*****ctrl descriptor phys addr %#lx\n",virt_to_phys(ctrl));
+	print_crypto_msg_desc(entry0, entry1, tx_id);
+	print_pkt_desc(param,seg);
+	print_cntl_instr(ctrl->desc0);
+	print_buf("ENC_KEY:", (uint8_t *)ctrl->key, ctrl->cipherkeylen);
+	print_buf("AUTH_KEY:", (uint8_t *)ctrl->key + ctrl->cipherkeylen, ctrl->hashkeylen);
+	print_buf("IV ",iv,16);
+	printk("iv_off: %d, cipher_off: %d, auth_off: %d\n",iv_off, cipher_off, auth_off);
+	printk("auth_len: %d, cipher_len: %d\n", auth_len, cipher_len);
+#endif
 
+	async->callback =  aead_request_callback;
+	async->args = (void *)&req->base; 
+	async->op  = op;
+	async->hash_addr = hash_addr;
+	async->authsize = authsize;
+	async->stat = ctx->stat;
+	async->bytes = req->cryptlen;
+	tx_id = (uint64_t)(unsigned long)async;
+
+	//construct pkt, send to engine and receive reply
+#ifdef CONFIG_32BIT
+	msgrng_access_enable(msgrng_flags);
+#endif
+	node_sae_base = (node << NODE_BASE_SHIFT_BIT) | crypto_vc_base;
+	err = nlm_hal_send_msg3(node_sae_base, 0 /*code */ , entry0, entry1, tx_id);
+#ifdef CONFIG_32BIT
+	msgrng_access_disable(msgrng_flags);
+#endif
+	if(err){
+		return -EAGAIN;
+	}
+
+	return -EINPROGRESS;
+}
 
 static int aead_crypt_3des(struct aead_request *req, unsigned int op)
 {
@@ -615,28 +1220,34 @@ static int aead_crypt_3des(struct aead_request *req, unsigned int op)
 	struct crypto_alg *alg = tfm->base.__crt_alg;
 	struct aead_alg *aead= &alg->cra_aead;
 	struct nlm_aead_ctx *ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
-	struct aead_tfm *crt = crypto_aead_crt(tfm);
 	struct nlm_crypto_pkt_param *param;
 	unsigned int cipher_off, iv_off;
 	unsigned int auth_len, cipher_len, auth_off;
-	unsigned char *actual_tag;
-	int seg =0, nr_enc_frags, ivsize;
-	unsigned int hash_source, nr_aad_frags;
+	int seg =0,  ivsize;
+	unsigned int hash_source;
 	uint64_t entry0, entry1;
 	uint64_t tx_id=0x12345678;
-	struct nlm_async_crypto *async = NULL;
+	struct nlm_async_crypto *async =  (struct nlm_async_crypto *)NLM_ASYNC_PTR_PARAM_OFFSET(aead_request_ctx(req));
 	uint8_t *hash_addr;
 	int fb_vc; 
 	int err=0;
 	unsigned int authsize,maxauthsize;
-	uint8_t *new_iv_ptr_lo = NULL;
-	uint8_t *new_iv_ptr_hi = NULL;
 	struct nlm_crypto_pkt_ctrl *ctrl = NULL;
-	ctx = (struct  nlm_aead_ctx *)(( unsigned long )(( uint8_t *)ctx + CTRL_DESC_SIZE + 63) & ~(0x3f));
+#ifdef CONFIG_32BIT
+	unsigned long msgrng_flags;
+#endif
+	unsigned int max_frags  = MAX_FRAGS;
+	int node_sae_base;
+	int node;
+
+	ctx = (struct  nlm_aead_ctx *)(( unsigned long )(( uint8_t *)ctx + CTRL_DESC_SIZE + 64) & ~(0x3fUL));
 	ctrl = &ctx->ctrl;
 	
-	authsize = crypto_aead_crt(crt->base)->authsize;
+	authsize = crypto_aead_authsize(tfm);
 	maxauthsize= aead->maxauthsize;
+	async->pkt_param = NULL;
+	async->src = req->src;
+	async->dst = req->dst;
 
 	param = (struct nlm_crypto_pkt_param *)NLM_CRYPTO_PKT_PARAM_OFFSET(aead_request_ctx(req));
 	hash_addr = (uint8_t *)NLM_HASH_OFFSET(aead_request_ctx(req));
@@ -649,51 +1260,28 @@ static int aead_crypt_3des(struct aead_request *req, unsigned int op)
 	cipher_len = op ? req->cryptlen:req->cryptlen - authsize;
 	auth_len = cipher_off + cipher_len;
 	hash_source = op;
+	do {
 
-	nr_aad_frags = fill_aead_aad(param, req, req->assoclen,seg);
-	if (nr_aad_frags == 0)
-		return -1;
-	seg = nr_aad_frags;
+	seg = fill_aead_aad(param, req, req->assoclen,seg,max_frags);
 
-	if (ivsize) {
-		nlm_crypto_fill_src_seg(param, seg, req->iv, ivsize);
-		nlm_crypto_fill_dst_seg(param, seg, req->iv, ivsize);
-		seg++;
-	}
+	if (ivsize) 
+		seg = nlm_crypto_fill_src_dst_seg(param, seg, max_frags, req->iv, ivsize);
 
-	nr_enc_frags = fill_aead_crypt(req, cipher_len, param, &actual_tag, op, seg);
-
-	if (nr_enc_frags == -1)
-		return -1;
-
-	if(ctx->cbc){
-		uint32_t tmp_len;
-		uint8_t *tmp_virt;
-
-		tmp_len = (param->segment[seg][1] >> 48) + 1;
-		if(tmp_len >= 16){
-			tmp_virt = phys_to_virt((param->segment[seg][1]) & 0xffffffffffULL);
-			new_iv_ptr_hi = tmp_virt;
-			new_iv_ptr_lo = new_iv_ptr_hi + 8;;
-		}else if(tmp_len >=8){
-			tmp_virt = phys_to_virt((param->segment[seg][1]) & 0xffffffffffULL);
-			new_iv_ptr_lo = tmp_virt;
-			/*goto next frag*/
-			if(nr_enc_frags > 1){
-				tmp_virt = phys_to_virt((param->segment[seg + 1][1]) & 0xffffffffffULL);
-				tmp_len = (uint32_t)(param->segment[seg + 1][1] >> 48) + 1;
-				if(tmp_len >= 8)
-					new_iv_ptr_hi = tmp_virt + 8;
-			}
-		}
-	}
+	seg = fill_src_dst_sg(req->src,req->dst, cipher_len, param, seg, max_frags,op);
 
-	seg += nr_enc_frags;
+	if ( seg > max_frags) {
+		max_frags = seg;
+		seg = alloc_pkt_param(async,&param,max_frags);
+		if ( seg == -1 )
+			return -1;
+		seg = 0;
+	}
+	}while(seg == 0);  
 
 	nlm_crypto_fill_cipher_auth_pkt_param(ctrl, param, op, op, iv_off, 
 			ivsize, auth_off, auth_len, 0, cipher_off, cipher_len, hash_addr);
 	
-	fb_vc = crypto_get_fb_vc();
+	fb_vc = crypto_get_fb_vc(&node);
 
 	entry0 = nlm_crypto_form_pkt_fmn_entry0(fb_vc, 0, 0, ctrl->cipherkeylen, virt_to_phys(ctrl));
 	entry1 = nlm_crypto_form_pkt_fmn_entry1(0, ctrl->hashkeylen, (seg + 2)<<4, virt_to_phys(param));
@@ -710,22 +1298,26 @@ static int aead_crypt_3des(struct aead_request *req, unsigned int op)
 	printk("auth_len: %d, cipher_len: %d\n", auth_len, cipher_len);
 #endif
 
-	async = (struct nlm_async_crypto *)NLM_ASYNC_PTR_PARAM_OFFSET(aead_request_ctx(req));
 	async->callback =  aead_request_callback;
 	async->args = (void *)&req->base; 
 	async->op  = op;
-	async->actual_tag = actual_tag;
 	async->hash_addr = hash_addr;
 	async->authsize = authsize;
 	async->stat = ctx->stat; 
 	async->bytes = req->cryptlen; 
-	tx_id = (uint64_t)async;
+	tx_id = (uint64_t)(unsigned long)async;
 
 	//construct pkt, send to engine and receive reply
-	err = nlm_hal_send_msg3(crypto_vc_base, 0 /*code */ , entry0, entry1, tx_id);
+#ifdef CONFIG_32BIT
+	msgrng_access_enable(msgrng_flags);
+#endif
+	node_sae_base = (node << NODE_BASE_SHIFT_BIT) | crypto_vc_base;
+	err = nlm_hal_send_msg3(node_sae_base, 0 /*code */ , entry0, entry1, tx_id);
+#ifdef CONFIG_32BIT
+	msgrng_access_disable(msgrng_flags);
+#endif
 	if(err){
-		printk("err\n");
-		return -EIO;
+		return -EAGAIN;
 	}
 
 
@@ -736,27 +1328,32 @@ static int aead_crypt(struct aead_request *req, unsigned int op)
 	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
 	struct crypto_alg *alg = tfm->base.__crt_alg;
 	struct aead_alg *aead= &alg->cra_aead;
-	struct aead_tfm *crt = crypto_aead_crt(tfm);
 	struct nlm_aead_ctx *ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
 	struct nlm_crypto_pkt_param *param;
 	unsigned int cipher_off, iv_off;
 	unsigned int auth_len, cipher_len, auth_off;
-	unsigned char *actual_tag;
-	int seg =0, nr_enc_frags, ivsize;
-	unsigned int hash_source, nr_aad_frags;
+	int seg =0, ivsize;
+	unsigned int hash_source;
 	uint64_t entry0, entry1;
 	uint64_t tx_id=0x12345678;
-	struct nlm_async_crypto *async = NULL;
+	struct nlm_async_crypto *async =  (struct nlm_async_crypto *)NLM_ASYNC_PTR_PARAM_OFFSET(aead_request_ctx(req));
 	uint8_t *hash_addr;
 	int fb_vc; 
 	int err=0;
 	unsigned int authsize,maxauthsize;
-	uint8_t *new_iv_ptr_lo = NULL;
-	uint8_t *new_iv_ptr_hi = NULL;
 	struct nlm_crypto_pkt_ctrl *ctrl = &ctx->ctrl;
-	
-	authsize = crypto_aead_crt(crt->base)->authsize;
+#ifdef CONFIG_32BIT
+	unsigned long msgrng_flags;
+#endif
+	unsigned int max_frags  = MAX_FRAGS;
+	int node_sae_base;
+	int node;
+
+	authsize = crypto_aead_authsize(tfm);
 	maxauthsize= aead->maxauthsize;
+	async->pkt_param = NULL;
+	async->src = req->src;
+	async->dst = req->dst;
 
 	param = (struct nlm_crypto_pkt_param *)NLM_CRYPTO_PKT_PARAM_OFFSET(aead_request_ctx(req));
 	hash_addr = (uint8_t *)NLM_HASH_OFFSET(aead_request_ctx(req));
@@ -770,50 +1367,28 @@ static int aead_crypt(struct aead_request *req, unsigned int op)
 	auth_len = cipher_off + cipher_len;
 	hash_source = op;
 
-	nr_aad_frags = fill_aead_aad(param, req, req->assoclen,seg);
-	if (nr_aad_frags == 0)
-		return -1;
-	seg = nr_aad_frags;
+	do { 
 
-	if (ivsize) {
-		nlm_crypto_fill_src_seg(param, seg, req->iv, ivsize);
-		nlm_crypto_fill_dst_seg(param, seg, req->iv, ivsize);
-		seg++;
-	}
+	seg = fill_aead_aad(param, req, req->assoclen,seg,max_frags);
 
-	nr_enc_frags = fill_aead_crypt(req, cipher_len, param, &actual_tag, op, seg);
-
-	if (nr_enc_frags == -1)
-		return -1;
-
-	if(ctx->cbc){
-		uint32_t tmp_len;
-		uint8_t *tmp_virt;
-
-		tmp_len = (param->segment[seg][1] >> 48) + 1;
-		if(tmp_len >= 16){
-			tmp_virt = phys_to_virt((param->segment[seg][1]) & 0xffffffffffULL);
-			new_iv_ptr_hi = tmp_virt;
-			new_iv_ptr_lo = new_iv_ptr_hi + 8;;
-		}else if(tmp_len >=8){
-			tmp_virt = phys_to_virt((param->segment[seg][1]) & 0xffffffffffULL);
-			new_iv_ptr_lo = tmp_virt;
-			/*goto next frag*/
-			if(nr_enc_frags > 1){
-				tmp_virt = phys_to_virt((param->segment[seg + 1][1]) & 0xffffffffffULL);
-				tmp_len = (uint32_t)(param->segment[seg + 1][1] >> 48) + 1;
-				if(tmp_len >= 8)
-					new_iv_ptr_hi = tmp_virt + 8;
-			}
-		}
-	}
+	if (ivsize) 
+		seg = nlm_crypto_fill_src_dst_seg(param, seg, max_frags, req->iv, ivsize);
 
-	seg += nr_enc_frags;
+	seg = fill_src_dst_sg(req->src,req->dst, cipher_len, param, seg, max_frags,op);
+
+	if ( seg > max_frags) {
+		max_frags = seg;
+		seg = alloc_pkt_param(async,&param,max_frags);
+		if ( seg == -1 )
+			return -1;
+		seg = 0;
+	}
+	}while(seg == 0);  
 
 	nlm_crypto_fill_cipher_auth_pkt_param(ctrl, param, op, op, iv_off, 
 			ivsize, auth_off, auth_len, 0, cipher_off, cipher_len, hash_addr);
 	
-	fb_vc = crypto_get_fb_vc();
+	fb_vc = crypto_get_fb_vc(&node);
 
 	entry0 = nlm_crypto_form_pkt_fmn_entry0(fb_vc, 0, 0, ctrl->cipherkeylen, virt_to_phys(ctrl));
 	entry1 = nlm_crypto_form_pkt_fmn_entry1(0, ctrl->hashkeylen, (seg + 2)<<4, virt_to_phys(param));
@@ -830,28 +1405,33 @@ static int aead_crypt(struct aead_request *req, unsigned int op)
 	printk("auth_len: %d, cipher_len: %d\n", auth_len, cipher_len);
 #endif
 
-	async = (struct nlm_async_crypto *)NLM_ASYNC_PTR_PARAM_OFFSET(aead_request_ctx(req));
 	async->callback =  aead_request_callback;
 	async->args = (void *)&req->base; 
 	async->op  = op;
-	async->actual_tag = actual_tag;
 	async->hash_addr = hash_addr;
 	async->authsize = authsize;
 	async->stat = ctx->stat; 
 	async->bytes = req->cryptlen; 
-	tx_id = (uint64_t)async;
+	tx_id = (uint64_t)(unsigned long)async;
+
 
 	//construct pkt, send to engine and receive reply
-	err = nlm_hal_send_msg3(crypto_vc_base, 0 /*code */ , entry0, entry1, tx_id);
+#ifdef CONFIG_32BIT
+	msgrng_access_enable(msgrng_flags);
+#endif
+	node_sae_base = (node << NODE_BASE_SHIFT_BIT) | crypto_vc_base;
+	err = nlm_hal_send_msg3(node_sae_base, 0 /*code */ , entry0, entry1, tx_id);
+#ifdef CONFIG_32BIT
+	msgrng_access_disable(msgrng_flags);
+#endif
 	if(err){
-		printk("err\n");
-		return -EIO;
+		return -EAGAIN;
 	}
 
-
 	return -EINPROGRESS;
 }
 
+
 /*
  *  All Encrypt Functions goes here.
  */
@@ -873,6 +1453,24 @@ xlp_des_cbc_encrypt(struct aead_request *req)
 	return aead_crypt(req, NETL_OP_ENCRYPT);
 }
 
+static int
+xlp_aes_gcm_encrypt(struct aead_request *req)
+{
+	return aead_crypt_gcm(req, NETL_OP_ENCRYPT);
+}
+
+static int
+xlp_aes_ccm_encrypt(struct aead_request *req)
+{
+	return aead_crypt_ccm(req, NETL_OP_ENCRYPT);
+}
+
+
+static int 
+xlp_aes_ctr_encrypt(struct aead_request *req)
+{
+	return aead_crypt_ctr(req, NETL_OP_ENCRYPT);
+}
 
 /*
  *  All Decrypt Functions goes here.
@@ -891,6 +1489,20 @@ static int xlp_des_cbc_decrypt(struct aead_request *req)
 {
 	return aead_crypt(req, NETL_OP_DECRYPT);
 }
+
+static int xlp_aes_gcm_decrypt(struct aead_request *req)
+{
+	return aead_crypt_gcm(req, NETL_OP_DECRYPT);
+}
+
+static int xlp_aes_ccm_decrypt(struct aead_request *req)
+{
+	return aead_crypt_ccm(req, NETL_OP_DECRYPT);
+}
+static int xlp_aes_ctr_decrypt(struct aead_request *req)
+{
+	return aead_crypt_ctr(req, NETL_OP_DECRYPT);
+} 
 /*
  *  All Givencrypt Functions goes here.
  */
@@ -902,7 +1514,7 @@ int xlp_aes_cbc_givencrypt(struct aead_givcrypt_request *req)
 
 	//TODO: Get the IV from random pool
 	memcpy(req->giv, nlm_ctx->iv_buf, nlm_ctx->iv_len);
-	*(uint64_t *)req->giv += req->seq;
+	*(uint64_t *)req->giv += ccpu_to_be64(req->seq);
 	memcpy(req->areq.iv, req->giv, nlm_ctx->iv_len);
 
 	return xlp_aes_cbc_encrypt(&req->areq);
@@ -914,7 +1526,7 @@ static int xlp_3des_cbc_givencrypt(struct aead_givcrypt_request *req)
 	struct nlm_aead_ctx *nlm_ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
 
 	memcpy(req->giv, nlm_ctx->iv_buf, nlm_ctx->iv_len);
-	*(uint64_t *)req->giv += req->seq;
+	*(uint64_t *)req->giv += ccpu_to_be64(req->seq);
 	memcpy(req->areq.iv, req->giv, nlm_ctx->iv_len);
 
 	return xlp_3des_cbc_encrypt(&req->areq);
@@ -926,13 +1538,69 @@ static int xlp_des_cbc_givencrypt(struct aead_givcrypt_request *req)
 	struct nlm_aead_ctx *nlm_ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
 	
 	memcpy(req->giv, nlm_ctx->iv_buf, nlm_ctx->iv_len);
-	*(uint64_t *)req->giv += req->seq;
+	*(uint64_t *)req->giv += ccpu_to_be64(req->seq);
 	memcpy(req->areq.iv, req->giv, nlm_ctx->iv_len);
 
 	return xlp_des_cbc_encrypt(&req->areq);
 }
 
 
+static int xlp_aes_gcm_givencrypt(struct aead_givcrypt_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(&req->areq);
+	struct nlm_aead_ctx *nlm_ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+	struct aead_request *areq = &req->areq;
+	char * req_iv = areq->iv;
+	int ret;
+
+	memcpy(req->giv, nlm_ctx->iv_buf + GCM_RFC4106_NONCE_SIZE,nlm_ctx->iv_len);
+	*(uint64_t *)req->giv += ccpu_to_be64(req->seq);
+
+	areq->iv = req->giv;
+	ret = xlp_aes_gcm_encrypt(&req->areq);
+	areq->iv = req_iv;
+	return ret;
+
+}
+
+static int xlp_aes_ccm_givencrypt(struct aead_givcrypt_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(&req->areq);
+	struct nlm_aead_ctx *nlm_ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+	struct aead_request *areq = &req->areq;
+	char * req_iv = areq->iv;
+	int ret;
+
+	memcpy(req->giv, nlm_ctx->iv_buf + CCM_RFC4309_NONCE_SIZE, nlm_ctx->iv_len);
+	*(uint64_t *)req->giv += ccpu_to_be64(req->seq);
+
+	areq->iv = req->giv;
+	ret = xlp_aes_ccm_encrypt(&req->areq);
+	areq->iv = req_iv;
+	return ret;
+
+}
+
+int xlp_aes_ctr_givencrypt(struct aead_givcrypt_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(&req->areq);
+	struct nlm_aead_ctx *nlm_ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+	void *iv = (uint8_t *)NLM_IV_OFFSET(aead_request_ctx(&req->areq));
+	memcpy(req->giv, nlm_ctx->iv_buf+CTR_RFC3686_NONCE_SIZE, nlm_ctx->iv_len);
+	*(uint64_t *)req->giv += req->seq;
+
+	memcpy(iv,  nlm_ctx->iv_buf, CTR_RFC3686_NONCE_SIZE);
+	iv += CTR_RFC3686_NONCE_SIZE;
+
+	 /*Copy IV*/
+	memcpy(iv, req->giv, CTR_RFC3686_IV_SIZE);
+	iv += CTR_RFC3686_IV_SIZE;
+
+	/*Set counter*/
+	*((uint32_t *)iv) = cpu_to_be32((uint32_t)1);
+	
+	return xlp_aes_ctr_encrypt(&req->areq);
+}
 /* commented out to avoid the search time */
 static struct crypto_alg xlp_aes_cbc_hmac_sha256_cipher_auth = {
 	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
@@ -1146,6 +1814,7 @@ static struct crypto_alg xlp_des_cbc_hmac_md5_cipher_auth = {
         .cra_name = "authenc(hmac(md5),cbc(des))",
         .cra_driver_name = "authenc-hmac-md5-cbc-des-xlp",
         .cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_alignmask = 0,
         .cra_blocksize = DES_BLOCK_SIZE,
         .cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
         .cra_ctxsize = CTRL_DESC_SIZE,
@@ -1170,6 +1839,7 @@ static struct crypto_alg xlp_des_cbc_hmac_sha256_cipher_auth = {
         .cra_driver_name = "authenc-hmac-sha256-cbc-des-xlp",
         .cra_priority = XLP_CRYPT_PRIORITY,
         .cra_blocksize = DES_BLOCK_SIZE,
+	.cra_alignmask = 0,
         .cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
         .cra_ctxsize = CTRL_DESC_SIZE,
         .cra_type = &crypto_aead_type,
@@ -1192,6 +1862,7 @@ static struct crypto_alg xlp_des_cbc_hmac_sha1_cipher_auth = {
         .cra_driver_name = "authenc-hmac-sha1-cbc-des-xlp",
         .cra_priority = XLP_CRYPT_PRIORITY,
         .cra_blocksize = DES_BLOCK_SIZE,
+	.cra_alignmask = 0,
         .cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
         .cra_ctxsize = CTRL_DESC_SIZE,
         .cra_type = &crypto_aead_type,
@@ -1209,12 +1880,150 @@ static struct crypto_alg xlp_des_cbc_hmac_sha1_cipher_auth = {
                      }
 };
 
+static struct crypto_alg xlp_aes_gcm_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "rfc4106(gcm(aes))",
+	.cra_driver_name = "rfc4106-gcm-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = AES_BLOCK_SIZE,
+	.cra_alignmask = 0,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+	.cra_type = &crypto_aead_type,
+	.cra_exit = aead_session_cleanup,
+        .cra_ctxsize = CTRL_DESC_SIZE,
+        .cra_init = aead_cra_init_gcm,
+	.cra_aead = {
+		.setkey = aead_gcm_rfc4106_setkey,
+		.setauthsize = aead_setauthsize,
+		.encrypt = xlp_aes_gcm_encrypt,
+		.decrypt = xlp_aes_gcm_decrypt,
+		.givencrypt = xlp_aes_gcm_givencrypt,
+		.geniv = "seqiv", 
+		.ivsize = GCM_RFC4106_IV_SIZE,
+		.maxauthsize = GCM_RFC4106_DIGEST_SIZE,
+	}
+};
+
+static struct crypto_alg xlp_aes_ccm_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "rfc4309(ccm(aes))",
+	.cra_driver_name = "rfc4309-ccm-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = AES_BLOCK_SIZE,
+	.cra_alignmask = 0,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+	.cra_type = &crypto_aead_type,
+        .cra_ctxsize = CTRL_DESC_SIZE,
+        .cra_init = aead_cra_init_ccm,
+	.cra_aead = {
+		.setkey = aead_ccm_rfc4309_setkey,
+		.setauthsize = aead_setauthsize,
+		.encrypt = xlp_aes_ccm_encrypt,
+		.decrypt = xlp_aes_ccm_decrypt,
+		.givencrypt = xlp_aes_ccm_givencrypt,
+		.geniv = "seqiv", 
+		.ivsize = CCM_RFC4309_IV_SIZE,
+		.maxauthsize = CCM_RFC4309_DIGEST_SIZE,
+	}
+};
+
+static struct crypto_alg xlp_aes_ctr_hmac_sha1_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(sha1),rfc3686(ctr(aes)))",
+	.cra_driver_name = "authenc-hmac-sha1-ctr-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = 1,
+	.cra_alignmask = 0,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC ,
+	.cra_ctxsize = CTRL_DESC_SIZE,
+	.cra_type = &crypto_aead_type,
+	.cra_init = aead_cra_init,
+	.cra_exit = aead_session_cleanup,
+	.cra_aead = {
+		     .setkey = xlp_aes_ctr_hmac_sha1_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_aes_ctr_encrypt,
+		     .decrypt = xlp_aes_ctr_decrypt,
+		     .givencrypt = xlp_aes_ctr_givencrypt,
+		     .geniv = "seqiv", 
+		     .ivsize = CTR_RFC3686_IV_SIZE,
+		     .maxauthsize = SHA1_DIGEST_SIZE,
+		     }
+};
+
+static struct crypto_alg xlp_aes_ctr_hmac_sha256_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(sha256),rfc3686(ctr(aes)))",
+	.cra_driver_name = "authenc-hmac-sha256-ctr-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = 1,
+	.cra_alignmask = 0,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC ,
+	.cra_ctxsize = CTRL_DESC_SIZE,
+	.cra_type = &crypto_aead_type,
+	.cra_init = aead_cra_init,
+	.cra_exit = aead_session_cleanup,
+	.cra_aead = {
+		     .setkey = xlp_aes_ctr_hmac_sha256_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_aes_ctr_encrypt,
+		     .decrypt = xlp_aes_ctr_decrypt,
+		     .givencrypt = xlp_aes_ctr_givencrypt,
+		     .geniv = "seqiv", 
+		     .ivsize = CTR_RFC3686_IV_SIZE,
+		     .maxauthsize = SHA256_DIGEST_SIZE,
+		     }
+};
+static struct crypto_alg xlp_aes_ctr_aes_xcbc_mac_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(xcbc(aes),rfc3686(ctr(aes)))",
+	.cra_driver_name = "authenc-aes-xcbc-mac-ctr-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = 1,
+	.cra_alignmask = 0,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC ,
+	.cra_ctxsize = CTRL_DESC_SIZE,
+	.cra_type = &crypto_aead_type,
+	.cra_init = aead_cra_init,
+	.cra_aead = {
+		     .setkey = xlp_aes_ctr_aes_xcbc_mac_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_aes_ctr_encrypt,
+		     .decrypt = xlp_aes_ctr_decrypt,
+		     .givencrypt = xlp_aes_ctr_givencrypt,
+		     .geniv = "seqiv", 
+		     .ivsize = CTR_RFC3686_IV_SIZE,
+		     .maxauthsize = XCBC_DIGEST_SIZE,
+		     }
+};
+
+static struct crypto_alg xlp_aes_ctr_hmac_md5_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(md5),rfc3686(ctr(aes)))",
+	.cra_driver_name = "authenc-hmac-sha1-ctr-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = 1,
+	.cra_alignmask = 0,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC ,
+	.cra_ctxsize = CTRL_DESC_SIZE,
+	.cra_type = &crypto_aead_type,
+	.cra_init = aead_cra_init,
+	.cra_aead = {
+		     .setkey = xlp_aes_ctr_hmac_md5_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_aes_ctr_encrypt,
+		     .decrypt = xlp_aes_ctr_decrypt,
+		     .givencrypt = xlp_aes_ctr_givencrypt,
+		     .geniv = "seqiv", 
+		     .ivsize = CTR_RFC3686_IV_SIZE,
+		     .maxauthsize = MD5_DIGEST_SIZE,
+		     }
+};
 
 int xlp_aead_alg_init(void)
 {
 	int ret = 0;
-	int no_of_alg_registered = 0;
-	
+
 	if ((ret = crypto_register_alg(&xlp_aes_cbc_hmac_sha256_cipher_auth)))
 		goto end;
 	no_of_alg_registered++;
@@ -1258,28 +2067,53 @@ int xlp_aead_alg_init(void)
 	if ((ret =  crypto_register_alg(&xlp_des_cbc_aes_xcbc_mac_cipher_auth)))
 		goto end;
 	no_of_alg_registered++;
+	if((ret = crypto_register_alg(&xlp_aes_gcm_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+	if ((ret = crypto_register_alg(&xlp_aes_ccm_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+
+	if ((ret = crypto_register_alg(&xlp_aes_ctr_hmac_sha1_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+
+	if ((ret = crypto_register_alg(&xlp_aes_ctr_hmac_sha256_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
 
+	if ((ret = crypto_register_alg(&xlp_aes_ctr_aes_xcbc_mac_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+
+	if ((ret = crypto_register_alg(&xlp_aes_ctr_hmac_md5_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
 end:
 	return no_of_alg_registered;
 } 
 
-void
+	void
 xlp_aead_alg_fini(void)
 {
-	
-	crypto_unregister_alg(&xlp_aes_cbc_hmac_sha1_cipher_auth);
-	crypto_unregister_alg(&xlp_aes_cbc_aes_xcbc_mac_cipher_auth);
-	crypto_unregister_alg(&xlp_aes_cbc_hmac_md5_cipher_auth);
-	crypto_unregister_alg(&xlp_3des_cbc_hmac_md5_cipher_auth);
-	crypto_unregister_alg(&xlp_3des_cbc_hmac_sha256_cipher_auth);
-	crypto_unregister_alg(&xlp_3des_cbc_hmac_sha1_cipher_auth);
-	crypto_unregister_alg(&xlp_3des_cbc_aes_xcbc_mac_cipher_auth);
-	crypto_unregister_alg(&xlp_des_cbc_hmac_md5_cipher_auth);
-	crypto_unregister_alg(&xlp_des_cbc_hmac_sha256_cipher_auth);
+	crypto_unregister_alg(&xlp_aes_ctr_hmac_md5_cipher_auth);
+	crypto_unregister_alg(&xlp_aes_ctr_aes_xcbc_mac_cipher_auth);
+	crypto_unregister_alg(&xlp_aes_ctr_hmac_sha256_cipher_auth);
+	crypto_unregister_alg(&xlp_aes_ctr_hmac_sha1_cipher_auth);
+	crypto_unregister_alg(&xlp_aes_ccm_cipher_auth);
+	crypto_unregister_alg(&xlp_aes_gcm_cipher_auth);
+	crypto_unregister_alg(&xlp_des_cbc_aes_xcbc_mac_cipher_auth);
 	crypto_unregister_alg(&xlp_des_cbc_hmac_sha1_cipher_auth);
+	crypto_unregister_alg(&xlp_des_cbc_hmac_sha256_cipher_auth);
+	crypto_unregister_alg(&xlp_des_cbc_hmac_md5_cipher_auth);
+	crypto_unregister_alg(&xlp_3des_cbc_aes_xcbc_mac_cipher_auth);
+	crypto_unregister_alg(&xlp_3des_cbc_hmac_sha1_cipher_auth);
+	crypto_unregister_alg(&xlp_3des_cbc_hmac_sha256_cipher_auth);
+	crypto_unregister_alg(&xlp_3des_cbc_hmac_md5_cipher_auth);
+	crypto_unregister_alg(&xlp_aes_cbc_hmac_md5_cipher_auth);
+	crypto_unregister_alg(&xlp_aes_cbc_aes_xcbc_mac_cipher_auth);
+	crypto_unregister_alg(&xlp_aes_cbc_hmac_sha1_cipher_auth);
 	crypto_unregister_alg(&xlp_aes_cbc_hmac_sha256_cipher_auth);
-	crypto_unregister_alg(&xlp_des_cbc_aes_xcbc_mac_cipher_auth);
-	return;
 }
 
 EXPORT_SYMBOL(xlp_aead_alg_init);
diff --git a/drivers/crypto/nlm-sae/nlm_async.h b/drivers/crypto/nlm-sae/nlm_async.h
index 639805a..2dbbed0 100644
--- a/drivers/crypto/nlm-sae/nlm_async.h
+++ b/drivers/crypto/nlm-sae/nlm_async.h
@@ -24,23 +24,16 @@ THE POSSIBILITY OF SUCH DAMAGE.
 *****************************#NETL_2#********************************/
 #ifndef __NLM_ASYNC_H
 #define __NLM_ASYNC_H
-struct nlm_async_crypto;
-#define MAX_CPU 32
-extern int crypto_get_fb_vc(void);
-
-#define nlm_processor_id()                              \
-        ({ int __res;                                   \
-         __asm__ __volatile__(                          \
-                 ".set\tmips32\n\t"                     \
-                 "mfc0\t%0, $15, 1\n\t"                 \
-                 "andi\t%0, 0x1f\n\t"                   \
-                 ".set\tmips0\n\t"                      \
-                 : "=r" (__res));                       \
-         __res;                                         \
-         })
 
+#include <crypto/scatterwalk.h>
+#include "nlmcrypto.h"
 
+struct nlm_async_crypto;
+#define MAX_CPU 128
+#define NODE_ID_SHIFT_BIT 5
+#define NODE_BASE_SHIFT_BIT 10
 
+extern int crypto_get_fb_vc(int * node);
 
 struct nlm_async_crypto
 {
@@ -48,8 +41,10 @@ struct nlm_async_crypto
 	void *args;
 	int op;
 	int authsize;
-	uint8_t *actual_tag;
 	uint8_t *hash_addr;
+	uint8_t * pkt_param;
+	struct scatterlist * src;
+	struct scatterlist * dst;
 	uint16_t stat;
 	uint32_t bytes;
 };
@@ -60,6 +55,9 @@ enum enc_stat {
 	AES128_CBC_STAT ,
 	AES192_CBC_STAT ,
 	AES256_CBC_STAT, 
+	AES128_CTR_STAT,
+	AES192_CTR_STAT,
+	AES256_CTR_STAT,
 	ENC_MAX_STAT
 };
 enum {
@@ -69,6 +67,8 @@ enum {
 	AES128_XCBC_STAT,
 	AES192_XCBC_STAT,
 	AES256_XCBC_STAT,
+	GCM_STAT,
+	CCM_STAT,
 	AUTH_MAX_STAT
 };
 
@@ -82,5 +82,11 @@ struct nlm_crypto_stat
 
 extern int crypto_vc_base;
 extern int crypto_vc_limit;
+extern int nlm_crypto_sae_num_seg_reqd(void *data, unsigned int buflen);
+extern int nlm_crypto_calc_rem_len(struct scatterlist *sg, unsigned int cipher_len);
+extern int alloc_pkt_param( struct nlm_async_crypto * async,struct nlm_crypto_pkt_param ** pkt_param , int max_frags);
 
+extern int fill_src_dst_sg(struct scatterlist *src_sg, struct scatterlist *dst_sg, unsigned int cipher_len, 
+		struct nlm_crypto_pkt_param *param, 
+		int seg, unsigned int max_frags, int op);
 #endif
diff --git a/drivers/crypto/nlm-sae/nlm_auth.c b/drivers/crypto/nlm-sae/nlm_auth.c
index 4cb5b1d..442dcea 100644
--- a/drivers/crypto/nlm-sae/nlm_auth.c
+++ b/drivers/crypto/nlm-sae/nlm_auth.c
@@ -26,8 +26,8 @@ THE POSSIBILITY OF SUCH DAMAGE.
 #include <nlm_hal_fmn.h>
 #include <crypto/aes.h>
 #include <crypto/internal/hash.h>
-#include <asm/netlogic/crypto/nlmcrypto.h>
 #include "nlm_async.h"
+#include <asm/netlogic/msgring.h>
 
 #define XLP_AUTH_PRIORITY      300
 #define XLP_HMAC_PRIORITY      300
@@ -46,6 +46,7 @@ extern int auth_mode_key_len[NLM_HASH_MAX][NLM_HASH_MODE_MAX];
 
 //#define SEC_DEBUG
 
+
 #ifdef SEC_DEBUG
 #ifdef __KERNEL__
 #define debug_print(fmt, args...) printk(fmt, ##args)
@@ -60,11 +61,16 @@ extern int auth_mode_key_len[NLM_HASH_MAX][NLM_HASH_MODE_MAX];
 #define free kfree
 #define NLM_AUTH_MAX_FRAGS	(20)
 
+#define MAX_AUTH_DATA 64000
+
 
 struct nlm_auth_ctx
 {
 	struct nlm_crypto_pkt_ctrl ctrl;
 	uint16_t stat;
+	uint8_t hashed_key[128];
+	struct crypto_shash * fallback_tfm;
+	uint8_t data[MAX_AUTH_DATA];
 	/*Don't change the order of this strucutre*/
 };
 
@@ -73,17 +79,19 @@ struct nlm_auth_ctx
 
 struct auth_pkt_desc
 {
-	uint32_t curr_index;
-	uint32_t total_len;
-	uint8_t pad[56];
-	//struct pkt_desc pkt_desc;
-	struct nlm_crypto_pkt_param pkt_param; 
+	uint16_t curr_index;
+	uint16_t total_len;
 	uint16_t stat;
+	uint16_t is_allocated;
+	int max_frags;
+	struct shash_desc * fallback;
+	uint8_t  * alloc_pkt_param;
+	struct nlm_crypto_pkt_param * pkt_param; /* maintain at the end */ 
 };
 
-#define PACKET_DESC_SIZE   (64+sizeof(struct auth_pkt_desc) + MAX_FRAGS*(2*64) + 64 + sizeof(struct nlm_async_crypto) + 64)
-#define NLM_CRYPTO_PKT_PARAM_OFFSET(addr)       (((unsigned long)addr + 64) & ~0x3fULL)
-#define NLM_ASYNC_PTR_PARAM_OFFSET(addr)        (((unsigned long)(addr + 64 + sizeof(struct nlm_crypto_pkt_param) + MAX_FRAGS*(2*64)) + 64) & ~0x3fULL)
+#define PACKET_DESC_SIZE   (64+sizeof(struct auth_pkt_desc) + MAX_FRAGS*(2*8) ) /* should be less than PAGE_SIZE/8 */ 
+#define NLM_CRYPTO_PKT_PARAM_OFFSET(addr)       (((unsigned long)addr + 64) & ~0x3fUL)
+#define NLM_ASYNC_PTR_PARAM_OFFSET(addr)        (((unsigned long)(addr + 64 + sizeof(struct nlm_crypto_pkt_param) + MAX_FRAGS*(2*8)) + 64) & ~0x3fUL)
 /*
    All extern declaration goes here.
  */
@@ -94,6 +102,7 @@ extern void print_crypto_msg_desc(uint64_t entry1, uint64_t entry2, uint64_t ent
 extern void print_pkt_desc(struct nlm_crypto_pkt_param * pkt_param, int index);
 extern struct nlm_crypto_stat crypto_stat[MAX_CPU];
 
+
 static inline void print_info(const char *func)
 {
 	extern void dump_stack(void);
@@ -129,7 +138,7 @@ static void print_buf(unsigned char *msg, unsigned char *buf, int len)
 static struct nlm_auth_ctx *nlm_shash_auth_ctx(struct crypto_shash *shash)
 {
 	uint8_t *ctx = crypto_tfm_ctx(crypto_shash_tfm(shash));
-	ctx = (uint8_t *)(((unsigned long)ctx + 63) & ~(0x3f));
+	ctx = (uint8_t *)(((unsigned long)ctx + 63) & ~(0x3fUL));
 	return (struct nlm_auth_ctx *)ctx;
 }
 
@@ -137,40 +146,112 @@ static struct nlm_auth_ctx *pkt_ctrl_auth_ctx(struct shash_desc *desc)
 {
 	return nlm_shash_auth_ctx(desc->tfm);
 }
+
+static int
+xlp_cra_xcbc_init(struct crypto_tfm *tfm)
+{
+	struct crypto_shash * hash = __crypto_shash_cast(tfm);
+	struct nlm_auth_ctx * nlm_ctx = nlm_shash_auth_ctx(hash);
+	nlm_ctx->fallback_tfm = crypto_alloc_shash("xcbc(aes-generic)",CRYPTO_ALG_TYPE_SHASH ,0 );
+	return 0;
+}
+
+static int
+xlp_cra_hmac_sha1_init(struct crypto_tfm *tfm)
+{
+	struct crypto_shash * hash = __crypto_shash_cast(tfm);
+	struct nlm_auth_ctx * nlm_ctx = nlm_shash_auth_ctx(hash);
+	nlm_ctx->fallback_tfm = crypto_alloc_shash("hmac(sha1-generic)",CRYPTO_ALG_TYPE_SHASH ,0 );
+	return 0;
+}
+static int
+xlp_cra_hmac_sha256_init(struct crypto_tfm *tfm)
+{
+	struct crypto_shash * hash = __crypto_shash_cast(tfm);
+	struct nlm_auth_ctx * nlm_ctx = nlm_shash_auth_ctx(hash);
+	nlm_ctx->fallback_tfm = crypto_alloc_shash("hmac(sha256-generic)",CRYPTO_ALG_TYPE_SHASH ,0 );
+	return 0;
+}
+
+static int
+xlp_cra_md5_init(struct crypto_tfm *tfm)
+{
+	struct crypto_shash * hash = __crypto_shash_cast(tfm);
+	struct nlm_auth_ctx * nlm_ctx = nlm_shash_auth_ctx(hash);
+	nlm_ctx->fallback_tfm = crypto_alloc_shash("hmac(md5-generic)",CRYPTO_ALG_TYPE_SHASH ,0 );
+	
+	return 0;
+}
 static int
 xlp_auth_init(struct shash_desc *desc)
 {
-	struct auth_pkt_desc *auth_pkt_desc = (struct auth_pkt_desc * )NLM_CRYPTO_PKT_PARAM_OFFSET(shash_desc_ctx(desc));
-	//printk("[%s] [%p]\n",__FUNCTION__, pkt_desc);
-	//printk("[%s] [%p]\n",__FUNCTION__, &pkt_desc->pkt_param.desc0);
-	//printk("[%s] [%p]\n",__FUNCTION__, &pkt_desc->pkt_param);
+	struct auth_pkt_desc *auth_pkt_desc = (struct auth_pkt_desc * )shash_desc_ctx(desc);
 	auth_pkt_desc->curr_index = 0;
 	auth_pkt_desc->total_len = 0;
+	auth_pkt_desc->fallback = NULL;
+	auth_pkt_desc->pkt_param = ( struct nlm_crypto_pkt_param  * )NLM_CRYPTO_PKT_PARAM_OFFSET((unsigned long )auth_pkt_desc + sizeof(struct auth_pkt_desc ));
+	auth_pkt_desc->max_frags = MAX_FRAGS;
+	auth_pkt_desc->is_allocated = 0;
 	return 0;
 }
-
 static int
 xlp_auth_update(struct shash_desc *desc,
 		const uint8_t * data, unsigned int length)
 {
-	struct auth_pkt_desc *auth_pkt_desc = (struct auth_pkt_desc *)NLM_CRYPTO_PKT_PARAM_OFFSET(shash_desc_ctx(desc));
+	struct auth_pkt_desc *auth_pkt_desc = (struct auth_pkt_desc *)shash_desc_ctx(desc);
+	struct nlm_auth_ctx * nlm_ctx = nlm_shash_auth_ctx(desc->tfm);
 	int index = auth_pkt_desc->curr_index;
-	struct nlm_crypto_pkt_param  *pkt_param  = &(auth_pkt_desc->pkt_param);
-	
-	nlm_crypto_fill_src_seg(pkt_param, index, (unsigned char*)data, length);
-	auth_pkt_desc->curr_index = nlm_crypto_fill_dst_seg(pkt_param, index , (unsigned char*)data, length);
+	unsigned char * data_index = &nlm_ctx->data[auth_pkt_desc->total_len];
+	struct nlm_crypto_pkt_param  *pkt_param  = auth_pkt_desc->pkt_param;
+
 	auth_pkt_desc->total_len += length;
+	if  (auth_pkt_desc->total_len >= MAX_AUTH_DATA)  {
+		if (auth_pkt_desc->fallback == NULL ) {
+			auth_pkt_desc->fallback = kmalloc((sizeof(struct shash_desc ) + 
+				crypto_shash_descsize(nlm_ctx->fallback_tfm))
+									,GFP_KERNEL);
+			auth_pkt_desc->fallback->tfm = nlm_ctx->fallback_tfm;
+			auth_pkt_desc->fallback->flags = desc->flags & CRYPTO_TFM_REQ_MAY_SLEEP;
+			crypto_shash_init(auth_pkt_desc->fallback);
+			auth_pkt_desc->total_len -= length;
+			crypto_shash_update(auth_pkt_desc->fallback,nlm_ctx->data,auth_pkt_desc->total_len);
+			auth_pkt_desc->total_len += length;
+		}
+		crypto_shash_update(auth_pkt_desc->fallback,data,length);
+		
+		return 0;
+	}
+
+	memcpy(data_index,data,length);
+	if ( (auth_pkt_desc->curr_index + 1 ) > auth_pkt_desc->max_frags ) {
+		uint8_t * mem = kmalloc(sizeof (struct nlm_crypto_pkt_param) 
+					+ ((auth_pkt_desc->max_frags + MAX_FRAGS) * 2 * 8 ) + 64 , GFP_KERNEL);
+		auth_pkt_desc->alloc_pkt_param = mem;
+		mem = (uint8_t *)NLM_CRYPTO_PKT_PARAM_OFFSET((unsigned long )mem);
+		memcpy(mem,pkt_param,sizeof (struct nlm_crypto_pkt_param)+(auth_pkt_desc->max_frags * 2 * 8));
+		
+		if ( auth_pkt_desc->is_allocated )
+			kfree(auth_pkt_desc->alloc_pkt_param);
+		pkt_param = auth_pkt_desc->pkt_param = ( struct nlm_crypto_pkt_param  * )mem;
+		auth_pkt_desc->is_allocated = 0;
+		auth_pkt_desc->max_frags += MAX_FRAGS;
+	} 
+		
+	auth_pkt_desc->curr_index = nlm_crypto_fill_src_dst_seg(pkt_param, index , auth_pkt_desc->max_frags, data_index, length);
 
 	return 0;
 }
 static int
-crypto_get_sync_fb_vc(void)
+crypto_get_sync_fb_vc(int * node)
 {
     int cpu;
+    int node_id;
     extern int ipsec_sync_vc;
 
     cpu = hard_smp_processor_id();      //processor_id();
-    cpu = cpu * 4 + ipsec_sync_vc;
+    node_id = (cpu >> NODE_ID_SHIFT_BIT);
+    cpu = (node_id << NODE_BASE_SHIFT_BIT) | (((cpu & 0x1f) * 4) + ipsec_sync_vc);
+    *node = node_id;
 
     return cpu;
 }
@@ -178,28 +259,56 @@ crypto_get_sync_fb_vc(void)
 static int
 xlp_auth_final(struct shash_desc *desc, uint8_t *out)
 {
-	struct auth_pkt_desc *auth_pkt_desc = (struct auth_pkt_desc *)NLM_CRYPTO_PKT_PARAM_OFFSET(shash_desc_ctx(desc));
+	struct auth_pkt_desc *auth_pkt_desc = (struct auth_pkt_desc *)shash_desc_ctx(desc);
 	int index = auth_pkt_desc->curr_index;
-	struct nlm_crypto_pkt_param  *pkt_param  = &(auth_pkt_desc->pkt_param);
+	struct nlm_crypto_pkt_param  *pkt_param  = auth_pkt_desc->pkt_param;
 	struct nlm_auth_ctx  * auth_ctx   = pkt_ctrl_auth_ctx(desc);
 	int fb_vc ;
-	uint64_t entry0, entry1, tx_id=0x12345678;
+	uint64_t entry0, entry1, tx_id=0x12345678ULL;
 	uint64_t  timeout = 0;
 	struct nlm_crypto_pkt_ctrl *ctrl = &auth_ctx->ctrl;
 	uint32_t size,code,src;
 	uint16_t stat = auth_ctx->stat;
-	int cpu = nlm_processor_id();
+	int cpu = hard_smp_processor_id();
         extern int ipsec_sync_vc;
-
+#ifdef CONFIG_32BIT
+	unsigned long msgrng_flags;
+#endif
+	int node_sae_base;
+	int node;
+
+	if ( (auth_pkt_desc->total_len == 0 ) ||  ( auth_pkt_desc->total_len > MAX_AUTH_DATA)) { 
+
+		if (auth_pkt_desc->fallback == NULL ) {
+			auth_pkt_desc->fallback = kmalloc((sizeof(struct shash_desc ) + 
+				crypto_shash_descsize(auth_ctx->fallback_tfm))
+									,GFP_KERNEL);
+			auth_pkt_desc->fallback->tfm = auth_ctx->fallback_tfm;
+			auth_pkt_desc->fallback->flags = desc->flags & CRYPTO_TFM_REQ_MAY_SLEEP;
+			crypto_shash_init(auth_pkt_desc->fallback);
+		}
+		crypto_shash_final(auth_pkt_desc->fallback,out);
+		return 0;
+	}
+	if ( auth_pkt_desc->fallback != NULL ) {
+		kfree(auth_pkt_desc->fallback);
+		auth_pkt_desc->fallback = NULL;
+	}
+		
 	nlm_crypto_fill_auth_pkt_param(ctrl,pkt_param,
 			0,auth_pkt_desc->total_len,0,out); 
 
 	preempt_disable();
+#ifdef CONFIG_32BIT
+	msgrng_access_enable(msgrng_flags);
+#endif
 
-	fb_vc = crypto_get_sync_fb_vc();
+	fb_vc = crypto_get_sync_fb_vc(&node);
 	entry0 = nlm_crypto_form_pkt_fmn_entry0(fb_vc, 0, 0, 0, virt_to_phys(ctrl));
 	entry1 = nlm_crypto_form_pkt_fmn_entry1(0, ctrl->hashkeylen, (32 + index * 16 ), virt_to_phys(pkt_param));
-	
+	node_sae_base = (node << NODE_BASE_SHIFT_BIT) | crypto_vc_base;
+
+	while (nlm_hal_send_msg3(node_sae_base, 0 /*code */ , entry0, entry1, tx_id) != 0 );
 
 #ifdef NLM_CRYPTO_DEBUG
 	print_crypto_msg_desc(entry0, entry1, tx_id);
@@ -208,33 +317,119 @@ xlp_auth_final(struct shash_desc *desc, uint8_t *out)
 #endif
 
 	//construct pkt, send to engine and receive reply
-	xlp_message_send_block_fast_3(0, crypto_vc_base, entry0, entry1, tx_id);
 	timeout = 0;
 	do {
 		timeout++;
 		nlm_hal_recv_msg2(ipsec_sync_vc, &src, &size, &code, &entry0, &entry1);
-	} while(entry0 != tx_id && timeout < 0xffffffff) ;
+	} while(entry0 != tx_id && timeout < 0xffffffffULL) ;
 	
 
-
-	if (timeout >= 0xffffffff) {
-		printk("Error: FreeBack message is not received");
+	if (timeout >= 0xffffffffULL) {
+		printk("\nError: FreeBack message is not received");
+#ifdef CONFIG_32BIT
+		msgrng_access_disable(msgrng_flags);
+#endif
+		if ( auth_pkt_desc->is_allocated )
+			kfree(auth_pkt_desc->pkt_param);
 		preempt_enable();
-		return -EIO;
+		return -EAGAIN;
 	}
+
 #ifdef NLM_CRYPTO_DEBUG
 	print_buf("AUTH:", out, 16);
 #endif
+#ifdef CONFIG_32BIT
+	msgrng_access_disable(msgrng_flags);
+#endif
+	preempt_enable();
+	if ( auth_pkt_desc->is_allocated )
+		kfree(auth_pkt_desc->pkt_param);
 	crypto_stat[cpu].auth[stat] ++;
 	crypto_stat[cpu].auth_tbytes[stat] += auth_pkt_desc->total_len + ctrl->taglen;
-	preempt_enable();
  	return 0;
 }
+static int xlp_auth_export(struct shash_desc *desc, void *out)
+{
+	struct auth_pkt_desc *auth_pkt_desc = (struct auth_pkt_desc *)shash_desc_ctx(desc);
+	if ( auth_pkt_desc->fallback == NULL )
+		return 0;
+	return crypto_shash_export(auth_pkt_desc->fallback,out);
+}
+
+static int xlp_auth_import(struct shash_desc *desc, const void *in)
+{
+	struct auth_pkt_desc *auth_pkt_desc = (struct auth_pkt_desc *)shash_desc_ctx(desc);
+	if ( auth_pkt_desc->fallback == NULL )
+		return 0;
+	return crypto_shash_import(auth_pkt_desc->fallback, in);
+}
+
 
 /*
    All Setkey goes here.
  */
 
+int hash_key(int alg, int mode, const uint8_t * key, unsigned int keylen, uint8_t * new_key)
+{
+
+	int fb_vc;
+	uint64_t entry0,entry1;
+	uint32_t size,code,src;
+        extern int ipsec_sync_vc;
+	uint64_t tx_id=0x12345678ULL;
+	uint64_t  timeout = 0;
+#ifdef CONFIG_32BIT
+	unsigned long msgrng_flags;
+#endif
+	int node_sae_base;
+	int node;
+	char * tmp = kmalloc(keylen + sizeof(struct nlm_crypto_pkt_ctrl) + sizeof( struct nlm_crypto_pkt_param ) 
+											+ 128, GFP_KERNEL);
+	struct nlm_crypto_pkt_ctrl * ctrl = (struct nlm_crypto_pkt_ctrl * ) ((((unsigned long)tmp + 63)) & ~(0x3fUL)); 
+	struct nlm_crypto_pkt_param * pkt_param = (struct nlm_crypto_pkt_param * )
+		(((unsigned long) ctrl + sizeof(struct nlm_crypto_pkt_ctrl) + 63) & ~(0x3fUL));
+	char *tmp_key = (char *)(((unsigned long) pkt_param + 
+			sizeof(struct nlm_crypto_pkt_param ) + 63)  & ~(0x3fUL));
+
+	memcpy(tmp_key,key,keylen);
+        nlm_crypto_fill_pkt_ctrl(ctrl,0,alg,mode,0,0,0,NULL,0,NULL,0);
+        nlm_crypto_fill_auth_pkt_param(ctrl,pkt_param,0,keylen,0,new_key);
+        nlm_crypto_fill_src_dst_seg(pkt_param,0,MAX_FRAGS,tmp_key,keylen);
+
+
+#ifdef CONFIG_32BIT
+	msgrng_access_enable(msgrng_flags);
+#endif
+        fb_vc = crypto_get_sync_fb_vc(&node);
+        entry0 = nlm_crypto_form_pkt_fmn_entry0(fb_vc, 0, 0, 0, virt_to_phys(ctrl));
+        entry1 = nlm_crypto_form_pkt_fmn_entry1(0, 0, (32 + 16 ), virt_to_phys(pkt_param));
+
+
+#ifdef NLM_CRYPTO_DEBUG
+       print_crypto_msg_desc(entry0, entry1, tx_id);
+       print_cntl_instr(ctrl->desc0);
+       print_pkt_desc(pkt_param,1);
+#endif
+
+        //construct pkt, send to engine and receive reply
+	node_sae_base = (node << NODE_BASE_SHIFT_BIT) | crypto_vc_base;
+
+	while (nlm_hal_send_msg3(node_sae_base, 0 /*code */ , entry0, entry1, tx_id) != 0 );
+        timeout = 0;
+        do {
+                timeout++;
+                nlm_hal_recv_msg2(ipsec_sync_vc, &src, &size, &code, &entry0, &entry1);
+        } while(entry0 != tx_id && timeout < 0xfffffUL) ;
+	kfree(tmp);
+#ifdef CONFIG_32BIT
+	msgrng_access_disable(msgrng_flags);
+#endif
+	return 0;
+
+	
+
+}
+
 static int xlp_auth_aes_xcbc_setkey(struct crypto_shash *tfm, const u8 * key, unsigned int keylen)
 {
 	struct nlm_auth_ctx * nlm_ctx = nlm_shash_auth_ctx(tfm);
@@ -258,10 +453,13 @@ static int xlp_auth_aes_xcbc_setkey(struct crypto_shash *tfm, const u8 * key, un
                        __FUNCTION__, keylen);
 	}
 
-
 	/*setup ctrl descriptor*/
 	nlm_crypto_fill_pkt_ctrl(&nlm_ctx->ctrl,0,hash_alg,NLM_HASH_MODE_XCBC,
 		NLM_CIPHER_BYPASS,NLM_CIPHER_MODE_ECB,0,NULL,0,(unsigned char *)key,keylen);
+	
+	crypto_shash_setkey(nlm_ctx->fallback_tfm,key,keylen);
+
+	
 	return 0;
 	
 }
@@ -272,13 +470,22 @@ xlp_auth_hmac_sha256_setkey(struct crypto_shash *tfm, const u8 * key, unsigned i
 {
 	struct nlm_auth_ctx * nlm_ctx = nlm_shash_auth_ctx(tfm);
 	struct nlm_crypto_pkt_ctrl * ctrl = &nlm_ctx->ctrl; 
+	
 	nlm_ctx->stat = H_SHA256_STAT;
+	if ( keylen > 64 ) {
+		hash_key(NLM_HASH_SHA,NLM_HASH_MODE_SHA256,key,keylen,&nlm_ctx->hashed_key[0]);
+		nlm_crypto_fill_pkt_ctrl(ctrl,1,NLM_HASH_SHA,NLM_HASH_MODE_SHA256,
+			NLM_CIPHER_BYPASS,NLM_CIPHER_MODE_ECB,0,NULL,0,(unsigned char*)nlm_ctx->hashed_key,64);
+		crypto_shash_setkey(nlm_ctx->fallback_tfm,nlm_ctx->hashed_key,64);
+		return 0;
+	}
 
 	/*setup ctrl descriptor*/
 	nlm_crypto_fill_pkt_ctrl(ctrl,1,NLM_HASH_SHA,NLM_HASH_MODE_SHA256,
 		NLM_CIPHER_BYPASS,NLM_CIPHER_MODE_ECB,0,NULL,0,(unsigned char*)key,keylen);
 	if ( ctrl->hashkeylen < auth_mode_key_len[NLM_HASH_SHA][NLM_HASH_MODE_SHA256]) 
 		ctrl->hashkeylen = auth_mode_key_len[NLM_HASH_SHA][NLM_HASH_MODE_SHA256];
+	crypto_shash_setkey(nlm_ctx->fallback_tfm,key,keylen);
 	return 0;
 	
 }
@@ -289,13 +496,22 @@ xlp_auth_hmac_md5_setkey(struct crypto_shash *tfm, const u8 * key, unsigned int
 {
 	struct nlm_auth_ctx * nlm_ctx = nlm_shash_auth_ctx(tfm);
 	struct nlm_crypto_pkt_ctrl * ctrl = &nlm_ctx->ctrl; 
+
 	nlm_ctx->stat = MD5_STAT;
+	if ( keylen > 64 ) {
+		hash_key(NLM_HASH_MD5,NLM_HASH_MODE_SHA1,key,keylen,&nlm_ctx->hashed_key[0]);
+		nlm_crypto_fill_pkt_ctrl(ctrl,1,NLM_HASH_MD5,NLM_HASH_MODE_SHA1,
+			NLM_CIPHER_BYPASS,NLM_CIPHER_MODE_ECB,0,NULL,0,(unsigned char *)nlm_ctx->hashed_key,64);
+		crypto_shash_setkey(nlm_ctx->fallback_tfm,nlm_ctx->hashed_key,64);
+		return 0;
+	}
 
 	/*setup ctrl descriptor*/
 	nlm_crypto_fill_pkt_ctrl(ctrl,1,NLM_HASH_MD5,NLM_HASH_MODE_SHA1,
 		NLM_CIPHER_BYPASS,NLM_CIPHER_MODE_ECB,0,NULL,0,(unsigned char *)key,keylen);
 	if ( ctrl->hashkeylen < auth_mode_key_len[NLM_HASH_MD5][NLM_HASH_MODE_SHA1]) 
 		ctrl->hashkeylen = auth_mode_key_len[NLM_HASH_MD5][NLM_HASH_MODE_SHA1];
+	crypto_shash_setkey(nlm_ctx->fallback_tfm,key,keylen);
 	return 0;
 	
 }
@@ -305,13 +521,23 @@ xlp_auth_hmac_sha1_setkey(struct crypto_shash *tfm, const u8 * key, unsigned int
 {
 	struct nlm_auth_ctx * nlm_ctx = nlm_shash_auth_ctx(tfm);
 	struct nlm_crypto_pkt_ctrl * ctrl = &nlm_ctx->ctrl; 
+
 	nlm_ctx->stat = H_SHA1_STAT;
+	if ( keylen > 64 ) {
+
+		hash_key(NLM_HASH_SHA,NLM_HASH_MODE_SHA1,key,keylen,&nlm_ctx->hashed_key[0]);
+		nlm_crypto_fill_pkt_ctrl(ctrl,1,NLM_HASH_SHA,NLM_HASH_MODE_SHA1,
+			NLM_CIPHER_BYPASS,NLM_CIPHER_MODE_ECB,0,NULL,0,(unsigned char *)nlm_ctx->hashed_key,64);
+		crypto_shash_setkey(nlm_ctx->fallback_tfm,nlm_ctx->hashed_key,64);
+		return 0;
+	}
 
 	/*setup ctrl descriptor*/
 	nlm_crypto_fill_pkt_ctrl(ctrl,1,NLM_HASH_SHA,NLM_HASH_MODE_SHA1,
 		NLM_CIPHER_BYPASS,NLM_CIPHER_MODE_ECB,0,NULL,0,(unsigned char *)key,keylen);
 	if ( ctrl->hashkeylen < auth_mode_key_len[NLM_HASH_SHA][NLM_HASH_MODE_SHA1]) 
 		ctrl->hashkeylen = auth_mode_key_len[NLM_HASH_SHA][NLM_HASH_MODE_SHA1];
+	crypto_shash_setkey(nlm_ctx->fallback_tfm,key,keylen);
 	return 0;
 	 
 	
@@ -321,6 +547,8 @@ static struct shash_alg xcbc_mac_alg = {
 	.digestsize = XCBC_DIGEST_SIZE,
 	.init = xlp_auth_init,
 	.update = xlp_auth_update,
+	.export = xlp_auth_export,
+	.import = xlp_auth_import,
 	.final = xlp_auth_final,
 	.setkey = xlp_auth_aes_xcbc_setkey,
 	.descsize = PACKET_DESC_SIZE,
@@ -332,6 +560,7 @@ static struct shash_alg xcbc_mac_alg = {
 		 .cra_blocksize = AES_BLOCK_SIZE,
 		 .cra_module = THIS_MODULE,
 		 .cra_ctxsize = CTRL_DESC_SIZE, 
+		 .cra_init = xlp_cra_xcbc_init,
 		 }
 };
 
@@ -339,6 +568,8 @@ static struct shash_alg sha256_hmac_alg = {
 	.digestsize = SHA256_DIGEST_SIZE,
 	.init = xlp_auth_init,
 	.update = xlp_auth_update,
+	.export = xlp_auth_export,
+	.import = xlp_auth_import,
 	.final = xlp_auth_final,
 	.setkey = xlp_auth_hmac_sha256_setkey,
 	.descsize = PACKET_DESC_SIZE,
@@ -350,6 +581,7 @@ static struct shash_alg sha256_hmac_alg = {
 		 .cra_blocksize = SHA256_BLOCK_SIZE,
 		 .cra_module = THIS_MODULE,
 		 .cra_ctxsize = CTRL_DESC_SIZE, 
+		 .cra_init = xlp_cra_hmac_sha256_init,
 		 }
 };
 
@@ -358,6 +590,8 @@ static struct shash_alg md5_hmac_alg = {
 	.init = xlp_auth_init,
 	.update = xlp_auth_update,
 	.final = xlp_auth_final,
+	.export = xlp_auth_export,
+	.import = xlp_auth_import,
 	.setkey = xlp_auth_hmac_md5_setkey,
 	.descsize = PACKET_DESC_SIZE,
 	.base = {
@@ -368,6 +602,7 @@ static struct shash_alg md5_hmac_alg = {
 		 .cra_blocksize = MD5_BLOCK_SIZE,
 		 .cra_module = THIS_MODULE,
 		 .cra_ctxsize = CTRL_DESC_SIZE, 
+		 .cra_init = xlp_cra_md5_init,
 		 }
 };
 static struct shash_alg sha1_hmac_alg = {
@@ -375,6 +610,8 @@ static struct shash_alg sha1_hmac_alg = {
 	.init = xlp_auth_init,
 	.update = xlp_auth_update,
 	.final = xlp_auth_final,
+	.export = xlp_auth_export,
+	.import = xlp_auth_import,
 	.setkey = xlp_auth_hmac_sha1_setkey,
 	.descsize = PACKET_DESC_SIZE,
 	.base = {
@@ -385,6 +622,7 @@ static struct shash_alg sha1_hmac_alg = {
 		 .cra_blocksize = SHA1_BLOCK_SIZE,
 		 .cra_module = THIS_MODULE,
 		 .cra_ctxsize = CTRL_DESC_SIZE, 
+		 .cra_init = xlp_cra_hmac_sha1_init,
 		 }
 };
 
@@ -409,7 +647,7 @@ xlp_auth_alg_init(void)
 	rc = crypto_register_shash(&xcbc_mac_alg);
 	if (rc)
 		goto out;
-	no_of_alg_registered++;
+	no_of_alg_registered++; 
 	//printk("Some of the FIPS test failed as the maximum key length supported is 64 bytes.\n");
 
 	printk(KERN_NOTICE "Using XLP hardware for SHA/MD5 algorithms.\n");
@@ -422,10 +660,10 @@ out:
 void
 xlp_auth_alg_fini(void)
 {
-	crypto_unregister_shash(&sha1_hmac_alg);
+	crypto_unregister_shash(&xcbc_mac_alg);
 	crypto_unregister_shash(&md5_hmac_alg);
 	crypto_unregister_shash(&sha256_hmac_alg);
-	crypto_unregister_shash(&xcbc_mac_alg);
+	crypto_unregister_shash(&sha1_hmac_alg);
 }
 
 EXPORT_SYMBOL(xlp_auth_alg_init);
diff --git a/drivers/crypto/nlm-sae/nlm_crypto.c b/drivers/crypto/nlm-sae/nlm_crypto.c
index 0b54e35..fc1bb4b 100644
--- a/drivers/crypto/nlm-sae/nlm_crypto.c
+++ b/drivers/crypto/nlm-sae/nlm_crypto.c
@@ -29,12 +29,11 @@ THE POSSIBILITY OF SUCH DAMAGE.
 
 #include <asm/netlogic/hal/nlm_hal_fmn.h>
 #include <asm/netlogic/hal/nlm_hal_macros.h>
+#include <asm/netlogic/hal/nlm_hal_xlp_dev.h>
 #include <linux/crypto.h>
 #include "nlm_async.h"
-#include <asm/netlogic/crypto/nlmcrypto.h>
 
 
-#define XLP_POLLING 1
 #ifdef TRACING
 #define TRACE_TEXT(str) printk(str);
 #define TRACE_RET printk(")")
@@ -43,15 +42,9 @@ THE POSSIBILITY OF SUCH DAMAGE.
 #define TRACE_RET ((void) 0)
 #endif				/* TRACING */
 #undef NLM_CRYPTO_DEBUG
+#define NETL_OP_ENCRYPT 1
+#define NETL_OP_DECRYPT 0
 
-#define DRIVER_NAME "nlmsae"
-
-#define NLM_CRYPTO_OP_IN_PROGRESS 0
-#define NLM_CRYPTO_OP_DONE	  1
-
-/**
- * @file_name crypto.c
- */
 
 /**
  * @defgroup crypto Crypto API
@@ -59,14 +52,9 @@ THE POSSIBILITY OF SUCH DAMAGE.
  */
 
 #define printf(a, b...) printk(KERN_ERR a, ##b)
-//#define printf(a, b...)
-#define malloc(a) kmalloc(a, GFP_ATOMIC)
-#define free kfree
 
 #define xtract_bits(x, bitpos, numofbits) ((x) >> (bitpos) & ((1ULL << (numofbits)) - 1))
 
-#define VC_MODE_ROUND_ROBIN 1
-#define NUM_VC 16
 
 extern struct proc_dir_entry *nlm_root_proc;
 extern int xlp_aead_alg_init(void);
@@ -84,6 +72,7 @@ static int xlp_sae_release(struct inode *, struct file *);
 struct nlm_crypto_stat crypto_stat[MAX_CPU];
 int crypto_vc_base;
 int crypto_vc_limit;
+int chip_feature = 0 ;
 
 
 
@@ -131,6 +120,7 @@ int auth_mode_key_len[NLM_HASH_MAX][NLM_HASH_MODE_MAX] = {
 
 #define MAX_KEY_LEN_IN_DW 20
 #define NLM_CRYPTO_MAX_STR_LEN 200
+#ifdef NLM_CRYPTO_DEBUG
 static char str_cipher_alg[NLM_CIPHER_MAX + 1][NLM_CRYPTO_MAX_STR_LEN] = {
 "bypass",       // NLM_CIPHER_BYPASS
 "des",          // NLM_CIPHER_DES
@@ -188,7 +178,6 @@ static char str_auth_mode[NLM_HASH_MODE_MAX + 1][NLM_CRYPTO_MAX_STR_LEN] = {
 "cbc mac",      // NLM_AUTH_MODE_CBC_MAC
 "undefined", // > max
 };
-#ifdef NLM_CRYPTO_DEBUG
 void hex_dump(char * description,unsigned char *in, int num)
 {
         int i, j;
@@ -264,10 +253,9 @@ char *nlm_crypto_auth_mode_get_name(unsigned int auth_mode)
 
 void print_crypto_msg_desc(uint64_t entry1, uint64_t entry2, uint64_t entry3)
 {
-
-        printk("Security Message Descriptor 0: 0x%lx\n", entry1);
-        printk("Security Message Descriptor 1: 0x%lx\n", entry2);
-        printk("Security Message Descriptor 2: 0x%lx\n", entry3);
+        printk("Security Message Descriptor 0: 0x%llx\n", entry1);
+        printk("Security Message Descriptor 1: 0x%llx\n", entry2);
+        printk("Security Message Descriptor 2: 0x%llx\n", entry3);
 
 
         printk("Free descriptor response destination : 0x%llx  \n", xtract_bits(entry1, 48, 16));
@@ -291,6 +279,7 @@ void print_cntl_instr(uint64_t cntl_desc)
 	unsigned int tmp;
 	char *x;
 	char s[NLM_CRYPTO_MAX_STR_LEN];
+	cntl_desc = ccpu_to_be64(cntl_desc);
 
 	printf("control description: 0x%016llx\n", (unsigned long long)cntl_desc);
 	printf("HMac = 0x%llx  \n", xtract_bits(cntl_desc, 61, 1));
@@ -333,8 +322,19 @@ struct designer_desc{
 };
 
 
-void print_pkt_desc(struct nlm_crypto_pkt_param  *pkt_desc, int index)
+void print_pkt_desc(struct nlm_crypto_pkt_param  *pkt_desc1, int index)
 {
+	int i;
+	unsigned long  phys;
+	void * virt;
+	
+	struct nlm_crypto_pkt_param * pkt_desc = kmalloc(sizeof(struct nlm_crypto_pkt_param) + 256 ,GFP_KERNEL);;
+	pkt_desc->desc0 = ccpu_to_be64(pkt_desc1->desc0); 
+	pkt_desc->desc1 = ccpu_to_be64(pkt_desc1->desc1); 
+	pkt_desc->desc2 = ccpu_to_be64(pkt_desc1->desc2); 
+	pkt_desc->desc3 = ccpu_to_be64(pkt_desc1->desc3); 
+	
+
 	printf("Packet desc address = %p\n",pkt_desc);
 	printf("Packet Descriptor 0: 0x%016llx\n", (unsigned long long)pkt_desc->desc0);
 	printf("Packet Descriptor 1: 0x%016llx\n", (unsigned long long)pkt_desc->desc1);
@@ -369,19 +369,19 @@ void print_pkt_desc(struct nlm_crypto_pkt_param  *pkt_desc, int index)
 	printf("arc4 sbox l3 alloc = 0x%llx  \n", xtract_bits(pkt_desc->desc3, 8, 1));
 	printf("arc4 save box = 0x%llx  \n", xtract_bits(pkt_desc->desc3, 6, 1));
 	printf("hmac ext pad key = 0x%llx  \n", xtract_bits(pkt_desc->desc3, 5, 1));
-
-        int i;
-	unsigned long  phys;
-	void * virt;
-	
+	int len;
 
         for (i=0; i < index; i++) {
-                printf("Packet Descriptor frag src %d: 0x%016llx\n", i, (unsigned long long)pkt_desc->segment[i][0]);
-                printf("Packet Descriptor frag dst %d: 0x%016llx\n", i, (unsigned long long)pkt_desc->segment[i][1]);
+		pkt_desc->segment[i][0] = ccpu_to_be64(pkt_desc1->segment[i][0]); 
+		pkt_desc->segment[i][1] = ccpu_to_be64(pkt_desc1->segment[i][1]);
+		printf("Packet Descriptor frag src %d: 0x%016llx\n", i, (unsigned long long)pkt_desc->segment[i][0]);
+		printf("Packet Descriptor frag dst %d: 0x%016llx\n", i, (unsigned long long)pkt_desc->segment[i][1]);
+
 		phys = xtract_bits(pkt_desc->segment[i][0], 0,40);
 		virt = phys_to_virt(phys);
-		hex_dump("src \n",virt, 30);
-		printk("virtual is %p and phys is %lx\n",virt,phys);
+		len = xtract_bits(pkt_desc->segment[i][0], 48, 16); 
+		len = (len > 64 ) ? 64 : len;
+		hex_dump("src",virt,len);
 
 
                 printf("frag src length = 0x%llx  \n", xtract_bits(pkt_desc->segment[i][0], 48, 16));
@@ -394,6 +394,118 @@ void print_pkt_desc(struct nlm_crypto_pkt_param  *pkt_desc, int index)
         }
 }
 #endif
+
+int alloc_pkt_param( struct nlm_async_crypto * async,struct nlm_crypto_pkt_param ** pkt_param , int max_frags)
+{
+	uint8_t * new_pkt_param = NULL;
+	async->pkt_param = kmalloc((sizeof (struct nlm_crypto_pkt_param) +( max_frags * 2 * 8 )+ 64),GFP_KERNEL);
+
+	if ( !async->pkt_param) {
+		return -1;
+	}
+
+	new_pkt_param = (uint8_t * )(((unsigned long)async->pkt_param + 64) & ~0x3fUL);
+
+	memcpy(new_pkt_param,*pkt_param,sizeof(struct nlm_crypto_pkt_param));
+	*pkt_param = (struct nlm_crypto_pkt_param *)new_pkt_param;
+
+	
+	return 1;
+}
+
+int nlm_crypto_sae_num_seg_reqd(void *data, unsigned int buflen)
+{
+	return nlm_crypto_num_segs_reqd(buflen);
+}
+
+int nlm_crypto_calc_rem_len(struct scatterlist *sg, unsigned int cipher_len)
+{
+	int len,seg = 0;
+	for (;cipher_len > 0;sg = scatterwalk_sg_next(sg)){
+		len = min(cipher_len, sg->length);
+		seg += nlm_crypto_sae_num_seg_reqd(NULL,len);
+		cipher_len -= len;
+	}
+	return seg;
+}
+
+int fill_src_dst_sg(struct scatterlist *src_sg, struct scatterlist *dst_sg, unsigned int cipher_len, 
+		struct nlm_crypto_pkt_param *param, 
+		int seg, unsigned int max_frags, int op)
+{
+	struct scatterlist *sg;
+	struct scatter_walk walk;
+	unsigned int len = 0;
+	uint8_t *virt = NULL;
+	int rv = 0;
+	int i;
+
+	if (src_sg == dst_sg ) {
+		for( sg = src_sg; sg != NULL ; sg = scatterwalk_sg_next(sg)) {
+			len = min(cipher_len, sg->length);
+			scatterwalk_start(&walk, sg);
+			virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
+			if ( cipher_len > 0 ) {
+				rv = nlm_crypto_fill_src_dst_seg(param, seg, max_frags, virt, len);
+				if ( rv < seg ) {
+					return nlm_crypto_calc_rem_len(sg,cipher_len) + max_frags; 
+				}
+				else
+					seg = rv;
+			}
+			cipher_len -= len;
+		}
+		return seg;
+	}
+	else {
+		int nr_src_frags = 0;
+		int nr_dst_frags = 0;
+		int index = seg;
+		int nbytes = cipher_len;
+		for (sg = src_sg,seg;cipher_len > 0; sg = scatterwalk_sg_next(sg)) {
+			len = min(cipher_len, sg->length);
+			scatterwalk_start(&walk, sg);
+			virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
+			rv = nlm_crypto_fill_src_seg(param,seg,max_frags,virt,len);
+			if ( rv <= seg )
+				return nlm_crypto_calc_rem_len(sg,cipher_len) + max_frags;
+			else
+				seg = rv;
+			cipher_len -= len;
+		}
+		nr_src_frags = seg;
+		cipher_len = nbytes;
+		for (sg = dst_sg,seg = index ;cipher_len > 0; sg = scatterwalk_sg_next(sg)) {
+			len = min(cipher_len, sg->length);
+			scatterwalk_start(&walk, sg);
+			virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
+			rv = nlm_crypto_fill_dst_seg(param,seg,max_frags,virt,len);
+			if ( rv <= seg )
+				return nlm_crypto_calc_rem_len(sg,cipher_len) + max_frags;
+			else
+				seg = rv;
+			cipher_len -= len;
+		}
+
+		nr_dst_frags = seg;
+
+		if ((nr_src_frags > nr_dst_frags) && (nr_src_frags < max_frags)) {
+			for (i = 0; i < nr_src_frags - nr_dst_frags; i++)
+				param->segment[index + nr_dst_frags + i][1] = 0ULL;
+			seg = nr_src_frags;
+		}
+		else  { 
+			if ((nr_src_frags < nr_dst_frags) && (nr_dst_frags < max_frags )){
+				for (i = 0; i < nr_dst_frags - nr_src_frags; i++)
+					param->segment[index + nr_src_frags + i][0] = 0ULL;
+			}
+			seg = nr_dst_frags;
+		}
+		return seg;
+	}
+
+}
+
     static void
 reset_crypto_stats(void)
 {
@@ -413,14 +525,17 @@ reset_crypto_stats(void)
 }
 
 int
-crypto_get_fb_vc(void)
+crypto_get_fb_vc(int * node)
 {
     int cpu;
+    int node_id = 0;
     extern int ipsec_async_vc;
 
 
     cpu = hard_smp_processor_id();	//processor_id();
-    cpu = cpu * 4 + ipsec_async_vc;
+    node_id = (cpu >> NODE_ID_SHIFT_BIT);
+    cpu = (node_id << NODE_BASE_SHIFT_BIT) | (((cpu & 0x1f) * 4) + ipsec_async_vc);
+    *node = node_id;
 
     return cpu;
 }
@@ -453,7 +568,7 @@ nlm_xlp_sae_msgring_handler(uint32_t vc, uint32_t src_id,
 	uint64_t msg0, uint64_t msg1,
 	uint64_t msg2, uint64_t msg3, void *data)
 {
-	struct nlm_async_crypto *async = (struct nlm_async_crypto *)(msg0);
+	struct nlm_async_crypto *async = (struct nlm_async_crypto *)(unsigned long )msg0;
 	if(async)	
 		async->callback(async, msg1);
 }
@@ -464,7 +579,6 @@ nlm_crypto_read_stats_proc(char *page, char **start, off_t off, int count,
 {
         int len = 0;
 	int i,j;
-	uint64_t  cnt;
 	off_t begin = 0;
 	uint64_t enc_tp[ENC_MAX_STAT];
 	uint64_t auth_tp[AUTH_MAX_STAT];
@@ -491,8 +605,14 @@ nlm_crypto_read_stats_proc(char *page, char **start, off_t off, int count,
 	if (!proc_pos_check(&begin, &len, off, count))
 		goto out;
 
-	len += sprintf(page + len,"AES192-CBC\t%lld\t\t%lld\nAES256-CBC\t%lld\t\t%lld\n",
-		enc_tp[AES192_CBC_STAT],enc_tb[AES192_CBC_STAT],enc_tp[AES256_CBC_STAT],enc_tb[AES256_CBC_STAT]);
+	len += sprintf(page + len,"AES192-CBC\t%lld\t\t%lld\nAES256-CBC\t%lld\t\t%lld\nAES128-CTR\t%lld\t\t%lld\n",
+		enc_tp[AES192_CBC_STAT],enc_tb[AES192_CBC_STAT],enc_tp[AES256_CBC_STAT],enc_tb[AES256_CBC_STAT],
+		enc_tp[AES128_CTR_STAT],enc_tb[AES128_CTR_STAT]);
+	if (!proc_pos_check(&begin, &len, off, count))
+		goto out;
+
+	len += sprintf(page + len,"AES192-CTR\t%lld\t\t%lld\nAES256-CTR\t%lld\t\t%lld\n",
+		enc_tp[AES192_CTR_STAT],enc_tb[AES192_CTR_STAT],enc_tp[AES256_CTR_STAT],enc_tb[AES256_CTR_STAT]);
 	if (!proc_pos_check(&begin, &len, off, count))
 		goto out;
 
@@ -516,6 +636,11 @@ nlm_crypto_read_stats_proc(char *page, char **start, off_t off, int count,
 	if (!proc_pos_check(&begin, &len, off, count))
 		 goto out;
 
+	len  += sprintf(page + len,"GCM\t\t%lld\t\t%lld\nCCM\t\t%lld\t\t%lld\n",auth_tp[GCM_STAT],auth_tb[GCM_STAT],auth_tp[CCM_STAT],auth_tb[CCM_STAT]);
+
+	if (!proc_pos_check(&begin, &len, off, count))
+		 goto out;
+
         *eof = 1;
 
       out:
@@ -529,12 +654,28 @@ nlm_crypto_read_stats_proc(char *page, char **start, off_t off, int count,
         return len;
 
 }
+int nlm_crypto_clear_stat_proc(struct file *file, const char __user *buffer, 
+		unsigned long count, void *data)
+{
+	char buf[16];
+	unsigned long val;
+
+	copy_from_user(buf, buffer, count);
+	val =  simple_strtol(buf, NULL, 0);	
+
+	if ( val  == 1)
+    		reset_crypto_stats();
+
+	return 1;
+
+}
 
 int
 nlm_crypto_init(void)
 {
     int ret = 0;
     struct proc_dir_entry *entry = NULL;
+    struct proc_dir_entry *clear_entry  = NULL;
 
     entry = create_proc_read_entry("crypto_stats", 0, nlm_root_proc,
 		    nlm_crypto_read_stats_proc,
@@ -546,6 +687,12 @@ nlm_crypto_init(void)
 	    ret = -EINVAL;
     }
 
+    clear_entry = create_proc_entry("clear_crypto_stats", S_IFREG|S_IRUGO|S_IWUSR,nlm_root_proc);
+
+    if ( clear_entry != NULL ) {
+	   clear_entry->write_proc = nlm_crypto_clear_stat_proc;
+    }
+
    nlm_hal_get_crypto_vc_nums(&crypto_vc_base, &crypto_vc_limit); 
 
     if (register_xlp_msgring_handler
@@ -559,6 +706,14 @@ nlm_crypto_init(void)
     return ret;
 }
 
+static void  init_sae()
+{
+	if( is_nlm_xlp2xx())
+		chip_feature = (ZUC | DES3_KEY_SWAP); 
+	else
+		chip_feature = 0x0;
+}
+
     static int __init
 xlp_sae_init(void)
 {
@@ -577,6 +732,7 @@ xlp_sae_init(void)
     	return -1;
     }
     nlm_crypto_init();
+    init_sae();
     if(ipsec_async_vc != -1){
     	xlp_crypt_alg_init();
     	xlp_aead_alg_init();
diff --git a/drivers/crypto/nlm-sae/nlm_enc.c b/drivers/crypto/nlm-sae/nlm_enc.c
old mode 100644
new mode 100755
index c7e0f9f..4e032d5
--- a/drivers/crypto/nlm-sae/nlm_enc.c
+++ b/drivers/crypto/nlm-sae/nlm_enc.c
@@ -22,23 +22,14 @@ CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
 THE POSSIBILITY OF SUCH DAMAGE.
 *****************************#NETL_2#********************************/
-#include <crypto/scatterwalk.h>
-/*#include <crypto/algapi.h>
-#include <linux/module.h>
-#include <linux/kernel.h>
-*/
 #include <linux/crypto.h>
 #include <crypto/aes.h>
 #include <crypto/des.h>
 #include <crypto/ctr.h>
-#include <asm/netlogic/crypto/nlmcrypto.h>
 #include <nlm_hal_fmn.h>
 #include <asm/netlogic/hal/nlm_hal.h>
+#include <asm/netlogic/msgring.h>
 
-/*#include <linux/pci.h>
-#include <linux/pci_ids.h>
-#include <asm/io.h>
-*/
 #include "nlm_async.h"
 #undef NLM_CRYPTO_DEBUG
 
@@ -49,24 +40,29 @@ THE POSSIBILITY OF SUCH DAMAGE.
 struct nlm_enc_ctx {
 	struct nlm_crypto_pkt_ctrl ctrl; 
 	uint16_t stat;
+	char nonce[4];
 };
 /* mem utilisation of CTX_SIZE */
-#define MAX_FRAGS               18
+#define MAX_FRAGS               18 
 #define CTRL_DESC_SIZE          (sizeof(struct nlm_enc_ctx) + 64)
 #define DES3_CTRL_DESC_SIZE     (2*CTRL_DESC_SIZE + 2*64)
 
 
 /* mem utilisation of req mem */
 
-#define PACKET_DESC_SIZE        (64 + sizeof(struct nlm_crypto_pkt_param) + MAX_FRAGS*(2*64) + 64 + sizeof(struct nlm_async_crypto) + 64)
-#define NLM_CRYPTO_PKT_PARAM_OFFSET(addr)       (((unsigned long)addr + 64) & ~0x3fULL)
-#define NLM_ASYNC_PTR_PARAM_OFFSET(addr)        (((unsigned long)(addr + 64 + sizeof(struct nlm_crypto_pkt_param) + MAX_FRAGS*(2*64)) + 64) & ~0x3fULL)
+#define PACKET_DESC_SIZE        (64 + sizeof(struct nlm_crypto_pkt_param) + (MAX_FRAGS*16) + 64 + sizeof(struct nlm_async_crypto) + 64)
+#define NLM_CRYPTO_PKT_PARAM_OFFSET(addr)       (((unsigned long)addr + 64) & ~0x3fUL)
+#define NLM_ASYNC_PTR_PARAM_OFFSET(addr)        (((unsigned long)addr + 64 + sizeof(struct nlm_crypto_pkt_param) + (MAX_FRAGS*16) + 64) & ~0x3fUL)
 #define NLM_IV_OFFSET(addr)			((unsigned long)addr + (PACKET_DESC_SIZE - 64))
 
 
+static int no_of_alg_registered = 0;
+
+
 
 extern uint32_t nlm_hal_send_msg3(uint32_t dst, uint32_t code, uint64_t data0, uint64_t data1, uint64_t data2);
 extern __inline__ uint32_t nlm_hal_recv_msg2(uint32_t dst, uint32_t *src, uint32_t *size, uint32_t *code, uint64_t *data0, uint64_t *data1);
+
 #ifdef NLM_CRYPTO_DEBUG
 extern void print_cntl_instr(uint64_t cntl_desc);
 extern void print_crypto_msg_desc(uint64_t entry1, uint64_t entry2, uint64_t entry3);
@@ -78,10 +74,6 @@ extern struct nlm_crypto_stat crypto_stat[MAX_CPU];
 
 
 
-//extern void print_cntl_instr(uint64_t cntl_desc);
-static void enc_session_cleanup(struct crypto_tfm *tfm)
-{
-}
 
 static int enc_cra_init(struct crypto_tfm *tfm)
 { 
@@ -91,10 +83,13 @@ static int enc_cra_init(struct crypto_tfm *tfm)
 
 static struct nlm_enc_ctx * nlm_crypto_ablkcipher_ctx(struct crypto_ablkcipher *tfm)
 {
-	return (struct  nlm_enc_ctx *)(((unsigned long)((uint8_t *)crypto_ablkcipher_ctx(tfm) + 63 )) & ~(0x3f));
+	return (struct  nlm_enc_ctx *)(((unsigned long)((uint8_t *)crypto_ablkcipher_ctx(tfm) + 63 )) & ~(0x3fUL));
 } 
 
-
+static struct nlm_enc_ctx * nlm_crypto_ablkcipher_ctx_2(struct crypto_ablkcipher *tfm)
+{
+	return (struct  nlm_enc_ctx *)(( unsigned long )(( uint8_t *)nlm_crypto_ablkcipher_ctx(tfm) + CTRL_DESC_SIZE + 63) & ~(0x3fUL));
+}
 
 static int
 xlp_setkey(struct crypto_ablkcipher *tfm, const u8 * in_key, unsigned int len, uint32_t cipher_alg, uint32_t cipher_mode,uint16_t stat)
@@ -108,16 +103,20 @@ xlp_setkey(struct crypto_ablkcipher *tfm, const u8 * in_key, unsigned int len, u
 	return 0;
 
 }
+
 static int
 xlp_setkey_des3(struct crypto_ablkcipher *tfm, const u8 * in_key, unsigned int len, uint32_t cipher_alg, uint32_t cipher_mode, uint16_t stat)
 {
-	struct nlm_enc_ctx * nlm_ctx = (struct  nlm_enc_ctx *)(( unsigned long )(( uint8_t *)nlm_crypto_ablkcipher_ctx(tfm) + CTRL_DESC_SIZE + 63) & ~(0x3f)); 
+	struct nlm_enc_ctx * nlm_ctx = nlm_crypto_ablkcipher_ctx_2(tfm); 
 	uint64_t key[3] ;
-	
-	memcpy(key,&in_key[16],8);
-        memcpy(&key[1],&in_key[8],8);
-        memcpy(&key[2],&in_key[0],8);
-
+	if( CHIP_SUPPORTS(DES3_KEY_SWAP) ) {
+		memcpy((uint8_t *)key,in_key,len);
+	}
+	else {
+		memcpy(key,&in_key[16],8);
+		memcpy(&key[1],&in_key[8],8);
+		memcpy(&key[2],&in_key[0],8);
+	}
 
 	nlm_crypto_fill_pkt_ctrl(&nlm_ctx->ctrl,0,0,0,cipher_alg,cipher_mode,0,(unsigned char*)&key[0],len,0,0);
 	crypto_ablkcipher_crt(tfm)->ivsize = cipher_mode_iv_len[cipher_alg][ cipher_mode];
@@ -127,7 +126,6 @@ xlp_setkey_des3(struct crypto_ablkcipher *tfm, const u8 * in_key, unsigned int l
 
 }
 
-
 static int
 xlp_des3_setkey(struct crypto_ablkcipher *tfm, const u8 * in_key, unsigned int len)
 {
@@ -197,6 +195,9 @@ xlp_aes_setkey(struct crypto_ablkcipher *tfm, const u8 *in_key,
 		crypto_ablkcipher_set_flags(tfm,flags);
 		return -EINVAL;
 	}
+	if ( mode == NLM_CIPHER_MODE_CTR )
+		stat += 3;
+
 	return xlp_setkey(tfm, in_key, len, cipher_alg, mode, stat);
 }
 
@@ -206,6 +207,24 @@ static int xlp_cbc_aes_setkey(struct crypto_ablkcipher *tfm, const u8 *key,
 	return xlp_aes_setkey(tfm,key,keylen,NLM_CIPHER_MODE_CBC);
 }
 
+static int xlp_ctr_rfc3686_setkey(struct crypto_ablkcipher *tfm, const u8 *key,
+                                 unsigned int keylen)
+{
+
+        int err;
+	struct nlm_enc_ctx  *nlm_ctx = ( struct nlm_enc_ctx  * )nlm_crypto_ablkcipher_ctx(tfm);
+	unsigned char  *nonce = &(nlm_ctx->nonce[0]); 
+
+        if (keylen < CTR_RFC3686_NONCE_SIZE)
+                return -EINVAL;
+        memcpy(nonce, key + (keylen - CTR_RFC3686_NONCE_SIZE),
+               CTR_RFC3686_NONCE_SIZE);
+        keylen -= CTR_RFC3686_NONCE_SIZE;
+        err = xlp_aes_setkey(tfm, key, keylen, NLM_CIPHER_MODE_CTR );
+
+        return err;
+}
+
 void enc_request_callback(struct nlm_async_crypto *async, uint64_t msg1 )
 {
 	struct crypto_async_request * base = (struct crypto_async_request *)async->args; 
@@ -221,108 +240,89 @@ void enc_request_callback(struct nlm_async_crypto *async, uint64_t msg1 )
 	}
 	crypto_stat[cpu].enc[stat]++;
 	crypto_stat[cpu].enc_tbytes[stat]+= async->bytes;
+	if ( async->pkt_param)
+		kfree(async->pkt_param);
 	
 	base->complete(base, err);
 }
+
 static int
 xlp_crypt(struct ablkcipher_request *req, unsigned int enc, int iv_size, struct nlm_crypto_pkt_ctrl *ctrl, uint16_t stat)
 {
 	int seg = 0;
-	int i;
 	uint64_t msg0, msg1;
-	struct scatterlist *sg;
-	struct scatter_walk walk;
-	uint8_t *virt;
-	int len;
 	int pktdescsize = 0;
-	
+	int fb_vc;
+#ifdef CONFIG_32BIT
+	unsigned long msgrng_flags;
+#endif
+	int node_sae_base;
+	int node ;
+	unsigned int max_frags= MAX_FRAGS;
 	unsigned int cipher_len = req->nbytes;
 	struct nlm_crypto_pkt_param * pkt_param = (struct nlm_crypto_pkt_param *) NLM_CRYPTO_PKT_PARAM_OFFSET(ablkcipher_request_ctx(req));
 	struct nlm_async_crypto * async =  (struct nlm_async_crypto *)NLM_ASYNC_PTR_PARAM_OFFSET(ablkcipher_request_ctx(req));;
-	int fb_vc;
+	int ret = -EINPROGRESS; 
+	int try = 0;
+
+	async->pkt_param = NULL;
 
 	nlm_crypto_fill_cipher_pkt_param(ctrl, pkt_param, enc,0,iv_size,iv_size ,req->nbytes); 
 
-	nlm_crypto_fill_src_seg(pkt_param,seg,(unsigned char *)req->info,iv_size);
-	nlm_crypto_fill_dst_seg(pkt_param,seg,(unsigned char *)req->info,iv_size);
+	nlm_crypto_fill_src_dst_seg(pkt_param,seg,MAX_FRAGS,(unsigned char *)req->info,iv_size);
 	seg++;
 
-	if ( req->src == req->dst) {
-		for (sg = req->src; cipher_len > 0; sg = scatterwalk_sg_next(sg)) {
-			len = min(cipher_len, sg->length);
-			scatterwalk_start(&walk, sg);
-			virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
-			nlm_crypto_fill_src_seg(pkt_param,seg,virt,len);
-			seg = nlm_crypto_fill_dst_seg(pkt_param,seg,virt,len);
-			cipher_len -= len;
-		}
-	}
-	else {
-		int nr_src_frags = 0;
-		int nr_dst_frags = 0;
-		int index = 0;
-		for (sg = req->src,index = seg;cipher_len > 0; sg = scatterwalk_sg_next(sg)) {
-			len = min(cipher_len, sg->length);
-			scatterwalk_start(&walk, sg);
-			virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
-			index = nlm_crypto_fill_src_seg(pkt_param,index,virt,len);
-			cipher_len -= len;
-		}
-		nr_src_frags = index;
+	do {
 		cipher_len = req->nbytes;
-		for (sg = req->dst, index = seg ;cipher_len > 0; sg = scatterwalk_sg_next(sg)) {
-			len = min(cipher_len, sg->length);
-			scatterwalk_start(&walk, sg);
-			virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
-			index = nlm_crypto_fill_dst_seg(pkt_param,index,virt,len);
-			cipher_len -= len;
-		}
-		nr_dst_frags = index;
+		seg = fill_src_dst_sg(req->src,req->dst,cipher_len,pkt_param,seg,max_frags,0);
 
-		if (nr_src_frags > nr_dst_frags) {
-			for (i = 0; i < nr_src_frags - nr_dst_frags; i++)
-				pkt_param->segment[seg + nr_dst_frags + i][1] = 0ULL;
-			seg = nr_src_frags;
+		if ( seg > max_frags ) {
+			max_frags = seg; 
+			seg = alloc_pkt_param(async,&pkt_param,max_frags);
 		}
-		else  { 
-			if (nr_src_frags < nr_dst_frags) {
-				for (i = 0; i < nr_dst_frags - nr_src_frags; i++)
-					pkt_param->segment[seg + nr_src_frags + i][0] = 0ULL;
-			}
-			seg = nr_dst_frags;
-		}
-		
-	}
+	}while(seg == 1 ); 
 
 	pktdescsize = 32 + seg * 16;
 
 	preempt_disable();
-	fb_vc = crypto_get_fb_vc(); 
+	fb_vc = crypto_get_fb_vc(&node); 
 
 	msg0 = nlm_crypto_form_pkt_fmn_entry0(fb_vc, 0, 0, ctrl->cipherkeylen,
-								virt_to_phys(ctrl));
+			virt_to_phys(ctrl));
 	msg1 = nlm_crypto_form_pkt_fmn_entry1(0,ctrl->hashkeylen, pktdescsize,
 				virt_to_phys(pkt_param));
 #ifdef NLM_CRYPTO_DEBUG
 	print_crypto_msg_desc(msg0,msg1,0xdeadbeef);
 	print_pkt_desc(pkt_param,seg);
-	print_cntl_instr(ctrl);
+	print_cntl_instr(ctrl->desc0);
 #endif
 	async->callback = &enc_request_callback;
 	async->args = &req->base;
 	async->stat = stat; 
 	async->bytes = req->nbytes; 
 	mb();
-	while( nlm_hal_send_msg3(crypto_vc_base, 0 /*code */ , msg0, msg1, (unsigned long )async) != 0 );
+#ifdef CONFIG_32BIT
+	msgrng_access_enable(msgrng_flags);
+#endif
+	node_sae_base = (node << NODE_BASE_SHIFT_BIT) | crypto_vc_base;
+	while( nlm_hal_send_msg3(node_sae_base, 0 /*code */ , msg0, msg1, (unsigned long )async) != 0 ){
+		if ( try++ > 16) {
+			ret = -EAGAIN;
+			break;
+		}
+	}
+#ifdef CONFIG_32BIT
+	msgrng_access_disable(msgrng_flags);
+#endif
 	preempt_enable();
-	return -EINPROGRESS;
+	return ret;
 }
 
 static int
 xlp_3des_cbc_decrypt( struct ablkcipher_request *req)
 {
       	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);
-	struct nlm_enc_ctx * nlm_ctx = (struct  nlm_enc_ctx *)(( unsigned long )(( uint8_t *)nlm_crypto_ablkcipher_ctx(tfm) + CTRL_DESC_SIZE + 63) & ~(0x3f)); 
+	struct nlm_enc_ctx * nlm_ctx = nlm_crypto_ablkcipher_ctx_2(tfm); 
 	return xlp_crypt(req, 0, DES3_EDE_BLOCK_SIZE,&nlm_ctx->ctrl,nlm_ctx->stat);
 }
 
@@ -352,6 +352,41 @@ xlp_cbc_encrypt( struct ablkcipher_request *req )
 	return xlp_crypt(req, 1, iv_size,&nlm_ctx->ctrl, nlm_ctx->stat);
 }
 
+static int crypto_rfc3686_crypt(struct ablkcipher_request *req,
+		unsigned int enc )
+{
+	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);
+	struct nlm_enc_ctx  *nlm_ctx = ( struct nlm_enc_ctx  * )nlm_crypto_ablkcipher_ctx(tfm);
+	u8 *iv = (uint8_t *)NLM_IV_OFFSET(ablkcipher_request_ctx(req));
+	int ret = 0;
+
+	/*  uniqueness is maintained by the req->info */
+	u8 *info = req->info;
+	unsigned char * nonce = &(nlm_ctx->nonce[0]); 
+
+	memcpy(iv, nonce, CTR_RFC3686_NONCE_SIZE);
+	memcpy(iv + CTR_RFC3686_NONCE_SIZE, info, CTR_RFC3686_IV_SIZE);
+
+	*(__be32 *)(iv + CTR_RFC3686_NONCE_SIZE + CTR_RFC3686_IV_SIZE) =
+				cpu_to_be32(1);
+	req->info = iv;
+	ret = xlp_crypt(req, enc, 16,&nlm_ctx->ctrl, nlm_ctx->stat);
+	req->info = info;
+	return ret;
+}
+
+static int
+xlp_ctr_rfc3686_decrypt( struct ablkcipher_request *req)
+{
+	return crypto_rfc3686_crypt(req, 0);
+}
+
+static int
+xlp_ctr_rfc3686_encrypt(struct ablkcipher_request *req)
+{
+	return crypto_rfc3686_crypt(req,1);
+}
+
 static struct crypto_alg xlp_cbc_aes_alg = {
 	.cra_name = "cbc(aes)",
 	.cra_driver_name = "cbc-aes-xlp",
@@ -363,7 +398,6 @@ static struct crypto_alg xlp_cbc_aes_alg = {
 	.cra_type = &crypto_ablkcipher_type,
 	.cra_module = THIS_MODULE,
 	.cra_init = enc_cra_init,
-	.cra_exit = enc_session_cleanup,
 	.cra_list = LIST_HEAD_INIT(xlp_cbc_aes_alg.cra_list),
 	.cra_u = {
 		.ablkcipher = {
@@ -388,7 +422,6 @@ static struct crypto_alg xlp_cbc_des_alg = {
 	.cra_type = &crypto_ablkcipher_type,
 	.cra_module = THIS_MODULE,
 	.cra_init = enc_cra_init,
-	.cra_exit = enc_session_cleanup,
 	.cra_list = LIST_HEAD_INIT(xlp_cbc_des_alg.cra_list),
 	.cra_u = {
 		.ablkcipher = {
@@ -413,7 +446,6 @@ static struct crypto_alg xlp_cbc_des3_alg = {
 	.cra_type = &crypto_ablkcipher_type,
 	.cra_module = THIS_MODULE,
 	.cra_init = enc_cra_init,
-	.cra_exit = enc_session_cleanup,
 	.cra_list = LIST_HEAD_INIT(xlp_cbc_des3_alg.cra_list),
 	.cra_u = {
 		.ablkcipher = {
@@ -427,10 +459,34 @@ static struct crypto_alg xlp_cbc_des3_alg = {
 	}
 };
 
+static struct crypto_alg xlp_ctr_aes_alg = {
+	.cra_name = "rfc3686(ctr(aes))",
+	.cra_driver_name = "rfc3686-ctr-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER|CRYPTO_ALG_ASYNC,
+	.cra_blocksize = 1,
+	.cra_ctxsize = CTRL_DESC_SIZE,
+	.cra_alignmask = 0,
+	.cra_type = &crypto_ablkcipher_type,
+	.cra_module = THIS_MODULE,
+	.cra_init = enc_cra_init,
+	.cra_list = LIST_HEAD_INIT(xlp_ctr_aes_alg.cra_list),
+	.cra_u = {
+		.ablkcipher = {
+			.min_keysize = AES_MIN_KEY_SIZE,
+			.max_keysize = AES_MAX_KEY_SIZE,
+			.setkey = xlp_ctr_rfc3686_setkey,
+			.encrypt = xlp_ctr_rfc3686_encrypt,
+			.decrypt = xlp_ctr_rfc3686_decrypt,
+			.ivsize = CTR_RFC3686_IV_SIZE,
+			.geniv = "seqiv",
+		}
+	}
+};
+
 int xlp_crypt_alg_init(void)
 {
 	int ret = 0;
-	int no_of_alg_registered = 0;
 	ret = crypto_register_alg(&xlp_cbc_des3_alg);
 	if (ret) {
 		goto end;
@@ -446,6 +502,11 @@ int xlp_crypt_alg_init(void)
 		goto end;
 	}
 	no_of_alg_registered++;
+	ret = crypto_register_alg(&xlp_ctr_aes_alg);
+	if (ret) {
+		goto end;
+	}
+	no_of_alg_registered++;
 	printk(KERN_NOTICE "Using XLP hardware for AES, DES, 3DES algorithms.\n");
 end:
 	return 0;
@@ -456,6 +517,7 @@ xlp_crypt_alg_fini(void) {
 	crypto_unregister_alg(&xlp_cbc_des3_alg);
 	crypto_unregister_alg(&xlp_cbc_des_alg);
 	crypto_unregister_alg(&xlp_cbc_aes_alg);
+	crypto_unregister_alg(&xlp_ctr_aes_alg);
 }
 
 EXPORT_SYMBOL(xlp_crypt_alg_init);
diff --git a/drivers/crypto/nlm-sae/nlmcrypto.h b/drivers/crypto/nlm-sae/nlmcrypto.h
new file mode 100755
index 0000000..f0e50c5
--- /dev/null
+++ b/drivers/crypto/nlm-sae/nlmcrypto.h
@@ -0,0 +1,990 @@
+/*********************************************************************
+
+  Copyright 2003-2010 Netlogic Microsystem, Inc. ("Netlogic"). All rights
+  reserved.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+  1. Redistributions of source code must retain the above copyright
+  notice, this list of conditions and the following disclaimer.
+  2. Redistributions in binary form must reproduce the above copyright
+  notice, this list of conditions and the following disclaimer in
+  the documentation and/or other materials provided with the
+  distribution.
+
+  THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems, Inc. ``AS IS'' AND
+  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL RMI OR CONTRIBUTORS BE LIABLE
+  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+  THE POSSIBILITY OF SUCH DAMAGE.
+
+  *****************************#NLM_2#**********************************/
+#ifndef _NLM_CRYPTO_H
+#define _NLM_CRYPTO_H
+
+#include "nlm_hal_crypto.h"
+/**
+* @file nlmcrypto.h
+*/
+
+
+/**
+* @brief Pointer to the ctx structure
+* @ingroup crypto
+*/
+typedef void nlm_crypto_ctx_t;
+
+/**
+* @brief Get the fd value associated with the context
+* @ingroup crypto
+*/
+#define nlm_crypto_get_ctxt_fd(ctxt) (*(int *)ctxt)
+
+/**
+* @brief Get the application arg associated with the context
+* @ingroup crypto
+*/
+#define nlm_crypto_get_ctxt_arg(ctxt) (*((unsigned long long *)ctxt + 1))
+
+/**
+* @brief Crypto return values
+* @ingroup crypto
+*/
+enum nlm_crypto_err { NLM_CRYPTO_OK = 0, NLM_CRYPTO_ERROR = -1, NLM_CRYPTO_EBUSY = -2 };
+
+/**
+* @brief Crypto modes
+* @ingroup crypto
+*/
+enum nlm_crypto_mode { NLM_CRYPTO_MODE_ASYNC = 1, NLM_CRYPTO_MODE_SYNC_EXLVC, NLM_CRYPTO_MODE_SYNC_SHDVC };
+
+/* Max value of this is 64, as we are using a uint64_t 
+   to find out the empty or nonemty status */
+#define NLM_CRYPTO_MAX_PENDING_REQS_PER_CTXT 64
+
+/* seglen is 16 bits */
+#define NLM_CRYPTO_MAX_SEG_LEN (64 * 1024) 
+
+enum chip_specific_features { 
+	ZUC = 0x1, 
+	DES3_KEY_SWAP = 0x2
+};
+extern int chip_feature ;
+#define CHIP_SUPPORTS(a) ( chip_feature & a )  
+
+#ifndef NLM_HAL_LINUX_KERNEL
+#define nlm_err_print(fmt, args...)  { \
+	        fprintf(stderr, fmt, ##args);   \
+	        fflush(stderr); \
+}
+#else
+#define nlm_err_print(fmt, args...)  { printk(fmt, ##args); }
+#endif
+#ifdef NLM_CRYPTO_DEBUG_EN
+#define nlm_dbg_print(fmt, args...)  fprintf(stderr, fmt, ##args);
+#else
+#define nlm_dbg_print(fmt, args...)  { }
+#endif
+
+#define crypto_swap64(x)   ( (unsigned long long)((x & 0x000000FF) << 56) | \
+		(unsigned long long)((x & 0x0000FF00) << 40) | \
+		(unsigned long long)((x & 0x00FF0000) << 24) | \
+		(unsigned long long)((x & 0xFF000000) <<  8) | \
+		((x & (unsigned long long)0xFF << 32) >>  8) | \
+		((x & (unsigned long long)0xFF << 40) >> 24) | \
+		((x & (unsigned long long)0xFF << 48) >> 40) | \
+		((x & (unsigned long long)0xFF << 56) >> 56)   \
+		)
+
+#if !defined(__MIPSEL__) && !defined(__MIPSEB__)
+	$(error ENDIANESS is not specified(__MIPSEL__ , __MIPSEB__))
+#endif
+
+#ifdef __MIPSEL__
+#define ccpu_to_be64(x) crypto_swap64(x)
+#else
+#define ccpu_to_be64(x) (x)
+#endif
+
+/* type value start and end */
+#define NLM_CRYPTO_ECCPRIME_TYPE_SVALUE 	0x0
+#define NLM_CRYPTO_ECCPRIME_TYPE_EVALUE 	0x8
+
+#define NLM_CRYPTO_ECCBIN_TYPE_SVALUE 		0x20
+#define NLM_CRYPTO_ECCBIN_TYPE_EVALUE 		0x28
+
+#define NLM_CRYPTO_RSA_TYPE_SVALUE 		0x40
+#define NLM_CRYPTO_RSA_TYPE_EVALUE 		0x44
+
+
+enum nlm_crypto_op_type_t { NLM_CRYPTO_RSA, NLM_CRYPTO_ECC, NLM_CRYPTO_PKT };
+
+/* RSA definitions */
+/* c = x^y mod n or c = x * y mod n */
+struct nlm_crypto_mod_exp { 
+	unsigned char	*x;
+	unsigned char	*y;
+	unsigned char	*n;
+};
+ 
+typedef struct nlm_crypto_mod_exp nlm_crypto_mod_exp_t;
+typedef struct nlm_crypto_mod_exp nlm_crypto_mod_mul_t;
+typedef struct nlm_crypto_mod_exp nlm_crypto_mod_add_t;
+typedef struct nlm_crypto_mod_exp nlm_crypto_mod_sub_t;
+typedef struct nlm_crypto_mod_exp nlm_crypto_mod_div_t;
+
+struct nlm_crypto_rsa_result
+{
+	unsigned char *r;
+};
+typedef struct nlm_crypto_rsa_result nlm_crypto_rsa_result_t;
+typedef struct nlm_crypto_rsa_result nlm_crypto_field_result_t;
+#define NLM_CRYPTO_RSA_RESULT_NELMNTS 1
+
+/* the values are same as of the encoding of fn field */
+enum nlm_crypto_rsa_op_t { NLM_CRYPTO_RSA_MOD_EXP = 0, NLM_CRYPTO_RSA_MOD_MUL, NLM_CRYPTO_RSA_MAX_OPS };
+
+/* The first 3 elements are same for rsa and ecc, do op can just typecast anyone of these
+ so don't change it */
+struct nlm_crypto_rsa_param   {
+	union {
+		nlm_crypto_mod_exp_t modexp;
+		nlm_crypto_mod_mul_t modmul;
+	};
+	/* result need to be allocated seperately and point it before doing do_op */
+	nlm_crypto_rsa_result_t *result;
+};
+
+#define NLM_CRYPTO_RSA_PARAMS_NELMNTS 3 /* x, y and n */
+
+struct nlm_crypto_rsa_ctrl  {
+	int op;
+	int blksz_nbits;
+};
+
+/* ECC defnitions , the values are same as of the encoding of fn field */
+enum nlm_crypto_ecc_op_t { 
+	NLM_CRYPTO_ECC_POINT_MUL = 0, NLM_CRYPTO_ECC_POINT_ADD, 
+	NLM_CRYPTO_ECC_POINT_DBL, NLM_CRYPTO_ECC_POINT_VERIFY, 
+	NLM_CRYPTO_ECC_FIELD_MOD_ADD, NLM_CRYPTO_ECC_FIELD_MOD_SUB,
+	NLM_CRYPTO_ECC_FIELD_MOD_MUL, NLM_CRYPTO_ECC_FIELD_MOD_DIV,
+	NLM_CRYPTO_ECC_FIELD_MOD_INV, NLM_CRYPTO_ECC_FIELD_MOD_RED,
+	NLM_CRYPTO_ECC_MAX_OPS
+};
+
+/* R(x, y) = k * P(x, y) , a(curve parameter), n(modulus)*/
+struct nlm_crypto_ecc_point_mul {  
+	unsigned char		*x;
+	unsigned char 		*y;
+	unsigned char		*a;
+	unsigned char		*k;
+	unsigned char	 	*n;
+};
+typedef struct nlm_crypto_ecc_point_mul nlm_crypto_ecc_point_mul_t;
+
+/* R(x, y) = P(x, y)  + Q(x, y), a(curve parameter), n(modulus)*/
+struct nlm_crypto_ecc_point_add {  
+	unsigned char		*xp;
+	unsigned char 		*yp;
+	unsigned char		*xq;
+	unsigned char 		*yq;
+	unsigned char		*a;
+	unsigned char	 	*n;
+};
+typedef struct nlm_crypto_ecc_point_add nlm_crypto_ecc_point_add_t;
+
+/* R(x, y) = 2 * P(x, y) , a(curve parameter), n(modulus)*/
+struct nlm_crypto_ecc_point_dbl {  
+	unsigned char		*x;
+	unsigned char 		*y;
+	unsigned char		*a;
+	unsigned char	 	*n;
+};
+typedef struct nlm_crypto_ecc_point_dbl nlm_crypto_ecc_point_dbl_t;
+
+/* P(x, y) , a(curve parameter), b(curve parameter), n(modulus)*/
+struct nlm_crypto_ecc_point_verify {  
+	unsigned char		*x;
+	unsigned char 		*y;
+	unsigned char		*a;
+	unsigned char	 	*b;
+	unsigned char	 	*n;
+};
+typedef struct nlm_crypto_ecc_point_verify nlm_crypto_ecc_point_verify_t;
+
+/* Modular inversion c = 1/x mod n , modular reduction c = x mod n*/
+struct nlm_crypto_mod_inv { 
+	unsigned char	*x;
+	unsigned char	*n;
+};
+typedef struct nlm_crypto_mod_inv nlm_crypto_mod_inv_t;
+typedef struct nlm_crypto_mod_inv nlm_crypto_mod_red_t;
+
+struct nlm_crypto_ecc_point_result 
+{
+	unsigned char *rx;
+	unsigned char *ry;
+};
+typedef struct nlm_crypto_ecc_point_result nlm_crypto_ecc_point_result_t;
+#define NLM_CRYPTO_ECC_RESULT_NELMNTS 2
+
+struct nlm_crypto_ecc_result {
+	union {
+		nlm_crypto_field_result_t 		fres;
+		nlm_crypto_ecc_point_result_t 		pres;
+	};
+};
+typedef struct nlm_crypto_ecc_result nlm_crypto_ecc_result_t;
+
+/* The first 3 elements are same for rsa and ecc, do op can just typecast anyone of these
+ so don't change it */
+struct  nlm_crypto_ecc_param {
+	union {
+		nlm_crypto_ecc_point_mul_t    	pmul;
+		nlm_crypto_ecc_point_add_t    	padd;
+		nlm_crypto_ecc_point_dbl_t    	pdbl;
+		nlm_crypto_ecc_point_verify_t	pverify;
+		nlm_crypto_mod_add_t		       	fadd;
+		nlm_crypto_mod_sub_t		       	fsub;
+		nlm_crypto_mod_mul_t		       	fmul;
+		nlm_crypto_mod_div_t		       	fdiv;
+		nlm_crypto_mod_inv_t		       	finv;
+		nlm_crypto_mod_red_t		       	fred;
+	};
+	/* result needs to be allocated seperately and point it here before doint do op*/
+	nlm_crypto_ecc_result_t 	*result;
+};
+
+struct nlm_crypto_ecc_ctrl  {
+	int op;
+	int blksz_nbits;
+	int prime; /* 1 for prime curvers, 0 for binary */
+};
+
+#define NLM_CRYPTO_MAX_RSA_BITS_LEN    8192
+#define NLM_CRYPTO_ECC_MAX_BLK_SIZE 576
+#define NLM_CRYPTO_ECC_PARAMS_NELMNTS 6
+
+#define NLM_CRYPTO_DEF_PARAM_ALIGNMENT	1024
+
+#ifndef NLM_ENCRYPT
+#define NLM_ENCRYPT 1
+#endif
+
+#ifndef NLM_DECRYPT
+#define NLM_DECRYPT 0
+#endif
+
+/**
+* @brief cipher algorithms
+* @ingroup crypto
+*/
+enum nlm_cipher_algo {
+	NLM_CIPHER_BYPASS = 0,
+	NLM_CIPHER_DES = 1,
+	NLM_CIPHER_3DES = 2,     
+	NLM_CIPHER_AES128 = 3,
+	NLM_CIPHER_AES192 = 4,
+	NLM_CIPHER_AES256 = 5, 	
+	NLM_CIPHER_ARC4 = 6,     
+	NLM_CIPHER_KASUMI_F8 = 7,
+	NLM_CIPHER_SNOW3G_F8 = 8,     
+	NLM_CIPHER_CAMELLIA128 = 9, 
+	NLM_CIPHER_CAMELLIA192 = 0xA, 
+	NLM_CIPHER_CAMELLIA256 = 0xB, 
+	NLM_CIPHER_UNDEFINED1 = 0xC,
+	NLM_CIPHER_ZUC = 0xD,
+	NLM_CIPHER_MAX = 0xE,
+};
+
+/**
+* @brief cipher modes
+* @ingroup crypto
+*/
+enum nlm_cipher_mode {
+	NLM_CIPHER_MODE_ECB = 0,
+	NLM_CIPHER_MODE_CBC = 1,
+	NLM_CIPHER_MODE_CFB = 2,
+	NLM_CIPHER_MODE_OFB = 3,
+	NLM_CIPHER_MODE_CTR = 4,
+	NLM_CIPHER_MODE_AES_F8 = 5,
+	NLM_CIPHER_MODE_GCM = 6,
+	NLM_CIPHER_MODE_CCM = 7,
+	NLM_CIPHER_MODE_UNDEFINED1 = 8,
+	NLM_CIPHER_MODE_UNDEFINED2 = 9,
+	NLM_CIPHER_MODE_LRW = 0xA,
+	NLM_CIPHER_MODE_XTS = 0xB,
+	NLM_CIPHER_MODE_MAX = 0xC,
+};
+
+/**
+* @brief hash algorithms
+* @ingroup crypto
+*/
+enum nlm_hash_algo {
+	NLM_HASH_BYPASS = 0,
+	NLM_HASH_MD5 = 1,
+	NLM_HASH_SHA = 2,
+	NLM_HASH_UNDEFINED = 3,
+	NLM_HASH_AES128 = 4,
+	NLM_HASH_AES192 = 5,
+	NLM_HASH_AES256 = 6,
+	NLM_HASH_KASUMI_F9 = 7,
+	NLM_HASH_SNOW3G_F9 = 8,
+	NLM_HASH_CAMELLIA128 = 9,
+	NLM_HASH_CAMELLIA192 = 0xA,
+	NLM_HASH_CAMELLIA256 = 0xB,
+	NLM_HASH_GHASH = 0xC,
+	NLM_HASH_ZUC = 0xD,
+	NLM_HASH_MAX = 0xE,
+};
+
+/**
+* @brief hash modes
+* @ingroup crypto
+*/
+enum nlm_hash_mode {
+	NLM_HASH_MODE_SHA1 = 0, 	/* Only SHA */
+	NLM_HASH_MODE_SHA224 = 1,       /* Only SHA */
+	NLM_HASH_MODE_SHA256 = 2,       /* Only SHA */
+	NLM_HASH_MODE_SHA384 = 3,       /* Only SHA */
+	NLM_HASH_MODE_SHA512 = 4,       /* Only SHA */
+	NLM_HASH_MODE_CMAC = 5, 	/* AES and Camellia */
+	NLM_HASH_MODE_XCBC = 6, 	/* AES and Camellia */
+	NLM_HASH_MODE_CBC_MAC = 7,      /* AES and Camellia */
+	NLM_HASH_MODE_CCM = 8,  	/* AES */
+	NLM_HASH_MODE_GCM = 9,  	/* AES */
+	NLM_HASH_MODE_MAX = 0xA,
+}; 
+
+#define MAX_KEY_LEN_IN_DW 58
+/**
+* @brief crypto control descriptor, should be cache aligned
+* @ingroup crypto
+*/
+struct nlm_crypto_pkt_ctrl {
+	unsigned long long desc0;
+	unsigned long long key[MAX_KEY_LEN_IN_DW]; /* combination of cipher and hash keys */
+	unsigned int cipherkeylen; 
+	unsigned int hashkeylen; 
+	unsigned int taglen;
+};
+
+/**
+* @brief crypto packet descriptor, should be cache aligned  
+* @ingroup crypto
+*/
+struct nlm_crypto_pkt_param {
+	unsigned long long desc0;
+ 	unsigned long long desc1;
+	unsigned long long desc2;
+	unsigned long long desc3;
+	unsigned long long segment[1][2];
+};
+
+#define NLM_CRYPTO_SEGS_ADDR_OFF 	0
+#define NLM_CRYPTO_SEGS_LEN_OFF 	48
+/* used internally */
+struct nlm_crypto_pkt_seg_desc {
+	unsigned long long src;
+	unsigned long long dst;
+};
+
+static inline int nlm_crypto_getnibble(unsigned char a) 
+{ 
+	if (a >= 'a' && a <= 'f')
+		return a - 'a' + 10;
+	if (a >= 'A' && a <= 'F')
+		return a - 'A' + 10;
+	return a - '0';
+}
+
+static inline void nlm_crypto_hex2bin(unsigned char *dst, unsigned char *src, int len)
+{
+	int i;
+
+	for (i = 0; i < len * 2; i = i + 2)
+		dst[i/2] = (nlm_crypto_getnibble(src[i]) << 4 ) | (nlm_crypto_getnibble(src[i + 1])) ;
+	return;
+}
+
+#define nlm_crypto_num_segs_reqd(bufsize) ((bufsize + NLM_CRYPTO_MAX_SEG_LEN - 1) / NLM_CRYPTO_MAX_SEG_LEN)
+#define nlm_crypto_desc_size(nsegs) (32 + (nsegs * 16))
+
+static inline int nlm_crypto_get_taglen(enum nlm_hash_algo hashalg, enum nlm_hash_mode hashmode)
+{
+	if(hashalg == NLM_HASH_MD5)
+		return 128;
+	else if(hashalg == NLM_HASH_SHA) {
+		switch(hashmode) {
+			case NLM_HASH_MODE_SHA1 : 
+				return 160;
+			case  NLM_HASH_MODE_SHA224 : 
+				return 224;
+			case NLM_HASH_MODE_SHA256 : 
+				return 256;
+			case NLM_HASH_MODE_SHA384 : 
+				return 384;
+			case NLM_HASH_MODE_SHA512 : 
+				return 512;
+			default:
+				nlm_err_print("Error : invalid shaid (%s)\n", __FUNCTION__);
+				return -1;
+		}
+	} else if (hashalg == NLM_HASH_SNOW3G_F9)  
+		return 32;
+	else if(hashmode == NLM_HASH_MODE_XCBC)
+		return 128;
+	else if(hashmode == NLM_HASH_MODE_GCM )
+		return 128;
+	else if(hashmode == NLM_HASH_MODE_CCM )
+		return 128;
+	else if (hashmode == NLM_HASH_MODE_CMAC)
+		return 128;
+	else if (hashalg ==  NLM_HASH_ZUC)
+		return 32;
+	else if(hashalg == NLM_HASH_BYPASS)
+		return 0;
+	else
+		nlm_err_print("Error : Hashalg not found, Tag length is setting to zero\n");
+
+	/* TODO : Add remining cases */
+	return 0; 
+}
+
+/**
+* @brief Generate fill cryto control info structure
+* @ingroup crypto
+* - hmac : 1 for hash with hmac 
+* - hashalg: see above,  hash_alg enums
+* - hashmode: see above, hash_mode enums
+* - cipherhalg: see above,  cipher_alg enums
+* - ciphermode: see above, cipher_mode enums
+* - keys_instr: 1 if the keys are specified in ascii values and it needs to be converted to binary form
+*/
+static inline int nlm_crypto_fill_pkt_ctrl(struct nlm_crypto_pkt_ctrl *ctrl, 
+		unsigned int hmac, 
+		enum nlm_hash_algo hashalg, enum nlm_hash_mode hashmode,
+		enum nlm_cipher_algo cipheralg, enum nlm_cipher_mode ciphermode,
+		unsigned int keys_instr,
+		unsigned char *cipherkey, unsigned int cipherkeylen, 
+		unsigned char *hashkey,   unsigned int hashkeylen)
+{
+	int taglen;
+	ctrl->desc0 = ccpu_to_be64(nlm_crypto_form_pkt_ctrl_desc(hmac, hashalg, hashmode, 
+			cipheralg, ciphermode, 0, 0, 0));
+	memset((char *)ctrl->key, 0, sizeof(ctrl->key));
+	if(cipherkey) {
+		if(keys_instr)
+			nlm_crypto_hex2bin((unsigned char *)ctrl->key, cipherkey, cipherkeylen);
+		else
+			memcpy((unsigned char *)ctrl->key, cipherkey, cipherkeylen);
+	}
+	if(hashkey) {
+		if(keys_instr)
+			nlm_crypto_hex2bin((unsigned char *)&ctrl->key[(cipherkeylen + 7) / 8],  hashkey, hashkeylen);
+		else
+			memcpy((unsigned char *)&ctrl->key[(cipherkeylen + 7) / 8],  hashkey, hashkeylen);
+	}
+	ctrl->cipherkeylen = cipherkeylen;
+	ctrl->hashkeylen = hashkeylen;
+	if((taglen = nlm_crypto_get_taglen(hashalg, hashmode)) < 0)
+		return -1;
+	ctrl->taglen = taglen;
+	
+	/* TODO : add the invalid checks and return error */
+	return 0;
+}
+
+#ifndef NLM_CRYPTO_EXCL_IFC
+#include "nlmcrypto_ifc.h"
+/**
+* @brief Top level function for generation pkt desc 0 to 3 for cipher auth
+* @ingroup crypto
+* - ctrl : pointer to control structure
+* - param : pointer to the param structure
+* - encrypt : 1(for encrypt) 0(for decrypt)
+* - hash_source : 1(encrypted data is sent to the auth engine) 0(plain data is sent to the auth engine)
+* - ivoff : iv offset from start of data
+* - ivlen : iv length in bytes
+* - hashoff : hash offset from start of data
+* - hashlen : hash length in bytes
+* - hmacpad : hmac padding required or not, 1 if already padded
+* - cipheroff : cipher offset from start of data
+* - cipherlen : cipher length in bytes
+* - hashdst_addr : hash destination physical address
+*/
+static inline void nlm_crypto_fill_cipher_auth_pkt_param(
+		struct nlm_crypto_pkt_ctrl *ctrl, struct nlm_crypto_pkt_param *param,
+		unsigned int encrypt, unsigned int hash_source,
+		unsigned int ivoff, unsigned int ivlen, 
+		unsigned int hashoff, unsigned int hashlen, unsigned int hmacpad,
+		unsigned int cipheroff, unsigned int chiperlen,
+		unsigned char *hashdst_addr)
+{
+	param->desc0 = ccpu_to_be64(nlm_crypto_form_pkt_desc0(0, hash_source, 1, encrypt, ivlen, crypto_virt_to_phys(hashdst_addr)));
+	param->desc1 = ccpu_to_be64(nlm_crypto_form_pkt_desc1(chiperlen, hashlen));
+	param->desc2 = ccpu_to_be64(nlm_crypto_form_pkt_desc2(ivoff, 0, cipheroff, 0, 0, hashoff));
+	param->desc3 = ccpu_to_be64(nlm_crypto_form_pkt_desc3(0, ctrl->taglen, 0, 0, hmacpad));
+	return;
+}
+
+/**
+* @brief Top level function for generation pkt desc 0 to 3 for cipher operation
+* @ingroup crypto
+* - ctrl : pointer to control structure
+* - param : pointer to the param structure
+* - encrypt : 1(for encrypt) 0(for decrypt)
+* - ivoff : iv offset from start of data
+* - ivlen : iv length in bytes
+* - cipheroff : cipher offset from start of data
+* - cipherlen : cipher length in bytes
+*/
+
+		
+static inline void nlm_crypto_fill_cipher_pkt_param(
+		struct nlm_crypto_pkt_ctrl *ctrl, struct nlm_crypto_pkt_param *param,
+		unsigned int encrypt, 
+		unsigned int ivoff, unsigned int ivlen, 
+		unsigned int cipheroff, unsigned int chiperlen )
+{
+	param->desc0 = ccpu_to_be64(nlm_crypto_form_pkt_desc0(0, 0, 0, encrypt, ivlen, 0ULL));
+	param->desc1 = ccpu_to_be64(nlm_crypto_form_pkt_desc1(chiperlen, 1));
+	param->desc2 = ccpu_to_be64(nlm_crypto_form_pkt_desc2(ivoff, 0, cipheroff, 0, 0, 0));
+	param->desc3 = ccpu_to_be64(nlm_crypto_form_pkt_desc3(0, ctrl->taglen, 0, 0, 0));
+
+	return;
+}
+
+/**
+* @brief Top level function for generation pkt desc 0 to 3 for auth operation
+* @ingroup crypto
+* - ctrl : pointer to control structure
+* - param : pointer to the param structure
+* - hashoff : hash offset from start of data
+* - hashlen : hash length in bytes
+* - hmacpad : hmac padding required or not, 1 if already padded
+* - hashdst_addr : hash destination physical address
+*/
+static inline void nlm_crypto_fill_auth_pkt_param(
+		struct nlm_crypto_pkt_ctrl *ctrl, struct nlm_crypto_pkt_param *param,
+		unsigned int hashoff, unsigned int hashlen, unsigned int hmacpad,
+		unsigned char *hashdst_addr)
+{
+
+	param->desc0 = ccpu_to_be64(nlm_crypto_form_pkt_desc0(0, 0, 1, 0, 1, crypto_virt_to_phys(hashdst_addr)));
+	param->desc1 = ccpu_to_be64(nlm_crypto_form_pkt_desc1(1, hashlen));
+	param->desc2 = ccpu_to_be64(nlm_crypto_form_pkt_desc2(0, 0, 0, 0, 0, hashoff));
+	param->desc3 = ccpu_to_be64(nlm_crypto_form_pkt_desc3(0, ctrl->taglen, 0, 0, hmacpad));
+
+
+	return;
+}
+
+/**
+* @brief Top level function for generating packet desc4 from source segments
+* @ingroup crypto
+* - param : pointer to the param structure
+* - s_seg : starting segment
+* - max_seg : maximum segments
+* - input : data start address
+* - inlen : data length
+*/
+static inline unsigned int nlm_crypto_fill_src_seg(struct nlm_crypto_pkt_param *param,  
+		unsigned int s_seg, unsigned int max_segs, unsigned char *input, unsigned int inlen)
+{
+	unsigned long long sval = 0ULL;
+	
+	return (crypto_fill_pkt_seg_paddr_len(input, inlen,  (struct nlm_crypto_pkt_seg_desc *)&param->segment[s_seg],
+			0, max_segs - s_seg,
+			1, 0,
+			sval, 0x0ULL) + s_seg);
+
+}
+/**
+* @brief Top level function for generating packet desc5 from cipher destination segments
+* @ingroup crypto
+* - param : pointer to the param structure
+* - s_seg : starting segment
+* - max_segs : maximum segments
+* - output : output start address
+* - outlen : output length
+*/
+static inline unsigned int nlm_crypto_fill_dst_seg(struct nlm_crypto_pkt_param *param, 
+		unsigned int s_seg, unsigned int max_segs, unsigned char *output, unsigned int outlen)
+{
+	/* By giving length 1, the length field will be cleared, Address and length gets modified in the below rtn */
+	unsigned long long dval = nlm_crypto_form_pkt_desc5(1, 1, 0, 0);
+	return (crypto_fill_pkt_seg_paddr_len(output, outlen,  (struct nlm_crypto_pkt_seg_desc *)&param->segment[s_seg],
+			0, max_segs - s_seg,
+			0, 1,
+			0x0ULL, dval) + s_seg);
+}
+
+/**
+* @brief Top level function for generating packet desc4 & desc5 from cipher destination segments
+* @ingroup crypto
+* - param : pointer to the param structure
+* - s_seg : starting segment
+* - max_segs : maximum segments
+* - output : output start address
+* - outlen : output length
+*/
+
+static inline unsigned int nlm_crypto_fill_src_dst_seg(struct nlm_crypto_pkt_param *param, 
+		unsigned int s_seg, unsigned int max_segs, unsigned char *output, unsigned int outlen)
+{
+	unsigned long long sval = 0ULL;
+	/* By giving length 1, the length field will be cleared, Address and length gets modified in the below rtn */
+	unsigned long long dval = nlm_crypto_form_pkt_desc5(1, 1, 0, 0);
+	return (crypto_fill_pkt_seg_paddr_len(output, outlen,  (struct nlm_crypto_pkt_seg_desc *)&param->segment[s_seg],
+			0, max_segs - s_seg,
+			1, 1,
+			sval, dval) + s_seg);
+}
+
+/**
+* @brief Top level function for modifying the packet hashlen and hashdstaddr in desc0 and desc1
+* @ingroup crypto
+* - param : pointer to the param structure
+* - hashlen : hash length in bytes
+* - hashdstaddr : hash destination physical address
+*/
+static inline void nlm_crypto_modify_auth_pkt_param(struct nlm_crypto_pkt_param *param, int hashlen, unsigned char * hashdstaddr)
+{
+	unsigned long long  desc;
+	desc =  ccpu_to_be64(param->desc0) & 0xffffff0000000000ULL;
+	param->desc0 = ccpu_to_be64((shift_lower_bits(crypto_virt_to_phys(hashdstaddr), 0, 40)|desc));
+	desc = ccpu_to_be64(param->desc1) & 0xffffffff00000000ULL;
+	param->desc1 = ccpu_to_be64( (shift_lower_bits_mask((hashlen - 1), 0, 32) | desc) );
+
+}
+
+/**
+* @brief Top level function for modifying the packet cipherlen, hashlen and hashdstaddr in desc0 and desc1
+* @ingroup crypto
+* - param : pointer to the param structure
+* - cipherlen : cipher length in bytes
+* - hashlen : hash length in bytes
+* - hashdstaddr : hash destination physical address
+*/
+static inline void nlm_crypto_modify_cipher_auth_pkt_param(struct nlm_crypto_pkt_param *param,
+		int cipherlen, int hashlen, unsigned char * hashdstaddr)
+{
+	unsigned long long  desc;
+	param->desc1 = ccpu_to_be64(nlm_crypto_form_pkt_desc1(cipherlen, hashlen));
+	desc =  ccpu_to_be64(param->desc0) & 0xFFFFFE0000000000ULL;
+	param->desc0 = ccpu_to_be64((shift_lower_bits(crypto_virt_to_phys(hashdstaddr), 0, 40)|desc));
+
+}
+#endif
+
+#ifndef MAX_CPUS
+#define MAX_CPUS		128
+#endif
+static inline int my_cpu_id(void)
+{
+	unsigned int pid = 0;
+
+	__asm__ volatile (".set push\n"
+			".set noreorder\n"
+			".set arch=xlp\n"
+			"rdhwr %0, $0\n"
+			".set pop\n"
+			: "=r" (pid)
+			:);
+
+	return pid;
+}
+
+typedef struct {
+	volatile unsigned int lock;
+} cryptolock_t;
+
+static __inline__ void crypto_lock_init(cryptolock_t *lock)
+{
+	lock->lock = 0;
+}
+
+static __inline__ void crypto_lock(cryptolock_t *lock)
+{
+	unsigned int tmp, pid;
+#ifdef NLM_HAL_LINUX_USER
+	pid = getpid();
+#else
+	pid = 1;
+#endif
+
+	__asm__ __volatile__(
+			".set 	push\n"
+			".set	noreorder\n"
+			"1:	ll %1, %2\n"
+			"bgtz	%1, 1b\n"
+			"move	%1, %3\n"
+			"sc	%1, %0\n"
+			"beqz	%1, 1b\n"
+			" sync	\n"
+			".set	pop\n"
+			: "=m" (lock->lock), "=&r" (tmp)
+			: "m" (lock->lock), "r" (pid)
+			: "memory");
+}
+
+static __inline__ void crypto_unlock(cryptolock_t *lock)
+{
+	__asm__ __volatile__(
+			".set 	push\n"
+			".set	noreorder\n"
+			"sync	\n"
+			"sw	$0, %0\n"
+			".set	pop\n"
+			: "=m" (lock->lock)
+			: "m" (lock->lock)
+			: "memory");
+}
+
+/* last bit set indication - from lsb to msb ,
+   clz instruction looks for number of cleared bit from msb to lsb
+   get_lbs(0) = 0, get_lbs(1) = 1, get_lbs(0x80000000) = 32
+ */
+static inline unsigned int crypto_get_lbs(unsigned int x)
+{
+	__asm__(".set push      \n"
+			".set mips32    \n"
+			"clz %0, %1     \n"
+			".set pop       \n"
+			:"=r" (x)
+			:"r" (x));
+	return 32 - x;
+}
+
+static inline int crypto_get_lbs64(unsigned long long x)
+{
+	__asm__(".set push	\n"
+		".set mips64	\n"
+		"dclz %0, %1	\n"
+		".set pop	\n"
+		: "=r" (x) 
+		: "r" (x));
+	return 64 - x;
+}
+
+/* find last bit cleared indication - from lsb to msb
+   clo instruction scans from msb to lsb for set bits 
+   get_lbc64(0) = 64, get_lbc64(0x80000000000000) = 63
+ */
+static inline int crypto_get_lbc64(unsigned long long x)
+{
+	__asm__(".set push	\n"
+		".set mips64	\n"
+		"dclo %0, %1	\n"
+		".set pop	\n"
+		: "=r" (x) 
+		: "r" (x));
+	return 64 - x;
+}
+
+/**
+* @brief Top level function for modifying the packet cipherlen in desc1
+* @ingroup crypto
+* - param : pointer to the param structure
+* - cipherlen : cipher length in bytes
+*/
+static inline void nlm_crypto_modify_cipher_pkt_param(struct nlm_crypto_pkt_param *param, int cipherlen)
+{
+	unsigned long long  desc;
+	desc = ccpu_to_be64(param->desc1) & 0x00000000ffffffffULL;
+	param->desc1 = ccpu_to_be64((shift_lower_bits_mask((cipherlen - 1), 32, 32) | desc) );
+
+
+}
+
+/**
+* @brief Top level function for adding designer descriptor at the end of src/dst fragments 
+* @ingroup crypto
+* - param : pointer to the param structure
+* - des_fb_start : designer feedback start segment  
+* - des_index : designer feedback index
+* - des_desc : designer feedback to be filled at the index specified by des_index
+*/
+static inline void nlm_crypto_fill_desfback_pkt_param(struct nlm_crypto_pkt_param *param, int des_fb_start, int des_index,unsigned long long des_desc)
+{
+
+	unsigned long long *seg = &param->segment[des_fb_start][0];
+	seg[des_index] = ccpu_to_be64(des_desc);
+	
+}
+
+
+/**
+* @brief Top level function for modifying cfb_mask in the control descriptor 
+* @ingroup crypto
+* - ctrl : pointer to control structure
+* - cfb_mask : cfb mask needed by the cfb_mode
+*/
+static inline void nlm_crypto_modify_cfbmask(struct nlm_crypto_pkt_ctrl *ctrl,int cfb_mask )
+{
+	unsigned long long desc =  ccpu_to_be64(ctrl->desc0);
+
+	desc = desc & 0xFFFFFFFFFFFFFFF8ULL;
+	desc = desc | shift_lower_bits(cfb_mask, 0, 3);
+	ctrl->desc0 = ccpu_to_be64(desc);
+}
+
+
+/**
+* @brief Top level function for modifying cipherbit_cnt, hashbit_cnt in the packet descriptor 
+* @ingroup crypto
+* - param : pointer to the param structure
+* - cipherbit_cnt : number of bits valid in the last cipher byte 
+* - hashbit_cnt: number of bits valid in the last hash byte
+*/
+static inline void nlm_crypto_modify_cipher_auth_bitcnt(struct nlm_crypto_pkt_param *param, int cipherbit_cnt, int hashbit_cnt)
+{
+	unsigned long long desc =  ccpu_to_be64(param->desc2); 
+	desc = desc & 0xFFFEE3FFFFC7FFFFULL;
+	desc = desc | shift_lower_bits(cipherbit_cnt, 42, 3) | shift_lower_bits(hashbit_cnt, 19, 3);
+	param->desc2 = ccpu_to_be64(desc);
+
+}
+
+
+/**
+* @brief Top level function for modifying for  packet descriptor for arc4 cipher
+* @ingroup crypto
+* - param : pointer to the param structure
+* - arc4_cipherkeylen : length of the arc4 cipher key
+* - arc4_keyinit : set to initialize the key for the first time
+* - arc4_state_save_l3 : sbox state is saved and transits through the L3 cache  
+* - arc4_save_state :  sbox state is saved 
+*/
+static inline void nlm_crypto_modify_arc4_cipher(struct nlm_crypto_pkt_ctrl *ctrl, struct nlm_crypto_pkt_param *param, 
+			int arc4_cipherkeylen, int arc4_keyinit, int arc4_state_save_l3, int arc4_save_state )
+{
+	unsigned long long desc =  ccpu_to_be64(ctrl->desc0);
+
+	desc = desc & 0xFFFFFFFFFF81FFFFULL;
+	desc = desc | shift_lower_bits(arc4_cipherkeylen, 18, 5) | shift_lower_bits(arc4_keyinit, 17, 1);
+	ctrl->desc0 = ccpu_to_be64(desc); 
+
+	desc =  ccpu_to_be64(param->desc3);
+	desc = desc & 0xFFFFFFFFFFFFFEBFULL;
+	desc = desc | shift_lower_bits(arc4_state_save_l3, 8, 1) | shift_lower_bits(arc4_save_state, 6, 1);
+	param->desc3 = ccpu_to_be64(desc);
+	
+}
+
+/**
+* @brief Top level function for modifying tls protocol enable 
+* @ingroup crypto
+* - param : pointer to the param structure
+* - tls : 1: enable 
+*/
+static inline void nlm_crypto_modify_tls_proto(struct nlm_crypto_pkt_param *param, int tls )
+{
+	unsigned long long desc =  ccpu_to_be64(param->desc0);
+	desc = (desc & 0x7FFFFFFFFFFFFFFFULL) | shift_lower_bits(tls, 63, 1);
+	param->desc0 = ccpu_to_be64(desc);
+}
+
+/**
+* @brief Top level function for modifying l3 alloc and clobber for auth 
+* @ingroup crypto
+* - param : pointer to the param structure
+* - hashout_l3alloc : 1,tag, when written out, is transited through the L3 cache
+* - hashclobber : output is written out as multiples of cache
+*                 lines. No read-modify-write is done.
+*/
+static inline void nlm_crypto_modify_auth_clobber_l3_alloc(struct nlm_crypto_pkt_param *param,
+	int hashout_l3alloc, int hashclobber )
+{
+	unsigned long long desc =  ccpu_to_be64(param->desc0);
+	desc = ( desc & 0xEFFFFFFFFFFFFFFF ) | shift_lower_bits(hashout_l3alloc, 60, 1);
+	param->desc0 = ccpu_to_be64(desc);
+	
+	desc = ccpu_to_be64(param->desc2);
+	desc = (desc & 0xFFFFFFFFFFFBFFFFULL ) | shift_lower_bits(hashclobber, 18, 1);;
+	param->desc2 = ccpu_to_be64(desc );
+}
+
+/**
+* @brief Top level function for modifying l3 alloc and clobber for cipher  
+* @ingroup crypto
+* - param : pointer to the param structure
+* - nsegs : Number of packet segments, not considered for rsa
+* - cipherout_l3alloc : output is saved, it transits through the L3 cache.
+* - cipherclobber : 1,output is written out as multiples of cache lines. No
+*                   read-modify-write is done. 
+*/
+static inline void nlm_crypto_modify_cipher_clobber_l3_alloc(struct nlm_crypto_pkt_param *param, int nsegs, int cipherout_l3alloc, int cipherclobber)
+{
+
+	int i;
+	unsigned long long desc;
+	for(i =0; i < nsegs; i++) {	
+		desc = ccpu_to_be64(param->segment[i][1]);	
+		desc = (desc & 0xFFFFBDFFFFFFFFFFULL ) | ( shift_lower_bits(cipherout_l3alloc, 46, 1) |
+					shift_lower_bits(cipherclobber, 41, 1));
+
+		param->segment[i][1] = ccpu_to_be64(desc ); 
+	}
+
+}
+
+
+#ifndef XLP_CACHELINE_SIZE
+#define XLP_CACHELINE_SIZE 64
+#endif 
+
+#ifndef NR_VCS_PER_THREAD
+#define NR_VCS_PER_THREAD 4
+#endif
+
+#define RSA_ERROR(x) ((x >> 53) & 0x1f)
+#define RSA_ENGINE(x) ( x >> 60)
+
+#define CRYPTO_ERROR(msg1) ((unsigned int)msg1)
+
+#define SAE_ERROR(etype, msg0, msg1) (etype == 1 ? CRYPTO_ERROR(msg1) : RSA_ERROR(msg0))
+
+#define NLM_MAX_NODES           4
+
+extern unsigned long xlp_rsa_base;
+extern unsigned long xlp_sae_base[NLM_MAX_NODES];
+int nlm_crypto_lib_init(void);
+int nlm_crypto_lib_finish(void);
+nlm_crypto_ctx_t *nlm_crypto_open_sync_session(int sync_mode, int cpu, void *arg);
+nlm_crypto_ctx_t *nlm_crypto_open_async_session(int max_outstanding_reqs, 
+		int (*callback)(nlm_crypto_ctx_t *ctxt, void *ctrl, void *param, void *arg), void *arg);
+int nlm_crypto_close_session(nlm_crypto_ctx_t *ctxt);
+void nlm_crypto_reset_session(nlm_crypto_ctx_t *ctxt);
+struct nlm_crypto_rsa_param *nlm_crypto_rsa_param_alloc(nlm_crypto_ctx_t *ctxt, enum nlm_crypto_rsa_op_t op, 
+		int blksz_nbits);
+struct  nlm_crypto_ecc_param *nlm_crypto_ecc_param_alloc(nlm_crypto_ctx_t *ctxt, 
+		enum nlm_crypto_ecc_op_t op, int blksz_nbits);
+void nlm_crypto_param_free(nlm_crypto_ctx_t *ctxt, void *param);
+struct nlm_crypto_rsa_result *nlm_crypto_rsa_result_alloc(nlm_crypto_ctx_t *ctxt, int blksz_nbits);
+struct nlm_crypto_ecc_result *nlm_crypto_ecc_result_alloc(nlm_crypto_ctx_t *ctxt,  
+		enum nlm_crypto_ecc_op_t op, int blksz_nbits);
+void nlm_crypto_result_free(nlm_crypto_ctx_t *ctxt, void *result);
+int nlm_crypto_do_op(nlm_crypto_ctx_t *ctxt, enum nlm_crypto_op_type_t optype, void *ctrl, 
+		void *param, int nsegs, void *arg);
+int nlm_crypto_rcv_op_result(nlm_crypto_ctx_t *ctxt, void **ctrl, void **param, void **arg);
+int nlm_crypto_aync_callback(enum nlm_crypto_op_type_t type, unsigned long long msg0, unsigned long long msg1);
+void nlm_crypto_get_configured_vc(int *rx_vc, int *rx_sync_vc);
+struct nlm_crypto_pkt_ctrl *nlm_crypto_pkt_ctrl_alloc(nlm_crypto_ctx_t *ctxt);
+struct nlm_crypto_pkt_param *nlm_crypto_pkt_param_alloc(nlm_crypto_ctx_t *ctxt, unsigned int nsegs);
+void nlm_crypto_pkt_ctrl_free(nlm_crypto_ctx_t *ctxt, void *ctrl);
+void *nlm_crypto_malign(unsigned int align, unsigned long long size);
+void nlm_crypto_mfree(void *ptr);
+void nlm_crypto_get_engine_vc(enum nlm_crypto_op_type_t type, int *base_vc, int *limit_vc);
+
+#endif
diff --git a/drivers/crypto/nlm-sae/nlmcrypto_ifc.h b/drivers/crypto/nlm-sae/nlmcrypto_ifc.h
index d0d89b8..e5dd44d 100644
--- a/drivers/crypto/nlm-sae/nlmcrypto_ifc.h
+++ b/drivers/crypto/nlm-sae/nlmcrypto_ifc.h
@@ -29,7 +29,6 @@
   *****************************#NLM_2#**********************************/
 #ifndef _NLM_CRYPTO_IFC_H
 #define _NLM_CRYPTO_IFC_H
-
 extern void *linuxu_shvaddr;
 extern unsigned long long linuxu_shoff ;
 
@@ -43,4 +42,27 @@ static inline void *crypto_phys_to_virt(unsigned long long paddr)
 	return phys_to_virt(paddr);
 }
 
+static inline int crypto_fill_pkt_seg_paddr_len(void *vaddr, unsigned int inlen, 
+	       struct nlm_crypto_pkt_seg_desc *segs, unsigned int s_seg, unsigned int max_segs,
+	       int fillsrc, int filldst, unsigned long long sinitval, unsigned long long dinitval)
+{
+	unsigned int remlen = inlen, sg = 0, len;
+	for(; remlen > 0;) {
+		if ( sg >= max_segs ) 
+			return -1;
+			
+		len = remlen > NLM_CRYPTO_MAX_SEG_LEN ? NLM_CRYPTO_MAX_SEG_LEN : remlen;
+		if(fillsrc)
+			segs[sg].src = ccpu_to_be64((virt_to_phys(vaddr) | 
+					((unsigned long long)(len - 1) << NLM_CRYPTO_SEGS_LEN_OFF) | sinitval));
+		if(filldst)
+			segs[sg].dst = ccpu_to_be64((virt_to_phys(vaddr) | 
+					((unsigned long long)(len - 1) << NLM_CRYPTO_SEGS_LEN_OFF) | dinitval));
+		remlen -= len;
+		vaddr += len;
+		sg++;
+	}
+	return sg;
+}
+
 #endif
-- 
1.7.0

