From b5bc6978697fac7530a2b41352c4dc37086c6bfd Mon Sep 17 00:00:00 2001
From: Yanjiang Jin <yanjiang.jin@windriver.com>
Date: Fri, 23 Mar 2012 11:55:33 +0800
Subject: [PATCH 16/47] nlm_xlp_64_be: add SAE support.

Add NetLogic's Security Acceleration Engine support,
based on NetLogic SDK 20120215_2.2.3 tag.

Signed-off-by: Alok Agrawat <aagrawat@netlogicmicro.com>
Integrated-off-by: Yanjiang Jin <yanjiang.jin@windriver.com>
---
 arch/mips/include/asm/netlogic/crypto/cryptodev.h  |  184 +++
 arch/mips/include/asm/netlogic/crypto/nlmcrypto.h  |  634 ++++++++++
 .../mips/include/asm/netlogic/hal/nlm_hal_crypto.h |  245 ++++
 drivers/crypto/Kconfig                             |   11 +
 drivers/crypto/Makefile                            |    2 +
 drivers/crypto/nlm-sae/Makefile                    |   13 +
 drivers/crypto/nlm-sae/nlm_aead.c                  | 1286 ++++++++++++++++++++
 drivers/crypto/nlm-sae/nlm_async.h                 |   86 ++
 drivers/crypto/nlm-sae/nlm_auth.c                  |  432 +++++++
 drivers/crypto/nlm-sae/nlm_crypto.c                |  610 ++++++++++
 drivers/crypto/nlm-sae/nlm_enc.c                   |  462 +++++++
 drivers/crypto/nlm-sae/nlmcrypto_ifc.h             |   46 +
 12 files changed, 4011 insertions(+), 0 deletions(-)
 create mode 100644 arch/mips/include/asm/netlogic/crypto/cryptodev.h
 create mode 100644 arch/mips/include/asm/netlogic/crypto/nlmcrypto.h
 create mode 100644 arch/mips/include/asm/netlogic/hal/nlm_hal_crypto.h
 create mode 100644 drivers/crypto/nlm-sae/Makefile
 create mode 100644 drivers/crypto/nlm-sae/nlm_aead.c
 create mode 100644 drivers/crypto/nlm-sae/nlm_async.h
 create mode 100644 drivers/crypto/nlm-sae/nlm_auth.c
 create mode 100644 drivers/crypto/nlm-sae/nlm_crypto.c
 create mode 100644 drivers/crypto/nlm-sae/nlm_enc.c
 create mode 100644 drivers/crypto/nlm-sae/nlmcrypto_ifc.h

diff --git a/arch/mips/include/asm/netlogic/crypto/cryptodev.h b/arch/mips/include/asm/netlogic/crypto/cryptodev.h
new file mode 100644
index 0000000..8dea01d
--- /dev/null
+++ b/arch/mips/include/asm/netlogic/crypto/cryptodev.h
@@ -0,0 +1,184 @@
+/*********************************************************************
+
+  Copyright 2003-2010 Netlogic Microsystem, Inc. ("Netlogic"). All rights
+  reserved.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+  1. Redistributions of source code must retain the above copyright
+  notice, this list of conditions and the following disclaimer.
+  2. Redistributions in binary form must reproduce the above copyright
+  notice, this list of conditions and the following disclaimer in
+  the documentation and/or other materials provided with the
+  distribution.
+
+  THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems, Inc. ``AS IS'' AND
+  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL RMI OR CONTRIBUTORS BE LIABLE
+  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+  THE POSSIBILITY OF SUCH DAMAGE.
+
+  *****************************#NLM_2#**********************************/
+#ifndef _NLM_CRYPTO_DEV_H
+#define _NLM_CRYPTO_DEV_H
+
+#define NLM_CRYPTO_DEV_NAME "nlmcrypto"
+
+enum crypto_ioctl_events { 
+		NLM_CRYPTO_GET_EVENTFD_CTXT = 1, NLM_CRYPTO_PUT_EVENTFD_CTXT,
+		NLM_CRYPTO_GET_RX_VC_NUMS, NLM_CRYPTO_GET_COMMON_SHM_ADDR_SZ,
+		NLM_CRYPTO_SHMMEM_ALLOC, NLM_CRYPTO_SHMMEM_FREE 
+};
+
+#ifndef XLP_CACHELINE_SIZE
+#define XLP_CACHELINE_SIZE 64
+#endif 
+
+/* type value start and end */
+#define NLM_CRYPTO_ECCPRIME_TYPE_SVALUE 	0x0
+#define NLM_CRYPTO_ECCPRIME_TYPE_EVALUE 	0x8
+
+#define NLM_CRYPTO_ECCBIN_TYPE_SVALUE 		0x20
+#define NLM_CRYPTO_ECCBIN_TYPE_EVALUE 		0x28
+
+#define NLM_CRYPTO_RSA_TYPE_SVALUE 		0x40
+#define NLM_CRYPTO_RSA_TYPE_EVALUE 		0x44
+
+/* In async when max_msgs > 1, the message send id is specified in 40th bit onwards
+ int the msg3 */
+#define NLM_CRYPTO_ASYNC_MSG_OUT_INDEX 40
+
+/* Can be used by the kernel/netos for storing some data */
+#define NLM_CRYPOT_CTX_PRIV_DATA_SZ    64
+
+/* 0 and 1 are return msg, 
+   2 contains param vaddr, 3 contain result address, used only in async mode */
+struct nlm_crypto_msg {
+	volatile uint64_t msg[5]; 
+};
+
+struct nlm_crypto_ctx {
+	/* This should present in the top, don't change it. 
+	   see get_ctx_fd function */
+	int fd; 
+	unsigned short mode;
+	unsigned short rx_vc;
+
+	/* This should present in the uint64_t + 1 position, don't change it. 
+	   see get_ctx_arg function */
+	uint64_t arg; 
+	
+	unsigned short cpu;
+	unsigned short max_msgs;
+	unsigned int pad;
+
+	unsigned int rsp_pend; /* number of pending responses */
+	unsigned int lock; /* */
+	unsigned int mhead; /* head and tail for async operation */
+	unsigned int mtail;
+
+	/* Can be used by the kernel/netos for storing some data,
+	   cannot be accessed by the application  */
+	unsigned char priv_data[NLM_CRYPOT_CTX_PRIV_DATA_SZ];
+
+	uint64_t fdctxt;
+	uint64_t async_callback;
+	struct nlm_crypto_msg msgs[1];
+};
+
+typedef struct {
+	volatile unsigned int lock;
+} cryptolock_t;
+
+static __inline__ void crypto_lock(cryptolock_t *lock)
+{
+	unsigned int tmp, pid;
+#ifdef NLM_HAL_LINUX_USER
+	pid = getpid();
+#else
+	pid = 1;
+#endif
+
+	__asm__ __volatile__(
+			".set\tnoreorder\t\t\t# crypto_lock\n"
+			"1:\tll\t%1, %2\n\t"
+			"bgtz\t%1, 1b\n\t"
+			"move\t%1, %3\n\t"
+			"sc\t%1, %0\n\t"
+			"beqz\t%1, 1b\n\t"
+			" sync\n\t"
+			".set\treorder"
+			: "=m" (lock->lock), "=&r" (tmp)
+			: "m" (lock->lock), "r" (pid)
+			: "memory");
+}
+
+static __inline__ void crypto_unlock(cryptolock_t *lock)
+{
+	__asm__ __volatile__(
+			".set\tnoreorder\t\t\t# crypto_unlock\n\t"
+			"sync\n\t"
+			"sw\t$0, %0\n\t"
+			".set\treorder"
+			: "=m" (lock->lock)
+			: "m" (lock->lock)
+			: "memory");
+}
+
+/* cleared bit indication, instruction scans from msb to lsb 
+ get_flc(0) = 0, get_flc(1) = 1, get_flc(0x80000000) = 32*/
+static inline int get_flc(uint32_t x)
+{
+	__asm__(".set push	\n"
+		".set mips32	\n"
+		"clz %0, %1	\n"
+		".set pop	\n"
+		:"=r" (x) 
+		:"r" (x));
+	return 32 - x;
+}
+
+/* cleared bit indication, instruction scans from msb to lsb 
+ get_flc(0) = 0, get_flc(1) = 1, */
+static inline int get_flc64(uint64_t x)
+{
+	__asm__(".set push	\n"
+		".set mips64	\n"
+		"dclz %0, %1	\n"
+		".set pop	\n"
+		: "=r" (x) 
+		: "r" (x));
+	return 64 - x;
+}
+
+
+
+extern int contig_memory_init(void *shmaddr, unsigned long long paddr, unsigned long long size);
+/* alignment */
+#define   crypto_align(x, y)     ((x) & (~((y)-1)))
+#define   crypto_roundup(x, y)   (crypto_align((x)+(y)-1, (y)))
+
+#define  fifo_full(t, nentries, h) (((t+1) % nentries) == h)
+#define  fifo_empty(t, h) (t == h)
+
+#ifdef NLM_CRYPTO_LINUX_U
+/* Linux specific calls, no need to provide by others */
+extern int app_set_affinity(int cpu);
+extern uint64_t crypto_get_eventfd_ctxt(uint64_t ctxtpaddr);
+extern int crypto_put_eventfd_ctxt(uint64_t ctxtpaddr);
+#else
+static inline int app_set_affinity(int cpu) { return 0; }
+static inline uint64_t crypto_get_eventfd_ctxt(uint64_t ctxtpaddr) { return 0; }
+static inline int crypto_put_eventfd_ctxt(uint64_t ctxtpaddr) { return 0; }
+#endif //NLM_CRYPTO_LINUX_U
+
+
+#endif
diff --git a/arch/mips/include/asm/netlogic/crypto/nlmcrypto.h b/arch/mips/include/asm/netlogic/crypto/nlmcrypto.h
new file mode 100644
index 0000000..d8f7db0
--- /dev/null
+++ b/arch/mips/include/asm/netlogic/crypto/nlmcrypto.h
@@ -0,0 +1,634 @@
+/*********************************************************************
+
+  Copyright 2003-2010 Netlogic Microsystem, Inc. ("Netlogic"). All rights
+  reserved.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+  1. Redistributions of source code must retain the above copyright
+  notice, this list of conditions and the following disclaimer.
+  2. Redistributions in binary form must reproduce the above copyright
+  notice, this list of conditions and the following disclaimer in
+  the documentation and/or other materials provided with the
+  distribution.
+
+  THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems, Inc. ``AS IS'' AND
+  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL RMI OR CONTRIBUTORS BE LIABLE
+  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+  THE POSSIBILITY OF SUCH DAMAGE.
+
+  *****************************#NLM_2#**********************************/
+#ifndef _NLM_CRYPTO_H
+#define _NLM_CRYPTO_H
+
+#include "nlm_hal_crypto.h"
+
+/**
+* @file_name nlmcrypto.h
+*/
+
+/**
+* @defgroup crypto  Crypto structures
+* @brief Description about the crypto structures and enums used. Also descritption about
+* the higher level apis which calls the hal crypto apis internally.
+*/
+
+/**
+* @brief Pointer to the ctx structure
+* @ingroup crypto
+*/
+typedef void nlm_crypto_ctx_t;
+
+/**
+* @brief Get the fd value associated with the context
+* @ingroup crypto
+*/
+#define nlm_crypto_get_ctxt_fd(ctxt) (*(int *)ctxt)
+
+/**
+* @brief Get the application arg associated with the context
+* @ingroup crypto
+*/
+#define nlm_crypto_get_ctxt_arg(ctxt) (*((unsigned long long *)ctxt + 1))
+
+#include "nlmcrypto_ifc.h"
+/**
+* @brief Crypto return values
+* @ingroup crypto
+*/
+enum nlm_crypto_err { NLM_CRYPTO_OK = 0, NLM_CRYPTO_ERROR = -1, NLM_CRYPTO_EBUSY = -2 };
+
+/**
+* @brief Crypto modes
+* @ingroup crypto
+*/
+enum nlm_crypto_mode { NLM_CRYPTO_MODE_ASYNC = 1, NLM_CRYPTO_MODE_SYNC_EXLVC, NLM_CRYPTO_MODE_SYNC_SHDVC };
+
+/* Max value of this is 64, as we are using a uint64_t 
+   to find out the empty or nonemty status */
+#define NLM_CRYPTO_MAX_OUT_REQS 64
+
+/* seglen is 16 bits */
+#define NLM_CRYPTO_MAX_SEG_LEN (64 * 1024) 
+
+#ifndef NLM_HAL_LINUX_KERNEL
+#define nlm_err_print(fmt, args...)  { \
+	        fprintf(stderr, fmt, ##args);   \
+	        fflush(stderr); \
+}
+#else
+#define nlm_err_print(fmt, args...)  { printk(fmt, ##args); }
+#endif
+#ifdef NLM_CRYPTO_DEBUG_EN
+#define nlm_dbg_print(fmt, args...)  fprintf(stderr, fmt, ##args);
+#else
+#define nlm_dbg_print(fmt, args...)  { }
+#endif
+
+
+
+enum nlm_crypto_op_type_t { NLM_CRYPTO_RSA, NLM_CRYPTO_ECC, NLM_CRYPTO_PKT };
+
+/* RSA definitions */
+/* c = x^y mod n or c = x * y mod n */
+struct nlm_crypto_mod_exp { 
+	unsigned char	*x;
+	unsigned char	*y;
+	unsigned char	*n;
+};
+ 
+typedef struct nlm_crypto_mod_exp nlm_crypto_mod_exp_t;
+typedef struct nlm_crypto_mod_exp nlm_crypto_mod_mul_t;
+typedef struct nlm_crypto_mod_exp nlm_crypto_mod_add_t;
+typedef struct nlm_crypto_mod_exp nlm_crypto_mod_sub_t;
+typedef struct nlm_crypto_mod_exp nlm_crypto_mod_div_t;
+
+struct nlm_crypto_rsa_result
+{
+	unsigned char *r;
+};
+typedef struct nlm_crypto_rsa_result nlm_crypto_rsa_result_t;
+typedef struct nlm_crypto_rsa_result nlm_crypto_field_result_t;
+#define NLM_CRYPTO_RSA_RESULT_NELMNTS 1
+
+/* the values are same as of the encoding of fn field */
+enum nlm_crypto_rsa_op_t { NLM_CRYPTO_RSA_MOD_EXP = 0, NLM_CRYPTO_RSA_MOD_MUL, NLM_CRYPTO_RSA_MAX_OPS };
+
+/* The first 3 elements are same for rsa and ecc, do op can just typecast anyone of these
+ so don't change it */
+struct nlm_crypto_rsa_param   {
+	union {
+		nlm_crypto_mod_exp_t modexp;
+		nlm_crypto_mod_mul_t modmul;
+	};
+	/* result need to be allocated seperately and point it before doing do_op */
+	nlm_crypto_rsa_result_t *result;
+};
+
+#define NLM_CRYPTO_RSA_PARAMS_NELMNTS 3 /* x, y and n */
+
+struct nlm_crypto_rsa_ctrl  {
+	int op;
+	int blksz_nbits;
+};
+
+/* ECC defnitions , the values are same as of the encoding of fn field */
+enum nlm_crypto_ecc_op_t { 
+	NLM_CRYPTO_ECC_POINT_MUL = 0, NLM_CRYPTO_ECC_POINT_ADD, 
+	NLM_CRYPTO_ECC_POINT_DBL, NLM_CRYPTO_ECC_POINT_VERIFY, 
+	NLM_CRYPTO_ECC_FIELD_MOD_ADD, NLM_CRYPTO_ECC_FIELD_MOD_SUB,
+	NLM_CRYPTO_ECC_FIELD_MOD_MUL, NLM_CRYPTO_ECC_FIELD_MOD_DIV,
+	NLM_CRYPTO_ECC_FIELD_MOD_INV, NLM_CRYPTO_ECC_FIELD_MOD_RED,
+	NLM_CRYPTO_ECC_MAX_OPS
+};
+
+/* R(x, y) = k * P(x, y) , a(curve parameter), n(modulus)*/
+struct nlm_crypto_ecc_point_mul {  
+	unsigned char		*x;
+	unsigned char 		*y;
+	unsigned char		*a;
+	unsigned char		*k;
+	unsigned char	 	*n;
+};
+typedef struct nlm_crypto_ecc_point_mul nlm_crypto_ecc_point_mul_t;
+
+/* R(x, y) = P(x, y)  + Q(x, y), a(curve parameter), n(modulus)*/
+struct nlm_crypto_ecc_point_add {  
+	unsigned char		*xp;
+	unsigned char 		*yp;
+	unsigned char		*xq;
+	unsigned char 		*yq;
+	unsigned char		*a;
+	unsigned char	 	*n;
+};
+typedef struct nlm_crypto_ecc_point_add nlm_crypto_ecc_point_add_t;
+
+/* R(x, y) = 2 * P(x, y) , a(curve parameter), n(modulus)*/
+struct nlm_crypto_ecc_point_dbl {  
+	unsigned char		*x;
+	unsigned char 		*y;
+	unsigned char		*a;
+	unsigned char	 	*n;
+};
+typedef struct nlm_crypto_ecc_point_dbl nlm_crypto_ecc_point_dbl_t;
+
+/* P(x, y) , a(curve parameter), b(curve parameter), n(modulus)*/
+struct nlm_crypto_ecc_point_verify {  
+	unsigned char		*x;
+	unsigned char 		*y;
+	unsigned char		*a;
+	unsigned char	 	*b;
+	unsigned char	 	*n;
+};
+typedef struct nlm_crypto_ecc_point_verify nlm_crypto_ecc_point_verify_t;
+
+/* Modular inversion c = 1/x mod n , modular reduction c = x mod n*/
+struct nlm_crypto_mod_inv { 
+	unsigned char	*x;
+	unsigned char	*n;
+};
+typedef struct nlm_crypto_mod_inv nlm_crypto_mod_inv_t;
+typedef struct nlm_crypto_mod_inv nlm_crypto_mod_red_t;
+
+struct nlm_crypto_ecc_point_result 
+{
+	unsigned char *rx;
+	unsigned char *ry;
+};
+typedef struct nlm_crypto_ecc_point_result nlm_crypto_ecc_point_result_t;
+#define NLM_CRYPTO_ECC_RESULT_NELMNTS 2
+
+struct nlm_crypto_ecc_result {
+	union {
+		nlm_crypto_field_result_t 		fres;
+		nlm_crypto_ecc_point_result_t 		pres;
+	};
+};
+typedef struct nlm_crypto_ecc_result nlm_crypto_ecc_result_t;
+
+/* The first 3 elements are same for rsa and ecc, do op can just typecast anyone of these
+ so don't change it */
+struct  nlm_crypto_ecc_param {
+	union {
+		nlm_crypto_ecc_point_mul_t    	pmul;
+		nlm_crypto_ecc_point_add_t    	padd;
+		nlm_crypto_ecc_point_dbl_t    	pdbl;
+		nlm_crypto_ecc_point_verify_t	pverify;
+		nlm_crypto_mod_add_t		       	fadd;
+		nlm_crypto_mod_sub_t		       	fsub;
+		nlm_crypto_mod_mul_t		       	fmul;
+		nlm_crypto_mod_div_t		       	fdiv;
+		nlm_crypto_mod_inv_t		       	finv;
+		nlm_crypto_mod_red_t		       	fred;
+	};
+	/* result needs to be allocated seperately and point it here before doint do op*/
+	nlm_crypto_ecc_result_t 	*result;
+};
+
+struct nlm_crypto_ecc_ctrl  {
+	int op;
+	int blksz_nbits;
+	int prime; /* 1 for prime curvers, 0 for binary */
+};
+
+#define NLM_CRYPTO_ECC_MAX_BLK_SIZE 576
+#define NLM_CRYPTO_ECC_PARAMS_NELMNTS 6
+
+
+/**
+* @brief cipher algorithms
+* @ingroup crypto
+*/
+enum nlm_cipher_algo {
+	NLM_CIPHER_BYPASS = 0,
+	NLM_CIPHER_DES = 1,
+	NLM_CIPHER_3DES = 2,     
+	NLM_CIPHER_AES128 = 3,
+	NLM_CIPHER_AES192 = 4,
+	NLM_CIPHER_AES256 = 5, 	
+	NLM_CIPHER_ARC4 = 6,     
+	NLM_CIPHER_KASUMI_F8 = 7,
+	NLM_CIPHER_SNOW3G_F8 = 8,     
+	NLM_CIPHER_CAMELLIA128 = 9, 
+	NLM_CIPHER_CAMELLIA192 = 0xA, 
+	NLM_CIPHER_CAMELLIA256 = 0xB, 
+	NLM_CIPHER_MAX = 0xC,
+};
+
+/**
+* @brief cipher modes
+* @ingroup crypto
+*/
+enum nlm_cipher_mode {
+	NLM_CIPHER_MODE_ECB = 0,
+	NLM_CIPHER_MODE_CBC = 1,
+	NLM_CIPHER_MODE_CFB = 2,
+	NLM_CIPHER_MODE_OFB = 3,
+	NLM_CIPHER_MODE_CTR = 4,
+	NLM_CIPHER_MODE_AES_F8 = 5,
+	NLM_CIPHER_MODE_GCM = 6,
+	NLM_CIPHER_MODE_CCM = 7,
+	NLM_CIPHER_MODE_UNDEFINED1 = 8,
+	NLM_CIPHER_MODE_UNDEFINED2 = 9,
+	NLM_CIPHER_MODE_LRW = 0xA,
+	NLM_CIPHER_MODE_XTS = 0xB,
+	NLM_CIPHER_MODE_MAX = 0xC,
+};
+
+/**
+* @brief hash algorithms
+* @ingroup crypto
+*/
+enum nlm_hash_algo {
+	NLM_HASH_BYPASS = 0,
+	NLM_HASH_MD5 = 1,
+	NLM_HASH_SHA = 2,
+	NLM_HASH_UNDEFINED = 3,
+	NLM_HASH_AES128 = 4,
+	NLM_HASH_AES192 = 5,
+	NLM_HASH_AES256 = 6,
+	NLM_HASH_KASUMI_F9 = 7,
+	NLM_HASH_SNOW3G_F9 = 8,
+	NLM_HASH_CAMELLIA128 = 9,
+	NLM_HASH_CAMELLIA192 = 0xA,
+	NLM_HASH_CAMELLIA256 = 0xB,
+	NLM_HASH_GHASH = 0xC,
+	NLM_HASH_MAX = 0xD
+};
+
+/**
+* @brief hash modes
+* @ingroup crypto
+*/
+enum nlm_hash_mode {
+	NLM_HASH_MODE_SHA1 = 0, 	/* Only SHA */
+	NLM_HASH_MODE_SHA224 = 1,       /* Only SHA */
+	NLM_HASH_MODE_SHA256 = 2,       /* Only SHA */
+	NLM_HASH_MODE_SHA384 = 3,       /* Only SHA */
+	NLM_HASH_MODE_SHA512 = 4,       /* Only SHA */
+	NLM_HASH_MODE_CMAC = 5, 	/* AES and Camellia */
+	NLM_HASH_MODE_XCBC = 6, 	/* AES and Camellia */
+	NLM_HASH_MODE_CBC_MAC = 7,      /* AES and Camellia */
+	NLM_HASH_MODE_CCM = 8,  	/* AES */
+	NLM_HASH_MODE_GCM = 9,  	/* AES */
+	NLM_HASH_MODE_MAX = 0xA,
+}; 
+
+#define MAX_KEY_LEN_IN_DW 20
+/**
+* @brief crypto control descriptor, should be cache aligned
+* @ingroup crypto
+*/
+struct nlm_crypto_pkt_ctrl {
+	unsigned long long desc0;
+	unsigned long long key[MAX_KEY_LEN_IN_DW]; /* combination of cipher and hash keys */
+	unsigned int cipherkeylen; 
+	unsigned int hashkeylen; 
+	unsigned int taglen;
+};
+
+/**
+* @brief crypto packet descriptor, should be cache aligned  
+* @ingroup crypto
+*/
+struct nlm_crypto_pkt_param {
+	unsigned long long desc0;
+ 	unsigned long long desc1;
+	unsigned long long desc2;
+	unsigned long long desc3;
+	unsigned long long segment[1][2];
+};
+
+
+static inline int nlm_crypto_getnibble(unsigned char a) 
+{ 
+	if (a >= 'a' && a <= 'f')
+		return a - 'a' + 10;
+	if (a >= 'A' && a <= 'F')
+		return a - 'A' + 10;
+	return a - '0';
+}
+
+static inline void nlm_crypto_hex2bin(unsigned char *dst, unsigned char *src, int len)
+{
+	int i;
+
+	for (i = 0; i < len * 2; i = i + 2)
+		dst[i/2] = (nlm_crypto_getnibble(src[i]) << 4 ) | (nlm_crypto_getnibble(src[i + 1])) ;
+	return;
+}
+
+#define nlm_crypto_num_segs_reqd(bufsize) ((bufsize + NLM_CRYPTO_MAX_SEG_LEN - 1) / NLM_CRYPTO_MAX_SEG_LEN)
+#define nlm_crypto_desc_size(nsegs) (32 + (nsegs * 16))
+
+static inline int nlm_crypto_get_taglen(enum nlm_hash_algo hashalg, enum nlm_hash_mode hashmode)
+{
+	if(hashalg == NLM_HASH_MD5)
+		return 128;
+	else if(hashalg == NLM_HASH_SHA) {
+		switch(hashmode) {
+			case NLM_HASH_MODE_SHA1 : 
+				return 160;
+			case  NLM_HASH_MODE_SHA224 : 
+				return 224;
+			case NLM_HASH_MODE_SHA256 : 
+				return 256;
+			case NLM_HASH_MODE_SHA384 : 
+				return 384;
+			case NLM_HASH_MODE_SHA512 : 
+				return 512;
+			default:
+				nlm_err_print("Error : invalid shaid (%s)\n", __FUNCTION__);
+				return -1;
+		}
+	} else if (hashalg == NLM_HASH_SNOW3G_F9)  
+		return 32;
+	else if(hashmode == NLM_HASH_MODE_XCBC)
+		return 128;
+	else if(hashalg == NLM_HASH_BYPASS)
+		return 0;
+	else
+		nlm_err_print("Error : Hashalg not found, Tag length is setting to zero\n");
+
+	/* TODO : Add remining cases */
+	return 0; 
+}
+
+/**
+* @brief Generate fill cryto control info structure
+* @ingroup crypto
+* - hmac : 1 for hash with hmac 
+* - hashalg: see above,  hash_alg enums
+* - hashmode: see above, hash_mode enums
+* - cipherhalg: see above,  cipher_alg enums
+* - ciphermode: see above, cipher_mode enums
+* - keys_instr: 1 if the keys are specified in ascii values and it needs to be converted to binary form
+*/
+static inline int nlm_crypto_fill_pkt_ctrl(struct nlm_crypto_pkt_ctrl *ctrl, 
+		unsigned int hmac, 
+		enum nlm_hash_algo hashalg, enum nlm_hash_mode hashmode,
+		enum nlm_cipher_algo cipheralg, enum nlm_cipher_mode ciphermode,
+		unsigned int keys_instr,
+		unsigned char *cipherkey, unsigned int cipherkeylen, 
+		unsigned char *hashkey,   unsigned int hashkeylen)
+{
+	int taglen;
+	ctrl->desc0 = nlm_crypto_form_pkt_ctrl_desc(hmac, hashalg, hashmode, 
+			cipheralg, ciphermode, 0, 0, 0);
+	memset((char *)ctrl->key, 0, sizeof(ctrl->key));
+	if(cipherkey) {
+		if(keys_instr)
+			nlm_crypto_hex2bin((unsigned char *)ctrl->key, cipherkey, cipherkeylen);
+		else
+			memcpy((unsigned char *)ctrl->key, cipherkey, cipherkeylen);
+	}
+	if(hashkey) {
+		if(keys_instr)
+			nlm_crypto_hex2bin((unsigned char *)&ctrl->key[(cipherkeylen + 7) / 8],  hashkey, hashkeylen);
+		else
+			memcpy((unsigned char *)&ctrl->key[(cipherkeylen + 7) / 8],  hashkey, hashkeylen);
+	}
+	ctrl->cipherkeylen = cipherkeylen;
+	ctrl->hashkeylen = hashkeylen;
+	if((taglen = nlm_crypto_get_taglen(hashalg, hashmode)) < 0)
+		return -1;
+	ctrl->taglen = taglen;
+	
+	/* TODO : add the invalid checks and return error */
+	return 0;
+}
+
+/**
+* @brief Top level function for generation pkt desc 0 to 3 for cipher auth
+* @ingroup crypto
+* - ctrl : pointer to control structure
+* - param : pointer to the param structure
+* - encrypt : 1(for encrypt) 0(for decrypt)
+* - hash_source : 1(encrypted data is sent to the auth engine) 0(plain data is sent to the auth engine)
+* - ivoff : iv offset from start of data
+* - ivlen : iv length in bytes
+* - hashoff : hash offset from start of data
+* - hashlen : hash length in bytes
+* - hmacpad : hmac padding required or not, 1 if already padded
+* - cipheroff : cipher offset from start of data
+* - cipherlen : cipher length in bytes
+* - hashdst_addr : hash destination physical address
+*/
+static inline void nlm_crypto_fill_cipher_auth_pkt_param(
+		struct nlm_crypto_pkt_ctrl *ctrl, struct nlm_crypto_pkt_param *param,
+		unsigned int encrypt, unsigned int hash_source,
+		unsigned int ivoff, unsigned int ivlen, 
+		unsigned int hashoff, unsigned int hashlen, unsigned int hmacpad,
+		unsigned int cipheroff, unsigned int chiperlen,
+		unsigned char *hashdst_addr)
+{
+	param->desc0 = nlm_crypto_form_pkt_desc0(0, hash_source, 1, encrypt, ivlen, crypto_virt_to_phys(hashdst_addr));
+	param->desc1 = nlm_crypto_form_pkt_desc1(chiperlen, hashlen);
+	param->desc2 = nlm_crypto_form_pkt_desc2(ivoff, 0, cipheroff, 0, 0, hashoff);
+	param->desc3 = nlm_crypto_form_pkt_desc3(0, ctrl->taglen, 0, 0, hmacpad);
+	return;
+}
+
+/**
+* @brief Top level function for generation pkt desc 0 to 3 for cipher operation
+* @ingroup crypto
+* - ctrl : pointer to control structure
+* - param : pointer to the param structure
+* - encrypt : 1(for encrypt) 0(for decrypt)
+* - ivoff : iv offset from start of data
+* - ivlen : iv length in bytes
+* - cipheroff : cipher offset from start of data
+* - cipherlen : cipher length in bytes
+*/
+
+		
+static inline void nlm_crypto_fill_cipher_pkt_param(
+		struct nlm_crypto_pkt_ctrl *ctrl, struct nlm_crypto_pkt_param *param,
+		unsigned int encrypt, 
+		unsigned int ivoff, unsigned int ivlen, 
+		unsigned int cipheroff, unsigned int chiperlen )
+{
+	param->desc0 = nlm_crypto_form_pkt_desc0(0, 0, 0, encrypt, ivlen, 0ULL);
+	param->desc1 = nlm_crypto_form_pkt_desc1(chiperlen, 1);
+	param->desc2 = nlm_crypto_form_pkt_desc2(ivoff, 0, cipheroff, 0, 0, 0);
+	param->desc3 = nlm_crypto_form_pkt_desc3(0, ctrl->taglen, 0, 0, 0);
+
+	return;
+}
+
+/**
+* @brief Top level function for generation pkt desc 0 to 3 for auth operation
+* @ingroup crypto
+* - ctrl : pointer to control structure
+* - param : pointer to the param structure
+* - hashoff : hash offset from start of data
+* - hashlen : hash length in bytes
+* - hmacpad : hmac padding required or not, 1 if already padded
+* - hashdst_addr : hash destination physical address
+*/
+static inline void nlm_crypto_fill_auth_pkt_param(
+		struct nlm_crypto_pkt_ctrl *ctrl, struct nlm_crypto_pkt_param *param,
+		unsigned int hashoff, unsigned int hashlen, unsigned int hmacpad,
+		unsigned char *hashdst_addr)
+{
+	param->desc0 = nlm_crypto_form_pkt_desc0(0, 0, 1, 0, 1, crypto_virt_to_phys(hashdst_addr));
+	param->desc1 = nlm_crypto_form_pkt_desc1(1, hashlen);
+	param->desc2 = nlm_crypto_form_pkt_desc2(0, 0, 0, 0, 0, hashoff);
+	param->desc3 = nlm_crypto_form_pkt_desc3(0, ctrl->taglen, 0, 0, hmacpad);
+
+	return;
+}
+
+/**
+* @brief Top level function for generating packet desc4 from source segments
+* @ingroup crypto
+* - param : pointer to the param structure
+* - seg : starting segment
+* - input : segment start address
+* - inlen : segment length
+*/
+static inline unsigned int nlm_crypto_fill_src_seg(struct nlm_crypto_pkt_param *param,  
+		int seg, unsigned char *input, unsigned int inlen)
+{
+	unsigned off = 0, len = 0;
+	unsigned int remlen = inlen;
+
+	for(; remlen > 0;) {
+		len = remlen > NLM_CRYPTO_MAX_SEG_LEN ? NLM_CRYPTO_MAX_SEG_LEN : remlen;
+		param->segment[seg][0] = nlm_crypto_form_pkt_desc4(len,  crypto_virt_to_phys((input + off)));
+		remlen -= len;
+		off += len;
+		seg++;
+	}
+	return seg;
+}
+
+/**
+* @brief Top level function for generating packet desc5 from cipher destination segments
+* @ingroup crypto
+* - param : pointer to the param structure
+* - seg : starting segment
+* - output : segment start address
+* - outlen : segment length
+*/
+static inline unsigned int nlm_crypto_fill_dst_seg(struct nlm_crypto_pkt_param *param, 
+		int seg, unsigned char *output, unsigned int outlen)
+{
+	unsigned off = 0, len = 0;
+	unsigned int remlen = outlen;
+
+	for(; remlen > 0;) {
+		len = remlen > NLM_CRYPTO_MAX_SEG_LEN ? NLM_CRYPTO_MAX_SEG_LEN : remlen;
+		param->segment[seg][1] = nlm_crypto_form_pkt_desc5(len, 1, 0, crypto_virt_to_phys(output + off));
+		remlen -= len;
+		off += len;
+		seg++;
+	}
+	return seg;
+}
+
+#ifndef MAX_CPUS
+#define MAX_CPUS		128
+#endif
+static inline int my_cpu_id(void)
+{
+	unsigned int pid = 0;
+
+	__asm__ volatile (".set push\n"
+			".set noreorder\n"
+			".set arch=xlp\n"
+			"rdhwr %0, $0\n"
+			".set pop\n"
+			: "=r" (pid)
+			:);
+
+	return pid;
+}
+
+#ifndef XLP_CACHELINE_SIZE
+#define XLP_CACHELINE_SIZE 64
+#endif 
+
+#ifndef NR_VCS_PER_THREAD
+#define NR_VCS_PER_THREAD 4
+#endif
+
+#define RSA_ERROR(x) ((x >> 53) & 0x1f)
+#define RSA_ENGINE(x) ( x >> 60)
+
+#define CRYPTO_ERROR(msg1) ((unsigned int)msg1)
+
+#define SAE_ERROR(etype, msg0, msg1) (etype == 1 ? CRYPTO_ERROR(msg1) : RSA_ERROR(msg0))
+
+
+int nlm_crypto_lib_init(void);
+nlm_crypto_ctx_t *nlm_crypto_open_sync_session(int sync_mode, int cpu, void *arg);
+nlm_crypto_ctx_t *nlm_crypto_open_async_session(int max_outstanding_reqs, 
+		int (*callback)(nlm_crypto_ctx_t *ctxt, void *ctrl, void *param, void *arg), void *arg);
+int nlm_crypto_close_session(nlm_crypto_ctx_t *ctxt);
+void nlm_crypto_reset_session(nlm_crypto_ctx_t *ctxt);
+struct nlm_crypto_rsa_param *nlm_crypto_rsa_param_alloc(nlm_crypto_ctx_t *ctxt, enum nlm_crypto_rsa_op_t op, 
+		int blksz_nbits);
+struct  nlm_crypto_ecc_param *nlm_crypto_ecc_param_alloc(nlm_crypto_ctx_t *ctxt, 
+		enum nlm_crypto_ecc_op_t op, int blksz_nbits);
+void nlm_crypto_param_free(nlm_crypto_ctx_t *ctxt, void *param);
+struct nlm_crypto_rsa_result *nlm_crypto_rsa_result_alloc(nlm_crypto_ctx_t *ctxt, int blksz_nbits);
+struct nlm_crypto_ecc_result *nlm_crypto_ecc_result_alloc(nlm_crypto_ctx_t *ctxt,  
+		enum nlm_crypto_ecc_op_t op, int blksz_nbits);
+void nlm_crypto_result_free(nlm_crypto_ctx_t *ctxt, void *result);
+int nlm_crypto_do_op(nlm_crypto_ctx_t *ctxt, enum nlm_crypto_op_type_t optype, void *ctrl, 
+		void *param, int nsegs, void *arg);
+int nlm_crypto_rcv_op_result(nlm_crypto_ctx_t *ctxt, void **ctrl, void **param, void **arg);
+int nlm_crypto_aync_callback(enum nlm_crypto_op_type_t type, unsigned long long msg0, unsigned long long msg1);
+void nlm_crypto_get_configured_vc(int *rx_vc, int *rx_sync_vc);
+
+#endif
diff --git a/arch/mips/include/asm/netlogic/hal/nlm_hal_crypto.h b/arch/mips/include/asm/netlogic/hal/nlm_hal_crypto.h
new file mode 100644
index 0000000..319b4f3
--- /dev/null
+++ b/arch/mips/include/asm/netlogic/hal/nlm_hal_crypto.h
@@ -0,0 +1,245 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#NETL_2#********************************/
+
+#ifndef _NLM_HAL_CRYPTO_H_
+#define _NLM_HAL_CRYPTO_H_
+
+#define shift_lower_bits(x, bitshift, numofbits) ((unsigned long long)(x) << (bitshift))
+
+#define shift_lower_bits_mask(x, bitshift, numofbits) (((unsigned long long)(x) & ((1ULL << (numofbits)) - 1)) << (bitshift))
+
+/**
+* @file_name nlm_hal_crypto.c
+*/
+
+/**
+* @defgroup crypto  Crypto HAL apis
+* @brief Description about the crypto HAL level apis for RSA/ECC and SAE engines
+*/
+
+/**
+* @brief Generate crypto rsa/ecc fmn message entry 0
+* @ingroup crypto
+* - l3alloc: 1 casuses source data to transit through l3 cache
+* - type: ecc prime/ecc bin/ me/ micorcode load
+* - func: point mul/add etc
+* - srcaddr : source address
+*/
+static inline unsigned long long nlm_crypto_form_rsa_ecc_fmn_entry0(unsigned int l3alloc, unsigned int type, 
+		unsigned int func, unsigned long long srcaddr)
+{
+	return shift_lower_bits(l3alloc, 61, 1) | 
+		shift_lower_bits(type, 46, 7) |
+		shift_lower_bits(func, 40, 6) |
+		shift_lower_bits(srcaddr, 0, 40);
+}
+
+/**
+* @brief Generate crypto rsa/ecc fmn message entry 1
+* @ingroup crypto
+* - dstclobber: 1 causes data to be written as 64byte cacheline
+* - l3alloc: 1 caused data written to the dram is also copied to l3 cache
+* - fbvc: freeback message vc
+* - dstaddr : destination address
+*/
+static inline unsigned long long nlm_crypto_form_rsa_ecc_fmn_entry1(unsigned int dstclobber, unsigned int l3alloc, 
+		unsigned int fbvc, unsigned long long dstaddr)
+{
+	return shift_lower_bits(dstclobber, 62, 1) |
+		shift_lower_bits(l3alloc, 61, 1) |
+		shift_lower_bits(fbvc, 40, 12) |
+		shift_lower_bits(dstaddr, 0, 40);
+}
+
+/**
+* @brief Generate crypto control descriptor
+* @ingroup crypto
+* - hmac : 1 for hash with hmac 
+* - hashalg: see hash_alg enums
+* - hashmode: see hash_mode enums
+* - cipherhalg: see  cipher_alg enums
+* - ciphermode: see  cipher_mode enums
+* - arc4_cipherkeylen : length of arc4 cipher key, 0 is interpreted as 32 
+* - arc4_keyinit : 
+* - cfbmask : cipher text for feedback, 
+*           0(1 bit), 1(2 bits), 2(4 bits), 3(8 bits), 4(16bits), 5(32 bits), 6(64 bits), 7(128 bits)
+*/
+static inline unsigned long long nlm_crypto_form_pkt_ctrl_desc(unsigned int hmac, 
+		unsigned int hashalg,  unsigned int hashmode,
+		unsigned int cipheralg, unsigned int ciphermode,
+		unsigned int arc4_cipherkeylen, unsigned int arc4_keyinit, unsigned int cfbmask)
+{
+	return shift_lower_bits(hmac, 61, 1) | 
+		shift_lower_bits(hashalg, 52, 8) | 
+		shift_lower_bits(hashmode, 43, 8) | 
+		shift_lower_bits(cipheralg, 34, 8) | 
+		shift_lower_bits(ciphermode, 25, 8) | 
+		shift_lower_bits(arc4_cipherkeylen, 18, 5) | 
+		shift_lower_bits(arc4_keyinit, 17, 1) | 
+		shift_lower_bits(cfbmask, 0, 3);
+}
+/**
+* @brief Generate crypto packet descriptor 0
+* @ingroup crypto
+* - tls : 1(tls enabled) 0(tls disabled)
+* - hash_source : 1(encrypted data is sent to the auth engine) 0(plain data is sent to the auth engine)
+* - hashout_l3alloc : 1(auth output is transited through l3 cache)
+* - encrypt : 1(for encrypt) 0(for decrypt)
+* - ivlen : iv length in bytes
+* - hashdst_addr : hash out physical address, byte aligned
+*/
+static inline unsigned long long nlm_crypto_form_pkt_desc0(unsigned int tls, unsigned int hash_source, 
+		unsigned int hashout_l3alloc,
+		unsigned int encrypt, unsigned int ivlen, unsigned long long hashdst_addr)
+{
+	return (shift_lower_bits(tls, 63, 1) |
+		shift_lower_bits(hash_source, 62, 1) |
+		shift_lower_bits(hashout_l3alloc, 60, 1) |
+		shift_lower_bits(encrypt, 59, 1) |
+		shift_lower_bits_mask((ivlen - 1), 41, 16) |
+		shift_lower_bits(hashdst_addr, 0, 40));
+}
+
+/**
+* @brief Generate crypto packet descriptor 1
+* @ingroup crypto
+* - cipherlen : cipher length in bytes
+* - hashlen : hash length in bytes
+*/
+static inline unsigned long long nlm_crypto_form_pkt_desc1(unsigned int cipherlen, unsigned int hashlen)
+{
+	return shift_lower_bits_mask((cipherlen - 1), 32, 32)
+		| shift_lower_bits_mask((hashlen - 1), 0, 32);
+}	
+
+/**
+* @brief Generate crypto packet descriptor 2
+* @ingroup crypto
+* - ivoff : iv offset, offset from start of src data addr
+* - ciperbit_cnt : number of valid bits in the last input byte to the cipher, 0(8 bits), 1(1 bit)..7(7 bits)
+* - cipheroff : cipher offset, offset from start of src data addr
+* - hashbit_cnt : number of valid bits in the last input byte to the auth, 0(8 bits), 1(1 bit)..7(7 bits)
+* - hashclobber : 1(hash output will be written as multiples of cachelines, no read modify write)
+* - hashoff : hash offset, offset from start of src data addr
+*/
+
+static inline unsigned long long nlm_crypto_form_pkt_desc2(unsigned int ivoff, unsigned int cipherbit_cnt, 
+		unsigned int cipheroff, 
+		unsigned int hashbit_cnt, unsigned int hashclobber, unsigned int hashoff)
+{
+	return shift_lower_bits(ivoff , 45, 16)
+		| shift_lower_bits(cipherbit_cnt, 42, 3) 
+		| shift_lower_bits(cipheroff, 22, 16)
+		| shift_lower_bits(hashbit_cnt, 19, 3)
+		| shift_lower_bits(hashclobber, 18, 1)
+		| shift_lower_bits(hashoff, 0, 16);
+}
+
+/**
+* @brief Generate crypto packet descriptor 3
+* @ingroup crypto
+* - designer_vc : designer freeback fmn destination id
+* - taglen : length in bits of the tag generated by the auth engine
+*          md5(128 bits), sha1(160), sha224(224), sha384(384), sha512(512), Kasumi(32), snow3g(32), gcm(128)
+* - hmacpad : 1 if hmac padding is already done 
+*/
+static  inline unsigned long long nlm_crypto_form_pkt_desc3(unsigned int designer_vc, unsigned int taglen, 
+	unsigned int arc4_state_save_l3, unsigned int arc4_save_state, unsigned int hmacpad)
+{
+	return shift_lower_bits(designer_vc, 48, 16)
+		| shift_lower_bits(taglen, 11, 16)
+		| shift_lower_bits(arc4_state_save_l3, 8, 1)
+		| shift_lower_bits(arc4_save_state, 6, 1)
+		| shift_lower_bits(hmacpad, 5, 1);
+}
+
+/**
+* @brief Generate crypto packet descriptor 4
+* @ingroup crypto
+* - srcfraglen : length of the source fragment(header + data + tail) in bytes
+* - srcfragaddr : physical address of the srouce fragment
+*/
+static inline unsigned long long nlm_crypto_form_pkt_desc4(unsigned int srcfraglen, unsigned long long srcfragaddr )
+{
+	return shift_lower_bits_mask((srcfraglen - 1), 48, 16)
+		| shift_lower_bits(srcfragaddr, 0, 40);
+}
+
+/**
+* @brief Generate crypto packet descriptor 5
+* @ingroup crypto
+* - dstfraglen : length of the dst fragment(header + data + tail) in bytes
+* - chipherout_l3alloc : 1(cipher output is transited through l3 cache)
+* - cipherclobber : 1(cipher output will be written as multiples of cachelines, no read modify write)
+* - chiperdst_addr : physical address of the cipher destination address
+*/
+static inline unsigned long long nlm_crypto_form_pkt_desc5(unsigned int dstfraglen, unsigned int cipherout_l3alloc, 
+	       unsigned int cipherclobber, unsigned long long cipherdst_addr)
+
+{
+	return shift_lower_bits_mask((dstfraglen - 1), 48, 16)
+		| shift_lower_bits(cipherout_l3alloc, 46, 1) 
+		| shift_lower_bits(cipherclobber, 41, 1)
+		| shift_lower_bits(cipherdst_addr, 0, 40);
+}
+
+/**
+  * @brief Generate crypto packet fmn message entry 0
+  * @ingroup crypto
+  * - freeback_vc: freeback response destination address
+  * - designer_fblen : Designer freeback length, 1 - 4
+  * - designerdesc_valid : designer desc valid or not
+  * - cipher_keylen : cipher key length in bytes
+  * - ctrldesc_addr : physicall address of the control descriptor
+  */
+static inline unsigned long long  nlm_crypto_form_pkt_fmn_entry0(unsigned int freeback_vc, unsigned int designer_fblen,
+		unsigned int designerdesc_valid, unsigned int cipher_keylen, unsigned long long cntldesc_addr)
+{
+	return shift_lower_bits(freeback_vc, 48, 16)
+		| shift_lower_bits_mask(designer_fblen - 1, 46, 2)
+		| shift_lower_bits(designerdesc_valid, 45, 1)
+		| shift_lower_bits_mask(((cipher_keylen + 7) >> 3), 40, 5)
+		| shift_lower_bits(cntldesc_addr >> 6, 0, 34);
+}
+
+/**
+  * @brief Generate crypto packet fmn message entry 1
+  * @ingroup crypto
+  * - arc4load_state : 1 if load state required 0 otherwise
+  * - hash_keylen : hash key length in bytes
+  * - pktdesc_size : packet descriptor size in bytes
+  * - pktdesc_addr : physicall address of the packet descriptor
+  */
+static inline unsigned long long nlm_crypto_form_pkt_fmn_entry1(unsigned int arc4load_state, unsigned int hash_keylen,
+		unsigned int pktdesc_size, unsigned long long pktdesc_addr)
+{
+	return shift_lower_bits(arc4load_state, 63, 1)
+		| shift_lower_bits_mask(((hash_keylen + 7) >> 3), 56, 5)
+		| shift_lower_bits_mask(((pktdesc_size >> 4) - 1), 43, 12)
+		| shift_lower_bits(pktdesc_addr >> 6, 0, 34);
+}
+
+
+#endif	
diff --git a/drivers/crypto/Kconfig b/drivers/crypto/Kconfig
index f23d613..65f3b10 100644
--- a/drivers/crypto/Kconfig
+++ b/drivers/crypto/Kconfig
@@ -251,4 +251,15 @@ config CRYPTO_DEV_EXTENS_T23X_SEC2
 	  SEC2 Legacy Driver Interface (t23xsec2) component intended to
 	  allow migration of applications dependent on the legacy driver
 
+config XLP_SAE
+        tristate "netlogic microsystems xlp SAE driver"
+        default m
+        help
+          This driver supports netlogic's Security Acceleration Engine.
+          For more information on xlp SAE driver, please visit
+
+          <http://support.netlogicmicro.com/support>
+
+	  To compile this driver as a module, choose M here. The module will be called sae.
+
 endif # CRYPTO_HW
diff --git a/drivers/crypto/Makefile b/drivers/crypto/Makefile
index c73a378..e7fe989 100644
--- a/drivers/crypto/Makefile
+++ b/drivers/crypto/Makefile
@@ -11,3 +11,5 @@ obj-$(CONFIG_CRYPTO_DEV_PPC4XX) += amcc/
 obj-$(CONFIG_CRYPTO_DEV_EXTENS_T23X) += t23x/t23xrm/
 obj-$(CONFIG_CRYPTO_DEV_EXTENS_T23X_RMTEST) += t23x/t23xtest/
 obj-$(CONFIG_CRYPTO_DEV_EXTENS_T23X_SEC2) += t23x/t23xsec2/
+
+obj-$(CONFIG_XLP_SAE)	+= nlm-sae/
diff --git a/drivers/crypto/nlm-sae/Makefile b/drivers/crypto/nlm-sae/Makefile
new file mode 100644
index 0000000..80b2cf0
--- /dev/null
+++ b/drivers/crypto/nlm-sae/Makefile
@@ -0,0 +1,13 @@
+
+################################################################################
+
+#
+# Makefile for xlp_sae security driver
+#
+
+#EXTRA_CFLAGS := -Werror
+EXTRA_CFLAGS := -DNLM_HAL_LINUX_KERNEL -Iarch/mips/include/asm/netlogic/hal
+EXTRA_CFLAGS += -Iarch/mips/netlogic/boot
+
+obj-$(CONFIG_XLP_SAE)	+= sae.o
+sae-objs		:= nlm_enc.o nlm_crypto.o nlm_auth.o nlm_aead.o
diff --git a/drivers/crypto/nlm-sae/nlm_aead.c b/drivers/crypto/nlm-sae/nlm_aead.c
new file mode 100644
index 0000000..a3daf10
--- /dev/null
+++ b/drivers/crypto/nlm-sae/nlm_aead.c
@@ -0,0 +1,1286 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#NETL_2#********************************/
+#include <linux/rtnetlink.h>
+#include <crypto/algapi.h>
+#include <crypto/aes.h>
+#include <crypto/des.h>
+#include <crypto/sha.h>
+#include <crypto/aead.h>
+#include <crypto/authenc.h>
+#include <crypto/scatterwalk.h>
+
+#include <asm/netlogic/hal/nlm_hal_fmn.h>
+#include <asm/netlogic/crypto/nlmcrypto.h>
+#include <asm/netlogic/msgring.h>
+#include "nlm_async.h"
+
+#undef NLM_CRYPTO_DEBUG
+#define Message(a, b...) //printk("[%s @ %d] "a"\n",__FUNCTION__,__LINE__, ##b)
+
+#define AES_CTR_IV_SIZE         8
+#define XLP_CRYPT_PRIORITY      310
+
+#define XCBC_DIGEST_SIZE        16
+#define MD5_DIGEST_SIZE         16
+#define MD5_BLOCK_SIZE          64
+#define CTR_RFC3686_IV_SIZE 8
+
+
+/*
+ 						CTRL DESC MEMORY LAYOUT
+	 ------------------------------------------------------------------------------------
+	|  64 bytes	 | struct nlm_aead_ctx	  | 64bytes for   | struct nlm_aead_ctx     |
+	|  for alignment | 			  | for alignment | (used only for 3des)    |
+	 ------------------------------------------------------------------------------------
+*/
+
+struct nlm_aead_ctx
+{
+	struct nlm_crypto_pkt_ctrl ctrl;
+	uint8_t iv_buf[16];
+	uint32_t iv_len;
+	int cbc;
+	uint16_t stat;
+};
+
+#define MAX_FRAGS		18
+#define CTRL_DESC_SIZE		(sizeof(struct nlm_aead_ctx) + 128)
+#define DES3_CTRL_DESC_SIZE	(2*CTRL_DESC_SIZE + 2*64)	//Allocate 2 separate control desc for encryption and decryption
+
+/*
+ 						PACKET DESC MEMORY LAYOUT
+	 ------------------------------------------------------------------------------------------------------
+	|  64 bytes	 | struct nlm_crypto_pkt_param +  | 64bytes for   | struct nlm_async_crypto | 64 bytes |
+	|  for alignment | 18 * (2*64)			  | for alignment |			    | for hash |
+	 ------------------------------------------------------------------------------------------------------
+ */
+
+#define PACKET_DESC_SIZE	(64 + sizeof(struct nlm_crypto_pkt_param) + MAX_FRAGS*(2*64) + 64 + sizeof(struct nlm_async_crypto) + 128 )
+#define NLM_CRYPTO_PKT_PARAM_OFFSET(addr)	(((unsigned long)addr + 64) & ~0x3fULL)
+#define NLM_ASYNC_PTR_PARAM_OFFSET(addr)	(((unsigned long)(addr + 64 + sizeof(struct nlm_crypto_pkt_param) + MAX_FRAGS*(2*64)) + 64) & ~0x3fULL)
+#define NLM_HASH_OFFSET(addr)			((unsigned long)addr + (PACKET_DESC_SIZE - 64))
+#define NLM_IV_OFFSET(addr)			((unsigned long)addr + (PACKET_DESC_SIZE - 128))
+
+
+#define XLP_CRYPT_PRIORITY	310
+
+#define NETL_OP_ENCRYPT 1
+#define NETL_OP_DECRYPT 0
+
+#define PKT_DESC_OFF 64
+
+#ifdef NLM_CRYPTO_DEBUG
+extern void print_crypto_msg_desc(uint64_t entry1, uint64_t entry2, uint64_t entry3);
+extern void print_cntl_instr(uint64_t cntl_desc);
+extern void print_pkt_desc(struct nlm_crypto_pkt_param  *pkt_desc, int index);
+#endif
+extern struct nlm_crypto_stat crypto_stat[MAX_CPU];
+
+
+/*
+   All extern declaration goes here.
+ */
+extern uint32_t nlm_hal_send_msg3(uint32_t dst, uint32_t code, uint64_t data0, uint64_t data1, uint64_t data2);
+extern uint32_t nlm_hal_recv_msg2(uint32_t dst, uint32_t *src, uint32_t *size, uint32_t *code, uint64_t *data0, uint64_t *data1);
+extern int auth_mode_key_len[NLM_HASH_MAX][NLM_HASH_MODE_MAX];
+extern int cipher_mode_iv_len[NLM_CIPHER_MAX][NLM_CIPHER_MODE_MAX];
+
+static void print_buf(unsigned char *msg, unsigned char *buf, int len)
+{
+#define TMP_BUF		50
+	char tmp_buf[TMP_BUF + 1];
+	int i, index = 0;
+
+	printk("**********%s************\n",msg);
+	for(i=0; i<len; i++){
+		sprintf(&tmp_buf[index*2], "%02x", buf[i]);
+		index++;
+		if(index == (TMP_BUF/2)){
+			tmp_buf[index*2] = '\0';
+			printk("[%s]\n",tmp_buf);
+			index = 0;
+		}
+	}
+	if(index){
+		tmp_buf[index*2] = '\0';
+		printk("[%s]\n",tmp_buf);
+	}
+}
+
+static struct nlm_aead_ctx *nlm_crypto_aead_ctx(struct crypto_aead *tfm)
+{
+	return (struct nlm_aead_ctx *)(((unsigned long)((uint8_t *)crypto_aead_ctx(tfm) + 63)) & ~(0x3f));
+}
+
+static struct nlm_aead_ctx *nlm_crypto_tfm_ctx(struct crypto_tfm *tfm)
+{
+	return (struct nlm_aead_ctx *)(((unsigned long)((uint8_t *)crypto_tfm_ctx(tfm) + 63)) & ~(0x3f));
+}
+
+
+
+static int aead_setauthsize(struct crypto_aead *tfm, unsigned int authsize)
+{
+	struct aead_tfm *crt = crypto_aead_crt(tfm);
+	crt->authsize = authsize;
+	return 0;
+}
+
+static void aead_session_cleanup(struct crypto_tfm *tfm)
+{
+}
+
+static int aead_cra_cbc_init(struct crypto_tfm *tfm)
+{
+	struct nlm_aead_ctx *nlm_ctx = (struct nlm_aead_ctx *)nlm_crypto_tfm_ctx(tfm);
+	nlm_ctx->cbc = 1;
+	tfm->crt_aead.reqsize = PACKET_DESC_SIZE; 
+	return 0;
+}
+
+static int aead_cra_init(struct crypto_tfm *tfm)
+{
+	printk("[%s %d]\n",__FUNCTION__, __LINE__);
+	tfm->crt_aead.reqsize = PACKET_DESC_SIZE;
+	printk("[%s %d]\n",__FUNCTION__, __LINE__);
+	return 0;
+}
+
+static int get_cipher_auth_keylen(const u8 *key, unsigned int keylen, int *cipher_keylen,
+			     int *auth_keylen)
+{
+	struct rtattr *rta = (struct rtattr *) key;
+	struct crypto_authenc_key_param *param;
+
+	if (!RTA_OK(rta, keylen)) {
+		goto badkey;
+	}
+
+	if (rta->rta_type != CRYPTO_AUTHENC_KEYA_PARAM) {
+		goto badkey;
+	}
+	if (RTA_PAYLOAD(rta) < sizeof (struct crypto_authenc_key_param)) {
+		goto badkey;
+	}
+
+	param = RTA_DATA(rta);
+	*cipher_keylen = be32_to_cpu(param->enckeylen);
+
+	key += RTA_ALIGN(rta->rta_len);
+	keylen -= RTA_ALIGN(rta->rta_len);
+
+	if (keylen < *cipher_keylen)
+		goto badkey;
+
+	*auth_keylen = keylen - *cipher_keylen;
+
+	return 0;
+badkey:
+	return -EINVAL;
+}
+
+static int get_cipher_aes_algid(unsigned int cipher_keylen)
+{
+
+	switch (cipher_keylen) {
+	case 16:
+		return NLM_CIPHER_AES128;
+		break;
+	case 24:
+		return NLM_CIPHER_AES192;
+		break;
+	case 32:
+		return NLM_CIPHER_AES256;
+		break;
+	default:
+		printk(KERN_WARNING "[%s]: Cannot handle keylen = %d\n",
+		       __FUNCTION__, cipher_keylen);
+		return -EINVAL;
+	}
+}
+
+/*
+   All Setkey goes here.
+ */
+
+static int xlp_aes_cbc_setkey( struct crypto_aead *tfm, uint8_t *key, unsigned int keylen,
+				int hash, int mode,uint16_t h_stat )
+{ 
+	struct nlm_aead_ctx *ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+	struct nlm_crypto_pkt_ctrl *ctrl = &ctx->ctrl;
+	unsigned int cipher_keylen=0, auth_keylen=0;
+	int ret;
+	int cipher_alg;
+	uint8_t auth_key[128];
+	uint8_t *cipher_key;
+	struct rtattr *rta = (struct rtattr *)key;
+	int hmac = ((mode == NLM_HASH_MODE_XCBC) ? 0: 1);
+	if ((ret = get_cipher_auth_keylen(key, keylen, &cipher_keylen,
+					  &auth_keylen)) < 0) {
+		crypto_aead_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+		printk("ERR: Bad key len\n");
+		return ret;
+	}
+
+	cipher_alg = get_cipher_aes_algid(cipher_keylen);
+	ctx->stat = cipher_alg - 1;
+	ctx->stat = ctx->stat | (h_stat << 8);
+	if (cipher_alg < 0) {
+		crypto_aead_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+		printk("ERR: Bad aes key len\n");
+		return -EINVAL;
+	}
+
+	printk("Initial authkeylen %d, cipherkeylen %d\n", cipher_keylen, auth_keylen);
+	key += RTA_ALIGN(rta->rta_len);
+	cipher_key = key + auth_keylen;
+	memcpy(auth_key, key, auth_keylen);
+	if(auth_mode_key_len[hash][mode] > auth_keylen)
+		memset(auth_key + auth_keylen, 0,  auth_mode_key_len[hash][mode] - auth_keylen);
+
+	if(cipher_mode_iv_len[cipher_alg][NLM_CIPHER_MODE_CBC] > 0)
+		ctx->iv_len = cipher_mode_iv_len[cipher_alg][NLM_CIPHER_MODE_CBC];
+
+	auth_keylen = auth_mode_key_len[hash][mode];
+
+	print_buf("ENC_KEY:", cipher_key, cipher_keylen);
+	print_buf("AUTH_KEY:", auth_key, auth_keylen);
+
+	ret =  nlm_crypto_fill_pkt_ctrl(ctrl, hmac, hash, 
+			mode, cipher_alg, NLM_CIPHER_MODE_CBC, 0, cipher_key, 
+			cipher_keylen, auth_key, auth_keylen);
+
+	return ret;
+}
+
+static int  xlp_3des_setkey(struct crypto_aead *tfm, u8 *key, unsigned int keylen, int hash, int mode,uint16_t h_stat )
+{
+	struct nlm_aead_ctx *ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+	struct nlm_aead_ctx * nlm_ctx = (struct  nlm_aead_ctx *)(( unsigned long )(( uint8_t *)ctx + CTRL_DESC_SIZE + 63) & ~(0x3f));
+        unsigned int cipher_keylen=0, auth_keylen=0;
+	struct nlm_crypto_pkt_ctrl *ctrl = &ctx->ctrl;
+        int ret;
+	uint8_t auth_key[128];
+	uint64_t d_key[3] ;
+	int cipher_alg = NLM_CIPHER_3DES;
+	struct rtattr *rta = (struct rtattr *)key;
+	uint8_t *cipher_key;
+	int hmac = ((mode == NLM_HASH_MODE_XCBC) ? 0: 1);
+
+        if ((ret = get_cipher_auth_keylen(key, keylen, &cipher_keylen,
+                                          &auth_keylen)) < 0) {
+                crypto_aead_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+                printk("ERR: Bad key len\n");
+                return ret;
+        }
+	ctx->stat = TDES_CBC_STAT | h_stat << 8;;
+	key += RTA_ALIGN(rta->rta_len);
+	memcpy(auth_key, key, auth_keylen);
+	cipher_key = key + auth_keylen;
+	if ( auth_mode_key_len[hash][mode] > auth_keylen)
+		memset(auth_key + auth_keylen, 0,  auth_mode_key_len[hash][mode] - auth_keylen);
+	
+	if(cipher_mode_iv_len[cipher_alg][NLM_CIPHER_MODE_CBC] > 0) {
+		ctx->iv_len = cipher_mode_iv_len[cipher_alg][NLM_CIPHER_MODE_CBC];
+		nlm_ctx->iv_len = cipher_mode_iv_len[cipher_alg][NLM_CIPHER_MODE_CBC];
+	}
+	auth_keylen = auth_mode_key_len[hash][mode];
+
+	ret =  nlm_crypto_fill_pkt_ctrl(ctrl, hmac, hash,
+		mode, cipher_alg,NLM_CIPHER_MODE_CBC,0,cipher_key,
+		 cipher_keylen, auth_key, auth_keylen);
+	
+	memcpy(d_key,&cipher_key[16],8);
+        memcpy(&d_key[1],&cipher_key[8],8);
+        memcpy(&d_key[2],&cipher_key[0],8);
+	ret =  nlm_crypto_fill_pkt_ctrl(&nlm_ctx->ctrl, hmac, hash,
+		mode, cipher_alg, NLM_CIPHER_MODE_CBC, 0, ( unsigned char * )d_key,
+		cipher_keylen, auth_key, auth_keylen);
+	print_buf("ENC_KEY:", cipher_key, cipher_keylen);
+	print_buf("AUTH_KEY:", auth_key, auth_keylen);
+	print_buf("DECRY_KEY",(unsigned char * )&d_key[0],cipher_keylen);
+
+        return ret;
+}
+
+static int xlp_des_setkey( struct crypto_aead *tfm, uint8_t  *key, unsigned int keylen, int hash, int mode, uint16_t h_stat)
+{
+	struct nlm_aead_ctx *ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+        unsigned int cipher_keylen=0, auth_keylen=0;
+	struct nlm_crypto_pkt_ctrl *ctrl = &ctx->ctrl;
+        int ret;
+	uint8_t auth_key[128];
+	uint64_t d_key[3] ;
+	int cipher_alg = NLM_CIPHER_DES;
+	struct rtattr *rta = (struct rtattr *)key;
+	uint8_t *cipher_key;
+	int hmac = ((mode == NLM_HASH_MODE_XCBC) ? 0: 1);
+	ctx->stat = DES_CBC_STAT | h_stat << 8;
+	
+
+        if ((ret = get_cipher_auth_keylen(key, keylen, &cipher_keylen,
+                                          &auth_keylen)) < 0) {
+                crypto_aead_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+                printk("ERR: Bad key len\n");
+                return ret;
+        }
+	key += RTA_ALIGN(rta->rta_len);
+	memcpy(auth_key, key, auth_keylen);
+	cipher_key = key + auth_keylen;
+	if ( auth_mode_key_len[hash][mode] > auth_keylen)
+		memset(auth_key + auth_keylen, 0,  auth_mode_key_len[hash][mode] - auth_keylen);
+	
+	if(cipher_mode_iv_len[cipher_alg][NLM_CIPHER_MODE_CBC] > 0) {
+		ctx->iv_len = cipher_mode_iv_len[cipher_alg][NLM_CIPHER_MODE_CBC];
+	}
+	auth_keylen = auth_mode_key_len[hash][mode];
+
+	ret =  nlm_crypto_fill_pkt_ctrl(ctrl, hmac, hash,
+		mode, cipher_alg,NLM_CIPHER_MODE_CBC,0,cipher_key,
+		 cipher_keylen, auth_key, auth_keylen);
+	
+	print_buf("ENC_KEY:", cipher_key, cipher_keylen);
+	print_buf("AUTH_KEY:", auth_key, auth_keylen);
+	print_buf("DECRY_KEY",(unsigned char *)&d_key[0],cipher_keylen);
+
+        return ret;
+
+
+}
+static int xlp_aes_cbc_hmac_sha256_setkey( struct crypto_aead *tfm, const u8 *key, unsigned int keylen)
+{
+	return xlp_aes_cbc_setkey(tfm, (uint8_t *)key, keylen,NLM_HASH_SHA,NLM_HASH_MODE_SHA256,H_SHA256_STAT);
+
+}
+
+static int xlp_aes_cbc_hmac_sha1_setkey(struct crypto_aead *tfm, const u8 *key, unsigned int keylen)
+{
+	return xlp_aes_cbc_setkey(tfm, (uint8_t *)key, keylen,NLM_HASH_SHA,NLM_HASH_MODE_SHA1,H_SHA1_STAT);
+}
+
+static int xlp_aes_cbc_aes_xcbc_mac_setkey(struct crypto_aead *tfm, const u8 *key, unsigned int keylen)
+{
+	return xlp_aes_cbc_setkey(tfm, (uint8_t *)key, keylen,NLM_HASH_AES128,NLM_HASH_MODE_XCBC,AES128_XCBC_STAT);
+}
+
+static int xlp_aes_cbc_hmac_md5_setkey(struct crypto_aead *tfm, const u8 *key, unsigned int keylen)
+{
+	return xlp_aes_cbc_setkey(tfm, (uint8_t *)key, keylen,NLM_HASH_MD5,0,MD5_STAT);
+}
+
+static int xlp_3des_cbc_hmac_md5_setkey(struct crypto_aead *tfm, const u8 *key,
+                                        unsigned int keylen)
+{
+	return xlp_3des_setkey(tfm,(uint8_t *)key,keylen,NLM_HASH_MD5,0,MD5_STAT);
+
+}
+static int xlp_3des_cbc_hmac_sha256_setkey(struct crypto_aead *tfm, const u8 *key,
+                                        unsigned int keylen)
+{
+	return xlp_3des_setkey(tfm,(uint8_t *)key,keylen,NLM_HASH_SHA,NLM_HASH_MODE_SHA256,H_SHA256_STAT);
+        
+}
+static int xlp_3des_cbc_hmac_sha1_setkey(struct crypto_aead *tfm, const u8 *key,
+                                        unsigned int keylen)
+{
+	return xlp_3des_setkey(tfm,(uint8_t *)key,keylen,NLM_HASH_SHA,NLM_HASH_MODE_SHA1,H_SHA1_STAT);
+}
+static int xlp_3des_cbc_aes_xcbc_mac_setkey(struct crypto_aead *tfm, const u8 *key,
+                                        unsigned int keylen)
+{
+	return xlp_3des_setkey(tfm,(uint8_t *)key,keylen,NLM_HASH_AES128,NLM_HASH_MODE_XCBC,AES128_XCBC_STAT);
+}
+static int xlp_des_cbc_aes_xcbc_mac_setkey( struct crypto_aead *tfm, const u8 *key,
+					unsigned int keylen)
+{
+	return xlp_des_setkey(tfm,(uint8_t *)key,keylen,NLM_HASH_AES128,NLM_HASH_MODE_XCBC,AES128_XCBC_STAT);
+}
+static int xlp_des_cbc_hmac_sha1_setkey(struct crypto_aead *tfm, const u8 *key,
+						 unsigned int keylen)
+{
+	return xlp_des_setkey(tfm,(uint8_t *)key,keylen, NLM_HASH_SHA,NLM_HASH_MODE_SHA1,H_SHA1_STAT);
+
+}
+static int xlp_des_cbc_hmac_sha256_setkey(struct crypto_aead *tfm, const u8 *key,
+							unsigned int keylen)
+{
+	return xlp_des_setkey(tfm,(uint8_t *)key,keylen, NLM_HASH_SHA,NLM_HASH_MODE_SHA256,H_SHA256_STAT);
+
+}
+static int xlp_des_cbc_hmac_md5_setkey( struct crypto_aead *tfm, const u8 *key,
+						unsigned int keylen)
+{
+	return xlp_des_setkey(tfm,(uint8_t *)key,keylen,NLM_HASH_MD5,0,MD5_STAT);
+
+}
+
+//returns nr_aad_frags... -1 for error
+int fill_aead_aad(struct nlm_crypto_pkt_param *param, struct aead_request *req, unsigned int aad_len,int seg)
+{
+	struct scatterlist *sg;
+	struct scatter_walk walk;
+	int len;
+	uint8_t *virt;
+	
+	for (sg = req->assoc; aad_len > 0; sg = scatterwalk_sg_next(sg), 
+			seg++) {
+
+		len = min(aad_len, sg->length);
+		scatterwalk_start(&walk, sg);
+		//virt = scatterwalk_map(&walk, 1);
+		virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
+		nlm_crypto_fill_src_seg(param, seg, virt, len);
+		nlm_crypto_fill_dst_seg(param, seg, virt, len);
+		aad_len -= len;
+	}
+	return seg;
+}
+
+int fill_aead_crypt(struct aead_request *req, unsigned int cipher_len, 
+		struct nlm_crypto_pkt_param *param, unsigned char **actual_tag, int op, int seg)
+{
+	struct scatterlist *sg;
+	struct scatter_walk walk;
+	unsigned int len = 0;
+	uint8_t *virt = NULL;
+	int nr_src_frags = 0;
+	int nr_dst_frags = 0;
+	int passed_len, i;
+	int index = 0;
+
+	if (req->src == req->dst) {
+		for (sg = req->src; cipher_len > 0; sg = scatterwalk_sg_next(sg), index++) {
+			len = min(cipher_len, sg->length);
+			scatterwalk_start(&walk, sg);
+			virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
+			nlm_crypto_fill_src_seg(param, seg + index, virt, len);
+			nlm_crypto_fill_dst_seg(param, seg + index, virt, len);
+			cipher_len -= len;
+		}
+		*actual_tag = virt + len;
+		return index;
+	}
+	passed_len = cipher_len;
+
+	for (sg = req->src, index = 0; cipher_len > 0; sg = scatterwalk_sg_next(sg),
+	     nr_src_frags++, index++) {
+		len = min(cipher_len, sg->length);
+		scatterwalk_start(&walk, sg);
+		virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
+		nlm_crypto_fill_src_seg(param, seg + index, virt, len);
+		cipher_len -= len;
+	}
+
+	if (op == NETL_OP_ENCRYPT)
+		*actual_tag = virt + len;
+
+	cipher_len = passed_len;
+
+	for (sg = req->dst, index=0; cipher_len > 0; sg = scatterwalk_sg_next(sg),
+	     nr_dst_frags++, index++) {
+		len = min(cipher_len, sg->length);
+		scatterwalk_start(&walk, sg);
+		virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
+		nlm_crypto_fill_dst_seg(param, seg + index, virt, len);
+		cipher_len -= len;
+	}
+
+	if (op == NETL_OP_DECRYPT)
+		*actual_tag = virt + len;
+
+	if (nr_src_frags > nr_dst_frags) {
+		for (i = 0; i < nr_src_frags - nr_dst_frags; i++)
+			param->segment[seg + nr_dst_frags + i][1] = 0ULL;
+		return nr_src_frags;
+	}else{
+		if (nr_src_frags < nr_dst_frags) {
+			for (i = 0; i < nr_dst_frags - nr_src_frags; i++)
+				param->segment[seg + nr_src_frags + i][0] = 0ULL;
+		}
+		return nr_dst_frags;
+	}
+}
+
+/*
+   Generic Encrypt / Decrypt Function
+ */
+//op is either encrypt or decrypt
+
+#if 1
+static inline void ncd_block_fast_send(unsigned int dest)
+{
+	int success;
+        __asm__ volatile (".set push\n"
+                          ".set noreorder\n"
+                          ".set arch=xlp\n"
+                          "sync\n"
+                          "1: msgsnds %0, %1\n"
+			  "beqz %0, 1b\n"
+			  "nop\n"
+                          ".set pop\n"
+			  : "=&r"(success)
+			  : "r" (dest));
+
+        return ;
+}
+
+static inline int ncd_fast_recv_msg2(uint32_t vc, uint64_t *msg0, uint64_t *msg1)
+{
+	if (!xlp_receive(vc))
+		return -1;
+
+	*msg0 = xlp_load_rx_msg0();
+	*msg1 = xlp_load_rx_msg1();
+	return 0;
+}
+
+static inline void ncd_fast_send_msg3(uint32_t dst, uint64_t data0, uint64_t data1, uint64_t data2)
+{
+  unsigned int dest = 0;
+
+
+  xlp_load_tx_msg0(data0);
+  xlp_load_tx_msg1(data1);
+  xlp_load_tx_msg2(data2);
+
+  dest = ((2 << 16) | dst);
+
+#ifdef MSGRING_DUMP_MESSAGES
+  nlm_hal_dbg_msg("Sending msg<%llx, %llx, %llx> to dest = %x\n", 
+	  data0, data1, data2, dest);
+#endif
+	
+  ncd_block_fast_send(dest);
+
+  return;
+}
+#endif
+
+static void aead_request_callback(struct nlm_async_crypto *async, uint64_t msg1)
+{
+	struct crypto_async_request *base = (struct crypto_async_request *)async->args;
+	int err = 0;
+	int cpu = nlm_processor_id();
+	int enc = async->stat & 0xff;
+	int auth = (async->stat >> 8 ) & 0xff;
+
+	if (msg1 & 0x7ff80) {
+		printk("\n Error: entry1 is %llx",msg1);
+		err = -EIO;
+		base->complete(base, err);
+		return;
+	}
+	if (async->op){
+		memcpy(async->actual_tag, async->hash_addr, async->authsize);
+	}else{
+		if(memcmp(async->actual_tag, async->hash_addr, async->authsize)){
+			err = -EBADMSG;
+			printk("TAG MISMATCH!!!\n");
+		}
+	}
+	crypto_stat[cpu].enc[enc]++;
+	crypto_stat[cpu].auth[auth]++;	
+	crypto_stat[cpu].enc_tbytes[enc] += async->bytes;
+	crypto_stat[cpu].auth_tbytes[auth] += async->bytes;
+	base->complete(base, err);
+	return;
+}
+
+
+static int aead_crypt_3des(struct aead_request *req, unsigned int op)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_alg *alg = tfm->base.__crt_alg;
+	struct aead_alg *aead= &alg->cra_aead;
+	struct nlm_aead_ctx *ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+	struct aead_tfm *crt = crypto_aead_crt(tfm);
+	struct nlm_crypto_pkt_param *param;
+	unsigned int cipher_off, iv_off;
+	unsigned int auth_len, cipher_len, auth_off;
+	unsigned char *actual_tag;
+	int seg =0, nr_enc_frags, ivsize;
+	unsigned int hash_source, nr_aad_frags;
+	uint64_t entry0, entry1;
+	uint64_t tx_id=0x12345678;
+	struct nlm_async_crypto *async = NULL;
+	uint8_t *hash_addr;
+	int fb_vc; 
+	int err=0;
+	unsigned int authsize,maxauthsize;
+	uint8_t *new_iv_ptr_lo = NULL;
+	uint8_t *new_iv_ptr_hi = NULL;
+	struct nlm_crypto_pkt_ctrl *ctrl = NULL;
+	ctx = (struct  nlm_aead_ctx *)(( unsigned long )(( uint8_t *)ctx + CTRL_DESC_SIZE + 63) & ~(0x3f));
+	ctrl = &ctx->ctrl;
+	
+	authsize = crypto_aead_crt(crt->base)->authsize;
+	maxauthsize= aead->maxauthsize;
+
+	param = (struct nlm_crypto_pkt_param *)NLM_CRYPTO_PKT_PARAM_OFFSET(aead_request_ctx(req));
+	hash_addr = (uint8_t *)NLM_HASH_OFFSET(aead_request_ctx(req));
+
+	ivsize = crypto_aead_ivsize(crypto_aead_reqtfm(req));
+	auth_off = 0;
+	cipher_off = req->assoclen + ivsize;
+	iv_off = req->assoclen;
+	//check if it should be aip->tag_len or can be taken from tfm
+	cipher_len = op ? req->cryptlen:req->cryptlen - authsize;
+	auth_len = cipher_off + cipher_len;
+	hash_source = op;
+
+	nr_aad_frags = fill_aead_aad(param, req, req->assoclen,seg);
+	if (nr_aad_frags == 0)
+		return -1;
+	seg = nr_aad_frags;
+
+	if (ivsize) {
+		nlm_crypto_fill_src_seg(param, seg, req->iv, ivsize);
+		nlm_crypto_fill_dst_seg(param, seg, req->iv, ivsize);
+		seg++;
+	}
+
+	nr_enc_frags = fill_aead_crypt(req, cipher_len, param, &actual_tag, op, seg);
+
+	if (nr_enc_frags == -1)
+		return -1;
+
+	if(ctx->cbc){
+		uint32_t tmp_len;
+		uint8_t *tmp_virt;
+
+		tmp_len = (param->segment[seg][1] >> 48) + 1;
+		if(tmp_len >= 16){
+			tmp_virt = phys_to_virt((param->segment[seg][1]) & 0xffffffffffULL);
+			new_iv_ptr_hi = tmp_virt;
+			new_iv_ptr_lo = new_iv_ptr_hi + 8;;
+		}else if(tmp_len >=8){
+			tmp_virt = phys_to_virt((param->segment[seg][1]) & 0xffffffffffULL);
+			new_iv_ptr_lo = tmp_virt;
+			/*goto next frag*/
+			if(nr_enc_frags > 1){
+				tmp_virt = phys_to_virt((param->segment[seg + 1][1]) & 0xffffffffffULL);
+				tmp_len = (uint32_t)(param->segment[seg + 1][1] >> 48) + 1;
+				if(tmp_len >= 8)
+					new_iv_ptr_hi = tmp_virt + 8;
+			}
+		}
+	}
+
+	seg += nr_enc_frags;
+
+	nlm_crypto_fill_cipher_auth_pkt_param(ctrl, param, op, op, iv_off, 
+			ivsize, auth_off, auth_len, 0, cipher_off, cipher_len, hash_addr);
+	
+	fb_vc = crypto_get_fb_vc();
+
+	entry0 = nlm_crypto_form_pkt_fmn_entry0(fb_vc, 0, 0, ctrl->cipherkeylen, virt_to_phys(ctrl));
+	entry1 = nlm_crypto_form_pkt_fmn_entry1(0, ctrl->hashkeylen, (seg + 2)<<4, virt_to_phys(param));
+
+
+#ifdef NLM_CRYPTO_DEBUG
+	printk("*****ctrl descriptor phys addr %#lx\n",virt_to_phys(ctrl));
+	print_crypto_msg_desc(entry0, entry1, tx_id);
+	print_cntl_instr(ctrl->desc0);
+	print_buf("ENC_KEY:", (uint8_t *)ctrl->key, ctrl->cipherkeylen);
+	print_buf("AUTH_KEY:", (uint8_t *)ctrl->key + ctrl->cipherkeylen, ctrl->hashkeylen);
+	print_pkt_desc(param,seg);
+	printk("iv_off: %d, cipher_off: %d, auth_off: %d\n",iv_off, cipher_off, auth_off);
+	printk("auth_len: %d, cipher_len: %d\n", auth_len, cipher_len);
+#endif
+
+	async = (struct nlm_async_crypto *)NLM_ASYNC_PTR_PARAM_OFFSET(aead_request_ctx(req));
+	async->callback =  aead_request_callback;
+	async->args = (void *)&req->base; 
+	async->op  = op;
+	async->actual_tag = actual_tag;
+	async->hash_addr = hash_addr;
+	async->authsize = authsize;
+	async->stat = ctx->stat; 
+	async->bytes = req->cryptlen; 
+	tx_id = (uint64_t)async;
+
+	//construct pkt, send to engine and receive reply
+	err = nlm_hal_send_msg3(crypto_vc_base, 0 /*code */ , entry0, entry1, tx_id);
+	if(err){
+		printk("err\n");
+		return -EIO;
+	}
+
+
+	return -EINPROGRESS;
+}
+static int aead_crypt(struct aead_request *req, unsigned int op)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(req);
+	struct crypto_alg *alg = tfm->base.__crt_alg;
+	struct aead_alg *aead= &alg->cra_aead;
+	struct aead_tfm *crt = crypto_aead_crt(tfm);
+	struct nlm_aead_ctx *ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+	struct nlm_crypto_pkt_param *param;
+	unsigned int cipher_off, iv_off;
+	unsigned int auth_len, cipher_len, auth_off;
+	unsigned char *actual_tag;
+	int seg =0, nr_enc_frags, ivsize;
+	unsigned int hash_source, nr_aad_frags;
+	uint64_t entry0, entry1;
+	uint64_t tx_id=0x12345678;
+	struct nlm_async_crypto *async = NULL;
+	uint8_t *hash_addr;
+	int fb_vc; 
+	int err=0;
+	unsigned int authsize,maxauthsize;
+	uint8_t *new_iv_ptr_lo = NULL;
+	uint8_t *new_iv_ptr_hi = NULL;
+	struct nlm_crypto_pkt_ctrl *ctrl = &ctx->ctrl;
+	
+	authsize = crypto_aead_crt(crt->base)->authsize;
+	maxauthsize= aead->maxauthsize;
+
+	param = (struct nlm_crypto_pkt_param *)NLM_CRYPTO_PKT_PARAM_OFFSET(aead_request_ctx(req));
+	hash_addr = (uint8_t *)NLM_HASH_OFFSET(aead_request_ctx(req));
+
+	ivsize = crypto_aead_ivsize(crypto_aead_reqtfm(req));
+	auth_off = 0;
+	cipher_off = req->assoclen + ivsize;
+	iv_off = req->assoclen;
+	//check if it should be aip->tag_len or can be taken from tfm
+	cipher_len = op ? req->cryptlen:req->cryptlen - authsize;
+	auth_len = cipher_off + cipher_len;
+	hash_source = op;
+
+	nr_aad_frags = fill_aead_aad(param, req, req->assoclen,seg);
+	if (nr_aad_frags == 0)
+		return -1;
+	seg = nr_aad_frags;
+
+	if (ivsize) {
+		nlm_crypto_fill_src_seg(param, seg, req->iv, ivsize);
+		nlm_crypto_fill_dst_seg(param, seg, req->iv, ivsize);
+		seg++;
+	}
+
+	nr_enc_frags = fill_aead_crypt(req, cipher_len, param, &actual_tag, op, seg);
+
+	if (nr_enc_frags == -1)
+		return -1;
+
+	if(ctx->cbc){
+		uint32_t tmp_len;
+		uint8_t *tmp_virt;
+
+		tmp_len = (param->segment[seg][1] >> 48) + 1;
+		if(tmp_len >= 16){
+			tmp_virt = phys_to_virt((param->segment[seg][1]) & 0xffffffffffULL);
+			new_iv_ptr_hi = tmp_virt;
+			new_iv_ptr_lo = new_iv_ptr_hi + 8;;
+		}else if(tmp_len >=8){
+			tmp_virt = phys_to_virt((param->segment[seg][1]) & 0xffffffffffULL);
+			new_iv_ptr_lo = tmp_virt;
+			/*goto next frag*/
+			if(nr_enc_frags > 1){
+				tmp_virt = phys_to_virt((param->segment[seg + 1][1]) & 0xffffffffffULL);
+				tmp_len = (uint32_t)(param->segment[seg + 1][1] >> 48) + 1;
+				if(tmp_len >= 8)
+					new_iv_ptr_hi = tmp_virt + 8;
+			}
+		}
+	}
+
+	seg += nr_enc_frags;
+
+	nlm_crypto_fill_cipher_auth_pkt_param(ctrl, param, op, op, iv_off, 
+			ivsize, auth_off, auth_len, 0, cipher_off, cipher_len, hash_addr);
+	
+	fb_vc = crypto_get_fb_vc();
+
+	entry0 = nlm_crypto_form_pkt_fmn_entry0(fb_vc, 0, 0, ctrl->cipherkeylen, virt_to_phys(ctrl));
+	entry1 = nlm_crypto_form_pkt_fmn_entry1(0, ctrl->hashkeylen, (seg + 2)<<4, virt_to_phys(param));
+
+
+#ifdef NLM_CRYPTO_DEBUG
+	printk("*****ctrl descriptor phys addr %#lx\n",virt_to_phys(ctrl));
+	print_crypto_msg_desc(entry0, entry1, tx_id);
+	print_cntl_instr(ctrl->desc0);
+	print_buf("ENC_KEY:", (uint8_t *)ctrl->key, ctrl->cipherkeylen);
+	print_buf("AUTH_KEY:", (uint8_t *)ctrl->key + ctrl->cipherkeylen, ctrl->hashkeylen);
+	print_pkt_desc(param,seg);
+	printk("iv_off: %d, cipher_off: %d, auth_off: %d\n",iv_off, cipher_off, auth_off);
+	printk("auth_len: %d, cipher_len: %d\n", auth_len, cipher_len);
+#endif
+
+	async = (struct nlm_async_crypto *)NLM_ASYNC_PTR_PARAM_OFFSET(aead_request_ctx(req));
+	async->callback =  aead_request_callback;
+	async->args = (void *)&req->base; 
+	async->op  = op;
+	async->actual_tag = actual_tag;
+	async->hash_addr = hash_addr;
+	async->authsize = authsize;
+	async->stat = ctx->stat; 
+	async->bytes = req->cryptlen; 
+	tx_id = (uint64_t)async;
+
+	//construct pkt, send to engine and receive reply
+	err = nlm_hal_send_msg3(crypto_vc_base, 0 /*code */ , entry0, entry1, tx_id);
+	if(err){
+		printk("err\n");
+		return -EIO;
+	}
+
+
+	return -EINPROGRESS;
+}
+
+/*
+ *  All Encrypt Functions goes here.
+ */
+
+static int 
+xlp_aes_cbc_encrypt(struct aead_request *req)
+{
+	return aead_crypt(req, NETL_OP_ENCRYPT);
+}
+
+static int
+xlp_3des_cbc_encrypt(struct aead_request *req)
+{
+	 return aead_crypt(req, NETL_OP_ENCRYPT);
+}
+static int 
+xlp_des_cbc_encrypt(struct aead_request *req)
+{
+	return aead_crypt(req, NETL_OP_ENCRYPT);
+}
+
+
+/*
+ *  All Decrypt Functions goes here.
+ */
+
+static int xlp_aes_cbc_decrypt(struct aead_request *req)
+{
+	return aead_crypt(req, NETL_OP_DECRYPT);
+}
+
+static int xlp_3des_cbc_decrypt( struct aead_request *req)
+{
+	return aead_crypt_3des(req, NETL_OP_DECRYPT);
+}
+static int xlp_des_cbc_decrypt(struct aead_request *req)
+{
+	return aead_crypt(req, NETL_OP_DECRYPT);
+}
+/*
+ *  All Givencrypt Functions goes here.
+ */
+
+int xlp_aes_cbc_givencrypt(struct aead_givcrypt_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(&req->areq);
+	struct nlm_aead_ctx *nlm_ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+
+	//TODO: Get the IV from random pool
+	memcpy(req->giv, nlm_ctx->iv_buf, nlm_ctx->iv_len);
+	*(uint64_t *)req->giv += req->seq;
+	memcpy(req->areq.iv, req->giv, nlm_ctx->iv_len);
+
+	return xlp_aes_cbc_encrypt(&req->areq);
+}
+
+static int xlp_3des_cbc_givencrypt(struct aead_givcrypt_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(&req->areq);
+	struct nlm_aead_ctx *nlm_ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+
+	memcpy(req->giv, nlm_ctx->iv_buf, nlm_ctx->iv_len);
+	*(uint64_t *)req->giv += req->seq;
+	memcpy(req->areq.iv, req->giv, nlm_ctx->iv_len);
+
+	return xlp_3des_cbc_encrypt(&req->areq);
+
+}
+static int xlp_des_cbc_givencrypt(struct aead_givcrypt_request *req)
+{
+	struct crypto_aead *tfm = crypto_aead_reqtfm(&req->areq);
+	struct nlm_aead_ctx *nlm_ctx = (struct nlm_aead_ctx *)nlm_crypto_aead_ctx(tfm);
+	
+	memcpy(req->giv, nlm_ctx->iv_buf, nlm_ctx->iv_len);
+	*(uint64_t *)req->giv += req->seq;
+	memcpy(req->areq.iv, req->giv, nlm_ctx->iv_len);
+
+	return xlp_des_cbc_encrypt(&req->areq);
+}
+
+
+/* commented out to avoid the search time */
+static struct crypto_alg xlp_aes_cbc_hmac_sha256_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(sha256),cbc(aes))",
+	.cra_driver_name = "authenc-hmac-sha256-cbc-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = AES_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC ,
+	.cra_ctxsize = CTRL_DESC_SIZE,
+	.cra_type = &crypto_aead_type,
+	.cra_init = aead_cra_cbc_init,
+	.cra_exit = aead_session_cleanup,
+	.cra_aead = {
+		     .setkey = xlp_aes_cbc_hmac_sha256_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_aes_cbc_encrypt,
+		     .decrypt = xlp_aes_cbc_decrypt,
+		     .givencrypt = xlp_aes_cbc_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = AES_BLOCK_SIZE,
+		     .maxauthsize = SHA256_DIGEST_SIZE,
+		     }
+};
+/* commented out to avoid the search time */
+static struct crypto_alg xlp_aes_cbc_hmac_sha1_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(sha1),cbc(aes))",
+	.cra_driver_name = "authenc-hmac-sha1-cbc-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = AES_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC ,
+	.cra_ctxsize = CTRL_DESC_SIZE,
+	.cra_type = &crypto_aead_type,
+	.cra_init = aead_cra_cbc_init,
+	.cra_exit = aead_session_cleanup,
+	.cra_aead = {
+		     .setkey = xlp_aes_cbc_hmac_sha1_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_aes_cbc_encrypt,
+		     .decrypt = xlp_aes_cbc_decrypt,
+		     .givencrypt = xlp_aes_cbc_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = AES_BLOCK_SIZE,
+		     .maxauthsize = SHA1_DIGEST_SIZE,
+		     }
+};
+
+static struct crypto_alg xlp_aes_cbc_aes_xcbc_mac_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(xcbc(aes),cbc(aes))",
+	.cra_driver_name = "authenc-xcbc-mac-aes-cbc-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = AES_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC ,
+	.cra_ctxsize = CTRL_DESC_SIZE,
+	.cra_type = &crypto_aead_type,
+	.cra_init = aead_cra_cbc_init,
+	.cra_exit = aead_session_cleanup,
+	.cra_aead = {
+		     .setkey = xlp_aes_cbc_aes_xcbc_mac_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_aes_cbc_encrypt,
+		     .decrypt = xlp_aes_cbc_decrypt,
+		     .givencrypt = xlp_aes_cbc_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = AES_BLOCK_SIZE,
+		     .maxauthsize = XCBC_DIGEST_SIZE,
+		     }
+};
+
+static struct crypto_alg xlp_aes_cbc_hmac_md5_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(md5),cbc(aes))",
+	.cra_driver_name = "authenc-hmac-md5-cbc-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = AES_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC ,
+	.cra_ctxsize = CTRL_DESC_SIZE,
+	.cra_type = &crypto_aead_type,
+	.cra_init = aead_cra_cbc_init,
+	.cra_exit = aead_session_cleanup,
+	.cra_aead = {
+		     .setkey = xlp_aes_cbc_hmac_md5_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_aes_cbc_encrypt,
+		     .decrypt = xlp_aes_cbc_decrypt,
+		     .givencrypt = xlp_aes_cbc_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = AES_BLOCK_SIZE,
+		     .maxauthsize = MD5_DIGEST_SIZE,
+		     }
+};
+
+static struct crypto_alg xlp_3des_cbc_hmac_md5_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(md5),cbc(des3_ede))",
+	.cra_driver_name = "authenc-hmac-md5-cbc-des3-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+	.cra_ctxsize = DES3_CTRL_DESC_SIZE, 
+	.cra_type = &crypto_aead_type,
+	.cra_init = aead_cra_init,
+	.cra_exit = aead_session_cleanup,
+	.cra_aead = {
+		     .setkey = xlp_3des_cbc_hmac_md5_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_3des_cbc_encrypt,
+		     .decrypt = xlp_3des_cbc_decrypt,
+		     .givencrypt = xlp_3des_cbc_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = DES3_EDE_BLOCK_SIZE,
+		     .maxauthsize = MD5_DIGEST_SIZE,
+		     }
+};
+
+static struct crypto_alg xlp_3des_cbc_hmac_sha256_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(sha256),cbc(des3_ede))",
+	.cra_driver_name = "authenc-hmac-sha256-cbc-des3-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+	.cra_ctxsize = DES3_CTRL_DESC_SIZE,
+	.cra_type = &crypto_aead_type,
+	.cra_init = aead_cra_init,
+	.cra_exit = aead_session_cleanup,
+	.cra_aead = {
+		     .setkey = xlp_3des_cbc_hmac_sha256_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_3des_cbc_encrypt,
+		     .decrypt = xlp_3des_cbc_decrypt,
+		     .givencrypt = xlp_3des_cbc_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = DES3_EDE_BLOCK_SIZE,
+		     .maxauthsize = SHA256_DIGEST_SIZE,
+		     }
+};
+
+static struct crypto_alg xlp_3des_cbc_hmac_sha1_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(hmac(sha1),cbc(des3_ede))",
+	.cra_driver_name = "authenc-hmac-sha1-cbc-des3-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+	.cra_ctxsize = DES3_CTRL_DESC_SIZE,
+	.cra_type = &crypto_aead_type,
+	.cra_init = aead_cra_init,
+	.cra_exit = aead_session_cleanup,
+	.cra_aead = {
+		     .setkey = xlp_3des_cbc_hmac_sha1_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_3des_cbc_encrypt,
+		     .decrypt = xlp_3des_cbc_decrypt,
+		     .givencrypt = xlp_3des_cbc_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = DES3_EDE_BLOCK_SIZE,
+		     .maxauthsize = SHA1_DIGEST_SIZE,
+		     }
+};
+
+static struct crypto_alg xlp_3des_cbc_aes_xcbc_mac_cipher_auth = {
+	/* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+	.cra_name = "authenc(xcbc(aes),cbc(des3_ede))",
+	.cra_driver_name = "authenc-aes-xcbc-cbc-des3-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+	.cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+	.cra_ctxsize = DES3_CTRL_DESC_SIZE,
+	.cra_type = &crypto_aead_type,
+	.cra_exit = aead_session_cleanup,
+	.cra_init = aead_cra_init,
+	.cra_aead = {
+		     .setkey = xlp_3des_cbc_aes_xcbc_mac_setkey,
+		     .setauthsize = aead_setauthsize,
+		     .encrypt = xlp_3des_cbc_encrypt,
+		     .decrypt = xlp_3des_cbc_decrypt,
+		     .givencrypt = xlp_3des_cbc_givencrypt,
+		     .geniv = "<built-in>",
+		     .ivsize = DES3_EDE_BLOCK_SIZE,
+		     .maxauthsize = XCBC_DIGEST_SIZE,
+		     }
+};
+
+
+static struct crypto_alg xlp_des_cbc_aes_xcbc_mac_cipher_auth = {
+        /* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+        .cra_name = "authenc(xcbc(aes),cbc(des))",
+        .cra_driver_name = "authenc-xcbc-mac-aes-cbc-des-xlp",
+        .cra_priority = XLP_CRYPT_PRIORITY,
+        .cra_blocksize = DES_BLOCK_SIZE,
+        .cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+        .cra_ctxsize = CTRL_DESC_SIZE,
+        .cra_type = &crypto_aead_type,
+        .cra_init = aead_cra_cbc_init,
+        .cra_exit = aead_session_cleanup,
+        .cra_aead = {
+                     .setkey = xlp_des_cbc_aes_xcbc_mac_setkey,
+                     .setauthsize = aead_setauthsize,
+                     .encrypt = xlp_des_cbc_encrypt,
+                     .decrypt = xlp_des_cbc_decrypt,
+                     .givencrypt = xlp_des_cbc_givencrypt,
+                     .geniv = "<built-in>",
+                     .ivsize = DES_BLOCK_SIZE,
+                     .maxauthsize = MD5_DIGEST_SIZE,
+                     }
+};
+static struct crypto_alg xlp_des_cbc_hmac_md5_cipher_auth = {
+        /* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+        .cra_name = "authenc(hmac(md5),cbc(des))",
+        .cra_driver_name = "authenc-hmac-md5-cbc-des-xlp",
+        .cra_priority = XLP_CRYPT_PRIORITY,
+        .cra_blocksize = DES_BLOCK_SIZE,
+        .cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+        .cra_ctxsize = CTRL_DESC_SIZE,
+        .cra_type = &crypto_aead_type,
+        .cra_init = aead_cra_cbc_init,
+        .cra_exit = aead_session_cleanup,
+        .cra_aead = {
+                     .setkey = xlp_des_cbc_hmac_md5_setkey,
+                     .setauthsize = aead_setauthsize,
+                     .encrypt = xlp_des_cbc_encrypt,
+                     .decrypt = xlp_des_cbc_decrypt,
+                     .givencrypt = xlp_des_cbc_givencrypt,
+                     .geniv = "<built-in>",
+                     .ivsize = DES_BLOCK_SIZE,
+                     .maxauthsize = MD5_DIGEST_SIZE,
+                     }
+};
+
+static struct crypto_alg xlp_des_cbc_hmac_sha256_cipher_auth = {
+        /* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+        .cra_name = "authenc(hmac(sha256),cbc(des))",
+        .cra_driver_name = "authenc-hmac-sha256-cbc-des-xlp",
+        .cra_priority = XLP_CRYPT_PRIORITY,
+        .cra_blocksize = DES_BLOCK_SIZE,
+        .cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+        .cra_ctxsize = CTRL_DESC_SIZE,
+        .cra_type = &crypto_aead_type,
+        .cra_init = aead_cra_cbc_init,
+        .cra_exit = aead_session_cleanup,
+        .cra_aead = {
+                     .setkey = xlp_des_cbc_hmac_sha256_setkey,
+                     .setauthsize = aead_setauthsize,
+                     .encrypt = xlp_des_cbc_encrypt,
+                     .decrypt = xlp_des_cbc_decrypt,
+                     .givencrypt = xlp_des_cbc_givencrypt,
+                     .geniv = "<built-in>",
+                     .ivsize = DES_BLOCK_SIZE,
+                     .maxauthsize = SHA256_DIGEST_SIZE,
+                     }
+};
+static struct crypto_alg xlp_des_cbc_hmac_sha1_cipher_auth = {
+        /* AEAD algorithms.  These use a single-pass ipsec_esp descriptor */
+        .cra_name = "authenc(hmac(sha1),cbc(des))",
+        .cra_driver_name = "authenc-hmac-sha1-cbc-des-xlp",
+        .cra_priority = XLP_CRYPT_PRIORITY,
+        .cra_blocksize = DES_BLOCK_SIZE,
+        .cra_flags = CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC,
+        .cra_ctxsize = CTRL_DESC_SIZE,
+        .cra_type = &crypto_aead_type,
+        .cra_exit = aead_session_cleanup,
+        .cra_init = aead_cra_cbc_init,
+        .cra_aead = {
+                     .setkey = xlp_des_cbc_hmac_sha1_setkey,
+                     .setauthsize = aead_setauthsize,
+                     .encrypt = xlp_des_cbc_encrypt,
+                     .decrypt = xlp_des_cbc_decrypt,
+                     .givencrypt = xlp_des_cbc_givencrypt,
+                     .geniv = "<built-in>",
+                     .ivsize = DES_BLOCK_SIZE,
+                     .maxauthsize = SHA1_DIGEST_SIZE,
+                     }
+};
+
+
+int xlp_aead_alg_init(void)
+{
+	int ret = 0;
+	int no_of_alg_registered = 0;
+	
+	if ((ret = crypto_register_alg(&xlp_aes_cbc_hmac_sha256_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+
+	if ((ret = crypto_register_alg(&xlp_aes_cbc_hmac_sha1_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+
+	if ((ret = crypto_register_alg(&xlp_aes_cbc_aes_xcbc_mac_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+
+	if ((ret = crypto_register_alg(&xlp_aes_cbc_hmac_md5_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+
+	if ((ret = crypto_register_alg(&xlp_3des_cbc_hmac_md5_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+
+	if ((ret = crypto_register_alg(&xlp_3des_cbc_hmac_sha256_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+
+	if ((ret = crypto_register_alg(&xlp_3des_cbc_hmac_sha1_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+
+	if ((ret = crypto_register_alg(&xlp_3des_cbc_aes_xcbc_mac_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+	if (( ret = crypto_register_alg(&xlp_des_cbc_hmac_md5_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+	if ( (ret = crypto_register_alg(&xlp_des_cbc_hmac_sha256_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+	if ((ret =  crypto_register_alg(&xlp_des_cbc_hmac_sha1_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+	if ((ret =  crypto_register_alg(&xlp_des_cbc_aes_xcbc_mac_cipher_auth)))
+		goto end;
+	no_of_alg_registered++;
+
+end:
+	return no_of_alg_registered;
+} 
+
+void
+xlp_aead_alg_fini(void)
+{
+	
+	crypto_unregister_alg(&xlp_aes_cbc_hmac_sha1_cipher_auth);
+	crypto_unregister_alg(&xlp_aes_cbc_aes_xcbc_mac_cipher_auth);
+	crypto_unregister_alg(&xlp_aes_cbc_hmac_md5_cipher_auth);
+	crypto_unregister_alg(&xlp_3des_cbc_hmac_md5_cipher_auth);
+	crypto_unregister_alg(&xlp_3des_cbc_hmac_sha256_cipher_auth);
+	crypto_unregister_alg(&xlp_3des_cbc_hmac_sha1_cipher_auth);
+	crypto_unregister_alg(&xlp_3des_cbc_aes_xcbc_mac_cipher_auth);
+	crypto_unregister_alg(&xlp_des_cbc_hmac_md5_cipher_auth);
+	crypto_unregister_alg(&xlp_des_cbc_hmac_sha256_cipher_auth);
+	crypto_unregister_alg(&xlp_des_cbc_hmac_sha1_cipher_auth);
+	crypto_unregister_alg(&xlp_aes_cbc_hmac_sha256_cipher_auth);
+	crypto_unregister_alg(&xlp_des_cbc_aes_xcbc_mac_cipher_auth);
+	return;
+}
+
+EXPORT_SYMBOL(xlp_aead_alg_init);
+EXPORT_SYMBOL(xlp_aead_alg_fini);
diff --git a/drivers/crypto/nlm-sae/nlm_async.h b/drivers/crypto/nlm-sae/nlm_async.h
new file mode 100644
index 0000000..639805a
--- /dev/null
+++ b/drivers/crypto/nlm-sae/nlm_async.h
@@ -0,0 +1,86 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#NETL_2#********************************/
+#ifndef __NLM_ASYNC_H
+#define __NLM_ASYNC_H
+struct nlm_async_crypto;
+#define MAX_CPU 32
+extern int crypto_get_fb_vc(void);
+
+#define nlm_processor_id()                              \
+        ({ int __res;                                   \
+         __asm__ __volatile__(                          \
+                 ".set\tmips32\n\t"                     \
+                 "mfc0\t%0, $15, 1\n\t"                 \
+                 "andi\t%0, 0x1f\n\t"                   \
+                 ".set\tmips0\n\t"                      \
+                 : "=r" (__res));                       \
+         __res;                                         \
+         })
+
+
+
+
+struct nlm_async_crypto
+{
+	void (*callback) (struct nlm_async_crypto *args, uint64_t entry1);
+	void *args;
+	int op;
+	int authsize;
+	uint8_t *actual_tag;
+	uint8_t *hash_addr;
+	uint16_t stat;
+	uint32_t bytes;
+};
+
+enum enc_stat {
+	DES_CBC_STAT = 0 ,
+	TDES_CBC_STAT ,
+	AES128_CBC_STAT ,
+	AES192_CBC_STAT ,
+	AES256_CBC_STAT, 
+	ENC_MAX_STAT
+};
+enum {
+	MD5_STAT,
+	H_SHA1_STAT,
+	H_SHA256_STAT,
+	AES128_XCBC_STAT,
+	AES192_XCBC_STAT,
+	AES256_XCBC_STAT,
+	AUTH_MAX_STAT
+};
+
+struct nlm_crypto_stat
+{
+	uint64_t enc[ENC_MAX_STAT];
+	uint64_t enc_tbytes[ENC_MAX_STAT];
+	uint64_t auth[AUTH_MAX_STAT];
+	uint64_t auth_tbytes[AUTH_MAX_STAT];
+};
+
+extern int crypto_vc_base;
+extern int crypto_vc_limit;
+
+#endif
diff --git a/drivers/crypto/nlm-sae/nlm_auth.c b/drivers/crypto/nlm-sae/nlm_auth.c
new file mode 100644
index 0000000..4cb5b1d
--- /dev/null
+++ b/drivers/crypto/nlm-sae/nlm_auth.c
@@ -0,0 +1,432 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#NETL_2#********************************/
+#include <crypto/sha.h>
+#include <nlm_hal_fmn.h>
+#include <crypto/aes.h>
+#include <crypto/internal/hash.h>
+#include <asm/netlogic/crypto/nlmcrypto.h>
+#include "nlm_async.h"
+
+#define XLP_AUTH_PRIORITY      300
+#define XLP_HMAC_PRIORITY      300
+
+#define XCBC_DIGEST_SIZE	16
+
+#define MD5_DIGEST_SIZE		16
+#define MD5_BLOCK_SIZE		64
+
+//#define AUTH_BUFFER_SIZE	(16 * 1024)
+void hex_dump(char * description,unsigned char *in, int num);
+extern int auth_mode_key_len[NLM_HASH_MAX][NLM_HASH_MODE_MAX];
+
+#define ASYNC_PTR_SIZE 128
+#define ASYNC_PTR_OFFSET (sizeof(struct auth_pkt_desc ) + (NLM_AUTH_MAX_FRAGS* 16))
+
+//#define SEC_DEBUG
+
+#ifdef SEC_DEBUG
+#ifdef __KERNEL__
+#define debug_print(fmt, args...) printk(fmt, ##args)
+#else				/* __KERNEL__ */
+#define debug_print(fmt, args...) printf(fmt, ##args)
+#endif				/* __KERNEL__ */
+#else				/* SEC_DEBUG */
+#define debug_print(fmt, args...)
+#endif				/* SEC_DEBUG */
+
+#define malloc(a) kmalloc(a, GFP_KERNEL)
+#define free kfree
+#define NLM_AUTH_MAX_FRAGS	(20)
+
+
+struct nlm_auth_ctx
+{
+	struct nlm_crypto_pkt_ctrl ctrl;
+	uint16_t stat;
+	/*Don't change the order of this strucutre*/
+};
+
+#define MAX_FRAGS               18
+#define CTRL_DESC_SIZE          (sizeof(struct nlm_auth_ctx) + 64)
+
+struct auth_pkt_desc
+{
+	uint32_t curr_index;
+	uint32_t total_len;
+	uint8_t pad[56];
+	//struct pkt_desc pkt_desc;
+	struct nlm_crypto_pkt_param pkt_param; 
+	uint16_t stat;
+};
+
+#define PACKET_DESC_SIZE   (64+sizeof(struct auth_pkt_desc) + MAX_FRAGS*(2*64) + 64 + sizeof(struct nlm_async_crypto) + 64)
+#define NLM_CRYPTO_PKT_PARAM_OFFSET(addr)       (((unsigned long)addr + 64) & ~0x3fULL)
+#define NLM_ASYNC_PTR_PARAM_OFFSET(addr)        (((unsigned long)(addr + 64 + sizeof(struct nlm_crypto_pkt_param) + MAX_FRAGS*(2*64)) + 64) & ~0x3fULL)
+/*
+   All extern declaration goes here.
+ */
+extern uint32_t nlm_hal_send_msg3(uint32_t dst, uint32_t code, uint64_t data0, uint64_t data1, uint64_t data2);
+extern uint32_t nlm_hal_recv_msg2(uint32_t dst, uint32_t *src, uint32_t *size, uint32_t *code, uint64_t *data0, uint64_t *data1);
+extern void print_cntl_instr(uint64_t cntl_desc);
+extern void print_crypto_msg_desc(uint64_t entry1, uint64_t entry2, uint64_t entry3);
+extern void print_pkt_desc(struct nlm_crypto_pkt_param * pkt_param, int index);
+extern struct nlm_crypto_stat crypto_stat[MAX_CPU];
+
+static inline void print_info(const char *func)
+{
+	extern void dump_stack(void);
+	printk("\n********[%s function called]**********\n",func);
+	dump_stack();
+	printk("\n*********[%s Dumpstack ends]***********\n\n\n",func);
+	return;
+}
+#ifdef NLM_CRYPTO_DEBUG
+static void print_buf(unsigned char *msg, unsigned char *buf, int len)
+{
+#define TMP_BUF		50
+	char tmp_buf[TMP_BUF + 1];
+	int i, index = 0;
+
+	printk("**********%s************\n",msg);
+	for(i=0; i<len; i++){
+		sprintf(&tmp_buf[index*2], "%02x", buf[i]);
+		index++;
+		if(index == (TMP_BUF/2)){
+			tmp_buf[index*2] = '\0';
+			printk("[%s]\n",tmp_buf);
+			index = 0;
+		}
+	}
+	if(index){
+		tmp_buf[index*2] = '\0';
+		printk("[%s]\n",tmp_buf);
+	}
+}
+#endif
+
+static struct nlm_auth_ctx *nlm_shash_auth_ctx(struct crypto_shash *shash)
+{
+	uint8_t *ctx = crypto_tfm_ctx(crypto_shash_tfm(shash));
+	ctx = (uint8_t *)(((unsigned long)ctx + 63) & ~(0x3f));
+	return (struct nlm_auth_ctx *)ctx;
+}
+
+static struct nlm_auth_ctx *pkt_ctrl_auth_ctx(struct shash_desc *desc)
+{
+	return nlm_shash_auth_ctx(desc->tfm);
+}
+static int
+xlp_auth_init(struct shash_desc *desc)
+{
+	struct auth_pkt_desc *auth_pkt_desc = (struct auth_pkt_desc * )NLM_CRYPTO_PKT_PARAM_OFFSET(shash_desc_ctx(desc));
+	//printk("[%s] [%p]\n",__FUNCTION__, pkt_desc);
+	//printk("[%s] [%p]\n",__FUNCTION__, &pkt_desc->pkt_param.desc0);
+	//printk("[%s] [%p]\n",__FUNCTION__, &pkt_desc->pkt_param);
+	auth_pkt_desc->curr_index = 0;
+	auth_pkt_desc->total_len = 0;
+	return 0;
+}
+
+static int
+xlp_auth_update(struct shash_desc *desc,
+		const uint8_t * data, unsigned int length)
+{
+	struct auth_pkt_desc *auth_pkt_desc = (struct auth_pkt_desc *)NLM_CRYPTO_PKT_PARAM_OFFSET(shash_desc_ctx(desc));
+	int index = auth_pkt_desc->curr_index;
+	struct nlm_crypto_pkt_param  *pkt_param  = &(auth_pkt_desc->pkt_param);
+	
+	nlm_crypto_fill_src_seg(pkt_param, index, (unsigned char*)data, length);
+	auth_pkt_desc->curr_index = nlm_crypto_fill_dst_seg(pkt_param, index , (unsigned char*)data, length);
+	auth_pkt_desc->total_len += length;
+
+	return 0;
+}
+static int
+crypto_get_sync_fb_vc(void)
+{
+    int cpu;
+    extern int ipsec_sync_vc;
+
+    cpu = hard_smp_processor_id();      //processor_id();
+    cpu = cpu * 4 + ipsec_sync_vc;
+
+    return cpu;
+}
+
+static int
+xlp_auth_final(struct shash_desc *desc, uint8_t *out)
+{
+	struct auth_pkt_desc *auth_pkt_desc = (struct auth_pkt_desc *)NLM_CRYPTO_PKT_PARAM_OFFSET(shash_desc_ctx(desc));
+	int index = auth_pkt_desc->curr_index;
+	struct nlm_crypto_pkt_param  *pkt_param  = &(auth_pkt_desc->pkt_param);
+	struct nlm_auth_ctx  * auth_ctx   = pkt_ctrl_auth_ctx(desc);
+	int fb_vc ;
+	uint64_t entry0, entry1, tx_id=0x12345678;
+	uint64_t  timeout = 0;
+	struct nlm_crypto_pkt_ctrl *ctrl = &auth_ctx->ctrl;
+	uint32_t size,code,src;
+	uint16_t stat = auth_ctx->stat;
+	int cpu = nlm_processor_id();
+        extern int ipsec_sync_vc;
+
+	nlm_crypto_fill_auth_pkt_param(ctrl,pkt_param,
+			0,auth_pkt_desc->total_len,0,out); 
+
+	preempt_disable();
+
+	fb_vc = crypto_get_sync_fb_vc();
+	entry0 = nlm_crypto_form_pkt_fmn_entry0(fb_vc, 0, 0, 0, virt_to_phys(ctrl));
+	entry1 = nlm_crypto_form_pkt_fmn_entry1(0, ctrl->hashkeylen, (32 + index * 16 ), virt_to_phys(pkt_param));
+	
+
+#ifdef NLM_CRYPTO_DEBUG
+	print_crypto_msg_desc(entry0, entry1, tx_id);
+	print_cntl_instr(ctrl->desc0);
+	print_pkt_desc(pkt_param,index);
+#endif
+
+	//construct pkt, send to engine and receive reply
+	xlp_message_send_block_fast_3(0, crypto_vc_base, entry0, entry1, tx_id);
+	timeout = 0;
+	do {
+		timeout++;
+		nlm_hal_recv_msg2(ipsec_sync_vc, &src, &size, &code, &entry0, &entry1);
+	} while(entry0 != tx_id && timeout < 0xffffffff) ;
+	
+
+
+	if (timeout >= 0xffffffff) {
+		printk("Error: FreeBack message is not received");
+		preempt_enable();
+		return -EIO;
+	}
+#ifdef NLM_CRYPTO_DEBUG
+	print_buf("AUTH:", out, 16);
+#endif
+	crypto_stat[cpu].auth[stat] ++;
+	crypto_stat[cpu].auth_tbytes[stat] += auth_pkt_desc->total_len + ctrl->taglen;
+	preempt_enable();
+ 	return 0;
+}
+
+/*
+   All Setkey goes here.
+ */
+
+static int xlp_auth_aes_xcbc_setkey(struct crypto_shash *tfm, const u8 * key, unsigned int keylen)
+{
+	struct nlm_auth_ctx * nlm_ctx = nlm_shash_auth_ctx(tfm);
+	uint32_t hash_alg = NLM_HASH_AES128;
+
+        switch (keylen) {
+        case 16:
+                hash_alg = NLM_HASH_AES128;
+		nlm_ctx->stat = AES128_XCBC_STAT;
+                break;
+        case 24:
+                hash_alg = NLM_HASH_AES192;
+		nlm_ctx->stat = AES192_XCBC_STAT;
+                break;
+        case 32:
+                hash_alg = NLM_HASH_AES256;
+		nlm_ctx->stat = AES256_XCBC_STAT;
+                break;
+        default:
+                printk(KERN_WARNING "[%s]: Cannot handle keylen = %d\n",
+                       __FUNCTION__, keylen);
+	}
+
+
+	/*setup ctrl descriptor*/
+	nlm_crypto_fill_pkt_ctrl(&nlm_ctx->ctrl,0,hash_alg,NLM_HASH_MODE_XCBC,
+		NLM_CIPHER_BYPASS,NLM_CIPHER_MODE_ECB,0,NULL,0,(unsigned char *)key,keylen);
+	return 0;
+	
+}
+
+
+static int
+xlp_auth_hmac_sha256_setkey(struct crypto_shash *tfm, const u8 * key, unsigned int keylen)
+{
+	struct nlm_auth_ctx * nlm_ctx = nlm_shash_auth_ctx(tfm);
+	struct nlm_crypto_pkt_ctrl * ctrl = &nlm_ctx->ctrl; 
+	nlm_ctx->stat = H_SHA256_STAT;
+
+	/*setup ctrl descriptor*/
+	nlm_crypto_fill_pkt_ctrl(ctrl,1,NLM_HASH_SHA,NLM_HASH_MODE_SHA256,
+		NLM_CIPHER_BYPASS,NLM_CIPHER_MODE_ECB,0,NULL,0,(unsigned char*)key,keylen);
+	if ( ctrl->hashkeylen < auth_mode_key_len[NLM_HASH_SHA][NLM_HASH_MODE_SHA256]) 
+		ctrl->hashkeylen = auth_mode_key_len[NLM_HASH_SHA][NLM_HASH_MODE_SHA256];
+	return 0;
+	
+}
+
+
+static int
+xlp_auth_hmac_md5_setkey(struct crypto_shash *tfm, const u8 * key, unsigned int keylen)
+{
+	struct nlm_auth_ctx * nlm_ctx = nlm_shash_auth_ctx(tfm);
+	struct nlm_crypto_pkt_ctrl * ctrl = &nlm_ctx->ctrl; 
+	nlm_ctx->stat = MD5_STAT;
+
+	/*setup ctrl descriptor*/
+	nlm_crypto_fill_pkt_ctrl(ctrl,1,NLM_HASH_MD5,NLM_HASH_MODE_SHA1,
+		NLM_CIPHER_BYPASS,NLM_CIPHER_MODE_ECB,0,NULL,0,(unsigned char *)key,keylen);
+	if ( ctrl->hashkeylen < auth_mode_key_len[NLM_HASH_MD5][NLM_HASH_MODE_SHA1]) 
+		ctrl->hashkeylen = auth_mode_key_len[NLM_HASH_MD5][NLM_HASH_MODE_SHA1];
+	return 0;
+	
+}
+
+static int
+xlp_auth_hmac_sha1_setkey(struct crypto_shash *tfm, const u8 * key, unsigned int keylen)
+{
+	struct nlm_auth_ctx * nlm_ctx = nlm_shash_auth_ctx(tfm);
+	struct nlm_crypto_pkt_ctrl * ctrl = &nlm_ctx->ctrl; 
+	nlm_ctx->stat = H_SHA1_STAT;
+
+	/*setup ctrl descriptor*/
+	nlm_crypto_fill_pkt_ctrl(ctrl,1,NLM_HASH_SHA,NLM_HASH_MODE_SHA1,
+		NLM_CIPHER_BYPASS,NLM_CIPHER_MODE_ECB,0,NULL,0,(unsigned char *)key,keylen);
+	if ( ctrl->hashkeylen < auth_mode_key_len[NLM_HASH_SHA][NLM_HASH_MODE_SHA1]) 
+		ctrl->hashkeylen = auth_mode_key_len[NLM_HASH_SHA][NLM_HASH_MODE_SHA1];
+	return 0;
+	 
+	
+}
+
+static struct shash_alg xcbc_mac_alg = {
+	.digestsize = XCBC_DIGEST_SIZE,
+	.init = xlp_auth_init,
+	.update = xlp_auth_update,
+	.final = xlp_auth_final,
+	.setkey = xlp_auth_aes_xcbc_setkey,
+	.descsize = PACKET_DESC_SIZE,
+	.base = {
+		 .cra_name = "xcbc(aes)",
+		 .cra_driver_name = "xcbc-aes-xlp",
+		 .cra_priority = XLP_HMAC_PRIORITY,
+		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_blocksize = AES_BLOCK_SIZE,
+		 .cra_module = THIS_MODULE,
+		 .cra_ctxsize = CTRL_DESC_SIZE, 
+		 }
+};
+
+static struct shash_alg sha256_hmac_alg = {
+	.digestsize = SHA256_DIGEST_SIZE,
+	.init = xlp_auth_init,
+	.update = xlp_auth_update,
+	.final = xlp_auth_final,
+	.setkey = xlp_auth_hmac_sha256_setkey,
+	.descsize = PACKET_DESC_SIZE,
+	.base = {
+		 .cra_name = "hmac(sha256)",
+		 .cra_driver_name = "hmac-sha256-xlp",
+		 .cra_priority = XLP_HMAC_PRIORITY,
+		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_blocksize = SHA256_BLOCK_SIZE,
+		 .cra_module = THIS_MODULE,
+		 .cra_ctxsize = CTRL_DESC_SIZE, 
+		 }
+};
+
+static struct shash_alg md5_hmac_alg = {
+	.digestsize = MD5_DIGEST_SIZE,
+	.init = xlp_auth_init,
+	.update = xlp_auth_update,
+	.final = xlp_auth_final,
+	.setkey = xlp_auth_hmac_md5_setkey,
+	.descsize = PACKET_DESC_SIZE,
+	.base = {
+		 .cra_name = "hmac(md5)",
+		 .cra_driver_name = "hmac-md5-xlp",
+		 .cra_priority = XLP_HMAC_PRIORITY,
+		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_blocksize = MD5_BLOCK_SIZE,
+		 .cra_module = THIS_MODULE,
+		 .cra_ctxsize = CTRL_DESC_SIZE, 
+		 }
+};
+static struct shash_alg sha1_hmac_alg = {
+	.digestsize = SHA1_DIGEST_SIZE,
+	.init = xlp_auth_init,
+	.update = xlp_auth_update,
+	.final = xlp_auth_final,
+	.setkey = xlp_auth_hmac_sha1_setkey,
+	.descsize = PACKET_DESC_SIZE,
+	.base = {
+		 .cra_name = "hmac(sha1)",
+		 .cra_driver_name = "hmac-sha1-xlp",
+		 .cra_priority = XLP_HMAC_PRIORITY,
+		 .cra_flags = CRYPTO_ALG_TYPE_SHASH,
+		 .cra_blocksize = SHA1_BLOCK_SIZE,
+		 .cra_module = THIS_MODULE,
+		 .cra_ctxsize = CTRL_DESC_SIZE, 
+		 }
+};
+
+int
+xlp_auth_alg_init(void)
+{
+	int rc = -ENODEV;
+	int no_of_alg_registered = 0;
+
+	rc = crypto_register_shash(&sha1_hmac_alg);
+	if (rc)
+		goto out;
+	no_of_alg_registered++;
+	rc = crypto_register_shash(&sha256_hmac_alg);
+	if (rc)
+		goto out;
+	no_of_alg_registered++;
+	rc = crypto_register_shash(&md5_hmac_alg);
+	if (rc)
+		goto out;
+	no_of_alg_registered++;
+	rc = crypto_register_shash(&xcbc_mac_alg);
+	if (rc)
+		goto out;
+	no_of_alg_registered++;
+	//printk("Some of the FIPS test failed as the maximum key length supported is 64 bytes.\n");
+
+	printk(KERN_NOTICE "Using XLP hardware for SHA/MD5 algorithms.\n");
+out:
+
+	return 0;
+
+}
+
+void
+xlp_auth_alg_fini(void)
+{
+	crypto_unregister_shash(&sha1_hmac_alg);
+	crypto_unregister_shash(&md5_hmac_alg);
+	crypto_unregister_shash(&sha256_hmac_alg);
+	crypto_unregister_shash(&xcbc_mac_alg);
+}
+
+EXPORT_SYMBOL(xlp_auth_alg_init);
+EXPORT_SYMBOL(xlp_auth_alg_fini);
diff --git a/drivers/crypto/nlm-sae/nlm_crypto.c b/drivers/crypto/nlm-sae/nlm_crypto.c
new file mode 100644
index 0000000..0b54e35
--- /dev/null
+++ b/drivers/crypto/nlm-sae/nlm_crypto.c
@@ -0,0 +1,610 @@
+/***********************************************************************
+  Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+  reserved.
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+ *****************************#NETL_2#********************************/
+
+#include <asm/netlogic/msgring.h>
+#include <linux/proc_fs.h>
+#include <asm/netlogic/proc.h>
+
+#include <asm/netlogic/hal/nlm_hal_fmn.h>
+#include <asm/netlogic/hal/nlm_hal_macros.h>
+#include <linux/crypto.h>
+#include "nlm_async.h"
+#include <asm/netlogic/crypto/nlmcrypto.h>
+
+
+#define XLP_POLLING 1
+#ifdef TRACING
+#define TRACE_TEXT(str) printk(str);
+#define TRACE_RET printk(")")
+#else				/* !TRACING */
+#define TRACE_TEXT(str) ((void) 0)
+#define TRACE_RET ((void) 0)
+#endif				/* TRACING */
+#undef NLM_CRYPTO_DEBUG
+
+#define DRIVER_NAME "nlmsae"
+
+#define NLM_CRYPTO_OP_IN_PROGRESS 0
+#define NLM_CRYPTO_OP_DONE	  1
+
+/**
+ * @file_name crypto.c
+ */
+
+/**
+ * @defgroup crypto Crypto API
+ * @brief Description about the crypto apis
+ */
+
+#define printf(a, b...) printk(KERN_ERR a, ##b)
+//#define printf(a, b...)
+#define malloc(a) kmalloc(a, GFP_ATOMIC)
+#define free kfree
+
+#define xtract_bits(x, bitpos, numofbits) ((x) >> (bitpos) & ((1ULL << (numofbits)) - 1))
+
+#define VC_MODE_ROUND_ROBIN 1
+#define NUM_VC 16
+
+extern struct proc_dir_entry *nlm_root_proc;
+extern int xlp_aead_alg_init(void);
+extern void xlp_aead_alg_fini(void);
+extern int xlp_crypt_alg_init(void);
+extern void xlp_crypt_alg_fini(void);
+extern int xlp_auth_alg_init(void);
+extern void xlp_auth_alg_fini(void);
+static void xlp_sae_cleanup(void);
+
+static int xlp_sae_major;
+static int xlp_sae_open(struct inode *, struct file *);
+static int xlp_sae_release(struct inode *, struct file *);
+
+struct nlm_crypto_stat crypto_stat[MAX_CPU];
+int crypto_vc_base;
+int crypto_vc_limit;
+
+
+
+/*
+ * is the following table needed for all modes?
+Cipher            keylen           iv_len
+*/
+
+//-1 indicates variable length IV
+// In case of AES/Camelia cipher and CBC-MAC auth, IV is not needed.
+// In case of AES/Camelia cipher and XCBC-MAC auth, IV is needed only for 
+//CBC, CFB, OFB and CTR modes..
+int cipher_mode_iv_len[NLM_CIPHER_MAX][NLM_CIPHER_MODE_MAX] = {
+/*		       ECB  CBC   CFB   OFB   CTR  AESF8    GCM  CCM    8   9  LRW   XTS */
+/* BYPASS */       {   0,    0,    0,    0,    0,    0,      0,    0,   0,  0,  0,    0,},
+/* DES */          {   0,    8,    0,    0,    0,    0,      0,    0,   0,  0,  0,    0,},
+/* 3DES */         {   0,    8,    0,    0,    0,    0,      0,    0,   0,  0,  0,    0,},
+/* AES128 */       {   0,    16,   16,   16,   8,   16,     8,   8,   0,  0,  16,   16,},
+/* AES192 */       {   0,    16,   16,   16,   8,   16,     8,   8,   0,  0,  16,   16,},
+/* AES256 */       {   0,    16,   16,   16,   8,   16,     8,   8,   0,  0,  16,   16,},
+/* ARC4 */         {   0,    0,    0,    0,    0,    0,      0,    0,   0,  0,  0,    0,},
+/* KASUMI F8 */    {   0,    0,    0,    0,    0,    0,      0,    0,   0,  0,  0,    0,},
+/* SNOW3G F8 */    {   0,    0,    0,    0,    0,    0,      0,    0,   0,  0,  0,    0,},
+/* CAMELLIA128 */  {   0,    16,   16,   16,   16,   16,     -1,   0,   0,  0,  16,   16,},
+/* CAMELLIA192 */  {   0,    16,   16,   16,   16,   16,     -1,   0,   0,  0,  16,   16,}, 
+/* CAMELLIA256 */  {   0,    16,   16,   16,   16,   16      -1,   0,   0,  0,  16,   16,},
+};
+
+int auth_mode_key_len[NLM_HASH_MAX][NLM_HASH_MODE_MAX] = {
+/*	               SHA1 SHA224 SHA256 SHA384 SHA512  CMAC  XCBC CBC_MAC CCM  GCM*/
+/* BYPASS */		{0,    0,     0,     0,     0,     0,   0,    0,     0,    0, },
+/* MD5 */		{64,   64,    64,   64,    64,    64,  64,   64,    64,   64, },
+/* SHA */		{64,   64,    64,   128,   128,    0,   0,    0,     0,    0, },
+/* 3 */			{0,    0,     0,     0,     0,     0,   0,    0,     0,    0, },
+/* AES128 */		{0,    0,     0,     0,     0,    16,  16,   16,    16,   16, },
+/* AES192 */		{0,    0,     0,     0,     0,    24,  24,   24,    24,   24, },
+/* AES256 */		{0,    0,     0,     0,     0,    32,  32,   32,    32,   32, },
+/* KASUMI_F9 */		{16,  16,    16,    16,    16,    16,  16,   16,    16,   16, },
+/* SNOW3G_F9 */		{16,  16,    16,    16,    16,    16,  16,   16,    16,   16, }, //sandip -> verify
+/* CAMELLIA128 */	{0,    0,     0,     0,     0,    16,  16,   16,    16,   16, },
+/* CAMELLIA192 */	{0,    0,     0,     0,     0,    24,  24,   24,    24,   24, },
+/* CAMELLIA256 */	{0,    0,     0,     0,     0,    32,  32,   32,    32,   32, },
+/* GHASH */		{0,    0,     0,     0,     0,    32,  32,   32,    32,   32, }, //todo:
+};
+
+#define MAX_KEY_LEN_IN_DW 20
+#define NLM_CRYPTO_MAX_STR_LEN 200
+static char str_cipher_alg[NLM_CIPHER_MAX + 1][NLM_CRYPTO_MAX_STR_LEN] = {
+"bypass",       // NLM_CIPHER_BYPASS
+"des",          // NLM_CIPHER_DES
+"3des",         // NLM_CIPHER_3DES
+"aes 128",      // NLM_CIPHER_AES128
+"aes 192",      // NLM_CIPHER_AES192
+"aes 256",      // NLM_CIPHER_AES256
+"arc4",         // NLM_CIPHER_ARC4
+"Kasumi f8",    // NLM_CIPHER_KASUMI_F8
+"snow3g f8",    // NLM_CIPHER_SNOW3G_F8
+"camellia 128", // NLM_CIPHER_CAMELLIA128
+"camelia 192",  // NLM_CIPHER_CAMELLIA192
+"camelia 256",  // NLM_CIPHER_CAMELLIA256
+"undefined",  // > max
+};
+static char str_cipher_mode[NLM_CIPHER_MODE_MAX+ 1][NLM_CRYPTO_MAX_STR_LEN] = {
+"ecb",          // NLM_CIPHER_MODE_ECB
+"cbc",          // NLM_CIPHER_MODE_CBC
+"cfb",          // NLM_CIPHER_MODE_CFB
+"ofb",          // NLM_CIPHER_MODE_OFB
+"ctr",          // NLM_CIPHER_MODE_CTR
+"aes f8",       // NLM_CIPHER_MODE_AES_F8
+"gcm",          // NLM_CIPHER_MODE_GCM
+"ccm",          // NLM_CIPHER_MODE_CCM
+"undefined",    // NLM_CIPHER_MODE_UNDEFINED1
+"undefined",    // NLM_CIPHER_MODE_UNDEFINED2
+"lrw",          // NLM_CIPHER_MODE_LRW
+"xts",          // NLM_CIPHER_MODE_XTS
+"undefined", // > max
+};
+static char str_auth_alg[NLM_HASH_MAX + 1][NLM_CRYPTO_MAX_STR_LEN] = {
+"bypass",       // NLM_AUTH_BYPASS
+"md5",          // NLM_AUTH_MD5
+"sha",          // NLM_AUTH_SHA
+"invalid",       // NLM_AUTH_UNDEFINED
+"aes 128",      // NLM_AUTH_AES128
+"aes 192",      // NLM_AUTH_AES192
+"aes 256",      // NLM_AUTH_AES256
+"kasumi f9",    // NLM_AUTH_KASUMI_F9
+"snow3g f9",    // NLM_AUTH_SNOW3G_F9
+"camellia 128", // NLM_AUTH_CAMELLIA128
+"camellia 192", // NLM_AUTH_CAMELLIA192
+"camellia 256", // NLM_AUTH_CAMELLIA256
+"ghash",        // NLM_AUTH_GHASH
+"undefined",    // > max
+};
+static char str_auth_mode[NLM_HASH_MODE_MAX + 1][NLM_CRYPTO_MAX_STR_LEN] = {
+"sha1",         // NLM_AUTH_MODE_SHA1
+"sha 224",      // NLM_AUTH_MODE_SHA224
+"sha 256",      // NLM_AUTH_MODE_SHA256
+"sha 384",      // NLM_AUTH_MODE_SHA384
+"sha 512",      // NLM_AUTH_MODE_SHA512
+"cmac",         // NLM_AUTH_MODE_CMAC
+"xcbc",         // NLM_AUTH_MODE_XCBC
+"cbc mac",      // NLM_AUTH_MODE_CBC_MAC
+"undefined", // > max
+};
+#ifdef NLM_CRYPTO_DEBUG
+void hex_dump(char * description,unsigned char *in, int num)
+{
+        int i, j;
+        char buf[50];
+        char *buf_ptr;
+        printk("%s\n",description);
+
+        for (i = 0; i < num; i+= 16) {
+                if (i + 16 > num) {
+                        buf_ptr = buf;
+                        sprintf(buf_ptr, "    ");
+                        buf_ptr += 4;
+                        for (j = 0 ; j < num - i ; j++) {
+                                sprintf(buf_ptr, "%02x ", in[j + i]);
+                                buf_ptr += 3;
+                        }
+                        *buf_ptr = '\0';
+                        printk("%s\n",buf);
+                        break;
+                }
+                printk("    %02x %02x %02x %02x %02x %02x %02x %02x %02x %02x %02x %02x %02x %02x %02x %02x\n",
+                        in[i + 0 ],
+                        in[i + 1 ],
+                        in[i + 2 ],
+                        in[i + 3 ],
+                        in[i + 4 ],
+                        in[i + 5 ],
+                        in[i + 6 ],
+                        in[i + 7 ],
+                        in[i + 8 ],
+                        in[i + 9 ],
+                        in[i + 10],
+                        in[i + 11],
+                        in[i + 12],
+                        in[i + 13],
+                        in[i + 14],
+                        in[i + 15]
+              );
+        }
+}
+
+char *nlm_crypto_cipher_alg_get_name(unsigned int cipher_alg)
+{
+	if (cipher_alg >= NLM_CIPHER_MAX)
+		return str_cipher_alg[NLM_CIPHER_MAX];
+	else 
+		return str_cipher_alg[cipher_alg];
+}
+
+char *nlm_crypto_cipher_mode_get_name(unsigned int cipher_mode)
+{
+	if (cipher_mode >= NLM_CIPHER_MODE_MAX)
+		return str_cipher_mode[NLM_CIPHER_MODE_MAX];
+	else 
+		return str_cipher_mode[cipher_mode];
+}
+
+char *nlm_crypto_auth_alg_get_name(unsigned int auth_alg)
+{
+	if (auth_alg >= NLM_HASH_MAX)
+		return str_auth_alg[NLM_HASH_MAX];
+	else 
+		return str_auth_alg[auth_alg];
+}
+
+char *nlm_crypto_auth_mode_get_name(unsigned int auth_mode)
+{
+	if (auth_mode >= NLM_HASH_MODE_MAX)
+		return str_auth_mode[NLM_HASH_MODE_MAX];
+	else 
+		return str_auth_mode[auth_mode];
+}
+
+void print_crypto_msg_desc(uint64_t entry1, uint64_t entry2, uint64_t entry3)
+{
+
+        printk("Security Message Descriptor 0: 0x%lx\n", entry1);
+        printk("Security Message Descriptor 1: 0x%lx\n", entry2);
+        printk("Security Message Descriptor 2: 0x%lx\n", entry3);
+
+
+        printk("Free descriptor response destination : 0x%llx  \n", xtract_bits(entry1, 48, 16));
+        printk("Use designer freeback : 0x%llx  \n", xtract_bits(entry1, 45, 1));
+        printk("cipher key length (in dwords) : 0x%llx  \n", xtract_bits(entry1, 40, 5));
+        printf("Control desc cacheline addr : 0x%llx  \n", xtract_bits(entry1, 0, 34));
+        if (xtract_bits(entry1, 45, 1)) {
+                printf("Designer freeback length (actual len - 1): 0x%llx  \n", xtract_bits(entry1, 46, 2));
+        }
+
+
+        printf("Arc4 load state : 0x%llx  \n", xtract_bits(entry2, 63, 1));
+        printf("Hash key length (in dwords) : 0x%llx  \n", xtract_bits(entry2, 56, 5));
+        printf("Pkt desc length (in multiple of 16 bytes - 1): 0x%llx  \n", xtract_bits(entry2, 43, 12));
+        printf("Pkt desc cacheline addr : 0x%llx  \n", xtract_bits(entry2, 0, 34));
+
+        printf("Software Scratch Pad : 0x%llx  \n", xtract_bits(entry3, 0, 34));
+}
+void print_cntl_instr(uint64_t cntl_desc)
+{
+	unsigned int tmp;
+	char *x;
+	char s[NLM_CRYPTO_MAX_STR_LEN];
+
+	printf("control description: 0x%016llx\n", (unsigned long long)cntl_desc);
+	printf("HMac = 0x%llx  \n", xtract_bits(cntl_desc, 61, 1));
+//	printk("Pad Hash = 0x%llx  \n", xtract_bits(cntl_desc, 62, 1));
+	/* Check cipher, hash type and mode b4 printing */
+	tmp = xtract_bits(cntl_desc, 52, 8);
+	x = nlm_crypto_auth_alg_get_name(tmp);
+	strncpy(s, x, NLM_CRYPTO_MAX_STR_LEN);
+	printf("Hash Type = 0x%llx(%s)  \n", xtract_bits(cntl_desc, 52, 8), s);
+	tmp = xtract_bits(cntl_desc, 43, 8);
+	x = nlm_crypto_auth_mode_get_name(tmp);
+	strncpy(s, x, NLM_CRYPTO_MAX_STR_LEN);
+	printf("Hash Mode = 0x%llx(%s)  \n", xtract_bits(cntl_desc, 43, 8), s);
+	tmp = xtract_bits(cntl_desc, 34, 8);
+	x = nlm_crypto_cipher_alg_get_name(tmp);
+	strncpy(s, x, NLM_CRYPTO_MAX_STR_LEN);
+	printf("Cipher Type = 0x%llx(%s)  \n", xtract_bits(cntl_desc, 34, 8), s);
+	tmp = xtract_bits(cntl_desc, 25, 8);
+	x = nlm_crypto_cipher_mode_get_name(tmp);
+	strncpy(s, x, NLM_CRYPTO_MAX_STR_LEN);
+	printf("Cipher Mode = 0x%llx(%s)  \n", xtract_bits(cntl_desc, 25, 8), s);
+
+
+	if (xtract_bits(cntl_desc, 34, 8) == NLM_CIPHER_ARC4) {
+		printf("Arc4 cipher key byte count= 0x%llx  \n", xtract_bits(cntl_desc, 18, 5));
+		printf("Arc4 key init = 0x%llx  \n", xtract_bits(cntl_desc, 17, 1));
+	}
+
+}
+struct pkt_desc_src_dst {
+	uint64_t pkt_desc4;
+	uint64_t pkt_desc5;
+};
+
+struct designer_desc{
+	uint64_t desc0;
+	uint64_t desc1;
+	uint64_t desc2;
+	uint64_t desc3;
+};
+
+
+void print_pkt_desc(struct nlm_crypto_pkt_param  *pkt_desc, int index)
+{
+	printf("Packet desc address = %p\n",pkt_desc);
+	printf("Packet Descriptor 0: 0x%016llx\n", (unsigned long long)pkt_desc->desc0);
+	printf("Packet Descriptor 1: 0x%016llx\n", (unsigned long long)pkt_desc->desc1);
+	printf("Packet Descriptor 2: 0x%016llx\n", (unsigned long long)pkt_desc->desc2);
+	printf("Packet Descriptor 3: 0x%016llx\n", (unsigned long long)pkt_desc->desc3);
+
+	printf("\nPacket Descriptor 0\n");
+	printf("TLS protocol = 0x%llx  \n", xtract_bits(pkt_desc->desc0, 63, 1));
+	printf("Hash source(0-plain, 1-encrypted text) = 0x%llx  \n", xtract_bits(pkt_desc->desc0, 62, 1));
+	printf("Hash output l3 alloc = 0x%llx  \n", xtract_bits(pkt_desc->desc0, 60, 1));
+	printf("Encrypt(1)/Decrypt(0)= 0x%llx  \n", xtract_bits(pkt_desc->desc0, 59, 1));
+	printf("IV length = 0x%llx  \n", xtract_bits(pkt_desc->desc0, 41, 16));
+	printf("Hash Dest addr = 0x%llx \n", xtract_bits(pkt_desc->desc0, 0, 39));
+
+	printf("\nPacket Descriptor 1\n");
+	printf("Cipher length = 0x%llx \n", xtract_bits(pkt_desc->desc1, 32, 32));
+	printf("Hash length = 0x%llx  \n", xtract_bits(pkt_desc->desc1, 0, 32));
+	printf("IV Offset = 0x%llx  \n", xtract_bits(pkt_desc->desc2, 45, 17));
+
+	printf("\nPacket Descriptor 2\n");
+	printf("Cipher bit count = 0x%llx  \n", xtract_bits(pkt_desc->desc2, 42, 3));
+	printf("Cipher Offset = 0x%llx  \n", xtract_bits(pkt_desc->desc2, 22, 16));
+	printf("Hash bit count = 0x%llx  \n", xtract_bits(pkt_desc->desc2, 19, 3));
+	printf("Hash clobber = 0x%llx  \n", xtract_bits(pkt_desc->desc2, 18, 1));
+	printf("Hash Offset = 0x%llx  \n", xtract_bits(pkt_desc->desc2, 0, 16));
+
+
+	printf("\nPacket Descriptor 3\n");
+	printf("designer fb dest id = 0x%llx  \n", xtract_bits(pkt_desc->desc3, 48, 16));
+	printf("tag length = 0x%llx  \n", xtract_bits(pkt_desc->desc3, 11, 16));
+
+	printf("arc4 sbox l3 alloc = 0x%llx  \n", xtract_bits(pkt_desc->desc3, 8, 1));
+	printf("arc4 save box = 0x%llx  \n", xtract_bits(pkt_desc->desc3, 6, 1));
+	printf("hmac ext pad key = 0x%llx  \n", xtract_bits(pkt_desc->desc3, 5, 1));
+
+        int i;
+	unsigned long  phys;
+	void * virt;
+	
+
+        for (i=0; i < index; i++) {
+                printf("Packet Descriptor frag src %d: 0x%016llx\n", i, (unsigned long long)pkt_desc->segment[i][0]);
+                printf("Packet Descriptor frag dst %d: 0x%016llx\n", i, (unsigned long long)pkt_desc->segment[i][1]);
+		phys = xtract_bits(pkt_desc->segment[i][0], 0,40);
+		virt = phys_to_virt(phys);
+		hex_dump("src \n",virt, 30);
+		printk("virtual is %p and phys is %lx\n",virt,phys);
+
+
+                printf("frag src length = 0x%llx  \n", xtract_bits(pkt_desc->segment[i][0], 48, 16));
+                printf("frag src = 0x%llx \n", xtract_bits(pkt_desc->segment[i][0], 0, 40));
+
+                printf("frag dest length = 0x%llx \n", xtract_bits(pkt_desc->segment[i][1], 48, 16));
+                printf("cipher output l3 alloc = 0x%llx \n", xtract_bits(pkt_desc->segment[i][1], 46, 1));
+                printf("cipher output write clobber = 0x%llx \n", xtract_bits(pkt_desc->segment[i][1], 41, 1));
+                printf("frag dest = 0x%llx \n", xtract_bits(pkt_desc->segment[i][1], 0, 40));
+        }
+}
+#endif
+    static void
+reset_crypto_stats(void)
+{
+    int i, j;
+    for (i = 0; i < MAX_CPU; i++) {
+	for (j = 0; j < ENC_MAX_STAT; j++) {
+		crypto_stat[i].enc[j] = 0;
+		crypto_stat[i].enc_tbytes[j] = 0;
+	}
+	for (j = 0; j < AUTH_MAX_STAT; j++) {
+		crypto_stat[i].auth[j] = 0;
+		crypto_stat[i].auth_tbytes[j] = 0;
+	}
+		
+    }
+
+}
+
+int
+crypto_get_fb_vc(void)
+{
+    int cpu;
+    extern int ipsec_async_vc;
+
+
+    cpu = hard_smp_processor_id();	//processor_id();
+    cpu = cpu * 4 + ipsec_async_vc;
+
+    return cpu;
+}
+
+static const struct file_operations xlp_sae_fops = {
+    .owner = THIS_MODULE,
+    .open = xlp_sae_open,
+    .release = xlp_sae_release,
+};
+
+/* Note that nobody ever sets xlp_sae_busy... */
+    static int
+xlp_sae_open(struct inode *inode, struct file *file)
+{
+    TRACE_TEXT("(xlp_sae_open");
+    return 0;
+}
+
+    static int
+xlp_sae_release(struct inode *inode, struct file *file)
+{
+    TRACE_TEXT("(xlp_sae_release");
+
+    return 0;
+}
+
+    static void
+nlm_xlp_sae_msgring_handler(uint32_t vc, uint32_t src_id,
+	uint32_t size, uint32_t code,
+	uint64_t msg0, uint64_t msg1,
+	uint64_t msg2, uint64_t msg3, void *data)
+{
+	struct nlm_async_crypto *async = (struct nlm_async_crypto *)(msg0);
+	if(async)	
+		async->callback(async, msg1);
+}
+
+static int
+nlm_crypto_read_stats_proc(char *page, char **start, off_t off, int count,
+                       int *eof, void *data)
+{
+        int len = 0;
+	int i,j;
+	uint64_t  cnt;
+	off_t begin = 0;
+	uint64_t enc_tp[ENC_MAX_STAT];
+	uint64_t auth_tp[AUTH_MAX_STAT];
+	uint64_t enc_tb[ENC_MAX_STAT];
+	uint64_t auth_tb[AUTH_MAX_STAT];
+
+	len += sprintf(page + len, "\t\tPkt\t\tTotal Bytes\n");
+
+	if (!proc_pos_check(&begin, &len, off, count))
+		goto out;
+	for(j =0 ;j <= ENC_MAX_STAT ; j++) {
+		enc_tp[j] = 0;	
+		enc_tb[j] = 0;
+		for(i = 0; i < MAX_CPU; i++)  {
+			enc_tp[j] = enc_tp[j] + crypto_stat[i].enc[j];
+			enc_tb[j] = enc_tb[j] + crypto_stat[i].enc_tbytes[j];
+		}
+			
+	}
+
+	len += sprintf(page + len,"DES-CBC\t\t%lld\t\t%lld\nTDES-CBC\t%lld\t\t%lld\nAES128-CBC\t%lld\t\t%lld\n",
+			enc_tp[DES_CBC_STAT],enc_tb[DES_CBC_STAT],enc_tp[TDES_CBC_STAT],enc_tb[TDES_CBC_STAT],
+			enc_tp[AES128_CBC_STAT],enc_tb[AES128_CBC_STAT]);
+	if (!proc_pos_check(&begin, &len, off, count))
+		goto out;
+
+	len += sprintf(page + len,"AES192-CBC\t%lld\t\t%lld\nAES256-CBC\t%lld\t\t%lld\n",
+		enc_tp[AES192_CBC_STAT],enc_tb[AES192_CBC_STAT],enc_tp[AES256_CBC_STAT],enc_tb[AES256_CBC_STAT]);
+	if (!proc_pos_check(&begin, &len, off, count))
+		goto out;
+
+	for(j =0 ;j < AUTH_MAX_STAT; j++) {
+		auth_tp[j] = 0;
+		auth_tb[j] = 0;
+		for(i = 0; i < MAX_CPU; i++) { 
+			auth_tp[j] +=  crypto_stat[i].auth[j];
+			auth_tb[j] += crypto_stat[i].auth_tbytes[j];
+		}
+	}
+
+	len  += sprintf(page + len,"MD5\t\t%lld\t\t%lld\nH-SHA1\t\t%lld\t\t%lld\nH-SHA256\t%lld\t\t%lld\n",
+				    auth_tp[MD5_STAT],auth_tb[MD5_STAT],auth_tp[H_SHA1_STAT],auth_tb[H_SHA1_STAT],auth_tp[H_SHA256_STAT],auth_tb[H_SHA256_STAT]);
+	if (!proc_pos_check(&begin, &len, off, count))
+		 goto out;
+
+	len  += sprintf(page + len,"AES128-XCBC\t%lld\t\t%lld\nAES198-XCBC\t%lld\t\t%lld\nAES256-XCBC\t%lld\t\t%lld\n",
+		auth_tp[AES128_XCBC_STAT],auth_tb[AES128_XCBC_STAT],auth_tp[AES192_XCBC_STAT],auth_tb[AES192_XCBC_STAT],auth_tp[AES256_XCBC_STAT],auth_tb[AES256_XCBC_STAT]);
+
+	if (!proc_pos_check(&begin, &len, off, count))
+		 goto out;
+
+        *eof = 1;
+
+      out:
+        *start = page + (off - begin);
+        len -= (off - begin);
+        if (len > count)
+                len = count;
+        if (len < 0)
+                len = 0;
+
+        return len;
+
+}
+
+int
+nlm_crypto_init(void)
+{
+    int ret = 0;
+    struct proc_dir_entry *entry = NULL;
+
+    entry = create_proc_read_entry("crypto_stats", 0, nlm_root_proc,
+		    nlm_crypto_read_stats_proc,
+		    0);
+
+    if(entry == NULL) {
+	    printk("%s:%d failed creating proc stats entry.\n",
+			    __FUNCTION__, __LINE__);
+	    ret = -EINVAL;
+    }
+
+   nlm_hal_get_crypto_vc_nums(&crypto_vc_base, &crypto_vc_limit); 
+
+    if (register_xlp_msgring_handler
+		    (XLP_MSG_HANDLE_CRYPTO, nlm_xlp_sae_msgring_handler, NULL)) {
+	    panic("can't register msgring handler for TX_STN_GMAC0");
+    }
+    reset_crypto_stats();
+
+
+
+    return ret;
+}
+
+    static int __init
+xlp_sae_init(void)
+{
+    extern int ipsec_sync_vc;
+    extern int ipsec_async_vc;
+    printk(KERN_ERR ",\n XLP SAE/Crypto Initialization \n");
+
+    xlp_sae_major = register_chrdev(0, "NLM_XLP_SAE", &xlp_sae_fops);
+    if (xlp_sae_major < 0) {
+	printk(KERN_ERR "XLP_SAE - cannot register device\n");
+	return xlp_sae_major;
+    }
+    //  printk (KERN_ERR ",XLP SAE MAJOR %d\n", xlp_sae_major);
+    if ( (ipsec_async_vc == -1) && (ipsec_sync_vc == -1) )  {
+	printk(KERN_ERR "XLP_SAE - cannot be loaeded,Please set ipsec-async-vc and ipsec-sync-vc in the dts file\n");
+    	return -1;
+    }
+    nlm_crypto_init();
+    if(ipsec_async_vc != -1){
+    	xlp_crypt_alg_init();
+    	xlp_aead_alg_init();
+    }else{
+	printk(KERN_ERR "Cannot perform aead/enc operation, Please set ipsec-async-vc in the dts file\n");
+    }
+    if(ipsec_sync_vc != -1){
+    	xlp_auth_alg_init();
+    }else{
+	printk(KERN_ERR "Cannot perform auth operation, Please exclude ipsec_sync_vc from the node-vc-mask in dts\n");	
+	return 0;
+    }
+
+    return 0;
+}
+
+    static void __exit
+xlp_sae_cleanup(void)
+{
+    xlp_crypt_alg_fini();
+    xlp_auth_alg_fini();
+    xlp_aead_alg_fini();
+    unregister_chrdev(xlp_sae_major, "NLM_XLP_SAE");
+}
+
+module_init(xlp_sae_init);
+module_exit(xlp_sae_cleanup);
+MODULE_DESCRIPTION("XLP Hardware crypto support for AES/DES/3DES/SHA/MD5 .");
+MODULE_LICENSE("GPL");
+MODULE_VERSION("0.1");
+MODULE_AUTHOR("Alok Agrawat");
diff --git a/drivers/crypto/nlm-sae/nlm_enc.c b/drivers/crypto/nlm-sae/nlm_enc.c
new file mode 100644
index 0000000..c7e0f9f
--- /dev/null
+++ b/drivers/crypto/nlm-sae/nlm_enc.c
@@ -0,0 +1,462 @@
+/***********************************************************************
+Copyright 2003-2010 Netlogic Microsystems (“Netlogic”). All rights
+reserved.
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in
+the documentation and/or other materials provided with the
+distribution.
+THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NETLOGIC OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+THE POSSIBILITY OF SUCH DAMAGE.
+*****************************#NETL_2#********************************/
+#include <crypto/scatterwalk.h>
+/*#include <crypto/algapi.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+*/
+#include <linux/crypto.h>
+#include <crypto/aes.h>
+#include <crypto/des.h>
+#include <crypto/ctr.h>
+#include <asm/netlogic/crypto/nlmcrypto.h>
+#include <nlm_hal_fmn.h>
+#include <asm/netlogic/hal/nlm_hal.h>
+
+/*#include <linux/pci.h>
+#include <linux/pci_ids.h>
+#include <asm/io.h>
+*/
+#include "nlm_async.h"
+#undef NLM_CRYPTO_DEBUG
+
+
+#define XLP_CRYPT_PRIORITY	300
+
+
+struct nlm_enc_ctx {
+	struct nlm_crypto_pkt_ctrl ctrl; 
+	uint16_t stat;
+};
+/* mem utilisation of CTX_SIZE */
+#define MAX_FRAGS               18
+#define CTRL_DESC_SIZE          (sizeof(struct nlm_enc_ctx) + 64)
+#define DES3_CTRL_DESC_SIZE     (2*CTRL_DESC_SIZE + 2*64)
+
+
+/* mem utilisation of req mem */
+
+#define PACKET_DESC_SIZE        (64 + sizeof(struct nlm_crypto_pkt_param) + MAX_FRAGS*(2*64) + 64 + sizeof(struct nlm_async_crypto) + 64)
+#define NLM_CRYPTO_PKT_PARAM_OFFSET(addr)       (((unsigned long)addr + 64) & ~0x3fULL)
+#define NLM_ASYNC_PTR_PARAM_OFFSET(addr)        (((unsigned long)(addr + 64 + sizeof(struct nlm_crypto_pkt_param) + MAX_FRAGS*(2*64)) + 64) & ~0x3fULL)
+#define NLM_IV_OFFSET(addr)			((unsigned long)addr + (PACKET_DESC_SIZE - 64))
+
+
+
+extern uint32_t nlm_hal_send_msg3(uint32_t dst, uint32_t code, uint64_t data0, uint64_t data1, uint64_t data2);
+extern __inline__ uint32_t nlm_hal_recv_msg2(uint32_t dst, uint32_t *src, uint32_t *size, uint32_t *code, uint64_t *data0, uint64_t *data1);
+#ifdef NLM_CRYPTO_DEBUG
+extern void print_cntl_instr(uint64_t cntl_desc);
+extern void print_crypto_msg_desc(uint64_t entry1, uint64_t entry2, uint64_t entry3);
+extern void print_pkt_desc(struct nlm_crypto_pkt_param * pkt_param, int index);
+#endif
+
+extern int cipher_mode_iv_len[NLM_CIPHER_MAX][NLM_CIPHER_MODE_MAX];
+extern struct nlm_crypto_stat crypto_stat[MAX_CPU];
+
+
+
+//extern void print_cntl_instr(uint64_t cntl_desc);
+static void enc_session_cleanup(struct crypto_tfm *tfm)
+{
+}
+
+static int enc_cra_init(struct crypto_tfm *tfm)
+{ 
+	tfm->crt_ablkcipher.reqsize = PACKET_DESC_SIZE; //reqsize of 512 bytes for packet desc
+	return 0;
+}
+
+static struct nlm_enc_ctx * nlm_crypto_ablkcipher_ctx(struct crypto_ablkcipher *tfm)
+{
+	return (struct  nlm_enc_ctx *)(((unsigned long)((uint8_t *)crypto_ablkcipher_ctx(tfm) + 63 )) & ~(0x3f));
+} 
+
+
+
+static int
+xlp_setkey(struct crypto_ablkcipher *tfm, const u8 * in_key, unsigned int len, uint32_t cipher_alg, uint32_t cipher_mode,uint16_t stat)
+{
+	struct nlm_enc_ctx * nlm_ctx = nlm_crypto_ablkcipher_ctx(tfm); 
+	
+	nlm_crypto_fill_pkt_ctrl(&nlm_ctx->ctrl,0,0,0,cipher_alg,cipher_mode,0,(unsigned char*)in_key,len,0,0);
+	crypto_ablkcipher_crt(tfm)->ivsize = cipher_mode_iv_len[cipher_alg][ cipher_mode];
+	nlm_ctx->stat = stat;
+
+	return 0;
+
+}
+static int
+xlp_setkey_des3(struct crypto_ablkcipher *tfm, const u8 * in_key, unsigned int len, uint32_t cipher_alg, uint32_t cipher_mode, uint16_t stat)
+{
+	struct nlm_enc_ctx * nlm_ctx = (struct  nlm_enc_ctx *)(( unsigned long )(( uint8_t *)nlm_crypto_ablkcipher_ctx(tfm) + CTRL_DESC_SIZE + 63) & ~(0x3f)); 
+	uint64_t key[3] ;
+	
+	memcpy(key,&in_key[16],8);
+        memcpy(&key[1],&in_key[8],8);
+        memcpy(&key[2],&in_key[0],8);
+
+
+	nlm_crypto_fill_pkt_ctrl(&nlm_ctx->ctrl,0,0,0,cipher_alg,cipher_mode,0,(unsigned char*)&key[0],len,0,0);
+	crypto_ablkcipher_crt(tfm)->ivsize = cipher_mode_iv_len[cipher_alg][ cipher_mode];
+	nlm_ctx->stat = stat;
+
+	return 0;
+
+}
+
+
+static int
+xlp_des3_setkey(struct crypto_ablkcipher *tfm, const u8 * in_key, unsigned int len)
+{
+	uint32_t cipher_alg;
+	u32 flags = 0;
+
+	switch (len) {
+	case DES3_EDE_KEY_SIZE:
+	        cipher_alg = NLM_CIPHER_3DES;
+		break;
+	default:
+		printk(KERN_WARNING "[%s]: Cannot handle keylen = %d\n",
+		       __FUNCTION__, len);
+		flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
+		crypto_ablkcipher_set_flags(tfm,flags);
+		return -EINVAL;
+	}
+	xlp_setkey(tfm, in_key, len, cipher_alg, NLM_CIPHER_MODE_CBC,TDES_CBC_STAT);
+	return xlp_setkey_des3(tfm, in_key, len, cipher_alg, NLM_CIPHER_MODE_CBC, TDES_CBC_STAT);
+}
+
+static int
+xlp_des_setkey(struct crypto_ablkcipher *tfm, const u8 * in_key, unsigned int len)
+{
+	uint32_t cipher_alg;
+	u32 flags = 0;
+
+	switch (len) {
+	case DES_KEY_SIZE:
+	        cipher_alg = NLM_CIPHER_DES;
+		break;
+	default:
+		printk(KERN_WARNING "[%s]: Cannot handle keylen = %d\n",
+		       __FUNCTION__, len);
+		flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
+		crypto_ablkcipher_set_flags(tfm, flags);
+		return -EINVAL;
+	}
+	return xlp_setkey(tfm, in_key, len, cipher_alg,NLM_CIPHER_MODE_CBC,DES_CBC_STAT);
+}
+
+static int
+xlp_aes_setkey(struct crypto_ablkcipher *tfm, const u8 *in_key, 
+		unsigned int len, uint32_t mode)
+{
+	uint32_t cipher_alg;
+	uint16_t stat;
+	u32 flags = 0;
+
+	switch (len) {
+	case 16:
+	        cipher_alg = NLM_CIPHER_AES128;
+		stat = AES128_CBC_STAT;
+		break;
+	case 24:
+		cipher_alg = NLM_CIPHER_AES192;
+		stat = AES192_CBC_STAT;
+		break;
+	case 32:
+		cipher_alg = NLM_CIPHER_AES256;
+		stat = AES256_CBC_STAT;
+		break;
+	default:
+		printk(KERN_WARNING "[%s]: Cannot handle keylen = %d\n",
+		       __FUNCTION__, len);
+		flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;
+		crypto_ablkcipher_set_flags(tfm,flags);
+		return -EINVAL;
+	}
+	return xlp_setkey(tfm, in_key, len, cipher_alg, mode, stat);
+}
+
+static int xlp_cbc_aes_setkey(struct crypto_ablkcipher *tfm, const u8 *key,
+					unsigned int keylen)
+{
+	return xlp_aes_setkey(tfm,key,keylen,NLM_CIPHER_MODE_CBC);
+}
+
+void enc_request_callback(struct nlm_async_crypto *async, uint64_t msg1 )
+{
+	struct crypto_async_request * base = (struct crypto_async_request *)async->args; 
+	int err =0;
+	int cpu = hard_smp_processor_id();
+	int stat = async->stat;
+
+	if (msg1 & 0x7ff80) {
+		printk("\n Error: entry1 is %llx",msg1);
+		err = -EIO;
+		base->complete(base, err);
+		return;
+	}
+	crypto_stat[cpu].enc[stat]++;
+	crypto_stat[cpu].enc_tbytes[stat]+= async->bytes;
+	
+	base->complete(base, err);
+}
+static int
+xlp_crypt(struct ablkcipher_request *req, unsigned int enc, int iv_size, struct nlm_crypto_pkt_ctrl *ctrl, uint16_t stat)
+{
+	int seg = 0;
+	int i;
+	uint64_t msg0, msg1;
+	struct scatterlist *sg;
+	struct scatter_walk walk;
+	uint8_t *virt;
+	int len;
+	int pktdescsize = 0;
+	
+	unsigned int cipher_len = req->nbytes;
+	struct nlm_crypto_pkt_param * pkt_param = (struct nlm_crypto_pkt_param *) NLM_CRYPTO_PKT_PARAM_OFFSET(ablkcipher_request_ctx(req));
+	struct nlm_async_crypto * async =  (struct nlm_async_crypto *)NLM_ASYNC_PTR_PARAM_OFFSET(ablkcipher_request_ctx(req));;
+	int fb_vc;
+
+	nlm_crypto_fill_cipher_pkt_param(ctrl, pkt_param, enc,0,iv_size,iv_size ,req->nbytes); 
+
+	nlm_crypto_fill_src_seg(pkt_param,seg,(unsigned char *)req->info,iv_size);
+	nlm_crypto_fill_dst_seg(pkt_param,seg,(unsigned char *)req->info,iv_size);
+	seg++;
+
+	if ( req->src == req->dst) {
+		for (sg = req->src; cipher_len > 0; sg = scatterwalk_sg_next(sg)) {
+			len = min(cipher_len, sg->length);
+			scatterwalk_start(&walk, sg);
+			virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
+			nlm_crypto_fill_src_seg(pkt_param,seg,virt,len);
+			seg = nlm_crypto_fill_dst_seg(pkt_param,seg,virt,len);
+			cipher_len -= len;
+		}
+	}
+	else {
+		int nr_src_frags = 0;
+		int nr_dst_frags = 0;
+		int index = 0;
+		for (sg = req->src,index = seg;cipher_len > 0; sg = scatterwalk_sg_next(sg)) {
+			len = min(cipher_len, sg->length);
+			scatterwalk_start(&walk, sg);
+			virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
+			index = nlm_crypto_fill_src_seg(pkt_param,index,virt,len);
+			cipher_len -= len;
+		}
+		nr_src_frags = index;
+		cipher_len = req->nbytes;
+		for (sg = req->dst, index = seg ;cipher_len > 0; sg = scatterwalk_sg_next(sg)) {
+			len = min(cipher_len, sg->length);
+			scatterwalk_start(&walk, sg);
+			virt = page_address(scatterwalk_page(&walk)) + offset_in_page(walk.offset);
+			index = nlm_crypto_fill_dst_seg(pkt_param,index,virt,len);
+			cipher_len -= len;
+		}
+		nr_dst_frags = index;
+
+		if (nr_src_frags > nr_dst_frags) {
+			for (i = 0; i < nr_src_frags - nr_dst_frags; i++)
+				pkt_param->segment[seg + nr_dst_frags + i][1] = 0ULL;
+			seg = nr_src_frags;
+		}
+		else  { 
+			if (nr_src_frags < nr_dst_frags) {
+				for (i = 0; i < nr_dst_frags - nr_src_frags; i++)
+					pkt_param->segment[seg + nr_src_frags + i][0] = 0ULL;
+			}
+			seg = nr_dst_frags;
+		}
+		
+	}
+
+	pktdescsize = 32 + seg * 16;
+
+	preempt_disable();
+	fb_vc = crypto_get_fb_vc(); 
+
+	msg0 = nlm_crypto_form_pkt_fmn_entry0(fb_vc, 0, 0, ctrl->cipherkeylen,
+								virt_to_phys(ctrl));
+	msg1 = nlm_crypto_form_pkt_fmn_entry1(0,ctrl->hashkeylen, pktdescsize,
+				virt_to_phys(pkt_param));
+#ifdef NLM_CRYPTO_DEBUG
+	print_crypto_msg_desc(msg0,msg1,0xdeadbeef);
+	print_pkt_desc(pkt_param,seg);
+	print_cntl_instr(ctrl);
+#endif
+	async->callback = &enc_request_callback;
+	async->args = &req->base;
+	async->stat = stat; 
+	async->bytes = req->nbytes; 
+	mb();
+	while( nlm_hal_send_msg3(crypto_vc_base, 0 /*code */ , msg0, msg1, (unsigned long )async) != 0 );
+	preempt_enable();
+	return -EINPROGRESS;
+}
+
+static int
+xlp_3des_cbc_decrypt( struct ablkcipher_request *req)
+{
+      	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);
+	struct nlm_enc_ctx * nlm_ctx = (struct  nlm_enc_ctx *)(( unsigned long )(( uint8_t *)nlm_crypto_ablkcipher_ctx(tfm) + CTRL_DESC_SIZE + 63) & ~(0x3f)); 
+	return xlp_crypt(req, 0, DES3_EDE_BLOCK_SIZE,&nlm_ctx->ctrl,nlm_ctx->stat);
+}
+
+static int
+xlp_3des_cbc_encrypt( struct ablkcipher_request *req )
+{
+      	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);
+	struct nlm_enc_ctx *nlm_ctx = nlm_crypto_ablkcipher_ctx(tfm); 
+	return xlp_crypt(req, 1, DES3_EDE_BLOCK_SIZE,&nlm_ctx->ctrl,nlm_ctx->stat);
+}
+
+static int
+xlp_cbc_decrypt( struct ablkcipher_request *req )
+{
+	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);
+	int iv_size = crypto_ablkcipher_ivsize(tfm);
+	struct nlm_enc_ctx *nlm_ctx = nlm_crypto_ablkcipher_ctx(tfm); 
+	return xlp_crypt(req, 0, iv_size,&nlm_ctx->ctrl,nlm_ctx->stat);
+}
+
+static int
+xlp_cbc_encrypt( struct ablkcipher_request *req )
+{
+	struct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);
+	int iv_size = crypto_ablkcipher_ivsize(tfm);
+	struct nlm_enc_ctx *nlm_ctx = nlm_crypto_ablkcipher_ctx(tfm); 
+	return xlp_crypt(req, 1, iv_size,&nlm_ctx->ctrl, nlm_ctx->stat);
+}
+
+static struct crypto_alg xlp_cbc_aes_alg = {
+	.cra_name = "cbc(aes)",
+	.cra_driver_name = "cbc-aes-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER|CRYPTO_ALG_ASYNC,
+	.cra_blocksize = AES_BLOCK_SIZE,
+	.cra_ctxsize = CTRL_DESC_SIZE,
+	.cra_alignmask = 15,
+	.cra_type = &crypto_ablkcipher_type,
+	.cra_module = THIS_MODULE,
+	.cra_init = enc_cra_init,
+	.cra_exit = enc_session_cleanup,
+	.cra_list = LIST_HEAD_INIT(xlp_cbc_aes_alg.cra_list),
+	.cra_u = {
+		.ablkcipher = {
+			.min_keysize = AES_MIN_KEY_SIZE,
+			.max_keysize = AES_MAX_KEY_SIZE,
+			.setkey = xlp_cbc_aes_setkey,
+			.encrypt = xlp_cbc_encrypt,
+			.decrypt = xlp_cbc_decrypt,
+			.ivsize = AES_BLOCK_SIZE,
+		}
+	}
+};
+
+static struct crypto_alg xlp_cbc_des_alg = {
+	.cra_name = "cbc(des)",
+	.cra_driver_name = "cbc-des-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER | CRYPTO_ALG_ASYNC,
+	.cra_blocksize = DES_BLOCK_SIZE,
+	.cra_ctxsize = CTRL_DESC_SIZE,
+	.cra_alignmask = 15,
+	.cra_type = &crypto_ablkcipher_type,
+	.cra_module = THIS_MODULE,
+	.cra_init = enc_cra_init,
+	.cra_exit = enc_session_cleanup,
+	.cra_list = LIST_HEAD_INIT(xlp_cbc_des_alg.cra_list),
+	.cra_u = {
+		.ablkcipher = {
+			.min_keysize = DES_KEY_SIZE,
+			.max_keysize = DES_KEY_SIZE,
+			.setkey = xlp_des_setkey,
+			.encrypt = xlp_cbc_encrypt,
+			.decrypt = xlp_cbc_decrypt,
+			.ivsize = DES_BLOCK_SIZE,
+		}
+	}
+};
+
+static struct crypto_alg xlp_cbc_des3_alg = {
+	.cra_name = "cbc(des3_ede)",
+	.cra_driver_name = "cbc-des3-ede-xlp",
+	.cra_priority = XLP_CRYPT_PRIORITY,
+	.cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER|CRYPTO_ALG_ASYNC,
+	.cra_blocksize = DES3_EDE_BLOCK_SIZE,
+	.cra_ctxsize = DES3_CTRL_DESC_SIZE,
+	.cra_alignmask = 15,
+	.cra_type = &crypto_ablkcipher_type,
+	.cra_module = THIS_MODULE,
+	.cra_init = enc_cra_init,
+	.cra_exit = enc_session_cleanup,
+	.cra_list = LIST_HEAD_INIT(xlp_cbc_des3_alg.cra_list),
+	.cra_u = {
+		.ablkcipher = {
+			.min_keysize = DES3_EDE_KEY_SIZE,
+			.max_keysize = DES3_EDE_KEY_SIZE,
+			.setkey = xlp_des3_setkey,
+			.encrypt = xlp_3des_cbc_encrypt,
+			.decrypt = xlp_3des_cbc_decrypt,
+			.ivsize = DES3_EDE_BLOCK_SIZE,
+		}
+	}
+};
+
+int xlp_crypt_alg_init(void)
+{
+	int ret = 0;
+	int no_of_alg_registered = 0;
+	ret = crypto_register_alg(&xlp_cbc_des3_alg);
+	if (ret) {
+		goto end;
+	}
+	no_of_alg_registered++;
+	ret = crypto_register_alg(&xlp_cbc_des_alg);
+	if (ret) {
+		goto end;
+	}
+	no_of_alg_registered++;
+	ret = crypto_register_alg(&xlp_cbc_aes_alg);
+	if (ret) {
+		goto end;
+	}
+	no_of_alg_registered++;
+	printk(KERN_NOTICE "Using XLP hardware for AES, DES, 3DES algorithms.\n");
+end:
+	return 0;
+}
+
+void
+xlp_crypt_alg_fini(void) {
+	crypto_unregister_alg(&xlp_cbc_des3_alg);
+	crypto_unregister_alg(&xlp_cbc_des_alg);
+	crypto_unregister_alg(&xlp_cbc_aes_alg);
+}
+
+EXPORT_SYMBOL(xlp_crypt_alg_init);
+EXPORT_SYMBOL(xlp_crypt_alg_fini);
diff --git a/drivers/crypto/nlm-sae/nlmcrypto_ifc.h b/drivers/crypto/nlm-sae/nlmcrypto_ifc.h
new file mode 100644
index 0000000..d0d89b8
--- /dev/null
+++ b/drivers/crypto/nlm-sae/nlmcrypto_ifc.h
@@ -0,0 +1,46 @@
+/*********************************************************************
+
+  Copyright 2003-2010 Netlogic Microsystem, Inc. ("Netlogic"). All rights
+  reserved.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+  1. Redistributions of source code must retain the above copyright
+  notice, this list of conditions and the following disclaimer.
+  2. Redistributions in binary form must reproduce the above copyright
+  notice, this list of conditions and the following disclaimer in
+  the documentation and/or other materials provided with the
+  distribution.
+
+  THIS SOFTWARE IS PROVIDED BY Netlogic Microsystems, Inc. ``AS IS'' AND
+  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL RMI OR CONTRIBUTORS BE LIABLE
+  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
+  THE POSSIBILITY OF SUCH DAMAGE.
+
+  *****************************#NLM_2#**********************************/
+#ifndef _NLM_CRYPTO_IFC_H
+#define _NLM_CRYPTO_IFC_H
+
+extern void *linuxu_shvaddr;
+extern unsigned long long linuxu_shoff ;
+
+static inline unsigned long long crypto_virt_to_phys(void *vaddr)
+{
+	return virt_to_phys(vaddr);
+}
+
+static inline void *crypto_phys_to_virt(unsigned long long paddr)
+{
+	return phys_to_virt(paddr);
+}
+
+#endif
-- 
1.7.0

