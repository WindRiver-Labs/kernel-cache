From 703aebf3248bc1b36420ef9bbed1389a4747a5b2 Mon Sep 17 00:00:00 2001
From: Sandeep Nair <sandeep_n@ti.com>
Date: Thu, 18 Jul 2013 08:57:56 -0400
Subject: [PATCH 232/257] crypto: keystone: Fix memory leak due to pkt drop

Without throttling in the Tx path SA was dropping packets if there is underrun in FDQ on Rx. Any such dropped pkts are a leak in the system.
This patch also exposes packets statistics through sysfs interface to debug this type of issue.

Signed-off-by: Sandeep Nair <sandeep_n@ti.com>
(cherry picked from commit 94317d629fb19fd3c83520b5cda014251bd3b900)
Signed-off-by: Zumeng Chen <zumeng.chen@windriver.com>
---
 drivers/crypto/keystone-sa.c | 372 +++++++++++++++++++++++++++++++++++--------
 1 file changed, 308 insertions(+), 64 deletions(-)

diff --git a/drivers/crypto/keystone-sa.c b/drivers/crypto/keystone-sa.c
index bba1025..3885137 100644
--- a/drivers/crypto/keystone-sa.c
+++ b/drivers/crypto/keystone-sa.c
@@ -16,8 +16,7 @@
 
 /* TODO:
  *  - Add support for all algorithms supported by SA
- *  - Add more driver statistics
- *  - Expose statistics to user space
+ *  - Add support for ABLKCIPHER & AHASH algorithms
  */
 
 #include <linux/clk.h>
@@ -121,9 +120,22 @@ struct sa_regs {
 
 /* Driver statistics */
 struct sa_drv_stats {
-	unsigned long	tx_dropped;
+	/* Number of data pkts dropped while submitting to CP_ACE */
+	atomic_t tx_dropped;
+	/* Number of tear-down pkts dropped while submitting to CP_ACE */
+	atomic_t sc_tear_dropped;
+	/* Number of crypto requests sent to CP_ACE */
+	atomic_t tx_pkts;
+	/* Number of crypto request completions received from CP_ACE */
+	atomic_t rx_pkts;
 };
 
+/*
+ * Minimum number of descriptors to be always
+ * available in the Rx free queue
+ */
+#define SA_MIN_RX_DESCS	4
+
 /* Crypto driver instance data */
 struct keystone_crypto_data {
 	struct platform_device	*pdev;
@@ -132,17 +144,36 @@ struct keystone_crypto_data {
 	struct dma_pool		*sc_pool;
 	struct sa_regs		*regs;
 	struct sa_dma_data	dma_data;
+
 	/* lock for SC-ID allocation */
 	spinlock_t		scid_lock;
+
+	/* Kobjects */
+	struct kobject		stats_kobj;
+
 	/* lock to prevent irq scheduling while dmaengine_submit() */
 	spinlock_t		irq_lock;
 	u16			sc_id_start;
 	u16			sc_id_end;
 	u16			sc_id;
+
 	/* Bitmap to keep track of Security context ID's */
 	unsigned long		ctx_bm[DIV_ROUND_UP(SA_MAX_NUM_CTX,
 					BITS_PER_LONG)];
+	/* Driver stats */
 	struct sa_drv_stats	stats;
+
+	/*
+	 * Number of pkts pending crypto processing completion
+	 * beyond which the driver will start dropping crypto
+	 * requests.
+	 */
+	int			tx_thresh;
+
+	/*
+	 * Number of pkts pending crypto processing completion
+	 */
+	atomic_t		pend_compl;
 };
 
 /* Packet structure used in Rx */
@@ -220,7 +251,7 @@ struct sa_ctx_info {
 };
 
 struct sa_tfm_ctx {
-	struct device *dev;
+	struct keystone_crypto_data *dev_data;
 	struct sa_ctx_info enc;
 	struct sa_ctx_info dec;
 	struct sa_ctx_info auth;
@@ -228,12 +259,13 @@ struct sa_tfm_ctx {
 
 /* Tx DMA callback param */
 struct sa_dma_req_ctx {
+	struct keystone_crypto_data *dev_data;
 	u32		cmdl[SA_MAX_CMDL_WORDS];
 	unsigned	map_idx;
 	struct sg_table sg_tbl;
-	struct device	*dev;
 	dma_cookie_t	cookie;
 	struct dma_chan *tx_chan;
+	bool		pkt;
 };
 
 /************************************************************/
@@ -1128,24 +1160,32 @@ static void sa_tx_dma_cb(void *data)
 				"dma completion failure, status == %d", status);
 	}
 
-	dma_unmap_sg(ctx->dev, &ctx->sg_tbl.sgl[ctx->map_idx],
-			ctx->sg_tbl.nents, DMA_TO_DEVICE);
+	dma_unmap_sg(&ctx->dev_data->pdev->dev,
+		&ctx->sg_tbl.sgl[ctx->map_idx],
+		ctx->sg_tbl.nents, DMA_TO_DEVICE);
 
 	if (likely(ctx->sg_tbl.sgl))
 		sg_free_table(&ctx->sg_tbl);
-	kfree(data);
+
+	if (likely(ctx->pkt)) {
+		atomic_inc(&ctx->dev_data->pend_compl);
+		atomic_inc(&ctx->dev_data->stats.tx_pkts);
+	}
+
+	kfree(ctx);
 }
 
 /* Rx completion callback */
 static void sa_desc_rx_complete(void *arg)
 {
-	struct sa_packet *rx = arg;
+	struct keystone_crypto_data *dev_data = NULL;
 	struct device *dev = keystone_dev;
+	struct sa_packet *rx = arg;
 	struct scatterlist *sg;
-	u32 *psdata;
-	u32 req_sub_type;
 	unsigned int alg_type;
 	unsigned int frags;
+	u32 req_sub_type;
+	u32 *psdata;
 
 	frags = 0;
 	sg = sg_next(&rx->sg[2]);
@@ -1162,13 +1202,15 @@ static void sa_desc_rx_complete(void *arg)
 	req_sub_type = psdata[0] >> SA_REQ_SUBTYPE_SHIFT;
 
 	if (likely(alg_type == CRYPTO_ALG_TYPE_AEAD)) {
+		int auth_words, auth_size, iv_size, enc_len, enc_offset, i;
 		struct aead_request *req;
 		struct crypto_aead *tfm;
-		int auth_words, auth_size, iv_size, enc_len, enc_offset, i;
 		int enc, err = 0;
 
 		req = (struct aead_request *)psdata[1];
 		tfm = crypto_aead_reqtfm(req);
+		dev_data =
+		((struct sa_tfm_ctx *)(crypto_tfm_ctx(&tfm->base)))->dev_data;
 		auth_size = crypto_aead_authsize(tfm);
 		iv_size = crypto_aead_ivsize(tfm);
 		enc_offset = req->assoclen + iv_size;
@@ -1228,7 +1270,7 @@ static void sa_desc_rx_complete(void *arg)
 			err = -EBADMSG;
 
 aead_err:
-		req->base.complete(&req->base, err);
+		aead_request_complete(req, err);
 	}
 
 	/* Free the Rx buffer */
@@ -1238,6 +1280,12 @@ aead_err:
 		sg = sg_next(sg);
 	}
 	kfree(rx);
+
+	/* update completion pending count */
+	if (dev_data) {
+		atomic_dec(&dev_data->pend_compl);
+		atomic_inc(&dev_data->stats.rx_pkts);
+	}
 	return;
 }
 
@@ -1472,6 +1520,26 @@ err_out:
 	return error;
 }
 
+/* Teardown DMA channels */
+static void sa_teardown_dma(struct keystone_crypto_data *dev_data)
+{
+	struct sa_dma_data *dma_data = &dev_data->dma_data;
+
+	if (dma_data->tx_chan) {
+		dmaengine_pause(dma_data->tx_chan);
+		dma_release_channel(dma_data->tx_chan);
+		dma_data->tx_chan = NULL;
+	}
+
+	if (dma_data->rx_chan) {
+		dmaengine_pause(dma_data->rx_chan);
+		dma_release_channel(dma_data->rx_chan);
+		dma_data->rx_chan = NULL;
+	}
+
+	return;
+}
+
 /******************************************************************************
  * Command Label Definitions and utility functions
  ******************************************************************************/
@@ -2122,7 +2190,6 @@ static int sa_tear_sc(struct sa_ctx_info *ctx,
 					1, DMA_TO_DEVICE);
 
 	if (dma_ctx->sg_tbl.nents != 1) {
-		pdata->stats.tx_dropped++;
 		dev_warn(keystone_dev, "failed to map null pkt\n");
 		ret = -ENXIO;
 		goto err;
@@ -2134,14 +2201,14 @@ static int sa_tear_sc(struct sa_ctx_info *ctx,
 					DMA_MEM_TO_DEV, DMA_HAS_EPIB);
 
 	if (IS_ERR_OR_NULL(desc)) {
-		pdata->stats.tx_dropped++;
 		dev_warn(keystone_dev, "failed to prep slave dma\n");
 		ret = -ENOBUFS;
 		goto err;
 	}
 
 	dma_ctx->tx_chan = pdata->dma_data.tx_chan;
-	dma_ctx->dev = keystone_dev;
+	dma_ctx->dev_data = pdata;
+	dma_ctx->pkt = false;
 	desc->callback = sa_tx_dma_cb;
 	desc->callback_param = dma_ctx;
 
@@ -2174,6 +2241,7 @@ static int sa_tear_sc(struct sa_ctx_info *ctx,
 	return 0;
 
 err:
+	atomic_inc(&pdata->stats.sc_tear_dropped);
 	sg_free_table(&dma_ctx->sg_tbl);
 	kfree(dma_ctx);
 	return ret;
@@ -2278,7 +2346,7 @@ static int sa_init_tfm(struct crypto_tfm *tfm)
 		sa_alg = container_of(alg, struct sa_alg_tmpl, alg.crypto);
 
 	memset(ctx, 0, sizeof(*ctx));
-	ctx->dev = keystone_dev;
+	ctx->dev_data = data;
 
 	if (sa_alg->type == CRYPTO_ALG_TYPE_AHASH) {
 		ret = sa_init_ctx_info(&ctx->auth, data);
@@ -2437,7 +2505,6 @@ static int sa_aead_setkey(struct crypto_aead *authenc,
 		goto badkey;
 
 	ctx->dec.cmdl_size = cmdl_len;
-
 	return 0;
 
 badkey:
@@ -2463,29 +2530,38 @@ static int sa_aead_perform(struct aead_request *req, u8 *iv, int enc)
 	struct device *dev = keystone_dev;
 	dma_cookie_t cookie;
 
+	struct keystone_crypto_data *pdata = dev_get_drvdata(dev);
 	unsigned ivsize = crypto_aead_ivsize(tfm);
 	u8 enc_offset = req->assoclen + ivsize;
-	u8 aad_len = 0;
-	u16 enc_len;
+	struct sa_dma_req_ctx *req_ctx = NULL;
+	struct dma_async_tx_descriptor *desc;
+	int assoc_sgents, src_sgents;
+	int psdata_offset, ret = 0;
+	unsigned long irq_flags;
 	u8 auth_offset = 0;
-	u16 auth_len;
 	u8 *auth_iv = NULL;
-	u8 *aad = NULL;
 	int sg_nents = 2; /* First 2 entries are for EPIB & PSDATA */
+	u8 *aad = NULL;
+	u8 aad_len = 0;
 	int sg_idx = 0;
-	int assoc_sgents, src_sgents;
-	struct dma_async_tx_descriptor *desc;
-	struct keystone_crypto_data *pdata = dev_get_drvdata(dev);
-	int psdata_offset, ret = 0;
+	u16 enc_len;
+	u16 auth_len;
 	u32 req_type;
-	unsigned long irq_flags;
+
 	gfp_t flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ?
 			GFP_KERNEL : GFP_ATOMIC;
-	struct sa_dma_req_ctx *req_ctx =
-			kmalloc(sizeof(struct sa_dma_req_ctx), flags);
 
-	if (unlikely(req_ctx == NULL))
-		return -ENOMEM;
+	if (unlikely(atomic_read(&pdata->pend_compl) >= pdata->tx_thresh)) {
+		ret = -EBUSY;
+		goto err;
+	}
+
+	req_ctx = kmalloc(sizeof(struct sa_dma_req_ctx), flags);
+
+	if (unlikely(req_ctx == NULL)) {
+		ret = -ENOMEM;
+		goto err;
+	}
 
 	enc_len = req->cryptlen;
 
@@ -2511,8 +2587,10 @@ static int sa_aead_perform(struct aead_request *req, u8 *iv, int enc)
 	if (likely(ivsize))
 		sg_nents += 1;
 
-	if (unlikely(sg_alloc_table(&req_ctx->sg_tbl, sg_nents, flags)))
-		return -ENOMEM;
+	if (unlikely(sg_alloc_table(&req_ctx->sg_tbl, sg_nents, flags))) {
+		ret = -ENOMEM;
+		goto err;
+	}
 
 	sg_set_buf(&req_ctx->sg_tbl.sgl[sg_idx++], sa_ctx->epib,
 			sizeof(sa_ctx->epib));
@@ -2563,9 +2641,8 @@ static int sa_aead_perform(struct aead_request *req, u8 *iv, int enc)
 					(sg_nents - req_ctx->map_idx),
 					DMA_TO_DEVICE);
 	if (unlikely(req_ctx->sg_tbl.nents != (sg_nents - req_ctx->map_idx))) {
-		pdata->stats.tx_dropped++;
-		dev_warn(dev, "failed to map tx pkt\n");
-		ret = -ENXIO;
+		dev_warn_ratelimited(dev, "failed to map tx pkt\n");
+		ret = -EIO;
 		goto err;
 	}
 
@@ -2577,14 +2654,14 @@ static int sa_aead_perform(struct aead_request *req, u8 *iv, int enc)
 	if (unlikely(IS_ERR_OR_NULL(desc))) {
 		dma_unmap_sg(dev, &req_ctx->sg_tbl.sgl[2],
 				(sg_nents - 2), DMA_TO_DEVICE);
-		pdata->stats.tx_dropped++;
-		dev_warn(dev, "failed to prep slave dma\n");
+		dev_warn_ratelimited(dev, "failed to prep slave dma\n");
 		ret = -ENOBUFS;
 		goto err;
 	}
 
 	req_ctx->tx_chan = pdata->dma_data.tx_chan;
-	req_ctx->dev = dev;
+	req_ctx->dev_data = pdata;
+	req_ctx->pkt = true;
 	desc->callback = sa_tx_dma_cb;
 	desc->callback_param = req_ctx;
 
@@ -2594,18 +2671,17 @@ static int sa_aead_perform(struct aead_request *req, u8 *iv, int enc)
 	spin_unlock_irqrestore(&pdata->irq_lock, irq_flags);
 
 	if (unlikely(dma_submit_error(cookie))) {
-		pdata->stats.tx_dropped++;
-		dev_warn(dev, "failed to submit tx pkt\n");
-		ret = -ENXIO;
+		dev_warn_ratelimited(dev, "failed to submit tx pkt\n");
+		ret = -EIO;
 		goto err;
 	}
 
 	return -EINPROGRESS;
 err:
-	if (req_ctx->sg_tbl.sgl)
+	atomic_inc(&pdata->stats.tx_dropped);
+	if (req_ctx && req_ctx->sg_tbl.sgl)
 		sg_free_table(&req_ctx->sg_tbl);
 	kfree(req_ctx);
-
 	return ret;
 }
 
@@ -2935,7 +3011,151 @@ static void sa_unregister_algos(const struct device *dev)
 }
 
 /************************************************************/
-/*	Driver registration function			*/
+/*	SYSFS interface functions			    */
+/************************************************************/
+struct sa_kobj_attribute {
+	struct attribute attr;
+	ssize_t (*show)(struct keystone_crypto_data *crypto,
+		struct sa_kobj_attribute *attr, char *buf);
+	ssize_t	(*store)(struct keystone_crypto_data *crypto,
+		struct sa_kobj_attribute *attr, const char *, size_t);
+};
+
+#define SA_ATTR(_name, _mode, _show, _store) \
+	struct sa_kobj_attribute sa_attr_##_name = \
+__ATTR(_name, _mode, _show, _store)
+
+static ssize_t sa_stats_show_tx_pkts(struct keystone_crypto_data *crypto,
+		struct sa_kobj_attribute *attr, char *buf)
+{
+	return scnprintf(buf, PAGE_SIZE, "%d\n",
+			atomic_read(&crypto->stats.tx_pkts));
+}
+
+static ssize_t sa_stats_reset_tx_pkts(struct keystone_crypto_data *crypto,
+		struct sa_kobj_attribute *attr, const char *buf, size_t len)
+{
+	atomic_set(&crypto->stats.tx_pkts, 0);
+	return len;
+}
+
+static ssize_t sa_stats_show_rx_pkts(struct keystone_crypto_data *crypto,
+		struct sa_kobj_attribute *attr, char *buf)
+{
+	return scnprintf(buf, PAGE_SIZE, "%d\n",
+			atomic_read(&crypto->stats.rx_pkts));
+}
+
+static ssize_t sa_stats_reset_rx_pkts(struct keystone_crypto_data *crypto,
+		struct sa_kobj_attribute *attr, const char *buf, size_t len)
+{
+	atomic_set(&crypto->stats.rx_pkts, 0);
+	return len;
+}
+
+static ssize_t sa_stats_show_tx_drop_pkts(struct keystone_crypto_data *crypto,
+		struct sa_kobj_attribute *attr, char *buf)
+{
+	return scnprintf(buf, PAGE_SIZE, "%d\n",
+			atomic_read(&crypto->stats.tx_dropped));
+}
+
+static ssize_t sa_stats_reset_tx_drop_pkts(struct keystone_crypto_data *crypto,
+		struct sa_kobj_attribute *attr, const char *buf, size_t len)
+{
+	atomic_set(&crypto->stats.tx_dropped, 0);
+	return len;
+}
+
+static ssize_t
+sa_stats_show_sc_tear_drop_pkts(struct keystone_crypto_data *crypto,
+		struct sa_kobj_attribute *attr, char *buf)
+{
+	return scnprintf(buf, PAGE_SIZE, "%d\n",
+			atomic_read(&crypto->stats.sc_tear_dropped));
+}
+
+static SA_ATTR(tx_pkts, S_IRUGO | S_IWUSR,
+		sa_stats_show_tx_pkts, sa_stats_reset_tx_pkts);
+static SA_ATTR(rx_pkts, S_IRUGO | S_IWUSR,
+		sa_stats_show_rx_pkts, sa_stats_reset_rx_pkts);
+static SA_ATTR(tx_drop_pkts, S_IRUGO | S_IWUSR,
+		sa_stats_show_tx_drop_pkts, sa_stats_reset_tx_drop_pkts);
+static SA_ATTR(sc_tear_drop_pkts, S_IRUGO,
+		sa_stats_show_sc_tear_drop_pkts, NULL);
+
+static struct attribute *sa_stats_attrs[] = {
+	&sa_attr_tx_pkts.attr,
+	&sa_attr_rx_pkts.attr,
+	&sa_attr_tx_drop_pkts.attr,
+	&sa_attr_sc_tear_drop_pkts.attr,
+	NULL
+};
+
+#define to_sa_kobj_attr(_attr) \
+	container_of(_attr, struct sa_kobj_attribute, attr)
+#define to_crypto_data_from_stats_obj(obj) \
+	container_of(obj, struct keystone_crypto_data, stats_kobj)
+
+static ssize_t sa_kobj_attr_show(struct kobject *kobj, struct attribute *attr,
+			     char *buf)
+{
+	struct sa_kobj_attribute *sa_attr = to_sa_kobj_attr(attr);
+	struct keystone_crypto_data *crypto =
+		to_crypto_data_from_stats_obj(kobj);
+	ssize_t ret = -EIO;
+
+	if (sa_attr->show)
+		ret = sa_attr->show(crypto, sa_attr, buf);
+	return ret;
+}
+
+static ssize_t sa_kobj_attr_store(struct kobject *kobj, struct attribute *attr,
+			     const char *buf, size_t len)
+{
+	struct sa_kobj_attribute *sa_attr = to_sa_kobj_attr(attr);
+	struct keystone_crypto_data *crypto =
+		to_crypto_data_from_stats_obj(kobj);
+	ssize_t ret = -EIO;
+
+	if (sa_attr->store)
+		ret = sa_attr->store(crypto, sa_attr, buf, len);
+	return ret;
+}
+
+static const struct sysfs_ops sa_stats_sysfs_ops = {
+	.show = sa_kobj_attr_show,
+	.store = sa_kobj_attr_store,
+};
+
+static struct kobj_type sa_stats_ktype = {
+	.sysfs_ops = &sa_stats_sysfs_ops,
+	.default_attrs = sa_stats_attrs,
+};
+
+static int sa_create_sysfs_entries(struct keystone_crypto_data *crypto)
+{
+	struct device *dev = &crypto->pdev->dev;
+	int ret;
+
+	ret = kobject_init_and_add(&crypto->stats_kobj, &sa_stats_ktype,
+		kobject_get(&dev->kobj), "stats");
+
+	if (ret) {
+		dev_err(dev, "failed to create sysfs entry\n");
+		kobject_put(&crypto->stats_kobj);
+		kobject_put(&dev->kobj);
+	}
+	return ret;
+}
+
+static void sa_delete_sysfs_entries(struct keystone_crypto_data *crypto)
+{
+	kobject_del(&crypto->stats_kobj);
+}
+
+/************************************************************/
+/*	Driver registration functions			*/
 /************************************************************/
 static int sa_read_dtb(struct device_node *node,
 			struct keystone_crypto_data *data)
@@ -2984,6 +3204,8 @@ static int sa_read_dtb(struct device_node *node,
 		dev_dbg(dev, "rx_queue_depth[%d]= %u\n", i,
 				dma_data->rx_queue_depths[i]);
 
+	data->tx_thresh = dma_data->rx_queue_depths[0] - SA_MIN_RX_DESCS;
+
 	ret = of_property_read_u32_array(node, "rx_buffer_size",
 			dma_data->rx_buffer_sizes, KEYSTONE_QUEUES_PER_CHAN);
 	if (ret < 0) {
@@ -3017,6 +3239,29 @@ static int sa_read_dtb(struct device_node *node,
 	return 0;
 }
 
+static int keystone_crypto_remove(struct platform_device *pdev)
+{
+	struct keystone_crypto_data *crypto = platform_get_drvdata(pdev);
+
+	/* un-register crypto algorithms */
+	sa_unregister_algos(&pdev->dev);
+	/* Delete SYSFS entries */
+	sa_delete_sysfs_entries(crypto);
+	/* Free Security context DMA pool */
+	if (crypto->sc_pool)
+		dma_pool_destroy(crypto->sc_pool);
+	/* Release DMA channels */
+	sa_teardown_dma(crypto);
+	/* Kill tasklets */
+	tasklet_kill(&crypto->rx_task);
+
+	clk_disable_unprepare(crypto->clk);
+	clk_put(crypto->clk);
+	kfree(crypto);
+	platform_set_drvdata(pdev, NULL);
+	return 0;
+}
+
 static int keystone_crypto_probe(struct platform_device *pdev)
 {
 	struct device *dev = &pdev->dev;
@@ -3027,10 +3272,8 @@ static int keystone_crypto_probe(struct platform_device *pdev)
 
 	keystone_dev = dev;
 	crypto = devm_kzalloc(dev, sizeof(*crypto), GFP_KERNEL);
-	if (!crypto) {
-		ret = -ENOMEM;
-		goto err;
-	}
+	if (!crypto)
+		return -ENOMEM;
 
 	crypto->clk = clk_get(dev, NULL);
 	if (IS_ERR_OR_NULL(crypto->clk)) {
@@ -3043,12 +3286,12 @@ static int keystone_crypto_probe(struct platform_device *pdev)
 	if (ret < 0) {
 		dev_err(dev, "Couldn't enable clock\n");
 		clk_put(crypto->clk);
+		ret = -ENODEV;
 		goto err;
 	}
 
 	crypto->pdev = pdev;
 	platform_set_drvdata(pdev, crypto);
-	dev_info(dev, "crypto accelerator enabled\n");
 
 	/* Read configuration from device tree */
 	ret = sa_read_dtb(node, crypto);
@@ -3097,27 +3340,28 @@ static int keystone_crypto_probe(struct platform_device *pdev)
 	/* Initialize the IRQ schedule prevention lock */
 	spin_lock_init(&crypto->irq_lock);
 
+	/* Initialize counters */
+	atomic_set(&crypto->stats.tx_dropped, 0);
+	atomic_set(&crypto->stats.sc_tear_dropped, 0);
+	atomic_set(&crypto->pend_compl, 0);
+	atomic_set(&crypto->stats.tx_pkts, 0);
+	atomic_set(&crypto->stats.rx_pkts, 0);
+
+	/* Create sysfs entries */
+	ret = sa_create_sysfs_entries(crypto);
+	if (ret)
+		goto err;
+
 	/* Register crypto algorithms */
 	sa_register_algos(dev);
+	dev_info(dev, "crypto accelerator enabled\n");
+	return 0;
 
-	ret = 0;
 err:
+	keystone_crypto_remove(pdev);
 	return ret;
 }
 
-static int keystone_crypto_remove(struct platform_device *pdev)
-{
-	struct keystone_crypto_data *crypto = platform_get_drvdata(pdev);
-
-	/* un-register crypto algorithms */
-	sa_unregister_algos(&pdev->dev);
-	dma_pool_destroy(crypto->sc_pool);
-	clk_disable_unprepare(crypto->clk);
-	clk_put(crypto->clk);
-	kfree(crypto);
-	return 0;
-}
-
 static struct of_device_id of_match[] = {
 	{ .compatible = "ti,keystone-crypto", },
 	{},
-- 
2.7.4

