From db24203effa4e30ad5adcb5da5459eb32ec233d6 Mon Sep 17 00:00:00 2001
From: Aurelien Jacquiot <a-jacquiot@ti.com>
Date: Fri, 10 Jun 2016 20:09:12 -0400
Subject: [PATCH 287/347] rapidio: keystone: add support of rapidio

This patch comes from:
  git://git.ti.com/processor-sdk/processor-sdk-linux.git

This patch adds platform specific support of rapidio on
TI's Keystone2 (Kepler) platform.

Signed-off-by: Aurelien Jacquiot <a-jacquiot@ti.com>
Signed-off-by: WingMan Kwok <w-kwok2@ti.com>
Signed-off-by: Jacob Stiffler <j-stiffler@ti.com>
(cherry picked from commit cf29b38d749ca61b8c0724a0c8d8405ba8a39e47)
Signed-off-by: Zumeng Chen <zumeng.chen@windriver.com>
---
 .../devicetree/bindings/rapidio/keystone-srio.txt  |  269 ++
 Documentation/rapidio/keystone.txt                 |  181 +
 drivers/rapidio/devices/Kconfig                    |    7 +
 drivers/rapidio/devices/Makefile                   |    4 +
 drivers/rapidio/devices/keystone_rio.c             | 3498 ++++++++++++++++++++
 drivers/rapidio/devices/keystone_rio.h             |  981 ++++++
 drivers/rapidio/devices/keystone_rio_dma.c         |  774 +++++
 drivers/rapidio/devices/keystone_rio_mp.c          | 1289 +++++++
 drivers/rapidio/devices/keystone_rio_serdes.c      | 2061 ++++++++++++
 drivers/rapidio/devices/keystone_rio_serdes.h      |  124 +
 10 files changed, 9188 insertions(+), 0 deletions(-)
 create mode 100644 Documentation/devicetree/bindings/rapidio/keystone-srio.txt
 create mode 100644 Documentation/rapidio/keystone.txt
 create mode 100644 drivers/rapidio/devices/keystone_rio.c
 create mode 100644 drivers/rapidio/devices/keystone_rio.h
 create mode 100644 drivers/rapidio/devices/keystone_rio_dma.c
 create mode 100644 drivers/rapidio/devices/keystone_rio_mp.c
 create mode 100644 drivers/rapidio/devices/keystone_rio_serdes.c
 create mode 100644 drivers/rapidio/devices/keystone_rio_serdes.h

diff --git a/Documentation/devicetree/bindings/rapidio/keystone-srio.txt b/Documentation/devicetree/bindings/rapidio/keystone-srio.txt
new file mode 100644
index 0000000..4f21643
--- /dev/null
+++ b/Documentation/devicetree/bindings/rapidio/keystone-srio.txt
@@ -0,0 +1,269 @@
+This document describes the device tree bindings associated with the KeyStone
+Serial RapidIO driver support.
+
+Required properties
+-------------------
+
+#address-cells
+#size-cells:      Should be '1' if the device has sub-nodes with 'reg' property.
+reg:              Address and length of the register set for the SRIO subsystem
+		  on the SOC.
+reg-names:        Indicates the name of the various register ranges defined with
+		  the "reg" property above. There are three mandatory ranges of
+		  registers:
+		   - RapidIO registers ("rio")
+		   - boot configuration registers (for JTAGID and K1 SerDes
+                     setup) ("boot_config")
+		   - SerDes configuration registers ("serdes")
+clocks:           Clock IDs array as required by the controller.
+clock-names:      Names of clocks corresponding to IDs in the clock property.
+compatible:       Should be "ti,keystone-rapidio".
+interrupts:       The two IRQ definitions for:
+		   - RIO special interrupts (doorbells, errors, port-writes in,
+		   ...)
+                   - LSU completions
+
+Optional properties
+-------------------
+
+dma-coherent:     Indicates if DirectIO operations support hw cache-coherency.
+		  On KeyStone platforms it is recommended to set this property
+		  when running on ARM cores.
+keystone2-serdes: Indicates that the driver needs to use KeyStone 2 SerDes
+		  initialization. If not specified, it will be KeyStone 1
+		  initialization instead.
+baudrate:         This the lane baudrate configuration number (from 0 to 3).
+		  If not specified the baudrate configuration 3 will be used
+                  (5 Gbps).
+path-mode:        This is the SerDes lanes to ports configuration number (from 0
+                  to 4). If not specified the mode 0 will be used (4 ports in
+                  1x).
+port-register-timeout:
+		  This is the delay in seconds for waiting the port registration
+                  at startup. if not specified a default 30 seconds value will
+                  be used.
+ports:            This is the bitfield of RapidIO port(s) to probe at startup.
+		  If not specified, no port will be used.
+ports-remote:     Allows to explicitly specify the remote port number (of the
+                  link partner) for each local port. There is one value per port
+                  for the 4 ports.
+		  If not specified, the remote ports will be detected at startup
+                  using the peer Switch Port Information CAR register if
+                  available (meaning that the link partner implements such
+                  register).
+		  The remote ports are needed for the software error recovery
+                  process.
+dev-id-size:      RapidIO common transport system size:
+                   0 - Small size, 8-bit deviceID fields, 256 devices.
+                   1 - Large size, 16-bit deviceID fields, 65536 devices.
+		  If not specified 0 (8-bit) value will be used.
+lsu:              Start and end indexes of the LSU range that will be used for
+                  DirectIO transfers and maintenance requests. Using multiple
+                  LSUs allows to allocate one for maintenance packets and
+                  different LSUs for each DMA channels used for DirectI/O
+                  transfers increasing the parallelism capabilities.
+                  If not specified LSU 0 will be used for all kind of transfers.
+num-dio-channels  Number of virtual DMA channels available for DirectI/O
+                  transfers. Note that one is always reserved for doorbells
+                  purpose. Default is 8 if not specified.
+rxu-map-range:    Define the range (start and end values) of RXU mapping entries
+                  that will be used for MP receive. Range must start from 0 and
+                  end up to 63. If not specified the full range will be used by
+		  Linux(0 to 63).
+num-mboxes:       Number of receive mailboxes used for MP. If not specified,
+                  only one mailbox will be used.
+pkt-forward:      This allows to define the routing table for hardware packet
+		  forwarding. There are 8 entries in the table. Each entry
+                  starts with the lower DeviceID boundary, then the upper
+                  DeviceID boundary and the output port to route packets whose
+                  DestID falls within the described range.
+		  For example, '0x3 0x3 1' will forward incoming packets with
+		  DestID 0x3 to port 1.
+		  All the 8 entries must be referenced with the 3 values.
+
+SerDes optional properties
+--------------------------
+
+These properties are used to tune the KeyStone 2 SerDes hardware parameters and
+coefficients. For more information please refer to KeyStone II Architecture
+Serializer/Deserializer (SerDes) User Guide (SPRUHO3) available on www.ti.com.
+
+serdes-1lsb:      Allows to set the TX driver 1 lsb pre emphasis setting for
+                  KeyStone 2 SerDes. There is one value per lane. Default value
+		  is 0.
+serdes-c1:        Allows to override value of TX driver C1 coefficient for
+		  KeyStone 2 SerDes. There is one value per lane. Default value
+		  if not specified is 6 for 3.125Gbps and 4 for other baudrates.
+serdes-c2:        Allows to override value of TX driver C2 coefficient for
+		  KeyStone 2 SerDes. There is one value per lane. Default value
+		  is 0 if not specified.
+serdes-cm:	  Allows to override value of TX driver CM coefficient for
+		  KeyStone 2 SerDes. There is one value per lane. Default value
+		  is 0.
+serdes-att:       Allows to set attenuator setting of TX driver for KeyStone 2
+		  SerDes. There is one value per lane. Default value is 12 if
+		  not specified.
+serdes-vreg:      Allows to set regulator voltage setting for TX driver for
+		  KeyStone 2 SerDes. There is one value per lane. Default value
+		  is 4 if not specified.
+serdes-vdreg:     Allows to set lane regulator output voltage setting for TX
+		  driver for KeyStone 2. Default value is 1 (VNOM) if not
+		  specified.
+serdes-rx-att-start:
+                  Allows to set attenuator start value of RX driver for Keystone
+		  2 SerDes. There is one value per lane. Default value is 3.
+serdes-rx-boost-start:
+                  Allows to set attenuator start value of RX driver for Keystone
+		  2 SerDes. There is one value per lane. Default value is 3.
+serdes-rx-att:    Allows to set attenuator static value of RX driver for
+		  Keystone 2 SerDes. If set to -1, dynamic calibration is used
+		  instead. There is one value per lane. Default value is -1.
+serdes-rx-boost:  Allows to set attenuator start value of RX driver for Keystone
+		  2 SerDes. If set to -1, dynamic calibration is used instead.
+                  There is one value per lane. Default value is -1.
+
+Sub-nodes
+---------
+
+Each mailbox (according to num-mboxes) must be added as subnodes "mbox-%d" with
+%d the mailbox number (from 0 to n).
+This sub-node has the following properties:
+
+rx-channel:       The packet DMA channel associated to this receive mailbox.
+		  This property is mandatory and must refer to a specified
+		  'navigator-dma-names' DMA client resource like specified in
+		  the Documentation/devicetree/bindings/soc/ti/
+		  keystone-navigator-dma.txt file
+rx-pool:	  This mandatory property defines the pool of descriptors that
+		  will be used for the receive mailbox. The first parameter is
+		  the size of descriptor, the second is the region Id where to
+		  get descriptors for receive DMA.
+rx-queue:         This property defines the hw queue Id used for receive.
+		  If not specified a QPEND queue will be allocated by the
+		  kernel.
+rx-queue-depth:   This can be at present a maximum of 4 queues per packet DMA
+		  channel. We can specify here the number of descriptors for
+		  each queue.
+		  If not specified only one queue of depth of 128 will be used.
+rx-buffer-size:   For each receive queue, we can specify the buffer size.
+		  Default is 4096 bytes if not specified.
+tx-channel:       This is the packet DMA channel name associated with this
+		  mailbox for transmit. This property is mandatory and works
+		  like the rx-channel property.
+tx-queue:         This mandatory property defines the hw transmit queue Id.
+		  This must be set in accordance to the Packet DMA channel
+		  used in transmit. Check the KeyStone SRIO hw documentation
+		  for more information.
+tx-completion-queue:
+	          This property defines the hw queue Id used for transmit
+		  completion. If not specified a QPEND queue will be allocated
+		  by the  kernel.
+tx-garbage-queue:
+	          This property defines the hw queue Id used for transmit
+		  errors. If not specified a QPEND queue will be allocated
+		  by the  kernel.
+tx-queue-depth:   Number of descriptors for the transmit queue. Default is 128
+                  descriptors if not specified
+stream-id:        If specified, the mailbox will be mapped to the indicated
+		  stream ID and packet type 9 will be used instead of packet
+                  type 11.
+
+Example
+-------
+
+knav_dmas: knav_dmas@0 {
+
+        ...
+
+	dma_srio: dma_srio@0 {
+		  reg = <0x2901000 0x020>,
+		        <0x2901400 0x200>,
+			<0x2901800 0x200>,
+			<0x2901c00 0x200>,
+			<0x2902000 0x280>;
+		  reg-names = "global", "txchan", "rxchan",
+                              "txsched", "rxflow";
+        };
+
+	...
+
+};
+
+rapidio: rapidio@2900000 {
+	compatible = "ti,keystone-rapidio";
+	#address-cells = <1>;
+	#size-cells = <1>;
+	reg = <0x2900000 0x40000        /* rio regs */
+	       0x2620000 0x1000         /* boot config regs */
+               0x232c000 0x2000>;       /* serdes config regs */
+	reg-names = "rio", "boot_config", "serdes";
+	clocks = <&clksrio>;
+	clock-names = "clk_srio";
+	dma-coherent;
+
+	ti,navigator-dmas = <&dma_srio 18>, /* phandle channel_id */
+			    <&dma_srio 19>,
+			    <&dma_srio 0>,
+			    <&dma_srio 1>;
+	ti,navigator-dma-names = "riorx0", "riorx1", "riotx0", "riotx1";
+
+	keystone2-serdes;
+
+	baudrate  = <3>;              /* 5 Gbps */
+	path-mode = <4>;              /* 1 port in 4x */
+	port-register-timeout = <30>; /* 30 seconds */
+
+	lsu = <0 0>;            /* available LSU range (start end) */
+	num-dio-channels = <8>; /* number of DIO DMA channels */
+
+	ports = <0x1>;      /* bitfield of port(s) to probe */
+	ports-remote = <0 1 2 3>; /* remote ports:
+			           * local port 0 is connected to peer port 0,
+				   * local port 1 is connected to peer port 1,
+				   * ...
+				   */
+
+	dev-id-size = <0>;  /* RapidIO common transport system
+			     * size.
+			     * 0 - Small size. 8-bit deviceID
+			     *     fields. 256 devices.
+			     * 1 - Large size, 16-bit deviceID
+			     *     fields. 65536 devices.
+			     */
+
+	interrupts = <0 152 0xf01
+                      0 153 0xf01>; /* RIO and LSU IRQs */
+
+	rxu-map-range = <0 15>; /* use only the 16 first RXU mapping entries */
+	num-mboxes = <2>;
+
+	mbox-0 {
+	        rx-channel = "riorx0";
+		rx-pool = <128 15>; /* size region_id */
+                rx-queue-depth  = <256 0 0 0>;
+                rx-buffer-size  = <4096 0 0 0>;
+		rx-queue = <8714>;
+                /*stream-id = <0>;*/
+		tx-channel = "riotx0";
+		tx-pool = <128 15>;
+		tx-queue-depth = <256>;
+		tx-queue = <672>; /* hw transmit queue for channel/flow 0 */
+		tx-completion-queue = <8716>;
+		tx-garbage-queue = <8717>;
+        };
+
+        mbox-1 {
+	        rx-channel = "riorx1";
+		rx-pool = <128 15>;
+                rx-queue-depth  = <256 0 0 0>;
+                rx-buffer-size  = <4096 0 0 0>;
+		rx-queue = <8715>;
+                /*stream-id = <1>;*/
+		tx-channel = "riotx1";
+		tx-pool = <128 15>;
+		tx-queue-depth = <256>;
+		tx-queue = <673>;  /* hw transmit queue for channel/flow 1 */
+	        tx-completion-queue = <8718>;
+		tx-garbage-queue = <8719>;
+        };
+};
diff --git a/Documentation/rapidio/keystone.txt b/Documentation/rapidio/keystone.txt
new file mode 100644
index 0000000..75d83ad
--- /dev/null
+++ b/Documentation/rapidio/keystone.txt
@@ -0,0 +1,181 @@
+RapidIO subsystem mport driver for Texas Instruments KeyStone devices.
+======================================================================
+
+The KeyStone mport driver provides the following functionalities:
+
+ - Maintenance read and write operations.
+ - Direct I/O operation support (NREAD, NWRITE_R, NWRITE, SWRITE).
+ - Inbound and outbound doorbell support.
+ - MP with both type 11 (message) and type 9 (Data Streaming) packets.
+ - Support for multiple mailboxes with dedicated messaging channels.
+ - Support of MP transmit error queues.
+ - Inbound maintenance port-writes management.
+ - Multi-port support.
+ - Support for hardware packet forwarding.
+ - Software DMA engine interface for Direct I/O.
+ - Support software error recovery mechanism (for aligning AckIds).
+ - Reset symbol handling support.
+ - Capability to rescan port status through sysfs.
+ - SerDes management with optional PRBS training capability.
+ - Implementing all currently defined RapidIO mport callback functions.
+
+All related DTS bindings are documented into the
+Documentation/devicetree/bindings/rapidio/keystone-srio.txt file.
+
+Direct I/O:
+
+ The DirectI/O and maintenance packet LSU can be specified in DTS using
+ the lsu = < DIO_LSU MAINT_LSU >; entry in DTS.
+
+ If not specified in DTS, LSU #0 is used by default.
+
+Message Passing:
+
+ For inbound messages the driver uses destination ID matching to forward
+ messages into the corresponding message queue.
+ Messaging callbacks are implemented to be fully compatible with RIONET driver.
+
+ The mport driver supports type 9 (Data Streaming) packets in addition to type
+ 11 (Message). In order to fit existing Linux rapidio message API the stream Ids
+ are mapped to mbox Ids.
+ Any inbound messages can be received with either type 9 or type 11 packets for
+ a given stream/mbox Id. COS is not used today.
+
+ The keystone mport driver supports up to 4 mailboxes for both Tx/Rx.
+ The different mailboxes must be configured through the board DTS file.
+
+ Each mailbox can be used for either type 11 or type 9 packets.
+ By default type 11 is used but if specifying the 'stream_id'
+ value in the DTS file type 9 is used instead and the corresponding
+ mailbox is mapped to the 'stream_id' stream id (note that this
+ mapping must be identical on all RapidIO nodes).
+
+ The RXU mapping resources range can be specified through DTS.
+ This can be used to exclude from Linux driver the RXU mapping resources
+ used by the DSP side.
+
+ Each mailbox has its own dedicated MP transmit channel.
+
+Packet Forwarding:
+
+ In order to use it, a 'pkt-forward' table must be added into the board DTS.
+ The format is the following:
+
+ pkt-forward = <0xffff 0xffff 0 /* dev_id_low dev_id_high port */
+                0xffff 0xffff 0
+                0xffff 0xffff 0
+                0xffff 0xffff 0
+                0xffff 0xffff 0
+                0xffff 0xffff 0
+                0xffff 0xffff 0
+                0xffff 0xffff 0>;
+
+ It consists in 8 entries, each entry starts with the lower DeviceID
+ boundary, then the upper DeviceID boundary and the output port to route
+ packets whose DestID falls within the described range.
+
+ For example, '0x3 0x3 1' will forward incoming packets with
+ DestID 0x3 to port 1.
+
+ All the 8 entries must be referenced with the 3 required values.
+
+Ports to lanes mapping:
+
+ The 'ports' entry in DTS is a bit field related to the lane
+ assignment.
+ For example if an user wants to use mode 3 with two ports,
+ 'ports' must be set to 0x5 (ports 0 and 2 mapped on lanes A and C).
+
+ Also the mode 1 and 2 were switched in the current TI SRIO documentation:
+ - mode 1 is port 0 in 2x, port 2 and 3 in 1x
+ - mode 2 is port 0 and 1 in 1x, port 2 in 2x
+
+Error recovery:
+
+ Error recovery needs to known the remote ports of the link partner for each
+ local port. If the remote ports is different from the local
+ one, the driver need to know this mapping.
+
+ This can be performed by either specifying the remote ports in the DTS using
+ an optional remote port definition.
+
+ DTS syntax is the following:
+   ports-remote = <0 1 2 3>;  /* remote link partner port numbers */
+
+ Otherwise if not specified, the remote ports will be detected at startup using
+ the peer Switch Port Information CAR register if available (meaning that the
+ link partner implements such register).
+
+ The ackid alignment process needs to writes to the registers of the defined
+ remote port (link partner).
+
+ There is an 'error_recovery' option for not using error recovery and use port
+ reset instead.
+
+ Because the recovery mechanism may not work with all kind of link
+ partners such as RapidIO CPS switches or some FPGA end-points, we can disable
+ the error recovery with this driver module option.
+ Instead a reset symbol is sent to the corresponding port and the whole RapidIO
+ controller is reset.
+
+ By default error recovery is enabled in the driver. Use the
+ 'keystone_rio_mport.error_recovery=0' kernel command line option to
+ disable it.
+
+Reset symbol handling:
+
+ RapidIO peripheral is reinitialized when reset control symbol is received.
+ SerDes lanes are disabled in interrupt context to trigger loss of link on
+ link partner ASAP.
+ Afterwards RapidIO peripheral and SerDes are shutdown then initialized again.
+
+Rescan port status through sysfs:
+
+ This feature allows to rescan SRIO ports from userspace after port
+ timeout to check if new RapidIO links can been established.
+
+ A 'ports' file is exported through sysfs at the SRIO controller
+ device level (/sys/devices/soc.2/2900000.rapidio/ports).
+ Reading this file provides the mask of the SRIO ports where link(s)
+ are currently established and tested.
+ Writing a new mask to this file will start again the link status
+ check procedure on these ports and eventually register the new
+ detected Linux mports.
+
+ example: echo '0xf' > /sys/devices/soc.2/2900000.rapidio/ports
+ will rescan ports 0,1,2 and 3.
+
+ Only the ports that are defined in the DTS ('ports' property) and
+ not already up can be scanned again. Also ports cannot be disabled
+ by clearing corresponding bits in the 'ports' file.
+
+ This does not replace the enumeration/discovery procedure at the
+ RapidIO logical level through the /sys/bus/rapidio/scan file.
+
+SerDes management:
+
+ All SerDes related functions are in a separate driver which exports
+ methods for both K1 and K2 through a keystone_serdes_ops structure.
+
+ SerDes Tx coefficients can be tuned through optional configurable
+ c1, c2, cm, attenuator and regulator voltage parameters. SerDes Rx
+ coefficients can be tuned as well (att/boost).
+ SerDes Tx/Rx coefficients can be changed by both DTS or using sysfs
+ entries (in the 'serdes' directory).
+
+ The driver provides the capability to perform an optional calibration of SerDes
+ Rx coefficients with PRBS training between two link partners.
+
+ It provides sysfs entries for initiating SerDes calibration
+ ('calibrate') and starting/stopping the SRIO controller ('start').
+ Writing a non-zero value in these files will start the service, writing a
+ zero will stop it.
+ Calibration can only be performed when SRIO controller has not been started.
+
+ The mport driver also provides module options to control the calibration
+ ('serdes_calibration', default 0) and the port enabling at boot time
+ ('enable_ports', default 1).
+
+ The SerDes driver implements the SerDes Tx termination workaround which
+ reads lane 0 termination and then applies to all lanes for consistent
+ impedance calibration.
diff --git a/drivers/rapidio/devices/Kconfig b/drivers/rapidio/devices/Kconfig
index c4cb087..8817b39 100644
--- a/drivers/rapidio/devices/Kconfig
+++ b/drivers/rapidio/devices/Kconfig
@@ -8,3 +8,10 @@ config RAPIDIO_TSI721
 	default "n"
 	---help---
 	  Include support for IDT Tsi721 PCI Express Serial RapidIO controller.
+
+config TI_KEYSTONE_RAPIDIO
+       tristate "TI Keystone RapidIO support"
+       depends on RAPIDIO && ARCH_KEYSTONE
+       default "n"
+       ---help---
+         This driver supports TI's Keystone RapidIO.
diff --git a/drivers/rapidio/devices/Makefile b/drivers/rapidio/devices/Makefile
index 927dbf8..fe39d29 100644
--- a/drivers/rapidio/devices/Makefile
+++ b/drivers/rapidio/devices/Makefile
@@ -6,3 +6,7 @@ obj-$(CONFIG_RAPIDIO_TSI721)	+= tsi721_mport.o
 tsi721_mport-y			:= tsi721.o
 tsi721_mport-$(CONFIG_RAPIDIO_DMA_ENGINE) += tsi721_dma.o
 obj-$(CONFIG_RAPIDIO_MPORT_CDEV) += rio_mport_cdev.o
+obj-$(CONFIG_TI_KEYSTONE_RAPIDIO) += keystone_rio_mport.o
+keystone_rio_mport-y            := keystone_rio.o keystone_rio_serdes.o \
+				keystone_rio_mp.o
+keystone_rio_mport-$(CONFIG_RAPIDIO_DMA_ENGINE) += keystone_rio_dma.o
diff --git a/drivers/rapidio/devices/keystone_rio.c b/drivers/rapidio/devices/keystone_rio.c
new file mode 100644
index 0000000..e165ff8
--- /dev/null
+++ b/drivers/rapidio/devices/keystone_rio.c
@@ -0,0 +1,3498 @@
+/*
+ * Copyright (C) 2010, 2011, 2012, 2013, 2014 Texas Instruments Incorporated
+ * Authors: Aurelien Jacquiot <a-jacquiot@ti.com>
+ * - Main driver implementation.
+ * - Updated for support on TI KeyStone 2 platform.
+ *
+ * Copyright (C) 2012, 2013 Texas Instruments Incorporated
+ * WingMan Kwok <w-kwok2@ti.com>
+ * - Updated for support on TI KeyStone 1 platform.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation version 2.
+ *
+ * This program is distributed "as is" WITHOUT ANY WARRANTY of any
+ * kind, whether express or implied; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+#include <linux/clk.h>
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/interrupt.h>
+#include <linux/device.h>
+#include <linux/of.h>
+#include <linux/of_address.h>
+#include <linux/of_irq.h>
+#include <linux/dma-mapping.h>
+#include <linux/delay.h>
+#include <linux/kfifo.h>
+#include <linux/platform_device.h>
+#include <linux/rio.h>
+#include <linux/rio_drv.h>
+
+#include "keystone_rio_serdes.h"
+#include "keystone_rio.h"
+
+#define DRIVER_VER "v1.4"
+
+static bool serdes_calibration;
+module_param(serdes_calibration, bool, 0);
+MODULE_PARM_DESC(
+	serdes_calibration,
+	"Perform Serdes calibration before starting RapidIO (default = 0)");
+
+static bool enable_ports = 1;
+module_param(enable_ports, bool, 0);
+MODULE_PARM_DESC(
+	enable_ports,
+	"Enable RapidIO ports at boottime (default = 1)");
+
+/*
+ * When using some RIO switches like CPS gen2, error recovery must be
+ * disabled. Local controller reset and reset symbol will be used instead.
+ */
+static bool error_recovery = 1;
+module_param(error_recovery, bool, 0);
+MODULE_PARM_DESC(
+	error_recovery,
+	"Use RapidIO error recovery mechanism (default = 1)");
+
+static void dbell_handler(
+	struct keystone_rio_data *krio_priv);
+static void keystone_rio_port_write_handler(
+	struct keystone_rio_data *krio_priv);
+static void keystone_rio_handle_logical_error(
+	struct keystone_rio_data *krio_priv);
+static int  keystone_rio_setup_controller(
+	struct keystone_rio_data *krio_priv);
+static void keystone_rio_shutdown_controller(
+	struct keystone_rio_data *krio_priv);
+
+/*
+ * Table with the various lanes per port configuration modes:
+ * path mode 0: 4 ports in 1x
+ * path mode 1: 3 ports in 2x/1x
+ * path mode 2: 3 ports in 1x/2x
+ * path mode 3: 2 ports in 2x
+ * path mode 4: 1 ports in 4x
+ */
+static struct keystone_lane_config keystone_lane_configs[5][4] = {
+	{ {0, 1}, {1, 2},   {2, 3},   {3, 4}   },
+	{ {0, 2}, {-1, -1}, {2, 3},   {3, 4}   },
+	{ {0, 1}, {1, 2},   {2, 4},   {-1, -1} },
+	{ {0, 2}, {-1, -1}, {2, 4},   {-1, -1} },
+	{ {0, 4}, {-1, -1}, {-1, -1}, {-1, -1} },
+};
+
+#define krio_write_reg(r, v)		writel(reg_val, reg)
+#define krio_read_reg(r)		readl(reg)
+
+#define krio_read(k, r)			readl(&k->regs->r)
+#define krio_write(k, r, v)		writel((v), &k->regs->r)
+
+#define krio_car_csr_read(k, r)		readl(&k->car_csr_regs->r)
+#define krio_car_csr_write(k, r, v)	writel((v), &k->car_csr_regs->r)
+
+#define krio_car_csr_read_ofs(k, ofs)		\
+		readl((void __iomem *)k->car_csr_regs + ofs)
+#define krio_car_csr_write_ofs(k, ofs, v)	\
+		writel((v), (void __iomem *)k->car_csr_regs + ofs)
+
+#define krio_sp_read(k, r)		readl(&k->serial_port_regs->r)
+#define krio_sp_write(k, r, v)		writel((v), &k->serial_port_regs->r)
+
+#define krio_err_read(k, r)		readl(&k->err_mgmt_regs->r)
+#define krio_err_write(k, r, v)		writel((v), &k->err_mgmt_regs->r)
+
+#define krio_phy_read(k, r)		readl(&k->phy_regs->r)
+#define krio_phy_write(k, r, v)		writel((v), &k->phy_regs->r)
+
+#define krio_tp_read(k, r)		readl(&k->transport_regs->r)
+#define krio_tp_write(k, r, v)		writel((v), &k->transport_regs->r)
+
+#define krio_pb_read(k, r)		readl(&k->pkt_buf_regs->r)
+#define krio_pb_write(k, r, v)		writel((v), &k->pkt_buf_regs->r)
+
+#define krio_ev_read(k, r)		readl(&k->evt_mgmt_regs->r)
+#define krio_ev_write(k, r, v)		writel((v), &k->evt_mgmt_regs->r)
+
+#define krio_pw_read(k, r)		readl(&k->port_write_regs->r)
+#define krio_pw_write(k, r, v)		writel((v), &k->port_write_regs->r)
+
+#define krio_lnk_read(k, r)		readl(&k->link_regs->r)
+#define krio_lnk_write(k, r, v)		writel((v), &k->link_regs->r)
+
+#define krio_fab_read(k, r)		readl(&k->fabric_regs->r)
+#define krio_fab_write(k, r, v)		writel((v), &k->fabric_regs->r)
+
+/*----------------------- Interrupt management -------------------------*/
+
+static irqreturn_t lsu_interrupt_handler(int irq, void *data)
+{
+#ifdef CONFIG_RAPIDIO_DMA_ENGINE
+	struct keystone_rio_data *krio_priv = data;
+	u32 pending_err_int = krio_read(krio_priv, lsu_int[0].status);
+
+	u32 pending_lsu_int;
+
+	/* Call DMA interrupt handling */
+	while ((pending_lsu_int = krio_read(krio_priv, lsu_int[1].status) &
+				  KEYSTONE_RIO_ICSR_LSU1_COMPLETE_MASK)) {
+		u32 lsu = __ffs(pending_lsu_int);
+
+		keystone_rio_dma_interrupt_handler(krio_priv, lsu, 0);
+		krio_write(krio_priv, lsu_int[1].clear, BIT(lsu));
+	}
+
+	/* In case of LSU completion with error */
+	if (pending_err_int & KEYSTONE_RIO_ICSR_LSU0_ERROR_MASK) {
+		keystone_rio_dma_interrupt_handler(krio_priv, 0, 1);
+
+		krio_write(krio_priv, lsu_int[0].clear,
+			   pending_err_int & KEYSTONE_RIO_ICSR_LSU0_ERROR_MASK);
+	}
+#endif
+
+	return IRQ_HANDLED;
+}
+
+static void keystone_rio_reset_symbol_handler(
+	struct keystone_rio_data *krio_priv)
+{
+	/* Disable SerDes lanes asap to generate a loss of link */
+	krio_priv->serdes.ops->disable_lanes(krio_priv->board_rio_cfg.lanes,
+					     &krio_priv->serdes);
+
+	/* Schedule SRIO peripheral reinitialization */
+	schedule_work(&krio_priv->reset_work);
+}
+
+static inline void keystone_rio_send_reset(
+	struct keystone_rio_data *krio_priv,
+	u32 port)
+{
+	/* Send a reset control symbol on appropriate port */
+	krio_sp_write(krio_priv, sp[port].link_maint_req, 0x3);
+}
+
+static void keystone_rio_port_error(struct keystone_rio_data *krio_priv,
+				    u32 port)
+{
+	if (error_recovery) {
+		dev_dbg(krio_priv->dev, "do recovery of port %d\n", port);
+
+		/* Add port to failed ports and schedule immediate recovery */
+		krio_priv->pe_cnt =
+			krio_priv->board_rio_cfg.port_register_timeout /
+			(KEYSTONE_RIO_REGISTER_DELAY / HZ);
+		krio_priv->pe_ports |= BIT(port);
+
+		schedule_delayed_work(&krio_priv->pe_work, 0);
+	} else {
+		dev_dbg(krio_priv->dev, "send reset symbol to port %d\n", port);
+
+		/* Send reset control symbol to peer */
+		if (krio_priv->board_rio_cfg.ports & BIT(port))
+			keystone_rio_send_reset(krio_priv, port);
+
+		/* Reset local ports (all) */
+		if (!krio_priv->ports_registering)
+			keystone_rio_reset_symbol_handler(krio_priv);
+	}
+}
+
+static void special_interrupt_handler(int ics,
+				      struct keystone_rio_data *krio_priv)
+{
+	/* Acknowledge the interrupt */
+	krio_write(krio_priv, err_rst_evnt_int_clear, BIT(ics));
+
+	dev_dbg(krio_priv->dev, "ics = %d\n", ics);
+
+	switch (ics) {
+	case KEYSTONE_RIO_MCAST_EVT_INT:
+		/* Multi-cast event control symbol interrupt received */
+		break;
+
+	case KEYSTONE_RIO_PORT_WRITEIN_INT:
+		/* Port-write-in request received on any port */
+		keystone_rio_port_write_handler(krio_priv);
+		break;
+
+	case KEYSTONE_RIO_EVT_CAP_ERROR_INT:
+		/* Logical layer error management event capture */
+		keystone_rio_handle_logical_error(krio_priv);
+		break;
+
+	case KEYSTONE_RIO_PORT0_ERROR_INT:
+	case KEYSTONE_RIO_PORT1_ERROR_INT:
+	case KEYSTONE_RIO_PORT2_ERROR_INT:
+	case KEYSTONE_RIO_PORT3_ERROR_INT:
+		/* Port error */
+		keystone_rio_port_error(krio_priv,
+					ics - KEYSTONE_RIO_PORT0_ERROR_INT);
+		break;
+
+	case KEYSTONE_RIO_RESET_INT:
+		/* Device reset interrupt on any port */
+		keystone_rio_reset_symbol_handler(krio_priv);
+		break;
+	}
+}
+
+static irqreturn_t rio_interrupt_handler(int irq, void *data)
+{
+	struct keystone_rio_data *krio_priv = data;
+	u32 pending_err_rst_evnt_int =
+			krio_read(krio_priv, err_rst_evnt_int_stat) &
+			KEYSTONE_RIO_ERR_RST_EVNT_MASK;
+
+	/* Handle special interrupts (error, reset, special event) */
+	while (pending_err_rst_evnt_int) {
+		u32 ics = __ffs(pending_err_rst_evnt_int);
+
+		pending_err_rst_evnt_int &= ~BIT(ics);
+		special_interrupt_handler(ics, krio_priv);
+	}
+
+	/* Call doorbell handler(s) */
+	dbell_handler(krio_priv);
+
+	return IRQ_HANDLED;
+}
+
+/*
+ * Map a SRIO event to a SRIO interrupt
+ */
+static void keystone_rio_interrupt_map(u32 __iomem *reg, u32 mask, u32 rio_int)
+{
+	int i;
+	u32 reg_val;
+
+	reg_val = krio_read_reg(reg);
+
+	for (i = 0; i <= 32; i += 4) {
+		if ((mask >> i) & 0xf) {
+			reg_val &= ~(0xf << i);
+			reg_val |= (rio_int << i);
+		}
+	}
+	krio_write_reg(reg, reg_val);
+}
+
+/*
+ * Setup RIO interrupts
+ */
+static int keystone_rio_interrupt_setup(struct keystone_rio_data *krio_priv)
+{
+	int res;
+	u8 lsu;
+
+	/* Clear all pending interrupts */
+	krio_write(krio_priv, doorbell_int[0].clear, 0x0000ffff);
+	krio_write(krio_priv, doorbell_int[1].clear, 0x0000ffff);
+	krio_write(krio_priv, doorbell_int[2].clear, 0x0000ffff);
+	krio_write(krio_priv, doorbell_int[3].clear, 0x0000ffff);
+	krio_write(krio_priv, err_rst_evnt_int_clear,
+		   KEYSTONE_RIO_ERR_RST_EVNT_MASK);
+
+	for (lsu = krio_priv->lsu_start; lsu <= krio_priv->lsu_end; lsu++)
+		krio_write(krio_priv, lsu_int[lsu].clear, 0xffffffff);
+
+	/* LSU interrupts are routed to RIO interrupt dest 1 (LSU) */
+	keystone_rio_interrupt_map(&krio_priv->regs->lsu0_int_route[0],
+				   0x11111111, KEYSTONE_LSU_RIO_INT);
+	keystone_rio_interrupt_map(&krio_priv->regs->lsu0_int_route[1],
+				   0x11111111, KEYSTONE_LSU_RIO_INT);
+	keystone_rio_interrupt_map(&krio_priv->regs->lsu0_int_route[2],
+				   0x11111111, KEYSTONE_LSU_RIO_INT);
+	keystone_rio_interrupt_map(&krio_priv->regs->lsu0_int_route[3],
+				   0x11111111, KEYSTONE_LSU_RIO_INT);
+	keystone_rio_interrupt_map(&krio_priv->regs->lsu1_int_route1,
+				   0x11111111, KEYSTONE_LSU_RIO_INT);
+
+	/*
+	 * Error, reset and special event interrupts are routed to RIO
+	 * interrupt dest 0 (Rx/Tx)
+	 */
+	keystone_rio_interrupt_map(&krio_priv->regs->err_rst_evnt_int_route[0],
+				   0x00000111, KEYSTONE_GEN_RIO_INT);
+	keystone_rio_interrupt_map(&krio_priv->regs->err_rst_evnt_int_route[1],
+				   0x00001111, KEYSTONE_GEN_RIO_INT);
+	keystone_rio_interrupt_map(&krio_priv->regs->err_rst_evnt_int_route[2],
+				   0x00000001, KEYSTONE_GEN_RIO_INT);
+
+	/*
+	 * The doorbell interrupts routing table is for the 16 general purpose
+	 * interrupts
+	 */
+	krio_write(krio_priv, interrupt_ctl, 0x1);
+
+	/* Do not use pacing */
+	krio_write(krio_priv, intdst_rate_disable, 0x0000ffff);
+
+	/* Attach interrupt handlers */
+	res = request_irq(krio_priv->board_rio_cfg.rio_irq,
+			  rio_interrupt_handler,
+			  0,
+			  "SRIO",
+			  krio_priv);
+	if (res) {
+		dev_err(krio_priv->dev,
+			"Failed to request RIO irq (%d)\n",
+			krio_priv->board_rio_cfg.rio_irq);
+		return res;
+	}
+
+	res = request_irq(krio_priv->board_rio_cfg.lsu_irq,
+			  lsu_interrupt_handler,
+			  0,
+			  "SRIO LSU",
+			  krio_priv);
+	if (res) {
+		dev_err(krio_priv->dev,
+			"Failed to request LSU irq (%d)\n",
+			krio_priv->board_rio_cfg.lsu_irq);
+		free_irq(krio_priv->board_rio_cfg.rio_irq, krio_priv);
+		return res;
+	}
+
+	return 0;
+}
+
+static void keystone_rio_interrupt_release(struct keystone_rio_data *krio_priv)
+{
+	free_irq(krio_priv->board_rio_cfg.rio_irq, krio_priv);
+	free_irq(krio_priv->board_rio_cfg.lsu_irq, krio_priv);
+}
+
+/*---------------------------- LSU management -------------------------------*/
+
+u8 keystone_rio_lsu_alloc(struct keystone_rio_data *krio_priv)
+{
+	u8 lsu_id = krio_priv->lsu_free++;
+
+	if (krio_priv->lsu_free > krio_priv->lsu_end)
+		krio_priv->lsu_free = krio_priv->lsu_start;
+
+	return lsu_id;
+}
+
+static u32 keystone_rio_lsu_get_cc(u32 lsu_id, u8 ltid, u8 *lcb,
+				   struct keystone_rio_data *krio_priv)
+{
+	u32 idx;
+	u32 shift;
+	u32 value;
+	u32 cc;
+	/*  LSU shadow register status mapping */
+	u32 lsu_index[8] = { 0, 9, 15, 20, 24, 33, 39, 44 };
+
+	/* Compute LSU stat index from LSU id and LTID */
+	idx   = (lsu_index[lsu_id] + ltid) >> 3;
+	shift = ((lsu_index[lsu_id] + ltid) & 0x7) << 2;
+
+	/* Get completion code and context */
+	value  = krio_read(krio_priv, lsu_stat_reg[idx]);
+	cc     = (value >> (shift + 1)) & 0x7;
+	*lcb   = (value >> shift) & 0x1;
+
+	return cc;
+}
+
+/*
+ * Initiate a LSU transfer
+ */
+int keystone_rio_lsu_start_transfer(int lsu,
+				    int port_id,
+				    u16 dest_id,
+				    dma_addr_t src_addr,
+				    u64 tgt_addr,
+				    u32 size_bytes,
+				    int size,
+				    u32 packet_type,
+				    u32 *lsu_context,
+				    int interrupt_req,
+				    struct keystone_rio_data *krio_priv)
+{
+	u32 count;
+	u32 status = 0;
+	u32 res = 0;
+	u8  lcb;
+	u8  ltid;
+
+	if (size_bytes > KEYSTONE_RIO_MAX_DIO_PKT_SIZE)
+		return -EINVAL;
+
+	size_bytes &= (KEYSTONE_RIO_MAX_DIO_PKT_SIZE - 1);
+
+	/* If interrupt mode, do not spin */
+	if (interrupt_req)
+		count = KEYSTONE_RIO_TIMEOUT_CNT;
+	else
+		count = 0;
+
+	/* Check if there is space in the LSU shadow reg and that it is free */
+	while (1) {
+		status = krio_read(krio_priv, lsu_reg[lsu].busy_full);
+		if (((status & KEYSTONE_RIO_LSU_FULL_MASK) == 0x0) &&
+		    ((status & KEYSTONE_RIO_LSU_BUSY_MASK) == 0x0))
+			break;
+
+		count++;
+		if (count >= KEYSTONE_RIO_TIMEOUT_CNT) {
+			dev_err(krio_priv->dev,
+				"no LSU%d shadow register available, status = 0x%x\n",
+				lsu, status);
+			res = -EBUSY;
+			goto out;
+		}
+		ndelay(KEYSTONE_RIO_TIMEOUT_NSEC);
+	}
+
+	/* Get LCB and LTID, LSU reg 6 is already read */
+	lcb  = (status >> 4) & 0x1;
+	ltid = status & 0xf;
+	*lsu_context = status;
+
+	/* LSU Reg 0 - MSB of destination */
+	krio_write(krio_priv, lsu_reg[lsu].addr_msb, (u32)(tgt_addr >> 32));
+
+	/* LSU Reg 1 - LSB of destination */
+	krio_write(krio_priv, lsu_reg[lsu].addr_lsb_cfg_ofs, (u32)tgt_addr);
+
+	/* LSU Reg 2 - source address */
+	krio_write(krio_priv, lsu_reg[lsu].phys_addr, src_addr);
+
+	/* LSU Reg 3 - byte count */
+	krio_write(krio_priv, lsu_reg[lsu].dbell_val_byte_cnt, size_bytes);
+
+	/*
+	 * LSU Reg 4 -
+	 * out port ID = rio.port
+	 * priority = LSU_PRIO
+	 * Xambs = 0
+	 * ID size = 8 or 16 bit
+	 * Dest ID specified as arg
+	 * interrupt request
+	 */
+	krio_write(krio_priv, lsu_reg[lsu].destid,
+		   ((port_id << 8)
+		    | (KEYSTONE_RIO_LSU_PRIO << 4)
+		    | (size ? BIT(10) : 0)
+		    | ((u32)dest_id << 16)
+		    | (interrupt_req & 0x1)));
+
+	/*
+	 * LSU Reg 5 -
+	 * doorbell info = packet_type[16-31],
+	 * hop count = packet_type [8-15]
+	 * FType = packet_type[4-7], TType = packet_type[0-3]
+	 * Writing this register will initiate the transfer
+	 */
+	krio_write(krio_priv, lsu_reg[lsu].dbell_info_fttype, packet_type);
+
+out:
+	return res;
+}
+
+/*
+ * Cancel a LSU transfer
+ */
+static inline void keystone_rio_lsu_cancel_transfer(
+	int lsu,
+	struct keystone_rio_data *krio_priv)
+{
+	u32 status;
+	u32 count = 0;
+
+	while (1) {
+		/* Read register 6 to get the lock */
+		status = krio_read(krio_priv, lsu_reg[lsu].busy_full);
+
+		/* If not busy or if full, we can flush */
+		if (((status & KEYSTONE_RIO_LSU_FULL_MASK) == 0x0) ||
+		    (status & KEYSTONE_RIO_LSU_BUSY_MASK))
+			break;
+
+		count++;
+		if (count >= KEYSTONE_RIO_TIMEOUT_CNT) {
+			dev_err(krio_priv->dev,
+				"no LSU%d shadow register available for flushing\n",
+				lsu);
+			return;
+		}
+
+		ndelay(KEYSTONE_RIO_TIMEOUT_NSEC);
+	}
+
+	if (status & KEYSTONE_RIO_LSU_FULL_MASK) {
+		/* Flush the transaction with our privID */
+		krio_write(krio_priv, lsu_reg[lsu].busy_full,
+			   BIT(0) | (8 << 28));
+	} else {
+		/* Flush the transaction with our privID and CBusy bit */
+		krio_write(krio_priv, lsu_reg[lsu].busy_full,
+			   BIT(0) | (8 << 28) | BIT(27));
+	}
+}
+
+/*
+ * Complete a LSU transfer
+ */
+int keystone_rio_lsu_complete_transfer(int lsu,
+				       u32 lsu_context,
+				       struct keystone_rio_data *krio_priv)
+{
+	u32 status = 0;
+	u32 res = 0;
+	u8  lcb = (lsu_context >> 4) & 0x1;
+	u8  ltid = (lsu_context) & 0xf;
+	u8  n_lcb;
+
+	/* Retrieve our completion code */
+	status = keystone_rio_lsu_get_cc(lsu, ltid, &n_lcb, krio_priv);
+	if (n_lcb != lcb)
+		return -EAGAIN;
+
+	if (unlikely(status))
+		dev_dbg(krio_priv->dev, "LSU%d status 0x%x\n", lsu, status);
+
+	switch (status & KEYSTONE_RIO_LSU_CC_MASK) {
+	case KEYSTONE_RIO_LSU_CC_TIMEOUT:
+		res = -ETIMEDOUT;
+		keystone_rio_lsu_cancel_transfer(lsu, krio_priv);
+		break;
+	case KEYSTONE_RIO_LSU_CC_XOFF:
+	case KEYSTONE_RIO_LSU_CC_ERROR:
+	case KEYSTONE_RIO_LSU_CC_INVALID:
+	case KEYSTONE_RIO_LSU_CC_DMA:
+		res = -EIO;
+		keystone_rio_lsu_cancel_transfer(lsu, krio_priv);
+		break;
+	case KEYSTONE_RIO_LSU_CC_RETRY:
+		res = -EBUSY;
+		break;
+	case KEYSTONE_RIO_LSU_CC_CANCELED:
+		res = -EIO;
+		break;
+	default:
+		break;
+	}
+
+	return res;
+}
+
+#ifdef CONFIG_RAPIDIO_DMA_ENGINE
+static int keystone_rio_lsu_dma_allocate_channel(struct rio_mport *mport)
+{
+	struct keystone_rio_data *krio_priv = mport->priv;
+	struct dma_chan *dchan;
+
+	dchan = rio_request_mport_dma(mport);
+	if (!dchan) {
+		dev_err(krio_priv->dev,
+			"cannot find DMA channel for port %d\n",
+			mport->index);
+		return -ENODEV;
+	}
+
+	dev_dbg(krio_priv->dev, "get channel 0x%p for port %d\n",
+		dchan, mport->index);
+
+	krio_priv->dma_chan[mport->index] = dchan;
+
+	return 0;
+}
+
+static void keystone_rio_lsu_dma_free_channel(struct rio_mport *mport)
+{
+	struct keystone_rio_data *krio_priv = mport->priv;
+	struct dma_chan *dchan = krio_priv->dma_chan[mport->index];
+
+	if (dchan) {
+		struct keystone_rio_dma_chan *chan = from_dma_chan(dchan);
+
+		/* Remove from global list */
+		list_del_init(&chan->node);
+
+		rio_release_dma(dchan);
+		krio_priv->dma_chan[mport->index] = NULL;
+	}
+}
+#endif /* CONFIG_RAPIDIO_DMA_ENGINE */
+
+/*----------------------------- Doorbell management --------------------------*/
+
+static inline int dbell_get(u32 *pending)
+{
+	if (*pending) {
+		int n = __ffs(*pending);
+		*pending &= ~BIT(n);
+		return n;
+	}
+
+	return -1;
+}
+
+static inline void dbell_call_handler(u32 dbell_num,
+				      struct keystone_rio_data *krio_priv)
+{
+	struct rio_dbell *dbell;
+	int i;
+	int found = 0;
+
+	for (i = 0; i < KEYSTONE_RIO_MAX_PORT; i++) {
+		if (krio_priv->mport[i]) {
+			struct rio_mport *mport = krio_priv->mport[i];
+
+			list_for_each_entry(dbell, &mport->dbells, node) {
+				if ((dbell->res->start <= dbell_num) &&
+				    (dbell->res->end   >= dbell_num)) {
+					found = 1;
+					break;
+				}
+			}
+
+			if (found && dbell->dinb) {
+				/* Call the registered handler if any */
+				dbell->dinb(mport,
+					    dbell->dev_id,
+					    -1, /* unknown source Id */
+					    mport->host_deviceid,
+					    dbell_num);
+				break;
+			}
+		}
+	}
+
+	if (!found)
+		dev_dbg(krio_priv->dev,
+			"DBELL: received spurious doorbell %d\n",
+			dbell_num);
+}
+
+static void dbell_handler(struct keystone_rio_data *krio_priv)
+{
+	u32 pending_dbell;
+	unsigned int i;
+
+	for (i = 0; i < KEYSTONE_RIO_DBELL_NUMBER; i++) {
+		pending_dbell = krio_read(krio_priv, doorbell_int[i].status);
+		if (pending_dbell)
+			/* Acknowledge the interrupts for these doorbells */
+			krio_write(krio_priv, doorbell_int[i].clear,
+				   pending_dbell);
+
+		while (pending_dbell) {
+			u32 dbell_num = dbell_get(&pending_dbell) + (i << 4);
+
+			/* Call the registered dbell handler(s) */
+			dbell_call_handler(dbell_num, krio_priv);
+		}
+	}
+}
+
+static int keystone_rio_dbell_send(struct rio_mport *mport,
+				   int index,
+				   u16 dest_id,
+				   u16 num)
+{
+#ifdef CONFIG_RAPIDIO_DMA_ENGINE
+
+	struct keystone_rio_data *krio_priv = mport->priv;
+	u32 lsu_context;
+	u32 count = 0;
+	int res;
+
+	/* Transform doorbell number into info field */
+	u16 info   = (num & 0xf) | (((num >> 4) & 0x3) << 5);
+	u32 packet_type = ((info & 0xffff) << 16)
+				| KEYSTONE_RIO_PACKET_TYPE_DBELL;
+
+	mutex_lock(&krio_priv->lsu_lock_maint);
+	res = keystone_rio_lsu_start_transfer(krio_priv->lsu_maint,
+					      mport->index,
+					      dest_id,
+					      0,
+					      0,
+					      0,
+					      mport->sys_size,
+					      packet_type,
+					      &lsu_context,
+					      1,
+					      krio_priv);
+	if (res) {
+		mutex_unlock(&krio_priv->lsu_lock_maint);
+		dev_err(krio_priv->dev, "DIO: send doorbell error %d\n", res);
+		return res;
+	}
+
+	while (1) {
+		res = keystone_rio_lsu_complete_transfer(krio_priv->lsu_maint,
+							 lsu_context,
+							 krio_priv);
+		if (res != -EAGAIN)
+			break;
+
+		count++;
+		if (count >= KEYSTONE_RIO_TIMEOUT_CNT) {
+			res = -EIO;
+			break;
+		}
+
+		ndelay(KEYSTONE_RIO_TIMEOUT_NSEC);
+	}
+
+	mutex_unlock(&krio_priv->lsu_lock_maint);
+
+	dev_info(krio_priv->dev, "DIO: send doorbell complete %u\n", count);
+	return res;
+#else /* CONFIG_RAPIDIO_DMA_ENGINE */
+
+	return -ENOTSUPP;
+
+#endif /* CONFIG_RAPIDIO_DMA_ENGINE */
+}
+
+/*---------------------- Maintenance Request Management  ---------------------*/
+
+/**
+ * keystone_rio_maint_request - Perform a maintenance request
+ * @port_id: output port ID
+ * @dest_id: destination ID of target device
+ * @hopcount: hopcount for this request
+ * @offset: offset in the RapidIO configuration space
+ * @buff: dma address of the data on the host
+ * @buff_len: length of the data
+ * @size: 1 for 16bit, 0 for 8bit ID size
+ * @type: packet type
+ *
+ * Returns %0 on success or %-EINVAL, %-EIO, %-EAGAIN or %-EBUSY on failure.
+ */
+static inline int keystone_rio_maint_request(
+	int port_id,
+	u32 dest_id,
+	u8  hopcount,
+	u32 offset,
+	dma_addr_t buff,
+	int buff_len,
+	u16 size,
+	u16 packet_type,
+	struct keystone_rio_data *krio_priv)
+{
+	int res;
+	u32 lsu_context;
+	u32 count = 0;
+	u32 type = ((hopcount & 0xff) << 8) | (packet_type & 0xff);
+
+	mutex_lock(&krio_priv->lsu_lock_maint);
+
+	res = keystone_rio_lsu_start_transfer(krio_priv->lsu_maint,
+					      port_id,
+					      dest_id,
+					      buff,
+					      offset,
+					      buff_len,
+					      size,
+					      type,
+					      &lsu_context,
+					      0,
+					      krio_priv);
+	if (res) {
+		mutex_unlock(&krio_priv->lsu_lock_maint);
+		dev_err(krio_priv->dev, "maintenance packet transfer error\n");
+		return res;
+	}
+
+	while (1) {
+		res = keystone_rio_lsu_complete_transfer(krio_priv->lsu_maint,
+							 lsu_context,
+							 krio_priv);
+		if (res != -EAGAIN)
+			break;
+
+		count++;
+		if (count >= KEYSTONE_RIO_TIMEOUT_CNT) {
+			res = -EIO;
+			break;
+		}
+
+		ndelay(KEYSTONE_RIO_TIMEOUT_NSEC);
+	}
+
+	mutex_unlock(&krio_priv->lsu_lock_maint);
+
+	return res;
+}
+
+static int keystone_rio_maint_read(struct keystone_rio_data *krio_priv,
+				   int port_id,
+				   u16 destid,
+				   u16 size,
+				   u8  hopcount,
+				   u32 offset,
+				   int len,
+				   u32 *val)
+{
+	u32 *tbuf;
+	int res;
+	dma_addr_t dma;
+	struct device *dev = krio_priv->dev;
+	size_t align_len = L1_CACHE_ALIGN(len);
+
+	tbuf = kzalloc(align_len, GFP_KERNEL | GFP_DMA);
+	if (!tbuf)
+		return -ENOMEM;
+
+	dma = dma_map_single(dev, tbuf, len, DMA_FROM_DEVICE);
+
+	res = keystone_rio_maint_request(port_id,
+					 destid,
+					 hopcount,
+					 offset,
+					 dma,
+					 len,
+					 size,
+					 KEYSTONE_RIO_PACKET_TYPE_MAINT_R,
+					 krio_priv);
+
+	dma_unmap_single(dev, dma, len, DMA_FROM_DEVICE);
+
+	/* Taking care of byteswap */
+	switch (len) {
+	case 1:
+		*val = *((u8 *)tbuf);
+		break;
+	case 2:
+		*val = ntohs(*((u16 *)tbuf));
+		break;
+	default:
+		*val = ntohl(*((u32 *)tbuf));
+		break;
+	}
+
+	dev_dbg(dev,
+		"maint_r: index %d destid %d hopcount %d offset 0x%x len %d val 0x%x res %d\n",
+		port_id, destid, hopcount, offset, len, *val, res);
+
+	kfree(tbuf);
+
+	return res;
+}
+
+static int keystone_rio_maint_write(struct keystone_rio_data *krio_priv,
+				    int port_id,
+				    u16 destid,
+				    u16 size,
+				    u8  hopcount,
+				    u32 offset,
+				    int len,
+				    u32 val)
+{
+	u32 *tbuf;
+	int res;
+	dma_addr_t dma;
+	struct device *dev = krio_priv->dev;
+	size_t align_len = L1_CACHE_ALIGN(len);
+
+	tbuf = kzalloc(align_len, GFP_KERNEL | GFP_DMA);
+	if (!tbuf)
+		return -ENOMEM;
+
+	/* Taking care of byteswap */
+	switch (len) {
+	case 1:
+		*tbuf = ((u8)val);
+		break;
+	case 2:
+		*tbuf = htons((u16)val);
+		break;
+	default:
+		*tbuf = htonl((u32)val);
+		break;
+	}
+
+	dma = dma_map_single(dev, tbuf, len, DMA_TO_DEVICE);
+
+	res = keystone_rio_maint_request(port_id,
+					 destid,
+					 hopcount,
+					 offset,
+					 dma,
+					 len,
+					 size,
+					 KEYSTONE_RIO_PACKET_TYPE_MAINT_W,
+					 krio_priv);
+
+	dma_unmap_single(dev, dma, len, DMA_TO_DEVICE);
+
+	dev_dbg(dev,
+		"maint_w: index %d destid %d hopcount %d offset 0x%x len %d val 0x%x res %d\n",
+		port_id, destid, hopcount, offset, len, val, res);
+
+	kfree(tbuf);
+
+	return res;
+}
+
+/*------------------------- RapidIO hw controller setup ---------------------*/
+
+/* Retrieve the corresponding lanes bitmask from ports bitmask and path_mode */
+static int keystone_rio_get_lane_config(u32 ports, u32 path_mode)
+{
+	u32 lanes = 0;
+
+	while (ports) {
+		u32 lane;
+		u32 port = __ffs(ports);
+
+		ports &= ~BIT(port);
+
+		if (keystone_lane_configs[path_mode][port].start == -1)
+			return -1;
+
+		for (lane = keystone_lane_configs[path_mode][port].start;
+		     lane < keystone_lane_configs[path_mode][port].end;
+		     lane++) {
+			lanes |= KEYSTONE_SERDES_LANE(lane);
+		}
+	}
+
+	return (int)lanes;
+}
+
+/**
+ * keystone_rio_lanes_init_and_wait - Initialize and wait lanes for a given
+ * RIO port
+ *
+ * @port: RIO port
+ * @start: if non null, lanes will be started
+ * @init_rx: if non null, lanes Rx coefficients will be applied
+ *
+ * Returns %0 on success or %1 if lane is not OK during the expected timeout
+ */
+static int keystone_rio_lanes_init_and_wait(u32 port, int start,
+					    struct keystone_rio_data *krio_priv)
+{
+	u32 path_mode = krio_priv->board_rio_cfg.path_mode;
+	int lanes     = keystone_rio_get_lane_config(BIT(port), path_mode);
+	int res;
+
+	dev_dbg(krio_priv->dev,
+		"initializing lane mask 0x%x for port %d",
+		lanes, port);
+
+	/* Eventually start the lane */
+	if (start) {
+		dev_dbg(krio_priv->dev,
+			"starting lane mask 0x%x for port %d",
+			lanes, port);
+
+		krio_priv->serdes.ops->start_tx_lanes((u32)lanes,
+						      &krio_priv->serdes);
+	}
+
+	/* Wait lanes to be OK */
+	res = krio_priv->serdes.ops->wait_lanes_ok(lanes,
+						   &krio_priv->serdes);
+	if (res < 0) {
+		dev_dbg(krio_priv->dev,
+			"port %d lane mask 0x%x is not OK\n",
+			port, lanes);
+
+		return 1;
+	}
+
+	return 0;
+}
+
+/*
+ * SerDes main configuration
+ */
+static int keystone_rio_serdes_init(u32 baud, int calibrate,
+				    struct keystone_rio_data *krio_priv)
+{
+	u32 path_mode = krio_priv->board_rio_cfg.path_mode;
+	u32 ports = krio_priv->board_rio_cfg.ports;
+	u32 lanes;
+	int res;
+
+	/* Retrieve lane termination */
+	res = keystone_rio_get_lane_config(ports, path_mode);
+	if (res <= 0)
+		return res;
+
+	/* Initialize SerDes */
+	lanes = (u32)res;
+	krio_priv->board_rio_cfg.lanes = lanes;
+
+	res = krio_priv->serdes.ops->config_lanes(lanes,
+						  baud,
+						  &krio_priv->serdes);
+	if (res < 0)
+		return res;
+
+	/* Check if we need to calibrate SerDes */
+	if (calibrate && !krio_priv->calibrating) {
+		krio_priv->calibrating = 1;
+
+		/* Set calibration timeout */
+		krio_priv->board_rio_cfg.serdes_config.cal_timeout =
+			krio_priv->board_rio_cfg.port_register_timeout;
+
+		/* Do the calibration */
+		krio_priv->serdes.ops->calibrate_lanes(lanes,
+						       &krio_priv->serdes);
+
+		krio_priv->calibrating = 0;
+	}
+
+	return 0;
+}
+
+/**
+ * keystone_rio_hw_init - Configure a RapidIO controller
+ * @baud: serdes baudrate
+ *
+ * Returns %0 on success or %-EINVAL or %-EIO on failure.
+ */
+static int keystone_rio_hw_init(u32 baud, struct keystone_rio_data *krio_priv)
+{
+	struct keystone_serdes_config *serdes_config =
+			&krio_priv->board_rio_cfg.serdes_config;
+	u32 lsu_mask = 0, val, block, port;
+	int res = 0, i;
+
+	/* Reset blocks */
+	krio_write(krio_priv, gbl_en, 0);
+	for (block = 0; block < KEYSTONE_RIO_BLK_NUM; block++) {
+		krio_write(krio_priv, blk[block].enable, 0);
+		while (krio_read(krio_priv, blk[block].status) & 0x1)
+			usleep_range(10, 50);
+	}
+
+	ndelay(KEYSTONE_RIO_TIMEOUT_NSEC);
+
+	/* Set SRIO out of reset */
+	krio_write(krio_priv, pcr,
+		   KEYSTONE_RIO_PER_RESTORE | KEYSTONE_RIO_PER_FREE);
+
+	/* Clear BOOT_COMPLETE bit (allowing write) */
+	krio_write(krio_priv, per_set_cntl, 0);
+
+	/* Set LSU timeout count to zero */
+	krio_write(krio_priv, lsu_setup_reg[1], 0x000000ff);
+
+	/* Enable blocks */
+	krio_write(krio_priv, gbl_en, 1);
+	for (block = 0; block < KEYSTONE_RIO_BLK_NUM; block++)
+		krio_write(krio_priv, blk[block].enable, 1);
+
+	/* Set control register 1 configuration */
+	krio_write(krio_priv, per_set_cntl1, 0);
+
+	/* Set control register */
+	krio_write(krio_priv, per_set_cntl,
+		   0x0009c000 | (krio_priv->board_rio_cfg.pkt_forwarding ?
+				 BIT(21) | BIT(8) : 0));
+
+	/* Initialize SerDes and eventually perform their calibration */
+	res = keystone_rio_serdes_init(
+		baud,
+		krio_priv->board_rio_cfg.serdes_calibration,
+		krio_priv);
+
+	if (res < 0) {
+		dev_err(krio_priv->dev, "initialization of SerDes failed\n");
+		return res;
+	}
+
+	/* Set prescalar for ip_clk */
+	krio_lnk_write(krio_priv, prescalar_srv_clk,
+		       serdes_config->prescalar_srv_clk);
+
+	/* Peripheral-specific configuration and capabilities */
+	krio_car_csr_write(krio_priv, dev_id, KEYSTONE_RIO_DEV_ID_VAL);
+	krio_car_csr_write(krio_priv, dev_info, KEYSTONE_RIO_DEV_INFO_VAL);
+	krio_car_csr_write(krio_priv, assembly_id, KEYSTONE_RIO_ID_TI);
+	krio_car_csr_write(krio_priv, assembly_info, KEYSTONE_RIO_EXT_FEAT_PTR);
+	krio_car_csr_write(krio_priv, base_dev_id, krio_priv->base_dev_id);
+
+	krio_priv->rio_pe_feat = RIO_PEF_PROCESSOR
+				 | RIO_PEF_CTLS
+				 | KEYSTONE_RIO_PEF_FLOW_CONTROL
+				 | RIO_PEF_EXT_FEATURES
+				 | RIO_PEF_ADDR_34
+				 | RIO_PEF_STD_RT
+				 | RIO_PEF_INB_DOORBELL
+				 | RIO_PEF_INB_MBOX;
+
+	krio_car_csr_write(krio_priv, pe_feature, krio_priv->rio_pe_feat);
+	krio_car_csr_write(krio_priv, sw_port, KEYSTONE_RIO_MAX_PORT << 8);
+
+	krio_car_csr_write(krio_priv, src_op,
+			   RIO_SRC_OPS_READ
+			   | RIO_SRC_OPS_WRITE
+			   | RIO_SRC_OPS_STREAM_WRITE
+			   | RIO_SRC_OPS_WRITE_RESPONSE
+			   | RIO_SRC_OPS_DATA_MSG
+			   | RIO_SRC_OPS_DOORBELL
+			   | RIO_SRC_OPS_ATOMIC_TST_SWP
+			   | RIO_SRC_OPS_ATOMIC_INC
+			   | RIO_SRC_OPS_ATOMIC_DEC
+			   | RIO_SRC_OPS_ATOMIC_SET
+			   | RIO_SRC_OPS_ATOMIC_CLR
+			   | RIO_SRC_OPS_PORT_WRITE);
+
+	krio_car_csr_write(krio_priv, dest_op,
+			   RIO_DST_OPS_READ
+			   | RIO_DST_OPS_WRITE
+			   | RIO_DST_OPS_STREAM_WRITE
+			   | RIO_DST_OPS_WRITE_RESPONSE
+			   | RIO_DST_OPS_DATA_MSG
+			   | RIO_DST_OPS_DOORBELL
+			   | RIO_DST_OPS_PORT_WRITE);
+
+	krio_car_csr_write(krio_priv, pe_logical_ctl, RIO_PELL_ADDR_34);
+
+	val = (((KEYSTONE_RIO_SP_HDR_NEXT_BLK_PTR & 0xffff) << 16) |
+	       KEYSTONE_RIO_SP_HDR_EP_REC_ID);
+	krio_sp_write(krio_priv, sp_maint_blk_hdr, val);
+
+	/* Clear high bits of local config space base addr */
+	krio_car_csr_write(krio_priv, local_cfg_hbar, 0);
+
+	/* Set local config space base addr */
+	krio_car_csr_write(krio_priv, local_cfg_bar, 0x00520000);
+
+	/* Enable HOST BIT(31) & MASTER_ENABLE BIT(30) bits */
+	krio_sp_write(krio_priv, sp_gen_ctl, 0xc0000000);
+
+	/* Set link timeout value */
+	krio_sp_write(krio_priv, sp_link_timeout_ctl, 0x000FFF00);
+
+	/* Set response timeout value */
+	krio_sp_write(krio_priv, sp_rsp_timeout_ctl, 0x01000000);
+
+	/* Allows SELF_RESET and PWDN_PORT resets to clear stcky reg bits */
+	krio_lnk_write(krio_priv, reg_rst_ctl, 0x00000001);
+
+	/* Clear all errors */
+	krio_err_write(krio_priv, err_det, 0);
+	krio_lnk_write(krio_priv, local_err_det, 0);
+
+	/* Set error detection */
+	if (krio_priv->board_rio_cfg.pkt_forwarding) {
+		/* Disable all error detection if using packet forwarding */
+		krio_lnk_write(krio_priv, local_err_en, 0);
+		krio_err_write(krio_priv, err_en, 0);
+	} else {
+		/* Enable logical layer error detection */
+		krio_err_write(krio_priv, err_en, BIT(24) | BIT(25) | BIT(31));
+		krio_lnk_write(krio_priv, local_err_en, BIT(22) | BIT(26));
+	}
+
+	/* Set err det block header */
+	val = (((KEYSTONE_RIO_ERR_HDR_NEXT_BLK_PTR & 0xffff) << 16) |
+	       KEYSTONE_RIO_ERR_EXT_FEAT_ID);
+	krio_err_write(krio_priv, err_report_blk_hdr, val);
+
+	/* Clear msb of err captured addr reg */
+	krio_err_write(krio_priv, h_addr_capt, 0);
+
+	/* Clear lsb of err captured addr reg */
+	krio_err_write(krio_priv, addr_capt, 0);
+
+	/* Clear err captured source and dest DevID reg */
+	krio_err_write(krio_priv, id_capt, 0);
+
+	/* Clear err captured packet info */
+	krio_err_write(krio_priv, ctrl_capt, 0);
+
+	/* Set per port information */
+	for (port = 0; port < KEYSTONE_RIO_MAX_PORT; port++) {
+		krio_phy_write(krio_priv, phy_sp[port].__rsvd[3], 0x41004141);
+
+		/* Set the baud rate to the port information */
+		val = krio_sp_read(krio_priv, sp[port].ctl2);
+		val |= BIT(24 - (baud << 1));
+		krio_sp_write(krio_priv, sp[port].ctl2, val);
+	}
+
+	/* Disable LSU to perform LSU configuration */
+	krio_write(krio_priv, blk[KEYSTONE_RIO_BLK_LSU_ID].enable, 0);
+	while (krio_read(krio_priv, blk[KEYSTONE_RIO_BLK_LSU_ID].status) & 0x1)
+		usleep_range(10, 50);
+
+	/* Set the SRIO shadow registers configuration to 4/4/4/4 */
+	krio_write(krio_priv, lsu_setup_reg[0], 0);
+
+	/* Use LSU completion interrupt per LSU (not per SRCID) */
+	for (i = krio_priv->lsu_start; i <= krio_priv->lsu_end; i++)
+		lsu_mask |= BIT(i);
+
+	krio_write(krio_priv, lsu_setup_reg[1], lsu_mask);
+
+	/* Enable LSU */
+	krio_write(krio_priv, blk[KEYSTONE_RIO_BLK_LSU_ID].enable, 1);
+
+	/* Global port-write generation */
+	if (krio_priv->board_rio_cfg.pkt_forwarding) {
+		/*
+		 * Disable generation of port-write requests if using
+		 * packet forwarding
+		 */
+		val = krio_ev_read(krio_priv, evt_mgmt_port_wr_enable);
+		krio_ev_write(krio_priv, evt_mgmt_port_wr_enable,
+			      val & ~(BIT(8) | BIT(28))); /* LOG | LOCALOG */
+
+		val = krio_ev_read(krio_priv, evt_mgmt_dev_port_wr_en);
+		krio_ev_write(krio_priv, evt_mgmt_dev_port_wr_en,
+			      val & ~BIT(0)); /* PW_EN */
+	} else {
+		/*
+		 * Enable generation of port-write requests
+		 */
+		val = krio_ev_read(krio_priv, evt_mgmt_port_wr_enable);
+		krio_ev_write(krio_priv, evt_mgmt_port_wr_enable,
+			      val | BIT(8) | BIT(28)); /* LOG | LOCALOG */
+
+		val = krio_ev_read(krio_priv, evt_mgmt_dev_port_wr_en);
+		krio_ev_write(krio_priv, evt_mgmt_dev_port_wr_en,
+			      val  | BIT(0)); /* PW_EN */
+	}
+
+	/* Set packet forwarding */
+	for (i = 0; i < KEYSTONE_RIO_MAX_PKT_FW_ENTRIES; i++) {
+		if ((krio_priv->board_rio_cfg.pkt_forwarding) && (i < 8)) {
+			struct keystone_routing_config *routing =
+				krio_priv->board_rio_cfg.routing_config;
+
+			/*
+			 * Enable packet forwarding DevId and port as defined
+			 * in DTS
+			 */
+			krio_write(krio_priv, pkt_fwd_cntl[i].pf_16b,
+				   routing[i].dev_id_low
+				   | routing[i].dev_id_high << 16);
+
+			krio_write(krio_priv, pkt_fwd_cntl[i].pf_8b,
+				   (routing[i].dev_id_low & 0xff)
+				   | (routing[i].dev_id_high & 0xff) << 8
+				   | routing[i].port << 16);
+
+			dev_info(krio_priv->dev,
+				 "enabling packet forwarding to port %d for DestID 0x%04x - 0x%04x\n",
+				 routing[i].port, routing[i].dev_id_low,
+				 routing[i].dev_id_high);
+		} else {
+			/* Disable packet forwarding */
+			krio_write(krio_priv, pkt_fwd_cntl[i].pf_16b,
+				   0xffffffff);
+			krio_write(krio_priv, pkt_fwd_cntl[i].pf_8b,
+				   0xffffffff);
+		}
+	}
+
+	if (!krio_priv->board_rio_cfg.pkt_forwarding)
+		dev_info(krio_priv->dev, "packet forwarding disabled\n");
+
+	/* Force all writes to finish */
+	val = krio_err_read(krio_priv, ctrl_capt);
+
+	return res;
+}
+
+/**
+ * keystone_rio_start - Start RapidIO controller
+ */
+static void keystone_rio_start(struct keystone_rio_data *krio_priv)
+{
+	u32 val;
+
+	/* Set PEREN bit to enable logical layer data flow */
+	krio_write(krio_priv, pcr, KEYSTONE_RIO_PER_EN | KEYSTONE_RIO_PER_FREE);
+
+	/* Set BOOT_COMPLETE bit */
+	val = krio_read(krio_priv, per_set_cntl);
+	krio_write(krio_priv, per_set_cntl, val | KEYSTONE_RIO_BOOT_COMPLETE);
+}
+
+static void keystone_rio_reset_dpc(struct work_struct *work)
+{
+	struct keystone_rio_data *krio_priv =
+		container_of(work, struct keystone_rio_data, reset_work);
+	u32 ports_rst;
+	u32 ports;
+	u32 port;
+
+	if (krio_priv->started == 0)
+		return;
+
+	ports_rst = krio_ev_read(krio_priv, evt_mgmt_rst_port_stat);
+
+	dev_dbg(krio_priv->dev,
+		"reset device request received on ports: 0x%x\n", ports_rst);
+
+	/* Acknowledge reset */
+	ports = ports_rst;
+	while (ports) {
+		port = __ffs(ports);
+		ports &= ~BIT(port);
+		krio_phy_write(krio_priv, phy_sp[port].status,
+			       KEYSTONE_RIO_PORT_PLM_STATUS_RST_REQ);
+	}
+
+	krio_ev_write(krio_priv, evt_mgmt_rst_port_stat, ports_rst);
+
+	/* Reinitialize SRIO peripheral */
+	keystone_rio_shutdown_controller(krio_priv);
+	keystone_rio_setup_controller(krio_priv);
+}
+
+/**
+ * keystone_rio_stop - Stop RapidIO controller
+ */
+static void keystone_rio_stop(struct keystone_rio_data *krio_priv)
+{
+	u32 val;
+
+	/* Disable PEREN bit to stop all new logical layer transactions */
+	val = krio_read(krio_priv, pcr);
+	krio_write(krio_priv, pcr, val & ~KEYSTONE_RIO_PER_EN);
+
+	/* Clear BOOT_COMPLETE bit */
+	val = krio_read(krio_priv, per_set_cntl);
+	krio_write(krio_priv, per_set_cntl, val & ~KEYSTONE_RIO_BOOT_COMPLETE);
+}
+
+static void keystone_rio_handle_logical_error(
+	struct keystone_rio_data *krio_priv)
+{
+	u32 err_det = krio_err_read(krio_priv, err_det);
+
+	while (err_det) {
+		u32 err = __ffs(err_det);
+		u32 val;
+
+		err_det &= ~BIT(err);
+
+		/* Acknowledge logical layer error */
+		val = krio_err_read(krio_priv, err_det);
+		krio_err_write(krio_priv, err_det, val & ~BIT(err));
+
+		dev_dbg(krio_priv->dev,
+			"logical layer error %d detected\n", err);
+
+		/* Acknowledge local logical layer error as well if needed */
+		if ((err == 22) || (err == 26)) {
+			val = krio_lnk_read(krio_priv, local_err_det);
+			krio_lnk_write(krio_priv, local_err_det,
+				       val & ~BIT(err));
+		}
+	}
+}
+
+static int keystone_rio_get_remote_port(
+	u8 port,
+	struct keystone_rio_data *krio_priv)
+{
+	int res;
+	u32 value;
+
+	res = keystone_rio_maint_read(krio_priv, port, 0xffff,
+				      krio_priv->board_rio_cfg.size,
+				      0, RIO_SWP_INFO_CAR, 4, &value);
+	if (res < 0)
+		return res;
+
+	return RIO_GET_PORT_NUM(value);
+}
+
+static int keystone_rio_port_sync_ackid(u32 port,
+					struct keystone_rio_data *krio_priv)
+{
+	u32 lm_resp, ackid_stat, l_ackid, r_ackid;
+	int i = 0;
+
+	/*
+	 * Clear valid bit in maintenance response register.
+	 * Send both Input-Status Link-Request and PNA control symbols and
+	 * wait for valid maintenance response
+	 */
+	krio_sp_read(krio_priv, sp[port].link_maint_resp);
+	krio_phy_write(krio_priv, phy_sp[port].long_cs_tx1, 0x2003f044);
+
+	do {
+		if (++i > KEYSTONE_RIO_TIMEOUT_CNT) {
+			dev_err(krio_priv->dev,
+				"port %d: Input-Status response timeout\n",
+				port);
+			return -1;
+		}
+		ndelay(KEYSTONE_RIO_TIMEOUT_NSEC);
+		lm_resp = krio_sp_read(krio_priv, sp[port].link_maint_resp);
+	} while (!(lm_resp & RIO_PORT_N_MNT_RSP_RVAL));
+
+	dev_dbg(krio_priv->dev,
+		"port %d: Input-Status response = 0x%08x\n", port, lm_resp);
+
+	/* Set outbound ackID to the value expected by link partner */
+	ackid_stat = krio_sp_read(krio_priv, sp[port].ackid_stat);
+	dev_dbg(krio_priv->dev, "port %d: ackid_stat = 0x%08x\n",
+		port, ackid_stat);
+
+	l_ackid = (ackid_stat & RIO_PORT_N_ACK_INBOUND) >> 24;
+	r_ackid = (lm_resp & RIO_PORT_N_MNT_RSP_ASTAT) >> 5;
+	krio_sp_write(krio_priv, sp[port].ackid_stat, l_ackid << 24 | r_ackid);
+
+	return (int)l_ackid;
+}
+
+static int keystone_rio_port_error_recovery(u32 port,
+					    struct keystone_rio_data *krio_priv)
+{
+	int res;
+	u32 err_stat;
+	u32 err_det;
+	u32 plm_status;
+	int r_port = krio_priv->board_rio_cfg.ports_remote[port];
+
+	if (unlikely(port >= KEYSTONE_RIO_MAX_PORT))
+		return -EINVAL;
+
+	err_stat   = krio_sp_read(krio_priv, sp[port].err_stat);
+	err_det    = krio_err_read(krio_priv, sp_err[port].det);
+	plm_status = krio_phy_read(krio_priv, phy_sp[port].status);
+
+	dev_dbg(krio_priv->dev,
+		"ER port %d: err_stat = 0x%08x, err_det = 0x%08x, plm_status = 0x%08x\n",
+		port, err_stat, err_det, plm_status);
+
+	if (unlikely(!(err_stat & RIO_PORT_N_ERR_STS_PORT_OK))) {
+		dev_dbg(krio_priv->dev,
+			"ER port %d not initialized - PORT_OK not set\n", port);
+		return -EINVAL;
+	}
+
+	/* Acknowledge errors on this port */
+	krio_sp_write(krio_priv, sp[port].err_stat,
+		      err_stat & KEYSTONE_RIO_PORT_ERROR_MASK);
+	krio_err_write(krio_priv, sp_err[port].det, 0);
+	krio_phy_write(krio_priv, phy_sp[port].status,
+		       plm_status & KEYSTONE_RIO_PORT_PLM_STATUS_ERRORS);
+
+	if (err_stat & RIO_PORT_N_ERR_STS_PW_OUT_ES) {
+		u32 ackid_stat, l_ackid, r_ackid;
+
+		/* Sync ackID */
+		res = keystone_rio_port_sync_ackid(port, krio_priv);
+		if (res == -1)
+			goto oes_rd_err;
+
+		l_ackid = (u32)res;
+
+		/*
+		 * We do not know the remote port but we may be lucky where
+		 * ackId did not changed...
+		 */
+		if (r_port < 0) {
+			dev_dbg(krio_priv->dev,
+				"ER port %d: remote port not yet detected!\n",
+				port);
+			return -EINVAL;
+		}
+
+		/* range values are by experiment */
+		usleep_range(50, 200);
+
+		/*
+		 * Reread outbound ackID as it may have changed as a result of
+		 * outstanding unacknowledged packets retransmission
+		 */
+		ackid_stat = krio_sp_read(krio_priv, sp[port].ackid_stat);
+		dev_dbg(krio_priv->dev, "ER port %d: ackid_stat = 0x%08x\n",
+			port, ackid_stat);
+
+		r_ackid = ackid_stat & RIO_PORT_N_ACK_OUTBOUND;
+
+		/*
+		 * Set link partner inbound ackID to outbound ackID + 1.
+		 * Set link partner outbound and outstanding ackID to inbound
+		 * ackID.
+		 */
+		res = keystone_rio_maint_write(
+			krio_priv,
+			port,
+			0xffff,
+			krio_priv->board_rio_cfg.size,
+			0,
+			0x100 + RIO_PORT_N_ACK_STS_CSR(r_port),
+			sizeof(u32),
+			((++r_ackid << 24) & RIO_PORT_N_ACK_INBOUND) |
+			(l_ackid << 8) | l_ackid);
+
+		if (res < 0) {
+			dev_dbg(krio_priv->dev,
+				"ER port %d: failed to align ackIDs with link partner port %d\n",
+				port, r_port);
+		}
+	}
+
+oes_rd_err:
+
+	if (err_stat & RIO_PORT_N_ERR_STS_PW_INP_ES) {
+		if (r_port < 0) {
+			dev_dbg(krio_priv->dev,
+				"ER port %d: remote port not yet detected!\n",
+				port);
+			return -EINVAL;
+		}
+
+		dev_dbg(krio_priv->dev,
+			"ER port %d: Input Error-Stopped recovery\n", port);
+
+		res = keystone_rio_maint_write(
+			krio_priv,
+			port,
+			0xffff,
+			krio_priv->board_rio_cfg.size,
+			0,
+			0x100 + RIO_PORT_N_MNT_REQ_CSR(r_port),
+			sizeof(u32),
+			RIO_MNT_REQ_CMD_IS);
+
+		if (res < 0) {
+			dev_dbg(krio_priv->dev,
+				"ER port %d: failed to issue Input-Status request from link partner port %d\n",
+				port, r_port);
+		}
+
+		/* range values are by experiment */
+		usleep_range(50, 200);
+	}
+
+	err_stat   = krio_sp_read(krio_priv, sp[port].err_stat);
+	err_det    = krio_err_read(krio_priv, sp_err[port].det);
+	plm_status = krio_phy_read(krio_priv, phy_sp[port].status);
+
+	dev_dbg(krio_priv->dev,
+		"ER port %d: ending with err_stat = 0x%08x, err_det = 0x%08x, plm_status = 0x%08x\n",
+		port, err_stat, err_det, plm_status);
+
+	return err_stat & KEYSTONE_RIO_PORT_ERRORS;
+}
+
+static void keystone_rio_pe_dpc(struct work_struct *work)
+{
+	struct keystone_rio_data *krio_priv = container_of(
+		to_delayed_work(work), struct keystone_rio_data, pe_work);
+	u32 port;
+
+	if (krio_priv->started == 0)
+		return;
+
+	dev_dbg(krio_priv->dev, "ER errors on ports: 0x%x\n",
+		krio_priv->pe_ports);
+
+	for (port = 0; port < KEYSTONE_RIO_MAX_PORT; port++) {
+		/* Skip port if we are currently registering it */
+		if (krio_priv->ports_registering & BIT(port))
+			continue;
+
+		if (test_and_clear_bit(port, (void *)&krio_priv->pe_ports)) {
+			/* Wait lanes to be OK */
+			if (keystone_rio_lanes_init_and_wait(port, 0,
+							     krio_priv)) {
+				dev_dbg(krio_priv->dev,
+					"ER port %d: lanes are not OK\n", port);
+				krio_priv->pe_ports |= BIT(port);
+			}
+
+			/*  Recover from port error state */
+			if (keystone_rio_port_error_recovery(port,
+							     krio_priv)) {
+				dev_dbg(krio_priv->dev,
+					"ER port %d: failed to perform error recovery\n",
+					port);
+				krio_priv->pe_ports |= BIT(port);
+			}
+		}
+	}
+
+	/* If error recovery failed delay another one if there is time left */
+	if (krio_priv->pe_ports) {
+		if (krio_priv->pe_cnt-- > 1) {
+			schedule_delayed_work(
+				&krio_priv->pe_work,
+				KEYSTONE_RIO_REGISTER_DELAY);
+		} else {
+			dev_err(krio_priv->dev,
+				"ER port %d: failed to recover from errors\n",
+				port);
+		}
+	}
+}
+
+/**
+ * keystone_rio_port_status - Return if the port is OK or not
+ * @port: index of the port
+ *
+ * Return %0 if the port is ready or %-EIO on failure.
+ */
+static int keystone_rio_port_status(int port,
+				    struct keystone_rio_data *krio_priv)
+{
+	u32 path_mode = krio_priv->board_rio_cfg.path_mode;
+	int lanes     = keystone_rio_get_lane_config(BIT(port), path_mode);
+	u32 count     = 0;
+	int res       = 0;
+	int solid_ok  = 0;
+	u32 value;
+
+	if (port >= KEYSTONE_RIO_MAX_PORT)
+		return -EINVAL;
+
+	/* Check port status */
+	for (count = 0; count < 300; count++) {
+		value = krio_sp_read(krio_priv, sp[port].err_stat);
+		if (value & RIO_PORT_N_ERR_STS_PORT_OK) {
+			solid_ok++;
+			if (solid_ok == 100)
+				break;
+		} else {
+			if (solid_ok) {
+				dev_dbg(krio_priv->dev,
+					"port %d (solid_ok = %d)\n",
+					port, solid_ok);
+				goto port_phy_error;
+			}
+			solid_ok = 0;
+		}
+		usleep_range(10, 50);
+	}
+
+	if (solid_ok == 100) {
+		/* Sync ackID */
+		res = keystone_rio_port_sync_ackid(port, krio_priv);
+		if (res == -1)
+			goto port_error;
+
+		/* Check if we need to retrieve the corresponding remote port */
+		if ((error_recovery) &&
+		    (krio_priv->board_rio_cfg.ports_remote[port] < 0)) {
+			int rport;
+
+			rport = keystone_rio_get_remote_port(port, krio_priv);
+			if (rport < 0) {
+				dev_warn(krio_priv->dev,
+					 "cannot retrieve remote port on port %d\n",
+					 port);
+			} else {
+				dev_info(krio_priv->dev,
+					 "detected remote port %d on port %d\n",
+					 rport, port);
+			}
+			krio_priv->board_rio_cfg.ports_remote[port] = rport;
+		}
+	} else {
+		dev_dbg(krio_priv->dev,
+			"port %d is not initialized - port is not solid ok\n",
+			port);
+
+		goto port_error;
+	}
+
+	return 0; /* Port must be solid OK */
+
+port_phy_error:
+	dev_dbg(krio_priv->dev, "recover lane mask 0x%x for port %d\n",
+		lanes, port);
+
+	krio_priv->serdes.ops->recover_lanes(lanes, &krio_priv->serdes);
+port_error:
+	return -EIO;
+}
+
+/**
+ * keystone_rio_port_disable - Disable a RapidIO port
+ * @port: index of the port to configure
+ */
+static void keystone_rio_port_disable(u32 port,
+				      struct keystone_rio_data *krio_priv)
+{
+	/* Disable port */
+	krio_sp_write(krio_priv, sp[port].ctl, 0x800000);
+}
+
+/**
+ * keystone_rio_port_enable - Enable a RapidIO port
+ * @port: index of the port to configure
+ */
+static void keystone_rio_port_enable(u32 port,
+				     struct keystone_rio_data *krio_priv)
+{
+	/* Enable port in input and output */
+	krio_sp_write(krio_priv, sp[port].ctl, 0x600000);
+}
+
+/**
+ * keystone_rio_port_init - Configure a RapidIO port
+ * @port: index of the port to configure
+ * @mode: serdes configuration
+ */
+static int keystone_rio_port_init(u32 port, u32 path_mode,
+				  struct keystone_rio_data *krio_priv)
+{
+	u32 val;
+
+	if (unlikely(port >= KEYSTONE_RIO_MAX_PORT))
+		return -EINVAL;
+
+	/* Silence and discovery timers */
+	if ((port == 0) || (port == 2)) {
+		krio_phy_write(krio_priv, phy_sp[port].silence_timer,
+			       0x20000000);
+		krio_phy_write(krio_priv, phy_sp[port].discovery_timer,
+			       0x20000000);
+	}
+
+	/* Increase the number of valid code-groups required for sync */
+	krio_phy_write(krio_priv, phy_sp[port].vmin_exp, 0x0f030300);
+
+	/* Program channel allocation to ports (1x, 2x or 4x) */
+	krio_phy_write(krio_priv, phy_sp[port].path_ctl, path_mode);
+
+	/*
+	 * Disable all errors reporting if using packet forwarding
+	 * otherwise enable them.
+	 */
+	krio_err_write(krio_priv, sp_err[port].rate_en,
+		       krio_priv->board_rio_cfg.pkt_forwarding ?
+		       0 : 0xffffffff);
+
+	/* Cleanup port error status */
+	krio_sp_write(krio_priv, sp[port].err_stat,
+		      KEYSTONE_RIO_PORT_ERROR_MASK);
+
+	krio_err_write(krio_priv, sp_err[port].det, 0);
+
+	/* Enable interrupt for reset request */
+	val = krio_ev_read(krio_priv, evt_mgmt_rst_int_en);
+	krio_ev_write(krio_priv, evt_mgmt_rst_int_en, val | BIT(port));
+
+	/* Enable all PLM interrupts */
+	krio_phy_write(krio_priv, phy_sp[port].int_enable, 0xffffffff);
+	krio_phy_write(krio_priv, phy_sp[port].all_int_en, 1);
+
+	/* Set unicast mode */
+	krio_tp_write(krio_priv, transport_sp[port].control, 0x00109000);
+
+	return 0;
+}
+
+/**
+ * keystone_rio_port_set_routing - Configure routing for a RapidIO port
+ * @port: index of the port to configure
+ */
+static void keystone_rio_port_set_routing(u32 port,
+					  struct keystone_rio_data *krio_priv)
+{
+	u32 base_dev_id = krio_priv->board_rio_cfg.size ?
+		krio_car_csr_read(krio_priv, base_dev_id) & 0xffff :
+		(krio_car_csr_read(krio_priv, base_dev_id) >> 16) & 0xff;
+	u32 brr = KEYSTONE_RIO_PKT_FW_BRR_NUM;
+
+	/*
+	 * Configure the Base Routing Register (BRR) to ensure that all packets
+	 * matching our DevId are admitted.
+	 */
+	krio_tp_write(krio_priv,
+		      transport_sp[port].base_route[brr].pattern_match,
+		      (base_dev_id << 16)
+		      | (krio_priv->board_rio_cfg.size ? 0xffff : 0xff));
+
+	dev_dbg(krio_priv->dev, "pattern_match = 0x%x for BRR %d\n",
+		krio_tp_read(krio_priv,
+			     transport_sp[port].base_route[brr].pattern_match),
+		brr);
+
+	/* Enable routing to LLM for this BRR and port */
+	krio_tp_write(krio_priv, transport_sp[port].base_route[brr].ctl,
+		      0x84000000);
+
+	/* Use next BRR */
+	brr += 1;
+
+	/*
+	 * Configure the Base Routing Register (BRR) to ensure that all
+	 * broadcast packets are admitted as well.
+	 */
+	krio_tp_write(krio_priv,
+		      transport_sp[port].base_route[brr].pattern_match,
+		      (0xffff << 16)
+		      | (krio_priv->board_rio_cfg.size ? 0xffff : 0xff));
+
+	dev_dbg(krio_priv->dev, "pattern_match = 0x%x for BRR %d\n",
+		krio_tp_read(krio_priv,
+			     transport_sp[port].base_route[brr].pattern_match),
+		brr);
+
+	/* Enable routing to LLM for this BRR and port */
+	krio_tp_write(krio_priv, transport_sp[port].base_route[brr].ctl,
+		      0x84000000);
+
+	/* Set multicast and packet forwarding mode */
+	krio_tp_write(krio_priv, transport_sp[port].control, 0x00209000);
+}
+
+/*------------------------- Configuration space mngt  ----------------------*/
+
+/**
+ * keystone_local_config_read - Generate a KeyStone local config space read
+ * @mport: RapidIO master port info
+ * @index: ID of RapidIO interface
+ * @offset: Offset into configuration space
+ * @len: Length (in bytes) of the maintenance transaction
+ * @data: Value to be read into
+ *
+ * Generates a KeyStone local configuration space read. Returns %0 on
+ * success or %-EINVAL on failure.
+ */
+static int keystone_local_config_read(struct rio_mport *mport,
+				      int index, u32 offset, int len, u32 *data)
+{
+	struct keystone_rio_data *krio_priv = mport->priv;
+
+	if (len != sizeof(u32))
+		return -EINVAL; /* only 32-bit access is supported */
+
+	if ((offset + len) > (krio_priv->board_rio_cfg.rio_regs_size
+			      - KEYSTONE_RIO_CAR_CSR_REGS))
+		return -EINVAL; /* only within the RIO regs range */
+
+	/*
+	 * Workaround for rionet: the processing element features must content
+	 * RIO_PEF_INB_MBOX and RIO_PEF_INB_DOORBELL bits that cannot be set on
+	 * KeyStone hardware. So cheat the read value in this case...
+	 */
+	if (unlikely(offset == RIO_PEF_CAR))
+		*data = krio_priv->rio_pe_feat;
+	else
+		*data = krio_car_csr_read_ofs(krio_priv, offset);
+
+	dev_dbg(krio_priv->dev,
+		"local_conf_r: index %d offset 0x%x data 0x%x\n",
+		index, offset, *data);
+
+	return 0;
+}
+
+/**
+ * keystone_local_config_write - Generate a KeyStone local config space write
+ * @mport: RapidIO master port info
+ * @index: ID of RapidIO interface
+ * @offset: Offset into configuration space
+ * @len: Length (in bytes) of the maintenance transaction
+ * @data: Value to be written
+ *
+ * Generates a KeyStone local configuration space write. Returns %0 on
+ * success or %-EINVAL on failure.
+ */
+static int keystone_local_config_write(struct rio_mport *mport,
+				       int index, u32 offset, int len, u32 data)
+{
+	struct keystone_rio_data *krio_priv = mport->priv;
+
+	if (len != sizeof(u32))
+		return -EINVAL; /* only 32-bit access is supported */
+
+	if ((offset + len) > (krio_priv->board_rio_cfg.rio_regs_size
+			      - KEYSTONE_RIO_CAR_CSR_REGS))
+		return -EINVAL; /* only within the RIO regs range */
+
+	dev_dbg(krio_priv->dev,
+		"local_conf_w: index %d offset 0x%x data 0x%x\n",
+		index, offset, data);
+
+	krio_car_csr_write_ofs(krio_priv, offset, data);
+
+	return 0;
+}
+
+/**
+ * keystone_rio_config_read - Generate a KeyStone read maintenance transaction
+ * @mport: RapidIO master port info
+ * @index: ID of RapidIO interface
+ * @destid: Destination ID of transaction
+ * @hopcount: Number of hops to target device
+ * @offset: Offset into configuration space
+ * @len: Length (in bytes) of the maintenance transaction
+ * @val: Location to be read into
+ *
+ * Generates a KeyStone read maintenance transaction. Returns %0 on
+ * success or %-EINVAL on failure.
+ */
+static int
+keystone_rio_config_read(struct rio_mport *mport, int index, u16 destid,
+			 u8 hopcount, u32 offset, int len, u32 *val)
+{
+	return keystone_rio_maint_read((struct keystone_rio_data *)mport->priv,
+				       mport->index, destid, mport->sys_size,
+				       hopcount, offset, len, val);
+}
+
+/**
+ * keystone__rio_config_write - Generate a KeyStone write
+ * maintenance transaction
+ * @mport: RapidIO master port info
+ * @index: ID of RapidIO interface
+ * @destid: Destination ID of transaction
+ * @hopcount: Number of hops to target device
+ * @offset: Offset into configuration space
+ * @len: Length (in bytes) of the maintenance transaction
+ * @val: Value to be written
+ *
+ * Generates an KeyStone write maintenance transaction. Returns %0 on
+ * success or %-EINVAL on failure.
+ */
+static int
+keystone_rio_config_write(struct rio_mport *mport, int index, u16 destid,
+			  u8 hopcount, u32 offset, int len, u32 val)
+{
+	return keystone_rio_maint_write((struct keystone_rio_data *)mport->priv,
+					mport->index, destid, mport->sys_size,
+					hopcount, offset, len, val);
+}
+
+/*----------------------------- Port-Write management ------------------------*/
+
+static int keystone_rio_port_write_enable(struct keystone_rio_data *krio_priv,
+					  u32 port,
+					  int enable)
+{
+	u32 val;
+
+	/* Clear port-write reception capture */
+	krio_pw_write(krio_priv, port_wr_rx_capt[port], 0);
+
+	if (enable) {
+		/*
+		 * Enable generation of port-write requests
+		 */
+		krio_phy_write(krio_priv, phy_sp[port].port_wr_enable,
+			       BIT(25) | BIT(26) | BIT(28));
+
+		val = krio_phy_read(krio_priv, phy_sp[port].all_port_wr_en);
+		krio_phy_write(krio_priv, phy_sp[port].all_port_wr_en,
+			       val | BIT(0)); /* PW_EN */
+	} else {
+		/*
+		 * Disable generation of port-write requests
+		 */
+		krio_phy_write(krio_priv, phy_sp[port].port_wr_enable, 0);
+
+		val = krio_phy_read(krio_priv, phy_sp[port].all_port_wr_en);
+		krio_phy_write(krio_priv, phy_sp[port].all_port_wr_en,
+			       val & ~(BIT(0))); /* PW_EN */
+	}
+
+	return 0;
+}
+
+static void keystone_rio_pw_dpc(struct work_struct *work)
+{
+	struct keystone_rio_data *krio_priv =
+		container_of(work, struct keystone_rio_data, pw_work);
+	union rio_pw_msg pwmsg;
+
+	if (!krio_priv->started)
+		return;
+
+	/*
+	 * Process port-write messages
+	 */
+	while (kfifo_out_spinlocked(&krio_priv->pw_fifo,
+				    (unsigned char *)&pwmsg,
+				    RIO_PW_MSG_SIZE,
+				    &krio_priv->pw_fifo_lock)) {
+		u32 port_id = pwmsg.em.is_port & 0xff;
+
+		/* Pass the port-write message to RIO core for processing */
+		if (krio_priv->mport[port_id])
+			rio_inb_pwrite_handler(krio_priv->mport[port_id],
+					       &pwmsg);
+	}
+}
+
+/**
+ *  keystone_rio_port_write_handler - KeyStone port write interrupt handler
+ *
+ * Handles port write interrupts. Parses a list of registered
+ * port write event handlers and executes a matching event handler.
+ */
+static void keystone_rio_port_write_handler(struct keystone_rio_data *krio_priv)
+{
+	int pw;
+
+	/* Check that we have a port-write-in case */
+	pw = krio_pw_read(krio_priv, port_wr_rx_stat) & 0x1;
+
+	/* Schedule deferred processing if PW was received */
+	if (pw) {
+		/*
+		 * Retrieve PW message
+		 */
+		krio_priv->port_write_msg.msg.em.comptag =
+			krio_pw_read(krio_priv, port_wr_rx_capt[0]);
+		krio_priv->port_write_msg.msg.em.errdetect =
+			krio_pw_read(krio_priv, port_wr_rx_capt[1]);
+		krio_priv->port_write_msg.msg.em.is_port =
+			krio_pw_read(krio_priv, port_wr_rx_capt[2]);
+		krio_priv->port_write_msg.msg.em.ltlerrdet =
+			krio_pw_read(krio_priv, port_wr_rx_capt[3]);
+
+		/*
+		 * Save PW message (if there is room in FIFO), otherwise
+		 * discard it.
+		 */
+		if (kfifo_avail(&krio_priv->pw_fifo) >= RIO_PW_MSG_SIZE) {
+			krio_priv->port_write_msg.msg_count++;
+			kfifo_in(&krio_priv->pw_fifo,
+				 (void const *)&krio_priv->port_write_msg.msg,
+				 RIO_PW_MSG_SIZE);
+		} else {
+			krio_priv->port_write_msg.discard_count++;
+			dev_warn(krio_priv->dev,
+				 "ISR Discarded Port-Write Msg(s) (%d)\n",
+				 krio_priv->port_write_msg.discard_count);
+		}
+		schedule_work(&krio_priv->pw_work);
+	}
+}
+
+/**
+ * keystone_rio_port_write_init - KeyStone port write interface init
+ * @mport: Master port implementing the port write unit
+ *
+ * Initializes port write unit hardware and buffer
+ * ring. Called from keystone_rio_setup(). Returns %0 on success
+ * or %-ENOMEM on failure.
+ */
+static int keystone_rio_port_write_init(struct keystone_rio_data *krio_priv)
+{
+	int ret;
+	int port;
+
+	for (port = 0; port < KEYSTONE_RIO_MAX_PORT; port++) {
+		/* Disabling port write */
+		keystone_rio_port_write_enable(krio_priv, port, 0);
+
+		/* Clear port-write-in capture registers */
+		krio_pw_write(krio_priv, port_wr_rx_capt[port], 0);
+	}
+
+	INIT_WORK(&krio_priv->pw_work, keystone_rio_pw_dpc);
+	spin_lock_init(&krio_priv->pw_fifo_lock);
+
+	ret = kfifo_alloc(&krio_priv->pw_fifo, RIO_PW_MSG_SIZE * 32,
+			  GFP_KERNEL);
+	if (ret) {
+		dev_err(krio_priv->dev, "FIFO allocation failed\n");
+		return -ENOMEM;
+	}
+
+	return 0;
+}
+
+/**
+ * keystone_rio_pw_enable - enable/disable port-write interface init
+ * @mport: Master port implementing the port write unit
+ * @enable: 1=enable; 0=disable port-write message handling
+ */
+static int keystone_rio_pw_enable(struct rio_mport *mport, int enable)
+{
+	return keystone_rio_port_write_enable(mport->priv,
+					      mport->index,
+					      enable);
+}
+
+/*------------------------ Inbound memory region management  -----------------*/
+
+/**
+ * keystone_rio_map_inb_mem -- Mapping inbound memory region.
+ * @mport: RapidIO master port
+ * @lstart: Local memory space start address.
+ * @rstart: RapidIO space start address.
+ * @size: The mapping region size.
+ * @flags: Flags for mapping. 0 for using default flags.
+ *
+ * Return: 0 -- Success.
+ *
+ * This function will create the inbound mapping
+ * from rstart to lstart.
+ */
+static int keystone_rio_map_inb_mem(struct rio_mport *mport, dma_addr_t lstart,
+				    u64 rstart, u32 size, u32 flags)
+{
+	struct keystone_rio_data *krio_priv = mport->priv;
+
+	dev_dbg(krio_priv->dev,
+		"mapping inbound window 0x%x to RIO space 0x%llx with size 0x%x\n",
+		lstart, rstart, size);
+
+	/*
+	 * Because we do not hw capability to map inbound mapping we need to
+	 * ensure that caller is asking us for a 1:1 direct mapping.
+	 */
+	if ((dma_addr_t)rstart != lstart)
+		return -EINVAL;
+
+	return 0;
+}
+
+/**
+ * keystone_rio_unmap_inb_mem -- Unmapping inbound memory region.
+ * @mport: RapidIO master port
+ * @lstart: Local memory space start address.
+ */
+static void keystone_rio_unmap_inb_mem(struct rio_mport *mport,
+				       dma_addr_t lstart)
+{
+	struct keystone_rio_data *krio_priv = mport->priv;
+
+	dev_dbg(krio_priv->dev, "unmapping inbound window 0x%x\n", lstart);
+}
+
+/*------------------------ Main Linux driver functions -----------------------*/
+
+static int keystone_rio_query_mport(struct rio_mport *mport,
+				    struct rio_mport_attr *attr)
+{
+	struct keystone_rio_data *krio_priv = mport->priv;
+	u32 port = mport->index;
+	u32 rval;
+
+	if (!attr)
+		return -EINVAL;
+
+	rval = krio_sp_read(krio_priv, sp[port].err_stat);
+	if (rval & RIO_PORT_N_ERR_STS_PORT_OK) {
+		rval = krio_sp_read(krio_priv, sp[port].ctl2);
+		attr->link_speed = (rval & RIO_PORT_N_CTL2_SEL_BAUD) >> 28;
+		rval = krio_sp_read(krio_priv, sp[port].ctl);
+		attr->link_width = (rval & RIO_PORT_N_CTL_IPW) >> 27;
+	} else {
+		attr->link_speed = RIO_LINK_DOWN;
+	}
+
+#ifdef CONFIG_RAPIDIO_DMA_ENGINE
+	/* Supporting DMA but not HW SG mode*/
+	attr->flags        = RIO_MPORT_DMA;
+
+	attr->dma_max_sge  = KEYSTONE_RIO_DMA_MAX_DESC - 1;
+	attr->dma_max_size = KEYSTONE_RIO_MAX_DIO_PKT_SIZE;
+	attr->dma_align    = KEYSTONE_RIO_DIO_ALIGNMENT;
+#else
+	attr->flags        = 0;
+	attr->dma_max_sge  = 0;
+	attr->dma_max_size = 0;
+	attr->dma_align    = 0;
+#endif
+
+	return 0;
+}
+
+static void keystone_rio_mport_release(struct device *dev)
+{
+	struct rio_mport *mport = to_rio_mport(dev);
+
+	dev_dbg(dev, "%s %s id=%d\n", __func__, mport->name, mport->id);
+}
+
+struct rio_mport *keystone_rio_register_mport(
+	u32 port_id,
+	u32 size,
+	struct keystone_rio_data *krio_priv)
+{
+	struct rio_ops   *ops;
+	struct rio_mport *mport;
+	int res;
+
+	ops = kzalloc(sizeof(*ops), GFP_KERNEL);
+
+	ops->lcread  = keystone_local_config_read;
+	ops->lcwrite = keystone_local_config_write;
+	ops->cread   = keystone_rio_config_read;
+	ops->cwrite  = keystone_rio_config_write;
+	ops->dsend   = keystone_rio_dbell_send;
+	ops->open_outb_mbox   = keystone_rio_open_outb_mbox;
+	ops->close_outb_mbox  = keystone_rio_close_outb_mbox;
+	ops->open_inb_mbox    = keystone_rio_open_inb_mbox;
+	ops->close_inb_mbox   = keystone_rio_close_inb_mbox;
+	ops->add_outb_message = keystone_rio_hw_add_outb_message;
+	ops->add_inb_buffer   = keystone_rio_hw_add_inb_buffer;
+	ops->get_inb_message  = keystone_rio_hw_get_inb_message;
+	ops->query_mport      = keystone_rio_query_mport;
+	ops->map_inb	      = keystone_rio_map_inb_mem;
+	ops->unmap_inb	      = keystone_rio_unmap_inb_mem;
+	ops->pwenable	      = keystone_rio_pw_enable;
+
+	mport = kzalloc(sizeof(*mport), GFP_KERNEL);
+
+	/* Initialize the mport structure */
+	res = rio_mport_initialize(mport);
+	if (res) {
+		kfree(mport);
+		return NULL;
+	}
+
+	/*
+	 * Set the SRIO port physical Id into the index field,
+	 * the id field has been set by rio_mport_initialize() to
+	 * the logical Id
+	 */
+	mport->index = port_id;
+	mport->priv  = krio_priv;
+	mport->dev.parent  = krio_priv->dev;
+	mport->dev.release = keystone_rio_mport_release;
+	INIT_LIST_HEAD(&mport->dbells);
+	INIT_LIST_HEAD(&mport->pwrites);
+
+	/*
+	 * Make a dummy per port region as ports are not
+	 * really separated on KeyStone
+	 */
+	mport->iores.start = (u32)(krio_priv->serial_port_regs) +
+		offsetof(struct keystone_rio_serial_port_regs,
+			 sp[port_id].link_maint_req);
+
+	mport->iores.end = (u32)(krio_priv->serial_port_regs) +
+		offsetof(struct keystone_rio_serial_port_regs,
+			 sp[port_id].ctl);
+
+	mport->iores.flags = IORESOURCE_MEM;
+
+	rio_init_dbell_res(&mport->riores[RIO_DOORBELL_RESOURCE], 0, 0xffff);
+	rio_init_mbox_res(&mport->riores[RIO_INB_MBOX_RESOURCE], 0,
+			  KEYSTONE_RIO_MAX_MBOX);
+	rio_init_mbox_res(&mport->riores[RIO_OUTB_MBOX_RESOURCE], 0,
+			  KEYSTONE_RIO_MAX_MBOX);
+
+	sprintf(mport->name, "RIO%d mport", port_id);
+
+	mport->ops      = ops;
+	mport->sys_size = size;
+	mport->phy_type = RIO_PHY_SERIAL;
+
+	/*
+	 * Hard coded here because in rio_disc_mport(), it is used in
+	 * rio_enum_complete() before it is retrieved in
+	 * rio_disc_peer() => rio_setup_device()
+	 */
+	mport->phys_efptr = 0x100;
+
+	/*
+	 * Register the new mport
+	 */
+	res = rio_register_mport(mport);
+	if (res) {
+		kfree(mport);
+		return NULL;
+	}
+
+	krio_priv->mport[port_id] = mport;
+
+#ifdef CONFIG_RAPIDIO_DMA_ENGINE
+	/*
+	 * Register the DMA engine for DirectIO transfers
+	 */
+	keystone_rio_dma_register(mport,
+				  krio_priv->board_rio_cfg.dma_channel_num);
+	/*
+	 * Reserve one channel for doorbells
+	 */
+	keystone_rio_lsu_dma_allocate_channel(mport);
+#endif
+
+	return mport;
+}
+
+static int krio_of_parse_mbox(int mbox, struct device_node *node_rio,
+			      struct keystone_rio_data *krio_priv)
+{
+	struct keystone_rio_rx_chan_info *krx_chan =
+					&krio_priv->rx_channels[mbox];
+	struct keystone_rio_tx_chan_info *ktx_chan =
+					&krio_priv->tx_channels[mbox];
+	struct device_node *node;
+	char node_name[24];
+	u32 temp[2];
+
+	snprintf(node_name, sizeof(node_name), "mbox-%d", mbox);
+	node = of_get_child_by_name(node_rio, node_name);
+	if (!node) {
+		dev_err(krio_priv->dev, "could not find %s node\n", node_name);
+		return -ENODEV;
+	}
+
+	dev_dbg(krio_priv->dev, "using node \"%s\"\n", node_name);
+
+	/* DMA rx chan config */
+	if (of_property_read_string(node, "rx-channel", &krx_chan->name) < 0) {
+		dev_err(krio_priv->dev,
+			"missing \"rx-channel\" parameter for mbox %d\n",
+			mbox);
+		of_node_put(node);
+		return -ENOENT;
+	}
+
+	if (of_property_read_u32_array(node, "rx-pool", temp, 2)) {
+		dev_err(krio_priv->dev,
+			"missing \"rx-pool\" parameter for mbox %d\n",
+			mbox);
+		of_node_put(node);
+		return -ENOENT;
+	}
+
+	krx_chan->pool_size = temp[0];
+	krx_chan->pool_region_id = temp[1];
+
+	if (of_property_read_u32_array(node, "rx-queue-depth",
+				       krx_chan->queue_depths,
+				       KNAV_DMA_FDQ_PER_CHAN) < 0) {
+		dev_warn(krio_priv->dev,
+			 "missing \"rx-queue-depth\" parameter for mbox %d\n",
+			 mbox);
+		krx_chan->queue_depths[0] = 128;
+	}
+
+	if (of_property_read_u32_array(node, "rx-buffer-size",
+				       krx_chan->buffer_sizes,
+				       KNAV_DMA_FDQ_PER_CHAN) < 0) {
+		dev_warn(krio_priv->dev,
+			 "missing \"rx-buffer-size\" parameter for mbox %d\n",
+			 mbox);
+		krx_chan->buffer_sizes[0] = 4096;
+	}
+
+	if (of_property_read_u32(node, "rx-queue",
+				 &krx_chan->queue_id)) {
+		dev_warn(krio_priv->dev, "missing \"rx-queue\" parameter for mbox %d, using qpend\n",
+			 mbox);
+		krx_chan->queue_id = KNAV_QUEUE_QPEND;
+	}
+
+	/*
+	 * If stream_id is defined, this mbox is mapped to the corresponding
+	 * streamid and the channel is for type 9 packets.
+	 */
+	if (of_property_read_u32(node, "stream-id",
+				 &krx_chan->stream_id) < 0) {
+		krx_chan->packet_type = RIO_PACKET_TYPE_MESSAGE;
+		krx_chan->stream_id = -1;
+	} else {
+		krx_chan->packet_type = RIO_PACKET_TYPE_STREAM;
+	}
+
+	/* DMA tx chan config */
+	if (of_property_read_string(node, "tx-channel", &ktx_chan->name) < 0) {
+		dev_err(krio_priv->dev, "missing \"tx-channel\" parameter for mbox %d\n",
+			mbox);
+		of_node_put(node);
+		return -ENOENT;
+	}
+
+	if (of_property_read_u32_array(node, "tx-pool", temp, 2)) {
+		dev_err(krio_priv->dev, "missing \"tx-pool\" parameter for mbox %d\n",
+			mbox);
+		of_node_put(node);
+		return -ENOENT;
+	}
+
+	ktx_chan->pool_size = temp[0];
+	ktx_chan->pool_region_id = temp[1];
+
+	if (of_property_read_u32(node, "tx-queue-depth",
+				 &ktx_chan->queue_depth) < 0) {
+		dev_warn(krio_priv->dev,
+			 "missing \"tx-queue-depth\" parameter for mbox %d\n",
+			 mbox);
+		ktx_chan->queue_depth = 128;
+	}
+
+	if (of_property_read_u32(node, "tx-queue",
+				 &ktx_chan->queue_id)) {
+		dev_err(krio_priv->dev,
+			"missing \"tx-queue\" parameter for mbox %d\n",
+			mbox);
+		of_node_put(node);
+		return -ENOENT;
+	}
+
+	if (of_property_read_u32(node, "tx-completion-queue",
+				 &ktx_chan->complet_queue_id)) {
+		dev_warn(krio_priv->dev,
+			 "missing \"tx-completion-queue\" parameter for mbox %d, using qpend\n",
+			 mbox);
+		ktx_chan->complet_queue_id = KNAV_QUEUE_QPEND;
+	}
+
+	if (of_property_read_u32(node, "tx-garbage-queue",
+				 &ktx_chan->garbage_queue_id)) {
+		dev_warn(krio_priv->dev,
+			 "missing \"tx-garbage-queue\" parameter for mbox %d, using qpend\n",
+			 mbox);
+		ktx_chan->garbage_queue_id = KNAV_QUEUE_QPEND;
+	}
+
+	of_node_put(node);
+
+	return 0;
+}
+
+static int krio_of_parse(struct device_node *node,
+			 struct keystone_rio_data *krio_priv)
+{
+	struct keystone_rio_board_controller_info *c =
+		&krio_priv->board_rio_cfg;
+	u32 temp[24];
+	int i;
+	int mbox;
+
+	/* Get SRIO registers */
+	i = of_property_match_string(node, "reg-names", "rio");
+	if (i < 0) {
+		dev_err(krio_priv->dev,
+			"missing reg-names \"boot_config\" parameter\n");
+		return -ENOENT;
+	}
+	if (of_property_read_u32_index(node, "reg", (i << 1), &temp[0])) {
+		dev_err(krio_priv->dev, "missing \"reg\" parameters\n");
+		return -ENOENT;
+	}
+	if (of_property_read_u32_index(node, "reg", (i << 1) + 1, &temp[1])) {
+		dev_err(krio_priv->dev, "missing \"reg\" parameters\n");
+		return -ENOENT;
+	}
+
+	c->rio_regs_base = temp[0];
+	c->rio_regs_size = temp[1];
+
+	/* Get boot config registers */
+	i = of_property_match_string(node, "reg-names", "boot_config");
+	if (i < 0) {
+		dev_err(krio_priv->dev,
+			"missing reg-names \"boot_config\" parameter\n");
+		return -ENOENT;
+	}
+	if (of_property_read_u32_index(node, "reg", (i << 1), &temp[2])) {
+		dev_err(krio_priv->dev, "missing \"reg\" parameters\n");
+		return -ENOENT;
+	}
+	if (of_property_read_u32_index(node, "reg", (i << 1) + 1, &temp[3])) {
+		dev_err(krio_priv->dev, "missing \"reg\" parameters\n");
+		return -ENOENT;
+	}
+
+	c->boot_cfg_regs_base = temp[2];
+	c->boot_cfg_regs_size = temp[3];
+
+	/* Get SerDes registers */
+	i = of_property_match_string(node, "reg-names", "serdes");
+	if (i < 0) {
+		dev_err(krio_priv->dev,
+			"missing reg-names \"serdes\" parameter\n");
+		return -ENOENT;
+	}
+	if (of_property_read_u32_index(node, "reg", (i << 1), &temp[4])) {
+		dev_err(krio_priv->dev, "missing \"reg\" parameters\n");
+		return -ENOENT;
+	}
+	if (of_property_read_u32_index(node, "reg", (i << 1) + 1, &temp[5])) {
+		dev_err(krio_priv->dev, "missing \"reg\" parameters\n");
+		return -ENOENT;
+	}
+
+	c->serdes_cfg_regs_base = temp[4];
+	c->serdes_cfg_regs_size = temp[5];
+
+	if (of_property_read_u32 (node, "dev-id-size", &c->size))
+		dev_warn(krio_priv->dev, "missing \"dev-id-size\" parameter\n");
+
+	if (of_property_read_u32 (node, "ports", &c->ports))
+		dev_warn(krio_priv->dev, "missing \"ports\" parameter\n");
+
+	if (of_property_read_u32_array(node, "ports-remote", c->ports_remote,
+				       KEYSTONE_RIO_MAX_PORT)) {
+		/* Remote ports will be detected during port status */
+		for (i = 0; i < KEYSTONE_RIO_MAX_PORT; i++)
+			c->ports_remote[i] = -1;
+	}
+
+	/* SerDes config */
+	if (!of_find_property(node, "keystone2-serdes", NULL)) {
+		/* K1 setup*/
+		c->serdes_type                     = KEYSTONE_SERDES_TYPE_K1;
+		c->serdes_config.prescalar_srv_clk = 0x001e;
+		c->serdes_config.do_phy_init_cfg   = 0;
+		c->path_mode                       = 0x0000;
+	} else {
+		/* K2 setup*/
+		c->serdes_type                     = KEYSTONE_SERDES_TYPE_K2;
+		c->serdes_config.prescalar_srv_clk = 0x001f;
+		c->serdes_config.do_phy_init_cfg   = 0;
+		c->path_mode                       = 0x0004;
+
+		if (of_property_read_u32(node, "baudrate",
+					 &c->serdes_baudrate)) {
+			dev_warn(krio_priv->dev,
+				 "missing \"baudrate\" parameter, using 5Gbps\n");
+			c->serdes_baudrate = KEYSTONE_SERDES_BAUD_5_000;
+		}
+	}
+
+	/* Set if performing optional SerDes calibration sequence at boot */
+	c->serdes_calibration = serdes_calibration;
+
+	/* SerDes pre-1lsb, c1, c2, cm, att and vreg config */
+	if (of_property_read_u32_array(node, "serdes-1lsb", &temp[0], 4)) {
+		for (i = 0; i < 4; i++)
+			c->serdes_config.tx[i].pre_1lsb = 0;
+	} else {
+		for (i = 0; i < 4; i++)
+			c->serdes_config.tx[i].pre_1lsb = temp[i];
+	}
+
+	if (of_property_read_u32_array(node, "serdes-c1", &temp[0], 4)) {
+		if (c->serdes_baudrate == KEYSTONE_SERDES_BAUD_3_125) {
+			for (i = 0; i < 4; i++)
+				c->serdes_config.tx[i].c1_coeff = 4;
+		} else {
+			for (i = 0; i < 4; i++)
+				c->serdes_config.tx[i].c1_coeff = 6;
+		}
+	} else {
+		for (i = 0; i < 4; i++)
+			c->serdes_config.tx[i].c1_coeff = temp[i];
+	}
+
+	if (of_property_read_u32_array(node, "serdes-c2", &temp[0], 4)) {
+		for (i = 0; i < 4; i++)
+			c->serdes_config.tx[i].c2_coeff = 0;
+	} else {
+		for (i = 0; i < 4; i++)
+			c->serdes_config.tx[i].c2_coeff = temp[i];
+	}
+
+	if (of_property_read_u32_array(node, "serdes-cm", &temp[0], 4)) {
+		for (i = 0; i < 4; i++)
+			c->serdes_config.tx[i].cm_coeff = 0;
+	} else {
+		for (i = 0; i < 4; i++)
+			c->serdes_config.tx[i].cm_coeff = temp[i];
+	}
+
+	if (of_property_read_u32_array(node, "serdes-att", &temp[0], 4)) {
+		for (i = 0; i < 4; i++)
+			c->serdes_config.tx[i].att = 12;
+	} else {
+		for (i = 0; i < 4; i++)
+			c->serdes_config.tx[i].att = temp[i];
+	}
+
+	if (of_property_read_u32_array(node, "serdes-vreg", &temp[0], 4)) {
+		for (i = 0; i < 4; i++)
+			c->serdes_config.tx[i].vreg = 4;
+	} else {
+		for (i = 0; i < 4; i++)
+			c->serdes_config.tx[i].vreg = temp[i];
+	}
+
+	if (of_property_read_u32_array(node, "serdes-vdreg", &temp[0], 4)) {
+		for (i = 0; i < 4; i++)
+			c->serdes_config.tx[i].vdreg = 1;
+	} else {
+		for (i = 0; i < 4; i++)
+			c->serdes_config.tx[i].vdreg = temp[i];
+	}
+
+	if (of_property_read_u32_array(node, "serdes-rx-att-start",
+				       &temp[0], 4)) {
+		for (i = 0; i < 4; i++)
+			c->serdes_config.rx[i].start_att = 3;
+	} else {
+		for (i = 0; i < 4; i++)
+			c->serdes_config.rx[i].start_att = temp[i];
+	}
+
+	if (of_property_read_u32_array(node, "serdes-rx-boost-start",
+				       &temp[0], 4)) {
+		for (i = 0; i < 4; i++)
+			c->serdes_config.rx[i].start_boost = 3;
+	} else {
+		for (i = 0; i < 4; i++)
+			c->serdes_config.rx[i].start_boost = temp[i];
+	}
+
+	if (of_property_read_u32_array(node, "serdes-rx-att", &temp[0], 4)) {
+		for (i = 0; i < 4; i++)
+			/* Use dynamic Rx calibration */
+			c->serdes_config.rx[i].mean_att = -1;
+	} else {
+		for (i = 0; i < 4; i++)
+			c->serdes_config.rx[i].mean_att = temp[i];
+	}
+
+	if (of_property_read_u32_array(node, "serdes-rx-boost", &temp[0], 4)) {
+		for (i = 0; i < 4; i++)
+			/* Use dynamic Rx calibration */
+			c->serdes_config.rx[i].mean_boost = -1;
+	} else {
+		for (i = 0; i < 4; i++)
+			c->serdes_config.rx[i].mean_boost = temp[i];
+	}
+
+	/* Path mode config (mapping of SerDes lanes to port widths) */
+	if (of_property_read_u32(node, "path-mode", &c->path_mode)) {
+		dev_warn(krio_priv->dev,
+			 "missing \"path-mode\" parameter\n");
+	}
+
+	/* Max possible ports configurations per path_mode */
+	if ((c->path_mode == 0 &&
+	     c->ports & ~KEYSTONE_RIO_MAX_PORTS_PATH_MODE_0) ||
+	    (c->path_mode == 1 &&
+	     c->ports & ~KEYSTONE_RIO_MAX_PORTS_PATH_MODE_1) ||
+	    (c->path_mode == 2 &&
+	     c->ports & ~KEYSTONE_RIO_MAX_PORTS_PATH_MODE_2) ||
+	    (c->path_mode == 3 &&
+	     c->ports & ~KEYSTONE_RIO_MAX_PORTS_PATH_MODE_3) ||
+	    (c->path_mode == 4 &&
+	     c->ports & ~KEYSTONE_RIO_MAX_PORTS_PATH_MODE_4)) {
+		dev_err(krio_priv->dev,
+			"\"path_mode\" and \"ports\" configuration mismatch\n");
+		return -EINVAL;
+	}
+
+	/* Port register timeout */
+	if (of_property_read_u32(node, "port-register-timeout",
+				 &c->port_register_timeout)) {
+		c->port_register_timeout = 30;
+	}
+
+	/* LSUs */
+	if (of_property_read_u32_array(node, "lsu", &temp[0], 2)) {
+		krio_priv->lsu_start = 0;
+		krio_priv->lsu_end   = 0;
+	} else {
+		krio_priv->lsu_start = (u8)temp[0];
+		krio_priv->lsu_end   = (u8)temp[1];
+	}
+
+	dev_dbg(krio_priv->dev, "using LSU %d - %d range\n",
+		krio_priv->lsu_start, krio_priv->lsu_end);
+
+#ifdef CONFIG_RAPIDIO_DMA_ENGINE
+	/* DIO (virtual) DMA channels */
+	if (of_property_read_u32(node, "num-dio-channels",
+				 &c->dma_channel_num) < 0) {
+		dev_warn(krio_priv->dev,
+			 "missing \"num-dio-channels\" parameter\n");
+		c->dma_channel_num = 8;
+	}
+#endif
+
+	/* RXU mapping resources */
+	if (of_property_read_u32_array(node, "rxu-map-range", &temp[0], 2)) {
+		krio_priv->rxu_map_start = KEYSTONE_RIO_RXU_MAP_MIN;
+		krio_priv->rxu_map_end   = KEYSTONE_RIO_RXU_MAP_MAX;
+	} else {
+		if ((temp[1] > KEYSTONE_RIO_RXU_MAP_MAX) ||
+		    (temp[0] > temp[1])) {
+			dev_err(krio_priv->dev,
+				"invalid \"rxu-map-range\" parameter\n");
+			return -EINVAL;
+		}
+
+		krio_priv->rxu_map_start = temp[0];
+		krio_priv->rxu_map_end   = temp[1];
+	}
+
+	dev_dbg(krio_priv->dev, "using RXU map %lu - %lu range\n",
+		krio_priv->rxu_map_start, krio_priv->rxu_map_end);
+
+	/* Mailboxes configuration */
+	if (of_property_read_u32(node, "num-mboxes",
+				 &krio_priv->num_mboxes) < 0) {
+		dev_warn(krio_priv->dev,
+			 "missing \"num-mboxes\" parameter\n");
+		krio_priv->num_mboxes = 1;
+	}
+
+	if (krio_priv->num_mboxes > KEYSTONE_RIO_MAX_MBOX) {
+		dev_warn(krio_priv->dev,
+			 "wrong \"num_mboxes\" parameter value %d, set to %d\n",
+			 krio_priv->num_mboxes, KEYSTONE_RIO_MAX_MBOX);
+		krio_priv->num_mboxes = KEYSTONE_RIO_MAX_MBOX;
+	}
+
+	/* Retrieve the per-mailboxes properties */
+	for (mbox = 0; mbox < krio_priv->num_mboxes; mbox++) {
+		int res;
+
+		res = krio_of_parse_mbox(mbox, node, krio_priv);
+		if (res)
+			return res;
+	}
+
+	/* Interrupt config */
+	c->rio_irq = irq_of_parse_and_map(node, 0);
+	if (c->rio_irq < 0) {
+		dev_err(krio_priv->dev, "missing \"rio_irq\" parameter\n");
+		return -ENOENT;
+	}
+
+	c->lsu_irq = irq_of_parse_and_map(node, 1);
+	if (c->lsu_irq < 0) {
+		dev_err(krio_priv->dev, "missing \"lsu_irq\" parameter\n");
+		return -ENOENT;
+	}
+
+	/* Packet forwarding */
+	if (of_property_read_u32_array(node, "pkt-forward", &temp[0], 24)) {
+		c->pkt_forwarding = 0;
+	} else {
+		c->pkt_forwarding = 1;
+
+		for (i = 0; i < 8; i++) {
+			c->routing_config[i].dev_id_low = (u16)temp[(i * 3)];
+			c->routing_config[i].dev_id_high =
+						(u16)temp[(i * 3) + 1];
+			c->routing_config[i].port = (u8)temp[(i * 3) + 2];
+		}
+	}
+
+	return 0;
+}
+
+static int keystone_rio_port_chk(struct keystone_rio_data *krio_priv, int init)
+{
+	unsigned long flags;
+	u32 ports;
+	u32 size  = krio_priv->board_rio_cfg.size;
+	struct rio_mport *mport;
+
+	/* first check those which are scan registering */
+	ports = krio_priv->ports_scan_registering;
+	while (ports) {
+		u32 port = __ffs(ports);
+
+		ports &= ~BIT(port);
+
+		if (!krio_priv->mport[port] || !krio_priv->mport[port]->nscan)
+			continue;
+
+		spin_lock_irqsave(&krio_priv->port_chk_lock, flags);
+		krio_priv->ports_scan_registering &= ~BIT(port);
+		spin_unlock_irqrestore(&krio_priv->port_chk_lock, flags);
+
+		/* now finish the remaining steps */
+		if (krio_priv->board_rio_cfg.pkt_forwarding)
+			keystone_rio_port_set_routing(port, krio_priv);
+
+		krio_priv->base_dev_id = krio_car_csr_read(krio_priv,
+							   base_dev_id);
+	}
+
+	/* next check those which are registering */
+	ports = krio_priv->ports_registering;
+	while (ports) {
+		int status;
+		u32 port = __ffs(ports);
+
+		ports &= ~BIT(port);
+
+		/* Eventually start lanes and wait them to be OK and with SD */
+		if (keystone_rio_lanes_init_and_wait(port, init, krio_priv))
+			continue;
+
+		/*
+		 * Check the port status here before calling the generic RapidIO
+		 * layer. Port status check is done in rio_mport_is_active() as
+		 * well but we need to do it our way first due to some delays in
+		 * hw initialization.
+		 */
+		status = keystone_rio_port_status(port, krio_priv);
+		if (status == 0) {
+			unsigned long flags;
+
+			/*
+			 * The link has been established from an hw standpoint
+			 * so do not try to check the port again.
+			 * Only mport registration may fail now.
+			 */
+			spin_lock_irqsave(&krio_priv->port_chk_lock, flags);
+			krio_priv->ports |= BIT(port);
+			krio_priv->ports_registering &= ~BIT(port);
+			spin_unlock_irqrestore(&krio_priv->port_chk_lock,
+					       flags);
+
+			/* Register mport only if this is initial port check */
+			if (!krio_priv->mport[port]) {
+				mport = keystone_rio_register_mport(
+					port, size, krio_priv);
+
+				if (!mport) {
+					dev_err(krio_priv->dev,
+						"failed to register mport %d\n",
+						port);
+					return -1;
+				} else if (!mport->nscan) {
+					dev_info(krio_priv->dev,
+						 "mport RIO%d@%p hdid %d registering scan\n",
+						 port, mport,
+						 mport->host_deviceid);
+					spin_lock_irqsave(
+						&krio_priv->port_chk_lock,
+						flags);
+					krio_priv->ports_scan_registering |=
+						BIT(port);
+					spin_unlock_irqrestore(
+						&krio_priv->port_chk_lock,
+						flags);
+					continue;
+				} else {
+					dev_info(krio_priv->dev,
+						 "port RIO%d host_deviceid %d registered\n",
+						 port, mport->host_deviceid);
+				}
+			} else {
+				/* should not happen */
+				dev_info(krio_priv->dev,
+					 "port RIO%d host_deviceid %d ready\n",
+					 port,
+					 krio_priv->mport[port]->host_deviceid);
+			}
+
+			/*
+			 * Update routing after discovery/enumeration
+			 * with new dev id
+			 */
+			if (krio_priv->board_rio_cfg.pkt_forwarding)
+				keystone_rio_port_set_routing(port, krio_priv);
+
+			/* Save the current base dev Id */
+			krio_priv->base_dev_id = krio_car_csr_read(krio_priv,
+								   base_dev_id);
+		} else {
+			if (status == -EINVAL)
+				return -1;
+
+			dev_dbg(krio_priv->dev, "port %d not ready\n", port);
+		}
+	}
+
+#ifdef CONFIG_RAPIDIO_ENUM_BASIC
+	if (!krio_priv->ports_scan_registering &&
+	    !krio_priv->ports_registering) {
+		rio_init_mports();
+	}
+#endif
+
+	return (krio_priv->ports_registering |
+		krio_priv->ports_scan_registering);
+}
+
+static void keystone_rio_port_chk_task(struct work_struct *work)
+{
+	struct keystone_rio_data *krio_priv =
+			container_of(to_delayed_work(work),
+				     struct keystone_rio_data,
+				     port_chk_task);
+	int res;
+
+	res = keystone_rio_port_chk(krio_priv, 0);
+	if (res) {
+		unsigned long flags;
+
+		if (res == -1)
+			return;
+
+		/* If port check failed schedule next check (if any) */
+		spin_lock_irqsave(&krio_priv->port_chk_lock, flags);
+		if (krio_priv->port_chk_cnt-- > 1) {
+			spin_unlock_irqrestore(&krio_priv->port_chk_lock,
+					       flags);
+
+			schedule_delayed_work(&krio_priv->port_chk_task,
+					      KEYSTONE_RIO_REGISTER_DELAY);
+		} else {
+			spin_unlock_irqrestore(&krio_priv->port_chk_lock,
+					       flags);
+
+			dev_info(krio_priv->dev,
+				 "RIO port register timeout, port mask 0x%x not ready",
+				 krio_priv->ports_registering |
+				 krio_priv->ports_scan_registering);
+		}
+	}
+}
+
+/*
+ * Sysfs management
+ */
+static ssize_t keystone_rio_ports_store(struct device *dev,
+					struct device_attribute *attr,
+					const char *buf,
+					size_t count)
+{
+	struct keystone_rio_data *krio_priv = (struct keystone_rio_data *)
+		dev->platform_data;
+	unsigned long ports;
+	unsigned long flags;
+
+	if (kstrtoul(buf, 0, &ports))
+		return -EINVAL;
+
+	if (ports > (BIT(KEYSTONE_RIO_MAX_PORT) - 1))
+		return -EINVAL;
+
+	/*
+	 * Only the ports defined in DTS and don't have a good hw port
+	 * status yet can be rescanned because SerDes initialization
+	 * is not restarted here, only link status check.
+	 */
+	ports &= krio_priv->board_rio_cfg.ports;
+
+	spin_lock_irqsave(&krio_priv->port_chk_lock, flags);
+	krio_priv->ports_registering = (ports & ~krio_priv->ports);
+	krio_priv->ports_scan_registering = 0;
+	spin_unlock_irqrestore(&krio_priv->port_chk_lock, flags);
+
+	if (krio_priv->ports_registering) {
+		unsigned long flags;
+
+		dev_dbg(dev, "initializing link for port mask 0x%x\n",
+			krio_priv->ports_registering);
+
+		spin_lock_irqsave(&krio_priv->port_chk_lock, flags);
+		krio_priv->port_chk_cnt =
+			krio_priv->board_rio_cfg.port_register_timeout /
+			(KEYSTONE_RIO_REGISTER_DELAY / HZ);
+		spin_unlock_irqrestore(&krio_priv->port_chk_lock, flags);
+
+		schedule_delayed_work(&krio_priv->port_chk_task, 0);
+	} else {
+		dev_info(dev, "No port to be reinitialized: port mask = 0x%x\n",
+			 krio_priv->ports_registering);
+	}
+
+	return count;
+}
+
+static ssize_t keystone_rio_ports_show(struct device *dev,
+				       struct device_attribute *attr,
+				       char *buf)
+{
+	struct keystone_rio_data *krio_priv =
+			(struct keystone_rio_data *)dev->platform_data;
+
+	if (!krio_priv)
+		return -EINVAL;
+
+	return scnprintf(buf, PAGE_SIZE, "0x%x\n", krio_priv->ports);
+}
+
+static ssize_t keystone_rio_start_store(struct device *dev,
+					struct device_attribute *attr,
+					const char *buf,
+					size_t count)
+{
+	struct keystone_rio_data *krio_priv = (struct keystone_rio_data *)
+		dev->platform_data;
+	unsigned long new_start;
+
+	if (kstrtoul(buf, 0, &new_start))
+		return -EINVAL;
+
+	/* Start SRIO peripheral if not started */
+	if ((new_start) && (krio_priv->started == 0)) {
+		keystone_rio_setup_controller(krio_priv);
+		return count;
+	}
+
+	/* Stop SRIO peripheral if started */
+	if ((new_start == 0) && (krio_priv->started == 1)) {
+		keystone_rio_shutdown_controller(krio_priv);
+		return count;
+	}
+
+	return count;
+}
+
+static ssize_t keystone_rio_start_show(struct device *dev,
+				       struct device_attribute *attr,
+				       char *buf)
+{
+	struct keystone_rio_data *krio_priv =
+			(struct keystone_rio_data *)dev->platform_data;
+
+	if (!krio_priv)
+		return -EINVAL;
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", krio_priv->started);
+}
+
+static ssize_t keystone_rio_calibrate_store(struct device *dev,
+					    struct device_attribute *attr,
+					    const char *buf,
+					    size_t count)
+{
+	struct keystone_rio_data *krio_priv = (struct keystone_rio_data *)
+		dev->platform_data;
+	unsigned long new_calibrate;
+
+	if (kstrtoul(buf, 0, &new_calibrate))
+		return -EINVAL;
+
+	/* Start SRIO calibration */
+	if (new_calibrate && !krio_priv->started) {
+		int res;
+		u32 block;
+
+		/* Enable RIO SerDes blocks */
+		krio_write(krio_priv, gbl_en, 1);
+		for (block = KEYSTONE_RIO_BLK_PORT0_ID;
+		     block <= KEYSTONE_RIO_BLK_PORT3_ID; block++)
+			krio_write(krio_priv, blk[block].enable, 1);
+
+		/* Do SerDes initialization and calibration */
+		res = keystone_rio_serdes_init(
+			krio_priv->board_rio_cfg.serdes_baudrate,
+			1,
+			krio_priv);
+
+		if (res < 0)
+			dev_err(krio_priv->dev,
+				"calibration of SerDes failed\n");
+	}
+
+	return count;
+}
+
+static ssize_t keystone_rio_calibrate_show(struct device *dev,
+					   struct device_attribute *attr,
+					   char *buf)
+{
+	struct keystone_rio_data *krio_priv =
+			(struct keystone_rio_data *)dev->platform_data;
+
+	if (!krio_priv)
+		return -EINVAL;
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", krio_priv->calibrating);
+}
+
+static DEVICE_ATTR(ports,
+		   S_IRUGO | S_IWUSR,
+		   keystone_rio_ports_show,
+		   keystone_rio_ports_store);
+
+static DEVICE_ATTR(start,
+		   S_IRUGO | S_IWUSR,
+		   keystone_rio_start_show,
+		   keystone_rio_start_store);
+
+static DEVICE_ATTR(calibrate,
+		   S_IRUGO | S_IWUSR,
+		   keystone_rio_calibrate_show,
+		   keystone_rio_calibrate_store);
+
+static void keystone_rio_sysfs_remove(struct device *dev)
+{
+	device_remove_file(dev, &dev_attr_ports);
+	device_remove_file(dev, &dev_attr_start);
+	device_remove_file(dev, &dev_attr_calibrate);
+}
+
+static int keystone_rio_sysfs_create(struct device *dev)
+{
+	int res = 0;
+
+	res = device_create_file(dev, &dev_attr_ports);
+	if (res) {
+		dev_err(dev, "unable create sysfs ports file\n");
+		return res;
+	}
+
+	res = device_create_file(dev, &dev_attr_start);
+	if (res) {
+		dev_err(dev, "unable create sysfs start file\n");
+		return res;
+	}
+
+	res = device_create_file(dev, &dev_attr_calibrate);
+	if (res)
+		dev_err(dev, "unable create sysfs calibrate file\n");
+
+	return res;
+}
+
+/*
+ * Platform configuration setup
+ */
+static int keystone_rio_setup_controller(struct keystone_rio_data *krio_priv)
+{
+	u32 ports;
+	u32 p;
+	u32 baud;
+	u32 path_mode;
+	u32 size = 0;
+	int res = 0;
+	char str[8];
+	unsigned long flags;
+
+	size      = krio_priv->board_rio_cfg.size;
+	ports     = krio_priv->board_rio_cfg.ports;
+	baud      = krio_priv->board_rio_cfg.serdes_baudrate;
+	path_mode = krio_priv->board_rio_cfg.path_mode;
+
+	krio_priv->started = 1;
+
+	dev_dbg(krio_priv->dev, "size = %d, ports = 0x%x, baud = %d, path_mode = %d\n",
+		size, ports, baud, path_mode);
+
+	if (baud > KEYSTONE_SERDES_BAUD_5_000) {
+		baud = KEYSTONE_SERDES_BAUD_5_000;
+		dev_warn(krio_priv->dev,
+			 "invalid baud rate, forcing it to 5Gbps\n");
+	}
+
+	switch (baud) {
+	case KEYSTONE_SERDES_BAUD_1_250:
+		snprintf(str, sizeof(str), "1.25");
+		break;
+	case KEYSTONE_SERDES_BAUD_2_500:
+		snprintf(str, sizeof(str), "2.50");
+		break;
+	case KEYSTONE_SERDES_BAUD_3_125:
+		snprintf(str, sizeof(str), "3.125");
+		break;
+	case KEYSTONE_SERDES_BAUD_5_000:
+		snprintf(str, sizeof(str), "5.00");
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	dev_info(krio_priv->dev,
+		 "initializing %s Gbps interface with port configuration %d\n",
+		 str, path_mode);
+
+	/* Hardware set up of the controller */
+	res = keystone_rio_hw_init(baud, krio_priv);
+	if (res < 0) {
+		dev_err(krio_priv->dev,
+			"initialization of SRIO hardware failed\n");
+		return res;
+	}
+
+	/* Initialize port write interface */
+	res = keystone_rio_port_write_init(krio_priv);
+	if (res)
+		return res;
+
+	/* Disable all ports */
+	for (p = 0; p < KEYSTONE_RIO_MAX_PORT; p++)
+		keystone_rio_port_disable(p, krio_priv);
+
+	/* Register all configured ports */
+	krio_priv->ports_registering = krio_priv->board_rio_cfg.ports;
+	krio_priv->ports_scan_registering = 0;
+
+	/* Initialize interrupts */
+	res = keystone_rio_interrupt_setup(krio_priv);
+	if (res)
+		return res;
+
+	/* Start the controller */
+	keystone_rio_start(krio_priv);
+
+	while (ports) {
+		u32 port = __ffs(ports);
+
+		ports &= ~BIT(port);
+
+		res = keystone_rio_port_init(port, path_mode, krio_priv);
+		if (res < 0) {
+			dev_err(krio_priv->dev,
+				"initialization of port %d failed\n", port);
+			return res;
+		}
+
+		/* Start the port */
+		keystone_rio_port_enable(port, krio_priv);
+	}
+
+	/* Complete port initialization and wait link */
+	res = keystone_rio_port_chk(krio_priv, 1);
+	if (res) {
+		if (res == -1)
+			return -ENOMEM;
+
+		/* If port check failed schedule asynchronous periodic check */
+		spin_lock_irqsave(&krio_priv->port_chk_lock, flags);
+		krio_priv->port_chk_cnt =
+			krio_priv->board_rio_cfg.port_register_timeout /
+			(KEYSTONE_RIO_REGISTER_DELAY / HZ);
+		spin_unlock_irqrestore(&krio_priv->port_chk_lock, flags);
+
+		schedule_delayed_work(&krio_priv->port_chk_task,
+				      KEYSTONE_RIO_REGISTER_DELAY);
+	}
+
+	return res;
+}
+
+static int keystone_rio_probe(struct platform_device *pdev)
+{
+	struct device_node *node = pdev->dev.of_node;
+	struct keystone_rio_data *krio_priv;
+	int r;
+	u16 serdes_type;
+	void __iomem *regs;
+
+	dev_info(&pdev->dev, "KeyStone RapidIO driver %s\n", DRIVER_VER);
+
+	if (!node) {
+		dev_err(&pdev->dev, "could not find device info\n");
+		return -EINVAL;
+	}
+
+	krio_priv = devm_kzalloc(&pdev->dev,
+				 sizeof(struct keystone_rio_data),
+				 GFP_KERNEL);
+	if (!krio_priv) {
+		dev_err(&pdev->dev, "memory allocation failed\n");
+		return -ENOMEM;
+	}
+
+	platform_set_drvdata(pdev, krio_priv);
+	krio_priv->dev = &pdev->dev;
+
+	/* Get default config from device tree */
+	r = krio_of_parse(node, krio_priv);
+	if (r < 0) {
+		dev_err(&pdev->dev, "failed to get configuration\n");
+		return r;
+	}
+
+	serdes_type = krio_priv->board_rio_cfg.serdes_type;
+
+	/* SRIO main driver (global resources) */
+	krio_priv->lsu_free = krio_priv->lsu_start;
+	krio_priv->lsu_maint = keystone_rio_lsu_alloc(krio_priv);
+
+	mutex_init(&krio_priv->lsu_lock_maint);
+
+	spin_lock_init(&krio_priv->port_chk_lock);
+
+	INIT_DELAYED_WORK(&krio_priv->port_chk_task,
+			  keystone_rio_port_chk_task);
+	INIT_DELAYED_WORK(&krio_priv->pe_work, keystone_rio_pe_dpc);
+	INIT_WORK(&krio_priv->reset_work, keystone_rio_reset_dpc);
+
+#ifdef CONFIG_RAPIDIO_DMA_ENGINE
+	for (r = 0; r < KEYSTONE_RIO_LSU_NUM; r++)
+		INIT_LIST_HEAD(&krio_priv->dma_channels[r]);
+#endif
+
+	/* Initial base dev Id */
+	krio_priv->base_dev_id = 0x00ffffff;
+
+	regs = ioremap(krio_priv->board_rio_cfg.boot_cfg_regs_base,
+		       krio_priv->board_rio_cfg.boot_cfg_regs_size);
+
+	krio_priv->jtagid_reg = regs + 0x0018;
+
+	if (serdes_type == KEYSTONE_SERDES_TYPE_K1)
+		krio_priv->serdes_sts_reg = regs + 0x154;
+
+	regs = ioremap(krio_priv->board_rio_cfg.serdes_cfg_regs_base,
+		       krio_priv->board_rio_cfg.serdes_cfg_regs_size);
+	krio_priv->serdes_regs = regs;
+
+	regs = ioremap(krio_priv->board_rio_cfg.rio_regs_base,
+		       krio_priv->board_rio_cfg.rio_regs_size);
+	krio_priv->regs		     = regs;
+	krio_priv->car_csr_regs	     = regs + KEYSTONE_RIO_CAR_CSR_REGS;
+	krio_priv->serial_port_regs  = regs + KEYSTONE_RIO_SERIAL_PORT_REGS;
+	krio_priv->err_mgmt_regs     = regs + KEYSTONE_RIO_ERR_MGMT_REGS;
+	krio_priv->phy_regs	     = regs + KEYSTONE_RIO_PHY_REGS;
+	krio_priv->transport_regs    = regs + KEYSTONE_RIO_TRANSPORT_REGS;
+	krio_priv->pkt_buf_regs	     = regs + KEYSTONE_RIO_PKT_BUF_REGS;
+	krio_priv->evt_mgmt_regs     = regs + KEYSTONE_RIO_EVT_MGMT_REGS;
+	krio_priv->port_write_regs   = regs + KEYSTONE_RIO_PORT_WRITE_REGS;
+	krio_priv->link_regs	     = regs + KEYSTONE_RIO_LINK_REGS;
+	krio_priv->fabric_regs	     = regs + KEYSTONE_RIO_FABRIC_REGS;
+
+	/* Register SerDes */
+	r = keystone_rio_serdes_register(
+		serdes_type,
+		krio_priv->serdes_regs,
+		krio_priv->serdes_sts_reg,
+		&pdev->dev,
+		&krio_priv->serdes,
+		&krio_priv->board_rio_cfg.serdes_config);
+
+	if (r < 0) {
+		dev_err(&pdev->dev, "cannot register SerDes type %d\n",
+			serdes_type);
+		return -EINVAL;
+	}
+
+	dev_info(&pdev->dev, "using K%d SerDes\n",
+		 (serdes_type == KEYSTONE_SERDES_TYPE_K2) ? 2 : 1);
+
+	/* Enable SRIO clock */
+	krio_priv->clk = clk_get(&pdev->dev, "clk_srio");
+	if (IS_ERR(krio_priv->clk)) {
+		dev_err(&pdev->dev, "Unable to get Keystone SRIO clock\n");
+		return -EBUSY;
+	}
+
+	/* Workaround for K1 SRIO clocks */
+	clk_prepare_enable(krio_priv->clk);
+	ndelay(100);
+	clk_disable_unprepare(krio_priv->clk);
+	ndelay(100);
+	clk_prepare_enable(krio_priv->clk);
+
+	pdev->dev.platform_data = (void *)krio_priv;
+
+	keystone_rio_sysfs_create(&pdev->dev);
+
+	/* Setup the SRIO controller */
+	if (enable_ports) {
+		r = keystone_rio_setup_controller(krio_priv);
+		if (r < 0) {
+			clk_disable_unprepare(krio_priv->clk);
+			clk_put(krio_priv->clk);
+			return r;
+		}
+	}
+
+	return 0;
+}
+
+static void keystone_rio_shutdown_controller(
+	struct keystone_rio_data *krio_priv)
+{
+	u32 lanes = krio_priv->board_rio_cfg.lanes;
+	int i;
+
+	dev_dbg(krio_priv->dev, "shutdown controller\n");
+
+	/* Unregister interrupt handlers */
+	keystone_rio_interrupt_release(krio_priv);
+
+	/* Shutdown associated SerDes */
+	krio_priv->serdes.ops->shutdown_lanes(lanes, &krio_priv->serdes);
+
+	/* Stop the hw controller */
+	keystone_rio_stop(krio_priv);
+
+	/* Disable blocks */
+	krio_write(krio_priv, gbl_en, 0);
+	for (i = 0; i < KEYSTONE_RIO_BLK_NUM; i++) {
+		krio_write(krio_priv, blk[i].enable, 0);
+		while (krio_read(krio_priv, blk[i].status) & 0x1)
+			usleep_range(10, 50);
+	}
+
+	krio_priv->started = 0;
+}
+
+static void keystone_rio_shutdown(struct platform_device *pdev)
+{
+	struct keystone_rio_data *krio_priv = platform_get_drvdata(pdev);
+
+	if (krio_priv->started)
+		keystone_rio_shutdown_controller(krio_priv);
+
+	/* Wait current DMA transfers to finish */
+	mdelay(10);
+
+	if (krio_priv->clk) {
+		clk_disable_unprepare(krio_priv->clk);
+		clk_put(krio_priv->clk);
+	}
+}
+
+static int keystone_rio_remove(struct platform_device *pdev)
+{
+	struct keystone_rio_data *krio_priv = platform_get_drvdata(pdev);
+	u32 ports = krio_priv->board_rio_cfg.ports;
+
+	/* Shutdown the hw controller */
+	keystone_rio_shutdown(pdev);
+
+	flush_scheduled_work();
+
+	/* Retrieve all registered mports */
+	ports = krio_priv->board_rio_cfg.ports;
+	while (ports) {
+		struct rio_mport *mport;
+		u32 port = __ffs(ports);
+
+		ports &= ~BIT(port);
+
+		mport = krio_priv->mport[port];
+
+		if (mport) {
+#ifdef CONFIG_RAPIDIO_DMA_ENGINE
+			keystone_rio_lsu_dma_free_channel(mport);
+			keystone_rio_dma_unregister(mport);
+#endif
+		}
+	}
+
+	/* Remove io mapping */
+	iounmap(krio_priv->jtagid_reg);
+	iounmap(krio_priv->serdes_regs);
+	iounmap(krio_priv->regs);
+
+	/* Unregister sysfs and free mport private structures */
+	keystone_rio_serdes_unregister(&pdev->dev, &krio_priv->serdes);
+	keystone_rio_sysfs_remove(&pdev->dev);
+	platform_set_drvdata(pdev, NULL);
+	kfree(krio_priv);
+
+	return 0;
+}
+
+static const struct of_device_id of_match[] = {
+	{ .compatible = "ti,keystone-rapidio", },
+	{},
+};
+
+MODULE_DEVICE_TABLE(of, keystone_hwqueue_of_match);
+
+static struct platform_driver keystone_rio_driver = {
+	.driver = {
+		.name	        = "keystone-rapidio",
+		.of_match_table	= of_match,
+	},
+	.probe	= keystone_rio_probe,
+	.remove = keystone_rio_remove,
+	.shutdown = keystone_rio_shutdown,
+};
+module_platform_driver(keystone_rio_driver);
+
+MODULE_AUTHOR("Aurelien Jacquiot");
+MODULE_DESCRIPTION("TI KeyStone RapidIO device driver");
+MODULE_LICENSE("GPL");
diff --git a/drivers/rapidio/devices/keystone_rio.h b/drivers/rapidio/devices/keystone_rio.h
new file mode 100644
index 0000000..d19efc8
--- /dev/null
+++ b/drivers/rapidio/devices/keystone_rio.h
@@ -0,0 +1,981 @@
+/*
+ * Copyright (C) 2010, 2011, 2012, 2013, 2014 Texas Instruments Incorporated
+ * Authors: Aurelien Jacquiot <a-jacquiot@ti.com>
+ * - Main driver implementation.
+ * - Updated for support on TI KeyStone 2 platform.
+ *
+ * Copyright (C) 2012, 2013 Texas Instruments Incorporated
+ * WingMan Kwok <w-kwok2@ti.com>
+ * - Updated for support on TI KeyStone 1 platform.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation version 2.
+ *
+ * This program is distributed "as is" WITHOUT ANY WARRANTY of any
+ * kind, whether express or implied; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+#ifndef KEYSTONE_RIO_H
+#define KEYSTONE_RIO_H
+
+#include <asm/setup.h>
+#include <linux/cache.h>
+#include <linux/uaccess.h>
+#include <asm/irq.h>
+#include <linux/io.h>
+#include <linux/dmaengine.h>
+#include <linux/soc/ti/knav_qmss.h>
+#include <linux/soc/ti/knav_dma.h>
+
+#define KEYSTONE_RIO_MAP_FLAG_SEGMENT	  BIT(0)
+#define KEYSTONE_RIO_MAP_FLAG_SRC_PROMISC BIT(1)
+#define KEYSTONE_RIO_MAP_FLAG_TT_16	  BIT(13)
+#define KEYSTONE_RIO_MAP_FLAG_DST_PROMISC BIT(15)
+#define KEYSTONE_RIO_DESC_FLAG_TT_16	  BIT(9)
+
+#define KEYSTONE_RIO_BOOT_COMPLETE	  BIT(24)
+#define KEYSTONE_RIO_PER_RESTORE          BIT(4)
+#define KEYSTONE_RIO_PER_EN		  BIT(2)
+#define KEYSTONE_RIO_PER_FREE		  BIT(0)
+#define KEYSTONE_RIO_PEF_FLOW_CONTROL	  BIT(7)
+
+/*
+ * Packet types
+ */
+#define KEYSTONE_RIO_PACKET_TYPE_NREAD    0x24
+#define KEYSTONE_RIO_PACKET_TYPE_NWRITE   0x54
+#define KEYSTONE_RIO_PACKET_TYPE_NWRITE_R 0x55
+#define KEYSTONE_RIO_PACKET_TYPE_SWRITE   0x60
+#define KEYSTONE_RIO_PACKET_TYPE_DBELL    0xa0
+#define KEYSTONE_RIO_PACKET_TYPE_MAINT_R  0x80
+#define KEYSTONE_RIO_PACKET_TYPE_MAINT_W  0x81
+#define KEYSTONE_RIO_PACKET_TYPE_MAINT_RR 0x82
+#define KEYSTONE_RIO_PACKET_TYPE_MAINT_WR 0x83
+#define KEYSTONE_RIO_PACKET_TYPE_MAINT_PW 0x84
+
+#define KEYSTONE_RIO_PACKET_TYPE_MASK     0xff
+
+#define KEYSTONE_RIO_PACKET_TYPE(t)       ((t) & KEYSTONE_RIO_PACKET_TYPE_MASK)
+#define IS_KEYSTONE_RIO_PACKET_TYPE(t, p) (KEYSTONE_RIO_PACKET_TYPE(t) == p)
+
+/*
+ * LSU defines
+ */
+#define KEYSTONE_RIO_LSU_PRIO             0
+#define KEYSTONE_RIO_LSU_NUM              8
+
+#define KEYSTONE_RIO_LSU_BUSY_MASK        BIT(31)
+#define KEYSTONE_RIO_LSU_FULL_MASK        BIT(30)
+
+#define KEYSTONE_RIO_LSU_CC_MASK          0x0f
+#define KEYSTONE_RIO_LSU_CC_TIMEOUT       0x01
+#define KEYSTONE_RIO_LSU_CC_XOFF          0x02
+#define KEYSTONE_RIO_LSU_CC_ERROR         0x03
+#define KEYSTONE_RIO_LSU_CC_INVALID       0x04
+#define KEYSTONE_RIO_LSU_CC_DMA           0x05
+#define KEYSTONE_RIO_LSU_CC_RETRY         0x06
+#define KEYSTONE_RIO_LSU_CC_CANCELED      0x07
+
+/*
+ * Max ports configuration per path modes
+ */
+#define KEYSTONE_RIO_MAX_PORTS_PATH_MODE_0	0xf /* 4 ports:    4-1x    */
+#define KEYSTONE_RIO_MAX_PORTS_PATH_MODE_1	0xd /* 3 ports: 2-1x, 1-2x */
+#define KEYSTONE_RIO_MAX_PORTS_PATH_MODE_2	0x7 /* 3 ports: 1-2x, 2-1x */
+#define KEYSTONE_RIO_MAX_PORTS_PATH_MODE_3	0x5 /* 2 ports: 1-2x, 1-2x */
+#define KEYSTONE_RIO_MAX_PORTS_PATH_MODE_4      0x1 /* 1 ports:       1-4x */
+
+/*
+ * Register range defines
+ */
+#define KEYSTONE_RIO_CAR_CSR_REGS         0x0b000
+#define KEYSTONE_RIO_SERIAL_PORT_REGS     0x0b100
+#define KEYSTONE_RIO_ERR_MGMT_REGS        0x0c000
+#define KEYSTONE_RIO_PHY_REGS             0x1b000
+#define KEYSTONE_RIO_TRANSPORT_REGS       0x1b300
+#define KEYSTONE_RIO_PKT_BUF_REGS         0x1b600
+#define KEYSTONE_RIO_EVT_MGMT_REGS        0x1b900
+#define KEYSTONE_RIO_PORT_WRITE_REGS      0x1ba00
+#define KEYSTONE_RIO_LINK_REGS            0x1bd00
+#define KEYSTONE_RIO_FABRIC_REGS          0x1be00
+
+/*
+ * Various RIO defines
+ */
+#define KEYSTONE_RIO_DBELL_NUMBER         4
+#define KEYSTONE_RIO_DBELL_VALUE_MAX      (KEYSTONE_RIO_DBELL_NUMBER * 16)
+#define KEYSTONE_RIO_DBELL_MASK           (KEYSTONE_RIO_DBELL_VALUE_MAX - 1)
+
+#define KEYSTONE_RIO_TIMEOUT_CNT	  1000
+#define KEYSTONE_RIO_TIMEOUT_MSEC         100
+#define KEYSTONE_RIO_TIMEOUT_NSEC         1000
+#define KEYSTONE_RIO_RETRY_CNT            1000
+#define KEYSTONE_RIO_REGISTER_DELAY	  (3 * HZ)
+
+/*
+ * RIO error, reset and special event interrupt defines
+ */
+#define KEYSTONE_RIO_PORT_ERROR_OUT_PKT_DROP		BIT(26)
+#define KEYSTONE_RIO_PORT_ERROR_OUT_FAILED		BIT(25)
+#define KEYSTONE_RIO_PORT_ERROR_OUT_DEGRADED		BIT(24)
+#define KEYSTONE_RIO_PORT_ERROR_OUT_RETRY		BIT(20)
+#define KEYSTONE_RIO_PORT_ERROR_OUT_ERROR		BIT(17)
+#define KEYSTONE_RIO_PORT_ERROR_IN_ERROR		BIT(9)
+#define KEYSTONE_RIO_PORT_ERROR_PW_PENDING		BIT(4)
+#define KEYSTONE_RIO_PORT_ERROR_PORT_ERR		BIT(2)
+
+#define KEYSTONE_RIO_PORT_ERROR_MASK			\
+	(KEYSTONE_RIO_PORT_ERROR_OUT_PKT_DROP	|	\
+	 KEYSTONE_RIO_PORT_ERROR_OUT_FAILED	|	\
+	 KEYSTONE_RIO_PORT_ERROR_OUT_DEGRADED	|	\
+	 KEYSTONE_RIO_PORT_ERROR_OUT_RETRY	|	\
+	 KEYSTONE_RIO_PORT_ERROR_OUT_ERROR	|	\
+	 KEYSTONE_RIO_PORT_ERROR_IN_ERROR	|	\
+	 KEYSTONE_RIO_PORT_ERROR_PW_PENDING	|	\
+	 KEYSTONE_RIO_PORT_ERROR_PORT_ERR)
+
+#define KEYSTONE_RIO_PORT_ERRORS		\
+	(RIO_PORT_N_ERR_STS_PW_OUT_ES	|	\
+	 RIO_PORT_N_ERR_STS_PW_INP_ES	|	\
+	 RIO_PORT_N_ERR_STS_PW_PEND	|	\
+	 RIO_PORT_N_ERR_STS_PORT_ERR)
+
+/* RIO PLM port event status defines */
+#define KEYSTONE_RIO_PORT_PLM_STATUS_MAX_DENIAL        BIT(31)
+#define KEYSTONE_RIO_PORT_PLM_STATUS_LINK_INIT         BIT(28)
+#define KEYSTONE_RIO_PORT_PLM_STATUS_DLT               BIT(27)
+#define KEYSTONE_RIO_PORT_PLM_STATUS_PORT_ERR          BIT(26)
+#define KEYSTONE_RIO_PORT_PLM_STATUS_OUTPUT_FAIL       BIT(25)
+#define KEYSTONE_RIO_PORT_PLM_STATUS_OUTPUT_DEGR       BIT(24)
+#define KEYSTONE_RIO_PORT_PLM_STATUS_RST_REQ           BIT(16)
+#define KEYSTONE_RIO_PORT_PLM_STATUS_MECS              BIT(12)
+
+#define KEYSTONE_RIO_PORT_PLM_STATUS_ERRORS			\
+	(KEYSTONE_RIO_PORT_PLM_STATUS_MAX_DENIAL      |		\
+	 KEYSTONE_RIO_PORT_PLM_STATUS_PORT_ERR        |		\
+	 KEYSTONE_RIO_PORT_PLM_STATUS_OUTPUT_FAIL     |		\
+	 KEYSTONE_RIO_PORT_PLM_STATUS_OUTPUT_DEGR)
+
+#define KEYSTONE_RIO_SP_HDR_NEXT_BLK_PTR	0x1000
+#define KEYSTONE_RIO_SP_HDR_EP_REC_ID		0x0002
+#define KEYSTONE_RIO_ERR_HDR_NEXT_BLK_PTR	0x3000
+#define KEYSTONE_RIO_ERR_EXT_FEAT_ID		0x0007
+
+/*
+ * RapidIO global definitions
+ */
+#define KEYSTONE_RIO_MAX_PORT		4
+#define KEYSTONE_RIO_MAX_MBOX		4    /* 4 in multi-segment,
+					      * 64 in single-seg
+					      */
+#define KEYSTONE_RIO_MAX_PKT_FW_ENTRIES 8    /* max of packet forwarding
+					      * mapping entries
+					      */
+#define KEYSTONE_RIO_MAINT_BUF_SIZE	64
+#define KEYSTONE_RIO_MSG_SSIZE		0xe
+#define KEYSTONE_RIO_SGLIST_SIZE	3
+
+#define KEYSTONE_RIO_PKT_FW_BRR_NUM     1    /* BRR used for packet fwd */
+
+#define KEYSTONE_RIO_RXU_MAP_MIN	0
+#define KEYSTONE_RIO_RXU_MAP_MAX	63   /* RXU mapping range */
+
+/*
+ * RapidIO logical block definitions
+ */
+#define KEYSTONE_RIO_BLK_MMR_ID         0
+#define KEYSTONE_RIO_BLK_LSU_ID         1
+#define KEYSTONE_RIO_BLK_MAU_ID         2
+#define KEYSTONE_RIO_BLK_TXU_ID         3
+#define KEYSTONE_RIO_BLK_RXU_ID         4
+#define KEYSTONE_RIO_BLK_PORT0_ID       5
+#define KEYSTONE_RIO_BLK_PORT1_ID       6
+#define KEYSTONE_RIO_BLK_PORT2_ID       7
+#define KEYSTONE_RIO_BLK_PORT3_ID       8
+
+#define KEYSTONE_RIO_BLK_NUM		9
+
+/*
+ * DMA engine definition
+ */
+#define KEYSTONE_RIO_DMA_MAX_CHANNEL    8    /* Num of (virtual) DMA channels */
+
+/* Max number of virtual DMA descriptors */
+#ifdef ARCH_HAS_SG_CHAIN
+#define KEYSTONE_RIO_DMA_MAX_DESC       1024
+#else
+#define KEYSTONE_RIO_DMA_MAX_DESC       SG_MAX_SINGLE_ALLOC
+#endif
+
+/*
+ * Dev Id and dev revision
+ */
+#define KEYSTONE_RIO_DEV_ID_VAL	\
+	((((__raw_readl(krio_priv->jtagid_reg)) << 4)  & 0xffff0000) | 0x30)
+
+#define KEYSTONE_RIO_DEV_INFO_VAL \
+	(((__raw_readl(krio_priv->jtagid_reg)) >> 28) & 0xf)
+
+#define KEYSTONE_RIO_ID_TI		(0x00000030)
+#define KEYSTONE_RIO_EXT_FEAT_PTR	(0x00000100)
+
+#define KEYSTONE_RIO_MAX_DIO_PKT_SIZE   0x100000 /* hw support up to 1MB */
+#define KEYSTONE_RIO_DIO_ALIGNMENT      8        /* hw required alignment */
+
+/*
+ * RIO error, reset and special event interrupt defines
+ */
+#define KEYSTONE_RIO_ERR_RST_EVNT_MASK  0x00010f07
+
+/* Refer to bits in KEYSTONE_RIO_ERR_RST_EVNT_MASK */
+#define KEYSTONE_RIO_RESET_INT          16  /* reset interrupt on any port */
+#define KEYSTONE_RIO_PORT3_ERROR_INT    11  /* port 3 error */
+#define KEYSTONE_RIO_PORT2_ERROR_INT    10  /* port 2 error */
+#define KEYSTONE_RIO_PORT1_ERROR_INT    9   /* port 1 error */
+#define KEYSTONE_RIO_PORT0_ERROR_INT    8   /* port 0 error */
+#define KEYSTONE_RIO_EVT_CAP_ERROR_INT  2   /* error management event capture */
+#define KEYSTONE_RIO_PORT_WRITEIN_INT   1   /* port-write-in request received */
+#define KEYSTONE_RIO_MCAST_EVT_INT      0   /* multicast event control symbol */
+
+/*
+ * Interrupts and DMA event mapping
+ */
+#define KEYSTONE_GEN_RIO_INT            0   /* RIO int for generic events */
+#define KEYSTONE_LSU_RIO_INT            1   /* RIO int for LSU events */
+
+/* Mask for error and good completion LSU interrupts */
+#define KEYSTONE_RIO_ICSR_LSU0_ERROR_MASK     0xffff0000
+#define KEYSTONE_RIO_ICSR_LSU0_COMPLETE_MASK  0x0000ffff
+#define KEYSTONE_RIO_ICSR_LSU1_COMPLETE_MASK  0x000000ff
+
+/*
+ * Definition of the different RapidIO packet types according to the RapidIO
+ * specification 2.0
+ */
+#define RIO_PACKET_TYPE_STREAM          9  /* Data Streaming */
+#define RIO_PACKET_TYPE_MESSAGE         11 /* Message */
+
+/*
+ * Routing configuration for packet forwarding
+ */
+struct keystone_routing_config {
+	u16 dev_id_low;
+	u16 dev_id_high;
+	u8  port;
+};
+
+/*
+ * Per board RIO devices controller configuration
+ */
+struct keystone_rio_board_controller_info {
+	u32		rio_regs_base;
+	u32		rio_regs_size;
+
+	u32		boot_cfg_regs_base;
+	u32		boot_cfg_regs_size;
+
+	u32		serdes_cfg_regs_base;
+	u32		serdes_cfg_regs_size;
+
+	/* bitfield of port(s) to probe on this controller */
+	u32             ports;
+	/* remote link partner port numbers */
+	int             ports_remote[KEYSTONE_RIO_MAX_PORT];
+
+	u32             id;     /* host id */
+	u32             size;   /* RapidIO common transport system size.
+				 * 0 - Small size. 256 devices.
+				 * 1 - Large size, 65536 devices.
+				 */
+	u16             serdes_type;
+	u32             serdes_baudrate;
+	u32             serdes_calibration;
+	u32             lanes;
+	u32             path_mode;
+	u32             port_register_timeout;
+	u32             pkt_forwarding;
+	u32             dma_channel_num; /* DMA channels for DIO transfers */
+
+	int             rio_irq;
+	int             lsu_irq;
+
+	struct keystone_serdes_config  serdes_config;
+	struct keystone_routing_config routing_config[8];
+};
+
+struct keystone_rio_data;
+
+struct keystone_rio_packet {
+	struct scatterlist		sg[KEYSTONE_RIO_SGLIST_SIZE];
+	int				sg_ents;
+	u32				epib[4];
+	u32				psdata[2];
+	u32				mbox;
+	u32                             slot;
+	void			       *buff;
+	struct keystone_rio_data       *priv;
+	enum dma_status			status;
+	dma_cookie_t			cookie;
+};
+
+struct keystone_rio_mbox_info {
+	struct rio_mport *port;
+	u32               running;
+	u32               entries;
+	atomic_t          slot;
+	void             *dev_id;
+	int		  rxu_map_id[2];
+};
+
+struct keystone_rio_rx_chan_info {
+	struct keystone_rio_data *priv;
+	void                     *channel;
+	void			 *queue;      /* Rx completion queue */
+	const char		 *name;
+	struct tasklet_struct	  tasklet;
+	void			 *pool;
+	u32                       pool_size;
+	u32                       pool_region_id;
+	int                       mbox_id;
+	void                     *fdq[KNAV_DMA_FDQ_PER_CHAN];
+	u32			  queue_depths[KNAV_DMA_FDQ_PER_CHAN];
+	u32			  buffer_sizes[KNAV_DMA_FDQ_PER_CHAN];
+	int			  flow_id;
+	int			  queue_id;   /* Rx completion queue */
+	u32                       packet_type;
+	int                       stream_id;  /* stream id for type 9 packet */
+};
+
+struct keystone_rio_tx_chan_info {
+	struct keystone_rio_data *priv;
+	void                     *channel;
+	void			 *complet_queue;    /* Tx completion queue */
+	void                     *queue;            /* Tx submit queue */
+	void                     *garbage_queue;    /* Tx garbage queue */
+	const char	         *name;
+	void			 *pool;
+	u32                       pool_size;
+	u32                       pool_region_id;
+	int                       mbox_id;
+	u32			  queue_depth;
+	u32                       complet_queue_id; /* Tx completion queue */
+	u32                       queue_id;         /* Tx submit queue */
+	u32                       garbage_queue_id; /* Tx garbage queue */
+};
+
+struct port_write_msg {
+	union rio_pw_msg msg;
+	u32              msg_count;
+	u32              err_count;
+	u32              discard_count;
+};
+
+#ifdef CONFIG_RAPIDIO_DMA_ENGINE
+
+/*
+ * Special DMA device control operation to prepare a raw packet
+ * (used for dbell)
+ */
+#define DMA_KEYSTONE_RIO_PREP_RAW_PACKET 0x1000
+
+enum keystone_rio_chan_state {
+	RIO_CHAN_STATE_UNUSED,
+	RIO_CHAN_STATE_ACTIVE,
+	RIO_CHAN_STATE_RUNNING,
+	RIO_CHAN_STATE_WAITING,
+};
+
+/*
+ * DMA engine data
+ */
+struct keystone_rio_dma_desc {
+	struct list_head	        node;
+	struct list_head	        tx_list;
+	bool			        last;
+	struct dma_async_tx_descriptor  adesc;
+	enum dma_status		        status;
+	u32                             retry_count;
+	dma_addr_t                      buff_addr;
+	u32                             size;
+	u16                             port_id;
+	u16			        dest_id;
+	u64			        rio_addr;
+	u8			        rio_addr_u;
+	u8                              sys_size;
+	u32                             lsu_context;
+	u32                             packet_type;
+} ____cacheline_aligned;
+
+struct keystone_rio_dma_chan {
+	struct list_head                node;
+	enum dma_transfer_direction     direction;
+	atomic_t		        state;
+	struct dma_chan		        dchan;
+	u8                              lsu;
+	struct list_head	        active_list;
+	struct list_head	        queue;
+	struct keystone_rio_dma_desc   *current_transfer;
+	dma_cookie_t		        completed_cookie;
+	/* to protect against concurrent access to dma channel */
+	spinlock_t		        lock;
+	struct tasklet_struct	        tasklet;
+	struct keystone_rio_data       *krio;
+};
+
+struct keystone_rio_dma_packet_raw {
+	u16                             port_id;
+	u16			        dest_id;
+	u64			        rio_addr;
+	u8			        rio_addr_u;
+	dma_addr_t                      buff_addr;
+	u32                             size;
+	u8                              sys_size;
+	u32                             packet_type;
+	struct dma_async_tx_descriptor *tx;
+};
+
+#define from_dma_chan(ch) container_of(ch, struct keystone_rio_dma_chan, dchan)
+#define to_dma_chan(ch)	  (&(ch)->dchan)
+#define chan_dev(ch)	  (&to_dma_chan(ch)->dev->device)
+#define chan_id(ch)	  (to_dma_chan(ch)->chan_id)
+#define chan_name(ch)	  ((ch)->dchan.name)
+
+void keystone_rio_dma_interrupt_handler(struct keystone_rio_data *krio_priv,
+					u32 lsu, u32 error);
+
+int keystone_rio_dma_prep_raw_packet(struct dma_chan *dchan,
+				     struct keystone_rio_dma_packet_raw *pkt);
+
+int keystone_rio_dma_register(struct rio_mport *mport, int channel_num);
+
+void keystone_rio_dma_unregister(struct rio_mport *mport);
+
+u8 keystone_rio_lsu_alloc(struct keystone_rio_data *krio_priv);
+
+int keystone_rio_lsu_complete_transfer(int lsu, u32 lsu_context,
+				       struct keystone_rio_data *krio_priv);
+
+int keystone_rio_lsu_start_transfer(int lsu,
+				    int port_id,
+				    u16 dest_id,
+				    dma_addr_t src_addr,
+				    u64 tgt_addr,
+				    u32 size_bytes,
+				    int size,
+				    u32 packet_type,
+				    u32 *lsu_context,
+				    int interrupt_req,
+				    struct keystone_rio_data *krio_priv);
+#endif /* CONFIG_RAPIDIO_DMA_ENGINE */
+
+/*
+ * Main KeyStone RapidIO driver data
+ *   ports: port mask of those ports defined in dts and
+ *          have good hw port status
+ */
+struct keystone_rio_data {
+	struct device	       *dev;
+	struct rio_mport       *mport[KEYSTONE_RIO_MAX_PORT];
+	struct clk	       *clk;
+
+	u32                     started;
+	u32                     calibrating;
+
+	/* To protect against concurrent lsu maint access */
+	struct mutex		lsu_lock_maint;
+	u8                      lsu_start;
+	u8                      lsu_end;
+	u8                      lsu_free;
+	u8                      lsu_maint;
+
+	u32			rio_pe_feat;
+	u32                     base_dev_id;
+
+	struct port_write_msg	port_write_msg;
+	struct work_struct	pw_work;
+	struct kfifo		pw_fifo;
+	/* to protect against concurrent port write */
+	spinlock_t		pw_fifo_lock;
+
+	u32                     pe_ports;
+	u32                     pe_cnt;
+	struct delayed_work     pe_work;
+
+	struct work_struct      reset_work;
+
+	u32                     ports;
+	u32			ports_scan_registering;
+	u32			ports_registering;
+
+	/* to protect against concurrent port check */
+	spinlock_t		port_chk_lock;
+	u32			port_chk_cnt;
+	struct delayed_work	port_chk_task;
+
+	unsigned long		rxu_map_start;
+	unsigned long		rxu_map_end;
+	unsigned long		rxu_map_bitmap[2];
+
+#ifdef CONFIG_RAPIDIO_DMA_ENGINE
+	struct list_head        dma_channels[KEYSTONE_RIO_LSU_NUM];
+	struct dma_chan        *dma_chan[KEYSTONE_RIO_MAX_PORT];
+#endif
+
+	u32                              num_mboxes;
+	struct keystone_rio_mbox_info    tx_mbox[KEYSTONE_RIO_MAX_MBOX];
+	struct keystone_rio_mbox_info    rx_mbox[KEYSTONE_RIO_MAX_MBOX];
+	struct keystone_rio_rx_chan_info rx_channels[KEYSTONE_RIO_MAX_MBOX];
+	struct keystone_rio_tx_chan_info tx_channels[KEYSTONE_RIO_MAX_MBOX];
+
+	u32 __iomem					*jtagid_reg;
+	u32 __iomem					*serdes_sts_reg;
+	void __iomem	                                *serdes_regs;
+	struct keystone_rio_regs __iomem	        *regs;
+
+	struct keystone_rio_car_csr_regs __iomem	*car_csr_regs;
+	struct keystone_rio_serial_port_regs __iomem	*serial_port_regs;
+	struct keystone_rio_err_mgmt_regs __iomem	*err_mgmt_regs;
+	struct keystone_rio_phy_layer_regs __iomem	*phy_regs;
+	struct keystone_rio_transport_layer_regs __iomem *transport_regs;
+	struct keystone_rio_pkt_buf_regs __iomem	*pkt_buf_regs;
+	struct keystone_rio_evt_mgmt_regs __iomem	*evt_mgmt_regs;
+	struct keystone_rio_port_write_regs __iomem	*port_write_regs;
+	struct keystone_rio_link_layer_regs __iomem	*link_regs;
+	struct keystone_rio_fabric_regs __iomem		*fabric_regs;
+
+	struct keystone_rio_board_controller_info	 board_rio_cfg;
+
+	struct keystone_serdes_data                      serdes;
+	/* To protect against concurrent accesses */
+	struct mutex					lsu_lock_dbell;
+	u8						lsu_dbell;
+};
+
+struct keystone_lane_config {
+	int start; /* lane start number of the port */
+	int end;   /* lane end number of the port */
+};
+
+/*
+ * RapidIO Registers
+ */
+
+/* RIO Registers  0000 - 2fff */
+struct keystone_rio_regs {
+/* Required Peripheral Registers */
+	u32	pid;			/* 0000 */
+	u32	pcr;			/* 0004 */
+	u32	__rsvd0[3];		/* 0008 - 0010 */
+
+/* Peripheral Setting Control Registers */
+	u32	per_set_cntl;		/* 0014 */
+	u32	per_set_cntl1;		/* 0018 */
+
+	u32	__rsvd1[2];		/* 001c - 0020 */
+
+	u32	gbl_en;			/* 0024 */
+	u32	gbl_en_stat;		/* 0028 */
+
+	struct {
+		u32 enable;		/* 002c */
+		u32 status;		/* 0030 */
+	} blk[10];			/* 002c - 0078 */
+
+	/* ID Registers */
+	u32	__rsvd2[17];		/* 007c - 00bc */
+	u32	multiid_reg[8];		/* 00c0 - 00dc */
+
+/* Hardware Packet Forwarding Registers */
+	struct {
+		u32	pf_16b;
+		u32	pf_8b;
+	} pkt_fwd_cntl[8];		/* 00e0 - 011c */
+
+	u32	__rsvd3[24];		/* 0120 - 017c */
+
+/* Interrupt Registers */
+	struct {
+		u32	status;
+		u32	__rsvd0;
+		u32	clear;
+		u32	__rsvd1;
+	} doorbell_int[4];		/* 0180 - 01bc */
+
+	struct {
+		u32	status;
+		u32	__rsvd0;
+		u32	clear;
+		u32	__rsvd1;
+	} lsu_int[2];			/* 01c0 - 01dc */
+
+	u32	err_rst_evnt_int_stat;	/* 01e0 */
+	u32	__rsvd4;
+	u32	err_rst_evnt_int_clear;	/* 01e8 */
+	u32	__rsvd5;
+
+	u32	__rsvd6[4];		/* 01f0 - 01fc */
+
+	struct {
+		u32 route;		/* 0200 */
+		u32 route2;		/* 0204 */
+		u32 __rsvd;		/* 0208 */
+	} doorbell_int_route[4];	/* 0200 - 022c */
+
+	u32	lsu0_int_route[4];		/* 0230 - 023c */
+	u32	lsu1_int_route1;		/* 0240 */
+
+	u32	__rsvd7[3];		/* 0244 - 024c */
+
+	u32	err_rst_evnt_int_route[3];	/* 0250 - 0258 */
+
+	u32	__rsvd8[2];		/* 025c - 0260 */
+
+	u32	interrupt_ctl;		/* 0264 */
+
+	u32	__rsvd9[26];		/* 0268, 026c, 0270 - 02cc */
+
+	u32	intdst_rate_cntl[16];	/* 02d0 - 030c */
+	u32	intdst_rate_disable;	/* 0310 */
+
+	u32	__rsvd10[59];		/* 0314 - 03fc */
+
+/* RXU Registers */
+	struct {
+		u32	ltr_mbox_src;
+		u32	dest_prom_seg;
+		u32	flow_qid;
+	} rxu_map[64];			/* 0400 - 06fc */
+
+	struct {
+		u32	cos_src;
+		u32	dest_prom;
+		u32	stream;
+	} rxu_type9_map[64];		/* 0700 - 09fc */
+
+	u32	__rsvd11[192];		/* 0a00 - 0cfc */
+
+/* LSU/MAU Registers */
+	struct {
+		u32 addr_msb;		/* 0d00 */
+		u32 addr_lsb_cfg_ofs;	/* 0d04 */
+		u32 phys_addr;		/* 0d08 */
+		u32 dbell_val_byte_cnt;	/* 0d0c */
+		u32 destid;		/* 0d10 */
+		u32 dbell_info_fttype;	/* 0d14 */
+		u32 busy_full;		/* 0d18 */
+	} lsu_reg[8];			/* 0d00 - 0ddc */
+
+	u32	lsu_setup_reg[2];	/* 0de0 - 0de4 */
+	u32	lsu_stat_reg[6];	/* 0de8 - 0dfc */
+	u32	lsu_flow_masks[4];	/* 0e00 - 0e0c */
+
+	u32	__rsvd12[16];		/* 0e10 - 0e4c */
+
+/* Flow Control Registers */
+	u32	flow_cntl[16];		/* 0e50 - 0e8c */
+	u32	__rsvd13[8];		/* 0e90 - 0eac */
+
+/* TXU Registers 0eb0 - 0efc */
+	u32	tx_cppi_flow_masks[8];	/* 0eb0 - 0ecc */
+	u32	tx_queue_sch_info[4];	/* 0ed0 - 0edc */
+	u32	garbage_coll_qid[3];	/* 0ee0 - 0ee8 */
+
+	u32	__rsvd14[69];		/* 0eec, 0ef0 - 0ffc */
+
+};
+
+/* CDMAHP Registers 1000 - 2ffc */
+struct keystone_rio_pktdma_regs {
+	u32	__rsvd[2048];		/* 1000 - 2ffc */
+};
+
+/* CSR/CAR Registers  b000+ */
+struct keystone_rio_car_csr_regs {
+	u32	dev_id;			/* b000 */
+	u32	dev_info;		/* b004 */
+	u32	assembly_id;		/* b008 */
+	u32	assembly_info;		/* b00c */
+	u32	pe_feature;		/* b010 */
+
+	u32	sw_port;		/* b014 */
+
+	u32	src_op;			/* b018 */
+	u32	dest_op;		/* b01c */
+
+	u32	__rsvd1[7];		/* b020 - b038 */
+
+	u32	data_stm_info;		/* b03c */
+
+	u32	__rsvd2[2];		/* b040 - b044 */
+
+	u32	data_stm_logical_ctl;	/* b048 */
+	u32	pe_logical_ctl;		/* b04c */
+
+	u32	__rsvd3[2];		/* b050 - b054 */
+
+	u32	local_cfg_hbar;		/* b058 */
+	u32	local_cfg_bar;		/* b05c */
+
+	u32	base_dev_id;		/* b060 */
+	u32	__rsvd4;
+	u32	host_base_id_lock;	/* b068 */
+	u32	component_tag;		/* b06c */
+					/* b070 - b0fc */
+};
+
+struct keystone_rio_serial_port_regs {
+	u32	sp_maint_blk_hdr;	/* b100 */
+	u32	__rsvd6[7];		/* b104 - b11c */
+
+	u32	sp_link_timeout_ctl;	/* b120 */
+	u32	sp_rsp_timeout_ctl;	/* b124 */
+	u32	__rsvd7[5];		/* b128 - b138 */
+	u32	sp_gen_ctl;		/* b13c */
+
+	struct {
+		u32	link_maint_req;	/* b140 */
+		u32	link_maint_resp;/* b144 */
+		u32	ackid_stat;	/* b148 */
+		u32	__rsvd[2];	/* b14c - b150 */
+		u32	ctl2;		/* b154 */
+		u32	err_stat;	/* b158 */
+		u32	ctl;		/* b15c */
+	} sp[4];			/* b140 - b1bc */
+
+					/* b1c0 - bffc */
+};
+
+struct keystone_rio_err_mgmt_regs {
+	u32	err_report_blk_hdr;	/* c000 */
+	u32	__rsvd9;
+	u32	err_det;		/* c008 */
+	u32	err_en;			/* c00c */
+	u32	h_addr_capt;		/* c010 */
+	u32	addr_capt;		/* c014 */
+	u32	id_capt;		/* c018 */
+	u32	ctrl_capt;		/* c01c */
+	u32	__rsvd10[2];		/* c020 - c024 */
+	u32	port_write_tgt_id;	/* c028 */
+	u32	__rsvd11[5];		/* c02c - c03c */
+
+	struct {
+		u32	det;		/* c040 */
+		u32	rate_en;	/* c044 */
+		u32	attr_capt_dbg0;	/* c048 */
+		u32	capt_0_dbg1;	/* c04c */
+		u32	capt_1_dbg2;	/* c050 */
+		u32	capt_2_dbg3;	/* c054 */
+		u32	capt_3_dbg4;	/* c058 */
+		u32	__rsvd0[3];	/* c05c - c064 */
+		u32	rate;		/* c068 */
+		u32	thresh;		/* c06c */
+		u32	__rsvd1[4];	/* c070 - c07c */
+	} sp_err[4];			/* c040 - c13c */
+
+	u32	__rsvd12[1972];		/* c140 - e00c */
+
+	struct {
+		u32	stat0;		/* e010 */
+		u32	stat1;		/* e014 */
+		u32	__rsvd[6];	/* e018 - e02c */
+	} lane_stat[4];			/* e010 - e08c */
+
+					/* e090 - 1affc */
+};
+
+struct keystone_rio_phy_layer_regs {
+	u32	phy_blk_hdr;		/* 1b000 */
+	u32	__rsvd14[31];		/* 1b004 - 1b07c */
+	struct {
+		u32	imp_spec_ctl;	/* 1b080 */
+		u32	pwdn_ctl;	/* 1b084 */
+		u32	__rsvd0[2];
+
+		u32	status;		/* 1b090 */
+		u32	int_enable;	/* 1b094 */
+		u32	port_wr_enable;	/* 1b098 */
+		u32	event_gen;	/* 1b09c */
+
+		u32	all_int_en;	/* 1b0a0 */
+		u32	all_port_wr_en;	/* 1b0a4 */
+		u32	__rsvd1[2];
+
+		u32	path_ctl;	/* 1b0b0 */
+		u32	discovery_timer;/* 1b0b4 */
+		u32	silence_timer;	/* 1b0b8 */
+		u32	vmin_exp;	/* 1b0bc */
+
+		u32	pol_ctl;	/* 1b0c0 */
+		u32	__rsvd2;
+		u32	denial_ctl;	/* 1b0c8 */
+		u32	__rsvd3;
+
+		u32	rcvd_mecs;	/* 1b0d0 */
+		u32	__rsvd4;
+		u32	mecs_fwd;	/* 1b0d8 */
+		u32	__rsvd5;
+
+		u32	long_cs_tx1;	/* 1b0e0 */
+		u32	long_cs_tx2;	/* 1b0e4 */
+		u32	__rsvd[6];	/* 1b0e8, 1b0ec, 1b0f0 - 1b0fc */
+	} phy_sp[4];			/* 1b080 - 1b27c */
+
+					/* 1b280 - 1b2fc */
+};
+
+struct keystone_rio_transport_layer_regs {
+	u32	transport_blk_hdr;	/* 1b300 */
+	u32	__rsvd16[31];		/* 1b304 - 1b37c */
+
+	struct {
+		u32	control;	/*1b380 */
+		u32	__rsvd0[3];
+
+		u32	status;		/* 1b390 */
+		u32	int_enable;	/* 1b394 */
+		u32	port_wr_enable;	/* 1b398 */
+		u32	event_gen;	/* 1b39c */
+
+		struct {
+			u32	ctl;		/* 1b3a0 */
+			u32	pattern_match;	/* 1b3a4 */
+			u32	__rsvd[2];	/* 1b3a8 - 1b3ac */
+		} base_route[4];		/* 1b3a0 - 1b3dc */
+
+		u32	__rsvd1[8];		/* 1b3e0 - 1b3fc */
+
+	} transport_sp[4];			/* 1b380 - 1b57c */
+
+						/* 1b580 - 1b5fc */
+};
+
+struct keystone_rio_pkt_buf_regs {
+	u32	pkt_buf_blk_hdr;	/* 1b600 */
+	u32	__rsvd18[31];		/* 1b604 - 1b67c */
+
+	struct {
+		u32	control;	/* 1b680 */
+		u32	__rsvd0[3];
+
+		u32	status;		/* 1b690 */
+		u32	int_enable;	/* 1b694 */
+		u32	port_wr_enable;	/* 1b698 */
+		u32	event_gen;	/* 1b69c */
+
+		u32	ingress_rsc;	/* 1b6a0 */
+		u32	egress_rsc;	/* 1b6a4 */
+		u32	__rsvd1[2];
+
+		u32	ingress_watermark[4];	/* 1b6b0 - 1b6bc */
+		u32	__rsvd2[16];	/* 1b6c0 - 1b6fc */
+
+	} pkt_buf_sp[4];		/* 1b680 - 1b87c */
+
+					/* 1b880 - 1b8fc */
+};
+
+struct keystone_rio_evt_mgmt_regs {
+	u32	evt_mgmt_blk_hdr;	/* 1b900 */
+	u32	__rsvd20[3];
+
+	u32	evt_mgmt_int_stat;	/* 1b910 */
+	u32	evt_mgmt_int_enable;	/* 1b914 */
+	u32	evt_mgmt_int_port_stat;	/* 1b918 */
+	u32	__rsvd21;
+
+	u32	evt_mgmt_port_wr_stat;	/* 1b920 */
+	u32	evt_mgmt_port_wr_enable;/* 1b924 */
+	u32	evt_mgmt_port_wr_port_stat;	/* 1b928 */
+	u32	__rsvd22;
+
+	u32	evt_mgmt_dev_int_en;	/* 1b930 */
+	u32	evt_mgmt_dev_port_wr_en;	/* 1b934 */
+	u32	__rsvd23;
+	u32	evt_mgmt_mecs_stat;	/* 1b93c */
+
+	u32	evt_mgmt_mecs_int_en;	/* 1b940 */
+	u32	evt_mgmt_mecs_cap_en;	/* 1b944 */
+	u32	evt_mgmt_mecs_trig_en;	/* 1b948 */
+	u32	evt_mgmt_mecs_req;	/* 1b94c */
+
+	u32	evt_mgmt_mecs_port_stat;/* 1b950 */
+	u32	__rsvd24[2];
+	u32	evt_mgmt_mecs_event_gen;/* 1b95c */
+
+	u32	evt_mgmt_rst_port_stat;	/* 1b960 */
+	u32	__rsvd25;
+	u32	evt_mgmt_rst_int_en;	/* 1b968 */
+	u32	__rsvd26;
+
+	u32	evt_mgmt_rst_port_wr_en;/* 1b970 */
+					/* 1b974 - 1b9fc */
+};
+
+struct keystone_rio_port_write_regs {
+	u32	port_wr_blk_hdr;	/* 1ba00 */
+	u32	port_wr_ctl;		/* 1ba04 */
+	u32	port_wr_route;		/* 1ba08 */
+	u32	__rsvd28;
+
+	u32	port_wr_rx_stat;	/* 1ba10 */
+	u32	port_wr_rx_event_gen;	/* 1ba14 */
+	u32	__rsvd29[2];
+
+	u32	port_wr_rx_capt[4];	/* 1ba20 - 1ba2c */
+					/* 1ba30 - 1bcfc */
+};
+
+struct keystone_rio_link_layer_regs {
+	u32	link_blk_hdr;		/* 1bd00 */
+	u32	__rsvd31[8];		/* 1bd04 - 1bd20 */
+	u32	whiteboard;		/* 1bd24 */
+	u32	port_number;		/* 1bd28 */
+
+	u32	__rsvd32;		/* 1bd2c */
+
+	u32	prescalar_srv_clk;	/* 1bd30 */
+	u32	reg_rst_ctl;		/* 1bd34 */
+	u32	__rsvd33[4];		/* 1bd38, 1bd3c, 1bd40, 1bd44 */
+	u32	local_err_det;		/* 1bd48 */
+	u32	local_err_en;		/* 1bd4c */
+
+	u32	local_h_addr_capt;	/* 1bd50 */
+	u32	local_addr_capt;	/* 1bd54 */
+	u32	local_id_capt;		/* 1bd58 */
+	u32	local_ctrl_capt;	/* 1bd5c */
+
+					/* 1bd60 - 1bdfc */
+};
+
+struct keystone_rio_fabric_regs {
+	u32	fabric_hdr;		/* 1be00 */
+	u32	__rsvd35[3];		/* 1be04 - 1be0c */
+
+	u32	fabric_csr;		/* 1be10 */
+	u32	__rsvd36[11];		/* 1be14 - 1be3c */
+
+	u32	sp_fabric_status[4];	/* 1be40 - 1be4c */
+};
+
+/* Message Passing management functions */
+int keystone_rio_open_outb_mbox(struct rio_mport *mport, void *dev_id,
+				int mbox, int entries);
+
+void keystone_rio_close_outb_mbox(struct rio_mport *mport, int mbox);
+
+int keystone_rio_hw_add_outb_message(struct rio_mport *mport,
+				     struct rio_dev *rdev, int mbox,
+				     void *buffer, const size_t len);
+
+int keystone_rio_open_inb_mbox(struct rio_mport *mport, void *dev_id,
+			       int mbox, int entries);
+
+void keystone_rio_close_inb_mbox(struct rio_mport *mport, int mbox);
+
+int keystone_rio_hw_add_inb_buffer(struct rio_mport *mport,
+				   int mbox, void *buffer);
+
+void *keystone_rio_hw_get_inb_message(struct rio_mport *mport, int mbox);
+#endif /* KEYSTONE_RIO_H */
diff --git a/drivers/rapidio/devices/keystone_rio_dma.c b/drivers/rapidio/devices/keystone_rio_dma.c
new file mode 100644
index 0000000..c2dc13a
--- /dev/null
+++ b/drivers/rapidio/devices/keystone_rio_dma.c
@@ -0,0 +1,774 @@
+/*
+ * Copyright (C) 2016 Texas Instruments Incorporated
+ * Authors: Aurelien Jacquiot <a-jacquiot@ti.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation version 2.
+ *
+ * This program is distributed "as is" WITHOUT ANY WARRANTY of any
+ * kind, whether express or implied; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+#include <linux/errno.h>
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/rio.h>
+#include <linux/rio_drv.h>
+#include <linux/dma-mapping.h>
+#include <linux/interrupt.h>
+#include <linux/kfifo.h>
+#include <linux/delay.h>
+
+#include "keystone_rio_serdes.h"
+#include "keystone_rio.h"
+
+static void keystone_rio_dma_start(struct keystone_rio_dma_chan *chan);
+
+static inline struct keystone_rio_dma_desc *desc_from_adesc(
+	struct dma_async_tx_descriptor *adesc)
+{
+	return container_of(adesc, struct keystone_rio_dma_desc, adesc);
+}
+
+static inline struct dma_async_tx_descriptor *desc_to_adesc(
+	struct keystone_rio_dma_desc *desc)
+{
+	return &desc->adesc;
+}
+
+static int keystone_rio_dma_chan_set_state(struct keystone_rio_dma_chan *chan,
+					   enum keystone_rio_chan_state old,
+					   enum keystone_rio_chan_state new)
+{
+	enum keystone_rio_chan_state cur;
+
+	cur = atomic_cmpxchg(&chan->state, old, new);
+
+	if (likely(cur == old))
+		return 0;
+
+	return -EINVAL;
+}
+
+static inline enum keystone_rio_chan_state
+keystone_rio_dma_chan_get_state(struct keystone_rio_dma_chan *chan)
+{
+	return atomic_read(&chan->state);
+}
+
+static inline void
+keystone_rio_dma_chan_force_state(struct keystone_rio_dma_chan *chan,
+				  enum keystone_rio_chan_state state)
+{
+	atomic_set(&chan->state, state);
+}
+
+/*
+ * Return the first descriptor of the active list (called with spinlock held)
+ */
+static inline struct keystone_rio_dma_desc *keystone_rio_dma_first_active(
+	struct keystone_rio_dma_chan *chan)
+{
+	if (list_empty(&chan->active_list))
+		return NULL;
+
+	return list_first_entry(&chan->active_list,
+				struct keystone_rio_dma_desc,
+				node);
+}
+
+/*
+ * Return the next descriptor of a transfer list (called with spinlock held)
+ */
+static inline struct keystone_rio_dma_desc *keystone_rio_dma_next(
+	struct keystone_rio_dma_chan *chan)
+{
+	struct keystone_rio_dma_desc *desc = chan->current_transfer;
+	struct keystone_rio_dma_desc *next;
+
+	next = list_entry(desc->tx_list.next, struct keystone_rio_dma_desc,
+			  tx_list);
+	if (next == desc)
+		return NULL;
+
+	return next;
+}
+
+static inline void keystone_rio_dma_complete_notify(
+	struct keystone_rio_data *krio_priv,
+	u32 lsu)
+{
+	struct keystone_rio_dma_chan *chan, *_c;
+
+	list_for_each_entry_safe(chan, _c, &krio_priv->dma_channels[lsu], node)
+		tasklet_schedule(&chan->tasklet);
+}
+
+void keystone_rio_dma_interrupt_handler(struct keystone_rio_data *krio_priv,
+					u32 lsu,
+					u32 error)
+{
+	if (unlikely(error)) {
+		/*
+		 * In case of error we do not know the LSU, so complete all
+		 * running channels
+		 */
+		u32 __lsu;
+
+		for (__lsu = 0; __lsu < KEYSTONE_RIO_LSU_NUM; __lsu++)
+			keystone_rio_dma_complete_notify(krio_priv, __lsu);
+	} else {
+		/* Notify all channels for the corresponding LSU */
+		keystone_rio_dma_complete_notify(krio_priv, lsu);
+	}
+}
+
+static inline void keystone_rio_dma_chain_complete(
+	struct keystone_rio_dma_chan *chan,
+	struct keystone_rio_dma_desc *desc)
+{
+	struct dma_async_tx_descriptor *adesc = desc_to_adesc(desc);
+	dma_async_tx_callback callback = adesc->callback;
+	void *param = adesc->callback_param;
+
+	chan->completed_cookie = adesc->cookie;
+
+	if (callback)
+		callback(param);
+}
+
+static inline void keystone_rio_dma_complete(struct keystone_rio_dma_chan *chan,
+					     struct list_head *list,
+					     int call_completion)
+{
+	struct keystone_rio_dma_desc *desc, *_d;
+	struct keystone_rio_dma_desc *frag_desc, *_fd;
+
+	list_for_each_entry_safe(desc, _d, list, node) {
+		/* Call completion handler */
+		if (call_completion)
+			keystone_rio_dma_chain_complete(chan, desc);
+
+		/* Free all fragment descriptors if any */
+		list_for_each_entry_safe(frag_desc, _fd, &desc->tx_list,
+					 tx_list)
+			kfree(frag_desc);
+
+		kfree(desc);
+	}
+}
+
+/*
+ * Do the completion of the whole transfer
+ */
+static void keystone_rio_dma_complete_all(struct keystone_rio_dma_chan *chan)
+{
+	LIST_HEAD(list);
+
+	/* Complete the current active transfer and prepare the next one */
+	spin_lock(&chan->lock);
+	list_splice_init(&chan->active_list, &list);
+	list_splice_init(&chan->queue, &chan->active_list);
+	spin_unlock(&chan->lock);
+
+	keystone_rio_dma_complete(chan, &list, 1);
+}
+
+/*
+ * Return the next transfer chunk to perform (called with spinlock held)
+ */
+static struct keystone_rio_dma_desc *
+keystone_rio_dma_next_work(struct keystone_rio_dma_chan *chan)
+{
+	if (!chan->current_transfer) {
+		/* If no current_transfer */
+		chan->current_transfer = keystone_rio_dma_first_active(chan);
+		dev_dbg(chan_dev(chan),
+			"%s: no current transfer, moving to first_active 0x%p\n",
+			__func__, chan->current_transfer);
+	} else {
+		/* Move to next part of the transfer */
+		chan->current_transfer = keystone_rio_dma_next(chan);
+		dev_dbg(chan_dev(chan),
+			"%s: moving to next part of the transfer 0x%p\n",
+			__func__, chan->current_transfer);
+	}
+	return chan->current_transfer;
+}
+
+/*
+ * Start a transfer for a given channel
+ */
+static void keystone_rio_dma_start(struct keystone_rio_dma_chan *chan)
+{
+	struct keystone_rio_dma_desc *desc;
+	unsigned long flags;
+	int res;
+
+	spin_lock_irqsave(&chan->lock, flags);
+
+	desc = chan->current_transfer;
+
+	if (unlikely((!desc) || (keystone_rio_dma_chan_get_state(chan)
+				 != RIO_CHAN_STATE_ACTIVE))) {
+		spin_unlock_irqrestore(&chan->lock, flags);
+		return;
+	}
+
+	keystone_rio_dma_chan_set_state(chan, RIO_CHAN_STATE_ACTIVE,
+					RIO_CHAN_STATE_RUNNING);
+
+	/* Perform the DIO transfer */
+	dev_dbg(chan_dev(chan),
+		"%s: perform current DIO transfer (desc = 0x%p)\n",
+		__func__, desc);
+
+	res = keystone_rio_lsu_start_transfer(chan->lsu,
+					      desc->port_id,
+					      desc->dest_id,
+					      desc->buff_addr,
+					      desc->rio_addr,
+					      desc->size,
+					      desc->sys_size,
+					      desc->packet_type,
+					      &desc->lsu_context,
+					      1,
+					      chan->krio);
+	if (res) {
+		desc->status = DMA_ERROR;
+		spin_unlock_irqrestore(&chan->lock, flags);
+		dev_err(chan_dev(chan), "DIO: transfer error %d\n", res);
+		return;
+	}
+
+	keystone_rio_dma_chan_set_state(chan, RIO_CHAN_STATE_RUNNING,
+					RIO_CHAN_STATE_WAITING);
+
+	spin_unlock_irqrestore(&chan->lock, flags);
+}
+
+static void keystone_rio_dma_tasklet(unsigned long data)
+{
+	struct keystone_rio_dma_chan *chan =
+				(struct keystone_rio_dma_chan *)data;
+	struct keystone_rio_dma_desc *desc;
+	int res = 0;
+
+	spin_lock(&chan->lock);
+
+	desc = chan->current_transfer;
+
+	dev_dbg(chan_dev(chan), "tasklet called for channel%d\n",
+		chan_id(chan));
+
+	if (unlikely((!desc) || (keystone_rio_dma_chan_get_state(chan)
+				 != RIO_CHAN_STATE_WAITING))) {
+		spin_unlock(&chan->lock);
+		return;
+	}
+
+	/* Check the completion code */
+	res = keystone_rio_lsu_complete_transfer(chan->lsu,
+						 desc->lsu_context,
+						 chan->krio);
+
+	if ((res == -EAGAIN) && (desc->retry_count-- > 0)) {
+		spin_unlock(&chan->lock);
+
+		dev_dbg(chan_dev(chan),
+			"LSU%d transfer not completed (busy) context 0x%x (0x%p), restart the channel completion\n",
+			chan->lsu, desc->lsu_context,
+			&desc->lsu_context);
+
+		tasklet_schedule(&chan->tasklet);
+		return;
+	}
+
+	if (res) {
+		desc->status = DMA_ERROR;
+		spin_unlock(&chan->lock);
+
+		dev_dbg(chan_dev(chan),
+			"%s: LSU%d DMA transfer failed with %d\n",
+			__func__, chan->lsu, res);
+
+		/* Stop current transfer */
+		return;
+	}
+
+	/* If last part of the transfer, do the DMA completion */
+	if (desc->last) {
+		spin_unlock(&chan->lock);
+		keystone_rio_dma_complete_all(chan);
+		spin_lock(&chan->lock);
+		chan->current_transfer = NULL;
+	}
+
+	/* Move to next transfer */
+	desc = keystone_rio_dma_next_work(chan);
+
+	keystone_rio_dma_chan_set_state(chan, RIO_CHAN_STATE_WAITING,
+					RIO_CHAN_STATE_ACTIVE);
+
+	spin_unlock(&chan->lock);
+
+	/* Start next transfer if any */
+	if (desc)
+		keystone_rio_dma_start(chan);
+}
+
+static int keystone_rio_dma_alloc_chan_resources(struct dma_chan *dchan)
+{
+	struct keystone_rio_dma_chan *chan = from_dma_chan(dchan);
+
+	dev_dbg(chan_dev(chan), "init DMA engine channel%d\n", chan_id(chan));
+
+	spin_lock_bh(&chan->lock);
+	WARN_ON(!list_empty(&chan->active_list));
+	WARN_ON(!list_empty(&chan->queue));
+
+	chan->completed_cookie = 1;
+	dchan->cookie = 1;
+	spin_unlock_bh(&chan->lock);
+	keystone_rio_dma_chan_set_state(chan, RIO_CHAN_STATE_UNUSED,
+					RIO_CHAN_STATE_ACTIVE);
+
+	tasklet_enable(&chan->tasklet);
+	return 0;
+}
+
+static void keystone_rio_dma_free_chan_resources(struct dma_chan *dchan)
+{
+	struct keystone_rio_dma_chan *chan = from_dma_chan(dchan);
+	LIST_HEAD(list);
+
+	dev_dbg(chan_dev(chan), "freeing DMA Engine channel%d\n",
+		chan_id(chan));
+
+	if (keystone_rio_dma_chan_get_state(chan) != RIO_CHAN_STATE_ACTIVE) {
+		dev_warn(chan_dev(chan),
+			 "freeing still running DMA channel %d!!!\n",
+			 chan_id(chan));
+	}
+
+	keystone_rio_dma_chan_force_state(chan, RIO_CHAN_STATE_UNUSED);
+
+	tasklet_disable(&chan->tasklet);
+
+	/* Purge the current active and queued transfers */
+	if (!list_empty(&chan->active_list)) {
+		dev_warn(chan_dev(chan),
+			 "transfer still active on DMA channel %d!!!\n",
+			 chan_id(chan));
+		spin_lock_bh(&chan->lock);
+		list_splice_init(&chan->active_list, &list);
+		spin_unlock_bh(&chan->lock);
+	}
+
+	if (!list_empty(&chan->queue)) {
+		dev_warn(chan_dev(chan),
+			 "queued transfers on DMA channel %d!!!\n",
+			 chan_id(chan));
+		spin_lock_bh(&chan->lock);
+		list_splice_init(&chan->queue, &list);
+		spin_unlock_bh(&chan->lock);
+	}
+
+	if (!list_empty(&list))
+		keystone_rio_dma_complete(chan, &list, 0);
+
+	chan->current_transfer = NULL;
+}
+
+static void keystone_rio_dma_issue_pending(struct dma_chan *dchan)
+{
+	struct keystone_rio_dma_chan *chan = from_dma_chan(dchan);
+
+	if (keystone_rio_dma_chan_get_state(chan) == RIO_CHAN_STATE_ACTIVE) {
+		struct keystone_rio_dma_desc *desc;
+
+		spin_lock_bh(&chan->lock);
+		desc = keystone_rio_dma_next_work(chan);
+		spin_unlock_bh(&chan->lock);
+
+		if (desc)
+			keystone_rio_dma_start(chan);
+
+	} else
+		dev_dbg(chan_dev(chan),	"%s: DMA channel busy, state = %d\n",
+			__func__, keystone_rio_dma_chan_get_state(chan));
+}
+
+static enum dma_status keystone_rio_dma_tx_status(struct dma_chan *dchan,
+						  dma_cookie_t cookie,
+						  struct dma_tx_state *txstate)
+{
+	struct keystone_rio_dma_chan *chan = from_dma_chan(dchan);
+	struct keystone_rio_dma_desc *desc = chan->current_transfer;
+	dma_cookie_t last_used;
+	dma_cookie_t last_completed;
+	enum dma_status status;
+
+	spin_lock_bh(&chan->lock);
+	last_completed = chan->completed_cookie;
+	last_used      = dchan->cookie;
+	spin_unlock_bh(&chan->lock);
+
+	/*
+	 * In case of error, totally complete the current transfer
+	 * and start the new then return error
+	 */
+	if ((desc) && (desc->status == DMA_ERROR)) {
+		dev_dbg(chan_dev(chan), "%s: DMA error\n", __func__);
+
+		keystone_rio_dma_complete_all(chan);
+		spin_lock_bh(&chan->lock);
+
+		/* Even if not the last, stop the current transfer */
+		chan->current_transfer = NULL;
+
+		keystone_rio_dma_chan_set_state(chan, RIO_CHAN_STATE_WAITING,
+						RIO_CHAN_STATE_ACTIVE);
+
+		spin_unlock_bh(&chan->lock);
+		keystone_rio_dma_issue_pending(dchan);
+		return DMA_ERROR;
+	}
+
+	status = dma_async_is_complete(cookie, last_completed, last_used);
+	dma_set_tx_state(txstate, last_completed, last_used, 0);
+
+	dev_dbg(chan_dev(chan),
+		"%s: exit, ret: %d, last_completed: %d, last_used: %d\n",
+		__func__, (int)status, last_completed, last_used);
+
+	return status;
+}
+
+static dma_cookie_t keystone_rio_dma_tx_submit(
+	struct dma_async_tx_descriptor *adesc)
+{
+	struct keystone_rio_dma_desc *desc = desc_from_adesc(adesc);
+	struct keystone_rio_dma_chan *chan = from_dma_chan(adesc->chan);
+	unsigned long flags;
+	dma_cookie_t cookie;
+
+	spin_lock_irqsave(&chan->lock, flags);
+
+	/* Increment the DMA cookie */
+	cookie = adesc->chan->cookie;
+	if (++cookie < 0)
+		cookie = 1;
+	adesc->chan->cookie = cookie;
+	adesc->cookie = cookie;
+
+	/* Add the transfer to the DMA */
+	if (list_empty(&chan->active_list)) {
+		list_add_tail(&desc->node, &chan->active_list);
+		if (!chan->current_transfer) {
+			/* if no current_transfer */
+			chan->current_transfer =
+				keystone_rio_dma_first_active(chan);
+		}
+		spin_unlock_irqrestore(&chan->lock, flags);
+
+		/* Initiate the transfer */
+		keystone_rio_dma_start(chan);
+	} else {
+		list_add_tail(&desc->node, &chan->queue);
+		spin_unlock_irqrestore(&chan->lock, flags);
+	}
+
+	return cookie;
+}
+
+static struct dma_async_tx_descriptor *
+keystone_rio_dma_prep_slave_sg(struct dma_chan *dchan,
+			       struct scatterlist *sgl,
+			       unsigned int sg_len,
+			       enum dma_transfer_direction dir,
+			       unsigned long flags,
+			       void *tinfo)
+{
+	struct keystone_rio_dma_chan *chan = from_dma_chan(dchan);
+	struct keystone_rio_dma_desc *desc = NULL;
+	struct keystone_rio_dma_desc *first = NULL;
+	struct rio_dma_ext *rext = (struct rio_dma_ext *)tinfo;
+	u64 rio_addr = rext->rio_addr; /* limited to 64-bit for now */
+	struct scatterlist *sg;
+	u32 packet_type, last_packet_type;
+	unsigned int i;
+
+	if (!sgl || !sg_len) {
+		dev_err(chan_dev(chan), "%s: no SG list\n", __func__);
+		return NULL;
+	}
+
+	if (sg_len > KEYSTONE_RIO_DMA_MAX_DESC) {
+		dev_err(chan_dev(chan), "%s: SG list is too long (%d)\n",
+			__func__, sg_len);
+		return NULL;
+	}
+
+	if (dir == DMA_DEV_TO_MEM) {
+		packet_type      = KEYSTONE_RIO_PACKET_TYPE_NREAD;
+		last_packet_type = KEYSTONE_RIO_PACKET_TYPE_NREAD;
+	} else if (dir == DMA_MEM_TO_DEV) {
+		switch (rext->wr_type) {
+		case RDW_DEFAULT:
+		case RDW_ALL_NWRITE:
+			packet_type      = KEYSTONE_RIO_PACKET_TYPE_NWRITE;
+			last_packet_type = KEYSTONE_RIO_PACKET_TYPE_NWRITE;
+			break;
+		case RDW_LAST_NWRITE_R:
+			packet_type      = KEYSTONE_RIO_PACKET_TYPE_NWRITE;
+			last_packet_type = KEYSTONE_RIO_PACKET_TYPE_NWRITE_R;
+			break;
+		case RDW_ALL_NWRITE_R:
+		default:
+			packet_type      = KEYSTONE_RIO_PACKET_TYPE_NWRITE_R;
+			last_packet_type = KEYSTONE_RIO_PACKET_TYPE_NWRITE_R;
+			break;
+		}
+	} else {
+		dev_err(chan_dev(chan),	"unsupported DMA direction option\n");
+		return NULL;
+	}
+
+	for_each_sg(sgl, sg, sg_len, i) {
+		/* Allocate a (virtual) DMA descriptor for this transfer */
+		desc = kmalloc(sizeof(*desc), GFP_KERNEL);
+		if (unlikely(!desc)) {
+			dev_err(chan_dev(chan),
+				"cannot allocate DMA transfer descriptor\n");
+			return NULL;
+		}
+
+		dma_async_tx_descriptor_init(&desc->adesc, dchan);
+		desc->adesc.tx_submit = keystone_rio_dma_tx_submit;
+		desc->adesc.flags = DMA_CTRL_ACK;
+
+		/* Fill the descriptor with the RapidIO information */
+		desc->retry_count = KEYSTONE_RIO_RETRY_CNT;
+		desc->status      = DMA_COMPLETE;
+		desc->port_id     = dma_to_mport(dchan->device)->index;
+		desc->dest_id     = rext->destid;
+		desc->rio_addr    = rio_addr;
+		desc->rio_addr_u  = 0;
+		desc->buff_addr   = sg_dma_address(sg);
+		desc->size        = sg_dma_len(sg);
+		desc->sys_size    = dma_to_mport(dchan->device)->sys_size;
+
+		INIT_LIST_HEAD(&desc->node);
+		INIT_LIST_HEAD(&desc->tx_list);
+
+		if (sg_is_last(sg)) {
+			desc->last  = true;
+			packet_type = last_packet_type;
+		} else {
+			desc->last = false;
+		}
+
+		/* Check if we can switch to SWRITE */
+		if ((packet_type == KEYSTONE_RIO_PACKET_TYPE_NWRITE) &&
+		    ((desc->size & 0x7) == 0) &&
+		    ((desc->rio_addr & 0x7) == 0) &&
+		    ((desc->buff_addr & 0x7) == 0)) {
+			packet_type = KEYSTONE_RIO_PACKET_TYPE_SWRITE;
+		}
+
+		desc->packet_type = packet_type;
+
+		rio_addr += sg_dma_len(sg);
+
+		if (!first)
+			first = desc;
+		else
+			list_add_tail(&desc->tx_list, &first->tx_list);
+	}
+
+	first->adesc.cookie = -EBUSY;
+	desc->adesc.flags  |= flags;
+
+	return &first->adesc;
+}
+
+int keystone_rio_dma_prep_raw_packet(
+	struct dma_chan *dchan,
+	struct keystone_rio_dma_packet_raw *pkt)
+{
+	struct keystone_rio_dma_chan *chan = from_dma_chan(dchan);
+	struct keystone_rio_dma_desc *desc;
+
+	/* Allocate a (virtual) DMA descriptor for this transfer */
+	desc = kmalloc(sizeof(*desc), GFP_KERNEL);
+	if (unlikely(!desc)) {
+		dev_err(chan_dev(chan),
+			"cannot allocate DMA transfer descriptor\n");
+		return -ENOMEM;
+	}
+
+	dma_async_tx_descriptor_init(&desc->adesc, dchan);
+	desc->adesc.tx_submit = keystone_rio_dma_tx_submit;
+	desc->adesc.flags     = DMA_CTRL_ACK | DMA_PREP_INTERRUPT;
+	desc->adesc.cookie    = -EBUSY;
+
+	/* Fill the descriptor with the RapidIO raw packet information */
+	desc->retry_count = KEYSTONE_RIO_RETRY_CNT;
+	desc->status      = DMA_COMPLETE;
+	desc->port_id     = pkt->port_id;
+	desc->dest_id     = pkt->dest_id;
+	desc->rio_addr    = pkt->rio_addr;
+	desc->rio_addr_u  = pkt->rio_addr_u;
+	desc->buff_addr   = pkt->buff_addr;
+	desc->size        = pkt->size;
+	desc->sys_size    = pkt->sys_size;
+	desc->packet_type = pkt->packet_type;
+	desc->last        = true;
+	desc->lsu_context = 0;
+
+	INIT_LIST_HEAD(&desc->node);
+	INIT_LIST_HEAD(&desc->tx_list);
+
+	pkt->tx = &desc->adesc;
+
+	return 0;
+}
+
+static int keystone_rio_dma_terminate_all(struct dma_chan *dchan)
+{
+	struct keystone_rio_dma_chan *chan = from_dma_chan(dchan);
+	LIST_HEAD(list);
+
+	/* Stop the current transfers */
+	spin_lock_bh(&chan->lock);
+	list_splice_init(&chan->active_list, &list);
+	list_splice_init(&chan->queue, &list);
+	chan->current_transfer = NULL;
+	spin_unlock_bh(&chan->lock);
+
+	keystone_rio_dma_chan_force_state(chan, RIO_CHAN_STATE_ACTIVE);
+
+	/* Complete all transfers */
+	keystone_rio_dma_complete(chan, &list, 1);
+
+	return 0;
+}
+
+static int keystone_rio_dma_device_pause(struct dma_chan *dchan)
+{
+	struct keystone_rio_dma_chan *chan = from_dma_chan(dchan);
+
+	tasklet_disable(&chan->tasklet);
+
+	return 0;
+}
+
+static int keystone_rio_dma_device_resume(struct dma_chan *dchan)
+{
+	struct keystone_rio_dma_chan *chan = from_dma_chan(dchan);
+
+	tasklet_enable(&chan->tasklet);
+
+	return 0;
+}
+
+int keystone_rio_dma_register(struct rio_mport *mport, int channel_num)
+{
+	struct keystone_rio_data *krio_priv = mport->priv;
+	u32 c;
+	int ret;
+
+	if (channel_num > KEYSTONE_RIO_DMA_MAX_CHANNEL)
+		return -EINVAL;
+
+	if (channel_num == 0)
+		return 0;
+
+	mport->dma.dev = krio_priv->dev;
+	mport->dma.chancnt = channel_num < 0 ?
+		KEYSTONE_RIO_DMA_MAX_CHANNEL : channel_num;
+
+	INIT_LIST_HEAD(&mport->dma.channels);
+
+	for (c = 0; c < mport->dma.chancnt; c++) {
+		struct keystone_rio_dma_chan *chan =
+			kzalloc(sizeof(struct keystone_rio_dma_chan),
+				GFP_KERNEL);
+
+		if (!chan) {
+			dev_err(krio_priv->dev,
+				"failed to allocate channel\n");
+			return -ENOMEM;
+		}
+
+		chan->dchan.device  = &mport->dma;
+		chan->dchan.cookie  = 1;
+		chan->dchan.chan_id = c;
+
+		spin_lock_init(&chan->lock);
+
+		INIT_LIST_HEAD(&chan->active_list);
+		INIT_LIST_HEAD(&chan->queue);
+
+		tasklet_init(&chan->tasklet,
+			     keystone_rio_dma_tasklet,
+			     (u32)chan);
+		tasklet_disable(&chan->tasklet);
+		list_add_tail(&chan->dchan.device_node, &mport->dma.channels);
+
+		chan->krio = krio_priv;
+
+		/* Allocate one LSU per channel */
+		chan->lsu = keystone_rio_lsu_alloc(chan->krio);
+
+		keystone_rio_dma_chan_force_state(chan, RIO_CHAN_STATE_UNUSED);
+
+		list_add_tail(&chan->node,
+			      &krio_priv->dma_channels[chan->lsu]);
+
+		dev_info(krio_priv->dev,
+			 "registering DMA channel %d (0x%p) using lsu %d for port %d\n",
+			 c, chan, chan->lsu, mport->index);
+	}
+
+	dma_cap_zero(mport->dma.cap_mask);
+	dma_cap_set(DMA_PRIVATE, mport->dma.cap_mask);
+	dma_cap_set(DMA_SLAVE, mport->dma.cap_mask);
+
+	mport->dma.device_alloc_chan_resources =
+		keystone_rio_dma_alloc_chan_resources;
+	mport->dma.device_free_chan_resources =
+		keystone_rio_dma_free_chan_resources;
+	mport->dma.device_tx_status =
+		keystone_rio_dma_tx_status;
+	mport->dma.device_issue_pending =
+		keystone_rio_dma_issue_pending;
+	mport->dma.device_prep_slave_sg =
+		keystone_rio_dma_prep_slave_sg;
+	mport->dma.device_terminate_all =
+		keystone_rio_dma_terminate_all;
+	mport->dma.device_pause =
+		keystone_rio_dma_device_pause;
+	mport->dma.device_resume =
+		keystone_rio_dma_device_resume;
+
+	ret = dma_async_device_register(&mport->dma);
+	if (ret)
+		dev_err(krio_priv->dev, "failed to register DMA device\n");
+
+	dev_dbg(mport->dma.dev, "%s: dma device registered\n", __func__);
+
+	return ret;
+}
+
+void keystone_rio_dma_unregister(struct rio_mport *mport)
+{
+	dma_async_device_unregister(&mport->dma);
+
+	dev_dbg(mport->dma.dev, "%s: dma device unregistered\n", __func__);
+}
diff --git a/drivers/rapidio/devices/keystone_rio_mp.c b/drivers/rapidio/devices/keystone_rio_mp.c
new file mode 100644
index 0000000..c64ab50
--- /dev/null
+++ b/drivers/rapidio/devices/keystone_rio_mp.c
@@ -0,0 +1,1289 @@
+/*
+ * Copyright (C) 2014 Texas Instruments Incorporated
+ * Authors: Aurelien Jacquiot <a-jacquiot@ti.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation version 2.
+ *
+ * This program is distributed "as is" WITHOUT ANY WARRANTY of any
+ * kind, whether express or implied; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/interrupt.h>
+#include <linux/dma-mapping.h>
+#include <linux/dmaengine.h>
+#include <linux/kfifo.h>
+#include <linux/rio.h>
+#include <linux/rio_drv.h>
+#include <linux/soc/ti/knav_qmss.h>
+#include <linux/soc/ti/knav_dma.h>
+#include <linux/soc/ti/knav_helpers.h>
+
+#include "keystone_rio_serdes.h"
+#include "keystone_rio.h"
+
+static void keystone_rio_tx_complete(void *data);
+static void keystone_rio_tx_error(void *data);
+
+/*
+ * DMA descriptor helper functions
+ */
+static inline void get_pkt_info(u32 *buff, u32 *buff_len, u32 *ndesc,
+				struct knav_dma_desc *desc)
+{
+	*buff_len = desc->buff_len;
+	*buff = desc->buff;
+	*ndesc = desc->next_desc;
+}
+
+static inline void get_pad_info(u32 *pad0, u32 *pad1,
+				struct knav_dma_desc *desc)
+{
+	*pad0 = desc->pad[0];
+	*pad1 = desc->pad[1];
+}
+
+static inline void get_org_pkt_info(u32 *buff, u32 *buff_len,
+				    struct knav_dma_desc *desc)
+{
+	*buff = desc->orig_buff;
+	*buff_len = desc->orig_len;
+}
+
+static inline void get_words(u32 *words, int num_words, u32 *desc)
+{
+	int i;
+
+	for (i = 0; i < num_words; i++)
+		words[i] = desc[i];
+}
+
+static inline void set_pkt_info(u32 buff, u32 buff_len, u32 ndesc,
+				struct knav_dma_desc *desc)
+{
+	desc->buff_len = buff_len;
+	desc->buff = buff;
+	desc->next_desc = ndesc;
+}
+
+static inline void set_desc_info(u32 desc_info, u32 pkt_info,
+				 struct knav_dma_desc *desc)
+{
+	desc->desc_info = desc_info;
+	desc->packet_info = pkt_info;
+}
+
+static inline void set_pad_info(u32 pad0, u32 pad1,
+				struct knav_dma_desc *desc)
+{
+	desc->pad[0] = pad0;
+	desc->pad[1] = pad1;
+}
+
+static inline void set_org_pkt_info(u32 buff, u32 buff_len,
+				    struct knav_dma_desc *desc)
+{
+	desc->orig_buff = buff;
+	desc->orig_len = buff_len;
+}
+
+static inline void set_words(u32 *words, int num_words, u32 *desc)
+{
+	int i;
+
+	for (i = 0; i < num_words; i++)
+		desc[i] = words[i];
+}
+
+/*
+ * Retrieve MP receive code completion
+ */
+static inline u32 keystone_rio_mp_get_cc(u32 psdata1, u32 packet_type)
+{
+	return (packet_type == RIO_PACKET_TYPE_MESSAGE ?
+		psdata1 >> 15 : psdata1 >> 8) & 0x3;
+}
+
+/*
+ * This function retrieves the packet type for a given mbox
+ */
+static inline u32 keystone_rio_mp_get_type(int mbox,
+					   struct keystone_rio_data *krio_priv)
+{
+	struct keystone_rio_rx_chan_info *krx_chan =
+					&krio_priv->rx_channels[mbox];
+
+	return (u32)((krx_chan->packet_type != RIO_PACKET_TYPE_STREAM) &&
+		     (krx_chan->packet_type != RIO_PACKET_TYPE_MESSAGE)) ?
+		RIO_PACKET_TYPE_MESSAGE : krx_chan->packet_type;
+}
+
+/*
+ * This function retrieves the mapping from Linux RIO mailbox to stream id for
+ * type 9 packets
+ */
+static inline u32
+keystone_rio_mbox_to_strmid(int mbox, struct keystone_rio_data *krio_priv)
+{
+	struct keystone_rio_rx_chan_info *krx_chan =
+				&krio_priv->rx_channels[mbox];
+
+	return (u32)krx_chan->stream_id;
+}
+
+void keystone_rio_chan_work_handler(unsigned long data)
+{
+	struct keystone_rio_rx_chan_info *krx_chan =
+		(struct keystone_rio_rx_chan_info *)data;
+	struct keystone_rio_data *krio_priv = krx_chan->priv;
+	struct keystone_rio_mbox_info *p_mbox;
+	int mbox = krx_chan->mbox_id;
+
+	p_mbox = &krio_priv->rx_mbox[mbox];
+	if (likely(p_mbox->running)) {
+		/* Client callback (slot is not used) */
+		p_mbox->port->inb_msg[mbox].mcback(
+			p_mbox->port,
+			p_mbox->dev_id,
+			mbox,
+			0);
+	}
+
+	knav_queue_enable_notify(krx_chan->queue);
+}
+
+static void keystone_rio_rx_notify(void *arg)
+{
+	struct keystone_rio_rx_chan_info *krx_chan = arg;
+
+	knav_queue_disable_notify(krx_chan->queue);
+
+	tasklet_schedule(&krx_chan->tasklet);
+}
+
+/* Release descriptors and attached buffers from Rx FDQ */
+static void keystone_rio_free_inb_buf(
+	struct keystone_rio_rx_chan_info *krx_chan,
+	int fdq)
+{
+	struct keystone_rio_data *krio_priv = krx_chan->priv;
+	struct knav_dma_desc *desc;
+	unsigned int buf_len, dma_sz;
+	dma_addr_t dma;
+	void *buffer;
+	u32 tmp;
+
+	/* Release inbound descriptors */
+	while ((dma = knav_queue_pop(krx_chan->fdq[fdq], &dma_sz))) {
+		desc = knav_pool_desc_unmap(krx_chan->pool, dma, dma_sz);
+		if (unlikely(!desc)) {
+			dev_err(krio_priv->dev, "failed to unmap Rx desc\n");
+			continue;
+		}
+
+		get_org_pkt_info(&dma, &buf_len, desc);
+		get_pad_info((u32 *)&buffer, &tmp, desc);
+
+		if (unlikely(!dma)) {
+			dev_err(krio_priv->dev, "NULL orig_buff in desc\n");
+			knav_pool_desc_put(krx_chan->pool, desc);
+			continue;
+		}
+
+		if (unlikely(!buffer)) {
+			dev_err(krio_priv->dev, "NULL bufptr in desc\n");
+			knav_pool_desc_put(krx_chan->pool, desc);
+			continue;
+		}
+
+		dma_unmap_single(krio_priv->dev, dma, buf_len,
+				 DMA_FROM_DEVICE);
+
+		/*
+		 * We do not free the buffer since the upper layer (mp client)
+		 * must be responsible for that task
+		 */
+		knav_pool_desc_put(krx_chan->pool, desc);
+	}
+}
+
+static void keystone_rio_inb_pool_free(
+	struct keystone_rio_rx_chan_info *krx_chan)
+{
+	struct keystone_rio_data *krio_priv = krx_chan->priv;
+	int i;
+
+	for (i = 0; i < KNAV_DMA_FDQ_PER_CHAN &&
+	     !IS_ERR_OR_NULL(krx_chan->fdq[i]); i++) {
+		dev_dbg(krio_priv->dev, "freeing fdq %d\n", i);
+		keystone_rio_free_inb_buf(krx_chan, i);
+	}
+
+	if (knav_pool_count(krx_chan->pool) != krx_chan->pool_size)
+		dev_err(krio_priv->dev,
+			"lost Rx (%d) descriptors\n",
+			krx_chan->pool_size - knav_pool_count(krx_chan->pool));
+
+	knav_pool_destroy(krx_chan->pool);
+	krx_chan->pool = NULL;
+}
+
+static void keystone_rio_inb_knav_free(
+	struct keystone_rio_rx_chan_info *krx_chan)
+{
+	int i;
+
+	/* Close channel */
+	if (!IS_ERR_OR_NULL(krx_chan->channel)) {
+		knav_dma_close_channel(krx_chan->channel);
+		krx_chan->channel = NULL;
+	}
+
+	/* Free pools and associated descriptors */
+	if (!IS_ERR_OR_NULL(krx_chan->pool))
+		keystone_rio_inb_pool_free(krx_chan);
+
+	/* Close completion queue */
+	if (!IS_ERR_OR_NULL(krx_chan->queue)) {
+		knav_queue_close(krx_chan->queue);
+		krx_chan->queue = NULL;
+	}
+
+	/* Close free queues */
+	for (i = 0; i < KNAV_DMA_FDQ_PER_CHAN &&
+	     !IS_ERR_OR_NULL(krx_chan->fdq[i]); ++i) {
+		knav_queue_close(krx_chan->fdq[i]);
+		krx_chan->fdq[i] = NULL;
+	}
+}
+
+static void keystone_rio_inb_exit(int mbox,
+				  struct keystone_rio_data *krio_priv)
+{
+	struct keystone_rio_rx_chan_info *krx_chan =
+					&krio_priv->rx_channels[mbox];
+
+	if (!(krx_chan->channel))
+		return;
+
+	knav_queue_disable_notify(krx_chan->queue);
+
+	/* Free all navigator Rx resources */
+	keystone_rio_inb_knav_free(krx_chan);
+}
+
+static int keystone_rio_mp_inb_init(int mbox,
+				    struct keystone_rio_data *krio_priv)
+{
+	struct keystone_rio_rx_chan_info *krx_chan =
+					&krio_priv->rx_channels[mbox];
+	struct knav_queue_notify_config notify_cfg;
+	struct knav_dma_cfg config;
+	u8 name[24];
+	u32 last_fdq = 0;
+	int err = -ENODEV;
+	int i;
+
+	/* If already initialized, return without error */
+	if (krx_chan->channel)
+		return 0;
+
+	if (!krx_chan->name) {
+		dev_err(krio_priv->dev,
+			"Rx channel name for mbox %d is not defined!\n",
+			mbox);
+		return err;
+	}
+
+	/* Create Rx descriptor pool */
+	snprintf(name, sizeof(name), "rx-pool-rio-mbox-%d", mbox);
+	krx_chan->pool = knav_pool_create(name,
+					  krx_chan->pool_size,
+					  krx_chan->pool_region_id);
+	if (IS_ERR_OR_NULL(krx_chan->pool)) {
+		dev_err(krio_priv->dev,
+			"couldn't create Rx pool\n");
+		err = PTR_ERR(krx_chan->pool);
+		goto fail;
+	}
+
+	/* Open Rx completion (receive) queue */
+	snprintf(name, sizeof(name), "rx-compl-rio-mbox-%d", mbox);
+	krx_chan->queue = knav_queue_open(name, krx_chan->queue_id, 0);
+	if (IS_ERR_OR_NULL(krx_chan->queue)) {
+		err = PTR_ERR(krx_chan->queue);
+		goto fail;
+	}
+
+	krx_chan->queue_id = knav_queue_get_id(krx_chan->queue);
+
+	/* Set notification for Rx completion */
+	notify_cfg.fn = keystone_rio_rx_notify;
+	notify_cfg.fn_arg = krx_chan;
+	err = knav_queue_device_control(krx_chan->queue,
+					KNAV_QUEUE_SET_NOTIFIER,
+					(unsigned long)&notify_cfg);
+	if (err)
+		goto fail;
+
+	knav_queue_disable_notify(krx_chan->queue);
+
+	/* Open Rx FDQs */
+	for (i = 0; i < KNAV_DMA_FDQ_PER_CHAN &&
+	     krx_chan->queue_depths[i] &&
+	     krx_chan->buffer_sizes[i]; ++i) {
+		snprintf(name, sizeof(name), "rx-fdq-rio-mbox-%d-%d", mbox, i);
+		krx_chan->fdq[i] = knav_queue_open(name, KNAV_QUEUE_GP, 0);
+		if (IS_ERR_OR_NULL(krx_chan->fdq[i])) {
+			err = PTR_ERR(krx_chan->fdq[i]);
+			goto fail;
+		}
+	}
+
+	memset(&config, 0, sizeof(config));
+	config.direction		= DMA_DEV_TO_MEM;
+	config.u.rx.einfo_present	= true;
+	config.u.rx.psinfo_present	= true;
+	config.u.rx.err_mode		= DMA_DROP;
+	config.u.rx.desc_type		= DMA_DESC_HOST;
+	config.u.rx.psinfo_at_sop	= false;
+	config.u.rx.sop_offset		= 0;
+	config.u.rx.dst_q		= krx_chan->queue_id;
+	config.u.rx.thresh		= DMA_THRESH_NONE;
+
+	for (i = 0; i < KNAV_DMA_FDQ_PER_CHAN; ++i) {
+		if (krx_chan->fdq[i])
+			last_fdq = knav_queue_get_id(krx_chan->fdq[i]);
+		config.u.rx.fdq[i] = last_fdq;
+	}
+
+	krx_chan->channel = knav_dma_open_channel(krio_priv->dev,
+						  krx_chan->name,
+						  &config);
+	if (IS_ERR_OR_NULL(krx_chan->channel)) {
+		dev_err(krio_priv->dev,
+			"failed opening Rx chan(%s\n",
+			krx_chan->name);
+		err = PTR_ERR(krx_chan->channel);
+		goto fail;
+	}
+
+	krx_chan->priv    = krio_priv;
+	krx_chan->flow_id = knav_dma_get_flow(krx_chan->channel);
+	krx_chan->mbox_id = mbox;
+
+	/* Initialize the associated Rx tasklet */
+	tasklet_init(&krx_chan->tasklet,
+		     keystone_rio_chan_work_handler,
+		     (unsigned long)krx_chan);
+
+	knav_queue_enable_notify(krx_chan->queue);
+
+	dev_dbg(krio_priv->dev,
+		"opened Rx channel: mbox = %d, complete_q = %d (0x%x), channel/flow = %d (0x%x), pkt_type = %d)\n",
+		mbox, krx_chan->queue_id, (u32)krx_chan->queue,
+		krx_chan->flow_id, (u32)krx_chan->channel,
+		krx_chan->packet_type);
+
+	return 0;
+
+fail:
+	keystone_rio_inb_knav_free(krx_chan);
+
+	return err;
+}
+
+static int keystone_rio_get_rxu_map(struct keystone_rio_data *krio_priv)
+{
+	int id;
+	unsigned long bit_sz = sizeof(krio_priv->rxu_map_bitmap) * 8;
+
+	id = find_first_zero_bit(&krio_priv->rxu_map_bitmap[0], bit_sz);
+
+	while (id < krio_priv->rxu_map_start)
+		id = find_next_zero_bit(&krio_priv->rxu_map_bitmap[0],
+					bit_sz, ++id);
+
+	if (id > krio_priv->rxu_map_end)
+		return -1;
+
+	__set_bit(id, &krio_priv->rxu_map_bitmap[0]);
+
+	return id;
+}
+
+static void keystone_rio_free_rxu_map(int id,
+				      struct keystone_rio_data *krio_priv)
+{
+	clear_bit(id, &krio_priv->rxu_map_bitmap[0]);
+}
+
+/**
+ * keystone_rio_map_mbox - Map a mailbox to a given queue for both type 11
+ * and type 9 packets.
+ * @mbox: mailbox to map
+ * @queue: associated queue number
+ * @flow_id: flow Id
+ * @size: device Id size
+ *
+ * Returns %0 on success or %-ENOMEM on failure.
+ */
+static int keystone_rio_map_mbox(int mbox,
+				 int queue,
+				 int flow_id,
+				 int size,
+				 struct keystone_rio_data *krio_priv)
+{
+	struct keystone_rio_mbox_info *rx_mbox = &krio_priv->rx_mbox[mbox];
+	u32 mapping_entry_low = 0;
+	u32 mapping_entry_high = 0;
+	u32 mapping_entry_qid;
+	u32 mapping_t9_reg[3];
+	u32 pkt_type;
+	int i;
+
+	/* Retrieve the packet type */
+	pkt_type = keystone_rio_mp_get_type(mbox, krio_priv);
+
+	if (pkt_type == RIO_PACKET_TYPE_MESSAGE) {
+		/*
+		 * Map the multi-segment mailbox to the corresponding Rx queue
+		 * for type 11.
+		 * Given mailbox, all letters, srcid = 0
+		 */
+		mapping_entry_low = ((mbox & 0x1f) << 16) | (0x3f000000);
+
+		/*
+		 * Multi-segment messaging and promiscuous (don't care about
+		 * src/dst id)
+		 */
+		mapping_entry_high = KEYSTONE_RIO_MAP_FLAG_SEGMENT
+			| KEYSTONE_RIO_MAP_FLAG_SRC_PROMISC
+			| KEYSTONE_RIO_MAP_FLAG_DST_PROMISC;
+	} else {
+		/*
+		 * Map the multi-segment mailbox for type 9
+		 * accept all COS and srcid = 0, use promiscuous (don't care
+		 * about src/dst id)
+		 */
+		mapping_t9_reg[0] = 0;
+		mapping_t9_reg[1] = KEYSTONE_RIO_MAP_FLAG_SRC_PROMISC
+			| KEYSTONE_RIO_MAP_FLAG_DST_PROMISC;
+		mapping_t9_reg[2] = (0xffff << 16)
+			| (keystone_rio_mbox_to_strmid(mbox, krio_priv));
+	}
+
+	/* Set TT flag */
+	if (size) {
+		mapping_entry_high |= KEYSTONE_RIO_MAP_FLAG_TT_16;
+		mapping_t9_reg[1]  |= KEYSTONE_RIO_MAP_FLAG_TT_16;
+	}
+
+	/* QMSS/PktDMA mapping (generic for both type 9 and 11) */
+	mapping_entry_qid = (queue & 0x3fff) | (flow_id << 16);
+
+	i = keystone_rio_get_rxu_map(krio_priv);
+	if (i < 0)
+		return -ENOMEM;
+
+	rx_mbox->rxu_map_id[0] = i;
+
+	dev_dbg(krio_priv->dev,
+		"using RXU map %d @ 0x%p: mbox=%d, flow_id=%d, queue=%d pkt_type=%d\n",
+		i, &krio_priv->regs->rxu_map[i], mbox,
+		flow_id, queue, pkt_type);
+
+	if (pkt_type == RIO_PACKET_TYPE_MESSAGE) {
+		/* Set packet type 11 rx mapping */
+		__raw_writel(mapping_entry_low,
+			     &krio_priv->regs->rxu_map[i].ltr_mbox_src);
+		__raw_writel(mapping_entry_high,
+			     &krio_priv->regs->rxu_map[i].dest_prom_seg);
+	} else {
+		/* Set packet type 9 rx mapping */
+		__raw_writel(mapping_t9_reg[0],
+			     &krio_priv->regs->rxu_type9_map[i].cos_src);
+		__raw_writel(mapping_t9_reg[1],
+			     &krio_priv->regs->rxu_type9_map[i].dest_prom);
+		__raw_writel(mapping_t9_reg[2],
+			     &krio_priv->regs->rxu_type9_map[i].stream);
+	}
+
+	__raw_writel(mapping_entry_qid,
+		     &krio_priv->regs->rxu_map[i].flow_qid);
+
+	if (pkt_type == RIO_PACKET_TYPE_MESSAGE) {
+		/*
+		 * The RapidIO peripheral looks at the incoming RapidIO msgs
+		 * and if there is only one segment (the whole msg fits into
+		 * one RapidIO msg), the peripheral uses the single segment
+		 * mapping table. Therefore we need to map the single-segment
+		 * mailbox too.
+		 * The same Rx CPPI Queue is used (as for the multi-segment
+		 * mailbox).
+		 */
+		mapping_entry_high &= ~KEYSTONE_RIO_MAP_FLAG_SEGMENT;
+
+		i = keystone_rio_get_rxu_map(krio_priv);
+		if (i < 0)
+			return -ENOMEM;
+
+		rx_mbox->rxu_map_id[1] = i;
+
+		dev_dbg(krio_priv->dev,
+			"using RXU map %d @ 0x%p: mbox=%d, flow_id=%d, queue=%d pkt_type=%d\n",
+			i, &krio_priv->regs->rxu_map[i], mbox,
+			flow_id, queue, pkt_type);
+
+		__raw_writel(mapping_entry_low,
+			     &krio_priv->regs->rxu_map[i].ltr_mbox_src);
+		__raw_writel(mapping_entry_high,
+			     &krio_priv->regs->rxu_map[i].dest_prom_seg);
+		__raw_writel(mapping_entry_qid,
+			     &krio_priv->regs->rxu_map[i].flow_qid);
+	}
+
+	return 0;
+}
+
+/**
+ * keystone_rio_open_inb_mbox - Initialize KeyStone inbound mailbox
+ * @mport: Master port implementing the inbound message unit
+ * @dev_id: Device specific pointer to pass on event
+ * @mbox: Mailbox to open
+ * @entries: Number of entries in the inbound mailbox ring
+ *
+ * Initializes buffer ring, request the inbound message interrupt,
+ * and enables the inbound message unit. Returns %0 on success
+ * and %-EINVAL, %-EBUSY or %-ENOMEM on failure.
+ */
+int keystone_rio_open_inb_mbox(struct rio_mport *mport,
+			       void *dev_id,
+			       int mbox,
+			       int entries)
+{
+	struct keystone_rio_data *krio_priv = mport->priv;
+	struct keystone_rio_mbox_info *rx_mbox = &krio_priv->rx_mbox[mbox];
+	struct keystone_rio_rx_chan_info *krx_chan =
+		&krio_priv->rx_channels[mbox];
+	int res;
+
+	if (mbox >= KEYSTONE_RIO_MAX_MBOX)
+		return -EINVAL;
+
+	/*
+	 * Check that number of entries is a power of two to ease ring
+	 * management
+	 */
+	if ((entries & (entries - 1)) != 0)
+		return -EINVAL;
+
+	/* Check if already initialized */
+	if (rx_mbox->port)
+		return -EBUSY;
+
+	dev_dbg(krio_priv->dev,
+		"open_inb_mbox: mport=0x%p, dev_id=0x%p, mbox=%d, entries=%d\n",
+		mport, dev_id, mbox, entries);
+
+	/* Initialization of RapidIO inbound MP */
+	res = keystone_rio_mp_inb_init(mbox, krio_priv);
+	if (res)
+		return res;
+
+	rx_mbox->dev_id    = dev_id;
+	rx_mbox->entries   = entries;
+	rx_mbox->port      = mport;
+	rx_mbox->running   = 1;
+
+	/* Map the mailbox to queue/flow */
+	res = keystone_rio_map_mbox(mbox,
+				    krx_chan->queue_id,
+				    krx_chan->flow_id,
+				    mport->sys_size,
+				    krio_priv);
+
+	return res;
+}
+
+/**
+ * keystone_rio_close_inb_mbox - Shutdown KeyStone inbound mailbox
+ * @mport: Master port implementing the inbound message unit
+ * @mbox: Mailbox to close
+ *
+ * Disables the outbound message unit, stop queues and free all resources
+ */
+void keystone_rio_close_inb_mbox(struct rio_mport *mport, int mbox)
+{
+	struct keystone_rio_data *krio_priv = mport->priv;
+	struct keystone_rio_mbox_info *rx_mbox = &krio_priv->rx_mbox[mbox];
+
+	dev_dbg(krio_priv->dev, "close inb mbox: mport = 0x%p, mbox = %d\n",
+		mport, mbox);
+
+	if (mbox >= KEYSTONE_RIO_MAX_MBOX)
+		return;
+
+	rx_mbox->running = 0;
+
+	if (!rx_mbox->port)
+		return;
+
+	rx_mbox->port = NULL;
+
+	keystone_rio_inb_exit(mbox, krio_priv);
+
+	/* Release associated resources */
+	keystone_rio_free_rxu_map(rx_mbox->rxu_map_id[0], krio_priv);
+	keystone_rio_free_rxu_map(rx_mbox->rxu_map_id[1], krio_priv);
+}
+
+/**
+ * keystone_rio_hw_add_inb_buffer - Add buffer to the KeyStone inbound message
+ * queue
+ * @mport: Master port implementing the inbound message unit
+ * @mbox: Inbound mailbox number
+ * @buf: Buffer to add to inbound queue
+ *
+ * Adds the @buf buffer to the KeyStone inbound message queue. Returns
+ * %0 on success or %-EINVAL on failure.
+ */
+int keystone_rio_hw_add_inb_buffer(struct rio_mport *mport,
+				   int mbox,
+				   void *buffer)
+{
+	struct keystone_rio_data *krio_priv = mport->priv;
+	struct keystone_rio_rx_chan_info *krx_chan =
+		&krio_priv->rx_channels[mbox];
+	struct knav_dma_desc *desc;
+	u32 desc_info, pkt_info;
+	unsigned int buf_len, dma_sz;
+	dma_addr_t dma, dma_addr;
+	u32 pad[2];
+	int fdq = 0;
+	int ret;
+
+	/* Allocate descriptor */
+	desc = knav_pool_desc_get(krx_chan->pool);
+	if (IS_ERR_OR_NULL(desc)) {
+		dev_dbg(krio_priv->dev, "out of rx pool desc\n");
+		return -ENOMEM;
+	}
+
+	/* Fill the proper free queue */
+	while (krx_chan->queue_depths[fdq] &&
+	       krx_chan->buffer_sizes[fdq] &&
+	       (knav_queue_get_count(krx_chan->fdq[fdq])
+		>= krx_chan->queue_depths[fdq])) {
+		int nfdq = fdq + 1;
+
+		if (unlikely(IS_ERR_OR_NULL(krx_chan->fdq[nfdq]) ||
+			     (nfdq >= KNAV_DMA_FDQ_PER_CHAN))) {
+			dev_err(krio_priv->dev,
+				"free queue %d is full and cannot use free queue %d!!!\n",
+				fdq, nfdq);
+			return -ENOMEM;
+		}
+
+		fdq = nfdq;
+	}
+
+	buf_len = krx_chan->buffer_sizes[fdq];
+	dma_addr = dma_map_single(krio_priv->dev,
+				  buffer,
+				  buf_len,
+				  DMA_TO_DEVICE);
+
+	if (IS_ERR_OR_NULL((void *)dma_addr)) {
+		dev_err(krio_priv->dev, "dma map failed\n");
+		knav_pool_desc_put(krx_chan->pool, desc);
+		return -EINVAL;
+	}
+
+	pad[0] = (u32)buffer;
+	pad[1] = 0; /* Slot is not used in receive */
+
+	desc_info  =  KNAV_DMA_DESC_PS_INFO_IN_DESC;
+	desc_info |= buf_len & KNAV_DMA_DESC_PKT_LEN_MASK;
+	pkt_info   =  KNAV_DMA_DESC_HAS_EPIB;
+	pkt_info  |= KNAV_DMA_NUM_PS_WORDS << KNAV_DMA_DESC_PSLEN_SHIFT;
+	pkt_info  |= (krx_chan->queue_id & KNAV_DMA_DESC_RETQ_MASK) <<
+		KNAV_DMA_DESC_RETQ_SHIFT;
+	set_pkt_info(dma_addr, buf_len, 0, desc);
+	set_org_pkt_info(dma_addr, buf_len, desc);
+	set_pad_info(pad[0], pad[1], desc);
+	set_desc_info(desc_info, pkt_info, desc);
+
+	/* Push to FDQs */
+	ret = knav_pool_desc_map(krx_chan->pool, desc, sizeof(*desc), &dma,
+				 &dma_sz);
+	if (unlikely(ret)) {
+		dev_err(krio_priv->dev, "%s() failed to map desc\n", __func__);
+		dma_unmap_single(krio_priv->dev, dma_addr, buf_len,
+				 DMA_TO_DEVICE);
+		knav_pool_desc_put(krx_chan->pool, desc);
+		return -ENOMEM;
+	}
+
+	knav_queue_push(krx_chan->fdq[fdq], dma, sizeof(*desc), 0);
+
+	return 0;
+}
+
+/**
+ * keystone_rio_hw_get_inb_message - Fetch inbound message from
+ * the KeyStone message unit
+ * @mport: Master port implementing the inbound message unit
+ * @mbox: Inbound mailbox number
+ *
+ * Gets the next available inbound message from the inbound message queue.
+ * A pointer to the message is returned on success or NULL on failure.
+ */
+void *keystone_rio_hw_get_inb_message(struct rio_mport *mport, int mbox)
+{
+	struct keystone_rio_data *krio_priv = mport->priv;
+	struct keystone_rio_rx_chan_info *krx_chan =
+					&krio_priv->rx_channels[mbox];
+	unsigned int dma_sz, buf_len;
+	struct knav_dma_desc *desc = NULL;
+	dma_addr_t dma_desc, dma_buff;
+	void *buffer = NULL;
+	u32 cc, pad1;
+
+	/* Get incoming descriptor */
+	dma_desc = knav_queue_pop(krx_chan->queue, &dma_sz);
+	if (!dma_desc)
+		goto end;
+
+	desc = knav_pool_desc_unmap(krx_chan->pool, dma_desc, dma_sz);
+	if (unlikely(!desc)) {
+		dev_err(krio_priv->dev, "failed to unmap Rx desc\n");
+		goto end;
+	}
+
+	get_pkt_info(&dma_buff, &buf_len, &dma_desc, desc);
+	get_pad_info((u32 *)&buffer, &pad1, desc);
+
+	if (unlikely(!buffer)) {
+		dev_err(krio_priv->dev, "NULL bufptr in desc\n");
+		goto end;
+	}
+
+	dma_unmap_single(krio_priv->dev, dma_buff, buf_len, DMA_FROM_DEVICE);
+
+	/* Check CC from PS descriptor word 1 */
+	cc = keystone_rio_mp_get_cc(desc->psdata[1], krx_chan->packet_type);
+	if (cc) {
+		dev_warn(krio_priv->dev,
+			 "MP receive completion code is non zero (0x%x)\n",
+			 cc);
+	}
+
+end:
+	/* Free the descriptor */
+	if (desc)
+		knav_pool_desc_put(krx_chan->pool, desc);
+
+	return buffer;
+}
+
+static void keystone_rio_outb_knav_free(
+	struct keystone_rio_tx_chan_info *ktx_chan)
+{
+	/* Close channel */
+	if (!IS_ERR_OR_NULL(ktx_chan->channel)) {
+		knav_dma_close_channel(ktx_chan->channel);
+		ktx_chan->channel = NULL;
+	}
+
+	/* Close transmit queue */
+	if (!IS_ERR_OR_NULL(ktx_chan->queue)) {
+		knav_queue_close(ktx_chan->queue);
+		ktx_chan->queue = NULL;
+	}
+
+	/* Close completion queue */
+	if (!IS_ERR_OR_NULL(ktx_chan->complet_queue)) {
+		knav_queue_close(ktx_chan->complet_queue);
+		ktx_chan->complet_queue = NULL;
+	}
+
+	/* Close garbage queue */
+	if (!IS_ERR_OR_NULL(ktx_chan->garbage_queue)) {
+		knav_queue_close(ktx_chan->garbage_queue);
+		ktx_chan->garbage_queue = NULL;
+	}
+
+	/* Free pools and associated descriptors */
+	if (!IS_ERR_OR_NULL(ktx_chan->pool)) {
+		knav_pool_destroy(ktx_chan->pool);
+		ktx_chan->pool = NULL;
+	}
+}
+
+static void keystone_rio_mp_outb_exit(struct keystone_rio_data *krio_priv,
+				      int mbox)
+{
+	struct keystone_rio_tx_chan_info *ktx_chan =
+					&krio_priv->tx_channels[mbox];
+
+	if (!(ktx_chan->channel))
+		return;
+
+	knav_queue_disable_notify(ktx_chan->complet_queue);
+	knav_queue_disable_notify(ktx_chan->garbage_queue);
+
+	/* Free all navigator Tx resources */
+	keystone_rio_outb_knav_free(ktx_chan);
+}
+
+static int keystone_rio_mp_outb_init(u8 port_id,
+				     int mbox,
+				     struct keystone_rio_data *krio_priv)
+{
+	struct keystone_rio_tx_chan_info *ktx_chan =
+					&krio_priv->tx_channels[mbox];
+	struct knav_queue_notify_config notify_cfg;
+	struct knav_dma_cfg config;
+	u8 name[24];
+	int err = -ENODEV;
+
+	/* If already initialized, return without error */
+	if (ktx_chan->channel)
+		return 0;
+
+	if (!ktx_chan->name) {
+		dev_err(krio_priv->dev,
+			"Tx channel name for mbox %d is not defined!\n",
+			mbox);
+		return err;
+	}
+
+	memset(&config, 0, sizeof(config));
+	config.direction         = DMA_MEM_TO_DEV;
+	config.u.tx.filt_einfo   = false;
+	config.u.tx.filt_pswords = false;
+	config.u.tx.priority     = DMA_PRIO_MED_L;
+
+	ktx_chan->channel = knav_dma_open_channel(krio_priv->dev,
+						  ktx_chan->name,
+						  &config);
+	if (IS_ERR_OR_NULL(ktx_chan->channel)) {
+		dev_err(krio_priv->dev,
+			"failed opening Tx chan(%s\n",
+			ktx_chan->name);
+		err = PTR_ERR(ktx_chan->channel);
+		goto fail;
+	}
+
+	/* Create Tx descriptor pool */
+	snprintf(name, sizeof(name), "tx-pool-rio-mbox-%d", mbox);
+	ktx_chan->pool = knav_pool_create(name,
+					  ktx_chan->pool_size,
+					  ktx_chan->pool_region_id);
+	if (IS_ERR_OR_NULL(ktx_chan->pool)) {
+		dev_err(krio_priv->dev,
+			"couldn't create Tx pool\n");
+		err = PTR_ERR(ktx_chan->pool);
+		goto fail;
+	}
+
+	ktx_chan->priv = krio_priv;
+	ktx_chan->mbox_id = mbox;
+
+	/* Open Tx completion queue */
+	snprintf(name, sizeof(name), "tx-compl-rio-mbox-%d", mbox);
+	ktx_chan->complet_queue = knav_queue_open(name,
+						  ktx_chan->complet_queue_id,
+						  0);
+	if (IS_ERR_OR_NULL(ktx_chan->complet_queue)) {
+		err = PTR_ERR(ktx_chan->complet_queue);
+		goto fail;
+	}
+
+	ktx_chan->complet_queue_id = knav_queue_get_id(ktx_chan->complet_queue);
+
+	/* Set notification for Tx completion */
+	notify_cfg.fn = keystone_rio_tx_complete;
+	notify_cfg.fn_arg = ktx_chan;
+	err = knav_queue_device_control(ktx_chan->complet_queue,
+					KNAV_QUEUE_SET_NOTIFIER,
+					(unsigned long)&notify_cfg);
+	if (err)
+		goto fail;
+
+	knav_queue_disable_notify(ktx_chan->complet_queue);
+
+	/* Open Tx submit queue */
+	snprintf(name, sizeof(name), "tx-submit-rio-mbox-%d", mbox);
+	ktx_chan->queue = knav_queue_open(name,
+					  ktx_chan->queue_id,
+					  0);
+	if (IS_ERR_OR_NULL(ktx_chan->queue)) {
+		err = PTR_ERR(ktx_chan->queue);
+		goto fail;
+	}
+
+	ktx_chan->queue_id = knav_queue_get_id(ktx_chan->queue);
+
+	/* Set the output port Id to the corresponding Tx queue */
+	__raw_writel(port_id << 4, &krio_priv->regs->tx_queue_sch_info[mbox]);
+
+	knav_queue_enable_notify(ktx_chan->complet_queue);
+
+	/* Open Tx garbage queue */
+	snprintf(name, sizeof(name), "tx-error-rio-mbox-%d", mbox);
+	ktx_chan->garbage_queue = knav_queue_open(name,
+						  ktx_chan->garbage_queue_id,
+						  0);
+	if (IS_ERR_OR_NULL(ktx_chan->garbage_queue)) {
+		err = PTR_ERR(ktx_chan->garbage_queue);
+		goto fail;
+	}
+
+	ktx_chan->garbage_queue_id = knav_queue_get_id(ktx_chan->garbage_queue);
+
+	/* Set notification for Tx garbage queue */
+	notify_cfg.fn = keystone_rio_tx_error;
+	notify_cfg.fn_arg = ktx_chan;
+	err = knav_queue_device_control(ktx_chan->garbage_queue,
+					KNAV_QUEUE_SET_NOTIFIER,
+					(unsigned long)&notify_cfg);
+	if (err)
+		goto fail;
+
+	/* Set the garbage queues */
+	__raw_writel((ktx_chan->garbage_queue_id << 16) |
+		     ktx_chan->garbage_queue_id,
+		     &krio_priv->regs->garbage_coll_qid[0]);
+	__raw_writel((ktx_chan->garbage_queue_id << 16) |
+		     ktx_chan->garbage_queue_id,
+		     &krio_priv->regs->garbage_coll_qid[1]);
+	__raw_writel(ktx_chan->garbage_queue_id,
+		     &krio_priv->regs->garbage_coll_qid[2]);
+
+	knav_queue_enable_notify(ktx_chan->garbage_queue);
+
+	dev_dbg(krio_priv->dev,
+		"opened Tx channel: mbox = %d, complete_q = %d (0x%x), tx_q = %d (0x%x), garbage_q = %d (0x%x), channel = 0x%x, port id = %d\n",
+		mbox, ktx_chan->complet_queue_id, (u32)ktx_chan->complet_queue,
+		ktx_chan->queue_id, (u32)ktx_chan->queue,
+		ktx_chan->garbage_queue_id, (u32)ktx_chan->garbage_queue,
+		(u32)ktx_chan->channel, port_id);
+
+	return 0;
+
+fail:
+	keystone_rio_outb_knav_free(ktx_chan->channel);
+
+	return err;
+}
+
+/**
+ * keystone_rio_open_outb_mbox - Initialize KeyStone outbound mailbox
+ * @mport: Master port implementing the outbound message unit
+ * @dev_id: Device specific pointer to pass on event
+ * @mbox: Mailbox to open
+ * @entries: Number of entries in the outbound mailbox ring
+ *
+ * Initializes buffer ring, request the outbound message interrupt,
+ * and enables the outbound message unit. Returns %0 on success and
+ * %-EINVAL, %-EBUSY or %-ENOMEM on failure.
+ */
+int keystone_rio_open_outb_mbox(struct rio_mport *mport,
+				void *dev_id,
+				int mbox,
+				int entries)
+{
+	struct keystone_rio_data *krio_priv = mport->priv;
+	struct keystone_rio_mbox_info *tx_mbox = &krio_priv->tx_mbox[mbox];
+	int res;
+
+	if (mbox >= KEYSTONE_RIO_MAX_MBOX)
+		return -EINVAL;
+
+	/*
+	 * Check that number of entries is a power of two to ease ring
+	 * management
+	 */
+	if ((entries & (entries - 1)) != 0)
+		return -EINVAL;
+
+	/* Check if already initialized */
+	if (tx_mbox->port)
+		return -EBUSY;
+
+	dev_dbg(krio_priv->dev,
+		"open_outb_mbox: mport = 0x%x, dev_id = 0x%x, mbox = %d, entries = %d\n",
+		(u32)mport, (u32)dev_id, mbox, entries);
+
+	/* Initialization of RapidIO outbound MP */
+	res = keystone_rio_mp_outb_init(mport->index, mbox, krio_priv);
+	if (res)
+		return res;
+
+	tx_mbox->dev_id  = dev_id;
+	tx_mbox->entries = entries;
+	tx_mbox->port    = mport;
+	tx_mbox->running = 1;
+	atomic_set(&tx_mbox->slot, 0);
+
+	return 0;
+}
+
+/**
+ * keystone_rio_close_outb_mbox - Shutdown KeyStone outbound mailbox
+ * @mport: Master port implementing the outbound message unit
+ * @mbox: Mailbox to close
+ *
+ * Disables the outbound message unit, stop queues and free all resources
+ */
+void keystone_rio_close_outb_mbox(struct rio_mport *mport, int mbox)
+{
+	struct keystone_rio_data *krio_priv = mport->priv;
+	struct keystone_rio_mbox_info *tx_mbox = &krio_priv->tx_mbox[mbox];
+
+	dev_dbg(krio_priv->dev, "close outb mbox: mport = 0x%x, mbox = %d\n",
+		(u32)mport, mbox);
+
+	if (mbox >= KEYSTONE_RIO_MAX_MBOX)
+		return;
+
+	tx_mbox->running = 0;
+
+	if (!tx_mbox->port)
+		return;
+
+	tx_mbox->port = NULL;
+
+	keystone_rio_mp_outb_exit(krio_priv, mbox);
+}
+
+/* Gerneric handling of both Tx completion and error */
+static void keystone_rio_tx_handler(
+	struct keystone_rio_tx_chan_info *ktx_chan,
+	void *queue,
+	int error)
+{
+	struct keystone_rio_data *krio_priv = ktx_chan->priv;
+	int mbox_id  = ktx_chan->mbox_id;
+	struct keystone_rio_mbox_info *mbox = &krio_priv->tx_mbox[mbox_id];
+	struct rio_mport *port = mbox->port;
+	void *dev_id  = mbox->dev_id;
+	void *buffer = NULL;
+	unsigned int dma_sz;
+	dma_addr_t dma;
+	int slot;
+
+	knav_queue_disable_notify(queue);
+
+	while ((dma = knav_queue_pop(queue, &dma_sz))) {
+		struct knav_dma_desc *desc;
+
+		/* Unmap descriptor */
+		desc = knav_pool_desc_unmap(ktx_chan->pool, dma, dma_sz);
+		if (unlikely(!desc)) {
+			dev_err(krio_priv->dev, "failed to unmap Tx desc\n");
+			continue;
+		}
+
+		/* Retrieve virtual temporary buffer if any and slot id */
+		get_pad_info((u32 *)&buffer, &slot, desc);
+
+		/* Release descriptor */
+		knav_pool_desc_put(ktx_chan->pool, desc);
+
+		/* kfree is working even if pointer is null */
+		kfree(buffer);
+
+		/*
+		 * If it is an error, we do not notify the upper layer which
+		 * should be able to manage non received acknowledgment with
+		 * its slot.
+		 */
+		if (likely((mbox->running) && (!error))) {
+			port->outb_msg[mbox_id].mcback(port,
+						       dev_id,
+						       mbox_id,
+						       slot % mbox->entries);
+		}
+
+		if (unlikely(error))
+			dev_dbg(krio_priv->dev, "error with Tx slot %d\n",
+				slot);
+	}
+
+	knav_queue_enable_notify(queue);
+}
+
+static void keystone_rio_tx_error(void *data)
+{
+	struct keystone_rio_tx_chan_info *ktx_chan = data;
+
+	keystone_rio_tx_handler(ktx_chan, ktx_chan->garbage_queue, 1);
+}
+
+static void keystone_rio_tx_complete(void *data)
+{
+	struct keystone_rio_tx_chan_info *ktx_chan = data;
+
+	keystone_rio_tx_handler(ktx_chan, ktx_chan->complet_queue, 0);
+}
+
+/**
+ * keystone_rio_hw_add_outb_message - Add a message to the KeyStone
+ * outbound message queue
+ * @mport: Master port with outbound message queue
+ * @rdev: Target of outbound message
+ * @mbox: Outbound mailbox
+ * @buffer: Message to add to outbound queue
+ * @len: Length of message
+ *
+ * Adds the @buffer message to the KeyStone outbound message queue. Returns
+ * %0 on success or %-EBUSY on failure.
+ */
+int keystone_rio_hw_add_outb_message(struct rio_mport *mport,
+				     struct rio_dev *rdev,
+				     int mbox,
+				     void *buffer,
+				     const size_t len)
+{
+	struct keystone_rio_data *krio_priv = mport->priv;
+	struct keystone_rio_tx_chan_info *ktx_chan =
+					&krio_priv->tx_channels[mbox];
+	struct keystone_rio_mbox_info *ktx_mbox =
+					&krio_priv->tx_mbox[mbox];
+	struct knav_dma_desc *desc;
+	dma_addr_t dma_addr, dma;
+	unsigned int dma_sz;
+	u32 plen;
+	u32 packet_type;
+	int ret = 0;
+	void *send_buffer = NULL, *p_buffer;
+	u32 desc_info, pkt_info;
+	u32 pad[2];
+
+	if (unlikely((ktx_mbox->port != mport) || (!rdev)))
+		return -EINVAL;
+
+	/*
+	 * Ensure that the number of bytes being transmitted is a multiple
+	 * of double-word. This is as per the specification.
+	 */
+	plen = ((len + 7) & ~0x7);
+
+#if defined(CONFIG_RAPIDIO_CHMAN) || defined(CONFIG_RAPIDIO_CHMAN_MODULE)
+	/*
+	 * Copy the outbound message in a temporary buffer. This is needed for
+	 * RIO_CM.
+	 */
+	send_buffer = kmalloc(plen, GFP_ATOMIC | GFP_DMA);
+	if (unlikely(!send_buffer)) {
+		dev_err(krio_priv->dev, "failed to alloc send buffer\n");
+		return -ENOMEM;
+	}
+
+	memcpy(send_buffer, buffer, plen);
+	p_buffer = send_buffer;
+#else
+	p_buffer = buffer;
+#endif
+
+	/* Map the linear buffer */
+	dma_addr = dma_map_single(krio_priv->dev, p_buffer, plen,
+				  DMA_TO_DEVICE);
+	if (unlikely(!dma_addr)) {
+		dev_err(krio_priv->dev, "failed to map skb buffer\n");
+		ret = -ENOMEM;
+		goto fail;
+	}
+
+	desc = knav_pool_desc_get(ktx_chan->pool);
+	if (unlikely(IS_ERR_OR_NULL(desc))) {
+		dev_err(krio_priv->dev, "out of TX desc\n");
+		dma_unmap_single(krio_priv->dev, dma_addr, plen, DMA_TO_DEVICE);
+		ret = -ENOMEM;
+		goto fail;
+	}
+
+	/* Save virtual buffer address for copy-case and slot */
+	pad[0] = (u32)send_buffer; /* NULL if not using copy */
+	pad[1] = atomic_read(&ktx_mbox->slot);
+
+	/*
+	 * Move slot index to the next message to be sent.
+	 * Client is in charge of freeing the associated buffers
+	 * because we do not have explicit hardware ring but queues, we
+	 * do not know where we are in the sw ring. Let's try to keep
+	 * slot in sync with client.
+	 */
+	atomic_inc(&ktx_mbox->slot);
+
+	/* Word 1: source id and dest id (common to packet 11 and packet 9) */
+	desc->psdata[0] = (rdev->destid & 0xffff)
+		| (mport->host_deviceid << 16);
+
+	/*
+	 * Warning - Undocumented HW requirement:
+	 *      For type 9, packet type MUST be set to 30 in
+	 *	keystone_hw_desc.desc_info[29:25] bits.
+	 *
+	 *	For type 11, setting packet type to 31 in
+	 *	those bits is optional.
+	 */
+	if (keystone_rio_mp_get_type(mbox, krio_priv)
+	    == RIO_PACKET_TYPE_MESSAGE) {
+		/* Packet 11 case (Message) */
+		packet_type = 31;
+
+		/* Word 2: ssize = 32 dword, 4 retries, letter = 0, mbox */
+		desc->psdata[1] = (KEYSTONE_RIO_MSG_SSIZE << 17) | (4 << 21)
+			| (mbox & 0x3f);
+	} else {
+		/* Packet 9 case (Data Streaming) */
+		packet_type = 30;
+
+		/* Word 2: COS = 0, stream id */
+		desc->psdata[1] =
+			keystone_rio_mbox_to_strmid(mbox, krio_priv) << 16;
+	}
+
+	if ((rdev->net) && (rdev->net->hport->sys_size))
+		desc->psdata[1] |= KEYSTONE_RIO_DESC_FLAG_TT_16; /* tt */
+
+	desc_info  = KNAV_DMA_DESC_PS_INFO_IN_DESC;
+	desc_info |= plen & KNAV_DMA_DESC_PKT_LEN_MASK;
+	desc_info |= packet_type << 25;
+	pkt_info   = KNAV_DMA_DESC_HAS_EPIB;
+	pkt_info  |= KNAV_DMA_NUM_PS_WORDS << KNAV_DMA_DESC_PSLEN_SHIFT;
+	pkt_info  |= (ktx_chan->complet_queue_id & KNAV_DMA_DESC_RETQ_MASK) <<
+		KNAV_DMA_DESC_RETQ_SHIFT;
+	set_pkt_info(dma_addr, len, 0, desc);
+	set_org_pkt_info(dma_addr, len, desc);
+	set_pad_info(pad[0], pad[1], desc);
+	set_desc_info(desc_info, pkt_info, desc);
+
+	/* Map and push the descriptor */
+	ret = knav_pool_desc_map(ktx_chan->pool, desc, sizeof(*desc), &dma,
+				 &dma_sz);
+	if (unlikely(ret)) {
+		dev_err(krio_priv->dev, "%s() failed to map desc\n", __func__);
+		dma_unmap_single(krio_priv->dev, dma_addr, plen, DMA_TO_DEVICE);
+		knav_pool_desc_put(ktx_chan->pool, desc);
+		ret = -ENOMEM;
+		goto fail;
+	}
+
+	knav_queue_push(ktx_chan->queue, dma, dma_sz, 0);
+
+	return ret;
+
+fail:
+	kfree(send_buffer);
+
+	return ret;
+}
diff --git a/drivers/rapidio/devices/keystone_rio_serdes.c b/drivers/rapidio/devices/keystone_rio_serdes.c
new file mode 100644
index 0000000..e41e3a1
--- /dev/null
+++ b/drivers/rapidio/devices/keystone_rio_serdes.c
@@ -0,0 +1,2061 @@
+/*
+ * Texas Instruments Keystone SerDes driver
+ * Authors: Aurelien Jacquiot <a-jacquiot@ti.com>
+ *
+ * This is the Rapidio SerDes driver for Keystone devices. This is
+ * required to support RapidIO functionality on K2HK devices.
+ *
+ * Copyright (C) 2016 Texas Instruments Incorporated - http://www.ti.com/
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ * Redistributions of source code must retain the above copyright
+ * notice, this list of conditions and the following disclaimer.
+ *
+ * Redistributions in binary form must reproduce the above copyright
+ * notice, this list of conditions and the following disclaimer in the
+ * documentation and/or other materials provided with the
+ * distribution.
+ *
+ * Neither the name of Texas Instruments Incorporated nor the names of
+ * its contributors may be used to endorse or promote products derived
+ * from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/device.h>
+#include <linux/platform_device.h>
+#include <linux/uaccess.h>
+#include <linux/io.h>
+#include <linux/delay.h>
+#include <linux/sysfs.h>
+
+#include "keystone_rio_serdes.h"
+
+#define for_each_lanes(lane_mask, lane)					\
+	for_each_set_bit((lane),					\
+			 (const unsigned long *)&(lane_mask),		\
+			 KEYSTONE_SERDES_MAX_LANES)
+
+#define reg_rmw(addr, value, mask)					\
+	__raw_writel(((__raw_readl(addr) & (~(mask))) | ((value) & (mask))), \
+		     (addr))
+
+#define reg_fmkr(msb, lsb, val) \
+	(((val) & ((1 << ((msb) - (lsb) + 1)) - 1)) << (lsb))
+
+#define reg_finsr(addr, msb, lsb, val)					\
+	__raw_writel(((__raw_readl(addr)				\
+		       & ~(((1 << ((msb) - (lsb) + 1)) - 1) << (lsb)))	\
+		      | reg_fmkr(msb, lsb, val)), (addr))
+
+static inline void reg_fill_field(void __iomem *regs, int offset, int width,
+				  u32 value)
+{
+	u32 mask;
+	int i;
+
+	for (i = 0, mask = 0; i < width; i++)
+		mask = (mask << 1) | 1;
+
+	value &= mask;
+	value <<= offset;
+	mask  <<= offset;
+
+	reg_rmw(regs, value, mask);
+}
+
+struct k1_rio_serdes_regs {
+	u32	pll;
+	struct {
+		u32	rx;
+		u32	tx;
+	} channel[4];
+};
+
+static int k1_rio_serdes_config_lanes(
+	u32 lanes,
+	u32 baud,
+	struct keystone_serdes_data *serdes)
+{
+	u32 lane;
+	int res = 0;
+	u32 val;
+	unsigned long timeout;
+	struct k1_rio_serdes_regs *serdes_regs =
+		(struct k1_rio_serdes_regs *)serdes->regs;
+	struct device *dev = serdes->dev;
+
+	dev_dbg(dev, "SerDes: configuring SerDes for lane mask 0x%x\n", lanes);
+
+	__raw_writel(0x0229, (void __iomem *)serdes_regs->pll);
+
+	for_each_lanes(lanes, lane) {
+		__raw_writel(0x00440495, &serdes_regs->channel[lane].rx);
+		__raw_writel(0x00180795, &serdes_regs->channel[lane].rx);
+	}
+
+	timeout = jiffies + msecs_to_jiffies(KEYSTONE_SERDES_TIMEOUT);
+
+	while (1) {
+		val = __raw_readl(serdes->sts_reg);
+
+		if ((val & 0x1) != 0x1)
+			break;
+
+		if (time_after(jiffies, timeout)) {
+			res = -EAGAIN;
+			break;
+		}
+
+		usleep_range(10, 50);
+	}
+
+	return res;
+}
+
+static int k1_rio_serdes_start_tx_lanes(
+	u32 lanes,
+	struct keystone_serdes_data *serdes)
+{
+	return 0;
+}
+
+static int k1_rio_serdes_wait_lanes_ok(u32 lanes,
+				       struct keystone_serdes_data *serdes)
+{
+	return 0;
+}
+
+static int k1_rio_serdes_disable_lanes(u32 lanes,
+				       struct keystone_serdes_data *serdes)
+{
+	struct k1_rio_serdes_regs *serdes_regs =
+		(struct k1_rio_serdes_regs *)serdes->regs;
+
+	u32 lane;
+
+	for_each_lanes(lanes, lane) {
+		__raw_writel(0, &serdes_regs->channel[lane].rx);
+		__raw_writel(0, &serdes_regs->channel[lane].tx);
+	}
+
+	return 0;
+}
+
+static int k1_rio_serdes_shutdown_lanes(u32 lanes,
+					struct keystone_serdes_data *serdes)
+{
+	return 0;
+}
+
+static void k1_rio_serdes_recover_lanes(u32 lanes,
+					struct keystone_serdes_data *serdes)
+{
+}
+
+static int k1_rio_serdes_calibrate_lanes(u32 lanes,
+					 struct keystone_serdes_data *serdes)
+{
+	return -ENOTSUPP;
+}
+
+static const struct keystone_serdes_ops k1_serdes_ops = {
+	.config_lanes       = k1_rio_serdes_config_lanes,
+	.start_tx_lanes     = k1_rio_serdes_start_tx_lanes,
+	.wait_lanes_ok      = k1_rio_serdes_wait_lanes_ok,
+	.disable_lanes      = k1_rio_serdes_disable_lanes,
+	.shutdown_lanes     = k1_rio_serdes_shutdown_lanes,
+	.recover_lanes      = k1_rio_serdes_recover_lanes,
+	.calibrate_lanes    = k1_rio_serdes_calibrate_lanes,
+};
+
+#define KEYSTONE_SERDES_PRBS_7                0
+#define KEYSTONE_SERDES_PRBS_15               1
+#define KEYSTONE_SERDES_PRBS_23               2
+#define KEYSTONE_SERDES_PRBS_31               3
+
+#define KEYSTONE_SERDES_MAX_COEFS             5
+#define KEYSTONE_SERDES_MAX_COMPS             5
+
+#define KEYSTONE_SERDES_OFFSETS_RETRIES       100
+#define KEYSTONE_SERDES_ATT_BOOST_NUM_REPEAT  20
+#define KEYSTONE_SERDES_ATT_BOOST_REPEAT_MEAN 14
+
+struct k2_rio_serdes_coef_offsets {
+	u32 coef1_ofs[KEYSTONE_SERDES_MAX_LANES][KEYSTONE_SERDES_MAX_COEFS];
+	u32 coef2_ofs[KEYSTONE_SERDES_MAX_LANES][KEYSTONE_SERDES_MAX_COEFS];
+	u32 coef3_ofs[KEYSTONE_SERDES_MAX_LANES][KEYSTONE_SERDES_MAX_COEFS];
+	u32 coef4_ofs[KEYSTONE_SERDES_MAX_LANES][KEYSTONE_SERDES_MAX_COEFS];
+	u32 coef5_ofs[KEYSTONE_SERDES_MAX_LANES][KEYSTONE_SERDES_MAX_COEFS];
+	u32 cmp_ofs[KEYSTONE_SERDES_MAX_LANES][KEYSTONE_SERDES_MAX_COMPS];
+};
+
+struct k2_rio_serdes_reg_field {
+	u32 reg;
+	u32 shift;
+};
+
+static int k2_rio_serdes_start_tx_lanes(u32 lanes,
+					struct keystone_serdes_data *serdes);
+
+static void k2_rio_serdes_write_tbus_addr(void __iomem *regs,
+					  int select, int offset)
+{
+	int two_laner;
+
+	two_laner = (__raw_readl(regs + 0x1fc0) >> 16) & 0x0ffff;
+
+	if ((two_laner == 0x4eb9) || (two_laner == 0x4ebd))
+		two_laner = 0;
+	else
+		two_laner = 1;
+
+	if (select && two_laner)
+		select++;
+
+	reg_rmw(regs + 0x0008, ((select << 5) + offset) << 24, 0xff000000);
+}
+
+static u32 k2_rio_serdes_read_tbus_val(void __iomem *regs)
+{
+	u32 tmp;
+
+	tmp  = (__raw_readl(regs + 0x0ec) >> 24) & 0x000ff;
+	tmp |= (__raw_readl(regs + 0x0fc) >> 16) & 0x00f00;
+
+	return tmp;
+}
+
+static u32 k2_rio_serdes_read_selected_tbus(void __iomem *regs,
+					    int select, int offset)
+{
+	k2_rio_serdes_write_tbus_addr(regs, select, offset);
+
+	return k2_rio_serdes_read_tbus_val(regs);
+}
+
+static int k2_rio_serdes_wait_rx_valid(u32 lane, void __iomem *regs)
+{
+	unsigned long timeout = jiffies
+		+ msecs_to_jiffies(KEYSTONE_SERDES_TIMEOUT);
+	unsigned int stat;
+
+	stat = (k2_rio_serdes_read_selected_tbus(regs, lane + 1, 0x2)
+		& 0x0060) >> 5;
+
+	while (stat != 3) {
+		stat = (k2_rio_serdes_read_selected_tbus(regs, lane + 1, 0x2)
+			& 0x0060) >> 5;
+
+		if (time_after(jiffies, timeout))
+			return 1;
+
+		usleep_range(10, 50);
+	}
+
+	return 0;
+}
+
+static inline void k2_rio_serdes_reacquire_sd(u32 lane, void __iomem *regs)
+{
+	reg_finsr(regs + (0x200 * lane) + 0x200 + 0x04, 2, 1, 0x0);
+}
+
+static inline u32 k2_rio_serdes_get_termination(void __iomem *regs)
+{
+	return k2_rio_serdes_read_selected_tbus(regs, 1, 0x1b) & 0x00ff;
+}
+
+static void k2_rio_serdes_termination_config(u32 lane,
+					     void __iomem *regs,
+					     u32 tx_term_np)
+{
+	reg_fill_field(regs + (lane * 0x200) + 0x200 + 0x7c, 24, 8, tx_term_np);
+	reg_fill_field(regs + (lane * 0x200) + 0x200 + 0x7c, 20, 1, 1);
+}
+
+static inline void k2_rio_serdes_get_att_boost(
+	u32 lane,
+	void __iomem *regs,
+	struct keystone_serdes_lane_rx_config *rx_coeff)
+{
+	u32 att;
+	u32 boost;
+
+	boost = k2_rio_serdes_read_selected_tbus(regs, lane + 1, 0x11);
+	att = boost;
+	rx_coeff->att   = (att   >> 4) & 0x0f;
+	rx_coeff->boost = (boost >> 8) & 0x0f;
+}
+
+static inline void k2_rio_serdes_set_tx_idle(u32 lane, void __iomem *regs)
+{
+	reg_fill_field(regs + (0x200 * lane) + 0x200 + 0xb8, 16, 2, 3);
+	reg_fill_field(regs + (lane * 4) + 0x1fc0 + 0x20, 24, 2, 3);
+	reg_fill_field(regs + (0x200 * lane) + 0x200 + 0x28, 20, 2, 0);
+}
+
+static inline void k2_rio_serdes_set_tx_normal(u32 lane, void __iomem *regs)
+{
+	reg_fill_field(regs + (0x200 * lane) + 0x200 + 0xb8, 16, 2, 0);
+	reg_fill_field(regs + (lane * 4) + 0x1fc0 + 0x20, 24, 2, 0);
+	reg_fill_field(regs + (0x200 * lane) + 0x200 + 0x28, 20, 2, 3);
+}
+
+static inline void k2_rio_serdes_disable_rx(u32 lane, void __iomem *regs)
+{
+	reg_fill_field(regs + (lane * 4) + 0x1fc0 + 0x20, 15, 1, 1);
+	reg_fill_field(regs + (lane * 4) + 0x1fc0 + 0x20, 13, 2, 0);
+}
+
+static inline void k2_rio_serdes_enable_rx(u32 lane, void __iomem *regs)
+{
+	reg_fill_field(regs + (lane * 4) + 0x1fc0 + 0x20, 15, 1, 1);
+	reg_fill_field(regs + (lane * 4) + 0x1fc0 + 0x20, 13, 2, 3);
+}
+
+static inline void k2_rio_serdes_reset_rx(u32 lane, void __iomem *regs)
+{
+	reg_finsr(regs + 0x004 + (0x200 * (lane + 1)), 2, 1, 0x2);
+}
+
+static int k2_rio_serdes_wait_lanes_ok(u32 lanes, void __iomem *regs)
+{
+	unsigned long timeout;
+	u32 val;
+	u32 val_mask;
+
+	val_mask = lanes << 8;
+
+	timeout = jiffies + msecs_to_jiffies(KEYSTONE_SERDES_TIMEOUT);
+	while (1) {
+		val = __raw_readl(regs + 0x1fc0 + 0x34);
+
+		if ((val & val_mask) == val_mask)
+			break;
+
+		if (time_after(jiffies, timeout))
+			return -EAGAIN;
+
+		usleep_range(10, 50);
+	}
+
+	return 0;
+}
+
+static int k2_rio_serdes_wait_lanes_sd(u32 lanes,
+				       struct keystone_serdes_data *serdes)
+{
+	void __iomem *regs = serdes->regs;
+	unsigned long timeout;
+	u32 val;
+	u32 val_mask;
+	u32 lane;
+
+	val_mask = lanes | (lanes << 8);
+
+	timeout = jiffies + msecs_to_jiffies(KEYSTONE_SERDES_TIMEOUT);
+	while (1) {
+		val = __raw_readl(regs + 0x1fc0 + 0x34);
+
+		if ((val & val_mask) == val_mask)
+			break;
+
+		if (time_after(jiffies, timeout))
+			return -EAGAIN;
+
+		usleep_range(10, 50);
+	}
+
+	for_each_lanes(lanes, lane) {
+		k2_rio_serdes_get_att_boost(lane, regs,
+					    &serdes->config->rx[lane]);
+
+		dev_dbg(serdes->dev,
+			"SerDes: Rx signal detected, att = %d, boost = %d for lane %d\n",
+			serdes->config->rx[lane].att,
+			serdes->config->rx[lane].boost,
+			lane);
+	}
+
+	return 0;
+}
+
+static void k2_rio_serdes_assert_reset(u32 lane, void __iomem *regs)
+{
+	unsigned int ui_tmpo;
+	unsigned int ui_tmp0;
+	unsigned int ui_tmp1;
+
+	ui_tmp0 = k2_rio_serdes_read_selected_tbus(regs, lane + 1, 0);
+
+	ui_tmp1 = k2_rio_serdes_read_selected_tbus(regs, lane + 1, 1);
+
+	ui_tmpo = 0;
+	ui_tmpo |= ((ui_tmp1 >> 9) & 0x003) << 1;
+	ui_tmpo |= ((ui_tmp0) & 0x003) << 3;
+	ui_tmpo |= ((ui_tmp0 >> 2) & 0x1FF) << 5;
+	ui_tmpo |= BIT(14);
+	ui_tmpo &= ~0x60;
+
+	reg_fill_field(regs + (lane * 0x200) + 0x200 + 0x28, 15, 15, ui_tmpo);
+}
+
+static inline void k2_rio_serdes_assert_full_reset(u32 lane,
+						   void __iomem *regs)
+{
+	reg_finsr(regs + (0 * 0x200) + (1 * 0x200) + 0x200 + 0x28, 29, 15,
+		  0x4260);
+}
+
+static inline int k2_rio_serdes_deassert_reset(u32 lane, void __iomem *regs,
+					       u32 block)
+{
+	reg_fill_field(regs + (lane * 0x200) + 0x200 + 0x28, 15, 1, 1);
+
+	if (block)
+		return k2_rio_serdes_wait_lanes_ok(BIT(lane), regs);
+
+	return 0;
+}
+
+static void k2_rio_serdes_clear_overlay_bit29(u32 lane, void __iomem *regs)
+{
+	reg_fill_field(regs + (lane * 0x200) + 0x200 + 0x28, 29, 1, 0);
+}
+
+static void k2_rio_serdes_set_overlay_bit29(u32 lane, void __iomem *regs)
+{
+	reg_fill_field(regs + (lane * 0x200) + 0x200 + 0x28, 29, 1, 1);
+}
+
+static void k2_rio_serdes_init_3g(void __iomem *reg)
+{
+	reg_finsr((reg + 0x0000), 31, 24, 0x00);
+	reg_finsr((reg + 0x0014),  7,  0, 0x82);
+	reg_finsr((reg + 0x0014), 15,  8, 0x82);
+	reg_finsr((reg + 0x0060),  7,  0, 0x48);
+	reg_finsr((reg + 0x0060), 15,  8, 0x2c);
+	reg_finsr((reg + 0x0060), 23, 16, 0x13);
+	reg_finsr((reg + 0x0064), 15,  8, 0xc7);
+	reg_finsr((reg + 0x0064), 23, 16, 0xc3);
+	reg_finsr((reg + 0x0078), 15,  8, 0xc0);
+
+	reg_finsr((reg + 0x0204),  7,  0, 0x80);
+	reg_finsr((reg + 0x0204), 31, 24, 0x78);
+	reg_finsr((reg + 0x0208),  7,  0, 0x24);
+	reg_finsr((reg + 0x020c), 31, 24, 0x02);
+	reg_finsr((reg + 0x0210), 31, 24, 0x1b);
+	reg_finsr((reg + 0x0214),  7,  0, 0x7c);
+	reg_finsr((reg + 0x0214), 15,  8, 0x6e);
+	reg_finsr((reg + 0x0218),  7,  0, 0xe4);
+	reg_finsr((reg + 0x0218), 23, 16, 0x80);
+	reg_finsr((reg + 0x0218), 31, 24, 0x75);
+	reg_finsr((reg + 0x022c), 15,  8, 0x08);
+	reg_finsr((reg + 0x022c), 23, 16, 0x20);
+	reg_finsr((reg + 0x0280),  7,  0, 0x70);
+	reg_finsr((reg + 0x0280), 23, 16, 0x70);
+	reg_finsr((reg + 0x0284),  7,  0, 0x85);
+	reg_finsr((reg + 0x0284), 23, 16, 0x0f);
+	reg_finsr((reg + 0x0284), 31, 24, 0x1d);
+	reg_finsr((reg + 0x028c), 15,  8, 0x3b);
+
+	reg_finsr((reg + 0x0404),  7,  0, 0x80);
+	reg_finsr((reg + 0x0404), 31, 24, 0x78);
+	reg_finsr((reg + 0x0408),  7,  0, 0x24);
+	reg_finsr((reg + 0x040c), 31, 24, 0x02);
+	reg_finsr((reg + 0x0410), 31, 24, 0x1b);
+	reg_finsr((reg + 0x0414),  7,  0, 0x7c);
+	reg_finsr((reg + 0x0414), 15,  8, 0x6e);
+	reg_finsr((reg + 0x0418),  7,  0, 0xe4);
+	reg_finsr((reg + 0x0418), 23, 16, 0x80);
+	reg_finsr((reg + 0x0418), 31, 24, 0x75);
+	reg_finsr((reg + 0x042c), 15,  8, 0x08);
+	reg_finsr((reg + 0x042c), 23, 16, 0x20);
+	reg_finsr((reg + 0x0480),  7,  0, 0x70);
+	reg_finsr((reg + 0x0480), 23, 16, 0x70);
+	reg_finsr((reg + 0x0484),  7,  0, 0x85);
+	reg_finsr((reg + 0x0484), 23, 16, 0x0f);
+	reg_finsr((reg + 0x0484), 31, 24, 0x1d);
+	reg_finsr((reg + 0x048c), 15,  8, 0x3b);
+
+	reg_finsr((reg + 0x0604),  7,  0, 0x80);
+	reg_finsr((reg + 0x0604), 31, 24, 0x78);
+	reg_finsr((reg + 0x0608),  7,  0, 0x24);
+	reg_finsr((reg + 0x060c), 31, 24, 0x02);
+	reg_finsr((reg + 0x0610), 31, 24, 0x1b);
+	reg_finsr((reg + 0x0614),  7,  0, 0x7c);
+	reg_finsr((reg + 0x0614), 15,  8, 0x6e);
+	reg_finsr((reg + 0x0618),  7,  0, 0xe4);
+	reg_finsr((reg + 0x0618), 23, 16, 0x80);
+	reg_finsr((reg + 0x0618), 31, 24, 0x75);
+	reg_finsr((reg + 0x062c), 15,  8, 0x08);
+	reg_finsr((reg + 0x062c), 23, 16, 0x20);
+	reg_finsr((reg + 0x0680),  7,  0, 0x70);
+	reg_finsr((reg + 0x0680), 23, 16, 0x70);
+	reg_finsr((reg + 0x0684),  7,  0, 0x85);
+	reg_finsr((reg + 0x0684), 23, 16, 0x0f);
+	reg_finsr((reg + 0x0684), 31, 24, 0x1d);
+	reg_finsr((reg + 0x068c), 15,  8, 0x3b);
+
+	reg_finsr((reg + 0x0804),  7,  0, 0x80);
+	reg_finsr((reg + 0x0804), 31, 24, 0x78);
+	reg_finsr((reg + 0x0808),  7,  0, 0x24);
+	reg_finsr((reg + 0x080c), 31, 24, 0x02);
+	reg_finsr((reg + 0x0810), 31, 24, 0x1b);
+	reg_finsr((reg + 0x0814),  7,  0, 0x7c);
+	reg_finsr((reg + 0x0814), 15,  8, 0x6e);
+	reg_finsr((reg + 0x0818),  7,  0, 0xe4);
+	reg_finsr((reg + 0x0818), 23, 16, 0x80);
+	reg_finsr((reg + 0x0818), 31, 24, 0x75);
+	reg_finsr((reg + 0x082c), 15,  8, 0x08);
+	reg_finsr((reg + 0x082c), 23, 16, 0x20);
+	reg_finsr((reg + 0x0880),  7,  0, 0x70);
+	reg_finsr((reg + 0x0880), 23, 16, 0x70);
+	reg_finsr((reg + 0x0884),  7,  0, 0x85);
+	reg_finsr((reg + 0x0884), 23, 16, 0x0f);
+	reg_finsr((reg + 0x0884), 31, 24, 0x1d);
+	reg_finsr((reg + 0x088c), 15,  8, 0x3b);
+
+	reg_finsr((reg + 0x0a00), 15,  8, 0x08);
+	reg_finsr((reg + 0x0a08), 23, 16, 0x72);
+	reg_finsr((reg + 0x0a08), 31, 24, 0x37);
+	reg_finsr((reg + 0x0a30), 15,  8, 0x77);
+	reg_finsr((reg + 0x0a30), 23, 16, 0x77);
+	reg_finsr((reg + 0x0a84), 15,  8, 0x06);
+	reg_finsr((reg + 0x0a94), 31, 24, 0x10);
+	reg_finsr((reg + 0x0aa0), 31, 24, 0x81);
+	reg_finsr((reg + 0x0abc), 31, 24, 0xff);
+	reg_finsr((reg + 0x0ac0),  7,  0, 0x8b);
+	reg_finsr((reg + 0x0a48), 15,  8, 0x8c);
+	reg_finsr((reg + 0x0a48), 23, 16, 0xfd);
+	reg_finsr((reg + 0x0a54),  7,  0, 0x72);
+	reg_finsr((reg + 0x0a54), 15,  8, 0xec);
+	reg_finsr((reg + 0x0a54), 23, 16, 0x2f);
+	reg_finsr((reg + 0x0a58), 15,  8, 0x21);
+	reg_finsr((reg + 0x0a58), 23, 16, 0xf9);
+	reg_finsr((reg + 0x0a58), 31, 24, 0x00);
+	reg_finsr((reg + 0x0a5c),  7,  0, 0x60);
+	reg_finsr((reg + 0x0a5c), 15,  8, 0x00);
+	reg_finsr((reg + 0x0a5c), 23, 16, 0x04);
+	reg_finsr((reg + 0x0a5c), 31, 24, 0x00);
+	reg_finsr((reg + 0x0a60),  7,  0, 0x00);
+	reg_finsr((reg + 0x0a60), 15,  8, 0x80);
+	reg_finsr((reg + 0x0a60), 23, 16, 0x00);
+	reg_finsr((reg + 0x0a60), 31, 24, 0x00);
+	reg_finsr((reg + 0x0a64),  7,  0, 0x20);
+	reg_finsr((reg + 0x0a64), 15,  8, 0x12);
+	reg_finsr((reg + 0x0a64), 23, 16, 0x58);
+	reg_finsr((reg + 0x0a64), 31, 24, 0x0c);
+	reg_finsr((reg + 0x0a68),  7,  0, 0x02);
+	reg_finsr((reg + 0x0a68), 15,  8, 0x06);
+	reg_finsr((reg + 0x0a68), 23, 16, 0x3b);
+	reg_finsr((reg + 0x0a68), 31, 24, 0xe1);
+	reg_finsr((reg + 0x0a6c),  7,  0, 0xc1);
+	reg_finsr((reg + 0x0a6c), 15,  8, 0x4c);
+	reg_finsr((reg + 0x0a6c), 23, 16, 0x07);
+	reg_finsr((reg + 0x0a6c), 31, 24, 0xb8);
+	reg_finsr((reg + 0x0a70),  7,  0, 0x89);
+	reg_finsr((reg + 0x0a70), 15,  8, 0xe9);
+	reg_finsr((reg + 0x0a70), 23, 16, 0x02);
+	reg_finsr((reg + 0x0a70), 31, 24, 0x3f);
+	reg_finsr((reg + 0x0a74),  7,  0, 0x01);
+	reg_finsr((reg + 0x0b20), 23, 16, 0x37);
+	reg_finsr((reg + 0x0b1c), 31, 24, 0x37);
+	reg_finsr((reg + 0x0b20),  7,  0, 0x5d);
+	reg_finsr((reg + 0x0000),  7,  0, 0x03);
+	reg_finsr((reg + 0x0a00),  7,  0, 0x5f);
+}
+
+static void k2_rio_serdes_init_5g(void __iomem *reg)
+{
+	reg_finsr((reg + 0x0000), 31, 24, 0x00);
+	reg_finsr((reg + 0x0014),  7,  0, 0x82);
+	reg_finsr((reg + 0x0014), 15,  8, 0x82);
+	reg_finsr((reg + 0x0060),  7,  0, 0x38);
+	reg_finsr((reg + 0x0060), 15,  8, 0x24);
+	reg_finsr((reg + 0x0060), 23, 16, 0x14);
+	reg_finsr((reg + 0x0064), 15,  8, 0xc7);
+	reg_finsr((reg + 0x0064), 23, 16, 0xc3);
+	reg_finsr((reg + 0x0078), 15,  8, 0xc0);
+
+	reg_finsr((reg + 0x0204),  7,  0, 0x80);
+	reg_finsr((reg + 0x0204), 31, 24, 0x78);
+	reg_finsr((reg + 0x0208),  7,  0, 0x26);
+	reg_finsr((reg + 0x020c), 31, 24, 0x02);
+	reg_finsr((reg + 0x0214),  7,  0, 0x38);
+	reg_finsr((reg + 0x0214), 15,  8, 0x6f);
+	reg_finsr((reg + 0x0218),  7,  0, 0xe4);
+	reg_finsr((reg + 0x0218), 23, 16, 0x80);
+	reg_finsr((reg + 0x0218), 31, 24, 0x75);
+	reg_finsr((reg + 0x022c), 15,  8, 0x08);
+	reg_finsr((reg + 0x022c), 23, 16, 0x20);
+	reg_finsr((reg + 0x0280),  7,  0, 0x86);
+	reg_finsr((reg + 0x0280), 23, 16, 0x86);
+	reg_finsr((reg + 0x0284),  7,  0, 0x85);
+	reg_finsr((reg + 0x0284), 23, 16, 0x0f);
+	reg_finsr((reg + 0x0284), 31, 24, 0x1d);
+	reg_finsr((reg + 0x028c), 15,  8, 0x2c);
+
+	reg_finsr((reg + 0x0404),  7,  0, 0x80);
+	reg_finsr((reg + 0x0404), 31, 24, 0x78);
+	reg_finsr((reg + 0x0408),  7,  0, 0x26);
+	reg_finsr((reg + 0x040c), 31, 24, 0x02);
+	reg_finsr((reg + 0x0414),  7,  0, 0x38);
+	reg_finsr((reg + 0x0414), 15,  8, 0x6f);
+	reg_finsr((reg + 0x0418),  7,  0, 0xe4);
+	reg_finsr((reg + 0x0418), 23, 16, 0x80);
+	reg_finsr((reg + 0x0418), 31, 24, 0x75);
+	reg_finsr((reg + 0x042c), 15,  8, 0x08);
+	reg_finsr((reg + 0x042c), 23, 16, 0x20);
+	reg_finsr((reg + 0x0480),  7,  0, 0x86);
+	reg_finsr((reg + 0x0480), 23, 16, 0x86);
+	reg_finsr((reg + 0x0484),  7,  0, 0x85);
+	reg_finsr((reg + 0x0484), 23, 16, 0x0f);
+	reg_finsr((reg + 0x0484), 31, 24, 0x1d);
+	reg_finsr((reg + 0x048c), 15,  8, 0x2c);
+
+	reg_finsr((reg + 0x0604),  7,  0, 0x80);
+	reg_finsr((reg + 0x0604), 31, 24, 0x78);
+	reg_finsr((reg + 0x0608),  7,  0, 0x26);
+	reg_finsr((reg + 0x060c), 31, 24, 0x02);
+	reg_finsr((reg + 0x0614),  7,  0, 0x38);
+	reg_finsr((reg + 0x0614), 15,  8, 0x6f);
+	reg_finsr((reg + 0x0618),  7,  0, 0xe4);
+	reg_finsr((reg + 0x0618), 23, 16, 0x80);
+	reg_finsr((reg + 0x0618), 31, 24, 0x75);
+	reg_finsr((reg + 0x062c), 15,  8, 0x08);
+	reg_finsr((reg + 0x062c), 23, 16, 0x20);
+	reg_finsr((reg + 0x0680),  7,  0, 0x86);
+	reg_finsr((reg + 0x0680), 23, 16, 0x86);
+	reg_finsr((reg + 0x0684),  7,  0, 0x85);
+	reg_finsr((reg + 0x0684), 23, 16, 0x0f);
+	reg_finsr((reg + 0x0684), 31, 24, 0x1d);
+	reg_finsr((reg + 0x068c), 15,  8, 0x2c);
+
+	reg_finsr((reg + 0x0804),  7,  0, 0x80);
+	reg_finsr((reg + 0x0804), 31, 24, 0x78);
+	reg_finsr((reg + 0x0808),  7,  0, 0x26);
+	reg_finsr((reg + 0x080c), 31, 24, 0x02);
+	reg_finsr((reg + 0x0814),  7,  0, 0x38);
+	reg_finsr((reg + 0x0814), 15,  8, 0x6f);
+	reg_finsr((reg + 0x0818),  7,  0, 0xe4);
+	reg_finsr((reg + 0x0818), 23, 16, 0x80);
+	reg_finsr((reg + 0x0818), 31, 24, 0x75);
+	reg_finsr((reg + 0x082c), 15,  8, 0x08);
+	reg_finsr((reg + 0x082c), 23, 16, 0x20);
+	reg_finsr((reg + 0x0880),  7,  0, 0x86);
+	reg_finsr((reg + 0x0880), 23, 16, 0x86);
+	reg_finsr((reg + 0x0884),  7,  0, 0x85);
+	reg_finsr((reg + 0x0884), 23, 16, 0x0f);
+	reg_finsr((reg + 0x0884), 31, 24, 0x1d);
+	reg_finsr((reg + 0x088c), 15,  8, 0x2c);
+
+	reg_finsr((reg + 0x0a00), 15,  8, 0x80);
+	reg_finsr((reg + 0x0a08), 23, 16, 0xd2);
+	reg_finsr((reg + 0x0a08), 31, 24, 0x38);
+	reg_finsr((reg + 0x0a30), 15,  8, 0x8d);
+	reg_finsr((reg + 0x0a30), 23, 16, 0x8d);
+	reg_finsr((reg + 0x0a84), 15,  8, 0x06);
+	reg_finsr((reg + 0x0a94), 31, 24, 0x10);
+	reg_finsr((reg + 0x0aa0), 31, 24, 0x81);
+	reg_finsr((reg + 0x0abc), 31, 24, 0xff);
+	reg_finsr((reg + 0x0ac0),  7,  0, 0x8b);
+	reg_finsr((reg + 0x0a48), 15,  8, 0x8c);
+	reg_finsr((reg + 0x0a48), 23, 16, 0xfd);
+	reg_finsr((reg + 0x0a54),  7,  0, 0x72);
+	reg_finsr((reg + 0x0a54), 15,  8, 0xec);
+	reg_finsr((reg + 0x0a54), 23, 16, 0x2f);
+	reg_finsr((reg + 0x0a58), 15,  8, 0x21);
+	reg_finsr((reg + 0x0a58), 23, 16, 0xf9);
+	reg_finsr((reg + 0x0a58), 31, 24, 0x00);
+	reg_finsr((reg + 0x0a5c),  7,  0, 0x60);
+	reg_finsr((reg + 0x0a5c), 15,  8, 0x00);
+	reg_finsr((reg + 0x0a5c), 23, 16, 0x04);
+	reg_finsr((reg + 0x0a5c), 31, 24, 0x00);
+	reg_finsr((reg + 0x0a60),  7,  0, 0x00);
+	reg_finsr((reg + 0x0a60), 15,  8, 0x80);
+	reg_finsr((reg + 0x0a60), 23, 16, 0x00);
+	reg_finsr((reg + 0x0a60), 31, 24, 0x00);
+	reg_finsr((reg + 0x0a64),  7,  0, 0x20);
+	reg_finsr((reg + 0x0a64), 15,  8, 0x12);
+	reg_finsr((reg + 0x0a64), 23, 16, 0x58);
+	reg_finsr((reg + 0x0a64), 31, 24, 0x0c);
+	reg_finsr((reg + 0x0a68),  7,  0, 0x02);
+	reg_finsr((reg + 0x0a68), 15,  8, 0x06);
+	reg_finsr((reg + 0x0a68), 23, 16, 0x3b);
+	reg_finsr((reg + 0x0a68), 31, 24, 0xe1);
+	reg_finsr((reg + 0x0a6c),  7,  0, 0xc1);
+	reg_finsr((reg + 0x0a6c), 15,  8, 0x4c);
+	reg_finsr((reg + 0x0a6c), 23, 16, 0x07);
+	reg_finsr((reg + 0x0a6c), 31, 24, 0xb8);
+	reg_finsr((reg + 0x0a70),  7,  0, 0x89);
+	reg_finsr((reg + 0x0a70), 15,  8, 0xe9);
+	reg_finsr((reg + 0x0a70), 23, 16, 0x02);
+	reg_finsr((reg + 0x0a70), 31, 24, 0x3f);
+	reg_finsr((reg + 0x0a74),  7,  0, 0x01);
+	reg_finsr((reg + 0x0b20), 23, 16, 0x37);
+	reg_finsr((reg + 0x0b1c), 31, 24, 0x37);
+	reg_finsr((reg + 0x0b20),  7,  0, 0x5d);
+	reg_finsr((reg + 0x0000),  7,  0, 0x03);
+	reg_finsr((reg + 0x0a00),  7,  0, 0x5f);
+}
+
+static void k2_rio_serdes_lane_init(u32 lane, void __iomem *regs, u32 rate)
+{
+	k2_rio_serdes_clear_overlay_bit29(lane, regs);
+
+	switch (rate) {
+	case KEYSTONE_SERDES_FULL_RATE:
+		__raw_writel(0xf3c0f0f0, regs + 0x1fe0 + (4 * lane));
+		break;
+	case KEYSTONE_SERDES_HALF_RATE:
+		__raw_writel(0xf7c0f4f0, regs + 0x1fe0 + (4 * lane));
+		break;
+	case KEYSTONE_SERDES_QUARTER_RATE:
+		__raw_writel(0xfbc0f8f0, regs + 0x1fe0 + (4 * lane));
+		break;
+	default:
+		return;
+	}
+}
+
+static inline void k2_rio_serdes_lane_enable(u32 lane, void __iomem *regs)
+{
+	reg_rmw(regs + 0x1fe0 + (4 * lane),
+		BIT(29) | BIT(30) | BIT(13) | BIT(14),
+		BIT(29) | BIT(30) | BIT(13) | BIT(14));
+}
+
+static inline void k2_rio_serdes_lane_disable(u32 lane, void __iomem *regs)
+{
+	reg_rmw(regs + 0x1fe0 + (4 * lane),
+		0,
+		BIT(29) | BIT(30) | BIT(13) | BIT(14));
+}
+
+static int k2_rio_serdes_calibrate_att(u32 lanes, void __iomem *regs, u32 rate)
+{
+	int res = 0;
+	u32 lane;
+	u32 att_read[KEYSTONE_SERDES_MAX_LANES];
+	u32 att_start[KEYSTONE_SERDES_MAX_LANES];
+	u32 reg;
+	u32 shift;
+
+	static struct k2_rio_serdes_reg_field __k2_rio_serdes_att_start[3] = {
+		{ 0x84, 16  },
+		{ 0x84, 24 },
+		{ 0x8c, 8  },
+	};
+
+	reg   = __k2_rio_serdes_att_start[rate].reg;
+	shift = __k2_rio_serdes_att_start[rate].shift;
+
+	for_each_lanes(lanes, lane)
+		att_start[lane] =
+		(__raw_readl(regs + (lane * 0x200) + 0x200 + reg)
+		 >> shift) & 0xf;
+
+	for_each_lanes(lanes, lane)
+		att_read[lane] =
+		(k2_rio_serdes_read_selected_tbus(regs, lane + 1, 0x11)
+		 >> 4) & 0xf;
+
+	for_each_lanes(lanes, lane)
+		reg_fill_field(regs + (lane * 0x200) + 0x200 + reg,
+			       shift, 4, att_read[lane]);
+
+	reg_fill_field(regs + 0x0a00 + 0x84, 0,  1, 0);
+	reg_fill_field(regs + 0x0a00 + 0x8c, 24, 1, 0);
+
+	reg_fill_field(regs + 0x0a00 + 0x98, 7, 1, 1);
+	reg_fill_field(regs + 0x0a00 + 0x98, 7, 1, 0);
+
+	for_each_lanes(lanes, lane) {
+		if (k2_rio_serdes_wait_rx_valid(lane, regs))
+			res = 1;
+
+		reg_fill_field(regs + (lane * 0x200) + 0x200 + reg, shift, 4,
+			       att_start[lane]);
+	}
+
+	reg_fill_field(regs + 0x0a00 + 0x84,  0, 1, 1);
+	reg_fill_field(regs + 0x0a00 + 0x8c, 24, 1, 1);
+
+	return res;
+}
+
+static int k2_rio_serdes_calibrate_boost(u32 lane, void __iomem *regs)
+{
+	u32 boost_read = 0;
+
+	if (k2_rio_serdes_wait_rx_valid(lane, regs))
+		return -1;
+
+	boost_read = (k2_rio_serdes_read_selected_tbus(regs, lane + 1, 0x11)
+		      >> 8) & 0xf;
+
+	if (boost_read != 0)
+		goto do_not_inc;
+
+	reg_fill_field(regs + (lane * 0x200) + 0x200 + 0x2c,  2, 1, 1);
+
+	reg_fill_field(regs + (lane * 0x200) + 0x200 + 0x2c, 12, 7, 0x2);
+
+	reg_fill_field(regs + (lane * 0x200) + 0x200 + 0x2c,  3, 7, 0x1);
+
+	reg_fill_field(regs + (lane * 0x200) + 0x200 + 0x2c, 10, 1, 0x1);
+	reg_fill_field(regs + (lane * 0x200) + 0x200 + 0x2c, 10, 1, 0x0);
+
+	reg_fill_field(regs + (lane * 0x200) + 0x200 + 0x2c,  2, 1, 0x0);
+	reg_fill_field(regs + (lane * 0x200) + 0x200 + 0x2c, 12, 7, 0x0);
+	reg_fill_field(regs + (lane * 0x200) + 0x200 + 0x2c,  3, 7, 0x0);
+
+do_not_inc:
+	if (k2_rio_serdes_wait_rx_valid(lane, regs))
+		return -1;
+
+	return 0;
+}
+
+static void
+k2_rio_serdes_calibrate_rx(u32 lane, u32 lanes, void __iomem *regs,
+			   struct keystone_serdes_lane_rx_config *rx_coeff,
+			   u32 rate)
+{
+	u32 repeat_index;
+	int att = 0, num_att = 0, boost = 0, num_boost = 0;
+	int att_array[KEYSTONE_SERDES_ATT_BOOST_NUM_REPEAT];
+	int boost_array[KEYSTONE_SERDES_ATT_BOOST_NUM_REPEAT];
+	int res;
+
+	for (repeat_index = 0;
+	     repeat_index < KEYSTONE_SERDES_ATT_BOOST_NUM_REPEAT;
+	     repeat_index++) {
+		if (k2_rio_serdes_wait_rx_valid(lane, regs)) {
+			att = -1;
+			boost = -1;
+			goto skip;
+		}
+
+		k2_rio_serdes_calibrate_att(lanes, regs, rate);
+
+		res = k2_rio_serdes_calibrate_boost(lane, regs);
+		if (res < 0) {
+			att = -1;
+			boost = -1;
+			goto skip;
+		}
+
+		boost = k2_rio_serdes_read_selected_tbus(regs, lane + 1, 0x11);
+		att = boost;
+		att   = (att   >> 4) & 0x0f;
+		boost = (boost >> 8) & 0x0f;
+
+skip:
+		att_array[repeat_index]   = att;
+		boost_array[repeat_index] = boost;
+		k2_rio_serdes_reset_rx(lane, regs);
+		usleep_range(10, 50);
+		k2_rio_serdes_reacquire_sd(lane, regs);
+	}
+
+	att = 0;
+	boost = 0;
+	num_att = 0;
+	num_boost = 0;
+
+	for (repeat_index = 0;
+	     repeat_index < KEYSTONE_SERDES_ATT_BOOST_NUM_REPEAT;
+	     repeat_index++) {
+		if ((att_array[repeat_index] > 0) &&
+		    (att_array[repeat_index] <
+		     KEYSTONE_SERDES_ATT_BOOST_REPEAT_MEAN)) {
+			att += att_array[repeat_index];
+			num_att++;
+		}
+
+		if ((boost_array[repeat_index] > 0) &&
+		    (boost_array[repeat_index] <
+		     KEYSTONE_SERDES_ATT_BOOST_REPEAT_MEAN)) {
+			boost += boost_array[repeat_index];
+			num_boost++;
+		}
+	}
+
+	rx_coeff[lane].mean_att = (num_att > 0) ?
+		(((att << 4) / num_att) + 8) >> 4 : -1;
+
+	rx_coeff[lane].mean_boost = (num_boost > 0) ?
+		(((boost << 4) / num_boost) + 8) >> 4 : -1;
+}
+
+static void k2_rio_serdes_set_att_boost(
+	u32 lane,
+	void __iomem *regs,
+	struct keystone_serdes_lane_rx_config *rx_coeff)
+{
+	k2_rio_serdes_disable_rx(lane, regs);
+
+	reg_finsr(regs + 0xa00 + 0x84, 10, 8, 0x0);
+
+	if (rx_coeff->mean_att != -1) {
+		reg_finsr(regs + 0xa00 + 0x84,  0,  0, 0x0);
+		reg_finsr(regs + 0xa00 + 0x8c, 24, 24, 0x0);
+		reg_finsr(regs + 0x200 * (lane + 1) + 0x8c, 11,  8,
+			  rx_coeff->mean_att);
+		reg_finsr(regs + 0x200 * (lane + 1) + 0x84, 27, 24,
+			  rx_coeff->mean_att);
+		reg_finsr(regs + 0x200 * (lane + 1) + 0x84, 19, 16,
+			  rx_coeff->mean_att);
+	}
+
+	if (rx_coeff->mean_boost != -1) {
+		reg_finsr(regs + 0xa00 + 0x84,  1,  1, 0x0);
+		reg_finsr(regs + 0xa00 + 0x8c, 25, 25, 0x0);
+		reg_finsr(regs + 0x200 * (lane + 1) + 0x8c, 15, 12,
+			  rx_coeff->mean_boost);
+		reg_finsr(regs + 0x200 * (lane + 1) + 0x84, 31, 28,
+			  rx_coeff->mean_boost);
+		reg_finsr(regs + 0x200 * (lane + 1) + 0x84, 23, 20,
+			  rx_coeff->mean_boost);
+	}
+
+	k2_rio_serdes_enable_rx(lane, regs);
+}
+
+static inline void k2_rio_serdes_config_set_tx_coeffs(
+	u32 lane,
+	void __iomem *regs,
+	struct keystone_serdes_lane_tx_config *tx_config)
+{
+	u32 val;
+
+	tx_config->c1_coeff &= 0x1f;
+	tx_config->c2_coeff &= 0xf;
+	tx_config->cm_coeff &= 0xf;
+	tx_config->pre_1lsb &= 0x1;
+	tx_config->att      &= 0xf;
+	tx_config->vreg     &= 0x7;
+
+	val = (tx_config->c1_coeff)
+		| ((tx_config->c2_coeff) << 8)
+		| ((tx_config->cm_coeff) << 12);
+	reg_rmw(regs + 0x008 + (0x200 * (lane + 1)), val, 0x0000ff1f);
+
+	val = (tx_config->att << 25) | (tx_config->pre_1lsb << 21);
+	reg_rmw(regs + 0x004 + (0x200 * (lane + 1)), val, 0x9e000000);
+
+	reg_rmw(regs + 0x084 + (0x200 * (lane + 1)), (tx_config->vreg << 5),
+		0x000000e0);
+}
+
+static void k2_rio_serdes_config_set_rx_coeffs(
+	u32 lanes,
+	void __iomem *regs,
+	struct device *dev,
+	struct keystone_serdes_lane_rx_config *rx_config)
+{
+	u32 lane;
+
+	for_each_lanes(lanes, lane) {
+		if ((rx_config[lane].mean_att == -1) &&
+		    (rx_config[lane].mean_boost == -1))
+			continue;
+
+		k2_rio_serdes_reset_rx(lane, regs);
+		usleep_range(10, 50);
+
+		dev_dbg(dev,
+			"SerDes: applying computed Rx att = %d, Rx boost = %d for lane %d\n",
+			rx_config[lane].mean_att,
+			rx_config[lane].mean_boost,
+			lane);
+
+		k2_rio_serdes_set_att_boost(lane, regs, &rx_config[lane]);
+		k2_rio_serdes_reacquire_sd(lane, regs);
+		usleep_range(10, 50);
+		k2_rio_serdes_wait_rx_valid(lane, regs);
+	}
+}
+
+static void k2_rio_serdes_tx_rx_set_equalizer(u32 lane,
+					      void __iomem *regs,
+					      int vreg_enable,
+					      u32 att_start,
+					      u32 boost_start)
+{
+	if (vreg_enable) {
+		reg_finsr(regs + 0x200 * (lane + 1) + 0x18, 25, 24, 0x2);
+		reg_finsr(regs + 0x200 * (lane + 1) + 0x18, 27, 26, 0x2);
+	}
+
+	reg_finsr(regs + 0x200 * (lane + 1) + 0x8c, 11,  8, att_start);
+	reg_finsr(regs + 0x200 * (lane + 1) + 0x84, 27, 24, att_start);
+	reg_finsr(regs + 0x200 * (lane + 1) + 0x84, 19, 16, att_start);
+
+	reg_finsr(regs + 0x200 * (lane + 1) + 0x8c, 15, 12, boost_start);
+	reg_finsr(regs + 0x200 * (lane + 1) + 0x84, 31, 28, boost_start);
+	reg_finsr(regs + 0x200 * (lane + 1) + 0x84, 23, 20, boost_start);
+}
+
+static void
+k2_rio_serdes_get_average_ofs(u32 lanes, void __iomem *regs,
+			      struct k2_rio_serdes_coef_offsets *coef_ofs)
+{
+	u32 i, lane, comp;
+	u32 cmp_offset_tmp;
+
+	for (i = 0; i < KEYSTONE_SERDES_OFFSETS_RETRIES; i++) {
+		for_each_lanes(lanes, lane) {
+			k2_rio_serdes_assert_full_reset(lane, regs);
+			k2_rio_serdes_deassert_reset(lane, regs, 1);
+		}
+
+		for_each_lanes(lanes, lane) {
+			for (comp = 1;
+			     comp < KEYSTONE_SERDES_MAX_COMPS;
+			     comp++) {
+				reg_finsr(regs + 0x0a00 + 0x8c, 23, 21, comp);
+
+				reg_finsr(regs + 0x008, 31, 24,
+					  0x12 + ((lane + 1) << 5));
+
+				cmp_offset_tmp = (
+					k2_rio_serdes_read_selected_tbus(
+						regs,
+						lane + 1,
+						0x12) & 0x0ff0) >> 4;
+
+				coef_ofs->cmp_ofs[lane][comp] +=
+					cmp_offset_tmp;
+			}
+		}
+	}
+
+	for_each_lanes(lanes, lane) {
+		for (comp = 1; comp < KEYSTONE_SERDES_MAX_COMPS; comp++) {
+			coef_ofs->cmp_ofs[lane][comp] /=
+				KEYSTONE_SERDES_OFFSETS_RETRIES;
+		}
+	}
+}
+
+static void
+k2_rio_serdes_write_phy_ofs(u32 lane, void __iomem *regs, u32 comp,
+			    struct k2_rio_serdes_coef_offsets *coef_offsets)
+
+{
+	reg_finsr(regs + 0x0a00 + 0xf0, 27, 26, lane + 1);
+
+	reg_finsr(regs + 0x0a00 + 0x98, 24, 24, 0x1);
+
+	reg_finsr(regs + 0x200 * (lane + 1) + 0x2c, 2, 2, 0x1);
+
+	reg_finsr(regs + 0x200 * (lane + 1) + 0x30, 7, 5, comp);
+
+	reg_finsr(regs + 0x200 * (lane + 1) + 0x5c, 31, 31, 0x1);
+
+	reg_finsr(regs + 0x0a00 + 0x9c, 7, 0,
+		  coef_offsets->cmp_ofs[lane][comp]);
+
+	reg_finsr(regs + 0x200 * (lane + 1) + 0x58, 30, 24,
+		  coef_offsets->coef1_ofs[lane][comp]);
+
+	reg_finsr(regs + 0x200 * (lane + 1) + 0x5c, 5, 0,
+		  coef_offsets->coef2_ofs[lane][comp]);
+
+	reg_finsr(regs + 0x200 * (lane + 1) + 0x5c, 13, 8,
+		  coef_offsets->coef3_ofs[lane][comp]);
+
+	reg_finsr(regs + 0x200 * (lane + 1) + 0x5c, 21, 16,
+		  coef_offsets->coef4_ofs[lane][comp]);
+
+	reg_finsr(regs + 0x200 * (lane + 1) + 0x5c, 29, 24,
+		  coef_offsets->coef5_ofs[lane][comp]);
+
+	reg_finsr(regs + 0x200 * (lane + 1) + 0x2c, 10, 10, 0x1);
+
+	reg_finsr(regs + 0x200 * (lane + 1) + 0x2c, 10, 10, 0x0);
+
+	reg_finsr(regs + 0x0a00 + 0x98, 24, 24, 0x0);
+
+	reg_finsr(regs + 0x200 * (lane + 1) + 0x2c, 2, 2, 0x0);
+
+	reg_finsr(regs + 0x200 * (lane + 1) + 0x5c, 31, 31, 0x0);
+}
+
+static void
+k2_rio_serdes_write_average_ofs(u32 lanes, void __iomem *regs,
+				struct k2_rio_serdes_coef_offsets *coef_offsets)
+{
+	u32 lane;
+	u32 comp;
+
+	for_each_lanes(lanes, lane) {
+		for (comp = 1; comp < KEYSTONE_SERDES_MAX_COMPS; comp++) {
+			k2_rio_serdes_write_phy_ofs(lane, regs, comp,
+						    coef_offsets);
+		}
+	}
+}
+
+static void k2_rio_serdes_phy_init_cfg(
+	u32 lanes,
+	void __iomem *regs,
+	struct device *dev,
+	struct keystone_serdes_lane_tx_config *tx_config)
+{
+	struct k2_rio_serdes_coef_offsets coef_offsets;
+	u32 lane;
+
+	for_each_lanes(lanes, lane)
+		k2_rio_serdes_reset_rx(lane, regs);
+
+	usleep_range(10, 50);
+
+	k2_rio_serdes_get_average_ofs(lanes, regs, &coef_offsets);
+
+	for_each_lanes(lanes, lane) {
+		u32 i;
+
+		for (i = 1; i < KEYSTONE_SERDES_MAX_COMPS; i++) {
+			dev_dbg(dev,
+				"SerDes: %s() lane %d cmp_ofs[%d] %d\n",
+				__func__, lane, i,
+				coef_offsets.cmp_ofs[lane][i]);
+		}
+	}
+
+	dev_dbg(dev, "SerDes: %s() write average offsets\n", __func__);
+
+	k2_rio_serdes_write_average_ofs(lanes, regs, &coef_offsets);
+
+	usleep_range(10, 50);
+}
+
+static inline int k2_rio_serdes_wait_bist_chk_synch(u32 lanes,
+						    void __iomem *regs,
+						    int valid)
+{
+	u32 lane;
+	u32 temp;
+	u32 bist_valid[KEYSTONE_SERDES_MAX_LANES];
+	unsigned long timeout = jiffies
+		+ msecs_to_jiffies(KEYSTONE_SERDES_TIMEOUT);
+
+	do {
+		for_each_lanes(lanes, lane) {
+			bist_valid[lane] =
+				(k2_rio_serdes_read_selected_tbus(regs,
+								  lane + 1,
+								  0xc)
+				 & 0x0400) >> 10;
+
+			if ((__raw_readl(regs + 0x1fc0 + 0x34)
+			     & BIT(lane)) == 0)
+				bist_valid[lane] = 0;
+		}
+
+		temp = 1;
+		for_each_lanes(lanes, lane)
+			temp &= bist_valid[lane];
+
+		if (time_after(jiffies, timeout))
+			return 1;
+
+		usleep_range(10, 50);
+
+	} while (temp != valid);
+
+	return 0;
+}
+
+static int k2_rio_serdes_prbs_check(u32 lane, void __iomem *regs)
+{
+	reg_finsr(regs + (0x200 * lane) + 0x200 + 0x04, 4, 4, 0x0);
+	reg_finsr(regs + (0x200 * lane) + 0x200 + 0x04, 3, 3, 0x1);
+	return k2_rio_serdes_wait_bist_chk_synch(BIT(lane), regs, 1);
+}
+
+static void k2_rio_serdes_ber_test_tx(u32 lanes, void __iomem *regs)
+{
+	u32 lane;
+
+	for_each_lanes(lanes, lane) {
+		reg_finsr(regs + (0x200 * lane) + 0x200 + 0x34, 5, 5, 0);
+		reg_finsr(regs + (0x200 * lane) + 0x200 + 0x30, 22, 22, 1);
+		reg_finsr(regs + (0x200 * lane) + 0x200 + 0x30, 23, 23, 1);
+		reg_finsr(regs + (0x200 * lane) + 0x200 + 0x30, 18, 18, 1);
+		reg_finsr(regs + (0x200 * lane) + 0x200 + 0x30, 16, 16, 0);
+		reg_finsr(regs + (0x200 * lane) + 0x200 + 0x34,  6,  6,  1);
+		reg_finsr(regs + (0x200 * lane) + 0x200 + 0x34, 7, 7, 0);
+		reg_finsr(regs + (0x200 * lane) + 0x200 + 0x34, 27, 27, 0);
+		reg_finsr(regs + (0x200 * lane) + 0x200 + 0x30, 21, 19,
+			  KEYSTONE_SERDES_PRBS_31);
+	}
+
+	reg_finsr(regs + (5 * 0x200) + 0x14, 25, 24, 0);
+
+	for_each_lanes(lanes, lane) {
+		reg_finsr(regs + (0x200 * lane) + 0x200 + 0x30, 31, 24, 0x83);
+		reg_finsr(regs + (0x200 * lane) + 0x200 + 0x34,  1,  0, 0x2);
+
+		reg_finsr(regs + (0x200 * lane) + 0x200 + 0x34, 17,  8, 0x17c);
+
+		__raw_writel(0, regs + (0x200 * lane) + 0x200 + 0x38);
+		__raw_writel(0, regs + (0x200 * lane) + 0x200 + 0x3c);
+		__raw_writel(0, regs + (0x200 * lane) + 0x200 + 0x40);
+
+		reg_finsr(regs + (0x200 * lane) + 0x200 + 0x44, 27,  0, 0x0);
+		reg_finsr(regs + (0x200 * lane) + 0x200 + 0x34,  4,  2, 0x0);
+		reg_finsr(regs + (0x200 * lane) + 0x200 + 0x34, 30, 28, 0x0);
+	}
+
+	usleep_range(1, 5);
+
+	for_each_lanes(lanes, lane) {
+		reg_finsr(regs + (0x200 * lane) + 0x200 + 0x34,  5,  5, 0x1);
+
+		reg_finsr(regs + (0x200 * lane) + 0x200 + 0x00,  7,  6, 0x2);
+		reg_finsr(regs + (0x200 * lane) + 0x200 + 0x00, 22, 20, 0x0);
+	}
+
+	usleep_range(1, 5);
+
+	for_each_lanes(lanes, lane)
+		reg_finsr(regs + (0x200 * lane) + 0x200 + 0x34,  7,  7, 0x1);
+
+	usleep_range(10, 50);
+
+	for_each_lanes(lanes, lane)
+		k2_rio_serdes_reacquire_sd(lane, regs);
+
+	usleep_range(10, 50);
+
+	for_each_lanes(lanes, lane)
+		reg_finsr(regs + (0x200 * lane) + 0x200 + 0x04, 6, 3,
+			  (BIT(1) | (0x2 << 2)));
+}
+
+static int k2_rio_serdes_prbs_test_start(u32 lane,
+					 void __iomem *regs,
+					 struct device *dev)
+{
+	int res;
+
+	dev_dbg(dev, "SerDes: enable the Tx PRBS pattern for lane %d\n", lane);
+
+	k2_rio_serdes_ber_test_tx(BIT(lane), regs);
+
+	dev_dbg(dev, "SerDes: check PRBS pattern for lane %d\n", lane);
+
+	res = k2_rio_serdes_prbs_check(lane, regs);
+	if (res) {
+		dev_dbg(dev,
+			"SerDes: timeout when checking PRBS pattern for lane %d\n",
+			lane);
+		return -EAGAIN;
+	}
+
+	return 0;
+}
+
+static void k2_rio_serdes_prbs_test_stop(u32 lanes,
+					 void __iomem *regs,
+					 struct device *dev)
+{
+	u32 lane;
+	u32 value;
+
+	dev_dbg(dev, "SerDes: disable Tx PRBS pattern for lanes 0x%x\n", lanes);
+
+	for_each_lanes(lanes, lane) {
+		reg_finsr(regs + (0x200 * lane) + 0x200 + 0x04, 4, 4, 0x1);
+
+		reg_finsr(regs + (0x200 * lane) + 0x200 + 0x04, 3, 3, 0x0);
+
+		reg_finsr(regs + (0x200 * lane) + 0x200 + 0x34, 5, 5, 0);
+	}
+
+	if (k2_rio_serdes_wait_bist_chk_synch(lanes, regs, 0))
+		dev_warn(dev,
+			 "SerDes: Tx PRBS not successfuly disabled for lanes 0x%x\n",
+			 lanes);
+
+	for_each_lanes(lanes, lane) {
+		value = __raw_readl(regs + (0x200 * lane) + 0x200 + 0x48);
+		if (value & 0xffff)
+			dev_warn(dev,
+				 "SerDes: Tx PRBS BIST error detected (0x%x) for lanes 0x%x\n",
+				 value & 0xffff, lanes);
+
+		reg_finsr(regs + (0x200 * lane) + 0x200 + 0x00,  7,  6, 0x0);
+		reg_finsr(regs + (0x200 * lane) + 0x200 + 0x00, 22, 20, 0x0);
+
+		usleep_range(1, 5);
+	}
+}
+
+static int k2_rio_serdes_cal_lane(u32 lane, u32 lanes,
+				  struct keystone_serdes_data *serdes)
+{
+	struct device *dev = serdes->dev;
+	void __iomem *regs = serdes->regs;
+	int res = 0;
+
+	res = k2_rio_serdes_wait_lanes_sd(BIT(lane), serdes);
+	if (res < 0) {
+		dev_dbg(dev, "SerDes: %s() lane %d not OK\n",
+			__func__, lane);
+		return res;
+	}
+
+	res = k2_rio_serdes_prbs_test_start(lane, regs, dev);
+	if (res != 0)
+		return res;
+
+	dev_dbg(dev, "SerDes: do the att/boost setting\n");
+	k2_rio_serdes_calibrate_rx(lane, lanes, regs, serdes->config->rx,
+				   serdes->config->rate);
+
+	dev_dbg(dev,
+		"SerDes: computed Rx att = %d, Rx boost = %d for lane %d\n",
+		serdes->config->rx[lane].mean_att,
+		serdes->config->rx[lane].mean_boost,
+		lane);
+
+	return res;
+}
+
+static int k2_rio_serdes_calibrate_lanes(u32 lanes,
+					 struct keystone_serdes_data *serdes)
+{
+	unsigned long timeout = jiffies + msecs_to_jiffies(
+		serdes->config->cal_timeout * MSEC_PER_SEC);
+	struct device *dev = serdes->dev;
+	void __iomem *regs = serdes->regs;
+	int res = 0;
+	u32 lane;
+	u32 __lanes = lanes;
+
+	for_each_lanes(lanes, lane) {
+		serdes->config->rx[lane].mean_att   = -1;
+		serdes->config->rx[lane].mean_boost = -1;
+	}
+
+	k2_rio_serdes_start_tx_lanes(lanes, serdes);
+
+	while (__lanes) {
+		for (lane = 0;
+		     lane < KEYSTONE_SERDES_MAX_LANES;
+		     lane++) {
+			if (BIT(lane) & __lanes) {
+				if (k2_rio_serdes_cal_lane(lane, lanes,
+							   serdes) == 0)
+					__lanes &= ~(BIT(lane));
+			}
+		}
+
+		if (time_after(jiffies, timeout)) {
+			res = -EAGAIN;
+			break;
+		}
+	}
+
+	usleep_range(10000, 20000);
+
+	k2_rio_serdes_prbs_test_stop(lanes, regs, dev);
+
+	for_each_lanes(lanes, lane)
+		k2_rio_serdes_set_tx_idle(lane, regs);
+
+	k2_rio_serdes_config_set_rx_coeffs(lanes, serdes->regs, serdes->dev,
+					   serdes->config->rx);
+
+	for_each_lanes(lanes, lane)
+		k2_rio_serdes_lane_disable(lane, regs);
+
+	return res;
+}
+
+static int k2_rio_serdes_config_lanes(u32 lanes,
+				      u32 baud,
+				      struct keystone_serdes_data *serdes)
+{
+	struct device *dev = serdes->dev;
+	void __iomem *regs = serdes->regs;
+	u32 rate;
+	u32 val;
+	u32 lane;
+	u32 tx_term_np;
+	int res;
+
+	dev_dbg(dev, "SerDes: configuring SerDes for lane mask 0x%x\n", lanes);
+
+	__raw_writel(0x80000000, regs + 0x1ff4);
+
+	switch (baud) {
+	case KEYSTONE_SERDES_BAUD_1_250:
+		rate = KEYSTONE_SERDES_QUARTER_RATE;
+		k2_rio_serdes_init_5g(regs);
+		break;
+	case KEYSTONE_SERDES_BAUD_2_500:
+		rate = KEYSTONE_SERDES_HALF_RATE;
+		k2_rio_serdes_init_5g(regs);
+		break;
+	case KEYSTONE_SERDES_BAUD_5_000:
+		rate = KEYSTONE_SERDES_FULL_RATE;
+		k2_rio_serdes_init_5g(regs);
+		break;
+	case KEYSTONE_SERDES_BAUD_3_125:
+		rate = KEYSTONE_SERDES_HALF_RATE;
+		k2_rio_serdes_init_3g(regs);
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	serdes->config->rate = rate;
+
+	lanes |= KEYSTONE_SERDES_LANE(0);
+
+	for_each_lanes(lanes, lane)
+		k2_rio_serdes_set_tx_idle(lane, regs);
+
+	for_each_lanes(lanes, lane)
+		k2_rio_serdes_lane_init(lane, regs, rate);
+
+	__raw_writel(0xe0000000, regs + 0x1ff4);
+
+	do {
+		val = __raw_readl(regs + 0x1ff4);
+	} while (!(val & BIT(28)));
+
+	res = k2_rio_serdes_wait_lanes_ok(lanes, regs);
+	if (res < 0) {
+		dev_dbg(dev, "SerDes: %s() lane mask 0x%x not OK\n",
+			__func__, lanes);
+		return res;
+	}
+
+	tx_term_np = k2_rio_serdes_get_termination(regs);
+	dev_dbg(dev, "SerDes: termination tx is 0x%x\n", tx_term_np);
+
+	for_each_lanes(lanes, lane) {
+		dev_dbg(dev,
+			"SerDes: Tx vdreg = %d, Rx att start = %d, Rx boost start = %d for lane %d\n",
+			serdes->config->tx[lane].vdreg,
+			serdes->config->rx[lane].start_att,
+			serdes->config->rx[lane].start_boost,
+			lane);
+
+		k2_rio_serdes_tx_rx_set_equalizer(
+			lane,
+			regs,
+			serdes->config->tx[lane].vdreg,
+			serdes->config->rx[lane].start_att,
+			serdes->config->rx[lane].start_boost);
+	}
+
+	for_each_lanes(lanes, lane) {
+		k2_rio_serdes_assert_full_reset(lane, regs);
+
+		k2_rio_serdes_config_set_tx_coeffs(lane, regs,
+						   &serdes->config->tx[lane]);
+
+		k2_rio_serdes_reset_rx(lane, regs);
+
+		k2_rio_serdes_deassert_reset(lane, regs, 1);
+
+		k2_rio_serdes_termination_config(lane, regs, tx_term_np);
+	}
+
+	k2_rio_serdes_wait_lanes_ok(lanes, regs);
+
+	if (serdes->config->do_phy_init_cfg) {
+		dev_dbg(dev, "SerDes: lanes OK, start phy init cfg\n");
+
+		k2_rio_serdes_phy_init_cfg(lanes, regs, dev,
+					   serdes->config->tx);
+	}
+
+	for_each_lanes(lanes, lane)
+		k2_rio_serdes_reacquire_sd(lane, regs);
+
+	usleep_range(10, 50);
+
+	k2_rio_serdes_config_set_rx_coeffs(lanes, serdes->regs, serdes->dev,
+					   serdes->config->rx);
+
+	k2_rio_serdes_wait_lanes_ok(lanes, regs);
+
+	for_each_lanes(lanes, lane)
+		k2_rio_serdes_lane_disable(lane, regs);
+
+	return res;
+}
+
+static int k2_rio_serdes_disable_lanes(u32 lanes,
+				       struct keystone_serdes_data *serdes)
+{
+	void __iomem *regs = serdes->regs;
+	u32 lane;
+
+	dev_dbg(serdes->dev, "disable lanes 0x%x\n", lanes);
+
+	for_each_lanes(lanes, lane)
+		k2_rio_serdes_lane_disable(lane, regs);
+
+	return 0;
+}
+
+static int k2_rio_serdes_shutdown_lanes(u32 lanes,
+					struct keystone_serdes_data *serdes)
+{
+	void __iomem *regs = serdes->regs;
+	u32 lane;
+
+	dev_dbg(serdes->dev, "shutdown lanes 0x%x\n", lanes);
+
+	for_each_lanes(lanes, lane) {
+		k2_rio_serdes_lane_disable(lane, regs);
+		k2_rio_serdes_set_overlay_bit29(lane, regs);
+	}
+
+	reg_finsr(regs + 0x1fc0 + 0x34, 30, 29, 0x0);
+
+	reg_finsr(regs + 0x10, 28, 28, 0x1);
+
+	return 0;
+}
+
+static void
+k2_rio_serdes_recover_lane(int lane, void __iomem *regs, struct device *dev,
+			   struct keystone_serdes_config *serdes_config)
+{
+	int res;
+	struct keystone_serdes_lane_rx_config rx_coeff;
+
+	dev_dbg(dev, "SerDes: recover lane %d\n", lane);
+
+	k2_rio_serdes_get_att_boost(lane, regs, &rx_coeff);
+
+	dev_dbg(dev,
+		"SerDes: current Rx att = %d, boost = %d for lane %d\n",
+		rx_coeff.att, rx_coeff.boost, lane);
+
+	k2_rio_serdes_reset_rx(lane, regs);
+
+	usleep_range(10, 50);
+
+	k2_rio_serdes_reacquire_sd(lane, regs);
+
+	dev_dbg(dev, "SerDes: lane %d Rx path reset done\n", lane);
+
+	res = k2_rio_serdes_calibrate_boost(lane, regs);
+	if (res) {
+		dev_warn(dev, "SerDes: lane %d boost setting failed\n", lane);
+		return;
+	}
+
+	dev_dbg(dev, "SerDes: lane %d boost/att setting done\n", lane);
+
+	k2_rio_serdes_get_att_boost(lane, regs, &rx_coeff);
+
+	dev_dbg(dev,
+		"SerDes: current Rx att = %d, boost = %d for lane %d\n",
+		rx_coeff.att, rx_coeff.boost, lane);
+
+	dev_dbg(dev, "SerDes: lane %d recovered\n", lane);
+}
+
+static void k2_rio_serdes_recover_lanes(u32 lanes,
+					struct keystone_serdes_data *serdes)
+{
+	struct device *dev = serdes->dev;
+	void __iomem *regs = serdes->regs;
+	unsigned int stat;
+	unsigned int i_dlpf;
+	u32 lane;
+
+	for_each_lanes(lanes, lane) {
+		stat = (k2_rio_serdes_read_selected_tbus(regs, lane + 1,
+							 0x2) & 0x0060) >> 5;
+
+		dev_dbg(dev, "SerDes: check rx valid for lane %d, stat = %d\n",
+			lane, stat);
+
+		if (stat == 3) {
+			i_dlpf = k2_rio_serdes_read_selected_tbus(
+				regs, lane + 1, 5) >> 10;
+
+			if ((i_dlpf == 0) || (i_dlpf == 3))
+				k2_rio_serdes_recover_lane(lane, regs, dev,
+							   serdes->config);
+		}
+	}
+}
+
+static int k2_rio_serdes_start_tx_lanes(
+	u32 lanes,
+	struct keystone_serdes_data *serdes)
+{
+	struct device *dev = serdes->dev;
+	void __iomem *regs = serdes->regs;
+	u32 lane;
+	int res;
+
+	for_each_lanes(lanes, lane) {
+		dev_dbg(dev, "SerDes: start transmit for lane %d\n", lane);
+
+		k2_rio_serdes_assert_reset(lane, regs);
+
+		k2_rio_serdes_config_set_tx_coeffs(lane, regs,
+						   &serdes->config->tx[lane]);
+
+		dev_dbg(dev,
+			"SerDes: lane %d: c1 = %d, c2 = %d, cm = %d, att = %d, 1lsb = %d, vreg = %d\n",
+			lane,
+			serdes->config->tx[lane].c1_coeff,
+			serdes->config->tx[lane].c2_coeff,
+			serdes->config->tx[lane].cm_coeff,
+			serdes->config->tx[lane].att,
+			serdes->config->tx[lane].pre_1lsb,
+			serdes->config->tx[lane].vreg);
+
+		k2_rio_serdes_set_tx_normal(lane, regs);
+
+		k2_rio_serdes_deassert_reset(lane, regs, 0);
+
+		k2_rio_serdes_lane_enable(lane, regs);
+	}
+
+	for_each_lanes(lanes, lane)
+		k2_rio_serdes_clear_overlay_bit29(lane, regs);
+
+	res = k2_rio_serdes_wait_lanes_ok(lanes, regs);
+	if (res < 0) {
+		dev_dbg(dev, "SerDes: %s() lane mask 0x%x not OK\n",
+			__func__, lanes);
+		return res;
+	}
+
+	return res;
+}
+
+static const struct keystone_serdes_ops k2_serdes_ops = {
+	.config_lanes       = k2_rio_serdes_config_lanes,
+	.start_tx_lanes     = k2_rio_serdes_start_tx_lanes,
+	.wait_lanes_ok      = k2_rio_serdes_wait_lanes_sd,
+	.disable_lanes      = k2_rio_serdes_disable_lanes,
+	.shutdown_lanes     = k2_rio_serdes_shutdown_lanes,
+	.recover_lanes      = k2_rio_serdes_recover_lanes,
+	.calibrate_lanes    = k2_rio_serdes_calibrate_lanes,
+};
+
+struct serdes_attribute {
+	struct attribute attr;
+	ssize_t (*show)(struct kobject *kobj,
+			struct serdes_attribute *attr,
+			char *buf);
+	ssize_t	(*store)(struct kobject *kobj,
+			 struct serdes_attribute *attr,
+			 const char *,
+			 size_t);
+	struct keystone_serdes_data *serdes;
+	void *context;
+};
+
+#define __SERDES_ATTR(_name, _mode, _show, _store, _ctxt) \
+	{						\
+		.attr = {				\
+			.name = __stringify(_name),	\
+			.mode = _mode },		\
+		.show	= _show,			\
+		.store	= _store,		\
+		.context = (_ctxt),			\
+	 }
+
+#define to_serdes_attr(_attr) container_of(_attr, struct serdes_attribute, attr)
+
+static ssize_t serdes_tx_attr_show(struct kobject *kobj,
+				   struct serdes_attribute *attr,
+				   char *buf)
+{
+	struct keystone_serdes_data *serdes =
+		(struct keystone_serdes_data *)attr->context;
+	struct keystone_serdes_config *serdes_config = serdes->config;
+	u32 lane;
+	int len = 0;
+
+	for (lane = 0; lane < KEYSTONE_SERDES_MAX_LANES; lane++) {
+		u32 val = -1;
+
+		if (strcmp("c1", attr->attr.name) == 0)
+			val = serdes_config->tx[lane].c1_coeff;
+		if (strcmp("c2", attr->attr.name) == 0)
+			val = serdes_config->tx[lane].c2_coeff;
+		if (strcmp("cm", attr->attr.name) == 0)
+			val = serdes_config->tx[lane].cm_coeff;
+		if (strcmp("pre_1lsb", attr->attr.name) == 0)
+			val = serdes_config->tx[lane].pre_1lsb;
+		if (strcmp("att", attr->attr.name) == 0)
+			val = serdes_config->tx[lane].att;
+		if (strcmp("vreg", attr->attr.name) == 0)
+			val = serdes_config->tx[lane].vreg;
+
+		if (lane == 0)
+			len = snprintf(buf + len, PAGE_SIZE, "%d", val);
+		else
+			len += snprintf(buf + len, PAGE_SIZE, " %d", val);
+	}
+
+	len += snprintf(buf + len, PAGE_SIZE, "\n");
+
+	return len;
+}
+
+static ssize_t serdes_tx_attr_store(struct kobject *kobj,
+				    struct serdes_attribute *attr,
+				    const char *buf,
+				    size_t size)
+{
+	struct keystone_serdes_data *serdes =
+		(struct keystone_serdes_data *)attr->context;
+	struct keystone_serdes_config *serdes_config = serdes->config;
+	u32 lane;
+	u32 val[4];
+
+	if (sscanf(buf, "%d %d %d %d", &val[0], &val[1], &val[2], &val[3]) < 4)
+		return -EINVAL;
+
+	for (lane = 0; lane < KEYSTONE_SERDES_MAX_LANES; lane++) {
+		if (strcmp("c1", attr->attr.name) == 0)
+			serdes_config->tx[lane].c1_coeff = val[lane];
+		if (strcmp("c2", attr->attr.name) == 0)
+			serdes_config->tx[lane].c2_coeff = val[lane];
+		if (strcmp("cm", attr->attr.name) == 0)
+			serdes_config->tx[lane].cm_coeff = val[lane];
+		if (strcmp("pre_1lsb", attr->attr.name) == 0)
+			serdes_config->tx[lane].pre_1lsb = val[lane];
+		if (strcmp("att", attr->attr.name) == 0)
+			serdes_config->tx[lane].att = val[lane];
+		if (strcmp("vreg", attr->attr.name) == 0)
+			serdes_config->tx[lane].vreg = val[lane];
+
+		k2_rio_serdes_assert_full_reset(lane, serdes->regs);
+
+		k2_rio_serdes_config_set_tx_coeffs(lane,
+						   serdes->regs,
+						   &serdes_config->tx[lane]);
+
+		k2_rio_serdes_deassert_reset(lane, serdes->regs, 1);
+	}
+
+	return size;
+}
+
+static ssize_t serdes_rx_attr_show(struct kobject *kobj,
+				   struct serdes_attribute *attr,
+				   char *buf)
+{
+	struct keystone_serdes_lane_rx_config rx_coeff;
+	struct keystone_serdes_data *serdes =
+		(struct keystone_serdes_data *)attr->context;
+	u32 lane;
+	int len = 0;
+
+	memset(&rx_coeff, -1, sizeof(rx_coeff));
+
+	for (lane = 0; lane < KEYSTONE_SERDES_MAX_LANES; lane++) {
+		u32 val;
+
+		k2_rio_serdes_get_att_boost(lane, serdes->regs, &rx_coeff);
+
+		if (rx_coeff.att == 0)
+			rx_coeff.att = serdes->config->rx[lane].mean_att;
+
+		if (rx_coeff.boost == 0)
+			rx_coeff.boost = serdes->config->rx[lane].mean_boost;
+
+		if (rx_coeff.att == -1)
+			rx_coeff.att = 0;
+
+		if (rx_coeff.boost == -1)
+			rx_coeff.boost = 0;
+
+		if (strcmp("att", attr->attr.name) == 0)
+			val = rx_coeff.att;
+		else
+			val = rx_coeff.boost;
+
+		if (lane == 0)
+			len = snprintf(buf + len, PAGE_SIZE, "%d", val);
+		else
+			len += snprintf(buf + len, PAGE_SIZE, " %d", val);
+	}
+
+	len += snprintf(buf + len, PAGE_SIZE, "\n");
+
+	return len;
+}
+
+static ssize_t serdes_rx_attr_store(struct kobject *kobj,
+				    struct serdes_attribute *attr,
+				    const char *buf,
+				    size_t size)
+{
+	struct keystone_serdes_data *serdes =
+		(struct keystone_serdes_data *)attr->context;
+	struct keystone_serdes_config *serdes_config = serdes->config;
+	u32 lane;
+	u32 lanes = 0;
+	u32 val[4];
+
+	if (sscanf(buf, "%d %d %d %d", &val[0], &val[1], &val[2], &val[3]) < 4)
+		return -EINVAL;
+
+	for (lane = 0; lane < KEYSTONE_SERDES_MAX_LANES; lane++) {
+		if (strcmp("att", attr->attr.name) == 0)
+			serdes_config->rx[lane].mean_att   = val[lane];
+		else
+			serdes_config->rx[lane].mean_boost = val[lane];
+
+		lanes |= BIT(lane);
+	}
+
+	return size;
+}
+
+static ssize_t serdes_n_attr_show(struct kobject *kobj,
+				  struct attribute *attr,
+				  char *buf)
+{
+	struct serdes_attribute *attribute = to_serdes_attr(attr);
+
+	if (!attribute->show)
+		return -EIO;
+
+	return attribute->show(kobj, attribute, buf);
+}
+
+static ssize_t serdes_n_attr_store(struct kobject *kobj,
+				   struct attribute *attr,
+				   const char *buf,
+				   size_t count)
+{
+	struct serdes_attribute *attribute = to_serdes_attr(attr);
+
+	if (!attribute->store)
+		return -EIO;
+
+	return attribute->store(kobj, attribute, buf, count);
+}
+
+static const struct sysfs_ops serdes_sysfs_ops = {
+	.show  = serdes_n_attr_show,
+	.store = serdes_n_attr_store,
+};
+
+static struct serdes_attribute serdes_rx_att_attribute =
+	__SERDES_ATTR(att, S_IRUGO | S_IWUSR,
+		      serdes_rx_attr_show,
+		      serdes_rx_attr_store,
+		      NULL);
+
+static struct serdes_attribute serdes_rx_boost_attribute =
+	__SERDES_ATTR(boost, S_IRUGO | S_IWUSR,
+		      serdes_rx_attr_show,
+		      serdes_rx_attr_store,
+		      NULL);
+
+static struct serdes_attribute serdes_tx_cm_attribute =
+	__SERDES_ATTR(cm, S_IRUGO | S_IWUSR,
+		      serdes_tx_attr_show,
+		      serdes_tx_attr_store,
+		      NULL);
+
+static struct serdes_attribute serdes_tx_c1_attribute =
+	__SERDES_ATTR(c1, S_IRUGO | S_IWUSR,
+		      serdes_tx_attr_show,
+		      serdes_tx_attr_store,
+		      NULL);
+
+static struct serdes_attribute serdes_tx_c2_attribute =
+	__SERDES_ATTR(c2, S_IRUGO | S_IWUSR,
+		      serdes_tx_attr_show,
+		      serdes_tx_attr_store,
+		      NULL);
+
+static struct serdes_attribute serdes_tx_att_attribute =
+	__SERDES_ATTR(att, S_IRUGO | S_IWUSR,
+		      serdes_tx_attr_show,
+		      serdes_tx_attr_store,
+		      NULL);
+
+static struct serdes_attribute serdes_tx_lsb_attribute =
+	__SERDES_ATTR(pre_1lsb, S_IRUGO | S_IWUSR,
+		      serdes_tx_attr_show,
+		      serdes_tx_attr_store,
+		      NULL);
+
+static struct serdes_attribute serdes_tx_vreg_attribute =
+	__SERDES_ATTR(vreg, S_IRUGO | S_IWUSR,
+		      serdes_tx_attr_show,
+		      serdes_tx_attr_store,
+		      NULL);
+
+static struct attribute *serdes_rx_attrs[] = {
+	&serdes_rx_att_attribute.attr,
+	&serdes_rx_boost_attribute.attr,
+	NULL
+};
+
+static struct attribute *serdes_tx_attrs[] = {
+	&serdes_tx_cm_attribute.attr,
+	&serdes_tx_c1_attribute.attr,
+	&serdes_tx_c2_attribute.attr,
+	&serdes_tx_att_attribute.attr,
+	&serdes_tx_lsb_attribute.attr,
+	&serdes_tx_vreg_attribute.attr,
+	NULL
+};
+
+static struct kobj_type serdes_rx_type = {
+	.sysfs_ops     = &serdes_sysfs_ops,
+	.default_attrs = serdes_rx_attrs,
+};
+
+static struct kobj_type serdes_tx_type = {
+	.sysfs_ops     = &serdes_sysfs_ops,
+	.default_attrs = serdes_tx_attrs,
+};
+
+static int keystone_rio_serdes_sysfs_create(
+	struct keystone_serdes_data *serdes,
+	struct keystone_serdes_config *serdes_config)
+{
+	struct device *dev = serdes->dev;
+	int res;
+
+	serdes->serdes_kobj = kobject_create_and_add(
+		"serdes",
+		kobject_get(&dev->kobj));
+
+	if (!serdes->serdes_kobj) {
+		dev_err(dev, "unable create sysfs serdes file\n");
+		kobject_put(&dev->kobj);
+		return -ENOMEM;
+	}
+
+	serdes_rx_att_attribute.context   = (void *)serdes;
+	serdes_rx_boost_attribute.context = (void *)serdes;
+
+	res = kobject_init_and_add(&serdes->serdes_rx_kobj,
+				   &serdes_rx_type,
+				   kobject_get(serdes->serdes_kobj),
+				   "rx");
+	if (res) {
+		dev_err(dev, "unable create sysfs serdes/rx files\n");
+		kobject_put(serdes->serdes_kobj);
+		kobject_del(serdes->serdes_kobj);
+		kobject_put(&dev->kobj);
+		return res;
+	}
+
+	serdes_tx_cm_attribute.context   = (void *)serdes;
+	serdes_tx_c1_attribute.context   = (void *)serdes;
+	serdes_tx_c2_attribute.context   = (void *)serdes;
+	serdes_tx_att_attribute.context  = (void *)serdes;
+	serdes_tx_lsb_attribute.context  = (void *)serdes;
+	serdes_tx_vreg_attribute.context = (void *)serdes;
+
+	res = kobject_init_and_add(&serdes->serdes_tx_kobj,
+				   &serdes_tx_type,
+				   kobject_get(serdes->serdes_kobj),
+				   "tx");
+	if (res) {
+		dev_err(dev, "unable create sysfs serdes/tx files\n");
+		kobject_del(&serdes->serdes_rx_kobj);
+		kobject_put(serdes->serdes_kobj);
+		kobject_del(serdes->serdes_kobj);
+		kobject_put(&dev->kobj);
+		return res;
+	}
+
+	return 0;
+}
+
+static void keystone_rio_serdes_sysfs_remove(
+	struct device *dev,
+	struct keystone_serdes_data *serdes)
+{
+	kobject_del(&serdes->serdes_tx_kobj);
+	kobject_put(serdes->serdes_kobj);
+	kobject_del(&serdes->serdes_rx_kobj);
+	kobject_put(serdes->serdes_kobj);
+	kobject_del(serdes->serdes_kobj);
+	kobject_put(&dev->kobj);
+}
+
+int keystone_rio_serdes_register(u16 serdes_type,
+				 void __iomem *regs,
+				 void __iomem *sts_reg,
+				 struct device *dev,
+				 struct keystone_serdes_data *serdes,
+				 struct keystone_serdes_config *serdes_config)
+{
+	int res = 0;
+
+	memset(serdes, 0, sizeof(*serdes));
+
+	if (!serdes)
+		return -EINVAL;
+
+	switch (serdes_type) {
+	case KEYSTONE_SERDES_TYPE_K1:
+		serdes->ops = &k1_serdes_ops;
+		break;
+	case KEYSTONE_SERDES_TYPE_K2:
+		serdes->ops = &k2_serdes_ops;
+		break;
+	default:
+		res = -EINVAL;
+	}
+
+	if (res)
+		goto error;
+
+	serdes->dev     = dev;
+	serdes->config  = serdes_config;
+	serdes->regs    = regs;
+	serdes->sts_reg = sts_reg;
+
+	res = keystone_rio_serdes_sysfs_create(serdes, serdes_config);
+
+error:
+	return res;
+}
+
+void keystone_rio_serdes_unregister(struct device *dev,
+				    struct keystone_serdes_data *serdes)
+{
+	keystone_rio_serdes_sysfs_remove(dev, serdes);
+}
diff --git a/drivers/rapidio/devices/keystone_rio_serdes.h b/drivers/rapidio/devices/keystone_rio_serdes.h
new file mode 100644
index 0000000..30f46ac
--- /dev/null
+++ b/drivers/rapidio/devices/keystone_rio_serdes.h
@@ -0,0 +1,124 @@
+/*
+ * Texas Instruments Keystone SerDes driver
+ * Authors: Aurelien Jacquiot <a-jacquiot@ti.com>
+ *
+ * This is the Rapidio SerDes driver for Keystone devices. This is
+ * required to support RapidIO functionality on K2HK devices.
+ *
+ * Copyright (C) 2016 Texas Instruments Incorporated - http://www.ti.com/
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ * Redistributions of source code must retain the above copyright
+ * notice, this list of conditions and the following disclaimer.
+ *
+ * Redistributions in binary form must reproduce the above copyright
+ * notice, this list of conditions and the following disclaimer in the
+ * documentation and/or other materials provided with the
+ * distribution.
+ *
+ * Neither the name of Texas Instruments Incorporated nor the names of
+ * its contributors may be used to endorse or promote products derived
+ * from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+#ifndef KEYSTONE_RIO_SERDES_H
+#define KEYSTONE_RIO_SERDES_H
+
+#define KEYSTONE_SERDES_TYPE_K1      0
+#define KEYSTONE_SERDES_TYPE_K2      1
+
+#define KEYSTONE_SERDES_LANE(lane)   (BIT(lane))
+#define KEYSTONE_SERDES_MAX_LANES    4
+#define KEYSTONE_SERDES_LANE_MASK    0xf
+
+#define KEYSTONE_SERDES_TIMEOUT      100
+
+#define KEYSTONE_SERDES_BAUD_1_250   0
+#define KEYSTONE_SERDES_BAUD_2_500   1
+#define KEYSTONE_SERDES_BAUD_3_125   2
+#define KEYSTONE_SERDES_BAUD_5_000   3
+#define KEYSTONE_SERDES_BAUD_6_250   4
+
+#define KEYSTONE_SERDES_QUARTER_RATE 0
+#define KEYSTONE_SERDES_HALF_RATE    1
+#define KEYSTONE_SERDES_FULL_RATE    2
+
+struct keystone_serdes_lane_tx_config {
+	u32 pre_1lsb;
+	u32 c1_coeff;
+	u32 c2_coeff;
+	u32 cm_coeff;
+	u32 att;
+	u32 vreg;
+	u32 vdreg;
+};
+
+struct keystone_serdes_lane_rx_config {
+	u32 att;
+	u32 boost;
+	u32 mean_att;
+	u32 mean_boost;
+	u32 start_att;
+	u32 start_boost;
+};
+
+struct keystone_serdes_config {
+	u16 prescalar_srv_clk;
+
+	struct keystone_serdes_lane_tx_config tx[KEYSTONE_SERDES_MAX_LANES];
+	struct keystone_serdes_lane_rx_config rx[KEYSTONE_SERDES_MAX_LANES];
+
+	u32 cal_timeout;
+	int do_phy_init_cfg;
+	u32 rate;
+};
+
+struct keystone_serdes_data;
+
+struct keystone_serdes_ops {
+	int (*config_lanes)(u32 lanes, u32 baud,
+			    struct keystone_serdes_data *serdes);
+	int (*start_tx_lanes)(u32 lanes, struct keystone_serdes_data *serdes);
+	int (*wait_lanes_ok)(u32 lanes, struct keystone_serdes_data *serdes);
+	int (*disable_lanes)(u32 lanes, struct keystone_serdes_data *serdes);
+	int (*shutdown_lanes)(u32 lanes, struct keystone_serdes_data *serdes);
+	void (*recover_lanes)(u32 lanes, struct keystone_serdes_data *serdes);
+	int (*calibrate_lanes)(u32 lanes, struct keystone_serdes_data *serdes);
+};
+
+struct keystone_serdes_data {
+	const struct keystone_serdes_ops	*ops;
+	struct device				*dev;
+	void __iomem				*regs;
+	void __iomem				*sts_reg;
+	struct keystone_serdes_config		*config;
+	struct kobject				*serdes_kobj;
+	struct kobject				serdes_rx_kobj;
+	struct kobject				serdes_tx_kobj;
+};
+
+int keystone_rio_serdes_register(u16 serdes_type,
+				 void __iomem *regs,
+				 void __iomem *sts_reg,
+				 struct device *dev,
+				 struct keystone_serdes_data *serdes,
+				 struct keystone_serdes_config *serdes_config);
+
+void keystone_rio_serdes_unregister(struct device *dev,
+				    struct keystone_serdes_data *serdes);
+
+#endif
-- 
1.7.5.4

