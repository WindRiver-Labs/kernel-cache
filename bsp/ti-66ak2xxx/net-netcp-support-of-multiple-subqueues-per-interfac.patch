From 873106f3ad55acc568267da4372d89275796cdfb Mon Sep 17 00:00:00 2001
From: WingMan Kwok <w-kwok2@ti.com>
Date: Thu, 28 Apr 2016 18:18:27 -0400
Subject: [PATCH 280/347] net: netcp: support of multiple subqueues per
 interface

This patch comes from:
  git://git.ti.com/processor-sdk/processor-sdk-linux.git

This patch adds the support of multiple subqueues per interface.  This
is particularly useful when support of NETCP QoS is enabled.  In that
case, each subqueue of the interface will be mapped to support one class
of service.

In this implementation, one subqueue is mapped to a NETCP txpipe with its
own tx-pool and tx-threshold and rx-threshold.  This allows the
flexibility of a configurable number of descriptors/buffers in order
to serve the need of that particular class of service.

This patch is backward compatible to the support of one subqueue
configured in the way prior to this patch. But it won't be able to
support the up coming NETCP QoS with the legacy subqueue configurations.

Signed-off-by: WingMan Kwok <w-kwok2@ti.com>
Signed-off-by: Jacob Stiffler <j-stiffler@ti.com>
(cherry picked from commit d3e832e738ba6ee2cde3abaf53228b3469529c18)
Signed-off-by: Zumeng Chen <zumeng.chen@windriver.com>
---
 .../devicetree/bindings/net/keystone-netcp.txt     |   18 ++-
 drivers/net/ethernet/ti/netcp.h                    |   24 ++-
 drivers/net/ethernet/ti/netcp_core.c               |  203 +++++++++++++-------
 3 files changed, 165 insertions(+), 80 deletions(-)

diff --git a/Documentation/devicetree/bindings/net/keystone-netcp.txt b/Documentation/devicetree/bindings/net/keystone-netcp.txt
index 0fb1eed..b95aa1f 100644
--- a/Documentation/devicetree/bindings/net/keystone-netcp.txt
+++ b/Documentation/devicetree/bindings/net/keystone-netcp.txt
@@ -141,8 +141,11 @@ Required properties:
 - rx-queue:	the navigator queue number associated with rx dma channel.
 - rx-pool:	specifies the number of descriptors to be used & the region-id
 		for creating the rx descriptor pool.
-- tx-pool:	specifies the number of descriptors to be used & the region-id
-		for creating the tx descriptor pool.
+- tx-pools:	a multiple of quadruples, each quadruple configures one tx-pool
+		which corresponds to one tx-pipe and hence one subqueue.
+		Entries in a quadruple specifies the number of descriptors to
+		be used, the region-id for creating the tx descriptor pool,
+		the tx-threshold and rx-threshold.
 - rx-queue-depth:	number of descriptors in each of the free descriptor
 			queue (FDQ) for the pktdma Rx flow. There can be at
 			present a maximum of 4 queues per Rx flow.
@@ -400,7 +403,16 @@ netcpx: netcpx@2f00000 {
 		interface-0 {
 			rx-channel = "xnetrx0";
 			rx-pool = <2048 12>; /* num_desc region-id */
-			tx-pool = <1024 12>; /* num_desc region-id */
+			/* 7 tx pools, hence 7 subqueues
+			 *   <#desc rgn-id tx-thresh rx-thresh>
+			 */
+			tx-pools = <1024 12 17 17
+				    64 12 17 17
+				    64 12 17 17
+				    64 12 17 17
+				    64 12 17 17
+				    64 12 17 17
+				    64 12 17 17>;
 			rx-queue-depth = <1024 1024 0 0>;
 			rx-buffer-size = <1536 4096 0 0>;
 			rx-queue = <532>;
diff --git a/drivers/net/ethernet/ti/netcp.h b/drivers/net/ethernet/ti/netcp.h
index 9672ea3..13c1025 100644
--- a/drivers/net/ethernet/ti/netcp.h
+++ b/drivers/net/ethernet/ti/netcp.h
@@ -41,6 +41,8 @@
 #define XGMII_LINK_MAC_PHY		10
 #define XGMII_LINK_MAC_MAC_FORCED	11
 
+#define NETCP_MAX_SUBQUEUES		32
+
 struct netcp_device;
 
 struct netcp_tx_pipe {
@@ -88,19 +90,27 @@ struct netcp_stats {
 	u32                     tx_dropped;
 };
 
+struct netcp_pool_info {
+	u32			num_descs;
+	u32			region_id;
+	unsigned int		pause_threshold;
+	unsigned int		resume_threshold;
+	void			*pool;
+};
+
 struct netcp_intf {
 	struct device		*dev;
 	struct device		*ndev_dev;
 	struct net_device	*ndev;
 	bool			big_endian;
 	bool			bridged;
+
+	u32			tx_subqueues;
 	unsigned int		tx_compl_qid;
-	void			*tx_pool;
-	struct list_head	txhook_list_head;
-	unsigned int		tx_pause_threshold;
+	u32			tx_compl_budget;
 	void			*tx_compl_q;
+	struct list_head	txhook_list_head;
 
-	unsigned int		tx_resume_threshold;
 	void			*rx_queue;
 	void			*rx_pool;
 	struct list_head	rxhook_list_head;
@@ -122,8 +132,6 @@ struct netcp_intf {
 	const char		*dma_chan_name;
 	u32			rx_pool_size;
 	u32			rx_pool_region_id;
-	u32			tx_pool_size;
-	u32			tx_pool_region_id;
 	struct list_head	module_head;
 	struct list_head	interface_list;
 	struct list_head	addr_list;
@@ -137,8 +145,12 @@ struct netcp_intf {
 
 	/* DMA configuration data */
 	u32			msg_enable;
+
+	struct netcp_pool_info	tx_pool[1];
+	/* NB: tx-pools (#tx-subqueues) are allocated dynamically */
 };
 
+#define	NETCP_EPIB_LEN			KNAV_DMA_NUM_EPIB_WORDS
 #define	NETCP_PSDATA_LEN		KNAV_DMA_NUM_PS_WORDS
 struct netcp_packet {
 	struct sk_buff		*skb;
diff --git a/drivers/net/ethernet/ti/netcp_core.c b/drivers/net/ethernet/ti/netcp_core.c
index 3f153a8..7d66e97 100644
--- a/drivers/net/ethernet/ti/netcp_core.c
+++ b/drivers/net/ethernet/ti/netcp_core.c
@@ -65,6 +65,8 @@
 #define for_each_module(netcp, intf_modpriv)			\
 	list_for_each_entry(intf_modpriv, &netcp->module_head, intf_list)
 
+#define BITS(x) (BIT(x) - 1)
+
 /* Module management structures */
 struct netcp_device {
 	struct list_head	device_list;
@@ -520,7 +522,7 @@ static void netcp_free_rx_desc_chain(struct netcp_intf *netcp,
 				     struct knav_dma_desc *desc)
 {
 	struct knav_dma_desc *ndesc;
-	dma_addr_t dma_desc, dma_buf;
+	dma_addr_t dma_desc = 0, dma_buf;
 	unsigned int buf_len, dma_sz = sizeof(*ndesc);
 	void *buf_ptr;
 	u32 tmp;
@@ -918,6 +920,7 @@ static void netcp_rx_notify(void *arg)
 }
 
 static void netcp_free_tx_desc_chain(struct netcp_intf *netcp,
+				     void *tx_pool,
 				     struct knav_dma_desc *desc,
 				     unsigned int desc_sz)
 {
@@ -935,10 +938,10 @@ static void netcp_free_tx_desc_chain(struct netcp_intf *netcp,
 			dev_warn(netcp->ndev_dev, "bad Tx desc buf(%pad), len(%d)\n",
 				 &dma_buf, buf_len);
 
-		knav_pool_desc_put(netcp->tx_pool, ndesc);
+		knav_pool_desc_put(tx_pool, ndesc);
 		ndesc = NULL;
 		if (dma_desc) {
-			ndesc = knav_pool_desc_unmap(netcp->tx_pool, dma_desc,
+			ndesc = knav_pool_desc_unmap(tx_pool, dma_desc,
 						     desc_sz);
 			if (!ndesc)
 				dev_err(netcp->ndev_dev, "failed to unmap Tx desc\n");
@@ -951,9 +954,11 @@ static int netcp_process_tx_compl_packets(struct netcp_intf *netcp,
 {
 	struct netcp_stats *tx_stats = &netcp->stats;
 	struct knav_dma_desc *desc;
+	struct netcp_pool_info *pi;
 	struct netcp_tx_cb *tx_cb;
 	struct sk_buff *skb;
 	unsigned int dma_sz;
+	unsigned desc_sz;
 	dma_addr_t dma;
 	int pkts = 0;
 
@@ -961,19 +966,17 @@ static int netcp_process_tx_compl_packets(struct netcp_intf *netcp,
 		dma = knav_queue_pop(netcp->tx_compl_q, &dma_sz);
 		if (!dma)
 			break;
-		desc = knav_pool_desc_unmap(netcp->tx_pool, dma, dma_sz);
+		desc = knav_desc_dma_to_virt(dma, &desc_sz);
 		if (unlikely(!desc)) {
 			dev_err(netcp->ndev_dev, "failed to unmap Tx desc\n");
 			tx_stats->tx_errors++;
 			continue;
 		}
 
-		/* warning!!!! We are retrieving the virtual ptr in the sw_data
-		 * field as a 32bit value. Will not work on 64bit machines
-		 */
-		skb = (struct sk_buff *)GET_SW_DATA0(desc);
-		knav_dma_get_pad_info((u32 *)&skb, &tmp, desc);
-		netcp_free_tx_desc_chain(netcp, desc, dma_sz);
+		knav_dma_get_pad_info((u32 *)&skb, (u32 *)&pi, desc);
+		knav_dma_desc_unmap(dma, dma_sz, desc, desc_sz, pi->pool);
+
+		netcp_free_tx_desc_chain(netcp, pi->pool, desc, dma_sz);
 		if (!skb) {
 			dev_err(netcp->ndev_dev, "No skb in Tx desc\n");
 			tx_stats->tx_errors++;
@@ -986,8 +989,7 @@ static int netcp_process_tx_compl_packets(struct netcp_intf *netcp,
 
 		if (netif_subqueue_stopped(netcp->ndev, skb) &&
 		    netif_running(netcp->ndev) &&
-		    (knav_pool_count(netcp->tx_pool) >
-		    netcp->tx_resume_threshold)) {
+		    (knav_pool_count(pi->pool) > pi->resume_threshold)) {
 			u16 subqueue = skb_get_queue_mapping(skb);
 
 			netif_wake_subqueue(netcp->ndev, subqueue);
@@ -1027,7 +1029,8 @@ static void netcp_tx_notify(void *arg)
 }
 
 static struct knav_dma_desc*
-netcp_tx_map_skb(struct sk_buff *skb, struct netcp_intf *netcp)
+netcp_tx_map_skb(struct sk_buff *skb, struct netcp_intf *netcp,
+		 struct netcp_pool_info *pi)
 {
 	struct knav_dma_desc *desc, *ndesc, *pdesc;
 	unsigned int pkt_len = skb_headlen(skb);
@@ -1043,7 +1046,7 @@ netcp_tx_map_skb(struct sk_buff *skb, struct netcp_intf *netcp)
 		return NULL;
 	}
 
-	desc = knav_pool_desc_get(netcp->tx_pool);
+	desc = knav_pool_desc_get(pi->pool);
 	if (IS_ERR_OR_NULL(desc)) {
 		dev_err(netcp->ndev_dev, "out of TX desc\n");
 		dma_unmap_single(dev, dma_addr, pkt_len, DMA_TO_DEVICE);
@@ -1077,14 +1080,14 @@ netcp_tx_map_skb(struct sk_buff *skb, struct netcp_intf *netcp)
 			goto free_descs;
 		}
 
-		ndesc = knav_pool_desc_get(netcp->tx_pool);
+		ndesc = knav_pool_desc_get(pi->pool);
 		if (IS_ERR_OR_NULL(ndesc)) {
 			dev_err(netcp->ndev_dev, "out of TX desc for frags\n");
 			dma_unmap_page(dev, dma_addr, buf_len, DMA_TO_DEVICE);
 			goto free_descs;
 		}
 
-		desc_dma = knav_pool_desc_virt_to_dma(netcp->tx_pool, ndesc);
+		desc_dma = knav_pool_desc_virt_to_dma(pi->pool, (void *)ndesc);
 		pkt_info =
 			(netcp->tx_compl_qid & KNAV_DMA_DESC_RETQ_MASK) <<
 				KNAV_DMA_DESC_RETQ_SHIFT;
@@ -1093,12 +1096,12 @@ netcp_tx_map_skb(struct sk_buff *skb, struct netcp_intf *netcp)
 		knav_dma_set_words(&desc_dma_32, 1, &pdesc->next_desc);
 		pkt_len += buf_len;
 		if (pdesc != desc)
-			knav_pool_desc_map(netcp->tx_pool, pdesc,
+			knav_pool_desc_map(pi->pool, pdesc,
 					   sizeof(*pdesc), &desc_dma, &dma_sz);
 		pdesc = ndesc;
 	}
 	if (pdesc != desc)
-		knav_pool_desc_map(netcp->tx_pool, pdesc, sizeof(*pdesc),
+		knav_pool_desc_map(pi->pool, pdesc, sizeof(*pdesc),
 				   &dma_addr, &dma_sz);
 
 	/* frag list based linkage is not supported for now. */
@@ -1115,13 +1118,14 @@ upd_pkt_len:
 	return desc;
 
 free_descs:
-	netcp_free_tx_desc_chain(netcp, desc, sizeof(*desc));
+	netcp_free_tx_desc_chain(netcp, pi->pool, desc, sizeof(*desc));
 	return NULL;
 }
 
 static int netcp_tx_submit_skb(struct netcp_intf *netcp,
 			       struct sk_buff *skb,
-			       struct knav_dma_desc *desc)
+			       struct knav_dma_desc *desc,
+			       struct netcp_pool_info *pi)
 {
 	struct netcp_tx_pipe *tx_pipe = NULL;
 	struct netcp_hook_list *tx_hook;
@@ -1164,12 +1168,12 @@ static int netcp_tx_submit_skb(struct netcp_intf *netcp,
 
 	/* update descriptor */
 	if (p_info.psdata_len) {
-		/* psdata points to both native-endian and device-endian data */
-		__le32 *psdata = (void __force *)p_info.psdata;
+		u32 *psdata = &desc->psdata[0];
 
 		knav_dma_set_words(
 			psdata + (KNAV_DMA_NUM_PS_WORDS - p_info.psdata_len),
 			p_info.psdata_len, psdata);
+
 		tmp |= (p_info.psdata_len & KNAV_DMA_DESC_PSLEN_MASK) <<
 			KNAV_DMA_DESC_PSLEN_SHIFT;
 	}
@@ -1189,6 +1193,7 @@ static int netcp_tx_submit_skb(struct netcp_intf *netcp,
 	 */
 	SET_SW_DATA0((u32)skb, desc);
 	knav_dma_set_words((u32 *)&skb, 1, &desc->pad[0]);
+	knav_dma_set_words((u32 *)&pi, 1, &desc->pad[1]);
 
 	if (tx_pipe->flags & SWITCH_TO_PORT_IN_TAGINFO) {
 		tmp = tx_pipe->switch_to_port;
@@ -1196,27 +1201,27 @@ static int netcp_tx_submit_skb(struct netcp_intf *netcp,
 	}
 
 	/* submit packet descriptor */
-	ret = knav_pool_desc_map(netcp->tx_pool, desc, sizeof(*desc), &dma,
-				 &dma_sz);
+	ret = knav_pool_desc_map(pi->pool, desc, sizeof(*desc), &dma, &dma_sz);
 	if (unlikely(ret)) {
 		dev_err(netcp->ndev_dev, "%s() failed to map desc\n", __func__);
 		ret = -ENOMEM;
 		goto out;
 	}
 	skb_tx_timestamp(skb);
-	knav_queue_push(tx_pipe->dma_queue, dma, dma_sz, 0);
+	knav_queue_push(tx_pipe->dma_queue, dma, dma_sz,
+			desc->desc_info & BITS(17));
 
 out:
 	return ret;
 }
 
-/* Submit the packet */
 static int netcp_ndo_start_xmit(struct sk_buff *skb, struct net_device *ndev)
 {
 	struct netcp_intf *netcp = netdev_priv(ndev);
 	struct netcp_stats *tx_stats = &netcp->stats;
-	int subqueue = skb_get_queue_mapping(skb);
-	struct knav_dma_desc *desc;
+	u32 subqueue = skb_get_queue_mapping(skb);
+	struct knav_dma_desc *desc = NULL;
+	struct netcp_pool_info *pi;
 	int desc_count, ret = 0;
 
 	if (unlikely(skb->len <= 0)) {
@@ -1236,14 +1241,22 @@ static int netcp_ndo_start_xmit(struct sk_buff *skb, struct net_device *ndev)
 		skb->len = NETCP_MIN_PACKET_SIZE;
 	}
 
-	desc = netcp_tx_map_skb(skb, netcp);
+	pi = &netcp->tx_pool[subqueue];
+	if (subqueue >= netcp->tx_subqueues || !pi->pool) {
+		dev_warn(netcp->ndev_dev,
+			 "no tx pool for queue mapping %d\n", subqueue);
+		ret = -ENXIO;
+		goto drop;
+	}
+
+	desc = netcp_tx_map_skb(skb, netcp, pi);
 	if (unlikely(!desc)) {
 		netif_stop_subqueue(ndev, subqueue);
 		ret = -ENOBUFS;
 		goto drop;
 	}
 
-	ret = netcp_tx_submit_skb(netcp, skb, desc);
+	ret = netcp_tx_submit_skb(netcp, skb, desc, pi);
 	if (ret) {
 		ret = (ret < 0) ? ret : NETDEV_TX_OK;
 		goto drop;
@@ -1252,8 +1265,8 @@ static int netcp_ndo_start_xmit(struct sk_buff *skb, struct net_device *ndev)
 	netif_trans_update(ndev);
 
 	/* Check Tx pool count & stop subqueue if needed */
-	desc_count = knav_pool_count(netcp->tx_pool);
-	if (desc_count < netcp->tx_pause_threshold) {
+	desc_count = knav_pool_count(pi->pool);
+	if (desc_count < pi->pause_threshold) {
 		dev_dbg(netcp->ndev_dev, "pausing tx, count(%d)\n", desc_count);
 		netif_stop_subqueue(ndev, subqueue);
 	}
@@ -1263,7 +1276,7 @@ drop:
 	if (ret != NETDEV_TX_OK)
 		tx_stats->tx_dropped++;
 	if (desc)
-		netcp_free_tx_desc_chain(netcp, desc, sizeof(*desc));
+		netcp_free_tx_desc_chain(netcp, pi->pool, desc, sizeof(*desc));
 	dev_kfree_skb(skb);
 	return ret;
 }
@@ -1527,9 +1540,11 @@ static void netcp_free_navigator_resources(struct netcp_intf *netcp)
 		netcp->tx_compl_q = NULL;
 	}
 
-	if (!IS_ERR_OR_NULL(netcp->tx_pool)) {
-		knav_pool_destroy(netcp->tx_pool);
-		netcp->tx_pool = NULL;
+	for (i = 0; i < netcp->tx_subqueues; i++) {
+		if (!IS_ERR_OR_NULL(netcp->tx_pool[i].pool)) {
+			knav_pool_destroy(netcp->tx_pool[i].pool);
+			netcp->tx_pool[i].pool = NULL;
+		}
 	}
 }
 
@@ -1553,13 +1568,18 @@ static int netcp_setup_navigator_resources(struct net_device *ndev)
 		goto fail;
 	}
 
-	snprintf(name, sizeof(name), "tx-pool-%s", ndev->name);
-	netcp->tx_pool = knav_pool_create(name, netcp->tx_pool_size,
-						netcp->tx_pool_region_id);
-	if (IS_ERR_OR_NULL(netcp->tx_pool)) {
-		dev_err(netcp->ndev_dev, "Couldn't create tx pool\n");
-		ret = PTR_ERR(netcp->tx_pool);
-		goto fail;
+	for (i = 0; i < netcp->tx_subqueues; i++) {
+		struct netcp_pool_info *pi = &netcp->tx_pool[i];
+
+		snprintf(name, sizeof(name), "tx-pool-%d-%s", i, ndev->name);
+		pi->pool = knav_pool_create(name, pi->num_descs, pi->region_id);
+
+		if (IS_ERR_OR_NULL(pi->pool)) {
+			dev_err(netcp->ndev_dev,
+				"Couldn't create %s tx pool %d\n", name, i);
+			ret = PTR_ERR(pi->pool);
+			goto fail;
+		}
 	}
 
 	/* open Tx completion queue */
@@ -1723,11 +1743,7 @@ static int netcp_ndo_stop(struct net_device *ndev)
 	netcp_empty_rx_queue(netcp);
 
 	/* Recycle Tx descriptors from completion queue */
-	netcp_process_tx_compl_packets(netcp, netcp->tx_pool_size);
-
-	if (knav_pool_count(netcp->tx_pool) != netcp->tx_pool_size)
-		dev_err(netcp->ndev_dev, "Lost (%d) Tx descs\n",
-			netcp->tx_pool_size - knav_pool_count(netcp->tx_pool));
+	netcp_process_tx_compl_packets(netcp, netcp->tx_compl_budget);
 
 	netcp_free_navigator_resources(netcp);
 	dev_dbg(netcp->ndev_dev, "netcp device %s stopped\n", ndev->name);
@@ -1781,11 +1797,11 @@ static int netcp_ndo_change_mtu(struct net_device *ndev, int new_mtu)
 static void netcp_ndo_tx_timeout(struct net_device *ndev)
 {
 	struct netcp_intf *netcp = netdev_priv(ndev);
-	unsigned int descs = knav_pool_count(netcp->tx_pool);
 
 	dev_err(netcp->ndev_dev, "transmit timed out tx descs(%d)\n", descs);
-	netcp_process_tx_compl_packets(netcp, netcp->tx_pool_size);
+	netcp_process_tx_compl_packets(netcp, netcp->tx_compl_budget);
 	netif_trans_update(ndev);
+
 	netif_tx_wake_all_queues(ndev);
 }
 
@@ -1940,10 +1956,44 @@ static int netcp_create_interface(struct netcp_device *netcp_device,
 	u32 efuse_mac = 0;
 	const void *mac_addr;
 	u8 efuse_mac_addr[6];
-	u32 temp[2];
-	int ret = 0;
+	int ret = 0, tx_subqueues = 0, if_sz, i;
+	u32 temp[NETCP_MAX_SUBQUEUES * 4], rx_temp[2];
+	bool legacy = false;
+
+	if (!of_get_property(node_interface, "tx-pools", &tx_subqueues)) {
+		/* for backward compatibile */
+		legacy = true;
+		tx_subqueues = 1;
+		ret = of_property_read_u32_array(node_interface, "tx-pool",
+						 temp, 2);
+		if (ret < 0) {
+			dev_err(dev, "missing \"tx-pool\" parameter\n");
+			return -ENODEV;
+		}
+	} else {
+		tx_subqueues /= (4 * sizeof(u32));
+		if (tx_subqueues < 1) {
+			dev_err(dev, "error in \"tx-pools\" parameter\n");
+			return -ENODEV;
+		} else if (tx_subqueues > NETCP_MAX_SUBQUEUES) {
+			dev_err(dev,
+				"requesting too many tx-pools(%d), max %d\n",
+				tx_subqueues, NETCP_MAX_SUBQUEUES);
+			return -ENODEV;
+		}
+
+		ret = of_property_read_u32_array(node_interface, "tx-pools",
+						 temp, 4 * tx_subqueues);
+		if (ret < 0) {
+			dev_err(dev, "error \"tx-pools\" parameter\n");
+			return -ENODEV;
+		}
+	}
 
-	ndev = alloc_etherdev_mqs(sizeof(*netcp), 1, 1);
+	if_sz = sizeof(*netcp) +
+			(tx_subqueues - 1) * sizeof(struct netcp_pool_info);
+
+	ndev = alloc_etherdev_mqs(if_sz, tx_subqueues, 1);
 	if (!ndev) {
 		dev_err(dev, "Error allocating netdev\n");
 		return -ENOMEM;
@@ -1967,8 +2017,6 @@ static int netcp_create_interface(struct netcp_device *netcp_device,
 	netcp->ndev = ndev;
 	netcp->ndev_dev  = &ndev->dev;
 	netcp->msg_enable = netif_msg_init(netcp_debug_level, NETCP_DEBUG);
-	netcp->tx_pause_threshold = MAX_SKB_FRAGS;
-	netcp->tx_resume_threshold = netcp->tx_pause_threshold;
 	netcp->node_interface = node_interface;
 
 	ret = of_property_read_u32(node_interface, "efuse-mac", &efuse_mac);
@@ -2034,30 +2082,43 @@ static int netcp_create_interface(struct netcp_device *netcp_device,
 		netcp->rx_queue_depths[0] = 128;
 	}
 
-	ret = of_property_read_u32_array(node_interface, "rx-pool", temp, 2);
+	ret = of_property_read_u32_array(node_interface, "rx-pool", rx_temp, 2);
 	if (ret < 0) {
 		dev_err(dev, "missing \"rx-pool\" parameter\n");
 		ret = -ENODEV;
 		goto quit;
 	}
-	netcp->rx_pool_size = temp[0];
-	netcp->rx_pool_region_id = temp[1];
+	netcp->rx_pool_size = rx_temp[0];
+	netcp->rx_pool_region_id = rx_temp[1];
 
-	ret = of_property_read_u32_array(node_interface, "tx-pool", temp, 2);
-	if (ret < 0) {
-		dev_err(dev, "missing \"tx-pool\" parameter\n");
-		ret = -ENODEV;
-		goto quit;
-	}
-	netcp->tx_pool_size = temp[0];
-	netcp->tx_pool_region_id = temp[1];
+	for (i = 0; i < tx_subqueues; i++) {
+		struct netcp_pool_info *pi = &netcp->tx_pool[i];
 
-	if (netcp->tx_pool_size < MAX_SKB_FRAGS) {
-		dev_err(dev, "tx-pool size too small, must be atleast(%ld)\n",
-			MAX_SKB_FRAGS);
-		ret = -ENODEV;
-		goto quit;
+		if (legacy && (i > 0))
+			break;
+
+		if (temp[4 * i] < MAX_SKB_FRAGS) {
+			dev_err(dev,
+				"tx-pool size %u too small, must be at least %ld\n",
+				temp[4 * i], MAX_SKB_FRAGS);
+			ret = -ENODEV;
+			goto quit;
+		}
+
+		pi->num_descs = temp[4 * i];
+		pi->region_id = temp[4 * i + 1];
+
+		if (!legacy) {
+			pi->pause_threshold = temp[4 * i + 2];
+			pi->resume_threshold = temp[4 * i + 3];
+		} else {
+			pi->pause_threshold = MAX_SKB_FRAGS;
+			pi->resume_threshold =  MAX_SKB_FRAGS;
+		}
+
+		netcp->tx_compl_budget += pi->num_descs;
 	}
+	netcp->tx_subqueues = tx_subqueues;
 
 	ret = of_property_read_u32(node_interface, "tx-completion-queue",
 				   &netcp->tx_compl_qid);
-- 
1.7.5.4

