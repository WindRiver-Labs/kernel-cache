From 3a3301588f856fea69f467ca246a86d459825583 Mon Sep 17 00:00:00 2001
From: Zhipeng Gong <zhipeng.gong@intel.com>
Date: Fri, 6 Nov 2015 00:30:23 -0500
Subject: [PATCH 12/14] drm/i915: clean up semaphore object when seqno wraps

The seqno in semaphore obj is not cleaned up when the seqno wraps,
as a result, the waiter request could passed immediately even if
the signaller request is not finished.

Here is an example.
RCS request with seqno FFFFFFFF is waiting for VECS request with FFFFFFFE,
and then seqno is wrapped  to 1, at this point the VECS signal seqno in
semaphore_obj keeps the value FFFFFFFE.
Next, the request with seqno 2 is submitted in VECS and the request
with seqno 3 is submitted in RCS, which needs to wait for seqno 2 in VECS.
However, since the VECS signal seqno FFFFFFFE is bigger than 2,
the RCS seqno 3 runs immediately. As a result, the RCS request with seqno 3
is completed before the VECS request with seqno 2 is completed.

[Original patch was taken from Intel MediaSDK]

Signed-off-by: Zhipeng Gong <zhipeng.gong@intel.com>
Signed-off-by: Pengyu Ma <pengyu.ma@windriver.com>
---
 drivers/gpu/drm/i915/i915_gem.c            |   20 ++++++++++++++++++++
 drivers/gpu/drm/i915/i915_gem_execbuffer.c |    4 ++++
 drivers/gpu/drm/i915/intel_ringbuffer.c    |    2 +-
 drivers/gpu/drm/i915/intel_ringbuffer.h    |    1 +
 4 files changed, 26 insertions(+), 1 deletions(-)

diff --git a/drivers/gpu/drm/i915/i915_gem.c b/drivers/gpu/drm/i915/i915_gem.c
index 8f42379..b0d5377 100644
--- a/drivers/gpu/drm/i915/i915_gem.c
+++ b/drivers/gpu/drm/i915/i915_gem.c
@@ -2307,6 +2307,25 @@ int i915_gem_set_seqno(struct drm_device *dev, u32 seqno)
 	return 0;
 }
 
+void cleanup_semaphore_obj(struct drm_device *dev)
+{
+	struct page *page;
+	size_t size;
+	void *vaddr;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	if (!dev_priv->semaphore_obj)
+		return;
+
+	size = I915_NUM_RINGS * I915_NUM_RINGS * i915_semaphore_seqno_size;
+	page = i915_gem_object_get_page(dev_priv->semaphore_obj, 0);
+
+	vaddr = kmap_atomic(page);
+	memset(vaddr, 0, size);
+	kunmap_atomic(vaddr);
+	i915_gem_clflush_object(dev_priv->semaphore_obj, true);
+}
+
 int
 i915_gem_get_seqno(struct drm_device *dev, u32 *seqno)
 {
@@ -2318,6 +2337,7 @@ i915_gem_get_seqno(struct drm_device *dev, u32 *seqno)
 		if (ret)
 			return ret;
 
+		cleanup_semaphore_obj(dev);
 		dev_priv->next_seqno = 1;
 	}
 
diff --git a/drivers/gpu/drm/i915/i915_gem_execbuffer.c b/drivers/gpu/drm/i915/i915_gem_execbuffer.c
index a1e59c1..0db0457 100644
--- a/drivers/gpu/drm/i915/i915_gem_execbuffer.c
+++ b/drivers/gpu/drm/i915/i915_gem_execbuffer.c
@@ -1383,6 +1383,10 @@ i915_gem_do_execbuffer(struct drm_device *dev, void *data,
 		 */
 	}
 
+	ret = intel_ring_alloc_seqno(ring);
+	if (ret)
+		goto err;
+
 	/* snb/ivb/vlv conflate the "batch in ppgtt" bit with the "non-secure
 	 * batch" bit. Hence we need to pin secure batches into the global gtt.
 	 * hsw should have this fixed, but bdw mucks it up again. */
diff --git a/drivers/gpu/drm/i915/intel_ringbuffer.c b/drivers/gpu/drm/i915/intel_ringbuffer.c
index ae17e77..ff5886d 100644
--- a/drivers/gpu/drm/i915/intel_ringbuffer.c
+++ b/drivers/gpu/drm/i915/intel_ringbuffer.c
@@ -1989,7 +1989,7 @@ int intel_ring_idle(struct intel_engine_cs *ring)
 	return i915_wait_seqno(ring, seqno);
 }
 
-static int
+int
 intel_ring_alloc_seqno(struct intel_engine_cs *ring)
 {
 	if (ring->outstanding_lazy_seqno)
diff --git a/drivers/gpu/drm/i915/intel_ringbuffer.h b/drivers/gpu/drm/i915/intel_ringbuffer.h
index 96479c8..cbeb933 100644
--- a/drivers/gpu/drm/i915/intel_ringbuffer.h
+++ b/drivers/gpu/drm/i915/intel_ringbuffer.h
@@ -409,6 +409,7 @@ void __intel_ring_advance(struct intel_engine_cs *ring);
 
 int __must_check intel_ring_idle(struct intel_engine_cs *ring);
 void intel_ring_init_seqno(struct intel_engine_cs *ring, u32 seqno);
+int intel_ring_alloc_seqno(struct intel_engine_cs *ring);
 int intel_ring_flush_all_caches(struct intel_engine_cs *ring);
 int intel_ring_invalidate_all_caches(struct intel_engine_cs *ring);
 
-- 
1.7.5.4

