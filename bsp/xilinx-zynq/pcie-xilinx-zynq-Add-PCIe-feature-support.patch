From df6dde60067b501095f17935de2db809a0443da8 Mon Sep 17 00:00:00 2001
From: Zhong Hongbo <hongbo.zhong@windriver.com>
Date: Fri, 22 Aug 2014 06:49:13 +0000
Subject: [PATCH 499/509] pcie: xilinx-zynq: Add PCIe feature support

Source: http://zedboard.org/support/design/2056/17
zynq_mini_itx_7z045_pcie_design_v2013_4_v1.zip

Signed-off-by: Zhong Hongbo <hongbo.zhong@windriver.com>
---
 arch/arm/mach-zynq/Kconfig                         |    6 +
 arch/arm/mach-zynq/Makefile                        |    1 +
 arch/arm/mach-zynq/k7_base_driver/Makefile         |    5 +
 .../k7_base_driver/include/xbasic_types.h          |  321 +++
 arch/arm/mach-zynq/k7_base_driver/include/xdebug.h |  151 ++
 .../mach-zynq/k7_base_driver/include/xdma_user.h   |  356 +++
 arch/arm/mach-zynq/k7_base_driver/include/xio.h    |  260 ++
 .../mach-zynq/k7_base_driver/include/xpmon_be.h    |  309 +++
 .../mach-zynq/k7_base_driver/include/xraw_init.h   |   54 +
 .../arm/mach-zynq/k7_base_driver/include/xstatus.h |  425 ++++
 arch/arm/mach-zynq/k7_base_driver/xdma/Makefile    |    6 +
 arch/arm/mach-zynq/k7_base_driver/xdma/xdma.c      |  215 ++
 arch/arm/mach-zynq/k7_base_driver/xdma/xdma.h      |  525 ++++
 arch/arm/mach-zynq/k7_base_driver/xdma/xdma_base.c | 2657 ++++++++++++++++++++
 arch/arm/mach-zynq/k7_base_driver/xdma/xdma_bd.h   |  536 ++++
 .../mach-zynq/k7_base_driver/xdma/xdma_bdring.c    | 1113 ++++++++
 .../mach-zynq/k7_base_driver/xdma/xdma_bdring.h    |  317 +++
 arch/arm/mach-zynq/k7_base_driver/xdma/xdma_hw.h   |  464 ++++
 arch/arm/mach-zynq/k7_base_driver/xdma/xdma_user.c |  542 ++++
 .../mach-zynq/k7_base_driver/xrawdata0/Makefile    |    6 +
 .../mach-zynq/k7_base_driver/xrawdata0/sguser.c    | 1257 +++++++++
 arch/arm/mm/Makefile                               |    2 +-
 arch/arm/mm/gup.c                                  |  161 ++
 23 files changed, 9688 insertions(+), 1 deletions(-)
 create mode 100644 arch/arm/mach-zynq/k7_base_driver/Makefile
 create mode 100644 arch/arm/mach-zynq/k7_base_driver/include/xbasic_types.h
 create mode 100644 arch/arm/mach-zynq/k7_base_driver/include/xdebug.h
 create mode 100644 arch/arm/mach-zynq/k7_base_driver/include/xdma_user.h
 create mode 100644 arch/arm/mach-zynq/k7_base_driver/include/xio.h
 create mode 100644 arch/arm/mach-zynq/k7_base_driver/include/xpmon_be.h
 create mode 100644 arch/arm/mach-zynq/k7_base_driver/include/xraw_init.h
 create mode 100644 arch/arm/mach-zynq/k7_base_driver/include/xstatus.h
 create mode 100644 arch/arm/mach-zynq/k7_base_driver/xdma/Makefile
 create mode 100644 arch/arm/mach-zynq/k7_base_driver/xdma/xdma.c
 create mode 100644 arch/arm/mach-zynq/k7_base_driver/xdma/xdma.h
 create mode 100644 arch/arm/mach-zynq/k7_base_driver/xdma/xdma_base.c
 create mode 100644 arch/arm/mach-zynq/k7_base_driver/xdma/xdma_bd.h
 create mode 100644 arch/arm/mach-zynq/k7_base_driver/xdma/xdma_bdring.c
 create mode 100644 arch/arm/mach-zynq/k7_base_driver/xdma/xdma_bdring.h
 create mode 100644 arch/arm/mach-zynq/k7_base_driver/xdma/xdma_hw.h
 create mode 100644 arch/arm/mach-zynq/k7_base_driver/xdma/xdma_user.c
 create mode 100644 arch/arm/mach-zynq/k7_base_driver/xrawdata0/Makefile
 create mode 100644 arch/arm/mach-zynq/k7_base_driver/xrawdata0/sguser.c
 create mode 100644 arch/arm/mm/gup.c

diff --git a/arch/arm/mach-zynq/Kconfig b/arch/arm/mach-zynq/Kconfig
index 75b123a..ca0d67b 100644
--- a/arch/arm/mach-zynq/Kconfig
+++ b/arch/arm/mach-zynq/Kconfig
@@ -44,6 +44,12 @@ config XILINX_AXIPCIE
 	  Host Bridge. This supports Message Signal Interrupts (MSI), if you
 	  want to use this feature select CONFIG_PCI_MSI from 'Bus Support ->'.
 
+config XILINX_NWL_PCIE_DMA_BASE
+	bool "Xilinx NWL DMA PCIe base driver"
+	depends on PCI && XILINX_AXIPCIE
+	help
+	  This builds PCIe end point base driver for Xilinx Kintex7 board.
+
 endmenu
 
 endif
diff --git a/arch/arm/mach-zynq/Makefile b/arch/arm/mach-zynq/Makefile
index 30ae83d..3bbac0a 100644
--- a/arch/arm/mach-zynq/Makefile
+++ b/arch/arm/mach-zynq/Makefile
@@ -12,3 +12,4 @@ KBUILD_AFLAGS = $(subst -march=armv6k,,$(ORIG_AFLAGS))
 AFLAGS_suspend.o 		+=-Wa,-march=armv7-a -mcpu=cortex-a9
 obj-$(CONFIG_SUSPEND)		+= pm.o suspend.o
 obj-$(CONFIG_XILINX_AXIPCIE)    += xaxipcie.o
+obj-$(CONFIG_XILINX_NWL_PCIE_DMA_BASE)		+= k7_base_driver/
diff --git a/arch/arm/mach-zynq/k7_base_driver/Makefile b/arch/arm/mach-zynq/k7_base_driver/Makefile
new file mode 100644
index 0000000..10d90bb
--- /dev/null
+++ b/arch/arm/mach-zynq/k7_base_driver/Makefile
@@ -0,0 +1,5 @@
+#
+# Makefile for the Xilinx kintex7 PCIe based DMA driver.
+#
+
+obj-y += xdma/ xrawdata0/ 
diff --git a/arch/arm/mach-zynq/k7_base_driver/include/xbasic_types.h b/arch/arm/mach-zynq/k7_base_driver/include/xbasic_types.h
new file mode 100644
index 0000000..c5416e8
--- /dev/null
+++ b/arch/arm/mach-zynq/k7_base_driver/include/xbasic_types.h
@@ -0,0 +1,321 @@
+/*******************************************************************************
+** © Copyright 2011 - 2012 Xilinx, Inc. All rights reserved.
+** This file contains confidential and proprietary information of Xilinx, Inc. and
+** is protected under U.S. and international copyright and other intellectual property laws.
+*******************************************************************************
+**   ____  ____
+**  /   /\/   /
+** /___/  \  /   Vendor: Xilinx
+** \   \   \/
+**  \   \
+**  /   /
+** \   \  /  \   Kintex-7 PCIe-DMA-DDR3 Base Targeted Reference Design
+**  \___\/\___\
+**
+**  Device: xc7k325t
+**  Reference: UG882
+*******************************************************************************
+**
+**  Disclaimer:
+**
+**    This disclaimer is not a license and does not grant any rights to the materials
+**    distributed herewith. Except as otherwise provided in a valid license issued to you
+**    by Xilinx, and to the maximum extent permitted by applicable law:
+**    (1) THESE MATERIALS ARE MADE AVAILABLE "AS IS" AND WITH ALL FAULTS,
+**    AND XILINX HEREBY DISCLAIMS ALL WARRANTIES AND CONDITIONS, EXPRESS, IMPLIED, OR STATUTORY,
+**    INCLUDING BUT NOT LIMITED TO WARRANTIES OF MERCHANTABILITY, NON-INFRINGEMENT, OR
+**    FITNESS FOR ANY PARTICULAR PURPOSE; and (2) Xilinx shall not be liable (whether in contract
+**    or tort, including negligence, or under any other theory of liability) for any loss or damage
+**    of any kind or nature related to, arising under or in connection with these materials,
+**    including for any direct, or any indirect, special, incidental, or consequential loss
+**    or damage (including loss of data, profits, goodwill, or any type of loss or damage suffered
+**    as a result of any action brought by a third party) even if such damage or loss was
+**    reasonably foreseeable or Xilinx had been advised of the possibility of the same.
+
+
+**  Critical Applications:
+**
+**    Xilinx products are not designed or intended to be fail-safe, or for use in any application
+**    requiring fail-safe performance, such as life-support or safety devices or systems,
+**    Class III medical devices, nuclear facilities, applications related to the deployment of airbags,
+**    or any other applications that could lead to death, personal injury, or severe property or
+**    environmental damage (individually and collectively, "Critical Applications"). Customer assumes
+**    the sole risk and liability of any use of Xilinx products in Critical Applications, subject only
+**    to applicable laws and regulations governing limitations on product liability.
+
+**  THIS COPYRIGHT NOTICE AND DISCLAIMER MUST BE RETAINED AS PART OF THIS FILE AT ALL TIMES.
+
+*******************************************************************************/
+/*****************************************************************************/
+/**
+*
+* @file xbasic_types.h
+*
+* This file contains basic types for Xilinx software IP.  These types do not
+* follow the standard naming convention with respect to using the component
+* name in front of each name because they are considered to be primitives.
+*
+* @note
+*
+* This file contains items which are architecture dependent.
+*
+* <pre>
+* MODIFICATION HISTORY:
+*
+* Ver   Who    Date   Changes
+* ----- ---- -------- -------------------------------------------------------
+* 1.00a rmm  12/14/01 First release
+*       rmm  05/09/03 Added "xassert always" macros to rid ourselves of diab
+*                     compiler warnings
+* 1.00a rpm  11/07/03 Added XNullHandler function as a stub interrupt handler
+* 1.00a rpm  07/21/04 Added XExceptionHandler typedef for processor exceptions
+* 1.00a xd   11/03/04 Improved support for doxygen.
+* 1.00a wre  01/25/07 Added Linux style data types u32, u16, u8, TRUE, FALSE
+* 1.00a rpm  04/02/07 Added ifndef KERNEL around u32, u16, u8 data types
+* </pre>
+*
+******************************************************************************/
+
+#ifndef XBASIC_TYPES_H  /* prevent circular inclusions */
+#define XBASIC_TYPES_H  /* by using protection macros */
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+
+/***************************** Include Files *********************************/
+
+
+/************************** Constant Definitions *****************************/
+
+#ifndef TRUE
+#  define TRUE    1
+#endif
+
+#ifndef FALSE
+#  define FALSE   0
+#endif
+
+#ifndef NULL
+#define NULL    0
+#endif
+
+#define XNULL   NULL    /**< Legacy return type. Deprecated. */
+#define XTRUE   TRUE    /**< Legacy return type. Deprecated. */
+#define XFALSE    FALSE   /**< Legacy return type. Deprecated. */
+
+#define XCOMPONENT_IS_READY     0x11111111  /**< Component has been initialized */
+#define XCOMPONENT_IS_STARTED   0x22222222  /**< Component has been started */
+
+/* The following constants and declarations are for unit test purposes and are
+ * designed to be used in test applications.
+ */
+#define XTEST_PASSED    0
+#define XTEST_FAILED    1
+
+#define XASSERT_NONE     0
+#define XASSERT_OCCURRED 1
+
+extern unsigned int XAssertStatus;
+extern void XAssert(char *, int);
+
+/**************************** Type Definitions *******************************/
+
+/** @name Legacy types
+ * Deprecated legacy types.
+ * @{
+ */
+typedef unsigned char Xuint8;   /**< unsigned 8-bit */
+typedef char        Xint8;    /**< signed 8-bit */
+typedef unsigned short  Xuint16;  /**< unsigned 16-bit */
+typedef short       Xint16;   /**< signed 16-bit */
+typedef unsigned long Xuint32;  /**< unsigned 32-bit */
+typedef long        Xint32;   /**< signed 32-bit */
+typedef float       Xfloat32; /**< 32-bit floating point */
+typedef double        Xfloat64; /**< 64-bit double precision FP */
+typedef unsigned long Xboolean; /**< boolean (XTRUE or XFALSE) */
+
+typedef struct
+{
+  Xuint32 Upper;
+  Xuint32 Lower;
+} Xuint64;
+
+/** @name New types
+ * New simple types.
+ * @{
+ */
+#ifndef __KERNEL__
+typedef Xuint32         u32;
+typedef Xuint16         u16;
+typedef Xuint8          u8;
+#else
+#include <linux/types.h>
+#endif
+
+/*@}*/
+
+/**
+ * This data type defines an interrupt handler for a device.
+ * The argument points to the instance of the component
+ */
+typedef void (*XInterruptHandler) (void *InstancePtr);
+
+/**
+ * This data type defines an exception handler for a processor.
+ * The argument points to the instance of the component
+ */
+typedef void (*XExceptionHandler) (void *InstancePtr);
+
+/**
+ * This data type defines a callback to be invoked when an
+ * assert occurs. The callback is invoked only when asserts are enabled
+ */
+typedef void (*XAssertCallback) (char *FilenamePtr, int LineNumber);
+
+/***************** Macros (Inline Functions) Definitions *********************/
+
+/*****************************************************************************/
+/**
+* Return the most significant half of the 64 bit data type.
+*
+* @param    x is the 64 bit word.
+*
+* @return   The upper 32 bits of the 64 bit word.
+*
+* @note     None.
+*
+******************************************************************************/
+#define XUINT64_MSW(x) ((x).Upper)
+
+/*****************************************************************************/
+/**
+* Return the least significant half of the 64 bit data type.
+*
+* @param    x is the 64 bit word.
+*
+* @return   The lower 32 bits of the 64 bit word.
+*
+* @note     None.
+*
+******************************************************************************/
+#define XUINT64_LSW(x) ((x).Lower)
+
+
+#ifndef NDEBUG
+
+/*****************************************************************************/
+/**
+* This assert macro is to be used for functions that do not return anything
+* (void). This in conjunction with the XWaitInAssert boolean can be used to
+* accomodate tests so that asserts which fail allow execution to continue.
+*
+* @param    expression is the expression to evaluate. If it evaluates to
+*           false, the assert occurs.
+*
+* @return   Returns void unless the XWaitInAssert variable is true, in which
+*           case no return is made and an infinite loop is entered.
+*
+* @note     None.
+*
+******************************************************************************/
+#define XASSERT_VOID(expression)                   \
+{                                                  \
+    if (expression)                                \
+    {                                              \
+        XAssertStatus = XASSERT_NONE;              \
+    }                                              \
+    else                                           \
+    {                                              \
+        XAssert(__FILE__, __LINE__);               \
+                XAssertStatus = XASSERT_OCCURRED;  \
+        return;                                    \
+    }                                              \
+}
+
+/*****************************************************************************/
+/**
+* This assert macro is to be used for functions that do return a value. This in
+* conjunction with the XWaitInAssert boolean can be used to accomodate tests so
+* that asserts which fail allow execution to continue.
+*
+* @param    expression is the expression to evaluate. If it evaluates to false,
+*           the assert occurs.
+*
+* @return   Returns 0 unless the XWaitInAssert variable is true, in which case
+*           no return is made and an infinite loop is entered.
+*
+* @note     None.
+*
+******************************************************************************/
+#define XASSERT_NONVOID(expression)                \
+{                                                  \
+    if (expression)                                \
+    {                                              \
+        XAssertStatus = XASSERT_NONE;              \
+    }                                              \
+    else                                           \
+    {                                              \
+        XAssert(__FILE__, __LINE__);               \
+                XAssertStatus = XASSERT_OCCURRED;  \
+        return 0;                                  \
+    }                                              \
+}
+
+/*****************************************************************************/
+/**
+* Always assert. This assert macro is to be used for functions that do not
+* return anything (void). Use for instances where an assert should always
+* occur.
+*
+* @return Returns void unless the XWaitInAssert variable is true, in which case
+*         no return is made and an infinite loop is entered.
+*
+* @note   None.
+*
+******************************************************************************/
+#define XASSERT_VOID_ALWAYS()                      \
+{                                                  \
+   XAssert(__FILE__, __LINE__);                    \
+           XAssertStatus = XASSERT_OCCURRED;       \
+   return;                                         \
+}
+
+/*****************************************************************************/
+/**
+* Always assert. This assert macro is to be used for functions that do return
+* a value. Use for instances where an assert should always occur.
+*
+* @return Returns void unless the XWaitInAssert variable is true, in which case
+*         no return is made and an infinite loop is entered.
+*
+* @note   None.
+*
+******************************************************************************/
+#define XASSERT_NONVOID_ALWAYS()                   \
+{                                                  \
+   XAssert(__FILE__, __LINE__);                    \
+           XAssertStatus = XASSERT_OCCURRED;       \
+   return 0;                                       \
+}
+
+
+#else
+
+#define XASSERT_VOID(expression)
+#define XASSERT_VOID_ALWAYS()
+#define XASSERT_NONVOID(expression)
+#define XASSERT_NONVOID_ALWAYS()
+#endif
+
+/************************** Function Prototypes ******************************/
+
+void XAssertSetCallback(XAssertCallback Routine);
+void XNullHandler(void *NullParameter);
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif  /* end of protection macro */
diff --git a/arch/arm/mach-zynq/k7_base_driver/include/xdebug.h b/arch/arm/mach-zynq/k7_base_driver/include/xdebug.h
new file mode 100644
index 0000000..5ba1505
--- /dev/null
+++ b/arch/arm/mach-zynq/k7_base_driver/include/xdebug.h
@@ -0,0 +1,151 @@
+/*******************************************************************************
+** © Copyright 2011 - 2012 Xilinx, Inc. All rights reserved.
+** This file contains confidential and proprietary information of Xilinx, Inc. and
+** is protected under U.S. and international copyright and other intellectual property laws.
+*******************************************************************************
+**   ____  ____
+**  /   /\/   /
+** /___/  \  /   Vendor: Xilinx
+** \   \   \/
+**  \   \
+**  /   /
+** \   \  /  \   Kintex-7 PCIe-DMA-DDR3 Base Targeted Reference Design
+**  \___\/\___\
+**
+**  Device: xc7k325t
+**  Reference: UG882
+*******************************************************************************
+**
+**  Disclaimer:
+**
+**    This disclaimer is not a license and does not grant any rights to the materials
+**    distributed herewith. Except as otherwise provided in a valid license issued to you
+**    by Xilinx, and to the maximum extent permitted by applicable law:
+**    (1) THESE MATERIALS ARE MADE AVAILABLE "AS IS" AND WITH ALL FAULTS,
+**    AND XILINX HEREBY DISCLAIMS ALL WARRANTIES AND CONDITIONS, EXPRESS, IMPLIED, OR STATUTORY,
+**    INCLUDING BUT NOT LIMITED TO WARRANTIES OF MERCHANTABILITY, NON-INFRINGEMENT, OR
+**    FITNESS FOR ANY PARTICULAR PURPOSE; and (2) Xilinx shall not be liable (whether in contract
+**    or tort, including negligence, or under any other theory of liability) for any loss or damage
+**    of any kind or nature related to, arising under or in connection with these materials,
+**    including for any direct, or any indirect, special, incidental, or consequential loss
+**    or damage (including loss of data, profits, goodwill, or any type of loss or damage suffered
+**    as a result of any action brought by a third party) even if such damage or loss was
+**    reasonably foreseeable or Xilinx had been advised of the possibility of the same.
+
+
+**  Critical Applications:
+**
+**    Xilinx products are not designed or intended to be fail-safe, or for use in any application
+**    requiring fail-safe performance, such as life-support or safety devices or systems,
+**    Class III medical devices, nuclear facilities, applications related to the deployment of airbags,
+**    or any other applications that could lead to death, personal injury, or severe property or
+**    environmental damage (individually and collectively, "Critical Applications"). Customer assumes
+**    the sole risk and liability of any use of Xilinx products in Critical Applications, subject only
+**    to applicable laws and regulations governing limitations on product liability.
+
+**  THIS COPYRIGHT NOTICE AND DISCLAIMER MUST BE RETAINED AS PART OF THIS FILE AT ALL TIMES.
+
+*******************************************************************************/
+/*****************************************************************************/
+/**
+*
+* @file xdebug.h
+*
+* This file contains logging tools for Xilinx software IP.
+*
+* @note
+*
+* This file contains items which are architecture dependent.
+*
+* <pre>
+* MODIFICATION HISTORY:
+*
+* Ver   Who    Date   Changes
+* ----- ---- -------- -------------------------------------------------------
+* </pre>
+*
+******************************************************************************/
+
+/***************************** Include Files *********************************/
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+
+/************************** Constant Definitions *****************************/
+
+#ifndef XDEBUG  /* prevent circular inclusions */
+#define XDEBUG  /* by using protection macros */
+
+//#define MYDEBUG
+#if defined(MYDEBUG) && !defined(NDEBUG)
+
+#ifndef XDEBUG_WARNING
+#define XDEBUG_WARNING
+//#warning DEBUG is enabled
+#endif
+
+#define XDBG_DEBUG_ERROR             0x00000001 /**< error condition messages */
+#define XDBG_DEBUG_GENERAL           0x00000002 /**< general debug  messages */
+#define XDBG_DEBUG_ALL               0xFFFFFFFF /**< all debugging data */
+
+#define XDBG_DEBUG_FIFO_REG          0x00000100 /**< display register reads/writes */
+#define XDBG_DEBUG_FIFO_RX           0x00000101 /**< receive debug messages */
+#define XDBG_DEBUG_FIFO_TX           0x00000102 /**< transmit debug messages */
+#define XDBG_DEBUG_FIFO_ALL          0x0000010F /**< all fifo debug messages */
+
+#define XDBG_DEBUG_TEMAC_REG         0x00000400 /**< display register reads/writes */
+#define XDBG_DEBUG_TEMAC_RX          0x00000401 /**< receive debug messages */
+#define XDBG_DEBUG_TEMAC_TX          0x00000402 /**< transmit debug messages */
+#define XDBG_DEBUG_TEMAC_ALL         0x0000040F /**< all temac  debug messages */
+
+#define XDBG_DEBUG_TEMAC_ADPT_RX     0x00000800 /**< receive debug messages */
+#define XDBG_DEBUG_TEMAC_ADPT_TX     0x00000801 /**< transmit debug messages */
+#define XDBG_DEBUG_TEMAC_ADPT_IOCTL  0x00000802 /**< ioctl debug messages */
+#define XDBG_DEBUG_TEMAC_ADPT_MISC   0x00000803 /**< debug msg for other routines */
+#define XDBG_DEBUG_TEMAC_ADPT_ALL    0x0000080F /**< all temac adapter debug messages */
+
+#define xdbg_current_types (XDBG_DEBUG_ERROR | XDBG_DEBUG_GENERAL | XDBG_DEBUG_TEMAC_REG)
+
+#define xdbg_stmnt(x)  x
+#define xdbg_printf(type, ...) (((type) & xdbg_current_types) ? printk (__VA_ARGS__) : 0)
+
+#else
+#define xdbg_stmnt(x)
+#define xdbg_printf(...)
+#endif
+
+
+/**************************** Type Definitions *******************************/
+
+
+/***************** Macros (Inline Functions) Definitions *********************/
+
+/* Macros for handling normal and verbose logging in adapter and DMA code */
+
+#ifdef DEBUG_VERBOSE /* Enable both normal and verbose logging */
+
+#define log_verbose(args...)    printk(args)
+#define log_normal(args...)     printk(args)
+
+#elif defined DEBUG_NORMAL /* Enable only normal logging */
+
+#define log_verbose(x...)
+#define log_normal(args...)     printk(args)
+
+#else
+
+#define log_normal(x...)
+#define log_verbose(x...)
+
+#endif /* DEBUG_VERBOSE and DEBUG_NORMAL */
+
+
+/************************** Function Prototypes ******************************/
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* XDEBUG */ /* end of protection macro */
diff --git a/arch/arm/mach-zynq/k7_base_driver/include/xdma_user.h b/arch/arm/mach-zynq/k7_base_driver/include/xdma_user.h
new file mode 100644
index 0000000..8898cf3
--- /dev/null
+++ b/arch/arm/mach-zynq/k7_base_driver/include/xdma_user.h
@@ -0,0 +1,356 @@
+/*******************************************************************************
+** © Copyright 2011 - 2012 Xilinx, Inc. All rights reserved.
+** This file contains confidential and proprietary information of Xilinx, Inc. and
+** is protected under U.S. and international copyright and other intellectual property laws.
+*******************************************************************************
+**   ____  ____
+**  /   /\/   /
+** /___/  \  /   Vendor: Xilinx
+** \   \   \/
+**  \   \
+**  /   /
+** \   \  /  \   Kintex-7 PCIe-DMA-DDR3 Base Targeted Reference Design
+**  \___\/\___\
+**
+**  Device: xc7k325t
+**  Reference: UG882
+*******************************************************************************
+**
+**  Disclaimer:
+**
+**    This disclaimer is not a license and does not grant any rights to the materials
+**    distributed herewith. Except as otherwise provided in a valid license issued to you
+**    by Xilinx, and to the maximum extent permitted by applicable law:
+**    (1) THESE MATERIALS ARE MADE AVAILABLE "AS IS" AND WITH ALL FAULTS,
+**    AND XILINX HEREBY DISCLAIMS ALL WARRANTIES AND CONDITIONS, EXPRESS, IMPLIED, OR STATUTORY,
+**    INCLUDING BUT NOT LIMITED TO WARRANTIES OF MERCHANTABILITY, NON-INFRINGEMENT, OR
+**    FITNESS FOR ANY PARTICULAR PURPOSE; and (2) Xilinx shall not be liable (whether in contract
+**    or tort, including negligence, or under any other theory of liability) for any loss or damage
+**    of any kind or nature related to, arising under or in connection with these materials,
+**    including for any direct, or any indirect, special, incidental, or consequential loss
+**    or damage (including loss of data, profits, goodwill, or any type of loss or damage suffered
+**    as a result of any action brought by a third party) even if such damage or loss was
+**    reasonably foreseeable or Xilinx had been advised of the possibility of the same.
+
+
+**  Critical Applications:
+**
+**    Xilinx products are not designed or intended to be fail-safe, or for use in any application
+**    requiring fail-safe performance, such as life-support or safety devices or systems,
+**    Class III medical devices, nuclear facilities, applications related to the deployment of airbags,
+**    or any other applications that could lead to death, personal injury, or severe property or
+**    environmental damage (individually and collectively, "Critical Applications"). Customer assumes
+**    the sole risk and liability of any use of Xilinx products in Critical Applications, subject only
+**    to applicable laws and regulations governing limitations on product liability.
+
+**  THIS COPYRIGHT NOTICE AND DISCLAIMER MUST BE RETAINED AS PART OF THIS FILE AT ALL TIMES.
+
+*******************************************************************************/
+/*****************************************************************************/
+/**
+*
+* @file xdma_user.h
+*
+* This file contains the data required for the interface between the
+* base DMA driver (xdma) and the application-specific drivers, for example,
+* the Raw Data0 driver (xrawdata0)  and Raw Data1 driver (xrawdata1).
+*
+* This interface has been architected in order to make it possible to
+* easily substitute the application-specific drivers that come with this
+* TRD with other application-specific drivers, without re-writing the
+* common DMA-handling functionality.
+*
+* Some xdma functions are called directly by the application-specific driver
+* and some are callbacks registered by the application-specific driver,
+* and are called by xdma.
+*
+* <pre>
+* Application-Specific Driver                               xdma Driver
+*                               Driver Registration
+*                 ---------------------------------------------->
+*                             Driver De-registration
+*                 ---------------------------------------------->
+*                           Transmit Block/Packet Data
+*                 ---------------------------------------------->
+*                            Complete Initialization
+*                <----------------------------------------------
+*                            Complete Driver Removal
+*                <----------------------------------------------
+*                            Complete Interrupt Tasks
+*                <----------------------------------------------
+*                             Get Packet Buffer for RX
+*                <----------------------------------------------
+*                               Return Packet Buffer
+*                <----------------------------------------------
+*                                    Set State
+*                <----------------------------------------------
+*                                    Get State
+*                <----------------------------------------------
+* </pre>
+* <b> Driver Registration/De-registration </b>
+*
+* The application-specific drivers are dependent on the DMA driver and
+* can be inserted as Linux kernel modules only after xdma has been loaded.
+*
+* To register itself with xdma, the application-specific driver does the
+* following, while specifying the engine number and BAR number it requires,
+* a set of function callback pointers, and the minimum packet size it
+* normally uses -
+* <pre> Handle = DmaRegister(int Engine, int Bar, UserPtrs * uptr, int PktSize); </pre>
+* The application-specific driver requires to know the kernel logical
+* address of the desired BAR in order to do any device-specific
+* initializations that may be required. For example, the xgbeth driver
+* requires to program the TEMAC and PHY registers, while the xaui driver
+* requires to control the test configuration.
+*
+* Before returning to the caller, DmaRegister() will invoke the function
+* callback registered to complete the initialization process, while
+* specifying the BAR's logical address, and a private data pointer that
+* was passed to it during registration. privData can be used by the user
+* driver to differentiate between multiple DmaRegister() invocations -
+* <pre> (uptr->UserInit)(BARbase, privData); </pre>
+* <b><i> Note: The application-specific driver's UserInit() function call must
+* be written in such a way that it works fine even when the DmaRegister()
+* call is not yet complete. </i></b>
+*
+* If the registration process is successful, a handle is returned which
+* should be used in all other function calls to xdma.
+*
+* To unregister itself from xdma, the application-specific driver does the
+* following, while passing the handle it received after registration -
+* <pre> DmaUnregister(Handle); </pre>
+* Before returning to the caller, DmaUnregister() will invoke the function
+* callback registered to enable any device-specific programming to be done
+* as part of the de-registration process.
+* <pre> (uptr->UserRemove)(Handle, privData); </pre>
+* Before returning to the caller, DmaUnregister() also returns all the
+* packet buffers that had been passed to it for transmission or reception.
+* Incase these buffers are still unused, they are flagged as PKT_UNUSED.
+* <pre> (uptr->UserPutPkt)(Handle, PktBuf * pkts, int NumPkts, privData); </pre>
+* <b><i> Note: UserRemove() is not being invoked in xdma v1.00, and will be
+* added in the future. </i></b>
+*
+* <b> Buffer Handling </b>
+*
+* For better performance, the DMA driver always sets up large buffer
+* descriptor (BD) rings, one for transmission and one for reception. The
+* TX BD ring will be consumed only when there is data for transmission.
+* The RX BD ring, on the other hand, will be entirely submitted for DMA
+* in order to maximize performance.
+*
+* <b> Data Transmission </b>
+*
+* When the application-specific driver wants to transmit a data block or
+* packet, it invokes the following, while passing an array of one or more
+* packets to be transmitted -
+* <pre> DmaSendPkt(Handle, PktBuf * pkts, int NumPkts); </pre>
+* When data transmission is completed, the packet buffers will be returned
+* to the application-specific driver by invoking the following -
+* <pre> (uptr->UserPutPkt)(Handle, PktBuf * pkts, int NumPkts, privData); </pre>
+* It is important to return these buffers to the application-specific
+* driver even though they are TX packets, since they would need to be
+* freed or re-used as the case may be.
+*
+* <b> Data Reception </b>
+*
+* The DMA driver needs to set up the complete BD ring with buffers ready
+* for reception. Hence, it invokes the following, while specifying the number
+* of packet buffers it wants, and their size -
+* <pre> (uptr->UserGetPkt)(Handle, PktBuf * pkts, uint Size, int NumPkts, privData); </pre>
+* As data reception happens and buffers get used, they are returned to the
+* application-specific driver, by invoking the following -
+* <pre> (uptr->UserPutPkt)(Handle, PktBuf * pkts, int NumPkts, privData); </pre>
+* The DMA driver will again invoke UserGetPkt() in order to replenish the
+* used BDs in its BD ring.
+*
+* <b> Interrupts </b>
+*
+* The DMA driver can operate in either a polled mode or interrupt-driven
+* mode, by modifying a compile-time flag. The application-specific driver
+* can optionally enable application-specific interrupts. When an interrupt
+* occurs, the DMA driver will invoke the following, to enable handling of
+* these interrupt events -
+* <pre> (uptr->UserIntr)(Handle, privData); </pre>
+* <b><i> Note: This callback is not being invoked by xdma v1.00, and will be
+* added in the future. </i></b>
+*
+* <b> Configuration, Status and Statistics </b>
+*
+* The Xilinx Performance Monitor GUI (xpmon) can be used to initiate a
+* data transfer test and measure DMA payload and PCIe link performance.
+* xdma invokes the following, while specifying whether loopback is enabled
+* or not, and the minimum/maximum bounds of packet sizes -
+* <pre> (uptr->UserSetState)(Handle, UserState * ustate, privData); </pre>
+* The same callback is used to start a test, and to stop a test.
+*
+* xdma invokes the following, in order to get current state of the
+* application-specific driver -
+* <pre> (uptr->UserGetState)(Handle, UserState * ustate, privData); </pre>
+* Information regarding current test state, link state, total number of
+* buffers is returned and can be displayed in the xpmon GUI.
+*
+* <pre>
+* MODIFICATION HISTORY:
+*
+* Ver   Who  Date     Changes
+* ----- ---- -------- -------------------------------------------------------
+* 1.00  ps   12/07/09 First version
+* </pre>
+*
+******************************************************************************/
+
+#ifndef XDMA_USER_H   /* prevent circular inclusions */
+#define XDMA_USER_H   /* by using protection macros */
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/***************************** Include Files *********************************/
+
+#include <xpmon_be.h>
+#include <linux/delay.h>
+
+/************************** Constant Definitions *****************************/
+
+/** @name Driver and engine states.
+ *  @{
+ */
+#define UNINITIALIZED       0           /**< State at system start */
+#define INITIALIZED         1           /**< After probe */
+#define USER_ASSIGNED       2           /**< Engine assigned to user */
+#define UNREGISTERING       3           /**< In the process of unregistering */
+/*@}*/
+
+/** @name Packet information set/read by the user drivers.
+ *  These flags match with the status reported by DMA. Additional flags
+ *  should be assigned from available bits.
+ *  @{
+ */
+#define PKT_SOP             0x80000000  /**< Buffer is the start-of-packet */
+#define PKT_EOP             0x40000000  /**< Buffer is the end-of-packet */
+#define PKT_ERROR           0x10000000  /**< Error while processing buffer */
+//#define PKT_USER            0x00008000  /**< User information is included */
+#define PKT_UNUSED          0x00004000  /**< Buffer is being returned unused */
+#define PKT_ALL             0x00800000  /**< All fragments must be sent */
+/*@}*/
+
+
+#define FIFO_EMPTY_CHECK
+//#undef FIFO_EMPTY_CHECK
+
+#ifdef FIFO_EMPTY_CHECK
+
+#define STABILITY_WAIT_TIME   5   // 5 miliseconds
+#define FIFO_EMPTY_TIMEOUT    100   // 100 milisecons
+
+#define STATUS_REG_OFFSET 0x9008
+#define DIR_TYPE_S2C    0   // maybe redundant
+#define DIR_TYPE_C2S    1   // may be redundant
+
+#define EMPTY_MASK_SHIFT  2
+#define AXI_MIG_RST_SHIFT 1
+
+#define HANDLE_0      0 // xraw0
+#define HANDLE_1      2 // xraw1
+
+//#define handle_id should be done in sguser.c [ id 0 for xraw0 else id 2]
+
+#endif
+
+
+/**************************** Type Definitions *******************************/
+
+/** Packet information passed between DMA and application-specific
+ *  drivers while transmitting and receiving data.
+ *  A PktBuf array can be used to pass multiple packets between user
+ *  and DMA drivers. It includes the following information -
+ *  - pktBuf is the virtual address of a packet buffer
+ *  - bufInfo is the per-packet identifier that the user driver may need to
+ *  associate with the packet. When the packet is returned from the
+ *  DMA driver to the user driver, this association remains intact and can
+ *  be retrieved by the user driver.
+ *  - The size of the packet buffer.
+ *  - When the user submits a packet for DMA, the flags can be a combination
+ *    of PKT_SOP, PKT_EOP and PKT_ALL. PKT_ALL indicates to the DMA driver
+ *    that all of the packets in the submitted array must be queued for DMA.
+ *    This will usually be done when the queued packets are fragments of a
+ *    larger user packet.
+ *  - When the DMA driver returns a packet to the user driver, the flags can
+ *    be a combination of PKT_SOP, PKT_EOP, PKT_ERROR and PKT_UNUSED.
+ *    PKT_UNUSED indicates that the packet buffer is being returned unused
+ *    and does not contain valid data. This is usually done when the drivers
+ *    are being unloaded and unregistered, and therefore, packets have been
+ *    retrieved without completing DMA.
+ */
+typedef struct {
+    unsigned char * pktBuf;     /**< Virtual Address of packet buffer */
+    unsigned char * bufInfo;    /**< Per-packet identifier */
+    unsigned int size;          /**< Size of packet buffer */
+    unsigned int flags;         /**< Flags associated with packet */
+    unsigned long long userInfo;/**< User info associated with packet */
+} PktBuf;
+
+/** User State Information passed between DMA and application-specific
+ *  drivers while changing configuration, and reading state/statistics.
+ *  - LinkState can be LINK_UP or LINK_DOWN
+ *  - When DMA driver sets the test state in the user driver, TestMode can
+ *    be a combination of TEST_STOP, TEST_START and ENABLE_LOOPBACK.
+ *  - When DMA driver gets the test state from the user driver, TestMode can
+ *    be a combination of TEST_IN_PROGRESS and ENABLE_LOOPBACK.
+ */
+typedef struct {
+    int LinkState;              /**< Link State */
+    int Errors;                 /**< Count of errors */
+    int Buffers;                /**< Count of buffers */
+    int MinPktSize;             /**< Min Packet Size */
+    int MaxPktSize;             /**< Max Packet Size */
+    unsigned int TestMode;      /**< Test Mode */
+} UserState;
+
+/** User instance function callbacks. Not all callbacks need to be
+ *  implemented.
+ */
+typedef struct {
+    unsigned int privData;  /**< User-specified private data */
+    int (* UserInit)(unsigned int barbase, unsigned int privdata);
+                        /**< User instance register completion callback */
+    int (* UserRemove)(void * handle, unsigned int privdata);
+            /**< User instance de-register completion callback - not
+              * available in v1.00
+              */
+    int (* UserIntr)(void * handle, unsigned int privdata);
+            /**< User instance interrupt callback - not available in v1.00 */
+    int (* UserGetPkt)(void * handle, PktBuf * vaddr, unsigned int size, int numpkts, unsigned int privdata);
+                        /**< User instance callback - get buffers for RX */
+    int (* UserPutPkt)(void * handle, PktBuf * vaddr, int numpkts, unsigned int privdata);
+                        /**< User instance callback - put RX/TX buffers */
+    int (* UserSetState)(void * handle, UserState * ustate, unsigned int privdata);
+                        /**< User instance callback - set state */
+    int (* UserGetState)(void * handle, UserState * ustate, unsigned int privdata);
+                        /**< User instance callback - get state */
+} UserPtrs;
+
+/***************** Macros (Inline Functions) Definitions *********************/
+
+
+/************************** Function Prototypes ******************************/
+
+/** @name Initialization and control functions in xdma.c
+ *  @{
+ */
+void * DmaRegister(int engine, int bar, UserPtrs * uptr, int pktsize);
+int DmaUnregister(void * handle);
+
+#ifdef FIFO_EMPTY_CHECK
+void DmaFifoEmptyWait(int handleId, u32 type);
+#endif
+
+int DmaSendPkt(void * handle, PktBuf * pkts, int numpkts);
+/*@}*/
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* end of protection macro */
diff --git a/arch/arm/mach-zynq/k7_base_driver/include/xio.h b/arch/arm/mach-zynq/k7_base_driver/include/xio.h
new file mode 100644
index 0000000..2b806ef
--- /dev/null
+++ b/arch/arm/mach-zynq/k7_base_driver/include/xio.h
@@ -0,0 +1,260 @@
+/*******************************************************************************
+** © Copyright 2011 - 2012 Xilinx, Inc. All rights reserved.
+** This file contains confidential and proprietary information of Xilinx, Inc. and
+** is protected under U.S. and international copyright and other intellectual property laws.
+*******************************************************************************
+**   ____  ____
+**  /   /\/   /
+** /___/  \  /   Vendor: Xilinx
+** \   \   \/
+**  \   \
+**  /   /
+** \   \  /  \   Kintex-7 PCIe-DMA-DDR3 Base Targeted Reference Design
+**  \___\/\___\
+**
+**  Device: xc7k325t
+**  Reference: UG882
+*******************************************************************************
+**
+**  Disclaimer:
+**
+**    This disclaimer is not a license and does not grant any rights to the materials
+**    distributed herewith. Except as otherwise provided in a valid license issued to you
+**    by Xilinx, and to the maximum extent permitted by applicable law:
+**    (1) THESE MATERIALS ARE MADE AVAILABLE "AS IS" AND WITH ALL FAULTS,
+**    AND XILINX HEREBY DISCLAIMS ALL WARRANTIES AND CONDITIONS, EXPRESS, IMPLIED, OR STATUTORY,
+**    INCLUDING BUT NOT LIMITED TO WARRANTIES OF MERCHANTABILITY, NON-INFRINGEMENT, OR
+**    FITNESS FOR ANY PARTICULAR PURPOSE; and (2) Xilinx shall not be liable (whether in contract
+**    or tort, including negligence, or under any other theory of liability) for any loss or damage
+**    of any kind or nature related to, arising under or in connection with these materials,
+**    including for any direct, or any indirect, special, incidental, or consequential loss
+**    or damage (including loss of data, profits, goodwill, or any type of loss or damage suffered
+**    as a result of any action brought by a third party) even if such damage or loss was
+**    reasonably foreseeable or Xilinx had been advised of the possibility of the same.
+
+
+**  Critical Applications:
+**
+**    Xilinx products are not designed or intended to be fail-safe, or for use in any application
+**    requiring fail-safe performance, such as life-support or safety devices or systems,
+**    Class III medical devices, nuclear facilities, applications related to the deployment of airbags,
+**    or any other applications that could lead to death, personal injury, or severe property or
+**    environmental damage (individually and collectively, "Critical Applications"). Customer assumes
+**    the sole risk and liability of any use of Xilinx products in Critical Applications, subject only
+**    to applicable laws and regulations governing limitations on product liability.
+
+**  THIS COPYRIGHT NOTICE AND DISCLAIMER MUST BE RETAINED AS PART OF THIS FILE AT ALL TIMES.
+
+*******************************************************************************/
+/*****************************************************************************/
+/**
+*
+* @file xio.h
+*
+* This file contains the Input/Output functions, and the changes
+* required for swapping endianness.
+*
+* @note
+*
+******************************************************************************/
+
+#ifndef XIO_H           /* prevent circular inclusions */
+#define XIO_H           /* by using protection macros */
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/***************************** Include Files *********************************/
+
+#include "xbasic_types.h"
+
+
+/************************** Constant Definitions *****************************/
+
+
+/**************************** Type Definitions *******************************/
+
+/**
+ * Typedef for an I/O address.  Typically correlates to the width of the
+ * address bus.
+ */
+typedef Xuint32 XIo_Address;
+
+/***************** Macros (Inline Functions) Definitions *********************/
+
+/* The following macro is specific to the GNU compiler. It performs an
+ * EIEIO instruction such that I/O operations are synced correctly.
+ * This macro is not necessarily portable across compilers since it uses
+ * inline assembly.
+ */
+#if defined __GNUC__
+#  define SYNCHRONIZE_IO __asm__ volatile ("eieio")
+#else
+#  define SYNCHRONIZE_IO
+#endif
+
+/* The following macros allow the software to be transportable across
+ * processors which use big or little endian memory models.
+ *
+ * Defined first are processor-specific endian conversion macros specific to
+ * the GNU compiler and the x86 family, as well as a no-op endian conversion
+ * macro. These macros are not to be used directly by software. Instead, the
+ * XIo_To/FromLittleEndianXX and XIo_To/FromBigEndianXX macros below are to be
+ * used to allow the endian conversion to only be performed when necessary.
+ */
+
+#define XIo_EndianNoop(Source, DestPtr)    (*DestPtr = Source)
+
+#if defined __GNUC__ && !defined X86_PC
+
+#define XIo_EndianSwap16(Source, DestPtr)  __asm__ __volatile__(\
+                                           "sthbrx %0,0,%1\n"\
+                                           : : "r" (Source), "r" (DestPtr)\
+                                           )
+
+#define XIo_EndianSwap32(Source, DestPtr)  __asm__ __volatile__(\
+                                           "stwbrx %0,0,%1\n"\
+                                           : : "r" (Source), "r" (DestPtr)\
+                                           )
+#else
+
+#define XIo_EndianSwap16(Source, DestPtr) \
+{\
+   Xuint16 src = (Source); \
+   Xuint16 *destptr = (DestPtr); \
+   *destptr = src >> 8; \
+   *destptr |= (src << 8); \
+}
+
+#define XIo_EndianSwap32(Source, DestPtr) \
+{\
+   unsigned int src = (Source); \
+   unsigned int *destptr = (DestPtr); \
+   *destptr = src >> 24; \
+   *destptr |= ((src >> 8)  & 0x0000FF00); \
+   *destptr |= ((src << 8)  & 0x00FF0000); \
+   *destptr |= ((src << 24) & 0xFF000000); \
+}
+
+#endif
+
+#ifdef XLITTLE_ENDIAN
+/* little-endian processor */
+
+#define XIo_ToLittleEndian16                XIo_EndianNoop
+#define XIo_ToLittleEndian32                XIo_EndianNoop
+#define XIo_FromLittleEndian16              XIo_EndianNoop
+#define XIo_FromLittleEndian32              XIo_EndianNoop
+
+#define XIo_ToBigEndian16(Source, DestPtr)  XIo_EndianSwap16(Source, DestPtr)
+#define XIo_ToBigEndian32(Source, DestPtr)  XIo_EndianSwap32(Source, DestPtr);
+#define XIo_FromBigEndian16                 XIo_ToBigEndian16
+#define XIo_FromBigEndian32(Source, DestPtr) XIo_ToBigEndian32(Source, DestPtr);
+
+#else
+/* big-endian processor */
+
+#define XIo_ToLittleEndian16(Source, DestPtr) XIo_EndianSwap16(Source, DestPtr)
+#define XIo_ToLittleEndian32(Source, DestPtr) XIo_EndianSwap32(Source, DestPtr)
+#define XIo_FromLittleEndian16                XIo_ToLittleEndian16
+#define XIo_FromLittleEndian32                XIo_ToLittleEndian32
+
+#define XIo_ToBigEndian16                     XIo_EndianNoop
+#define XIo_ToBigEndian32                     XIo_EndianNoop
+#define XIo_FromBigEndian16                   XIo_EndianNoop
+#define XIo_FromBigEndian32                   XIo_EndianNoop
+
+#endif
+
+
+/************************** Function Prototypes ******************************/
+
+/* The following functions allow the software to be transportable across
+ * processors which may use memory mapped I/O or I/O which is mapped into a
+ * seperate address space such as X86.  The functions are better suited for
+ * debugging and are therefore the default implementation. Macros can instead
+ * be used if USE_IO_MACROS is defined.
+ */
+#ifndef USE_IO_MACROS
+
+/* Functions */
+Xuint8 XIo_In8(XIo_Address InAddress);
+Xuint16 XIo_In16(XIo_Address InAddress);
+Xuint32 XIo_In32(XIo_Address InAddress);
+
+void XIo_Out8(XIo_Address OutAddress, Xuint8 Value);
+void XIo_Out16(XIo_Address OutAddress, Xuint16 Value);
+void XIo_Out32(XIo_Address OutAddress, Xuint32 Value);
+
+#else
+
+/* The following macros allow optimized I/O operations for memory mapped I/O
+ * Note that the SYNCHRONIZE_IO may be moved by the compiler during
+ * optimization.
+ */
+
+#ifdef X86_PC
+
+#include <asm/io.h>
+
+#ifdef NWLDMA
+/* NWL DMA design is little-endian, so values need not be swapped.
+ */
+#define XIo_In32(addr)      (readl((unsigned int *)(addr)))
+#define XIo_Out32(addr, data) (writel((data), (unsigned int *)(addr)))
+
+#else                // NWLDMA
+
+static inline unsigned int readbe2le(unsigned int * addr)
+{
+  unsigned int source, dest;
+  //printk("srcaddr is %p\n", (addr));
+  source = readl(addr);
+  XIo_FromBigEndian32(source, &dest);
+  //printk("Read orig 0x%x new 0x%x\n", source, dest);
+  return dest;
+}
+static inline void writele2be(unsigned int data, unsigned int * addr)
+{
+  unsigned int wdest;
+  XIo_ToBigEndian32((data), &wdest);
+  //printk("Writing to %p, orig 0x%x, new 0x%x\n", addr, data, wdest);
+  writel(wdest, (unsigned long *)(addr));
+}
+
+#define XIo_In32(addr)      (readbe2le((unsigned int *)(addr)))
+#define XIo_Out32(addr, data) (writele2be((data), (unsigned int *)(addr)))
+
+#endif          // NWLDMA
+
+#else           // X86_PC
+
+#define XIo_In8(InputPtr)  (*(volatile Xuint8  *)(InputPtr)); SYNCHRONIZE_IO;
+#define XIo_In16(InputPtr) (*(volatile Xuint16 *)(InputPtr)); SYNCHRONIZE_IO;
+#define XIo_In32(InputPtr) (*(volatile Xuint32 *)(InputPtr)); SYNCHRONIZE_IO;
+
+#define XIo_Out8(OutputPtr, Value)  \
+    { (*(volatile Xuint8  *)(OutputPtr) = Value); SYNCHRONIZE_IO; }
+#define XIo_Out16(OutputPtr, Value) \
+    { (*(volatile Xuint16 *)(OutputPtr) = Value); SYNCHRONIZE_IO; }
+#define XIo_Out32(OutputPtr, Value) \
+    { (*(volatile Xuint32 *)(OutputPtr) = Value); SYNCHRONIZE_IO; }
+
+#endif          // X86_PC
+
+#endif          // USE_IO_MACROS
+
+/* The following functions handle IO addresses where data must be swapped
+ * They cannot be implemented as macros
+ */
+Xuint16 XIo_InSwap16(XIo_Address InAddress);
+Xuint32 XIo_InSwap32(XIo_Address InAddress);
+void XIo_OutSwap16(XIo_Address OutAddress, Xuint16 Value);
+void XIo_OutSwap32(XIo_Address OutAddress, Xuint32 Value);
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif          /* end of protection macro */
diff --git a/arch/arm/mach-zynq/k7_base_driver/include/xpmon_be.h b/arch/arm/mach-zynq/k7_base_driver/include/xpmon_be.h
new file mode 100644
index 0000000..d055528
--- /dev/null
+++ b/arch/arm/mach-zynq/k7_base_driver/include/xpmon_be.h
@@ -0,0 +1,309 @@
+/*******************************************************************************
+** © Copyright 2011 - 2012 Xilinx, Inc. All rights reserved.
+** This file contains confidential and proprietary information of Xilinx, Inc. and
+** is protected under U.S. and international copyright and other intellectual property laws.
+*******************************************************************************
+**   ____  ____
+**  /   /\/   /
+** /___/  \  /   Vendor: Xilinx
+** \   \   \/
+**  \   \
+**  /   /
+** \   \  /  \   Kintex-7 PCIe-DMA-DDR3 Base Targeted Reference Design
+**  \___\/\___\
+**
+**  Device: xc7k325t
+**  Reference: UG882
+*******************************************************************************
+**
+**  Disclaimer:
+**
+**    This disclaimer is not a license and does not grant any rights to the materials
+**    distributed herewith. Except as otherwise provided in a valid license issued to you
+**    by Xilinx, and to the maximum extent permitted by applicable law:
+**    (1) THESE MATERIALS ARE MADE AVAILABLE "AS IS" AND WITH ALL FAULTS,
+**    AND XILINX HEREBY DISCLAIMS ALL WARRANTIES AND CONDITIONS, EXPRESS, IMPLIED, OR STATUTORY,
+**    INCLUDING BUT NOT LIMITED TO WARRANTIES OF MERCHANTABILITY, NON-INFRINGEMENT, OR
+**    FITNESS FOR ANY PARTICULAR PURPOSE; and (2) Xilinx shall not be liable (whether in contract
+**    or tort, including negligence, or under any other theory of liability) for any loss or damage
+**    of any kind or nature related to, arising under or in connection with these materials,
+**    including for any direct, or any indirect, special, incidental, or consequential loss
+**    or damage (including loss of data, profits, goodwill, or any type of loss or damage suffered
+**    as a result of any action brought by a third party) even if such damage or loss was
+**    reasonably foreseeable or Xilinx had been advised of the possibility of the same.
+
+
+**  Critical Applications:
+**
+**    Xilinx products are not designed or intended to be fail-safe, or for use in any application
+**    requiring fail-safe performance, such as life-support or safety devices or systems,
+**    Class III medical devices, nuclear facilities, applications related to the deployment of airbags,
+**    or any other applications that could lead to death, personal injury, or severe property or
+**    environmental damage (individually and collectively, "Critical Applications"). Customer assumes
+**    the sole risk and liability of any use of Xilinx products in Critical Applications, subject only
+**    to applicable laws and regulations governing limitations on product liability.
+
+**  THIS COPYRIGHT NOTICE AND DISCLAIMER MUST BE RETAINED AS PART OF THIS FILE AT ALL TIMES.
+
+*******************************************************************************/
+/*****************************************************************************/
+/**
+*
+* @file xpmon_be.h
+*
+* This file contains the data required for the interface between the
+* xpmon GUI and the xdma driver.
+*
+* The Xilinx Performance Monitor GUI (xpmon) contacts the DMA driver (xdma)
+* in order to start/stop a packet generation test, and also to periodically
+* read the state and statistics of the drivers, the PCIe link and the DMA
+* engine and payload.
+*
+* xpmon opens the device file <pre> /dev/xdma_stat </pre> which enables it
+* to communicate with the xdma driver.
+*
+* <b> Driver IOCTLs </b>
+*
+* After opening the device file, xpmon issues various IOCTLs in order
+* to read different kinds of information from the DMA driver, and to
+* modify the driver behaviour.
+*
+* <b> Start/Stop Test </b>
+*
+* To start a test, xpmon does the following, specifying minimum/maximum
+* packet sizes, and whether to enable loopback or not. Loopback is not enabled
+* in this TRD -
+* <pre>
+* ioctl(int fd, ISTART_TEST, TestCmd * testCmd);
+* </pre>
+* ... and to stop a test, xpmon does the following -
+* <pre>
+* ioctl(int fd, ISTOP_TEST, TestCmd * testCmd);
+* </pre>
+*
+* <b> Per-one-second IOCTLs </b>
+*
+* In order to read the DMA engine payload statistics,
+* <pre>
+* ioctl(int fd, IGET_DMA_STATISTICS, EngStatsArray * es);
+* </pre>
+*
+* In order to read the driver's software-level statistics,
+* <pre>
+* ioctl(int fd, IGET_SW_STATISTICS, SWStatsArray * ssa);
+* </pre>
+*
+* In order to read the PCIe TRN statistics,
+* <pre>
+* ioctl(int fd, IGET_TRN_STATISTICS, TRNStatsArray * tsa);
+* </pre>
+*
+* <b> Per-five-second IOCTLs </b>
+*
+* In order to read the DMA and Software status,
+* <pre>
+* ioctl(int fd, IGET_ENG_STATE, EngState * enginfo);
+* </pre>
+*
+* In order to read the PCIe link status,
+* <pre>
+* ioctl(int fd, IGET_PCI_STATE, PCIState * ps);
+* </pre>
+*
+* @note
+* <pre>
+* MODIFICATION HISTORY:
+*
+* Ver   Date     Changes
+* ----- -------- -------------------------------------------------------
+* 1.2   09/01/10 Updates to include version info in PCI state structure.
+* </pre>
+*
+*
+******************************************************************************/
+
+#ifndef XPMON_BE_H
+#define XPMON_BE_H
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/***************************** Include Files *********************************/
+
+
+/************************** Constant Definitions *****************************/
+
+/* Defining constants require us to decide upon four things
+ *   1. Type or magic number (type)
+ *   2. Sequence number which is eight bits wide. This means we can have up
+ *      to 256 IOCTL commands (nr)
+ *   3. Direction, whether we are reading or writing
+ *   4. Size of user data involved
+ *
+ *  To arrive at unique numbers easily, we use the following macros:
+ *  _IO(type, nr);
+ *  _IOW(type, nr, dataitem)
+ *  _IOR(type, nr, dataitem)
+ *  _IOWR(type, nr, dataitem)
+ */
+
+/* Selecting magic number for our ioctls */
+#define XPMON_MAGIC 'C'     /**< Magic number for use in IOCTLs */
+
+#define XPMON_MAX_CMD 8     /**< Total number of IOCTLs */
+
+/** Get the current test state from the driver */
+#define IGET_TEST_STATE     _IOR(XPMON_MAGIC, 1, TestCmd)
+
+/** Start a test in the driver */
+#define ISTART_TEST         _IOW(XPMON_MAGIC, 2, TestCmd)
+
+/** Stop a test in the driver */
+#define ISTOP_TEST          _IOW(XPMON_MAGIC, 3, TestCmd)
+
+/** Get PCIe state from the driver */
+#define IGET_PCI_STATE      _IOR(XPMON_MAGIC, 4, PCIState)
+
+/** Get DMA engine state from the driver */
+#define IGET_ENG_STATE      _IOR(XPMON_MAGIC, 5, EngState)
+
+/** Get DMA engine statistics from the driver */
+#define IGET_DMA_STATISTICS _IOR(XPMON_MAGIC, 6, EngStatsArray)
+
+/** Get PCIe TRN engine statistics from the driver */
+#define IGET_TRN_STATISTICS _IOR(XPMON_MAGIC, 7, TRNStatsArray)
+
+/** Get driver software statistics from the driver */
+#define IGET_SW_STATISTICS  _IOR(XPMON_MAGIC, 8, SWStatsArray)
+
+/* State of test - shared in TestMode flag */
+#define TEST_STOP           0x00000000  /**< Stop the test */
+#define TEST_START          0x00008000  /**< Start the test */
+#define TEST_IN_PROGRESS    0x00004000  /**< Test is in progress */
+
+#ifdef K7_TRD
+#define ENABLE_PKTCHK       0x00000100  /**< Enable TX-side packet checker */
+#define ENABLE_PKTGEN       0x00000400  /**< Enable RX-side packet generator */
+#else
+#define ENABLE_TX           0x00000100  /**< Enable loopback mode in test */
+#endif
+#define ENABLE_LOOPBACK     0x00000200  /**< Enable loopback mode in test */
+
+/* Link States */
+#define LINK_UP             1           /**< Link State is Up */
+#define LINK_DOWN           0           /**< Link State is Down */
+
+/* PCI-related states */
+#define INT_MSIX            0x3         /**< MSI-X Interrupts capability */
+#define INT_MSI             0x2         /**< MSI Interrupts capability */
+#define INT_LEGACY          0x1         /**< Legacy Interrupts capability */
+#define INT_NONE            0x0         /**< No Interrupt capability */
+#define LINK_SPEED_25       1           /**< 2.5 Gbps */
+#define LINK_SPEED_5        2           /**< 5 Gbps */
+
+/* The following initialisation should be changed in case of any changes in
+ * the hardware demo design.
+ */
+
+#define MAX_ENGS    4       /**< Max DMA engines being used in this design */
+#define MAX_TRN     2       /**< Max TRN types being used in this design */
+#define TX_MODE     0x1     /**< Incase there are screens specific to TX */
+#define RX_MODE     0x2     /**< Incase there are screens specific to RX */
+
+
+/***************** Macros (Inline Functions) Definitions *********************/
+
+
+/**************************** Type Definitions *******************************/
+
+/** Structure used in IOCTL to get PCIe state from driver */
+typedef struct {
+    unsigned int Version;       /**< Hardware design version info */
+    int LinkState;              /**< Link State - up or down */
+    int LinkSpeed;              /**< Link Speed */
+    int LinkWidth;              /**< Link Width */
+    unsigned int VendorId;      /**< Vendor ID */
+    unsigned int DeviceId;      /**< Device ID */
+    int IntMode;                /**< Legacy or MSI interrupts */
+    int MPS;                    /**< Max Payload Size */
+    int MRRS;                   /**< Max Read Request Size */
+#ifdef K7_TRD
+    int InitFCCplD;             /**< Initial FC Credits for Completion Data */
+    int InitFCCplH;             /**< Initial FC Credits for Completion Header */
+    int InitFCNPD;              /**< Initial FC Credits for Non-Posted Data */
+    int InitFCNPH;              /**< Initial FC Credits for Non-Posted Data */
+    int InitFCPD;               /**< Initial FC Credits for Posted Data */
+    int InitFCPH;               /**< Initial FC Credits for Posted Data */
+#endif
+} PCIState;
+
+/** Structure used in IOCTL to get DMA engine state from driver */
+typedef struct {
+    int Engine;                 /**< Engine Number */
+    int BDs;                    /**< Total Number of BDs */
+    int Buffers;                /**< Total Number of buffers */
+    unsigned int MinPktSize;    /**< Minimum packet size */
+    unsigned int MaxPktSize;    /**< Maximum packet size */
+    int BDerrs;                 /**< Total BD errors */
+    int BDSerrs;                /**< Total BD short errors - only TX BDs */
+    int IntEnab;                /**< Interrupts enabled or not */
+    unsigned int TestMode;      /**< Current Test Mode */
+} EngState;
+
+/** Structure used to hold DMA engine statistics */
+typedef struct {
+    int Engine;                 /**< Engine Number */
+    unsigned int LBR;           /**< Last Byte Rate */
+    unsigned int LAT;           /**< Last Active Time */
+    unsigned int LWT;           /**< Last Wait Time */
+} DMAStatistics;
+
+/** Structure used in IOCTL to get DMA engine statistics from driver */
+typedef struct {
+    int Count;                  /**< Number of statistics captures */
+    DMAStatistics * engptr;     /**< Pointer to array to store statistics */
+} EngStatsArray;
+
+/** Structure used to hold PCIe TRN statistics */
+typedef struct {
+    unsigned int LTX;           /**< Last TX Byte Rate */
+    unsigned int LRX;           /**< Last RX Byte Rate */
+} TRNStatistics;
+
+/** Structure used in IOCTL to get PCIe TRN statistics from driver */
+typedef struct {
+    int Count;                  /**< Number of statistics captures */
+    TRNStatistics * trnptr;     /**< Pointer to array to store statistics */
+} TRNStatsArray;
+
+/** Structure used to hold software statistics */
+typedef struct {
+    int Engine;                 /**< Engine Number */
+    unsigned int LBR;           /**< Last Byte Rate */
+} SWStatistics;
+
+/** Structure used in IOCTL to get software statistics from driver */
+typedef struct {
+    int Count;                  /**< Number of statistics captures */
+    SWStatistics * swptr;       /**< Pointer to array to store statistics */
+} SWStatsArray;
+
+/** Structure used in IOCTL to start/stop a test & to get current test state */
+typedef struct {
+    int Engine;                 /**< Engine Number */
+    unsigned int TestMode;      /**< Test Mode - Enable TX, Enable loopback */
+    unsigned int MinPktSize;    /**< Min packet size */
+    unsigned int MaxPktSize;    /**< Max packet size */
+} TestCmd;
+
+
+/************************** Function Prototypes ******************************/
+
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif  /* end of protection macro */
+
diff --git a/arch/arm/mach-zynq/k7_base_driver/include/xraw_init.h b/arch/arm/mach-zynq/k7_base_driver/include/xraw_init.h
new file mode 100644
index 0000000..afecdee
--- /dev/null
+++ b/arch/arm/mach-zynq/k7_base_driver/include/xraw_init.h
@@ -0,0 +1,54 @@
+/*******************************************************************************
+** Â© Copyright 2011 - 2012 Xilinx, Inc. All rights reserved.
+** This file contains confidential and proprietary information of Xilinx, Inc. and
+** is protected under U.S. and international copyright and other intellectual property laws.
+*******************************************************************************
+**   ____  ____
+**  /   /\/   /
+** /___/  \  /   Vendor: Xilinx
+** \   \   \/
+**  \   \
+**  /   /
+** \   \  /  \   Kintex-7 PCIe-DMA-DDR3 Base Targeted Reference Design
+**  \___\/\___\
+**
+**  Device: xc7k325t
+**  Reference: UG882
+*******************************************************************************
+**
+**  Disclaimer:
+**
+**    This disclaimer is not a license and does not grant any rights to the materials
+**    distributed herewith. Except as otherwise provided in a valid license issued to you
+**    by Xilinx, and to the maximum extent permitted by applicable law:
+**    (1) THESE MATERIALS ARE MADE AVAILABLE "AS IS" AND WITH ALL FAULTS,
+**    AND XILINX HEREBY DISCLAIMS ALL WARRANTIES AND CONDITIONS, EXPRESS, IMPLIED, OR STATUTORY,
+**    INCLUDING BUT NOT LIMITED TO WARRANTIES OF MERCHANTABILITY, NON-INFRINGEMENT, OR
+**    FITNESS FOR ANY PARTICULAR PURPOSE; and (2) Xilinx shall not be liable (whether in contract
+**    or tort, including negligence, or under any other theory of liability) for any loss or damage
+**    of any kind or nature related to, arising under or in connection with these materials,
+**    including for any direct, or any indirect, special, incidental, or consequential loss
+**    or damage (including loss of data, profits, goodwill, or any type of loss or damage suffered
+**    as a result of any action brought by a third party) even if such damage or loss was
+**    reasonably foreseeable or Xilinx had been advised of the possibility of the same.
+
+
+**  Critical Applications:
+**
+**    Xilinx products are not designed or intended to be fail-safe, or for use in any application
+**    requiring fail-safe performance, such as life-support or safety devices or systems,
+**    Class III medical devices, nuclear facilities, applications related to the deployment of airbags,
+**    or any other applications that could lead to death, personal injury, or severe property or
+**    environmental damage (individually and collectively, "Critical Applications"). Customer assumes
+**    the sole risk and liability of any use of Xilinx products in Critical Applications, subject only
+**    to applicable laws and regulations governing limitations on product liability.
+
+**  THIS COPYRIGHT NOTICE AND DISCLAIMER MUST BE RETAINED AS PART OF THIS FILE AT ALL TIMES.
+
+*******************************************************************************/
+#ifndef XRAW_INIT_H   /* prevent circular inclusions */
+#define XRAW_INIT_H   /* by using protection macros */
+
+void raw_transmit_init();
+
+#endif
diff --git a/arch/arm/mach-zynq/k7_base_driver/include/xstatus.h b/arch/arm/mach-zynq/k7_base_driver/include/xstatus.h
new file mode 100644
index 0000000..9aa3b61
--- /dev/null
+++ b/arch/arm/mach-zynq/k7_base_driver/include/xstatus.h
@@ -0,0 +1,425 @@
+/*******************************************************************************
+** © Copyright 2011 - 2012 Xilinx, Inc. All rights reserved.
+** This file contains confidential and proprietary information of Xilinx, Inc. and
+** is protected under U.S. and international copyright and other intellectual property laws.
+*******************************************************************************
+**   ____  ____
+**  /   /\/   /
+** /___/  \  /   Vendor: Xilinx
+** \   \   \/
+**  \   \
+**  /   /
+** \   \  /  \   Kintex-7 PCIe-DMA-DDR3 Base Targeted Reference Design
+**  \___\/\___\
+**
+**  Device: xc7k325t
+**  Reference: UG882
+*******************************************************************************
+**
+**  Disclaimer:
+**
+**    This disclaimer is not a license and does not grant any rights to the materials
+**    distributed herewith. Except as otherwise provided in a valid license issued to you
+**    by Xilinx, and to the maximum extent permitted by applicable law:
+**    (1) THESE MATERIALS ARE MADE AVAILABLE "AS IS" AND WITH ALL FAULTS,
+**    AND XILINX HEREBY DISCLAIMS ALL WARRANTIES AND CONDITIONS, EXPRESS, IMPLIED, OR STATUTORY,
+**    INCLUDING BUT NOT LIMITED TO WARRANTIES OF MERCHANTABILITY, NON-INFRINGEMENT, OR
+**    FITNESS FOR ANY PARTICULAR PURPOSE; and (2) Xilinx shall not be liable (whether in contract
+**    or tort, including negligence, or under any other theory of liability) for any loss or damage
+**    of any kind or nature related to, arising under or in connection with these materials,
+**    including for any direct, or any indirect, special, incidental, or consequential loss
+**    or damage (including loss of data, profits, goodwill, or any type of loss or damage suffered
+**    as a result of any action brought by a third party) even if such damage or loss was
+**    reasonably foreseeable or Xilinx had been advised of the possibility of the same.
+
+
+**  Critical Applications:
+**
+**    Xilinx products are not designed or intended to be fail-safe, or for use in any application
+**    requiring fail-safe performance, such as life-support or safety devices or systems,
+**    Class III medical devices, nuclear facilities, applications related to the deployment of airbags,
+**    or any other applications that could lead to death, personal injury, or severe property or
+**    environmental damage (individually and collectively, "Critical Applications"). Customer assumes
+**    the sole risk and liability of any use of Xilinx products in Critical Applications, subject only
+**    to applicable laws and regulations governing limitations on product liability.
+
+**  THIS COPYRIGHT NOTICE AND DISCLAIMER MUST BE RETAINED AS PART OF THIS FILE AT ALL TIMES.
+
+*******************************************************************************/
+/*****************************************************************************/
+/**
+*
+* @file xstatus.h
+*
+* This file contains Xilinx software status codes.  Status codes have their
+* own data type called int.  These codes are used throughout the Xilinx
+* device drivers.
+*
+******************************************************************************/
+
+#ifndef XSTATUS_H   /* prevent circular inclusions */
+#define XSTATUS_H   /* by using protection macros */
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/***************************** Include Files *********************************/
+
+#include "xbasic_types.h"
+
+/************************** Constant Definitions *****************************/
+
+/*********************** Common statuses 0 - 500 *****************************/
+
+#define XST_SUCCESS                     0L
+#define XST_FAILURE                     1L
+#define XST_DEVICE_NOT_FOUND            2L
+#define XST_DEVICE_BLOCK_NOT_FOUND      3L
+#define XST_INVALID_VERSION             4L
+#define XST_DEVICE_IS_STARTED           5L
+#define XST_DEVICE_IS_STOPPED           6L
+#define XST_FIFO_ERROR                  7L  /* an error occurred during an
+                                 * operation with a FIFO such as
+                                 * an underrun or overrun, this
+                                 * error requires the device to
+                                 * be reset */
+#define XST_RESET_ERROR                 8L  /* an error occurred which requires
+                                 * the device to be reset */
+#define XST_DMA_ERROR                   9L  /* a DMA error occurred, this error
+                                 * typically requires the device
+                                 * using the DMA to be reset */
+#define XST_NOT_POLLED                  10L /* the device is not configured for
+                                 * polled mode operation */
+#define XST_FIFO_NO_ROOM                11L /* a FIFO did not have room to put
+                                 * the specified data into */
+#define XST_BUFFER_TOO_SMALL            12L /* the buffer is not large enough
+                                 * to hold the expected data */
+#define XST_NO_DATA                     13L /* there was no data available */
+#define XST_REGISTER_ERROR              14L /* a register did not contain the
+                                 * expected value */
+#define XST_INVALID_PARAM               15L /* an invalid parameter was passed
+                                 * into the function */
+#define XST_NOT_SGDMA                   16L /* the device is not configured for
+                                 * scatter-gather DMA operation */
+#define XST_LOOPBACK_ERROR              17L /* a loopback test failed */
+#define XST_NO_CALLBACK                 18L /* a callback has not yet been
+                                 * registered */
+#define XST_NO_FEATURE                  19L /* device is not configured with
+                                 * the requested feature */
+#define XST_NOT_INTERRUPT               20L /* device is not configured for
+                                 * interrupt mode operation */
+#define XST_DEVICE_BUSY                 21L /* device is busy */
+#define XST_ERROR_COUNT_MAX             22L /* the error counters of a device
+                                 * have maxed out */
+#define XST_IS_STARTED                  23L /* used when part of device is
+                                 * already started i.e.
+                                 * sub channel */
+#define XST_IS_STOPPED                  24L /* used when part of device is
+                                 * already stopped i.e.
+                                 * sub channel */
+#define XST_DATA_LOST                   26L /* driver defined error */
+#define XST_RECV_ERROR                  27L /* generic receive error */
+#define XST_SEND_ERROR                  28L /* generic transmit error */
+#define XST_NOT_ENABLED                 29L /* a requested service is not
+                                 * available because it has not
+                                 * been enabled */
+
+/***************** Utility Component statuses 401 - 500  *********************/
+
+#define XST_MEMTEST_FAILED              401L  /* memory test failed */
+
+
+/***************** Common Components statuses 501 - 1000 *********************/
+
+/********************* Packet Fifo statuses 501 - 510 ************************/
+
+#define XST_PFIFO_LACK_OF_DATA          501L  /* not enough data in FIFO   */
+#define XST_PFIFO_NO_ROOM               502L  /* not enough room in FIFO   */
+#define XST_PFIFO_BAD_REG_VALUE         503L  /* self test, a register value
+                                     * was invalid after reset */
+#define XST_PFIFO_ERROR                 504L  /* generic packet FIFO error */
+#define XST_PFIFO_DEADLOCK              505L  /* packet FIFO is reporting
+                                     * empty and full simultaneously
+                                     */
+
+/************************** DMA statuses 511 - 530 ***************************/
+
+#define XST_DMA_TRANSFER_ERROR          511L  /* self test, DMA transfer
+                                     * failed */
+#define XST_DMA_RESET_REGISTER_ERROR    512L  /* self test, a register value
+                                     * was invalid after reset */
+#define XST_DMA_SG_LIST_EMPTY           513L  /* scatter gather list contains
+                                     * no buffer descriptors ready
+                                     * to be processed */
+#define XST_DMA_SG_IS_STARTED           514L  /* scatter gather not stopped */
+#define XST_DMA_SG_IS_STOPPED           515L  /* scatter gather not running */
+#define XST_DMA_SG_LIST_FULL            517L  /* all the buffer desciptors of
+                                     * the scatter gather list are
+                                     * being used */
+#define XST_DMA_SG_BD_LOCKED            518L  /* the scatter gather buffer
+                                     * descriptor which is to be
+                                     * copied over in the scatter
+                                     * list is locked */
+#define XST_DMA_SG_NOTHING_TO_COMMIT    519L  /* no buffer descriptors have
+                                     * been put into the scatter
+                                     * gather list to be commited */
+#define XST_DMA_SG_COUNT_EXCEEDED       521L  /* the packet count threshold
+                                     * specified was larger than the
+                                     * total # of buffer descriptors
+                                     * in the scatter gather list */
+#define XST_DMA_SG_LIST_EXISTS          522L  /* the scatter gather list has
+                                     * already been created */
+#define XST_DMA_SG_NO_LIST              523L  /* no scatter gather list has
+                                     * been created */
+#define XST_DMA_SG_BD_NOT_COMMITTED     524L  /* the buffer descriptor which
+                                     * was being started was not
+                                                 * committed to the list */
+#define XST_DMA_SG_NO_DATA              525L  /* the buffer descriptor to
+                                     * start has already been used
+                                                 * by the hardware so it can't
+                                                 * be reused
+                                     */
+#define XST_DMA_SG_LIST_ERROR           526L  /* general purpose list access
+                                     * error */
+#define XST_DMA_BD_ERROR                527L  /* general buffer descriptor
+                                     * error */
+
+/************************** IPIF statuses 531 - 550 ***************************/
+
+#define XST_IPIF_REG_WIDTH_ERROR        531L  /* an invalid register width
+                                     * was passed into the
+                                                 * function */
+#define XST_IPIF_RESET_REGISTER_ERROR   532L  /* the value of a register at
+                                     * reset was not valid */
+#define XST_IPIF_DEVICE_STATUS_ERROR    533L  /* a write to the device
+                                                 * interrupt status register
+                                                 * did not read back
+                                                 * correctly */
+#define XST_IPIF_DEVICE_ACK_ERROR       534L  /* the device interrupt status
+                                     * register did not reset when
+                                     * acked */
+#define XST_IPIF_DEVICE_ENABLE_ERROR    535L  /* the device interrupt enable
+                                     * register was not updated when
+                                     * other registers changed */
+#define XST_IPIF_IP_STATUS_ERROR        536L  /* a write to the IP interrupt
+                                     * status register did not read
+                                     * back correctly */
+#define XST_IPIF_IP_ACK_ERROR           537L  /* the IP interrupt status
+                                     * register did not reset
+                                                 * when acked */
+#define XST_IPIF_IP_ENABLE_ERROR        538L  /* IP interrupt enable register
+                                                 * was not updated correctly
+                                                 * when other registers
+                                                 * changed */
+#define XST_IPIF_DEVICE_PENDING_ERROR   539L  /* The device interrupt pending
+                                     * register did not indicate the
+                                     * expected value */
+#define XST_IPIF_DEVICE_ID_ERROR        540L  /* The device interrupt ID
+                                                 * register did not indicate
+                                                 * the expected value */
+#define XST_IPIF_ERROR                  541L  /* generic ipif error */
+
+/****************** Device specific statuses 1001 - 4095 *********************/
+
+/********************* Ethernet statuses 1001 - 1050 *************************/
+
+#define XST_EMAC_MEMORY_SIZE_ERROR  1001L /* Memory space is not big enough
+                                 * to hold the minimum number of
+                                 * buffers or descriptors */
+#define XST_EMAC_MEMORY_ALLOC_ERROR 1002L /* Memory allocation failed */
+#define XST_EMAC_MII_READ_ERROR     1003L /* MII read error */
+#define XST_EMAC_MII_BUSY           1004L /* An MII operation is in
+                                             * progress */
+#define XST_EMAC_OUT_OF_BUFFERS     1005L /* Driver is out of buffers */
+#define XST_EMAC_PARSE_ERROR        1006L /* Invalid driver init string */
+#define XST_EMAC_COLLISION_ERROR    1007L /* Excess deferral or late
+                                 * collision on polled send */
+
+/*********************** UART statuses 1051 - 1075 ***************************/
+#define XST_UART
+
+#define XST_UART_INIT_ERROR         1051L
+#define XST_UART_START_ERROR        1052L
+#define XST_UART_CONFIG_ERROR       1053L
+#define XST_UART_TEST_FAIL          1054L
+#define XST_UART_BAUD_ERROR         1055L
+#define XST_UART_BAUD_RANGE         1056L
+
+
+/************************ IIC statuses 1076 - 1100 ***************************/
+
+#define XST_IIC_SELFTEST_FAILED         1076  /* self test failed */
+#define XST_IIC_BUS_BUSY                1077  /* bus found busy */
+#define XST_IIC_GENERAL_CALL_ADDRESS    1078  /* mastersend attempted with
+                                       * general call address */
+#define XST_IIC_STAND_REG_RESET_ERROR   1079  /* A non parameterizable reg
+                                       * value after reset not valid
+                                                 */
+#define XST_IIC_TX_FIFO_REG_RESET_ERROR 1080  /* Tx fifo included in design
+                                       * value after reset not valid
+                                                 */
+#define XST_IIC_RX_FIFO_REG_RESET_ERROR 1081  /* Rx fifo included in design
+                                                 * value after reset not valid
+                                                 */
+#define XST_IIC_TBA_REG_RESET_ERROR     1082  /* 10 bit addr incl in design
+                                                 * value after reset not valid
+                                                 */
+#define XST_IIC_CR_READBACK_ERROR       1083  /* Read of the control register
+                                       * didn't return value written
+                                                 */
+#define XST_IIC_DTR_READBACK_ERROR      1084  /* Read of the data Tx reg
+                                                 * didn't return value written
+                                                 */
+#define XST_IIC_DRR_READBACK_ERROR      1085  /* Read of the data Receive reg
+                                                 * didn't return value written
+                                                 */
+#define XST_IIC_ADR_READBACK_ERROR      1086  /* Read of the data Tx reg
+                                                 * didn't return value written
+                                                 */
+#define XST_IIC_TBA_READBACK_ERROR      1087  /* Read of the 10 bit addr reg
+                                                 * didn't return written value
+                                                 */
+#define XST_IIC_NOT_SLAVE               1088  /* The device isn't a slave */
+
+/*********************** ATMC statuses 1101 - 1125 ***************************/
+
+#define XST_ATMC_ERROR_COUNT_MAX    1101L /* the error counters in the ATM
+                                 * controller hit the max value
+                                 * which requires the statistics
+                                 * to be cleared
+                                             */
+
+/*********************** Flash statuses 1126 - 1150 **************************/
+
+#define XST_FLASH_BUSY                1126L /* Flash is erasing or programming */
+#define XST_FLASH_READY               1127L /* Flash is ready for commands */
+#define XST_FLASH_ERROR               1128L /* Flash had detected an internal
+                                 * error. Use XFlash_DeviceControl
+                                 * to retrieve device specific codes
+                                 */
+#define XST_FLASH_ERASE_SUSPENDED     1129L /* Flash in suspended erase state */
+#define XST_FLASH_WRITE_SUSPENDED     1130L /* Flash in suspended write state */
+#define XST_FLASH_PART_NOT_SUPPORTED  1131L /* Flash type not supported by
+                                             * driver */
+#define XST_FLASH_NOT_SUPPORTED       1132L /* Operation not supported */
+#define XST_FLASH_TOO_MANY_REGIONS    1133L /* Too many erase regions */
+#define XST_FLASH_TIMEOUT_ERROR       1134L /* Programming or erase operation
+                                 * aborted due to a timeout */
+#define XST_FLASH_ADDRESS_ERROR       1135L /* Accessed flash outside its
+                                 * addressible range */
+#define XST_FLASH_ALIGNMENT_ERROR     1136L /* Write alignment error */
+#define XST_FLASH_BLOCKING_CALL_ERROR 1137L /* Couldn't return immediately from
+                                 * write/erase function with
+                                 * XFL_NON_BLOCKING_WRITE/ERASE
+                                 * option cleared */
+#define XST_FLASH_CFI_QUERY_ERROR     1138L /* Failed to query the device */
+
+/*********************** SPI statuses 1151 - 1175 ****************************/
+
+#define XST_SPI_MODE_FAULT          1151  /* master was selected as slave */
+#define XST_SPI_TRANSFER_DONE       1152  /* data transfer is complete */
+#define XST_SPI_TRANSMIT_UNDERRUN   1153  /* slave underruns transmit
+                                             * register
+                                             */
+#define XST_SPI_RECEIVE_OVERRUN     1154  /* device overruns receive
+                                             * register
+                                             */
+#define XST_SPI_NO_SLAVE            1155  /* no slave has been selected yet */
+#define XST_SPI_TOO_MANY_SLAVES     1156  /* more than one slave is being
+                                 * selected
+                                             */
+#define XST_SPI_NOT_MASTER          1157  /* operation is valid only as
+                                             * master
+                                             */
+#define XST_SPI_SLAVE_ONLY          1158  /* device is configured as
+                                             * slave-only
+                                 */
+#define XST_SPI_SLAVE_MODE_FAULT    1159  /* slave was selected while
+                                             * disabled
+                                             */
+
+/********************** OPB Arbiter statuses 1176 - 1200 *********************/
+
+#define XST_OPBARB_INVALID_PRIORITY  1176 /* the priority registers have
+                                 * either one master assigned to
+                                 * two or more priorities, or one
+                                             * master not assigned to any
+                                             * priority
+                                             */
+#define XST_OPBARB_NOT_SUSPENDED     1177 /* an attempt was made to modify
+                                             * the priority levels without
+                                             * first suspending the use of
+                                             * priority levels
+                                             */
+#define XST_OPBARB_PARK_NOT_ENABLED  1178 /* bus parking by id was enabled
+                                             * but bus parking was not enabled
+                                 */
+#define XST_OPBARB_NOT_FIXED_PRIORITY 1179  /* the arbiter must be in fixed
+                                 * priority mode to allow the
+                                 * priorities to be changed
+                                 */
+
+/************************ Intc statuses 1201 - 1225 **************************/
+
+#define XST_INTC_FAIL_SELFTEST      1201  /* self test failed */
+#define XST_INTC_CONNECT_ERROR      1202  /* interrupt already in use */
+
+/********************** TmrCtr statuses 1226 - 1250 **************************/
+
+#define XST_TMRCTR_TIMER_FAILED     1226  /* self test failed */
+
+/********************** WdtTb statuses 1251 - 1275 ***************************/
+
+#define XST_WDTTB_TIMER_FAILED      1251L
+
+/********************** PlbArb statuses 1276 - 1300 **************************/
+
+#define XST_PLBARB_FAIL_SELFTEST    1276L
+
+/********************** Plb2Opb statuses 1301 - 1325 *************************/
+
+#define XST_PLB2OPB_FAIL_SELFTEST   1301L
+
+/********************** Opb2Plb statuses 1326 - 1350 *************************/
+
+#define XST_OPB2PLB_FAIL_SELFTEST   1326L
+
+/********************** SysAce statuses 1351 - 1360 **************************/
+
+#define XST_SYSACE_NO_LOCK          1351L /* No MPU lock has been granted */
+
+/********************** PCI Bridge statuses 1361 - 1375 **********************/
+
+#define XST_PCI_INVALID_ADDRESS     1361L
+
+/********************** FlexRay constants 1400 - 1410 *************************/
+
+#define XST_FR_TX_ERROR                1400L
+#define XST_FR_TX_BUSY                 1401L
+#define XST_FR_BUF_LOCKED              1402L
+#define XST_FR_NO_BUF                  1403L
+
+/****************** USB constants 1410 - 1420  *******************************/
+
+#define XST_USB_ALREADY_CONFIGURED  1410
+#define XST_USB_BUF_ALIGN_ERROR   1411
+#define XST_USB_NO_DESC_AVAILABLE 1412
+#define XST_USB_BUF_TOO_BIG   1413
+#define XST_USB_NO_BUF      1414
+
+
+/**************************** Type Definitions *******************************/
+
+typedef int XStatus;
+
+/***************** Macros (Inline Functions) Definitions *********************/
+
+
+/************************** Function Prototypes ******************************/
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* end of protection macro */
diff --git a/arch/arm/mach-zynq/k7_base_driver/xdma/Makefile b/arch/arm/mach-zynq/k7_base_driver/xdma/Makefile
new file mode 100644
index 0000000..62dd461
--- /dev/null
+++ b/arch/arm/mach-zynq/k7_base_driver/xdma/Makefile
@@ -0,0 +1,6 @@
+#
+# Makefile for the Xilinx K7 PCIe DMA base driver.
+#
+ccflags-y  := -DK7_TRD -DX86_PC -DNWLDMA -DXENV_LINUX -DUSE_IO_MACROS -DXLITTLE_ENDIAN -DPCI_LINUX -I$(srctree)/arch/arm/mach-zynq/k7_base_driver/include/ 
+obj-y += xdma_k7.o
+xdma_k7-y	:= xdma_base.o xdma.o xdma_bdring.o xdma_user.o
diff --git a/arch/arm/mach-zynq/k7_base_driver/xdma/xdma.c b/arch/arm/mach-zynq/k7_base_driver/xdma/xdma.c
new file mode 100644
index 0000000..2db38f9
--- /dev/null
+++ b/arch/arm/mach-zynq/k7_base_driver/xdma/xdma.c
@@ -0,0 +1,215 @@
+/*******************************************************************************
+** © Copyright 2011 - 2012 Xilinx, Inc. All rights reserved.
+** This file contains confidential and proprietary information of Xilinx, Inc. and
+** is protected under U.S. and international copyright and other intellectual property laws.
+*******************************************************************************
+**   ____  ____
+**  /   /\/   /
+** /___/  \  /   Vendor: Xilinx
+** \   \   \/
+**  \   \
+**  /   /
+** \   \  /  \   Kintex-7 PCIe-DMA-DDR3 Base Targeted Reference Design
+**  \___\/\___\
+**
+**  Device: xc7k325t
+**  Reference: UG882
+*******************************************************************************
+**
+**  Disclaimer:
+**
+**    This disclaimer is not a license and does not grant any rights to the materials
+**    distributed herewith. Except as otherwise provided in a valid license issued to you
+**    by Xilinx, and to the maximum extent permitted by applicable law:
+**    (1) THESE MATERIALS ARE MADE AVAILABLE "AS IS" AND WITH ALL FAULTS,
+**    AND XILINX HEREBY DISCLAIMS ALL WARRANTIES AND CONDITIONS, EXPRESS, IMPLIED, OR STATUTORY,
+**    INCLUDING BUT NOT LIMITED TO WARRANTIES OF MERCHANTABILITY, NON-INFRINGEMENT, OR
+**    FITNESS FOR ANY PARTICULAR PURPOSE; and (2) Xilinx shall not be liable (whether in contract
+**    or tort, including negligence, or under any other theory of liability) for any loss or damage
+**    of any kind or nature related to, arising under or in connection with these materials,
+**    including for any direct, or any indirect, special, incidental, or consequential loss
+**    or damage (including loss of data, profits, goodwill, or any type of loss or damage suffered
+**    as a result of any action brought by a third party) even if such damage or loss was
+**    reasonably foreseeable or Xilinx had been advised of the possibility of the same.
+
+
+**  Critical Applications:
+**
+**    Xilinx products are not designed or intended to be fail-safe, or for use in any application
+**    requiring fail-safe performance, such as life-support or safety devices or systems,
+**    Class III medical devices, nuclear facilities, applications related to the deployment of airbags,
+**    or any other applications that could lead to death, personal injury, or severe property or
+**    environmental damage (individually and collectively, "Critical Applications"). Customer assumes
+**    the sole risk and liability of any use of Xilinx products in Critical Applications, subject only
+**    to applicable laws and regulations governing limitations on product liability.
+
+**  THIS COPYRIGHT NOTICE AND DISCLAIMER MUST BE RETAINED AS PART OF THIS FILE AT ALL TIMES.
+
+*******************************************************************************/
+/*****************************************************************************/
+/**
+*
+* @file xdma.c
+*
+* This file implements initialization and control related functions. For more
+* information on this driver, see xdma.h.
+*
+* <pre>
+* MODIFICATION HISTORY:
+*
+* Ver   Who  Date     Changes
+* ----- ---- -------- -------------------------------------------------------
+* 1.0   ps   12/07/09 First release
+* </pre>
+******************************************************************************/
+
+/***************************** Include Files *********************************/
+
+#include <linux/string.h>
+#include <linux/kernel.h>
+
+#include "xbasic_types.h"
+#include "xdebug.h"
+#include "xstatus.h"
+#include "xio.h"
+#include "xdma.h"
+#include "xdma_hw.h"
+
+
+/************************** Constant Definitions *****************************/
+
+
+/**************************** Type Definitions *******************************/
+
+
+/***************** Macros (Inline Functions) Definitions *********************/
+
+
+/************************** Function Prototypes ******************************/
+
+
+/************************** Variable Definitions *****************************/
+
+
+/*****************************************************************************/
+/**
+ * This function initializes a DMA engine.  This function must be called
+ * prior to using the DMA engine. Initialization of an engine includes setting
+ * up the register base address, setting up the instance data, and ensuring the
+ * hardware is in a quiescent state.
+ *
+ * @param  InstancePtr is a pointer to the DMA engine instance to be worked on.
+ * @param  BaseAddress is where the registers for this engine can be found.
+ *
+ * @return None.
+ *
+ *****************************************************************************/
+void Dma_Initialize(Dma_Engine * InstancePtr, u32 BaseAddress, u32 Type)
+{
+    log_verbose(KERN_INFO "Initializing DMA()\n");
+
+  /* Set up the instance */
+    log_verbose(KERN_INFO "Clearing DMA instance %p\n", InstancePtr);
+  memset(InstancePtr, 0, sizeof(Dma_Engine));
+
+    log_verbose(KERN_INFO "DMA base address is 0x%x\n", BaseAddress);
+  InstancePtr->RegBase = BaseAddress;
+    InstancePtr->Type = Type;
+
+    /* Initialize the engine and ring states. */
+    InstancePtr->BdRing.RunState = XST_DMA_SG_IS_STOPPED;
+    InstancePtr->EngineState = INITIALIZED;
+
+  /* Initialize the ring structure */
+  InstancePtr->BdRing.ChanBase = BaseAddress;
+    if(Type == DMA_ENG_C2S)
+        InstancePtr->BdRing.IsRxChannel = 1;
+    else
+        InstancePtr->BdRing.IsRxChannel = 0;
+
+  /* Reset the device and return */
+  Dma_Reset(InstancePtr);
+}
+
+/*****************************************************************************/
+/**
+* Reset the DMA engine.
+*
+* Should not be invoked during initialization stage because hardware has
+* just come out of a system reset. Should be invoked during shutdown stage.
+*
+* New BD fetches will stop immediately. Reset will be completed once the
+* user logic completes its reset. DMA disable will be completed when the
+* BDs already being processed are completed.
+*
+* @param  InstancePtr is a pointer to the DMA engine instance to be worked on.
+*
+* @return None.
+*
+* @note
+*   - If the hardware is not working properly, and the self-clearing reset
+*     bits do not clear, this function will be terminated after a timeout.
+*
+******************************************************************************/
+void Dma_Reset(Dma_Engine * InstancePtr)
+{
+  Dma_BdRing *RingPtr;
+    int i=0;
+    u32 dirqval;
+
+  log_verbose(KERN_INFO "Resetting DMA instance %p\n", InstancePtr);
+
+   spin_lock(&(InstancePtr->bdring_lock)); 
+
+  RingPtr = &Dma_mGetRing(InstancePtr);
+
+  /* Disable engine interrupts before issuing software reset */
+  Dma_mEngIntDisable(InstancePtr);
+
+  /* Start reset process then wait for completion. Disable DMA and
+     * assert reset request at the same time. This causes user logic to
+     * be reset.
+     */
+    log_verbose(KERN_INFO "Disabling DMA. User reset request.\n");
+    i=0;
+  Dma_mSetCrSr(InstancePtr, (DMA_ENG_DISABLE|DMA_ENG_USER_RESET));
+
+  /* Loop until the reset is done. The bits will self-clear. */
+  while (Dma_mGetCrSr(InstancePtr) &
+                        (DMA_ENG_STATE_MASK|DMA_ENG_USER_RESET)) {
+        i++;
+        if(i >= 100000)
+        {
+            printk(KERN_INFO "CR is now 0x%x\n", Dma_mGetCrSr(InstancePtr));
+            break;
+        }
+  }
+
+  /* Now reset the DMA engine, and wait for its completion. */
+    log_verbose(KERN_INFO "DMA reset request.\n");
+    i=0;
+  Dma_mSetCrSr(InstancePtr, (DMA_ENG_RESET));
+
+  /* Loop until the reset is done. The bit will self-clear. */
+  while (Dma_mGetCrSr(InstancePtr) & DMA_ENG_RESET) {
+        i++;
+        if(i >= 100000)
+        {
+            printk(KERN_INFO "CR is now 0x%x\n", Dma_mGetCrSr(InstancePtr));
+            break;
+        }
+  }
+
+  /* Clear Interrupt register. Not doing so may cause interrupts
+     * to be asserted after the engine reset if there is any
+   * interrupt left over from before.
+   */
+    dirqval = Dma_mGetCrSr(InstancePtr);
+    log_verbose(KERN_INFO"While resetting engine, got %x in eng status reg\n", dirqval);
+    if(dirqval & DMA_ENG_INT_ACTIVE_MASK)
+        Dma_mEngIntAck(InstancePtr, (dirqval & DMA_ENG_ALLINT_MASK));
+
+  RingPtr->RunState = XST_DMA_SG_IS_STOPPED;
+   spin_unlock(&(InstancePtr->bdring_lock)); 
+}
+
diff --git a/arch/arm/mach-zynq/k7_base_driver/xdma/xdma.h b/arch/arm/mach-zynq/k7_base_driver/xdma/xdma.h
new file mode 100644
index 0000000..2ac5c08
--- /dev/null
+++ b/arch/arm/mach-zynq/k7_base_driver/xdma/xdma.h
@@ -0,0 +1,525 @@
+/*******************************************************************************
+** © Copyright 2011 - 2012 Xilinx, Inc. All rights reserved.
+** This file contains confidential and proprietary information of Xilinx, Inc. and
+** is protected under U.S. and international copyright and other intellectual property laws.
+*******************************************************************************
+**   ____  ____
+**  /   /\/   /
+** /___/  \  /   Vendor: Xilinx
+** \   \   \/
+**  \   \
+**  /   /
+** \   \  /  \   Kintex-7 PCIe-DMA-DDR3 Base Targeted Reference Design
+**  \___\/\___\
+**
+**  Device: xc7k325t
+**  Reference: UG882
+*******************************************************************************
+**
+**  Disclaimer:
+**
+**    This disclaimer is not a license and does not grant any rights to the materials
+**    distributed herewith. Except as otherwise provided in a valid license issued to you
+**    by Xilinx, and to the maximum extent permitted by applicable law:
+**    (1) THESE MATERIALS ARE MADE AVAILABLE "AS IS" AND WITH ALL FAULTS,
+**    AND XILINX HEREBY DISCLAIMS ALL WARRANTIES AND CONDITIONS, EXPRESS, IMPLIED, OR STATUTORY,
+**    INCLUDING BUT NOT LIMITED TO WARRANTIES OF MERCHANTABILITY, NON-INFRINGEMENT, OR
+**    FITNESS FOR ANY PARTICULAR PURPOSE; and (2) Xilinx shall not be liable (whether in contract
+**    or tort, including negligence, or under any other theory of liability) for any loss or damage
+**    of any kind or nature related to, arising under or in connection with these materials,
+**    including for any direct, or any indirect, special, incidental, or consequential loss
+**    or damage (including loss of data, profits, goodwill, or any type of loss or damage suffered
+**    as a result of any action brought by a third party) even if such damage or loss was
+**    reasonably foreseeable or Xilinx had been advised of the possibility of the same.
+
+
+**  Critical Applications:
+**
+**    Xilinx products are not designed or intended to be fail-safe, or for use in any application
+**    requiring fail-safe performance, such as life-support or safety devices or systems,
+**    Class III medical devices, nuclear facilities, applications related to the deployment of airbags,
+**    or any other applications that could lead to death, personal injury, or severe property or
+**    environmental damage (individually and collectively, "Critical Applications"). Customer assumes
+**    the sole risk and liability of any use of Xilinx products in Critical Applications, subject only
+**    to applicable laws and regulations governing limitations on product liability.
+
+**  THIS COPYRIGHT NOTICE AND DISCLAIMER MUST BE RETAINED AS PART OF THIS FILE AT ALL TIMES.
+
+*******************************************************************************/
+/*****************************************************************************/
+/**
+*
+* @file xdma.h
+*
+* The Xilinx Scatter Gather DMA driver for the Northwest Logic DMA Engine.
+* Each DMA engine may either be for TX (System-to-Card or S2C) or RX (Card-to-
+* System or C2S) operation. Each driver instance supports one S2C and one C2S
+* DMA engines, allowing full-duplex operation per instance.
+*
+* Please note that the Xilinx DMA driver uses only the Packet DMA engine
+* of the Northwest Logic DMA Engine, and not the Block DMA engine type. As
+* such, the description below pertains to the Packet DMA flow.
+*
+* This component is designed to be used as a basic building block for
+* designing a device driver. It provides register accesses such that all
+* DMA processing can be maintained easier, but the device driver designer
+* must still understand all the details of the DMA channel.
+*
+* For a full description of DMA features, please see the hardware specification.
+* This driver supports the following features:
+*
+*   - Scatter-Gather DMA (SGDMA)
+*   - Interrupts
+*   - Interrupts are coalesced by the driver to improve performance
+*   - 32-bit addressing for Buffer Descriptors (BDs) and data buffers
+*   - APIs to manage BD movement to and from the SGDMA engine
+*   - Virtual memory support
+*   - PCI Express support
+*   - Performance measurement statistics
+*   - APIs to enable DMA driver to be used by other application drivers
+*
+* <b>Packet DMA Transactions</b>
+*
+* To describe a DMA transaction in its simplest form, you need source address,
+* destination address, and the number of bytes to transfer. When using a
+* packet DMA receive channel, the source address is within some piece of IP
+* hardware and doesn't require the software to explicitly set it. And the
+* destination address points to a system buffer into which received data
+* must be copied.
+*
+* Likewise, with a packet DMA transmit channel, the destination address is
+* within the hardware and need not be specified by the software. And the
+* source address points to a system buffer that must be transmitted.
+*
+* Therefore, some of the packet DMA transaction attributes include:
+*
+*   - An application buffer address
+*   - The number bytes to transfer
+*   - Whether this transaction represents the start of a packet, or end of a
+*     packet, or neither (middle of a packet)
+*
+* The object used to describe a transaction is referred to as a Buffer
+* Descriptor (BD). See xdma_bd.h for a detailed description of and the APIs
+* for manipulation of these objects.
+*
+* <b>Scatter-Gather DMA (SGDMA)</b>
+*
+* SGDMA allows the application to define a list of transactions in memory which
+* the hardware will process without further application intervention. During
+* this time, the application is free to continue adding more work to keep the
+* hardware busy.
+*
+* Notification of completed transactions can be done either by polling the
+* hardware, or using interrupts that signal a transaction has completed, or
+* by examining BDs for a completion status.
+*
+* SGDMA processes whole packets. A packet is defined as a series of
+* data bytes that represent a message. SGDMA allows a packet of data to be
+* broken up into one or more transactions. For example, take an Ethernet IP
+* packet which consists of a 14-byte header followed by a data payload of one
+* or more bytes. With SGDMA, the application may point a BD to the header
+* and another BD to the payload, then transfer them as a single message.
+* This strategy can make a TCP/IP stack more efficient by allowing it to
+* keep packet headers and data in different memory regions instead of
+* assembling packets into contiguous blocks of memory.
+*
+* <b>SGDMA Ring Management</b>
+*
+* The hardware expects BDs to be setup as a singly linked list. As a BD is
+* completed, the DMA engine will dereference BD.Next and load the next BD
+* to process. These BDs may be arranged in a large chain or in a large ring.
+* This driver uses a fixed buffer ring where each BD is linked to the
+* next BD in adjacent memory. The last BD in the ring is linked to the first.
+*
+* Within the ring, the driver maintains four groups of BDs. Each group consists
+* of 0 or more adjacent BDs:
+*
+*   - Free: Those BDs that can be allocated for DMA with Dma_BdRingAlloc().
+*
+*   - Pre-process: Those BDs that have been allocated with Dma_BdRingAlloc().
+*     These BDs can now be modified in preparation for future DMA
+*     transactions.
+*
+*   - Hardware: Those BDs that have been enqueued to hardware with
+*     Dma_BdRingToHw(). These BDs are under hardware control and may be in a
+*     state of awaiting hardware processing, in process, or processed by
+*     hardware. It is considered an error for the driver to change BDs
+*     while they are in this group. Doing so can cause data corruption and
+*     lead to system instability.
+*
+*   - Post-process: Those BDs that have been processed by hardware and have
+*     been recovered by the driver with Dma_BdRingFromHw(). These BDs
+*     are now under driver control. The driver may access these BDs to
+*     determine the result of DMA transactions. When the driver is
+*     finished, Dma_BdRingFree() should be called to place them back into
+*     the Free group.
+*
+*
+* Normally BDs are moved in the following way:
+* <pre>
+*
+*         Dma_BdRingAlloc()                      Dma_BdRingToHw()
+*   Free ------------------------> Pre-process ----------------------> Hardware
+*                                                                      |
+*    /|\                                                               |
+*     |   Dma_BdRingFree()                       Dma_BdRingFromHw()    |
+*     +--------------------------- Post-process <----------------------+
+*
+* </pre>
+*
+* There are a few exceptions to the flow above. One is that after BDs are
+* moved from the Free group to the Pre-process group, the driver decides for
+* whatever reason that these BDs are not ready and could not be given to
+* hardware. In this case, these BDs could be moved back to Free group using
+* Dma_BdRingUnAlloc() function to help keep the BD ring in great shape and
+* recover the error. See comments of the function for details.
+*
+* <pre>
+*
+*           Dma_BdRingUnAlloc()
+*   Free <----------------------- Pre-process
+*
+* </pre>
+*
+* The second exception to the flow above is when a registered
+* application-specific driver unregisters from the DMA driver, then the
+* buffers queued for DMA must be retrieved from the BD ring and returned to
+* the driver for freeing. This is the only time that this flow is used.
+*
+* <pre>
+*
+*         Dma_BdRingAlloc()                      Dma_BdRingToHw()
+*   Free ------------------------> Pre-process ----------------------> Hardware
+*                                                                        |
+*    /|\                                                                 |
+*     |   Dma_BdRingFree()                       Dma_BdRingForceFromHw() |
+*     +--------------------------- Post-process <------------------------+
+*
+* </pre>
+*
+* The API provides macros that allow BD list traversal. These macros should be
+* used with care as they do not understand where one group ends and another
+* begins.
+*
+* The driver does not cache or keep copies of any BD. When it modifies BDs
+* returned by Dma_BdRingAlloc() or Dma_BdRingFromHw(), it is modifying the
+* same BD that hardware accesses.
+*
+* Certain pairs of list modification functions have usage restrictions. See
+* the function headers for Dma_BdRingAlloc() and Dma_BdRingFromHw() for
+* more information.
+*
+* <b>SGDMA Descriptor Ring Creation</b>
+*
+* During initialization, the function Dma_BdRingCreate() is used to set up
+* a memory block to contain all BDs for the DMA channel. This function takes
+* as an argument the number of BDs to place in the list.
+*
+* The caller allocates a memory block large enough to contain the desired
+* number of BDs plus an extra one to take care of any shifting in order to
+* meet the 32-byte alignment needs of the Northwest Logic DMA hardware.
+* This has been covered in more detail below.
+*
+* Once the list has been created, it can be used right away to perform DMA
+* transactions.
+*
+* <b>Interrupts</b>
+*
+* The driver has a compile-time flag to enable either interrupts or polled
+* mode of operation. In some configurations, the polled mode is seen to have
+* better performance than the interrupt mode driver.
+*
+* When interrupts are enabled, the interrupt handler !!!MUST!!! clear
+* pending interrupts before handling the BDs processed by the DMA engine.
+* Otherwise the following corner case could raise some issue:
+*
+* - A packet is transmitted(/received) and asserts a TX(/RX) interrupt, and if
+*   this interrupt handler deals with the BDs finished by the DMA before it
+*   clears the interrupt, another packet could get transmitted(/received)
+*   and assert the interrupt between when the BDs are taken care and when
+*   the interrupt clearing operation begins, and the interrupt clearing
+*   operation will clear the interrupt raised by the second packet and will
+*   never process it according BDs until a new interrupt occurs.
+*
+* Changing the sequence to "Clear interrupts before handling BDs" solves this
+* issue:
+*
+* - If the interrupt raised by the second packet is before the interrupt
+*   clearing operation, the descriptors associated with the second packet must
+*   have been finished by hardware and ready for the handler to deal with,
+*   and those descriptors will processed with those BDs of the first packet
+*   during the handling of the interrupt asserted by the first packet.
+*
+* - if the interrupt of the second packet is asserted after the interrupt
+*   clearing operation but its BDs are finished before the handler starts to
+*   deal with BDs, the packet's buffer descriptors will be handled with
+*   those of the first packet during the handling of the interrupt asserted
+*   by the first packet.
+*
+* - Otherwise, the BDs of the second packet is not ready when the interrupt
+*   handler starts to deal with the BDs of the first packet. Those BDs will
+*   be handled next time the interrupt handled gets invoked as the interrupt
+*   of the second packet is not cleared in current pass and thereby will
+*   cause the handler to get invoked again
+*
+* Please note if the second case above occurs, the handler will find
+* NO buffer descriptor is finished by the hardware (i.e., Dma_BdRingFromHw()
+* returns 0) during the handling of the interrupt asserted by the second
+* packet. This is valid and the driver should NOT consider this is a
+* hardware error and have no need to reset the hardware.
+*
+* <b>Interrupt Coalescing</b>
+*
+* On a high-speed link, significant processor overhead may be used in
+* servicing interrupts. Interrupt coalescing can be achieved by the driver
+* enabling BD completion interrupts, not on every BD, but on a set of BDs,
+* thus reducing the frequency of interrupts. The macro INT_COAL_CNT in
+* xdma_hw.h can be used for this purpose.
+*
+* In addition, the driver processes outstanding BDs to be processed after an
+* idle timeout with no transactions elapses.
+*
+* <b> Software Initialization </b>
+*
+* The driver does the following steps in order to prepare the DMA engine
+* to be ready to process DMA transactions:
+*
+* - DMA Initialization using Dma_Initialize() function. This step
+*   initializes a driver instance for the given DMA engine and resets the
+*   engine. One driver instance exists for a pair of (S2C and C2S) engines.
+* - BD Ring creation. A BD ring is needed per engine and can be built by
+*   calling Dma_BdRingCreate(). A parameter passed to this function is the
+*   number of BDs fit in a given memory range, and Dma_mBdRingCntCalc() helps
+*   calculate the value.
+* - (RX channel only) Prepare BDs with attached data buffers and give them to
+*   the RX channel. First, allocate BDs using Dma_BdRingAlloc(), then populate
+*   data buffer address, data buffer size and the control word fields of each
+*   allocated BD with valid values. Last call Dma_BdRingToHw() to give the
+*   BDs to the channel.
+* - Enable interrupts if interrupt mode is chosen. The application is
+*   responsible for setting up the interrupt system, which includes providing
+*   and connecting interrupt handlers and call back functions, before
+*   the interrupts are enabled.
+* - Start DMA channels: Call Dma_BdRingStart() to start a channel
+*
+* <b> How to start DMA transactions </b>
+*
+* RX channel is ready to start RX transactions once the initialization (see
+* Initialization section above) is finished. The DMA transactions are triggered
+* by the user IP (like Local Link TEMAC).
+*
+* Starting TX transactions needs some work. The application calls
+* Dma_BdRingAlloc() to allocate a BD list, then populates necessary
+* attributes of each allocated BD including data buffer address, data size,
+* and control word, and last passes those BDs to the TX channel
+* (see Dma_BdRingToHw()). The added BDs will be processed as soon as the
+* TX channel reaches them.
+*
+* <b> Software Post-Processing on completed DMA transactions </b>
+*
+* Some software post-processing is needed after DMA transactions are finished.
+*
+* If interrupts are set up and enabled, DMA channels notify the software
+* the finishing of DMA transactions using interrupts,  Otherwise the
+* application could poll the channels (see Dma_BdRingFromHw()).
+*
+* - Once BDs are finished by a channel, the application first needs to fetch
+*   them from the channel (see Dma_BdRingFromHw()).
+* - On TX side, the application now could free the data buffers attached to
+*   those BDs as the data in the buffers has been transmitted.
+* - On RX side, the application now could use the received data in the buffers
+*   attached to those BDs
+* - For both channels, those BDs need to be freed back to the Free group (see
+*   Dma_BdRingFree()) so they are allocatable for future transactions.
+* - On RX side, it is the application's responsibility for having BDs ready
+*   to receive data at any time. Otherwise the RX channel will refuse to
+*   accept any data once it runs out of RX BDs. As we just freed those hardware
+*   completed BDs in the previous step, it is good timing to allocate them
+*   back (see Dma_BdRingAlloc()), prepare them, and feed them to the RX
+*   channel again (see Dma_BdRingToHw())
+*
+* <b>Address Translation</b>
+*
+* When the BD list is setup with Dma_BdRingCreate(), a physical and
+* virtual address is supplied for the segment of memory containing the
+* descriptors. The driver will handle any translations internally. Subsequent
+* access of descriptors by the application is done in terms of their virtual
+* address.
+*
+* Any application data buffer address attached to a BD must be physical
+* address. The application is responsible for calculating the physical address
+* before assigns it to the buffer address field in the BD.
+*
+* <b> Memory Barriers </b>
+*
+* The DMA hardware expects the update to its Next BD pointer register to be
+* the event which initiates DMA processing. Hence, memory barrier wmb() calls
+* have been used to ensure this.
+*
+* <b>Alignment</b>
+*
+* <b> For BDs: </b>
+* The Northwest Logic DMA hardware requires BDs to be aligned at 32-byte
+* boundaries. In addition to the this, the driver has its own alignment
+* requirements. It needs to store per-packet information in each BD, for
+* example, the buffer virtual address. In order to do this, the software
+* view of the BD may be larger than the hardware view of the BD. For example,
+* DMA_BD_SW_NUM_WORDS can be set to 16 words (64 bytes), even though
+* DMA_BD_HW_NUM_WORDS is 8 words (32 bytes). Due to this, the driver
+* gets additional space in which to store per-BD private information.
+*
+* Minimum alignment is defined by the constant DMA_BD_MINIMUM_ALIGNMENT.
+* This is the smallest alignment allowed by both hardware and software for them
+* to properly work. Other than DMA_BD_MINIMUM_ALIGNMENT, multiples of the
+* constant are the only valid alignments for BDs.
+*
+* If the descriptor ring is to be placed in cached memory, alignment also MUST
+* be at least the processor's cache-line size. If this requirement is not met
+* then system instability will result. This is also true if the length of a BD
+* is longer than one cache-line, in which case multiple cache-lines are needed
+* to accommodate each BD.
+*
+* Aside from the initial creation of the descriptor ring (see
+* Dma_BdRingCreate()), there are no other run-time checks for proper
+* alignment.
+*
+* <b>For application data buffers:</b>
+* Application data buffer alignment is taken care of by the
+* application-specific drivers.
+*
+* <b>Reset After Stopping</b>
+*
+* This driver is designed to allow for stop-reset-start cycles of the DMA
+* hardware while keeping the BD list intact. When restarted after a reset, this
+* driver will point the DMA engine to where it left off after stopping it.
+*
+* It is possible to load an application-specific driver, run it for some
+* time, and then unload it. Without unloading the DMA driver as well, it
+* should be possible to load another instance of the application-specific
+* driver and it should work fine.
+*
+*
+* <pre>
+* MODIFICATION HISTORY:
+*
+* Ver   Who  Date     Changes
+* ----- ---- -------- -------------------------------------------------------
+* 1.0   ps   06/30/09 First version
+* 1.0   ps   12/07/09 Release version
+* </pre>
+*
+******************************************************************************/
+
+#ifndef XDMA_H    /* prevent circular inclusions */
+#define XDMA_H    /* by using protection macros */
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/***************************** Include Files *********************************/
+
+#include <linux/timer.h>
+
+#include "xbasic_types.h"
+#include "xdma_bdring.h"
+#include "xdma_user.h"
+
+/************************** Constant Definitions *****************************/
+
+#define MAX_BARS                6       /**< Maximum number of BARs */
+#define MAX_DMA_ENGINES         64      /**< Maximum number of DMA engines */
+
+#define DMA_ALL_BDS         0xFFFFFFFF  /**< Indicates all valid BDs */
+
+/**************************** Type Definitions *******************************/
+
+/** @name DMA engine instance data
+ * @{
+ */
+typedef struct {
+    struct pci_dev * pdev;  /**< PCIe Device handle */
+    u32 RegBase;            /**< Virtual base address of DMA engine */
+
+    u32 EngineState;        /**< State of the DMA engine */
+  Dma_BdRing BdRing;      /**< BD container for DMA engine */
+    u32 Type;               /**< Type of DMA engine - C2S or S2C */
+    UserPtrs user;          /**< User callback functions */
+    int pktSize;            /**< User-specified usual size of packets */
+
+#ifdef TH_BH_ISR
+    int intrCount;          /**< Counter to control interrupt coalescing */
+#endif
+
+    dma_addr_t descSpacePA; /**< Physical address of BD space */
+    u32 descSpaceSize;      /**< Size of BD space in bytes */
+    u32 * descSpaceVA;      /**< Virtual address of BD space */
+    u32 delta;              /**< Shift from original start for alignment */
+     spinlock_t bdring_lock;
+} Dma_Engine;
+/*@}*/
+
+/** @name Private per-device data
+ * The PCI device entry points to this as driver-private data. In some
+ * cases, pointer back to PCI device entry is also required.
+ * @{
+ */
+struct privData {
+    struct pci_dev * pdev;          /**< PCI device entry */
+
+    /** BAR information discovered on probe. BAR0 is understood by this driver.
+     * Other BARs will be used as app. drivers register with this driver.
+     */
+    u32 barMask;                    /**< Bitmask for BAR information */
+    struct {
+        unsigned long basePAddr;    /**< Base address of device memory */
+        unsigned long baseLen;      /**< Length of device memory */
+        void __iomem * baseVAddr;   /**< VA - mapped address */
+    } barInfo[MAX_BARS];
+
+  //struct list_head rcv;
+  //struct list_head xmit;
+
+  u32 index;                    /**< Which interface is this */
+
+    /**
+     * The user driver instance data. An instance must be allocated
+     * for each user request. The user driver request will request separately
+     * for one C2S and one S2C DMA engine instances, if required. The DMA
+     * driver will not allocate a pair of engine instances on its own.
+     */
+    long long engineMask;           /**< For storing a 64-bit mask */
+  Dma_Engine Dma[MAX_DMA_ENGINES];/**< Per-engine information */
+
+    int userCount;                  /**< Number of registered users */
+};
+extern struct privData * dmaData;
+/*@}*/
+
+extern u32 DriverState;
+
+/* for exclusion of all program flows (processes, ISRs and BHs) */
+extern spinlock_t DmaLock;
+extern spinlock_t GlobalDataLock; 
+extern spinlock_t DmaTXLock;
+extern spinlock_t DmaRXLock;
+
+/***************** Macros (Inline Functions) Definitions *********************/
+
+
+/************************** Function Prototypes ******************************/
+
+/** @name Initialization and control functions in xdma.c
+ * @{
+ */
+int descriptor_init(struct pci_dev *pdev, Dma_Engine * eptr);
+void descriptor_free(struct pci_dev *pdev, Dma_Engine * eptr);
+void Dma_Initialize(Dma_Engine * InstancePtr, u32 BaseAddress, u32 Type);
+void Dma_Reset(Dma_Engine * InstancePtr);
+/*@}*/
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* end of protection macro */
diff --git a/arch/arm/mach-zynq/k7_base_driver/xdma/xdma_base.c b/arch/arm/mach-zynq/k7_base_driver/xdma/xdma_base.c
new file mode 100644
index 0000000..6f9d660
--- /dev/null
+++ b/arch/arm/mach-zynq/k7_base_driver/xdma/xdma_base.c
@@ -0,0 +1,2657 @@
+/*******************************************************************************
+** © Copyright 2011 - 2012 Xilinx, Inc. All rights reserved.
+** This file contains confidential and proprietary information of Xilinx, Inc. and
+** is protected under U.S. and international copyright and other intellectual property laws.
+*******************************************************************************
+**   ____  ____
+**  /   /\/   /
+** /___/  \  /   Vendor: Xilinx
+** \   \   \/
+**  \   \
+**  /   /
+** \   \  /  \   Kintex-7 PCIe-DMA-DDR3 Base Targeted Reference Design
+**  \___\/\___\
+**
+**  Device: xc7k325t
+**  Reference: UG882
+*******************************************************************************
+**
+**  Disclaimer:
+**
+**    This disclaimer is not a license and does not grant any rights to the materials
+**    distributed herewith. Except as otherwise provided in a valid license issued to you
+**    by Xilinx, and to the maximum extent permitted by applicable law:
+**    (1) THESE MATERIALS ARE MADE AVAILABLE "AS IS" AND WITH ALL FAULTS,
+**    AND XILINX HEREBY DISCLAIMS ALL WARRANTIES AND CONDITIONS, EXPRESS, IMPLIED, OR STATUTORY,
+**    INCLUDING BUT NOT LIMITED TO WARRANTIES OF MERCHANTABILITY, NON-INFRINGEMENT, OR
+**    FITNESS FOR ANY PARTICULAR PURPOSE; and (2) Xilinx shall not be liable (whether in contract
+**    or tort, including negligence, or under any other theory of liability) for any loss or damage
+**    of any kind or nature related to, arising under or in connection with these materials,
+**    including for any direct, or any indirect, special, incidental, or consequential loss
+**    or damage (including loss of data, profits, goodwill, or any type of loss or damage suffered
+**    as a result of any action brought by a third party) even if such damage or loss was
+**    reasonably foreseeable or Xilinx had been advised of the possibility of the same.
+
+
+**  Critical Applications:
+**
+**    Xilinx products are not designed or intended to be fail-safe, or for use in any application
+**    requiring fail-safe performance, such as life-support or safety devices or systems,
+**    Class III medical devices, nuclear facilities, applications related to the deployment of airbags,
+**    or any other applications that could lead to death, personal injury, or severe property or
+**    environmental damage (individually and collectively, "Critical Applications"). Customer assumes
+**    the sole risk and liability of any use of Xilinx products in Critical Applications, subject only
+**    to applicable laws and regulations governing limitations on product liability.
+
+**  THIS COPYRIGHT NOTICE AND DISCLAIMER MUST BE RETAINED AS PART OF THIS FILE AT ALL TIMES.
+
+*******************************************************************************/
+/*****************************************************************************/
+/**
+ *
+ * @file xdma_base.c
+ *
+ * This is the Linux base driver for the DMA engine core. It provides
+ * multi-channel DMA capability with the help of the Northwest Logic
+ * DMA engine.
+ *
+ * Author: Xilinx, Inc.
+ *
+ * 2007-2010 (c) Xilinx, Inc. This file is licensed uner the terms of the GNU
+ * General Public License version 2.1. This program is licensed "as is" without
+ * any warranty of any kind, whether express or implied.
+ *
+ * <pre>
+ * MODIFICATION HISTORY:
+ *
+ * Ver   Date     Changes
+ * ----- -------- -------------------------------------------------------
+ * 1.0   12/07/09 First release
+ * 1.2   09/01/10 Updates to read version register and convey to GUI.
+ * </pre>
+ *
+ *****************************************************************************/
+
+/***************************** Include Files *********************************/
+#include <linux/pci.h>
+#include <linux/interrupt.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/spinlock.h>
+#include <linux/fs.h>
+#include <linux/types.h>
+#include <linux/kdev_t.h>
+#include <linux/cdev.h>
+#include <asm/uaccess.h>
+#include <linux/version.h>
+#include <linux/mm.h>                   
+#include <linux/slab.h>
+
+#include <linux/delay.h>
+
+#include <xpmon_be.h>
+#include "xdebug.h"
+#include "xbasic_types.h"
+#include "xstatus.h"
+#include "xdma.h"
+#include "xdma_hw.h"
+#include "xdma_bdring.h"
+#include "xdma_user.h"
+#include "xraw_init.h"
+#include <linux/kthread.h>
+
+/************************** Constant Definitions *****************************/
+
+/** @name Macros for PCI probing
+ * @{
+ */
+#define PCI_VENDOR_ID_DMA   0x10EE      /**< Vendor ID - Xilinx */
+
+#define PCI_DEVICE_ID_DMA   0x7082      /**< Xilinx's Device ID */
+
+/** Driver information */
+#define DRIVER_NAME         "xdma_driver"
+#define DRIVER_DESCRIPTION  "Xilinx DMA Linux driver"
+#define DRIVER_VERSION      "1.0"
+
+/** Driver Module information */
+MODULE_AUTHOR("Xilinx, Inc.");
+MODULE_DESCRIPTION(DRIVER_DESCRIPTION);
+MODULE_VERSION(DRIVER_VERSION);
+MODULE_LICENSE("GPL");
+
+/** PCI device structure which probes for targeted design */
+static struct pci_device_id ids[] = {
+        { PCI_VENDOR_ID_DMA,    PCI_DEVICE_ID_DMA,
+          PCI_ANY_ID,               PCI_ANY_ID,
+          0,            0,          0UL },
+          { }     /* terminate list with empty entry */
+};
+
+/**
+ * Macro to export pci_device_id to user space to allow hot plug and
+ * module loading system to know what module works with which hardware device
+ */
+MODULE_DEVICE_TABLE(pci, ids);
+
+/*@}*/
+
+/** Engine bitmask is 64-bit because there are 64 engines */
+#define DMA_ENGINE_PER_SIZE     0x100   /**< Separation between engine regs */
+#define DMA_OFFSET              0       /**< Starting register offset */
+                                        /**< Size of DMA engine reg space */
+#define DMA_SIZE                (MAX_DMA_ENGINES * DMA_ENGINE_PER_SIZE)
+
+/**
+ * Default S2C and C2S descriptor ring size.
+ * BD Space needed is (DMA_BD_CNT*sizeof(Dma_Bd)).
+ */
+
+#define DMA_BD_CNT 1999
+
+/* Size of packet pool */
+#define MAX_POOL    10
+
+/* Structures to store statistics - the latest 100 */
+#define MAX_STATS   100
+
+/************************** Variable Names ***********************************/
+/** Pool of packet arrays to use while processing packets */
+struct PktPool
+{
+    PktBuf * pbuf;
+    struct PktPool * next;
+};
+
+PktBuf pktArray[MAX_POOL][DMA_BD_CNT]; // used for passing pkts between drivers.
+struct PktPool pktPool[MAX_POOL];
+struct PktPool * pktPoolHead=NULL;
+struct PktPool * pktPoolTail=NULL;
+
+PktBuf RxpktArray[MAX_POOL][DMA_BD_CNT]; // used for passing pkts between drivers.
+struct PktPool RxpktPool[MAX_POOL];
+struct PktPool * RxpktPoolHead=NULL;
+struct PktPool * RxpktPoolTail=NULL;
+
+struct timer_list stats_timer;
+struct task_struct  *rx_task;
+#ifdef USE_LATER
+struct task_struct *rx_freebuf_task;
+#endif
+struct task_struct  *tx_cleanup_task;
+
+struct cdev * xdmaCdev=NULL;
+
+/** DMA driver state-related variables */
+struct privData * dmaData = NULL;
+u32 DriverState = UNINITIALIZED;
+
+/* for exclusion of all program flows (processes, ISRs and BHs) */
+static DEFINE_SPINLOCK(DmaStatsLock);
+DEFINE_SPINLOCK(DmaLock);
+DEFINE_SPINLOCK(GlobalDataLock);
+static DEFINE_SPINLOCK(IntrLock);
+static DEFINE_SPINLOCK(PktPoolLock);
+static DEFINE_SPINLOCK(RxPktPoolLock);
+
+/* Statistics-related variables */
+int UserOpen=0;
+DMAStatistics DStats[MAX_DMA_ENGINES][MAX_STATS];
+SWStatistics SStats[MAX_DMA_ENGINES][MAX_STATS];
+TRNStatistics TStats[MAX_STATS];
+int dstatsRead[MAX_DMA_ENGINES], dstatsWrite[MAX_DMA_ENGINES];
+int dstatsNum[MAX_DMA_ENGINES], sstatsRead[MAX_DMA_ENGINES];
+int sstatsWrite[MAX_DMA_ENGINES], sstatsNum[MAX_DMA_ENGINES];
+int tstatsRead, tstatsWrite, tstatsNum;
+u32 SWrate[MAX_DMA_ENGINES];
+
+
+/************************** Function Prototypes ******************************/
+static int xdma_probe(struct pci_dev *pdev, const struct pci_device_id *ent);
+static void xdma_remove(struct pci_dev *pdev);
+static int xdma_dev_open(struct inode * in, struct file * filp);
+static int xdma_dev_release(struct inode * in, struct file * filp);
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,36)
+static int xdma_dev_ioctl(struct inode * in, struct file * filp,
+                          unsigned int cmd, unsigned long arg);
+#else
+static long xdma_dev_ioctl(struct file * filp,
+                          unsigned int cmd, unsigned long arg);
+#endif
+static int ReadPCIState(struct pci_dev * pdev, PCIState * pcistate);
+
+#ifdef DEBUG_VERBOSE
+void disp_bd_ring(Dma_BdRing *);
+#endif
+
+static void PutUnusedPkts(Dma_Engine * eptr, PktBuf * pbuf, int numpkts);
+static void DmaSetupRecvBuffers(struct pci_dev *, Dma_Engine *);
+
+#ifdef DEBUG_VERBOSE
+void disp_frag(unsigned char *, u32);
+//static void ReadRoot(struct pci_dev *);
+#endif
+
+#if defined(DEBUG_VERBOSE) || defined(DEBUG_NORMAL)
+static void ReadConfig(struct pci_dev *);
+#endif
+
+/** xdma Driver information */
+static struct pci_driver xdma_driver = {
+    .name = DRIVER_NAME,
+    .id_table = ids,
+    .probe = xdma_probe,
+    .remove = xdma_remove
+};
+
+/* The callback function for completed frames sent in SGDMA mode.
+ * In the interrupt-mode, these functions are scheduled as bottom-halves.
+ * In the polled-mode, these functions are invoked as functions.
+ */
+static void PktHandler(int eng, Dma_Engine * eptr);
+static void rxPktHandler(int eng, Dma_Engine * eptr);
+
+#ifdef TH_BH_ISR
+unsigned long long PendingMask = 0x0LL;
+int LastIntr[MAX_DMA_ENGINES]={ 0, };
+int MSIEnabled=0;
+static void IntrBH(unsigned long unused);
+DECLARE_TASKLET(DmaBH, IntrBH, 0);
+#endif
+
+static void ReadDMAEngineConfiguration(struct pci_dev *, struct privData *);
+static void poll_stats(unsigned long __opaque);
+
+/* Functions to enqueue and dequeue packet arrays in packet pools */
+struct PktPool * DQPool(void)
+{
+    struct PktPool * ppool;
+
+    spin_lock(&PktPoolLock);
+    ppool = pktPoolHead;
+    pktPoolHead = ppool->next;
+    if(pktPoolHead == NULL)
+	log_verbose(KERN_ERR "pktPoolHead is NULL. This should never happen\n");
+    spin_unlock(&PktPoolLock);
+
+    return ppool;
+}
+
+void EQPool(struct PktPool * pp)
+{
+
+    spin_lock(&PktPoolLock);
+    pktPoolTail->next = pp;
+    pp->next = NULL;
+    pktPoolTail = pp;
+    spin_unlock(&PktPoolLock);
+}
+
+struct PktPool * RxDQPool(void)
+{
+    struct PktPool * ppool;
+
+    spin_lock(&RxPktPoolLock);
+    ppool = RxpktPoolHead;
+    RxpktPoolHead = ppool->next;
+    if(RxpktPoolHead == NULL)
+	log_verbose(KERN_ERR "pktPoolHead is NULL. This should never happen\n");
+    spin_unlock(&RxPktPoolLock);
+
+    return ppool;
+}
+
+void RxEQPool(struct PktPool * pp)
+{
+
+    spin_lock(&RxPktPoolLock);
+    RxpktPoolTail->next = pp;
+    pp->next = NULL;
+    RxpktPoolTail = pp;
+    spin_unlock(&RxPktPoolLock);
+}
+
+#ifdef TH_BH_ISR
+
+static void IntrBH(unsigned long unused)
+{
+    struct pci_dev *pdev;
+    struct privData *lp;
+    Dma_Engine * eptr;
+    unsigned long flags;
+    int i;
+
+    pdev = dmaData->pdev;
+    lp = pci_get_drvdata(pdev);
+
+    log_verbose("IntrBH with PendingMask %llx\n", PendingMask);
+
+    //while(PendingMask)
+    for(i=0; PendingMask && i<MAX_DMA_ENGINES; i++)
+    {
+        if(!(PendingMask & (1LL << i))) continue;
+        spin_lock_irqsave(&IntrLock, flags);
+
+        /* At this point, we have engine identified. */
+
+        /* First, reset mask bit */
+        PendingMask &= ~(1LL << i);
+
+        spin_unlock_irqrestore(&IntrLock, flags);
+
+        eptr = &(lp->Dma[i]);
+        if(eptr->EngineState != USER_ASSIGNED)      // Should not happen
+            continue;
+
+        /* The spinlocks need to be handled within this function, so
+         * don't do them here.
+         */
+
+        PktHandler(i, eptr);
+
+        spin_lock_irqsave(&IntrLock, flags);
+        Dma_mEngIntEnable(eptr);
+
+        /* Update flag to synchronise between ISR and poll_routine */
+        LastIntr[i] = jiffies;
+        spin_unlock_irqrestore(&IntrLock, flags);
+    }
+}
+
+u32 Acks(u32 dirqval)
+{
+    u32 retval=0;
+
+    retval |= (dirqval & DMA_ENG_ENABLE_MASK) ? DMA_ENG_ENABLE : 0;
+    retval |= (dirqval & DMA_ENG_INT_ACTIVE_MASK) ? DMA_ENG_INT_ACK : 0;
+    retval |= (dirqval & DMA_ENG_INT_BDCOMP) ? DMA_ENG_INT_BDCOMP_ACK : 0;
+    retval |= (dirqval & DMA_ENG_INT_ALERR) ? DMA_ENG_INT_ALERR_ACK : 0;
+    retval |= (dirqval & DMA_ENG_INT_FETERR) ? DMA_ENG_INT_FETERR_ACK : 0;
+    retval |= (dirqval & DMA_ENG_INT_ABORTERR) ? DMA_ENG_INT_ABORTERR_ACK : 0;
+    retval |= (dirqval & DMA_ENG_INT_CHAINEND) ? DMA_ENG_INT_CHAINEND_ACK : 0;
+
+    if(dirqval & DMA_ENG_INT_ACTIVE_MASK)
+        retval &= ~(DMA_ENG_INT_ENABLE);        \
+
+    log_verbose(KERN_INFO "Acking %x with %x\n", dirqval, retval);
+    return retval;
+}
+
+/* This function serves to handle the initial interrupt, as well as to
+ * check again on pending interrupts, from the BH. If this is not done,
+ * interrupts can stall.
+ */
+int IntrCheck(struct pci_dev * dev)
+{
+    u32 girqval, dirqval;
+    struct privData *lp;
+    u32 base, imask;
+    Dma_Engine * eptr;
+    int i, retval=XST_FAILURE;
+    static int count0=0, count1=0, count2=0, count3=0;
+
+    lp = pci_get_drvdata(dev);
+    log_verbose(KERN_INFO "IntrCheck: device %x\n", (u32) dev);
+
+    base = (u32)(lp->barInfo[0].baseVAddr);
+    girqval = Dma_mReadReg(base, REG_DMA_CTRL_STATUS);
+    //if(!(girqval & (DMA_INT_ACTIVE_MASK|DMA_INT_PENDING_MASK|DMA_USER_INT_ACTIVE_MASK)))
+    //if(!(girqval & (DMA_INT_ACTIVE_MASK|DMA_USER_INT_ACTIVE_MASK)))
+    //    return;
+
+    /* Now, check each S2C DMA engine (0 to 7) */
+    imask = (girqval & DMA_S2C_ENG_INT_VAL) >> 16;
+    for(i=0; i<7; i++)
+    {
+        if(!imask) break;
+        if(!(imask & (0x01<<i))) continue;
+
+        if(!((lp->engineMask) & (1LL << i)))
+            continue;
+
+        eptr = &(lp->Dma[i]);
+
+        dirqval = Dma_mGetCrSr(eptr);
+        log_verbose("Eng %d: dirqval is %x\n", i, dirqval);
+
+        /* Check whether interrupt is enabled, otherwise it could be a
+         * re-check of the last checked engine before its bottom half has run.
+         */
+        if((dirqval & DMA_ENG_INT_ACTIVE_MASK) &&
+           (dirqval & DMA_ENG_INT_ENABLE))
+        {
+            spin_lock(&IntrLock);
+            //Dma_mEngIntAck(eptr, Acks(dirqval));
+            Dma_mSetCrSr(eptr, Acks(dirqval));         \
+
+            if(dirqval & (DMA_ENG_INT_ALERR|DMA_ENG_INT_FETERR|DMA_ENG_INT_ABORTERR))
+                printk("Eng %d: Came with error dirqval %x\n", i, dirqval);
+            if(dirqval & DMA_ENG_INT_BDCOMP)
+            {
+                //Dma_mEngIntDisable(eptr); // Already disabled
+                PendingMask |= (1LL << i);
+            }
+            spin_unlock(&IntrLock);
+
+            if(i==0) count0++;
+            else if(i==1) count1++;
+            retval = XST_SUCCESS;
+        }
+else if(dirqval & DMA_ENG_INT_ACTIVE_MASK) log_normal("1");
+    }
+
+    /* Now, check each C2S DMA engine (32 to 39) */
+    imask = (girqval & DMA_C2S_ENG_INT_VAL) >> 24;
+    for(i=32; i<39; i++)
+    {
+        if(!imask) break;
+        if(!(imask & (0x01<<(i-32)))) continue;
+
+        if(!((lp->engineMask) & (1LL << i)))
+            continue;
+
+        eptr = &(lp->Dma[i]);
+
+        dirqval = Dma_mGetCrSr(eptr);
+        log_verbose("Eng %d: dirqval is %x\n", i, dirqval);
+
+        /* Check whether interrupt is enabled, otherwise it could be a
+         * re-check of the last checked engine before its bottom half has run.
+         */
+        if((dirqval & DMA_ENG_INT_ACTIVE_MASK) &&
+           (dirqval & DMA_ENG_INT_ENABLE))
+        {
+            spin_lock(&IntrLock);
+            Dma_mEngIntAck(eptr, Acks(dirqval));
+
+            if(dirqval & (DMA_ENG_INT_ALERR|DMA_ENG_INT_FETERR|DMA_ENG_INT_ABORTERR))
+                printk("Eng %d: Came with error dirqval %x\n", i, dirqval);
+            if(dirqval & DMA_ENG_INT_BDCOMP)
+            {
+                Dma_mEngIntDisable(eptr);
+                PendingMask |= (1LL << i);
+            }
+            spin_unlock(&IntrLock);
+
+            if(i==32) count2++;
+            else if(i==33) count3++;
+            retval = XST_SUCCESS;
+        }
+        else if(dirqval & DMA_ENG_INT_ACTIVE_MASK) log_normal("~");
+    }
+
+    spin_lock(&IntrLock);
+    if(PendingMask && (retval == XST_SUCCESS))
+    {
+        tasklet_schedule(&DmaBH);
+    }
+    spin_unlock(&IntrLock);
+
+    return retval;
+}
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,19)
+static irqreturn_t DmaInterrupt(int irq, void *dev_id, struct pt_regs *regs)
+#else
+static irqreturn_t DmaInterrupt(int irq, void *dev_id)
+#endif
+{
+    struct pci_dev *dev = dev_id;
+
+  /* Handle DMA and any user interrupts */
+  if(IntrCheck(dev) == XST_SUCCESS)
+        return IRQ_HANDLED;
+    else
+        return IRQ_NONE;
+}
+
+#endif
+
+static void PktHandler(int eng, Dma_Engine * eptr)
+{
+    struct pci_dev * pdev;
+    Dma_BdRing * rptr;
+    UserPtrs * uptr;
+    Dma_Bd *BdPtr, *BdCurPtr;
+    int result = XST_SUCCESS;
+    unsigned int bd_processed, bd_processed_save;
+    dma_addr_t bufPA;
+    int j;
+    static int txcount = 0;
+    static int rxcount = 0;
+    PktBuf * pbuf;
+    struct PktPool * ppool;
+    u32 flag;
+
+    rptr = &(eptr->BdRing);
+    uptr = &(eptr->user);
+    pdev = eptr->pdev;
+
+    //log_verbose("[PH%d] ", eng);
+
+    if(rptr->IsRxChannel) flag = PCI_DMA_FROMDEVICE;
+    else flag = PCI_DMA_TODEVICE;
+
+    spin_lock(&(eptr->bdring_lock));
+
+    /* Handle engine operations */
+    bd_processed_save = 0;
+    if ((bd_processed = Dma_BdRingFromHw(rptr, DMA_BD_CNT, &BdPtr)) > 0)
+    {
+        log_verbose(KERN_INFO "PktHandler: Processed %d BDs\n", bd_processed);
+
+        /* First, get a pool of packets to work with */
+        ppool = DQPool();
+
+        if(rptr->IsRxChannel) rxcount += bd_processed;
+        else txcount += bd_processed;
+        log_verbose("PktHandler: txcount %d rxcount %d\n", txcount, rxcount);
+
+        bd_processed_save = bd_processed;
+        BdCurPtr = BdPtr;
+        j = 0;
+        do
+        {
+            pbuf = &((ppool->pbuf)[j]);
+
+            bufPA = (dma_addr_t) Dma_mBdGetBufAddr(BdCurPtr);
+            pbuf->size = Dma_mBdGetStatLength(BdCurPtr);
+            pbuf->bufInfo = (unsigned char *)Dma_mBdGetId(BdCurPtr);
+
+            /* For now, do this. Temac driver does not actually look
+             * at pbuf->pktBuf, but eventually, this is not the right
+             * thing to do.
+             */
+            pbuf->pktBuf = pbuf->bufInfo;
+            pbuf->flags = Dma_mBdGetStatus(BdCurPtr);
+            pbuf->userInfo = Dma_mBdGetUserData(BdCurPtr);
+
+            log_verbose(KERN_INFO "Length %d Buf %x\n", pbuf->size, (u32) bufPA);
+            pci_unmap_single(pdev, bufPA, pbuf->size, flag);
+
+            /* reset BD id */
+            Dma_mBdSetId(BdCurPtr, NULL);
+
+            BdCurPtr = Dma_mBdRingNext(rptr, BdCurPtr);
+            bd_processed--;
+            j++;
+
+#ifdef USE_LATER
+            /* Add to SW payload stats counters */
+            spin_lock(&DmaStatsLock);
+            SWrate[eng] += pbuf->size;
+            spin_unlock(&DmaStatsLock);
+#endif
+        } while (bd_processed > 0);
+
+        result = Dma_BdRingFree(rptr, bd_processed_save, BdPtr);
+        if (result != XST_SUCCESS) {
+            printk(KERN_ERR "PktHandler: BdRingFree() error %d.\n", result);
+            EQPool(ppool);
+            spin_unlock(&(eptr->bdring_lock));
+            return;
+        }
+        spin_unlock(&(eptr->bdring_lock));
+
+        if (bd_processed_save)
+        {
+            log_verbose(KERN_INFO "PktHandler processed %d BDs\n", bd_processed_save);
+            (uptr->UserPutPkt)(eptr, ppool->pbuf, bd_processed_save, uptr->privData);
+        }
+
+        /* Now return packet pool to list */
+        EQPool(ppool);
+    spin_lock(&(eptr->bdring_lock));
+    }
+
+    /* Handle any RX-specific engine operations */
+    if(rptr->IsRxChannel)
+    {
+        /* Replenish BDs in the RX ring */
+        DmaSetupRecvBuffers(pdev, eptr);
+    }
+        spin_unlock(&(eptr->bdring_lock));
+}
+
+static void rxPktHandler(int eng, Dma_Engine * eptr)
+{
+    struct pci_dev * pdev;
+    Dma_BdRing * rptr;
+    UserPtrs * uptr;
+    Dma_Bd *BdPtr, *BdCurPtr;
+    int result = XST_SUCCESS;
+    unsigned int bd_processed, bd_processed_save;
+    dma_addr_t bufPA;
+    int j;
+    static int txcount = 0;
+    static int rxcount = 0;
+    PktBuf * pbuf;
+    struct PktPool * ppool;
+    u32 flag;
+
+    rptr = &(eptr->BdRing);
+    uptr = &(eptr->user);
+    pdev = eptr->pdev;
+
+    //log_verbose("[PH%d] ", eng);
+
+    if(rptr->IsRxChannel) flag = PCI_DMA_FROMDEVICE;
+    else flag = PCI_DMA_TODEVICE;
+
+    spin_lock(&(eptr->bdring_lock));
+
+    /* Handle engine operations */
+    bd_processed_save = 0;
+    if ((bd_processed = Dma_BdRingFromHw(rptr, DMA_BD_CNT, &BdPtr)) > 0)
+    {
+        log_verbose(KERN_INFO "PktHandler: Processed %d BDs\n", bd_processed);
+
+        /* First, get a pool of packets to work with */
+        ppool = RxDQPool();
+
+        if(rptr->IsRxChannel) rxcount += bd_processed;
+        else txcount += bd_processed;
+        log_verbose("PktHandler: txcount %d rxcount %d\n", txcount, rxcount);
+
+        bd_processed_save = bd_processed;
+        BdCurPtr = BdPtr;
+        j = 0;
+        do
+        {
+            pbuf = &((ppool->pbuf)[j]);
+
+            bufPA = (dma_addr_t) Dma_mBdGetBufAddr(BdCurPtr);
+            pbuf->size = Dma_mBdGetStatLength(BdCurPtr);
+            pbuf->bufInfo = (unsigned char *)Dma_mBdGetId(BdCurPtr);
+
+            /* For now, do this. Temac driver does not actually look
+             * at pbuf->pktBuf, but eventually, this is not the right
+             * thing to do.
+             */
+            pbuf->pktBuf = pbuf->bufInfo;
+            pbuf->flags = Dma_mBdGetStatus(BdCurPtr);
+            pbuf->userInfo = Dma_mBdGetUserData(BdCurPtr);
+
+            log_verbose(KERN_INFO "Length %d Buf %x\n", pbuf->size, (u32) bufPA);
+            pci_unmap_single(pdev, bufPA, pbuf->size, flag);
+
+            /* reset BD id */
+            Dma_mBdSetId(BdCurPtr, NULL);
+
+            BdCurPtr = Dma_mBdRingNext(rptr, BdCurPtr);
+            bd_processed--;
+            j++;
+
+#ifdef USE_LATER
+            /* Add to SW payload stats counters */
+            spin_lock(&DmaStatsLock);
+            SWrate[eng] += pbuf->size;
+            spin_unlock(&DmaStatsLock);
+#endif
+        } while (bd_processed > 0);
+
+        result = Dma_BdRingFree(rptr, bd_processed_save, BdPtr);
+        if (result != XST_SUCCESS) {
+            printk(KERN_ERR "PktHandler: BdRingFree() error %d.\n", result);
+            RxEQPool(ppool);
+            spin_unlock(&(eptr->bdring_lock));
+            return;
+        }
+        spin_unlock(&(eptr->bdring_lock));
+
+        if (bd_processed_save)
+        {
+            log_verbose(KERN_INFO "PktHandler processed %d BDs\n", bd_processed_save);
+            (uptr->UserPutPkt)(eptr, ppool->pbuf, bd_processed_save, uptr->privData);
+        }
+
+        /* Now return packet pool to list */
+        RxEQPool(ppool);
+    spin_lock(&(eptr->bdring_lock));
+    }
+    /* Handle any RX-specific engine operations */
+    if(rptr->IsRxChannel)
+    {
+        /* Replenish BDs in the RX ring */
+        DmaSetupRecvBuffers(pdev, eptr);
+    }
+        spin_unlock(&(eptr->bdring_lock));
+}
+
+static void poll_stats(unsigned long __opaque)
+{
+    struct pci_dev *pdev = (struct pci_dev *)__opaque;
+    struct privData *lp;
+    Dma_Engine * eptr;
+    Dma_BdRing * rptr;
+    int i, offset = 0;
+    u32 at, wt, cb, t1, t2;
+    u32 base;
+
+    if(DriverState == UNREGISTERING)
+        return;
+
+    lp = pci_get_drvdata(pdev);
+    log_verbose("s%d ", get_cpu());
+
+    /* First, get DMA payload statistics */
+    for(i=0; i<MAX_DMA_ENGINES; i++)
+    {
+        if(!((lp->engineMask) & (1LL << i)))
+            continue;
+
+	if((i==1) || (i==33))
+	    continue;
+
+
+        eptr = &(lp->Dma[i]);
+        rptr = &(eptr->BdRing);
+
+        spin_lock(&DmaStatsLock);
+
+        /* First, read the DMA engine payload registers */
+        at = Dma_mReadReg(rptr->ChanBase, REG_DMA_ENG_ACTIVE_TIME);
+        wt = Dma_mReadReg(rptr->ChanBase, REG_DMA_ENG_WAIT_TIME);
+        cb = Dma_mReadReg(rptr->ChanBase, REG_DMA_ENG_COMP_BYTES);
+
+        /* Want to store the latest set of statistics. If the GUI is not
+         * running, the statistics will build up. So, read pointer should
+         * move forward alongwith the write pointer.
+         */
+        DStats[i][dstatsWrite[i]].LAT = 4*(at>>2);
+        DStats[i][dstatsWrite[i]].LWT = 4*(wt>>2);
+        DStats[i][dstatsWrite[i]].LBR = 4*(cb>>2);
+        dstatsWrite[i] += 1;
+        if(dstatsWrite[i] >= MAX_STATS) dstatsWrite[i] = 0;
+
+        if(dstatsNum[i] < MAX_STATS)
+            dstatsNum[i] += 1;
+        /* else move the read pointer forward */
+        else
+        {
+            dstatsRead[i] += 1;
+            if(dstatsRead[i] >= MAX_STATS) dstatsRead[i] = 0;
+        }
+
+        /* Next, read the SW statistics counters */
+        t1 = SWrate[i];
+        SStats[i][sstatsWrite[i]].LBR = t1;
+        SWrate[i] = 0;
+        sstatsWrite[i] += 1;
+        if(sstatsWrite[i] >= MAX_STATS) sstatsWrite[i] = 0;
+
+        if(sstatsNum[i] < MAX_STATS)
+            sstatsNum[i] += 1;
+        /* else move the read pointer forward */
+        else
+        {
+            sstatsRead[i] += 1;
+            if(sstatsRead[i] >= MAX_STATS) sstatsRead[i] = 0;
+        }
+
+        log_normal(KERN_INFO "[%d]: active=[%d]%u, wait=[%d]%u, comp bytes=[%d]%u, sw=%u\n",
+                i, (at&0x3), 4*(at>>2), (wt&0x3), 4*(wt>>2),
+                (cb&0x3), 4*(cb>>2), t1);
+
+        spin_unlock(&DmaStatsLock);
+    }
+
+    /* Now, get the TRN statistics */
+
+    /* Registers to be read for TRN stats */
+    base = (u32)(dmaData->barInfo[0].baseVAddr);
+
+    /* This counts all TLPs including header */
+   // t1 = XIo_In32(base+0x8200);
+   // t2 = XIo_In32(base+0x8204);
+    t1 = XIo_In32(base+0x900c);
+    t2 = XIo_In32(base+0x9010);
+    //printk(KERN_INFO "TRN util: TX [%d]%u, RX [%d]%u\n",
+    //      (t1&0x3), 4*(t1>>2), (t2&0x3), 4*(t2>>2));
+    TStats[tstatsWrite].LTX = 4*(t1>>2);
+    TStats[tstatsWrite].LRX = 4*(t2>>2);
+    tstatsWrite += 1;
+    if(tstatsWrite >= MAX_STATS) tstatsWrite = 0;
+
+    if(tstatsNum < MAX_STATS)
+        tstatsNum += 1;
+    /* else move the read pointer forward */
+    else
+    {
+        tstatsRead += 1;
+        if(tstatsRead >= MAX_STATS) tstatsRead = 0;
+    }
+
+    /* This counts only payload of TLPs - buffer and BDs */
+    t1 = XIo_In32(base+0x9014);
+    t2 = XIo_In32(base+0x9018);
+    //printk(KERN_INFO "TRN payload: TX [%d]%u, RX [%d]%u\n",
+    //      (t1&0x3), 4*(t1>>2), (t2&0x3), 4*(t2>>2));
+
+    /* Reschedule poll routine */
+    offset = -3;
+    stats_timer.expires = jiffies + HZ + offset;
+    add_timer(&stats_timer);
+}
+
+#ifdef DEBUG_VERBOSE
+void disp_frag(unsigned char * addr, u32 len)
+{
+  int i;
+
+  for(i=0; i<len; i++)
+  {
+    printk("%02x ", addr[i]);
+    if(!((i+1)%4))
+      printk(", ");
+    if(!((i+1)%16))
+      printk("\n");
+  }
+  printk("\n");
+}
+#endif
+
+/* This function returns all unused packets to the app driver. Will either
+ * happen because packets got for reception could not be queued for DMA,
+ * or while the app driver is unregistering itself, just prior to unloading.
+ */
+static void PutUnusedPkts(Dma_Engine * eptr, PktBuf * pbuf, int numpkts)
+{
+    int i;
+    UserPtrs * uptr;
+
+    uptr = &(eptr->user);
+
+    for(i=0; i < numpkts; i++)
+        pbuf[i].flags = PKT_UNUSED;
+
+    (uptr->UserPutPkt)(eptr, pbuf, numpkts, uptr->privData);
+}
+
+/*
+ * DmaSetupRecvBuffers allocates as many packet buffers as it can up to
+ * the number of free C2S buffer descriptors, and sets up the C2S
+ * buffer descriptors to DMA into the buffers.
+ */
+static void DmaSetupRecvBuffers(struct pci_dev *pdev, Dma_Engine * eptr)
+{
+  struct privData *lp = NULL;
+    Dma_BdRing * rptr;
+    UserPtrs * uptr;
+  int free_bd_count ;
+    int numbds;
+  dma_addr_t bufPA;
+  Dma_Bd *BdPtr, *BdCurPtr;
+  int result, num, numgot;
+    int i, len;
+    struct PktPool * ppool;
+#ifdef TH_BH_ISR
+    u32 mask;
+#endif
+
+#if defined DEBUG_NORMAL || defined DEBUG_VERBOSE
+    //static int recv_count=1;
+    //log_normal(KERN_INFO "DmaSetupRecvBuffers: #%d\n", recv_count++);
+#endif
+
+    lp = pci_get_drvdata(pdev);
+    rptr = &(eptr->BdRing);
+    uptr = &(eptr->user);
+    free_bd_count = Dma_mBdRingGetFreeCnt(rptr);
+
+    /* Maintain a separation between start and end of BD ring. This is
+     * required because DMA will stall if the two pointers coincide -
+     * this will happen whether ring is full or empty.
+     */
+    if(free_bd_count > 2) free_bd_count -= 2;
+    else return;
+
+    log_verbose(KERN_INFO "SetupRecv: Free BD count is %d\n", free_bd_count);
+
+    /* First, get a pool of packets to work with */
+    ppool = RxDQPool();
+
+    numbds = 0;
+    do {
+        /* Get buffers from user */
+        num = free_bd_count;
+        log_verbose(KERN_INFO "Trying to get %d buffers from user driver\n", num);
+    numgot = (uptr->UserGetPkt)(eptr, ppool->pbuf, eptr->pktSize, num, uptr->privData);
+    if (!numgot) {
+            log_verbose(KERN_ERR "Could not get any packet for RX from user\n");
+      break;
+    }
+
+        /* Allocate BDs from ring */
+        result = Dma_BdRingAlloc(rptr, numgot, &BdPtr);
+        if (result != XST_SUCCESS) {
+            /* We really shouldn't get this. Return unused buffers to app */
+            printk(KERN_ERR "DmaSetupRecvBuffers: BdRingAlloc unsuccessful (%d)\n",
+                   result);
+            PutUnusedPkts(eptr, ppool->pbuf, numgot);
+            break;
+        }
+
+        log_verbose(KERN_INFO "User returned %d RX buffers\n", numgot);
+        BdCurPtr = BdPtr;
+        for(i = 0; i < numgot; i++)
+        {
+            PktBuf * pbuf;
+
+            pbuf = &((ppool->pbuf)[i]);
+            bufPA = pci_map_single(pdev, (u32 *)(pbuf->pktBuf), pbuf->size, PCI_DMA_FROMDEVICE);
+            log_verbose(KERN_INFO "The buffer after alloc is at VA %x PA %x size %d\n",
+                            (u32) pbuf->pktBuf, bufPA, pbuf->size);
+
+            Dma_mBdSetBufAddr(BdCurPtr, bufPA);
+            Dma_mBdSetCtrlLength(BdCurPtr, pbuf->size);
+            Dma_mBdSetId(BdCurPtr, pbuf->bufInfo);
+            Dma_mBdSetCtrl(BdCurPtr, 0);        // Disable interrupts also.
+            Dma_mBdSetUserData(BdCurPtr, 0LL);
+
+#ifdef TH_BH_ISR
+            /* Enable interrupts for errors and completion based on
+             * coalesce count.
+             */
+            mask = DMA_BD_INT_ERROR_MASK;
+            if(!(eptr->intrCount % INT_COAL_CNT))
+                mask |= DMA_BD_INT_COMP_MASK;
+            eptr->intrCount += 1;
+            Dma_mBdSetCtrl(BdCurPtr, mask);
+#endif
+
+            BdCurPtr = Dma_mBdRingNext(rptr, BdCurPtr);
+        }
+
+        /* Enqueue all Rx BDs with attached buffers such that they are
+         * ready for frame reception.
+         */
+        result = Dma_BdRingToHw(rptr, numgot, BdPtr);
+        if (result != XST_SUCCESS) {
+            /* Should not come here. Incase of error, unmap buffers,
+             * unallocate BDs, and return buffers to app driver.
+             */
+            printk(KERN_ERR "DmaSetupRecvBuffers: BdRingToHw unsuccessful (%d)\n",
+                   result);
+            BdCurPtr = BdPtr;
+            for(i=0; i < numgot; i++)
+            {
+                bufPA = Dma_mBdGetBufAddr(BdCurPtr);
+                len = Dma_mBdGetCtrlLength(BdCurPtr);
+                pci_unmap_single(pdev, bufPA, len, PCI_DMA_FROMDEVICE);
+                Dma_mBdSetId(BdCurPtr, NULL);
+                BdCurPtr = Dma_mBdRingNext(rptr, BdCurPtr);
+            }
+            Dma_BdRingUnAlloc(rptr, numgot, BdPtr);
+            PutUnusedPkts(eptr, ppool->pbuf, numgot);
+            break;
+        }
+
+        free_bd_count -= numgot;
+        numbds += numgot;
+        log_verbose(KERN_INFO "free_bd_count %d, numbds %d, numgot %d\n",
+                            free_bd_count, numbds, numgot);
+    } while (free_bd_count > 0);
+
+    /* Return packet pool to list */
+    RxEQPool(ppool);
+
+#ifdef DEBUG_VERBOSE
+    if(numbds)
+        log_verbose(KERN_INFO "DmaSetupRecvBuffers: %d new RX BDs queued up\n",
+                                        numbds);
+#endif
+}
+
+/*****************************************************************************/
+/**
+ * This function initializes the DMA BD ring as follows -
+ * - Calculates the space required by the DMA BD ring
+ * - Allocates the space, and aligns it as per DMA engine requirement
+ * - Creates the BD ring structure in the allocated space
+ * - If it is a RX DMA engine, allocates buffers from the user driver, and
+ *   associates each BD in the RX BD ring with a buffer
+ *
+ * @param  pdev is the PCI/PCIe device instance
+ * @param  eptr is a pointer to the DMA engine instance to be worked on.
+ *
+ * @return 0 if successful
+ * @return negative value if unsuccessful
+ *
+ *****************************************************************************/
+int descriptor_init(struct pci_dev *pdev, Dma_Engine * eptr)
+{
+  int dftsize, numbds;
+    u32 * BdPtr;
+    dma_addr_t BdPhyAddr ;
+  int result;
+    u32 delta = 0;
+
+  /* Calculate size of descriptor space pool - extra to allow for
+     * alignment adjustment.
+     */
+  dftsize = sizeof(u32) * DMA_BD_SW_NUM_WORDS * (DMA_BD_CNT + 1);
+  log_normal(KERN_INFO "XDMA: BD space: %d (0x%0x)\n", dftsize, dftsize);
+
+    if((BdPtr = pci_alloc_consistent(pdev, dftsize, &BdPhyAddr)) == NULL)
+    {
+        printk(KERN_ERR "BD ring pci_alloc_consistent() failed\n");
+        return -1;
+    }
+
+    log_normal(KERN_INFO "BD ring space allocated from %p, PA 0x%x\n",
+                                                BdPtr, BdPhyAddr);
+    numbds = Dma_BdRingAlign((u32)BdPtr, dftsize, DMA_BD_MINIMUM_ALIGNMENT, &delta);
+    if(numbds <= 0) {
+        log_normal(KERN_ERR "Unable to align allocated BD space\n");
+        /* Free allocated space !!!! */
+        return -1;
+    }
+
+    eptr->descSpacePA = BdPhyAddr + delta;
+    eptr->descSpaceVA = BdPtr + delta;
+  eptr->descSpaceSize = dftsize - delta;
+    eptr->delta = delta;
+
+  if (eptr->descSpaceVA == 0) {
+    return -1;
+  }
+
+  log_normal(KERN_INFO
+        "XDMA: (descriptor_init) PA: 0x%x, VA: 0x%x, Size: %d, Delta: %d\n",
+         eptr->descSpacePA, (unsigned int) eptr->descSpaceVA,
+         eptr->descSpaceSize, eptr->delta);
+
+  result = Dma_BdRingCreate(&(eptr->BdRing), (u32) eptr->descSpacePA,
+             (u32) eptr->descSpaceVA, DMA_BD_MINIMUM_ALIGNMENT, numbds);
+  if (result != XST_SUCCESS)
+  {
+    printk(KERN_ERR "XDMA: DMA Ring Create. Error: %d\n", result);
+    return -EIO;
+  }
+
+    if((eptr->Type & DMA_ENG_DIRECTION_MASK) == DMA_ENG_C2S)
+        DmaSetupRecvBuffers(pdev, eptr);
+
+#ifdef DEBUG_VERBOSE
+  log_verbose(KERN_INFO "BD Ring buffers:\n");
+  disp_bd_ring(&eptr->BdRing);
+#endif
+
+  return 0;
+}
+
+/*****************************************************************************/
+/**
+ * In order to free allocated space (and avoid memory leaks), this function
+ * is called when the user driver unregisters itself from the DMA base driver.
+ * It does the following -
+ * - Forcibly retrieves the buffers which have been queued up for DMA with
+ *   the DMA engine hardware
+ * - Unmaps these buffers from the PCI/PCIe space
+ * - Returns these buffers to the user driver, which will free them
+ * - Frees the BD ring
+ * - De-allocates the space used for the BD ring, and unmaps it from
+ *   the PCI/PCIe space
+ *
+ * @param  pdev is the PCI/PCIe device instance
+ * @param  eptr is a pointer to the DMA engine instance to be worked on.
+ *
+ * @return None.
+ *
+ *****************************************************************************/
+void descriptor_free(struct pci_dev *pdev, Dma_Engine * eptr)
+{
+    Dma_Bd *BdPtr, *BdCurPtr;
+    unsigned int bd_processed, bd_processed_save;
+    Dma_BdRing * rptr;
+    UserPtrs * uptr;
+    PktBuf * pbuf;
+    dma_addr_t bufPA;
+    int j, result;
+    struct PktPool * ppool;
+u32 flag;
+
+    log_verbose(KERN_INFO "descriptor_free: \n");
+
+    rptr = &(eptr->BdRing);
+    uptr = &(eptr->user);
+
+    if(rptr->IsRxChannel) flag = PCI_DMA_FROMDEVICE;
+    else flag = PCI_DMA_TODEVICE;
+
+    spin_lock(&(eptr->bdring_lock));
+
+    /* First recover buffers and BDs queued up for DMA, then pass to user */
+    bd_processed_save = 0;
+    if ((bd_processed = Dma_BdRingForceFromHw(rptr, DMA_BD_CNT, &BdPtr)) > 0)
+    {
+        log_normal(KERN_INFO "descriptor_free: Forced %d BDs from hw\n", bd_processed);
+        /* First, get a pool of packets to work with */
+        if(rptr->IsRxChannel)
+        ppool = RxDQPool();
+        else
+        ppool = DQPool();
+
+        bd_processed_save = bd_processed;
+        BdCurPtr = BdPtr;
+        j = 0;
+        do
+        {
+            pbuf = &((ppool->pbuf)[j]);
+
+            bufPA = (dma_addr_t) Dma_mBdGetBufAddr(BdCurPtr);
+            pbuf->size = Dma_mBdGetCtrlLength(BdCurPtr);
+            pbuf->bufInfo = (unsigned char *)Dma_mBdGetId(BdCurPtr);
+            /* For now, do this. Temac driver does not actually look
+             * at pbuf->pktBuf, but eventually, this is not the right
+             * thing to do.
+             */
+            pbuf->pktBuf = pbuf->bufInfo;
+            pbuf->flags = PKT_UNUSED;
+            pbuf->userInfo = Dma_mBdGetUserData(BdCurPtr);
+
+            /* Now unmap this buffer */
+#ifdef DEBUG_VERBOSE
+            log_verbose(KERN_INFO "Length %d Buf %x\n", pbuf->size, (u32) bufPA);
+#endif
+            pci_unmap_single(pdev, bufPA, pbuf->size, flag);
+
+            /* reset BD id */
+            Dma_mBdSetId(BdCurPtr, NULL);
+
+            BdCurPtr = Dma_mBdRingNext(rptr, BdCurPtr);
+            bd_processed--;
+            j++;
+        } while (bd_processed > 0);
+
+        spin_unlock(&(eptr->bdring_lock));
+
+        if (bd_processed_save)
+        {
+            log_normal("DmaUnregister pushing %d buffers to user\n", bd_processed_save);
+            (uptr->UserPutPkt)(eptr, ppool->pbuf, bd_processed_save, uptr->privData);
+        }
+
+    spin_lock(&(eptr->bdring_lock));
+
+        result = Dma_BdRingFree(rptr, bd_processed_save, BdPtr);
+        if (result != XST_SUCCESS) {
+            /* Will be freeing the ring below. */
+            printk(KERN_ERR "DmaUnregister: BdRingFree() error %d.\n", result);
+            //return;
+        }
+
+        if(rptr->IsRxChannel)
+        RxEQPool(ppool);
+        else
+        EQPool(ppool);
+    }
+        spin_unlock(&(eptr->bdring_lock));
+
+    /* Now free BD ring itself */
+  if (eptr->descSpaceVA == 0) {
+        printk(KERN_ERR "Unable to free BD ring NULL\n");
+    return;
+  }
+    //spin_lock_bh(&DmaLock);
+  log_verbose(KERN_INFO
+        "XDMA: (descriptor_free) BD ring PA: 0x%x, VA: 0x%x, Size: %d, Delta: %d\n",
+         (eptr->descSpacePA - eptr->delta),
+           (unsigned int) (eptr->descSpaceVA - eptr->delta),
+         (eptr->descSpaceSize + eptr->delta), eptr->delta);
+    pci_free_consistent(pdev, (eptr->descSpaceSize + eptr->delta),
+                       (eptr->descSpaceVA - eptr->delta),
+                       (eptr->descSpacePA - eptr->delta));
+    //spin_unlock_bh(&DmaLock);
+}
+
+#ifdef DEBUG_VERBOSE
+
+void disp_bd_ring(Dma_BdRing *bd_ring)
+{
+  int num_bds = bd_ring->AllCnt;
+  u32 *dptr ;
+  int idx;
+
+/*
+  printk("ChanBase: %p\n", (void *) bd_ring->ChanBase);
+  printk("FirstBdPhysAddr: %p\n", (void *) bd_ring->FirstBdPhysAddr);
+  printk("FirstBdAddr: %p\n", (void *) bd_ring->FirstBdAddr);
+  printk("LastBdAddr: %p\n", (void *) bd_ring->LastBdAddr);
+  printk("Length: %d (0x%0x)\n", bd_ring->Length, bd_ring->Length);
+  printk("RunState: %d (0x%0x)\n", bd_ring->RunState, bd_ring->RunState);
+  printk("Separation: %d (0x%0x)\n", bd_ring->Separation,
+         bd_ring->Separation);
+  printk("BD Count: %d\n", bd_ring->AllCnt);
+
+  printk("\n");
+
+  printk("FreeHead: %p\n", (void *) bd_ring->FreeHead);
+  printk("PreHead: %p\n", (void *) bd_ring->PreHead);
+  printk("HwHead: %p\n", (void *) bd_ring->HwHead);
+  printk("HwTail: %p\n", (void *) bd_ring->HwTail);
+  printk("PostHead: %p\n", (void *) bd_ring->PostHead);
+  printk("BdaRestart: %p\n", (void *) bd_ring->BdaRestart);
+*/
+  printk("Ring %p Contents:\n", bd_ring);
+  printk("Idx Status / UStatusL UStatusH  CAddrL  Control/ SysAddrL SysAddrH NextBD\n");
+  printk("      BC                               CAddrH/BC \n");
+  printk("--- -------- -------- -------- -------- -------- -------- -------- --------\n");
+
+  dptr = (u32 *)bd_ring->FirstBdAddr;
+  for (idx = 0; idx < num_bds; idx++)
+  {
+    int i;
+    printk("%3d ", idx);
+    for(i=0; i<8; i++)
+    {
+            printk("%08x ", *dptr);
+            dptr++;
+    }
+        printk("\n");
+    printk("    ");
+    for(i=0; i<8; i++)
+    {
+            printk("%08x ", *dptr);
+            dptr++;
+    }
+        printk("\n");
+  }
+
+  printk("--------------------------------------- Done ---------------------------------------\n");
+}
+
+#endif
+
+#if defined(DEBUG_VERBOSE) || defined(DEBUG_NORMAL)
+static void ReadConfig(struct pci_dev * pdev)
+{
+  int i;
+  u8 valb;
+  u16 valw;
+  u32 valdw;
+  unsigned long reg_base, reg_len;
+
+  /* Read PCI configuration space */
+  printk(KERN_INFO "PCI Configuration Space:\n");
+  for(i=0; i<0x40; i++)
+  {
+    pci_read_config_byte(pdev, i, &valb);
+    printk("0x%x ", valb);
+    if((i % 0x10) == 0xf)
+      printk("\n");
+  }
+  printk("\n");
+
+  /* Now read each element - one at a time */
+
+  /* Read Vendor ID */
+  pci_read_config_word(pdev, PCI_VENDOR_ID, &valw);
+  printk("Vendor ID: 0x%x, ", valw);
+
+  /* Read Device ID */
+  pci_read_config_word(pdev, PCI_DEVICE_ID, &valw);
+  printk("Device ID: 0x%x, ", valw);
+
+  /* Read Command Register */
+  pci_read_config_word(pdev, PCI_COMMAND, &valw);
+  printk("Cmd Reg: 0x%x, ", valw);
+
+  /* Read Status Register */
+  pci_read_config_word(pdev, PCI_STATUS, &valw);
+  printk("Stat Reg: 0x%x, ", valw);
+
+  /* Read Revision ID */
+  pci_read_config_byte(pdev, PCI_REVISION_ID, &valb);
+  printk("Revision ID: 0x%x, ", valb);
+
+  /* Read Class Code */
+/*
+  pci_read_config_dword(pdev, PCI_CLASS_PROG, &valdw);
+  printk("Class Code: 0x%lx, ", valdw);
+  valdw &= 0x00ffffff;
+  printk("Class Code: 0x%lx, ", valdw);
+*/
+  /* Read Reg-level Programming Interface */
+  pci_read_config_byte(pdev, PCI_CLASS_PROG, &valb);
+  printk("Class Prog: 0x%x, ", valb);
+
+  /* Read Device Class */
+  pci_read_config_word(pdev, PCI_CLASS_DEVICE, &valw);
+  printk("Device Class: 0x%x, ", valw);
+
+  /* Read Cache Line */
+  pci_read_config_byte(pdev, PCI_CACHE_LINE_SIZE, &valb);
+  printk("Cache Line Size: 0x%x, ", valb);
+
+  /* Read Latency Timer */
+  pci_read_config_byte(pdev, PCI_LATENCY_TIMER, &valb);
+  printk("Latency Timer: 0x%x, ", valb);
+
+  /* Read Header Type */
+  pci_read_config_byte(pdev, PCI_HEADER_TYPE, &valb);
+  printk("Header Type: 0x%x, ", valb);
+
+  /* Read BIST */
+  pci_read_config_byte(pdev, PCI_BIST, &valb);
+  printk("BIST: 0x%x\n", valb);
+
+  /* Read all 6 BAR registers */
+  for(i=0; i<=5; i++)
+  {
+    /* Physical address & length */
+    reg_base = pci_resource_start(pdev, i);
+    reg_len = pci_resource_len(pdev, i);
+    printk("BAR%d: Addr:0x%lx Len:0x%lx,  ", i, reg_base, reg_len);
+
+    /* Flags */
+    if((pci_resource_flags(pdev, i) & IORESOURCE_MEM))
+      printk("Region is for memory\n");
+    else if((pci_resource_flags(pdev, i) & IORESOURCE_IO))
+      printk("Region is for I/O\n");
+  }
+    printk("\n");
+
+  /* Read CIS Pointer */
+  pci_read_config_dword(pdev, PCI_CARDBUS_CIS, &valdw);
+  printk("CardBus CIS Pointer: 0x%x, ", valdw);
+
+  /* Read Subsystem Vendor ID */
+  pci_read_config_word(pdev, PCI_SUBSYSTEM_VENDOR_ID, &valw);
+  printk("Subsystem Vendor ID: 0x%x, ", valw);
+
+  /* Read Subsystem Device ID */
+  pci_read_config_word(pdev, PCI_SUBSYSTEM_ID, &valw);
+  printk("Subsystem Device ID: 0x%x\n", valw);
+
+  /* Read Expansion ROM Base Address */
+  pci_read_config_dword(pdev, PCI_ROM_ADDRESS, &valdw);
+  printk("Expansion ROM Base Address: 0x%x\n", valdw);
+
+  /* Read IRQ Line */
+  pci_read_config_byte(pdev, PCI_INTERRUPT_LINE, &valb);
+  printk("IRQ Line: 0x%x, ", valb);
+
+  /* Read IRQ Pin */
+  pci_read_config_byte(pdev, PCI_INTERRUPT_PIN, &valb);
+  printk("IRQ Pin: 0x%x, ", valb);
+
+  /* Read Min Gnt */
+  pci_read_config_byte(pdev, PCI_MIN_GNT, &valb);
+  printk("Min Gnt: 0x%x, ", valb);
+
+  /* Read Max Lat */
+  pci_read_config_byte(pdev, PCI_MAX_LAT, &valb);
+  printk("Max Lat: 0x%x\n", valb);
+}
+#endif
+
+#ifdef DEBUG_VERBOSE
+static void ReadRoot(struct pci_dev * pdev)
+{
+  int i;
+  u8 valb;
+  struct pci_bus * parent;
+  struct pci_bus * me;
+
+  /* Read PCI configuration space for all devices on this bus */
+  parent = pdev->bus->parent;
+  for(i=0; i<256; i++)
+  {
+    pci_bus_read_config_byte(parent, 8, i, &valb);
+    printk("%02x ", valb);
+    if(!((i+1) % 16)) printk("\n");
+  }
+
+  printk("Device %p details:\n", pdev);
+  printk("Bus_list %p\n", &(pdev->bus_list));
+  printk("Bus %p\n", pdev->bus);
+  printk("Subordinate %p\n", pdev->subordinate);
+  printk("Sysdata %p\n", pdev->sysdata);
+  printk("Procent %p\n", pdev->procent);
+  printk("Devfn %d\n", pdev->devfn);
+  printk("Vendor %x\n", pdev->vendor);
+  printk("Device %x\n", pdev->device);
+  printk("Subsystem_vendor %x\n", pdev->subsystem_vendor);
+  printk("Subsystem_device %x\n", pdev->subsystem_device);
+  printk("Class %d\n", pdev->class);
+  printk("Hdr_type %d\n", pdev->hdr_type);
+  printk("Rom_base_reg %d\n", pdev->rom_base_reg);
+  printk("Pin %d\n", pdev->pin);
+  printk("Driver %p\n", pdev->driver);
+  printk("Dma_mask %lx\n", (unsigned long)(pdev->dma_mask));
+  printk("Vendor_compatible: ");
+  //for(i=0; i<DEVICE_COUNT_COMPATIBLE; i++)
+  //  printk("%x ", pdev->vendor_compatible[i]);
+  //printk("\n");
+  //printk("Device_compatible: ");
+  //for(i=0; i<DEVICE_COUNT_COMPATIBLE; i++)
+  //  printk("%x ", pdev->device_compatible[i]);
+  //printk("\n");
+  printk("Cfg_size %d\n", pdev->cfg_size);
+  printk("Irq %d\n", pdev->irq);
+  printk("Transparent %d\n", pdev->transparent);
+  printk("Multifunction %d\n", pdev->multifunction);
+  //printk("Is_enabled %d\n", pdev->is_enabled);
+  printk("Is_busmaster %d\n", pdev->is_busmaster);
+  printk("No_msi %d\n", pdev->no_msi);
+  printk("No_dld2 %d\n", pdev->no_d1d2);
+  printk("Block_ucfg_access %d\n", pdev->block_ucfg_access);
+  printk("Broken_parity_status %d\n", pdev->broken_parity_status);
+  printk("Msi_enabled %d\n", pdev->msi_enabled);
+  printk("Msix_enabled %d\n", pdev->msix_enabled);
+  printk("Rom_attr_enabled %d\n", pdev->rom_attr_enabled);
+
+  me = pdev->bus;
+  printk("Bus details:\n");
+  printk("Parent %p\n", me->parent);
+  printk("Children %p\n", &(me->children));
+  printk("Devices %p\n", &(me->devices));
+  printk("Self %p\n", me->self);
+  printk("Sysdata %p\n", me->sysdata);
+  printk("Procdir %p\n", me->procdir);
+  printk("Number %d\n", me->number);
+  printk("Primary %d\n", me->primary);
+  printk("Secondary %d\n", me->secondary);
+  printk("Subordinate %d\n", me->subordinate);
+  printk("Name %s\n", me->name);
+  printk("Bridge_ctl %d\n", me->bridge_ctl);
+  printk("Bridge %p\n", me->bridge);
+}
+#endif
+
+static void ReadDMAEngineConfiguration(struct pci_dev * pdev, struct privData * dmaInfo)
+{
+    u32 base, offset;
+    u32 val, type, dirn, num, bc;
+    int i;
+    Dma_Engine * eptr;
+
+    /* DMA registers are in BAR0 */
+    base = (u32)(dmaInfo->barInfo[0].baseVAddr);
+
+    log_verbose(KERN_INFO "Hardware design version %x\n", XIo_In32(base+0x8000));
+
+    /* Walk through the capability register of all DMA engines */
+    for(offset = DMA_OFFSET, i=0; offset < DMA_SIZE; offset += DMA_ENGINE_PER_SIZE, i++)
+    {
+        log_verbose(KERN_INFO "Reading engine capability from %x\n",
+                                            (base+offset+REG_DMA_ENG_CAP));
+        val = Dma_mReadReg((base+offset), REG_DMA_ENG_CAP);
+        log_verbose(KERN_INFO "REG_DMA_ENG_CAP returned %x\n", val);
+
+        if(val & DMA_ENG_PRESENT_MASK)
+        {
+            log_verbose(KERN_INFO "Engine capability is %x\n", val);
+            eptr = &(dmaInfo->Dma[i]);
+
+            log_verbose(KERN_INFO "DMA Engine present at offset %x: ", offset);
+
+            dirn = (val & DMA_ENG_DIRECTION_MASK);
+            if(dirn == DMA_ENG_C2S)
+                log_verbose(KERN_INFO"C2S, ");
+            else
+                log_verbose(KERN_INFO"S2C, ");
+
+            type = (val & DMA_ENG_TYPE_MASK);
+            if(type == DMA_ENG_BLOCK)
+                log_verbose(KERN_INFO"Block DMA, ");
+            else if(type == DMA_ENG_PACKET)
+                log_verbose(KERN_INFO"Packet DMA, ");
+            else
+                log_verbose(KERN_INFO"Unknown DMA %x, ", type);
+
+            num = (val & DMA_ENG_NUMBER) >> DMA_ENG_NUMBER_SHIFT;
+            log_verbose(KERN_INFO"Eng. Number %d, ", num);
+
+            bc = (val & DMA_ENG_BD_MAX_BC) >> DMA_ENG_BD_MAX_BC_SHIFT;
+            log_verbose(KERN_INFO"Max Byte Count 2^%d\n", bc);
+
+            if(type != DMA_ENG_PACKET) {
+                log_normal(KERN_ERR "This driver is capable of only Packet DMA\n");
+                continue;
+            }
+
+            /* Initialise this engine's data structure. This will also
+             * reset the DMA engine.
+             */
+            Dma_Initialize(eptr, (base + offset), dirn);
+            eptr->pdev = pdev;
+
+            dmaInfo->engineMask |= (1LL << i);
+        }
+    }
+    log_verbose(KERN_INFO "Engine mask is 0x%llx\n", dmaInfo->engineMask);
+}
+
+/* Character device file operations */
+static int xdma_dev_open(struct inode * in, struct file * filp)
+{
+    if(DriverState != INITIALIZED)
+    {
+        printk("Driver not yet ready!\n");
+        return -1;
+    }
+#ifdef USE_LATER
+    if(UserOpen)
+    {
+        printk("Device already in use\n");
+        return -EBUSY;
+    }
+#endif
+    spin_lock(&DmaStatsLock);
+    UserOpen++;                 /* To prevent more than one GUI */
+    spin_unlock(&DmaStatsLock);
+
+    return 0;
+}
+
+static int xdma_dev_release(struct inode * in, struct file * filp)
+{
+    if(!UserOpen)
+    {
+        /* Should not come here */
+        printk("Device not in use\n");
+        return -EFAULT;
+    }
+
+    spin_lock(&DmaStatsLock);
+    UserOpen-- ;
+    spin_unlock(&DmaStatsLock);
+
+    return 0;
+}
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,36)
+static int xdma_dev_ioctl(struct inode * in, struct file * filp,
+                          unsigned int cmd, unsigned long arg)
+#else
+static long xdma_dev_ioctl(struct file * filp,
+                          unsigned int cmd, unsigned long arg)
+#endif
+{
+    int retval=0;
+    EngState eng;
+    EngStatsArray es;
+    TRNStatsArray tsa;
+    SWStatsArray ssa;
+    DMAStatistics * ds;
+    TRNStatistics * ts;
+    SWStatistics * ss;
+    TestCmd tc;
+    UserState ustate;
+    PCIState pcistate;
+    int len, i;
+    Dma_Engine * eptr;
+    Dma_BdRing * rptr;
+    UserPtrs * uptr;
+
+    if(DriverState != INITIALIZED)
+    {
+        /* Should not come here */
+        printk("Driver not yet ready!\n");
+        return -1;
+    }
+
+    /* Check cmd type and value */
+    if(_IOC_TYPE(cmd) != XPMON_MAGIC) return -ENOTTY;
+    if(_IOC_NR(cmd) > XPMON_MAX_CMD) return -ENOTTY;
+
+    /* Check read/write and corresponding argument */
+    if(_IOC_DIR(cmd) & _IOC_READ)
+        if(!access_ok(VERIFY_WRITE, (void *)arg, _IOC_SIZE(cmd)))
+            return -EFAULT;
+    if(_IOC_DIR(cmd) & _IOC_WRITE)
+        if(!access_ok(VERIFY_READ, (void *)arg, _IOC_SIZE(cmd)))
+            return -EFAULT;
+
+    /* Looks ok, let us continue */
+    switch(cmd)
+    {
+    case IGET_TEST_STATE:
+        if(copy_from_user(&tc, (TestCmd *)arg, sizeof(TestCmd)))
+        {
+            printk("copy_from_user failed\n");
+            retval = -EFAULT;
+            break;
+        }
+
+        i = tc.Engine;
+        //printk("For engine %d\n", i);
+
+        /* First, check if requested engine is valid */
+        if((i >= MAX_DMA_ENGINES) ||
+          (!((dmaData->engineMask) & (1LL << i))))
+        {
+            printk("Invalid engine %d\n", i);
+            retval = -EFAULT;
+            break;
+        }
+        eptr = &(dmaData->Dma[i]);
+        uptr = &(eptr->user);
+
+        /* Then check if the user function registered */
+        if((eptr->EngineState != USER_ASSIGNED) ||
+           (uptr->UserGetState == NULL))
+        {
+            log_normal(KERN_ERR "UserGetState function does not exist\n");
+            retval = -EFAULT;
+            break;
+        }
+
+    if(!(uptr->UserGetState)(eptr, &ustate, uptr->privData))
+        {
+            tc.TestMode = ustate.TestMode;
+            tc.MinPktSize = ustate.MinPktSize;
+            tc.MaxPktSize = ustate.MaxPktSize;
+            if(copy_to_user((TestCmd *)arg, &tc, sizeof(TestCmd)))
+            {
+                printk("copy_to_user failed\n");
+                retval = -EFAULT;
+                break;
+            }
+        }
+        else
+        {
+            printk("UserGetState returned failure\n");
+            retval = -EFAULT;
+            break;
+        }
+        break;
+
+    case ISTART_TEST:
+    case ISTOP_TEST:
+        if(copy_from_user(&tc, (TestCmd *)arg, sizeof(TestCmd)))
+        {
+            printk("copy_from_user failed\n");
+            retval = -EFAULT;
+            break;
+        }
+
+        i = tc.Engine;
+        //printk("For engine %d\n", i);
+
+        /* First, check if requested engine is valid */
+        if((i >= MAX_DMA_ENGINES) ||
+          (!((dmaData->engineMask) & (1LL << i))))
+        {
+            printk("Invalid engine %d\n", i);
+            retval = -EFAULT;
+            break;
+        }
+        eptr = &(dmaData->Dma[i]);
+        uptr = &(eptr->user);
+
+        /* Then check if the user function registered */
+        if((eptr->EngineState != USER_ASSIGNED) ||
+           (uptr->UserGetState == NULL))
+        {
+            log_normal(KERN_ERR "UserSetState function does not exist\n");
+            retval = -EFAULT;
+            break;
+        }
+
+        /* Use the whole bitmask, because we should not disturb any
+         * other running test.
+         */
+        log_verbose("For engine %d, TestMode %x\n", i, tc.TestMode);
+        ustate.TestMode = tc.TestMode;
+        ustate.MinPktSize = tc.MinPktSize;
+        ustate.MaxPktSize = tc.MaxPktSize;
+    retval = (uptr->UserSetState)(eptr, &ustate, uptr->privData);
+        if(retval)
+            printk("UserSetState returned failure %d\n", retval);
+        break;
+
+    case IGET_PCI_STATE:
+        ReadPCIState(dmaData->pdev, &pcistate);
+        if(copy_to_user((PCIState *)arg, &pcistate, sizeof(PCIState)))
+        {
+            printk("copy_to_user failed\n");
+            retval = -EFAULT;
+            break;
+        }
+        break;
+
+    case IGET_ENG_STATE:
+        if(copy_from_user(&eng, (EngState *)arg, sizeof(EngState)))
+        {
+            printk("\ncopy_from_user failed\n");
+            retval = -EFAULT;
+            break;
+        }
+
+        i = eng.Engine;
+        //printk("For engine %d\n", i);
+
+        /* First, check if requested engine is valid */
+        if((i >= MAX_DMA_ENGINES) ||
+          (!((dmaData->engineMask) & (1LL << i))))
+        {
+            printk("Invalid engine %d\n", i);
+            retval = -EFAULT;
+            break;
+        }
+        eptr = &(dmaData->Dma[i]);
+        rptr = &(eptr->BdRing);
+        uptr = &(eptr->user);
+
+        /* Then check if the user function registered */
+        if((eptr->EngineState != USER_ASSIGNED) ||
+           (uptr->UserGetState == NULL))
+        {
+            log_normal(KERN_ERR "UserGetState function does not exist\n");
+            retval = -EFAULT;
+            break;
+        }
+
+        /* First, get the user state */
+    if(!(uptr->UserGetState)(eptr, &ustate, uptr->privData))
+        {
+            eng.Buffers = ustate.Buffers;
+            eng.MinPktSize = ustate.MinPktSize;
+            eng.MaxPktSize = ustate.MaxPktSize;
+            eng.TestMode = ustate.TestMode;
+        }
+        else
+        {
+            printk("UserGetState returned failure\n");
+            retval = -EFAULT;
+            break;
+        }
+
+        /* Now add the DMA state */
+        eng.BDs = DMA_BD_CNT;
+        eng.BDerrs = rptr->BDerrs;
+        eng.BDSerrs = rptr->BDSerrs;
+#ifdef TH_BH_ISR
+        eng.IntEnab = 1;
+#else
+        eng.IntEnab = 0;
+#endif
+        if(copy_to_user((EngState *)arg, &eng, sizeof(EngState)))
+        {
+            printk("copy_to_user failed\n");
+            retval = -EFAULT;
+            break;
+        }
+        break;
+
+    case IGET_DMA_STATISTICS:
+        if(copy_from_user(&es, (EngStatsArray *)arg, sizeof(EngStatsArray)))
+        {
+            printk("copy_from_user failed\n");
+            retval = -1;
+            break;
+        }
+
+        ds = es.engptr;
+        len = 0;
+        for(i=0; i<es.Count; i++)
+        {
+            DMAStatistics from;
+            int j;
+
+            /* Must copy in a round-robin manner so that reporting is fair */
+            for(j=0; j<MAX_DMA_ENGINES; j++)
+            {
+                if(!dstatsNum[j]) continue;
+
+                spin_lock(&DmaStatsLock);
+                from = DStats[j][dstatsRead[j]];
+                from.Engine = j;
+                dstatsNum[j] -= 1;
+                dstatsRead[j] += 1;
+                if(dstatsRead[j] == MAX_STATS)
+                    dstatsRead[j] = 0;
+                spin_unlock(&DmaStatsLock);
+
+                if(copy_to_user(ds, &from, sizeof(DMAStatistics)))
+                {
+                    printk("copy_to_user failed\n");
+                    retval = -EFAULT;
+                    break;
+                }
+
+                len++;
+                i++;
+                if(i >= es.Count) break;
+                ds++;
+            }
+            if(retval < 0) break;
+        }
+        es.Count = len;
+        if(copy_to_user((EngStatsArray *)arg, &es, sizeof(EngStatsArray)))
+        {
+            printk("copy_to_user failed\n");
+            retval = -EFAULT;
+            break;
+        }
+        break;
+
+    case IGET_TRN_STATISTICS:
+        if(copy_from_user(&tsa, (TRNStatsArray *)arg, sizeof(TRNStatsArray)))
+        {
+            printk("copy_from_user failed\n");
+            retval = -1;
+            break;
+        }
+
+        ts = tsa.trnptr;
+        len = 0;
+        for(i=0; i<tsa.Count; i++)
+        {
+            TRNStatistics from;
+
+            if(!tstatsNum) break;
+
+            spin_lock(&DmaStatsLock);
+            from = TStats[tstatsRead];
+            tstatsNum -= 1;
+            tstatsRead += 1;
+            if(tstatsRead == MAX_STATS)
+                tstatsRead = 0;
+            spin_unlock(&DmaStatsLock);
+
+            if(copy_to_user(ts, &from, sizeof(TRNStatistics)))
+            {
+                printk("copy_to_user failed\n");
+                retval = -EFAULT;
+                break;
+            }
+
+            len++;
+            ts++;
+        }
+        tsa.Count = len;
+        if(copy_to_user((TRNStatsArray *)arg, &tsa, sizeof(TRNStatsArray)))
+        {
+            printk("copy_to_user failed\n");
+            retval = -EFAULT;
+            break;
+        }
+        break;
+
+    case IGET_SW_STATISTICS:
+        if(copy_from_user(&ssa, (SWStatsArray *)arg, sizeof(SWStatsArray)))
+        {
+            printk("copy_from_user failed\n");
+            retval = -1;
+            break;
+        }
+
+        ss = ssa.swptr;
+        len = 0;
+        for(i=0; i<ssa.Count; i++)
+        {
+            SWStatistics from;
+            int j;
+
+            /* Must copy in a round-robin manner so that reporting is fair */
+            for(j=0; j<MAX_DMA_ENGINES; j++)
+            {
+                if(!sstatsNum[j]) continue;
+
+                spin_lock(&DmaStatsLock);
+                from = SStats[j][sstatsRead[j]];
+                from.Engine = j;
+                sstatsNum[j] -= 1;
+                sstatsRead[j] += 1;
+                if(sstatsRead[j] == MAX_STATS)
+                    sstatsRead[j] = 0;
+                spin_unlock(&DmaStatsLock);
+
+                if(copy_to_user(ss, &from, sizeof(SWStatistics)))
+                {
+                    printk("copy_to_user failed\n");
+                    retval = -EFAULT;
+                    break;
+                }
+
+                len++;
+                i++;
+                if(i >= ssa.Count) break;
+                ss++;
+            }
+            if(retval < 0) break;
+        }
+        ssa.Count = len;
+        if(copy_to_user((SWStatsArray *)arg, &ssa, sizeof(SWStatsArray)))
+        {
+            printk("copy_to_user failed\n");
+            retval = -EFAULT;
+            break;
+        }
+        break;
+
+    default:
+        printk("Invalid command %d\n", cmd);
+        retval = -1;
+        break;
+    }
+
+    return retval;
+}
+
+static int ReadPCIState(struct pci_dev * pdev, PCIState * pcistate)
+{
+  int pos;
+  u16 valw;
+  u8 valb;
+#if defined(K7_TRD)
+    u32 base;
+#endif
+
+    /* Since probe has succeeded, indicates that link is up. */
+    pcistate->LinkState = LINK_UP;
+    pcistate->VendorId = PCI_VENDOR_ID_DMA;
+    pcistate->DeviceId = PCI_DEVICE_ID_DMA;
+
+    /* Read Interrupt setting - Legacy or MSI/MSI-X */
+    pci_read_config_byte(pdev, PCI_INTERRUPT_PIN, &valb);
+    if(!valb)
+    {
+        if(pci_find_capability(pdev, PCI_CAP_ID_MSIX))
+            pcistate->IntMode = INT_MSIX;
+        else if(pci_find_capability(pdev, PCI_CAP_ID_MSI))
+            pcistate->IntMode = INT_MSI;
+        else
+            pcistate->IntMode = INT_NONE;
+    }
+    else if((valb >= 1) && (valb <= 4))
+        pcistate->IntMode = INT_LEGACY;
+    else
+        pcistate->IntMode = INT_NONE;
+
+    if((pos = pci_find_capability(pdev, PCI_CAP_ID_EXP)))
+    {
+        /* Read Link Status */
+        pci_read_config_word(pdev, pos+PCI_EXP_LNKSTA, &valw);
+        pcistate->LinkSpeed = (valw & 0x0003);
+        pcistate->LinkWidth = (valw & 0x03f0) >> 4;
+
+        /* Read MPS & MRRS */
+        pci_read_config_word(pdev, pos+PCI_EXP_DEVCTL, &valw);
+        pcistate->MPS = 128 << ((valw & PCI_EXP_DEVCTL_PAYLOAD) >> 5);
+        pcistate->MRRS = 128 << ((valw & PCI_EXP_DEVCTL_READRQ) >> 12);
+    }
+    else
+    {
+        printk("Cannot find PCI Express Capabilities\n");
+        pcistate->LinkSpeed = pcistate->LinkWidth = 0;
+        pcistate->MPS = pcistate->MRRS = 0;
+    }
+
+#if defined(K7_TRD)
+    /* Read Initial Flow Control Credits information */
+    base = (u32)(dmaData->barInfo[0].baseVAddr);
+#ifdef USE_LATER
+    pcistate->InitFCCplD = XIo_In32(base+0x8210) & 0x00000FFF;
+    pcistate->InitFCCplH = XIo_In32(base+0x8214) & 0x000000FF;
+    pcistate->InitFCNPD  = XIo_In32(base+0x8218) & 0x00000FFF;
+    pcistate->InitFCNPH  = XIo_In32(base+0x821C) & 0x000000FF;
+    pcistate->InitFCPD   = XIo_In32(base+0x8220) & 0x00000FFF;
+    pcistate->InitFCPH   = XIo_In32(base+0x8224) & 0x000000FF;
+#endif
+
+    pcistate->InitFCCplD = XIo_In32(base+0x901c) & 0x00000FFF;
+    pcistate->InitFCCplH = XIo_In32(base+0x9020) & 0x000000FF;
+    pcistate->InitFCNPD  = XIo_In32(base+0x9024) & 0x00000FFF;
+    pcistate->InitFCNPH  = XIo_In32(base+0x9028) & 0x000000FF;
+    pcistate->InitFCPD   = XIo_In32(base+0x902c) & 0x00000FFF;
+    pcistate->InitFCPH   = XIo_In32(base+0x9030) & 0x000000FF;
+   // printk("#####FCD %d FCH %d FCNPD %d FCNPH %d FCPD %d FCPH %d#####\n",pcistate->InitFCCplD,pcistate->InitFCCplH,pcistate->InitFCNPD,pcistate->InitFCNPH,pcistate->InitFCPD ,pcistate->InitFCPH);
+    pcistate->Version    = XIo_In32(base+0x9000);
+#endif
+
+    return 0;
+}
+
+int tx_pkt_handler (void *pdev)
+{
+	int i;
+	struct privData *lp = NULL;
+	long rc;
+	Dma_Engine *eptr = NULL;
+	
+        set_user_nice(current, -20); //highest priority
+
+	rc = sched_setaffinity(current->pid, cpumask_of(1));
+	if (rc != 0)
+		printk(KERN_ERR"Couldn't set affinity to 2nd cpu (%ld)\n", rc);
+
+	lp = pci_get_drvdata(pdev);
+
+	while(1) 
+	{
+		/* Suspend for 1 tick */
+		set_current_state(TASK_UNINTERRUPTIBLE);
+		//set_current_state(TASK_INTERRUPTIBLE);
+              //  msleep(1);
+		schedule_timeout (1); //10ms on ZynQ
+    for(i=0; i<MAX_DMA_ENGINES; i++)
+    {
+#ifdef TH_BH_ISR
+        /* Do housekeeping only if adequate time has elapsed since
+         * last ISR.
+         */
+        if(jiffies < (LastIntr[i] + (HZ/50))) continue;
+#endif
+
+        if(!((lp->engineMask) & (1LL << i)))
+            continue;
+
+        eptr = &(lp->Dma[i]);
+         if((eptr->EngineState != USER_ASSIGNED) || ((eptr->BdRing).IsRxChannel == 1)) //Not processing Tx channels.
+            continue;
+
+        /* The spinlocks need to be handled within this function, so
+         * don't do them here.
+         */
+        PktHandler(i, eptr);
+		break;//TODO: Comment out break when we have more than 1 Tx and Rx channels.
+    }
+        }
+
+	return 0;
+}
+
+int rx_pkt_handler (void *pdev)
+{
+	int i;
+	struct privData *lp = NULL;
+	long rc;
+	Dma_Engine *eptr = NULL;
+	
+        set_user_nice(current, -20); //highest priority
+
+	rc = sched_setaffinity(current->pid, cpumask_of(0));
+	if (rc != 0)
+		printk(KERN_ERR"Couldn't set affinity to 1st cpu (%ld)\n", rc);
+
+	lp = pci_get_drvdata(pdev);
+
+	while(1) 
+	{
+		/* Suspend for 1 tick */
+		set_current_state(TASK_UNINTERRUPTIBLE);
+	//	set_current_state(TASK_INTERRUPTIBLE);
+           //     msleep(1);
+		schedule_timeout (1); //10ms on ZynQ
+    for(i=0; i<MAX_DMA_ENGINES; i++)
+    {
+#ifdef TH_BH_ISR
+        /* Do housekeeping only if adequate time has elapsed since
+         * last ISR.
+         */
+        if(jiffies < (LastIntr[i] + (HZ/50))) continue;
+#endif
+
+        if(!((lp->engineMask) & (1LL << i)))
+            continue;
+
+        eptr = &(lp->Dma[i]);
+         if((eptr->EngineState != USER_ASSIGNED) || ((eptr->BdRing).IsRxChannel != 1)) //Not processing Tx channels.
+            continue;
+
+        /* The spinlocks need to be handled within this function, so
+         * don't do them here.
+         */
+        rxPktHandler(i, eptr);
+		break;//TODO: Comment out break when we have more than 1 Tx and Rx channels.
+    }
+        }
+
+	return 0;
+}
+#ifdef USE_LATER
+int rx_buf_pumper (void *pdev)
+{
+	int i;
+	struct privData *lp = NULL;
+	long rc;
+	Dma_Engine *eptr = NULL;
+	
+        set_user_nice(current, -20); //highest priority
+
+	rc = sched_setaffinity(current->pid, cpumask_of(0));
+	if (rc != 0)
+		printk(KERN_ERR"Couldn't set affinity to 1st cpu (%ld)\n", rc);
+
+	lp = pci_get_drvdata(pdev);
+
+	while(1) 
+	{
+		/* Suspend for 1 tick */
+		set_current_state(TASK_UNINTERRUPTIBLE);
+	//	set_current_state(TASK_INTERRUPTIBLE);
+           //     msleep(1);
+		schedule_timeout (1); //10ms on ZynQ
+    for(i=0; i<MAX_DMA_ENGINES; i++)
+    {
+#ifdef TH_BH_ISR
+        /* Do housekeeping only if adequate time has elapsed since
+         * last ISR.
+         */
+        if(jiffies < (LastIntr[i] + (HZ/50))) continue;
+#endif
+
+        if(!((lp->engineMask) & (1LL << i)))
+            continue;
+
+        eptr = &(lp->Dma[i]);
+         if((eptr->EngineState != USER_ASSIGNED) || ((eptr->BdRing).IsRxChannel != 1)) //Not processing Tx channels.
+            continue;
+
+        /* The spinlocks need to be handled within this function, so
+         * don't do them here.
+         */
+        spin_lock(&(eptr->bdring_lock));
+        
+        DmaSetupRecvBuffers(pdev, eptr);
+        spin_unlock(&(eptr->bdring_lock));
+		break;//TODO: Comment out break when we have more than 1 Tx and Rx channels.
+    }
+        }
+
+	return 0;
+}
+#endif
+
+/********************************************************************/
+/*  PCI probing function */
+/********************************************************************/
+//static int __devinit xdma_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
+static int xdma_probe(struct pci_dev *pdev, const struct pci_device_id *ent)
+{
+    int pciRet, chrRet;
+    int i;
+    dev_t xdmaDev;
+    static struct file_operations xdmaDevFileOps;
+
+    /* Initialize device before it is used by driver. Ask low-level
+     * code to enable I/O and memory. Wake up the device if it was
+     * suspended. Beware, this function can fail.
+     */
+  pciRet = pci_enable_device(pdev);
+  if (pciRet < 0)
+    {
+        printk(KERN_ERR "PCI device enable failed.\n");
+        return pciRet;
+    }
+
+    /* Initialise packet pools for passing of packet arrays between this
+     * and user drivers.
+     */
+    for(i=0; i<MAX_POOL; i++)
+    {
+        pktPool[i].pbuf = pktArray[i];      // Associate array with pool.
+
+        if(i == (MAX_POOL-1))
+            pktPool[i].next = NULL;
+        else
+            pktPool[i].next = &pktPool[i+1];
+    }
+    pktPoolTail = &pktPool[MAX_POOL-1];
+    pktPoolHead = &pktPool[0];
+	
+    for(i=0; i<MAX_POOL; i++)
+    {
+        RxpktPool[i].pbuf = RxpktArray[i];      // Associate array with pool.
+
+        if(i == (MAX_POOL-1))
+            RxpktPool[i].next = NULL;
+        else
+            RxpktPool[i].next = &RxpktPool[i+1];
+    }
+    RxpktPoolTail = &RxpktPool[MAX_POOL-1];
+    RxpktPoolHead = &RxpktPool[0];
+	
+#ifdef DEBUG_VERBOSE
+    for(i=0; i<MAX_POOL; i++)
+        printk("pktPool[%d] %p pktarray %p\n", i, &pktPool[i], pktPool[i].pbuf);
+    printk("pktPoolHead %p pktPoolTail %p\n", pktPoolHead, pktPoolTail);
+#endif
+    /* Allocate space for holding driver-private data - for storing driver
+     * context.
+     */
+    dmaData = kmalloc(sizeof(struct privData), GFP_KERNEL);
+    if(dmaData == NULL)
+    {
+        printk(KERN_ERR "Unable to allocate DMA private data.\n");
+        pci_disable_device(pdev);
+        return XST_FAILURE;
+    }
+//printk("dmaData at %p\n", dmaData);
+    dmaData->barMask = 0;
+    dmaData->engineMask = 0;
+    dmaData->userCount = 0;
+
+#if defined(DEBUG_NORMAL) || defined(DEBUG_VERBOSE)
+  /* Display PCI configuration space of device. */
+  ReadConfig(pdev);
+#endif
+
+#ifdef DEBUG_VERBOSE
+  /* Display PCI information on parent. */
+  ReadRoot(pdev);
+#endif
+
+    /*
+     * Enable bus-mastering on device. Calls pcibios_set_master() to do
+     * the needed architecture-specific settings.
+     */
+  pci_set_master(pdev);
+
+    /* Reserve PCI I/O and memory resources. Mark all PCI regions
+     * associated with PCI device as being reserved by owner. Do not
+     * access any address inside the PCI regions unless this call returns
+     * successfully.
+     */
+    pciRet = pci_request_regions(pdev, DRIVER_NAME);
+    if (pciRet < 0) {
+        printk(KERN_ERR "Could not request PCI regions.\n");
+        kfree(dmaData);
+        pci_disable_device(pdev);
+        return pciRet;
+    }
+
+    /* Returns success if PCI is capable of 32-bit DMA */
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,36)
+    pciRet = pci_set_dma_mask(pdev, DMA_32BIT_MASK);
+#else
+    pciRet = pci_set_dma_mask(pdev, DMA_BIT_MASK(32));
+#endif
+    if (pciRet < 0) {
+        printk(KERN_ERR "pci_set_dma_mask failed\n");
+        pci_release_regions(pdev);
+        kfree(dmaData);
+        pci_disable_device(pdev);
+        return pciRet;
+    }
+
+    /* First read all the BAR-related information. Then read all the
+     * DMA engine information. Map the BAR region to the system only
+     * when it is needed, for example, when a user requests it.
+     */
+    for(i=0; i<MAX_BARS; i++) {
+        u32 size;
+
+        /* Atleast BAR0 must be there. */
+        if ((size = pci_resource_len(pdev, i)) == 0) {
+            if (i == 0) {
+                printk(KERN_ERR "BAR 0 not valid, aborting.\n");
+                pci_release_regions(pdev);
+                kfree(dmaData);
+                pci_disable_device(pdev);
+                return XST_FAILURE;
+            }
+            else
+                continue;
+        }
+        /* Set a bitmask for all the BARs that are present. */
+        else
+            (dmaData->barMask) |= ( 1 << i );
+
+        /* Check all BARs for memory-mapped or I/O-mapped. The driver is
+         * intended to be memory-mapped.
+         */
+        if (!(pci_resource_flags(pdev, i) & IORESOURCE_MEM)) {
+            printk(KERN_ERR "BAR %d is of wrong type, aborting.\n", i);
+            pci_release_regions(pdev);
+            kfree(dmaData);
+            pci_disable_device(pdev);
+            return XST_FAILURE;
+        }
+
+        /* Get base address of device memory and length for all BARs */
+        dmaData->barInfo[i].basePAddr = pci_resource_start(pdev, i);
+        dmaData->barInfo[i].baseLen = size;
+
+        /* Map bus memory to CPU space. The ioremap may fail if size
+         * requested is too long for kernel to provide as a single chunk
+         * of memory, especially if users are sharing a BAR region. In
+         * such a case, call ioremap for more number of smaller chunks
+         * of memory. Or mapping should be done based on user request
+         * with user size. Neither is being done now - maybe later.
+         */
+        if((dmaData->barInfo[i].baseVAddr =
+            ioremap((dmaData->barInfo[i].basePAddr), size)) == 0UL)
+        {
+            printk(KERN_ERR "Cannot map BAR %d space, invalidating.\n", i);
+            (dmaData->barMask) &= ~( 1 << i );
+        }
+        else
+            log_verbose(KERN_INFO "[BAR %d] Base PA %x Len %d VA %x\n", i,
+                (u32) (dmaData->barInfo[i].basePAddr),
+                (u32) (dmaData->barInfo[i].baseLen),
+                (u32) (dmaData->barInfo[i].baseVAddr));
+    }
+    log_verbose(KERN_INFO "Bar mask is 0x%x\n", (dmaData->barMask));
+  log_normal(KERN_INFO "DMA Base VA %x\n",
+                                (u32)(dmaData->barInfo[0].baseVAddr));
+
+    /* Disable global interrupts */
+    Dma_mIntDisable(dmaData->barInfo[0].baseVAddr);
+
+    dmaData->pdev=pdev;
+    dmaData->index = pdev->device;
+
+    /* Initialize DMA common registers? !!!! */
+
+    /* Read DMA engine configuration and initialise data structures */
+    ReadDMAEngineConfiguration(pdev, dmaData);
+
+    /* Save private data pointer in device structure */
+  pci_set_drvdata(pdev, dmaData);
+
+    /* The following code is for registering as a character device driver.
+     * The GUI will use /dev/xdma_state file to read state & statistics.
+     * Incase of any failure, the driver will come up without device
+     * file support, but statistics will still be visible in the system log.
+     */
+    /* First allocate a major/minor number. */
+    chrRet = alloc_chrdev_region(&xdmaDev, 0, 1, "xdma_stat");
+    if(IS_ERR((int *)chrRet))
+        log_normal(KERN_ERR "Error allocating char device region\n");
+    else
+    {
+        /* Register our character device */
+        xdmaCdev = cdev_alloc();
+        if(IS_ERR(xdmaCdev))
+        {
+            log_normal(KERN_ERR "Alloc error registering device driver\n");
+            unregister_chrdev_region(xdmaDev, 1);
+            chrRet = -1;
+        }
+        else
+        {
+            xdmaDevFileOps.owner = THIS_MODULE;
+            xdmaDevFileOps.open = xdma_dev_open;
+            xdmaDevFileOps.release = xdma_dev_release;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,36)
+            xdmaDevFileOps.ioctl = xdma_dev_ioctl;
+#else
+            xdmaDevFileOps.unlocked_ioctl = xdma_dev_ioctl;
+#endif
+            xdmaCdev->owner = THIS_MODULE;
+            xdmaCdev->ops = &xdmaDevFileOps;
+            xdmaCdev->dev = xdmaDev;
+            chrRet = cdev_add(xdmaCdev, xdmaDev, 1);
+            if(chrRet < 0)
+            {
+                log_normal(KERN_ERR "Add error registering device driver\n");
+                unregister_chrdev_region(xdmaDev, 1);
+            }
+        }
+    }
+
+    if(!IS_ERR((int *)chrRet))
+    {
+        log_verbose(KERN_INFO "Device registered with major number %d\n",
+                                            MAJOR(xdmaDev));
+        /* Initialise all stats pointers */
+        for(i=0; i<MAX_DMA_ENGINES; i++)
+        {
+            dstatsRead[i] = dstatsWrite[i] = dstatsNum[i] = 0;
+            sstatsRead[i] = sstatsWrite[i] = sstatsNum[i] = 0;
+            SWrate[i] = 0;
+        }
+        tstatsRead = tstatsWrite = tstatsNum = 0;
+
+        /* Start stats polling routine */
+        log_normal(KERN_INFO "probe: Starting stats poll routine with %x\n",
+                                            (u32)pdev);
+        /* Now start timer */
+        init_timer(&stats_timer);
+        stats_timer.expires=jiffies + HZ;
+        stats_timer.data=(unsigned long) pdev;
+        stats_timer.function = poll_stats;
+        add_timer(&stats_timer);
+    }
+       raw_transmit_init(); 
+    {
+	    tx_cleanup_task = kthread_run(tx_pkt_handler, pdev,"Tx_handler");
+
+	    if (IS_ERR(tx_cleanup_task)) 
+	    {
+		    printk(KERN_ERR"Unable to start TX handler thread!\n");
+	    }
+    }
+    {
+	    rx_task = kthread_run(rx_pkt_handler, pdev,"Rx_handler");
+
+	    if (IS_ERR(rx_task)) 
+	    {
+		    printk(KERN_ERR"Unable to start RX handler thread!\n");
+	    }
+    }
+#ifdef USE_LATER
+    {
+	    rx_freebuf_task = kthread_run(rx_buf_pumper, pdev,"Rxbufpumper");
+
+	    if (IS_ERR(rx_freebuf_task)) 
+	    {
+		    printk(KERN_ERR"Unable to start RX buf pumper thread!\n");
+	    }
+    }
+#endif
+    DriverState = INITIALIZED;
+
+#ifdef TH_BH_ISR
+    /* Now enable interrupts using MSI mode */
+    if(!pci_enable_msi(pdev))
+    {
+        log_normal(KERN_INFO "MSI enabled\n");
+        MSIEnabled = 1;
+    }
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,18)
+    pciRet = request_irq(pdev->irq, DmaInterrupt, SA_SHIRQ, "xdma", pdev);
+#else
+    pciRet = request_irq(pdev->irq, DmaInterrupt, IRQF_SHARED, "xdma", pdev);
+#endif
+
+    if(pciRet)
+    {
+        printk(KERN_ERR "xdma could not allocate interrupt %d\n", pdev->irq);
+        printk(KERN_ERR "Unload driver and try running with polled mode instead\n");
+    }
+
+    /* Set flag to synchronise between ISR and poll_routine */
+    for(i=0; i<MAX_DMA_ENGINES; i++)
+        LastIntr[i] = jiffies;
+
+    /* Now, enable global interrupts. Engine interrupts will be enabled
+     * only when they are used.
+     */
+    Dma_mIntEnable(dmaData->barInfo[0].baseVAddr);
+
+#endif
+
+    log_verbose(KERN_INFO"Value of HZ is %d\n", HZ);
+    log_verbose(KERN_INFO"End of probe\n");
+
+  return 0;
+}
+
+static void xdma_remove(struct pci_dev *pdev)
+{
+  struct privData *lp;
+    int i;
+  Dma_Engine *eptr = NULL,*txeptr = NULL,*rxeptr = NULL;
+  UserPtrs * uptr;
+  Dma_BdRing *rptr;
+ 
+#ifdef TH_BH_ISR
+    u32 girqval, base;
+#endif
+
+#ifdef FIFO_EMPTY_CHECK
+    u32 barBase = (u32)(dmaData->barInfo[0].baseVAddr);  //todo: need to confirm that this will never change
+    //printk("### value of 0x9008 ==> 0x%x", XIo_In32(barBase+0x9008));
+#endif
+
+log_verbose("Came to xdma_remove\n");
+    lp = pci_get_drvdata(pdev);
+
+    /* The driver state flag has already been changed */
+
+    mdelay(1000);
+    for(i=0; i<MAX_DMA_ENGINES; i++) 
+    {
+        eptr = &(lp->Dma[i]);
+        if(eptr->EngineState != USER_ASSIGNED)
+            continue;
+
+        uptr = &(eptr->user);
+        if(!((lp->engineMask) & (1LL << i)))
+            continue;
+
+        rptr = &(eptr->BdRing);
+        if(rptr->IsRxChannel){
+            rxeptr = eptr;//TODO:Revisit code when number of Tx and Rx channels are more than 1.
+        }
+        else {
+            txeptr = eptr;
+        }
+    }
+
+    /* Stop the polling routines */
+    spin_lock(&DmaStatsLock);
+    del_timer_sync(&stats_timer);
+    spin_unlock(&DmaStatsLock);
+    eptr = txeptr;//TODO:Doing this as there is only one transmit and receive path. Need to revisit code.
+    spin_lock(&(eptr->bdring_lock));
+    kthread_stop(tx_cleanup_task);
+   // del_timer_sync(&poll_timer0);
+    spin_unlock(&(eptr->bdring_lock));
+
+    eptr = rxeptr;
+    spin_lock(&(eptr->bdring_lock));
+    kthread_stop(rx_task);
+    spin_unlock(&(eptr->bdring_lock));
+
+#ifdef TH_BH_ISR
+    base = (u32)(dmaData->barInfo[0].baseVAddr);
+    Dma_mIntDisable(base);
+
+    /* Disable MSI and interrupts */
+    free_irq(pdev->irq, pdev);
+    if(MSIEnabled) pci_disable_msi(pdev);
+    girqval = Dma_mReadReg(base, REG_DMA_CTRL_STATUS);
+    //if(girqval & (DMA_INT_ACTIVE_MASK|DMA_INT_PENDING_MASK|DMA_USER_INT_ACTIVE_MASK))
+        //Dma_mWriteReg(base, REG_DMA_CTRL_STATUS, girqval);
+    printk("While disabling interrupts, got %x\n", girqval);
+#endif
+
+#ifdef FIFO_EMPTY_CHECK
+// should be done in a loop so that when more channels are added, code doesn't change.
+    DmaFifoEmptyWait(HANDLE_0,DIR_TYPE_S2C);
+    DmaFifoEmptyWait(HANDLE_1,DIR_TYPE_S2C);
+    DmaFifoEmptyWait(HANDLE_0,DIR_TYPE_C2S);
+    DmaFifoEmptyWait(HANDLE_1,DIR_TYPE_C2S);
+
+    // wait for appropriate time to stabalize
+    mdelay(STABILITY_WAIT_TIME);
+
+#endif
+
+
+#ifdef FIFO_EMPTY_CHECK
+    // do axi-mig reset here.
+    XIo_Out32(barBase + STATUS_REG_OFFSET, 1 << AXI_MIG_RST_SHIFT); //only this bit is writtable. so no need to read and mask.
+
+#endif
+
+    /* Reset DMA - this includes disabling interrupts and DMA. */
+  log_normal(KERN_INFO "Doing DMA reset.\n");
+    for(i=0; i<MAX_DMA_ENGINES; i++)
+    {
+        if((lp->engineMask) & (1LL << i))
+            Dma_Reset(&(lp->Dma[i]));
+    }
+
+    for(i=0; i<MAX_BARS; i++)
+    {
+        if((dmaData->barMask) & ( 1 << i ))
+            iounmap(dmaData->barInfo[i].baseVAddr);
+    }
+
+    if(xdmaCdev != NULL)
+    {
+        printk("Unregistering char device driver\n");
+        cdev_del(xdmaCdev);
+        unregister_chrdev_region(xdmaCdev->dev, 1);
+    }
+
+  log_normal(KERN_INFO "PCI release regions and disable device.\n");
+    pci_release_regions(pdev);
+    pci_disable_device(pdev);
+  pci_set_drvdata(pdev, NULL);
+}
+
+static int __init xdma_init(void)
+{
+  /* Initialize the locks */
+  spin_lock_init(&GlobalDataLock);
+  spin_lock_init(&DmaLock);
+  spin_lock_init(&IntrLock);
+  spin_lock_init(&DmaStatsLock);
+  spin_lock_init(&PktPoolLock);
+  spin_lock_init(&RxPktPoolLock);
+
+  /* Just register the driver. No kernel boot options used. */
+  log_verbose(KERN_INFO "XDMA: Inserting Xilinx base DMA driver in kernel.\n");
+    return pci_register_driver(&xdma_driver);
+}
+
+static void __exit xdma_cleanup(void)
+{
+    struct PktPool * ppool;
+    int oldstate;
+
+    printk("Came to xdma_cleanup\n");
+
+    /* First, change the driver state - so that other entry points
+     * will not make a difference from this point on.
+     */
+    oldstate = DriverState;
+    spin_lock(&GlobalDataLock);
+    DriverState = UNREGISTERING;
+    spin_unlock(&GlobalDataLock);
+
+    /* Then, unregister driver with PCI in order to free up resources */
+    pci_unregister_driver(&xdma_driver);
+
+    if(dmaData != NULL)
+    {
+        printk("User count %d\n", dmaData->userCount);
+        printk("GUI user open? %d\n", UserOpen);
+        kfree(dmaData);
+    }
+    else
+        printk("DriverState still %d\n", oldstate);
+
+    /* Check whether pools are good */
+    ppool = pktPoolHead;
+    while (ppool != NULL)
+    {
+        log_verbose("pktPool %p pktarray %p\n", ppool, ppool->pbuf);
+        ppool = ppool->next;
+    }
+    log_verbose("pktPoolHead %p pktPoolTail %p\n", pktPoolHead, pktPoolTail);
+
+    printk(KERN_INFO "XDMA: Unregistering Xilinx base DMA driver from kernel.\n");
+}
+
+module_init(xdma_init);
+module_exit(xdma_cleanup);
+
+EXPORT_SYMBOL(DmaRegister);
+EXPORT_SYMBOL(DmaUnregister);
+#ifdef FIFO_EMPTY_CHECK
+EXPORT_SYMBOL(DmaFifoEmptyWait);
+#endif
+EXPORT_SYMBOL(DmaSendPkt);
diff --git a/arch/arm/mach-zynq/k7_base_driver/xdma/xdma_bd.h b/arch/arm/mach-zynq/k7_base_driver/xdma/xdma_bd.h
new file mode 100644
index 0000000..2219173
--- /dev/null
+++ b/arch/arm/mach-zynq/k7_base_driver/xdma/xdma_bd.h
@@ -0,0 +1,536 @@
+/*******************************************************************************
+** © Copyright 2011 - 2012 Xilinx, Inc. All rights reserved.
+** This file contains confidential and proprietary information of Xilinx, Inc. and
+** is protected under U.S. and international copyright and other intellectual property laws.
+*******************************************************************************
+**   ____  ____
+**  /   /\/   /
+** /___/  \  /   Vendor: Xilinx
+** \   \   \/
+**  \   \
+**  /   /
+** \   \  /  \   Kintex-7 PCIe-DMA-DDR3 Base Targeted Reference Design
+**  \___\/\___\
+**
+**  Device: xc7k325t
+**  Reference: UG882
+*******************************************************************************
+**
+**  Disclaimer:
+**
+**    This disclaimer is not a license and does not grant any rights to the materials
+**    distributed herewith. Except as otherwise provided in a valid license issued to you
+**    by Xilinx, and to the maximum extent permitted by applicable law:
+**    (1) THESE MATERIALS ARE MADE AVAILABLE "AS IS" AND WITH ALL FAULTS,
+**    AND XILINX HEREBY DISCLAIMS ALL WARRANTIES AND CONDITIONS, EXPRESS, IMPLIED, OR STATUTORY,
+**    INCLUDING BUT NOT LIMITED TO WARRANTIES OF MERCHANTABILITY, NON-INFRINGEMENT, OR
+**    FITNESS FOR ANY PARTICULAR PURPOSE; and (2) Xilinx shall not be liable (whether in contract
+**    or tort, including negligence, or under any other theory of liability) for any loss or damage
+**    of any kind or nature related to, arising under or in connection with these materials,
+**    including for any direct, or any indirect, special, incidental, or consequential loss
+**    or damage (including loss of data, profits, goodwill, or any type of loss or damage suffered
+**    as a result of any action brought by a third party) even if such damage or loss was
+**    reasonably foreseeable or Xilinx had been advised of the possibility of the same.
+
+
+**  Critical Applications:
+**
+**    Xilinx products are not designed or intended to be fail-safe, or for use in any application
+**    requiring fail-safe performance, such as life-support or safety devices or systems,
+**    Class III medical devices, nuclear facilities, applications related to the deployment of airbags,
+**    or any other applications that could lead to death, personal injury, or severe property or
+**    environmental damage (individually and collectively, "Critical Applications"). Customer assumes
+**    the sole risk and liability of any use of Xilinx products in Critical Applications, subject only
+**    to applicable laws and regulations governing limitations on product liability.
+
+**  THIS COPYRIGHT NOTICE AND DISCLAIMER MUST BE RETAINED AS PART OF THIS FILE AT ALL TIMES.
+
+*******************************************************************************/
+/*****************************************************************************/
+/**
+ *
+ * @file xdma_bd.h
+ *
+ * This header provides operations to manage buffer descriptors (BDs) in
+ * support of Northwest Logic's scatter-gather DMA (see xdma.h).
+ *
+ * The API exported by this header defines abstracted macros that allow the
+ * driver to read/write specific BD fields.
+ *
+ * <b>Buffer Descriptors</b>
+ *
+ * A buffer descriptor defines a DMA transaction. The macros defined by this
+ * header file allow access to most fields within a BD to tailor a DMA
+ * transaction according to application and hardware requirements.
+ *
+ * The Dma_Bd structure defines a BD. The organization of this structure is
+ * driven mainly by the hardware for use in scatter-gather DMA transfers.
+ *
+ * <b>Accessor Macros</b>
+ *
+ * Most of the BD attributes can be accessed through macro functions defined
+ * here in this API. The BD fields should be accessed using Dma_mBdRead()
+ * and Dma_mBdWrite() macros. The USR fields are used to carry application
+ * information. For example, they may implement checksum offloading fields
+ * for Ethernet devices.
+ *
+ * <b>Alignment</b>
+ *
+ * In order to improve performance, BDs are required by the DMA engine to
+ * be 32-byte aligned, and are themselves 32 bytes in size, from the hardware
+ * perspective. The software can have a larger data structure for BDs in
+ * order to incorporate private information.
+ *
+ * <pre>
+ * MODIFICATION HISTORY:
+ *
+ * Ver   Who  Date     Changes
+ * ----- ---- -------- -------------------------------------------------------
+ * 1.0   ps   06/30/09 First release
+ * </pre>
+ *
+ *****************************************************************************/
+
+#ifndef XDMA_BD_H   /* prevent circular inclusions */
+#define XDMA_BD_H   /* by using protection macros */
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/***************************** Include Files *********************************/
+
+#include "xio.h"
+
+/************************** Constant Definitions *****************************/
+
+/** @name Buffer Descriptor Alignment
+ *  @{
+ */
+#define DMA_BD_MINIMUM_ALIGNMENT    0x40  /**< Minimum byte alignment
+                                               requirement for descriptors to
+                                               satisfy both hardware/software
+                                               needs */
+/* Other BD characteristics */
+#define DMA_BD_SW_NUM_WORDS         16  /**< SW view of BD size in words */
+#define DMA_BD_HW_NUM_WORDS         8   /**< HW view of BD size in words */
+
+/*@}*/
+
+/** @name Buffer Descriptor offsets
+ *  USR fields are defined by higher level IP. For example, checksum offload
+ *  setup for EMAC type devices. The 1st 8 words are utilized by hardware. Any
+ *  words after the 8th are for software use only.
+ *  @{
+ */
+#define DMA_BD_BUFL_STATUS_OFFSET   0x00 /**< Buffer length + status */
+#define DMA_BD_USRL_OFFSET          0x04 /**< User logic specific - LSBytes */
+#define DMA_BD_USRH_OFFSET          0x08 /**< User logic specific - MSBytes */
+#define DMA_BD_CARDA_OFFSET         0x0C /**< Card address */
+#define DMA_BD_BUFL_CTRL_OFFSET     0x10 /**< Buffer length + control */
+#define DMA_BD_BUFAL_OFFSET         0x14 /**< Buffer address LSBytes */
+#define DMA_BD_BUFAH_OFFSET         0x18 /**< Buffer address MSBytes */
+#define DMA_BD_NDESC_OFFSET         0x1C /**< Next descriptor pointer */
+
+/* Driver stores private information beyond the main BD as understood by
+ * the hardware, such as the virtual address of the associated data buffer,
+ * which it requires for post-DMA processing.
+ */
+//#define DMA_BD_VBUFAL_OFFSET        DMA_BD_CARDA_OFFSET
+#define DMA_BD_VBUFAL_OFFSET        0x20 /**< Buffer virtual address LSBytes */
+#define DMA_BD_VBUFAH_OFFSET        0x24 /**< Buffer virtual address MSBytes */
+
+/* Bit masks for some BD fields */
+#define DMA_BD_BUFL_MASK            0x000FFFFF /**< Byte count */
+#define DMA_BD_STATUS_MASK          0xFF000000 /**< Status Flags */
+#define DMA_BD_CTRL_MASK            0xFF000000 /**< Control Flags */
+
+/* Bit masks for BD control field */
+#define DMA_BD_INT_ERROR_MASK       0x02000000 /**< Intr on error */
+#define DMA_BD_INT_COMP_MASK        0x01000000 /**< Intr on BD completion */
+
+/* Bit masks for BD status field */
+#define DMA_BD_SOP_MASK             0x80000000 /**< Start of packet */
+#define DMA_BD_EOP_MASK             0x40000000 /**< End of packet */
+#define DMA_BD_ERROR_MASK           0x10000000 /**< BD had error */
+#define DMA_BD_USER_HIGH_ZERO_MASK  0x08000000 /**< User High Status zero */
+#define DMA_BD_USER_LOW_ZERO_MASK   0x04000000 /**< User Low Status zero */
+#define DMA_BD_SHORT_MASK           0x02000000 /**< BD not fully used */
+#define DMA_BD_COMP_MASK            0x01000000 /**< BD completed */
+
+/*@}*/
+
+
+/**************************** Type Definitions *******************************/
+
+/**
+ * Dma_Bd is the type for buffer descriptors (BDs).
+ */
+typedef u32 Dma_Bd[DMA_BD_SW_NUM_WORDS];
+
+
+/***************** Macros (Inline Functions) Definitions *********************/
+
+/*****************************************************************************/
+/**
+*
+* Read the given Buffer Descriptor word.
+*
+* @param    BaseAddress is the base address of the BD to read
+* @param    Offset is the word offset to be read
+*
+* @return   The 32-bit value of the field
+*
+* @note
+* C-style signature:
+*    u32 Dma_mBdRead(u32 BaseAddress, u32 Offset)
+*
+******************************************************************************/
+#define Dma_mBdRead(BaseAddress, Offset)        \
+  XIo_In32((unsigned int *)((unsigned int)(BaseAddress) + (unsigned int)(Offset)))
+
+
+/*****************************************************************************/
+/**
+*
+* Write the given Buffer Descriptor word.
+*
+* @param    BaseAddress is the base address of the BD to write
+* @param    Offset is the word offset to be written
+* @param    Data is the 32-bit value to write to the field
+*
+* @return   None.
+*
+* @note
+* C-style signature:
+*    void Dma_mBdWrite(u32 BaseAddress, u32 RegOffset, u32 Data)
+*
+******************************************************************************/
+#define Dma_mBdWrite(BaseAddress, Offset, Data)     \
+  XIo_Out32((unsigned int *)((unsigned int)BaseAddress+(unsigned int)Offset), (unsigned int)Data)
+
+
+/*****************************************************************************/
+/**
+ * Zero out all BD fields
+ *
+ * @param  BdPtr is the BD to operate on
+ *
+ * @return Nothing
+ *
+ * @note
+ * C-style signature:
+ *    void Dma_mBdClear(Dma_Bd* BdPtr)
+ *
+ *****************************************************************************/
+#define Dma_mBdClear(BdPtr)                    \
+  memset((BdPtr), 0, sizeof(Dma_Bd))
+
+
+/*****************************************************************************/
+/**
+ * Set the BD's Control field. This operation is implemented as a read-
+ * modify-write operation.
+ *
+ * @param  BdPtr is the BD to operate on
+ * @param  Data is the value to write to the Control field.
+ *
+ * @note
+ * C-style signature:
+ *    u32 Dma_mBdSetCtrl(Dma_Bd* BdPtr, u32 Data)
+ *
+ *****************************************************************************/
+#define Dma_mBdSetCtrl(BdPtr, Data)                                 \
+  Dma_mBdWrite((BdPtr), DMA_BD_BUFL_CTRL_OFFSET,                  \
+    (Dma_mBdRead((BdPtr), DMA_BD_BUFL_CTRL_OFFSET) &            \
+         DMA_BD_BUFL_MASK) | ((Data) & DMA_BD_CTRL_MASK))
+
+
+/*****************************************************************************/
+/**
+ * Read the BD's Control field.
+ *
+ * @param  BdPtr is the BD to operate on
+ *
+ * @return Word at offset DMA_BD_CTRL_OFFSET.
+ *
+ * @note
+ * C-style signature:
+ *    u32 Dma_mBdGetCtrl(Dma_Bd* BdPtr)
+ *
+ *****************************************************************************/
+#define Dma_mBdGetCtrl(BdPtr)                                       \
+  (Dma_mBdRead((BdPtr), DMA_BD_BUFL_CTRL_OFFSET) & DMA_BD_CTRL_MASK)
+
+
+/*****************************************************************************/
+/**
+ * Set the BD's Status field. This operation is implemented as a read-
+ * modify-write operation.
+ *
+ * @param  BdPtr is the BD to operate on
+ * @param  Data is the value to write to the Status field.
+ *
+ * @note
+ * C-style signature:
+ *    u32 Dma_mBdSetStatus(Dma_Bd* BdPtr, u32 Data)
+ *
+ *****************************************************************************/
+#define Dma_mBdSetStatus(BdPtr, Data)                                 \
+  Dma_mBdWrite((BdPtr), DMA_BD_BUFL_STATUS_OFFSET,                  \
+    (Dma_mBdRead((BdPtr), DMA_BD_BUFL_STATUS_OFFSET) &            \
+         DMA_BD_BUFL_MASK) | ((Data) & DMA_BD_STATUS_MASK))
+
+
+/*****************************************************************************/
+/**
+ * Read the BD's Status field.
+ *
+ * @param  BdPtr is the BD to operate on
+ *
+ * @return Word at offset DMA_BD_STATUS_OFFSET.
+ *
+ * @note
+ * C-style signature:
+ *    u32 Dma_mBdGetStatus(Dma_Bd* BdPtr)
+ *
+ *****************************************************************************/
+#define Dma_mBdGetStatus(BdPtr)                                       \
+  (Dma_mBdRead((BdPtr), DMA_BD_BUFL_STATUS_OFFSET) & DMA_BD_STATUS_MASK)
+
+
+/*****************************************************************************/
+/**
+ * Set the length field for the given BD. The length must be set each time
+ * before a BD is submitted to hardware. The DMA engine requires the value
+ * to be updated both in the control and status fields - ONLY for TX.
+ *
+ * For TX channels, the value passed in should be the number of bytes to
+ * transmit from the TX buffer associated with the given BD.
+ *
+ * For RX channels, the value passed in should be the size of the RX buffer
+ * associated with the given BD in bytes. This is to notify the RX channel
+ * the capability of the RX buffer to avoid buffer overflow.
+ *
+ * @param  BdPtr is the BD to operate on.
+ * @param  LenBytes is the number of bytes to transfer for TX channel or the
+ *         size of receive buffer in bytes for RX channel.
+ *
+ * @note
+ * C-style signature:
+ *    void Dma_mBdSetCtrlLength(Dma_Bd* BdPtr, u32 LenBytes)
+ *    void Dma_mBdSetStatLength(Dma_Bd* BdPtr, u32 LenBytes)
+ *
+ *****************************************************************************/
+#define Dma_mBdSetCtrlLength(BdPtr, LenBytes)                       \
+{                                                                   \
+    Dma_mBdWrite((BdPtr), DMA_BD_BUFL_CTRL_OFFSET,                  \
+      ((Dma_mBdRead((BdPtr), DMA_BD_BUFL_CTRL_OFFSET) &           \
+         DMA_BD_CTRL_MASK) | (LenBytes & DMA_BD_BUFL_MASK)));       \
+}
+
+#define Dma_mBdSetStatLength(BdPtr, LenBytes)                       \
+{                                                                   \
+  Dma_mBdWrite((BdPtr), DMA_BD_BUFL_STATUS_OFFSET,                \
+      ((Dma_mBdRead((BdPtr), DMA_BD_BUFL_STATUS_OFFSET) &         \
+         DMA_BD_STATUS_MASK) | (LenBytes & DMA_BD_BUFL_MASK)));     \
+}
+
+
+/*****************************************************************************/
+/**
+ * Retrieve the length field value from the given BD.  The returned value is
+ * read from the length field updated by the DMA engine.
+ *
+ * For both TX and RX, the value is read from DMA_BD_BUFL_STATUS_OFFSET.
+ * The actual received data could be equal to or smaller than the length
+ * of the original buffer. This field is updated by the DMA engine - it
+ * indicates number of transmitted bytes in the case of S2C and number of
+ * received bytes in the case of C2S.
+ *
+ * @param  BdPtr is the BD to operate on.
+ *
+ * @return The BD length field value set by the DMA engine.
+ *
+ * @note
+ * C-style signature:
+ *    u32 Dma_mBdGetStatLength(Dma_Bd* BdPtr)
+ *
+ *****************************************************************************/
+#define Dma_mBdGetStatLength(BdPtr)                      \
+  (Dma_mBdRead((BdPtr), DMA_BD_BUFL_STATUS_OFFSET) & DMA_BD_BUFL_MASK)
+
+
+/*****************************************************************************/
+/**
+ * Retrieve the length field value from the given BD.  The returned value is
+ * read from the length field provided to the DMA engine.
+ *
+ * This value is read from DMA_BD_BUFL_CTRL_OFFSET. This macro is called
+ * when the driver wants information on the packet yet to be transmitted.
+ *
+ * @param  BdPtr is the BD to operate on.
+ *
+ * @return The BD length field value set by the DMA engine.
+ *
+ * @note
+ * C-style signature:
+ *    u32 Dma_mBdGetCtrlLength(Dma_Bd* BdPtr)
+ *
+ *****************************************************************************/
+#define Dma_mBdGetCtrlLength(BdPtr)                      \
+  (Dma_mBdRead((BdPtr), DMA_BD_BUFL_CTRL_OFFSET) & DMA_BD_BUFL_MASK)
+
+
+/*****************************************************************************/
+/**
+ * Set the ID field of the given BD. The ID is an arbitrary piece of data the
+ * application can associate with a specific BD. For example, the virtual
+ * address of a packet buffer can be stored in the BD, to be used during
+ * post-DMA processing.
+ *
+ * @param  BdPtr is the BD to operate on
+ * @param  Id is a 32 bit quantity to set in the BD
+ *
+ * @note
+ * C-style signature:
+ *    void Dma_mBdSetId(Dma_Bd* BdPtr, void Id)
+ *
+ *****************************************************************************/
+#define Dma_mBdSetId(BdPtr, Id)                                      \
+  (Dma_mBdWrite((BdPtr), DMA_BD_VBUFAL_OFFSET, (u32)(Id)))
+
+
+/*****************************************************************************/
+/**
+ * Retrieve the ID field of the given BD previously set with Dma_mBdSetId.
+ *
+ * @param  BdPtr is the BD to operate on
+ *
+ * @note
+ * C-style signature:
+ *    u32 Dma_mBdGetId(Dma_Bd* BdPtr)
+ *
+ *****************************************************************************/
+#define Dma_mBdGetId(BdPtr) (Dma_mBdRead((BdPtr), DMA_BD_VBUFAL_OFFSET))
+
+
+/*****************************************************************************/
+/**
+ * Set the BD's buffer address. The driver currently supports only
+ * 32-bit addresses.
+ *
+ * @param  BdPtr is the BD to operate on
+ * @param  Addr is the address to set
+ *
+ * @note
+ * C-style signature:
+ *    void Dma_mBdSetBufAddr(Dma_Bd* BdPtr, u32 Addr)
+ *
+ *****************************************************************************/
+#define Dma_mBdSetBufAddr(BdPtr, Addr)                          \
+{                                                               \
+  Dma_mBdWrite((BdPtr), DMA_BD_BUFAL_OFFSET, (u32)(Addr));    \
+  Dma_mBdWrite((BdPtr), DMA_BD_BUFAH_OFFSET, (u32)0);         \
+}
+
+
+/*****************************************************************************/
+/**
+ * Get the BD's buffer address
+ *
+ * @param  BdPtr is the BD to operate on
+ *
+ * @note
+ * C-style signature:
+ *    u32 Dma_mBdGetBufAddr(Dma_Bd* BdPtr)
+ *
+ *****************************************************************************/
+#define Dma_mBdGetBufAddr(BdPtr)                     \
+  (Dma_mBdRead((BdPtr), DMA_BD_BUFAL_OFFSET))
+
+
+/*****************************************************************************/
+/**
+ * Set the BD's user data.
+ *
+ * @param  BdPtr is the BD to operate on
+ * @param  User is the value to set
+ *
+ * @note
+ * C-style signature:
+ *    void Dma_mBdSetUserData(Dma_Bd* BdPtr, unsigned long long User)
+ *
+ *****************************************************************************/
+#define Dma_mBdSetUserData(BdPtr, User)                             \
+{                                                                   \
+    u32 val;                                                        \
+    val = (u32)(User & 0xFFFFFFFFLL);                               \
+  Dma_mBdWrite((BdPtr), DMA_BD_USRL_OFFSET, val);                 \
+    val = (u32)((User>>32) & 0xFFFFFFFFLL);                         \
+  Dma_mBdWrite((BdPtr), DMA_BD_USRH_OFFSET, val);                 \
+}
+
+
+/*****************************************************************************/
+/**
+ * Get the BD's user data.
+ *
+ * @param  BdPtr is the BD to operate on
+ *
+ * @note
+ * C-style signature:
+ *    u32 Dma_mBdGetUserData(Dma_Bd* BdPtr)
+ *
+ *****************************************************************************/
+static inline unsigned long long Dma_mBdGetUserData(Dma_Bd * BdPtr)
+{
+    unsigned long long val;
+    u32 val1, val2;
+  val1 = (Dma_mBdRead((BdPtr), DMA_BD_USRH_OFFSET));
+  val2 = (Dma_mBdRead((BdPtr), DMA_BD_USRL_OFFSET));
+    val = val1;
+    val <<= 32;
+    val |= val2;
+    return val;
+}
+
+
+/*****************************************************************************/
+/**
+ * Compute the virtual address of a descriptor from its physical address
+ *
+ * @param BdPtr is the physical address of the BD
+ *
+ * @returns Virtual address of BdPtr
+ *
+ * @note Assume BdPtr is always a valid BD in the ring
+ * @note RingPtr is an implicit parameter
+ *****************************************************************************/
+#define Dma_mPhysToVirt(BdPtr) \
+  ((u32)(BdPtr) + (RingPtr->FirstBdAddr - RingPtr->FirstBdPhysAddr))
+
+
+/*****************************************************************************/
+/**
+ * Compute the physical address of a descriptor from its virtual address
+ *
+ * @param BdPtr is the virtual address of the BD
+ *
+ * @returns Physical address of BdPtr
+ *
+ * @note Assume BdPtr is always a valid BD in the ring
+ * @note RingPtr is an implicit parameter
+ *****************************************************************************/
+#define Dma_mVirtToPhys(BdPtr) \
+  ((u32)(BdPtr) - (RingPtr->FirstBdAddr - RingPtr->FirstBdPhysAddr))
+
+
+/************************** Function Prototypes ******************************/
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* end of protection macro */
+
diff --git a/arch/arm/mach-zynq/k7_base_driver/xdma/xdma_bdring.c b/arch/arm/mach-zynq/k7_base_driver/xdma/xdma_bdring.c
new file mode 100644
index 0000000..1abea01
--- /dev/null
+++ b/arch/arm/mach-zynq/k7_base_driver/xdma/xdma_bdring.c
@@ -0,0 +1,1113 @@
+/*******************************************************************************
+** © Copyright 2011 - 2012 Xilinx, Inc. All rights reserved.
+** This file contains confidential and proprietary information of Xilinx, Inc. and
+** is protected under U.S. and international copyright and other intellectual property laws.
+*******************************************************************************
+**   ____  ____
+**  /   /\/   /
+** /___/  \  /   Vendor: Xilinx
+** \   \   \/
+**  \   \
+**  /   /
+** \   \  /  \   Kintex-7 PCIe-DMA-DDR3 Base Targeted Reference Design
+**  \___\/\___\
+**
+**  Device: xc7k325t
+**  Reference: UG882
+*******************************************************************************
+**
+**  Disclaimer:
+**
+**    This disclaimer is not a license and does not grant any rights to the materials
+**    distributed herewith. Except as otherwise provided in a valid license issued to you
+**    by Xilinx, and to the maximum extent permitted by applicable law:
+**    (1) THESE MATERIALS ARE MADE AVAILABLE "AS IS" AND WITH ALL FAULTS,
+**    AND XILINX HEREBY DISCLAIMS ALL WARRANTIES AND CONDITIONS, EXPRESS, IMPLIED, OR STATUTORY,
+**    INCLUDING BUT NOT LIMITED TO WARRANTIES OF MERCHANTABILITY, NON-INFRINGEMENT, OR
+**    FITNESS FOR ANY PARTICULAR PURPOSE; and (2) Xilinx shall not be liable (whether in contract
+**    or tort, including negligence, or under any other theory of liability) for any loss or damage
+**    of any kind or nature related to, arising under or in connection with these materials,
+**    including for any direct, or any indirect, special, incidental, or consequential loss
+**    or damage (including loss of data, profits, goodwill, or any type of loss or damage suffered
+**    as a result of any action brought by a third party) even if such damage or loss was
+**    reasonably foreseeable or Xilinx had been advised of the possibility of the same.
+
+
+**  Critical Applications:
+**
+**    Xilinx products are not designed or intended to be fail-safe, or for use in any application
+**    requiring fail-safe performance, such as life-support or safety devices or systems,
+**    Class III medical devices, nuclear facilities, applications related to the deployment of airbags,
+**    or any other applications that could lead to death, personal injury, or severe property or
+**    environmental damage (individually and collectively, "Critical Applications"). Customer assumes
+**    the sole risk and liability of any use of Xilinx products in Critical Applications, subject only
+**    to applicable laws and regulations governing limitations on product liability.
+
+**  THIS COPYRIGHT NOTICE AND DISCLAIMER MUST BE RETAINED AS PART OF THIS FILE AT ALL TIMES.
+
+*******************************************************************************/
+/*****************************************************************************/
+/**
+*
+* @file xdma_bdring.c
+*
+* This file implements buffer descriptor ring related functions. For more
+* information on this driver, see xdma.h.
+*
+* <pre>
+* MODIFICATION HISTORY:
+*
+* Ver   Who  Date     Changes
+* ----- ---- -------- -------------------------------------------------------
+* 1.0   ps   06/30/09 First release
+* </pre>
+******************************************************************************/
+
+/***************************** Include Files *********************************/
+
+#include <linux/kernel.h>
+#include <linux/string.h>
+
+#include "xbasic_types.h"
+#include "xdebug.h"
+#include "xstatus.h"
+#include "xio.h"
+#include "xdma_bdring.h"
+#include "xdma_hw.h"
+
+
+/************************** Constant Definitions *****************************/
+
+
+/**************************** Type Definitions *******************************/
+
+
+/***************** Macros (Inline Functions) Definitions *********************/
+
+
+/************************** Function Prototypes ******************************/
+
+
+/************************** Variable Definitions *****************************/
+
+
+/*****************************************************************************/
+/**
+ * Using a memory segment allocated by the caller, create and setup the BD list
+ * for the given SGDMA ring.
+ *
+ * @param RingPtr is the ring to be worked on.
+ * @param PhysAddr is the physical base address of allocated memory region.
+ * @param VirtAddr is the virtual base address of the allocated memory
+ *        region.
+ * @param Alignment governs the byte alignment of individual BDs. This function
+ *        will enforce a minimum alignment of DMA_BD_MINIMUM_ALIGNMENT bytes
+ *        with no maximum as long as it is specified as a power of 2.
+ * @param BdCount is the number of BDs to set up in the application memory
+ *        region. It is assumed the region is large enough to contain the BDs.
+ *        Refer to the "SGDMA Descriptor Ring Creation" section  in xdma.h
+ *        for more information. The minimum valid value for this parameter is 1.
+ *
+ * @return
+ *
+ * - XST_SUCCESS if initialization was successful
+ * - XST_INVALID_PARAM under any of the following conditions: 1) PhysAddr
+ *   and/or VirtAddr are not aligned to the given Alignment parameter;
+ *   2) Alignment parameter does not meet minimum requirements or is not a
+ *   power of 2 value; 3) BdCount is 0.
+ * - XST_DMA_SG_LIST_ERROR if the memory segment containing the list spans
+ *   over address 0x00000000 in virtual address space.
+ *
+ *****************************************************************************/
+int Dma_BdRingCreate(Dma_BdRing * RingPtr, u32 PhysAddr, u32 VirtAddr, u32 Alignment, unsigned BdCount)
+{
+  unsigned i;
+  u32 BdVirtAddr;
+  u32 BdPhysAddr;
+
+  /* In case there is a failure prior to creating list, make sure the
+   * following attributes are 0 to prevent calls to other SG functions
+   * from doing anything
+   */
+  RingPtr->AllCnt = 0;
+  RingPtr->FreeCnt = 0;
+  RingPtr->HwCnt = 0;
+  RingPtr->PreCnt = 0;
+  RingPtr->PostCnt = 0;
+    RingPtr->BDerrs = 0;
+    RingPtr->BDSerrs = 0;
+
+  /* Make sure Alignment parameter meets minimum requirements */
+  if (Alignment < DMA_BD_MINIMUM_ALIGNMENT) {
+    return (XST_INVALID_PARAM);
+  }
+
+  /* Make sure Alignment is a power of 2 */
+  if ((Alignment - 1) & Alignment) {
+    return (XST_INVALID_PARAM);
+  }
+
+  /* Make sure PhysAddr and VirtAddr are on same Alignment */
+  if ((PhysAddr % Alignment) || (VirtAddr % Alignment)) {
+    return (XST_INVALID_PARAM);
+  }
+
+  /* Is BdCount reasonable? */
+  if (BdCount == 0) {
+    return (XST_INVALID_PARAM);
+  }
+
+  /* Compute how many bytes will be between the start of adjacent BDs */
+  RingPtr->Separation =
+    (sizeof(Dma_Bd) + (Alignment - 1)) & ~(Alignment - 1);
+
+  /* Must make sure the ring doesn't span address 0x00000000. If it does,
+   * then the next/prev BD traversal macros will fail.
+   */
+  if (VirtAddr > (VirtAddr + (RingPtr->Separation * BdCount) - 1)) {
+    return (XST_DMA_SG_LIST_ERROR);
+  }
+
+  /* Initial ring setup:
+   *  - Clear the entire space
+   *  - Setup each BD's next pointer with the physical address of the
+   *    next BD
+   */
+  log_verbose(KERN_INFO "Zeroing out BD ring space - %d bytes\n",
+                                        (RingPtr->Separation * BdCount));
+  memset((void *) VirtAddr, 0, (RingPtr->Separation * BdCount));
+
+  BdVirtAddr = VirtAddr;
+  BdPhysAddr = PhysAddr + RingPtr->Separation;
+  for (i = 1; i < BdCount; i++) {
+    Dma_mBdWrite(BdVirtAddr, DMA_BD_NDESC_OFFSET, BdPhysAddr);
+    BdVirtAddr += RingPtr->Separation;
+    BdPhysAddr += RingPtr->Separation;
+  }
+
+  /* At the end of the ring, link the last BD back to the top */
+  Dma_mBdWrite(BdVirtAddr, DMA_BD_NDESC_OFFSET, PhysAddr);
+
+  /* Set up and initialize pointers and counters */
+  RingPtr->RunState = XST_DMA_SG_IS_STOPPED;
+  RingPtr->FirstBdAddr = VirtAddr;
+  RingPtr->FirstBdPhysAddr = PhysAddr;
+  RingPtr->LastBdAddr = BdVirtAddr;
+  RingPtr->Length = RingPtr->LastBdAddr - RingPtr->FirstBdAddr +
+    RingPtr->Separation;
+  RingPtr->AllCnt = BdCount;
+  RingPtr->FreeCnt = BdCount;
+  RingPtr->FreeHead = (Dma_Bd *) VirtAddr;
+  RingPtr->PreHead = (Dma_Bd *) VirtAddr;
+  RingPtr->HwHead = (Dma_Bd *) VirtAddr;
+  RingPtr->HwTail = (Dma_Bd *) VirtAddr;
+  RingPtr->PostHead = (Dma_Bd *) VirtAddr;
+  RingPtr->BdaRestart = (Dma_Bd *) PhysAddr;
+    {
+  if(RingPtr->IsRxChannel)
+    log_verbose(KERN_INFO "Seems to be an Rx channel\n");
+  log_verbose(KERN_INFO "Ring Ptr %p:\n", RingPtr);
+  log_verbose(KERN_INFO "ChanBase 0x%x, ", RingPtr->ChanBase);
+  log_verbose(KERN_INFO "first Bd PA 0x%x, first Bd VA 0x%x, last BD VA 0x%x\n",
+    RingPtr->FirstBdPhysAddr, RingPtr->FirstBdAddr, RingPtr->LastBdAddr);
+  log_verbose(KERN_INFO "length %d, state 0x%x, separation %d\n",
+    RingPtr->Length, RingPtr->RunState, RingPtr->Separation);
+  log_verbose(KERN_INFO "free count %d, pre count %d, HW count %d, post count %d, all count %d\n",
+    RingPtr->FreeCnt, RingPtr->PreCnt, RingPtr->HwCnt, RingPtr->PostCnt,
+    RingPtr->AllCnt);
+    log_verbose(KERN_INFO "HwTail is at %x\n", (u32)(RingPtr->HwTail));
+    }
+  return (XST_SUCCESS);
+}
+
+/*****************************************************************************/
+/**
+ * Allow DMA transactions to commence on the given channels if descriptors are
+ * ready to be processed.
+ *
+ * @param EngPtr is a pointer to the engine instance to be worked on.
+ *
+ * @return
+ * - XST_SUCCESS if the channel was started.
+ * - XST_DMA_SG_NO_LIST if the channel has no initialized BD ring.
+ *
+ *****************************************************************************/
+int Dma_BdRingStart(Dma_BdRing * RingPtr)
+{
+
+  /* BD list has yet to be created for this channel */
+  if (RingPtr->AllCnt == 0) {
+    return (XST_DMA_SG_NO_LIST);
+  }
+
+  /* Do nothing if already started */
+  if (RingPtr->RunState == XST_DMA_SG_IS_STARTED) {
+    return (XST_SUCCESS);
+  }
+
+  /* Flag ring as started */
+  RingPtr->RunState = XST_DMA_SG_IS_STARTED;
+
+  /* Sync hardware and driver with the last unprocessed BD or the 1st BD
+   * in the ring since this is the first time starting the channel. Update
+     * tail pointer as well, so that both pointers are equal before enabling.
+   */
+    Dma_mWriteReg(RingPtr->ChanBase, REG_DMA_ENG_NEXT_BD,
+                                            (u32) RingPtr->BdaRestart);
+    Dma_mWriteReg(RingPtr->ChanBase, REG_SW_NEXT_BD,
+                                            (u32) RingPtr->BdaRestart);
+    wmb();
+
+
+    /* Finally enable the DMA engine */
+    Dma_mEnable(RingPtr->ChanBase);
+
+    wmb();
+
+  /* If there are unprocessed BDs then we want the channel to begin
+   * processing right away.
+   */
+  if (RingPtr->HwCnt > 0) {
+        Dma_mWriteReg(RingPtr->ChanBase, REG_SW_NEXT_BD,
+           Dma_mVirtToPhys(RingPtr->HwTail));
+  }
+    if (RingPtr->IsRxChannel)
+        Dma_mReadReg(RingPtr->ChanBase, 0x18);
+
+  return (XST_SUCCESS);
+}
+
+/*****************************************************************************/
+/**
+ * Reserve locations in the BD ring. The set of returned BDs may be modified in
+ * preparation for future DMA transactions. Once the BDs are ready to be
+ * submitted to hardware, the application must call Dma_BdRingToHw() in the
+ * same order which they were allocated here. Example:
+ *
+ * <pre>
+ *        NumBd = 2;
+ *        Status = Dma_RingBdAlloc(MyRingPtr, NumBd, &MyBdSet);
+ *
+ *        if (Status != XST_SUCCESS)
+ *        {
+ *            // Not enough BDs available for the request
+ *        }
+ *
+ *        CurBd = MyBdSet;
+ *        for (i=0; i<NumBd; i++)
+ *        {
+ *            // Prepare CurBd.....
+ *
+ *            // Onto next BD
+ *            CurBd = Dma_mBdRingNext(MyRingPtr, CurBd);
+ *        }
+ *
+ *        // Give list to hardware
+ *        Status = Dma_BdRingToHw(MyRingPtr, NumBd, MyBdSet);
+ * </pre>
+ *
+ * A more advanced use of this function may allocate multiple sets of BDs.
+ * They must be allocated and given to hardware in the correct sequence:
+ * <pre>
+ *        // Legal
+ *        Dma_BdRingAlloc(MyRingPtr, NumBd1, &MySet1);
+ *        Dma_BdRingToHw(MyRingPtr, NumBd1, MySet1);
+ *
+ *        // Legal
+ *        Dma_BdRingAlloc(MyRingPtr, NumBd1, &MySet1);
+ *        Dma_BdRingAlloc(MyRingPtr, NumBd2, &MySet2);
+ *        Dma_BdRingToHw(MyRingPtr, NumBd1, MySet1);
+ *        Dma_BdRingToHw(MyRingPtr, NumBd2, MySet2);
+ *
+ *        // Not legal
+ *        Dma_BdRingAlloc(MyRingPtr, NumBd1, &MySet1);
+ *        Dma_BdRingAlloc(MyRingPtr, NumBd2, &MySet2);
+ *        Dma_BdRingToHw(MyRingPtr, NumBd2, MySet2);
+ *        Dma_BdRingToHw(MyRingPtr, NumBd1, MySet1);
+ * </pre>
+ *
+ * Use the APIs defined in xdma_bd.h to modify individual BDs. Traversal of the
+ * BD set can be done using Dma_mBdRingNext() and Dma_mBdRingPrev().
+ *
+ * @param RingPtr is a pointer to the descriptor ring instance to be worked on.
+ * @param NumBd is the number of BDs to allocate
+ * @param BdSetPtr is an output parameter, it points to the first BD available
+ *        for modification.
+ *
+ * @return
+ *   - XST_SUCCESS if the requested number of BDs was returned in the BdSetPtr
+ *     parameter.
+ *   - XST_FAILURE if there were not enough free BDs to satisfy the request.
+ *
+ * @note This function should not be preempted by another Dma_BdRing
+ *       function call that modifies the BD space. It is the caller's
+ *       responsibility to provide a mutual exclusion mechanism.
+ *
+ * @note Do not modify more BDs than the number requested with the NumBd
+ *       parameter. Doing so will lead to data corruption and system
+ *       instability.
+ *
+ *****************************************************************************/
+int Dma_BdRingAlloc(Dma_BdRing * RingPtr, unsigned NumBd, Dma_Bd ** BdSetPtr)
+{
+  /* Enough free BDs available for the request? */
+  if (RingPtr->FreeCnt < NumBd) {
+    log_verbose(KERN_INFO "Ring %x free count is only %d\n",
+                                        (u32) RingPtr, (RingPtr->FreeCnt));
+    return (XST_FAILURE);
+  }
+
+
+  /* Set the return argument and move FreeHead forward */
+  *BdSetPtr = RingPtr->FreeHead;
+  Dma_mRingSeekahead(RingPtr, RingPtr->FreeHead, NumBd);
+  RingPtr->FreeCnt -= NumBd;
+  RingPtr->PreCnt += NumBd;
+
+
+    log_verbose(KERN_INFO "Ring %x free %d pre %d hw %d post %d\n",
+        (u32) RingPtr, RingPtr->FreeCnt, RingPtr->PreCnt, RingPtr->HwCnt,
+        RingPtr->PostCnt);
+
+  return (XST_SUCCESS);
+}
+
+
+/*****************************************************************************/
+/**
+ * Fully or partially undo a Dma_BdRingAlloc() operation. Use this function
+ * if all the BDs allocated by Dma_BdRingAlloc() could not be transferred to
+ * hardware with Dma_BdRingToHw().
+ *
+ * This function helps out in situations when an unrelated error occurs after
+ * BDs have been allocated but before they have been given to hardware.
+ *
+ * This function is not the same as Dma_BdRingFree(). The Free function
+ * returns BDs to the free list after they have been processed by hardware,
+ * while UnAlloc returns them before being processed by hardware.
+ *
+ * There are two scenarios where this function can be used. Full UnAlloc or
+ * Partial UnAlloc. A Full UnAlloc means all the BDs Alloc'd will be returned:
+ *
+ * <pre>
+ *    Status = Dma_BdRingAlloc(MyRingPtr, 10, &BdPtr);
+ *        .
+ *        .
+ *    if (Error)
+ *    {
+ *        Status = Dma_BdRingUnAlloc(MyRingPtr, 10, &BdPtr);
+ *    }
+ * </pre>
+ *
+ * A partial UnAlloc means some of the BDs Alloc'd will be returned:
+ *
+ * <pre>
+ *    Status = Dma_BdRingAlloc(MyRingPtr, 10, &BdPtr);
+ *    BdsLeft = 10;
+ *    CurBdPtr = BdPtr;
+ *
+ *    while (BdsLeft)
+ *    {
+ *       if (Error)
+ *       {
+ *          Status = Dma_BdRingUnAlloc(MyRingPtr, BdsLeft, CurBdPtr);
+ *       }
+ *
+ *       CurBdPtr = Dma_mBdRingNext(MyRingPtr, CurBdPtr);
+ *       BdsLeft--;
+ *    }
+ * </pre>
+ *
+ * A partial UnAlloc must include the last BD in the list that was Alloc'd.
+ *
+ * @param RingPtr is a pointer to the descriptor ring instance to be worked on.
+ * @param NumBd is the number of BDs to unallocate
+ * @param BdSetPtr points to the first of the BDs to be returned.
+ *
+ * @return
+ *   - XST_SUCCESS if the BDs were unallocated.
+ *   - XST_FAILURE if NumBd parameter was greater that the number of BDs in the
+ *     preprocessing state.
+ *
+ * @note This function should not be preempted by another Dma ring function
+ *       call that modifies the BD space. It is the caller's responsibility to
+ *       provide a mutual exclusion mechanism.
+ *
+ *****************************************************************************/
+int Dma_BdRingUnAlloc(Dma_BdRing * RingPtr, unsigned NumBd, Dma_Bd * BdSetPtr)
+{
+  /* Enough BDs in the pre-allocated state for the request? */
+  if (RingPtr->PreCnt < NumBd) {
+    return (XST_FAILURE);
+  }
+
+  /* Set the return argument and move FreeHead backward */
+  Dma_mRingSeekback(RingPtr, RingPtr->FreeHead, NumBd);
+  RingPtr->FreeCnt += NumBd;
+  RingPtr->PreCnt -= NumBd;
+
+  return (XST_SUCCESS);
+}
+
+
+/*****************************************************************************/
+/**
+ * Enqueue a set of BDs to hardware that were previously allocated by
+ * Dma_BdRingAlloc(). Once this function returns, the argument BD set goes
+ * under hardware control. Any changes made to these BDs after this point will
+ * corrupt the BD list leading to data corruption and system instability.
+ *
+ * The set will be rejected if the last BD of the set does not mark the end of
+ * a packet.
+ *
+ * @param RingPtr is a pointer to the descriptor ring instance to be worked on.
+ * @param NumBd is the number of BDs in the set.
+ * @param BdSetPtr is the first BD of the set to commit to hardware.
+ *
+ * @return
+ *   - XST_SUCCESS if the set of BDs was accepted and enqueued to hardware
+ *   - XST_FAILURE if the set of BDs was rejected because the first BD
+ *     did not have its start-of-packet bit set, the last BD did not have
+ *     its end-of-packet bit set, or any one of the BD set has 0 as length
+ *     value
+ *   - XST_DMA_SG_LIST_ERROR if this function was called out of sequence with
+ *     Dma_BdRingAlloc()
+ *
+ * @note This function should not be preempted by another Dma ring function
+ *       call that modifies the BD space. It is the caller's responsibility to
+ *       provide a mutual exclusion mechanism.
+ *
+ *****************************************************************************/
+int Dma_BdRingToHw(Dma_BdRing * RingPtr, unsigned NumBd, Dma_Bd * BdSetPtr)
+{
+  Dma_Bd *CurBdPtr;
+  unsigned i;
+  u32 BdStsCr;
+
+  /* If the commit set is empty, do nothing */
+  if (NumBd == 0) {
+    return (XST_SUCCESS);
+  }
+
+  /* Make sure we are in sync with Dma_BdRingAlloc() */
+  if ((RingPtr->PreCnt < NumBd) || (RingPtr->PreHead != BdSetPtr)) {
+        printk(KERN_INFO "PreCnt is %d, PreHead is %x\n",
+                            RingPtr->PreCnt, (u32) (RingPtr->PreHead));
+        printk(KERN_INFO "returning XST_DMA_SG_LIST_ERROR\n");
+    return (XST_DMA_SG_LIST_ERROR);
+  }
+
+  CurBdPtr = BdSetPtr;
+    BdStsCr = Dma_mBdRead(CurBdPtr, DMA_BD_BUFL_CTRL_OFFSET);
+
+    /* In case of Tx channel, the first BD should have been marked
+     * as start-of-packet.
+     */
+    if (!(RingPtr->IsRxChannel) && !(BdStsCr & DMA_BD_SOP_MASK)) {
+        printk(KERN_WARNING "First TX BD should have SOP\n");
+        return (XST_FAILURE);
+    }
+
+  /* For each BD being submitted, do checks, and clear the status field. */
+  for (i = 0; i < NumBd; i++) {
+        /* Check for the control flags */
+        BdStsCr = Dma_mBdRead(CurBdPtr, DMA_BD_BUFL_CTRL_OFFSET);
+
+    /* Make sure the length value in the BD is non-zero. Actually, the
+         * last TX BD need not have non-zero length, but to simplify things,
+         * that is ignored.
+         */
+    if ((BdStsCr & DMA_BD_BUFL_MASK) == 0) {
+            printk(KERN_WARNING "BDs should have length.\n");
+      break;
+    }
+
+        /* Clear status field. */
+    Dma_mBdSetStatus(CurBdPtr, 0);
+
+    CurBdPtr = Dma_mBdRingNext(RingPtr, CurBdPtr);
+    }
+
+    if(i != NumBd) {
+        printk(KERN_ERR "%d BDs instead of %d\n", i, NumBd);
+    return (XST_FAILURE);
+    }
+
+    /* Last TX BD should have end-of-packet bit set */
+    if (!(RingPtr->IsRxChannel) && !(BdStsCr & DMA_BD_EOP_MASK)) {
+        log_normal(KERN_WARNING "Last TX BD should have EOP\n");
+    }
+
+  /* This set has completed pre-processing, adjust ring pointers and
+   * counters
+   */
+  Dma_mRingSeekahead(RingPtr, RingPtr->PreHead, NumBd);
+  RingPtr->PreCnt -= NumBd;
+  RingPtr->HwTail = CurBdPtr;
+  RingPtr->HwCnt += NumBd;
+
+  /* If it was enabled, tell the engine to begin processing */
+  if (RingPtr->RunState == XST_DMA_SG_IS_STARTED) {
+
+        /* Ensure that all the descriptor updates are completed before
+         * informing the hardware.
+         */
+        wmb();
+
+        /* In NWL DMA engine, the tail descriptor pointer should actually
+         * point to the next (unused BD).
+         */
+    Dma_mWriteReg(RingPtr->ChanBase, REG_SW_NEXT_BD,
+                         Dma_mVirtToPhys(RingPtr->HwTail));
+        log_verbose(KERN_INFO "Writing %x into %x\n",
+                    Dma_mVirtToPhys(RingPtr->HwTail),
+                    (RingPtr->ChanBase+ REG_SW_NEXT_BD));
+  }
+    log_verbose(KERN_INFO "ToHw with %d BDs\n", NumBd);
+
+  return (XST_SUCCESS);
+}
+
+
+/*****************************************************************************/
+/**
+ * Returns a set of BDs that have been processed by hardware. The returned
+ * BDs may be examined by the caller to determine the outcome of the DMA
+ * transactions. Once the BDs have been examined, the caller must call
+ * Dma_BdRingFree() in the same order which they were retrieved here.
+ *
+ * Example:
+ *
+ * <pre>
+ *        NumBd = Dma_BdRingFromHw(MyRingPtr, DMA_ALL_BDS, &MyBdSet);
+ *
+ *        if (NumBd == 0)
+ *        {
+ *           // hardware has nothing ready for us yet.
+ *        }
+ *
+ *        CurBd = MyBdSet;
+ *        for (i=0; i<NumBd; i++)
+ *        {
+ *           // Examine CurBd for post processing.....
+ *
+ *           // Onto next BD
+ *           CurBd = Dma_mBdRingNext(MyRingPtr, CurBd);
+ *        }
+ *
+ *        Dma_BdRingFree(MyRingPtr, NumBd, MyBdSet); // Return the list
+ * </pre>
+ *
+ * A more advanced use of this function may allocate multiple sets of BDs.
+ * They must be retrieved from hardware and freed in the correct sequence:
+ * <pre>
+ *        // Legal
+ *        Dma_BdRingFromHw(MyRingPtr, NumBd1, &MySet1);
+ *        Dma_BdRingFree(MyRingPtr, NumBd1, MySet1);
+ *
+ *        // Legal
+ *        Dma_BdRingFromHw(MyRingPtr, NumBd1, &MySet1);
+ *        Dma_BdRingFromHw(MyRingPtr, NumBd2, &MySet2);
+ *        Dma_BdRingFree(MyRingPtr, NumBd1, MySet1);
+ *        Dma_BdRingFree(MyRingPtr, NumBd2, MySet2);
+ *
+ *        // Not legal
+ *        Dma_BdRingFromHw(MyRingPtr, NumBd1, &MySet1);
+ *        Dma_BdRingFromHw(MyRingPtr, NumBd2, &MySet2);
+ *        Dma_BdRingFree(MyRingPtr, NumBd2, MySet2);
+ *        Dma_BdRingFree(MyRingPtr, NumBd1, MySet1);
+ * </pre>
+ *
+ * If hardware has partially completed a packet spanning multiple BDs, then
+ * none of the BDs for that packet will be included in the results.
+ *
+ * @param RingPtr is a pointer to the descriptor ring instance to be worked on.
+ * @param BdLimit is the maximum number of BDs to return in the set. Use
+ *        DMA_ALL_BDS to return all BDs that have been processed.
+ * @param BdSetPtr is an output parameter, it points to the first BD available
+ *        for examination.
+ *
+ * @return
+ *   The number of BDs processed by hardware. A value of 0 indicates that no
+ *   data is available. No more than BdLimit BDs will be returned.
+ *
+ * @note Treat BDs returned by this function as read-only.
+ *
+ * @note This function should not be preempted by another Dma ring function
+ *       call that modifies the BD space. It is the caller's responsibility to
+ *       provide a mutual exclusion mechanism.
+ *
+ *****************************************************************************/
+unsigned Dma_BdRingFromHw(Dma_BdRing * RingPtr, unsigned BdLimit,
+           Dma_Bd ** BdSetPtr)
+{
+  Dma_Bd *CurBdPtr;
+  unsigned BdCount;
+  unsigned BdPartialCount;
+  u32 BdStsCr, BdCtrl;
+    unsigned long long userInfo;
+
+  CurBdPtr = RingPtr->HwHead;
+  BdCount = 0;
+  BdPartialCount = 0;
+
+  /* If no BDs in work group, then there's nothing to search */
+  if (RingPtr->HwCnt == 0) {
+        //log_verbose(KERN_INFO "HwCnt is 0\n");
+    *BdSetPtr = NULL;
+    return (0);
+  }
+
+  /* Starting at HwHead, keep moving forward in the list until one of
+     * of the following conditions is reached:
+   *  - A BD is encountered with its completed bit and error bit clear
+     *    in the status word which means hardware has not completed
+     *    processing of that BD.
+     *  - TX: Check EOP bit in control field, RX: check EOP bit in status
+   *  - In RX channel case, a BD has EOP flag set, with non-zero User
+     *    Status, but its User Status fields are still showing zero.
+   *    This means hardware has not completed updating the RX BD structure.
+   *  - RingPtr->HwTail is reached
+   *  - The number of requested BDs has been processed
+   */
+  while (BdCount < BdLimit) {
+    /* Read the status and control fields */
+    BdStsCr = Dma_mBdGetStatus(CurBdPtr);
+    BdCtrl = Dma_mBdGetCtrl(CurBdPtr);
+        userInfo = Dma_mBdGetUserData(CurBdPtr);
+        //log_verbose(KERN_INFO "BD Status is %x\n", BdStsCr);
+
+        /* How to handle errors? Not doing anything for now. */
+    if (BdStsCr & DMA_BD_ERROR_MASK)
+        {
+            if (!(RingPtr->IsRxChannel))
+                printk(KERN_ERR "TX: BD %p had error\n", CurBdPtr);
+            else
+                printk(KERN_ERR "RX: BD %p had error\n", CurBdPtr);
+            RingPtr->BDerrs ++;
+            printk("BD status %x next BD's PA %x\n", BdStsCr, Dma_mBdRead((CurBdPtr), DMA_BD_NDESC_OFFSET));
+            printk("Buffer PA %x\n", Dma_mBdRead((CurBdPtr), DMA_BD_BUFAL_OFFSET));
+            //printk("DMA Engine Control %x\n", Dma_mReadReg(RingPtr->ChanBase, 0x4));
+            //printk("DMA Head Ptr %x\n", Dma_mReadReg(RingPtr->ChanBase, 0x8));
+            //printk("DMA SW Desc Ptr %x\n", Dma_mReadReg(RingPtr->ChanBase, 0xC));
+            //printk("DMA Comp Desc Ptr %x\n", Dma_mReadReg(RingPtr->ChanBase, 0x10));
+        }
+
+        /* For a TX BD, short processing is an error. */
+        if (!(RingPtr->IsRxChannel) && (BdStsCr & DMA_BD_SHORT_MASK))
+        {
+            printk(KERN_ERR "TX BD %p had short error\n", CurBdPtr);
+            printk("BD status %x next BD's PA %x\n", BdStsCr, Dma_mBdRead((CurBdPtr), DMA_BD_NDESC_OFFSET));
+            printk("Buffer PA %x\n", Dma_mBdRead((CurBdPtr), DMA_BD_BUFAL_OFFSET));
+            RingPtr->BDSerrs ++;
+        }
+
+    /* If the hardware still hasn't processed this BD then we are done */
+    if (!(BdStsCr & DMA_BD_COMP_MASK))
+      break;
+
+        /* Is it really done? For RX EOP BDs, check the user info field also */
+        if ((RingPtr->IsRxChannel) && (BdStsCr & DMA_BD_EOP_MASK))
+        {
+            if(!(BdStsCr & DMA_BD_USER_HIGH_ZERO_MASK) &&
+               !(userInfo & 0xFFFFFFFF00000000LL)) break;
+            if(!(BdStsCr & DMA_BD_USER_LOW_ZERO_MASK) &&
+               !(userInfo & 0xFFFFFFFFLL)) break;
+        }
+
+
+
+    BdCount++;
+
+    /* Hardware has processed this BD so check the EOP bit. If
+     * it is clear, then there are more BDs for the current packet.
+     * Keep a count of these partial packet BDs.
+         * Note that - for RX, check EOP bit in status field, and for
+         * TX, check EOP bit in control field.
+         * If error bits are set, EOP should not be a requirement.
+     */
+    if (((RingPtr->IsRxChannel) && (BdStsCr & DMA_BD_EOP_MASK)) ||
+        (!(RingPtr->IsRxChannel) && (BdCtrl & DMA_BD_EOP_MASK)) ||
+            (BdStsCr & DMA_BD_ERROR_MASK))
+        {
+      BdPartialCount = 0;
+    }
+    else {
+      BdPartialCount++;
+    }
+
+    /* Move on to next BD in work group */
+    CurBdPtr = Dma_mBdRingNext(RingPtr, CurBdPtr);
+
+    /* Reached the end of the work group */
+    if (CurBdPtr == RingPtr->HwTail) {
+      break;
+    }
+  }
+
+  /* Subtract off any partial packet BDs found */
+  BdCount -= BdPartialCount;
+
+  /* If BdCount is non-zero then BDs were found to return. Set return
+   * parameters, update pointers and counters, return success
+   */
+  if (BdCount) {
+/*
+if (!(RingPtr->IsRxChannel))
+    printk("TX: FromHw with %d BDs\n", BdCount);
+else
+    printk("RX: FromHw with %d BDs\n", BdCount);
+*/
+//printk("FromHw returns with %x %d BDs\n", (u32)(RingPtr->HwHead), BdCount);
+    *BdSetPtr = RingPtr->HwHead;
+    RingPtr->HwCnt -= BdCount;
+    RingPtr->PostCnt += BdCount;
+    Dma_mRingSeekahead(RingPtr, RingPtr->HwHead, BdCount);
+    return (BdCount);
+  }
+  else {
+    *BdSetPtr = NULL;
+    return (0);
+  }
+}
+
+/*****************************************************************************/
+/**
+ * Returns a set of BDs that had been enqueued to hardware, whether the
+ * hardware has processed them yet or not. This should be called only
+ * when the driver is freeing up buffers and about to free the ring.
+ * The returned BDs may be examined by the caller in order to recover
+ * the buffers. Once the BDs have been examined, the caller must call
+ * Dma_BdRingFree() in the same order which they were retrieved here.
+ *
+ * @param RingPtr is a pointer to the descriptor ring instance to be worked on.
+ * @param BdLimit is the maximum number of BDs to return in the set. Use
+ *        DMA_ALL_BDS to return all BDs that have been processed.
+ * @param BdSetPtr is an output parameter, it points to the first BD available
+ *        for examination.
+ *
+ * @return
+ *   The number of BDs processed by hardware. A value of 0 indicates that no
+ *   data is available. No more than BdLimit BDs will be returned.
+ *
+ * @note Treat BDs returned by this function as read-only.
+ *
+ * @note This function should not be preempted by another Dma ring function
+ *       call that modifies the BD space. It is the caller's responsibility to
+ *       provide a mutual exclusion mechanism.
+ *
+ *****************************************************************************/
+unsigned Dma_BdRingForceFromHw(Dma_BdRing * RingPtr, unsigned BdLimit,
+                               Dma_Bd ** BdSetPtr)
+{
+  Dma_Bd *CurBdPtr;
+  unsigned BdCount;
+  u32 BdStsCr;
+
+  CurBdPtr = RingPtr->HwHead;
+  BdCount = 0;
+
+    printk(KERN_INFO "ForceFromHw: HwCnt is %d\n", RingPtr->HwCnt);
+  /* If no BDs in work group, then there's nothing to search */
+  if (RingPtr->HwCnt == 0) {
+    *BdSetPtr = NULL;
+    return (0);
+  }
+
+  /* Starting at HwHead, keep moving forward in the list until one of
+     * of the following conditions is reached:
+   *  - RingPtr->HwTail is reached
+   *  - The number of requested BDs has been processed
+   */
+  while (BdCount < BdLimit) {
+    /* Read the status */
+    BdStsCr = Dma_mBdGetStatus(CurBdPtr);
+        log_verbose(KERN_INFO "BD Status is %x\n", BdStsCr);
+
+        /* How to handle errors? Not doing anything for now. */
+    if (BdStsCr & DMA_BD_ERROR_MASK)
+            printk(KERN_ERR "BD %p had error\n", CurBdPtr);
+
+    BdCount++;
+
+    /* Move on to next BD in work group */
+    CurBdPtr = Dma_mBdRingNext(RingPtr, CurBdPtr);
+
+    /* Reached the end of the work group */
+    if (CurBdPtr == RingPtr->HwTail) {
+      break;
+    }
+  }
+
+  /* If BdCount is non-zero then no BDs were found to return. Set return
+   * parameters, update pointers and counters, return success.
+   */
+  if (BdCount) {
+    *BdSetPtr = RingPtr->HwHead;
+    RingPtr->HwCnt -= BdCount;
+    RingPtr->PostCnt += BdCount;
+    Dma_mRingSeekahead(RingPtr, RingPtr->HwHead, BdCount);
+    return (BdCount);
+  }
+  else {
+    *BdSetPtr = NULL;
+    return (0);
+  }
+}
+
+/*****************************************************************************/
+/**
+ * Frees a set of BDs that had been previously retrieved with
+ * Dma_BdRingFromHw(). This function also clear all control/status bits
+ * (like SOP/EOP) of each BD in the set.
+ *
+ * @param RingPtr is a pointer to the descriptor ring instance to be worked on.
+ * @param NumBd is the number of BDs to free.
+ * @param BdSetPtr is the head of a list of BDs returned by Dma_BdRingFromHw().
+ *
+ * @return
+ *   - XST_SUCCESS if the set of BDs was freed.
+ *   - XST_DMA_SG_LIST_ERROR if this function was called out of sequence with
+ *     Dma_BdRingFromHw().
+ *
+ * @note This function should not be preempted by another Dma function call
+ *       that modifies the BD space. It is the caller's responsibility to
+ *       provide a mutual exclusion mechanism.
+ *
+ * @internal
+ *   The Interrupt handler provided by application MUST clear pending
+ *   interrupts before handling them by calling the call back. Otherwise
+ *   the following corner case could raise some issue:
+ *
+ *   - A packet was transmitted and asserted an TX interrupt, and if
+ *     the interrupt handler calls the call back before clears the
+ *     interrupt, another packet could get transmitted (and assert the
+ *     interrupt) between when the call back function returned and when
+ *     the interrupt clearing operation begins, and the interrupt
+ *     clearing operation will clear the interrupt raised by the second
+ *     packet and won't never process its according buffer descriptors
+ *     until a new interrupt occurs.
+ *
+ *   Changing the sequence to "Clear interrupts, then handle" solves this
+ *   issue. If the interrupt raised by the second packet is before the
+ *   the interrupt clearing operation, the descriptors associated with
+ *   the second packet must have been finished by hardware and ready for
+ *   the handling by the call back; otherwise, the interrupt raised by
+ *   the second packet is after the interrupt clearing operation,
+ *   the packet's buffer descriptors will be handled by the call back in
+ *   current pass, if the descriptors are finished before the call back
+ *   is invoked, or next pass otherwise.
+ *
+ *   Please note that if the second packet is handled by the call back
+ *   in current pass, the next pass could find no buffer descriptor
+ *   finished by the hardware. (i.e., Dma_BdRingFromHw() returns 0).
+ *   As Dma_BdRingFromHw() and Dma_BdRingFree() are used in pair,
+ *   Dma_BdRingFree() covers this situation by checking if the BD
+ *   list to free is empty.
+ *****************************************************************************/
+int Dma_BdRingFree(Dma_BdRing * RingPtr, unsigned NumBd, Dma_Bd * BdSetPtr)
+{
+  Dma_Bd * CurBdPtr;
+  int i;
+
+  /* If the BD Set to free is empty, return immediately with value
+   * XST_SUCCESS. See the @internal comment block above for detailed
+   * information.
+   */
+  if (NumBd == 0) {
+    return XST_SUCCESS;
+  }
+
+  /* Make sure we are in sync with Dma_BdRingFromHw() */
+  if ((RingPtr->PostCnt < NumBd) || (RingPtr->PostHead != BdSetPtr)) {
+    log_normal(KERN_WARNING "Some out-of-sync error.\n");
+    return (XST_DMA_SG_LIST_ERROR);
+  }
+
+  /* Clear Status/Control field for all BDs in the set. Since the BDs
+     * have been freed anyway, we are not bothering to keep the length info.
+     */
+  CurBdPtr = BdSetPtr;
+  for (i = 0; i < NumBd; i++) {
+    Dma_mBdWrite(CurBdPtr, DMA_BD_BUFL_STATUS_OFFSET, 0);
+    Dma_mBdWrite(CurBdPtr, DMA_BD_BUFL_CTRL_OFFSET, 0);
+
+    /* Move on to next BD in work group */
+    CurBdPtr = Dma_mBdRingNext(RingPtr, CurBdPtr);
+  }
+
+  /* Update pointers and counters */
+  RingPtr->FreeCnt += NumBd;
+  RingPtr->PostCnt -= NumBd;
+  Dma_mRingSeekahead(RingPtr, RingPtr->PostHead, NumBd);
+
+    log_verbose(KERN_INFO "Ring %x free %d pre %d hw %d post %d\n",
+        (u32) RingPtr, RingPtr->FreeCnt, RingPtr->PreCnt, RingPtr->HwCnt,
+        RingPtr->PostCnt);
+
+  return (XST_SUCCESS);
+}
+
+
+/*****************************************************************************/
+/**
+ * Check the internal data structures of the BD ring for the provided channel.
+ * The following checks are made:
+ *
+ *   - Is the BD ring linked correctly in physical address space.
+ *   - Do the internal pointers point to BDs in the ring.
+ *   - Do the internal counters add up.
+ *
+ * The channel should be stopped prior to calling this function.
+ *
+ * @param RingPtr is a pointer to the descriptor ring to be worked on.
+ *
+ * @return
+ *   - XST_SUCCESS if no errors were found.
+ *   - XST_DMA_SG_NO_LIST if the ring has not been created.
+ *   - XST_IS_STARTED if the channel is not stopped.
+ *   - XST_DMA_SG_LIST_ERROR if a problem is found with the internal data
+ *     structures. If this value is returned, the channel should be reset to
+ *     avoid data corruption or system instability.
+ *
+ * @note This function should not be preempted by another Dma ring function
+ *       call that modifies the BD space. It is the caller's responsibility to
+ *       provide a mutual exclusion mechanism.
+ *
+ *****************************************************************************/
+int Dma_BdRingCheck(Dma_BdRing * RingPtr)
+{
+  u32 AddrV, AddrP;
+  unsigned i;
+
+  /* Is the list created */
+  if (RingPtr->AllCnt == 0) {
+    return (XST_DMA_SG_NO_LIST);
+  }
+
+  /* Can't check if channel is running */
+  if (RingPtr->RunState == XST_DMA_SG_IS_STARTED) {
+    return (XST_IS_STARTED);
+  }
+
+  /* RunState doesn't make sense */
+  else if (RingPtr->RunState != XST_DMA_SG_IS_STOPPED) {
+    return (XST_DMA_SG_LIST_ERROR);
+  }
+
+  /* Verify internal pointers point to correct memory space */
+  AddrV = (u32) RingPtr->FreeHead;
+  if ((AddrV < RingPtr->FirstBdAddr) || (AddrV > RingPtr->LastBdAddr)) {
+    return (XST_DMA_SG_LIST_ERROR);
+  }
+
+  AddrV = (u32) RingPtr->PreHead;
+  if ((AddrV < RingPtr->FirstBdAddr) || (AddrV > RingPtr->LastBdAddr)) {
+    return (XST_DMA_SG_LIST_ERROR);
+  }
+
+  AddrV = (u32) RingPtr->HwHead;
+  if ((AddrV < RingPtr->FirstBdAddr) || (AddrV > RingPtr->LastBdAddr)) {
+    return (XST_DMA_SG_LIST_ERROR);
+  }
+
+  AddrV = (u32) RingPtr->HwTail;
+  if ((AddrV < RingPtr->FirstBdAddr) || (AddrV > RingPtr->LastBdAddr)) {
+    return (XST_DMA_SG_LIST_ERROR);
+  }
+
+  AddrV = (u32) RingPtr->PostHead;
+  if ((AddrV < RingPtr->FirstBdAddr) || (AddrV > RingPtr->LastBdAddr)) {
+    return (XST_DMA_SG_LIST_ERROR);
+  }
+
+  /* Verify internal counters add up */
+  if ((RingPtr->HwCnt + RingPtr->PreCnt + RingPtr->FreeCnt +
+       RingPtr->PostCnt) != RingPtr->AllCnt) {
+    return (XST_DMA_SG_LIST_ERROR);
+  }
+
+  /* Verify BDs are linked correctly */
+  AddrV = RingPtr->FirstBdAddr;
+  AddrP = RingPtr->FirstBdPhysAddr + RingPtr->Separation;
+  for (i = 1; i < RingPtr->AllCnt; i++) {
+    /* Check next pointer for this BD. It should equal to the
+     * physical address of next BD
+     */
+    if (Dma_mBdRead(AddrV, DMA_BD_NDESC_OFFSET) != AddrP) {
+      return (XST_DMA_SG_LIST_ERROR);
+    }
+
+    /* Move on to next BD */
+    AddrV += RingPtr->Separation;
+    AddrP += RingPtr->Separation;
+  }
+
+  /* Last BD should point back to the beginning of ring */
+  if (Dma_mBdRead(AddrV, DMA_BD_NDESC_OFFSET) != RingPtr->FirstBdPhysAddr) {
+    return (XST_DMA_SG_LIST_ERROR);
+  }
+
+  /* No problems found */
+  return (XST_SUCCESS);
+}
+
+/*****************************************************************************/
+/**
+* Align the BD space.
+*
+* The DMA engine requires aligned BDs. This function aligns the space as
+* required by the caller.
+*
+* @param  AllocPtr is the virtual address of the allocated BD space.
+* @param  Size is the size of the allocated BD space.
+* @param  Align is the number of bytes of alignment required.
+* @param  Delta is the shift required in virtual and physical addresses
+*         in order to achieve the alignment. This should be applied by the
+*         caller.
+*
+* @return Number of BDs that will fit in the re-aligned BD space.
+*
+* @note
+*
+******************************************************************************/
+u32 Dma_BdRingAlign(u32 AllocPtr, u32 Size, u32 Align, u32 * Delta)
+{
+    u32 numbds;
+    u32 i;
+    int bytealign;
+
+    log_verbose(KERN_INFO "BD space %x, Size %d, Align %d\n",
+                                    AllocPtr, Size, Align);
+
+    /* Check for valid arguments. Minimum size check also. */
+    if(!AllocPtr || !Size || (Size < Align))
+    {
+        log_normal(KERN_ERR "Bad arguments Alloc %x Size %d\n",
+                                            AllocPtr, Size);
+        *Delta = 0;
+        return 0;
+    }
+
+    bytealign = (sizeof(u32)*DMA_BD_SW_NUM_WORDS);
+    numbds = Size / bytealign;
+    log_verbose(KERN_INFO "Number of BDs before aligning is %d\n", numbds);
+
+    if(AllocPtr % bytealign)
+    {
+        /* Alignment is required. */
+        log_verbose(KERN_INFO "Realignment is required\n");
+
+        i = bytealign - (AllocPtr & 0xFF);
+        Size -= i;
+        *Delta = i;
+        numbds = Size / bytealign;
+    }
+    else
+        log_verbose(KERN_INFO "Alignment is fine\n");
+
+    log_normal(KERN_INFO "BD space should be shifted by %d bytes\n", *Delta);
+    log_normal(KERN_INFO "After aligning, # BDs %d, size %d\n", numbds, Size);
+    return numbds;
+}
+
diff --git a/arch/arm/mach-zynq/k7_base_driver/xdma/xdma_bdring.h b/arch/arm/mach-zynq/k7_base_driver/xdma/xdma_bdring.h
new file mode 100644
index 0000000..a5620b2
--- /dev/null
+++ b/arch/arm/mach-zynq/k7_base_driver/xdma/xdma_bdring.h
@@ -0,0 +1,317 @@
+/*******************************************************************************
+** © Copyright 2011 - 2012 Xilinx, Inc. All rights reserved.
+** This file contains confidential and proprietary information of Xilinx, Inc. and
+** is protected under U.S. and international copyright and other intellectual property laws.
+*******************************************************************************
+**   ____  ____
+**  /   /\/   /
+** /___/  \  /   Vendor: Xilinx
+** \   \   \/
+**  \   \
+**  /   /
+** \   \  /  \   Kintex-7 PCIe-DMA-DDR3 Base Targeted Reference Design
+**  \___\/\___\
+**
+**  Device: xc7k325t
+**  Reference: UG882
+*******************************************************************************
+**
+**  Disclaimer:
+**
+**    This disclaimer is not a license and does not grant any rights to the materials
+**    distributed herewith. Except as otherwise provided in a valid license issued to you
+**    by Xilinx, and to the maximum extent permitted by applicable law:
+**    (1) THESE MATERIALS ARE MADE AVAILABLE "AS IS" AND WITH ALL FAULTS,
+**    AND XILINX HEREBY DISCLAIMS ALL WARRANTIES AND CONDITIONS, EXPRESS, IMPLIED, OR STATUTORY,
+**    INCLUDING BUT NOT LIMITED TO WARRANTIES OF MERCHANTABILITY, NON-INFRINGEMENT, OR
+**    FITNESS FOR ANY PARTICULAR PURPOSE; and (2) Xilinx shall not be liable (whether in contract
+**    or tort, including negligence, or under any other theory of liability) for any loss or damage
+**    of any kind or nature related to, arising under or in connection with these materials,
+**    including for any direct, or any indirect, special, incidental, or consequential loss
+**    or damage (including loss of data, profits, goodwill, or any type of loss or damage suffered
+**    as a result of any action brought by a third party) even if such damage or loss was
+**    reasonably foreseeable or Xilinx had been advised of the possibility of the same.
+
+
+**  Critical Applications:
+**
+**    Xilinx products are not designed or intended to be fail-safe, or for use in any application
+**    requiring fail-safe performance, such as life-support or safety devices or systems,
+**    Class III medical devices, nuclear facilities, applications related to the deployment of airbags,
+**    or any other applications that could lead to death, personal injury, or severe property or
+**    environmental damage (individually and collectively, "Critical Applications"). Customer assumes
+**    the sole risk and liability of any use of Xilinx products in Critical Applications, subject only
+**    to applicable laws and regulations governing limitations on product liability.
+
+**  THIS COPYRIGHT NOTICE AND DISCLAIMER MUST BE RETAINED AS PART OF THIS FILE AT ALL TIMES.
+
+*******************************************************************************/
+/*****************************************************************************/
+/**
+*
+* @file xdma_bdring.h
+*
+* This file contains DMA engine related structures and constant definitions,
+* as well as function prototypes. Each DMA engine is managed by a Buffer
+* Descriptor ring, and so Dma_BdRing is chosen as the symbol prefix used in
+* this file. See xdma.h for more information.
+*
+* <pre>
+* MODIFICATION HISTORY:
+*
+* Ver   Who  Date     Changes
+* ----- ---- -------- -------------------------------------------------------
+* 1.0   ps   06/30/09 First version
+* </pre>
+*
+******************************************************************************/
+
+#ifndef DMA_BDRING_H    /* prevent circular inclusions */
+#define DMA_BDRING_H    /* by using protection macros */
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+#include "xdma_bd.h"
+
+/**************************** Type Definitions *******************************/
+
+/** Container structure for buffer descriptor (BD) storage. All addresses
+ * and pointers excluding FirstBdPhysAddr are expressed in terms of the
+ * virtual address.
+ *
+ * The BD ring is not arranged as a conventional linked list. Instead,
+ * a separation value is added to navigate to the next BD, since all the
+ * BDs are contiguously placed, and the ring is placed from lower memory
+ * to higher memory and back.
+ */
+typedef struct {
+    u32 ChanBase;
+  u32 IsRxChannel;        /**< Is this a receive channel ? */
+  u32 RunState;         /**< Flag to indicate state of engine/ring */
+  u32 FirstBdPhysAddr;    /**< Physical address of 1st BD in list */
+  u32 FirstBdAddr;        /**< Virtual address of 1st BD in list */
+  u32 LastBdAddr;         /**< Virtual address of last BD in the list */
+  u32 Length;             /**< Total size of ring in bytes */
+  u32 Separation;         /**< Number of bytes between the starting
+                               address of adjacent BDs */
+  Dma_Bd *FreeHead;       /**< First BD in the free group */
+  Dma_Bd *PreHead;        /**< First BD in the pre-work group */
+  Dma_Bd *HwHead;         /**< First BD in the work group */
+  Dma_Bd *HwTail;         /**< Last BD in the work group */
+  Dma_Bd *PostHead;       /**< First BD in the post-work group */
+  Dma_Bd *BdaRestart;     /**< BD to load when channel is started */
+  u32 FreeCnt;          /**< Number of allocatable BDs in free group */
+  u32 PreCnt;             /**< Number of BDs in pre-work group */
+  u32 HwCnt;              /**< Number of BDs in work group */
+  u32 PostCnt;          /**< Number of BDs in post-work group */
+  u32 AllCnt;             /**< Total Number of BDs for channel */
+
+    u32 BDerrs;             /**< Total BD errors reported by DMA */
+    u32 BDSerrs;            /**< Total TX BD short errors reported by DMA */
+} Dma_BdRing;
+
+
+/***************** Macros (Inline Functions) Definitions *********************/
+
+/****************************************************************************/
+/**
+* Retrieve the ring object. This object can be used in the various Ring
+* API functions. This could be a C2S or S2C ring.
+*
+* @param  InstancePtr is the DMA engine to operate on.
+*
+* @return BdRing object
+*
+* @note
+* C-style signature:
+*    Dma_BdRing Dma_mGetRing(Dma_Engine * InstancePtr)
+*
+*****************************************************************************/
+#define Dma_mGetRing(InstancePtr) ((InstancePtr)->BdRing)
+
+
+/****************************************************************************/
+/**
+* Return the total number of BDs allocated by this channel with
+* Dma_BdRingCreate().
+*
+* @param  RingPtr is the BD ring to operate on.
+*
+* @return The total number of BDs allocated for this channel.
+*
+* @note
+* C-style signature:
+*    u32 Dma_mBdRingGetCnt(Dma_BdRing* RingPtr)
+*
+*****************************************************************************/
+#define Dma_mBdRingGetCnt(RingPtr) ((RingPtr)->AllCnt)
+
+
+/****************************************************************************/
+/**
+* Return the number of BDs allocatable with Dma_BdRingAlloc() for pre-
+* processing.
+*
+* @param  RingPtr is the BD ring to operate on.
+*
+* @return The number of BDs currently allocatable.
+*
+* @note
+* C-style signature:
+*    u32 Dma_mBdRingGetFreeCnt(Dma_BdRing* RingPtr)
+*
+*****************************************************************************/
+#define Dma_mBdRingGetFreeCnt(RingPtr)  ((RingPtr)->FreeCnt)
+
+/* Maintain a separation between start and end of BD ring. This is
+ * required because DMA will stall if the two pointers coincide -
+ * this will happen whether ring is full or empty.
+ */
+/*
+#define Dma_mBdRingGetFreeCnt(RingPtr)  \
+    (((RingPtr)->FreeCnt - 2) < 0 ? 0 : ((RingPtr)->FreeCnt - 2))
+*/
+
+/****************************************************************************/
+/**
+* Snap shot the latest BD that a BD ring is processing.
+*
+* @param  RingPtr is the BD ring to operate on.
+*
+* @return None
+*
+* @note
+* C-style signature:
+*    void Dma_mBdRingSnapShotCurrBd(Dma_BdRing* RingPtr)
+*
+*****************************************************************************/
+#define Dma_mBdRingSnapShotCurrBd(RingPtr)            \
+{                                           \
+    (RingPtr)->BdaRestart =                         \
+        (Dma_Bd *)Dma_mReadReg((RingPtr)->ChanBase, REG_DMA_ENG_NEXT_BD);   \
+}
+
+
+/****************************************************************************/
+/**
+* Return the next BD in the ring.
+*
+* @param  RingPtr is the BD ring to operate on.
+* @param  BdPtr is the current BD.
+*
+* @return The next BD in the ring relative to the BdPtr parameter.
+*
+* @note
+* C-style signature:
+*    Dma_Bd *Dma_mBdRingNext(Dma_BdRing* RingPtr, Dma_Bd *BdPtr)
+*
+*****************************************************************************/
+#define Dma_mBdRingNext(RingPtr, BdPtr)                 \
+    (((u32)(BdPtr) >= (RingPtr)->LastBdAddr) ?          \
+      (Dma_Bd*)(RingPtr)->FirstBdAddr :                 \
+      (Dma_Bd*)((u32)(BdPtr) + (RingPtr)->Separation))
+
+
+/****************************************************************************/
+/**
+* Return the previous BD in the ring.
+*
+* @param  InstancePtr is the DMA channel to operate on.
+* @param  BdPtr is the current BD.
+*
+* @return The previous BD in the ring relative to the BdPtr parameter.
+*
+* @note
+* C-style signature:
+*    Dma_Bd *Dma_mBdRingPrev(Dma_BdRing* RingPtr, Dma_Bd *BdPtr)
+*
+*****************************************************************************/
+#define Dma_mBdRingPrev(RingPtr, BdPtr)               \
+    (((u32)(BdPtr) <= (RingPtr)->FirstBdAddr) ?       \
+      (Dma_Bd*)(RingPtr)->LastBdAddr :                \
+      (Dma_Bd*)((u32)(BdPtr) - (RingPtr)->Separation))
+
+
+/******************************************************************************
+ * Move the BdPtr argument ahead an arbitrary number of BDs wrapping around
+ * to the beginning of the ring if needed.
+ *
+ * We know that a wraparound should occur if the new BdPtr is greater than
+ * the high address in the ring OR if the new BdPtr crosses the 0xFFFFFFFF
+ * to 0 boundary.
+ *
+ * @param RingPtr is the ring BdPtr appears in
+ * @param BdPtr on input is the starting BD position and on output is the
+ *        final BD position
+ * @param NumBd is the number of BD spaces to increment
+ *
+ *****************************************************************************/
+#define Dma_mRingSeekahead(RingPtr, BdPtr, NumBd)         \
+  {                   \
+    u32 Addr = (u32)(BdPtr);            \
+                      \
+    Addr += ((RingPtr)->Separation * (NumBd));        \
+    if ((Addr > (RingPtr)->LastBdAddr) || ((u32)(BdPtr) > Addr))\
+    {                 \
+      Addr -= (RingPtr)->Length;          \
+    }                 \
+                      \
+    (BdPtr) = (Dma_Bd*)Addr;            \
+  }
+
+
+/******************************************************************************
+ * Move the BdPtr argument backwards an arbitrary number of BDs wrapping
+ * around to the end of the ring if needed.
+ *
+ * We know that a wraparound should occur if the new BdPtr is less than
+ * the base address in the ring OR if the new BdPtr crosses the 0xFFFFFFFF
+ * to 0 boundary.
+ *
+ * @param RingPtr is the ring BdPtr appears in
+ * @param BdPtr on input is the starting BD position and on output is the
+ *        final BD position
+ * @param NumBd is the number of BD spaces to increment
+ *
+ *****************************************************************************/
+#define Dma_mRingSeekback(RingPtr, BdPtr, NumBd)            \
+  {                                                                     \
+    u32 Addr = (u32)(BdPtr);              \
+                        \
+    Addr -= ((RingPtr)->Separation * (NumBd));          \
+    if ((Addr < (RingPtr)->FirstBdAddr) || ((u32)(BdPtr) < Addr)) \
+    {                   \
+      Addr += (RingPtr)->Length;            \
+    }                   \
+                        \
+    (BdPtr) = (Dma_Bd*)Addr;              \
+  }
+
+
+/************************* Function Prototypes ******************************/
+
+/*
+ * Descriptor ring functions xdma_bdring.c
+ */
+int Dma_BdRingCreate(Dma_BdRing * RingPtr, u32 PhysAddr,
+      u32 VirtAddr, u32 Alignment, unsigned BdCount);
+int Dma_BdRingStart(Dma_BdRing * RingPtr);
+int Dma_BdRingAlloc(Dma_BdRing * RingPtr, unsigned NumBd, Dma_Bd ** BdSetPtr);
+int Dma_BdRingUnAlloc(Dma_BdRing * RingPtr, unsigned NumBd, Dma_Bd * BdSetPtr);
+int Dma_BdRingToHw(Dma_BdRing * RingPtr, unsigned NumBd, Dma_Bd * BdSetPtr);
+unsigned Dma_BdRingFromHw(Dma_BdRing * RingPtr, unsigned BdLimit,
+                                                    Dma_Bd ** BdSetPtr);
+unsigned Dma_BdRingForceFromHw(Dma_BdRing * RingPtr, unsigned BdLimit,
+                                                   Dma_Bd ** BdSetPtr);
+int Dma_BdRingFree(Dma_BdRing * RingPtr, unsigned NumBd, Dma_Bd * BdSetPtr);
+int Dma_BdRingCheck(Dma_BdRing * RingPtr);
+u32 Dma_BdRingAlign(u32 AllocPtr, u32 Size, u32 Align, u32 * Delta);
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* end of protection macro */
+
diff --git a/arch/arm/mach-zynq/k7_base_driver/xdma/xdma_hw.h b/arch/arm/mach-zynq/k7_base_driver/xdma/xdma_hw.h
new file mode 100644
index 0000000..080a5ee
--- /dev/null
+++ b/arch/arm/mach-zynq/k7_base_driver/xdma/xdma_hw.h
@@ -0,0 +1,464 @@
+/*******************************************************************************
+** © Copyright 2011 - 2012 Xilinx, Inc. All rights reserved.
+** This file contains confidential and proprietary information of Xilinx, Inc. and
+** is protected under U.S. and international copyright and other intellectual property laws.
+*******************************************************************************
+**   ____  ____
+**  /   /\/   /
+** /___/  \  /   Vendor: Xilinx
+** \   \   \/
+**  \   \
+**  /   /
+** \   \  /  \   Kintex-7 PCIe-DMA-DDR3 Base Targeted Reference Design
+**  \___\/\___\
+**
+**  Device: xc7k325t
+**  Reference: UG882
+*******************************************************************************
+**
+**  Disclaimer:
+**
+**    This disclaimer is not a license and does not grant any rights to the materials
+**    distributed herewith. Except as otherwise provided in a valid license issued to you
+**    by Xilinx, and to the maximum extent permitted by applicable law:
+**    (1) THESE MATERIALS ARE MADE AVAILABLE "AS IS" AND WITH ALL FAULTS,
+**    AND XILINX HEREBY DISCLAIMS ALL WARRANTIES AND CONDITIONS, EXPRESS, IMPLIED, OR STATUTORY,
+**    INCLUDING BUT NOT LIMITED TO WARRANTIES OF MERCHANTABILITY, NON-INFRINGEMENT, OR
+**    FITNESS FOR ANY PARTICULAR PURPOSE; and (2) Xilinx shall not be liable (whether in contract
+**    or tort, including negligence, or under any other theory of liability) for any loss or damage
+**    of any kind or nature related to, arising under or in connection with these materials,
+**    including for any direct, or any indirect, special, incidental, or consequential loss
+**    or damage (including loss of data, profits, goodwill, or any type of loss or damage suffered
+**    as a result of any action brought by a third party) even if such damage or loss was
+**    reasonably foreseeable or Xilinx had been advised of the possibility of the same.
+
+
+**  Critical Applications:
+**
+**    Xilinx products are not designed or intended to be fail-safe, or for use in any application
+**    requiring fail-safe performance, such as life-support or safety devices or systems,
+**    Class III medical devices, nuclear facilities, applications related to the deployment of airbags,
+**    or any other applications that could lead to death, personal injury, or severe property or
+**    environmental damage (individually and collectively, "Critical Applications"). Customer assumes
+**    the sole risk and liability of any use of Xilinx products in Critical Applications, subject only
+**    to applicable laws and regulations governing limitations on product liability.
+
+**  THIS COPYRIGHT NOTICE AND DISCLAIMER MUST BE RETAINED AS PART OF THIS FILE AT ALL TIMES.
+
+*******************************************************************************/
+/*****************************************************************************/
+/**
+*
+* @file xdma_hw.h
+*
+* This header file contains identifiers and register-level driver functions
+* and macros that can be used to access the Northwest Logic Scatter-gather
+* Direct Memory Access device.
+*
+* For more information about the operation of this device, see the hardware
+* specification documentation.
+*
+* <pre>
+* MODIFICATION HISTORY:
+*
+* Ver   Who  Date     Changes
+* ----- ---- -------- -------------------------------------------------------
+* 1.0   ps   6/30/09  First version
+* </pre>
+*
+******************************************************************************/
+
+#ifndef XDMA_HW_H   /* prevent circular inclusions */
+#define XDMA_HW_H   /* by using protection macros */
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/***************************** Include Files *********************************/
+
+#include "xio.h"
+
+
+/************************** Constant Definitions *****************************/
+
+/** @name Device register offset definitions. Register access is 32-bit.
+ *  @{
+ */
+
+/* Common DMA registers */
+#define REG_DMA_CTRL_STATUS     0x4000      /**< DMA Common Ctrl & Status */
+
+/* These engine registers are applicable to both S2C and C2S channels.
+ * Register field mask and shift definitions are later in this file.
+ */
+
+#define REG_DMA_ENG_CAP         0x00000000  /**< DMA Engine Capabilities */
+#define REG_DMA_ENG_CTRL_STATUS 0x00000004  /**< DMA Engine Control */
+#define REG_DMA_ENG_NEXT_BD     0x00000008  /**< HW Next desc pointer */
+#define REG_SW_NEXT_BD          0x0000000C  /**< SW Next desc pointer */
+#define REG_DMA_ENG_LAST_BD     0x00000010  /**< HW Last completed pointer */
+#define REG_DMA_ENG_ACTIVE_TIME 0x00000014  /**< DMA Engine Active Time */
+#define REG_DMA_ENG_WAIT_TIME   0x00000018  /**< DMA Engine Wait Time */
+#define REG_DMA_ENG_COMP_BYTES  0x0000001C  /**< DMA Engine Completed Bytes */
+
+/*@}*/
+
+/* Register masks. The following constants define bit locations of various
+ * control bits in the registers. For further information on the meaning of
+ * the various bit masks, refer to the hardware spec.
+ *
+ * Masks have been written assuming HW bits 0-31 correspond to SW bits 0-31
+ */
+
+/** @name Bitmasks of REG_DMA_CTRL_STATUS register.
+ * @{
+ */
+#define DMA_INT_ENABLE              0x00000001  /**< Enable global interrupts */
+#define DMA_INT_DISABLE             0x00000000  /**< Disable interrupts */
+#define DMA_INT_ACTIVE_MASK         0x00000002  /**< Interrupt active? */
+#define DMA_INT_PENDING_MASK        0x00000004  /**< Engine interrupt pending */
+#define DMA_INT_MSI_MODE            0x00000008  /**< MSI or Legacy mode? */
+#define DMA_USER_INT_ENABLE         0x00000010  /**< Enable user interrupts */
+#define DMA_USER_INT_ACTIVE_MASK    0x00000020  /**< Int - user interrupt */
+#define DMA_USER_INT_ACK            0x00000020  /**< Acknowledge */
+#define DMA_MPS_USED                0x00000700  /**< MPS Used */
+#define DMA_MRRS_USED               0x00007000  /**< MRRS Used */
+#define DMA_S2C_ENG_INT_VAL         0x00FF0000  /**< IRQ value of 1st 8 S2Cs */
+#define DMA_C2S_ENG_INT_VAL         0xFF000000  /**< IRQ value of 1st 8 C2Ss */
+
+/** @name Bitmasks of REG_DMA_ENG_CAP register.
+ * @{
+ */
+/* DMA engine characteristics */
+#define DMA_ENG_PRESENT_MASK    0x00000001  /**< DMA engine present? */
+#define DMA_ENG_DIRECTION_MASK  0x00000002  /**< DMA engine direction */
+#define DMA_ENG_C2S             0x00000002  /**< DMA engine - C2S */
+#define DMA_ENG_S2C             0x00000000  /**< DMA engine - S2C */
+#define DMA_ENG_TYPE_MASK       0x00000010  /**< DMA engine type */
+#define DMA_ENG_BLOCK           0x00000000  /**< DMA engine - Block type */
+#define DMA_ENG_PACKET          0x00000010  /**< DMA engine - Packet type */
+#define DMA_ENG_NUMBER          0x0000FF00  /**< DMA engine number */
+#define DMA_ENG_BD_MAX_BC       0x3F000000  /**< DMA engine max buffer size */
+
+/* Shift constants for selected masks */
+#define DMA_ENG_NUMBER_SHIFT        8
+#define DMA_ENG_BD_MAX_BC_SHIFT     24
+
+/*@}*/
+
+
+/** @name Bitmasks of REG_DMA_ENG_CTRL_STATUS register.
+ * @{
+ */
+/* Interrupt activity and acknowledgement bits */
+#define DMA_ENG_INT_ENABLE          0x00000001  /**< Enable interrupts */
+#define DMA_ENG_INT_DISABLE         0x00000000  /**< Disable interrupts */
+#define DMA_ENG_INT_ACTIVE_MASK     0x00000002  /**< Interrupt active? */
+#define DMA_ENG_INT_ACK             0x00000002  /**< Interrupt ack */
+#define DMA_ENG_INT_BDCOMP          0x00000004  /**< Int - BD completion */
+#define DMA_ENG_INT_BDCOMP_ACK      0x00000004  /**< Acknowledge */
+#define DMA_ENG_INT_ALERR           0x00000008  /**< Int - BD align error */
+#define DMA_ENG_INT_ALERR_ACK       0x00000008  /**< Acknowledge */
+#define DMA_ENG_INT_FETERR          0x00000010  /**< Int - BD fetch error */
+#define DMA_ENG_INT_FETERR_ACK      0x00000010  /**< Acknowledge */
+#define DMA_ENG_INT_ABORTERR        0x00000020  /**< Int - DMA abort error */
+#define DMA_ENG_INT_ABORTERR_ACK    0x00000020  /**< Acknowledge */
+#define DMA_ENG_INT_CHAINEND        0x00000080  /**< Int - BD chain ended */
+#define DMA_ENG_INT_CHAINEND_ACK    0x00000080  /**< Acknowledge */
+
+/* DMA engine control */
+#define DMA_ENG_ENABLE_MASK         0x00000100  /**< DMA enabled? */
+#define DMA_ENG_ENABLE              0x00000100  /**< Enable DMA */
+#define DMA_ENG_DISABLE             0x00000000  /**< Disable DMA */
+#define DMA_ENG_STATE_MASK          0x00000C00  /**< Current DMA state? */
+#define DMA_ENG_RUNNING             0x00000400  /**< DMA running */
+#define DMA_ENG_IDLE                0x00000000  /**< DMA idle */
+#define DMA_ENG_WAITING             0x00000800  /**< DMA waiting */
+#define DMA_ENG_STATE_WAITED        0x00001000  /**< DMA waited earlier */
+#define DMA_ENG_WAITED_ACK          0x00001000  /**< Acknowledge */
+#define DMA_ENG_USER_RESET          0x00004000  /**< Reset only user logic */
+#define DMA_ENG_RESET               0x00008000  /**< Reset DMA engine + user */
+
+#define DMA_ENG_ALLINT_MASK         0x000000BE  /**< To get only int events */
+
+/*@}*/
+
+/** @name Bitmasks of performance registers.
+ * @{
+ */
+
+#define REG_DMA_SAMPLE_CTR_MASK     0x00000003  /**< Sync counter for regs */
+
+/* Shift constants for performance registers */
+#define REG_DMA_ENG_ACTIVE_TIME_SHIFT   2
+#define REG_DMA_ENG_WAIT_TIME_SHIFT     2
+#define REG_DMA_ENG_COMP_BYTES_SHIFT    2
+
+/*@}*/
+
+/**************************** Type Definitions *******************************/
+
+/***************** Macros (Inline Functions) Definitions *********************/
+
+#ifdef TH_BH_ISR
+#define INT_COAL_CNT        16  /* Interrupt coalesce count */
+#endif
+
+/* Basic DMA read/write functions - are 32-bit */
+
+#define Dma_mIn32  XIo_In32
+#define Dma_mOut32 XIo_Out32
+
+/*****************************************************************************/
+/**
+*
+* Read the given register.
+*
+* @param    BaseAddress is the base address of the device
+* @param    RegOffset is the register offset to be read
+*
+* @return   The 32-bit value of the register
+*
+* @note
+* C-style signature:
+*    u32 Dma_mReadReg(u32 BaseAddress, u32 RegOffset)
+*
+******************************************************************************/
+#define Dma_mReadReg(BaseAddress, RegOffset)             \
+    Dma_mIn32((BaseAddress) + (RegOffset))
+
+
+/*****************************************************************************/
+/**
+*
+* Write the given register.
+*
+* @param    BaseAddress is the base address of the device
+* @param    RegOffset is the register offset to be written
+* @param    Data is the 32-bit value to write to the register
+*
+* @return   None.
+*
+* @note
+* C-style signature:
+*    void Dma_mWriteReg(u32 BaseAddress, u32 RegOffset, u32 Data)
+*
+******************************************************************************/
+#define Dma_mWriteReg(BaseAddress, RegOffset, Data)     \
+    Dma_mOut32((BaseAddress) + (RegOffset), (Data))
+
+
+/****************************************************************************/
+/**
+* Enable global interrupt bits. This operation will read-modify-write
+* the REG_DMA_CTRL_STATUS register.
+*
+* @param  BaseAddress is the BAR0 address.
+*
+* @note
+* C-style signature:
+*    void Dma_mIntEnable(u32 BaseAddress)
+*
+*****************************************************************************/
+#define Dma_mIntEnable(BaseAddress)        \
+{                           \
+    u32 Reg = Dma_mReadReg(BaseAddress, REG_DMA_CTRL_STATUS);   \
+    Reg |= (DMA_INT_ENABLE | DMA_USER_INT_ENABLE);            \
+    Dma_mWriteReg(BaseAddress, REG_DMA_CTRL_STATUS, Reg);       \
+}
+
+
+/****************************************************************************/
+/**
+* Clear global interrupt enable bits. This operation will read-modify-write
+* the REG_DMA_CTRL_STATUS register.
+*
+* @param  BaseAddress is the BAR0 address.
+*
+* @note
+* C-style signature:
+*    void Dma_mIntDisable(u32 BaseAddress)
+*
+*****************************************************************************/
+#define Dma_mIntDisable(BaseAddress)        \
+{                           \
+    u32 Reg = Dma_mReadReg(BaseAddress, REG_DMA_CTRL_STATUS);   \
+    Reg &= ~(DMA_INT_ENABLE | DMA_USER_INT_ENABLE);           \
+    Dma_mWriteReg(BaseAddress, REG_DMA_CTRL_STATUS, Reg);       \
+}
+
+
+/****************************************************************************/
+/**
+* Acknowledge asserted global interrupts.
+*
+* @param  BaseAddress is the base address of the device
+* @param  Mask has the interrupt signals to be acknowledged and is made
+*         by the caller OR'ing one or more of the INT_*_ACK bits.
+*
+* @note
+* C-style signature:
+*    u32 Dma_mIntAck(u32 BaseAddress, u32 Mask)
+*
+*****************************************************************************/
+/* Currently implemented like this. May have a performance hit. In
+ * that case, will re-implement to avoid the extra read. !!!!
+ */
+#define Dma_mIntAck(BaseAddress, Mask)  \
+{                       \
+    u32 Reg = Dma_mReadReg(BaseAddress, REG_DMA_CTRL_STATUS);   \
+    Reg |= Mask;                    \
+    Dma_mWriteReg(BaseAddress, REG_DMA_CTRL_STATUS, Reg);       \
+}
+
+
+/****************************************************************************/
+/**
+* Retrieve the contents of the DMA engine control & status register
+* REG_DMA_ENG_CTRL_STATUS.
+*
+* @param  InstancePtr is the DMA engine instance to operate on.
+*
+* @return Current contents of the DMA engine control & status register.
+*
+* @note
+* C-style signature:
+*    u32 Dma_mGetCrSr(Dma_Engine * InstancePtr)
+*
+*****************************************************************************/
+#define Dma_mGetCrSr(InstancePtr)                                   \
+    Dma_mReadReg((InstancePtr)->RegBase, REG_DMA_ENG_CTRL_STATUS)   \
+
+
+/****************************************************************************/
+/**
+* Set the contents of the DMA engine control & status register
+* REG_DMA_ENG_CTRL_STATUS.
+*
+* @param  InstancePtr is the DMA engine instance to operate on.
+* @param  Data is the data to write to the DMA engine control register.
+*
+* @note
+* C-style signature:
+*    u32 Dma_mSetCrSr(Dma_Engine* InstancePtr, u32 Data)
+*
+*****************************************************************************/
+#define Dma_mSetCrSr(InstancePtr, Data)                                     \
+    Dma_mWriteReg((InstancePtr)->RegBase, REG_DMA_ENG_CTRL_STATUS, (Data))
+
+
+/****************************************************************************/
+/**
+* Set DMA enable bit for an instance. This operation will read-modify-write
+* the REG_DMA_ENG_CTRL_STATUS register.
+*
+* @param  BaseAddress is the base address of the device
+*
+* @note
+* C-style signature:
+*    void Dma_mEnable(u32 BaseAddress)
+*
+*****************************************************************************/
+#define Dma_mEnable(BaseAddress)                                  \
+{                                                         \
+    u32 val = Dma_mReadReg(BaseAddress, REG_DMA_ENG_CTRL_STATUS);       \
+    val |= DMA_ENG_ENABLE;                                              \
+    Dma_mWriteReg(BaseAddress, REG_DMA_ENG_CTRL_STATUS, val);           \
+}
+
+
+/****************************************************************************/
+/**
+* Check if the current DMA engine is busy with a DMA operation.
+*
+* @param  InstancePtr is the engine instance to operate on.
+*
+* @return TRUE if the DMA is busy. FALSE otherwise.
+*
+* @note
+* C-style signature:
+*    XBoolean Dma_mRunning(Dma_Engine* InstancePtr)
+*
+*****************************************************************************/
+#define Dma_mBdRingBusy(InstancePtr)                          \
+    ((Dma_mGetCrSr(InstancePtr) & DMA_ENG_RUNNING) ? TRUE : FALSE)
+
+
+/****************************************************************************/
+/**
+* Set interrupt enable bits for an instance. This operation will
+* read-modify-write the REG_DMA_ENG_CTRL_STATUS register.
+*
+* @param  InstancePtr is the DMA engine instance to operate on.
+*
+* @note
+* C-style signature:
+*    void Dma_mEngIntEnable(Dma_Engine* InstancePtr)
+*
+*****************************************************************************/
+#define Dma_mEngIntEnable(InstancePtr)                                  \
+{                                                         \
+    /* Interrupts should not be enabled before DMA engine is ready */   \
+    if((InstancePtr)->BdRing.RunState == XST_DMA_SG_IS_STARTED)         \
+    {                                                   \
+        u32 Reg = Dma_mGetCrSr(InstancePtr);                            \
+        Reg |= DMA_ENG_INT_ENABLE;                                      \
+        Dma_mSetCrSr(InstancePtr, Reg);                                 \
+    }                                                     \
+    else log_normal(KERN_NOTICE "DMA Engine not yet ready to enable interrupts\n"); \
+}
+
+
+/****************************************************************************/
+/**
+* Clear interrupt enable bits for a channel. This operation will
+* read-modify-write the REG_DMA_ENG_CTRL_STATUS register.
+*
+* @param  InstancePtr is the DMA engine instance to operate on.
+*
+* @note
+* C-style signature:
+*    void Dma_mEngIntDisable(Dma_Engine* InstancePtr)
+*
+*****************************************************************************/
+#define Dma_mEngIntDisable(InstancePtr)        \
+{                           \
+    u32 Reg = Dma_mGetCrSr(InstancePtr);    \
+    Reg &= ~(DMA_ENG_INT_ENABLE);       \
+    Dma_mSetCrSr(InstancePtr, Reg);         \
+}
+
+
+/****************************************************************************/
+/**
+* Acknowledge asserted engine interrupts.
+*
+* @param  InstancePtr is the DMA engine instance to operate on.
+* @param  Mask has the interrupt signals to be acknowledged and is made
+*         by the caller OR'ing one or more of the INT_*_ACK bits.
+*
+* @note
+* C-style signature:
+*    u32 Dma_mEngIntAck(Dma_Engine* InstancePtr, u32 Mask)
+*
+*****************************************************************************/
+/* Currently implemented like this. May have a performance hit. In
+ * that case, will re-implement to avoid the extra read. !!!!
+ */
+#define Dma_mEngIntAck(InstancePtr, Mask)   \
+{                           \
+    u32 Reg = Dma_mGetCrSr(InstancePtr);    \
+    Reg |= Mask;                        \
+    Dma_mSetCrSr(InstancePtr, Reg);         \
+}
+
+
+/************************** Function Prototypes ******************************/
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* end of protection macro */
+
diff --git a/arch/arm/mach-zynq/k7_base_driver/xdma/xdma_user.c b/arch/arm/mach-zynq/k7_base_driver/xdma/xdma_user.c
new file mode 100644
index 0000000..d294ed6c
--- /dev/null
+++ b/arch/arm/mach-zynq/k7_base_driver/xdma/xdma_user.c
@@ -0,0 +1,542 @@
+/*******************************************************************************
+** © Copyright 2011 - 2012 Xilinx, Inc. All rights reserved.
+** This file contains confidential and proprietary information of Xilinx, Inc. and
+** is protected under U.S. and international copyright and other intellectual property laws.
+*******************************************************************************
+**   ____  ____
+**  /   /\/   /
+** /___/  \  /   Vendor: Xilinx
+** \   \   \/
+**  \   \
+**  /   /
+** \   \  /  \   Kintex-7 PCIe-DMA-DDR3 Base Targeted Reference Design
+**  \___\/\___\
+**
+**  Device: xc7k325t
+**  Reference: UG882
+*******************************************************************************
+**
+**  Disclaimer:
+**
+**    This disclaimer is not a license and does not grant any rights to the materials
+**    distributed herewith. Except as otherwise provided in a valid license issued to you
+**    by Xilinx, and to the maximum extent permitted by applicable law:
+**    (1) THESE MATERIALS ARE MADE AVAILABLE "AS IS" AND WITH ALL FAULTS,
+**    AND XILINX HEREBY DISCLAIMS ALL WARRANTIES AND CONDITIONS, EXPRESS, IMPLIED, OR STATUTORY,
+**    INCLUDING BUT NOT LIMITED TO WARRANTIES OF MERCHANTABILITY, NON-INFRINGEMENT, OR
+**    FITNESS FOR ANY PARTICULAR PURPOSE; and (2) Xilinx shall not be liable (whether in contract
+**    or tort, including negligence, or under any other theory of liability) for any loss or damage
+**    of any kind or nature related to, arising under or in connection with these materials,
+**    including for any direct, or any indirect, special, incidental, or consequential loss
+**    or damage (including loss of data, profits, goodwill, or any type of loss or damage suffered
+**    as a result of any action brought by a third party) even if such damage or loss was
+**    reasonably foreseeable or Xilinx had been advised of the possibility of the same.
+
+
+**  Critical Applications:
+**
+**    Xilinx products are not designed or intended to be fail-safe, or for use in any application
+**    requiring fail-safe performance, such as life-support or safety devices or systems,
+**    Class III medical devices, nuclear facilities, applications related to the deployment of airbags,
+**    or any other applications that could lead to death, personal injury, or severe property or
+**    environmental damage (individually and collectively, "Critical Applications"). Customer assumes
+**    the sole risk and liability of any use of Xilinx products in Critical Applications, subject only
+**    to applicable laws and regulations governing limitations on product liability.
+
+**  THIS COPYRIGHT NOTICE AND DISCLAIMER MUST BE RETAINED AS PART OF THIS FILE AT ALL TIMES.
+
+*******************************************************************************/
+/*****************************************************************************/
+/**
+ *
+ * @file xdma_user.c
+ *
+ * Contains the APIs used by the application-specific drivers.
+ *
+ * Author: Xilinx, Inc.
+ *
+ * 2007-2010 (c) Xilinx, Inc. This file is licensed uner the terms of the GNU
+ * General Public License version 2.1. This program is licensed "as is" without
+ * any warranty of any kind, whether express or implied.
+ *
+ * <pre>
+ * MODIFICATION HISTORY:
+ *
+ * Ver   Date     Changes
+ * ----- -------- -------------------------------------------------------
+ * 1.0   12/07/09 First release
+ * </pre>
+ *
+ *****************************************************************************/
+
+/***************************** Include Files *********************************/
+#include <linux/kernel.h>
+#include <linux/timer.h>
+#include <linux/jiffies.h>
+#include <linux/pci.h>
+
+#include "xbasic_types.h"
+#include "xdebug.h"
+#include "xstatus.h"
+#include "xio.h"
+
+#include "xdma.h"
+#include "xdma_bd.h"
+#include "xdma_user.h"
+#include "xdma_hw.h"
+
+/************************** Function Prototypes ******************************/
+#ifdef DEBUG_VERBOSE
+extern void disp_frag(unsigned char * addr, u32 len);
+#endif
+
+/*****************************************************************************/
+/**
+ * This function must be called by the user driver to register itself with
+ * the base DMA driver. After doing required checks to verify the choice
+ * of engine and BAR register, it initializes the engine and the BD ring
+ * associated with the engine, and enables interrupts if required.
+ *
+ * Only one user is supported per engine at any given time. Incase the
+ * engine has already been registered with another user driver, an error
+ * will be returned.
+ *
+ * @param engine is the DMA engine the user driver wants to use.
+ * @param bar is the BAR register the user driver wants to use.
+ * @param uptr is a pointer to the function callbacks in the user driver.
+ * @param pktsize is the size of packets that the user driver will normally
+ *        use.
+ *
+ * @return NULL incase of any error in completing the registration.
+ * @return Handle with which the user driver is registered.
+ *
+ * @note This function should not be called in an interrupt context
+ *
+ *****************************************************************************/
+void * DmaRegister(int engine, int bar, UserPtrs * uptr, int pktsize)
+{
+    Dma_Engine * eptr;
+    u32 barbase;
+    int result;
+
+    log_verbose(KERN_INFO "User register for engine %d, BAR %d, pktsize %d\n",
+                engine, bar, pktsize);
+
+    if(DriverState != INITIALIZED)
+    {
+        printk(KERN_ERR "DMA driver state %d - not ready\n", DriverState);
+        return NULL;
+    }
+
+    if((bar < 0) || (bar > 5)) {
+        printk(KERN_ERR "Requested BAR %d is not valid\n", bar);
+        return NULL;
+    }
+
+    if(!((dmaData->engineMask) & (1LL << engine))) {
+        printk(KERN_ERR "Requested engine %d does not exist\n", engine);
+        return NULL;
+    }
+    eptr = &(dmaData->Dma[engine]);
+    barbase = (u32)(dmaData->barInfo[bar].baseVAddr);
+
+    if(eptr->EngineState != INITIALIZED) {
+        printk(KERN_ERR "Requested engine %d is not free\n", engine);
+        return NULL;
+    }
+
+    /* Later, add check for reasonable packet size !!!! */
+
+    /* Later, add check for mandatory user function pointers. For optional
+     * ones, assign a stub function pointer. This is better than doing
+     * a NULL value check in the performance path. !!!!
+     */
+
+    /* Copy user-supplied parameters */
+    eptr->user = *uptr;
+    eptr->pktSize = pktsize;
+
+    /* Should check for errors returned here !!!! */
+    (uptr->UserInit)(barbase, uptr->privData);
+
+     spin_lock_init(&(eptr->bdring_lock));
+
+     spin_lock(&(eptr->bdring_lock));
+
+    /* Should inform the user of the errors !!!! */
+  result = descriptor_init(eptr->pdev, eptr);
+  if (result) {
+        /* At this point, handle has not been returned to the user.
+         * So, user refuses to prepare buffers. Will be trying again in
+         * the poll_routine. So, do not abort here.
+         */
+        printk(KERN_ERR "Cannot create BD ring, will try again later.\n");
+        //return NULL;
+    }
+
+    /* Change the state of the engine, and increment the user count */
+    eptr->EngineState = USER_ASSIGNED;
+    dmaData->userCount ++ ;
+
+    /* Start the DMA engine */
+  if (Dma_BdRingStart(&(eptr->BdRing)) == XST_FAILURE) {
+    log_normal(KERN_ERR "DmaRegister: Could not start Dma channel\n");
+    return NULL;
+  }
+
+#ifdef TH_BH_ISR
+    printk("Now enabling interrupts\n");
+    Dma_mEngIntEnable(eptr);
+#endif
+
+     spin_unlock(&(eptr->bdring_lock));
+
+    log_verbose(KERN_INFO "Returning user handle %p\n", eptr);
+
+    return eptr;
+}
+
+/*****************************************************************************/
+/**
+ * This function must be called by the user driver to unregister itself from
+ * the base DMA driver. After doing required checks to verify the handle
+ * and engine state, the DMA engine is reset, interrupts are disabled if
+ * required, and the BD ring is freed, while returning all the packet buffers
+ * to the user driver.
+ *
+ * @param handle is the handle which was assigned during the registration
+ * process.
+ *
+ * @return XST_FAILURE incase of any error
+ * @return 0 incase of success
+ *
+ * @note This function should not be called in an interrupt context
+ *
+ *****************************************************************************/
+int DmaUnregister(void * handle)
+{
+    Dma_Engine * eptr;
+
+    // todo: make sure that caller has ensure that the fifo's are empty
+    // caller should have cleared the fifo before calling this function.
+
+    printk(KERN_INFO "User unregister for handle %p\n", handle);
+    if(DriverState != INITIALIZED)
+    {
+        printk(KERN_ERR "DMA driver state %d - not ready\n", DriverState);
+        return XST_FAILURE;
+    }
+
+    eptr = (Dma_Engine *)handle;
+
+    /* Check if this engine's pointer is valid */
+    if(eptr == NULL)
+    {
+        printk(KERN_ERR "Handle is a NULL value\n");
+        return XST_FAILURE;
+    }
+
+    /* Is the engine assigned to any user? */
+    if(eptr->EngineState != USER_ASSIGNED) {
+        printk(KERN_ERR "Engine is not assigned to any user\n");
+        return XST_FAILURE;
+    }
+
+
+    /* Change DMA engine state */
+    eptr->EngineState = UNREGISTERING;
+
+    /* First, reset DMA engine, so that buffers can be removed. */
+    printk(KERN_INFO "Resetting DMA engine\n");
+    Dma_Reset(eptr);
+
+    /* Next, return all the buffers in the BD ring to the user.
+     * And free the BD ring.
+     */
+
+    printk("Now checking all descriptors\n");
+    descriptor_free(eptr->pdev, eptr);
+
+    /* Change DMA engine state */
+    eptr->EngineState = INITIALIZED;
+    spin_lock(&GlobalDataLock); 
+    dmaData->userCount --;
+    spin_unlock(&GlobalDataLock);
+    printk("DMA driver user count is %d\n", dmaData->userCount);
+
+
+    return 0;
+}
+
+
+#ifdef FIFO_EMPTY_CHECK
+
+void DmaFifoEmptyWait(int handleId, u32 type)
+{
+  u32 barBase = (u32)(dmaData->barInfo[0].baseVAddr);  //todo: need to confirm that this will never change
+  u32 statusReg = barBase + STATUS_REG_OFFSET;
+  u32 data = 0;
+  //todo: need to callibrate the timeout value
+  u32 timeout = FIFO_EMPTY_TIMEOUT;  // effectively its 100 miliseconds
+
+  int check = 1 << (handleId + type);
+
+  check <<= EMPTY_MASK_SHIFT;
+
+  do
+  {
+    data = XIo_In32(statusReg);
+    log_verbose("\n########read reg: 0x%x + 0x%x ==> 0x%x & check:0x%x\n",barBase, STATUS_REG_OFFSET, data,check);
+    if (data & check)
+    {
+      break;
+    }
+    else
+    {
+      log_verbose("\nDDR FIFO Not empty");
+            mdelay(1);
+    }
+  }while(timeout--);
+
+  if(timeout == -1)
+  {
+    log_verbose("************** Timeout DDR FIFO not empty **************");
+  }
+
+}
+
+#endif
+/*****************************************************************************/
+/**
+ * This function must be called by the user driver to unregister itself from
+ * the base DMA driver. After doing required checks to verify the handle
+ * and engine state, the DMA engine is reset, interrupts are disabled if
+ * required, and the BD ring is freed, while returning all the packet buffers
+ * to the user driver.
+ *
+ * @param handle is the handle which was assigned during the registration
+ * process.
+ * @param pkts is a PktBuf array, with the array of packets to be sent.
+ * @param numpkts is the number of packets in the PktBuf array.
+ *
+ * @return 0 incase of any error
+ * @return Number of packets successfully queued for DMA send, incase of
+ * success.
+ *
+ * @note There may not be enough BDs to accomodate the number of packets
+ * requested by the user driver, so the returned value may not match with
+ * the requested numpkts. The user driver must be written to manage this
+ * appropriately.
+ *
+ *
+ *****************************************************************************/
+int DmaSendPkt(void * handle, PktBuf * pkts, int numpkts)
+{
+#if defined DEBUG_NORMAL || defined DEBUG_VERBOSE
+    static int send_count=1;
+#endif
+  int free_bd_count ;
+    Dma_Engine * eptr;
+    Dma_BdRing * rptr;
+    struct pci_dev *pdev;
+  struct privData *lp = NULL;
+  Dma_Bd *BdPtr, *BdCurPtr, *PartialBDPtr=NULL;
+    dma_addr_t bufPA;
+  int result;
+    PktBuf * pbuf;
+    u32 flags, uflags;
+    int i, len;
+    int partialBDcount = 0, partialOK = 0;
+
+    log_verbose(KERN_INFO "User send pkt for engine %p, numpkts %d\n",
+                            handle, numpkts);
+    if(DriverState != INITIALIZED)
+    {
+        printk(KERN_ERR "DMA driver state %d - not ready\n", DriverState);
+        return 0;
+    }
+
+    eptr = (Dma_Engine *)handle;
+    rptr = &(eptr->BdRing);
+
+    /* Check if this engine's pointer is valid */
+    if(eptr == NULL)
+    {
+        printk(KERN_ERR "Handle is a NULL value\n");
+        return 0;
+    }
+
+    /* Is the number valid? */
+    if(numpkts <= 0) {
+        log_normal(KERN_ERR "Packet count should be non-zero\n");
+        return 0;
+    }
+
+    /* Is the engine assigned to any user? */
+    if(eptr->EngineState != USER_ASSIGNED) {
+        log_normal(KERN_ERR "Engine is not assigned to any user\n");
+        return 0;
+    }
+
+    /* Is the engine an S2C one? */
+    if(rptr->IsRxChannel)
+    {
+        log_normal(KERN_ERR "The requested engine cannot send packets\n");
+        return 0;
+    }
+
+    pdev = eptr->pdev;
+    lp = pci_get_drvdata(pdev);
+
+    /* Protect this entry point from the handling of sent packets */
+    spin_lock(&(eptr->bdring_lock));
+
+    /* Ensure that requested number of packets can be queued up */
+    free_bd_count = Dma_mBdRingGetFreeCnt(rptr);
+
+#if defined DEBUG_NORMAL || defined DEBUG_VERBOSE
+    log_normal(KERN_INFO "DmaSendPkt: #%d \n", send_count);
+    send_count += numpkts;
+    log_verbose(KERN_INFO "BD ring %x Free BD count is %d\n",
+                                (u32)rptr, free_bd_count);
+    //disp_frag((unsigned char *)bufVA, pktsize);
+#endif
+
+    /* Maintain a separation between start and end of BD ring. This is
+     * required because DMA will stall if the two pointers coincide -
+     * this will happen whether ring is full or empty.
+     */
+    if(free_bd_count > 2) free_bd_count -= 2;
+    else
+    {
+        log_verbose(KERN_ERR "Not enough BDs to handle %d pkts\n", numpkts);
+        spin_unlock(&(eptr->bdring_lock));
+        return 0;
+    }
+
+    log_normal("DmaSendPkt: numpkts %d free_bd_count %d\n", numpkts, free_bd_count);
+
+    /* Fewer BDs are available than required */
+    if(numpkts > free_bd_count)
+        numpkts = free_bd_count;
+
+    /* Allocate BDs from ring */
+    result = Dma_BdRingAlloc(rptr, numpkts, &BdPtr);
+    if (result != XST_SUCCESS) {
+        /* we really shouldn't get this */
+        printk(KERN_ERR "DmaSendPkt: BdRingAlloc unsuccessful (%d)\n", result);
+        spin_unlock(&(eptr->bdring_lock));
+        return 0;
+    }
+
+    BdCurPtr = BdPtr;
+    partialBDcount = 0;
+    for(i=0; i<numpkts; i++)
+    {
+        pbuf = &(pkts[i]);
+        bufPA = pci_map_single(pdev, pbuf->pktBuf, pbuf->size, PCI_DMA_TODEVICE);
+        log_verbose(KERN_INFO "DmaSendPkt: BD %x buf PA %x VA %x size %d\n",
+                   (u32)BdCurPtr, bufPA, (u32) (pbuf->pktBuf), pbuf->size);
+
+        Dma_mBdSetBufAddr(BdCurPtr, bufPA);
+        Dma_mBdSetCtrlLength(BdCurPtr, pbuf->size);
+        Dma_mBdSetStatLength(BdCurPtr, pbuf->size);      // Required for TX BDs
+        Dma_mBdSetId(BdCurPtr, pbuf->bufInfo);
+
+        uflags = pbuf->flags;
+        flags = 0;
+        if(uflags & DMA_BD_SOP_MASK)
+        {
+            flags |= DMA_BD_SOP_MASK;
+
+            partialOK = (uflags & PKT_ALL) ? 0 : 1;
+            if(!partialOK) PartialBDPtr = BdCurPtr;
+        }
+
+        /* Keep track of whether the buffer is the last one in a packet */
+        if(uflags & DMA_BD_EOP_MASK)
+        {
+            flags |= DMA_BD_EOP_MASK;
+            partialBDcount = 0;
+        }
+        else
+            partialBDcount++;
+        //printk("partialBDcount = %d partialOK = %d PartialBDPtr = %x\n",
+        //                        partialBDcount, partialOK, (u32)PartialBDPtr);
+
+        Dma_mBdSetCtrl(BdCurPtr, flags);                 // No ints also.
+        Dma_mBdSetUserData(BdCurPtr, pbuf->userInfo);
+
+#ifdef TH_BH_ISR
+        /* Enable interrupts for errors and completion based on
+         * coalesce count.
+         */
+        flags |= DMA_BD_INT_ERROR_MASK;
+        if(!(eptr->intrCount % INT_COAL_CNT))
+            flags |= DMA_BD_INT_COMP_MASK;
+        eptr->intrCount += 1;
+        Dma_mBdSetCtrl(BdCurPtr, flags);
+#endif
+
+        log_verbose("DmaSendPkt: free %d BD %x buf PA %x VA %x size %d flags %x\n",
+                 free_bd_count, (u32)BdCurPtr, bufPA, (u32) (pbuf->pktBuf),
+                 pbuf->size, flags);
+
+        BdCurPtr = Dma_mBdRingNext(rptr, BdCurPtr);
+    }
+
+    /* Ensure that the user does not require all or no fragments of a packet
+     * to be handled. For example, incase of a SG-list of multi-BD packets,
+     * the user might require all fragments to be handled together.
+     */
+    if(partialBDcount && !partialOK)
+        log_normal(KERN_ERR "Cannot accomodate %d buffers. Discarding %d.\n",
+                                           numpkts, partialBDcount);
+
+    /* enqueue TxBD with the attached buffer such that it is
+     * ready for frame transmission.
+     */
+    result = Dma_BdRingToHw(rptr, (numpkts-partialBDcount), BdPtr);
+    if((result != XST_SUCCESS) || partialBDcount)
+    {
+        int count=0;
+        if(result != XST_SUCCESS)
+        {
+            /* We should not come here. Incase of error, unmap buffers,
+             * unallocated BDs, and return zero count so that app driver
+             * can recover unused buffers.
+             */
+            printk(KERN_ERR "DmaSendPkt: BdRingToHw unsuccessful (%d)\n",
+                                                               result);
+            BdCurPtr = BdPtr;
+            count = numpkts;
+        }
+        else if(partialBDcount)
+        {
+            /* Don't allow partial packets to be queued for DMA incase user
+             * does not wish it. Unmap buffers, unallocate BDs, return the
+             * count so that app driver can recover unused buffers.
+             */
+            log_verbose(KERN_ERR "DmaSendPkt: Recovering partial buffers\n");
+
+            BdPtr = BdCurPtr = PartialBDPtr;
+            count = partialBDcount;
+        }
+
+        for(i=0; i<count; i++)
+        {
+            bufPA = Dma_mBdGetBufAddr(BdCurPtr);
+            len = Dma_mBdGetCtrlLength(BdCurPtr);
+            pci_unmap_single(pdev, bufPA, len, PCI_DMA_TODEVICE);
+            Dma_mBdSetId(BdCurPtr, NULL);
+            BdCurPtr = Dma_mBdRingNext(rptr, BdCurPtr);
+        }
+        Dma_BdRingUnAlloc(rptr, count, BdPtr);
+        numpkts -= count;
+    }
+
+    spin_unlock(&(eptr->bdring_lock));
+
+    log_verbose("DmaSendPkt: Successfully transmitted %d buffers\n", numpkts);
+    return numpkts;
+}
diff --git a/arch/arm/mach-zynq/k7_base_driver/xrawdata0/Makefile b/arch/arm/mach-zynq/k7_base_driver/xrawdata0/Makefile
new file mode 100644
index 0000000..03c880e
--- /dev/null
+++ b/arch/arm/mach-zynq/k7_base_driver/xrawdata0/Makefile
@@ -0,0 +1,6 @@
+#
+# Makefile for the Xilinx K7 PCIe DMA Base App driver.
+#
+ccflags-y  := -DK7_TRD -DX86_PC -DUSE_IO_MACROS -DNWLDMA -DXRAWDATA0 -I$(srctree)/arch/arm/mach-zynq/k7_base_driver/include/
+obj-y += xrawdata0.o
+xrawdata0-y	:= sguser.o
diff --git a/arch/arm/mach-zynq/k7_base_driver/xrawdata0/sguser.c b/arch/arm/mach-zynq/k7_base_driver/xrawdata0/sguser.c
new file mode 100644
index 0000000..c1b3a4a
--- /dev/null
+++ b/arch/arm/mach-zynq/k7_base_driver/xrawdata0/sguser.c
@@ -0,0 +1,1257 @@
+/*******************************************************************************
+** © Copyright 2011 - 2012 Xilinx, Inc. All rights reserved.
+** This file contains confidential and proprietary information of Xilinx, Inc. and
+** is protected under U.S. and international copyright and other intellectual property laws.
+*******************************************************************************
+**   ____  ____
+**  /   /\/   /
+** /___/  \  /   Vendor: Xilinx
+** \   \   \/
+**  \   \
+**  /   /
+** \   \  /  \   Kintex-7 PCIe-DMA-DDR3 Base Targeted Reference Design
+**  \___\/\___\
+**
+**  Device: xc7k325t
+**  Reference: UG882
+*******************************************************************************
+**
+**  Disclaimer:
+**
+**    This disclaimer is not a license and does not grant any rights to the materials
+**    distributed herewith. Except as otherwise provided in a valid license issued to you
+**    by Xilinx, and to the maximum extent permitted by applicable law:
+**    (1) THESE MATERIALS ARE MADE AVAILABLE "AS IS" AND WITH ALL FAULTS,
+**    AND XILINX HEREBY DISCLAIMS ALL WARRANTIES AND CONDITIONS, EXPRESS, IMPLIED, OR STATUTORY,
+**    INCLUDING BUT NOT LIMITED TO WARRANTIES OF MERCHANTABILITY, NON-INFRINGEMENT, OR
+**    FITNESS FOR ANY PARTICULAR PURPOSE; and (2) Xilinx shall not be liable (whether in contract
+**    or tort, including negligence, or under any other theory of liability) for any loss or damage
+**    of any kind or nature related to, arising under or in connection with these materials,
+**    including for any direct, or any indirect, special, incidental, or consequential loss
+**    or damage (including loss of data, profits, goodwill, or any type of loss or damage suffered
+**    as a result of any action brought by a third party) even if such damage or loss was
+**    reasonably foreseeable or Xilinx had been advised of the possibility of the same.
+
+
+**  Critical Applications:
+**
+**    Xilinx products are not designed or intended to be fail-safe, or for use in any application
+**    requiring fail-safe performance, such as life-support or safety devices or systems,
+**    Class III medical devices, nuclear facilities, applications related to the deployment of airbags,
+**    or any other applications that could lead to death, personal injury, or severe property or
+**    environmental damage (individually and collectively, "Critical Applications"). Customer assumes
+**    the sole risk and liability of any use of Xilinx products in Critical Applications, subject only
+**    to applicable laws and regulations governing limitations on product liability.
+
+**  THIS COPYRIGHT NOTICE AND DISCLAIMER MUST BE RETAINED AS PART OF THIS FILE AT ALL TIMES.
+
+*******************************************************************************/
+/*****************************************************************************/
+/**
+ *
+ * @file xdma_base.c
+ *
+ * This is the Linux base driver for the DMA engine core. It provides
+ * multi-channel DMA capability with the help of the Northwest Logic
+ * DMA engine.
+ *
+ * Author: Xilinx, Inc.
+ *
+ * 2007-2010 (c) Xilinx, Inc. This file is licensed uner the terms of the GNU
+ * General Public License version 2.1. This program is licensed "as is" without
+ * any warranty of any kind, whether express or implied.
+ *
+ * <pre>
+ * MODIFICATION HISTORY:
+ *
+ * Ver   Date     Changes
+ * ----- -------- -------------------------------------------------------
+ * 1.0   12/07/09 First release
+ * </pre>
+ *
+ *****************************************************************************/
+
+#include <linux/version.h>
+#include <linux/module.h>
+#include <linux/delay.h>
+#include <linux/spinlock.h>
+#include <linux/kthread.h>
+
+#include <xdma_user.h>
+#include "xdebug.h"
+#include "xio.h"
+#include "xraw_init.h"
+
+/* Driver states */
+#define UNINITIALIZED   0       /* Not yet come up */
+#define INITIALIZED     1       /* But not yet set up for polling */
+#define POLLING         2       /* But not yet registered with DMA */
+#define REGISTERED      3       /* Registered with DMA */
+#define CLOSED          4       /* Driver is being brought down */
+
+/* DMA characteristics */
+#define MYBAR           0
+
+#ifdef XRAWDATA0
+#define MYNAME   "Raw Data 0"
+#define MYHANDLE  HANDLE_0
+#else
+#define MYNAME   "Raw Data 1"
+#define MYHANDLE  HANDLE_1
+#endif
+
+#ifdef XRAWDATA0
+
+#define TX_CONFIG_ADDRESS       0x9108
+#define RX_CONFIG_ADDRESS       0x9100
+#define PKT_SIZE_ADDRESS        0x9104
+#define STATUS_ADDRESS          0x910C
+
+
+/* Test start / stop conditions */
+#define LOOPBACK            0x00000002  /* Enable TX data loopback onto RX */
+//#define LOOPBACK            0x0001
+
+/* Link status conditions */
+//#define RX_LINK_UP          0x00000080  /**< RX link up / down */
+//#define RX_ALIGNED          0x00000040  /**< RX link aligned */
+
+#else
+
+#define TX_CONFIG_ADDRESS   0x9208  /* Reg for controlling TX data */
+#define RX_CONFIG_ADDRESS   0x9200  /* Reg for controlling RX pkt generator */
+#define PKT_SIZE_ADDRESS    0x9204  /* Reg for programming packet size */
+#define STATUS_ADDRESS      0x920C  /* Reg for checking TX pkt checker status */
+
+/* Test start / stop conditions */
+#define LOOPBACK            0x00000002  /* Enable TX data loopback onto RX */
+#endif
+
+/* Test start / stop conditions */
+#define PKTCHKR             0x00000001  /* Enable TX packet checker */
+#define PKTGENR             0x00000001  /* Enable RX packet generator */
+#define CHKR_MISMATCH       0x00000001  /* TX checker reported data mismatch */
+
+#ifdef XRAWDATA0
+#define ENGINE_TX       0
+#define ENGINE_RX       32
+#else
+#define ENGINE_TX       1
+#define ENGINE_RX       33
+#endif
+
+/* Packet characteristics */
+#define BUFSIZE         (PAGE_SIZE)
+
+#ifdef XRAWDATA0
+#define MAXPKTSIZE      (8*PAGE_SIZE)
+#else
+#define MAXPKTSIZE      (8*PAGE_SIZE)
+#endif
+
+#define MINPKTSIZE      (32)
+#define NUM_BUFS        4000
+#define BUFALIGN        8
+#define BYTEMULTIPLE    8   /**< Lowest sub-multiple of memory path */
+
+
+#define START_ADRS_0  0x9300
+#define START_ADRS_1  0x9310
+#define START_ADRS_2  0x9320
+#define START_ADRS_3  0x9330
+
+#define END_ADRS_0  0x9304
+#define END_ADRS_1  0x9314
+#define END_ADRS_2  0x9324
+#define END_ADRS_3  0x9334
+
+#define WRBURST_0 0x9308
+#define WRBURST_1 0x9318
+#define WRBURST_2 0x9318
+#define WRBURST_3 0x9318
+
+#define RDBURST_0 0x930C
+#define RDBURST_1 0x931C
+#define RDBURST_2 0x931C
+#define RDBURST_3 0x931C
+
+
+#define BURST_SIZE  64
+
+
+int DriverState0 = UNINITIALIZED;
+struct timer_list poll_timer;
+struct task_struct  *tx_task;
+void * handle[4] = {NULL, NULL, NULL, NULL};
+u32 TXbarbase, RXbarbase;
+u32 polldata = 0xaa55;
+u32 RawTestMode = TEST_STOP;
+u32 RawMinPktSize=MINPKTSIZE, RawMaxPktSize=MAXPKTSIZE;
+
+typedef struct {
+    int TotalNum;
+    int AllocNum;
+    int FirstBuf;
+    int LastBuf;
+    int FreePtr;
+    int AllocPtr;
+    unsigned char * origVA[NUM_BUFS];
+    spinlock_t lockbuf;
+} Buffer;
+
+Buffer TxBufs;
+Buffer RxBufs;
+PktBuf pkts[NUM_BUFS];
+
+/* For exclusion */
+spinlock_t RawLock;
+
+#ifdef XRAWDATA0
+#define DRIVER_NAME         "xrawdata0_driver"
+#define DRIVER_DESCRIPTION  "Xilinx Raw Data0 Driver "
+#else
+#define DRIVER_NAME         "xrawdata1_driver"
+#define DRIVER_DESCRIPTION  "Xilinx Raw Data1 Driver"
+#endif
+
+static void InitBuffers(Buffer * bptr);
+
+#ifdef DATA_VERIFY
+static void FormatBuffer(unsigned char * buf, int pktsize, int bufsize, int fragment);
+static void VerifyBuffer(unsigned char * buf, int size, unsigned long long uinfo);
+#endif
+
+int myInit(unsigned int, unsigned int);
+int myFreePkt(void *, unsigned int *, int, unsigned int);
+static int DmaSetupTransmit(void *, int);
+int myGetRxPkt(void *, PktBuf *, unsigned int, int, unsigned int);
+int myPutTxPkt(void *, PktBuf *, int, unsigned int);
+int myPutRxPkt(void *, PktBuf *, int, unsigned int);
+int mySetState(void * hndl, UserState * ustate, unsigned int privdata);
+int myGetState(void * hndl, UserState * ustate, unsigned int privdata);
+
+int tx_pkt_sender (void *);
+extern unsigned int CRC(unsigned int * buf, int len);
+
+/* For checking data integrity */
+unsigned int TxBufCnt=0;
+unsigned int RxBufCnt=0;
+unsigned int ErrCnt=0;
+
+unsigned short TxSeqNo = 0;
+unsigned short RxSeqNo = 0;
+
+/* Simplistic buffer management algorithm. Buffers must be freed in the
+ * order in which they have been allocated. Out of order buffer frees will
+ * result in the wrong buffer being freed and may cause a system hang during
+ * the next buffer/DMA access.
+ */
+static void InitBuffers(Buffer * bptr)
+{
+    unsigned char * bufVA;
+    int i;
+
+    /* Initialise */
+    bptr->TotalNum = bptr->AllocNum = 0;
+    bptr->FirstBuf = 0;
+    bptr->LastBuf = 0;
+    bptr->FreePtr = 0;
+    bptr->AllocPtr = 0;
+
+    /* Allocate for TX buffer pool - have not taken care of alignment */
+    for(i = 0; i < NUM_BUFS; i++)
+    {
+        if((bufVA = (unsigned char *)__get_free_pages(GFP_KERNEL, get_order(BUFSIZE))) == NULL)
+        {
+            printk("InitBuffers: Unable to allocate [%d] TX buffer for data\n", i);
+            break;
+        }
+        bptr->origVA[i] = bufVA;
+    }
+    log_verbose("Allocated %d buffers from %p\n", i, (void *)bufVA);
+
+#ifdef USE_LATER
+    /* Do the buffer alignment adjustment, if required */
+    if((u32)bufVA % BUFALIGN)
+        bufVA += (BUFALIGN - ((u32)bufVA & 0xFF));
+#endif
+
+    if(i)
+    {
+        bptr->FirstBuf = 0;
+        bptr->LastBuf = i;      // One more than last valid buffer.
+        bptr->FreePtr = 0;
+        bptr->AllocPtr = 0;
+        bptr->TotalNum = i;
+        bptr->AllocNum = 0;
+    }
+    //printk("Buffers allocated first %x last %x, number %d\n",
+    //                (u32)(bptr->origVA[0]), (u32)(bptr->origVA[i-1]), i);
+}
+
+static inline unsigned char * AllocBuf(Buffer * bptr)
+{
+    unsigned char * cptr;
+    int freeptr;
+
+    if(bptr->AllocNum == bptr->TotalNum)
+    {
+        //printk("No buffers available to allocate\n");
+        return NULL;
+    }
+
+    freeptr = bptr->FreePtr;
+
+    //printk("Before allocating:\n");
+    //printk("Allocated %d buffers\n", bptr->AllocNum);
+    //printk("FreePtr %d AllocPtr %d\n", bptr->FreePtr, bptr->AllocPtr);
+
+    cptr = bptr->origVA[freeptr];
+    bptr->FreePtr ++;
+    if(bptr->FreePtr == bptr->LastBuf)
+        bptr->FreePtr = 0;
+    bptr->AllocNum++;
+
+    //printk("After allocating:\n");
+    //printk("Allocated %d buffers\n", bptr->AllocNum);
+    //printk("FreePtr %d AllocPtr %d\n", bptr->FreePtr, bptr->AllocPtr);
+
+    return cptr;
+}
+
+static inline void FreeUsedBuf(Buffer * bptr, int num)
+{
+    /* This version differs from FreeUnusedBuf() in that this frees used
+     * buffers by moving the AllocPtr, while the other frees unused buffers
+     * by moving the FreePtr.
+     */
+    int i;
+
+    //printk("Trying to free %d buffers\n", num);
+    //printk("Allocated %d buffers\n", bptr->AllocNum);
+    //printk("FreePtr %d AllocPtr %d\n", bptr->FreePtr, bptr->AllocPtr);
+
+    for(i=0; i<num; i++)
+    {
+        if(bptr->AllocNum == 0)
+        {
+            printk("No more buffers allocated, cannot free anything\n");
+            break;
+        }
+
+        bptr->AllocPtr ++;
+        if(bptr->AllocPtr == bptr->LastBuf)
+            bptr->AllocPtr = 0;
+        bptr->AllocNum--;
+    }
+    //printk("After freeing:\n");
+    //printk("Allocated %d buffers\n", bptr->AllocNum);
+    //printk("FreePtr %d AllocPtr %d\n", bptr->FreePtr, bptr->AllocPtr);
+}
+
+static inline void FreeUnusedBuf(Buffer * bptr, int num)
+{
+    /* This version differs from FreeUsedBuf() in that this frees unused buffers
+     * by moving the FreePtr, while the other frees used buffers by moving
+     * the AllocPtr.
+     */
+    int i;
+
+    //printk("Trying to free %d unused buffers\n", num);
+    //printk("Allocated %d buffers\n", bptr->AllocNum);
+    //printk("FreePtr %d AllocPtr %d\n", bptr->FreePtr, bptr->AllocPtr);
+
+    for(i=0; i<num; i++)
+    {
+        if(bptr->AllocNum == 0)
+        {
+            printk("No more buffers allocated, cannot free anything\n");
+            break;
+        }
+
+        if(bptr->FreePtr == 0)
+            bptr->FreePtr = bptr->LastBuf;
+        bptr->FreePtr --;
+        bptr->AllocNum--;
+    }
+    //printk("After freeing:\n");
+    //printk("Allocated %d buffers\n", bptr->AllocNum);
+    //printk("FreePtr %d AllocPtr %d\n", bptr->FreePtr, bptr->AllocPtr);
+}
+
+static inline void PrintSummary(void)
+{
+#ifdef USE_LATER
+    u32 val;
+
+    printk("---------------------------------------------------\n");
+    printk("%s Driver results Summary:-\n", MYNAME);
+    printk("Current Run Min Packet Size = %d, Max Packet Size = %d\n",
+                            RawMinPktSize, RawMaxPktSize);
+    printk("Buffers Transmitted = %u, Buffers Received = %u, Error Count = %u\n", TxBufCnt, RxBufCnt, ErrCnt);
+    printk("TxSeqNo = %u, RxSeqNo = %u\n", TxSeqNo, RxSeqNo);
+
+    val = XIo_In32(TXbarbase+STATUS_ADDRESS);
+    printk("Data Mismatch Status = %x\n", val);
+
+    printk("---------------------------------------------------\n");
+#endif
+}
+
+#ifdef DATA_VERIFY
+static void FormatBuffer(unsigned char * buf, int pktsize, int bufsize, int fragment)
+{
+    int i;
+
+    /* Apply data pattern in the buffer */
+    for(i = 0; i < bufsize - 1; i = i+2)
+        *(unsigned short *)(buf + i) = TxSeqNo;
+
+    /* Update header information for the first fragment in packet */
+    if(!fragment)
+    {
+        /* Apply packet length and sequence number */
+        *(unsigned short *)(buf + 0) = pktsize;
+        *(unsigned short *)(buf + 2) = TxSeqNo;
+    }
+  // else
+  //  printk("TX Buffer has:Size %d  \n",bufsize);
+
+   if(bufsize % 2)
+    {
+        *(unsigned short *)(buf + bufsize - 1) = TxSeqNo;
+    }
+
+#ifdef DEBUG_VERBOSE
+    printk("TX Buffer has:\n");
+    for(i=0; i<bufsize; i++)
+    {
+        if(!(i % 32)) printk("\n");
+        printk("%02x ", buf[i]);
+    }
+    printk("\n");
+#endif
+}
+#endif
+
+#ifdef DATA_VERIFY
+static void VerifyBuffer(unsigned char * buf, int size, unsigned long long uinfo)
+{
+    unsigned short check2;
+    unsigned char * bptr;
+
+  int check_bit = 2;
+
+  if (size < 2)
+    return;
+
+  if(size%2)
+    check_bit += 1;
+
+#ifdef DEBUG_VERBOSE
+    int i;
+
+    printk("%s: RX packet len %d uinfo %llx\n", MYNAME, size, uinfo);
+    for(i=0; i<size; i++)
+    {
+        if(i && !(i % 16)) printk("\n");
+        printk("%02x ", buf[i]);
+    }
+    printk("\n");
+#endif
+
+    bptr = buf+size;
+
+    check2 = *(unsigned short *)(bptr-check_bit);
+
+    if(check2 != RxSeqNo)
+    {
+        ErrCnt++;
+        printk("Mismatch: Size %x SeqNo %x uinfo %x, buf check2 %x  bit %x\n",
+                        size, (*(unsigned short *)(buf+2)),
+                        (unsigned int)uinfo, check2,check_bit);
+        printk("RxSeqNo %x\n", RxSeqNo);
+        /* Update RxSeqNo */
+        RxSeqNo = check2;
+    }
+}
+#endif
+
+int myInit(unsigned int barbase, unsigned int privdata)
+{
+    log_normal("Reached myInit with barbase %x and privdata %x\n",
+                barbase, privdata);
+
+    spin_lock(&RawLock);
+    if(privdata == 0x54545454)  // So that this is done only once
+    {
+        TXbarbase = barbase;
+    }
+    else if(privdata == 0x54545456)  // So that this is done only once
+    {
+        RXbarbase = barbase;
+    }
+    TxBufCnt = 0; RxBufCnt = 0;
+    ErrCnt = 0;
+    TxSeqNo = RxSeqNo = 0;
+
+    /* Stop any running tests. The driver could have been unloaded without
+     * stopping running tests the last time. Hence, good to reset everything.
+     */
+    XIo_Out32(TXbarbase+TX_CONFIG_ADDRESS, 0);
+    XIo_Out32(TXbarbase+RX_CONFIG_ADDRESS, 0);
+
+    spin_unlock(&RawLock);
+
+    return 0;
+}
+
+int myPutRxPkt(void * hndl, PktBuf * vaddr, int numpkts, unsigned int privdata)
+{
+    int i, unused=0;
+    unsigned int flags;
+
+    //printk("Reached myPutRxPkt with handle %p, VA %x, size %d, privdata %x\n",
+    //            hndl, (u32)vaddr, size, privdata);
+
+    /* Check driver state */
+    if(DriverState0 != REGISTERED)
+    {
+        printk("Driver does not seem to be ready\n");
+        return -1;
+    }
+
+    /* Check handle value */
+    if(hndl != handle[2])
+    {
+        log_normal("Came with wrong handle %x\n", (u32)hndl);
+        return -1;
+    }
+
+    spin_lock(&RawLock);
+
+    for(i=0; i<numpkts; i++)
+    {
+        flags = vaddr->flags;
+        //printk("RX pkt flags %x\n", flags);
+        if(flags & PKT_UNUSED)
+        {
+            unused = 1;
+            break;
+        }
+        if(flags & PKT_EOP)
+        {
+#ifdef DATA_VERIFY
+            VerifyBuffer((unsigned char *)(vaddr->bufInfo), (vaddr->size), (vaddr->userInfo));
+#endif
+            RxSeqNo++;
+        }
+        RxBufCnt++;
+        vaddr++;
+    }
+
+    /* Return packet buffers to free pool */
+
+    //printk("PutRxPkt: Freeing %d packets unused %d\n", numpkts, unused);
+    if(unused)
+        FreeUnusedBuf(&RxBufs, numpkts);
+    else
+        FreeUsedBuf(&RxBufs, numpkts);
+
+    spin_unlock(&RawLock);
+
+    return 0;
+}
+
+int myGetRxPkt(void * hndl, PktBuf * vaddr, unsigned int size, int numpkts, unsigned int privdata)
+{
+    unsigned char * bufVA;
+    PktBuf * pbuf;
+    int i;
+
+    //printk(KERN_INFO "myGetRxPkt: Came with handle %p size %d privdata %x\n",
+    //                        hndl, size, privdata);
+
+    /* Check driver state */
+    if(DriverState0 != REGISTERED)
+    {
+        log_verbose(KERN_INFO"Driver does not seem to be ready\n");
+        return 0;
+    }
+
+    /* Check handle value */
+    if(hndl != handle[2])
+    {
+        log_verbose(KERN_INFO"Came with wrong handle\n");
+        return 0;
+    }
+
+    /* Check size value */
+    if(size != BUFSIZE)
+        log_verbose(KERN_INFO"myGetRxPkt: Requested size %d does not match mine %d\n",
+                                size, (u32)BUFSIZE);
+
+    spin_lock(&RawLock);
+
+    for(i=0; i<numpkts; i++)
+    {
+        pbuf = &(vaddr[i]);
+        /* Allocate a buffer. DMA driver will map to PCI space. */
+        bufVA = AllocBuf(&RxBufs);
+        log_verbose(KERN_INFO "myGetRxPkt: The buffer after alloc is at address %x size %d\n",
+                            (u32) bufVA, (u32) BUFSIZE);
+        if (bufVA == NULL)
+        {
+            log_normal(KERN_ERR "RX: AllocBuf failed\n");
+            break;
+        }
+
+        pbuf->pktBuf = bufVA;
+        pbuf->bufInfo = bufVA;
+        pbuf->size = BUFSIZE;
+    }
+    spin_unlock(&RawLock);
+
+    log_verbose(KERN_INFO "Requested %d, allocated %d buffers\n", numpkts, i);
+    return i;
+}
+
+int myPutTxPkt(void * hndl, PktBuf * vaddr, int numpkts, unsigned int privdata)
+{
+    int nomore=0;
+    int i;
+    unsigned int flags;
+
+    log_verbose(KERN_INFO "Reached myPutTxPkt with handle %p, numpkts %d, privdata %x\n",
+                hndl, numpkts, privdata);
+
+    /* Check driver state */
+    if(DriverState0 != REGISTERED)
+    {
+        log_verbose(KERN_INFO"Driver does not seem to be ready\n");
+        return -1;
+    }
+
+    /* Check handle value */
+    if(hndl != handle[0])
+    {
+        log_verbose(KERN_INFO"Came with wrong handle\n");
+        return -1;
+    }
+
+    /* Just check if we are on the way out */
+    for(i=0; i<numpkts; i++)
+    {
+        flags = vaddr->flags;
+        //printk("TX pkt flags %x\n", flags);
+        if(flags & PKT_UNUSED)
+        {
+            nomore = 1;
+            break;
+        }
+        vaddr++;
+    }
+
+    spin_lock(&(TxBufs.lockbuf));
+
+    /* Return packet buffer to free pool */
+    //printk("PutTxPkt: Freed %d packets nomore %d\n", numpkts, nomore);
+    if(nomore)
+        FreeUnusedBuf(&TxBufs, numpkts);
+    else
+        FreeUsedBuf(&TxBufs, numpkts);
+    spin_unlock(&(TxBufs.lockbuf));
+    return 0;
+}
+
+int mySetState(void * hndl, UserState * ustate, unsigned int privdata)
+{
+    int val;
+    static unsigned int testmode;
+
+    log_verbose(KERN_INFO "Reached mySetState with privdata %x\n", privdata);
+
+    /* Check driver state */
+    if(DriverState0 != REGISTERED)
+    {
+        log_verbose(KERN_INFO"Driver does not seem to be ready\n");
+        return EFAULT;
+    }
+
+    /* Check handle value */
+    if((hndl != handle[0]) && (hndl != handle[2]))
+    {
+        log_verbose(KERN_INFO"Came with wrong handle\n");
+        return EBADF;
+    }
+
+    /* Valid only for TX engine */
+    if(privdata == 0x54545454)
+    {
+        spin_lock(&RawLock);
+
+        /* Set up the value to be written into the register */
+        RawTestMode = ustate->TestMode;
+
+        if(RawTestMode & TEST_START)
+        {
+            testmode = 0;
+            if(RawTestMode & ENABLE_LOOPBACK) testmode |= LOOPBACK;
+            if(RawTestMode & ENABLE_PKTCHK) testmode |= PKTCHKR;
+            if(RawTestMode & ENABLE_PKTGEN) testmode |= PKTGENR;
+        }
+        else
+        {
+            /* Deliberately not clearing the loopback bit, incase a
+             * loopback test was going on - allows the loopback path
+             * to drain off packets. Just stopping the source of packets.
+             */
+            if(RawTestMode & ENABLE_PKTCHK) testmode &= ~PKTCHKR;
+            if(RawTestMode & ENABLE_PKTGEN) testmode &= ~PKTGENR;
+        }
+
+        log_verbose("SetState TX with RawTestMode %x, reg value %x\n",
+                                                    RawTestMode, testmode);
+
+        /* Now write the registers */
+        if(RawTestMode & TEST_START)
+        {
+            if(!(RawTestMode & (ENABLE_PKTCHK|ENABLE_PKTGEN|ENABLE_LOOPBACK)))
+            {
+                printk("%s Driver: TX Test Start with wrong mode %x\n",
+                                                MYNAME, testmode);
+                RawTestMode = 0;
+                spin_unlock(&RawLock);
+                return EBADRQC;
+            }
+
+            log_verbose("%s Driver: Starting the test - mode %x, reg %x\n",
+                                            MYNAME, RawTestMode, testmode);
+
+            /* Next, set packet sizes. Ensure they don't exceed PKTSIZEs */
+            RawMinPktSize = ustate->MinPktSize;
+            RawMaxPktSize = ustate->MaxPktSize;
+
+            /* Set RX packet size for memory path */
+            val = RawMaxPktSize;
+            if(val % BYTEMULTIPLE)
+      {
+        log_verbose("********** ODD PACKET SIZE **********\n");
+              //val -= (val % BYTEMULTIPLE);
+      }
+            log_verbose("Reg %x = %x\n", PKT_SIZE_ADDRESS, val);
+            RawMinPktSize = RawMaxPktSize = val;
+
+            /* Now ensure the sizes remain within bounds */
+            if(RawMaxPktSize > MAXPKTSIZE)
+                RawMinPktSize = RawMaxPktSize = MAXPKTSIZE;
+            if(RawMinPktSize < MINPKTSIZE)
+                RawMinPktSize = RawMaxPktSize = MINPKTSIZE;
+            if(RawMinPktSize > RawMaxPktSize)
+                RawMinPktSize = RawMaxPktSize;
+            val = RawMaxPktSize;
+
+            log_verbose("========Reg %x = %d\n", PKT_SIZE_ADDRESS, val);
+            XIo_Out32(TXbarbase+PKT_SIZE_ADDRESS, val);
+            log_verbose("RxPktSize %d\n", val);
+
+/*
+      #ifdef XRAWDATA0
+        XIo_Out32(TXbarbase+START_ADRS_0, 0x00000000);
+        XIo_Out32(TXbarbase+START_ADRS_1, 0x04000000);
+        XIo_Out32(TXbarbase+END_ADRS_0, 0x03000000);
+        XIo_Out32(TXbarbase+END_ADRS_1, 0x07000000);
+        XIo_Out32(TXbarbase+WRBURST_0, BURST_SIZE );
+        XIo_Out32(TXbarbase+RDBURST_0, BURST_SIZE );
+        XIo_Out32(TXbarbase+WRBURST_1, BURST_SIZE );
+        XIo_Out32(TXbarbase+RDBURST_1, BURST_SIZE );
+      #else
+        XIo_Out32(TXbarbase+START_ADRS_2, 0x08000000);
+        XIo_Out32(TXbarbase+START_ADRS_3, 0x0C000000);
+        XIo_Out32(TXbarbase+END_ADRS_2, 0x0B000000);
+        XIo_Out32(TXbarbase+END_ADRS_3, 0x0F000000);
+        XIo_Out32(TXbarbase+WRBURST_2, BURST_SIZE );
+        XIo_Out32(TXbarbase+RDBURST_2, BURST_SIZE );
+        XIo_Out32(TXbarbase+WRBURST_3, BURST_SIZE );
+        XIo_Out32(TXbarbase+RDBURST_3, BURST_SIZE );
+      #endif
+*/
+
+/* Incase the last test was a loopback test, that bit may not be cleared. */
+            XIo_Out32(TXbarbase+TX_CONFIG_ADDRESS, 0);
+            mdelay(200);
+            if(RawTestMode & (ENABLE_PKTCHK|ENABLE_LOOPBACK))
+            {
+                TxSeqNo = 0;
+                if(RawTestMode & ENABLE_LOOPBACK)
+                    RxSeqNo = 0;
+                log_verbose("========Reg %x = %x\n", TX_CONFIG_ADDRESS, testmode);
+                XIo_Out32(TXbarbase+TX_CONFIG_ADDRESS, testmode);
+                mdelay(200);
+                
+            }
+            if(RawTestMode & ENABLE_PKTGEN)
+            {
+                RxSeqNo = 0;
+                log_verbose("========Reg %x = %x\n", RX_CONFIG_ADDRESS, testmode);
+                XIo_Out32(TXbarbase+RX_CONFIG_ADDRESS, testmode);
+                mdelay(200);
+            }
+
+        }
+        /* Else, stop the test. Do not remove any loopback here because
+         * the DMA queues and hardware FIFOs must drain first.
+         */
+        else
+        {
+            log_verbose("%s Driver: Stopping the test, mode %x\n", MYNAME, testmode);
+            log_verbose("========Reg %x = %x\n", TX_CONFIG_ADDRESS, testmode);
+            XIo_Out32(TXbarbase+TX_CONFIG_ADDRESS, testmode);
+            log_verbose("========Reg %x = %x\n", RX_CONFIG_ADDRESS, testmode);
+            XIo_Out32(TXbarbase+RX_CONFIG_ADDRESS, testmode);
+
+            /* Not resetting sequence numbers here - causes problems
+             * in debugging. Instead, reset the sequence numbers when
+             * starting a test.
+             */
+        }
+
+        PrintSummary();
+        spin_unlock(&RawLock);
+    }
+
+    return 0;
+}
+
+int myGetState(void * hndl, UserState * ustate, unsigned int privdata)
+{
+    static int iter=0;
+
+    log_verbose("Reached myGetState with privdata %x\n", privdata);
+
+    /* Same state is being returned for both engines */
+
+    ustate->LinkState = LINK_UP;
+    ustate->Errors = 0;
+    ustate->MinPktSize = RawMinPktSize;
+    ustate->MaxPktSize = RawMaxPktSize;
+    ustate->TestMode = RawTestMode;
+    if(privdata == 0x54545454)
+        ustate->Buffers = TxBufs.TotalNum;
+    else
+        ustate->Buffers = RxBufs.TotalNum;
+
+    if(iter++ >= 4)
+    {
+        PrintSummary();
+
+        iter = 0;
+    }
+
+    return 0;
+}
+
+static int DmaSetupTransmit(void * hndl, int num)
+{
+    int i, result;
+    static int pktsize=0;
+    int total, bufsize, fragment;
+    int bufindex;
+    unsigned char * bufVA;
+    PktBuf * pbuf;
+    int origseqno;
+    //static unsigned short lastno=0;
+
+    log_verbose("Reached DmaSetupTransmit with handle %p, num %d\n", hndl, num);
+
+    /* Check driver state */
+    if(DriverState0 != REGISTERED)
+    {
+        printk("Driver does not seem to be ready\n");
+        return 0;
+    }
+
+    /* Check handle value */
+    if(hndl != handle[0])
+    {
+        log_verbose(KERN_INFO"Came with wrong handle\n");
+        return 0;
+    }
+
+    /* Check number of packets */
+    if(!num)
+    {
+        printk("Came with 0 packets for sending\n");
+        return 0;
+    }
+
+    /* Hold the spinlock only when calling the buffer management APIs. */
+    spin_lock(&(TxBufs.lockbuf));
+    origseqno = TxSeqNo;
+    for(i=0, bufindex=0; i<num; i++)            /* Total packets loop */
+    {
+        //printk("i %d bufindex %d\n", i, bufindex);
+
+            /* Fix the packet size to be the maximum entered in GUI */
+            pktsize = RawMaxPktSize;
+
+        //printk("pktsize is %d\n", pktsize);
+
+        total = 0;
+        fragment = 0;
+        while(total < pktsize)      /* Packet fragments loop */
+        {
+            //printk("Buf loop total %d bufindex %d\n", total, bufindex);
+
+            pbuf = &(pkts[bufindex]);
+
+            /* Allocate a buffer. DMA driver will map to PCI space. */
+            bufVA = AllocBuf(&TxBufs);
+
+            log_verbose(KERN_INFO "TX: The buffer after alloc is at address %x size %d\n",
+                                (u32) bufVA, (u32) BUFSIZE);
+            if (bufVA == NULL)
+            {
+                //printk("TX: AllocBuf failed\n");
+                //printk("[%d]", (num-i-1));
+                break;
+            }
+            pbuf->pktBuf = bufVA;
+            pbuf->bufInfo = bufVA;
+            bufsize = ((total + BUFSIZE) > pktsize) ?
+                                        (pktsize - total) : BUFSIZE ;
+            total += bufsize;
+
+            //printk("bufsize %d total %d\n", bufsize, total);
+
+#ifdef DATA_VERIFY
+            log_verbose(KERN_INFO "Calling FormatBuffer pktsize %d bufsize %d fragment %d\n",
+                                pktsize, bufsize, fragment);
+            FormatBuffer(bufVA, pktsize, bufsize, fragment);
+#endif
+
+            pbuf->size = bufsize;
+            pbuf->userInfo = TxSeqNo;
+            pbuf->flags = PKT_ALL;
+            if(!fragment)
+                pbuf->flags |= PKT_SOP;
+            if(total == pktsize)
+            {
+                pbuf->flags |= PKT_EOP;
+                pbuf->size = bufsize;
+            }
+
+            //printk("flags %x\n", pbuf->flags);
+            //printk("TxSeqNo %u\n", TxSeqNo);
+
+            bufindex++;
+            fragment++;
+        }
+        if(total < pktsize)
+        {
+            /* There must have been some error in the middle of the packet */
+
+            if(fragment)
+            {
+                /* First, adjust the number of buffers to queue up or else
+                 * partial packets will get transmitted, which will cause a
+                 * problem later.
+                 */
+                bufindex -= fragment;
+
+                /* Now, free any unused buffers from the partial packet, so
+                 * that buffers are not lost.
+                 */
+                log_normal(KERN_ERR "Tried to send pkt of size %d, only %d fragments possible\n",
+                                                pktsize, fragment);
+                FreeUnusedBuf(&TxBufs, fragment);
+            }
+            break;
+        }
+
+
+        /* Increment packet sequence number */
+        //if(lastno != TxSeqNo) printk(" %u-%u.", lastno, TxSeqNo);
+        TxSeqNo++;
+        //lastno = TxSeqNo;
+    }
+    spin_unlock(&(TxBufs.lockbuf));
+
+    //printk("[p%d-%d-%d] ", num, i, bufindex);
+
+    if(i == 0)
+        /* No buffers available */
+        return 0;
+
+    log_verbose("%s: Sending packet length %d seqno %d\n",
+                                        MYNAME, pktsize, TxSeqNo);
+    result = DmaSendPkt(hndl, pkts, bufindex);
+    TxBufCnt += result;
+    if(result != bufindex)
+    {
+        log_normal(KERN_ERR "Tried to send %d pkts in %d buffers, sent only %d\n",
+                                    num, bufindex, result);
+        //printk("[s%d-%d,%d-%d]", bufindex, result, TxSeqNo, origseqno);
+        if(result) TxSeqNo = pkts[result].userInfo;
+        else TxSeqNo = origseqno;
+        //printk("-%u-", TxSeqNo);
+        //lastno = TxSeqNo;
+
+    spin_lock(&(TxBufs.lockbuf));
+        FreeUnusedBuf(&TxBufs, (bufindex-result));
+    spin_unlock(&(TxBufs.lockbuf));
+        return 0;
+    }
+    else return 1;
+}
+
+void raw_transmit_init()
+{
+	    tx_task = kthread_run(tx_pkt_sender, NULL,"Tx_sender");
+
+	    if (IS_ERR(tx_task)) 
+	    {
+		    printk(KERN_ERR"Unable to start TX sending thread!\n");
+	    }
+}
+
+
+
+static int __init rawdata_init(void)
+{
+  /* Just register the driver. No kernel boot options used. */
+  log_verbose(KERN_INFO "%s Init: Inserting Xilinx driver in kernel.\n",
+                                        MYNAME);
+
+    DriverState0 = INITIALIZED;
+    spin_lock_init(&RawLock);
+
+    /* First allocate the buffer pool and set the driver state
+     * because GetPkt routine can potentially be called immediately
+     * after Register is done.
+     */
+    //printk("PAGE_SIZE is %ld\n", PAGE_SIZE);
+    msleep(5);
+
+    InitBuffers(&TxBufs);
+    InitBuffers(&RxBufs);
+
+    spin_lock_init(&(TxBufs.lockbuf));
+    spin_lock_init(&(RxBufs.lockbuf));
+
+    /* Start polling routine - change later !!!! */
+    log_verbose(KERN_INFO "%s Init: Starting poll routine with %x\n",
+                                        MYNAME, polldata);
+#if 0
+    {
+	    tx_task = kthread_run(tx_pkt_sender, NULL,"Tx_sender");
+
+	    if (IS_ERR(tx_task)) 
+	    {
+		    printk(KERN_ERR"Unable to start TX sending thread!\n");
+	    }
+    }
+#endif
+    return 0;
+}
+
+static void __exit rawdata_cleanup(void)
+{
+    int i;
+
+    /* Stop the polling routine */
+    del_timer_sync(&poll_timer);
+    //DriverState0 = CLOSED;
+    kthread_stop(tx_task);
+    /* Stop any running tests, else the hardware's packet checker &
+     * generator will continue to run.
+     */
+    XIo_Out32(TXbarbase+TX_CONFIG_ADDRESS, 0);
+
+    XIo_Out32(TXbarbase+RX_CONFIG_ADDRESS, 0);
+
+    printk(KERN_INFO "%s: Unregistering Xilinx driver from kernel.\n", MYNAME);
+    if (TxBufCnt != RxBufCnt)
+    {
+        printk("%s: Buffers Transmitted %u Received %u\n", MYNAME, TxBufCnt, RxBufCnt);
+        printk("TxSeqNo = %u, RxSeqNo = %u\n", TxSeqNo, RxSeqNo);
+        mdelay(1);
+    }
+#ifdef FIFO_EMPTY_CHECK
+    DmaFifoEmptyWait(MYHANDLE,DIR_TYPE_S2C);
+    // wait for appropriate time to stabalize
+    mdelay(STABILITY_WAIT_TIME);
+#endif
+
+    DmaUnregister(handle[0]);
+
+#ifdef FIFO_EMPTY_CHECK
+    DmaFifoEmptyWait(MYHANDLE,DIR_TYPE_C2S);
+    // wait for appropriate time to stabalize
+    mdelay(STABILITY_WAIT_TIME);
+#endif
+    DmaUnregister(handle[2]);
+
+    PrintSummary();
+
+    mdelay(1000);
+
+    /* Not sure if free_page() sleeps or not. */
+    spin_lock(&(TxBufs.lockbuf));
+    printk("Freeing user buffers\n");
+    for(i=0; i<TxBufs.TotalNum; i++)
+        //kfree(TxBufs.origVA[i]);
+        free_page((unsigned long)(TxBufs.origVA[i]));
+    spin_unlock(&(TxBufs.lockbuf));
+    spin_lock(&RawLock);
+
+    for(i=0; i<RxBufs.TotalNum; i++)
+        //kfree(RxBufs.origVA[i]);
+        free_page((unsigned long)(RxBufs.origVA[i]));
+    spin_unlock(&RawLock);
+}
+
+
+int tx_pkt_sender (void *tempptr)
+{
+//	int i;
+//	struct privData *lp = NULL;
+	long rc;
+//	Dma_Engine *eptr = NULL;
+    UserPtrs ufuncs;
+    int offset = (2*HZ);
+
+	set_user_nice(current, -20); //highest priority
+
+	rc = sched_setaffinity(current->pid, cpumask_of(1));
+	if (rc != 0)
+		printk(KERN_ERR"Couldn't set affinity to 2nd cpu (%ld)\n", rc);
+
+//	lp = pci_get_drvdata(pdev);
+
+	while(1) 
+	{
+		/* Suspend for 1 tick */
+		set_current_state(TASK_UNINTERRUPTIBLE);
+//		set_current_state(TASK_INTERRUPTIBLE);
+		schedule_timeout (1); //10ms on ZynQ
+
+
+    /* Register with DMA incase not already done so */
+    if(DriverState0 < POLLING)
+    {
+        spin_lock(&RawLock);
+        log_verbose(KERN_INFO"Calling DmaRegister on engine %d and %d\n",
+                            ENGINE_TX, ENGINE_RX);
+        DriverState0 = REGISTERED;
+
+        ufuncs.UserInit = myInit;
+        ufuncs.UserPutPkt = myPutTxPkt;
+        ufuncs.UserSetState = mySetState;
+        ufuncs.UserGetState = myGetState;
+        ufuncs.privData = 0x54545454;
+        spin_unlock(&RawLock);
+
+#ifdef FIFO_EMPTY_CHECK
+        // no need, just if h/w is not in good shape, we will see messages in dmesg
+        DmaFifoEmptyWait(MYHANDLE,DIR_TYPE_S2C);
+#endif
+
+        if((handle[0]=DmaRegister(ENGINE_TX, MYBAR, &ufuncs, BUFSIZE)) == NULL)
+        {
+            printk("Register for engine %d failed. Stopping.\n", ENGINE_TX);
+            spin_lock(&RawLock);
+            DriverState0 = UNINITIALIZED;
+            spin_unlock(&RawLock);
+            return -1;     // This polling will not happen again.
+        }
+        log_verbose("Handle for engine %d is %p\n", ENGINE_TX, handle[0]);
+
+        spin_lock(&RawLock);
+        ufuncs.UserInit = myInit;
+        ufuncs.UserPutPkt = myPutRxPkt;
+        ufuncs.UserGetPkt = myGetRxPkt;
+        ufuncs.UserSetState = mySetState;
+        ufuncs.UserGetState = myGetState;
+        ufuncs.privData = 0x54545456;
+        spin_unlock(&RawLock);
+
+#ifdef FIFO_EMPTY_CHECK
+        // no need, just if h/w is not in good shape, we will see messages in dmesg
+        DmaFifoEmptyWait(MYHANDLE,DIR_TYPE_C2S);
+#endif
+        if((handle[2]=DmaRegister(ENGINE_RX, MYBAR, &ufuncs, BUFSIZE)) == NULL)
+        {
+            printk("Register for engine %d failed. Stopping.\n", ENGINE_RX);
+            spin_lock(&RawLock);
+            DriverState0 = UNINITIALIZED;
+            spin_unlock(&RawLock);
+            return -1;     // This polling will not happen again.
+        }
+        log_verbose("Handle for engine %d is %p\n", ENGINE_RX, handle[2]);
+             //   msleep(offset + 1);
+
+#ifdef USE_LATER
+        /* Reschedule poll routine */
+        timer->expires = jiffies + offset;
+        add_timer(timer);
+#endif
+    }
+    else if(DriverState0 == REGISTERED)
+    {
+        /* Only if the test mode is set, and only for TX direction */
+        if((RawTestMode & TEST_START) &&
+           (RawTestMode & (ENABLE_PKTCHK|ENABLE_LOOPBACK)))
+        {
+            spin_lock(&RawLock);
+            /* First change the state */
+            RawTestMode &= ~TEST_START;
+            RawTestMode |= TEST_IN_PROGRESS;
+            spin_unlock(&RawLock);
+
+            /* Now, queue up one packet to start the transmission */
+            DmaSetupTransmit(handle[0], 1);
+
+            /* For the first packet, give some gap before the next TX */
+            offset = HZ;
+         //       msleep(offset + 1);
+        }
+        else if(RawTestMode & TEST_IN_PROGRESS)
+        {
+            int avail;
+            int times;
+
+            //    msleep(offset + 1);
+            for(times = 0; times < 2000; times++)
+            {
+                avail = (TxBufs.TotalNum - TxBufs.AllocNum);
+                if(avail <= 0) break;
+
+                /* Queue up many packets to continue the transmission */
+                if(DmaSetupTransmit(handle[0], 2) == 0) break;
+            }
+
+            /* Do the next TX as soon as possible */
+            offset = 0;
+        }
+
+#ifdef USE_LATER
+        /* Reschedule poll routine */
+        timer->expires = jiffies + offset;
+        add_timer(timer);
+#endif
+    }
+    
+	}
+
+	return 0;
+}
+module_init(rawdata_init);
+module_exit(rawdata_cleanup);
+
+EXPORT_SYMBOL(raw_transmit_init);
+
+MODULE_AUTHOR("Xilinx, Inc.");
+MODULE_DESCRIPTION(DRIVER_DESCRIPTION);
+MODULE_LICENSE("GPL");
+
diff --git a/arch/arm/mm/Makefile b/arch/arm/mm/Makefile
index 224a9cc..d2b996a9 100644
--- a/arch/arm/mm/Makefile
+++ b/arch/arm/mm/Makefile
@@ -6,7 +6,7 @@ obj-y				:= dma-mapping.o extable.o fault.o init.o \
 				   iomap.o
 
 obj-$(CONFIG_MMU)		+= fault-armv.o flush.o idmap.o ioremap.o \
-				   mmap.o pgd.o mmu.o
+				   mmap.o pgd.o mmu.o gup.o
 
 ifneq ($(CONFIG_MMU),y)
 obj-y				+= nommu.o
diff --git a/arch/arm/mm/gup.c b/arch/arm/mm/gup.c
new file mode 100644
index 0000000..75e6be7
--- /dev/null
+++ b/arch/arm/mm/gup.c
@@ -0,0 +1,161 @@
+/*
+ * linux/arch/arm/mm/gup.c - Lockless get_user_pages_fast for arm
+ *
+ * Copyright (c) 2013 Samsung Electronics Co., Ltd.
+ *		http://www.samsung.com
+ * Author : Chanho Park <chanho61.park@samsung.com>
+ *
+ * This code is written on reference from the x86 and PowerPC versions, by:
+ *
+ *	Copyright (C) 2008 Nick Piggin
+ *	Copyright (C) 2008 Novell Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/sched.h>
+#include <linux/mm.h>
+#include <linux/pagemap.h>
+#include <linux/rwsem.h>
+#include <asm/pgtable.h>
+
+/*
+ * The performance critical leaf functions are made noinline otherwise gcc
+ * inlines everything into a single function which results in too much
+ * register pressure.
+ */
+static noinline int gup_pte_range(pmd_t *pmdp, unsigned long addr,
+		unsigned long end, int write, struct page **pages, int *nr)
+{
+	pte_t *ptep, pte;
+
+	ptep = pte_offset_kernel(pmdp, addr);
+	do {
+		struct page *page;
+
+		pte = *ptep;
+		smp_rmb();
+
+		if (!pte_present_user(pte) || (write && !pte_write(pte)))
+			return 0;
+
+		VM_BUG_ON(!pfn_valid(pte_pfn(pte)));
+		page = pte_page(pte);
+
+		if (!page_cache_get_speculative(page))
+			return 0;
+
+		pages[*nr] = page;
+		(*nr)++;
+
+	} while (ptep++, addr += PAGE_SIZE, addr != end);
+
+	return 1;
+}
+
+static int gup_pmd_range(pud_t *pudp, unsigned long addr, unsigned long end,
+		int write, struct page **pages, int *nr)
+{
+	unsigned long next;
+	pmd_t *pmdp;
+
+	pmdp = pmd_offset(pudp, addr);
+	do {
+		next = pmd_addr_end(addr, end);
+		if (pmd_none(*pmdp))
+			return 0;
+		else if (!gup_pte_range(pmdp, addr, next, write, pages, nr))
+			return 0;
+	} while (pmdp++, addr = next, addr != end);
+
+	return 1;
+}
+
+static int gup_pud_range(pgd_t *pgdp, unsigned long addr, unsigned long end,
+		int write, struct page **pages, int *nr)
+{
+	unsigned long next;
+	pud_t *pudp;
+
+	pudp = pud_offset(pgdp, addr);
+	do {
+		next = pud_addr_end(addr, end);
+		if (pud_none(*pudp))
+			return 0;
+		else if (!gup_pmd_range(pudp, addr, next, write, pages, nr))
+			return 0;
+	} while (pudp++, addr = next, addr != end);
+
+	return 1;
+}
+
+int get_user_pages_fast(unsigned long start, int nr_pages, int write,
+			struct page **pages)
+{
+	struct mm_struct *mm = current->mm;
+	unsigned long addr, len, end;
+	unsigned long next;
+	pgd_t *pgdp;
+	int nr = 0;
+
+	start &= PAGE_MASK;
+	addr = start;
+	len = (unsigned long) nr_pages << PAGE_SHIFT;
+	end = start + len;
+
+	if (unlikely(!access_ok(write ? VERIFY_WRITE : VERIFY_READ,
+					start, len)))
+		goto slow_irqon;
+
+	/*
+	 * This doesn't prevent pagetable teardown, but does prevent
+	 * the pagetables from being freed on arm.
+	 *
+	 * So long as we atomically load page table pointers versus teardown,
+	 * we can follow the address down to the the page and take a ref on it.
+	 */
+	local_irq_disable();
+
+	pgdp = pgd_offset(mm, addr);
+	do {
+		next = pgd_addr_end(addr, end);
+		if (pgd_none(*pgdp))
+			goto slow;
+		else if (!gup_pud_range(pgdp, addr, next, write, pages, &nr))
+			goto slow;
+	} while (pgdp++, addr = next, addr != end);
+
+	local_irq_enable();
+
+	VM_BUG_ON(nr != (end - start) >> PAGE_SHIFT);
+	return nr;
+
+	{
+		int ret;
+
+slow:
+		local_irq_enable();
+slow_irqon:
+		/* Try to get the remaining pages with get_user_pages */
+		start += nr << PAGE_SHIFT;
+		pages += nr;
+
+		down_read(&mm->mmap_sem);
+		ret = get_user_pages(current, mm, start,
+			(end - start) >> PAGE_SHIFT, write, 0, pages, NULL);
+		up_read(&mm->mmap_sem);
+
+		/* Have to be a bit careful with return values */
+		if (nr > 0) {
+			if (ret < 0)
+				ret = nr;
+			else
+				ret += nr;
+		}
+
+		return ret;
+	}
+}
+
-- 
1.7.5.4

