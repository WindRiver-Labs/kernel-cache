From 10609a6dcb3cf30eccdc34411a56f952d5ebc2c6 Mon Sep 17 00:00:00 2001
From: Alex Porosanu <alexandru.porosanu@freescale.com>
Date: Wed, 24 Jun 2015 17:28:04 +0300
Subject: [PATCH 260/451] merge CAAM sources (drivers/crypto/caam) from SDK
 1.8

Change-Id: Ie666b21628796a4f2833d70ebf3e35b454182f64
[Xulin: Original patch taken from
Linux-LS1043A-SDK-V0.4-SOURCE-20150826-yocto.iso]
Signed-off-by: Xulin Sun <xulin.sun@windriver.com>
---
 drivers/crypto/caam/Kconfig       |   38 +-
 drivers/crypto/caam/Makefile      |   11 +-
 drivers/crypto/caam/caamalg.c     | 2731 ++++++++++++++++++++++++++++++++++---
 drivers/crypto/caam/caamalg_qi.c  | 2345 +++++++++++++++++++++++++++++++
 drivers/crypto/caam/caamhash.c    |   18 +-
 drivers/crypto/caam/caampkc.c     | 1520 +++++++++++++++++++++
 drivers/crypto/caam/compat.h      |    2 +
 drivers/crypto/caam/ctrl.c        |  490 +++++--
 drivers/crypto/caam/desc.h        |   69 +-
 drivers/crypto/caam/desc_constr.h |   10 +-
 drivers/crypto/caam/error.c       |   10 +
 drivers/crypto/caam/fsl_jr_uio.c  |  273 ++++
 drivers/crypto/caam/fsl_jr_uio.h  |   20 +
 drivers/crypto/caam/intern.h      |   22 +-
 drivers/crypto/caam/jr.c          |  311 ++++--
 drivers/crypto/caam/key_gen.c     |   29 +-
 drivers/crypto/caam/key_gen.h     |    6 +-
 drivers/crypto/caam/pdb.h         |  256 ++++-
 drivers/crypto/caam/pkc_desc.c    |  388 ++++++
 drivers/crypto/caam/pkc_desc.h    |  225 +++
 drivers/crypto/caam/qi.c          |  840 ++++++++++++
 drivers/crypto/caam/qi.h          |  177 +++
 drivers/crypto/caam/regs.h        |    8 +
 drivers/crypto/caam/sg_sw_qm.h    |   82 ++
 drivers/crypto/caam/sg_sw_sec4.h  |   40 +-
 25 files changed, 9449 insertions(+), 472 deletions(-)
 create mode 100644 drivers/crypto/caam/caamalg_qi.c
 create mode 100644 drivers/crypto/caam/caampkc.c
 create mode 100644 drivers/crypto/caam/fsl_jr_uio.c
 create mode 100644 drivers/crypto/caam/fsl_jr_uio.h
 create mode 100644 drivers/crypto/caam/pkc_desc.c
 create mode 100644 drivers/crypto/caam/pkc_desc.h
 create mode 100644 drivers/crypto/caam/qi.c
 create mode 100644 drivers/crypto/caam/qi.h
 create mode 100644 drivers/crypto/caam/sg_sw_qm.h

diff --git a/drivers/crypto/caam/Kconfig b/drivers/crypto/caam/Kconfig
index e7555ff..1d2db39 100644
--- a/drivers/crypto/caam/Kconfig
+++ b/drivers/crypto/caam/Kconfig
@@ -75,7 +75,7 @@ config CRYPTO_DEV_FSL_CAAM_INTC_TIME_THLD
 
 config CRYPTO_DEV_FSL_CAAM_CRYPTO_API
 	tristate "Register algorithm implementations with the Crypto API"
-	depends on CRYPTO_DEV_FSL_CAAM && CRYPTO_DEV_FSL_CAAM_JR
+	depends on CRYPTO_DEV_FSL_CAAM && CRYPTO_DEV_FSL_CAAM_JR && NET
 	default y
 	select CRYPTO_ALGAPI
 	select CRYPTO_AUTHENC
@@ -87,6 +87,37 @@ config CRYPTO_DEV_FSL_CAAM_CRYPTO_API
 	  To compile this as a module, choose M here: the module
 	  will be called caamalg.
 
+config CRYPTO_DEV_FSL_CAAM_CRYPTO_API_QI
+        tristate "Queue Interface as Crypto API backend"
+        depends on CRYPTO_DEV_FSL_CAAM_CRYPTO_API && FSL_QMAN
+        default y
+        help
+          Selecting this will use SEC Queue Interface for sending
+          & receiving crypto jobs to/from SEC. This gives better
+          performance than job ring interface when the number of
+          cores are more than the number of job rings assigned to
+          the kernel. The number of portals assigned to the kernel
+          should also be more than the number of job rings.
+
+          Currently, only AEAD algorithms have been implemented on
+          top of SEC-QI backend interface. The rest of the algorithms
+          use job ring interface.
+
+          To compile this as a module, choose M here: the module
+          will be called caamalg_qi.
+
+config FSL_CAAM_PKC_SUPPORT
+	tristate "Public Key Cryptography Support in CAAM driver"
+	depends on CRYPTO_DEV_FSL_CAAM_CRYPTO_API
+	default y
+	help
+	  Selecting this will allow SEC Public key support for
+	  RSA, DSA, DH, ECDH, ECDSA. Supported operations includes
+	  Keygen, Sign and Verify.
+
+	  To compile this as a module, choose M here: the module
+	  will be called caam_pkc.
+
 config CRYPTO_DEV_FSL_CAAM_AHASH_API
 	tristate "Register hash algorithm implementations with Crypto API"
 	depends on CRYPTO_DEV_FSL_CAAM && CRYPTO_DEV_FSL_CAAM_JR
@@ -119,3 +150,8 @@ config CRYPTO_DEV_FSL_CAAM_DEBUG
 	help
 	  Selecting this will enable printing of various debug
 	  information in the CAAM driver.
+
+config CRYPTO_DEV_FSL_CAAM_JR_UIO
+	tristate "Freescale Job Ring UIO support"
+	depends on CRYPTO_DEV_FSL_CAAM && CRYPTO_DEV_FSL_CAAM_JR && UIO
+	default y
diff --git a/drivers/crypto/caam/Makefile b/drivers/crypto/caam/Makefile
index 550758a..d63f005 100644
--- a/drivers/crypto/caam/Makefile
+++ b/drivers/crypto/caam/Makefile
@@ -2,14 +2,23 @@
 # Makefile for the CAAM backend and dependent components
 #
 ifeq ($(CONFIG_CRYPTO_DEV_FSL_CAAM_DEBUG), y)
-	EXTRA_CFLAGS := -DDEBUG
+	ccflags-y := -DDEBUG
 endif
 
 obj-$(CONFIG_CRYPTO_DEV_FSL_CAAM) += caam.o
 obj-$(CONFIG_CRYPTO_DEV_FSL_CAAM_JR) += caam_jr.o
 obj-$(CONFIG_CRYPTO_DEV_FSL_CAAM_CRYPTO_API) += caamalg.o
+obj-$(CONFIG_CRYPTO_DEV_FSL_CAAM_CRYPTO_API_QI) += caamalg_qi.o
 obj-$(CONFIG_CRYPTO_DEV_FSL_CAAM_AHASH_API) += caamhash.o
 obj-$(CONFIG_CRYPTO_DEV_FSL_CAAM_RNG_API) += caamrng.o
+obj-$(CONFIG_CRYPTO_DEV_FSL_CAAM_JR_UIO) += fsl_jr_uio.o
 
 caam-objs := ctrl.o
 caam_jr-objs := jr.o key_gen.o error.o
+ifneq ($(CONFIG_CRYPTO_DEV_FSL_CAAM_CRYPTO_API_QI),)
+        ccflags-y += -DCONFIG_CAAM_QI
+        caam-objs += qi.o
+endif
+
+obj-$(CONFIG_FSL_CAAM_PKC_SUPPORT) += caam_pkc.o
+caam_pkc-objs := caampkc.o pkc_desc.o
diff --git a/drivers/crypto/caam/caamalg.c b/drivers/crypto/caam/caamalg.c
index b71f2fd..0f117d0 100644
--- a/drivers/crypto/caam/caamalg.c
+++ b/drivers/crypto/caam/caamalg.c
@@ -66,17 +66,38 @@
 
 /* length of descriptors text */
 #define DESC_AEAD_BASE			(4 * CAAM_CMD_SZ)
-#define DESC_AEAD_ENC_LEN		(DESC_AEAD_BASE + 16 * CAAM_CMD_SZ)
-#define DESC_AEAD_DEC_LEN		(DESC_AEAD_BASE + 21 * CAAM_CMD_SZ)
+#define DESC_AEAD_ENC_LEN		(DESC_AEAD_BASE + 15 * CAAM_CMD_SZ)
+#define DESC_AEAD_DEC_LEN		(DESC_AEAD_BASE + 18 * CAAM_CMD_SZ)
 #define DESC_AEAD_GIVENC_LEN		(DESC_AEAD_ENC_LEN + 7 * CAAM_CMD_SZ)
 
+#define DESC_AEAD_NULL_BASE		(3 * CAAM_CMD_SZ)
+#define DESC_AEAD_NULL_ENC_LEN		(DESC_AEAD_NULL_BASE + 14 * CAAM_CMD_SZ)
+#define DESC_AEAD_NULL_DEC_LEN		(DESC_AEAD_NULL_BASE + 17 * CAAM_CMD_SZ)
+
+#define DESC_TLS_BASE			(4 * CAAM_CMD_SZ)
+#define DESC_TLS10_ENC_LEN		(DESC_TLS_BASE + 29 * CAAM_CMD_SZ)
+
+#define DESC_GCM_BASE			(3 * CAAM_CMD_SZ)
+#define DESC_GCM_ENC_LEN		(DESC_GCM_BASE + 23 * CAAM_CMD_SZ)
+#define DESC_GCM_DEC_LEN		(DESC_GCM_BASE + 19 * CAAM_CMD_SZ)
+
+#define DESC_RFC4106_BASE		(3 * CAAM_CMD_SZ)
+#define DESC_RFC4106_ENC_LEN		(DESC_RFC4106_BASE + 15 * CAAM_CMD_SZ)
+#define DESC_RFC4106_DEC_LEN		(DESC_RFC4106_BASE + 14 * CAAM_CMD_SZ)
+#define DESC_RFC4106_GIVENC_LEN		(DESC_RFC4106_BASE + 21 * CAAM_CMD_SZ)
+
+#define DESC_RFC4543_BASE		(3 * CAAM_CMD_SZ)
+#define DESC_RFC4543_ENC_LEN		(DESC_RFC4543_BASE + 25 * CAAM_CMD_SZ)
+#define DESC_RFC4543_DEC_LEN		(DESC_RFC4543_BASE + 27 * CAAM_CMD_SZ)
+#define DESC_RFC4543_GIVENC_LEN		(DESC_RFC4543_BASE + 30 * CAAM_CMD_SZ)
+
 #define DESC_ABLKCIPHER_BASE		(3 * CAAM_CMD_SZ)
 #define DESC_ABLKCIPHER_ENC_LEN		(DESC_ABLKCIPHER_BASE + \
 					 20 * CAAM_CMD_SZ)
 #define DESC_ABLKCIPHER_DEC_LEN		(DESC_ABLKCIPHER_BASE + \
 					 15 * CAAM_CMD_SZ)
 
-#define DESC_MAX_USED_BYTES		(DESC_AEAD_GIVENC_LEN + \
+#define DESC_MAX_USED_BYTES		(DESC_RFC4543_GIVENC_LEN + \
 					 CAAM_MAX_KEY_SIZE)
 #define DESC_MAX_USED_LEN		(DESC_MAX_USED_BYTES / CAAM_CMD_SZ)
 
@@ -104,27 +125,14 @@ static inline void append_dec_op1(u32 *desc, u32 type)
 }
 
 /*
- * Wait for completion of class 1 key loading before allowing
- * error propagation
- */
-static inline void append_dec_shr_done(u32 *desc)
-{
-	u32 *jump_cmd;
-
-	jump_cmd = append_jump(desc, JUMP_CLASS_CLASS1 | JUMP_TEST_ALL);
-	set_jump_tgt_here(desc, jump_cmd);
-	append_cmd(desc, SET_OK_NO_PROP_ERRORS | CMD_LOAD);
-}
-
-/*
  * For aead functions, read payload and write payload,
  * both of which are specified in req->src and req->dst
  */
 static inline void aead_append_src_dst(u32 *desc, u32 msg_type)
 {
+	append_seq_fifo_store(desc, 0, FIFOST_TYPE_MESSAGE_DATA | KEY_VLF);
 	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_BOTH |
 			     KEY_VLF | msg_type | FIFOLD_TYPE_LASTBOTH);
-	append_seq_fifo_store(desc, 0, FIFOST_TYPE_MESSAGE_DATA | KEY_VLF);
 }
 
 /*
@@ -186,9 +194,9 @@ static void append_key_aead(u32 *desc, struct caam_ctx *ctx,
 		append_key_as_imm(desc, ctx->key, ctx->split_key_pad_len,
 				  ctx->split_key_len, CLASS_2 |
 				  KEY_DEST_MDHA_SPLIT | KEY_ENC);
-		append_key_as_imm(desc, (void *)ctx->key +
-				  ctx->split_key_pad_len, ctx->enckeylen,
-				  ctx->enckeylen, CLASS_1 | KEY_DEST_CLASS_REG);
+		append_key_as_imm(desc, ctx->key + ctx->split_key_pad_len,
+				  ctx->enckeylen, ctx->enckeylen,
+				  CLASS_1 | KEY_DEST_CLASS_REG);
 	} else {
 		append_key(desc, ctx->key_dma, ctx->split_key_len, CLASS_2 |
 			   KEY_DEST_MDHA_SPLIT | KEY_ENC);
@@ -211,9 +219,200 @@ static void init_sh_desc_key_aead(u32 *desc, struct caam_ctx *ctx,
 	append_key_aead(desc, ctx, keys_fit_inline);
 
 	set_jump_tgt_here(desc, key_jump_cmd);
+}
+
+static int aead_null_set_sh_desc(struct crypto_aead *aead)
+{
+	struct aead_tfm *tfm = &aead->base.crt_aead;
+	struct caam_ctx *ctx = crypto_aead_ctx(aead);
+	struct device *jrdev = ctx->jrdev;
+	bool keys_fit_inline;
+	u32 *key_jump_cmd, *jump_cmd, *read_move_cmd, *write_move_cmd;
+	u32 *desc;
+
+	/*
+	 * Job Descriptor and Shared Descriptors
+	 * must all fit into the 64-word Descriptor h/w Buffer
+	 */
+	if (DESC_AEAD_NULL_ENC_LEN + DESC_JOB_IO_LEN +
+	    ctx->split_key_pad_len <= CAAM_DESC_BYTES_MAX)
+		keys_fit_inline = true;
+	else
+		keys_fit_inline = false;
+
+	/* aead_encrypt shared descriptor */
+	desc = ctx->sh_desc_enc;
+
+	init_sh_desc(desc, HDR_SHARE_SERIAL);
+
+	/* Skip if already shared */
+	key_jump_cmd = append_jump(desc, JUMP_JSL | JUMP_TEST_ALL |
+				   JUMP_COND_SHRD);
+	if (keys_fit_inline)
+		append_key_as_imm(desc, ctx->key, ctx->split_key_pad_len,
+				  ctx->split_key_len, CLASS_2 |
+				  KEY_DEST_MDHA_SPLIT | KEY_ENC);
+	else
+		append_key(desc, ctx->key_dma, ctx->split_key_len, CLASS_2 |
+			   KEY_DEST_MDHA_SPLIT | KEY_ENC);
+	set_jump_tgt_here(desc, key_jump_cmd);
+
+	/* cryptlen = seqoutlen - authsize */
+	append_math_sub_imm_u32(desc, REG3, SEQOUTLEN, IMM, ctx->authsize);
+
+	/*
+	 * NULL encryption; IV is zero
+	 * assoclen = (assoclen + cryptlen) - cryptlen
+	 */
+	append_math_sub(desc, VARSEQINLEN, SEQINLEN, REG3, CAAM_CMD_SZ);
+
+	/* read assoc before reading payload */
+	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_CLASS2 | FIFOLD_TYPE_MSG |
+			     KEY_VLF);
+
+	/* Prepare to read and write cryptlen bytes */
+	append_math_add(desc, VARSEQINLEN, ZERO, REG3, CAAM_CMD_SZ);
+	append_math_add(desc, VARSEQOUTLEN, ZERO, REG3, CAAM_CMD_SZ);
+
+	/*
+	 * MOVE_LEN opcode is not available in all SEC HW revisions,
+	 * thus need to do some magic, i.e. self-patch the descriptor
+	 * buffer.
+	 */
+	read_move_cmd = append_move(desc, MOVE_SRC_DESCBUF |
+				    MOVE_DEST_MATH3 |
+				    (0x6 << MOVE_LEN_SHIFT));
+	write_move_cmd = append_move(desc, MOVE_SRC_MATH3 |
+				     MOVE_DEST_DESCBUF |
+				     MOVE_WAITCOMP |
+				     (0x8 << MOVE_LEN_SHIFT));
+
+	/* Class 2 operation */
+	append_operation(desc, ctx->class2_alg_type |
+			 OP_ALG_AS_INITFINAL | OP_ALG_ENCRYPT);
+
+	/* Read and write cryptlen bytes */
+	aead_append_src_dst(desc, FIFOLD_TYPE_MSG | FIFOLD_TYPE_FLUSH1);
+
+	set_move_tgt_here(desc, read_move_cmd);
+	set_move_tgt_here(desc, write_move_cmd);
+	append_cmd(desc, CMD_LOAD | DISABLE_AUTO_INFO_FIFO);
+	append_move(desc, MOVE_SRC_INFIFO_CL | MOVE_DEST_OUTFIFO |
+		    MOVE_AUX_LS);
+
+	/* Write ICV */
+	append_seq_store(desc, ctx->authsize, LDST_CLASS_2_CCB |
+			 LDST_SRCDST_BYTE_CONTEXT);
+
+	ctx->sh_desc_enc_dma = dma_map_single(jrdev, desc,
+					      desc_bytes(desc),
+					      DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, ctx->sh_desc_enc_dma)) {
+		dev_err(jrdev, "unable to map shared descriptor\n");
+		return -ENOMEM;
+	}
+#ifdef DEBUG
+	print_hex_dump(KERN_ERR,
+		       "aead null enc shdesc@"__stringify(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, desc,
+		       desc_bytes(desc), 1);
+#endif
+
+	/*
+	 * Job Descriptor and Shared Descriptors
+	 * must all fit into the 64-word Descriptor h/w Buffer
+	 */
+	if (DESC_AEAD_NULL_DEC_LEN + DESC_JOB_IO_LEN +
+	    ctx->split_key_pad_len <= CAAM_DESC_BYTES_MAX)
+		keys_fit_inline = true;
+	else
+		keys_fit_inline = false;
+
+	desc = ctx->sh_desc_dec;
+
+	/* aead_decrypt shared descriptor */
+	init_sh_desc(desc, HDR_SHARE_SERIAL);
+
+	/* Skip if already shared */
+	key_jump_cmd = append_jump(desc, JUMP_JSL | JUMP_TEST_ALL |
+				   JUMP_COND_SHRD);
+	if (keys_fit_inline)
+		append_key_as_imm(desc, ctx->key, ctx->split_key_pad_len,
+				  ctx->split_key_len, CLASS_2 |
+				  KEY_DEST_MDHA_SPLIT | KEY_ENC);
+	else
+		append_key(desc, ctx->key_dma, ctx->split_key_len, CLASS_2 |
+			   KEY_DEST_MDHA_SPLIT | KEY_ENC);
+	set_jump_tgt_here(desc, key_jump_cmd);
+
+	/* Class 2 operation */
+	append_operation(desc, ctx->class2_alg_type |
+			 OP_ALG_AS_INITFINAL | OP_ALG_DECRYPT | OP_ALG_ICV_ON);
+
+	/* assoclen + cryptlen = seqinlen - ivsize - authsize */
+	append_math_sub_imm_u32(desc, REG3, SEQINLEN, IMM,
+				ctx->authsize + tfm->ivsize);
+	/* assoclen = (assoclen + cryptlen) - cryptlen */
+	append_math_sub(desc, REG2, SEQOUTLEN, REG0, CAAM_CMD_SZ);
+	append_math_sub(desc, VARSEQINLEN, REG3, REG2, CAAM_CMD_SZ);
+
+	/* read assoc before reading payload */
+	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_CLASS2 | FIFOLD_TYPE_MSG |
+			     KEY_VLF);
+
+	/* Prepare to read and write cryptlen bytes */
+	append_math_add(desc, VARSEQINLEN, ZERO, REG2, CAAM_CMD_SZ);
+	append_math_add(desc, VARSEQOUTLEN, ZERO, REG2, CAAM_CMD_SZ);
+
+	/*
+	 * MOVE_LEN opcode is not available in all SEC HW revisions,
+	 * thus need to do some magic, i.e. self-patch the descriptor
+	 * buffer.
+	 */
+	read_move_cmd = append_move(desc, MOVE_SRC_DESCBUF |
+				    MOVE_DEST_MATH2 |
+				    (0x6 << MOVE_LEN_SHIFT));
+	write_move_cmd = append_move(desc, MOVE_SRC_MATH2 |
+				     MOVE_DEST_DESCBUF |
+				     MOVE_WAITCOMP |
+				     (0x8 << MOVE_LEN_SHIFT));
+
+	/* Read and write cryptlen bytes */
+	aead_append_src_dst(desc, FIFOLD_TYPE_MSG | FIFOLD_TYPE_FLUSH1);
+
+	/*
+	 * Insert a NOP here, since we need at least 4 instructions between
+	 * code patching the descriptor buffer and the location being patched.
+	 */
+	jump_cmd = append_jump(desc, JUMP_TEST_ALL);
+	set_jump_tgt_here(desc, jump_cmd);
+
+	set_move_tgt_here(desc, read_move_cmd);
+	set_move_tgt_here(desc, write_move_cmd);
+	append_cmd(desc, CMD_LOAD | DISABLE_AUTO_INFO_FIFO);
+	append_move(desc, MOVE_SRC_INFIFO_CL | MOVE_DEST_OUTFIFO |
+		    MOVE_AUX_LS);
+	append_cmd(desc, CMD_LOAD | ENABLE_AUTO_INFO_FIFO);
+
+	/* Load ICV */
+	append_seq_fifo_load(desc, ctx->authsize, FIFOLD_CLASS_CLASS2 |
+			     FIFOLD_TYPE_LAST2 | FIFOLD_TYPE_ICV);
+
+	ctx->sh_desc_dec_dma = dma_map_single(jrdev, desc,
+					      desc_bytes(desc),
+					      DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, ctx->sh_desc_dec_dma)) {
+		dev_err(jrdev, "unable to map shared descriptor\n");
+		return -ENOMEM;
+	}
+#ifdef DEBUG
+	print_hex_dump(KERN_ERR,
+		       "aead null dec shdesc@"__stringify(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, desc,
+		       desc_bytes(desc), 1);
+#endif
 
-	/* Propagate errors from shared to job descriptor */
-	append_cmd(desc, SET_OK_NO_PROP_ERRORS | CMD_LOAD);
+	return 0;
 }
 
 static int aead_set_sh_desc(struct crypto_aead *aead)
@@ -221,14 +420,17 @@ static int aead_set_sh_desc(struct crypto_aead *aead)
 	struct aead_tfm *tfm = &aead->base.crt_aead;
 	struct caam_ctx *ctx = crypto_aead_ctx(aead);
 	struct device *jrdev = ctx->jrdev;
-	bool keys_fit_inline = false;
-	u32 *key_jump_cmd, *jump_cmd;
+	bool keys_fit_inline;
 	u32 geniv, moveiv;
 	u32 *desc;
 
-	if (!ctx->enckeylen || !ctx->authsize)
+	if (!ctx->authsize)
 		return 0;
 
+	/* NULL encryption / decryption */
+	if (!ctx->enckeylen)
+		return aead_null_set_sh_desc(aead);
+
 	/*
 	 * Job Descriptor and Shared Descriptors
 	 * must all fit into the 64-word Descriptor h/w Buffer
@@ -237,6 +439,9 @@ static int aead_set_sh_desc(struct crypto_aead *aead)
 	    ctx->split_key_pad_len + ctx->enckeylen <=
 	    CAAM_DESC_BYTES_MAX)
 		keys_fit_inline = true;
+	else
+		keys_fit_inline = false;
+
 
 	/* aead_encrypt shared descriptor */
 	desc = ctx->sh_desc_enc;
@@ -253,7 +458,7 @@ static int aead_set_sh_desc(struct crypto_aead *aead)
 	/* assoclen + cryptlen = seqinlen - ivsize */
 	append_math_sub_imm_u32(desc, REG2, SEQINLEN, IMM, tfm->ivsize);
 
-	/* assoclen + cryptlen = (assoclen + cryptlen) - cryptlen */
+	/* assoclen = (assoclen + cryptlen) - cryptlen */
 	append_math_sub(desc, VARSEQINLEN, REG2, REG3, CAAM_CMD_SZ);
 
 	/* read assoc before reading payload */
@@ -295,31 +500,21 @@ static int aead_set_sh_desc(struct crypto_aead *aead)
 	    ctx->split_key_pad_len + ctx->enckeylen <=
 	    CAAM_DESC_BYTES_MAX)
 		keys_fit_inline = true;
-
-	desc = ctx->sh_desc_dec;
+	else
+		keys_fit_inline = false;
 
 	/* aead_decrypt shared descriptor */
-	init_sh_desc(desc, HDR_SHARE_SERIAL);
-
-	/* Skip if already shared */
-	key_jump_cmd = append_jump(desc, JUMP_JSL | JUMP_TEST_ALL |
-				   JUMP_COND_SHRD);
-
-	append_key_aead(desc, ctx, keys_fit_inline);
+	desc = ctx->sh_desc_dec;
 
-	/* Only propagate error immediately if shared */
-	jump_cmd = append_jump(desc, JUMP_TEST_ALL);
-	set_jump_tgt_here(desc, key_jump_cmd);
-	append_cmd(desc, SET_OK_NO_PROP_ERRORS | CMD_LOAD);
-	set_jump_tgt_here(desc, jump_cmd);
+	init_sh_desc_key_aead(desc, ctx, keys_fit_inline);
 
 	/* Class 2 operation */
 	append_operation(desc, ctx->class2_alg_type |
 			 OP_ALG_AS_INITFINAL | OP_ALG_DECRYPT | OP_ALG_ICV_ON);
 
-	/* assoclen + cryptlen = seqinlen - ivsize */
+	/* assoclen + cryptlen = seqinlen - ivsize - authsize */
 	append_math_sub_imm_u32(desc, REG3, SEQINLEN, IMM,
-				ctx->authsize + tfm->ivsize)
+				ctx->authsize + tfm->ivsize);
 	/* assoclen = (assoclen + cryptlen) - cryptlen */
 	append_math_sub(desc, REG2, SEQOUTLEN, REG0, CAAM_CMD_SZ);
 	append_math_sub(desc, VARSEQINLEN, REG3, REG2, CAAM_CMD_SZ);
@@ -340,7 +535,6 @@ static int aead_set_sh_desc(struct crypto_aead *aead)
 	/* Load ICV */
 	append_seq_fifo_load(desc, ctx->authsize, FIFOLD_CLASS_CLASS2 |
 			     FIFOLD_TYPE_LAST2 | FIFOLD_TYPE_ICV);
-	append_dec_shr_done(desc);
 
 	ctx->sh_desc_dec_dma = dma_map_single(jrdev, desc,
 					      desc_bytes(desc),
@@ -363,6 +557,8 @@ static int aead_set_sh_desc(struct crypto_aead *aead)
 	    ctx->split_key_pad_len + ctx->enckeylen <=
 	    CAAM_DESC_BYTES_MAX)
 		keys_fit_inline = true;
+	else
+		keys_fit_inline = false;
 
 	/* aead_givencrypt shared descriptor */
 	desc = ctx->sh_desc_givenc;
@@ -441,88 +637,1581 @@ static int aead_set_sh_desc(struct crypto_aead *aead)
 	return 0;
 }
 
-static int aead_setauthsize(struct crypto_aead *authenc,
-				    unsigned int authsize)
-{
-	struct caam_ctx *ctx = crypto_aead_ctx(authenc);
+static int aead_setauthsize(struct crypto_aead *authenc,
+				    unsigned int authsize)
+{
+	struct caam_ctx *ctx = crypto_aead_ctx(authenc);
+
+	ctx->authsize = authsize;
+	aead_set_sh_desc(authenc);
+
+	return 0;
+}
+
+static int tls_set_sh_desc(struct crypto_aead *aead)
+{
+	struct aead_tfm *tfm = &aead->base.crt_aead;
+	struct caam_ctx *ctx = crypto_aead_ctx(aead);
+	struct device *jrdev = ctx->jrdev;
+	bool keys_fit_inline;
+	u32 *key_jump_cmd, *zero_payload_jump_cmd, *skip_zero_jump_cmd;
+	u32 genpad, idx_ld_datasz, idx_ld_pad, jumpback, stidx;
+	u32 *desc;
+	unsigned int blocksize = crypto_aead_blocksize(aead);
+	/* Associated data length is always = 13 for TLS */
+	unsigned int assoclen = 13;
+	/*
+	 * Pointer Size bool determines the size of address pointers.
+	 * false - Pointers fit in one 32-bit word.
+	 * true - Pointers fit in two 32-bit words.
+	 */
+	static const bool ps = (CAAM_PTR_SZ != CAAM_CMD_SZ);
+
+	if (!ctx->enckeylen || !ctx->authsize)
+		return 0;
+
+	/*
+	 * TLS 1.0 encrypt shared descriptor
+	 * Job Descriptor and Shared Descriptor
+	 * must fit into the 64-word Descriptor h/w Buffer
+	 */
+
+	/*
+	 * Compute the index (in bytes) for the LOAD with destination of
+	 * Class 1 Data Size Register and for the LOAD that generates padding
+	 */
+	if (DESC_TLS10_ENC_LEN + DESC_JOB_IO_LEN +
+	    ctx->split_key_pad_len + ctx->enckeylen <=
+	    CAAM_DESC_BYTES_MAX) {
+		keys_fit_inline = true;
+
+		idx_ld_datasz = DESC_TLS10_ENC_LEN + ctx->split_key_pad_len +
+				ctx->enckeylen - 4 * CAAM_CMD_SZ;
+		idx_ld_pad = DESC_TLS10_ENC_LEN + ctx->split_key_pad_len +
+			     ctx->enckeylen - 2 * CAAM_CMD_SZ;
+	} else {
+		keys_fit_inline = false;
+
+		idx_ld_datasz = DESC_TLS10_ENC_LEN + 2 * CAAM_PTR_SZ -
+				4 * CAAM_CMD_SZ;
+		idx_ld_pad = DESC_TLS10_ENC_LEN + 2 * CAAM_PTR_SZ -
+			     2 * CAAM_CMD_SZ;
+	}
+
+	desc = ctx->sh_desc_enc;
+
+	stidx = 1 << HDR_START_IDX_SHIFT;
+	init_sh_desc(desc, HDR_SHARE_SERIAL | stidx);
+
+	/* skip key loading if they are loaded due to sharing */
+	key_jump_cmd = append_jump(desc, JUMP_JSL | JUMP_TEST_ALL |
+				   JUMP_COND_SHRD);
+	append_key_aead(desc, ctx, keys_fit_inline);
+	set_jump_tgt_here(desc, key_jump_cmd);
+
+	/* class 2 operation */
+	append_operation(desc, ctx->class2_alg_type |
+			 OP_ALG_AS_INITFINAL | OP_ALG_ENCRYPT);
+	/* class 1 operation */
+	append_operation(desc, ctx->class1_alg_type |
+			 OP_ALG_AS_INITFINAL | OP_ALG_ENCRYPT);
+
+	/* payloadlen = input data length - (assoclen + ivlen) */
+	append_math_sub_imm_u32(desc, REG0, SEQINLEN, IMM, assoclen +
+				tfm->ivsize);
+
+	/* math1 = payloadlen + icvlen */
+	append_math_add_imm_u32(desc, REG1, REG0, IMM, ctx->authsize);
+
+	/* padlen = block_size - math1 % block_size */
+	append_math_and_imm_u32(desc, REG3, REG1, IMM, blocksize - 1);
+	append_math_sub_imm_u32(desc, REG2, IMM, REG3, blocksize);
+
+	/* cryptlen = payloadlen + icvlen + padlen */
+	append_math_add(desc, VARSEQOUTLEN, REG1, REG2, 4);
+
+	/*
+	 * update immediate data with the padding length value
+	 * for the LOAD in the class 1 data size register.
+	 */
+	append_move(desc, MOVE_SRC_DESCBUF | MOVE_DEST_MATH2 |
+			(idx_ld_datasz << MOVE_OFFSET_SHIFT) | 7);
+	append_move(desc, MOVE_WAITCOMP | MOVE_SRC_MATH2 | MOVE_DEST_DESCBUF |
+			(idx_ld_datasz << MOVE_OFFSET_SHIFT) | 8);
+
+	/* overwrite PL field for the padding iNFO FIFO entry  */
+	append_move(desc, MOVE_SRC_DESCBUF | MOVE_DEST_MATH2 |
+			(idx_ld_pad << MOVE_OFFSET_SHIFT) | 7);
+	append_move(desc, MOVE_WAITCOMP | MOVE_SRC_MATH2 | MOVE_DEST_DESCBUF |
+			(idx_ld_pad << MOVE_OFFSET_SHIFT) | 8);
+
+	/* store encrypted payload, icv and padding */
+	append_seq_fifo_store(desc, 0, FIFOST_TYPE_MESSAGE_DATA | LDST_VLF);
+
+	/* if payload length is zero, jump to zero-payload commands */
+	append_math_add(desc, VARSEQINLEN, ZERO, REG0, 4);
+	zero_payload_jump_cmd = append_jump(desc, JUMP_TEST_ALL |
+					    JUMP_COND_MATH_Z);
+
+	/* read assoc for authentication */
+	append_seq_fifo_load(desc, assoclen, FIFOLD_CLASS_CLASS2 |
+			     FIFOLD_TYPE_MSG);
+	/* load iv in context1 */
+	append_cmd(desc, CMD_SEQ_LOAD | LDST_SRCDST_WORD_CLASS_CTX |
+		   LDST_CLASS_1_CCB | tfm->ivsize);
+	/* insnoop payload */
+	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_BOTH | FIFOLD_TYPE_MSG |
+			     FIFOLD_TYPE_LAST2 | FIFOLDST_VLF);
+	/* jump the zero-payload commands */
+	append_jump(desc, JUMP_TEST_ALL | 3);
+
+	/* zero-payload commands */
+	set_jump_tgt_here(desc, zero_payload_jump_cmd);
+	/* assoc data is the only data for authentication */
+	append_seq_fifo_load(desc, assoclen, FIFOLD_CLASS_CLASS2 |
+			     FIFOLD_TYPE_MSG | FIFOLD_TYPE_LAST2);
+	/* load iv in context1 */
+	append_cmd(desc, CMD_SEQ_LOAD | LDST_SRCDST_WORD_CLASS_CTX |
+		   LDST_CLASS_1_CCB | tfm->ivsize);
+
+	/* send icv to encryption */
+	append_move(desc, MOVE_SRC_CLASS2CTX | MOVE_DEST_CLASS1INFIFO |
+		    ctx->authsize);
+
+	/* update class 1 data size register with padding length */
+	append_load_imm_u32(desc, 0, LDST_CLASS_1_CCB |
+			    LDST_SRCDST_WORD_DATASZ_REG | LDST_IMM);
+
+	/* generate padding and send it to encryption */
+	genpad = NFIFOENTRY_DEST_CLASS1 | NFIFOENTRY_LC1 | NFIFOENTRY_FC1 |
+	      NFIFOENTRY_STYPE_PAD | NFIFOENTRY_DTYPE_MSG | NFIFOENTRY_PTYPE_N;
+	append_load_imm_u32(desc, genpad, LDST_CLASS_IND_CCB |
+			    LDST_SRCDST_WORD_INFO_FIFO | LDST_IMM);
+
+	ctx->sh_desc_enc_dma = dma_map_single(jrdev, desc,
+					      desc_bytes(desc),
+					      DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, ctx->sh_desc_enc_dma)) {
+		dev_err(jrdev, "unable to map shared descriptor\n");
+		return -ENOMEM;
+	}
+#ifdef DEBUG
+	print_hex_dump(KERN_ERR, "tls enc shdesc@"__stringify(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, desc,
+		       desc_bytes(desc), 1);
+#endif
+
+	/*
+	 * TLS 1.0 decrypt shared descriptor
+	 * Keys do not fit inline, regardless of algorithms used
+	 */
+	desc = ctx->sh_desc_dec;
+
+	stidx = 1 << HDR_START_IDX_SHIFT;
+	init_sh_desc(desc, HDR_SHARE_SERIAL | stidx);
+
+	/* skip key loading if they are loaded due to sharing */
+	key_jump_cmd = append_jump(desc, JUMP_JSL | JUMP_TEST_ALL |
+				   JUMP_COND_SHRD);
+	append_key(desc, ctx->key_dma, ctx->split_key_len, CLASS_2 |
+		   KEY_DEST_MDHA_SPLIT | KEY_ENC);
+	append_key(desc, ctx->key_dma + ctx->split_key_pad_len,
+		   ctx->enckeylen, CLASS_1 | KEY_DEST_CLASS_REG);
+	set_jump_tgt_here(desc, key_jump_cmd);
+
+	/* class 2 operation */
+	append_operation(desc, ctx->class2_alg_type |
+			 OP_ALG_AS_INITFINAL | OP_ALG_DECRYPT | OP_ALG_ICV_ON);
+	/* class 1 operation */
+	append_operation(desc, ctx->class1_alg_type |
+			 OP_ALG_AS_INITFINAL | OP_ALG_DECRYPT);
+
+	/* VSIL = input data length - 2 * block_size */
+	append_math_sub_imm_u32(desc, VARSEQINLEN, SEQINLEN, IMM, 2 *
+				blocksize);
+
+	/*
+	 * payloadlen + icvlen + padlen = input data length - (assoclen +
+	 * ivsize)
+	 */
+	append_math_sub_imm_u32(desc, REG3, SEQINLEN, IMM, assoclen +
+				tfm->ivsize);
+
+	/* skip data to the last but one cipher block */
+	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_SKIP | LDST_VLF);
+
+	/* load iv for the last cipher block */
+	append_cmd(desc, CMD_SEQ_LOAD | LDST_SRCDST_WORD_CLASS_CTX |
+		   LDST_CLASS_1_CCB | tfm->ivsize);
+
+	/* read last cipher block */
+	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_CLASS1 | FIFOLD_TYPE_MSG |
+			FIFOLD_TYPE_LAST1 | blocksize);
+
+	/* move decrypted block into math0 and math1 */
+	append_move(desc, MOVE_WAITCOMP | MOVE_SRC_OUTFIFO | MOVE_DEST_MATH0 |
+		    blocksize);
+
+	/* reset AES CHA */
+	append_load_imm_u32(desc, CCTRL_RESET_CHA_AESA, LDST_CLASS_IND_CCB |
+			    LDST_SRCDST_WORD_CHACTRL | LDST_IMM);
+
+	/* rewind input sequence */
+	append_seq_in_ptr_intlen(desc, 0, 65535, SQIN_RTO);
+
+	/* key1 is in decryption form */
+	append_operation(desc, ctx->class1_alg_type | OP_ALG_AAI_DK |
+			 OP_ALG_AS_INITFINAL | OP_ALG_DECRYPT);
+
+	/* read sequence number */
+	append_seq_fifo_load(desc, 8, FIFOLD_CLASS_CLASS2 | FIFOLD_TYPE_MSG);
+	/* load Type, Version and Len fields in math0 */
+	append_cmd(desc, CMD_SEQ_LOAD | LDST_CLASS_DECO |
+		   LDST_SRCDST_WORD_DECO_MATH0 | (3 << LDST_OFFSET_SHIFT) | 5);
+
+	/* load iv in context1 */
+	append_cmd(desc, CMD_SEQ_LOAD | LDST_CLASS_1_CCB |
+		   LDST_SRCDST_WORD_CLASS_CTX | tfm->ivsize);
+
+	/* compute (padlen - 1) */
+	append_math_and_imm_u64(desc, REG1, REG1, IMM, 255);
+
+	/* math2 = icvlen + (padlen - 1) + 1 */
+	append_math_add_imm_u32(desc, REG2, REG1, IMM, ctx->authsize + 1);
+
+	append_jump(desc, JUMP_TEST_ALL | JUMP_COND_CALM | 1);
+
+	/* VSOL = payloadlen + icvlen + padlen */
+	append_math_add(desc, VARSEQOUTLEN, ZERO, REG3, 4);
+
+#ifdef __LITTLE_ENDIAN
+	append_moveb(desc, MOVE_WAITCOMP |
+		     MOVE_SRC_MATH0 | MOVE_DEST_MATH0 | 8);
+#endif
+	/* update Len field */
+	append_math_sub(desc, REG0, REG0, REG2, 8);
+
+	/* store decrypted payload, icv and padding */
+	append_seq_fifo_store(desc, 0, FIFOST_TYPE_MESSAGE_DATA | LDST_VLF);
+
+	/* VSIL = (payloadlen + icvlen + padlen) - (icvlen + padlen)*/
+	append_math_sub(desc, VARSEQINLEN, REG3, REG2, 4);
+
+	zero_payload_jump_cmd = append_jump(desc, JUMP_TEST_ALL |
+					    JUMP_COND_MATH_Z);
+
+	/* send Type, Version and Len(pre ICV) fields to authentication */
+	append_move(desc, MOVE_WAITCOMP |
+		    MOVE_SRC_MATH0 | MOVE_DEST_CLASS2INFIFO |
+		    (3 << MOVE_OFFSET_SHIFT) | 5);
+
+	/* outsnooping payload */
+	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_BOTH |
+			     FIFOLD_TYPE_MSG1OUT2 | FIFOLD_TYPE_LAST2 |
+			     FIFOLDST_VLF);
+	skip_zero_jump_cmd = append_jump(desc, JUMP_TEST_ALL | 2);
+
+	set_jump_tgt_here(desc, zero_payload_jump_cmd);
+	/* send Type, Version and Len(pre ICV) fields to authentication */
+	append_move(desc, MOVE_WAITCOMP | MOVE_AUX_LS |
+		    MOVE_SRC_MATH0 | MOVE_DEST_CLASS2INFIFO |
+		    (3 << MOVE_OFFSET_SHIFT) | 5);
+
+	set_jump_tgt_here(desc, skip_zero_jump_cmd);
+	append_math_add(desc, VARSEQINLEN, ZERO, REG2, 4);
+
+	/* load icvlen and padlen */
+	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_CLASS1 | FIFOLD_TYPE_MSG |
+			     FIFOLD_TYPE_LAST1 | FIFOLDST_VLF);
+
+	/* VSIL = (payloadlen + icvlen + padlen) - icvlen + padlen */
+	append_math_sub(desc, VARSEQINLEN, REG3, REG2, 4);
+
+	/*
+	 * Start a new input sequence using the SEQ OUT PTR command options,
+	 * pointer and length used when the current output sequence was defined.
+	 */
+	if (ps) {
+		/*
+		 * Move the lower 32 bits of Shared Descriptor address, the
+		 * SEQ OUT PTR command, Output Pointer (2 words) and
+		 * Output Length into math registers.
+		 */
+#ifdef __LITTLE_ENDIAN
+		append_move(desc, MOVE_WAITCOMP | MOVE_SRC_DESCBUF |
+			    MOVE_DEST_MATH0 | (55 * 4 << MOVE_OFFSET_SHIFT) |
+			    20);
+#else
+		append_move(desc, MOVE_WAITCOMP | MOVE_SRC_DESCBUF |
+			    MOVE_DEST_MATH0 | (54 * 4 << MOVE_OFFSET_SHIFT) |
+			    20);
+#endif
+		/* Transform SEQ OUT PTR command in SEQ IN PTR command */
+		append_math_and_imm_u32(desc, REG0, REG0, IMM,
+					~(CMD_SEQ_IN_PTR ^ CMD_SEQ_OUT_PTR));
+		/* Append a JUMP command after the copied fields */
+		jumpback = CMD_JUMP | (char)-9;
+		append_load_imm_u32(desc, jumpback, LDST_CLASS_DECO | LDST_IMM |
+				    LDST_SRCDST_WORD_DECO_MATH2 |
+				    (4 << LDST_OFFSET_SHIFT));
+		append_jump(desc, JUMP_TEST_ALL | JUMP_COND_CALM | 1);
+		/* Move the updated fields back to the Job Descriptor */
+#ifdef __LITTLE_ENDIAN
+		append_move(desc, MOVE_WAITCOMP | MOVE_SRC_MATH0 |
+			    MOVE_DEST_DESCBUF | (55 * 4 << MOVE_OFFSET_SHIFT) |
+			    24);
+#else
+		append_move(desc, MOVE_WAITCOMP | MOVE_SRC_MATH0 |
+			    MOVE_DEST_DESCBUF | (54 * 4 << MOVE_OFFSET_SHIFT) |
+			    24);
+#endif
+		/*
+		 * Read the new SEQ IN PTR command, Input Pointer, Input Length
+		 * and then jump back to the next command from the
+		 * Shared Descriptor.
+		 */
+		append_jump(desc, JUMP_TEST_ALL | JUMP_COND_CALM | 6);
+	} else {
+		/*
+		 * Move the SEQ OUT PTR command, Output Pointer (1 word) and
+		 * Output Length into math registers.
+		 */
+#ifdef __LITTLE_ENDIAN
+		append_move(desc, MOVE_WAITCOMP | MOVE_SRC_DESCBUF |
+			    MOVE_DEST_MATH0 | (54 * 4 << MOVE_OFFSET_SHIFT) |
+			    12);
+#else
+		append_move(desc, MOVE_WAITCOMP | MOVE_SRC_DESCBUF |
+			    MOVE_DEST_MATH0 | (53 * 4 << MOVE_OFFSET_SHIFT) |
+			    12);
+#endif
+		/* Transform SEQ OUT PTR command in SEQ IN PTR command */
+		append_math_and_imm_u64(desc, REG0, REG0, IMM,
+			~(((u64)(CMD_SEQ_IN_PTR ^ CMD_SEQ_OUT_PTR)) << 32));
+		/* Append a JUMP command after the copied fields */
+		jumpback = CMD_JUMP | (char)-7;
+		append_load_imm_u32(desc, jumpback, LDST_CLASS_DECO | LDST_IMM |
+				    LDST_SRCDST_WORD_DECO_MATH1 |
+				    (4 << LDST_OFFSET_SHIFT));
+		append_jump(desc, JUMP_TEST_ALL | JUMP_COND_CALM | 1);
+		/* Move the updated fields back to the Job Descriptor */
+#ifdef __LITTLE_ENDIAN
+		append_move(desc, MOVE_WAITCOMP | MOVE_SRC_MATH0 |
+			    MOVE_DEST_DESCBUF | (54 * 4 << MOVE_OFFSET_SHIFT) |
+			    16);
+#else
+		append_move(desc, MOVE_WAITCOMP | MOVE_SRC_MATH0 |
+			    MOVE_DEST_DESCBUF | (53 * 4 << MOVE_OFFSET_SHIFT) |
+			    16);
+#endif
+		/*
+		 * Read the new SEQ IN PTR command, Input Pointer, Input Length
+		 * and then jump back to the next command from the
+		 * Shared Descriptor.
+		 */
+		append_jump(desc, JUMP_TEST_ALL | JUMP_COND_CALM | 5);
+	}
+
+	/* skip payload */
+	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_SKIP | FIFOLDST_VLF);
+	/* check icv */
+	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_CLASS2 | FIFOLD_TYPE_ICV |
+			     FIFOLD_TYPE_LAST2 | ctx->authsize);
+
+	ctx->sh_desc_dec_dma = dma_map_single(jrdev, desc,
+					      desc_bytes(desc),
+					      DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, ctx->sh_desc_dec_dma)) {
+		dev_err(jrdev, "unable to map shared descriptor\n");
+		return -ENOMEM;
+	}
+#ifdef DEBUG
+	print_hex_dump(KERN_ERR, "tls dec shdesc@"__stringify(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, desc,
+		       desc_bytes(desc), 1);
+#endif
+
+	return 0;
+}
+
+static int tls_setauthsize(struct crypto_aead *tls, unsigned int authsize)
+{
+	struct caam_ctx *ctx = crypto_aead_ctx(tls);
+
+	ctx->authsize = authsize;
+
+	return 0;
+}
+
+static int gcm_set_sh_desc(struct crypto_aead *aead)
+{
+	struct aead_tfm *tfm = &aead->base.crt_aead;
+	struct caam_ctx *ctx = crypto_aead_ctx(aead);
+	struct device *jrdev = ctx->jrdev;
+	bool keys_fit_inline = false;
+	u32 *key_jump_cmd, *zero_payload_jump_cmd,
+	    *zero_assoc_jump_cmd1, *zero_assoc_jump_cmd2;
+	u32 *desc;
+
+	if (!ctx->enckeylen || !ctx->authsize)
+		return 0;
+
+	/*
+	 * AES GCM encrypt shared descriptor
+	 * Job Descriptor and Shared Descriptor
+	 * must fit into the 64-word Descriptor h/w Buffer
+	 */
+	if (DESC_GCM_ENC_LEN + DESC_JOB_IO_LEN +
+	    ctx->enckeylen <= CAAM_DESC_BYTES_MAX)
+		keys_fit_inline = true;
+
+	desc = ctx->sh_desc_enc;
+
+	init_sh_desc(desc, HDR_SHARE_SERIAL);
+
+	/* skip key loading if they are loaded due to sharing */
+	key_jump_cmd = append_jump(desc, JUMP_JSL | JUMP_TEST_ALL |
+				   JUMP_COND_SHRD | JUMP_COND_SELF);
+	if (keys_fit_inline)
+		append_key_as_imm(desc, (void *)ctx->key, ctx->enckeylen,
+				  ctx->enckeylen, CLASS_1 | KEY_DEST_CLASS_REG);
+	else
+		append_key(desc, ctx->key_dma, ctx->enckeylen,
+			   CLASS_1 | KEY_DEST_CLASS_REG);
+	set_jump_tgt_here(desc, key_jump_cmd);
+
+	/* class 1 operation */
+	append_operation(desc, ctx->class1_alg_type |
+			 OP_ALG_AS_INITFINAL | OP_ALG_ENCRYPT);
+
+	/* cryptlen = seqoutlen - authsize */
+	append_math_sub_imm_u32(desc, REG3, SEQOUTLEN, IMM, ctx->authsize);
+
+	/* assoclen + cryptlen = seqinlen - ivsize */
+	append_math_sub_imm_u32(desc, REG2, SEQINLEN, IMM, tfm->ivsize);
+
+	/* assoclen = (assoclen + cryptlen) - cryptlen */
+	append_math_sub(desc, REG1, REG2, REG3, CAAM_CMD_SZ);
+
+	/* if cryptlen is ZERO jump to zero-payload commands */
+	append_math_add(desc, VARSEQOUTLEN, ZERO, REG3, CAAM_CMD_SZ);
+	zero_payload_jump_cmd = append_jump(desc, JUMP_TEST_ALL |
+					    JUMP_COND_MATH_Z);
+	/* read IV */
+	append_seq_fifo_load(desc, tfm->ivsize, FIFOLD_CLASS_CLASS1 |
+			     FIFOLD_TYPE_IV | FIFOLD_TYPE_FLUSH1);
+
+	/* if assoclen is ZERO, skip reading the assoc data */
+	append_math_add(desc, VARSEQINLEN, ZERO, REG1, CAAM_CMD_SZ);
+	zero_assoc_jump_cmd1 = append_jump(desc, JUMP_TEST_ALL |
+					   JUMP_COND_MATH_Z);
+
+	/* read assoc data */
+	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_CLASS1 | FIFOLDST_VLF |
+			     FIFOLD_TYPE_AAD | FIFOLD_TYPE_FLUSH1);
+	set_jump_tgt_here(desc, zero_assoc_jump_cmd1);
+
+	append_math_add(desc, VARSEQINLEN, ZERO, REG3, CAAM_CMD_SZ);
+
+	/* write encrypted data */
+	append_seq_fifo_store(desc, 0, FIFOST_TYPE_MESSAGE_DATA | FIFOLDST_VLF);
+
+	/* read payload data */
+	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_CLASS1 | FIFOLDST_VLF |
+			     FIFOLD_TYPE_MSG | FIFOLD_TYPE_LAST1);
+
+	/* jump the zero-payload commands */
+	append_jump(desc, JUMP_TEST_ALL | 7);
+
+	/* zero-payload commands */
+	set_jump_tgt_here(desc, zero_payload_jump_cmd);
+
+	/* if assoclen is ZERO, jump to IV reading - is the only input data */
+	append_math_add(desc, VARSEQINLEN, ZERO, REG1, CAAM_CMD_SZ);
+	zero_assoc_jump_cmd2 = append_jump(desc, JUMP_TEST_ALL |
+					   JUMP_COND_MATH_Z);
+	/* read IV */
+	append_seq_fifo_load(desc, tfm->ivsize, FIFOLD_CLASS_CLASS1 |
+			     FIFOLD_TYPE_IV | FIFOLD_TYPE_FLUSH1);
+
+	/* read assoc data */
+	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_CLASS1 | FIFOLDST_VLF |
+			     FIFOLD_TYPE_AAD | FIFOLD_TYPE_LAST1);
+
+	/* jump to ICV writing */
+	append_jump(desc, JUMP_TEST_ALL | 2);
+
+	/* read IV - is the only input data */
+	set_jump_tgt_here(desc, zero_assoc_jump_cmd2);
+	append_seq_fifo_load(desc, tfm->ivsize, FIFOLD_CLASS_CLASS1 |
+			     FIFOLD_TYPE_IV | FIFOLD_TYPE_FLUSH1 |
+			     FIFOLD_TYPE_LAST1);
+
+	/* write ICV */
+	append_seq_store(desc, ctx->authsize, LDST_CLASS_1_CCB |
+			 LDST_SRCDST_BYTE_CONTEXT);
+
+	ctx->sh_desc_enc_dma = dma_map_single(jrdev, desc,
+					      desc_bytes(desc),
+					      DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, ctx->sh_desc_enc_dma)) {
+		dev_err(jrdev, "unable to map shared descriptor\n");
+		return -ENOMEM;
+	}
+#ifdef DEBUG
+	print_hex_dump(KERN_ERR, "gcm enc shdesc@"__stringify(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, desc,
+		       desc_bytes(desc), 1);
+#endif
+
+	/*
+	 * Job Descriptor and Shared Descriptors
+	 * must all fit into the 64-word Descriptor h/w Buffer
+	 */
+	keys_fit_inline = false;
+	if (DESC_GCM_DEC_LEN + DESC_JOB_IO_LEN +
+	    ctx->enckeylen <= CAAM_DESC_BYTES_MAX)
+		keys_fit_inline = true;
+
+	desc = ctx->sh_desc_dec;
+
+	init_sh_desc(desc, HDR_SHARE_SERIAL);
+
+	/* skip key loading if they are loaded due to sharing */
+	key_jump_cmd = append_jump(desc, JUMP_JSL |
+				   JUMP_TEST_ALL | JUMP_COND_SHRD |
+				   JUMP_COND_SELF);
+	if (keys_fit_inline)
+		append_key_as_imm(desc, (void *)ctx->key, ctx->enckeylen,
+				  ctx->enckeylen, CLASS_1 | KEY_DEST_CLASS_REG);
+	else
+		append_key(desc, ctx->key_dma, ctx->enckeylen,
+			   CLASS_1 | KEY_DEST_CLASS_REG);
+	set_jump_tgt_here(desc, key_jump_cmd);
+
+	/* class 1 operation */
+	append_operation(desc, ctx->class1_alg_type |
+			 OP_ALG_AS_INITFINAL | OP_ALG_DECRYPT | OP_ALG_ICV_ON);
+
+	/* assoclen + cryptlen = seqinlen - ivsize - icvsize */
+	append_math_sub_imm_u32(desc, REG3, SEQINLEN, IMM,
+				ctx->authsize + tfm->ivsize);
+
+	/* assoclen = (assoclen + cryptlen) - cryptlen */
+	append_math_sub(desc, REG2, SEQOUTLEN, REG0, CAAM_CMD_SZ);
+	append_math_sub(desc, REG1, REG3, REG2, CAAM_CMD_SZ);
+
+	/* read IV */
+	append_seq_fifo_load(desc, tfm->ivsize, FIFOLD_CLASS_CLASS1 |
+			     FIFOLD_TYPE_IV | FIFOLD_TYPE_FLUSH1);
+
+	/* jump to zero-payload command if cryptlen is zero */
+	append_math_add(desc, VARSEQOUTLEN, ZERO, REG2, CAAM_CMD_SZ);
+	zero_payload_jump_cmd = append_jump(desc, JUMP_TEST_ALL |
+					    JUMP_COND_MATH_Z);
+
+	append_math_add(desc, VARSEQINLEN, ZERO, REG1, CAAM_CMD_SZ);
+	/* if asoclen is ZERO, skip reading assoc data */
+	zero_assoc_jump_cmd1 = append_jump(desc, JUMP_TEST_ALL |
+					   JUMP_COND_MATH_Z);
+	/* read assoc data */
+	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_CLASS1 | FIFOLDST_VLF |
+			     FIFOLD_TYPE_AAD | FIFOLD_TYPE_FLUSH1);
+	set_jump_tgt_here(desc, zero_assoc_jump_cmd1);
+
+	append_math_add(desc, VARSEQINLEN, ZERO, REG2, CAAM_CMD_SZ);
+
+	/* store encrypted data */
+	append_seq_fifo_store(desc, 0, FIFOST_TYPE_MESSAGE_DATA | FIFOLDST_VLF);
+
+	/* read payload data */
+	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_CLASS1 | FIFOLDST_VLF |
+			     FIFOLD_TYPE_MSG | FIFOLD_TYPE_FLUSH1);
+
+	/* jump the zero-payload commands */
+	append_jump(desc, JUMP_TEST_ALL | 4);
+
+	/* zero-payload command */
+	set_jump_tgt_here(desc, zero_payload_jump_cmd);
+
+	/* if assoclen is ZERO, jump to ICV reading */
+	append_math_add(desc, VARSEQINLEN, ZERO, REG1, CAAM_CMD_SZ);
+	zero_assoc_jump_cmd2 = append_jump(desc, JUMP_TEST_ALL |
+					   JUMP_COND_MATH_Z);
+	/* read assoc data */
+	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_CLASS1 | FIFOLDST_VLF |
+			     FIFOLD_TYPE_AAD | FIFOLD_TYPE_FLUSH1);
+	set_jump_tgt_here(desc, zero_assoc_jump_cmd2);
+
+	/* read ICV */
+	append_seq_fifo_load(desc, ctx->authsize, FIFOLD_CLASS_CLASS1 |
+			     FIFOLD_TYPE_ICV | FIFOLD_TYPE_LAST1);
+
+	ctx->sh_desc_dec_dma = dma_map_single(jrdev, desc,
+					      desc_bytes(desc),
+					      DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, ctx->sh_desc_dec_dma)) {
+		dev_err(jrdev, "unable to map shared descriptor\n");
+		return -ENOMEM;
+	}
+#ifdef DEBUG
+	print_hex_dump(KERN_ERR, "gcm dec shdesc@"__stringify(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, desc,
+		       desc_bytes(desc), 1);
+#endif
+
+	return 0;
+}
+
+static int gcm_setauthsize(struct crypto_aead *authenc, unsigned int authsize)
+{
+	struct caam_ctx *ctx = crypto_aead_ctx(authenc);
+
+	ctx->authsize = authsize;
+	gcm_set_sh_desc(authenc);
+
+	return 0;
+}
+
+static int rfc4106_set_sh_desc(struct crypto_aead *aead)
+{
+	struct aead_tfm *tfm = &aead->base.crt_aead;
+	struct caam_ctx *ctx = crypto_aead_ctx(aead);
+	struct device *jrdev = ctx->jrdev;
+	bool keys_fit_inline = false;
+	u32 *key_jump_cmd, *move_cmd, *write_iv_cmd;
+	u32 *desc;
+	u32 geniv;
+
+	if (!ctx->enckeylen || !ctx->authsize)
+		return 0;
+
+	/*
+	 * RFC4106 encrypt shared descriptor
+	 * Job Descriptor and Shared Descriptor
+	 * must fit into the 64-word Descriptor h/w Buffer
+	 */
+	if (DESC_RFC4106_ENC_LEN + DESC_JOB_IO_LEN +
+	    ctx->enckeylen <= CAAM_DESC_BYTES_MAX)
+		keys_fit_inline = true;
+
+	desc = ctx->sh_desc_enc;
+
+	init_sh_desc(desc, HDR_SHARE_SERIAL);
+
+	/* Skip key loading if it is loaded due to sharing */
+	key_jump_cmd = append_jump(desc, JUMP_JSL | JUMP_TEST_ALL |
+				   JUMP_COND_SHRD);
+	if (keys_fit_inline)
+		append_key_as_imm(desc, (void *)ctx->key, ctx->enckeylen,
+				  ctx->enckeylen, CLASS_1 | KEY_DEST_CLASS_REG);
+	else
+		append_key(desc, ctx->key_dma, ctx->enckeylen,
+			   CLASS_1 | KEY_DEST_CLASS_REG);
+	set_jump_tgt_here(desc, key_jump_cmd);
+
+	/* Class 1 operation */
+	append_operation(desc, ctx->class1_alg_type |
+			 OP_ALG_AS_INITFINAL | OP_ALG_ENCRYPT);
+
+	/* cryptlen = seqoutlen - authsize */
+	append_math_sub_imm_u32(desc, REG3, SEQOUTLEN, IMM, ctx->authsize);
+	append_math_add(desc, VARSEQOUTLEN, ZERO, REG3, CAAM_CMD_SZ);
+
+	/* assoclen + cryptlen = seqinlen - ivsize */
+	append_math_sub_imm_u32(desc, REG2, SEQINLEN, IMM, tfm->ivsize);
+
+	/* assoclen = (assoclen + cryptlen) - cryptlen */
+	append_math_sub(desc, VARSEQINLEN, REG2, REG3, CAAM_CMD_SZ);
+
+	/* Read Salt */
+	append_fifo_load_as_imm(desc, (void *)(ctx->key + ctx->enckeylen),
+				4, FIFOLD_CLASS_CLASS1 | FIFOLD_TYPE_IV);
+	/* Read AES-GCM-ESP IV */
+	append_seq_fifo_load(desc, tfm->ivsize, FIFOLD_CLASS_CLASS1 |
+			     FIFOLD_TYPE_IV | FIFOLD_TYPE_FLUSH1);
+
+	/* Read assoc data */
+	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_CLASS1 | FIFOLDST_VLF |
+			     FIFOLD_TYPE_AAD | FIFOLD_TYPE_FLUSH1);
+
+	/* Will read cryptlen bytes */
+	append_math_add(desc, VARSEQINLEN, ZERO, REG3, CAAM_CMD_SZ);
+
+	/* Write encrypted data */
+	append_seq_fifo_store(desc, 0, FIFOST_TYPE_MESSAGE_DATA | FIFOLDST_VLF);
+
+	/* Read payload data */
+	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_CLASS1 | FIFOLDST_VLF |
+			     FIFOLD_TYPE_MSG | FIFOLD_TYPE_LAST1);
+
+	/* Write ICV */
+	append_seq_store(desc, ctx->authsize, LDST_CLASS_1_CCB |
+			 LDST_SRCDST_BYTE_CONTEXT);
+
+	ctx->sh_desc_enc_dma = dma_map_single(jrdev, desc,
+					      desc_bytes(desc),
+					      DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, ctx->sh_desc_enc_dma)) {
+		dev_err(jrdev, "unable to map shared descriptor\n");
+		return -ENOMEM;
+	}
+#ifdef DEBUG
+	print_hex_dump(KERN_ERR, "rfc4106 enc shdesc@"__stringify(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, desc,
+		       desc_bytes(desc), 1);
+#endif
+
+	/*
+	 * Job Descriptor and Shared Descriptors
+	 * must all fit into the 64-word Descriptor h/w Buffer
+	 */
+	keys_fit_inline = false;
+	if (DESC_RFC4106_DEC_LEN + DESC_JOB_IO_LEN +
+	    ctx->enckeylen <= CAAM_DESC_BYTES_MAX)
+		keys_fit_inline = true;
+
+	desc = ctx->sh_desc_dec;
+
+	init_sh_desc(desc, HDR_SHARE_SERIAL);
+
+	/* Skip key loading if it is loaded due to sharing */
+	key_jump_cmd = append_jump(desc, JUMP_JSL |
+				   JUMP_TEST_ALL | JUMP_COND_SHRD);
+	if (keys_fit_inline)
+		append_key_as_imm(desc, (void *)ctx->key, ctx->enckeylen,
+				  ctx->enckeylen, CLASS_1 | KEY_DEST_CLASS_REG);
+	else
+		append_key(desc, ctx->key_dma, ctx->enckeylen,
+			   CLASS_1 | KEY_DEST_CLASS_REG);
+	set_jump_tgt_here(desc, key_jump_cmd);
+
+	/* Class 1 operation */
+	append_operation(desc, ctx->class1_alg_type |
+			 OP_ALG_AS_INITFINAL | OP_ALG_DECRYPT | OP_ALG_ICV_ON);
+
+	/* assoclen + cryptlen = seqinlen - ivsize - icvsize */
+	append_math_sub_imm_u32(desc, REG3, SEQINLEN, IMM,
+				ctx->authsize + tfm->ivsize);
+
+	/* assoclen = (assoclen + cryptlen) - cryptlen */
+	append_math_sub(desc, REG2, SEQOUTLEN, REG0, CAAM_CMD_SZ);
+	append_math_sub(desc, VARSEQINLEN, REG3, REG2, CAAM_CMD_SZ);
+
+	/* Will write cryptlen bytes */
+	append_math_sub(desc, VARSEQOUTLEN, SEQOUTLEN, REG0, CAAM_CMD_SZ);
+
+	/* Read Salt */
+	append_fifo_load_as_imm(desc, (void *)(ctx->key + ctx->enckeylen),
+				4, FIFOLD_CLASS_CLASS1 | FIFOLD_TYPE_IV);
+	/* Read AES-GCM-ESP IV */
+	append_seq_fifo_load(desc, tfm->ivsize, FIFOLD_CLASS_CLASS1 |
+			     FIFOLD_TYPE_IV | FIFOLD_TYPE_FLUSH1);
+
+	/* Read assoc data */
+	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_CLASS1 | FIFOLDST_VLF |
+			     FIFOLD_TYPE_AAD | FIFOLD_TYPE_FLUSH1);
+
+	/* Will read cryptlen bytes */
+	append_math_add(desc, VARSEQINLEN, ZERO, REG2, CAAM_CMD_SZ);
+
+	/* Store payload data */
+	append_seq_fifo_store(desc, 0, FIFOST_TYPE_MESSAGE_DATA | FIFOLDST_VLF);
+
+	/* Read encrypted data */
+	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_CLASS1 | FIFOLDST_VLF |
+			     FIFOLD_TYPE_MSG | FIFOLD_TYPE_FLUSH1);
+
+	/* Read ICV */
+	append_seq_fifo_load(desc, ctx->authsize, FIFOLD_CLASS_CLASS1 |
+			     FIFOLD_TYPE_ICV | FIFOLD_TYPE_LAST1);
+
+	ctx->sh_desc_dec_dma = dma_map_single(jrdev, desc,
+					      desc_bytes(desc),
+					      DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, ctx->sh_desc_dec_dma)) {
+		dev_err(jrdev, "unable to map shared descriptor\n");
+		return -ENOMEM;
+	}
+#ifdef DEBUG
+	print_hex_dump(KERN_ERR, "rfc4106 dec shdesc@"__stringify(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, desc,
+		       desc_bytes(desc), 1);
+#endif
+
+	/*
+	 * Job Descriptor and Shared Descriptors
+	 * must all fit into the 64-word Descriptor h/w Buffer
+	 */
+	keys_fit_inline = false;
+	if (DESC_RFC4106_GIVENC_LEN + DESC_JOB_IO_LEN +
+	    ctx->split_key_pad_len + ctx->enckeylen <=
+	    CAAM_DESC_BYTES_MAX)
+		keys_fit_inline = true;
+
+	/* rfc4106_givencrypt shared descriptor */
+	desc = ctx->sh_desc_givenc;
+
+	init_sh_desc(desc, HDR_SHARE_SERIAL);
+
+	/* Skip key loading if it is loaded due to sharing */
+	key_jump_cmd = append_jump(desc, JUMP_JSL | JUMP_TEST_ALL |
+				   JUMP_COND_SHRD);
+	if (keys_fit_inline)
+		append_key_as_imm(desc, (void *)ctx->key, ctx->enckeylen,
+				  ctx->enckeylen, CLASS_1 | KEY_DEST_CLASS_REG);
+	else
+		append_key(desc, ctx->key_dma, ctx->enckeylen,
+			   CLASS_1 | KEY_DEST_CLASS_REG);
+	set_jump_tgt_here(desc, key_jump_cmd);
+
+	/* Generate IV */
+	geniv = NFIFOENTRY_STYPE_PAD | NFIFOENTRY_DEST_DECO |
+		NFIFOENTRY_DTYPE_MSG | NFIFOENTRY_LC1 |
+		NFIFOENTRY_PTYPE_RND | (tfm->ivsize << NFIFOENTRY_DLEN_SHIFT);
+	append_load_imm_u32(desc, geniv, LDST_CLASS_IND_CCB |
+			    LDST_SRCDST_WORD_INFO_FIFO | LDST_IMM);
+	append_cmd(desc, CMD_LOAD | DISABLE_AUTO_INFO_FIFO);
+	move_cmd = append_move(desc, MOVE_SRC_INFIFO | MOVE_DEST_DESCBUF |
+			       (tfm->ivsize << MOVE_LEN_SHIFT));
+	append_cmd(desc, CMD_LOAD | ENABLE_AUTO_INFO_FIFO);
+
+	/* Copy generated IV to OFIFO */
+	write_iv_cmd = append_move(desc, MOVE_SRC_DESCBUF | MOVE_DEST_OUTFIFO |
+				   (tfm->ivsize << MOVE_LEN_SHIFT));
+
+	/* Class 1 operation */
+	append_operation(desc, ctx->class1_alg_type |
+			 OP_ALG_AS_INITFINAL | OP_ALG_ENCRYPT);
+
+	/* ivsize + cryptlen = seqoutlen - authsize */
+	append_math_sub_imm_u32(desc, REG3, SEQOUTLEN, IMM, ctx->authsize);
+
+	/* assoclen = seqinlen - (ivsize + cryptlen) */
+	append_math_sub(desc, VARSEQINLEN, SEQINLEN, REG3, CAAM_CMD_SZ);
+
+	/* Will write ivsize + cryptlen */
+	append_math_add(desc, VARSEQOUTLEN, REG3, REG0, CAAM_CMD_SZ);
+
+	/* Read Salt and generated IV */
+	append_cmd(desc, CMD_FIFO_LOAD | FIFOLD_CLASS_CLASS1 | FIFOLD_TYPE_IV |
+		   FIFOLD_TYPE_FLUSH1 | IMMEDIATE | 12);
+	/* Append Salt */
+	append_data(desc, (void *)(ctx->key + ctx->enckeylen), 4);
+	set_move_tgt_here(desc, move_cmd);
+	set_move_tgt_here(desc, write_iv_cmd);
+	/* Blank commands. Will be overwritten by generated IV. */
+	append_cmd(desc, 0x00000000);
+	append_cmd(desc, 0x00000000);
+	/* End of blank commands */
+
+	/* No need to reload iv */
+	append_seq_fifo_load(desc, tfm->ivsize, FIFOLD_CLASS_SKIP);
+
+	/* Read assoc data */
+	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_CLASS1 | FIFOLDST_VLF |
+			     FIFOLD_TYPE_AAD | FIFOLD_TYPE_FLUSH1);
+
+	/* Will read cryptlen */
+	append_math_add(desc, VARSEQINLEN, SEQINLEN, REG0, CAAM_CMD_SZ);
+
+	/* Store generated IV and encrypted data */
+	append_seq_fifo_store(desc, 0, FIFOST_TYPE_MESSAGE_DATA | FIFOLDST_VLF);
+
+	/* Read payload data */
+	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_CLASS1 | FIFOLDST_VLF |
+			     FIFOLD_TYPE_MSG | FIFOLD_TYPE_LAST1);
+
+	/* Write ICV */
+	append_seq_store(desc, ctx->authsize, LDST_CLASS_1_CCB |
+			 LDST_SRCDST_BYTE_CONTEXT);
+
+	ctx->sh_desc_givenc_dma = dma_map_single(jrdev, desc,
+						 desc_bytes(desc),
+						 DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, ctx->sh_desc_givenc_dma)) {
+		dev_err(jrdev, "unable to map shared descriptor\n");
+		return -ENOMEM;
+	}
+#ifdef DEBUG
+	print_hex_dump(KERN_ERR,
+		       "rfc4106 givenc shdesc@"__stringify(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, desc,
+		       desc_bytes(desc), 1);
+#endif
+
+	return 0;
+}
+
+static int rfc4106_setauthsize(struct crypto_aead *authenc,
+			       unsigned int authsize)
+{
+	struct caam_ctx *ctx = crypto_aead_ctx(authenc);
+
+	ctx->authsize = authsize;
+	rfc4106_set_sh_desc(authenc);
+
+	return 0;
+}
+
+static int rfc4543_set_sh_desc(struct crypto_aead *aead)
+{
+	struct aead_tfm *tfm = &aead->base.crt_aead;
+	struct caam_ctx *ctx = crypto_aead_ctx(aead);
+	struct device *jrdev = ctx->jrdev;
+	bool keys_fit_inline = false;
+	u32 *key_jump_cmd, *write_iv_cmd, *write_aad_cmd;
+	u32 *read_move_cmd, *write_move_cmd;
+	u32 *desc;
+	u32 geniv;
+
+	if (!ctx->enckeylen || !ctx->authsize)
+		return 0;
+
+	/*
+	 * RFC4543 encrypt shared descriptor
+	 * Job Descriptor and Shared Descriptor
+	 * must fit into the 64-word Descriptor h/w Buffer
+	 */
+	if (DESC_RFC4543_ENC_LEN + DESC_JOB_IO_LEN +
+	    ctx->enckeylen <= CAAM_DESC_BYTES_MAX)
+		keys_fit_inline = true;
+
+	desc = ctx->sh_desc_enc;
+
+	init_sh_desc(desc, HDR_SHARE_SERIAL);
+
+	/* Skip key loading if it is loaded due to sharing */
+	key_jump_cmd = append_jump(desc, JUMP_JSL | JUMP_TEST_ALL |
+				   JUMP_COND_SHRD);
+	if (keys_fit_inline)
+		append_key_as_imm(desc, (void *)ctx->key, ctx->enckeylen,
+				  ctx->enckeylen, CLASS_1 | KEY_DEST_CLASS_REG);
+	else
+		append_key(desc, ctx->key_dma, ctx->enckeylen,
+			   CLASS_1 | KEY_DEST_CLASS_REG);
+	set_jump_tgt_here(desc, key_jump_cmd);
+
+	/* Class 1 operation */
+	append_operation(desc, ctx->class1_alg_type |
+			 OP_ALG_AS_INITFINAL | OP_ALG_ENCRYPT);
+
+	/* Load AES-GMAC ESP IV into Math1 register */
+	append_cmd(desc, CMD_SEQ_LOAD | LDST_SRCDST_WORD_DECO_MATH1 |
+		   LDST_CLASS_DECO | tfm->ivsize);
+
+	/* Wait the DMA transaction to finish */
+	append_jump(desc, JUMP_TEST_ALL | JUMP_COND_CALM |
+		    (1 << JUMP_OFFSET_SHIFT));
+
+	/* Overwrite blank immediate AES-GMAC ESP IV data */
+	write_iv_cmd = append_move(desc, MOVE_SRC_MATH1 | MOVE_DEST_DESCBUF |
+				   (tfm->ivsize << MOVE_LEN_SHIFT));
+
+	/* Overwrite blank immediate AAD data */
+	write_aad_cmd = append_move(desc, MOVE_SRC_MATH1 | MOVE_DEST_DESCBUF |
+				    (tfm->ivsize << MOVE_LEN_SHIFT));
+
+	/* cryptlen = seqoutlen - authsize */
+	append_math_sub_imm_u32(desc, REG3, SEQOUTLEN, IMM, ctx->authsize);
+
+	/* assoclen = (seqinlen - ivsize) - cryptlen */
+	append_math_sub(desc, VARSEQINLEN, SEQINLEN, REG3, CAAM_CMD_SZ);
+
+	/* Read Salt and AES-GMAC ESP IV */
+	append_cmd(desc, CMD_FIFO_LOAD | FIFOLD_CLASS_CLASS1 | IMMEDIATE |
+		   FIFOLD_TYPE_IV | FIFOLD_TYPE_FLUSH1 | (4 + tfm->ivsize));
+	/* Append Salt */
+	append_data(desc, (void *)(ctx->key + ctx->enckeylen), 4);
+	set_move_tgt_here(desc, write_iv_cmd);
+	/* Blank commands. Will be overwritten by AES-GMAC ESP IV. */
+	append_cmd(desc, 0x00000000);
+	append_cmd(desc, 0x00000000);
+	/* End of blank commands */
+
+	/* Read assoc data */
+	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_CLASS1 | FIFOLDST_VLF |
+			     FIFOLD_TYPE_AAD);
+
+	/* Will read cryptlen bytes */
+	append_math_add(desc, VARSEQINLEN, ZERO, REG3, CAAM_CMD_SZ);
+
+	/* Will write cryptlen bytes */
+	append_math_add(desc, VARSEQOUTLEN, ZERO, REG3, CAAM_CMD_SZ);
+
+	/*
+	 * MOVE_LEN opcode is not available in all SEC HW revisions,
+	 * thus need to do some magic, i.e. self-patch the descriptor
+	 * buffer.
+	 */
+	read_move_cmd = append_move(desc, MOVE_SRC_DESCBUF | MOVE_DEST_MATH3 |
+				    (0x6 << MOVE_LEN_SHIFT));
+	write_move_cmd = append_move(desc, MOVE_SRC_MATH3 | MOVE_DEST_DESCBUF |
+				     (0x8 << MOVE_LEN_SHIFT));
+
+	/* Authenticate AES-GMAC ESP IV  */
+	append_cmd(desc, CMD_FIFO_LOAD | FIFOLD_CLASS_CLASS1 | IMMEDIATE |
+		   FIFOLD_TYPE_AAD | tfm->ivsize);
+	set_move_tgt_here(desc, write_aad_cmd);
+	/* Blank commands. Will be overwritten by AES-GMAC ESP IV. */
+	append_cmd(desc, 0x00000000);
+	append_cmd(desc, 0x00000000);
+	/* End of blank commands */
+
+	/* Read and write cryptlen bytes */
+	aead_append_src_dst(desc, FIFOLD_TYPE_AAD);
+
+	set_move_tgt_here(desc, read_move_cmd);
+	set_move_tgt_here(desc, write_move_cmd);
+	append_cmd(desc, CMD_LOAD | DISABLE_AUTO_INFO_FIFO);
+	/* Move payload data to OFIFO */
+	append_move(desc, MOVE_SRC_INFIFO_CL | MOVE_DEST_OUTFIFO);
+
+	/* Write ICV */
+	append_seq_store(desc, ctx->authsize, LDST_CLASS_1_CCB |
+			 LDST_SRCDST_BYTE_CONTEXT);
+
+	ctx->sh_desc_enc_dma = dma_map_single(jrdev, desc,
+					      desc_bytes(desc),
+					      DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, ctx->sh_desc_enc_dma)) {
+		dev_err(jrdev, "unable to map shared descriptor\n");
+		return -ENOMEM;
+	}
+#ifdef DEBUG
+	print_hex_dump(KERN_ERR, "rfc4543 enc shdesc@"__stringify(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, desc,
+		       desc_bytes(desc), 1);
+#endif
+
+	/*
+	 * Job Descriptor and Shared Descriptors
+	 * must all fit into the 64-word Descriptor h/w Buffer
+	 */
+	keys_fit_inline = false;
+	if (DESC_RFC4543_DEC_LEN + DESC_JOB_IO_LEN +
+	    ctx->enckeylen <= CAAM_DESC_BYTES_MAX)
+		keys_fit_inline = true;
+
+	desc = ctx->sh_desc_dec;
+
+	init_sh_desc(desc, HDR_SHARE_SERIAL);
+
+	/* Skip key loading if it is loaded due to sharing */
+	key_jump_cmd = append_jump(desc, JUMP_JSL |
+				   JUMP_TEST_ALL | JUMP_COND_SHRD);
+	if (keys_fit_inline)
+		append_key_as_imm(desc, (void *)ctx->key, ctx->enckeylen,
+				  ctx->enckeylen, CLASS_1 | KEY_DEST_CLASS_REG);
+	else
+		append_key(desc, ctx->key_dma, ctx->enckeylen,
+			   CLASS_1 | KEY_DEST_CLASS_REG);
+	set_jump_tgt_here(desc, key_jump_cmd);
+
+	/* Class 1 operation */
+	append_operation(desc, ctx->class1_alg_type |
+			 OP_ALG_AS_INITFINAL | OP_ALG_DECRYPT | OP_ALG_ICV_ON);
+
+	/* Load AES-GMAC ESP IV into Math1 register */
+	append_cmd(desc, CMD_SEQ_LOAD | LDST_SRCDST_WORD_DECO_MATH1 |
+		   LDST_CLASS_DECO | tfm->ivsize);
+
+	/* Wait the DMA transaction to finish */
+	append_jump(desc, JUMP_TEST_ALL | JUMP_COND_CALM |
+		    (1 << JUMP_OFFSET_SHIFT));
+
+	/* assoclen + cryptlen = (seqinlen - ivsize) - icvsize */
+	append_math_sub_imm_u32(desc, REG3, SEQINLEN, IMM, ctx->authsize);
+
+	/* Overwrite blank immediate AES-GMAC ESP IV data */
+	write_iv_cmd = append_move(desc, MOVE_SRC_MATH1 | MOVE_DEST_DESCBUF |
+				   (tfm->ivsize << MOVE_LEN_SHIFT));
+
+	/* Overwrite blank immediate AAD data */
+	write_aad_cmd = append_move(desc, MOVE_SRC_MATH1 | MOVE_DEST_DESCBUF |
+				    (tfm->ivsize << MOVE_LEN_SHIFT));
+
+	/* assoclen = (assoclen + cryptlen) - cryptlen */
+	append_math_sub(desc, REG2, SEQOUTLEN, REG0, CAAM_CMD_SZ);
+	append_math_sub(desc, VARSEQINLEN, REG3, REG2, CAAM_CMD_SZ);
+
+	/*
+	 * MOVE_LEN opcode is not available in all SEC HW revisions,
+	 * thus need to do some magic, i.e. self-patch the descriptor
+	 * buffer.
+	 */
+	read_move_cmd = append_move(desc, MOVE_SRC_DESCBUF | MOVE_DEST_MATH3 |
+				    (0x6 << MOVE_LEN_SHIFT));
+	write_move_cmd = append_move(desc, MOVE_SRC_MATH3 | MOVE_DEST_DESCBUF |
+				     (0x8 << MOVE_LEN_SHIFT));
+
+	/* Read Salt and AES-GMAC ESP IV */
+	append_cmd(desc, CMD_FIFO_LOAD | FIFOLD_CLASS_CLASS1 | IMMEDIATE |
+		   FIFOLD_TYPE_IV | FIFOLD_TYPE_FLUSH1 | (4 + tfm->ivsize));
+	/* Append Salt */
+	append_data(desc, (void *)(ctx->key + ctx->enckeylen), 4);
+	set_move_tgt_here(desc, write_iv_cmd);
+	/* Blank commands. Will be overwritten by AES-GMAC ESP IV. */
+	append_cmd(desc, 0x00000000);
+	append_cmd(desc, 0x00000000);
+	/* End of blank commands */
+
+	/* Read assoc data */
+	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_CLASS1 | FIFOLDST_VLF |
+			     FIFOLD_TYPE_AAD);
+
+	/* Will read cryptlen bytes */
+	append_math_add(desc, VARSEQINLEN, ZERO, REG2, CAAM_CMD_SZ);
+
+	/* Will write cryptlen bytes */
+	append_math_add(desc, VARSEQOUTLEN, ZERO, REG2, CAAM_CMD_SZ);
+
+	/* Authenticate AES-GMAC ESP IV  */
+	append_cmd(desc, CMD_FIFO_LOAD | FIFOLD_CLASS_CLASS1 | IMMEDIATE |
+		   FIFOLD_TYPE_AAD | tfm->ivsize);
+	set_move_tgt_here(desc, write_aad_cmd);
+	/* Blank commands. Will be overwritten by AES-GMAC ESP IV. */
+	append_cmd(desc, 0x00000000);
+	append_cmd(desc, 0x00000000);
+	/* End of blank commands */
+
+	/* Store payload data */
+	append_seq_fifo_store(desc, 0, FIFOST_TYPE_MESSAGE_DATA | FIFOLDST_VLF);
+
+	/* In-snoop cryptlen data */
+	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_BOTH | FIFOLDST_VLF |
+			     FIFOLD_TYPE_AAD | FIFOLD_TYPE_LAST2FLUSH1);
+
+	set_move_tgt_here(desc, read_move_cmd);
+	set_move_tgt_here(desc, write_move_cmd);
+	append_cmd(desc, CMD_LOAD | DISABLE_AUTO_INFO_FIFO);
+	/* Move payload data to OFIFO */
+	append_move(desc, MOVE_SRC_INFIFO_CL | MOVE_DEST_OUTFIFO);
+	append_cmd(desc, CMD_LOAD | ENABLE_AUTO_INFO_FIFO);
+
+	/* Read ICV */
+	append_seq_fifo_load(desc, ctx->authsize, FIFOLD_CLASS_CLASS1 |
+			     FIFOLD_TYPE_ICV | FIFOLD_TYPE_LAST1);
+
+	ctx->sh_desc_dec_dma = dma_map_single(jrdev, desc,
+					      desc_bytes(desc),
+					      DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, ctx->sh_desc_dec_dma)) {
+		dev_err(jrdev, "unable to map shared descriptor\n");
+		return -ENOMEM;
+	}
+#ifdef DEBUG
+	print_hex_dump(KERN_ERR, "rfc4543 dec shdesc@"__stringify(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, desc,
+		       desc_bytes(desc), 1);
+#endif
+
+	/*
+	 * Job Descriptor and Shared Descriptors
+	 * must all fit into the 64-word Descriptor h/w Buffer
+	 */
+	keys_fit_inline = false;
+	if (DESC_RFC4543_GIVENC_LEN + DESC_JOB_IO_LEN +
+	    ctx->enckeylen <= CAAM_DESC_BYTES_MAX)
+		keys_fit_inline = true;
+
+	/* rfc4543_givencrypt shared descriptor */
+	desc = ctx->sh_desc_givenc;
+
+	init_sh_desc(desc, HDR_SHARE_SERIAL);
+
+	/* Skip key loading if it is loaded due to sharing */
+	key_jump_cmd = append_jump(desc, JUMP_JSL | JUMP_TEST_ALL |
+				   JUMP_COND_SHRD);
+	if (keys_fit_inline)
+		append_key_as_imm(desc, (void *)ctx->key, ctx->enckeylen,
+				  ctx->enckeylen, CLASS_1 | KEY_DEST_CLASS_REG);
+	else
+		append_key(desc, ctx->key_dma, ctx->enckeylen,
+			   CLASS_1 | KEY_DEST_CLASS_REG);
+	set_jump_tgt_here(desc, key_jump_cmd);
+
+	/* Generate IV */
+	geniv = NFIFOENTRY_STYPE_PAD | NFIFOENTRY_DEST_DECO |
+		NFIFOENTRY_DTYPE_MSG | NFIFOENTRY_LC1 |
+		NFIFOENTRY_PTYPE_RND | (tfm->ivsize << NFIFOENTRY_DLEN_SHIFT);
+	append_load_imm_u32(desc, geniv, LDST_CLASS_IND_CCB |
+			    LDST_SRCDST_WORD_INFO_FIFO | LDST_IMM);
+	append_cmd(desc, CMD_LOAD | DISABLE_AUTO_INFO_FIFO);
+	/* Move generated IV to Math1 register */
+	append_move(desc, MOVE_SRC_INFIFO | MOVE_DEST_MATH1 |
+		    (tfm->ivsize << MOVE_LEN_SHIFT));
+	append_cmd(desc, CMD_LOAD | ENABLE_AUTO_INFO_FIFO);
+
+	/* Overwrite blank immediate AES-GMAC IV data */
+	write_iv_cmd = append_move(desc, MOVE_SRC_MATH1 | MOVE_DEST_DESCBUF |
+				   (tfm->ivsize << MOVE_LEN_SHIFT));
+
+	/* Overwrite blank immediate AAD data */
+	write_aad_cmd = append_move(desc, MOVE_SRC_MATH1 | MOVE_DEST_DESCBUF |
+				    (tfm->ivsize << MOVE_LEN_SHIFT));
+
+	/* Copy generated IV to OFIFO */
+	append_move(desc, MOVE_SRC_MATH1 | MOVE_DEST_OUTFIFO |
+		    (tfm->ivsize << MOVE_LEN_SHIFT));
+
+	/* Class 1 operation */
+	append_operation(desc, ctx->class1_alg_type |
+			 OP_ALG_AS_INITFINAL | OP_ALG_ENCRYPT);
+
+	/* ivsize + cryptlen = seqoutlen - authsize */
+	append_math_sub_imm_u32(desc, REG3, SEQOUTLEN, IMM, ctx->authsize);
+
+	/* assoclen = seqinlen - (ivsize + cryptlen) */
+	append_math_sub(desc, VARSEQINLEN, SEQINLEN, REG3, CAAM_CMD_SZ);
+
+	/* Will write ivsize + cryptlen */
+	append_math_add(desc, VARSEQOUTLEN, REG3, REG0, CAAM_CMD_SZ);
+
+	/*
+	 * MOVE_LEN opcode is not available in all SEC HW revisions,
+	 * thus need to do some magic, i.e. self-patch the descriptor
+	 * buffer.
+	 */
+	read_move_cmd = append_move(desc, MOVE_SRC_DESCBUF | MOVE_DEST_MATH3 |
+				    (0x6 << MOVE_LEN_SHIFT));
+	write_move_cmd = append_move(desc, MOVE_SRC_MATH3 | MOVE_DEST_DESCBUF |
+				     (0x8 << MOVE_LEN_SHIFT));
+
+	/* Read Salt and AES-GMAC generated IV */
+	append_cmd(desc, CMD_FIFO_LOAD | FIFOLD_CLASS_CLASS1 | IMMEDIATE |
+		   FIFOLD_TYPE_IV | FIFOLD_TYPE_FLUSH1 | (4 + tfm->ivsize));
+	/* Append Salt */
+	append_data(desc, (void *)(ctx->key + ctx->enckeylen), 4);
+	set_move_tgt_here(desc, write_iv_cmd);
+	/* Blank commands. Will be overwritten by AES-GMAC generated IV. */
+	append_cmd(desc, 0x00000000);
+	append_cmd(desc, 0x00000000);
+	/* End of blank commands */
+
+	/* No need to reload iv */
+	append_seq_fifo_load(desc, tfm->ivsize, FIFOLD_CLASS_SKIP);
+
+	/* Read assoc data */
+	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_CLASS1 | FIFOLDST_VLF |
+			     FIFOLD_TYPE_AAD);
+
+	/* Will read cryptlen */
+	append_math_add(desc, VARSEQINLEN, SEQINLEN, REG0, CAAM_CMD_SZ);
+
+	/* Authenticate AES-GMAC IV  */
+	append_cmd(desc, CMD_FIFO_LOAD | FIFOLD_CLASS_CLASS1 | IMMEDIATE |
+		   FIFOLD_TYPE_AAD | tfm->ivsize);
+	set_move_tgt_here(desc, write_aad_cmd);
+	/* Blank commands. Will be overwritten by AES-GMAC IV. */
+	append_cmd(desc, 0x00000000);
+	append_cmd(desc, 0x00000000);
+	/* End of blank commands */
+
+	/* Read and write cryptlen bytes */
+	aead_append_src_dst(desc, FIFOLD_TYPE_AAD);
+
+	set_move_tgt_here(desc, read_move_cmd);
+	set_move_tgt_here(desc, write_move_cmd);
+	append_cmd(desc, CMD_LOAD | DISABLE_AUTO_INFO_FIFO);
+	/* Move payload data to OFIFO */
+	append_move(desc, MOVE_SRC_INFIFO_CL | MOVE_DEST_OUTFIFO);
+
+	/* Write ICV */
+	append_seq_store(desc, ctx->authsize, LDST_CLASS_1_CCB |
+			 LDST_SRCDST_BYTE_CONTEXT);
+
+	ctx->sh_desc_givenc_dma = dma_map_single(jrdev, desc,
+						 desc_bytes(desc),
+						 DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, ctx->sh_desc_givenc_dma)) {
+		dev_err(jrdev, "unable to map shared descriptor\n");
+		return -ENOMEM;
+	}
+#ifdef DEBUG
+	print_hex_dump(KERN_ERR,
+		       "rfc4543 givenc shdesc@"__stringify(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, desc,
+		       desc_bytes(desc), 1);
+#endif
+
+	return 0;
+}
+
+static int rfc4543_setauthsize(struct crypto_aead *authenc,
+			       unsigned int authsize)
+{
+	struct caam_ctx *ctx = crypto_aead_ctx(authenc);
+
+	ctx->authsize = authsize;
+	rfc4543_set_sh_desc(authenc);
+
+	return 0;
+}
+
+static u32 gen_split_aead_key(struct caam_ctx *ctx, const u8 *key_in,
+			      u32 authkeylen)
+{
+	return gen_split_key(ctx->jrdev, ctx->key, ctx->split_key_len,
+			       ctx->split_key_pad_len, key_in, authkeylen,
+			       ctx->alg_op);
+}
+
+static int aead_setkey(struct crypto_aead *aead,
+			       const u8 *key, unsigned int keylen)
+{
+	/* Sizes for MDHA pads (*not* keys): MD5, SHA1, 224, 256, 384, 512 */
+	static const u8 mdpadlen[] = { 16, 20, 32, 32, 64, 64 };
+	struct caam_ctx *ctx = crypto_aead_ctx(aead);
+	struct device *jrdev = ctx->jrdev;
+	struct rtattr *rta = (void *)key;
+	struct crypto_authenc_key_param *param;
+	unsigned int authkeylen;
+	unsigned int enckeylen;
+	int ret = 0;
+
+	param = RTA_DATA(rta);
+	enckeylen = be32_to_cpu(param->enckeylen);
+
+	key += RTA_ALIGN(rta->rta_len);
+	keylen -= RTA_ALIGN(rta->rta_len);
+
+	if (keylen < enckeylen)
+		goto badkey;
+
+	authkeylen = keylen - enckeylen;
+
+	if (keylen > CAAM_MAX_KEY_SIZE)
+		goto badkey;
+
+	/* Pick class 2 key length from algorithm submask */
+	ctx->split_key_len = mdpadlen[(ctx->alg_op & OP_ALG_ALGSEL_SUBMASK) >>
+				      OP_ALG_ALGSEL_SHIFT] * 2;
+	ctx->split_key_pad_len = ALIGN(ctx->split_key_len, 16);
+
+#ifdef DEBUG
+	printk(KERN_ERR "keylen %d enckeylen %d authkeylen %d\n",
+	       keylen, enckeylen, authkeylen);
+	printk(KERN_ERR "split_key_len %d split_key_pad_len %d\n",
+	       ctx->split_key_len, ctx->split_key_pad_len);
+	print_hex_dump(KERN_ERR, "key in @"__stringify(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, key, keylen, 1);
+#endif
+
+	ret = gen_split_aead_key(ctx, key, authkeylen);
+	if (ret) {
+		goto badkey;
+	}
+
+	/* postpend encryption key to auth split key */
+	memcpy(ctx->key + ctx->split_key_pad_len, key + authkeylen, enckeylen);
+
+	ctx->key_dma = dma_map_single(jrdev, ctx->key, ctx->split_key_pad_len +
+				       enckeylen, DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, ctx->key_dma)) {
+		dev_err(jrdev, "unable to map key i/o memory\n");
+		return -ENOMEM;
+	}
+#ifdef DEBUG
+	print_hex_dump(KERN_ERR, "ctx.key@"__stringify(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, ctx->key,
+		       ctx->split_key_pad_len + enckeylen, 1);
+#endif
+
+	ctx->enckeylen = enckeylen;
+
+	ret = aead_set_sh_desc(aead);
+	if (ret) {
+		dma_unmap_single(jrdev, ctx->key_dma, ctx->split_key_pad_len +
+				 enckeylen, DMA_TO_DEVICE);
+	}
+
+	return ret;
+badkey:
+	crypto_aead_set_flags(aead, CRYPTO_TFM_RES_BAD_KEY_LEN);
+	return -EINVAL;
+}
+
+static int tls_setkey(struct crypto_aead *aead, const u8 *key,
+					  unsigned int keylen)
+{
+	/* Sizes for MDHA pads (*not* keys): MD5, SHA1, 224, 256, 384, 512 */
+	static const u8 mdpadlen[] = { 16, 20, 32, 32, 64, 64 };
+	struct caam_ctx *ctx = crypto_aead_ctx(aead);
+	struct device *jrdev = ctx->jrdev;
+	struct rtattr *rta = (void *)key;
+	struct crypto_authenc_key_param *param;
+	unsigned int authkeylen;
+	unsigned int enckeylen;
+	int ret = 0;
+
+	param = RTA_DATA(rta);
+	enckeylen = be32_to_cpu(param->enckeylen);
+
+	key += RTA_ALIGN(rta->rta_len);
+	keylen -= RTA_ALIGN(rta->rta_len);
+
+	if (keylen < enckeylen)
+		goto badkey;
+
+	authkeylen = keylen - enckeylen;
+
+	if (keylen > CAAM_MAX_KEY_SIZE)
+		goto badkey;
+
+	/* Pick class 2 key length from algorithm submask */
+	ctx->split_key_len = mdpadlen[(ctx->alg_op & OP_ALG_ALGSEL_SUBMASK) >>
+				      OP_ALG_ALGSEL_SHIFT] * 2;
+	ctx->split_key_pad_len = ALIGN(ctx->split_key_len, 16);
+
+#ifdef DEBUG
+	dev_err(jrdev, "keylen %d enckeylen %d authkeylen %d\n", keylen,
+		enckeylen, authkeylen);
+	dev_err(jrdev, "split_key_len %d split_key_pad_len %d\n",
+		ctx->split_key_len, ctx->split_key_pad_len);
+	print_hex_dump(KERN_ERR, "key in @"__stringify(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, key, keylen, 1);
+#endif
+
+	ret = gen_split_aead_key(ctx, key, authkeylen);
+	if (ret)
+		goto badkey;
+
+	/* postpend encryption key to auth split key */
+	memcpy(ctx->key + ctx->split_key_pad_len, key + authkeylen, enckeylen);
+
+	ctx->key_dma = dma_map_single(jrdev, ctx->key, ctx->split_key_pad_len +
+				       enckeylen, DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, ctx->key_dma)) {
+		dev_err(jrdev, "unable to map key i/o memory\n");
+		return -ENOMEM;
+	}
+#ifdef DEBUG
+	print_hex_dump(KERN_ERR, "ctx.key@"__stringify(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, ctx->key,
+		       ctx->split_key_pad_len + enckeylen, 1);
+#endif
+
+	ctx->enckeylen = enckeylen;
+
+	ret = tls_set_sh_desc(aead);
+	if (ret) {
+		dma_unmap_single(jrdev, ctx->key_dma, ctx->split_key_pad_len +
+				 enckeylen, DMA_TO_DEVICE);
+	}
+
+	return ret;
+badkey:
+	crypto_aead_set_flags(aead, CRYPTO_TFM_RES_BAD_KEY_LEN);
+	return -EINVAL;
+}
+
+static int gcm_setkey(struct crypto_aead *aead,
+		      const u8 *key, unsigned int keylen)
+{
+	struct caam_ctx *ctx = crypto_aead_ctx(aead);
+	struct device *jrdev = ctx->jrdev;
+	int ret = 0;
+
+#ifdef DEBUG
+	print_hex_dump(KERN_ERR, "key in @"__stringify(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, key, keylen, 1);
+#endif
+
+	memcpy(ctx->key, key, keylen);
+	ctx->key_dma = dma_map_single(jrdev, ctx->key, keylen,
+				      DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, ctx->key_dma)) {
+		dev_err(jrdev, "unable to map key i/o memory\n");
+		return -ENOMEM;
+	}
+	ctx->enckeylen = keylen;
+
+	ret = gcm_set_sh_desc(aead);
+	if (ret) {
+		dma_unmap_single(jrdev, ctx->key_dma, ctx->enckeylen,
+				 DMA_TO_DEVICE);
+	}
+
+	return ret;
+}
+
+static int rfc4106_setkey(struct crypto_aead *aead,
+			  const u8 *key, unsigned int keylen)
+{
+	struct caam_ctx *ctx = crypto_aead_ctx(aead);
+	struct device *jrdev = ctx->jrdev;
+	int ret = 0;
+
+	if (keylen < 4)
+		return -EINVAL;
+
+#ifdef DEBUG
+	print_hex_dump(KERN_ERR, "key in @"__stringify(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, key, keylen, 1);
+#endif
+
+	memcpy(ctx->key, key, keylen);
+
+	/*
+	 * The last four bytes of the key material are used as the salt value
+	 * in the nonce. Update the AES key length.
+	 */
+	ctx->enckeylen = keylen - 4;
 
-	ctx->authsize = authsize;
-	aead_set_sh_desc(authenc);
+	ctx->key_dma = dma_map_single(jrdev, ctx->key, ctx->enckeylen,
+				      DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, ctx->key_dma)) {
+		dev_err(jrdev, "unable to map key i/o memory\n");
+		return -ENOMEM;
+	}
 
-	return 0;
-}
+	ret = rfc4106_set_sh_desc(aead);
+	if (ret) {
+		dma_unmap_single(jrdev, ctx->key_dma, ctx->enckeylen,
+				 DMA_TO_DEVICE);
+	}
 
-static u32 gen_split_aead_key(struct caam_ctx *ctx, const u8 *key_in,
-			      u32 authkeylen)
-{
-	return gen_split_key(ctx->jrdev, ctx->key, ctx->split_key_len,
-			       ctx->split_key_pad_len, key_in, authkeylen,
-			       ctx->alg_op);
+	return ret;
 }
 
-static int aead_setkey(struct crypto_aead *aead,
-			       const u8 *key, unsigned int keylen)
+static int rfc4543_setkey(struct crypto_aead *aead,
+			  const u8 *key, unsigned int keylen)
 {
-	/* Sizes for MDHA pads (*not* keys): MD5, SHA1, 224, 256, 384, 512 */
-	static const u8 mdpadlen[] = { 16, 20, 32, 32, 64, 64 };
 	struct caam_ctx *ctx = crypto_aead_ctx(aead);
 	struct device *jrdev = ctx->jrdev;
-	struct crypto_authenc_keys keys;
 	int ret = 0;
 
-	if (crypto_authenc_extractkeys(&keys, key, keylen) != 0)
-		goto badkey;
-
-	/* Pick class 2 key length from algorithm submask */
-	ctx->split_key_len = mdpadlen[(ctx->alg_op & OP_ALG_ALGSEL_SUBMASK) >>
-				      OP_ALG_ALGSEL_SHIFT] * 2;
-	ctx->split_key_pad_len = ALIGN(ctx->split_key_len, 16);
-
-	if (ctx->split_key_pad_len + keys.enckeylen > CAAM_MAX_KEY_SIZE)
-		goto badkey;
+	if (keylen < 4)
+		return -EINVAL;
 
 #ifdef DEBUG
-	printk(KERN_ERR "keylen %d enckeylen %d authkeylen %d\n",
-	       keys.authkeylen + keys.enckeylen, keys.enckeylen,
-	       keys.authkeylen);
-	printk(KERN_ERR "split_key_len %d split_key_pad_len %d\n",
-	       ctx->split_key_len, ctx->split_key_pad_len);
 	print_hex_dump(KERN_ERR, "key in @"__stringify(__LINE__)": ",
 		       DUMP_PREFIX_ADDRESS, 16, 4, key, keylen, 1);
 #endif
 
-	ret = gen_split_aead_key(ctx, keys.authkey, keys.authkeylen);
-	if (ret) {
-		goto badkey;
-	}
+	memcpy(ctx->key, key, keylen);
 
-	/* postpend encryption key to auth split key */
-	memcpy(ctx->key + ctx->split_key_pad_len, keys.enckey, keys.enckeylen);
+	/*
+	 * The last four bytes of the key material are used as the salt value
+	 * in the nonce. Update the AES key length.
+	 */
+	ctx->enckeylen = keylen - 4;
 
-	ctx->key_dma = dma_map_single(jrdev, ctx->key, ctx->split_key_pad_len +
-				      keys.enckeylen, DMA_TO_DEVICE);
+	ctx->key_dma = dma_map_single(jrdev, ctx->key, ctx->enckeylen,
+				      DMA_TO_DEVICE);
 	if (dma_mapping_error(jrdev, ctx->key_dma)) {
 		dev_err(jrdev, "unable to map key i/o memory\n");
 		return -ENOMEM;
 	}
-#ifdef DEBUG
-	print_hex_dump(KERN_ERR, "ctx.key@"__stringify(__LINE__)": ",
-		       DUMP_PREFIX_ADDRESS, 16, 4, ctx->key,
-		       ctx->split_key_pad_len + keys.enckeylen, 1);
-#endif
-
-	ctx->enckeylen = keys.enckeylen;
 
-	ret = aead_set_sh_desc(aead);
+	ret = rfc4543_set_sh_desc(aead);
 	if (ret) {
-		dma_unmap_single(jrdev, ctx->key_dma, ctx->split_key_pad_len +
-				 keys.enckeylen, DMA_TO_DEVICE);
+		dma_unmap_single(jrdev, ctx->key_dma, ctx->enckeylen,
+				 DMA_TO_DEVICE);
 	}
 
 	return ret;
-badkey:
-	crypto_aead_set_flags(aead, CRYPTO_TFM_RES_BAD_KEY_LEN);
-	return -EINVAL;
 }
 
 static int ablkcipher_setkey(struct crypto_ablkcipher *ablkcipher,
@@ -532,7 +2221,7 @@ static int ablkcipher_setkey(struct crypto_ablkcipher *ablkcipher,
 	struct ablkcipher_tfm *tfm = &ablkcipher->base.crt_ablkcipher;
 	struct device *jrdev = ctx->jrdev;
 	int ret = 0;
-	u32 *key_jump_cmd, *jump_cmd;
+	u32 *key_jump_cmd;
 	u32 *desc;
 
 #ifdef DEBUG
@@ -563,9 +2252,6 @@ static int ablkcipher_setkey(struct crypto_ablkcipher *ablkcipher,
 
 	set_jump_tgt_here(desc, key_jump_cmd);
 
-	/* Propagate errors from shared to job descriptor */
-	append_cmd(desc, SET_OK_NO_PROP_ERRORS | CMD_LOAD);
-
 	/* Load iv */
 	append_cmd(desc, CMD_SEQ_LOAD | LDST_SRCDST_BYTE_CONTEXT |
 		   LDST_CLASS_1_CCB | tfm->ivsize);
@@ -603,11 +2289,7 @@ static int ablkcipher_setkey(struct crypto_ablkcipher *ablkcipher,
 			  ctx->enckeylen, CLASS_1 |
 			  KEY_DEST_CLASS_REG);
 
-	/* For aead, only propagate error immediately if shared */
-	jump_cmd = append_jump(desc, JUMP_TEST_ALL);
 	set_jump_tgt_here(desc, key_jump_cmd);
-	append_cmd(desc, SET_OK_NO_PROP_ERRORS | CMD_LOAD);
-	set_jump_tgt_here(desc, jump_cmd);
 
 	/* load IV */
 	append_cmd(desc, CMD_SEQ_LOAD | LDST_SRCDST_BYTE_CONTEXT |
@@ -619,13 +2301,10 @@ static int ablkcipher_setkey(struct crypto_ablkcipher *ablkcipher,
 	/* Perform operation */
 	ablkcipher_append_src_dst(desc);
 
-	/* Wait for key to load before allowing propagating error */
-	append_dec_shr_done(desc);
-
 	ctx->sh_desc_dec_dma = dma_map_single(jrdev, desc,
 					      desc_bytes(desc),
 					      DMA_TO_DEVICE);
-	if (dma_mapping_error(jrdev, ctx->sh_desc_enc_dma)) {
+	if (dma_mapping_error(jrdev, ctx->sh_desc_dec_dma)) {
 		dev_err(jrdev, "unable to map shared descriptor\n");
 		return -ENOMEM;
 	}
@@ -649,10 +2328,10 @@ static int ablkcipher_setkey(struct crypto_ablkcipher *ablkcipher,
  * @dst_nents: number of segments in output scatterlist
  * @dst_chained: if destination is chained
  * @iv_dma: dma address of iv for checking continuity and link table
- * @desc: h/w descriptor (variable length; must not exceed MAX_CAAM_DESCSIZE)
  * @sec4_sg_bytes: length of dma mapped sec4_sg space
  * @sec4_sg_dma: bus physical mapped address of h/w link table
  * @hw_desc: the h/w job descriptor followed by any referenced link tables
+ * (variable length; must not exceed MAX_CAAM_DESCSIZE)
  */
 struct aead_edesc {
 	int assoc_nents;
@@ -760,11 +2439,8 @@ static void aead_encrypt_done(struct device *jrdev, u32 *desc, u32 err,
 	edesc = (struct aead_edesc *)((char *)desc -
 		 offsetof(struct aead_edesc, hw_desc));
 
-	if (err) {
-		char tmp[CAAM_ERROR_STR_MAX];
-
-		dev_err(jrdev, "%08x: %s\n", err, caam_jr_strstatus(tmp, err));
-	}
+	if (err)
+		caam_jr_strstatus(jrdev, err);
 
 	aead_unmap(jrdev, edesc, req);
 
@@ -811,11 +2487,8 @@ static void aead_decrypt_done(struct device *jrdev, u32 *desc, u32 err,
 		       req->cryptlen - ctx->authsize, 1);
 #endif
 
-	if (err) {
-		char tmp[CAAM_ERROR_STR_MAX];
-
-		dev_err(jrdev, "%08x: %s\n", err, caam_jr_strstatus(tmp, err));
-	}
+	if (err)
+		caam_jr_strstatus(jrdev, err);
 
 	aead_unmap(jrdev, edesc, req);
 
@@ -845,6 +2518,96 @@ static void aead_decrypt_done(struct device *jrdev, u32 *desc, u32 err,
 	aead_request_complete(req, err);
 }
 
+static void tls_encrypt_done(struct device *jrdev, u32 *desc, u32 err,
+				   void *context)
+{
+	struct aead_request *req = context;
+	struct aead_edesc *edesc;
+
+#ifdef DEBUG
+	dev_err(jrdev, "%s %d: err 0x%x\n", __func__, __LINE__, err);
+#endif
+
+	edesc = (struct aead_edesc *)((char *)desc -
+		 offsetof(struct aead_edesc, hw_desc));
+
+	if (err)
+		caam_jr_strstatus(jrdev, err);
+
+	aead_unmap(jrdev, edesc, req);
+
+#ifdef DEBUG
+	print_hex_dump(KERN_ERR, "assoc  @"__stringify(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, sg_virt(req->assoc),
+		       req->assoclen, 1);
+	print_hex_dump(KERN_ERR, "dst    @"__stringify(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, sg_virt(req->dst),
+		       edesc->dst_nents ? 100 : req->cryptlen, 1);
+#endif
+
+	kfree(edesc);
+
+	aead_request_complete(req, err);
+}
+
+static void tls_decrypt_done(struct device *jrdev, u32 *desc, u32 err,
+				   void *context)
+{
+	struct aead_request *req = context;
+	struct aead_edesc *edesc;
+	int cryptlen = req->cryptlen;
+	u8 padsize;
+	u8 padding[255]; /* padding can be 0-255 bytes */
+	int i;
+
+#ifdef DEBUG
+	dev_err(jrdev, "%s %d: err 0x%x\n", __func__, __LINE__, err);
+#endif
+
+	edesc = (struct aead_edesc *)((char *)desc -
+		 offsetof(struct aead_edesc, hw_desc));
+
+#ifdef DEBUG
+	print_hex_dump(KERN_ERR, "dst    @"__stringify(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, sg_virt(req->dst),
+		       req->cryptlen, 1);
+#endif
+
+	if (err)
+		caam_jr_strstatus(jrdev, err);
+
+	aead_unmap(jrdev, edesc, req);
+
+	/*
+	 * verify hw auth check passed else return -EBADMSG
+	 */
+	if ((err & JRSTA_CCBERR_ERRID_MASK) == JRSTA_CCBERR_ERRID_ICVCHK) {
+		err = -EBADMSG;
+		goto out;
+	}
+
+	/* Padding checking */
+	cryptlen -= 1;
+	scatterwalk_map_and_copy(&padsize, req->dst, cryptlen, 1, 0);
+	if (padsize > cryptlen) {
+		err = -EBADMSG;
+		goto out;
+	}
+	cryptlen -= padsize;
+	scatterwalk_map_and_copy(padding, req->dst, cryptlen, padsize, 0);
+	/* the padding content must be equal with padsize */
+	for (i = 0; i < padsize; i++)
+		if (padding[i] != padsize) {
+			err = -EBADMSG;
+			break;
+		}
+
+out:
+	kfree(edesc);
+
+	aead_request_complete(req, err);
+}
+
 static void ablkcipher_encrypt_done(struct device *jrdev, u32 *desc, u32 err,
 				   void *context)
 {
@@ -860,11 +2623,8 @@ static void ablkcipher_encrypt_done(struct device *jrdev, u32 *desc, u32 err,
 	edesc = (struct ablkcipher_edesc *)((char *)desc -
 		 offsetof(struct ablkcipher_edesc, hw_desc));
 
-	if (err) {
-		char tmp[CAAM_ERROR_STR_MAX];
-
-		dev_err(jrdev, "%08x: %s\n", err, caam_jr_strstatus(tmp, err));
-	}
+	if (err)
+		caam_jr_strstatus(jrdev, err);
 
 #ifdef DEBUG
 	print_hex_dump(KERN_ERR, "dstiv  @"__stringify(__LINE__)": ",
@@ -895,11 +2655,8 @@ static void ablkcipher_decrypt_done(struct device *jrdev, u32 *desc, u32 err,
 
 	edesc = (struct ablkcipher_edesc *)((char *)desc -
 		 offsetof(struct ablkcipher_edesc, hw_desc));
-	if (err) {
-		char tmp[CAAM_ERROR_STR_MAX];
-
-		dev_err(jrdev, "%08x: %s\n", err, caam_jr_strstatus(tmp, err));
-	}
+	if (err)
+		caam_jr_strstatus(jrdev, err);
 
 #ifdef DEBUG
 	print_hex_dump(KERN_ERR, "dstiv  @"__stringify(__LINE__)": ",
@@ -932,6 +2689,7 @@ static void init_aead_job(u32 *sh_desc, dma_addr_t ptr,
 	u32 out_options = 0, in_options;
 	dma_addr_t dst_dma, src_dma;
 	int len, sec4_sg_index = 0;
+	bool is_gcm = false;
 
 #ifdef DEBUG
 	debug("assoclen %d cryptlen %d authsize %d\n",
@@ -950,15 +2708,23 @@ static void init_aead_job(u32 *sh_desc, dma_addr_t ptr,
 		       desc_bytes(sh_desc), 1);
 #endif
 
+	if (((ctx->class1_alg_type & OP_ALG_ALGSEL_MASK) ==
+	      OP_ALG_ALGSEL_AES) &&
+	    ((ctx->class1_alg_type & OP_ALG_AAI_MASK) == OP_ALG_AAI_GCM))
+		is_gcm = true;
+
 	len = desc_len(sh_desc);
 	init_job_desc_shared(desc, ptr, len, HDR_SHARE_DEFER | HDR_REVERSE);
 
 	if (all_contig) {
-		src_dma = sg_dma_address(req->assoc);
+		if (is_gcm)
+			src_dma = edesc->iv_dma;
+		else
+			src_dma = sg_dma_address(req->assoc);
 		in_options = 0;
 	} else {
 		src_dma = edesc->sec4_sg_dma;
-		sec4_sg_index += (edesc->assoc_nents ? : 1) + 1 +
+		sec4_sg_index += edesc->assoc_nents + 1 +
 				 (edesc->src_nents ? : 1);
 		in_options = LDST_SGF;
 	}
@@ -971,7 +2737,7 @@ static void init_aead_job(u32 *sh_desc, dma_addr_t ptr,
 			dst_dma = sg_dma_address(req->src);
 		} else {
 			dst_dma = src_dma + sizeof(struct sec4_sg_entry) *
-				  ((edesc->assoc_nents ? : 1) + 1);
+				  (edesc->assoc_nents + 1);
 			out_options = LDST_SGF;
 		}
 	} else {
@@ -1008,6 +2774,7 @@ static void init_aead_giv_job(u32 *sh_desc, dma_addr_t ptr,
 	u32 out_options = 0, in_options;
 	dma_addr_t dst_dma, src_dma;
 	int len, sec4_sg_index = 0;
+	bool is_gcm = false;
 
 #ifdef DEBUG
 	debug("assoclen %d cryptlen %d authsize %d\n",
@@ -1025,11 +2792,19 @@ static void init_aead_giv_job(u32 *sh_desc, dma_addr_t ptr,
 		       desc_bytes(sh_desc), 1);
 #endif
 
+	if (((ctx->class1_alg_type & OP_ALG_ALGSEL_MASK) ==
+	      OP_ALG_ALGSEL_AES) &&
+	    ((ctx->class1_alg_type & OP_ALG_AAI_MASK) == OP_ALG_AAI_GCM))
+		is_gcm = true;
+
 	len = desc_len(sh_desc);
 	init_job_desc_shared(desc, ptr, len, HDR_SHARE_DEFER | HDR_REVERSE);
 
 	if (contig & GIV_SRC_CONTIG) {
-		src_dma = sg_dma_address(req->assoc);
+		if (is_gcm)
+			src_dma = edesc->iv_dma;
+		else
+			src_dma = sg_dma_address(req->assoc);
 		in_options = 0;
 	} else {
 		src_dma = edesc->sec4_sg_dma;
@@ -1044,7 +2819,8 @@ static void init_aead_giv_job(u32 *sh_desc, dma_addr_t ptr,
 	} else {
 		if (likely(req->src == req->dst)) {
 			dst_dma = src_dma + sizeof(struct sec4_sg_entry) *
-				  edesc->assoc_nents;
+				  (edesc->assoc_nents +
+				   (is_gcm ? 1 + edesc->src_nents : 0));
 			out_options = LDST_SGF;
 		} else {
 			dst_dma = edesc->sec4_sg_dma +
@@ -1059,6 +2835,95 @@ static void init_aead_giv_job(u32 *sh_desc, dma_addr_t ptr,
 }
 
 /*
+ * Fill in tls job descriptor either for encrypt or decrypt
+ */
+static void init_tls_job(u32 *sh_desc, dma_addr_t ptr,
+			  struct aead_edesc *edesc,
+			  struct aead_request *req,
+			  bool all_contig, bool encrypt, unsigned int padsize)
+{
+	struct crypto_aead *aead = crypto_aead_reqtfm(req);
+	struct caam_ctx *ctx = crypto_aead_ctx(aead);
+	int ivsize = crypto_aead_ivsize(aead);
+	int authsize = ctx->authsize;
+	u32 *desc = edesc->hw_desc;
+	u32 out_options = 0, in_options;
+	dma_addr_t dst_dma, src_dma;
+	int len, sec4_sg_index = 0;
+	bool is_gcm = false;
+
+#ifdef DEBUG
+	debug("assoclen %d cryptlen %d authsize %d\n",
+	      req->assoclen, req->cryptlen, authsize);
+	print_hex_dump(KERN_ERR, "assoc  @"__stringify(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, sg_virt(req->assoc),
+		       req->assoclen , 1);
+	print_hex_dump(KERN_ERR, "presciv@"__stringify(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, req->iv,
+		       edesc->src_nents ? 100 : ivsize, 1);
+	print_hex_dump(KERN_ERR, "src    @"__stringify(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, sg_virt(req->src),
+			edesc->src_nents ? 100 : req->cryptlen, 1);
+	print_hex_dump(KERN_ERR, "shrdesc@"__stringify(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, sh_desc,
+		       desc_bytes(sh_desc), 1);
+#endif
+
+	if (((ctx->class1_alg_type & OP_ALG_ALGSEL_MASK) ==
+	      OP_ALG_ALGSEL_AES) &&
+	    ((ctx->class1_alg_type & OP_ALG_AAI_MASK) == OP_ALG_AAI_GCM))
+		is_gcm = true;
+
+	len = desc_len(sh_desc);
+	init_job_desc_shared(desc, ptr, len, HDR_SHARE_DEFER | HDR_REVERSE);
+
+	if (all_contig) {
+		if (is_gcm)
+			src_dma = edesc->iv_dma;
+		else
+			src_dma = sg_dma_address(req->assoc);
+		in_options = 0;
+	} else {
+		src_dma = edesc->sec4_sg_dma;
+		sec4_sg_index += (edesc->assoc_nents ? : 1) + 1 +
+				 (edesc->src_nents ? : 1);
+		in_options = LDST_SGF;
+	}
+
+	if (likely(req->src == req->dst)) {
+		if (all_contig) {
+			dst_dma = sg_dma_address(req->src);
+		} else {
+			dst_dma = src_dma + sizeof(struct sec4_sg_entry) *
+				  ((edesc->assoc_nents ? : 1) + 1);
+			out_options = LDST_SGF;
+		}
+	} else {
+		if (!edesc->dst_nents) {
+			dst_dma = sg_dma_address(req->dst);
+		} else {
+			dst_dma = edesc->sec4_sg_dma +
+				  sec4_sg_index *
+				  sizeof(struct sec4_sg_entry);
+			out_options = LDST_SGF;
+		}
+	}
+
+	/*
+	 * For decrypt, do not strip ICV, Padding, Padding length since
+	 * upper layer(s) perform padding checking.
+	 */
+	if (encrypt)
+		append_seq_out_ptr(desc, dst_dma, req->cryptlen + padsize +
+				   authsize, out_options);
+	else
+		append_seq_out_ptr(desc, dst_dma, req->cryptlen, out_options);
+
+	append_seq_in_ptr(desc, src_dma, req->assoclen + ivsize +
+			  req->cryptlen, in_options);
+}
+
+/*
  * Fill in ablkcipher job descriptor
  */
 static void init_ablkcipher_job(u32 *sh_desc, dma_addr_t ptr,
@@ -1090,7 +2955,7 @@ static void init_ablkcipher_job(u32 *sh_desc, dma_addr_t ptr,
 		in_options = 0;
 	} else {
 		src_dma = edesc->sec4_sg_dma;
-		sec4_sg_index += (iv_contig ? 0 : 1) + edesc->src_nents;
+		sec4_sg_index += edesc->src_nents + 1;
 		in_options = LDST_SGF;
 	}
 	append_seq_in_ptr(desc, src_dma, req->nbytes + ivsize, in_options);
@@ -1120,7 +2985,7 @@ static void init_ablkcipher_job(u32 *sh_desc, dma_addr_t ptr,
  */
 static struct aead_edesc *aead_edesc_alloc(struct aead_request *req,
 					   int desc_bytes, bool *all_contig_ptr,
-					   bool encrypt)
+					   bool encrypt, bool strip_icv)
 {
 	struct crypto_aead *aead = crypto_aead_reqtfm(req);
 	struct caam_ctx *ctx = crypto_aead_ctx(aead);
@@ -1136,14 +3001,20 @@ static struct aead_edesc *aead_edesc_alloc(struct aead_request *req,
 	int ivsize = crypto_aead_ivsize(aead);
 	int sec4_sg_index, sec4_sg_len = 0, sec4_sg_bytes;
 	unsigned int authsize = ctx->authsize;
+	bool is_gcm = false;
 
-	assoc_nents = sg_count(req->assoc, req->assoclen, &assoc_chained);
+	assoc_nents = __sg_count(req->assoc, req->assoclen, &assoc_chained);
 
 	if (unlikely(req->dst != req->src)) {
+		int extralen;
+
 		src_nents = sg_count(req->src, req->cryptlen, &src_chained);
-		dst_nents = sg_count(req->dst,
-				     req->cryptlen +
-					(encrypt ? authsize : (-authsize)),
+
+		if (encrypt)
+			extralen = authsize;
+		else
+			extralen = strip_icv ? (-authsize) : 0;
+		dst_nents = sg_count(req->dst, req->cryptlen + extralen,
 				     &dst_chained);
 	} else {
 		src_nents = sg_count(req->src,
@@ -1151,9 +3022,9 @@ static struct aead_edesc *aead_edesc_alloc(struct aead_request *req,
 					(encrypt ? authsize : 0),
 				     &src_chained);
 	}
-
-	sgc = dma_map_sg_chained(jrdev, req->assoc, assoc_nents ? : 1,
-				 DMA_TO_DEVICE, assoc_chained);
+	if (req->assoclen)
+		sgc = dma_map_sg_chained(jrdev, req->assoc, assoc_nents,
+					DMA_TO_DEVICE, assoc_chained);
 	if (likely(req->src == req->dst)) {
 		sgc = dma_map_sg_chained(jrdev, req->src, src_nents ? : 1,
 					 DMA_BIDIRECTIONAL, src_chained);
@@ -1164,16 +3035,36 @@ static struct aead_edesc *aead_edesc_alloc(struct aead_request *req,
 					 DMA_FROM_DEVICE, dst_chained);
 	}
 
-	/* Check if data are contiguous */
 	iv_dma = dma_map_single(jrdev, req->iv, ivsize, DMA_TO_DEVICE);
-	if (assoc_nents || sg_dma_address(req->assoc) + req->assoclen !=
-	    iv_dma || src_nents || iv_dma + ivsize !=
-	    sg_dma_address(req->src)) {
-		all_contig = false;
-		assoc_nents = assoc_nents ? : 1;
+	if (dma_mapping_error(jrdev, iv_dma)) {
+		dev_err(jrdev, "unable to map IV\n");
+		return ERR_PTR(-ENOMEM);
+	}
+
+	if (((ctx->class1_alg_type & OP_ALG_ALGSEL_MASK) ==
+	      OP_ALG_ALGSEL_AES) &&
+	    ((ctx->class1_alg_type & OP_ALG_AAI_MASK) == OP_ALG_AAI_GCM))
+		is_gcm = true;
+
+	/*
+	 * Check if data are contiguous.
+	 * GCM expected input sequence: IV, AAD, text
+	 * All other - expected input sequence: AAD, IV, text
+	 */
+	if (is_gcm)
+		all_contig = ((assoc_nents == 1) &&
+			      iv_dma + ivsize == sg_dma_address(req->assoc) &&
+			      !src_nents && sg_dma_address(req->assoc) +
+			      req->assoclen == sg_dma_address(req->src));
+	else
+		all_contig = ((assoc_nents == 1) && sg_dma_address(req->assoc) +
+			      req->assoclen == iv_dma && !src_nents &&
+			      iv_dma + ivsize == sg_dma_address(req->src));
+	if (!all_contig) {
 		src_nents = src_nents ? : 1;
 		sec4_sg_len = assoc_nents + 1 + src_nents;
 	}
+
 	sec4_sg_len += dst_nents;
 
 	sec4_sg_bytes = sec4_sg_len * sizeof(struct sec4_sg_entry);
@@ -1196,30 +3087,46 @@ static struct aead_edesc *aead_edesc_alloc(struct aead_request *req,
 	edesc->sec4_sg_bytes = sec4_sg_bytes;
 	edesc->sec4_sg = (void *)edesc + sizeof(struct aead_edesc) +
 			 desc_bytes;
-	edesc->sec4_sg_dma = dma_map_single(jrdev, edesc->sec4_sg,
-					    sec4_sg_bytes, DMA_TO_DEVICE);
 	*all_contig_ptr = all_contig;
 
 	sec4_sg_index = 0;
 	if (!all_contig) {
-		sg_to_sec4_sg(req->assoc,
-			      (assoc_nents ? : 1),
-			      edesc->sec4_sg +
-			      sec4_sg_index, 0);
-		sec4_sg_index += assoc_nents ? : 1;
+		if (!is_gcm && assoc_nents) {
+			sg_to_sec4_sg(req->assoc,
+				      assoc_nents,
+				      edesc->sec4_sg + sec4_sg_index,
+				      0);
+			sec4_sg_index += assoc_nents;
+		}
+
 		dma_to_sec4_sg_one(edesc->sec4_sg + sec4_sg_index,
 				   iv_dma, ivsize, 0);
 		sec4_sg_index += 1;
+
+		if (is_gcm && assoc_nents) {
+			sg_to_sec4_sg(req->assoc,
+				      assoc_nents,
+				      edesc->sec4_sg + sec4_sg_index,
+				      0);
+			sec4_sg_index += assoc_nents;
+		}
+
 		sg_to_sec4_sg_last(req->src,
-				   (src_nents ? : 1),
+				   src_nents,
 				   edesc->sec4_sg +
 				   sec4_sg_index, 0);
-		sec4_sg_index += src_nents ? : 1;
+		sec4_sg_index += src_nents;
 	}
 	if (dst_nents) {
 		sg_to_sec4_sg_last(req->dst, dst_nents,
 				   edesc->sec4_sg + sec4_sg_index, 0);
 	}
+	edesc->sec4_sg_dma = dma_map_single(jrdev, edesc->sec4_sg,
+					    sec4_sg_bytes, DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, edesc->sec4_sg_dma)) {
+		dev_err(jrdev, "unable to map S/G table\n");
+		return ERR_PTR(-ENOMEM);
+	}
 
 	return edesc;
 }
@@ -1236,7 +3143,7 @@ static int aead_encrypt(struct aead_request *req)
 
 	/* allocate extended descriptor */
 	edesc = aead_edesc_alloc(req, DESC_JOB_IO_LEN *
-				 CAAM_CMD_SZ, &all_contig, true);
+				 CAAM_CMD_SZ, &all_contig, true, true);
 	if (IS_ERR(edesc))
 		return PTR_ERR(edesc);
 
@@ -1273,7 +3180,7 @@ static int aead_decrypt(struct aead_request *req)
 
 	/* allocate extended descriptor */
 	edesc = aead_edesc_alloc(req, DESC_JOB_IO_LEN *
-				 CAAM_CMD_SZ, &all_contig, false);
+				 CAAM_CMD_SZ, &all_contig, false, true);
 	if (IS_ERR(edesc))
 		return PTR_ERR(edesc);
 
@@ -1325,6 +3232,7 @@ static struct aead_edesc *aead_giv_edesc_alloc(struct aead_givcrypt_request
 	int ivsize = crypto_aead_ivsize(aead);
 	bool assoc_chained = false, src_chained = false, dst_chained = false;
 	int sec4_sg_index, sec4_sg_len = 0, sec4_sg_bytes;
+	bool is_gcm = false;
 
 	assoc_nents = sg_count(req->assoc, req->assoclen, &assoc_chained);
 	src_nents = sg_count(req->src, req->cryptlen, &src_chained);
@@ -1339,31 +3247,65 @@ static struct aead_edesc *aead_giv_edesc_alloc(struct aead_givcrypt_request
 		sgc = dma_map_sg_chained(jrdev, req->src, src_nents ? : 1,
 					 DMA_BIDIRECTIONAL, src_chained);
 	} else {
-		sgc = dma_map_sg_chained(jrdev, req->src, src_nents ? : 1,
-					 DMA_TO_DEVICE, src_chained);
-		sgc = dma_map_sg_chained(jrdev, req->dst, dst_nents ? : 1,
-					 DMA_FROM_DEVICE, dst_chained);
+		sgc = dma_map_sg_chained(jrdev, req->src, src_nents ? : 1,
+					 DMA_TO_DEVICE, src_chained);
+		sgc = dma_map_sg_chained(jrdev, req->dst, dst_nents ? : 1,
+					 DMA_FROM_DEVICE, dst_chained);
+	}
+
+	iv_dma = dma_map_single(jrdev, greq->giv, ivsize, DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, iv_dma)) {
+		dev_err(jrdev, "unable to map IV\n");
+		return ERR_PTR(-ENOMEM);
+	}
+
+	if (((ctx->class1_alg_type & OP_ALG_ALGSEL_MASK) ==
+	      OP_ALG_ALGSEL_AES) &&
+	    ((ctx->class1_alg_type & OP_ALG_AAI_MASK) == OP_ALG_AAI_GCM))
+		is_gcm = true;
+
+	/*
+	 * Check if data are contiguous.
+	 * GCM expected input sequence: IV, AAD, text
+	 * All other - expected input sequence: AAD, IV, text
+	 */
+
+	if (is_gcm) {
+		if (assoc_nents || iv_dma + ivsize !=
+		    sg_dma_address(req->assoc) || src_nents ||
+		    sg_dma_address(req->assoc) + req->assoclen !=
+		    sg_dma_address(req->src))
+			contig &= ~GIV_SRC_CONTIG;
+	} else {
+		if (assoc_nents ||
+		    sg_dma_address(req->assoc) + req->assoclen != iv_dma ||
+		    src_nents || iv_dma + ivsize != sg_dma_address(req->src))
+			contig &= ~GIV_SRC_CONTIG;
 	}
 
-	/* Check if data are contiguous */
-	iv_dma = dma_map_single(jrdev, greq->giv, ivsize, DMA_TO_DEVICE);
-	if (assoc_nents || sg_dma_address(req->assoc) + req->assoclen !=
-	    iv_dma || src_nents || iv_dma + ivsize != sg_dma_address(req->src))
-		contig &= ~GIV_SRC_CONTIG;
 	if (dst_nents || iv_dma + ivsize != sg_dma_address(req->dst))
 		contig &= ~GIV_DST_CONTIG;
-	if (unlikely(req->src != req->dst)) {
-		dst_nents = dst_nents ? : 1;
-		sec4_sg_len += 1;
-	}
+
 	if (!(contig & GIV_SRC_CONTIG)) {
 		assoc_nents = assoc_nents ? : 1;
 		src_nents = src_nents ? : 1;
 		sec4_sg_len += assoc_nents + 1 + src_nents;
-		if (likely(req->src == req->dst))
+		if (req->src == req->dst &&
+		    (src_nents || iv_dma + ivsize != sg_dma_address(req->src)))
 			contig &= ~GIV_DST_CONTIG;
 	}
-	sec4_sg_len += dst_nents;
+
+	/*
+	 * Add new sg entries for GCM output sequence.
+	 * Expected output sequence: IV, encrypted text.
+	 */
+	if (is_gcm && req->src == req->dst && !(contig & GIV_DST_CONTIG))
+		sec4_sg_len += 1 + src_nents;
+
+	if (unlikely(req->src != req->dst)) {
+		dst_nents = dst_nents ? : 1;
+		sec4_sg_len += 1 + dst_nents;
+	}
 
 	sec4_sg_bytes = sec4_sg_len * sizeof(struct sec4_sg_entry);
 
@@ -1385,24 +3327,40 @@ static struct aead_edesc *aead_giv_edesc_alloc(struct aead_givcrypt_request
 	edesc->sec4_sg_bytes = sec4_sg_bytes;
 	edesc->sec4_sg = (void *)edesc + sizeof(struct aead_edesc) +
 			 desc_bytes;
-	edesc->sec4_sg_dma = dma_map_single(jrdev, edesc->sec4_sg,
-					    sec4_sg_bytes, DMA_TO_DEVICE);
 	*contig_ptr = contig;
 
 	sec4_sg_index = 0;
 	if (!(contig & GIV_SRC_CONTIG)) {
-		sg_to_sec4_sg(req->assoc, assoc_nents,
-			      edesc->sec4_sg +
-			      sec4_sg_index, 0);
-		sec4_sg_index += assoc_nents;
+		if (!is_gcm) {
+			sg_to_sec4_sg(req->assoc, assoc_nents,
+				      edesc->sec4_sg + sec4_sg_index, 0);
+			sec4_sg_index += assoc_nents;
+		}
+
 		dma_to_sec4_sg_one(edesc->sec4_sg + sec4_sg_index,
 				   iv_dma, ivsize, 0);
 		sec4_sg_index += 1;
+
+		if (is_gcm) {
+			sg_to_sec4_sg(req->assoc, assoc_nents,
+				      edesc->sec4_sg + sec4_sg_index, 0);
+			sec4_sg_index += assoc_nents;
+		}
+
 		sg_to_sec4_sg_last(req->src, src_nents,
 				   edesc->sec4_sg +
 				   sec4_sg_index, 0);
 		sec4_sg_index += src_nents;
 	}
+
+	if (is_gcm && req->src == req->dst && !(contig & GIV_DST_CONTIG)) {
+		dma_to_sec4_sg_one(edesc->sec4_sg + sec4_sg_index,
+				   iv_dma, ivsize, 0);
+		sec4_sg_index += 1;
+		sg_to_sec4_sg_last(req->src, src_nents,
+				   edesc->sec4_sg + sec4_sg_index, 0);
+	}
+
 	if (unlikely(req->src != req->dst && !(contig & GIV_DST_CONTIG))) {
 		dma_to_sec4_sg_one(edesc->sec4_sg + sec4_sg_index,
 				   iv_dma, ivsize, 0);
@@ -1410,6 +3368,12 @@ static struct aead_edesc *aead_giv_edesc_alloc(struct aead_givcrypt_request
 		sg_to_sec4_sg_last(req->dst, dst_nents,
 				   edesc->sec4_sg + sec4_sg_index, 0);
 	}
+	edesc->sec4_sg_dma = dma_map_single(jrdev, edesc->sec4_sg,
+					    sec4_sg_bytes, DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, edesc->sec4_sg_dma)) {
+		dev_err(jrdev, "unable to map S/G table\n");
+		return ERR_PTR(-ENOMEM);
+	}
 
 	return edesc;
 }
@@ -1459,6 +3423,109 @@ static int aead_givencrypt(struct aead_givcrypt_request *areq)
 	return ret;
 }
 
+static int aead_null_givencrypt(struct aead_givcrypt_request *areq)
+{
+	return aead_encrypt(&areq->areq);
+}
+
+static int tls_encrypt(struct aead_request *req)
+{
+	struct aead_edesc *edesc;
+	struct crypto_aead *aead = crypto_aead_reqtfm(req);
+	struct caam_ctx *ctx = crypto_aead_ctx(aead);
+	struct device *jrdev = ctx->jrdev;
+	bool all_contig;
+	u32 *desc;
+	int ret = 0;
+	unsigned int blocksize = crypto_aead_blocksize(aead);
+	unsigned int padsize;
+
+	padsize = blocksize - ((req->cryptlen +  ctx->authsize) % blocksize);
+
+	/*
+	 * allocate extended tls descriptor
+	 * TLS 1.0 has no explicit IV in the packet, but it is needed as input
+	 * since it is used by CBC.
+	 * ctx->authsize is temporary set to include also padlen
+	 */
+	ctx->authsize += padsize;
+	edesc = aead_edesc_alloc(req, DESC_JOB_IO_LEN * CAAM_CMD_SZ,
+				 &all_contig, true, true);
+	if (IS_ERR(edesc))
+		return PTR_ERR(edesc);
+	ctx->authsize -= padsize;
+
+	/* Create and submit job descriptor */
+	init_tls_job(ctx->sh_desc_enc, ctx->sh_desc_enc_dma, edesc, req,
+		     all_contig, true, padsize);
+#ifdef DEBUG
+	print_hex_dump(KERN_ERR, "tls enc jobdesc@"__stringify(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, edesc->hw_desc,
+		       desc_bytes(edesc->hw_desc), 1);
+#endif
+
+	desc = edesc->hw_desc;
+	ret = caam_jr_enqueue(jrdev, desc, tls_encrypt_done, req);
+	if (!ret) {
+		ret = -EINPROGRESS;
+	} else {
+		aead_unmap(jrdev, edesc, req);
+		kfree(edesc);
+	}
+
+	return ret;
+}
+
+static int tls_decrypt(struct aead_request *req)
+{
+	struct aead_edesc *edesc;
+	struct crypto_aead *aead = crypto_aead_reqtfm(req);
+	struct caam_ctx *ctx = crypto_aead_ctx(aead);
+	struct device *jrdev = ctx->jrdev;
+	bool all_contig;
+	u32 *desc;
+	int ret = 0;
+
+	/*
+	 * allocate extended tls descriptor
+	 * TLS 1.0 has no explicit IV in the packet, but it is needed as input
+	 * since it is used by CBC.
+	 * Assumption: since padding and ICV are not stripped (upper layer
+	 * checks padding), req->dst has to be big enough to hold payloadlen +
+	 * padlen + icvlen.
+	 */
+	edesc = aead_edesc_alloc(req, DESC_JOB_IO_LEN * CAAM_CMD_SZ,
+				 &all_contig, false, false);
+	if (IS_ERR(edesc))
+		return PTR_ERR(edesc);
+
+#ifdef DEBUG
+	print_hex_dump(KERN_ERR, "dec src@"__stringify(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, sg_virt(req->src),
+		       req->cryptlen, 1);
+#endif
+
+	/* Create and submit job descriptor*/
+	init_tls_job(ctx->sh_desc_dec, ctx->sh_desc_dec_dma, edesc, req,
+		     all_contig, false, 0);
+#ifdef DEBUG
+	print_hex_dump(KERN_ERR, "tls dec jobdesc@"__stringify(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, edesc->hw_desc,
+		       desc_bytes(edesc->hw_desc), 1);
+#endif
+
+	desc = edesc->hw_desc;
+	ret = caam_jr_enqueue(jrdev, desc, tls_decrypt_done, req);
+	if (!ret) {
+		ret = -EINPROGRESS;
+	} else {
+		aead_unmap(jrdev, edesc, req);
+		kfree(edesc);
+	}
+
+	return ret;
+}
+
 /*
  * allocate and map the ablkcipher extended descriptor for ablkcipher
  */
@@ -1496,11 +3563,16 @@ static struct ablkcipher_edesc *ablkcipher_edesc_alloc(struct ablkcipher_request
 					 DMA_FROM_DEVICE, dst_chained);
 	}
 
+	iv_dma = dma_map_single(jrdev, req->info, ivsize, DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, iv_dma)) {
+		dev_err(jrdev, "unable to map IV\n");
+		return ERR_PTR(-ENOMEM);
+	}
+
 	/*
 	 * Check if iv can be contiguous with source and destination.
 	 * If so, include it. If not, create scatterlist.
 	 */
-	iv_dma = dma_map_single(jrdev, req->info, ivsize, DMA_TO_DEVICE);
 	if (!src_nents && iv_dma + ivsize == sg_dma_address(req->src))
 		iv_contig = true;
 	else
@@ -1539,6 +3611,11 @@ static struct ablkcipher_edesc *ablkcipher_edesc_alloc(struct ablkcipher_request
 
 	edesc->sec4_sg_dma = dma_map_single(jrdev, edesc->sec4_sg,
 					    sec4_sg_bytes, DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, edesc->sec4_sg_dma)) {
+		dev_err(jrdev, "unable to map S/G table\n");
+		return ERR_PTR(-ENOMEM);
+	}
+
 	edesc->iv_dma = iv_dma;
 
 #ifdef DEBUG
@@ -1643,11 +3720,130 @@ struct caam_alg_template {
 	u32 class1_alg_type;
 	u32 class2_alg_type;
 	u32 alg_op;
+	int min_era;
 };
 
 static struct caam_alg_template driver_algs[] = {
 	/* single-pass ipsec_esp descriptor */
 	{
+		.name = "authenc(hmac(md5),ecb(cipher_null))",
+		.driver_name = "authenc-hmac-md5-ecb-cipher_null-caam",
+		.blocksize = NULL_BLOCK_SIZE,
+		.type = CRYPTO_ALG_TYPE_AEAD,
+		.template_aead = {
+			.setkey = aead_setkey,
+			.setauthsize = aead_setauthsize,
+			.encrypt = aead_encrypt,
+			.decrypt = aead_decrypt,
+			.givencrypt = aead_null_givencrypt,
+			.geniv = "<built-in>",
+			.ivsize = NULL_IV_SIZE,
+			.maxauthsize = MD5_DIGEST_SIZE,
+			},
+		.class1_alg_type = 0,
+		.class2_alg_type = OP_ALG_ALGSEL_MD5 | OP_ALG_AAI_HMAC_PRECOMP,
+		.alg_op = OP_ALG_ALGSEL_MD5 | OP_ALG_AAI_HMAC,
+	},
+	{
+		.name = "authenc(hmac(sha1),ecb(cipher_null))",
+		.driver_name = "authenc-hmac-sha1-ecb-cipher_null-caam",
+		.blocksize = NULL_BLOCK_SIZE,
+		.type = CRYPTO_ALG_TYPE_AEAD,
+		.template_aead = {
+			.setkey = aead_setkey,
+			.setauthsize = aead_setauthsize,
+			.encrypt = aead_encrypt,
+			.decrypt = aead_decrypt,
+			.givencrypt = aead_null_givencrypt,
+			.geniv = "<built-in>",
+			.ivsize = NULL_IV_SIZE,
+			.maxauthsize = SHA1_DIGEST_SIZE,
+			},
+		.class1_alg_type = 0,
+		.class2_alg_type = OP_ALG_ALGSEL_SHA1 | OP_ALG_AAI_HMAC_PRECOMP,
+		.alg_op = OP_ALG_ALGSEL_SHA1 | OP_ALG_AAI_HMAC,
+	},
+	{
+		.name = "authenc(hmac(sha224),ecb(cipher_null))",
+		.driver_name = "authenc-hmac-sha224-ecb-cipher_null-caam",
+		.blocksize = NULL_BLOCK_SIZE,
+		.type = CRYPTO_ALG_TYPE_AEAD,
+		.template_aead = {
+			.setkey = aead_setkey,
+			.setauthsize = aead_setauthsize,
+			.encrypt = aead_encrypt,
+			.decrypt = aead_decrypt,
+			.givencrypt = aead_null_givencrypt,
+			.geniv = "<built-in>",
+			.ivsize = NULL_IV_SIZE,
+			.maxauthsize = SHA224_DIGEST_SIZE,
+			},
+		.class1_alg_type = 0,
+		.class2_alg_type = OP_ALG_ALGSEL_SHA224 |
+				   OP_ALG_AAI_HMAC_PRECOMP,
+		.alg_op = OP_ALG_ALGSEL_SHA224 | OP_ALG_AAI_HMAC,
+	},
+	{
+		.name = "authenc(hmac(sha256),ecb(cipher_null))",
+		.driver_name = "authenc-hmac-sha256-ecb-cipher_null-caam",
+		.blocksize = NULL_BLOCK_SIZE,
+		.type = CRYPTO_ALG_TYPE_AEAD,
+		.template_aead = {
+			.setkey = aead_setkey,
+			.setauthsize = aead_setauthsize,
+			.encrypt = aead_encrypt,
+			.decrypt = aead_decrypt,
+			.givencrypt = aead_null_givencrypt,
+			.geniv = "<built-in>",
+			.ivsize = NULL_IV_SIZE,
+			.maxauthsize = SHA256_DIGEST_SIZE,
+			},
+		.class1_alg_type = 0,
+		.class2_alg_type = OP_ALG_ALGSEL_SHA256 |
+				   OP_ALG_AAI_HMAC_PRECOMP,
+		.alg_op = OP_ALG_ALGSEL_SHA256 | OP_ALG_AAI_HMAC,
+	},
+	{
+		.name = "authenc(hmac(sha384),ecb(cipher_null))",
+		.driver_name = "authenc-hmac-sha384-ecb-cipher_null-caam",
+		.blocksize = NULL_BLOCK_SIZE,
+		.type = CRYPTO_ALG_TYPE_AEAD,
+		.template_aead = {
+			.setkey = aead_setkey,
+			.setauthsize = aead_setauthsize,
+			.encrypt = aead_encrypt,
+			.decrypt = aead_decrypt,
+			.givencrypt = aead_null_givencrypt,
+			.geniv = "<built-in>",
+			.ivsize = NULL_IV_SIZE,
+			.maxauthsize = SHA384_DIGEST_SIZE,
+			},
+		.class1_alg_type = 0,
+		.class2_alg_type = OP_ALG_ALGSEL_SHA384 |
+				   OP_ALG_AAI_HMAC_PRECOMP,
+		.alg_op = OP_ALG_ALGSEL_SHA384 | OP_ALG_AAI_HMAC,
+	},
+	{
+		.name = "authenc(hmac(sha512),ecb(cipher_null))",
+		.driver_name = "authenc-hmac-sha512-ecb-cipher_null-caam",
+		.blocksize = NULL_BLOCK_SIZE,
+		.type = CRYPTO_ALG_TYPE_AEAD,
+		.template_aead = {
+			.setkey = aead_setkey,
+			.setauthsize = aead_setauthsize,
+			.encrypt = aead_encrypt,
+			.decrypt = aead_decrypt,
+			.givencrypt = aead_null_givencrypt,
+			.geniv = "<built-in>",
+			.ivsize = NULL_IV_SIZE,
+			.maxauthsize = SHA512_DIGEST_SIZE,
+			},
+		.class1_alg_type = 0,
+		.class2_alg_type = OP_ALG_ALGSEL_SHA512 |
+				   OP_ALG_AAI_HMAC_PRECOMP,
+		.alg_op = OP_ALG_ALGSEL_SHA512 | OP_ALG_AAI_HMAC,
+	},
+	{
 		.name = "authenc(hmac(md5),cbc(aes))",
 		.driver_name = "authenc-hmac-md5-cbc-aes-caam",
 		.blocksize = AES_BLOCK_SIZE,
@@ -1665,6 +3861,7 @@ static struct caam_alg_template driver_algs[] = {
 		.class1_alg_type = OP_ALG_ALGSEL_AES | OP_ALG_AAI_CBC,
 		.class2_alg_type = OP_ALG_ALGSEL_MD5 | OP_ALG_AAI_HMAC_PRECOMP,
 		.alg_op = OP_ALG_ALGSEL_MD5 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
 	},
 	{
 		.name = "authenc(hmac(sha1),cbc(aes))",
@@ -1684,6 +3881,7 @@ static struct caam_alg_template driver_algs[] = {
 		.class1_alg_type = OP_ALG_ALGSEL_AES | OP_ALG_AAI_CBC,
 		.class2_alg_type = OP_ALG_ALGSEL_SHA1 | OP_ALG_AAI_HMAC_PRECOMP,
 		.alg_op = OP_ALG_ALGSEL_SHA1 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
 	},
 	{
 		.name = "authenc(hmac(sha224),cbc(aes))",
@@ -1704,6 +3902,7 @@ static struct caam_alg_template driver_algs[] = {
 		.class2_alg_type = OP_ALG_ALGSEL_SHA224 |
 				   OP_ALG_AAI_HMAC_PRECOMP,
 		.alg_op = OP_ALG_ALGSEL_SHA224 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
 	},
 	{
 		.name = "authenc(hmac(sha256),cbc(aes))",
@@ -1724,6 +3923,7 @@ static struct caam_alg_template driver_algs[] = {
 		.class2_alg_type = OP_ALG_ALGSEL_SHA256 |
 				   OP_ALG_AAI_HMAC_PRECOMP,
 		.alg_op = OP_ALG_ALGSEL_SHA256 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
 	},
 	{
 		.name = "authenc(hmac(sha384),cbc(aes))",
@@ -1744,6 +3944,7 @@ static struct caam_alg_template driver_algs[] = {
 		.class2_alg_type = OP_ALG_ALGSEL_SHA384 |
 				   OP_ALG_AAI_HMAC_PRECOMP,
 		.alg_op = OP_ALG_ALGSEL_SHA384 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
 	},
 
 	{
@@ -1765,6 +3966,7 @@ static struct caam_alg_template driver_algs[] = {
 		.class2_alg_type = OP_ALG_ALGSEL_SHA512 |
 				   OP_ALG_AAI_HMAC_PRECOMP,
 		.alg_op = OP_ALG_ALGSEL_SHA512 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
 	},
 	{
 		.name = "authenc(hmac(md5),cbc(des3_ede))",
@@ -1784,6 +3986,7 @@ static struct caam_alg_template driver_algs[] = {
 		.class1_alg_type = OP_ALG_ALGSEL_3DES | OP_ALG_AAI_CBC,
 		.class2_alg_type = OP_ALG_ALGSEL_MD5 | OP_ALG_AAI_HMAC_PRECOMP,
 		.alg_op = OP_ALG_ALGSEL_MD5 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
 	},
 	{
 		.name = "authenc(hmac(sha1),cbc(des3_ede))",
@@ -1803,6 +4006,7 @@ static struct caam_alg_template driver_algs[] = {
 		.class1_alg_type = OP_ALG_ALGSEL_3DES | OP_ALG_AAI_CBC,
 		.class2_alg_type = OP_ALG_ALGSEL_SHA1 | OP_ALG_AAI_HMAC_PRECOMP,
 		.alg_op = OP_ALG_ALGSEL_SHA1 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
 	},
 	{
 		.name = "authenc(hmac(sha224),cbc(des3_ede))",
@@ -1823,6 +4027,7 @@ static struct caam_alg_template driver_algs[] = {
 		.class2_alg_type = OP_ALG_ALGSEL_SHA224 |
 				   OP_ALG_AAI_HMAC_PRECOMP,
 		.alg_op = OP_ALG_ALGSEL_SHA224 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
 	},
 	{
 		.name = "authenc(hmac(sha256),cbc(des3_ede))",
@@ -1843,6 +4048,7 @@ static struct caam_alg_template driver_algs[] = {
 		.class2_alg_type = OP_ALG_ALGSEL_SHA256 |
 				   OP_ALG_AAI_HMAC_PRECOMP,
 		.alg_op = OP_ALG_ALGSEL_SHA256 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
 	},
 	{
 		.name = "authenc(hmac(sha384),cbc(des3_ede))",
@@ -1863,6 +4069,7 @@ static struct caam_alg_template driver_algs[] = {
 		.class2_alg_type = OP_ALG_ALGSEL_SHA384 |
 				   OP_ALG_AAI_HMAC_PRECOMP,
 		.alg_op = OP_ALG_ALGSEL_SHA384 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
 	},
 	{
 		.name = "authenc(hmac(sha512),cbc(des3_ede))",
@@ -1883,6 +4090,7 @@ static struct caam_alg_template driver_algs[] = {
 		.class2_alg_type = OP_ALG_ALGSEL_SHA512 |
 				   OP_ALG_AAI_HMAC_PRECOMP,
 		.alg_op = OP_ALG_ALGSEL_SHA512 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
 	},
 	{
 		.name = "authenc(hmac(md5),cbc(des))",
@@ -1902,6 +4110,7 @@ static struct caam_alg_template driver_algs[] = {
 		.class1_alg_type = OP_ALG_ALGSEL_DES | OP_ALG_AAI_CBC,
 		.class2_alg_type = OP_ALG_ALGSEL_MD5 | OP_ALG_AAI_HMAC_PRECOMP,
 		.alg_op = OP_ALG_ALGSEL_MD5 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
 	},
 	{
 		.name = "authenc(hmac(sha1),cbc(des))",
@@ -1921,6 +4130,7 @@ static struct caam_alg_template driver_algs[] = {
 		.class1_alg_type = OP_ALG_ALGSEL_DES | OP_ALG_AAI_CBC,
 		.class2_alg_type = OP_ALG_ALGSEL_SHA1 | OP_ALG_AAI_HMAC_PRECOMP,
 		.alg_op = OP_ALG_ALGSEL_SHA1 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
 	},
 	{
 		.name = "authenc(hmac(sha224),cbc(des))",
@@ -1941,6 +4151,7 @@ static struct caam_alg_template driver_algs[] = {
 		.class2_alg_type = OP_ALG_ALGSEL_SHA224 |
 				   OP_ALG_AAI_HMAC_PRECOMP,
 		.alg_op = OP_ALG_ALGSEL_SHA224 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
 	},
 	{
 		.name = "authenc(hmac(sha256),cbc(des))",
@@ -1961,6 +4172,7 @@ static struct caam_alg_template driver_algs[] = {
 		.class2_alg_type = OP_ALG_ALGSEL_SHA256 |
 				   OP_ALG_AAI_HMAC_PRECOMP,
 		.alg_op = OP_ALG_ALGSEL_SHA256 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
 	},
 	{
 		.name = "authenc(hmac(sha384),cbc(des))",
@@ -1981,6 +4193,7 @@ static struct caam_alg_template driver_algs[] = {
 		.class2_alg_type = OP_ALG_ALGSEL_SHA384 |
 				   OP_ALG_AAI_HMAC_PRECOMP,
 		.alg_op = OP_ALG_ALGSEL_SHA384 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
 	},
 	{
 		.name = "authenc(hmac(sha512),cbc(des))",
@@ -2001,6 +4214,83 @@ static struct caam_alg_template driver_algs[] = {
 		.class2_alg_type = OP_ALG_ALGSEL_SHA512 |
 				   OP_ALG_AAI_HMAC_PRECOMP,
 		.alg_op = OP_ALG_ALGSEL_SHA512 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
+	},
+	{
+		.name = "rfc4106(gcm(aes))",
+		.driver_name = "rfc4106-gcm-aes-caam",
+		.blocksize = 1,
+		.type = CRYPTO_ALG_TYPE_AEAD,
+		.template_aead = {
+			.setkey = rfc4106_setkey,
+			.setauthsize = rfc4106_setauthsize,
+			.encrypt = aead_encrypt,
+			.decrypt = aead_decrypt,
+			.givencrypt = aead_givencrypt,
+			.geniv = "<built-in>",
+			.ivsize = 8,
+			.maxauthsize = AES_BLOCK_SIZE,
+			},
+		.class1_alg_type = OP_ALG_ALGSEL_AES | OP_ALG_AAI_GCM,
+		.min_era = 2,
+	},
+	{
+		.name = "rfc4543(gcm(aes))",
+		.driver_name = "rfc4543-gcm-aes-caam",
+		.blocksize = 1,
+		.type = CRYPTO_ALG_TYPE_AEAD,
+		.template_aead = {
+			.setkey = rfc4543_setkey,
+			.setauthsize = rfc4543_setauthsize,
+			.encrypt = aead_encrypt,
+			.decrypt = aead_decrypt,
+			.givencrypt = aead_givencrypt,
+			.geniv = "<built-in>",
+			.ivsize = 8,
+			.maxauthsize = AES_BLOCK_SIZE,
+			},
+		.class1_alg_type = OP_ALG_ALGSEL_AES | OP_ALG_AAI_GCM,
+		.min_era = 3,
+	},
+	/* TLS record descriptors */
+	{
+		.name = "tls10(hmac(sha1),cbc(aes))",
+		.driver_name = "tls10-hmac-sha1-cbc-aes-caam",
+		.blocksize = AES_BLOCK_SIZE,
+		.type = CRYPTO_ALG_TYPE_AEAD,
+		.template_aead = {
+			.setkey = tls_setkey,
+			.setauthsize = tls_setauthsize,
+			.encrypt = tls_encrypt,
+			.decrypt = tls_decrypt,
+			.givencrypt = NULL,
+			.geniv = "<built-in>",
+			.ivsize = AES_BLOCK_SIZE,
+			.maxauthsize = SHA1_DIGEST_SIZE,
+			},
+		.class1_alg_type = OP_ALG_ALGSEL_AES | OP_ALG_AAI_CBC,
+		.class2_alg_type = OP_ALG_ALGSEL_SHA1 | OP_ALG_AAI_HMAC_PRECOMP,
+		.alg_op = OP_ALG_ALGSEL_SHA1 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
+	},
+	/* Galois Counter Mode */
+	{
+		.name = "gcm(aes)",
+		.driver_name = "gcm-aes-caam",
+		.blocksize = 1,
+		.type = CRYPTO_ALG_TYPE_AEAD,
+		.template_aead = {
+			.setkey = gcm_setkey,
+			.setauthsize = gcm_setauthsize,
+			.encrypt = aead_encrypt,
+			.decrypt = aead_decrypt,
+			.givencrypt = NULL,
+			.geniv = "<built-in>",
+			.ivsize = 12,
+			.maxauthsize = AES_BLOCK_SIZE,
+			},
+		.class1_alg_type = OP_ALG_ALGSEL_AES | OP_ALG_AAI_GCM,
+		.min_era = 2,
 	},
 	/* ablkcipher descriptor */
 	{
@@ -2018,6 +4308,7 @@ static struct caam_alg_template driver_algs[] = {
 			.ivsize = AES_BLOCK_SIZE,
 			},
 		.class1_alg_type = OP_ALG_ALGSEL_AES | OP_ALG_AAI_CBC,
+		.min_era = 2,
 	},
 	{
 		.name = "cbc(des3_ede)",
@@ -2034,6 +4325,7 @@ static struct caam_alg_template driver_algs[] = {
 			.ivsize = DES3_EDE_BLOCK_SIZE,
 			},
 		.class1_alg_type = OP_ALG_ALGSEL_3DES | OP_ALG_AAI_CBC,
+		.min_era = 2,
 	},
 	{
 		.name = "cbc(des)",
@@ -2050,6 +4342,7 @@ static struct caam_alg_template driver_algs[] = {
 			.ivsize = DES_BLOCK_SIZE,
 			},
 		.class1_alg_type = OP_ALG_ALGSEL_DES | OP_ALG_AAI_CBC,
+		.min_era = 2,
 	}
 };
 
@@ -2067,6 +4360,16 @@ static int caam_cra_init(struct crypto_tfm *tfm)
 	struct caam_crypto_alg *caam_alg =
 		 container_of(alg, struct caam_crypto_alg, crypto_alg);
 	struct caam_ctx *ctx = crypto_tfm_ctx(tfm);
+	/* Digest sizes for MD5, SHA1, SHA-224, SHA-256, SHA-384, SHA-512 */
+	static const u8 digest_size[] = {
+		MD5_DIGEST_SIZE,
+		SHA1_DIGEST_SIZE,
+		SHA224_DIGEST_SIZE,
+		SHA256_DIGEST_SIZE,
+		SHA384_DIGEST_SIZE,
+		SHA512_DIGEST_SIZE
+	};
+	u8 op_id;
 
 	ctx->jrdev = caam_jr_alloc();
 	if (IS_ERR(ctx->jrdev)) {
@@ -2079,6 +4382,25 @@ static int caam_cra_init(struct crypto_tfm *tfm)
 	ctx->class2_alg_type = OP_TYPE_CLASS2_ALG | caam_alg->class2_alg_type;
 	ctx->alg_op = OP_TYPE_CLASS2_ALG | caam_alg->alg_op;
 
+	/*
+	 * Need authsize, in case setauthsize callback not called
+	 * by upper layer (e.g. TLS).
+	 */
+	if (caam_alg->alg_op) {
+		op_id = (ctx->alg_op & OP_ALG_ALGSEL_SUBMASK)
+				>> OP_ALG_ALGSEL_SHIFT;
+		if (op_id < ARRAY_SIZE(digest_size)) {
+			ctx->authsize = digest_size[op_id];
+		} else {
+			dev_err(ctx->jrdev, "incorrect op_id %d; must be less than %zu\n",
+					op_id, ARRAY_SIZE(digest_size));
+			caam_jr_free(ctx->jrdev);
+			return -EINVAL;
+		}
+	} else {
+		ctx->authsize = 0;
+	}
+
 	return 0;
 }
 
@@ -2099,6 +4421,11 @@ static void caam_cra_exit(struct crypto_tfm *tfm)
 		dma_unmap_single(ctx->jrdev, ctx->sh_desc_givenc_dma,
 				 desc_bytes(ctx->sh_desc_givenc),
 				 DMA_TO_DEVICE);
+	if (ctx->key_dma &&
+	    !dma_mapping_error(ctx->jrdev, ctx->key_dma))
+		dma_unmap_single(ctx->jrdev, ctx->key_dma,
+				 ctx->enckeylen + ctx->split_key_pad_len,
+				 DMA_TO_DEVICE);
 
 	caam_jr_free(ctx->jrdev);
 }
@@ -2164,15 +4491,49 @@ static struct caam_crypto_alg *caam_alg_alloc(struct caam_alg_template
 
 static int __init caam_algapi_init(void)
 {
+	struct device_node *dev_node;
+	struct platform_device *pdev;
+	struct device *ctrldev;
+	struct caam_drv_private *priv;
 	int i = 0, err = 0;
 
+	dev_node = of_find_compatible_node(NULL, NULL, "fsl,sec-v4.0");
+	if (!dev_node) {
+		dev_node = of_find_compatible_node(NULL, NULL, "fsl,sec4.0");
+		if (!dev_node)
+			return -ENODEV;
+	}
+
+	pdev = of_find_device_by_node(dev_node);
+	if (!pdev)
+		return -ENODEV;
+
+	ctrldev = &pdev->dev;
+	priv = dev_get_drvdata(ctrldev);
+	of_node_put(dev_node);
+
+	/*
+	 * If priv is NULL, it's probably because the caam driver wasn't
+	 * properly initialized (e.g. RNG4 init failed). Thus, bail out here.
+	 */
+	if (!priv)
+		return -ENODEV;
+
+
 	INIT_LIST_HEAD(&alg_list);
 
 	/* register crypto algorithms the device supports */
 	for (i = 0; i < ARRAY_SIZE(driver_algs); i++) {
-		/* TODO: check if h/w supports alg */
 		struct caam_crypto_alg *t_alg;
 
+		/* check if h/w supports alg */
+		if (priv->era > 0 && priv->era < driver_algs[i].min_era) {
+			pr_warn("%s needs Era %d or higher but SEC is Era %d, skipping it\n",
+				driver_algs[i].driver_name,
+				driver_algs[i].min_era, priv->era);
+			continue;
+		}
+
 		t_alg = caam_alg_alloc(&driver_algs[i]);
 		if (IS_ERR(t_alg)) {
 			err = PTR_ERR(t_alg);
diff --git a/drivers/crypto/caam/caamalg_qi.c b/drivers/crypto/caam/caamalg_qi.c
new file mode 100644
index 0000000..cc47085
--- /dev/null
+++ b/drivers/crypto/caam/caamalg_qi.c
@@ -0,0 +1,2345 @@
+/*
+ * Freescale FSL CAAM support for crypto API over QI backend
+ *
+ * Copyright 2008-2011, 2013 Freescale Semiconductor, Inc.
+ *
+ */
+
+
+#include "compat.h"
+
+#include "regs.h"
+#include "intern.h"
+#include "desc_constr.h"
+#include "error.h"
+#include "sg_sw_sec4.h"
+#include "sg_sw_qm.h"
+#include "key_gen.h"
+#include "qi.h"
+#include "jr.h"
+#include "ctrl.h"
+
+/*
+ * crypto alg
+ */
+#define CAAM_CRA_PRIORITY		4000
+/* max key is sum of AES_MAX_KEY_SIZE, max split key size */
+#define CAAM_MAX_KEY_SIZE		(AES_MAX_KEY_SIZE + \
+					 SHA512_DIGEST_SIZE * 2)
+/* max IV is max of AES_BLOCK_SIZE, DES3_EDE_BLOCK_SIZE */
+#define CAAM_MAX_IV_LENGTH		16
+
+/* length of descriptors text */
+#define DESC_AEAD_BASE			(4 * CAAM_CMD_SZ)
+#define DESC_AEAD_ENC_LEN		(DESC_AEAD_BASE + 16 * CAAM_CMD_SZ)
+#define DESC_AEAD_DEC_LEN		(DESC_AEAD_BASE + 21 * CAAM_CMD_SZ)
+#define DESC_AEAD_GIVENC_LEN		(DESC_AEAD_ENC_LEN + 7 * CAAM_CMD_SZ)
+
+#define DESC_TLS_BASE			(4 * CAAM_CMD_SZ)
+#define DESC_TLS10_ENC_LEN		(DESC_TLS_BASE + 29 * CAAM_CMD_SZ)
+
+#define DESC_MAX_USED_BYTES		(DESC_AEAD_GIVENC_LEN + \
+					 CAAM_MAX_KEY_SIZE)
+#define DESC_MAX_USED_LEN		(DESC_MAX_USED_BYTES / CAAM_CMD_SZ)
+
+/* Set DK bit in class 1 operation if shared */
+static inline void append_dec_op1(u32 *desc, u32 type)
+{
+	u32 *jump_cmd, *uncond_jump_cmd;
+
+	jump_cmd = append_jump(desc, JUMP_TEST_ALL | JUMP_COND_SHRD);
+	append_operation(desc, type | OP_ALG_AS_INITFINAL |
+			 OP_ALG_DECRYPT);
+	uncond_jump_cmd = append_jump(desc, JUMP_TEST_ALL);
+	set_jump_tgt_here(desc, jump_cmd);
+	append_operation(desc, type | OP_ALG_AS_INITFINAL |
+			 OP_ALG_DECRYPT | OP_ALG_AAI_DK);
+	set_jump_tgt_here(desc, uncond_jump_cmd);
+}
+
+/*
+ * For aead functions, read payload and write payload,
+ * both of which are specified in req->src and req->dst
+ */
+static inline void aead_append_src_dst(u32 *desc, u32 msg_type)
+{
+	append_seq_fifo_store(desc, 0, FIFOST_TYPE_MESSAGE_DATA | KEY_VLF);
+	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_BOTH |
+			     KEY_VLF | msg_type | FIFOLD_TYPE_LASTBOTH);
+}
+
+/*
+ * For aead encrypt and decrypt, read iv for both classes
+ */
+static inline void aead_append_ld_iv(u32 *desc, int ivsize)
+{
+	append_cmd(desc, CMD_SEQ_LOAD | LDST_SRCDST_BYTE_CONTEXT |
+		   LDST_CLASS_1_CCB | ivsize);
+	append_move(desc, MOVE_SRC_CLASS1CTX | MOVE_DEST_CLASS2INFIFO | ivsize);
+}
+
+/*
+ * If all data, including src (with assoc and iv) or dst (with iv only) are
+ * contiguous
+ */
+#define GIV_SRC_CONTIG		1
+#define GIV_DST_CONTIG		(1 << 1)
+
+enum optype {
+	ENCRYPT,
+	DECRYPT,
+	GIVENCRYPT,
+	NUM_OP
+};
+/*
+ * per-session context
+ */
+struct caam_ctx {
+	struct device *jrdev;
+	u32 sh_desc_enc[DESC_MAX_USED_LEN];
+	u32 sh_desc_dec[DESC_MAX_USED_LEN];
+	u32 sh_desc_givenc[DESC_MAX_USED_LEN];
+	u32 class1_alg_type;
+	u32 class2_alg_type;
+	u32 alg_op;
+	u8 key[CAAM_MAX_KEY_SIZE];
+	dma_addr_t key_dma;
+	unsigned int enckeylen;
+	unsigned int split_key_len;
+	unsigned int split_key_pad_len;
+	unsigned int authsize;
+	struct device *qidev;
+	spinlock_t lock;	/* Protects multiple init of driver context */
+	struct caam_drv_ctx *drv_ctx[NUM_OP];
+};
+
+static void append_key_aead(u32 *desc, struct caam_ctx *ctx,
+			    int keys_fit_inline)
+{
+	if (keys_fit_inline) {
+		append_key_as_imm(desc, ctx->key, ctx->split_key_pad_len,
+				  ctx->split_key_len, CLASS_2 |
+				  KEY_DEST_MDHA_SPLIT | KEY_ENC);
+		append_key_as_imm(desc, (void *)ctx->key +
+				  ctx->split_key_pad_len, ctx->enckeylen,
+				  ctx->enckeylen, CLASS_1 | KEY_DEST_CLASS_REG);
+	} else {
+		append_key(desc, ctx->key_dma, ctx->split_key_len, CLASS_2 |
+			   KEY_DEST_MDHA_SPLIT | KEY_ENC);
+		append_key(desc, ctx->key_dma + ctx->split_key_pad_len,
+			   ctx->enckeylen, CLASS_1 | KEY_DEST_CLASS_REG);
+	}
+}
+
+static void init_sh_desc_key_aead(u32 *desc, struct caam_ctx *ctx,
+				  int keys_fit_inline)
+{
+	u32 *key_jump_cmd;
+
+	init_sh_desc(desc, HDR_SHARE_SERIAL);
+
+	/* Skip if already shared */
+	key_jump_cmd = append_jump(desc, JUMP_JSL | JUMP_TEST_ALL |
+				   JUMP_COND_SHRD);
+
+	append_key_aead(desc, ctx, keys_fit_inline);
+
+	set_jump_tgt_here(desc, key_jump_cmd);
+}
+
+static int aead_set_sh_desc(struct crypto_aead *aead)
+{
+	struct aead_tfm *tfm = &aead->base.crt_aead;
+	struct caam_ctx *ctx = crypto_aead_ctx(aead);
+	bool keys_fit_inline;
+	u32 *key_jump_cmd;
+	u32 geniv, moveiv;
+	u32 *desc;
+
+	if (!ctx->enckeylen || !ctx->authsize)
+		return 0;
+
+	/*
+	 * Job Descriptor and Shared Descriptors
+	 * must all fit into the 64-word Descriptor h/w Buffer
+	 */
+	if (DESC_AEAD_ENC_LEN + DESC_JOB_IO_LEN +
+	    ctx->split_key_pad_len + ctx->enckeylen <=
+	    CAAM_DESC_BYTES_MAX)
+		keys_fit_inline = true;
+	else
+		keys_fit_inline = false;
+
+	/* aead_encrypt shared descriptor */
+	desc = ctx->sh_desc_enc;
+
+	init_sh_desc_key_aead(desc, ctx, keys_fit_inline);
+
+	/* Class 2 operation */
+	append_operation(desc, ctx->class2_alg_type |
+			 OP_ALG_AS_INITFINAL | OP_ALG_ENCRYPT);
+
+	/* cryptlen = seqoutlen - authsize */
+	append_math_sub_imm_u32(desc, REG3, SEQOUTLEN, IMM, ctx->authsize);
+
+	/* assoclen + cryptlen = seqinlen - ivsize */
+	append_math_sub_imm_u32(desc, REG2, SEQINLEN, IMM, tfm->ivsize);
+
+	/* assoclen + cryptlen = (assoclen + cryptlen) - cryptlen */
+	append_math_sub(desc, VARSEQINLEN, REG2, REG3, CAAM_CMD_SZ);
+
+	/* read assoc before reading payload */
+	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_CLASS2 | FIFOLD_TYPE_MSG |
+			     KEY_VLF);
+	aead_append_ld_iv(desc, tfm->ivsize);
+
+	/* Class 1 operation */
+	append_operation(desc, ctx->class1_alg_type |
+			 OP_ALG_AS_INITFINAL | OP_ALG_ENCRYPT);
+
+	/* Read and write cryptlen bytes */
+	append_math_add(desc, VARSEQINLEN, ZERO, REG3, CAAM_CMD_SZ);
+	append_math_add(desc, VARSEQOUTLEN, ZERO, REG3, CAAM_CMD_SZ);
+	aead_append_src_dst(desc, FIFOLD_TYPE_MSG1OUT2);
+
+	/* Write ICV */
+	append_seq_store(desc, ctx->authsize, LDST_CLASS_2_CCB |
+			 LDST_SRCDST_BYTE_CONTEXT);
+
+#ifdef DEBUG
+	print_hex_dump(KERN_ERR,
+		       "aead enc shdesc@"__stringify(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, desc,
+		       desc_bytes(desc), 1);
+#endif
+
+	/*
+	 * Job Descriptor and Shared Descriptors
+	 * must all fit into the 64-word Descriptor h/w Buffer
+	 */
+	if (DESC_AEAD_DEC_LEN + DESC_JOB_IO_LEN +
+	    ctx->split_key_pad_len + ctx->enckeylen <=
+	    CAAM_DESC_BYTES_MAX)
+		keys_fit_inline = true;
+	else
+		keys_fit_inline = false;
+
+	desc = ctx->sh_desc_dec;
+
+	/* aead_decrypt shared descriptor */
+	init_sh_desc(desc, HDR_SHARE_SERIAL);
+
+	/* Skip if already shared */
+	key_jump_cmd = append_jump(desc, JUMP_JSL | JUMP_TEST_ALL |
+				   JUMP_COND_SHRD);
+
+	append_key_aead(desc, ctx, keys_fit_inline);
+
+	set_jump_tgt_here(desc, key_jump_cmd);
+
+	/* Class 2 operation */
+	append_operation(desc, ctx->class2_alg_type |
+			 OP_ALG_AS_INITFINAL | OP_ALG_DECRYPT | OP_ALG_ICV_ON);
+
+	/* assoclen + cryptlen = seqinlen - ivsize */
+	append_math_sub_imm_u32(desc, REG3, SEQINLEN, IMM,
+				ctx->authsize + tfm->ivsize);
+	/* assoclen = (assoclen + cryptlen) - cryptlen */
+	append_math_sub(desc, REG2, SEQOUTLEN, REG0, CAAM_CMD_SZ);
+	append_math_sub(desc, VARSEQINLEN, REG3, REG2, CAAM_CMD_SZ);
+
+	/* read assoc before reading payload */
+	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_CLASS2 | FIFOLD_TYPE_MSG |
+			     KEY_VLF);
+
+	aead_append_ld_iv(desc, tfm->ivsize);
+
+	append_dec_op1(desc, ctx->class1_alg_type);
+
+	/* Read and write cryptlen bytes */
+	append_math_add(desc, VARSEQINLEN, ZERO, REG2, CAAM_CMD_SZ);
+	append_math_add(desc, VARSEQOUTLEN, ZERO, REG2, CAAM_CMD_SZ);
+	aead_append_src_dst(desc, FIFOLD_TYPE_MSG);
+
+	/* Load ICV */
+	append_seq_fifo_load(desc, ctx->authsize, FIFOLD_CLASS_CLASS2 |
+			     FIFOLD_TYPE_LAST2 | FIFOLD_TYPE_ICV);
+
+#ifdef DEBUG
+	print_hex_dump(KERN_ERR, "aead dec shdesc@"
+		       __stringify(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, desc,
+		       desc_bytes(desc), 1);
+#endif
+
+	/*
+	 * Job Descriptor and Shared Descriptors
+	 * must all fit into the 64-word Descriptor h/w Buffer
+	 */
+	if (DESC_AEAD_GIVENC_LEN + DESC_JOB_IO_LEN +
+	    ctx->split_key_pad_len + ctx->enckeylen <=
+	    CAAM_DESC_BYTES_MAX)
+		keys_fit_inline = true;
+	else
+		keys_fit_inline = false;
+
+	/* aead_givencrypt shared descriptor */
+	desc = ctx->sh_desc_givenc;
+
+	init_sh_desc_key_aead(desc, ctx, keys_fit_inline);
+
+	/* Generate IV */
+	geniv = NFIFOENTRY_STYPE_PAD | NFIFOENTRY_DEST_DECO |
+		NFIFOENTRY_DTYPE_MSG | NFIFOENTRY_LC1 |
+		NFIFOENTRY_PTYPE_RND | (tfm->ivsize << NFIFOENTRY_DLEN_SHIFT);
+	append_load_imm_u32(desc, geniv, LDST_CLASS_IND_CCB |
+			    LDST_SRCDST_WORD_INFO_FIFO | LDST_IMM);
+	append_cmd(desc, CMD_LOAD | DISABLE_AUTO_INFO_FIFO);
+	append_move(desc, MOVE_SRC_INFIFO |
+		    MOVE_DEST_CLASS1CTX | (tfm->ivsize << MOVE_LEN_SHIFT));
+	append_cmd(desc, CMD_LOAD | ENABLE_AUTO_INFO_FIFO);
+
+	/* Copy IV to class 1 context */
+	append_move(desc, MOVE_SRC_CLASS1CTX |
+		    MOVE_DEST_OUTFIFO | (tfm->ivsize << MOVE_LEN_SHIFT));
+
+	/* Return to encryption */
+	append_operation(desc, ctx->class2_alg_type |
+			 OP_ALG_AS_INITFINAL | OP_ALG_ENCRYPT);
+
+	/* ivsize + cryptlen = seqoutlen - authsize */
+	append_math_sub_imm_u32(desc, REG3, SEQOUTLEN, IMM, ctx->authsize);
+
+	/* assoclen = seqinlen - (ivsize + cryptlen) */
+	append_math_sub(desc, VARSEQINLEN, SEQINLEN, REG3, CAAM_CMD_SZ);
+
+	/* read assoc before reading payload */
+	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_CLASS2 | FIFOLD_TYPE_MSG |
+			     KEY_VLF);
+
+	/* Copy iv from class 1 ctx to class 2 fifo*/
+	moveiv = NFIFOENTRY_STYPE_OFIFO | NFIFOENTRY_DEST_CLASS2 |
+		 NFIFOENTRY_DTYPE_MSG | (tfm->ivsize << NFIFOENTRY_DLEN_SHIFT);
+	append_load_imm_u32(desc, moveiv, LDST_CLASS_IND_CCB |
+			    LDST_SRCDST_WORD_INFO_FIFO | LDST_IMM);
+	append_load_imm_u32(desc, tfm->ivsize, LDST_CLASS_2_CCB |
+			    LDST_SRCDST_WORD_DATASZ_REG | LDST_IMM);
+
+	/* Class 1 operation */
+	append_operation(desc, ctx->class1_alg_type |
+			 OP_ALG_AS_INITFINAL | OP_ALG_ENCRYPT);
+
+	/* Will write ivsize + cryptlen */
+	append_math_add(desc, VARSEQOUTLEN, SEQINLEN, REG0, CAAM_CMD_SZ);
+
+	/* Not need to reload iv */
+	append_seq_fifo_load(desc, tfm->ivsize,
+			     FIFOLD_CLASS_SKIP);
+
+	/* Will read cryptlen */
+	append_math_add(desc, VARSEQINLEN, SEQINLEN, REG0, CAAM_CMD_SZ);
+	aead_append_src_dst(desc, FIFOLD_TYPE_MSG1OUT2);
+
+	/* Write ICV */
+	append_seq_store(desc, ctx->authsize, LDST_CLASS_2_CCB |
+			 LDST_SRCDST_BYTE_CONTEXT);
+
+#ifdef DEBUG
+	print_hex_dump(KERN_ERR,
+		       "aead givenc shdesc@"__stringify(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, desc,
+		       desc_bytes(desc), 1);
+#endif
+
+	return 0;
+}
+
+static int aead_setauthsize(struct crypto_aead *authenc,
+				    unsigned int authsize)
+{
+	struct caam_ctx *ctx = crypto_aead_ctx(authenc);
+
+	ctx->authsize = authsize;
+	aead_set_sh_desc(authenc);
+
+	return 0;
+}
+
+static int tls_set_sh_desc(struct crypto_aead *aead)
+{
+	struct aead_tfm *tfm = &aead->base.crt_aead;
+	struct caam_ctx *ctx = crypto_aead_ctx(aead);
+	bool keys_fit_inline;
+	u32 *key_jump_cmd, *zero_payload_jump_cmd, *skip_zero_jump_cmd;
+	u32 genpad, idx_ld_datasz, idx_ld_pad, jumpback, stidx;
+	u32 *desc;
+	unsigned int blocksize = crypto_aead_blocksize(aead);
+	/* Associated data length is always = 13 for TLS */
+	unsigned int assoclen = 13;
+	/*
+	 * Pointer Size bool determines the size of address pointers.
+	 * false - Pointers fit in one 32-bit word.
+	 * true - Pointers fit in two 32-bit words.
+	 */
+	static const bool ps = (CAAM_PTR_SZ != CAAM_CMD_SZ);
+
+	if (!ctx->enckeylen || !ctx->authsize)
+		return 0;
+
+	/*
+	 * TLS 1.0 encrypt shared descriptor
+	 * Job Descriptor and Shared Descriptor
+	 * must fit into the 64-word Descriptor h/w Buffer
+	 */
+
+	/*
+	 * Compute the index (in bytes) for the LOAD with destination of
+	 * Class 1 Data Size Register and for the LOAD that generates padding
+	 */
+	if (DESC_TLS10_ENC_LEN + DESC_JOB_IO_LEN +
+	    ctx->split_key_pad_len + ctx->enckeylen <=
+	    CAAM_DESC_BYTES_MAX) {
+		keys_fit_inline = true;
+
+		idx_ld_datasz = DESC_TLS10_ENC_LEN + ctx->split_key_pad_len +
+				ctx->enckeylen - 4 * CAAM_CMD_SZ;
+		idx_ld_pad = DESC_TLS10_ENC_LEN + ctx->split_key_pad_len +
+			     ctx->enckeylen - 2 * CAAM_CMD_SZ;
+	} else {
+		keys_fit_inline = false;
+
+		idx_ld_datasz = DESC_TLS10_ENC_LEN + 2 * CAAM_PTR_SZ -
+				4 * CAAM_CMD_SZ;
+		idx_ld_pad = DESC_TLS10_ENC_LEN + 2 * CAAM_PTR_SZ -
+			     2 * CAAM_CMD_SZ;
+	}
+
+	desc = ctx->sh_desc_enc;
+
+	stidx = 1 << HDR_START_IDX_SHIFT;
+	init_sh_desc(desc, HDR_SHARE_SERIAL | stidx);
+
+	/* skip key loading if they are loaded due to sharing */
+	key_jump_cmd = append_jump(desc, JUMP_JSL | JUMP_TEST_ALL |
+				   JUMP_COND_SHRD);
+	append_key_aead(desc, ctx, keys_fit_inline);
+	set_jump_tgt_here(desc, key_jump_cmd);
+
+	/* class 2 operation */
+	append_operation(desc, ctx->class2_alg_type |
+			 OP_ALG_AS_INITFINAL | OP_ALG_ENCRYPT);
+	/* class 1 operation */
+	append_operation(desc, ctx->class1_alg_type |
+			 OP_ALG_AS_INITFINAL | OP_ALG_ENCRYPT);
+
+	/* payloadlen = input data length - (assoclen + ivlen) */
+	append_math_sub_imm_u32(desc, REG0, SEQINLEN, IMM, assoclen +
+				tfm->ivsize);
+
+	/* math1 = payloadlen + icvlen */
+	append_math_add_imm_u32(desc, REG1, REG0, IMM, ctx->authsize);
+
+	/* padlen = block_size - math1 % block_size */
+	append_math_and_imm_u32(desc, REG3, REG1, IMM, blocksize - 1);
+	append_math_sub_imm_u32(desc, REG2, IMM, REG3, blocksize);
+
+	/* cryptlen = payloadlen + icvlen + padlen */
+	append_math_add(desc, VARSEQOUTLEN, REG1, REG2, 4);
+
+	/*
+	 * update immediate data with the padding length value
+	 * for the LOAD in the class 1 data size register.
+	 */
+	append_move(desc, MOVE_SRC_DESCBUF | MOVE_DEST_MATH2 |
+			(idx_ld_datasz << MOVE_OFFSET_SHIFT) | 7);
+	append_move(desc, MOVE_WAITCOMP | MOVE_SRC_MATH2 | MOVE_DEST_DESCBUF |
+			(idx_ld_datasz << MOVE_OFFSET_SHIFT) | 8);
+
+	/* overwrite PL field for the padding iNFO FIFO entry  */
+	append_move(desc, MOVE_SRC_DESCBUF | MOVE_DEST_MATH2 |
+			(idx_ld_pad << MOVE_OFFSET_SHIFT) | 7);
+	append_move(desc, MOVE_WAITCOMP | MOVE_SRC_MATH2 | MOVE_DEST_DESCBUF |
+			(idx_ld_pad << MOVE_OFFSET_SHIFT) | 8);
+
+	/* store encrypted payload, icv and padding */
+	append_seq_fifo_store(desc, 0, FIFOST_TYPE_MESSAGE_DATA | LDST_VLF);
+
+	/* if payload length is zero, jump to zero-payload commands */
+	append_math_add(desc, VARSEQINLEN, ZERO, REG0, 4);
+	zero_payload_jump_cmd = append_jump(desc, JUMP_TEST_ALL |
+					    JUMP_COND_MATH_Z);
+
+	/* read assoc for authentication */
+	append_seq_fifo_load(desc, assoclen, FIFOLD_CLASS_CLASS2 |
+			     FIFOLD_TYPE_MSG);
+	/* load iv in context1 */
+	append_cmd(desc, CMD_SEQ_LOAD | LDST_SRCDST_WORD_CLASS_CTX |
+		   LDST_CLASS_1_CCB | tfm->ivsize);
+	/* insnoop payload */
+	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_BOTH | FIFOLD_TYPE_MSG |
+			     FIFOLD_TYPE_LAST2 | FIFOLDST_VLF);
+	/* jump the zero-payload commands */
+	append_jump(desc, JUMP_TEST_ALL | 3);
+
+	/* zero-payload commands */
+	set_jump_tgt_here(desc, zero_payload_jump_cmd);
+	/* assoc data is the only data for authentication */
+	append_seq_fifo_load(desc, assoclen, FIFOLD_CLASS_CLASS2 |
+			     FIFOLD_TYPE_MSG | FIFOLD_TYPE_LAST2);
+	/* load iv in context1 */
+	append_cmd(desc, CMD_SEQ_LOAD | LDST_SRCDST_WORD_CLASS_CTX |
+		   LDST_CLASS_1_CCB | tfm->ivsize);
+
+	/* send icv to encryption */
+	append_move(desc, MOVE_SRC_CLASS2CTX | MOVE_DEST_CLASS1INFIFO |
+		    ctx->authsize);
+
+	/* update class 1 data size register with padding length */
+	append_load_imm_u32(desc, 0, LDST_CLASS_1_CCB |
+			    LDST_SRCDST_WORD_DATASZ_REG | LDST_IMM);
+
+	/* generate padding and send it to encryption */
+	genpad = NFIFOENTRY_DEST_CLASS1 | NFIFOENTRY_LC1 | NFIFOENTRY_FC1 |
+	      NFIFOENTRY_STYPE_PAD | NFIFOENTRY_DTYPE_MSG | NFIFOENTRY_PTYPE_N;
+	append_load_imm_u32(desc, genpad, LDST_CLASS_IND_CCB |
+			    LDST_SRCDST_WORD_INFO_FIFO | LDST_IMM);
+
+#ifdef DEBUG
+	print_hex_dump(KERN_ERR, "tls enc shdesc@"__stringify(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, desc,
+		       desc_bytes(desc), 1);
+#endif
+
+	/*
+	 * TLS 1.0 decrypt shared descriptor
+	 * Keys do not fit inline, regardless of algorithms used
+	 */
+	desc = ctx->sh_desc_dec;
+
+	stidx = 1 << HDR_START_IDX_SHIFT;
+	init_sh_desc(desc, HDR_SHARE_SERIAL | stidx);
+
+	/* skip key loading if they are loaded due to sharing */
+	key_jump_cmd = append_jump(desc, JUMP_JSL | JUMP_TEST_ALL |
+				   JUMP_COND_SHRD);
+	append_key(desc, ctx->key_dma, ctx->split_key_len, CLASS_2 |
+		   KEY_DEST_MDHA_SPLIT | KEY_ENC);
+	append_key(desc, ctx->key_dma + ctx->split_key_pad_len,
+		   ctx->enckeylen, CLASS_1 | KEY_DEST_CLASS_REG);
+	set_jump_tgt_here(desc, key_jump_cmd);
+
+	/* class 2 operation */
+	append_operation(desc, ctx->class2_alg_type |
+			 OP_ALG_AS_INITFINAL | OP_ALG_DECRYPT | OP_ALG_ICV_ON);
+	/* class 1 operation */
+	append_operation(desc, ctx->class1_alg_type |
+			 OP_ALG_AS_INITFINAL | OP_ALG_DECRYPT);
+
+	/* VSIL = input data length - 2 * block_size */
+	append_math_sub_imm_u32(desc, VARSEQINLEN, SEQINLEN, IMM, 2 *
+				blocksize);
+
+	/*
+	 * payloadlen + icvlen + padlen = input data length - (assoclen +
+	 * ivsize)
+	 */
+	append_math_sub_imm_u32(desc, REG3, SEQINLEN, IMM, assoclen +
+				tfm->ivsize);
+
+	/* skip data to the last but one cipher block */
+	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_SKIP | LDST_VLF);
+
+	/* load iv for the last cipher block */
+	append_cmd(desc, CMD_SEQ_LOAD | LDST_SRCDST_WORD_CLASS_CTX |
+		   LDST_CLASS_1_CCB | tfm->ivsize);
+
+	/* read last cipher block */
+	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_CLASS1 | FIFOLD_TYPE_MSG |
+			FIFOLD_TYPE_LAST1 | blocksize);
+
+	/* move decrypted block into math0 and math1 */
+	append_move(desc, MOVE_WAITCOMP | MOVE_SRC_OUTFIFO | MOVE_DEST_MATH0 |
+		    blocksize);
+
+	/* reset AES CHA */
+	append_load_imm_u32(desc, CCTRL_RESET_CHA_AESA, LDST_CLASS_IND_CCB |
+			    LDST_SRCDST_WORD_CHACTRL | LDST_IMM);
+
+	/* rewind input sequence */
+	append_seq_in_ptr_intlen(desc, 0, 65535, SQIN_RTO);
+
+	/* key1 is in decryption form */
+	append_operation(desc, ctx->class1_alg_type | OP_ALG_AAI_DK |
+			 OP_ALG_AS_INITFINAL | OP_ALG_DECRYPT);
+
+	/* read sequence number */
+	append_seq_fifo_load(desc, 8, FIFOLD_CLASS_CLASS2 | FIFOLD_TYPE_MSG);
+	/* load Type, Version and Len fields in math0 */
+	append_cmd(desc, CMD_SEQ_LOAD | LDST_CLASS_DECO |
+		   LDST_SRCDST_WORD_DECO_MATH0 | (3 << LDST_OFFSET_SHIFT) | 5);
+
+	/* load iv in context1 */
+	append_cmd(desc, CMD_SEQ_LOAD | LDST_CLASS_1_CCB |
+		   LDST_SRCDST_WORD_CLASS_CTX | tfm->ivsize);
+
+	/* compute (padlen - 1) */
+	append_math_and_imm_u64(desc, REG1, REG1, IMM, 255);
+
+	/* math2 = icvlen + (padlen - 1) + 1 */
+	append_math_add_imm_u32(desc, REG2, REG1, IMM, ctx->authsize + 1);
+
+	append_jump(desc, JUMP_TEST_ALL | JUMP_COND_CALM | 1);
+
+	/* VSOL = payloadlen + icvlen + padlen */
+	append_math_add(desc, VARSEQOUTLEN, ZERO, REG3, 4);
+
+#ifdef __LITTLE_ENDIAN
+	append_moveb(desc, MOVE_WAITCOMP |
+		     MOVE_SRC_MATH0 | MOVE_DEST_MATH0 | 8);
+#endif
+	/* update Len field */
+	append_math_sub(desc, REG0, REG0, REG2, 8);
+
+	/* store decrypted payload, icv and padding */
+	append_seq_fifo_store(desc, 0, FIFOST_TYPE_MESSAGE_DATA | LDST_VLF);
+
+	/* VSIL = (payloadlen + icvlen + padlen) - (icvlen + padlen)*/
+	append_math_sub(desc, VARSEQINLEN, REG3, REG2, 4);
+
+	zero_payload_jump_cmd = append_jump(desc, JUMP_TEST_ALL |
+					    JUMP_COND_MATH_Z);
+
+	/* send Type, Version and Len(pre ICV) fields to authentication */
+	append_move(desc, MOVE_WAITCOMP |
+		    MOVE_SRC_MATH0 | MOVE_DEST_CLASS2INFIFO |
+		    (3 << MOVE_OFFSET_SHIFT) | 5);
+
+	/* outsnooping payload */
+	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_BOTH |
+			     FIFOLD_TYPE_MSG1OUT2 | FIFOLD_TYPE_LAST2 |
+			     FIFOLDST_VLF);
+	skip_zero_jump_cmd = append_jump(desc, JUMP_TEST_ALL | 2);
+
+	set_jump_tgt_here(desc, zero_payload_jump_cmd);
+	/* send Type, Version and Len(pre ICV) fields to authentication */
+	append_move(desc, MOVE_WAITCOMP | MOVE_AUX_LS |
+		    MOVE_SRC_MATH0 | MOVE_DEST_CLASS2INFIFO |
+		    (3 << MOVE_OFFSET_SHIFT) | 5);
+
+	set_jump_tgt_here(desc, skip_zero_jump_cmd);
+	append_math_add(desc, VARSEQINLEN, ZERO, REG2, 4);
+
+	/* load icvlen and padlen */
+	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_CLASS1 | FIFOLD_TYPE_MSG |
+			     FIFOLD_TYPE_LAST1 | FIFOLDST_VLF);
+
+	/* VSIL = (payloadlen + icvlen + padlen) - icvlen + padlen */
+	append_math_sub(desc, VARSEQINLEN, REG3, REG2, 4);
+
+	/*
+	 * Start a new input sequence using the SEQ OUT PTR command options,
+	 * pointer and length used when the current output sequence was defined.
+	 */
+	if (ps) {
+		/*
+		 * Move the lower 32 bits of Shared Descriptor address, the
+		 * SEQ OUT PTR command, Output Pointer (2 words) and
+		 * Output Length into math registers.
+		 */
+#ifdef __LITTLE_ENDIAN
+		append_move(desc, MOVE_WAITCOMP | MOVE_SRC_DESCBUF |
+			    MOVE_DEST_MATH0 | (55 * 4 << MOVE_OFFSET_SHIFT) |
+			    20);
+#else
+		append_move(desc, MOVE_WAITCOMP | MOVE_SRC_DESCBUF |
+			    MOVE_DEST_MATH0 | (54 * 4 << MOVE_OFFSET_SHIFT) |
+			    20);
+#endif
+		/* Transform SEQ OUT PTR command in SEQ IN PTR command */
+		append_math_and_imm_u32(desc, REG0, REG0, IMM,
+					~(CMD_SEQ_IN_PTR ^ CMD_SEQ_OUT_PTR));
+		/* Append a JUMP command after the copied fields */
+		jumpback = CMD_JUMP | (char)-9;
+		append_load_imm_u32(desc, jumpback, LDST_CLASS_DECO | LDST_IMM |
+				    LDST_SRCDST_WORD_DECO_MATH2 |
+				    (4 << LDST_OFFSET_SHIFT));
+		append_jump(desc, JUMP_TEST_ALL | JUMP_COND_CALM | 1);
+		/* Move the updated fields back to the Job Descriptor */
+#ifdef __LITTLE_ENDIAN
+		append_move(desc, MOVE_WAITCOMP | MOVE_SRC_MATH0 |
+			    MOVE_DEST_DESCBUF | (55 * 4 << MOVE_OFFSET_SHIFT) |
+			    24);
+#else
+		append_move(desc, MOVE_WAITCOMP | MOVE_SRC_MATH0 |
+			    MOVE_DEST_DESCBUF | (54 * 4 << MOVE_OFFSET_SHIFT) |
+			    24);
+#endif
+		/*
+		 * Read the new SEQ IN PTR command, Input Pointer, Input Length
+		 * and then jump back to the next command from the
+		 * Shared Descriptor.
+		 */
+		append_jump(desc, JUMP_TEST_ALL | JUMP_COND_CALM | 6);
+	} else {
+		/*
+		 * Move the SEQ OUT PTR command, Output Pointer (1 word) and
+		 * Output Length into math registers.
+		 */
+#ifdef __LITTLE_ENDIAN
+		append_move(desc, MOVE_WAITCOMP | MOVE_SRC_DESCBUF |
+			    MOVE_DEST_MATH0 | (54 * 4 << MOVE_OFFSET_SHIFT) |
+			    12);
+#else
+		append_move(desc, MOVE_WAITCOMP | MOVE_SRC_DESCBUF |
+			    MOVE_DEST_MATH0 | (53 * 4 << MOVE_OFFSET_SHIFT) |
+			    12);
+#endif
+		/* Transform SEQ OUT PTR command in SEQ IN PTR command */
+		append_math_and_imm_u64(desc, REG0, REG0, IMM,
+			~(((u64)(CMD_SEQ_IN_PTR ^ CMD_SEQ_OUT_PTR)) << 32));
+		/* Append a JUMP command after the copied fields */
+		jumpback = CMD_JUMP | (char)-7;
+		append_load_imm_u32(desc, jumpback, LDST_CLASS_DECO | LDST_IMM |
+				    LDST_SRCDST_WORD_DECO_MATH1 |
+				    (4 << LDST_OFFSET_SHIFT));
+		append_jump(desc, JUMP_TEST_ALL | JUMP_COND_CALM | 1);
+		/* Move the updated fields back to the Job Descriptor */
+#ifdef __LITTLE_ENDIAN
+		append_move(desc, MOVE_WAITCOMP | MOVE_SRC_MATH0 |
+			    MOVE_DEST_DESCBUF | (54 * 4 << MOVE_OFFSET_SHIFT) |
+			    16);
+#else
+		append_move(desc, MOVE_WAITCOMP | MOVE_SRC_MATH0 |
+			    MOVE_DEST_DESCBUF | (53 * 4 << MOVE_OFFSET_SHIFT) |
+			    16);
+#endif
+		/*
+		 * Read the new SEQ IN PTR command, Input Pointer, Input Length
+		 * and then jump back to the next command from the
+		 * Shared Descriptor.
+		 */
+		 append_jump(desc, JUMP_TEST_ALL | JUMP_COND_CALM | 5);
+	}
+
+	/* skip payload */
+	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_SKIP | FIFOLDST_VLF);
+	/* check icv */
+	append_seq_fifo_load(desc, 0, FIFOLD_CLASS_CLASS2 | FIFOLD_TYPE_ICV |
+			     FIFOLD_TYPE_LAST2 | ctx->authsize);
+
+#ifdef DEBUG
+	print_hex_dump(KERN_ERR, "tls dec shdesc@"__stringify(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, desc,
+		       desc_bytes(desc), 1);
+#endif
+
+	return 0;
+}
+
+static int tls_setauthsize(struct crypto_aead *tls, unsigned int authsize)
+{
+	struct caam_ctx *ctx = crypto_aead_ctx(tls);
+
+	ctx->authsize = authsize;
+
+	return 0;
+}
+
+static u32 gen_split_aead_key(struct caam_ctx *ctx, const u8 *key_in,
+			      u32 authkeylen)
+{
+	return gen_split_key(ctx->jrdev, ctx->key, ctx->split_key_len,
+			       ctx->split_key_pad_len, key_in, authkeylen,
+			       ctx->alg_op);
+}
+
+static int aead_setkey(struct crypto_aead *aead,
+			       const u8 *key, unsigned int keylen)
+{
+	/* Sizes for MDHA pads (*not* keys): MD5, SHA1, 224, 256, 384, 512 */
+	static const u8 mdpadlen[] = { 16, 20, 32, 32, 64, 64 };
+	struct caam_ctx *ctx = crypto_aead_ctx(aead);
+	struct device *jrdev = ctx->jrdev;
+	struct rtattr *rta = (void *)key;
+	struct crypto_authenc_key_param *param;
+	unsigned int authkeylen;
+	unsigned int enckeylen;
+	int ret = 0;
+
+	param = RTA_DATA(rta);
+	enckeylen = be32_to_cpu(param->enckeylen);
+
+	key += RTA_ALIGN(rta->rta_len);
+	keylen -= RTA_ALIGN(rta->rta_len);
+
+	if (keylen < enckeylen)
+		goto badkey;
+
+	authkeylen = keylen - enckeylen;
+
+	if (keylen > CAAM_MAX_KEY_SIZE)
+		goto badkey;
+
+	/* Pick class 2 key length from algorithm submask */
+	ctx->split_key_len = mdpadlen[(ctx->alg_op & OP_ALG_ALGSEL_SUBMASK) >>
+				      OP_ALG_ALGSEL_SHIFT] * 2;
+	ctx->split_key_pad_len = ALIGN(ctx->split_key_len, 16);
+
+#ifdef DEBUG
+	printk(KERN_ERR "keylen %d enckeylen %d authkeylen %d\n",
+	       keylen, enckeylen, authkeylen);
+	printk(KERN_ERR "split_key_len %d split_key_pad_len %d\n",
+	       ctx->split_key_len, ctx->split_key_pad_len);
+	print_hex_dump(KERN_ERR,
+		       "key in @"__stringify(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, key, keylen, 1);
+#endif
+
+	ret = gen_split_aead_key(ctx, key, authkeylen);
+	if (ret)
+		goto badkey;
+
+	/* postpend encryption key to auth split key */
+	memcpy(ctx->key + ctx->split_key_pad_len, key + authkeylen, enckeylen);
+
+	ctx->key_dma = dma_map_single(jrdev, ctx->key, ctx->split_key_pad_len +
+				       enckeylen, DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, ctx->key_dma)) {
+		dev_err(jrdev, "unable to map key i/o memory\n");
+		return -ENOMEM;
+	}
+#ifdef DEBUG
+	print_hex_dump(KERN_ERR, "ctx.key@"__stringify(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, ctx->key,
+		       ctx->split_key_pad_len + enckeylen, 1);
+#endif
+
+	ctx->enckeylen = enckeylen;
+
+	ret = aead_set_sh_desc(aead);
+	if (ret) {
+		dma_unmap_single(jrdev, ctx->key_dma, ctx->split_key_pad_len +
+				 enckeylen, DMA_TO_DEVICE);
+		goto badkey;
+	}
+
+	/* Now update the driver contexts with the new shared descriptor */
+	if (ctx->drv_ctx[ENCRYPT]) {
+		ret = caam_drv_ctx_update(ctx->drv_ctx[ENCRYPT],
+					  ctx->sh_desc_enc);
+		if (ret) {
+			dev_err(jrdev, "driver enc context update failed\n");
+			goto badkey;
+		}
+	}
+
+	if (ctx->drv_ctx[DECRYPT]) {
+		ret = caam_drv_ctx_update(ctx->drv_ctx[DECRYPT],
+					  ctx->sh_desc_dec);
+		if (ret) {
+			dev_err(jrdev, "driver dec context update failed\n");
+			goto badkey;
+		}
+	}
+
+	if (ctx->drv_ctx[GIVENCRYPT]) {
+		ret = caam_drv_ctx_update(ctx->drv_ctx[GIVENCRYPT],
+					  ctx->sh_desc_givenc);
+		if (ret) {
+			dev_err(jrdev, "driver givenc context update failed\n");
+			goto badkey;
+		}
+	}
+
+	return ret;
+badkey:
+	crypto_aead_set_flags(aead, CRYPTO_TFM_RES_BAD_KEY_LEN);
+	return -EINVAL;
+}
+
+
+static int tls_setkey(struct crypto_aead *aead, const u8 *key,
+					  unsigned int keylen)
+{
+	/* Sizes for MDHA pads (*not* keys): MD5, SHA1, 224, 256, 384, 512 */
+	static const u8 mdpadlen[] = { 16, 20, 32, 32, 64, 64 };
+	struct caam_ctx *ctx = crypto_aead_ctx(aead);
+	struct device *jrdev = ctx->jrdev;
+	struct rtattr *rta = (void *)key;
+	struct crypto_authenc_key_param *param;
+	unsigned int authkeylen;
+	unsigned int enckeylen;
+	int ret = 0;
+
+	param = RTA_DATA(rta);
+	enckeylen = be32_to_cpu(param->enckeylen);
+
+	key += RTA_ALIGN(rta->rta_len);
+	keylen -= RTA_ALIGN(rta->rta_len);
+
+	if (keylen < enckeylen)
+		goto badkey;
+
+	authkeylen = keylen - enckeylen;
+
+	if (keylen > CAAM_MAX_KEY_SIZE)
+		goto badkey;
+
+	/* Pick class 2 key length from algorithm submask */
+	ctx->split_key_len = mdpadlen[(ctx->alg_op & OP_ALG_ALGSEL_SUBMASK) >>
+				      OP_ALG_ALGSEL_SHIFT] * 2;
+	ctx->split_key_pad_len = ALIGN(ctx->split_key_len, 16);
+
+#ifdef DEBUG
+	dev_err(jrdev, "keylen %d enckeylen %d authkeylen %d\n", keylen,
+		enckeylen, authkeylen);
+	dev_err(jrdev, "split_key_len %d split_key_pad_len %d\n",
+		ctx->split_key_len, ctx->split_key_pad_len);
+	print_hex_dump(KERN_ERR, "key in @"__stringify(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, key, keylen, 1);
+#endif
+
+	ret = gen_split_aead_key(ctx, key, authkeylen);
+	if (ret)
+		goto badkey;
+
+	/* postpend encryption key to auth split key */
+	memcpy(ctx->key + ctx->split_key_pad_len, key + authkeylen, enckeylen);
+
+	ctx->key_dma = dma_map_single(jrdev, ctx->key, ctx->split_key_pad_len +
+				       enckeylen, DMA_TO_DEVICE);
+	if (dma_mapping_error(jrdev, ctx->key_dma)) {
+		dev_err(jrdev, "unable to map key i/o memory\n");
+		return -ENOMEM;
+	}
+#ifdef DEBUG
+	print_hex_dump(KERN_ERR, "ctx.key@"__stringify(__LINE__)": ",
+		       DUMP_PREFIX_ADDRESS, 16, 4, ctx->key,
+		       ctx->split_key_pad_len + enckeylen, 1);
+#endif
+
+	ctx->enckeylen = enckeylen;
+
+	ret = tls_set_sh_desc(aead);
+	if (ret) {
+		dma_unmap_single(jrdev, ctx->key_dma, ctx->split_key_pad_len +
+				 enckeylen, DMA_TO_DEVICE);
+	}
+
+	/* Now update the driver contexts with the new shared descriptor */
+	if (ctx->drv_ctx[ENCRYPT]) {
+		ret = caam_drv_ctx_update(ctx->drv_ctx[ENCRYPT],
+					  ctx->sh_desc_enc);
+		if (ret) {
+			dev_err(jrdev, "driver enc context update failed\n");
+			goto badkey;
+		}
+	}
+
+	if (ctx->drv_ctx[DECRYPT]) {
+		ret = caam_drv_ctx_update(ctx->drv_ctx[DECRYPT],
+					  ctx->sh_desc_dec);
+		if (ret) {
+			dev_err(jrdev, "driver dec context update failed\n");
+			goto badkey;
+		}
+	}
+
+	if (ctx->drv_ctx[GIVENCRYPT]) {
+		ret = caam_drv_ctx_update(ctx->drv_ctx[GIVENCRYPT],
+					  ctx->sh_desc_givenc);
+		if (ret) {
+			dev_err(jrdev, "driver givenc context update failed\n");
+			goto badkey;
+		}
+	}
+
+	return ret;
+badkey:
+	crypto_aead_set_flags(aead, CRYPTO_TFM_RES_BAD_KEY_LEN);
+	return -EINVAL;
+}
+
+/*
+ * aead_edesc - s/w-extended aead descriptor
+ * @assoc_nents: number of segments in associated data (SPI+Seq) scatterlist
+ * @assoc_chained: if source is chained
+ * @src_nents: number of segments in input scatterlist
+ * @src_chained: if source is chained
+ * @dst_nents: number of segments in output scatterlist
+ * @dst_chained: if destination is chained
+ * @iv_dma: dma address of iv for checking continuity and link table
+ * @desc: h/w descriptor (variable length; must not exceed MAX_CAAM_DESCSIZE)
+ * @qm_sg_bytes: length of dma mapped sec4_sg space
+ * @qm_sg_dma: bus physical mapped address of h/w link table
+ * @hw_desc: the h/w job descriptor followed by any referenced link tables
+ */
+struct aead_edesc {
+	int assoc_nents;
+	bool assoc_chained;
+	int src_nents;
+	bool src_chained;
+	int dst_nents;
+	bool dst_chained;
+	dma_addr_t iv_dma;
+	int qm_sg_bytes;
+	dma_addr_t qm_sg_dma;
+	struct caam_drv_req drv_req;
+	struct qm_sg_entry sgt[0];
+};
+
+
+static void caam_unmap(struct device *dev, struct scatterlist *src,
+		       struct scatterlist *dst, int src_nents,
+		       bool src_chained, int dst_nents, bool dst_chained,
+		       dma_addr_t iv_dma, int ivsize, dma_addr_t qm_sg_dma,
+		       int qm_sg_bytes)
+{
+	if (dst != src) {
+		dma_unmap_sg_chained(dev, src, src_nents ? : 1, DMA_TO_DEVICE,
+				     src_chained);
+		dma_unmap_sg_chained(dev, dst, dst_nents ? : 1, DMA_FROM_DEVICE,
+				     dst_chained);
+	} else {
+		dma_unmap_sg_chained(dev, src, src_nents ? : 1,
+				     DMA_BIDIRECTIONAL, src_chained);
+	}
+
+	if (iv_dma)
+		dma_unmap_single(dev, iv_dma, ivsize, DMA_TO_DEVICE);
+	if (qm_sg_bytes)
+		dma_unmap_single(dev, qm_sg_dma, qm_sg_bytes, DMA_BIDIRECTIONAL);
+}
+
+static void aead_unmap(struct device *dev,
+		       struct aead_edesc *edesc,
+		       struct aead_request *req)
+{
+	struct crypto_aead *aead = crypto_aead_reqtfm(req);
+	int ivsize = crypto_aead_ivsize(aead);
+
+	dma_unmap_sg_chained(dev, req->assoc, edesc->assoc_nents,
+			     DMA_BIDIRECTIONAL, edesc->assoc_chained);
+
+	caam_unmap(dev, req->src, req->dst,
+		   edesc->src_nents, edesc->src_chained, edesc->dst_nents,
+		   edesc->dst_chained, edesc->iv_dma, ivsize,
+		   edesc->qm_sg_dma, edesc->qm_sg_bytes);
+}
+
+static void aead_done(struct caam_drv_req *drv_req, u32 status)
+{
+	struct device *qidev;
+	struct aead_edesc *edesc;
+	struct aead_request *aead_req = drv_req->app_ctx;
+	struct crypto_aead *aead = crypto_aead_reqtfm(aead_req);
+	struct caam_ctx *caam_ctx = crypto_aead_ctx(aead);
+	int ecode = 0;
+
+	qidev = caam_ctx->qidev;
+
+	if (unlikely(status)) {
+		caam_jr_strstatus(qidev, status);
+		ecode = -EIO;
+	}
+
+	edesc = container_of(drv_req, struct aead_edesc, drv_req);
+	aead_unmap(qidev, edesc, aead_req);
+
+	aead_request_complete(aead_req, ecode);
+	qi_cache_free(edesc);
+}
+
+/* For now, identical to aead_done */
+static inline void tls_encrypt_done(struct caam_drv_req *drv_req, u32 status)
+{
+	struct device *qidev;
+	struct aead_edesc *edesc;
+	struct aead_request *aead_req = drv_req->app_ctx;
+	struct crypto_aead *aead = crypto_aead_reqtfm(aead_req);
+	struct caam_ctx *caam_ctx = crypto_aead_ctx(aead);
+	int ecode = 0;
+
+	qidev = caam_ctx->qidev;
+
+	if (status) {
+		caam_jr_strstatus(qidev, status);
+		ecode = -EIO;
+	}
+
+	edesc = container_of(drv_req, struct aead_edesc, drv_req);
+	aead_unmap(qidev, edesc, aead_req);
+
+	aead_request_complete(aead_req, ecode);
+	qi_cache_free(edesc);
+}
+
+static inline void tls_decrypt_done(struct caam_drv_req *drv_req, u32 status)
+{
+	struct device *qidev;
+	struct aead_edesc *edesc;
+	struct aead_request *aead_req = drv_req->app_ctx;
+	struct crypto_aead *aead = crypto_aead_reqtfm(aead_req);
+	struct caam_ctx *caam_ctx = crypto_aead_ctx(aead);
+	int ecode = 0;
+	int cryptlen = aead_req->cryptlen;
+	u8 padsize;
+	u8 padding[255]; /* padding can be 0-255 bytes */
+	int i;
+
+	qidev = caam_ctx->qidev;
+
+	if (status) {
+		caam_jr_strstatus(qidev, status);
+		ecode = -EIO;
+	}
+
+	edesc = container_of(drv_req, struct aead_edesc, drv_req);
+	aead_unmap(qidev, edesc, aead_req);
+
+	/*
+	 * verify hw auth check passed else return -EBADMSG
+	 */
+	if ((status & JRSTA_CCBERR_ERRID_MASK) == JRSTA_CCBERR_ERRID_ICVCHK) {
+		ecode = -EBADMSG;
+		goto out;
+	}
+
+	/* Padding checking */
+	cryptlen -= 1;
+	scatterwalk_map_and_copy(&padsize, aead_req->dst, cryptlen, 1, 0);
+	if (padsize > cryptlen) {
+		ecode = -EBADMSG;
+		goto out;
+	}
+	cryptlen -= padsize;
+	scatterwalk_map_and_copy(padding, aead_req->dst, cryptlen, padsize, 0);
+	/* the padding content must be equal with padsize */
+	for (i = 0; i < padsize; i++)
+		if (padding[i] != padsize) {
+			ecode = -EBADMSG;
+			break;
+		}
+
+out:
+	aead_request_complete(aead_req, ecode);
+	qi_cache_free(edesc);
+}
+
+/*
+ * allocate and map the aead extended descriptor
+ */
+static struct aead_edesc *aead_edesc_alloc(struct aead_request *req,
+					   bool encrypt, bool strip_icv)
+{
+	struct crypto_aead *aead = crypto_aead_reqtfm(req);
+	struct caam_ctx *ctx = crypto_aead_ctx(aead);
+	struct device *qidev = ctx->qidev;
+	gfp_t flags = (req->base.flags & (CRYPTO_TFM_REQ_MAY_BACKLOG |
+		       CRYPTO_TFM_REQ_MAY_SLEEP)) ? GFP_KERNEL : GFP_ATOMIC;
+	int assoc_nents, src_nents, dst_nents = 0;
+	struct aead_edesc *edesc;
+	dma_addr_t iv_dma = 0, qm_sg_dma;
+	int sgc;
+	bool all_contig = true;
+	bool assoc_chained = false, src_chained = false, dst_chained = false;
+	int ivsize = crypto_aead_ivsize(aead);
+	unsigned int authsize = ctx->authsize;
+
+	int qm_sg_index, qm_sg_ents = 0, qm_sg_bytes;
+	struct qm_sg_entry *sg_table, *fd_sgt;
+	struct caam_drv_req *drv_req;
+	bool src_is_dst = true;
+
+	assoc_nents = sg_count(req->assoc, req->assoclen, &assoc_chained);
+
+	if (likely(req->dst == req->src)) {
+		src_nents = sg_count(req->src,
+				     req->cryptlen +
+					(encrypt ? authsize : 0),
+				     &src_chained);
+		sgc = dma_map_sg_chained(qidev, req->src, src_nents ? : 1,
+					 DMA_BIDIRECTIONAL, src_chained);
+	} else {
+		int extralen;
+		src_is_dst = false;
+		src_nents = sg_count(req->src, req->cryptlen, &src_chained);
+
+		if (encrypt)
+			extralen = authsize;
+		else
+			extralen = strip_icv ? (-authsize) : 0;
+		dst_nents = sg_count(req->dst, req->cryptlen + extralen,
+				     &dst_chained);
+
+		sgc = dma_map_sg_chained(qidev, req->src, src_nents ? : 1,
+					 DMA_TO_DEVICE, src_chained);
+		sgc = dma_map_sg_chained(qidev, req->dst, dst_nents ? : 1,
+					 DMA_FROM_DEVICE, dst_chained);
+
+	}
+
+	sgc = dma_map_sg_chained(qidev, req->assoc, assoc_nents ? : 1,
+				 DMA_TO_DEVICE, assoc_chained);
+
+	/* Check if data are contiguous */
+	iv_dma = dma_map_single(qidev, req->iv, ivsize, DMA_TO_DEVICE);
+	if (assoc_nents ||
+	    sg_dma_address(req->assoc) + req->assoclen != iv_dma ||
+	    src_nents || iv_dma + ivsize != sg_dma_address(req->src)) {
+		all_contig = false;
+		assoc_nents = assoc_nents ? : 1;
+		src_nents = src_nents ? : 1;
+		qm_sg_ents = assoc_nents + 1 + src_nents;
+	}
+
+	qm_sg_ents += dst_nents;
+	qm_sg_bytes = qm_sg_ents * sizeof(struct qm_sg_entry);
+
+	/* allocate space for base edesc and hw desc commands, link tables */
+	edesc = qi_cache_alloc(GFP_DMA | flags);
+	if (unlikely(!edesc)) {
+		dev_err(qidev, "could not allocate extended descriptor\n");
+		return ERR_PTR(-ENOMEM);
+	}
+
+	qm_sg_index = 0;
+	drv_req = &edesc->drv_req;
+	sg_table = &edesc->sgt[0];
+	fd_sgt = &drv_req->fd_sgt[0];
+
+	qm_sg_dma = dma_map_single(qidev, sg_table,
+				qm_sg_bytes, DMA_BIDIRECTIONAL);
+
+	edesc->assoc_nents = assoc_nents;
+	edesc->assoc_chained = assoc_chained;
+	edesc->src_nents = src_nents;
+	edesc->src_chained = src_chained;
+	edesc->dst_nents = dst_nents;
+	edesc->dst_chained = dst_chained;
+	edesc->iv_dma = iv_dma;
+	edesc->qm_sg_dma = qm_sg_dma;
+	edesc->qm_sg_bytes = qm_sg_bytes;
+
+	fd_sgt[0].final = 0;
+	fd_sgt[0].__reserved2 = 0;
+	fd_sgt[0].bpid = 0;
+	fd_sgt[0].__reserved3 = 0;
+	fd_sgt[0].offset = 0;
+
+	fd_sgt[1].final = 1;
+	fd_sgt[1].__reserved2 = 0;
+	fd_sgt[1].bpid = 0;
+	fd_sgt[1].__reserved3 = 0;
+	fd_sgt[1].offset = 0;
+
+	if (!all_contig) {
+		fd_sgt[1].extension = 1;
+		fd_sgt[1].addr = qm_sg_dma;
+
+		sg_to_qm_sg(req->assoc, assoc_nents, sg_table, 0);
+		qm_sg_index += assoc_nents;
+
+		dma_to_qm_sg_one(sg_table + qm_sg_index, iv_dma, ivsize, 0);
+		qm_sg_index += 1;
+
+		sg_to_qm_sg_last(req->src, src_nents,
+				 sg_table + qm_sg_index, 0);
+		qm_sg_index += src_nents;
+
+	} else {
+		fd_sgt[1].extension = 0;
+		fd_sgt[1].addr = sg_dma_address(req->assoc);
+	}
+
+	if (dst_nents)
+		sg_to_qm_sg_last(req->dst, dst_nents,
+				 sg_table + qm_sg_index, 0);
+
+	if (likely(src_is_dst)) {
+		if (src_nents <= 1) {
+			fd_sgt[0].addr = sg_dma_address(req->src);
+			fd_sgt[0].extension = 0;
+		} else {
+			fd_sgt[0].extension = 1;
+			fd_sgt[0].addr = fd_sgt[1].addr +
+				sizeof(struct qm_sg_entry) *
+					((edesc->assoc_nents ? : 1) + 1);
+		}
+	} else {
+		if (!dst_nents) {
+			fd_sgt[0].addr = sg_dma_address(req->dst);
+			fd_sgt[0].extension = 0;
+		} else {
+			fd_sgt[0].addr = qm_sg_dma +
+				(sizeof(struct qm_sg_entry) * qm_sg_index);
+			fd_sgt[0].extension = 1;
+		}
+	}
+
+	return edesc;
+}
+
+static struct caam_drv_ctx *get_drv_ctx(struct caam_ctx *ctx,
+					enum optype type)
+{
+	/* This function is called on the fast path with values of 'type'
+	 * known at compile time. Invalid arguments are not expected and
+	 * thus no checks are made */
+	struct caam_drv_ctx *drv_ctx = ctx->drv_ctx[type];
+	u32 *desc;
+
+	if (unlikely(!drv_ctx)) {
+		spin_lock(&ctx->lock);
+
+		/* Read again to check if some other core init drv_ctx */
+		drv_ctx = ctx->drv_ctx[type];
+		if (!drv_ctx) {
+			int cpu;
+
+			if (ENCRYPT == type)
+				desc = ctx->sh_desc_enc;
+			else if (DECRYPT == type)
+				desc = ctx->sh_desc_dec;
+			else /* (GIVENCRYPT == type) */
+				desc = ctx->sh_desc_givenc;
+
+			cpu = smp_processor_id();
+			drv_ctx = caam_drv_ctx_init(ctx->qidev, &cpu, desc);
+
+			ctx->drv_ctx[type] = drv_ctx;
+		}
+
+		spin_unlock(&ctx->lock);
+	}
+
+	return drv_ctx;
+}
+
+static int aead_encrypt(struct aead_request *req)
+{
+	struct aead_edesc *edesc;
+	struct crypto_aead *aead = crypto_aead_reqtfm(req);
+	struct caam_ctx *ctx = crypto_aead_ctx(aead);
+	int ivsize = crypto_aead_ivsize(aead);
+	struct device *qidev = ctx->qidev;
+	struct caam_drv_ctx *drv_ctx;
+	struct caam_drv_req *drv_req;
+	int ret;
+
+	drv_ctx = get_drv_ctx(ctx, ENCRYPT);
+	if (unlikely(IS_ERR_OR_NULL(drv_ctx)))
+		return PTR_ERR(drv_ctx);
+
+	if (unlikely(caam_drv_ctx_busy(drv_ctx)))
+		return -EAGAIN;
+
+	/* allocate extended descriptor */
+	edesc = aead_edesc_alloc(req, true, true);
+	if (IS_ERR(edesc))
+		return PTR_ERR(edesc);
+
+	/* Create and submit job descriptor */
+	drv_req = &edesc->drv_req;
+	drv_req->app_ctx = req;
+	drv_req->cbk = aead_done;
+	drv_req->fd_sgt[0].length = req->cryptlen + ctx->authsize;
+	drv_req->fd_sgt[1].length = req->assoclen + ivsize + req->cryptlen;
+
+	drv_req->drv_ctx = drv_ctx;
+	ret = caam_qi_enqueue(qidev, drv_req);
+	if (!ret) {
+		ret = -EINPROGRESS;
+	} else {
+		aead_unmap(qidev, edesc, req);
+		qi_cache_free(edesc);
+	}
+
+	return ret;
+}
+
+static int aead_decrypt(struct aead_request *req)
+{
+	struct aead_edesc *edesc;
+	struct crypto_aead *aead = crypto_aead_reqtfm(req);
+	struct caam_ctx *ctx = crypto_aead_ctx(aead);
+	int ivsize = crypto_aead_ivsize(aead);
+	struct device *qidev = ctx->qidev;
+	struct caam_drv_ctx *drv_ctx;
+	struct caam_drv_req *drv_req;
+	int ret = 0;
+
+	drv_ctx = get_drv_ctx(ctx, DECRYPT);
+	if (unlikely(IS_ERR_OR_NULL(drv_ctx)))
+		return PTR_ERR(drv_ctx);
+
+	if (unlikely(caam_drv_ctx_busy(drv_ctx)))
+		return -EAGAIN;
+
+	/* allocate extended descriptor */
+	edesc = aead_edesc_alloc(req, false, true);
+	if (IS_ERR(edesc))
+		return PTR_ERR(edesc);
+
+	/* Create and submit job descriptor */
+	drv_req = &edesc->drv_req;
+	drv_req->app_ctx = req;
+	drv_req->cbk = aead_done;
+	drv_req->fd_sgt[0].length = req->cryptlen - ctx->authsize;
+	drv_req->fd_sgt[1].length = req->assoclen + ivsize + req->cryptlen;
+
+	drv_req->drv_ctx = drv_ctx;
+	ret = caam_qi_enqueue(qidev, drv_req);
+	if (!ret) {
+		ret = -EINPROGRESS;
+	} else {
+		aead_unmap(qidev, edesc, req);
+		qi_cache_free(edesc);
+	}
+
+	return ret;
+}
+
+static int tls_encrypt(struct aead_request *req)
+{
+	struct aead_edesc *edesc;
+	struct crypto_aead *aead = crypto_aead_reqtfm(req);
+	struct caam_ctx *ctx = crypto_aead_ctx(aead);
+	int ivsize = crypto_aead_ivsize(aead);
+	struct device *qidev = ctx->qidev;
+	struct caam_drv_ctx *drv_ctx;
+	struct caam_drv_req *drv_req;
+	int ret;
+	unsigned int blocksize = crypto_aead_blocksize(aead);
+	unsigned int padsize;
+
+	drv_ctx = get_drv_ctx(ctx, ENCRYPT);
+	if (unlikely(IS_ERR_OR_NULL(drv_ctx)))
+		return PTR_ERR(drv_ctx);
+
+	if (unlikely(caam_drv_ctx_busy(drv_ctx)))
+		return -EAGAIN;
+
+	padsize = blocksize - ((req->cryptlen + ctx->authsize) % blocksize);
+
+	/*
+	 * allocate extended tls descriptor
+	 * TLS 1.0 has no explicit IV in the packet, but it is needed as input
+	 * since it is used by CBC.
+	 * ctx->authsize is temporary set to include also padlen
+	 */
+	ctx->authsize += padsize;
+	edesc = aead_edesc_alloc(req, true, true);
+	if (IS_ERR(edesc))
+		return PTR_ERR(edesc);
+	ctx->authsize -= padsize;
+
+	/* Create and submit job descriptor */
+	drv_req = &edesc->drv_req;
+	drv_req->app_ctx = req;
+	drv_req->cbk = tls_encrypt_done;
+	drv_req->fd_sgt[0].length = req->cryptlen + padsize + ctx->authsize;
+	drv_req->fd_sgt[1].length = req->assoclen + ivsize + req->cryptlen;
+
+	drv_req->drv_ctx = drv_ctx;
+	ret = caam_qi_enqueue(qidev, drv_req);
+	if (!ret) {
+		ret = -EINPROGRESS;
+	} else {
+		aead_unmap(qidev, edesc, req);
+		qi_cache_free(edesc);
+	}
+
+	return ret;
+}
+
+static int tls_decrypt(struct aead_request *req)
+{
+	struct aead_edesc *edesc;
+	struct crypto_aead *aead = crypto_aead_reqtfm(req);
+	struct caam_ctx *ctx = crypto_aead_ctx(aead);
+	int ivsize = crypto_aead_ivsize(aead);
+	struct device *qidev = ctx->qidev;
+	struct caam_drv_ctx *drv_ctx;
+	struct caam_drv_req *drv_req;
+	int ret = 0;
+
+	drv_ctx = get_drv_ctx(ctx, DECRYPT);
+	if (unlikely(IS_ERR_OR_NULL(drv_ctx)))
+		return PTR_ERR(drv_ctx);
+
+	if (unlikely(caam_drv_ctx_busy(drv_ctx)))
+		return -EAGAIN;
+
+	/*
+	 * allocate extended descriptor
+	 * TLS 1.0 has no explicit IV in the packet, but it is needed as input
+	 * since it is used by CBC.
+	 * Assumption: since padding and ICV are not stripped (upper layer
+	 * checks padding), req->dst has to be big enough to hold payloadlen +
+	 * padlen + icvlen.
+	 */
+	edesc = aead_edesc_alloc(req, false, false);
+	if (IS_ERR(edesc))
+		return PTR_ERR(edesc);
+
+	/* Create and submit job descriptor */
+	drv_req = &edesc->drv_req;
+	drv_req->app_ctx = req;
+	drv_req->cbk = tls_decrypt_done;
+	/*
+	 * For decrypt, do not strip ICV, Padding, Padding length since
+	 * upper layer(s) perform padding checking.
+	 */
+	drv_req->fd_sgt[0].length = req->cryptlen;
+	drv_req->fd_sgt[1].length = req->assoclen + ivsize + req->cryptlen;
+
+	drv_req->drv_ctx = drv_ctx;
+	ret = caam_qi_enqueue(qidev, drv_req);
+	if (!ret) {
+		ret = -EINPROGRESS;
+	} else {
+		aead_unmap(qidev, edesc, req);
+		qi_cache_free(edesc);
+	}
+
+	return ret;
+}
+
+/*
+ * allocate and map the aead extended descriptor for aead givencrypt
+ */
+static struct aead_edesc *aead_giv_edesc_alloc(struct aead_givcrypt_request
+					       *greq)
+{
+	struct aead_request *req = &greq->areq;
+	struct crypto_aead *aead = crypto_aead_reqtfm(req);
+	struct caam_ctx *ctx = crypto_aead_ctx(aead);
+	struct device *qidev = ctx->qidev;
+	gfp_t flags = (req->base.flags & (CRYPTO_TFM_REQ_MAY_BACKLOG |
+		       CRYPTO_TFM_REQ_MAY_SLEEP)) ? GFP_KERNEL : GFP_ATOMIC;
+	int assoc_nents, src_nents, dst_nents = 0;
+	struct aead_edesc *edesc;
+	dma_addr_t iv_dma = 0, qm_sg_dma;
+	int sgc;
+	u32 contig = GIV_SRC_CONTIG | GIV_DST_CONTIG;
+	int ivsize = crypto_aead_ivsize(aead);
+	bool assoc_chained = false, src_chained = false, dst_chained = false;
+
+	int qm_sg_index, qm_sg_ents = 0, qm_sg_bytes;
+	struct qm_sg_entry *sg_table, *fd_sgt;
+	struct caam_drv_req *drv_req;
+
+	bool src_is_dst = true;
+
+	assoc_nents = sg_count(req->assoc, req->assoclen, &assoc_chained);
+	src_nents = sg_count(req->src, req->cryptlen, &src_chained);
+	sgc = dma_map_sg_chained(qidev, req->assoc, assoc_nents ? : 1,
+				 DMA_TO_DEVICE, assoc_chained);
+
+	if (likely(req->src == req->dst)) {
+		sgc = dma_map_sg_chained(qidev, req->src, src_nents ? : 1,
+					 DMA_BIDIRECTIONAL, src_chained);
+	} else {
+		src_is_dst = false;
+
+		dst_nents = sg_count(req->dst, req->cryptlen + ctx->authsize,
+				     &dst_chained);
+
+		sgc = dma_map_sg_chained(qidev, req->src, src_nents ? : 1,
+					 DMA_TO_DEVICE, src_chained);
+		sgc = dma_map_sg_chained(qidev, req->dst, dst_nents ? : 1,
+					 DMA_FROM_DEVICE, dst_chained);
+	}
+
+	/* Check if data are contiguous */
+	iv_dma = dma_map_single(qidev, greq->giv, ivsize, DMA_TO_DEVICE);
+
+	if (assoc_nents ||
+	    sg_dma_address(req->assoc) + req->assoclen != iv_dma ||
+	    src_nents || iv_dma + ivsize != sg_dma_address(req->src))
+		contig &= ~GIV_SRC_CONTIG;
+
+	if (dst_nents || iv_dma + ivsize != sg_dma_address(req->dst))
+		contig &= ~GIV_DST_CONTIG;
+
+	if (unlikely(!src_is_dst)) {
+		dst_nents = dst_nents ? : 1;
+		qm_sg_ents += 1;
+	}
+
+	if (!(contig & GIV_SRC_CONTIG)) {
+		assoc_nents = assoc_nents ? : 1;
+		src_nents = src_nents ? : 1;
+		qm_sg_ents += assoc_nents + 1 + src_nents;
+		if (likely(src_is_dst))
+			contig &= ~GIV_DST_CONTIG;
+	}
+
+	qm_sg_ents += dst_nents;
+
+	qm_sg_bytes = qm_sg_ents * sizeof(struct qm_sg_entry);
+
+	/* allocate space for base edesc and hw desc commands, link tables */
+	edesc = qi_cache_alloc(GFP_DMA | flags);
+	if (unlikely(!edesc)) {
+		dev_err(qidev, "could not allocate extended descriptor\n");
+		return ERR_PTR(-ENOMEM);
+	}
+
+	drv_req = &edesc->drv_req;
+	sg_table = &edesc->sgt[0];
+	fd_sgt = &drv_req->fd_sgt[0];
+
+	qm_sg_dma = dma_map_single(qidev, sg_table,
+				qm_sg_bytes, DMA_BIDIRECTIONAL);
+
+	edesc->assoc_nents = assoc_nents;
+	edesc->assoc_chained = assoc_chained;
+	edesc->src_nents = src_nents;
+	edesc->src_chained = src_chained;
+	edesc->dst_nents = dst_nents;
+	edesc->dst_chained = dst_chained;
+	edesc->iv_dma = iv_dma;
+	edesc->qm_sg_bytes = qm_sg_bytes;
+	edesc->qm_sg_dma = qm_sg_dma;
+
+	fd_sgt[0].final = 0;
+	fd_sgt[0].extension = 0;
+	fd_sgt[0].__reserved2 = 0;
+	fd_sgt[0].bpid = 0;
+	fd_sgt[0].__reserved3 = 0;
+	fd_sgt[0].offset = 0;
+
+	fd_sgt[1].final = 1;
+	fd_sgt[1].extension = 0;
+	fd_sgt[1].__reserved2 = 0;
+	fd_sgt[1].bpid = 0;
+	fd_sgt[1].__reserved3 = 0;
+	fd_sgt[1].offset = 0;
+
+	qm_sg_index = 0;
+	if (unlikely(!(contig & GIV_SRC_CONTIG))) {
+		fd_sgt[1].extension = 1;
+		fd_sgt[1].addr = qm_sg_dma;
+
+		sg_to_qm_sg(req->assoc, assoc_nents,
+			    sg_table + qm_sg_index, 0);
+
+		qm_sg_index += assoc_nents;
+
+		dma_to_qm_sg_one(sg_table + qm_sg_index,
+				 iv_dma, ivsize, 0);
+
+		qm_sg_index += 1;
+
+		sg_to_qm_sg_last(req->src, src_nents,
+				 sg_table + qm_sg_index, 0);
+
+		qm_sg_index += src_nents;
+	} else {
+		fd_sgt[1].addr = sg_dma_address(req->assoc);
+	}
+
+	if (unlikely(!src_is_dst && !(contig & GIV_DST_CONTIG))) {
+		fd_sgt[0].addr = qm_sg_dma +
+				(sizeof(struct qm_sg_entry) * qm_sg_index);
+		fd_sgt[0].extension = 1;
+
+		dma_to_qm_sg_one(sg_table + qm_sg_index, iv_dma, ivsize, 0);
+		qm_sg_index += 1;
+		sg_to_qm_sg_last(req->dst, dst_nents,
+				 sg_table + qm_sg_index, 0);
+	} else {
+		if (src_is_dst && !(contig & GIV_DST_CONTIG)) {
+			fd_sgt[0].extension = 1;
+			fd_sgt[0].addr = edesc->qm_sg_dma +
+					sizeof(struct qm_sg_entry) *
+					edesc->assoc_nents;
+		} else {
+			fd_sgt[0].addr = edesc->iv_dma;
+		}
+	}
+
+	return edesc;
+}
+
+static int aead_givencrypt(struct aead_givcrypt_request *areq)
+{
+	struct aead_request *req = &areq->areq;
+	struct crypto_aead *aead = crypto_aead_reqtfm(req);
+	struct caam_ctx *ctx = crypto_aead_ctx(aead);
+	struct device *qidev = ctx->qidev;
+	struct caam_drv_ctx *drv_ctx;
+	struct caam_drv_req *drv_req;
+	int ivsize = crypto_aead_ivsize(aead);
+	struct aead_edesc *edesc;
+	int ret;
+
+	drv_ctx = get_drv_ctx(ctx, GIVENCRYPT);
+	if (unlikely(IS_ERR_OR_NULL(drv_ctx)))
+		return PTR_ERR(drv_ctx);
+
+	if (unlikely(caam_drv_ctx_busy(drv_ctx)))
+		return -EAGAIN;
+
+	/* allocate extended descriptor */
+	edesc = aead_giv_edesc_alloc(areq);
+	if (IS_ERR(edesc))
+		return PTR_ERR(edesc);
+
+	drv_req = &edesc->drv_req;
+	drv_req->app_ctx = req;
+	drv_req->cbk = aead_done;
+	drv_req->fd_sgt[0].length = ivsize + req->cryptlen + ctx->authsize;
+	drv_req->fd_sgt[1].length = req->assoclen + ivsize + req->cryptlen;
+
+	drv_req->drv_ctx = drv_ctx;
+	ret = caam_qi_enqueue(qidev, drv_req);
+	if (!ret) {
+		ret = -EINPROGRESS;
+	} else {
+		aead_unmap(qidev, edesc, req);
+		qi_cache_free(edesc);
+	}
+
+	return ret;
+}
+
+#define template_aead		template_u.aead
+#define template_ablkcipher	template_u.ablkcipher
+struct caam_alg_template {
+	char name[CRYPTO_MAX_ALG_NAME];
+	char driver_name[CRYPTO_MAX_ALG_NAME];
+	unsigned int blocksize;
+	u32 type;
+	union {
+		struct ablkcipher_alg ablkcipher;
+		struct aead_alg aead;
+		struct blkcipher_alg blkcipher;
+		struct cipher_alg cipher;
+		struct compress_alg compress;
+		struct rng_alg rng;
+	} template_u;
+	u32 class1_alg_type;
+	u32 class2_alg_type;
+	u32 alg_op;
+	int min_era;
+};
+
+static struct caam_alg_template driver_algs[] = {
+	/* single-pass ipsec_esp descriptor */
+	{
+		.name = "authenc(hmac(md5),cbc(aes))",
+		.driver_name = "authenc-hmac-md5-cbc-aes-caam-qi",
+		.blocksize = AES_BLOCK_SIZE,
+		.type = CRYPTO_ALG_TYPE_AEAD,
+		.template_aead = {
+			.setkey = aead_setkey,
+			.setauthsize = aead_setauthsize,
+			.encrypt = aead_encrypt,
+			.decrypt = aead_decrypt,
+			.givencrypt = aead_givencrypt,
+			.geniv = "<built-in>",
+			.ivsize = AES_BLOCK_SIZE,
+			.maxauthsize = MD5_DIGEST_SIZE,
+			},
+		.class1_alg_type = OP_ALG_ALGSEL_AES | OP_ALG_AAI_CBC,
+		.class2_alg_type = OP_ALG_ALGSEL_MD5 | OP_ALG_AAI_HMAC_PRECOMP,
+		.alg_op = OP_ALG_ALGSEL_MD5 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
+	},
+	{
+		.name = "authenc(hmac(sha1),cbc(aes))",
+		.driver_name = "authenc-hmac-sha1-cbc-aes-caam-qi",
+		.blocksize = AES_BLOCK_SIZE,
+		.type = CRYPTO_ALG_TYPE_AEAD,
+		.template_aead = {
+			.setkey = aead_setkey,
+			.setauthsize = aead_setauthsize,
+			.encrypt = aead_encrypt,
+			.decrypt = aead_decrypt,
+			.givencrypt = aead_givencrypt,
+			.geniv = "<built-in>",
+			.ivsize = AES_BLOCK_SIZE,
+			.maxauthsize = SHA1_DIGEST_SIZE,
+			},
+		.class1_alg_type = OP_ALG_ALGSEL_AES | OP_ALG_AAI_CBC,
+		.class2_alg_type = OP_ALG_ALGSEL_SHA1 | OP_ALG_AAI_HMAC_PRECOMP,
+		.alg_op = OP_ALG_ALGSEL_SHA1 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
+	},
+	{
+		.name = "authenc(hmac(sha224),cbc(aes))",
+		.driver_name = "authenc-hmac-sha224-cbc-aes-caam-qi",
+		.blocksize = AES_BLOCK_SIZE,
+		.type = CRYPTO_ALG_TYPE_AEAD,
+		.template_aead = {
+			.setkey = aead_setkey,
+			.setauthsize = aead_setauthsize,
+			.encrypt = aead_encrypt,
+			.decrypt = aead_decrypt,
+			.givencrypt = aead_givencrypt,
+			.geniv = "<built-in>",
+			.ivsize = AES_BLOCK_SIZE,
+			.maxauthsize = SHA224_DIGEST_SIZE,
+			},
+		.class1_alg_type = OP_ALG_ALGSEL_AES | OP_ALG_AAI_CBC,
+		.class2_alg_type = OP_ALG_ALGSEL_SHA224 |
+				   OP_ALG_AAI_HMAC_PRECOMP,
+		.alg_op = OP_ALG_ALGSEL_SHA224 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
+	},
+	{
+		.name = "authenc(hmac(sha256),cbc(aes))",
+		.driver_name = "authenc-hmac-sha256-cbc-aes-caam-qi",
+		.blocksize = AES_BLOCK_SIZE,
+		.type = CRYPTO_ALG_TYPE_AEAD,
+		.template_aead = {
+			.setkey = aead_setkey,
+			.setauthsize = aead_setauthsize,
+			.encrypt = aead_encrypt,
+			.decrypt = aead_decrypt,
+			.givencrypt = aead_givencrypt,
+			.geniv = "<built-in>",
+			.ivsize = AES_BLOCK_SIZE,
+			.maxauthsize = SHA256_DIGEST_SIZE,
+			},
+		.class1_alg_type = OP_ALG_ALGSEL_AES | OP_ALG_AAI_CBC,
+		.class2_alg_type = OP_ALG_ALGSEL_SHA256 |
+				   OP_ALG_AAI_HMAC_PRECOMP,
+		.alg_op = OP_ALG_ALGSEL_SHA256 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
+	},
+	{
+		.name = "authenc(hmac(sha384),cbc(aes))",
+		.driver_name = "authenc-hmac-sha384-cbc-aes-caam-qi",
+		.blocksize = AES_BLOCK_SIZE,
+		.type = CRYPTO_ALG_TYPE_AEAD,
+		.template_aead = {
+			.setkey = aead_setkey,
+			.setauthsize = aead_setauthsize,
+			.encrypt = aead_encrypt,
+			.decrypt = aead_decrypt,
+			.givencrypt = aead_givencrypt,
+			.geniv = "<built-in>",
+			.ivsize = AES_BLOCK_SIZE,
+			.maxauthsize = SHA384_DIGEST_SIZE,
+			},
+		.class1_alg_type = OP_ALG_ALGSEL_AES | OP_ALG_AAI_CBC,
+		.class2_alg_type = OP_ALG_ALGSEL_SHA384 |
+				   OP_ALG_AAI_HMAC_PRECOMP,
+		.alg_op = OP_ALG_ALGSEL_SHA384 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
+	},
+
+	{
+		.name = "authenc(hmac(sha512),cbc(aes))",
+		.driver_name = "authenc-hmac-sha512-cbc-aes-caam-qi",
+		.blocksize = AES_BLOCK_SIZE,
+		.type = CRYPTO_ALG_TYPE_AEAD,
+		.template_aead = {
+			.setkey = aead_setkey,
+			.setauthsize = aead_setauthsize,
+			.encrypt = aead_encrypt,
+			.decrypt = aead_decrypt,
+			.givencrypt = aead_givencrypt,
+			.geniv = "<built-in>",
+			.ivsize = AES_BLOCK_SIZE,
+			.maxauthsize = SHA512_DIGEST_SIZE,
+			},
+		.class1_alg_type = OP_ALG_ALGSEL_AES | OP_ALG_AAI_CBC,
+		.class2_alg_type = OP_ALG_ALGSEL_SHA512 |
+				   OP_ALG_AAI_HMAC_PRECOMP,
+		.alg_op = OP_ALG_ALGSEL_SHA512 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
+	},
+	{
+		.name = "authenc(hmac(md5),cbc(des3_ede))",
+		.driver_name = "authenc-hmac-md5-cbc-des3_ede-caam-qi",
+		.blocksize = DES3_EDE_BLOCK_SIZE,
+		.type = CRYPTO_ALG_TYPE_AEAD,
+		.template_aead = {
+			.setkey = aead_setkey,
+			.setauthsize = aead_setauthsize,
+			.encrypt = aead_encrypt,
+			.decrypt = aead_decrypt,
+			.givencrypt = aead_givencrypt,
+			.geniv = "<built-in>",
+			.ivsize = DES3_EDE_BLOCK_SIZE,
+			.maxauthsize = MD5_DIGEST_SIZE,
+			},
+		.class1_alg_type = OP_ALG_ALGSEL_3DES | OP_ALG_AAI_CBC,
+		.class2_alg_type = OP_ALG_ALGSEL_MD5 | OP_ALG_AAI_HMAC_PRECOMP,
+		.alg_op = OP_ALG_ALGSEL_MD5 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
+	},
+	{
+		.name = "authenc(hmac(sha1),cbc(des3_ede))",
+		.driver_name = "authenc-hmac-sha1-cbc-des3_ede-caam-qi",
+		.blocksize = DES3_EDE_BLOCK_SIZE,
+		.type = CRYPTO_ALG_TYPE_AEAD,
+		.template_aead = {
+			.setkey = aead_setkey,
+			.setauthsize = aead_setauthsize,
+			.encrypt = aead_encrypt,
+			.decrypt = aead_decrypt,
+			.givencrypt = aead_givencrypt,
+			.geniv = "<built-in>",
+			.ivsize = DES3_EDE_BLOCK_SIZE,
+			.maxauthsize = SHA1_DIGEST_SIZE,
+			},
+		.class1_alg_type = OP_ALG_ALGSEL_3DES | OP_ALG_AAI_CBC,
+		.class2_alg_type = OP_ALG_ALGSEL_SHA1 | OP_ALG_AAI_HMAC_PRECOMP,
+		.alg_op = OP_ALG_ALGSEL_SHA1 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
+	},
+	{
+		.name = "authenc(hmac(sha224),cbc(des3_ede))",
+		.driver_name = "authenc-hmac-sha224-cbc-des3_ede-caam-qi",
+		.blocksize = DES3_EDE_BLOCK_SIZE,
+		.type = CRYPTO_ALG_TYPE_AEAD,
+		.template_aead = {
+			.setkey = aead_setkey,
+			.setauthsize = aead_setauthsize,
+			.encrypt = aead_encrypt,
+			.decrypt = aead_decrypt,
+			.givencrypt = aead_givencrypt,
+			.geniv = "<built-in>",
+			.ivsize = DES3_EDE_BLOCK_SIZE,
+			.maxauthsize = SHA224_DIGEST_SIZE,
+			},
+		.class1_alg_type = OP_ALG_ALGSEL_3DES | OP_ALG_AAI_CBC,
+		.class2_alg_type = OP_ALG_ALGSEL_SHA224 |
+				   OP_ALG_AAI_HMAC_PRECOMP,
+		.alg_op = OP_ALG_ALGSEL_SHA224 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
+	},
+	{
+		.name = "authenc(hmac(sha256),cbc(des3_ede))",
+		.driver_name = "authenc-hmac-sha256-cbc-des3_ede-caam-qi",
+		.blocksize = DES3_EDE_BLOCK_SIZE,
+		.type = CRYPTO_ALG_TYPE_AEAD,
+		.template_aead = {
+			.setkey = aead_setkey,
+			.setauthsize = aead_setauthsize,
+			.encrypt = aead_encrypt,
+			.decrypt = aead_decrypt,
+			.givencrypt = aead_givencrypt,
+			.geniv = "<built-in>",
+			.ivsize = DES3_EDE_BLOCK_SIZE,
+			.maxauthsize = SHA256_DIGEST_SIZE,
+			},
+		.class1_alg_type = OP_ALG_ALGSEL_3DES | OP_ALG_AAI_CBC,
+		.class2_alg_type = OP_ALG_ALGSEL_SHA256 |
+				   OP_ALG_AAI_HMAC_PRECOMP,
+		.alg_op = OP_ALG_ALGSEL_SHA256 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
+	},
+	{
+		.name = "authenc(hmac(sha384),cbc(des3_ede))",
+		.driver_name = "authenc-hmac-sha384-cbc-des3_ede-caam-qi",
+		.blocksize = DES3_EDE_BLOCK_SIZE,
+		.type = CRYPTO_ALG_TYPE_AEAD,
+		.template_aead = {
+			.setkey = aead_setkey,
+			.setauthsize = aead_setauthsize,
+			.encrypt = aead_encrypt,
+			.decrypt = aead_decrypt,
+			.givencrypt = aead_givencrypt,
+			.geniv = "<built-in>",
+			.ivsize = DES3_EDE_BLOCK_SIZE,
+			.maxauthsize = SHA384_DIGEST_SIZE,
+			},
+		.class1_alg_type = OP_ALG_ALGSEL_3DES | OP_ALG_AAI_CBC,
+		.class2_alg_type = OP_ALG_ALGSEL_SHA384 |
+				   OP_ALG_AAI_HMAC_PRECOMP,
+		.alg_op = OP_ALG_ALGSEL_SHA384 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
+	},
+	{
+		.name = "authenc(hmac(sha512),cbc(des3_ede))",
+		.driver_name = "authenc-hmac-sha512-cbc-des3_ede-caam-qi",
+		.blocksize = DES3_EDE_BLOCK_SIZE,
+		.type = CRYPTO_ALG_TYPE_AEAD,
+		.template_aead = {
+			.setkey = aead_setkey,
+			.setauthsize = aead_setauthsize,
+			.encrypt = aead_encrypt,
+			.decrypt = aead_decrypt,
+			.givencrypt = aead_givencrypt,
+			.geniv = "<built-in>",
+			.ivsize = DES3_EDE_BLOCK_SIZE,
+			.maxauthsize = SHA512_DIGEST_SIZE,
+			},
+		.class1_alg_type = OP_ALG_ALGSEL_3DES | OP_ALG_AAI_CBC,
+		.class2_alg_type = OP_ALG_ALGSEL_SHA512 |
+				   OP_ALG_AAI_HMAC_PRECOMP,
+		.alg_op = OP_ALG_ALGSEL_SHA512 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
+	},
+	{
+		.name = "authenc(hmac(md5),cbc(des))",
+		.driver_name = "authenc-hmac-md5-cbc-des-caam-qi",
+		.blocksize = DES_BLOCK_SIZE,
+		.type = CRYPTO_ALG_TYPE_AEAD,
+		.template_aead = {
+			.setkey = aead_setkey,
+			.setauthsize = aead_setauthsize,
+			.encrypt = aead_encrypt,
+			.decrypt = aead_decrypt,
+			.givencrypt = aead_givencrypt,
+			.geniv = "<built-in>",
+			.ivsize = DES_BLOCK_SIZE,
+			.maxauthsize = MD5_DIGEST_SIZE,
+			},
+		.class1_alg_type = OP_ALG_ALGSEL_DES | OP_ALG_AAI_CBC,
+		.class2_alg_type = OP_ALG_ALGSEL_MD5 | OP_ALG_AAI_HMAC_PRECOMP,
+		.alg_op = OP_ALG_ALGSEL_MD5 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
+	},
+	{
+		.name = "authenc(hmac(sha1),cbc(des))",
+		.driver_name = "authenc-hmac-sha1-cbc-des-caam-qi",
+		.blocksize = DES_BLOCK_SIZE,
+		.type = CRYPTO_ALG_TYPE_AEAD,
+		.template_aead = {
+			.setkey = aead_setkey,
+			.setauthsize = aead_setauthsize,
+			.encrypt = aead_encrypt,
+			.decrypt = aead_decrypt,
+			.givencrypt = aead_givencrypt,
+			.geniv = "<built-in>",
+			.ivsize = DES_BLOCK_SIZE,
+			.maxauthsize = SHA1_DIGEST_SIZE,
+			},
+		.class1_alg_type = OP_ALG_ALGSEL_DES | OP_ALG_AAI_CBC,
+		.class2_alg_type = OP_ALG_ALGSEL_SHA1 | OP_ALG_AAI_HMAC_PRECOMP,
+		.alg_op = OP_ALG_ALGSEL_SHA1 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
+	},
+	{
+		.name = "authenc(hmac(sha224),cbc(des))",
+		.driver_name = "authenc-hmac-sha224-cbc-des-caam-qi",
+		.blocksize = DES_BLOCK_SIZE,
+		.type = CRYPTO_ALG_TYPE_AEAD,
+		.template_aead = {
+			.setkey = aead_setkey,
+			.setauthsize = aead_setauthsize,
+			.encrypt = aead_encrypt,
+			.decrypt = aead_decrypt,
+			.givencrypt = aead_givencrypt,
+			.geniv = "<built-in>",
+			.ivsize = DES_BLOCK_SIZE,
+			.maxauthsize = SHA224_DIGEST_SIZE,
+			},
+		.class1_alg_type = OP_ALG_ALGSEL_DES | OP_ALG_AAI_CBC,
+		.class2_alg_type = OP_ALG_ALGSEL_SHA224 |
+				   OP_ALG_AAI_HMAC_PRECOMP,
+		.alg_op = OP_ALG_ALGSEL_SHA224 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
+	},
+	{
+		.name = "authenc(hmac(sha256),cbc(des))",
+		.driver_name = "authenc-hmac-sha256-cbc-des-caam-qi",
+		.blocksize = DES_BLOCK_SIZE,
+		.type = CRYPTO_ALG_TYPE_AEAD,
+		.template_aead = {
+			.setkey = aead_setkey,
+			.setauthsize = aead_setauthsize,
+			.encrypt = aead_encrypt,
+			.decrypt = aead_decrypt,
+			.givencrypt = aead_givencrypt,
+			.geniv = "<built-in>",
+			.ivsize = DES_BLOCK_SIZE,
+			.maxauthsize = SHA256_DIGEST_SIZE,
+			},
+		.class1_alg_type = OP_ALG_ALGSEL_DES | OP_ALG_AAI_CBC,
+		.class2_alg_type = OP_ALG_ALGSEL_SHA256 |
+				   OP_ALG_AAI_HMAC_PRECOMP,
+		.alg_op = OP_ALG_ALGSEL_SHA256 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
+	},
+	{
+		.name = "authenc(hmac(sha384),cbc(des))",
+		.driver_name = "authenc-hmac-sha384-cbc-des-caam-qi",
+		.blocksize = DES_BLOCK_SIZE,
+		.type = CRYPTO_ALG_TYPE_AEAD,
+		.template_aead = {
+			.setkey = aead_setkey,
+			.setauthsize = aead_setauthsize,
+			.encrypt = aead_encrypt,
+			.decrypt = aead_decrypt,
+			.givencrypt = aead_givencrypt,
+			.geniv = "<built-in>",
+			.ivsize = DES_BLOCK_SIZE,
+			.maxauthsize = SHA384_DIGEST_SIZE,
+			},
+		.class1_alg_type = OP_ALG_ALGSEL_DES | OP_ALG_AAI_CBC,
+		.class2_alg_type = OP_ALG_ALGSEL_SHA384 |
+				   OP_ALG_AAI_HMAC_PRECOMP,
+		.alg_op = OP_ALG_ALGSEL_SHA384 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
+	},
+	{
+		.name = "authenc(hmac(sha512),cbc(des))",
+		.driver_name = "authenc-hmac-sha512-cbc-des-caam-qi",
+		.blocksize = DES_BLOCK_SIZE,
+		.type = CRYPTO_ALG_TYPE_AEAD,
+		.template_aead = {
+			.setkey = aead_setkey,
+			.setauthsize = aead_setauthsize,
+			.encrypt = aead_encrypt,
+			.decrypt = aead_decrypt,
+			.givencrypt = aead_givencrypt,
+			.geniv = "<built-in>",
+			.ivsize = DES_BLOCK_SIZE,
+			.maxauthsize = SHA512_DIGEST_SIZE,
+			},
+		.class1_alg_type = OP_ALG_ALGSEL_DES | OP_ALG_AAI_CBC,
+		.class2_alg_type = OP_ALG_ALGSEL_SHA512 |
+				   OP_ALG_AAI_HMAC_PRECOMP,
+		.alg_op = OP_ALG_ALGSEL_SHA512 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
+	},
+	/* TLS record descriptors */
+	{
+		.name = "tls10(hmac(sha1),cbc(aes))",
+		.driver_name = "tls10-hmac-sha1-cbc-aes-caam-qi",
+		.blocksize = AES_BLOCK_SIZE,
+		.type = CRYPTO_ALG_TYPE_AEAD,
+		.template_aead = {
+			.setkey = tls_setkey,
+			.setauthsize = tls_setauthsize,
+			.encrypt = tls_encrypt,
+			.decrypt = tls_decrypt,
+			.givencrypt = NULL,
+			.geniv = "<built-in>",
+			.ivsize = AES_BLOCK_SIZE,
+			.maxauthsize = SHA1_DIGEST_SIZE,
+			},
+		.class1_alg_type = OP_ALG_ALGSEL_AES | OP_ALG_AAI_CBC,
+		.class2_alg_type = OP_ALG_ALGSEL_SHA1 | OP_ALG_AAI_HMAC_PRECOMP,
+		.alg_op = OP_ALG_ALGSEL_SHA1 | OP_ALG_AAI_HMAC,
+		.min_era = 2,
+	}
+};
+
+struct caam_crypto_alg {
+	struct list_head entry;
+	struct device *ctrldev;
+	int class1_alg_type;
+	int class2_alg_type;
+	int alg_op;
+	struct crypto_alg crypto_alg;
+};
+
+static int caam_cra_init(struct crypto_tfm *tfm)
+{
+	struct crypto_alg *alg = tfm->__crt_alg;
+	struct caam_crypto_alg *caam_alg =
+		 container_of(alg, struct caam_crypto_alg, crypto_alg);
+	struct caam_ctx *ctx = crypto_tfm_ctx(tfm);
+	struct caam_drv_private *priv = dev_get_drvdata(caam_alg->ctrldev);
+	/* Digest sizes for MD5, SHA1, SHA-224, SHA-256, SHA-384, SHA-512 */
+	static const u8 digest_size[] = {
+		MD5_DIGEST_SIZE,
+		SHA1_DIGEST_SIZE,
+		SHA224_DIGEST_SIZE,
+		SHA256_DIGEST_SIZE,
+		SHA384_DIGEST_SIZE,
+		SHA512_DIGEST_SIZE
+	};
+	u8 op_id;
+
+	/*
+	 * distribute tfms across job rings to ensure in-order
+	 * crypto request processing per tfm
+	 */
+	ctx->jrdev = caam_jr_alloc();
+	if (IS_ERR(ctx->jrdev)) {
+		pr_err("Job Ring Device allocation for transform failed\n");
+		return PTR_ERR(ctx->jrdev);
+	}
+
+	/* copy descriptor header template value */
+	ctx->class1_alg_type = OP_TYPE_CLASS1_ALG | caam_alg->class1_alg_type;
+	ctx->class2_alg_type = OP_TYPE_CLASS2_ALG | caam_alg->class2_alg_type;
+	ctx->alg_op = OP_TYPE_CLASS2_ALG | caam_alg->alg_op;
+
+	/*
+	 * Need authsize, in case setauthsize callback not called
+	 * by upper layer (e.g. TLS).
+	 */
+	if (caam_alg->alg_op) {
+		op_id = (ctx->alg_op & OP_ALG_ALGSEL_SUBMASK)
+				>> OP_ALG_ALGSEL_SHIFT;
+		if (op_id < ARRAY_SIZE(digest_size)) {
+			ctx->authsize = digest_size[op_id];
+		} else {
+			dev_err(ctx->jrdev,
+				"incorrect op_id %d; must be less than %zu\n",
+				op_id, ARRAY_SIZE(digest_size));
+			caam_jr_free(ctx->jrdev);
+			return -EINVAL;
+		}
+	} else {
+		ctx->authsize = 0;
+	}
+
+	ctx->qidev = priv->qidev;
+
+	spin_lock_init(&ctx->lock);
+	ctx->drv_ctx[ENCRYPT] = NULL;
+	ctx->drv_ctx[DECRYPT] = NULL;
+	ctx->drv_ctx[GIVENCRYPT] = NULL;
+
+	return 0;
+}
+
+static void caam_cra_exit(struct crypto_tfm *tfm)
+{
+	struct caam_ctx *ctx = crypto_tfm_ctx(tfm);
+
+	caam_drv_ctx_rel(ctx->drv_ctx[ENCRYPT]);
+	caam_drv_ctx_rel(ctx->drv_ctx[DECRYPT]);
+	caam_drv_ctx_rel(ctx->drv_ctx[GIVENCRYPT]);
+
+	caam_jr_free(ctx->jrdev);
+}
+
+static struct list_head alg_list;
+static void __exit caam_qi_algapi_exit(void)
+{
+	struct caam_crypto_alg *t_alg, *n;
+
+	if (!alg_list.next)
+		return;
+
+	list_for_each_entry_safe(t_alg, n, &alg_list, entry) {
+		crypto_unregister_alg(&t_alg->crypto_alg);
+		list_del(&t_alg->entry);
+		kfree(t_alg);
+	}
+}
+
+static struct caam_crypto_alg *caam_alg_alloc(struct device *ctrldev,
+					      struct caam_alg_template
+					      *template)
+{
+	struct caam_crypto_alg *t_alg;
+	struct crypto_alg *alg;
+
+	t_alg = kzalloc(sizeof(struct caam_crypto_alg), GFP_KERNEL);
+	if (!t_alg) {
+		dev_err(ctrldev, "failed to allocate t_alg\n");
+		return ERR_PTR(-ENOMEM);
+	}
+
+	alg = &t_alg->crypto_alg;
+
+	snprintf(alg->cra_name, CRYPTO_MAX_ALG_NAME, "%s", template->name);
+	snprintf(alg->cra_driver_name, CRYPTO_MAX_ALG_NAME, "%s",
+		 template->driver_name);
+	alg->cra_module = THIS_MODULE;
+	alg->cra_init = caam_cra_init;
+	alg->cra_exit = caam_cra_exit;
+	alg->cra_priority = CAAM_CRA_PRIORITY;
+	alg->cra_blocksize = template->blocksize;
+	alg->cra_alignmask = 0;
+	alg->cra_ctxsize = sizeof(struct caam_ctx);
+	alg->cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_KERN_DRIVER_ONLY |
+			 template->type;
+	switch (template->type) {
+	case CRYPTO_ALG_TYPE_ABLKCIPHER:
+		alg->cra_type = &crypto_ablkcipher_type;
+		alg->cra_ablkcipher = template->template_ablkcipher;
+		break;
+	case CRYPTO_ALG_TYPE_AEAD:
+		alg->cra_type = &crypto_aead_type;
+		alg->cra_aead = template->template_aead;
+		break;
+	}
+
+	t_alg->class1_alg_type = template->class1_alg_type;
+	t_alg->class2_alg_type = template->class2_alg_type;
+	t_alg->alg_op = template->alg_op;
+	t_alg->ctrldev = ctrldev;
+
+	return t_alg;
+}
+
+static int __init caam_qi_algapi_init(void)
+{
+	struct device_node *dev_node;
+	struct platform_device *pdev;
+	struct device *ctrldev;
+	struct caam_drv_private *priv;
+	int i = 0, err = 0;
+
+	dev_node = of_find_compatible_node(NULL, NULL, "fsl,sec-v4.0");
+	if (!dev_node) {
+		dev_node = of_find_compatible_node(NULL, NULL, "fsl,sec4.0");
+		if (!dev_node)
+			return -ENODEV;
+	}
+
+	pdev = of_find_device_by_node(dev_node);
+	if (!pdev)
+		return -ENODEV;
+
+	ctrldev = &pdev->dev;
+	priv = dev_get_drvdata(ctrldev);
+	/*
+	* If priv is NULL, it's probably because the caam driver wasn't
+	* properly initialized (e.g. RNG4 init failed). Thus, bail out here.
+	*/
+	if (!priv)
+		return -ENODEV;
+	of_node_put(dev_node);
+
+	INIT_LIST_HEAD(&alg_list);
+
+	/* register crypto algorithms the device supports */
+	for (i = 0; i < ARRAY_SIZE(driver_algs); i++) {
+		struct caam_crypto_alg *t_alg;
+
+		/* check if h/w supports alg */
+		if (priv->era > 0 && priv->era < driver_algs[i].min_era) {
+			dev_warn(priv->qidev, "%s needs Era %d or higher but SEC is Era %d, skipping it\n",
+				 driver_algs[i].driver_name,
+				 driver_algs[i].min_era, priv->era);
+			continue;
+		}
+
+		t_alg = caam_alg_alloc(ctrldev, &driver_algs[i]);
+		if (IS_ERR(t_alg)) {
+			err = PTR_ERR(t_alg);
+			dev_warn(priv->qidev, "%s alg allocation failed\n",
+				 driver_algs[i].driver_name);
+			continue;
+		}
+
+		err = crypto_register_alg(&t_alg->crypto_alg);
+		if (err) {
+			dev_warn(priv->qidev, "%s alg registration failed\n",
+				 t_alg->crypto_alg.cra_driver_name);
+			kfree(t_alg);
+		} else {
+			list_add_tail(&t_alg->entry, &alg_list);
+		}
+	}
+
+	if (!list_empty(&alg_list))
+		dev_info(priv->qidev, "%s algorithms registered in /proc/crypto\n",
+			 (char *)of_get_property(dev_node, "compatible", NULL));
+
+	return err;
+}
+
+module_init(caam_qi_algapi_init);
+module_exit(caam_qi_algapi_exit);
+
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("Support for crypto API using CAAM-QI backend");
+MODULE_AUTHOR("Freescale Semiconductor - NMG/STC");
diff --git a/drivers/crypto/caam/caamhash.c b/drivers/crypto/caam/caamhash.c
index d97a03d..4a1b622 100644
--- a/drivers/crypto/caam/caamhash.c
+++ b/drivers/crypto/caam/caamhash.c
@@ -1143,6 +1143,7 @@ static int ahash_final_no_ctx(struct ahash_request *req)
 	edesc->dst_dma = map_seq_out_ptr_result(desc, jrdev, req->result,
 						digestsize);
 	edesc->src_nents = 0;
+	edesc->sec4_sg_bytes = 0;
 
 #ifdef DEBUG
 	print_hex_dump(KERN_ERR, "jobdesc@"__stringify(__LINE__)": ",
@@ -1685,7 +1686,8 @@ static int caam_hash_cra_init(struct crypto_tfm *tfm)
 					 HASH_MSG_LEN + SHA256_DIGEST_SIZE,
 					 HASH_MSG_LEN + 64,
 					 HASH_MSG_LEN + SHA512_DIGEST_SIZE };
-	int ret = 0;
+	int ret;
+	u8 op_id;
 
 	/*
 	 * Get a Job ring from Job Ring driver to ensure in-order
@@ -1700,14 +1702,24 @@ static int caam_hash_cra_init(struct crypto_tfm *tfm)
 	ctx->alg_type = OP_TYPE_CLASS2_ALG | caam_hash->alg_type;
 	ctx->alg_op = OP_TYPE_CLASS2_ALG | caam_hash->alg_op;
 
-	ctx->ctx_len = runninglen[(ctx->alg_op & OP_ALG_ALGSEL_SUBMASK) >>
-				  OP_ALG_ALGSEL_SHIFT];
+	op_id = (ctx->alg_op & OP_ALG_ALGSEL_SUBMASK) >> OP_ALG_ALGSEL_SHIFT;
+	if (op_id >= ARRAY_SIZE(runninglen)) {
+		dev_err(ctx->jrdev, "incorrect op_id %d; must be less than %zu\n",
+				op_id, ARRAY_SIZE(runninglen));
+		ret = -EINVAL;
+		goto out_err;
+	}
+	ctx->ctx_len = runninglen[op_id];
 
 	crypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),
 				 sizeof(struct caam_hash_state));
 
 	ret = ahash_set_sh_desc(ahash);
+	if (ret == 0)
+		return ret;
 
+out_err:
+	caam_jr_free(ctx->jrdev);
 	return ret;
 }
 
diff --git a/drivers/crypto/caam/caampkc.c b/drivers/crypto/caam/caampkc.c
new file mode 100644
index 0000000..e9944a4
--- /dev/null
+++ b/drivers/crypto/caam/caampkc.c
@@ -0,0 +1,1520 @@
+/*
+ * \file - caampkc.c
+ * \brief - Freescale FSL CAAM support for Public Key Cryptography
+ *
+ * Author: Yashpal Dutta <yashpal.dutta@freescale.com>
+ *
+ * Copyright 2012 Freescale Semiconductor, Inc.
+ *
+ * There is no shared descriptor for PKC but Job descriptor must carry
+ * all the desired key parameters, input and output pointers
+ *
+ */
+
+#include "pkc_desc.h"
+
+/* PKC Priority */
+#define CAAM_PKC_PRIORITY 3000
+
+#ifdef DEBUG
+/* for print_hex_dumps with line references */
+#define debug(format, arg...) pr_debug(format, arg)
+#else
+#define debug(format, arg...)
+#endif
+
+/* Internal context of CAAM driver. May carry session specific
+     PKC information like key */
+struct caam_pkc_context_s {
+	/* Job Ring Device pointer for current request */
+	struct device *dev;
+};
+
+struct caam_pkc_alg {
+	struct list_head entry;
+	struct device *ctrldev;
+	struct crypto_alg crypto_alg;
+};
+
+static void rsa_unmap(struct device *dev,
+		      struct rsa_edesc *edesc, struct pkc_request *req)
+{
+	switch (req->type) {
+	case RSA_PUB:
+		{
+			struct rsa_pub_req_s *pub_req = &req->req_u.rsa_pub_req;
+			struct rsa_pub_desc_s *rsa_pub_desc =
+			    (struct rsa_pub_desc_s *)edesc->hw_desc;
+
+			dma_unmap_single(dev, rsa_pub_desc->n_dma,
+					 pub_req->n_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, rsa_pub_desc->e_dma,
+					 pub_req->e_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, rsa_pub_desc->g_dma,
+					 pub_req->g_len, DMA_FROM_DEVICE);
+			dma_unmap_single(dev, rsa_pub_desc->f_dma,
+					 pub_req->f_len, DMA_TO_DEVICE);
+			break;
+		}
+	case RSA_PRIV_FORM1:
+		{
+			struct rsa_priv_frm1_req_s *priv_req =
+			    &req->req_u.rsa_priv_f1;
+			struct rsa_priv_frm1_desc_s *rsa_priv_desc =
+			    (struct rsa_priv_frm1_desc_s *)edesc->hw_desc;
+
+			dma_unmap_single(dev, rsa_priv_desc->n_dma,
+					 priv_req->n_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, rsa_priv_desc->d_dma,
+					 priv_req->d_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, rsa_priv_desc->f_dma,
+					 priv_req->f_len, DMA_FROM_DEVICE);
+			dma_unmap_single(dev, rsa_priv_desc->g_dma,
+					 priv_req->g_len, DMA_TO_DEVICE);
+			break;
+		}
+	case RSA_PRIV_FORM2:
+		{
+			struct rsa_priv_frm2_req_s *priv_req =
+			    &req->req_u.rsa_priv_f2;
+			struct rsa_priv_frm2_desc_s *rsa_priv_desc =
+			    (struct rsa_priv_frm2_desc_s *)edesc->hw_desc;
+
+			dma_unmap_single(dev, rsa_priv_desc->p_dma,
+					 priv_req->p_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, rsa_priv_desc->q_dma,
+					 priv_req->q_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, rsa_priv_desc->d_dma,
+					 priv_req->d_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, rsa_priv_desc->g_dma,
+					 priv_req->g_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, rsa_priv_desc->f_dma,
+					 priv_req->f_len, DMA_FROM_DEVICE);
+			dma_unmap_single(dev,
+					 edesc->dma_u.rsa_priv_f2_edesc.
+					 tmp1_dma, priv_req->p_len,
+					 DMA_BIDIRECTIONAL);
+			dma_unmap_single(dev,
+					 edesc->dma_u.rsa_priv_f2_edesc.
+					 tmp2_dma, priv_req->q_len,
+					 DMA_BIDIRECTIONAL);
+			kfree(edesc->dma_u.rsa_priv_f2_edesc.tmp1);
+			kfree(edesc->dma_u.rsa_priv_f2_edesc.tmp2);
+			break;
+		}
+	case RSA_PRIV_FORM3:
+		{
+			struct rsa_priv_frm3_req_s *priv_req =
+			    &req->req_u.rsa_priv_f3;
+			struct rsa_priv_frm3_desc_s *rsa_priv_desc =
+			    (struct rsa_priv_frm3_desc_s *)edesc->hw_desc;
+
+			dma_unmap_single(dev, rsa_priv_desc->p_dma,
+					 priv_req->p_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, rsa_priv_desc->q_dma,
+					 priv_req->q_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, rsa_priv_desc->dq_dma,
+					 priv_req->dq_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, rsa_priv_desc->dp_dma,
+					 priv_req->dp_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, rsa_priv_desc->c_dma,
+					 priv_req->c_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, rsa_priv_desc->g_dma,
+					 priv_req->g_len, DMA_TO_DEVICE);
+			dma_unmap_single(dev, rsa_priv_desc->f_dma,
+					 priv_req->f_len, DMA_FROM_DEVICE);
+			dma_unmap_single(dev,
+					 edesc->dma_u.rsa_priv_f3_edesc.
+					 tmp1_dma, priv_req->p_len,
+					 DMA_BIDIRECTIONAL);
+			dma_unmap_single(dev,
+					 edesc->dma_u.rsa_priv_f3_edesc.
+					 tmp2_dma, priv_req->q_len,
+					 DMA_BIDIRECTIONAL);
+			kfree(edesc->dma_u.rsa_priv_f3_edesc.tmp1);
+			kfree(edesc->dma_u.rsa_priv_f3_edesc.tmp2);
+			break;
+		}
+	default:
+		dev_err(dev, "Unable to find request type\n");
+	}
+}
+
+/* RSA Job Completion handler */
+static void rsa_op_done(struct device *dev, u32 *desc, u32 err, void *context)
+{
+	struct pkc_request *req = context;
+	struct rsa_edesc *edesc;
+
+	edesc = (struct rsa_edesc *)((char *)desc -
+				     offsetof(struct rsa_edesc, hw_desc));
+
+	if (err)
+		caam_jr_strstatus(dev, err);
+
+	rsa_unmap(dev, edesc, req);
+	kfree(edesc);
+
+	pkc_request_complete(req, err);
+}
+
+static void dh_unmap(struct device *dev,
+		      struct dh_edesc_s *edesc, struct pkc_request *req)
+{
+	struct dh_key_req_s *dh_req = &req->req_u.dh_req;
+	struct dh_key_desc_s *dh_desc =
+	    (struct dh_key_desc_s *)edesc->hw_desc;
+	dma_unmap_single(dev, dh_desc->q_dma,
+			 dh_req->q_len, DMA_TO_DEVICE);
+	dma_unmap_single(dev, dh_desc->w_dma,
+			 dh_req->pub_key_len, DMA_TO_DEVICE);
+	dma_unmap_single(dev, dh_desc->s_dma,
+			 dh_req->s_len, DMA_TO_DEVICE);
+	dma_unmap_single(dev, dh_desc->z_dma,
+			 dh_req->z_len, DMA_FROM_DEVICE);
+	if (edesc->req_type == ECDH_COMPUTE_KEY)
+		dma_unmap_single(dev, dh_desc->ab_dma,
+				 dh_req->ab_len, DMA_TO_DEVICE);
+}
+
+static void dsa_unmap(struct device *dev,
+		       struct dsa_edesc_s *edesc, struct pkc_request *req)
+{
+	switch (req->type) {
+	case DSA_SIGN:
+	case ECDSA_SIGN:
+	{
+		struct dsa_sign_req_s *dsa_req = &req->req_u.dsa_sign;
+		struct dsa_sign_desc_s *dsa_desc =
+		    (struct dsa_sign_desc_s *)edesc->hw_desc;
+		dma_unmap_single(dev, dsa_desc->q_dma,
+				 dsa_req->q_len, DMA_TO_DEVICE);
+		dma_unmap_single(dev, dsa_desc->r_dma,
+				 dsa_req->r_len, DMA_TO_DEVICE);
+		dma_unmap_single(dev, dsa_desc->g_dma,
+				 dsa_req->g_len, DMA_TO_DEVICE);
+		dma_unmap_single(dev, dsa_desc->s_dma,
+				 dsa_req->priv_key_len, DMA_TO_DEVICE);
+		dma_unmap_single(dev, dsa_desc->f_dma,
+				 dsa_req->m_len, DMA_TO_DEVICE);
+		dma_unmap_single(dev, dsa_desc->c_dma,
+				 dsa_req->d_len, DMA_FROM_DEVICE);
+		dma_unmap_single(dev, dsa_desc->d_dma,
+				 dsa_req->d_len, DMA_FROM_DEVICE);
+		if (req->type == ECDSA_SIGN)
+			dma_unmap_single(dev, edesc->ab_dma,
+					 dsa_req->ab_len, DMA_TO_DEVICE);
+	}
+	break;
+	case DSA_VERIFY:
+	case ECDSA_VERIFY:
+	{
+		struct dsa_verify_req_s *dsa_req = &req->req_u.dsa_verify;
+		struct dsa_verify_desc_s *dsa_desc =
+		    (struct dsa_verify_desc_s *)edesc->hw_desc;
+		dma_unmap_single(dev, dsa_desc->q_dma,
+				 dsa_req->q_len, DMA_TO_DEVICE);
+		dma_unmap_single(dev, dsa_desc->r_dma,
+				 dsa_req->r_len, DMA_TO_DEVICE);
+		dma_unmap_single(dev, dsa_desc->g_dma,
+				 dsa_req->g_len, DMA_TO_DEVICE);
+		dma_unmap_single(dev, dsa_desc->w_dma,
+				 dsa_req->pub_key_len, DMA_TO_DEVICE);
+		dma_unmap_single(dev, dsa_desc->f_dma,
+				 dsa_req->m_len, DMA_TO_DEVICE);
+		dma_unmap_single(dev, dsa_desc->c_dma,
+				 dsa_req->d_len, DMA_TO_DEVICE);
+		dma_unmap_single(dev, dsa_desc->d_dma,
+				 dsa_req->d_len, DMA_TO_DEVICE);
+		if (req->type == ECDSA_VERIFY) {
+			dma_unmap_single(dev, dsa_desc->tmp_dma,
+					 2*edesc->l_len, DMA_BIDIRECTIONAL);
+			dma_unmap_single(dev, edesc->ab_dma,
+					 dsa_req->ab_len, DMA_TO_DEVICE);
+		} else {
+			dma_unmap_single(dev, dsa_desc->tmp_dma,
+				 edesc->l_len, DMA_BIDIRECTIONAL);
+		}
+		kfree(edesc->tmp);
+	}
+	break;
+	case DLC_KEYGEN:
+	case ECC_KEYGEN:
+	{
+		struct keygen_req_s *key_req = &req->req_u.keygen;
+		struct dlc_keygen_desc_s *key_desc =
+		    (struct dlc_keygen_desc_s *)edesc->hw_desc;
+		dma_unmap_single(dev, key_desc->q_dma,
+				 key_req->q_len, DMA_TO_DEVICE);
+		dma_unmap_single(dev, key_desc->r_dma,
+				 key_req->r_len, DMA_TO_DEVICE);
+		dma_unmap_single(dev, key_desc->g_dma,
+				 key_req->g_len, DMA_TO_DEVICE);
+		dma_unmap_single(dev, key_desc->s_dma,
+				 key_req->priv_key_len, DMA_FROM_DEVICE);
+		dma_unmap_single(dev, key_desc->w_dma,
+				 key_req->pub_key_len, DMA_FROM_DEVICE);
+		if (req->type == ECC_KEYGEN)
+			dma_unmap_single(dev, edesc->ab_dma,
+					 key_req->ab_len, DMA_TO_DEVICE);
+	}
+	break;
+	default:
+		dev_err(dev, "Unable to find request type\n");
+	}
+}
+
+/* DSA Job Completion handler */
+static void dsa_op_done(struct device *dev, u32 *desc, u32 err, void *context)
+{
+	struct pkc_request *req = context;
+	struct dsa_edesc_s *edesc;
+
+	edesc = (struct dsa_edesc_s *)((char *)desc -
+				     offsetof(struct dsa_edesc_s, hw_desc));
+
+	if (err)
+		caam_jr_strstatus(dev, err);
+
+	dsa_unmap(dev, edesc, req);
+	kfree(edesc);
+
+	pkc_request_complete(req, err);
+}
+static int caam_dsa_sign_edesc(struct pkc_request *req,
+				struct dsa_edesc_s *edesc)
+{
+	struct crypto_pkc *tfm = crypto_pkc_reqtfm(req);
+	struct caam_pkc_context_s *ctxt = crypto_pkc_ctx(tfm);
+	struct device *dev = ctxt->dev;
+	struct dsa_sign_req_s *dsa_req = &req->req_u.dsa_sign;
+
+	edesc->l_len = dsa_req->q_len;
+	edesc->n_len = dsa_req->r_len;
+	edesc->req_type = req->type;
+	edesc->curve_type = req->curve_type;
+	edesc->q_dma = dma_map_single(dev, dsa_req->q, dsa_req->q_len,
+					  DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, edesc->q_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto q_map_fail;
+	}
+
+	edesc->r_dma = dma_map_single(dev, dsa_req->r, dsa_req->r_len,
+					  DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, edesc->r_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto r_map_fail;
+	}
+
+	edesc->g_dma = dma_map_single(dev, dsa_req->g, dsa_req->g_len,
+					  DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, edesc->g_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto g_map_fail;
+	}
+
+	edesc->f_dma = dma_map_single(dev, dsa_req->m, dsa_req->m_len,
+				      DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, edesc->f_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto m_map_fail;
+	}
+
+	edesc->key_dma = dma_map_single(dev, dsa_req->priv_key,
+					dsa_req->priv_key_len, DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, edesc->key_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto key_map_fail;
+	}
+
+	edesc->c_dma = dma_map_single(dev, dsa_req->c, dsa_req->d_len,
+					  DMA_FROM_DEVICE);
+	if (dma_mapping_error(dev, edesc->c_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto c_map_fail;
+	}
+
+	edesc->d_dma = dma_map_single(dev, dsa_req->d, dsa_req->d_len,
+					  DMA_FROM_DEVICE);
+	if (dma_mapping_error(dev, edesc->d_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto d_map_fail;
+	}
+
+	if (edesc->req_type == ECDSA_SIGN) {
+		edesc->ab_dma = dma_map_single(dev, dsa_req->ab,
+					       dsa_req->ab_len, DMA_TO_DEVICE);
+		if (dma_mapping_error(dev, edesc->ab_dma)) {
+			dev_err(dev, "Unable to map  memory\n");
+			goto ab_map_fail;
+		}
+	}
+	return 0;
+ab_map_fail:
+	if (edesc->req_type == ECDSA_SIGN)
+		dma_unmap_single(dev, edesc->d_dma, dsa_req->d_len,
+				 DMA_TO_DEVICE);
+d_map_fail:
+	dma_unmap_single(dev, edesc->c_dma, dsa_req->d_len, DMA_FROM_DEVICE);
+c_map_fail:
+	dma_unmap_single(dev, edesc->key_dma, dsa_req->priv_key_len,
+			 DMA_TO_DEVICE);
+key_map_fail:
+	dma_unmap_single(dev, edesc->f_dma, dsa_req->m_len, DMA_FROM_DEVICE);
+m_map_fail:
+	dma_unmap_single(dev, edesc->g_dma, dsa_req->g_len, DMA_TO_DEVICE);
+g_map_fail:
+	dma_unmap_single(dev, edesc->r_dma, dsa_req->r_len, DMA_TO_DEVICE);
+r_map_fail:
+	dma_unmap_single(dev, edesc->q_dma, dsa_req->q_len, DMA_TO_DEVICE);
+q_map_fail:
+	return -EINVAL;
+}
+
+static int caam_dsa_verify_edesc(struct pkc_request *req,
+				  struct dsa_edesc_s *edesc)
+{
+	struct crypto_pkc *tfm = crypto_pkc_reqtfm(req);
+	struct caam_pkc_context_s *ctxt = crypto_pkc_ctx(tfm);
+	struct device *dev = ctxt->dev;
+	uint32_t tmp_len;
+	struct dsa_verify_req_s *dsa_req = &req->req_u.dsa_verify;
+
+	edesc->l_len = dsa_req->q_len;
+	edesc->n_len = dsa_req->r_len;
+	edesc->req_type = req->type;
+	edesc->curve_type = req->curve_type;
+	if (edesc->req_type == ECDSA_VERIFY)
+		tmp_len = 2*dsa_req->q_len;
+	else
+		tmp_len = dsa_req->q_len;
+
+	edesc->tmp = kzalloc(tmp_len, GFP_DMA);
+	if (!edesc->tmp) {
+		pr_debug("Failed to allocate temp buffer for DSA Verify\n");
+		return -ENOMEM;
+	}
+
+	edesc->tmp_dma = dma_map_single(dev, edesc->tmp, tmp_len,
+					  DMA_BIDIRECTIONAL);
+	if (dma_mapping_error(dev, edesc->tmp_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto tmp_map_fail;
+	}
+
+	edesc->q_dma = dma_map_single(dev, dsa_req->q, dsa_req->q_len,
+					  DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, edesc->q_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto q_map_fail;
+	}
+
+	edesc->r_dma = dma_map_single(dev, dsa_req->r, dsa_req->r_len,
+					  DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, edesc->r_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto r_map_fail;
+	}
+
+	edesc->g_dma = dma_map_single(dev, dsa_req->g, dsa_req->g_len,
+					  DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, edesc->g_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto g_map_fail;
+	}
+
+	edesc->f_dma = dma_map_single(dev, dsa_req->m, dsa_req->m_len,
+					  DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, edesc->f_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto m_map_fail;
+	}
+
+	edesc->key_dma = dma_map_single(dev, dsa_req->pub_key,
+					dsa_req->pub_key_len, DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, edesc->key_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto key_map_fail;
+	}
+
+	edesc->c_dma = dma_map_single(dev, dsa_req->c, dsa_req->d_len,
+					  DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, edesc->c_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto c_map_fail;
+	}
+
+	edesc->d_dma = dma_map_single(dev, dsa_req->d, dsa_req->d_len,
+					  DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, edesc->d_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto d_map_fail;
+	}
+
+	if (edesc->req_type == ECDSA_VERIFY) {
+		edesc->ab_dma = dma_map_single(dev, dsa_req->ab,
+					       dsa_req->ab_len, DMA_TO_DEVICE);
+		if (dma_mapping_error(dev, edesc->ab_dma)) {
+			dev_err(dev, "Unable to map  memory\n");
+			goto ab_map_fail;
+		}
+	}
+	return 0;
+ab_map_fail:
+	if (edesc->req_type == ECDSA_VERIFY)
+		dma_unmap_single(dev, edesc->d_dma, dsa_req->d_len,
+				 DMA_TO_DEVICE);
+d_map_fail:
+	dma_unmap_single(dev, edesc->c_dma, dsa_req->d_len, DMA_TO_DEVICE);
+c_map_fail:
+	dma_unmap_single(dev, edesc->key_dma, dsa_req->pub_key_len,
+			 DMA_TO_DEVICE);
+key_map_fail:
+	dma_unmap_single(dev, edesc->f_dma, dsa_req->m_len, DMA_TO_DEVICE);
+m_map_fail:
+	dma_unmap_single(dev, edesc->g_dma, dsa_req->g_len, DMA_TO_DEVICE);
+g_map_fail:
+	dma_unmap_single(dev, edesc->r_dma, dsa_req->r_len, DMA_TO_DEVICE);
+r_map_fail:
+	dma_unmap_single(dev, edesc->q_dma, dsa_req->q_len, DMA_TO_DEVICE);
+q_map_fail:
+	dma_unmap_single(dev, edesc->tmp_dma, tmp_len, DMA_BIDIRECTIONAL);
+tmp_map_fail:
+	kfree(edesc->tmp);
+	return -EINVAL;
+}
+
+static int caam_keygen_edesc(struct pkc_request *req,
+				struct dsa_edesc_s *edesc)
+{
+	struct crypto_pkc *tfm = crypto_pkc_reqtfm(req);
+	struct caam_pkc_context_s *ctxt = crypto_pkc_ctx(tfm);
+	struct crypto_alg *alg = crypto_pkc_tfm(tfm)->__crt_alg;
+	struct caam_pkc_alg *caam_alg =
+			container_of(alg, struct caam_pkc_alg, crypto_alg);
+	struct caam_drv_private *caam_priv = dev_get_drvdata(caam_alg->ctrldev);
+	struct device *dev = ctxt->dev;
+	struct keygen_req_s *key_req = &req->req_u.keygen;
+
+	edesc->l_len = key_req->q_len;
+	edesc->n_len = key_req->r_len;
+	edesc->req_type = req->type;
+	edesc->curve_type = req->curve_type;
+	edesc->erratum_A_006899 = caam_priv->errata & SEC_ERRATUM_A_006899;
+
+	edesc->q_dma = dma_map_single(dev, key_req->q, key_req->q_len,
+					  DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, edesc->q_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto q_map_fail;
+	}
+
+	edesc->r_dma = dma_map_single(dev, key_req->r, key_req->r_len,
+				DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, edesc->r_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto r_map_fail;
+	}
+
+	edesc->g_dma = dma_map_single(dev, key_req->g, key_req->g_len,
+					  DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, edesc->g_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto g_map_fail;
+	}
+
+	if (edesc->erratum_A_006899) {
+		dma_to_sec4_sg_one(&(edesc->g_sg), edesc->g_dma,
+				   key_req->g_len, 0);
+		edesc->g_sg.len |= SEC4_SG_LEN_FIN;
+
+		edesc->g_sg_dma = dma_map_single(dev, &(edesc->g_sg),
+						 sizeof(struct sec4_sg_entry),
+						 DMA_TO_DEVICE);
+		if (dma_mapping_error(dev, edesc->g_sg_dma)) {
+			dev_err(dev, "unable to map S/G table\n");
+			goto g_sg_dma_fail;
+		}
+	}
+
+	edesc->key_dma = dma_map_single(dev, key_req->pub_key,
+					key_req->pub_key_len, DMA_FROM_DEVICE);
+	if (dma_mapping_error(dev, edesc->key_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto key_map_fail;
+	}
+
+	edesc->s_dma = dma_map_single(dev, key_req->priv_key,
+				      key_req->priv_key_len, DMA_FROM_DEVICE);
+	if (dma_mapping_error(dev, edesc->s_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto s_map_fail;
+	}
+
+	if (edesc->req_type == ECC_KEYGEN) {
+		edesc->ab_dma = dma_map_single(dev, key_req->ab,
+						key_req->ab_len, DMA_TO_DEVICE);
+		if (dma_mapping_error(dev, edesc->ab_dma)) {
+			dev_err(dev, "Unable to map  memory\n");
+			goto ab_map_fail;
+		}
+	}
+
+	return 0;
+ab_map_fail:
+	if (edesc->req_type == ECC_KEYGEN)
+		dma_unmap_single(dev, edesc->s_dma, key_req->priv_key_len,
+				 DMA_FROM_DEVICE);
+s_map_fail:
+	dma_unmap_single(dev, edesc->key_dma, key_req->pub_key_len,
+			 DMA_FROM_DEVICE);
+key_map_fail:
+	if (edesc->erratum_A_006899)
+		dma_unmap_single(dev, edesc->g_sg_dma, key_req->g_len,
+				 DMA_TO_DEVICE);
+g_sg_dma_fail:
+	dma_unmap_single(dev, edesc->g_dma, key_req->g_len, DMA_TO_DEVICE);
+g_map_fail:
+	dma_unmap_single(dev, edesc->r_dma, key_req->r_len, DMA_TO_DEVICE);
+r_map_fail:
+	dma_unmap_single(dev, edesc->q_dma, key_req->q_len, DMA_TO_DEVICE);
+q_map_fail:
+	return -EINVAL;
+}
+
+static int caam_rsa_pub_edesc(struct pkc_request *req, struct rsa_edesc *edesc)
+{
+	struct crypto_pkc *tfm = crypto_pkc_reqtfm(req);
+	struct caam_pkc_context_s *ctxt = crypto_pkc_ctx(tfm);
+	struct device *dev = ctxt->dev;
+	struct rsa_pub_req_s *pub_req = &req->req_u.rsa_pub_req;
+	struct rsa_pub_edesc_s *pub_edesc = &edesc->dma_u.rsa_pub_edesc;
+
+	if (pub_req->n_len > pub_req->g_len) {
+		pr_err("Output buffer length less than parameter n\n");
+		return -EINVAL;
+	}
+
+	pub_edesc->n_dma = dma_map_single(dev, pub_req->n, pub_req->n_len,
+					  DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, pub_edesc->n_dma)) {
+		dev_err(dev, "Unable to map  modulus memory\n");
+		goto n_pub_fail;
+	}
+
+	pub_edesc->e_dma = dma_map_single(dev, pub_req->e, pub_req->e_len,
+					  DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, pub_edesc->e_dma)) {
+		dev_err(dev, "Unable to map exponent memory\n");
+		goto e_pub_fail;
+	}
+
+	pub_edesc->f_dma = dma_map_single(dev, pub_req->f, pub_req->f_len,
+					  DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, pub_edesc->f_dma)) {
+		dev_err(dev, "Unable to map input buffer memory\n");
+		goto f_pub_fail;
+	}
+
+	pub_edesc->g_dma = dma_map_single(dev, pub_req->g, pub_req->g_len,
+					  DMA_FROM_DEVICE);
+	if (dma_mapping_error(dev, pub_edesc->g_dma)) {
+		dev_err(dev, "Unable to map output memory\n");
+		goto g_pub_fail;
+	}
+
+	/* TBD: Set SG flags in case input is SG */
+	pub_edesc->sg_flgs.e_len = pub_req->e_len;
+	pub_edesc->sg_flgs.n_len = pub_req->n_len;
+	pub_edesc->f_len = pub_req->f_len;
+/* Enable once we check SG */
+#ifdef SG_ENABLED
+	pub_edesc->sg_flgs.sg_f = 1;
+	pub_edesc->sg_flgs.sg_g = 1;
+	pub_edesc->sg_flgs.sg_e = 1;
+	pub_edesc->sg_flgs.sg_n = 1;
+#endif
+
+	return 0;
+g_pub_fail:
+	dma_unmap_single(dev, pub_edesc->f_dma, pub_req->f_len, DMA_TO_DEVICE);
+f_pub_fail:
+	dma_unmap_single(dev, pub_edesc->e_dma, pub_req->e_len, DMA_TO_DEVICE);
+e_pub_fail:
+	dma_unmap_single(dev, pub_edesc->n_dma, pub_req->n_len, DMA_TO_DEVICE);
+n_pub_fail:
+	return -EINVAL;
+}
+
+static int caam_rsa_priv_f1_edesc(struct pkc_request *req,
+				  struct rsa_edesc *edesc)
+{
+	struct crypto_pkc *tfm = crypto_pkc_reqtfm(req);
+	struct caam_pkc_context_s *ctxt = crypto_pkc_ctx(tfm);
+	struct device *dev = ctxt->dev;
+	struct rsa_priv_frm1_req_s *priv_req = &req->req_u.rsa_priv_f1;
+	struct rsa_priv_frm1_edesc_s *priv_edesc =
+	    &edesc->dma_u.rsa_priv_f1_edesc;
+
+	priv_edesc->n_dma = dma_map_single(dev, priv_req->n, priv_req->n_len,
+					   DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, priv_edesc->n_dma)) {
+		dev_err(dev, "Unable to map  modulus memory\n");
+		goto n_f1_fail;
+	}
+
+	priv_edesc->d_dma = dma_map_single(dev, priv_req->d, priv_req->d_len,
+					   DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, priv_edesc->d_dma)) {
+		dev_err(dev, "Unable to map exponent memory\n");
+		goto d_f1_fail;
+	}
+
+	priv_edesc->f_dma = dma_map_single(dev, priv_req->f, priv_req->f_len,
+					   DMA_FROM_DEVICE);
+	if (dma_mapping_error(dev, priv_edesc->f_dma)) {
+		dev_err(dev, "Unable to map output buffer memory\n");
+		goto f_f1_fail;
+	}
+
+	priv_edesc->g_dma = dma_map_single(dev, priv_req->g, priv_req->g_len,
+					   DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, priv_edesc->g_dma)) {
+		dev_err(dev, "Unable to map input memory\n");
+		goto g_f1_fail;
+	}
+
+/* Enable once we check SG */
+#ifdef SG_ENABLED
+	priv_edesc->sg_flgs.sg_f = 1;
+	priv_edesc->sg_flgs.sg_g = 1;
+	priv_edesc->sg_flgs.sg_d = 1;
+	priv_edesc->sg_flgs.sg_n = 1;
+#endif
+	priv_edesc->sg_flgs.d_len =  priv_req->d_len;
+	priv_edesc->sg_flgs.n_len = priv_req->n_len;
+
+	return 0;
+g_f1_fail:
+	dma_unmap_single(dev, priv_edesc->f_dma, priv_req->f_len,
+			 DMA_FROM_DEVICE);
+f_f1_fail:
+	dma_unmap_single(dev, priv_edesc->d_dma, priv_req->d_len,
+			 DMA_TO_DEVICE);
+d_f1_fail:
+	dma_unmap_single(dev, priv_edesc->n_dma, priv_req->n_len,
+			 DMA_TO_DEVICE);
+n_f1_fail:
+	return -EINVAL;
+}
+
+static int caam_rsa_priv_f2_edesc(struct pkc_request *req,
+				  struct rsa_edesc *edesc)
+{
+	struct crypto_pkc *tfm = crypto_pkc_reqtfm(req);
+	struct caam_pkc_context_s *ctxt = crypto_pkc_ctx(tfm);
+	struct device *dev = ctxt->dev;
+	struct rsa_priv_frm2_req_s *priv_req = &req->req_u.rsa_priv_f2;
+	struct rsa_priv_frm2_edesc_s *priv_edesc =
+	    &edesc->dma_u.rsa_priv_f2_edesc;
+
+	/* tmp1 must be as long as p */
+	priv_edesc->tmp1 = kzalloc(priv_req->p_len, GFP_DMA);
+
+	if (!priv_edesc->tmp1)
+		return -ENOMEM;
+
+	/* tmp2 must be as long as q */
+	priv_edesc->tmp2 = kzalloc(priv_req->q_len, GFP_DMA);
+	if (!priv_edesc->tmp2) {
+		kfree(priv_edesc->tmp1);
+		return -ENOMEM;
+	}
+
+	priv_edesc->tmp1_dma =
+	    dma_map_single(dev, priv_edesc->tmp1, priv_req->p_len,
+			   DMA_BIDIRECTIONAL);
+	if (dma_mapping_error(dev, priv_edesc->tmp1_dma)) {
+		dev_err(dev, "Unable to map  modulus memory\n");
+		goto tmp1_f2_fail;
+	}
+
+	priv_edesc->tmp2_dma =
+	    dma_map_single(dev, priv_edesc->tmp2, priv_req->q_len,
+			   DMA_BIDIRECTIONAL);
+	if (dma_mapping_error(dev, priv_edesc->tmp2_dma)) {
+		dev_err(dev, "Unable to map  modulus memory\n");
+		goto tmp2_f2_fail;
+	}
+
+	priv_edesc->p_dma = dma_map_single(dev, priv_req->p, priv_req->p_len,
+					   DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, priv_edesc->p_dma)) {
+		dev_err(dev, "Unable to map  modulus memory\n");
+		goto p_f2_fail;
+	}
+
+	priv_edesc->q_dma = dma_map_single(dev, priv_req->q, priv_req->q_len,
+					   DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, priv_edesc->q_dma)) {
+		dev_err(dev, "Unable to map exponent memory\n");
+		goto q_f2_fail;
+	}
+
+	priv_edesc->d_dma = dma_map_single(dev, priv_req->d, priv_req->d_len,
+					   DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, priv_edesc->d_dma)) {
+		dev_err(dev, "Unable to map exponent memory\n");
+		goto d_f2_fail;
+	}
+
+	priv_edesc->f_dma = dma_map_single(dev, priv_req->f, priv_req->f_len,
+					   DMA_FROM_DEVICE);
+	if (dma_mapping_error(dev, priv_edesc->f_dma)) {
+		dev_err(dev, "Unable to map output buffer memory\n");
+		goto f_f2_fail;
+	}
+
+	priv_edesc->g_dma = dma_map_single(dev, priv_req->g, priv_req->g_len,
+					   DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, priv_edesc->g_dma)) {
+		dev_err(dev, "Unable to map input memory\n");
+		goto g_f2_fail;
+	}
+	priv_edesc->sg_flgs.d_len = priv_req->d_len;
+	priv_edesc->sg_flgs.n_len = priv_req->n_len;
+	priv_edesc->q_len = priv_req->q_len;
+	priv_edesc->p_len = priv_req->p_len;
+
+	/* TBD: Set SG flags in case input is SG */
+	return 0;
+g_f2_fail:
+	dma_unmap_single(dev, priv_edesc->f_dma, priv_req->f_len,
+			 DMA_FROM_DEVICE);
+f_f2_fail:
+	dma_unmap_single(dev, priv_edesc->d_dma, priv_req->d_len,
+			 DMA_TO_DEVICE);
+d_f2_fail:
+	dma_unmap_single(dev, priv_edesc->q_dma, priv_req->q_len,
+			 DMA_TO_DEVICE);
+q_f2_fail:
+	dma_unmap_single(dev, priv_edesc->p_dma, priv_req->p_len,
+			 DMA_TO_DEVICE);
+p_f2_fail:
+	dma_unmap_single(dev, priv_edesc->tmp2_dma, priv_req->q_len,
+			 DMA_TO_DEVICE);
+tmp2_f2_fail:
+	dma_unmap_single(dev, priv_edesc->tmp1_dma, priv_req->p_len,
+			 DMA_BIDIRECTIONAL);
+	kfree(priv_edesc->tmp2);
+tmp1_f2_fail:
+	kfree(priv_edesc->tmp1);
+	return -EINVAL;
+}
+
+static int caam_rsa_priv_f3_edesc(struct pkc_request *req,
+				  struct rsa_edesc *edesc)
+{
+	struct crypto_pkc *tfm = crypto_pkc_reqtfm(req);
+	struct caam_pkc_context_s *ctxt = crypto_pkc_ctx(tfm);
+	struct device *dev = ctxt->dev;
+	struct rsa_priv_frm3_req_s *priv_req = &req->req_u.rsa_priv_f3;
+	struct rsa_priv_frm3_edesc_s *priv_edesc =
+	    &edesc->dma_u.rsa_priv_f3_edesc;
+
+	priv_edesc->tmp1 = kzalloc(priv_req->p_len, GFP_DMA);
+
+	if (!priv_edesc->tmp1)
+		return -ENOMEM;
+
+	priv_edesc->tmp2 = kzalloc(priv_req->q_len, GFP_DMA);
+	if (!priv_edesc->tmp2) {
+		kfree(priv_edesc->tmp1);
+		return -ENOMEM;
+	}
+
+	priv_edesc->tmp1_dma =
+	    dma_map_single(dev, priv_edesc->tmp1, priv_req->p_len,
+			   DMA_BIDIRECTIONAL);
+	if (dma_mapping_error(dev, priv_edesc->tmp1_dma)) {
+		dev_err(dev, "Unable to map  modulus memory\n");
+		goto tmp1_f2_fail;
+	}
+
+	priv_edesc->tmp2_dma =
+	    dma_map_single(dev, priv_edesc->tmp2, priv_req->q_len,
+			   DMA_BIDIRECTIONAL);
+	if (dma_mapping_error(dev, priv_edesc->tmp2_dma)) {
+		dev_err(dev, "Unable to map  modulus memory\n");
+		goto tmp2_f2_fail;
+	}
+
+	priv_edesc->p_dma = dma_map_single(dev, priv_req->p, priv_req->p_len,
+					   DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, priv_edesc->p_dma)) {
+		dev_err(dev, "Unable to map  modulus memory\n");
+		goto p_f3_fail;
+	}
+
+	priv_edesc->q_dma = dma_map_single(dev, priv_req->q, priv_req->q_len,
+					   DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, priv_edesc->q_dma)) {
+		dev_err(dev, "Unable to map exponent memory\n");
+		goto q_f3_fail;
+	}
+
+	priv_edesc->dp_dma =
+	    dma_map_single(dev, priv_req->dp, priv_req->dp_len, DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, priv_edesc->dp_dma)) {
+		dev_err(dev, "Unable to map dp memory\n");
+		goto dp_f3_fail;
+	}
+
+	priv_edesc->dq_dma =
+	    dma_map_single(dev, priv_req->dq, priv_req->dq_len, DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, priv_edesc->dq_dma)) {
+		dev_err(dev, "Unable to map dq memory\n");
+		goto dq_f3_fail;
+	}
+
+	priv_edesc->c_dma = dma_map_single(dev, priv_req->c, priv_req->c_len,
+					   DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, priv_edesc->c_dma)) {
+		dev_err(dev, "Unable to map Coefficient memory\n");
+		goto c_f3_fail;
+	}
+
+	priv_edesc->f_dma = dma_map_single(dev, priv_req->f, priv_req->f_len,
+					   DMA_FROM_DEVICE);
+	if (dma_mapping_error(dev, priv_edesc->f_dma)) {
+		dev_err(dev, "Unable to map output buffer memory\n");
+		goto f_f3_fail;
+	}
+
+	priv_edesc->g_dma = dma_map_single(dev, priv_req->g, priv_req->g_len,
+					   DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, priv_edesc->g_dma)) {
+		dev_err(dev, "Unable to map input memory\n");
+		goto g_f3_fail;
+	}
+
+	priv_edesc->sg_flgs.n_len = priv_req->f_len;
+	priv_edesc->q_len = priv_req->q_len;
+	priv_edesc->p_len = priv_req->p_len;
+
+	return 0;
+g_f3_fail:
+	dma_unmap_single(dev, priv_edesc->f_dma, priv_req->f_len,
+			 DMA_FROM_DEVICE);
+f_f3_fail:
+	dma_unmap_single(dev, priv_edesc->c_dma, priv_req->c_len,
+			 DMA_TO_DEVICE);
+c_f3_fail:
+	dma_unmap_single(dev, priv_edesc->dq_dma, priv_req->dq_len,
+			 DMA_TO_DEVICE);
+dq_f3_fail:
+	dma_unmap_single(dev, priv_edesc->dp_dma, priv_req->dp_len,
+			 DMA_TO_DEVICE);
+dp_f3_fail:
+	dma_unmap_single(dev, priv_edesc->q_dma, priv_req->q_len,
+			 DMA_TO_DEVICE);
+q_f3_fail:
+	dma_unmap_single(dev, priv_edesc->p_dma, priv_req->p_len,
+			 DMA_TO_DEVICE);
+p_f3_fail:
+	dma_unmap_single(dev, priv_edesc->tmp2_dma, priv_req->q_len,
+			 DMA_TO_DEVICE);
+tmp2_f2_fail:
+	dma_unmap_single(dev, priv_edesc->tmp1_dma, priv_req->p_len,
+			 DMA_BIDIRECTIONAL);
+	kfree(priv_edesc->tmp2);
+tmp1_f2_fail:
+	kfree(priv_edesc->tmp1);
+
+	return -EINVAL;
+}
+
+/* CAAM Descriptor creator for RSA Public Key operations */
+static void *caam_rsa_desc_init(struct pkc_request *req)
+{
+	void *desc = NULL;
+	struct rsa_edesc *edesc = NULL;
+
+	switch (req->type) {
+	case RSA_PUB:
+		{
+			edesc =
+			    kzalloc(sizeof(*edesc) +
+				    sizeof(struct rsa_pub_desc_s), GFP_DMA);
+
+			if (!edesc)
+				return NULL;
+
+			if (caam_rsa_pub_edesc(req, edesc)) {
+				kfree(edesc);
+				return NULL;
+			}
+
+			desc = caam_rsa_pub_desc(edesc);
+			break;
+		}
+	case RSA_PRIV_FORM1:
+		{
+			edesc =
+			    kzalloc(sizeof(*edesc) +
+				    sizeof(struct rsa_priv_frm1_desc_s),
+				    GFP_DMA);
+			if (!edesc)
+				return NULL;
+
+			if (caam_rsa_priv_f1_edesc(req, edesc)) {
+				kfree(edesc);
+				return NULL;
+			}
+
+			desc = caam_rsa_priv_f1_desc(edesc);
+			break;
+		}
+	case RSA_PRIV_FORM2:
+		{
+			edesc =
+			    kzalloc(sizeof(*edesc) +
+				    sizeof(struct rsa_priv_frm2_desc_s),
+				    GFP_DMA);
+			if (!edesc)
+				return NULL;
+
+			if (caam_rsa_priv_f2_edesc(req, edesc)) {
+				kfree(edesc);
+				return NULL;
+			}
+
+			desc = caam_rsa_priv_f2_desc(edesc);
+			break;
+		}
+	case RSA_PRIV_FORM3:
+		{
+			edesc = kzalloc(sizeof(*edesc) +
+					sizeof(struct rsa_priv_frm3_desc_s),
+					GFP_DMA);
+			if (!edesc)
+				return NULL;
+
+			if (caam_rsa_priv_f3_edesc(req, edesc)) {
+				kfree(edesc);
+				return NULL;
+			}
+
+			desc = caam_rsa_priv_f3_desc(edesc);
+			break;
+		}
+	default:
+		pr_debug("Unknown request type\n");
+		return NULL;
+	}
+
+	edesc->req_type = req->type;
+	return desc;
+}
+
+/* CAAM Descriptor creator for RSA Public Key operations */
+static void *caam_dsa_desc_init(struct pkc_request *req)
+{
+	void *desc = NULL;
+	struct dsa_edesc_s *edesc = NULL;
+
+	switch (req->type) {
+	case DSA_SIGN:
+		{
+			edesc = kzalloc(sizeof(*edesc) +
+					sizeof(struct dsa_sign_desc_s),
+					GFP_DMA);
+			if (!edesc)
+				return NULL;
+
+			if (caam_dsa_sign_edesc(req, edesc)) {
+				kfree(edesc);
+				return NULL;
+			}
+
+			desc = caam_dsa_sign_desc(edesc);
+		}
+		break;
+	case DSA_VERIFY:
+		{
+			edesc = kzalloc(sizeof(*edesc) +
+					sizeof(struct dsa_verify_desc_s),
+					GFP_DMA);
+			if (!edesc)
+				return NULL;
+
+			if (caam_dsa_verify_edesc(req, edesc)) {
+				kfree(edesc);
+				return NULL;
+			}
+
+			desc = caam_dsa_verify_desc(edesc);
+		}
+		break;
+	case DLC_KEYGEN:
+		{
+			edesc = kzalloc(sizeof(*edesc) +
+					sizeof(struct dlc_keygen_desc_s),
+					GFP_DMA);
+			if (!edesc)
+				return NULL;
+
+			if (caam_keygen_edesc(req, edesc)) {
+				kfree(edesc);
+				return NULL;
+			}
+
+			desc = caam_keygen_desc(edesc);
+		}
+		break;
+	case ECDSA_SIGN:
+		{
+			edesc = kzalloc(sizeof(*edesc) +
+					sizeof(struct ecdsa_sign_desc_s),
+					GFP_DMA);
+			if (!edesc)
+				return NULL;
+
+			if (caam_dsa_sign_edesc(req, edesc)) {
+				kfree(edesc);
+				return NULL;
+			}
+
+			desc = caam_dsa_sign_desc(edesc);
+		}
+		break;
+	case ECDSA_VERIFY:
+		{
+			edesc = kzalloc(sizeof(*edesc) +
+					sizeof(struct ecdsa_verify_desc_s),
+					GFP_DMA);
+			if (!edesc)
+				return NULL;
+
+			if (caam_dsa_verify_edesc(req, edesc)) {
+				kfree(edesc);
+				return NULL;
+			}
+
+			desc = caam_dsa_verify_desc(edesc);
+		}
+		break;
+	case ECC_KEYGEN:
+		{
+			edesc = kzalloc(sizeof(*edesc) +
+					sizeof(struct ecc_keygen_desc_s),
+					GFP_DMA);
+			if (!edesc)
+				return NULL;
+
+			if (caam_keygen_edesc(req, edesc)) {
+				kfree(edesc);
+				return NULL;
+			}
+
+			desc = caam_keygen_desc(edesc);
+		}
+		break;
+	default:
+		pr_debug("Unknown DSA Desc init request\n");
+		return NULL;
+	}
+	edesc->req_type = req->type;
+	return desc;
+}
+
+static int caam_dh_key_edesc(struct pkc_request *req, struct dh_edesc_s *edesc)
+{
+	struct crypto_pkc *tfm = crypto_pkc_reqtfm(req);
+	struct caam_pkc_context_s *ctxt = crypto_pkc_ctx(tfm);
+	struct device *dev = ctxt->dev;
+	struct dh_key_req_s *dh_req = &req->req_u.dh_req;
+
+	edesc->l_len = dh_req->q_len;
+	edesc->n_len = dh_req->s_len;
+	edesc->req_type = req->type;
+	edesc->curve_type = req->curve_type;
+	edesc->q_dma = dma_map_single(dev, dh_req->q, dh_req->q_len,
+					  DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, edesc->q_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto q_map_fail;
+	}
+
+	edesc->w_dma = dma_map_single(dev, dh_req->pub_key, dh_req->pub_key_len,
+					  DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, edesc->w_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto w_map_fail;
+	}
+
+	edesc->s_dma = dma_map_single(dev, dh_req->s, dh_req->s_len,
+					  DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, edesc->s_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto s_map_fail;
+	}
+
+	edesc->z_dma = dma_map_single(dev, dh_req->z, dh_req->z_len,
+					  DMA_FROM_DEVICE);
+	if (dma_mapping_error(dev, edesc->z_dma)) {
+		dev_err(dev, "Unable to map  memory\n");
+		goto z_map_fail;
+	}
+	if (req->type == ECDH_COMPUTE_KEY) {
+		edesc->ab_dma = dma_map_single(dev, dh_req->ab, dh_req->ab_len,
+					  DMA_TO_DEVICE);
+		if (dma_mapping_error(dev, edesc->ab_dma)) {
+			dev_err(dev, "Unable to map  memory\n");
+			goto ab_map_fail;
+		}
+	}
+	return 0;
+ab_map_fail:
+	dma_unmap_single(dev, edesc->z_dma, dh_req->z_len, DMA_FROM_DEVICE);
+z_map_fail:
+	dma_unmap_single(dev, edesc->s_dma, dh_req->s_len, DMA_TO_DEVICE);
+s_map_fail:
+	dma_unmap_single(dev, edesc->w_dma, dh_req->pub_key_len, DMA_TO_DEVICE);
+w_map_fail:
+	dma_unmap_single(dev, edesc->q_dma, dh_req->q_len, DMA_TO_DEVICE);
+q_map_fail:
+	return -EINVAL;
+}
+
+/* DSA operation Handler */
+static int dsa_op(struct pkc_request *req)
+{
+	struct crypto_pkc *pkc_tfm = crypto_pkc_reqtfm(req);
+	struct caam_pkc_context_s *ctxt = crypto_pkc_ctx(pkc_tfm);
+	struct device *dev = ctxt->dev;
+	int ret = 0;
+	void *desc = NULL;
+
+	desc = caam_dsa_desc_init(req);
+	if (!desc) {
+		dev_err(dev, "Unable to allocate descriptor\n");
+		return -ENOMEM;
+	}
+
+	ret = caam_jr_enqueue(dev, desc, dsa_op_done, req);
+	if (!ret)
+		ret = -EINPROGRESS;
+
+	return ret;
+}
+
+/* CAAM Descriptor creator for DH Public Key operations */
+static void *caam_dh_desc_init(struct pkc_request *req)
+{
+	void *desc = NULL;
+	struct dh_edesc_s *edesc = NULL;
+
+	switch (req->type) {
+	case DH_COMPUTE_KEY:
+	case ECDH_COMPUTE_KEY:
+		{
+			edesc = kzalloc(sizeof(*edesc) +
+					sizeof(struct dh_key_desc_s),
+					GFP_DMA);
+			if (!edesc)
+				return NULL;
+
+			if (caam_dh_key_edesc(req, edesc)) {
+				kfree(edesc);
+				return NULL;
+			}
+			desc = caam_dh_key_desc(edesc);
+		}
+		break;
+	default:
+		pr_debug("Unknown DH Desc init request\n");
+		return NULL;
+	}
+	edesc->req_type = req->type;
+	return desc;
+}
+
+/* DH Job Completion handler */
+static void dh_op_done(struct device *dev, u32 *desc, u32 err, void *context)
+{
+	struct pkc_request *req = context;
+	struct dh_edesc_s *edesc;
+
+	edesc = (struct dh_edesc_s *)((char *)desc -
+				     offsetof(struct dh_edesc_s, hw_desc));
+
+	if (err)
+		caam_jr_strstatus(dev, err);
+
+	dh_unmap(dev, edesc, req);
+	kfree(edesc);
+
+	pkc_request_complete(req, err);
+}
+
+static int dh_op(struct pkc_request *req)
+{
+	struct crypto_pkc *pkc_tfm = crypto_pkc_reqtfm(req);
+	struct caam_pkc_context_s *ctxt = crypto_pkc_ctx(pkc_tfm);
+	struct device *dev = ctxt->dev;
+	int ret = 0;
+	void *desc = NULL;
+	desc = caam_dh_desc_init(req);
+	if (!desc) {
+		dev_err(dev, "Unable to allocate descriptor\n");
+		return -ENOMEM;
+	}
+
+	ret = caam_jr_enqueue(dev, desc, dh_op_done, req);
+	if (!ret)
+		ret = -EINPROGRESS;
+
+	return ret;
+}
+
+/* RSA operation Handler */
+static int rsa_op(struct pkc_request *req)
+{
+	struct crypto_pkc *pkc_tfm = crypto_pkc_reqtfm(req);
+	struct caam_pkc_context_s *ctxt = crypto_pkc_ctx(pkc_tfm);
+	struct device *dev = ctxt->dev;
+	int ret = 0;
+	void *desc = NULL;
+
+	desc = caam_rsa_desc_init(req);
+	if (!desc) {
+		dev_err(dev, "Unable to allocate descriptor\n");
+		return -ENOMEM;
+	}
+
+	ret = caam_jr_enqueue(dev, desc, rsa_op_done, req);
+	if (!ret)
+		ret = -EINPROGRESS;
+
+	return ret;
+}
+
+/* PKC Descriptor Template */
+struct caam_pkc_template {
+	char name[CRYPTO_MAX_ALG_NAME];
+	char driver_name[CRYPTO_MAX_ALG_NAME];
+	char pkc_name[CRYPTO_MAX_ALG_NAME];
+	char pkc_driver_name[CRYPTO_MAX_ALG_NAME];
+	u32 type;
+	struct pkc_alg template_pkc;
+};
+
+static struct caam_pkc_template driver_pkc[] = {
+	/* RSA driver registeration hooks */
+	{
+	 .name = "rsa",
+	 .driver_name = "rsa-caam",
+	 .pkc_name = "pkc(rsa)",
+	 .pkc_driver_name = "pkc-rsa-caam",
+	 .type = CRYPTO_ALG_TYPE_PKC_RSA,
+	 .template_pkc = {
+			  .pkc_op = rsa_op,
+			  .min_keysize = 512,
+			  .max_keysize = 4096,
+			  },
+	 },
+	/* DSA driver registeration hooks */
+	{
+	 .name = "dsa",
+	 .driver_name = "dsa-caam",
+	 .pkc_name = "pkc(dsa)",
+	 .pkc_driver_name = "pkc-dsa-caam",
+	 .type = CRYPTO_ALG_TYPE_PKC_DSA,
+	 .template_pkc = {
+			  .pkc_op = dsa_op,
+			  .min_keysize = 512,
+			  .max_keysize = 4096,
+			  },
+	 },
+	/* DH driver registeration hooks */
+	{
+	 .name = "dh",
+	 .driver_name = "dh-caam",
+	 .pkc_name = "pkc(dh)",
+	 .pkc_driver_name = "pkc-dh-caam",
+	 .type = CRYPTO_ALG_TYPE_PKC_DH,
+	 .template_pkc = {
+			  .pkc_op = dh_op,
+			  .min_keysize = 512,
+			  .max_keysize = 4096,
+			  },
+	 }
+};
+
+/* Per session pkc's driver context creation function */
+static int caam_pkc_cra_init(struct crypto_tfm *tfm)
+{
+	struct caam_pkc_context_s *ctx = crypto_tfm_ctx(tfm);
+
+	ctx->dev = caam_jr_alloc();
+
+	if (IS_ERR(ctx->dev)) {
+		pr_err("Job Ring Device allocation for transform failed\n");
+		return PTR_ERR(ctx->dev);
+	}
+	return 0;
+}
+
+/* Per session pkc's driver context cleanup function */
+static void caam_pkc_cra_exit(struct crypto_tfm *tfm)
+{
+	/* Nothing to cleanup in private context */
+}
+
+static struct caam_pkc_alg *caam_pkc_alloc(struct device *ctrldev,
+					   struct caam_pkc_template *template,
+					   bool keyed)
+{
+	struct caam_pkc_alg *t_alg;
+	struct crypto_alg *alg;
+
+	t_alg = kzalloc(sizeof(*t_alg), GFP_KERNEL);
+	if (!t_alg) {
+		dev_err(ctrldev, "failed to allocate t_alg\n");
+		return NULL;
+	}
+
+	alg = &t_alg->crypto_alg;
+	alg->cra_pkc = template->template_pkc;
+
+	if (keyed) {
+		snprintf(alg->cra_name, CRYPTO_MAX_ALG_NAME, "%s",
+			 template->pkc_name);
+		snprintf(alg->cra_driver_name, CRYPTO_MAX_ALG_NAME, "%s",
+			 template->pkc_driver_name);
+	} else {
+		snprintf(alg->cra_name, CRYPTO_MAX_ALG_NAME, "%s",
+			 template->name);
+		snprintf(alg->cra_driver_name, CRYPTO_MAX_ALG_NAME, "%s",
+			 template->driver_name);
+	}
+	alg->cra_module = THIS_MODULE;
+	alg->cra_init = caam_pkc_cra_init;
+	alg->cra_exit = caam_pkc_cra_exit;
+	alg->cra_ctxsize = sizeof(struct caam_pkc_context_s);
+	alg->cra_priority = CAAM_PKC_PRIORITY;
+	alg->cra_alignmask = 0;
+	alg->cra_flags = CRYPTO_ALG_ASYNC | template->type;
+	alg->cra_type = &crypto_pkc_type;
+	t_alg->ctrldev = ctrldev;
+
+	return t_alg;
+}
+
+/* Public Key Cryptography module initialization handler */
+static int __init caam_pkc_init(void)
+{
+	struct device_node *dev_node;
+	struct platform_device *pdev;
+	struct device *ctrldev;
+	struct caam_drv_private *priv;
+	int i = 0, err = 0;
+
+	dev_node = of_find_compatible_node(NULL, NULL, "fsl,sec-v4.0");
+	if (!dev_node) {
+		dev_node = of_find_compatible_node(NULL, NULL, "fsl,sec4.0");
+		if (!dev_node)
+			return -ENODEV;
+	}
+
+	pdev = of_find_device_by_node(dev_node);
+	if (!pdev)
+		return -ENODEV;
+
+	ctrldev = &pdev->dev;
+	priv = dev_get_drvdata(ctrldev);
+	of_node_put(dev_node);
+
+	/*
+	 * If priv is NULL, it's probably because the caam driver wasn't
+	 * properly initialized (e.g. RNG4 init failed). Thus, bail out here.
+	 */
+	if (!priv)
+		return -ENODEV;
+
+	INIT_LIST_HEAD(&priv->pkc_list);
+
+	/* register crypto algorithms the device supports */
+	for (i = 0; i < ARRAY_SIZE(driver_pkc); i++) {
+		/* TODO: check if h/w supports alg */
+		struct caam_pkc_alg *t_alg;
+
+		/* register pkc algorithm */
+		t_alg = caam_pkc_alloc(ctrldev, &driver_pkc[i], true);
+		if (IS_ERR(t_alg)) {
+			err = PTR_ERR(t_alg);
+			dev_warn(ctrldev, "%s alg allocation failed\n",
+				 driver_pkc[i].driver_name);
+			continue;
+		}
+
+		err = crypto_register_alg(&t_alg->crypto_alg);
+		if (err) {
+			dev_warn(ctrldev, "%s alg registration failed\n",
+				 t_alg->crypto_alg.cra_driver_name);
+			kfree(t_alg);
+		} else {
+			list_add_tail(&t_alg->entry, &priv->pkc_list);
+		}
+	}
+
+	if (!list_empty(&priv->pkc_list))
+		dev_info(ctrldev, "%s algorithms registered in /proc/crypto\n",
+			 (char *)of_get_property(dev_node, "compatible", NULL));
+
+	return err;
+}
+
+static void __exit caam_pkc_exit(void)
+{
+	struct device_node *dev_node;
+	struct platform_device *pdev;
+	struct device *ctrldev;
+	struct caam_drv_private *priv;
+	struct caam_pkc_alg *t_alg, *n;
+
+	dev_node = of_find_compatible_node(NULL, NULL, "fsl,sec-v4.0");
+
+	if (!dev_node) {
+		dev_node = of_find_compatible_node(NULL, NULL, "fsl,sec4.0");
+		if (!dev_node)
+			return;
+	}
+
+	pdev = of_find_device_by_node(dev_node);
+
+	if (!pdev)
+		return;
+
+	ctrldev = &pdev->dev;
+	of_node_put(dev_node);
+	priv = dev_get_drvdata(ctrldev);
+
+	if (!priv->pkc_list.next)
+		return;
+
+	list_for_each_entry_safe(t_alg, n, &priv->pkc_list, entry) {
+		crypto_unregister_alg(&t_alg->crypto_alg);
+		list_del(&t_alg->entry);
+		kfree(t_alg);
+	}
+}
+
+module_init(caam_pkc_init);
+module_exit(caam_pkc_exit);
+
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("FSL CAAM support for PKC functions of crypto API");
+MODULE_AUTHOR("Yashpal Dutta <yashpal.dutta@freescale.com>");
diff --git a/drivers/crypto/caam/compat.h b/drivers/crypto/caam/compat.h
index 762aeff..5691028 100644
--- a/drivers/crypto/caam/compat.h
+++ b/drivers/crypto/caam/compat.h
@@ -14,6 +14,8 @@
 #include <linux/hash.h>
 #include <linux/hw_random.h>
 #include <linux/of_platform.h>
+#include <linux/of_address.h>
+#include <linux/of_irq.h>
 #include <linux/dma-mapping.h>
 #include <linux/io.h>
 #include <linux/spinlock.h>
diff --git a/drivers/crypto/caam/ctrl.c b/drivers/crypto/caam/ctrl.c
index 63fb1af..7dade6d 100644
--- a/drivers/crypto/caam/ctrl.c
+++ b/drivers/crypto/caam/ctrl.c
@@ -5,16 +5,16 @@
  * Copyright 2008-2012 Freescale Semiconductor, Inc.
  */
 
-#include <linux/of_address.h>
-#include <linux/of_irq.h>
-
 #include "compat.h"
 #include "regs.h"
 #include "intern.h"
 #include "jr.h"
 #include "desc_constr.h"
 #include "error.h"
-#include "ctrl.h"
+
+#ifdef CONFIG_CAAM_QI
+#include "qi.h"
+#endif
 
 /*
  * Descriptor to instantiate RNG State Handle 0 in normal mode and
@@ -76,47 +76,65 @@ static void build_deinstantiation_desc(u32 *desc, int handle)
  * Return: - 0 if no error occurred
  *	   - -ENODEV if the DECO couldn't be acquired
  *	   - -EAGAIN if an error occurred while executing the descriptor
+ *	   - -EINVAL if the descriptor length is incorrect
  */
 static inline int run_descriptor_deco0(struct device *ctrldev, u32 *desc,
 					u32 *status)
 {
 	struct caam_drv_private *ctrlpriv = dev_get_drvdata(ctrldev);
-	struct caam_full __iomem *topregs;
+	struct caam_ctrl __iomem *ctrl = ctrlpriv->ctrl;
+	struct caam_deco __iomem *deco = ctrlpriv->deco;
 	unsigned int timeout = 100000;
 	u32 deco_dbg_reg, flags;
-	int i;
+	int i, ret, dlen;
+
+
+	if (ctrlpriv->virt_en == 1) {
+		setbits32(&ctrl->deco_rsr, DECORSR_JR0);
+
+		while (!(rd_reg32(&ctrl->deco_rsr) & DECORSR_VALID) &&
+		       --timeout)
+			cpu_relax();
 
-	/* Set the bit to request direct access to DECO0 */
-	topregs = (struct caam_full __iomem *)ctrlpriv->ctrl;
-	setbits32(&topregs->ctrl.deco_rq, DECORR_RQD0ENABLE);
+		timeout = 100000;
+	}
+
+	setbits32(&ctrl->deco_rq, DECORR_RQD0ENABLE);
 
-	while (!(rd_reg32(&topregs->ctrl.deco_rq) & DECORR_DEN0) &&
+	while (!(rd_reg32(&ctrl->deco_rq) & DECORR_DEN0) &&
 								 --timeout)
 		cpu_relax();
 
 	if (!timeout) {
 		dev_err(ctrldev, "failed to acquire DECO 0\n");
-		clrbits32(&topregs->ctrl.deco_rq, DECORR_RQD0ENABLE);
-		return -ENODEV;
+		ret = -ENODEV;
+		goto out_err;
+	}
+
+	dlen = desc_len(desc);
+	if (dlen > MAX_CAAM_DESCSIZE) {
+		dev_err(ctrldev, "invalid descriptor length\n");
+		ret = -EINVAL;
+		goto out_err;
 	}
 
-	for (i = 0; i < desc_len(desc); i++)
-		wr_reg32(&topregs->deco.descbuf[i], *(desc + i));
+	for (i = 0; i < dlen; i++)
+		wr_reg32(&deco->descbuf[i], wr_en_val32(*(desc + i)));
 
 	flags = DECO_JQCR_WHL;
 	/*
 	 * If the descriptor length is longer than 4 words, then the
 	 * FOUR bit in JRCTRL register must be set.
 	 */
-	if (desc_len(desc) >= 4)
+	if (dlen >= 4)
 		flags |= DECO_JQCR_FOUR;
 
 	/* Instruct the DECO to execute it */
-	wr_reg32(&topregs->deco.jr_ctl_hi, flags);
+	wr_reg32(&deco->jr_ctl_hi, flags);
 
 	timeout = 10000000;
 	do {
-		deco_dbg_reg = rd_reg32(&topregs->deco.desc_dbg);
+		deco_dbg_reg = rd_reg32(&deco->desc_dbg);
 		/*
 		 * If an error occured in the descriptor, then
 		 * the DECO status field will be set to 0x0D
@@ -127,16 +145,18 @@ static inline int run_descriptor_deco0(struct device *ctrldev, u32 *desc,
 		cpu_relax();
 	} while ((deco_dbg_reg & DESC_DBG_DECO_STAT_VALID) && --timeout);
 
-	*status = rd_reg32(&topregs->deco.op_status_hi) &
+	*status = rd_reg32(&deco->op_status_hi) &
 		  DECO_OP_STATUS_HI_ERR_MASK;
 
-	/* Mark the DECO as free */
-	clrbits32(&topregs->ctrl.deco_rq, DECORR_RQD0ENABLE);
+	if (ctrlpriv->virt_en == 1)
+		clrbits32(&ctrl->deco_rsr, DECORSR_JR0);
 
-	if (!timeout)
-		return -EAGAIN;
+	ret = timeout ? 0 : -EAGAIN;
 
-	return 0;
+out_err:
+	/* Mark the DECO as free */
+	clrbits32(&ctrl->deco_rq, DECORR_RQD0ENABLE);
+	return ret;
 }
 
 /*
@@ -157,19 +177,17 @@ static inline int run_descriptor_deco0(struct device *ctrldev, u32 *desc,
  *	   - -EAGAIN if an error occurred when executing the descriptor
  *	      f.i. there was a RNG hardware error due to not "good enough"
  *	      entropy being aquired.
+ *	   - -EINVAL if the descriptor length is incorrect
  */
 static int instantiate_rng(struct device *ctrldev, int state_handle_mask,
 			   int gen_sk)
 {
 	struct caam_drv_private *ctrlpriv = dev_get_drvdata(ctrldev);
-	struct caam_full __iomem *topregs;
-	struct rng4tst __iomem *r4tst;
+	struct caam_ctrl __iomem *ctrl;
 	u32 *desc, status, rdsta_val;
 	int ret = 0, sh_idx;
 
-	topregs = (struct caam_full __iomem *)ctrlpriv->ctrl;
-	r4tst = &topregs->ctrl.r4tst[0];
-
+	ctrl = (struct caam_ctrl __iomem *)ctrlpriv->ctrl;
 	desc = kmalloc(CAAM_CMD_SZ * 7, GFP_KERNEL);
 	if (!desc)
 		return -ENOMEM;
@@ -187,7 +205,8 @@ static int instantiate_rng(struct device *ctrldev, int state_handle_mask,
 
 		/* Try to run it through DECO0 */
 		ret = run_descriptor_deco0(ctrldev, desc, &status);
-
+		if (ret)
+			break;
 		/*
 		 * If ret is not 0, or descriptor status is not 0, then
 		 * something went wrong. No need to try the next state
@@ -198,12 +217,12 @@ static int instantiate_rng(struct device *ctrldev, int state_handle_mask,
 		 * CAAM eras), then try again.
 		 */
 		rdsta_val =
-			rd_reg32(&topregs->ctrl.r4tst[0].rdsta) & RDSTA_IFMASK;
-		if (status || !(rdsta_val & (1 << sh_idx)))
+			rd_reg32(&ctrl->r4tst[0].rdsta) & RDSTA_IFMASK;
+		if ((status && status != JRSTA_SSRC_JUMP_HALT_CC) ||
+		    !(rdsta_val & (1 << sh_idx)))
 			ret = -EAGAIN;
 		if (ret)
 			break;
-
 		dev_info(ctrldev, "Instantiated RNG4 SH%d\n", sh_idx);
 		/* Clear the contents before recreating the descriptor */
 		memset(desc, 0x00, CAAM_CMD_SZ * 7);
@@ -226,6 +245,7 @@ static int instantiate_rng(struct device *ctrldev, int state_handle_mask,
  *	   - -ENOMEM if there isn't enough memory to allocate the descriptor
  *	   - -ENODEV if DECO0 couldn't be acquired
  *	   - -EAGAIN if an error occurred when executing the descriptor
+ *	   - -EINVAL if the descriptor length is incorrect
  */
 static int deinstantiate_rng(struct device *ctrldev, int state_handle_mask)
 {
@@ -271,12 +291,12 @@ static int caam_remove(struct platform_device *pdev)
 {
 	struct device *ctrldev;
 	struct caam_drv_private *ctrlpriv;
-	struct caam_full __iomem *topregs;
+	struct caam_ctrl __iomem *ctrl;
 	int ring, ret = 0;
 
 	ctrldev = &pdev->dev;
 	ctrlpriv = dev_get_drvdata(ctrldev);
-	topregs = (struct caam_full __iomem *)ctrlpriv->ctrl;
+	ctrl = (struct caam_ctrl __iomem *)ctrlpriv->ctrl;
 
 	/* Remove platform devices for JobRs */
 	for (ring = 0; ring < ctrlpriv->total_jobrs; ring++) {
@@ -284,6 +304,10 @@ static int caam_remove(struct platform_device *pdev)
 			of_device_unregister(ctrlpriv->jrpdev[ring]);
 	}
 
+#ifdef CONFIG_CAAM_QI
+	if (ctrlpriv->qidev)
+		caam_qi_shutdown(ctrlpriv->qidev);
+#endif
 	/* De-initialize RNG state handles initialized by this driver. */
 	if (ctrlpriv->rng4_sh_init)
 		deinstantiate_rng(ctrldev, ctrlpriv->rng4_sh_init);
@@ -294,7 +318,7 @@ static int caam_remove(struct platform_device *pdev)
 #endif
 
 	/* Unmap controller region */
-	iounmap(&topregs->ctrl);
+	iounmap(&ctrl);
 
 	kfree(ctrlpriv->jrpdev);
 	kfree(ctrlpriv);
@@ -305,19 +329,18 @@ static int caam_remove(struct platform_device *pdev)
 /*
  * kick_trng - sets the various parameters for enabling the initialization
  *	       of the RNG4 block in CAAM
- * @pdev - pointer to the platform device
+ * @pdev - pointer to the caam device
  * @ent_delay - Defines the length (in system clocks) of each entropy sample.
  */
-static void kick_trng(struct platform_device *pdev, int ent_delay)
+static void kick_trng(struct device *dev, int ent_delay)
 {
-	struct device *ctrldev = &pdev->dev;
-	struct caam_drv_private *ctrlpriv = dev_get_drvdata(ctrldev);
-	struct caam_full __iomem *topregs;
+	struct caam_drv_private *ctrlpriv = dev_get_drvdata(dev);
+	struct caam_ctrl __iomem *ctrl;
 	struct rng4tst __iomem *r4tst;
 	u32 val;
 
-	topregs = (struct caam_full __iomem *)ctrlpriv->ctrl;
-	r4tst = &topregs->ctrl.r4tst[0];
+	ctrl = (struct caam_ctrl __iomem *)ctrlpriv->ctrl;
+	r4tst = &ctrl->r4tst[0];
 
 	/* put RNG4 into program mode */
 	setbits32(&r4tst->rtmctl, RTMCTL_PRGM);
@@ -344,59 +367,131 @@ static void kick_trng(struct platform_device *pdev, int ent_delay)
 	wr_reg32(&r4tst->rtsdctl, val);
 	/* min. freq. count, equal to 1/4 of the entropy sample length */
 	wr_reg32(&r4tst->rtfrqmin, ent_delay >> 2);
-	/* max. freq. count, equal to 8 times the entropy sample length */
-	wr_reg32(&r4tst->rtfrqmax, ent_delay << 3);
+	/* disable maximum frequency count */
+	wr_reg32(&r4tst->rtfrqmax, RTFRQMAX_DISABLE);
+	/* read the control register */
+	val = rd_reg32(&r4tst->rtmctl);
+	/*
+	 * select raw sampling in both entropy shifter
+	 * and statistical checker
+	 */
+	setbits32(&val, RTMCTL_SAMP_MODE_RAW_ES_SC);
 	/* put RNG4 into run mode */
-	clrbits32(&r4tst->rtmctl, RTMCTL_PRGM);
+	clrbits32(&val, RTMCTL_PRGM);
+	/* write back the control register */
+	wr_reg32(&r4tst->rtmctl, val);
 }
 
 /**
  * caam_get_era() - Return the ERA of the SEC on SoC, based
- * on the SEC_VID register.
- * Returns the ERA number (1..4) or -ENOTSUPP if the ERA is unknown.
- * @caam_id - the value of the SEC_VID register
+ * on "sec-era" propery in the DTS. This property is updated by u-boot.
  **/
-int caam_get_era(u64 caam_id)
+int caam_get_era(void)
 {
-	struct sec_vid *sec_vid = (struct sec_vid *)&caam_id;
-	static const struct {
-		u16 ip_id;
-		u8 maj_rev;
-		u8 era;
-	} caam_eras[] = {
-		{0x0A10, 1, 1},
-		{0x0A10, 2, 2},
-		{0x0A12, 1, 3},
-		{0x0A14, 1, 3},
-		{0x0A14, 2, 4},
-		{0x0A16, 1, 4},
-		{0x0A11, 1, 4}
-	};
-	int i;
-
-	for (i = 0; i < ARRAY_SIZE(caam_eras); i++)
-		if (caam_eras[i].ip_id == sec_vid->ip_id &&
-			caam_eras[i].maj_rev == sec_vid->maj_rev)
-				return caam_eras[i].era;
-
-	return -ENOTSUPP;
+	struct device_node *caam_node;
+	int ret;
+	u32 prop;
+
+	caam_node = of_find_compatible_node(NULL, NULL, "fsl,sec-v4.0");
+	ret = of_property_read_u32(caam_node, "fsl,sec-era", &prop);
+	of_node_put(caam_node);
+
+	return IS_ERR_VALUE(ret) ? -ENOTSUPP : prop;
 }
 EXPORT_SYMBOL(caam_get_era);
 
+static int caam_rng_init(struct device *dev)
+{
+	int gen_sk, ent_delay = RTSDCTL_ENT_DLY_MIN;
+	struct caam_drv_private *ctrlpriv = dev_get_drvdata(dev);
+	u64 cha_vid_ls;
+	struct caam_ctrl __iomem *ctrl;
+	int ret = 0;
+
+	ctrl = (struct caam_ctrl __iomem *)ctrlpriv->ctrl;
+	cha_vid_ls = rd_reg32(&ctrl->perfmon.cha_id_ls);
+	/*
+	 * If SEC has RNG version >= 4 and RNG state handle has not been
+	 * already instantiated, do RNG instantiation
+	 */
+	if ((cha_vid_ls & CHA_ID_LS_RNG_MASK) >> CHA_ID_LS_RNG_SHIFT >= 4) {
+		ctrlpriv->rng4_sh_init =
+			rd_reg32(&ctrl->r4tst[0].rdsta);
+		/*
+		 * If the secure keys (TDKEK, JDKEK, TDSK), were already
+		 * generated, signal this to the function that is instantiating
+		 * the state handles. An error would occur if RNG4 attempts
+		 * to regenerate these keys before the next POR.
+		 */
+		gen_sk = ctrlpriv->rng4_sh_init & RDSTA_SKVN ? 0 : 1;
+		ctrlpriv->rng4_sh_init &= RDSTA_IFMASK;
+		do {
+			int inst_handles =
+				rd_reg32(&ctrl->r4tst[0].rdsta) &
+								RDSTA_IFMASK;
+			/*
+			 * If either SH were instantiated by somebody else
+			 * (e.g. u-boot) then it is assumed that the entropy
+			 * parameters are properly set and thus the function
+			 * setting these (kick_trng(...)) is skipped.
+			 * Also, if a handle was instantiated, do not change
+			 * the TRNG parameters.
+			 */
+			if (!(ctrlpriv->rng4_sh_init || inst_handles)) {
+					dev_info(dev,
+						 "Entropy delay = %u\n",
+						 ent_delay);
+				kick_trng(dev, ent_delay);
+				ent_delay += 400;
+			}
+			/*
+			 * if instantiate_rng(...) fails, the loop will rerun
+			 * and the kick_trng(...) function will modfiy the
+			 * upper and lower limits of the entropy sampling
+			 * interval, leading to a sucessful initialization of
+			 * the RNG.
+			 */
+			ret = instantiate_rng(dev, inst_handles,
+					      gen_sk);
+			if (ret == -EAGAIN)
+				/*
+				 * if here, the loop will rerun,
+				 * so don't hog the CPU
+				 */
+				cpu_relax();
+		} while ((ret == -EAGAIN) && (ent_delay < RTSDCTL_ENT_DLY_MAX));
+		if (ret) {
+			dev_err(dev, "failed to instantiate RNG");
+			return ret;
+		}
+		/*
+		 * Set handles init'ed by this module as the complement of the
+		 * already initialized ones
+		 */
+		ctrlpriv->rng4_sh_init = ~ctrlpriv->rng4_sh_init & RDSTA_IFMASK;
+
+		/* Enable RDB bit so that RNG works faster */
+		setbits32(&ctrl->scfgr, SCFGR_RDBENABLE);
+	}
+	return 0;
+}
+
 /* Probe routine for CAAM top (controller) level */
 static int caam_probe(struct platform_device *pdev)
 {
-	int ret, ring, rspec, gen_sk, ent_delay = RTSDCTL_ENT_DLY_MIN;
+	int ret, ring, rspec;
 	u64 caam_id;
+	u32 caam_id_ms;
 	struct device *dev;
 	struct device_node *nprop, *np;
 	struct caam_ctrl __iomem *ctrl;
-	struct caam_full __iomem *topregs;
 	struct caam_drv_private *ctrlpriv;
 #ifdef CONFIG_DEBUG_FS
 	struct caam_perfmon *perfmon;
 #endif
-	u64 cha_vid;
+	u32 mcr, scfgr, comp_params;
+	int pg_size;
+	int BLOCK_OFFSET = 0;
 
 	ctrlpriv = kzalloc(sizeof(struct caam_drv_private), GFP_KERNEL);
 	if (!ctrlpriv)
@@ -414,20 +509,66 @@ static int caam_probe(struct platform_device *pdev)
 		dev_err(dev, "caam: of_iomap() failed\n");
 		return -ENOMEM;
 	}
-	ctrlpriv->ctrl = (struct caam_ctrl __force *)ctrl;
+	/* Finding the page size for using the CTPR_MS register */
+	comp_params = rd_reg32(&ctrl->perfmon.comp_parms_ms);
+	pg_size = (comp_params & CTPR_MS_PG_SZ_MASK) >> CTPR_MS_PG_SZ_SHIFT;
 
-	/* topregs used to derive pointers to CAAM sub-blocks only */
-	topregs = (struct caam_full __iomem *)ctrl;
+	/* Allocating the BLOCK_OFFSET based on the supported page size on
+	 * the platform
+	 */
+	if (pg_size == 0)
+		BLOCK_OFFSET = PG_SIZE_4K;
+	else
+		BLOCK_OFFSET = PG_SIZE_64K;
+
+	ctrlpriv->ctrl = (struct caam_ctrl __force *)ctrl;
+	ctrlpriv->assure = (struct caam_assurance __force *)
+			   ((uint8_t *)ctrl +
+			    BLOCK_OFFSET * ASSURE_BLOCK_NUMBER
+			   );
+	ctrlpriv->deco = (struct caam_deco __force *)
+			 ((uint8_t *)ctrl +
+			 BLOCK_OFFSET * DECO_BLOCK_NUMBER
+			 );
 
 	/* Get the IRQ of the controller (for security violations only) */
-	ctrlpriv->secvio_irq = irq_of_parse_and_map(nprop, 0);
+	ctrlpriv->secvio_irq = of_irq_to_resource(nprop, 0, NULL);
 
 	/*
 	 * Enable DECO watchdogs and, if this is a PHYS_ADDR_T_64BIT kernel,
 	 * long pointers in master configuration register
 	 */
-	setbits32(&topregs->ctrl.mcr, MCFGR_WDENABLE |
-		  (sizeof(dma_addr_t) == sizeof(u64) ? MCFGR_LONG_PTR : 0));
+	mcr = rd_reg32(&ctrl->mcr);
+	mcr = (mcr & ~MCFGR_AWCACHE_MASK) | (0x2 << MCFGR_AWCACHE_SHIFT) |
+	      MCFGR_WDENABLE | (sizeof(dma_addr_t) == sizeof(u64) ?
+				MCFGR_LONG_PTR : 0);
+	wr_reg32(&ctrl->mcr, mcr);
+
+	/*
+	 *  Read the Compile Time paramters and SCFGR to determine
+	 * if Virtualization is enabled for this platform
+	 */
+	scfgr = rd_reg32(&ctrl->scfgr);
+
+	ctrlpriv->virt_en = 0;
+	if (comp_params & CTPR_MS_VIRT_EN_INCL) {
+		/* VIRT_EN_INCL = 1 & VIRT_EN_POR = 1 or
+		 * VIRT_EN_INCL = 1 & VIRT_EN_POR = 0 & SCFGR_VIRT_EN = 1
+		 */
+		if ((comp_params & CTPR_MS_VIRT_EN_POR) ||
+		    (!(comp_params & CTPR_MS_VIRT_EN_POR) &&
+		       (scfgr & SCFGR_VIRT_EN)))
+				ctrlpriv->virt_en = 1;
+	} else {
+		/* VIRT_EN_INCL = 0 && VIRT_EN_POR_VALUE = 1 */
+		if (comp_params & CTPR_MS_VIRT_EN_POR)
+				ctrlpriv->virt_en = 1;
+	}
+
+	if (ctrlpriv->virt_en == 1)
+		setbits32(&ctrl->jrstart, JRSTART_JR0_START |
+			  JRSTART_JR1_START | JRSTART_JR2_START |
+			  JRSTART_JR3_START);
 
 	if (sizeof(dma_addr_t) == sizeof(u64))
 		if (of_device_is_compatible(nprop, "fsl,sec-v5.0"))
@@ -454,7 +595,7 @@ static int caam_probe(struct platform_device *pdev)
 	ctrlpriv->jrpdev = kzalloc(sizeof(struct platform_device *) * rspec,
 								GFP_KERNEL);
 	if (ctrlpriv->jrpdev == NULL) {
-		iounmap(&topregs->ctrl);
+		iounmap(&ctrl);
 		return -ENOMEM;
 	}
 
@@ -467,6 +608,11 @@ static int caam_probe(struct platform_device *pdev)
 			pr_warn("JR%d Platform device creation error\n", ring);
 			continue;
 		}
+		ctrlpriv->jr[ring] = (struct caam_job_ring __force *)
+				     ((uint8_t *)ctrl +
+				     (ring + JR_BLOCK_NUMBER) *
+				      BLOCK_OFFSET
+				     );
 		ctrlpriv->total_jobrs++;
 		ring++;
 	}
@@ -485,12 +631,22 @@ static int caam_probe(struct platform_device *pdev)
 	}
 
 	/* Check to see if QI present. If so, enable */
-	ctrlpriv->qi_present = !!(rd_reg64(&topregs->ctrl.perfmon.comp_parms) &
-				  CTPR_QI_MASK);
+	ctrlpriv->qi_present =
+			!!(rd_reg32(&ctrl->perfmon.comp_parms_ms) &
+			   CTPR_MS_QI_MASK);
 	if (ctrlpriv->qi_present) {
-		ctrlpriv->qi = (struct caam_queue_if __force *)&topregs->qi;
+		ctrlpriv->qi = (struct caam_queue_if __force *)
+			       ((uint8_t *)ctrl +
+				BLOCK_OFFSET * QI_BLOCK_NUMBER
+			       );
 		/* This is all that's required to physically enable QI */
-		wr_reg32(&topregs->qi.qi_control_lo, QICTL_DQEN);
+		wr_reg32(&ctrlpriv->qi->qi_control_lo, QICTL_DQEN);
+
+		/* If QMAN driver is present, init CAAM-QI backend */
+#ifdef CONFIG_CAAM_QI
+		if (caam_qi_init(pdev, nprop))
+			dev_err(dev, "caam qi i/f init failed\n");
+#endif
 	}
 
 	/* If no QI and no rings specified, quit and go home */
@@ -500,71 +656,26 @@ static int caam_probe(struct platform_device *pdev)
 		return -ENOMEM;
 	}
 
-	cha_vid = rd_reg64(&topregs->ctrl.perfmon.cha_id);
-
-	/*
-	 * If SEC has RNG version >= 4 and RNG state handle has not been
-	 * already instantiated, do RNG instantiation
-	 */
-	if ((cha_vid & CHA_ID_RNG_MASK) >> CHA_ID_RNG_SHIFT >= 4) {
-		ctrlpriv->rng4_sh_init =
-			rd_reg32(&topregs->ctrl.r4tst[0].rdsta);
-		/*
-		 * If the secure keys (TDKEK, JDKEK, TDSK), were already
-		 * generated, signal this to the function that is instantiating
-		 * the state handles. An error would occur if RNG4 attempts
-		 * to regenerate these keys before the next POR.
-		 */
-		gen_sk = ctrlpriv->rng4_sh_init & RDSTA_SKVN ? 0 : 1;
-		ctrlpriv->rng4_sh_init &= RDSTA_IFMASK;
-		do {
-			int inst_handles =
-				rd_reg32(&topregs->ctrl.r4tst[0].rdsta) &
-								RDSTA_IFMASK;
-			/*
-			 * If either SH were instantiated by somebody else
-			 * (e.g. u-boot) then it is assumed that the entropy
-			 * parameters are properly set and thus the function
-			 * setting these (kick_trng(...)) is skipped.
-			 * Also, if a handle was instantiated, do not change
-			 * the TRNG parameters.
-			 */
-			if (!(ctrlpriv->rng4_sh_init || inst_handles)) {
-				kick_trng(pdev, ent_delay);
-				ent_delay += 400;
-			}
-			/*
-			 * if instantiate_rng(...) fails, the loop will rerun
-			 * and the kick_trng(...) function will modfiy the
-			 * upper and lower limits of the entropy sampling
-			 * interval, leading to a sucessful initialization of
-			 * the RNG.
-			 */
-			ret = instantiate_rng(dev, inst_handles,
-					      gen_sk);
-		} while ((ret == -EAGAIN) && (ent_delay < RTSDCTL_ENT_DLY_MAX));
-		if (ret) {
-			dev_err(dev, "failed to instantiate RNG");
-			caam_remove(pdev);
-			return ret;
-		}
-		/*
-		 * Set handles init'ed by this module as the complement of the
-		 * already initialized ones
-		 */
-		ctrlpriv->rng4_sh_init = ~ctrlpriv->rng4_sh_init & RDSTA_IFMASK;
-
-		/* Enable RDB bit so that RNG works faster */
-		setbits32(&topregs->ctrl.scfgr, SCFGR_RDBENABLE);
+	ret = caam_rng_init(dev);
+	if (ret) {
+		caam_remove(pdev);
+		return ret;
 	}
 
 	/* NOTE: RTIC detection ought to go here, around Si time */
 
-	caam_id = rd_reg64(&topregs->ctrl.perfmon.caam_id);
+	ctrlpriv->era = caam_get_era();
+
+	caam_id_ms = rd_reg32(&ctrl->perfmon.caam_id_ms);
+	if (caam_id_ms == SEC_ID_MS_T1040)
+		ctrlpriv->errata |= SEC_ERRATUM_A_006899;
+
+	caam_id = (u64)caam_id_ms << 32 |
+		  (u64)rd_reg32(&ctrl->perfmon.caam_id_ls);
 
 	/* Report "alive" for developer to see */
 	dev_info(dev, "device ID = 0x%016llx (Era %d)\n", caam_id,
-		 caam_get_era(caam_id));
+		 ctrlpriv->era);
 	dev_info(dev, "job rings = %d, qi = %d\n",
 		 ctrlpriv->total_jobrs, ctrlpriv->qi_present);
 
@@ -651,6 +762,96 @@ static int caam_probe(struct platform_device *pdev)
 	return 0;
 }
 
+#ifdef CONFIG_PM
+static int caam_stop_qi(struct caam_queue_if __iomem *qi)
+{
+	int qi_stopped, loop = 0;
+
+	setbits32(&qi->qi_control_lo, QICTL_STOP);
+
+	/*
+	 * Wait till QI Job's in Holding tank/deco are completed.
+	 * No dequeue from QI will be happen till QI interface is
+	 * reenabled.
+	 */
+	while (loop <= 100000) {
+		qi_stopped = rd_reg32(&qi->qi_status) &
+			 QISTA_STOPD;
+		if (qi_stopped) {
+			wr_reg32(&qi->qi_control_lo,
+				 QICTL_STOP);
+			return 0;
+		}
+		loop++;
+	}
+
+	/* Failed to stop QI interface. Reenable QI Interface */
+	wr_reg32(&qi->qi_control_lo, QICTL_DQEN);
+	return -EBUSY;
+}
+
+/* Suspend handler for caam device */
+static int caam_suspend(struct device *dev)
+{
+	struct caam_drv_private *caam_priv;
+	struct caam_ctrl __iomem *ctrl;
+	struct caam_queue_if __iomem *qi;
+	int ret = 0;
+
+	caam_priv = dev_get_drvdata(dev);
+	ctrl = caam_priv->ctrl;
+	qi = caam_priv->qi;
+
+	/* QI Interface graceful stoppping during suspend */
+	if (caam_priv->qi_present) {
+		int qi_dqen;
+
+		qi_dqen = rd_reg32(&qi->qi_control_lo) &
+			QICTL_DQEN;
+		if (qi_dqen)
+			ret = caam_stop_qi(qi);
+	}
+
+	return ret;
+}
+
+/* Resume handler for caam device */
+static int caam_resume(struct device *dev)
+{
+	struct caam_drv_private *caam_priv;
+	struct caam_ctrl __iomem *ctrl;
+	struct caam_queue_if __iomem *qi;
+	u32 mcr;
+	int ret;
+
+	caam_priv = dev_get_drvdata(dev);
+	ctrl = caam_priv->ctrl;
+	qi = caam_priv->qi;
+	/*
+	 * Enable DECO watchdogs and, if this is a PHYS_ADDR_T_64BIT kernel,
+	 * long pointers in master configuration register
+	 */
+	mcr = rd_reg32(&ctrl->mcr);
+	mcr = (mcr & ~MCFGR_AWCACHE_MASK) | (0x2 << MCFGR_AWCACHE_SHIFT) |
+	      MCFGR_WDENABLE | (sizeof(dma_addr_t) == sizeof(u64) ?
+				MCFGR_LONG_PTR : 0);
+	wr_reg32(&ctrl->mcr, mcr);
+
+	/* Enable QI interface of SEC */
+	if (caam_priv->qi_present)
+		wr_reg32(&qi->qi_control_lo, QICTL_DQEN);
+
+	ret = caam_rng_init(dev);
+
+	return ret;
+}
+
+const struct dev_pm_ops caam_pm_ops = {
+	.suspend = caam_suspend,
+	.resume = caam_resume,
+};
+#endif /* CONFIG_PM */
+
 static struct of_device_id caam_match[] = {
 	{
 		.compatible = "fsl,sec-v4.0",
@@ -667,6 +868,9 @@ static struct platform_driver caam_driver = {
 		.name = "caam",
 		.owner = THIS_MODULE,
 		.of_match_table = caam_match,
+#ifdef CONFIG_PM
+		.pm = &caam_pm_ops,
+#endif
 	},
 	.probe       = caam_probe,
 	.remove      = caam_remove,
diff --git a/drivers/crypto/caam/desc.h b/drivers/crypto/caam/desc.h
index 7e4500f..e57fefa 100644
--- a/drivers/crypto/caam/desc.h
+++ b/drivers/crypto/caam/desc.h
@@ -2,7 +2,7 @@
  * CAAM descriptor composition header
  * Definitions to support CAAM descriptor instruction generation
  *
- * Copyright 2008-2011 Freescale Semiconductor, Inc.
+ * Copyright 2008-2012 Freescale Semiconductor, Inc.
  */
 
 #ifndef DESC_H
@@ -15,6 +15,7 @@ struct sec4_sg_entry {
 	u32 len;
 	u8 reserved;
 	u8 buf_pool_id;
+#define SEC4_SG_OFFSET_MASK 0x1FFF
 	u16 offset;
 };
 
@@ -37,6 +38,7 @@ struct sec4_sg_entry {
 #define CMD_SEQ_LOAD		(0x03 << CMD_SHIFT)
 #define CMD_FIFO_LOAD		(0x04 << CMD_SHIFT)
 #define CMD_SEQ_FIFO_LOAD	(0x05 << CMD_SHIFT)
+#define CMD_MOVEB		(0x07 << CMD_SHIFT)
 #define CMD_STORE		(0x0a << CMD_SHIFT)
 #define CMD_SEQ_STORE		(0x0b << CMD_SHIFT)
 #define CMD_FIFO_STORE		(0x0c << CMD_SHIFT)
@@ -237,6 +239,7 @@ struct sec4_sg_entry {
 #define LDST_SRCDST_WORD_DESCBUF_SHARED	(0x42 << LDST_SRCDST_SHIFT)
 #define LDST_SRCDST_WORD_DESCBUF_JOB_WE	(0x45 << LDST_SRCDST_SHIFT)
 #define LDST_SRCDST_WORD_DESCBUF_SHARED_WE (0x46 << LDST_SRCDST_SHIFT)
+#define LDST_SRCDST_WORD_INFO_FIFO_SZM	(0x71 << LDST_SRCDST_SHIFT)
 #define LDST_SRCDST_WORD_INFO_FIFO	(0x7a << LDST_SRCDST_SHIFT)
 
 /* Offset in source/destination */
@@ -279,6 +282,55 @@ struct sec4_sg_entry {
 #define LDLEN_SET_OFIFO_OFFSET_SHIFT	0
 #define LDLEN_SET_OFIFO_OFFSET_MASK	(3 << LDLEN_SET_OFIFO_OFFSET_SHIFT)
 
+/* CCB Clear Written Register bits */
+#define CLRW_CLR_C1MODE              0x1
+#define CLRW_CLR_C1DATAS             0x4
+#define CLRW_CLR_C1ICV               0x8
+#define CLRW_CLR_C1CTX               0x20
+#define CLRW_CLR_C1KEY               0x40
+#define CLRW_CLR_PK_A                0x1000
+#define CLRW_CLR_PK_B                0x2000
+#define CLRW_CLR_PK_N                0x4000
+#define CLRW_CLR_PK_E                0x8000
+#define CLRW_CLR_C2MODE              0x10000
+#define CLRW_CLR_C2KEYS              0x20000
+#define CLRW_CLR_C2DATAS             0x40000
+#define CLRW_CLR_C2CTX               0x200000
+#define CLRW_CLR_C2KEY               0x400000
+#define CLRW_RESET_CLS2_DONE         0x04000000u /* era 4 */
+#define CLRW_RESET_CLS1_DONE         0x08000000u /* era 4 */
+#define CLRW_RESET_CLS2_CHA          0x10000000u /* era 4 */
+#define CLRW_RESET_CLS1_CHA          0x20000000u /* era 4 */
+#define CLRW_RESET_OFIFO             0x40000000u /* era 3 */
+#define CLRW_RESET_IFIFO_DFIFO       0x80000000u /* era 3 */
+
+/* CHA Control Register bits */
+#define CCTRL_RESET_CHA_ALL          0x1
+#define CCTRL_RESET_CHA_AESA         0x2
+#define CCTRL_RESET_CHA_DESA         0x4
+#define CCTRL_RESET_CHA_AFHA         0x8
+#define CCTRL_RESET_CHA_KFHA         0x10
+#define CCTRL_RESET_CHA_SF8A         0x20
+#define CCTRL_RESET_CHA_PKHA         0x40
+#define CCTRL_RESET_CHA_MDHA         0x80
+#define CCTRL_RESET_CHA_CRCA         0x100
+#define CCTRL_RESET_CHA_RNG          0x200
+#define CCTRL_RESET_CHA_SF9A         0x400
+#define CCTRL_RESET_CHA_ZUCE         0x800
+#define CCTRL_RESET_CHA_ZUCA         0x1000
+#define CCTRL_UNLOAD_PK_A0           0x10000
+#define CCTRL_UNLOAD_PK_A1           0x20000
+#define CCTRL_UNLOAD_PK_A2           0x40000
+#define CCTRL_UNLOAD_PK_A3           0x80000
+#define CCTRL_UNLOAD_PK_B0           0x100000
+#define CCTRL_UNLOAD_PK_B1           0x200000
+#define CCTRL_UNLOAD_PK_B2           0x400000
+#define CCTRL_UNLOAD_PK_B3           0x800000
+#define CCTRL_UNLOAD_PK_N            0x1000000
+#define CCTRL_UNLOAD_PK_A            0x4000000
+#define CCTRL_UNLOAD_PK_B            0x8000000
+#define CCTRL_UNLOAD_SBOX            0x10000000
+
 /*
  * FIFO_LOAD/FIFO_STORE/SEQ_FIFO_LOAD/SEQ_FIFO_STORE
  * Command Constructs
@@ -404,8 +456,16 @@ struct sec4_sg_entry {
 #define FIFOST_TYPE_MESSAGE_DATA (0x30 << FIFOST_TYPE_SHIFT)
 #define FIFOST_TYPE_RNGSTORE	 (0x34 << FIFOST_TYPE_SHIFT)
 #define FIFOST_TYPE_RNGFIFO	 (0x35 << FIFOST_TYPE_SHIFT)
+#define FIFOST_TYPE_META_DATA	 (0x3e << FIFOST_TYPE_SHIFT)
 #define FIFOST_TYPE_SKIP	 (0x3f << FIFOST_TYPE_SHIFT)
 
+/* AUX field */
+#define FIFOST_AUX_SHIFT		25
+#define FIFOST_AUX_TYPE0	 (0x0 << FIFOST_AUX_SHIFT)
+#define FIFOST_AUX_TYPE1	 (0x1 << FIFOST_AUX_SHIFT)
+#define FIFOST_AUX_TYPE2	 (0x2 << FIFOST_AUX_SHIFT)
+#define FIFOST_AUX_TYPE3	 (0x3 << FIFOST_AUX_SHIFT)
+
 /*
  * OPERATION Command Constructs
  */
@@ -438,6 +498,9 @@ struct sec4_sg_entry {
 #define OP_PCLID_PUBLICKEYPAIR	(0x14 << OP_PCLID_SHIFT)
 #define OP_PCLID_DSASIGN	(0x15 << OP_PCLID_SHIFT)
 #define OP_PCLID_DSAVERIFY	(0x16 << OP_PCLID_SHIFT)
+#define OP_PCLID_DH             (0x17 << OP_PCLID_SHIFT)
+#define OP_PCLID_RSAENC_PUBKEY  (0x18 << OP_PCLID_SHIFT)
+#define OP_PCLID_RSADEC_PRVKEY  (0x19 << OP_PCLID_SHIFT)
 
 /* Assuming OP_TYPE = OP_TYPE_DECAP_PROTOCOL/ENCAP_PROTOCOL */
 #define OP_PCLID_IPSEC		(0x01 << OP_PCLID_SHIFT)
@@ -463,6 +526,7 @@ struct sec4_sg_entry {
 #define OP_PCL_IPSEC_DES_IV64			 0x0100
 #define OP_PCL_IPSEC_DES			 0x0200
 #define OP_PCL_IPSEC_3DES			 0x0300
+#define OP_PCL_IPSEC_NULL_ENC			 0x0b00
 #define OP_PCL_IPSEC_AES_CBC			 0x0c00
 #define OP_PCL_IPSEC_AES_CTR			 0x0d00
 #define OP_PCL_IPSEC_AES_XTS			 0x1600
@@ -1435,9 +1499,12 @@ struct sec4_sg_entry {
 #define MATH_SRC1_REG3		(0x03 << MATH_SRC1_SHIFT)
 #define MATH_SRC1_IMM		(0x04 << MATH_SRC1_SHIFT)
 #define MATH_SRC1_DPOVRD	(0x07 << MATH_SRC0_SHIFT)
+#define MATH_SRC1_VARSEQINLEN	(0x08 << MATH_SRC1_SHIFT)
+#define MATH_SRC1_VARSEQOUTLEN	(0x09 << MATH_SRC1_SHIFT)
 #define MATH_SRC1_INFIFO	(0x0a << MATH_SRC1_SHIFT)
 #define MATH_SRC1_OUTFIFO	(0x0b << MATH_SRC1_SHIFT)
 #define MATH_SRC1_ONE		(0x0c << MATH_SRC1_SHIFT)
+#define MATH_SRC1_ZERO		(0x0f << MATH_SRC1_SHIFT)
 
 /* Destination selectors */
 #define MATH_DEST_SHIFT		8
diff --git a/drivers/crypto/caam/desc_constr.h b/drivers/crypto/caam/desc_constr.h
index cd5f678..1a20bb7 100644
--- a/drivers/crypto/caam/desc_constr.h
+++ b/drivers/crypto/caam/desc_constr.h
@@ -4,6 +4,8 @@
  * Copyright 2008-2012 Freescale Semiconductor, Inc.
  */
 
+#ifndef _DESC_CONSTR_H_
+#define _DESC_CONSTR_H_
 #include "desc.h"
 
 #define IMMEDIATE (1 << 23)
@@ -178,6 +180,7 @@ static inline void append_##cmd(u32 *desc, u32 options) \
 }
 APPEND_CMD(operation, OPERATION)
 APPEND_CMD(move, MOVE)
+APPEND_CMD_RET(moveb, MOVEB)	
 
 #define APPEND_CMD_LEN(cmd, op) \
 static inline void append_##cmd(u32 *desc, unsigned int len, u32 options) \
@@ -188,6 +191,8 @@ static inline void append_##cmd(u32 *desc, unsigned int len, u32 options) \
 APPEND_CMD_LEN(seq_store, SEQ_STORE)
 APPEND_CMD_LEN(seq_fifo_load, SEQ_FIFO_LOAD)
 APPEND_CMD_LEN(seq_fifo_store, SEQ_FIFO_STORE)
+APPEND_CMD_LEN(seq_out_ptr_pre_rto, SEQ_OUT_PTR)
+APPEND_CMD_LEN(seq_in_ptr_pre_rto, SEQ_IN_PTR)
 
 #define APPEND_CMD_PTR(cmd, op) \
 static inline void append_##cmd(u32 *desc, dma_addr_t ptr, unsigned int len, \
@@ -320,7 +325,7 @@ append_cmd(desc, CMD_MATH | MATH_FUN_##op | MATH_DEST_##dest | \
 	APPEND_MATH(LSHIFT, desc, dest, src0, src1, len)
 #define append_math_rshift(desc, dest, src0, src1, len) \
 	APPEND_MATH(RSHIFT, desc, dest, src0, src1, len)
-#define append_math_ldshift(desc, dest, src0, src1, len) \
+#define append_math_shld(desc, dest, src0, src1, len) \
 	APPEND_MATH(SHLD, desc, dest, src0, src1, len)
 
 /* Exactly one source is IMM. Data is passed in as u32 value */
@@ -358,7 +363,7 @@ do { \
 	if (upper) \
 		append_u64(desc, data); \
 	else \
-		append_u32(desc, data); \
+		append_u32(desc, lower_32_bits(data)); \
 } while (0)
 
 #define append_math_add_imm_u64(desc, dest, src0, src1, data) \
@@ -379,3 +384,4 @@ do { \
 	APPEND_MATH_IMM_u64(LSHIFT, desc, dest, src0, src1, data)
 #define append_math_rshift_imm_u64(desc, dest, src0, src1, data) \
 	APPEND_MATH_IMM_u64(RSHIFT, desc, dest, src0, src1, data)
+#endif
diff --git a/drivers/crypto/caam/error.c b/drivers/crypto/caam/error.c
index 0eabd81..0b97425 100644
--- a/drivers/crypto/caam/error.c
+++ b/drivers/crypto/caam/error.c
@@ -112,6 +112,16 @@ static void report_ccb_status(u32 status, char *outstr)
 		SPRINTFCAT(outstr, "unidentified err_id value 0x%02x",
 			   err_id, sizeof("ff"));
 	}
+
+	/*
+	 * CCB ICV check failures are part of normal operation life;
+	 * we leave the upper layers to do what they want with them.
+	 */
+	if (err_id != JRSTA_CCBERR_ERRID_ICVCHK)
+		dev_err(jrdev, "%08x: %s: %s %d: %s%s: %s%s\n",
+			status, error, idx_str, idx,
+			cha_str, cha_err_code,
+			err_str, err_err_code);
 }
 
 static void report_jump_status(u32 status, char *outstr)
diff --git a/drivers/crypto/caam/fsl_jr_uio.c b/drivers/crypto/caam/fsl_jr_uio.c
new file mode 100644
index 0000000..ee640cf
--- /dev/null
+++ b/drivers/crypto/caam/fsl_jr_uio.c
@@ -0,0 +1,273 @@
+/*
+ * Copyright 2013 Freescale Semiconductor, Inc.
+ *
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/of_address.h>
+#include <linux/of_irq.h>
+#include <linux/of_platform.h>
+#include <linux/io.h>
+#include <linux/uio_driver.h>
+#include <linux/slab.h>
+#include <linux/list.h>
+#include "regs.h"
+#include "fsl_jr_uio.h"
+
+static const char jr_uio_version[] = "fsl JR UIO driver v1.0";
+
+#define NAME_LENGTH 30
+#define JR_INDEX_OFFSET 12
+
+static const char uio_device_name[] = "fsl-jr";
+static LIST_HEAD(jr_list);
+
+struct jr_uio_info {
+	atomic_t ref; /* exclusive, only one open() at a time */
+	struct uio_info uio;
+	char name[NAME_LENGTH];
+};
+
+struct jr_dev {
+	u32 revision;
+	u32 index;
+	u32 irq;
+	struct caam_job_ring __iomem *global_regs;
+	struct device *dev;
+	struct resource *res;
+	struct jr_uio_info info;
+	struct list_head node;
+	struct list_head jr_list;
+};
+
+static int jr_uio_open(struct uio_info *info, struct inode *inode)
+{
+	struct jr_uio_info *uio_info = container_of(info,
+					struct jr_uio_info, uio);
+
+	if (!atomic_dec_and_test(&uio_info->ref)) {
+		pr_err("%s: failing non-exclusive open()\n", uio_info->name);
+		atomic_inc(&uio_info->ref);
+		return -EBUSY;
+	}
+
+	return 0;
+}
+
+static int jr_uio_release(struct uio_info *info, struct inode *inode)
+{
+	struct jr_uio_info *uio_info = container_of(info,
+					struct jr_uio_info, uio);
+	atomic_inc(&uio_info->ref);
+
+	return 0;
+}
+
+static irqreturn_t jr_uio_irq_handler(int irq, struct uio_info *dev_info)
+{
+	struct jr_dev *jrdev = dev_info->priv;
+	u32 irqstate;
+	irqstate = rd_reg32(&jrdev->global_regs->jrintstatus);
+	if (!irqstate) {
+		return IRQ_NONE;
+	}
+
+	if (irqstate & JRINT_JR_ERROR)
+		dev_info(jrdev->dev, "uio job ring error - irqstate: %08x\n",
+			 irqstate);
+
+	/*mask valid interrupts */
+	setbits32(&jrdev->global_regs->rconfig_lo, JRCFG_IMSK);
+
+	/* Have valid interrupt at this point, just ACK and trigger */
+	wr_reg32(&jrdev->global_regs->jrintstatus, irqstate);
+
+	return IRQ_HANDLED;
+}
+
+static int jr_uio_irqcontrol(struct uio_info *dev_info, int irqon)
+{
+	struct jr_dev *jrdev = dev_info->priv;
+
+	switch (irqon) {
+	case SEC_UIO_SIMULATE_IRQ_CMD:
+		uio_event_notify(dev_info);
+		break;
+	case SEC_UIO_ENABLE_IRQ_CMD:
+		/* Enable Job Ring interrupt */
+		clrbits32(&jrdev->global_regs->rconfig_lo, JRCFG_IMSK);
+		break;
+	case SEC_UIO_DISABLE_IRQ_CMD:
+		/* Disable Job Ring interrupt */
+		setbits32(&jrdev->global_regs->rconfig_lo, JRCFG_IMSK);
+		break;
+	default:
+		break;
+	}
+	return 0;
+}
+
+static int __init jr_uio_init(struct jr_dev *uio_dev)
+{
+	int ret;
+	struct jr_uio_info *info;
+
+	info = &uio_dev->info;
+	atomic_set(&info->ref, 1);
+	info->uio.version = jr_uio_version;
+	info->uio.name = uio_dev->info.name;
+	info->uio.mem[0].name = "JR config space";
+	info->uio.mem[0].addr = uio_dev->res->start;
+	info->uio.mem[0].size = uio_dev->res->end - uio_dev->res->start + 1;
+	info->uio.mem[0].internal_addr = uio_dev->global_regs;
+	info->uio.mem[0].memtype = UIO_MEM_PHYS;
+	info->uio.irq = uio_dev->irq;
+	info->uio.irq_flags = IRQF_SHARED;
+	info->uio.handler = jr_uio_irq_handler;
+	info->uio.irqcontrol = jr_uio_irqcontrol;
+	info->uio.open = jr_uio_open;
+	info->uio.release = jr_uio_release;
+	info->uio.priv = uio_dev;
+
+	ret = uio_register_device(uio_dev->dev, &info->uio);
+	if (ret) {
+		pr_err("jr_uio: UIO registration failed\n");
+		return ret;
+	}
+
+	return 0;
+}
+
+static const struct of_device_id jr_ids[] = {
+	{ .compatible = "fsl,sec-v4.0-job-ring", },
+	{ .compatible = "fsl,sec-v4.4-job-ring", },
+	{ .compatible = "fsl,sec-v5.0-job-ring", },
+	{ .compatible = "fsl,sec-v6.0-job-ring", },
+	{},
+};
+
+static int fsl_jr_probe(struct platform_device *dev)
+{
+	struct resource regs;
+	struct jr_dev *jr_dev;
+	struct device_node *jr_node;
+	int ret, count = 0;
+	struct list_head *p;
+
+	jr_node = dev->dev.of_node;
+	if (!jr_node) {
+		dev_err(&dev->dev, "Device OF-Node is NULL\n");
+		return -EFAULT;
+	}
+
+	jr_dev = devm_kzalloc(&dev->dev, sizeof(struct jr_dev), GFP_KERNEL);
+	if (!jr_dev) {
+		dev_err(&dev->dev, "kzalloc failed\n");
+		return -ENOMEM;
+	}
+
+	/* Creat name and index */
+	list_for_each(p, &jr_list) {
+		count++;
+	}
+	jr_dev->index = count;
+
+	snprintf(jr_dev->info.name, sizeof(jr_dev->info.name) - 1,
+		 "%s%d", uio_device_name, jr_dev->index);
+
+	jr_dev->dev = &dev->dev;
+	platform_set_drvdata(dev, jr_dev);
+
+	/* Get the resource from dtb node */
+	ret = of_address_to_resource(jr_node, 0, &regs);
+	if (unlikely(ret < 0)) {
+		dev_err(&dev->dev, "OF-Address-to-resource Failed\n");
+		ret = -EFAULT;
+		goto abort;
+	}
+
+	jr_dev->res = devm_request_mem_region(&dev->dev, regs.start,
+			regs.end-regs.start + 1,
+			jr_dev->info.name);
+	if (unlikely(!jr_dev->res)) {
+		dev_err(jr_dev->dev, "devm_request_mem_region failed\n");
+		ret = -ENOMEM;
+		goto abort;
+	}
+
+	jr_dev->global_regs = devm_ioremap(&dev->dev, jr_dev->res->start,
+			jr_dev->res->end-jr_dev->res->start+1);
+	if (unlikely(jr_dev->global_regs == 0)) {
+		dev_err(jr_dev->dev, "devm_ioremap failed\n");
+		ret = -EIO;
+		goto abort;
+	}
+	jr_dev->irq = irq_of_parse_and_map(jr_node, 0);
+	dev_dbg(jr_dev->dev, "errirq: %d\n", jr_dev->irq);
+
+	/* Register UIO */
+	ret = jr_uio_init(jr_dev);
+	if (ret) {
+		dev_err(&dev->dev, "UIO init Failed\n");
+		goto abort;
+	}
+
+	list_add_tail(&jr_dev->node, &jr_list);
+
+	dev_info(jr_dev->dev, "UIO device full name %s initialized\n",
+		 jr_dev->info.name);
+
+	return 0;
+
+abort:
+	return ret;
+}
+
+static int fsl_jr_remove(struct platform_device *dev)
+{
+	struct jr_dev *jr_dev = platform_get_drvdata(dev);
+
+	if (!jr_dev)
+		return 0;
+
+	list_del(&jr_dev->node);
+	uio_unregister_device(&jr_dev->info.uio);
+	platform_set_drvdata(dev, NULL);
+
+	return 0;
+}
+
+static struct platform_driver fsl_jr_driver = {
+	.driver = {
+		.name = "fsl-jr-uio",
+		.owner = THIS_MODULE,
+		.of_match_table = jr_ids,
+	},
+	.probe = fsl_jr_probe,
+	.remove = fsl_jr_remove,
+};
+
+static __init int fsl_jr_init(void)
+{
+	int ret;
+
+	ret = platform_driver_register(&fsl_jr_driver);
+	if (unlikely(ret < 0))
+		pr_warn(": %s:%hu:%s(): platform_driver_register() = %d\n",
+			__FILE__, __LINE__, __func__, ret);
+
+	return ret;
+}
+
+static void __exit fsl_jr_exit(void)
+{
+	platform_driver_unregister(&fsl_jr_driver);
+}
+
+module_init(fsl_jr_init);
+module_exit(fsl_jr_exit);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Ruchika Gupta <ruchika.gupta@freescale.com>");
+MODULE_DESCRIPTION("FSL SEC UIO Driver");
diff --git a/drivers/crypto/caam/fsl_jr_uio.h b/drivers/crypto/caam/fsl_jr_uio.h
new file mode 100644
index 0000000..2c4b7c4
--- /dev/null
+++ b/drivers/crypto/caam/fsl_jr_uio.h
@@ -0,0 +1,20 @@
+/*
+ * CAAM Job RING UIO support header file
+ *
+ * Copyright 2013 Freescale Semiconductor, Inc
+ */
+
+#ifndef FSL_JR_UIO_H
+#define FSL_JR_UIO_H
+
+/** UIO command used by user-space driver to request
+ *  disabling IRQs on a certain job ring */
+#define SEC_UIO_DISABLE_IRQ_CMD         0
+/** UIO command used by user-space driver to request
+ *  enabling IRQs on a certain job ring */
+#define SEC_UIO_ENABLE_IRQ_CMD          1
+/** UIO command used by user-space driver to request SEC kernel driver
+ *  to simulate that an IRQ is generated on a certain job ring */
+#define SEC_UIO_SIMULATE_IRQ_CMD        2
+
+#endif
diff --git a/drivers/crypto/caam/intern.h b/drivers/crypto/caam/intern.h
index 6d85fcc..4c46efc 100644
--- a/drivers/crypto/caam/intern.h
+++ b/drivers/crypto/caam/intern.h
@@ -2,7 +2,7 @@
  * CAAM/SEC 4.x driver backend
  * Private/internal definitions between modules
  *
- * Copyright 2008-2011 Freescale Semiconductor, Inc.
+ * Copyright 2008-2012 Freescale Semiconductor, Inc.
  *
  */
 
@@ -23,6 +23,8 @@
 #define JOBR_INTC_COUNT_THLD 0
 #endif
 
+#define CAAM_NAPI_WEIGHT	63
+
 /*
  * Storage for tracking each in-process entry moving across a ring
  * Each entry on an output ring needs one of these
@@ -41,7 +43,8 @@ struct caam_drv_private_jr {
 	struct device		*dev;
 	int ridx;
 	struct caam_job_ring __iomem *rregs;	/* JobR's register space */
-	struct tasklet_struct irqtask;
+	struct napi_struct __percpu *irqtask;
+	struct net_device __percpu *net_dev;
 	int irq;			/* One per queue */
 
 	/* Number of scatterlist crypt transforms active on the JobR */
@@ -54,6 +57,8 @@ struct caam_drv_private_jr {
 	int inp_ring_write_index;	/* Input index "tail" */
 	int head;			/* entinfo (s/w ring) head index */
 	dma_addr_t *inpring;	/* Base of input ring, alloc DMA-safe */
+	dma_addr_t inpbusaddr;	/* Input ring physical address */
+	dma_addr_t outbusaddr;	/* Output ring physical address */
 	spinlock_t outlock ____cacheline_aligned; /* Output ring index lock */
 	int out_ring_read_index;	/* Output index "tail" */
 	int tail;			/* entinfo (s/w ring) tail index */
@@ -67,6 +72,9 @@ struct caam_drv_private {
 
 	struct device *dev;
 	struct platform_device **jrpdev; /* Alloc'ed array per sub-device */
+#ifdef CONFIG_CAAM_QI
+	struct device *qidev;
+#endif
 	struct platform_device *pdev;
 
 	/* Physical-presence section */
@@ -82,6 +90,16 @@ struct caam_drv_private {
 	u8 total_jobrs;		/* Total Job Rings in device */
 	u8 qi_present;		/* Nonzero if QI present in device */
 	int secvio_irq;		/* Security violation interrupt number */
+	/* list of registered pkc algorithms */
+	struct list_head pkc_list;
+
+	int era;		/* SEC Era */
+
+#define SEC_ERRATUM_A_006899 0x01
+	/* SEC errata bitmask */
+	u32 errata;
+
+	int virt_en;            /* Virtualization enabled in CAAM */
 
 #define	RNG4_MAX_HANDLES 2
 	/* RNG4 block */
diff --git a/drivers/crypto/caam/jr.c b/drivers/crypto/caam/jr.c
index b512a4b..1854972 100644
--- a/drivers/crypto/caam/jr.c
+++ b/drivers/crypto/caam/jr.c
@@ -68,23 +68,25 @@ static int caam_reset_hw_jr(struct device *dev)
 int caam_jr_shutdown(struct device *dev)
 {
 	struct caam_drv_private_jr *jrp = dev_get_drvdata(dev);
-	dma_addr_t inpbusaddr, outbusaddr;
-	int ret;
+	int i, ret;
 
 	ret = caam_reset_hw_jr(dev);
 
-	tasklet_kill(&jrp->irqtask);
+	for_each_possible_cpu(i) {
+		napi_disable(per_cpu_ptr(jrp->irqtask, i));
+		netif_napi_del(per_cpu_ptr(jrp->irqtask, i));
+	}
+
+	free_percpu(jrp->irqtask);
+	free_percpu(jrp->net_dev);
 
 	/* Release interrupt */
 	free_irq(jrp->irq, dev);
 
-	/* Free rings */
-	inpbusaddr = rd_reg64(&jrp->rregs->inpring_base);
-	outbusaddr = rd_reg64(&jrp->rregs->outring_base);
 	dma_free_coherent(dev, sizeof(dma_addr_t) * JOBR_DEPTH,
-			  jrp->inpring, inpbusaddr);
+			  jrp->inpring, jrp->inpbusaddr);
 	dma_free_coherent(dev, sizeof(struct jr_outentry) * JOBR_DEPTH,
-			  jrp->outring, outbusaddr);
+			  jrp->outring, jrp->outbusaddr);
 	kfree(jrp->entinfo);
 
 	return ret;
@@ -153,86 +155,100 @@ static irqreturn_t caam_jr_interrupt(int irq, void *st_dev)
 	wr_reg32(&jrp->rregs->jrintstatus, irqstate);
 
 	preempt_disable();
-	tasklet_schedule(&jrp->irqtask);
+	napi_schedule(per_cpu_ptr(jrp->irqtask, smp_processor_id()));
 	preempt_enable();
 
 	return IRQ_HANDLED;
 }
 
-/* Deferred service handler, run as interrupt-fired tasklet */
-static void caam_jr_dequeue(unsigned long devarg)
+/* Consume the processed output ring Job */
+static inline void caam_jr_consume(struct device *dev)
 {
 	int hw_idx, sw_idx, i, head, tail;
-	struct device *dev = (struct device *)devarg;
 	struct caam_drv_private_jr *jrp = dev_get_drvdata(dev);
 	void (*usercall)(struct device *dev, u32 *desc, u32 status, void *arg);
 	u32 *userdesc, userstatus;
 	void *userarg;
 
-	while (rd_reg32(&jrp->rregs->outring_used)) {
+	head = ACCESS_ONCE(jrp->head);
+	spin_lock(&jrp->outlock);
+
+	sw_idx = jrp->tail;
+	tail = jrp->tail;
+	hw_idx = jrp->out_ring_read_index;
+
+	for (i = 0; CIRC_CNT(head, tail + i, JOBR_DEPTH) >= 1; i++) {
+		sw_idx = (tail + i) & (JOBR_DEPTH - 1);
+
+		/* Read Barrier for desc address comparision */
+		smp_read_barrier_depends();
+		if (jrp->outring[hw_idx].desc ==
+		    rd_en_dma(jrp->entinfo[sw_idx].desc_addr_dma))
+			break; /* found */
+	}
+	/* we should never fail to find a matching descriptor */
+	BUG_ON(CIRC_CNT(head, tail + i, JOBR_DEPTH) <= 0);
+
+	/* Unmap just-run descriptor so we can post-process */
+	dma_unmap_single(dev, jrp->outring[hw_idx].desc,
+			 jrp->entinfo[sw_idx].desc_size,
+			 DMA_TO_DEVICE);
 
-		head = ACCESS_ONCE(jrp->head);
+	/* mark completed, avoid matching on a recycled desc addr */
+	jrp->entinfo[sw_idx].desc_addr_dma = 0;
 
-		spin_lock(&jrp->outlock);
+	/* Stash callback params for use outside of lock */
+	usercall = jrp->entinfo[sw_idx].callbk;
+	userarg = jrp->entinfo[sw_idx].cbkarg;
+	userdesc = jrp->entinfo[sw_idx].desc_addr_virt;
+	userstatus = rd_en_val32(jrp->outring[hw_idx].jrstatus);
 
-		sw_idx = tail = jrp->tail;
-		hw_idx = jrp->out_ring_read_index;
+	/* set done */
+	wr_reg32(&jrp->rregs->outring_rmvd, 1);
 
-		for (i = 0; CIRC_CNT(head, tail + i, JOBR_DEPTH) >= 1; i++) {
-			sw_idx = (tail + i) & (JOBR_DEPTH - 1);
+	jrp->out_ring_read_index = (jrp->out_ring_read_index + 1) &
+				   (JOBR_DEPTH - 1);
 
+	/*
+	 * if this job completed out-of-order, do not increment
+	 * the tail.  Otherwise, increment tail by 1 plus the
+	 * number of subsequent jobs already completed out-of-order
+	 */
+	if (sw_idx == tail) {
+		do {
+			tail = (tail + 1) & (JOBR_DEPTH - 1);
 			smp_read_barrier_depends();
+		} while (CIRC_CNT(head, tail, JOBR_DEPTH) >= 1 &&
+			 jrp->entinfo[tail].desc_addr_dma == 0);
 
-			if (jrp->outring[hw_idx].desc ==
-			    jrp->entinfo[sw_idx].desc_addr_dma)
-				break; /* found */
-		}
-		/* we should never fail to find a matching descriptor */
-		BUG_ON(CIRC_CNT(head, tail + i, JOBR_DEPTH) <= 0);
-
-		/* Unmap just-run descriptor so we can post-process */
-		dma_unmap_single(dev, jrp->outring[hw_idx].desc,
-				 jrp->entinfo[sw_idx].desc_size,
-				 DMA_TO_DEVICE);
-
-		/* mark completed, avoid matching on a recycled desc addr */
-		jrp->entinfo[sw_idx].desc_addr_dma = 0;
-
-		/* Stash callback params for use outside of lock */
-		usercall = jrp->entinfo[sw_idx].callbk;
-		userarg = jrp->entinfo[sw_idx].cbkarg;
-		userdesc = jrp->entinfo[sw_idx].desc_addr_virt;
-		userstatus = jrp->outring[hw_idx].jrstatus;
-
-		/* set done */
-		wr_reg32(&jrp->rregs->outring_rmvd, 1);
-
-		jrp->out_ring_read_index = (jrp->out_ring_read_index + 1) &
-					   (JOBR_DEPTH - 1);
-
-		/*
-		 * if this job completed out-of-order, do not increment
-		 * the tail.  Otherwise, increment tail by 1 plus the
-		 * number of subsequent jobs already completed out-of-order
-		 */
-		if (sw_idx == tail) {
-			do {
-				tail = (tail + 1) & (JOBR_DEPTH - 1);
-				smp_read_barrier_depends();
-			} while (CIRC_CNT(head, tail, JOBR_DEPTH) >= 1 &&
-				 jrp->entinfo[tail].desc_addr_dma == 0);
-
-			jrp->tail = tail;
-		}
+		jrp->tail = tail;
+	}
 
-		spin_unlock(&jrp->outlock);
+	spin_unlock(&jrp->outlock);
 
-		/* Finally, execute user's callback */
-		usercall(dev, userdesc, userstatus, userarg);
+	/* Finally, execute user's callback */
+	usercall(dev, userdesc, userstatus, userarg);
+}
+
+/* Deferred service handler, run as interrupt-fired tasklet */
+static int caam_jr_dequeue(struct napi_struct *napi, int budget)
+{
+	struct device *dev = &napi->dev->dev;
+	struct caam_drv_private_jr *jrp = dev_get_drvdata(dev);
+	int cleaned = 0;
+
+	while (rd_reg32(&jrp->rregs->outring_used) && cleaned < budget) {
+		caam_jr_consume(dev);
+		cleaned++;
 	}
 
-	/* reenable / unmask IRQs */
-	clrbits32(&jrp->rregs->rconfig_lo, JRCFG_IMSK);
+	if (cleaned < budget) {
+		napi_complete(per_cpu_ptr(jrp->irqtask, smp_processor_id()));
+		/* reenable / unmask IRQs */
+		clrbits32(&jrp->rregs->rconfig_lo, JRCFG_IMSK);
+	}
+
+	return cleaned;
 }
 
 /**
@@ -326,7 +342,7 @@ int caam_jr_enqueue(struct device *dev, u32 *desc,
 	int head, tail, desc_size;
 	dma_addr_t desc_dma;
 
-	desc_size = (*desc & HDR_JD_LENGTH_MASK) * sizeof(u32);
+	desc_size = (rd_en_val32(*desc) & HDR_JD_LENGTH_MASK) * sizeof(u32);
 	desc_dma = dma_map_single(dev, desc, desc_size, DMA_TO_DEVICE);
 	if (dma_mapping_error(dev, desc_dma)) {
 		dev_err(dev, "caam_jr_enqueue(): can't map jobdesc\n");
@@ -352,7 +368,7 @@ int caam_jr_enqueue(struct device *dev, u32 *desc,
 	head_entry->cbkarg = areq;
 	head_entry->desc_addr_dma = desc_dma;
 
-	jrp->inpring[jrp->inp_ring_write_index] = desc_dma;
+	jrp->inpring[jrp->inp_ring_write_index] = wr_en_dma(desc_dma);
 
 	smp_wmb();
 
@@ -368,18 +384,158 @@ int caam_jr_enqueue(struct device *dev, u32 *desc,
 }
 EXPORT_SYMBOL(caam_jr_enqueue);
 
+#ifdef CONFIG_PM
+/* Return Failure for Job pending in input ring */
+static void caam_fail_inpjobs(struct device *dev)
+{
+	struct caam_drv_private_jr *jrp = dev_get_drvdata(dev);
+	void (*usercall)(struct device *dev, u32 *desc, u32 status, void *arg);
+	u32 *userdesc;
+	void *userarg;
+	int sw_idx;
+
+	/* Check for jobs left after reaching output ring and return error */
+	for (sw_idx = 0; sw_idx < JOBR_DEPTH; sw_idx++) {
+		if (jrp->entinfo[sw_idx].desc_addr_dma) {
+			usercall = jrp->entinfo[sw_idx].callbk;
+			userarg = jrp->entinfo[sw_idx].cbkarg;
+			userdesc = jrp->entinfo[sw_idx].desc_addr_virt;
+			usercall(dev, userdesc, -EIO, userarg);
+			jrp->entinfo[sw_idx].desc_addr_dma = 0;
+		}
+	}
+}
+
+/* Suspend handler for Job Ring */
+static int jr_suspend(struct device *dev)
+{
+	struct caam_drv_private_jr *jrp = dev_get_drvdata(dev);
+	unsigned int timeout = 100000;
+	int ret = 0;
+
+	/*
+	 * mask interrupts since we are going to poll
+	 * for reset completion status
+	 */
+	setbits32(&jrp->rregs->rconfig_lo, JRCFG_IMSK);
+
+	/*
+	 * Process all the pending completed Jobs to make room for
+	 * Job's coming to Outring during flush. This gives caam
+	 * some more space in outring and process some more Job's
+	 */
+	while (rd_reg32(&jrp->rregs->outring_used))
+		caam_jr_consume(dev);
+
+	/* initiate flush (required prior to reset) */
+	wr_reg32(&jrp->rregs->jrcommand, JRCR_RESET);
+	while (((rd_reg32(&jrp->rregs->jrintstatus) & JRINT_ERR_HALT_MASK) ==
+		JRINT_ERR_HALT_INPROGRESS) && --timeout)
+		cpu_relax();
+
+	if ((rd_reg32(&jrp->rregs->jrintstatus) & JRINT_ERR_HALT_MASK) !=
+	    JRINT_ERR_HALT_COMPLETE || timeout == 0) {
+		dev_err(dev, "failed to flush job ring %d\n", jrp->ridx);
+		ret = -EIO;
+		goto err;
+	}
+
+	/*
+	 * Disallow any further addition in Job Ring by making input_ring
+	 * size ZERO. If output complete ring processing try to enqueue
+	 * more Job's back to JR, it will return -EBUSY
+	 */
+	wr_reg32(&jrp->rregs->inpring_size, 0);
+
+	while (rd_reg32(&jrp->rregs->outring_used))
+		caam_jr_consume(dev);
+
+	/* Its too late, now newely added jobs will be failed */
+	caam_fail_inpjobs(dev);
+
+	/* initiate reset */
+	timeout = 100000;
+	wr_reg32(&jrp->rregs->jrcommand, JRCR_RESET);
+	while ((rd_reg32(&jrp->rregs->jrcommand) & JRCR_RESET) && --timeout)
+		cpu_relax();
+
+	if (timeout == 0) {
+		dev_err(dev, "failed to reset job ring %d\n", jrp->ridx);
+		ret = -EIO;
+		goto err;
+	}
+
+err:
+	/* unmask interrupts */
+	clrbits32(&jrp->rregs->rconfig_lo, JRCFG_IMSK);
+	return ret;
+}
+
+/* Resume handler for Job Ring */
+static int jr_resume(struct device *dev)
+{
+	struct caam_drv_private_jr *jrp;
+
+	jrp = dev_get_drvdata(dev);
+
+	memset(jrp->entinfo, 0, sizeof(struct caam_jrentry_info) * JOBR_DEPTH);
+
+	/* Setup rings */
+	jrp->inp_ring_write_index = 0;
+	jrp->out_ring_read_index = 0;
+	jrp->head = 0;
+	jrp->tail = 0;
+
+	/* Setup ring base registers */
+	wr_reg64(&jrp->rregs->inpring_base, jrp->inpbusaddr);
+	wr_reg64(&jrp->rregs->outring_base, jrp->outbusaddr);
+	/* Setup ring size */
+	wr_reg32(&jrp->rregs->inpring_size, JOBR_DEPTH);
+	wr_reg32(&jrp->rregs->outring_size, JOBR_DEPTH);
+
+	setbits32(&jrp->rregs->rconfig_lo, JOBR_INTC |
+		  (JOBR_INTC_COUNT_THLD << JRCFG_ICDCT_SHIFT) |
+		  (JOBR_INTC_TIME_THLD << JRCFG_ICTT_SHIFT));
+	return 0;
+}
+
+const struct dev_pm_ops jr_pm_ops = {
+	.suspend = jr_suspend,
+	.resume = jr_resume,
+};
+#endif /* CONFIG_PM */
+
 /*
  * Init JobR independent of platform property detection
  */
 static int caam_jr_init(struct device *dev)
 {
 	struct caam_drv_private_jr *jrp;
-	dma_addr_t inpbusaddr, outbusaddr;
 	int i, error;
 
 	jrp = dev_get_drvdata(dev);
+	/* Connect job ring interrupt handler. */
+	jrp->irqtask = alloc_percpu(struct napi_struct);
+	if (!jrp->irqtask) {
+		dev_err(dev, "can't allocate percpu irqtask memory\n");
+		return -ENOMEM;
+	}
+
+	jrp->net_dev = alloc_percpu(struct net_device);
+	if (!jrp->net_dev) {
+		dev_err(dev, "can't allocate percpu net_dev memory\n");
+		free_percpu(jrp->irqtask);
+		return -ENOMEM;
+	}
 
-	tasklet_init(&jrp->irqtask, caam_jr_dequeue, (unsigned long)dev);
+	for_each_possible_cpu(i) {
+		(per_cpu_ptr(jrp->net_dev, i))->dev = *dev;
+		INIT_LIST_HEAD(&per_cpu_ptr(jrp->net_dev, i)->napi_list);
+		netif_napi_add(per_cpu_ptr(jrp->net_dev, i),
+			       per_cpu_ptr(jrp->irqtask, i),
+			       caam_jr_dequeue, CAAM_NAPI_WEIGHT);
+		napi_enable(per_cpu_ptr(jrp->irqtask, i));
+	}
 
 	/* Connect job ring interrupt handler. */
 	error = request_irq(jrp->irq, caam_jr_interrupt, IRQF_SHARED,
@@ -397,10 +553,11 @@ static int caam_jr_init(struct device *dev)
 		return error;
 
 	jrp->inpring = dma_alloc_coherent(dev, sizeof(dma_addr_t) * JOBR_DEPTH,
-					  &inpbusaddr, GFP_KERNEL);
+					  &jrp->inpbusaddr, GFP_KERNEL);
 
 	jrp->outring = dma_alloc_coherent(dev, sizeof(struct jr_outentry) *
-					  JOBR_DEPTH, &outbusaddr, GFP_KERNEL);
+					  JOBR_DEPTH, &jrp->outbusaddr,
+					  GFP_KERNEL);
 
 	jrp->entinfo = kzalloc(sizeof(struct caam_jrentry_info) * JOBR_DEPTH,
 			       GFP_KERNEL);
@@ -412,17 +569,14 @@ static int caam_jr_init(struct device *dev)
 		return -ENOMEM;
 	}
 
-	for (i = 0; i < JOBR_DEPTH; i++)
-		jrp->entinfo[i].desc_addr_dma = !0;
-
 	/* Setup rings */
 	jrp->inp_ring_write_index = 0;
 	jrp->out_ring_read_index = 0;
 	jrp->head = 0;
 	jrp->tail = 0;
 
-	wr_reg64(&jrp->rregs->inpring_base, inpbusaddr);
-	wr_reg64(&jrp->rregs->outring_base, outbusaddr);
+	wr_reg64(&jrp->rregs->inpring_base, jrp->inpbusaddr);
+	wr_reg64(&jrp->rregs->outring_base, jrp->outbusaddr);
 	wr_reg32(&jrp->rregs->inpring_size, JOBR_DEPTH);
 	wr_reg32(&jrp->rregs->outring_size, JOBR_DEPTH);
 
@@ -483,7 +637,7 @@ static int caam_jr_probe(struct platform_device *pdev)
 		dma_set_mask(jrdev, DMA_BIT_MASK(32));
 
 	/* Identify the interrupt */
-	jrpriv->irq = irq_of_parse_and_map(nprop, 0);
+	jrpriv->irq = of_irq_to_resource(nprop, 0, NULL);
 
 	/* Now do the platform independent part */
 	error = caam_jr_init(jrdev); /* now turn on hardware */
@@ -516,6 +670,9 @@ static struct platform_driver caam_jr_driver = {
 		.name = "caam_jr",
 		.owner = THIS_MODULE,
 		.of_match_table = caam_jr_match,
+#ifdef CONFIG_PM
+		.pm = &jr_pm_ops,
+#endif
 	},
 	.probe       = caam_jr_probe,
 	.remove      = caam_jr_remove,
diff --git a/drivers/crypto/caam/key_gen.c b/drivers/crypto/caam/key_gen.c
index b872eed..ea2e406 100644
--- a/drivers/crypto/caam/key_gen.c
+++ b/drivers/crypto/caam/key_gen.c
@@ -51,29 +51,23 @@ int gen_split_key(struct device *jrdev, u8 *key_out, int split_key_len,
 	u32 *desc;
 	struct split_key_result result;
 	dma_addr_t dma_addr_in, dma_addr_out;
-	int ret = -ENOMEM;
+	int ret = 0;
 
 	desc = kmalloc(CAAM_CMD_SZ * 6 + CAAM_PTR_SZ * 2, GFP_KERNEL | GFP_DMA);
 	if (!desc) {
 		dev_err(jrdev, "unable to allocate key input memory\n");
-		return ret;
+		return -ENOMEM;
 	}
 
+	init_job_desc(desc, 0);
+
 	dma_addr_in = dma_map_single(jrdev, (void *)key_in, keylen,
 				     DMA_TO_DEVICE);
 	if (dma_mapping_error(jrdev, dma_addr_in)) {
 		dev_err(jrdev, "unable to map key input memory\n");
-		goto out_free;
+		kfree(desc);
+		return -ENOMEM;
 	}
-
-	dma_addr_out = dma_map_single(jrdev, key_out, split_key_pad_len,
-				      DMA_FROM_DEVICE);
-	if (dma_mapping_error(jrdev, dma_addr_out)) {
-		dev_err(jrdev, "unable to map key output memory\n");
-		goto out_unmap_in;
-	}
-
-	init_job_desc(desc, 0);
 	append_key(desc, dma_addr_in, keylen, CLASS_2 | KEY_DEST_CLASS_REG);
 
 	/* Sets MDHA up into an HMAC-INIT */
@@ -90,6 +84,13 @@ int gen_split_key(struct device *jrdev, u8 *key_out, int split_key_len,
 	 * FIFO_STORE with the explicit split-key content store
 	 * (0x26 output type)
 	 */
+	dma_addr_out = dma_map_single(jrdev, key_out, split_key_pad_len,
+				      DMA_FROM_DEVICE);
+	if (dma_mapping_error(jrdev, dma_addr_out)) {
+		dev_err(jrdev, "unable to map key output memory\n");
+		kfree(desc);
+		return -ENOMEM;
+	}
 	append_fifo_store(desc, dma_addr_out, split_key_len,
 			  LDST_CLASS_2_CCB | FIFOST_TYPE_SPLIT_KEK);
 
@@ -117,10 +118,10 @@ int gen_split_key(struct device *jrdev, u8 *key_out, int split_key_len,
 
 	dma_unmap_single(jrdev, dma_addr_out, split_key_pad_len,
 			 DMA_FROM_DEVICE);
-out_unmap_in:
 	dma_unmap_single(jrdev, dma_addr_in, keylen, DMA_TO_DEVICE);
-out_free:
+
 	kfree(desc);
+
 	return ret;
 }
 EXPORT_SYMBOL(gen_split_key);
diff --git a/drivers/crypto/caam/key_gen.h b/drivers/crypto/caam/key_gen.h
index c5588f6..8ce751b 100644
--- a/drivers/crypto/caam/key_gen.h
+++ b/drivers/crypto/caam/key_gen.h
@@ -1,10 +1,11 @@
 /*
  * CAAM/SEC 4.x definitions for handling key-generation jobs
  *
- * Copyright 2008-2011 Freescale Semiconductor, Inc.
+ * Copyright 2008-2012 Freescale Semiconductor, Inc.
  *
  */
-
+#ifndef _KEY_GEN_H_
+#define _KEY_GEN_H_
 struct split_key_result {
 	struct completion completion;
 	int err;
@@ -15,3 +16,4 @@ void split_key_done(struct device *dev, u32 *desc, u32 err, void *context);
 int gen_split_key(struct device *jrdev, u8 *key_out, int split_key_len,
 		    int split_key_pad_len, const u8 *key_in, u32 keylen,
 		    u32 alg_op);
+#endif
diff --git a/drivers/crypto/caam/pdb.h b/drivers/crypto/caam/pdb.h
index 3a87c0c..b794e2b 100644
--- a/drivers/crypto/caam/pdb.h
+++ b/drivers/crypto/caam/pdb.h
@@ -66,9 +66,15 @@ struct ipsec_encap_ctr {
 
 struct ipsec_encap_ccm {
 	u32 salt; /* lower 24 bits */
+#ifdef __BIG_ENDIAN
 	u8 b0_flags;
 	u8 ctr_flags;
 	u16 ctr_initial;
+#else
+	u16 ctr_initial;
+	u8 ctr_flags;
+	u8 b0_flags;
+#endif
 	u32 iv[2];
 };
 
@@ -79,10 +85,17 @@ struct ipsec_encap_gcm {
 };
 
 struct ipsec_encap_pdb {
+#ifdef __BIG_ENDIAN
 	u8 hmo_rsvd;
 	u8 ip_nh;
 	u8 ip_nh_offset;
 	u8 options;
+#else
+	u8 options;
+	u8 ip_nh_offset;
+	u8 ip_nh;
+	u8 hmo_rsvd;
+#endif
 	u32 seq_num_ext_hi;
 	u32 seq_num;
 	union {
@@ -92,8 +105,13 @@ struct ipsec_encap_pdb {
 		struct ipsec_encap_gcm gcm;
 	};
 	u32 spi;
+#ifdef __BIG_ENDIAN
 	u16 rsvd1;
 	u16 ip_hdr_len;
+#else
+	u16 ip_hdr_len;
+	u16 rsvd1;
+#endif
 	u32 ip_hdr[0]; /* optional IP Header content */
 };
 
@@ -108,9 +126,15 @@ struct ipsec_decap_ctr {
 
 struct ipsec_decap_ccm {
 	u32 salt;
+#ifdef __BIG_ENDIAN
 	u8 iv_flags;
 	u8 ctr_flags;
 	u16 ctr_initial;
+#else
+	u16 ctr_initial;
+	u8 ctr_flags;
+	u8 iv_flags;
+#endif
 };
 
 struct ipsec_decap_gcm {
@@ -119,9 +143,15 @@ struct ipsec_decap_gcm {
 };
 
 struct ipsec_decap_pdb {
+#ifdef __BIG_ENDIAN
 	u16 hmo_ip_hdr_len;
 	u8 ip_nh_offset;
 	u8 options;
+#else
+	u8 options;
+	u8 ip_nh_offset;
+	u16 hmo_ip_hdr_len;
+#endif
 	union {
 		struct ipsec_decap_cbc cbc;
 		struct ipsec_decap_ctr ctr;
@@ -285,7 +315,7 @@ struct dtls_block_encap_pdb {
 	u16 epoch;
 	u16 seq_num[3];
 	u32 iv[4];
-};
+} __packed;
 
 struct tls_block_decap_pdb {
 	u8 rsvd[3];
@@ -310,7 +340,7 @@ struct dtls_block_decap_pdb {
 	u16 seq_num[3];
 	u32 iv[4];
 	u64 antireplay_scorecard;
-};
+} __packed;
 
 /*
  * SRTP Protocol Data Blocks
@@ -373,30 +403,204 @@ struct srtp_decap_pdb {
 
 #define DSA_PDB_N_MASK		0x7f
 
-struct dsa_sign_pdb {
-	u32 sgf_ln; /* Use DSA_PDB_ defintions per above */
-	u8 *q;
-	u8 *r;
-	u8 *g;	/* or Gx,y */
-	u8 *s;
-	u8 *f;
-	u8 *c;
-	u8 *d;
-	u8 *ab; /* ECC only */
-	u8 *u;
-};
+struct dsa_sign_desc_s {
+	uint32_t	desc_hdr;
+	uint32_t sgf_ln; /* Use DSA_PDB_ definitions per above */
+	dma_addr_t q_dma;
+	dma_addr_t r_dma;
+	dma_addr_t g_dma;	/* or Gx,y */
+	dma_addr_t s_dma;
+	dma_addr_t f_dma;
+	dma_addr_t c_dma;
+	dma_addr_t d_dma;
+	uint32_t	op;
+} __packed;
+
+struct dsa_verify_desc_s {
+	uint32_t	desc_hdr;
+	uint32_t sgf_ln;
+	dma_addr_t q_dma;
+	dma_addr_t r_dma;
+	dma_addr_t g_dma;	/* or Gx,y */
+	dma_addr_t w_dma; /* or Wx,y */
+	dma_addr_t f_dma;
+	dma_addr_t c_dma;
+	dma_addr_t d_dma;
+	dma_addr_t tmp_dma; /* temporary data block */
+	uint32_t	op;
+} __packed;
+
+struct dlc_keygen_desc_s {
+	uint32_t desc_hdr;
+	uint32_t sgf_ln;
+	dma_addr_t q_dma;
+	dma_addr_t r_dma;
+	dma_addr_t g_dma;
+	dma_addr_t s_dma;
+	dma_addr_t w_dma;
+	uint32_t op;
+} __packed;
+
+struct ecdsa_sign_desc_s {
+	uint32_t	desc_hdr;
+	uint32_t sgf_ln; /* Use ECDSA_PDB_ definitions per above */
+	dma_addr_t q_dma;
+	dma_addr_t r_dma;
+	dma_addr_t g_dma;	/* or Gx,y */
+	dma_addr_t s_dma;
+	dma_addr_t f_dma;
+	dma_addr_t c_dma;
+	dma_addr_t d_dma;
+	dma_addr_t ab_dma;
+	uint32_t	op;
+} __packed;
+
+struct ecdsa_verify_desc_s {
+	uint32_t	desc_hdr;
+	uint32_t sgf_ln;
+	dma_addr_t q_dma;
+	dma_addr_t r_dma;
+	dma_addr_t g_dma;	/* or Gx,y */
+	dma_addr_t w_dma; /* or Wx,y */
+	dma_addr_t f_dma;
+	dma_addr_t c_dma;
+	dma_addr_t d_dma;
+	dma_addr_t tmp_dma; /* temporary data block */
+	dma_addr_t ab_dma;
+	uint32_t	op;
+} __packed;
+
+struct ecc_keygen_desc_s {
+	uint32_t desc_hdr;
+	uint32_t sgf_ln;
+	dma_addr_t q_dma;
+	dma_addr_t r_dma;
+	dma_addr_t g_dma;
+	dma_addr_t s_dma;
+	dma_addr_t w_dma;
+	dma_addr_t ab_dma;
+	uint32_t op;
+} __packed;
+
+#define DH_PDB_L_SHIFT         7
+#define DH_PDB_L_MASK          (0x3ff << DH_PDB_L_SHIFT)
+#define DH_PDB_N_MASK          0x7f
+#define DH_PDB_SGF_SHIFT       24
+#define DH_PDB_SGF_MASK        (0xff << DH_PDB_SGF_SHIFT)
+#define DH_PDB_SGF_Q           (0x80 << DH_PDB_SGF_SHIFT)
+#define DH_PDB_SGF_R           (0x40 << DH_PDB_SGF_SHIFT)
+#define DH_PDB_SGF_W           (0x20 << DH_PDB_SGF_SHIFT)
+#define DH_PDB_SGF_S           (0x10 << DH_PDB_SGF_SHIFT)
+#define DH_PDB_SGF_Z           (0x08 << DH_PDB_SGF_SHIFT)
+#define DH_PDB_SGF_AB          (0x04 << DH_PDB_SGF_SHIFT)
+
+struct dh_key_desc_s {
+	uint32_t	desc_hdr;
+	uint32_t sgf_ln; /* Use DH_PDB_ definitions per above */
+	dma_addr_t q_dma;
+	dma_addr_t r_dma;
+	dma_addr_t w_dma;
+	dma_addr_t s_dma;
+	dma_addr_t z_dma;
+	dma_addr_t ab_dma;
+	uint32_t	op;
+} __packed;
+
+/* DSA/ECDSA Protocol Data Blocks */
+#define RSA_PDB_SGF_SHIFT       28
+#define RSA_PDB_MSG_FMT_SHIFT   12
+#define RSA_PDB_E_SHIFT       12
+#define RSA_PDB_E_MASK        (0xFFF << RSA_PDB_E_SHIFT)
+#define RSA_PDB_D_SHIFT       12
+#define RSA_PDB_D_MASK        (0xFFF << RSA_PDB_D_SHIFT)
+#define RSA_PDB_Q_SHIFT       12
+#define RSA_PDB_Q_MASK        (0xFFF << RSA_PDB_Q_SHIFT)
+
+
+#define RSA_PDB_SGF_F		(0x8 << RSA_PDB_SGF_SHIFT)
+#define RSA_PDB_SGF_G		(0x4 << RSA_PDB_SGF_SHIFT)
+#define RSA_PDB_SGF_N		(0x2 << RSA_PDB_SGF_SHIFT)
+#define RSA_PDB_SGF_E		(0x1 << RSA_PDB_SGF_SHIFT)
+
+#define RSA_PRIV_KEY_FRM_1	0
+#define RSA_PRIV_KEY_FRM_2	1
+#define RSA_PRIV_KEY_FRM_3	2
+
+#define RSA_PDB_PVT_FRM1_SGF_SHIFT  28
+#define RSA_PDB_PVT_FRM1_D_SHIFT 10
+
+#define RSA_PDB_PVT_MSG_FMT_SHIFT   12
+#define RSA_PDB_PVT_FRM3_SGF_SHIFT  23
+#define RSA_PDB_PVT_FRM3_Q_SHIFT  12
+
+/* RSA Pub_Key Descriptor */
+struct rsa_pub_desc_s {
+	uint32_t	desc_hdr;
+	uint32_t	sgf_flg;
+	dma_addr_t	f_dma;
+	dma_addr_t	g_dma;
+	dma_addr_t	n_dma;
+	dma_addr_t	e_dma;
+	uint32_t	msg_len;
+	uint32_t	op;
+} __packed;
 
-struct dsa_verify_pdb {
-	u32 sgf_ln;
-	u8 *q;
-	u8 *r;
-	u8 *g;	/* or Gx,y */
-	u8 *w; /* or Wx,y */
-	u8 *f;
-	u8 *c;
-	u8 *d;
-	u8 *tmp; /* temporary data block */
-	u8 *ab; /* only used if ECC processing */
-};
+/*
+ * Form1 Priv_key Decryption Descriptor
+ * Private key is represented by (n,d)
+ * f is decrypted data pointer while g is pointer
+ * of encrypted data
+ */
+struct rsa_priv_frm1_desc_s {
+	uint32_t	desc_hdr;
+	uint32_t	sgf_flg;
+	dma_addr_t	g_dma;
+	dma_addr_t	f_dma;
+	dma_addr_t	n_dma;
+	dma_addr_t	d_dma;
+	uint32_t	op;
+} __packed;
+
+/*
+ * Form2 Priv_key Decryption Descriptor
+ * Private key is represented by (p,q,d)
+ * f is decrypted data pointer while g is pointer
+ * of encrypted data
+ */
+struct rsa_priv_frm2_desc_s {
+	uint32_t	desc_hdr;
+	uint32_t	sgf_flg;
+	dma_addr_t	g_dma;
+	dma_addr_t	f_dma;
+	dma_addr_t	d_dma;
+	dma_addr_t	p_dma;
+	dma_addr_t	q_dma;
+	dma_addr_t	tmp1_dma;
+	dma_addr_t	tmp2_dma;
+	uint32_t	p_q_len;
+	uint32_t	op;
+} __packed;
+
+/*
+ * Form3 Priv_key Decryption Descriptor
+ * Private key is represented by (p,q,dp,dq,c)
+ * f is decrypted data pointer while g is input pointer
+ * of encrypted data
+ */
+struct rsa_priv_frm3_desc_s {
+	uint32_t	desc_hdr;
+	uint32_t	sgf_flg;
+	dma_addr_t	g_dma;
+	dma_addr_t	f_dma;
+	dma_addr_t	c_dma;
+	dma_addr_t	p_dma;
+	dma_addr_t	q_dma;
+	dma_addr_t	dp_dma;
+	dma_addr_t	dq_dma;
+	dma_addr_t	tmp1_dma;
+	dma_addr_t	tmp2_dma;
+	uint32_t	p_q_len;
+	uint32_t	op;
+} __packed;
 
 #endif
diff --git a/drivers/crypto/caam/pkc_desc.c b/drivers/crypto/caam/pkc_desc.c
new file mode 100644
index 0000000..2e81519
--- /dev/null
+++ b/drivers/crypto/caam/pkc_desc.c
@@ -0,0 +1,388 @@
+/*
+ * \file - pkc_desc.c
+ * \brief - Freescale FSL CAAM support for Public Key Cryptography descriptor
+ *
+ * Author: Yashpal Dutta <yashpal.dutta@freescale.com>
+ *
+ * Copyright 2012 Freescale Semiconductor, Inc.
+ *
+ * There is no shared descriptor for PKC but Job descriptor must carry
+ * all the desired key parameters, input and output pointers
+ *
+ */
+#include "pkc_desc.h"
+
+/*#define CAAM_DEBUG */
+/* Descriptor for RSA Public operation */
+void *caam_rsa_pub_desc(struct rsa_edesc *edesc)
+{
+	u32 start_idx, desc_size;
+	struct rsa_pub_desc_s *rsa_pub_desc =
+	    (struct rsa_pub_desc_s *)edesc->hw_desc;
+	struct rsa_pub_edesc_s *pub_edesc = &edesc->dma_u.rsa_pub_edesc;
+#ifdef CAAM_DEBUG
+	uint32_t i;
+	uint32_t *buf = (uint32_t *)rsa_pub_desc;
+#endif
+
+	desc_size = sizeof(struct rsa_pub_desc_s) / sizeof(u32);
+	start_idx = desc_size - 1;
+	start_idx &= HDR_START_IDX_MASK;
+	init_job_desc(edesc->hw_desc, (start_idx << HDR_START_IDX_SHIFT) |
+		      (start_idx & HDR_DESCLEN_MASK) | HDR_ONE);
+	rsa_pub_desc->n_dma = pub_edesc->n_dma;
+	rsa_pub_desc->e_dma = pub_edesc->e_dma;
+	rsa_pub_desc->f_dma = pub_edesc->f_dma;
+	rsa_pub_desc->g_dma = pub_edesc->g_dma;
+	rsa_pub_desc->sgf_flg = (pub_edesc->sg_flgs.e_len << RSA_PDB_E_SHIFT)
+	    | pub_edesc->sg_flgs.n_len;
+	rsa_pub_desc->msg_len = pub_edesc->f_len;
+	rsa_pub_desc->op = CMD_OPERATION | OP_TYPE_UNI_PROTOCOL |
+	    OP_PCLID_RSAENC_PUBKEY;
+#ifdef CAAM_DEBUG
+	for (i = 0; i < desc_size; i++)
+		pr_debug("[%d] %x\n", i, buf[i]);
+#endif
+	return rsa_pub_desc;
+}
+
+/* Descriptor for RSA Private operation Form1 */
+void *caam_rsa_priv_f1_desc(struct rsa_edesc *edesc)
+{
+	u32 start_idx, desc_size;
+	struct rsa_priv_frm1_desc_s *rsa_priv_desc =
+	    (struct rsa_priv_frm1_desc_s *)edesc->hw_desc;
+	struct rsa_priv_frm1_edesc_s *priv_edesc =
+	    &edesc->dma_u.rsa_priv_f1_edesc;
+
+	desc_size = sizeof(struct rsa_priv_frm1_desc_s) / sizeof(u32);
+	start_idx = desc_size - 1;
+	start_idx &= HDR_START_IDX_MASK;
+	init_job_desc(edesc->hw_desc, (start_idx << HDR_START_IDX_SHIFT) |
+		      (start_idx & HDR_DESCLEN_MASK) | HDR_ONE);
+	rsa_priv_desc->n_dma = priv_edesc->n_dma;
+	rsa_priv_desc->d_dma = priv_edesc->d_dma;
+	rsa_priv_desc->f_dma = priv_edesc->f_dma;
+	rsa_priv_desc->g_dma = priv_edesc->g_dma;
+	/* TBD. Support SG flags */
+	rsa_priv_desc->sgf_flg = (priv_edesc->sg_flgs.d_len << RSA_PDB_D_SHIFT)
+	    | priv_edesc->sg_flgs.n_len;
+	rsa_priv_desc->op = CMD_OPERATION | OP_TYPE_UNI_PROTOCOL |
+	    OP_PCLID_RSADEC_PRVKEY | RSA_PRIV_KEY_FRM_1;
+	return rsa_priv_desc;
+}
+
+/* Descriptor for RSA Private operation Form2 */
+void *caam_rsa_priv_f2_desc(struct rsa_edesc *edesc)
+{
+	u32 start_idx, desc_size;
+	struct rsa_priv_frm2_desc_s *rsa_priv_desc =
+	    (struct rsa_priv_frm2_desc_s *)edesc->hw_desc;
+	struct rsa_priv_frm2_edesc_s *priv_edesc =
+	    &edesc->dma_u.rsa_priv_f2_edesc;
+
+	desc_size = sizeof(struct rsa_priv_frm2_desc_s) / sizeof(u32);
+	start_idx = desc_size - 1;
+	start_idx &= HDR_START_IDX_MASK;
+	init_job_desc(edesc->hw_desc, (start_idx << HDR_START_IDX_SHIFT) |
+		      (start_idx & HDR_DESCLEN_MASK) | HDR_ONE);
+	rsa_priv_desc->p_dma = priv_edesc->p_dma;
+	rsa_priv_desc->q_dma = priv_edesc->q_dma;
+	rsa_priv_desc->d_dma = priv_edesc->d_dma;
+	rsa_priv_desc->f_dma = priv_edesc->f_dma;
+	rsa_priv_desc->g_dma = priv_edesc->g_dma;
+	rsa_priv_desc->tmp1_dma = priv_edesc->tmp1_dma;
+	rsa_priv_desc->tmp2_dma = priv_edesc->tmp2_dma;
+	rsa_priv_desc->sgf_flg = (priv_edesc->sg_flgs.d_len << RSA_PDB_D_SHIFT)
+	    | priv_edesc->sg_flgs.n_len;
+	rsa_priv_desc->p_q_len = (priv_edesc->q_len << RSA_PDB_Q_SHIFT)
+	    | priv_edesc->p_len;
+	rsa_priv_desc->op = CMD_OPERATION | OP_TYPE_UNI_PROTOCOL |
+	    OP_PCLID_RSADEC_PRVKEY | RSA_PRIV_KEY_FRM_2;
+	return rsa_priv_desc;
+}
+
+/* Descriptor for RSA Private operation Form3 */
+void *caam_rsa_priv_f3_desc(struct rsa_edesc *edesc)
+{
+	u32 start_idx, desc_size;
+	struct rsa_priv_frm3_desc_s *rsa_priv_desc =
+	    (struct rsa_priv_frm3_desc_s *)edesc->hw_desc;
+	struct rsa_priv_frm3_edesc_s *priv_edesc =
+	    &edesc->dma_u.rsa_priv_f3_edesc;
+#ifdef CAAM_DEBUG
+	uint32_t *buf = (uint32_t *)rsa_priv_desc;
+	uint32_t i;
+#endif
+
+	desc_size = sizeof(struct rsa_priv_frm3_desc_s) / sizeof(u32);
+	start_idx = desc_size - 1;
+	start_idx &= HDR_START_IDX_MASK;
+	init_job_desc(edesc->hw_desc, (start_idx << HDR_START_IDX_SHIFT) |
+		      (start_idx & HDR_DESCLEN_MASK) | HDR_ONE);
+	rsa_priv_desc->p_dma = priv_edesc->p_dma;
+	rsa_priv_desc->q_dma = priv_edesc->q_dma;
+	rsa_priv_desc->dp_dma = priv_edesc->dp_dma;
+	rsa_priv_desc->dq_dma = priv_edesc->dq_dma;
+	rsa_priv_desc->c_dma = priv_edesc->c_dma;
+	rsa_priv_desc->f_dma = priv_edesc->f_dma;
+	rsa_priv_desc->g_dma = priv_edesc->g_dma;
+	rsa_priv_desc->tmp1_dma = priv_edesc->tmp1_dma;
+	rsa_priv_desc->tmp2_dma = priv_edesc->tmp2_dma;
+	rsa_priv_desc->p_q_len = (priv_edesc->q_len << RSA_PDB_Q_SHIFT)
+	    | priv_edesc->p_len;
+	/* TBD: SG Flags to be filled */
+	rsa_priv_desc->sgf_flg = priv_edesc->sg_flgs.n_len;
+	rsa_priv_desc->op = CMD_OPERATION | OP_TYPE_UNI_PROTOCOL |
+	    OP_PCLID_RSADEC_PRVKEY | RSA_PRIV_KEY_FRM_3;
+#ifdef CAAM_DEBUG
+	for (i = 0; i < desc_size; i++)
+		pr_debug("[%d] %x\n", i, buf[i]);
+#endif
+	return rsa_priv_desc;
+}
+
+/* DH sign CAAM descriptor */
+void *caam_dh_key_desc(struct dh_edesc_s *edesc)
+{
+	u32 start_idx, desc_size;
+	void *desc;
+#ifdef CAAM_DEBUG
+	uint32_t i;
+	uint32_t *buf;
+#endif
+	struct dh_key_desc_s *dh_desc =
+	    (struct dh_key_desc_s *)edesc->hw_desc;
+	desc_size = sizeof(struct dh_key_desc_s) / sizeof(u32);
+	start_idx = desc_size - 1;
+	start_idx &= HDR_START_IDX_MASK;
+	init_job_desc(edesc->hw_desc, (start_idx << HDR_START_IDX_SHIFT) |
+		      (start_idx & HDR_DESCLEN_MASK) | HDR_ONE);
+	dh_desc->sgf_ln = (edesc->l_len << DH_PDB_L_SHIFT) |
+		((edesc->n_len & DH_PDB_N_MASK));
+	dh_desc->q_dma = edesc->q_dma;
+	dh_desc->w_dma = edesc->w_dma;
+	dh_desc->s_dma = edesc->s_dma;
+	dh_desc->z_dma = edesc->z_dma;
+	dh_desc->op = CMD_OPERATION | OP_TYPE_UNI_PROTOCOL |
+	    OP_PCLID_DH;
+	if (edesc->req_type == ECDH_COMPUTE_KEY) {
+		dh_desc->ab_dma = edesc->ab_dma;
+		dh_desc->op |= OP_PCL_PKPROT_ECC;
+		if (edesc->curve_type == ECC_BINARY)
+			dh_desc->op |= OP_PCL_PKPROT_F2M;
+	}
+
+	desc = dh_desc;
+#ifdef CAAM_DEBUG
+	buf = desc;
+	pr_debug("%d DH Descriptor is:\n", desc_size);
+	for (i = 0; i < desc_size; i++)
+		pr_debug("[%d] %x\n", i, buf[i]);
+#endif
+	return desc;
+}
+
+/* DSA sign CAAM descriptor */
+void *caam_dsa_sign_desc(struct dsa_edesc_s *edesc)
+{
+	u32 start_idx, desc_size;
+	void *desc;
+#ifdef CAAM_DEBUG
+	uint32_t i;
+	uint32_t *buf;
+#endif
+
+	if (edesc->req_type == ECDSA_SIGN) {
+		struct ecdsa_sign_desc_s *ecdsa_desc =
+		    (struct ecdsa_sign_desc_s *)edesc->hw_desc;
+		desc_size = sizeof(struct ecdsa_sign_desc_s) / sizeof(u32);
+		start_idx = desc_size - 1;
+		start_idx &= HDR_START_IDX_MASK;
+		init_job_desc(edesc->hw_desc,
+			      (start_idx << HDR_START_IDX_SHIFT) |
+			      (start_idx & HDR_DESCLEN_MASK) | HDR_ONE);
+		ecdsa_desc->sgf_ln = (edesc->l_len << DSA_PDB_L_SHIFT) |
+			((edesc->n_len & DSA_PDB_N_MASK));
+		ecdsa_desc->q_dma = edesc->q_dma;
+		ecdsa_desc->r_dma = edesc->r_dma;
+		ecdsa_desc->g_dma = edesc->g_dma;
+		ecdsa_desc->s_dma = edesc->key_dma;
+		ecdsa_desc->f_dma = edesc->f_dma;
+		ecdsa_desc->c_dma = edesc->c_dma;
+		ecdsa_desc->d_dma = edesc->d_dma;
+		ecdsa_desc->ab_dma = edesc->ab_dma;
+		ecdsa_desc->op = CMD_OPERATION | OP_TYPE_UNI_PROTOCOL |
+		    OP_PCLID_DSASIGN | OP_PCL_PKPROT_ECC;
+		if (edesc->curve_type == ECC_BINARY)
+			ecdsa_desc->op |= OP_PCL_PKPROT_F2M;
+
+		desc = ecdsa_desc;
+	} else {
+		struct dsa_sign_desc_s *dsa_desc =
+		    (struct dsa_sign_desc_s *)edesc->hw_desc;
+		desc_size = sizeof(struct dsa_sign_desc_s) / sizeof(u32);
+		start_idx = desc_size - 1;
+		start_idx &= HDR_START_IDX_MASK;
+		init_job_desc(edesc->hw_desc,
+			      (start_idx << HDR_START_IDX_SHIFT) |
+			      (start_idx & HDR_DESCLEN_MASK) | HDR_ONE);
+		dsa_desc->sgf_ln = (edesc->l_len << DSA_PDB_L_SHIFT) |
+			((edesc->n_len & DSA_PDB_N_MASK));
+		dsa_desc->q_dma = edesc->q_dma;
+		dsa_desc->r_dma = edesc->r_dma;
+		dsa_desc->g_dma = edesc->g_dma;
+		dsa_desc->s_dma = edesc->key_dma;
+		dsa_desc->f_dma = edesc->f_dma;
+		dsa_desc->c_dma = edesc->c_dma;
+		dsa_desc->d_dma = edesc->d_dma;
+		dsa_desc->op = CMD_OPERATION | OP_TYPE_UNI_PROTOCOL |
+		    OP_PCLID_DSASIGN;
+		desc = dsa_desc;
+	}
+#ifdef CAAM_DEBUG
+	buf = desc;
+	pr_debug("DSA Descriptor is:");
+	for (i = 0; i < desc_size; i++)
+		pr_debug("[%d] %x ", i, buf[i]);
+	pr_debug("\n");
+#endif
+
+	return desc;
+}
+
+/* DSA/ECDSA/DH/ECDH keygen CAAM descriptor */
+void *caam_keygen_desc(struct dsa_edesc_s *edesc)
+{
+	u32 start_idx, desc_size;
+	void *desc;
+#ifdef CAAM_DEBUG
+	uint32_t i;
+	uint32_t *buf;
+#endif
+
+	if (edesc->req_type == ECC_KEYGEN) {
+		struct ecc_keygen_desc_s *ecc_desc =
+		    (struct ecc_keygen_desc_s *)edesc->hw_desc;
+		desc_size = sizeof(struct ecc_keygen_desc_s) / sizeof(u32);
+		start_idx = desc_size - 1;
+		start_idx &= HDR_START_IDX_MASK;
+		init_job_desc(edesc->hw_desc,
+			      (start_idx << HDR_START_IDX_SHIFT) |
+			      (start_idx & HDR_DESCLEN_MASK) | HDR_ONE);
+		ecc_desc->sgf_ln = (edesc->l_len << DSA_PDB_L_SHIFT) |
+				   (edesc->n_len & DSA_PDB_N_MASK);
+		if (edesc->erratum_A_006899) {
+			ecc_desc->sgf_ln |= DSA_PDB_SGF_G;
+			ecc_desc->g_dma = edesc->g_sg_dma;
+		} else {
+			ecc_desc->g_dma = edesc->g_dma;
+		}
+		ecc_desc->q_dma = edesc->q_dma;
+		ecc_desc->r_dma = edesc->r_dma;
+		ecc_desc->s_dma = edesc->s_dma;
+		ecc_desc->w_dma = edesc->key_dma;
+		ecc_desc->ab_dma = edesc->ab_dma;
+		ecc_desc->op = CMD_OPERATION | OP_TYPE_UNI_PROTOCOL |
+		    OP_PCLID_PUBLICKEYPAIR | OP_PCL_PKPROT_ECC;
+		if (edesc->curve_type == ECC_BINARY)
+			ecc_desc->op |= OP_PCL_PKPROT_F2M;
+
+		desc = ecc_desc;
+	} else {
+		struct dlc_keygen_desc_s *key_desc =
+		    (struct dlc_keygen_desc_s *)edesc->hw_desc;
+		desc_size = sizeof(struct dlc_keygen_desc_s) / sizeof(u32);
+		start_idx = desc_size - 1;
+		start_idx &= HDR_START_IDX_MASK;
+		init_job_desc(edesc->hw_desc,
+			      (start_idx << HDR_START_IDX_SHIFT) |
+			      (start_idx & HDR_DESCLEN_MASK) | HDR_ONE);
+		key_desc->sgf_ln = (edesc->l_len << DSA_PDB_L_SHIFT) |
+			((edesc->n_len & DSA_PDB_N_MASK));
+		key_desc->q_dma = edesc->q_dma;
+		key_desc->r_dma = edesc->r_dma;
+		key_desc->g_dma = edesc->g_dma;
+		key_desc->s_dma = edesc->s_dma;
+		key_desc->w_dma = edesc->key_dma;
+		key_desc->op = CMD_OPERATION | OP_TYPE_UNI_PROTOCOL |
+		    OP_PCLID_PUBLICKEYPAIR;
+		desc = key_desc;
+	}
+#ifdef CAAM_DEBUG
+	buf = desc;
+	pr_debug("DSA Keygen Descriptor is:");
+	for (i = 0; i < desc_size; i++)
+		pr_debug("[%d] %x ", i, buf[i]);
+	pr_debug("\n");
+#endif
+
+	return desc;
+}
+
+/* DSA verify CAAM descriptor */
+void *caam_dsa_verify_desc(struct dsa_edesc_s *edesc)
+{
+	u32 start_idx, desc_size;
+	void *desc;
+#ifdef CAAM_DEBUG
+	uint32_t i;
+	uint32_t *buf;
+#endif
+
+	if (edesc->req_type == ECDSA_VERIFY) {
+		struct ecdsa_verify_desc_s *ecdsa_desc =
+		    (struct ecdsa_verify_desc_s *)edesc->hw_desc;
+		desc_size = sizeof(struct ecdsa_verify_desc_s) / sizeof(u32);
+		start_idx = desc_size - 1;
+		start_idx &= HDR_START_IDX_MASK;
+		init_job_desc(edesc->hw_desc,
+			      (start_idx << HDR_START_IDX_SHIFT) |
+			      (start_idx & HDR_DESCLEN_MASK) | HDR_ONE);
+		ecdsa_desc->sgf_ln = (edesc->l_len << DSA_PDB_L_SHIFT) |
+			((edesc->n_len & DSA_PDB_N_MASK));
+		ecdsa_desc->q_dma = edesc->q_dma;
+		ecdsa_desc->r_dma = edesc->r_dma;
+		ecdsa_desc->g_dma = edesc->g_dma;
+		ecdsa_desc->w_dma = edesc->key_dma;
+		ecdsa_desc->f_dma = edesc->f_dma;
+		ecdsa_desc->c_dma = edesc->c_dma;
+		ecdsa_desc->d_dma = edesc->d_dma;
+		ecdsa_desc->tmp_dma = edesc->tmp_dma;
+		ecdsa_desc->ab_dma = edesc->ab_dma;
+		ecdsa_desc->op = CMD_OPERATION | OP_TYPE_UNI_PROTOCOL |
+		    OP_PCLID_DSAVERIFY | OP_PCL_PKPROT_ECC;
+		if (edesc->curve_type == ECC_BINARY)
+			ecdsa_desc->op |= OP_PCL_PKPROT_F2M;
+		desc = ecdsa_desc;
+	} else {
+		struct dsa_verify_desc_s *dsa_desc =
+		    (struct dsa_verify_desc_s *)edesc->hw_desc;
+		desc_size = sizeof(struct dsa_verify_desc_s) / sizeof(u32);
+		start_idx = desc_size - 1;
+		start_idx &= HDR_START_IDX_MASK;
+		init_job_desc(edesc->hw_desc,
+			      (start_idx << HDR_START_IDX_SHIFT) |
+			      (start_idx & HDR_DESCLEN_MASK) | HDR_ONE);
+		dsa_desc->sgf_ln = (edesc->l_len << DSA_PDB_L_SHIFT) |
+			((edesc->n_len & DSA_PDB_N_MASK));
+		dsa_desc->q_dma = edesc->q_dma;
+		dsa_desc->r_dma = edesc->r_dma;
+		dsa_desc->g_dma = edesc->g_dma;
+		dsa_desc->w_dma = edesc->key_dma;
+		dsa_desc->f_dma = edesc->f_dma;
+		dsa_desc->c_dma = edesc->c_dma;
+		dsa_desc->d_dma = edesc->d_dma;
+		dsa_desc->tmp_dma = edesc->tmp_dma;
+		dsa_desc->op = CMD_OPERATION | OP_TYPE_UNI_PROTOCOL |
+		    OP_PCLID_DSAVERIFY;
+		desc = dsa_desc;
+	}
+#ifdef CAAM_DEBUG
+	buf = desc;
+	pr_debug("DSA Descriptor is:\n");
+	for (i = 0; i < desc_size; i++)
+		pr_debug("[%d] %x\n", i, buf[i]);
+#endif
+	return desc;
+}
diff --git a/drivers/crypto/caam/pkc_desc.h b/drivers/crypto/caam/pkc_desc.h
new file mode 100644
index 0000000..e859e40
--- /dev/null
+++ b/drivers/crypto/caam/pkc_desc.h
@@ -0,0 +1,225 @@
+/*
+ * \file - pkc_desc.h
+ * \brief - Freescale FSL CAAM support for Public Key Cryptography
+ *
+ * Author: Yashpal Dutta <yashpal.dutta@freescale.com>
+ *
+ * Copyright 2012 Freescale Semiconductor, Inc.
+ *
+ * There is no shared descriptor for PKC but Job descriptor must carry
+ * all the desired key parameters, input and output pointers
+ *
+ */
+#ifndef _PKC_DESC_H_
+#define _PKC_DESC_H_
+
+#include "compat.h"
+
+#include "regs.h"
+#include "intern.h"
+#include "desc_constr.h"
+#include "jr.h"
+#include "error.h"
+#include "sg_sw_sec4.h"
+#include <linux/crypto.h>
+#include "pdb.h"
+
+/* Public key SG Flags and e, n lengths */
+struct sg_public_flg_s {
+	uint32_t sg_f:1;
+	uint32_t sg_g:1;
+	uint32_t sg_n:1;
+	uint32_t sg_e:1;
+	uint32_t reserved:4;
+	uint32_t e_len:12;
+	uint32_t n_len:12;
+};
+
+/* RSA public Extended Descriptor fields
+ @n_dma - n, e represents the public key
+ @e_dma - Public key exponent,  n is modulus
+ @g_dma - Output RSA-encrypted value
+ @f_dma - Input RSA-Plain value
+ @n_len, e_len - Public key param length
+ @ f_len - input data len
+ */
+struct rsa_pub_edesc_s {
+	dma_addr_t n_dma;
+	dma_addr_t e_dma;
+	dma_addr_t g_dma;
+	dma_addr_t f_dma;
+	struct sg_public_flg_s sg_flgs;
+	uint32_t reserved:20;
+	uint32_t f_len:12;
+};
+
+/* Private key Form1 SG Flags and d, n lengths */
+struct sg_priv_f1_flg_s {
+	uint32_t sg_g:1;
+	uint32_t sg_f:1;
+	uint32_t sg_n:1;
+	uint32_t sg_d:1;
+	uint32_t reserved:4;
+	uint32_t d_len:12;
+	uint32_t n_len:12;
+};
+
+/* RSA PrivKey Form1 Extended Descriptor fields
+ @n_dma - n, d represents the private key form1 representation
+ @d_dma - d is the private exponent, n is the modules
+ @g_dma - Input RSA-encrypted value
+ @f_dma - Output RSA-Plain value
+ */
+struct rsa_priv_frm1_edesc_s {
+	dma_addr_t n_dma;
+	dma_addr_t d_dma;
+	dma_addr_t f_dma;
+	dma_addr_t g_dma;
+	struct sg_priv_f1_flg_s sg_flgs;
+};
+
+/* Private key Form2 SG Flags and d, n lengths */
+struct sg_priv_f2_flg_s {
+	uint32_t sg_g:1;
+	uint32_t sg_f:1;
+	uint32_t sg_d:1;
+	uint32_t sg_p:1;
+	uint32_t sg_q:1;
+	uint32_t sg_tmp1:1;
+	uint32_t sg_tmp2:1;
+	uint32_t reserved:1;
+	uint32_t d_len:12;
+	uint32_t n_len:12;
+};
+
+/* RSA PrivKey Form2
+ @p, q, d represents the private key form2 representation
+ @d - d is private exponent, p and q are the two primes
+ @f - output pointer
+ @g - input pointer
+ */
+struct rsa_priv_frm2_edesc_s {
+	dma_addr_t p_dma;
+	dma_addr_t q_dma;
+	dma_addr_t d_dma;
+	dma_addr_t f_dma;
+	dma_addr_t g_dma;
+	int8_t *tmp1;
+	int8_t *tmp2;
+	dma_addr_t tmp1_dma;
+	dma_addr_t tmp2_dma;
+	struct sg_priv_f2_flg_s sg_flgs;
+	uint32_t reserved:8;
+	uint32_t q_len:12;
+	uint32_t p_len:12;
+};
+
+/* Private key Form3 SG Flags and d, n lengths */
+struct sg_priv_f3_flg_s {
+	uint32_t sg_g:1;
+	uint32_t sg_f:1;
+	uint32_t sg_c:1;
+	uint32_t sg_p:1;
+	uint32_t sg_q:1;
+	uint32_t sg_dp:1;
+	uint32_t sg_dq:1;
+	uint32_t sg_tmp1:1;
+	uint32_t sg_tmp2:1;
+	uint32_t reserved:11;
+	uint32_t n_len:12;
+};
+
+/* RSA PrivKey Form3
+ @n - p, q, dp, dq, c represents the private key form3 representation
+ @dp - First CRT exponent factor
+ @dq - Second CRT exponent factor
+ @c - CRT Coefficient
+ @f - output pointer
+ @g - input pointer
+ */
+struct rsa_priv_frm3_edesc_s {
+	dma_addr_t p_dma;
+	dma_addr_t q_dma;
+	dma_addr_t dp_dma;
+	dma_addr_t dq_dma;
+	dma_addr_t c_dma;
+	dma_addr_t f_dma;
+	dma_addr_t g_dma;
+	int8_t *tmp1;
+	int8_t *tmp2;
+	dma_addr_t tmp1_dma;
+	dma_addr_t tmp2_dma;
+	struct sg_priv_f3_flg_s sg_flgs;
+	uint32_t reserved:8;
+	uint32_t q_len:12;
+	uint32_t p_len:12;
+};
+
+/*
+ * rsa_edesc - s/w-extended rsa descriptor
+ * @hw_desc: the h/w job descriptor
+ */
+struct rsa_edesc {
+	enum pkc_req_type req_type;
+	union {
+		struct rsa_pub_edesc_s rsa_pub_edesc;
+		struct rsa_priv_frm1_edesc_s rsa_priv_f1_edesc;
+		struct rsa_priv_frm2_edesc_s rsa_priv_f2_edesc;
+		struct rsa_priv_frm3_edesc_s rsa_priv_f3_edesc;
+	} dma_u;
+	u32 hw_desc[];
+};
+
+/*
+ * dsa_edesc - s/w-extended for dsa and ecdsa descriptors
+ * @hw_desc: the h/w job descriptor
+ */
+struct dsa_edesc_s {
+	enum pkc_req_type req_type;
+	enum curve_t curve_type;
+	uint32_t l_len;
+	uint32_t n_len;
+	dma_addr_t key_dma;
+	dma_addr_t s_dma;
+	dma_addr_t f_dma;
+	dma_addr_t q_dma;
+	dma_addr_t r_dma;
+	dma_addr_t c_dma;
+	dma_addr_t d_dma;
+	dma_addr_t ab_dma;
+	dma_addr_t g_dma;
+	dma_addr_t g_sg_dma;
+	dma_addr_t tmp_dma;
+	uint8_t *tmp; /* Allocate locally for dsa_verify */
+	struct sec4_sg_entry g_sg;
+	bool erratum_A_006899;
+	u32 hw_desc[];
+};
+
+/*
+ * dh_edesc - s/w-extended for dh and ecdh descriptors
+ * @hw_desc: the h/w job descriptor
+ */
+struct dh_edesc_s {
+	enum pkc_req_type req_type;
+	enum curve_t curve_type;
+	uint32_t l_len;
+	uint32_t n_len;
+	dma_addr_t q_dma;
+	dma_addr_t ab_dma;
+	dma_addr_t w_dma;
+	dma_addr_t s_dma;
+	dma_addr_t z_dma;
+	u32 hw_desc[];
+};
+
+void *caam_rsa_pub_desc(struct rsa_edesc *);
+void *caam_rsa_priv_f1_desc(struct rsa_edesc *);
+void *caam_rsa_priv_f2_desc(struct rsa_edesc *);
+void *caam_rsa_priv_f3_desc(struct rsa_edesc *);
+void *caam_dsa_sign_desc(struct dsa_edesc_s *);
+void *caam_dsa_verify_desc(struct dsa_edesc_s *);
+void *caam_keygen_desc(struct dsa_edesc_s *);
+void *caam_dh_key_desc(struct dh_edesc_s *);
+
+#endif
diff --git a/drivers/crypto/caam/qi.c b/drivers/crypto/caam/qi.c
new file mode 100644
index 0000000..e6911b2a
--- /dev/null
+++ b/drivers/crypto/caam/qi.c
@@ -0,0 +1,840 @@
+/*
+ * CAAM/SEC 4.x QI transport/backend driver
+ * Queue Interface backend functionality
+ *
+ * Copyright 2013 Freescale Semiconductor, Inc.
+ */
+
+#include <linux/cpumask.h>
+#include <linux/kthread.h>
+#include <linux/fsl_qman.h>
+
+#include "regs.h"
+#include "qi.h"
+#include "desc.h"
+#include "intern.h"
+#include "desc_constr.h"
+
+#define PRE_HDR_LEN		2	/* Length in u32 words */
+#define PREHDR_RSLS_SHIFT	31
+#ifndef CONFIG_FSL_DPAA_ETH
+/* If DPA_ETH is not available, then use a reasonably backlog per CPU */
+#define MAX_RSP_FQ_BACKLOG_PER_CPU	64
+#endif
+#define CAAM_QI_MEMCACHE_SIZE	256	/* Length of a single buffer in
+					   the QI driver memory cache. */
+
+/*
+ * The jobs are processed by the driver against a driver context.
+ * With every cryptographic context, a driver context is attached.
+ * The driver context contains data for private use by driver.
+ * For the applications, this is an opaque structure.
+ */
+
+struct caam_drv_ctx {
+	u32 prehdr[PRE_HDR_LEN];	/* Preheader placed before shrd desc */
+	u32 sh_desc[MAX_SDLEN];		/* Shared descriptor */
+	dma_addr_t context_a; /* shared descriptor dma address */
+	struct qman_fq *req_fq;		/* Request frame queue to caam */
+	struct qman_fq *rsp_fq;		/* Response frame queue from caam */
+	int cpu;			/* cpu on which to recv caam rsp */
+	struct device *qidev;		/* device pointer for QI backend */
+} ____cacheline_aligned;
+
+/*
+ * percpu private data structure to main list of pending responses expected
+ * on each cpu.
+ */
+struct caam_qi_pcpu_priv {
+	struct napi_struct irqtask;	/* IRQ task for QI backend */
+	struct net_device net_dev;	/* netdev used by NAPI */
+	struct qman_fq rsp_fq;		/* Response FQ from CAAM */
+} ____cacheline_aligned;
+
+static DEFINE_PER_CPU(struct caam_qi_pcpu_priv, pcpu_qipriv);
+
+struct caam_qi_priv {
+	struct qman_cgr rsp_cgr;	/* QMAN response CGR */
+	struct platform_device *qi_pdev; /* Platform device for QI backend */
+};
+
+static struct caam_qi_priv qipriv ____cacheline_aligned;
+
+/*
+ * This is written by one core - the one that initialized the CGR, and
+ * read by multiple cores (all the others)
+ */
+static bool caam_congested __read_mostly;
+
+/*
+ * CPU from where the module initialised. This is required because
+ * QMAN driver requires CGRs to be removed from same CPU from where
+ * they were originally allocated
+ */
+static int mod_init_cpu;
+
+/*
+ * This is a a cache of buffers, from which the users of CAAM QI driver
+ * can allocate short (currently 128B) buffers. It's speedier than
+ * doing malloc on the hotpath.
+ * NOTE: A more elegant solution would be to have some headroom in the frames
+ *       being processed. This can be added by the dpa_eth driver. This would
+ *       pose a problem for userspace application processing which cannot
+ *       know of this limitation. So for now, this will work.
+ * NOTE: The memcache is SMP-safe. No need to handle spinlocks in-here
+ */
+static struct kmem_cache *qi_cache;
+
+bool caam_drv_ctx_busy(struct caam_drv_ctx *drv_ctx)
+{
+	return caam_congested;
+}
+EXPORT_SYMBOL(caam_drv_ctx_busy);
+
+int caam_qi_enqueue(struct device *qidev, struct caam_drv_req *req)
+{
+	struct qm_fd fd;
+	int ret;
+	const size_t size = 2 * sizeof(struct qm_sg_entry);
+	int num_retries = 0;
+
+	fd.cmd = 0;
+	fd.format = qm_fd_compound;
+	fd.cong_weight = req->fd_sgt[1].length;
+
+	fd.addr = dma_map_single(qidev, req->fd_sgt, size,
+				 DMA_BIDIRECTIONAL);
+	if (dma_mapping_error(qidev, fd.addr)) {
+		dev_err(qidev, "DMA mapping error for QI enqueue request\n");
+		return -EIO;
+	}
+
+	do {
+		ret = qman_enqueue(req->drv_ctx->req_fq, &fd, 0);
+		if (likely(!ret))
+			return 0;
+
+		if (-EBUSY != ret)
+			break;
+		num_retries++;
+	} while (num_retries < 10000);
+
+	dev_err(qidev, "qman_enqueue failed: %d\n", ret);
+
+	return ret;
+}
+EXPORT_SYMBOL(caam_qi_enqueue);
+
+static void caam_fq_ern_cb(struct qman_portal *qm, struct qman_fq *fq,
+			   const struct qm_mr_entry *msg)
+{
+	const struct qm_fd *fd;
+	struct caam_drv_req *drv_req;
+	const size_t size = 2 * sizeof(struct qm_sg_entry);
+	struct device *qidev = &per_cpu(pcpu_qipriv.net_dev,
+					smp_processor_id()).dev;
+
+	fd = &msg->ern.fd;
+
+	if (qm_fd_compound != fd->format) {
+		dev_err(qidev, "Non compound FD from CAAM\n");
+		return;
+	}
+
+	drv_req = ((struct caam_drv_req *)phys_to_virt(fd->addr));
+	if (!drv_req) {
+		dev_err(qidev,
+			"Can't find original request for caam response\n");
+		return;
+	}
+
+	dma_unmap_single(drv_req->drv_ctx->qidev, fd->addr,
+			 size, DMA_BIDIRECTIONAL);
+
+	drv_req->cbk(drv_req, -EIO);
+}
+
+static struct qman_fq *create_caam_req_fq(struct device *qidev,
+					  struct qman_fq *rsp_fq,
+					  dma_addr_t hwdesc,
+					  int fq_sched_flag)
+{
+	int ret, flags;
+	struct qman_fq *req_fq;
+	struct qm_mcc_initfq opts;
+
+	req_fq = kzalloc(sizeof(*req_fq), GFP_ATOMIC);
+	if (!req_fq) {
+		dev_err(qidev, "Mem alloc for CAAM req FQ failed\n");
+		return ERR_PTR(-ENOMEM);
+	}
+
+	req_fq->cb.ern = caam_fq_ern_cb;
+	req_fq->cb.fqs = NULL;
+
+	flags = QMAN_FQ_FLAG_DYNAMIC_FQID |
+		QMAN_FQ_FLAG_TO_DCPORTAL |
+		QMAN_FQ_FLAG_LOCKED;
+
+	ret = qman_create_fq(0, flags, req_fq);
+	if (ret) {
+		dev_err(qidev, "Failed to create session REQ FQ\n");
+		goto create_req_fq_fail;
+	}
+
+	flags = fq_sched_flag;
+	opts.we_mask = QM_INITFQ_WE_FQCTRL | QM_INITFQ_WE_DESTWQ |
+			QM_INITFQ_WE_CONTEXTB | QM_INITFQ_WE_CONTEXTA;
+
+	opts.fqd.fq_ctrl = QM_FQCTRL_CPCSTASH;
+	opts.fqd.dest.channel = qm_channel_caam;
+	opts.fqd.dest.wq = 0;
+	opts.fqd.context_b = qman_fq_fqid(rsp_fq);
+	opts.fqd.context_a.hi = upper_32_bits(hwdesc);
+	opts.fqd.context_a.lo = lower_32_bits(hwdesc);
+
+	ret = qman_init_fq(req_fq, flags, &opts);
+	if (ret) {
+		dev_err(qidev, "Failed to init session req FQ\n");
+		goto init_req_fq_fail;
+	}
+#ifdef DEBUG
+	dev_info(qidev, "Allocated request FQ %u for CPU %u\n",
+		 req_fq->fqid, smp_processor_id());
+#endif
+	return req_fq;
+
+init_req_fq_fail:
+	qman_destroy_fq(req_fq, 0);
+
+create_req_fq_fail:
+	kfree(req_fq);
+	return ERR_PTR(ret);
+}
+
+static int empty_retired_fq(struct device *qidev, struct qman_fq *fq)
+{
+	int ret;
+	enum qman_fq_state state;
+
+	u32 flags = QMAN_VOLATILE_FLAG_WAIT_INT | QMAN_VOLATILE_FLAG_FINISH;
+	u32 vdqcr = QM_VDQCR_PRECEDENCE_VDQCR | QM_VDQCR_NUMFRAMES_TILLEMPTY;
+
+	ret = qman_volatile_dequeue(fq, flags, vdqcr);
+	if (ret) {
+		dev_err(qidev, "Volatile dequeue fail for FQ: %u\n", fq->fqid);
+		return ret;
+	}
+
+	do {
+		qman_poll_dqrr(16);
+		qman_fq_state(fq, &state, &flags);
+	} while (flags & QMAN_FQ_STATE_NE);
+
+	return 0;
+}
+
+static int kill_fq(struct device *qidev, struct qman_fq *fq)
+{
+	enum qman_fq_state state;
+	u32 flags;
+	int ret;
+
+	ret = qman_retire_fq(fq, &flags);
+	if (ret < 0) {
+		dev_err(qidev, "qman_retire_fq failed\n");
+		return ret;
+	}
+
+	if (!ret)
+		goto empty_fq;
+
+	/* Async FQ retirement condition */
+	if (1 == ret) {
+		/* Retry till FQ gets in retired state */
+		do {
+			msleep(20);
+			qman_fq_state(fq, &state, &flags);
+		} while (qman_fq_state_retired != state);
+
+		WARN_ON(flags & QMAN_FQ_STATE_BLOCKOOS);
+		WARN_ON(flags & QMAN_FQ_STATE_ORL);
+	}
+
+empty_fq:
+	if (flags & QMAN_FQ_STATE_NE) {
+		ret = empty_retired_fq(qidev, fq);
+		if (ret) {
+			dev_err(qidev, "empty_retired_fq fail for FQ: %u\n",
+				fq->fqid);
+			return ret;
+		}
+	}
+
+	ret = qman_oos_fq(fq);
+	if (ret)
+		dev_err(qidev, "OOS of FQID: %u failed\n", fq->fqid);
+
+	qman_destroy_fq(fq, 0);
+
+	return ret;
+}
+
+/*
+ * TODO: This CAAM FQ empty logic can be improved. We can enqueue a NULL
+ * job descriptor to the FQ. This must be the last enqueue request to the
+ * FQ. When the response of this job comes back, the FQ is empty. Also
+ * holding tanks are guaranteed to be not holding any jobs from this FQ.
+ */
+static int empty_caam_fq(struct qman_fq *fq)
+{
+	int ret;
+	struct qm_mcr_queryfq_np np;
+
+	/* Wait till the older CAAM FQ get empty */
+	do {
+		ret = qman_query_fq_np(fq, &np);
+		if (ret)
+			return ret;
+
+		if (!np.frm_cnt)
+			break;
+
+		msleep(20);
+	} while (1);
+
+	/*
+	 * Give extra time for pending jobs from this FQ in holding tanks
+	 * to get processed
+	 */
+	msleep(20);
+	return 0;
+}
+
+int caam_drv_ctx_update(struct caam_drv_ctx *drv_ctx, u32 *sh_desc)
+{
+	size_t size;
+	u32 num_words;
+	int ret;
+	struct qman_fq *new_fq, *old_fq;
+	struct device *qidev = drv_ctx->qidev;
+
+	/* Check the size of new shared descriptor */
+	num_words = desc_len(sh_desc);
+	if (num_words > MAX_SDLEN) {
+		dev_err(qidev, "Invalid descriptor len: %d words\n",
+			num_words);
+		return -EINVAL;
+	}
+
+	/* Note down older req FQ */
+	old_fq = drv_ctx->req_fq;
+
+	/* Create a new req FQ in parked state */
+	new_fq = create_caam_req_fq(drv_ctx->qidev, drv_ctx->rsp_fq,
+				    drv_ctx->context_a, 0);
+	if (unlikely(IS_ERR_OR_NULL(new_fq))) {
+		dev_err(qidev, "FQ allocation for shdesc update failed\n");
+		return PTR_ERR(new_fq);
+	}
+
+	/* Hook up new FQ to context so that new requests keep queueing */
+	drv_ctx->req_fq = new_fq;
+
+	/* Empty and remove the older FQ */
+	ret = empty_caam_fq(old_fq);
+	if (ret) {
+		dev_err(qidev, "Old SEC FQ empty failed\n");
+
+		/* We can revert to older FQ */
+		drv_ctx->req_fq = old_fq;
+
+		if (kill_fq(qidev, new_fq)) {
+			dev_warn(qidev, "New SEC FQ: %u kill failed\n",
+				 new_fq->fqid);
+		}
+
+		return ret;
+	}
+
+	/*
+	 * Now update the shared descriptor for driver context.
+	 * Re-initialise pre-header. Set RSLS and SDLEN
+	 */
+	drv_ctx->prehdr[0] = (1 << PREHDR_RSLS_SHIFT) | num_words;
+
+	/* Copy the new shared descriptor now */
+	memcpy(drv_ctx->sh_desc, sh_desc, desc_bytes(sh_desc));
+
+	size = sizeof(drv_ctx->sh_desc) + sizeof(drv_ctx->prehdr);
+	dma_sync_single_for_device(qidev, drv_ctx->context_a,
+				   size, DMA_BIDIRECTIONAL);
+
+	/* Put the new FQ in scheduled state */
+	ret = qman_schedule_fq(new_fq);
+	if (ret) {
+		dev_err(qidev, "Fail to sched new SEC FQ, ecode = %d\n", ret);
+
+		/*
+		 * We can kill new FQ and revert to old FQ.
+		 * Since the desc is already modified, it is success case
+		 */
+
+		drv_ctx->req_fq = old_fq;
+
+		if (kill_fq(qidev, new_fq)) {
+			dev_warn(qidev, "New SEC FQ: %u kill failed\n",
+				 new_fq->fqid);
+		}
+	} else {
+		/* Remove older FQ */
+		if (kill_fq(qidev, old_fq)) {
+			dev_warn(qidev, "Old SEC FQ: %u kill failed\n",
+				 old_fq->fqid);
+		}
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL(caam_drv_ctx_update);
+
+
+
+struct caam_drv_ctx *caam_drv_ctx_init(struct device *qidev,
+				       int *cpu,
+				       u32 *sh_desc)
+{
+	size_t size;
+	u32 num_words;
+	dma_addr_t hwdesc;
+	struct qman_fq *rsp_fq;
+	struct caam_drv_ctx *drv_ctx;
+	const cpumask_t *cpus = qman_affine_cpus();
+	static DEFINE_PER_CPU(int, last_cpu);
+
+	num_words = desc_len(sh_desc);
+	if (num_words > MAX_SDLEN) {
+		dev_err(qidev, "Invalid descriptor len: %d words\n",
+			num_words);
+		return ERR_PTR(-EINVAL);
+	}
+
+	drv_ctx = kzalloc(sizeof(*drv_ctx), GFP_ATOMIC);
+	if (!drv_ctx) {
+		dev_err(qidev, "Mem alloc for driver context failed\n");
+		return ERR_PTR(-ENOMEM);
+	}
+
+	/* Initialise pre-header. Set RSLS and SDLEN */
+	drv_ctx->prehdr[0] = (1 << PREHDR_RSLS_SHIFT) | num_words;
+
+	/* Copy the shared descriptor now */
+	memcpy(drv_ctx->sh_desc, sh_desc, desc_bytes(sh_desc));
+
+	/* Map address for pre-header + descriptor */
+	size = sizeof(drv_ctx->prehdr) + sizeof(drv_ctx->sh_desc);
+	hwdesc = dma_map_single(qidev, drv_ctx->prehdr,
+				size, DMA_BIDIRECTIONAL);
+	if (dma_mapping_error(qidev, hwdesc)) {
+		dev_err(qidev, "DMA map error for preheader+shdesc\n");
+		kfree(drv_ctx);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	drv_ctx->context_a = hwdesc;
+
+	/*
+	 * If the given CPU does not own the portal, choose another
+	 * one with a portal.
+	 */
+	if (!cpumask_test_cpu(*cpu, cpus)) {
+		last_cpu = cpumask_next(last_cpu, cpus);
+		if (last_cpu >= nr_cpu_ids)
+			last_cpu = cpumask_first(cpus);
+		 *cpu = last_cpu;
+	}
+
+	drv_ctx->cpu = *cpu;
+
+	/* Find response FQ hooked with this CPU*/
+	rsp_fq = &per_cpu(pcpu_qipriv.rsp_fq, drv_ctx->cpu);
+	drv_ctx->rsp_fq = rsp_fq;
+
+	/*Attach request FQ*/
+	drv_ctx->req_fq = create_caam_req_fq(qidev, rsp_fq,
+					     hwdesc, QMAN_INITFQ_FLAG_SCHED);
+	if (unlikely(IS_ERR_OR_NULL(drv_ctx->req_fq))) {
+		dev_err(qidev, "create_caam_req_fq failed\n");
+		dma_unmap_single(qidev, hwdesc, size, DMA_BIDIRECTIONAL);
+		kfree(drv_ctx);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	drv_ctx->qidev = qidev;
+	return drv_ctx;
+}
+EXPORT_SYMBOL(caam_drv_ctx_init);
+
+void *qi_cache_alloc(gfp_t flags)
+{
+	return kmem_cache_alloc(qi_cache, flags);
+}
+EXPORT_SYMBOL(qi_cache_alloc);
+
+void qi_cache_free(void *obj)
+{
+	kmem_cache_free(qi_cache, obj);
+}
+EXPORT_SYMBOL(qi_cache_free);
+
+static int caam_qi_poll(struct napi_struct *napi, int budget)
+{
+	int cleaned = qman_poll_dqrr(budget);
+
+	if (cleaned < budget) {
+		napi_complete(napi);
+		qman_irqsource_add(QM_PIRQ_DQRI);
+	}
+
+	return cleaned;
+}
+
+
+void caam_drv_ctx_rel(struct caam_drv_ctx *drv_ctx)
+{
+	size_t size;
+
+	if (IS_ERR_OR_NULL(drv_ctx))
+		return;
+
+	size = sizeof(drv_ctx->sh_desc) + sizeof(drv_ctx->prehdr);
+
+	/* Remove request FQ*/
+	if (kill_fq(drv_ctx->qidev, drv_ctx->req_fq))
+		dev_err(drv_ctx->qidev, "Crypto session Req FQ kill failed\n");
+
+	dma_unmap_single(drv_ctx->qidev, drv_ctx->context_a,
+			 size, DMA_BIDIRECTIONAL);
+
+	kfree(drv_ctx);
+}
+EXPORT_SYMBOL(caam_drv_ctx_rel);
+
+int caam_qi_shutdown(struct device *qidev)
+{
+	struct caam_qi_priv *priv = dev_get_drvdata(qidev);
+	int i, ret;
+
+	const cpumask_t *cpus = qman_affine_cpus();
+	struct cpumask old_cpumask = *tsk_cpus_allowed(current);
+
+	for_each_cpu(i, cpus) {
+		napi_disable(&per_cpu(pcpu_qipriv.irqtask, i));
+		netif_napi_del(&per_cpu(pcpu_qipriv.irqtask, i));
+		if (kill_fq(qidev, &per_cpu(pcpu_qipriv.rsp_fq, i)))
+			dev_err(qidev, "Rsp FQ kill failed, cpu: %d\n", i);
+	}
+
+	/*
+	 * QMAN driver requires CGRs to be deleted from same CPU from where
+	 * they were instantiated. Hence we get the module removal execute
+	 * from the same CPU from where it was originally inserted.
+	 */
+	set_cpus_allowed_ptr(current, get_cpu_mask(mod_init_cpu));
+
+	ret = qman_delete_cgr(&priv->rsp_cgr);
+	if (ret)
+		dev_err(qidev, "Delete response CGR failed: %d\n", ret);
+	else
+		qman_release_cgrid(priv->rsp_cgr.cgrid);
+
+	if (qi_cache)
+		kmem_cache_destroy(qi_cache);
+
+	/* Now that we're done with the CGRs, restore the cpus allowed mask */
+	set_cpus_allowed_ptr(current, &old_cpumask);
+
+	platform_device_unregister(priv->qi_pdev);
+	return ret;
+}
+
+static void rsp_cgr_cb(struct qman_portal *qm, struct qman_cgr *cgr,
+			int congested)
+{
+	caam_congested = congested;
+
+	if (congested)
+		pr_warn_ratelimited("CAAM rsp path congested\n");
+	else
+		pr_info_ratelimited("CAAM rsp path congestion state exit\n");
+}
+
+static int caam_qi_napi_schedule(struct napi_struct *napi)
+{
+	/*
+	 * In case of threaded ISR for RT enable kernel,
+	 * in_irq() does not return appropriate value, so use
+	 * in_serving_softirq to distinguish softirq or irq context.
+	 */
+	if (unlikely(in_irq() || !in_serving_softirq())) {
+		/* Disable QMan IRQ and invoke NAPI */
+		int ret = qman_irqsource_remove(QM_PIRQ_DQRI);
+		if (likely(!ret)) {
+			napi_schedule(napi);
+			return 1;
+		}
+	}
+	return 0;
+}
+
+static enum qman_cb_dqrr_result caam_rsp_fq_dqrr_cb(struct qman_portal *p,
+					struct qman_fq *rsp_fq,
+					const struct qm_dqrr_entry *dqrr)
+{
+	struct napi_struct *napi;
+	struct caam_drv_req *drv_req;
+	const struct qm_fd *fd;
+	const size_t size = 2 * sizeof(struct qm_sg_entry);
+	struct device *qidev = &per_cpu(pcpu_qipriv.net_dev,
+					smp_processor_id()).dev;
+
+	napi = &per_cpu(pcpu_qipriv.irqtask, smp_processor_id());
+	if (caam_qi_napi_schedule(napi))
+		return qman_cb_dqrr_stop;
+
+	fd = &dqrr->fd;
+	if (unlikely(fd->status))
+		dev_err(qidev, "Error: %#x in CAAM response FD\n", fd->status);
+
+	if (unlikely(qm_fd_compound != fd->format)) {
+		dev_err(qidev, "Non compound FD from CAAM\n");
+		return qman_cb_dqrr_consume;
+	}
+
+	drv_req = (struct caam_drv_req *)phys_to_virt(fd->addr);
+	if (unlikely(!drv_req)) {
+		dev_err(qidev,
+			"Can't find original request for caam response\n");
+		return qman_cb_dqrr_consume;
+	}
+
+	dma_unmap_single(drv_req->drv_ctx->qidev, fd->addr,
+			 size, DMA_BIDIRECTIONAL);
+
+	drv_req->cbk(drv_req, fd->status);
+
+	return qman_cb_dqrr_consume;
+}
+
+static int alloc_rsp_fq_cpu(struct device *qidev, unsigned int cpu)
+{
+	struct qm_mcc_initfq opts;
+	struct qman_fq *fq;
+	int ret;
+	u32 flags;
+
+	fq = &per_cpu(pcpu_qipriv.rsp_fq, cpu);
+
+	fq->cb.dqrr = caam_rsp_fq_dqrr_cb;
+
+	flags = QMAN_FQ_FLAG_NO_ENQUEUE | QMAN_FQ_FLAG_DYNAMIC_FQID;
+
+	ret = qman_create_fq(0, flags, fq);
+	if (ret) {
+		dev_err(qidev, "Rsp FQ create failed\n");
+		return -ENODEV;
+	}
+
+	flags = QMAN_INITFQ_FLAG_SCHED;
+
+	opts.we_mask = QM_INITFQ_WE_FQCTRL | QM_INITFQ_WE_DESTWQ |
+		QM_INITFQ_WE_CONTEXTB | QM_INITFQ_WE_CONTEXTA |
+		QM_INITFQ_WE_CGID | QMAN_INITFQ_FLAG_LOCAL;
+
+	opts.fqd.fq_ctrl = QM_FQCTRL_CTXASTASHING |
+			   QM_FQCTRL_CPCSTASH |
+			   QM_FQCTRL_CGE;
+
+	opts.fqd.dest.channel = qman_affine_channel(cpu);
+	opts.fqd.cgid = qipriv.rsp_cgr.cgrid;
+	opts.fqd.dest.wq = 0;
+	opts.fqd.context_a.stashing.exclusive =
+					QM_STASHING_EXCL_CTX |
+					QM_STASHING_EXCL_DATA;
+
+	opts.fqd.context_a.stashing.data_cl = 1;
+	opts.fqd.context_a.stashing.context_cl = 1;
+
+	ret = qman_init_fq(fq, flags, &opts);
+	if (ret) {
+		dev_err(qidev, "Rsp FQ init failed\n");
+		return -ENODEV;
+	}
+#ifdef DEBUG
+	dev_info(qidev, "Allocated response FQ %u for CPU %u",
+		 fq->fqid, cpu);
+#endif
+	return 0;
+}
+
+static int alloc_cgrs(struct device *qidev)
+{
+	struct qm_mcc_initcgr opts;
+	int ret;
+	const u64 cpus = *(u64 *)qman_affine_cpus();
+	const int num_cpus = __builtin_popcountll(cpus);
+	u64 val;
+
+	/*Allocate response CGR*/
+	ret = qman_alloc_cgrid(&qipriv.rsp_cgr.cgrid);
+	if (ret) {
+		dev_err(qidev, "CGR alloc failed for rsp FQs");
+		return ret;
+	}
+
+	qipriv.rsp_cgr.cb = rsp_cgr_cb;
+	memset(&opts, 0, sizeof(opts));
+	opts.we_mask = QM_CGR_WE_CSCN_EN | QM_CGR_WE_CS_THRES |
+			QM_CGR_WE_MODE;
+	opts.cgr.cscn_en = QM_CGR_EN;
+	opts.cgr.mode = QMAN_CGR_MODE_FRAME;
+#ifdef CONFIG_FSL_DPAA_ETH
+	/*
+	 * This effectively sets the to-CPU threshold equal to half of the
+	 * number of buffers available to dpa_eth driver. It means that at most
+	 * half of the buffers can be in the queues from SEC, waiting
+	 * to be transmitted to the core (and then on the TX queues).
+	 * NOTE: This is an arbitrary division; the factor '2' below could
+	 *       also be '3' or '4'. It also depends on the number of devices
+	 *       using the dpa_eth buffers (which can be >1 if f.i. PME/DCE are
+	 *       also used.
+	 */
+	val = num_cpus * CONFIG_FSL_DPAA_ETH_MAX_BUF_COUNT / 2;
+#else
+	val = num_cpus * MAX_RSP_FQ_BACKLOG_PER_CPU;
+#endif
+	qm_cgr_cs_thres_set64(&opts.cgr.cs_thres, val, 1);
+
+	ret = qman_create_cgr(&qipriv.rsp_cgr,
+				QMAN_CGR_FLAG_USE_INIT, &opts);
+	if (ret) {
+		dev_err(qidev, "Error %d creating CAAM rsp CGRID: %u\n",
+			ret, qipriv.rsp_cgr.cgrid);
+		return ret;
+	}
+#ifdef DEBUG
+	dev_info(qidev, "CAAM to CPU threshold set to %llu\n", val);
+#endif
+	return 0;
+}
+
+static int alloc_rsp_fqs(struct device *qidev)
+{
+	const cpumask_t *cpus = qman_affine_cpus();
+	int ret, i;
+
+	/*Now create response FQs*/
+	for_each_cpu(i, cpus) {
+		ret = alloc_rsp_fq_cpu(qidev, i);
+		if (ret) {
+			dev_err(qidev, "CAAM rsp FQ alloc failed, cpu: %u", i);
+			return ret;
+		}
+	}
+
+	return 0;
+}
+
+int caam_qi_init(struct platform_device *caam_pdev, struct device_node *np)
+{
+	struct platform_device *qi_pdev;
+	struct device *ctrldev, *qidev;
+	struct caam_drv_private *ctrlpriv;
+	int err, i;
+	const cpumask_t *cpus = qman_affine_cpus();
+	struct cpumask old_cpumask = *tsk_cpus_allowed(current);
+
+	/*
+	 * QMAN requires that CGR must be removed from same CPU+portal from
+	 * where it was originally allocated. Hence we need to note down
+	 * the initialisation CPU and use the same CPU for module exit.
+	 * We select the first CPU to from the list of portal owning
+	 * CPUs. Then we pin module init to this CPU.
+	 */
+	mod_init_cpu = cpumask_first(cpus);
+	set_cpus_allowed_ptr(current, get_cpu_mask(mod_init_cpu));
+
+	qi_pdev = platform_device_register_simple("caam_qi", 0, NULL, 0);
+	if (IS_ERR(qi_pdev))
+		return PTR_ERR(qi_pdev);
+
+	ctrldev = &caam_pdev->dev;
+	ctrlpriv = dev_get_drvdata(ctrldev);
+	qidev = &qi_pdev->dev;
+
+	qipriv.qi_pdev = qi_pdev;
+	dev_set_drvdata(qidev, &qipriv);
+
+	/* Copy dma mask from controlling device */
+	err = dma_set_mask(qidev, dma_get_mask(ctrldev));
+	if (err) {
+		platform_device_unregister(qi_pdev);
+		return -ENODEV;
+	}
+
+	/* Response path cannot be congested */
+	caam_congested = false;
+
+	/* kmem_cache wasn't yet allocated */
+	qi_cache = NULL;
+
+	/* Initialise the CGRs congestion detection */
+	err = alloc_cgrs(qidev);
+	if (err) {
+		dev_err(qidev, "Can't allocate CGRs\n");
+		platform_device_unregister(qi_pdev);
+		return err;
+	}
+
+	/* Initialise response FQs */
+	err = alloc_rsp_fqs(qidev);
+	if (err) {
+		dev_err(qidev, "Can't allocate SEC response FQs\n");
+		platform_device_unregister(qi_pdev);
+		return err;
+	}
+
+	/*
+	 * Enable the NAPI contexts on each of the core which has a affine
+	 * portal.
+	 */
+	for_each_cpu(i, cpus) {
+		per_cpu(pcpu_qipriv.net_dev, i).dev = *qidev;
+
+		INIT_LIST_HEAD(&per_cpu(pcpu_qipriv.net_dev, i).napi_list);
+
+		netif_napi_add(&per_cpu(pcpu_qipriv.net_dev, i),
+			       &per_cpu(pcpu_qipriv.irqtask, i),
+			       caam_qi_poll, CAAM_NAPI_WEIGHT);
+
+		napi_enable(&per_cpu(pcpu_qipriv.irqtask, i));
+	}
+
+	/* Hook up QI device to parent controlling caam device */
+	ctrlpriv->qidev = qidev;
+
+	qi_cache = kmem_cache_create("caamqicache", 256, 0,
+				     SLAB_CACHE_DMA, NULL);
+	if (!qi_cache) {
+		dev_err(qidev, "Can't allocate SEC cache\n");
+		platform_device_unregister(qi_pdev);
+		return err;
+	}
+
+	/* Done with the CGRs; restore the cpus allowed mask */
+	set_cpus_allowed_ptr(current, &old_cpumask);
+
+	dev_info(qidev, "Linux CAAM Queue I/F driver initialised\n");
+
+	return 0;
+}
diff --git a/drivers/crypto/caam/qi.h b/drivers/crypto/caam/qi.h
new file mode 100644
index 0000000..33f7470
--- /dev/null
+++ b/drivers/crypto/caam/qi.h
@@ -0,0 +1,177 @@
+/*
+ * CAAM public-level include definitions for the QI backend
+ *
+ * Copyright 2013 Freescale Semiconductor, Inc.
+ */
+
+#ifndef __QI_H__
+#define __QI_H__
+
+#include "compat.h"
+#include "desc.h"
+#include "linux/fsl_qman.h"
+
+/*
+ * The CAAM QI hardware constructs a job descriptor which points
+ * to shared descriptor (as pointed by context_a of FQ to CAAM).
+ * When the job descriptor is executed by deco, the whole job
+ * descriptor together with shared descriptor gets loaded in
+ * deco buffer which is 64 words long (each 32-bit).
+ *
+ * The job descriptor constructed by QI hardware has layout:
+ *
+ *	HEADER		(1 word)
+ *	Shdesc ptr	(1 or 2 words)
+ *	SEQ_OUT_PTR	(1 word)
+ *	Out ptr		(1 or 2 words)
+ *	Out length	(1 word)
+ *	SEQ_IN_PTR	(1 word)
+ *	In ptr		(1 or 2 words)
+ *	In length	(1 word)
+ *
+ * The shdesc ptr is used to fetch shared descriptor contents
+ * into deco buffer.
+ *
+ * Apart from shdesc contents, the total number of words that
+ * get loaded in deco buffer are '8' or '11'. The remaining words
+ * in deco buffer can be used for storing shared descriptor.
+ */
+#define MAX_SDLEN	((CAAM_DESC_BYTES_MAX - DESC_JOB_IO_LEN)/CAAM_CMD_SZ)
+
+/*
+ * This is the request structure the driver application should fill while
+ * submitting a job to driver.
+ */
+struct caam_drv_req;
+
+/*
+ * Application's callback function invoked by the driver when the request
+ * has been successfully processed.
+ *
+ * drv_req:	Original request that was submitted
+ * stats:	Completion status of request.
+ *		0		- Success
+ *		Non-zero	- Error code
+ */
+typedef void (*caam_qi_cbk)(struct caam_drv_req *drv_req,
+			    u32 status);
+
+/*
+ * The jobs are processed by the driver against a driver context.
+ * With every cryptographic context, a driver context is attached.
+ * The driver context contains data for private use by driver.
+ * For the applications, this is an opaque structure.
+ */
+struct caam_drv_ctx;
+
+/*
+ * This is the request structure the driver application should fill while
+ * submitting a job to driver.
+ *
+ * fd_sgt[0] - QMAN S/G pointing to output buffer
+ * fd_sgt[1] - QMAN S/G pointing to input buffer
+ * cbk	     - Callback function to invoke when job is completed
+ * app_ctx   - Arbit context attached with request by the application
+ *
+ * The fields mentioned below should not be used by application.
+ * These are for private use by driver.
+ *
+ * hdr__     - Linked list header to maintain list of outstanding requests
+ *	       to CAAM.
+ * hwaddr    - DMA address for the S/G table.
+ */
+struct caam_drv_req {
+	struct qm_sg_entry fd_sgt[2];
+	struct caam_drv_ctx *drv_ctx;
+	caam_qi_cbk cbk;
+	void *app_ctx;
+} ____cacheline_aligned;
+
+/*
+ * caam_drv_ctx_init - Initialise a QI drv context.
+ *
+ * A QI driver context must be attached with each cryptographic context.
+ * This function allocates memory for QI context an returns a handle to
+ * the application. This handle must be submitted along with each enqueue
+ * request to the driver by the application.
+ *
+ * cpu	-	CPU where the application prefers to the driver to receive
+ *		CAAM responses. The request completion callback would be
+ *		issued from this CPU.
+ * sh_desc -	Shared descriptor pointer to be attached with QI driver
+ *		context.
+ *
+ * Returns a driver context on success or negative error code on failure.
+ */
+extern struct caam_drv_ctx *caam_drv_ctx_init(struct device *qidev,
+					      int *cpu, u32 *sh_desc);
+
+/*
+ * caam_qi_enqueue - Submit a request to QI backend driver.
+ *
+ * The request structure must be properly filled as described above.
+ *
+ * Returns 0 on success or negative error code on failure.
+ */
+extern int caam_qi_enqueue(struct device *qidev, struct caam_drv_req *req);
+
+/*
+ * caam_drv_ctx_busy - Check if there are too many jobs pending with CAAM.
+ *		       or too many CAAM responses are pending to be processed.
+ *
+ * drv_ctx - Driver context for which job is to be submitted.
+ *
+ * Returns caam congestion status 'true/false'
+ */
+extern bool caam_drv_ctx_busy(struct caam_drv_ctx *drv_ctx);
+
+/*
+ * caam_drv_ctx_update - Upate QI drv context.
+ *
+ * Invoked when shared descriptor is required to be change in driver context.
+ *
+ * drv_ctx -	Driver context to be updated
+ *
+ * sh_desc -	New shared descriptor pointer to be updated in QI driver
+ *		context.
+ *
+ * Returns 0 on success or negative error code on failure.
+ */
+extern int caam_drv_ctx_update(struct caam_drv_ctx *drv_ctx, u32 *sh_desc);
+
+/*
+ * caam_drv_ctx_rel - Release a QI driver context.
+ *
+ * drv_ctx - Context to be released.
+ *
+ */
+extern void caam_drv_ctx_rel(struct caam_drv_ctx *drv_ctx);
+
+extern int caam_qi_init(struct platform_device *pdev, struct device_node *np);
+extern int caam_qi_shutdown(struct device *dev);
+
+/*
+ * qi_cache_alloc - Allocate buffers from CAAM-QI cache
+ *
+ * Invoked when a user of the CAAM-QI (i.e. caamalg-qi) needs data which has
+ * to be allocated on the hotpath. Instead of using malloc, one can use the
+ * services of the CAAM QI memory cache (backed by kmem_cache). The buffers
+ * will have a size of 256B, which is sufficient for hosting 16 SG entries.
+ *
+ * flags -	flags that would be used for the equivalent malloc(..) call
+ * *
+ * Returns a pointer to a retrieved buffer on success or NULL on failure.
+ */
+extern void *qi_cache_alloc(gfp_t flags);
+
+/*
+ * qi_cache_free - Frees buffers allocated from CAAM-QI cache
+ *
+ * Invoked when a user of the CAAM-QI (i.e. caamalg-qi) no longer needs
+ * the buffer previously allocated by a qi_cache_alloc call.
+ * No checking is being done, the call is a passthrough call to
+ * kmem_cache_free(...)
+ */
+extern void qi_cache_free(void *obj);
+
+#endif /* QI_H */
diff --git a/drivers/crypto/caam/regs.h b/drivers/crypto/caam/regs.h
index d50174f..91b47f4 100644
--- a/drivers/crypto/caam/regs.h
+++ b/drivers/crypto/caam/regs.h
@@ -64,6 +64,12 @@
  *   performance counters.
  *
  */
+ 
+#ifdef CONFIG_ARM
+/* These are common macros for Power, put here for ARMs */
+#define setbits32(_addr, _v) writel((readl(_addr) | (_v)), (_addr))
+#define clrbits32(_addr, _v) writel((readl(_addr) & ~(_v)), (_addr))
+#endif
 
 #ifdef __BIG_ENDIAN
 #define wr_reg32(reg, data) out_be32(reg, data)
@@ -160,6 +166,8 @@ struct sec_vid {
 	u8 min_rev;
 };
 
+#define SEC_ID_MS_T1040		0x0A120400
+
 struct caam_perfmon {
 	/* Performance Monitor Registers			f00-f9f */
 	u64 req_dequeued;	/* PC_REQ_DEQ - Dequeued Requests	     */
diff --git a/drivers/crypto/caam/sg_sw_qm.h b/drivers/crypto/caam/sg_sw_qm.h
new file mode 100644
index 0000000..778e4be
--- /dev/null
+++ b/drivers/crypto/caam/sg_sw_qm.h
@@ -0,0 +1,82 @@
+/* Copyright 2013 Freescale Semiconductor, Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of Freescale Semiconductor nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ *
+ *
+ * ALTERNATIVELY, this software may be distributed under the terms of the
+ * GNU General Public License ("GPL") as published by the Free Software
+ * Foundation, either version 2 of that License or (at your option) any
+ * later version.
+ *
+ * THIS SOFTWARE IS PROVIDED BY Freescale Semiconductor ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL Freescale Semiconductor BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef __SG_SW_QM_H
+#define __SG_SW_QM_H
+
+#include "linux/fsl_qman.h"
+
+static inline void dma_to_qm_sg_one(struct qm_sg_entry *qm_sg_ptr,
+				      dma_addr_t dma, u32 len, u16 offset)
+{
+	qm_sg_ptr->addr = dma;
+	qm_sg_ptr->extension = 0;
+	qm_sg_ptr->final = 0;
+	qm_sg_ptr->length = len;
+	qm_sg_ptr->__reserved2 = 0;
+	qm_sg_ptr->bpid = 0;
+	qm_sg_ptr->__reserved3 = 0;
+	qm_sg_ptr->offset = offset & QM_SG_OFFSET_MASK;
+}
+
+/*
+ * convert scatterlist to h/w link table format
+ * but does not have final bit; instead, returns last entry
+ */
+static inline struct qm_sg_entry *
+sg_to_qm_sg(struct scatterlist *sg, int sg_count,
+	    struct qm_sg_entry *qm_sg_ptr, u16 offset)
+{
+	while (sg_count && sg) {
+		dma_to_qm_sg_one(qm_sg_ptr, sg_dma_address(sg),
+				 sg_dma_len(sg), offset);
+		qm_sg_ptr++;
+		sg = scatterwalk_sg_next(sg);
+		sg_count--;
+	}
+	return qm_sg_ptr - 1;
+}
+
+
+/*
+ * convert scatterlist to h/w link table format
+ * scatterlist must have been previously dma mapped
+ */
+static inline void sg_to_qm_sg_last(struct scatterlist *sg, int sg_count,
+				      struct qm_sg_entry *qm_sg_ptr,
+				      u16 offset)
+{
+	qm_sg_ptr = sg_to_qm_sg(sg, sg_count, qm_sg_ptr, offset);
+	qm_sg_ptr->final = 1;
+}
+
+#endif /* __SG_SW_QM_H */
+
diff --git a/drivers/crypto/caam/sg_sw_sec4.h b/drivers/crypto/caam/sg_sw_sec4.h
index ce28a56..752134c 100644
--- a/drivers/crypto/caam/sg_sw_sec4.h
+++ b/drivers/crypto/caam/sg_sw_sec4.h
@@ -11,13 +11,13 @@ struct sec4_sg_entry;
  * convert single dma address to h/w link table format
  */
 static inline void dma_to_sec4_sg_one(struct sec4_sg_entry *sec4_sg_ptr,
-				      dma_addr_t dma, u32 len, u32 offset)
+				      dma_addr_t dma, u32 len, u16 offset)
 {
 	sec4_sg_ptr->ptr = dma;
 	sec4_sg_ptr->len = len;
 	sec4_sg_ptr->reserved = 0;
 	sec4_sg_ptr->buf_pool_id = 0;
-	sec4_sg_ptr->offset = offset;
+	sec4_sg_ptr->offset = offset & SEC4_SG_OFFSET_MASK;
 #ifdef DEBUG
 	print_hex_dump(KERN_ERR, "sec4_sg_ptr@: ",
 		       DUMP_PREFIX_ADDRESS, 16, 4, sec4_sg_ptr,
@@ -31,9 +31,12 @@ static inline void dma_to_sec4_sg_one(struct sec4_sg_entry *sec4_sg_ptr,
  */
 static inline struct sec4_sg_entry *
 sg_to_sec4_sg(struct scatterlist *sg, int sg_count,
-	      struct sec4_sg_entry *sec4_sg_ptr, u32 offset)
+	      struct sec4_sg_entry *sec4_sg_ptr, u16 offset)
 {
-	while (sg_count) {
+	if (!sg)
+		return NULL;
+
+	while (sg_count && sg) {
 		dma_to_sec4_sg_one(sec4_sg_ptr, sg_dma_address(sg),
 				   sg_dma_len(sg), offset);
 		sec4_sg_ptr++;
@@ -49,8 +52,10 @@ sg_to_sec4_sg(struct scatterlist *sg, int sg_count,
  */
 static inline void sg_to_sec4_sg_last(struct scatterlist *sg, int sg_count,
 				      struct sec4_sg_entry *sec4_sg_ptr,
-				      u32 offset)
+				      u16 offset)
 {
+	if (!sg)
+		return;
 	sec4_sg_ptr = sg_to_sec4_sg(sg, sg_count, sec4_sg_ptr, offset);
 	sec4_sg_ptr->len |= SEC4_SG_LEN_FIN;
 }
@@ -62,7 +67,7 @@ static inline int __sg_count(struct scatterlist *sg_list, int nbytes,
 	struct scatterlist *sg = sg_list;
 	int sg_nents = 0;
 
-	while (nbytes > 0) {
+	while (nbytes > 0 && sg) {
 		sg_nents++;
 		nbytes -= sg->length;
 		if (!sg_is_last(sg) && (sg + 1)->length == 0)
@@ -85,13 +90,16 @@ static inline int sg_count(struct scatterlist *sg_list, int nbytes,
 	return sg_nents;
 }
 
-static int dma_map_sg_chained(struct device *dev, struct scatterlist *sg,
-			      unsigned int nents, enum dma_data_direction dir,
-			      bool chained)
+static int __maybe_unused dma_map_sg_chained(struct device *dev,
+	struct scatterlist *sg, unsigned int nents, enum dma_data_direction dir,
+	bool chained)
 {
+	if (!sg || !nents)
+		return 0;
+
 	if (unlikely(chained)) {
 		int i;
-		for (i = 0; i < nents; i++) {
+		for (i = 0; i < nents && sg; i++) {
 			dma_map_sg(dev, sg, 1, dir);
 			sg = scatterwalk_sg_next(sg);
 		}
@@ -101,18 +109,20 @@ static int dma_map_sg_chained(struct device *dev, struct scatterlist *sg,
 	return nents;
 }
 
-static int dma_unmap_sg_chained(struct device *dev, struct scatterlist *sg,
-				unsigned int nents, enum dma_data_direction dir,
-				bool chained)
+static void __maybe_unused dma_unmap_sg_chained(struct device *dev,
+	struct scatterlist *sg, unsigned int nents, enum dma_data_direction dir,
+	bool chained)
 {
+	if (!sg || !nents)
+		return;
+
 	if (unlikely(chained)) {
 		int i;
-		for (i = 0; i < nents; i++) {
+		for (i = 0; i < nents && sg; i++) {
 			dma_unmap_sg(dev, sg, 1, dir);
 			sg = scatterwalk_sg_next(sg);
 		}
 	} else {
 		dma_unmap_sg(dev, sg, nents, dir);
 	}
-	return nents;
 }
-- 
1.7.5.4

