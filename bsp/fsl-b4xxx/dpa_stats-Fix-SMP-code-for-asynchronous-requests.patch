From 438d3dede93d705151b4687f3b4407569fb5bdd5 Mon Sep 17 00:00:00 2001
From: Anca-Jeanina Floarea <anca.floarea@freescale.com>
Date: Thu, 25 Oct 2012 23:13:21 +0000
Subject: [PATCH 334/518] dpa_stats: Fix SMP code for asynchronous requests

The storage area offset is used to identify a specific
asynchronous request and obtain the user application
callback. A hash table implementation was used in order
to minimize the time needed to search for a callback based
on the storage area offset.

Signed-off-by: Anca Jeanina FLOAREA <anca.floarea@freescale.com>
[Grabbed from the branch, LINUX_IR5.2.0, of
https://git.freescale.com/git-private/cgit.cgi/ppc/alu-b4860/linux.git.]
Signed-off-by: Tiejun Chen <tiejun.chen@windriver.com>
---
 drivers/staging/fsl_dpa_offload/wrp_dpa_stats.c |  258 +++++++++++++++++------
 drivers/staging/fsl_dpa_offload/wrp_dpa_stats.h |   26 ++-
 2 files changed, 216 insertions(+), 68 deletions(-)

diff --git a/drivers/staging/fsl_dpa_offload/wrp_dpa_stats.c b/drivers/staging/fsl_dpa_offload/wrp_dpa_stats.c
index 7fb42e7..f0e439c 100644
--- a/drivers/staging/fsl_dpa_offload/wrp_dpa_stats.c
+++ b/drivers/staging/fsl_dpa_offload/wrp_dpa_stats.c
@@ -33,9 +33,9 @@
 /*
  * DPA Stats Wrapper implementation.
  */
-#include <linux/fsl_dpa_stats.h>
 #include "wrp_dpa_stats.h"
 #include "dpa_stats_ioctl.h"
+#include "crc8.h"
 
 /* Other includes */
 #include <linux/kernel.h>
@@ -79,6 +79,9 @@ static int copy_key_descriptor(struct dpa_offload_lookup_key *src,
 
 static int copy_class_members(void *objs, unsigned int size, void *dst);
 
+static long store_get_cnts_async_params(
+		struct ioc_dpa_stats_cnt_request_params *kprm);
+
 int wrp_dpa_stats_init(void)
 {
 	/* Cannot initialize the wrapper twice */
@@ -180,7 +183,7 @@ static void wrp_dpa_stats_event_queue_init(
 		struct dpa_stats_event_queue *event_queue)
 {
 	INIT_LIST_HEAD(&event_queue->lh);
-	spin_lock_init(&event_queue->lock);
+	mutex_init(&wrp_dpa_stats.event_queue_lock);
 	atomic_set(&event_queue->count, 0);
 	init_waitqueue_head(&event_queue->wq);
 }
@@ -189,24 +192,21 @@ static void wrp_dpa_stats_event_queue_free(
 		struct dpa_stats_event_queue *event_queue)
 {
 	struct dpa_stats_event   *entry, *tmp;
-	unsigned long		 irqFlags;
 
 	/* Remove remaining events from the event queue */
-	spin_lock_irqsave(&event_queue->lock, irqFlags);
+	mutex_lock(&wrp_dpa_stats.event_queue_lock);
 	list_for_each_entry_safe(entry, tmp, &event_queue->lh, lh)
 	{
 		list_del(&entry->lh);
 		atomic_dec(&event_queue->count);
 		kfree(entry);
 	}
-	spin_unlock_irqrestore(&event_queue->lock, irqFlags);
+	mutex_unlock(&wrp_dpa_stats.event_queue_lock);
 }
 
 static int wrp_dpa_stats_queue_event(struct dpa_stats_event_queue *event_queue,
 		struct dpa_stats_event *event)
 {
-	unsigned long    irqFlags;
-
 	/* If the event queue is already full, abort: */
 	if (atomic_read(&event_queue->count) >= QUEUE_MAX_EVENTS) {
 		pr_err("Event queue is full!\n");
@@ -214,11 +214,10 @@ static int wrp_dpa_stats_queue_event(struct dpa_stats_event_queue *event_queue,
 	}
 
 	/* Add the event to the event queue */
-	spin_lock_irqsave(&event_queue->lock, irqFlags);
+	mutex_lock(&wrp_dpa_stats.event_queue_lock);
 	list_add_tail(&event->lh, &event_queue->lh);
 	atomic_inc(&event_queue->count);
-	spin_unlock_irqrestore(&event_queue->lock, irqFlags);
-
+	mutex_unlock(&wrp_dpa_stats.event_queue_lock);
 	/* Wake up consumers */
 	wake_up_interruptible(&event_queue->wq);
 
@@ -230,15 +229,14 @@ static struct dpa_stats_event *wrp_dpa_stats_dequeue_event(
 		struct dpa_stats_event_queue *event_queue, unsigned int block)
 {
 	struct dpa_stats_event	*event;
-	unsigned long	irqFlags;
 
 	/* If the event queue is empty we perform an interruptible sleep
 	 * until an event is inserted into the queue. We use the event queue
 	 * spinlock to protect ourselves from race conditions. */
-	spin_lock_irqsave(&event_queue->lock, irqFlags);
+	mutex_lock(&wrp_dpa_stats.event_queue_lock);
 
 	while (list_empty(&event_queue->lh)) {
-		spin_unlock_irqrestore(&event_queue->lock, irqFlags);
+		mutex_unlock(&wrp_dpa_stats.event_queue_lock);
 
 		/* If a non blocking action was requested, return failure: */
 		if (block & O_NONBLOCK)
@@ -249,7 +247,7 @@ static struct dpa_stats_event *wrp_dpa_stats_dequeue_event(
 			/* Woken up by some signal... */
 			return NULL;
 
-		spin_lock_irqsave(&event_queue->lock, irqFlags);
+		mutex_lock(&wrp_dpa_stats.event_queue_lock);
 	}
 
 	/* Consume one event */
@@ -257,7 +255,7 @@ static struct dpa_stats_event *wrp_dpa_stats_dequeue_event(
 			struct dpa_stats_event, lh);
 	list_del(&event->lh);
 	atomic_dec(&event_queue->count);
-	spin_unlock_irqrestore(&event_queue->lock, irqFlags);
+	mutex_unlock(&wrp_dpa_stats.event_queue_lock);
 
 	return event;
 }
@@ -267,20 +265,55 @@ void do_ioctl_req_done_cb(int dpa_stats_id,
 		int bytes_written)
 {
 	struct dpa_stats_event *event = NULL;
+	struct dpa_stats_async_req_ev *async_req_ev;
+	struct list_head *async_req_grp, *pos;
+	uint8_t grp_idx = 0;
+	bool found = false;
+
+	/* Obtain the group the request belongs to */
+	grp_idx = crc8((uint8_t *)&storage_area_offset, sizeof(unsigned int));
+	async_req_grp = &wrp_dpa_stats.async_req_group[grp_idx];
+	mutex_lock(&wrp_dpa_stats.async_req_lock);
+	BUG_ON(list_empty(async_req_grp));
+
+	/* Search in the request group the request event */
+	list_for_each(pos, async_req_grp) {
+		async_req_ev = list_entry(pos,
+				struct dpa_stats_async_req_ev,
+				node);
+
+		if (async_req_ev->storage_area_offset == storage_area_offset) {
+			list_del(&async_req_ev->node);
+			list_add_tail(&async_req_ev->node,
+					&wrp_dpa_stats.async_req_pool);
+			found = true;
+			break;
+		}
+	}
+
+	if (!found) {
+		pr_err("Event was not found in the event list!\n");
+		mutex_unlock(&wrp_dpa_stats.async_req_lock);
+		return;
+	}
 
 	/* Generate new event description: */
 	event = (struct dpa_stats_event *)
 			kmalloc(sizeof(struct dpa_stats_event), GFP_KERNEL);
 	if (!event) {
 		pr_err("No more memory for events !\n");
+		mutex_unlock(&wrp_dpa_stats.async_req_lock);
 		return;
 	}
 
-	/* Fill up the event parameters data structure: */
+	/* Fill up the event parameters data structure */
 	event->params.dpa_stats_id = dpa_stats_id;
 	event->params.storage_area_offset = storage_area_offset;
 	event->params.cnts_written = cnts_written;
 	event->params.bytes_written = bytes_written;
+	event->params.request_done = async_req_ev->request_done;
+
+	mutex_unlock(&wrp_dpa_stats.async_req_lock);
 
 	/* Queue this event */
 	if (wrp_dpa_stats_queue_event(&wrp_dpa_stats.ev_queue, event) != 0) {
@@ -292,40 +325,92 @@ void do_ioctl_req_done_cb(int dpa_stats_id,
 	return;
 }
 
-static long do_ioctl_stats_init(void *args)
+static long do_ioctl_stats_init(struct ioc_dpa_stats_params *prm)
 {
-	struct ioc_dpa_stats_params kparam;
+	struct dpa_stats_async_req_ev *async_req_ev;
 	long ret = 0;
-
-	if (copy_from_user(&kparam, (void *) args, sizeof(kparam))) {
-		pr_err("Could not read dpa_stats_init user space args");
-		return -EBUSY;
-	}
+	uint16_t i;
 
 	/* Save user-space memory area pointer */
-	wrp_dpa_stats.us_mem = kparam.stats_params.storage_area;
+	wrp_dpa_stats.us_mem = prm->stats_params.storage_area;
 
 	/* Allocate kernel-space memory area to use for storing the statistics*/
-	kparam.stats_params.storage_area = kzalloc(
-			kparam.stats_params.storage_area_len, GFP_KERNEL);
-	if (!kparam.stats_params.storage_area) {
+	prm->stats_params.storage_area = kzalloc(
+			prm->stats_params.storage_area_len, GFP_KERNEL);
+	if (!prm->stats_params.storage_area) {
 		pr_err("Could not allocate kernel storage area");
 		return -ENOMEM;
 	}
 
 	/* Save kernel-space memory area pointer */
-	wrp_dpa_stats.k_mem = kparam.stats_params.storage_area;
+	wrp_dpa_stats.k_mem = prm->stats_params.storage_area;
 
 	/* Call init function */
-	ret = dpa_stats_init(&kparam.stats_params, &kparam.dpa_stats_id);
+	ret = dpa_stats_init(&prm->stats_params, &prm->dpa_stats_id);
 	if (ret < 0)
 		return ret;
 
-	if (copy_to_user((void *) args, &kparam, sizeof(kparam))) {
-		pr_err("Could not write dpa_stats_init result");
-		return -EBUSY;
+	/* Init CRC8 table */
+	init_crc8_table(CRC8_WCDMA_POLY);
+
+	/* Allocate asynchronous requests groups lists */
+	wrp_dpa_stats.async_req_group = kmalloc(DPA_STATS_MAX_NUM_OF_REQUESTS *
+			sizeof(struct list_head), GFP_KERNEL);
+	if (!wrp_dpa_stats.async_req_group) {
+		pr_err("Could not allocate memory for async requests group\n");
+		return -ENOMEM;
 	}
 
+	/* Initialize list of free async requests nodes */
+	INIT_LIST_HEAD(&wrp_dpa_stats.async_req_pool);
+
+	for (i = 0; i < DPA_STATS_MAX_NUM_OF_REQUESTS; i++) {
+
+		/* Initialize the list of async requests in the same group */
+		INIT_LIST_HEAD(&wrp_dpa_stats.async_req_group[i]);
+
+		/* Allocate an asynchronous request event node */
+		async_req_ev = kzalloc(sizeof(*async_req_ev), GFP_KERNEL);
+		if (!async_req_ev) {
+			pr_err("Could not allocate "
+				"memory for asynchronous request event\n");
+			return -ENOMEM;
+		}
+
+		list_add_tail(&async_req_ev->node,
+				&wrp_dpa_stats.async_req_pool);
+	}
+
+	mutex_init(&wrp_dpa_stats.async_req_lock);
+
+	return ret;
+}
+
+static long do_ioctl_stats_free(void *args)
+{
+	struct dpa_stats_async_req_ev *async_req_ev, *tmp;
+	int dpa_stats_id;
+	long ret;
+
+	if (copy_from_user(&dpa_stats_id, (int *)args,
+			    sizeof(int))) {
+		pr_err("Could not copy parameters\n");
+		return -EINVAL;
+	}
+
+	/* Release kernel allocated memory */
+	kfree(wrp_dpa_stats.k_mem);
+
+	list_for_each_entry_safe(async_req_ev,
+			tmp, &wrp_dpa_stats.async_req_pool, node) {
+		list_del(&async_req_ev->node);
+		kfree(async_req_ev);
+	}
+
+	ret = dpa_stats_free(dpa_stats_id);
+	if (ret < 0)
+		return ret;
+
 	return ret;
 }
 
@@ -631,7 +716,7 @@ static int do_ioctl_stats_modify_class_counter(void *args)
 static int do_ioctl_stats_get_counters(void *args)
 {
 	struct ioc_dpa_stats_cnt_request_params prm;
-	int *req_cnts = NULL;
+	int *cnts_ids;
 	long ret = 0;
 
 	if (copy_from_user(&prm, args, sizeof(prm))) {
@@ -640,39 +725,41 @@ static int do_ioctl_stats_get_counters(void *args)
 	}
 
 	/* Allocate kernel-space memory area to copy the counters ids */
-	req_cnts = kzalloc(prm.req_params.cnts_ids_len *
+	cnts_ids = kzalloc(prm.req_params.cnts_ids_len *
 			sizeof(int), GFP_KERNEL);
-	if (!req_cnts) {
-		pr_err("Could not allocate requested counters array");
+	if (!cnts_ids) {
+		pr_err("Could not allocate requested counters array\n");
 		return -ENOMEM;
 	}
 
 	/* Copy the user provided counter ids */
-	if (copy_from_user(req_cnts,
+	if (copy_from_user(cnts_ids,
 			prm.req_params.cnts_ids,
 			(prm.req_params.cnts_ids_len * sizeof(int)))) {
-		pr_err("Could not copy requested counters ids");
-		kfree(req_cnts);
+		pr_err("Could not copy requested counters ids\n");
+		kfree(prm.req_params.cnts_ids);
 		return -EINVAL;
 	}
 
-	prm.req_params.cnts_ids = req_cnts;
+	prm.req_params.cnts_ids = cnts_ids;
 
-	/* If counters request is asynchronous, replace the application
-	 * callback with wrapper function */
-	if (prm.request_done)
-		prm.request_done = do_ioctl_req_done_cb;
+	/* If counters request is asynchronous */
+	if (prm.request_done) {
+		ret = store_get_cnts_async_params(&prm);
+		if (ret < 0)
+			return ret;
+	}
 
 	ret = dpa_stats_get_counters(prm.req_params,
 			&prm.cnts_len,
 			prm.request_done);
 	if (ret < 0) {
-		kfree(req_cnts);
-		return -EINVAL;
+		kfree(prm.req_params.cnts_ids);
+		return ret;
 	}
 
 	/* Request was sent, release the array of counter ids */
-	kfree(req_cnts);
+	kfree(prm.req_params.cnts_ids);
 
 	/* If request is synchronous copy counters values to user space */
 	if (!prm.request_done) {
@@ -684,6 +771,11 @@ static int do_ioctl_stats_get_counters(void *args)
 			pr_err("Couldn't copy counters values to storage area\n");
 			return -EINVAL;
 		}
+
+		if (copy_to_user((void *)args, &prm, sizeof(prm))) {
+			pr_err("Could not copy to user the Counter ID");
+			ret = -EINVAL;
+		}
 	}
 
 	return ret;
@@ -696,27 +788,31 @@ long wrp_dpa_stats_do_ioctl(struct file *filp, unsigned int cmd,
 
 	switch (cmd) {
 	case DPA_STATS_IOC_INIT:
-		/* Call function */
-		ret = do_ioctl_stats_init((void *)args);
-		if (ret < 0)
-			return ret;
-		break;
-	case DPA_STATS_IOC_FREE: {
-		int dpa_stats_id;
-		if (copy_from_user(&dpa_stats_id, (int *)args,
-				    sizeof(int))) {
-			pr_err("Could not copy parameters");
-			return -EINVAL;
-		}
+	{
+		struct ioc_dpa_stats_params kparam;
 
-		/* Release kernel allocated memory */
-		kfree(wrp_dpa_stats.k_mem);
+		/* Copy parameters from user-space */
+		if (copy_from_user(&kparam, (void *) args, sizeof(kparam))) {
+			pr_err("Could not read dpa_stats_init user space args");
+			return -EBUSY;
+		}
 
-		ret = dpa_stats_free(dpa_stats_id);
+		ret = do_ioctl_stats_init(&kparam);
 		if (ret < 0)
 			return ret;
+
+		/* Copy paramters to user-space */
+		if (copy_to_user((void *) args, &kparam, sizeof(kparam))) {
+			pr_err("Could not write dpa_stats_init result");
+			return -EBUSY;
+		}
 		break;
 	}
+	case DPA_STATS_IOC_FREE:
+		ret = do_ioctl_stats_free((void *)args);
+		if (ret < 0)
+			return ret;
+		break;
 	case DPA_STATS_IOC_CREATE_COUNTER:
 		ret = do_ioctl_stats_create_counter((void *)args);
 		if (ret < 0)
@@ -759,6 +855,44 @@ long wrp_dpa_stats_do_ioctl(struct file *filp, unsigned int cmd,
 	return ret;
 }
 
+static long store_get_cnts_async_params(
+		struct ioc_dpa_stats_cnt_request_params *kprm)
+{
+	struct dpa_stats_async_req_ev *async_req_ev;
+	struct list_head *async_req_grp;
+	uint8_t grp_idx = 0;
+
+	mutex_lock(&wrp_dpa_stats.async_req_lock);
+	if (list_empty(&wrp_dpa_stats.async_req_pool)) {
+		pr_err("Reached maximum supported number "
+			"of simultaneous asynchronous requests\n");
+		kfree(kprm->req_params.cnts_ids);
+		mutex_unlock(&wrp_dpa_stats.async_req_lock);
+		return -EDOM;
+	}
+	mutex_unlock(&wrp_dpa_stats.async_req_lock);
+
+	/* Add in the associated group the request event */
+	grp_idx = crc8((uint8_t *)&kprm->req_params.storage_area_offset,
+			sizeof(unsigned int));
+	async_req_grp = &wrp_dpa_stats.async_req_group[grp_idx];
+
+	/* Obtain a free request event and add in the group list */
+	mutex_lock(&wrp_dpa_stats.async_req_lock);
+	async_req_ev = list_entry(wrp_dpa_stats.async_req_pool.next,
+			struct dpa_stats_async_req_ev, node);
+	list_del(&async_req_ev->node);
+	async_req_ev->request_done = kprm->request_done;
+	async_req_ev->storage_area_offset =
+			kprm->req_params.storage_area_offset;
+	list_add_tail(&async_req_ev->node, async_req_grp);
+	mutex_unlock(&wrp_dpa_stats.async_req_lock);
+
+	/* Replace the application callback with wrapper function */
+	kprm->request_done = do_ioctl_req_done_cb;
+	return 0;
+}
+
 static int copy_key_descriptor(struct dpa_offload_lookup_key *src,
 		struct dpa_offload_lookup_key *dst)
 {
diff --git a/drivers/staging/fsl_dpa_offload/wrp_dpa_stats.h b/drivers/staging/fsl_dpa_offload/wrp_dpa_stats.h
index f67d154..453ee71 100644
--- a/drivers/staging/fsl_dpa_offload/wrp_dpa_stats.h
+++ b/drivers/staging/fsl_dpa_offload/wrp_dpa_stats.h
@@ -35,6 +35,7 @@
 
 /* Other includes */
 #include "linux/fs.h"
+#include <linux/fsl_dpa_stats.h>
 
 #define DPA_STATS_CDEV				"dpa_stats"
 
@@ -59,25 +60,38 @@ struct dpa_stats_event_params {
 	unsigned int	storage_area_offset;
 	unsigned int	cnts_written;
 	int		bytes_written;
+	dpa_stats_request_cb request_done;
 };
 
 struct dpa_stats_event_queue {
-	struct list_head    lh;     /**< Double linked list of events */
-	spinlock_t          lock;   /**< Spinlock protecting the queue
+	struct list_head    lh;     /* Double linked list of events */
+	spinlock_t          lock;   /* Spinlock protecting the queue
 					from concurrent access */
-	wait_queue_head_t   wq;     /**< Waitqueue for reader processes */
-	atomic_t            count;  /**< Number of events in the event queue */
+	wait_queue_head_t   wq;     /* Waitqueue for reader processes */
+	atomic_t            count;  /* Number of events in the event queue */
 };
 
 struct dpa_stats_event {
-	struct dpa_stats_event_params  params;     /**< Event data */
-	struct list_head    lh;         /**< Event queue list head */
+	struct dpa_stats_event_params  params;     /* Event data */
+	struct list_head    lh;         /* Event queue list head */
+};
+
+struct dpa_stats_async_req_ev {
+	dpa_stats_request_cb request_done; /* Request done callback */
+	unsigned int storage_area_offset; /* Storage offset for this request */
+	struct list_head node; /* Pointers to other async requests
+					in the current set  */
 };
 
 struct wrp_dpa_stats_cb {
 	void  *us_mem; /* Pointer to user-space storage area memory */
 	void  *k_mem;  /* Pointer to kernel-space storage area memory */
 	struct dpa_stats_event_queue ev_queue; /* Event queue */
+	struct list_head *async_req_group; /* Group of asynchronous
+					requests based on CRC collision */
+	struct list_head async_req_pool; /* List of free async request nodes */
+	struct mutex async_req_lock; /* Mutex for operations on async reqs */
+	struct mutex event_queue_lock; /* Mutex for operations on async reqs */
 };
 
 #endif	/* WRP_DPA_STATS_H_ */
-- 
1.7.5.4

