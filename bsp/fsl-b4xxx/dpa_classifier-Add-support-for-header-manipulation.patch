From 65ee4dde88e74053be79df80c6cf42d48ec83724 Mon Sep 17 00:00:00 2001
From: Marian Chereji <marian.chereji@freescale.com>
Date: Mon, 6 Aug 2012 15:57:42 +0000
Subject: [PATCH 240/518] dpa_classifier: Add support for header manipulation

Add Header Manipulation API to the DPA Classifier. Add HM API
implementation.

Signed-off-by: Marian Chereji <marian.chereji@freescale.com>
[Grabbed from the branch, LINUX_IR5.2.0, of
https://git.freescale.com/git-private/cgit.cgi/ppc/alu-b4860/linux.git.]
Signed-off-by: Tiejun Chen <tiejun.chen@windriver.com>
---
 drivers/staging/fsl_dpa_offload/dpa_classifier.c | 2051 +++++++++++++++++++++-
 drivers/staging/fsl_dpa_offload/dpa_classifier.h |   53 +
 include/linux/fsl_dpa_classifier.h               | 1014 +++++++++++-
 3 files changed, 3108 insertions(+), 10 deletions(-)

diff --git a/drivers/staging/fsl_dpa_offload/dpa_classifier.c b/drivers/staging/fsl_dpa_offload/dpa_classifier.c
index 430d5f7..796c020 100644
--- a/drivers/staging/fsl_dpa_offload/dpa_classifier.c
+++ b/drivers/staging/fsl_dpa_offload/dpa_classifier.c
@@ -48,6 +48,11 @@
 /* Granularity of the descriptor tables */
 #define DPA_CLS_ARRAYSIZEGRANULARITY				10
 
+#define PPP_HEADER_OFFSET					0
+#define PPP_HEADER_SIZE						2 /* bytes */
+#define ETHERTYPE_OFFSET					12
+#define ETHERTYPE_SIZE						2 /* bytes */
+
 
 /* DPA Classifier table descriptor table */
 struct dpa_cls_descriptor_table		table_array = {
@@ -63,6 +68,75 @@ struct dpa_cls_descriptor_table		hm_array = {
 	.object			= NULL
 };
 
+
+static struct dpa_cls_hm_node
+	*find_compatible_hm_node(enum dpa_cls_hm_node_type type,
+							struct list_head *list);
+
+static int import_hm_nodes_to_chain(void * const *node_array,
+	unsigned int num_nodes, struct dpa_cls_hm *hm);
+
+static int init_hm_chain(void *fm_pcd, struct list_head *chain_head,
+						struct list_head *current);
+
+static int remove_hm_chain(struct list_head	*chain_head,
+			struct list_head	*current);
+
+static int import_nat_hm(struct dpa_cls_hm *pnat_hm,
+				const struct dpa_cls_hm_nat_resources *res);
+
+static int init_nat_hm(struct dpa_cls_hm *pnat_hm);
+
+static int fwd_hm_check_params(const struct dpa_cls_hm_fwd_params *fwd_params);
+
+static int import_fwd_hm(struct dpa_cls_hm *pfwd_hm,
+				const struct dpa_cls_hm_fwd_resources *res);
+
+static int init_fwd_hm(struct dpa_cls_hm *pfwd_hm);
+
+static int remove_hm_check_params(const struct dpa_cls_hm_remove_params
+	*remove_params);
+
+static int import_remove_hm(struct dpa_cls_hm *premove_hm,
+				const struct dpa_cls_hm_remove_resources *res);
+
+static int init_remove_hm(struct dpa_cls_hm *premove_hm);
+
+static int create_new_hm_op(int *hmd, int next_hmd);
+
+static int insert_hm_check_params(const struct dpa_cls_hm_insert_params
+	*insert_params);
+
+static int import_insert_hm(struct dpa_cls_hm *pinsert_hm,
+				const struct dpa_cls_hm_insert_resources *res);
+
+static int init_insert_hm(struct dpa_cls_hm *pinsert_hm);
+
+static int update_hm_check_params(const struct dpa_cls_hm_update_params
+	*update_params);
+
+static int import_update_hm(struct dpa_cls_hm *pupdate_hm,
+				const struct dpa_cls_hm_update_resources *res);
+
+static int init_update_hm(struct dpa_cls_hm *pupdate_hm);
+
+static int
+	vlan_hm_check_params(const struct dpa_cls_hm_vlan_params *vlan_params);
+
+static int import_vlan_hm(struct dpa_cls_hm *pvlan_hm,
+				const struct dpa_cls_hm_vlan_resources *res);
+
+static int init_vlan_hm(struct dpa_cls_hm *pvlan_hm);
+
+static int mpls_hm_check_params(const struct dpa_cls_hm_mpls_params
+	*mpls_params);
+
+static int import_mpls_hm(struct dpa_cls_hm *pmpls_hm,
+				const struct dpa_cls_hm_mpls_resources *res);
+
+static int init_mpls_hm(struct dpa_cls_hm *pmpls_hm);
+
+
 int dpa_classif_table_create(const struct dpa_cls_tbl_params	*params,
 				int				*td)
 {
@@ -1868,6 +1942,8 @@ static int action_to_next_engine_params(const struct dpa_cls_tbl_action *action,
 				t_FmPcdCcNextEngineParams *next_engine_params)
 {
 	struct dpa_cls_table *next_table;
+	struct dpa_cls_hm *hm;
+	struct dpa_cls_hm_node *hm_node;
 
 	BUG_ON(!action);
 	BUG_ON(!next_engine_params);
@@ -1897,7 +1973,46 @@ static int action_to_next_engine_params(const struct dpa_cls_tbl_action *action,
 			e_FM_PCD_ENQ_FRAME;
 		next_engine_params->params.enqueueParams.newFqid =
 				action->enq_params.new_fqid;
-		next_engine_params->h_Manip = (t_Handle) action->enq_params.hm;
+
+		if (action->enq_params.hmd != DPA_OFFLD_DESC_NONE) {
+			/* Verify that the header manip op is valid */
+			if ((action->enq_params.hmd < 0) ||
+				(action->enq_params.hmd >=
+				hm_array.num_descriptors) ||
+				(!hm_array.object[action->enq_params.hmd])) {
+				pr_err("ERROR: %s, %s (%d): Invalid HM "
+					"descriptor (hmd=%d).\n", __FILE__,
+					__func__, __LINE__,
+					action->enq_params.hmd);
+				return -EINVAL;
+			}
+
+			hm = (struct dpa_cls_hm *)
+					hm_array.object[action->enq_params.hmd];
+
+			/*
+			 * Verify that the header manipulation op is a chain
+			 * head
+			 */
+			if (!hm->chain_head) {
+				pr_err("ERROR %s, %s (%d): Only a HM chain head "
+					"can be attached to a classifier table "
+					"entry. hmd=%d is not a chain head.\n",
+					__FILE__, __func__, __LINE__,
+					action->enq_params.hmd);
+				return -EINVAL;
+			}
+
+			/*
+			 * Acquire the hm_node structure that is head of the
+			 * header manipulation chain
+			 */
+			hm_node = list_entry(hm->hm_chain,
+					struct dpa_cls_hm_node, list_node);
+			next_engine_params->h_Manip = (t_Handle)hm_node->node;
+		} else
+			next_engine_params->h_Manip = NULL;
+
 		if (action->enq_params.override_fqid)
 			next_engine_params->params.enqueueParams.overrideFqid =
 				TRUE;
@@ -2062,3 +2177,1937 @@ static inline void key_apply_mask(const struct dpa_cls_tbl_key	*key,
 	for (i = 0; i < key_size; i++)
 		new_key[i] = key->byte[i] & key->mask[i];
 }
+
+static int fwd_hm_check_params(const struct dpa_cls_hm_fwd_params *fwd_params)
+{
+	if (fwd_params->out_if_type == DPA_CLS_HM_IF_TYPE_PPPoE) {
+		pr_err("ERROR: %s, %s (%d): Forwarding HM: PPPoE output "
+			"interface not supported yet.\n", __FILE__, __func__,
+			__LINE__);
+		return -ENOSYS;
+	}
+
+	return 0;
+}
+
+static int remove_hm_check_params(const struct dpa_cls_hm_remove_params
+	*remove_params)
+{
+	xx_assert(remove_params);
+
+	switch (remove_params->type) {
+	case DPA_CLS_HM_REMOVE_ETHERNET:
+		pr_err("ERROR: %s, %s (%d): Unsupported HM: remove Ethernet.\n",
+			__FILE__, __func__, __LINE__);
+		return -ENOSYS;
+		break;
+	case DPA_CLS_HM_REMOVE_PPPoE:
+		pr_err("ERROR: %s, %s (%d): Unsupported HM: remove PPPoE.\n",
+			__FILE__, __func__, __LINE__);
+		return -ENOSYS;
+		break;
+	default:
+		break;
+	}
+
+	return 0;
+}
+
+static int insert_hm_check_params(const struct dpa_cls_hm_insert_params
+	*insert_params)
+{
+	xx_assert(insert_params);
+
+	switch (insert_params->type) {
+	case DPA_CLS_HM_INSERT_PPPoE:
+		pr_err("ERROR: %s, %s (%d): Unsupported HM: insert PPPoE.\n",
+			__FILE__, __func__, __LINE__);
+		return -ENOSYS;
+		break;
+	case DPA_CLS_HM_INSERT_ETHERNET:
+		if (insert_params->param.eth.num_tags >
+			DPA_CLS_HM_MAX_VLANs) {
+			pr_err("ERROR: %s, %s (%d): Insert HM: Can only insert "
+				"a maximum of %d VLANs.\n", __FILE__, __func__,
+				__LINE__, DPA_CLS_HM_MAX_VLANs);
+			return -EINVAL;
+		}
+		break;
+	default:
+		break;
+	}
+
+	return 0;
+}
+
+static int update_hm_check_params(const struct dpa_cls_hm_update_params
+	*update_params)
+{
+	const int update_ops_mask =	DPA_CLS_HM_UPDATE_IPv4_UPDATE |
+					DPA_CLS_HM_UPDATE_IPv6_UPDATE |
+					DPA_CLS_HM_UPDATE_UDP_TCP_UPDATE;
+	const int replace_ops_mask =	DPA_CLS_HM_REPLACE_IPv4_BY_IPv6 |
+					DPA_CLS_HM_REPLACE_IPv6_BY_IPv4;
+	int ops;
+
+	if ((update_params->op_flags == DPA_CLS_HM_UPDATE_NONE) &&
+		(update_params->ip_frag_params.mtu == 0)) {
+		pr_err("ERROR: %s, %s (%d): Refusing to create an empty update "
+			"HM.\n", __FILE__, __func__, __LINE__);
+		return -EINVAL;
+	}
+
+	ops = update_params->op_flags & update_ops_mask;
+	if (ops) {
+		while ((ops & 0x1) == 0)
+			ops >>= 1;
+		if (ops > 1) {
+			pr_err("ERROR: %s, %s (%d): Only one UPDATE operation "
+				"is allowed.\n", __FILE__, __func__, __LINE__);
+			return -EINVAL;
+		}
+
+		if (update_params->op_flags & DPA_CLS_HM_UPDATE_IPv4_UPDATE) {
+			if ((update_params->update.l3.field_flags &
+				DPA_CLS_HM_IP_UPDATE_IPSA) &&
+				(update_params->update.l3.ipsa.version != 4)) {
+				pr_err("ERROR: %s, %s (%d): Only IPv4 addresses "
+					"are accepted for IPv4 IPSA update.\n",
+					__FILE__, __func__, __LINE__);
+				return -EINVAL;
+			}
+
+			if ((update_params->update.l3.field_flags &
+				DPA_CLS_HM_IP_UPDATE_IPDA) &&
+				(update_params->update.l3.ipda.version != 4)) {
+				pr_err("ERROR: %s, %s (%d): Only IPv4 addresses "
+					"are accepted for IPv4 IPDA update.\n",
+					__FILE__, __func__, __LINE__);
+				return -EINVAL;
+			}
+		}
+
+		if (update_params->op_flags & DPA_CLS_HM_UPDATE_IPv6_UPDATE) {
+			if ((update_params->update.l3.field_flags &
+				DPA_CLS_HM_IP_UPDATE_IPSA) &&
+				(update_params->update.l3.ipsa.version != 6)) {
+				pr_err("ERROR: %s, %s (%d): Only IPv6 addresses "
+					"are accepted for IPv6 IPSA update.\n",
+					__FILE__, __func__, __LINE__);
+				return -EINVAL;
+			}
+
+			if ((update_params->update.l3.field_flags &
+				DPA_CLS_HM_IP_UPDATE_IPDA) &&
+				(update_params->update.l3.ipda.version != 6)) {
+				pr_err("ERROR: %s, %s (%d): Only IPv6 addresses "
+					"are accepted for IPv6 IPDA update.\n",
+					__FILE__, __func__, __LINE__);
+				return -EINVAL;
+			}
+		}
+	}
+
+	ops = update_params->op_flags & replace_ops_mask;
+	if (ops) {
+		while ((ops & 0x1) == 0)
+			ops >>= 1;
+		if (ops > 1) {
+			pr_err("ERROR: %s, %s (%d): Only one REPLACE operation "
+				"is allowed.\n", __FILE__, __func__, __LINE__);
+			return -EINVAL;
+		}
+	}
+
+	return 0;
+}
+
+static int
+	vlan_hm_check_params(const struct dpa_cls_hm_vlan_params *vlan_params)
+{
+	xx_assert(vlan_params);
+
+	if ((vlan_params->type != DPA_CLS_HM_VLAN_EGRESS) &&
+		(vlan_params->params.egress.update_op !=
+		DPA_CLS_HM_VLAN_UPDATE_VPri)) {
+		pr_err("ERROR: %s, %s (%d): Unsupported VLAN specific header "
+			"manipulation.\n", __FILE__, __func__, __LINE__);
+		return -ENOSYS;
+	}
+
+	switch (vlan_params->type) {
+	case DPA_CLS_HM_VLAN_INGRESS:
+		if (vlan_params->params.ingress.num_tags !=
+			DPA_CLS_HM_VLAN_CNT_ALL_QTAGS) {
+			pr_err("ERROR: %s, %s (%d): Ingress VLAN QTags remove "
+				"HM: Only \"remove all QTags\" is currenly "
+				"supported.\n", __FILE__, __func__, __LINE__);
+			return -EINVAL;
+		}
+		break;
+	case DPA_CLS_HM_VLAN_EGRESS:
+		if (vlan_params->params.egress.num_tags >
+						DPA_CLS_HM_MAX_VLANs) {
+			pr_err("ERROR: %s, %s (%d): Egress VLAN HM: Can only "
+				"insert a maximum of %d VLANs.\n", __FILE__,
+				__func__, __LINE__, DPA_CLS_HM_MAX_VLANs);
+			return -EINVAL;
+		}
+		break;
+	default:
+		pr_err("ERROR: %s, %s (%d): Invalid VLAN specific HM type.\n",
+			__FILE__, __func__, __LINE__);
+		return -EINVAL;
+		break;
+	}
+
+	return 0;
+}
+
+static int
+	mpls_hm_check_params(const struct dpa_cls_hm_mpls_params *mpls_params)
+{
+	if ((mpls_params->type == DPA_CLS_HM_MPLS_INSERT_LABELS) &&
+		(mpls_params->num_labels > DPA_CLS_HM_MAX_MPLS_LABELS)) {
+		pr_err("ERROR: %s, %s (%d): MPLS HM: Can only insert a maximum "
+			"of %d MPLS labels.\n", __FILE__, __func__, __LINE__,
+			DPA_CLS_HM_MAX_MPLS_LABELS);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int import_hm_nodes_to_chain(void * const *node_array,
+	unsigned int num_nodes, struct dpa_cls_hm *hm)
+{
+	struct dpa_cls_hm *pnext_hm = NULL;
+	struct list_head *chain = NULL;
+	struct dpa_cls_hm_node *hm_node;
+	int i, j;
+
+	if (!list_empty(&hm->list_node)) {
+		pnext_hm = list_entry(hm->list_node.next,
+				struct dpa_cls_hm,
+				list_node);
+
+		chain = pnext_hm->hm_chain;
+	}
+
+	if (pnext_hm) {
+		bool found;
+		struct dpa_cls_hm *pcurrent;
+
+		/* This HM operation is linked to another HM op */
+		j = 0;
+		for (i = 0; i < num_nodes; i++) {
+			/* If the node is empty, skip to the next */
+			if (!node_array[i])
+				continue;
+
+			found = false;
+			list_for_each_entry(hm_node, pnext_hm->hm_chain,
+				list_node) {
+				if ((unsigned long)hm_node->node ==
+					(unsigned long)node_array[i]) {
+					/*
+					 * This node already exists in the
+					 * chain
+					 */
+					found = true;
+					break;
+				}
+			}
+
+			if (found)
+				hm->hm_node[j++] = hm_node;
+			else {
+				hm->hm_node[j] = (struct dpa_cls_hm_node *)
+					kzalloc(sizeof(struct dpa_cls_hm_node),
+						GFP_KERNEL);
+				if (!hm->hm_node[j]) {
+					pr_err("ERROR: %s, %s (%d): Not enough "
+						"memory for HM node "
+						"management.\n", __FILE__,
+						__func__, __LINE__);
+					return -ENOMEM;
+				}
+
+				/* Fill in the node */
+				hm->hm_node[j]->node = node_array[i];
+				INIT_LIST_HEAD(&hm->hm_node[j]->list_node);
+
+				/* Add this new node to the HM chain: */
+				list_add_tail(&hm->hm_node[j]->list_node,
+						chain);
+				chain = &hm->hm_node[j]->list_node;
+
+				j++;
+			}
+		}
+
+		/*
+		 * Update the low level ops chain head on all the other
+		 * high level ops in the high level ops chain:
+		 */
+		if (pnext_hm->hm_chain != chain) {
+			list_for_each_entry(pcurrent,
+				&hm->list_node, list_node) {
+				pcurrent->hm_chain = chain;
+			}
+		}
+	} else {
+		j = 0;
+		/* This is an isolated HM operation */
+		for (i = 0; i < num_nodes; i++) {
+			/* If the node is empty, skip to the next */
+			if (!node_array[i])
+				continue;
+
+			hm->hm_node[j] = (struct dpa_cls_hm_node *)
+				kzalloc(sizeof(struct dpa_cls_hm_node),
+					GFP_KERNEL);
+			if (!hm->hm_node[j]) {
+				pr_err("ERROR: %s, %s (%d): Not enough memory "
+					"for HM node management.\n", __FILE__,
+					__func__, __LINE__);
+				return -ENOMEM;
+			}
+
+			/* Fill in the node */
+			hm->hm_node[j]->node = node_array[i];
+			INIT_LIST_HEAD(&hm->hm_node[j]->list_node);
+
+			if (j <= 0)
+				chain = &hm->hm_node[j]->list_node;
+			else
+				list_add_tail(&hm->hm_node[j]->list_node,
+						chain);
+			j++;
+		}
+	}
+
+	hm->hm_chain = chain;
+
+	return 0;
+}
+
+static int add_local_hm_nodes_to_chain(struct dpa_cls_hm *phm)
+{
+	struct dpa_cls_hm *pnext_hm = NULL;
+	struct dpa_cls_hm *pcurrent;
+	int i;
+
+	if (!list_empty(&phm->list_node)) {
+		pnext_hm = list_entry(phm->list_node.next,
+				struct dpa_cls_hm,
+				list_node);
+		phm->hm_chain = pnext_hm->hm_chain;
+	} else
+		phm->hm_chain = NULL;
+
+	for (i = 0; (i < phm->num_nodes); i++)
+		/*
+		 * If the node exists and it is not already integrated in a HM
+		 * chain...
+		 */
+		if (phm->hm_node[i]) {
+			if (list_empty(&phm->hm_node[i]->list_node)) {
+				if (phm->hm_chain)
+					list_add_tail(&phm->hm_node[i]->
+						list_node, phm->hm_chain);
+			}
+
+			phm->hm_chain = &phm->hm_node[i]->list_node;
+		}
+
+	if (pnext_hm)
+		/*
+		 * Update the low level ops chain head for all the other
+		 * high level ops in the high level ops chain:
+		 */
+		list_for_each_entry(pcurrent, &phm->list_node, list_node) {
+			pcurrent->hm_chain = phm->hm_chain;
+		}
+
+	return 0;
+}
+
+static int init_hm_chain(void *fm_pcd, struct list_head *chain_head,
+						struct list_head *current)
+{
+	int err = 0;
+	struct dpa_cls_hm_node *pcurrent, *pnext;
+
+	if (current->next != chain_head) {
+		/* Initialize the rest of the HM chain */
+		err = init_hm_chain(fm_pcd, chain_head, current->next);
+		if (err)
+			return err;
+	}
+
+	/* Initialize the current node: */
+	pcurrent = list_entry(current, struct dpa_cls_hm_node, list_node);
+	pnext = list_entry(current->next, struct dpa_cls_hm_node, list_node);
+
+	pcurrent->params.h_NextManip = (t_Handle)pnext->node;
+	pcurrent->node = (void *) FM_PCD_ManipNodeSet((t_Handle) fm_pcd,
+							&pcurrent->params);
+	if (!pcurrent->node)
+		err = -EINVAL;
+
+	return err;
+}
+
+static int remove_hm_chain(struct list_head	*chain_head,
+			struct list_head	*current)
+{
+	int err = 0;
+	struct dpa_cls_hm_node *pcurrent;
+
+	if (current->next != chain_head) {
+		/* Remove the rest of the HM chain */
+		err = remove_hm_chain(chain_head, current->next);
+		if (err)
+			return err;
+	}
+
+	/* Remove the current node: */
+	pcurrent = list_entry(current, struct dpa_cls_hm_node, list_node);
+
+	if (FM_PCD_ManipNodeDelete((t_Handle) pcurrent->node) != E_OK)
+		return -EBUSY;
+
+	list_del(current);
+
+	return err;
+}
+
+static struct dpa_cls_hm_node
+	*find_compatible_hm_node(enum dpa_cls_hm_node_type	type,
+				struct list_head		*list)
+{
+	struct dpa_cls_hm_node *phm_node;
+	e_FmPcdManipHdrFieldUpdateType val;
+
+	switch (type) {
+	case DPA_CLS_HM_NODE_IPv4_HDR_UPDATE:
+		val = e_FM_PCD_MANIP_HDR_FIELD_UPDATE_IPV4;
+		break;
+	case DPA_CLS_HM_NODE_IPv6_HDR_UPDATE:
+		val = e_FM_PCD_MANIP_HDR_FIELD_UPDATE_IPV6;
+		break;
+	case DPA_CLS_HM_NODE_TCPUDP_HDR_UPDATE:
+		val = e_FM_PCD_MANIP_HDR_FIELD_UPDATE_TCP_UDP;
+		break;
+	default:
+		pr_err("ERROR: %s, %s (%d): Don't know how to search for nodes"
+			"compatible with type=%d.\n", __FILE__, __func__,
+			__LINE__, type);
+		return NULL;
+	}
+
+	list_for_each_entry(phm_node, list, list_node) {
+		if ((phm_node->params.type == e_FM_PCD_MANIP_HDR) &&
+			(phm_node->params.u.hdr.fieldUpdate) &&
+			(phm_node->params.u.hdr.fieldUpdateParams.type == val))
+				return phm_node;
+	}
+
+	return NULL;
+}
+
+static int create_new_hm_op(int *hmd, int next_hmd)
+{
+	int err;
+	struct dpa_cls_hm *hm;
+	struct dpa_cls_hm *next_hm;
+
+	*hmd = DPA_OFFLD_DESC_NONE;
+
+	if (next_hmd != DPA_OFFLD_DESC_NONE)
+		/* Check whether [next_hmd] is a valid descriptor */
+		xx_sanity_check_return_value(((next_hmd >= 0) &&
+			(next_hmd < hm_array.num_descriptors)), "next_hmd",
+			-EINVAL);
+
+	err = acquire_new_descriptor(&hm_array, hmd);
+	if (err < 0)
+		return err;
+
+	/* Allocate header manipulation object */
+	hm_array.object[*hmd] = kzalloc(sizeof(struct dpa_cls_hm),
+		GFP_KERNEL);
+	if (!hm_array.object[*hmd]) {
+		pr_err("ERROR: %s, %s (%d): No more memory for header manip "
+			"ops.\n", __FILE__, __func__, __LINE__);
+		return -ENOMEM;
+	}
+
+	hm = (struct dpa_cls_hm *) hm_array.object[*hmd];
+
+	INIT_LIST_HEAD(&hm->list_node);
+
+	if (next_hmd != DPA_OFFLD_DESC_NONE) {
+		/*
+		 * In case this high level op is chained with another high
+		 * level op, add it to the list.
+		 */
+		next_hm = (struct dpa_cls_hm *)hm_array.object[next_hmd];
+
+		if (!next_hm) {
+			pr_err("ERROR: %s, %s (%d): Link to an invalid HM "
+				"(hmd=%d).\n", __FILE__, __func__, __LINE__,
+				next_hmd);
+			dpa_classif_free_hm(*hmd);
+			*hmd = DPA_OFFLD_DESC_NONE;
+			return -EINVAL;
+		}
+
+		list_add_tail(&hm->list_node, &next_hm->list_node);
+	}
+
+	return err;
+}
+
+int dpa_classif_set_nat_hm(const struct dpa_cls_hm_nat_params	*nat_params,
+			int					next_hmd,
+			int					*hmd,
+			bool					chain_head,
+			const struct dpa_cls_hm_nat_resources	*res)
+{
+	int err;
+	struct dpa_cls_hm *pnat_hm;
+
+	xx_sanity_check_return_value(nat_params, "nat_params", -EINVAL);
+	xx_sanity_check_return_value(hmd, "hmd", -EINVAL);
+
+	*hmd = DPA_OFFLD_DESC_NONE;
+
+	pr_err("ERROR: %s, %s (%d): NAT header manipulation is not supported "
+		"yet.\n", __FILE__, __func__, __LINE__);
+	return -ENOSYS;
+
+	err = create_new_hm_op(hmd, next_hmd);
+	if (err < 0)
+		return err;
+
+	pnat_hm = (struct dpa_cls_hm *) hm_array.object[*hmd];
+
+	pnat_hm->type		= DPA_CLS_HM_TYPE_NAT;
+	pnat_hm->chain_head	= chain_head;
+
+	/* Copy the NAT parameters locally */
+	memcpy(&pnat_hm->params.nat, nat_params, sizeof(*nat_params));
+
+	if (res)
+		/* Need to import */
+		err = import_nat_hm(pnat_hm, res);
+	else {
+		/* Need to create */
+		err = init_nat_hm(pnat_hm);
+		if (err) {
+			dpa_classif_free_hm(*hmd);
+			*hmd = DPA_OFFLD_DESC_NONE;
+			return err;
+		}
+
+		if (chain_head)
+			/* Initialize low level HM ops chain */
+			err = init_hm_chain(pnat_hm->params.nat.fm_pcd,
+				pnat_hm->hm_chain, pnat_hm->hm_chain);
+	}
+
+	if (err) {
+		dpa_classif_free_hm(*hmd);
+		*hmd = DPA_OFFLD_DESC_NONE;
+	}
+
+	return err;
+}
+
+static int import_nat_hm(struct dpa_cls_hm *pnat_hm,
+				const struct dpa_cls_hm_nat_resources *res)
+{
+	void * const *phm_nodes;
+	int err;
+
+	xx_assert(pnat_hm);
+	xx_assert(res);
+
+	phm_nodes = &res->l3_update_node;
+
+	err = import_hm_nodes_to_chain(phm_nodes, 2, pnat_hm);
+
+	/* Update here the HM nodes parameters */
+
+	return err;
+}
+
+static int init_nat_hm(struct dpa_cls_hm *pnat_hm)
+{
+	struct dpa_cls_hm_node *hm_node = NULL;
+	struct dpa_cls_hm *pnext_hm = NULL;
+	struct dpa_cls_hm *pcurrent;
+
+	if (!list_empty(&pnat_hm->list_node))
+		pnext_hm = list_entry(pnat_hm->list_node.next,
+					struct dpa_cls_hm,
+					list_node);
+
+	if (pnext_hm)
+		/*
+		 * Check if there already is another TCP/UDP update in the
+		 * existing HM chain
+		 */
+		hm_node = find_compatible_hm_node(
+			DPA_CLS_HM_NODE_TCPUDP_HDR_UPDATE,
+			pnext_hm->hm_chain);
+
+	if (!hm_node) {
+		/* Create a TCP/UDP update node: */
+		hm_node = kzalloc(sizeof(*hm_node), GFP_KERNEL);
+		if (!hm_node) {
+			pr_err("ERROR: %s, %s (%d): No more memory for header "
+				"manip nodes.\n", __FILE__, __func__, __LINE__);
+			return -ENOMEM;
+		}
+
+		hm_node->params.type = e_FM_PCD_MANIP_HDR;
+		hm_node->params.u.hdr.fieldUpdate = TRUE;
+		hm_node->params.u.hdr.fieldUpdateParams.type =
+				DPA_CLS_HM_NODE_TCPUDP_HDR_UPDATE;
+
+		if (pnext_hm)
+			list_add_tail(&hm_node->list_node,
+					pnext_hm->hm_chain);
+		else
+			INIT_LIST_HEAD(&hm_node->list_node);
+
+		pnat_hm->hm_chain = &hm_node->list_node;
+	} else
+		pnat_hm->hm_chain = pnext_hm->hm_chain;
+
+	/* PARAMETERS FOR LOW LEVEL HM NODE NOT INITIALIZED */
+
+	pnat_hm->hm_node[0] = hm_node;
+
+	hm_node = NULL;
+
+	if (pnext_hm)
+		/*
+		 * Check if there already is another IPv4 update in the
+		 * existing HM chain
+		 */
+		hm_node = find_compatible_hm_node(
+			DPA_CLS_HM_NODE_IPv4_HDR_UPDATE,
+			pnext_hm->hm_chain);
+
+	if (!hm_node) {
+		/* Create an IPv4 update node: */
+		hm_node = kzalloc(sizeof(*hm_node), GFP_KERNEL);
+		if (!hm_node) {
+			pr_err("ERROR: %s, %s (%d): No more memory for header "
+				"manip nodes.\n", __FILE__, __func__, __LINE__);
+			return -ENOMEM;
+		}
+
+		hm_node->params.type = e_FM_PCD_MANIP_HDR;
+		hm_node->params.u.hdr.fieldUpdate = TRUE;
+		hm_node->params.u.hdr.fieldUpdateParams.type =
+				DPA_CLS_HM_NODE_IPv4_HDR_UPDATE;
+
+		if (pnext_hm)
+			list_add(&hm_node->list_node,
+					pnext_hm->hm_chain);
+		else
+			INIT_LIST_HEAD(&hm_node->list_node);
+
+		pnat_hm->hm_chain = &hm_node->list_node;
+	}
+
+	/* PARAMETERS FOR LOW LEVEL HM NODE NOT INITIALIZED */
+
+	pnat_hm->hm_node[1] = hm_node;
+	pnat_hm->num_nodes = 2;
+
+	/*
+	 * Update the low level ops chain head for all the other
+	 * high level ops in the high level ops chain:
+	 */
+	if (pnat_hm->hm_chain != pnext_hm->hm_chain) {
+		list_for_each_entry(pcurrent,
+			&pnat_hm->list_node, list_node) {
+			pcurrent->hm_chain = pnat_hm->hm_chain;
+		}
+	}
+
+	return 0;
+}
+
+int dpa_classif_modify_nat_hm(int hmd,
+	const struct dpa_cls_hm_nat_params *new_nat_params, int modify_flags)
+{
+	pr_err("ERROR: %s, %s (%d): NAT HM runtime modification not supported "
+		"yet.\n", __FILE__, __func__, __LINE__);
+	return -ENOSYS;
+}
+
+int dpa_classif_set_fwd_hm(const struct dpa_cls_hm_fwd_params	*fwd_params,
+			int					next_hmd,
+			int					*hmd,
+			bool					chain_head,
+			const struct dpa_cls_hm_fwd_resources	*res)
+{
+	int err;
+	struct dpa_cls_hm *pfwd_hm;
+
+	xx_sanity_check_return_value(fwd_params, "fwd_params", -EINVAL);
+	xx_sanity_check_return_value(hmd, "hmd", -EINVAL);
+
+	*hmd = DPA_OFFLD_DESC_NONE;
+
+	pr_err("ERROR: %s, %s (%d): Forwarding header manipulation is not "
+		"supported yet.\n", __FILE__, __func__, __LINE__);
+	return -ENOSYS;
+
+	err = fwd_hm_check_params(fwd_params);
+	if (err < 0)
+		return err;
+
+	err = create_new_hm_op(hmd, next_hmd);
+	if (err < 0)
+		return err;
+
+	pfwd_hm = (struct dpa_cls_hm *) hm_array.object[*hmd];
+
+	pfwd_hm->type		= DPA_CLS_HM_TYPE_FORWARDING;
+	pfwd_hm->chain_head	= chain_head;
+
+	/* Copy the NAT parameters locally */
+	memcpy(&pfwd_hm->params.fwd, fwd_params, sizeof(*fwd_params));
+
+	if (res)
+		/* Need to import */
+		err = import_fwd_hm(pfwd_hm, res);
+	else {
+		/* Need to create */
+		err = init_fwd_hm(pfwd_hm);
+		if (err) {
+			dpa_classif_free_hm(*hmd);
+			*hmd = DPA_OFFLD_DESC_NONE;
+			return err;
+		}
+
+		if (chain_head)
+			/* Initialize low level HM ops chain */
+			err = init_hm_chain(pfwd_hm->params.fwd.fm_pcd,
+				pfwd_hm->hm_chain, pfwd_hm->hm_chain);
+	}
+
+	if (err) {
+		dpa_classif_free_hm(*hmd);
+		*hmd = DPA_OFFLD_DESC_NONE;
+	}
+
+	return err;
+}
+
+static int import_fwd_hm(struct dpa_cls_hm *pfwd_hm,
+				const struct dpa_cls_hm_fwd_resources *res)
+{
+	void * const *phm_nodes;
+	int err;
+
+	xx_assert(pfwd_hm);
+	xx_assert(res);
+
+	phm_nodes = &res->fwd_node;
+
+	err = import_hm_nodes_to_chain(phm_nodes, 3, pfwd_hm);
+
+	/* Update here the hm nodes parameters */
+
+	return err;
+}
+
+static int init_fwd_hm(struct dpa_cls_hm *pfwd_hm)
+{
+	struct dpa_cls_hm_node *hm_node = NULL;
+	uint8_t size;
+	uint8_t *pdata;
+
+	/* Create a forwarding node: */
+	hm_node = kzalloc(sizeof(*hm_node), GFP_KERNEL);
+	if (!hm_node) {
+		pr_err("ERROR: %s, %s (%d): No more memory for header manip "
+			"nodes.\n", __FILE__, __func__, __LINE__);
+		return -ENOMEM;
+	}
+
+	xx_assert(pfwd_hm->params.fwd.out_if_type != DPA_CLS_HM_IF_TYPE_PPPoE);
+
+	switch (pfwd_hm->params.fwd.out_if_type) {
+	case DPA_CLS_HM_IF_TYPE_ETHERNET:
+		/* Update Ethernet MACS */
+		hm_node->params.type			= e_FM_PCD_MANIP_HDR;
+		hm_node->params.u.hdr.insrt		= TRUE;
+		hm_node->params.u.hdr.insrtParams.type	=
+						e_FM_PCD_MANIP_INSRT_GENERIC;
+		hm_node->params.u.hdr.insrtParams.u.generic.replace = TRUE;
+
+		size = sizeof(struct ethhdr) - ETHERTYPE_SIZE;
+		pdata = kzalloc(size, GFP_KERNEL);
+		if (!pdata) {
+			pr_err("ERROR: %s, %s (%d): Not enough RAM for "
+				"forwarding HM.\n", __FILE__, __func__,
+				__LINE__);
+			kfree(hm_node);
+			return -ENOMEM;
+		}
+
+		memcpy(pdata, pfwd_hm->params.fwd.param.eth.macda, ETH_ALEN);
+		memcpy(&pdata[ETH_ALEN], pfwd_hm->params.fwd.param.eth.macsa,
+			ETH_ALEN);
+
+		hm_node->params.u.hdr.insrtParams.u.generic.offset = 0;
+		hm_node->params.u.hdr.insrtParams.u.generic.size = size;
+		hm_node->params.u.hdr.insrtParams.u.generic.p_Data = pdata;
+	case DPA_CLS_HM_IF_TYPE_PPPoE:
+		/* Update Ethernet MACS; insert PPPoE */
+		/* Insert PPPoE is not supported yet */
+		break;
+	case DPA_CLS_HM_IF_TYPE_PPP:
+		/* Remove Ethernet and VLANs; insert PPP */
+		hm_node->params.type			= e_FM_PCD_MANIP_HDR;
+		hm_node->params.u.hdr.rmv		= TRUE;
+		hm_node->params.u.hdr.rmvParams.type	=
+					e_FM_PCD_MANIP_RMV_BY_HDR;
+		hm_node->params.u.hdr.rmvParams.u.byHdr.type =
+					e_FM_PCD_MANIP_RMV_BY_HDR_SPECIFIC_L2;
+		hm_node->params.u.hdr.rmvParams.u.byHdr.u.specificL2 =
+					e_FM_PCD_MANIP_HDR_RMV_ETHERNET;
+
+		hm_node->params.u.hdr.insrt		= TRUE;
+		hm_node->params.u.hdr.insrtParams.type	=
+					e_FM_PCD_MANIP_INSRT_GENERIC;
+
+		size	= PPP_HEADER_SIZE;
+		pdata	= kzalloc(size, GFP_KERNEL);
+		if (!pdata) {
+			pr_err("ERROR: %s, %s (%d): Not enough memory for "
+				"forwarding HM.\n", __FILE__, __func__,
+				__LINE__);
+			kfree(hm_node);
+			return -ENOMEM;
+		}
+
+		memcpy(pdata, &pfwd_hm->params.fwd.param.ppp.ppp_pid,
+			PPP_HEADER_SIZE);
+
+		hm_node->params.u.hdr.insrtParams.u.generic.offset = 0;
+		hm_node->params.u.hdr.insrtParams.u.generic.size = size;
+		hm_node->params.u.hdr.insrtParams.u.generic.p_Data = pdata;
+		break;
+	default:
+		pr_err("ERROR: %s, %s (%d): Forwarding HM: Unknown output port "
+			"type (%d).\n", __FILE__, __func__, __LINE__,
+			pfwd_hm->params.fwd.out_if_type);
+		kfree(hm_node);
+		return -EINVAL;
+	}
+
+	INIT_LIST_HEAD(&hm_node->list_node);
+	pfwd_hm->hm_node[0]	= hm_node;
+	pfwd_hm->num_nodes	= 1;
+
+	add_local_hm_nodes_to_chain(pfwd_hm);
+
+	return 0;
+}
+
+int dpa_classif_modify_fwd_hm(int hmd,
+	const struct dpa_cls_hm_fwd_params *new_fwd_params, int modify_flags)
+{
+	pr_err("ERROR: %s, %s (%d): Forwarding HM runtime modification not "
+		"supported yet.\n", __FILE__, __func__, __LINE__);
+	return -ENOSYS;
+}
+
+int dpa_classif_set_remove_hm(const struct dpa_cls_hm_remove_params
+	*remove_params, int next_hmd, int *hmd, bool chain_head,
+	const struct dpa_cls_hm_remove_resources *res)
+{
+	int err;
+	struct dpa_cls_hm *premove_hm;
+
+	xx_sanity_check_return_value(remove_params, "remove_params", -EINVAL);
+	xx_sanity_check_return_value(hmd, "hmd", -EINVAL);
+
+	*hmd = DPA_OFFLD_DESC_NONE;
+
+	if (res) {
+		pr_err("ERROR: %s, %s (%d): Header manipulation import is not "
+			"yet supported.\n", __FILE__, __func__, __LINE__);
+		return -ENOSYS;
+	}
+
+	err = remove_hm_check_params(remove_params);
+	if (err < 0)
+		return err;
+
+	err = create_new_hm_op(hmd, next_hmd);
+	if (err < 0)
+		return err;
+
+	premove_hm = (struct dpa_cls_hm *) hm_array.object[*hmd];
+
+	premove_hm->type	= DPA_CLS_HM_TYPE_REMOVE;
+	premove_hm->chain_head	= chain_head;
+
+	/* Copy the remove HM parameters locally */
+	memcpy(&premove_hm->params.remove, remove_params,
+						sizeof(*remove_params));
+
+	if (res)
+		err = import_remove_hm(premove_hm, res);
+	else {
+		err = init_remove_hm(premove_hm);
+		if (err) {
+			dpa_classif_free_hm(*hmd);
+			*hmd = DPA_OFFLD_DESC_NONE;
+			return err;
+		}
+
+		if (chain_head)
+			/* Initialize low level HM ops chain */
+			err = init_hm_chain(premove_hm->params.remove.fm_pcd,
+				premove_hm->hm_chain, premove_hm->hm_chain);
+	}
+
+	if (err) {
+		dpa_classif_free_hm(*hmd);
+		*hmd = DPA_OFFLD_DESC_NONE;
+	}
+
+	return err;
+}
+
+static int import_remove_hm(struct dpa_cls_hm *premove_hm,
+				const struct dpa_cls_hm_remove_resources *res)
+{
+	void * const *phm_nodes;
+	int err;
+
+	xx_assert(premove_hm);
+	xx_assert(res);
+
+	phm_nodes = &res->remove_node;
+
+	err = import_hm_nodes_to_chain(phm_nodes, 1, premove_hm);
+
+	/* Update here the hm nodes parameters */
+
+	return err;
+}
+
+static int init_remove_hm(struct dpa_cls_hm *premove_hm)
+{
+	struct dpa_cls_hm_node *hm_node = NULL;
+
+	/* Create a header remove node: */
+	hm_node = kzalloc(sizeof(*hm_node), GFP_KERNEL);
+	if (!hm_node) {
+		pr_err("ERROR: %s, %s (%d): No more memory for header manip "
+			"nodes.\n", __FILE__, __func__, __LINE__);
+		return -ENOMEM;
+	}
+
+	hm_node->params.type			= e_FM_PCD_MANIP_HDR;
+	hm_node->params.u.hdr.rmv		= TRUE;
+	hm_node->params.u.hdr.rmvParams.type	= e_FM_PCD_MANIP_RMV_GENERIC;
+
+	switch (premove_hm->params.remove.type) {
+	case DPA_CLS_HM_REMOVE_PPP:
+		hm_node->params.u.hdr.rmvParams.u.generic.offset =
+							PPP_HEADER_OFFSET;
+		hm_node->params.u.hdr.rmvParams.u.generic.size =
+							PPP_HEADER_SIZE;
+		break;
+	case DPA_CLS_HM_REMOVE_CUSTOM:
+		hm_node->params.u.hdr.rmvParams.u.generic.offset =
+					premove_hm->params.remove.custom.offset;
+		hm_node->params.u.hdr.rmvParams.u.generic.size =
+					premove_hm->params.remove.custom.size;
+		break;
+	default:
+		break;
+	}
+
+	INIT_LIST_HEAD(&hm_node->list_node);
+	premove_hm->hm_node[0]	= hm_node;
+	premove_hm->num_nodes	= 1;
+
+	add_local_hm_nodes_to_chain(premove_hm);
+
+	return 0;
+}
+
+int dpa_classif_modify_remove_hm(int hmd,
+	const struct dpa_cls_hm_remove_params *new_remove_params,
+	int modify_flags)
+{
+	pr_err("ERROR: %s, %s (%d): Remove HM runtime modification not "
+		"supported yet.\n", __FILE__, __func__, __LINE__);
+	return -ENOSYS;
+}
+
+int dpa_classif_set_insert_hm(const struct dpa_cls_hm_insert_params
+	*insert_params, int next_hmd, int *hmd, bool chain_head,
+	const struct dpa_cls_hm_insert_resources *res)
+{
+	int err;
+	struct dpa_cls_hm *pinsert_hm;
+
+	xx_sanity_check_return_value(insert_params, "insert_params", -EINVAL);
+	xx_sanity_check_return_value(hmd, "hmd", -EINVAL);
+
+	*hmd = DPA_OFFLD_DESC_NONE;
+
+	if (res) {
+		pr_err("ERROR: %s, %s (%d): Header manipulation import is not "
+			"yet supported.\n", __FILE__, __func__, __LINE__);
+		return -ENOSYS;
+	}
+
+	err = insert_hm_check_params(insert_params);
+	if (err < 0)
+		return err;
+
+	err = create_new_hm_op(hmd, next_hmd);
+	if (err < 0)
+		return err;
+
+	pinsert_hm = (struct dpa_cls_hm *) hm_array.object[*hmd];
+
+	pinsert_hm->type	= DPA_CLS_HM_TYPE_INSERT;
+	pinsert_hm->chain_head	= chain_head;
+
+	/* Copy the insert HM parameters locally */
+	memcpy(&pinsert_hm->params.insert, insert_params,
+						sizeof(*insert_params));
+
+	if (res)
+		err = import_insert_hm(pinsert_hm, res);
+	else {
+		err = init_insert_hm(pinsert_hm);
+		if (err) {
+			dpa_classif_free_hm(*hmd);
+			*hmd = DPA_OFFLD_DESC_NONE;
+			return err;
+		}
+
+		if (chain_head)
+			/* Initialize low level HM ops chain */
+			err = init_hm_chain(pinsert_hm->params.insert.fm_pcd,
+				pinsert_hm->hm_chain, pinsert_hm->hm_chain);
+	}
+
+	if (err) {
+		dpa_classif_free_hm(*hmd);
+		*hmd = DPA_OFFLD_DESC_NONE;
+	}
+
+	return err;
+}
+
+static int import_insert_hm(struct dpa_cls_hm *pinsert_hm,
+				const struct dpa_cls_hm_insert_resources *res)
+{
+	void * const *phm_nodes;
+	int err;
+
+	xx_assert(pinsert_hm);
+	xx_assert(res);
+
+	phm_nodes = &res->insert_node;
+
+	err = import_hm_nodes_to_chain(phm_nodes, 1, pinsert_hm);
+
+	/* Update here the hm nodes parameters */
+
+	return err;
+}
+
+static int init_insert_hm(struct dpa_cls_hm *pinsert_hm)
+{
+	struct dpa_cls_hm_node *hm_node = NULL;
+	unsigned int size;
+	unsigned int offset = 0;
+	uint8_t *pdata;
+
+	/* Create a header insert node: */
+	hm_node = kzalloc(sizeof(*hm_node), GFP_KERNEL);
+	if (!hm_node) {
+		pr_err("ERROR: %s, %s (%d): No more memory for header manip "
+			"nodes.\n", __FILE__, __func__, __LINE__);
+		return -ENOMEM;
+	}
+
+	hm_node->params.type			= e_FM_PCD_MANIP_HDR;
+	hm_node->params.u.hdr.insrt		= TRUE;
+	hm_node->params.u.hdr.insrtParams.type	= e_FM_PCD_MANIP_INSRT_GENERIC;
+
+	switch (pinsert_hm->params.insert.type) {
+	case DPA_CLS_HM_INSERT_ETHERNET:
+		size = sizeof(struct ethhdr) +
+			(pinsert_hm->params.insert.param.eth.num_tags *
+			sizeof(struct vlan_header));
+		pdata = kzalloc(size, GFP_KERNEL);
+		if (!pdata) {
+			pr_err("ERROR: %s, %s (%d): Not enough memory for "
+				"insert HM.\n", __FILE__, __func__, __LINE__);
+			kfree(hm_node);
+			return -ENOMEM;
+		}
+
+		if (pinsert_hm->params.insert.param.eth.num_tags) {
+			/* Copy Ethernet header data except the EtherType */
+			memcpy(pdata,
+				&pinsert_hm->params.insert.param.eth.eth_header,
+				sizeof(struct ethhdr) - ETHERTYPE_SIZE);
+			offset += sizeof(struct ethhdr) - ETHERTYPE_SIZE;
+			/* Copy the VLAN tags */
+			memcpy(&pdata[offset],
+				&pinsert_hm->params.insert.param.eth.qtag,
+				pinsert_hm->params.insert.param.eth.num_tags *
+				sizeof(struct vlan_header));
+			offset += pinsert_hm->params.insert.param.eth.num_tags *
+				sizeof(struct vlan_header);
+			/* Copy the EtherType */
+			memcpy(&pdata[offset],
+		&pinsert_hm->params.insert.param.eth.eth_header.h_proto,
+				ETHERTYPE_SIZE);
+		} else
+			memcpy(pdata,
+				&pinsert_hm->params.insert.param.eth.eth_header,
+				sizeof(struct ethhdr));
+
+		offset = 0;
+		break;
+	case DPA_CLS_HM_INSERT_PPP:
+		size	= PPP_HEADER_SIZE;
+		pdata	= kzalloc(size, GFP_KERNEL);
+		if (!pdata) {
+			pr_err("ERROR: %s, %s (%d): Not enough memory for "
+				"insert HM.\n", __FILE__, __func__, __LINE__);
+			kfree(hm_node);
+			return -ENOMEM;
+		}
+
+		memcpy(pdata, &pinsert_hm->params.insert.param.ppp_pid,
+			PPP_HEADER_SIZE);
+		break;
+	case DPA_CLS_HM_INSERT_CUSTOM:
+		size	= pinsert_hm->params.insert.param.custom.size;
+		pdata	= pinsert_hm->params.insert.param.custom.data;
+		offset	= pinsert_hm->params.insert.param.custom.offset;
+		break;
+	default:
+		/* Will never get here */
+		return -EINVAL;
+	}
+
+	hm_node->params.u.hdr.insrtParams.u.generic.offset	= offset;
+	hm_node->params.u.hdr.insrtParams.u.generic.size	= size;
+	hm_node->params.u.hdr.insrtParams.u.generic.p_Data	= pdata;
+	hm_node->params.u.hdr.insrtParams.u.generic.replace	= FALSE;
+
+	INIT_LIST_HEAD(&hm_node->list_node);
+	pinsert_hm->hm_node[0]	= hm_node;
+	pinsert_hm->num_nodes	= 1;
+
+	add_local_hm_nodes_to_chain(pinsert_hm);
+
+	return 0;
+}
+
+int dpa_classif_modify_insert_hm(int hmd,
+	const struct dpa_cls_hm_insert_params *new_insert_params,
+	int modify_flags)
+{
+	pr_err("ERROR: %s, %s (%d): Insert HM runtime modification not "
+		"supported yet.\n", __FILE__, __func__, __LINE__);
+	return -ENOSYS;
+}
+
+int dpa_classif_set_update_hm(const struct dpa_cls_hm_update_params
+	*update_params, int next_hmd, int *hmd, bool chain_head,
+	const struct dpa_cls_hm_update_resources *res)
+{
+	int err;
+	struct dpa_cls_hm *pupdate_hm;
+
+	xx_sanity_check_return_value(update_params, "update_params", -EINVAL);
+	xx_sanity_check_return_value(hmd, "hmd", -EINVAL);
+
+	*hmd = DPA_OFFLD_DESC_NONE;
+
+	if (res) {
+		pr_err("ERROR: %s, %s (%d): Header manipulation import is not "
+			"yet supported.\n", __FILE__, __func__, __LINE__);
+		return -ENOSYS;
+	}
+
+	err = update_hm_check_params(update_params);
+	if (err < 0)
+		return err;
+
+	err = create_new_hm_op(hmd, next_hmd);
+	if (err < 0)
+		return err;
+
+	pupdate_hm = (struct dpa_cls_hm *) hm_array.object[*hmd];
+
+	pupdate_hm->type	= DPA_CLS_HM_TYPE_UPDATE;
+	pupdate_hm->chain_head	= chain_head;
+
+	/* Copy the update HM parameters locally */
+	memcpy(&pupdate_hm->params.update, update_params,
+						sizeof(*update_params));
+
+	if (res)
+		err = import_update_hm(pupdate_hm, res);
+	else {
+		err = init_update_hm(pupdate_hm);
+		if (err) {
+			dpa_classif_free_hm(*hmd);
+			*hmd = DPA_OFFLD_DESC_NONE;
+			return err;
+		}
+
+		if (chain_head)
+			/* Initialize low level HM ops chain */
+			err = init_hm_chain(pupdate_hm->params.update.fm_pcd,
+				pupdate_hm->hm_chain, pupdate_hm->hm_chain);
+	}
+
+	if (err) {
+		dpa_classif_free_hm(*hmd);
+		*hmd = DPA_OFFLD_DESC_NONE;
+	}
+
+	return err;
+}
+
+static int import_update_hm(struct dpa_cls_hm *pupdate_hm,
+				const struct dpa_cls_hm_update_resources *res)
+{
+	void * const *phm_nodes;
+	int err;
+
+	xx_assert(pupdate_hm);
+	xx_assert(res);
+
+	phm_nodes = &res->update_node;
+
+	err = import_hm_nodes_to_chain(phm_nodes, 1, pupdate_hm);
+
+	/* Update here the hm nodes parameters */
+
+	return err;
+}
+
+static int init_update_hm(struct dpa_cls_hm *pupdate_hm)
+{
+	struct dpa_cls_hm_node *hm_node = NULL;
+	struct dpa_cls_hm *pnext_hm = NULL;
+	uint8_t *data;
+	uint8_t size = 0;
+	int update_ops, replace_ops;
+
+	/* Check if we can attach to an existing update node */
+	if (!list_empty(&pupdate_hm->list_node)) {
+		pnext_hm = list_entry(pupdate_hm->list_node.next,
+				struct dpa_cls_hm,
+				list_node);
+
+		if (pupdate_hm->params.update.op_flags &
+			DPA_CLS_HM_UPDATE_IPv4_UPDATE)
+			/*
+			 * See if there is any other IPv4 update node in this
+			 * chain
+			 */
+			hm_node = find_compatible_hm_node(
+				DPA_CLS_HM_NODE_IPv4_HDR_UPDATE,
+				pnext_hm->hm_chain);
+
+		if (pupdate_hm->params.update.op_flags &
+			DPA_CLS_HM_UPDATE_IPv6_UPDATE)
+			/*
+			 * See if there is any other IPv6 update node in this
+			 * chain
+			 */
+			hm_node = find_compatible_hm_node(
+				DPA_CLS_HM_NODE_IPv6_HDR_UPDATE,
+				pnext_hm->hm_chain);
+
+		if (pupdate_hm->params.update.op_flags &
+			DPA_CLS_HM_UPDATE_UDP_TCP_UPDATE)
+			/*
+			 * See if there is any other TCP/UDP header update node
+			 * in this chain
+			 */
+			hm_node = find_compatible_hm_node(
+				DPA_CLS_HM_NODE_TCPUDP_HDR_UPDATE,
+				pnext_hm->hm_chain);
+	}
+
+	if (!hm_node) {
+		/* Create a header manip node for this update: */
+		hm_node = kzalloc(sizeof(*hm_node), GFP_KERNEL);
+		size = (uint8_t)sizeof(*hm_node);
+	}
+
+	if (!hm_node) {
+		pr_err("ERROR: %s, %s (%d): No more memory for header manip "
+			"nodes.\n", __FILE__, __func__, __LINE__);
+		return -ENOMEM;
+	}
+
+	hm_node->params.type = e_FM_PCD_MANIP_HDR;
+
+	update_ops = DPA_CLS_HM_UPDATE_IPv4_UPDATE |
+			DPA_CLS_HM_UPDATE_IPv6_UPDATE |
+			DPA_CLS_HM_UPDATE_UDP_TCP_UPDATE;
+
+	replace_ops = DPA_CLS_HM_REPLACE_IPv4_BY_IPv6 |
+			DPA_CLS_HM_REPLACE_IPv6_BY_IPv4;
+
+	if (pupdate_hm->params.update.op_flags & update_ops) {
+
+		hm_node->params.u.hdr.fieldUpdate = TRUE;
+
+		if (pupdate_hm->params.update.op_flags &
+				DPA_CLS_HM_UPDATE_IPv4_UPDATE) {
+			hm_node->params.u.hdr.fieldUpdateParams.type =
+					e_FM_PCD_MANIP_HDR_FIELD_UPDATE_IPV4;
+
+			if (pupdate_hm->params.update.update.l3.field_flags &
+				DPA_CLS_HM_IP_UPDATE_IPSA) {
+				hm_node->params.u.hdr.fieldUpdateParams.u.ipv4.
+					validUpdates |=
+					HDR_MANIP_IPV4_SRC;
+				hm_node->params.u.hdr.fieldUpdateParams.u.ipv4.
+					src =
+					pupdate_hm->params.update.update.l3.
+					ipsa.addr.ipv4.word;
+			}
+
+			if (pupdate_hm->params.update.update.l3.field_flags &
+				DPA_CLS_HM_IP_UPDATE_IPDA) {
+				hm_node->params.u.hdr.fieldUpdateParams.u.ipv4.
+					validUpdates |=
+					HDR_MANIP_IPV4_DST;
+				hm_node->params.u.hdr.fieldUpdateParams.u.ipv4.
+					dst =
+					pupdate_hm->params.update.update.l3.
+					ipda.addr.ipv4.word;
+			}
+
+			if (pupdate_hm->params.update.update.l3.field_flags &
+				DPA_CLS_HM_IP_UPDATE_TOS_TC) {
+				hm_node->params.u.hdr.fieldUpdateParams.u.ipv4.
+					validUpdates |=
+					HDR_MANIP_IPV4_TOS;
+				hm_node->params.u.hdr.fieldUpdateParams.u.ipv4.
+					tos =
+					pupdate_hm->params.update.update.l3.
+					tos_tc;
+			}
+
+			if (pupdate_hm->params.update.update.l3.field_flags &
+				DPA_CLS_HM_IP_UPDATE_ID) {
+				hm_node->params.u.hdr.fieldUpdateParams.u.ipv4.
+					validUpdates |=
+					HDR_MANIP_IPV4_ID;
+				hm_node->params.u.hdr.fieldUpdateParams.u.ipv4.
+					id =
+					pupdate_hm->params.update.update.l3.
+					initial_id;
+			}
+
+			if (pupdate_hm->params.update.update.l3.field_flags &
+				DPA_CLS_HM_IP_UPDATE_TTL_HOPL_DECREMENT)
+				hm_node->params.u.hdr.fieldUpdateParams.u.ipv4.
+					validUpdates |=
+					HDR_MANIP_IPV4_TTL;
+		}
+
+		if (pupdate_hm->params.update.op_flags &
+				DPA_CLS_HM_UPDATE_IPv6_UPDATE) {
+			hm_node->params.u.hdr.fieldUpdateParams.type =
+					e_FM_PCD_MANIP_HDR_FIELD_UPDATE_IPV6;
+
+			if (pupdate_hm->params.update.update.l3.field_flags &
+				DPA_CLS_HM_IP_UPDATE_IPSA) {
+				hm_node->params.u.hdr.fieldUpdateParams.u.ipv6.
+					validUpdates |=
+					HDR_MANIP_IPV6_SRC;
+				memcpy(hm_node->params.u.hdr.fieldUpdateParams.
+					u.ipv6.src, pupdate_hm->params.update.
+					update.l3.ipsa.addr.ipv6.byte,
+					DPA_OFFLD_IPv6_ADDR_LEN_BYTES);
+			}
+
+			if (pupdate_hm->params.update.update.l3.field_flags &
+				DPA_CLS_HM_IP_UPDATE_IPDA) {
+				hm_node->params.u.hdr.fieldUpdateParams.u.ipv4.
+					validUpdates |=
+					HDR_MANIP_IPV6_DST;
+				memcpy(hm_node->params.u.hdr.fieldUpdateParams.
+					u.ipv6.dst, pupdate_hm->params.update.
+					update.l3.ipda.addr.ipv6.byte,
+					DPA_OFFLD_IPv6_ADDR_LEN_BYTES);
+			}
+
+			if (pupdate_hm->params.update.update.l3.field_flags &
+				DPA_CLS_HM_IP_UPDATE_TOS_TC) {
+				hm_node->params.u.hdr.fieldUpdateParams.u.ipv6.
+					validUpdates |=
+					HDR_MANIP_IPV6_TC;
+				hm_node->params.u.hdr.fieldUpdateParams.u.ipv6.
+					trafficClass =
+					pupdate_hm->params.update.update.l3.
+					tos_tc;
+			}
+
+			if (pupdate_hm->params.update.update.l3.field_flags &
+				DPA_CLS_HM_IP_UPDATE_TTL_HOPL_DECREMENT)
+				hm_node->params.u.hdr.fieldUpdateParams.u.ipv6.
+					validUpdates |=
+					HDR_MANIP_IPV6_HL;
+		}
+
+		if (pupdate_hm->params.update.op_flags &
+				DPA_CLS_HM_UPDATE_UDP_TCP_UPDATE) {
+			hm_node->params.u.hdr.fieldUpdateParams.type =
+					e_FM_PCD_MANIP_HDR_FIELD_UPDATE_TCP_UDP;
+
+			if (pupdate_hm->params.update.update.l4.field_flags &
+				DPA_CLS_HM_L4_UPDATE_SPORT) {
+				hm_node->params.u.hdr.fieldUpdateParams.u.
+					tcpUdp.validUpdates |=
+					HDR_MANIP_TCP_UDP_SRC;
+				hm_node->params.u.hdr.fieldUpdateParams.u.
+					tcpUdp.src =
+					pupdate_hm->params.update.update.l4.
+					sport;
+			}
+
+			if (pupdate_hm->params.update.update.l4.field_flags &
+				DPA_CLS_HM_L4_UPDATE_DPORT) {
+				hm_node->params.u.hdr.fieldUpdateParams.u.
+					tcpUdp.validUpdates |=
+					HDR_MANIP_TCP_UDP_DST;
+				hm_node->params.u.hdr.fieldUpdateParams.u.
+					tcpUdp.dst =
+					pupdate_hm->params.update.update.l4.
+					dport;
+			}
+
+			if (pupdate_hm->params.update.update.l4.field_flags &
+				DPA_CLS_HM_L4_UPDATE_CALCULATE_CKSUM) {
+				hm_node->params.u.hdr.fieldUpdateParams.u.
+					tcpUdp.validUpdates |=
+					HDR_MANIP_TCP_UDP_CHECKSUM;
+			}
+		}
+	}
+
+	if (pupdate_hm->params.update.op_flags & replace_ops) {
+
+		hm_node->params.u.hdr.custom = TRUE;
+		hm_node->params.u.hdr.customParams.type =
+				e_FM_PCD_MANIP_HDR_CUSTOM_IP_REPLACE;
+
+		if (pupdate_hm->params.update.op_flags &
+				DPA_CLS_HM_REPLACE_IPv4_BY_IPv6) {
+
+			hm_node->params.u.hdr.customParams.u.ipHdrReplace.
+				replaceType =
+				e_FM_PCD_MANIP_HDR_CUSTOM_REPLACE_IPV4_BY_IPV6;
+			hm_node->params.u.hdr.customParams.u.ipHdrReplace.
+				hdrSize = sizeof(struct ipv6_header);
+
+			memcpy(hm_node->params.u.hdr.customParams.u.
+				ipHdrReplace.hdr,
+				&pupdate_hm->params.update.replace.ipv6_hdr,
+				sizeof(struct ipv6_header));
+		}
+
+		if (pupdate_hm->params.update.op_flags &
+				DPA_CLS_HM_REPLACE_IPv6_BY_IPv4) {
+
+			hm_node->params.u.hdr.customParams.u.ipHdrReplace.
+				replaceType =
+				e_FM_PCD_MANIP_HDR_CUSTOM_REPLACE_IPV6_BY_IPV4;
+			hm_node->params.u.hdr.customParams.u.ipHdrReplace.
+				hdrSize = sizeof(struct iphdr);
+
+			data = kzalloc(sizeof(struct iphdr), GFP_KERNEL);
+			if (!data) {
+				kfree(hm_node);
+				pr_err("ERROR: %s, %s (%d): No more memory for "
+					"header manipulation.\n", __FILE__,
+					__func__, __LINE__);
+				return -ENOMEM;
+			}
+
+			memcpy(hm_node->params.u.hdr.customParams.u.
+				ipHdrReplace.hdr,
+				&pupdate_hm->params.update.replace.ipv4_hdr,
+				sizeof(struct iphdr));
+		}
+	}
+
+	pupdate_hm->hm_node[0]	= hm_node;
+	pupdate_hm->num_nodes	= 1;
+
+	if (size)
+		INIT_LIST_HEAD(&pupdate_hm->hm_node[0]->list_node);
+
+	if (pupdate_hm->params.update.ip_frag_params.mtu) {
+		/* IP fragmentation option is enabled */
+		/* Create a header manip node: */
+		hm_node = kzalloc(sizeof(*hm_node), GFP_KERNEL);
+		if (!hm_node) {
+			pr_err("ERROR: %s, %s (%d): No more memory for header "
+				"manip nodes.\n", __FILE__, __func__, __LINE__);
+			return -ENOMEM;
+		}
+
+		hm_node->params.type = e_FM_PCD_MANIP_FRAG;
+		hm_node->params.u.frag.hdr = HEADER_TYPE_IPv4;
+		hm_node->params.u.frag.u.ipFrag.sizeForFragmentation =
+				pupdate_hm->params.update.ip_frag_params.mtu;
+		hm_node->params.u.frag.u.ipFrag.scratchBpid =
+				pupdate_hm->params.update.ip_frag_params.
+					scratch_bpid;
+		switch (pupdate_hm->params.update.ip_frag_params.df_action) {
+		case DPA_CLS_HM_DF_ACTION_FRAG_ANYWAY:
+			hm_node->params.u.frag.u.ipFrag.dontFragAction =
+					e_FM_PCD_MANIP_FRAGMENT_PACKET;
+			break;
+		case DPA_CLS_HM_DF_ACTION_DONT_FRAG:
+			hm_node->params.u.frag.u.ipFrag.dontFragAction =
+					e_FM_PCD_MANIP_CONTINUE_WITHOUT_FRAG;
+			break;
+		case DPA_CLS_HM_DF_ACTION_DROP:
+			hm_node->params.u.frag.u.ipFrag.dontFragAction =
+				e_FM_PCD_MANIP_ENQ_TO_ERR_Q_OR_DISCARD_PACKET;
+			break;
+		}
+
+		INIT_LIST_HEAD(&hm_node->list_node);
+
+		pupdate_hm->hm_node[1]	= hm_node;
+		pupdate_hm->num_nodes	= 2;
+	}
+
+	add_local_hm_nodes_to_chain(pupdate_hm);
+
+	return 0;
+}
+
+int dpa_classif_modify_update_hm(int hmd,
+	const struct dpa_cls_hm_update_params *new_update_params,
+	int modify_flags)
+{
+	pr_err("ERROR: %s, %s (%d): Update HM runtime modification not "
+		"supported yet.\n", __FILE__, __func__, __LINE__);
+	return -ENOSYS;
+}
+
+int dpa_classif_set_vlan_hm(const struct dpa_cls_hm_vlan_params	*vlan_params,
+			int					next_hmd,
+			int					*hmd,
+			bool					chain_head,
+			const struct dpa_cls_hm_vlan_resources	*res)
+{
+	int err;
+	struct dpa_cls_hm *pvlan_hm;
+
+	xx_sanity_check_return_value(vlan_params, "vlan_params", -EINVAL);
+	xx_sanity_check_return_value(hmd, "hmd", -EINVAL);
+
+	*hmd = DPA_OFFLD_DESC_NONE;
+
+	if (res) {
+		pr_err("ERROR: %s, %s (%d): Header manipulation import is not "
+			"yet supported.\n", __FILE__, __func__, __LINE__);
+		return -ENOSYS;
+	}
+
+	err = vlan_hm_check_params(vlan_params);
+	if (err < 0)
+		return err;
+
+	err = create_new_hm_op(hmd, next_hmd);
+	if (err < 0)
+		return err;
+
+	pvlan_hm = (struct dpa_cls_hm *) hm_array.object[*hmd];
+
+	pvlan_hm->type		= DPA_CLS_HM_TYPE_VLAN;
+	pvlan_hm->chain_head	= chain_head;
+
+	/* Copy the VLAN specific HM parameters locally */
+	memcpy(&pvlan_hm->params.vlan, vlan_params, sizeof(*vlan_params));
+
+	if (res)
+		err = import_vlan_hm(pvlan_hm, res);
+	else {
+		err = init_vlan_hm(pvlan_hm);
+		if (err) {
+			dpa_classif_free_hm(*hmd);
+			*hmd = DPA_OFFLD_DESC_NONE;
+			return err;
+		}
+
+		if (chain_head)
+			/* Initialize low level HM ops chain */
+			err = init_hm_chain(pvlan_hm->params.vlan.fm_pcd,
+				pvlan_hm->hm_chain, pvlan_hm->hm_chain);
+	}
+
+	if (err) {
+		dpa_classif_free_hm(*hmd);
+		*hmd = DPA_OFFLD_DESC_NONE;
+	}
+
+	return err;
+}
+
+static int import_vlan_hm(struct dpa_cls_hm *pvlan_hm,
+				const struct dpa_cls_hm_vlan_resources *res)
+{
+	void * const *phm_nodes;
+	int err;
+
+	xx_assert(pvlan_hm);
+	xx_assert(res);
+
+	phm_nodes = &res->vlan_node;
+
+	err = import_hm_nodes_to_chain(phm_nodes, 1, pvlan_hm);
+
+	/* Update here the hm nodes parameters */
+
+	return err;
+}
+
+static int init_vlan_hm(struct dpa_cls_hm *pvlan_hm)
+{
+	struct dpa_cls_hm_node *hm_node = NULL;
+	uint8_t size;
+	uint8_t *pdata;
+
+	/* Create a header manip node for VLAN: */
+	hm_node = kzalloc(sizeof(*hm_node), GFP_KERNEL);
+	if (!hm_node) {
+		pr_err("ERROR: %s, %s (%d): No more memory for header manip "
+			"nodes.\n", __FILE__, __func__, __LINE__);
+		return -ENOMEM;
+	}
+
+	hm_node->params.type = e_FM_PCD_MANIP_HDR;
+
+	switch (pvlan_hm->params.vlan.type) {
+	case DPA_CLS_HM_VLAN_INGRESS:
+		hm_node->params.u.hdr.rmv = TRUE;
+		hm_node->params.u.hdr.rmvParams.type	=
+					e_FM_PCD_MANIP_RMV_BY_HDR;
+		hm_node->params.u.hdr.rmvParams.u.byHdr.type =
+					e_FM_PCD_MANIP_RMV_BY_HDR_SPECIFIC_L2;
+		hm_node->params.u.hdr.rmvParams.u.byHdr.u.specificL2 =
+					e_FM_PCD_MANIP_HDR_RMV_STACKED_QTAGS;
+
+		break;
+	case DPA_CLS_HM_VLAN_EGRESS:
+		if (pvlan_hm->params.vlan.params.egress.num_tags) {
+
+			hm_node->params.u.hdr.insrt = TRUE;
+			hm_node->params.u.hdr.insrtParams.type =
+						e_FM_PCD_MANIP_INSRT_GENERIC;
+			hm_node->params.u.hdr.insrtParams.u.generic.offset =
+							ETHERTYPE_OFFSET;
+
+			size = pvlan_hm->params.vlan.params.egress.num_tags *
+						sizeof(struct vlan_header);
+			pdata = kzalloc(size, GFP_KERNEL);
+			if (!pdata) {
+				pr_err("ERROR: %s, %s (%d): Not enough memory "
+					"for VLAN specific HM.\n", __FILE__,
+					__func__, __LINE__);
+				kfree(hm_node);
+				return -ENOMEM;
+			}
+
+			memcpy(pdata, pvlan_hm->params.vlan.params.egress.qtag,
+				size);
+
+			hm_node->params.u.hdr.insrtParams.u.generic.size =
+									size;
+			hm_node->params.u.hdr.insrtParams.u.generic.p_Data =
+									pdata;
+			hm_node->params.u.hdr.insrtParams.u.generic.replace =
+									FALSE;
+		}
+
+		if (pvlan_hm->params.vlan.params.egress.update_op !=
+					DPA_CLS_HM_VLAN_UPDATE_NONE) {
+
+			hm_node->params.u.hdr.fieldUpdate = TRUE;
+			hm_node->params.u.hdr.fieldUpdateParams.type =
+					e_FM_PCD_MANIP_HDR_FIELD_UPDATE_VLAN;
+
+			switch (pvlan_hm->params.vlan.params.egress.
+								update_op) {
+			case DPA_CLS_HM_VLAN_UPDATE_VPri:
+				hm_node->params.u.hdr.fieldUpdateParams.u.vlan.
+					updateType =
+				e_FM_PCD_MANIP_HDR_FIELD_UPDATE_VLAN_VPRI;
+				hm_node->params.u.hdr.fieldUpdateParams.u.vlan.
+					u.vpri =
+					pvlan_hm->params.vlan.params.egress.
+					update_params.vpri;
+				break;
+			case DPA_CLS_HM_VLAN_UPDATE_VPri_BY_DSCP:
+				hm_node->params.u.hdr.fieldUpdateParams.u.vlan.
+					updateType =
+				e_FM_PCD_MANIP_HDR_FIELD_UPDATE_DSCP_TO_VLAN;
+				memcpy(hm_node->params.u.hdr.fieldUpdateParams.
+					u.vlan.u.dscpToVpri.dscpToVpriTable,
+					pvlan_hm->params.vlan.params.egress.
+					update_params.dscp_to_vpri,
+					FM_PCD_MANIP_DSCP_TO_VLAN_TRANS);
+				break;
+			default:
+				pr_err("ERROR: %s, %s (%d): Unknown VLAN update "
+					"type.\n", __FILE__, __func__,
+					__LINE__);
+				kfree(hm_node);
+				return -EINVAL;
+				break;
+			}
+		}
+
+		break;
+	default: /* Will never get here */
+		break;
+	}
+
+	INIT_LIST_HEAD(&hm_node->list_node);
+	pvlan_hm->hm_node[0]	= hm_node;
+	pvlan_hm->num_nodes	= 1;
+
+	add_local_hm_nodes_to_chain(pvlan_hm);
+
+	return 0;
+}
+
+int dpa_classif_modify_vlan_hm(int hmd,
+	const struct dpa_cls_hm_vlan_params *new_vlan_params, int modify_flags)
+{
+	pr_err("ERROR: %s, %s (%d): VLAN HM runtime modification not supported "
+		"yet.\n", __FILE__, __func__, __LINE__);
+	return -ENOSYS;
+}
+
+int dpa_classif_set_mpls_hm(const struct dpa_cls_hm_mpls_params	*mpls_params,
+			int					next_hmd,
+			int					*hmd,
+			bool					chain_head,
+			const struct dpa_cls_hm_mpls_resources	*res)
+{
+	int err;
+	struct dpa_cls_hm *pmpls_hm;
+
+	xx_sanity_check_return_value(mpls_params, "mpls_params", -EINVAL);
+	xx_sanity_check_return_value(hmd, "hmd", -EINVAL);
+
+	*hmd = DPA_OFFLD_DESC_NONE;
+
+	pr_err("ERROR: %s, %s (%d): MPLS specific header manipulation is not "
+		"supported yet.\n", __FILE__, __func__, __LINE__);
+	return -ENOSYS;
+
+	err = mpls_hm_check_params(mpls_params);
+	if (err < 0)
+		return err;
+
+	err = create_new_hm_op(hmd, next_hmd);
+	if (err < 0)
+		return err;
+
+	pmpls_hm = (struct dpa_cls_hm *) hm_array.object[*hmd];
+
+	pmpls_hm->type		= DPA_CLS_HM_TYPE_MPLS;
+	pmpls_hm->chain_head	= chain_head;
+
+	/* Copy the VLAN specific HM parameters locally */
+	memcpy(&pmpls_hm->params.mpls, mpls_params, sizeof(*mpls_params));
+
+	if (res)
+		err = import_mpls_hm(pmpls_hm, res);
+	else {
+		err = init_mpls_hm(pmpls_hm);
+		if (err) {
+			dpa_classif_free_hm(*hmd);
+			*hmd = DPA_OFFLD_DESC_NONE;
+			return err;
+		}
+
+		if (chain_head)
+			/* Initialize low level HM ops chain */
+			err = init_hm_chain(pmpls_hm->params.mpls.fm_pcd,
+				pmpls_hm->hm_chain, pmpls_hm->hm_chain);
+	}
+
+	if (err) {
+		dpa_classif_free_hm(*hmd);
+		*hmd = DPA_OFFLD_DESC_NONE;
+	}
+
+	return err;
+}
+
+static int import_mpls_hm(struct dpa_cls_hm *pmpls_hm,
+				const struct dpa_cls_hm_mpls_resources *res)
+{
+	void * const *phm_nodes;
+	int err;
+
+	xx_assert(pmpls_hm);
+	xx_assert(res);
+
+	phm_nodes = &res->ins_rm_node;
+
+	err = import_hm_nodes_to_chain(phm_nodes, 1, pmpls_hm);
+
+	/* Update here the hm nodes parameters */
+
+	return err;
+}
+
+static int init_mpls_hm(struct dpa_cls_hm *pmpls_hm)
+{
+	struct dpa_cls_hm_node *hm_node = NULL;
+	uint8_t size;
+	uint8_t *pdata;
+
+	/* Create a header manip node for MPLS: */
+	hm_node = kzalloc(sizeof(*hm_node), GFP_KERNEL);
+	if (!hm_node) {
+		pr_err("ERROR: %s, %s (%d): No more RAM for header manip "
+			"nodes.\n", __FILE__, __func__, __LINE__);
+		return -ENOMEM;
+	}
+
+	hm_node->params.type = e_FM_PCD_MANIP_HDR;
+
+	switch (pmpls_hm->params.mpls.type) {
+	case DPA_CLS_HM_MPLS_REMOVE_ALL_LABELS:
+		hm_node->params.u.hdr.rmv = TRUE;
+		hm_node->params.u.hdr.rmvParams.type =
+					e_FM_PCD_MANIP_RMV_BY_HDR;
+		hm_node->params.u.hdr.rmvParams.u.byHdr.type =
+					e_FM_PCD_MANIP_RMV_BY_HDR_SPECIFIC_L2;
+		hm_node->params.u.hdr.rmvParams.u.byHdr.u.specificL2 =
+					e_FM_PCD_MANIP_HDR_RMV_MPLS;
+
+		break;
+	case DPA_CLS_HM_MPLS_INSERT_LABELS:
+		hm_node->params.u.hdr.insrt = TRUE;
+		hm_node->params.u.hdr.insrtParams.type =
+					e_FM_PCD_MANIP_INSRT_BY_HDR;
+		hm_node->params.u.hdr.insrtParams.u.byHdr.type =
+					e_FM_PCD_MANIP_INSRT_BY_HDR_SPECIFIC_L2;
+		hm_node->params.u.hdr.insrtParams.u.byHdr.u.specificL2Params.
+					specificL2 =
+					e_FM_PCD_MANIP_HDR_INSRT_MPLS;
+
+		size = pmpls_hm->params.mpls.num_labels *
+						sizeof(struct mpls_header);
+		pdata = kzalloc(size, GFP_KERNEL);
+		if (!pdata) {
+			pr_err("ERROR: %s, %s (%d): Not enough memory for MPLS "
+				"specific HM.\n", __FILE__, __func__, __LINE__);
+			kfree(hm_node);
+			return -ENOMEM;
+		}
+
+		memcpy(pdata, pmpls_hm->params.mpls.mpls_hdr, size);
+
+		hm_node->params.u.hdr.insrtParams.u.byHdr.u.specificL2Params.
+			size = size;
+		hm_node->params.u.hdr.insrtParams.u.byHdr.u.specificL2Params.
+			p_Data = pdata;
+
+		break;
+	default:
+		kfree(hm_node);
+		pr_err("ERROR: %s, %s (%d): Unknown MPLS header manipulation "
+			"(%d).\n", __FILE__, __func__, __LINE__,
+			pmpls_hm->params.mpls.type);
+		return -EINVAL;
+	}
+
+	INIT_LIST_HEAD(&hm_node->list_node);
+	pmpls_hm->hm_node[0]	= hm_node;
+	pmpls_hm->num_nodes	= 1;
+
+	add_local_hm_nodes_to_chain(pmpls_hm);
+
+	return 0;
+}
+
+int dpa_classif_modify_mpls_hm(int hmd,
+	const struct dpa_cls_hm_mpls_params *new_mpls_params, int modify_flags)
+{
+	pr_err("ERROR: %s, %s (%d): MPLS HM runtime modification not supported "
+		"yet.\n", __FILE__, __func__, __LINE__);
+	return -ENOSYS;
+}
+
+int dpa_classif_free_hm(int hmd)
+{
+	struct dpa_cls_hm *phm;
+
+	phm = (struct dpa_cls_hm *) hm_array.object[hmd];
+
+	if (!phm)
+		/* Descriptor already free */
+		return 0;
+
+	if (!list_empty(&phm->list_node))
+		list_del(&phm->list_node);
+
+	if (phm->chain_head) {
+		/*
+		 * This is a header manip chain head, hence remove the entire
+		 * chain of low level ops
+		 */
+		if (remove_hm_chain(phm->hm_chain, phm->hm_chain) < 0)
+			pr_err("WARNING: Failed to remove HM nodes for chain "
+				"hmd=%d.\n", hmd);
+	}
+
+	kfree(phm);
+	hm_array.object[hmd] = NULL;
+
+	put_descriptor(&hm_array, hmd);
+
+	return 0;
+}
diff --git a/drivers/staging/fsl_dpa_offload/dpa_classifier.h b/drivers/staging/fsl_dpa_offload/dpa_classifier.h
index ab68042..e44a035 100644
--- a/drivers/staging/fsl_dpa_offload/dpa_classifier.h
+++ b/drivers/staging/fsl_dpa_offload/dpa_classifier.h
@@ -57,6 +57,8 @@
  */
 #define DPA_CLS_TBL_MAXSHADOWTABLESIZE				256
 
+#define DPA_CLS_HM_MAX_NODES_PER_OP				3
+
 /* Index management entry */
 struct dpa_cls_tbl_entry {
 
@@ -205,6 +207,57 @@ struct dpa_cls_descriptor_table {
 	void		**object;
 };
 
+struct dpa_cls_hm_node {
+	void			*node;
+	t_FmPcdManipParams	params;
+	struct list_head	list_node;
+};
+
+enum dpa_cls_hm_node_type {
+	DPA_CLS_HM_NODE_IPv4_HDR_UPDATE,
+	DPA_CLS_HM_NODE_IPv6_HDR_UPDATE,
+	DPA_CLS_HM_NODE_TCPUDP_HDR_UPDATE,
+	DPA_CLS_HM_NODE_HDR_REPLACE_IPv4_BY_IPv6,
+	DPA_CLS_HM_NODE_HDR_REPLACE_IPv6_BY_IPv4,
+	DPA_CLS_HM_NODE_LAST_ENTRY
+};
+
+enum dpa_cls_hm_type {
+	DPA_CLS_HM_TYPE_NAT,
+	DPA_CLS_HM_TYPE_FORWARDING,
+	DPA_CLS_HM_TYPE_REMOVE,
+	DPA_CLS_HM_TYPE_INSERT,
+	DPA_CLS_HM_TYPE_UPDATE,
+	DPA_CLS_HM_TYPE_VLAN,
+	DPA_CLS_HM_TYPE_MPLS,
+	DPA_CLS_HM_LAST_ENTRY
+};
+
+struct dpa_cls_hm {
+	enum dpa_cls_hm_type				type;
+
+	bool						chain_head;
+
+	union {
+		struct dpa_cls_hm_nat_params		nat;
+		struct dpa_cls_hm_fwd_params		fwd;
+		struct dpa_cls_hm_remove_params		remove;
+		struct dpa_cls_hm_insert_params		insert;
+		struct dpa_cls_hm_update_params		update;
+		struct dpa_cls_hm_vlan_params		vlan;
+		struct dpa_cls_hm_mpls_params		mpls;
+	} params;
+
+	struct dpa_cls_hm_node		*hm_node[DPA_CLS_HM_MAX_NODES_PER_OP];
+
+	unsigned int			num_nodes;
+
+	/* Pointer to the low level driver HM node chain */
+	struct list_head		*hm_chain;
+
+	struct list_head		list_node;
+};
+
 
 /*
  * Allocates the array of internally managed Cc nodes based on
diff --git a/include/linux/fsl_dpa_classifier.h b/include/linux/fsl_dpa_classifier.h
index 28a794b..7516822 100644
--- a/include/linux/fsl_dpa_classifier.h
+++ b/include/linux/fsl_dpa_classifier.h
@@ -38,17 +38,30 @@
 #define __FSL_DPA_CLASSIFIER_H
 
 
+#include <linux/ip.h>
+
 /* DPA offloading layer includes */
 #include "fsl_dpa_compat.h"
 #include "fsl_dpa_offload.h"
 
 
+/* General definitions */
+
+/* Maximum number of VLAN tags supported by the insert header manipulation */
+#define DPA_CLS_HM_MAX_VLANs					6
+/* Maximum number of MPLS labels supported by the insert header manipulation */
+#define DPA_CLS_HM_MAX_MPLS_LABELS				6
+/* Standard size of the DSCP-to-VPri mapping table */
+#define DPA_CLS_HM_DSCP_TO_VLAN_TABLE_SIZE			32
+/* DPA Classifier maximum size of a lookup key, in bytes */
+#define DPA_CLS_TBL_MAXENTRYKEYSIZE				56
+/* Number of entries in the DSCP-to-VPri mapping table */
+#define DPA_CLS_HM_DSCP_TO_VPRI_TABLE_SIZE			64
 
 /* API functions, definitions and enums */
 
 
-/* DPA Classifier maximum size of a lookup key, in bytes */
-#define DPA_CLS_TBL_MAXENTRYKEYSIZE				56
+/* Table API */
 
 
 /* DPA Classifier Table Types */
@@ -97,6 +110,7 @@ enum dpa_cls_tbl_entry_mgmt {
 					* their reference (Id) */
 };
 
+
 /* DPA Classifier HASH table parameters */
 struct dpa_cls_tbl_hash_params {
 
@@ -177,8 +191,6 @@ struct dpa_cls_tbl_policer_params {
 						* is nonzero */
 };
 
-struct dpa_cls_tbl_header_manip;
-
 /* Enqueue action parameters */
 struct dpa_cls_tbl_enq_action_desc {
 
@@ -201,10 +213,11 @@ struct dpa_cls_tbl_enq_action_desc {
 	struct dpa_cls_tbl_policer_params	*policer_params;
 
 	/*
-	 * Pointer to the header manipulation. If NULL, no header
-	 * manipulation will be performed.
+	 * Descriptor of the header manipulation chain to use with this
+	 * entry. Use a negative value if no header manipulation should be
+	 * performed for this entry.
 	 */
-	struct dpa_cls_tbl_header_manip		*hm;
+	int					hmd;
 };
 
 /* Action parameters to route to a new classifier table */
@@ -297,8 +310,8 @@ struct dpa_cls_tbl_entry_stats {
  * ownership of the node, unpredictable behavior and data
  * inconsistency can occur.
  */
-int	dpa_classif_table_create(const struct dpa_cls_tbl_params *params,
-				int *td);
+int dpa_classif_table_create(const struct dpa_cls_tbl_params	*params,
+				int				*td);
 
 /*
  * Releases all resources associated with a DPA Classifier table
@@ -433,4 +446,987 @@ int dpa_classif_table_reset_entry_stats_by_ref(int		td,
 int dpa_classif_table_get_params(int td, struct dpa_cls_tbl_params *params);
 
 
+/* Header Manipulation API */
+
+
+/* Supported protocols for NAT header manipulations */
+enum dpa_cls_hm_nat_proto {
+	DPA_CLS_NAT_PROTO_UDP,
+	DPA_CLS_NAT_PROTO_TCP,
+	DPA_CLS_NAT_PROTO_ICMP,
+	DPA_CLS_NAT_PROTO_LAST_ENTRY
+};
+
+/* NAT operation type */
+enum dpa_cls_hm_nat_type {
+	DPA_CLS_HM_NAT_TYPE_TRADITIONAL,
+	DPA_CLS_HM_NAT_TYPE_NAT_PT,
+	DPA_CLS_HM_NAT_TYPE_LAST_ENTRY
+};
+
+/*
+ * Flag values indicating the possible fields to be updated with the
+ * NAT header manipulation
+ */
+enum dpa_cls_hm_nat_flags {
+	DPA_CLS_HM_NAT_UPDATE_SIP	= 0x01,
+	DPA_CLS_HM_NAT_UPDATE_DIP	= 0x02,
+	DPA_CLS_HM_NAT_UPDATE_SPORT	= 0x04,
+	DPA_CLS_HM_NAT_UPDATE_DPORT	= 0x08
+};
+
+/* Type of protocol translation for NAT */
+enum dpa_cls_hm_nat_pt_type {
+	DPA_CLS_HM_NAT_PT_IPv6_TO_IPv4,
+	DPA_CLS_HM_NAT_PT_IPv4_TO_IPv6
+};
+
+/*
+ * Flag values indicating which attributes of the NAT header manipulation to
+ * modify
+ */
+enum dpa_cls_hm_nat_modify_flags {
+	DPA_CLS_HM_NAT_MOD_FLAGS	= 0x01,
+	DPA_CLS_HM_NAT_MOD_SIP		= 0x02,
+	DPA_CLS_HM_NAT_MOD_DIP		= 0x04,
+	DPA_CLS_HM_NAT_MOD_SPORT	= 0x08,
+	DPA_CLS_HM_NAT_MOD_DPORT	= 0x10,
+	DPA_CLS_HM_NAT_MOD_IP_HDR	= 0x20
+};
+
+/* NAT header manipulation low level driver resources */
+struct dpa_cls_hm_nat_resources {
+	/*
+	 * Handle to a header manipulation node which may combine a local
+	 * IPv4/IPv6 update header manipulation with an IP protocol replace.
+	 * This is a FMan driver header manipulation node handle and it is
+	 * mandatory for the import to succeed.
+	 */
+	void	*l3_update_node;
+
+	/*
+	 * Handle to the local TCP/UDP update header manipulation node. This is
+	 * a FMan driver header manipulation node handle and it is optional
+	 * (can be NULL in case no L4 header updates are necessary for this NAT
+	 * flow).
+	 */
+	void	*l4_update_node;
+};
+
+/* Traditional NAT parameters */
+struct dpa_cls_hm_traditional_nat_params {
+	/* New source IP address */
+	struct dpa_offload_ip_address		sip;
+
+	/* New destination IP address */
+	struct dpa_offload_ip_address		dip;
+};
+
+/* NAT-PT parameters */
+struct dpa_cls_hm_nat_pt_params {
+	/*
+	 * Specifies the protocol replacement for NAT-PT: either IPv4-to-IPv6
+	 * or IPv6-to-IPv4
+	 */
+	enum dpa_cls_hm_nat_pt_type		type;
+
+	union {
+		/* IPv4 header data to replace IPv6 with */
+		struct iphdr			ipv4;
+
+		/* IPv6 header data to replace IPv4 with */
+		struct ipv6_header		ipv6;
+	} header;
+};
+
+/* Definition of a NAT related header manipulation */
+struct dpa_cls_hm_nat_params {
+	/*
+	 * NAT operation flags specify which fields in the packet should be
+	 * updated. This is a combination of the values in the
+	 * dpa_cls_hm_nat_flags enum.
+	 */
+	int							flags;
+
+	/* Protocol to perform NAT for */
+	enum dpa_cls_hm_nat_proto				proto;
+
+	/* Selects the flavor of NAT to configure */
+	enum dpa_cls_hm_nat_type				type;
+
+
+	union {
+		/*
+		 * Traditional NAT header manipulation parameters. Used only
+		 * when traditional NAT is selected using the [type] attribute.
+		 */
+		struct dpa_cls_hm_traditional_nat_params	nat;
+
+		/*
+		 * NAT-PT header manipulation parameters. Used only when NAT-PT
+		 * is selected using the [type] attribute.
+		 */
+		struct dpa_cls_hm_nat_pt_params			nat_pt;
+	} params;
+
+	/*
+	 * New L4 protocol source port number; used when selected using the
+	 * flags attribute.
+	 */
+	uint16_t						sport;
+
+	/*
+	 * New L4 protocol destination port number; used only when selected
+	 * using the flags attribute
+	 */
+	uint16_t						dport;
+
+	/*
+	 * Handle to the low level driver PCD to use when creating the header
+	 * manipulation object.
+	 */
+	void							*fm_pcd;
+};
+
+/* Output interface type for forwarding */
+enum dpa_cls_hm_out_if_type {
+	DPA_CLS_HM_IF_TYPE_ETHERNET,
+	DPA_CLS_HM_IF_TYPE_PPPoE,
+	DPA_CLS_HM_IF_TYPE_PPP,
+	DPA_CLS_HM_IF_TYPE_LAST_ENTRY
+};
+
+/*
+ * Flag values indicating which forwarding header manipulation attributes to
+ * modify
+ */
+enum dpa_cls_hm_fwd_modify_flags {
+	DPA_CLS_HM_FWD_MOD_ETH_MACSA		= 0x01,
+	DPA_CLS_HM_FWD_MOD_ETH_MACDA		= 0x02,
+	DPA_CLS_HM_FWD_MOD_PPPoE_HEADER		= 0x04,
+	DPA_CLS_HM_FWD_MOD_PPP_PID		= 0x08,
+	DPA_CLS_HM_FWD_MOD_IP_FRAG_MTU		= 0x10,
+	DPA_CLS_HM_FWD_MOD_IP_FRAG_SCRATCH_BPID	= 0x20
+};
+
+enum dpa_cls_hm_frag_df_action {
+	DPA_CLS_HM_DF_ACTION_FRAG_ANYWAY,
+	DPA_CLS_HM_DF_ACTION_DONT_FRAG,
+	DPA_CLS_HM_DF_ACTION_DROP
+};
+
+/* IP fragmentation parameters */
+struct dpa_cls_hm_ip_frag_params {
+	/* Maximum Transfer Unit. Use zero to disable IP fragmentation. */
+	uint16_t				mtu;
+
+	/*
+	 * Scratch buffer pool ID. This is necessary for the IP fragmentation.
+	 * It is ignored if IP fragmentation is disabled
+	 */
+	uint8_t					scratch_bpid;
+
+	/* Specifies how to deal with packets with DF flag on */
+	enum dpa_cls_hm_frag_df_action		df_action;
+};
+
+struct dpa_cls_hm_fwd_l2_param {
+	/* New Ethernet destination MAC address to update the L2 header */
+	uint8_t				macda[ETH_ALEN];
+
+	/* New Ethernet source MAC address to update the L2 header */
+	uint8_t				macsa[ETH_ALEN];
+};
+
+/* Forwarding header manipulation parameters for a PPPoE output interface */
+struct dpa_cls_hm_fwd_pppoe_param {
+	/* L2 header update parameters */
+	struct dpa_cls_hm_fwd_l2_param		l2;
+
+	/*
+	 * PPPoE header to be inserted in the packets. The PPPoE payload length
+	 * field is updated automatically (you can set it to zero).
+	 */
+	struct pppoe_header			pppoe_header;
+};
+
+/* Forwarding header manipulation parameters for a PPP output interface */
+struct dpa_cls_hm_fwd_ppp_param {
+	/* PPP PID value to use in the PPP header to be inserted */
+	uint16_t				ppp_pid;
+};
+
+/* Forwarding header manipulation low level driver resources */
+struct dpa_cls_hm_fwd_resources {
+	/*
+	 * Handle to the forwarding header manipulation node.
+	 *
+	 * In case of an Ethernet or a PPPoE output interface this is a local
+	 * header replace header manipulation node (for Ethernet MAC addresses).
+	 *
+	 * In case of a PPP output interface this is a protocol specific header
+	 * removal node (for Ethernet and VLAN tags) combined with an internal
+	 * header insert.
+	 *
+	 * This is a FMan driver header manipulation node handle and it is
+	 * mandatory for the import to succeed.
+	 */
+	void	*fwd_node;
+
+	/*
+	 * Handle to the PPPoE specific node. This is an internal protocol
+	 * specific insert PPPoE header manipulation node. This is a FMan driver
+	 * header manipulation node handle and it is optional (can be NULL in
+	 * case the output interface type is not PPPoE).
+	 */
+	void	*pppoe_node;
+
+	/*
+	 * Handle to the IP fragmentation node. This is a FMan driver header
+	 * manipulation node handle and it is optional (can be NULL in case no
+	 * IP fragmentation is enabled for this IP forwarding flow).
+	 */
+	void	*ip_frag_node;
+};
+
+/* Forwarding header manipulation parameters */
+struct dpa_cls_hm_fwd_params {
+	/*
+	 * Output interface type. Based on this selection the DPA Classifier
+	 * decides which header manipulations are needed to perform forwarding.
+	 */
+	enum dpa_cls_hm_out_if_type			out_if_type;
+
+	union {
+		/* Necessary parameters for an Ethernet output interface */
+		struct dpa_cls_hm_fwd_l2_param		eth;
+
+		/* Necessary parameters for a PPPoE output interface */
+		struct dpa_cls_hm_fwd_pppoe_param	pppoe;
+
+		/* Necessary parameters for a PPP output interface */
+		struct dpa_cls_hm_fwd_ppp_param		ppp;
+	} param;
+
+	/* Parameters related to optional IP fragmentation */
+	struct dpa_cls_hm_ip_frag_params		ip_frag_params;
+
+	/*
+	 * Handle to the low level driver PCD to use when creating the header
+	 * manipulation object.
+	 */
+	void						*fm_pcd;
+};
+
+/* Types of the remove header manipulation operations */
+enum dpa_cls_hm_remove_type {
+	DPA_CLS_HM_REMOVE_ETHERNET,	/* removes ETH and all QTags */
+	DPA_CLS_HM_REMOVE_PPPoE,	/* removes ETH, all QTags and PPPoE */
+	DPA_CLS_HM_REMOVE_PPP,
+	DPA_CLS_HM_REMOVE_CUSTOM,	/* General remove */
+	DPA_CLS_HM_REMOVE_LAST_ENTRY
+};
+
+/*
+ * Flag values indicating which attributes of the remove header manipulation
+ * to modify
+ */
+enum dpa_cls_hm_remove_modify_flags {
+	DPA_CLS_HM_RM_MOD_TYPE		= 0x01,
+	DPA_CLS_HM_RM_MOD_CUSTOM_OFFSET	= 0x02,
+	DPA_CLS_HM_RM_MOD_CUSTOM_SIZE	= 0x04
+};
+
+/* General (custom) remove header manipulation parameters */
+struct dpa_cls_hm_custom_rm_params {
+	/*
+	 * Offset in bytes, relative to the start of the packet, to start
+	 * removing data from
+	 */
+	uint8_t						offset;
+
+	/* The size in bytes of the section to remove */
+	uint8_t						size;
+};
+
+/* Ingress remove header manipulation low level driver resources */
+struct dpa_cls_hm_remove_resources {
+	/*
+	 * Handle to either a header removal node or a protocol specific header
+	 * removal node (for Ethernet and all VLAN tags). This is a FMan driver
+	 * header manipulation node handle and it is mandatory for the import
+	 * to succeed.
+	 */
+	void	*remove_node;
+};
+
+/* Ingress (remove) header manipulation parameters */
+struct dpa_cls_hm_remove_params {
+
+	/*
+	 * Selects the type of the remove header manipulation operation  to
+	 * perform. Protocol specific header removals don't need any further
+	 * parameters.
+	 */
+	enum dpa_cls_hm_remove_type			type;
+
+	/*
+	 * Parameters for the custom remove header manipulation. If [type] is
+	 * anything else than "custom remove", these parameters are ignored
+	 */
+	struct dpa_cls_hm_custom_rm_params		custom;
+
+	/*
+	 * Handle to the low level driver PCD to use when creating the header
+	 * manipulation object.
+	 */
+	void						*fm_pcd;
+};
+
+/* Types of insert header manipulation operations */
+enum dpa_cls_hm_insert_type {
+	DPA_CLS_HM_INSERT_ETHERNET,	/* Insert Ethernet + QTags */
+	DPA_CLS_HM_INSERT_PPPoE,	/* Insert PPPoE, ETH and QTags */
+	DPA_CLS_HM_INSERT_PPP,
+	DPA_CLS_HM_INSERT_CUSTOM,	/* General insert */
+	DPA_CLS_HM_INSERT_LAST_ENTRY
+};
+
+/*
+ * Flag values indicating which attributes of the insert header manipulation
+ * to modify
+ */
+enum dpa_cls_hm_insert_modify_flags {
+	/* Ethernet and PPPoE insert group */
+	DPA_CLS_HM_INS_MOD_ETH_HEADER		= 0x01,
+	DPA_CLS_HM_INS_MOD_NUM_QTAGS		= 0x02,
+	DPA_CLS_HM_INS_MOD_QTAGS_ARRAY		= 0x04,
+	DPA_CLS_HM_INS_MOD_PPPoE_HEADER		= 0x08,
+
+	/* PPP insert group */
+	DPA_CLS_HM_INS_MOD_PPP_PID		= 0x10,
+
+	/* Custom insert group */
+	DPA_CLS_HM_INS_MOD_CUSTOM_OFFSET	= 0x20,
+	DPA_CLS_HM_INS_MOD_CUSTOM_SIZE		= 0x40,
+	DPA_CLS_HM_INS_MOD_CUSTOM_DATA		= 0x80
+};
+
+/* General insert parameters */
+struct dpa_cls_hm_custom_ins_params {
+	/*
+	 * Offset in bytes relative to the start of the frame to insert new
+	 * header at.
+	 */
+	uint8_t		offset;
+
+	/* The size in bytes of the header to insert */
+	uint8_t		size;
+
+	/*
+	 * The data buffer containing the header to insert. This buffer must be
+	 * at least [size] bytes long
+	 */
+	uint8_t		*data;
+};
+
+/* Egress insert header manipulation low level driver resources */
+struct dpa_cls_hm_insert_resources {
+	/*
+	 * Handle to either an internal header insert or an internal protocol
+	 * specific header insert node. This is a FMan driver header
+	 * manipulation node handle and it is mandatory for the import to
+	 * succeed.
+	 */
+	void	*insert_node;
+};
+
+/* Ethernet header insert params */
+struct dpa_cls_hm_eth_ins_params {
+	/* Ethernet header to insert */
+	struct ethhdr				eth_header;
+
+	/*
+	 * Number of VLAN tags to insert. If zero, no VLAN tags will be inserted
+	 * in the packet
+	 */
+	unsigned int				num_tags;
+
+	/*
+	 * Relevant only if [num_tags] is not zero. Contains an array with the
+	 * data of the VLAN QTags to insert
+	 */
+	struct vlan_header			qtag[DPA_CLS_HM_MAX_VLANs];
+};
+
+/* PPPoE header insert params */
+struct dpa_cls_hm_pppoe_ins_params {
+	/*
+	 * Parameters of the Ethernet header to insert together with PPPoE
+	 * header
+	 */
+	struct dpa_cls_hm_eth_ins_params	eth;
+
+	/* PPPoE header to insert */
+	struct pppoe_header			pppoe_header;
+};
+
+/* Ethernet header insert params */
+struct dpa_cls_hm_insert_params {
+	/* Specifies the type of insert header manipulation */
+	enum dpa_cls_hm_insert_type			type;
+
+	union {
+		/*
+		 * Ethernet header insert parameters if type is "insert
+		 * Ethernet"
+		 */
+		struct dpa_cls_hm_eth_ins_params	eth;
+
+		/* PPPoE header insert parameters if type is "insert PPPoE" */
+		struct dpa_cls_hm_pppoe_ins_params	pppoe;
+
+		/*
+		 * PPP PID value to use in the PPP header if type is "insert
+		 * PPP"
+		 */
+		uint16_t				ppp_pid;
+
+		/*
+		 * Custom insert header manipulation operation parameters.
+		 * These are relevant only if a custom insert header
+		 * manipulation operation is selected.
+		 */
+		struct dpa_cls_hm_custom_ins_params	custom;
+	} param;
+
+	/*
+	 * Handle to the low level driver PCD to use when creating the header
+	 * manipulation object.
+	 */
+	void							*fm_pcd;
+};
+
+/* Update header manipulation op flags */
+enum dpa_cls_hm_update_op_flags {
+	DPA_CLS_HM_UPDATE_NONE			= 0,
+
+	DPA_CLS_HM_UPDATE_IPv4_UPDATE		= 0x01,
+	DPA_CLS_HM_UPDATE_IPv6_UPDATE		= 0x02,
+	DPA_CLS_HM_UPDATE_UDP_TCP_UPDATE	= 0x04,
+
+	DPA_CLS_HM_REPLACE_IPv4_BY_IPv6		= 0x08,
+	DPA_CLS_HM_REPLACE_IPv6_BY_IPv4		= 0x10
+};
+
+/* Update header manipulation field flags */
+enum dpa_cls_hm_l3_field_flags {
+	DPA_CLS_HM_IP_UPDATE_IPSA		= 0x01,
+	DPA_CLS_HM_IP_UPDATE_IPDA		= 0x02,
+	DPA_CLS_HM_IP_UPDATE_TOS_TC		= 0x04,
+	DPA_CLS_HM_IP_UPDATE_ID			= 0x08,
+	DPA_CLS_HM_IP_UPDATE_TTL_HOPL_DECREMENT	= 0x10
+};
+
+/* L4 header update field flags */
+enum dpa_cls_hm_l4_field_flags {
+	DPA_CLS_HM_L4_UPDATE_SPORT		= 0x01,
+	DPA_CLS_HM_L4_UPDATE_DPORT		= 0x02,
+	DPA_CLS_HM_L4_UPDATE_CALCULATE_CKSUM	= 0x04
+};
+
+/*
+ * Flag values indicating which attributes of the update header manipulation
+ * to modify
+ */
+enum dpa_cls_hm_update_modify_flags {
+	DPA_CLS_HM_UPDATE_MOD_IPHDR		= 0x0001,
+
+	/* L3 protocol flags group */
+	DPA_CLS_HM_UPDATE_MOD_SIP		= 0x0002,
+	DPA_CLS_HM_UPDATE_MOD_DIP		= 0x0004,
+	DPA_CLS_HM_UPDATE_MOD_TOS_TC		= 0x0008,
+	DPA_CLS_HM_UPDATE_MOD_IP_ID		= 0x0010,
+	DPA_CLS_HM_UPDATE_MOD_L3_FLAGS		= 0x0020,
+
+	/* L4 protocol flags group */
+	DPA_CLS_HM_UPDATE_MOD_SPORT		= 0x0040,
+	DPA_CLS_HM_UPDATE_MOD_DPORT		= 0x0080,
+	DPA_CLS_HM_UPDATE_MOD_L4_FLAGS		= 0x0100,
+
+	DPA_CLS_HM_UPDATE_MOD_IP_FRAG_MTU	= 0x0200,
+	DPA_CLS_HM_UPDATE_MOD_IP_FRAG_SCRATCH_BPID = 0x0400
+};
+
+/* L3 protocols field update parameters */
+struct dpa_cls_hm_l3_update_params {
+	/* New source IP address */
+	struct dpa_offload_ip_address		ipsa;
+
+	/* New destination IP address */
+	struct dpa_offload_ip_address		ipda;
+
+	/* New TOS (for IPv4) or Traffic Class (for IPv6) */
+	uint8_t					tos_tc;
+
+	/*
+	 * Initial IPv4 ID. This is used only if op_flags selected IPv4 update
+	 */
+	uint16_t				initial_id;
+
+	/*
+	 * A combination of flags designating the header fields to replace. The
+	 * available options are defined in the dpa_cls_hm_l3_update_params enum
+	 */
+	int					field_flags;
+};
+
+/* L4 protocols field update parameters */
+struct dpa_cls_hm_l4_update_params {
+	uint16_t	sport; /* new L4 source port value */
+	uint16_t	dport; /* new L4 destination port value */
+
+	/*
+	 * A combination of flags designating the header fields to replace. The
+	 * available options are defined in the dpa_cls_hm_l4_update_params enum
+	 */
+	int		field_flags;
+};
+
+/* Egress update header manipulation low level driver resources */
+struct dpa_cls_hm_update_resources {
+	/*
+	 * Handle to a header manipulation node with different header
+	 * manipulations enabled, depending on the options selected in the
+	 * parameters: local IPv4/IPv6 update header manipulation, a local
+	 * TCP/UDP update header manipulation and an internal IP header replace.
+	 * This is a FMan driver header manipulation node handle and it is
+	 * optional (can be NULL in case no L3 or L4 field updates or header
+	 * replace features are enabled for this flow)
+	 */
+	void	*update_node;
+
+	/*
+	 * Handle to the IP fragmentation node. This is a FMan driver header
+	 * manipulation node handle and it is optional (can be NULL in case no
+	 * IP fragmentation is enabled for this flow).
+	 */
+	void	*ip_frag_node;
+};
+
+/* Egress update header manipulation parameters */
+struct dpa_cls_hm_update_params {
+	/*
+	 * Flags defining the header manipulation operations to perform. They
+	 * are a combination of the flags defined in the
+	 * dpa_cls_hm_update_op_flags enum.
+	 */
+	int						op_flags;
+
+	union {
+		/*
+		 * IPv4 header data. This header is used IPv6 to IPv4 header
+		 * replace.
+		 */
+		struct iphdr				ipv4_hdr;
+
+		/*
+		 * IPv6 header data. This header is used for IPv4 to IPv6
+		 * header replace.
+		 */
+		struct ipv6_header			ipv6_hdr;
+	} replace;
+
+	union {
+		/*
+		 * L3 protocol field values. This data is used for L3 protocol
+		 * header updates
+		 */
+		struct dpa_cls_hm_l3_update_params	l3;
+
+		/*
+		 * L4 protocol field values. This data is used for L4 protocol
+		 * header updates.
+		 */
+		struct dpa_cls_hm_l4_update_params	l4;
+	} update;
+
+	/*
+	 * IP fragmentation parameters. This is an optional operation and can
+	 * be disabled.
+	 */
+	struct dpa_cls_hm_ip_frag_params		ip_frag_params;
+
+	/*
+	 * Handle to the low level driver PCD to use when creating the header
+	 * manipulation object.
+	 */
+	void							*fm_pcd;
+};
+
+/* VLAN specific header manipulation operation types */
+enum dpa_cls_hm_vlan_type {
+	DPA_CLS_HM_VLAN_INGRESS,
+	DPA_CLS_HM_VLAN_EGRESS,
+	DPA_CLS_HM_VLAN_LAST_ENTRY
+};
+
+/* Types of supported VLAN update operations */
+enum dpa_cls_hm_vlan_update_type {
+	DPA_CLS_HM_VLAN_UPDATE_NONE,
+	DPA_CLS_HM_VLAN_UPDATE_VPri,	/* manual VPri update */
+	DPA_CLS_HM_VLAN_UPDATE_VPri_BY_DSCP,
+	DPA_CLS_HM_VLAN_UPDATE_LAST_ENTRY
+};
+
+/* VLAN QTag identifier */
+enum dpa_cls_hm_vlan_count {
+	DPA_CLS_HM_VLAN_CNT_NONE,
+	DPA_CLS_HM_VLAN_CNT_1QTAG,	/* outer QTag */
+	DPA_CLS_HM_VLAN_CNT_2QTAGS,	/* outer most 2 QTags */
+	DPA_CLS_HM_VLAN_CNT_3QTAGS,	/* outer most 3 QTags */
+	DPA_CLS_HM_VLAN_CNT_4QTAGS,	/* outer most 4 QTags */
+	DPA_CLS_HM_VLAN_CNT_5QTAGS,	/* outer most 5 QTags */
+	DPA_CLS_HM_VLAN_CNT_6QTAGS,	/* outer most 6 QTags */
+	DPA_CLS_HM_VLAN_CNT_ALL_QTAGS,
+	DPA_CLS_HM_VLAN_CNT_LAST_ENTRY
+};
+
+/*
+ * Flag values indicating which attributes of the VLAN specific header
+ * manipulation to modify
+ */
+enum dpa_cls_hm_vlan_modify_flags {
+	/* This flag cannot be combined with any other flags */
+	DPA_CLS_HM_VLAN_MOD_INGRESS_NUM_QTAGS		= 0x01,
+
+	DPA_CLS_HM_VLAN_MOD_EGRESS_NUM_QTAGS		= 0x02,
+	DPA_CLS_HM_VLAN_MOD_EGRESS_QTAGS_ARRAY		= 0x04,
+	DPA_CLS_HM_VLAN_MOD_EGRESS_UPDATE_OP		= 0x08,
+	DPA_CLS_HM_VLAN_MOD_EGRESS_VPRI			= 0x10,
+	DPA_CLS_HM_VLAN_MOD_EGRESS_DSCP_TO_VPRI_ARRAY	= 0x20
+};
+
+/* Ingress VLAN specific header manipulation parameters */
+struct dpa_cls_hm_ingress_vlan_params {
+	/* Number of VLAN tags to remove */
+	enum dpa_cls_hm_vlan_count		num_tags;
+};
+
+/* Egress VLAN specific header manipulation parameters */
+struct dpa_cls_hm_egress_vlan_params {
+	enum dpa_cls_hm_vlan_update_type	update_op;
+
+	/*
+	 * Number of VLAN tags to insert. If zero, no VLAN tags will be
+	 * inserted in the packet.
+	 */
+	unsigned int				num_tags;
+
+	/*
+	 * Relevant only if [num_tags] is not zero. Contains an array with the
+	 * data of the VLAN tags to insert.
+	 */
+	struct vlan_header			qtag[DPA_CLS_HM_MAX_VLANs];
+
+	union {
+		/*
+		 * New VPri field value if [update_flag] selects manual VPri
+		 * update.
+		 */
+		uint8_t vpri;
+
+		/*
+		 * DSCP-to-VPri mapping table to use for VPri field update if
+		 * [update_flag] selects VPri update by mapping to DSCP.
+		 */
+		uint8_t dscp_to_vpri[DPA_CLS_HM_DSCP_TO_VPRI_TABLE_SIZE];
+
+	} update_params;
+};
+
+/* VLAN specific header manipulation low level resources */
+struct dpa_cls_hm_vlan_resources {
+	/*
+	 * Handle to a header manipulation node with different operations
+	 * depending on the selected type of VLAN specific header manipulation.
+	 *
+	 * In case of VLAN ingress header manipulation this is a VLAN protocol
+	 * specific removal node.
+	 *
+	 * In case of VLAN egress header manipulation this is a header
+	 * manipulation node which may combine an internal header insert (in
+	 * case there are VLANs to insert) with a protocol specific VLAN update
+	 * operation.
+	 *
+	 * This is a FMan driver header manipulation node handle and it is
+	 * mandatory for the import to succeed.
+	 */
+	void	*vlan_node;
+};
+
+/* VLAN specific header manipulation parameters */
+struct dpa_cls_hm_vlan_params {
+	/* Selects the type of the VLAN specific header manipulation */
+	enum dpa_cls_hm_vlan_type			type;
+
+	union {
+		/* Parameters for ingress VLAN header manipulations */
+		struct dpa_cls_hm_ingress_vlan_params	ingress;
+
+		/* Parameters for egress VLAN header manipulations */
+		struct dpa_cls_hm_egress_vlan_params	egress;
+	} params;
+
+	/*
+	 * Handle to the low level driver PCD to use when creating the header
+	 * manipulation object.
+	 */
+	void						*fm_pcd;
+};
+
+/* MPLS specific header manipulation operation types */
+enum dpa_cls_hm_mpls_type {
+	DPA_CLS_HM_MPLS_INSERT_LABELS,
+	DPA_CLS_HM_MPLS_REMOVE_ALL_LABELS,
+	DPA_CLS_HM_MPLS_LAST_ENTRY
+};
+
+/*
+ * Flag values indicating which attributes of the MPLS specific header
+ * manipulation to modify
+ */
+enum dpa_cls_hm_mpls_modify_flags {
+	DPA_CLS_HM_MPLS_MOD_NUM_LABELS	= 0x01,
+	DPA_CLS_HM_MPLS_MOD_HDR_ARRAY	= 0x02,
+};
+
+/* MPLS specific header manipulation low level driver resources */
+struct dpa_cls_hm_mpls_resources {
+	/*
+	 * Handle to the protocol specific header insert (MPLS) or to the
+	 * protocol specific header removal (MPLS) node. This is a FMan driver
+	 * header manipulation node handle and it is mandatory for the import
+	 * to succeed.
+	 */
+	void	*ins_rm_node;
+};
+
+/* MPLS specific header manipulation parameters */
+struct dpa_cls_hm_mpls_params {
+	/* Specifies the type of header manipulation */
+	enum dpa_cls_hm_mpls_type	type;
+
+	/*
+	 * Stores the MPLS labels to insert if the operation type is "insert
+	 * MPLS labels"
+	 */
+	struct mpls_header		mpls_hdr[DPA_CLS_HM_MAX_MPLS_LABELS];
+
+	/*
+	 * Number of MPLS labels to insert. This is relevant only if the
+	 * operation type is "insert MPLS labels" */
+	unsigned int			num_labels;
+
+	/*
+	 * Handle to the low level driver PCD to use when creating the header
+	 * manipulation object.
+	 */
+	void				*fm_pcd;
+};
+
+
+/*
+ * Creates or imports a NAT type header manipulation object. If the function is
+ * successful it returns at the [hmd] location the descriptor of the created
+ * header manipulation object.
+ *
+ * If the [res] parameter is provided, the function will import the low level
+ * driver resources specified therein rather than create them. In this case the
+ * [fm_pcd] handle in the parameters structure is not used and can be provided
+ * as NULL. When working in this mode the function doesn't allocate MURAM.
+ */
+int dpa_classif_set_nat_hm(const struct dpa_cls_hm_nat_params	*nat_params,
+			int					next_hmd,
+			int					*hmd,
+			bool					chain_head,
+			const struct dpa_cls_hm_nat_resources	*res);
+
+/*
+ * Modify the parameters of an existing NAT header manipulation.
+ *
+ * [modify_flags] is a combination of flags indicating which header manipulation
+ * attributes to modify (and hence indicating which of the attributes in the
+ * [new_nat_params] data structure are valid). Select the flag values from the
+ * dpa_cls_hm_nat_modify_flags enum and combine them using the "or" logical
+ * operand.
+ */
+int dpa_classif_modify_nat_hm(int hmd,
+	const struct dpa_cls_hm_nat_params *new_nat_params, int modify_flags);
+
+/*
+ * Creates or imports a forwarding type header manipulation object. DPA
+ * Classifier takes into account an Ethernet/IP frame to start with and,
+ * depending on the selection of output interface type, it decides what header
+ * manipulations are necessary.
+ *
+ * If the [res] parameter is provided, the function will import the low level
+ * driver resources specified therein rather than create them. In this case the
+ * [fm_pcd] handle in the parameters structure is not used and can be provided
+ * as NULL. When working in this mode the function doesn't allocate MURAM.
+ */
+int dpa_classif_set_fwd_hm(const struct dpa_cls_hm_fwd_params	*fwd_params,
+			int					next_hmd,
+			int					*hmd,
+			bool					chain_head,
+			const struct dpa_cls_hm_fwd_resources	*res);
+
+/*
+ * Modify the parameters of an existing forwarding type header manipulation.
+ *
+ * [modify_flags] is a combination of flags indicating which header manipulation
+ * attributes to modify (and hence indicating which of the attributes in the
+ * [new_fwd_params] data structure are valid). Select the flag values from the
+ * dpa_cls_hm_fwd_modify_flags enum and combine them using the "or" logical
+ * operand.
+ */
+int dpa_classif_modify_fwd_hm(int hmd,
+	const struct dpa_cls_hm_fwd_params *new_fwd_params, int modify_flags);
+
+/* Creates or imports a remove type header manipulation object.
+ *
+ * If the [res] parameter is provided, the function will import the low level
+ * driver resources specified therein rather than create them. In this case the
+ * [fm_pcd] handle in the parameters structure is not used and can be provided
+ * as NULL. When working in this mode the function doesn't allocate MURAM.
+ */
+int dpa_classif_set_remove_hm(const struct dpa_cls_hm_remove_params
+	*remove_params, int next_hmd, int *hmd, bool chain_head,
+	const struct dpa_cls_hm_remove_resources *res);
+
+/*
+ * Modify the parameters of an existing remove type header manipulation.
+ *
+ * [modify_flags] is a combination of flags indicating which header manipulation
+ * attributes to modify (and hence indicating which of the attributes in the
+ * [new_remove_params] data structure are valid). Select the flag values from
+ * the dpa_cls_hm_remove_modify_flags enum and combine them using the "or"
+ * logical operand.
+ */
+int dpa_classif_modify_remove_hm(int hmd,
+	const struct dpa_cls_hm_remove_params *new_remove_params,
+	int modify_flags);
+
+/*
+ * Creates or imports an insert type header manipulation object.
+ *
+ * If the [res] parameter is provided, the function will import the low level
+ * driver resources specified therein rather than create them. In this case the
+ * [fm_pcd] handle in the parameters structure is not used and can be provided
+ * as NULL. When working in this mode the function doesn't allocate MURAM.
+ */
+int dpa_classif_set_insert_hm(const struct dpa_cls_hm_insert_params
+	*insert_params, int next_hmd, int *hmd, bool chain_head,
+	const struct dpa_cls_hm_insert_resources *res);
+
+/*
+ * Modify the parameters of an existing insert header manipulation.
+ *
+ * [modify_flags] is a combination of flags indicating which header manipulation
+ * attributes to modify (and hence indicating which of the attributes in the
+ * [new_insert_params] data structure are valid). Select the flag values from
+ * the dpa_cls_hm_insert_modify_flags enum and combine them using the "or"
+ * logical operand.
+ */
+int dpa_classif_modify_insert_hm(int hmd,
+	const struct dpa_cls_hm_insert_params *new_insert_params,
+	int modify_flags);
+
+/*
+ * Creates or imports an update type header manipulation object.
+ *
+ * If the [res] parameter is provided, the function will import the low level
+ * driver resources specified therein rather than create them. In this case the
+ * [fm_pcd] handle in the parameters structure is not used and can be provided
+ * as NULL. When working in this mode the function doesn't allocate MURAM.
+ */
+int dpa_classif_set_update_hm(const struct dpa_cls_hm_update_params
+	*update_params, int next_hmd, int *hmd, bool chain_head,
+	const struct dpa_cls_hm_update_resources *res);
+
+/*
+ * Modify the parameters of an existing update header manipulation.
+ *
+ * [modify_flags] is a combination of flags indicating which header manipulation
+ * attributes to modify (and hence indicating which of the attributes in the
+ * [new_update_params] data structure are valid). Select the flag values from
+ * the dpa_cls_hm_update_modify_flags enum and combine them using the "or"
+ * logical operand.
+ */
+int dpa_classif_modify_update_hm(int hmd,
+	const struct dpa_cls_hm_update_params *new_update_params,
+	int modify_flags);
+
+/*
+ * Creates or imports a VLAN specific header manipulation (either ingress or
+ * egress) object.
+ *
+ * If the [res] parameter is provided, the function will import the low level
+ * driver resources specified therein rather than create them. In this case the
+ * [fm_pcd] handle in the parameters structure is not used and can be provided
+ * as NULL. When working in this mode the function doesn't allocate MURAM.
+ */
+int dpa_classif_set_vlan_hm(const struct dpa_cls_hm_vlan_params	*vlan_params,
+			int					next_hmd,
+			int					*hmd,
+			bool					chain_head,
+			const struct dpa_cls_hm_vlan_resources	*res);
+
+/*
+ * Modify the parameters of an existing VLAN specific header manipulation.
+ *
+ * [modify_flags] is a combination of flags indicating which header manipulation
+ * attributes to modify (and hence indicating which of the attributes in the
+ * [new_vlan_params] data structure are valid). Select the flag values from the
+ * dpa_cls_hm_vlan_modify_flags enum and combine them using the "or" logical
+ * operand.
+ */
+int dpa_classif_modify_vlan_hm(int hmd,
+	const struct dpa_cls_hm_vlan_params *new_vlan_params, int modify_flags);
+
+/*
+ * Creates or imports a MPLS specific header manipulation object.
+ *
+ * If the [res] parameter is provided, the function will import the low level
+ * driver resources specified therein rather than create them. In this case the
+ * [fm_pcd] handle in the parameters structure is not used and can be provided
+ * as NULL. When working in this mode the function doesn't allocate MURAM.
+ */
+int dpa_classif_set_mpls_hm(const struct dpa_cls_hm_mpls_params	*mpls_params,
+			int					next_hmd,
+			int					*hmd,
+			bool					chain_head,
+			const struct dpa_cls_hm_mpls_resources	*res);
+
+/*
+ * Modify the parameters of an existing MPLS specific header manipulation.
+ *
+ * [modify_flags] is a combination of flags indicating which header manipulation
+ * attributes to modify (and hence indicating which of the attributes in the
+ * [new_mpls_params] data structure are valid). Select the flag values from the
+ * dpa_cls_hm_mpls_modify_flags enum and combine them using the "or" logical
+ * operand.
+ */
+int dpa_classif_modify_mpls_hm(int hmd,
+	const struct dpa_cls_hm_mpls_params *new_mpls_params, int modify_flags);
+
+/*
+ * Releases a header manipulation object and frees up all related resources
+ * allocated for it. The header manipulation operations must be removed in the
+ * reverse order they were created in (i.e. starting with the header
+ * manipulation chain head and working towards the tail).
+ */
+int dpa_classif_free_hm(int hmd);
+
+
 #endif /* __FSL_DPA_CLASSIFIER_H */
-- 
1.7.5.4

